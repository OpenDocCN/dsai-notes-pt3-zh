# P7ï¼šL7 - ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(ä¸‹) - ShowMeAI - BV1Tf4y1L7wg

Helloï¼Œ welcome everybody to the next lecture of the Advanced deep Learning for Computer visionsion Coã€‚

In the last lectureï¼Œ we have talkedã€‚The first time about generative modelsã€‚

 specifically we've been talking for quite a while about GNsã€‚

 generative factorarial networks and today I would like to continue the discussion and the first thing I would like to talk about are common G architectures and there's of course a large variety of different architecturesã€‚

 we have seen a couple of them alreadyï¼Œ we have seen the DCganã€‚

 the deep convolutional G that's a very well basic architecture that's a good starting point whenever you're starting to train againã€‚

 this is probably one of the architectures you would like to get started withã€‚Howeverï¼Œ there areã€‚

Many architectures of course and by no means I can go over all of them and one important concept about GNs I would like to address though is how do you scale them amount to bigger resolution so the challenges for most parts is that again can generate very well the distribution locally and I mean that's kind of obvious because we having convolutional kernels and these convolutional kernels have always a local context there's a receptive field in locally speaking they can help you to produce good detailã€‚

 but a global structure is pretty challenging for GNs and this problem becomes more challenging when the resolution of the images becomes higher and this traditionally know since people have been playing around with GNs and there has been a lot of research was one of the core challenges how do you generate higher resolution imagesã€‚

And one of the things that people have been adopting is very related to multiscale optimization so what people are doing there is like the optimizing a problem on different scales and then using the respective initializations for the next level and this is something that and people are doing for GAsza as well in this case what you can do is you can start here and basically on a lower resolution so the idea is you you have a generator in this case here it takes random variable Z here is input rightã€‚

 generates a low resolution imageã€‚It then ups samplesles that oneã€‚

It has another generator that adds some detail now on top of that imageã€‚It ups samplesles againã€‚

 adds more detail on itï¼Œ adds more detail on it on and so on and what you have here is you basically have multiple scales of generations right you have one generator here G3 takes random vector Zs input G2 G1 and G0 and the idea is that you essentially have the structure given at the lower levels and then you have a learnable lab you have an upsseling and then you have a learnable another learnable generator that adds the detail to that in order to match that resolution and'm not arguing here about the specific quality of these images again keep in mind that these image this work has been already published five years agoã€‚

 so this was still in the beginning phases andã€‚GNs have been developed much further by nowã€‚

 but the core idea is still very relevantï¼Œ rightï¼Œ you basically go aheadï¼Œ you generateã€‚

Things at a lower resolution first with a generator that sees kind of the global contextã€‚

 but it doesn't know about the diesel so much right so that's what this guy here is doingã€‚

 And then you can ups samplebleï¼Œ ups sampleble ups samplebleã€‚

 ups sampleble and eventually you're going to get a higher resolution imageã€‚ğŸ˜Šã€‚

Now the idea here is of course we have a lot of generators in this case we have four different generatorsã€‚

 what we want to do is we also want to have a discriminator at every level and the idea is pretty straightforward right we' doing the same thing what we've done for the generators and it's just saying ohã€‚

 this image that has been generated here rightï¼ŸSo the image that has been generated here is being discriminatedã€‚

 the image has been generated here is being discriminated and so on right so every resolution you're gonna have a discriminator so we have here G0 G1 G2 G3 and we're gonna have discriminationriminators D0 t1 D2 and D3 that can do the discri of every little and that is exactly2 and what I've said is basically here you discriminating oh is that is that detail enough and here is basically more is the structure enough So you have different ways to do that this specific paper herepot at all what they're doing is they're doing the discrimination essentiallyã€‚

On these deelta imagesï¼Œ you can do it this wayï¼Œ you can also do it on the original imagesã€‚

 so they have been verbs that do it either way but I think the core concept what's very important about this one is I feel you can actually do this at different scalesã€‚

And because of thatï¼Œ you have the advantage that you can force the network to focus either on global structureã€‚

On the cos levelsï¼Œ rightï¼Œ or you can focus on the fine scale visionã€‚Andã€‚Againã€‚

 I mentioned that these images might be not too impressive at this pointï¼Œ but againã€‚

 be aware this is a paper that was published five years agoã€‚

 But the reason why Im mentioning in many of the recent works have actually built up on thatã€‚

 And one of the very notable works is the progressive growing GNsã€‚

 This was a paper that in video published in 2017ã€‚ and it has a very very similar conceptã€‚

 And here you can already see that these images are already muchï¼Œ muchï¼Œ much higher qualityã€‚ In factã€‚

 this was one of the papers where where people thoughtï¼Œ yeahï¼Œ GNsï¼Œ okayã€‚

 they don't quite work so wellã€‚ And then these guys published a paper and then it realizedï¼Œ okayã€‚

 it looks pretty goodã€‚ they're still a way to get GNs to workã€‚ğŸ˜Šã€‚

It doesn't make the game training a lot easierï¼Œ but there the least there's like proof that you canã€‚

 this experimental validation that you can actually generate for a good imageã€‚

And the idea of this paper is very similar to the previous oneã€‚

 except they don't keep all the multiscale discriminators and generators around in the same wayã€‚

 but they naturally grow both generator and discriminator at the same time So what they do they have a generator here that goes from a lant code generates a 4 by4 imageã€‚

Gives gives you an image4 by4 right straightforward has 16 pixelsã€‚

 you have a discriminator here and it checks if this4 by4 image is real or is it not realã€‚

And you train this for a while until you get a reasonable distribution here and of course4 by4 you won't see a whole lot rightã€‚

 but assuming you train this on facesï¼Œ you actually yeah you can train this much easier than if you have a high quality in which because the distribution here is much easier to satisfyã€‚

ğŸ˜Šï¼ŒNowï¼Œ onceã€‚This generator and this discriminator is trained jointly and quote unqu produces good results Then what you do is you expand your networkã€‚

 So what you do is you take the generator and youend another layer here in the middleã€‚

And another block of layers for the discriminator you do the same thingã€‚

 you also have another block hereï¼Œ so the resolution here in the middle becomes8 by8 instead of4 by4ã€‚

 but the core idea is that these lower resolution or these networks and these four by four and the four by4 generating discriminatorã€‚

 these ones they're actually going to stay there for a secondã€‚

 so the network this bigger network now is being initialized with the pre-trained version of these ones and then the whole network here is traallyã€‚

So the idea is in a sense that oh this 4 by4 generator produced kind of reasonable resultsã€‚

 this 4 by4 generator was kind of okay at discriminating 4 by4 imagesã€‚

Let let's have another up and wing block basicallyã€‚ğŸ˜Šï¼ŒAndã€‚

Then we getting it by images and the is of course that this stabilizes the training a lot right because you have already network weights in the lower solutions that knows how to generate the global structure and then it produces kind of the fine scale detail laterã€‚

And this is being actually this training processï¼Œ this progressive growing training process is being repeated until you end up at 1024 square images so it goes basically HD images almost rightã€‚

 and it takesï¼Œ of courseï¼Œ a long time right instead of having a single training processã€‚

 you know have manyï¼Œ many training processes at every resolution howeverã€‚

Each training process is already being initialized with the previous layersï¼Œ rightï¼Ÿ

So yeah is yeah that is one interesting thingï¼Œ so basically yes you have more training processesã€‚

 but each training process in principle takes not so much time because you already have good initializations in the first layersã€‚

ğŸ˜Šï¼Œå—¯ã€‚Yeahï¼Œ so they train thisã€‚ they basically have a fixed schedule when to append the next layer and at the end of the day they can actually produce pretty good resultsã€‚

 I want to go quickly into a few details hereï¼Œ So one thing that is not so easy to understand is how do you append thisã€‚

 So what's important is that you append or expand the network both generated and discriminator at the same timeã€‚

 of courseï¼Œ if the generator gets bigger I also need a bigger discriminatorã€‚ğŸ˜Šï¼ŒNowã€‚

 that is something you have to be very careful how to do thatã€‚

But let's have a look at the quick details so the generator in this caseã€‚

 its a4 by4 sorry it's a 4 by4 image that you're generatingã€‚

 you're taking this4 by4 image this is actually a feature map right nowï¼Œ this is an important thingã€‚

What they doï¼Œ like the problemï¼Œ what you're going to do is you not actually this four by four network here is not generating an image by itselfã€‚

But what it's doing it's actually generating a feature setã€‚

 and then what you do is you have a two RGB layerï¼Œ so to say which is basically a series of one by one convolutionsã€‚

 it's actually three by by one convolutions right that converts these features hereã€‚



![](img/3f357201e6169fecca51f9c88788bc01_1.png)

Into an R T imageã€‚And now this RTB image is the output of the generator at this 4x4 resolutionã€‚

Now the advantage of that is when you're now appending the next blockã€‚

 you're simply doing an upseling here and having the8 by8 blocks hereã€‚

 but now you're not taking the RGB values here or the assembleã€‚

 but rather you're taking the features of this 4x4 block that you assemble and then again once you have here feature map you're doing another two RGB operator and you're getting the 8 by8 imageã€‚

So the quality is you actually can maintain the feature maps hereã€‚

 but you can still have a two RGB operator that gets you at the current resolution to the RGB image right and you do this before you do it for8 by8 and so on you just do it all the way down in this multiscale hierarchyã€‚

The second thing what you're doing now isï¼Œ let's say we trained this party here firstã€‚

 the four by four partï¼Œ and we are pending like this party here afterwardsã€‚

This would be a very rapid changeï¼Œ rightï¼Œ and the problem would beã€‚

That suddenly you're getting a whole new loss and this whole new loss is going to potentially destabilize your training processã€‚

So what these guys are doingï¼Œ they're suggesting ohï¼Œ let's do a linear crossphã€‚

 so we don't just have like a hard transition hereã€‚

 but we have a linear crossf between how to take the losses from here and how to take the losses from here and then you just gradually you gradually include this part for training basically right so this helps you a lot because then it's not like oh now suddenly you have like another layer and we kind of make the training very unstableã€‚

 but like the step by step you're getting more weight to the higher resolutionã€‚And anywayã€‚

 and then you basically you continue that rightï¼Œ you go to the next levelã€‚

 go to the next level and so onã€‚The discriminator trained in the exact is doing exact same thing rightã€‚

 and the discriminator just you have a from RGV operator that basically takes this image againã€‚

 goes to an 8 by8 feature mapã€‚Downs it4 by four and so on right but same idea so there's two key concepts you can just append another layer to the you can just generate an RGB image and then absent the RGB imageã€‚

 but rather than that you want to take the feature mapsã€‚And the second thing isã€‚

 you actually want to go ahead andã€‚Yeahï¼Œ you actually to want to go ahead and do this linear crossph that makes like a smooth transition right you have a linear blending function basically between the two levelsã€‚

Yeahï¼Œ that's the core ideaï¼Œ but the principle of again is still very similar to beforeã€‚

And the nice thing is with that nowï¼Œ you can train it and you have a relatively stable process to produceã€‚

ğŸ˜Šï¼ŒHigher resolution results or relatively high resolution resultsã€‚

So let's have a look at the training process hereã€‚ğŸ˜Šã€‚



![](img/3f357201e6169fecca51f9c88788bc01_3.png)

This is the the training process that is being visualized here So here we have yeah we have the progressiveive growing shown on the right hand side and here we have the final resolution as the output right So here we see it's still relatively low resolution now comes the next levelã€‚

Now comes another levelï¼Œ and so onã€‚And you see it's you know it's getting sharper and sharper and sharper training process is a little bit funkyã€‚

 but as you continue trainingï¼Œ it gets better and better and this is an output here about the 34 hoursã€‚

 35 hours of trainingï¼Œ you have to be aware this is an network they trained I think on a DTX which is an HGPU high end machineã€‚

 so that's actually quite some computational effort behind it in order to produce these resultsã€‚

But you can already see wellï¼Œ I meanï¼Œ the results hereï¼Œ they look actually pretty decent rightã€‚

 sure you have some artifacts like hereã€‚But globally speakingï¼Œ you actually you're getting veryã€‚

 veryï¼Œ very good results hereã€‚What is interestingï¼Œ once you have trained that networkã€‚

The really cool thing is you have this manifold that you've got rightã€‚

 so you can go from latent space to an imageã€‚And what's fun about it is when you're training again and you train it successfullyã€‚

 you can take this latent space and interpolate in this lant space in order to you know kind of do all kind of funky operations and this is what these guys have been doing here tooã€‚

ğŸ˜Šï¼ŒSo they doã€‚1024 times 1024 resolutions outputï¼Œ and now they do linear interpos in the latent spaceã€‚



![](img/3f357201e6169fecca51f9c88788bc01_5.png)

And it's kind of funny like how the transitions between one and another imageã€‚Happens hereã€‚

 but againï¼Œ these are simply take a bunch of random vectors right and then like interlate and traverse the latent space here and it's kind of mesmerizing how things transition between the different between the different photos of people you can also see of course not everything is perfect So you see for instance here when you look at the hair like this part on the side is always always a bit tricky like how they grow hair and so onã€‚

 how the mouth changes rightã€‚ğŸ˜Šã€‚

![](img/3f357201e6169fecca51f9c88788bc01_7.png)

But it's pretty impressive compared to all the previous scanned papers we have seenã€‚

 this is one of the papers thatï¼Œ you knowï¼Œ showed really viable results at a higher resolution that most other papers have been showing beforeã€‚

 admittedmittedlyï¼Œ they did use a lot of GPsã€‚ğŸ˜Šï¼ŒBut I think the results look pretty stunningã€‚

 so I mean I'm a big fan of this paper because it also has a nice kind of intuition behind it like they use this multiscale training right and they can grow the network and can do this kind of classical multi scale optimization what people typically useã€‚

All rightã€‚So I wanted to show you a couple of these results just to see what is possible with currentgansã€‚

Of courseï¼Œ we can't cover all the G papers right now in this lectureã€‚

There's a lot of variations right What I would like you to understand is a bit more is like what are the trends rightã€‚

 what can you doã€‚There'sï¼Œ there's a lot of different things in terms of the loss functionsï¼Œ andã€‚

The one thing isï¼Œ I don't think the loss functions make the biggest differenceã€‚ I meanã€‚

 most people still use a heuristic loss or a wasachestein lossã€‚

 These are the two kind of dominant losses that have so far emergedï¼Œ I thinkã€‚But yeahã€‚

 there's a lot of papers that do different lossesï¼Œ but in practiceã€‚

 I think they don't make such a big differenceã€‚The big challenge is still that the GNs are really hard to evaluate right and mean of courseã€‚

 if you see a results like this one from the progressive Gã€‚You're going to get prettyã€‚

It's pretty impressiveï¼Œ rightï¼Œ it's like this is what an AI can generate todayï¼Œ it's not too badã€‚Andã€‚

One of the big challenges that is always hidden in research papers is there'sï¼Œ of courseã€‚

 a lot of engineering that went into the respective hyperparrameterid optimizationã€‚

And I wanted to say a few things about that one is basicallyï¼Œ for instanceã€‚

 the progressive game paperï¼Œ they proposed this nice theoretically progressive growing and at this time they produced the best resultsã€‚

ğŸ˜Šï¼Œå—¯ã€‚But one thing is also clearï¼Œ rightï¼Œ They spend a lot of engineering effortsï¼Œ for instanceã€‚

 on the dataã€‚ They created a new data set just specificallyï¼Œ you knowã€‚

 to make the training work hereã€‚ And by new dataï¼Œ you have to basically make sure that the images are all relatively alignedã€‚

 that the nose and the eye regions are relatively aligned they have a specific procedure how to generate the data and so onã€‚

 So there's a lot of data engineering also going into that in order to produce good resultsã€‚

 And that is a veryã€‚ğŸ˜Šï¼ŒA veryï¼Œ very important thing that is worthwhile looking atã€‚å—¯ã€‚ğŸ˜Šï¼ŒOf courseã€‚

 when you're reading a research paperï¼Œ one thing I would like you to do is be a bit cautious about it like what made the big breakthrough happen and there was a pretty funny paper actually this was from Google Brain and what these guys said is basically well the papers called agans are create equal and what they did is they just ran an evolution on a bunch of different G methods and what they found is the loss functionsã€‚

 design choices and stuff like thatã€‚ğŸ˜Šï¼ŒDidn't matter too muchã€‚ Soï¼Œ wellã€‚

 to say it a bit more provocative is basicallyï¼Œ it didn't really matter what Gny used as long as he knew how to optimize itã€‚

 you always could produce good resultsã€‚ You knowï¼Œ some were harder to optimizeã€‚

 Some were easy to optimize someã€‚More prone to more collapse and stuff like thatã€‚

 others were less proneã€‚ But in principleï¼Œ all these G papers kind of could do the same thingã€‚

 and that us to the interesting questionï¼Œ do we have a fundamental way of training gamess right I did try in the last video in last lecture video Try to tell you a little bit what the Gant tricks are right what people can do But nonethelessã€‚

 I mean we have to be very clearï¼Œ It is still veryï¼Œ very tricky to train the GNsã€‚ğŸ˜Šï¼ŒAnd but trickyã€‚

 what that means and practice is often you train itã€‚

 it just collapses and you don't know why or it just doesn't produce better resultsã€‚

 So you don't know whyã€‚ then you change some stuff like learning rateã€‚ It still doesn't get betterã€‚

 right That's kind of the thing what happens when I'm sayingï¼Œ ohï¼Œ it's hard to trainã€‚There's againã€‚

 there's a lot of theories now behind itï¼Œ but practically it's still veryã€‚

 very difficult to train gamessã€‚Goodï¼Œ so I hope I gave you a reasonable overview view of you what current Gs are and kind of the high level trends of G architecturesã€‚

The one thing what we have been doing now is we have been always assuming we have a fully generative modelã€‚

 meaning that we have a la vector Z and from this La vector Z we want to create an image we have only focused on the image domain with a later focus on other things but the one thing I would like to talk about right now is the conditional caseã€‚

Conditional generative adversarial networksï¼Œ This doesn't count only for GNSã€‚ In principleã€‚

 you want to have a conditional case for all generative models because you would like to have some control of the input rightã€‚

 I just don't want to have a single latent vector Z that is just give me random variableã€‚ Tell me ohã€‚

 here is a great imageã€‚That's not so interestingï¼Œ but what's more interesting is if you can actually use these GNs for real world applicationsã€‚

And there's modelling applications such as give me a sketchï¼Œ make a real limit out of itã€‚ All rightã€‚

 add semantic meaning to the manifoldï¼Œ generate me specific faces that smile at laughï¼Œ thatã€‚

 you knowï¼Œ have a certain appearanceï¼Œ certain shadow or certain reflectionï¼Œ stuff like thatã€‚

 And I think without these kind of additional controlsã€‚ğŸ˜Šï¼ŒThe GNs are not so usefulã€‚

 so we have to figure out how to input that in order to make them useful for their respective purposesã€‚

And this is what conditionalgans are So the big question is how do we make them conditionalï¼ŸğŸ˜Šã€‚

The second thing is domain transfer for instance you have like labels you want to transfer that's very relevant for training standard networks Do transfer is still a very tricky and challenging story we have to see if you have time in this class over that I want to fit it in or not but that's also a similar thing right if you have images from day and you want to transfer them to the night and then we train a selfdra car it that is also a case where you would want to have a conditional again but for now we want to talk more about modeling things which I think is pretty interesting for you know entertainment industry and stuff like that there is of course a lot of yeah cool things happeningã€‚

ğŸ˜Šï¼ŒAnd in that spaceï¼Œ especially in the last few yearsï¼Œ we had tremendous progress thereã€‚

Now what we have done and I'm basically just reiterating the last lecture is we have training data right and with this training data training a manifold and this manifold means ohã€‚

 give me random vector Z from this random vector Z draw samples and generate images out of it right and this is what the G manifold is giving us so this manifold is kind of spanning ideally it's representing a distribution of real world images and these images are taken from the training set of course they're not all kinds of real world images this is only the domain we did fit or we did put into our in our training setã€‚

Yeahï¼Œ what we haven't done so far is we haven't thought about what do any of the variables in Zã€‚

 how do they impact the image generation processï¼ŸWe saw a little bit in the progressive growing Gsã€‚

 right when you do this gain iniplationï¼Œ you go from one image to another imageã€‚

You get different images in betweenï¼Œ you get these transitionsã€‚

 but there's no semantic meaning yet behind itï¼Œ rightï¼Œ There's no like ohã€‚Yeahï¼Œ I wantnaã€‚

 I want to generate a person with blonde hairã€‚ I want to generate a person with longer hairã€‚

 I want to change the gender and stuff like thatï¼Œ rightï¼Œ These kind of thingsã€‚

They are probably somewhere in this manifold maybeã€‚

 but we never explicitly enforce them if they are thereã€‚

 they have been there because of the data distributions and they also never leveraged it so farã€‚

 so we didn't try to force itï¼Œ ohï¼Œ make this hair blond or somethingã€‚

But what we love to do is something like that actuallyï¼Œ rightï¼Œ we would love to go ahead and sayã€‚

 oh yeahï¼Œ we're gonna have manifoldï¼Œ we're gonna have latent codesï¼Œ we have ABCã€‚

 so we have we have images here instead set of images with sunglasses we haveã€‚ğŸ˜Šã€‚

Male images without sunglas and appear female imagesã€‚å—¯ã€‚With outside sunglassesã€‚ and ideallyã€‚

 we would like to do some sort of latent space vector arithmeticï¼Œ which is kind ofï¼Œ you knowã€‚

 we have kind ofï¼Œ ohï¼Œ we know there's some semantic meaning in the imageã€‚

 we just don't quite know how to express itã€‚ But if we saidï¼Œ ohã€‚

 we want to have this type of image with sunglassesã€‚å—¯ã€‚L which is a rightã€‚

 so we want to have sunglassesã€‚ We subtract male from itï¼Œ so we make it kind of neutralã€‚

 and then we add the female face to itã€‚ We would like to getã€‚ğŸ˜Šï¼ŒYou knowã€‚

 female faced with sunglass out of itã€‚That's kind of the high level ideaã€‚You can already seeã€‚

 well the result here looks Oã€‚I should say againï¼Œ this is a little bit of an older paperã€‚

 I'm not talking about high quality manifolds here right Againï¼Œ you could argue ohã€‚

 if we had a better manifoldï¼Œ we trained as long as we would probably get higher quality outputsã€‚

 but in factï¼Œ this manifold training is pretty difficult when we want to add these kind of arithmetic operatorsã€‚

 semantic operations and so on soã€‚ğŸ˜Šï¼ŒYeahï¼Œ that is unfortunately a veryï¼Œ very tricky problemã€‚

 but it's also super interesting rightï¼Œ if you're sayingï¼Œ ohã€‚

 we have kind of all this manipulation efforts like what Photoshop was doing by handï¼Œ rightã€‚

 we could kind of do by just sayingï¼Œ ohï¼Œ we take a little bit of that image a little bit of that image and then we get a photoistic outputã€‚

ğŸ˜Šï¼ŒAnd here we see yeah another example again manifold right here we see basically ohã€‚

 we can get images with and without sunglasses by that right so we can take these tricks to get some results hereã€‚

ğŸ˜Šï¼ŒYeah if you're going furtherï¼Œ I already mentioned this part in the progressive GNs when we do latent space interpolation right so what we can do is we have la code Z0 we have a latent code Z1 and we just linearly interpolate between that lant code and generate all the images in between that was what they have been doing when they interpolate between the faces togetherã€‚

And it's just the linearippolationï¼Œ so you just have Z0ã€‚

 Z1 right and you just do this linear manipulationippolation and you can generate all the intermediate images of outputã€‚

Now for semanticsï¼Œ this is kind of interesting already because if I have something attached to this one rightã€‚

 and if I have some labels attached to this oneï¼Œ I know how to interpolate between the labelsã€‚

If you're getting good intermediate resultsã€‚ğŸ˜Šï¼ŒThat will depend on the manifold right this depends how you trained it and so on so that's still yeahã€‚

 you can ignore it of courseã€‚That's what these guys have me doing hereï¼Œ they get somethingã€‚

 but you see already oh okayã€‚There's not too much control in it yet right Also againã€‚

 the quality here is still relatively lowã€‚ This was a paper from 2015ã€‚

 This was from the Redford table this very popular paper if you don't know itï¼Œ have a lookã€‚Andã€‚Yeahã€‚

 the quality is not so high again you could argue if you have like progressive growing and these kind of things you will get better resultsã€‚

 but for simplicity what's interesting to us right now is how can we deal with these la codes This is one thing you can do in diplomaationsã€‚

But still at this point we still don't have any conditioning right we generate stuff from a random codeã€‚

 but we' the generator doesn't know about anything we want to tell themã€‚

 the only thing we want to do is we want to say ohï¼Œ here's a late go produce an imageã€‚Nowã€‚

 conditional GNsï¼Œ in a senseï¼Œ what we would love to doï¼Œ instead of having a random codeã€‚

 we would like to take some conditionless inputã€‚So let's say if we want to take an input image of a cat hereã€‚

 there is a sketchï¼Œ I would love to use a generator that generates an output imageã€‚

And that isri and tells me heyï¼Œ this is a real image you're done greatã€‚

 So but what I would love to do is I would love to have some correlation between what I put in and what I generators output right So in other wordsã€‚

 in this case I would have to have to sketches of the hatã€‚ğŸ˜Šï¼ŒAnd I would like to get the catã€‚

 the real world image of the cat as an outputã€‚And yeahã€‚

 that's what the conditional gang should do for usã€‚

you could design an architecture like that which is relatively straightforward we could go ahead and say well we're feeding in this imageã€‚

 maybe have an encoder firstï¼Œ we get a latent space out of that one and then we have decoder which is our generator right that generates a real open image and then we have a discriminator that tells me is is a real image on it rightï¼Ÿ

So I can design a network like that and you can train the network like that in practice what you will get isã€‚

ğŸ˜Šï¼ŒYou will run into a simple problemã€‚And the simple problem is this discriminator hereã€‚

Just tells youï¼Œ is it a real cat or notã€‚It doesn't tell you any correlation to this one hereã€‚

 So if my generator now decidesï¼Œ I'm going generateã€‚A different output imageã€‚

My discriminator would still say it's a real imageï¼Œ which is greatã€‚Howeverï¼Œ the output image is realã€‚

 but it doesn't have anything to do with the inputã€‚ğŸ˜Šã€‚

So you can try this very simple case here in this case you will quickly realize that this input is pretty much being ignored it's just being used to create like a random initialization or a random vector Z but the semantics or the content here will most likely be ignored because the discriminator doesn't know about this input image right so that's a challenge by you can't just design a conditional ga like that you can't just go ahead and say ohã€‚

 we don't even take this as input here the generator and then generate an output because there's no guarantee that these two things have anything to do with each otherã€‚

ğŸ˜Šï¼ŒSo I'm going to start with one of the first verbs in that space andã€‚This is still very relevantã€‚

 even though it was trained on a very simple manifoldã€‚ğŸ˜Šï¼ŒAnd the idea isã€‚

What we would love to do is we would like to take a photoï¼Œ andã€‚We would love to figure outã€‚

A latent space Z by basically project this photo onto the manifold that we have learned with againã€‚

This is called an eye game paper interactiveactive GNs it wasn't this is still a very popular paperã€‚

 it was published in 2016 and this photo hereã€‚Can be projected onto the manifold by sayingï¼Œ ohã€‚

 what is the lant code Z the bestï¼Œ best expressed this imageã€‚

This is an operation by the way we'll talk about right now a little bit and in in though this was published in 2016ã€‚

 this is still super relevant todayï¼Œ this is what a lot of people do right now for manipulating latent spaces and so onã€‚

ğŸ˜Šï¼ŒBut the idea isï¼Œ wellï¼Œ if you could do thatï¼Œ I could go ahead and do some editing hereï¼Œ rightï¼Ÿ

And I can basically modify the shoe size in this case and stuff like that I can you know do latent space andation editing and then what I could do is I could basically figure outã€‚

 ohï¼Œ what did I change here in image space and I'm going to yeah repro this editor transfer the original imageã€‚

ğŸ˜Šï¼ŒWhat's I meanï¼Œ the reason why they do like this reproion here is because at this point they didn't have strong manifold here to produce photoalistic results I meanã€‚

 I want to talk about more about this step here This step is more interesting right now I mean sure you can back project the two and can use these basically you can figure out like some sort of analogy rightã€‚

 How was this image editedï¼Œ is just edited in the same way and you can formulate this as a non nonlinear optimization probe I'll mention it brieflyã€‚

 but that's not the main partï¼Œ I want to focus on this part hereã€‚

 I want to focus on how do we take an original photoã€‚

 how do we project it on the manifold and this is a very interesting operator because the question what we have now is we have a real image Xã€‚

ğŸ˜Šï¼ŒWe have a pretrained Gã€‚ So the manifold is fixed right nowã€‚ rightã€‚

 I'm not training the G G is again is G is given to meã€‚

 I have trained again from a random variable Z that produces a certain outputï¼Œ rightï¼Ÿ

 What you want to do is you want to go ahead and sayï¼Œ give me for this waterfall imageã€‚ğŸ˜Šã€‚

Give me the la code Zã€‚And the way you can do this is you can formulate as an optimization so you can sayã€‚

 wellã€‚Let's say we have a reconstruction loss L hereã€‚We have a generative model Gã€‚

 we have a random vector Zã€‚ This is our optimization variableã€‚ This is what were try to findã€‚

 and this is our real imageã€‚So I would like to figure out find a generated image that is close to my query imageã€‚

Find the respective or the optimal Z for that Againï¼Œ this is the generative model pretrainedã€‚

 I'm not trying the generative model here and then I have a reconstruction lossã€‚

 which is you know an L1 loss also an L2 lossï¼Œ whatever you want to use I'm not arguing this is the best loss hereã€‚

 but this is just something you can use for the false simplicity And again this is an optimization that tries to minimize tries to find a Z that minimizes the distance between the generated image and the query imageã€‚

ğŸ˜Šï¼ŒAnd this is what this one is doing here right so I'm feeling in this waterfall hereã€‚

 I'm optimizing this is a I don't know it goes new an optimizer or so whatever they used and you're going to get a la code Z and if you generated the image with Z you're going to get an image that looks like that and I think these ones are the distance metric so this is like 0ã€‚

196 away from the original imageã€‚ğŸ˜Šï¼ŒSame thingï¼Œ very imageã€‚

Generine target query image generate target and this is kind of cool right So now we have a pretrain network and what we can do is we can find with an optimization procedure we can try to find what is the closest query image what is the best optimal Z and project this image under the manifold rightã€‚

ğŸ˜Šï¼ŒYeahï¼Œ it turns out neural networks are quite highly nonlinearï¼Œ well this is a thingï¼Œ of courseã€‚

 by design choiceï¼Œ but it turns out this optimization here is actually very much nonlinearã€‚ğŸ˜Šï¼ŒAndã€‚

Whenever we have optimizations are' nonlinear and we don't know how to solve them properly because you could argue well these ones don't look so similarã€‚

 maybe we didn't find the right global optimum hereã€‚

 maybe we can find a better optimization procedureã€‚

 well we could think about making the optimization better or we could do the naive thing and say ohã€‚

 we don't use an optimization at all anymore we just train a neural network does the same job for usã€‚

ğŸ˜Šï¼ŒAnd this is the alternative wayã€‚In this caseï¼Œ what you want to do is you would like to train a neural network P and this network P takes the input image and predicts a lant vector Zã€‚

Rightã€‚å—¯ã€‚Wellï¼Œ how do you get thatï¼Ÿ Wellï¼Œ you in factã€‚

 can just generate the train data for free for you because originally when you train your manifoldã€‚

You you basically or when you have lant variable Z rightã€‚

 you know the ground truthth pairs because so this network is easy to train because all you're doing is you're feeding in random random variable Z generating some stuff as output and then what you're training is you're training the opposite network through inverting the network and sayingã€‚

 oh now for this Zï¼Œ what was the sorry for this Xï¼Œ what is the Z you fit inã€‚

And I can train an a if I have a pre traineded networkã€‚å—¯ã€‚A pret can networkã€‚

 I can generate this training pairs pretty much for freeï¼Œ right I just feed in random variables Zã€‚

 random random random randomï¼Œ generate imagesï¼Œ and then I'm training a network that takes the images and produces the respective theseã€‚

As outputï¼Œ rightï¼Œ so I have z is the ground truth for my giving input Xï¼Œ rightï¼Ÿå—¯ã€‚

And these two things do the same thingï¼Œ rightï¼ŸAnd in this caseã€‚

 what we're doing then is instead of using this optimizationã€‚ğŸ˜Šã€‚

What we're doing is we' using this network to do that for usã€‚

 so were feeding in the network here that produces the Z and then we' just generating the respective outputã€‚

 rightï¼ŸIn this caseï¼Œ we're getting resolved that look like these onesã€‚ğŸ˜Šï¼Œå“å‘€ã€‚

This one is it's a simple auto encodeder with a fixed decoder G right when you train it like the G is fixedã€‚

å—¯ã€‚That's the thingï¼Œ like you're training the whole thing then and that's very straightforwardã€‚

And now what you can do is you can just take the image X here againï¼Œ get a Z as outputã€‚

 and you're going to get some target here and you see already if you do thatã€‚

 you're getting kind of similarish resultsã€‚Sometimes looks a bit betterã€‚

 sometimes looks a little bit worse and you see it the distances here are also felt relatively relatively similarã€‚

 so we didn't necessarily get better results hereï¼Œ but we got an alternative way and if you're doing these two things you will see that the Z and the Z you would get here theyre actually quite different so even though they produce similar results you're still getting different Z'sã€‚

ğŸ˜Šï¼ŒSo what they have done then is saidï¼Œ wellï¼Œ you knowã€‚

 inverting the network gets you kind of into the right local minima and then you do a fine tuning optimization after itã€‚

And this is kind of this high hybrid methodã€‚ So you say use the network as an initialitizationã€‚

 use this one firstï¼Œ and then follow up by the optimizationã€‚ And if you do thatã€‚

 you get actually quite better resultsã€‚ rightï¼Œ You see now the error goes a little bit down from likeã€‚

0ã€‚196 to 0ã€‚153 hereï¼Œ and you see the images match it a little bit betterã€‚å—¯ã€‚

I'm not arguing right now like how to weight this and how to precisely train this network hereã€‚

 but the key things I feel that are worthwhile remembering here is there's two options we can go ahead and say give me an image in a fixed manifold that is pretrained I can optimize foruzz Z with a standard classical optimization approach I can also train a network that does the same thing by taking the data that I generated beforehandã€‚

And I can also combine these two things rightï¼Œ and this kind of gives you a bunch of tool setsã€‚

 how to go from one to the respective otherï¼Œ rightï¼ŸOkayã€‚

 and I think this this part I feel is like super cool because now we can go ahead and we can we can basically doã€‚

 yeahï¼Œ we can project things on the manifoldã€‚ And this is what this editing operator is doing nowã€‚

 And yeahï¼Œ I found this pretty cool because now we can go ahead and use sketches to find lant variables Z that merge to the sketches right soã€‚

ğŸ˜Šï¼ŒSo let's have a lookã€‚ We have here some sketches that we're going to do nowã€‚

 This is our guidance quote unquã€‚ğŸ˜Šï¼ŒAnd we then get let code Z from thatã€‚And that produces ourã€‚

And our respective imageã€‚The term here for the optimization is a little bit modified here in this case what we are saying is well we have a generator we have the user guidance hereã€‚

 this is the guidance the guidance vector this is our sketchã€‚

 we'll see this in a second what this meansã€‚ğŸ˜Šï¼ŒWe have here our generative model rightã€‚

 we have here some loss that compares these two images togetherï¼Œ whatever we generatedã€‚

 and whatever we compared hereã€‚Andã€‚This is our data terms so we want to make sure that whatever we're generating is close to what we have fedest input and now there's a manifold smoothness term saying that ohã€‚

 you initialize this whole thing with a C0ï¼Œ let's say this is our initial vector that we used and then we making some edits so we don't boly jump back and forth in the manifold space but we saying oh stay relatively close to the original image and just do some local edits there So we have a smooth traveral of the manifold right Yeah this is our constrained violation loss but now what we can do is we can have these guide strength right now we can have oh I'm going go ahead and do some drawingã€‚

ğŸ˜Šï¼ŒMore drawing hereï¼Œ rightï¼Œ more drawing hereï¼Œ more drawing hereï¼Œ more drawing hereã€‚

 more drawing hereã€‚ And basically this is what you're getting as an output hereã€‚

 So you're still producing shoes by producing shoesï¼Œ our sketches don't look like itã€‚ wellã€‚

 because ourã€‚Yeahï¼Œ our pre trained manifold is basically trained on shoesã€‚

 so whatever we're feeding in here is going to produce shoesã€‚Butã€‚

The manifold protection that we're doing with this optimization will now find like edits thatvo best reflect according to this loss function here what we're doing in terms of edits this loss function here I mentioned it you could take an L1 loss you could take an L2 loss you could also use feature losses like Lnet features and stuff like that there's a couple of variations I'm not arguing for one of the other but I think what's cool about it is that with these sketchs you can now get this protection to the late space Z and then you can do the editsã€‚

Yeahï¼Œ I think this is very fascinating and this is a thing thatã€‚Againã€‚

 this is a paper that has been published in 2016ï¼Œ but there's actually papers nowadays where they just use the exact same strategyã€‚

 but they may be using a bit of better manifold they trainedï¼Œ rightï¼Ÿ

So that's something I feel that is like super coolï¼Œ super interestingã€‚Okay yeahï¼Œ this paperã€‚

 this is the interesting thing I feel this paper also this other thing what I mentioned they go then oh they're learning these transitions hereã€‚

 so they're doing basically if I go quickly backï¼Œ they're doing this part of the user interaction right and then what they do is they project they're trying to form a nonlinear optimization because they sayã€‚

 oh well these G generated editsï¼Œ they're not looking perfect but what you can do is you can learn how these things editedã€‚

ğŸ˜Šï¼ŒAnd you can formula an optimization problem to protect these edits here on top of the original images now I'm not well I maybe explaining very briefly but the ideas is relatively easyã€‚

 So there' like this integral it's going to be disctizedï¼Œ of course we're going to sayã€‚Wellã€‚

 this is our input image that we want to manipulateï¼Œ this is what our again gives usã€‚

 this is our target in the latent Space Zï¼Œ these are our latent insã€‚

Could be an edit could be also linear nelation for simplicityã€‚

 this is a linear manipulationlation and now the idea is that how was this guy here edited to hereã€‚

 we just want to do the same edit to here right and againã€‚

Same edit from here to here right we want to do from here to here and so on and we're just gonna continue with through it and the way these these edits like these arrows here are being optimized with this term here So there's a color regularization term meaning that oh the color should behave similarly there's a spatial regularization term saying that this should be the distortion should be similar and then it's the data term that says ohã€‚

 the edits you're applying from here to here had an operator that should be applied in the same way from here toã€‚

Againï¼Œ this is maybe not the core of the learning part right nowã€‚

 but you can see how you can kind of clearly combine learning techniques on this end and then you would also be able to reproject them to the actual edits but again I want to say this in my opinion is still an artifact that the G results here are not so perfect at this point if we had perfect results here at this point we could go aheadã€‚

Didn't need this step because everything here would be already photoistic This is actually our main final goal as a community you not quite a right still like it's still trickyã€‚

 but that's our final goal at the end of the dayã€‚ğŸ˜Šï¼ŒBut nonethelessã€‚

 I think this paper was really cool and it's still really cool because they they did this manifold projectã€‚

 This is something I really liked about this paperã€‚ So they have these user edit hereï¼Œ rightã€‚

 They have these sketchesï¼Œ and then they can generate various images here as outputã€‚ Yeahã€‚

 and the nice thing is also thisï¼Œ this kind of stuff work interactivelyã€‚ğŸ˜Šã€‚



![](img/3f357201e6169fecca51f9c88788bc01_9.png)

And soï¼Œ you knowï¼Œ they had a live demoï¼Œ which kind of would coolã€‚



![](img/3f357201e6169fecca51f9c88788bc01_11.png)

Yeah here's another exampleï¼Œ here's again original inputsã€‚

 here's the reconstruction via optimizationï¼Œ this projection what I just mentionedã€‚

 this is via the network and this is the hybrid oneã€‚ğŸ˜Šï¼ŒThey look similar in a senseã€‚

But the hybrid one consistently give lower error that's why they used it at the end of the dayã€‚

Good yeahï¼Œ let's have a look at the interactive part againã€‚ This wasã€‚

 I think something I liked a lotã€‚ So what they did now is they have this user interface right they have some sketches here And here you see the intermediate result that the networks would do right and now you can go ahead and inter between the source and the target with with the optimizationã€‚

 This is what the network is doing and this what they see here is then the reproion with this optimizationã€‚

ğŸ˜Šã€‚

![](img/3f357201e6169fecca51f9c88788bc01_13.png)

So pretty cool was done 2016 quite a while ago in modern agesï¼Œ that's already an eternity butã€‚ğŸ˜Šã€‚

These these concepts still exist right so this idea of protecting back to the latent space is still superã€‚

 super relevant today and we will use it later on and I'll show you a couple of examples with things like progressive GNs and so onã€‚



![](img/3f357201e6169fecca51f9c88788bc01_15.png)

![](img/3f357201e6169fecca51f9c88788bc01_16.png)

![](img/3f357201e6169fecca51f9c88788bc01_17.png)

Goodï¼Œ I should mentionï¼Œ thoughã€‚Like mapping in lant space is very difficultã€‚

So what we have been doing right now is we ignored conveniently the problem of semantics andã€‚ğŸ˜Šã€‚

We basically went ahead and saidï¼Œ ohï¼Œ I have an imageã€‚ I first used that image to project the codeã€‚

 and then I used that for editing right So I'm kind of I circumvented this problem of ohã€‚

 this conditional G problemã€‚ I't I didn't train a conditional G yetã€‚

 right it's just I circumvented it by traversing the manifoldã€‚

And that the problem right now is still that in the latent spaceï¼Œ we still don't have any semanticsã€‚

 rightã€‚The pose for instanceï¼Œ the presence of the glass and stuff like that there was a pretty cool paper info againã€‚

 you should have a look at it they're kind of proposing how you can learn this in a very efficient way if you don't have labels for instanceã€‚

ğŸ˜Šï¼ŒYeah in most cases you don't have labels available that's a big problem so you need some sort of unsupervised disentanglement of the representation and the way you do this in practice is you just do some clustering that's what we always do already if you don't have data we do some clustering traditionally we've always done it with a PCA yeah nowadays you would use an order encoder or something like that and then say oh what are youia clusters in your latent spaces and so onã€‚

ğŸ˜Šï¼ŒRightBut yeahï¼Œ nonethelessï¼Œ mapping semantics to the latent space is veryã€‚

 very tricky and yeah still an unsolved problemï¼Œ of courseã€‚Okayã€‚

 but I'm going to quickly discuss that part a little bitã€‚ the labels labels is a big problemã€‚

 of courseï¼Œ and there's no difference in the conditional case becauseã€‚ğŸ˜Šã€‚

We have kind of these two settings nowï¼Œ we have settings where we do have paired data and we do have unpaired dataã€‚

And this is this terminology of paired versus unpaired is something you might want to remember in the paired settingã€‚

 we could go ahead and train like a network that maps me from this cat to that catã€‚

We have the sketches and the cat imageã€‚ this is a pairã€‚Rightã€‚

 and we have all kind of pairs here between the sketches and the imagesã€‚

Theres also an unpaired versionï¼Œ but we don't have that if you have horses in zebrasã€‚

 this is an unpaired settingã€‚Andã€‚Most of the time we're in the setting actually but often we can make shortcuts and kind of fall back to the paired setting so if we have paired data a lot of things get easier but often we don't have it in the following I would like to take yeah I would like to present two of the methods that people have been doing for either cases so I wanted to look at the paired setting first a big question is how do we get to the paired settingã€‚

A parent setting means yeahï¼Œ I knowï¼Œ like how do Iï¼Œ how do Iï¼Œ how do I know when I have a sketchã€‚

 how do I get to the catï¼ŸRightDo I do I have to draw a sketch and then go to the internet and search the real ver image Yeah no of course not you would do it the other way around right I'm gonna go ahead and start the catã€‚

 start the filter that gives me the edges right That's how you do it and paired settings for the most time doesn't mean that we annotated them manually paired settings for the most time means that we can in a selfsvised way in further labelsã€‚

å—¯ã€‚Most timeï¼Œ not alwaysã€‚ I wanted to show a couple of examplesã€‚ And this is a paperã€‚

 This is this ps to Ps paperï¼Œ image to image translationã€‚ This is a paper thatã€‚

Precisely focuses on the parent settingï¼Œ rightï¼Œ they want to consider specifically these casesã€‚Andã€‚

Yeahï¼Œ they have a couple of examples actually they have quite a comprehensive set of samples here and what they do is they have labels they have image generation measures that go from labels to scenesã€‚

NowIn this caseï¼Œ theyã€‚Take semantic labels on cityscapes there's a data right and they're turning a network to turn this one into that one in this case they have actual annotations so there's people that have annotated cityscapes they have semantic labels for it and yeah there's basically a one to one mapping between those twoã€‚

ğŸ˜Šï¼ŒAnd there's other things like aerial mappsã€‚ This is from Google Maps here you're gonna to have the satellite photos versus the you know the abstractions from the maps visualizer here you have it implicitly given because at some point probably Google Maps also has a method that goes from a satellite image and produces these ones So for that oneã€‚

 we have a lot of free data available right this is also freeã€‚ğŸ˜Šï¼Œå—¯ã€‚Da night imagesã€‚ in this caseã€‚

 you have to make sure you have the cameras are being alignedã€‚ So the images taking the same stuffã€‚

 And then you have one to one maps and you just take the images a different timeã€‚

 Black and white is grayscale images rightï¼Œ and color image is super easyã€‚

 I'm just gonna take a color imageã€‚ I'm gonna convert it to gray scaleã€‚ I have these pairsã€‚

 And then I can train a network that goes from gray scale to the other sideã€‚

 sketches we mentioned rightï¼Œ you take typically that one has inputã€‚

 you convert it to a sketch this way perã€‚ the other way runs a bit harderã€‚ğŸ˜Šï¼ŒOkayã€‚

 but let's have a look at this one hereã€‚Yeahï¼Œ I wanted to go back to this caseã€‚

 what we discussed beforeï¼Œ how would we train these conditional networks hereï¼Ÿ

And if you're looking at the traditional formulation but Godfe rightã€‚

 we have in here what we're trying to doï¼Œ we have a discriminatorã€‚

 we have a generator trying to maximize a discriminator and minimize the generator right so these two they fight each otherã€‚

ğŸ˜Šï¼ŒWe have a binary cross entropy loss right again we want to make sure our generator here produces stuff that he thinks is speeding the discriminator and the discriminator wants to make sure he' is always right right that these are the two things that come fight each other we've done this for random variable Z we've fed this into a generatorã€‚

 got an image out of itï¼Œ have a discriminator discrimin tells us it real isn not realã€‚Againã€‚

 this one gives us a manifold and we've discussed like manifold traersal and stuff like thisã€‚

 but the problem what we had isï¼Œ wellï¼Œ when we are feeding in hereã€‚ğŸ˜Šï¼ŒOkayã€‚

 so same formulation here when we're feeding in hereã€‚

 this sketch of a bag have a generator that takes this maybe through an encoding as inputã€‚

Generates a real output hereï¼Œ discriminator says it's real effect say it's real we're going to have this issue that no matter what we generate here each of these ones will be true right and the reason is the discriminator just checks just whatever we're doing here as an output is that part of our distribution rightã€‚

ğŸ˜Šï¼ŒOf the real distributionã€‚ But the discrimator doesn't care about whatever you're feeding in hereã€‚

 So in other wordsï¼Œ if you're training thisã€‚The generator eventually will maybe use that for random seedã€‚

 but not it will not cover the ets or semantics or whatsoeverï¼Œ rightã€‚

 Well why would you wouldn't want to do thatï¼ŸOkayï¼Œ the core idea is that the core problem here is that this discrimator doesn't know about this guyã€‚

This is something we have to fix and the easiest way to fix itã€‚

 we just make the discriminator aware of itã€‚ğŸ˜Šï¼ŒAnd that's what we're gonna do nowã€‚

 We're gonna take this inputï¼Œ and we have two imagesã€‚ nowã€‚

 One is what we generate here with our generator and what we haveã€‚What we have fed as inputã€‚

And now we have the discriminator takes the pair here' inputï¼Œ this is why it's a pair setting by wayã€‚

 this is where the term comes fromã€‚ Now we have a pair between the sketch and the generated outputã€‚ğŸ˜Šã€‚

Andã€‚The discriminator needs to decideï¼Œ is this pairï¼ŸGood together or notã€‚

 it's not just is that a real imageï¼Œ but it's alsoã€‚

 is it a real pair that would exist in the training setï¼ŸIn this caseã€‚

 well this is what our fake pair right this is our fake pairã€‚

 our discriminator takes now x is the imageï¼Œ sorry x is the conditional imageã€‚

 x is the conditioning and G what we generate in this pairã€‚

 both of them is fed into the discriminatorã€‚ğŸ˜Šï¼Œå—¯ã€‚Yeahã€‚

 this is the fake pair and of course we have a real pair here tooã€‚

 and this is why we need ground truth hereã€‚ This is why we need the paired ground truth dataã€‚

 what we've previously discussed right The discriminator now needs to get both X and Y S inputã€‚ğŸ˜Šã€‚

Drimator checks is this real of courseï¼Œ but it also checksï¼Œ is it belonging to that sketch hereã€‚

 these two things need to work because our generator doesn't do anything hereã€‚

 this one will be of course true this is always a real oneã€‚

 but this case we want to make sure that we're generating something that matches to this one hereã€‚

Yeah and now what our G is learning this is now a true conditional again because it's conditioned on x and it wants to generate something that matches to x right in this case we want to match the joint distribution right we want to make sure that the distribution of real pairs matches the pairs what we are just generatingã€‚

Okayï¼Œ yeahï¼Œ that's the the whole ideaã€‚ basicallyã€‚ But very importantlyã€‚

 we need to make sure that the discriminator is aware ofã€‚Of these pairsï¼Œ rightã€‚

 that the conditioning needs to be fed into the discriminatorï¼Œ otherwiseã€‚

Theres no way of making sure that our pairs whatll be generating are actually real person or good personã€‚

And this is what this pixel top paper is doing and I think again this is a very important paper because they introduced this concept of conditioning the discriminator with it rightã€‚

 and what we're doing here is again we have here X is our conditioningã€‚

 we have a generator practically generates the encode a decoder architectureã€‚

 takes this image here as inputï¼Œ goes to latest spaceï¼Œ goes back to decoderã€‚

 generates something as outputã€‚This one is just being forwardedã€‚

 Discriminator takes these two hair input and has a binary cross entropy is it real always This one is typically an encode architecture of course right and it's just a classifierã€‚

Againï¼Œ we have to make sure it's scminated and generally roughly the same sizeã€‚

 but in this case we compared against a real world pairã€‚

 so we have here Y and X are being compared to each otherã€‚è¿™ã€‚

Yeah so I think this is really cool and this is of course now the first time where we're training a manifold that is conditioned on pairs in this case we have this paired setting right so what we need we need these pairs in the ground truth data this is what we need for training we need these pairs for images to edges yeah there's this data set by she2ã€‚

ğŸ˜Šï¼ŒBasically you have these pairs that's what they trained it on practically what you do is you would start here from the image generate the edges that when you can probably do algorithmically or you hire a bunch of artists that would do like sketching for that both as an option ideally you want to do selfsvis you don't want to hand annotate it rightã€‚

Okayï¼Œ this one goes from edges to imagesã€‚å—¯ã€‚Yeahï¼Œ the pair setting is kind of a very popular oneã€‚

 It's great when we have free data I like to call it self supervised in practice the paired setting is always self supervised right you really want to want to annotate like a specific setting but yeahã€‚

 think about these settings I think this is cool I think this one we've just shown you right the sketches to imagesã€‚

ğŸ˜Šï¼ŒThat's an easy oneï¼Œ there's a couple of data sets like theseã€‚å—¯ã€‚Yeahã€‚

 anyway there's many of these ones and the selfsupvised case I mentioned like gray scaled like and white and stuff like this is interestingã€‚

 there's a cool there's a cool demo actually you should go to this website a find layer do co Ps S are we and here you can basically go ahead and do your sketches run Ps to picks that just wrap the Ps to picks model and then you get exam outputs right here you have a catã€‚

 the model is random cats on cat pairsï¼Œ you're gonna get this as outputã€‚ğŸ˜Šã€‚

You can try pretty cool thingsã€‚And looks like that you can try yeahï¼Œ to have arbitrary shapes hereã€‚

 your p to pixel model will try to fit these arbitrary shapes in hereã€‚ğŸ˜Šï¼ŒI think it's coolã€‚

 I think you can play around with itã€‚ This is an interactive demo you can just go to the website and try it out yourselfã€‚

ğŸ˜Šï¼ŒYeah looks like that right you first draw the cat you get this as an output you can get pretty funky shapes if you're doing various things I have tried the two in the previous lectures I did this live my drawing skills are pretty bad but I think you should try it out it's kind of fun to do itã€‚

ğŸ˜Šï¼ŒOkayï¼Œ this is fors to ets to cats right this was a model that was trained specifically in cats there's models sell civilvised data from Google I mentioned that you can take Google Mapsã€‚

 you have the satellite imagesï¼Œ you can train models that go from here to here Yeah interestingly enoughã€‚

ğŸ˜Šï¼ŒThis is the ground truthã€‚One thing you will see very quickly is in these casesã€‚

How do you expect this is an undercontrain problemï¼Œ I think in practiceï¼Œ rightã€‚

 Like you don't really knowã€‚Yeahï¼Œ I don't knowã€‚ You don't necessarily know like what are theã€‚

 what are theï¼Œ what is the ground truth thereï¼Œ but probably multiple multiple solutions that would give you a good image for that input hereã€‚

 rightã€‚å—¯ã€‚Looks plausibleã€‚With you swimming inï¼Œ you will see there a lot of artifactsã€‚

 but on the high levelï¼Œ it looks pretty plaã€‚ğŸ˜Šï¼ŒOkayã€‚

 you have the grayscale stuff that when I mentioned input here trend the outputã€‚

 you get actually kind of good results but again grayscale is also under constrainã€‚

 you don't know for sure how they really look likeã€‚ğŸ˜Šï¼Œå—¯ã€‚But yeah I think it looksï¼Œ pretty coolã€‚

 There's a few ideas behind Ps to pixels I wanted to mentionã€‚ğŸ˜Šã€‚

And it's not just this conditional again that we've just said in practiceã€‚

 what they do is you're training also L1 lossï¼Œ rightï¼ŸYou have a lambda parameter that saysã€‚Yeahã€‚

 go from this imageï¼Œ go to this targetï¼Œ this would be an El1 doseã€‚

 typically you have some weight on this one and this weight is actually still a dominant weightã€‚

What you can do is you can actually get rid of the generator at the discriminator lossã€‚

 just have the generator with L1ã€‚And then successively make this weight hereã€‚

 sorry successively reduce this weight hereï¼Œ rightã€‚

 this is one thing that makes ps to picks more constrained and probably that's why it works relatively wellã€‚

å—¯ã€‚They have a unit architectureï¼Œ they have skip connections to preserve the structure in the generatorã€‚

 makes sense for some casesï¼Œ for other cases you wouldn't want to do itã€‚

 but for these cases we've just seen here it make a lot of senseï¼Œ like for grayscale imagesã€‚

 totally makes sense you want to preserve the structure makes senseã€‚

There's a few interesting things about picks to picksã€‚ğŸ˜Šï¼ŒThere's no latent variable Z in this caseã€‚

 they use dropout for noiseï¼Œ so you just have a dropout variableã€‚

 you run the network multiple times to get different resultsã€‚One problemã€‚

 what always happens if you train ZGNsï¼Œ if you're feeling in a vector Zã€‚

 it tends to ignore this random variableï¼Œ that's what I said already beforeã€‚

 if you don't have a paired setting and you're just feeling in the sketch and have a generatorã€‚

 you will just ignore the inputï¼Œ it's just not going to consider itã€‚Yeahã€‚

 the noise is pretty important here because you still want to have a probabilistic model for many casesã€‚

 not for all casesã€‚ It depends a little bit how constrained your problem isã€‚ if youã€‚

 if you having an overconstrained problemï¼Œ meaning that theres there's a specific solution you would expect for that for that inputã€‚

 But you have things like sketchesã€‚Wellï¼Œ or this Google Mapss imagesï¼Œ rightã€‚

 like there's many solutions that are probably okayã€‚ So in this caseã€‚

 you always want to have a probabilistic modelã€‚ Otherwiseã€‚

 your network will just take the average right If you just take the average then hereã€‚

 but it doesn't look so greatï¼Œ soã€‚Typicallyï¼Œ you want to have a probabilistic model hereã€‚Yeahã€‚

 Ps to picks had a few other ideasã€‚One of them is that the L1 or L2 lawsï¼Œ in this caseã€‚

 the use in L1 loss is good for the low frequency detailsã€‚Andã€‚

Then the idea is that the again kind of locally on a per patch level figures out the high frequency detailsã€‚

 So what they did is also they introduce a patch scan versionã€‚

 So this model that I' just shown you is actually fully convolutional right so both discriminate and generator are fully convolutional The reason why this works is because you don't have a latent vector right so you can whatever input you're feeling in it is the same size as the output So it's fully convolutional only operators but the generatorã€‚

Produces basically patchesï¼Œ sorryã€‚The discriminator is basically only being applied to certain patchesã€‚

 you have a patch sizeï¼Œ like how much the generator discriminator actually seesã€‚ğŸ˜Šï¼Œå—¯ã€‚

It won't help you to generate global structureã€‚ But since it's a condition problemã€‚

InThe structure you can simply preserve with an El1 or L2 lossã€‚En with the skip connectionsã€‚

Because of thatï¼Œ this patch game makes a lot of senseã€‚

And it's a lot easier to train than generating a higher resolution output from scratchã€‚

 you can take a higher solution inputï¼Œ you have the patches and a smaller resolution and again tells you is this a reasonable patch with reasonable amount of detail and stuffï¼Ÿ

Yeahã€‚Yeahï¼Œ I should sayã€‚Whenever you're doing a thesis or like guide research or stuff like this or projectã€‚

 I would always recommend start with this Pix to Pix code for generative modelsã€‚

 This is a really clean code baseã€‚ These guys spend a lot of effort in making it actually usableã€‚

 This is a veryï¼Œ very popular Github pageï¼Œ I would recommend I always recommend people to start with thatã€‚

 this is always a very good baselineï¼Œ even if though results don't look perfectã€‚

 but it's relatively easy to useã€‚ğŸ˜Šï¼ŒYeahï¼Œ there's been a lot of work already on as a follow up one work is Ps to Ps HD now they're sayingã€‚

 oh yeahï¼Œ we're trying to do similar ideas like multi resolution and stuff like thatã€‚

 a multi scale on top of Ps to picksï¼Œ the idea now is you have of course defined generatorã€‚

Don't just have a single patchï¼Œ but go ahead and say go course first and then go into more detailsã€‚

 same thing for the discriminator The idea is that the generators and discriminators are the sameã€‚

But since they operate in different resolutionsï¼Œ they get different receptive fieldsã€‚Yeahã€‚

 in practiceï¼Œ it looks something like thatã€‚ So you have hereã€‚

Yeah F generator goes from semantic labelsï¼Œ rightï¼ŸProducesã€‚Some feature mapsã€‚At the same timeã€‚

You can downsble the import againï¼Œ have a bunch of resonnet blocksã€‚Produces some feature mapsã€‚

 Now you have the decoder that goes to the resulting image at the lower solutionã€‚å—¯ã€‚

These features at the lower resolution are now concateninated to the features of the higher resolutionã€‚

 You have a bunch of residual blocks hereï¼Œ and you go up to the original image right So you have kind of a network in a network hereã€‚

 like in between the networkï¼Œ we have we have another residual blockã€‚

 It's very similar to progressive growingï¼Œ actuallyï¼Œ it's a very similar ideaã€‚

 except it's a conditional network now and you have now you have conditions at this level and you have conditions at this levelã€‚

 rightã€‚ğŸ˜Šï¼ŒSoï¼Œ that is also helpingã€‚To make it easier in this caseã€‚

 Ps to Ps HDD is using multi scale discriminators in this case they're using three I think this is pretty arbitrary in a sense I think they just did it because that's what they fit best in their GPã€‚

ğŸ˜Šï¼ŒSoã€‚Yeahï¼Œ theres basicallyã€‚Various combinations of stackingï¼Œ discriminated and generatorã€‚

 I'm not arguing this is the best versionï¼Œ but these guys spend a reasonable effort in getting good resultsã€‚

 rightï¼Œ soã€‚So there has a lot of optimizations you can still doã€‚

 but it's still similar to this progressive G idea right so you say you have low resolution and then upsemble and go to high resolution There is an interesting question thoughã€‚

 is basically you' quickly going backã€‚ğŸ˜Šï¼ŒYou haveã€‚A very interesting thing that a lot of people often don't mentionã€‚

 So in a senseï¼Œ what you do is right you're designing your architecture subject that fits more or less on your GPUã€‚

ğŸ˜Šï¼ŒSo now you're having two optionsï¼Œ one option is I'm going to say I have a single hierarchy level and I'm going to put all my weights into the single hierarchy levelã€‚

The second option isï¼Œ I have multiple hierarchy levelsï¼Œ but each hierarchy level will haveã€‚ğŸ˜Šã€‚

A smaller number of weightsï¼Œ because no matter what I have fixed budget of weights or of capacity that I can fit on my GP Pã€‚

 the argument is now that by having this multi resolutionï¼Œ a multi scale hierarchyã€‚

 it's more efficient to use the weightsã€‚ That's eventually going to be the high level argument we have to makeã€‚

And this is what they are showing and it seems pretty reasonable for their setting againã€‚

 they're using three levels they have basically three discriminatorsã€‚



![](img/3f357201e6169fecca51f9c88788bc01_19.png)

And re generators thereï¼Œ rightï¼ŸOkayï¼Œ take a results that look like thisã€‚

This is a synthesized image already from againï¼Œ this is cityscapes imageã€‚ Yeahã€‚

 looks looks pretty decentï¼Œ rightï¼Œ I meanï¼Œ look looks not too badã€‚From a detailã€‚

 you can't zoom in hereï¼Œ but I think the number of signs are still pretty blurryã€‚

For the better for the firstï¼Œ but you can see thats it's not quite perfectã€‚



![](img/3f357201e6169fecca51f9c88788bc01_21.png)

They have done it on facesï¼Œ so here use a face mask as input rightã€‚

 and then they're synthesizing different results for the faces againã€‚

 it doesn't look perfect like the teeth here or so and not perfectã€‚I looks looks prettyã€‚

Here's another exampleã€‚ this's an interactive one so they running multiple times for the same image right so they're gettingã€‚

Different results hereã€‚ You can look at the number platesã€‚ It looks kind of fineã€‚ Tex is super hardã€‚

 rightï¼Œ I meanï¼Œ painted like a lot of small scale detail of of these signs hereï¼Œ rightã€‚

 they also it's not perfectï¼Œ Not perfectï¼Œ but it's getting thereï¼Œ rightï¼Œ Just give itã€‚

 give it a few more years to be getting betterã€‚ğŸ˜Šï¼ŒYeahï¼Œ anywayï¼Œ this is one of the latest papersã€‚

 There's there's a few more papersï¼Œ of courseï¼Œ but this is one of them like this pixels to pixels lineã€‚

 I think is prettyï¼Œ pretty popularã€‚ and I I can highly recommend looking at that oneã€‚ğŸ˜Šï¼ŒGoodï¼Œ so farã€‚

 we have looked at the paired settingã€‚And I already mentioned we need to get the parentsï¼Œ rightã€‚

 we need labels and so onã€‚And it is expensive to collect the pairsã€‚ Howeverã€‚

 there's also scenarios where it's virtually impossible to collect pairsã€‚

 So let's say we want to convert our horses intozebrasã€‚Rightï¼Œ how does our horse look as zebraï¼Œ Iã€‚

 this is very difficultã€‚ Do you want to go ahead and catch a bunch of horses and paint them in zebra stripesã€‚

Might be difficult to get enough training data like that rightï¼Œ so that's a very difficult problemã€‚ğŸ˜Šã€‚

So this leads us to the unpared itã€‚Again picks to picks and these kind of thingsã€‚

 conditional Gs for paired settingsï¼Œ but now do we do conditional GNs in the unpaired settingï¼Ÿrightã€‚

So and that is actually a very common caseï¼Œ but we'll see this laterã€‚

 there's a couple of interesting scenarios where this happensã€‚ğŸ˜Šã€‚



![](img/3f357201e6169fecca51f9c88788bc01_23.png)

A couple of timesã€‚ So in the unpaired settingã€‚Youï¼Œ stuff like thatï¼Œ rightã€‚

 You have a bunch of images of of horsesã€‚ You have a bunch of images of zebrasã€‚

 But between those twoï¼Œ there is noï¼Œ theres no direct correlationï¼Œ rightã€‚

 I don't know how this horse would look as a zebraã€‚

 I also don't know how this zebra would look as a horseã€‚ Againï¼Œ I can't paint them so easilyã€‚

 I can't go ahead and paint this guy the zebraã€‚So I have a source domain Xã€‚

 and I have a target domain Yã€‚I know which one is whichã€‚

 but I don't have one to one maps and this is the unpaired settingã€‚Soã€‚

How could we formulate a conditional G in the unpaired settingï¼ŸWellã€‚

 what you can do is let let's do the same thing startna E right let's say we start with an image from the domain X rightã€‚

 this is a horse we feeding into a generatorã€‚ğŸ˜Šï¼Œå—¯ã€‚And then we have all discrimininated right our discriminator takes now this guy hereã€‚

 it wants to generate this one yearï¼Œ but it doesn't have the input output prepared rightã€‚

 And this one should say real and not realã€‚So what you could do is you could simply go ahead and sayã€‚

 ohï¼Œ let's ignore this part at all again and let's say our generator generates something that sorry our discriminatorator says is this from the Cbra domain I'm just going to go take horse feeding the generatorã€‚

Produce anything from the target domain zebraã€‚And the discriminator saysï¼Œ is it fromzebraã€‚

So it's realï¼Œ we're going to have the same problem what I discussed before in the parent setting already like this in this caseã€‚

It's both realï¼Œ this screen and will say greatï¼Œ both look likey bra usï¼Œ you're all good to goã€‚

So the again does not force any correlation here between those twoã€‚

 same thing what we've discussed with the catsï¼Œ same thing for horses and zebrasï¼Œ rightï¼Ÿ

Now the question is how do we figure out there's some correlation between theseã€‚

 That is the big questionã€‚And wellã€‚One ideaã€‚ What you could do is you could sayï¼Œ wellã€‚

 I have a bunch of horses hereã€‚And I train this network that I just describedã€‚

The most likely output you would actually get is you would get some odd collapse because everything would map to the same target and everything here would be part of our real domain in the targetã€‚

Nowï¼Œ how do weï¼Œ if we had a way to figure outã€‚To force that this guy hereã€‚

If we had a way to figure out to avoid more collapseã€‚In other wordsã€‚

 if we had a way to say this imageï¼Œ this image and this image should all map to a zebra imageã€‚

 but these images must be differentã€‚So in other wordsï¼Œ we want to have a byche mappingã€‚

 we want to have a mapping that goes from here to one imageï¼Œ this one goes from one image to hereã€‚

 this one goes to one image to hereï¼Œ and precisely only one of them goes hereã€‚

And the ideas of a pie checkive mapping that we have a month to one map from here to here and from here to hereã€‚

 So if we had a piecheive mappingï¼Œ we could force avoid the mod collapseã€‚

And the idea to do that is a cycle consistency approachã€‚So the idea is we have images from Xã€‚

 we have images from yã€‚The idea is that if I map from x to Yã€‚

 I have a discriminator who tells me heyï¼Œ I'm from Yã€‚

And the idea now is that be taking that same image and mappingping it backã€‚

And this is the original image againã€‚And this is actually a thing that people have been doing for language creditã€‚

 but there's actually this famous approach for Mark Twain rightï¼Œ where you're sayingã€‚ğŸ˜Šã€‚

If youre translating from language A to language Bã€‚You shouldï¼Œ and then from B back to Aã€‚

 you should get the same resultã€‚Rightï¼Œ and this is literally enforcing a byche if mapping if you're doing thatã€‚

Rightï¼Œ againï¼Œ if you're translating fromï¼Œ in this caseã€‚

 English to French and then from French back to Englishã€‚

 you hope that you get the same sentence as outputã€‚ If not something went wrongã€‚

 Something got lost in the translationã€‚ Sam thing for the imageã€‚ This is image translationã€‚

 but unfairpairã€‚And the way you're doing thisã€‚ğŸ˜Šï¼ŒIt is the cycleconency loopã€‚

 So what we do is we have againï¼Œ we have domain x hereï¼Œ horses hereã€‚

 We have a generator that maps this guy to zebraã€‚ So this generator takes x this image here's input and produces Y hatã€‚

Gener takes X input produces by headï¼Œ discriminator here tells us I'm a zebra or not a zebraã€‚Okayã€‚

 so x match to yã€‚ And then what we're doing is we're mapping this one backã€‚

 Now we have another generatorï¼Œ Fï¼Œ I againï¼Œ note the F and Gã€‚At both generatorsã€‚

This man maps my head back to exitã€‚And then all we're doing is with checking the same input delete the same result of the fittedã€‚

Please case you can just use an L one more right you literally want to have the same image againã€‚

 so again feeding x inï¼Œ mapping the domain yï¼Œ mapping back to domain X should give you the same imageã€‚

ğŸ˜Šï¼ŒThat's the core idea of the cycle consistencyã€‚ And it's forcing a by checkive weaponã€‚å—¯ã€‚

And you don't know necessarily what why wasï¼Œ but the idea isã€‚

 if you're doing this over a large distributionã€‚It forces that note the same image X maps to the same image in yã€‚

In other wordsï¼Œ if you're doing this one here againã€‚

 so this is what I mentioned with the mod collapseã€‚So let's say here we have horses hereã€‚

They could all map to the zebrasã€‚Rightï¼Œ andã€‚If all of them map to the zebras to the same zebra image hereã€‚

 right if all of them map to the same thingã€‚I would go back with this F generator and I would get the same image here as outputã€‚

Now my cycle loss will tell meï¼Œ heyï¼Œ this one looks greatï¼Œ and this image is the sameã€‚

But my psychoos will tell me this to this and this to thatã€‚They don't look very goodï¼Œ rightï¼Ÿ

 So we have a bit of a problem hereã€‚ This is a high lossã€‚

 And that's good for us because what we want to do is we want to split these apartã€‚ We want to sayã€‚

 ohï¼Œ they cannot map to the same image because if they do thatï¼Œ these losses here will be very highã€‚

 And this is good for usã€‚ Sos consistency makes sure we have a true project mappingã€‚

 or at least we trying to have a true project mappingã€‚ğŸ˜Šï¼ŒOkayã€‚

Yeah we're doing it the other way around two of courseï¼Œ we're doing it in both directionsã€‚

 so now if we have a zebra and we're mapping it to a horse with our F generator F goes from zebras to horsesã€‚

 were getting X hat hereï¼Œ we have a discriminator that tells usï¼Œ ohï¼Œ it's a horseã€‚ğŸ˜Šã€‚

And then we're mapping it back with Gã€‚And she gives us by hat and this is a zebra again right again note that this generator and this generator here is both the sameã€‚

 these two are trained at the same timeã€‚And when we're mapping it back hereã€‚

 we comparing again is that image again the same as this image right this one here and this one these should matchã€‚

 this is our cycleã€‚So in this caseï¼Œ againï¼Œ we do a simple reconstruction error saying if we're feeding thezebra of y inã€‚

Applying Fã€‚Now we have a horse and now we're applying Chi againï¼Œ now we should get a zebra againã€‚

 we should get the very same image againã€‚What we fed in here in Neã€‚

And these two losses we train at the chain at the same timeï¼Œ right this is two cyclesã€‚

 we go both ways and note again this F and this F is the sameï¼Œ this G and this G is the same rightï¼Ÿ

And the lower case Xï¼Œ lower case Yï¼Œ other respective training samplesï¼Œ rightï¼Œ D Gsã€‚

 the discriminator tells meï¼Œ ohï¼Œ it's a verse and Dy tells meï¼Œ ohï¼Œ it's azebraã€‚Okayï¼Œ yeahã€‚

 that's pretty much itã€‚ And now were train all of them together we can already see how many losses do we have rightã€‚

 we have one cycle loss in this directionï¼Œ one cycle loss in that direction and we have the two discriminatorsã€‚

 that's basically what we haveã€‚ğŸ˜Šã€‚

![](img/3f357201e6169fecca51f9c88788bc01_25.png)

Okayï¼Œ wellï¼Œ now we have generated that goes from A to Bã€‚This one hereï¼Œ and we have generatorã€‚Okayã€‚

 there's another A2 Bã€‚ There's two A to Bs hereã€‚ I'm not sure why the slide has two A to Bs anywayã€‚

 but if you're doing thisï¼Œ we're getting results that look like these onesã€‚ a little bit funnyã€‚

 this looks funny right nowï¼Œ we have this horse that is a zebraã€‚ This horse is also zebraã€‚

 And this is kind of the results you're gettingã€‚ğŸ˜Šï¼ŒYou can do this with horses and zebrasã€‚

 you can do this with painting stylesï¼Œ here are Monet paintingsã€‚They can be mapped to real photosã€‚



![](img/3f357201e6169fecca51f9c88788bc01_27.png)

This is a real photo now from Monet pendingï¼Œ we can do another monet painting to another real photo looks like thatã€‚

We can doã€‚Photos here as inputï¼Œ we can changeï¼Œ I think the seasonsï¼Œ I thinkï¼Œ rightï¼ŸYoure going hereã€‚

 what you meanã€‚

![](img/3f357201e6169fecca51f9c88788bc01_29.png)

Seasonsï¼Œ we can go here the other way aroundã€‚This one actually don't remember what they trend it onã€‚

 but you get the idea rightï¼Œ you basically have orangesã€‚

 you want to make apples out of them or you have oranges or you want to make apples out of itã€‚

That's the corner dealã€‚å—¯ã€‚Yeahï¼Œ I think that was mainly one I wanted to go through in this lectureã€‚

 we have talked a lot right now aboutã€‚Conditional GNsï¼Œ we have talked about architectures for GNsã€‚

I think this is really interesting because now we have actually control over again and we can do something with themã€‚

 rightï¼Œ We can turn horses into zebraã€‚ I think that's amazingã€‚ğŸ˜Šï¼ŒAndã€‚

There are still a few things that we haven't discussed yetã€‚ generatively speakingã€‚

 we can do stuff on imagesã€‚ One thing I would like to talk aboutã€‚

 especially in the next lecture is videos right how do we go and make no videos out of it This is also something that my group does quite a bit and we do a lot of stuff in normal rendering and 3D deep learning So these are the things we're gonna cover in the next lectures So yeah I hope you still have fun working on the projects and please take the opportunity to spend your effort with itã€‚

ğŸ˜Šï¼ŒYeahï¼Œ let us know if there's any questions and so onã€‚Yeahï¼Œ otherwiseï¼Œ stay safeï¼Œ stay healthyã€‚

 See you for the next lectureã€‚ Thanks a lotã€‚ğŸ˜Šã€‚

![](img/3f357201e6169fecca51f9c88788bc01_31.png)