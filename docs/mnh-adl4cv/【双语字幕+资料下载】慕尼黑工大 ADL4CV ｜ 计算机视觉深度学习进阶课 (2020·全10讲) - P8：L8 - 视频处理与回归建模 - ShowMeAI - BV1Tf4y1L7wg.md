# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ…•å°¼é»‘å·¥å¤§ ADL4CV ï½œ è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ è¿›é˜¶è¯¾ (2020Â·å…¨10è®²) - P8ï¼šL8 - è§†é¢‘å¤„ç†ä¸å›å½’å»ºæ¨¡ - ShowMeAI - BV1Tf4y1L7wg

Helloï¼Œ welcome everyone to the Advanced team Learn Liatureã€‚

In the previous lecture we have talked now a lot about generative modelã€‚

 we also have talked a lot about GNSã€‚Andã€‚In this lectureï¼Œ I would like toã€‚

Have a few additions in terms of Gs like one of the very recent state of the artã€‚

 style Gs is a very popular paper that a lot of people probably have seen we had one of the invited speakers last yearã€‚

In the introduction to deep learning courseã€‚And I would quickly want to mention thatã€‚

 I would also like to go afterwards over autoag models like as an alternative paradigm to GNsã€‚

 but finally I think we want to go from images to videos and want to see how we can actually generate and sequences of framesã€‚

ğŸ˜Šï¼ŒNowã€‚Yeahï¼Œ last thing we stopped was we had different architectures and one of these architectures we discussed actually in the last lecture was the progressiveive growing architectureã€‚

So the idea was be having a discrimator and a generator and we're growing each of these layers and we're adding layer by lever in the layer in this blending based fashion so we can get relatively higher solution outputs while still managing to avoid things like mod collapse and so onã€‚

So stylekin isï¼Œ in a senseï¼Œ very similarï¼Œ so it builds upon traditionalã€‚

You know these generators and discriminators that are trained with progressive can fashion and well if I'm looking at a traditional latent space base scan hereã€‚

 this is a again that takes here this lant vector to Z right and what it's doing is it has here generator rightï¼Ÿ

And this one is well it has first fully connected layer has a bunch of cons upsemble con ups sampleble con ups and flaps and gl so on so this whole thing here is a traditional generator right and at some point we have a discriminator at the end but what's happening here is that everything that we're feeding in here in Z will determine the output of the generated image right so for one given Z we will get a specific output image at the end of the dayã€‚

Now the challenge there isï¼Œ of course I similar with the conditionalgans and we have seen that this Z here doesn't have too much control right we can really control and manage to generate things that we wish like to right I meanã€‚

 of course there's things like semantic n and stuff like that infofog where you can basically make sure that this latent vector Z corresponds to some to some semantics and so onã€‚

 However it's still pretty tricky and controlling C and making sure that you can generate images in a specific way So the idea of stylegan now isã€‚

A little bit differentã€‚ So the idea here isã€‚And what we doï¼Œ we haveï¼Œ on one handã€‚

 we have a La vector Zã€‚ We have basically a couple of fully connected layers and these fully connected layers they form a mapping networkã€‚

 rightï¼Œ So this is kind of one network by itself that maps this fullyã€‚

With these fully connected layers maps the latent vector Z to some sort of indepreable style vector that we can feed into the generation networkã€‚

Andã€‚The idea is that the Gene network is actually pretty similar to progressive growing againã€‚

 They pretty much build it in the same architectureã€‚ maybe a few tweak likeã€‚ğŸ˜Šã€‚

I'll go into the second they basically you have moved traditional inputsï¼Œ they add noise inputsã€‚

 the mixed regularizationï¼Œ stuff like thatï¼Œ but essentially it's still a progressively growing based generator and what they do isã€‚

ğŸ˜Šï¼ŒThey essentiallyï¼Œ on one handï¼Œ they're feeding some noise after every few layers inã€‚

 so in this case they have oneï¼Œ twoï¼Œ threeï¼Œ fourï¼Œ four points here where they're feeding in different noise vectorsã€‚

And in addition to thatï¼Œ they're also feeding the style output from the latent vector Znã€‚

 so we have here different vectors that are being basically addedã€‚ğŸ˜Šã€‚

To the current feature maps right so and the way they're being added is with this error and I mean this is the details hereã€‚

 it's not so important how they're being added right nowã€‚

 of course they tried a lot of different thingsï¼Œ but the key insight here is that you having this progressively growing based can that you can train like that however now what you do is for every resolution so to say you're feeding in a noise vector and addition you're feeding in a style vector that youre adding to the noise and to the current feature map right and youre training this whole thing end to end and the idea is that this mapping network here will produce style vectors that are then being used for the synthesis part and the hope is that because these things hereã€‚

 they operate on different resolutions that we essentially have style vectors and lower resolutionsã€‚

 we have some of them admit resolutions and some of them at higher resolutions So some of them are more like structural elements some of them are more like for mediumã€‚

ğŸ˜Šï¼ŒDetail and some of them are fine scale detail right so you have kind of this multiresol control of the image in other wordsã€‚

 if I'm going ahead and changing these style vectors hereã€‚

 I should get different well depending on which one I'm changing I should get a different behavior right and that's kind of the idea of stylegan and again these people they this is this team in Finland and NviDdia and what they they have really a lot of expertise in terms of training this scan so they tried a lot of fiber parameters and so on the architecture I mentioned is very similar to the progressive game they made a couple of changes soã€‚

ğŸ˜Šï¼ŒThey have a few different things they changeï¼Œ they have different tuningã€‚

 they have a bilinear up and down sampling instead of just a nearest neighbor up and down samplingã€‚

 they have this mapping of the stylesï¼Œ theyre removing the traditionalal inputã€‚

 they have noise input and they have this regularizationã€‚å—¯ã€‚This is an evaluation on FIT scoresã€‚

Basically gets a little bit better than the progressive gainã€‚

 but my point here that I wanted to make I don't well I mean it's nice to get better results of courseã€‚

 but the really interesting thing is that now what they adding with style again is this explicit control off the again it's similar to a conditioning right it's similar to the way things like Pixto picks have been trained except if you're going back here that here we have this mapping network that essentially generate these style vectors that we can then add to the feature vectors of the generator right so we have the synthesis network G and we have the mapping network F and these two things they interact with each other by having the style vectors being added to the feature vectorsã€‚

And in practiceã€‚ğŸ˜Šï¼ŒThey can generate results to look like these onesã€‚

 And the reason why I'm showing them is because these results look reallyï¼Œ really impressiveã€‚

 So what they have is they have course styles right they have this at a very low resolutionã€‚

 They have middle styles from 16 to 32 squared and they have fine styles on 64 squared to 1024 squaredã€‚

 And the idea is what you can do now is you can here feed in images that generate U these styles so we can use that image to get a style vectorã€‚

 we can use that image to get a style vectorï¼Œ we can use that image to get a style vector And depending on where at which resolutions we're feeding it inã€‚

 we gonna get a different effect of the respective output and this is the generated output then you we can see that this is a very high quality result also a relatively higher resolutionã€‚

 I think this is still 1024ã€‚ğŸ˜Šï¼ŒAnd now the idea though is because we have this multi resolution separation of the stylesã€‚

 we can now go ahead and can change the stylesã€‚ğŸ˜Šï¼Œevery resolution independently so I can go ahead and change the styles only this resolutionã€‚

 I will have a specific effect hereï¼Œ I can change here the minute resolution againã€‚

 I will get a certain specific effect here and so right so let's have a look how this looks like in practiceã€‚

ğŸ˜Šï¼ŒSo in this caseï¼Œ the course styles are being changed rightã€‚

 so this change is kind of the high levelã€‚Appearance of the faceï¼Œ rightã€‚If you change the mid stylesã€‚

 you see well that the corese style of the face is roughly the same right the shape is more or less there and then like the finer you goã€‚

 the more you're changing the local the high frequency appearance but the global part the global structure is already been fixed right and this one is mostly the most control is basically here and then the fine scale control is basically here well I mean depending how fine you go rightï¼Ÿ

ğŸ˜Šï¼Œå—¯ã€‚And I think that's pretty interestingã€‚This's still not quite at the point where you can sayï¼Œ ohã€‚

 I'm having like specific semantic features that I can change at different resolutionsã€‚

 but this one has actually already some interesting control that you're having by having these different resolution levelsã€‚

 And I think that's a very cool thingã€‚ Wellï¼Œ aside from the factï¼Œ of courseï¼Œ that the outputs hereã€‚

 if you againï¼Œ if youre starting this video againï¼Œ rightï¼Œ they look reallyï¼Œ really impressiveã€‚

 I meanï¼Œ this is one of the state of the art papers hereã€‚ğŸ˜Šã€‚

That can do these kind of high quality images it takes a lot of trainingï¼Œ takes a lot of GPUsã€‚

 and a lot of expertise in terms of tuning the hybrid pã€‚Specifically to going up to thisï¼Œ yeahã€‚

 relativelyã€‚Relatively high resolution I think that's one of the yeahï¼Œ that's one of the keys rightã€‚

 I meanï¼Œ the high resolution is something that is very tricky to achieve and but in the sense you know this supports it that you have these different resolution levels okayã€‚

å—¯ã€‚Thisï¼Œ so this is a paper that has been published 2019ã€‚ It was actually a very popular paperã€‚

 like everybody liked it becauseï¼Œ you knowï¼Œ obviously the results here are pretty good and the same group actuallyã€‚

They published another paper on top of that the style can too this is actually a 2020 paper right now and they basically still they have a lot of analysis how they can still improve on the style can right so they have really a detailed series of experiments in terms of oh what hyperparameter had which effect on they're not of course claiming that they know ohã€‚

 this is the only gmination hyperparametersã€‚ğŸ˜Šï¼ŒBut based on the original sin modelã€‚

 they still show certain improvementsï¼Œ sometimes they have these speck artifacts and stuff like thisã€‚

 and that is something they can actually they didã€‚ğŸ˜Šï¼ŒThey did analyze veryã€‚

 very well and the results are pretty good actuallyã€‚ğŸ˜Šï¼ŒThis is another evaluation hereã€‚

 this is also an FD course they basically train you know the like eightã€‚

 nine days right you see there's still a difference in terms of the quantitative evaluation here so Stgan1 and Stgan 2 Stcan2 with all of these hypoparmeter optimizations they're making gets still a bit better an interesting high level change they're making is they don't use the progressive growing anymore so for whatever reason they found a way to train it without that I'm not arguing here for one or the other but what's interesting is like it seems to be there's kind of you know a series of islands in these GN modelsã€‚

are good hyperparameters right so you have a bunch of good settings that go together and these ones they produce pretty good results and as soon as they deviate from themã€‚

 it's problematicï¼Œ but there are other islands of hyperparameters that are also goodã€‚

 but it seems the challenge is to make the connections between those This is probably still one of the most tricky things I would really encourage you to have a look at this paper this is a really nice paper to read and look at specifically the reason why I think it's nice is they also have their models online and they have a really well-trained lightnt manifold space right So what you can do now is you can do things like take this pre-trained manifold with this paper which is very tricky to do they train also on faces and what you can do now is you can basically in this space have a parameter search that finds you good images and do things like image animation and so on right so you can basically in the latent spaceã€‚

ğŸ˜Šï¼Œã§ã™ã€‚Try to find good animation parametersï¼Œ which means just change the lant space Z to animate things like the phase and zoneã€‚

 and it's a very nice separation sometimes in terms of shapeï¼Œ pose and zoneã€‚

 they have also a bunch of really cool scripts online so they have already optimization scripts that help you find these latent space parameters they have a couple of piecessis actually in our lab right now they work with that if you're interested in Gs this is definitely a must paper to look at's it's very reason it's pretty cool demos that they have and you can really do cool research on top of it by using their pretrained models and doing things like I don't know we're trying to do stuff like oh give me audio signalã€‚

 try to look up the animation for the phase and stuff like thatã€‚

 So this is kind of nice you can do a couple of pretty interesting things hereã€‚ğŸ˜Šã€‚

Okay obviously the again researchï¼Œ as you seeï¼Œ this is a 2020 paper is still superã€‚

 super relevant and it's super super interesting rightï¼Œ I meanã€‚

 this is like nothing really works when you startï¼Œ but then you know after some time it gets better and better so it's kind of an interesting research trajectoryã€‚

ğŸ˜Šï¼ŒIn addition to GNsï¼Œ I wanted to also mention what auto aggressive models are doingã€‚

Our aggressive models and GNS is kind of these two generative models that we mostly use in practiceã€‚

Network wise often similarï¼Œ but paradigm wise very differentã€‚

So if you're talking about auto aggressivegressive models or let's say if we have GNs right what we did with GNsã€‚

 we just have a data set and what we're doing is we're hoping that our model learns this implicit distribution of this data right so the hope is I'm going to give you I don't knowã€‚

 like a few hundred thousand images right of faces and I'm going to train againã€‚ğŸ˜Šã€‚

And then I'm going get a model that replicates the distribution of these facesã€‚

 and based on these hundred000 facesï¼Œ I can go ahead and generate another millions or tens of millions of different faces that look realistic that match this original distribution but I actually not part of the training set anymore right but in this training process of the GNs there's no explicit notion of ever saying ohã€‚

 this has to follow a certain distributionã€‚ Yeah this is also a problemï¼Œ of courseã€‚

 because that makes the trainings so tricky I can't verify is it actually fulfilling the distribution and so onã€‚

 The only thing what I can do is I can do things like I can use use Varsstein losses that tell me oh how close are the distributions matching and so onã€‚

 So there's a couple of ideas we've looked at but nonetheless modeling the distributionã€‚ğŸ˜Šã€‚

In this implicit fashion is very difficult so autoreggresssive models kind of do the elevator way round so in this case what you're doing is you basically have a loss function that models the distribution right so the output are probabilities for instance with a softmax and because you have a softm right you make sure that this is a probability that models your distribution right and this is governed by a prior imposed by the model structureã€‚

ğŸ˜Šï¼ŒAder aggressivegressive models are veryï¼Œ very popular still they state of the art methods on imagesã€‚

 of courseï¼Œ I'll show a fewã€‚ they produce pretty good resultsã€‚

 The early results I would say in auto aggressivegressive models had very similar problems than ga results but again this is not about like I don't want to just talk about oh this paper route this good results this doesn't I think what's very important is to see kind of high level trends and concepts in order to figure out you know what are the next steps for new and good research and what are the thingsã€‚

That are worthwhile looking at in the futureã€‚å—¯ã€‚Yeahã€‚

 I should also say auto aggressivegressive models are veryã€‚

 very popular still in are probably most popular versions for other domains than images tooã€‚

 like audio and so onã€‚ we'll see this probably in one of the future lectures when I'm be talking about things like waveveNe so there has been a a lot of work on that and one of the differences there is probably that you have instead of images right you have like a fewã€‚

ğŸ˜Šï¼ŒFew quot for pixels and we're like audioï¼Œ so you would have a smaller dimension right you only have a one dimensional signalã€‚

 but you have more samples stillã€‚Okayï¼Œ so one of the first papers there I would like to mention is Pix RNã€‚

And Pixel La N is kind of a yeahï¼Œ one ofï¼Œ I meanï¼Œ it's not the first of ho aggressive papers that have been papers beforeã€‚

 of courseï¼Œ and there's a long history of thatã€‚ but Pixel La N was one of the papers that did this successfullyã€‚

 I would say on on natural imagesã€‚ So the idea isã€‚The only thing you're feeding into this model is also a set of imagesã€‚

And basicallyï¼Œ what you want to do is you want to model the distribution by a sequence of pixelsã€‚

 rightã€‚ So what you're doing is you interpret the pixelsã€‚

Often image as a product of conditional distributionsï¼Œ rightï¼Œ So in other senseã€‚

 if I look at the first pixel rightã€‚So I have a data set rightã€‚

 and I'm just looking at the first pixel I'm just checking out and trying to fit a distribution with a networkã€‚

 of courseï¼Œ that can predict what color this pixel hasï¼Œ the very first pixelã€‚Rightã€‚

 if I'm looking at the second pixelï¼Œ now what I'm going to do is I'm going to checkã€‚

And the conditional probabilityï¼Œ what the first pixel wasã€‚Based on the dataset setã€‚

 what is the most likely color for the next pixelï¼ŸAnd I'm going to go and continue that so I'm doing the third pixel right taking the first two and so onã€‚

 so I'm going to successively add more and more pixels in this conditional probabilistic formulationã€‚

 rightï¼ŸAnd essentially it maps down to a sequence problem so modeling an image is just a sequence of predicting pixel by pixel by pixel by pixel in practice you also predict pixel one at a time and the pixels are determined by all previously predicted pixel so it's a conditional probability you have right pixelix RNAN suggestsã€‚

Based on the nameï¼Œ what it already saysï¼Œ rightï¼Œ use a recurrentin neuralal networkã€‚ğŸ˜Šã€‚

In order to model this probabilistic predictionï¼Œ rightï¼Œ this conditional sequence hereï¼Œ rightã€‚

 in practiceï¼Œ it looks like thatã€‚ Let's say we have an N imageï¼Œ rightï¼Œ So we have here andã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_1.png)

Pixels here and we have here n pixels so we have n pixels and rows and pixels in the column and what we would like to do is we would like to go ahead and predictã€‚

The colorï¼Œ the probability of the color of this pixel here right and the way you're doing is you're just looking at all the past at all of these in the past at all the pixels here and here and here and here and you're multiplying based on the previous predictionsã€‚

 what is the most likely probability for a given color on the current on the current pixel right In principleã€‚

 it's very easy the easiest way to think of it if you look at the first pixel right this doesn't have any predecessorsã€‚

 So this is literally just modeling the distributions of the very firstã€‚ğŸ˜Šï¼ŒPixels in all the imagesã€‚

And you can see here that this can actually get quite long rightï¼Œ Well its in this caseã€‚

 it's n squared pixelsã€‚ So you can already guess about doing this for higher solutions isã€‚ğŸ˜Šã€‚

It's at least challengingï¼Œ right because you have a very long sequence in this RN modelã€‚å—¯ã€‚

In this case what we're doing is we predicting a single pixel color per pixel in practice when you have something like RGVã€‚

 you simply expand your sequence and for every pixel you're going to have three sequence steps right so you're saying first you predict R then G and then which is again this conditioned on all the previous pixels and it's conditioned on the two previous colors whereas r is always only conditioned on the previous pixel and so onã€‚

So in this caseï¼Œ this product just becomes three times the length because you're predicting RB and G values independentlyã€‚

It makes the sequence longerï¼Œ but in principleï¼Œ the theoretical part of theã€‚

Of the model is not changed rightï¼Œ it's still the same ideaã€‚

 you still have this R&N that goes over all the pixels and all the colors then too rightï¼Ÿ

Quest is still how do we model the predictions in one pixel pixel r and nï¼Œ as I saidã€‚

 it's an explicit distribution and the way it's modeled this is a softmaxã€‚

 so you're saying for one pixel valueï¼Œ youre having 255 you have basically a bit right so you haveã€‚ğŸ˜Šã€‚

I had 255 different valuesã€‚In this case we have a softmax formulationã€‚

 so we're treating each of them equally and all you're doing right now is you're feeding in your againã€‚

 let's look at the first pixel here rightï¼Œ you have your training set and all you're trying to do is predict what is the probability of a certain color to be coming for that pixel rightï¼Ÿ

If all my training images were green then my probability would be pretty high to have green right if not if it's mixed then it would basically give a different distribution right so that's kind of the high level idea of what we're having here in the RTB case I would simply have three times 256 ways of mees right that's ignored here but it's pretty straightforward so you train it by simply going ahead and saying oh I'm just looking at my training data I have all my images and all I'm trying to do is I'm trying to xã€‚

ğŸ˜Šï¼ŒForulated based on the data set and based on the previously predicted pixelsã€‚

 what is the most likely next predictionï¼Œ rightï¼Œ And it'sï¼Œ of courseã€‚

 it's all completely selfsupvisedï¼Œ rightï¼Œ I don't need any labelsã€‚

 All I have to do is I have to look at the imagesã€‚ and I have to figure out what is the most likely next prediction in this modelã€‚

Now the tricky thing about Pix RN of courseï¼Œ is the sequence is very longï¼Œ rightï¼ŸğŸ˜Šã€‚

it's a pretty big issue there's two things why this is problematic first of all for training it's an issue I have to basically train this whole RNN right that's challenging I might have vanish ingredients in my RNN and it might take some time to train it right the other thing though isã€‚

ğŸ˜Šï¼ŒIt might also take quite some time to evaluate because what I have to do at test timeã€‚

 I have to basically run one networkã€‚For every pixel rightï¼Œ againã€‚

 what I'm doing is I'm running one network to evaluate one of these probabilitiesã€‚

That basically how it's being formulated in the naive versionã€‚

 And so I have to run the network first for this pixel then for that pixelã€‚

 then for that pixel and so onï¼Œ rightï¼Œ So there's not a lot of parallelism going And at this point nowã€‚

This was a paper that was published 2016 and after that people thought yeahã€‚

 what are the variations of that and what can we do with it and the interesting thing is the methodology people were really excited about so what they thought is maybe there's different ways of how we change the ordering here in terms of dependencies so one variation of this we can use a row LSTM model architecture So the image is processed row by row we have a hidden state of a pixel and that depends on three pixels above respectivelyã€‚

ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_3.png)

So the idea is we have something like thisï¼Œ so if we're having this pixel that is being predicted right nowã€‚

 rightï¼Œ we basically depend on these pixels here at the top respectivelyï¼Œ rightï¼Ÿ

Not the previous pixelï¼Œ rightï¼Œ that's the difference from the naE version of the pixel on And right if I took this pixel I would againã€‚

 I would have all this this pyramid here thatã€‚ğŸ˜Šï¼ŒThat respectivelyã€‚

 I only take the three previous months hereï¼Œ rightï¼Ÿ

So the advantage here is the sequence length is a lot shorter that's one big advantage rightã€‚

 I don't have to basically go through here then through here and then through here to predict that pixel but instead I can only I can take this oneã€‚

The other thing that's pretty nice is I can compute pixels in one row in apparelã€‚

 so this pixel here and this pixel here can be computed in parallelel in principleï¼Œ rightï¼Ÿ

which is pretty nice in this case I can basically implement it on a GPU I basically start multiple kilo cons at the same time and I'm predicting that at the same timeã€‚

So that is that is pretty niceï¼Œ That's a big advantage of this LSTM model and it kind of makes it actually practicalã€‚

ğŸ˜Šï¼ŒThe downside is we have now this pixel here is not being used as input to that pixel anymoreã€‚

 so every pixel in Monroe would get independent predictions so we kind of have incomplete context for this one pixelã€‚

Could say that's okayï¼Œ rightï¼ŸğŸ˜Šï¼ŒAnd in practice it might be okay rightã€‚

 but there's this thing you have to keep in mind that basically you don't condition it anymore on the previous pixels in onero right so you would only take the ones that are aboveã€‚

And and that what happened was pretty funnyã€‚ A lot of there were a lot of different variations now being proposedã€‚

 This is a row LSDM model rightï¼Œ and then people tried a diagonal by LSDM model rightï¼Ÿ

 So in this caseã€‚ğŸ˜Šï¼ŒYou basically start here rightï¼Œ you basically condition thisã€‚

 so this guy's condition on that oneï¼Œ this guy's condition on that oneã€‚

 this kind condition on these twoï¼Œ this guy's condition on these twoï¼Œ and so onï¼Œ rightï¼Ÿ

The advantage here isï¼Œ againï¼Œ I have a much shorter sequence length than the naiveve Rnn right because if get in this one I only need to go hereã€‚

 If I go hereï¼Œ I also need to go one hereï¼Œ one here right So that's a lot easier and based on this pattern hereã€‚

 I'm solving was incomplete context problem what I had in this row-based LSTM I'm again making sure that all my previous predictions at some point influence some future predictionsã€‚

 which is what you want to have right So in this case rightã€‚

 this guy here is actually being influenced by every pixel hereã€‚ğŸ˜Šï¼ŒBeforeï¼Œ rightï¼Œ which is niceã€‚

 That's what we want to haveã€‚So the hidden state of the pixel of a specific pixel when you do this in practice depends on two previous pixelsã€‚

 namely this one and that oneï¼Œ which is nice andã€‚ğŸ˜Šã€‚

The good thing is you can actually still do the processing in a diagonal way right I could also so this so let let's see this pixel here only depends on that pixel right that one I can processã€‚

å—¯ã€‚This pixel here depends also on that pixel and on that pixelã€‚And this pixel depends on that pixelã€‚

 So thisï¼Œ this and this pixelï¼Œ I can process in parallelã€‚This pixelï¼Œ that pixelã€‚

 that pixel and that pixelã€‚Doesn't depend on each other but one I can also processã€‚

 so we have this diagonal parallelism that we can do rightï¼Œ which is niceã€‚

 so we have some parallelism and we're solving incomplete context problemã€‚ğŸ˜Šã€‚

So this is a thing that people have been basically doing when this pixelix alarm and Heva came outã€‚

 it solved this incomplete comics problem while still allowing for some parallelism to be processedã€‚

 rightï¼ŸYeah in the pixel R an architecture the idea to implement it and in practice was you had these mask convolutions so only previously predicted values can be used as context right so in other words we have basically different masks so here whatever was previously processed we have Rg and B and basically what you want to do is right in order to get the respective next one you cutting off the connections here right so here the context is not being used for these ones and here basically this be's cutting off for that one right and in this case we have two different masks we have one mask that during the first conf that restricts the context that's what's happening here so this context is not being used here and we have a mask B for the subsequent confs right and masking is simply done by setting the respective values to0 and and by this way byã€‚

these masksï¼Œ you can enforce that the network is just not forwarding whatever the current image has seen right So the reason why we need the mask is basically we seeing an image and the way we training it is were simply masking out the current pixel and based on the previous predictions or based on the previous pixel that we have in this imageã€‚

 we're trying to predict the current pixel right That's why but in order to do thatã€‚

 we need to mask out the current pixel Otherwise you would just forward it right So there would be no learning going onã€‚

 So in this caseï¼Œ the network is being forced with these masksã€‚

To learn based on all the previous pixelsï¼Œ what is the probability of the current pixel colorï¼Œ rightï¼Ÿ

mï¼ŒOkayã€‚So this is Pixel RNã€‚Againï¼Œ this was one of the first papers that did it basically on images in this autoaggressive fashion and they got a results to look like these onesã€‚

 againï¼Œ dont don't judge the quality maybe too much at this pointã€‚

 this was the paper in 2016 this was trend image nowã€‚

 so it's a pretty difficult dataset set you see you know it's challenging it's not not that high quality what we have seen like the things like style againnesa right now but it also is not at the same hyper parameter levelã€‚

 but at the time the this was consider to be an interesting alternative to GNs and it showed actually quite some some promising resultsã€‚

 So people thoughtï¼Œ yeah it's a pretty cool idea we can probably use it now the one thingã€‚ğŸ˜Šã€‚

What I mentioned already is this dependency causes a couple of issuesï¼Œ rightï¼Ÿ

With pixel Las you have this issue that you have to go through this like you have to have an LSTM architecture to model and sequence right that's the challenge it's not just challenge from a training perspectiveã€‚

 but it is also challenging from a evaluation perspective so in practiceã€‚This becomes veryã€‚

 very costlyã€‚And there's been kind of a follow up version by a similar set of authors also from now Deep Minindã€‚

And in this caseï¼Œ we wanted to think about can we use convolutions there instead of this CNNã€‚

 so we're using kind of a mask version of convolutionsã€‚å—¯ã€‚

But lets let's have a look again right So the problem is this row in diagonal LTMã€‚

 They have this issue of this potentially unbounded dependency range right which depends obviously learn the pixel on the image size right so that's a big problemã€‚

 And this is computationally rather challengingã€‚ We'll see this later on also when people do it in audio like these first wavening models that took a really long time to elevate And people had to think about you know doing something potentially a little bit smarter thereã€‚

ğŸ˜Šï¼ŒOkay so pixel CNNï¼Œ the idea now is we're using standard convolutions to capture a bounded receptive fieldã€‚

 so one layer has a bounded receptive field we can set the receptive field to a specific sizeã€‚

 so for one given layer you see only in the past to a specific sizeã€‚And because of thatã€‚

 all pixel features can be computed at once during training and during testing it' very similarã€‚Andã€‚

But basically during training this is a massive speedup right and in practice it's going to look like thatã€‚

ğŸ˜Šï¼ŒYou're going to have an architecture that looks roughly like thisã€‚We have here input RGBã€‚

 we have hiddenened widthï¼Œ rightï¼Œ we have this mask a convolutionã€‚

Similar to the mask we've seen beforeï¼Œ but this is now a 2D convolution hereï¼Œ rightã€‚

 we have a reite blockï¼Œ relo con freelo conf and thenof Maxï¼Œ rightï¼ŸOkayã€‚

 and the idea now is this mask what it's doingã€‚It's simply for the current prediction is at the center of the maskã€‚

 you're masking out the entire future context including the pixel itself right it doesn't see the pixel itselfã€‚

 it doesn't see anything of the futureã€‚And the idea is if you apply this mask to every pixelã€‚

 you're naturally getting a similar pattern than what the pixel on end would be doingï¼Œ rightï¼ŸğŸ˜Šï¼ŒSoã€‚ğŸ˜Šã€‚

Yeahï¼Œ you're simply masking out the future vlogs hereï¼Œ rightï¼ŸAnd that is much nicer to do nowã€‚

 that is because this model here preserves the spatial dimensionsã€‚ğŸ˜Šï¼ŒAnd the mass convolutionsã€‚

Can be simplyã€‚They to mimic the same behavior of the sequence right they simply avoid you don't see the future contextã€‚

 you still basically conditioning the current pixel predictions or the current feature generation based on the previous pixels rightï¼Ÿ

Andã€‚So they have these gated pixel CNNsã€‚ They have these gated blocksã€‚ Basicallyã€‚

 they're using these masksï¼Œ rightï¼Œ And another thing they're doing is they they realizedã€‚ğŸ˜Šã€‚

If they just used the mask with P CNN and Pix learning Nï¼Œ the P CNN didn't perform so wellã€‚

 And one hypothesis was that theã€‚The the LSDMã€‚With with with the sigmoid in the 10h functionsã€‚

 they actually heldã€‚ They didn't quiteï¼Œ you knowï¼Œ it was not quite clear Yã€‚

 but basically the expectation was that the sigmoid in the 10 h functionsï¼Œ they were betterã€‚

 So now what happened with gated pixel CNNsï¼Œ this gated blocks basically they're using also the 10 h and sigmoid functions and then you have an element wise productã€‚

 So againï¼Œ what's happening here is you have the K layerï¼Œ rightï¼Œ these are the weightsã€‚ğŸ˜Šã€‚

These are the weightsï¼Œ you're feeding in the current inputï¼Œ you have a sigmoidï¼Œ you have a 10 hã€‚

Then you have an element calledï¼Œ and then you're getting the current output of this blockã€‚

And the idea was by replacing the res with a skated block of sigmoids and 10 Hsã€‚

 it's kind of very similar what the LSDM is doing right And if they did thatï¼Œ they noticedï¼Œ ohï¼Œ okayã€‚

 if we're getting betterã€‚Better performance outputs again in this caseï¼Œ rightï¼Ÿ

And this is kind of the high level idea of the pixel CNNNs right in this caseã€‚

 this is literally this gated block is basically mimicking what the pixel RnN is doingã€‚

 but now the difference is it's computationally much much more efficientã€‚ğŸ˜Šï¼ŒIt has one downsideã€‚

And there's a blind spot actuallyï¼Œ if you're looking closely so let's say we have here a 5 by5 image right in this case we have a 5 by5 image and let's say we have a  three by3 convolution What we want to do right is we basically in order to go from here to the next layer we are now applying this three by three conf at all valid locations right so you know we're just sliding this three by three image here through until we get here to the middle right So this is the last one and when you're seeing the last one hereã€‚

 you notice very quickly that in order to compute this output here you're never going to see this pixel here is input rightã€‚

ğŸ˜Šï¼ŒIf I'm going you to the respective next oneï¼Œ I have the same issueã€‚

 so if I' going if I'm applying the filter kernel hereï¼Œ it doesn't take this pixel hereã€‚

 this pixel here is missing right nowï¼Œ rightï¼ŸSo okayã€‚Start overï¼Œ it starts hereã€‚

 this pixel here is never being usedã€‚Rightã€‚Here it's masked out and then it's not usedï¼Œ rightã€‚

 This one here in the next row would also not be usedã€‚å—¯ã€‚And that is this blind spotã€‚

 so basically this pixel here is unseen context and this blind spot this goes all the way through the image actuallyã€‚

 rightï¼ŸğŸ˜Šï¼ŒAnd that's a bit of a problem because now you're having the situation where you have kind of this unseen context that is not being used for the respective future predictionsã€‚

 rightï¼ŸğŸ˜Šï¼ŒYeah the question is how can you avoid this with a series of convolutionsã€‚

 there's a relatively simple idea behind itï¼Œ you just have to apply the convolutions and modify the kernels a little bitã€‚

ğŸ˜Šï¼ŒAnd this blind spot can be evaluated by splitting the convolutions in two stacksã€‚

 So you have a horizontal stack and you have a vertical stackï¼Œ rightï¼Œ So in this caseã€‚

 you have in here the vertical stackã€‚ğŸ˜Šï¼ŒAnd you have a horizontal stack here of convolutions and the idea is you first run the vertical stack and then you're running the horizontal stack right so that's kind of the high level idea in this case this vertical stack makes actually sure that you're taking all of of the inputsã€‚

Here in and it's a very simple idea it comes a little bit of overhead because now we have twoã€‚

Two steps basicallyï¼Œ and that makes it a little bit harder to doã€‚

For whatever reason my window just decided to open let me see if I can actually close it againã€‚

Of course it doesn't workã€‚Whateverï¼Œ let's just go ahead and let's see if I canã€‚ê·¸ã€‚Okayï¼Œ all rightã€‚

 so we can eliminate the blind spotã€‚This is really excitingï¼Œ actuallyã€‚All rightï¼Œ I'll leave it onã€‚

 think computer science buildingã€‚Wellï¼Œ if anybody complains about the video quality nowã€‚

 please send an email to our custodians not to automatically remove the blind when the sun is like really burning hereã€‚

å—¯ã€‚Okay Iã€‚Yeahï¼Œ so we're eliminating the blind spotã€‚

 we can also do relatively simple things like conditional pixel CMã€‚ğŸ˜Šã€‚

Basically what we can do is when we applying these blocks here like this one hereï¼Œ rightã€‚

 we can do conditional image generation by simply appending a vector here and here respectively to the weights that are multiplied with the respective inputsã€‚

 rightï¼ŸğŸ˜Šï¼ŒThis is simply a latent vector that is used for the conditioning right in this case you can use a class vector so let's say I have a classifier from network right I'm using these class specific features and I'mending them here to indicate ohã€‚

 this is a specific class rightï¼ŸğŸ˜Šï¼ŒAnd this gives some controlã€‚

 it's actually very similar to things like picks to picks where you basically have also some sort of Y the discriminator sees the different conditioningsã€‚

 in this case we simply have this condition lant vector hereã€‚ğŸ˜Šï¼ŒOkayã€‚

 if youre running this model we getting results that look like these onesã€‚

 so this one is on coral reefs and and horses right so you see well you know you're getting getting realistic images here as output rightï¼Ÿ

ğŸ˜Šï¼ŒYou have these horsesï¼Œ they look like horses if you're looking closely you will see the resolution not so highã€‚

 but again you have to make a fair comparison hereã€‚

 this is a model that was that was trained 2016 right it's already quite some time agoã€‚

 but the pixel scene and model is actually something people have been working on and they have improved it it also got better hyper parametersã€‚

and yeahï¼Œ the results I think got also better over timeã€‚

 I show you in a sec like what the state of the art can do hereã€‚

But I think it's a very interesting question of what generative model you want to use rightã€‚

 do you want to rather use auto aggressivegress modelsï¼ŸğŸ˜Šã€‚

Or do you rather want to use scans The big advantage of autoregressive models is this explicitã€‚ğŸ˜Šã€‚

Modeling of the probability dens gives you much more stable trainingã€‚ like training a pixel CNNã€‚

 I meanï¼Œ againï¼Œ it's comp also costlyï¼Œ rightï¼Œ but it's not gonna be like this crazy hyper parametera tuningã€‚

 like what you have to do for genï¼Œ right that isï¼Œ that is a reallyï¼Œ a really big advantageã€‚

 So I would say the training is much much easierã€‚ It's much more stableã€‚

 And this is something that is worthwhile doingã€‚ğŸ˜Šï¼ŒIt can be applied to both discrete and continuous data as long as you can model a probability many times what you see isã€‚

ğŸ˜Šï¼ŒPeople go ahead and discretize the data and use something like a soft mix to model itã€‚

 we'll see in the next lecturesã€‚ Also a few more examples of these cases where auto aggressivegressive models have been veryã€‚

 very successfulã€‚ So one case I've mentioned is the audio caseï¼Œ Another case will be shape modelingã€‚

 There was a very popular paper polygen that came out only a few weeks ago that is also fallen into this auto aggressivegressive categoryã€‚

 So from a pure theoreticalical standpoint from your toolbox of deep learning tools yeah auto aggressivegressive models are veryã€‚

 very important actually Yeahï¼Œ the advantages of Gsã€‚ğŸ˜Šã€‚

Empirically people think oh you know they can produce higher quality images I think you know the community is developing in certain waysã€‚

 sometimes these are a bit betterï¼Œ sometimes these it is indeed true that the GNs currently produce better resultsã€‚

 they're also faster to train they're faster to train but they're much more difficult to train right so this is kind of the pros and the caveats what we're having about these modelsã€‚

ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_5.png)

I also wanted to show this is one of the very recent results on auto aggressiveive models this VQ VE2 it's basically variational autoencoder and they have a very they have a vector quantized formulation and these results are pretty impressive right and if you're looking closely they might not quite be at the same level you know what style gain is doing these days but still very impressive also high resolution also takes a long time to train but but super impressive results right it's very it's very interesting so if you're interested in this results look at this paper this is also 2019 paper very active research area there's still a lot of improvements that can be made you know like how to quantize thingsã€‚

ğŸ˜Šï¼ŒWhat encodes do useï¼Œ what losses are you're using and stuff like thatã€‚

 So there's a lot of interesting design choices still in the architectures hereã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_7.png)

Okay good yeah with that I would like to well more or less conclude the generative models and imagesã€‚

 but now I want to see how can we do things on videos and as you can imagineã€‚

 you know like going from an image which is already a higher resolution going to videos which is a muchã€‚

 much much higher resolution is a very very challenging endeavorã€‚

 and I would say theres still two different categoriesï¼Œ one categoriesã€‚ğŸ˜Šã€‚

Like what again is doing a purely gener model go from a latent vector to videoã€‚

But then there's also methods that go fromã€‚More like this conditional game case right so you have some guiding and then you can go there but let's go to the first case first right I want to go ahead now and basically sayã€‚

 wellï¼Œ give me one of multiple random vectors and generate generate a video as the outputã€‚

And let's have a very simple version firstï¼Œ let's go back to GNs for a time being and let's imagine how would we do this on videos if you had to design your own GAN modelã€‚

 what are the optionsï¼Œ how would you design your architecture very naively right like without thinking too muchã€‚

å—¯ã€‚If you're looking atã€‚At Gsï¼Œ rightï¼Œ we know we have a latent code Zã€‚

 and basically there's two optionsï¼Œ rightï¼Œ One thing I can do is I can say I have a single random variableã€‚

ğŸ˜Šï¼ŒThat is used to generate and determine the entirety of the video right so one single vector determines every single frame that comes out of it right in this caseã€‚

Wellï¼Œ the output is very highly dimensionalï¼Œ of courseã€‚The question is basicallyï¼Œ how do I knowï¼Ÿ

Weant to stopã€‚So if you did this pastã€‚Part hereï¼Œ what you could do is I can go aheadã€‚

 have agan that gives me a la code Zã€‚ I could do something like an LTMï¼Œ rightï¼Œ and I could predictã€‚ğŸ˜Šã€‚

In the first frame from letting vector de Zï¼Œ I predict one imageï¼Œ rightï¼Œ that oneã€‚

 something I can doã€‚ Nowï¼Œ if I want to predict the next frameã€‚

And only condition that one frame on the previous frameã€‚

You're going to have this issue that for this one frameã€‚

 you're always going to get the same next frameã€‚And you will see that this is already fundamentally a challenging issue becauseã€‚

If I'm going do thatã€‚I like if if I'm in a real sceneryï¼Œ rightï¼Œ of courseï¼Œ for a give still frameã€‚

 I can have multiple future predictions where this video will developã€‚ğŸ˜Šã€‚

So if I used the late code Z only to predict the first frame and then only use the first frame to predict the next frameã€‚

 the next frame to predict the very next frame and so onã€‚

 I would not have a lot of variety in the video like how the motion would go right I would only keep basically the current frame and this is the challenge now if I only have a random Z this Z needs to be also input in some way to the next frameã€‚

Rightã€‚Because againï¼Œ otherwise you're going to have this issue that if you only predict in the previous frameã€‚

 you always will get the same motion in practice you get no motion in practice this is just going more collapse so you have to have a way to get the Z to the very next frame propagated in one way or another either the feature maps right or the Z directly by itselfã€‚

ğŸ˜Šï¼Œå—¯ã€‚Yeahï¼Œ so future frames deterministic given the past rightï¼Œ that's a bigã€‚

 that's a big challenge we have to address hereã€‚ So thisï¼Œ this is this thingã€‚

 if you have a single vector Zï¼Œ that's a problemï¼Œ rightã€‚ğŸ˜Šï¼ŒThe second option isã€‚

What you can do is I can go ahead and have a random variableã€‚That generates me frame Aã€‚

The first frames generated withï¼Œ with one random seedã€‚I'm going to have another random vector zã€‚ğŸ˜Šã€‚

I'm going to generate the next frameï¼Œ and when I have another random vector generates the very next frameã€‚

So by doing thatã€‚ğŸ˜Šï¼ŒI can throw basically independent frames with inencies if I have trained again now the challenge isã€‚

 of courseï¼Œ if I want to have a videoï¼Œ I need to have in addition to that some conditioning to each otherã€‚

ğŸ˜Šï¼Œå—¯ã€‚Andã€‚Wellï¼Œ what you can do now is generate the first frameã€‚

 wouldn't have any previous conditioningï¼Œ rightï¼Œ It has has it has no past framesã€‚

 The next frame would get as inputã€‚A single rate vector again for the current frame as well as the previous imageã€‚

 so the task in this case would be take previous frame plus take new rate in code Zã€‚

 generate the next frame that corresponds to each otherã€‚

And you can do that so basically what I just described is you need this conditioning for the future from the pastã€‚

ğŸ˜Šï¼ŒBut the big challenge you're going to get now when you train thisã€‚

 how to get the combination of pass frames plus the random vectors during training right So if I have a given videoã€‚

ğŸ˜Šï¼ŒI gave this one a la vector Z and I'm generating the next frame with another la vector Z in my training sample in this video I combined I have some implicit correlation between the frames rightã€‚

ğŸ˜Šï¼ŒBut I don't have a correlation between the lant coatsã€‚And that makes it very tricky to trainã€‚

 because how do you get this combination right between training and testï¼Ÿ

 So this is very tricky here to get to workã€‚If you try this naivelyã€‚

 often what happens is simply ignores the latent vector of the previous of the future framesï¼Œ rightï¼Ÿ

So this is very challenging hereï¼Œ so we're having two options and both of them have problemsï¼Œ rightï¼Ÿ

ğŸ˜Šï¼ŒAnd theres channel issuesï¼Œ the channel issue is how do we make sure we get temporal coherencyã€‚

 rightï¼Œ how do we make sure we know jumping around again if I'm just having a gun and drawing two frames or generate two imagesã€‚

ğŸ˜Šï¼ŒThere are going to be two frames right that's just going to be random stuff so that's a big issue how to get the temporal coherency in practice I need to have some part in my discriminator that tells me oh don't flicker be smooth be a real video and stuff like that rightï¼Ÿ

ğŸ˜Šï¼ŒThe second problem what you're going to have is there might be some drift over time and often what happens if you're running these video generation methods that exist right nowã€‚

 they often collapse to an average frame right the first few frames are fine and then we have longer sequences they drift away and you're going to have a pretty interesting problem also to deal with when you're training itã€‚

ğŸ˜Šï¼ŒWhat you do is let's say you have a sequence model that predicts frame by frameã€‚

 If you have a sequence model for frame by frameï¼Œ you're taking the previous frames as inputã€‚

Now your training obviously on ground truth frames here is input hereï¼Œ but when you're testingã€‚

 you're having the previous predictionsï¼Œ so suddenly with previous predictionsã€‚

 you're not in the ground truth anymore and the further you goã€‚

 the more you drift away what is ground truthï¼Œ in principle that the quality will degenerate at some pointã€‚

 So this drift possibly more collapse to a mean image is very is a very problematic case for videosã€‚

ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_9.png)

Yeah I wanted to show one architecture this is a paper from Deep Mind DVD G this is a paper also from 2019 you can see videos are very challenging still what they do is basically I don't want to go into all the details because there's a lot of variations of there's a lot of the stuff is crazy hyper parametermeter tuning they have a random distribution here right they have one hot class vector and what they're doing is they're basically saying ohre using this one vectorã€‚

 youre distributing it in one way or another to multiple frames you have for every frame you have basically a generator then but it's determined by this one random vector so this one guy determines all of them at the same time and the discriminators what they're doing is basically they're taking the stack of this stuff in and they're telling you is it again a reasonableã€‚

ğŸ˜Šï¼ŒRightï¼ŸğŸ˜Šï¼ŒAgainï¼Œ I don't want to go into detailï¼Œ but the interesting thing is what you see is hereã€‚

 the discriminatorsï¼Œ they can't just look at a single imageï¼Œ of courseï¼Œ rightã€‚

 they need to look at more stuff at the same time in order to make sure is it a videoã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_11.png)

So this is one of the state of the arc papers you typically get results that look like these onesã€‚

 you have few different resolutions this is 256 this here is 128 and this one is 64 you see very quickly the sequences are not so long you're going to get very short sequences because longer sequences the drifting problem is a massive issueã€‚

ğŸ˜Šï¼Œå—¯ã€‚You can see you get some results that look reasonable here you have this server video right but you can already see the quality of these videos is significantly lower than whatever people have been doing with things like Staion and Z right So doing this from Sp is challenging and again this one was the state of the art example so most of the time these things are trained on the kinetic data setsã€‚

 this is kinetic 600 right they train it up to 48 frames 48 frames is like less than two seconds probably if you assuming like 30 frames a second So you see this is still a pretty this is a challenging problem right the output is pretty hardã€‚

ğŸ˜Šï¼ŒIt's pretty interestingã€‚ The one thing what I should say there's a bit of a cheat you can do for video generationã€‚

ğŸ˜Šï¼ŒOne thing you could do instead of training this completely from scratchã€‚

 you could go ahead and take something like style againï¼Œ take the manifoldã€‚

 and you just try to figure out in this manifold what would be the next spaceã€‚

 What would the next frameã€‚ And this is something that works surprisingly betterï¼Œ actuallyã€‚

 But but generating the frames completely from scratch is is still very difficultã€‚ Againã€‚

 this is a really impressive paperã€‚ And this guyï¼Œ they really know what they're doingã€‚

 This is like they've been working on this for quite a whileã€‚

 This this is not straightforward to get these to these resultsã€‚ğŸ˜Šï¼ŒBut it isï¼Œ of courseã€‚

 very interesting to do this ideallyï¼Œ we would want to also have better operatorsã€‚

 not just 2D convolutionsï¼Œ maybe some video basedã€‚ğŸ˜Šï¼ŒConvolutionsã€‚

 right that would consider the whole context at the same timeã€‚

So doing this from scratch is very difficultã€‚ I mentioned you want remedy possibly using a pretrained manifoldã€‚

 there's another potential remedy and it turns out conditional GNs are much better for videosã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_13.png)

This is a paperï¼Œ this is Ps to Ps HD hereï¼Œ this is run simply on a frame to frame basisã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_15.png)

RightWhat you're doing here is you're taking the semantic series inputã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_17.png)

And you're running this ps to Ps model from the semantics to get to hereã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_19.png)

Againï¼Œ of course it's flickering thisï¼Œ but in the network here there's absolutely zero temporal coherency right there's no temporal constraints between the different framesã€‚

 The only temporal coherence we're getting here is based on these labelsã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_21.png)

The assumption is if these labels are temporal consistentã€‚

You will also get temporal consistent outputã€‚ The next assumption is if this input is overconstraining the output enough and is deterministicã€‚

 meaning we have enough information and would always produce the same resultã€‚

 which in practice you don'tï¼Œ of courseï¼Œ But if it didï¼Œ you would also get better predictionsï¼Œ rightã€‚

And I would say if I' playing this againï¼Œ it's actually not too badï¼Œ rightï¼Ÿ Yesï¼Œ it is flickeringã€‚

 it's not perfectï¼Œ but the car is here always is thereã€‚ Of course it's here tooã€‚

 there's a bit of inconsistency here in the labelsï¼Œ maybe if these ones are not thereã€‚

 it would be betterï¼Œ but this one is already pretty goodï¼Œ I think rightã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_23.png)

![](img/5e82fa7faa43312461dac4d2e5900323_24.png)

![](img/5e82fa7faa43312461dac4d2e5900323_25.png)

So the conditional case is very encouraging because that is much easier than doing it from scratchã€‚

 there is still a challenge to get this one temporarily consistentã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_27.png)

![](img/5e82fa7faa43312461dac4d2e5900323_28.png)

And there's also a follow up paper from Pix to P space that leads called w to w it's basically using a sequential generator rightã€‚

 So the idea what you have is you have the past L generated framesã€‚

 they wrote this down as this like nice formula but in practice they said L to2 So these two of the two of the past framesã€‚

 the last L source framesï¼Œ rightã€‚ğŸ˜Šï¼ŒAgainï¼Œ El said to tooã€‚

 it's like basically taking the last two framesã€‚ğŸ˜Šï¼ŒAnd then it's a conditional probability on the previously generated frames and the previously conditioning framesã€‚

 rightï¼Œ These two are being fed in to generate the next frame here as outputï¼Œ rightã€‚

 That's what we want to doã€‚ğŸ˜Šï¼ŒAnd of courseï¼Œ this is a sequenceã€‚

 so we also we have to multiply these probabilities togetherï¼Œ so we have a sequential generatorã€‚

 rightï¼ŸğŸ˜Šï¼ŒThat's the high level idea of doing bit tobit video to video synthesis in this case there's also two discriminatorã€‚

 one discriminator DI is trained to sayï¼Œ is it a real imageï¼Ÿ

And another discriminator D is telling you is is temporarily consistent what these guys are doing roughly is this D here is just checking the flow right so you extract the flow from an image and you're telling it oh whatever you generate is that corresponding to flow what you would expect in the real videoã€‚

 So is the motion kind of smooth is consistent stuff like thatã€‚

 that's something this discriminator would be would be doingã€‚

 So the full objective here is basically we have the loss from the image here rightã€‚

 this is li D and you have F and D right So you train these two jointly which is which is pretty good I can say howeverã€‚

 this paper has a lot of finict details it's very tricky to train and probably also trends around the week even for the simple data sets but the results look really impressive that's why I wanted to show it So this is an example hereã€‚

ğŸ˜Šï¼ŒWhere we see here again the input labels we see here Ps to Ps HD this' is another baseline and here is picks here's bit to width basically right and now if you're running this we can run this hereã€‚

ğŸ˜Šï¼ŒIf you look in here this one actually looks if you're looking at it very closelyã€‚

 I think this looks really amazing actually I did a couple of funny things like the road hereã€‚

 this line markers it's not quite clear what's happening there right but if you're just looking roughly at at the motion here it looks actually pretty good right So this one I would say is' probably state of the art and compared to the purely generative caseã€‚

 this conditional case is significantly easier to train I don't think that's cool right I mean you get pretty good results of thatã€‚

ğŸ˜Šï¼ŒSo yeahï¼Œ if you're looking again at the key ideas hereï¼Œ we need two discriminators rightã€‚

 we need to figure out is it a real image and also is it a good sequenceï¼Œ this one is very importantã€‚

ğŸ˜Šï¼ŒIn this caseï¼Œ it's based on optical flowã€‚ There's probably other versions how to do thatã€‚

 You need to consider the recent history of previous framesã€‚ in this caseï¼Œ the 72ã€‚Wellï¼Œ I meanã€‚

 they wish they probably wanted to say it something largerã€‚

 but I think at some point you just run out of GPU memoryï¼Œ rightã€‚

 that's that's the practical that's the practical issueã€‚Yeahã€‚

 we have to basically consider the recent history and then we train all these things jointly that's kind of the the key idea yeah so which is super nice so this conditioning makes the whole problem of video generation very tractableã€‚

ğŸ˜Šï¼ŒThat's also something that we've been doing in the con of facesã€‚

 this was a collaboration with the MP togetherï¼Œ the video pottery paper I would say it's relatively popular paper what we had is kind of building on these blocks too so the idea is you can say well we want to condition basically we want to generate the conditioning on the fly in order to control the respective video outputã€‚

ğŸ˜Šï¼ŒAnd the video portraysï¼Œ the idea is what you want to do is you want to take a face here of Obamaã€‚

 you want to take this video and you want to control and animate the face hereï¼Œ rightï¼Ÿ

It's a very interesting thing what you can do hereï¼Œ right in this caseã€‚ğŸ˜Šã€‚

What the conditioning of the network is only this right hand sideã€‚

This is essentially it's essentially a version of pixelixs to picksã€‚

 It's a bit of a more fancy version it uses multiple frames at the same time as input and predicts only one output so it it sees the temporal context didn't make the biggest of a difference but the key insight what I want to say here is we have a bunch of synthetic rendering of faces hereã€‚

 ignore the left-hand side right now just look at this party in the middle This party in the middle is a bunch of synthetically rendered faces in this case it's just RTP values these are 3D coordinates of the model and it's a bit hard to see these are the eye locations hereã€‚

ğŸ˜Šï¼ŒAnd all of these are all of these three from one frameï¼Œ are stack togetherã€‚

They have fed into this pixels to picksï¼Œ it's a unit version of pixels to picks rightã€‚

 and this network is being trained to generate realistic imagesã€‚From this one video sequenceã€‚

 this ps to p here is only trained on one Obama videoã€‚

It's the only trained in this one video it's maybe 1 thousand0ã€‚

2000 frames it's not so much right so this network will make sure that whatever I'm feeding in here is correlated with whatever the Obama video is looking like so please take this and make an Obama frame out of that again it's also taking multiple frames here I think we took five frames so a window of five framesã€‚

ğŸ˜Šï¼ŒTook all of them as input and predicted only the middle frameã€‚

 so the network sees the future in the pastï¼Œ but it didn't make the biggest of a differenceã€‚

 and I'll tell you in a second why it doesn't make such a big differenceã€‚

And the reason now is here how we generate this conditioning the conditioning now what you can do is you can take the original videoã€‚

 you can run a face tracker the face triggerer gives you a bunch of parameters like illlimination identity pose expression in I right basically all of these things here you can control with a 3D modelã€‚

 you can use a graphics method right nowï¼Œ take a GPU rendererã€‚

 take these parameters here as input and rerender themã€‚ğŸ˜Šã€‚

So this re renderndering is of course not a realistic video because these parameters doesn't have stuff like the background on the shadow yeah and the material parameters and so on also not super accurateã€‚

 this is just a high level representation basicallyã€‚ğŸ˜Šã€‚

But the nice thing about this presentation is that it's fully controllable so we can go ahead and change these parameters as we wish right we can just change the poseã€‚

 we can rotate the headï¼Œ we have an absolute notion in how to rotate the headã€‚ğŸ˜Šã€‚

The ability if we don't have yet here on this part it's just a forward GPU renderã€‚

 it looks kind of like a video game from the 80s so sayï¼Œ rightï¼ŸBut in the combination nowã€‚

 is we can have the control here in our track modelã€‚

And then use the network to make realistic videos out of it againã€‚ So in other wordsã€‚

 we can control this spaceã€‚Andã€‚The reason why this works isã€‚

This trackingre here is pretty accurate like this is a per pixel based tracking it's based on methodology wise it's similar to the face to face methodã€‚

 rightï¼Œ you can track a per pixel error hereï¼Œ you're getting very good fits of the faceã€‚ğŸ˜Šã€‚

And this makes it relatively easy for the network to learn the correlation between this motion of the face here or these pixel of the face here and respectively hereã€‚

ğŸ˜Šï¼ŒBut now what we can do isï¼Œ of courseï¼Œ we can do editsã€‚

 You can take a different source sequence to take a bunchï¼Œ track the sequenceã€‚

 take a bunch of parameters here and recombine the parameters here and then renderã€‚Wellã€‚

 and then change basicallyï¼Œ for instanceï¼Œ things like post expression and I in this rendering and then the networkbu again will make sure that we're getting a realistic Obama again so in practice looks like thatã€‚

ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_30.png)

We have here the target model here of herï¼Œ so we took her videoï¼Œ we reconstructed her faceã€‚Andã€‚

Here are the positions of the 3D modelï¼Œ here the eyesã€‚

 and now what we do is we have a source sequenceï¼Œ that source sequence is being used only to control the synthetic 3D model hereã€‚

And then the neural network converts these three images again to a realistic phaseã€‚

And if you're running thatï¼Œ you can resolve the look like thisã€‚

So you're seeing the motion here it's transferred to the synthetic model and then the network makes sure that this one is yeahã€‚

 it's basically recreated to a target video of her and it's pretty interesting in the shadow here is moving around right in this model there is no shadowã€‚

 but the network learns whether the head goes left and rightã€‚

 the shadow also moved left and right It doesn't have a correct illumination model or whatsoeverã€‚

 but it's just a very easy example because it's a portrait video and then it learns how to translate that rightã€‚

ğŸ˜Šï¼ŒIt's a few more examplesï¼Œ here's he's controlling the virtual phase of Obamama here and the video is basically making sure that you can generate a realistic output here at the endã€‚

 rightï¼ŸğŸ˜Šï¼Œå—¯ã€‚Yeah the eyes are very importantï¼Œ you need a special image otherwise the eyes are being ignored otherwise they're too small of a region on and the loss to be considered right that made this one to work you can do manual editing you don't have to take a source sequence you can go ahead and take an arbitrary video or image and you can basically specify how should the post here of the person look like you can change the smiling parameters in this case that's part of a PCA if you're interested in that yeah you have to look at what the parameters these face models are but the interesting thing is part of this class here is reallyã€‚

ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_32.png)

By controlling the synthetic 3D modelï¼Œ we can generate a video out of it with an neural network and again this pixel to pixel model doesn't have a lot of temporal coherenceã€‚

 yes it sees a windowï¼Œ but the reason why this works is because we are seeing multiple frames at the same timeã€‚

The reason by averbs is because our trackingre is temporally consistent right So the only temporal coherenceã€‚

 well aside from this windowï¼Œ what we're getting is based on the training dataã€‚

 here have a few more examplesã€‚ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_34.png)

å—¯ã€‚We have here Yaã€‚We just have a lot of fun of itï¼Œ of courseã€‚

 right you can basically change and edit videos now in a pretty arbitrary arbitrary fashionã€‚Okayã€‚

 there's a couple of cool insights that I wanted to talk aboutã€‚

 And the one thing which I think is super surprising isã€‚ğŸ˜Šã€‚

Well the synthetic data for tracking is a great anchor right basically this eliminates the swift problem that the standard GNs would have for videosã€‚

 you having basically a sequence of synthetic inputs that you just want to convert and translate to different domainã€‚

 but they serve as a great anchor if they are temporarily stable the output will be also temporarily stable and that's pretty nice right so that's a pretty good thingã€‚

ğŸ˜Šï¼Œå—¯ã€‚The overfitting on small data sets for this kind of stuff work surprisingly wellï¼Œ rightã€‚

 We have maybe a few thousand frames of the videoï¼Œ potentially get with a few hundredã€‚

 but a few thousand is probably betterï¼Œ but it's not like hundreds of thousandsã€‚

 right compared to like stylegannerã€‚ So you need like hundreds of thousands of training frames in a lot of augmentationã€‚

 much easier hereï¼Œ rightï¼Œ much easier problem statementã€‚ğŸ˜Šã€‚

And can generate temporarily consistent videos right admittedly of course you need the conditioning right this is a big challengeã€‚

ğŸ˜Šï¼ŒYeahï¼Œ the one thing you need to do is you need to stay within the training setï¼Œ in other wordsã€‚

 you cannot expect that this network generalizes to a sense thatm yeah I don't know the person suddenly turns aroundã€‚

 you have never seen the back of the headï¼Œ so of course you would never get any realistic results there rightï¼Ÿ

So you have to stay within the training set with respect to motionï¼Œ with respect to eliminationã€‚

 with respect to poses and stuff like thatï¼Œ rightï¼ŸğŸ˜Šï¼ŒSo that's the thing nowã€‚In a senseã€‚

 what's pretty interesting about itï¼Œ I would like to make you think a little bit about what is learning actuallyã€‚

 rightï¼ŸğŸ˜Šï¼ŒSo learning this case means in this training sequence that they're giving the networkã€‚

 we can interpolateã€‚ We can also extrapolate to some degreeã€‚

 but not too much right there's no learning going onã€‚ It's just an optimization problem nowã€‚

 how do you recombine this existing training set and how do you encode it in the model with STD So it's kind of the neural networks is more like an optimization frameworkã€‚

 And I think that's the thing that to me sounds veryï¼Œ very appealingã€‚

 I think this is a super cool research direction for the future This is something if you interest in researchã€‚

 This is something you should think about right like how can we kind of leverage the advantages of deep learningaring frameworks for various optimization problems because they work so wellã€‚

ğŸ˜Šï¼ŒSo this is something what we've done on facesï¼Œ but you can do this on other things tooã€‚

 you can do this on bodiesï¼Œ there's the super cool paper everybody dance nowã€‚ğŸ˜Šã€‚

This is a paper that was done at Berkeley by Chaand Al and what these guys are doing is a very simple idea except to do it for human bodies so what you do here is you taking as trainingã€‚

ğŸ˜Šï¼ŒYou basically have a p to Ps model rightï¼Œ So what you do is you have here why is the input image you haveã€‚

ğŸ˜Šï¼ŒAn extractor here that gives you a human skeletonã€‚

The human skeleton is generated in this case with I think open pose also it's just a standard deep learning library that figures out how to get the poses hereã€‚

 This one is I think iss not even entrainï¼Œ I think this is just the preprocessing step basically that generates you the skeleton for this video so you have the kinematics basically now you take a generator that learns how do you go from hereã€‚

To the to the underlying video againã€‚And then the discriminator takes the pair of human skeleton with respect to the generated output and tells you is it real or is is it fake And the real example is justã€‚

 wellï¼Œ you do the same thing for the real image just track the skeleton and check oh these this is a pair that matches right So these two things sayã€‚

 ohï¼Œ this is a real pair and this is the fake pair And you knowã€‚

 the discriminator generator same way Ps to picks fights each otherã€‚ they also useã€‚A VTT laws hereã€‚

 they're basically sayingï¼Œ well whatever you're generating here in feature space should match what you had originally right so you want to make sure you're generating realistic imagesã€‚

 they don't use an L1 loss here they use a VTT lossã€‚

And this one actually works to little bit better than the L bundlesã€‚

 right it doesn't have to solve a smoothing problem and so onã€‚ğŸ˜Šï¼ŒBut in this caseã€‚

 of course it's super easy to get the ground truth the too right what you have to do is you just take a videoã€‚

 you run your skeleton trackerï¼Œ you have these pairsï¼Œ you have the pairs here for the Bgiossã€‚

 you have this pair here for the skeleton and you have here the ground truth pairs respectively right and again this network here this generator learns to only generate videos from this one given target sequence it doesn't have to generalize between many sequences it's pretty cool right and the transfer then works by simply saying well now what he's doing is you simply can animate this skeletonã€‚

ğŸ˜Šï¼ŒBecause you don't want to animate it by handï¼Œ you're just taking a different source sequence by primeã€‚

 you're running the skeleton tracker here again using that skeletonã€‚

 and then you're feeding this one into the networkã€‚

What they did is they normalizedize the skeleton because one thing they realized is if you take one videoã€‚

 this skeleton has a certain height depending the person on the personï¼Œ of courseã€‚

 so there's some normalization stepï¼Œ there's a bit of augmentation step in how the deal with the skeleton but the quality is pretty straightforward right the quality is you just take this input imageã€‚

 track itï¼Œ fit in the generator generator makes it really image out of it again rightã€‚ğŸ˜Šã€‚

Sounds pretty easy and they get pretty cool results I would really encourage you to check out their video they also spend a lot of time in putting this video onlineã€‚

ğŸ˜Šï¼ŒIt's artisticallyï¼Œ I think it's very well doneã€‚ This is also why I've got a lot of attentionã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_36.png)

You can see that so this is a sourceï¼Œ this is a target these political training sequencesã€‚

And now they're using the source and em mapping the motion of him to herã€‚Rightã€‚I meanã€‚

 you see that the hands are not perfectï¼Œ but you know it looks I think it looks pretty cool actually right and again this is trying only basically human one sequenceã€‚

ğŸ˜Šï¼ŒYeahï¼Œ I think we should we should watch it a bitã€‚ I think it's it's a cool videoã€‚

 I'm always pretty excited because I think they did also a really good job in terms of you knowã€‚

 using the artistic contentã€‚ğŸ˜Šï¼ŒOkayï¼Œ so you seeï¼Œ I mean of course this is still a very simple setting rightã€‚

 the camera points always front facingï¼Œ same with like the video portrait is always is very front facingã€‚

ğŸ˜Šã€‚

![](img/5e82fa7faa43312461dac4d2e5900323_38.png)

But it works surprisingly well for the simple setupã€‚ğŸ˜Šï¼ŒThey are stillã€‚I meanã€‚

 the nice thing is here this works on different inputã€‚

The problem what you're seeing here often is if you're looking at the armsã€‚

 the arms here are pretty broadã€‚And of course kind of I picked this example here where it doesn't work so wellã€‚

 the reason why I wanted to show that specific example is the reason why it doesn't work so well is because the tracker is not 100% accurateã€‚

ğŸ˜Šï¼ŒIf you had perfect trekkingï¼Œ I would expect this to work much better for the facesã€‚

 you would not see that but the face trackingre is a bit more accurate than the bodiesã€‚

 but bodies I mean the human skelealã€‚ The reason why it's challenging isï¼Œ wellã€‚

 if you have an arm like this right You're gonna have this issue Where exactly is my bone here in the middle I just don't know it right There's no notion where the bone isã€‚

 you don't even have ground truth data for that right you can do an x-ray like antiate stuff and so it's tricky right And this one makes this for body still a bit more challenging And you see the resolutionals on the face is not not that highã€‚

ğŸ˜Šï¼ŒBut the direction is really cool rightï¼Œ so you can think about if you got possibly better trekkingã€‚

 you would probably also getã€‚ğŸ˜Šï¼ŒBetter results hereã€‚

Another thing we're seeing right now is this network here has no notion of 3Dï¼Œ same4 video potterã€‚

 same idea right you have basically you have some conditioningã€‚

 you get a 3D proxy that renders stuff here in this case the Sckelelon T video porter is both the faceã€‚

ğŸ˜Šï¼ŒBut there's no explicit 3D notion right in the network itselfã€‚

 the network is just a series of 2D commvolutionsã€‚ and I think that's in a senseã€‚

 a pretty bad design choiceã€‚ So we'll go into this in the next lectureã€‚

 but this is something I think that is still a draw hereã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_40.png)

But the insights I think here is the conditioning by tracking is super cool right I mean it's just so much easier and you can do so much more cool stuff than generating stuff from scratchã€‚

ğŸ˜Šï¼ŒSo the conditioning case of of the Gs for videosï¼Œ I think is super excitingã€‚

 I think that's a really nice and nice directionã€‚ And we's just started rightã€‚

 if doing a bunch of stuff on facesï¼Œ people have done a bunch of stuff on on bodies nowã€‚

 but you know you can basically create virtual reality thingsã€‚

 You can do computer graphics now with neural networks and stuff like thatã€‚

 So that'' that's interestingã€‚ It's surprisingly simple right you just track the human skeleton and run itã€‚

 in this caseï¼Œ they run open poseï¼Œ I think it's not so temporarily stableã€‚ğŸ˜Šã€‚

coCould make this a bit betterã€‚ And so the tracking quality is everything here right The fun fact was also after this paper came outã€‚

 they have a couple of other papers they tried the same thing or they did similar thingsã€‚

 similar ideasï¼Œ of courseï¼Œ different hyper parametersã€‚

 but this conditioning was kind of in the same spurã€‚ğŸ˜Šï¼ŒOkayï¼Œ that's all I wanted to say todayã€‚

 In the next lectureï¼Œ we're gonna to talk about the shortcomings hereã€‚

 We're going to talk about the facts thatã€‚We want to have a 3D notion in the network actuallyã€‚

 and this is what basically what neural rendering is like this is also kind of neural renderingã€‚

 what I've shown you right nowï¼Œ but I wanted to introduce what neural networkã€‚

 what neural rendering isï¼Œ go a little bit into 3D deep learning laterã€‚

 And I hope that's gonna be cool thing for you to look at because that's something I'm very personally very excited aboutã€‚

 This is also what a lot of research you're doing right now in my groupã€‚ğŸ˜Šï¼ŒOkayã€‚

 I hope you're still having fun on the projectï¼Œ otherwise see you for the next lectureã€‚ğŸ˜Šã€‚



![](img/5e82fa7faa43312461dac4d2e5900323_42.png)