# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ…•å°¼é»‘å·¥å¤§ ADL4CV ï½œ è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ è¿›é˜¶è¯¾ (2020Â·å…¨10è®²) - P6ï¼šL6  - ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(ä¸Š) - ShowMeAI - BV1Tf4y1L7wg

Hello everyone welcomelcome to the advancedance deep learning for computeruter vision lecture today it's the first time that I will be giving the lecture well by giving I mean I will make the video but I think we have some very interesting content we going to talk about generative neural networks and the first thing I would like to do is I would like to give you a bit of an overview of generative neural networks obviously you have heard a lot of the terms we have also heard in the previous lectures a lot of discriminative networks but now we want to talk about generative networks mostly in the context of computer vision applications meaning that we want to generate for instance image or we want to generate a video however we also want to go over theã€‚

ğŸ˜Šï¼ŒYeahï¼Œ if there are any concepts that could possibly be applied to other domains such as audio generation and so onï¼Ÿ

There's this famous taxonomy of generative models and there's quite a few different ones actually and the first thing we have to consider always isã€‚

ğŸ˜Šï¼ŒWhat does it mean to have a generative modelï¼Œ what does it meanï¼Œ rightï¼Ÿ Typã€‚

 you have a training set rightï¼Œ And from this training set such as imagesã€‚

 you want to learn a generative model that can kind ofã€‚

you know generate new images of the same type and the same type can mean various different things and most of the time when we're talking about you know what are image types and stuff like that it's mostly referred to a density in other words we have a training set right and we want to learn some sort of density from which we can later on sample in order to generate new samples that follow the same distribution butã€‚

Of courseï¼Œ the samples should not match whatever we have found in the training setã€‚

Now there's two different types there's like implic densities and there's going to be explicit densities and the difference between those is in explicit density you're going to have eventually a loss function that explicitly models the density you're doing we'll later talk about auto aggressive networks you have seen variational auto encoders these kind of things fall into the categories of explicit densities because here you have a loss function that explicitly models itã€‚

ğŸ˜Šï¼ŒAt the same timeï¼Œ there's other alternative of networks such as implicit densitiesã€‚å—¯ã€‚

Generative adversarial networks is the main thing we will talk about mostly today that falls into that category here we are still learning a densityã€‚

 but it is not explicitly modeled by a loss function and you know there are certain implications if you do that you have certain features but also certain drawbacks if you having an implicit density Yeah I would first like to give you a bit of an overview of the generative models of the generative and neural networks we're going to talk about andã€‚

First of allï¼Œ they're going to talk about about Gs because you knowã€‚

 most of you might have heard about themï¼Œ but I still want to talk about it because they're just so relevant todayã€‚

 rightï¼Œ There are so many methods and so many differentã€‚

Re works around GNs that in one way or another build on on generative around networks and we will also talk about conditional GNsã€‚

 meaning that at some point we have learned some sort of implicit density with G right at some point you want to control the generation process if youre talking about a plan G you don't have a lot of control but we will talk about how can we actually make GNs useful for certain applications how can we force certain styles of images to be createdã€‚

 how can we force for instanceï¼Œ a video with certain animations to be created and so on And I this party is pretty excitingã€‚

 specifically for you know con creation applicationsï¼Œ artistic applications and so onã€‚

 So when you want to make GNs practical this is very interesting because here you can basically give it the right control depending on what what the underlying task is going to be we will also talk about autoaggressive networksã€‚

 mainly because I think theyã€‚Really very interesting because these are the dominant generative models right now that actually have explicit densitiesã€‚

They might not be as popular right now as Gsï¼Œ but I thinkã€‚You know that comes in Coã€‚

 there's like certain research trendsï¼Œ I think auto aggressiveive networks are pretty important for various reasons and not just in the visual domainã€‚

 mostly for audiosï¼Œ most of the state of the art networks actually are auto aggressiveive there at this pointã€‚

We will also talk about neural renderingã€‚ That's kind of an interesting thing that a lot of the research we' be doingã€‚

 for instanceï¼Œ in our groupã€‚ And the idea there is we want a coupleï¼Œ you knowï¼Œ traditionalã€‚ğŸ˜Šã€‚

Generation methods such as what we know from computer graphicsï¼Œ for instanceã€‚

 and want to combine it with generative neural networks I will go into a little bit of detail later on what I mean by thatã€‚

 but I think it is fair to say that you know if you're looking at some of the progress in the research community in the last like two or three years neural rendering has become a really big thing basicallyã€‚

You can now instead of having explicit 3D graphics methodsã€‚

 you can use neural networks in order to kind of generate cutting edge contentã€‚

 especially for video generation over capturing 3D environmentsã€‚

 this is currently one of the state of the arts so in a sense we have basically two theoretical blocks we want to go over the GNs and the autoive networks and then we have the conditionalganNs and the neural rendering techniques to make things a little bit more practicalã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_1.png)

Now I mentioned I wanted to start with G and Paï¼Œ as I saidã€‚

 they have the implicit densities and the idea isã€‚AndThen we can learn these from a given training dataset and then we can draw new samples from certain distributions and then we can these samples lie inside of the distributionã€‚

 but they are new samples that are not identical with any of the training samples right so for instance if you have a dataset set of facesã€‚

 you can basically generate new facesï¼Œ they still are faces but you can generate new variations of the different people in these spaceã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_3.png)

Good yeahï¼Œ geneative aversarial networksï¼Œ I will probably always refer to the asksgs in the next few slidesã€‚

 they have reached a lot of popularity actuallyã€‚ğŸ˜Šã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_5.png)

And one one thing to measure popularity is if you're looking at the number of research papers that have appearedã€‚

 this is not quite up to dateï¼Œ this graph it goes until you know early of 2019ã€‚

 but you can see here is the the time period of papers rightã€‚

 and here's the number of papers that have appeared at the respectiveã€‚Public menu in this caseã€‚

 I think they from archiveã€‚ you can seeï¼Œ you knowï¼Œ like 2014ï¼Œ you had barely any again papersã€‚

 And now you have per monthï¼Œ you have almost 500 papers rightï¼Œ So this is kind of a veryã€‚

 this is the very rapid growth rates of papers that have againã€‚

InIn the title and this is pretty impressiveï¼Œ I think so you can see there's a lot of people looking into itã€‚

Yeahï¼Œ the reason why I think it's so interesting is because it kind of showed different ways now how to generate images compared to standard pipelines and it opened up a lot of possibilities for practical reasons and we'll talk about this in this lecture and I hope at the end of the day you know we can all use GNs in various geneative models to do kind of fun stuffã€‚

Now I want to quickly go over the basics againã€‚ I know some of the things we have already covered in the introduction to deep learning courseã€‚

 but just for completenessï¼Œ not everybody might have heard it in the last semester so I still want to quickly do a small recap hereã€‚

 We have talked about convolutions and decocomvolutionsã€‚

 I understand everybody knows what a convolutional network is right now right So we have here we have here some input image right we're gonna here have the output and we have this three by three conf kernel that is that is being shifted over the input image and in this case we have four valid locationsã€‚

 there's no padding no stridesã€‚ So this four by four image we're gonna generate a two by two feature map with this conf kernel Now the transpose convolutions are essentially the opposite right So here we have the input here is this grid hereã€‚

 we have basically two by twoã€‚ğŸ˜Šã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_7.png)

Belellet pixels here in the input and what we want to do is we want to make out of two by twoã€‚

 we want to make four by four pixels as the output Yeah in the way you do this is kind of this learn little up samplingling in this case again we have no padding and no straight in this case we have yeah we have these four feature locations but basically we just expand the values right and then we run this conf kernel over it same thing as convolution except now we have a transpose kernel and this way we can basically have instead of four instead of going down the resolution we can go up in the resolutionã€‚

Nowï¼Œ using the theã€‚Wellï¼Œ I should say one thing right obviously in the literature decomvolutions are often renamed various different waysã€‚

 We have seen them as up convolutionsï¼Œ transpose convolutionsï¼Œ decomvolutionsï¼Œ same thingã€‚

 different people call them differentlyï¼Œ I don't have a strong preference most of the time I would just say it's a decomvolutionã€‚

 but that's just because I got used to itï¼Œ but it's not like and the other terms are necessarily wrongã€‚

Yeah with these building blocks with convolutions and decocomvolutions you can dev architectures such as auto encoders and the idea of an auto encoder is we have a series of convolutions that go from a higher resolution input to a lower resolution feature map and this feature map here in the middle is called the bottleneck layerã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_9.png)

It's doing what it says right it's a bottleneck basically it's compressing the content to a smaller dimensionality of the features hereã€‚

 meaning that you have to have to learn some sort of encoding of the image and then what you do is you have a decoder this is composed of a series of decovolutions so you have this learnable upsemblingã€‚

And we go here from this bottomne layer again to the original resolutionã€‚

And the idea is that we have these convolutionsï¼Œ we have these decoconvolutionsã€‚

 this can be a fully convolutional architecture rightã€‚

 sometimes you have a couple of fully connected layers here in the middleã€‚

 but often you also have you don't have that you just have a fully fully convolutional architecture andã€‚

å—¯ã€‚Yeah with that you can design this architecture now right the question is how do we train that Wellã€‚

 the most common thing for architecture for auto encoders is were going to have a reconstruction loss So what we're doing here is we have some sort of input image on the lefthand side we' feeding this into our encoder into the bottleneck layer being compressed down to a lower resolution feature map we have a decoder that goes up to the original resolutionã€‚

 we have an output image and what we do is we have a reconstruction loss that says oh this output image should be similar to the input image you can use an L1 or L2 loss hereã€‚

It's a very simple regression loss basically and and the network is being optimized such that it wants to do this reconstruction Yeah practically what you have is this latent space Z hereã€‚

 this is what we get in the bottleneck this dimensionality is typically significantly lower than the input right so of course the spatial resolution gets smaller the feature resolution gets higher but the total dimensionality of the latent space Z is supposed to be significantly lower rightï¼Ÿ

ğŸ˜Šã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_11.png)

And yeahï¼Œ I mean if you do this and train this you have here a bunch of input images right and you're training this and you're getting a series of reconstructive images there's also the distinction right now if you're saying you have a training in a validation set right if you're taking a training set essentially what this network here is learning it's kind of learning a lowering approximation of the input it's actually very similar what a PC would do except that this is not a linear but a nonlinearar model but you have the same idea that you have kind of this encoding of a higherdiional input in this case an image right into a lower dimensionality in the bottleneck layerã€‚

And yeah and then we can do the reconstruction by having a loss at the end of the day It's a self-supervised learningã€‚

 a lot of people call it unsupervised learningï¼Œ I like to call it selfsupervised because you still have a loss hereã€‚

 but the nice thing is to train an auto encoder you don't need any additional annotations or labels right it's completely self-supvised you're gonna to feed in an image you're going reconstruct the image again so all you have to do is you have to have a data set of images in this case to train itã€‚

ğŸ˜Šï¼ŒYeah now we can argue what are we learning and the hopeã€‚

 or at least what would we assume what we learn is we learn kind of a compression networkï¼ŸğŸ˜Šï¼ŒSo weã€‚

 be compressing this higher level input into a lower levelã€‚

Feature map and then via going back to the original resolutionã€‚

's in a sense it's like learning some sort of compressionã€‚

Now it's not perfect it's not lossless compression as we can already see here right we have here the input images and we hear the reconstructed image you see these images are a little bit blurry that is based on the fact that well depending on how we do the optimization it's based on what kind of loss we're going to use we will very quickly see that the loss two loss might not be the ideal one because essentially learning some sort of mean average over the training set and so onã€‚

ğŸ˜Šï¼ŒBut I'm sure you've heard of auto codes in the context a lot for unsupervised learningã€‚

 you can use clustering in this feature spaceï¼Œ you can find dominant accessã€‚

 similar to principal component axis right this is why I'm making the comparison to PCA hereã€‚

 you can kind of find clustering in your dataset setã€‚

But now we wanted to talk about generative modelsã€‚ The question isã€‚

 how is an auto encoder useful hereï¼Œ Wellï¼Œ the idea is what we can do is we can train this auto encoderã€‚

At training time with the training set and then at test time what we're doing is we're simply chopping off the encoder part and now what we do is we just have a decoder so this decoder was pretrained in this case and at test time what we do is we just feed in random vectors here in the latent layer and then we're hoping we're getting an original image as output againã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_13.png)

And assuming we've done a good job atï¼Œ you knowï¼Œ learning thisã€‚

 if I going back quickly at learning this latent space here from the inputã€‚

 we assume that we have a reasonableã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_15.png)

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_16.png)

But we have a reasonable decoder that can kind of figure out how to make a real image out of this one again so you can kind of reconstruct from this compressed representation a real image right So the idea why I mentioned compression here is a good one because if this is compressed then I can kind of have the space of all valid real images of a certain domain in here encode itã€‚

 this is a very dense space compared to the possible output space and if that is the case right you would assume that whatever you're feeling in here you're gonna get a reasonable images outputã€‚

ğŸ˜Šã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_18.png)

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_19.png)

NowOf course in practice it's not going to be so simpleã€‚

 of course the space is still pretty high dimensionally in the lant space and you can still get a lot of garbages outputã€‚

 but that's kind of the high level intuition how you can assume this rightï¼ŸğŸ˜Šï¼ŒOkayï¼Œ but anywayã€‚

 so test time what we do is we just feed in a random vector here so and we want to do reconstruction from this random vectorã€‚

And then we're going to getã€‚You going get an output imageã€‚And what you can do also is you can sayã€‚

 wellï¼Œ you pretrain the auto encodederï¼Œ but you can also go ahead and sayï¼Œ wellï¼Œ you knowã€‚

 instead of pretraining itï¼Œ I'm just going train it by fitting in random vectors here to begin with and then you're reconstructing the output image so you can also train directly on this one so you don't have to pretrain on the auto encoder itselfã€‚

 but you can also just random vector give me some outputã€‚And then you can also go ahead and sayï¼Œ ohã€‚

 because I trained on thisï¼Œ I can also add some sort of semantic meaning into this hereï¼Œ rightã€‚

 depending on what kind of training that you haveã€‚ so you can sayï¼Œ ohï¼Œ if my firstã€‚

My first component of my latent vector is going to be having high value that corresponds to certain semantic correlationã€‚

 what we see later on in the output image in this caseã€‚Yeahï¼Œ in this caseã€‚

 what you will assume is thatï¼Œ you knowï¼Œ the network just learns the mapping from these from the latent space to the respective outputã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_21.png)

And this is one of the very first worksï¼Œ this is actually paper from Alex Zwkyã€‚

 he was in Frebrook at the time and you can train a network like this and then you can go ahead and sayã€‚

 oh I'm going to interpolate between my latent space I can go ahead and sayã€‚

Oh I have a latent space of this image I have a latent space of that image and I just want to do a linear interpolation in this latent code that I've just optimized for and then I'm getting intermediate results that show kind of a natural transition between the two imagesã€‚

ğŸ˜Šï¼ŒAnd these guys have been doing this on wellï¼Œ these guys have been doing this on on single shapesã€‚

 So it was a veryï¼Œ very early paperã€‚ you see this 2014 is the publication date so they could basically interlateã€‚

ğŸ˜Šï¼ŒBetween these two channelsï¼Œ this is the sourceã€‚ This is one image that they haveã€‚

 They compute a lant code for thatã€‚ and they have another one hereã€‚ This is the targetã€‚

 And then they simply interpolate in the lant codeã€‚ and this likeã€‚

Gradually transitions between these chairsã€‚ And the nice thing is now we have our first gene modelã€‚

 meaning that we can nowï¼Œ based on this la codeï¼Œ we can control or we can generate new images hereã€‚

Very good Yeah in practice you can also change view points hereã€‚

 you can morph between the chairs right and you have some certain control of the appearanceã€‚

 You see this is not very sharpã€‚ This is not an artifact right now in the video this is actually a little bit lower resolutionã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_23.png)

but againï¼Œ this is a paper from like six years agoã€‚

 but I wanted to highlight this was one of the first papers that actually had this explicit control now from a latent code to a respective targetã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_25.png)

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_26.png)

Againï¼Œ this is still the same ideaã€‚ We still have this simple yeah decoder from an own encoder as a generic model right nowã€‚

 rightï¼Œ very straightforward andã€‚We're getting these kind of resultsã€‚Nowã€‚

 one thing is what we notice very quicklyã€‚ what I mentioned is you knowã€‚

 the image outputs were relatively blurry and one reason why that happens is because of our lossã€‚

 in this caseï¼Œ rightï¼Œ we we have this L2 loss that I mentioned That's what a lot of people doã€‚

 you can useï¼Œ of courseï¼Œ an L1 or hoba loss2ã€‚ But in practiceã€‚

 you have one of these losses and if we having an L1 loss the optimum is kind of the mean rightã€‚

 like you always tend to blur because if you have one outlierã€‚

 you have like a sharp edge that gives you an outlier and the lossã€‚

 the net will favor like smooth transitionsã€‚ğŸ˜Šï¼ŒOver sharp edgesã€‚

 it's just naturally when you're using an L2 doseã€‚That's by definitionï¼Œ what L Toos is doingã€‚

So it's not going to favor like outliersï¼Œ but in this case we care about these outliers because this is where we get sharp edges that make an image look realisticã€‚

 right and less blurringã€‚Soã€‚ğŸ˜Šï¼ŒThe idea now isã€‚The problem is we want to replace this L2 loss and well ideally what I would like to do is I would like to go ahead and tell you what is a good loss function that tells me is it a realistic image or not right like how do I do thatã€‚

 Wellï¼Œ I'm going to look at the image and I just know it immediately because I have a good sense of what makes a real image withs a synthetic image right But if you're telling this a loss functionã€‚

 this is more difficult rightã€‚ğŸ˜Šï¼ŒAnd the core idea now isã€‚

 instead of using like a fixed function hereï¼Œ what we would like to do is we like to learn that functionã€‚

 So in other wordsï¼Œ weï¼Œ in other wordsï¼Œ we want to haveã€‚ğŸ˜Šï¼ŒA function that tells me how realisticã€‚

Is the imageã€‚Or maybe not even realisticã€‚ So how much does it match a certain distributionï¼Ÿ

 So if I have a training distributionï¼Œ I would like to train a loss that tells meï¼Œ ohï¼Œ like howï¼Œ howã€‚

 how good or how well does it match my distributionã€‚And this is the core idea of Gsã€‚

 This is the core idea of a generative everyellerial networkã€‚

 So here what we're doing is well this part here is our decoder of an auto encodeder basically its the decoder part and we have in here our latent vector which we had before we have here our generator which is nothing else but a decoder network right it goes from a lower dimensional feature vector in this case it's a random variableã€‚

 but it could be a low dimensional feature vector2 it goes to an image in this caseã€‚

 that's our sample so the generated G takes us inputã€‚

 the random variable Z that's latent space code and then it generates an image output and now instead of having an L2 loss hereã€‚

 what we do is we feeding we trying to learn is this a real image or not and since we have no clue of how to define this loss function which is use another neural network in this case our discriminatorã€‚

 this is a second neural networkã€‚ğŸ˜Šï¼ŒThere is a binary classifier that tells us is this image that we just generate with Gã€‚

 is this real or is it fakeï¼ŸAnd this is our Dã€‚ This is our discriminatorã€‚Nowã€‚

 how does the discriminator know whether this is real or not he needs to have some comparison to a real imageã€‚

So what we're doing is we have a binary classification task and we're feeding in real samples and fake samplesã€‚

ğŸ˜Šï¼ŒAnd the discriminator tries to distinguish whether this one or whether it tries to figure out for a given sampleã€‚

 is it real or is it fakeï¼Œ so what we do is we randomly feed in either real or either fake ones and the discriminator is being trained is it true or is it not true with a simple binary cross entropy lawsã€‚

And the idea isï¼Œ of courseï¼Œ I'm not going to feed always this one nameã€‚

 So I'm just going to randomly feed in fake and real samplesã€‚ rightã€‚

 So the discriminator has to figure out based on the conduct of the imageã€‚ Is it real or is it fakeã€‚

And the idea of why we do have the discriminatorï¼Œ thoughã€‚

 is not that we have a discriminator at the endã€‚ The idea is that we train both generate and discriminator at the same timeã€‚

 and the goal of the generator now is to make sure that itfuls this kind of learned loss function from the discriminatorã€‚

And the loan loss function means I want to trick the discriminatorã€‚ I want to make sure thatã€‚

This guy here can generate images that this guy here can distinguish anymore from these onesã€‚

If I had a perfect discriminatorã€‚Andã€‚I knew exactlyï¼Œ ohï¼Œ that's a real I notã€‚

I can generate an image like thatã€‚That this guy can't tell apart any more than by definition has to be a real imageã€‚

Now it's a little bit more tricky than thatï¼Œ rightã€‚

 And the reason why it's a bit more tricky is becauseï¼Œ wellï¼Œ if this is a fixed networkã€‚

Then you're going to have this problem that if I'm just training a generatorã€‚

 you will just generate some artificial noise call this adversarial noise that will be generated eventually that will trick this networkã€‚

So you have to train these two things at the same timeã€‚

 you have to train a generator and a discriminator at the same timeã€‚And eventuallyã€‚

 the discriminator will give you gradients that help to make the generation betterã€‚

 That's the core idea of Gsã€‚ This is like this is why it's an esarial networkã€‚

 least the disc generator and the discriminator they fight each otherï¼Œ rightã€‚

 They try to trick each otherã€‚ The generator tries to generate samples that look realã€‚

That the discriminator can't tell apart from the real images anymoreã€‚

 and the discriminator tries to tell apartã€‚Whether this is a real element everyoneã€‚

Okay typically for the notation here we typically call the latent random variable here is Zã€‚

 this is just the random vectorï¼Œ typically sample from a Gaussianã€‚

 or could be a uniform distribution2ï¼Œ mostly it's a Gasianï¼Œ we have a generated Gã€‚

 we have G of z which is our generated sampleï¼Œ we have x which is our real world imageã€‚

 real world sampleï¼Œ again doesn't have to be an imageï¼Œ but we can mostly do it for images right nowã€‚

We have discriminationriminity and T of xã€‚ğŸ˜Šï¼ŒShould you know it' the real sample that the discriminator should sayã€‚

 hey it's real and D G or Z is the discriminator is now evaluating the generated sample and well hopefully we discriminate it's going to say it's a fake image rightï¼Ÿ



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_28.png)

ï¼ŒOkayã€‚And yeahï¼Œ if you're looking at these two thingsã€‚ğŸ˜Šã€‚

We have real data on one hand side and fake data on the other hand side and the idea is but by this you know competing processã€‚

 but this adversarial process hereï¼Œ we eventually getting good fake imagesã€‚

 we eventually getting good images that the generator can do and the idea is that this then lies essentially what we're hoping that we can this is a certain distribution of real imagesã€‚

 we hope that the generator will also generate certain images that lie inside of the distributionã€‚

 that's also why we call it an implicit distributionã€‚

 what we're learning here because we have no explicit loss to match the distribution hereã€‚

 we just hoping by training a generator and discriminator at the same timeã€‚

 we hope that were learning this distribution implicitlyã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_30.png)

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_31.png)

Okayï¼Œ yeahï¼Œ but if you're looking at the at the two discriminator and generator processesã€‚

 if you're taking a real sample hereï¼Œ rightï¼Œ we're putting a real sample here as inputã€‚

To our discriminator T D and D of x tries to be near1ï¼Œ rightï¼Œ It tries to sayï¼Œ heyã€‚

 this is a real imageï¼Œ I try to recognize that this one is a real imageã€‚For the fake imagesã€‚

 I'm going to start with some random noise and out of this random noise or random variable random lant codeã€‚

 I would like to generate an imageï¼Œ and this is what my generator G is doingã€‚

So it generates an imageï¼Œ which is it's a sample from our modelã€‚Rightã€‚å—¯ã€‚

This one be feeding it into a discriminatorï¼Œ and the discriminator tries to make sure thatã€‚

D of the generated sample is near  zeroã€‚And she tries to make sure that this one gets near oneã€‚

 right This monthï¼Œ the generator tries to make sureï¼Œ ohï¼Œ I'm fooling my discriminatorã€‚

 I'm trying to make sure that I can generate something that youã€‚

 you can't tell apart anymore from the real worldã€‚And the discriminator tries to make sure I I'm going to nail you downã€‚

 I'm going to know which one is real and which one is fakeï¼Œ rightï¼ŸAnd these two things run in perilã€‚

 right so I'm going to feed in real imagesã€‚ğŸ˜Šï¼ŒTraing the discriminatorã€‚

And I'm going to train feeding random noise variables to train both discriminate and generated at the same timeã€‚

Nowï¼Œ if you're looking at the loss functionsï¼Œ we have here a discriminator loss and we have a generator lossã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_33.png)

If you're talking about the discriminator I mentioned this beforeã€‚

 this is nothing else but a binary cross entropy loss right so if we are feeding in so in this case we are here here the real sample X rightã€‚

 this is our positive labelï¼Œ this is our one label and then we have here our generated sample this is our negative label where the discriminator should say oh this one should be wrongã€‚

ğŸ˜Šï¼ŒThis one should be 1 and this one should be0ã€‚And the generator is just trying to fool the discriminatorã€‚

 rightï¼Œ we're just going to go ahead and sayï¼Œ ohï¼Œ yeahã€‚Just pleaseã€‚

 please maximize the loss basically of the discriminatorã€‚Okayã€‚

 and we're optimizing for these two things at the same timeï¼Œ so we're playing a little gameã€‚

 were having this miniax gameï¼Œ so G minimizes the probability that D is correctã€‚ğŸ˜Šï¼ŒRightã€‚

 we want to make sure that ourã€‚Hereï¼Œ our generator tries to fool the discriminatorã€‚

And equilibrium is essentially the saddle point right there's no con in this caseã€‚

 like theres there's nothing like it says oh now now I fool the discriminatorã€‚

 but then I can train the discriminator again and it's going to say oh no I know it again and these two things they mutually get better by helping each other to get betterã€‚

ğŸ˜Šï¼ŒAnd I mentioned this motivation before where we were sayingï¼Œ ohã€‚

 we have kind of this learnable loss function that would desã€‚ğŸ˜Šã€‚

D is essentially a learnable loss functionã€‚ if youï¼Œ if you will want to say soã€‚

 And D then provides the loss function or this learnable loss functionã€‚

 This discriminator provides the supervision for cheap because were training these at the same timeã€‚

 And because of thatã€‚ğŸ˜Šï¼Œwe're getting the supervision for the generatorã€‚

Now one thing what people have noticedï¼Œ this was the very first formulation of the Gs here that Goodfeelow provided in this very popular 2014 paperã€‚

This idea of like saddle points and equilibriums is not necessarily something that was introduced in the G papersã€‚

 but for this specific contextï¼Œ it was andã€‚The idea now is basically that were having this discriminator hereã€‚

 one problem what might happen is that the generator hereã€‚

 yeah this might be still a bit of a problem because the generator might always be wrong so basically if you're never gonna get a a good real sample sorry a good fake sample generatorã€‚

 then this loss from the generator will always be high and this loss will always be low in you the problem is there's no like smooth function basically that tells the generator how to get betterã€‚

And oftentimes what people do is instead of having this like just the negative discriminatorã€‚

 what people do is they use this heuristic methodã€‚ If I'm going backã€‚

 this one is still a binary cross entropy for the discriminatorã€‚

 but for the generator now we have the negative lock likelihoodã€‚

the discriminator of the generator is of the generated images is rightã€‚

 and this is a little bit different in this case we maximize the lock probability of D being mistakestaten instead of just taking the negative loss function of bothã€‚

The idea behind this heuristic method is that she can still learnã€‚

Even when the discriminator checks all the samples rightã€‚

 and that's often the case because in principlely going to have this issueã€‚

 that generation is a lot harder than discriminationã€‚If I have one wrong pixel in principleã€‚

 I know it's a fake imageã€‚I mean it's not so easyï¼Œ it has to still learn itã€‚

 but generation is in principle harder than sorry discrimination is easier than generationã€‚ğŸ˜Šã€‚

And this one on this five we have typically this negative local likelihood and I can already say like this is the standard G formulation that most people use today again you're going to go into a lot of different formulations laterã€‚

 but this is one of the formulations that a lot of people are usingã€‚ğŸ˜Šï¼ŒYeahã€‚

 so the idea is that she can still learn even when checks all samples because this log loss will not be like so extremeã€‚

 this log likelihood that makes it easierã€‚Okay in practice when you're training this you just have both losses right and you just train them at the same time now if you do this as I already mentioned what often happens is the discriminator will just always dominate the generatorã€‚

And often you do an alternating gradient updateï¼Œ meaning that you fix the generatorã€‚

 you do an update to the discriminatorï¼Œ so you only train the discriminator for one stepã€‚Andã€‚

The next step would be now with fixing the discriminator and we're performing one gradient step to the generator and you just alternate right you do this and that and this and that and this and that and this and that and this and thatã€‚

This is easier for certain thingsã€‚ The one thing is basicallyã€‚

 we only have gradients for half of the networkã€‚ So we either need to train generator discriminatorã€‚

 We can have bigger networksã€‚ That's one advantageã€‚

 but it's also a little bit more stable because you don't have so many unknowns Youre training at the same timeã€‚

 So often this alternating updateã€‚Has been being usedï¼Œ I'm saying has because nowadaysã€‚

 I think people mostly train it actually jointlyï¼Œ they figured out how to stabilize it with various tricksã€‚

 rightï¼ŸBut one thing you can do in these alternating update stepsï¼Œ you can sayï¼Œ wellã€‚

 if my discriminator is too strongï¼Œ I can simply train my generator hereã€‚

I can train the generator more often them so I can do one step hereï¼Œ I can let's say five steps hereã€‚

 one stepï¼Œ five stepsï¼Œ one stepï¼Œ five stepsã€‚Or I can also do itã€‚

Adaptively I can check the loss function until my discriminator loss goes down to a certain level I'm going to do these updates Now I fool the generator now I'm going to go ahead and go to the generator againã€‚

 going to train the generator until my generator loss goes downã€‚ğŸ˜Šã€‚

And this way you can balance the two lossesï¼Œ so you' you don't collapse and I'm mentioning this collapsell for now a couple of timesã€‚

 Collapsing means typically when the generator is always wrong and the discriminator is always rightã€‚

 which is a problem when you don't generate anythingã€‚

 you basically have a teacher that tells you whatever you're doing is wrongã€‚

 but it doesn't tell you howã€‚Okayï¼Œ so this is the vanilla againã€‚

So what you do is right for number of training iterationsï¼Œ what you doã€‚

 you sample a mini batch of M noise samplesã€‚ This is our latent codeã€‚

 So we have our random variable Zã€‚ We sample ourï¼Œ our real images X from the training dataã€‚

And then what we do is we update the discriminator a couple of timesï¼Œ rightï¼ŸAndã€‚In this caseã€‚

 we have this k loop here that goes over this one hereï¼Œ right andã€‚What we do is we simply trainã€‚

 in this caseï¼Œ the binary cross entropy laws of the discriminatorã€‚

Now you can argue that you're still updating both weights hereï¼Œ you could also sayï¼Œ ohã€‚

 I'm only going to update the discriminatorsï¼Œ that's what I'm saying like sometimes you can split this apart depending on how it goesã€‚

The second part then is hereï¼Œ now you sample the mini batch of the noise samplesã€‚

And then you're only train the generator partner in this case hereã€‚

 we actually train the discrimator a couple of timesï¼Œ we only train the generator onceã€‚Yeahã€‚

 I should say this might not always be a good ideaã€‚

 it depends a little bit on how these noise functions behaveã€‚How these training functions behaveã€‚

 you can see in the losses if your training loss goes wild and wild in this case means if my discriminator goes to0 Im pretty I did something wrong a very good deep I can understand at the beginning of training again is train this function and check that it goes down to0 if the discriminator can go down to0 without out traininging the generator then something must be fishyã€‚

That's just a simple debu step because we will see training these scans is actually veryã€‚

 very trickyã€‚ğŸ˜Šï¼ŒOkayï¼Œ but anywayï¼Œ what I'm trying to say is in the vanilla again versionï¼Œ you canã€‚

 you can decideã€‚Like there's various versions how to alternate between these thingsã€‚

 and typically what people do is they look at the loss functionsã€‚

 they want to like in the training schedule to check already like how how strong are we updateã€‚Okayã€‚

 if you're looking at a training processï¼Œ it looks something like thisï¼Œ this strain on Msã€‚

 so we hopefully generate some real numbersã€‚For Msï¼Œ this problem isã€‚Relatively easyã€‚Rightã€‚

 meaning that we have a small data set right and hopefully after a couple of minimax gamesã€‚

 we hopefully we hopefully get something reasonable againï¼Œ we're not converging hereã€‚

 we are still in this equilibrium state where the generate and the discriminator they balance each other outã€‚

ğŸ˜Šï¼Œå—¯ã€‚Yeahï¼Œ let's have a look at the loss curves and that is actually something that is very interestingã€‚

ğŸ˜Šã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_35.png)

In this oh yeahï¼Œ one thing I should say hereï¼Œ like whenever you're visualizing the GNsã€‚

 what you do is like in your1 support frameworkï¼Œ for instanceï¼Œ what you're going to do is you haveã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_37.png)

This one is a fixed lant sandï¼Œ so you have a couple of fixed random vectorsã€‚

Where you want to display stuff and for every iteration you're displaying always the same random variable hereã€‚

 same random variable hereï¼Œ same random variable hereã€‚

 so they will chiter around based on the training progressã€‚

 but in principle we should get sharper and sharper results and ideally in this case you should see the visual appearance of these numbersã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_39.png)

Okayï¼Œ and one thing that is very importantï¼Œ I'm looking at the loss functionsã€‚ğŸ˜Šã€‚

We see here two examplesï¼Œ we see a standard mini Maxax gameï¼Œ we see a heuristic methodï¼Œ againã€‚

 practice most people to use heuristic methods todayã€‚ğŸ˜Šï¼Œå—¯ã€‚

Train losses should look something like that or can look something like thatï¼Œ rightï¼Œ you have hereã€‚

I ignore the loss curves here right now this depends of course how you display itã€‚

 but I mean if the generator is like if going back to the loss function hereã€‚

 this is just the plane loss in this caseï¼Œ this is just negative discriminator that's why this loss is negative hereã€‚

 but let's ignore that for now basically like in this case the generator would like to go up here right sorry generator would like to go down here in this guy would like to go down hereã€‚

ğŸ˜Šï¼ŒOkayï¼Œ butã€‚What we see in this training process is that we see a relatively stable training right so these two losses they don't change too much and even in this case here have the heuristic methods well yeah they go a little bit apart but they don't fully go apart a little bit yesã€‚

 but not too much and after a few iterations you would expect that they're relatively stableã€‚

This is an interesting thingï¼Œ so now the problem is when you're looking at the loss curvesã€‚

 I don't know when I'm convergedï¼Œ I have no clue like this loss function doesn't tell me how far am I my training process yet or how good is the quality of my generated samples yetã€‚

ğŸ˜Šï¼ŒA good learning a good training curve for again is that it's relatively stable I will go into second also about bad learning curves sorry bad training curves and bad training curves would be that if this discriminator here goes to 0 it's still a 0ã€‚

6 here if this one went down to 0 then you have a problem because that would mean that the collapse and the discriminator is always right that's an issue I should say also one thing whenever the discriminator collapses to 0 there's no recovery once this happensã€‚

 then you can restart your trainingã€‚ğŸ˜Šï¼ŒThis often happens already at the very beginning which is in a sense good news for youã€‚

 you don't have to wait two daysï¼Œ you know immediately when you collapse and then you can already go ahead and change some parametersã€‚

 fix your box and so on and restart the processã€‚ğŸ˜Šï¼ŒYeah this is typically how these losses look likeã€‚

 these are some generated samplesã€‚ğŸ˜Šï¼ŒI meanï¼Œ on Msï¼Œ everything looks kind of O so there's no big difference between minim and heuristicsã€‚

 You could argue this one looks a little bit pro hereï¼Œ this one looks a bit Yeah that looks similarã€‚

 I guess there's not so much of a difference hereã€‚Okayã€‚

 these are very simple loss functions right now for the game that we useã€‚

 so we have a generating discriminator lossï¼Œ rightï¼Œ we trying to optimize them jointlyã€‚

 either with alternating or with joint training and so onã€‚ğŸ˜Šï¼ŒNowã€‚What we want to do now isï¼Œ of courseã€‚

 we want to generate something that is a bit more interesting than endlessnes imagesã€‚

 but we want to go to real imagesã€‚ and there was thisã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_41.png)

Well one of the first architectures here that people have proposed was a convolutional architecture it's this DC G paperã€‚

 deep convolutional G paperï¼Œ the idea is we're starting with 100 dimensional latent code Z we reshaping this to a four by four quote unquote feature map each feature is 1024 dimensional then we have one con fun con1 con Funcom this al deco convolutions this is al decoder right basically what's happening here is we're going reduce the feature dimensionã€‚

Right until we at a feature dimension of three for RGP for our image and we're going to make our spatial size bigger from 4 to 4 to 8 by8 to 16 times 16ã€‚

132 times 32 and 64 times 64ã€‚ so this generator here generates 64 squared imagesã€‚Andã€‚

We can apply this to all kinds of dataã€‚ So this again is one of the first architectures people have used there and it's still being used today So it's a reasonable architecture right it's basicallyã€‚

 wellï¼Œ what you're doing you' starting is a random vector right you're reshaping it to a4 by four feature map and then you're doing decocomutions until you end up with your desired image sizeã€‚

ğŸ˜Šï¼ŒOh yeahï¼Œ by the wayï¼Œ the bigger the image sizeï¼Œ the harder this gets right in order to get a big image coherently generatedã€‚

 you have to do a lot of things right with the generator and we will see that later how difficult it is to generate yeah bigger imagesã€‚

ğŸ˜Šï¼ŒOkayï¼Œ yeah here are some results in MNT again looks pretty reasonable here are some results on cellab A Cab A is one of the dataset sets youll probably use a couple of times in the conts of GNs these are basicallyã€‚

Portray photos of human faces where the face is relatively centeredã€‚

 It looks relatively straightforwardï¼Œ which makes the problem somewhat more tractableï¼Œ rightã€‚

 Basicallyï¼Œ since we are trying to implicit implicit distribution hereã€‚

 if if the distribution is smallerï¼Œ the problem becomes a lot a lot easierï¼Œ rightã€‚

 if I'm taking like this problem here of Mnesï¼Œ my distribution of these punch of like Mnes numbers is is pretty straightforwardã€‚

 So of courseï¼Œ it's an easy problemã€‚ Yeahï¼Œ faces are more complicatedã€‚

 But the problem is here for for cell of aï¼Œ it's a bit la because they're all centeredï¼Œ rightã€‚

 and there's all sorts of biasesã€‚ I'm notï¼Œ I'm not gonnaã€‚ğŸ˜Šã€‚

I'm not going to go into too much detail hereï¼Œ but it's not a realistic representation of the human population here right these are celebritiess data set and they are relatively small distribution of people basically okayã€‚

ğŸ˜Šï¼ŒBottom lineï¼Œ thoughï¼Œ is this is a good data set to start training against because it's a bit easierã€‚

 And in this caseï¼Œ we have about 200000 images we trainï¼Œ rightï¼Œ In this caseã€‚

 these are the results that you're getting with these againã€‚ Yeahï¼Œ looks O rightã€‚

 It's not too bad getting getting some some facesã€‚ They look O theyre not perfectã€‚ğŸ˜Šï¼ŒThatã€‚

But you can see already now we getting to to somewhat reasonably sized images right it's not like just simple image numbers anymoreã€‚

 we can do something with facesã€‚ Here's another result on the Asian F data setã€‚Againã€‚

 we get reasonably reasonable faces again I would like to highlight these also it's also a relatively easy data setã€‚

 againï¼Œ portrait mainly face centered makes it a lot easier for the networkã€‚

Here is an example when you sample through the laden codes this isã€‚

Oh no sorry this is actually the training process I think this is basically it starts hereã€‚

 it starts blurryï¼Œ this is always the same lighting code and then in theory it gets sharp and sharper Yeah again same problem we don't know when were converged all we can do basically for depoing is be looking at these generated imagesã€‚

ğŸ˜Šï¼Œå—¯ã€‚Yeahï¼Œ I wanted to talk a little bit about that one right nowã€‚

 I wanted to talk about how do we know this stuff is working and not working and what do we do if it doesn't work and I can tell you a fair warning at the beginning when you're trying this outã€‚

 a lot of things won't work because it's very tricky to train these gamessã€‚ğŸ˜Šã€‚

And we'll get into this in a second whyã€‚ It's so trickyã€‚ But let letã€‚

 let's look at the de parkinging steps what we can doã€‚ Wellã€‚

 what we can do is we can definitely do this hereã€‚ We can again set up tensor boardã€‚

 every iterationsï¼Œ we justã€‚ğŸ˜Šï¼ŒTake a fixed number of latent sample codes rightã€‚

 and we're looking up always the same set of latent codesã€‚And ideallyã€‚

 they're getting sharper and sharper and sharperã€‚Which I guess you can see here to some degreeã€‚

 but yeah I guess this gift here starts at the very beginning and it starts it's very pry and then it gets better and betterã€‚

Ifã€‚Yeah there's a few things we can later talk about one problem might be moreco one problem might be that all of these ones look the same eventually that's a problem we'll talk a little bit about that one that means wellma generatedã€‚

 generated one real image but it's always the same image that's also a bit of a problem sometimes you get end up with a bunch of them this is a common problem you can see very quickly here if you moret collapse and if you more collapse early on I mentioned this you can hardly recover from that during the trainã€‚

ğŸ˜Šï¼ŒWe can look at the lost curvesã€‚Yeahï¼Œ these are good loss curvesï¼Œ by the wayã€‚ good loss curvesã€‚

 meaning I have the discriminator loss hereã€‚ We have the generator lossã€‚

 It's a bit wild at the beginning at the very beginningï¼Œ rightï¼Œ it fluctuates a bitã€‚

 then have a little bitã€‚ you knowï¼Œ you so this one goes a little bit downã€‚

 this one goes a little bit upï¼Œ but eventually you knowï¼Œ they kind of stabilize hereã€‚ğŸ˜Šã€‚

Don't go to zeroï¼Œ none of themã€‚They don't go wellï¼Œ there's a little bit of fluctuation but in principle they are doing all fineã€‚

 that is considered to be a good training curve right because we kind of managing to hold this equilibriumã€‚

 at least from a law standpointã€‚ğŸ˜Šï¼ŒThat doesn't mean necessarily it worksã€‚

But if the loss was not stableï¼Œ then I know for sure it did not workã€‚

 So it it's one criterion what we can do here to look at itã€‚ Yeahï¼Œ when have we done hereã€‚

 we don't know we can train it for a long time In principle it's getting better and better and assuming my model capacity is good enough and I have enough training data and my distribution should also get better but I can tell youã€‚

 oh now I'm done with my training right For this one I have to go backã€‚

 look at the images I'll later go into a few details in how to evaluateã€‚

 is it a good image or not Yeah but I can already tell you it's difficultã€‚Yeahã€‚

 I can also show you some bad curvesã€‚ Ba training curves would look like thatã€‚ This is canonal curveã€‚

 a lot of people will get when they try guns out the first timeã€‚

 What's going to happen is this is all a nice discriminatorã€‚ rightï¼Œ This is all againï¼Œ train epochsã€‚

 And what happensï¼Œ This one goes straight down to0ã€‚ This one goes upã€‚

 It will still stay here and go a little bitã€‚ They will be still a bit of fluctuationã€‚ğŸ˜Šã€‚

But you can see this one collapsedï¼Œ this one went to zeroï¼Œ this one went upï¼Œ no recovery from hereã€‚

 restartï¼Œ fix your boxï¼Œ fix the training dataï¼Œ fix your schedule and stuff like that in this case something went terribly wrongã€‚

Again good training curves look like that I like showing them they look pretty right they start wild a little bit at the beginningã€‚

 but eventually they're going to they're on to balance each other out this is like what's considered to be a good curve again very important if you're looking at the discriminator discriminator should not go down to zero if discriminator goes point to zeroã€‚

 something went really really wrongã€‚ğŸ˜Šï¼ŒWellï¼Œ we have yet another oneï¼Œ by the wayã€‚

 I just just typed into Google good training curves for GNsï¼Œ actually no I didn't do thatã€‚

 I just typed in training curves for GNs and then I picked the good and the bad onesã€‚ğŸ˜Šï¼ŒOkayï¼Œ againã€‚

 good one goes a little bit well hereï¼Œ rightï¼Œ discriminator stabilizes goes a little bit wellierã€‚

 it's generated also stabilizes againï¼Œ very important that the stabilization happensã€‚ğŸ˜Šï¼ŒOkayï¼Œ yeahã€‚

 I mentioned the training schedules we've seen for discriminative networks and new training schedules like how to adapt the learning writing and stuff like thatã€‚

 We have the same thing hereã€‚ Nowï¼Œ the problem is we have to adapt both of themã€‚

 We have to train discriminator and generatorã€‚ I mentioned this in the alternating schedulesã€‚ğŸ˜Šã€‚

Basicallyï¼Œ what you do is you have these thresholds while loss discriminate gradient than thresholdã€‚

 train discriminatorï¼Œ while loss generate a greater than loss a threshold of generatorã€‚

 train generatorã€‚Againï¼Œ this is not always what's happening todayã€‚

 you can train them at the same time if you fit in your modelã€‚

 if your batch size can be large enough for whatever you need and so onã€‚Andã€‚Yeahï¼Œ there's a lot ofã€‚

 there's a lot of different thingsã€‚ How youã€‚How you can balance with these two things so this is basically the one thing that you have to play around with and it's you need to get a bit of experienceã€‚

 which setting to useã€‚ğŸ˜Šï¼ŒAnd the problem isï¼Œ of courseï¼Œ depending on your problem statementã€‚

 these settings will be completely differentã€‚ğŸ˜Šï¼ŒSoã€‚Yeahï¼Œ one thingã€‚

 I want to discuss a little bit about the strength of the discriminatorã€‚

So what is the strength of the discriator going to doï¼ŸAndã€‚Wellã€‚

 the whole point of again is that we have this equilibrium but these two things they balance each other out and none of them collapses because you could sayã€‚

 wellï¼Œ my discriminator is weakï¼Œ it's not a big problem right because my generator can now generate great imagesã€‚

But my problem isï¼Œ if my discriminator is too weakã€‚

 I'm getting not very good gradients to train my generator againã€‚

 there's no explicit loss on the generator aside from training the discriminator at the same timeã€‚

So if my discriminator is badã€‚Andã€‚I just say everything is kind of wrongã€‚

 or everything is kind of rightã€‚ I'm not getting a signal for my generator to generate better imagesã€‚

 so you cannot get better than the teacher if my discrimator is too weakï¼Œ I have a problemï¼Œ rightã€‚

 It has to beã€‚ it has to be strong enough to tell me what to do and tell me what to do means it has to discriminate and give me crs how I get better myselfã€‚

 rightã€‚ This is like how this process worksã€‚On the other handã€‚

 if my generator is too weak or if the discrimator is too strongï¼Œ rightï¼Œ I meanã€‚

 these are obviously synonymsã€‚ğŸ˜Šï¼ŒMy discriminator will always be rightã€‚

 if my discriminator is always rightï¼Œ that's also a problem because then the teacher is going to tell you you're wrongã€‚

 you're wrongï¼Œ you're wrongï¼Œ you're wrongï¼Œ and now even more wrongã€‚

 but it doesn't tell you a good gradient how to get thereã€‚ğŸ˜Šï¼ŒIn practiceã€‚

 you need good balance and good balance means roughly well discriminated generator should have a similar capacityã€‚

It depends a bit on the problemã€‚The first set of problems where you could sayï¼Œ ohã€‚

 sometimes the generally needs to be twice as bigï¼Œ but it's unlikely you could see something likeã€‚

 ohï¼Œ my discriminator is like 20 times as big as my discriminatorã€‚

So generator discriminator should roughly be in the same scaleã€‚

If you find that the discriminator gets too strong and this is something that will happenã€‚

 you have to figure it out with the right training schedule right you have to go back here and check how to do thatã€‚

 possibly how to have different regularization and so onï¼Œ but in practiceã€‚

You wanna have roughly the same capacityã€‚So that's veryï¼Œ very importantã€‚

The next point I want to mention is mode collapseï¼Œ I mentioned it alreadyã€‚

And mode collapse is the problemã€‚Whenã€‚We only generate a small number of good imagesã€‚

When the generator is generating a small number of good images and the discriminator saysï¼Œ wellã€‚

 they look like real images seems good to meã€‚But you don't have enough varietyã€‚

 So you need to have enough varietyã€‚And this is a problem that happens often very early in the training processã€‚

 and this is a real issueã€‚And there's this fundamental difference right nowã€‚

 how we are ordering our training if we have the discriminator in a loop or the generator in a loopã€‚

 rightï¼ŸIf my discriminator is in the inner loopï¼Œ so let's see what's happening hereã€‚

 right I have hereï¼Œ I maximizeã€‚The discriminatorï¼Œ this is my inner loop and minimize the generator and the outer loop right thereã€‚

 here's the other way around hereï¼Œ the outer loop is the discriminatorã€‚

 maximize that one and the generator in the inner loopã€‚å—¯ã€‚If my generator is in the inner loopã€‚

 which is this case hereï¼Œ it's very easy to converge to one sample because what I'm going to do isã€‚

I'm going to at the momentã€‚I have a given discriminatorã€‚

 and all I want to do is I want to fool that one discriminatorã€‚

 If I always generate the same right sample that the discriminator thinks is okayã€‚

 I will not get a very good distributionã€‚ So this will alwaysã€‚Not always one exampleã€‚

 but it will converge very likely to a small number of samplesã€‚

If you have the discriminator in the inner loopã€‚And the in the generator here on the outer loopã€‚

 then you should supposedlyã€‚Converge to the right distributionï¼Œ rightã€‚å—¯ã€‚And that's very importantã€‚

 rightï¼Œ That is a very important thingã€‚Because in this caseã€‚

I have to always be this guy will basically be trainedã€‚While this guy's being trainedï¼Œ rightï¼Ÿ

And this is what I mentioned for in this vanilla againã€‚

 This is why you have typically first the discriminated loopsï¼Œ which is that oneã€‚

 and then the generator loopsã€‚ Howeverï¼Œ you can still have different schedules where you can train the generator a couple of times hereã€‚

 tooï¼Œ right you can still do thatã€‚ This really dependsï¼Œ thoughã€‚

 but you have to be aware of this problem that it arisesã€‚

If you're training this guy now in the inner loopï¼Œ you will typically more collapseã€‚

 and that's a thing to considerã€‚Yeahï¼Œ I'm going to talk a little bit more about more collapseã€‚

 Another interesting thing is the data dimensionã€‚ğŸ˜Šï¼ŒWhat are you usingï¼ŸSo this is a graph hereã€‚

 this hereï¼Œ these different curves have anyfold dimensionsï¼Œ rightã€‚

 This one is the number of modes that we're trying to recoverã€‚

And this is the percentage what the actually did recoverï¼Œ rightï¼ŸSo if were having more modesã€‚It'sã€‚

 of courseï¼Œ harder to recover all of themã€‚Meaning that the recovery rate will typically go downã€‚Nowã€‚

What we're trying to measure here is we're trying to have the mode recovery versus the number of modesã€‚

 so the performanceã€‚Correoids with a number of nodesã€‚ rightï¼Œ This what I'm sayingã€‚

 If you have more modesï¼Œ it willï¼Œ it will be harderã€‚If my manifold dimension is going to be highã€‚

Right this 128ã€‚å—¯ã€‚Yeahï¼Œ then this is even more extremeã€‚If a manifold dimension is lowerã€‚

 this is a bit easierï¼Œ so the higher my dimensionï¼Œ the harderã€‚å—¯ã€‚Yeahã€‚

 so part of the reason why we often see GNs on specific domainsã€‚

Is because we want to simplify our distributionï¼Œ rightï¼Œ We simplify our manifoldï¼Œ basicallyã€‚

And because of thatï¼Œ I can most likely recoverã€‚Wellï¼Œ I wouldn't say all modesã€‚

 but I cover a lot of modesï¼Œ like you never recover all of the modesã€‚

And that's unlikely well this this is a toy exampleï¼Œ this is not a real image exampleï¼Œ rightã€‚

 but in practiceã€‚ğŸ˜Šï¼ŒIf you're having a simpler domainã€‚

 you can recover more modes because you can sample the full domain and then the training becomes easierã€‚

 if you're having a larger domainï¼Œ In other wordsï¼Œ what could happen is a generator picks a pretty good sampleã€‚

 looks like a real imageã€‚ğŸ˜Šï¼ŒIt matches most of the other stuffã€‚But then it turns the same imageã€‚

 againã€‚Rightï¼Œ then the generator and the discriminator can't tell it apartã€‚ So in practiceã€‚

 this is very important here that you're sayingã€‚å—¯ã€‚Number of modesã€‚Next more recovery rateã€‚

Lower because it gets harderã€‚And often what we see isï¼Œ wellï¼Œ we're seeing like smaller distributionsã€‚

 so we don't have to recover so much stuffã€‚This is why faces work pretty wellã€‚

 an image and that doesn't work so wellã€‚Let's have a look again at the manifoldã€‚

 well and we had this partially from the other oneã€‚

 but if you're looking at the mode recovery versus the manifold dimensionï¼ŒğŸ˜Šã€‚

A larger latent space meansã€‚More mode collapseï¼Œ which makes sense because it's harder for the discriminator to figure out this entire space at onceã€‚

RightIt can monitor itself to seeã€‚ So here what we're seeing is here we have the recovery rate againã€‚

And here we have the manyifold dimensionsï¼Œ rightï¼ŸWe have also the data dimensionã€‚

 like manyifold is like late vector Z dimension right and this is the data dimensionã€‚

 the data dimension doesn't seem to matter too much hereã€‚But basicallyã€‚

 it goes down the lo of the spaceã€‚ğŸ˜Šï¼ŒBut you also want to have enough control eventuallyã€‚

 so you want to have reasonably large spaceã€‚So this is something you have to also consider like how big you want to make that spaceã€‚

Yeahï¼Œ the reason why I talked about this mod collapseã€‚

 this is something you can and should figure out early on in your training processesï¼Œ againã€‚

 how you do itï¼Œ you just visualize a bunch of samples in Tensor board and then you see what's happeningã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_43.png)

I also want to mention some of the problems that GNs haveã€‚

And the funny thing is these images on the first glanceand look kind of cool and look like a real imageã€‚

We getting in full by this Na tooã€‚But what we seeã€‚They are not real imagesã€‚If you're looking closelyã€‚

 I don't know what's happening here or hereï¼Œ rightï¼Œ It's looked like a dogï¼Œ like a bit of talk hereã€‚

 bit of talk thereã€‚ looks like oneï¼Œ2ï¼Œ3 legs hereã€‚Something went really wrong in these imagesã€‚

 but on the other handï¼Œ they look kind of a reasonable color distributionã€‚

So the global structure is a big problem for GNSï¼Œ and it's not surprising that if I'm making my image dimension bigger and I have a larger image to generateã€‚

 this global structure will even be harder to be achievedã€‚In practice you have to train longerã€‚

 in practice you have to have bigger modelsï¼Œ but the bigger your modelï¼Œ the longer you have to trainã€‚

 the more likely to have things like mod collapse or your loss is divergingã€‚So that's an issueã€‚

 Turn speaking with a structureï¼Œ a one one common problem is you have often problems with countingã€‚

these are fun samplesï¼Œ you have a lot of ice sometimes here a lot of ice I thinkã€‚ğŸ˜Šã€‚

I don't know what's happening hereã€‚Oh here rightï¼Œ something went really wrong like locally the features are okay if I'm going to give you this little patch hereã€‚

 it looks perfectly fineï¼Œ but the global accountss are just wrongã€‚ğŸ˜Šï¼Œå—¯ã€‚Yeahã€‚

 one thing you have to also considerã€‚Like if theoretical alreadyically what we're doing Gs are perfectly sound rightã€‚

 We have this generatorã€‚And we're learning kind of this implicit distributionã€‚

Now the problem is implicit distribution is still being represented by a series of 2D deconvolutionsã€‚

 right you have a bunch of 2D operators that have to encode the whole space of images in the training setã€‚

And convolutions are inherently a local operatorã€‚ğŸ˜Šï¼ŒLocal means wellã€‚

 I mean I have a three per three confï¼Œ I can locally check what's going on in order to get a larger receptive field I have to have a series of convolutionsã€‚

And that's a thingã€‚That isã€‚Very much exhibited in these cases here where you the larger it getsã€‚

 the more global the structure has to beï¼Œ the harder it is for confident to do itã€‚å—¯ã€‚Yeahã€‚

 it's just how it goesã€‚Yeahï¼Œ okayã€‚ so these are common things but we see where we have problemsã€‚

I talked a little bit about what you can do when you train GNs and how to evaluate itã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_45.png)

The question isï¼Œ how do we quantitatively evaluate the performance of Gsã€‚ We already noticed thatã€‚

Yeahï¼Œ the loss function is not going to cut it for usï¼Œ rightã€‚

 The loss function is not going to be so easy to look atã€‚å—¯ã€‚

The main difficulty what we have with GNs is we just don't know how good we areã€‚

 we don't know whether they convergeï¼Œ we don't know whether they need to train longerã€‚

 we don't know whether it's not working right now and it will get better and so onï¼Œ but it is a veryã€‚

 very difficult problemã€‚And the research communityï¼Œ I meanï¼Œ I'm a researcherã€‚

 I can say that I'm part of this communityï¼Œ but one weakness of the community is often in papersã€‚

 you see veryï¼Œ very good resultsã€‚And what we can do is you can just sample a couple of times and take the good results and ch pick themã€‚

Andã€‚That is a problemï¼Œ rightï¼Œ because we would love to quantitatively evaluateã€‚

 but it's very difficultã€‚ So does it always look good or is only some of them being goodï¼Œ rightã€‚

 What are the good were the bad casesã€‚ And that's very hardã€‚ I can't put it so easily in a tableã€‚

The second question is do we just memorizeï¼Œ do we analyze rightï¼Œ I mentioned ohï¼Œ wellã€‚

 we have a training set in the real samplesï¼Œ do we just memorize a bunch of themã€‚

If were memorizing these ones from the training setï¼Œ my discriminatorï¼Œ by the wayã€‚

 if I'm perfectly memorizing itï¼Œ my discriminator can't tell them apartã€‚ There's no wayã€‚

 So the idea is my training set has to be big enough that my model capacity can't fit in itã€‚ğŸ˜Šã€‚

That's still the same thingã€‚ You can still have overfitting problems hereï¼Œ tooã€‚Yeahã€‚

 but it's difficultï¼Œ it's veryï¼Œ very difficultã€‚ğŸ˜Šï¼ŒSo what do we do in practice while in practiceã€‚

 one thing I told you we look at the imagesï¼Œ were looking at the human evaluationsã€‚

 every in update stepsï¼Œ you show a series of predictionsã€‚

 these predictions typically should be from the same length code right you have maybe you're showing like eight by8 imagesã€‚

 but they're always the same late and co the visualizingã€‚ğŸ˜Šã€‚

Check the training curves if you're reveringï¼Œ if discriminateator is collapsingã€‚Hereã€‚

 check if it's getting better or check if we have mode collapseã€‚ These are the things we can doã€‚å‘ƒã€‚

At the beginningã€‚It doesn't look very goodï¼Œ but you can check if you have enough variety if you visualize 8 by images and you only have like three different types of these 8 by8 imagesã€‚

 and you have a lot of duplicates in thereã€‚ even they don't look very good and you have a lot of duplicatesã€‚

It's a problem because you just more collapsedï¼Œ rightï¼ŸThis one you see very much early onã€‚

 And if this one goes wrong at the beginningï¼Œ it'll never recoverã€‚ that's just the pigishã€‚Yeahã€‚

 it doesn't look goodã€‚ What do we doï¼Œ We go back different hybrid parametersï¼Œ different networksã€‚

 different learningã€‚ most of the time the learning schedules make a big differenceã€‚That'sã€‚

 that's one of the number one things you can tryã€‚å—¯ã€‚Okayï¼Œ this is human evolutionï¼ŸThere's alsoã€‚

Some ideas of doing quantitative evolutionsã€‚And one of the most popular ones is called I Inception scoresã€‚

å—¯ã€‚I can already say there's a caveatï¼Œ the inceptance courseã€‚They are not perfectã€‚

They're not telling you 100% what's going onã€‚So the inception scores diminishes salliency and diversityã€‚

å—¯ã€‚You typicallyã€‚The idea is basically training a classifierã€‚Rightã€‚

 you train an image generation modelï¼Œ and then you check how accurate is the classifierã€‚

So can you kind of recognize the generated image that's kind of a high level ideaã€‚

 and you make some assumption of the underlying dataã€‚So what do we want to do isã€‚Let's sayã€‚

I'm gonna generate images from 10 different classesã€‚And I want to make sureã€‚That my classifier canã€‚

Associate each of the generated samplesã€‚Precisely to one classã€‚

So let's say I'm generating cats and dogsã€‚ If the classifier tells me it's a dogï¼Œ 100% dogï¼Œ I'm goodã€‚

 if it tells me it's 100% it's a catï¼Œ I'm goodã€‚If it's half dogï¼Œ half catï¼Œ then not so goodã€‚

 then I generated something weirdã€‚RightThat's kind of the ideaã€‚ And this is called salliencyã€‚

 This is the first term we have to considerã€‚è¯¶ã€‚Check whether the generating image can be classified with a high confidence and high con means you just look at the classification scoresã€‚

ğŸ˜Šï¼ŒReally simple idea works to some degree rightï¼Œ I mean of courseã€‚

 that's important we need however a second part which is telling us we need some diversityã€‚

It doesnn't help us if we only generate good dogsï¼Œ but no catsã€‚ It's not good to thingã€‚

 So we need to check that we get itã€‚A uniform distribution of all classesã€‚

 This is what how religion magic should tell usã€‚Yeahï¼Œ becauseã€‚

It's its we want to have both right we want to have diversityã€‚ You don' want to have staencyã€‚

 that's what the inceptionious course I'm measuringã€‚ There's still a problemã€‚And one problem isã€‚

 what happens if we have only one good image per classã€‚And if you have enough classesã€‚

And we have one image that is good per classï¼Œ but every other image per class is not so goodã€‚

 or we don't have a lot of other imagesã€‚ we might still be more collapsed and we might not notice it so well with the inception coursesã€‚

So even though inception scores might be highï¼Œ diversity might not perfectly be coveredï¼Œ rightï¼ŸSoã€‚

The short story here inception scores is what people use to quantitatively evaluate itã€‚

 There's a couple of variations of this like FIT scores and stuff like thatã€‚ğŸ˜Šã€‚

They're all very similarã€‚But I can already tell youï¼Œ they're not perfectã€‚

But none of them are perfectã€‚So we have to still look at itã€‚ This is a difficult problemï¼Œ rightã€‚

 It's basically likeï¼Œ ohï¼Œ look at a pictureã€‚ Tell me howï¼Œ how realistic is the pictureã€‚

 Like that's yeahï¼Œ that's the question we had at the very beginning when we saidï¼Œ wellã€‚

 we need to have a loss functionã€‚To reconstruct or generate imagesã€‚

 right That's what we're learning with againï¼Œ but the again is learning and we have to develop it again nowã€‚

 So it's very difficult to do this by handã€‚There's a few tricks you can doã€‚

 you can check the discriminatorã€‚ğŸ˜Šï¼ŒIf the discriminatorï¼Œ by the wayï¼Œ it's a good debug metricã€‚

 whenever you start training againï¼Œ all this trying to discriminator first check if it's going down to zeroã€‚

 Of courseï¼Œ you're not getting reasonable resultsï¼Œ but you're checking that the discriminator can go to zeroã€‚

 if that one doesn't hurttï¼Œ then you have somethingã€‚

But one idea what you can do is you can train a discriminator and again you can take the features of the discriminatorã€‚

 can put this into a classification network as a pretrained feature learnerã€‚

 find in a little bit and see if you get good features with thatã€‚If that is the caseã€‚

 then you would assume you have trained a good discriminator because you got got features out of itã€‚

If that happenedï¼Œ if you got these good features out of a discriminatorã€‚

You would assume the channel item must be also goodã€‚It's just how it goesã€‚ if the generatorã€‚Yeahã€‚

 the generator must be goodd in that caseã€‚å—¯ã€‚That's the thing I feel is a bit hackyã€‚

 but it's at least one way to figure outï¼Œ oh do we good features or notï¼ŸOkayã€‚

That's for now it's for how to evaluate Gsã€‚But in practiceã€‚

 I can already guarantee you it's very tricky to trainã€‚

 so I wanted to talk a little bit about what can we do to make Ginsburg in practiceï¼Ÿ

In practice there's three different things you can doï¼Œ you can fix hyper parametersï¼Œ rightã€‚

 training schedulesï¼Œ learning ratesï¼Œ possibly optimizersï¼Œ possibly optimizersï¼Œ possibly samplingã€‚

 possibly data distributionï¼Œ training setsï¼Œ stuff like thatã€‚

That's the most important thing we're going to look at There's a choice of loss functionsã€‚

 We have seen two loss functions so farï¼Œ we have seen the the standard miniax scan right we have seen the heuristic method we'll introduce a couple of more and then there's a choice of architectureã€‚

What decomvolutions do you useï¼Œ stuff like thatï¼Œ For the most partã€‚

 we only kind of look at this one to bebugã€‚This is the most reasonable thing to look at In practiceã€‚

 people always look at the other twoï¼Œ which I don't understand whyã€‚

 So they' changing loss one is all the time and then theyã€‚

Because nothing works change the loss functionï¼Œ that's always a thingã€‚

 but I think it's a terrible advice you should always look at these ones here firstã€‚In practiceã€‚

Starting with a heuristic method here is a good ideaï¼Œ architecture just started the DC gameã€‚

 that's why I mentioned it is also a good ideaã€‚They might not be the best onesã€‚

 they might be better onesï¼Œ but these things they work if you're choosing the right higher tournamentã€‚

Okayï¼Œ yeahï¼Œ let's have a look at a couple of practical things what people have been looking atã€‚

And there's this cool website called Cananhexï¼Œ some of stuff might be a bit older now older like two years oldã€‚

 maybe so but there's a couple of cool things on this Kanhex website so I wanted to quickly go over itã€‚

ğŸ˜Šï¼ŒThere's a few things you want to doï¼Œ normalize the inputsã€‚

 that's an no brainer because you want to make sure your distribution is within a reasonable spaceã€‚

 like if the parameter space of a distribution is badï¼Œ then it's very hard to learnã€‚å—¯ã€‚

What's very importantã€‚ And this is a channel thingã€‚ by the wayã€‚

 check out your last layer of your generatorã€‚ğŸ˜Šï¼ŒMost people useã€‚

 if you normalize it here between minus1 and 1ï¼Œ you have a 10 h that maps it between minus1 and 1ã€‚

So by definitionï¼Œ your generator will generate samples align in this distributionã€‚

Let's say for simplicityï¼Œ I'm going to go ahead and I'm going to use instead of a 10 Hã€‚

 I'm going to use a sigmoidã€‚A second one will map by values between0 and1ã€‚

But if my inputs are between -1 and 1ï¼Œ I think my discriminator has a pretty easy thing to sayï¼Œ wellã€‚

 if there's no negative numbersï¼Œ it's all fakeã€‚ rightï¼Œ So these are common problems what people haveã€‚

 Either of re lookï¼Œ Ohï¼Œ by the wayï¼Œ this is the most common bug that people haveã€‚

 They just in the last layer of the generatorã€‚They have a reloã€‚

 and this one is some weird normalization that goes negativeã€‚

So make sure these two things match each other if they don'tï¼Œ youve got a problem by the wayã€‚

 obviously you don't learn anythingï¼Œ your curse will just immediately collapseã€‚ğŸ˜Šã€‚

By the way try it out rightï¼Œ I meanï¼Œ just do something like this and use it relu here or sigmoidã€‚

I think your discriator should just go immediately to 0ï¼Œ rightã€‚ It's just straightforwardã€‚å—¯ã€‚Samplingã€‚

 what kind of samples do you useï¼Œ I mentioned beforeã€‚People can use a uniform distributionã€‚

 most people use a cautionus distribution to sample in the latent spaceã€‚What people often say isã€‚

Uses fertile sampling and fertile samples means it is a Gausutian sampleã€‚

 but it lies the on the sphereã€‚ So your length of the vector is always going to be oneã€‚

So you just normalize your sample spaceï¼Œ it makes it a little bit easierã€‚

 The reason is you just you still have the same control in terms of the dimensionalityã€‚

 but the number of the dimensionality sorry effective parameter space goes becomes a lot smallerã€‚

Okayï¼Œ and when you do interpolationsï¼Œ you can interpolateã€‚On on thisï¼Œ on this surgery sample hereã€‚

 you can sayï¼Œ ohï¼Œ you have to check on the great surgeryï¼Œ rightï¼Œ Likeã€‚

 how are you gonna play from one sample to anotherã€‚

There have a lot of literature by Hawaii way how to do the samplingã€‚

 you can normalize it to make sure it's on aphereï¼Œ you can also not do itã€‚

There's a lot of different thingsã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_47.png)

It depends a little bit on the problemã€‚å—¯ã€‚Bten norm is a fun thing This is a thing I would take with a bit of a caveat a lot of people say oh don't use batch norm anymore I would say depends but it it's one thing you can play around with at least sometimes your also just need a few optionsã€‚

ğŸ˜Šï¼ŒWithWithout Pe armï¼Œ that's one thingã€‚ typically people use Pe armã€‚

 that's what they recommend here tooï¼Œ it depends a little on the problemã€‚

What you can do is you can construct different mini batches for real and fakeã€‚

Meaning that you can say one mini page contains either all real or all generated imagesã€‚

So that helps to stabilize the training a lotã€‚ The reason facing you're getting all gradients for real or all gradients for generateã€‚

And that makesï¼Œ especially for the discriminatorï¼Œ it makes it much easier to trainã€‚

 And that's the thing typically people doã€‚ It depends a little bit on the batch sizeã€‚

 I don't have an advice hereã€‚One problem is theres very thisã€‚

 we'll see this later in the next few lecturesã€‚There's the two state of the ArcG methodsã€‚

 one of them is the stylegan line from NviDdia and the other one is the Biggan I think it's from Google these two are probably leading groups like training crazy big G models for like weeks basically and try to get good resultsã€‚

One of them uses small patchesï¼Œ one of them uses big patchesã€‚è¯¶ã€‚Which is contradicting the settingsã€‚

 but they both get good resultsï¼Œ but in between nothing worksï¼Œ so it's an interesting thingã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_49.png)

The optimizersï¼Œ people typically use Adam for the generatorï¼Œ SG for the discriminatorã€‚

 that's the thing you can play around withã€‚ä¸‰ã€‚This was then a thingï¼Œ I don't knowã€‚

You see this very quickly based on the training curvesã€‚ Ifï¼Œ if the training curves don't look goodã€‚

 you can change stuff hereã€‚ That's the first thing I would always doã€‚ Check your training curvesã€‚

 and you see this after a few minutesï¼Œ basicallyã€‚ğŸ˜Šï¼ŒOne thing is one side of label smoothingã€‚ğŸ˜Šã€‚

That's an easy oneã€‚ you can say in the discriminator rightï¼Œ like for the real samplesã€‚

You just penalize them part a little bitã€‚So you have a Lada function here that makes the discriminator here a little bit weak in terms of the confidenceã€‚

ğŸ˜Šï¼ŒGifts it like 0ã€‚9 or soã€‚Yeahï¼Œ this encourages a bit more extreme samplessï¼Œ which is goodã€‚

 right It doesn't always try to be on the safe sideã€‚ So it makes this part a little weakerã€‚

 so it reduces the confidence here of the discriminator which which is good by confidence rightã€‚

 obviously what you're getting now your score functions for the real images will just be a little bit lowerã€‚

 That's all you're doingã€‚ This is this one sided labels movingã€‚ The sum works reasonably wellã€‚

 can do it to extremeï¼Œ of courseï¼Œ0ã€‚9 is a reasonable number here where people use seems plausibleã€‚ğŸ˜Šã€‚

Another thing people do is they use historically generator patchesã€‚

 so what you do is you have a discriminator network and a generator network right and what you can do is you can generate stuff over a history of iterationsã€‚

And you can just mix your your generated samples in the current batchã€‚

 So your discriminator will see not just the results of oneã€‚Discriminatorã€‚

 so it will basically see discrimininate ass from multiple iterationsï¼Œ especially at the beginningã€‚

 This is very importantï¼Œ oh it's not importantã€‚ it's a very good idea because you will actually stabilize the discriminator a little bit It's not so heavily overfitting to one type of generatorã€‚

 but it's spreading it a little bitã€‚ğŸ˜Šï¼ŒSo it's learning kind of a distribution not of a specific generatorã€‚

 but it's learning a distribution of all generatorsã€‚

 well not all of them but the last n or somethingã€‚ğŸ˜Šï¼ŒThis is a relatively good ideaã€‚

 it's an easy implementation and it seems to help quite a bit at the beginningã€‚ğŸ˜Šã€‚

Another thing is sparse gradientsã€‚Stipability of the gang game suffers from gradient and sparsã€‚

 that makes sense that's the same thing for every other neural network right if you have no good gradients you kind of screwedã€‚

Licky reloos is a thing that people often used for both generators and discriminatorsã€‚

I've always seen parametric res actuallyï¼Œ looking really seems to be a good compromise of what people useã€‚

Still high level advice just use use again to start and then once you get this one train then you can play around with these things this one will not make a break it right away if you have a broken architectureã€‚

Down sampling use average pooling conscious stridesï¼Œ upsseble use decocon strideã€‚And so onã€‚

 there's things like pixel shuffle you can do helps a little bitã€‚But yeahï¼Œ I meanã€‚

 this is something you can actually debugã€‚ You can go ahead andã€‚

Literally visualize your gradients right you can see how good the crians are if your gradients go crazyã€‚

 then something went viral wrongï¼Œ This is something you should be aware ofã€‚

But you can see that actually rightï¼Œ you can check U crinians if they go to crazy then it's a problemã€‚

And you see this also relevant early onã€‚Another idea is exponential averaging of weightsã€‚ğŸ˜Šã€‚

One problem isï¼Œ well discriminator can be noy through STGDï¼Œ which makes senseã€‚

The problem is what you have at the very end of your training processã€‚

You're gonna have a lot of impact from the last iterationã€‚ rightã€‚ There's a lot of noise thereã€‚

So one thing people do isã€‚They take basically an exponential average of the weights of the neural networkã€‚

 So what I'm going to do is I'm going to storeã€‚Let's say I'm going to run my network for new iterationsã€‚

And I'm going to take my network and take an average of the last 100 durations and just average the neural network weightsã€‚

And you can of courseï¼Œ not just do a simple averageã€‚

 you can do an exponential average and can compute this running average while you optim itã€‚

Um and this is very good at the end because then you don't have these outlier samples anymoreã€‚

 these noisy thingsã€‚ğŸ˜Šï¼ŒSo you kind of want to avoid this bias towards the last iteration and this exponential average is easy to implementã€‚

 you just keep a second vector of the weights aroundï¼Œ you just have one update thereã€‚

 almost no cost helps a little bitã€‚This year is not helping your training too much but it will help your overall results potentially a little bit it's not going to a major leap but it's still a good idea for your final deploymentã€‚

Okay so these other things you can play around with I give you a lot of optionsã€‚

 there's no guarantee that one or the other will fixer will make or break it but at least you want to have a bunch of options what you can play around with what I could always recommend start very simpleã€‚

 type a simple architecture like these againï¼Œ try it out and end this firstã€‚

 then go to cell of a play around with that and see how you again behaveã€‚

Now I still want to introduce a bunch of objective functionsã€‚Off the Gsã€‚

There's a couple of actually as many objective function againï¼Œ I'm not going to go all of themã€‚ğŸ˜Šï¼Œå—¯ã€‚

With the caveat right nowã€‚Thatã€‚Don't change the objectives too many timesã€‚

 typicallyypically just use the heuristic methodã€‚ That's a good ideaã€‚

I still want to show you the implications of changing loss functionsã€‚

 but most of the time your again will not just suddenly work because of a different lossã€‚

I mentioned heuristics of standard that's the easiest wayã€‚

 it's not necessarily a standard for deployed networksï¼Œ but it's something you can always start withã€‚

I want to go over a couple of themï¼Œ one is EBGï¼Œ PganNï¼Œ WKN and LSGNã€‚

The loss function again alone one rigid workã€‚å—¯ã€‚I want to quickly go over theseã€‚

 I would encourage you Heath a read afterwards at these papersã€‚

 I might go quickly over it and I' want to spend too much time on itã€‚

But I think it's interesting when you read these papers what people thought about their insightsã€‚å—¯ã€‚

Let's have a look at the EB G paper The EPGN paper paper is basically EB stands for energy based scanã€‚

 The idea here is the reformulating the discriminator so basically they're saying my discriminator takes some sample X and the idea is I want to reconstruct this image perfectly This is just an autoencor formulation My discriminator says I want to have a good autoenr functionã€‚

å—¯ã€‚This is what my discriminator is going to do and I want to basically for real imagesã€‚

 I just want to sayï¼Œ wellï¼Œ I have good reconstructionï¼Œ that's it or encoderã€‚

 we want good reconstructionã€‚ğŸ˜Šï¼ŒNowï¼Œ what we're doing is we want to say if we are having a good reconstruction hereã€‚

And we pretrained this guyã€‚Then if it's a good reconstructionï¼Œ then it was a real imageã€‚

If we have a good discriminator train on real images hereã€‚

And we're feeding a new image in and it doesn't have a good reconstructionã€‚

 then it's not a real image because it wasn't reconstructed wellã€‚ that's kind of the high level ideaã€‚

And the way you do this is you simply say well when' training the discriminator I'm feeding a real image and a fake image or a la code in thereã€‚

 well let's start with the generator firstï¼Œ generator gets la codeã€‚

 generates image is being fed into the discriminator rightï¼Œ going to optimize for that functionã€‚

 that this generator tries to make sure that in the same way the discriminator could reconstruct stuff in the same way you could reconstruct stuff with the generator rightï¼Ÿ

ğŸ˜Šï¼ŒUm that's what I'm sayingï¼Œ like there should be no difference in the reconstruction quality if it's a real or it's a fake imageã€‚

ğŸ˜Šï¼ŒThe discriminatorï¼Œ however now saysï¼Œ wellï¼Œ I'm going to train the discriminator that's this part the first possible of the loss functionã€‚

 and now I'm going to sayï¼Œ I want to penalize the discriminator if the reconstruction error for generating imageã€‚

Drops below a certain valueã€‚ So I'mï¼Œ I'm just going to penalize and sayï¼Œ wellï¼Œ okayï¼Œ lookï¼Œ I wantnaã€‚

 I want to drive you apartã€‚ I want to say you were a fake imageã€‚

 You should not be well reconstructedã€‚But I'm only going to do this up to value n that's kind of what the EBN is doing right and I'm going to train these too jointly and hopefully I will drive these two apartã€‚

 I will get a laymen coding for realï¼Œ I will get a layden coding for fake and that is if I train both of them at the same timeã€‚

 my generator will figure out how to move them together and the disc scriminator tries to move them apartã€‚

ğŸ˜Šï¼ŒSo we're trying to get these lant spacesã€‚Separated out or togetherã€‚

 depending on whether you discriminate or the generatedï¼Œ it's a little bit reformulatedã€‚ğŸ˜Šï¼ŒBï¼Œ againã€‚

Has a similar idea on thatã€‚So B again is similar than E againã€‚

 except now instead of reconstructing a lossã€‚Or we measure the difference in the data distributions of the real and the generated imagess In other wordsã€‚

 we're going to go ahead and we have a real distribution hereã€‚

 we have a fake distribution here and we're trying to measure the difference between those twoã€‚ğŸ˜Šã€‚

There's various ways of measuring these differencesã€‚ğŸ˜Šã€‚

A very common way with people doing is looking atã€‚The vstein distance or the Earth move distanceã€‚

 same terminologyã€‚And this is this double you againï¼Œ which is essentially it followss out of thatã€‚

 The idea is I have a distribution hereã€‚ there's a bunch of blocks and I want to reorder these blocks and move them from here to hereã€‚

 So I have a distribution P and I have a distribution Q and I want to figure out how different are these two distributions and this v time distanceã€‚

Tells me how different they are right and how different are they Wellï¼Œ I want to figure out how muchã€‚

 how manyï¼Œ how much work do you have to do to move this one from here to hereã€‚ This isã€‚

I have to move this block from here to here sixï¼Œ moving this5 from here to here costs also some work right so everything you're moving from hereã€‚

 moving the earth from here to here costs you some effortã€‚UmAndã€‚

That tells you how different if I have n samples hereï¼Œ I have n samples hereã€‚

 I can figure out how different are these two distributionsã€‚This works for discrete casesã€‚

 you can formulate it on continuous cases too for the sake of the theoretical framework here were going to think about continuous distribution right but it measures the difference between these two different distributionsã€‚

ğŸ˜Šï¼Œå—¯ã€‚One like problem here is this is a very costly operatorï¼Œ rightï¼Œ computing faster distance veryã€‚

 very costlyã€‚ğŸ˜Šï¼ŒThere's a few ways how to reformulate this problemï¼Œ rightï¼Œ what you can do isã€‚

You can say you have a function f of xã€‚You want to figure out basically samples from one and samples from another distributionã€‚

 and you to want to figure out what the difference here is andã€‚The the E Dï¼Œ the earth of differenceã€‚

 can be formulated as the supreme of the difference between these two functionsã€‚

Sorry between these two different distributionsã€‚Andã€‚This oneï¼Œ howeverã€‚

Only holds if this function F that be used for the sample Xã€‚Is one lipitz constantã€‚

 It's a1 lip functionã€‚By the way if you call lipstitz means if you have the difference between the two functions between x so you found function fã€‚

 you're feeding in x1 or x2ï¼Œ the difference between the function values is smaller or equal in the differenceã€‚

 the absolute difference between the parameter values you fit in right so it's an upper bound between the densitiesã€‚

ğŸ˜Šï¼ŒI'm not going to go into detail how this one here is derivedã€‚ This takes a little bit more mathã€‚

 You just have to believe me right nowï¼Œ taking that estimate hereï¼Œ the sume will measureã€‚

It's basically the dual of the Earth move distanceã€‚And this is something we want to look atã€‚Andã€‚

The challenge what we need now is you need to find a function F here that fulfills the lips constraintstrainã€‚

Andã€‚If in this caseã€‚Is a critic functionã€‚That tells us the quality of x basically the sample is from PR and the sample here is from PQã€‚

 and we want to figure out how far apart are these guysã€‚Andã€‚

What would be nice if I had a critic that would tell me this right awayã€‚

If I had a critic that told me this for every sample of the distributionã€‚

 and here I have a critic for every sample of the distributionã€‚

And I'm getting the extended values of the difference hereã€‚ğŸ˜Šã€‚

Then I would know how close my distributions areã€‚Nowã€‚

 the problem is this function is very hard to define because let's say I have a bunch of imagesã€‚Wellã€‚

 who knows how close the distribution between these images areï¼Œ can do this on a per pixel basisã€‚

 but computing this earth move distanceï¼Œ as we know is very expensiveã€‚

 especially if you have a bunch of high resolution imagesã€‚ğŸ˜Šï¼ŒSo we do what we always doã€‚

And when we don't know itï¼Œ simply we learn this function Fï¼Œ so we're learning this credit functionã€‚

Andã€‚In this caseã€‚F is a neural network because we want to learn that function we don't know it right the only thing what we have to make sure now is we have to make sure we don't violate this onelipious constraintã€‚

ğŸ˜Šï¼ŒSo F needs to be a oneliperitz constraintã€‚ğŸ˜Šï¼ŒAnd in the optimization processã€‚

 we just have to make sure that this lips constraint is not being violated rightã€‚

 So what we do is we restrict the maximum weight value in Fã€‚å—¯ã€‚

And the way we do this is the weights of the discriminatorã€‚

 we just control them by a hyperparmeter C and this is called bait clippingã€‚

 so what we do is we just say oh my weights can get closer or smaller than minus C and plus C right so we just restrict the function values basically so we have eip estimate with C if I'm taking a bigger Cã€‚

ğŸ˜Šï¼ŒWell then then I'm more looseï¼Œ if I have a smaller Cã€‚

 then I more tighter and we go into this a second but this means if you have a smaller and a larger seaã€‚

Okayï¼Œ so this is controlled by this hyper parameter C and the weights are still being updated off this cr functionã€‚

 Againï¼Œ this is just for the critic hereã€‚ is for this F will become a critic function or is a critic function now that we model at the neural networkã€‚

 We want to optimize the parameters of the neural network by having RMS Proã€‚ we do clippingã€‚ğŸ˜Šï¼ŒAndã€‚å—¯ã€‚

What we're optimizing now isre optimizing both a generator and a criticã€‚

And this is what's happening hereã€‚ So here we we have a generatorã€‚We have a criticã€‚

And the critic is trying to measure the distributionã€‚

 the distance of the distribution from the generator and from the real imagesã€‚ So rightï¼Œ againã€‚

 this is our function that we have the supre let be optimizing for againï¼Œ quickly going backã€‚

 that's what we're optimizing forã€‚ So we're just taking a discrete number of samplesã€‚

 which is m samplesd hereã€‚We want to make sure the difference between these twoã€‚

It's going to be minimalã€‚At the same timeï¼Œ we have to optimize for Fã€‚LikeBecause F doesn't exist yetã€‚

 F is a neural networkï¼Œ I don't know the functionï¼Œ how to compute the earth move a distance between my samplesã€‚

So I have to jointly optimize for both the critic that tells me how close are the distributions and I have to minimize the distance between the two distributionsã€‚

 that's what Bastan G is doingã€‚ğŸ˜Šï¼Œå—¯ã€‚Yeahï¼Œ what we do here in practice is we have hereã€‚

 we have the function Fï¼Œ here's the real sampleï¼Œ we have the function F and here's the fake sampleã€‚

 right doingã€‚Againï¼Œ in order to optimize our generatorã€‚

 we are optimizing these two together we computing the gradientsã€‚Of F and Gã€‚

 right G has also weights and at the same time were trying to make sure that we getting a good credit functionã€‚

If you're comparing this to the standard GNs and the reason why I'm going over the Wasakhan Gsã€‚

 theyre actually widely used in practiceï¼Œ if you're comparing this to the standard GNsã€‚

 we have in here the GN formulationï¼Œ the GN formulation we had here that the gradients here for the discriminator was the binary cross entropyã€‚

 now what we have in the Wasakhangan for the discriminator is this critic that measures that uses a critic function tells me what is the difference between these two distributionsã€‚

ğŸ˜Šï¼ŒIn the can formulation for the generatorï¼Œ well I had this negative local likelihoodã€‚

 I basically want to make sure my generator produces stuff that discriminator doesn't recognizeã€‚

 And here I have gradients that that that optimize for F meaning thatã€‚Yeahï¼Œ'mã€‚Sorryã€‚

 I'm basically saying that my critic tells meï¼Œ ohï¼Œ it's a good distributionã€‚Okayã€‚

 if you're optimizing thatï¼Œ you will get something like thatã€‚ğŸ˜Šï¼Œè¯¶ã€‚Yeahï¼Œ what do we getã€‚

 have here we have here a loop let's say well we're not convergedã€‚ğŸ˜Šã€‚

And conversions will become a new meaning now because like here againã€‚

 we're optimizing for this critic hereã€‚ğŸ˜Šï¼Œå—¯ã€‚Andã€‚Now what we do is be optimizing for the criticã€‚

 we basically drawing samples from real dataï¼Œ we drawing samplesã€‚

 so we' generating samples with La vector Zï¼Œ we are optimizing the critic hereï¼Œ rightï¼Ÿ

And we doing the red clippingã€‚We do this a couple of times our estimates for our F gets betterã€‚

Then we sample from real and we're trying to make sure our gradient generate stuff that with this current function F cannot be distinguishedã€‚

Fromã€‚But sorryï¼Œ the critic says it's a good estimateã€‚And then you go ahead and then you iterateã€‚

These guys set critic to fiveï¼Œ so this loop runs five times for critic optimization and then only one generate updateã€‚

Prety updateï¼Œ generate updateã€‚

![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_51.png)

Okayã€‚If you're looking at the G lossesï¼Œ remember we had ga losses that kind of stabilized like that and we had this issue that it didn't we didn't know when it converged the nice thing about the Vaachesteingans now is we getting curves to look likeã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_53.png)

Theseï¼Œ meaning that ohï¼Œ it goes down actually rightï¼Œ and why does it go downã€‚

 it just means that this estimate here for the critic is getting better and betterã€‚

If we having a good critic function F that can estimate the distribution between these two set of samplesã€‚

And this is being very smallï¼Œ this is our loss function by the way we ploting hereã€‚

If the Spstein estimate becomes goodï¼Œ then in principleï¼Œ wellã€‚

 then my two distributions are the sameã€‚And that's very nice about the Wasstein laws is that it actually has a reasonable convergence plotã€‚

So my images at the beginning look bad and then the actually look better and better again DC scan architecture here only replace v span lossã€‚

I'm not saying here that v time produces necessarily better resultsï¼Œ the may or may not thisã€‚

 a lot of discussion about thatï¼Œ but what I'm saying is it's very nice to look at the loss curve here because it actually goes downã€‚

I mentioned this beforeã€‚Vazashsteinzstein G is one of the things people do actually use a lotã€‚Yeahã€‚

 forã€‚For Gsï¼Œ so either heuristic ofsteinï¼Œ the other two maybe notã€‚

 but this one is a very popular oneã€‚Yeah Pakistanakkistan again is great and now we see what we're doing so we have actual conversiongsã€‚

 in theory it should mitigate more collapseã€‚ğŸ˜Šï¼ŒYou can argue about thatã€‚

 generator still learns when critic performs well right these two they are not necessarily competingã€‚

 They are on the same sideã€‚å—¯ã€‚One problem is this lips constraint is difficultã€‚ W gl is a dirtyã€‚

 dirty heckã€‚ This is not a good ideaã€‚ There's a couple of variationsã€‚

Of whatever grading penalty and stuff like this what people useã€‚Baakely is not the best idea everã€‚

If it's too highï¼Œ it takes a long time to reach the limit right Basically if you're allowing for for too much of a variationã€‚

 this lipss estimate is not very good because it's too loose right That's what we we be doingã€‚

 if it's too smallã€‚We often get vanish ingredients because we don't learn anymoreã€‚

 especially when you have picnic ropebs that's a big issueã€‚ğŸ˜Šã€‚

Either one you don't train a lot if it's too small because my weights are being too small or my weight changes are being too smallã€‚

 or if it's too highï¼Œ my then my v estimate is badã€‚I don't want these two a problemã€‚

 so fixing the C parameter here is a problemï¼Œ rightï¼ŸAnd there's a couple of variations nowã€‚

 but this is a challenging taskã€‚For the lessonsã€‚ğŸ˜Šï¼ŒThere are a lot of variations of gain lossesã€‚

There's a high level understanding inã€‚You knowã€‚There's the high level understanding in meta lossesã€‚

 trainingï¼Œ stuff like that rightï¼Œ but the practical thing is that T provides the gradient right like these our learned loss were motivated at the beginningã€‚

 these are learned loss that we used in orthorenncheã€‚But there's many variations of thatï¼Œ of courseã€‚

ğŸ˜Šï¼ŒMy recommendation is always start very simpleã€‚ That's always my recommendationã€‚

 Like always dividing conquer is keyï¼Œ rightï¼Œ we want to start simpleï¼Œ get simple stuff to workã€‚

 and then we make things more complicatedã€‚ğŸ˜Šï¼ŒAlways start simple if things don't convergeã€‚

Don't randomly shuffle the loss aroundã€‚ This is what I've seen unfortunately manyã€‚

 many times changing loss is the last thing you should doã€‚

 I would always start with the heuristic first that's the easiest one to get around maybe bus time but but that's about itã€‚

 I've seen like people trying to do20 different lossesã€‚

 none of them worked and they spend a lot of time on it all start simpleã€‚If you doing Gsã€‚

 I would always start simpleï¼Œ I would start with an auto encoder first without the Gã€‚

Get this architecture to workï¼Œ use the variational orange encoder nextã€‚

And then use say simple wrist againï¼Œ and then you can compare the results of the auto encoder with a variational all encoder and then with the reverse againã€‚

 rightï¼ŸThis way you have a direct comparison and the reason why I'm mentioning it this overhead is it's just very trickyã€‚

To evaluate or to see whether the organ is workingã€‚Okayï¼Œ yeahï¼Œ I still have a few more slidesã€‚

 but I would like to yeahã€‚ğŸ˜Šï¼ŒRoughly come to an end of this video because we already have an hour 38 minutes I hope this was an interesting first introduction I know a lot of people if you have already heard about GNs but I think it's still a good exercise to rethink all these kind of key ideas of the GNs and then in the next video in this lecture we will talk a little bit more about GN architectures and what we can actually do with them in practiceã€‚

Otherwiseï¼Œ stay safeï¼Œ stay healthyï¼Œ see you next timeã€‚



![](img/bb5ec175fa5f4ec4be52baf1d6a7ab87_55.png)