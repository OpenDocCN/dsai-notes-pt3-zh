# P3ï¼šL3 - å­ªç”Ÿç½‘ç»œä¸Žç›¸ä¼¼åº¦å­¦ä¹  - ShowMeAI - BV1Tf4y1L7wg

![](img/4b126a9c0436433d6b4aae9ace6be002_0.png)

Welcome to the lecture on semi's neural networks and similarity learningã€‚So so farã€‚

 we have seen at I2DLï¼Œ but also at the beginning of ADL4 CVã€‚

Several problems that we have tackled with machine learning and in particular with deep neural networksã€‚

So machine learning is actually really good for classification problems and we have seen the kind of the standard problem in computer vision which is image classification that is you have as an input an image for example this image showing a catã€‚

 you pass it through a neural network and this neural network gives you the label CA so in this case it classifies the whole image with a label of what kind of object is this image representingã€‚

So this a classification problem rightï¼Œ there's a certain fixed number of classes for which we have train our neural network and at test time we have an input image and the neural network picks one of these classes as an outputã€‚

Now we can do this at a really large scale right so on IageNe we can actually tackle the classification problem on thousands of categoriesã€‚

 so this is not really a restricted problemï¼Œ but it can actually solve and classify and understand a bunch of imagesã€‚

 a bunch of categories up to thousandsã€‚Now in this graphã€‚

 we can see the actual performance of neural networks on the imageNe datasetã€‚

Now the size of the blocks indicates a number of parametersã€‚

 and we can see here at the bottom left cornerï¼Œ the AlexNe architectureã€‚

 kind of the starting point for computer vision scientists to realize that deep learning was actually really powerful for the task of image classification and beyondã€‚

And you can see that since AlexNetï¼Œ there have been a lot of architectures that have been developedã€‚

 GoogleNetï¼Œ ResNetï¼Œ a bunch of architectures that we saw also at I2DLã€‚

 and these have been increasing steadily the accuracy on the INe classification challengeã€‚

Now there's another type of problems that we can actually tackle with machine learning with dim neural networks and these are regression problemsã€‚

 so in this case we don't have a fixed number of outputsï¼Œ for exampleï¼Œ a fixed number of classesã€‚

 but what we want is an output of our neural network is a continuous valueã€‚So for exampleã€‚

 the task of post regression would be a regression problemï¼Œ so in this case we would have as inputã€‚

 an imageã€‚And this would be processed by a neural networkã€‚

 you would return an embedding and from this embeddingï¼Œ you would actually predict the poseã€‚

Of the camera from which this image was taken and the pose is actually parameterized by a positionã€‚

 the center of the camera and a quaernion which actually indicates the orientation of the camera Now these two values are continuous and therefore they can take any any value so there is no fixed predetermined number of values that our neural networks is actually choosing from like with a classification problem but we have an output which takes a continuous valueã€‚

Now we can divideï¼Œ in our mindï¼Œ the neural network in two partsã€‚

 the feature extraction part in which we process the imageï¼Œ and we return this embeddingã€‚

 in this caseï¼Œ this 2048 dimensional embedding that represents the image and the lineargressor that actually projects this embedding into the output space that we're interested inã€‚

Now the regression problem can also help us to tackleï¼Œ for exampleï¼Œ the tracking problemã€‚

 so in this case what we want is to track this face that is kind of moving around so imagine that you're trying to track my face so you're trying to put this bounding box around my faceã€‚

And soã€‚How the authors of this paperï¼Œ this 2016 paper actually pose this problem is by comparisonã€‚

 so they actually have two framesï¼Œ the current frame and the previous frame and what they want to do is they want to compare these two framesã€‚

 hence these two frames are processed in parallelã€‚And then these three fully connected layers what they do is essentially they solveã€‚

 they map this comparison into the regression into the output that we're actually interested inã€‚

 which is actually the predicted location of the target so in that case the input would beã€‚

The search regionï¼Œ so I assume that my target is in the search regionã€‚Here on the bottom headã€‚

 we would have actually the object that we're interested inã€‚

 kind of the template that we're looking forï¼Œ so the face in this caseã€‚

 we would compare these two and finally we would get a bound box that tells us what is the most likely location for this face that we actually wanted to trackã€‚

And you can see here already interesting things like for exampleï¼Œ these two headsã€‚

 so there is one processing pipeline here at the top that processes one image and the other that processes the previous frameã€‚

 another image and these two actually run in parallel and we will see today a lot of examples like this because this is in fact a AamE network architectureã€‚

Now there is a third type of problems and these are the problems that we will tackle in this lectureã€‚

And these are the problems of comparisonï¼Œ so let's assume that we have these two images of myself and Professor Lisnerã€‚

 now a classification problem with actually analyze these two imagesã€‚

So there is a third type of problems that we can actually tackle with deep learning and this is the problems of comparisonã€‚

So in this case we would have these two imagesï¼Œ AMBï¼Œ which depict myself and Professor Nisnerã€‚

 if we were to actually analyze these images from a classification pro perspectiveã€‚

 we could actually say that both of them are personï¼Œ both of them depict the faceã€‚

 A depicts a femaleï¼Œ B depicts a maleï¼Œ etcaï¼Œ et ceterã€‚

 all the characteristics that you can actually think ofï¼Œ that you can actually train forã€‚

But the question that a classification problem cannot answer is actually the question of is this the same person right so I'm really looking into categories into all of these definitions person face female male and then comparing and then deciding whether this the person is the same person or notã€‚

 but actually want my neural network to directly answer this questionã€‚

And so the idea is that we need to have a way to train our neural networks for similarity learningã€‚

 to actually compare to imagesï¼Œ to actually rank two imagesã€‚

So let's assume the user case in which we have this application of unlocking your iPhone with your faceã€‚

 so nowadays most of the phones can be unlocked with either a fingerprint or a faceã€‚

So how can we actually do this with neural network so first of allã€‚

 what I have to do is I have to collect a training setã€‚

 so a bunch of images that actually depict my faceã€‚And ideallyã€‚

 this would be at different illumination conditionsï¼Œ wearing glasses without glassesã€‚

 hair with different indi hair stylesã€‚So all the type of diversity that you can actually think ofã€‚

And then what I would do is I would actually train my neural network with all of these images and at test time there would come two persons in front of my iPhoneã€‚

 there would be me image A and Matias in image Bã€‚And at test time what my iPhone would then say is it would solve a classification problem right so it would compare the images that it has been trained for and at test time we would sayã€‚

 wellï¼Œ image A does belong or is similar to the images that I've seen beforeã€‚

 therefore I classify it as yesï¼Œ this is the same phase and image B I classifiedify as noã€‚

 this is a different phaseã€‚So for this particular applicationã€‚

 we could actually solve it as a classification problemã€‚

Now let's assume that we want to do the exact same thingã€‚

 but instead of recognizing only my face because this is my iPhoneã€‚

 therefore it has to recognize only the face of one personã€‚

I actually want to train a face recognition system so that students when they're entering an exam roomã€‚

 I can directly know the ID of these students which without actually needing a manual ID checkã€‚

 you know so as soon as they enterï¼Œ camera analyzes the face and immediately identify themã€‚

Now what I could do is I could collect my training setã€‚

In the same way that I did before right so I know which students are actually registered for the exam and so I asked them to send a bunch of images of themselvesã€‚

 so I collect the images for student oneï¼Œ I collect the images for student threeã€‚

 two and I collect the images for student threeã€‚So I collect images for all these students and now I train a classifier right so instead of having the output yes and no this binary outputã€‚

 I will have the output of person 1ï¼Œ23ï¼Œ45 and as many students as I have in the examã€‚

 as many students as I registered in the examã€‚Nowï¼Œ of course what is the problem with this approach and now I would actually ask you to pause this video and think about this yourselfã€‚

 come up with a series of problems and see if you were right by playing the video after a whileã€‚

So the main problem with this approach is actually scalability right so every time that a new student registers to the course or register to the examã€‚

 I actually need to retrain my neural networkã€‚So let's imagine that I've trained my neural network for 99 students and one minute before the exam if this was allowedã€‚

 actually another student registers of course he or she has the right to enter the exam but what happens is that now I don't have time to retrain my neural network for 100 students instead of 99 so there's clearly a problem with scalabilityã€‚

And at the same timeï¼Œ every semesterï¼Œ what I would need to do is I would need to ask the students for a bunch of pictures of themselvesã€‚

 retrain my neural network and do this for every course and every semesterã€‚

 so this method is clearly not scalableã€‚So the question that I actually want to solve here is or the question that I want to answer isã€‚

 can I actually train only one model and use it every year for the purpose of face recognitionï¼Ÿ

So it turns to actually Kenï¼Œ and for thisï¼Œ instead of posing the problem as a classification problemã€‚

 I'm going to pose the problem as a similarity learning problemã€‚So againï¼Œ I have my images A and Bã€‚

 and now what I want to do is I want to train neural network that when the images A and B depict a different personã€‚

 in this caseï¼Œ me and Matiasï¼ŒThe neural network would give me a low similarity scoreã€‚

And when the two images are depicting the same personï¼Œ I would get a high similarity scoreã€‚

So notice that we're not training for classificationï¼Œ we're not training for yes and noã€‚

But we're training the neural network to actually output a continuous value right low similarity score can be a range of scoresã€‚

 high similarity score again can be a range of scoresã€‚

 so this similarity score is going to take a continuous valueã€‚

So a test what I do is I would actually have my neural network trainï¼Œ I would get these two imagesã€‚

 and I would say if the distance between two images is larger than a thresholdã€‚

 then it means that it's not the same personã€‚If the distance between two images is lower than a thresholdã€‚

 then it means that it's the same personã€‚And you can actually make the same argumentã€‚

 but instead of distance using the similarityã€‚So another main question is how do you actually train a neural network to learn this similarity functionï¼Ÿ

And this is where Siamese neural networks come in handyã€‚So the first thing that we need to doã€‚

It's actually we need to process the two images in parallel and we need to process them in the exact same wayã€‚

This means that I'm going to take my image Aï¼Œ I'm going to pass it through a CNN and obtain my embeddingã€‚

 so this embedding actually represents the phase depicted in image Aã€‚

So let's assume that now what I can do is I can take any image and I could compress it in 128 valuesã€‚

Now what I want to do is I actually want to process any image that I want to compare with Aã€‚

 I want to process it in the exact same wayï¼Œ so I want to obtain also for image Bã€‚

 this 128 factor representationï¼Œ and I want to obtain it of course with the exact same CNNã€‚

So now what I can do is I can start comparing these two representations since two embeddingsã€‚

 F of A and F of Bï¼Œ and see how similar they areã€‚Nowã€‚

 the interesting thing here is that I have processed the two images A and B in parallelã€‚

And thereforeï¼Œ this architecture is actually called a Siamese network because it has these two branches that actually process the information in parallelã€‚

And the interesting thing is that they actually have the shared weightsã€‚

 so they are the exact same two neural networks with the exact same weights processing images A and Bã€‚

Nowã€‚Since we have used the same network to obtain an encoding of the image F of a and to obtain an encoding of the image F of Bã€‚

Now the only thing that we need to do is to actually compare these two encodings and remember that if A andB depict the same personã€‚

 we wanted these encodings to be very similar and if A andB depict a different personã€‚

 then we want these two encodes to be very dissimilarã€‚

And so what we can actually do is we can train for that right so the first thing is that we define a distance functionã€‚

 D and Bï¼Œ which directly compares the embedding F of a with the embedding F of B and in this case we use for example the out normã€‚

Noã€‚A train time what I want to do is I want to train my neural networksã€‚

 I want to learn the parameters of the neural network such that if A andB actually depict the same personã€‚

 then be smallã€‚And if A and B depicts a different personï¼Œ then D is largeã€‚And I can actuallyã€‚

Nicely express this in my loss functionã€‚So let's take a look for exampleã€‚

 at what happens for a positive pairï¼Œ so I know that A andB depict the same personã€‚

 so I want D to be smallã€‚So I can define a loss function that is exactly the alter norm between the embeddings f of A and F of bã€‚

 so I know that if these two depict the same personã€‚

 then I can minimize these loss and I will automatically be minimizing Dã€‚

Now what happens with a negative pair right so in this case I know that A andB depicts a different personã€‚

 so I want D to be largeã€‚So in this caseï¼Œ I'm going to use not directly the L2 normã€‚

 but I'm going to use a hinge lossã€‚And the idea here is that I want to separate F of A and F of Bã€‚

I want to keep separating them until we hit a margin Mã€‚And the idea is thatã€‚

It makes no sense to actually push F of A and F of B further and further and further apart if they are already as far as the margin Mã€‚

And this is why we're going to use the hinge lossã€‚So if the two elements F of A and F of P are already really far awayã€‚

 I'm not going to spend any energy in pulling them even further awayã€‚

 so this loss is going to become zero when F of A and F of P are far apart enoughã€‚

So that I put both things togetherï¼Œ I come up with a formulation of what is called the contrastive lossã€‚

 so in this case I'm going to have my positive pair so I know that my level y is going to be one because I know that A andB represent the same personã€‚

And when y is actually oneï¼Œ what I want is to minimize the distance between these two embeddingsã€‚

And thereforeï¼Œ I have the distance here depicted as the L2 norm of F of A and F of Pã€‚

When I have a negative pairï¼Œ so when1 minus y is actually oneã€‚

I actually want to apply the hinge slotsã€‚So I want to bring the elements F of A and F of B further and further apart up to a margin Mã€‚

So if I put both things togetherï¼Œ I come up with a contrastive loss to train my Siamese neural networkã€‚

So how do we actually train these semi neural networksï¼ŸWellï¼Œ first of allã€‚

 the weights for each of the channelsï¼Œ for each of the branches have to actually be updatedã€‚

 right and you have the two branches that are in parallel actually processing the informationã€‚

And you get a loss for each of the branchesï¼Œ then what you can do is actually update the weights for each of the channels independently and then average themã€‚

Nowã€‚With this contrastive lossï¼Œ this this kind of the simplest loss for similarity learningï¼Œ rightã€‚

 so we have actuallyual learned to bring positive pairs togetherã€‚

And we have learned to bring negative pairs apartã€‚But the question isã€‚

 can we actually do better or let's sayï¼Œ what are the problems with the contrastive lawsï¼Ÿ

So this is how basically we came to the formulation of the tripleoli lossã€‚So in this caseã€‚

 what the tripleE loss allows us in contrast with the contrastive laws is that we can actually learn a rankingã€‚

 so it's not only about similarity being close together or being further apartã€‚

 but now we actually want to learn how much closer am I compared to another imageã€‚So in this caseã€‚

 in order to do thisï¼Œ of course you can no longer train with pairs of imagesã€‚

 but you actually have to train with tripletsã€‚So you have what is called an anchor imageã€‚

 then a positive image which actually depicts the same person and a negative image which depicts a different personã€‚

And now what you actually want to do is you want to bring the embeddings or the anchor and the positive image closer togetherã€‚

And in this caseï¼Œ it's not infinitely closerï¼Œ but you actually want to bring it close so that the distance between anchor and positive is smaller than the distance between anchor and negativeã€‚

So essentiallyï¼Œ if my anchor and positive images are more similar than the anchor and the negative imagesã€‚

 then I'm going to be happy like thisï¼Œ so the loss is not going to be further improvedã€‚

So in order to actually express this condition in a loss functionã€‚

 we actually have to shift a couple of things aroundï¼Œ so I take here the comparisonã€‚

 the L norm between f of a and f of nï¼Œ and I bring it to the left side of the equationã€‚

 leaving a zero on the right sideã€‚And actuallyï¼Œ I'm going to now also use a margin here like I did for the negative pairs in the contrastive lossã€‚

 just because of courseï¼Œ I don't want to be rigid about my decisionsã€‚

And so essentially now I can express the triplet lossã€‚U a function that takes an anchorã€‚

 a positive imageï¼Œ and a negative imageã€‚And actually is trying to optimize this function hereã€‚

 so this is going to be again a hinge loss type of function we're going to have this maximum zeroã€‚

 we're going to have the margin just like we had beforeã€‚And so essentiallyã€‚

 what I'm saying with this triple loss formulation is that I'm going to bring together the embeddings of AMPã€‚

Until the distance between F of A and F ofP is smaller than the distance between F of A and F of Vã€‚

All the way up to a marginã€‚So the idea is that I'm not making the distance between F of A and F of P infinitely smallerã€‚

 I'm just making it small enough so that A andP are closer than A and N in the embedding space of my neural networkã€‚

So now of course the first question probably that you have is what if I choose APNN in the wrong way such that for exampleã€‚

 A&P are obviously the same person and so these task is very easy What is going to happen is that my neural network is not really going to learn much because from the beginning A&P are going to have a smaller distance than A&Nã€‚

So what I can do actually to boost the performance of my neuro network train with a triple loss is to do what is called hard negative miningã€‚

 which means training with the hard casesã€‚So I'm going to train a neural network with randomly selected triplets for a few eposã€‚

And then what I'm going to doã€‚As I'm going to look at the cases where the distance between A&P is veryã€‚

 very similar to the distance between A and Nï¼Œ and I'm going to choose only these cases to further refine to further fine tune my neural networkã€‚

 so I'm going to describe all these cases and focus only on training on the hard casesã€‚

And we're going to see a little bit more about choosing the triple platets in a few slidesã€‚

So essentially what the Trit loss is going to doã€‚It's even the beginningã€‚

 the distance between my anchor and the negative image was smaller than the distance between anchor and positive imageã€‚

After trainingï¼Œ what is going to happen in the manifold of my neural network is that the positive image will come closer to the anchorã€‚

And the negative image will become further away from the anchorã€‚

So this is kind of visually the intuition of what happens with the triplet lossã€‚And againã€‚

 note that in contrast to the contrastive lossï¼Œ actually the positive image and the anchor don't come infinitely together and the neural network is spending a lot of time in training so that the anchor and the positive image have a distance of zero in the endã€‚

And at the same timeï¼Œ my negative image is not going further and further and further apartã€‚

So the only condition that the triplet loss is actually fulfilling is that positive and anchor are closer than anchor and negativeã€‚

And this's actually enough to learn the rankingã€‚So now a test time is very easyã€‚

 right one can just do nearest neighbor searchï¼Œ so I have my query imageï¼Œ for exampleï¼Œ in this caseã€‚

 this bird in a certain poseã€‚And now I have a bunch of imagesã€‚

 I actually compute the embeddings of all of these imagesã€‚And I actually plot the top four imagesã€‚

That have an embedding that is closest to the embedding of my query image right so F of a F of my query imageã€‚

And now I compute embeddings of all my test imagesï¼Œ and I pick the top fourã€‚

 the four that are closest to my query image in this embedding distanceã€‚

And it turns out that a neural network trend with the triple loss can work relatively wellã€‚

 so you can see that for exampleï¼Œ the owl is retrieved properlyã€‚

 the cup is mostly retrieved properly and of course there areï¼Œ for exampleã€‚

 in this case of the lamp there are cases in which you have different types of lamps that are also retrievedã€‚

Now of course this regular loss is not perfect right so one of the challenges that we have briefly mentioned is that random sampling of3+ actually does not workã€‚

So the number of pro triplets is actually hugeï¼Œ so imagine your training set is a thousand0 imageã€‚

 you can actually compute how many triplets you would have with this small training setã€‚

So if you want to actually explore all possible tripletsï¼Œ you need to train this network for a veryã€‚

 veryï¼Œ very long timeã€‚And if you actually do random sampling of the tripleletsã€‚

 it could happen that the hard cases are not actually sampled and therefore you never see these hard cases where you have one positive and one negative image that are veryã€‚

 very similar both to the anchor and your neural network never gets to see this case and gets to betrained for this caseã€‚

So random sampling is strictly for the attribute lossã€‚And even with high negative miningã€‚

 you can actually get stuck in local minimumã€‚So in order to actually further improve singularity learningã€‚

 there are several approaches that have been presentedã€‚

So there are several ways of improving the way you train your neural networks for similarity learningã€‚

 we have divided them into four categories that we want to briefly discussã€‚

 and then we provide a lot of references so that if you want you can go deepã€‚

 you can read these different works that improve the way you train neural networks for similarity learningã€‚

So one of the ways clearly is to improve the loss so we have already seen the contrastive loss versus the triplet loss and how actually the triple loss is superior because it leads not only to the similarity value yes and no close or far awayã€‚

 but it leads to a sort of rankingã€‚And also training with a triple loss is much more efficient and actually leads to better results for similarity learningã€‚

But there are three other ways in which you can actually improve the way you train neural own similarity learning one of them is sampling we have also briefly talked about that how if you actually choose all of the tree plates for your training setã€‚

 this is computationally impossibleã€‚So sampling is a way to actually choose the best triple to train your neural network withã€‚

And in this caseï¼Œ the main intuition is that you need to sample the space wiselyã€‚

 so you need to sample the diversity of the classesã€‚

 and at the same time sample the hard cases so that you can keep improvingã€‚

 you can keep refining these decision boundaries that your neural network is being trained forã€‚

Now the other is ensemblesï¼Œ so instead of making all decision with one neural networkã€‚

 you actually use several networks and each of them is strained with a subset of tripletsã€‚

 for exampleã€‚And finallyï¼Œ we will see a list of words that actually use a classification loss for similarity learningã€‚

We will not further discuss or present works that deal with the definition of different losses for similarity learningã€‚

 but we do recommend that you read these two papers on angular loss and correcting the triple selection bias for triplesã€‚

 which are very interesting works that deal with actually the definition of the loss for similarity learningã€‚

And we'll jump directly to sampling methodsï¼Œ so in this case we will present and overview the intuition of one sampling method called the chierarchical triplet lossã€‚

And in this caseï¼Œ the idea is that you actually want to build a keraraical treeã€‚

where the leaves of the tree represent the image classesï¼Œ so you can seeï¼Œ for exampleã€‚

 in this exampleï¼Œ you have eight classes of different birds and at the level zero of the treeã€‚

 you have one node for each of these eight classesã€‚

And now the idea is that you recursively merge most similar classes until you reach one single root noteã€‚

And you can already see the intuition behind this hierarchical representationã€‚

 so of course in this hierarchical representationï¼Œ classes one and two are more similar are closer together in this feature space then classes1 and three and at the same time classes three and four are closer together than classes three and6ã€‚

So how you actually build this tree is by first of course defining a distance function right so you do have to define a distance between classes and the intuition is that if the distance is smallã€‚

 then two classes will be merged earlier in the tree so they will be merged for exampleã€‚

 at the next level of the treeã€‚So if classes three and four are close togetherã€‚

 maybe at level one they already mergeã€‚If classes two and five are further apartã€‚

 then maybe we need to wait until level three to actually see the merge of these two classesã€‚

And the way you define the distance between two classes is by using the feature embeddingã€‚

 so this RI and RJï¼Œ so you get this al norm of the deep features of your neural networkã€‚

And then you basically normalize them by the current enality of classes P and Qã€‚

 so NPs essentially how many samples do we have for class Pï¼Ÿ

So once you have defined your hierarchical tree based on these distancesï¼Œ rightã€‚

 we don't go into details onto when do you actually merge two nodesã€‚

 but let's assume that we have a systematic way of merging notes and we have built this hierarchical treeã€‚

Now what we want to do is want to find the anchorsã€‚

 we want to find these images that are going to be used to actually build our tripletsã€‚

And what you do is you start level zeroï¼Œ the level where we have all of the classesã€‚

 and you randomly select L prime nodesã€‚So you randomly select LPnk classesã€‚

Now the idea behind this first step is that you actually want to preserve some kind of class diversity in the mini batch so you don't want to sample all of the anchors from class4ã€‚

 for exampleï¼Œ so the first thing you do is to sample L prime classesã€‚Now from this Alheim crisisã€‚

 what you're going to do is still at level zeroã€‚You're going to select M minus1 nearest classesã€‚

So for each of these nodes that you have selectedï¼Œ let's say you have selected Note fourã€‚

Based on the distance to the other classes at level zeroï¼Œ you select M minus1 nearest classesã€‚Nowã€‚

 the idea behind this step is thatã€‚So you have to keep in mind our final goalã€‚

 right to perform this similarity learningã€‚And at the same timeã€‚

 you want to select these hard cases right so now you sample class fourï¼Œ which is one type of birdã€‚

And you look at all the other birds and select the most similar oneã€‚

And the idea is that we actually want the model to learn these discri featuresï¼Œ rightã€‚

 we want the model to separate class four from all the other birds which are veryï¼Œ very similarã€‚

So we actually want to sample from these very similar classes right so step one is to sample diverse classesã€‚

 step two is to sample similar visually similar classes to the classes sampled in the first stepã€‚

And finally what you do is you actually collect images right so up until now we have only sample classes and now what we want to do is actually sample images so we sample T images for each of the classes that we have collected and this sample is going to happen randomly so you're going to select T random images within from each of the classesã€‚

So in the endï¼Œ what you end up with is a mini batch which contains T times M times L prime imagesã€‚

And other ideas well we have to construct the triplets right so what we do is actually we construct all triplets so in this mini batch we have these T times M times L prime images and we're going to construct all the tripletsã€‚

With this number of imagesã€‚And the interesting thing about the loss formulation of the hierarchical triplet loss is that not only is a way to actually sample images in a clever wayã€‚

But actuallyï¼Œ it also allows you to define a margin in this caseï¼Œ this alpha Zã€‚

That depends on the distances computed on this hierarchical treeã€‚

And the idea actually is that it makes no sense to have this fixed marginã€‚

 so in the case before for the normal triplet loss we have Mã€‚

 which was a fixed margin for all tripletsã€‚But the idea is that since we have an idea of how similar two classes areã€‚

 we can also use this idea to adapt the margin right so that the loss is going to adapt to the class distribution and it's going to adapt to the differences of the samples within these classes now it's not important what is the exact formulation of this alpha but if you're interested in itã€‚

 I recommend that you read the paper on the hierarchical triplet lossã€‚

So there's a bunch of other worksï¼Œ the one that I would definitely recommend you to read is the first oneã€‚

 this ICCV 2017 on sampling matters for metric deep metric learningã€‚

Wwhich is kind of the original paper that actually propose a sampling methods to sample diverse and hard casesã€‚

 but there's also a bunch of other work that try to improve further the training of neural networks forã€‚

Pmetric learning by using more and more clever sampling methodsã€‚Okayï¼Œ so after samplingã€‚

 we turn into ensemblesã€‚So the idea behind ensembles is the idea of divide and conquerã€‚

 which you might have seen in other lectures in computer scienceã€‚

And so the idea is that we have manyï¼Œ many classesã€‚

And we actually want to learn distance function between all classes at the same timeã€‚

And this is just too much for a neuroã€‚So the idea isï¼Œ okayã€‚

 I'm learned to take it one step at a timeï¼Œ and I'm going to first of allã€‚

 divide my space intoque clustersã€‚And have one learner focusã€‚On one clusterã€‚

 so I'm going to have in this caseï¼Œ K learnersï¼Œ Kï¼Œ for exampleï¼Œ neural networksã€‚

And each of the neural networks is just going to focus on performing similarity learning insideã€‚

The classes of the case clusterã€‚So in the first stepï¼Œ in the divide stepã€‚

 you actually divide the space into these K clustersï¼Œ divide the cluster into K clustersã€‚

 and in the con stepï¼Œ you actually train the neural networks for each of the clustersã€‚

So we can see this in more detail how the divide and conquer methodology actually worksã€‚

So first of allï¼Œ we have our neural networkï¼Œ which already provides us with a representation for each of the images and each of the classesã€‚

So it's already trained and what we're going to do is we're going to use these features to actually divide the embedding space into K classesã€‚

 and we're simply going to do this with Kenyonsã€‚Nowã€‚

 the idea that now what I want to do is I want to build these can dependent learnersã€‚

But instead of having a completely different neural network trained for each of the K clustersã€‚

 what I'm going to do is I'm going to have a backboneã€‚

 I'm going to have a CNN that is going to be shared by the key independent learners and I'm going to have a series of fully connected layers which are going to be separateã€‚

 so K different sets of fully connected layersã€‚And these are going to constitute actually the decay learnersã€‚

 and these are going to be trained each for one clusterã€‚Nowï¼Œ within a clusterã€‚

 what were going to do is we're going to train our learner by sampling a mini batchï¼Œ randomlyã€‚

 samplingï¼Œ for exampleï¼Œ triplets randomlyã€‚And then updating these fully connected layersã€‚

And so once our network has convergedï¼Œ what we can do is we canã€‚Put everything togetherã€‚

 so use all the learners at the same time and fine tune our network with all the training setã€‚

So first you divide the problem into small chunksï¼Œ K clustersï¼Œ you train your another for thatã€‚

 once you have reached a stable pointï¼Œ then you fine tune your network for a few eposã€‚

With all the training set for the full problemã€‚So this is the divide and conquer scenarioã€‚

And then you can go back to one and repeat several times and so on and so forthã€‚Now for ensemblesã€‚

 you can also haveï¼Œ for exampleï¼Œ separate networks that have different architectures or they all have the same architectureã€‚

 so they are actually different ways of using ensembles and these are some of the words that we actually recommend you to read if you want to get a little bit deeper into ensemble workã€‚

And finallyã€‚We actually want to present this kind of funny idea that we could actually use a classification loss for similarity learning right so this seems to contradict the beginning of the lecture where we said you know there's this classification problem and similarity learningã€‚

 two different problems and we need a different way to train bothã€‚But as it turns outã€‚

 latest works have actually shown that you can train your neural network for classificationã€‚

 so with a classification lossï¼Œ for exampleï¼Œ cross entropyã€‚

And still obtain an embedding that is good for similarity learning and these are some of the works including our own at the end that actually show that you can use these classification lossesã€‚

And then still obtain embeddings that are significant enough that are good enough to actually perform similarity learning so these type of works you can see that these are very new right it started roughly in 2017 but then theres several works in 2019 and from this year that actually push for this concept of using classification laws for similarity learning so very interesting new research that is happening on this frontã€‚

Okayï¼Œ so let's see some resultsï¼Œ so these are three of the data sets that are commonly used for this problemã€‚

 the CAP cars and ST for online productsã€‚If you want one of the best methodsã€‚

 the method that performs the bestï¼Œ you can check this HORDE method published in ICCV 2019ã€‚

 so very recent stuffã€‚And one of the metrics that is most important for similarity learningã€‚

iss the R1 metric or recall one and this actually means that in the test scenario that we saw before where I have my query image and I order all of my training imagesã€‚

According toï¼Œ for example theã€‚The feature embeddingsã€‚I actually get the matchã€‚

 the best match so the actual correspondence that I want to find I find it already in the first imageã€‚

 so recall one means that the first image is the one that I was looking for so I immediately make the correct assignmentã€‚

 recall two is that the correct image is within the first two nearest neighbors recall8 it means that it's in the first eight nearest neighbors so this is why you see that the accuracy actually grows from recall one to recall8ã€‚

And here we see some methods that are comparinging loss plus sampling methodsã€‚

 we have also some teacher student method that we haven't talked aboutã€‚

For example loss was ensemble and here I just want to show you that the exact same method which is called RLL when you're actually using one networkã€‚

 you get a recall  one of 57ã€‚4 when you use three networks so when you actually use this power of ensemble you actually boost the results all the way to 61ã€‚

3 for recall one so just by using instead of one network using three exact similar networksã€‚

Train with different tripletsï¼Œ you actually can boost the performanceã€‚

 so you see that this divide and conquer idea is actually very powerfulã€‚So okayã€‚

 so after all this softwareï¼Œ you might be askingï¼Œ wellï¼Œ okayã€‚

 I'm interested in having a model for similarity learningï¼Œ what kind of model can I useã€‚

 what kind of method can I use to train my network for similarity learningï¼Œ rightï¼Ÿ

So it turns out that in a paper that has been published recently this year called the metric Learning Real Checkã€‚

Turns out that when you train all of these different methodsï¼Œ all of these different lossesã€‚

 the different sampling methodsï¼Œ when you use the same backbone means the same CNN architectureã€‚

 ReSNeï¼Œ Alexï¼ŒNetï¼Œ VGGï¼Œ whatever you wantã€‚When you use the same embedding spaceï¼Œ the same dimensionsã€‚

 when you don't use extra tricks for boosting resultsã€‚It turns out that for all of these methodsã€‚

 the difference in accuracy is not really largeã€‚So something that happens quite commonly in research is that sometimes results are not directly comparableã€‚

 so for exampleï¼Œ one method uses a different backboneã€‚

 which means that the expressive power of your neural network is completely different compared to other methodsã€‚

 and therefore your methods cannot be really comparedã€‚

So it turns out that these studies that actually take all of these methods and compare them in a fair way are very useful and sometimes give us surprises like this where we actually find out that all of these methods perform veryã€‚

 very similarlyã€‚But some tricks that you can actually useï¼Œ first of allã€‚

 use the simple baseline so even contrastive lossï¼Œ triplelet lossã€‚

 and classification losses can perform really well when they're trained correctlyã€‚

Sampling is actually very importantï¼Œ so every method can be boosted if you actually have an intelligent sampling strategyã€‚

And furthermoreï¼Œ you can use other tricks that you will find in the references that we have providedã€‚

 temperature for soft masksï¼Œ freezing batchome layer using multiple centersã€‚

 so these are all kinds of tricks that can actually be used to boost further your neural network training for similarity learningã€‚

And further tricks are that if you use naive ensembleã€‚

 so simply copying your neural networks three times instead of one training with different tripletsã€‚

 you can actually significantly boost performance so this kind of an easy way even though it's expensive because now you have to train three neural networks instead of oneã€‚

 but you can boost your results in an easy wayã€‚And we actually would recommend two good out of the box choicesã€‚

 one is proxy NCAï¼Œ the other one is soft triplet loss because they perform really well out of the boxã€‚

 they do not require massive hyperparameter search and the most important thing they do have code online so if you want to include any of these techniques for similarity learningã€‚

 we recommend this to where you can just download the code quickly train your neural network and get this in resultsã€‚

Now one thing to note that the contrastive loss and triple loss further give a similarity score in addition to the feature embeddingã€‚

 while for exampleï¼Œ if you use a classification lossã€‚

 then the only thing that you can do is you can focus on your feature embedding and you can simply doã€‚

 for example and and search and orderã€‚The images that areï¼Œ for exampleï¼Œ closer to your query imageã€‚

 while the striplet loss would give you immediately also a similarity scoreã€‚And of courseã€‚

 the stronger the backboneï¼Œ for exampleï¼Œ dense netsï¼Œ the stronger are going to be your resultsã€‚

Now let's see some of the applications of Siamese neural networks in Comp visionsionã€‚

So the first thing that we can immediately see is that SiIS neural networks in particularly trained for similarity learning can be actually used for clusteringã€‚

 so in this case we see this image where we see clustering happening on the MIS dataset that is the dataset set of numbers from zero to9 and you can actually see that if you train a Siese network for similarity learningã€‚

 you obtain this nice 10 clusters so you can see each of the numbers from zero to9 color coded so express with one color and you can see the nice cluster of colors so you can effectively say that Siamese neural networks can be used for clusteringã€‚

Now another thing that we can use them for and this is something that a lot of works are focusing on is using them for image correspondenceã€‚

 so the image correspondence problem has been one of the oldest problems in computer vision and in this case the idea is that you want to find interesting points in the image where we can actually find a direct correspondence in another image that is roughly similar so in this case you see all these correspondences which are marked by these red arrows where actually depict the exact same point in one image and the otherã€‚

And this is used for exampleï¼Œ for panoramas teaching so when you do this panoramas with your phone effectively what the phone is doing is that it's extracting all these features from the consecutive images that they're takingã€‚

 it's establishing correspondence between images and then it's computingã€‚

 what kind of warp should I doï¼Œ what kind of transformation should I do from one image to the other so that actually the two images fit together like we can see here in this pictureã€‚

And thisï¼Œ as I saidï¼Œ is quite a fundamental problem in computer vision and it's used in a bunch of computer vision applications such as image teaching or image alignmentã€‚

 as we have seenï¼Œ also object recognitionï¼Œ instance recognitionï¼Œ 3D reconstructionï¼Œ object trackingã€‚

 image retrievalï¼Œ you name itã€‚Soã€‚ðŸ˜Šï¼ŒMany of these applications actually are now being targeted or let's say we're doing research on targeting these applications directly with neural networks and we will actually see some of these applications further along in this courseã€‚

So I just want to quickly go over the classic pipeline for establishing image correspondences so that you can see that it doesn't differ too much from what a neural network is actually internally doingã€‚

So what we would do in a classic image correspondence pipeline is to actually extract manually designed feature descriptorsã€‚

 so you would define a bunch of descriptors that would be useful for creating correspondences you would then extract the features from the two imagesã€‚

And these features can beï¼Œ for exampleï¼Œ the famous Sift descriptorï¼Œ sift key pointsï¼Œ serveã€‚

 even Harry's cornersï¼Œ so most of these are based on image gradientsã€‚

 and they extract locally image properties that you can then find in the other imageã€‚

Of course they do have some problemsï¼Œ they suffer under extreme illumination changesã€‚

 they suffer when the viewpoint is changed a lotã€‚And it's furtherï¼Œ veryã€‚

 very slow to extract all of these features for each pixel of your imageã€‚

So once you have this manually designed featured descriptorsã€‚

 when you have extracted them for both imagesï¼Œ now what you have to do is you have to match these descriptors from the two imagesã€‚

And many of these descriptors are very similar right so remember that we're just looking at very local gradient properties of a small pageã€‚

 so for example you would see this kind of edgeï¼Œ this kind of tilted edge hereã€‚

There can be many places in your image where this edge is actually happeningã€‚

So the first thing that we need to do is we need to filter out possible double matches and keep only the matches that are veryã€‚

 very reliableï¼Œ so this is already a process that usually goes right but it can go wrong especially under these extreme illumination conditionsã€‚

And with this you can do really cool stuffï¼Œ like for exampleã€‚

 this ICCC 2009 paper calledBuilding Rome in a day in which you actually gathered a bunch of pictures of the city of Rome and you would get a 3D reconstruction of the city by just using these images and these image correspondence methodã€‚

So this is really powerful computer vision that we have right hereã€‚

Another question is can we actually do this with neuro networks right so let's start smallã€‚

 let's start by just establishing image correspondences between two patches so in this case we have this patch one patch twoã€‚

And what we want this one to use is ce's neural network right so process these two patches in a similar wayã€‚

 then have a series of fully connected layers that are going to tell you how similar are these two patchesã€‚

 rightï¼ŸSo this is a classic architecture that we have been discussing in this lectureã€‚

 and you can actually use it to train neural network to actually judge patch similarityã€‚

And so this is of courseï¼Œ end to end learning for patch similarityï¼Œ rightã€‚

 you go directly from patch to a similarity valueã€‚And it is really fast to actually allow dense extractionã€‚

 so for each patch in the image you can obtain one similarity valueã€‚

 so for each pixel even in the image you can obtain a similarity value to the other pixelã€‚

And you can actually train it so that it's invari to a wider array of transformationã€‚

 illmination transformationsï¼Œ viewpoint transformationsï¼Œ etc ceteraï¼Œ et ceterã€‚And of courseã€‚

 at the core of this method is a Siamese neural networkã€‚So in the classic semi architectureã€‚

The idea is that there's a series of shared layers that simulate this feature extraction and these shared layers actually are the ones that containã€‚

 for exampleï¼Œ the convolutions that're analyzing our images in parallelã€‚And in the endã€‚

 you would have one decision layerï¼Œ one common decision layerï¼Œ which isï¼Œ for exampleã€‚

 a series of fully connected layers that actually made the decision or predict this similarity valueã€‚

å—¯ã€‚Now we switch the problem of image retrievalï¼Œ so in image retrievalã€‚

 the idea is that you take a picture of anywhereï¼Œ let's sayï¼Œ a famous buildingã€‚

And you want to know exactly where you are so we have a data set of all famous buildings and now what you want to do is you want to compare these famous buildings so you can already start seeing where similarity learning comes into play here right we want to bring all the pictures of the same famous building really really close togetherã€‚

And at the same time we want to separate the images that show different famous buildingsã€‚

 so this is one work that actually uses the contrastive laws for that and creates this descriptor for the building that is exactly train so that a test them you can use simply nearest neighbor search to determine which famous building are you actually looking at when you take the pictureã€‚

Now we can also do other cool stuff with semi neural networks like for example unsupervised learningã€‚

 so unsupervised learning is when you actually do not have labelsã€‚

 but you still want to train your neural network for a specific taskã€‚

And so here the idea is that we're going to use videos and we're going to track the objects in the videoã€‚

 and this tracking actually provides the necessary supervision to train our neural networkã€‚

So it is unsupervised because we have not manually labeled this objectã€‚

But since we have a method that allows us to track this objectã€‚

We can now obtain sort of an automatic label for these videosã€‚

 and this automatic label is what we're going to use to actually train the neural networkã€‚

So the main idea is that if you have a videoï¼Œ for exampleï¼Œ of this biker hereã€‚

 this is showing the biker in different poses and we have pretty robust methods to track these biker so we can actually obtain a bounding box around the bikerã€‚

And now you have these other videos thatï¼Œ for exampleï¼Œ depict dogsï¼Œ depict other types of bikersã€‚

 etc ceteraï¼Œ et ceaã€‚And what we want to do is want to train a neural network that brings all of the images of the biker closer together because we know that is the same objectã€‚

And brings the image of the biker further apart from all the other images of the other videosï¼Œ catsã€‚

 dogsï¼Œ whatever the other videos are depictingã€‚And this is because I am sure that if I track one object through a videoã€‚

 all of the instances that I obtain in all the frames of the videoã€‚

Are going to be of this same objectã€‚And therefore I can use these pairs for similarity learningã€‚

 I can use them as positive samples and I can use all the other pairs as negative samplesã€‚

 so this is exactly what this work is proposing for learning from videos in an unsupervised way using similarity learning and in this case a similarity neural network with a triplet lossã€‚

Nowï¼Œ another very interesting application is that of optical flowã€‚

So optical flow is also one of the classic problems in computer visionã€‚

And the idea here is that you have two consecutive imagesï¼Œ let's sayï¼Œ from a videoã€‚

And you want to determine what is the displacement of every pixel from one image to the otherã€‚

So in this case what you actually want to have as an output is the perceived 2D motion right it's not really the real motion of the object in 3Dã€‚

 but it's the projected motion in the camera spaceã€‚

And how this actually looks like is something like thisã€‚

 so in this case we have this spiker that is moving to the leftã€‚

 therefore the optical flow would predict all these arrows that are pointing leftã€‚

 these blue arrows that are pointing leftã€‚While you have the two pedestrians that are moving rightã€‚

 so the optical flow would be all of these tiny arrows that are pointing to the rightã€‚

 so for each pixel you actually predict the displacement from one image to the next and usually of course this displacement is small because these are two consecutive imagesã€‚



![](img/4b126a9c0436433d6b4aae9ace6be002_2.png)

Now you can see here another representation of the optical flow in this case on the left you have these two superimposed images you see how for example the leg of the tennis player has moved quite a lot from one frame to the next and also the racket and here we have a color representation of the optical flow where the colorã€‚

 the hue from green to blue actually represents the direction of motionã€‚

While the saturation of the color represents how large this motion isã€‚

 so essentially the length of the vector of the optical flowã€‚



![](img/4b126a9c0436433d6b4aae9ace6be002_4.png)

And so one thing that you can do is you can actually train a neural network to directly predict optical flowã€‚

So as I saidï¼Œ the input is to be two imagesï¼Œ so immediately we're thinking about a Siamese architecture hereã€‚



![](img/4b126a9c0436433d6b4aae9ace6be002_6.png)

And indeedï¼Œ you can actually train neural networks to compute optical flow almost in real timeã€‚

 so these are really fastï¼Œ they obtain quite good resultsã€‚And they're actually trainedã€‚

Mostly with synthetic dataï¼Œ so I do recommend you to read this paper flownet quite a milestone for optical flow prediction with neural networksã€‚



![](img/4b126a9c0436433d6b4aae9ace6be002_8.png)

And so the architectureï¼Œ there's two types of architecture that are proposed in this workã€‚

 the first one is not a Siese architectureï¼Œ but actually propose that you take both RGB images and you stack them and this is now going to be your inputã€‚

So your input instead of being an RGB image with three channelsï¼Œ it's two RGB imagesã€‚

 therefore are six channelsï¼Œ the rest of the computation with the convolutional neural networks stays the sameã€‚

And the second architecture is more similar to the architectures that we have been discussing in this lectureã€‚

 which are actually Siamese architecture where you process the two images in parallel with these Siamese branches with share weights and in this case there's a fair amount of parallel computation and then at some point the two feature maps are fused together and then a few convolutional layers are used on these  fused representationã€‚

And now I want to draw attention to oneï¼Œ or let's say two key design choicesã€‚

 but especially one of themï¼Œ which is actually the operation that they use to combine the information from both imagesã€‚

 and this is actually an operation that is very interesting and very much used in neural networksã€‚

So what they propose to use is actually what is called the correlation layerã€‚

And this correlation layer is veryï¼Œ very simpleï¼Œ it contains no parameters to be learnedã€‚

 so it's kind of a fixed operationï¼Œ doesn't add parameters to your neural network because it's a fixed operationã€‚

And simply what it does is it takes two feature mapsï¼Œ F1 and F2ã€‚

 and what it does is it multiplies a feature vector with another feature vectorã€‚

And if you multiply all of the feature vectors of F1 with all of the feature vectors of F2ã€‚

What you have in the end is a matrixã€‚Which actually contains all the matching scores of the features of F1 with the features of F2ã€‚

So just to be more preciseã€‚What I have here is a flattened version of F1ã€‚

Where each FiI is a representation for each of the pixels of my imageã€‚When I say each of the pixelsã€‚

 of courseï¼Œ this representation comes from applying a series of convolutions to the imageã€‚

 so it's not going to be exactly one representation for each pixel but actually the reduced version after all the convolutions and poems so now my image is represented by this featureã€‚

A representationation of height H and with W and for each of these positionsã€‚

 which of course represent a patch in the original imageã€‚

 I actually have a feature F that represents this positionã€‚

And now what I want to do is I want to multiply each location F of F1 with each location of F2ã€‚

 and this is exactly what is happening here with this operationã€‚

 this is simply the multiplication elementwise multiplication and then summing up to obtain the score SIJã€‚

And this score actually represents how similar these two locationsã€‚

 I and J of feature  one and feature two areã€‚So this correlation layer is actually very powerful when we want to compare two imagesã€‚

 when we want to compare the embeddings of locations in one image and locations in another imageã€‚

And againï¼Œ the important thing is that this is a fixed operation right I just did this multiplicationã€‚

 dis sumationï¼Œ there are no learnable weights hereã€‚

 so I'm going to be able to back propagate through this operation but I'm not going to update any weights hereã€‚

 so very cheap operation to include in your neural networksã€‚

And the matching score will represent actually how correlated two feature vectors areã€‚

 and this is a very powerful representationã€‚So this was first presented to actually find image correspondencesã€‚

 right the problem that we were talking about where we want to find correspondences between these two imagesã€‚

 in this case we want to find semantic correspondences right like front wheel with the front wheel of the other motorbiã€‚

 you can actually train a same' neural network with a correlation layer to actually output these correspondencesã€‚

Andã€‚With these correspondsï¼Œ now what you can do is find the transformation from image A to image B so that you can see whether this transformation actually makes senseã€‚

And this actually gives you really nice semantic correspondences betweenï¼Œ in this caseï¼Œ the birdã€‚

 the wineï¼Œ the carï¼Œ so you can actually train it for a variety of objectsã€‚Okayã€‚

 thank you very much for attending the lecture on Siamese neural networkss and similarity learningã€‚

 stay tuned for the next lecturesã€‚

![](img/4b126a9c0436433d6b4aae9ace6be002_10.png)