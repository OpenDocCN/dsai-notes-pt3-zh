# P9ï¼šL9 - ç¥ç»ç½‘ç»œå›¾åƒä¸è§†é¢‘æ¸²æŸ“ - ShowMeAI - BV1Tf4y1L7wg

Helloï¼Œ welcome everybody to the advancedanced deep learninging Lectureã€‚ğŸ˜Šã€‚

Today I wanted to follow up essentially what we have talked about in the previous lectures on generative models right so we have seen a lot of things on GNs we have seen autoaggressive models and we have also seen how we can use possibly GNs on videos and so on right today I want to talk a bit about rendering in general specifically leading into neural rendering and what that actually means I mean it's a term that has been come up very recently basically in the last year or so there have been veryã€‚

 very many research papers around thatã€‚Andã€‚First of allã€‚

 I wanted to actually twist a step backwards now I want to go away from neural networks for a second and I wanted to explain quickly what rendering means actually and where this comes from so rendering is a thing that comes mostly from computer graphicsã€‚

ğŸ˜Šï¼ŒAnd that's essentially how we make videosã€‚In movies or how we do video games and basically how we can go from a synthetic scene description and how we can create images out of these onesã€‚

 So if you have taken some graphic coursesï¼Œ what you will learn first is you basically have a scene description and the scene description is a 3D description that tells you what is in your environment right so you basically have a bunch of objects here in the sceneã€‚

 you have a bunch of shapesï¼Œ each of these shapes have different materials that defines their appearanceã€‚

 you have lightï¼Œ you have wellï¼Œ the geometryï¼Œ of courseï¼Œ typically it's a triangle meshã€‚

 often you have animationsï¼Œ when you have people in it and so onã€‚

 and all these kind of things they make up your 3D scene representation basically right so it defines what's in the sceneã€‚

ğŸ˜Šï¼ŒAnd now in order to take the scene representation and make an image out of itã€‚

 you need to define the camera parameters and you have two types of camera parameters you have the intrinsic camera parameters that's basically how the projection matrix works right how do you go from a current viewpoint into a 2D image plane and that essentially defines like know things like depth the field would also be in here motion blur and stuff like this you would have here but in practice often you only have a focal length and you have a principle point that's what people using computer vision of so it's a bit of an oversimplified pinhole camera model of the real worldã€‚

And the second thingï¼Œ in addition to the intrinsics are the camera extrinsicsï¼Œ the postã€‚

 this is the camera viewpointã€‚ This is where do you want to place this camera hereã€‚ğŸ˜Šã€‚

In order to look at the sceneï¼Œ so it's basically changing the viewpoint rightã€‚

 in practice you have6 degrees of freedomï¼Œ you have three for the rotation and three for the translationã€‚

That's enough to define where to put the current camera location right and again if you're putting all of these three things here together right so we have the scene description here we have a camera viewpoint here so where do we look at the stuff and then we have the project goes from this thrust them here into this image plane here and that tells us basically how do we generate an image from the current 3D description right and this is a process that has been examined in computer graphics for decades andã€‚

ğŸ˜Šï¼ŒThere's various techniquesï¼Œ how to goï¼Œ for instanceï¼Œ from this 3D scene then to the 2D imageã€‚

 you can do restorationï¼Œ you can do rate racingã€‚Various different ways in terms of how to define materialsã€‚

 there's different waysï¼Œ how to define lightingï¼Œ geometry and so onã€‚ So all these kind of thingsã€‚

They they all gonna be researched in graphics right and people have made a lot of progressã€‚

 and most of these things what they do is they kind of they have this idea of photoistic image synthesisã€‚

 here's a very popular paper from computer graphicsã€‚

 the rendering equation that essentially this integral here defines light transport in the sceneã€‚

 So if you have light from a source from a light source being emitted these light raceã€‚

 they all bounce around in the sceneï¼Œ they're gonna be reflected and refracted on the objectsã€‚

 And eventually that determines the appearance of the respective objectã€‚

 then you have a projection matrix and go to to the current imageã€‚ğŸ˜Šã€‚

Pracically you have like this is the 3D definition rightï¼Œ then you you get theã€‚

The shape accordingly you texture it and you evaluate the lighting and you're going to get these kind of resolved images This is from avatar in this case right this is what people are doing all the time like again for entertainment purposes for video gamesã€‚

ğŸ˜Šï¼ŒMovies and so onã€‚I should say this process is very well understoodã€‚

 So if I'm going to give you a really good description in 3D and I know all the parameters I need for rendering with state of the art graphics methodsã€‚

 you can get actually pretty good looking images and these images are actually looking very realistic so they're going to be photo sure this is not a real character hereã€‚

 but if you're taking some things like cars and so on I can render them to a degree that they are basically indistinguishable from realityã€‚

 So if you're looking at at the images of a news of like a photo magazine or so today most of these rendered images renderings and they're not real photos actionã€‚

ğŸ˜Šï¼ŒSo this process works pretty wellï¼Œ but the big challenge you're going to have there is in computer graphics today is how do you get the needï¼Ÿ

How do you get the 3D content So this 3 representation like I mean setting up a camera position is easyã€‚

 but the 3D content that's the hard part in practice what you need is you need the geometry right that defines your scene you need textures that are met on top of this geometry and you're going to have lighting and material parametersã€‚

And for these kind of thingsï¼Œ what you typically see at the very end of either video game of a movieã€‚

 you see in the creditsï¼Œ you see a lot of people who have done thatã€‚

 these are the content creation artists and this's typically for a Hollywood movie today everything you see cheap basically so you have like4 or5600 people creating this content for you and geometry could include a little bit more than just the environment here it could also be you have animation right you have facial animation parameters and stuff like that so all of these things are going to be actually in in the 3D geometry in the 3D content and this has to be manually createdã€‚

One of the motivations what people had from graphics in a whileï¼Œ they thoughtï¼Œ wellã€‚

 maybe we can automatically generate this process and we can reconstruct all of the material lighting and texture and geometry parameters from real world images and this is what computer vision is basically trying to do writing computer vision you have the opposite problemã€‚

 you have a 2D image and you're trying to get back to the 3D representationã€‚

 3D understanding and stuff like thatã€‚ğŸ˜Šï¼ŒAnd this is a thing in computer visionã€‚

 there has been a lot of work trying to do thatï¼Œ like trying to do 3D reconstructionï¼Œ multi stereoã€‚

 slamM methodsï¼Œ all these kind of things that that's what people have been working onã€‚ğŸ˜Šï¼ŒAnd this wasã€‚

 for instanceï¼Œ a veryï¼Œ very important paper here was building room in a dayã€‚

Very famous paper in computer vision in 2009ã€‚ This was like you knowï¼Œ 10 years agoã€‚

 this is structure from motion work with the bundle of cheststerã€‚

 And what they did is this is from Trovniã€‚ğŸ˜Šï¼ŒThey took a bunch of from the city of Tuborrovnikã€‚

 they took a bunch of touristic images here and what they did is they they basically found featuresã€‚

 they ran their bundle chester and they got these point clouds as a result of the structure of emotion processã€‚

 right andã€‚You can already seeï¼Œ wellï¼Œ you get the structureã€‚

 you see here all these frost them at the bottomã€‚ they're visualizing where the original images came fromã€‚

 you get a rough intuition what the 3D stuff looks likeã€‚

 but it doesn't look nearly as good as if you hired like a couple of hundred artists like in a video game or in a movie So that's still a challengeã€‚

 So if you're talking about 3D digitizations this is often what what I'm trying to refer to the gap between graphics and visionã€‚

 if I'm looking at computer graphics models they look like thisã€‚

 This is a rendered image from a car and it looks pretty perfectã€‚ and on the other sideã€‚

 if I look at images in computer vision papers this is something I'm very excited about right nowã€‚

 but they don't look so greatã€‚ And the reason why they don't look so great is because this is this difficult inverse reconstruction process these are actually a bunch of Liar reconstructions from Kittyã€‚

 it's a very famous data set in computer vision but you see there's a big visual discrepancy between those two thingsã€‚

ğŸ˜Šï¼ŒAnd now the big challenges of courseï¼Œ how could we get from hereã€‚

 how can we make this stuff here look like that one and this is the big opportunity here that I would like to highlight with neural rendering right this is the key insight is maybe we can figure out how can we use a neural network to make this stuff here and look like that stuff we've already seen this a little bit in the last lectureã€‚

 but this is what I want to follow up a little bitã€‚ğŸ˜Šï¼ŒSoï¼Œ if you're talking aboutã€‚

Traditional computer graphicsã€‚On one hand sideã€‚And on the other hand side we can talk about deep learning methods and specifically we talk about the generative models that we have seen in the previous and lecturesã€‚

 so obviously both of these things have very similar goals right what you're trying to do is you have some sort of description of an environment or of some sort of image and you want to generate an image or possibly a video as an output rightã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_1.png)

In computeruter graphicsï¼Œ I've just given you kind of aï¼Œ you knowã€‚A 10 minute crash courseã€‚

 what you can doï¼Œ you basically have these 3D models right you have the texturesã€‚

 the shading and then you synthesize you get a synthetic image out of it and this is like an example for instance from Star Wars where they had the actor being remodeled and then synthetically inserted into the movie this is kind of the decomposition here of the different layers in the lighting pipelineã€‚

ğŸ˜Šï¼ŒAnd on the other handã€‚ğŸ˜Šï¼ŒAnd againï¼Œ here we have like there's artists that created these modelsã€‚

 there's a lot of many labor that went into this kind of stuffã€‚ğŸ˜Šã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_3.png)

And on the other sideï¼Œ we have things likeã€‚The progressive game paper right where we have our discriminatorã€‚

 our generator right and we have this loss function that tells us is it in the distribution of real images and in this case we don't have a lot of control for this method hereã€‚

 but rather we have we have this la vector Z where we be taking this vector in and we generating the images output So these are very different paradigms here were purely learning from data we just want to mimic our dataã€‚

 we have very little control but we also don need a lot of manual effort right we just have a neural networkã€‚

 this neural network learns this distributionï¼Œ I can synthesize new images whereas here's the oppositeã€‚

 I have all control I have a 3D model I can use the 3D model and yeah I can basically go ahead and yeah do whatever I want in the resolving output right but it costs me a lot of effort right So this is kind of the drawbackã€‚

ğŸ˜Šï¼ŒAnd the idea of neural rendering is essentially and this is a very broad definition what I'm making right now you will see like people innder a bit of different things with it one thing you could say about neural rendering is you want to do something like novel viewpoint synthesis so instead of having this explicit 3D content that I've just explained to you you could train a conditional G or a conditional generative network that takes as input a camera rightã€‚

ğŸ˜Šï¼ŒPosition and implicitly in the neural networkï¼Œ it encodes all the entirety of the scene descriptionã€‚

 like the lighting materialsï¼Œ shading and all of these kind of informationsã€‚

 they could be all encoded in a black box neural networkï¼Œ rightï¼ŸğŸ˜Šï¼ŒYeahï¼Œ then and then as outputã€‚

 you could basically get the novel fu and all you have to do is you have to change the6 degree of camerasã€‚

 rightï¼ŸAnd it's kind of this idea of moral renderingï¼Œ and now we kind ofã€‚

We're taking a conditional neural networkï¼Œ rightï¼Œ a conditional generative neural network and we conditioning it on the current camera pose and at test time we want to basically generate a video as output that is corresponding to the current camera pose that we're feedingã€‚

 so instead having this graphics pipeline with the rate is the reststerizer we just having this black box network that gets us the respective outputã€‚

ğŸ˜Šï¼ŒNowã€‚One way to do that is we already saw that actually is actually relatively straightforward with the techniques what we have already learned and we can do novel viewpoint synthesiss with Psto picks for instance right so psto picks is our conditional again and what we do is for the ground truth data for training what we need is we need images with the respective camera poses right so what we could do is we take a bunch of photos of the scene right and we know with some sort of structure for motion method we know the current camera pose and all we're doing right now is were training this conditional again the condition is the current camera position and then we have the re renderndering loss that tells us for this camera position you have to produce this kind of image rightï¼Ÿ

ğŸ˜Šï¼ŒAnd then the hope isï¼Œ let's say we overfitting this network to one specific sceneã€‚

 So let's say for one sceneï¼Œ we have a couple of hundred thousandsï¼Œ maybe of imagesã€‚

We train on this one scene now for testing you would still have the same scene so it is overfited to the sceneã€‚

 but we give the network of unseen pose to generate new images from these different poses right so you want to have like a camera check that goes around an object does so onã€‚

ğŸ˜Šï¼ŒSo this is still has to channelizeï¼Œ rightï¼Œ thisï¼Œ This network has to figure out how to extrapolate or interpolate between the current camera poses andã€‚

As I mentionedï¼Œ camera pose it's a6 degree free in vector right so we have a sixdial vector and it turns out six dimensions is actually pretty high already it's an untrial dimensionalityã€‚

 We have to deal with itã€‚ So it's not such an easy problem but if we're doing this very not with Ps to picksã€‚

 It's pretty straightforward right we just we just need to get this input dataã€‚

 We need the pose for itã€‚ğŸ˜Šï¼ŒWe canstrain the rewriting lawsã€‚

 and then we generate new images for new postsï¼Œ rightï¼ŸBy running the network with new conditionsã€‚

And let's say for simplicity we're doing this on synthetic data so were going ahead right we having in here we have the spinning cube and we used original graphics methods to create the ground truth data that makes it a bit easier we have ground truth poses we know the current like how this cube here is spinning andã€‚

ğŸ˜Šï¼ŒWe also we also you knowï¼Œ know that have the ground truth images that we rendered to begin with and now we we training this Ps to Ps network such that it's conditioned on that respective targetã€‚

 rightï¼ŸYou seeï¼Œ it looks not perfectï¼Œ butï¼Œ you knowã€‚

 like it's pretty straightforward to train and you're getting results that look like these ones hereã€‚

 This is just a naive conditional ga that is conditioned on the current fuel point and it's kind of replacing this complicatedã€‚

ğŸ˜Šï¼ŒOh complicatedlicï¼Œ the graphics pipelineï¼Œ rightã€‚ğŸ˜Šã€‚

And has an end to end neural network here that generates the respective output renderingsã€‚

And we have in fact seen other versions of this alreadyï¼Œ not just for novel viewpointsã€‚

 novel viewpoints was now one example where the conditioning is only the camera poseã€‚ğŸ˜Šã€‚

But you could go ahead and sayï¼Œ wellï¼Œ instead of the camera poseã€‚

 be conditioning it on different thingsï¼Œ such asã€‚Faces which we have seen with the video pottery paper the video potterers is basically the conditioning here is the 2D image of the faces right the positions in the ice and then you have ps to picks that makes kind of a resulting output rendering out this or everybody dance now same thing right so we here a conditioning it on the human skeleton and picks to picks again out of this detected skeleton here at the bottom what do you see it makes a realistic image out of it againã€‚

So you can do this as various different conditionings but you will see sometimes it works betterã€‚

 sometimes it works worse so this is like the basic idea of neural rendering right now so we're replacing this graphics pipeline with all the methodologies that we have seen now in the previous oneã€‚

 twoï¼Œ three lectures right so so far I haven't really told you anything new in terms of you know how to do neural rendering probably arem more or less just reformted the problem from a different perspective right so now we're looking at it from a graphics standpoint and we're trying to replace the standard graphics pipeline with our neural network to get the rendering done for us and this is what we call neural renderingã€‚

ğŸ˜Šï¼ŒBut I wanted to quickly go back to this example of the novel viewpoint synthesis and it turns out this is actually a pretty complicated setting here and in this case we have the spinning cube right and the spinning cube actually has text on it so it has very fine scale detail so the network has to actually learn how to go from the current camera pose to the respective target renderingã€‚

ğŸ˜Šï¼ŒAnd this turns out to be pretty challengingï¼Œ especially for theseã€‚Fine scale text builds hereã€‚

 rightï¼ŸAnd the reason why this is actually really challenging if you're going back to our neural networkã€‚

The big challenge is we have a 2D convolution in every layer a bunch of 2D convolutionsã€‚

 we have a series of 2D convolutions that make up that have to learn kind of this 3D space now right it's a pretty challenged thingã€‚

 We have a 3D space but're kind of abusing a 2D network to try to learn it And as a result you see that you have these like swimming and flickering artifacts right it's not perfect it doesn't look perfectã€‚

ğŸ˜Šï¼ŒAndï¼Œ and one idea to remedy thisã€‚ and this is a very recent trend in neural networks is to sayã€‚

 wellï¼Œ maybe these 2D convolutionsï¼Œ they are not so greatã€‚ Maybe we want to goã€‚

To 3D and this is what what Dboxoxels is doingã€‚ This is one of the yeah first tables that looks at this andã€‚

The main idea is basicallyï¼Œ it's very similar toã€‚To picks to picks from a high level conditioning partã€‚

 but the difference is why do we have to learn these 3D operators with 3D convolutions right why don't we do this directly in 3D so the network has a much easier time to handle thatã€‚

We know how 3D transformations work right if they have a camera postï¼Œ it's a rotational translationã€‚

 it's a six do vectorï¼Œ we know how to apply this toï¼Œ let's say a voxel grid or a mesh or rightã€‚

And the idea is we can incorporate these operators directly into the architecture and as long as they are differentiable you can still learn the missing pieces and like this is what this deboxoxel paper is doing and this is an example application that also works for all little viewpoint synthesis so in this case again give me the rigid posts of the camera and then generate me a new viewpoint for thatã€‚

ğŸ˜Šï¼ŒSo if were having a simplified pipeline of deboxsï¼Œ it's still very similar than beforeã€‚

 so the idea is at training timeï¼Œ what you have is a source imageã€‚

You have a 2D network that extracts featuresã€‚The standard unitã€‚

 you're taking this unit and you're protecting these features from 2D to 3D this is now a boxal grid in the middle hereã€‚

 The way I have to do that is I have to basically from this image I need to know the postã€‚

 this is a no from the training dataï¼Œ this is this R&Tï¼Œ this is my training Richard postã€‚

And I need to know how to project from 2D to 3D that when I know with the intrinsic parameters of the camera matrix right then I run a bunch of 3D convolutions in 3Dã€‚

 this is a standard it's pretty much a standard 3D unitã€‚Architectureã€‚

 there's nothing super special about it except that in pixels it runs and voxels now and then what you do is you reprojecting these three features to a new few So now for a respective target viewã€‚

 you have again a rotation translation for your camera postã€‚

 you have a projection layer goes from 3D to 2D againã€‚

 you know they're from the intrinsic you run a bunch of 2D convolutions and then you force this network or you constrain this network to produce this output again and this whole thing the idea is this can be trained end to endã€‚

And what you're learning is basically you're learning the feature extractors from hereï¼Œ here to hereã€‚

ğŸ˜Šï¼ŒBut you don't have to learn the 2 d 3D projections and you don't have to learn the rotations and the translations because they implicit they explicitly encoded into the network architectures alreadyã€‚

Nowï¼Œ this is already pretty interestingã€‚ There's one small detail that I ignored here right nowã€‚

 This is a bit of a simplified version of this D box will workã€‚

 And one problem that we gonna have later is we have to checkã€‚ We don't have the depth data here yetã€‚

 So this is something there's a bit of more stuff missingã€‚ So theã€‚

 the whole pipeline would look something like thisã€‚ğŸ˜Šï¼Œå—¯ã€‚In this caseã€‚

 we have actually again the 2D partï¼Œ we have the 2D networkï¼Œ we get 2D featuresã€‚

 we're lifting these features to 3Dã€‚ğŸ˜Šï¼ŒWe having some 3D operators hereï¼Œ basically the 3D unitã€‚è¯¶ã€‚

Then you're reproing to 2D now here's an occlusion network I'll explain to this in a secondã€‚

 this part actually enforces depth the depth prediction because you don't have the depth so this part here is actually not so straightforward because you don't know the depth how to project this and so you need to know whether the current voxel is visible in the current 2D pointã€‚

And then what you're doing is you're considering some output and you have a discriminator loss at the end of the day that tells you is this output in the distribution of this current objectï¼Ÿ

ğŸ˜Šï¼ŒAnd this whole thing is being used for trainingã€‚ so so this part here is all the training in a testing you would only use that part and looks like this here in this case we have a test timeã€‚

 the whole scene or the whole object description is encodedã€‚Into the network hereã€‚

 into these featuresã€‚ And now what you can do is you feed in different rotations and translations to force the network to produce different target output results right this is how you test and and now that's pretty nice because you's starting from a 3D representation at test time and all you have to do is you have to change the respective camera viewpoint by explicitly feeding it it So network doesn't have to learn that complicated transformation anymoreã€‚

ğŸ˜Šï¼ŒAnd I'll show you in a second that this is actually working muchã€‚

 much better and the reason why it works betterï¼Œ you don't have to learn the 3D operatorã€‚

 you already have it implicitly encodedã€‚ğŸ˜Šï¼ŒOkayï¼Œ one thing I this occlusion network I mentioned we don't have the depthã€‚

 The way this works isï¼Œ let's say you have this feature grid in 3D hereã€‚

 you have the camera pulse hereã€‚So along this rayï¼Œ you know which box will touch this rayã€‚

And what you can do is you can think about this as canonical viewã€‚ So you just have this voxelã€‚

 that voxelï¼Œ that voxelï¼Œ that voxelï¼Œ that voxel along this rayï¼Œ you just put it in an canonical gridã€‚

 It's just the reordering of of the frames hereï¼Œ And now what you know is there's gonna be one valueã€‚

 which is the visible valueï¼Œ this is the visible reasoningã€‚

 so to say right know what you need to know which pixel is the visible one basically right you don't know itã€‚

 but you can learn itã€‚ So all you're doing is you're running simply a softmax here that tells you which one is the right probability And this is basically a depth estimation network what's happening hereã€‚

 So this network implicitly estimates the depth by knowing that the reproion error needs to be needs to be goodã€‚

 So there's no explicit loss here that tells you what this occlusion network should beã€‚

 The only thing it knows that oh this reproion needs to look likeã€‚

 whatever it is on the counterpart of the real examples rightã€‚ğŸ˜Šï¼ŒOkayã€‚

So this one works surprisingly wellã€‚ If you're comparing thisã€‚

 you're getting results that look like these onesã€‚ So againï¼Œ here at the bottomã€‚

 we see the Ps to pick baselinesã€‚ğŸ˜Šï¼ŒAnd here we see the Dboxoxels comparison like if you're looking at this first spinning cube hereã€‚

 you see that suddenly you can see the letters and everything else right so it looks significantly better than here And againã€‚

 this is the same amount of training data roughly the same network size except now you have these explicit 3D operators encoded in that representation right that you don't need so much data if you know something already that's pretty good you can put this in the network and make it differentiable and then be more efficient So that's a very recent trend actually we will be seen a couple of times now thatã€‚

ğŸ˜Šï¼ŒYeahï¼Œ that that these these rendering networks and so onã€‚

 you kind of have stuff you know how to do like some part of the rendering pipelineï¼Œ you knowã€‚

 like3 the operatorsï¼Œ like projections and so onã€‚ You know themã€‚

 you just make them differential and put them in a networkã€‚

 You don't have to learn them because you already know how these things workã€‚

 Like if you want to add two numbersï¼Œ rightï¼Œ you don't learn how to add two numbersã€‚

 You just have an operator to add themã€‚ But other thingsï¼Œ you don't quite know how they workã€‚

 like the shadingï¼Œ the lighting variationã€‚ Sometimes this is hard to capture for construction purposesã€‚

 And this partï¼Œ you let the network take overã€‚ And this is kind of this idea of neural rendering that you can mix these ideasã€‚

 you have explicit representations and implicit representationã€‚ğŸ˜Šã€‚

So here's another direct comparison against the very two data sets hereã€‚Pix to picks and deboxsã€‚

 rightï¼Œ So it looks just much betterã€‚ If you're looking closelyã€‚

 you see it's also not perfect on the edges hereã€‚ So here it's a little bit chiteryã€‚ğŸ˜Šã€‚

And the problem thereï¼Œ basically isã€‚That there's an explicit voxel sizeã€‚

 This voxel size is this cube is only in a 32 cube volumeã€‚

 And you see the voxel representations hereã€‚ And that issue is something you can see in the final renderingsã€‚

ğŸ˜Šï¼ŒSo if we had a higher out resolutionï¼Œ it would be betterã€‚

 but unfortunately there's going to be some limit in terms of what it can fit actually onto the GPUã€‚

But that's a very cool insight hereã€‚ And the the voxel groupã€‚

 This lifting from 3D to 3D works really greatã€‚ Nowï¼Œ take care for like temporalfold coherencyã€‚

 You have the 3D operators implicitly encodedã€‚ They need a bit differentiableã€‚ Of courseã€‚

 But it works pretty wellã€‚ this was done for normal fewpoint synthesisã€‚ In principleã€‚

 you could also do this for like dynamicsã€‚ You could just move the v aroundï¼Œ rightã€‚

 That's kind of the high level ideaã€‚ limitation right now here was that it wasã€‚

 it was limited due to the dense3D voxel gridã€‚ğŸ˜Šï¼ŒSo this was one of the methods I wanted to show you a bit in more detailã€‚

 but now I want to go backã€‚One level bagï¼Œ rightï¼Œ one a little bit more high levelã€‚

 And this was done on boxelsã€‚ But in principleï¼Œ you can do this forï¼Œ for other thingsï¼Œ tooã€‚

And they have been actually a couple of different papers that appearedï¼Œ you can do thisã€‚

 so on Wuxel grids we've just seen thatï¼Œ rightï¼ŸğŸ˜Šï¼ŒYou can do multi multiplay imagesã€‚

 you can do some compositing between the imagesï¼Œ you can do it completely image basedã€‚

 you can use point cloudï¼Œ there was a neural point based graphics paperã€‚Very recentlyã€‚

Very cool paperã€‚ And I'll I'll show you references laterã€‚ You can look these ones upã€‚

 I don't have time to go into all of themï¼Œ but very similar ideaã€‚ instead of having voxelsã€‚

 you have points or you can actually do something more extremeã€‚

 you can do completely implicit functionsã€‚ This is one thing that has been shown very recently to sayã€‚

 okayï¼Œ well it's let'sã€‚ğŸ˜Šï¼ŒUse the 3D part for the supervisionã€‚

 but the network itself is just a fully connected layer basically it sounds pretty cool when you name it implicit functions but in practice that's just a fully connected layer and then theres different versions of renders like this one we've just seen volumetric graybased that's what this occlusion network was basically doing if you take multiple images decomose them that's like alpha compositioning restization for points you would use splitting and for implicit functions you can do stuff like like sphere tracing and so on rightã€‚

ğŸ˜Šï¼ŒSo basically we have these two aspects we have on one hand we have the scene representation and we have the differentiable render and this is one thing that will come a little bit this term differentialiable render you will hear a couple of times now because basically what you have to do now is you have to train this whole pipeline end to end right and from going from these 3D feature voxels like in D voxelsã€‚

 you have to basically learn these features by this project matrix and this is essentially a differential renderã€‚

 it could be array basedï¼Œ it could be also restization basedã€‚ğŸ˜Šã€‚

There's a couple of pros and cons between these representations right I you doing this kind of stuff on imagesã€‚

 you have very fast rendering high quality consï¼Œ you don't have so much flexibilityã€‚

 You only have a two yeah you have only 2ã€‚5 D size right that's a drawbackã€‚ğŸ˜Šï¼ŒThe deboxoxelsã€‚

 it's true 3Dã€‚ That's greatã€‚ We have all the controlã€‚ We don't have to learn most of the thingsã€‚

 but the memory grows veryï¼Œ very quicklyã€‚' that's the drawback right if you're doing the restorationization based well then it's high qualityã€‚

 I didn't go into too much detail hereã€‚ This is maybe something you should you should look up yourselfã€‚

 but it's it's in the same lines right So you also have to differentiate again through the restization process and then you can do these these kind of things and use like blending weightsã€‚

 how to combine different imagesã€‚ğŸ˜Šï¼ŒPoint cloudsã€‚Point clouds in principle are pretty goodã€‚ğŸ˜Šã€‚

Yeah they are high quality to some degreeï¼Œ but if you don't if you miss a bunch of points you're pretty screwed so you need good quality of the structure promotion motion to begin with if you're missing points you don't have it in the representation and the network can't just hallninate points where they aren't points that's a big problem of thisã€‚

ğŸ˜Šï¼ŒBut there's a couple of really cool papers there was a group from Scalek and Samsung and I in Moscow they have done this a rural point based graphics paperã€‚

 I would encourage you to look that one upï¼Œ this is a really nice paperã€‚Okayï¼Œ implicit functionsã€‚

 that's something we haven't talked too much aboutã€‚

 And this is what I wanted to talk a little bit right nowã€‚

 These ones are all explicit representations and a very new trend in neural rendering is to just get rid completely of the explicit representationsã€‚

ğŸ˜Šï¼Œå—¯ã€‚And yeahï¼Œ the idea is basicallyï¼Œ so you have a camera rightã€‚

 and you're shooting a ray and the network tells you how far can you shoot the rayã€‚ğŸ˜Šã€‚

Until you find the surface pointã€‚Nowï¼Œ the way you would implement this is you take a scene representation that is literally just an MLP right it gets basically as input it checks where you are in the viewpointã€‚

ğŸ˜Šï¼ŒSo this point here right nowï¼Œ this 3D pointï¼Œ it has a directionã€‚

 it shoots this point in this directionï¼Œ and it tells you how far can you stepã€‚

Before you find the surface pointã€‚And this is kind of like sphere tracing if you've heard of thisã€‚

 So sphere tracing means if you're in a distance field or so you can make a step equal to the distance and you're guaranteed it to not hit the surface before thatã€‚

 And this is kind of what this render here learnsã€‚ğŸ˜Šã€‚

And the idea is that eventually once you iterate this processã€‚

 eventually you will hit the surface and this is completely encoded in this implicit network so there's going to be it's kind of like a query you' just shooting a array you're asking how far can I go before I go to the next thingã€‚

 there's no explicit representation anymore you only have this iterative query pointã€‚ğŸ˜Šï¼ŒRightã€‚

 and this actually can generalizeã€‚ So basicallyï¼Œ once you have enough raceï¼Œ you're shootingã€‚ğŸ˜Šã€‚

Then at some pointï¼Œ the network within a sceneï¼Œ it knows how to extrapolate and has some basic generalization how to look at different thingsã€‚

And they had a couple of coolã€‚ This was a nerve paper from last year so they had a couple of cool results so they basically can do stuff like reconstructions from a single image that looks like looks like this one so they have this image as input and now they can channelize and can hallucinate what the respective surface looks like right and this is kind of the normal field by getting it looks kind of cool it's a very interesting idea so basically they saidã€‚

 okayï¼Œ let's not use an explicit representationï¼Œ It's just use this implicit notation of a multilayer perceptron and query how far can I shoot the ray in order to hit a surfaceã€‚

ğŸ˜Šï¼ŒAnd veryï¼Œ very recentlyï¼Œ there's another follow up payã€‚

 I'm very excited about that one because they showed really good resultsã€‚

 It's called Nerf neural Raance fieldsã€‚ This is Pay from Berkeley and Googleã€‚

 And what these guys are doing isã€‚ğŸ˜Šï¼ŒThey're having a very similar ideaï¼Œ they also have a camerasã€‚

 they have a new plane hereï¼Œ far plane hereï¼Œ this defines the frost themã€‚

And they also have an implicit scene representationã€‚ So what they're saying is basicallyï¼Œ againã€‚

 shouldnni arrayã€‚ğŸ˜Šï¼ŒAnd then they have so in this case they have an MLP rightã€‚

 they have a positional encoding that tells you where it isã€‚

 they have a few direction and then they're going to predict this radiance field along this ray what the color is basically so they go just from from r6 to from r6 to R3 and this is again all encoded in a multilayer perceptionceptã€‚

 they have a couple of layers of courseï¼Œ but there's no explicit 3D structureã€‚ğŸ˜Šã€‚

And what they have done is they have the renderer What it's doing is it's just take my current ray Well I have my camera pointã€‚

 I have my ray direction right that's this R6 here my few direction and then this network just predicts you along this few direction what is the current color going to look like but you need this positional encoding This is a bit of a detail I'm ignoring right nowã€‚

 but what I would like you to understand is this is just an implicit network that basically for giving ray tells you what the color along this ray isã€‚

ğŸ˜Šï¼ŒWhat's interesting about this network is they don't aim to do any generalizationã€‚ğŸ˜Šã€‚

They only basically for every surface point in 3Dï¼Œ this network kind of memorizes the color right So if you have a arrayã€‚

 you' learn this mapping from the rate to the surface point and from to the surface point it knows what the color should beã€‚

 and they also do novel viewpoint synthesisã€‚ğŸ˜Šï¼ŒAnd they have really impressive resultsã€‚

 so what they do is they record also they took highcol images in fairness so that's why it looks pretty goodã€‚

 they took DSLR images of a couple of objects here and then they use the network to generate to basically interulate the view points it's only interlation network there's no generalization going on but the results look like this and it looks actually pretty decentã€‚

ğŸ˜Šï¼ŒIf you're comparing this to traditional computer vision methodsã€‚

 you're not getting a point lot anymoreã€‚ but in this caseï¼Œ you're getting actuallyã€‚

Pretty good reconstructionsã€‚ And these images are not these viewpoints are not exactly in in the training setã€‚

 rightï¼Œ It's a good questionï¼Œ Like how much can it extrapolate from the training viewsã€‚

 But from a bunch of viewsï¼Œ you can now move around the object in a free viewpoint notationã€‚

 And this is kind of a nice thingã€‚ So I really encourage you to have a look at thisã€‚

 This paper came outã€‚ğŸ˜Šï¼ŒI don't know like two months ago soï¼Œ and it's pretty interestingã€‚

 I really enjoyed reading itã€‚å—¯ã€‚Yeahï¼Œ so these implicit functions is kind of a new thing that is going in addition to thatã€‚

 in this caseï¼Œ we're going to have sphere tracingã€‚ğŸ˜Šã€‚

Wwhich is kind of the renderer or some volumetric tracing rightï¼Œ It's super high super high qualityã€‚

 If you have good data as inputï¼Œ channelization is basically not happening right nowã€‚

 So you're not like doing something like completion it's still veryï¼Œ very very challengingã€‚

 it's very expensive to trainï¼Œ takes forever basically shooting rate by rate right and for every rate you're giving is one training sample right It takes a while it takes also a volatile render because againã€‚

 you have to evaluate for every pixel is one forward pass basically in the networkã€‚ğŸ˜Šã€‚

UmSo that's the consï¼Œ but it's this is very recent and if you're interested in research directionsã€‚

 I encourage you to have a closer look at thisã€‚ğŸ˜Šï¼ŒYeahï¼Œ I wanted to show you one final paperã€‚

 and this is actually a paper from our groupã€‚This neural texture paperã€‚

 you can basically do neural rendering when you encode the features not in a volume as wellã€‚

 you can encode it if you encode it in a textureã€‚So if you have3 geometry that has been reconstructed with multiy stereoã€‚

 looks like thatï¼Œ so you have a surface textureã€‚That is embeddedã€‚Into intoto the geometryã€‚

So you have a density V parameterizationï¼Œ but instead of having RGPE textureã€‚

 what you have in this case is features that live on top of the meshã€‚

And that's pretty nice because basically every feature here is supposed to encode the local appearance of the meshã€‚

ğŸ˜Šï¼ŒAnd the way you train this is with a differential rendering stepã€‚ So you're taking theã€‚

This 3D representationï¼Œ you render it to a given viewã€‚

 so you're giving a 2D UV map and sample this textureã€‚

Then you have a 2D unit that recreates a realistic image out of it againï¼Œ rightï¼Ÿ

And the idea is that you're training this whole thing end to endã€‚ So in other wordsã€‚

 you're showing this network a couple of different output images for a set of given posesã€‚ Againã€‚

 the poses are explicitly given hereã€‚And what you're optimizing for is the neural textsã€‚

 so that text these texture valuesï¼Œ in this caseï¼Œ they could be like 8 to 16 dimensionalã€‚

 as well as the rendererï¼Œ which is also the neural network that takes these texts again and makes a real image out of it againã€‚

 rightï¼ŸğŸ˜Šï¼ŒAnd againï¼Œ this process can be trained and to endï¼Œ which is veryï¼Œ very niceã€‚ğŸ˜Šã€‚

And at test time what you would do is you would basically take only this part here right and what you do is you're simply changing the RNs here so you're changing the viewpoint and then you're getting new images that you can render as outputã€‚

so yeahï¼Œ this isã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_5.png)

We call this paper deferred neural rendering if you've done graphicsã€‚

 you know what a deferred render is doing it first renders all the properties like material lighting UV and stuff like that into 2D image space and these handcrafted maps are then taken by the neural by the deferred renderer and the render is then applying the shading equation and stuff like that right andã€‚

ğŸ˜Šï¼ŒThe idea now is if youre having imperfect reconstructionsï¼Œ we simply learning these feature mapsã€‚

And we taking the deferinnal renderer to make an image out of it againã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_7.png)

Yeah and this is then the final pipeline and the core idea now is why this is actually useful is the assumption is if I'm getting a lot of these imagesã€‚

 I would love to reconstruct 3D geometryã€‚ğŸ˜Šï¼ŒBut this 3 geometry might not be perfect if I'm running a multiysterereo structure of motion pipelineã€‚

 it will not be 100% perfectã€‚ So instead of sayingï¼Œ ohã€‚

 re solving computer vision and getting perfect3 reconstructionsã€‚

 we're saying we're getting O reconstructions and then changing the rendering pipeline with the neural renderer in order to get four realistic imagesã€‚

 despite the fact that the 3D content was imperfectã€‚ğŸ˜Šã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_9.png)

And you can apply these kind of things for a lot of different applicationsã€‚

 you can do novel fieldborn synthesisesis weve already seen a couple of theseã€‚

 you can do scene editingã€‚So you can simply copy paste objects with a neural appearanceã€‚

Or you can do animation and synthesis and stuff like thatã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_11.png)

Yeahï¼Œ I wanted to show you a few here first this is novel fewpoint synthesis this was actually one of the sequences I showed you earlier in this case we have the input U map right that is rendered so it's not the input this is like the rendered UV map that you're getting when you render this imperfect geometry of this sphereã€‚

ğŸ˜Šï¼ŒAnd then you have the neural renderer that makes this one look like a real image again and if you're comparing this to Chme truth again these images are in the test not in the training setã€‚

 these are test images and you see that whatever we are rendering here looks actually pretty damn close to whatever the real image would look like even though this image here was not seen during training so the channelizability here is pretty good becauseã€‚

You have this 3D model and the neural render just has to make sure that it looks like a realistic image of that sceneryã€‚

 but the 3D part is all abstracted away with a coarse rough reconstructionã€‚

Yeah you can do scene editing in a very same wayï¼Œ so we have here and input sequence we're running in this case call map multifuse stereo method you get in some reconstructed geometry and what you're doing is you optimizing again for the neural render as well as the features on the mesh so this part here is the statue that's actually it's actually here at UM this bust that we scanned and now what you can do is you can take a crop of this geometry and copy paste this geometry twice hereã€‚

ğŸ˜Šï¼ŒAnd as a resultï¼Œ you also copy paste the neural features here of this geometryï¼Œ rightï¼ŸAnd nowã€‚

 if you're running the neural render on this imperfect 3D geometric representationã€‚

 we can create novel viewpoint synthesis from the edited sceneã€‚ so you can do stuff like thisã€‚

 This was the original oneã€‚ This was edited with usï¼Œ and this is the resulting outputï¼Œ rightã€‚ğŸ˜Šã€‚

So you can see these two busts hereï¼Œ they have been insertedã€‚Can show it againã€‚

And this is kind of the results you would get hereã€‚Okayã€‚You seeï¼Œ it's not perfectã€‚ actuallyã€‚

 one thing youll see is missingã€‚ This past here has a shadow and the smaller ones don'tã€‚

 And the reason why they don't is our differentiable renderer is just a forward renderer and doesn't deal with shadowsã€‚

 But this would be an obvious next step for research project to figure out how to get the shadows hereã€‚

 but we didn' really we didn't really bother too muchã€‚ This was kind of a nice to have applicationã€‚

 in addition to Nova fewpoint synthesisã€‚ what you can do now is if you simply Kobe paste some of the geometry aroundã€‚

ğŸ˜Šï¼ŒOkayï¼Œ you can do stuff for dynamic scenes in this case we have in here an Obama video from the original Obama video what we did is we reconstructed the face so we have here this phase templateã€‚

ğŸ˜Šï¼ŒAnd the neural textures are now embedded on the face template and what the goal isã€‚

 the neural renderer should then figure out how to inpae this region here in the middleã€‚

 and this on the right hand side is then the rendered respective target outputã€‚

In this case we're taking a source video hereï¼Œ this one is not contributing to the actual neural rendererã€‚

 This one is only taking the poses that we're then projecting on the reconstructed phase from Obama and again this one is then a completely synthetically generated output video but's the same technique as before the no viewpoint synthesisã€‚

 The only difference is now that the 3D model is parameterized by a source video thatã€‚

Tracks that is being trackedã€‚ and based on these post parameters of the face modelã€‚

 you can then change the 3D model here of the face and the neural render will make sure this one gets in painteded againã€‚

And yeahï¼Œ this is actually a pretty high resolution now and compared to the othergan methodsã€‚

 these kind of things they can be trained in a few hoursã€‚

 so it's pretty fast to train and needs only a few hundred maybe a few thousand images to trainã€‚

 So it's pretty efficient and the reason why it's so efficient to get this for videos is because we have this hybrid representation right we have the network on one side and we have the 3D mesh on the other sideã€‚

 So we don't have to learn all the 3D stuff right thats kind of makes this middle rendering direction pretty niceã€‚

ğŸ˜Šï¼ŒOkayï¼Œ but I wanted to show you some results hereï¼Œ now this is the target againã€‚

 this is completely syntheticã€‚ğŸ˜Šï¼ŒAnd whyï¼Œ I would really say Clintonã€‚

 probably I would have to say Clinton and whyï¼Œ there was a little spiritã€‚ franklyã€‚

 he would have been hadã€‚And I would but I see you can kind of get it and of courseã€‚

 you know we had a lot of fun with itï¼Œ we tried our favorite target peopleï¼Œ we thought yeahã€‚

 these two will fit very well togetherï¼Œ Bernie Sanders and Macronã€‚ğŸ˜Šã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_13.png)

President Trump has stated tonight and over and over again in recent weeks that this country faces a national emergencyã€‚

Or our favorite tech companiesã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_15.png)

II joined Google 15 years agoã€‚ and been privileged to serve as CEO for the past three yearsã€‚ Yeahã€‚

 you can look up these this videoã€‚ they're obviously all onlineã€‚ But yeahï¼Œ I think it's pretty coolã€‚

 actuallyï¼Œ can with this kind of stuff you can get veryã€‚

 very very nice renderings and the nice thing what you can do now is you can actually go ahead and sayã€‚

 wellï¼Œ instead of taking this source videoï¼Œ you can take any other input basically like the cool thing is we have this small render render produces kind of always good resultsã€‚

 So we thoughtï¼Œ wellï¼Œ why don't we use for instanceã€‚

 voices input right So you can animate this guy with taking voice or even text inputã€‚

 So this is a very recent project but also useless has been doing voice poetry the idea here is basically we want to feed in kind of we want to basically give virtual assistant a face right So you have like Siri and Alexa and these kind of virtual assistantã€‚

 and we want to basically synthesize the respective video avatar to itã€‚ But insteadã€‚ğŸ˜Šã€‚

Hiring an artist to model a video avatar for usï¼Œ we just want to train a roll renderer to figure outã€‚

How it should look like Yeah in practice we have some application that looks like thatã€‚

 So right we have here some text and from this text we want to synthesize the respective videoã€‚ğŸ˜Šã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_17.png)

Heyï¼Œ Siriï¼Œ can you show me your faceã€‚SureSureã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_19.png)

What about this one with neural voice puppetryã€‚ I can have every face you likeã€‚

 You can also control different faces just with your voiceã€‚ Let's have a look at some resultsã€‚ğŸ˜Šã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_21.png)

This is all wrongã€‚I shouldn't be up hereã€‚I should be back in school on the other side of the oceanã€‚ğŸ˜®ã€‚

So a question is how does it work pipeline Wellï¼Œ most of the stuff I've already shown youã€‚

The good thing is you can basically take only the text here as inputã€‚

And the idea here is before first taking some audio featuresï¼Œ we just use deep speechã€‚

 that's the state of the art audio networkã€‚ğŸ˜Šï¼ŒAnd the output of this network here from Deeppe is a set of features and these features from the audio here from Deeppe beingã€‚

We're feeding then into another linear layer basically that we're mapping to audio expressions that drive a facial 3D modelã€‚

 and then we have our standard neural rendering pipelineï¼Œ we have a 3D facial modelã€‚

 we have a neural renderer that generates a respective image or video out of itã€‚ğŸ˜Šã€‚

I'm not gonna go into all the details hereï¼Œ but the key part is hereï¼Œ basicallyã€‚

 how do you translate audio features into the respective blend shapesã€‚

 We didn't talk too much about the blend shapes here of the faceã€‚ But think about it this wayã€‚

 This is there's some animation control of the face that is being driven byã€‚

By the audio then and this is learnedã€‚ So all you're doing right now is at training time youã€‚

Taking a videoï¼Œ you're tracking the face of that videoã€‚

 you're correlating the audio to the T 3D phase modelã€‚

 and then what you do is you're using the deep speech features input here and you're figuring out how do the audio features see a map to the respective phase parametersã€‚

 rightï¼ŸThese onesï¼Œ thenã€‚There are this is person specificã€‚

 so this phase model here is being fitted specifically to this one videoã€‚

And the neural render is also optimized for this one videoï¼Œ but everything up to hereã€‚

 these audio featuresï¼Œ these audio expression featuresï¼Œ they are being channelized across everythingã€‚

 And the reason why it works so well is because we using in deep speech that's a very powerfulã€‚ğŸ˜Šã€‚

Audio network that is basically trained on speech to text or pre traineded on speech to textã€‚

Okay so then we have audio expression trainingï¼Œ that's what I've just explained the way you train this this channelized part in this case we took a bunch of videos from the German TV stationsã€‚

 we reconstruct the faces hereï¼Œ we have up to hundreds of commentators of available videos and all with the kind of neutral talking style right and neutral is important you didn't want to have too much variety that made it a little bit easier basically for us rightã€‚

ğŸ˜Šã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_23.png)

Yeahï¼Œ and then you're getting results like this isna for case basically this model here how does map to the respective3 So we can basically an model But now we have render in is taking this model here is input and then generates the respective video outputã€‚

ğŸ˜Šã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_25.png)

![](img/c753c9ffe031c68c3d0685b4c7465525_26.png)

Okay yeahï¼Œ and in this case we also blend it with a backgroundã€‚

 same thing as before right you have our target video frameã€‚

 you having this face part that should be animated and then you have a compositing network that figures out how the respective target is this is the same thing what we had before pretty much and then you can animate the whole thing with the audio input right so here we have audio input and we then getting the respective video outputã€‚

ğŸ˜Šã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_28.png)

Science makes progress by stepsã€‚ Most of those steps are smallï¼Œ Some are slightly biggerã€‚

Seen from the outsideï¼Œ sometimes people have the impression that ohã€‚

 there's this big breakthrough breakthrough and journalists like to talk about breakthroughã€‚

 breakthrough breakthrough breakthroughï¼Œ but actuallyï¼Œ science is veryã€‚

 very progressive because we gradually understand better the worldã€‚

Okay so one thing I didn't mention hereã€‚ Obviously this network needs audio inputã€‚

 but you can run it with text as wellã€‚ you simply run a text to speech method right there's a bunch of text to speech engines you can use taco droneone and stuff like thatã€‚

 I think Amazon Pol is one of them but there's a bunch of them and you can just take these onesã€‚

 And since this audio model though is channelized it basically worked with any inputã€‚

 And because of thatï¼Œ this model is actually quite versatileã€‚

 So we had a demo actually online from neural voice Puppetry but you can put in some text and then you can can control your favorite avatarã€‚

 which was kind of interestingã€‚ unfortunatelyï¼Œ we had to take it down because people started using it a little bitã€‚

 maybe we underestimated this whole thing a little bit what you can do with itã€‚

 we were actually mainly hoping that you can do your virtual assistantã€‚

 but obviously there's also some ethical concerns we should be consideringã€‚

 So the moment we disabled it but we will have to see whether we put it up againã€‚ğŸ˜Šï¼ŒOkayï¼Œ yeahã€‚

 but getting back to neural renderingï¼Œ I meanï¼Œ the reason why I'm so excited about is kind of like all these problems in computer vision that are pretty difficult to do from a visual standpoint you can now do with neural render works veryã€‚

 very easily and and this hybrid of taking some 3D information from graphicsã€‚ğŸ˜Šã€‚

And then using the neural networks on top of it works surprisingly well for manyï¼Œ many thingsã€‚

So there's of course still a lot of open challengesã€‚

 I think this whole thing of like photoistic reconstruction is kind of cool rightã€‚

 How do you go from here to here This is indeedï¼Œ of course still a veryã€‚

 very challenging problem and by no means yeah have we done with that So there there's still a lot of research to be done here I think the biggest challenge right now is basicallyã€‚

ğŸ˜Šã€‚

![](img/c753c9ffe031c68c3d0685b4c7465525_30.png)

If you're talking about traditional computer vision methodsï¼Œ rightï¼Œ they get us to hearã€‚

And we're talking about the fine images that we want at the end of the dayã€‚

Let's say this kind of stuff here is being done with a neural network right nowã€‚

 with a neural rendererã€‚The biggest challenge right now isã€‚How much does the network doï¼Ÿ

And how much How good does the reconstruction have to beã€‚I meanï¼Œ traditional speakingã€‚

 the computer vision community has been trying to make this one here better right it's a tricky problem going from 2D back to the 3D representation because it's veryã€‚

 very under constrainï¼Œ it requires a lot of priors and thought processesã€‚ğŸ˜Šã€‚

Yeah so it's a challenging problem neural rendering has now enabled the idea of like okayã€‚

 we just sayï¼Œ okayï¼Œ it's still brokenï¼Œ we can deal with itã€‚

 but we take a neural network and fix the rendering processã€‚ğŸ˜Šï¼ŒSo obviously these two thingsã€‚

 there are some synergy rightï¼Œ like how much can the network do and how good does the reconstruction have to be at the very end of the dayã€‚

 And this is a very very fundamental question that I think is a very interesting open challengeã€‚

 but I think ultimately these things they should go together rightï¼ŸğŸ˜Šã€‚

Maybe you can do this end to endï¼Œ you can have a differentiable3 reconstruction right and get better reconstructions based on the respective target appearances and so onã€‚

We've tried a few funny experiments hereã€‚ This is when you're taking a box and this box is our 3D proxy that we're taking as the neural rendering inputã€‚

ğŸ˜Šï¼ŒIn order to recreate this image of the weightsï¼Œ so obviously this UV map here or this 3D mesh here is pretty bad in terms of the approximation of the baseã€‚

The neural rendering networkï¼Œ the 2D partã€‚Can actually figure out how to go from here to hereã€‚

Allright we're getting pretty closeã€‚ this 3D geometry is good enough to get us temporal coherence because the motion here and the motion hereã€‚

 they correlate pretty wellï¼Œ right You're rotating both around the z axis hereã€‚ğŸ˜Šï¼ŒHoweverã€‚

We see that it's prettyï¼Œ pretty blury in the middleï¼Œ so it's not perfectï¼Œ of courseã€‚

 right it's a pretty tricky problemã€‚ Eric has to basically figure out how to go from this box to this spaceã€‚

ğŸ˜Šï¼ŒSo it's not perfectã€‚Andã€‚Yeahï¼Œ that'sï¼Œ that's the questionã€‚ Alï¼Œ rightã€‚ I meanã€‚

 how much can I do from one to the otherï¼Ÿ And one thing I can probably tell you alreadyã€‚

 if the trackingï¼Œ if this motion here was inconsistent with that motionï¼Œ it would horrendously breakã€‚

We've seen some of these examples for everybody dance now right this is a black box networkã€‚

 there's no 3D underlying context in architecture yetã€‚å—¯ã€‚In that caseï¼Œ if the tracking is not perfectã€‚

 it's inconsistentã€‚ You would have wrong mappings for wrong pointsã€‚ But if youï¼Œ if allã€‚

 if you map wrong and you consistently map wrongã€‚That's okayã€‚ as long as it is consistentï¼Œ rightã€‚å—¯ã€‚

Yeahï¼Œ so that's kind of interestingã€‚Andã€‚Yeahï¼Œ now the big challenge here is how muchã€‚

 how much can the I do and how good do I have to reconstructã€‚ And this is aï¼Œ I meanã€‚

 as you saw most of these paperss they have appeared basically a few monthsï¼Œ maybe half a year agoã€‚

 Most of this stuff is still super open research topicã€‚ There's still a lot of stuff missing thereã€‚å—¯ã€‚

ã„ã‚„ã€ so that'sã€‚It'sã¨ã€‚Up in the air right nowã€‚Okayï¼Œ the other big challenge isã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_32.png)

3D operatorsï¼Œ or even 40 operators in the networksï¼Œ rightï¼Ÿ

 I'm mentioning here one example at the capsule network paperã€‚

 This was a paper in 2017 from Cheinton H's teamã€‚ that was very popularizedï¼Œ it didn't work so wellã€‚

 But the idea was kind of coolã€‚ They saidï¼Œ oh computer vision is inverse graphicsã€‚

 So we kind of need to learn fine transformations growing from 2D back to 3D and then have like capsules to understand what's going on thereã€‚

 And the idea was kind of if you have these capsulesï¼Œ you need significantly less training dataã€‚

 and they got a lot of good results for very small trivial tasksã€‚

 but the scalability of the training was very challengingã€‚ So didn't convert so wellã€‚ I meanã€‚

 eventually the results didn't quite live up to the expectationã€‚

 But I think that the idea behind it was really cool because they thoughtã€‚

 well don't just use 2D convolutions and aggregate features with some pooling operator at the endã€‚

 But rather learning underlying structures from an image back toã€‚ğŸ˜Šã€‚

I like this is a fine transformation spaceã€‚So I think channel is speakingã€‚

 if you talk about neural networksï¼Œ rightï¼Œ I meanï¼Œ from a research sideã€‚

 like app a bunch of 20 layersï¼Œ doing some neural architecture researchï¼Œ possibly that's aã€‚

 wouldn't say done dealï¼Œ but it's a research why it's pretty exploredã€‚ The question is more like nowã€‚

 wellï¼Œ how do weï¼Œ how do we learnã€‚3D operatorsï¼Œ How do we embed 3D operatorsã€‚

Differentiable nonop learning and stuff like thatã€‚ That's a thing that we should really consider into neural networks these daysã€‚

 But that's aï¼Œ that's a very interestingã€‚ğŸ˜Šï¼ŒYeahï¼Œ problem we have to addressã€‚Yeahï¼Œ I would alsoã€‚

I would also ask you to have a lookã€‚At the state of the art reportï¼Œ this is very recentlyã€‚

 this was a eographicsã€‚ There's a lot of people involved themã€‚

 including our team too between like Facebookï¼Œ Googleï¼Œ Adobeã€‚

 like all the people that do this kind of rendering So there's a lot of resources that you can look up and this goes basically yeah to a lot of different to a lot of different things from novel viewpoint synthesis to animation to facial retargeting text to video and stuff like thatã€‚

 So all of these kind of things are are covered hereã€‚ and a lot of the things I talked about todayã€‚

 a lot of the slides were actually from this courseã€‚

 they're going be there there's also another video up there for field to look that up you can check it out on or I think on my Twitter account So and these kind of things are pretty interestingã€‚

 I think if youre interesting in research in these directions or even for the projects right now on this course I would highly recommend to look up these kind of things I'm very excited about itã€‚

ğŸ˜Šï¼ŒA lot of new research coming up that's worthwhile looking at itã€‚Okayã€‚

 yeah I'm a little bit done early actually with a slide deckã€‚

 I hope I didn't rush too quickly through itã€‚ I still hope that a lot of these things are very interesting to youã€‚

ğŸ˜Šï¼ŒAndã€‚We have basically one more lecture right nowã€‚ I wanted to talk a little bit about you knowã€‚

 high level of representationsï¼Œ point cloudsï¼Œ3D seen understandingã€‚

 you've already seen a bit of 3D stuff today as part of the role renderingã€‚

 But there's a little bit more coming coming to it rightã€‚

 So that's something I feel that is also still very interestingã€‚ againã€‚

 like 2 d deep learning has been a bit exploredã€‚ So we have to think about you know what's next like 3 dã€‚

40 maybe Yeahï¼Œ there's a couple of coolï¼Œ interesting things coming up other than thatã€‚

 I hope you you're still enjoying your projectã€‚ keep pushingã€‚

 This is your opportunity right now to realize your own deep learning projectã€‚ And otherwiseã€‚

 yeah see you in the next see you in the next lecture things a lot for the attentionã€‚ğŸ˜Šã€‚



![](img/c753c9ffe031c68c3d0685b4c7465525_34.png)