# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ…•å°¼é»‘å·¥å¤§ ADL4CV ï½œ è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ è¿›é˜¶è¯¾ (2020Â·å…¨10è®²) - P10ï¼šL10 - é«˜ç»´æ•°æ®æ·±åº¦å­¦ä¹  - ShowMeAI - BV1Tf4y1L7wg

Hello everyone to the advanced deep learning lecture in the recent lectures we've talked about generative models and very recently we've talked about neural renderingã€‚

 one of the things weve already discussed in the context of neural rendering is like these hybrid neural network architectures where you have some elements being transferred to 3D and this is a thing I wanted to chat about today specifically how to do deep learning in higher dimensionsã€‚

RightSo obviously there's all kind of data signalsï¼Œ rightï¼ŸğŸ˜Šï¼ŒMainly in computer visionã€‚

 we often look at 2D signalsï¼Œ like imagesï¼Œ possibly 3D on videosã€‚

 but most of the times we are looking at 2D conf operators and the reason why they work so well is because you can apply them on images and then they extract these featuresã€‚

That it can then accumulate in order to get good classification resultsã€‚Nowã€‚

There's of course many other thingsï¼Œ one of them I mentioned last time we already had the discussions about neural renderingã€‚

 but there's of course many more so if you're thinking about the 1D domain we have audio and speechã€‚

We have images we mentioned thatï¼Œ but in one we also have things like point clouds when you're thinking about self driving carsã€‚

 these have lidDR scanners and with these lidar scannersã€‚

They capture point clouds and these point clouds can be used also in terms of map the environmentã€‚

 and then you want to do some sort of semantic scene understanding ofã€‚

The reason why points are actually1D is because every point in this case would have three dimensions and then you just stack them together to a linear vector right so it's a 1D conf you could in theory reply I'm not arguing that a conf in this case is a good operator but you could of course run a 1D convolution on that for audio speech is very clear right we have like samples for every audio signal that is being generated like I don't know something like 16 bit or so audio signals rightã€‚

It depends a little bit on the frequency thereã€‚ I think there's something between 16000 and 32000 is a standard thing what people storeã€‚

 and that is something you can do with a1D con for instanceã€‚

 with 3D convolutions we know we have videos right and you also have seen what I mentioned the rendering parts where you can transform thing to voxel grid and then do some 3D operators on 3D features and you can have things like 3 renets and all the things you have seen in 2D you can basically directly transfer to voxel grids and there's also things in 4D in principleã€‚

 there's things like fluid simulationï¼Œ dynamics dynamic videos basically So if you have a volume rightã€‚

 you have a 3D environment and then you have anything that is moving over time in principle that would be 4D con So in the literatureã€‚

 of courseï¼Œ we have seen mostly 2D con that's just you know what's very popular and there has been a lot of research recently on 3Dã€‚

And that's something I want to talk a little bit about more todayï¼Œ 4D consï¼Œ not too muchã€‚

 but there are a few ver and there are few ver aroundã€‚How does it work in high dimensionsï¼Œ Wellã€‚

 if you're looking at at the convolutionsï¼Œ rightï¼Œ I meanï¼Œ all you're doing here is basicallyã€‚

We have in here our we having here our data rightï¼Œ so we have an input for instanceã€‚

 and then we have here our kernel and then we can simply do the sliding of the kernel here in 1D right it's pretty straightforwardã€‚

Maybe least slide this kernel and you know this kernel will be invariant towards being applied and what we're doing is we're learning the weights of this kernel so there's not a lot of surprises going on here in1D I'm sure you can figure out how to implement this is' not so difficult one of the very popular works I mentioned already in the context of autoaggressive methods was WaveNenet and Wavenet is basically they use 1D convolutions and they have this autoaggressive architecture so you know they sample literally every audio sample right and then they have convolutions over that one this is a paper that was actually done in 2016 by Aaron Ferã€‚

It's a pretty cool paper because it was one of the first ones that showed that you can do things like auto aggressivegressive models on a very large number of samplesã€‚

 rightï¼Œ Likeï¼Œ againï¼Œ if you're thinking about like thousands of samples per second and you have relatively long sequencesã€‚

 there's a lot of samples going onã€‚ So auto aggressiveive models tend to work relatively well in these dependencies in practiceã€‚

 how wave networks worksã€‚ğŸ˜Šï¼ŒIs were gonna have some sort of input here right we're gonna have a bunch of hidden layers here and what you're doing is you're literally generating sample by sample as you can see here right So it's like sample by sample is generated and every sample that is being generated here is going to be one forward pass in our in our network So you can already imagine it's it's well it's pretty slow right you have to I think they have I don't knowã€‚

Like 90 minutes or something like this for a few seconds of video or so sorry for a few seconds of audio there's now versions that do this fasterã€‚

 there's parallel versions of WaveNe how to do the evaluationã€‚

 how to cache this efficiently at runtime it's also relatively slow to train but there's better versions right now that can do this faster but in principle this idea of running one confs on the audio signal and having an autoaggressive model basically modeling the probabilities and predicting sample by sample by sample in the output stream is a pretty good idea and WaveNenet actually has made a lot of impactã€‚

ğŸ˜Šï¼ŒLike variations of that are basically in the Google system it's not the original paper anymore there's some variations that you can see now on the Android phone right so text to speech systems would use architectures like that today again there's some combinations of that of course it's not exactly that anymore they got it also a lot faster which was very important and but the quality is actually pretty decent so you can have a look at it and you can see what waveveNet is generatingã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so this is something that people have been doing in WDã€‚

 The reason why I'm mentioning it is you knowï¼Œ often when you want to do something likeã€‚

 I don't knowï¼Œ you want to train a network then generate some audio signal to a videoã€‚

 So you have to look at generative models basically for thisã€‚

 So when you want to do speech to text rightï¼Œ you would do a1D con on the audio samples and thenã€‚

And eventually going to get some features out of itã€‚

 the deep speech is a very popular network you could look at here for feature extractions and audioã€‚

Yeahï¼Œ that's something that is also encouragingï¼Œ I think it might be interesting for you to look atã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_1.png)

One of the things we do a lot actually in my group is we look at a lot of things in 3D like 3D shape or single classificationã€‚

 The idea is basically you have something like a K scan or you have a lidDAR scan in a self-driving car you're gonna have a bunch of 3D reconstructions here the idea is you're taking these 3D meshesã€‚

 the meshes are a polygon soup of triangles and faces like faces right and these ones you can then convert into a voxel grid so this is the naive implementation here that you say ohã€‚

 I'm going go and mark every vel either is being occupied or empty and then you can do some 3D cons over that oneã€‚

ğŸ˜Šï¼Œå—¯ã€‚I will probably motivate this a little bit more why 3D works so wellã€‚

And the reason is why we have 2D networks that are so big is because you have to learn the viewpoint invariã€‚

 So in other wordsï¼Œ if I'm taking an image of a car and then I take another image of a car from a different viewpoint right the 2D pixel values will change rapidly because my camera viewpoint changed but I want to train a network that gives me the same results want to get the same features in the same class vector at the end of the day and in 3D you don't have that and one of the advantages in 3D is then you don't need so much training data which is pretty interesting right so obviously we need a lot more memory because suddenly we have 3D cons butã€‚

Yeah we don't need so much data anymore know the turning times are much lower which is interesting I'll talk about this one in a little bit more anywayã€‚

 but the first thing we want to do in this case we want to take these 3D reconstructions of shapes and want to do 3D classification so in this case you have one model as input right and you're going to get a class label as outputã€‚

These are of course toy tasks in this case right you have to segment out the object first and figure out this is a bathtub line in practice you can do tasks like 3D semantic segmentation right here we have an example from scannet that's what we've done a while ago it's a relatively popular Indo data set it's recorded the structure sensorã€‚

 it's similar to a Kã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_3.png)

And what you have isã€‚You're going to have semantic labels on top of these scansã€‚Andã€‚

For every surface pointï¼Œ you know which object class it is right So this is a sofa chair right this is another chairã€‚

 this is the floorï¼Œ this is the wall and so onã€‚ In this case you want to train a neural network right that you basically predict a class label for each of these surface points and this is what 3D semantic segmentationã€‚

 same idea what you had in 2D you can then do in 3D and then you can go further and say you can do instant segmentationã€‚

 object detectionï¼Œ all these kind of things they all work in 3D but now I wanted to talk a little bit about the volumetric gridsã€‚

 I wanted to talk about how do you do how do you feed the data into the network you know we've discussed already a little bit how convolutions work right I mean that's something you should probably know by nowã€‚

ğŸ˜Šï¼ŒThe 3D operator and 3D on Vgri seems relatively straightforwardã€‚

 But there's actually a couple of different data structures you can useã€‚

 One thing you can do is occup secretsã€‚ That's what I mentioned beforeã€‚ where you sayï¼Œ ohï¼Œ for everyã€‚

 for everyã€‚ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_5.png)

Wellï¼Œ I guess I should quickly for those who don't know idea the volumet structuresï¼Œ the voxel gridsã€‚

 a voxel is the same as a pixel but in 3D right so we have a 3D array for every spatial location we have one associated so the simplest version of that is the occupancy grid so for every voxel where there's a surfaceã€‚

Point youre just gonna say it's occupied right And if it's not occupied it's going to be zero so occupied it's one not occupied in zero right you can have an occupancy secret where you only model a surfaceã€‚

 you can also have a solid velization where you sayï¼Œ ohã€‚

 I'm going to vel everything inside the object so depends a little bit how you generate itã€‚

Most of the time when we're talking about it right nowï¼Œ we're going to talk about only the hullã€‚

 so only the surface will be modeled right not the interior because if you're having a 3D scannerã€‚

 we can only scan the surfaceï¼Œ but not what's behind itã€‚

There's also things like turnary grids that gets a little bit more complicatedã€‚

 so for turnernary gridsï¼Œ what you do is you encode free spaceã€‚ğŸ˜Šã€‚

An occupied space the same way as the occup secretã€‚

 But you have a third state that tells you you don't know what it isã€‚ So in other wordsã€‚

 if I have a 3D scanner and I'm taking likeï¼Œ I have my hand hereï¼Œ rightï¼Œ and I'm looking right nowã€‚

 from my eyesï¼Œ I'm looking to my handï¼Œ rightï¼Œ So I'm looking hereã€‚

 If I measure the distance from my hand to my eyeã€‚ And I know the depth valueã€‚

 I know that there's a surface point here on my handã€‚Ifã€‚This hand moves thisã€‚

 this step value will changeï¼Œ rightï¼Ÿ What I do know is I know that everything here in front of my hand is free spaceã€‚

 right becauseã€‚Ifã€‚There was an object in front of my handã€‚ I would have not measured my handã€‚

 I would have measured whatever object is in front of my handã€‚But if I have observed a that valueã€‚

 I knowã€‚Wellï¼Œ there might be some uncertainty plus minus a few centimeters where this hand isã€‚

But when I have the depth value hereï¼Œ I know there's nothing in front of itã€‚

 So there's free space in front of my hand right nowã€‚ Howeverï¼Œ on the other handã€‚

 I do not know what is behind my handã€‚ If I only have my currentï¼Œ like if I basically haveã€‚

This piece of paper hereï¼Œ rightï¼Œ I only know what's in front of the piece of paperã€‚

 but I don't know what's behind itã€‚So everything in front of itï¼Œ I know is free spaceã€‚

 everything behind itï¼Œ I have no idea there could be another object behind itã€‚

 there could also be no object behind itã€‚And this is what the turnary grids are encodingã€‚

 So the turner grids sayï¼Œ is it a surface pointï¼Œ Yes and noã€‚

 Is it free space or is it unknown space rightï¼Ÿ And so you basically have three three states you can you can associate to a vableã€‚

 And this helps you quite a bitï¼Œ rightï¼Œ for instanceã€‚

 if you combining 3D scanning information from different viewsã€‚

 like I have seen like this surface here againï¼Œ from the front and from the backã€‚

 then I know that all of it this free spaceã€‚ and that's much more information that for instanceã€‚

 in networking leverage in order to do proper classificationã€‚ğŸ˜Šï¼Œå—¯ã€‚ğŸ˜Šã€‚

Yet then there's implicit functionsï¼Œ these are distance fields and sign distance fieldsã€‚

So a distance field is also anchored in a voxel gridï¼Œ but instead of just having a binaryã€‚Yesã€‚

 and no answerï¼Œ every vel is encoding how far is the closest surfaceã€‚

So distance field in practice looks like thatã€‚ So look at this ship hereï¼Œ rightã€‚

 This ship is a 3D shapeã€‚ Everything on top of the surface hereã€‚Is going to be 0ã€‚

0 means there's a surface pointï¼Œ and if I'm going to deviate one voxle away from my surfaceã€‚

 I will have a distance value of1ã€‚ If I go two voxes away from my surfaceã€‚

 I will have a distance value of 2ã€‚And every voxel tells me basically how far am I away from the closest surface pointã€‚

 that means in a distance fieldï¼Œ every voxel on the surface will be zeroã€‚

 and the further I go away from the surfaceï¼Œ the higher the distance values goã€‚å—¯ã€‚Wellã€‚

 what does it help us withï¼ŸIf you took friendsã€‚T the gradient in this distance fieldã€‚

 you would directly point to the surface basically then the gradient of the distance field tells you how to find the surfaceã€‚

 So in this caseï¼Œ you're actually encoding quite some information which is much more than the occupant secretã€‚

The next thing what you do isã€‚It's not just the surface point is typically not exactly on a voxel center right so what you can do is this distance field gives you some sort of super resolution because you can do a trinary interpolation between the distance values on the voxel gridã€‚

And that defines the surface that could be between two voxels so you get a higher solution with a distance fieldã€‚

Often what people doï¼Œ they sayï¼Œ wellï¼Œ the distance values are only relevant close to the surface so they say they store only distance valuesã€‚

 I don't know within plus minus and three voxels of the surface and everything that is further way than three voxels will be clam to three in this case this would be a truncation So often people use truncated distance fields they sayã€‚

 well you know I have all the signal in my neighborhood of the surface that's where I care about that's where I want to for instance learn features from and even a further way it''s not so relevant like just ignore it or clam it to some max values so then would be a truncated distance fieldã€‚

ğŸ˜Šï¼ŒSo these are distance hereï¼Œ they are these implicit notations of a 3D shape and you basically look at the super resolution in this 3D voxal gridã€‚

 rightï¼ŸğŸ˜Šï¼ŒAndã€‚There's also signine distance fields and signine distance field is the analog what we had in the turnernary gridsã€‚

 so the sine value tells us whether it's in front or behind the surfaceã€‚Which isï¼Œ againã€‚

 if I have thisï¼Œ this piece of paper hereï¼Œ everything here in front of it would have a positive distance value and everything behind it would have a negative distance valueã€‚

 rightï¼Œ So this is what a sign distance field isã€‚ It's the same as the turner gridã€‚ Nowã€‚

 the sign then meansã€‚Is it known free space or if it's negativeï¼Œ is it unknown spaceã€‚

 which I don't really knowï¼Œ I only know how far it is behind the surfaceã€‚

So the signine distance field in principles store the most information here because they tell you A you have the advantage of a continuous representation of the surface because it's a distance fieldã€‚

 but in addition to that you also have the signine value that tells it whether it's non-fr space or whether it's unknown space behind the surfaceã€‚

ğŸ˜Šï¼ŒOften for many scanning applications we use sign distance fields anyway for the surface reconstructionã€‚

 there's papers like Kfusionï¼Œ this very old paper from Curless and Levoyã€‚

 if you're interested in it check out these works that it's very interesting how to accumulate different frames from different frames as how you accumulate the depth values from different frames and then surface reconstruction and the nice thing is you can actually do direct feature extraction with neural network in 3D on top of these reconstructionã€‚

ğŸ˜Šï¼ŒYeah I put some numbers here for shape completion so shape completion is one of the examples that you can do for instance as generative models here I don't want to talk too much about the numbers that maybe not so interesting but what's interesting is that you can have different resolutions so in this case you see if you're doing shape completion you're gonna get resolutions often for like 32 cube 128 cubed so it's not super high right and the reason why it's not super high is becauseã€‚

ğŸ˜Šï¼ŒOf courseï¼Œ it needs a lot of memory right now to define the convolutions rightã€‚

 So convolutions now have three dimensionsã€‚ Wellï¼Œ it's actually three plus oneã€‚

 So it's three spatial dimensions plus one for the feature depth rightã€‚ So actuallyã€‚

 that convolutions become 40 if I on a convolution in 2Dã€‚

 they are three dimensional because I'm going to have x and y and then the feature depth where that is dependent on the channels of the previous layerã€‚

 Okay I already mentioned one of the tasks in addition to classification could be shape completionã€‚

 The reason why I'm mentioning itï¼Œ it's a relatively straightforward paper actually that uses an autoencor architectureã€‚

 So you haveï¼Œ for instanceï¼Œ a distance field hereã€‚ plus an observed stateã€‚Essentialã€‚

 that's a sine value that's a sine distance functionã€‚ it's anchored in a 32q gridã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_7.png)

What you do is you have a 3D encoder maps to Laden spaceï¼Œ 3D decoder maps back to a distance fieldã€‚

Againï¼Œ32 cubedã€‚Noteã€‚I don't want to talk about the classification network right nowã€‚

 I just want to look at these thingsã€‚ This is an encode and the decoderã€‚

 and you have some skip connectionsã€‚ So this is basically a unit in 3Dï¼Œ rightã€‚

 Nothing special about itã€‚ What's interestingï¼Œ thoughã€‚

 is this one takes us input a distance field plus observed stateã€‚ The distance fieldï¼Œ againã€‚

 is this embedding of 3D shape in the distance field is this known space versus unknown spaceã€‚ğŸ˜Šã€‚

This one looks like this as inputï¼Œ this is the input to the network that is encode as a 32 cube grid in the sign distance fieldã€‚

 and it's a partial of 3D scan and the goal is you want to predict a complete scan like this oneã€‚

And now the output is a distance fieldï¼Œ so input sign distance fieldï¼Œ output distance fieldã€‚ğŸ˜Šã€‚

And the reason why I'm mentionagining this is so you can see what the sign meansã€‚

 the sign in this case means if it's negativeï¼Œ I don't know the answer if it's occupied or not of it's in front or behind the surfaceã€‚

 but the top of the network is to force a prediction for every voxelã€‚Here I have some partial scansã€‚

 I don't know everything and the network is supposed to predict a value everywhere and this is what this distance field here is going to give youã€‚

Yeahï¼Œ there's a few more tricks now in this methodã€‚ This one is maybe not the most important oneã€‚

 you can combineã€‚Completion features hereã€‚ This is from the encode decoder structureã€‚

 it's unit with skip connectionsã€‚You can also run a 3Dclassifierã€‚

 append these features in the latest space and hope you get better resultsã€‚å—¯ã€‚

What's interesting about this this ifify actually didn't help too muchã€‚

 but the other way helped her a lot actuallyï¼Œ if you're doing a completion network with an or encoder and using these features hereã€‚

ğŸ˜Šï¼ŒIn addition to the classification features for classificationã€‚

 then the classifier gets a whole lot betterï¼Œ so the classifier benefit a whole lot from the completionã€‚

 the other way around maybe not so much butã€‚ğŸ˜Šï¼ŒIt's quite an interesting argument because you can basically learn 3D structures and because you're learning 3D structuresã€‚

 you're getting better classification resultsã€‚Now this is for shape completionã€‚

 you can do the reason why this works is wellï¼Œ in this caseï¼Œ I have a 32 Q gridï¼Œ rightã€‚

 and I'm feeding inã€‚ğŸ˜Šï¼Œå—¯ã€‚I'm feeding in one single shape that is always anchored in this 32 cube gridã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_9.png)

Nowï¼Œ if I'm talking about 3D scenesã€‚Like semantic segmentation in 3Dã€‚

 I will find a very interesting problem very quicklyã€‚And one problem is that my scenes at test timeã€‚

 also training timeï¼Œ they're going to have a different size every timeã€‚Which is confusingï¼Œ rightã€‚

 because for standard convolutional networks what we have seenã€‚å—¯ã€‚

That an image is mostly the same if Im running every TT architectureã€‚

 they always take the what is it to 224 squared images as of inputï¼Œ rightï¼Ÿ

So that is something that is in 3 d is very different in 3Dã€‚

 we're going to have basically an arbitrary scene sizeï¼Œ an arbitrary scan sizeã€‚

 like if I have a car going through a whole city and I want to do semantic segmentation for every point that my Li scanner is scanã€‚

 I have a pretty big sceneryã€‚One thing what you can do is you can define things like semantic segmentation in 3D on chunks of the dataã€‚

 so what you could do is in this case I'm going to take as input a 62 times 31 times 31 vel grid right 62 is the heightã€‚

I don't knowã€‚ I think this was roughlyã€‚I don't knowï¼Œ likeã€‚3ã€‚

4 centimeters so such that it's basically2ï¼Œ3 me in height and this block is like 1ã€‚

5 m to2 m square or so in width andã€‚And depthã€‚ But the important thing is basically you can run a sub chunkunk of the data as inputã€‚

 So it's 62 times 31 times 31ã€‚ you run a 3D continentã€‚

You're going to get a classification vector for the center voxel columnã€‚

 so were going to get for these 62 values in the middle of this chunk we're going to get results right nowã€‚

 so we're going to get 62 classification we're going to get classification scores for 62 voxels in this case we have 22 classes so each of these 62 voxels will get 22 class scores as upwardã€‚

And the way youve then run this whole thing is you slide it through right so for every middle column here of these chunks you're going to slide it throughã€‚

 I'll show this in a second how this looks likeï¼Œ but basically you just run the network manyã€‚

 many times at training timeï¼Œ test time and at training time you train it on separate chunks independentlyã€‚

And in this case it's an occupancy grids inputï¼Œ it's a known unknown spaceã€‚

 so it's this turnner grid what I mentionedï¼Œ and the output is a 22 dimensional class vector per voxel and we have 32 sorry 62 voxels that we're predicting values for at the same timeã€‚

Okayã€‚This is an arbitrary choiceã€‚ This is a bit of an older pa in practiceã€‚

 You would also use scientist distance field the dayã€‚ It just works a little bit betterã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_11.png)

This slid window I mentioned in practice what we have is we have now a 3D sceneï¼Œ rightï¼ŸğŸ˜Šï¼ŒAgainã€‚

 these are my different classesã€‚ This is a valization of the sceneã€‚

 These are the bro thing of the wallsï¼Œ rightï¼Œ This is the floorã€‚ This is a better thingã€‚ğŸ˜Šã€‚

I think this is the table and so on rightã€‚This is a current chunk that we're cropping out rightã€‚

 the scenes are aligned with the x Y axis and now what you're doing is you're basically running this network for each locationã€‚

You do this a training and you do this a test timeï¼Œ rightï¼Œ At training timeã€‚

 you just randomly sample it in the test timeï¼Œ you would do it in a structureã€‚Andã€‚Yeahã€‚

 then you basically can get these kind of predictions that you're seeing hereã€‚Andã€‚The good thing isã€‚

By running the sliding window versionï¼Œ this isã€‚Actually independent of a sizeï¼Œ rightã€‚

 You can run this for arbitrary large seamsï¼Œ but the downside isï¼Œ of courseã€‚

 it's going to be very slowã€‚The nice thing I should have a few things later to thatã€‚

 but the nice thing is you could this is a 2017 paper nowadays you would do this also fully combvolution now the nice thing is if you have convolutionsã€‚

 you could also do this in one shot similar ideas what other people had in 2D for semantic segmentation like the FCN paper from Berkeley you can do these kind of tricks in 3D as well you can do other things like not just 3D semantic segmentation you can actually also use these 3D networks for surface reconstruction very cool paper actually is surface netã€‚

ğŸ˜Šï¼ŒSurface in the potato is they take images as inputï¼Œ this is an imageï¼Œ this is an imageã€‚

 and now what they're doing is they're projecting the color of this pixel value hereã€‚

To every voxel along this rayã€‚This pixel to everyï¼Œ color of this pixel to these v along this rayã€‚

 and then they're running a 3D confin on these spec projected color values in order to get the services outputã€‚

They're running it around 32q blocksï¼Œ they're running in a relatively high resolutionã€‚

 meaning these 32q blocks are relatively small in terms of spatial extentsã€‚

 so it takes really a long timeã€‚ So this is maybe maybe not the best idea to do itã€‚

 but it's a pretty cool paper actually I like this one a lot because they got really good reconstruction resultã€‚

 The only downside is it took a really long timeã€‚ğŸ˜Šã€‚

Now I mentioned this already the way to do this in practice is you would do this in a fully convolutional sense and this is one idea what you can do similar to this again this perkelay paper in 2D you can train in this case you just train on chunks so you have a partial chunk here and you're trying to force the shape completion of this chunk again this is also simple autoencoder network there's nothing special about it but youre training on these chunks of let's sayã€‚

 I don't know if you cube meters in size and then what you do is if this is fully convolutionalã€‚

 you can apply this to an entire scene at the same time and what this entire scene at the same time mean well these are convolutions convolutions are special invariant so we can run this on a single forward pass assuming you fit in the entire scene here as inputã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_13.png)

And what's pretty funny is in this case of this workã€‚

 this one fits on a GPU for most sceneries of a whole floorã€‚

 so you can do shape completion or semantic segmentation on the whole floor in one shotã€‚è¯¶ã€‚Yeahã€‚

 there's a few things to make the predictions here betterã€‚ It's not just a simple auto encoderã€‚

 You can do things like autoaggressive models in 3Dã€‚ It's similar to a pixel CNNï¼Œ but in 3Dã€‚

 the idea is you have these parallel auto aggressivegressive networks where you have like voxel group 1ã€‚

 voxel group 2ï¼Œ voxel group 3ï¼Œ voxel group 4 and so onã€‚

 So basically what you're doing is you're predicting these voxel firstã€‚

 Then these voxels that are chasingten to a next condition on these onesã€‚ğŸ˜Šã€‚

Then the next v group condition on the first twoï¼Œ then the fourth one condition on the first three and so onã€‚

 So instead of in so this parallel auto aggressive stuff in 2D had four groupsï¼Œ rightã€‚

Now we havenova in 3Dï¼Œ we have to cluster the voxels into a set of eight distinct join sets where each of them doesn't have any neighbors in itself rightã€‚

 So this one has no chasing in neighbors but This one doesn't have any neighbors by itself and so onã€‚

 So practically what you do is you're training eight networksã€‚

 So you have these parallel auto regressiongressive networks in 3Dã€‚

 you can also do stuff like course to find predictionsã€‚ In this caseã€‚

 you can feed you can just train in lower resolution firstï¼Œ then taking these predictionsã€‚

 feeding these ones to the next level inã€‚ you have a partial scene here againã€‚

 you get the predictions of the next levelã€‚Againï¼Œ feed this to the next level and so onã€‚

 So in practiceï¼Œ in this caseï¼Œ we have three times 8 networks that we're trainingï¼Œ1ï¼Œ2ï¼Œ3ã€‚

 full hierarchy levelsï¼Œ8ã€‚ğŸ˜Šï¼ŒFor the auto aggressive blocks and then you can run this stuff on an entire scene in one shot so you get scenes that look like these ones as inputã€‚

 this is the partial scenes and you're getting predictions that roughly look like these ones here so and it's pretty nice and this runs in one shot and you get actually pretty decent 3D reconstructions that you can use here this method also does the semantic segmentation in parallelil and this has also been shown that basically if you do completion and segmentation at the same timeã€‚

 the completion helps the semantics the other way around not that muchã€‚å—¯ã€‚

So if you're looking at the conclusions so farã€‚I meanï¼Œ it's very intuitive what we're doing hereã€‚

 rightï¼Œ We basically have volumetric gridsã€‚ They are relatively easy to understandã€‚

 They can encode the free space very wellã€‚ They can be encoded withã€‚

Where you can use distance fieldsï¼Œ right in order to get super resolution of the voxelsã€‚ğŸ˜Šã€‚

But we're going to always run into issues like memoryã€‚

In memory in this case we're running both for its whole training data size because this stuff is really heavy right I mean you can get it right if you're feeling an 128 cubed voxel grid with a floating point number for each voxel that's quite a lot of memoryã€‚

ğŸ˜Šï¼ŒIf we have likeï¼Œ I don't know a few million train samplesã€‚So it needs a lot of memoryã€‚ğŸ˜Šã€‚

It also needs a lot of processing time because you have to generate the vization and everything runs in 3Dã€‚

 Wellï¼Œ in the networkï¼Œ then it's in 3 plus oneã€‚ if you consider the feature depth as wellã€‚

 the good thing is you can use sliding windows of fully convolutional networks in practiceã€‚

 you would now always do fully convolutional network as long as you fit it into memory or test timeã€‚

 And that's pretty nice because then youã€‚ğŸ˜Šï¼ŒYeahï¼Œ then you can basically try these kind of thingsã€‚

On on arbitrary large scenesï¼Œ so you can train on arbitrary chunksï¼Œ rightã€‚

 but you can test sorry train on fixed sized chunks and then you can test on arbitrary scenesã€‚Yeahã€‚

 so if you're looking at the memory that's obviously going to be a big problemã€‚

 one thing we will see veryï¼Œ very quickly here is if you're taking a3 shape and you're valilizing itã€‚

 the higher the the higher the voxal resolution goesã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_15.png)

The smaller the occupancy he getsã€‚ In other wordsï¼Œ I will have fewer boxes that will have for one and more boxessã€‚

 relatively speakingï¼Œ that will have a 0 andã€‚ğŸ˜Šï¼ŒOf courseã€‚

 we want to have a higher resolution because we want to capture all the fine scale detailï¼Œ rightã€‚

 we want to capture all these details here at the bottomã€‚ It's not a higher resolutionã€‚

 but it's at least higher than this oneã€‚So we're going to 1 from 10% occupancycc hereã€‚

 I think this isã€‚32 cubedã€‚To 128 would be 2ã€‚1% right it's quite a difference actuallyã€‚ğŸ˜Šã€‚

And this is actually the main issue in the memory right nowã€‚

 we are very inefficient in terms of encoding these shapes if you're talking about the volumetric encodingã€‚

And one way you could think about it isã€‚Wellï¼Œ we want to reduce the memoryã€‚

 so maybe we only want to reduce the memoryã€‚Oh we only want to store the voxels explicitly that are close to the surfaceã€‚

 right at someï¼Œ this becomes just much easierã€‚ And wondering to do thatã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_17.png)

Is using volumetric cararchiesã€‚The idea is basically you want to have a high resolution when you're close to the surface and a low resolution when you're far enough away because when you're far enough awayã€‚

 you just want to store a high distance value or an empty block anywayã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_19.png)

And this is a couple of papers that did thisï¼Œ Onet OCNN popular examples of thatã€‚

 they did this for shape classificationã€‚So let's have a look at this one here firstã€‚

 if I'm having a dense confinï¼Œ what I would have is this is my lower solutionã€‚

 I'm encoding my shape hereï¼Œ I subdivide it and I subdivide it again and basically I have a lot of vels here that are not related to the surfaceã€‚

The idea of Onet is I have a volumetric hierarchy in this case in 2D it would be an tree sorry in 2 would be a quad tree and in 3D it would be an tree that's why it's called Onet So the idea is so the visualization is a quad tree of course right so you have the this is the original resolution right this is an8 cubed resolutionã€‚

ğŸ˜Šï¼ŒBut now what you can do is you can sayï¼Œ wellï¼Œ here this oneã€‚

 you have only a very high resolution here when it's far awayã€‚And then when it's closerã€‚

 you subdividide it once basicallyï¼Œ rightï¼Œ or here you subdividide it twice or here you subdivid it three timesã€‚

 rightï¼ŸğŸ˜Šï¼ŒAnd the idea is if I'm using a distance fieldï¼Œ or if I use an occupancy gridã€‚

 this whole v tells me that this whole space here is emptyã€‚And which is enough in this caseã€‚

 because it's far enough away from the surface that I know all of the stuff in there is emptyã€‚

 if there was a voxel in itã€‚That is not emptyï¼Œ I would have to subdivide it rightã€‚

 but I would do this subdivision adaptively and because of that I'm saving a lot of memoryã€‚

In this caseï¼Œ I can do that for discrimininated tasksã€‚

 it's relatively easy like classification because I do know the structure of my modelã€‚

 I know the 3D shape already in advanceï¼Œ rightï¼ŸğŸ˜Šï¼ŒAnd because of that I know how to do the subdivision and this is why OCnet and OCNN We grow well for these tasksã€‚

 rightï¼ŸAnd yeahï¼Œ if you compare it hereï¼Œ you see like this is encoding the same surface down hereã€‚

 but you just need significantlyï¼Œ significantly less memory hereï¼Œ rightï¼ŸğŸ˜Šï¼ŒOkayã€‚

 so in terms of performanceã€‚Its it's relatively interestingã€‚ I meanã€‚

 this is what they decided in the paperã€‚ I think in terms of performanceï¼Œ what they showed isï¼Œ wellã€‚

 at some pointï¼Œ you plateau in terms of accuracyã€‚ this is what they had as a baseline with a densenetã€‚

 the Onet was a little bit betterã€‚ But essentially the image the import resolutionï¼Œ somebody Pauusã€‚

 But at the beginningï¼Œ if you have a very low resolutionï¼Œ you haveã€‚ğŸ˜Šã€‚

You have a penalty in terms of the classification accuracy you're having at the endã€‚

So Onum basically shows that you getã€‚Densenet and Onet get basically the same performanceã€‚

But the Onet needs significantly less memoryã€‚ I don't have the exact number hereã€‚

 but it's at least a factor of 10 or soï¼Œ maybe moreã€‚

And that is pretty good because one of them grow cubically and the other one grows graticallyã€‚ğŸ˜Šã€‚

Kind of the nature of a cubic gridï¼Œ right state of the art is a little bit higherã€‚

 But that's mostly because of like different architectures and so onã€‚

 And But the interesting thing about Onet is once you encode a network like thatã€‚

 if you're running atã€‚ğŸ˜Šï¼ŒIf you're running a lookupã€‚

 you just check if you want to interlolate between this vel and that velã€‚

 you just interlo between those twoã€‚ so in this case from a surface it's an identical representation what Onet can encodeã€‚

Howeverï¼Œ there's one small differenceï¼Œ it's not so smallã€‚ the feature maps are also being reducedã€‚

 so in other wordsï¼Œ this feature mapï¼Œ I have less storage for this region of a feature map than I would have hereã€‚

 So in principle you could say maybe it is useful for the network to have valuable features stored in these vs here that here you don't have space forã€‚

ğŸ˜Šï¼ŒSo that one you can argueï¼Œ but this one' doesn't make a big big performance hit for some stuff it is a little bit worse in terms of performanceã€‚

 but it just requires so much less memory that this is a relatively cool architectureã€‚

You can do similar things for generative tasks In this caseã€‚

 we want to infer the structure so we have denses at the beginning rightã€‚

 and then we have two ay levels that basicallyã€‚There's two of themï¼Œ two of these papers appearedã€‚

 One of them I really like is a generating networksã€‚

 So here the idea is the train is actually end to endã€‚ So what they do isã€‚

The end method is basically saying for every voxelã€‚

 you're gonna to predict is it fully empty or is it fully occupied or is it partially occupied if it's partially occupied in the processã€‚

You will do a subdivisionï¼Œ and this is not predeterminedã€‚ So in this caseã€‚

 the network makes a determination at test time whether the voxla has to be subdivided and be allocated in more spaceã€‚

ğŸ˜Šï¼ŒAnd this one can be trained intoï¼Œ so basically you have the dense version then you predict one ay levelã€‚

 next  ay levelï¼Œ next  ay levelï¼Œ and so onã€‚ğŸ˜Šï¼ŒSo this is pretty cool and this works for generative tasks I like itotman this is enter andtrain because basically then the fine scale structure will tell you how you have to subdivide the early levels here because you get gradients for itã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_21.png)

Okayï¼Œ so the conclusion so on hereï¼Œ the hierarchiesã€‚ğŸ˜Šï¼ŒThey're great at reducing memory in runtimeã€‚

 rightï¼Œ because you just need a fraction of the storageã€‚

 And because that you can also make it fasterã€‚ It does come out a little bit of a performance hitã€‚

 That's what I was mentioning is basically the feature maps are not stored everywhere anymoreï¼Œ rightã€‚

 soã€‚ğŸ˜Šï¼ŒYeahï¼Œ you don't have the same amount of feature provocation what you would have in a tense gridã€‚

It's a bit easier for discriminative tasks when you know the structure in advanceã€‚

 but it is possible for generative tasks too when you can formulate this as a classification right where you sayã€‚

 ohï¼Œ do you have to subdte the v willï¼Œ or is it partially occluded or not rightï¼ŸğŸ˜Šã€‚

And then it works relatively wellï¼Œ I would sayï¼Œ so hierarchies are pretty interestingã€‚ğŸ˜Šï¼ŒYeahã€‚

 this this one is is is quite funnyã€‚ One thing a lot of people at the time and these papers came all discussed wasã€‚

How much does the3D part actually help right andã€‚One type of networks uses actually multife informationã€‚

 So they sayingï¼Œ wellï¼Œ2 networks on images are pretty goodã€‚

You have pre trained versions on image and and like thatã€‚And the idea code beã€‚

 maybe we want to use these features and protect them to 3D and then do something in 3Dã€‚

Or we accumulate the features for multiple viewsã€‚This is very similar what we've seen in neural renderingã€‚

 except now this is not for image generationï¼Œ but it's for classification tasksã€‚ğŸ˜Šã€‚

And there was this one cool paperï¼Œ this multi few confident paper for 3D shape recognitionã€‚

 these ones to multi fewã€‚ğŸ˜Šï¼ŒShape classificationã€‚ So what you have is this is purely syntheticã€‚

 There's still no real data right nowã€‚ you have a model here from Shapenett Shapenet is this model database that people use often for 3D learningã€‚

 they do have a bunch of cameras hereã€‚ virtual cameras around this objectã€‚

 They render this object from these from these enfuseã€‚ğŸ˜Šï¼ŒThey get the render imagesã€‚Nowã€‚

 what they do is they're running a 2D con for each of these images independentlyã€‚

They're going to get a feature map for each of themã€‚

But instead of doing the classification here on a per image basis for each of the renderingsã€‚

 they have a few poolingï¼Œ so you have just the max pool basically you can do other poolsã€‚

 but they use the max pool this few pooling is nothing else but a max operator that selects the features from each of these maps and then you have another CNN that works on the concateninated features sorry on the max pool featuresã€‚

ğŸ˜Šï¼ŒAnd then you get the classification scoresã€‚And this one can be trained at to end because the pooling is differentialã€‚

 rightï¼ŸğŸ˜Šï¼ŒAndã€‚This works remarkably well and the reason why this works remarkably well is because these networks hereã€‚

 they all share are this is CNNnn1 hereã€‚ they all share the feature the same weightsã€‚

 These months work so well because they are pre-train on imagenet right so you can do your 2D pretraining and then you few pullinging and then you still aggregate information from multiple views at the same timeã€‚

 The other thing what this is a very nice network is there few pullinging is independent of the number of images here So I could train5 and then test on 10ã€‚

 So this is invari into the number of images you feeling here is inputted and this is pretty coolã€‚ğŸ˜Šã€‚

And it gave at the time actually pretty good results from these shape net classification task benchmarksã€‚

 this was at that timeï¼Œ Oh it probably still is one of the best papersã€‚

 one disadvantage we have hereï¼Œ thoughï¼Œ this is a toolD network on the renderingsã€‚

 And you're doing a few pooling of the feature mapsã€‚ğŸ˜Šï¼ŒAt this pointã€‚

 you're using the spatial correlation right you don't know anymore like this pixel at the top here and this pixel at the top hereã€‚

 they will be pooledã€‚Togetherï¼Œ but they are totally different 3D positionsã€‚ rightã€‚

 This part in this part here is a different 3D locationã€‚

 even though they are the same intuitivetuityï¼Œ because the viewpoint changedã€‚

 they have the same intuitivetuityï¼Œ but different 3D positionï¼Œ rightã€‚ So that's a big problemã€‚ğŸ˜Šï¼Œå—¯ã€‚

Yeahï¼Œ so at the end of the dayï¼Œ we're still going to get a classification vector and we're going to getã€‚

ğŸ˜Šï¼ŒYeahï¼Œ we're going to get these results at the end of the day in this caseã€‚

 only is the rendered R toB here and no spatial correlation was takenã€‚

There's a way though to still maintain the spatial information and this is what people have been doing actually it was the same group they did this for multi few networks for shape segmentation so in this case the goal is you want to have a 3D label for every surface point on this plane here in these are the wings right and so on so you have a bunch of a bunch of semantic labels for the object parts hereã€‚

ğŸ˜Šï¼ŒAnd the idea how this works is similar as beforeã€‚So we're having a 3D shapeã€‚

 we're having a bunch of view points around for each of the few points we rendering an imageã€‚ğŸ˜Šï¼Œå—¯ã€‚

For each of these images we're running a 2D segmentation network in this caseã€‚

 FCN just because it's an easy networkï¼Œ for every pixelã€‚

 you're going to get a confidence map for each semantic labelã€‚

 so these are the score functions for the classes basically for the semantic classesã€‚

For every pixel its a twomantic segmentation networkã€‚ Nowï¼Œ what's being done is thisã€‚

Confidence maps are back projected againï¼Œ to the surfaceã€‚ğŸ˜Šï¼ŒOn the surface pointsã€‚Rightã€‚

And then there's a conditional random field that is being run in order to aggregate these features you could also run like a per surface point pooling or something like this it's in a way similarã€‚

 but in this case they use it CRF and then they have a loss functions for the labels that is being predicted based on the combination of these per few object maps that are back projected to 3D and fed into this conditional random field and this CRF is actually differentiable so they can train this whole process all into endã€‚

ğŸ˜Šï¼ŒAnd the nice thing here is right nowï¼Œ now we maintain the spatial structureã€‚Rightï¼Œ so in this caseã€‚

Due to the spec protection stepsï¼Œ this point of the wing and this point of the wing will be met back to the same surface pointã€‚

And I try to end endï¼Œ and I think this is coolï¼Œ a pretty cool ideaã€‚ğŸ˜Šï¼ŒYeahã€‚

 there's a few things you can tryã€‚ One thing we realized at some pointã€‚

 we played around with this a little bitã€‚ğŸ˜Šï¼ŒWhen you do multiy renderingsã€‚

 so you have a 3D shape hereã€‚You're going to get a three shapeï¼Œ you can vize itã€‚

You can have different renderingsï¼Œ how to render this voxel gridã€‚ You can render the meshã€‚

 You can render the voxel gridã€‚ One thing we tried is we use a multi fupher rendering so you can render each v list a sphereã€‚

 and this works surprisingly wellã€‚ You can also render these spheres with multiple resolutionsã€‚

 And at the time of this of this model at 40 benchmarkã€‚

 it's also another benchmark in this shape classificationã€‚ğŸ˜Šã€‚

These spheres work surprisingly better than everything elseã€‚ So apparentlyã€‚

 the printer and image features were very good for this kind of combinationã€‚ğŸ˜Šï¼ŒSo yeahã€‚

 I think that's kind of coolã€‚ So in this case thoughã€‚

 none of these networks right now explicitly use the 3D information right away but they use a 2D rendering rightã€‚

 and then there's going to be some sort of feature agregationã€‚ğŸ˜Šï¼ŒIn the 2D domainã€‚Right now againã€‚

 I mentioned this this question what I raised before is now which one is better do we do it in 3D or do we do these like 2D renderings and kind of circumvent the problem that we do in 3Dã€‚

 Now the it is we can use hybrids between thoseã€‚ We can use volumetric networks and we can use multife networksã€‚

 and specifically for real 3D scansï¼Œ This is very interestingã€‚ğŸ˜Šï¼ŒBecause in the real 3D scansã€‚

 I'm going to have a geometric channel from the geo from a La scannerï¼Œ for instanceã€‚

 so from a connectï¼Œ but I also have RGB signal from an RGB cameraã€‚

And the first thing what you know what people renewableã€‚

 so we tried that is it's a very simple experimentã€‚

 so we try to do semantic segmentation in this case on scannetã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_23.png)

And the idea was if youre only feeding in the geometry in a 3D continentã€‚

 you're gonna get something like 504ish percent class accuracyã€‚

 average class accuracy right it's not a supert your networkã€‚

 it performed reasonably well but it's 54ã€‚ there was kind of a baselineã€‚

 And then we thought Now if were adding the color valuesã€‚

 we should get a higher signal right for every voxelï¼Œ we just gonna add add a color valueï¼Œ you knowã€‚

 And just in the 3 d voxelsï¼Œ we write one color value per velã€‚ So with geometry plus voxelsã€‚

 and this increased the numberï¼Œ but only veryï¼Œ very marginallyã€‚

 So we were disappointed when we saw that numberï¼Œ we thought like so color only helps you by like 1%ã€‚

 that's not so greatã€‚ We thoughtã€‚ğŸ˜Šï¼ŒIt would be nice if you combined itï¼Œ we would get better resultsã€‚

And the reason why this doesn't work better is because there's a U resolution in this muchã€‚ğŸ˜Šã€‚

In other wordsï¼Œ when you have a depth sensorï¼Œ typically the depth is a lower solution and the colors is at a higher solutionã€‚

Nowï¼Œ in this caseï¼Œ this network here learns at the same resolution becauseã€‚

 you know we're feeding in one color value and one occupancy value per vxyyle inã€‚

 and that's a big mismatch so then you can really take advantage of the colorã€‚

So the remedy to that is this hybrid network that I already hintedã€‚

 so in this case you say we're going to have a 3D scan right with a geometry and this scan has also been the associated RTB images like these ones hereã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_25.png)

And the hope is that we want to predict semantic levels for each of the surface points here right like this bed floor chairsã€‚

 couchs on right and the idea is because we have the alignments of the images here to the skin we can actually we can actually design a hybrid network that leverages both at the same time and the idea is relatively easy so what you're doing is you have the color input hereã€‚

ğŸ˜Šï¼ŒAndã€‚With the color inputã€‚You extracting 2D featuresï¼Ÿ

And these2D features are done with a 2D network independentlyã€‚You be project these features to 3Dã€‚

And these backproed features to 3D are then thenproed to the voxsã€‚

But the difference now is it's not ca values that are being back projected hereã€‚

It's actually voxels that get feature maps alreadyã€‚

 So because there's a 2D network that runs and extracts features on every image independentlyã€‚

 this network can also be printed on imagenetã€‚ğŸ˜Šï¼ŒThey are better than just havingã€‚

 like the naive color resolutionã€‚B can already encode some information about the appearance in the local regionã€‚

 rightï¼ŸAnd the idea is you're backward checking features from a 2D networkã€‚

 so running a 2D network for every image firstï¼Œ getting 2D featuresï¼Œ backward checking these to 3Dã€‚

 backward checking themoxelã€‚ğŸ˜Šï¼ŒNow you're running a 3D conf on the Voxelsï¼Œ rightï¼ŸAndã€‚

And then what you can do is you also get a 3D feature map here for the voxelsã€‚For the geometryã€‚

 we get a feature map here for the color and then what you do is you just concatenate these twoã€‚

 and then you get the class levels togetherã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_27.png)

So architectally speaking this look like thatã€‚ Let's start hereã€‚ We have either 3D geometryã€‚

 we have a bunch of 3D consï¼Œ a bunch of more 3D consã€‚

 and for every vel we're going to predict se landing levels right So it's like depending on how many classes have that's how many core outputs we have per voxelã€‚

ğŸ˜Šï¼Œå—¯ã€‚Now we have 2D images on the 2D images we run a 2D network we 2D featuresã€‚

 and this case you have a 2D lossï¼Œ it's a proxy loss that's just a segmentation in 2Dã€‚

 but instead of the 2D labelsï¼Œ the 2D features are being backproedã€‚

From each of the 2D images to a 3D volumeï¼Œ and then there's a bunch of 3D consã€‚

But run on these back project features and colorã€‚ Then these are in the same space and the geometryã€‚

 and they're being concateed with the geometryã€‚ and then they're being runã€‚

 There's one more caveatatã€‚ sometimes you haveã€‚Voxels that are not seen by any 2D imagesã€‚

 then you just sort out the featuresï¼Œ Sometimes you have voxes that are seen by one imageã€‚

Then you just have this feature map that you back projectã€‚

 sometimes you also have multiple images that have seen one voxel in this caseã€‚

 you just do simple max pooling so it's invariant to how many images have seen that actually rightã€‚

So nice that's kind of a nice ideaï¼Œ actuallyã€‚ğŸ˜Šï¼ŒYeahã€‚

 and the good thing is this whole thing can be trained into endã€‚ğŸ˜Šã€‚

And now the big question remains if you're taking color onlyã€‚

 so if you're only taking this party here at the bottom have 3D constant hereã€‚

 or we have the geometry only beginning these two results if you color it using color only you're getting 58% classification accuracyã€‚

 geometry onlyï¼Œ you're getting 54ã€‚4% accuracyã€‚ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_29.png)

So what does it tell usï¼Œ it tells us color and geometry work roughly the sameã€‚ğŸ˜Šã€‚

Color has a little bit of more signal in this case for this specific architectureã€‚

 but that's not really my pointã€‚ This's like 4%ã€‚ It's not such a big differenceã€‚

But the interesting thing is what happens if we train them now both at the same time and then we're getting 75% accuracy so the assumption assumption here is that both color and geometry have complementary signalsã€‚

ğŸ˜Šï¼ŒThat significantly help each other to get better results at the end of the outputã€‚Okayã€‚

 you can also try this out withã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_31.png)

Yeahï¼Œ different different viewsï¼Œ you can have a varying number of viewsã€‚

 the more views you using the better you getï¼Œ but eventually you going to plateauã€‚So in this caseã€‚

 the state of the office was like 75% accuracy or soã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_33.png)

Yeah this is the full ablation table hereï¼Œ this is when you only use one image then you get 27 if you had three imagesã€‚

 only the images you get 44 if you're running the 3D columns afterwards only on the color still then you get 58 if you're running only the geometry you get 54 if you're running 3D geometry plus voxel colors you get 55ã€‚

9 almost the same color per voxels doesn't workï¼Œ but now if you're adding the features in and more views then we're getting more and more better resultsã€‚

ğŸ˜Šï¼ŒSo feature is better than colors from 2D because then each feature is already encoding like aã€‚

Reach and so to sayï¼Œ and we can use preter to the imageã€‚So these hybrid networksã€‚

 theyre a really nice way to combine geometry and color informationï¼Œ and as we expectedã€‚

 or as we hopedï¼Œ there's actually different signal in color and geometryã€‚

 which is pretty nice right so this is a very nice thingã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_35.png)

So this gives great performanceã€‚ğŸ˜Šï¼ŒIt's the best so far for segmentation state of the art networks still use kind of similar things they might use a little bit different 3D architecturesã€‚

But this is still the best way to combine color geometryã€‚

 right end to end helps less than be hoped for This oneï¼Œ if I go quickly back hereã€‚

 this one is when you're not training into end and when you' training into end only makes half a percent of difference So it's a bit disappointingã€‚

 We thought this was betterï¼Œ but it is okayã€‚ It could be a bit fasterã€‚

 you basically have one loop that goes around every imageï¼Œ but it's okayã€‚

 you can do it relatively efficient still in like almost real time or interactivelyã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_37.png)

![](img/d01848c02ede5ebfc9c01f50058ec02b_38.png)

Okayï¼Œ so these so farã€‚Volumeric networks that operate in voxelgrisã€‚Nowï¼Œ the alternative to that isã€‚

 this is whatã€‚Many people do todayï¼Œ is use point cloudsã€‚

So point clouds is for every 3D point that you're capturing with a LiDR sensor instead of mapping into a unified voxel gridã€‚

 you just store a linear list of 3D points right so a point is x y and z tells your spatial location and then you have I don't knowã€‚

 like1ï¼Œ00ï¼Œ20003ï¼Œ4000 points in a linear arrayã€‚And you can do deep learning directly on the point cloudsã€‚

 I mentioned before this is a 1 T encodingã€‚ you could use a convolutionã€‚

 but people tried that and it's not so great if you just naively do a convolution on the 1D array because you're convolving over spatially disjoined regions right so that's not so idealã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_40.png)

A very popular paperï¼Œ this is from Stanford's appointmentã€‚ğŸ˜Šã€‚

So pointnet is a paper for doing things like classificationã€‚

 part segmentation or semantic segmentationã€‚On the point lots directlyã€‚

And the way this ro is basically they got rid of the convolutionsï¼Œ they basically use they use MLPsã€‚

 they use fully connected layers and the idea is you have an input point cloud here that is n times 3 so if n pointsã€‚

 every point has three locations x1 z that's what I said right the idea is you basically run an MLP hereã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_42.png)

On this arrayã€‚And you're going to get a bunch of features out of it while you're running this for a couple of layersã€‚

 rightï¼ŸAnd eventuallyï¼Œ after this series of fully connected layers on the pointsï¼Œ youre gonna getã€‚ğŸ˜Šã€‚

1024 feature vector for every pointï¼Œ so it's n times 1024ã€‚Andã€‚Nowã€‚The high level thought isã€‚

 well this point cloud is unstructuredã€‚Rightï¼Œ so you have an unstructured point cloudã€‚Andã€‚

What you would love to do is do some sort of sortingã€‚But we don't know really how sorting wordsã€‚

 so the only thing what we can do is we can run a max pool operator to select the most indicative featuresã€‚

So what you can do now is youã€‚Runnerax poolã€‚Of the 1024ã€‚

Features that you would like to extract from this n times 1024 feature vectorsã€‚

 we get a global feature vector of 1024 lengthã€‚And then you run simply another multilayer perceptronã€‚

To get output scores so ignore the stuff here at the top at the bottom first like just look at the blue stuff this is a classification network so you get points inputã€‚

 you run MLlPsï¼Œ eventually do max pooling to get a global vector this max pooling concates all the features from different regions basically rightã€‚

And then you run another LLP and you're going to get output scores for the shape classification and this thing can also be drain enter end of courseã€‚

 and this is the basic version of pointã€‚ğŸ˜Šï¼ŒNowã€‚What he're doing here is you're losing when you do this next poolã€‚

 you're losing the spatial correlationsï¼Œ of courseï¼Œ rightï¼Œ you're just getting featuresã€‚

 you're aggregating the featuresï¼Œ and then essentially you're kind of votingã€‚

 so to say what to do with thatã€‚You're getting an output scores of thatã€‚

 The interesting thing is these global featuresã€‚ğŸ˜Šï¼ŒThey lose the spatial information right for classificationã€‚

 this is fine because we just want to always that a chairs the sofa is a couchã€‚

 If we did semantic segmentationï¼Œ then we would have a problem because we would have to map this back to the global to the global shape againã€‚

 And this is what they did for the semantic segmentation network hereã€‚

 So you have here these n times 24ã€‚ğŸ˜Šï¼ŒYou have 24 dimensional vectors here per pointã€‚

 you have n for every point right it's n n points Now what you do is you simply concatenate this the global feature vector to each of themã€‚

 so this the very same 1024 feature dimensions that defines the entirety of the point cloudã€‚ğŸ˜Šã€‚

It's being concateed to each point featureã€‚And then you have another bunch of multilayer perments that operate on this point levelã€‚

 and then you get n times M outputs so at every point you're going to get M output scores where M is the number ofã€‚

Classes that you haveï¼Œ rightï¼ŸFor semantic segmentation and this one now is spatially correlated because this one is for every nã€‚

 you're getting m dimensional output scoresï¼Œ rightï¼ŸğŸ˜Šï¼Œå—¯ã€‚Okayï¼Œ well that's itã€‚

 that's pointnet and it is this whole thing can also be trend into end of courseã€‚

 so you can do this for semantic segmentation and you can do this for shape classificationã€‚ğŸ˜Šï¼Œå—¯ã€‚

This was a very fundamental paper because this was the first one that showed that he can do these kind of things on points and don't need to go place like painfulã€‚

 painful process of converting everything to volume your gridã€‚å—¯ã€‚One drawback of pointing atã€‚

 It's very dependent on the sampling of the pointsã€‚

 If you have a lot of points in one region and very few points in another regionã€‚

 this is going to be an issueã€‚ So that's why the same groupã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_44.png)

Proposed group from Stanfordï¼Œ they proposed point plus plusã€‚

Which is essentially addressing this issue of the sampling mismatchã€‚

 So the idea is kind of they have variousï¼Œ they have kind of a multiscale grouping of pointã€‚

 They have very many pointnet locally speakingã€‚ So they learn this hierarchical representation of a point cloud and then they apply multiple point nets at the different locations and scalesã€‚

 and then they aggregate this later on right So in each scaleï¼Œ they do furthest point samplingã€‚

 they have some way of making this as uniform as possibleã€‚

 and then they have a multi scale multire grouping for sampling density of the robustness andã€‚ğŸ˜Šã€‚

This one gives much better resultsï¼Œ especially when you have larger scenesï¼Œ rightã€‚

 when you have some sort of sampling mismatchã€‚ you can often get these sampling mismatches because it depends on the locations of theã€‚

Of theï¼Œ of the pointsã€‚So these two papersï¼Œ point N and pointant plus plus are very popular architectures to do stuff in 3Dã€‚

 and they're very lightweight architecturesï¼Œ they don't have a lot of weightsã€‚

And they don't use convolutionsï¼Œ which is funny enoughã€‚ So people thoughtï¼Œ wellã€‚

 maybe you can also use convolutionsã€‚And there's a couple of papers like theseã€‚

 there's point convolutionsã€‚å—¯ã€‚What you can do for point convolutions is basically you transform the pointsã€‚

To some sort ofã€‚Our three space representation like radial basis functions and stuff like thatã€‚

Then you canvol in this spaceã€‚And you stick the results to the points and basically project it backã€‚

So these kind of papers for pointsã€‚Work to some degreeã€‚

And when I'm saying to some degree most of the timeã€‚

 these methods they didn't propose to use real data yetã€‚And one challenge for real data isã€‚

That if youre missing like a chair legï¼Œ you don't in the point cloudã€‚

 you don't have an encoding that there's no information thereã€‚ You just don't have any points thereã€‚

But like the free space information that you would have in a volumeary grid is not in hereã€‚

 right you don't have a sign distance field hereã€‚So for real dataï¼Œ the convolutions are not so idealã€‚

 like they didn't seem to work so wellã€‚ And there's a few new versions of itã€‚

 but practically speaking that's a bit of a problemã€‚

 So but the conclusions so far of the point is they train veryï¼Œ very fastã€‚

 they also super lightweight for testingã€‚ So if you want to do something on a phone or so point cloud appointment versions are pretty goodã€‚

ğŸ˜Šï¼ŒThey can cover very large spaces depending on the sampling densityï¼Œ of courseã€‚

 if you run like an 8 or 60 k pointsï¼Œ you can cover basically a whole room and you run it in one shot and it's very very light right this is a very big advantage a big disadvantage is what I said is they cannot represent the free spaceã€‚

 which is you're missing a bunch of information that the sensor would give you but you're not using as part of your network trainingã€‚

ğŸ˜Šï¼ŒBut explainingï¼Œ you knowï¼Œ if you wanted to see that againï¼Œ just go back in the videoã€‚

So the performance in practice is worse than the volumetric onesã€‚ğŸ˜Šã€‚

Like this is still like we have to say that right I mean if you're looking at the state of their benchmarksã€‚

 pointed versions are super lightweight to terrainï¼Œ they're easy to useã€‚

 there's a lot of cool code pages online if you want to get started I always recommend you something like that because it's easy to use but the overall performance is often a little bit worse so that that is often a bitã€‚

That challengeï¼Œ rightï¼Ÿå—¯ã€‚ğŸ˜Šï¼ŒThere's still a lot of ongoing research going onã€‚

 how to encode the points efficiently when you do convolutions locally and then you aggregate globally with point net and stuff like thatã€‚

So I added a couple of more references if you're interested in itã€‚ğŸ˜Šï¼ŒYeahï¼Œ there's a point of courseã€‚

 point plus Laï¼Œ there's point CNNï¼Œ that's also pretty interesting it's currently I think on an archive paper and these are global methodsã€‚

Meaning that you take the whole point cloud at some point and you aggregate into a global feature like then one way or another one well I think it's pretty cool if you have these point sets local and then do the aggregation globally rightã€‚

So there's there's a couple of things like tensionbased commvolutionsã€‚ It's a cool paperã€‚

 I really like thatã€‚ There's near point neighborsã€‚ you basically have a pointã€‚

 you check the local neighborhoodã€‚ and then you you run some operator first data to get some feature and then you run a point it afterwards soã€‚

 So that's pretty interesting combinations in terms of doing something local first and then you're making global or vice versaã€‚

 And I think that combination is kind of interesting also for future researchã€‚

 So if you're interested in thisï¼Œ I don't have the very recent oneï¼Œ this is like 2018 Nã€‚

 the Affs probably at 2019 paperss nowï¼Œ there's still a lot of ongoing workã€‚

 This is not not solved at allã€‚ğŸ˜Šï¼ŒSo this is basicallyï¼Œ in addition to volumesã€‚

 we can do things on pointsã€‚Another disadvantage of points is we don't represent the surface right if you're talking about 3D models from like computer graphicsã€‚

 you would think oh noï¼Œ wait a second you only have vertices you don't have facesã€‚

And that's indeed a disadvantageï¼Œ so you have to have a sampling strategies or sampling all that determine the performance in this caseã€‚

One thing the remedy here is mesh based methodsã€‚So you can sayï¼Œ wellã€‚

 we're taking the surface pointsï¼Œ but we're also going to go and have some triangles that are in values and stuff like thatã€‚

This is a very interesting areaã€‚ It's very tricky because you need a bit of understanding how measures workã€‚

 a bit of differential with geometryï¼Œ probablyã€‚ It also doesn't work so wellï¼Œ but in principleã€‚

 he has a lot of potentialã€‚ And that's why I think it's really this would be cool for more researchã€‚

 there's a lot of cool work already by Michael Bnstein's groupã€‚

 He's a professor at Imerial in Londonã€‚ andã€‚ğŸ˜Šï¼ŒThese guysï¼Œ they define basically various waysã€‚

 convolutions on a meshã€‚Andã€‚One idea what they can do is you can go into the lu spelld spaceã€‚

 canigen can take the eigenfunction in that space and can do convolutions in that space Now for synthetic meshes like this data sets like I think what is it like Fal so some of these data setsã€‚

 they all have synthetic data for synthetic data setsï¼Œ this work greatã€‚ğŸ˜Šï¼ŒBut in practiceã€‚

 it's challenging becauseã€‚If you have partial data on a real scanã€‚

 these local operators like a love plus Brami changes significantly so then the features will change and this is very difficult for the network to learn so this is like a very interesting ongoing research direction but it's not so clear how to solve it yet so if you interesting research why this is I think a cool direction Another thing what's pretty cool about the meshes is you can think about them as graphs again like microproduct group is doing a lot of cool stuff there like you can model it as faces and vertices and ets right and the idea is you can do operators on this connected graph and this is very interesting right because then you consider faces vertices and so on independently and there's gonna be a lot of interesting work coming up that does it on graphs directly there's message passing networks and so onã€‚

ğŸ˜Šï¼ŒThis is alsoï¼Œ I thinkï¼Œ a very interesting oneã€‚ So the conclusions here so far is that meshes and surfacesã€‚

 they typically use some differential geometric approximationationã€‚ You't even know thatã€‚

 And the conclusions is then in the spaceã€‚ Most of the timeã€‚

 I haven't seen a lot of results in real data yetã€‚ğŸ˜Šã€‚

Because they're very prone to noise and incomplete scansã€‚

 and these local differentiable geometric actual excation don't work so wellã€‚ğŸ˜Šï¼ŒThis is a thingã€‚

 but it's I think this shouldn't prevent us as researchers to look into it because there's a lot of potential in it because ideallyã€‚

 if you represent the surfaceï¼Œ you should be able to get pretty good results right like you should in principleã€‚

 get better results than point itï¼Œ but so far that doesn't seem to be the caseï¼Œ but againã€‚ğŸ˜Šã€‚

I think making this one to work on real scansï¼Œ this would be coolã€‚Okayã€‚

 I have one last category I want to talk aboutã€‚ğŸ˜Šï¼ŒAnd yeahï¼Œ if this was a bit too fastã€‚

 maybe pause the video for a secondï¼Œ take a coffee and then continue but the last category I want to talk is about sparse commvolutions and this is going to send an alternative to wellã€‚

 it's kind of a combination of similar to pointsã€‚ğŸ˜Šï¼ŒBut it uses local convolutionsã€‚

 but only in the regions where we have actually a surfaceã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_46.png)

So if we're visualizing this in 2Dï¼Œ we would get something like thatã€‚

 right if you are running a conf operator in 2Dï¼Œ we have a regular convolution hereã€‚

 we are starting now like thisï¼Œ running one confï¼Œ running another conf for another confã€‚

 so the surface of this circle here is kind of being dilated into the rest of the image right so every conf is spreading the features into the spaceã€‚

ğŸ˜Šï¼ŒBut what that means is we have at the beginningï¼Œ we have a lot of zeros here on the outside and we only have active regions across the surface because there's featuresã€‚

ğŸ˜Šï¼ŒSo the set of active regions basically grows very quickly as soonã€‚As we running comesã€‚

This is similar to what we had with dense convolutions on the volumetric grids versus the operators that we had with the hierarchiesã€‚

 like Onet and so onï¼Œ rightï¼ŸğŸ˜Šï¼ŒYeahï¼Œ so there's two things hereã€‚

 so this takes a lot of memory because these active regions spread everywhere and of courseã€‚

 someone we have occupanciesã€‚ğŸ˜Šï¼ŒAnd the idea isã€‚å—¯ã€‚Yeahã€‚

 the idea here is the sparse convolutions don't spread hit to nonactive regionsã€‚

 So the idea is if I'm having a sparse convolution hereã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_48.png)

So if I have densï¼Œ rightï¼Œ we're running itï¼Œ it spreadsã€‚If it's sparseã€‚

 the idea is you're running a masked kernel and only taking the regions that are activeã€‚

 so only the green regions here are active and the red regions are masksed out againã€‚

 green regions are activeï¼Œ red regions are masksed outed and so on and these kernels are learnedã€‚ğŸ˜Šã€‚

But the masking is depending on the data that is coming inã€‚ so whenever you don't have dataã€‚

 you just mask it outã€‚ but the nice thing is hereã€‚You can run these convolutions and after one layerã€‚

 you still have the same active friã€‚ So whatever is green stays greenã€‚

 whatevers red stays red right and this is pretty niceã€‚

 So you can have basically convolutions only around the surface so you don't have to even store this whole rest here you never have to allocate any memory for it and in practice it looks like that right So this kernel here is then being like used for the entiretyã€‚

 Of course this can be done in parallel too So the active set remains unchangedã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_50.png)

Green remains greenï¼Œ and red remains redã€‚å—¯ã€‚And you might ask how do we get feature provocationï¼Œ wellã€‚

 of course this kernel this is a three by three kernel hereã€‚

 but you have various size kernels this is called yeah thisã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_52.png)

This paper is called submanifold sparse combvolution networks that did thisã€‚

 but the idea here is that of course if you have varying kernel sizes and so onã€‚

 you're connecting disconnect disconnected pieces if the kernels are larger but at first you might not communicate so you need a couple of layers and so on right so eventually we'll have this by striding pooling constant and so on so you need to be careful how the kernel size and the strides and everything affects the rest olation possibly toã€‚

ğŸ˜Šï¼ŒSo yeahï¼Œ these this is a bit of a drawback so you don't have features everywhereã€‚

 but the nice thing is you can run this in a reallyï¼Œ reallyï¼Œ really high resolutionï¼Œ rightï¼ŸğŸ˜Šï¼ŒAndã€‚

And in practice how this is actually implementedï¼Œ I'm not sure if I have this hereã€‚ğŸ˜Šã€‚



![](img/d01848c02ede5ebfc9c01f50058ec02b_54.png)

![](img/d01848c02ede5ebfc9c01f50058ec02b_55.png)

Yeahï¼Œ in practiceï¼Œ this is implemented with a hash functionã€‚

 So what you're doing basically is you just store every green voxel here or every green pixelã€‚

 you store as an active entry in the linear arrayï¼Œ you only store the stuff that is activeã€‚

 and then when you run the convolutions you know where you produce output that's just a copy of this green array and you know how to compute itã€‚

 you just look up in a hash tableã€‚ What are the previous entries that contribute to this current region of the next layerã€‚

 right So it's very efficient to doï¼Œ there's one interactiondirect in the lookupã€‚

 but I'm saving like whatever 90% of my memory so I can go to reallyï¼Œ really high resolutionã€‚

 a performance actually is slower because I have this interaction than a dense conf but I can have basically significantly higher resolution And the reason why this stuff and I'll show you a few numbers later why this stuff works so well is because you can just go to higher resolutionsã€‚

 So higher resolutions just give you better results for most tasks right you can capture more signalã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_57.png)

Dis guy that this for a couple of things that did it in in 1 D in 2D for like land drawings and so on and for 3D where you have aã€‚

Like semantic segmentation and so onã€‚ So the conclusions here so far is I mentioned this this is implemented with a spatial hash functionã€‚

 If you enter look at the codeï¼Œ it's actually super easy to understandã€‚

 So Facebook release this codeã€‚ they only have the features around the surfaceã€‚

 It requires significantly less memoryã€‚ğŸ˜Šï¼ŒAnd so it also a higher solution gives you better featuresã€‚

Better performanceï¼Œ but slower runtime because you have this indirection of the hash table lookupã€‚

 which is it's still on the GPUï¼Œ but it's slower than the dense consã€‚

So I wanted to look at a result hereã€‚ğŸ˜Šï¼ŒAnd this is the scan benchmark you see here at the top this sparse con paper I mentionedã€‚

 this is semantic segmentationï¼Œ the numbers mean you look at it hereï¼Œ this is average IOUã€‚

 this is separated by classesï¼Œ this number here is 0ã€‚

7 something which is a pretty high number so these sparse cons currently perform the best on this benchmarkã€‚

Soã€‚The reason why they perform best isã€‚They just can operate at a higher resolution than ends convolutionsã€‚

 So the Mikovsky net is actually a very similar ideaï¼Œ also uses sparse convolutionsã€‚

 I think they did a bunch of hyperbromatingã€‚ I think Minkovsky is now in front of thisã€‚

 but theyre very similar in terms of the techniquesã€‚ğŸ˜Šï¼Œå—¯ã€‚

The idea is basically you have higher solution because of that you're getting better numbers they don't interestinglyã€‚

 they don't use any distance valuesã€‚ you could do thatã€‚ we've tried this internallyã€‚

 it gave a bit better results stillã€‚ğŸ˜Šï¼ŒBut it takes also a lot of effort to get there there's texture based versions like this is texturenet I think this was CPL payment we had a while agoã€‚

 these methods they basically run textures on top of the meshã€‚There's dense volumetric onesã€‚

 this is like the 3 d MVï¼Œ that's what I showed you when you combine color and geometry in this caseã€‚

It' use dense convolutions if you use this one with sparse cons you actually hear at the top againã€‚

 but color still helpsã€‚If you're comparing here quickly pointnet is roughly hereã€‚

 so pointed architectures don't work so well in the dense month that's roughly the sameã€‚

 there's one version that does dense and point jointly that works betterã€‚

 but this is a complicated architecture and yeah the sparse con is currently dominated allã€‚

This is actually already a bit of an old versionã€‚ I didn't pull the very recent numbersã€‚

 you can have a look at the benchmarkã€‚ You can see where we are roughlyã€‚

 but still very open research directionã€‚ So this is currently on semantic segmentationã€‚

 And then you can also look at the other things likeã€‚ğŸ˜Šï¼ŒSemantic instant segmentation and so onã€‚

Okay I'm very excited about this areaï¼Œ this is also something I hope some of you do in the projectã€‚

 but also of course later if youre interested in research directionã€‚

 this is something we're always looking for people to have a passã€‚ğŸ˜Šã€‚

So this is actually the last lecture right now with the content from the lecturesã€‚

 I'm sorry that'll be were a little bit lateï¼Œ we're all working on the nerve Stline right nowã€‚

 which is this weekã€‚ğŸ˜Šï¼ŒOf course have fun with the project I think this is a really unique opportunity for everyone Think about your research opportunities right think about all the cool things you you can do we try to make this course relatively the lectures relatively research oriented so we talked a lot about research papers a lot of the details of course we couldn't cover in all the lecture slides you have to look at the papers itself we try to give you a bunch of references I also appended a bunch of references to this slide deck yeah and I hope I hope I'm gonna to see we going to see some some really cool results in the project so yeah see you there and see in the presentations take care stay safe and healthyã€‚

ğŸ˜Šã€‚

![](img/d01848c02ede5ebfc9c01f50058ec02b_59.png)