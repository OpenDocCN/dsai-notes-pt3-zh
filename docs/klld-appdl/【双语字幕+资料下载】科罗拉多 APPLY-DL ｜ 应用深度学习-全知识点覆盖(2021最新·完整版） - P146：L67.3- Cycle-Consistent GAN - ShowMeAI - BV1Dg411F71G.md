# P146：L67.3- Cycle-Consistent GAN - ShowMeAI - BV1Dg411F71G

Okay in that case， let's move on for the previous paper you had pairs of labels you had input output and they were paired together What happens if they are not paired you don't know the corresponding label for a particular image but you have only two sets of datasets this is gonna to become clear as I go through the math but what are the applications you can change the style of some of your photos you can give a photo to your geneative aversar on run network and that's going to give you the monet style of that photo or you can take a monet image and then that's going to give you your photo the corresponding photo you can change you can turn summer to winter winter to summer these are useful for self-driving cars etc。

 but how are you doing it previously you had pairs of input output you had the sketch you had the corresponding label you had another sketch you had the corresponding label and these are pairs you know that this is this input。



![](img/5f96424698b3a3a9b83dd6de57caaf62_1.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_2.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_3.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_4.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_5.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_6.png)

Correpons to this output， this input does not correspond to this output So you have extra information Now all you know is that you have some images maybe taken during summer。

 you have some other images of some other places taken during winter and you don't know whether this image corresponds to that particular image or any other image at all。

 maybe there is actually no correspondence for this image in your output data So now you don't you have less information these are on per that is perd So whatever you're going to do you're going to have two generators One is going to generate images in the y set So it's going to generate images here。

 and x is going to be the input you're gonna have another generator that is generating image in the X domain given y as input and now you want to learn the distribution of y and X Why do you need two because you don't have curves of labels you can actually。



![](img/5f96424698b3a3a9b83dd6de57caaf62_8.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_9.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_10.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_11.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_12.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_13.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_14.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_15.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_16.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_17.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_18.png)

Say that F should be the inverse of G Basically you are going to say that I take this image。

 generate an image， you take a generated image I go back and that should give me the same image This is how you are pairing your dataset sets together Okay you take G you take X you push it through G that's going to give you an image in Y now you take G you go back to the x domain and you're gonna say that this should be identity the same thing should happen for the other part if you take an image from y you go to x and you come back it should give you the same image and this is the concept of cycle consistent and the reason you're doing it is because you don't have pair data if you have pair data your life would be much simpler Now you don't have it Okay and the question is how we're going to train G and F What is your loss function you're gonna to put two discriminators and those two discriminators are going to give you your loss functions The first discriminator is discriminating over。



![](img/5f96424698b3a3a9b83dd6de57caaf62_20.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_21.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_22.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_23.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_24.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_25.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_26.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_27.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_28.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_29.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_30.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_31.png)

The X domain and that's why you call it Dx， the other discriminator is going to work in the Y domain you call it dy Dx is trying to discriminate between real images these are the real images and the fake images coming from Y and the other one is trying to discriminate between images that are real in the Y domain that and the images that are being generated by G you write down your loss function as usual you have your discriminator discriminating between y and the generated images in the Y domain you are going have a similar loss for the X domain so I'm not going to write it down it's exactly the same formula but here is going to be the of X and this is going to be X and that's going to be f of Y and then that' your objective you maximize with respect to D minimize with respect to G similarly for the other loss function that I didn't write it's going to be F dx Y and X you maximize with respect to Dx and。



![](img/5f96424698b3a3a9b83dd6de57caaf62_33.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_34.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_35.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_36.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_37.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_38.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_39.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_40.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_41.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_42.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_43.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_44.png)

MI we respect to the generator and in addition you're gonna enforce this you're gonna say that if I take an image X go to the Y domain and come back to the X domain it should give me the same x so you want it to be cycle consistent so you want F and G to be the inverse of each other it's not going to be perfect but you're gonna to get closer to that through training so in the end hopefully F and G are the inverse of each other if you have labels you knew exactly what you need to match you knew exactly that you have the ground truth Y if you generate an image that should be equal to Yi now you don't have that information but you can mimic that information by cycle consistency and then you're gonna weight these losses properly not only you have your generator losses you're going to have your cycle consistency loss and then that's going to give you your best GS star and FS star and I went through lease squares loss。



![](img/5f96424698b3a3a9b83dd6de57caaf62_46.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_47.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_48.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_49.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_50.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_51.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_52.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_53.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_54.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_55.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_56.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_57.png)

This paper is using L squares loss and as you can see the task is different and maybe that's the reason you need a different loss。

 the original GN loss was not working so they needed to replace it so they generated images。



![](img/5f96424698b3a3a9b83dd6de57caaf62_59.png)

![](img/5f96424698b3a3a9b83dd6de57caaf62_60.png)

Should。The label for generated images coming out of the discriminator should be one and this is also one here and the other label for the real data is zero actually for the generated data you generate data you push it through your discriminator the corresponding label is zero so that's when you're going to lose your lease squares loss and youre going there is a trick in reinforcement learning that you keep a replay buffer。

 you keep a buffer you keep pushing examples into that buffer and then you use that example so you sample from that list of examples that you save and you sample from that you do the same thing here you're going keep a history of generated images and then you're going to update your discriminator based on that history so previously you would do a couple of rounds of the generator a couple round of the discriminator and things were dynamic at each。

Iteration you were generating images and the discriminator was discriminating between real and fake on the fly。

 Now you keep a history。 you keep a buffer and these buffer。

 the images that are generated could be generated by some generator that had some other parameters other than the ones that you're currently using in these iteration of your optimization So again that one is needed for the congence and then what can you do you can make your images photo realistic so this is an image you take with your cell phone and that's an image that is beautiful and it's taken by a professional focusing on the flower only and there are some bad cases that could happen you want to turn a horse to a zebra but then bad things could happen okay these are the failure cases So a good paper not only it needs to show impressive results and the same time it needs to show bad examples failure cases。

Okay， any questions I have a question So in this like I guess in particular for this Monet to photo case or photo to Monet case。

 I don't know how many paintings Monet has but I'm assuming it's a pretty small amount so like one of the data sets might be quite small compared to the other one but I guess my question is is that okay assuming that one of your data sets is large Yes it should be okay and then you can play around with the scales of your generator with the scales of your loss and these framework is going to work because you're going to treat X as a set y as another set and you don't care about the size。

There might be some bias that you can debias them using properly weighting your last functions Okay。

 any other questions？Again， there is nothing new here in terms of the generative process。

 the only new thing is the cycle consistency and there is a reason for that because you don't have pair data。

 you're going to generate pair data on the fly and we knew about these squares loss we didn't also know about this idea of keeping a buffer we are going to see that over and over again when we do reinforcement learning so if you don't get it here you're going to definitely get it there。

