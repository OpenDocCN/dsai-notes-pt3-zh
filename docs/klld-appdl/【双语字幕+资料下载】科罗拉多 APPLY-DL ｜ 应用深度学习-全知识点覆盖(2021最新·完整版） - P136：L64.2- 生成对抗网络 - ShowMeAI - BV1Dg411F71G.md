# P136：L64.2- 生成对抗网络 - ShowMeAI - BV1Dg411F71G

The next paper is gonna try to get rid of the likelihood The idea is still the same。

 you want to generate images but you want to get rid of the likelihood so it's a totally different way of thinking and it's going to make a lot of the statisticians uncomfortable this idea but it's a powerful idea and let's see and the idea is that you don't like that variational approximation because there is no guarantee that you're going to be able to approximate complicated distributions your posterior could be very complicated and there is no guarantee that you're going to be able to approximate it properly using a variational distribution so again we are going to generate samples from a noise variable and that's a prior assumption that you make let's say this is normal or uniform you're going generate samples from that these are simple distributions and if you remember the idea of reparametricization tree。



![](img/8187e74a2b0c723fc6391e02f6f165c4_1.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_2.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_3.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_4.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_5.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_6.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_7.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_8.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_9.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_10.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_11.png)

That we just used you can take that Z， you can push it through a complex neural network that has its own parameters let's say this is a deconvolution network and then it's going to generate images so it's going to learn complex distribution the distribution of P is simple the distribution that is going to come out of G could be complicated could be complex okay that's the generator part you generate a simple example from a simple distribution you push it through a neural network and it's going to give you a sample from that distribution from that P ofG distribution so these are generated samples okay the question is how we're going to train this or you're going to write down your likelihood and then you go back to the variational distributions again or you have a new idea we know that we are really good at or neural networks are really good at solving discriminative problems they're really good at classifying so let's call it classifier and say。



![](img/8187e74a2b0c723fc6391e02f6f165c4_13.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_14.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_15.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_16.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_17.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_18.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_19.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_20.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_21.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_22.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_23.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_24.png)

Okay this image that you just saw is this a real data or is this a data generated by the generator so that's the question that you're asking your classifier real or not real real or generated and we know that the classifier is going to output the probability of belonging to the first class for instance the first class here is being real data so D ofx is going to have its own parameters is the probability that x belongs to the real data okay perfect whatever we're going to do our discriminator should be good at discriminating between real and fake data so we are maximizing some objective function we respect that parameters of the discriminator and then we wanted to correctly classify real data as real data so you want to increase the probability of real data being classified as real data and you want to increase the probability。



![](img/8187e74a2b0c723fc6391e02f6f165c4_26.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_27.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_28.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_29.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_30.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_31.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_32.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_33.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_34.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_35.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_36.png)

Of the fake data being classified as fake so the real data should be classified as real the fake data should be classified as fake and the probability of being classified as fake is one minus the probability of being classified as real okay so you're maximizing this you're maximizing that that the job of the discriminator and the generator is gonna to try to fool the discriminator it's gonna say this sample that I generated is very good。

 it's very good and the discriminator cannot discriminated it cannot say whether it's real or fake so that's why this guy is maximizing the other guy is minimizing you are maximizing with respect to D and you're minimizing with respected G so basically whenever you're solving this objective function with respected G you don't need to worry about this there that's just a constant G is appearing here only but what is the problem。



![](img/8187e74a2b0c723fc6391e02f6f165c4_38.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_39.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_40.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_41.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_42.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_43.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_44.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_45.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_46.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_47.png)

This objective can somebody tell math mathematically it's fine。

 but what could go wrong when you're doing training Could it just be a lot better than the discriminator and not learn Sorry。

 I couldn't hear you Could you just get stuck So if either the next discriminator or the generator is really good and the other one just doesn't learn Yes。

 that's that's a very good point。

![](img/8187e74a2b0c723fc6391e02f6f165c4_49.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_50.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_51.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_52.png)

Initially during training your D the discriminator is going to be very good because these are easy samples。

 it's very easy because your generator initially is very bad。

 it's going to generate very bad images and your discriminator is going to be having a very easy time discriminating between fake and real so what's going to happen？



![](img/8187e74a2b0c723fc6391e02f6f165c4_54.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_55.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_56.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_57.png)

The problem is that this term initially during training is going to saturate and if it saturates。

 it's going to become zero or a constant and therefore it's derivative is going to be zero and there is no learning happening for G because if the gradients are zero there is no learning what is the problem D can reject generated samples with high confidence what does it mean it means that d of de generated samples are going to be very close to zero the probability that your generated samples are real is going to be very low so that's going to be zero now one minus that is going to be approximately1 and the log of one we know that it's zero so this term is going to saturate initially during your training so there is not much gradient for your G to learn so how can you mitigate that rather than minimizing these objective with respect to G you can equivalently max。



![](img/8187e74a2b0c723fc6391e02f6f165c4_59.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_60.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_61.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_62.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_63.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_64.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_65.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_66.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_67.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_68.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_69.png)

Smize the log of d of g of Z as if you are minimizing this guy， there is a minus sign here。

 it's as if you are maximizing that there okay so equivalently you can maximize with respect to G So now you have two objectives One is the objective function for D given G your discriminator is gonna to discriminate between real and fake and given D your generator is gonna to generate sample that are gonna make a discriminator。

 make mistake So if this is a generated sample the probability of that generated sample should be low but now you're making it go higher So you're faking it you're saying that the discriminator should discriminate this as real increase the probability of this guy being real So let's do some math let's fix G given G the optimal discriminator So given G you're solving this maximization problem the optimal discriminator is just P of data divided by P of data plus P of g Why is that。



![](img/8187e74a2b0c723fc6391e02f6f165c4_71.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_72.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_73.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_74.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_75.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_76.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_77.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_78.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_79.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_80.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_81.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_82.png)

Let's write down this objective function and turn these expectations into integrals so you are doing an integral over x of log of d times the probability of your data plus now z is being sampled from the noise distribution this is equivalent to sampling x from the generated distribution and because we want to work with x we don't want to work with z anymore our integral is over x this is going give you P of g of x that's that integral there and log of1 minus d of x okay now you want to maximize this let's try to do that let's call this a so I'm just renaming that that's a log of d just call it log of y for some y let's call this B and this is log of1 minus y now let's go ahead and take derivative of this objective function with respect to Y set it to zero and that's going to give you the best y star and that's a over。



![](img/8187e74a2b0c723fc6391e02f6f165c4_84.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_85.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_86.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_87.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_88.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_89.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_90.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_91.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_92.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_93.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_94.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_95.png)

A plus B so that's a simple algebra here Now it's going to give you P of data A was P of data and B was P of G so given G you know what is D now let's take this D and put it back in our objective function that's going to give you a function of G only so it's going to give you C of G basically you are taking this term and putting it inside this objective function there is expectation with respect your data log of the optimal。

 this is DSR expectation with respect to X coming out of your generated distribution and that's the optimal now let's go ahead and try to turn these basically rearrange your furniture a little bit so that you get KL divergence out so you use the definition of your kL divergence and then after rearranging your furniture a log of four is going come out and then two kL divergence one is P of data。



![](img/8187e74a2b0c723fc6391e02f6f165c4_97.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_98.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_99.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_100.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_101.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_102.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_103.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_104.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_105.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_106.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_107.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_108.png)

P of data plus P of g divided by two and there is going to be a P ofG this term is just a definition of genen Shan and divergence。



![](img/8187e74a2b0c723fc6391e02f6f165c4_110.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_111.png)

So Jansen channel divergence is going to besymmetricsymmetricsymmetric。

 so you can switch the role of PfG and P data and you're going to be fine going to give you the same definition。



![](img/8187e74a2b0c723fc6391e02f6f165c4_113.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_114.png)

But KL divergence is not so it matters what terms go first okay you get genen channel divergence we are minimizing this G this C of g we this to G and the minimum is going to be when this term is zero because this is always positive this is a divergence this is a distance so this term is always positive and the minimum is going to be negative of log of4 and when this is zero this is zero when those two distributions are the same so it means that once the training is done your generator is going to have the distribution of your data once the training is done and what you get out of it you can generate samples from distribution you can generate samples from MNist these are generated samples this is real sample you can generate phases and this is C4 10 any questions so I needed to go through the genen channel divergence because we are going to need that this is not the only divergence that you can。



![](img/8187e74a2b0c723fc6391e02f6f165c4_116.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_117.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_118.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_119.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_120.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_121.png)

![](img/8187e74a2b0c723fc6391e02f6f165c4_122.png)

Work with okay so this objective function is going to give you gensen channel divergence so is the genen chain divergence is just the sum of those two kl divergences is just a way of like  symsymmetricatizing we're making symmetric the kL exactly so this is just a definition Okay the definition of gensen channel divergence is2 kL so kL of your distribution and。

Half of the addition of those yeah distributions okay plus the other term divided by two that's why you have a two here yeah and you have to you have to divide the distributions like on the second part of the kl you have to divide them by two to make sure that they're proper PDFfs or I guess yes yes that's that's correct okay okay any other questions Okay in that case let's move on。

