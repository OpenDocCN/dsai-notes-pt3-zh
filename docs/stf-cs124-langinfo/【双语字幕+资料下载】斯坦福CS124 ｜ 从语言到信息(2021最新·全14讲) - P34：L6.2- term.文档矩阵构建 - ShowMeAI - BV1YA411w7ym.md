# P34ï¼šL6.2- term.æ–‡æ¡£çŸ©é˜µæž„å»º - ShowMeAI - BV1YA411w7ym

Hello in this section I'm going to introduce the important idea of a term document matrixã€‚

 but also I'm going to explain why it isn't actually a practical data structure for an information retrieval systemã€‚



![](img/dcfb56452f7f7b937a671cba2a48016d_1.png)

We take us our example doing information retrieval over the works of William Shakespeareã€‚

 So let's suppose we have this concrete questionï¼Œ which place of Shakespeare contain the words Brutus and Caesarã€‚

 but not Calperniaã€‚ðŸ˜Šï¼ŒWellï¼Œ if you're starting from a very basic level of text searching commandsã€‚

 the first way that you'd think about to solve this problem is by using searching through the text of the documents exhaustively what's known in the Unix world as Grpingã€‚

And so we could kind of first of allï¼Œ gr for plays that contain Brutus and Caesarã€‚

 and then if you know your Gr command wellï¼Œ you can give a flag for files that do not match and you could get out the ones that don't contain Calperniaã€‚

Nowï¼Œ these days for works of the size of William Shakespeare for this kind of queryã€‚

 Grapping as a perfectly satisfactory solutionã€‚ Our disk strives and computers are sufficiently fast that you could use this methodã€‚

 and it takes no time at all to find the answerã€‚ But neverthelessã€‚

 that isn't a good answer for the full information retrieval problemã€‚

 It falls flat in a number of waysã€‚ Once your corpus becomes largeã€‚

 And so that means something like everything on your hard disk or even more so the world wide webã€‚

 we can't afford to do a linear scan through all our documents every time we have a queryã€‚ðŸ˜Šã€‚

Then some parts of it like the not part become less trivial to implement than just finding thingsã€‚

 but even more so than the not part we'll have more complex queries like finding uses of the word Romans near countrymenã€‚

 and we can't do that with a Gr commandã€‚But even more than thatã€‚

 the big thing that's happened in information retrieval is the idea of rankingã€‚

 finding the best documents to return for a queryã€‚ And that's something that we just can't get out of the linear scan model of finding things that matchã€‚

 And we'll talk about all of these issues in the way they're handled in modern information retrieval systems in later lecturesã€‚

But let's first go to this idea of a term document matrixã€‚

 so what we do in a term document matrix is that we have the rows of the matrix are our words or often they're also called in information and retrievalã€‚

 the termsï¼Œ and then the columns of the matrix are our documentsã€‚ðŸ˜Šã€‚

And what we hit doing here is a very simple thing where're simply sayingã€‚

 let's fill in every cell in this Boolean matrix by whether the word appears in the play or doã€‚

 So An appears in Anton and Cleopatraï¼Œ but Calperia does not appear in Anton and Cleopatraã€‚ðŸ˜Šã€‚

So this matrix represents the appearance of words and documentsï¼Œ and if we have this matrixã€‚

 it's straightforward to then answer Boolean queries such as our example beforeã€‚

 queries for documents that contain Bruus and Caesar but not Calperniaã€‚ðŸ˜Šã€‚

Let's just go through concretely how we do thatã€‚ So what we're going to do is we're going to take the vectors for the terms in the queryã€‚

 and then we're going to put them together with Boolean operationsã€‚ So first of allã€‚

 we can take out the row that is referring to Brutusã€‚ðŸ˜Šï¼ŒWhich goes up hereã€‚Thenã€‚

We can take the row for Caesar and end it thereã€‚And then finallyï¼Œ we can take the row for Calperniaã€‚

 complement itï¼Œ and then stick it down hereã€‚ So Calpernia only appears in Julius Caesarã€‚

 And so we've complemented it to a vector where everything is one apart from Julius Caesarã€‚

 And at that pointï¼Œ we can just add those three vectors togetherã€‚ And our answer isã€‚ðŸ˜Šï¼ŒThis one of 1ã€‚

0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0ã€‚ And so we've been able to do information retrieval all successfully and can tell that theseã€‚

 this query is satisfied by the documentsã€‚ Anthony and Cleopatra and Hamletã€‚ And indeedã€‚

 we can then go off to the document collection and confirm that that is the caseã€‚ So here we areã€‚

 So in An and Cleopatraã€‚ when Anthony found Julius Caesar deadã€‚

 he cried almost a roaring and he wept when at Philippiï¼Œ he found Bru Slain and similarlyã€‚

 we find both words occurring in Hamletã€‚ðŸ˜Šï¼ŒOkayï¼Œ so that suggests that we could do information retrieval simply by working with this term document matrixã€‚

 So an important thing to realize is that that doesn't really work once we go to sensible size collections and so let's just go through that for a minute let's go through a sensible size but still small collection So suppose that we have1 million documents and we'll often use n to refer to the number of documentsã€‚

 each of which is on average 1 thousand words longã€‚ðŸ˜Šï¼ŒOkayã€‚

 so what does that mean in terms of the size of ourï¼ŸðŸ˜Šï¼ŒDocument collectionã€‚

 and in terms of the size of our matrixã€‚So if we have an average of six bytes per wordã€‚

 including spaces and punctuationï¼Œ the amount of data we're talking about here is 6 gigteã€‚

 so that's a teeny fraction of one modern hard disk in your laptopã€‚ðŸ˜Šã€‚

But let us then suppose we try and work out how many distinct terms there are in our document collectionã€‚

 and we need to know the number of distinct terms because that corresponds to the number of columns now matrixã€‚

 And let's suppose they're about half a millionã€‚ That'll be a typical number for a million documentsã€‚

 And soll often refer to this number of different terms as Mã€‚ Wellï¼Œ what does that meanï¼Œ Wellã€‚

 what it means is that even with that size document collectionã€‚

 we can't build this term document matrixã€‚ because it'll have 500000 rows and a million columnsã€‚

 And that's half a trillion zeros and onesï¼Œ it's already huge and probably bigger than we have space to storeã€‚

 And as the document collection gets bigger than a million documentsã€‚

 things are just going to get worseã€‚But there's this really important observationï¼Œ which isã€‚

 although the matrix here had half a trillion zeros and ones in it that actuallyã€‚

Almost all of the entries are 0 that the document has at most 1 billionã€‚

 and it'll be good for you guys to stop and think for a fraction of a secondã€‚

 Why is it that they're at most 1 billionsã€‚And the answer to that isï¼Œ wellã€‚

 if we have 1 million documents and the average document is 1000 words longï¼Œ as we said last timeã€‚

 then the actual number of word tokens is only 1 billionã€‚

 So even if we assume that every word and every document were differentã€‚

 we could only have at most 1 billion one entriesã€‚ And most likely we have far less than that because we'll have common words like that of and two occurring manyã€‚

 many times in each documentã€‚ And so thereforeï¼Œ the key observation is the matrix we're dealing with is veryã€‚

 veryï¼Œ very sparseã€‚ And so the central question in the design of information retrieval data structures is taking advantage of that sparsity and coming up with a better data representationã€‚

And the secret of doing that and having an efficient storage mechanism is we want to only record the positions that hold a one and not the positions that hold a0ã€‚



![](img/dcfb56452f7f7b937a671cba2a48016d_3.png)

Okay so I hope that's given you us an understanding of the term document matrixã€‚

 it's an important conceptual data structure that we keep on coming back to again and again when we talk about various kinds of algorithms we think about them in terms of that matrix as you'll seeã€‚

 but when we actually come to doing storage and computer systemsã€‚

 we can also see that we never actually want to store documents and their information retrieval representation in that formã€‚

ðŸ˜Šã€‚

![](img/dcfb56452f7f7b937a671cba2a48016d_5.png)