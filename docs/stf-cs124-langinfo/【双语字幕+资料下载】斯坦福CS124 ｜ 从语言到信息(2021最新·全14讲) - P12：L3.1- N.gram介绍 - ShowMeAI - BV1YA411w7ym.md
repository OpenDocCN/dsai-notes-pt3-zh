# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P12ÔºöL3.1- N.gram‰ªãÁªç - ShowMeAI - BV1YA411w7ym

Today we're going to introduce the topic of language modeling„ÄÇ

 one of the most important topics in natural language processing„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_1.png)

The goal of language modeling is to assign a probability to a sentence„ÄÇ

 Why would we want to assign a probability to a sentenceÔºüThis comes up in all sorts of applications„ÄÇ

 In machine translationÔºå for exampleÔºå we'd like to be able to distinguish between good and bad translations by their probabilities„ÄÇ

 So high winds to night might be a better translation than large wins to night because high end winds go together well in spelling correction„ÄÇ

 we see a phrase like 15 minues from my house„ÄÇ That's more likely to be a mistake from minutes„ÄÇ

 And one piece of information that lets us decide that is that 15 minutes from is a much more likely phrase than 15 minues from„ÄÇ

 And in speech recognitionÔºå a phrase like I saw a van is much more likely than a phrase that sounds phonetically similar eyes„ÄÇ

 awe of an„ÄÇ but is's much less likely to have that sequence of words„ÄÇ

 And it turns out language modelings play a role in summarization and question answering really everywhere„ÄÇ

üòä„ÄÇ

![](img/ffb302372336ae672a67cbb7919ca3ca_3.png)

So the goal of a language model is to compute the probability of a sentence or a sequence of words„ÄÇ

So given some sequence of words W1 through WNÔºå we're going to compute their probability P of W„ÄÇ

 and we'll use capital W to mean a sequence from W1 to WN„ÄÇ

Now this is related to the task of computing the probability of an upcoming word„ÄÇ

 so P of W5 given W1 through W4 is very related to the task of computing P of W1Ôºå W2W3Ôºå W4Ôºå W5„ÄÇ

A model that computes either of these thingsÔºå either P of wÔºå capital WÔºå meaning a string„ÄÇ

 the joint probability of the whole stringÔºå or the condition probability of the last word given the previous words„ÄÇ

 either of those we call that a language model„ÄÇNow it might have been better to call this the grammar„ÄÇ

 I mean technically what this is is telling us something about how good these words fit together and we normally use the word grammar for that„ÄÇ

 but it turns out that the word language model and often we'll see the acronym LM is standard„ÄÇ

 so we're going to go with that„ÄÇ

![](img/ffb302372336ae672a67cbb7919ca3ca_5.png)

So how are we going to compute this joint probabilityÔºå we want to compute„ÄÇ

 let's say the probability of the phrase itss water is so transparent that this little part of a sentence„ÄÇ

 and the intuition for how language modeling works is that we were going to rely on the chain rule of probability and just to remind you about the chain rule of probability„ÄÇ

 Let's think about the definition of conditional probability„ÄÇ So P of a given B„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_7.png)

Equals P of a comma B„ÄÇOver P of B„ÄÇAnd we can rewrite that„ÄÇ So P of a given B„ÄÇTimes P of B„ÄÇ

Equals P of a comma B„ÄÇOr„ÄÇTing it around P of A comma B„ÄÇEquals P of A given B„ÄÇ

Make sure it's a given times P of B„ÄÇAnd then we could generalize this to more variables„ÄÇ

 so the joint probability of a whole sequenceÔºå ABCD„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_9.png)

Is the probability of a times B given A times C condition on A and B times D condition of E BC„ÄÇ

 so this is the chain rule in a more general form of the chain rule we have here„ÄÇ

 the probability of any joint probability of any sequence variables is the first times the conditional of the second and the first times the third condition of the first two up until the last condition on the first and minus1„ÄÇ

AlrightÔºå the chain rule„ÄÇ So now the chain rule can be applied to compute the joint probability of words in a sentence„ÄÇ

 So let's suppose we have our phraseÔºå its water is so transparent„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_11.png)

By the chain ruleÔºå the probability of that sequence is the probability of it„ÄÇ

 times the probability of water given itÔºå times the probability of is given its water„ÄÇ

 times the probability of so given its water isÔºå and finally times the probability of transparent given its water is so„ÄÇ

 or more formally the probability of a joint probability of a sequence of words is the product overall I of the probability of each word times the prefix up until that word„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_13.png)

How are we going to estimate these probabilitiesÔºå Could we just count and divide„ÄÇ

 We often compute probabilities by counting and dividingÔºå So the probability of the„ÄÇ

 given its water is so transparent that we could just count how many times its water is so transparent that the occurs and divide by the number of times its water is so transparent occurs so we could divide this by this„ÄÇ

And and get a probability„ÄÇWe can't do thatÔºå and the reason we can't do that is there's just far too many possible sentences for us to ever estimate these„ÄÇ

 There's no way we could get enough data to see the counts of all possible sentences of English„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_15.png)

So what we do instead is we apply a simplifying assumption called the Markov assumption named for Andre Markov„ÄÇ

And the Markov assumption suggests that we estimate the probability of the given its water is so transparent that just by computing instead„ÄÇ

 the probability of the given the word that„ÄÇOr the very last that meaning the last word in the sequence„ÄÇ

 or maybe we compute the probability of the given its water is so transparent that given just the last two words„ÄÇ

 so the given transparent that so that's the markov assumption„ÄÇ

 let's just look at the previous or maybe the couple previous words rather than the entire context„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_17.png)

More formallyÔºå the mark of assumption says the probability of a sequence of words is the product for each word of the conditional probability of that word„ÄÇ

 given some prefix of the last few words„ÄÇSo in other words„ÄÇ

 in the chain rule product of all the probabilities we're multiplying together„ÄÇ

 we estimate the probability of WI given the entire prefix from 1 to I minus1 by a simpler to compute probability„ÄÇ

 WIÔºå given just the last few words„ÄÇ

![](img/ffb302372336ae672a67cbb7919ca3ca_19.png)

The simplest case of a Markov model is called the unigram model In the unigram model„ÄÇ

 we simply estimate the probability of a whole sequence of words by the product probabilities of individual words„ÄÇ

 unigrams„ÄÇ and if we generated sentences by randomly picking words„ÄÇ

 you can see that it would look like word salad so heres some automatically generated sentences generated by Dan Klein and you can see that with the word fifth„ÄÇ

 the word an the word of this doesn't look a sentence at allÔºå it's just a random sequence of words„ÄÇ

 thrift did 80 said„ÄÇU to the properties of a unigram modelÔºå words are independent in this model„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_21.png)

Slightly more intelligent is a bygram model where we condition on the single previous word„ÄÇ So again„ÄÇ

 we estimate the probability of a word given the entire prefix from the beginning to the previous word just by the previous word„ÄÇ

 So now if we use that and generate random sentences from a bygram model„ÄÇ

 the sentences look a little bit more like English still something's wrong with them clearly outside new car„ÄÇ

 well new car looks pretty good„ÄÇ car parking is pretty good parking lot„ÄÇ

 but together outside new car parking lot of the agreement reached that's not English„ÄÇ

 So even the bygram model by giving up this conditioning that English has„ÄÇ

 we're simplifying the ability of the model to model what's going on in a language„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_23.png)

Now we can extend the engram model further to tri grams that's  three grams or four grams or five grams„ÄÇ

But in generalÔºå it's clear that Ngram modeling is an insufficient model of language„ÄÇ

 and the reason is that language has long distance dependencies„ÄÇ So if I want to say predict„ÄÇ

The computer which I had just put into the machine room on the fifth floor„ÄÇ

 and I hadn't seen this next wordÔºå and I want to say what's my likelihood of the next word„ÄÇ

 and I conditioned it just in the previous wordfÔºå I'd be very unlucky to guess crashed„ÄÇButÔºå really„ÄÇ

The crashash is the main verb of a sentenceÔºå and computer is the subject„ÄÇ

 the head of the subject noun phrase„ÄÇ So if if we knew computer was the subject„ÄÇ

 we're much more likely to guess crashed„ÄÇ So these kind of long distance dependencies mean that in the limit„ÄÇ

 a really good model of predicting English words will have to take into account lots of long distance information„ÄÇ

 But it turns out that in practiceÔºå we can often get away with these ngram models„ÄÇ

 because the local informationÔºå especially as we get up to trigrams and four gras„ÄÇ

 will turn out to be just constraining enough that in most casesÔºå it'll solve our problems for us„ÄÇ



![](img/ffb302372336ae672a67cbb7919ca3ca_25.png)

![](img/ffb302372336ae672a67cbb7919ca3ca_26.png)