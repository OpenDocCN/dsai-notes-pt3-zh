# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P22ÔºöL4.4- ÊÉÖÊÑüÂàÜÊûê‰∏éÊú¥Á¥†Ë¥ùÂè∂ÊñØ - ShowMeAI - BV1YA411w7ym

In this lectureÔºå we'll do a worked example of the naive Bay approach to sentiment analysis„ÄÇ

 and we'll also introduce the binary naive Bays algorithm„ÄÇ



![](img/3a74d194d8a66c8d939329d669c37658_1.png)

Let's look at this simple pedagogical example„ÄÇHere we've got three negative„ÄÇ

Documents and two positive documents„ÄÇ And we're going to see how to compute the probabilities from our training set„ÄÇ

 And then we'll see how do we assign a sentiment value to our test sentence„ÄÇ

 So five training sentencesÔºå one test sentence„ÄÇ So the first thing we're going to do is compute the prior probability of the two classes„ÄÇ

 negative and positive„ÄÇ and recall from the previous lecture that we compute the probability of a class by taking the number of documents in that class over the total number of documents„ÄÇ

 So there are three negative documents and two positive documents„ÄÇ



![](img/3a74d194d8a66c8d939329d669c37658_3.png)

So our probability of negative is  three out of the total  five documents„ÄÇ

 and our prior probability of the positive class is the two out of 5„ÄÇ Next„ÄÇ

 we drop any word in the test set that's not in our vocabulary„ÄÇ

 The word with never occurred in our training set„ÄÇ So we're going to drop it here from the test set as if it never occurred„ÄÇ

 NowÔºå let's look at computing the likelihoods„ÄÇ And we're going to use the add one smoothing equation that we saw in the prior lecture„ÄÇ

So we're going to take the count of for any word in the in the class how often the word occurs in documents in that class„ÄÇ

 we're going to add one to it and for the denominator we're going to sum for all words in the vocabulary their count in this class and we're going to add the size of the vocabulary thats that's the sum of the add one smoothing that we saw in the previous lecture so we need to know for a given word what its count is in the class and we're going to need to know the sum of all the words in that class or we're going to know the vocabulary size„ÄÇ

So how many words are in the negative class 1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6Ôºå7Ôºå8Ôºå9Ôºå10Ôºå11Ôºå12Ôºå1314„ÄÇ So there are 14 words„ÄÇ

 That's word tokens in the negative class„ÄÇ So 14 tokens„ÄÇAnd there are 9 tokens in the positive class„ÄÇ

 And how about the vocabulary size„ÄÇ What turns out the vocabulary size is 20„ÄÇ

 If we sum up all of these tokensÔºå there's actually two copies of the and two of vary and two of and„ÄÇ

 And we're left with V equals 20„ÄÇ AgainÔºå That's just from the training set„ÄÇ

 We're in the training phase right now„ÄÇ We don't look at the test set yet„ÄÇüòäÔºåAl right„ÄÇ

 let's see how to compute the likelihood of three wordsÔºå the words predictableÔºå no and fun„ÄÇ

 I'm picking those three because those are the three that are going to occur in our test set so we can see very clearly how the math works„ÄÇ

All rightÔºå so how about the word predictableÔºå How many times does predictable occur„ÄÇ

 let's say in the negative class WellÔºå it occurs once„ÄÇ So count of W comeacy is going to be one„ÄÇ

 So the numerator is going to be one plus1„ÄÇ and how about the denominator denominator is going to be 14 plus 20„ÄÇ

 So sure enough here we have it printed out The probability of predictable given the negative class the likelihood here is one plus1 over 14 plus 20„ÄÇ

 and similarÔºå the probability of no the likelihood of no is one plus one over 14 plus 20 and the probability of fun„ÄÇ

 fun never occurs here in the training set in the negative class So it's count in the negative class is 0„ÄÇ

 And againÔºå we divide it by 14 plus 20„ÄÇ And I've done the similar computation here for the same three words in the positive class Pre doesn't occur in the positive class in training no doesn't occur in the positive class in training„ÄÇ

But fun doesÔºå and now we're dividing by a different denominator„ÄÇSo now we're ready„ÄÇ

 we have we've computed our six relevant probabilitiesÔºå of courseÔºå in real My Bay„ÄÇ

 you would have computed them for all the words because you don't know what's going to be in the test set we're just cheating here a little„ÄÇ

 So let's look at the test set„ÄÇWe're going to compute the probability of the negative class and the probability of the positive class and take whichever is higher„ÄÇ

 and to compute each probability we'll multiply the prior by the likelihood„ÄÇ

 So the prior for the negative class is 3 over 5„ÄÇ We got that from up here„ÄÇ

 The prior for the negative class is 2 out of 5„ÄÇ We got that from up here„ÄÇ And then we multiply by„ÄÇ

 in this caseÔºå the three likelihoods multiply together„ÄÇ

2 out of 34 times 2 out of 34 times 1 out of 34„ÄÇAnd for the negative class„ÄÇ

 we get a total estimate of 6„ÄÇ1 times 10 to the minus5„ÄÇAnd our score„ÄÇ

 we compute the score for the positive class in a similar wayÔºå But we get a lower probability„ÄÇ

 meaning that we choose the negative class as our prediction for this particular sentence„ÄÇ

 predictable with no fun„ÄÇ NowÔºå it turns out that for tasks like sentiment„ÄÇ

 the occurrence of a word seems to be more important than its frequency„ÄÇ In other words„ÄÇ

 if we see the word fantasticÔºå that tells us a lot about how the user feels about the movie„ÄÇ

 But the fact that it occurs five times may not tell us that much more than its original occurrence„ÄÇ

üòä„ÄÇ

![](img/3a74d194d8a66c8d939329d669c37658_5.png)

And we take advantage of this intuition with an algorithm called binary multinomial naive bays„ÄÇ

 and we'll just call it binary naive bays for shortÔºå and the algorithm is very simple„ÄÇ

Inside each documentÔºå we clip our word counts at one„ÄÇ So if a word occurs twice in the document„ÄÇ

 we just pretend it occurred once„ÄÇ and I want to point out this is different than a similar algorithm called Bnoulli naive Bayes„ÄÇ

 which is not generally used for text classification„ÄÇ

 It's used when naive bays is applied to other other situations„ÄÇ

 and you can see more about that in the textbook at the end of the chapter„ÄÇ

The algorithm for binary multinial naive bays is very similar to our non binary naive bay algorithm„ÄÇ

 So in trainingÔºå we compute the priors in the exact same way„ÄÇ

 And there's just a slight difference in how we compute the likelihoods before we do our likelihood computation„ÄÇ

We just remove all the duplicates in each documentÔºå in other words„ÄÇ

 for each word type W in a particular document JÔºå if there's more than one copy of W„ÄÇ

 we just keep one of them„ÄÇAnd then we concatenate the text together and compute our likelihoods just as we did before„ÄÇ

On a test document D„ÄÇWe similarly remove all the duplicate words from D„ÄÇ

 and then we compute N Bays using our standard equations before„ÄÇLet's walk through another example„ÄÇ

With binary naive bays„ÄÇ So imagine we have these four documentsÔºå two positive and too negative„ÄÇ

So if we look at our countsÔºå the word and occurs twice„ÄÇIn positive documents„ÄÇ

 zero times in the negative documents„ÄÇ And so we'll have a positive count up for and of2 in a negative count of 0„ÄÇ

 The word great appears three times„ÄÇOnly in positive documents it neverÔºå Oh noÔºå there it is„ÄÇ

 It appears once in the negative documentÔºå so we'll have a count of three„ÄÇ

And then in the positive documents and account of one in the negative documents and so on„ÄÇ

 we do this for all the words„ÄÇ And now let's look at what the documents look like after we've done our per document binization„ÄÇ

 there used to be two ands here and now we're down to one and and there were two greats here and we're down to one great and some other words have changed as well„ÄÇ

 So if we look at the countsÔºå we notice that it used to be that the word and occurred twice in positive documents Now it only occurs once because that document has button binized„ÄÇ

AndÔºå for exampleÔºå the word great used to have a count of three in positive documents„ÄÇ

 And now it has a count of two„ÄÇ Notice that the counts can still be greater than one„ÄÇ The word great„ÄÇ

 although we binized it in this document„ÄÇüòäÔºåIt still occurs in two documents in the positive class„ÄÇ

 so the binary Niveasease can still have counts greater than one by summing binaryized documents„ÄÇ



![](img/3a74d194d8a66c8d939329d669c37658_7.png)

We've now seen in detail the naive Bay's approach to sentiment analysis and the binary version of Naive Bays that works well with sentiment tasks„ÄÇ



![](img/3a74d194d8a66c8d939329d669c37658_9.png)