# P46ï¼šL7.8- æœç´¢å¼•æ“è¯„ä¼° - ShowMeAI - BV1YA411w7ym

In this sectionï¼Œ I'll tell you a little bit more about evaluating the quality of a search engineã€‚ğŸ˜Šã€‚



![](img/10a79c75fbe0d484c6ec607785acfcb0_1.png)

There are many measures for the quality of a search engineã€‚

 there are technical ones such as how fast does it index and how fast does it searchã€‚

 we can look at things like the expressiveness of the query language whether they're able to express complex informational needs with things like phrase queries nesã€‚

 disjunctionsã€‚ğŸ˜Šï¼ŒPeople have other desires like having an uncluttered UI and a system that doesn't cost a lot to useã€‚

 All of these are measures that are measurableï¼Œ that we can quantify them and we can get some kind of score of what is their goodnessã€‚

 but in practiceã€‚All of those measures while importantã€‚

 tend to be dominated by another measure of user happiness that is the user happy in using this search engine and speed of response and size of the index is certainly factors but by themselves blindingly fast useless answers won't make a user happyã€‚

 so a huge part of user happiness is are the results return results that they want and so that's the metric of relevance of results to a user's information needã€‚

ğŸ˜Šï¼ŒI mentioned this right at the beginningï¼Œ but just to reiterate once more when evaluating the IR system that we evaluate with respect to an information needã€‚

 so an information need is translated into a query and that's what the IR system actually runsã€‚

 but relevance is assessed relative to the information need not the query so for example if the information need is the person is looking for information on whether drinking red wine is more effective than white wineã€‚

ğŸ˜Šï¼ŒFor reducing your risk of heart attacksï¼Œ they'll come up with some queryï¼Œ for exampleã€‚

 it might be wide redï¼Œ white heart attack effective and that'll be submitted to a search engine and in evaluating the effectiveness of the search engine and returning relevant resultsã€‚

 we're not asking are the results that the search engine returnsã€‚

 documents that simply have those words on the pageã€‚

 rather we're saying do these documents address the user' information needã€‚ğŸ˜Šï¼ŒOkayï¼Œ wellã€‚

 how can we go about doing thatï¼Ÿ Wellï¼Œ if the search engine returns a set of resultsï¼Œ Wellã€‚

 then what we can do for evaluation is if we start off with three thingsï¼Œ if we have someã€‚

Benchmark collection of documents that we can use for evaluation and we have some benchmark set of queries which are in some sense representative of information needs of interest and then we've gathered this third thing which is assessor judgments of whether particular documents are relevant to particular queries commonly in practice this can't be assembled exhaustivelyã€‚

 certainly not if the document collection is largeã€‚

 but at least for a particular set of documents that are returned by particular search engines we can get the assessor to judge whether those documents are relevant to the queries Wellã€‚

 if we have a results set with just these three things were're in business because we can use exactly the sameã€‚

ğŸ˜Šï¼ŒMeasures that we looked at previously precision recall and the F measure that combines themã€‚

 and these are suitable good measures for exactly the same reason that there are good measures for when we're talking about things like named entity recognition that normally only a few documents will be relevant to a particular query and so we can measure that much better by looking at these measures of precision and recallã€‚

ğŸ˜Šï¼ŒBut what if we've now moved on to a search engine that returns ranked resultsã€‚

 we can't just totally straightforwardly use these measures of precision recall and F measure because the system can return any number of resultsã€‚

 in fact the number it returnsï¼Œ normally depends on how often we keep on clicking asking for moreã€‚

 but if we sort of look at any initial subset of the resultsã€‚

 we can then work out the precision and recall for that subset and then by putting them together we can come up with a precision recall curveã€‚

 let's look at how that worksã€‚ğŸ˜Šï¼ŒSo here are the first 10 results for a search engine where we've labeled each result is either relevant or not relevant according to an assessor's judgment and so then we can take any initial sub sequenceequence of these documents and work out a recall and precision so for the first document the system got it right it's a relevant document and let's assume that overall there are 10 relevant documents in the collection so it's gotten one out of the 10 relevant documents and so it' recall is 0ã€‚

1ã€‚And wellï¼Œ since that document was relevantï¼Œ the system was right on the first answerã€‚

 its precision is one at this pointã€‚ Wellï¼Œ the next document was not relevantã€‚

 so the recall of the first two documents we're down to here now is 0ã€‚1 and the precision is now 0ã€‚5ã€‚

Another not relevant documentï¼Œ so the precision is sorry the recall is still 0ã€‚

1 and the precision is now dropped to 0ã€‚3ï¼Œ3ã€‚And if we look at the set of the top four documentsã€‚

 we've now found two of the 10 relevant onesï¼Œ so recall is 0ã€‚2ã€‚

 and our precision has gone back up again to 0ã€‚5ã€‚The fifth one is also relevantã€‚

 So now our recall is up to 0ã€‚3 and our recall is up toã€‚3 out of 50ã€‚6ã€‚ and we can keep on going downã€‚

 Maybe you guys could work out what some of these entries are down hereã€‚

The other measure I want to mention one of the most most used recent measures is mean average precision If we look at the ranked retrieval results ordered this way to give me a bit more room and so the first document return is relevant the second one is not relevantã€‚

 say the third one is not relevant then a relevant one and another relevant oneã€‚

 not relevant relevant relevant let's say those are our top8 results what you're doing for mean average precision for first of all you're working on average precision for one query and so the way you do thatã€‚

ğŸ˜Šï¼ŒIs by saying let's work out the precision at each point that a relevant document is returned because that'sã€‚



![](img/10a79c75fbe0d484c6ec607785acfcb0_3.png)

Umï¼Œ when you're increasing recallï¼Œ so here the precision is oneã€‚

Here there are now four documents so the precision is a halfã€‚ğŸ¤¢ã€‚

Here there are five documents so the precision is 0ã€‚6ã€‚ hereã€‚

 there are 7 documents of which four of them are relevantï¼Œ so that'sã€‚This is around 0ã€‚

58 you guys can correct my arithmetic here we now have eight documents of which five are relevant and that's 0ã€‚

625 and then what we're doing to work out the mean average precision is we're kind of keeping on calculating those numbers in practice normally they aren't calculated exhaustively but they're calculated up to some point let's say100 and then you're calculating an average function of all those numbers and that's then the average precision for one query you then calculate the same average precision for all the other queries in your benchmark query collection and you again take the average of that and that then gives you mean average precision so in particular this is what's referred to as macroavveraging it's each query that counts equally in the calculation of mean average precisionã€‚

ğŸ˜Šï¼ŒSo this is a good measure thatã€‚ğŸ˜Šï¼ŒEvaluates to some extent precision at different recall levels while still being weighted most to what the precision is like for the top few return documentsã€‚

 and then at the level of across different queries it's giving equal weight to different queries which tends to be a useful thing to do because you're always interested in how systems return work on queries of rare terms as well as queries of common terms so this is a pretty good measure to think about using for evaluating information retrieval systemsã€‚

ğŸ˜Šï¼ŒOkayï¼Œ there's even more methods that I could talk aboutã€‚

 but that's probably a good sense of how about how to go about evaluating the performance of a ranked retrieval systemã€‚

ğŸ˜Šã€‚

![](img/10a79c75fbe0d484c6ec607785acfcb0_5.png)