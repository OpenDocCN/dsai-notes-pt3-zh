# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P41ï¼šL7.3- è¯é¢‘æƒé‡ - ShowMeAI - BV1YA411w7ym

The next thing I'd like to introduce is term frequency weightingã€‚

 which is one of the components of the kind of document scores that are regularly used in information retrieval systemsã€‚



![](img/f8bbd8aa807628cde892d224fc012499_1.png)

Let's go back to where we began with the term document incidence matrix So with this matrix we recorded a number which was either one or0 in each cell of the matrixã€‚

 depending on whether the word occurred in the document if we then think about what the representation of each document is Well what we have is a vectorã€‚

 it's a binary vectorï¼Œ which the dimensionality of the vector is the size of the vocabularyã€‚

And it recording these ones or zeroesã€‚ But we don't have to limit ourselves to a binary vector like thisã€‚

 An obvious alternative is instead to move to account vectorã€‚ So now we still have a vectorã€‚

For each documentï¼Œ but rather than simply putting ones and zeros in itã€‚

 we're putting in the number of times the word occurs in the documentã€‚

So he's still got a vector of sizeï¼Œ the vocabularyï¼Œ where is now a vector inã€‚

The natural number of vector spaceã€‚Previously in the Boolean retrieval modelã€‚

 we were just looking at a set of words that occurred in the document and doing set operations like and or orã€‚

 Nowï¼Œ with this count modelï¼Œ we've moved to the commonly used bag of words modelã€‚

 So in the bag of words model we're not considering the ordering of the words in the documentã€‚

 but we are considering how many times a word occurs in the document and this word bag is commonly used for an extension to sets which else record how often a word is usedã€‚

ğŸ˜Šï¼ŒSo the BWs model has some huge limitationsï¼Œ so John is quicker than Mary and Mary is quicker than John have exactly the same vectorsã€‚

 there's no differentiation between themã€‚ğŸ˜Šï¼ŒAnd that obviously has its limitationsã€‚ So in a senseã€‚

 this is a step back Earl onï¼Œ when we introduced positional indicesã€‚

 they were able to distinguish these two documents by either proximity or freeze queriesã€‚ğŸ˜Šã€‚

And we'll want to get back to thatï¼Œ we'll look later at recovering positional informationã€‚

 but for now we're going to develop the bag of words model and how it's used in vector space retrieval modelsã€‚

ğŸ˜Šï¼ŒSo we have this quantity of the term frequency of a term in a documentã€‚

 which is just the number of times that it occurs and so the question then is how can we use that in a retrieval scoreï¼Ÿ

Thinking about it a littleï¼Œ I hope you can be convinced that raw term frequency is perhaps not what we really wantã€‚

 so the idea underlying making use of term frequency is if I'm searching for something like squirrelsã€‚

 then I should prefer a document that has the word squirrel in it three times over one that just has the wordirrel in at onceã€‚

ğŸ˜Šï¼ŒBut on the other handï¼Œ if I find a document that has the words squirrel in it 30 timesã€‚

 it's not clear that I should prefer it 30 times as much as the document that only mentions squirrel onceã€‚

 And so the suggestion is that relevance goes up with number of mentionsï¼Œ but not linearlyã€‚

 And so we want to come up with some way of scaling term frequencies that isã€‚ğŸ˜Šã€‚

Relative to its frequencyï¼Œ but less than linearã€‚Before I go on to outline such a measure let me just highlight one last point we talk here about term frequency now the word frequency actually has two usagesã€‚

 one is the rate at which something occursï¼Œ the frequency of burglaries and the other sense of it is the one that's always used in information retrieval so when we talk about frequency in information retrieval frequency just means the countã€‚

 so the count of a word in a documentã€‚Okayï¼Œ so this now is what is standardly done with the term frequencyã€‚

 What we do is we take the log of the term frequencyã€‚ Nowï¼Œ if the term frequency is 0ã€‚

The word doesn't occur in the documentã€‚ Well the log of0 is negative infinityã€‚

 So that's slightly problematicã€‚ So the standard fix for that is we have this two case construction where we add one to the term frequency if the term does occur in the documentã€‚

 So if it occurs onceï¼Œ then this value will become one because the log will be 0 and then we'll add one to itã€‚

 and we return an answer of0 of the word doesn't occurã€‚ So that means thatã€‚Ifã€‚

Going on a little and if we use base 10 logarithms as here you can see how we're getting this less than linear growth so if a word occurs twice in a document it gets a weight of 1ã€‚

3 a little moreï¼Œ if it occurs 10 times it gets a weight of twoã€‚

 a thousand times a weight of four and so onã€‚ğŸ˜Šï¼ŒSo in order to score a document query pairã€‚

 we're just going to sum over these terms for each word in the query and the documentã€‚

 so it's sufficient to take the intersection of words that are in both the query in the document because everything else will contribute nothing to the score and then for each of those terms we're going to calculate this quantity and sum them upã€‚

ğŸ˜Šï¼ŒAnd so note in particular that the score is indeed still zero of none of the query terms as present in the documentã€‚

ğŸ˜Šã€‚

![](img/f8bbd8aa807628cde892d224fc012499_3.png)

Okayï¼Œ so that's the idea of term frequency weighting and how it can be used to give a score for documents for a particular query which can be used to rank the documents returnedã€‚

ğŸ˜Šã€‚

![](img/f8bbd8aa807628cde892d224fc012499_5.png)