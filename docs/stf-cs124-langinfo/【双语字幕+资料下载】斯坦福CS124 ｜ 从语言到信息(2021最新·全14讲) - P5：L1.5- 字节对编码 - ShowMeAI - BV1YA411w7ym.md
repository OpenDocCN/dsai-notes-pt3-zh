# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P5ÔºöL1.5- Â≠óËäÇÂØπÁºñÁ†Å - ShowMeAI - BV1YA411w7ym

In this lectureÔºå we introduce the Bte pair encoding or BPE algorithm„ÄÇ

 which uses corpus statistics to decide how to segment a text into tokens„ÄÇ



![](img/7d563e2e96bf172f1c8b32a4bab82024_1.png)

Instead of just breaking up wordss at every white space„ÄÇ

Or as we saw for Chinese breaking up words at every character„ÄÇ

 the algorithms we're going to introduce now use the data to tell us how to tokenize„ÄÇ

 And this family of algorithms is often called subward tokenizations„ÄÇ

 because the tokens can be parts of wordsÔºå as well as whole words„ÄÇ

There are three common subward tokenization algorithmsÔºå Bte pair encoding or B P E„ÄÇ

 which we'll talk about now„ÄÇ an algorithm often called unigram for unigram language model„ÄÇ

 tokenization„ÄÇ and the word piece algorithm„ÄÇ These algorithms all have two parts„ÄÇ First„ÄÇ

 a token learner that takes a training corpus and induces a vocabularyÔºå set of tokens„ÄÇ

 That is the vocabulary that the tokenizer will try to map things into„ÄÇ

 And then the second part is a token segmenter that takes a test sentence and tokenizes it„ÄÇ

 according to the vocabulary it learned from the training corpus„ÄÇ

 So let's start by talking about the first part„ÄÇ the token learner„ÄÇ

Let's imagine that we start with a vocabulary that is the set of all individual characters„ÄÇ

 So maybe we have all the capital letters and all the lowercase letters„ÄÇ That's our vocabulary„ÄÇ

 And we're going to repeat the following K times„ÄÇ We first choose the two symbols that are most frequently adjacent in the training corpus„ÄÇ

 Two letters that occur most often next to each other„ÄÇ Let's just say that these are A and B„ÄÇ

 And now we're going to do is we're going to add a new symbol„ÄÇ

 The merge symbol A B to the vocabulary„ÄÇ And we're going to replace in the entire training corpus„ÄÇ

 every adjacent A And B with this new symbol A B„ÄÇ And we're going to do this until K merges have been done And K is a parameter of the algorithm„ÄÇ

üòäÔºåWe can see this more formally hereÔºå so we begin with a vocabulary V„ÄÇ

 all the unique characters in the corpusÔºå and then K times and the string corpus C and the number of merges K are parameters to the algorithm„ÄÇ

 K times we're going to choose the two most frequent adjacent tokens we'll make a new token with their concatenation„ÄÇ

 we'll add that to the vocabulary„ÄÇ and now we just replace all the individual pairs of both two tokens in the corpus with this new token and then when we're done this K times„ÄÇ

 we return the new learned vocabulary„ÄÇNow there is an addendum to the BPE algorithm„ÄÇ

 which is that in practice most subward algorithms are run inside space separated tokens„ÄÇ

 so we commonly first add a special end of token symbol and I'll use underbar to represent that before each space in the training corpus before we separate into letters„ÄÇ

 so let's walk through an example„ÄÇImagine the following corpusÔºå loÔºå lowÔºå lowÔºå lowÔºå lowÔºå lowestest„ÄÇ

 lowestÔºå newerÔºå newerÔºå newerÔºå newerÔºå newerÔºå newerÔºå widerÔºå widerÔºå widerÔºå newÔºå new„ÄÇ

And let's take that litarily fascinating corpus and add a little underbar for an end of word token before every space„ÄÇ

 And the result's going to be this vocabulary„ÄÇ We have each of the letters in the original corpus D„ÄÇ

 EÔºå I LÔºå N O R S T W and the new token the underbar„ÄÇ And for convenience„ÄÇ

 I'm going to represent the corpus in the following way„ÄÇ

 which is I'm going to use counts of different strings of letters followed by underbars„ÄÇ

 rather than copying this entire long corpus representation every time„ÄÇ I'm just going to use this„ÄÇ

 So the word the sequenceÔºå L O W underbar occurs  five times There it is 1Ôºå2Ôºå3Ôºå4Ôºå5 and so on„ÄÇ

 I haven't written the underbars up here„ÄÇ So we're going to refer to the corpus this way„ÄÇ

 And the vocabulary this way„ÄÇ And let's walk through the algorithm seeing how we change the corpus and augment the vocabulary„ÄÇ

So we're going to begin with the corpus as we started with it and our original vocabulary and we're going to first say which two letters are next to each other most often and that is going to be E and R are next to each other six times in the context of NW and three times in the context of WID so that's nine times and we're going to merge the E next to R into a new symbol ER„ÄÇ

We're going to add ER to our vocabularyÔºå we're going to merge all the cases of ER in the corpus„ÄÇ

And now we're going to move on„ÄÇWhat's the next most frequent pair„ÄÇ

 It's another thing that occurs 9 times„ÄÇ The new symbol„ÄÇ

 E R is now next to the underbar six times here and three times there„ÄÇ

 So it's going to appear in the corpus a total of9 times„ÄÇ

 So we're going to merge E R next to underbar to a new symbol E R underbar„ÄÇ

 We're going to add that symbol to our vocabulary„ÄÇ And we're going to merge them in our corpus„ÄÇ

 The most frequent next symbol is N„ÄÇNext to E occurs six times in this context„ÄÇ

 two times in that context for a total of eight times„ÄÇ

 So we're going to merge N next to E to a new symbol N„ÄÇ So we add the new symbol N„ÄÇ

And now we merge them in our corpus„ÄÇAnd we keep moving on in that way„ÄÇ

 We're next going to merge any with W producing a new token N WÔºå L with O L OW„ÄÇ

 So we have a token L OWÔºå and then NW with ER underbar„ÄÇ

 So we have a new token N underbar and low with an underbar„ÄÇ So we have a token low underbar„ÄÇ

 So now we have the falling vocabulary that we've added a whole bunch of new words to„ÄÇ

So that's our training set„ÄÇNextÔºå we're going to take that set of merges we've learned in training and apply it to the segmentary algorithm running on our test set„ÄÇ

 and we're going to run those merges greedily in the order„ÄÇ we learned themÔºå not based on frequency„ÄÇ

 So remember in our training setÔºå we picked our merges based on how frequently letters occurred together in our test set„ÄÇ

 we're not going to look at the test frequencies at all„ÄÇ

 we're just going to run our merges in the order we learned them in the training set„ÄÇ

 So we're just going toÔºå for exampleÔºå firstÔºå because the first merge we did is merge E with R in the training set in the test set„ÄÇ

 we're going to merge all the Ers to Er„ÄÇ And nextÔºå we're going merge all the Er underbars to ER underbars and so on„ÄÇ

 And so the result is that the string N E W E underbar would be tokenized as a full word because if we look back at our vocabulary there it is N E WE underbar is a word„ÄÇ

It's in our vocabulary„ÄÇBut if we see the string L O W E R underbar„ÄÇ

 that's going to be tokenized as two separate tokensÔºå low and ER underbarÔºå because again„ÄÇ

 looking back at our vocabularyÔºå there is no token L O W E R underbarÔºå there's a token low„ÄÇLow„ÄÇ

 sorryÔºå there's a token low„ÄÇ and there's a token E R under bar„ÄÇ So it'll„ÄÇ

 it'll separate the words into those two tokens„ÄÇNowÔºå the resulting B„ÄÇ

 P E tokens will occur words that occur frequently in the corpus„ÄÇ

 So frequent words are going to be tokenizedÔºå and also frequent sub wordss„ÄÇ

 And often those frequent sub wordss are morphmes like EÔºå S T or EÔºå R„ÄÇ

 A morphme is the smallest meaning bearing unit of a language„ÄÇ SoÔºå for example„ÄÇ

 the word unlikelyliestÔºå has the three morphmes Un an S„ÄÇ

And often these BE tokens turn out to correspond to morphemesÔºå although not always„ÄÇ



![](img/7d563e2e96bf172f1c8b32a4bab82024_3.png)

The B pair encoding algorithm we've described is one of a set of corpus based tokenizers that are extremely widely used throughout natural language processing„ÄÇ



![](img/7d563e2e96bf172f1c8b32a4bab82024_5.png)