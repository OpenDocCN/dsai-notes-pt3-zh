# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P70ï¼šL11.8 - è¯„ä¼°å¯¹è¯ç³»ç»Ÿ - ShowMeAI - BV1YA411w7ym

![](img/5f9ba3f233c67e7567855eebbdfe21dd_0.png)

How do we evaluate dialogue systemsï¼ŸHow can we tell if a dialogue system is fulfilling its goalï¼Ÿ

Task based dialogue is evaluated mainly by measuring whether they succeed at their taskã€‚

 like booking the right flightã€‚For chatbotsï¼Œ often the goal is to be enjoyable to humansã€‚

 so chatbots are evaluated mainly with humansã€‚Let's begin with chat boxã€‚

 we evaluate these by asking humans to assign a scoreã€‚This can be the human who talk to the chatbotã€‚

 which we call participant evaluationï¼Œ or a third party who reads a transcript of a human chatbot conversationã€‚

 which we call observer evaluationã€‚In the participant evaluation used by C at allã€‚

 the human evaluator chats with the model for six turns and rates the chatbot using liquid scales on eight dimensionsã€‚

 capturing conversational qualityï¼Œ avoiding repetitionï¼Œ interestingnessï¼Œ making senseï¼Œ fluencyã€‚

 listeningï¼Œ inquisitivenessï¼Œ humanness and engagingnessã€‚

So here we've shown three of those dimensions and a li scalesã€‚

 how repetitive was this user repeated themselves over and over to always had something new and so onã€‚



![](img/5f9ba3f233c67e7567855eebbdfe21dd_2.png)

Observer evaluations use third party annotators to look at the text of a complete conversationã€‚

Sometimes we're interested in having Rars assign a score to each system turnã€‚

 so we could ask Raers to mark how coherent each turn isã€‚Oftenï¼Œ howeverã€‚

 we just want a single high level score to know if system A is better than system Bã€‚

The acute avalmetric is such an observer evaluation in which annotators look at two separate human computer conversations and choose the one in which the dialogue system performed betterã€‚

 answering questions about four propertiesï¼Œ engaging thisï¼Œ interestingnessã€‚

 humanness and knowledgeabilityã€‚

![](img/5f9ba3f233c67e7567855eebbdfe21dd_4.png)

Here's an example of the acute Aval annotator taskã€‚

 comparing two dialogues and choosing between speakerea1 in light blue and speakerea 2 in dark blueã€‚



![](img/5f9ba3f233c67e7567855eebbdfe21dd_6.png)

Automatic evaluations like the blue scores used to evaluate machine translationï¼Œ for exampleã€‚

 are generally not used for chatbotsï¼Œ since blue scores correlate poorly with human judgments on chatbotsã€‚

Developing possible automatic evaluation metrics is an open research problemã€‚

 One novel paradigm is called adversarial evaluationï¼Œ inspired by the Tring testã€‚

 The idea is to train a Tring like evaluator classifier to distinguish between human generated responses and machine generated responsesã€‚

 The more successful a response generation system is at fooling the evaluatorï¼Œ the better the systemã€‚

ğŸ˜Šï¼ŒHow about evaluating task based dialogueï¼ŸIf the task is unambiguousã€‚

 we can simply measure absolute task successã€‚Did the system book the right plane flight or put the right event on the calendarï¼Ÿ

A slightlylight more fine grainedï¼Œ we could measure the slot error rateã€‚

 the percentage of slots filled with incorrect valuesã€‚

 the number of mistaken slots over the number of total slotsã€‚Consider a system given this sentenceã€‚

 make an appointment with Chris at 1030 in Gs 104ï¼Œ which extracts the following candidate slot structureã€‚

 person Chrisï¼Œ time 1130ï¼Œ room gates 104ã€‚Hereï¼Œ the slot error rate is one third since the time is wrongã€‚

 Instead of error rateï¼Œ slot precision recall and F score can also be usedã€‚

 and slot error rate is also sometimes called concept error rateã€‚

We can also measure task success as well as the slide error rateã€‚

To get a more fine grain idea of user happinessï¼Œ we can also compute user satisfaction readingsã€‚

 where we have users interact with a dialogue system to form a task and complete a questionnaireã€‚

 Here's some sample questionsã€‚ responsesses can be mapped to the same range and then averaged overall questions to get a total user satisfaction ratingã€‚

We can also measure other factors like efficiency via total elapsse time for the dialogue in seconds or the number of total terms or the number of system termsã€‚

Quality cost measures other aspects of the interactions that affect users' perception of the systemã€‚

 One such measure is the number of times the AR system fail to return any sentence or the number of AR rejection promptsã€‚

Similar metrics include the number of times the user had to barge in and interrupt the systemã€‚



![](img/5f9ba3f233c67e7567855eebbdfe21dd_8.png)

We've seen standard ways to evaluate both chatbots and task based dialogue systemsã€‚



![](img/5f9ba3f233c67e7567855eebbdfe21dd_10.png)