# P23ÔºöL4.5- ÊÉÖÊÑüÂàÜÊûêÂª∂Â±ïÂÜÖÂÆπ - ShowMeAI - BV1YA411w7ym

Let's look at some other aspects of sentiment analysis„ÄÇ

 like linguistic negation and the use of lexicons„ÄÇ One important issue that comes up in sentiment classification is linguistic negation„ÄÇ



![](img/e2b8bd3bdc19ea44215e602efe6086e4_1.png)

So the sentence I really like this movie is very different than the sentence I really don't like this movie„ÄÇ

So here negation changes the meaning of like to a negative„ÄÇ

 and it can also change the meaning of a negative to somewhat more positive„ÄÇ

 a negative sentence like dismiss this film can be softened by don'tÔºå negative to positive„ÄÇ

 not quite as strong as positive to negativeÔºå but a very important word to deal with„ÄÇ

A simple baseline method dating back to the really the beginning of sentiment classification is to just add the string N„ÄÇ

 O T under bar to every word between a negationÔºå a linguistic negation„ÄÇ

 a not or a never or no or a don't„ÄÇ And the following punctuation„ÄÇ

 So we might take a string like didn't like this movie commaÔºå but I„ÄÇ

 And because didn't has a negation in it„ÄÇ we add not underbar to like this and movie and stop when we get to the comma„ÄÇ

And what we've basically done is we've doubled our vocabulary size„ÄÇ

 we've added a not like and a not this and a not movie token to our vocabulary„ÄÇ

 and now the not like token is going to be a great cue for a negative movie review„ÄÇ

Sometimes we don't have enough label training data or the training data and the test data are drawn from different distributions„ÄÇ

 and in such casesÔºå it can be useful to make use of a pre builtilt word list called a lexicon„ÄÇ

 And there's lots of publicly available lexiconons that are used for sentiment„ÄÇ

 So just to give a couple of examplesÔºå the MP QA subjectivity cues„ÄÇ

 lexicon labels about 7000 words for whether theyre positive or negative„ÄÇ So admirable„ÄÇ

 beautiful and confident or positive words and awfulÔºå bad catastrophe or negative words„ÄÇ

Or the general inquirerÔºå another early lexicon dating from the 60s has list of positive and negative words and„ÄÇ

 and lots of other list of words active versus passive or pleasure or pain or strongÔºå weak words„ÄÇüòä„ÄÇ



![](img/e2b8bd3bdc19ea44215e602efe6086e4_3.png)

The way we use lexiconons and sentiment classification is to add a new feature that gets a count whenever a word from lexicon occurs„ÄÇ

 So you might think of this feature as this word occurs in the positive lexicon„ÄÇ

 So when we see a positive word like good or great or beautiful any one of them„ÄÇ

 we add one to that count„ÄÇ So before we are adding one to good and one to the count for great and to the count for beautiful with lexiconons„ÄÇ

 we simply add one for any of these occurring„ÄÇ So that kind of sums over all the possible positive words or negative words for the negative word feature„ÄÇ

 And so now we've only got two featuresÔºå a positive word feature„ÄÇ

 a negative word feature where before we had lots and lots of lexical features„ÄÇNonetheless„ÄÇ

 this can be helpful when the training data sparse„ÄÇ

 we don't have really good counts to make use of or perhaps the training data is not representative of the test set„ÄÇ

 So we can't rely on the same words appearing in training and test„ÄÇ So in those kind of cases„ÄÇ

 these dense lexicon features can help„ÄÇüòä„ÄÇ

![](img/e2b8bd3bdc19ea44215e602efe6086e4_5.png)

Of courseÔºå naivebease is useful for other kinds of tasks with text than sentiment„ÄÇ

 So spam filtering is one famous example„ÄÇ So some features from a naivebe spam classifier„ÄÇ

 if the text mentions millions of dollars or if the from starts with numbers or if the subject line is in all caps or if you see the phrase 100% guaranteed or some claim that you can be removed from the list„ÄÇ

 these are all features that suggest that you're looking at spam and we can build a naivebe classifier with features like this„ÄÇ

Niveb can also be used for language identification„ÄÇ

 So that's the task of determining what language a piece of text is written in„ÄÇ

 And it's a very important preprocessing step„ÄÇ And it turns out that features based on character Ngrams do very well for this task„ÄÇ

 So certain kinds of character engrams are very distinctive for certain languages„ÄÇ

 It is important to train on different varieties of each language„ÄÇ SoÔºå for example„ÄÇ

 for English you want to make sure you train on American English varieties like Africanam Americanican English or English varieties around the world like Indian English„ÄÇ

In summaryÔºå naive Bayase really isn't that naive„ÄÇ It's very fast„ÄÇ It has low storage requirements„ÄÇ

 It works well with very small amounts of training data„ÄÇ

 which is not true for many of the more powerful algorithms„ÄÇ

 We'll see it tends to be robust to irrelevant features„ÄÇ

 It's good in domains with many equally important features„ÄÇ

 which is not true for some other machine learning algorithms like decision trees„ÄÇAnd in fact„ÄÇ

 knifeive Base is optimal in the rare case in the world where the independence assumptions actually hold„ÄÇ

 So it's a good dependable baseline for text classification„ÄÇHoweverÔºå we'll see other classifiers„ÄÇ

 logistic agressionÔºå neural networks of various kinds that give better accuracy when there's enough training data„ÄÇ



![](img/e2b8bd3bdc19ea44215e602efe6086e4_7.png)

We've now seen further aspects of sentiment classification and other tasks that naive bays can be applied to„ÄÇ

 like spam detection or language identification„ÄÇ

![](img/e2b8bd3bdc19ea44215e602efe6086e4_9.png)