# P52ï¼šL8.6- Word2Vec - ShowMeAI - BV1YA411w7ym

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_0.png)

Let's now introduce the important word to Vc embedding methodã€‚In the previous lecturesã€‚

 we saw how to represent a word as a sparseï¼Œ long vector with dimensions corresponding to words in the vocabulary or documents in a collectionã€‚

 We now introduce a more powerful word representationã€‚ embeddingsï¼Œ shortï¼Œ dense vectorsã€‚

 Unlike the vectors we've seen so farã€‚ embedding are short with the number of dimensions D ranging from 50 to 1000ã€‚

 rather than the much larger vocabulary size Vï¼Œ which could be 60000ã€‚

 or the number of documents that we've seenã€‚ These D dimensions don't have a clear interpretationã€‚

 and the vectors are dense instead of vector entries being sparseã€‚

 mostly 0 counts or functions of countsã€‚ The values will be real valued numbers that can be negativeã€‚

ðŸ˜Šã€‚

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_2.png)

It turns out that dense vectors work better in every NL P task than sparse vectorsã€‚

 While we don't completely understand all the reasons for thisï¼Œ we have some intuitionsã€‚

 representinging words as 700 dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50000 dimensional vectors and the smaller parameter space possibly helps with generalization and avoiding overfittingã€‚

Dense vectors may also do a better job of capturing synonyomyã€‚ For exampleã€‚

 in a sparse vector representationï¼Œ dimensions for synonyms like car and automobile are distinct and unrelatedã€‚

 Sprse vectors may thus fail to capture the similarity between a word with car as a neighbor and a word with automobile as a neighborã€‚



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_4.png)

In this lectureï¼Œ we introduce one method for computing embeddingsï¼Œ S graram with negative samplingã€‚

 sometimes called SGNSã€‚The Sip Gram algorithm is one of two algorithms in a software package called word to Vecã€‚

 And so sometimes the algorithm is loosely referred to as word to Vecã€‚

 The word to Vc methods are fastï¼Œ efficient to train and easily available onlineã€‚

Word tove embeddings are static embeddingsï¼Œ meaning that the method learns one fixed embedding for each word in the vocabularyã€‚

An alternative to these static embeddings are more recent methods for learning dynamic contextual embeddingsã€‚

 like the popular burnt representations in which the vector for each word is different in different contextsã€‚



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_6.png)

The intuition of word tovec is that instead of counting how often each word W occurs near another wordã€‚

 say apricot will instead train a classifier on a binary prediction taskã€‚

 Is word W likely to show up near Apricotã€‚And we don't actually care about this prediction taskã€‚

 insteadï¼Œ we'll take the learned classifier weights as the word embeddingsã€‚

The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifierã€‚

 A word C that occurs near the target word acts as a goldï¼Œ correct answer to the questionã€‚

Is word C likely to show up near the target wordã€‚This methodï¼Œ often called self supervisionã€‚

 avoids the need for any sort of hand labeled supervision signalã€‚

 The idea was first proposed in the task of neural language modellingã€‚

 but word tove is a much simpler model than the neural network language modelã€‚



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_8.png)

The intuition of skipipgramham is to treat the target word T and a neighboring context word C as positive examples of words that can occur near each otherã€‚

 then randomly sample other words in the lexicon to get negative examples and use logistic regression to train a classifier to distinguish these two cases and then use the learned weights as the embedding representation of the wordsã€‚

Let's start by thinking about the classification taskï¼Œ and in the next lectureã€‚

 we'll turn to how to trainã€‚Imagine a sentence like the following with the target word apricot and assume we're using a window of plus or minus two context wordsã€‚

Our goal to train a classifier such that given a tuple Wa C of a target word paired with a candidate context word like Apricot and jamã€‚

 or maybe Apricot and Arvarkï¼Œ it'll return the probability that C is a real context wordã€‚

 true for jam false forardvarkã€‚ So we'd like P of plus given Apricota jam to be highã€‚And P of minusã€‚

 given aprico Arvarkã€‚To be highã€‚And in factï¼Œ the probability that word C is not a real context word for W is just one minus the probability that it is a context wordã€‚

The intuition of the SkGram model is to base this probability on embedding similarityã€‚

A word is likely to occur near the target if it's embedding similar to the target embeddingã€‚

To compute similarity between these dense embeddingsã€‚

 we rely on the intuition that two vectors are similar if they have a high dot productã€‚ After allã€‚

 cosine is just a normalized dot productã€‚In other wordsã€‚

 the similarity between a word embedding and a context embedding is proportional to Wã€‚cã€‚

Will need to normalize to turn this similarity into a probabilityã€‚That's because the dot productï¼Œ Cã€‚

 dot W or W dot C is not a probabilityã€‚ It's just a number ranging from negative infinity to positive infinityã€‚

 Since the elements in words to vacuummb can be negativeï¼Œ the dot product can be negativeã€‚

So to turn the dot product into a probabilityï¼Œ we'll use the logistic or sigmoid function sigma that we saw in logistic regressionã€‚

 So we model the probability that word C is a real context word for target word W as sigma of C dot W or 1 over 1 plus x of minus c dot Wã€‚

Now to make this a probabilityï¼Œ we also need the total probability of the two possible eventsã€‚

 C is a context wordï¼Œ C isn't the context wordï¼Œ to sum to1ã€‚

 and so we estimate the probability that word C is not a real context word for w as1 minus p of plus or1 over1 plus x of cw without the minusã€‚

So the equation we just saw gives us the probability for one wordã€‚

 but there are many context words in the windowã€‚ Skipgramm makes the simplifying assumption that all context words are independentã€‚

 allowing us just to multiply their probabilitiesã€‚ So for all of the L words in the context windowã€‚

 we're simply going to multiply the probabilities or in log spaceã€‚

 we're going to add their log probabilitiesã€‚In summaryã€‚

 Sipgramm trains a probabilistic classifier that given a target word W and its context window of L wordsã€‚

 C1 through L assigns a probability based on how similar the context window is to the target wordã€‚

 and this probability is based on applying the logistic function the sigmoid function to the dot product of the embeddings of the target word with each context wordã€‚

To compute this probabilityï¼Œ we just need embeddings for each target word and context word in the vocabularyã€‚

Here's the intuition of those parameters we're going to needã€‚

 and we'll learn them in the next lectureã€‚Skip graham stores two embeddings for each wordã€‚

 one for the word as a target and one for the word considered as a contextã€‚ Thusã€‚

 the parameters we need to learn are two matricesã€‚W and Cã€‚

 each containing and embedding for every one of the V words in the vocabularyã€‚



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_10.png)

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_11.png)

We've seen how the skip graram classifier functions in the next lecture we'll talk about learning its weightsã€‚

 which is the purpose of the classifierã€‚

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_13.png)