# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P52ÔºöL8.6- Word2Vec - ShowMeAI - BV1YA411w7ym

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_0.png)

Let's now introduce the important word to Vc embedding method„ÄÇIn the previous lectures„ÄÇ

 we saw how to represent a word as a sparseÔºå long vector with dimensions corresponding to words in the vocabulary or documents in a collection„ÄÇ

 We now introduce a more powerful word representation„ÄÇ embeddingsÔºå shortÔºå dense vectors„ÄÇ

 Unlike the vectors we've seen so far„ÄÇ embedding are short with the number of dimensions D ranging from 50 to 1000„ÄÇ

 rather than the much larger vocabulary size VÔºå which could be 60000„ÄÇ

 or the number of documents that we've seen„ÄÇ These D dimensions don't have a clear interpretation„ÄÇ

 and the vectors are dense instead of vector entries being sparse„ÄÇ

 mostly 0 counts or functions of counts„ÄÇ The values will be real valued numbers that can be negative„ÄÇ

üòä„ÄÇ

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_2.png)

It turns out that dense vectors work better in every NL P task than sparse vectors„ÄÇ

 While we don't completely understand all the reasons for thisÔºå we have some intuitions„ÄÇ

 representinging words as 700 dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50000 dimensional vectors and the smaller parameter space possibly helps with generalization and avoiding overfitting„ÄÇ

Dense vectors may also do a better job of capturing synonyomy„ÄÇ For example„ÄÇ

 in a sparse vector representationÔºå dimensions for synonyms like car and automobile are distinct and unrelated„ÄÇ

 Sprse vectors may thus fail to capture the similarity between a word with car as a neighbor and a word with automobile as a neighbor„ÄÇ



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_4.png)

In this lectureÔºå we introduce one method for computing embeddingsÔºå S graram with negative sampling„ÄÇ

 sometimes called SGNS„ÄÇThe Sip Gram algorithm is one of two algorithms in a software package called word to Vec„ÄÇ

 And so sometimes the algorithm is loosely referred to as word to Vec„ÄÇ

 The word to Vc methods are fastÔºå efficient to train and easily available online„ÄÇ

Word tove embeddings are static embeddingsÔºå meaning that the method learns one fixed embedding for each word in the vocabulary„ÄÇ

An alternative to these static embeddings are more recent methods for learning dynamic contextual embeddings„ÄÇ

 like the popular burnt representations in which the vector for each word is different in different contexts„ÄÇ



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_6.png)

The intuition of word tovec is that instead of counting how often each word W occurs near another word„ÄÇ

 say apricot will instead train a classifier on a binary prediction task„ÄÇ

 Is word W likely to show up near Apricot„ÄÇAnd we don't actually care about this prediction task„ÄÇ

 insteadÔºå we'll take the learned classifier weights as the word embeddings„ÄÇ

The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier„ÄÇ

 A word C that occurs near the target word acts as a goldÔºå correct answer to the question„ÄÇ

Is word C likely to show up near the target word„ÄÇThis methodÔºå often called self supervision„ÄÇ

 avoids the need for any sort of hand labeled supervision signal„ÄÇ

 The idea was first proposed in the task of neural language modelling„ÄÇ

 but word tove is a much simpler model than the neural network language model„ÄÇ



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_8.png)

The intuition of skipipgramham is to treat the target word T and a neighboring context word C as positive examples of words that can occur near each other„ÄÇ

 then randomly sample other words in the lexicon to get negative examples and use logistic regression to train a classifier to distinguish these two cases and then use the learned weights as the embedding representation of the words„ÄÇ

Let's start by thinking about the classification taskÔºå and in the next lecture„ÄÇ

 we'll turn to how to train„ÄÇImagine a sentence like the following with the target word apricot and assume we're using a window of plus or minus two context words„ÄÇ

Our goal to train a classifier such that given a tuple Wa C of a target word paired with a candidate context word like Apricot and jam„ÄÇ

 or maybe Apricot and ArvarkÔºå it'll return the probability that C is a real context word„ÄÇ

 true for jam false forardvark„ÄÇ So we'd like P of plus given Apricota jam to be high„ÄÇAnd P of minus„ÄÇ

 given aprico Arvark„ÄÇTo be high„ÄÇAnd in factÔºå the probability that word C is not a real context word for W is just one minus the probability that it is a context word„ÄÇ

The intuition of the SkGram model is to base this probability on embedding similarity„ÄÇ

A word is likely to occur near the target if it's embedding similar to the target embedding„ÄÇ

To compute similarity between these dense embeddings„ÄÇ

 we rely on the intuition that two vectors are similar if they have a high dot product„ÄÇ After all„ÄÇ

 cosine is just a normalized dot product„ÄÇIn other words„ÄÇ

 the similarity between a word embedding and a context embedding is proportional to W„ÄÇc„ÄÇ

Will need to normalize to turn this similarity into a probability„ÄÇThat's because the dot productÔºå C„ÄÇ

 dot W or W dot C is not a probability„ÄÇ It's just a number ranging from negative infinity to positive infinity„ÄÇ

 Since the elements in words to vacuummb can be negativeÔºå the dot product can be negative„ÄÇ

So to turn the dot product into a probabilityÔºå we'll use the logistic or sigmoid function sigma that we saw in logistic regression„ÄÇ

 So we model the probability that word C is a real context word for target word W as sigma of C dot W or 1 over 1 plus x of minus c dot W„ÄÇ

Now to make this a probabilityÔºå we also need the total probability of the two possible events„ÄÇ

 C is a context wordÔºå C isn't the context wordÔºå to sum to1„ÄÇ

 and so we estimate the probability that word C is not a real context word for w as1 minus p of plus or1 over1 plus x of cw without the minus„ÄÇ

So the equation we just saw gives us the probability for one word„ÄÇ

 but there are many context words in the window„ÄÇ Skipgramm makes the simplifying assumption that all context words are independent„ÄÇ

 allowing us just to multiply their probabilities„ÄÇ So for all of the L words in the context window„ÄÇ

 we're simply going to multiply the probabilities or in log space„ÄÇ

 we're going to add their log probabilities„ÄÇIn summary„ÄÇ

 Sipgramm trains a probabilistic classifier that given a target word W and its context window of L words„ÄÇ

 C1 through L assigns a probability based on how similar the context window is to the target word„ÄÇ

 and this probability is based on applying the logistic function the sigmoid function to the dot product of the embeddings of the target word with each context word„ÄÇ

To compute this probabilityÔºå we just need embeddings for each target word and context word in the vocabulary„ÄÇ

Here's the intuition of those parameters we're going to need„ÄÇ

 and we'll learn them in the next lecture„ÄÇSkip graham stores two embeddings for each word„ÄÇ

 one for the word as a target and one for the word considered as a context„ÄÇ Thus„ÄÇ

 the parameters we need to learn are two matrices„ÄÇW and C„ÄÇ

 each containing and embedding for every one of the V words in the vocabulary„ÄÇ



![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_10.png)

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_11.png)

We've seen how the skip graram classifier functions in the next lecture we'll talk about learning its weights„ÄÇ

 which is the purpose of the classifier„ÄÇ

![](img/30a156b0e11cfb71bcfbd45b9c7cb4de_13.png)