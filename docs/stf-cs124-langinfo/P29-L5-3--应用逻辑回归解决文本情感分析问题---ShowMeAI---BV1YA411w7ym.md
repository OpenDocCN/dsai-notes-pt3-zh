# P29ï¼šL5.3- åº”ç”¨é€»è¾‘å›å½’è§£å†³æ–‡æœ¬æƒ…æ„Ÿåˆ†æé—®é¢˜ - ShowMeAI - BV1YA411w7ym

![](img/205fd987c71f2e04c4b35df0890f452a_0.png)

Let's walk through an example using logistic regression to do sentiment classificationã€‚

Suppose we're doing binary sentiment classification on a movie review textã€‚

 And we'd like to know whether to assign the sentiment class 1ï¼Œ meaning positive or 0ã€‚

 meaning negative to this following reviewã€‚ It's Hokeyã€‚ There are virtually no surprisesã€‚

 The writing is second rateã€‚So why was it so enjoyableï¼Œ you get the ideaã€‚

Let's assume each review is an observationã€‚ And we're going to represent each of these observations by six featuresã€‚

 the following six featuresã€‚ So one feature X1 is the count of words in some positive lexicon in the documentã€‚

 What is the value of that feature in our exampleã€‚ Wellï¼Œ let's suppose that the words enjoyableã€‚

 great and nice are all in our positive lexiconã€‚ So the value of X 1 is 3ã€‚

 There are three positive words in this reviewã€‚ How about x 2ã€‚

 the count of negative words in the in the documentã€‚ğŸ˜Šï¼ŒWellã€‚

 here we have the word hokey and we have the word second rateã€‚

 so our value for the second feature is2ã€‚And we can have other features likeã€‚

 does the word no occur in the documentï¼Œ And it doesã€‚ There's the word noã€‚

We can count the number of pronounsï¼Œ we could have a real valued feature like the log of the number of words in the documentã€‚

 there are 66 words in this document and if we put in the natural logã€‚

 we get a value for feature x6 of 4ã€‚19ã€‚So those are the feature valuesã€‚

 And let's assume for the moment that we've already learned a real valued weight for each of these features and that the six weights corresponding to the six features are as followsã€‚

 weight 1 is 2ã€‚5ï¼Œ weight 2 is negative 5ï¼Œ weight 3 is negative 1ã€‚2 and so on for the6 weightsã€‚

 And let's assume that we have learned a bias ã€‚1ã€‚ We'll talk later about how we learn these thingsã€‚

 So the weight W1ï¼Œ for exampleï¼Œ indicates how important a featureã€‚

 the number of positive lexicon wordsï¼Œ feature x1ï¼Œ how important a feature this is to making a positive sentiment decisionã€‚

 And W2 tells us the importance of negative lexicon wordsã€‚ğŸ˜Šï¼ŒNowï¼Œ notice that feature W1 is positiveã€‚

 and W2 is negativeï¼Œ meaning that negative words are negatively associated with a positive sentiment decisionã€‚

 And furtherï¼Œ they're about twice as important as positive wordsã€‚ So the weights tell us somethingã€‚

So let's plug the x's and wsï¼Œ the feature values and their weights into our equation for computing the probability of our classesã€‚

To compute the probability that this is a positive documentã€‚

 that's to say the probability that y equals1ã€‚ we compute sigma of W X plus Bã€‚ What's W dot Xã€‚ Wellã€‚

 here's our weight vector that we just talked aboutã€‚

 Here's our feature values that we saw on the slide beforeã€‚ Andï¼Œ and this is our learned biasã€‚

 So it's sigma of 0ã€‚833ï¼Œ which turns out to be 0ã€‚7ã€‚

And the probability of that this is a negative review is just going to be one minus the probabilityã€‚

 that's a positive reviewï¼Œ so 0ã€‚3ã€‚ So this classifier would guess that this is a positive reviewã€‚Nowã€‚

 it turns out we can build features for logistic regression for any classification taskã€‚

 So let's pick one at random period disambiguationã€‚ So that's the taskï¼Œ for exampleã€‚

 of deciding what these different periods meanã€‚ This period here is an end of sentence periodã€‚

 where this period here is not an end of sentenceã€‚So we might use features like x1 hereã€‚

 expressing that the current word is lowercaseï¼Œ perhaps with a positive weightã€‚

 because lowercase words with a period at the end are maybe more likely to be end of sentenceã€‚

Or perhaps that the current word is on our acronyms and abbreviations dictionaryã€‚

 So like maybe S T is in our acronyms or D R for doctor or drive or in our acronym abbreviations dictionaryã€‚

 and now the weight might be negative because periods after acronyms are not likely to be end of sentence periods and features can express quite complicated combination of propertiesã€‚

 For exampleï¼Œ a period following an uppercase word is likely to be an end of sentenceã€‚

 But if the word is S T dot and the previous word is capitalizedã€‚

 then it's probably part of a shortening of the word streetã€‚So in summaryã€‚

 for classification and binary logistic regressionï¼Œ we have a set of classesã€‚

 let's say positive and negative sentimentï¼Œ which we can just refer to as0 and 1ã€‚

 we'll have a vector of features and those features values have things like counts or logs of thingsã€‚

 we'll have a vector of weightsï¼Œ one for every featureï¼Œ plus a biasã€‚

 and to compute the probability of the positive class will just run the W x plus B through a sigmoidã€‚



![](img/205fd987c71f2e04c4b35df0890f452a_2.png)

We've now seen how logistic aggress can take feature values and their weights and compute a class for an input exampleã€‚

