# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P28ï¼šL5.2- é€»è¾‘å›å½’åˆ†ç±» - ShowMeAI - BV1YA411w7ym

![](img/d6b049281decbf37247b970b5a9b3eca_0.png)

In this lectureï¼Œ we'll see how to do classification with logistic regression and we'll introduce the important sigmoid functionã€‚



![](img/d6b049281decbf37247b970b5a9b3eca_2.png)

Classification can be used for many kinds of text tasksã€‚

 like determining positive or negative sentiment or determining if a message is spam or not spam or determining authorshipã€‚

 recall the famous case of the Federalist papers written by Hamilton or by Madisonã€‚ And againã€‚

 remember that for text classificationï¼Œ we have a document Xï¼Œ we have a fixed set of classesã€‚

 capital Cã€‚And then we're going to output a predicted class Y hatã€‚

 which is one of these classes in Cï¼Œ given Xã€‚And given a sequence of input output pairsã€‚

 so remember that we're using the superscript for each observation X and its label Y for each such observationã€‚

 we're going to represent a device set of feature vectorsï¼Œ x1 through xNã€‚

 and then we're going to give it a predicted class Y hat from the set 0 or 1ã€‚

 so binary classificationã€‚In logistic regressionï¼Œ we're going to make use of featuresã€‚

 each feature X sub I is a feature expressing some fact about the textã€‚

 and it's going to have a weight W sub I that tells us how important X sub I is to that classification decisionã€‚

 So imagine that one feature X sub I might beã€‚ The review contains the word awesomeã€‚ğŸ˜Šã€‚

And let's say the weight W sub I is plus 10ï¼Œ suggesting that for a sentiment classifierã€‚

 having the word awesome is a highly weighted positive cu toward the review being positive sentimentã€‚

 By contrastï¼Œ abysmal might have a weight W sub Jã€‚ The feature review contains abysmal might have a feature W sub J of negative 10ã€‚

 which it means that abysmal is highly weightedï¼Œ but a negative weightã€‚

 So that's highly weighted toward the negative classã€‚ğŸ˜Šï¼ŒAnd a different featureï¼Œ x sub Kã€‚

 the review contains the word mediocre might be slightly negativeã€‚

 so our weight is slightly negativeï¼Œ leaning us very slightly towardã€‚The negative classã€‚

So in summary for logistic regression we're going to take an input observationã€‚

 we're going to represent it as a set of featuresï¼Œ X will be represented as a set of n featuresã€‚

 we'll have a set of n weightsï¼Œ W1 through Wnï¼Œ and sometimes you'll see these weights called thetasã€‚

 theta 1 through theta n and sometimes we call the entire set of parameters thetaã€‚

 and we're going to output a predicted class Y hat from the said01ã€‚

That's binary logistic regression we'll see later a generalization to multinomial logistic regressiong where the output could be from some larger set of possible classesã€‚

 but we'll stick to binary for nowã€‚How do we do this classificationï¼Œ Againã€‚

 the weight W tells us the importance of the feature will also have a bias termã€‚

 sometimes called the interceptã€‚ And that's another real number that's added to the weighted inputsã€‚

And we'll sum up all these weighted features and the biasã€‚ So we'll just take each feature Wã€‚

 each feature X and its's weight Wï¼Œ and we'll multiply them togetherã€‚ sum them all upï¼Œ add the biasã€‚

 and we'll compute a score for the sum of all of these features in these lectures will represent such sums using the dot product notation from linear algebraã€‚

 So the dot product of two vectors A and B written as a dot B is the sum of the products of the corresponding elementsã€‚

 So here the sum of all these W sub I X subs will represent as W dot xã€‚ğŸ˜Šã€‚

So we're going to compute this sum Zã€‚W dot x plus Bï¼Œ and if it's highï¼Œ we'll sayï¼Œ yepã€‚

 this is a positive classï¼Œ this is positive sentimentï¼Œ let's sayã€‚

 and if W x plus b is low if z is lowï¼Œ then we'll say that it's the negative class y equals zeroã€‚

Our goalï¼Œ howeverï¼Œ is a probabilistic classifierã€‚ We'd like to formalize this idea of if the sum is high that I mentioned on the previous slideã€‚

 We like a principal classifier giving us a probability just like naive bath didã€‚

 So we want a model that can give us actually a probabilityï¼Œ the probability that y is1ã€‚

 given x parameter is by all the weights theta or the probability that y 0 given input example x and againã€‚

 parameterized by thetaã€‚ And the problem is that Z to W x plus Bã€‚ It's not a probabilityã€‚

 It's just some numberã€‚ In factï¼Œ nothing forces Wx plus B to lie between 0 and1 weights are real valuedã€‚

 It might even be negativeã€‚ Zï¼Œ in factï¼Œ ranges from negative infinity to positive infinityã€‚

 So insteadï¼Œ we're going to use a function of z that goes from0 to1ã€‚

 and the function we're going to use is the very useful sigmoid or logistic functionã€‚

 So Y equals sigma of Z and the sigmoid is1 over1 plus E to the minus Z or as we're goingã€‚

Refer to it from now onï¼Œ1 over1 plus x of minus zã€‚Here's a picture of the sigmoid function so called because it looks like an Sã€‚

 The function takes a real value and maps it neatly into the rangeï¼Œ 0ï¼Œ1ã€‚

You notice that it's nearly linear around zeroã€‚But outlier values get squashed towards 001ã€‚

 so it's a very useful functionã€‚So here's how logistic regression is going to workã€‚ We'll computeã€‚

 we have our features and our weights and our bias termï¼Œ we'll compute W dot x plus Bã€‚

 we'll pass it through the sigmoid functionï¼Œ and then we'll just treat it like a probabilityã€‚

 So now we just have to figure out how do we turn sigmoid outputs into probabilitiesã€‚Wellã€‚

 we're almost thereã€‚So if we apply the sigmoid to the sum of the weighted featuresã€‚

 we get a number between 0 and1ã€‚ So this will be between 0 and 1ã€‚ And to make it a probabilityã€‚

 we just need to make sure that the two cases P of y equals1 and p of y equals 0ï¼Œ sum to1ã€‚

 And we can do that as followsã€‚ we'll just make p of y equals 0ï¼Œ be1 minus sigma of w X plus Bã€‚

 which is p of y equals1 So we're guaranteed these two will sum to1ã€‚

 We can do a little arithmetic hereï¼Œ plugging in one over1 plus x of minus zã€‚By the wayã€‚

 the sigmoid function has a useful property that one minus sigma of x is the same as sigma of minus of xã€‚

 So oneï¼Œ the probability of p of y equals 0ï¼Œ which we refer to as1 minus sigma of Wx plus Bã€‚

 we could also refer to as sigma of minus Wx plus Bã€‚ We'll sometimes see it in that formã€‚

So now to turn the probability into classifierï¼Œ our estimateã€‚

 Y hat our estimate of the class of our example X is oneã€‚

 if p of y equals1 is greater than 05 and 0 otherwiseã€‚ So 05 here is the decision boundaryã€‚

 So here I have graphed W X plus B along the X axisã€‚ and the probability produced by our sigmoidã€‚

 Y equals 1 on the y axisã€‚ So heres our sigma of w x plus B graphã€‚ and here's our decision boundaryã€‚

 So anything greater than 05ã€‚ If w x plus B is greater than 05ã€‚

 then P of y equals 1 is greater than 0ã€‚5ã€‚ and we're going to label this example positiveã€‚ğŸ˜Šã€‚

So in other wordsï¼Œ given an example xï¼Œ we're going to give it Y hat its class1ã€‚

 if W x plus b is greater than 0ï¼Œ and we're going to call it class 0ã€‚

 if W x plus B is less than or equal to 0ã€‚

![](img/d6b049281decbf37247b970b5a9b3eca_4.png)

We've now seen how logistic regression uses the sigmoid function to take weighted features from an input example X and assign it to a class y1 or0ã€‚

