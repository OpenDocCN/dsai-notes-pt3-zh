# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P4ÔºöL1.4- ÂàÜËØç‰∏éÈ¢ÑÂ§ÑÁêÜ - ShowMeAI - BV1YA411w7ym

In the next few lecturesÔºå we'll introduce text normalization„ÄÇ

 the process of turning a text into a standard formatting of words or sentences„ÄÇ

 And we'll start by thinking about word tokenization„ÄÇ

 breaking up a text into tokens that represent individual words or parts of words„ÄÇ

 Every anlopyask requires text normalization„ÄÇ And we normally think about at least three processes involved in normalization„ÄÇ

 One is tokenizing or segmenting out the words„ÄÇ And then once we've tokenize these words„ÄÇ

 we want to normalize them into a format„ÄÇ So we have a standard format for our text„ÄÇ

 And we're also going to have to segment larger chunksÔºå sentences or sometimes even paragraphs„ÄÇ Now„ÄÇ

 the simplest way to tokenize is just to use the space between characters„ÄÇ

 and that works well for languages that have space characters„ÄÇ

 So languages that use the Latin writing system or Arabic or Cyyrilllic or Greek„ÄÇ

 This is a very useful way to break off a word a word as a thing in between spaces„ÄÇ

 So let's introduce some simple euchx tools„ÄÇ

![](img/602f5080d536e330773045c8bbcf085e_1.png)

For text processing and starting with the Unix TR command„ÄÇ

 which is useful for space based word tokenization„ÄÇ

 And our goal here is to take a text file and output word tokens and their frequencies„ÄÇ



![](img/602f5080d536e330773045c8bbcf085e_3.png)

We're going to introduce some standard Uni tools that are used for text processing„ÄÇ

So I have here a corpus of Shakespeare„ÄÇShakespeare's complete works„ÄÇ

You can see here's the sonnets and it goes on through all the plays„ÄÇ

 so let's start by extracting all the words in the corpus„ÄÇ

So we're going to do this using the TR program„ÄÇAlrightÔºå so the TR program takes„ÄÇCharacter„ÄÇ

 and it maps every instance of that character into another character„ÄÇ And we specify T R dash C„ÄÇ

 which means compliment„ÄÇ So it means take every character that's not one of these characters and turn it into this character„ÄÇ

 So in this caseÔºå it's take every non alphaphabettic character and turn it into a carriage return„ÄÇ

 So we're going to replace all the periods and commas and spaces in Shakespeare with new line„ÄÇ

 So we're going create one lineÔºå one word per line in this way„ÄÇ So let's look at that„ÄÇüòä„ÄÇ

So there's we've now turned this onnet to one word per line„ÄÇAnd now„ÄÇWe're going to sort those„ÄÇWords„ÄÇ

So let us look at the unique word types„ÄÇSo let's do that„ÄÇAnd you can see here's all the A's„ÄÇ

 there's a lot of themÔºå A occurs a lot in Shakespeare„ÄÇ

And thats this is a very boring way to look through all of Shakespeare„ÄÇ

 We don't want to do this So let's„ÄÇInstead„ÄÇUse the program unique„ÄÇAnd the program unique will„ÄÇ

Take that sorted file and tell us for each unique typeÔºå the count of times that it occurs„ÄÇ

 So let's try that„ÄÇ So here we have all the words in Shakespeare with a count along the left„ÄÇ

 This is the product of the unique program„ÄÇAnd we can walk through„ÄÇSo we know that in Shakespeare„ÄÇ

 the word achievement with a capital A occurs onceÔºå the word Achilles appears 79 times„ÄÇ

 the word a quaint six times and so on„ÄÇSo that's interestingÔºå but„ÄÇ

It would be nice if we didn't have to just look at these words in alphabetical order„ÄÇ

 but if we could look at them in frequency orderÔºå so let's take this same list of words and now re it by frequency„ÄÇ

So now we have„ÄÇThe most frequent word in Shakespeare is the word the followed by the word I followed by the word and and we have the actual accounts in Shakespeare so that here is our lexicon of Shakespeare sorted in frequency order„ÄÇ

 Here' are some problems„ÄÇ One is that the word and occurs twice because we didn't map our uppercase words to lowercase words„ÄÇ

 So let's let's fix the mapping of case first let's try that again we're going to map all of the„ÄÇ

Uppercase letters„ÄÇTo lowercase letters„ÄÇIn Shakespeare„ÄÇAnd we're going to pipe that„ÄÇ

To another instance of the TR program„ÄÇWwhich replaces all of the non alphabetics with new lines and now we're going to do our sorting as we did before„ÄÇ

 we're going to use unique to find all the individual types„ÄÇ

 unique dash she tells us the actual countÔºå and then we're going to sort again„ÄÇ

 means numerically and our means start from the highest one„ÄÇAnd then we'll look at those„ÄÇ

 So let's do that„ÄÇAll rightÔºå so now we've solved the problem of the and„ÄÇ

 so now we only have lowercase and we don't have our uppercase and appearing„ÄÇ

 but we have another problem„ÄÇWe have this D hereÔºå Y is the word D or the word SÔºü

Why are they so frequent in ShakespeareÔºü

![](img/602f5080d536e330773045c8bbcf085e_5.png)

Of courseÔºå tokenization in most real situations isn't as simple as I've suggested with the simple Unix tools„ÄÇ

 So one problem is you can't just blindly remove punctuation because you have words in which the punctuation is part of the word Ph D or A and T„ÄÇ

 There are lots of classes of these punctuation tokenization interactions„ÄÇ

 So prices have dollar signs or periods or eurosybols and dates might have slashes or dashes and„ÄÇ

 of courseÔºå Us in hashtags and email addresses all have punctuations who have to deal with that in a special way„ÄÇ

 Another problem is critics„ÄÇ a cl is a word that doesn't stand on its own„ÄÇ

 So the word R in in the English word weir is shortened and of attached to the word we or in French the words or the word tend to attach to their neighboring words„ÄÇ

 And these clinicsÔºå we have to decide whether those are separate words we want to pull them off or not„ÄÇ

 And this question about what counts as a word applies for multiword expressionsÔºå words„ÄÇ

Like New YorkÔºå should that be one word or two wordsÔºå rock and rollÔºå one word or threeÔºü

So most standard tokenization programs for English or languages with similar writing systems deal with each of these issues„ÄÇ

 So here'sÔºå for exampleÔºå a simple Python tokenizer in the natural language toolkit that has little regular expressions for dealing with hyphens and abbreviations and currency and what not„ÄÇ

 But what about all the languages that don't have spaces between words„ÄÇ Many languagesÔºå Chinese„ÄÇ

 Japanese tie or among themÔºå don't use spaces to separate words„ÄÇ

 How do we decide where token boundaries should be in these languages„ÄÇSo let'sÔºå for example„ÄÇ

 look at word tokenization in Chinese„ÄÇ Chinese words are composed of characters called Hanza„ÄÇ

 or sometimes just„ÄÇAnd each of these characters represents a meaning unit called a morphine„ÄÇ

 We'll talk about morphemmes later„ÄÇ Every word has about two and a half characters on average„ÄÇ

 but deciding what counts as a word is complex in Chinese and not agreed upon„ÄÇ

 So imagine the following Chinese sentenceÔºå meaning that Yao Ming reaches the finals„ÄÇ

Yao Ming Jin Ru Dong Jsai„ÄÇ NowÔºå is that three words„ÄÇ Yao Ming reaches the finals„ÄÇ

 Maybe that should be five words„ÄÇ Maybe we separate Yaom Ming's first and last names„ÄÇ

 and maybe will finals really has got two parts„ÄÇ the overall part and the rest of finals„ÄÇ

 Or we could just separate it into characters altogether„ÄÇ And now the two parts of the word reach„ÄÇ

 which are both verbs themselves become separate words„ÄÇ So we justÔºå everything becomes a character„ÄÇ

 SoÔºå in factÔºå this last solution is very common„ÄÇ It's very common in Chinese to just treat characters as tokens„ÄÇ

 And then segmentation becomes very simple„ÄÇBut in other languages like Thai and Japanese„ÄÇ

 more complex word segmentation is required„ÄÇ And here„ÄÇ

 the standard algorithms are neural sequence models trained by supervised machine learning„ÄÇ

 things that we'll talk about later on in the course„ÄÇ

 Word tokenization is an important step in text normalization„ÄÇ Here we introduce two baseline method„ÄÇ

 space based tokenization and character based tokenization„ÄÇ

 and we'll turn to more advanced methods and future lectures„ÄÇ



![](img/602f5080d536e330773045c8bbcf085e_7.png)

![](img/602f5080d536e330773045c8bbcf085e_8.png)