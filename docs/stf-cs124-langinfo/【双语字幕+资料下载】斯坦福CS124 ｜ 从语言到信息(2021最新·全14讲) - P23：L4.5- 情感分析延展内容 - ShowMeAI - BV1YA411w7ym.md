# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P23ï¼šL4.5- æƒ…æ„Ÿåˆ†æå»¶å±•å†…å®¹ - ShowMeAI - BV1YA411w7ym

Let's look at some other aspects of sentiment analysisã€‚

 like linguistic negation and the use of lexiconsã€‚ One important issue that comes up in sentiment classification is linguistic negationã€‚



![](img/e2b8bd3bdc19ea44215e602efe6086e4_1.png)

So the sentence I really like this movie is very different than the sentence I really don't like this movieã€‚

So here negation changes the meaning of like to a negativeã€‚

 and it can also change the meaning of a negative to somewhat more positiveã€‚

 a negative sentence like dismiss this film can be softened by don'tï¼Œ negative to positiveã€‚

 not quite as strong as positive to negativeï¼Œ but a very important word to deal withã€‚

A simple baseline method dating back to the really the beginning of sentiment classification is to just add the string Nã€‚

 O T under bar to every word between a negationï¼Œ a linguistic negationã€‚

 a not or a never or no or a don'tã€‚ And the following punctuationã€‚

 So we might take a string like didn't like this movie commaï¼Œ but Iã€‚

 And because didn't has a negation in itã€‚ we add not underbar to like this and movie and stop when we get to the commaã€‚

And what we've basically done is we've doubled our vocabulary sizeã€‚

 we've added a not like and a not this and a not movie token to our vocabularyã€‚

 and now the not like token is going to be a great cue for a negative movie reviewã€‚

Sometimes we don't have enough label training data or the training data and the test data are drawn from different distributionsã€‚

 and in such casesï¼Œ it can be useful to make use of a pre builtilt word list called a lexiconã€‚

 And there's lots of publicly available lexiconons that are used for sentimentã€‚

 So just to give a couple of examplesï¼Œ the MP QA subjectivity cuesã€‚

 lexicon labels about 7000 words for whether theyre positive or negativeã€‚ So admirableã€‚

 beautiful and confident or positive words and awfulï¼Œ bad catastrophe or negative wordsã€‚

Or the general inquirerï¼Œ another early lexicon dating from the 60s has list of positive and negative words andã€‚

 and lots of other list of words active versus passive or pleasure or pain or strongï¼Œ weak wordsã€‚ğŸ˜Šã€‚



![](img/e2b8bd3bdc19ea44215e602efe6086e4_3.png)

The way we use lexiconons and sentiment classification is to add a new feature that gets a count whenever a word from lexicon occursã€‚

 So you might think of this feature as this word occurs in the positive lexiconã€‚

 So when we see a positive word like good or great or beautiful any one of themã€‚

 we add one to that countã€‚ So before we are adding one to good and one to the count for great and to the count for beautiful with lexicononsã€‚

 we simply add one for any of these occurringã€‚ So that kind of sums over all the possible positive words or negative words for the negative word featureã€‚

 And so now we've only got two featuresï¼Œ a positive word featureã€‚

 a negative word feature where before we had lots and lots of lexical featuresã€‚Nonethelessã€‚

 this can be helpful when the training data sparseã€‚

 we don't have really good counts to make use of or perhaps the training data is not representative of the test setã€‚

 So we can't rely on the same words appearing in training and testã€‚ So in those kind of casesã€‚

 these dense lexicon features can helpã€‚ğŸ˜Šã€‚

![](img/e2b8bd3bdc19ea44215e602efe6086e4_5.png)

Of courseï¼Œ naivebease is useful for other kinds of tasks with text than sentimentã€‚

 So spam filtering is one famous exampleã€‚ So some features from a naivebe spam classifierã€‚

 if the text mentions millions of dollars or if the from starts with numbers or if the subject line is in all caps or if you see the phrase 100% guaranteed or some claim that you can be removed from the listã€‚

 these are all features that suggest that you're looking at spam and we can build a naivebe classifier with features like thisã€‚

Niveb can also be used for language identificationã€‚

 So that's the task of determining what language a piece of text is written inã€‚

 And it's a very important preprocessing stepã€‚ And it turns out that features based on character Ngrams do very well for this taskã€‚

 So certain kinds of character engrams are very distinctive for certain languagesã€‚

 It is important to train on different varieties of each languageã€‚ Soï¼Œ for exampleã€‚

 for English you want to make sure you train on American English varieties like Africanam Americanican English or English varieties around the world like Indian Englishã€‚

In summaryï¼Œ naive Bayase really isn't that naiveã€‚ It's very fastã€‚ It has low storage requirementsã€‚

 It works well with very small amounts of training dataã€‚

 which is not true for many of the more powerful algorithmsã€‚

 We'll see it tends to be robust to irrelevant featuresã€‚

 It's good in domains with many equally important featuresã€‚

 which is not true for some other machine learning algorithms like decision treesã€‚And in factã€‚

 knifeive Base is optimal in the rare case in the world where the independence assumptions actually holdã€‚

 So it's a good dependable baseline for text classificationã€‚Howeverï¼Œ we'll see other classifiersã€‚

 logistic agressionï¼Œ neural networks of various kinds that give better accuracy when there's enough training dataã€‚



![](img/e2b8bd3bdc19ea44215e602efe6086e4_7.png)

We've now seen further aspects of sentiment classification and other tasks that naive bays can be applied toã€‚

 like spam detection or language identificationã€‚

![](img/e2b8bd3bdc19ea44215e602efe6086e4_9.png)