# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P20ÔºöL 4.2- Êú¥Á¥†Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô® - ShowMeAI - BV1YA411w7ym

In this lectureÔºå we describe the naive Bayes classifier„ÄÇ

 a basic text classifier that will allow us to introduce many of the issues that arise in text classification„ÄÇ



![](img/bb23eb6ed09dbe6b6b9a6c274c5e3d55_1.png)

Nive Bayes is a simple classification method based on Bay's ruleÔºå which we'll introduce in a bit„ÄÇ

 and it relies on a very simple representation of the document called the bag of words„ÄÇ

Let's look at the bag of words„ÄÇHere's a sample movie review„ÄÇ I love this movie„ÄÇ It's sweet„ÄÇ

 but with satirical humor„ÄÇ In the bag of words representation„ÄÇ

 we imagine all the words from this review thrown into a big paper bag and mixed up together„ÄÇ

 So we don't know what order the words occurred in„ÄÇ So the word I„ÄÇ

 There's the word I and there it is in the bag„ÄÇ and the word love is here„ÄÇ

 and there's love in the bag„ÄÇ And there's this„ÄÇ But you see the bag doesn't represent the ordering of the words„ÄÇ

 And we can think of this bag of words as really just being a list of words„ÄÇ

 And the counts how many times the word occurs„ÄÇ The word it occurs  six times in this bag„ÄÇ

 Here's oneÔºå2Ôºå3Ôºå4 and so onÔºå5Ôºå6„ÄÇ The word I occurs five times the word the four times and so on down to words like humor or have or great„ÄÇ

 which all occur one time„ÄÇüòä„ÄÇ

![](img/bb23eb6ed09dbe6b6b9a6c274c5e3d55_3.png)

The job of the classification functionÔºå we can think of a function gamma that takes this list of words and counts and makes a classification decision a thumbs up or thumbs down„ÄÇ

 If we're doing binary classification„ÄÇ Or if we have more classes„ÄÇ

 we can map it to one of more classes„ÄÇ So our job is to map from this list of words and counts to some class„ÄÇ

üòä„ÄÇ

![](img/bb23eb6ed09dbe6b6b9a6c274c5e3d55_5.png)

In classificationÔºå we have a document D and a class C„ÄÇ

 and our goal is to compute the probability of for each class of its conditional probability given a document„ÄÇ

And we're going to see that we're going to use this probability to pick the best class„ÄÇ

Now how do we compute the probability of a class given a document by Bay's rule„ÄÇ

 this is equal to the probability of the document given the class times the probability of the class over the probability of the document„ÄÇ

Let's see how to use that in the classifier„ÄÇThe best classÔºå the maximum a postii class„ÄÇ

 the class that we're looking for to assign this document to„ÄÇIs„ÄÇOut of all classes„ÄÇ

The one that maximizes the probability of that class given the document„ÄÇ

So we're looking for the class whose probability given the document is greatest„ÄÇBy Bay's rule„ÄÇ

That's the sameÔºå whichever class maximizes probability of C given DÔºå also maximizes this equation„ÄÇ

 the probability of D given CÔºå probability of the class over the probability of the document„ÄÇ

And as is traditional in Bayesian classification„ÄÇWhichever documentÔºå excuse me„ÄÇ

 whichever class maximizes this equation also maximizes this equation„ÄÇ

And what we've done here is we've dropped the denominatorÔºå crossed out the denominator„ÄÇ

 Why is it okayÔºüto cross out the denominator d„ÄÇProbability of D is how likely the document is„ÄÇ Now„ÄÇ

 if I give you a document„ÄÇ and I say which of these 10 classes does this document belong to„ÄÇ

 And for each of these classesÔºå I'm computing the probability of the document given the class„ÄÇ

 the probability of the class and the probability of the document„ÄÇ

 the probability of the document is identical for all 10 classes for each classÔºå one more time„ÄÇ

 I have to compute the probability of the documentÔºå And that means that if I'm comparing 10 things„ÄÇ

 each of which is divided by probability of the document„ÄÇ

 the probability of the document is a constant„ÄÇ And I can eliminate that„ÄÇ

So the most likely class C map is that class which maximizes the product of two probabilities„ÄÇ

 the probability of the document given the classÔºå we'll call it the likelihood„ÄÇ

And the probability of the class we'll call that the prior„ÄÇThe prior probability of the class„ÄÇ

So the most likely class is the one that maximizes the product of these two probabilities„ÄÇ

The probability of the class will turn out to be relatively simple to compute what do I mean by the probability of a document given the class„ÄÇ

 what do I mean to say this particular movie review was how likely is it it given the class positive„ÄÇ

 It seems like a very complicated and confusing thing to compute„ÄÇ

And one way to operationalize that is to sayÔºå let's represent the document by a whole set of features„ÄÇ

 x1 through Xn„ÄÇ So when I say the probability of a document given a class„ÄÇ

 I'm going to say that all that means is the probability of a vector of features given the class P of D given C„ÄÇ

 we're going to represent that probability by the joint probability of x1 x2 up through XN given the class„ÄÇ

In other words„ÄÇWere this document D as a set of features X1 through Xn that still doesn't tell me how to compute this probability„ÄÇ

 but it's a start„ÄÇSo let's talk about these two pieces now„ÄÇHow do I compute probability of a class„ÄÇ

WellÔºå really that's just askingÔºå how often does this class occur„ÄÇ

 are positive reviews much more common than negative reviewsÔºüIs Madison a much more frequent authorÔºü

So to decide to this„ÄÇComputing the probability of a class can be done just by counting relative frequencies in some corpus or data set„ÄÇ

So the probability of a class is relatively easy to compute„ÄÇ

What about the likelihood of the document of these features in a document given the classÔºüWell„ÄÇ

 there's a lot of parameters for this probability„ÄÇThere's if there's n different features„ÄÇ

And each of them has a certain lengthÔºå that's a lot of parameters that have to be computed and we have to compute them one for each class„ÄÇ

 so that's far too many parameters that we could possibly compute we can only estimate this number if we had a huge number of training examples and we usually don't have such an enormous amount of training examples„ÄÇ

So we're going to make some simplifying assumptions in the IEBase classifier to make this computation more possible„ÄÇ



![](img/bb23eb6ed09dbe6b6b9a6c274c5e3d55_7.png)

The first simplifying assumption we're going to make is called the bag of words assumption„ÄÇ

And we're going to assume that the position in the document doesn't matter„ÄÇ

 so this is what I gave you the intuition of a few slides ago„ÄÇ

The position of the word in the documentÔºå whether it's the first word or the seventh word or the 150th word isn't going to matter„ÄÇ

 all we care about is which word or which feature occurs„ÄÇ

And the thing we going the second assumption we're going to make„ÄÇ

Is we're going to assume that the different featuresÔºå x1Ôºå x2Ôºå x3„ÄÇ

 that their probabilities are independent given the class„ÄÇ

So that the the whether one feature occurs given a class and whether another feature occurs given a class are independently going to be true„ÄÇ

 and of course this is a„ÄÇBoth of these assumptions are incorrect simplifying assumptions„ÄÇ

 they're absolutely wrongÔºå they're terribly completely not true„ÄÇ

 nonetheless by making these simplifying these incorrect simplifying assumptions„ÄÇ

 we can make our problem so much simpler that in practice we're able to solve the problem with a high degree of accuracy despite the simplifications„ÄÇ

So the result of these two simplifying assumptions is we're going to represent the probability„ÄÇ

 the joint probability of a whole set of features x1 through x1 conditioned on a class„ÄÇ

As the product of a whole bunch of independent probabilitiesÔºå probability of x1 given the class„ÄÇ

 probability of x2 given the classÔºå probability of x3 given the classÔºå and so on„ÄÇ

 up to probability of xn given the classÔºå we're just going to multiply them altogether together„ÄÇ

 We're not going to care about x1 which position it occurred in all we care about is that it was this particular word or feature and we're not going to care about the dependencies between x1 and x2„ÄÇ

In other words„ÄÇIn order to computeÔºå our simplifying Naive Bay's assumption to compute the most likely class„ÄÇ

By multiplying a likelihoodÔºå the probability of a whole joint string of features times a prior probability of a class„ÄÇ

We're going to simplify that and say that the best class by the NB assumption is that class that maximizes these the prior probability of the class„ÄÇ

 so that's the sameÔºå but now more simply we're going to just going to multiply for every feature in the set of features the probability of that feature given the class„ÄÇ

Much simpler equation„ÄÇSo now looking specifically at text„ÄÇFirst„ÄÇ

 we're going to assume we're going to look at all positionsÔºå all word positions in a text document„ÄÇ

So we have a text document and it has 100 words in itÔºå so for all for position of word number one„ÄÇ

 position number twoÔºå position number three„ÄÇWe're going to take„ÄÇ

Look at all the classes and for each classÔºå we're going to say what's the probability of the class„ÄÇ

 and then for each classÔºå we're going to walk through every position in the text„ÄÇ

 And for each positionÔºå we're going to look at the word in that position and ask what's its probability given the class I'm looking at„ÄÇ

 So we'll do this for class1„ÄÇWe'll compute P of class1„ÄÇTimes the product over all the eyes„ÄÇ

A P of word I given class1„ÄÇSo we'll compute that and then we'll do the same thing for class2 for class two„ÄÇ

 we'll compute P of class 2„ÄÇAnd then the product over positions I of the P of I given class2„ÄÇ

And then we're going to pick whichever of these two is the highestÔºå if this is higher„ÄÇ

 we're going to pick class2 and assign it to the documentÔºå if this is higher„ÄÇ

 we'll assign class1 to the documentÔºå and of course I've shown you this with just two classes„ÄÇ

 but in general this is true f for any number of classes„ÄÇ

There's a problem with this algorithm for choosing the most likely class„ÄÇ

 And that is that this arg max requires that we multiply a lot of probabilities„ÄÇ

 We're going to multiply a probability for every position in the document„ÄÇ That's lots of words„ÄÇ

 and multiplying lots of small probabilities„ÄÇ Probabilities are all numbers between 0 and 1 can result in floating point underflow„ÄÇ

 We're multiplying together a lot of probabilities„ÄÇ

 and the result going to be an extremely small number„ÄÇOur solution to this is to use logs„ÄÇ

 because it turns out conveniently that the log of a times B is equal to the log of a plus the log of B„ÄÇ

 So in generalÔºå we're going to sum logs of probabilities instead of multiplying probabilities„ÄÇ

So instead of the equation we've seen where we choose the class by taking the Om over the prior times a product of a whole lot of likelihoods where instead„ÄÇ

Going to take the Ag max over the log of a probability plus the sum over the probabilities of each of the words in each of the positions„ÄÇ

NowÔºå taking the log importantlyÔºå doesn't change the ranking of the classes„ÄÇ

 The class with the highest probability also will have the highest log probability and doing everything as sums of logs means that we're creating a linear model„ÄÇ

 We're taking a max of a sum of weights„ÄÇ So it's a linear function of the input„ÄÇ

 and that makes it clear that naive Bayes is a linear classifier„ÄÇ

 Linear classifiers are an important family of simple classifiers„ÄÇ



![](img/bb23eb6ed09dbe6b6b9a6c274c5e3d55_9.png)

We've now seen the basic principles of Naive Be's text classification„ÄÇ



![](img/bb23eb6ed09dbe6b6b9a6c274c5e3d55_11.png)