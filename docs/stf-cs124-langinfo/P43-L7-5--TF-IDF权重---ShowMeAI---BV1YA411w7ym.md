# P43ï¼šL7.5- TF-IDFæƒé‡ - ShowMeAI - BV1YA411w7ym

We've now introduced two weights for terms and documents to use in our information retrievalã€‚

Ranking processï¼Œ term frequency and inverse document frequency in this segment we're going to put them together to give the TF IDF weight of termsã€‚



![](img/bd87294f6dc0be7904836792bbd394f8_1.png)

The TFIF weight of a term in a documentã€‚Right here is simply the product of its TF weight scaled with a log termã€‚

 as we discussed beforeï¼Œ times its inverse document frequency weightã€‚

This is the best known waiting scheme for terms and information retrievalã€‚

 There's been a lot of researchï¼Œ and there are many othersã€‚ But if you only know oneã€‚

 it's the one to knowã€‚ Note in particularï¼Œ one fine pointã€‚ So this littleã€‚

Dash or hyphen here in this TF IDF weighting that's a hyphen It's not a minus sign that we're taking a productã€‚

 so sometimes people indicate that more explicitly by using a dot or using a multiplication sign So what are the features of TF IDF weighting TF IDF weighting increases with the number of times the term occurs in the document so that the TFIf weight for a query term depends on the document it's not independent of the document and then the TF IDF weight for a term also goes up with the rarity of the term in the collection that's from the IDF weighting hereã€‚

ğŸ˜Šï¼ŒSo using this to the ranking of documents for a queryã€‚

 what we're doing to work out the score of the query in the document is we're taking the terms that appear in both the query and the documentã€‚

 the rest of them have no waitinging and then we're working out this TF IDF weight for each of those terms and then we're summing them to give the score of the document with respect to the queryã€‚

ğŸ˜Šï¼ŒSo what have we done hereï¼Œ what we've done is we've gradually moved from first binary vectors in the original model of doing balling information retrieval to count vectors which we used when we just had an unscaled term frequency to now we have weight vectors for a document and hence a weight matrix between terms and documents and that's now what we see hereã€‚

ğŸ˜Šï¼ŒSo each document is now being represented by a real valued vectorã€‚

 so for example the document Julius Caesar is being represented by this vectorã€‚

 so that for each document it's in the vector space of real valued numbers where the dimensionality is the number of different terms in our collection againã€‚

Okayï¼Œ and then when we have a bunch of these vectors reached document in our collectionã€‚

 we have a a term document matrix which is now a real valued matrixã€‚



![](img/bd87294f6dc0be7904836792bbd394f8_3.png)

We'll say a little bit more later about what some of the properties of this isã€‚

 but hopefully seeing this kind of term document matrix of real numbers there's enough to see how we can doã€‚

ğŸ˜Šï¼ŒA ranking of documents according to some queryï¼Œ according to these TFã€‚

IDF scores that we've assigned for each term and each documentã€‚So that's a TF IDDF weightingã€‚

 one of the most central concepts in information retrieval systemsã€‚



![](img/bd87294f6dc0be7904836792bbd394f8_5.png)