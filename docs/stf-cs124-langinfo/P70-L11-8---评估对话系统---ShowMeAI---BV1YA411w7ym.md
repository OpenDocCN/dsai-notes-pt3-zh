# P70ÔºöL11.8 - ËØÑ‰º∞ÂØπËØùÁ≥ªÁªü - ShowMeAI - BV1YA411w7ym

![](img/5f9ba3f233c67e7567855eebbdfe21dd_0.png)

How do we evaluate dialogue systemsÔºüHow can we tell if a dialogue system is fulfilling its goalÔºü

Task based dialogue is evaluated mainly by measuring whether they succeed at their task„ÄÇ

 like booking the right flight„ÄÇFor chatbotsÔºå often the goal is to be enjoyable to humans„ÄÇ

 so chatbots are evaluated mainly with humans„ÄÇLet's begin with chat box„ÄÇ

 we evaluate these by asking humans to assign a score„ÄÇThis can be the human who talk to the chatbot„ÄÇ

 which we call participant evaluationÔºå or a third party who reads a transcript of a human chatbot conversation„ÄÇ

 which we call observer evaluation„ÄÇIn the participant evaluation used by C at all„ÄÇ

 the human evaluator chats with the model for six turns and rates the chatbot using liquid scales on eight dimensions„ÄÇ

 capturing conversational qualityÔºå avoiding repetitionÔºå interestingnessÔºå making senseÔºå fluency„ÄÇ

 listeningÔºå inquisitivenessÔºå humanness and engagingness„ÄÇ

So here we've shown three of those dimensions and a li scales„ÄÇ

 how repetitive was this user repeated themselves over and over to always had something new and so on„ÄÇ



![](img/5f9ba3f233c67e7567855eebbdfe21dd_2.png)

Observer evaluations use third party annotators to look at the text of a complete conversation„ÄÇ

Sometimes we're interested in having Rars assign a score to each system turn„ÄÇ

 so we could ask Raers to mark how coherent each turn is„ÄÇOftenÔºå however„ÄÇ

 we just want a single high level score to know if system A is better than system B„ÄÇ

The acute avalmetric is such an observer evaluation in which annotators look at two separate human computer conversations and choose the one in which the dialogue system performed better„ÄÇ

 answering questions about four propertiesÔºå engaging thisÔºå interestingness„ÄÇ

 humanness and knowledgeability„ÄÇ

![](img/5f9ba3f233c67e7567855eebbdfe21dd_4.png)

Here's an example of the acute Aval annotator task„ÄÇ

 comparing two dialogues and choosing between speakerea1 in light blue and speakerea 2 in dark blue„ÄÇ



![](img/5f9ba3f233c67e7567855eebbdfe21dd_6.png)

Automatic evaluations like the blue scores used to evaluate machine translationÔºå for example„ÄÇ

 are generally not used for chatbotsÔºå since blue scores correlate poorly with human judgments on chatbots„ÄÇ

Developing possible automatic evaluation metrics is an open research problem„ÄÇ

 One novel paradigm is called adversarial evaluationÔºå inspired by the Tring test„ÄÇ

 The idea is to train a Tring like evaluator classifier to distinguish between human generated responses and machine generated responses„ÄÇ

 The more successful a response generation system is at fooling the evaluatorÔºå the better the system„ÄÇ

üòäÔºåHow about evaluating task based dialogueÔºüIf the task is unambiguous„ÄÇ

 we can simply measure absolute task success„ÄÇDid the system book the right plane flight or put the right event on the calendarÔºü

A slightlylight more fine grainedÔºå we could measure the slot error rate„ÄÇ

 the percentage of slots filled with incorrect values„ÄÇ

 the number of mistaken slots over the number of total slots„ÄÇConsider a system given this sentence„ÄÇ

 make an appointment with Chris at 1030 in Gs 104Ôºå which extracts the following candidate slot structure„ÄÇ

 person ChrisÔºå time 1130Ôºå room gates 104„ÄÇHereÔºå the slot error rate is one third since the time is wrong„ÄÇ

 Instead of error rateÔºå slot precision recall and F score can also be used„ÄÇ

 and slot error rate is also sometimes called concept error rate„ÄÇ

We can also measure task success as well as the slide error rate„ÄÇ

To get a more fine grain idea of user happinessÔºå we can also compute user satisfaction readings„ÄÇ

 where we have users interact with a dialogue system to form a task and complete a questionnaire„ÄÇ

 Here's some sample questions„ÄÇ responsesses can be mapped to the same range and then averaged overall questions to get a total user satisfaction rating„ÄÇ

We can also measure other factors like efficiency via total elapsse time for the dialogue in seconds or the number of total terms or the number of system terms„ÄÇ

Quality cost measures other aspects of the interactions that affect users' perception of the system„ÄÇ

 One such measure is the number of times the AR system fail to return any sentence or the number of AR rejection prompts„ÄÇ

Similar metrics include the number of times the user had to barge in and interrupt the system„ÄÇ



![](img/5f9ba3f233c67e7567855eebbdfe21dd_8.png)

We've seen standard ways to evaluate both chatbots and task based dialogue systems„ÄÇ



![](img/5f9ba3f233c67e7567855eebbdfe21dd_10.png)