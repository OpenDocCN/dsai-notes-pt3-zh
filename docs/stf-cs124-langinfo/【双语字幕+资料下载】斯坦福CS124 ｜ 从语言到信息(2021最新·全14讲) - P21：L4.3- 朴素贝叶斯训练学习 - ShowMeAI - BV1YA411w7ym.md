# P21ÔºöL4.3- Êú¥Á¥†Ë¥ùÂè∂ÊñØËÆ≠ÁªÉÂ≠¶‰π† - ShowMeAI - BV1YA411w7ym

![](img/bd8aad9b5382ecea18d9261abdcd47f1_0.png)

How do we learn the parameters for naive BsÔºå The simplest way to learn the multinomial naive Bay model is to use maximum likelihood estimates„ÄÇ

 Simply use the frequencies in the data„ÄÇ So if we're trying to compute the prior probability of a particular document being in a class class J„ÄÇ

 we have the count of all the documents and out of thoseÔºå how many of the documents are in class J„ÄÇ

 That's our prior that a random document will be in class J for the likelihood„ÄÇ

 the probability of word sub IÔºå given class sub J„ÄÇ we count the number of times word subi occurs in documents of class J„ÄÇ

And we normalize by the total number of words in class J„ÄÇ

 so the sum over all the words in the vocabulary of the count of those words in documents of class J„ÄÇ

 so out of all the words in documents of class JÔºå how many of them are our particular word words a buy we're looking atÔºü



![](img/bd8aad9b5382ecea18d9261abdcd47f1_2.png)

So we're going to compute the fraction of times a word WY appears among all the words in the document of this particular topic C sub J„ÄÇ

 we're going to do this by creating a kind of mega document for topic J by concatenating all the documents that have that topic together„ÄÇ

 and then we're going to use the frequency of W in this mega document„ÄÇ

So for sentiment analysis we might have a document sub positive„ÄÇ

 a mega document for all the positive documentsÔºå so we're just going to concatenate them all together into some big mega document and we might have a document subNG and so on„ÄÇ



![](img/bd8aad9b5382ecea18d9261abdcd47f1_4.png)

NowÔºå I've been lying to you„ÄÇ We don't in factÔºå use maximum likelihood for naive B„ÄÇ

 And the reason is the following„ÄÇ Imagine we're looking at the word fantastic„ÄÇ

 we're interested in the word fantastic might occur in the test set„ÄÇ

 but it happens not to have appeared in the training set in the topic positiveÔºå let's say„ÄÇ

 So the probability of fantasticÔºå given the class positive in our training set„ÄÇ

 by maximum likelihood is the count of fantastic occurring in positive„ÄÇ

 normalized by the sum overall words of the count of those words in positive„ÄÇ

 but fantastic never occurs„ÄÇ So that count is0„ÄÇ So the maximum likelihood estimate for the likelihood of fantastic given positive will be0„ÄÇ

 Why is that badÔºå Because these zero probabilities can never be conditioned away„ÄÇ

 we're looking for the most likely class„ÄÇ that's the arg max over all classes of the prior times the likelihood„ÄÇ

 And if one of those likelihood terms is0Ôºå Then this whole thing is0 and we're never going to pick that class„ÄÇ

üòä„ÄÇ

![](img/bd8aad9b5382ecea18d9261abdcd47f1_6.png)

The solution is very simple„ÄÇ Add one smoothing„ÄÇ So here's the computation without smoothing„ÄÇ

 and add one smoothing„ÄÇ we simply add one to each of those counts„ÄÇ

 So we'll add one to the count when it's the numerator„ÄÇ

 and every time that count occurs in the denominatorÔºå we' added one to that2„ÄÇ

And we can rewrite that in a form we've seen before where we're taking the total number of tokens that were in class C„ÄÇ

 and we've added the vocabulary size because we added one for every vocabulary item into the denominator„ÄÇ

 so classic Laplace or add one smoothing„ÄÇSo let's walk through the calculation of these parameters first from the training corpus„ÄÇ

 we're going to extract the vocabulary the list of words„ÄÇ

Next we're going to calculate the priors for each class C sub J„ÄÇ

 so for each class we're going to get list the set of all documents that have that class„ÄÇ

 we'll call that set doc sub J„ÄÇAnd the number of documents in that set„ÄÇ

Divided by the total number of documents that will give us our prior probability of that particular class„ÄÇ

Now for the likelihood we're going to want to compute the likelihood for every word W subK given every topic C subJ„ÄÇ

 So first we're going to create our mega document by concatenating all the documents subj into a mega document called text subj and now for each word W subK in our vocabulary we're going to count the number of times W subK occurs in this mega document text subj so that will be n subK„ÄÇ

And now the probability the likelihood of word subK given class subj is just the add1 smooth„ÄÇ

 or I've shown you the add alpha smooth version of the9b algorithm„ÄÇ

 so we've added alpha to n subK and then in denominator we have n the total number of tokens in the class J„ÄÇ



![](img/bd8aad9b5382ecea18d9261abdcd47f1_8.png)

What about unknown words that appear in our test dataÔºå but not in our training data or vocabulary„ÄÇ

We ignore them„ÄÇ That isÔºå we just remove them from the test documentÔºå pretendt they weren't there„ÄÇ

 We don't include any probability for them at all„ÄÇWhy don't we build an unknown word modelÔºüWell„ÄÇ

 it turns out it doesn't help„ÄÇ If you think about itÔºå if I had a probability for the unknown word„ÄÇ

 that was some kind of unknown word probability„ÄÇAnd I class A had five unknown words and class B hadn't seen seven of the words„ÄÇ

 WellÔºå we're just really adding in this probability five times for class A and seven times for class B and knowing which class has more unknown words is not generally helpful in choosing the right class„ÄÇ



![](img/bd8aad9b5382ecea18d9261abdcd47f1_10.png)

Some systems ignore stop words„ÄÇ Stop words are very frequent words like the or The way we find stop words is we sort the vocabulary by the word frequency in the training set„ÄÇ

 and we take the top K wordsÔºå the top 50 or the top 100 or the top 10„ÄÇ

 we call those words the stop word list„ÄÇ AndÔºå you knowÔºå because words like the are very frequent„ÄÇ

 they're going to appear in that stop word list„ÄÇ and then we just remove all the words on the stop word list from both the training and test sets„ÄÇ

 as if they never occurred„ÄÇAlthough removing stop wordss helps in some N LP algorithms„ÄÇ

 it turns out is not usually helpful in naive Bays text classification„ÄÇ So in practice„ÄÇ

 most naive Bay's algorithms use all the words and don't use stop wordss lists„ÄÇ

 So learning the parameters for naive bays a simple computation for the prior and a slightly more complex computation for the likelihood in which we can use add one smoothing and we could deal with unknown words„ÄÇ



![](img/bd8aad9b5382ecea18d9261abdcd47f1_12.png)

![](img/bd8aad9b5382ecea18d9261abdcd47f1_13.png)