# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P48ï¼šL8.2- è¯­ä¹‰å‘é‡ - ShowMeAI - BV1YA411w7ym

![](img/fd9b27b7fd0e335f6c4d9644508a2751_0.png)

Here we introduce vector semanticsï¼Œ the standard way to represent meaning and language processingã€‚



![](img/fd9b27b7fd0e335f6c4d9644508a2751_2.png)

Vectctor semantics is the standard way to represent word meaning in NLPã€‚

 helping us model many of the aspects of word meaning we saw in the previous sectionã€‚

 The roots of the model lie in the 1950s when two big ideas convergedã€‚

The first draws on the philosopher Witgenstein's idea that the meaning of a word should be tied to how it is usedã€‚

Linguist like Harris and Fth had the related idea of defining the meaning of a word by its distribution in language useã€‚

 meaning its neighboring words or its neighboring grammatical environmentsã€‚

 Here's a quote from Zic Harrisã€‚ If A and B have almost identical environmentsã€‚

 meaning surrounding words or grammatical structuresï¼Œ we say that they are synonymsã€‚

Let's consider an exampleã€‚ Sose you don't know the meaning of the word Eg Choiã€‚

 a recent borrowing from Cantonese into Englishã€‚But you see it in the following contextã€‚

 Onng Choi is deliciousï¼Œ sauteed with garlicã€‚ It is superb over riceã€‚

 Onng Choi leaves have salty saucesã€‚ğŸ˜Šï¼ŒAnd you've also seen these very similar wordsã€‚

 garlic and rice and leaves with words like spinachã€‚ There's garlic or charrardã€‚

 There's leaves or collaredã€‚We might conclude that engchoi is a leafy greenã€‚

 like spinach ch or collaredã€‚ And we could conclude this based on the fact that words like leaves or garlic or rice or delicious occur both for Onchoi and for words we already know like spinach and chardã€‚

ğŸ˜Šï¼ŒIndeedï¼Œ Eg Choi is Ipomaa aquaica water spinach here's a picture for you food fansã€‚

So this first idea from the 50s is that we'll define the meaning of a word by its distribution in language useã€‚

 meaning its neighboring words or grammatical environmentsã€‚

And the second idea is Osgod's 1957 idea we mentioned in the prior lecture in which the connotation of a word is represented by three numbersã€‚

 its valenceï¼Œ arousal and dominanceã€‚So rememberï¼Œ a word like love might have a high valence or a word like mellow might have a low arousalã€‚

 but every word has some score on all three dimensionsã€‚

But if every word has a score in all three dimensionsã€‚

 that means we're essentially representing a word's connotationã€‚

By a point in three dimensional spaceã€‚ And if we can represent connotation as a point in spaceã€‚

 perhaps we can represent more about meaning as a point in spaceã€‚

So we're going to combine these two ideasï¼Œ defining meaning bi linguistic distributionã€‚

And representing meaning as a point in multidimensional spaceã€‚In vector semanticsã€‚

 we define meaning as a point in space based on distributionã€‚ So each word is a vectorã€‚

 rather than a string of letters like Gï¼Œ Oï¼Œ O D or an index like W sub 45ã€‚

 and similar words are near by in semantic space and cruciallyï¼Œ as we'll seeã€‚

 we build the space automaticallyï¼Œ by seeing which words are near by in textã€‚

Here is a visualization of embeddings learned for a sentiment analysis projectã€‚

Showing the locations of selected words projected down from high dimensional spaceã€‚ In this caseã€‚

 space of 60 dimensionsï¼Œ not just three down into a two dimensional space so we can look at itã€‚

And notice the distinct regions containing positive wordsï¼Œ negative wordsã€‚

 and neutral function wordsã€‚In summaryï¼Œ we define the meaning of a word as a vectorã€‚

 and these vectors are commonly called embeddings for historical reasonsã€‚

 involving them being embedded into a spaceã€‚ And you can see the textbook for further details about embeddingsã€‚

 And these embeddings are the standard way to represent meaning an NLPã€‚

 Every modern NLP algorithm uses some kind of embeddings to represent word meaningã€‚

And these embeddings allow us to have a fine grain model of meaning that's especially good for questions involving similarityã€‚

Why is it helpful to move from strings of letters or indices to vectors to represent word meaningï¼Ÿ

Consider the task of sentiment analysisã€‚So let's suppose we're doing sentiment classification using wordsã€‚

 so we have a feature that's the identity of a wordã€‚

 so we might have a feature like the previous word was T ERR IBLEã€‚

So that feature fires if we see the exact same word in the training set and the test setã€‚

 and otherwise doesn't fireã€‚With embeddingsï¼Œ by contrastï¼Œ the feature is a vectorã€‚

 So we might represent the feature as the previous word was vector 35ï¼Œ22ï¼Œ17 and so onã€‚

 So now in the test setï¼Œ we might see a word like horribleï¼Œ not the same word is terribleã€‚

 but perhaps it as a similar vectorï¼Œ and our classifiers can generalize to similar but unseen wordsã€‚

In the next lecturesï¼Œ we'll discuss two broad families of embeddingsã€‚

Will'll introduce T F I DF embeddingsã€‚ T F I DFï¼Œ the workhorse for information retrievalã€‚

 but also a common baseline model for embeddingsã€‚ TF I DF vectors are sparseã€‚

 meaning theyre very long vectorsï¼Œ most of whose values are 0ã€‚

And the values in a TF IDF vector are some simple function of just counts of nearby wordsã€‚

We'll also introduce Word to Vecã€‚The simplest kind of dense vector model in dense vector modelsã€‚

 most of the values are non0ï¼Œ and these vectors will be much shorter vectors than T F I D vectorsã€‚

 Word to V representations are created by training a classifier to predict whether a word is likely to appear nearby or notã€‚

Later onï¼Œ we'll discuss even richer kinds of embeddings called contextual embeddingsã€‚So from now onã€‚

 as we're representing words for semantic or meaning related tasksã€‚

 we're going to try to compute using meaning representations instead of string representationsã€‚

And I'll leave you with this inspiring quote from the Chinese philosopher Zhuangzaã€‚ Nes are for fishã€‚

 Once you get the fishï¼Œ you can forget the netã€‚ Word are for meaningã€‚ Once you get the meaningã€‚

 you can forget the wordsã€‚ğŸ˜Šã€‚

![](img/fd9b27b7fd0e335f6c4d9644508a2751_4.png)

We've now seen the intuition for vectorctor semanticsï¼Œ and we'll see the details in further lecturesã€‚



![](img/fd9b27b7fd0e335f6c4d9644508a2751_6.png)