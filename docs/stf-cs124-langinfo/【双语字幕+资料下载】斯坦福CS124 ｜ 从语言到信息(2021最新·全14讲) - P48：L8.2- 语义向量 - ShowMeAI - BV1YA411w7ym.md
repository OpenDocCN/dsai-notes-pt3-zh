# P48ÔºöL8.2- ËØ≠‰πâÂêëÈáè - ShowMeAI - BV1YA411w7ym

![](img/fd9b27b7fd0e335f6c4d9644508a2751_0.png)

Here we introduce vector semanticsÔºå the standard way to represent meaning and language processing„ÄÇ



![](img/fd9b27b7fd0e335f6c4d9644508a2751_2.png)

Vectctor semantics is the standard way to represent word meaning in NLP„ÄÇ

 helping us model many of the aspects of word meaning we saw in the previous section„ÄÇ

 The roots of the model lie in the 1950s when two big ideas converged„ÄÇ

The first draws on the philosopher Witgenstein's idea that the meaning of a word should be tied to how it is used„ÄÇ

Linguist like Harris and Fth had the related idea of defining the meaning of a word by its distribution in language use„ÄÇ

 meaning its neighboring words or its neighboring grammatical environments„ÄÇ

 Here's a quote from Zic Harris„ÄÇ If A and B have almost identical environments„ÄÇ

 meaning surrounding words or grammatical structuresÔºå we say that they are synonyms„ÄÇ

Let's consider an example„ÄÇ Sose you don't know the meaning of the word Eg Choi„ÄÇ

 a recent borrowing from Cantonese into English„ÄÇBut you see it in the following context„ÄÇ

 Onng Choi is deliciousÔºå sauteed with garlic„ÄÇ It is superb over rice„ÄÇ

 Onng Choi leaves have salty sauces„ÄÇüòäÔºåAnd you've also seen these very similar words„ÄÇ

 garlic and rice and leaves with words like spinach„ÄÇ There's garlic or charrard„ÄÇ

 There's leaves or collared„ÄÇWe might conclude that engchoi is a leafy green„ÄÇ

 like spinach ch or collared„ÄÇ And we could conclude this based on the fact that words like leaves or garlic or rice or delicious occur both for Onchoi and for words we already know like spinach and chard„ÄÇ

üòäÔºåIndeedÔºå Eg Choi is Ipomaa aquaica water spinach here's a picture for you food fans„ÄÇ

So this first idea from the 50s is that we'll define the meaning of a word by its distribution in language use„ÄÇ

 meaning its neighboring words or grammatical environments„ÄÇ

And the second idea is Osgod's 1957 idea we mentioned in the prior lecture in which the connotation of a word is represented by three numbers„ÄÇ

 its valenceÔºå arousal and dominance„ÄÇSo rememberÔºå a word like love might have a high valence or a word like mellow might have a low arousal„ÄÇ

 but every word has some score on all three dimensions„ÄÇ

But if every word has a score in all three dimensions„ÄÇ

 that means we're essentially representing a word's connotation„ÄÇ

By a point in three dimensional space„ÄÇ And if we can represent connotation as a point in space„ÄÇ

 perhaps we can represent more about meaning as a point in space„ÄÇ

So we're going to combine these two ideasÔºå defining meaning bi linguistic distribution„ÄÇ

And representing meaning as a point in multidimensional space„ÄÇIn vector semantics„ÄÇ

 we define meaning as a point in space based on distribution„ÄÇ So each word is a vector„ÄÇ

 rather than a string of letters like GÔºå OÔºå O D or an index like W sub 45„ÄÇ

 and similar words are near by in semantic space and cruciallyÔºå as we'll see„ÄÇ

 we build the space automaticallyÔºå by seeing which words are near by in text„ÄÇ

Here is a visualization of embeddings learned for a sentiment analysis project„ÄÇ

Showing the locations of selected words projected down from high dimensional space„ÄÇ In this case„ÄÇ

 space of 60 dimensionsÔºå not just three down into a two dimensional space so we can look at it„ÄÇ

And notice the distinct regions containing positive wordsÔºå negative words„ÄÇ

 and neutral function words„ÄÇIn summaryÔºå we define the meaning of a word as a vector„ÄÇ

 and these vectors are commonly called embeddings for historical reasons„ÄÇ

 involving them being embedded into a space„ÄÇ And you can see the textbook for further details about embeddings„ÄÇ

 And these embeddings are the standard way to represent meaning an NLP„ÄÇ

 Every modern NLP algorithm uses some kind of embeddings to represent word meaning„ÄÇ

And these embeddings allow us to have a fine grain model of meaning that's especially good for questions involving similarity„ÄÇ

Why is it helpful to move from strings of letters or indices to vectors to represent word meaningÔºü

Consider the task of sentiment analysis„ÄÇSo let's suppose we're doing sentiment classification using words„ÄÇ

 so we have a feature that's the identity of a word„ÄÇ

 so we might have a feature like the previous word was T ERR IBLE„ÄÇ

So that feature fires if we see the exact same word in the training set and the test set„ÄÇ

 and otherwise doesn't fire„ÄÇWith embeddingsÔºå by contrastÔºå the feature is a vector„ÄÇ

 So we might represent the feature as the previous word was vector 35Ôºå22Ôºå17 and so on„ÄÇ

 So now in the test setÔºå we might see a word like horribleÔºå not the same word is terrible„ÄÇ

 but perhaps it as a similar vectorÔºå and our classifiers can generalize to similar but unseen words„ÄÇ

In the next lecturesÔºå we'll discuss two broad families of embeddings„ÄÇ

Will'll introduce T F I DF embeddings„ÄÇ T F I DFÔºå the workhorse for information retrieval„ÄÇ

 but also a common baseline model for embeddings„ÄÇ TF I DF vectors are sparse„ÄÇ

 meaning theyre very long vectorsÔºå most of whose values are 0„ÄÇ

And the values in a TF IDF vector are some simple function of just counts of nearby words„ÄÇ

We'll also introduce Word to Vec„ÄÇThe simplest kind of dense vector model in dense vector models„ÄÇ

 most of the values are non0Ôºå and these vectors will be much shorter vectors than T F I D vectors„ÄÇ

 Word to V representations are created by training a classifier to predict whether a word is likely to appear nearby or not„ÄÇ

Later onÔºå we'll discuss even richer kinds of embeddings called contextual embeddings„ÄÇSo from now on„ÄÇ

 as we're representing words for semantic or meaning related tasks„ÄÇ

 we're going to try to compute using meaning representations instead of string representations„ÄÇ

And I'll leave you with this inspiring quote from the Chinese philosopher Zhuangza„ÄÇ Nes are for fish„ÄÇ

 Once you get the fishÔºå you can forget the net„ÄÇ Word are for meaning„ÄÇ Once you get the meaning„ÄÇ

 you can forget the words„ÄÇüòä„ÄÇ

![](img/fd9b27b7fd0e335f6c4d9644508a2751_4.png)

We've now seen the intuition for vectorctor semanticsÔºå and we'll see the details in further lectures„ÄÇ



![](img/fd9b27b7fd0e335f6c4d9644508a2751_6.png)