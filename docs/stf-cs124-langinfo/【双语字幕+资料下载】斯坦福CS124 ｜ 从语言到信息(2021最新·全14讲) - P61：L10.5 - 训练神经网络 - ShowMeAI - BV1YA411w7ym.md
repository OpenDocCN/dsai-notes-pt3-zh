# P61ÔºöL10.5 - ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªú - ShowMeAI - BV1YA411w7ym

![](img/01928d21ce892a52b6799a7c24acbbb5_0.png)

How do we train neural networksÔºå Let's start with an overview„ÄÇ

The intuition of neural net training is the forward computation of the loss and the backward computation of the weight updates„ÄÇ

So given an input X„ÄÇWe run a forward pass through the networkÔºå computing the system outputÔºå Y hat„ÄÇ

Then we compare Y hat to the true answerÔºå Y to get a loss for the example„ÄÇ

 and then we are going to do a backward path through the network„ÄÇ

 computing the gradients we need to update the weights„ÄÇ

Let's see that same thing in a slightly more formal way for every training tuuppleÔºå input X„ÄÇ

 correct answer y„ÄÇWe're going to run our forward computation to find what our network thinks Y hat is„ÄÇ

 our estimate of Y hat„ÄÇ and then we're going to run a backward computation to update weights„ÄÇ

 And we're thinking here about a two layer network„ÄÇ So first for every output node„ÄÇ

 we're going to compute the loss between the true y and the estimated y and we're going to take all the weights from the hidden layer to the output layer and update those weights from that loss„ÄÇ

And nextÔºå we're going to go to the hidden nodes„ÄÇAnd we're going to find a way to assess how much blame that node deserves for the correct answer and then for every weight from the input layer to the hidden layer„ÄÇ

 we're going to update that weightÔºå so we'll see how to do this in a more formal way„ÄÇ

Let's next get an intuition from what we did for logistic regression„ÄÇ Remember„ÄÇ

 our loss function for binary logistic regression„ÄÇ We want to learn weights that maximize the log probability of the correct label„ÄÇ

 So P of y given x„ÄÇüòäÔºåAnd remember to turn this into a loss function because we're minimizing„ÄÇ

 we want to flip the sign„ÄÇAnd now we can plug in our estimates of the probability that are computed by the sigmoid of the weights„ÄÇ

And let's recall from the lectureron logistic agression how gradient descent works for weight updates„ÄÇ

The magnitude of the amount to move in gradient descent„ÄÇ

Is the value of the gradient of the loss function with respect to the weights weighted by a learning rate Eta„ÄÇ

 So we have our old weight W super T and to compute our new weight„ÄÇ

 we move the old weight by the gradient of the loss weighted by the learning rate„ÄÇ

 higher learning rate means we should move the W more on each step„ÄÇFor logistic regression„ÄÇ

 we saw that this derivative of the loss function with respect to one weight w subj is sigma of w dot x plus b minus y„ÄÇ

 so our y hat minus RY times x subj„ÄÇWhere did that derivative come fromÔºå using the chain rule„ÄÇ

 RememberÔºå the chain rule says if we have a composite functionÔºå F of x equals u of v of x„ÄÇ

 the derivative of f of x is the derivative of u of x with respect to V times the derivative of V of x with respect to x„ÄÇ

And you can see the text in chapter 5 for the details„ÄÇ

But the intuition we can see from this neural unitÔºå which is the same as logistic regression„ÄÇ

 it's computing the loss as a function of yÔºå which is computed as the sigmoid of the sum the sum of the weights times the values„ÄÇ

So if we want to do the derivative of the entire loss function with respect to one weight W sub I„ÄÇ

 we can compute this using the chain rule„ÄÇThe derivative of L with respect to W sub I is the product of the derivative of L with respect to Y„ÄÇ

 the derivative of the loss„ÄÇTimes the derivative of y with respect to Z„ÄÇ

 the derivative of the activationÔºå times the derivative of Z with respect to W sub I„ÄÇ

 the derivative of that weighted sum„ÄÇThe derivatives we used on the prior slide for logistic agress only give the updates for that last weight layer„ÄÇ

Logistic regression only has one weight layerÔºå but what about deeper networks where we have lots of layers and furthermore we have different activation functions„ÄÇ

 not just sigmoids„ÄÇWe'll see the solution in the next lecture and we'll have even more use of the chain rule and we'll introduce the big idea of computation graphs and backward differentiation„ÄÇ



![](img/01928d21ce892a52b6799a7c24acbbb5_2.png)

We've seen an overview of neuralNe training in the next lecture we'll see the details„ÄÇ



![](img/01928d21ce892a52b6799a7c24acbbb5_4.png)