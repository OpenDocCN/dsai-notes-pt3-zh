# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P61ï¼šL10.5 - è®­ç»ƒç¥ç»ç½‘ç»œ - ShowMeAI - BV1YA411w7ym

![](img/01928d21ce892a52b6799a7c24acbbb5_0.png)

How do we train neural networksï¼Œ Let's start with an overviewã€‚

The intuition of neural net training is the forward computation of the loss and the backward computation of the weight updatesã€‚

So given an input Xã€‚We run a forward pass through the networkï¼Œ computing the system outputï¼Œ Y hatã€‚

Then we compare Y hat to the true answerï¼Œ Y to get a loss for the exampleã€‚

 and then we are going to do a backward path through the networkã€‚

 computing the gradients we need to update the weightsã€‚

Let's see that same thing in a slightly more formal way for every training tuuppleï¼Œ input Xã€‚

 correct answer yã€‚We're going to run our forward computation to find what our network thinks Y hat isã€‚

 our estimate of Y hatã€‚ and then we're going to run a backward computation to update weightsã€‚

 And we're thinking here about a two layer networkã€‚ So first for every output nodeã€‚

 we're going to compute the loss between the true y and the estimated y and we're going to take all the weights from the hidden layer to the output layer and update those weights from that lossã€‚

And nextï¼Œ we're going to go to the hidden nodesã€‚And we're going to find a way to assess how much blame that node deserves for the correct answer and then for every weight from the input layer to the hidden layerã€‚

 we're going to update that weightï¼Œ so we'll see how to do this in a more formal wayã€‚

Let's next get an intuition from what we did for logistic regressionã€‚ Rememberã€‚

 our loss function for binary logistic regressionã€‚ We want to learn weights that maximize the log probability of the correct labelã€‚

 So P of y given xã€‚ğŸ˜Šï¼ŒAnd remember to turn this into a loss function because we're minimizingã€‚

 we want to flip the signã€‚And now we can plug in our estimates of the probability that are computed by the sigmoid of the weightsã€‚

And let's recall from the lectureron logistic agression how gradient descent works for weight updatesã€‚

The magnitude of the amount to move in gradient descentã€‚

Is the value of the gradient of the loss function with respect to the weights weighted by a learning rate Etaã€‚

 So we have our old weight W super T and to compute our new weightã€‚

 we move the old weight by the gradient of the loss weighted by the learning rateã€‚

 higher learning rate means we should move the W more on each stepã€‚For logistic regressionã€‚

 we saw that this derivative of the loss function with respect to one weight w subj is sigma of w dot x plus b minus yã€‚

 so our y hat minus RY times x subjã€‚Where did that derivative come fromï¼Œ using the chain ruleã€‚

 Rememberï¼Œ the chain rule says if we have a composite functionï¼Œ F of x equals u of v of xã€‚

 the derivative of f of x is the derivative of u of x with respect to V times the derivative of V of x with respect to xã€‚

And you can see the text in chapter 5 for the detailsã€‚

But the intuition we can see from this neural unitï¼Œ which is the same as logistic regressionã€‚

 it's computing the loss as a function of yï¼Œ which is computed as the sigmoid of the sum the sum of the weights times the valuesã€‚

So if we want to do the derivative of the entire loss function with respect to one weight W sub Iã€‚

 we can compute this using the chain ruleã€‚The derivative of L with respect to W sub I is the product of the derivative of L with respect to Yã€‚

 the derivative of the lossã€‚Times the derivative of y with respect to Zã€‚

 the derivative of the activationï¼Œ times the derivative of Z with respect to W sub Iã€‚

 the derivative of that weighted sumã€‚The derivatives we used on the prior slide for logistic agress only give the updates for that last weight layerã€‚

Logistic regression only has one weight layerï¼Œ but what about deeper networks where we have lots of layers and furthermore we have different activation functionsã€‚

 not just sigmoidsã€‚We'll see the solution in the next lecture and we'll have even more use of the chain rule and we'll introduce the big idea of computation graphs and backward differentiationã€‚



![](img/01928d21ce892a52b6799a7c24acbbb5_2.png)

We've seen an overview of neuralNe training in the next lecture we'll see the detailsã€‚



![](img/01928d21ce892a52b6799a7c24acbbb5_4.png)