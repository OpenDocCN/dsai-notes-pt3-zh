# P30ÔºöL5.4- ‰∫§ÂèâÁÜµÊçüÂ§± - ShowMeAI - BV1YA411w7ym

![](img/544e39cd2e591088c9093592282e58ed_0.png)

Let's now turn to learning the parameters for logistic regressionggression„ÄÇ

 we'll start with the cross entropy loss function„ÄÇLogistic regression is an instance of supervised classification in which we know the correct label Y„ÄÇ

 either 0 or 1 for each observation X„ÄÇ But what the system produces is an estimate„ÄÇ

 We'll call it Y hat„ÄÇ What we want to do is learn parameters meaning W And B that make Y hat for each training observation as close as possible to the true y„ÄÇ

So we'll need two things for thisÔºå we'll need a distance estimator„ÄÇ

 a loss function or a cost functionÔºå and we'll need an optimization algorithm to update W andB to minimize this loss function„ÄÇ

So that's two components„ÄÇ A metric for how close the current label Y hat is to the true gold label Y„ÄÇ

 And rather than measure similarityÔºå we usually talk about the opposite of this„ÄÇ

 the distance between the system output and the gold output„ÄÇ

 And we call this distance the loss functionÔºå or sometimes the cost function„ÄÇ

We'll introduce the loss function that is commonly used for logistic regression and also for neural networks„ÄÇ

 that's the cross entropy loss„ÄÇAnd the second thing we need is an optimization algorithm for iteratively updating the weight so as to minimize this loss function„ÄÇ

 And the standard algorithm for that is gradient descent„ÄÇ

 and we'll introduce the stochastic gradient descent algorithm in the following lecture„ÄÇ

So we need a loss function that expresses for an observation X„ÄÇ How close the classifier output„ÄÇ

 That's Y hatÔºå which we get by running W X plus B through a sigmoid„ÄÇ

 How close that classifier output is to the true outputÔºå which is either 0 or 1„ÄÇ

 So how close is Y hat to y„ÄÇAnd we'll call the difference between these the loss L„ÄÇ

 how much Y hat differs from the true Y„ÄÇAnd we do this via loss function that prefers the correct class labels of the training examples to be more likely„ÄÇ

 This is called conditional maximum likelihood estimation„ÄÇ We chooseuse the parameters W and B„ÄÇ

 that maximize the log probability of the true Y labels in the training data Given the observation X„ÄÇ

 And this resulting loss function is the negative log likelihood loss„ÄÇ

 generally called the cross entropy loss„ÄÇ Let's derive this loss function applied to a single observation X„ÄÇ

We'd like to learn weights that maximize the probability of the correct label Y„ÄÇ

 So that's P of y given x„ÄÇ NowÔºå since there are only two discrete outcomes 0 or1„ÄÇ

 we can express the probabilityÔºå P of y given x from our classifier as the product of these two terms„ÄÇ

 Y hat to the Y and one minus Y hat to the1 minus y„ÄÇ NowÔºå what happens if we plug in the true values„ÄÇ

 y equals 1 or y equals 0 to this equation„ÄÇ If y equals1„ÄÇWe get„ÄÇWhy hat to the oneÔºü

One minus y hat to the zeroÔºå and this cancels out and we get just y hatÔºå that's if y equals1„ÄÇ

If the true y equals 0Ôºå we get y hat to the 0 times 1 minus y hat to the one„ÄÇ

 And now this one cancels outÔºå in other words„ÄÇIf y equals1Ôºå this equation simplifies to y hat„ÄÇ

 If y equals 0Ôºå this simplifies to one minus y hat„ÄÇSo our goal is to maximize this probability„ÄÇ

 Our job is to learn parameters that will make the correct label Y the most likely have the highest probability„ÄÇ

 And for mathematical convenienceÔºå we'll take the log of both sides„ÄÇ

 So now we want to maximize the log probability of y given x„ÄÇ

 And so now we're maximizing y log Y hat plus1 minus y log1 minus y hat„ÄÇ

 We can do this because whatever values max the log of the probability will' also maximize the probability„ÄÇ

So here I've just put the same equation up hereÔºå our goal is to maximize log p of y given x„ÄÇ

And instead of maximizing a probabilityÔºå it's more common to talk about minimizing a loss„ÄÇ

 So we're going to turn this thing we're trying to maximize into something we're trying to minimize„ÄÇ

 And that will turn it into the cross entropy loss„ÄÇ So all we do is take this log P of y given x„ÄÇ

 andÔºå and we negate it„ÄÇ So the cross entropy lossÔºå we're trying to minimize the negative log probability of y given x„ÄÇ

 And as we've seen this can be estimated with this sum of terms„ÄÇ

 We can also plug in our definition of y hat to remind ourselves how we're going to compute this„ÄÇ

And the cross entropy lossÔºå againÔºå between y hat and y„ÄÇ

 is the negative of y log sigma of Wx plus B plus 1 minus y log of 1 minus sigma of Wx plus B„ÄÇ

Al rightÔºå let's see if this crossendpy loss works in our sentiment example„ÄÇIntuitively„ÄÇ

 we'd like the loss to be smaller if the model estimate is close to correct and bigger if the model' is confused„ÄÇ

 So let's look at both those cases and see if the estimate is doing the right thing„ÄÇ

So let's first take our sentiment example„ÄÇ It's Hokey„ÄÇ There are virtually no surprises„ÄÇ

 so why was it so enjoyableÔºå Let's suppose the true label of this example is one Y equals1„ÄÇ

 This is a positive review„ÄÇ let's sayÔºå And let's see if that works with our features„ÄÇ

 What probability does our model assign to the value Y equals 1„ÄÇ Well„ÄÇ

 we can compute that just from the sigmoid sigma of w X plus BÔºå which isÔºå again„ÄÇ

 we dot product our weights with our feature valuesÔºå our ws and our x's„ÄÇ

 and we add our our bias term BÔºå And we end up with a probability of 0„ÄÇ7„ÄÇ So pretty good„ÄÇüòä„ÄÇ

What's the lossÔºüWell we can just plug in that 0„ÄÇ7„ÄÇInto our equation for cross entropy loss„ÄÇ

 and we see a loss of 0„ÄÇ36Ôºå a relatively small loss„ÄÇBy contrast„ÄÇ

 let's pretend instead that the example was actually negative„ÄÇThat is„ÄÇ

 the true Y was 0 instead of one„ÄÇ So perhaps the reviewer saidÔºå bottom lineÔºå this movie's terrible„ÄÇ

 I beg you not to see it„ÄÇSo in this caseÔºå our model„ÄÇ

 which assigned a pretty high probability to this being a positive exampleÔºå is confused„ÄÇ

 The models wrong„ÄÇ So we'd hope that the loss will be higher„ÄÇSo let's plug in y equals zero„ÄÇ

So what's the probability of y equals 0„ÄÇ And that probability is one minus the probability of y equals 1„ÄÇ

 which we got from sigma of W X plus B„ÄÇ And that's 03„ÄÇ So the model doesn't think that the„ÄÇ

That the answer is zeroÔºå doesn't think this is a negative review„ÄÇ

 signs it a small probability of only 0„ÄÇ3„ÄÇAnd if we look at the loss„ÄÇPlugging in that „ÄÇ3 now into„ÄÇ

Now that now y is 0„ÄÇThe true whyÔºå we are estimating 0„ÄÇSoÔºå we can„ÄÇThis all cancels out„ÄÇ

And now we get a negative log of 0„ÄÇ3„ÄÇ We get a higher lossÔºå1„ÄÇ

2 than the much smaller loss we saw in the previous slide„ÄÇWhen the model was right„ÄÇ

 we had a small loss when the model was wrong„ÄÇWe had a big loss„ÄÇSo sure enough„ÄÇ

 cross entropy loss is doing the right thingÔºå the loss is bigger when the model is wrong„ÄÇ



![](img/544e39cd2e591088c9093592282e58ed_2.png)

We've derived the cross entropy loss and seen how it applies to our sentiment example„ÄÇ

Cros entropy loss is equally important for neural networksÔºå as we'll see in future lectures„ÄÇ



![](img/544e39cd2e591088c9093592282e58ed_4.png)