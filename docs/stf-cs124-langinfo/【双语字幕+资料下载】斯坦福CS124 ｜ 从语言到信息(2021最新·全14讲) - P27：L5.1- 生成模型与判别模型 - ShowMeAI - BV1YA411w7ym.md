# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘æ–¯å¦ç¦CS124 ï½œ ä»è¯­è¨€åˆ°ä¿¡æ¯(2021æœ€æ–°Â·å…¨14è®²) - P27ï¼šL5.1- ç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹ - ShowMeAI - BV1YA411w7ym

In this lectureï¼Œ we talk about the difference between generative and discriminative classifiers and the naive bays and logistic regression classifiers and their relationshipã€‚



![](img/c94d5ea106b6475f35cbf59c69ec87ad_1.png)

Logistic regression is an important analytic tool in the natural sciences and the social sciencesã€‚

 It's also the baseline supervised machine learning tool for classificationã€‚

 and it's the foundation for neural networksï¼Œ As we'll see laterã€‚

 a feed forward neural network can be thought of just as a sequence of stacked logistic regression classifiersã€‚

 There's an important difference between naive Baysã€‚

 the classifier we've seen so far and logistic regressionã€‚

 naive Bays is a generative classifier While logistic regression is a discriminative classifierã€‚

 Let's think about what that meansã€‚ğŸ˜Šï¼ŒSuppose we're distinguishing cat from dog imagesã€‚

 we're building a classifier to tell us if something is a cat or a dogã€‚

A generative classifier thinks of the problem this wayã€‚

 We're going to build a model of what's in a cat imageã€‚

 It might know about whiskers and ears and eyesã€‚ And it's going to assign a probability to an imageã€‚

 How cat like is this imageã€‚ And we're going to do the same thing for dogsã€‚

 We're going to build a model of what makes up a dogï¼Œ the floppy earsï¼Œ the nose and so onã€‚

And now we're given a new imageï¼Œ we're going to run our cat classifier and our dog classifierã€‚

 and we're just going to see which one fits betterã€‚

 which one models the details of this image betterã€‚By contrastã€‚

 a discriminative classifier's job is just to distinguish the dogs from the catsã€‚ So it might noticeã€‚

 for exampleï¼Œ ohï¼Œ lookã€‚It just happens that in the training setï¼Œ all the dog images have collarsã€‚

 So a discrimminative classifier is going to pick up on features that discriminate the two imagesã€‚

 It's trained to discriminateï¼Œ and it's going to find features that discriminate the two imagesã€‚

 and it might ignore everything elseã€‚More formallyã€‚

 we have the task of finding the correct class C from a document D in a generative or discriminative classifierã€‚

 Nowï¼Œ in naive baysï¼Œ as we've seen beforeï¼Œ we're computing the most likely class over all classesã€‚

 And we do this by multiplying together the likelihoodã€‚

 That's the likelihood of of this image given the classï¼Œ How likely for my cat detectorã€‚

 am I to see these particular features I'm seeing on this particular imageã€‚Times the priorã€‚

 how likely are cat photosï¼Œ in generalï¼Œ Lo regressionï¼Œ On the other handï¼Œ is doing a similar a maxã€‚

 We're trying to pick the class over out of all classesï¼Œ which maximizes a scoreã€‚

 But here the score is the probabilityï¼Œ the direct posterior probabilityã€‚

 We're just going to directly say which class C is most likely given the imageã€‚

 So generative classifiersã€‚ We have a likelihood in the prior that we saw in naive Bayã€‚

 Log regressionã€‚ we're going to learn how to directly compute the probability of this classã€‚

A probabilistic machine learning classifier has a set of components where given M input output pairsã€‚

 I'm going to use the superscript I here to mean a particular observationã€‚

 so we have one observation X super I comma Y super I where x is the input and y is the outputã€‚

And we're going to have a feature representation of that inputã€‚

 So for each input observation x super Iï¼Œ we're going to have a vector of features x1 x2 up to X Nã€‚

 And we're going to refer to that particular feature J for input x super I as x sub Jã€‚

 So here x sub2 is feature 2ã€‚And if we want to be more completeã€‚

 we'll make it clear that I mean x subj for input Iã€‚

 and occasionally you'll see the notation F subj feature subj of xã€‚

So we have our feature representationã€‚ Then we're going to need a classification function that computes Y hatã€‚

 Why hat is the estimated classã€‚ So Y hat can be dog or Y clt can be cat or Y clt can be0ã€‚

 Y hat can be oneã€‚And we're it's going to do that by computing a probability P of y given X and the classification function we're going to introduce in this lecture is the sigmoid or the related softm functions very important functionsã€‚

 We're going to need to learn the parameters for this functionã€‚

 and for learning we need an objective and we're going to use cross-enttropy loss as the objective and we'll talk about what that meansã€‚

 and finallyï¼Œ we need an algorithm for optimizing whatever this objective isã€‚

 whatever our goal is that we're trying to get our classifier to be good at and we're going to introduce the stochastic gradient descent algorithm for optimizing in objectiveã€‚

Logistic regression has two phasesï¼Œ like many machine learning classifiersã€‚

We have the training phase in the test phase in the training phaseã€‚

 we're going to learn weights and we're going to see that these are going to be a set of weights W and Bã€‚

 we're going to use stochastic gradient descent to find optimal weights and we're going to use cross entrytropy lost to measure how good the weights areã€‚

And then in the test phaseï¼Œ we're going to be given a test example Xã€‚

 and we're going to compute the probability of Y of x for each possible class Y using these learned weightsã€‚

 And we're going to return whichever labelã€‚ let's say it' we're doing binary classã€‚ So y is 0ã€‚

1 or y is 0ã€‚ We're going to return whichever one has the higher probabilityã€‚

 We've now seen the high level of intuition of the logistic aggression classifier and its relationship to the naive base classifier in further lectures we'll see more detailsã€‚



![](img/c94d5ea106b6475f35cbf59c69ec87ad_3.png)