# P27ÔºöL5.1- ÁîüÊàêÊ®°Âûã‰∏éÂà§Âà´Ê®°Âûã - ShowMeAI - BV1YA411w7ym

In this lectureÔºå we talk about the difference between generative and discriminative classifiers and the naive bays and logistic regression classifiers and their relationship„ÄÇ



![](img/c94d5ea106b6475f35cbf59c69ec87ad_1.png)

Logistic regression is an important analytic tool in the natural sciences and the social sciences„ÄÇ

 It's also the baseline supervised machine learning tool for classification„ÄÇ

 and it's the foundation for neural networksÔºå As we'll see later„ÄÇ

 a feed forward neural network can be thought of just as a sequence of stacked logistic regression classifiers„ÄÇ

 There's an important difference between naive Bays„ÄÇ

 the classifier we've seen so far and logistic regression„ÄÇ

 naive Bays is a generative classifier While logistic regression is a discriminative classifier„ÄÇ

 Let's think about what that means„ÄÇüòäÔºåSuppose we're distinguishing cat from dog images„ÄÇ

 we're building a classifier to tell us if something is a cat or a dog„ÄÇ

A generative classifier thinks of the problem this way„ÄÇ

 We're going to build a model of what's in a cat image„ÄÇ

 It might know about whiskers and ears and eyes„ÄÇ And it's going to assign a probability to an image„ÄÇ

 How cat like is this image„ÄÇ And we're going to do the same thing for dogs„ÄÇ

 We're going to build a model of what makes up a dogÔºå the floppy earsÔºå the nose and so on„ÄÇ

And now we're given a new imageÔºå we're going to run our cat classifier and our dog classifier„ÄÇ

 and we're just going to see which one fits better„ÄÇ

 which one models the details of this image better„ÄÇBy contrast„ÄÇ

 a discriminative classifier's job is just to distinguish the dogs from the cats„ÄÇ So it might notice„ÄÇ

 for exampleÔºå ohÔºå look„ÄÇIt just happens that in the training setÔºå all the dog images have collars„ÄÇ

 So a discrimminative classifier is going to pick up on features that discriminate the two images„ÄÇ

 It's trained to discriminateÔºå and it's going to find features that discriminate the two images„ÄÇ

 and it might ignore everything else„ÄÇMore formally„ÄÇ

 we have the task of finding the correct class C from a document D in a generative or discriminative classifier„ÄÇ

 NowÔºå in naive baysÔºå as we've seen beforeÔºå we're computing the most likely class over all classes„ÄÇ

 And we do this by multiplying together the likelihood„ÄÇ

 That's the likelihood of of this image given the classÔºå How likely for my cat detector„ÄÇ

 am I to see these particular features I'm seeing on this particular image„ÄÇTimes the prior„ÄÇ

 how likely are cat photosÔºå in generalÔºå Lo regressionÔºå On the other handÔºå is doing a similar a max„ÄÇ

 We're trying to pick the class over out of all classesÔºå which maximizes a score„ÄÇ

 But here the score is the probabilityÔºå the direct posterior probability„ÄÇ

 We're just going to directly say which class C is most likely given the image„ÄÇ

 So generative classifiers„ÄÇ We have a likelihood in the prior that we saw in naive Bay„ÄÇ

 Log regression„ÄÇ we're going to learn how to directly compute the probability of this class„ÄÇ

A probabilistic machine learning classifier has a set of components where given M input output pairs„ÄÇ

 I'm going to use the superscript I here to mean a particular observation„ÄÇ

 so we have one observation X super I comma Y super I where x is the input and y is the output„ÄÇ

And we're going to have a feature representation of that input„ÄÇ

 So for each input observation x super IÔºå we're going to have a vector of features x1 x2 up to X N„ÄÇ

 And we're going to refer to that particular feature J for input x super I as x sub J„ÄÇ

 So here x sub2 is feature 2„ÄÇAnd if we want to be more complete„ÄÇ

 we'll make it clear that I mean x subj for input I„ÄÇ

 and occasionally you'll see the notation F subj feature subj of x„ÄÇ

So we have our feature representation„ÄÇ Then we're going to need a classification function that computes Y hat„ÄÇ

 Why hat is the estimated class„ÄÇ So Y hat can be dog or Y clt can be cat or Y clt can be0„ÄÇ

 Y hat can be one„ÄÇAnd we're it's going to do that by computing a probability P of y given X and the classification function we're going to introduce in this lecture is the sigmoid or the related softm functions very important functions„ÄÇ

 We're going to need to learn the parameters for this function„ÄÇ

 and for learning we need an objective and we're going to use cross-enttropy loss as the objective and we'll talk about what that means„ÄÇ

 and finallyÔºå we need an algorithm for optimizing whatever this objective is„ÄÇ

 whatever our goal is that we're trying to get our classifier to be good at and we're going to introduce the stochastic gradient descent algorithm for optimizing in objective„ÄÇ

Logistic regression has two phasesÔºå like many machine learning classifiers„ÄÇ

We have the training phase in the test phase in the training phase„ÄÇ

 we're going to learn weights and we're going to see that these are going to be a set of weights W and B„ÄÇ

 we're going to use stochastic gradient descent to find optimal weights and we're going to use cross entrytropy lost to measure how good the weights are„ÄÇ

And then in the test phaseÔºå we're going to be given a test example X„ÄÇ

 and we're going to compute the probability of Y of x for each possible class Y using these learned weights„ÄÇ

 And we're going to return whichever label„ÄÇ let's say it' we're doing binary class„ÄÇ So y is 0„ÄÇ

1 or y is 0„ÄÇ We're going to return whichever one has the higher probability„ÄÇ

 We've now seen the high level of intuition of the logistic aggression classifier and its relationship to the naive base classifier in further lectures we'll see more details„ÄÇ



![](img/c94d5ea106b6475f35cbf59c69ec87ad_3.png)