# P35ï¼šL6.3- å€’æ’ç´¢å¼•ï¼šä¿¡æ¯æ£€ç´¢èƒŒåçš„æ ¸å¿ƒæ•°æ®ç»“æ„ - ShowMeAI - BV1YA411w7ym

Hello againã€‚ In this segmentï¼Œ we're going to talk about the inverted index and how it's constructedã€‚

An inverted index is the key data structure that underlies all modern information retrieval systems from systems running on a single laptop to those running in the biggest commercial search enginesã€‚

 An inverted index is a data structure that exploits this sparsity of the term document matrix that we talked about in the preceding segment and allows for very inefficientã€‚

 sorryã€‚ğŸ˜Šï¼ŒAnd allows for very efficient retrievalã€‚ It's essentially without peer as the data structure used in information retrieval systemsã€‚



![](img/088093133a133983e1e56917ffeda6c1_1.png)

So let me go through what's in an inverted indexã€‚ So for each termï¼Œ each wordã€‚

 we must store a list of all the documents that contain the wordã€‚

Let's identify each document by a doc IDï¼Œ which is just the document serial number so you can think of us starting with the first document called oneã€‚

 then2ï¼Œ3ï¼Œ etcã€‚ğŸ˜Šï¼ŒAnd then the question isï¼Œ what data structure should we useï¼Œ I meanã€‚

 one idea might be to use fixed arrays like the vectors that are in the term document matrixã€‚

 but that's very inefficient because while for some wordsï¼Œ they'll appear in a lot of documentsã€‚

 other words will appear in very few documentsã€‚ Moreoverï¼Œ there are perhaps other problemsã€‚

 if we think about a dynamic index where some documents are added later onã€‚

 that then we have or documents are changedï¼Œ Then we'll have difficult things in adjusting our vector sizesã€‚

For these reasonsï¼Œ one way or anotherï¼Œ we need to use variable size lists to store the documents in which a word occurs and in standard information retrieval terminologyã€‚

 these lists are called postings lists postings lists traditionally we usually stored on diskã€‚

 though that may not be the case but now for big search engines and if you're storing postings lists on diskã€‚

 the right way to store them is as one continuous run of postings because that gives the most efficient method of then being able to load them off disk back into memory when you interested in the postings for a particular word In memoryã€‚

 a postings lists can be represented as a data structure like a linked list or variable length arrays with some obvious trade offs in the size versus the ease of insertionã€‚

ğŸ˜Šï¼ŒSo the data structure that we end up with for an invert index is like the one that I'm showing hereã€‚

 so we have the terms that are in any of our documents and then for each term we've then got a pointer to a postings list that is then giving the different documents which are described by their document ID in which it occursã€‚

ğŸ˜Šï¼ŒOkay so one occurrence of a word document pair is referred to as a posting and the sum of all of the postings lists are then referred to as the postings and so overall then we have the parts of on the left hand side we have the dictionary and then on the right hand side we have the postingsã€‚

And these and of property of the postings is that they're sorted by document Idã€‚

 and very soon now we'll explain why that's essentialã€‚So these two data structuresã€‚

 the dictionary and the postings have somewhat different statusesï¼Œ because in global sizeã€‚

 the dictionary is relatively smallï¼Œ but it's normally essential that it's in memoryã€‚

Whereas the postings are largeï¼Œ but at least for something like a small scale enterprise search engineã€‚

 these will normally be stored on diskã€‚Let me move now to how an inverted index is constructedã€‚

 So the starting off point is we have a bunch of documents be indexedã€‚

 and each of those documents we think of as being a sequence of characters willll assume that we've already dealt withã€‚

 perhaps by someone else's software conversion from Pf and Microsoft Word documents and things like thatã€‚

 So then we're going to go through first and preprocessing stepsã€‚

 So we need a tokenizer that turns the document into a sequence of word tokensã€‚

 which are the basic units of indexingã€‚ but we often don't index exactly the words that are contained in the documentã€‚

 there might be various linguistic modulesï¼Œ then some way modify the tokens to put them into some kind of canonical formã€‚

 Soï¼Œ for instanceï¼Œ here we're saying that for friends hereã€‚

 it's being both lowercase and as being stem to remove the S plural endingã€‚ğŸ˜Šï¼ŒOkayã€‚

 so then it's those modified tokensï¼Œ which will be fed to the indexerã€‚

 which is the thing that builds the inverted index that I was just talking aboutã€‚

 So here' is the inverted indexã€‚ And it's this step here of the indexer that is the main thing that I want to talk aboutã€‚

 But let me first just briefly mention those initial stages of text processingã€‚

 So in just a fraction more detailã€‚ the things that happen in those initial stages is firstly tokenizationsã€‚

 that's just how we decide to cut the character sequence into word tokensã€‚

 And there are various issues thereï¼Œ there are punctuation that come up against wordsã€‚

 how to treat possesssï¼Œ hyphenated terms and all that kind of stuff we can talk about in more detailã€‚

 then normalization is this issue that while certain things like USA with and without the dotsã€‚

 you probably want to treat as the same term and map both the text andã€‚ğŸ˜Šã€‚

TheQu terms to the same form so that they will matchã€‚

You might want to do other kinds of mapping such as stemming so that author and authorization are both being mapped to the same stem so that they straightforwardly match in a queryã€‚

And finallyï¼Œ you may not want to index at all the most common wordsã€‚ Traditionallyã€‚

 many search engines have left out very common words like that are to and of from the indexingã€‚

 It's not clear that in the modern world when our amount of storage is so vast that that's such a good idea because there are queries that you might like to do such as for the song to be or not to be where you really need the stop wordsã€‚

 And it turns out that with modern indexesï¼Œ it's not that inefficient to store themã€‚ğŸ˜Šï¼ŒOkayã€‚

 now let's go through in detail how the indexer goes from the sequence of perhaps normalized tokens to building an inverted indexã€‚

 So for this exampleï¼Œ we're assuming we have two documentsï¼Œ Doc 1 and doc2 hereã€‚

 So there are a key sequence of steps that we go throughã€‚Soï¼Œ our import is that we haveã€‚

The sequence of tokens of the first document in the order that they come in the text and the sequence of tokens of the second document in the order in which they come in the textã€‚

 So the first step is that we do a sortã€‚ and we sort as the primary key by the terms putting them in alphabetical orderã€‚

 So here we have this alphabetical list of termsã€‚ And if we have the same term appearing in multiple document we do a secondary sort by the document Iã€‚

 So the word Caesar appears twice once in document I 1 and twice in document I 2ã€‚

 and we're sorting it secondarily by document I Dã€‚ And so that's a core and expensive indexing stepã€‚

Once we've got that farï¼Œ what we then do is essentially a consolidation of what we found over here on the rightã€‚

 So we take that Here it is againã€‚ğŸ˜Šï¼ŒAnd multiple entries in a single document are mergedã€‚

 So that's the two instances of Caesarã€‚ And they're just treat as oneã€‚ And then we also mergeã€‚

All instances of a particularã€‚Termï¼Œ and so then we re represent that as over hereã€‚

 So we say we have the dictionary entry Caesarã€‚We record its total frequency in the collectionã€‚

 I'll come back to that a bit laterï¼Œ and then we build for it the postings listã€‚

 which is the list of documents in which it occursã€‚ And straightforwardlyã€‚

 because of a consequence of our sort in the previous stepã€‚

 that this postings list is itself going to be sorted by the document I Dã€‚

So in thinking about the size of an inverted indexã€‚

 we can think for a minute about where do we pay in storageã€‚

 So we pay some amount for the list of terms and their accountsã€‚

 But the number of terms will be relatively modestã€‚ In our example beforehandã€‚

 there are 500000 termsã€‚We pay for a pointer two that identifies where the postings lists areã€‚

 But againï¼Œ that's of the order of 500000 thingsã€‚ And then we pay for the actual postings lists themselvesã€‚

 And these postings lists are by far the biggest partã€‚

 But even then theyre bounded by the size of the number of tokens in the collectionã€‚

 So an in our example before of the million documents of average length1000 words with still less than 1 billion items thereã€‚

 And so storage is manageableã€‚Soã€‚When we are actually building an efficient IR system implementationã€‚

 we think further about these questionsï¼Œ we think about how can we make the index as efficient as possible for retrieval and how can we minimize the storage on both sides of this both on this side and this side in terms of various sorts of compression we're not going to get into the details of that nowã€‚

 but what I hope you can start to see is that the inverted index gives an efficient basis on which to do retrieval operationsã€‚

ğŸ˜Šã€‚

![](img/088093133a133983e1e56917ffeda6c1_3.png)

And that's something that we'll talk about in more detail in the next segmentã€‚

 but at any rate now you know the underlying data structureã€‚

 it's really not that complex that underlies all modern information retrieval systemsã€‚ğŸ˜Šã€‚



![](img/088093133a133983e1e56917ffeda6c1_5.png)