# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èCS124 ÔΩú ‰ªéËØ≠Ë®ÄÂà∞‰ø°ÊÅØ(2021ÊúÄÊñ∞¬∑ÂÖ®14ËÆ≤) - P31ÔºöL5.5- ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç - ShowMeAI - BV1YA411w7ym

![](img/b7e5b8a066b61cda0a7087d2c96a9174_0.png)

In this lectureÔºå we introduced the stochastic gradient descent algorithm that's used for optimizing the weights for logistic regression and for neural networks„ÄÇ



![](img/b7e5b8a066b61cda0a7087d2c96a9174_2.png)

Our goal with gradient descent is to find the optimal weights„ÄÇ

 minimize the loss function we've defined for the model„ÄÇ

We'll explicitly represent here the fact that the loss function L is parameterized by the weights„ÄÇ

 which we can refer to in machine learning terms in general as theta„ÄÇ

 so in the case of logistic regressionÔºå theta is W and B„ÄÇ

And will also represent Y hat the function that we're computing as F of x parameterized by theta to make the dependence on theta more obvious„ÄÇ

 So the goal is to find the set of weights that minimizes overall all possible weights„ÄÇ

The loss function„ÄÇOf this is why hatÔºå comma Y„ÄÇ So the Ithe example„ÄÇ

 our estimate of Y hat for that example„ÄÇHow different is that from the true why„ÄÇ

 that loss averaged over all of the M exemplars„ÄÇHow do you find the minimum of a loss functionÔºü

Graadient descent is a method that finds a minimum of a function by figuring out in which direction in the space of parameters theta„ÄÇ

 the function slope is rising the most steep and moving in the opposite direction„ÄÇ

 The intuition is that suppose you're hiking in a canyon and you're here at position X„ÄÇ

And you want to descend as quickly as possible to the river you're very thirsty„ÄÇ

 so you might look around yourself 360 degreesÔºå find whichever direction the ground is sloping maybe the most uphill„ÄÇ

And go the opposite way down toward the river„ÄÇFor logistic regression„ÄÇ

 this loss function is conveniently convex„ÄÇ A convex function has just one minimum„ÄÇ

 There are no local minima to get stuck in„ÄÇ So gradient descent starting from any point is guaranteed to find the minimum„ÄÇ

 By contrastÔºå the loss for multilayer neural networks is non convex and gradient descent may get stuck in local minima for neural network training and never find the global optimum„ÄÇ

Although the algorithm and the concept of gradient are designed for direction vectors„ÄÇ

 let's first consider a visualization of the case where the parameter of our system is just a single scale or W„ÄÇ

So assuming the loss function L has this shapeÔºå so in blue I've shown you the loss function„ÄÇ

 here's the loss function„ÄÇAlong the X axisÔºå I have some parameter W„ÄÇ

Maybe it starts off at some value W super1 we'll just say it starts at zero to make your life easy and you can see by inspection that the goal„ÄÇ

 the minimum for this loss function is going to be at this point hereÔºå W supermin„ÄÇ

So if we're starting here at the place where W is0 on the loss function„ÄÇ

 we want the algorithm to tell us should we be moving to the right or should we be moving to the left if we move left„ÄÇ

 we mean that W super2 should be smaller than W1„ÄÇ We move to the right We're saying that W super2 should be bigger than W1 and we can see that the correct answer should be to move to the right„ÄÇ

NowÔºå the gradient ascent algorithm answers this question by finding the gradient of the loss function at the current point and moving in the opposite direction„ÄÇ

 So the gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function„ÄÇ

 and the gradient is a multivariable generalization of the slope„ÄÇ

 So for a function of one variable like the one in this figure„ÄÇ

 we can informally think of the gradient as the slope„ÄÇ

 So this dotted line shows the slope of this hypothetical loss function at this point where W equals 0„ÄÇ

 W equals w sub1„ÄÇ So you can see the slope of this dotted line is negative„ÄÇüòä„ÄÇ

And if the slope is negative„ÄÇThat's telling us that we should be moving positive„ÄÇThus„ÄÇ

 to find the minimumÔºå gradient descent tells us to go in the opposite direction„ÄÇ

 Mo W in a positive direction„ÄÇAnd we're showing here that we're now moving from the original W1 to this gray point„ÄÇ

 which is going to be W super2„ÄÇTo summarizeÔºå the gradient of a function of many variables is a vector pointing in the direction of the greatest increase in the function„ÄÇ

 and gradient descent is an algorithm for finding the gradient of a loss function at the current point and then moving in the opposite direction„ÄÇ

But I haven't said how much we move in that direction„ÄÇ

The magnitude of the amount to move in gradient descent is the value of the slope of the loss function„ÄÇ

 with respect to W weighted by a learning rateÔºå Eda„ÄÇ

A higher or faster learning rate means that we should move W more on each step„ÄÇ

The change we make in our parameters is the learning rate times the gradient or the slope in our single variable case„ÄÇ

NowÔºå let's extend the intuition from a function of one scalar variable W to many variables„ÄÇ

 because we don't just want to move left to right„ÄÇ We want to know where in the n dimensional space of the n parameters that make up theta we should move„ÄÇ

 And the gradient is just such a vector„ÄÇ It expresses the directional components of the sharpest slope along each of those n dimensions„ÄÇ

If we're just imagining two weight dimensionsÔºå say for one weight W and one bias B„ÄÇ

 the gradient might be a vector with two orthogonal components„ÄÇ

 each of which tells us how much the ground slopes in the W direction and the B direction„ÄÇ

 And here we see a visualization of the value of a two dimensional gradient vector taken at the red point„ÄÇ

 So if we're here at this pointÔºå we want to move„ÄÇAlong„ÄÇThis much in WÔºå excusecus me„ÄÇ

 This much in B and this much in W„ÄÇGiving us the combined vector red vector„ÄÇ

 So we'll move from this point here to„ÄÇThis point here„ÄÇIn an actual logistic regression„ÄÇ

 the parameter vector W is much longer than one or two„ÄÇ

 since the input feature vector x can be quite longÔºå and we need a weight W sub I for each x sub I„ÄÇ

For each dimension or variable W sub I in W plus the bias B„ÄÇ

 the gradient will have a component that tells us the slope with respect to that variable„ÄÇ

EssentiallyÔºå we're asking how much would a small change in that variable W sub I influence the total loss function L in each dimension W sub I„ÄÇ

 We express the slope as a partial derivative of the loss function with respect to W I„ÄÇ

 The gradient is then defined as a vector of these partials„ÄÇHere's a vector of gradients„ÄÇ Again„ÄÇ

 we're representing Y hat is F of x comma theta to make the dependence on theta more obvious„ÄÇ

 So the gradient with respect to theta of the loss function of Y had comm a Y„ÄÇ

Is this vector each one a partial derivative with respect to a different set of weights or the bias would be in here tooÔºü

So the final equation for updating theta based on the gradient is that the next„ÄÇ

Time step value for theta is the old one minus the learning rate times the gradient„ÄÇ

In order to update thetaÔºå we need a definition for that gradient„ÄÇ

And recall that for logistic regressionÔºå the cross entropy loss function is this negative y log sigma plus 1 minus y log 1 minus sigma of the W X plus B's„ÄÇ

 NowÔºå it happens that the very elegant derivative of this function is simply sigma of W X plus B minus the true y„ÄÇ

All times the input value X„ÄÇ But notice that the gradient with respect to the single weight W sub J represents a very intuitive value„ÄÇ

 It's the difference between the true Y and our estimated Y hat for that observation„ÄÇ

Multiplied by the corresponding input valueÔºå x sub J„ÄÇSo in summary„ÄÇ

 stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example and nudging theta in the right direction„ÄÇ

 the opposite direction of the gradient„ÄÇ So for each training tuuppleÔºå we compute Y hat„ÄÇ

 we compute the loss telling us how far off is Y hat from the true output Y„ÄÇ

We compute the gradient to tell us in what direction we should move theta to maximize the loss„ÄÇ

 and we go the other way„ÄÇThe algorithm can terminate when it converges or when the gradient norm is less than some preset epsilon„ÄÇ

 or when progress haltsÔºå For exampleÔºå when the lawft starts going up again on a held outset„ÄÇ

The learning rateÔºå Eda is a hyperparameter that must be adjusted„ÄÇ If it's too high„ÄÇ

 the learner will take steps that are too largeÔºå overhooting the minimum of the loss function„ÄÇ

 If it's too lowÔºå the learner will take steps that are too small and take too long to get to the minimum„ÄÇ

 It's common to start with a higher learning rate and then slowly decrease it„ÄÇIn general„ÄÇ

 hyperparameters are a special kind of parametermeter for any machine learning model„ÄÇ

 Unlike regular parametersÔºå weights like W and BÔºå which are learned by the algorithm from the training set„ÄÇ

 Hyparameters are special parameters chosen by the algorithm designer that affect how the algorithm works„ÄÇ



![](img/b7e5b8a066b61cda0a7087d2c96a9174_4.png)

We've now introduced the important stochastic gradient descent algorithm„ÄÇ

 we'll give more details in the next lecture„ÄÇ

![](img/b7e5b8a066b61cda0a7087d2c96a9174_6.png)