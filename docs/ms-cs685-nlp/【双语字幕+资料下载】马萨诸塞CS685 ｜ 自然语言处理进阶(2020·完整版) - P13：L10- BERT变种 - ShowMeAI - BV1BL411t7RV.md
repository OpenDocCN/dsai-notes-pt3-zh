# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é©¬è¨è¯¸å¡CS685 ï½œ è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶(2020Â·å®Œæ•´ç‰ˆ) - P13ï¼šL10- BERTå˜ç§ - ShowMeAI - BV1BL411t7RV

Hey everyoneï¼Œ today we're going to be talking about better BRTtã€‚

 so these are recent papers that have improved over Bt in a number of different areas and so we're going to be switching back and forth between the iPad and some slides which are essentially just taking figures and tables from the papers to make the results and models more clearã€‚

So let's get startedã€‚Provements on Bã€‚So there are four specific improvements that I want to discuss todayã€‚

 the first are kind of small training improvementsã€‚And also in combination with more dataã€‚And theseã€‚

2ã€‚Features of the Roberta paperï¼Œ which was in your assigned readingã€‚The next is longer sequencesã€‚

 so we know that during pretraining BRT takes in sequences of 512 tokens each and remember that each input to Bt is actually two distinct segments that are separated by the separator token and the CLS vector is as to predict does a second segment follow the first segmentã€‚

 so really like if those are two noncontiguous segmentsï¼Œ you only get 256 tokens that are contiguousã€‚

So not that much textï¼Œ it's going to be hard for you to model a very long document or a book using Bt because it can only model such small sequences at a timeã€‚

So one way that you could think about tackling this is using a mechanism like the one in the XL net paperã€‚

 which relies on the transformer XLã€‚And I had you read the Excelnet paperã€‚

 but I think I should have made you read the transformformer Excel paper where X stands for extra longã€‚

 So this improvement allows the model to condition its predictions on much longer sequences of contextã€‚

 but it still has a practical limit of the amount of text that it can look back and modelsã€‚

 so it's not perfectï¼Œ but it is an effort in this vein there have since been other improvements proposed that do this better than the transformer Excelã€‚

 but this was basically the first or one of the very first attempts to integrate this long context modeling into the transformer modelã€‚

The third thing we're going to talk about is more efficient objectivesã€‚And so we've seen in Bertã€‚

 right the pretraining objective is a mask language modelï¼Œ I take my inputã€‚

 I mask out 15% of the tokens at randomï¼Œ and then I have the model feed everything through a transformer and predict each of those mask tokensã€‚

 what was the word that should have gone into that maskã€‚

But we look at the electro model which proposes a much simpler objective function that's far more efficient and achieves the same resultsã€‚

 so a big win for those of you who are interested in pretraining your own models the E paper has a small model that they said they trained on one GPU in four days so that's you know pretty awesome maybe something to look intoã€‚

Okayï¼Œ and finallyï¼Œ we have smaller models and this was the Albert paperã€‚

Although I guess their best results were with models with roughly equal parameters to BRTã€‚

 but it's very interesting how you can add more parameters in BRTã€‚

 it's not necessarily adding more layersï¼Œ but rather you can get away with sharing parameters at each individual transformer blockã€‚

 but just making the hidden states much largerï¼Œ so very interesting result in that paper as wellã€‚ğŸ˜Šã€‚

So we have basically four differentã€‚Model architectures and strategies that we're going to be covering todayã€‚

 and let's go ahead and get started with Robert firstã€‚Okayã€‚

 so Roberta doesn't just have like one single advance that it proposedã€‚

 but actually there's like four or five different things that are proposed in the Roberta paper that when taken together result in pretty significant improvements over Bt and all of these modifications are at the serviceã€‚

 very simple so very simpleã€‚ğŸ˜Šï¼ŒCollectionã€‚Of modificationsã€‚

But this is the kind of paper that you really need a lot of resources to run right a lot of GPUs Roberta was done by Facebook AI so it's not the kind of paper that you can do with you know your collab notebook so even though it's very simpleã€‚

 a lot of compute went into getting these results and hopefully they're you know generalizable to people who want to build upon them So the first modification that was made in the Roberta paper is to train with bigger batchesã€‚

So remember that to train these kinds of models we usually use mini batch training right so in Bt each batch consists of a bunch of these 512 token sequences and in Robertaã€‚

 they showed that you can actually train with a larger batch size and kind of see the same number of examples but do fewer updates right so if I do a smaller number of batchesã€‚

ğŸ˜Šï¼ŒWith larger batch sizeã€‚å—¯ã€‚This actually results in better performance than having a smaller batch size and a larger number ofã€‚

Gradient updatesã€‚So one thing that you might be thinking of here is wellã€‚

 it's not easy for you to use a bigger batch size rightã€‚

 you would like to train with the biggest batch size that you canã€‚

 but the issue is that you're limited by your GPU at the end of the day right so your GPU mightã€‚

Only allow you to train with a batch size of 10ã€‚ and you knowã€‚

 the models in this paper are trained the batch size of like 8000ã€‚

 So how are you with your single GPU going to be able to train with a larger batch size and one like really simple way of doing this in a memory constrained setting is gradient accumulationã€‚

To bypassã€‚GPu memoryã€‚Requirementsã€‚So the observation that using larger batch sizes results in better performance is not a novel contribution of the Roberta paperã€‚

 it has been shown previously to be true in neural machine translation and not only in NLP but also in some vision applications so gradient accumulation is reallyã€‚

 really simpleï¼Œ all you have to do is oh sorry this shouldn't say requirementï¼Œ it should sayã€‚ğŸ˜Šã€‚

Limitationï¼Œ I don't knowã€‚ I requirementã€‚So the way you do this is let's say you want to train with a batch size of 8000ã€‚

 but your memory GPU memory can only fit in maybe sayã€‚

 I don't know 800 examples per batch at a time so what you do is you feed in 800 examples and instead of updating so you compute the gradient through batch crop like you normally would for these 800 examples but then instead of actually applying the update remember we talked about multiplying the gradient with the learning rate and then taking a step in the direction of the negative gradient instead of doing that at the end of seeing these 800 examples I'm just going to store that gradient and I'm going to see the next batch of 800 examples and then I'll accumulate the gradient from the new batch using by just adding it to the gradient from the old batch that I haveã€‚

ğŸ˜Šï¼ŒAnd now I can just do this 10 more times until I've seen 8000 examples and then I will finally do my update so effectively I've done an update with a batch size of 8ã€‚

000ï¼Œ but I've just taken 10 passes through the GPU to get to this point so really simple trickã€‚

 but it lets you kind of simulate the large batch setting even if you have a tiny GPUã€‚ğŸ˜Šï¼ŒOkayã€‚

 so the first contribution was that training with bigger batches improves your downstream performance the second one is a simplificationã€‚

 so it's no next sentenceã€‚Predictionã€‚So remember that in Bertã€‚

 we have these two segments and the next sentence prediction is a classifier put over the Cs token to predict whether the second segment follows the firstã€‚

 In Robertaï¼Œ they find out that you don't actually need this for any downstream gainsã€‚

 And instead of doing thisï¼Œ you can just have a single contiguous 512 token long segment as input instead of these two distinct segmentã€‚

 And you just don't have the next sentence prediction on top of your Cs tokenã€‚

 and it doesn't seem to hurt you at allã€‚ So this was pretty interesting because the Bt paper kind of makes a big deal that the next sentence prediction is helpful for a lot of tasksã€‚

 butã€‚ğŸ˜Šï¼ŒPerhaps in combination with many of the other improvements proposed in the Roberta paperã€‚

 this really isn't a big dealï¼Œ and it is kind of curious why the Bt authors defaulted to the next sentence prediction task without first trying it without next sentence prediction and with a single contiguous segmentã€‚

 I feel like they had something in their paper that did this ablationã€‚

 but I'm not remembering clearly at the moment what the conclusions wereã€‚

So the downstream performance here is unaffectedã€‚And it's actuallyï¼Œ in some casesï¼Œ slightly improvedã€‚

 And a consequence is the CLs tokenã€‚Gets no pre trainingã€‚So the CLS token is never maskedã€‚

 it's not really used in pretraining for doing anything and maybe that was some of the intuition behind the original idea in the bird paper which was maybe we need to have some sort of classification task on top of the CLS token so that when we fine tune it it's already you know learn to perform some classification task on the sequence so it can be more easily adapted to other classification tasks this intuition might be supported by some of the results with the intermediate fine tuning that we talked about sorry last lectureã€‚

ğŸ˜Šï¼ŒBut despite the fact that the CLS token has no pretrainingã€‚

 it's still used in the computations rightï¼Œ the forward pass includes the self-attention computation at the CLS token in every layer so it's still contextualized representation of the entire rest of the sequence and maybe you don't need to actually put a classifier on top during pretrainingã€‚

 the fine tuning process could be sufficient to force the CLS token to specialize to a particular downstream taskã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so that was the second advanceï¼Œ the third advance was we're going to pre trainã€‚Onã€‚More dataã€‚

So in particular they do this experiment where they take 16 billion token oh sorry 16 gigabytes of tokensã€‚

 I wish more papers would just report theã€‚Exact number of tokens in their data set rather than the size of the data setã€‚

 but they scale from 16 gigabytesï¼Œ which was used in the bird paper to 160 gigabytes of pretraining data for this mask language modeling this data in the 160 Gb setting comes from a filtered version of the common crawl as well as this data set that was constructed by looking at or basically downloading all web pages that were linked linked to from Redddit postsã€‚

So URLs from Redditã€‚The choice to use this kind of data is pretty interestingã€‚

 I guess you could say that if a URL has been linked from a Reddit post to maybe with some number of upvoatsã€‚

 then it likely contains some useful or interesting information and that is the kind of stuff you want to pretrain onã€‚

 but I'm not sure there's probably some better intuition behind the choice of dataã€‚

At the scale thoughï¼Œ it's more just like can I get as much data as possible and maybe this was a good way to do thatã€‚

Finallyï¼Œ the last contribution here was pre train for longerã€‚So like more batchesï¼Œ epochsã€‚

So they actually do 500 k stepsï¼Œ I think a step is equivalent to a batch here or maybe like an every time you do an actual gradient updateã€‚

 so it might not be equivalent to a batch if you're doing gradient accumulationã€‚Okayã€‚

 so these were the four main improvements and so maybe we'll switch over to the slides now just to look at the results of each oneã€‚



![](img/bbfb665bd849a609a104cd892c741d57_1.png)

![](img/bbfb665bd849a609a104cd892c741d57_2.png)

Okayï¼Œ coolï¼Œ so first of allï¼Œ let's move to the the first point which was the bigger batch size is betterã€‚

 And so here you can see three different batch sizesï¼Œ 256ã€‚

200 and8000 and so they're controlling for the same number of examples seen by the model so you can see that the number of steps has decreased as the batch size increases so you can run a pretty controlled experiment And if you look at the downstream performance hereã€‚

 So we have MnLIï¼Œ which is a classification task and SSTã€‚

 which is a sentiment classification task you'll see thatã€‚ğŸ˜Šï¼ŒThe results aren't super conclusiveã€‚

 but it looks like using a batch size of 2000 yields the best performanceã€‚

 maybe these results aren't that impressiveï¼Œ but overall at this scaleã€‚

 even an improvement of less than a point is kind of significantã€‚

So increase batch size that's the first thingã€‚ Second thing is removing next sentence predictionã€‚

 So in this table they if you just look at the first four or I guess six rows hereã€‚

 they have a perfectly controlled experiment where they take their modelã€‚

 they add the next sentence prediction lossï¼Œ this NSP loss and then they have the same thing without the NP loss and they evaluate it over squadã€‚

 the QA data set MNLI entailment sentiment and race actually embarrassingly forget what race is I think it might be a coRA data set anywayã€‚

 so the results show if you compare them that without the next sentence prediction task you're getting maybe a better F1 on squad you're getting slightly better on MNLI maybe a little worseã€‚

ğŸ˜Šï¼ŒComparableï¼Œ I don't know on sentiment and better on raceã€‚ so it it's not like a huge dealã€‚

 but if it's not conclusively improving your performance across the boardã€‚

 then it makes sense to get rid of it right and just simplify your modelã€‚

Okay so that was the next sentence prediction they also had this third contribution of pretraining on more data and also pretraining longer so this table was pretty cool Roberta with books and wki is their kind of analog to the BERT data said it has 16 gigabytes of text and you see its performance on S and then lion SST and then they add additional data so this is going from 16 gigabytes to 160 gigabytes using the common crawl and those redit linked web pages they kept the number of steps and the batch size the same here and you see that there's maybe a slight improvement across the board but of course you've kept the number of steps the same so the model maybe hasn't got a chance to properly leverage this huge collection of data because it's definitely taken fewer passes through the entire dataã€‚

ğŸ˜Šï¼Œã•ã€‚So we have pretrain longerï¼Œ which increases the number of steps to 3000 and you see a slight improvements and if you pretrain even longerã€‚

 this extremely scientific term hereï¼Œ to 500ï¼Œ000 steps then you get to 89ã€‚4 F1 on S ohï¼Œ yeah on S 2ã€‚

Okayï¼Œ I assume these are F1s and 90ã€‚2 accuracy on MMLI 96ã€‚

4 on sentiment and just comparing this to Bt largeã€‚

 so these models have the same architecture the same size you can see that all of these improvements now we're combining the no net next sentence prediction with the bigger batches and more data pretrain for longer all of these incremental games tend to add upã€‚

 So now we've gone to from 89ã€‚4 F or F1 on squad2 compared to 81ã€‚

8 with Bt large and you can see that the improvements are more significant for the other two data sets as wellã€‚

 So in conclusion all of these small they might seem like pretty minor improvementsã€‚

 but when combined together they do make roã€‚ğŸ˜Šï¼ŒA powerful model and I meanï¼Œ of courseã€‚

 adding more data is not something that is easily measuredã€‚

 like what happens if you pretrain Bt large with 160 gigabytes of data for 500 k steps that would have been interesting to seeã€‚

 but of course you can only do so many so many experimentsã€‚ So anywayï¼Œ yeahã€‚

 that's all I wanted to say a Robertaã€‚ğŸ˜Šï¼ŒAnd the pretrain model for Roberta is available in Huging faceã€‚

 it will be part of your homework oneï¼Œ so you will get a chance to play around with itã€‚



![](img/bbfb665bd849a609a104cd892c741d57_4.png)

Okayï¼Œ so for the next partï¼Œ I'm going to switch back to the iPadã€‚



![](img/bbfb665bd849a609a104cd892c741d57_6.png)

And now we're going to start talking about the second improvement over Btï¼Œ which is theã€‚å—¯ã€‚

The limited context size that it hasã€‚ So we'll switch now to the transformer X Lã€‚

Which is the base component of XL netã€‚And so the the motivating question behind transformformer Excel is that Bertã€‚

Hasã€‚A fixedã€‚Tokenedã€‚Limit of 512ã€‚For its inputsã€‚How can we modelã€‚Longer sequencesã€‚æ˜¯ã€‚

And one issue with just naively applying the transformer toï¼Œ you knowï¼Œ instead of 512ã€‚

 why don't we do 1024ï¼Œ or why don't we do 50ï¼Œ000ï¼Œ is that you quickly run up against the or memory limitations of your GPUã€‚

Andï¼Œ you knowï¼Œ if you're doing likeï¼Œ I don't knowï¼Œ50000 token sequencesã€‚

 you probably can't even fit a batch size of one on your standard GPUã€‚

 So this brute force way of just increasing the token in the numberã€‚

 sorry increasing the sequence length isn't going to workã€‚

 So you might need to make a modification to your architectureã€‚

 And that's where the transformer Excel comes inã€‚ its idea at a high level is to add a recurrentã€‚ğŸ˜Šã€‚

Mechanismã€‚That connectsã€‚Adjacent segmentsã€‚And so you might be thinkingï¼Œ wellã€‚

 doesn't that just effectively increase or double our sequence lengthã€‚

 so isn't this going to also run into these memory issuesï¼Œ but they get around this byã€‚

Not back propping into the previous sequenceã€‚ So there's no gradient flowã€‚To the previous segmentã€‚

All the hidden states from the previous segment are cachedã€‚

So we don't even do another forward to compute themã€‚

 we basically just take these cached hidden states that we computed from the previous batch and we cache them and we have a recurrent mechanism that allows our model at the current time stepss that it's a to attend over parts of the cache representations as well as do the normal self-attention over the sequence that you're predicting overã€‚

So this was again one of the first approaches to try and extend the context size of the transformerã€‚

 but it's not perfectï¼Œ it does not let you just attend over context of arbitrary sizeï¼Œ practicalã€‚è¯¶ã€‚

Limitã€‚To this extended context windowã€‚And thisã€‚The effective context window here in the transformformer Excel scales linearly with the number of layers in the modelã€‚

 so you need to make it deeper to make it effectively attend over longer sequences in the transformer Excel paperã€‚

 they say that the effective context size is 900 wordsã€‚So you know Bert is 512 wordsã€‚

 transformformer Excel is 900 wordsï¼Œ it's almost doubled but it's still like not maybe the scale that we would like to have if we're modeling like very large inputsã€‚

 so when we get to generation we'll talk about some other models that leverage retrieval to attend over huge data setsã€‚

 this is kind of in a different vein because it ignores large portions of the contextã€‚

 but it allows you at least to first do some retrieval to get important things into your context and then use a standard transformer self-attend over all of those things in the contextã€‚

Okay so I don't want to go into too much detail about how the transformer Excel's particular mechanism worksã€‚

 there have been numerous papers since then that have proposed different ways in which you can increase the context size but I just wanted to give some intuition over how this worksã€‚

 so let's switch back over to the slidesã€‚ğŸ˜Šï¼ŒSo in the Transformer XL paper which I should have assigned but didn'tã€‚

 they give the following figuresï¼Œ so this is their depiction of the standard transformer language modelã€‚

 and I think this is instructive because it shows you what actually is happening when you want to predict tokens that are in different segments of the same documentã€‚

So here the segment lengthï¼Œ the max sequence length for any input is four just for demonstration so you can think of this as analogous to the 512 token limit in Bt So if I have a document that's eight tokens I'm going to split it into two segment of four tokens each and I'm going to run my language model over each segment completely independently of the other one so if I have x1 through x8 here I'm going to do my standard thing run the transformer with the masked attention right so my first token position is not cheating and attending over future words and at every position I'm trying to predict the next wordã€‚

ğŸ˜Šï¼ŒSo this is how it worksã€‚ And then when I get to X sub 5ã€‚

 right So x sub 5 is actually part of the next segmentã€‚ So in the vanilla transformerã€‚

 I have no knowledge of what came before thisã€‚ And so I'm restricted to just attending over myselfã€‚

 I have no context and a master predict X sub 6ã€‚ Soï¼Œ of courseï¼Œ this is not desirableã€‚

 I'm losing out on all this context to make this prediction of X sub 6ã€‚

 And even my prediction of X sub 9 right has no idea about the words that preceded X sub 6ã€‚

 So this is the particular problem that the transformer X sets out to solveã€‚

 It tries to connect these two segmentsã€‚ So I have a larger effective context sizeã€‚ğŸ˜Šã€‚

So here is the second figure from the paper that kind of depicts the model that's proposed by the transformer Excelã€‚

 so this gray out box here corresponds to segment1 and at this point in the computation this has been precomputd so I've already passed this first segment through a batch I've gotten all the hidden states at each of these circles through my transformer and I've cache them so I have them I can attend over them therere in my memoryã€‚

ğŸ˜Šï¼ŒSo when I go to predict X sub6 note in the previous vanilla transformformerã€‚

 I can't attend over any contextï¼Œ but here I have these recurrent computations that allow me to access information from the cached hidden states of the previous batches inputã€‚

So I have access to all of these hidden states that are depicted by these circles and these green lines represent the connections between each layer and these cached hidden states so in the transformer Excel they set it up such that each layer of the current thing that you're predicting attends over cache representations at the previous layerã€‚

 so you can see this kind of downward flow and the way this is set up is essentially this cache representation the cache representations are kind of concatenated with the query in value representation that you get at this sequence and then that kind of is the way that the cache representations influence the computations of this new segment so predicting X sub6 now I have access to all of this information I'm not just blindly making a new predictionã€‚

ğŸ˜Šï¼ŒSo theoretically you could imagine just if I connect all adjacent segments togetherã€‚

 then my prediction at say x sub9 will be fully contextualized and it will be aware of the entire sequenceã€‚

 but practically this isn't trueï¼Œ this mechanism actually does have a specific limit on the number of context tokens that say x sub 9 can condition on sorry when you're predicting x sub9ã€‚

ğŸ˜Šï¼ŒSo if you're interested in the specific mechanism you can look in the paper for more detailsã€‚

 but this is just the high level intuitionï¼Œ so the idea is you cache these hidden statesã€‚

 no gradient information flows to them and you attend over a specific subset of the hidden states at every layer so you don't completely blow up your memory computation for sorry you don't completely blow up your GPU memory with this mechanismã€‚

Okayï¼Œ and yeahï¼Œ just to further demonstrateï¼Œ if you wanted to now predict the next segmentã€‚

 say X sub9 to X sub 12ï¼Œ you're now going to do this recurrent connection to the previous segment that you've computed hereã€‚

Okayï¼Œ so this is just one way in which you can think about extending your window sizeã€‚

 the transformer Excelã€‚And now let's go back to the iPad for a discussion of the next improvement over Bertã€‚

 which wasã€‚ğŸ¤§Electtraã€‚And in Electï¼Œ we were looking at a cheaper objective functionã€‚

And so I think this one will be easy if I just draw it outã€‚ So let's say we have theã€‚The sentenceã€‚

 Janeã€‚Goes toã€‚Maskã€‚Practiceã€‚And so here in my standard Bt architectureã€‚

 right I'm going to have some embedding of each of these wordsã€‚

I'm going to feed them all into a transformerã€‚It's going to spit outã€‚

Some contextualized representations ofã€‚Each word at the final layerã€‚

RightAnd then for this masked word hereï¼Œ I'm going to predict the correct word that should have gone in thereã€‚

 which maybe was baseballã€‚So one issue with this setup that the authors of the E paper noticed was that I'm only getting a prediction signal here for the word that's maskedã€‚

 rightï¼Œ so all of these tokens I'll highlight them in redã€‚The tokens that are unmaskedã€‚

 there's no prediction on top of themï¼Œ there's no classifier right the ground truth words we're not trying to predict anything here and so if we're only masking 15% of our input sequenceã€‚

 our 512 token input sequence were not being very simple efficient because the majority of predictionsã€‚

 85 sorryï¼Œ the majority of tokensï¼Œ 85% of them don't have any sort of classification layer over themã€‚

 those tokens are of course contributing to the contextualized representationsã€‚

 but we might not be getting enough signal out of this entire sequence that we would if we actually had a prediction over every single token in the sequenceã€‚

ğŸ˜Šï¼ŒSo the intuition of the electro model isã€‚Pretty straightforwardã€‚

 So I'm just going to copy this whole thingã€‚And paste it down hereã€‚

So we're not going to change our model architecture at allã€‚

But we are going to get rid of this mask tokenã€‚I don't know if I can delete these highlightsï¼Œ whoopsã€‚

 that's a bad ideaã€‚Okayï¼Œ whateverã€‚Ex sorryã€‚Ohã€‚Okayï¼Œ that was coolã€‚ Alrightï¼Œ so I have thisã€‚

 but instead of introducing this mask tokenï¼Œ this explicit mask token in the vocabulary like Bert doesã€‚

 I'm instead going to replace the things that I would mask with fake tokens like other words from the vocabularyã€‚

 So here I might sayï¼Œ I don't knowï¼Œ it's a random wordã€‚ğŸ˜Šï¼Œ3eã€‚Jane goes to tree practiceã€‚

I guess tree practice is not a thingï¼Œ so now the model is going to do this predictionã€‚å—¯ã€‚

At every single tokenã€‚Predictã€‚Realï¼Œ because this was a real tokenï¼Œ predict realï¼Œ predict realã€‚

 And once it gets to the fake token that I've createdï¼Œ it's going to predictã€‚

So we've replaced this softmax classification over the entire vocabulary that like we were doing in BERT with this simple binary classifier of fake versus real that's applied to every single final layer token representation of this transformer so this is more sample efficient as they show in the paperã€‚

 but one thing that I should note is there's some interesting questions to ask about how exactly you get these fake words right soã€‚

ğŸ˜Šï¼ŒHow do I decideã€‚Whichã€‚Wordsã€‚Fakeã€‚Whichã€‚In words areã€‚å‘ƒã€‚Sorryã€‚IS thinking when I was writing thisã€‚

How do I decide which words to replaceã€‚And with whatã€‚

So the electro paper takes a kind ofgan inspired approach so for those of you familiar with the generative adversarial networks in that it has a separate generator network and a separate discriminator networkã€‚

 so in E our generatorã€‚ğŸ˜Šï¼ŒIs going to be responsible forã€‚Coming up withã€‚Faake wordsã€‚Nowï¼Œ of courseã€‚

 if I have a model and I instead of putting in tree hereï¼Œ put in the word and or something like thatã€‚

 this is gonna to be very easy for the model to detect that and is fake right because it's not even grammatically correct to add in this in this context So and even adding in the word tree is kind of dumb right because like there's no such thing as tree practice maybe there is for like lumberjas butã€‚

Okayï¼Œ but you can imagine some other word that makes literally no senseã€‚

 but is grammatically correct and that that would also be easy for these models right so you might want to give it a challenge to force it to learn to model more propertiesã€‚

 more linguistic properties of the inputã€‚So how do you come up with challenging fake words and what they do is they essentially train a small BRT modelã€‚

And they use its predictions as candidates for fake valuesã€‚

 so you can imagine I put the following through Btã€‚Jane goes to maskã€‚Practiceã€‚And my BERT modelã€‚

 after training predicts a couple of different values for this maskï¼Œ maybe it predicts footballã€‚

 maybe it predicts basketballã€‚Baseballï¼Œ which is the ground truthã€‚

 So I'm just gonna sample from this distributionã€‚ And so maybe I sample footballã€‚

 And so in given just the single sentenceï¼Œ maybe it's impossible to disambiguateï¼Œ rightã€‚

 All three of these choices make senseã€‚But you can imagine if it's surrounded by 512 tokens of contextã€‚

 it's probably much moreï¼Œ there are many more constraints on what could go in here and so sampling football when the context just clearly about basketball is probablyã€‚

 you knowï¼Œ it gives a model some way of distinguishing that this is not the correct word hereã€‚

So you train a small bird model that your generatedï¼Œ that forms your generator and then sampledã€‚

Wordsã€‚From generatorã€‚Forã€‚Fakeã€‚Words for Eã€‚So E is primarily the representation learning part is the discriminatorã€‚

 so this model here which is making these binary real fake predictionsã€‚So the important thingã€‚

Every singleã€‚Tokenã€‚å—¯ã€‚Is associatedã€‚Withã€‚A predictionã€‚Of real fakeã€‚

So the model has to figure out not only which tokens are fakeã€‚

 but also all tokens are all of them either real or fakeã€‚

 so we're not just putting this classifier over 15% of wordsã€‚Okayã€‚

 so now if we switch over to the slidesï¼Œ I wanted to just go over some of the advantages of this setupã€‚

And most of the advantages can be summed up by this plotã€‚

 so on the y axis here you have the glue score which is an aggregate score over I think eight or nine different text classification tasks and so you can see that first we have ElLmo down here at above 70 on this average metric Bt small at 75 so the x axis here is the number of flops required for pretraining and so the larger this valueã€‚

 the more compute is required to reach the particular glue score on the Y axisã€‚ğŸ˜Šã€‚

So we have Bt small and Bt large is up hereï¼Œ it gets close to 85 glue scoreã€‚

 but it takes many more flops than something like Elmoã€‚

So the red line here is the E training curve and you can see that E small is getting about 80 or over 80 glue score same size as the BRT model but many fewer I mean a bigger gain here and similarly we have electro base is also showing a consistent advantage and then you can look at the difference here between electro large and Roberta Roberta with 100000 steps takes a lot of flops E large is far more efficient and it reaches a similar or maybe even slightly higher average glue score so E very sample efficient and takes less compute is much more feasible if you want to implement a pretrainã€‚

ğŸ˜Šï¼ŒMoel yourself from scratchï¼Œ you should go with this route and do not go with the mask language modeling like we have in BRTã€‚

As an asideï¼Œ it would be interesting to see what happens if you get rid of the generator here completely and just randomly sample fake words I suspect that might be might not result in as good performance but would still be not terribleã€‚

 even though the predictions would be mostly easy to determine if something is a fake cell or notã€‚

 but maybe they could be restricted like you sample a fake cell with the same part of speech as the ground truth word so it doesn't rely on some underlying burntlike model to generate these fake candidates anyway some interesting things to think aboutã€‚

ğŸ˜Šï¼ŒAll rightï¼Œ so whoopsã€‚

![](img/bbfb665bd849a609a104cd892c741d57_8.png)

Sorryã€‚

![](img/bbfb665bd849a609a104cd892c741d57_10.png)

Oopsã€‚è¯¶ã€‚Go back to the iPadã€‚So the last thing I wanted to do was tuckã€‚

Very quickly about the final paperï¼Œ Albertã€‚I guess by nowã€‚

 many of you are probably tired of all these Bt related namesã€‚

Hopefully the thing that finally supplants Bt will have no part of its name associated with any sort of Sesame street characterã€‚

Okayï¼Œ but the intuition behind Albert is that moreã€‚

Parameterters doesn't necessarily meanops better modelã€‚

And so in the general trend of Bt related researchã€‚

 we make a model bigger by simply adding more layers on top right so Bert base as a 12 layer transformer Bt large is a 24 layer transformer and you've you know made the model much biggerã€‚

 you can also increase the hidden states of each layer to make the model even biggerã€‚

 but you know maybe common wisdom is that it's needed to add more parameters at every layer at a new set of parametersã€‚

 but Albert kind of bucks that assumptionï¼Œ it proposes to do cross layer parametermeter sharingã€‚ğŸ˜Šã€‚

So what are the parameters at every layer of a transformerï¼Œ rightã€‚

 you have your query key value projection matricesã€‚And you have theã€‚

W matrices associated with the feed forward layersã€‚And basicallyã€‚

 these are the new parameters that you introduce at every layer or every transformer block that you addã€‚

And so in Albertï¼Œ these things are sharedã€‚Acrossã€‚All layersã€‚

 So there's only one distinct query projection matrixï¼Œ only one keyã€‚

 only one value projection matrixã€‚ the feed forward layer matrices are all shared across all layersã€‚

 And so here just adding more layers doesn't doesn't actually increase the number of parameters in your modelã€‚

 So I guess I shouldã€‚ğŸ˜Šï¼ŒMake the explicit comparison that they give in their paperã€‚

 so they show that birdt largeã€‚Has 334 million parametersã€‚And Albert largege or whateverï¼Œ Albertsã€‚

Whatever they call it Tward large I guessï¼Œ which is the same exact architecture but does parameter sharing has only 18 million parametersã€‚

 so that's a huge reduction in the number of parametersã€‚

 Most of these 18 million parameters are likely in the token embeddings right so the word embeddings of the modelã€‚

ğŸ˜Šï¼ŒAnd the Albert paper also proposes a way to reduce the number of perimeter that are contained in the embedding layerã€‚

 but we won't discuss that hereã€‚So if you have 18 million parametersï¼Œ that's greatã€‚

 And if you look in their paperï¼Œ it shows that this modelï¼Œ Albert performs slightly worse than Btã€‚

 but it's still very impressiveï¼Œ but there is another way to scale this Albert settingï¼Œ rightã€‚

 So you only have one set of parametersï¼Œ butã€‚ğŸ˜Šï¼ŒWhat ifã€‚Be makeã€‚Orã€‚Sharedã€‚Set of parameter is biggerã€‚

And so they proposeã€‚The cleverly named Albert Xï¼Œ Xï¼Œ Lï¼Œ which has 235 million parametersã€‚Umã€‚

 even thoughï¼Œ you knowï¼Œ it's still sharing parameters at every layerã€‚

 but it's using a really big hidden dimension of 4096ã€‚And they show that this outperformsã€‚

Without Perth formsã€‚Btã€‚Argeã€‚Which still has more parametersï¼Œ rightï¼Œ 334 million versus 235 millionã€‚

 But this model where you're sharing the same set of parameters at every layer outperforms Bt large on the downstream tasks like glue and so onã€‚

 So this is a pretty cool resultï¼Œ it kind of ties into some more theoretical work that has been proposed in just general machine learning conferences like the universal transformer who also show that you can share parameters at every layer of the transformer And yeahã€‚

 it's an interesting question if the same strategy works for other types of model architectures or maybe the self attentionten here is doing a lot of work and you can get away with sharing these projectionsã€‚

 But yeahï¼Œ pretty interesting paperã€‚ Soï¼Œ yeahï¼Œ I guess that sums it upã€‚

 Those were the four variant that I wanted to discussã€‚ And I think with this lectureã€‚

 we also conclude theã€‚ğŸ˜Šï¼ŒPretraining and fine tuning part of the semester so we've discussed a bunch of different variants of BRTã€‚

 we've discussed how to apply BRT to downstream tasksã€‚

 we've discussed even methods to do more intermediate fine tuning and so on so starting next week we'll start focusing on text generation tasks where we have you know some sort of input and we want to generate some text conditioned on that input so a lot of the models we've seen all these conditional language models just a transformer language model all of those are going to be playing big roles here but there are many different tasks and cool methods proposed that are kind of specialized to each task that we can take a look at so immediately our next lecture we be focusing on open Aã€‚

I set of three language models GPTï¼Œ GPT2 and GPT3ï¼Œ and yeah also briefly talk about what the future of that line of just brute force scaling isã€‚



![](img/bbfb665bd849a609a104cd892c741d57_12.png)

So yeahï¼Œ you can look forward to thatï¼Œ all rightã€‚