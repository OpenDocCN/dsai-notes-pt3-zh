# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÈ©¨Ëê®ËØ∏Â°ûCS685 ÔΩú Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËøõÈò∂(2020¬∑ÂÆåÊï¥Áâà) - P10ÔºöL7.2- BERT - ShowMeAI - BV1BL411t7RV

OkayÔºå heyÔºå everyoneÔºå sorry for the delay„ÄÇ I I thought I was live„ÄÇ

 but I didn't click the go live buttonÔºå so that was doneme„ÄÇ

It would be really great if someone could just type something in the chat box to indicate that they can hear me okay and that everything is working„ÄÇ

ÂóØ„ÄÇI'm not sure from looking at the current thing if anyone is watching this„ÄÇOkayÔºå cool„ÄÇ

 So I'm gonna start„ÄÇ And unlike last time I'm going to just ignore all of the weird error messages that I'm getting from time to time on my streaming software thing„ÄÇ

 So if at any point you lose connection or I freeze or you can't hear me or things are moving slowly or whatever let me know„ÄÇ

 and I will try and fix it somehow„ÄÇüòäÔºåOkayÔºå so today's topic is Bert„ÄÇ

 so we're going to pick off from where we concluded last time„ÄÇ

 if you remember we spent some time talking about Elmo which had this forward language model and the reverse language model we got a couple of questions on on why exactly we need both of these directions„ÄÇ

 maybe I didn't do a good job explaining properlyÔºå so oh there's some freezing lag even now„ÄÇAlright„ÄÇ

 let me see if I can„ÄÇDo something about that„ÄÇHonestlyÔºå I„ÄÇDon't really know what the„ÄÇ

Various settings on this thing do„ÄÇ but let's go with that„ÄÇ Maybe that will help„ÄÇOkay„ÄÇ

 I try to reduce the bit rateÔºå I assume that might help„ÄÇOkayÔºå so anywayÔºå right„ÄÇ

 today's topic is BertÔºå So today„ÄÇFrom„ÄÇElmo to Bert„ÄÇ

And we are also going to be talking about a new training objective to explain this shift„ÄÇ

 And that's going to be from language modeling„ÄÇTo masked language modeling„ÄÇAnd as before„ÄÇ

 if at any pointÔºå you have any questions about the content of the lecture or even about like course logistics or whatever„ÄÇ

 just let me know and I will answer them from the chat box obviously„ÄÇOkay„ÄÇ

 so before we get into mass language modelingÔºå I just wanted to kind of reiterate the purpose of this whole pre training phase„ÄÇ

Gal„ÄÇOf pre trainingÔºå because I think from some of the questions„ÄÇ

 there was a little bit of misunderstanding as to what the role of these various language models is during pre training„ÄÇ

All rightÔºå so our goal of pre training is to use„ÄÇThese„ÄÇsorry„ÄÇBig L Ms„ÄÇAs text encoders„ÄÇ

So you'll hear this term a lot the encoderÔºå and what does that actually mean„ÄÇ

 it's referring to getting representations of words and sentences that can then be plugged into downstream tasks„ÄÇ

 so essentially their goal„ÄÇIs to enable„ÄÇDownstream models„ÄÇTo focus„ÄÇOn the task at handÔºå instead„ÄÇ

Instead of learning„ÄÇOhÔºå that was cool„ÄÇTurningearning„ÄÇHow language works„ÄÇ

And remember why we want the separation rightÔºå Why do we want our language models to handle the you know learning these basic fundamental linguistic properties„ÄÇ

 it's because if we force the downstream model to handle this„ÄÇ

 it has a very limited amount of training data right so let's say for a sentiment analysis task„ÄÇ

 I might have 10000 sentences that are labeled with positive or negative„ÄÇ

 and this is the extent of the data I haveÔºå And so if you expect a model to learn from just 10000 sentences„ÄÇ

 all the various intricacies of English or whatever language your data citizen„ÄÇ

 your model probably isn't going to be able to pick up on all of this right and so that's the whole point of these models like Elmo and BerRT is that we are able to pretrain them on insanely large amounts of language so we can actually reasonably expect the models to pick up„ÄÇ

Properties of language that we expect will be helpful to solve our downstream task„ÄÇ

So in the example from last classÔºå our downstream task was sentiment analysis„ÄÇ

 and our pre training task was left to right and right to left language modeling„ÄÇ

 So just to clarify againÔºå and this is going to be useful when we eventually get to Bt and try and draw some contrasts to Bt in Elmo„ÄÇ

We did the followingÔºå right„ÄÇ We had a sentence„ÄÇ Let's sayÔºå this movie is great„ÄÇ

And we had two recurrent language models that were implemented using this LSTM architecture that„ÄÇ

 againÔºå we're not going to discuss in detail„ÄÇShould have used a different color for these vectors„ÄÇ

 So anywayÔºå let's call this theÔºå these blue vectors come from a„ÄÇleft„ÄÇTo write„ÄÇL M„ÄÇ

And let's call the hidden states of this modelÔºå maybe HL 1Ôºå3„ÄÇSo this is a left to right model„ÄÇ

 And let's think about why we actually need a right to left model as well„ÄÇ

 If our goal is to use this as a text encoder„ÄÇ So if you'd think about„ÄÇThis hidden stateÔºå let's say„ÄÇ

 H L sub is„ÄÇTheÔºå the hidden state associated with the word isÔºå this captures„ÄÇInformation„ÄÇ

About the context„ÄÇThis movie is„ÄÇBut it doesn't include anything about the word greatÔºå rightÔºå No info„ÄÇ

About„ÄÇThe wordÔºå great„ÄÇAnd so why does this not include information about the word great„ÄÇ

 You should think about how the model is actually trainedÔºå right„ÄÇ

 the pretraining objective is to predict the next wordÔºå rightÔºå SoÔºå okayÔºå the video is very pixelated„ÄÇ

Okay„ÄÇLet's see if I can change that„ÄÇLooks like a„ÄÇAre the slide or is the note part hard to read or is it just the camera part in the corner that's pixelatedÔºü

Becauseuse if it's just that partÔºå then it's probably not a big deal„ÄÇ but if it's the„ÄÇ

If it's the notesÔºå then I should probably change that„ÄÇIt's good for you„ÄÇO„ÄÇÂóØ„ÄÇÂóØ„ÄÇüòäÔºåAll right„ÄÇHingan„ÄÇ

OkayÔºå so for some of youÔºå it's good for othersÔºå it's not„ÄÇThat's not a good sign„ÄÇOkay„ÄÇ

 give me one second„ÄÇ Maybe I'll just restart„ÄÇÁöÑ„ÄÇThe streamÔºå it looks like it's set to low latency„ÄÇ

 So maybe that's the reason„ÄÇÂóØ„ÄÇOhÔºå it's good now„ÄÇOkay„ÄÇ

 so I won't restart it if if it turns out to be blurry againÔºå please do complain and I willÔºå okay„ÄÇ

 so it looks like adjusting the bit rate in my streaming thing actually makes a difference„ÄÇüòä„ÄÇ

That's a good note for the future„ÄÇAll rightÔºå so back to the notes„ÄÇ

 remember that in a language modelingÔºå we're trying to predict the next word„ÄÇ

 so I can't actually have this representation at the word is incorporate knowledge of the word great because that's the word I'm trying to predict right And so if I want to use this model as a text encoder of the sentence„ÄÇ

 this movie is greatÔºå which means I'm going to use it for say some sentiment analysis task„ÄÇ

 There's no reason for the representations of these words to be unaware of other words that come in the future right because in the sentiment analysis task„ÄÇ

 My goal is not to predict the next word of each input sentenceÔºås I'm given an input sentence„ÄÇ

 and then I'm asked to predict its sentiment„ÄÇ So there's a discrepancy between the pretraining task where I actually need this property of my current context can't see any part of the future and the downstream„ÄÇ

TaskÔºå which in our example is sentiment analysis in which I have access to the full sentence„ÄÇ

 and I want to do some prediction at the end„ÄÇ So this is why in Elmo„ÄÇ

 we have two directions of language models„ÄÇ And the second direction is intended to get around this issue of the word great here not being represented in this single token representation for is„ÄÇ

So in our left to right language modelÔºå you can think about it essentially as just reversing the inputs and then learning a language model„ÄÇ

 so intuitively I just have great is movie there„ÄÇüòäÔºåAnd I do the exact same thing„ÄÇ

So this is the left or the right to left language model„ÄÇ

 It's just trained to predict the the previous word here„ÄÇ

 I'm just writing it in the exact same format„ÄÇ And let's just make these vectors green„ÄÇ So in this„ÄÇ

In this settingÔºå the right to left model„ÄÇAnd we'll call this one's hidden states or one through N„ÄÇ

 if you think about„ÄÇThis vector hereÔºå the one associated with is„ÄÇ

What information does this vector haveÔºüWellÔºå it captures„ÄÇThe contextÔºå great„ÄÇIs„ÄÇRight„ÄÇ

 so if I were to concatenate both this blue vector and green vector for is together„ÄÇ

 now my vector for is has a representation of both the previous context and the future context„ÄÇ

 and importantlyÔºå the pretraining taskÔºå the predict the next wordÔºå it never has to cheat right„ÄÇ

 because we're still in each of these separate directionsÔºå we have a valid language model„ÄÇ

 But when we use this to solve our downstream task„ÄÇ

 we can get around this by the limitation of only seeing the previous words by concatenating the left to right and right to left representations together„ÄÇ

 So let's actually take a look at how we use this in our downstream task„ÄÇ

 So this is a similar picture to the one we had last weekÔºå except now since we've learned about Elmo„ÄÇ

 it's a little more„ÄÇüòäÔºåSpecificÔºå whoopsÔºå this movie is goodÔºå rightÔºüThis movie„ÄÇ

I I should have drawn these out fartherÔºå okay„ÄÇIs good„ÄÇIs it goodÔºå great„ÄÇOkay„ÄÇ

 I keep forgetting great„ÄÇSo here I'm going to have„ÄÇÂëÉ„ÄÇThese„ÄÇConcatenated vectors„ÄÇ

 which are the word embeddingsÔºå and half of them are going to be the part from the right to left language model„ÄÇ

 and the other half are going to be from the left to right language model„ÄÇ

So here you can see how let me actually make this more clear and write downstream usage„ÄÇ

So now in our pre training taskÔºå we're training all the word embeddings and stuff from scratch„ÄÇ

 but in our downstream testÔºå we initialize our word embeddings with these concatenated Elmo representations„ÄÇ

 and then what we do is have a task specific model on top of it„ÄÇ

 So in our example from last time we used a recurrent neural language model„ÄÇ sorry„ÄÇ

 a recurrent neural networkÔºå which does the following„ÄÇ

But I just wanted to make it more clear and show you that this network could look like anything„ÄÇ

 So it takes in the ElLmo representationsÔºå and then we know that the final hidden state here„ÄÇ

 this thing represents the whole sentenceÔºå This movie is great and we can use it to predict positive and how do we do this softm layer„ÄÇ

 right we've gone over this before„ÄÇThis is just one instantiation of a downstream model„ÄÇ right„ÄÇ

 I could take these same„ÄÇEbeddings here„ÄÇAnd I could do I could have a different downstream model„ÄÇ

 which maybe takes the element wise average of all of these vectors„ÄÇ

It's called this element wise average„ÄÇAnd then has a softm layer on top of this„ÄÇ

To predict positive rightÔºå this could be any composition function„ÄÇ

 this downstream model and the important part with ElLmo is that I'm just basically feeding the word embeddings that are input to this downstream model instead of randomly initializing them„ÄÇ

 I am initializing them with ElLmo and I'm keeping them basically fixed with the exception of this because in ElLmo we're aggregating information across multiple hidden layers„ÄÇ

üòäÔºåOkayÔºå so I kind of want to stop here„ÄÇ And maybe this is a good time to take some questions„ÄÇ

 So let's see the first question„ÄÇ Could we not just„ÄÇ

Can not consider the ordering here and learn it in the fine tuning stage That way„ÄÇ

 we wouldn't need bidirectional network„ÄÇ So the order„ÄÇ

 the left or right order of the words in the sentence give us a lot of information about the syntax and semantics of various words and phrases and human sentences„ÄÇ

 rightÔºå So one very important property in sentiment analysis is a negation scope„ÄÇ So as an example„ÄÇ

So„ÄÇMaybe I should distinguish this from the rest of the notes„ÄÇIn response to question„ÄÇ

Think about theÔºå the sentence„ÄÇ I don't know„ÄÇ I love„ÄÇThe acting„ÄÇBut the„ÄÇRt„ÄÇOf the movie„ÄÇ

It was terrible„ÄÇSo in this settingÔºå if for a model that does not consider the order of the words in this sentence„ÄÇ

 I might know that the word butt existsÔºå but I don't know what words actually the word but is applied to right but is a contrastive conjunction„ÄÇ

 So it's kind of affecting a negative over whatever is it follows in the clause that it's used in„ÄÇ

 So here we have but the rest of the movie was terrible right„ÄÇ

 So this is kind of the scope of this conjunction„ÄÇ and I would love for a model that has information as to the order of these words to pick up that all of the words in this maybe I should have used highlighting for that„ÄÇ

All of the words in this highlighted section are kind of negated through the use of this word„ÄÇ

 but so this is one particular property that's very useful for sentiment analysisÔºå but in general„ÄÇ

 like the order of the words is extremely important for learning rich representations„ÄÇ

 and so we really need to put this into the pre-training phase and not just the fine tuning phase„ÄÇ

OkayÔºå the second questionÔºå is it possible to train my downstream task model over a data set after it is being pretrained by Elmo or BertÔºü

ÂóØ„ÄÇSoÔºå yeahÔºå in the case of ElmoÔºå the downstream task model is completely separate from the pretrain model„ÄÇ

 rightÔºå So if you look at these two examples in this first exampleÔºå the downstream model„ÄÇIs an RN„ÄÇÂóØ„ÄÇ

Classifier„ÄÇAnd in this example„ÄÇDownstream„ÄÇModel„ÄÇIs aÔºå I don't knowÔºå averaged„ÄÇInbedding„ÄÇ

 classifier or whatever you want to call thisÔºå actually„ÄÇThis model is usually called a neural„ÄÇ

Bag of words„ÄÇBecause it is not considering the order of the words„ÄÇ

 although the pretrain representations are„ÄÇ So yeahÔºå to get to this question„ÄÇ

 in the case of the in ElmoÔºå the downstream model is always different from the pre training architecture„ÄÇ

 in BertÔºå this is not the case„ÄÇ So maybe this is a good segue to the next part of this lecture„ÄÇ

So yeahÔºå I meanÔºå many of you have you knowÔºå expressed some concerns or questions about why do we need to have these two language models„ÄÇ

 rightÔºå the ElLmo approach„ÄÇOf two„ÄÇSeparate„ÄÇBms„ÄÇThat earth then„ÄÇCan catnated together„ÄÇ

Is a little hacky„ÄÇSo what we really want is to accomplish our goal of having representations that are aware of every single other word in the sentence„ÄÇ

 not just the words that preceded itÔºå but maybe we do this in a single model as opposed to having two separate models„ÄÇ

Can we„ÄÇAccomp„ÄÇThe same goal„ÄÇWithin a single model„ÄÇAnd so here is where we're going to move to a different pre training objective„ÄÇ

SoÔºå change„ÄÇPre training„ÄÇObjective„ÄÇFrom„ÄÇLanguage modeling to masked language modeling„ÄÇ

And so we've spent a lot of time this semester on the language modeling objective„ÄÇLuckily„ÄÇ

 mass language modeling is like veryÔºå very similar in terms of how it's implemented all of the different components of neural networks we've seen like softm layer„ÄÇ

 transformerÔºå etc ceter„ÄÇ those are all useful for implementing mass language models„ÄÇ

 the only thing that changes is the objective function„ÄÇüòäÔºåSo in mask language modeling„ÄÇThe„ÄÇ

 the change compared to inÔºå you knowÔºå standard left to right language modeling is„ÄÇ

We are given a full sequence of wordsÔºå not just a prefix„ÄÇNot just prefix„ÄÇWhere„ÄÇ

X percent of the words„ÄÇHave been masked out„ÄÇAnd instead of the model predicting the next word„ÄÇ

Instead of predicting„ÄÇThe next word„ÄÇWe„ÄÇOnly predict„ÄÇMasked words„ÄÇSo when I draw this out„ÄÇ

 hopefully it'll become a little more clear as to how this actually„ÄÇ

Helps us in modeling both sides of the context within a single model„ÄÇ

 So let's take a look at our standard language modeling example„ÄÇ

 the students opening their books or or whatever it was„ÄÇ So we have students„ÄÇüòäÔºåOpened„ÄÇTheir books„ÄÇ

And so„ÄÇInstead of having the word opened here in our training dataÔºå we are going to mask this out„ÄÇ

 So this is going to be replaced with the special mask token„ÄÇ

And so now we're going to just do a standard transformer on top of these the word embeddings associated with this input„ÄÇ

 so note that the mask token here is considered a separate word type in our vocabulary„ÄÇ

 the mask token is also associated with its own word embedding and it's treated just like any other word„ÄÇ

So we knowÔºå likeÔºå in a transformer„ÄÇWe are going to be projecting these things toÔºå you know„ÄÇ

 query key value„ÄÇ We're going to do a lot of self attention„ÄÇ

All the fun stuff we've talked about before with transformers„ÄÇ

So I will just summarize all of that into„ÄÇTranformer„ÄÇ

Where N here is the number of layers that are in this transformer„ÄÇ And then at the end of the day„ÄÇ

 I should get more space„ÄÇone sec„ÄÇIt's true„ÄÇYeahÔºå so at the end of the dayÔºå I„ÄÇ

After all of the layers of my transformerÔºå I get„ÄÇA word representation associated with each of the vectors„ÄÇ

 each of the original words in my sentenceÔºå rightÔºå this is the output of a self attention layer and transformer block„ÄÇ

And so note that this vector hereÔºå the one that I will highlight in red„ÄÇ

 corresponds to the mask token„ÄÇ So instead of doing a prediction at every single one of these tokens token representations„ÄÇ

 as we would in a standard left to right language model here„ÄÇ

 I'm only making one prediction at the position that corresponds to the mask token„ÄÇ

 and I'm going to predict„ÄÇüòäÔºåThe ground truth word that should be occurring in this masked position„ÄÇ

And how do we do this predictionÔºüSimp by using this Huffm layer„ÄÇ

 just as we saw in neural language models„ÄÇSo this in essence„ÄÇ

 is mask language modeling and once we switch over to the slides in a little bit„ÄÇ

 I'm going to talk more about specifically different masking strategies„ÄÇ

 what we need to think about when we're preproces our data for this task„ÄÇüòäÔºåBut note that the„ÄÇ

 the cool thing about this model is that each of these representations here„ÄÇAre fully contextualized„ÄÇ

 so„ÄÇHow do I say thisÔºüAll of the„ÄÇFinal layer„ÄÇRepresentations„ÄÇ2„ÄÇFully„ÄÇContextualized„ÄÇ

This means that they are aware„ÄÇOf„ÄÇWords„ÄÇIn the past„ÄÇ

 so the standard prefix that we see in neural language models„ÄÇAs well„ÄÇAs words in the future„ÄÇ

 So this is„ÄÇThis is a difference from„ÄÇThis is the main difference between this setup and the left to right language modeling setup„ÄÇ

 So we can't use mass language modeling to give us the probability of a sentence right because it is breaking our chain rule assumption of the like we can't do this product of conditional probabilities anymore because now every single representation is aware of everything else in the every other word in the sentence„ÄÇ

 both past and futureÔºå so use this for many of the applications that we talked about at the very beginning of the semester for language models and it's hard to use this model to actually generate text„ÄÇ

 although we will discuss some models in later weeks of the semester that do use these kinds of models to generate text„ÄÇ

 but for now the important thing to take away from this setup is that it gives us a pretraining„ÄÇ

In which we don't need to have two separate directions because the model here is at every position aware of the words at every other position and it can do this because it's not cheating in the sense that its input is actually masked so it has no idea what the actual word that goes into this mask token is and so that makes it a valid prediction task„ÄÇ

Do we then assign the embedding for mask to the word predicted This is a question No so this is a good question In mask language modeling„ÄÇ

 it generally proceeds as we mask out some percentage of the tokens in a largeish input so maybe like 512 words or so and maybe roughly I don't know„ÄÇ

 like maybe 70 of them are masked we do all of the predictions of the masked words just using a soft mask layer at each position and there's there's no like feeding of the predicted word or the ground truth word back into the model for another round of prediction„ÄÇ

üòäÔºåThat actually is a strategy that is used by some of these models when they're trying to generate text„ÄÇ

 but that's not used in the pretraining tasks„ÄÇ So this essentially all these predictions happen at one shot and that's it„ÄÇ

 So unlike ElmoÔºå which is using language models which can technically generate text and give us probabilities of sentences„ÄÇ

 in Bt the entire focus is using this architecture„ÄÇ

 we don't care about getting a probability if a sentence if all we want is powerful representation of the words in that sentence right so this pretraining objective is sufficient to make the model learn important linguistic properties if you just think about this task students mask their books„ÄÇ

To give a good prediction of this mask tokenÔºå I have to know that it's very likely that a verb will appear here„ÄÇ

 right in this particular context and also I have to know a bit of world knowledgeÔºå right„ÄÇ

 I have to know that there are only a certain subset of verbs that are appropriate to predict in this particular context„ÄÇ

 probably associated with students and books and schools and learning and so on„ÄÇ So yeah„ÄÇ

 you can see intuitively how this objective function„ÄÇ

 very similarly to the original language modeling objective is trying to get the model to pick up important linguistic properties of the text„ÄÇ

üòäÔºåWhat if the masked embeddings are not matching with the actual word embedding that was maskedÔºüÂóØ„ÄÇI„ÄÇ

 I don't that question„ÄÇ Maybe you can reword it and I will answer it after that„ÄÇOkay„ÄÇ

 so just to clarifyÔºå same„ÄÇTraining„ÄÇProcedure„ÄÇI guess I should say loss„ÄÇAs„ÄÇN Lms„ÄÇ

So we're still using the cross entropy lossÔºå rightÔºå We're minimizing„ÄÇThe negative log likelihood of„ÄÇ

Of the masked„ÄÇTokens„ÄÇSorryÔºå I should say the„ÄÇÂëÉ„ÄÇOf the ground truth„ÄÇUnmasked tokens„ÄÇso in this case„ÄÇ

 the negative log likelihood of opened here„ÄÇÂóØ„ÄÇYeahÔºå so all of the stuff you learned from„ÄÇ

Neural language models applies when you're training thisÔºå unlike neural with the mask„ÄÇ

 like we did in the transformer language modelsÔºå remember„ÄÇ

 we had that mask that was preventing the model from attending over future tokens here„ÄÇ

 it's completely allowed to attend over future tokens because the whole point is that the input is actually it doesn't contain the information about what you're trying to predict„ÄÇ

 So we're not cheating in this sense„ÄÇSo when we're going from Elmo to Bert„ÄÇ

We're making two fundamental changes„ÄÇFirstÔºå we're going from two unit directional„ÄÇ

L Ms to one masked L M„ÄÇAnd secondÔºå we're going from recurrent„ÄÇModels„ÄÇTo transformers„ÄÇAnd finally„ÄÇ

 third„ÄÇBirt„ÄÇWas pre traineded„ÄÇOn a lot more data„ÄÇSo these are the critical changes when we're going from Elmo to Bt„ÄÇ

 so just to be completely clear„ÄÇIf we take our same example from beforeÔºå which was what was it„ÄÇ

 the movie is good„ÄÇThe movie is greatÔºå okay„ÄÇSo we have the movie is great„ÄÇ

And say I have a pretrained masked language model that I've pre traineded on auto ton of data„ÄÇ

 Then the question isÔºå how do I use this in a downstream model right to get these contextualized word representations„ÄÇ

 and I can just feed this sentence into my pretrained mask language model without masking any of the words in the input So„ÄÇ

Just abstract away hereÔºå pre trained„ÄÇMasked„ÄÇLM„ÄÇÂóØ„ÄÇAnd this is going to give me„ÄÇMy contextualized„ÄÇ

Token level vectors„ÄÇContextualized„ÄÇToken vectors„ÄÇIn one„ÄÇPass through the model„ÄÇ

So I don't have to actually„ÄÇFeed it through two separate models and concatenate in in a mass language model„ÄÇ

 I just feed my downstream sentence„ÄÇ This movie is great without masking any of the tokens and get the representations out of it„ÄÇ

 So this is a much cleaner way of getting these contextualized representations„ÄÇüòäÔºåOkay„ÄÇ

 back to some questions„ÄÇApart from using two models verse one model„ÄÇ

 is there any other advantage of using mass language modeling So there are some„ÄÇ

 if you look at the Bt paperÔºå there are some ablation experiments that compare the approach used in Elmo with the left or right prediction and right to left prediction concateninated together versus using a mask language model„ÄÇ

 I think they showed some gainsÔºå maybeÔºå but I know there's other work that shows that just simply scaling up the Elmo approach of training these two separate direction models on like the same scale data as Bert was trained on also reach a similar performance„ÄÇ

 but yeahÔºå it's mostly like cleanerÔºå I think to just have a single model to deal with„ÄÇ

 and its I guess a more natural tasks where we don't care about actually generating text or predicting the next word or modeling the problem„ÄÇ

üòäÔºåOf a sentenceÔºå which are all features of a unidirectional language model that we don't need if our whole goal is just transfer learning„ÄÇ

 So this mask language modeling thing doesn't have those same use cases„ÄÇ

 but it it does do exactly what we want for you knowÔºå learning a powerful text encoder„ÄÇ So yeah„ÄÇ

 I would encourage you to check out the birdt paper for some more ablations„ÄÇ

 And I'll try and put some links to the other papers that I mentioned„ÄÇOkayÔºå so another question„ÄÇ

 why do prediction at allÔºå why not just use the current architecture to generate embeddingsÔºü

I'm not sure what you mean by why do prediction at all„ÄÇ

 So the masked language model needs an objective to learn good parameters of the transformer right so if I did not have this prediction task here of predicting opened„ÄÇ

 then the parameters of this model wouldn't learn any sort of linguistic properties right because I need something to adjust the models's parameters such that it's able to extract all this important information from the text„ÄÇ

 So similarly to how in language models we're predicting the next word here„ÄÇ

 we're predicting the masked wordÔºå but they have the same effect of forcing the model to try and at least at some level understand the text„ÄÇ

Are we learning the embeddings alongside the contextualized representations„ÄÇ

 even though they are not used hereÔºå YeahÔºå so with all of these pretraining tasks with with mass language modeling and with language modeling with a large scale task„ÄÇ

 we start all our parameters from scratch the word embeddings and we train them on this huge data set„ÄÇ

 So yeahÔºå we learn those and in the downstream task for Elmo„ÄÇ

 we we basically just throw everything away except the contextualized embeddings„ÄÇ

 we're going to see what happens in BertÔºå it's actually quite different„ÄÇ

 So maybe I'll move on to that now and I'll take more questions later„ÄÇ

 So the last thing I want to talk about before switching over to slides is how we actually use Bert in a downstream task„ÄÇ

Or how do we use„ÄÇSo hereÔºå Bert is the first large scale pre trained mask language model„ÄÇ

 so I'm just using it as a proxy for a pre trained mask language model„ÄÇFor a downstream task„ÄÇ

So we talked briefly about this at a high level last time„ÄÇ

 but I wanted to be more specific this time now that we've covered the different objective functions„ÄÇ

 So remember in ElmoÔºå we do the transfer strictly at the word embedding levelÔºå right„ÄÇ

 we still as we saw in„ÄÇThis figureÔºå we still have this entire downstream model here or the downstream model here„ÄÇ

 And all these parameters are trained from scratch„ÄÇIn Bt„ÄÇ

 the pre training model and the downstream model are much more closely linked„ÄÇ In fact„ÄÇ

 they're exactly the same„ÄÇPretrained„ÄÇArchitecture„ÄÇIs the same„ÄÇAs the„ÄÇDownstream model„ÄÇ

Or I should sayÔºå almost the same„ÄÇAlmost„ÄÇOkayÔºå so what does this mean„ÄÇ

 Let's move forward and take our same example sentenceÔºå this„ÄÇMovie„ÄÇIs ohÔºå sorry„ÄÇ

 I want to add one more thing to„ÄÇ So in Bert„ÄÇWe basically want to„ÄÇ

So let's say our example is sentiment analysis„ÄÇOne interesting thing that you might have noticed from the the Elmo examples is that the role of the downstream model is essentially to compose together all these pre trained embeddings into a single vector and then put a softm layer over that composed vector„ÄÇ

 rightÔºå So„ÄÇüòäÔºåIn this exampleÔºå this was the composed vector„ÄÇ

 And here the final hidden state of this RN was the composed vectorÔºå but„ÄÇIn BtÔºå what they do„ÄÇ

 instead of forcing you to come up with this external composition function that is trained from scratch on the downstream data„ÄÇ

 they simply add a special token to the beginning of every sequence during both pretrain and a downstream task„ÄÇ

 So they're going add a special token„ÄÇüòäÔºåAnd„ÄÇBeginning„ÄÇOf every sequence„ÄÇAnd in Bert„ÄÇ

 this is called the CÔºå L S token„ÄÇWhich I assume stands for classification or something like that„ÄÇ

So the role of this special CLS token is that„ÄÇIf for a downstream task„ÄÇ

 I want to do a classification problemÔºå I can simply just put my softmax layer over the token representation for the CS token„ÄÇ

 and we know that in a transformer in a mask language model„ÄÇ

 every token representation is contextualized with every other token in this input sentence„ÄÇ

 And so the CS token representation contains some information about the entire rest of the sentence„ÄÇ

 so you can view it as kind of already doing the composition process that in the ElLmo case„ÄÇ

 we would expect a downstream model trained from scratch to be doing„ÄÇ

 and this enables us to share the same architecture between pretraining and fine tuning without adding any external composition modules like an RNN or an elementwise average or so on„ÄÇ

üòäÔºåSo now to make that more clear„ÄÇWe are adding this C L S token to the beginning of every input sequence„ÄÇ

 And then we have this movie is great„ÄÇAnd so just like the mask tokenÔºå this C„ÄÇ

 LS token is associated with a word embedding„ÄÇ It hasÔºå you knowÔºå its own entry in the vocabulary„ÄÇ

And so we do the normal thing„ÄÇ We have our word embes„ÄÇThey get passed all through the pretrained„ÄÇ

Masked L M„ÄÇAnd I get as outputÔºå I'm going need to move this„ÄÇThe contextualized embeddings„ÄÇ

So in this caseÔºå I have„ÄÇOne extra oneÔºå because I have this special CÔºå L S token„ÄÇ So let me quickly„ÄÇ

Move this down„ÄÇAnd the important thing in Bert is that this token here„ÄÇ

 the contextualized CLS representationÔºå I'm just going to put a softm layer on top of this„ÄÇ

To predict„ÄÇPositive„ÄÇSo there's literally no downstream architecture here right we're using the exact same parameters of the mask language model to perform this composition„ÄÇ

 and we're just going to assume that all of their important information about the sentence regarding its sentiment will be squeezed into the CLS vector once I perform fine tuning so„ÄÇ

Now that I've done thisÔºå I back prop„ÄÇThe error signal„ÄÇFrom„ÄÇThe sentiment„ÄÇClassifier„ÄÇThrough„ÄÇEntire„ÄÇ

Pretrained„ÄÇMaskked L M„ÄÇ So I'm actually adjusting the parameters of my mask language model in the downstream task using the signal that I get from this sentiment analysis downstream data set„ÄÇ

ÂóØ„ÄÇSo this process is called fine tuning„ÄÇOkayÔºå so I hope the difference between this setup and the Elmo setup are clear here„ÄÇ

 there's no external„ÄÇDownstream model„ÄÇIn factÔºå the„ÄÇThe only thing we're adding„ÄÇOnly new component„ÄÇ

Is a single„ÄÇSoft max layer„ÄÇAnd this is used to predict the sentiment„ÄÇ

And the second thing is that we're fine tuning this pretraining language modelÔºå right„ÄÇ

 We're not leaving it fixedÔºå as we do in in Elmo„ÄÇ So hopefully at this point„ÄÇ

 you have an understanding of how in this Bt case we're using the same exact model architecture for both the pre traininging task and the downstream task„ÄÇ

 And you have some idea of why this might be good alsoÔºå rightÔºå because„ÄÇ

The pre training task gives us a very powerful composition function that's been trained over„ÄÇ

 you knowÔºå billions and billions of words„ÄÇüòäÔºåWhyÔºå why would we want to actually sh function on our much smaller label data set as we do in ElLmo„ÄÇ

 it makes a lot of sense to share these parameters across both pretraining and fine tuning„ÄÇ

 and so these are the critical differences here„ÄÇOkayÔºå so it looks like we have another question„ÄÇ

 Can you have a more complex downstream model in Bert or only the CLS Somax would work„ÄÇ Yeah„ÄÇ

 that's a good question„ÄÇ So you canÔºå in fact use the final layer representations in Bt just like you would do in Elmo and use them as word embeddings to some downstream model„ÄÇ

 but in practiceÔºå no one does this because you don't see any gains from doing this and yeah„ÄÇ

 it turns out that these huge transformer models are good enough for most tasks that simply putting a softmax layer on top is enough to get great performance„ÄÇ

 So we'll look a little more at the results in a bit„ÄÇ

 but I should also mention that the CS token here is useful for classification tasks„ÄÇ

 but when we move on and we'll do this in the next lecture to other types of tasks like sequence labeling or question answering these other„ÄÇ

üòäÔºåRepresentations here„ÄÇThe other token representations are also useful for putting classifiers on top„ÄÇ

ÂóØ„ÄÇOkayÔºå so I think at this pointÔºå I will switch over to the slides„ÄÇOh my god„ÄÇ

 it's already been 50 minutes„ÄÇReally bad at making these short„ÄÇ



![](img/c6f24929fd8fd39a2cd55342d10b650f_1.png)

OkayÔºå so yes„ÄÇSoÔºå here„ÄÇ

![](img/c6f24929fd8fd39a2cd55342d10b650f_3.png)

ÂóØ„ÄÇThis screened„ÄÇ

![](img/c6f24929fd8fd39a2cd55342d10b650f_5.png)

OkayÔºå so I want to talk more now about the specific implementation of BRTÔºå how it was trained„ÄÇ

 what data it was trained on„ÄÇ I saw theres a question above on training data as well„ÄÇ And yeah„ÄÇ

 basically give some more concrete details as to this„ÄÇ

 Another thing I want to talk about is the pretraining method for the C L S token„ÄÇ

 although this has been shown after the Bt paper to be less important„ÄÇ

 But for those of you who actually read the birdt paperÔºå you might be wondering about this„ÄÇOkay„ÄÇ

 so we talked about this the problem with prior methods like ElLmoÔºå for instance„ÄÇ

 that they use only the current representation only modelsÔºå the prefix but not the future„ÄÇ

 remember that in language modelingÔºå the pre-training task„ÄÇ

 we need to have this directionality because otherwise we'll be cheating„ÄÇ

 predicting future words we're not going to be able to actually predict these good wellform probability distributions„ÄÇ

 but we don't care about thisÔºå if we're only interested in using representations out of the model for transfer learning„ÄÇ

OkayÔºå and yeahÔºå the the problem with just naively porting over language modeling to a bidirectional setting where maybe you don't do the masking of future words is that you're cheating„ÄÇ

 So your model isn't going actually learn anything useful about the input right„ÄÇ

 it can just say that ohÔºå well I have this information already in my input„ÄÇ

 So let me just get the information of the next word and predict it„ÄÇ

 So it's not actually going to learn anything useful„ÄÇ

 So this is the motivation again for the mask language models where we mask out some percentage of the input words and then predict the mask words„ÄÇ

üòäÔºåSo one good question to ask here is how do I determine how many words in this pretraining task to mask out So in this example„ÄÇ

 the man went to the mask to buy a mask of milkÔºå we can reasonably make some predictions about what these masked words would be right But imagine I masked out every word in the sentence except the word went„ÄÇ

Then the model is going to have no information to predict these mask tokensÔºå right„ÄÇ

 because everything is masked„ÄÇ you have very little meaningful words to make these predictions„ÄÇ

And so you want to choose a mask percentage such that you have enough context„ÄÇ

 The model has enough contextÔºå such that it can make meaningful predictions„ÄÇ

 but we also want to be efficient right so you can think about if I have 500 words in my input and I only mask a single one of them I only get one prediction to backpro through my model and that might be very inefficient when I'm trying to train this over billions and billions of words if in every single example„ÄÇ

 I only get an error signal from one word„ÄÇ So you basically want to balance out the efficiency of training and how fast your training process is which you can increase by increase the mask percentage with the the context limitations if I mask out too many words„ÄÇ

 then I can't effectively predict the ones that are masked„ÄÇSo in the bird paper they use 15%„ÄÇ

 so 15% of the tokens and the input are masked and predicted„ÄÇ

They also do some like weird other replacements to the mask token„ÄÇ

 so some of the time instead of replacing a word with a mask token„ÄÇ

 they replace it with a random word or some of the time they just replace it with like the same word„ÄÇ

 so it has no effect„ÄÇAnd all of these things„ÄÇ So the bird paper was done by Google researchers who have access to tremendous amounts of compute„ÄÇ

 So I assume they tried many variants of these strategies and found that this one achieved the best downstream performance„ÄÇ

üòäÔºåÂóØ„ÄÇOkayÔºå so before I get to this questionÔºå there's a couple questions in the chat box now„ÄÇ

 What do you mean by backt from theÔºå I guess that means backdrop from the sentiment classifier to the entire pretrain model is it from the softm layer back to the„ÄÇ

 so by sentiment classifierÔºå I'm referring to the softm layer because in the Bt setting„ÄÇ

 the only parameters specific to the downstream task of sentiment analysis are the softm layer„ÄÇ So„ÄÇ

 yesÔºå those areÔºå that's what I mean„ÄÇ sorry for the confusion„ÄÇ

How does model fusion work in case of BtÔºå so many state of the art models are variants of Bt„ÄÇYeah„ÄÇ

 we'll get to the different variants of Bert a little later on„ÄÇ

 I still haven't decided which ones to coverÔºå but realistically there have been many models that have come after Bert and the ones that have made impact are not because of any significant changes to the mass language modeling objective or to the model architecture„ÄÇ

 but they are essentially just bigger models that have been trained on more data„ÄÇ

 So I'll probably assign a paper called RobertaÔºå which is a variant of Bt that actually simplifies many of the things in the Bt paper and trains on like much larger amount of training pretraining data and gets better performance across the board„ÄÇ

 so yeahÔºå there have been many variants of BRT proposed„ÄÇ

 but I think the actual impact of those variants and„ÄÇüòäÔºåCon that they offered on top of Bt is„ÄÇ

Generally lowÔºå except for just showing that more data and bigger models„ÄÇOkay so in the Bt paper„ÄÇ

 if you read itÔºå you might have been a little confused about what this next sentence prediction task is and so remember that in Bt we have this CLS token and I haven't actually described how we pretrain the CLS token right do we actually want to mask out the CS token right it doesn't make sense and it doesn't offer us any new information to help our predictions of the masked words„ÄÇ

 So in Bt they actually put a classifier a separate softm layer on top of just the CLS token which is responsible for some sequence level of predictions„ÄÇ

 not just token level of predictions„ÄÇ so specifically they have this task of next sentence prediction„ÄÇ

 So in Bt's pretraining each input is two sequences of text not necessarily sentences although in this slide it says sentences„ÄÇ

 two sequences of text that are taken from either the same„ÄÇ

Document and follow each other in the same document„ÄÇ

 Or they're just randomly selected two chunks from different documents or yeah„ÄÇ

 different places of the same document„ÄÇ So the task then for the classifier on top of the Cs vector is can you determine whether the second sequence actually follows the first sequence in the real text or whether the second sequence is completely different and from a completely different document or something like that„ÄÇ

 So in the example here„ÄÇThe man went to the storeÔºå he bought a gallon of milk„ÄÇ

 these sentences might have occurred right next to each otherÔºå and so this is a positive class label„ÄÇ

 whereas the man went to the store and penguins are flightless„ÄÇ

 this second sentence is a random randomly sampled sentence„ÄÇ

 and so we would expect the classifier on top of the CLS token to predict that it's not the actual next sentence„ÄÇ

So in the Roberta paper which I mentioned beforeÔºå they actually removed this separate pretraining objective of next sentence prediction„ÄÇ

 They kept the CLS tokenÔºå but they just didn't put anything special over it and it turns out that it doesn't actually it's not required to get the performance„ÄÇ

 So I would have gone into more detail about thisÔºå but it seems like the mask language modeling task is by far the most important thing and all these secondary objectives are not as impactful„ÄÇ

OkayÔºå so this slide just kind of summarizes the input to the original Bt model„ÄÇ

 and it's also a good summary ofÔºå againÔºå the inputs to a transformer„ÄÇ So we start„ÄÇ

 we have this sequence„ÄÇ starting again with a CS token„ÄÇ My dog is cute„ÄÇ The separator token„ÄÇ

 which isÔºå rememberÔºå now we have two sequences that are stuck together„ÄÇ He likes playing„ÄÇüòä„ÄÇ

And note that this token hereÔºå the INGÔºå this is an instance of the subword encoding right„ÄÇ

 the stem play and this suffix ING„ÄÇ So we talk briefly about this before of doing this to reduce the incidence of like unseen words or having a way of modeling words that we at test time that we don't see at training time„ÄÇ

So we have our token embeddings for each of these tokens in the sequence„ÄÇ

 these are analogous to our word embeddingsÔºå we also have the position embeddings which we know are necessary in transformers and in BRT we have these segment embeddings that are supposed to help the model reason about the two different sequences in the same input to help with the next sentence prediction„ÄÇ

OkayÔºå and so all of these representations are summed upÔºå these different embeddings„ÄÇ

 and only then are they passed into the transformer for mask language modeling„ÄÇüòäÔºåSo in BERT„ÄÇ

 they use the word peace method for achieving the subword segmentation„ÄÇ

 so they have a vocabulary of 30Ôºå000 of these words and sub wordss like ING„ÄÇOkayÔºå and the model„ÄÇ

 we've already seen this„ÄÇ it's a transformerÔºå It has multiheaded self attention„ÄÇ

 the feed forward layersÔºå this block we've seen before„ÄÇ So nothing new modeling wiseÔºå which yeah„ÄÇ

 is pretty interesting„ÄÇ The transformer originally proposed for machine translation then port it over to language modeling and mask language modeling and has had success in both of these areas„ÄÇ

üòäÔºåSo Bert was trained on Wikipedia and also this book corpus„ÄÇ

 which is a data set of unpublished novelsÔºå so the total combined number of words in these dataset sets is over 3 billion which is a lot or at least was a lot at that time nowadays we have models like the ones from open AI that are trained on like hundreds of billions of words these numbers might seem small in retrospect„ÄÇ

 but this was a huge number for the timeÔºå which was„ÄÇAgain„ÄÇ

 like literally last year or like one and a half years ago„ÄÇ So things move really fast in this field„ÄÇ

And they trained this on TUsÔºå so not GPUus for four days I think at the time people did the at the time Google did this there were people on Twitter I remember who were estimating how much this would cost like a normal researcher to run on something like AWS„ÄÇ

 and it was like training on a four GPU box for six months or something like that would be the equivalent of this„ÄÇ

 So really something that benefited heavily from the amount of compute that Google has„ÄÇ

And they trained two variants of their modelÔºå so one was a 12 layer model that had at each layer„ÄÇ

 12 heads and the dimensionality of the hidden states the thing that we get at each the output of each of these selfat blocks of 768 and they trained out large model„ÄÇ

 Bt largeÔºå which was 24 layersÔºå more headsÔºå bigger hidden dimensionalityÔºå and that one worked better„ÄÇ

So yeahÔºå I guess I will I talk briefly about how we fine tune these models using the CS token for other tasks„ÄÇ

 like question answering and sequence labeling we do very similar things„ÄÇ

 but use not we don't use a CS token„ÄÇ we use the other token representations and we'll spend a lot of next lecture talking about those methods„ÄÇ

 So I won't cover these slides here„ÄÇ I just wanted to shift over to the results to show how much of an improvement this Bt model was for NLP as a whole„ÄÇ

 So the glue benchmark as a bunch of these sentence level tasks that are usually I think all of these are classification tasks„ÄÇ

 So this one is a textual entailment data set„ÄÇ This I think is a paraphrase data set„ÄÇ

 SST as a sentiment analysis data set and so on„ÄÇ So there's like these„ÄÇüòä„ÄÇ

8ight or so tasks that are you know pretty well known within NLP„ÄÇ

 and we had these previous numbers on all of these tasks were using unidirectional language models of open AIs GP GPT was trained on a lot of text and it achieved an average score of about 75 across all of these different eight tasks and a model with ElLMmo in it was a little worse at about 71„ÄÇ

But you can see that BertÔºå especially Bert largege„ÄÇ

 really just achieved the state of the art on all of these tasks and had a pretty huge gain in terms of the average score„ÄÇ

 So this was a really impactful result that has basically changed the the course of research in N LP since its release„ÄÇ

üòäÔºåYeahÔºå and so this plot from the original bird paper„ÄÇ

 or at least from the slide really shows the impact of model size on the downstream performance„ÄÇ

 So how effective this model will have to be„ÄÇHow effective this model will be for transfer learning so you can see that as you increase the number of parameters in your transformer„ÄÇ

 your accuracy on this downstreamÔºå these two downstream tasks goes up right and importantly„ÄÇ

 it's not like these curves have plateaued right they're still going up this plot was what the Google researchers were able to do at that time„ÄÇ

 but it shows that by simply just increasing the model size and also the training data we will get better and better performance and the insight from this plot has been borne out by subsequent results showing just bigger and bigger models give you larger improvements„ÄÇ

So for your projectsÔºå many of you might be interested in working on languages that are not English„ÄÇ

 so Bert was trained specifically on English Wikipedia and this book corpus was also completely English but soon after the release of the BtRT model of the Google researchers also released multilingual Bt which is very interesting it's a single model that's trained on the union of 104 different language Wikipedias so they did not train a separate model for each language„ÄÇ

 they instead just basically concatenated all of these different Wikipedias together and encoded them using the same vocabulary so like subwords from different languages are all kind of in the same vocabulary and they trained it using this mass language modeling objective and it has some very interesting„ÄÇ

üòäÔºåProperties like you can do„ÄÇYou you can solve so actually it also at the time set the state of the art on many tasks„ÄÇ

 even though it was trained in this kind of strange manner„ÄÇ

 but you can also imagine that training in this way allows the model to share information across different languages„ÄÇ

 which is very useful if you have a language whose Wikipedia might have„ÄÇ

 you know only like a couple thousand articles versus English where you probably have millions of articles having this transfer between different languages could be very important„ÄÇ

üòäÔºåYeah„ÄÇSo this is just a picture„ÄÇ and this picture is outdatedÔºå by the way„ÄÇ

 although I think it was made maybe six months or so ago„ÄÇ

 that shows kind of how research in this area has developed since Bt was released„ÄÇ

 So we started out with theseelmo like models„ÄÇ And there were other models that kind of followed this train unit unidirectional language models„ÄÇ

 But you can see just the number of offshoots from Bt here is tremendousÔºå right„ÄÇ

 So there's a lot of Mppet like names following Elmo and Bt„ÄÇ there's two different earningsnies„ÄÇ

 There's a grover„ÄÇ there's probably other things that ohÔºå there's definitely a kermit So yeah„ÄÇ

 is crazy how just this simple paradigm of mass language modeling has been there's been so many variance of it since then„ÄÇ

 So this whole line of workÔºå video Bt V Bt visual„ÄÇüòä„ÄÇ

Images together like maybe images and their captions„ÄÇ

 there's work that's trying to focus on modeling longer sequences so that's in the realm of XL net there's work in the line of Roberta„ÄÇ

 which is just training for more time bigger modelÔºå more data and so on„ÄÇ

 So a lot of interesting things will'll cover some of these in future lectures„ÄÇüòäÔºåOkay„ÄÇ

 so I guess at this pointÔºå let me look more at the questions here„ÄÇ

 Are we replacing mask words with random words to reduce overconfidence„ÄÇ

 I think that's unlikely because the masked words it's just our random process„ÄÇ

 like which words are masked at every batch„ÄÇ So it's very unlikely that the model„ÄÇ

 even though it's huge is going to be able to overfit the training data and be incredibly confident on particular predictions„ÄÇ

 And„ÄÇYeahÔºå I think replacing it with random words just gives it I guess„ÄÇ

 a little different objective where the model also has to be aware of when a word doesn't make sense in a particular context„ÄÇ

 So I would wager that was more likely the reason the intuitive reason behind it„ÄÇ

 It's very hard with these huge models to judge if your intuition actually matches up with the the results that you're getting„ÄÇ

 though„ÄÇIs there a constraint on what kind of words are masked„ÄÇ That's a great question„ÄÇ

 going back to this slide„ÄÇ One of these ErniesÔºå I think this one did experiment with looking at different like parts of speech and masking them out in particular„ÄÇ

 they found that if you just mask out named entities like names of places or people or objects or whatever„ÄÇ

 you get better performance on tasks that are entitycentric„ÄÇ

 there have since been works that explore masking out contiguous sequences of text„ÄÇ

 So there's things like span Bt that do this„ÄÇ And that seems to help also on tasks that deal with like phrases„ÄÇ

 But yeahÔºå the original bird paperÔºå just randomly masks words„ÄÇüòäÔºåOkay„ÄÇ

 so I wanted to conclude with some takeaway questions„ÄÇ

 So these slides were actually shared with me by the first author of the birdir paper„ÄÇ

 Jacob Devvelin and„ÄÇüòäÔºåAfter I've described all of thisÔºå I think„ÄÇ

You might be thinking like why did no one ever even do this before right it seems quite obvious that these kinds of pretraining tasks do encode a lot of knowledge about Liby forcing our downstream models to learn everything about linguistics from just a very small pretraining or sorry downstream data and really the answer is that the hardware improved to a point where we could train huge scale models and model size has made a huge impact in in this field so in 2013„ÄÇ

 if I wanted to train a two layerÔºå very small LSTM„ÄÇ

 I might have had to train it for eight hours to get this result and now we're on to something like BurRT which has 24 layers of huge hidden states doing this expensive quadratic self-ten computation at every layer„ÄÇ

üòäÔºåAnd the fact that they can train this in four days on their TPU pods is pretty insane and you know even since the release of BRT„ÄÇ

 we've had better and better GPUs released and it's become more feasible for even academic researchers to build these models and train them in a reasonable amount of time so„ÄÇ

YeahÔºå that that is basically the main reason we've been driven by advances in GPUs and TPUs to„ÄÇ

 to get to this point where we can scale„ÄÇ

![](img/c6f24929fd8fd39a2cd55342d10b650f_7.png)

OkayÔºå so I guess I want to end here„ÄÇ And yeahÔºå if you have any other questionsÔºå ohÔºå ohÔºå sorry„ÄÇ

 there is another question„ÄÇ How can you compare your variant of Bt„ÄÇTo Bert„ÄÇ

 if you don't have the sameÔºå this is a great question„ÄÇ So theÔºå sorryÔºå the question is„ÄÇ

 if I make a new Bt and I don't have access to these same TPU pods as Google„ÄÇ

 how do I compare my variant to theirs and make sure that„ÄÇ

If we scale it up to the same data set and model size that our model is actually better than theirs„ÄÇ

So„ÄÇIn the research communityÔºå people generally start from the pretrained BRT model and just fine tune it or add an additional component on top of it and then fine tune it„ÄÇ

 but they share a lot of parameters from the pretrain the models like Roberta„ÄÇ

 which train on more data„ÄÇ Those were done by industry labs with access to similar resources„ÄÇ

 So Roberta was done by Facebook„ÄÇ They trained it on like hundreds of GPUsÔºå not TUs„ÄÇ

 but basically same effect„ÄÇ you can scale up to insane data sets and model sizes„ÄÇ

Yeah this question is very important for academic researchers because even currently we cannot compete at this scale with the industry labs and so it's kind of resulted in this situation where researchers both in academia and at smaller companies that don't have access to these resources just wait for pre-train models to be released by labs like Google or open AI or Facebook and then find clever ways to use them but yeah„ÄÇ

 hopefully that changes in the future with cheaper and more accessible computational resources and also maybe we'll figure out ways to train these models in such a way that they're not so data hungryung and maybe are more perimeter efficient and so we'll talk on in the semester but yeah„ÄÇ

 I guess that's it for this lecture just the fundamentals of Bert and next time we'll talk about„ÄÇüòä„ÄÇ

Actually using BERT to solve many different downstream tasks„ÄÇ

 so you'll get exposure to a broader variety of tasks other than just like language modeling and sentiment analysis that we've seen at this point„ÄÇ

So yeahÔºå you can look forward to that„ÄÇ

![](img/c6f24929fd8fd39a2cd55342d10b650f_9.png)