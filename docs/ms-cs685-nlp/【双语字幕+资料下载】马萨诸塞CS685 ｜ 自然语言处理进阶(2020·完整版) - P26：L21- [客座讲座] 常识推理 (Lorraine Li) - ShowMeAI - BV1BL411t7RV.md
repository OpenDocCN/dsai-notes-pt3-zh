# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÈ©¨Ëê®ËØ∏Â°ûCS685 ÔΩú Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËøõÈò∂(2020¬∑ÂÆåÊï¥Áâà) - P26ÔºöL21- [ÂÆ¢Â∫ßËÆ≤Â∫ß] Â∏∏ËØÜÊé®ÁêÜ (Lorraine Li) - ShowMeAI - BV1BL411t7RV

OkayÔºå hopefully this works„ÄÇOkayÔºå we are now live so oh wellow16 people people are excited for this topic so hey everyone today we're having our guest lecture from Lorraine on common sense and before we start I just wanted to let you know that we have an extra credit assignment Lorraine would really appreciate some feedback on her lecture„ÄÇ

 the pacing the content how useful you thought it was and whether you had the right background to understand everything so there's a feedback form in the LinkedIn Piazza and also on the website which you can click and complete write some comments about the lecture and if you do that by next Monday we'll give you a small amount of extra credit so this is not this will be on top of your existing current„ÄÇ

üòäÔºåExtrare credit assignment„ÄÇOther than thatÔºå yeahÔºå I mean„ÄÇ

 obviously your only remaining deadline other than the quiz is the final project report„ÄÇ

Make sure you're working on that and finally for this lectureÔºå if you have any questions„ÄÇ

 I'll be monitoring the chat window so whenever you have a question just write it in there as normal and I will ask it to Lorraine„ÄÇ

üòäÔºåSo okayÔºå with that out of the way when don't you go ahead with the lecture Lorraine„ÄÇ

Thanks so much for the introduction Mo ahead and thanks for having me here„ÄÇ

 I'm really excited to be here and giving this lecture giving this guest lecture„ÄÇ

 you can stop me at any time during the lecture of with questions if there is any you don't have to save them up So let's get started„ÄÇ

 I'm learning a fifth year PhD student working with Andrew Mcallum at easel and my research interest lies mainly around common sense reasoning ranging from learning a good representation that has the correct inductive bias towards representing certain kind of common sense knowledge to how can we design benchmark data to evaluate models with certain kind of common sense knowledge„ÄÇ

üòä„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_1.png)

So first of allÔºå what do we mean here by saying common sense knowledgeÔºü

One type of knowledge as shown in this example is it is obvious for us to know that a lake were divided into two lakes when the water level is full„ÄÇ

 so a five year old would probably know this and some other things a five year old would know is when they're playing this kind of brick toys if you don't place it correctly then the brick is going to fall down„ÄÇ

So learn what do we mean here by common sense knowledgeÔºå it's a sound practical judgment„ÄÇ

With concerning everyday matters and„ÄÇItIt's about someone's basic ability to perceive„ÄÇ

 understand or judge certain situationsÔºå and it is also shared commonly to nearly all people„ÄÇ

So what are we defining here is apparently we're not referring to everyone in the world because if it's a child who grows up in Amazon Jungle who would never played this kind of fancy toys ever„ÄÇ

 so sadly with that in mindÔºå we're not referring to every single person in the world„ÄÇ

 but instead we're referring to nearly all peopleÔºå we're targeting this group of people„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_3.png)

And then specifically in NLPÔºå it's actually everywhereÔºå for exampleÔºå in machine translation„ÄÇ

 when we tries to translate an English sentence to a German sentence„ÄÇ

 so software or the tool needs to understand what the word working mean here because there is different two words for working in German„ÄÇ

 either it means a labor or profession such as the electrician is working„ÄÇ

 or another another meaning would be the telephone is workingÔºå meaning is functional properly„ÄÇ

 So those aret actually different to wordsÔºå the software needs to know what this English working actually meaning here in order to get the correct translation in German„ÄÇ

And furthermoreÔºå in smart home assistant like Google Home„ÄÇ

 this is the real example I saw someone posted on Twitter the other day„ÄÇ

 this person is not able to find their phoneÔºå so they ask for help„ÄÇ He saidÔºå heyÔºå Google„ÄÇ

 can you call me please and Google said you sure this personÔºå yesÔºå I'm sure and then Google said„ÄÇ

 okayÔºå yeahÔºå I will call you please from now on„ÄÇSo this is apparently a misunderstanding and but from our point of view„ÄÇ

 it's actually really easy to understand thisÔºå but it's kind of just really hard task for machine to do this„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_5.png)

So since it is so importantÔºå there must be previous attempt that tries to solve it„ÄÇ

There has been big effortÔºå has been pushed in the 70s or 80s by researchers of using logical formulation to represent common sense knowledge„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_7.png)

And the Turing Award winner John McCarthy is one of the frontiers that formalized commonis sense as a logical based problem„ÄÇ

He published several papers using logic system to represent Common sense knowledge„ÄÇ

In one of his first papersÔºå this paper called Program with Common S„ÄÇ

This paper is especially important because it was probably the first paper on logical AI back in 1959„ÄÇ

And it is also the first paper pointing out common sense reasoning as the key ability for AI system to have„ÄÇ

So in this paperÔºå he saidÔºå in order for a program to be capable of learning something„ÄÇ

 it must first be able to be capable of being told it„ÄÇ

 That's why we need this logical formulation to tell the machine that this is the kind of knowledge you have to know„ÄÇ

So in this paperÔºå he proposed something called advice taker„ÄÇ

 which represent knowledge in the following wayÔºå it's almost like a triple like I at desk desk at home„ÄÇ

 it's very similar to the knowledge based representation right now„ÄÇ

 but I think it very different in a number of different ways„ÄÇ

But this is one of the first attempts to representing logic comm sense knowledge using logical formulations„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_9.png)

And there after thatÔºå there has been many logical systems being applied in the domain of common sense„ÄÇ

And this is probably an incomple list of all these different logic systems„ÄÇ

 but just to give you idea of how diverse these logic systems are„ÄÇ

With propositional logic as a start and you can do different modifications on top of that that gives you first order logic or mod logic these are all deductive reasoning logics and apart from this there are also plausible reasoning logics„ÄÇ

 including non monoonic logic probabilistic logic or fuzzy logic„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_11.png)

With this many different logic systems have been proposed or have been tried„ÄÇ

 there was some progress with modeling common sense knowledge back in the days„ÄÇ

 but the overall story is it did not end up well„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_13.png)

As said by Ye Jin CuÔºå who is currently a professor in University of Washington and is one of the professors who is now actively working on commonis sense recently these days„ÄÇ

 she said in her PhD timeÔºå she was told not to speak of the word commonis sense„ÄÇ

 which is at around 2000 at the start of 2000 or sometimes like that„ÄÇ

But she also concluded that the past failure of common representing common sense knowledge is inconclusive„ÄÇ

 including the following reasons„ÄÇ but this is not an exhaustive list of the following reasons„ÄÇ

 But these reasons probably contribute to the failure of the common sense reasoning part„ÄÇ

 There are not enough computation powers„ÄÇ There was not GPSus available„ÄÇ

 and not so much data and there's no like a tool called mechanical Turk„ÄÇ

 such that we can gather enough informations from people to annotate this data for us„ÄÇ

And also theres not at strong computation models like there was neural networks„ÄÇ

 but due to the following the above two reasons neural network actually was not working great„ÄÇ

And alsoÔºå there is no ideal concept transition or representations to represent this kind of knowledge„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_15.png)

So with that in mindÔºå I think we can have a brief outline of today's lecture„ÄÇ

 I've been talked about common sense knowledge and we said there is not enough there is no correct representation„ÄÇ

 so the next part will be how can we learn the right representation and we'll also introduce some common sense knowledge in whether there are common sense knowledge between language models or not and then finally we'll talk about how to build benchmarkmarkg data for evaluationÔºü

So how can we learn the kind of representation or what is the kind of right representation to represent certain type of knowledgeÔºü

In LLPÔºå we typically learn a vector representation of words as in Wordtuva or like bird representations„ÄÇ

And this learned represent vectors showed word semantic similarity through this distance between words as a result of that„ÄÇ

 similar words will get clustered togetherÔºå as you can see here„ÄÇ

 there is an animal cluster and there is a furniture cluster„ÄÇ

But if we look closer to rabbit and mammalÔºå mammo is a much general concepts„ÄÇ

 including lots of small concepts like rabbitÔºå but this point representation cannot capture the size of the region referring to the word where the size of the region represent the number of concepts can be expressed using this word„ÄÇ

And moreoverÔºå rabbit is a mammalÔºå but mammo is not grait„ÄÇ

 The kind of naive score function like dot product cosine similarity between vectors cannot capture this asymmetric relationships„ÄÇ

So our labÔºå proposed in 2014Ôºå proposed to expand this point of representation into a Gaussian distribution„ÄÇ

 so the representation will now become region based„ÄÇ

It is also asymmetric because the rabbit GaussianÔºå as we can see here„ÄÇ

 will be inside of the mammo Gaussian to refer that rabbit is a mammal but not vice versa„ÄÇ

FurthermoreÔºå it can express the the jointness between concepts„ÄÇ

 as there are no region overlap between deer and rabbit if we give certain threshold to the Gausian distribution„ÄÇ

But one drawback of this representation is that they're not closed under intersection„ÄÇ

The intersection between two Gausss are not necessarily a Gaussian distribution„ÄÇ

 which makes the model is really hard to train and the computation really hard as well„ÄÇ Lorraine„ÄÇ

 can I interrupt really quickÔºü There's there's one question about your example that you showed a while back with the brick„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_17.png)

![](img/5410349b74b16c817978db5c87eec601_18.png)

And„ÄÇI guess there's a commentÔºå is this really common sense understanding or more observation based understanding of the like physics of our world since there are programs or graphics engines„ÄÇ

 stuff like that that have encoded these kinds of physics based behaviors already into their system„ÄÇ

YeahÔºå this is a great question and I think by seeing common sense reasoning it's actually a big umbrella term that involves a lot of different types and physical common sense is definitely one type of them„ÄÇ

 one thing would fall down naturally that the gravity would tell you that that's definitely one type of common sense„ÄÇ

And there has been active areas in vision or A working on this as well Do you I guess there's a follow up question from the same person„ÄÇ

 have there been any methods in NLP for a common sense reasoning that explicitly encode this kind of information about like physical behavior physics etca into the modelÔºü

YeahÔºå that that's that's a great question„ÄÇ Follow up question„ÄÇ

 because there is definitely benchmark data that targeting this kind of physical common question answer in task„ÄÇ

 which I would introduce later„ÄÇ So I believe then the models who get really good scores on that dataset will have a way of encoding this physical knowledge into the model in order to perform well for that data„ÄÇ

 cool„ÄÇ„ÅÑ„ÇÑ„ÄÇSo yeahÔºå back to our representation learning„ÄÇ

 we have this Gaing representation and as we saidÔºå there is one drawback of this representation because they're not closed under intersection„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_20.png)

And then after thatÔºå in 2015Ôºå another representation comes out„ÄÇ corn representation„ÄÇ

 me to this problem by representing each conceptÔºå using a coin„ÄÇ

Referring to the region towards infinity„ÄÇ And now it is closed under intersection because the intersection between two cons is still a cone„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_22.png)

And in 2017Ôºå there is another paper extend this representation to a probabilistic model by constraining the space into a unit space and assigning less density to each column as the coin extend to infinity„ÄÇ

LorraineÔºå sorryÔºå sorry to interrupt againÔºå there's a clarification question on the Gaussian embeddings„ÄÇ

 whyÔºå why are these regions called GausianÔºå I'm familiar with the term in probability distributions„ÄÇ

 data setsÔºå but I'm not sure what it means in the context of regions„ÄÇ that's the question„ÄÇüòä„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_24.png)

GreatÔºå sorry for not making it super clearÔºå but in this representation here„ÄÇ

 we actually represent each concept using a Gaussian distribution„ÄÇ

 So if you want to make it super specificÔºå it's two vectors„ÄÇ

 one vector represent me and the other vector represent the variance of of this distribution„ÄÇ

So it's literally a Gaussian distribution for each world„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_26.png)

These are great questions„ÄÇ feel free to yeah ask more„ÄÇAnd„ÄÇWhere were we„ÄÇ

 we have this probabilistic notion of this con embedding grid„ÄÇ So yeahÔºå the probability„ÄÇ

 now we have this probability cone embedding representation and the probability of each concept will equals to the volume of the cone integrated under this exponential function„ÄÇ

 but each con will still extend to infinity„ÄÇAn immediate concern is that we cannot make rabbit and deer in this case„ÄÇ

 disjoint anymore„ÄÇAnd howeverÔºå the situation will get even worse„ÄÇ

 Conition on one or more variables will always increase the probability„ÄÇFor exampleÔºå here„ÄÇ

 assume the probability of being a rabbit is like 12 point point off„ÄÇ

 and we want to get the probability of rabbit condition on deer„ÄÇ

If you want to guess the probability of rabbit conditional deer should be zero because if you're a deer„ÄÇ

 there's no way you can be a rabbit„ÄÇBut for this modelÔºå after conditional on deer„ÄÇ

 the universe of this calculation becomes the deboxÔºå which will gives us likeÔºå for exampleÔºå0„ÄÇ

4 probabilityÔºå conditional probabilityÔºå which doesn't make sense at all is this is a bigger„ÄÇ

 this is a very bad thing for a probabilistic model„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_28.png)

So following up this work in 2018Ôºå we propose we try to fix this problem in a natural way by giving the upper bound to coin embedding„ÄÇ

SoÔºå and to by representing each concept using a„ÄÇAes aligned hyper rectangs„ÄÇBoxesÔºå in other words„ÄÇ

 boxes„ÄÇ And this is a 2D illustration„ÄÇIn this representation„ÄÇ

 it is region based and it can represent asymmetric relationships because one can inside of another to represent is asymmetric„ÄÇ

And they're also disjoint because they're also closed under the intersection because the intersection of two boxes will still be a box„ÄÇ

More importantlyÔºå these concepts like rabbit and deer can be represented„ÄÇNow„ÄÇ

 the probability of rabbit given deer should be 0„ÄÇSo I think you addressed this question mostly in what you just said„ÄÇ

 but if you go back to that previous slideÔºå there's a question on why is it necessary for these concepts to be represented such that the properties like region asysymmetricmmetry disjointness„ÄÇ

 et ceteraÔºå are held„ÄÇLike why are those properties important in this kind of representation good good good question so region because we want as we said„ÄÇ

 the vector representations memo and rabbit will have the same representation there is no notion of how general this concept is so we kind of want this region representation to represent how general this concept is and how can I refer can I using this word to refer more general things or I can you only use this where to refer very specific stuff„ÄÇ

That's why we want region and asymmetric is„ÄÇFor is this kind of is relationsÔºå Rabbit is a mammal„ÄÇ

 We want to have„ÄÇ Rabbit is a mammalÔºå but mammo is not rabbit„ÄÇ

 This relationship between these two words are not symmetric„ÄÇAnd thenessÔºå as we said„ÄÇ

 because that probabilistic model will not give us if you condition on one thing„ÄÇ

 its the probability of deer given rabbit will becomes bigger„ÄÇ

 which means something can both become a rait and a deerÔºå it's a very unintuitive thing„ÄÇ

 I don't think that's commonissance„ÄÇ and why they're closed why do we want closed under intersection„ÄÇ

 because that makes the calculation extremely easy easy because in the end we want to get the representation of these things„ÄÇ

 So there are two more questions„ÄÇ One regarding closed under intersection„ÄÇ

 Could you please explain what that means„ÄÇ closed under intersection means if we take two two of these regions and we take the intersection of these two regions„ÄÇ

Are they still the same unit representation you was working on beforeÔºå Like boxes„ÄÇ

 you take the intersection of two boxesÔºå It's still a box cons„ÄÇ

 you take the intersection of two cons„ÄÇ It's still a cone„ÄÇ

 That's what do we mean by that's what we mean by closed under intersection„ÄÇGreat„ÄÇ

 and the second question isÔºå is the region based representation similar to doing something like a hypernym discovery on vectors space„ÄÇ

That's a great question„ÄÇ That's how we get motivated„ÄÇ

 And this kind of region representation should works extremely well for higher radical relations„ÄÇ

 which is hypernams„ÄÇGreat„ÄÇYeahÔºå those are all great questions„ÄÇ

And now we have this box representation„ÄÇAnd it can„ÄÇYeah„ÄÇIt can represent this strong concept„ÄÇ

 That's why we want this destroyedstredness„ÄÇ We don't want anything„ÄÇ Give you a„ÄÇ

 give me a probability of rabbit given deer„ÄÇ We don't want it to be more than 0„ÄÇAnd as we said„ÄÇ

 this is good for common sense representation„ÄÇ What can we do with this„ÄÇ

 We also try to use this box model to model common sense„ÄÇ And as a first step„ÄÇ

 we train the LTM to take arbitrary short phrase as input and then output the box„ÄÇ

This is an illustration of foxes trained on an enitment task„ÄÇ

And as shown in this cherry picked exampleÔºå if something is beyond on bench„ÄÇ

 it's likely you are sitting down„ÄÇ But if you sit down„ÄÇ

 there is much less probability of you are sitting down on a bench because there are lots of other things you can sit it down„ÄÇ

In additionÔºå if we add the third box of man relaxÔºå it is going to intersect with be on bench and sitting down by or a small portion„ÄÇ

 but it's going to be a big box because there are lots of other different ways for a person to get relaxed„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_30.png)

To give you more intuition of how this model is getting trained„ÄÇ

 I want to show you this video of training a 2D boxes on seven movies„ÄÇüòä„ÄÇ

Even though movie are not not common sense knowledge„ÄÇ

 but boxes is like more general than only representing common sense knowledge„ÄÇ

 it's a representation can be viewed as a probabilistic model that can capture any kind of joint distribution with for binary random variables„ÄÇ

So the goal here is we want to see similar movies should end up having a similar box representation and the size of the boxes represent how many people have rate this movie in a relatively high score„ÄÇ

As you can seeÔºå all the boxes are initialized randomly„ÄÇAnd„ÄÇAfter a while of training„ÄÇ

 we can see similar movies get cluster togetherÔºå liing and alling their overlapping with each other a lot„ÄÇ

üòäÔºåAnd Forreest Gump is a pretty big box because presumably a lot of people just like this movie and read it pretty high in the high school„ÄÇ

Also rear window north by one Northwet are both movies directed by Hitchkirk„ÄÇ

 they probably have the same group of audience as wellÔºå and also the lot of learning series„ÄÇ

There's a questionÔºå could you clarify again how the box representation is defined mathematically„ÄÇ

 is it a matrix or a bunch of coordinatesÔºå what is itÔºüGreatÔºå it's a bunch of coordinates„ÄÇ

 because if you can see hereÔºå we actually represent the boxes using a lower left coordinate and the upper right coordinates„ÄÇ

 So in when you actually do the trainingÔºå it's two vectors„ÄÇüòä„ÄÇ

But it has certain like geometric constraintsÔºå like the upper right has to be bigger than the lower left„ÄÇ

So that it won't violate thisÔºå you actually have a size of their boxes„ÄÇGreat„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_32.png)

But there is training difficulties with this model„ÄÇIf we the current model„ÄÇ

 if one box is inside of another and the goal of the training objective is to make them disjoint„ÄÇ

One thing we cannot doÔºå or it's really hard to doÔºå is to„ÄÇTo deal with this kind of situation„ÄÇ

 Because imagineÔºå no matter how you move this purple box„ÄÇ

 a blue box inside the gradient of the the intersections„ÄÇ

 that the box' intersection size will not change„ÄÇ So as a result of thatÔºå the loss will not change„ÄÇ

 So you will get  zero gradient„ÄÇAnd yeahÔºå hard box training result will be like this„ÄÇ

And this is just one caseÔºå another case would be what if two boxes are disjoint and the goal is for them to intersect with each other„ÄÇ

It's the we're in the similar situationÔºå no matter how you move these two boxes„ÄÇ

 the intersection doesn't change so that your training result will still probably look like this„ÄÇ

There are lots of local minimums in this training objective„ÄÇ So as a follow upÔºå we proposed„ÄÇ

This is a loss function of the original box paper„ÄÇ As we can seeÔºå there is a big dropÔºå but„ÄÇ

Within allÔºå if you're outsideÔºå it's all the same„ÄÇ If you're insideÔºå it's all the same„ÄÇ

The eye clear paper in 2019Ôºå we tries to solve the sp problem on the outside„ÄÇ

And then in the NewOs paperÔºå which is coming out in this year's New„ÄÇ

 we represent boxes using a gumbo distribution where we have gumbo boxes„ÄÇ and now we just can„ÄÇ

 there is a smooth loss function everywhere„ÄÇAnd these are„ÄÇLike it's very high level„ÄÇ

 And if you're interestedÔºå you can always contact me for more details of how do we do this gumbo box and smooth box„ÄÇ

üòäÔºåBut in generalÔºå we what we want to domestic we want to convey here is it's a good representation„ÄÇ

 And now it doesn't have thisÔºå all this kind of training difficulties anymore„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_34.png)

As we propose the different ways to represent common sense knowledge„ÄÇ

 let's look like let's look at a properly training example„ÄÇ

 See that example is before the probability of being there„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_36.png)

If you condition on different thingsÔºå you will get different probabilities„ÄÇLike not white„ÄÇ

 you will get a little bit higher if it's an animalÔºå is's gonna be much higher„ÄÇ

 And after you conditionÔºå if it's hub ofvo„ÄÇ and if it's not whiteÔºå not rabbitÔºå is animal is' a hubvo„ÄÇ

 it's going to be pretty high probability that this thing is a deer„ÄÇ

 But the moment you change ha ofvo to not a hub„ÄÇ The probability becomes zero„ÄÇ

And this ability of being able to answer queries with negativeg variables is kind of unique to geometric representations because there is no way you can do it for vector representations unless you enter through all the possible possibilities and which is really expensive„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_38.png)

We have one more question„ÄÇ It seems that the intersection of boxes would have a higher possibility to be smaller in high dimension space„ÄÇ

 Is that rightÔºå That's that's great„ÄÇ YesÔºå that's correct„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_40.png)

And that can give us some like numer instability when things become super small„ÄÇ

 So you have in the actual implementationÔºå you have to use numer stability functions to avoid this kind of problems„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_42.png)

Makes sense„ÄÇThat's a word example„ÄÇ And this is a sentence example„ÄÇ

We tried boxes as we should before on sentences by using LTM to output a box representation„ÄÇ

 and this is a data set we tried it onÔºå it's a Fler data set„ÄÇ

 it's an enitment data set containing image of 45 million of image caps and after training on this you can see the probabilities with one condition on another varies from 0„ÄÇ

0 to 1„ÄÇ 1„ÄÇ0„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_44.png)

It's your condition on a blond woman walking down the sidewalk„ÄÇ

 the probability of person walk is one„ÄÇ And then on the other hand„ÄÇ

 if you say condition on whole instrumentÔºå the probability of you being on being in the basement is pretty low„ÄÇ

It's still probableÔºå but you probably it's a very low probability that you're actually in the basement„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_46.png)

So we've talked about how can we learn certain kind of representation that motivated that have this certain inductive bias to represent hierarchtical knowledge and concurrently vector representation have gained a lot of success by training on by training on large compass so now we're going to talk about common sense knowledge in pretrained language molecules„ÄÇ

Before thatÔºå do we have any more questions about boxes„ÄÇI paid to answer more„ÄÇ

I guess I will ask one of my own so we've talked in this class for this whole semester mainly about self supervised objective functions like language modeling or mask language modeling„ÄÇ

 things like that Do you think there are any ways to devise selfsvised objective functions for these kinds of geometric representations that will also give you some of these properties that are you know„ÄÇ

 obviously desirableÔºå but seems like you can only get currently by training on some sort of structured dataÔºü

YeahÔºå we get that question a lot by yeah your representation seems great„ÄÇ

 but what does it perform when you apply it to downstream tasks like can you do language model modeling with it and the current answer is we're still exploring because for vectors you can define the addition and multiplication with vectors pretty easily„ÄÇ

 which is actually the core of neural networks and the core of how all of those language modeling„ÄÇüòä„ÄÇ

But in boxesÔºå it's actually hard to defining those operations„ÄÇ

 What do you mean if you do addition with two boxes and multiplication with two boxes„ÄÇ So we are„ÄÇ

My advisorÔºå AndrewÔºå is super interested in thisÔºå and he's been pushing us for working on deep boxes for a long time so„ÄÇ

 but it's it's not an easy problemÔºå so we're still working on that„ÄÇüòäÔºåÂèØ„ÄÇYeahÔºå number of things„ÄÇ

 it's a great transition„ÄÇ You've been talked about a lot about language models„ÄÇ

 Does those language models have common sense knowledge or not„ÄÇ

 That's the next question we're going to answer„ÄÇWe often see a lot of news headlines like a robot wrote this entire article„ÄÇ

 Are you scared yet humanÔºå which probably means yeahÔºå the language models works really good„ÄÇ

 And this misleading news concept are giving us a lot of it's almost giving me heart attack„ÄÇ

 Like oh my God am I my researchers just done„ÄÇ like there's no need talk on this this recently at all„ÄÇ

 So luckilyÔºå researchers have have the same question as me as well„ÄÇ

 So their last works trying to answer this question„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_48.png)

![](img/5410349b74b16c817978db5c87eec601_49.png)

Does the pre traineded language model already capture common sense knowledgeÔºü

One common approach is that people will take take existing using existing common sense knowledge base to probe language models„ÄÇ

 the way people do it is to take existing common sense knowledge base„ÄÇ

 change that into a natural language template and pass this template to a language model using language models to do query or scoring on your transformed natural language„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_51.png)

There are two papers from these two groupsÔºå almost concurrently at EMOP last year„ÄÇ

 and they take the similar approach by both taking knowledge base and language models to do its job„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_53.png)

This is some detailsÔºå most of them using this data knowledge base called conceptNe„ÄÇ

 which is probably the most user friendly common sense knowledge base data setÔºå knowledge base„ÄÇ

 common sense knowledge base out there and also there are Wiki data or doing some mining from Wikipedia„ÄÇ

 but this is a common sense knowledge base used„ÄÇThey use handcrafted rules to transform triple representation to a natural language„ÄÇ

They evaluate on different modelsÔºå Elmo or BRT„ÄÇAnd here are the conclusions„ÄÇ

D Lamma paper said Bird performs wellÔºå but almost all the models performs really poorly in many too many relations„ÄÇ

And then the other paper said this kind of problem performs much worse that supervised method if you try to do a classification task on concept that„ÄÇ

 but it is more likely to generalize to different domains„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_55.png)

As we saidÔºå those two papers are all use handcrafted„ÄÇ

Are all using handcrafted templates to transform this relation triple to a natural language„ÄÇ

 So does this prompt that the sentence you give it to the language model matter„ÄÇ Does this matter„ÄÇ

The answer isÔºå yesÔºå it matters In this year's EMOPÔºå there is this paper called autopro„ÄÇ

 propose a way to generate prompt using automatic method„ÄÇ

The intuition is to generate prompt tokens such that this can make such that the token can make the biggest effect when predicting the label„ÄÇ

And the effect is measured by gradient„ÄÇ So this is also called gradient guided prompt generation„ÄÇ

One thing to notice here is that this automatic way of generating prompt„ÄÇ

 probably the sentence generated by this is not guaranteed to be grammatical„ÄÇ

 but it can make the most effect on the model predictions„ÄÇAs shown in the result here„ÄÇ

For the same Lama probing as we did beforeÔºå the Lama paper only gets MR score on 40„ÄÇ

 but this auto prompt gets up to 53 MR score„ÄÇ So that's saying the the kind of sentences you pass into models definitely matters„ÄÇ

 So keep in mind of that when you try to do some probing into language models„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_57.png)

Another line of research that people have been using to probe language model with common sense knowledge is to check the properties of the concept„ÄÇ

WaitÔºå sorryÔºå one second„ÄÇ One question I have with that is„ÄÇ

Can you really say that a language model encodes some fact if it only produces it when you probe it with a„ÄÇ

One particular contextÔºå and it outputs something completely different with other like different paraphrases of that„ÄÇ

 what does that mean exactlyÔºüYeah I'm not sure whether like the case you said before you said you just mentioned is's a pretty extreme case my guess would be there are certain group of ways of doing formulaized sentence such that it will predict a reasonable answer I won't say there there will be very different predictions„ÄÇ

üòäÔºåBut it's really hard to sayÔºå I think that's something we all need to keep in mind of„ÄÇ

 like it's really hard to say this model has commissance because that's a really hard light until you're trying to make there„ÄÇ

So I think we can all just do a little bit of careful study on that topic„ÄÇAnd yeah„ÄÇ

 so this is another line of research people have been use is to check the properties of concept„ÄÇ

They tries to ask two questions here„ÄÇ Do pre train language model correctly distinguish concepts associated with a given set of properties„ÄÇ

And alsoÔºå can a pretrained language model be used to list the properties associated with a given conceptÔºü



![](img/5410349b74b16c817978db5c87eec601_59.png)

The first questionÔºå can the language model distinguish concepts associated with given properties„ÄÇ

 They design this task by giving the model a set of properties and then let the model predict which concept having this property„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_61.png)

SoÔºå for exampleÔºå hereÔºå a blank has fur„ÄÇ You would have lots of different options„ÄÇ A lion„ÄÇ

 a bear cat dog„ÄÇ They all has fur„ÄÇAs you gradually add properties to this prompt„ÄÇ

Ha fur is big and has cloudsÔºå now you you're supposed to like rule out some of the concepts and as you grow bigger„ÄÇ

 the list grows bigger is an animalÔºå is brown than you only probably left with this one single concept bear„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_63.png)

For the answer of this question in that paperÔºå they said the model generally get good performance and robot performs better than B„ÄÇ

But there are some properties it's easier to distinguish than some other properties„ÄÇ

 like perceptual propertiesÔºå which requires visual reasoning„ÄÇAnd also„ÄÇ

 so perceptual performs worse than all perceptual properties„ÄÇ

And also the highly ranked incorrect answers generated by the model is typically applied to a subset of properties and saying the model is not being like wrong by a large margin is getting some of the stuff correct„ÄÇ

 but not entirely correct„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_65.png)

NowÔºå here is the second question„ÄÇ Can theÔºå if it can distinguish the properties„ÄÇ

 then can it list the properties associated with this given concept„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_67.png)

For exampleÔºå here we have bear and everyoneÔºå if you give this to human„ÄÇ

 everyone knows that a bear has human response like fur class teeth caused these things„ÄÇ

 And then Roberta said it has teethclo eyes in general„ÄÇ

 it has a low correlation with human illicit illicit properties„ÄÇ

 but they're coherent and mostly verifiable by humanÔºå as we can see here„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_69.png)

So I hope now weÔºå based on the probing papers or the pure probing experiments we see before„ÄÇ

 the hypothesisthes might be there is some knowledge in the language modelÔºå but„ÄÇCertainly„ÄÇ

 not all of them„ÄÇ And then if there is some knowledge„ÄÇ

 can we trust the language the knowledge from the language models„ÄÇThe general answer isÔºå no„ÄÇ

 you should be careful with that„ÄÇBecause there are just two papers saying„ÄÇ

Take different aspects of what kind of wrong facts language models can generate„ÄÇFor exampleÔºå here„ÄÇ

 Brox's wife is HillaryÔºå which is not correct„ÄÇ And alsoÔºå B can talkÔºå but cannot fly„ÄÇ

 These two paper titles just said very clearly of what kind of knowledge wrong knowledge is the model can generate„ÄÇ

 They're either distributionally related but incorrectÔºå or they're syntactically similar„ÄÇ

 but also they're incorrect„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_71.png)

So those are some problembing works that tries to do to check whether there are common sense knowledge in the pret language models or not„ÄÇ

Following thatÔºå thereÔºå except for this kind of probing task„ÄÇ

 There should always be a benchmark data set for evaluation to tell us„ÄÇ

Are there standard benchmark data to evaluate the model that contains commons as knowledgeÔºü

A standard way of doing this is using a question answer in dataset„ÄÇBesides that„ÄÇ

 you can also do natural language inference as the example we should before a women is walking along the sidewalk can infer a person is walking„ÄÇ

Or reference resolutionÔºå like John asked AlexÔºå where is the library„ÄÇ He saidÔºå I don't know„ÄÇ

 Then where does what whichÔºå whatÔºå who is this he refers to in this example„ÄÇ

And there are also many other ways of designing the benchmark dataset set„ÄÇ

 but these are some like examples„ÄÇHere I tried to list different types of common sense data set we have in this community„ÄÇ

 and this might be incomplete listÔºå but just give you a sense of how how people have been tried to create such benchmark data„ÄÇ

The first one is hollow sweat„ÄÇAnd it's a multi choice question answering problem„ÄÇ

Targeting at grounded common sense„ÄÇFor exampleÔºå hereÔºå a woman is outside with a bucket and a dog„ÄÇ

 The dog is running around trying to avoid a bus„ÄÇ She„ÄÇ

 you are supposed to choose like the correct answer from this four choices„ÄÇ She like„ÄÇ

 gets the dog wet and then run away again„ÄÇ SeeÔºå is the correct answer„ÄÇAnd the best model performance„ÄÇ

 currently the„ÄÇThe model performance to human performance is only not enough„ÄÇ

 like less than 2% difference„ÄÇThat's grounded common sense„ÄÇ

 there is this objective NLI in takes the format of natural language inference„ÄÇ

 but still is a multi choice selection problem iss targeting objective reasoning and the difference is three point difference„ÄÇ

Coss QA targeting reading comprehensions with common sense knowledge„ÄÇHas around 2% difference„ÄÇ

And physical IQAÔºå as I mentionedÔºå this is a common sense data set„ÄÇ

Target explicitly on naive physical resume„ÄÇAnd the performance difference between best model and human is around not even 5%„ÄÇ

And the social IQÔºå how would you like there is certain aspect people are expecting of when we talk to people„ÄÇ

 So this is about social commons„ÄÇAgainÔºå5% of difference and Win grand„ÄÇ

 which is the co reference resolution problemÔºå as we talked before„ÄÇ

Even for this that it seems a really hard taskÔºå there is only three points of difference between human and the best performing models„ÄÇ

And finallyÔºå there has been effort trying to combine vision and language together this VCR Bu common since recently„ÄÇ

Even for thisÔºå there is only 2% of difference„ÄÇSoÔºå so one thing you probably noticed already is the gap between models and human performance These all very low„ÄÇ

 but doesn' that mean we actually solve this problem„ÄÇ

I would strongly disagree with this argument because if you look at this type of tasks„ÄÇ

 theyre all multi choice selections„ÄÇYou only need to select one correct choice from two to four candidate choices„ÄÇ

But this is not really how we do common sense recently„ÄÇ

 If you ask a human what's the property of a bearÔºå you're not supposed to select between„ÄÇ

 is it brown or is it redÔºå you're not doing thatÔºå but instead you're coming up with this word brown or it's big„ÄÇ

 you're making this selection from all your word vocabularies„ÄÇ

s not just a selection between two choices„ÄÇThat means a more natural way of doing such evaluation is probably by generating answers„ÄÇ

 giving me a question quick questionÔºå any evaluation data set of logic problemsÔºüYeahÔºå yeah„ÄÇ

 I think there is„ÄÇNot that I have listed hereÔºå but I think I can't remember the name of the data set I can try to find it out later„ÄÇ

 but I think there definitely is I think there's also the like synthetic baby data set from Facebook which„ÄÇ

I don't know if we consider that a real data set or notÔºå but yeah„ÄÇ

 if there were a real natural language version of thatÔºå that would be an example yeah„ÄÇYeahÔºå yeah„ÄÇYep„ÄÇ

But I think there are apart from baby there are all different logical reasoning data set as well I just saw a paper reporting new numbers on that data set but I forgot to include them here there's too many benchmark data„ÄÇ

So but in generalÔºå we want to do generative evaluation instead of multi choice selection„ÄÇ

Why haven't people died yetÔºüBecause it's a hard way to do itÔºå there are multiple correct answers„ÄÇ

 and when you're trying to do thisÔºå the stream matching between two answer„ÄÇ

 your answer and the ground truth answer might not be general enough to capture all the answers„ÄÇ

And in additionÔºå some answers is probably more common than the others„ÄÇ How can we capture this„ÄÇ

As a first attempt to this in EMOPÔºå we propose this data set called protoQA„ÄÇ

 capture prototypical situations that with generative evaluation„ÄÇ

Like name something that people usually do before they leave the house for work„ÄÇ

There are so many different options you can answer like here we ask 100 people and 4043 of them said shower„ÄÇ

 30 of them at breakfastÔºå and then there are also people saying goodbyeÔºå pray getting dressed„ÄÇ

 locked door So those are all available answers and in this data we tries to capture all of this„ÄÇ

So our data set has multiple correct answers for each questionÔºå and for each answerÔºå we have a score„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_73.png)

In additionÔºå we propose this generative evaluation that can evaluate multiple correct answers generated by the model As we said„ÄÇ

 stream matching might be not be good enoughÔºå so in the paper we also proposed the wordnet matching or you use the vectors„ÄÇ

 the distance between vectors to match the similarity between two words„ÄÇ

And for this generative evaluationÔºå we also tries to reward model has the corrected ranking of dancer list„ÄÇ

 You don't want to the model have the same score if they output shower pre or pre shower because clearly the first one is preferred„ÄÇ

We also reward model with a higher coverage of the athlete„ÄÇ

So this is another of our work at EMOP tries to do thisÔºå how does the language models do othersÔºü



![](img/5410349b74b16c817978db5c87eec601_75.png)

First of allÔºå we test on question answering data on question answer model„ÄÇ It's not doing great„ÄÇ

 mainly because this is really an open domain question answering and we don't have ground truth supporting articles„ÄÇ

 We used distant supervised supporting articlesÔºå which can be really noisy„ÄÇ

 so that might contribute to why this Q model is not performing great„ÄÇ

GP 2 is definitely doing better than UA„ÄÇ And in order to to be like to make it fair for GP2 to do the job„ÄÇ

 we change the prompt to name something that you would do to one thing you would do blahÔºå blah is„ÄÇ

So yeahÔºå GPT2 performs a little bit better after fine tuning with our training data„ÄÇ

 GPT2 performs much more betterÔºå but one thing I want to stress here is GPT2 did not generalize after reading our training data I think it means it's more like GPT2 is getting used to the task format and the answer is probably already there already is just without fine tuning it's hard for GP2 to understand what you are looking for here„ÄÇ



![](img/5410349b74b16c817978db5c87eec601_77.png)

But stillÔºå all the models has a large gap compared to human performance„ÄÇ

 indicating this is a hard task„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_79.png)

And as a summary of today's talkÔºå I talked about common sense knowledge in general and its past„ÄÇ

Failure of modelingÔºå using logical formulation to model to do such modeling and also what kind of representations we can learn in this case„ÄÇ

 box representations to capture certain type of common sense knowledge„ÄÇAnd also„ÄÇ

 whether there are commons knowledge in pretrain language models„ÄÇ

 what kind of province we can down and have been done„ÄÇIn the end„ÄÇ

 how can we generate a benchmark data set for evaluation of such common sense knowledge rhythm partÔºü

And that concludes my talk or teaching„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_81.png)

thingsh I talked about some papers I like to send all my collabators and that's it thanks Laorraine„ÄÇ

 I don't see any new student questionsÔºå but I guess I will ask one to kind of relate this back to a previous lecture we had where we discussed G3 and also the Emily Bennder and Koler paper on whether a model can learn meaning from just form do you think that like just a language model trained on text alone is going to be able to pick up all of these different common sense facts or do you think we need to have some„ÄÇ

üòä„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_83.png)

![](img/5410349b74b16c817978db5c87eec601_84.png)

Way to ground these models and other modalities or experiences for them to properly learn this„ÄÇ

Yeah that's that's a great question I don't I know GP3 is doing extremely well on few short learning„ÄÇ

 which means especially on common sense actuallyÔºå if you give it a few common sense examples„ÄÇ

 it can capture a lot of stuff„ÄÇBut I do think that being said„ÄÇ

 it captured a lot of knowledge probably alreadyÔºå I myself haven't played your history a lot„ÄÇ

 but that's what I put„ÄÇBut apart from thatÔºå I think that we still need different modalities to capture the physical aspect of this common' knowledge and also for rare things that people rarely mention in their text like rabbit is a mammal„ÄÇ

 you would probably never see that in your text and so those kind of things„ÄÇ

Still needs to be either injected or like learn it from a different sourceÔºå I think„ÄÇGreatÔºå okay„ÄÇ

 now we have more questions„ÄÇIs the shared info or a common ground between a speaker like two speakers in a conversation„ÄÇ

 is that counted as common senseÔºüI think it'sÔºå it„ÄÇIt's like„ÄÇIt's hard to say„ÄÇ It depends on how„ÄÇ

 how do you want to categorize this paper or something„ÄÇ But in my shallow understanding„ÄÇ

 I think that's more like a compact right a share„ÄÇ OkayÔºå there are two background saying„ÄÇ

The the common thing you are having likeÔºå for example„ÄÇIt's runninging outside„ÄÇ

 one person said it's runninging outside and the other person saidÔºå ohÔºå take your umbrella„ÄÇ

There are a lot of shared things here„ÄÇ You are assuming when is rainingÔºå you need the umbrella„ÄÇ

And you need umbrella to keep you from getting wet„ÄÇ So that's why I shared information„ÄÇ

 and I would definitely consider that as a commonis source knowledgeÔºå but there are also„ÄÇ

More specific domain shares like in this courseÔºå we all know we almost know like what the encoder is„ÄÇ

 what the LTM isÔºå I would not consider that as common sense knowledge right add to our definition before it must be commonly known to merely all human„ÄÇ

GreatÔºå another questionÔºå how was the generative task evaluatedÔºü



![](img/5410349b74b16c817978db5c87eec601_86.png)

OhÔºå they are our product QA data setÔºå great„ÄÇIts so the model is supposed to give to generate rank the list of answers„ÄÇ

 Then we match this answer screen to some to the ground truth„ÄÇ

 and then you would each answer screen will get mapped to what we call answer cluster where the answer cluster contains all different surface forms of this one concept like shower„ÄÇ

 there are different ways of seeing thatÔºå take a shower take a bus„ÄÇ therere all shower„ÄÇ

 So in this answer clusterÔºå we have all those different expressions to say this„ÄÇ

 So we first match this answer screen to the answer cluster„ÄÇ Then we compute like reward matrix„ÄÇ

 as you can see here„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_88.png)

The reward matrix gives you a reward of like according to how big the size of the cluster is because that represents how many people is answering the question with this answer„ÄÇ

We want to reward that fact„ÄÇAnd after we have this reward matrix„ÄÇ

 we do matching to actually assigning one answerÔºå one one answer screen to a cluster„ÄÇ

 we get a point and then we calculate the two metric here„ÄÇ Why is max answer„ÄÇ

 you can only give me10 answers and what's your performance over there and also there's max incorrect because you can give me infinite number of answers„ÄÇ

 but after you get three of them wrongÔºå you that's max answer at three so we consider two type of evaluation magic Q„ÄÇ

OkayÔºå great„ÄÇ I don't see any more questions„ÄÇ So yeahÔºå thanks„ÄÇ

 Lorraine and for all the students againÔºå if you want to„ÄÇüòäÔºåOhÔºå sorry„ÄÇ there are more questions„ÄÇ Oh„ÄÇ

 let me just ask those before concluding„ÄÇ SoÔºå ohÔºå I forgot to ask this 1„ÄÇ

 I meant to ask it at the end„ÄÇFor your box embedding research„ÄÇ

 did you start this direction with a particular use case in mindÔºå Can you share it if yes„ÄÇ

I think the general answer is yesÔºå because as you can see„ÄÇ

 we derive from a colon embedding which is generally used for representing higher correlations„ÄÇ

So yeahÔºå we targeted on hypernas or enement tasks„ÄÇGreatÔºå and another questionÔºå ohÔºå more questions„ÄÇ

 could you please explain again maybe briefly how the box representations are trainedÔºü

So in a two dimension caseÔºå you would have two vectors„ÄÇ you yeah„ÄÇ

 we don't even need to make it a two dimension„ÄÇ It can be like D dimension„ÄÇ

 You have two vectors representing the lower left corner and the upper right corner„ÄÇ

 You initialize them randomly and the training goal is to„ÄÇ

To give give the conditional probability between two boxes„ÄÇ

 which means the sides of one the the volume of intersection box divided by the box you condition on and after doing that„ÄÇ

 you have this box predicted conditional probability„ÄÇ

 you have that in addition you have them to target conditional probability you try to use final cross centraltro P to match them and train them by gradient distance So this is trained in a supervised manner then just to clarify yes„ÄÇ

 correct Okay and okayÔºå fast forwarding back to this this current proto Qa slide regarding the evaluation would you say that the evaluation would give different results if you show the answers to humans and ask them to rate if the answers are reasonable or not just because of the huge subjectivity involved„ÄÇ

OhÔºå that's a great question„ÄÇ We tries to mitigate the subjectivity by asking the answer to 100 people„ÄÇ

And we think getting a large pool of samples definitely reduce it„ÄÇ

 That's I think based on our collectionÔºå people would because it's not one single people s„ÄÇ Go it„ÄÇ

OkayÔºå so yeahÔºå let's let's stop here againÔºå if you have feedback or comments or anything or praise even for Lorraine please do send it along on the feedback forum and get your extra credit this is also the last lecture of the semester so thanks everyone for sitting through these videos and asking questions and yeah I think ohops it's„ÄÇ

üòä„ÄÇ

![](img/5410349b74b16c817978db5c87eec601_90.png)

It's been a good semesterÔºå so I hope you learned somethingÔºå at least about NLP„ÄÇ

