# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é©¬è¨è¯¸å¡CS685 ï½œ è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶(2020Â·å®Œæ•´ç‰ˆ) - P16ï¼šL13- é‡Šä¹‰ç”Ÿæˆ - ShowMeAI - BV1BL411t7RV

All rightï¼Œ hey Riï¼Œ I think I'm live so today we're going to be talking about paraphrase generation and a couple of its different use cases so we're going to talk about using paraphrases to produce adversarial examples and we'll go over what that means shortly we're going to talk about using them for data augmentation and you'll also see instances of that in your homework and finally we're going to talk about using paraphrases for style transfer so paraphrase generation is really quite an interesting and upcoming area and LP research so it's I think pretty important to learn about at this pointã€‚

ğŸ˜Šï¼ŒIt also ties nicely to some of the things we talked about last week related to machine translation and evaluation so rightã€‚

 as usual if you have any questions about logistics stuff orã€‚ğŸ˜Šï¼ŒThe materialã€‚

 write them in the chat boxã€‚So homework one was finally released I would highly recommend starting early there's we tried to make it such that you don't have to do a lot of implementation yourself there's quite a lot of scaffolding code that you can rely upon andã€‚

ğŸ˜Šï¼ŒWe originally hadï¼Œ yeah a lot more implementation than what currently is in the homeworkã€‚

 but that saidï¼Œ I think this will be useful for many of you who want to play around with these models in your project to get a sense of how to use them and some different ways of applying them to different tasksã€‚

Soã€‚Yeahï¼Œ we release the homeworkã€‚ It's due October 28ã€‚

 so we give you a couple extra days over two weeks to to finish itã€‚

 The exam now has an official date of November 5 and 6ã€‚ so the plan is we will release the examã€‚ğŸ˜Šã€‚

On November 5th and give you 48 hours to complete itã€‚Soã€‚Yeahï¼Œ againã€‚

 it'll be open book and open internet and everythingï¼Œ although yeah no collaborationã€‚

And you'll get more about that over the next couple of weeksã€‚

Okay so let's get into the topic of the dayï¼Œ which is adversarial examplesï¼Œ paraphrases and so onã€‚

 So before we startï¼Œ we need to at least define what an adversarial example is So a common task in computer vision is image classification and that's where this whole line of work kind of aose and now has made its way over to NLP but a way of kind of diagnosing your models and seeing if they're able to you know be as robust to natural image inputs as humans are is to do something like the falling so first of allã€‚

 we take this image of a panda the model predicts the correct label for this panda with you know 60% confidence and then the classic setup in computer vision is you add a very tiny amount of noise to this original image such that the resultingã€‚

ğŸ˜Šï¼ŒTo a human eye is it looks identical to the original right we added a tiny amount of random noise we can't really tell as humans what the difference between these two images is but for neural network image classifierã€‚

 the second image here gets predicted as a givenbb with 99% confidence so it's obviously not working on this adversarial example even though as humans we know that this image still contains a panda in it right so we would like to design models that are robust to this kind of these smallã€‚

 imperceptible perturbations in the inputã€‚ğŸ˜Šï¼Œå—¯ã€‚So the interesting problem then is how do we do this for text right in images we haveã€‚

 you knowï¼Œ an image is just a bunch of pixels and these have you know numerical value so I can add actually a small amount of noise to each pixel and I'll get a new image's outputã€‚

 but how does this work for text rightï¼Œ I have some sentence and I can't just like naively add noise to it right that doesn't make senseã€‚

ğŸ˜Šï¼ŒAnd so just to kind of motivate this problem of adversarial examples for NLP right I just forget for a moment about how we produce these thingsã€‚

 they do exist and their prevalent even with the huge scale models that we've seen up till this point in the class so I wanted to introduce task that we haven't talked too much about the semester just because it's a good example for the kinds of adversaries that we'll be talking aboutã€‚

 so textual entailment is basically a pairwise sentence task in which we're given two sentences and were asked whether the first sentence implies the second sentence as an example in this visualization comes from the Aen NLP demo we have two sentences they're called a premise and a hypothesis so two women are wandering along the shore drinking ice tea So this is our first sentenceã€‚

ğŸ˜Šï¼ŒAnd then the second one is two women are sitting on a blanket near some rocks talking about politics and we're asked to judge whether the second sentence is implied by the first sentence or notã€‚

 obviously in this case the first sentence kind of contradicts the second one right because the first one says the two women are wandering the second one says they're sitting so this is a Roberta classifier trained for this task it correctly judges that the second sentence sorry I've got my sentences backwards the premise is here contradicts the hypothesis so the first sentence contradicts the second oneã€‚

Howeverï¼Œ this model doesn't work perfectlyã€‚ and I had made these slides like last yearã€‚

 where the model on Ellen NLP was not using one of these huge scale language modelsã€‚

 but I tried it again today just to update these models and I'm using this model is a fine-ted Robertaã€‚

 So it is basically the state of the artã€‚ And if you give it a premise of the dog ate all of the chickens and the hypothesis of just chickensã€‚

 It says that the premise entails the hypothesis right even though this doesn't make any sense these twoã€‚

 first of allï¼Œ the hypothesis is not even a reasonable sentence and second of allã€‚

 it definitely does not follow from the premise which involves the dog and eatingï¼Œ etcterã€‚ğŸ˜Šã€‚

As another exampleï¼Œ thisï¼Œ againï¼Œ with the Roberta modelã€‚

 we have the red box is in the blue box and the hypothesis sentence of red is blueã€‚

 and the Roberta model judges that the premise sentence here entails a hypothesis sentenceã€‚

 these sentences obviously to a humanï¼Œ rightï¼Œ Chis and red is blueã€‚

 we know just without even thinking that they're not even close to being the same thingã€‚

 the semantics of these sentences are completely differentï¼Œ but for a modelï¼Œ it's not so clearã€‚

 And one possible reason is that the model hasn't been trained to see examples of negative sentences or neutral sentences like thisã€‚

 rightï¼Œ Chiickens is not a wellform sentenceã€‚ possibly red is blue is out of distribution for this data set it might be much shorter than other sentencesã€‚

 It's also kind of aã€‚Yourd sentence that doesn't really make sense butã€‚In any caseã€‚

 the model should judge this as if anything neutralï¼Œ rightã€‚

 so it's not behaving the way a human would when confronted with these examplesã€‚So in generalã€‚

 adversarial examples has been a popular topic in NLP since about 2017ï¼Œ so it is pretty recentã€‚

 again it followed from the foundational work in computer visionã€‚

 but there have been workshops dedicated specifically to developing these adversarial examples that break existing modelsã€‚

 and you can do this in several interesting waysï¼Œ this build it break it workshop focused on creating adversaries that are kind of linguistically interesting perhaps you come up with some syntactic construction that hasn't been seen before in the models training data in the Roberta case we would have fine tuning data obviouslyã€‚

 so yeah maybe in these cases the the training data is insufficient to capture all of the interestã€‚ğŸ˜Šã€‚

linguistic variation that might be associated with a particular task and also we know like from many different results that our NLP systems are quite brittleã€‚

 especially with kind of rare constructionsï¼Œ rare wordsã€‚

 things like that and human language users on the other handã€‚

 are very robust to even things that they haven't seen beforeã€‚ğŸ˜Šï¼ŒOkayï¼Œ greatã€‚

 so so far I see no questions in the chat boxï¼Œ which is fineã€‚

 but I'm not sure if it's working or not because it has not worked in the pastã€‚ So yeahã€‚

 anytime anyone has any question just againï¼Œ feel free to writeã€‚Okayã€‚

 so let's move on to how we might create an adversarial example in the NLP contextã€‚

 so probably the dumbest way possible to think about this is to just do synonym replacement right so I could take up theaurus or something like that and pick a word in my input sentence and then replace it with a synonymã€‚

ğŸ˜Šï¼ŒSo we have here exactly the kind of unexpected delight one hopes for every time lights go downã€‚

 this is a sentiment analysis example and the model predicts positiveã€‚

 but and this is a real adversarial exampleï¼Œ if you replace the phrase unexpectedex delight with thrillã€‚

 the sentiment model predicts negative here even with this very simple method of just doing simple synonym replacementã€‚

 the model gets confusedã€‚ğŸ˜Šï¼ŒThere is another class of adversaries called syntactic adversariesã€‚

 So in this caseï¼Œ we don't prioritize lexical substitutionã€‚

 So lexical is referring to this word level modificationã€‚

 here we're going to change the actual structure of this sentenceã€‚ So here we have a sentenceã€‚

 American drama doesn't get any more meaty and muscular than this which the model predicts as positiveã€‚

 But when you restructure the sentenceã€‚ So you put this part here firstã€‚

 So doesn't get any more meaty and muscular than this American drama the model the sentiment model predicts negative sentiment hereã€‚

ğŸ˜Šï¼ŒSo one thing that we might want to do is figure out for any given model what is a set of adversarial examples on which humans have a very easy time making a prediction but the model fails and once you have this set of examplesã€‚

 you can imagine augmenting your training data set with these adversarial examples such that the model is retrained or fine- tuneuned on these adversarial examples with the idea that if it sees them during trainingã€‚

 then it won't get them wrong again when we go to test itã€‚ğŸ˜Šã€‚

So you can imagine this kind of iterative process where we create adversarial examples for a given modelã€‚

 we retrain the model on its original data set plus adversarial examplesã€‚

 and its robustness will increase and we can repeat this process as long as we wishã€‚

So if we want to combine both of these things togetherã€‚

 like lexical adversaries and syntactic adversariesã€‚

 and we want to do this without you know relying on some external rules or thesaurusã€‚

 we might want to reformulate this task as one of paraphrasse generationã€‚So in paraphrase generationã€‚

 we take an input sentence and we want to produce an output sentence that has the same semantic meaning as the inputã€‚

 but might have some sort of diversity either lexically or syntactically or bothã€‚

So what are the qualities of a paraphrase model that introduces some sort of syntactic diversity into these paraphrases We obviously want them to be grammatically correct and fluent in whatever the language of the input sentences right and not only should they be fluent but they should also be retaining the semantic meaning of the original input sentence so if they're producing something that's completely diverges from the meaning of the original sentence that it's no longer a paraphrase and we can't really call it an adversarial example because we're not going to be able to preserve the label associated with that sentence right so I know that this second sentence has the same label as the first sentence because it's a paraphrase of the first sentence right so I know that the model when it gets it wrong that this is something to be concerned about because this one should have the same label as the original sentenceã€‚

ğŸ˜Šï¼Œã†ã€‚So I might also want to improve diversity right I might want to do a lot of this kind of reordering with the idea that the model might not be robust to certain constructionsã€‚

 it might not have seen you know this syntactic construction that many times compared to this more standard simple sentence hereã€‚

ğŸ˜Šï¼ŒSo as an example of the kinds of paraphrases that we might want for an adversarial example generatorã€‚

 if we have the sentenceï¼Œ usually you require inventory only when you plan to sell your assetsã€‚

 just some examples of things that we might want are usually you required the inventory only if you were planning to sell the assetsã€‚

 So here we've added a commaï¼Œ which kind of changes the structure here or this more significant reorderingã€‚

 when you plan to sell your assetsï¼Œ you usually require inventory or you need inventoryã€‚

 when you plan to sell your assetsã€‚ there are many different ways I could phrase this sentenceã€‚

 And we saw something similar to this with theã€‚ğŸ˜Šï¼ŒMachine translation evaluationï¼Œ rightã€‚

 We saw that there's not just one single reference translationï¼Œ rightï¼Œ But there's rather manyã€‚

 many different paraphrases of the reference that could all be valid translationsã€‚

 And so we might want a metric that evaluates a system output against all of the reference paraphrasesã€‚

 which kind of motivated theã€‚The blue score that we were talking about last timeã€‚So againã€‚

 these sentences are grammaticalï¼Œ they're syntactically very diverseã€‚

 and they all preserve the meaning of the inputsã€‚So this problem of paraphrasing has been worked on a lot in natural language processing not just recentlyã€‚

 but over the past few decadesï¼Œ even so we started with these rule template theaurusbased approaches and you can even do syntactic paraphrasing using rules right we haven't talked too much about parsingã€‚

 but you could imagine writing rules on top of parse trees to kind of reorder various chunks of a sentence to accomplish possibly syntactic paraphrases like thisã€‚

 but it becomes a little harder when you have more complicated sentences that don't necessarily match your rules right so you're limited in these approaches by having to first spend the time and effort to create these rules which is not trivialã€‚

 you might need some linguistic expertise and secondlyã€‚

 you're only going to have paraphrases when you have a sentenceã€‚ğŸ˜Šï¼ŒThat matches one of your rulesã€‚

 so you're going to get very low output diversity hereã€‚ğŸ˜Šã€‚

In contrast and what we're going kind of talk a little bit more about now is translation based paraphrasing So this kind of builds on what we talked about last weekã€‚

 but Michigan translation systems kind of naturally encode some linguistic variance when you do the translation and so we can actually get a lot of uncontrolled paraphrases out ofã€‚

ğŸ˜Šï¼ŒAnd existing machine translation systemsã€‚So here over historically you can get a lot of diversityã€‚

 so many different variants of the same sentence that differ quite substantially in terms of their syntaxã€‚

 but with traditional machine translation systems like before the state of the art ones that we have these daysã€‚

 you get pretty low grammaticality and you don't actually have a way of controlling the syntax of the output right so you can't tell it you know generate a sentence with this specific structure given this inputã€‚

ğŸ˜Šï¼ŒOkayï¼Œ and and now we're going toï¼Œ you know talk about this translation based methodã€‚

 but link it up to the deep learning neural network stuff that we've seen in this semesterã€‚

So one model for this task which I'm going to talk about here is called a syntactically controlled paraphrase networkã€‚

 which I help developã€‚ but there are many other more recent models for this taskã€‚

 and towards the endï¼Œ I'll talk about an even simpler approach to leverage some of the huge scale models language models like G3 that we've seen recently to further improve on this kind of thingã€‚

 But in this settingï¼Œ there' is essentially three stepsã€‚

 So first we create a parallel data set of paraphrase pairsã€‚ So remember in machine translationã€‚

 we have a parallel data set of source language paired with the target language asc sentence and source languageã€‚

 I have a corresponding sentence in the target language and I train my translation system in this wayã€‚

 so I can create the same sort of data for paraphrasing right here's a sentence and here's a corresponding paraphraseã€‚

ğŸ˜Šï¼ŒSentï¼Œ I meanï¼Œ these will be in the same languageï¼Œ of courseã€‚

 but I can still construct such a parallel data set if I have a way of getting a paraphrase in the first place right so paraphrases don't just naturally occur in the wild right It's very rare to have someone you know sit downã€‚

 take a sentence and write several different versions of itã€‚

 So it's harder to get paraphrase critical for you knowï¼Œ say United Nations proceedingsã€‚

 you have many people speaking different languagesï¼Œ they all need to communicateã€‚

 there are huge amounts of ground truth expert translations that are available for us to train our systemsã€‚

 but for a paraphrase we don't have anything like thatã€‚ğŸ˜Šã€‚

So we're going to automatically create a parallel data set of paraphrases using a process called back translationã€‚

 which I'll explain a little laterï¼Œ once we have these back translated pairsã€‚

 we can kind of view these as uncontrolled paraphrasesã€‚

 we haven't specified like generate a paraphrase with this particular syntactic structureã€‚

 but just using a huge data set of these automatically generated paraphrase pairsã€‚

 we can start to create labels for an eventual model that can produce these kinds of controlled paraphrasesã€‚

ğŸ˜Šï¼ŒSo I want to spend quite at least some time on this process of back translation because it's increasingly importantã€‚

 not just for paraphrasseï¼Œ but also just as a general augmentation strategy and it's used in unsupervised machine translationã€‚

 which we'll talk about next weekã€‚ So it's a very useful thing to know especially these daysã€‚

 but the basic concept is simpleã€‚ So I take a sentenceï¼Œ let's say a sentence in Englishã€‚

 In't that more a topic for your priestã€‚ I'm going to translate it to some different language using some pretrain machine translation systemã€‚

 So maybe I have an English to check systemã€‚ I'm going to translate it to this check sentenceã€‚

 and then I'm going to translate it back to Englishã€‚ So when I do thisã€‚

 I may not get the exact sentence that I passed in as input back rightã€‚ğŸ˜Šã€‚

There might have been some words dropped in this Czech translationã€‚

 or maybe I didn't get a perfect translationã€‚ Maybe the orderã€‚

 the syntax of these languages is very differentã€‚ So it's not natural to translate to English in this wayã€‚

 And so in this exampleï¼Œ I get something like are you sure that's not a topic for you to discuss with your priest rightã€‚

 which looks very different than the input sentenceã€‚

 So I now have both lexical and syntactic variation from this process of back translationã€‚ğŸ˜Šã€‚

So your homework one includes a component of back translationã€‚

 I just wanted to give you some insight into how this worksã€‚

 so I made this simple collab notebook which kind of copies some of the code from your homework oneã€‚

 can I go full screenã€‚

![](img/461fd5a1f19fd649a39bbb3561d347a7_1.png)

Yeahï¼Œ okayï¼Œ and maybe I'll increase the fontã€‚Okayï¼Œ I wish I could do word wrap hereã€‚ whateverã€‚

 So I have this sentenceã€‚ This is the first sentence from the bookï¼Œ100 years of solitudeã€‚

 And you can see that it's fairly complicatedï¼Œ rightï¼Œ It has a lot of different clausesã€‚

 and it's a pretty long sentenceï¼Œ rightï¼Œ many years laterã€‚

 as he faced the firing squad Colonel Oriano Wewia was to remember that distant afternoonã€‚

 when his father took him to discover iceã€‚And so in this exampleã€‚

 we're just going to use Google Translateï¼Œ which we're also going to use in your homeworkã€‚

 But you can imagine using some huge scale neural network specialized for particular language direction in generalã€‚

 it's kind of interesting in Google Translate is much better than any model that you could train yourself Google has access to you know tons of proprietary translation data that's not publicly availableã€‚

 So their translation systems are betterï¼Œ which means there's often less noise in the translationã€‚

 So you often do get the exact sentence backã€‚ But let's see what happensã€‚ So so farã€‚

 I've written this command to translate the sentence into finishã€‚

 I think so let's see what happens when I do thisã€‚ğŸ˜Šï¼ŒSo I assume this is a reasonable translationã€‚

 I don't speak Finnã€‚ In generalï¼Œ when we're doing back translationã€‚

 we call this intermediate languageï¼Œ the pivot languageã€‚ So here if Iã€‚Okayã€‚

 so let's say I'm going to translate this back to English nowã€‚å—¯ã€‚Oopsï¼Œ put that textã€‚

SoSo the source language is now finished right I have this finished translation and I'm going to translate it back to English so let's see what happens hereã€‚

oopsã€‚It's late to translateã€‚Okayï¼Œ so if I look at the resultã€‚

 it's actually different than the original sentenceï¼Œ rightï¼Œ I see many years laterã€‚

 when he encountered a shooting groupï¼Œ rightï¼Œ So firing squad got converted a shooting groupã€‚

 Colonel Or Oreliano Buweia had to remember that distant afternoon when his father took him in search of iceã€‚

 So here to discover ice has been replaced within search of iceã€‚

 But the rest of it is pretty similarã€‚So one way you could think of to add more noise to this processã€‚

 and I think this is a pretty reasonable paraphraseï¼Œ rightï¼Ÿ

 You could imagine adding a second pivot languageã€‚ so maybe we canã€‚Before we go back to Englishã€‚

 we can translate this finished sentence toï¼Œ let's sayï¼Œ Chineseã€‚

And then we'll translate the Chinese to Englishã€‚Okayï¼Œ so here we get some more differencesã€‚

 So we now see years laterï¼Œ we still see the shooting group partã€‚

 but the rest of the sentence is kind of unchangedã€‚So let's see if we canã€‚

Add some more stuff into here toã€‚Add more noiseã€‚ But you see the the general processï¼Œ rightã€‚

 as I kind of mess with this sentence and add more pivot languagesã€‚ So here I'll translate to checkã€‚

Maybe I'll just go all out and translateã€‚The checkzech to Arabic and then the Arabic to Chineseã€‚

 Okayï¼Œ this should beã€‚Okayï¼Œ so our final version hereï¼Œ years laterï¼Œ when he met a shooting groupã€‚

 Colonel Orlano Wend must remember that distant era when his father was looking for him on the iceã€‚

 So we see that through this processï¼Œ we've actually corrupted the semantics of the sentenceã€‚

 we've gone from his father took him to discover ice to his father was looking for him on the iceã€‚

 So this is one of the disadvantages and what I was trying to show through this process is that sometimes the noise that's injected can be so much that it warps the semantics of the sentenceã€‚

 but sometimes he can actually tolerate quite a bit of semantic warping in this adversarial example caseã€‚

 rightï¼Œ as long as it doesn't change the label of the sentence in the downstream taskã€‚

 it's probably a reasonable exampleã€‚ But ideallyï¼Œ we would want it to be a paraphraseã€‚ Okayã€‚

 I see a number of questions hereã€‚ğŸ˜Šï¼ŒAre there some languages that work better than others for English paraphrase generation This is a great question I think it's an important research question that hasn't been well studied because just for the fact that it's very hard to control this task so ideally you would have you know a couple paraphrases you generated by through finish some through checkã€‚

 some through Arabicï¼Œ some through Chinese and you would compare their quality somehow but the problem is that the data sets that you have for the translation task right I need to get this translation model in the first placeã€‚

 you can't really control for the size of those data sets so for exampleã€‚

 we have a huge amount of English to check parallel data we don't have as much English to finish parallel data and you can't just say okayã€‚

 let's use a fixed number of training sentences for each language pairã€‚ğŸ˜Šã€‚

That's kind of unfair for different languages because some languages might just be very dissimilar to the source language and require more training examplesã€‚

 So yeah it's very unclear what what languages work better than others for you know adding diversity while still maintaining the semantics intuitively it could be possible that languages that have a very different structure overall from the source language could be good pivotsã€‚

 but I don't think this has been proven yeahã€‚ğŸ˜Šï¼ŒOkayï¼Œ so another questionã€‚

 how about English to finish to English to Arabic to Englishï¼Œ Sureï¼Œ let's try that English to finishã€‚

Finish to Englishã€‚English to Arabicï¼Œ Arabic to Englishã€‚

And get rid of this one let's see see what happens hereã€‚Ohï¼Œ this one hasï¼Œ yeahï¼Œ some moreã€‚

Several years laterï¼Œ when faced with a group shootingã€‚å—¯ã€‚Yeahï¼Œ but thisï¼Œ this part is interestingã€‚

 His father escorted him in search of iceã€‚ So at least it doesn't make the same error as beforeã€‚

Can't we translate from English to Englishï¼Œ Wellï¼Œ likeï¼Œ what does that really meanï¼Œ rightã€‚

 How are you going to get data for English to English translationã€‚

 that's kind of what I was talking about earlier that there is a lot of parallel data for translation between different languages just in the wild right people have to create translations of various documentsã€‚

 But there isn't that magnitude of data for just paraphrasesï¼Œ rightã€‚

 So English to English is just a paraphrase taskã€‚ But you rarely see like here's one documentã€‚

 Here's the same documentï¼Œ completely paraphrasedï¼Œ right that that's not a natural task for humans to doã€‚

 And it's very difficult to obtain that kind of dataã€‚ So yeahï¼Œ actuallyã€‚

 I do wonder what happens if you do English to English translation here not to labor this pointã€‚ğŸ˜Šã€‚

Yeahï¼Œ it just outputs the exact same sentenceï¼Œ so it probablyã€‚

 it's definitely not using some model to do thisã€‚

![](img/461fd5a1f19fd649a39bbb3561d347a7_3.png)

Okayï¼Œ greatï¼Œ so back to the slidesã€‚Oopsï¼Œ how do I play sledge and do okayï¼Ÿ



![](img/461fd5a1f19fd649a39bbb3561d347a7_5.png)

So againï¼Œ in your homework you'll get a chance to play around with it's a kind of open- ended problem we've asked you to figure out some back translation strategy that kind of works best for the specific downstream tasks that we've given you so you can experiment with a bunch of different pivot languages and pick some strategy that seems to work best and also try and explain why that's working better than other things that you've choseã€‚

ğŸ˜Šï¼ŒIntuitivelyï¼Œ thoughï¼Œ you want to maximize the diversity both lexically and syntactically while still maintaining at least reasonably the semantics of the sentenceã€‚

Okayï¼Œ so this back translation process that I've just shown you is an instance of uncontrolled paraphrase generation right there's no way for me to tell the translation model I want a specific syntactic construction to be produced out of this back translation process rightã€‚

 the back translation process were just kind of at mercy of whatever these translation models have learned and whatever the inductive biases we've put into our models and whatever kind of training distribution we've observed in the translation dataã€‚

ğŸ˜Šï¼ŒBut in the case of adversarial examplesï¼Œ we might want to achieve some level of precise control over the syntactic form so that we can tell the modelã€‚

 I want you to produce an instance of this like very rare constructionã€‚

 because we might suspect that the model is not robust to thatã€‚

So let's just say we have a huge corpusï¼Œ a data set that we've constructed through back translation of paraphrase pairsã€‚

 So one way I can come up with a controlled paraphrasse generation system is to label these paraphrasse pairs with different kind of properties of the transition between them rightã€‚

 So I might have a paraphrasse pair that I've generated through back translation that is like she drives home versus she is driven homeã€‚

ğŸ˜Šï¼ŒActuallyï¼Œ I don'tã€‚Know of this is active toã€‚ Yeahï¼Œ this is an active to passive transformationã€‚

 rightï¼Œ So the first sentence is written in active voiceã€‚Oh noï¼Œ this is notã€‚ Sorryã€‚

 This example is badã€‚ These two sentences mean different thingsã€‚Okayã€‚

 I can come up with I'll fix this on the slidesï¼Œ but rightï¼Œ I couldã€‚Or I guess maybe she isã€‚Okayã€‚

 I can't think of the passive transformation off the top of my headã€‚ Anywayã€‚

 So like you could imagine if I had a valid example here that I could mark these two sentences asã€‚ğŸ˜Šã€‚

Like the first one is written in active voiceã€‚ The second one is written in passive voiceã€‚

 and then I can train a supervised model to given in this first sentence and some labelã€‚

 like passiveï¼Œ produce the second sentenceã€‚ The problem isã€‚

 it's hard for me to come up with a lot of these rulesã€‚

 And I might want some more general way of controlling the the modelã€‚

So another way to do it is use a kind of full syntactic parse as supervision to this modelã€‚

 So if I have a sentenceï¼Œ likeï¼Œ isnn't that more a topic for your priest and my back translation system producedã€‚

 Are you sure that's not a topic for you to discuss with your priestã€‚

 I can extract some sort of syntactic form for each of these sentencesã€‚

 and then tell the model that given sentence 1ï¼Œ I'm going to generate sentence 2 conditioned on the syntactic form of sentence 2ã€‚

So we haven't again talked a lot about parsingï¼Œ but againï¼Œ you knowã€‚

 sentences follow this kind of tree structure in a common grammarã€‚

 we have constituents that are common to many different sentencesï¼Œ for exampleã€‚

Joe and Jill here is a type of noun phraseï¼Œ We shopping is a verb phrase and so onã€‚

 so I can kind of get a tree structure for any sentence by passing it through syntactic parserã€‚

 and I can use the output of the parser like all of these little labels here and the structure as a form of supervision to tell my paraphrase model that I want to generate a paraphrase with this specific tree structureã€‚

 that's essentially the intuition hereã€‚ğŸ˜Šï¼ŒSo if I give itï¼Œ isn't that more a topic for your priestã€‚

 and then I give it the syntactic parse of the second sentenceã€‚ Nowã€‚

 you can imagine that this model is a little more constrained because it has this syntactic inputã€‚

 and it's more likely to generate this sentence versus in the back translation caseã€‚

 there's no way for us to specify that we want to get this this sentence as outputã€‚

So the kind of intuition hereï¼Œ the exact details are not or too importantã€‚

 but the distinction is that back translation produces uncontrolled paraphrases and we can get controlled paraphrases on top by kind of labeling the output of a back translation systemã€‚

Okayï¼Œ soã€‚What if we don't have a paraphrase with a given tree structureï¼Œ That's the thingã€‚

 So these paraphrases hereï¼Œ isn't that more topic for your priestã€‚

 And are you sure that's not a topic for you to discuss with your priestã€‚

 These two sentences are created by sorryï¼Œ the second sentence is created through back translationã€‚

 So we use it as a kind of pseudopar data here that we have this original sentenceã€‚

 We create the second sentence through back translationã€‚ Then we parse the second sentenceã€‚

 So we get we get the tree structure of the second sentenceã€‚

 Then we train a different model to conditioned on the parse that we have the ground truth parse for the second sentence to produce a second sentenceã€‚

So we always know that we have a paraphrase with this tree structure because we got the ground tree paraphrase from the back translationã€‚

 that's the intuitionã€‚How do we input these conditions for the syntactic controlã€‚

 Do we have a model that gives a syntactic structure of a given sentenceï¼Ÿ Yeahã€‚

 so I haven't talked about that hereï¼Œ but the thing that produces this tree is an external parserã€‚

 So these are tools that are available in say Spaceyã€‚

 which is a common NLP library that people use a lotã€‚

 it produces structure like this from any sentence that you given in addition to lots of other types of preprocessingã€‚

 you could imagine doing the same thing with a part of speech tag sequence or something like that as wellã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so I'm going to actually gloss over the model here or maybe I can go through it fastã€‚

 So if we have these two sentencesï¼Œ the man is standing in in the water at the base of a waterfall and the man at the base of the waterfall comma is standing in the waterã€‚

 We know first of all that the second sentence was produced by back translation processã€‚

 we've parsed both of these sentencesã€‚ğŸ˜Šï¼ŒAnd now our goal is given the first sentence and the syntax of the second sentenceã€‚

 produce the second sentenceã€‚So you can already with all of this stuff that you've learned so far build a system to do this very easily and using the Hugingface library that you're using in Home Oneã€‚

 it should be fairly straightforward to set up this modelã€‚

 but essentially we're going to first encode our input sentence right we know various ways of encodecoding sentences at this pointã€‚

 I can use Elmoï¼Œ I can use Bï¼Œ I can use Roberta or whatever right so some method to encode the meaning of the first sentenceã€‚

ğŸ˜Šï¼ŒWe also want some way of encoding that parse form right so in our workï¼Œ we used a recurrent modelã€‚

 but you could also imagine just fine tuning Bt on this new vocabulary of syntactic constituentsã€‚

 just something to give you representations of these different parts of the parseã€‚

 and then you can tie everything together with a decoderã€‚

 so this is essentially a sequence to sequence model hereã€‚

 we have a decoder that's producing the man at the base of the waterfall stain in the waterã€‚

 the second sentenceã€‚ It's conditioned on both the encodeder and the parse encoder hereã€‚ğŸ˜Šã€‚

And so we obviously know many different ways of decoding rightï¼Œ we can use a transformerã€‚

 we can use just substitute a pretrain language model here and fine to unit it for this task there are many different ways of hooking up all of these different modules together so I've just given some examples hereã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so I'm going to skip over this template thingã€‚Essential for usabilityã€‚

 you might not want to put in a full parse right at test timeã€‚

 it's kind of hard for a human to come up with thisã€‚

 but we can relax this to just a high level template form of the sentence right so instead of saying she drove home is like this long bracketed tree structureã€‚

 we can just say it's a simple sentenceã€‚ There's a noun phrase she and a verb phrase drove homeã€‚ğŸ˜Šã€‚

Okayï¼Œ so how do you evaluate paraphrases rightï¼Œ we know for a machine translation that we have the blue scoreã€‚

 but we may not want to use blue score for paraphrase evaluation so it's probably a good exercise to think about why but just off the top of your head right it's obvious thatã€‚

ğŸ˜Šï¼ŒAny paraphrase system is going to produce words or even engrams that are very likely to overlap with the input sentence just based on the definition of paraphrasingã€‚

 but that doesn't necessarily mean that the sentence that you've output is preserving the semantics of the original sentenceã€‚

So paraphrase is generally evaluated through human evaluations rather than automatic ones and a common scale that people use is this three point scale so here a score of zero means that the output sentence has no semantic relation to the input that it's completely warp the semantics and a score of one means that you did kind of preserve the semantics but your paraphrase was not fluent it was ungrammatical and then a score of two which is what you want is your paraphrase was both grammatical and it preserve the semantics of the input sentenceã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so this controlled model that I just described gets a two a score of two which is what you want about 60% of the time this back translation here NMT BT BT stands for back translation performs about similarlyã€‚

 so I would expect this has been about two years since we got these numbers nowadays the problem theã€‚

ğŸ˜Šï¼ŒPercentage of paraphrases that score a2 is probably much higher than 65 due to a lot of advances in model scale and training data sizeã€‚

 as we've seen with things like GPT2ã€‚Okayï¼Œ but more interestinglyã€‚

 can we use these paraphrases to see like how many of them break our models in the setting of adversarial examplesï¼Ÿ

ğŸ˜Šï¼ŒSo what do we mean by breakï¼Œ basically just looking at the examples from beforeã€‚

 if I have some held out example that the model originally predicts correctlyã€‚

 but for at least one paraphrase that we generate the model predicts it incorrectlyã€‚

 then we consider this example to be brokenã€‚And we evaluate this on sentiment analysis and textual entailmentã€‚

 but I just wanted to show some examples just to give you an idea of how you can use paraphrase for this problem for adversarial examples here we have this sentence I'd have to say the star and director are the big problems here and so this is obviously has a negative sentiment right they're criticizing the acting and directing of this movie you can feed this to this controlled paraphrase model and then you get some resultã€‚

 by the wayï¼Œ you know the star and director or the big problemsã€‚ğŸ˜Šï¼ŒObviouslyã€‚

 the sentence has not changed as semantics or at least a sentiment related part of the sentence in any meaningful wayã€‚

 but the model predicts positive for this paraphrase and negative for the original sentenceã€‚

 So we can say that this paraphrase is an adversarial exampleã€‚Similarlyï¼Œ for the case of entailmentã€‚

 we have the man is standing in the water at the base of a waterfallã€‚

 and a man is standing in the water at the base of a waterfallã€‚

 So this pair of sentences was marked as there is an entailment relation between these two sentencesã€‚

 They're basically identicalã€‚But if you rephrase this first sentenceã€‚

 so you see we move this clause at the base of the waterfall into the middle of the sentence using the syntactic paraphraserã€‚

 now the entailment is no longer predicted and it's a neutral sentenceã€‚Okayã€‚

 so we show that you can you know break a huge portion of a lot of these data sets in this wayã€‚

 which means that our models are not robust to all paraphrases of a given input sentence right and you need to be aware of the training distributionã€‚

 the like distribution of syntactic formsï¼Œ the distribution across your vocabulary and this is a good avenue to explore what type of augmentation might be best for your data set taking a step back if you are using an NLP model for some task and it gets some certain performance and you want to try and improve itã€‚

A natural way is to figure out what kinds of inputs it fails on and then add instances of those kinds of failures to your data set so that it won't make those errors anymoreã€‚

And just as an exampleï¼Œ if you do simple data augmentation where you just add these adversarial paraphrases into your training data and retrain your modelã€‚

 the percentage of examples you can break in this way decreasesã€‚

 although it's still a nontribual percentageã€‚ğŸ˜Šï¼ŒOkayï¼Œ and finallyï¼Œ just to show some examples of thisã€‚

And also some failure cases hereã€‚ we have this sentenceã€‚

 you seem to be an excellent burglar when the time comes and you can put in these various kinds of syntactic forms like here you want some dependent clause followed by a comma than a simple sentence and it does this kind of reordering when the time comesã€‚

 you'll be a great thief but you can even extend this to like more bizarre formã€‚

 So here this form has some quotation marksï¼Œ like it wants to put in some dialogueã€‚

 I guess and the model just hallucinates phrases or words to fill in this form right it adds quotesã€‚

 it's kind of quoting the second person you hereï¼Œ this is obviously changing the semantics of the sentenceã€‚

 but it's doing you know a reasonable job with this input construction you can also ask the model to make this into a question which you might want to do I guess for some purposes can I get a good burglar when the timeã€‚

ğŸ˜Šï¼ŒYou knowï¼Œ it's obviously not a paraphrase of this sentence you can't really get a paraphrase of the sentence in a question form but it's doing a good job and there are obvious failure cases here look at the time the thief comes If you provide some form that's completely unsuitable for the input sentence So this is just an idea like one specific instance of paraphrase generation in this SCPN frameworkã€‚

Wellï¼Œ okayï¼Œ I wanted toã€‚Kind of there are more examples that you can look at in the slidesã€‚

 but for the sake of timeï¼Œ I just wanted to spend the last few minutes here talking about a different use case of paraphraseã€‚

 So we've so far focused on this adversarial example generation and data augmentation angleã€‚

 but there are newer instances of using paraphrase generation in NLP research that I want to discussã€‚

 but before thatã€‚ğŸ˜Šï¼ŒThere's a question do we have a public collection data set of adversarial examples generated through paraphrase to test our model or do you have to generate them yourselves that's a good question So this original workshop the build it break it workshop that I mentioned at the beginning of the lecture they did release a couple examples of adversaries that break their modelsã€‚

 but it's challenging to have this public data because a lot of these adversaries are specific to certain modelsã€‚

 certain model architectures in certain data sets like downstream data sets so yeah it's not really something that is a static collection since our models are consistently getting better and very rapidly getting better getting trained on bigger and bigger data sets it's not clear that those adversaries will generalize to different kinds different settingsã€‚

ğŸ˜Šï¼ŒNoï¼Œ there's no like large publicly available collection of adversariesã€‚

 you should generate them yourselves if you're interested in this kind ofã€‚Analysisã€‚Okayã€‚

 so another task that has over the past two years seen a lot of attention within NLP is very related to paraphrase generation is the task of style transferã€‚

 it's another instance of controlled paraphrase generation so we're asked to given an input sentence basically generate a paraphrase of that sentence but also modify the output to contain some certain stylistic propertiesã€‚

ğŸ˜Šï¼ŒAnd at this point you might wonder what is a stylistic property and that's a great question which I can't answer and many different fields have studiedã€‚

 you know what exactly style is some fields don't even have a distinction between the style of a sentence and its semantic meaningã€‚

 this is a common view in sociolinguisticsã€‚ğŸ˜Šï¼Œbut here and in generalã€‚

 we're going to consider style to just loosely represent lexical and syntactic patterns that are associated with some particular collection of documentsã€‚

And you knowï¼Œ it's hard to defineï¼Œ but it is kind of something that we know at least extreme examples that when we see itã€‚

 we we can easily recognize it right these are two examples of different stylesã€‚

 Shakespeare is the first right we can see that if we are familiar with Shakespeareã€‚

 this is obviously Shakespeare and not modern text right the syntactic constructions are not patterns that we use these days the lexicon here is different right we don't use tis anymore right we would say whether it is I guess that's what tis isã€‚

ğŸ˜Šï¼ŒYeahï¼Œ so rightï¼Œ no one no one speaks like this or writes like this anymoreã€‚

 This is clearly an instance of the styleï¼Œ whether it's specific to Shakespeare himself or that time period how people were writing in that time and placeClearly it's a distinct style on the second handã€‚

 on the other handï¼Œ we have you knowï¼Œ just tweets so I collected some random tweets today here and we can observe that this is completely different than Twitter right we have theseã€‚

ğŸ˜Šï¼ŒYou knowï¼Œ things that are notã€‚Gramaticalï¼Œ we have lots of fragmentsï¼Œ abbreviationsã€‚

 dropping of punctuationï¼Œ rightï¼Œ The apostrophe is droppedã€‚ We have slang languageã€‚ğŸ˜Šï¼ŒAnd so onã€‚

So againï¼Œ a very distinct styleã€‚And this task of style transfer has a lot of applications kind of related to all of the applications that paraphrase hasã€‚

 but I haven't talked about some of these so I just wanted to highlight them So you've seen data augmentation text simplification is another very useful application of these things so you might write some sort of Shakespearean sentence and then realize that your audience is not gonna to appreciate that right maybe you want to tweet it out So mean okay that's an extreme exampleã€‚

 but maybe you want to simplify your writing make it easier to understand and more readable to a more general audience it could also help with writing assistance maybe your style is yeah againã€‚

 not suited for a particular audience you can get examples of sentences that you write that are in a different style that might be more desirableã€‚

ğŸ˜Šï¼ŒAn interesting application is author anonymizationã€‚

 so you might want to write something without anyone being able to tell that you actually wrote itã€‚

And actually machines are very good at detecting or doing author identification this is one of the few tasks at which machines are actually better than humans at performingã€‚

 so things like author identification so telling predicting who wrote a particular text or deception detection is another interesting example where machines are much more reliably able to detect whether say a yelp review is fake or realã€‚

 compared to humansã€‚ğŸ˜Šï¼ŒAnd there are many other applications of this kind of thingã€‚

So how do we use paraphrase models for style transfer and in this project we're going to also integrate in these huge scale language models that we've seen in the past so the first thing to do is collect data sets of sentences that come from different styles right so I could crawl Twitter I could download all of Shakespeare's plays I could do many different things to get different data sets of stylistically distinct textã€‚

ğŸ˜Šï¼ŒAnd againï¼Œ just like with paraphrase generation we have the same problem with style transfer right no one is going to sit down and translate this like broron haters and shambles into Shakespeare language right so I'm not going to be able to get publicly available parallel data of style transfer right so I need again to create my own parallel data using the tools that I have access to and specifically back translation again is the way that people nowadays turn to when they want to create this kind of pseudopar dataã€‚

ğŸ˜Šï¼ŒSo I could createï¼Œ I could crawl Twitterï¼Œ I could take all of these tweets that I've collected and I could run them through a back translation system right so I could do exactly what I did in that coab notebookã€‚

ğŸ˜Šï¼ŒActuallyï¼Œ not to waste too much timeï¼Œ but I am kind of interested in knowing what would happen if I put like this sentence into thatã€‚



![](img/461fd5a1f19fd649a39bbb3561d347a7_7.png)

Translation thingï¼Œ rightï¼Œ because the translation systems are not trained for texts like thisã€‚

 They're trained on like European parliamentary proceedings or movie scripts and stuff like thatï¼Œ soã€‚

Ohï¼Œ hopefully you can see my screen still butã€‚Okayï¼Œ so I'm going to translate this to sayã€‚Finishã€‚

Let's just pass it through this whole block ofã€‚Translations that we have hereã€‚å—¯ã€‚ç–²ã‚Œ Chineseã€‚ğŸ˜€å‘µã€‚ğŸ˜Šï¼ŒOkayã€‚

 so it actually didn't do a terrible jobã€‚Braron haters are confused about wishing his retirement so badã€‚

That is much better than I expected it to beã€‚Okayï¼Œ but anywayã€‚

 you can see how I could come up with a paraphrase of even a tweet through the same processã€‚

 even though these models aren't trained on tweetsã€‚

 they still can output reasonable predictions aboutã€‚Things that they're not trained onã€‚

 I think Google Translate is probably particularly good in this regardã€‚å—¯ã€‚Ohpeï¼Œ whoopsã€‚

Sorry about thatã€‚Okayï¼Œ whatever we can just stick with this So anyway I have created this data set of paraphrased stylistic sentences and now I'm just going to fine tune some existing huge scale language model to perform a task of that we call inverse paraphrasingã€‚

ğŸ˜Šï¼ŒSo how does this work step one we just saw in the collab notebookï¼Œ I take a sentenceã€‚

 this one is written in Shakespeareï¼Œ why Uncle Tis a shameme and I feed it through my back translation processã€‚

 I get an output like it's a shame uncleã€‚In the second step we train a model to perform inverse paraphrasingã€‚

 so we train essentially a reverse model to go from the output of the back translation process to the original sentenceã€‚

And in this processï¼Œ this inverse paraphrase model is going to learn properties of the original style in which this input sentence was writtenã€‚

 rightï¼Œ If I force this inverse paraphraser to always generate a sentence that's in Shakespearean textã€‚

 it's going to naturally learn all this stylistic or many stylistic properties associated with Shakespeareã€‚

So I can do this for not just Shakespeareï¼Œ right I've also collected some Twitter data setsã€‚

 So the second group of arrows hereï¼Œ I have the sentence no lieï¼Œ I would jump inã€‚

 I back translate itï¼Œ that's this plaque arrow and I get something like I jump in there no doubtã€‚

 and then I'm going to inverse pair ofphrase itã€‚ So during training I'm going to train a separate model to convertã€‚

 I jump in thereï¼Œ no doubt back to no lie I would jump inã€‚

And then the interesting thing is at test timeï¼Œ so now I have two separate modelsã€‚

 two separate inverse paraphrasersï¼Œ I have the inverse paraphraser for Shakespeare that's only producing output paraphrases in Shakespearean style and I have my output paraphraser for Twitter so at test time I can actually kind of mix and match these thingsã€‚

ğŸ˜Šï¼ŒSo if I have this sentenceï¼Œ oh wilt thou leave me so unsatisfiedã€‚

 I'm going to back translate it as usualã€‚ Oh you're going to leave me unsatisfiedï¼Œ rightã€‚

 But now instead of using the Shakespeare inverse paraphraserã€‚

 I can do style transfer from Shakespeare into Twitter by swapping out the inverse paraphraser with the Twitter inverse paraphraser that I trained over here and I get something likeooã€‚

 y'all will leave me unhappy LoLã€‚ So in this exampleã€‚

 I've actually translated I've done style transfer from Shakespeare to Twitter while still kind of reasonably retaining the semantics of this original sentenceã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so we have a question do we have a publicly available model for style transfer can we use any style in it or are theres some standard styles that these models can do transfer to So there are many models that have been made available for very specific instances of style transfer so one is like polite sentences to informal sentences or Shakespeares sentences to modern English but like as far as a general purpose style transfer system you can if you're interested in getting access to a model that can do something like thisã€‚

 I would recommend talking to the T Colpeche who actually develop this model that I'm presenting here and his model is or will soon be available and we can get you access to that if you want to play around with itã€‚

 but it can basically do style transfer between like 11 different stylesã€‚ğŸ˜Šã€‚

What is a style of the output text we get from the example of broze hatersã€‚

 what if we want the output text from back translation in a specific style So switching back to the coab notebook hereã€‚

Already thatã€‚So remember that in this back translation processã€‚

 we have no level of control as to what is spit out by this processï¼Œ rightã€‚

 So I've fed this through you knowï¼Œ numerous different language pairs and I don't know what's gonna happenã€‚

 Like I literally didn't know what was going to happen when I put this sentence into this this pipeline hereã€‚

 So I have no way of telling the back translation system to produce something that looks Shakespeareanã€‚

 The back translationï¼Œ you can kind of view as like a black boxã€‚

 It just kind of adds some noise syntactically and lexically to the original sentenceã€‚

 and it kind of normalizes its styleï¼Œ rightï¼Œ So here it's unclear of this sentence as a Twitterã€‚

 a tweet rightï¼Œ or could be a tweetã€‚ I meanï¼Œ I guess you couldã€‚ğŸ˜Šã€‚

Reasonably predict that it's something you could see on Twitterï¼Œ but compared to this input sentenceã€‚

 which isï¼Œ you knowï¼Œ so obviously from some sort of social media platformã€‚

 this one is is more formal in gramizingï¼Œ sling and so onã€‚

 So it kind of accomplishes a process of normalizationï¼Œ but going back to thisã€‚ğŸ˜Šã€‚

In this pipeline that I describedï¼Œ we essentially just need the back translation to modify the input sentence in some way and then the role of the inverse paraphraser is to inject the stylistic information into its outputã€‚

 so the inverse paraphraser is where the style actually is encoded in the models that we're learningã€‚

ğŸ˜Šï¼ŒDo we need to make sure that paraphrase is not in Shakespeare Twitter style Do you mean the output of the back translation system Because if so remember that this is happening at scale right so we have you know potentially hundreds of thousands of tweets that we fed through back translation process and yeahã€‚

 in general it's fine if some of the time this back translation process fails and it fails to like normalize the text so maybe like some of the time this sentence could also look Shakespearean but as long as for some amount of some number of instances there's a significant change in the backtransd paraphrase that gives the inverse paraphras or enough signal to learn theã€‚

ğŸ˜Šï¼Œå—¯ã€‚To inject some stylistic informationã€‚Is this style transfer similar to that used for neural style transfer for art or images Not really so the neural style transfer for computer vision works very differently in that you actually can take the layers the layerwise representations of a convolutional network and kind of modify forward pass of one function of one image as fed through a convolutional network using the representations that you get from a style specific image like a texture or a painting or something but in this case you're actually training a model in the image caseã€‚

 at least for the original style transfer stuff you didn't need to train anything at this scaleã€‚ğŸ˜Šã€‚

For some regional languages like African Englishï¼Œ I guess you mean Africanam Americanan English can you can style transfer work for converting basic standard English to African or Philippine English yeah so actually one of our results is doing something like this so this is just a table of a bunch of different style transfer directions so here we have one data that comes from an existing data of African Americanican English tweets so you can see that it does inject some common forms of Africanam Americanan English into the output there's also things like you know converting a sentence that was written in 1990 to the style of an English sentence in 1810 so you can see significant lexical and syntactic differencesã€‚

ğŸ˜Šï¼ŒOr converting a tweet to biblical text right so here when you're going through an episode and can't cope anymore as a tweetã€‚

 when thou art in an episode and cannot stand it anymore more semi say what the model produces in this kind of biblical formatã€‚

I kind of missed itï¼Œ but how is the neural the finetu back translation better than just repeated Google translation Oh well Google Translate requires you to use this API right and make API calls you don't actually have direct access to the model itself so it's kind of expensive to do this at scale in this approach that I've just mentioned we don't use Google translateã€‚

 that's just something that we're using in your homework to make things a little easier but yeah in general you want control of your models so not only because it's faster to generate these back translated paraphs but you can also do a lot of filtering on top of the output of these models to for exampleã€‚

 incentivize diversity lexical diversity or syntactic diversity you can similar to how we went through the syntactically controlledã€‚

ğŸ˜Šï¼ŒParaphrase modelï¼Œ you can also fine tune a paraphrase system on back translated data to kind of emphasize diversity lexically and syntactically soã€‚

ğŸ˜Šï¼ŒYeahï¼Œ actuallyï¼Œ it's unclear if using Google Translate would be better than using you some existing pretrain back translation system that yeahã€‚

 someone might want to try that in the futureã€‚Okay so I guess that's all I wanted to say next time we're going to switch gears and talk about kind of the root of all NLPã€‚

 which is data data creation collection and specifically how you kind of proper ways of doing this in a crowdsourced setting using say mechanical Turk so this has become increasingly more important nowadays as we try and create more and more complicated data sets for to test out the abilities of our model like we went through you know GT3 can do all of these fancy can reason at a fairly high level can it solve this task can it solve this task and as we've seen examples of these tasks are hard to get just naturally in the wild examples of paraphrases for exampleã€‚

 it's hard to get so yeahã€‚Often resort to crowdsource data collection for this and that's what we'll be talking about next timeã€‚

 so slightly different less technical topic than other lectures but also very interesting and importantã€‚



![](img/461fd5a1f19fd649a39bbb3561d347a7_9.png)

Okayã€‚