# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÈ©¨Ëê®ËØ∏Â°ûCS685 ÔΩú Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËøõÈò∂(2020¬∑ÂÆåÊï¥Áâà) - P7ÔºöL5- Ê≥®ÊÑèÂäõÊú∫Âà∂ - ShowMeAI - BV1BL411t7RV

Okay hey everyoneÔºå so today we're going to be going over the concept of attention mechanisms„ÄÇ

 so these are a core building block of modern day NLP systemsÔºå specifically the transformer model„ÄÇ

 so we'll start by introducing attention mechanisms in the context of recurrent neural networks which we covered in a previous video and then we'll switch over to the self-atten mechanism which powers most of our modern models„ÄÇ

Before we do that„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_1.png)

We had a couple things from last time first your homework zeros should hopefully all be submitted by now„ÄÇ

 we'll have the grading done by sometime next week homework one you should also look out for that it'll be released within the next couple of weeks and the purpose of this homework is to get you acquainted with using some of these neural language models specifically in the context of transfer learning which will be our topic for next week„ÄÇ

ÂóØ„ÄÇAnd finallyÔºå all of the group assignments for your final project have been finalized at this point and your proposals are due September 21t„ÄÇ

 I'd highly encourage you to come to office hours if you're unsure about your current idea or how feasible it is either come to mind office hours or any of the TAs„ÄÇ

 so all of us should be able to give you useful feedback„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_3.png)

Okay so with that out of the way let's get into our topic for the day so this is just a refresher of the recurrent neural language model so remember we saw this a couple lectures ago as an alternative to the fixed window feed forward network and this model again precedes left to right word by word so we first compute a representation of this prefix of the sentence with just the word the so that's this hidden state then we compute one for the students that's hidden state number two H sub2 and the students open then the students open there so this representation H sub4 contains all of the information that we need to know about this prefix in order to predict the next word so in the immediately preceding video to this one we looked at the cross„ÄÇ

üòäÔºåEntropy loss as a way to actually train these neural language models to predict the next word specifically we do this by minimizing the negative log likelihood of the correct word so in this case if we observe the students open their books we would minimize the negative log probability of books and that's our loss function we went over and implementation of a fixed window feed forward language model in the previous video as an exercise it would be you a useful thing to do to take that collab notebook and replace the architecture with a recurrent one so you understand how the sequential prediction works„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_5.png)

OkayÔºå so this was just a review and just to go over the advantages and disadvantages of this model again„ÄÇ

 firstÔºå unlike the fixed window feed forward model„ÄÇ

 the recurrent neural network or RN can theoretically handle context of any length because it proceeds left to right„ÄÇ

 there's no concept of a window size in an RN and because it shares parameters right so this WH and this W sub E„ÄÇ

 these are shared across all the time steps so we're not adding any extra parameters„ÄÇ

 the more words we process unlike the fixed window model in which we have to make our projection matrix larger every time we increase the window size„ÄÇ

üòäÔºåOkayÔºå but so these are all good things but why don't people use RNNs much these days anymore so there's a couple of reasons one is that this kind of recurrent computation is slow especially at training time and you'll note that the computation of H sub3 here depends on the computation of H sub2 so I can't compute H sub3 without having first computed H sub2 without having first computed H sub1 and so in training I can't compute all of these hidden states at parallel right I need to wait for the first one to have been computed before I can get to the second one and the class of models that we'll be getting into next enables us to compute all of these time steps in parallel which is one reason why we prefer it over the recurrent neural network„ÄÇ

Another reason is that although theoretically the recurrent neural network is able to handle arbitrarily sized context in practice this isn't actually true„ÄÇ

 because of the left to right nature of this model we tend to notice that more recent words so like words that are closest to the current time step„ÄÇ

 they tend to be overrepresented in this representation words that are far away„ÄÇ

 so maybe there were like 10 sentences before this one„ÄÇ

 the words in the first sentence here is are unlikely to significantly influenced this representation this is due to a number of issues perhaps most importantly the vanishing gradient issue„ÄÇ

 so„ÄÇüòäÔºåWhat that means is the signal that we get for predicting this word books is a direct function of this time step H sub4„ÄÇ

 which in turn is a function of all the previous time steps„ÄÇ

 but as we went through the back propagation algorithm„ÄÇ

 you'll note that to compute these gradients that are farther away from the error signal„ÄÇ

 we have to do more and more matrix multiplications„ÄÇ

 so here we see repeated multiplications with for example„ÄÇ

 the transpose of these weight matrices and the derivatives of the non nonlinearities that are used in this so the vanish and gradients problems suggest that once you have these longer chains of products„ÄÇ

 the derivative signal starts to vanish which means that if I have a time step that's 50 words in the past„ÄÇ

 it's going to receive very little error„ÄÇSignal as to how that particular time step influence the prediction of a word 50 time steps in the future„ÄÇ

So I'll put some more links to read about the vanish ingredient problem in Piazza or on the website„ÄÇ

 but these are two of the motivating reasons why we might want to try a different kind of model architecture„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_7.png)

Okay so before we get into that one thing that just to kind of reiterate the points that were made in the previous lecture about how to train these models„ÄÇ

 we looked at training in the context of the fixed window language model in our coab example but how does it actually work for a recurrent neural language model we're still going to use the same cross-enttropy loss right so the negative log probability of the ground truth word and we're going to average this loss across all of the token level predictions in our training data so to be more clear„ÄÇ

 let's take a look at an example So here I have the students opened their exams as my input the same example we've been looking at before except we've been switching this word exams for books„ÄÇ

 laptopsÔºå other things so instead of doing what we did in our exercise before in this setting we're going to predict all of the words in this„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_9.png)

Sequence just using one left to right computation process„ÄÇ

 So as soon as I produce the hidden state associated with the first time step here„ÄÇ

 this word I'm going to put a softmax layer on top and predict the next word students Then I'm going to process the word students I'm going to compute a new hidden state and I'm going to use that with the same softmax layer„ÄÇ

 you see the weight W sub2 is shared across all of the time steps„ÄÇ

 I'm going to use that to predict the next word opened and so on„ÄÇ So at every time step here„ÄÇ

 I get a loss from this softmax layer that's telling me how bad did I do it predicting the ground truth next word So that's this J in the slide and I'm going to simply store up all of these losses at every time step and once I'm finished with this entire sequence„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_11.png)

Just going to average the token level cross entropy loss across all time steps and that forms my total loss for this sequence„ÄÇ

 So in the coab exercise we looked at the concept of batching So in batching I might have multiple different sequences in one batch that are all being processed s simultaneously So this also is true for RNNs„ÄÇ

 I can have multiple different sequences in the same batch and I'm going to average my loss is going to be the average cross entropy for every single token in that batch„ÄÇ

OkayÔºå so this is just a way of taking what we learned from the previous lecture and applying it to RNNs„ÄÇ

 the concept is exactly the same right we're using the crossenttropy loss to measure how good our model's prediction was in this case we're just applying the softmax layer over every time step of the RNN to predict the next word in this language modeling setting„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_13.png)

All rightÔºå so let's get into one of these issues that we discussed„ÄÇ

 which is that RNNs have this problem of recency bias right they encode a lot of information about the words that are closest to the current time step but not much about the words that are farther away and to generalize this is an example of the bottleneck problem„ÄÇ

 the problem is that we're forcing this model to represent the entire context that it's seen up till this point into a single vector right the time step„ÄÇ

 the hidden state at the previous time step„ÄÇSo if we have this sequence„ÄÇ

 the students open there this representation here H sub4 has to encode all of the information„ÄÇ

 all of the relevant linguistic properties of this context the students open there maybe this isn't such a big issue with just a four word context like we have in this example„ÄÇ

 but imagine if our sequence was actually like 500 words along right now we have one vector„ÄÇ

 the hidden state at the 500 time step which is supposedly representing every word that it's seen in the past and in practice like the longer these sequences again„ÄÇ

 the less able the model will be to encode all of the relevant information about the context also you know longer sequences are more complex there's many more interesting linguistic properties than might be useful for this next word prediction„ÄÇ

üòäÔºåIf I have multiple sentencesÔºå maybe I want to model the discourse relations between those sentences and so on„ÄÇ

 so you're asking a lot from your model to stuff all of the relevant information about the context into one low dimensional real value vector„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_15.png)

OkayÔºå and theres a now famous quote by Ray Mooney who's a professor works on NLP at UT Austin„ÄÇ

 you can't cram the meaning of a wholebleeping sentence into a single bleeping beer„ÄÇ

 this quote has become so popular that some paper titles actually quote part of this in NLP conferences„ÄÇ

 and it is kind of a motivating quote for the attention mechanism although originally it wasn't intended as such„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_17.png)

So the idea behind the attention mechanism is what if we go from representing a context with a single vector to using multiple vectors„ÄÇ

 so right now with what we've learned so farÔºå we represent this entire phrase the students open there with this the fourth hidden state right the single vector„ÄÇ

But what if we instead decided to represent this phrase with a function that takes in all four hidden states as input and kind of gives each of them equal opportunity to contribute to the representation of this phraseÔºü

üòäÔºåSo how does that actually workÔºü

![](img/08494681f5d7ba792601ad559962f157_19.png)

And the solution is the attention mechanismÔºå so this was originally proposed as a way to combat this problem of long sequences not being translated well and the problem with the vanishing gradient it was originally proposed for machine translation and the idea there is that attention between the target language and the source language can pick up things that are similar to word alignment so like maybe you have a word in German that always translates to some word in English maybe you always when you're about to translate that word in English want to look over at the German equivalent in the source sentence so this was the original intuition but it was so useful in machine translation that it kind of crept over to other problems within NLP and now it's pretty much used in„ÄÇ

üòäÔºåLiterally every single NLP application„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_21.png)

OkayÔºå so how does it workÔºå The basic concept is that so remember we're now going to represent our phrase with all of the hidden states associated with the words in that phrase„ÄÇ

 So to generalize we're going to have a single query vector and we're gonna to have multiple key vectors and the goal is to score these key vectors using the query vector„ÄÇ

 So that's kind of highlel summary let's make it more concrete by looking at our neural language model setting„ÄÇ

 So if you think about itÔºå we can say that the query of our neural language model is the current hidden state and the keys are all of the hidden states that came previously to that one and so if I have a scoring function that is a function of the query„ÄÇ

 the current hidden state and the keys all of the previous time states sorry the current„ÄÇüòä„ÄÇ

Hidden state and the previous hidden statesÔºå I now have a way of kind of upweighting keys that occur far in the past„ÄÇ

 right because if my query and some key that happened to be a hidden state like 500 tokens ago had a high score„ÄÇ

 however we define our scoring functionÔºå then I'm going to have a resulting representation that upweights that token from 500 words ago in my new representation„ÄÇ

üòäÔºåSo let's take a look at a concrete example to see if this makes more sense„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_23.png)

So let's look at this exampleÔºå the students opened their books„ÄÇ

So here we're going to take the query vector to be the hidden state at the point where we have seen the word books and now we want to go ahead and predict the word that comes after books so before we do that before we get to our softmax layer„ÄÇ

 we're not going to just say that this single representation„ÄÇ

 this green one contains all of the information we need about the students open their books„ÄÇ

 we're now going to allow some of these vectors that occur in the past like this first one to have a chance to contribute more than the recurrent computation would normally allow them„ÄÇ

So how do we do thisÔºå We take our query and our keys remember are these red vectors„ÄÇ

 the previous hidden statesÔºå the things that we've observed in the past„ÄÇ

 so a simple way to do this is just take a dot product between this query vector and each of these key vectors so that's going to give me a single scalar score for every single key vector right and we can interpret these scores as kind of how important is the current time step sorry how important is a particular time step in the past to making the prediction of the next word given that we've now seen the word books„ÄÇ

üòäÔºåSo we get these scoresÔºå one for each key vector by taking the dot product between the query and the key vectors„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_25.png)

And the next thing we do is normalize these scores„ÄÇ So currently I take the dot product right„ÄÇ

 there's no guarantee that the scale of these these scores is meaningful or consistent there could be negative numbers„ÄÇ

 There could be positive numbersÔºå the softm function„ÄÇ

 as we've already seen is a useful way to turn any vector of numbers into a probability distribution„ÄÇ

 which is certainly more interpretable for us right So we're just going to apply the softm function over these a vector formed by these four scores and we're going to get a probability distribution„ÄÇ

 So maybe in this case for whatever reason the model decided to place a high probability on the word on the hidden state associated with the word the So the first hidden state and a low score„ÄÇ

 a low probability on all of the subsequent hidden states„ÄÇüòä„ÄÇ

So we've now gone from our query key formulation to a set of actual scores each associated with a key a previous hidden state of the recurrent model„ÄÇ

So how do we convert this attention distribution into a representation of the previous context that we've seenÔºü



![](img/08494681f5d7ba792601ad559962f157_27.png)

![](img/08494681f5d7ba792601ad559962f157_28.png)

And the way we do this is very simple„ÄÇ We just take a weighted average of all of our key vectors„ÄÇ

 So remember our key vectors againÔºå are previous hidden states using these probabilities that we got as the weights„ÄÇ

 So if I do this in this example that means I'm multiplying this hidden state with this probability„ÄÇ

 which as you can see is higher than all of these I'm then adding that term to this hidden state multiplied by this low probability and so on with all of these other keys„ÄÇ

 So in the end in this example the first hidden state because it was multiplied by presumably quite a high probability score is going to be overrepresented in this final weighted average while all of these hidden states are going to be lower right they're not going to be as prominent„ÄÇ

üòäÔºåAnd so you can see that this kind of gets it allows the hidden states that occur far in the past to contribute a lot more to the current sorry„ÄÇ

 to the next word prediction„ÄÇOkayÔºå so this is kind of an intuitive explanation of what's happening here and note that I mentioned when we first went over the softmax function that it's useful for things just beyond getting final output probability distribution over your labels„ÄÇ

 and this is one of the examplesÔºå right the softm function in here is used to get a probability distribution over previous hidden states„ÄÇ

 which are then used to compute this weighted average„ÄÇüòäÔºåOkayÔºå so now it's kind of interesting right„ÄÇ

 we have the current hidden representation for books„ÄÇ

 this green vector which is which was computed by just using our normal recurrent computation or normal recurrent update equation„ÄÇ

 but we also now have this attention output which is a function of all of the previous hidden states„ÄÇ

 how do we combine these together to get a final representation that we can pass through a softmax layer„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_30.png)

So the original attention papers just simply do this with concatenationÔºå so I take this vector„ÄÇ

 this attention output the red vector at the top I concatennate it with this green vector„ÄÇ

 the current hidden state and then I pass those through a softm layer which will predict the next word so let's say the next word was unwillingly so how does this proceed to predict the following word after unwillingly„ÄÇ

 well we just compute our„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_32.png)

OhÔºå what just happenedÔºüOh here we go whoops So we just compute the next hidden state using our recurrent update equation then we compute new sorry we compute a dot product using the new hidden state as the query and all of these hidden states as keys sorry there should also be an arrow here to the books hidden state„ÄÇ

 this one which I forgot to put in here but yeah there should be five scores here instead of four and so just imagine there's five scores we get a distribution over all five of the previous hidden states and maybe the second one„ÄÇ

 the one associated with students is the one that is most probable here So when we compute the attention weighted average the hidden state associated with students is going to contribute the most to this average we're then going to„ÄÇ

üòäÔºåate this red vector with the green vector for unwillingly and feed that test softax layer to predict the next word and so on„ÄÇ

So this is an explanation of how attention works in neural language models„ÄÇ

 specifically recurrent neural language models„ÄÇBut as we'll see one interesting question is„ÄÇ

 is the recurrent step actually needed at all right so the recurrent step we already noted has some issues with being unable to parallelize the computation of all of these steps right because the hidden state at one time step depends on the hidden state of the previous time step and if we get rid of this recurrent computation right all of these left to right arrows„ÄÇ

 then there are no such dependencies and we can compute all of these representations in parallel„ÄÇüòä„ÄÇ

So this is kind of the intuition of the transformer model„ÄÇ

 but we'll talk more about that in the following video„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_34.png)

OkayÔºå so let's summarize this implementation of attention within recurrent neural language models„ÄÇ

Attention solves this bottleneck issue right so instead of forcing all of the information of the previous context into a single vector now we get to look at all of the previous words through attention„ÄÇ

üòäÔºåRight so it kind of allows equal opportunity for words in the past to contribute more to the current words prediction it also like in this process contributes to the kind of mitigating this vanishing gradient issue because now my time step 500 words ago is not only connected to the current prediction through the recurrent computations which as we said results in the vanishing gradient as we get farther and farther away from the error signal but now the attention component allows us to be connected to all of the hidden states at all time stepss right so my current when I'm predicting the current word I now have direct access to each of the previous time stepss through the attention function so of course if the attention is small I'm probably not going to have a huge error„ÄÇ

üòäÔºåsignal to whatever time step it isÔºå but since the whole point of attention is that we can get we can go beyond just focusing on the immediate preceding like three or four words and focus on some words that are well in the past this helps in reducing this issue„ÄÇ

A third thing that is kind of questionable given new research is that attention provides some level of interpretability into our model so because these attention the scores are actually a probability distribution over the previous words that we've observed„ÄÇ

 we can actually look at that distribution and see what the model is focusing most of its attention on„ÄÇ

 and we can maybe use that to understand how or why the model is making certain predictions„ÄÇ

 although we need to be very careful with thatÔºå some new work has shown that which we'll get into later in the semester has shown that attention is not a good proxy for an explanation of the model's behavior„ÄÇ

üòäÔºåIn factÔºå you can significantly modify the attention weights of a model and force it to focus on something completely different and still have it spit out the same predictions„ÄÇ

 which is a concern if you're trying to explain your model through attention„ÄÇ

So in the machine translation context and this is something we'll talk about a little later in the semester when we switch over to text generation tasks is that because in machine translation you have a source sentence in a source language so here the poor don't have any money and you have it target sentence right the translation that you want to produce„ÄÇ

 I'm not going to try and say these French wordsÔºå you can actually see that the alignments from the source to the target are kind of reflected in the attention scores„ÄÇ

 so here the dark gray out squares represent higher scores„ÄÇ

 you can see that the wordpo gives high attention to I assume it's French translation„ÄÇüòä„ÄÇ

So the concept of word alignments between different languages is something that attention has been shown to pick up in machine translation„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_36.png)

OkayÔºå so we haven't actually talked too much about the implementation of attention„ÄÇ

 We've just so far focused on the dot productÔºå rightÔºå this third line between the query and the key„ÄÇ

 but there have been manyÔºå many different variants„ÄÇProposed for the scoring function„ÄÇ

 the thing that comes before the softne„ÄÇRight I can really use anything that given a query vector in a key vector will give me a single scalar score„ÄÇ

 so the original formulation of this a of Q of a is the attention function Q is a query key Q is a query K is the keys in the original formulation it was basically a feed forward layer that takes as input„ÄÇ

 the concateninated query and key representation multiplies that by a weight matrix„ÄÇ

 then applies a non nonlinearityÔºå then multiplies it by a weight vector to get a single scalar score so this vector is the same dimensionality as the projected query key concateninated vector„ÄÇ

üòäÔºåSo this one was originally proposed maybe because they felt like they needed to get some learned parameters separately into the attention computation there have since been simpler version so this is just the biline product„ÄÇ

 it's basically like the dot product with an extra projection matrix in there then just the dot product it also works fairly well and the one that's most commonly used nowadays is a scaled dot product„ÄÇ

 So you take the dot product and the scale it by the magnitude of the key vector this is to combat the observation that if you have queries and keys that are larger in dimensionality„ÄÇ

 the dot product also tends to be bigger so you might want to normalize out that effect„ÄÇüòäÔºåOkay„ÄÇ

 but remember that to compute attentionÔºå these are you can think of as the way of getting the unnormalized logics as we looked at in the previous video and we apply the softmax to get the actual probability distribution on top of the keys so you can wrap any of these functions in the softmax to get the full attention equation„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_38.png)

OkayÔºå so before we switch over to selfattenÔºå I also wanted to point out so there are many„ÄÇ

 many different use cases of attentionÔºå there's also„ÄÇ

 I mean it's useful in multimodal applications with images and text you can use attention to given like some text input focus on a particular region of an image that contains some relevant information„ÄÇ

 but I wanted to highlight that attention can also be used to copy tokens from the context in our language modeling setting„ÄÇ

üòäÔºåSo this is also known as a copy mechanism and to explain this note that the only way that we've seen so far in this class to predict a word is to compute the softmax over the entire vocabulary right so this is what we have down here at the bottom the softmax RNN box we see that you know AV has some predicted probability Bernanke has some predicted probability„ÄÇ

 zebra has some probability and so on so here I probably have a distribution over you know tens of thousands of word types in my vocabulary„ÄÇ

üòäÔºåOne observation in language modeling on a document is that words that I've already seen in the same document are probably likely„ÄÇ

 more likely than not to appear again in the document„ÄÇ

 so in this example I have document that contains feded chairÔºå Janet yellenÔºå blahÔºå blahÔºå blah„ÄÇ

 raised rates period and the current word that we've observed as miss and the model is asked to predict the next word„ÄÇ

üòäÔºåAnd so if you see something like this as a human„ÄÇ

 you might be inclined to fill in the next word with an entity„ÄÇ

 a name that you've already seen in the previous document right„ÄÇ

 and so maybe you would want to put in Yelleen if Jane and Yelleen is the one being referenced to in this context„ÄÇ

So one issue is that if you're using this probability distribution over your vocabulary and Yelleen is an entity„ÄÇ

 it's a person's nameÔºå it's probably very low frequency in your training data might only appear in this document„ÄÇ

So if that's the caseÔºå then it's going to be hard to actually predict this word given that you've seen it very rarely in your training data or even not at all so you might want a way to up kind of upwe the probability of yelling in this document given you've already seen it before and that's where the attention distribution comes in so at this point at the current time step associated with the word miss I can do attention over all of the previous hidden states right and maybe my attention score for the word yelling the hidden state associated with that is quite high so that's what's shown in this pointer box up here so maybe the probability of yelling in my attention computation is high so this is a good way of possibly up„ÄÇ

üòäÔºåWaiting the probability of predicting this word as the continuation of this prefix„ÄÇSo in effect„ÄÇ

 I have two distributions rightÔºå I have one produced by my softmax and I have one produced by my attention over the previous time steps and a copy mechanism essentially averages these two distributions together„ÄÇ

 so I take the probability of yellen for my attention score„ÄÇ

 I average it with the probability of yelln predicted by my softmax layer and that forms my final prediction probability for the word yelln„ÄÇ

üòäÔºåSo there are clever ways of like learning in what instances should I rely more on the attention weights and what instances should I rely more on the predicted SoOmax output„ÄÇ

 but we're not going to get into thoseÔºå at least in this lecture„ÄÇ

But I just wanted to know that you attention has more uses than just as a way to up the contribution of certain hidden states to the current prediction„ÄÇ

 it can also be used to just directly influence the prediction of certain words„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_40.png)

OkayÔºå so now let's move on to the concept of self attention„ÄÇ

 so we already talked briefly about how if attention is so powerful„ÄÇ

 do we even need the recurrent step that goes from left to right as that has all sorts of issues with speed training and so on„ÄÇ

So the concept of self attention is that if I have a sentence or some piece of text„ÄÇ

 I can get rid of all of the occurrence and instead just make the representations of each of the words in the sentence„ÄÇ

 exclusively a function of attention mechanisms„ÄÇSo here you see that if I have the sentence this as an example„ÄÇ

 I'm going to make each word a query as well as a key and so when I look at the word this used as a query„ÄÇ

 I'm going to use it to score the key vectors of all of the other words in the sentence and including this and maybe I find that I'm going to upweight the contribution of the key vector of the word this I'm going to maybe do the same thing for all of the other words in the sentence„ÄÇ

 so use their query vector to score all of the key vectors in the sentence and then I'm just going to take a weighted average of the key vectors or as we'll see later another projection of the original embeddings„ÄÇ

 the value vectorsÔºå to form our final representation of every word in the sentence„ÄÇüòäÔºåSo importantly„ÄÇ

 in this frameworkÔºå wordÔºå every representation is a function of all of the other words in the input„ÄÇ

 so each representation is fully contextualizedÔºå no left to right aspect of self attention„ÄÇüòä„ÄÇ

So to be more clearÔºå let's look at the particular formulation of self-attention that's used in the transformer and to do this we will we'll look through some slides that were made by Emma Sbel„ÄÇ

 Emma is currently a professor at Carnegie MellonÔºå she was previously a PhD student of Andrew McCallum here at UMass„ÄÇ

 she made these awesome slides to explain self-attention and Transformers and yeah„ÄÇ

 there's nothing better I've seen to explain thisÔºå so yeahÔºå let's say we're given this sentence„ÄÇ

 Nobel committee awards Strrickland who advanced optics„ÄÇüòä„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_42.png)

So let's assume this was our prefixÔºå rightÔºüFrom what we've learned so far with a recurrent neural language model„ÄÇ

 I would get a representation of this entire context by first looking at the word embedding for Nobel„ÄÇ

 and then applying my recurrent update equationÔºå then looking at the embedding for a committee„ÄÇ

 integrating that into my current hidden representation using the recurrent update and then proceeding left to right„ÄÇ

 and finally I get a vector associated with every word in this sentence„ÄÇ

So selfat works a little differentlyÔºå so we still start with the word embeddings right but the next step is this projection into three different vector spaces„ÄÇ

 so because in self attention the queries and the keys come from the same text so here we have a separate query projection„ÄÇ

 we have a separate key projection and a separate value projection So all of these projections are just linear layers on top of the original word embeddings„ÄÇ

üòäÔºåSo we have essentially three different weight matrices that are projecting the word embedding into three different spaces„ÄÇ

OkayÔºå so let's say we have all of these vectors now what do we actually do to compute the attention scoresÔºü

So let's start with this word NobelÔºå we're going to take its query vector that we've computed„ÄÇ

 and we're going to take the dot product between this query vector and each of the key vectors in this sentence„ÄÇ

So and actually in the transformerÔºå we'll be using the scaled dot product variant that I mentioned before„ÄÇ

OkayÔºå so when we do thisÔºå we get an attention distribution right that after we apply that Somax„ÄÇ

 and we can similarly do this with every single other word in this sentence just by using the query vector for that word and take the dot product to all of the key vectors in the sentence„ÄÇ

üòäÔºåSo I get a separate attention distribution for every single word in this sentence„ÄÇ

 Now what do I doÔºüIt's exactly what we saw before in the recurrent networkÔºå right„ÄÇ

 I'm going to take use this distribution to take a weighted average of but the key difference here is now I'm not taking a weighted average of my hidden states„ÄÇ

 I'm instead taking a weighted average of these value vectors„ÄÇ

 So why do we have these value vectors Why can't we just average the key vectors„ÄÇ

 you probably actually could I think this was just a way to get some more parameters into the model because otherwise„ÄÇ

 like you know in the recurrent modelÔºå we have all these weight matrices that are defining our update equation here„ÄÇ

 if we're just taking you know the dot product of linear projection with the output of another projected vector you know„ÄÇ

 maybe the original designers here thought that there weren't that many parameters„ÄÇ

 there have since been abllationations that show that many of these components aren't necessary„ÄÇ

 but intuitively you can just think of the value vectors as„ÄÇüòä„ÄÇ

The one that are getting average togetherÔºå the query and the key are used to compute the attention„ÄÇ

 the value vectors are used to compute the attention weighted representation„ÄÇüòäÔºåOkay„ÄÇ

 so I am now just going to do this for every single word in my sentenceÔºå right„ÄÇ

 I'm going to take the weighted average of the value vectors using this distribution as the weights„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_44.png)

So after thisÔºå I have a single representation of each word in the sentenceÔºå and importantly„ÄÇ

 this representation is completely contextualized right„ÄÇ

 So the representation here of Nobel actually has information about all of the words in the sentence right in a recurrent neural network„ÄÇ

 this isn't the case right because when I start with NobelÔºå I then have to„ÄÇüòä„ÄÇ

MyMy hidden state at the first time step only knows that I've seen the word NobelÔºå right„ÄÇ

 It hasn't seen any of these other words before because it proceeds left to rightÔºå right„ÄÇ

 So the recurrent the hidden state at this time step does not have all of the information about the rest of the sentence„ÄÇ

 unlike the attention weighted representation that we get here„ÄÇOkay„ÄÇ

 so we've done this and in the transformer model we're actually going to expand this to multiple heads so what that means is that we're going to have many different projection matrices to get into so maybe we have four different query vectors„ÄÇ

 key vectors and value vectors and we're going to compute an attention distribution for each of these sets of query key value pairs and the intuition here is that each head so each head is just basically a different projection of the queries and keys is going to allow us to look for different sorts of linguistic information that might be important for creating a representation of the current token so maybe you know this first head could look for other nouns in the sentence the second head could look for words that are„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_46.png)

a close by to this word the third head could look for I don't know some other part of speech or look for just named entities that occur so on so this is kind of the intuition in practice it's very hard to tell what each head is doing and if they have like interpretable behaviors but the intuition is clear right its it's there to allow the model to learn to specialize heads to particular information that might be useful„ÄÇ

So in the endÔºå I have now not just one attention weighted representation of each token„ÄÇ

 but H where H is the number of heads in the model„ÄÇ

So I'm now just going to pass all of these through a feed forward layer„ÄÇ

 you can use whatever composition function you want„ÄÇ

 I believe in the transformer they simply just concatenate all of the head representations and pass them through a feed forward layer to project them back down into the same dimensionality as the input word embeddings„ÄÇ

So we saw an example of this in the fixed window feed forward model„ÄÇ

 this concept of concatenation followed by projection„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_48.png)

And finallyÔºå we're going to apply another feed forward layer and we get the final output of the self attention layer so you can see that„ÄÇ

üòäÔºåThese representations include like a variety of different combinations of all of the words in the sentence„ÄÇ

 and if we're doing language modeling hereÔºå note that if we want to predict the next word„ÄÇ

 we can put a softm layer over the this final representation of what we've seen up till this point and similarly to a recurrent neural language model„ÄÇ

 the input to our softmax layer is just this final token level representation„ÄÇüòäÔºåOkay„ÄÇ

 so you can kind of make this more abstract and replace all of this with just like composition function right so we've seen now fixed window language models recurrent neural networks and now this kind of self-at way of getting representations for for language right sequences of word embeddings so this is an alternative to the recurrent neural network because it's just purely attention„ÄÇ

 there's no left to right connections here and we'll talk more next lecture about how actually you can make this work right one thing you might notice now is that there's no concept of position right the model because it just sees these query key value vectors„ÄÇ

 it doesn't have any concept that you know committee came after Nobel or that optics is the last word in this prefix„ÄÇ

üòäÔºåor so onÔºå so there are ways to get around this through things called position embeddings which we'll talk more about next time„ÄÇ

 but in generalÔºå the transformer model essentially is just a bunch of stacked layers of the self-atten computation followed by feed forward layers so you can stack these up to you know insanely large depths and train these on you know billions of words and you get pretty powerful language models„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_50.png)

OkayÔºå so hopefully this lecture has demonstrated the value of attention„ÄÇ

 we'll go over some of the parts associated with self-atten again in the next lecture when we switch over to focusing on the transformer architecture specifically but yeah„ÄÇ

 this is very important concept in modern NLPÔºå we'll also when we talk about the transformer introduce the concept of the encoder decoder paradigm which so far we've just looked at language neural language models which comprise the decoder portion of the this paradigm but for things like machine translation where you have a distinct input and output„ÄÇ

 it's useful to think about breaking these into two different distinct components and finally the other„ÄÇ

üòä„ÄÇ

![](img/08494681f5d7ba792601ad559962f157_52.png)

For next week after we covered the Transformer is getting to transfer learning„ÄÇ

 so now that we've learned basically all of the concepts that are underlying these state of the art methods such as Elmo and BerRT„ÄÇ

 we're actually going to go over them and that's going to form the base for the rest of the semester„ÄÇ



![](img/08494681f5d7ba792601ad559962f157_54.png)

![](img/08494681f5d7ba792601ad559962f157_55.png)

Okay„ÄÇ