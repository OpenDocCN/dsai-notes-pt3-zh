# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é©¬è¨è¯¸å¡CS685 ï½œ è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶(2020Â·å®Œæ•´ç‰ˆ) - P18ï¼šL15- æ¨¡å‹è’¸é¦å’ŒæŠ½å– - ShowMeAI - BV1BL411t7RV

Okay looks like I am live so great let's get started today we're going to be talking about a couple methods of distilling models and in essence we're interested in first compressing our models so can we get much smaller models that perform similarly or slightly worse than much larger pretrain models like those that we've seen so far and so before we start it might be good to think about why we might want to do this and the main reason is that we care about the latency of our models at inference time so if I have you know some Bt based QA task and I want to deploy that as a service to customers if I'm useã€‚

ğŸ˜Šï¼ŒSay Bt largeï¼Œ this is a 24 layer transformer right so it might you know take a significant amount of time to even return the answer to a single query and now if I have thousands or tens of thousands of users potentially querying it simultaneously I'm going to need a lot of compute to be able to efficiently serve this model without ruining the user experience So one way in which I can speed up these models at test time is to make them smaller we know that as we reduce say the number of layers in the transformer we're getting rid of a lot of computations that could be causing a slowdown at test time also it makes us more memory efficient we can potentially use a larger batch size if we're batch serving our queries atã€‚

Test timeã€‚ So there's a number of benefits that are speed and memory related associated with making your models smallerã€‚

 Butï¼Œ of courseï¼Œ our models are large because we get better performance when we make them largeã€‚

 rightï¼Œ We know thatï¼Œ for exampleï¼Œ Bt base compared to Bt largeã€‚

 There's a pretty large performance gapã€‚ Ohï¼Œ it looks like myã€‚ğŸ˜Šï¼Œå—¯ã€‚Things are no longer streamingã€‚å—¯ã€‚

Am I connected to themã€‚Ohï¼Œ okayã€‚Okayï¼Œ I do seem to be connected to the internetã€‚

Could you type in the chat if you can hear meã€‚ All rightï¼Œ I think I was not streaming forã€‚

A large amount of time thereã€‚Okayï¼Œ so it says it should be backã€‚Right nowã€‚All rightã€‚

 I'm just going to proceedï¼Œ I think it's now workingï¼Œ hopefully that doesn't happen againã€‚All rightã€‚

 soã€‚Rightï¼Œ let's so basically we want to make our models smallerã€‚

 but still have them retain the same performance of larger modelsã€‚All rightã€‚

 so before we jump into itï¼Œ we did get quite a few people requesting topics on both piazza and also the anonymous form one of those topics seemed quite important for just to make sure that you properly understand the transformer it might be good to also see it in code or at least a simplified version of it encode code next to the equations and architecture diagrams that we've already covered so we're going to do that on Wednesdays and we'll see how far I can get through a collab notebook and also your homework oneã€‚

 the due date is coming up in I don't know like nineish days for that we've made a couple of small modifications there was a bug in one of the the final problem which we've fixed so you should be able toã€‚

Ohï¼Œ looks like it's buffering againã€‚Okayï¼Œ yeahï¼Œ I don't know what's going on with my internet connection todayã€‚

Anywayï¼Œ we fixed a couple of bugs in the last problem of homework oneï¼Œ and yeahã€‚

 I think it should be going pretty smoothlyã€‚All right so our method of making our model smaller is called knowledge distillation so this was proposed many years ago originallyã€‚

 but has recently become very popular as our models are quickly growing you know out of hand in terms of size for serving them efficiently so in knowledge distillation we assume that at training time we've already trained or we have access to some huge model which we'll call the teacher and we're going to start with this teacher model and we're going to use it to train a much smaller model called the student model the student is going to be trained to mimic the predictions of the teacher model so it's kind of different than just taking a tiny model and train it from scratch onã€‚

ğŸ˜Šï¼ŒSay that you could pretrain it if you're trying to mimic BRTã€‚

 you could take you know like a three layer transformer and pretrain it using the mask language modeling objective on three billion words or you could take the standard Bt 12 layer transformer and somehow use that to guide the training of your three layer transformer and this second approach works a lot better than training your tiny model just from scratch and so we'll go over how this process works it was originally proposed and shown to work on vision tasks but is now also obviously important for NLPã€‚

And againï¼Œ I mean I don't think we need to go over this at this pointã€‚

 we've seen a bunch of these models but this is just a figure from a recent paper called Distill Btã€‚

 which is one of many efforts to you know make Bt smaller but preserve its performance this plot is basically showing the number of parameters of a couple different models that have been proposed in the last two yearsã€‚

 so we see Elmo is over here also the earliest model on this chart at around 94 million parametersã€‚

 we got to GT1 Bt large then we had GPT2 which was a billion parametersï¼Œ 1ã€‚

5 billion we had a bunch of modelsï¼Œ Robertaï¼Œ etc ceã€‚

 GPT3 is like off the chart it has way more parameters than anything else we've seen but the method proposed in this paper distillã€‚

BRTIt has only 66 billion parametersï¼Œ so it's definitely smaller than something like Btã€‚Okayã€‚

 so let's go through this process of knowledge distillationã€‚ It's actually veryã€‚

 very simple and we've already learned all of the components of this process in previous classesã€‚

 so it should be very straightforward to understand let's assume we have our teacher modelã€‚

 so we'll call it Bt sorry I guess this is the Bt large pretrain model has 24 layers right so we like the performance of this modelã€‚

 but we don't like that it's so big and it's so slow at test timeã€‚ğŸ˜Šã€‚

So we know that Bt works by this mask language model objective right so I give it some input with a mask token Bob went to the mask to get a buzz cut and the model is going to produce a probability distribution over the words sorry over the words that could fill in this mask token right so maybe barbershop and barber and salon and styllist or all with some probability and other words that are not important or not relevant to this context get a low probability hereã€‚

ğŸ˜Šï¼ŒSo for every mask token we can get a predicted probability distribution like this over our entire vocabulary using our teacher model right so we just assumed that we have this teacher modelã€‚

ğŸ˜Šï¼ŒAnd so we're going to call the predicted distribution of our teacher model soft targetsã€‚

 So note that if I'm training my teacher modelï¼Œ I use the cross entropy lossã€‚

 computed against the ground truth word that fills in this mask tokenã€‚

 right So maybe in our training dataï¼Œ we observed Bob went to the barber shop to get a buzz cutã€‚

 So now my ground truth probability distribution is this one hot vectorï¼Œ rightã€‚

 It's putting 100% of the probability mass on barbershop and 0% of the probability mass everywhere elseã€‚

 rightã€‚ğŸ˜Šï¼ŒIn contrastï¼Œ we can view the predicted distribution produced by our teacher model as another kind of ground truth distribution for training a student modelã€‚

 So now if I have a student modelã€‚Instead of predicting barbershop with 100% probabilityã€‚

 I'm instead going to predict this entire distribution that comes out of the teacher modelã€‚

 so I want my model to produce 54% probability of barbershop 20% probability of barber 6% probability of salon and so on right so this is very different than a oneho target where all of those components of the loss drop away except for the negative log probability of the correct word so in this example barbershop in this case every single word in your vocabularies contributing some amount to your loss right so I want my model to predict that styllist has a 4% probability and we'll go through a little later why this method of training with soft target is potentially more informative than just training ourã€‚

ğŸ˜Šï¼ŒEven model with hard targetsã€‚So we've got these soft targets which are just the predicted probability distributions over these masked words and now we're going train a student model so in distill Bt this is a six layer transformer so smaller of course than our teacher model and we're going to give it the same training data so Bob went to the mask to get a buzz cut but here the student model instead of predicting 100% probability on barbershopã€‚

 it's going to predict this entire distribution these soft targets and the loss function is exactly the same as what we were doing when we had the hard target right just the 100% probability on barbershop it's the same equation for the crossenttropy loss that we've seen but note that now the T subi comes from these soft targets and T subi is no longer zero for all of the other non-barbershop elements in the vocabularyã€‚

ğŸ˜Šï¼ŒSo I getï¼Œ you knowï¼Œ if I'm at the word for Barberï¼Œ this would be 0ã€‚

2 log and then the prediction of the student model for the predicted probability for Barberã€‚

So this is the general process of knowledge distillationã€‚

 we're using the predicted distribution of the teacher to train our student model instead of just relying on the hard targets Now in distillberrt and in many other cases of knowledge distillationã€‚

 you train with a combination of soft targets as well as hard targets so this model is also going to get a separate component of its lossã€‚

 also using the crossenttropy loss with just the 100% of the probability focused on the correct word barbershopã€‚

 but you're going to do a weighted average of the hard target loss with this loss over the soft targets which are produced by the teacher modelã€‚

ğŸ˜Šï¼ŒOkay so that is the general principle of knowledge distillation let's we have a couple of questions here so what are the measures for stating or classifying a model as small or large That's a good question going back to this plot when Elmo is released it was considered a pretty large model in its own right so it really depends on kind of the hardware limitations of the current time period something like Bt large was considered huge when it was released but is now just like a normal model that everyone is able to fine tune on you know just coabab GPUsã€‚

 for exampleï¼Œ something like GT3 obviously is completely infeasible to fine tune on one or a few GPUs so that is considered large but yeah it's just a function of kind of hardwareã€‚

ğŸ˜Šï¼ŒAvances in GPUs and what can and can't fit or be fine tuned with a reasonable number of consumer grade GPUsã€‚

Okayï¼Œ so other questions are soft targets produced after training is complete for the teacher model Yeah so you assume that you've pretrained this teacher model and it's of course pretrained using the standard mask language modeling objective and it's you treat it as fixed so it's just it sole purpose in the distillation process is to produce probability distributions of these mask tokens and those are then used as the ground truth for the student modelã€‚

ğŸ˜Šï¼ŒCan you please explain the difference between this is your standard pretrained BRT it's trained only using hard targets and what I mean by that is if this sentence in the training data was Bob went to the barbershop to get a buzz cut and barbershop got masked out to create this training example then when we predict the masked word hereã€‚

 our cross entropy loss our ground truth distribution is going to consider barbershop with 100% probability and a0% probability to all of the other words in the vocabulary right that's the one hot distribution that we're doing a pretraining timeã€‚

So we call this a hard target in that only one word is correct and all of the other words have zero probabilityã€‚

 The difference between a hard and soft target is when we use a soft target to trainã€‚

 we're actually using distributionï¼Œ the predicted distribution over the entire vocabulary as trainingã€‚

 not just this one hot distribution where all of the probability mass concentrated on a single wordã€‚

 So here like if we use the teachers predicted distribution over this mask token as soft targetsã€‚

 then we have a predicted probability for every single word in the vocabularyã€‚

 So we're going to force the student model to predict not just that barbershop should have a high probabilityã€‚

 but also that these other words like barber and salon and stylus should also have probabilities that are higher than other words that don't make sense at all within this contextã€‚

ğŸ˜Šï¼ŒSo that's the general intuition here you can think of it as soft targets have the probability mass distributed across all of the items in the vocabulary and hard targets just have them concentrated on the single correct wordã€‚

ğŸ˜Šï¼ŒHopefully that cleared it upã€‚If we have the hardware to run the large modelã€‚

 why not just use that So there are many considerations right if you have the hardware to run the large model but maybe you don't have the hardware to scale it up to you know like1000 or 10ã€‚

000 queries at coming in at the same time rightï¼Œ then maybe you benefit from having a smaller model that you can scale to the actual practical use case that you haveã€‚

 I mean this is a very important problem in industryã€‚

 I personallym working with some companies who are very interested in using distillation to make their models more efficient so that if you have them in something like a customer facing chatbot or some kind of APIã€‚

 you're not going to need to use as much compute to serve these modelsã€‚ğŸ˜Šï¼ŒYour target audienceã€‚

And another response to that question is you might have the hardware to you know run the model with some reasonable latency with a batch size of oneã€‚

 but maybe it breaks down once you have a harder sorry a larger batch size right and there are all other types of kind of peri that you need a vary dependent depending on your practical use case and in generalã€‚

 like if you have a smaller model you're going to have end of the dayã€‚

 making things faster is always going to be good forã€‚ğŸ˜Šï¼ŒYeahã€‚

 like decreasing the costs that are associated with serving this modelã€‚

So this distillation is sort of like function approximationï¼Œ yeahã€‚

 so you're trying to approximate the predictions of the teacher modelï¼Œ rightï¼Ÿ

So in a way we're saying that capturing a soft target distribution is in some sense simpler No not really So what we are saying thoughã€‚

 is that the soft targets are more informative than the single hard target right so if you look at the top four words in this simple example here you see that they're all related to barber haircut type of things and if I just have a hard target I have 100% of my mass focused on barbershopã€‚

 I miss out on all these like synonymous words that are also reasonable things to put into this context so the soft target is not simpler to predictã€‚

 but it does contain more information than just the simple oneho target which is the intuitive reason why doing this training with soft targets is better than just training your small model from scratchã€‚

ğŸ˜Šï¼ŒOn the hard targetã€‚Okayï¼Œ so for training the studentã€‚

 we run a forward pass through the teacher then use the stop targets to train the student networkã€‚

 This happens for every data point in our training test in our trainingã€‚I guess I mean set Yesã€‚

 so you can make this more efficient right because you if you're training your student modelã€‚

 you may not want to run a forward pass through the teacher model at every batch So instead what you could do is do a single preprocessing step essentially where you run all of your training data through the teacher model and cash the predicted distributions and then use those to train your student modelã€‚

 So there's no actual forward pass of the teacher model during the training processã€‚ but yeahã€‚

 I meanï¼Œ if it's fineï¼Œ if you don't care about training timeã€‚

 you a simpler solution might be to just not do any caching and do a forward pass through the teacher model to get the soft targetsã€‚

 But in principle you're rightï¼Œ that's what's happeningã€‚ğŸ˜Šã€‚

Do we reuse the fine tuning data that we use for the teacher for training the studentï¼Ÿ Yeahã€‚

 so that's a good questionã€‚ There are many different points at which this distillation can be done right so what I've shown here is distilling this mask language modeling objectives so the pretraining step there's no downstream task used here So the expectation would be you take the distilled Bt modelã€‚

 distill Bt and then you're gonna to fine tune it on the downstream training data set as you normally would but you could also at a distillation step at the fine tuning phase where you make the model predict the same probability distribution over the downstream task that you would that the teacher model does like a finet teacher model there are many variants of this you can check out the papers that were assigned for reading for more details but they give a good empirical comparison of all of these different points at which you could add distillationã€‚

ğŸ˜Šï¼ŒAll rightï¼Œ even though soft targets are more informativeã€‚

 are we expecting our small model with lower capability or capacity to be able to be trained equivalently to a larger model Yeahã€‚

 so this is a good pointã€‚ Noï¼Œ we don't expect it to be perfectly able to predict these probability distributionsã€‚

 but that's not our goal here so in essenceï¼Œ if you're going into distillation with the goal of getting a smaller model you're always going to be tolerant of some level of performance loss you're just hoping to keep that the difference in performance from the student and teacher model as small as possible and having it predict these more informative soft targets is a better way of like preserving as much information of the teacher model as we canã€‚

ğŸ˜Šï¼ŒOkayã€‚So just to kind of reinforce some of the stuff that I've already mentioned in answers to your questionsã€‚

 these soft targets are more informative than just the correct word or the hard target that we train our teacher model onã€‚

And as an intuitive reason rightï¼Œ let's consider two words from this example that weren't shown right that are probably low probability given this context right so one word could be church and the other word could be and So if we look at filling in this mass token with those two words we have Bob went to the church to get a buzz cut just kind of low probable but you know at least it makes sense and it's like grammatical and a church is a location that Bob could feasibly go to there could be some world in which some church was you knowã€‚

 giving out buzz cuts for whatever reasonï¼Œ but if you compare that to Bob went to the and to get a buzz cutã€‚

 that's not grammatical at all right So even though both of these words will have a very low probability from the teacher model we would expect church to have a higher probability than and right So even though there's very little probabilityã€‚

ğŸ˜Šï¼ŒAss to these the relative order of these words and the relative difference and probability is very meaningful right it tells us that and is muchã€‚

 much less likely to fill in this mask than church so we get all of these kinds of comparisons even with low probability words in the soft targets that we aren't getting from the hard targetsã€‚

ğŸ˜Šï¼ŒSo for exampleï¼Œ this distinction church and endï¼Œ its telling us it's telling the student model that this mask is likely to be a nounã€‚

 it's also likely to be a location it's not likely to be a function wordã€‚

 so all of this kind of information we get just from a single predicted distribution which we don't get from the hard targetã€‚

ğŸ˜Šï¼ŒWhat kind of trade off between the performance and the size of the smaller model and comparisons into to the bigger model do we expect Are there any tasks where we would prefer using the smaller model That's a good segue to the next slideã€‚

 So these are the results from distill Btã€‚ they say that in generalã€‚

 it retains 97% of Bt performance on downstream tasksã€‚ğŸ˜Šï¼ŒSo you can see here they have a tableã€‚

 these are all of the tasks in the glue benchmarkï¼Œ which is a kind of standard classification benchmark for NLPã€‚

 we've seen some of these tasks before like MNLI is the entailment task SST is a sentiment analysis task so we see here Bt base which is a 12 layer transformer its you know it's of course better than this small distill Bt model across the board but the differences aren't that large and in some cases distill Bt is actually better although these data sets are veryã€‚

 very small so I don't expect these differences to be significant at allã€‚ğŸ˜Šã€‚

You can see that some tasks there are huge gaps and on other tasks like sentiment analysisã€‚

 the distillberr does just as well butã€‚You knowï¼Œ you might be wondering at this pointã€‚

How well do I do if I just take the distilled birdt architectureï¼Œ so againã€‚

 that's the six layer transformerï¼Œ it's one and half the size of birdt base and what happens if I just train a six layer transformer from scratch like just a birdRT model and then evaluate a fine unit for all these downstream tasksã€‚

So oh sorry before I do that I wanted to mention that there are other works that try and distill other components of the modelã€‚

 not just the final predicted probaï¼Œ so these I think are quite interesting approaches this paper that I've referenced here introduces this model called tiny Bt where in addition to doing the distillation over the vocabulary they do distillation on the hidden states and the attention matrices at each layer of the transformer so very interesting it's basically saying we want the hidden states of our small model to be kind of predictive of or be similar to the hidden states that you observe from the teacher model we want the attention heads to be looking at the same places in the teacher model that they do in the student model so you can get pretty inepã€‚

ğŸ˜Šï¼ŒWith how much you're going to apply distillation and what components of the model you're going to apply it toã€‚

So this tiny bird does a lot more distillation than the distilled bird model does and these loss functionsã€‚

 of course the hidden state one can't really be a classification loss rightã€‚

 it's a regression loss so you can use like the square loss which we went over when we were introducing back prop as to kind of make the teacher hidden state and the student hidden state as close to each other as possibleã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so now I'm want to get back to the thing I was talking about beforeã€‚

 which was what happens if you just train the student model from scratch instead of doing distillation So there's this paper which is called well read students learn better which compared a bunch of different variants of distillation to this training from scratch methods So if you see on the slide this bottom line hereã€‚

 the dash dotted line with the x'sï¼Œ this is a model where you just train birdt from scratch with a much smaller architecture so they tried a bunch of different architecturesã€‚

 tiny mini small medium baseï¼Œ you can see the number of layers in the hidden states here sorry the hidden dimensionality here So and these three other linesã€‚

 the orange blue and green line represent different types of distillation and the red lineã€‚ğŸ˜Šã€‚

The red dotted line at the top is the teacher modelï¼Œ so this is kind of the upper boundã€‚

So you can see that all of these different variants of distillation are significantly in most cases outperforming the training from scratch method and in some cases this difference is really large right so if you look at sentiment analysis our teacher model is like about 94% accuracy our train from scratch model isn't really improving even if we make it bigger and it's also stuck around like 82% accuracy so that's like pretty huge difference right meanwhileã€‚

 we see these distillation modelsã€‚ğŸ˜Šï¼ŒThey actually improve as you make the model bigger and biggerã€‚

 the student modelã€‚And they're coming closer and closer to the teacher's performance and in some cases even outperforming the teacherã€‚

Soï¼Œ all rightï¼Œ we have a question here when hidden states are compared considering the number of hidden states are differentã€‚

 Ohï¼Œ I guess you mean the dimensions modelã€‚ So in this caseã€‚

 you're assuming that the hidden size is the same for both teacher and student and you're just cutting down the number of layersã€‚

 essentiallyã€‚ so butï¼Œ but like the attention head sizeã€‚ Everything is the sameã€‚

 you're just compressing the model by reducing the number of layersã€‚ So you can sayï¼Œ you knowã€‚

 I'm going to remove every other layerã€‚ And then I can haveã€‚ğŸ˜Šã€‚

Each layer predict like the one of the layers from the teacher model I'm not sure exactly what procedure that they followed to align the teacher and student models in this paperã€‚

 but you should check it out if you're interestedï¼Œ but yeah if you're going to do this kind of hidden state similarity distillationã€‚

 you're going to need to have some kind of alignment hereã€‚Al rightã€‚

 so one thing that's interesting from this paper is how likeã€‚

 what exactly is the method used to get this green curveï¼Œ Because this is theã€‚Ohï¼Œ Iã€‚Okayã€‚

 I guess I missed when it was buffering then fortunateï¼Œ hopefully it's better nowï¼Œ sorry about thisã€‚

 I can't really control my internet being badã€‚Rightã€‚

 so what is this green curve here so in this this paper Well read students learn betterã€‚

 they propose this method called pretrained distillationã€‚

 which is supposed to be better than the method that we showed with distill Bã€‚

 So essentially you start with a small student model and you train it from scratch using the hard targets with on your pretraining dataã€‚

 So let's say I have a Bt teacher model that was trained on Wikipedia using the mask language modeling objectiveã€‚

 This is gonna to be my teacher modelã€‚ Now I'm going take a much smaller Bt architectureã€‚

 say like a three layer transformerï¼Œ I'm going to pretrain it using hard targets on the same exact training dataã€‚

 So say Wikipediaã€‚ So that's this first gray box hereã€‚ I've now pretrained my small modelã€‚ğŸ˜Šã€‚

Now in the next stepï¼Œ I'm just going to do distillationã€‚

 so I'm going take the parameters of my small model and I'm just going basically fine tune it using the distillation objective using the soft targets from the teacher So I'm starting with a small model that's already been pre-trained and now I'm fine- tuning it to make it predicted distributions look more like the large teacher models so you can see the arrow here from the large teacher into this distillation step and finally now this is my final pretrain model and now I can just finet this on labeled data if I want to use it on a downstream task and this is the process that is reflected by this screen curve and outperforms you know kind of doing both of these distillation and pre-training steps at the same time or just doing distillation from scratch without doing this pre-training stepã€‚

ğŸ˜Šï¼ŒSo if you're interested in thisï¼Œ you should check out this paper if you're interested in using these techniquesã€‚

 but it's a very simple method to maximize the effectiveness of your distillation and as you can see from these graphsã€‚

 it has quite a significant improvement over the more naive methods of distillationã€‚

So as layer one compared to layer  oneï¼Œ I don't actually knowã€‚

 So I assume you're talking about this tiny bird setupã€‚

 I'm not actually sure how they aligned the layers of theã€‚Teacher and studentã€‚

 so you would have to go check out this paper for more detailsã€‚

 but I assume they have some method of doing this alignmentã€‚

What is the intuition behind pretraining Well of course we know that pretraining is you know basically the thing that makes NLP work these days and it is yeah I guess pretraining to convergence using hard targets results in the the small model having a a good initial set of parameters right as opposed to just randomly initializing them and then doing the distillation from scratch so yeah I think probably the gap here is because pretraining gives you a much better starting point for the distillationã€‚

ğŸ˜Šï¼ŒWhat about the case that the student is better than the teacherã€‚

 I honestly don't know what to say about thatã€‚ I don't know how you can really explain thatã€‚

 It might be some random effect in generalï¼Œ you should not expect a smaller model to be better than a larger oneã€‚

 although I will note that at least for this caseï¼Œ the size of the model is the same as the size of the teacher model but yeahã€‚

 even in this medium caseï¼Œ there's a slight increase in performanceã€‚ I honestly can't explain thatã€‚

 I'm not sure why that's happening what is the unlabeled transfer data and distillation rightã€‚

 I imagine this is the difference between the soft targets and the hard targetsã€‚

 So here when we have unlabeled language modeling dataã€‚

 this is just the hard targets and unlabeled transfer data is referring to the soft targets that you're getting from the teacher modelã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so that is the process of distillation and its many variants you can of course explore just doing things at the output layer using the predicted probability over the vocabulary you can also think about distilling different components of the model there's a lot more work to be done hereã€‚

ğŸ˜Šï¼ŒI wanted to briefly switch over to another very interesting recent development in the machine learning communityã€‚

 which is the lottery ticket hypothesisï¼Œ which is kind of another approach to obtain a smaller model that doesn't rely on distillationã€‚

So this paper was published last year is called the lottery ticket Hypothã€‚

 and it states that a randomly initialized dense neural network contains a sub network that is initialized such that when train in isolationã€‚

 it can match the test accuracy of the original network after training for at most the same number of iterations So what this is saying is that there is some if you take like a much some huge networkã€‚

 there is some subset of the parameters of this network that are responsible or can reach the same performance as that full huge network if you remove all of the other parameters and just train this new sub network from scratchã€‚

ğŸ˜Šï¼ŒSo the process by which they devise to find the subnet is first you take your huge network and you randomly initialize it as you normally would now you train this network for some number of iterations maybe until convergence so you can imagine doing this for BRT right so I randomly initialize my BRT architectureã€‚

 I train it on the mask language modeling objective for some number of epochs and I get some final set of parameters we're going to call it theta subjã€‚

So now once I have my big BRT modelï¼Œ I'm just going to prune away some percentage of its parametersã€‚

 which essentially creates a mask like a binary maskï¼Œ zero or one for each perimeter in the modelã€‚

 is it pruned or is it preservedï¼ŸAnd in this paperã€‚

 they use a very simple strategy of pruning weights after the model has been trainedã€‚ They justã€‚

 for every layer in the modelï¼Œ remove some percentage of the weights that had the lowest magnitudeã€‚

 So this is you knowï¼Œ a very simple heuristic that seems to workã€‚

 There are probably other ways in which you could do this pruningã€‚

 but they just chose this the simple magnitude based method to do itã€‚

So I'm ne prunne for every layerï¼Œ like 20% or 40 or 60% of the parameters that had the lowest magnitudes in that layerã€‚

Now the very interesting finding here is that now I have some number of parameters that have not been pruned in this wayã€‚

 right the parameters that had the highest magnitudesã€‚

 So now I'm going to reset those parameters to their original random initialization and I'm going to train this network from scratchã€‚

 without any contribution from the pruned parametersã€‚

 so those are going to be masked out of this new network and this is called the winning ticket because even though I've removed P percentage of the parameters of this modelã€‚

 I end up reaching very close to the the original models's performance So this is at least for me it was quite unintuitive and it just shows the importance of the random initialization right there's some subset of parameters that thereã€‚

ğŸ˜Šï¼ŒInitialization is somehow contributing to making the optimization easierã€‚

 maybe or making them play a more important role than other parametersã€‚

 and you can detect these parameters by just pruning from a pretrain modelã€‚ğŸ˜Šã€‚

So there's this paper that was assigned on the websiteã€‚

 Lotery tickets for BRT Network that explores this hypothesis using the BRT modelsã€‚

 so I just took their result table just to show the results of applying this pruning strategy to BRT base so these are all of the glue tasks againã€‚

 and here they're showing differentrs sparsity values on each of these data sets so 70% means that they prune 70% of the parameters of this model for something like squ they were only able to prune say 40% of the parameters versus forã€‚

ğŸ˜Šï¼ŒThis QQP data set they could proverune 90%ã€‚So you can see the performance of the BRT base model here is reported in this rowã€‚

 and the performance of their approachï¼Œ which is just taking this mask and multiplying it with the random initialization and then retraining that model from the original initializationã€‚

 they are able to almost exactly match the performance of the Bt base modelã€‚

In some cases with quite a high amount of sparsityã€‚

 so this is super interesting and just to evaluateï¼Œ they also try taking a random maskã€‚

 so instead of selecting a mask that corresponded to removing the weights that had the lowest magnitudes you could also just randomly mask out P percent the parameters and you can see that these randomly massed networks perform really poorly on the downstream taskã€‚

 so this indicates that this heuristic of selecting the weights that have the highest magnitudes is actually quite important and there is these results do show that there is a lottery ticket the metaphor at least holds here So yeah this is perhaps another route that you can go if you're interested in making your models fasterã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so some more commentsï¼Œ I can imagine that pruning can be done based on the task at hand for yeahã€‚

 so you could definitely use some gradient based method to figure out which parameters are maybe contributing the most to the loss function or a particular prediction they didn't do that I think they tried in this paperã€‚

 some other methods of pruningï¼Œ but yeah this method seems to be quite effectiveã€‚

It would be interesting to try other methodsï¼Œ thoughï¼Œ I agree by pruningã€‚

 do we mean just removing the parameter from the model altogether or just set them to zero and no training happens to themã€‚

 I meanï¼Œ I'm not actually sure what the implementation looks likeã€‚

 I imagine it's the ladder where they those parameters as you can see here you're just multiplying this binary mask to the parametersã€‚

 So I imagine that those parameters are just set to zeroã€‚

 but like if you are really interested in a more efficient implementation you would just remove them altogetherã€‚

 So yeahï¼Œ I think those two things are equivalentï¼Œ but or actually maybe they're not equivalentã€‚

 but you you probably get some sparsity sorry some speed ups during training just by making your your computation sparse but I'll have to look into yeah exactly what the implementation so those doã€‚

ğŸ˜Šï¼ŒThat's a good questionã€‚Okayï¼Œ ohï¼Œ wowï¼Œ it's been 45 minutesã€‚ Alrightã€‚

 so maybe I'll try and go a little faster over this next partã€‚ with lost connection againã€‚ğŸ˜Šã€‚

All rightï¼Œ hopefully it comes backã€‚Zã€‚å—¯ã€‚Okayï¼Œ so it seems like it's recoveredã€‚

Sorry again about the bufferingã€‚é‚£ã€‚Al rightï¼Œ is itã€‚It just hasn't recoveredã€‚Okayï¼Œ finallyï¼Œ backã€‚

 I thinkã€‚Sorry about thatã€‚Okayï¼Œ soã€‚Rightï¼Œ as I was sayingã€‚å‘ƒã€‚

So there is an interesting connection between the process of distillation and theã€‚

The a couple of security vulnerabilities in these modelsã€‚ And so let's assume that you're a companyã€‚

 let's say you're Googleï¼Œ for exampleï¼Œ and you're serving a model like Google translateï¼Œ sayã€‚

 so your customers who are using this modelã€‚ They don't have access to the predicted probability distributions that your model is producing rightã€‚

 they don't have access toã€‚ğŸ˜Šï¼ŒThese full probability distributionsã€‚

 Google Translate will just give you the actual words that correspond to your translationã€‚

So even if you don't have access to the full distributionã€‚

 you just have access to the say AGmax prediction of the teacher model it turns out that you can still using a process similar to distillationã€‚

 extract a model that performs very similarly to the teacher model so this is a case of an attack against one of these pretrain language modelã€‚

 it's called a model extraction attackï¼Œ and so instead of calling it a teacher model we'll call it a victim modelã€‚

 it's a model that an attacker wants to steal maybe I mean you could come up with all sorts of reasons why someone might want to steal Google translate right if you steal a Google translate modelã€‚

 then you don't have to pay Google for using it to you know translate your own data right so there are many advantagesã€‚

ğŸ˜Šï¼ŒThat you as a malicious attacker or as a competitive companyã€‚

 can apply these kinds of attacks to models that are publicly served behind APIsã€‚Okayï¼Œ so this isã€‚

 I just wanted to maybe wrap up quickly by describing some work that one of our TAs actually Colpeish did on this using process similar to distillationã€‚

 but I've also linked a paper on the website showing how the same process can be applied to stealing Google Translate successfullyã€‚

 but we'll just go over some simpler data sets hereã€‚Okay so againã€‚

 this is called a model extraction attack and I'm just going to outline the basic framework by which we perform these attacks so we have a victim model in our distillation terminologyã€‚

 this would be called the teacher model which is being served by some black box API so what that means is I as a customer am able to put in some text and it sends it over to this server the prediction happens on the server side and then it sends back its predicted AGmax prediction on on the input so if I had a sentiment analysis modelã€‚

 I would have something like this is a great movie which I feed into this API and it gives me positiveã€‚

 it may not even give me the probability associated with this prediction it might just tell me that the sentence is positive not that it's you like 78% positive or something like thatã€‚

ğŸ˜Šï¼ŒThis is the only information that Iï¼Œ as an attacker who's trying to steal this model have access toã€‚

Okayï¼Œ so right this model is released as an API so I can submit as many questions to essentially form a training data set for my own stolen model so that's generally how this process works you can view the stolen model as the kind of analog of the student in the distillation processã€‚

ğŸ˜Šï¼Œå—¯ã€‚So here for my queriesï¼Œ I in our workï¼Œ we experimented with like the most naive strategy possibleã€‚

 which is just randomly sample words and concatenate them together to form inputs that you can send over to these models so for this sentiment analysis model I generate this query 173 period Miles Vegas it's ungrammaticalã€‚

 it makes no sense but you know this black box model is going to spit out some prediction positive or negativeã€‚

 even if the input doesn't make senseï¼Œ So maybe it predicts positive on the sentence and for this one circle forward had support why rulers broken Jan family it predict negativeã€‚

ğŸ˜Šï¼ŒSo I can submit as many of these queries as I want using whatever process I want to generate these inputs that I send over to the model and I'm collecting for each queryã€‚

 the corresponding labelã€‚ So in the endï¼Œ I get the set of like gibberish sentences paired with what the model the victim model predicted for each of these gibberish sentencesã€‚

So now I'm basically just going to kind of replicate the distillation process but using hard targets instead of So targets so I'm going to take my pretrained burnRT modelã€‚

 and I'm going to fine tune it on the gibberish data set that I've collected by doing this process of querying the victim modelã€‚

So I'm fine tuning my bird to be able toï¼Œ given 173 miles Vegas predict positiveã€‚

 This seems like a meaningless task to a humanã€‚ but it turns out that even though you use these gibberish queries to fine tune your stolen modelã€‚

 it actually turns out to work pretty well on real English dataï¼Œ which is is kind of crazyã€‚

 So now I have my fine tune stolen modelï¼Œ which we call the extracted modelã€‚

 and it actually is able to make legitimate predictionsã€‚

 despite not even seeing a legitimate instance of the sentiment task at the fine tuning during the fine tuning processã€‚

 So this extracted model can predictï¼Œ given this is a great movie that thisã€‚ğŸ˜Šã€‚

ASent has positive sentimentã€‚So of course this is a huge problem right because as an attackerã€‚

 I didn't even have to think about anything to accomplish this processã€‚

 I submitted completely random inputsï¼Œ I didn't have to use any strategy to cleverly construct inputs designed to reveal the most information about this victim model I didn't care at all about that I just like submitted random gibberish got back on predictions and if this model is able to work wellã€‚

 then this is a veryï¼Œ very low cost attack right So we did these attacks on like estimated their costs using Google's they have an NLP API that serves a bunch of these models and it's actually quite cheap So if you wanted to use Google's NLP API it's about a buck for a thousand inputs or 1 thousand00 API calls So if you wanted toã€‚

ğŸ˜Šï¼ŒExtract a sentiment analysis model and say you wanted to use the same number of inputs as there are training examples in the original data setã€‚

 this only costs you $62ã€‚ so it's veryï¼Œ very feasible to do these attacks right and once you've extracted this model you can then serve it yourself and charge other people to use this right so that's kind of crazy from a competitive standpointã€‚

I see a good question hereã€‚ Do we need to make sure the victim model has the same architecture with the extracted modelã€‚

 Noï¼Œ it's not necessary actuallyï¼Œ So we did some experiments that I'm not going to talk about where these were mismatchedã€‚

 So like this modelï¼Œ victim could be something like X net and the extracted model could be Bt and it actually works pretty wellã€‚

 in factï¼Œ if your victim model is some like reallyï¼Œ really huge scale modelã€‚ğŸ˜Šã€‚

Even if your extracted model is smallerï¼Œ it seems to work even better than if both the architectures were matchedã€‚

 so yeah there's some very interesting results here which are kind of favorable from the attacker's point of view right they don't even have to know what the underlying black box model looks likeã€‚

ğŸ˜Šï¼ŒAnd so we had this estimated cost for translationã€‚

 but the paper that I linked on the website actually did this for translation and stole a Google Trans model that had very similar blue scores to the actual Google Trans so you know Google Translate is a veryã€‚

 very valuable model right and the fact that it can be easily stolen in this way is definitely concerningã€‚

ğŸ˜Šï¼ŒSo compared to the distillation process that we went over in the beginning of the class hereã€‚

 we're not assuming that we even have access to the original training data of the victim model so if we go all the way back to this exampleã€‚

 we were assuming that both the teacher and the student have the same training data right and we're just varying the labels that the student model is being trained onã€‚

But in this extraction caseï¼Œ we don't even make that assumption rightã€‚

 this model just observes this random gibbish text and it's still able to work despite not seeing a real example of a sentiment like positive negative sentenceã€‚

ğŸ˜Šï¼ŒAnd hereï¼Œ like we don't care about compressionï¼Œ we're simply trying to maximize the performance of our stolen modelã€‚

So in this paper we tried it out for sentiment analysisï¼Œ NLIã€‚

 which we've seen before and Sd style QAï¼Œ we used two really stupid input generatorsã€‚

 one was a random gibber sequences that we looked at beforeã€‚

 the other one was trying to at least use more grammatical sentencesã€‚

 we just sampled random sentences from Wikipedia so like even if they had no sentiment in themã€‚

 we would just randomly sample one and feed it into the modelã€‚

So for something like Squa there's actually two inputs right there's a paragraph as well as a question so we had like these two settings where in the random setting the question was a subset of the random words in the paragraph because we know that there's some kind of relation between question and paragraph there was another variant we tried where we added some sort of questionish words to the random sequence so we had this random question caim Kzim further as and gloworm we just added the word what and a question mark at the endã€‚

 but I mean these are also very easy heuristics for an attacker to add in if they know that they're trying to stealke a QA modelã€‚

ğŸ˜Šï¼ŒSo let's look at some results Our victim model was getting an F1 on squad of 90ã€‚

6 so this is like a BRT pretrain model the random model was not doing so well if we limited the number of queries to the same number of examples that are in the original squad training sets of like 100000 but it improves significantly if you multiply the number of random queries by 10 times so if you use say a million queries hereã€‚

 you are able to close this gap we didn't try any more than that but we assume that as you add more and more you can close the gap further the strategy of just sampling a random Wikipedia sentence and if you do it with a million random Wikipedia sentences you get very very close to the performance of the victim model without having seen a single legitimate instance of a questionã€‚

age pairã€‚You can see that on sentiment analysisï¼Œ the gap is similarly small and on MMLIã€‚

 it's a little harder to match the performance of the victim modelã€‚All rightã€‚

 doesn't seem like there's any way to prevent thisã€‚ Yeahã€‚

 we'll talk about that briefly a little laterï¼Œ but in generalï¼Œ that is trueã€‚ Yeahã€‚

 there is not a single method that you can use to prevent these kinds of attacksã€‚

A industry level pre trained birdss readily available because if they areã€‚

 aren't we really only stealing the feed forward layerï¼ŸWellï¼Œ I meanã€‚

 all of the models that are served by these APIs are they have to be state of the artã€‚

 the way that we know to get state of the art performance right now is through fine tuning pre-trained language modelsã€‚

 so it's easy to assume that all of the models that are being served by state of the art APIs are using this kind of pretraining in some way right we already know that Google search has now switched over to using BERT to do the rankingsã€‚

 So I think this is a pretty fair assumption for most like state of the art APIsã€‚ğŸ˜Šã€‚

Is it possible that Google Translate is just so good that we are essentially learning from a near human level parallel data instead of stealing the modelï¼Ÿ

Wellï¼Œ it depends on how you're constructing your queries to Google Trans right Of courseã€‚

 if you put in like random sequences of wordsï¼Œ you will be learning the one to one mappings between like different languages at a word level right so you'll at least be getting some alignment if you then just put in like you know Wikipedia sentences now you're learning some grammar as well and yeahã€‚

 you're just assuming that Google translate is is doing such a good job actually there's an interesting anecdote I heard about this apparently so Google oftentimes minds like parallel data from the web and they like periodically retrain their models on more and more data that they've collected and someone was telling me that they noticed I think this wasã€‚

ğŸ˜Šï¼ŒA couple of years agoï¼Œ they noticed that some of the text that they were getting was not actually like human written parallel textã€‚

 but text that other people had passed through Google Trans and then posted on the internet so they were like training their models on worse versions of Google translate outputsã€‚

 so they had to develop some method for like filtering outã€‚

Text that was generated from older versions of Google Translateã€‚

 which I thought was pretty interestingï¼Œ but it's kind of not the problem with people attacking or stealing the modelã€‚

 but rather their own previous iterations of the model were making the current model worseã€‚

 which is interestingã€‚Does this signify that the underlying model does not matter that muchï¼Œ I meanã€‚

 of courseï¼Œ you want it to be powerful enough that its predictions are at least somewhat meaningful on queries that have some signal that is kind of related to the task that you care aboutã€‚

 but yeahï¼Œ I meanï¼Œ the experiments that we've done show that it doesn't really matter if your underlying model is like B or X net or Roberta large or whateverã€‚

 as long as it's producing reasonable predictionsï¼Œ youre pretty you have a good chance of success from stealing itã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so let's just conclude briefly by talking about some possible defensesã€‚

 although they are very limitedã€‚ So thereï¼Œ too that we exploredã€‚

 one is called API water marking and the other is membership classificationã€‚

 So we'll briefly talk about both theseã€‚ The first is water markingã€‚

 So here you as the person training and hosting the victim modelã€‚

 you understand and acknowledge that there are going to be people who are trying to steal itã€‚

 And so what you're going to do is from some very small fraction of queriesã€‚ So maybe like 0ã€‚1%ã€‚

 you're going to intentionally return incorrect outputã€‚So for exampleï¼Œ let's say on this exampleã€‚

 Ccle Ford had supportï¼Œ the model might predict 29% positiveï¼Œ 70% negativeã€‚

 so instead of returning what the model predicted I could just flip its prediction and return positive here even though it predicted negativeã€‚

ğŸ˜Šï¼ŒSo we're going to call this a watermark because we're going to kind of store this in a database that for this particular userã€‚

 we returned the wrong label on this query that that user submitted and so now if attacker is going to finet their model on the the data that they get out from the victim modelã€‚

 they're going to include some of this watermark data in their fine tuning process right and we know from things like GT3 that these models are incredible at memorizing their training or fine tuning data so it's very likely that if an attacker were to finet their model on watermark data that the model would memorize that for this particular watermarked input the model would return not the the predictionã€‚

Of the victim modelï¼Œ but the opposite of that prediction or something like thatã€‚

So if this is happening because we know that these models are memorizing can memorize what they see in their training dataã€‚

 then we have now a method of potentially detecting models that have been extracted from our victim model so what I want to highlight is if you fine tune on a watermarked version of the MNLI task where you have 0ã€‚

1% of the queries that are watermarkedï¼Œ the model memorizes the wrong label for 87% of the watermarked queries so now you have a clear way of detecting whether or not a model has been extracted from your victim right because if the model is performing at if it's predicting the wrong label for the watermarked data on a large percentage of those examplesã€‚

 it's very likely that it's memorized stuff from theï¼ŒUm the outputs that you gave itã€‚

 and so you can maybe try and take some legal action against the the whoever extracted this modelã€‚

Of courseï¼Œ there's a huge limitation with this approachã€‚

 which is that you're requiring the attacker to make their stolen model publicly available rightã€‚

 so maybe this makes senseï¼Œ Ohï¼Œ I see I'm buffering again sadã€‚Okayï¼Œ let's wait for it toã€‚Correctã€‚

All rightï¼Œ looks like it's working againã€‚å—¯ã€‚Yeahï¼Œ so I was basically saying that the big limitation of the watermarking approach is that if the attacker doesn't make their model publicly availableã€‚

 like maybe they're using it for some privateï¼Œ nefarious purposeã€‚

 you're gonna have no way of knowing if they've stolen your model or notã€‚

 because it's really it only makes sense if maybe in a setting where your company and you believe that another company is serving a model that was extracted from your API and you can test out whether this is true by measuring that other company's model on your watermark dataã€‚

 but manyï¼Œ many use cases of stolen models don't fall into this So this is a very limited approach there's also strategies that people have proposed since then to even even if you're going to make your model publicã€‚

 you can get around this approach if you know that watermarking is a potential defenseã€‚Okayã€‚

 the second approach is also similarly flawed it's membership classificationã€‚

 so in this case we are basically saying that we're going to try and detect inputs that seem like they're not really associated with a taskã€‚

 so if I have like a random sequence of wordsï¼Œ I might want to have a classifier to predict oh this looks like it's a kind of weird input that seems kind of useless for any real customer so maybe I'm going to return a random label or prediction for this weird example and as such an attacker that's using this random strategy is not going to be able to extract a useful modelã€‚

ğŸ˜Šï¼ŒAnd so you can train you know models to detect whether the some attacker is randomly sampling sentences or sampling them from Wikipedia or whatever but this is really not a feasible defense because you're really limited by the performance of this classifier there can be many legitimate queries that can be out of distribution for your training data maybe someone wants to use your sentiment analysis for like legal documents or social media or something that gets flagged by this membership classifier and your model start turning random labels you're going to make the customers mad right so really this is not a feasible defense either and in generalã€‚

 yeah so far there is no real defense against these kinds of extraction attacksã€‚Okayã€‚

 so before we conclude last questionï¼Œ is the watermark just the incorrect label or the exact probability distributionã€‚

 I believe you can do bothï¼Œ I assume it works better with the exact probability distributionã€‚

 but most of these APIs don't produce the exact probability distributionã€‚ So yeahï¼Œ I meanã€‚

 it also works if you watermark with just the Argmax labelã€‚

 but you likely need a much larger number of watermark examples to be able to accurately determine if a model has been stolen from your victim model or notã€‚



![](img/e69ac7ac6f0f8b3d026aef4e823b14f4_1.png)

All rightï¼Œ so that's that's it for today sorry again about the internet issuesã€‚

 hopefully they don't occur on Wednesday Yeah and on Wednesday we'll be going overã€‚



![](img/e69ac7ac6f0f8b3d026aef4e823b14f4_3.png)