# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é©¬è¨è¯¸å¡CS685 ï½œ è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶(2020Â·å®Œæ•´ç‰ˆ) - P4ï¼šL3- åå‘ä¼ æ’­ - ShowMeAI - BV1BL411t7RV

Okay hey everyoneï¼Œ so today we're going to be doing something a little different Our topic for the day is back propagation which is an algorithm we use to train our neural networks and we've been talking about neural language models so we're going to look more specifically in that context today but instead of me talking over a bunch of slides I've got this iPad weird and I'm going to be writing on it instead of the standard lecturing so this is my first time doing this I hope it is somewhat watchable let me know on Piazza if I should continue using this format going forward or stick to slides or maybe do some combination of bothã€‚



![](img/3c45fb4d9857b998e509767f9953972a_1.png)

![](img/3c45fb4d9857b998e509767f9953972a_2.png)

Okayï¼Œ so let's kind of start with an overview of what we're going to talk about todayã€‚

 so first brief review of neural language models focusing on things like what are the parameters and this is going to kind of frame the discussion of gradient descent and back prop laterã€‚

Nextï¼Œ we're going to talk about gradient disscentï¼Œ which is an algorithm that we use to train our neural networks and of course many other machine learning modelsã€‚

 for those of you who have taken a machine learning class before you are certainly familiar with this concept of gradient descent for others will briefly review the algorithm hereã€‚

 and you should also plan to read more about it on your own if after you watch this video you're still confusedã€‚

The next thing we're going to talk about is back propagationã€‚

 which is really a way to compute the gradients and we're going to go over this in both a single neuron settingã€‚

 which makes it easy to explain the intuitionï¼Œ as well as a back propã€‚Of a linear layerã€‚

Which we've seen before in our neural language modelsã€‚

 the linear layer used to project the concatenated word embedding sequence into a hidden spaceã€‚Soã€‚

These two concepts are used toã€‚Trainã€‚And endsï¼Œ neural networksï¼Œ and in our contextã€‚

 we're going to use it to adjust the parameters of our neural language model such that it makes better predictions of what the next word isã€‚

Allrightï¼Œ so that's a quick overview Now let's briefly review that feed forward fixed window language model and just use it to identify things like the model parameters which we're eventually going to be computing gradients on so if you remember we had this prefix from last time and we started with these word embeddings for each word in the prefix in our feed forward fixed window language model we concatenated them all together and we passed that through a hidden layer then we used the resulting representation to predict the word books and we did this using the Somax layer so we went through the forward propagation phase of this model last timeã€‚

 but now I wanted to kind of emphasizeï¼ŒğŸ˜Šï¼ŒWhat components of this model are the actual parameters So remember we have the word embedding C sub1ã€‚

 C sub2ï¼Œ C sub3ï¼Œ these are associated with each of the words in the prefix and remember they are lowdial real valued vectors So when we start training this model these embeddings are randomly initializedã€‚

 say they don't have any correspondence with the semantics or syntactic role of the word that they're associated withã€‚

 but we would hope that once we train the model adjust the values in these vectorsã€‚

That the vector space of the word embedding starts becoming more meaningfulã€‚

So remember when we did our hidden layer projectionï¼Œ we used this parametermeter matrix W sub1ã€‚

 similarly when we did our softm layerï¼Œ we used this other parameter matrix W sub2ã€‚

So when we wrote down the equations of this modelï¼Œ it looked kind of like thisã€‚

 We had H equals F of W sub1 times C1ã€‚C 2ï¼Œ C3ã€‚ So this is the concatenated representationã€‚

 and let's say we had Oï¼Œ which is our output probabilityï¼Œ which is the softmax of W sub2 times Hã€‚

Rightï¼Œ so here note that we have two intermediate variables we've got let me use a different colorã€‚

 we have H and Oï¼Œ so H and O are not model parametersã€‚ğŸ˜Šï¼Œbig use the same colorã€‚

 This color thing is pretty coolã€‚ So H and Oï¼Œ Oï¼Œ is that readable maybeã€‚Orã€‚Intermediateã€‚A yousã€‚Notã€‚

Parametertersã€‚If you look at these equationsï¼Œ they are functions of the model parametersã€‚å—¯ã€‚Which areã€‚

I'm probably going to go overboard on the highlighting apologies in advanceã€‚Oh's not the red colorã€‚

 but these the highlighted things are the model parametersã€‚

 So the highlighted things are what we're going to adjust and what we eventually care about is that the output of this softm layer right this probability distribution O is more in line with what we would expect to predictã€‚

 So for this prefix maybe be assigning a high probability to booksã€‚ğŸ˜Šï¼ŒOkayï¼Œ and so hereã€‚

We've got our model parametersã€‚2ã€‚Mã®ã€‚Rrimsã€‚Nowï¼Œ how do we train themã€‚To yieldã€‚Goodã€‚Predictionsã€‚

Apologies for my handwritingï¼Œ you're just going to have to deal with that for this lecture and also other lectures in which I use this iPadã€‚

So this is a questionï¼Œ the answer is gradient descentã€‚Soã€‚Umã€‚

 we are going to look at gradient descent and it's kind of broken down into three different stepsã€‚

So the first step is we need some way of determining how good our current set of parameters is at predicting the next wordã€‚

 rightï¼Œ So we need to measure that somehowã€‚ and we're going to do that by defining a loss functionã€‚ğŸ˜Šã€‚

We'll call it Lã€‚And L is a function of our model parametersã€‚

 We commonly use theta to refer to all of our model parametersã€‚

 So if I look at the example from aboveï¼Œ we highlighted all of these model parameters for our neural language modelã€‚

 So for NLMï¼Œ we had W1ï¼Œ W sub2ï¼Œ C1ã€‚ğŸ˜Šï¼ŒTo how many other words are in our vocabulary right so the C's are the word embeddings so that theta in this example would correspond to all of these parametersã€‚

 this is supposed to be an arrowã€‚But I'm writing this now more generallyã€‚

 this the could be the parameters of any modelã€‚And this loss functionï¼Œ L theta is going to measureã€‚

How badã€‚å—¯ã€‚Currentã€‚Moelã€‚Is at in our neural language model exampleï¼Œ predicting the next wordã€‚

But this this objective could be different for different tasksï¼Œ rightã€‚

 it could be you know answering a question or predicting the sentiment or so onã€‚

 You just need some scalar value that defines the current performance of the modelã€‚Okayã€‚

 so once you have a way to measure how well your model is doingï¼Œ the next step is to calculateã€‚

The gradientã€‚Of this loss function with respect to the model parametersã€‚

So we're going to use this notation DLD theta to denote the gradientã€‚

 and we're going to use it for all of the partial derivatives that this gradient encompasses in the futureã€‚

Okayï¼Œ so what exactly is the gradient for those of you who are unfamiliar with itã€‚

 it basically tells us so I have the scalar loss function and let's say I have a particular parameter in my model if I change that parameter if I increase that parameter how much will the loss function increase and in general the gradient is giving us the direction of steepest ascent with respect to all of the model parameters so let me write that downã€‚

ğŸ˜Šï¼ŒThe gradientã€‚I meanï¼Œ intuitivelyï¼Œ it tells usã€‚How the lossã€‚Changesã€‚Whenã€‚We modifyã€‚The modelã€‚Pmsã€‚

Theta and importantlyï¼Œ it gives us directionã€‚Sepestã€‚Theentã€‚Is this important so we willã€‚I write itã€‚

Okayï¼Œ so I'll give you an intuitive example of this in a bit with a veryã€‚You knowï¼Œ simple functionã€‚

 but before I do thatï¼Œ so we defined our loss functionï¼Œ we've computed this gradientã€‚

 How do we use the gradientã€‚ What we're going to do is take a stepã€‚In the directionã€‚Of the negativeã€‚

Graientã€‚So remember that if the gradient is giving us the direction of steepest ascentã€‚

 so increasing the lossï¼Œ we know that the loss is telling us how bad the model is currently doing right so we want to actually minimize this loss functionã€‚

 so going in the direction of the negative gradient will take us in a direction that minimizes this loss functionã€‚

ğŸ˜Šï¼ŒSo doã€‚Minimizingã€‚The lossã€‚So what does it mean to take a step in the direction of the negative gradientï¼Ÿ

So let's say that our model parameters were theta oldã€‚

 these are our current model parameters and we want to get a new set of parameters theta new thatã€‚

Have that result in a lower lossã€‚So to do thisï¼Œ we're going to take our old parameters and we are going to subtract this eda multiplied by the gradientã€‚

And so what is Ada hereï¼Œ Eda is a variable that corresponds to something called a learning rateã€‚

 which controlsã€‚The step sizeï¼Œ so how much of a step we want to make in the direction of the negative gradientã€‚

Seems highlighter worthy as wellã€‚Okayï¼Œ so that was a lot of information let's look at a simple example that'll hopefully we make it more clear so let's just look in a two dimensional plot I have one parameter and one loss function sorry I have obviously a scalar loss function so we have our loss on the y axis L sub theta and we have theta here so let's say theta is also a scalar hereã€‚

ğŸ˜Šï¼ŒAnd let's say the loss function looks something like thisã€‚

So this is a convex function in practice we won't be dealing with functions like thisã€‚

 but we're still going to use gradient descent this is just for demonstrationã€‚

All right let me pick a different colorï¼Œ so let's assume that I randomly initialize thetaã€‚

 so I have no idea before starting training what the best value of theta is that minimizes this loss functionã€‚

So maybe I start up hereã€‚å—¯ã€‚Say this is randomã€‚Initializationã€‚

So I set my theta to sum value over hereï¼Œ I compute the loss function over my training data and I find that it's pretty highã€‚

So now I'm going to compute the gradient right and so I compute the gradient at this pointã€‚

 it's this line hereï¼Œ this tangent lineï¼Œ and I decide to go in this direction rightã€‚

 the direction of steepest descentï¼Œ So maybe I take a step in this directionã€‚

And I end up over here with my next when I use this update functionã€‚ So this one was theta 0ã€‚

 our random initializationã€‚ This one is theta 1 after one stepã€‚

 And this direction here was given to us by D Lï¼Œ D theta 0ã€‚ğŸ˜Šï¼ŒOkayã€‚

 so now we're going to just repeat this processã€‚So I'm going to compute a new gradientï¼Œ go over hereã€‚

 compute a new gradientï¼Œ go over hereï¼Œ and eventually maybe I finally reach the bottom hereã€‚

 the minimum of this functionï¼Œ and I claim that I'm doneï¼Œ I've reached the minimumã€‚å—¯ã€‚

 so my goal is to get to this pointã€‚And the way in which I'm going to do that is by computing these gradientsã€‚

 taking a step in the direction of the negative gradient where this learning rate value controls how far I step so as an example of what happens if I have a big learning rateã€‚

 imagine that if I had a huge learning rate and I started back at this pointã€‚

 the randomly initialized pointï¼Œ I do run into the danger of completely overshooting the the minimum and ending up on the other side here and that's obviously not good and if my learning rate is too large I might see inã€‚

ğŸ˜Šï¼ŒAKind ofã€‚Zigzag pattern of my loss function where eventually I might get to the minimum or I could just overshoot it completely and go somewhere else in the perimeter spaceã€‚

 which results in a higher lossã€‚ Maybe I'll get rid of theseã€‚

 That was just a demonstration of what happens if your learning rate is too highã€‚ So in generalã€‚

 a learning rate is part of your model hyper parametersmeterã€‚æ˜¯ã€‚

So a hyperparameter refers to something that is not learned by the modelã€‚

 but is rather something that you as the designer of the model set before the training process even starts in general you might have to train multiple different models using different hyperparameters before you can find one that maybe results in the smallest lossã€‚

So hyperparameters are tunedã€‚I mean in most casesï¼Œ nowadaysã€‚

 they're I think still chosen roughly manuallyï¼Œ people who have a lot of experience training neural networks tend to know what range of hyperparameters tend to work best for given problemsã€‚

 there's also a whole research field on optimizing these hyperparameters automaticallyã€‚

 but we're not going to talk about that in this classã€‚Okayï¼Œ so rightã€‚

 going back to this you might wonderï¼Œ well for such a simple functionã€‚

 can't I just get to the minimum compute the minimum in a closed form and not have to go through this whole process of iteratively computing the gradient and adjusting my model parametersã€‚

 this is true for simple models like least squares as a famous example that have closed form solutionsã€‚

 but in practice when you're dealing with these neural networks that have know highly nonlinear relationships between inputs and outputs we went over that last time there is no closed form solution for computing the best set of parameters all of the loss functions we deal with our nonconvex and we still use gradient descent to approximate the minimumã€‚

 so none of the methods we're using guarantee that we're going to reach the global minimum ofã€‚ğŸ˜Šã€‚

The set of parameters optimize that reaches the global minimumï¼Œ but in practice these methodsã€‚

 gradient descent seems to give you a reasonable solution in most casesã€‚

I should also mention that this update rule here is just vanilla gradient descent in practice there are several more advanced optimizersã€‚

 things like Adam or Ada grad that people might have seen if you'veã€‚ğŸ˜Šã€‚

Dealt with neural networks beforeï¼Œ they're all kind of just variants on this equation with clever methods of determining step sizes adaptively and so onã€‚

Okayã€‚So that is the high level of gradient descentã€‚

 so hopefully at this point you understand why we are computing these partial derivativesã€‚

 it's because they tell us the direction of steepest asscent and then why we're stepping in the negative direction of that so that we can minimize our loss function so hopefully that's all clear if notã€‚

 I'd encourage you to do more reading on just general machine learning materials and maybe I'll put some links to those on Piazza or the websiteã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so now that we've gone over at least this high level process of gradient descentã€‚

 let's look at a concrete exampleï¼Œ this one is going to be fairly straightforwardã€‚

 but this is also going to serve to introduce back propagationã€‚

 which is a method that we use to more efficiently compute the gradientã€‚ğŸ˜Šã€‚

So we'll look at this in a single neuron exampleï¼Œ so we'll be dealing with completely with scalrsã€‚

 no matrices or vectors in this example just to make things easyã€‚

So let's depart a little bit from our neural language model framework and because that's kind of hard to do in this toy setting insteadã€‚

 let's just say we have a generic problem where we have some inputã€‚ğŸ˜Šï¼ŒOur input is a single numberã€‚

We multiply that number by some scalar weightï¼Œ againã€‚

 just a single number to get some hidden representationã€‚å—¯ã€‚

Then we multiply that with another weight W sub2 to get a scalar outputã€‚All rightã€‚

 so one question that you should be able to answer at this point or is sorryã€‚

 what are the model parametersï¼ŸOkayï¼Œ so in this caseï¼Œ we have just two model parametersã€‚

We have W sub1 and W sub2 x is our inputï¼Œ so x in this case is not a word embedding that's learned by the model X is just a number that we observe in our training data setã€‚

And so maybe I'll make that more clearã€‚Okay parameters are W1ï¼Œ W2 inputsã€‚Or look likeã€‚X Y pairsã€‚

 so X is a single number input and y is the target output in our training data setã€‚

 So we're just trying to learn a function to map one number to another number and our loss function is going to be defined by how close is the output O to the target output Yã€‚

But before we get into thatï¼Œ let's look at how we compute these intermediate variablesã€‚

 so let's define this first equation as tan0 H of W1xã€‚

 so here we're introducing a new nonlinear functionã€‚

 tan0 H is a common activation function used in recurrent neural networksã€‚

 something that you should be familiar withã€‚Okayï¼Œ and let's sayï¼Œ oh also uses a10 H of this productã€‚

So again here note that we have two intermediate variables right we have H and Oã€‚

 these are not our model parametersï¼Œ they're intermediate variablesã€‚

 we don't have to compute derivatives with respect to H and O specifically to do our model updates right our model updates our theta is just W1 and W2ã€‚

Okayï¼Œ so let's state this again a little more concretely just to make the training situation more clearã€‚

 so let's say assumeã€‚å‘ƒ inã€‚Wï¼Œ that was terribleã€‚All rightï¼Œ inputsã€‚Carã€‚Xï¼Œ Yã€‚Scalarã€‚Pearsã€‚And our goalã€‚

 we wantã€‚To predictã€‚Whyã€‚From Xã€‚So what do we do hereã€‚

 The first step is something that we've seen beforeï¼Œ it's called forward propagationã€‚

So given an outputï¼Œ sorryï¼Œ an input Xã€‚Yeah goã€‚Different highlightã€‚So givenã€‚Maybe I'll do this tooã€‚

So given an X in our training data setï¼Œ we will first compute the value of each neuronã€‚

So here I'm using the term neuron to refer to these three like nodes in this graphã€‚So the inputã€‚

 the intermediate and the intermediate variables hereï¼Œ O is actually our outputã€‚

Compe the value of eachã€‚What is it trying to say each neuronï¼Ÿå—¯ã€‚So these are H and Oã€‚Usingã€‚

The model equationsã€‚Okayï¼Œ so let us make that more clearã€‚What are the model equationsã€‚

 there are these two thingsã€‚Okayï¼Œ and nextï¼Œ what are we going to doï¼Œ We're going toã€‚Predictã€‚

The valueã€‚Of whyï¼Œ actuallyï¼Œ this is not really a separate stepã€‚ but we did it in the firstã€‚

 but the first thingï¼Œ but our O value is going to be the model's prediction of yã€‚å—¯ã€‚And finallyã€‚

 what do we wantï¼Œ what is our goalï¼Ÿå“¥ã€‚Makeã€‚Oh as closeã€‚To y as possibleã€‚Okayã€‚

So this is all stuff that we saw from beforeï¼Œ let's look at step twoï¼Œ stepã€‚2ã€‚Compute loss functionã€‚

So this is going to tell usã€‚I don't even know why I'm doing these highlightsã€‚

 So this is going to tell us is going to measure that closenessï¼Œ rightã€‚

 So since we're in the simplified caseï¼Œ we'll just use the square lossã€‚ğŸ˜Šï¼ŒIn this exampleã€‚

For simplicityã€‚In the next videoï¼Œ we're going to look at the loss function used for neural language modelsã€‚

 which is a cross entropy lossã€‚ but here we'll stick to something simpleã€‚

The square loss is just defined as follows our ground truth minus our predictions squaredã€‚

So obviouslyï¼Œ if O and y are close to each otherï¼Œ then this loss function is going to be lowã€‚

 If O is a terrible prediction of yï¼Œ then this loss will be highã€‚ So againï¼Œ O is the predictionã€‚

Why is the targetï¼ŸAnd we've observed why in our training data setã€‚Okayï¼Œ so now the fundsã€‚

You can call it fun starts as an asideï¼Œ when I was doing my PhDã€‚

 we didn't have all these fancy libraries that automatically compute gradientsã€‚

 So most of my research time was spent implementingã€‚

partial derivatives of various complicated neural networksã€‚

 but nowadays you don't have to do all of that because there's tools that do it automaticallyã€‚

 which is really awesome and has contributed a lot to advancing research in NLP over the last couple of yearsã€‚

So we'll get to that a little bit more in another video where I go through how you actually code up these things in Pytorrchã€‚

Okayï¼Œ but not to get two off track here in our exampleï¼Œ we have just two parametersã€‚

 W sub1 and W sub2ï¼Œ So all we want to do is compute what just happenedã€‚1mentï¼Œ okayã€‚

D L DW sub 1 and D L DW sub 2ï¼Œ rightï¼Œ I want to compute these two partial derivativesã€‚Okayã€‚

 so let's start with the actual math portion hereã€‚How do I compute these two thingsï¼Œ Let's startã€‚

 let's work our way backwardsã€‚ This is generally what we doã€‚ So going from the outputã€‚

 we'll go back like thisã€‚In that direction and so the first parameter we encounter here is W sub2ã€‚

 so let's start computing that derivative first and then we'll get to W sub1ã€‚Okayã€‚All rightã€‚è‡ªå·±ã€‚

s to be consistentã€‚Okayï¼Œ so startã€‚Wellã€‚This oneã€‚And an important concept to note hereã€‚

 we've already gone over the chain rule of probabilityã€‚

 but important in these kinds of computations is the chainã€‚Rule of calculusã€‚

So for those of you who have forgotten what this isã€‚

 it basically says if I want to differentiate some functionã€‚

That itself is this kind of composed function two functions composed togetherã€‚

 so here if I want to differentiate G of f of x with respect to xã€‚

 this is equivalent to the derivative of G of x with respect to Xã€‚

 sorry with respect to F I just don't doã€‚ğŸ˜Šï¼ŒF times the derivative of F with respect to xã€‚Okayã€‚

 so this is a basic thing that you likely learned in calculusã€‚

But it's going to be incredibly useful for everything we doã€‚After this point in the lectureã€‚Okayã€‚

 and againï¼Œ this meansã€‚This notation means partial derivative with respect to Xã€‚Okayã€‚

 so we know our loss function is thisã€‚And we also know that O here is a function of W sub 2ã€‚Okayã€‚

 so we kind of work our way backï¼Œ we have this loss functionï¼Œ we go over hereã€‚

 then we see that W2 occurs hereã€‚So the first thing we're going to do this easier to read is define an intermediate variableã€‚

 which will make applying the chain rule a little more intuitiveã€‚We use the same colorã€‚Wellowã€‚We doã€‚

Okayï¼Œ as you can seeï¼Œ I'm still not superã€‚Confident with this interfaceã€‚Let's sayã€‚å‘ƒã€‚A equals W2 Hã€‚

 where a is an intermediate variableã€‚Now we can rewriteã€‚This equation as o equals 10 h of aã€‚å—¯ã€‚

Allrightï¼Œ so now let's go through all of this and work our way back and apply the chain rule to actually compute this partial derivative DL Dw2 equals the first thing we encounter is this O right we know O is a function of W sub2 so we're going to do D O and but remember that O is actually two functions right it's this W sub2 times H and then the tan0 H applied on top of that so we introduce this intermediate variable A we're going to take the derivative with respect to A and then finally take the derivative with respect to W sub2ã€‚

This whole processã€‚å—¯ã€‚We use the chain rule to get these equationsã€‚

 if you didn't follow that you should go over this to completely understand what is happening hereã€‚

All rightï¼Œ so let's start with this derivative hereã€‚D LD Oï¼Œ if you look at Lã€‚

 this is pretty straightforwardï¼Œ rightï¼Œ we're going to take this exponentã€‚

 multiply it with the one halfã€‚ The one half is essentially there to just get rid of this constant termã€‚

 We see that the O is also negated hereã€‚ So the derivativeã€‚Total is just thisã€‚And now D O D Aï¼Œ rightã€‚

 So thisï¼Œ we're not going to actually deriveã€‚ This is just a little thing thatã€‚You should knowã€‚

Is that derivative of xï¼Œ the derivative 10 h of x with respect to x is 1 minus 10 h squared xã€‚

Wait thisã€‚So a nice property of this function 10 H is that its derivative is it includes the computation of the original functionã€‚

 so there's no need for me to compute 10 h of x again to compute the derivative because all I can do all I have to do is just square it square the original computationã€‚

So based on this equationï¼Œ all I have to do is1 minus o squared so you can see how I didn't need to do anything fancy hereã€‚

 I already computed O in the forward pass and now I just do1 minus o squared in the backward pass to compute this derivative and finally D A Dw2 we know that a is just W2 times H so the partial derivative a with respect to W sub2 is Hã€‚

ğŸ˜Šï¼ŒSo there you goï¼Œ this is the full derivative ofã€‚The loss function with respect to W sub2ã€‚All rightã€‚

 so we also have another function rightï¼Œ we have DL DW sub1ã€‚And if we look back at ourã€‚Diagram hereã€‚

 we know that W sub1 is used in the computation of H right this initial computation of Hã€‚

 so we need to work our way back one more step from where we were previously and this is where the intuition of back prop comes into playã€‚

 So if we work our way backã€‚ğŸ˜Šï¼ŒAlso note that this H againã€‚

 is a kind of composed function of 10 H and the product of W1 and and Xã€‚ So before we startã€‚

 let's justã€‚When we do this beforeï¼Œ yesã€‚We will introduce another intermediate variableã€‚Enterã€‚Meteã€‚

Variable B equals W1 xã€‚So whatever is inside of this 10 H term for computing Hã€‚çœ‹çœ‹ã€‚Okayã€‚

 so now that we have intermediate variable A and intermediate variable Bï¼Œ we can just write this outã€‚

 it's EL Dï¼Œ Oï¼Œ Dï¼Œ Oï¼Œ D Aã€‚So we're at this point and now we see that a is W2 Hã€‚

 but this time we don't want to differentiate with respect to W sub2 right because we know that H is itself a function of W sub1ã€‚

 so here we depart from what we did previously and we do thisã€‚

And now we know that H is made up of this intermediate variableã€‚

 which itself includes this computation of Dw sub1ã€‚

So if I were to compute all of these individual derivatives and multiply them togetherã€‚

 I would get the partial derivative of L with respect to W sub1ã€‚Okayï¼Œ so the key point in backproã€‚

 one of the key points is that if you look at these two equations hereã€‚

 there are some common terms rightï¼Œ so specifically these two terms are shared for both computationsã€‚

Thereforeï¼Œ I can cache this fromã€‚The computation of DLã€‚

 D W sub2 and all I have to do is compute these termsã€‚So you can probably see how if I have a veryã€‚

 very long network with many different functions and layers and so on how thistuition helps right because if I can cache a large percentage of the terms in this productã€‚

 then I don't have to do those computations from scratch anymoreã€‚

 I save a lot of time and so this is one of the intuitions behind back propagation we start from the very top of the networkã€‚

 the output layer and work our way backwards so that we are able to use the cache derivatives of what I've just computed to speedily compute a of a perimeter that's used further down in the networkã€‚

ğŸ˜Šï¼ŒSo I'll write that againã€‚What is thatã€‚Oh noï¼Œ my pen has failed to writeã€‚å—¯ã€‚Oï¼Œ okayã€‚Okayã€‚

 so intuitionã€‚Orã€‚Backã€‚Proropã€‚Cashã€‚Deririvativesã€‚That appearã€‚Pigher upã€‚In the plantã€‚Andã€‚Use theã€‚

To computeã€‚Lower levelã€‚Gsã€‚When I say things like high level and low levelã€‚

 I'm referring specifically to essentially the distance from the output layerã€‚

 so things that are close to the output layer are up high and things that are farther down are low level So in our neural language modelã€‚

 the most low level parameters are the word embeddings themselves right they're the furthest away from the end word predictionã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so then just as a example of the parameter update stepã€‚Remember from beforeã€‚

 in my gradient descent thingã€‚å—¯ã€‚Where is thatï¼ŸPsã€‚Here I have this update equationï¼Œ rightã€‚

 So how do I apply this update equation down hereï¼Œ Nowï¼Œ it's pretty straightforwardï¼Œ rightã€‚

 now that I've computed these partial derivativesï¼Œ I can make a new W sub1 by taking the old value of it subtractingã€‚

ğŸ˜Šï¼ŒOhï¼Œ whoopsï¼Œ that's not the right way to write an Eda learning rate times this partial derivativeã€‚

 similarlyã€‚Word2 new is word2 old minusã€‚BLDç­‰äº37ã€‚So this is the parameter update stageã€‚Okayã€‚

 so at least this part of I just want to make sure I didn incorrectly okayã€‚So at least to this pointã€‚

 things should be fairly clearï¼Œ we've dealt with a very simple example and the derivatives are also fairly straightforward if you're having some issues with this partã€‚

 you should almost certainly spend the time to understand it because this is essentially the core algorithm that all of the models will be discussing for the rest of the semester use for trainingã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so for the last part of this lecture videoï¼Œ I know it's already kind of longã€‚

 hopefully I can get through this in not too much timeã€‚

 I wanted to give some intuition on how this works when we're not talking about single neurons anymoreã€‚

But we'reã€‚Talking about multiple neuronsã€‚ So this is much closer to the setting of our neural language modelã€‚

 So specifically hereï¼Œ we're going to move away from these scalrs to the matrix vector notation that we sawã€‚

In the slides from last timeã€‚So for this partï¼Œ I justã€‚

 I'm not going to derive the gradient for like the fixed window neural language modelã€‚

 I wanted to go over just how do we derive the partial derivative for a very commonly used component in these models the linear layer which we already saw in that modelã€‚

Soã€‚Linearã€‚Whereã€‚Simpleã€‚Componentã€‚å‘ƒã€‚And endsã€‚It used in the neural language model to project that concatenated representation to the hidden spaceã€‚

 and we're going to look at the definition as followsã€‚å‘ƒå‘ƒã€‚Hereï¼Œ W is our weight matrixã€‚

And X is a matrix of a training exampleã€‚ So this also will serve to introduce the concept of batched trainingã€‚

So here we'll call this the input matrixï¼Œ I guessã€‚Okayï¼Œ so now we no longer have scalrsã€‚

 both of these x and W things here are matricesï¼Œ but W is the parameter that we're interested inã€‚

Okayã€‚So I guess I'll just highlight that linear layer I'm talking about thisã€‚

 this is also called feed forward layer or fully connected layerã€‚If so onã€‚Okayã€‚

 you also might see this with the order of the product switch aroundï¼Œ so w of Wxï¼Œ sorryã€‚

 y equals F of Wxï¼Œ but we're going to go with this because it makes the derivation a little more straightforwardã€‚

But yeahï¼Œ in practice this general concept applies in general So what do I mean by input matrixã€‚

 this is a concept that we haven't talked about to this pointã€‚Ehï¼Œ why does my pen keep breakingã€‚

Start working againã€‚Okayï¼Œ so what do I mean by input matrixï¼Œ rightï¼Œ what does this containï¼ŸSo Iã€‚

 I meanï¼Œ that each rowã€‚Of X containsã€‚A vectorã€‚It correspondsã€‚To usã€‚Singleã€‚Training exampleã€‚

AndThis whole collection of examples that occurs in X is called a batchã€‚

So that is some terminology thatã€‚We'll see over and over againï¼Œ so in modern neural networksã€‚

 we don't like we did up hereã€‚Put single inputs through a model at a timeã€‚

 So instead we feed a batch of examples through the modelã€‚

 estimate the gradient on that entire batch of inputs and then do the update So this is often referred to as mini batch gradient descent or batch gradient descent and another important hyperparameter here is the batch size of the modelã€‚

 which means how many examples are you going to have per batchã€‚

 this is going to determine the number of rows in your X matrixã€‚ğŸ˜Šï¼ŒSo what are these vectorsã€‚

 it kind of depends on where in the model you areï¼Œ where this linear layer is in our neural language model exampleã€‚

 we use a linear layer to project the concatenated representation of the embeddings into some hidden space so in this case the x matrix could contain concatenated representations for a bunch of different prefixes so each row contains a concatenated representation of a different training prefixã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so let's introduce some variables hereï¼Œ N is a number of examplesã€‚Or theã€‚Batch sizeã€‚

Let's say D is the dimensionalityã€‚Of theã€‚Whatever is in the xï¼Œ the dimensionality of the vectorsã€‚

 will'll just say dimensionality of hidden spaceã€‚Andã€‚Okayï¼Œ let's sayã€‚All rightã€‚

 maybe we'll make this more similar to the example and say D is a dimensionality ofã€‚

Ebeddings and M is dimensionalityã€‚Of hiddenã€‚The actualã€‚For this exampleã€‚

 this these two don't matter this is a more general exampleã€‚ So let's say we have a toy exampleã€‚

Where N is2ï¼Œ D is also2ï¼Œ and M is 3ã€‚ So this example is inspired by one of Justin Johnson's notesã€‚

 He's a professor in computer vision at Michiganã€‚But I think this is a great example to demonstrate some of the tricks that are incorporated into back propagation when you go from scalarrs to the matricesã€‚

Okayï¼Œ so let's say we haveã€‚A matrix Xï¼Œ in our caseï¼Œ Xã€‚

 the input matrix is a number of examples in the batchï¼Œ which is n by the dimensionality ofï¼Œ sayã€‚

 the embeddings is could just be the inputã€‚ So it's a two by two matrixã€‚

 And if we write out each element of the matrixï¼Œ we get something like this that's a2ã€‚ğŸ˜Šã€‚

Let's also say we have a W matrixã€‚ So this W is trying to project these x vectors into this hidden space of M dimensionsã€‚

 Thereforeï¼Œ W is a 2 by three matrixã€‚ So it looks likeã€‚1ï¼Œ1ï¼Œ W1ï¼Œ2ï¼Œ W 1ï¼Œ3ï¼Œ W 2ï¼Œ1ã€‚W2ï¼Œ2ã€‚W2ï¼Œ3ã€‚

This is the twoã€‚ Okayï¼Œ And let's say just to make things easy that this F is going to be an identity functionã€‚

F of x equals xã€‚ So we're not going to have to deal with chain ruling another nonlinear functionã€‚

Okayï¼Œ so we're interested in this yï¼Œ rightï¼Œ Y is the output and what does y look like since it's the product of x and Wã€‚

å—¯ã€‚If we write out the terms and yï¼Œ first of allï¼Œ what is its dimensionalityã€‚

 you've now seen this in homework 0 rightï¼Œ we're taking this two by two matrix multiplying it by a  two by three matrixã€‚

 therefore the resulting matrix is also a two by three matrixã€‚ğŸ˜Šã€‚

So when we compute this product we get x11 times w11 right these two plus x12 w21 and this is the first element of our matrix hereã€‚

 so let's look at the second so here we are multiplying this row by this column sorry this first row by this columnã€‚

So it's x1ï¼Œ1ï¼Œ W1ï¼Œ2 plus x1ï¼Œ2ï¼Œ W22ï¼Œ and so onã€‚And at the bottomã€‚

 let's just fill in the last cell hereï¼Œ it's x21ï¼Œ W1ï¼Œ3 plus x2ã€‚X2ï¼Œ2ï¼Œ W2ï¼Œ3ã€‚Okayã€‚

 so this is what Y actually looks likeï¼Œ rightï¼ŸAnd let'sã€‚Assumeã€‚We're givenã€‚å—¯ã€‚D Lï¼Œ DYã€‚

So this is the partial derivative of some loss function with respect to why so we're ignoring all of the rest of the computations in this networkã€‚

 let's just assume we've already computed this through backpro and now we're at the pointã€‚

And let's say we want to computeã€‚Want to computeã€‚I meanï¼Œ there are two things of interest hereã€‚

 DLDX and DLDWã€‚Rightï¼ŸHow do we do thisï¼ŸLet let's look at this one firstã€‚

 we know through the chain rule that DLDX simply is DLDYï¼Œ which we said we already hadã€‚

 and we multiply that by DY Dxã€‚Rightï¼Œ so this follows from the chain rule and this term here is called the Jacobianã€‚

So the Jacobian matrix is essentially every element of y differentiated with respect to every element of xã€‚

 And so in practiceï¼Œ actually forming the Jacobian matrix can be impracticalï¼Œ rightï¼Ÿ

 Let's think about the size of this matrix since it's every element of yï¼Œ rightã€‚

 How many things are in Yã€‚ Y is an N by M matrixï¼Œ and then every element of Xã€‚

 which is an N by D matrixã€‚ğŸ˜Šï¼ŒRight so you can imagine on the scale at which modern neural networks neural language models are trained atã€‚

 we don't want to be forming this kind of matrix where we're going to run out of memory pretty fast with deeper models and so the whole point of this current exercise is to show you how you can compute DLDX without explicitly forming DYDXã€‚

å‘ƒã€‚Oopsã€‚Sorryï¼Œ the per is acting upã€‚å¾ˆç–¼ã€‚Okayï¼Œ there it isã€‚I guess I will edit that outã€‚

 but the point isï¼Œ we can computeã€‚å—¯ã€‚Dï¼Œ Lï¼Œ Tï¼Œ Xï¼Œ withoutã€‚Formingã€‚Dï¼Œ Y D Xï¼Œ of courseã€‚

 to demonstrate this because in this toy exampleï¼Œ it is feasible to look at this Jacobian matrixã€‚

 Let's just proceed by forming D Y D Xã€‚ And then this will give you some intuition on why we don't have to form it in the futureã€‚

So DL Dx equals so remember this is what we want to compute this is our goalã€‚ first of allã€‚

 the partial derivative of the loss with respect to sum perimeter is going to be the same dimensionality as that perimeter so since x here is a2 by2 matrix DL Dx is also going to be a2 by2 matrix and we can explicitly represent each element of this matrix with these scalrsã€‚

Dï¼Œ Lï¼Œ Dï¼Œ X1ï¼Œ1ï¼Œ Dï¼Œ Lï¼Œ Dï¼Œ X 1ï¼Œ2ï¼Œ and so onã€‚So we're interested in computing these four elements and let's look at what actually happens if you want to individually compute each of these four numbersã€‚

So we can start with this oneï¼Œ DLï¼Œ D X 11ã€‚And so we know from our equation up here that this is the same as a multiplication ofã€‚

DL DY times D Y D X 11ã€‚ so note that L is a scalar here and x is a scalarã€‚

 so this whole thing is also a scalar and we want a way to get a scalar out of this and we know that DL DY is going to be a3 by2 sorry a 2 by 3 matrix because it's the same shape as y and we can also write this term here as a 2 by3 matrix and compute this this scalar with a element wise product of these twoã€‚

I'm sorryï¼Œ the dot productã€‚ So if we go over hereã€‚Let's expand out what this term looks likeã€‚

 I said we can represent it as a two by3 matrixã€‚ So if you look at the elements of yã€‚

 which we wrote up hereï¼Œ you'll see that we have if we're differentiating with respect to sorry x11 this is an x11 and this is an x11 and it also occurs hereã€‚

 So the occurrences of x sub11 are all in the top row of this y matrix and you can look at what is being multiplied with x11 to get an idea of what the derivative isã€‚

 So here it's w11 here W12 and in this caseï¼Œ W13ã€‚So we get W1ï¼Œ1ï¼Œ W1ï¼Œ2ï¼Œ and W1ï¼Œ3ã€‚

And in the second rowï¼Œ note that there are no occurrences ofã€‚X1ï¼Œ1 right now we're in the second rowã€‚

 so we're looking at x21 instead or x22ï¼Œ x23ï¼Œ so all of these terms are0ã€‚Okayã€‚

 so if we do the same exercise for xï¼Œ say1ï¼Œ twoã€‚å—¯ã€‚Weã€‚Can expand out this x12 term hereã€‚

 and it looks like thisã€‚Oopsï¼Œ that is it2 on2ã€‚These are all zerosï¼Œ similarlyï¼Œ if we doã€‚

Let's look at one of them on the bottom rowã€‚ So x 21ã€‚And this term looks like thisã€‚

 So here the top elements are0ã€‚The bottom elements are thisã€‚And finallyï¼Œ let's just not evenã€‚Wellã€‚

 I guess we can do the last oneã€‚And what is this thingã€‚It's againï¼Œ zeros in the topã€‚

 but we have this on the bottomã€‚Soã€‚We've computed these elements of the Jacobianã€‚Okayã€‚

 so we've looked at the second term of this product rightï¼Œ of these productsã€‚

 these elements of the Jacobianã€‚ Now let's look at what this isï¼Œ this DLDYã€‚

So remember that we're given thisï¼Œ rightï¼Œ so we can also just break this down into a bunch of different elementsã€‚

TheLDY11ï¼Œ rememberï¼Œ this is going to be a two by three matrixã€‚Because y is a two by three matrixï¼Ÿ

This one2ï¼Œ1ã€‚6ã€‚Toã€‚All rightï¼Œ so now we're just going to take the dot product of these two thingsã€‚å—¯ã€‚

So we have this up hereã€‚And if we do Dï¼Œ Lï¼Œ Dï¼Œ Y times Dï¼Œ Yï¼Œ Dï¼Œ X 1ï¼Œ1ï¼Œ we getã€‚Dï¼Œ Lï¼Œ Dï¼Œ Y 1ã€‚

1 times W 1ï¼Œ1ã€‚ rightï¼Œ This the first element hereã€‚å—¯ã€‚And then we're going to add that to D Lï¼Œ D Y 1ã€‚

2 times W 1ï¼Œ2 plus D Lã€‚Eï¼Œ byï¼Œ1ï¼Œ3ã€‚W1ï¼Œ3ã€‚Okayï¼Œ so at this pointã€‚

 we can substitute this into the first element of our DLDX matrixã€‚

 so let's make that more clear with some highlighting so this term hereã€‚



![](img/3c45fb4d9857b998e509767f9953972a_4.png)

Is now replaceable with this term hereã€‚So if we rewrite now this DLDxã€‚Using what we've just computedã€‚

 firstï¼Œ we'll plug in the screen partã€‚And then the second element it kind of just follows we're now using this matrix for the Dx sub12 so that one maybe I canã€‚

Be more economical in how I write it is DY11ã€‚W 2ï¼Œ1 plusã€‚E Dï¼Œ U I 1ï¼Œ2ï¼Œ W 2ï¼Œ2 plusï¼Œ okayã€‚

 looks like I can'tã€‚3ã€‚Okayï¼Œ so similarlyï¼Œ let's just write the Okayï¼Œ so this is supposed to beã€‚

AndSo hereï¼Œ let me just divide these to make it more clear and we can also just write out this one in case you want to go back and make sense of thisã€‚

Bï¼Œ Y 2ï¼Œ2ï¼Œ W 2ï¼Œ2 plus Dï¼Œ Lï¼Œ Dï¼Œ Y 2ï¼Œ3ã€‚W2ï¼Œ3ï¼Œ Larryã€‚ That's a little messyã€‚Okayã€‚

 so we get this this derivation of DL D Xã€‚ and we did this by forming these intermediate matrices hereã€‚

 But the key intuition hereï¼Œ the takeaway is that this can just be rewrittenã€‚ğŸ˜Šï¼ŒAs matrixã€‚Yesã€‚

The matrix productã€‚å—¯ã€‚Ofã€‚D L DYã€‚Which were given againã€‚The L DY 2ï¼Œ1ï¼Œ the L DY 2ï¼Œ2ã€‚

W and the transpose of the weight matrixã€‚ So W1ï¼Œ1ï¼Œ W21ï¼Œ W1ï¼Œ2ï¼Œ W2ï¼Œ2ï¼Œ W2ï¼Œ2ï¼Œ Wã€‚Sorryï¼Œ this isã€‚W1ï¼Œ3ï¼Œ W2ã€‚

3ã€‚So if you go back and look at our original weight matrix hereï¼Œ this matrix is just the transposeã€‚

Over hereã€‚And soã€‚Just to be clearï¼Œ if you look at the first term in this product right this is a two by3 matrix multiplied by 3 by 2 matrixã€‚

 so their end result is a2 by2 matrixï¼Œ which again is what we want for DLDx and let's look at the first elementã€‚

 this thing hereï¼Œ this is just the dot product between this row vectorï¼Œ and this column vectorã€‚

And the same is true for all of the other values in DLDXã€‚

So the important thing here is that we can just simply rewrite DLDX as DLDYã€‚Wopsã€‚

 times the transpose of the weight matrixã€‚Soã€‚And practiceã€‚We do thisã€‚Insteadã€‚Of computingã€‚

The Jacobanã€‚Because this is far more efficientï¼Œ if you want an exerciseï¼Œ you can compute DLDWã€‚

 the solution is kind of follows from thisï¼Œ it's the transpose of the x matrix times DLDYã€‚Okayã€‚

 so what have we covered todayï¼Œ todayï¼ŸWe've covered gradient descentã€‚Rightï¼ŒThis is importantã€‚

 You should definitely know how this worksã€‚ You shouldã€‚Youã€‚Shouldã€‚Be ableã€‚To implementã€‚Back propã€‚

For a singleã€‚Nuronã€‚Networkã€‚So this is all scalarrs and you should be able to understand generally what's going on in the matrix vector settingã€‚

 especially this concept of batches we're not going to actually test you on whether you can you know derive these gradients in the matrix vector setting but it's an important conceptã€‚

 this is what's happening in the background right so we also covered back propã€‚

And the key takeaways of back prop areã€‚Chain ruleï¼Œ that'sã€‚Major componentï¼Œ andã€‚Alsoï¼Œ cachingã€‚

These intermediateã€‚Derivativesã€‚Andï¼Œ finallyã€‚å—¯ã€‚Using efficientã€‚Computationsã€‚To avoidã€‚

 when you can formingã€‚å‘ƒ Jacobcobã€‚So this latter partã€‚

 where we derive these updates for the linear layerã€‚

 these are kind of things that are happening in the background when you use something like Ptorrch to compute derivativesã€‚

 so it's important to know what's going onã€‚Okayï¼Œ so for next timeï¼Œ next weekã€‚

 well be looking at first an implementation of the single neuron case using Pytorchã€‚

 and we'll also go over the loss function training objective that's used for language modeling and we will start our introduction to the transformer modelã€‚

 which is the final model that we'll be covering in this semesterã€‚



![](img/3c45fb4d9857b998e509767f9953972a_6.png)