# P10ï¼šL7.2- BERT - ShowMeAI - BV1BL411t7RV

Okayï¼Œ heyï¼Œ everyoneï¼Œ sorry for the delayã€‚ I I thought I was liveã€‚

 but I didn't click the go live buttonï¼Œ so that was donemeã€‚

It would be really great if someone could just type something in the chat box to indicate that they can hear me okay and that everything is workingã€‚

å—¯ã€‚I'm not sure from looking at the current thing if anyone is watching thisã€‚Okayï¼Œ coolã€‚

 So I'm gonna startã€‚ And unlike last time I'm going to just ignore all of the weird error messages that I'm getting from time to time on my streaming software thingã€‚

 So if at any point you lose connection or I freeze or you can't hear me or things are moving slowly or whatever let me knowã€‚

 and I will try and fix it somehowã€‚ðŸ˜Šï¼ŒOkayï¼Œ so today's topic is Bertã€‚

 so we're going to pick off from where we concluded last timeã€‚

 if you remember we spent some time talking about Elmo which had this forward language model and the reverse language model we got a couple of questions on on why exactly we need both of these directionsã€‚

 maybe I didn't do a good job explaining properlyï¼Œ so oh there's some freezing lag even nowã€‚Alrightã€‚

 let me see if I canã€‚Do something about thatã€‚Honestlyï¼Œ Iã€‚Don't really know what theã€‚

Various settings on this thing doã€‚ but let's go with thatã€‚ Maybe that will helpã€‚Okayã€‚

 I try to reduce the bit rateï¼Œ I assume that might helpã€‚Okayï¼Œ so anywayï¼Œ rightã€‚

 today's topic is Bertï¼Œ So todayã€‚Fromã€‚Elmo to Bertã€‚

And we are also going to be talking about a new training objective to explain this shiftã€‚

 And that's going to be from language modelingã€‚To masked language modelingã€‚And as beforeã€‚

 if at any pointï¼Œ you have any questions about the content of the lecture or even about like course logistics or whateverã€‚

 just let me know and I will answer them from the chat box obviouslyã€‚Okayã€‚

 so before we get into mass language modelingï¼Œ I just wanted to kind of reiterate the purpose of this whole pre training phaseã€‚

Galã€‚Of pre trainingï¼Œ because I think from some of the questionsã€‚

 there was a little bit of misunderstanding as to what the role of these various language models is during pre trainingã€‚

All rightï¼Œ so our goal of pre training is to useã€‚Theseã€‚sorryã€‚Big L Msã€‚As text encodersã€‚

So you'll hear this term a lot the encoderï¼Œ and what does that actually meanã€‚

 it's referring to getting representations of words and sentences that can then be plugged into downstream tasksã€‚

 so essentially their goalã€‚Is to enableã€‚Downstream modelsã€‚To focusã€‚On the task at handï¼Œ insteadã€‚

Instead of learningã€‚Ohï¼Œ that was coolã€‚Turningearningã€‚How language worksã€‚

And remember why we want the separation rightï¼Œ Why do we want our language models to handle the you know learning these basic fundamental linguistic propertiesã€‚

 it's because if we force the downstream model to handle thisã€‚

 it has a very limited amount of training data right so let's say for a sentiment analysis taskã€‚

 I might have 10000 sentences that are labeled with positive or negativeã€‚

 and this is the extent of the data I haveï¼Œ And so if you expect a model to learn from just 10000 sentencesã€‚

 all the various intricacies of English or whatever language your data citizenã€‚

 your model probably isn't going to be able to pick up on all of this right and so that's the whole point of these models like Elmo and BerRT is that we are able to pretrain them on insanely large amounts of language so we can actually reasonably expect the models to pick upã€‚

Properties of language that we expect will be helpful to solve our downstream taskã€‚

So in the example from last classï¼Œ our downstream task was sentiment analysisã€‚

 and our pre training task was left to right and right to left language modelingã€‚

 So just to clarify againï¼Œ and this is going to be useful when we eventually get to Bt and try and draw some contrasts to Bt in Elmoã€‚

We did the followingï¼Œ rightã€‚ We had a sentenceã€‚ Let's sayï¼Œ this movie is greatã€‚

And we had two recurrent language models that were implemented using this LSTM architecture thatã€‚

 againï¼Œ we're not going to discuss in detailã€‚Should have used a different color for these vectorsã€‚

 So anywayï¼Œ let's call this theï¼Œ these blue vectors come from aã€‚leftã€‚To writeã€‚L Mã€‚

And let's call the hidden states of this modelï¼Œ maybe HL 1ï¼Œ3ã€‚So this is a left to right modelã€‚

 And let's think about why we actually need a right to left model as wellã€‚

 If our goal is to use this as a text encoderã€‚ So if you'd think aboutã€‚This hidden stateï¼Œ let's sayã€‚

 H L sub isã€‚Theï¼Œ the hidden state associated with the word isï¼Œ this capturesã€‚Informationã€‚

About the contextã€‚This movie isã€‚But it doesn't include anything about the word greatï¼Œ rightï¼Œ No infoã€‚

Aboutã€‚The wordï¼Œ greatã€‚And so why does this not include information about the word greatã€‚

 You should think about how the model is actually trainedï¼Œ rightã€‚

 the pretraining objective is to predict the next wordï¼Œ rightï¼Œ Soï¼Œ okayï¼Œ the video is very pixelatedã€‚

Okayã€‚Let's see if I can change thatã€‚Looks like aã€‚Are the slide or is the note part hard to read or is it just the camera part in the corner that's pixelatedï¼Ÿ

Becauseuse if it's just that partï¼Œ then it's probably not a big dealã€‚ but if it's theã€‚

If it's the notesï¼Œ then I should probably change thatã€‚It's good for youã€‚Oã€‚å—¯ã€‚å—¯ã€‚ðŸ˜Šï¼ŒAll rightã€‚Hinganã€‚

Okayï¼Œ so for some of youï¼Œ it's good for othersï¼Œ it's notã€‚That's not a good signã€‚Okayã€‚

 give me one secondã€‚ Maybe I'll just restartã€‚çš„ã€‚The streamï¼Œ it looks like it's set to low latencyã€‚

 So maybe that's the reasonã€‚å—¯ã€‚Ohï¼Œ it's good nowã€‚Okayã€‚

 so I won't restart it if if it turns out to be blurry againï¼Œ please do complain and I willï¼Œ okayã€‚

 so it looks like adjusting the bit rate in my streaming thing actually makes a differenceã€‚ðŸ˜Šã€‚

That's a good note for the futureã€‚All rightï¼Œ so back to the notesã€‚

 remember that in a language modelingï¼Œ we're trying to predict the next wordã€‚

 so I can't actually have this representation at the word is incorporate knowledge of the word great because that's the word I'm trying to predict right And so if I want to use this model as a text encoder of the sentenceã€‚

 this movie is greatï¼Œ which means I'm going to use it for say some sentiment analysis taskã€‚

 There's no reason for the representations of these words to be unaware of other words that come in the future right because in the sentiment analysis taskã€‚

 My goal is not to predict the next word of each input sentenceï¼Œs I'm given an input sentenceã€‚

 and then I'm asked to predict its sentimentã€‚ So there's a discrepancy between the pretraining task where I actually need this property of my current context can't see any part of the future and the downstreamã€‚

Taskï¼Œ which in our example is sentiment analysis in which I have access to the full sentenceã€‚

 and I want to do some prediction at the endã€‚ So this is why in Elmoã€‚

 we have two directions of language modelsã€‚ And the second direction is intended to get around this issue of the word great here not being represented in this single token representation for isã€‚

So in our left to right language modelï¼Œ you can think about it essentially as just reversing the inputs and then learning a language modelã€‚

 so intuitively I just have great is movie thereã€‚ðŸ˜Šï¼ŒAnd I do the exact same thingã€‚

So this is the left or the right to left language modelã€‚

 It's just trained to predict the the previous word hereã€‚

 I'm just writing it in the exact same formatã€‚ And let's just make these vectors greenã€‚ So in thisã€‚

In this settingï¼Œ the right to left modelã€‚And we'll call this one's hidden states or one through Nã€‚

 if you think aboutã€‚This vector hereï¼Œ the one associated with isã€‚

What information does this vector haveï¼ŸWellï¼Œ it capturesã€‚The contextï¼Œ greatã€‚Isã€‚Rightã€‚

 so if I were to concatenate both this blue vector and green vector for is togetherã€‚

 now my vector for is has a representation of both the previous context and the future contextã€‚

 and importantlyï¼Œ the pretraining taskï¼Œ the predict the next wordï¼Œ it never has to cheat rightã€‚

 because we're still in each of these separate directionsï¼Œ we have a valid language modelã€‚

 But when we use this to solve our downstream taskã€‚

 we can get around this by the limitation of only seeing the previous words by concatenating the left to right and right to left representations togetherã€‚

 So let's actually take a look at how we use this in our downstream taskã€‚

 So this is a similar picture to the one we had last weekï¼Œ except now since we've learned about Elmoã€‚

 it's a little moreã€‚ðŸ˜Šï¼ŒSpecificï¼Œ whoopsï¼Œ this movie is goodï¼Œ rightï¼ŸThis movieã€‚

I I should have drawn these out fartherï¼Œ okayã€‚Is goodã€‚Is it goodï¼Œ greatã€‚Okayã€‚

 I keep forgetting greatã€‚So here I'm going to haveã€‚å‘ƒã€‚Theseã€‚Concatenated vectorsã€‚

 which are the word embeddingsï¼Œ and half of them are going to be the part from the right to left language modelã€‚

 and the other half are going to be from the left to right language modelã€‚

So here you can see how let me actually make this more clear and write downstream usageã€‚

So now in our pre training taskï¼Œ we're training all the word embeddings and stuff from scratchã€‚

 but in our downstream testï¼Œ we initialize our word embeddings with these concatenated Elmo representationsã€‚

 and then what we do is have a task specific model on top of itã€‚

 So in our example from last time we used a recurrent neural language modelã€‚ sorryã€‚

 a recurrent neural networkï¼Œ which does the followingã€‚

But I just wanted to make it more clear and show you that this network could look like anythingã€‚

 So it takes in the ElLmo representationsï¼Œ and then we know that the final hidden state hereã€‚

 this thing represents the whole sentenceï¼Œ This movie is great and we can use it to predict positive and how do we do this softm layerã€‚

 right we've gone over this beforeã€‚This is just one instantiation of a downstream modelã€‚ rightã€‚

 I could take these sameã€‚Ebeddings hereã€‚And I could do I could have a different downstream modelã€‚

 which maybe takes the element wise average of all of these vectorsã€‚

It's called this element wise averageã€‚And then has a softm layer on top of thisã€‚

To predict positive rightï¼Œ this could be any composition functionã€‚

 this downstream model and the important part with ElLmo is that I'm just basically feeding the word embeddings that are input to this downstream model instead of randomly initializing themã€‚

 I am initializing them with ElLmo and I'm keeping them basically fixed with the exception of this because in ElLmo we're aggregating information across multiple hidden layersã€‚

ðŸ˜Šï¼ŒOkayï¼Œ so I kind of want to stop hereã€‚ And maybe this is a good time to take some questionsã€‚

 So let's see the first questionã€‚ Could we not justã€‚

Can not consider the ordering here and learn it in the fine tuning stage That wayã€‚

 we wouldn't need bidirectional networkã€‚ So the orderã€‚

 the left or right order of the words in the sentence give us a lot of information about the syntax and semantics of various words and phrases and human sentencesã€‚

 rightï¼Œ So one very important property in sentiment analysis is a negation scopeã€‚ So as an exampleã€‚

Soã€‚Maybe I should distinguish this from the rest of the notesã€‚In response to questionã€‚

Think about theï¼Œ the sentenceã€‚ I don't knowã€‚ I loveã€‚The actingã€‚But theã€‚Rtã€‚Of the movieã€‚

It was terribleã€‚So in this settingï¼Œ if for a model that does not consider the order of the words in this sentenceã€‚

 I might know that the word butt existsï¼Œ but I don't know what words actually the word but is applied to right but is a contrastive conjunctionã€‚

 So it's kind of affecting a negative over whatever is it follows in the clause that it's used inã€‚

 So here we have but the rest of the movie was terrible rightã€‚

 So this is kind of the scope of this conjunctionã€‚ and I would love for a model that has information as to the order of these words to pick up that all of the words in this maybe I should have used highlighting for thatã€‚

All of the words in this highlighted section are kind of negated through the use of this wordã€‚

 but so this is one particular property that's very useful for sentiment analysisï¼Œ but in generalã€‚

 like the order of the words is extremely important for learning rich representationsã€‚

 and so we really need to put this into the pre-training phase and not just the fine tuning phaseã€‚

Okayï¼Œ the second questionï¼Œ is it possible to train my downstream task model over a data set after it is being pretrained by Elmo or Bertï¼Ÿ

å—¯ã€‚Soï¼Œ yeahï¼Œ in the case of Elmoï¼Œ the downstream task model is completely separate from the pretrain modelã€‚

 rightï¼Œ So if you look at these two examples in this first exampleï¼Œ the downstream modelã€‚Is an RNã€‚å—¯ã€‚

Classifierã€‚And in this exampleã€‚Downstreamã€‚Modelã€‚Is aï¼Œ I don't knowï¼Œ averagedã€‚Inbeddingã€‚

 classifier or whatever you want to call thisï¼Œ actuallyã€‚This model is usually called a neuralã€‚

Bag of wordsã€‚Because it is not considering the order of the wordsã€‚

 although the pretrain representations areã€‚ So yeahï¼Œ to get to this questionã€‚

 in the case of the in Elmoï¼Œ the downstream model is always different from the pre training architectureã€‚

 in Bertï¼Œ this is not the caseã€‚ So maybe this is a good segue to the next part of this lectureã€‚

So yeahï¼Œ I meanï¼Œ many of you have you knowï¼Œ expressed some concerns or questions about why do we need to have these two language modelsã€‚

 rightï¼Œ the ElLmo approachã€‚Of twoã€‚Separateã€‚Bmsã€‚That earth thenã€‚Can catnated togetherã€‚

Is a little hackyã€‚So what we really want is to accomplish our goal of having representations that are aware of every single other word in the sentenceã€‚

 not just the words that preceded itï¼Œ but maybe we do this in a single model as opposed to having two separate modelsã€‚

Can weã€‚Accompã€‚The same goalã€‚Within a single modelã€‚And so here is where we're going to move to a different pre training objectiveã€‚

Soï¼Œ changeã€‚Pre trainingã€‚Objectiveã€‚Fromã€‚Language modeling to masked language modelingã€‚

And so we've spent a lot of time this semester on the language modeling objectiveã€‚Luckilyã€‚

 mass language modeling is like veryï¼Œ very similar in terms of how it's implemented all of the different components of neural networks we've seen like softm layerã€‚

 transformerï¼Œ etc ceterã€‚ those are all useful for implementing mass language modelsã€‚

 the only thing that changes is the objective functionã€‚ðŸ˜Šï¼ŒSo in mask language modelingã€‚Theã€‚

 the change compared to inï¼Œ you knowï¼Œ standard left to right language modeling isã€‚

We are given a full sequence of wordsï¼Œ not just a prefixã€‚Not just prefixã€‚Whereã€‚

X percent of the wordsã€‚Have been masked outã€‚And instead of the model predicting the next wordã€‚

Instead of predictingã€‚The next wordã€‚Weã€‚Only predictã€‚Masked wordsã€‚So when I draw this outã€‚

 hopefully it'll become a little more clear as to how this actuallyã€‚

Helps us in modeling both sides of the context within a single modelã€‚

 So let's take a look at our standard language modeling exampleã€‚

 the students opening their books or or whatever it wasã€‚ So we have studentsã€‚ðŸ˜Šï¼ŒOpenedã€‚Their booksã€‚

And soã€‚Instead of having the word opened here in our training dataï¼Œ we are going to mask this outã€‚

 So this is going to be replaced with the special mask tokenã€‚

And so now we're going to just do a standard transformer on top of these the word embeddings associated with this inputã€‚

 so note that the mask token here is considered a separate word type in our vocabularyã€‚

 the mask token is also associated with its own word embedding and it's treated just like any other wordã€‚

So we knowï¼Œ likeï¼Œ in a transformerã€‚We are going to be projecting these things toï¼Œ you knowã€‚

 query key valueã€‚ We're going to do a lot of self attentionã€‚

All the fun stuff we've talked about before with transformersã€‚

So I will just summarize all of that intoã€‚Tranformerã€‚

Where N here is the number of layers that are in this transformerã€‚ And then at the end of the dayã€‚

 I should get more spaceã€‚one secã€‚It's trueã€‚Yeahï¼Œ so at the end of the dayï¼Œ Iã€‚

After all of the layers of my transformerï¼Œ I getã€‚A word representation associated with each of the vectorsã€‚

 each of the original words in my sentenceï¼Œ rightï¼Œ this is the output of a self attention layer and transformer blockã€‚

And so note that this vector hereï¼Œ the one that I will highlight in redã€‚

 corresponds to the mask tokenã€‚ So instead of doing a prediction at every single one of these tokens token representationsã€‚

 as we would in a standard left to right language model hereã€‚

 I'm only making one prediction at the position that corresponds to the mask tokenã€‚

 and I'm going to predictã€‚ðŸ˜Šï¼ŒThe ground truth word that should be occurring in this masked positionã€‚

And how do we do this predictionï¼ŸSimp by using this Huffm layerã€‚

 just as we saw in neural language modelsã€‚So this in essenceã€‚

 is mask language modeling and once we switch over to the slides in a little bitã€‚

 I'm going to talk more about specifically different masking strategiesã€‚

 what we need to think about when we're preproces our data for this taskã€‚ðŸ˜Šï¼ŒBut note that theã€‚

 the cool thing about this model is that each of these representations hereã€‚Are fully contextualizedã€‚

 soã€‚How do I say thisï¼ŸAll of theã€‚Final layerã€‚Representationsã€‚2ã€‚Fullyã€‚Contextualizedã€‚

This means that they are awareã€‚Ofã€‚Wordsã€‚In the pastã€‚

 so the standard prefix that we see in neural language modelsã€‚As wellã€‚As words in the futureã€‚

 So this isã€‚This is a difference fromã€‚This is the main difference between this setup and the left to right language modeling setupã€‚

 So we can't use mass language modeling to give us the probability of a sentence right because it is breaking our chain rule assumption of the like we can't do this product of conditional probabilities anymore because now every single representation is aware of everything else in the every other word in the sentenceã€‚

 both past and futureï¼Œ so use this for many of the applications that we talked about at the very beginning of the semester for language models and it's hard to use this model to actually generate textã€‚

 although we will discuss some models in later weeks of the semester that do use these kinds of models to generate textã€‚

 but for now the important thing to take away from this setup is that it gives us a pretrainingã€‚

In which we don't need to have two separate directions because the model here is at every position aware of the words at every other position and it can do this because it's not cheating in the sense that its input is actually masked so it has no idea what the actual word that goes into this mask token is and so that makes it a valid prediction taskã€‚

Do we then assign the embedding for mask to the word predicted This is a question No so this is a good question In mask language modelingã€‚

 it generally proceeds as we mask out some percentage of the tokens in a largeish input so maybe like 512 words or so and maybe roughly I don't knowã€‚

 like maybe 70 of them are masked we do all of the predictions of the masked words just using a soft mask layer at each position and there's there's no like feeding of the predicted word or the ground truth word back into the model for another round of predictionã€‚

ðŸ˜Šï¼ŒThat actually is a strategy that is used by some of these models when they're trying to generate textã€‚

 but that's not used in the pretraining tasksã€‚ So this essentially all these predictions happen at one shot and that's itã€‚

 So unlike Elmoï¼Œ which is using language models which can technically generate text and give us probabilities of sentencesã€‚

 in Bt the entire focus is using this architectureã€‚

 we don't care about getting a probability if a sentence if all we want is powerful representation of the words in that sentence right so this pretraining objective is sufficient to make the model learn important linguistic properties if you just think about this task students mask their booksã€‚

To give a good prediction of this mask tokenï¼Œ I have to know that it's very likely that a verb will appear hereã€‚

 right in this particular context and also I have to know a bit of world knowledgeï¼Œ rightã€‚

 I have to know that there are only a certain subset of verbs that are appropriate to predict in this particular contextã€‚

 probably associated with students and books and schools and learning and so onã€‚ So yeahã€‚

 you can see intuitively how this objective functionã€‚

 very similarly to the original language modeling objective is trying to get the model to pick up important linguistic properties of the textã€‚

ðŸ˜Šï¼ŒWhat if the masked embeddings are not matching with the actual word embedding that was maskedï¼Ÿå—¯ã€‚Iã€‚

 I don't that questionã€‚ Maybe you can reword it and I will answer it after thatã€‚Okayã€‚

 so just to clarifyï¼Œ sameã€‚Trainingã€‚Procedureã€‚I guess I should say lossã€‚Asã€‚N Lmsã€‚

So we're still using the cross entropy lossï¼Œ rightï¼Œ We're minimizingã€‚The negative log likelihood ofã€‚

Of the maskedã€‚Tokensã€‚Sorryï¼Œ I should say theã€‚å‘ƒã€‚Of the ground truthã€‚Unmasked tokensã€‚so in this caseã€‚

 the negative log likelihood of opened hereã€‚å—¯ã€‚Yeahï¼Œ so all of the stuff you learned fromã€‚

Neural language models applies when you're training thisï¼Œ unlike neural with the maskã€‚

 like we did in the transformer language modelsï¼Œ rememberã€‚

 we had that mask that was preventing the model from attending over future tokens hereã€‚

 it's completely allowed to attend over future tokens because the whole point is that the input is actually it doesn't contain the information about what you're trying to predictã€‚

 So we're not cheating in this senseã€‚So when we're going from Elmo to Bertã€‚

We're making two fundamental changesã€‚Firstï¼Œ we're going from two unit directionalã€‚

L Ms to one masked L Mã€‚And secondï¼Œ we're going from recurrentã€‚Modelsã€‚To transformersã€‚And finallyã€‚

 thirdã€‚Birtã€‚Was pre trainededã€‚On a lot more dataã€‚So these are the critical changes when we're going from Elmo to Btã€‚

 so just to be completely clearã€‚If we take our same example from beforeï¼Œ which was what was itã€‚

 the movie is goodã€‚The movie is greatï¼Œ okayã€‚So we have the movie is greatã€‚

And say I have a pretrained masked language model that I've pre traineded on auto ton of dataã€‚

 Then the question isï¼Œ how do I use this in a downstream model right to get these contextualized word representationsã€‚

 and I can just feed this sentence into my pretrained mask language model without masking any of the words in the input Soã€‚

Just abstract away hereï¼Œ pre trainedã€‚Maskedã€‚LMã€‚å—¯ã€‚And this is going to give meã€‚My contextualizedã€‚

Token level vectorsã€‚Contextualizedã€‚Token vectorsã€‚In oneã€‚Pass through the modelã€‚

So I don't have to actuallyã€‚Feed it through two separate models and concatenate in in a mass language modelã€‚

 I just feed my downstream sentenceã€‚ This movie is great without masking any of the tokens and get the representations out of itã€‚

 So this is a much cleaner way of getting these contextualized representationsã€‚ðŸ˜Šï¼ŒOkayã€‚

 back to some questionsã€‚Apart from using two models verse one modelã€‚

 is there any other advantage of using mass language modeling So there are someã€‚

 if you look at the Bt paperï¼Œ there are some ablation experiments that compare the approach used in Elmo with the left or right prediction and right to left prediction concateninated together versus using a mask language modelã€‚

 I think they showed some gainsï¼Œ maybeï¼Œ but I know there's other work that shows that just simply scaling up the Elmo approach of training these two separate direction models on like the same scale data as Bert was trained on also reach a similar performanceã€‚

 but yeahï¼Œ it's mostly like cleanerï¼Œ I think to just have a single model to deal withã€‚

 and its I guess a more natural tasks where we don't care about actually generating text or predicting the next word or modeling the problemã€‚

ðŸ˜Šï¼ŒOf a sentenceï¼Œ which are all features of a unidirectional language model that we don't need if our whole goal is just transfer learningã€‚

 So this mask language modeling thing doesn't have those same use casesã€‚

 but it it does do exactly what we want for you knowï¼Œ learning a powerful text encoderã€‚ So yeahã€‚

 I would encourage you to check out the birdt paper for some more ablationsã€‚

 And I'll try and put some links to the other papers that I mentionedã€‚Okayï¼Œ so another questionã€‚

 why do prediction at allï¼Œ why not just use the current architecture to generate embeddingsï¼Ÿ

I'm not sure what you mean by why do prediction at allã€‚

 So the masked language model needs an objective to learn good parameters of the transformer right so if I did not have this prediction task here of predicting openedã€‚

 then the parameters of this model wouldn't learn any sort of linguistic properties right because I need something to adjust the models's parameters such that it's able to extract all this important information from the textã€‚

 So similarly to how in language models we're predicting the next word hereã€‚

 we're predicting the masked wordï¼Œ but they have the same effect of forcing the model to try and at least at some level understand the textã€‚

Are we learning the embeddings alongside the contextualized representationsã€‚

 even though they are not used hereï¼Œ Yeahï¼Œ so with all of these pretraining tasks with with mass language modeling and with language modeling with a large scale taskã€‚

 we start all our parameters from scratch the word embeddings and we train them on this huge data setã€‚

 So yeahï¼Œ we learn those and in the downstream task for Elmoã€‚

 we we basically just throw everything away except the contextualized embeddingsã€‚

 we're going to see what happens in Bertï¼Œ it's actually quite differentã€‚

 So maybe I'll move on to that now and I'll take more questions laterã€‚

 So the last thing I want to talk about before switching over to slides is how we actually use Bert in a downstream taskã€‚

Or how do we useã€‚So hereï¼Œ Bert is the first large scale pre trained mask language modelã€‚

 so I'm just using it as a proxy for a pre trained mask language modelã€‚For a downstream taskã€‚

So we talked briefly about this at a high level last timeã€‚

 but I wanted to be more specific this time now that we've covered the different objective functionsã€‚

 So remember in Elmoï¼Œ we do the transfer strictly at the word embedding levelï¼Œ rightã€‚

 we still as we saw inã€‚This figureï¼Œ we still have this entire downstream model here or the downstream model hereã€‚

 And all these parameters are trained from scratchã€‚In Btã€‚

 the pre training model and the downstream model are much more closely linkedã€‚ In factã€‚

 they're exactly the sameã€‚Pretrainedã€‚Architectureã€‚Is the sameã€‚As theã€‚Downstream modelã€‚

Or I should sayï¼Œ almost the sameã€‚Almostã€‚Okayï¼Œ so what does this meanã€‚

 Let's move forward and take our same example sentenceï¼Œ thisã€‚Movieã€‚Is ohï¼Œ sorryã€‚

 I want to add one more thing toã€‚ So in Bertã€‚We basically want toã€‚

So let's say our example is sentiment analysisã€‚One interesting thing that you might have noticed from the the Elmo examples is that the role of the downstream model is essentially to compose together all these pre trained embeddings into a single vector and then put a softm layer over that composed vectorã€‚

 rightï¼Œ Soã€‚ðŸ˜Šï¼ŒIn this exampleï¼Œ this was the composed vectorã€‚

 And here the final hidden state of this RN was the composed vectorï¼Œ butã€‚In Btï¼Œ what they doã€‚

 instead of forcing you to come up with this external composition function that is trained from scratch on the downstream dataã€‚

 they simply add a special token to the beginning of every sequence during both pretrain and a downstream taskã€‚

 So they're going add a special tokenã€‚ðŸ˜Šï¼ŒAndã€‚Beginningã€‚Of every sequenceã€‚And in Bertã€‚

 this is called the Cï¼Œ L S tokenã€‚Which I assume stands for classification or something like thatã€‚

So the role of this special CLS token is thatã€‚If for a downstream taskã€‚

 I want to do a classification problemï¼Œ I can simply just put my softmax layer over the token representation for the CS tokenã€‚

 and we know that in a transformer in a mask language modelã€‚

 every token representation is contextualized with every other token in this input sentenceã€‚

 And so the CS token representation contains some information about the entire rest of the sentenceã€‚

 so you can view it as kind of already doing the composition process that in the ElLmo caseã€‚

 we would expect a downstream model trained from scratch to be doingã€‚

 and this enables us to share the same architecture between pretraining and fine tuning without adding any external composition modules like an RNN or an elementwise average or so onã€‚

ðŸ˜Šï¼ŒSo now to make that more clearã€‚We are adding this C L S token to the beginning of every input sequenceã€‚

 And then we have this movie is greatã€‚And so just like the mask tokenï¼Œ this Cã€‚

 LS token is associated with a word embeddingã€‚ It hasï¼Œ you knowï¼Œ its own entry in the vocabularyã€‚

And so we do the normal thingã€‚ We have our word embesã€‚They get passed all through the pretrainedã€‚

Masked L Mã€‚And I get as outputï¼Œ I'm going need to move thisã€‚The contextualized embeddingsã€‚

So in this caseï¼Œ I haveã€‚One extra oneï¼Œ because I have this special Cï¼Œ L S tokenã€‚ So let me quicklyã€‚

Move this downã€‚And the important thing in Bert is that this token hereã€‚

 the contextualized CLS representationï¼Œ I'm just going to put a softm layer on top of thisã€‚

To predictã€‚Positiveã€‚So there's literally no downstream architecture here right we're using the exact same parameters of the mask language model to perform this compositionã€‚

 and we're just going to assume that all of their important information about the sentence regarding its sentiment will be squeezed into the CLS vector once I perform fine tuning soã€‚

Now that I've done thisï¼Œ I back propã€‚The error signalã€‚Fromã€‚The sentimentã€‚Classifierã€‚Throughã€‚Entireã€‚

Pretrainedã€‚Maskked L Mã€‚ So I'm actually adjusting the parameters of my mask language model in the downstream task using the signal that I get from this sentiment analysis downstream data setã€‚

å—¯ã€‚So this process is called fine tuningã€‚Okayï¼Œ so I hope the difference between this setup and the Elmo setup are clear hereã€‚

 there's no externalã€‚Downstream modelã€‚In factï¼Œ theã€‚The only thing we're addingã€‚Only new componentã€‚

Is a singleã€‚Soft max layerã€‚And this is used to predict the sentimentã€‚

And the second thing is that we're fine tuning this pretraining language modelï¼Œ rightã€‚

 We're not leaving it fixedï¼Œ as we do in in Elmoã€‚ So hopefully at this pointã€‚

 you have an understanding of how in this Bt case we're using the same exact model architecture for both the pre traininging task and the downstream taskã€‚

 And you have some idea of why this might be good alsoï¼Œ rightï¼Œ becauseã€‚

The pre training task gives us a very powerful composition function that's been trained overã€‚

 you knowï¼Œ billions and billions of wordsã€‚ðŸ˜Šï¼ŒWhyï¼Œ why would we want to actually sh function on our much smaller label data set as we do in ElLmoã€‚

 it makes a lot of sense to share these parameters across both pretraining and fine tuningã€‚

 and so these are the critical differences hereã€‚Okayï¼Œ so it looks like we have another questionã€‚

 Can you have a more complex downstream model in Bert or only the CLS Somax would workã€‚ Yeahã€‚

 that's a good questionã€‚ So you canï¼Œ in fact use the final layer representations in Bt just like you would do in Elmo and use them as word embeddings to some downstream modelã€‚

 but in practiceï¼Œ no one does this because you don't see any gains from doing this and yeahã€‚

 it turns out that these huge transformer models are good enough for most tasks that simply putting a softmax layer on top is enough to get great performanceã€‚

 So we'll look a little more at the results in a bitã€‚

 but I should also mention that the CS token here is useful for classification tasksã€‚

 but when we move on and we'll do this in the next lecture to other types of tasks like sequence labeling or question answering these otherã€‚

ðŸ˜Šï¼ŒRepresentations hereã€‚The other token representations are also useful for putting classifiers on topã€‚

å—¯ã€‚Okayï¼Œ so I think at this pointï¼Œ I will switch over to the slidesã€‚Oh my godã€‚

 it's already been 50 minutesã€‚Really bad at making these shortã€‚



![](img/c6f24929fd8fd39a2cd55342d10b650f_1.png)

Okayï¼Œ so yesã€‚Soï¼Œ hereã€‚

![](img/c6f24929fd8fd39a2cd55342d10b650f_3.png)

å—¯ã€‚This screenedã€‚

![](img/c6f24929fd8fd39a2cd55342d10b650f_5.png)

Okayï¼Œ so I want to talk more now about the specific implementation of BRTï¼Œ how it was trainedã€‚

 what data it was trained onã€‚ I saw theres a question above on training data as wellã€‚ And yeahã€‚

 basically give some more concrete details as to thisã€‚

 Another thing I want to talk about is the pretraining method for the C L S tokenã€‚

 although this has been shown after the Bt paper to be less importantã€‚

 But for those of you who actually read the birdt paperï¼Œ you might be wondering about thisã€‚Okayã€‚

 so we talked about this the problem with prior methods like ElLmoï¼Œ for instanceã€‚

 that they use only the current representation only modelsï¼Œ the prefix but not the futureã€‚

 remember that in language modelingï¼Œ the pre-training taskã€‚

 we need to have this directionality because otherwise we'll be cheatingã€‚

 predicting future words we're not going to be able to actually predict these good wellform probability distributionsã€‚

 but we don't care about thisï¼Œ if we're only interested in using representations out of the model for transfer learningã€‚

Okayï¼Œ and yeahï¼Œ the the problem with just naively porting over language modeling to a bidirectional setting where maybe you don't do the masking of future words is that you're cheatingã€‚

 So your model isn't going actually learn anything useful about the input rightã€‚

 it can just say that ohï¼Œ well I have this information already in my inputã€‚

 So let me just get the information of the next word and predict itã€‚

 So it's not actually going to learn anything usefulã€‚

 So this is the motivation again for the mask language models where we mask out some percentage of the input words and then predict the mask wordsã€‚

ðŸ˜Šï¼ŒSo one good question to ask here is how do I determine how many words in this pretraining task to mask out So in this exampleã€‚

 the man went to the mask to buy a mask of milkï¼Œ we can reasonably make some predictions about what these masked words would be right But imagine I masked out every word in the sentence except the word wentã€‚

Then the model is going to have no information to predict these mask tokensï¼Œ rightã€‚

 because everything is maskedã€‚ you have very little meaningful words to make these predictionsã€‚

And so you want to choose a mask percentage such that you have enough contextã€‚

 The model has enough contextï¼Œ such that it can make meaningful predictionsã€‚

 but we also want to be efficient right so you can think about if I have 500 words in my input and I only mask a single one of them I only get one prediction to backpro through my model and that might be very inefficient when I'm trying to train this over billions and billions of words if in every single exampleã€‚

 I only get an error signal from one wordã€‚ So you basically want to balance out the efficiency of training and how fast your training process is which you can increase by increase the mask percentage with the the context limitations if I mask out too many wordsã€‚

 then I can't effectively predict the ones that are maskedã€‚So in the bird paper they use 15%ã€‚

 so 15% of the tokens and the input are masked and predictedã€‚

They also do some like weird other replacements to the mask tokenã€‚

 so some of the time instead of replacing a word with a mask tokenã€‚

 they replace it with a random word or some of the time they just replace it with like the same wordã€‚

 so it has no effectã€‚And all of these thingsã€‚ So the bird paper was done by Google researchers who have access to tremendous amounts of computeã€‚

 So I assume they tried many variants of these strategies and found that this one achieved the best downstream performanceã€‚

ðŸ˜Šï¼Œå—¯ã€‚Okayï¼Œ so before I get to this questionï¼Œ there's a couple questions in the chat box nowã€‚

 What do you mean by backt from theï¼Œ I guess that means backdrop from the sentiment classifier to the entire pretrain model is it from the softm layer back to theã€‚

 so by sentiment classifierï¼Œ I'm referring to the softm layer because in the Bt settingã€‚

 the only parameters specific to the downstream task of sentiment analysis are the softm layerã€‚ Soã€‚

 yesï¼Œ those areï¼Œ that's what I meanã€‚ sorry for the confusionã€‚

How does model fusion work in case of Btï¼Œ so many state of the art models are variants of Btã€‚Yeahã€‚

 we'll get to the different variants of Bert a little later onã€‚

 I still haven't decided which ones to coverï¼Œ but realistically there have been many models that have come after Bert and the ones that have made impact are not because of any significant changes to the mass language modeling objective or to the model architectureã€‚

 but they are essentially just bigger models that have been trained on more dataã€‚

 So I'll probably assign a paper called Robertaï¼Œ which is a variant of Bt that actually simplifies many of the things in the Bt paper and trains on like much larger amount of training pretraining data and gets better performance across the boardã€‚

 so yeahï¼Œ there have been many variants of BRT proposedã€‚

 but I think the actual impact of those variants andã€‚ðŸ˜Šï¼ŒCon that they offered on top of Bt isã€‚

Generally lowï¼Œ except for just showing that more data and bigger modelsã€‚Okay so in the Bt paperã€‚

 if you read itï¼Œ you might have been a little confused about what this next sentence prediction task is and so remember that in Bt we have this CLS token and I haven't actually described how we pretrain the CLS token right do we actually want to mask out the CS token right it doesn't make sense and it doesn't offer us any new information to help our predictions of the masked wordsã€‚

 So in Bt they actually put a classifier a separate softm layer on top of just the CLS token which is responsible for some sequence level of predictionsã€‚

 not just token level of predictionsã€‚ so specifically they have this task of next sentence predictionã€‚

 So in Bt's pretraining each input is two sequences of text not necessarily sentences although in this slide it says sentencesã€‚

 two sequences of text that are taken from either the sameã€‚

Document and follow each other in the same documentã€‚

 Or they're just randomly selected two chunks from different documents or yeahã€‚

 different places of the same documentã€‚ So the task then for the classifier on top of the Cs vector is can you determine whether the second sequence actually follows the first sequence in the real text or whether the second sequence is completely different and from a completely different document or something like thatã€‚

 So in the example hereã€‚The man went to the storeï¼Œ he bought a gallon of milkã€‚

 these sentences might have occurred right next to each otherï¼Œ and so this is a positive class labelã€‚

 whereas the man went to the store and penguins are flightlessã€‚

 this second sentence is a random randomly sampled sentenceã€‚

 and so we would expect the classifier on top of the CLS token to predict that it's not the actual next sentenceã€‚

So in the Roberta paper which I mentioned beforeï¼Œ they actually removed this separate pretraining objective of next sentence predictionã€‚

 They kept the CLS tokenï¼Œ but they just didn't put anything special over it and it turns out that it doesn't actually it's not required to get the performanceã€‚

 So I would have gone into more detail about thisï¼Œ but it seems like the mask language modeling task is by far the most important thing and all these secondary objectives are not as impactfulã€‚

Okayï¼Œ so this slide just kind of summarizes the input to the original Bt modelã€‚

 and it's also a good summary ofï¼Œ againï¼Œ the inputs to a transformerã€‚ So we startã€‚

 we have this sequenceã€‚ starting again with a CS tokenã€‚ My dog is cuteã€‚ The separator tokenã€‚

 which isï¼Œ rememberï¼Œ now we have two sequences that are stuck togetherã€‚ He likes playingã€‚ðŸ˜Šã€‚

And note that this token hereï¼Œ the INGï¼Œ this is an instance of the subword encoding rightã€‚

 the stem play and this suffix INGã€‚ So we talk briefly about this before of doing this to reduce the incidence of like unseen words or having a way of modeling words that we at test time that we don't see at training timeã€‚

So we have our token embeddings for each of these tokens in the sequenceã€‚

 these are analogous to our word embeddingsï¼Œ we also have the position embeddings which we know are necessary in transformers and in BRT we have these segment embeddings that are supposed to help the model reason about the two different sequences in the same input to help with the next sentence predictionã€‚

Okayï¼Œ and so all of these representations are summed upï¼Œ these different embeddingsã€‚

 and only then are they passed into the transformer for mask language modelingã€‚ðŸ˜Šï¼ŒSo in BERTã€‚

 they use the word peace method for achieving the subword segmentationã€‚

 so they have a vocabulary of 30ï¼Œ000 of these words and sub wordss like INGã€‚Okayï¼Œ and the modelã€‚

 we've already seen thisã€‚ it's a transformerï¼Œ It has multiheaded self attentionã€‚

 the feed forward layersï¼Œ this block we've seen beforeã€‚ So nothing new modeling wiseï¼Œ which yeahã€‚

 is pretty interestingã€‚ The transformer originally proposed for machine translation then port it over to language modeling and mask language modeling and has had success in both of these areasã€‚

ðŸ˜Šï¼ŒSo Bert was trained on Wikipedia and also this book corpusã€‚

 which is a data set of unpublished novelsï¼Œ so the total combined number of words in these dataset sets is over 3 billion which is a lot or at least was a lot at that time nowadays we have models like the ones from open AI that are trained on like hundreds of billions of words these numbers might seem small in retrospectã€‚

 but this was a huge number for the timeï¼Œ which wasã€‚Againã€‚

 like literally last year or like one and a half years agoã€‚ So things move really fast in this fieldã€‚

And they trained this on TUsï¼Œ so not GPUus for four days I think at the time people did the at the time Google did this there were people on Twitter I remember who were estimating how much this would cost like a normal researcher to run on something like AWSã€‚

 and it was like training on a four GPU box for six months or something like that would be the equivalent of thisã€‚

 So really something that benefited heavily from the amount of compute that Google hasã€‚

And they trained two variants of their modelï¼Œ so one was a 12 layer model that had at each layerã€‚

 12 heads and the dimensionality of the hidden states the thing that we get at each the output of each of these selfat blocks of 768 and they trained out large modelã€‚

 Bt largeï¼Œ which was 24 layersï¼Œ more headsï¼Œ bigger hidden dimensionalityï¼Œ and that one worked betterã€‚

So yeahï¼Œ I guess I will I talk briefly about how we fine tune these models using the CS token for other tasksã€‚

 like question answering and sequence labeling we do very similar thingsã€‚

 but use not we don't use a CS tokenã€‚ we use the other token representations and we'll spend a lot of next lecture talking about those methodsã€‚

 So I won't cover these slides hereã€‚ I just wanted to shift over to the results to show how much of an improvement this Bt model was for NLP as a wholeã€‚

 So the glue benchmark as a bunch of these sentence level tasks that are usually I think all of these are classification tasksã€‚

 So this one is a textual entailment data setã€‚ This I think is a paraphrase data setã€‚

 SST as a sentiment analysis data set and so onã€‚ So there's like theseã€‚ðŸ˜Šã€‚

8ight or so tasks that are you know pretty well known within NLPã€‚

 and we had these previous numbers on all of these tasks were using unidirectional language models of open AIs GP GPT was trained on a lot of text and it achieved an average score of about 75 across all of these different eight tasks and a model with ElLMmo in it was a little worse at about 71ã€‚

But you can see that Bertï¼Œ especially Bert largegeã€‚

 really just achieved the state of the art on all of these tasks and had a pretty huge gain in terms of the average scoreã€‚

 So this was a really impactful result that has basically changed the the course of research in N LP since its releaseã€‚

ðŸ˜Šï¼ŒYeahï¼Œ and so this plot from the original bird paperã€‚

 or at least from the slide really shows the impact of model size on the downstream performanceã€‚

 So how effective this model will have to beã€‚How effective this model will be for transfer learning so you can see that as you increase the number of parameters in your transformerã€‚

 your accuracy on this downstreamï¼Œ these two downstream tasks goes up right and importantlyã€‚

 it's not like these curves have plateaued right they're still going up this plot was what the Google researchers were able to do at that timeã€‚

 but it shows that by simply just increasing the model size and also the training data we will get better and better performance and the insight from this plot has been borne out by subsequent results showing just bigger and bigger models give you larger improvementsã€‚

So for your projectsï¼Œ many of you might be interested in working on languages that are not Englishã€‚

 so Bert was trained specifically on English Wikipedia and this book corpus was also completely English but soon after the release of the BtRT model of the Google researchers also released multilingual Bt which is very interesting it's a single model that's trained on the union of 104 different language Wikipedias so they did not train a separate model for each languageã€‚

 they instead just basically concatenated all of these different Wikipedias together and encoded them using the same vocabulary so like subwords from different languages are all kind of in the same vocabulary and they trained it using this mass language modeling objective and it has some very interestingã€‚

ðŸ˜Šï¼ŒProperties like you can doã€‚You you can solve so actually it also at the time set the state of the art on many tasksã€‚

 even though it was trained in this kind of strange mannerã€‚

 but you can also imagine that training in this way allows the model to share information across different languagesã€‚

 which is very useful if you have a language whose Wikipedia might haveã€‚

 you know only like a couple thousand articles versus English where you probably have millions of articles having this transfer between different languages could be very importantã€‚

ðŸ˜Šï¼ŒYeahã€‚So this is just a pictureã€‚ and this picture is outdatedï¼Œ by the wayã€‚

 although I think it was made maybe six months or so agoã€‚

 that shows kind of how research in this area has developed since Bt was releasedã€‚

 So we started out with theseelmo like modelsã€‚ And there were other models that kind of followed this train unit unidirectional language modelsã€‚

 But you can see just the number of offshoots from Bt here is tremendousï¼Œ rightã€‚

 So there's a lot of Mppet like names following Elmo and Btã€‚ there's two different earningsniesã€‚

 There's a groverã€‚ there's probably other things that ohï¼Œ there's definitely a kermit So yeahã€‚

 is crazy how just this simple paradigm of mass language modeling has been there's been so many variance of it since thenã€‚

 So this whole line of workï¼Œ video Bt V Bt visualã€‚ðŸ˜Šã€‚

Images together like maybe images and their captionsã€‚

 there's work that's trying to focus on modeling longer sequences so that's in the realm of XL net there's work in the line of Robertaã€‚

 which is just training for more time bigger modelï¼Œ more data and so onã€‚

 So a lot of interesting things will'll cover some of these in future lecturesã€‚ðŸ˜Šï¼ŒOkayã€‚

 so I guess at this pointï¼Œ let me look more at the questions hereã€‚

 Are we replacing mask words with random words to reduce overconfidenceã€‚

 I think that's unlikely because the masked words it's just our random processã€‚

 like which words are masked at every batchã€‚ So it's very unlikely that the modelã€‚

 even though it's huge is going to be able to overfit the training data and be incredibly confident on particular predictionsã€‚

 Andã€‚Yeahï¼Œ I think replacing it with random words just gives it I guessã€‚

 a little different objective where the model also has to be aware of when a word doesn't make sense in a particular contextã€‚

 So I would wager that was more likely the reason the intuitive reason behind itã€‚

 It's very hard with these huge models to judge if your intuition actually matches up with the the results that you're gettingã€‚

 thoughã€‚Is there a constraint on what kind of words are maskedã€‚ That's a great questionã€‚

 going back to this slideã€‚ One of these Erniesï¼Œ I think this one did experiment with looking at different like parts of speech and masking them out in particularã€‚

 they found that if you just mask out named entities like names of places or people or objects or whateverã€‚

 you get better performance on tasks that are entitycentricã€‚

 there have since been works that explore masking out contiguous sequences of textã€‚

 So there's things like span Bt that do thisã€‚ And that seems to help also on tasks that deal with like phrasesã€‚

 But yeahï¼Œ the original bird paperï¼Œ just randomly masks wordsã€‚ðŸ˜Šï¼ŒOkayã€‚

 so I wanted to conclude with some takeaway questionsã€‚

 So these slides were actually shared with me by the first author of the birdir paperã€‚

 Jacob Devvelin andã€‚ðŸ˜Šï¼ŒAfter I've described all of thisï¼Œ I thinkã€‚

You might be thinking like why did no one ever even do this before right it seems quite obvious that these kinds of pretraining tasks do encode a lot of knowledge about Liby forcing our downstream models to learn everything about linguistics from just a very small pretraining or sorry downstream data and really the answer is that the hardware improved to a point where we could train huge scale models and model size has made a huge impact in in this field so in 2013ã€‚

 if I wanted to train a two layerï¼Œ very small LSTMã€‚

 I might have had to train it for eight hours to get this result and now we're on to something like BurRT which has 24 layers of huge hidden states doing this expensive quadratic self-ten computation at every layerã€‚

ðŸ˜Šï¼ŒAnd the fact that they can train this in four days on their TPU pods is pretty insane and you know even since the release of BRTã€‚

 we've had better and better GPUs released and it's become more feasible for even academic researchers to build these models and train them in a reasonable amount of time soã€‚

Yeahï¼Œ that that is basically the main reason we've been driven by advances in GPUs and TPUs toã€‚

 to get to this point where we can scaleã€‚

![](img/c6f24929fd8fd39a2cd55342d10b650f_7.png)

Okayï¼Œ so I guess I want to end hereã€‚ And yeahï¼Œ if you have any other questionsï¼Œ ohï¼Œ ohï¼Œ sorryã€‚

 there is another questionã€‚ How can you compare your variant of Btã€‚To Bertã€‚

 if you don't have the sameï¼Œ this is a great questionã€‚ So theï¼Œ sorryï¼Œ the question isã€‚

 if I make a new Bt and I don't have access to these same TPU pods as Googleã€‚

 how do I compare my variant to theirs and make sure thatã€‚

If we scale it up to the same data set and model size that our model is actually better than theirsã€‚

Soã€‚In the research communityï¼Œ people generally start from the pretrained BRT model and just fine tune it or add an additional component on top of it and then fine tune itã€‚

 but they share a lot of parameters from the pretrain the models like Robertaã€‚

 which train on more dataã€‚ Those were done by industry labs with access to similar resourcesã€‚

 So Roberta was done by Facebookã€‚ They trained it on like hundreds of GPUsï¼Œ not TUsã€‚

 but basically same effectã€‚ you can scale up to insane data sets and model sizesã€‚

Yeah this question is very important for academic researchers because even currently we cannot compete at this scale with the industry labs and so it's kind of resulted in this situation where researchers both in academia and at smaller companies that don't have access to these resources just wait for pre-train models to be released by labs like Google or open AI or Facebook and then find clever ways to use them but yeahã€‚

 hopefully that changes in the future with cheaper and more accessible computational resources and also maybe we'll figure out ways to train these models in such a way that they're not so data hungryung and maybe are more perimeter efficient and so we'll talk on in the semester but yeahã€‚

 I guess that's it for this lecture just the fundamentals of Bert and next time we'll talk aboutã€‚ðŸ˜Šã€‚

Actually using BERT to solve many different downstream tasksã€‚

 so you'll get exposure to a broader variety of tasks other than just like language modeling and sentiment analysis that we've seen at this pointã€‚

So yeahï¼Œ you can look forward to thatã€‚

![](img/c6f24929fd8fd39a2cd55342d10b650f_9.png)