# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é©¬è¨è¯¸å¡CS685 ï½œ è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶(2020Â·å®Œæ•´ç‰ˆ) - P6ï¼šL4- åœ¨ PyTorch ä¸­å®ç°ç¥ç»è¯­è¨€æ¨¡å‹ - ShowMeAI - BV1BL411t7RV

Hey everyoneï¼Œ so today we're going to spend most of our time actually implementing a neural language model in Pytorch and we're going to be switching shortly from this iPad to a Google collab notebook but before we do that there's one more important thing that we need to cover before you're ready to actually understand what's going on when we implement this and that specifically is the loss function that's used in neural language models and more generally for neural classification problems and that is the cross entropy lossã€‚

So againï¼Œ this is usedã€‚Nlms neural language modelsã€‚As wellã€‚Asã€‚Otherã€‚Classification tasksã€‚

So remember we are treating in the neural language modeling setting this task as a classification problem right given a prefixã€‚

 we're asking the model to predict which word out of the set of all word types in our vocabulary follows this prefix right so it's a classification problem where the labelL space is equal to the size of the vocabulary so it's a large scale classification problem and the loss function that we've seen that we saw previously the square loss is not suitable for such a large scale classification problem and so we're going to review a better one the cross entropy loss before jumping over to the implementationã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so quick review from last timeï¼Œ what is a loss functionï¼ŸIntuitivelyã€‚Tells usã€‚How badã€‚A modelã€‚

Is doingã€‚At predictingã€‚The training dataã€‚Right so in the neural language modeling settingã€‚

 if I have a model that gets his input the prefix students open there and is predicting the word lamps with a high probabilityã€‚

 we want a loss function that will assign a high loss to this instance because the models clearly not predicting the training labels if in the training set we saw a lot of examples of students open their books or their laptops or so onã€‚

Okayï¼Œ so in N LMsï¼Œ specifically how badã€‚Is the modelã€‚At predictreingã€‚The next wordã€‚Okayã€‚

 so let us whoops dive into this by looking again at the example that we've been following for the first few videos hereã€‚

Okayï¼Œ so assumeã€‚We haveã€‚A training exampleã€‚Studentsã€‚Openedã€‚There this is the prefixã€‚

 and it's paired with booksã€‚Okayï¼Œ so we have our inputsã€‚here and our output booksã€‚

So the model takes this as inputã€‚Ohoopsã€‚å‘ƒã€‚And thisï¼Œ remember isã€‚Desiredã€‚Moelã€‚Predictionã€‚

So remember that our model is actually producing a probability distribution over the whole vocabularyã€‚

 so what we really want is a predicted distribution in which the probability associated with the word books given this prefix is very highã€‚

ğŸ˜Šï¼ŒSo to make that more concreteï¼Œ our model producesã€‚Givenã€‚And we want toã€‚Maximizeã€‚This probabilityã€‚Oã€‚

So we know that we want to maximize the probability of the correct continuationã€‚

 given the correct continuationï¼Œ meaning the thing that appears in our training dataã€‚

 given our modelã€‚So remember that a loss function is a measure of how bad the model's current prediction isã€‚

 so we can rewrite this as we wantã€‚To minimize and rememberã€‚

 we talked about gradient descent in the concept when we talked about gradient descentã€‚

 we talked about minimizing a loss functionï¼Œ rightã€‚

 we use gradient descent to go in the direction of the negative gradient and find some sort of minimum of the loss function so we can easily turn this into a loss functionã€‚

 We want to minimize negativeã€‚ğŸ˜Šï¼ŒLog probabilityã€‚So remember again that we work in log probabilitiesã€‚

 not raw probabilities due to many reasonsï¼Œ practically speaking due to numerical underflowã€‚

 we don't want to deal with these super tiny numbersã€‚å—¯ã€‚And in our exampleã€‚

 we want to minimize a negative log probability of booksã€‚

 so our loss function is going to look like negative log P of booksã€‚Given students open thereã€‚

So this is pretty straightforwardã€‚But before we continue hereã€‚

 it's important to understand why this is called a cross entropy lossã€‚

 So many of you are probably familiar from previous classesã€‚

 machine learning classes or so on with the concept of a cross entropy loss right So why is this calledã€‚

ğŸ˜Šï¼ŒThe cross interview lossã€‚And remember that the cross entropy loss intuitively gives us a measure of the distance between two probability distributionsã€‚

 so if my modelï¼Œ let's say my model gets maybe I'll draw the model to make things more clear so we have studentsã€‚

ğŸ˜Šï¼ŒOpenedã€‚Thereï¼Œ remember that I have word embeddings for all these thingsã€‚

 I combine them into single vectorï¼Œ and I produce a probability distributionï¼Œ rightï¼Ÿ

 So what does that actually look likeã€‚If we say all of our words areã€‚Booksï¼Œ laptoppsï¼Œ I don't knowã€‚

 Maybe you can make up some other wordsï¼Œ planesï¼Œ lampsï¼Œ etc ceteraã€‚

 So all the words in our vocabularyï¼Œ rightï¼Œ And let's say this is what our model is doingã€‚

 So maybe it assigns some probability to booksã€‚ It assigns some probability to laptopsã€‚

 some probability to planesï¼Œ a very high probability to lamps and so onã€‚ğŸ˜Šï¼ŒSo in our training dataã€‚

 we observe thatï¼Œ of courseï¼Œ the correct continuation of this is booksã€‚

So we treat our training distributionï¼Œ so maybe I willã€‚This is a modelã€‚ and let's say this colorã€‚

Is used for the model's predicted probability distributionã€‚So next we have our dataã€‚And in our dataã€‚

 we observe studentsã€‚Openedã€‚Their booksã€‚So in this specific exampleã€‚

 there is no ambiguity as to what follows students open there right so we consider this as a one hot distribution remember we talked about one hot vectors as a way of encoding words in an earlier videoã€‚

So we can have our same distribution hereï¼Œ booksã€‚Ptopsã€‚Planesã€‚Lampsã€‚

And this model is going to assign all of the probabilityã€‚ So P worksã€‚Gä»¬ã€‚å—¯ã€‚There oopsã€‚Andã€‚

And these are all zeroï¼Œ so let's make this clearï¼Œ this is what we observe in our dataã€‚

The what color I not used of thisã€‚So intuitivelyï¼Œ we have a model's predicted probability distributionã€‚

 and we have the ground truth probability distributionï¼Œ which is this one hot vectorï¼Œ rightã€‚

 essentially with one at the ground truth continuationã€‚

 and we want to make the models predicted distribution as close to the ground truth distribution as possibleã€‚

 So remember that cross entropy loss is defined asã€‚ğŸ˜Šï¼ŒWe all go down hereã€‚Sorryï¼Œ the cross entropyã€‚

Of twoã€‚Distributionsã€‚Let's call them P And Qã€‚This essentially again quantifiesã€‚Distanceã€‚Betweenã€‚

The distributionsã€‚So if the cross entropy is lowï¼Œ then the distributions are fairly close togetherã€‚

 if the cross entropy is highï¼Œ that means that they're pretty far apartã€‚And the equation for this isã€‚

So in our neural language model settingï¼Œ remember that the differentã€‚

Classes that we have are the word types in our vocabularyã€‚Okayã€‚

 and so these are the conditional probabilities that written in short form hereã€‚

 so this is we can treat as our ground truth distributionï¼Œ one for booksã€‚0ï¼Œ everyã€‚Otherã€‚Word typeã€‚

 sorryï¼Œ that should be 0 for every other word typeã€‚ And this is our predictedã€‚Conditionalã€‚Gsã€‚

So this is the definition of cross entropy lossï¼Œ which we have ported over to our neural language model setting where P is the ground truth distribution and Q is the predicted distributionã€‚

 so maybe I'll make that more clear using the same colors from aboveã€‚

 so we used Q sorry P with this colorã€‚Qã€‚ã„ã‹ã€‚Okayã€‚So let's take a lookã€‚

 go back and take a look at our loss function that we came up with from beforeã€‚

 which was just followed naturally rightï¼Œ we want to minimize the negative log probability of the word that appears in our training data setã€‚

 books given this prefix if you look at this definition of cross entropy hereã€‚

 you will see that since the P term is zero for every word other than booksã€‚

 this simply just reduces to exactly what we had beforeã€‚Given the prefixã€‚

So hopefully this makes sense since this summation isï¼Œ there's only one non zero termã€‚

 and that's when this W equals booksã€‚Okayï¼Œ so that's a bit of intuition onã€‚

What kind of loss function we're using for neural language modelsã€‚

 but the important intuition is that you know a loss function in our case is a defining a measure of how close the model's predicted probability distribution is to what we've seen in our training dataã€‚

 later on in the semester we'll talk about methods where this here is not a one hot vector so you actually have to use the full definition of cross entropy there are many non-zero termsã€‚

 but for now we're going to stick with the simple caseã€‚ğŸ˜Šï¼ŒOkayã€‚

 so that's all I wanted to say about the type of loss function we're using in neural language modeling and now let's quickly switch over here to the collab notebook that I want to shareã€‚

 so give me a second to do thoseã€‚ğŸ˜Šã€‚

![](img/3aa8ae5afa5ccba3efd817626d99d2d3_1.png)

![](img/3aa8ae5afa5ccba3efd817626d99d2d3_2.png)

Okayã€‚Yesã€‚That's too bigã€‚å—¯ã€‚Okayï¼Œ so now we're going to switch over to the kind of live coding exercise where I will walk you through the implementation of a very simple neural language model on a toy data set of basically I guess like four prefixes and yeahã€‚

 I highly recommend for this to be productive that you open up a collab notebook of your own while I'm doing this and follow along with what I'm doingã€‚

 there are a number of interesting places where you can branch off of the code that I'm writing and implement more complicated networks or training optimizers or architectures or so on so I think you'll learn a lot if you if you code this up alongã€‚

With with me when I'm doing itã€‚Okayï¼Œ so I'm going to click new notebook hereã€‚Uã€‚Okayã€‚

 this is going slower than I expectedï¼Œ All rightï¼Œ let's name this Cs 685ã€‚Nlmã€‚

So let's start by implementing the fixed window feed for neural network that we've seen in a previous videoã€‚

 so to begin I'll come up with some sample sentences so let's say my sentences are I don't knowã€‚

 we should make them short so maybe likeï¼ŒUBsï¼Œ sheepã€‚Alice runs fastã€‚Cs 685 is funã€‚

 hopefully that's true for some of you or all of youã€‚I love Liã€‚Okayï¼Œ so we have these four sentencesã€‚

 They're all three words longã€‚ so we're going toã€‚Let's say do a fixed window neural language model where our prefix size is twoã€‚

 so we're going to take the first two words and try and predict the third wordã€‚

So maybe I'll write some stuff down in the commentsï¼Œ so given the first two words of each sentenceã€‚

We will try to predict theã€‚OopsThird word using a fixed window NlMã€‚ So this is our goalã€‚

But before we do thatï¼Œ we have to start by doing something that you've done in homework 0ã€‚

 which is tokenize the inputã€‚ So before we startã€‚Any fancy modelingã€‚

 we have to tokenize our input and the second thing we have to do is convert the words to indicesã€‚

Remember because our model takes as input sequences of indices rather than sequences of textã€‚

 so we need to do these two things before proceeding any furtherã€‚ğŸ˜Šï¼ŒOkayï¼Œ so let's do both of theseã€‚

I think the screen is still visibleã€‚ Oh maybe I should increase the font sizeã€‚Ofã€‚Okayã€‚

 whatever probably hopefully it's readableã€‚Okayï¼Œ so let's start by defining an empty vocabulary so this is going toã€‚

Map from word type to indexï¼Œ so every single unique word type in our vocabulary will be assigned a separate indexã€‚

 and then we'll have a list of inputs that we're going to store so it's going to be just like the sentences except each of the elements in this list is going to be a sequence of indices rather than a stringã€‚

Soã€‚Uã€‚Sttores an indexified version of each sentenceã€‚Okayã€‚

 so a simple way to do this is compute the vocabulary while converting the sentences to indices on the fly so we can loop through our sentencesã€‚

Let's say we make a variable forï¼Œ I don't knowï¼Œ sent indexesã€‚

So these are going to be the mappings for the current sentenceï¼Œ we're going to split on white spaceã€‚

 so this is something you saw in homework zeroã€‚Tokenize withã€‚White space againã€‚

 not the best tokenizer you a lot of you posted on piazza about things associated with you know like new line characters or punctuation being included in the word types and there are much better tokenizersã€‚

 but you know this is a simple toy exampleï¼Œ there are onlyã€‚

 I mean white space tokenization is exactly what we want in this caseã€‚All rightã€‚

 so we're going to loop through the wordsã€‚And for every wordã€‚

 we're first going to check if that word is in the vocabularyã€‚I'm sorryrryã€‚

 if the word is not in the vocabularyã€‚ So if the word is not in the vocabularyã€‚

 we want to add it to the vocabularyï¼Œ and we also want to assign it to a unique indexã€‚

 So this line of code assigns it to the current size of the vocabularyã€‚

 So it ensures that every new word we add we're basically incrementing the the index that we'reã€‚

So the wholeã€‚Point of this is to add newer types to the vocabulary and pair them up with indexes and finally we are going to append the index associated with the current word that we're at to the sent IDX's variableã€‚

Soã€‚å—¯ã€‚VabSo this is going to if we're at the word Bobã€‚

 it's going to add a mapping in the vocab dictionary from Bob to0 and then it's going to append a0 to this list that I've created here So once I've looped through all the words for this first sentence I probably have012 because these are the first three words in my vocabularyã€‚

ğŸ˜Šï¼ŒActuallyï¼Œ I've realized that I haven't even shared any words in any of these sentencesã€‚

 so I guess there's 12 differentã€‚Wordsï¼Œ maybe we'll change thatã€‚ so I'll make it Alice is fastã€‚

 So now there's two occurrences if the word isã€‚å—¯ã€‚All rightã€‚

 and once I'm done looping through all of the words in this particular sentenceã€‚

 I'm going to append the whole sequenceï¼Œ the sent IDX's list to my inputs listã€‚Okayã€‚

 so that was a lot of stuffã€‚ let's see what all of this looks likeã€‚

 so first we can look at our vocabularyã€‚å—¯ã€‚We need to read some of these lines So as a tipã€‚

 if you want to a shortcut to run a coab cellï¼Œ you can type shift enterã€‚

 at least that's what it is on my Mac and we see that every word type in our vocabulary Bob likes sheep Alice runs fast is blahã€‚

 blah blah is assigned to a particular index in the vocabulary and the word is only appears once right because there are two tokens of is in our data setã€‚

 but only one word type associated with isã€‚Let's also take a look just to be clear as to what the inputs object looks likeã€‚

So you see that it's a list of lists and each list contains a bunch of indices right rather than actual wordsã€‚

 so we see here that ohï¼Œ I guess I have to run this all again to updateã€‚Yeahã€‚

 so we see the first sentence Bob likes sheep is mapped to 012ï¼Œ the second one Alice is fastï¼Œ 345ã€‚

 the third oneï¼Œ CS685 is fun is 647ï¼Œ so note that the index associated with is occurs in two of these examplesã€‚

Okayï¼Œ so we're almost done with our preprocessing stuff and we're almost on to more fun thingsã€‚

 But before we can actually feed this list of lists into our pytorrch modelsã€‚

 we need to convert them into pytorch tensor objects so this is also something you saw in homework zeroã€‚

 we use the long tensor object for indices So the embedding module in pytorrch expects long tensor indices in the form of long tensorsã€‚

 So it's very easy to convert these thingsï¼Œ we can simply just cast them into long tensorsã€‚

 I'm mean in import torch because I haven't done that yetã€‚ğŸ˜Šï¼Œå‘ƒã€‚Okayï¼Œ and let's just sayã€‚Oh rightã€‚

 so the other thing I want to doï¼Œ so okayï¼Œ two thingsã€‚Oneï¼Œ convert to long tensorsã€‚And twoã€‚

 define inputs and outputsã€‚ So right nowï¼Œ these are just sequencesï¼Œ rightã€‚

 So I actually want to split them up into given Bob likes the inputï¼Œ predict sheep the next wordã€‚

So let's do that first we'll call our inputs the prefixesã€‚

 so we know we can get prefixes by simply just chopping off the last element of each of these listsã€‚

So let's say sent and this slicing in Pythonï¼Œ if you're not familiar with this notationã€‚

 you should make sure to understand itï¼Œ but thisã€‚Right here is just indexing everything in thisã€‚

This first sequence here except for the last the negative one elementã€‚

And I'm going to just loop over all of the sentences in my inputã€‚And this is a list comprehensionã€‚

 So againï¼Œ for those of you who aren't familiar with Python or aren't super familiar with itã€‚

 you you should read read about things like list compre list comprehensions and array slicing and so onã€‚

 they're very useful concepts in generalã€‚ Okayï¼Œ so similarlyã€‚

 I can create another list with just the labelã€‚ So this is the the last word in each sequenceã€‚ğŸ˜Šã€‚

Like thisï¼ŸSo now just to show you what I did if I print the prefixes and the labelsã€‚

We so this first thing here is a prefixesï¼Œ so note that they are just the first two elements of each list chopped with the last element chopped off and the labels are just a sequence of the last element from each list so this is exactly what we want in our language modeling setupã€‚

Okayï¼Œ so againï¼Œ to get these into long tensor format is very simpleï¼Œ all I do is wrap thisã€‚

Long tensor thing around each oneã€‚And now let's say I print prefixesã€‚

You'll see that I have this tensor object and contains exact same dataã€‚

 but I can now do all these pytorch operations to figure out what size this is and so on so now we're ready to start building up our model we have our data and input output format in the data structure this tensor thing that Pytorch expects so let's move on to the next cell where we're actually going to define the networkã€‚

Onto defining the networkã€‚This is where most of the fun stuff happens and before we start let's import just forã€‚

It easier for me to type things with shorthand and so the first thing we're going to do is make a class for our neural language modelã€‚

And so all neural networks when youã€‚Build them in Pytorchã€‚

 have to inherit from this NN dot module classã€‚Soï¼Œ yeahã€‚Okayã€‚

 so there's two things that you need to doã€‚If you're going to use Pytorrch to make a neural networkã€‚

 so the first thing is write the init functionã€‚This initializes all the parametersã€‚Of the networkã€‚

 So we'll get to this in a bitã€‚ But rememberï¼Œ we've spent quite some time now in the previous videos identifying the parameters of various models and thenã€‚

 of courseï¼Œ computing the partial derivatives of our loss with respect to those parametersã€‚

 The second thing we need to do is a forward functionã€‚ğŸ˜Šã€‚

So this defines the forward propagation computationsã€‚

So this is what we went through before looking at backpro right how you go from the embeddings to the final prediction of the probability distribution and we did this remember through a bunch of linear layers and softm layers and so on so that's it you just have two things to do you might be wondering what about the backpro right we spent all this time learning about how we use backpro and gradient and descent to train our models don't we have to spend time you know actually implementing those derivatives and this is the really nice thing about Pytorrch is that you don't and it computes all the derivatives for you and just one function call so we'll get to that a little later but it's you know incredibly useful you can implement super complicated networks without worrying at all aboutã€‚

ğŸ˜Šï¼Œmanuallyally deriving the gradientã€‚Okayï¼Œ so let's start with the in functionã€‚å—¯ã€‚

So there's a couple of things that in our fixed window neural language model we want to define So if you think about the hyperparameters of this model the things that we want to define as the people who are making this network and we don't want the model to learn it itself these are things like the dimensionality of the word embeddings the dimensionality of the hidden state so this is remember when we concatenate all the embeddings togetherã€‚

 we then project that to a lower dimensional space using a linear layer so this is defining the size of that hidden space we may also want to define our window size for this model which we've already looked at in our training data is too but note that in a standard implementation you won't be making like static training examples like this youã€‚

There be kind of incrementally constructing the window while scanning from left through left to right through a larger documentã€‚

 This is just for an example to make the preproces code and the training loop not super complicatedã€‚

Okayï¼Œ and the final thing we might want to do is tell the model how big the vocabulary is right it needs to know this for both the embedding parameter initializationã€‚

 like how many embeddings does it need to create as well as the softm layer rightã€‚

 how many word types do I need to predict probabilities forï¼ŸğŸ˜Šï¼ŒAl rightã€‚

 so the first thing I'm going to do is initialize the super class of thisã€‚

 the NN dot module and and anything that's higher up the base module class and then I can proceed withã€‚

Creating parameters that are specific to my networkã€‚ So I know that thereã€‚ğŸ¼3ã€‚

 basically three types of parameters in this modelã€‚ So the first is the embeddingsã€‚

 and we looked at this in homework 0ï¼Œ rightï¼Œ use this an N dot embedding object and it takes as argumentsã€‚

 I think it shouldã€‚Tell me what the arguments areã€‚Okayï¼Œ maybe notã€‚

 but the arguments are the number of word types in the vocabularyã€‚

 which we are passing in asL vocab and the dimensionality of the embeddingsã€‚

 which we're saying is a de embeddingã€‚Okayï¼Œ next up we have our hidden projection right so we're going from the concatenated version of these embeddings to the hidden layerã€‚

 so we'll make a matrix for this or we're actually going to use Pytorrch is built in NN dot linearã€‚

 which does this hidden projectionï¼Œ which is name it W slash Hiã€‚ğŸ˜Šã€‚

So we have an NN dot linear and this the arguments here are essentially specifying the shape of the projection matrixã€‚

 so we know in our case that we have thebedding time embedding dimensionality times the window size because that's how big that concatenated representation is going to be and we're going to project that to the hidden dimensionalityã€‚

So this thing here is the size of the concateninated representationã€‚Maybe I willã€‚Concatenatedã€‚

Ebeddingsã€‚ğŸ¼To hiddenï¼Œ That's the purpose of this linear layerã€‚

 And the other thing we have is hidden to output probability distribution over vocabã€‚Okayã€‚

 so we can call that selfã€‚w sub outï¼Œ it's another linear layer and goes from D hidden to the vocabulary sizeã€‚

Okayï¼Œ so this in our extremely simple neural language model is what we're going toã€‚

 these are the parametersï¼Œ currently we haven't applied any nonlinearitiesã€‚

 we could do that and maybe we can do that in our forward functionã€‚Okayã€‚å—¯ã€‚Greatã€‚

 so let's move on then to the forward functionã€‚And the forward arguments always take an inputã€‚

 which is in our case going to correspond to the prefixesã€‚

 So each input will be a batch of prefixes in our caseï¼Œ since we only have four prefixesã€‚

 we're just going to consider a batch of all four of them in practiceï¼Œ of courseã€‚

 you would have manyï¼Œ many more sentences and prefixesã€‚

 and so your batches would contain different prefixes per batchã€‚ Soã€‚Let's go back down hereã€‚

In this caseï¼Œ fourã€‚So that's what is going to be in this inputã€‚

 So you can think of the input to this forward function as the prefixes variable that weã€‚

We made up hereã€‚All rightï¼Œ so I just want to make sure this is recording properlyã€‚

 okay looks like it isã€‚å—¯ã€‚Let's go ahead and startã€‚ So this is fairly straightforwardã€‚

 What we're going to do is firstï¼Œ let's just get some statistics about the inputã€‚Prefixã€‚

Length equals input dot sizeã€‚ So againï¼Œ these are the dimensionã€‚ This is the dimensionality of theã€‚

The prefixes tensor that's going to be input here and now let's just go ahead and get embeddings for this input We can do this by simply calling self do embeddings on our inputã€‚

Okayï¼Œ so what does this look like and in general a good way to proceed with building your forward function is to incrementally debug it I know people like to use debuggs and stuff Iã€‚

Don'tã€‚Really do that and instead just use simple print statementsã€‚

 which also works probably not as efficientï¼Œ but whateverã€‚

 so let's make let's actually construct an instance of this NLM classã€‚ğŸ˜Šï¼ŒSo I can do that byã€‚Againã€‚

 this calls the init functionã€‚ So I have to specify all of these thingsã€‚

 Let's say my D embedding is going beã€‚I don't knowã€‚ What should it be5ã€‚The hiddenã€‚Say 12ã€‚Window sizeã€‚

 we defined as2ï¼Œ and when vocabulary equalsã€‚The actual length of this vocabulary dictionaryã€‚Oã€‚

So if I run thisï¼Œ it's going to execute the in functionã€‚Okayï¼Œ soã€‚Let's make sure that it doesã€‚

 And there are no errorsã€‚ Okayï¼Œ And nextï¼Œ we are just going to pass our prefixes through this forward functionã€‚

 So if I do and you don't even have to call the forward functionï¼Œ you can justã€‚

Write this and it automatically calls the forward function that you've writtenã€‚

So just to make that clearï¼Œ if I print the size of this embedding objectã€‚I get four by two by5ã€‚

 so it's a three dimensional tensorï¼Œ and let's think about why its these are the this is the dimensionality of this M's variableã€‚

So four we know is the number of prefixes in this tensor right we have four examples so this corresponds to the batch size two is the number of words in our prefix right the number of words in the prefix is two so in the first example our prefixes Bob likes and so we have an embedding for each of those words and five is the dimensionality of our embeddings so for each of the four prefixes and each of the two words in that prefix we have a fivedial embeddingã€‚

So that's how you can interpret thisã€‚Okayã€‚So now that we have thisï¼Œ what's the next stepï¼Ÿ

Let's just write the dimensionality out so we can have it as reference for laterã€‚

 So in the neural language modelï¼Œ next weã€‚Want to concatenateã€‚The prefix embeddings togetherã€‚

So here our window size is just twoï¼Œ so we're going to concatenate these two dimensions together and sorry these two embeddings each associated with the prefix togetherã€‚

 So our ideal expected output is going to have a dimensionality of four by 10 right the first five dimensions of that 10 dimensional vector I'm going to correspond to the prefix embedding of the first word and the second five are going be the embeddings of the second wordã€‚

 So in this case we can simply just accomplish this with a reshape operation So if I say the concatet ms equals ms and the view function is a good way of reshaping your dataã€‚

 So I've already got this batch size which I know is forã€‚

 but we're just going to use these variables in case you know at some other pointã€‚

 maybe a test time I want to use a batch size that's not four right I can use a variableã€‚ğŸ˜Šã€‚

Number for my batch sizeã€‚So I know the first I want to keep the four prefixes distinct from each other and if I put in negative oneã€‚

 that's just going to essentially squeeze the rest of the dimensions into a single vectorã€‚

 but in practice of course I know that this is going to be window sizeã€‚ğŸ˜Šï¼ŒWhich I did not store hereã€‚

 sorryã€‚Canè¿™ä¸ªã€‚Window size timesã€‚Let me just store a variable for the embedding dimensionalityã€‚

So dot Dã€‚Okayï¼Œ so this simply takes this embeddings variable and reshapes it to a4 by 10ã€‚

 let's make sure this actually workedã€‚Hopefully it didã€‚Okayã€‚

 so I'll put some print statements along the wayï¼Œ soã€‚We can make sense of what's going onã€‚

Embbedding sizeã€‚ and yeahï¼Œ as a practical tipï¼Œ the sizeã€‚

 the dimensionality of these intermediate variables that youre creating is one of the most useful things that you can look at because a bunch of the errors that you're going to get if you're implementing this and other more complicated networks are going to be maybe you messed up and you didn't keep track of your dimensions properlyã€‚

 you swapped around window size and embeing dimensionalityã€‚

 And now your matrix multiplications don't work and so onã€‚ So you should always keep track ofã€‚ğŸ˜Šã€‚

The size of the variables that you're creating in the forward functionã€‚ Alrightã€‚

 so let's print these things outã€‚ Whoopsï¼Œ what did I call itã€‚Ohï¼Œ Dsã€‚Okayã€‚

 so we see that the embedding size was4 by 2 by5 and now the concatenated embedding size is 4 by 10ã€‚

 so we remove the sequence dimension and just concatenate all of the embeddings associated with the prefix together to form one 10 dimensional vector per input prefixã€‚

å—¯ã€‚ğŸ¼Okayï¼Œ and maybe we can validate thisï¼Œ rightï¼Œ So if I print the embeddings out for the first prefix in my my batchã€‚

 which corresponds toã€‚Bob likesã€‚å—¯ã€‚So if I print this outï¼Œ I get these two embeddingsã€‚

 you can see there's two separate embeddingsï¼Œ one on each lineã€‚And if I print outã€‚Can cat Ms zeroï¼Ÿ

You see there's just one 10 dimensional vector where the order here is the same as the order here is just we've squeezed these two vectors togetherã€‚

Okayï¼Œ soã€‚Now let's get on with the next computationã€‚Now we project this to the hidden spaceã€‚

And it's very easy to do thisï¼Œ we can just use our linear layer that we defined previously so I can say hiddens selfã€‚

whiã€‚And couldn cat emsã€‚And so now if I want to look at the size of thisã€‚

 remember that I've specified a hidden dimensionality of 12 hereã€‚

 So hopefully this thing is going to beã€‚A4 by 12ã€‚Mattrixï¼Œ and it is greatã€‚ Okayã€‚

 so I've gotten to this pointã€‚ Now I'm at the hidden spaceã€‚ and finallyã€‚ğŸ˜Šã€‚

Project His to vocabulariesã€‚ã§ã™ã€‚And againï¼Œ we just follow this execï¼Œ in factï¼Œ I will just copy thisã€‚

So we'll call this outã€‚Out equals w outã€‚Hidsï¼Œ and we're going to printã€‚

Output size equals outtop sizeã€‚And 11ï¼Œ so our vocabulary remember is 11 because there's 11 word typesã€‚

 we have this word is that appears twice and all the other words appear just once eachã€‚

 so we have a vocabulary size of 11ã€‚ğŸ˜Šï¼ŒOkayï¼Œ so are we done wellï¼Œ if we print outã€‚

We will see that these are definitely not probability distributionsï¼Œ rightã€‚

 and the reason for this is because we have not applied the softmax functionã€‚

So in practice to compute our loss functionï¼Œ which remember is the negative log probability of the target wordã€‚

 we're going to do a log softmax followed by the negative log likelihoodã€‚

 but Pytorch actually combines both of these into a single loss function called the cross entropy loss as we'll see laterã€‚

 but just as an exampleã€‚ğŸ¼Maybe I want toï¼Œ you knowï¼Œ show that I can get probabilities out of theseã€‚

 these outputsã€‚ so I can do this byã€‚ and you saw this also inã€‚In homework zeroã€‚

 I can apply to softftmax to outs and I have to specify the dimensionã€‚

 so here I want to take this over the not the batch dimension but the actual the logicsã€‚Oopsã€‚

 I didn't print it outã€‚So if I did thisã€‚Thenã€‚The resulting vectorsï¼Œ as you can see hereã€‚

 this is for the first prefixï¼Œ you can just take my word for it that this sums up to one and you can see that all of these numbers are positiveã€‚

ğŸ˜Šï¼ŒActually we canã€‚Evenã€‚Show that they sum up to oneï¼Œ so these are the sums of each of the vectorsã€‚

Which just enforcing that we can get probability distributions out of these if we wantï¼Œ but for nowã€‚

 we're just going to return the unnmalized probabilitiesã€‚Also known as Losã€‚

 so the logics are the values of this vector before applying the Somaxã€‚Okayã€‚

 so now that we're confident that at least the network isn't crashing right when I feed the inputs through itã€‚

 I'm going to get rid of some of these print statements to make it cleanerã€‚ğŸ˜Šã€‚

And we get this as outputï¼Œ the logicsã€‚ And you'll see here that I have this grad function thing when I print out this out tensorã€‚

 And this is what's going to allow us to perform backproã€‚ The pytorrch just keeps track ofã€‚ğŸ˜Šã€‚

All the computations I need to do in order to compute the gradient for everything that I've defined in my forward functionã€‚

 so every one of these operations when I embed something or when I reshape it when I apply a linear layerã€‚

 all of these have been in ptorch sort of paired up with the functions that you need to apply to compute the derivatives whenever they're applied and so we because these things are all this functionalities already thereã€‚

 we don't have to manually compute the derivatives ourselvesã€‚Okayã€‚

 so let's just assume that this network is workingï¼Œ hopefully it isã€‚

 Now let's actually write a training loop to train this modelã€‚ğŸ˜Šï¼ŒSo to do thatã€‚

 I need to define a couple of thingsã€‚Firstï¼Œ I need to define something called the number of epochs that I'm going to train my modelã€‚

 So let's sayã€‚Uã€‚Set it to 10ã€‚ So an epoch is essentially one pass through the training dataã€‚

 So the number of epochs defines how many times we are going to go through our entire training data setã€‚

 In this caseï¼Œ it's just four prefixesã€‚ This is aï¼Œ againï¼Œ super simple toy exampleã€‚

 The next thing we want to do is define our learning rateã€‚ Rememberã€‚

 when we're doing gradient descentï¼Œ We need to define the step sizeã€‚

 How big of a step do we want to take in the direction of the negative gradientã€‚ğŸ˜Šã€‚

We can maybe we'll set it to 0 oneã€‚ againï¼Œ this is a hyper parametermeter so we we canã€‚

Modify this if training isn'tã€‚Convergingï¼Œ etc ceï¼Œ we saw some of the failure cases associated with big learning ratesã€‚

 you might see the loss zigzagging around or going up in some cases if your learning rate is too bigã€‚

 so it's important to you mess with it until you see a nice smooth curve in your loss function that's decreasingã€‚

So Pytorch defines this cross entropy plusï¼Œ you can see it hereã€‚

 and this applies the log Softmax function and then applies a negative log likelihood computation which accomplishes exactly what we went through on the iPadã€‚

 it computes the negative log probability of the target word and averages it across all of the examples in the batchã€‚

So averageã€‚Cross entropy loss over each prefix in batchã€‚Okayã€‚

 and the last thing we need to do is select an optimizer so we went over simple gradient descent to get that we can do torchã€‚

opimã€‚sgdstochastic gradient descent all optimizers require you to pass in the networks parameters which you can do by just doing network do parametersã€‚

ğŸ˜Šï¼ŒNot too difficultï¼Œ and then we have to pass it in our learning rateï¼Œ which we specified up hereã€‚

Okayï¼Œ soã€‚Alrightï¼Œ maybe I willã€‚We will use vanilla gradient descentã€‚You can experiment with othersã€‚

Adamã€‚And stuff simply by just changing the so you have atomï¼Œ AM maxï¼Œ othersï¼Œ these are all differentã€‚

 more complex and sometimes better variants of SGD that help for particularly complicated networksã€‚

Okayï¼Œ so next we're going to write our training loopã€‚So this this loop will passã€‚

 it'll be over the number of epochs loopops for INã€‚Range epochsã€‚

 we're going to loop over their training setã€‚We're going to compute these logicsï¼Œ rightã€‚

 I'm returning those in the forward functionã€‚So I will call my network on the what did I call this thing prefixesã€‚

 Yeahï¼Œ the prefixesï¼Œ the input tensor that I've madeã€‚å—¯ã€‚

And then I'm going to compute my loss functionï¼Œ The cross entropy lossï¼Œ very easy to do thisã€‚

 I will pass in the logicsï¼Œ and I will also pass inã€‚ğŸ˜Šï¼ŒThe labelsï¼Œ which contain theã€‚

 the words that I'm supposed to predictã€‚Okayï¼Œ so I have computed the labelsã€‚And noã€‚

Let's update our PRs to make the loss smallerã€‚ So before we do thisã€‚

 maybe we can just print the lossã€‚ firstï¼Œ I want to see if this was working and I didn't make any dumb errorsã€‚

 And secondlyï¼Œ we should observe the loss is not changingã€‚

So why isn't the loss changing because we haven't actually updated the parameters using our optimizerã€‚

 So when we do thisï¼Œ hopefully our loss will start going downã€‚So the first thing we have to doã€‚

 remember isã€‚Step oneï¼Œ compute to gradientã€‚Okay gradientã€‚å—¯ã€‚

Partial derivatives of loss with respect to parametersã€‚

And I remember before I told you this was just one line and it's literally this simpleã€‚

 I just type lost stop backward and done I have the gradient I have the partial derivatives with respect to every peri and nextã€‚

Update perms using gradient descentã€‚To do thisï¼Œ I take my optimizer that I've created from before and just type stepã€‚

å—¯ã€‚I think I also want to zero the gradientsã€‚So this willã€‚Step  threeã€‚

 zero out the gradients for the next epochã€‚All rightï¼Œ let's put some spaces make this more readableã€‚

So if I don't do thisï¼Œ the optimizer is going to accumulate the gradients across every epoch which we don't wantã€‚

 So after we make one stepï¼Œ we want to go backï¼Œ use our new model parameters to compute a new loss functionã€‚

 recompute the gradientï¼Œ take another stepã€‚ So this zero gradã€‚

 make sure we don't accumulate gradients across the different epochsã€‚Allrightã€‚

 so let's go ahead and also print out our loss functionã€‚

 So I will print out the epoch that we're on and the lossã€‚And I'll make the loss a little readableã€‚

Losï¼Œ all rightï¼Œ so whoopsã€‚What did I do hereï¼ŸNot all arguments converted during stringï¼Œ okayã€‚

 whateverã€‚Maybe I will justã€‚Ohï¼Œ I forgot aã€‚Percentage signã€‚Oã€‚Greatï¼Œ so we see the loss is going downã€‚

 it started at 2ã€‚64 and now it's at 1ã€‚27ã€‚ let's see if we can make it get to zero so let's train it for maybe 20 epochsã€‚

All rightï¼Œ so it's getting down pretty close to 0ï¼Œ we started at 2ã€‚24ã€‚

 this is the average loss across all the elements in the prefix and we got down by Epoch 20 to 0ã€‚17ã€‚

 so huge reduction in the lossã€‚So to concludeã€‚Let't sayã€‚å‘ƒã€‚Our loss has dropped to close to zeroã€‚

 but are we confident that this is actually workingï¼Œ So we would be confident if I can put inã€‚

 say Bob likes into this model and observe that it predicts sheep with a very high probability rightã€‚

 or if I put in Alice is and it predicts fastã€‚So is this actually happening orï¼Ÿ

Is our loss low but our model still isn't predicting reasonableã€‚Conditional probabilitiesï¼Œ let's seeã€‚

But is it actually workingï¼ŸLet's seeã€‚Okayï¼Œ so note that up till this point I've been working entirely with the indices corresponding to the wordsã€‚

 but now I might want to actually see like the probability that the model assigns to a particular word following the prefix so to make this easy I can define this reverse vocabulary mappingã€‚

ğŸ˜Šï¼ŒP'sã€‚Firstï¼Œ defineã€‚A reverseã€‚ğŸ¼Vocabulary mappingã€‚From I from index to word typeã€‚

We can call it Rev vocab and I can do this by simply iterating over all of the items in my vocab dictionaryã€‚

å—¯ã€‚Okayï¼Œ did this workï¼ŸI think Iï¼Œ ohï¼Œ whoopsï¼Œ I forgot aã€‚Itemsï¼Œ so that should workã€‚

And let's just see what this looks like to be clearã€‚

So it's just the reverse of the vocab zero is mapped to Bobï¼Œ one is mapped to likes and so onã€‚Okayã€‚

 so let's just see what happens when I put in Bob likesã€‚

 I know that's the first element of my prefix tensor soã€‚

Ill just call this variable above like equals prefixes zeroã€‚ğŸ¼So prediction equalsã€‚Networkã€‚

Bob likes and since my model is set up to take in two dimensional inputs right where the first dimension is the batch sizeã€‚

 I need to feed it if I'm interested only in this bo likes predictionã€‚

 I need to feed it a 2D matrix not just a single vector and so I need to make this batch size dimension one so right now where am Iã€‚

Down hereï¼Œ this bo likes variable if I print its sizeã€‚It's just a two dimensional vectorï¼Œ rightã€‚

 because it has the indexes for Bob and likesã€‚ And what I'm going to do is unsqueezeã€‚

Which means I create a new dimension and I put it in the first dimension and this is going to be the equivalent of my batch size and unsqueeed just makes the new dimension size oneã€‚

 So if I do this now you see it's a one by two and now I can pass it into my modelã€‚ğŸ˜Šï¼ŒSo if I do thisã€‚

 now I get a predictionï¼Œ let's see what the prediction looks likeã€‚Againã€‚

 these are the unormalized logicsï¼Œ so maybe to make this more readableã€‚

 let's go ahead and apply the softm functionã€‚I already imported thisï¼Œ rightï¼Ÿ

Softm prediction equals oneï¼Œ let's just call this probabilitiesã€‚Call it prosã€‚ we'll call this losã€‚

 We'll call it thisã€‚Logetsï¼Œ and I have to specify the dimension of oneã€‚ So if we print prosã€‚

Now we get the actual probability distributionã€‚ These are not the log probabilitiesã€‚

 just to be clearï¼Œ these are theã€‚The raw probabilitiesã€‚ and if you'll notice this is also a 1 by 11ã€‚

 so if I want to get it into if I want to flatten itã€‚

 so basically do the reverse of this unseze operation I can do squeezeã€‚

 which gives me just a single vectorã€‚And now I can find the index that has the max probabilityã€‚

 rightï¼Œ This is what the model is actually predicting so I canã€‚ğŸ˜Šã€‚

rite out the AGm index equals just use the AGmax functionã€‚And if I want to getã€‚

 so this is going to return a tensor objectï¼Œ if I want to actually use this to index into my revv vocab mappingã€‚

 I have to use the dot itemï¼Œ which will get it back into just a Python numberã€‚Soã€‚Let's go ahead andã€‚

ğŸ¼Make this more specificã€‚ So given Bob likesï¼Œ the model predictsã€‚The word as the next word withã€‚

Let's give itã€‚Let's sayã€‚The precisionã€‚Probabilityã€‚And we willã€‚å—¯ã€‚The rev vocab of the Agm indexã€‚

 So this is going to reverseï¼Œ map the Argm index into a word in our vocabularyã€‚

 And let's also look at the probability of that wordã€‚Okayï¼Œ hopefully this worked It did notã€‚Godã€‚

 I'm really struggling with the Oã€‚What have I done wrong hereï¼Œ Ohï¼Œ I forgot parenthesesã€‚Okayã€‚

 so given Bob likesï¼Œ the model predicts sheep as an next word with 0ã€‚7851 probabilityã€‚

 get rid this to make it easierã€‚And this is this is not greatï¼Œ rightï¼Ÿ

 There's only there's four prefixes in hereï¼Œ so I would hope that this can get closer to oneã€‚

 And maybe if I train for more epochsï¼Œ I willã€‚ So let's go ahead and train this for 100 epochsã€‚ğŸ˜Šã€‚

Okayï¼Œ my loss is now at 0ã€‚02ï¼Œ so it's very lowã€‚ I expect this to be a much higher probabilityã€‚

 all rightï¼Œ98ã€‚93 probabilityã€‚ So you see that as a loss gets lower and lowerã€‚

 the probability of predicting the right continuation increasesã€‚All rightï¼Œ so just to summarizeï¼Œ ohã€‚

 this video has goneã€‚Much longer than I anticipated as usualï¼Œ I guessï¼Œ in this notebookã€‚

 we've gone from brassï¼Œ just text of four different sentences to a fixed window neural language model that can predict the next word given any of these four prefixesã€‚

 Soï¼Œ of courseï¼Œ this is not a usable modelï¼Œ rightï¼Œ It's a very simple vocabularyã€‚

 We are evaluating the model on our training dataï¼Œ which you knowã€‚

 I said was one of the things you definitely should not ever doã€‚ this is just a demonstrationã€‚

 there's no expectation that this model is going to generalize to anything outside of its training setã€‚

ğŸ˜Šï¼ŒAnd so yeahï¼Œ in practiceï¼Œ you would want to have a separate test set that you're evaluating your model on that it hasn't seen beforeã€‚

Okayï¼Œ so with thisï¼Œ we've concludedï¼Œ we've kind of synthesized all of the concepts that we've talked about till this point right this notion of tokens and typesã€‚

 the idea of going from words to indices which then allow us to index into an embedding matrix which is then composed using a neural network architecture in this case a very simple one to get an output probability distribution right and how do we actually train the model to produce a better estimate of these conditional probabilities we first define a loss function that tells us model how bad the model is doingã€‚

 we compute the loss on our training data we compute the partial derivatives I didn't actually get to show you much regarding this step because it's again one lineã€‚

 soã€‚ğŸ˜Šï¼ŒWe compute the we do back propagation on this one lineã€‚

 and we do gradient descent in this lineã€‚And once we loop over our training data a bunch of timesã€‚

 we get a model that achieves a very low lossã€‚ and we see that the model is doing a pretty good job at predictingã€‚

What it sees in its training dataï¼Œ Bob likes sheepã€‚Okayï¼Œ so for next time we're going to focus onã€‚

The actual architecture that goes into the forward function here so we talked about two different ones in addition to this fixed window modelã€‚

 we talked about the recurrent neural language modelã€‚

 but for next time we're going to move to the state of the art composition function which is the transformer so we're going to start with selfattentionã€‚

 which is a basic building block of the transformer and then move on to how the forward propagation actually looks in the transformer this is all setting up for next week where we talk finally about the applications of largescale transformer language models for solving NLP tasks that are not language modeling you all are probably sick of just looking at language modeling by this pointã€‚

 but I promise soon we will advance past this but it's very important to get familiar with howï¼ŒğŸ˜Šã€‚

These models work and how they're trained and so onã€‚All rightï¼Œ greatã€‚



![](img/3aa8ae5afa5ccba3efd817626d99d2d3_4.png)