# P1ÔºöL0- ËØæÁ®ã‰ªãÁªç - ShowMeAI - BV1BL411t7RV

All rightÔºå hey everyoneÔºå so this is the first introductory lecture of CS685 the Advanced NLP class this fall„ÄÇ

 my name is MoHtÔºå I'll be the instructorÔºå I'm a professor here at UMass whose primary research focus is in natural language processing and I'm excited to focus on some very recent and revolutionary techniques that have come up within the field of NLP over the past couple years„ÄÇ

So before I get into exactly what kind of topics we're going to be covering here„ÄÇ

 I wanted to go over some logisticsÔºå I know this is especially important this semester where everything is remote and it's certainly a new experience for me and maybe also for many of you incoming graduate students„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_1.png)

All right„ÄÇSo like it's posted on the course website and again you should certainly check out the course website for everything This class will be completely asynchronous so all lectures will be pre-recorded videos like the one you're watching now either to be posted on YouTube or somewhere else if people are having issues with quality or access and so the plan is every week I will post new lecture videos definitely on Mondays and sometimes on Wednesdays as well and for each video there will be some accompanying readings which you should hopefully plan to do before watching the videos things will make more sense that way hopefully and all of the links to these things will be posted on the course website„ÄÇ

üòäÔºåSo this is not something that I've done in the past but because we're remote and I really have no way to enforce that you are actually watching these videos like I would normally with you know taking attendance or having some attendance checks in lie of all of that I will have a weekly quiz on all of the topics that we covered in that week these will be lightly graded so just to make sure that you put in a good faith effort to answer the questions and theyll be submitted on Grcope so basically every assignment you have here will be submitted on Grcope„ÄÇ

 you should be added to you all should be added to gradecope by the end of this first week„ÄÇ

 also to Moodle and Piazza„ÄÇOkayÔºå rightÔºå there's also no quiz for this first week„ÄÇ

 I think we're probably going to have to work through various issues that come up with students and their you and your access to the various course materials„ÄÇ

 especially for those of you in countries with restricted internet„ÄÇ

I also should mention that we will have office hours every day from Monday through Thursday„ÄÇ

 we've scheduled these so that they will be accessible to people who are not only in the US but also in other time zones like India and China where I know many of the grad students currently are„ÄÇ

 which means that for most of the TAs and myself we'll be having 8 amm Eastern time office hours„ÄÇ

But yeahÔºå hopefully this will provide a way for you to get some more like live in person feedback or any questions that you have during lecture„ÄÇ

 you can ask during these office hours so these will be on Zoom„ÄÇ

So I'll talk about this a little more but there will be one midterm in this class and occasionally so every other week or so we'll release optional practice problem so you don't have to do these but they will help you prepare for the exams so because everything is remote this semester we obviously cannot have an in-person exam which means really the only way to do this is to have you know like an open book open internet exam so the problems will be more conceptual in nature and hopefully not something you can easily Google I guess hopefully for me not for you but yeah the intent behind releasing these practice problems is to help you get prepared for the types of questions that will be on the exam and to give you something else to discuss with us during office hours so again you don't„ÄÇ

to do these if you don't want toÔºå but they're just there for„ÄÇ

In case you're worried about some aspect of the material„ÄÇ

All right so the TAs for this class are my own PhD students so all three of them to Son and Calpeche they're very experienced with NLP research they've all publish papers in top NLP conferences and so you should feel free to ask any of them questions at any level or depth regarding the material here„ÄÇ

 they're all very familiar with it some topics they might even be more knowledgeable than I am so yeah please do attend their office hours if those times are convenient for you so again the links to the office hours oh I think that's on the next slide but yeah they're pasteed on the course website regarding email please use this instructor's Gmail account for everything you could possibly want to email me or the Ts about„ÄÇ

üòä„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_3.png)

This could involve you know medical absences or other sorts of things that are preventing you from accessing or completing the assignments yeah do not email me directly I won't respond so please just use the instructor's account it's meant for this course„ÄÇ

All right„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_5.png)

FinallyÔºå office hoursÔºå so rightÔºå as I mentioned beforeÔºå we have three office hours scheduled at 8 a„ÄÇ

mÔºå so this is to I think this is the hour that works for most time zones„ÄÇ

 although it is a little late in ChinaÔºå I believe for those of you on the West Coast of the United States Colp will have office hours at 2 p„ÄÇ

m on Thursdays„ÄÇWe are open to adjusting these times if it turns out that there aren't any office hours that are convenient for many of you„ÄÇ

So againÔºå do let us know on Piazza or on email or on other there's other ways you can contact us to tell us about these issues„ÄÇ

 there will be no office hours for this first week the TAs are completing internships and we're still you know getting prepared with the materials so all office hours will begin on August 31st„ÄÇ

 which is the start of the second week of the semester„ÄÇ

for days or weeks in which homeworks are due or like the week before the exam„ÄÇ

 we often extend our office hours for having past semesters to two hours each and we are prepared to do that for this semester as well„ÄÇ

 so do keep that in mind„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_7.png)

All rightÔºå so there are many ways in which you can contact us beyond just the course instructor's Gmail account„ÄÇ

 we have an anonymous formÔºå this is also linked on the course website so if you don't want your name associated with a comment and you don't want anyone else to know about it„ÄÇ

 you can use this formÔºå there's also piazza you will be all enrolled at some point soon where you can post you know questions or comments that other people in the class can also help answer so myself and the TAs will be fairly responsive on Piazza or at least we try to be but if you have a general question it's probably in your best interest to post on Piazza because there are like 120 something other students who can also answer or chime in if they have the same question so Ive decided to not record all my videos„ÄÇ

Like way ahead of timeÔºå but instead record them week to week„ÄÇ

 so whatever pertinent questions or comments that come up on the anonymous form or on Piazza„ÄÇ

 I will try and respond to them in the weekly videos„ÄÇ

 so this is a good way if something is confusing to you or to a bunch of you and this is indicated on Piazza„ÄÇ

 I will address these these issues„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_9.png)

All right„ÄÇSo the class has no official prerequisites„ÄÇ

But the following will certainly be useful and if any of these things you maybe have forgotten or not covered in a previous class„ÄÇ

 then you should try and brush up on them outside of class„ÄÇ

I guess everything is outside of class nowÔºå but yeah like outside of the assigned reading and stuff like that„ÄÇ

 so we will provide resources like links to tutorials and easy to understand blog posts or notes that you can use to familiarize yourself with some of these concepts„ÄÇ

 but programming is certainly importantÔºå we'll be using Python and Pytororch throughout the class„ÄÇ

 so Pytororch is a library for deep learning that all of my students use for their research and many NLP toolkits are built on so it's very useful to learn not only for NLP but for any sort of machine learning research and even in industry„ÄÇ

So you'll need to be okay with probabilityÔºå linear algebra and basic mathematical notation„ÄÇ

Since this is an advanced NLP classÔºå we will not be reviewing too much background on this„ÄÇ

 so again you will need to put in some effort outside of the assigned readings and assignments to brush up on this if you need to„ÄÇ

One thing that is especially important with the types of models we'll be dealing with is matrix calculus„ÄÇ

 so we'll be taking a lot of partial derivatives of various loss functions with respect to different weight matrices in neural network models we won't be going in this class into too much depth into weeds of back propagation„ÄÇ

 but it will be important to at least understand the notation here when I talk about representing a word as a vector and then projecting it into another space using a feed forward layer these are just basic matrix vector products which you've probably seen in linear algebra but again these will come up throughout the class so just want to„ÄÇ

Put that expectation out there now„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_11.png)

Okay„ÄÇWhat do we have next right so how are you going to be graded as I mentioned before there will be these weekly quizzes starting the week after next those will account for 10% of your grade„ÄÇ

We will have three problem setsÔºå so they'll be all combinations of written answers where you might have to write out some math or provide a short answer response to some sort of question and they'll also include coding components so all the homeworks will be hosted on Google Collab which is a free service that Google provides to researchers that allows you to to access a machine with a GPU for free this is incredibly useful if you want to be training models at any scale that's non-trivial„ÄÇ

 although it does have its limitations like you can't you know train an insanely large model for very long they do have timeout restrictions of I think 12 hours so like they'll kill your whatever process you're running after 12 hours„ÄÇ

 but on the whole it's much better than you know trying„ÄÇDo something on your laptop by yourself„ÄÇ

That said againÔºå I know many of you could be in countries where access to things like Google Collab is restricted„ÄÇ

 so I do want to hear from you if that's the case because we can also posts things like notebooks and zip files with associated Python files that you can download and submit but Google Collab makes everything easy„ÄÇ

 we've used it in previous semesters and it's been pretty successful so I hope that it works out this semester„ÄÇ

All rightÔºå so around mid October there'll be a midterm again„ÄÇ

 this will be challenging on the part of myself and the TAs to actually create because we expect„ÄÇ

 I mean it's hard to release an exam and say don't use Google right there's just no way to enforce it so the types of problems on here again will be maybe a little more conceptual and like what would you do in this situation of kind of problems„ÄÇ

Obviously you're not supposed to collaborate on this midtermÔºå this is not something we can enforce„ÄÇ

 but we would hope that you you don't do this it's a violation of the honor code„ÄÇAll right„ÄÇ

 and then the last part of your grade will be and the biggest 35% will be final project so tentatively right now we're envisioning groups of four„ÄÇ

 but it kind of depends on how the class grows and what we think is manageable on the part of myself in the Ts but essentially we're going to let you choose any topic that you want that's related to NLP obviously and you will have to first propose a project with your group and then on the last day of final exams which I totally forget the date I think December 4th or something submit your final report„ÄÇ

AndW whichch is worth a quarter of your grade„ÄÇ So for the final projects„ÄÇ

 we're still deciding how to organize thisÔºå but usually we„ÄÇ

Have a period of maybe a week or two at the beginning of the semester where we'll allow people to form groups on using Piazza so there's a thread on piazza to let you know post your ideas and see if anyone's interested in joining your team after the period for forming groups expires anyone who isn't already in a group will be randomly assigned to to a group so that's generally the process we will do this very early in the semester to maximize the time that you have to work on your project„ÄÇ

We still haven't decided yet how to do so traditionally we have these project presentations where you know every group has a poster and they you know present what they've done to the rest of the class instructors and other people from like companies and other faculty at UMass and so on I'm not sure how we can do this on something like Zoom„ÄÇ

 but maybe we canÔºå so that's something that I'll let you know about in later weeks as the final report deadline comes closer„ÄÇ

üòäÔºåAll right so do you need to buy a textbook no you don't everything you need will be provided as links to PDF on the website again„ÄÇ

 this is an advanced NLP class it's intended for I mean it's a grad level class right there's a lot of PhD students taking this class and a lot of master's students and really there's more of a research focus in the topics that we'll be discussing throughout the semester so most of the readings will be„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_13.png)

Pretty recent conference papers from venues such as ACL and EmNLP and so on so these are some of the most prominent conferences research conferences in the field of NLP there will be this week and maybe next week„ÄÇ

 some readings from an NLP textbook but aside from that will primarily be looking at research papers So your first homework will also involve picking out conference paper that you think sounds interesting in writing a little summary of it so this is just to get you some experience in reading these things because oftentimes they do not contain all of the background knowledge that you will need to understand what's going on in the paper a lot of that knowledge is assumed and so a large part of this course will be developing the background that you need to understand these„ÄÇ

üòäÔºåThe newest wave of NLP papers„ÄÇSo while this class is very research focused that doesn't mean the methods and stuff we're discussing here will be are like completely impractical in industry settings that's not at all the case„ÄÇ

 actually NLP is a very fast moving field and most of the stuff that we'll be talking about at least the high-level ideas are already used in cutting edge industry NLP labs and products„ÄÇ

 so as an example Google Trans which I'm sure many of you have used over the years has completely changed in the underlying model from a more traditional phrase-based MT system to a neural network system that is used many of the models that will be discussing in this class and likewise for many other tasks as we'll talk about later in the same video the way of approaching„ÄÇ

These tasks has completely changed over the past two years„ÄÇ

 so it's a very exciting time for NLP and it's also a time where the research and industry are kind of intersecting they slowly converging to the same methods„ÄÇ

üòäÔºåO„ÄÇSo now that all the logistics stuff is out of the way„ÄÇ

 let's just briefly get into an introduction of what exactly NLP is„ÄÇ

 so we could start with what is a natural language„ÄÇSo by natural„ÄÇ

 we mean languages that have evolved through human use and not things that say humans have invented„ÄÇ

 like clling onÔºå obviously is a controlled fictional language and programming languages are also not something that we'll be discussing in this class„ÄÇ

üòäÔºåOkayÔºå so that's a pretty broad set of languages right„ÄÇ

 but then there are many tasks that kind of are under the umbrella of natural language processing„ÄÇ

 so basically anything that refers to modeling language using computers can be classed as NLP„ÄÇüòä„ÄÇ

And before we get into thatÔºå we we should talk a little bit about what the different kind of levels of linguistic structure there are„ÄÇ

 because it's important for many tasks to model just one of these levels or multiple levels and we'll look later in this class at models that kind of apply to across this the spectrum of levels„ÄÇ

 So let me move my„ÄÇüòä„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_15.png)

Video up here„ÄÇAs an asideÔºå I also imagine my video recording skills will improve considerably through this semester„ÄÇ

Hopefully that's the case„ÄÇOkayÔºå so of course the first level of text right is characters right a the alphabet„ÄÇ

 for exampleÔºå we can split up this sentenceÔºå Alice talked to Bob into these individual characters and each one in isolation doesn't tell us very much right but when they're all put together they convey some meaning„ÄÇ

üòäÔºåBut„ÄÇSo characters can be combined into things like sub wordss„ÄÇ

 which is under the umbrella of morphologyÔºå so this word talked is actually it has this stem of talk which carries the semantics of this verb„ÄÇ

 and then also the suffix ED which indicates that this talking action happened in the past„ÄÇüòä„ÄÇ

So beyond thatÔºå we get into just normal words„ÄÇAlice talked to BobÔºå there's five different words„ÄÇ

 we include the period there at the endÔºå and now each of these words is conveying some meaning in isolation„ÄÇ

 right Alice is a person an entity two is a preposition Bob is another person and we can associate these things with their parts of speech so for example„ÄÇ

 talked as a verb in the past tense and two is a prepositionÔºå a period is a punctuation mark„ÄÇüòä„ÄÇ

But so this is a low level of syntaxÔºå the parts of speech they apply to individual words„ÄÇ

 but at a higher level we also want to know about how these words combine together to convey the meaning of the entire sentence right so this is where we get into the kind of tree structure of language where we have these different constituents So for example„ÄÇ

 this phrase to Bob here is a prepositional phrase it's composed of a preposition and a noun and then when you compose this with this verb talked„ÄÇ

 you get a verb phrase talk to Bob finally you get this full sentence when you add the punctuation and Alice Alice talk to Bob So syntax at this level is kind of referring to the grammatical rules that make up a language and these are the rules that define how you can„ÄÇ

üòäÔºåUse words together„ÄÇAll rightÔºå so then we get to things like semantics and discourse which are now getting closer to the actual meaning of these we started all the way down at characters right but now we're talking about things like Alice is an agent„ÄÇ

 Bob is a recipient and Bob is the recipient of this communication event right Alice is conveying some information to him through his this talk event„ÄÇ

 there's also other things you might want to encode in a semantic representation right that what time did this event happen of course„ÄÇ

 in the pastÔºå we know that from the morphology right talk and the stics„ÄÇ

So a lot of what we'll be focusing on this semester is on these last three or four sorry the top three or four levels of the hierarchy here and in particular we're not going to be forming explicit symbolic representations of semantics like in this example„ÄÇ

 but rather we will be looking at vector space representations of wordsÔºå phrases„ÄÇ

 sentences and even documentsÔºå so these are things that humans can't easily interpret like they can with this„ÄÇ

 like if I read this box at the topÔºå I clearly see that Alices and agent and Bob is a recipient„ÄÇ

 I can interpret itÔºå but with the outputs of the models that we have and use nowadays I can't just look at a sequence of you know 10 different„ÄÇ

Real valued numbers and come to similar conclusions on what is contained by those numbers„ÄÇAll right„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_17.png)

So let's talk about some of the tasks that we can tackle within NLP„ÄÇ

One probably the most common paradigm learning paradigm for NLP is supervised learning so here we assume that we have access to something called a training data where we have examples that are paired with labels and so these labels can come from human annotation and as an example I could you know go to Amazon and crawl a bunch of product reviews and also get the corresponding review score with each review right so I might have a review that says you know this camera was completely bad and it didn't work and it's associated with a score of one out of five so if I collect you know hundreds of thousands of these examples now I have a very large training data set for this task of you know given a review„ÄÇ

üòäÔºåickThe review score„ÄÇSo this task is an example of sentiment analysis which is of course very popular in industry„ÄÇ

 people want to know what companies want to know what people are saying about their product right so I might deploy a sentiment analysis model on Twitter to see you know for every tweet that mentions my product„ÄÇ

 what is a sentiment of that tweet and this could give me some information on the popular perception of this product„ÄÇ

So other tasks that are tackled in a supervised setting where it's required to collect this label data set of examples which in the NLP case are pieces of text so documents or sentences or so on paired with labels so in the sentiment case this will be positive or negative and for other tasks the label space looks different so for a question answering one common research problem within NLP is this kind of reading comprehension setup up so you're given a question and you're given a document and you assume that the answer to the question lies somewhere in the document so your model is asked to find the answer of that question„ÄÇ

üòäÔºåThere are many variants of this setup right I could be given a single question and a huge collection of documents and asked to locate the answer within that collection„ÄÇ

 I could be given just a question and nothing else and a model might have to generate an answer to that question using any resources it has available so this could be you know a model that's able to„ÄÇ

üòäÔºåAccess the internet or some huge knowledge source like Wikipedia„ÄÇ

There are many other subtasks within question answering that we could discuss and maybe we'll discuss later in the semester„ÄÇ

There's also tasks like textual entailmentÔºå so this is commonly framed as given two sentences identify whether the first sentence entails or contradicts the second one this is closely related to the problem of paraphrasse detection right and another popular task that will spend some time on the semester's paraphrase generation so given a sentence can I have a model that actually produces a new sentence that is a paraphrase so it has the same approximately the same semantics of the first sentence„ÄÇ

Machine translation is an obvious„ÄÇYou knowÔºå a well known application of NLP„ÄÇ

 you're given a sentence in some source language and a user asks a model to produce a translation of that sentence in whatever the user picks as a target language„ÄÇ

So machine translation also very commonly a supervised learning problem where I have ahead of time a big data set of sentences in language A„ÄÇ

 and their are corresponding translations which have already been produced by some external translators in a target language B„ÄÇ

All rightÔºå but I will say that you know there is increasingly more work on doing these things without any labeled data„ÄÇ

 so unsupervised machine translation is a very exciting new research field within NLP where you're asked to build an NLP model to translation sentences but you have no parallel data so you don't know for any sentence in the source language what its translation in the target languages and it's pretty impressive that given no mapping between these two languages„ÄÇ

 you can still learn pretty decent translation model so if you've seen the movie arrival it's not anything as cool as what goes on there but„ÄÇ

Similar in principle„ÄÇMaybeÁ¨î„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_19.png)

OkayÔºå so we just talked about supervised learningÔºå the more recent developments in NLP and really most of the progress„ÄÇ

 ohÔºå I should move my camera„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_21.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_22.png)

Sorry„ÄÇThis„ÄÇMoving back and forth will hopefully be resolved at some point in the future„ÄÇAll right„ÄÇ

 so rightÔºå as I was sayingÔºå most recent progress in NLP has been fueled by advances in selfsupervised learning so this is actually a term that wasn't widely used and only started being used popularly within the last maybe year year and a half so just another point that all of this stuff is super new but in self-supervised learning we're not restricted to a smallish labeled data set that you know people had to put an effort to create here we just have a collection of text right so I can get text anywhere I could download the entire internet and have trillions or more I don't know how many tokens or sorry words they would be in in the internet but probably an insane amount„ÄÇ

üòäÔºåBut I don't have any extra labelsÔºå rightÔºå No one's going to go on and label the entire internet with the random things like is this positive or negative„ÄÇ

 It's just not feasible„ÄÇ So in self supervised learningÔºå we don't have these labels„ÄÇ

 but we instead use the text that we have to create our own labels„ÄÇ And then„ÄÇüòä„ÄÇ

Because we're no longer bounded by the size of our training data right as I just said„ÄÇ

 we can download as much text as exists that humans have ever written and is available in machine readable format and I can create labels out of this text and the primary use for self-supervised learning is to learn representations of text so this could be words of phrases or sentences or documents„ÄÇ

So what does it mean to create labels out of the text that that probably doesn't make sense right now one of the most popular objectives for self-supervised learning is language modeling and we'll spend a lot of time on language modeling over the next couple of weeks„ÄÇ

 but at its core basically you're giving a model the beginning of a sentence or a document or some piece of text and you're asking it to predict the next word so what word comes next in this document„ÄÇ

üòäÔºåSo if you think about itÔºå you can see how this task can be used in a self-supvised fashion right if I'm given an insanely large collection of text„ÄÇ

 I can easily create training data out of this text for this task of language modeling right because I can just keep chopping it up into chunks or prefixes of a document right so if I had the sentence let's go back„ÄÇ

üòä„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_24.png)

Sorry„ÄÇSo if I had the sentenceÔºå Alice talked to Bob right„ÄÇ

 I can produce a bunch of examples for language modeling out of this sentenceÔºå for example„ÄÇ

 I can give the model Alice and ask it to predict talked„ÄÇ

 I can give the model Alice talked and ask it to predict two„ÄÇ

 I can give the model Alice talk to Bob and ask it to predict the period„ÄÇ

 right so I'm basically just cutting the sentence in various points and for every single example„ÄÇ

 every prefix of the sentenceÔºå I have a corresponding label„ÄÇ

 the label in this case is the next word that occurs in that sentence„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_26.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_27.png)

Okay so there's language modelingÔºå another popular objective function for self-supervised learning is masked language modeling„ÄÇ

 so it's pretty similar except here I give you the entire document not just some prefix„ÄÇ

 but I remove certain words or spans of text and then I ask a model to given the unmasked text„ÄÇ

 predict the missing words so predict what was masked„ÄÇüòä„ÄÇ

Okay I guess just as another aside I like to have these like questions and yellow boxes throughout my lectures„ÄÇ

 usually these are questions that I asked to the class in like in-person lectures but since I can't do that here these are questions that I will usually leave for quizzes and stuff that I'll go over in subsequent weeks„ÄÇ

 but the answer to this question of course is I've already answered it right it's how much ever data that I can get in machine reutable format so you know humans have produced an insane amount of text and this is why self-supervised learning is so powerful is I have access to many orders of magnitude more data for these tasks like language modeling and mask language modeling„ÄÇ

All right so what does representation learning mean I've kind of glossed over that this is a very important point in modern NLP and the goal is if I'm given some text I want to create a representation of that text that captures there's a typo on the slide captures its linguistic properties„ÄÇ

üòä„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_29.png)

These could be things like syntaxÔºå Disccourse semantics that we discussed on that previous slide so currently our representations of text„ÄÇ

 our state of the art representations are real value low dimensional vectors so as an example here these are word representation so the word today could be represented with a4 dimensional vector of 0„ÄÇ

35 negative 1„ÄÇ3 2„ÄÇ2 and 0„ÄÇ003„ÄÇüòäÔºåSo of courseÔºå I can't look at this and make any sense of what that word means„ÄÇ

 rightÔºüBut for a computerÔºå if it gets you know a huge list of these words along with their representations„ÄÇ

 it can start making sense of the vector space in which all of these words are embedded„ÄÇ

 so maybe words like cat are similar in the space to words like dog and not similar to words like today„ÄÇ

 maybe words that have the same part of speech are similar to each other in this vector space„ÄÇ

 maybe you know words that are semantically similar are close to each other„ÄÇ

 so these are the kinds of intuitions that we have when we're talking about these vector space representations of text„ÄÇ

üòäÔºåSo when we have a selfsupervised modelÔºå we're able to learn very powerful representations of text because we have access to unlimited training data and if you imagine this task of say language modeling„ÄÇ

 predict the next wordÔºå rightÔºå Alice talk toÔºå predict the next wordÔºå if I have that input„ÄÇüòä„ÄÇ

Think about what a model has to understand in order to make Bob a highly probable continuation of that sentence under its model prediction„ÄÇ

It has to know that it's likely that a person is going to follow that prefix Alice talked to right because people generally talk to other people or at least some sort of noun or some sort of object„ÄÇ

It's definitely not going to be another preposition rightÔºå I wouldn't have well„ÄÇ

 I guess it could be that's a lieÔºå but it's not going to be„ÄÇA verb„ÄÇOkayÔºå now and' Alice talked to„ÄÇ

YeahÔºå so it's very unlikely to be a verb rightÔºå but it's likely to be a noun„ÄÇ

 it's likely to be a name of a person and so already you can see that the model has to learn these linguistic properties of the prefix in order to make a reasonable prediction of what comes next This is some of the intuition why self-supervised learning is such a powerful objective„ÄÇ

üòäÔºåAll rightÔºå and so finallyÔºå when we put these two things together„ÄÇ

 the self supervised learning and the supervised learningÔºå we run into this paradigm that is„ÄÇüòä„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_31.png)

The method of choice for basically every NLP task right nowÔºå which is transfer learning„ÄÇ

 So I'm going to have two phases hereÔºå a pretraining phase where I take a large selfsupvised model which is trained on as much text as I can feed it„ÄÇ

 And then I'm going to take my small supervised data set„ÄÇ

 maybe my Amazon reviews and fine tune the parameters of my huge pretrain model on the label data So you can think of it as my pretraining step„ÄÇ

 the self-supervised step is learning a lot of general linguistic properties and things like syntax semantics discourse that are broadly useful for all NLP tasks„ÄÇ

 rightÔºå anything that requires understanding language and the fine tuning phase is going to kind of guide the model to perform well on of much more specific tasks„ÄÇ

 So maybe it doesn't need„ÄÇüòäÔºåTo remember syntax to solve whatever this downstream task is„ÄÇ

 or maybe it doesn't need to know anything at the semantic level„ÄÇ Maybe it's like aÔºå I don't know„ÄÇ

 So the fine tuning phase is essentially„ÄÇKind of focusing the model on a particular task„ÄÇAll right„ÄÇ

 so since this is such a successful paradigmÔºå most of this class will be focusing on this high level concept of transfer learning within NLP„ÄÇ

So it's a very exciting directionÔºå many of my own students work in this„ÄÇ

 so it's a good chance for everyone to learn more aboutÔºå yeahÔºå these cool new methods„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_33.png)

All rightÔºå so a little bit more now that we've provided an overview of what topics generally we're going to cover in this transfer learning framework„ÄÇ

 more specifically I'll be dividing this course into about four high level units„ÄÇ

 each of which will be three weeks„ÄÇSo the first unit will be background and again„ÄÇ

 since this is an advanced class normally we might spend much more time on this for something like an undergrad or even 500 level class„ÄÇ

 but we will be spending roughly three weeks on background so we'll focus on language modeling we'll start in our next lecture on„ÄÇ

üòäÔºåThings like so non neuralural approaches to language modeling which involve counting and this will include a basic probability refresher„ÄÇ

 but then we'll quickly move over to neural language models from there well look at popular neural architectures that people use today so things like the Transer and then we'll shift to the transfer learning unit where we will talk specifically about the recent transfer learning methods a lot of them are named after Mppets so Elmo Bt they're like Sesame Street models„ÄÇ

We look at variants of these modelsÔºå so different training objectives„ÄÇ

 we talked about language modeling and mask language modeling as self supervised objectives„ÄÇ

 there have been many other things proposed since thenÔºå but know those are the core things„ÄÇüòä„ÄÇ

Also analysis right so if my model is producing these„ÄÇVs of vector representations of text„ÄÇ

 how can we make sense of what the model is learning so there is some pretty cool work on this front also that we'll cover oh„ÄÇ

 I forgot to mention applications so how exactly do we apply this transfer learning paradigm to solve tasks like sentiment„ÄÇ

 question answering and so on„ÄÇüòäÔºåThe next unit will focus on the general tasks that are within text generation„ÄÇ

 so these are things like translationÔºå paraphrase generation summarization and so on„ÄÇ

 but we'll focus on more recent work that has shown a lot of promise and things like few shot learning so if you've seen open AIs GPT3„ÄÇ

 it has a lot of exciting results on learning from very limited examples„ÄÇ

 actually not even learning its„ÄÇSolving tasks with kind of priming really priming a language model so this might be one direction that future research and NLP is going to take in which we minimize the role of these labeled dataset sets for the fine tuning phase over time well also look at very interesting work on the intersection between retrieval and text generation but yeah that will make more sense once we get into that unit and finally we have a kind of grab bag of important topics that we're going to cover at the end of the course so things like how do you construct a good data set for the fine tuning phase this is very important nowadays because the models are so powerful that they can latch onto any sort of artifacts during the data set creation method„ÄÇ

And this has happened several times recently within NLP research„ÄÇ

It's important to go over this whole process to make sure you're taking the best steps you can to create a usable and robust data set„ÄÇ

There's also evaluation so as we get more and more tasks that are possible to accomplish within NLP because of these know more and more powerful models„ÄÇ

 evaluating them becomes increasingly tricky right so for example some people in my group work on creative text generation so like generating stories and it's very unclear how you can evaluate these right we can't just use something like accuracy like you might use with sentiment analysis because that concept doesn't make sense„ÄÇ

üòäÔºåThen we'll talk about security there's as these models get more powerful and get deployed„ÄÇ

 there are many more security concerns some of which are specific to the types of models being used„ÄÇ

 for exampleÔºå there is a recent work„ÄÇüòäÔºåOut of„ÄÇI believe Berkeley that„ÄÇ

The researchers were able to steal the model underlying Google Translate„ÄÇ

Using relatively little money„ÄÇIf you're in industry or thinking about going into industry and working on NLP„ÄÇ

 these concerns are becoming very important„ÄÇFinally„ÄÇ

 we'll talk about ethical considerations of a lot of this work which are becoming increasingly important as we have models that you know are capable of generating text and are deployed to users in the wild what exactly do these do these models do when they're confronted with out of distribution inputs also what kinds of biases do they encode about various you know subsets of the population and so on so this is what will close out our semester withÔºü

üòäÔºåOkay„ÄÇSo for next time be on the lookout for homework zero„ÄÇ

 it's going to be released hopefully this Wednesday and due September 4th„ÄÇ

 this should be fairly straightforward it's mainly a review of math and also an introduction to Pytororch or a refresher for those of you who already have experience with it„ÄÇ

 there will also be some as I mentioned earlierÔºå questions on reading a research paper and writing something about it„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_35.png)

So the next videoÔºå next set of lecture videosÔºå I haven't decided yet how to do these exactly if I should make shorter videos or longer videos„ÄÇ

 this video is already far longer than I intended it to„ÄÇ

 but yeah please do comment on your preferences on Piazza or using any of these other methods to let us know„ÄÇ

üòäÔºåOf courseÔºå if you have technical issues or course or registration or access issues again„ÄÇ

 please do contact usÔºå hopefully we can get these all resolved within the first week All right„ÄÇ

 so I wanted to conclude by actually going through some demos of NLP systems just to have give you a more concrete idea on what some of these tasks look like and later in the semester well be actually you know designing models in your homework assignments to solve some of these tasks but right now I just want to give a high level overview so„ÄÇ

üòä„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_37.png)

If I switch windows„ÄÇYesÔºå okay„ÄÇAll rightÔºå also let me move my thing over here„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_39.png)

Okay„ÄÇSo this website is hosted by Alllan NLP which is part of the Allen Institute for Artificial Intelligence I actually did a postdoc here before joining UMass„ÄÇ

 so I know many of the people that develop this demo but I think it's a great way of getting a basic introduction to a lot of different NLP tasks and actually it allows you to interact with the models directly rather than me just showing you random examples on slides„ÄÇ

So we'll start with this reading comprehension task I mentioned this before as you have some document and you have a question about that document and you want the model to figure out what part of the document answers this question so here we have a description of a basketball game between the Mavericks and the Sps I guess it's a playoff series between the Mavericks and the Sps and the question is how many points do the Sps beat the Mavericks spy and Game7„ÄÇ

So one of the constraints of these models is that they cannot produce any text that is not contained by the document„ÄÇ

So in Game sevenÔºå we see if we go to the end of this document„ÄÇ

 game7 was on the Spurs home court and the Sps beat the Mavericks 119 to 96„ÄÇ

 so I guess this is like a 23 pointÔºå so the answer to this would be 23 points„ÄÇ

 but there is no mention here of 23 points„ÄÇSo what can the model do This is an example of some of the limitations of the model if we had run here„ÄÇ

 this is actually querying a model with this input and we see that the answer was 119 to 96„ÄÇ

 this is what the model predictedÔºå which I guess is the best thing it could do right„ÄÇ

 it gives you the actual score instead of the margin„ÄÇüòä„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_41.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_42.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_43.png)

So rightÔºå this is an example of the model's limitations„ÄÇ

 but also it's not that bad right it's doing something decent„ÄÇ

 we could ask it an easier question so again I can just type in questions„ÄÇ

 I could ask about where was game Oh yeah yesÔºå where was game„ÄÇüòäÔºå7„ÄÇOÔºå didn„ÄÇOkay„ÄÇ

 so here I would expect the answer to be maybe San Antonio or Spurs Home Court„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_45.png)

And then the spurs home court„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_47.png)

So it's you know for these simple kinds of questions it's pretty good but for more complicated questions these models can really struggle so I would encourage you if you're interested to play around with this model and see if you can break it one of your homework assignments later on in the semester will be very related to doing stuff like this so coming up with inputs that break existing models let's also take a look at sentiment analysis because we discuss this as well so I'm going to use this Roberta model which is one of the best performing models for the task of sentiment analysis„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_49.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_50.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_51.png)

UmÔºåMaybe I'll use a existing example so I don't have to come up with the one„ÄÇ

So unremittingly awful that labeling it a dog probably constitutes cruelty to canines„ÄÇ

So this is clearly negative about this movieÔºå all of these sentences are movie reviews„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_53.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_54.png)

AndThe model is 100% sure that it is negative„ÄÇ So right„ÄÇüòäÔºåAnd let's seeÔºå I'll go over one more task„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_56.png)

RightÔºå so they have a language modeling demoÔºå which againÔºå we talked about before„ÄÇ

 So let me put in our sentence„ÄÇ Alice talked to„ÄÇAnd„ÄÇOh my god„ÄÇÂØπ„ÄÇOkay„ÄÇ

I guess that didn't work out so wellÔºå I'm not sure what this„ÄÇ

Clearly I didn't plan these examples out beforehand„ÄÇLet's try another example„ÄÇ

I'm just copying this from the„ÄÇLanguage modeling is a task of„ÄÇAll right„ÄÇ

 so modeling is probably not a great continuation to the sentence how about we go with using„ÄÇ

The model then predicts„ÄÇ So you can see it's predicting the next word„ÄÇ

 And then we're just going to keep predicting the next word„ÄÇ

 So language modeling is the task of using data to„ÄÇPredict the future behavior of„ÄÇA group of„ÄÇ

Individuals periodÔºå okayÔºå so it's not greatÔºå but it did produce a grammatically correct sentence„ÄÇ

And yeahÔºå hopefully that gave you a good insight into this task right of predicting the next word given a prefix How about mask language modeling„ÄÇ

 I'm just going to use an example here to avoid what just happened with Alice talk to Bob the doctor and the emergency room to see mask patient So what goes in the mask the model predicts his„ÄÇ

üòä„ÄÇ

![](img/9f6f35569e3aea3c81a2ade7981caabe_58.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_59.png)

![](img/9f6f35569e3aea3c81a2ade7981caabe_60.png)

This is also an example of gender bias within these models„ÄÇ

 you see the low percentage assigned to her hereÔºå this is an issue with the well„ÄÇ

 a lot of the issues with the training data of this model seeing just in general text wherever it was trained on„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_62.png)

That doctors were more likely to be associated with masculine pronouns„ÄÇBut of course„ÄÇ

 it's not only an issue with the dataÔºå but also with the models„ÄÇ

 this is something we'll talk about in the final unit of this lecture„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_64.png)

Okay and then there's many other tasks that you can play around with here„ÄÇ

 so I think this has already gone on far too long so I will stop hereÔºå but yeah„ÄÇ

 look out for the homework and next video to be released on Wednesday„ÄÇ



![](img/9f6f35569e3aea3c81a2ade7981caabe_66.png)

All right„ÄÇ