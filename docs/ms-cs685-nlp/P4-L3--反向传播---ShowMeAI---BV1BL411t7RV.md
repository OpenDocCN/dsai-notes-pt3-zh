# P4ÔºöL3- ÂèçÂêë‰º†Êí≠ - ShowMeAI - BV1BL411t7RV

Okay hey everyoneÔºå so today we're going to be doing something a little different Our topic for the day is back propagation which is an algorithm we use to train our neural networks and we've been talking about neural language models so we're going to look more specifically in that context today but instead of me talking over a bunch of slides I've got this iPad weird and I'm going to be writing on it instead of the standard lecturing so this is my first time doing this I hope it is somewhat watchable let me know on Piazza if I should continue using this format going forward or stick to slides or maybe do some combination of both„ÄÇ



![](img/3c45fb4d9857b998e509767f9953972a_1.png)

![](img/3c45fb4d9857b998e509767f9953972a_2.png)

OkayÔºå so let's kind of start with an overview of what we're going to talk about today„ÄÇ

 so first brief review of neural language models focusing on things like what are the parameters and this is going to kind of frame the discussion of gradient descent and back prop later„ÄÇ

NextÔºå we're going to talk about gradient disscentÔºå which is an algorithm that we use to train our neural networks and of course many other machine learning models„ÄÇ

 for those of you who have taken a machine learning class before you are certainly familiar with this concept of gradient descent for others will briefly review the algorithm here„ÄÇ

 and you should also plan to read more about it on your own if after you watch this video you're still confused„ÄÇ

The next thing we're going to talk about is back propagation„ÄÇ

 which is really a way to compute the gradients and we're going to go over this in both a single neuron setting„ÄÇ

 which makes it easy to explain the intuitionÔºå as well as a back prop„ÄÇOf a linear layer„ÄÇ

Which we've seen before in our neural language models„ÄÇ

 the linear layer used to project the concatenated word embedding sequence into a hidden space„ÄÇSo„ÄÇ

These two concepts are used to„ÄÇTrain„ÄÇAnd endsÔºå neural networksÔºå and in our context„ÄÇ

 we're going to use it to adjust the parameters of our neural language model such that it makes better predictions of what the next word is„ÄÇ

AllrightÔºå so that's a quick overview Now let's briefly review that feed forward fixed window language model and just use it to identify things like the model parameters which we're eventually going to be computing gradients on so if you remember we had this prefix from last time and we started with these word embeddings for each word in the prefix in our feed forward fixed window language model we concatenated them all together and we passed that through a hidden layer then we used the resulting representation to predict the word books and we did this using the Somax layer so we went through the forward propagation phase of this model last time„ÄÇ

 but now I wanted to kind of emphasizeÔºåüòäÔºåWhat components of this model are the actual parameters So remember we have the word embedding C sub1„ÄÇ

 C sub2Ôºå C sub3Ôºå these are associated with each of the words in the prefix and remember they are lowdial real valued vectors So when we start training this model these embeddings are randomly initialized„ÄÇ

 say they don't have any correspondence with the semantics or syntactic role of the word that they're associated with„ÄÇ

 but we would hope that once we train the model adjust the values in these vectors„ÄÇ

That the vector space of the word embedding starts becoming more meaningful„ÄÇ

So remember when we did our hidden layer projectionÔºå we used this parametermeter matrix W sub1„ÄÇ

 similarly when we did our softm layerÔºå we used this other parameter matrix W sub2„ÄÇ

So when we wrote down the equations of this modelÔºå it looked kind of like this„ÄÇ

 We had H equals F of W sub1 times C1„ÄÇC 2Ôºå C3„ÄÇ So this is the concatenated representation„ÄÇ

 and let's say we had OÔºå which is our output probabilityÔºå which is the softmax of W sub2 times H„ÄÇ

RightÔºå so here note that we have two intermediate variables we've got let me use a different color„ÄÇ

 we have H and OÔºå so H and O are not model parameters„ÄÇüòäÔºåbig use the same color„ÄÇ

 This color thing is pretty cool„ÄÇ So H and OÔºå OÔºå is that readable maybe„ÄÇOr„ÄÇIntermediate„ÄÇA yous„ÄÇNot„ÄÇ

Parameterters„ÄÇIf you look at these equationsÔºå they are functions of the model parameters„ÄÇÂóØ„ÄÇWhich are„ÄÇ

I'm probably going to go overboard on the highlighting apologies in advance„ÄÇOh's not the red color„ÄÇ

 but these the highlighted things are the model parameters„ÄÇ

 So the highlighted things are what we're going to adjust and what we eventually care about is that the output of this softm layer right this probability distribution O is more in line with what we would expect to predict„ÄÇ

 So for this prefix maybe be assigning a high probability to books„ÄÇüòäÔºåOkayÔºå and so here„ÄÇ

We've got our model parameters„ÄÇ2„ÄÇM„ÅÆ„ÄÇRrims„ÄÇNowÔºå how do we train them„ÄÇTo yield„ÄÇGood„ÄÇPredictions„ÄÇ

Apologies for my handwritingÔºå you're just going to have to deal with that for this lecture and also other lectures in which I use this iPad„ÄÇ

So this is a questionÔºå the answer is gradient descent„ÄÇSo„ÄÇUm„ÄÇ

 we are going to look at gradient descent and it's kind of broken down into three different steps„ÄÇ

So the first step is we need some way of determining how good our current set of parameters is at predicting the next word„ÄÇ

 rightÔºå So we need to measure that somehow„ÄÇ and we're going to do that by defining a loss function„ÄÇüòä„ÄÇ

We'll call it L„ÄÇAnd L is a function of our model parameters„ÄÇ

 We commonly use theta to refer to all of our model parameters„ÄÇ

 So if I look at the example from aboveÔºå we highlighted all of these model parameters for our neural language model„ÄÇ

 So for NLMÔºå we had W1Ôºå W sub2Ôºå C1„ÄÇüòäÔºåTo how many other words are in our vocabulary right so the C's are the word embeddings so that theta in this example would correspond to all of these parameters„ÄÇ

 this is supposed to be an arrow„ÄÇBut I'm writing this now more generally„ÄÇ

 this the could be the parameters of any model„ÄÇAnd this loss functionÔºå L theta is going to measure„ÄÇ

How bad„ÄÇÂóØ„ÄÇCurrent„ÄÇMoel„ÄÇIs at in our neural language model exampleÔºå predicting the next word„ÄÇ

But this this objective could be different for different tasksÔºå right„ÄÇ

 it could be you know answering a question or predicting the sentiment or so on„ÄÇ

 You just need some scalar value that defines the current performance of the model„ÄÇOkay„ÄÇ

 so once you have a way to measure how well your model is doingÔºå the next step is to calculate„ÄÇ

The gradient„ÄÇOf this loss function with respect to the model parameters„ÄÇ

So we're going to use this notation DLD theta to denote the gradient„ÄÇ

 and we're going to use it for all of the partial derivatives that this gradient encompasses in the future„ÄÇ

OkayÔºå so what exactly is the gradient for those of you who are unfamiliar with it„ÄÇ

 it basically tells us so I have the scalar loss function and let's say I have a particular parameter in my model if I change that parameter if I increase that parameter how much will the loss function increase and in general the gradient is giving us the direction of steepest ascent with respect to all of the model parameters so let me write that down„ÄÇ

üòäÔºåThe gradient„ÄÇI meanÔºå intuitivelyÔºå it tells us„ÄÇHow the loss„ÄÇChanges„ÄÇWhen„ÄÇWe modify„ÄÇThe model„ÄÇPms„ÄÇ

Theta and importantlyÔºå it gives us direction„ÄÇSepest„ÄÇTheent„ÄÇIs this important so we will„ÄÇI write it„ÄÇ

OkayÔºå so I'll give you an intuitive example of this in a bit with a very„ÄÇYou knowÔºå simple function„ÄÇ

 but before I do thatÔºå so we defined our loss functionÔºå we've computed this gradient„ÄÇ

 How do we use the gradient„ÄÇ What we're going to do is take a step„ÄÇIn the direction„ÄÇOf the negative„ÄÇ

Graient„ÄÇSo remember that if the gradient is giving us the direction of steepest ascent„ÄÇ

 so increasing the lossÔºå we know that the loss is telling us how bad the model is currently doing right so we want to actually minimize this loss function„ÄÇ

 so going in the direction of the negative gradient will take us in a direction that minimizes this loss function„ÄÇ

üòäÔºåSo do„ÄÇMinimizing„ÄÇThe loss„ÄÇSo what does it mean to take a step in the direction of the negative gradientÔºü

So let's say that our model parameters were theta old„ÄÇ

 these are our current model parameters and we want to get a new set of parameters theta new that„ÄÇ

Have that result in a lower loss„ÄÇSo to do thisÔºå we're going to take our old parameters and we are going to subtract this eda multiplied by the gradient„ÄÇ

And so what is Ada hereÔºå Eda is a variable that corresponds to something called a learning rate„ÄÇ

 which controls„ÄÇThe step sizeÔºå so how much of a step we want to make in the direction of the negative gradient„ÄÇ

Seems highlighter worthy as well„ÄÇOkayÔºå so that was a lot of information let's look at a simple example that'll hopefully we make it more clear so let's just look in a two dimensional plot I have one parameter and one loss function sorry I have obviously a scalar loss function so we have our loss on the y axis L sub theta and we have theta here so let's say theta is also a scalar here„ÄÇ

üòäÔºåAnd let's say the loss function looks something like this„ÄÇ

So this is a convex function in practice we won't be dealing with functions like this„ÄÇ

 but we're still going to use gradient descent this is just for demonstration„ÄÇ

All right let me pick a different colorÔºå so let's assume that I randomly initialize theta„ÄÇ

 so I have no idea before starting training what the best value of theta is that minimizes this loss function„ÄÇ

So maybe I start up here„ÄÇÂóØ„ÄÇSay this is random„ÄÇInitialization„ÄÇ

So I set my theta to sum value over hereÔºå I compute the loss function over my training data and I find that it's pretty high„ÄÇ

So now I'm going to compute the gradient right and so I compute the gradient at this point„ÄÇ

 it's this line hereÔºå this tangent lineÔºå and I decide to go in this direction right„ÄÇ

 the direction of steepest descentÔºå So maybe I take a step in this direction„ÄÇ

And I end up over here with my next when I use this update function„ÄÇ So this one was theta 0„ÄÇ

 our random initialization„ÄÇ This one is theta 1 after one step„ÄÇ

 And this direction here was given to us by D LÔºå D theta 0„ÄÇüòäÔºåOkay„ÄÇ

 so now we're going to just repeat this process„ÄÇSo I'm going to compute a new gradientÔºå go over here„ÄÇ

 compute a new gradientÔºå go over hereÔºå and eventually maybe I finally reach the bottom here„ÄÇ

 the minimum of this functionÔºå and I claim that I'm doneÔºå I've reached the minimum„ÄÇÂóØ„ÄÇ

 so my goal is to get to this point„ÄÇAnd the way in which I'm going to do that is by computing these gradients„ÄÇ

 taking a step in the direction of the negative gradient where this learning rate value controls how far I step so as an example of what happens if I have a big learning rate„ÄÇ

 imagine that if I had a huge learning rate and I started back at this point„ÄÇ

 the randomly initialized pointÔºå I do run into the danger of completely overshooting the the minimum and ending up on the other side here and that's obviously not good and if my learning rate is too large I might see in„ÄÇ

üòäÔºåAKind of„ÄÇZigzag pattern of my loss function where eventually I might get to the minimum or I could just overshoot it completely and go somewhere else in the perimeter space„ÄÇ

 which results in a higher loss„ÄÇ Maybe I'll get rid of these„ÄÇ

 That was just a demonstration of what happens if your learning rate is too high„ÄÇ So in general„ÄÇ

 a learning rate is part of your model hyper parametersmeter„ÄÇÊòØ„ÄÇ

So a hyperparameter refers to something that is not learned by the model„ÄÇ

 but is rather something that you as the designer of the model set before the training process even starts in general you might have to train multiple different models using different hyperparameters before you can find one that maybe results in the smallest loss„ÄÇ

So hyperparameters are tuned„ÄÇI mean in most casesÔºå nowadays„ÄÇ

 they're I think still chosen roughly manuallyÔºå people who have a lot of experience training neural networks tend to know what range of hyperparameters tend to work best for given problems„ÄÇ

 there's also a whole research field on optimizing these hyperparameters automatically„ÄÇ

 but we're not going to talk about that in this class„ÄÇOkayÔºå so right„ÄÇ

 going back to this you might wonderÔºå well for such a simple function„ÄÇ

 can't I just get to the minimum compute the minimum in a closed form and not have to go through this whole process of iteratively computing the gradient and adjusting my model parameters„ÄÇ

 this is true for simple models like least squares as a famous example that have closed form solutions„ÄÇ

 but in practice when you're dealing with these neural networks that have know highly nonlinear relationships between inputs and outputs we went over that last time there is no closed form solution for computing the best set of parameters all of the loss functions we deal with our nonconvex and we still use gradient descent to approximate the minimum„ÄÇ

 so none of the methods we're using guarantee that we're going to reach the global minimum of„ÄÇüòä„ÄÇ

The set of parameters optimize that reaches the global minimumÔºå but in practice these methods„ÄÇ

 gradient descent seems to give you a reasonable solution in most cases„ÄÇ

I should also mention that this update rule here is just vanilla gradient descent in practice there are several more advanced optimizers„ÄÇ

 things like Adam or Ada grad that people might have seen if you've„ÄÇüòä„ÄÇ

Dealt with neural networks beforeÔºå they're all kind of just variants on this equation with clever methods of determining step sizes adaptively and so on„ÄÇ

Okay„ÄÇSo that is the high level of gradient descent„ÄÇ

 so hopefully at this point you understand why we are computing these partial derivatives„ÄÇ

 it's because they tell us the direction of steepest asscent and then why we're stepping in the negative direction of that so that we can minimize our loss function so hopefully that's all clear if not„ÄÇ

 I'd encourage you to do more reading on just general machine learning materials and maybe I'll put some links to those on Piazza or the website„ÄÇ

üòäÔºåOkayÔºå so now that we've gone over at least this high level process of gradient descent„ÄÇ

 let's look at a concrete exampleÔºå this one is going to be fairly straightforward„ÄÇ

 but this is also going to serve to introduce back propagation„ÄÇ

 which is a method that we use to more efficiently compute the gradient„ÄÇüòä„ÄÇ

So we'll look at this in a single neuron exampleÔºå so we'll be dealing with completely with scalrs„ÄÇ

 no matrices or vectors in this example just to make things easy„ÄÇ

So let's depart a little bit from our neural language model framework and because that's kind of hard to do in this toy setting instead„ÄÇ

 let's just say we have a generic problem where we have some input„ÄÇüòäÔºåOur input is a single number„ÄÇ

We multiply that number by some scalar weightÔºå again„ÄÇ

 just a single number to get some hidden representation„ÄÇÂóØ„ÄÇ

Then we multiply that with another weight W sub2 to get a scalar output„ÄÇAll right„ÄÇ

 so one question that you should be able to answer at this point or is sorry„ÄÇ

 what are the model parametersÔºüOkayÔºå so in this caseÔºå we have just two model parameters„ÄÇ

We have W sub1 and W sub2 x is our inputÔºå so x in this case is not a word embedding that's learned by the model X is just a number that we observe in our training data set„ÄÇ

And so maybe I'll make that more clear„ÄÇOkay parameters are W1Ôºå W2 inputs„ÄÇOr look like„ÄÇX Y pairs„ÄÇ

 so X is a single number input and y is the target output in our training data set„ÄÇ

 So we're just trying to learn a function to map one number to another number and our loss function is going to be defined by how close is the output O to the target output Y„ÄÇ

But before we get into thatÔºå let's look at how we compute these intermediate variables„ÄÇ

 so let's define this first equation as tan0 H of W1x„ÄÇ

 so here we're introducing a new nonlinear function„ÄÇ

 tan0 H is a common activation function used in recurrent neural networks„ÄÇ

 something that you should be familiar with„ÄÇOkayÔºå and let's sayÔºå oh also uses a10 H of this product„ÄÇ

So again here note that we have two intermediate variables right we have H and O„ÄÇ

 these are not our model parametersÔºå they're intermediate variables„ÄÇ

 we don't have to compute derivatives with respect to H and O specifically to do our model updates right our model updates our theta is just W1 and W2„ÄÇ

OkayÔºå so let's state this again a little more concretely just to make the training situation more clear„ÄÇ

 so let's say assume„ÄÇÂëÉ in„ÄÇWÔºå that was terrible„ÄÇAll rightÔºå inputs„ÄÇCar„ÄÇXÔºå Y„ÄÇScalar„ÄÇPears„ÄÇAnd our goal„ÄÇ

 we want„ÄÇTo predict„ÄÇWhy„ÄÇFrom X„ÄÇSo what do we do here„ÄÇ

 The first step is something that we've seen beforeÔºå it's called forward propagation„ÄÇ

So given an outputÔºå sorryÔºå an input X„ÄÇYeah go„ÄÇDifferent highlight„ÄÇSo given„ÄÇMaybe I'll do this too„ÄÇ

So given an X in our training data setÔºå we will first compute the value of each neuron„ÄÇ

So here I'm using the term neuron to refer to these three like nodes in this graph„ÄÇSo the input„ÄÇ

 the intermediate and the intermediate variables hereÔºå O is actually our output„ÄÇ

Compe the value of each„ÄÇWhat is it trying to say each neuronÔºüÂóØ„ÄÇSo these are H and O„ÄÇUsing„ÄÇ

The model equations„ÄÇOkayÔºå so let us make that more clear„ÄÇWhat are the model equations„ÄÇ

 there are these two things„ÄÇOkayÔºå and nextÔºå what are we going to doÔºå We're going to„ÄÇPredict„ÄÇ

The value„ÄÇOf whyÔºå actuallyÔºå this is not really a separate step„ÄÇ but we did it in the first„ÄÇ

 but the first thingÔºå but our O value is going to be the model's prediction of y„ÄÇÂóØ„ÄÇAnd finally„ÄÇ

 what do we wantÔºå what is our goalÔºüÂì•„ÄÇMake„ÄÇOh as close„ÄÇTo y as possible„ÄÇOkay„ÄÇ

So this is all stuff that we saw from beforeÔºå let's look at step twoÔºå step„ÄÇ2„ÄÇCompute loss function„ÄÇ

So this is going to tell us„ÄÇI don't even know why I'm doing these highlights„ÄÇ

 So this is going to tell us is going to measure that closenessÔºå right„ÄÇ

 So since we're in the simplified caseÔºå we'll just use the square loss„ÄÇüòäÔºåIn this example„ÄÇ

For simplicity„ÄÇIn the next videoÔºå we're going to look at the loss function used for neural language models„ÄÇ

 which is a cross entropy loss„ÄÇ but here we'll stick to something simple„ÄÇ

The square loss is just defined as follows our ground truth minus our predictions squared„ÄÇ

So obviouslyÔºå if O and y are close to each otherÔºå then this loss function is going to be low„ÄÇ

 If O is a terrible prediction of yÔºå then this loss will be high„ÄÇ So againÔºå O is the prediction„ÄÇ

Why is the targetÔºüAnd we've observed why in our training data set„ÄÇOkayÔºå so now the funds„ÄÇ

You can call it fun starts as an asideÔºå when I was doing my PhD„ÄÇ

 we didn't have all these fancy libraries that automatically compute gradients„ÄÇ

 So most of my research time was spent implementing„ÄÇ

partial derivatives of various complicated neural networks„ÄÇ

 but nowadays you don't have to do all of that because there's tools that do it automatically„ÄÇ

 which is really awesome and has contributed a lot to advancing research in NLP over the last couple of years„ÄÇ

So we'll get to that a little bit more in another video where I go through how you actually code up these things in Pytorrch„ÄÇ

OkayÔºå but not to get two off track here in our exampleÔºå we have just two parameters„ÄÇ

 W sub1 and W sub2Ôºå So all we want to do is compute what just happened„ÄÇ1mentÔºå okay„ÄÇ

D L DW sub 1 and D L DW sub 2Ôºå rightÔºå I want to compute these two partial derivatives„ÄÇOkay„ÄÇ

 so let's start with the actual math portion here„ÄÇHow do I compute these two thingsÔºå Let's start„ÄÇ

 let's work our way backwards„ÄÇ This is generally what we do„ÄÇ So going from the output„ÄÇ

 we'll go back like this„ÄÇIn that direction and so the first parameter we encounter here is W sub2„ÄÇ

 so let's start computing that derivative first and then we'll get to W sub1„ÄÇOkay„ÄÇAll right„ÄÇËá™Â∑±„ÄÇ

s to be consistent„ÄÇOkayÔºå so start„ÄÇWell„ÄÇThis one„ÄÇAnd an important concept to note here„ÄÇ

 we've already gone over the chain rule of probability„ÄÇ

 but important in these kinds of computations is the chain„ÄÇRule of calculus„ÄÇ

So for those of you who have forgotten what this is„ÄÇ

 it basically says if I want to differentiate some function„ÄÇ

That itself is this kind of composed function two functions composed together„ÄÇ

 so here if I want to differentiate G of f of x with respect to x„ÄÇ

 this is equivalent to the derivative of G of x with respect to X„ÄÇ

 sorry with respect to F I just don't do„ÄÇüòäÔºåF times the derivative of F with respect to x„ÄÇOkay„ÄÇ

 so this is a basic thing that you likely learned in calculus„ÄÇ

But it's going to be incredibly useful for everything we do„ÄÇAfter this point in the lecture„ÄÇOkay„ÄÇ

 and againÔºå this means„ÄÇThis notation means partial derivative with respect to X„ÄÇOkay„ÄÇ

 so we know our loss function is this„ÄÇAnd we also know that O here is a function of W sub 2„ÄÇOkay„ÄÇ

 so we kind of work our way backÔºå we have this loss functionÔºå we go over here„ÄÇ

 then we see that W2 occurs here„ÄÇSo the first thing we're going to do this easier to read is define an intermediate variable„ÄÇ

 which will make applying the chain rule a little more intuitive„ÄÇWe use the same color„ÄÇWellow„ÄÇWe do„ÄÇ

OkayÔºå as you can seeÔºå I'm still not super„ÄÇConfident with this interface„ÄÇLet's say„ÄÇÂëÉ„ÄÇA equals W2 H„ÄÇ

 where a is an intermediate variable„ÄÇNow we can rewrite„ÄÇThis equation as o equals 10 h of a„ÄÇÂóØ„ÄÇ

AllrightÔºå so now let's go through all of this and work our way back and apply the chain rule to actually compute this partial derivative DL Dw2 equals the first thing we encounter is this O right we know O is a function of W sub2 so we're going to do D O and but remember that O is actually two functions right it's this W sub2 times H and then the tan0 H applied on top of that so we introduce this intermediate variable A we're going to take the derivative with respect to A and then finally take the derivative with respect to W sub2„ÄÇ

This whole process„ÄÇÂóØ„ÄÇWe use the chain rule to get these equations„ÄÇ

 if you didn't follow that you should go over this to completely understand what is happening here„ÄÇ

All rightÔºå so let's start with this derivative here„ÄÇD LD OÔºå if you look at L„ÄÇ

 this is pretty straightforwardÔºå rightÔºå we're going to take this exponent„ÄÇ

 multiply it with the one half„ÄÇ The one half is essentially there to just get rid of this constant term„ÄÇ

 We see that the O is also negated here„ÄÇ So the derivative„ÄÇTotal is just this„ÄÇAnd now D O D AÔºå right„ÄÇ

 So thisÔºå we're not going to actually derive„ÄÇ This is just a little thing that„ÄÇYou should know„ÄÇ

Is that derivative of xÔºå the derivative 10 h of x with respect to x is 1 minus 10 h squared x„ÄÇ

Wait this„ÄÇSo a nice property of this function 10 H is that its derivative is it includes the computation of the original function„ÄÇ

 so there's no need for me to compute 10 h of x again to compute the derivative because all I can do all I have to do is just square it square the original computation„ÄÇ

So based on this equationÔºå all I have to do is1 minus o squared so you can see how I didn't need to do anything fancy here„ÄÇ

 I already computed O in the forward pass and now I just do1 minus o squared in the backward pass to compute this derivative and finally D A Dw2 we know that a is just W2 times H so the partial derivative a with respect to W sub2 is H„ÄÇ

üòäÔºåSo there you goÔºå this is the full derivative of„ÄÇThe loss function with respect to W sub2„ÄÇAll right„ÄÇ

 so we also have another function rightÔºå we have DL DW sub1„ÄÇAnd if we look back at our„ÄÇDiagram here„ÄÇ

 we know that W sub1 is used in the computation of H right this initial computation of H„ÄÇ

 so we need to work our way back one more step from where we were previously and this is where the intuition of back prop comes into play„ÄÇ

 So if we work our way back„ÄÇüòäÔºåAlso note that this H again„ÄÇ

 is a kind of composed function of 10 H and the product of W1 and and X„ÄÇ So before we start„ÄÇ

 let's just„ÄÇWhen we do this beforeÔºå yes„ÄÇWe will introduce another intermediate variable„ÄÇEnter„ÄÇMete„ÄÇ

Variable B equals W1 x„ÄÇSo whatever is inside of this 10 H term for computing H„ÄÇÁúãÁúã„ÄÇOkay„ÄÇ

 so now that we have intermediate variable A and intermediate variable BÔºå we can just write this out„ÄÇ

 it's EL DÔºå OÔºå DÔºå OÔºå D A„ÄÇSo we're at this point and now we see that a is W2 H„ÄÇ

 but this time we don't want to differentiate with respect to W sub2 right because we know that H is itself a function of W sub1„ÄÇ

 so here we depart from what we did previously and we do this„ÄÇ

And now we know that H is made up of this intermediate variable„ÄÇ

 which itself includes this computation of Dw sub1„ÄÇ

So if I were to compute all of these individual derivatives and multiply them together„ÄÇ

 I would get the partial derivative of L with respect to W sub1„ÄÇOkayÔºå so the key point in backpro„ÄÇ

 one of the key points is that if you look at these two equations here„ÄÇ

 there are some common terms rightÔºå so specifically these two terms are shared for both computations„ÄÇ

ThereforeÔºå I can cache this from„ÄÇThe computation of DL„ÄÇ

 D W sub2 and all I have to do is compute these terms„ÄÇSo you can probably see how if I have a very„ÄÇ

 very long network with many different functions and layers and so on how thistuition helps right because if I can cache a large percentage of the terms in this product„ÄÇ

 then I don't have to do those computations from scratch anymore„ÄÇ

 I save a lot of time and so this is one of the intuitions behind back propagation we start from the very top of the network„ÄÇ

 the output layer and work our way backwards so that we are able to use the cache derivatives of what I've just computed to speedily compute a of a perimeter that's used further down in the network„ÄÇ

üòäÔºåSo I'll write that again„ÄÇWhat is that„ÄÇOh noÔºå my pen has failed to write„ÄÇÂóØ„ÄÇOÔºå okay„ÄÇOkay„ÄÇ

 so intuition„ÄÇOr„ÄÇBack„ÄÇProrop„ÄÇCash„ÄÇDeririvatives„ÄÇThat appear„ÄÇPigher up„ÄÇIn the plant„ÄÇAnd„ÄÇUse the„ÄÇ

To compute„ÄÇLower level„ÄÇGs„ÄÇWhen I say things like high level and low level„ÄÇ

 I'm referring specifically to essentially the distance from the output layer„ÄÇ

 so things that are close to the output layer are up high and things that are farther down are low level So in our neural language model„ÄÇ

 the most low level parameters are the word embeddings themselves right they're the furthest away from the end word prediction„ÄÇ

üòäÔºåOkayÔºå so then just as a example of the parameter update step„ÄÇRemember from before„ÄÇ

 in my gradient descent thing„ÄÇÂóØ„ÄÇWhere is thatÔºüPs„ÄÇHere I have this update equationÔºå right„ÄÇ

 So how do I apply this update equation down hereÔºå NowÔºå it's pretty straightforwardÔºå right„ÄÇ

 now that I've computed these partial derivativesÔºå I can make a new W sub1 by taking the old value of it subtracting„ÄÇ

üòäÔºåOhÔºå whoopsÔºå that's not the right way to write an Eda learning rate times this partial derivative„ÄÇ

 similarly„ÄÇWord2 new is word2 old minus„ÄÇBLDÁ≠â‰∫é37„ÄÇSo this is the parameter update stage„ÄÇOkay„ÄÇ

 so at least this part of I just want to make sure I didn incorrectly okay„ÄÇSo at least to this point„ÄÇ

 things should be fairly clearÔºå we've dealt with a very simple example and the derivatives are also fairly straightforward if you're having some issues with this part„ÄÇ

 you should almost certainly spend the time to understand it because this is essentially the core algorithm that all of the models will be discussing for the rest of the semester use for training„ÄÇ

üòäÔºåOkayÔºå so for the last part of this lecture videoÔºå I know it's already kind of long„ÄÇ

 hopefully I can get through this in not too much time„ÄÇ

 I wanted to give some intuition on how this works when we're not talking about single neurons anymore„ÄÇ

But we're„ÄÇTalking about multiple neurons„ÄÇ So this is much closer to the setting of our neural language model„ÄÇ

 So specifically hereÔºå we're going to move away from these scalrs to the matrix vector notation that we saw„ÄÇ

In the slides from last time„ÄÇSo for this partÔºå I just„ÄÇ

 I'm not going to derive the gradient for like the fixed window neural language model„ÄÇ

 I wanted to go over just how do we derive the partial derivative for a very commonly used component in these models the linear layer which we already saw in that model„ÄÇ

So„ÄÇLinear„ÄÇWhere„ÄÇSimple„ÄÇComponent„ÄÇÂëÉ„ÄÇAnd ends„ÄÇIt used in the neural language model to project that concatenated representation to the hidden space„ÄÇ

 and we're going to look at the definition as follows„ÄÇÂëÉÂëÉ„ÄÇHereÔºå W is our weight matrix„ÄÇ

And X is a matrix of a training example„ÄÇ So this also will serve to introduce the concept of batched training„ÄÇ

So here we'll call this the input matrixÔºå I guess„ÄÇOkayÔºå so now we no longer have scalrs„ÄÇ

 both of these x and W things here are matricesÔºå but W is the parameter that we're interested in„ÄÇ

Okay„ÄÇSo I guess I'll just highlight that linear layer I'm talking about this„ÄÇ

 this is also called feed forward layer or fully connected layer„ÄÇIf so on„ÄÇOkay„ÄÇ

 you also might see this with the order of the product switch aroundÔºå so w of WxÔºå sorry„ÄÇ

 y equals F of WxÔºå but we're going to go with this because it makes the derivation a little more straightforward„ÄÇ

But yeahÔºå in practice this general concept applies in general So what do I mean by input matrix„ÄÇ

 this is a concept that we haven't talked about to this point„ÄÇEhÔºå why does my pen keep breaking„ÄÇ

Start working again„ÄÇOkayÔºå so what do I mean by input matrixÔºå rightÔºå what does this containÔºüSo I„ÄÇ

 I meanÔºå that each row„ÄÇOf X contains„ÄÇA vector„ÄÇIt corresponds„ÄÇTo us„ÄÇSingle„ÄÇTraining example„ÄÇ

AndThis whole collection of examples that occurs in X is called a batch„ÄÇ

So that is some terminology that„ÄÇWe'll see over and over againÔºå so in modern neural networks„ÄÇ

 we don't like we did up here„ÄÇPut single inputs through a model at a time„ÄÇ

 So instead we feed a batch of examples through the model„ÄÇ

 estimate the gradient on that entire batch of inputs and then do the update So this is often referred to as mini batch gradient descent or batch gradient descent and another important hyperparameter here is the batch size of the model„ÄÇ

 which means how many examples are you going to have per batch„ÄÇ

 this is going to determine the number of rows in your X matrix„ÄÇüòäÔºåSo what are these vectors„ÄÇ

 it kind of depends on where in the model you areÔºå where this linear layer is in our neural language model example„ÄÇ

 we use a linear layer to project the concatenated representation of the embeddings into some hidden space so in this case the x matrix could contain concatenated representations for a bunch of different prefixes so each row contains a concatenated representation of a different training prefix„ÄÇ

üòäÔºåOkayÔºå so let's introduce some variables hereÔºå N is a number of examples„ÄÇOr the„ÄÇBatch size„ÄÇ

Let's say D is the dimensionality„ÄÇOf the„ÄÇWhatever is in the xÔºå the dimensionality of the vectors„ÄÇ

 will'll just say dimensionality of hidden space„ÄÇAnd„ÄÇOkayÔºå let's say„ÄÇAll right„ÄÇ

 maybe we'll make this more similar to the example and say D is a dimensionality of„ÄÇ

Ebeddings and M is dimensionality„ÄÇOf hidden„ÄÇThe actual„ÄÇFor this example„ÄÇ

 this these two don't matter this is a more general example„ÄÇ So let's say we have a toy example„ÄÇ

Where N is2Ôºå D is also2Ôºå and M is 3„ÄÇ So this example is inspired by one of Justin Johnson's notes„ÄÇ

 He's a professor in computer vision at Michigan„ÄÇBut I think this is a great example to demonstrate some of the tricks that are incorporated into back propagation when you go from scalarrs to the matrices„ÄÇ

OkayÔºå so let's say we have„ÄÇA matrix XÔºå in our caseÔºå X„ÄÇ

 the input matrix is a number of examples in the batchÔºå which is n by the dimensionality ofÔºå say„ÄÇ

 the embeddings is could just be the input„ÄÇ So it's a two by two matrix„ÄÇ

 And if we write out each element of the matrixÔºå we get something like this that's a2„ÄÇüòä„ÄÇ

Let's also say we have a W matrix„ÄÇ So this W is trying to project these x vectors into this hidden space of M dimensions„ÄÇ

 ThereforeÔºå W is a 2 by three matrix„ÄÇ So it looks like„ÄÇ1Ôºå1Ôºå W1Ôºå2Ôºå W 1Ôºå3Ôºå W 2Ôºå1„ÄÇW2Ôºå2„ÄÇW2Ôºå3„ÄÇ

This is the two„ÄÇ OkayÔºå And let's say just to make things easy that this F is going to be an identity function„ÄÇ

F of x equals x„ÄÇ So we're not going to have to deal with chain ruling another nonlinear function„ÄÇ

OkayÔºå so we're interested in this yÔºå rightÔºå Y is the output and what does y look like since it's the product of x and W„ÄÇ

ÂóØ„ÄÇIf we write out the terms and yÔºå first of allÔºå what is its dimensionality„ÄÇ

 you've now seen this in homework 0 rightÔºå we're taking this two by two matrix multiplying it by a  two by three matrix„ÄÇ

 therefore the resulting matrix is also a two by three matrix„ÄÇüòä„ÄÇ

So when we compute this product we get x11 times w11 right these two plus x12 w21 and this is the first element of our matrix here„ÄÇ

 so let's look at the second so here we are multiplying this row by this column sorry this first row by this column„ÄÇ

So it's x1Ôºå1Ôºå W1Ôºå2 plus x1Ôºå2Ôºå W22Ôºå and so on„ÄÇAnd at the bottom„ÄÇ

 let's just fill in the last cell hereÔºå it's x21Ôºå W1Ôºå3 plus x2„ÄÇX2Ôºå2Ôºå W2Ôºå3„ÄÇOkay„ÄÇ

 so this is what Y actually looks likeÔºå rightÔºüAnd let's„ÄÇAssume„ÄÇWe're given„ÄÇÂóØ„ÄÇD LÔºå DY„ÄÇ

So this is the partial derivative of some loss function with respect to why so we're ignoring all of the rest of the computations in this network„ÄÇ

 let's just assume we've already computed this through backpro and now we're at the point„ÄÇ

And let's say we want to compute„ÄÇWant to compute„ÄÇI meanÔºå there are two things of interest here„ÄÇ

 DLDX and DLDW„ÄÇRightÔºüHow do we do thisÔºüLet let's look at this one first„ÄÇ

 we know through the chain rule that DLDX simply is DLDYÔºå which we said we already had„ÄÇ

 and we multiply that by DY Dx„ÄÇRightÔºå so this follows from the chain rule and this term here is called the Jacobian„ÄÇ

So the Jacobian matrix is essentially every element of y differentiated with respect to every element of x„ÄÇ

 And so in practiceÔºå actually forming the Jacobian matrix can be impracticalÔºå rightÔºü

 Let's think about the size of this matrix since it's every element of yÔºå right„ÄÇ

 How many things are in Y„ÄÇ Y is an N by M matrixÔºå and then every element of X„ÄÇ

 which is an N by D matrix„ÄÇüòäÔºåRight so you can imagine on the scale at which modern neural networks neural language models are trained at„ÄÇ

 we don't want to be forming this kind of matrix where we're going to run out of memory pretty fast with deeper models and so the whole point of this current exercise is to show you how you can compute DLDX without explicitly forming DYDX„ÄÇ

ÂëÉ„ÄÇOops„ÄÇSorryÔºå the per is acting up„ÄÇÂæàÁñº„ÄÇOkayÔºå there it is„ÄÇI guess I will edit that out„ÄÇ

 but the point isÔºå we can compute„ÄÇÂóØ„ÄÇDÔºå LÔºå TÔºå XÔºå without„ÄÇForming„ÄÇDÔºå Y D XÔºå of course„ÄÇ

 to demonstrate this because in this toy exampleÔºå it is feasible to look at this Jacobian matrix„ÄÇ

 Let's just proceed by forming D Y D X„ÄÇ And then this will give you some intuition on why we don't have to form it in the future„ÄÇ

So DL Dx equals so remember this is what we want to compute this is our goal„ÄÇ first of all„ÄÇ

 the partial derivative of the loss with respect to sum perimeter is going to be the same dimensionality as that perimeter so since x here is a2 by2 matrix DL Dx is also going to be a2 by2 matrix and we can explicitly represent each element of this matrix with these scalrs„ÄÇ

DÔºå LÔºå DÔºå X1Ôºå1Ôºå DÔºå LÔºå DÔºå X 1Ôºå2Ôºå and so on„ÄÇSo we're interested in computing these four elements and let's look at what actually happens if you want to individually compute each of these four numbers„ÄÇ

So we can start with this oneÔºå DLÔºå D X 11„ÄÇAnd so we know from our equation up here that this is the same as a multiplication of„ÄÇ

DL DY times D Y D X 11„ÄÇ so note that L is a scalar here and x is a scalar„ÄÇ

 so this whole thing is also a scalar and we want a way to get a scalar out of this and we know that DL DY is going to be a3 by2 sorry a 2 by 3 matrix because it's the same shape as y and we can also write this term here as a 2 by3 matrix and compute this this scalar with a element wise product of these two„ÄÇ

I'm sorryÔºå the dot product„ÄÇ So if we go over here„ÄÇLet's expand out what this term looks like„ÄÇ

 I said we can represent it as a two by3 matrix„ÄÇ So if you look at the elements of y„ÄÇ

 which we wrote up hereÔºå you'll see that we have if we're differentiating with respect to sorry x11 this is an x11 and this is an x11 and it also occurs here„ÄÇ

 So the occurrences of x sub11 are all in the top row of this y matrix and you can look at what is being multiplied with x11 to get an idea of what the derivative is„ÄÇ

 So here it's w11 here W12 and in this caseÔºå W13„ÄÇSo we get W1Ôºå1Ôºå W1Ôºå2Ôºå and W1Ôºå3„ÄÇ

And in the second rowÔºå note that there are no occurrences of„ÄÇX1Ôºå1 right now we're in the second row„ÄÇ

 so we're looking at x21 instead or x22Ôºå x23Ôºå so all of these terms are0„ÄÇOkay„ÄÇ

 so if we do the same exercise for xÔºå say1Ôºå two„ÄÇÂóØ„ÄÇWe„ÄÇCan expand out this x12 term here„ÄÇ

 and it looks like this„ÄÇOopsÔºå that is it2 on2„ÄÇThese are all zerosÔºå similarlyÔºå if we do„ÄÇ

Let's look at one of them on the bottom row„ÄÇ So x 21„ÄÇAnd this term looks like this„ÄÇ

 So here the top elements are0„ÄÇThe bottom elements are this„ÄÇAnd finallyÔºå let's just not even„ÄÇWell„ÄÇ

 I guess we can do the last one„ÄÇAnd what is this thing„ÄÇIt's againÔºå zeros in the top„ÄÇ

 but we have this on the bottom„ÄÇSo„ÄÇWe've computed these elements of the Jacobian„ÄÇOkay„ÄÇ

 so we've looked at the second term of this product rightÔºå of these products„ÄÇ

 these elements of the Jacobian„ÄÇ Now let's look at what this isÔºå this DLDY„ÄÇ

So remember that we're given thisÔºå rightÔºå so we can also just break this down into a bunch of different elements„ÄÇ

TheLDY11Ôºå rememberÔºå this is going to be a two by three matrix„ÄÇBecause y is a two by three matrixÔºü

This one2Ôºå1„ÄÇ6„ÄÇTo„ÄÇAll rightÔºå so now we're just going to take the dot product of these two things„ÄÇÂóØ„ÄÇ

So we have this up here„ÄÇAnd if we do DÔºå LÔºå DÔºå Y times DÔºå YÔºå DÔºå X 1Ôºå1Ôºå we get„ÄÇDÔºå LÔºå DÔºå Y 1„ÄÇ

1 times W 1Ôºå1„ÄÇ rightÔºå This the first element here„ÄÇÂóØ„ÄÇAnd then we're going to add that to D LÔºå D Y 1„ÄÇ

2 times W 1Ôºå2 plus D L„ÄÇEÔºå byÔºå1Ôºå3„ÄÇW1Ôºå3„ÄÇOkayÔºå so at this point„ÄÇ

 we can substitute this into the first element of our DLDX matrix„ÄÇ

 so let's make that more clear with some highlighting so this term here„ÄÇ



![](img/3c45fb4d9857b998e509767f9953972a_4.png)

Is now replaceable with this term here„ÄÇSo if we rewrite now this DLDx„ÄÇUsing what we've just computed„ÄÇ

 firstÔºå we'll plug in the screen part„ÄÇAnd then the second element it kind of just follows we're now using this matrix for the Dx sub12 so that one maybe I can„ÄÇ

Be more economical in how I write it is DY11„ÄÇW 2Ôºå1 plus„ÄÇE DÔºå U I 1Ôºå2Ôºå W 2Ôºå2 plusÔºå okay„ÄÇ

 looks like I can't„ÄÇ3„ÄÇOkayÔºå so similarlyÔºå let's just write the OkayÔºå so this is supposed to be„ÄÇ

AndSo hereÔºå let me just divide these to make it more clear and we can also just write out this one in case you want to go back and make sense of this„ÄÇ

BÔºå Y 2Ôºå2Ôºå W 2Ôºå2 plus DÔºå LÔºå DÔºå Y 2Ôºå3„ÄÇW2Ôºå3Ôºå Larry„ÄÇ That's a little messy„ÄÇOkay„ÄÇ

 so we get this this derivation of DL D X„ÄÇ and we did this by forming these intermediate matrices here„ÄÇ

 But the key intuition hereÔºå the takeaway is that this can just be rewritten„ÄÇüòäÔºåAs matrix„ÄÇYes„ÄÇ

The matrix product„ÄÇÂóØ„ÄÇOf„ÄÇD L DY„ÄÇWhich were given again„ÄÇThe L DY 2Ôºå1Ôºå the L DY 2Ôºå2„ÄÇ

W and the transpose of the weight matrix„ÄÇ So W1Ôºå1Ôºå W21Ôºå W1Ôºå2Ôºå W2Ôºå2Ôºå W2Ôºå2Ôºå W„ÄÇSorryÔºå this is„ÄÇW1Ôºå3Ôºå W2„ÄÇ

3„ÄÇSo if you go back and look at our original weight matrix hereÔºå this matrix is just the transpose„ÄÇ

Over here„ÄÇAnd so„ÄÇJust to be clearÔºå if you look at the first term in this product right this is a two by3 matrix multiplied by 3 by 2 matrix„ÄÇ

 so their end result is a2 by2 matrixÔºå which again is what we want for DLDx and let's look at the first element„ÄÇ

 this thing hereÔºå this is just the dot product between this row vectorÔºå and this column vector„ÄÇ

And the same is true for all of the other values in DLDX„ÄÇ

So the important thing here is that we can just simply rewrite DLDX as DLDY„ÄÇWops„ÄÇ

 times the transpose of the weight matrix„ÄÇSo„ÄÇAnd practice„ÄÇWe do this„ÄÇInstead„ÄÇOf computing„ÄÇ

The Jacoban„ÄÇBecause this is far more efficientÔºå if you want an exerciseÔºå you can compute DLDW„ÄÇ

 the solution is kind of follows from thisÔºå it's the transpose of the x matrix times DLDY„ÄÇOkay„ÄÇ

 so what have we covered todayÔºå todayÔºüWe've covered gradient descent„ÄÇRightÔºåThis is important„ÄÇ

 You should definitely know how this works„ÄÇ You should„ÄÇYou„ÄÇShould„ÄÇBe able„ÄÇTo implement„ÄÇBack prop„ÄÇ

For a single„ÄÇNuron„ÄÇNetwork„ÄÇSo this is all scalarrs and you should be able to understand generally what's going on in the matrix vector setting„ÄÇ

 especially this concept of batches we're not going to actually test you on whether you can you know derive these gradients in the matrix vector setting but it's an important concept„ÄÇ

 this is what's happening in the background right so we also covered back prop„ÄÇ

And the key takeaways of back prop are„ÄÇChain ruleÔºå that's„ÄÇMajor componentÔºå and„ÄÇAlsoÔºå caching„ÄÇ

These intermediate„ÄÇDerivatives„ÄÇAndÔºå finally„ÄÇÂóØ„ÄÇUsing efficient„ÄÇComputations„ÄÇTo avoid„ÄÇ

 when you can forming„ÄÇÂëÉ Jacobcob„ÄÇSo this latter part„ÄÇ

 where we derive these updates for the linear layer„ÄÇ

 these are kind of things that are happening in the background when you use something like Ptorrch to compute derivatives„ÄÇ

 so it's important to know what's going on„ÄÇOkayÔºå so for next timeÔºå next week„ÄÇ

 well be looking at first an implementation of the single neuron case using Pytorch„ÄÇ

 and we'll also go over the loss function training objective that's used for language modeling and we will start our introduction to the transformer model„ÄÇ

 which is the final model that we'll be covering in this semester„ÄÇ



![](img/3c45fb4d9857b998e509767f9953972a_6.png)