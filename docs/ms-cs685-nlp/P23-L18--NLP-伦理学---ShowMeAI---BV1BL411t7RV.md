# P23ï¼šL18- NLP ä¼¦ç†å­¦ - ShowMeAI - BV1BL411t7RV

Okayï¼Œ hey everybodyï¼Œ so today we have a couple of things to do one I'm just going to briefly go over the answers to the midterm questionsã€‚

And then we're going to talk about ethical considerations in NLPã€‚

 which are growing increasingly more important as our systems get more powerful and are applied to a greater variety of tasksã€‚

ğŸ˜Šï¼ŒBefore we get started with these exam solutionsï¼Œ I just wanted to say thatã€‚

Grading is going to be pretty slow for this examã€‚Mainly because the questions were pretty open-endedã€‚

 there's a bunch of ways of getting partial credit and so we actually have to spend a lot of time reading your answers so you can expect to have grades back towards the end of the month homework one will be starting grading relatively soon so you should be able to have those grades within a week or twoã€‚

 but the actual exam will be graded towards the end of the semester sorry at the end of the month and then your final reports are due early December and those will also take a while to grade so yeah don't make annoying piazza posts about exam grading they're not going to be done till the end of the monthã€‚

Okayï¼Œ so let's go over these questions the first one actually one of the Ts went through a couple maybe like a bunch of different responses here and estimated that maybe only 20% of you got it correct soã€‚

So it turned out to be much more difficult than intendedã€‚

 but the issue with this decoder mask is that remember that this is a multi layer transformer model right so it has 16 layers and if the mask was set at this at just the first layer and it was a single layer model then this would work perfectly it would actually be a trigram model in that each time steps prediction is only looking at the previous two words the problem is when you go to multiple layers because then your mask is being applied on the output of the previous layers or representationsã€‚

 so that means that at layer2ï¼Œ when I mask when I unmask this representation this thing already has information about the previous two words so gradually as you go up in the network even though your maskã€‚

ğŸ˜Šï¼ŒI only allowing you to see the last two representations at the previous layer you're going to be increasing your receptive field so by 16 layers you've probably have looked at quite a few if not all of the tokens in your context just because the mask is applied to the previous layers or representations not just the original word embeddingã€‚

 so in effect this is leaking information about the words that you've seen in the past beyond just the past two words and this is why this model is not anywhere near as bad as Sarah expected here that's because the model has access to manyã€‚

 many more context tokensï¼Œ not just the last two So this turned out to be a pretty tricky problem the solution to this problem is very straightforward there's actually a couple solutions that we'll accept the oneã€‚

ğŸ˜Šï¼ŒWe're thinking of is you just reshape your input data so that every single sequence is only three tokens and you only do the prediction at the last time step sorry it could be two tokens and you do the prediction at the last time step so this way there's no the language model doesn't have any ability to cheat and look at tokens that are more than two words in the past because your input is such that it has no way of looking at thatã€‚

And another implementation could be you keep Sarah's normal mask at the first layer and then at layers 2 through 16 you use just an identity matrix where you only have an unmasked position on the current time step and zeros everywhere else note that this is not the same as the reshaping the input data approach from before because here the upper layers of the transformer are not actually doing self attentionten over a three word windowã€‚

 they're just doing it over a one representation windowï¼Œ but it will still accept that solutionã€‚ğŸ˜Šã€‚

So this question depends on your response to the previous one if you do the data modificationã€‚

 the input size modificationï¼Œ you should note that you're going to have way more training examples if you cut off each batch to just include three words and it turns out that this dominates the time that you save for making your self-at computations less because of course you have shorter sequencesã€‚

 but we'll accept answers that at least mention both of these two major contributors to the training time if you do the mask modification then the training time will be the same so yeah this depends on what you wroteã€‚

All rightï¼Œ so this oneï¼Œ I'm not going to go over in detail because there's many possible answersã€‚

 I think what we were looking for mainly with your modeling approach was that I mean the most reasonable thing to do is treat this as a text generation problem just because these questions are very uniqueã€‚

 it's rare to see questions like these just in the wild and so if you chose to treat this as a retrieval problemã€‚

 you would need to heavily justify like what your retrieval data set looks likeã€‚

 because some of these questionsï¼Œ they're just so weird and complicated that you would never observe a relevant response in even something like the common Cã€‚

So yeahï¼Œ I meanï¼Œ any sort of sequence to sequence model trained on this data would be good if you incorporated pretraining into your decoder thats that's good too and yeah so of course there's many other aspects of this problem that you could have considered like what do you do with multiple responses to the same problem sorry to the same question maybe you do some sort of filtering during preprocessing to remove some of them maybe based on upboats or whatever butã€‚

ğŸ˜Šï¼ŒYeahï¼Œ this was a very open ended problemã€‚ We will take off points if you decided to for some reasonã€‚

 treat this like a squad taskï¼Œ an extractive question answering task because that makes no senseã€‚

Okayï¼Œ so for the evaluationï¼Œ mainly we were expecting you to write about human evaluations hereã€‚

 if you wrote something about blue score or Rouge or something like thatã€‚

 it doesn't make sense for this particular task because the task is very unconstrained so you might have justified this in some wayã€‚

 maybe with some examples from the dataï¼Œ but in generalã€‚

 there's no way blue is going to be a reliable evaluation metric for such an open-ended taskã€‚

 Engramham overlap with the answer with a single ground truth answer is yeahã€‚

 it's not a viable indicator hereï¼Œ if you did something with using multiple referencesã€‚

 we can take that into account and give you partial credit but really the only way to evaluate this is through a human evaluation and there's many different ways in which you could have devised this evaluation so yeahã€‚

ğŸ˜Šï¼ŒWe'll grade these on a case by case basissã€‚å—¯ã€‚Okay so this question again turned out to be trickier than expectedã€‚

 so this was basically testing your understanding of backpro gradient descent optimization and what would happen in this scenario So the key insight to this problem is that the word embeddings are trained as part of the model and so what does this mean it means that this loss function has a trivial solution right if all the embeddings are set to zero then I trivially make my loss zero without actually learning any sort of useful representation of the task right and this is in fact what does happen when you set up these autoencoderrs and have all parts of the model be trainable unless you design your loss function in a more clever way to avoid these kinds of degenerate solutions So yeah again we willã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_1.png)

I mean some of you said things like the word embeddings will become identicalã€‚

 which is kind of almost thereï¼Œ but yeahï¼Œ and I think others of you said things like the activation function was saturatedã€‚

 but I mean we didn't specify what the initialization of these parameters wasã€‚

 it could be the case if we had a really really bad initialization that the TNH would be saturated but this is very unlikely and not the major issue with this problemã€‚

 this will happen regardless of the initializationã€‚

 so we will give some partial credit for those other types of solutionsã€‚Okayã€‚

 so finally this last questionï¼Œ there were many things wrong with this setupã€‚

 but the major issue with question 4ã€‚1ï¼Œ regardless of whether or not Bert is able to memorize the Social security number or not is that this number is going to be split up into multiple subward tokens using Bert's tokenizer right so it's never going to be able to produce this entire number with just one mask token so yeah that's basically the answer to thatã€‚

And here we wereï¼Œ I mean there are many possible answersã€‚

 one of them could be that you modify the masking strategy of your BRT model to only mask out numbers and contiguous spans of sub wordss so thats that's another I mean there could be other possible solutions hereã€‚

 so we'll again look at each of your answersï¼Œ there are manyã€‚

 many reasons why this is not generalizingï¼Œ but if you said something like it just memorizes this specific sequence and is not generalizing to the semantic meaning of the sentence that's perfectly fineã€‚

 you might be able to solve this with some sort of paraphrasebased augmentationï¼Œ but maybe notã€‚ğŸ˜Šã€‚

So anywayï¼Œ there are lots of potential issues that you could have pointed out and we'll make sure to give full credit whenever it's dueã€‚

Okayï¼Œ I see a question I tried to code this example I never get the word weight of word embeddings to all zerosã€‚

 but the embedding does appear to be identical Oh interesting Okayã€‚

 so yeah I guess we'll give give full credit to anyone who said that the embeddings are going to be identicalã€‚

ğŸ˜Šï¼ŒYeahï¼Œ just anecdotallyï¼Œ I've coded up several instances of these kinds of autoencors and encountered this degenerate solution many timesã€‚

 not with such a simple setupï¼Œ thoughï¼Œ I meanï¼Œ no one would ever implement this kind of autoencoderã€‚

 but yeahï¼Œ that's interesting to know that it converges to identical embeddings that are not zeroã€‚

It's kind of unexpectedã€‚Okayï¼Œ so that's those are the answersã€‚

 hopefully that gives you a rough idea of how you didã€‚ But againï¼Œ grades will be coming much laterã€‚

 Let's switch over to our topic for the oh yeahï¼Œs that's a good pointã€‚

 That's actually what I was I forgot to mentionã€‚ but yeahï¼Œ it would have to go to zeroã€‚

 maybe the implementation of yangï¼Œ Tommy used identical sentence lengths in which case that wouldn't be a constraintã€‚

 But yeahï¼Œ if you had variable like sequences I agree that does seem like a big problemã€‚

 So maybe you can check your implementationã€‚ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_3.png)

![](img/62e0e925c50531752bcd7b52477ec528_4.png)

All rightï¼Œ so we're going to be talking about ethical issues in modern NLP nowã€‚

 so this is kind of a departure from our more technical topics from the beforeã€‚

 but it is of course increasingly importantï¼Œ especially what things like GPT3 where we're not really even sure how they're being usedã€‚

 how they can be used and what the implications of their usage in a variety of real world situations areã€‚

ğŸ˜Šï¼ŒSo you knowï¼Œ maybe like 10 or 20 years agoï¼Œ this wasn't such a huge deal because NLP wasã€‚

 you know our systems weren't that greatï¼Œ they could only solve pretty either synthetic tasks or tasks that were kind of far removed from real worldorld applicationsã€‚

 but now you know we have actual systems deployed as conversational agents that might be you know talking to customers and there's actually you know some integration of machine learning methods into systems that make real decisions about people's lives like judicial system or hiring decisions or immigration and so on there's lots of different ways in which NLP and machine learning affect us nowadays and the ethical questions now areã€‚

Growing increasing weed graã€‚So that'sï¼Œ yeahï¼Œ what is said by this slideã€‚And there is actuallyã€‚

 in factï¼Œ at CMUï¼Œ a courseï¼Œ an entire course on ethical concerns in NLPã€‚

 I forget what exactly the course is called but it's actually linked on the course pageã€‚

 so you can check that outï¼Œ as many of the slides that I have here are from that courseã€‚

 it's taught by Uo SpedCOvã€‚ğŸ˜Šï¼ŒSo some questions that I meanã€‚

 some of you have actually asked in the anonymous formã€‚

 as we've covered things like GPT3 and other you huge scale models is what are the you knowã€‚

 chances of various types of bias creeping into these modelsï¼Ÿ

And whenever we build something now nowadaysï¼Œ it's usually important to think about what are the benefits and who benefits from the thing that we're building and also what are the potential harms and this this latter question probably doesn't get thought get thought about as much as it should beã€‚

So as an exampleï¼Œ this is just from a few weeks agoï¼Œ there's this articleã€‚

 someone let a GPT3 bought loose un Reddit and it didn't end wellã€‚

This is one of the issues with the way that Open AI has released GP3 is like they chose a couple different applications and they allowed these these applications to access the GP3 APIã€‚

 which means that they can you know feed in context to GP3 and have it generate some outputã€‚

 and so there is this one application called like the philosopher bot or something which generated like philosophical textã€‚

 I forget exactly what it didï¼Œ but it was you know like a pretty innocent applicationã€‚

But the problem is that people started taking the output of this philosopher botã€‚

 which is coming from GT3 and then like posting it into Reddit and that also might seem fairly innocent on the surface right Reddit is just like mostly wellã€‚

 I don't actually knowï¼Œ but I imagine it's mostly like informal conversationsã€‚

 but there are some subredits that areã€‚ğŸ˜Šï¼ŒMore serious in terms of subject matter so this post on this page comes from the ask Reddit subreditã€‚

 which is where your exam question was from and the question was about like for those of you that have had suicidal thoughts what helped you get better and this response was generated by GPT3 so it brings up a lot of questions right do we want models that and you know we talked about with large scale language models do they really understand meaning if they're only trained on form right remember the octopus test example by Emily Bennder that we went over do we really want models that we know are limited in these ways to be you know posting advice to people who believe that this is actually a real personã€‚

Behind that username and not some weird huge language model that doesn't really understand what it's sayingã€‚

 so I don't think the people who created the philosopher bot ever foresaw such a use case where their bot was like posting advice to depress people on Redditã€‚

 butï¼ŒYeahï¼Œ I meanï¼Œ we probably don't want this to happenï¼Œ rightï¼Œ But this is something that was hardã€‚

 maybe hard to foresee when open AI was creating this language modelã€‚ But it's something that weã€‚

 we should think about nowã€‚ And in generalï¼Œ I think these kinds of unconditional language models that just are capable of generating text based on some input words there the benefit to harm ratio at this point is probably not worth it rightã€‚

 because the the models are still flawed and deploying them in the real world like in the wild with no checks can can do a lot of harm like possibly in cases like thisã€‚

 I think this is probably why the G3 paper was framed as a kind of few shot learning advanceã€‚

 because that is really cool and has a lot of promiseï¼Œ but G3 is also inã€‚ğŸ˜Šã€‚

credibly powerful language modelï¼Œ which yeahï¼Œ the risk to reward trade off is probably not there yetã€‚

Okayï¼Œ so what are some sources of some of these ethical issues in NLP and one big one is the data that we use to train our models as you know as we've been talking about through the course of the semester we really benefit from a huge amount of training data right we've seen many instances where we didn't change much about our model if anything we just you know increased our data by an order of magnitude and got got a lot more gains right I mean the Roberta paper essentially does this and you know where is this data coming from it's coming from normally the common crawl right or these social media forums and who really is posting or writing on these pagess we need to be aware of that if we're and you know like what kinds of stereotypes exist in thisã€‚

ğŸ˜Šï¼ŒData if we're just using it as a general representation ofã€‚

 you know English or some other languageï¼Œ it's not that right it's the the data on the internet is a function of who is able to spend enough time on the internet to actually create all of that contentã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_6.png)

And so some examples in computer vision it's far easier I think to find these thingsã€‚

 but this is an instance from Google Iage search where you can see the clear difference if you search for three black teenagers versus three white teenagers here you see all the white teenagers smilingã€‚

 playing sports and on this side you see fewer smiles and fewer activities so obviously if you're creating some sort of image labelbeling data set using Google image search you might run into this kind of bias and then your models are going to learn from the data that you've given it and maybe they learn to associate on smiling faces with you know black people or something like that so even if that's not your original intention just the way in which the data presents itself is going to have a huge impact on what kinds of biases here modelã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_8.png)

Learsã€‚We see this is also true with things like doctor right so in 2017 if you just Google image search doctorã€‚

 you're going to get mostly men and mostly white men as a resultã€‚ğŸ˜Šã€‚



![](img/62e0e925c50531752bcd7b52477ec528_10.png)

Whereas if you Google image search Nurï¼Œ you're going to get mostly womenã€‚



![](img/62e0e925c50531752bcd7b52477ec528_12.png)

And you see many other instances of this kind of biasï¼Œ so againã€‚

 it's just a historic example of what happens when you're unaware of the kinds of information and stereotypes that are in your training data right CEOã€‚

 you just get a bunch of white menã€‚ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_14.png)

So againï¼Œ if your data is biasedï¼Œ your models are going to be biased and there are some ways in which people have proposed to fix these kinds of issuesã€‚

 there's been a lot of work on debiasing specifically regarding word embeddings that we'll touch on a little bit later but there's also work on filtering or getting rid of problematic data and not really messing with the model too muchã€‚

 so many different strategies that you can go to kind of mitigate some of these issues but there's also several interesting questions about what you actually want to accomplish through all of these debiasing methods andã€‚

ğŸ˜Šï¼ŒYou know how far do you want to go down that direction and what does it really mean to remove you knowã€‚

 all stereotypes from your data is that even possible all of these things are open questionsã€‚So yeahã€‚

 I mean there is gender racial bias on the webï¼Œ this is certainly in the common crawl which GPT3 and other models have been trained onã€‚

 probably also manifests on Redditï¼Œ so yeahï¼Œ it's important to at least be aware of these things before you deploy models trained on themã€‚

And there's been a lot of work within NLP on various types of bias and deicing strategiesã€‚

 so we'll take a closer look at some of this work in a bitã€‚ğŸ˜Šã€‚

This is an example of gender bias and coreference systems so co-reference is a task that we haven't talked about too much in this classã€‚

 it actually is a task where the models are still interesting and not just completely birdified although maybe that's happened recently and I haven't been awareã€‚

 but here so basically the task is given a pronounã€‚

 find which entity the pronoun is referring to so here the physician hired the secretary because he was overwhelmed with clientsã€‚

 that he here is referring to the physician and if you just change the pronoun to sheã€‚

 the physician hired the secretary because she was overwhelmed with clientsã€‚

 the model will now predict that she is referring to the secretary instead of the physician even though we haven't changed anything else about this sentenceã€‚

ğŸ˜Šï¼ŒSo this is an instance of the data that we're feeding into these modelsã€‚

 probably had a lot of examples of male doctorsï¼Œ and that is kind of overriding the world knowledge in this example that's required to make the proper connectionã€‚

Okayï¼Œ so we've already talked about bias in dataï¼Œ but there's also sampling bias in even like labeled data sets that people are creating for various tasks like sentiment analysisã€‚

 so for exampleï¼Œ if I'm creating a sentiment data set and I'm going to just crawl Yelpã€‚

 I might want to know what are the demographics of people that post on Yelp right because I might be only collecting reviews that are written from a particular subset of the population and maybe that's not representative of the overall population which I am most interested in modelingã€‚

So if you look at the US demographics of Yelp usersï¼Œ the majority of themï¼Œ 60% are college educatedã€‚

 50% of them make more than 100 k a yearï¼Œ and you you should be aware of this when you're training a model based on Yelpã€‚

All rightï¼Œ so then there's also reporting bias this is important for many different phenomena that you might want to capture if you think about things like common sense right people aren't just going to say things like you know if I drop this cup it'll fall to the ground right this is kind of implicit in everyone's understanding of the world and so we don't have to talk about these things maybe we're going to talk more about things that are surprising or controversial or things that we find interesting and so you know that that might play a factor in what kind of data that that we're collectingã€‚

ğŸ˜Šï¼ŒThere's also you know some sort of system bias on the part of who's actually maintaining these onlineã€‚

 you know social media sites or web forums and who what kind of algorithms go into various search functionality and so on and at some point we're not even going to be able to know what's going on in the background right we don't have access to any of these algorithms we just have to trust the providers and it's unclear what kinds of factors they're using to tune and improve their algorithms and whether those are the same as the ones that we have in mind when we're building a systemã€‚

ğŸ˜Šï¼ŒAnd finally theres kind of dialectical and associate economic influence in various kinds of linguistic communities and we'll talk a little bit more about that in a bit there's also just the dominance of English in data sets like the common C versus any other language that and also in the research community in general right you know all of the papers that we've read and all of these conferences require you to submit conference papers written in Englishã€‚

 the vast majority of NLP research focuses on English language data setsã€‚

 some of the methods that we're building may not even scale even though they work on English because we have so much dataã€‚

 they may not even scale to other languages for which we just don't have as much data this is a plot of the number of Wikipedia articles asã€‚

ğŸ˜Šï¼ŒOf the number of speakers of a language so you see that English here you have by far the most Wikipedia articles and you have roughly 400 million speakers and you see something like Hindi down here where there's way fewer orders magnitude fewer articlesã€‚

 Wikipedia articles on Indian Wikipedia then there is on English Wikipediaã€‚

 but you know the number of speakers is is hugeï¼Œ so it's really know if you're training something on Wikipediaã€‚

 even if you take the union of all different languagesï¼Œ Wikipediasã€‚

 which is what the multilingual Bt didï¼Œ you're not getting the same sort of representation of all languages and usually you're just way over representing Englishã€‚

 which might be to the detriment of other languageã€‚ğŸ˜Šï¼ŒOkayã€‚

 so I see some comments and a question has someone tried taking each of the sentences in a data set and swapping gender pronouns and train on 2x dataã€‚

 yeahï¼Œ so I believe that is what is done for some co-reence solutions it might also be done for some of the word embedding debiing stuffã€‚

 but yeah I can't think of a specific work off the top of my head but I'm pretty sure I've seen things that do this so you might want to take a look more closely if you're interested in that and maybe some of the papers that are in these slides very relevantã€‚

So let's take a look at a specific case of where this kind oh another questionã€‚

 is there an adversarial challenge data set that focuses on gender or racial bias Yeah there definitely is one for coreference resolution I think the paper that is linked down here and also in these slides both of these introduce a challenge data set for coreference resolution that has these kind of mismatched pronouns and I believe there's another data set as well also about co-reference that has a similar kind of properties so yeah there are a couple that I'm aware of and I'm sure there's a lot more that I'm not aware of so yeah this is becoming increasingly more common as a way of measuring these kinds of biases but yeah actually removing them or mitigatingã€‚

ğŸ˜Šï¼ŒI meanï¼Œ it's also useful to measure your mitigation strategy on these challenge data setsã€‚Allrightã€‚

 let me get through these animations again so this example is a language identification task which is a very basic text classification task you get us input some piece of text and you're asked to predict what language it is and you know you might imagine that these systems are very accurate this doesn't really require too much modeling of semantics or syntax a lot of the time you know a particular word is just going to be a word in one language and no other language so it'll be fairly easy for you to make the determination of which language it isã€‚

ğŸ˜Šï¼ŒAnd people have even gone as far as to say that language identification is a solved problemï¼Œ rightã€‚

 this paper from 2005 says using data obtained from the World Wide Webã€‚

 we achieve accuracy approaching 100% on a test suite comprised of 10 European languagesã€‚ğŸ˜Šã€‚

So one of the PhD students in the UMass NLP group who recently graduated Suinã€‚

 she had a research project on examining existing language identification models on African American vernacularã€‚

 and she found that these systems degrade considerably when you give them tweets that are written in AAEã€‚

And this actually can have a huge impact on existing systems that integrate language ID language identification since it's close to a solved problem is you know a reasonable component to add in some of these pipeline systemsã€‚

 so for exampleï¼Œ in health monitoring you might have you some access to a Twitter stream and for each tweet you detect the language IDã€‚

 then you apply some sort of keyword filters to see whether the tweet is indicating whether or not the person is sick or notã€‚

 and then you you can start to address the illnessã€‚

 but if you get this language identification part wrongã€‚

 then your keywords are going to be in the wrong language as wellã€‚

 which means that you're not going to be able to accurately measure whether this tweet is actually exhibiting some sign of illness or notã€‚

ğŸ˜Šï¼ŒSo rightï¼Œ if if you're since this is a component early in the pipelineï¼Œ if it failsã€‚

 then you're just going to get complete garbage and this is not going to be usable for particular languages so we see in this case we're doing some sort of code switching between multiple different languages which is of course challenging for these kinds of modelsã€‚

 but it's also very commonly observed on Twitterã€‚Okayã€‚

 so there is a study in 2017 that shows that the accuracy of language identification systems on English tweets is kind of degrades as the human development index of the person whose tweetings origin country gets lower and lowerã€‚

 so for exampleï¼Œ and this index is a function of things like average education life expectancy in income of the citizens of that countryã€‚

 so for a country like India which gets a score of like 0ã€‚6ã€‚

 you can see that the accuracy for tweets written in English by people from India is much lower than tweets from you know the US or Britain or Australia where the human development index is much higher these systems are likeã€‚

ğŸ˜Šï¼Œnot trained on data from all of these different populationsã€‚

 but if they're deployed on data that includes examples from people in India or other countriesã€‚

 then they're going to fail more oftenã€‚So how can you fix thisï¼Œ I mean it's very hardã€‚

 but of course you might want to sample your data set better to increase its diversity so you can increase diversity topically or geographically or different types of formality and of course different types of languagesã€‚

And so this this graph just shows what happens with a method that accounts for all of this by resamplingã€‚

 it seems to do betterï¼Œ but there's still this kind of downward trendã€‚

Okay so another source of issues with NLP is the objective functions that are used and so sorry this example is not an NLP example of the next one I'll show you will be this was a pretty famous caseã€‚

 this compass system which was trying to answer this question and so it was a machine learning model that what is trained to predict the probability that a person would commit a serious crime in the future as a function of the sentence that you give them now and the issue with this formulation is that you can't ever get labels for this task right because this is an event that's going to happen in the future so what you're going to do sorry what they did to build this compass system is instead of getting labels for whoã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_16.png)

![](img/62e0e925c50531752bcd7b52477ec528_17.png)

will commit the crimeï¼Œ they use a proxy for thatï¼Œ which is who is more likely to be convicted so they use convictions as a proxy for committing a crimeã€‚

 but of course this has a lot of issuesï¼Œ even though they tried to balance their training data with you know people of all races and they didn't use race as a particular feature in their their classifier there's a lot of biases in the judicial system regarding conviction rate as a function of race and so even though they tried to balance their training dataã€‚

 the labels themselves didn't really match the objective that they were trying to aim for and so this system was deployed and it got a lot of pushback and partly because of thisã€‚



![](img/62e0e925c50531752bcd7b52477ec528_19.png)

So as an NLP exampleï¼Œ this paper actually was published in last year's EMNLPã€‚

 the goal is given a description of a particular caseï¼Œ predict the charges of that caseã€‚

 so predict like what the charges and also what the prison term isã€‚ğŸ˜Šï¼ŒSo this is a you knowã€‚

 probably something that you would want a judge to be doing and not some kind of machine learning systemã€‚

 the authors here built a system and they collected a data setã€‚

 they found that it worked you know like moderately wellï¼Œ but it had a lot of errorã€‚



![](img/62e0e925c50531752bcd7b52477ec528_21.png)

And finallyï¼Œ at the end of the paper there was a quick paragraph about the ethical issues of deploying this system it says the mistake of legal judgment is seriousã€‚

 it is about people losing years of their lives in prison or dangerous criminals being released or reoffend we should pay attention to how to avoid judges over dependenceence on the systemã€‚

 they're saying if their system was deployedï¼Œ we don't want judges to depend too much on it in practice we recommend deploying our systems in the review phase or other judges to check the judgment resultã€‚

 our system can serve as a checker but you know there was a lot of debate after this paper was published as to whether this kind of work should be done at all was there sufficient consideration of the ethical questions here what would actually happen if this system were deployed it makes soã€‚

ğŸ˜Šï¼Œman errors thatï¼Œ you knowï¼Œ practically speakingï¼Œ if it were deployedï¼Œ no one would rely on itã€‚

 no reasonable judge would consider it if it'sï¼Œ you knowã€‚

 giving you the wrong judgment or charge or prison termï¼Œ 40% of the timeï¼Œ rightï¼Ÿ

But what if it were betterï¼Œ I think something like judges depending on this or even maybe it replaces the judge that seemsã€‚

Yeahï¼Œ just seems wrongï¼Œ rightã€‚ So yeahï¼Œ I mean these are these kinds ofã€‚

NLP questions are just like as the space of tasks that we're able to feasibly solve increases the number of these kinds of ethically dubious tasks also increasesã€‚



![](img/62e0e925c50531752bcd7b52477ec528_23.png)

Okayï¼Œ I see some questionsã€‚å—¯ã€‚How would large scale machine translation on Englishã€‚

 Wikipedia to some other language and then using it in the training data set workï¼Œ Yeahï¼Œ there'sã€‚

 that's a good questionã€‚ There was a recent paperã€‚That was pointed to me about biases in machine translation where machine translation tends to normalize the like stylistic signature of its inputsã€‚

 meaning that there was this experiment where they took a bunch of sentences and they knew you know like the age and gender of the people that wrote those sentences and then they passed them through a machine translation model and it showed that I believe it made the predictions of a classifier that was predicting things like the age or gender of the person that wrote the sentenceã€‚

 these things started skewing more towards I shouldn't misrepresent the findings of thisã€‚

 I'll put a link to it on Piazzaï¼Œ but yeahï¼Œ translation definitely does introduce some kind of biases as wellã€‚

Do we have a reliable compass about where machine learning model should be applied or some readings regarding this question I think there's probably lots of papers about where and where not these things can be deployed I can't think of any off the top of my head but the reading that was linked for this particular lectureã€‚

 there's a lot of relevant material on that page that you can check outã€‚ğŸ˜Šã€‚

the is the tweet model sort of failing because with the HD index the grammar and spelling degrades tooã€‚

 I meanï¼Œ it's not a question of degradesï¼Œ but rather just that the variant of English spoken in these countries is not represented in the data it's not that like the grammar is more correct in one country in less correct in a different countryã€‚

 these are all just patterns of English usage that we need to account for if if our goal is you know to measure something like whether people have the flu or not we don't really care what kind of grammar they're using right we just want a model that's kind of robust to all of these factorsã€‚

So yeahã€‚All rightï¼Œ so the next thing that there's been a lot of work on over the past couple of years is bias in word embeddingsã€‚

 so I'm going to just briefly go through this since you know things like word tove have been very recently superseded by the contextualized embeddings that we get from things like Btã€‚

 but there's been a lot of work on this and specifically these kinds of analogies have been found in word embeddings that are trained on massive amounts of dataã€‚

 where if you take the embedding for man and subtract from itï¼Œ the embedding from womenã€‚

 women this is like veryï¼Œ very similar to a vector you get by taking the vector for computer programmer and subtracting the vector for homemakers so obviously indicating that some of these stereotypes are baked into the actual word embeddings of our modelsã€‚

ğŸ˜Šï¼ŒAnd so you seeï¼Œ you knowï¼Œ if you look at just occupations of people and look at which ones are closest to the she embedding versus the he embeddingã€‚

 you see these kinds of stereotypes creeping in here as wellã€‚

And so one way you can fix this and one of the standard ways to reduce this problem in word embeddings is to first identify a subspace of gendered words so these are words that actually convey some sort of gender not things like computer programmer and homemaker so you know things like she and he and then you identify words that should be gender neutral so these could be things like computer programmer or homemaker or nurse or doctor right so in this figure here you see some gender definitional words and you also see these gender neutral words and what you want to do is make these kinds of wordsã€‚

 the ones that should be gender neutral to be far away from the gender definitional subspaceã€‚ğŸ˜Šã€‚

SoAnd you knowï¼Œ the alternative thingï¼Œ of courseï¼Œ is that you don't want your transformation hereã€‚

 this debiasing transformation to change the embeddings too much right because you know that they contain some semantic information about you know what a doctor is and what a programmer is you don't lose all of that by applying this debiasing strategy at the same time you just want to make these things shift a little farther away from the gender subspace of the modelã€‚

So that's like the high level intuition of how you do thisã€‚

 and you can do this by just adding some sorts of regularizers on top of your word embeddingsã€‚ğŸ˜Šã€‚

And I believe people have looked into deicing strategies in contextualized models like Bert and Elmo as wellã€‚

Okayï¼Œ so another thing that's interesting is that so we've talked a lot about data being biasedã€‚

 but what happens on the model side rightï¼Œ the model also contributes a lot to the bias and downstream task and many models do something called bias amplification where there might be some amount of bias in the training dataã€‚

 but the model just dramatically increases that bias when it makes predictionsã€‚

So there's this interesting paper men also like shoppingï¼Œ reducing gender bias amplificationã€‚

 and this is a task that kind of builds on the vision and language stuff we discussed the week prior to the exam so here you get as input and image and you're asked to first identify the activity that's going on in the image So here cooking and then also identify all of these different roles that are occurring with this action So here like who is cookingã€‚

 what are they cookingï¼Œ what container are they using and what tool are they using and you would have to predict each of these things like woman vegetable pots spatula and in addition to predicting that this cooking activity is happeningã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_25.png)

So of courseï¼Œ the way you would solve this just following from our standard vision and language models is we're going to represent the image using a convolutional network and we're going to represent and we're going to just predict these things from the representation that we get out for a CNN so these could be like classification tasks and practice this is actually a structured prediction taskã€‚

ğŸ˜Šï¼ŒWe use something called a conditional random fieldï¼Œ which we haven't talked about in this classã€‚

 but is useful for structured predictionã€‚

![](img/62e0e925c50531752bcd7b52477ec528_27.png)

Okayï¼Œ so let's take a look at the activity of cooking in this data setã€‚

 If you take a look at all of the images that are related to cooking in the dataã€‚

 33% of them involve men and 66% of them involve womenã€‚

 so there is this you know skewew towards more women being associated with the cooking activity in the data setã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_29.png)

Howeverï¼Œ once you train a model on this to predict the activityã€‚

 the resulting model predicts that only 16% of examples of cooking are associated with men and 84% of them are associated with womenã€‚

 so you see here including examples where there's clearly men involved in the in the image the model learns this bias and amplifies it in its predictionsã€‚

ğŸ˜Šï¼ŒSo this is a case of the data is biased and the model is even more biased nowã€‚

And so when we look at why this happensï¼Œ I mean there's another example here where these two pictures are almost identical the only difference is that here we have a girl washing her hands and here we have a guy washing his hands and on the left the model predicts that this is a woman cooking and on the right this predicts the there's a man fixing a faucet so I mean literally melting has changed except how the person looks and you get a completely different activity and also you know both of the activities are wrong so this is a case where you know the biases in the dataã€‚

 the model is just fitting these biases and not really modeling the semantics of the imageã€‚

So this is a big problem with these kinds of bias data sets is that the model can trivially minimize its objective by latching onto these simple biases and just ignoring anything else that might be relevant so yeahã€‚



![](img/62e0e925c50531752bcd7b52477ec528_31.png)

So you can actually quantify this and I'll go through this pretty fastã€‚

 but basically we are just going to compute the ratio of the actual data set for a particular activityã€‚

 the ratio of like female to male associated with cooking and we can also compute this ratio on the model's own predictionsã€‚



![](img/62e0e925c50531752bcd7b52477ec528_33.png)

![](img/62e0e925c50531752bcd7b52477ec528_34.png)

And so here you see that we're doing this and now we can compute this amplification right so here you see on the X axis the training gender ratio so I believe this is yeah the number of men associated with cooking over the total so things like cooking and serving and microwaving all have a pretty low training gender ratio and then you see the predicted gender ratio on the Y axisã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_36.png)

![](img/62e0e925c50531752bcd7b52477ec528_37.png)

And so if things were pretty balanced between training and predictionã€‚

 then all of these red dots would be on this blue lineã€‚

 but the ones that are deviating sharply from the blue line are cases where the model has amplified the bias that was in the training data setã€‚

And so these are cases that we want to avoid and the paper proposes a way of kind of figuring out which things are violating this the training gender ratio by a huge margin and then kind of correcting themã€‚

 so I'm not going to go over the actual method if you're interested in it you should check out the linked paperã€‚

Okay so finallyï¼Œ there are issues with both the data and the models right the data contain stereotypesã€‚

 the models can amplify themï¼Œ people like researchers and people in industry may not be thinking about all the potential consequences of building new models for new tasks or deploying models to various different types of populations and yeahã€‚

 there's just a need for I mean that's not to say that we shouldn't continue to keep improving our language models right but maybe we shouldn't just make them you know be able to post on Reddit right but they don't really understand language the way that we do and there's potentially dangerous scenarios associated with you know having them give advice for example where they may not even understand what they're giving advice on they're just you knowã€‚

ğŸ˜Šï¼Œdoing some sort of fancy pattern matching and maybe some in some cases they give good adviceã€‚

 but just the fact that you can't even interpret why they decided to make a prediction or maybe there's some randomnessã€‚

 there's a sampling process in the decoder right we've talked about this do we really want that to come into and have an impact on what these models are producing in real world settingsï¼Ÿ

Yeahï¼Œ lots of important stuff to think aboutã€‚All rightï¼Œ so that's all I had for todayã€‚

 I see there's some questions and commentsã€‚ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_39.png)

The model probably doesn't know that the misspelled word is misspelled or does it wouldn't it think it's a new never seen before token Yeahã€‚

 it might think that but it might have also learned from its training if the training data included unseen tokens that those were associated with a particular language or something like thatã€‚

 which could cause it to predictings incorrectlyã€‚Was there any pretraining done on the model in the model bias paper I'm not sure what model bias paper you're referencingã€‚

 I imagine this one in which caseï¼Œ yeahï¼Œ the convolutional network was pretrained on Inet and then fine tune on this taskã€‚

 which is the case with most vision and language and like vision applicationsã€‚All rightã€‚

 so for next time we're going to be talking about probe tasksã€‚

 so this is an important topic that of kind of left till the endã€‚

 but we've talked about all these huge models and you know there are billions and hundreds of billions of parametersã€‚

 how do we actually know what they've learned or why they're making a particular predictionã€‚

 so this is an unsolved problem but probe tasks are one way in which we can better understand what these models are learning so that kind of interpretation of these models is what we'll be discussing on Wednesdayã€‚

ğŸ˜Šã€‚

![](img/62e0e925c50531752bcd7b52477ec528_41.png)