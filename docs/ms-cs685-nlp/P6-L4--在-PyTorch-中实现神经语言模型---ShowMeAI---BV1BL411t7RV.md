# P6ÔºöL4- Âú® PyTorch ‰∏≠ÂÆûÁé∞Á•ûÁªèËØ≠Ë®ÄÊ®°Âûã - ShowMeAI - BV1BL411t7RV

Hey everyoneÔºå so today we're going to spend most of our time actually implementing a neural language model in Pytorch and we're going to be switching shortly from this iPad to a Google collab notebook but before we do that there's one more important thing that we need to cover before you're ready to actually understand what's going on when we implement this and that specifically is the loss function that's used in neural language models and more generally for neural classification problems and that is the cross entropy loss„ÄÇ

So againÔºå this is used„ÄÇNlms neural language models„ÄÇAs well„ÄÇAs„ÄÇOther„ÄÇClassification tasks„ÄÇ

So remember we are treating in the neural language modeling setting this task as a classification problem right given a prefix„ÄÇ

 we're asking the model to predict which word out of the set of all word types in our vocabulary follows this prefix right so it's a classification problem where the labelL space is equal to the size of the vocabulary so it's a large scale classification problem and the loss function that we've seen that we saw previously the square loss is not suitable for such a large scale classification problem and so we're going to review a better one the cross entropy loss before jumping over to the implementation„ÄÇ

üòäÔºåOkayÔºå so quick review from last timeÔºå what is a loss functionÔºüIntuitively„ÄÇTells us„ÄÇHow bad„ÄÇA model„ÄÇ

Is doing„ÄÇAt predicting„ÄÇThe training data„ÄÇRight so in the neural language modeling setting„ÄÇ

 if I have a model that gets his input the prefix students open there and is predicting the word lamps with a high probability„ÄÇ

 we want a loss function that will assign a high loss to this instance because the models clearly not predicting the training labels if in the training set we saw a lot of examples of students open their books or their laptops or so on„ÄÇ

OkayÔºå so in N LMsÔºå specifically how bad„ÄÇIs the model„ÄÇAt predictreing„ÄÇThe next word„ÄÇOkay„ÄÇ

 so let us whoops dive into this by looking again at the example that we've been following for the first few videos here„ÄÇ

OkayÔºå so assume„ÄÇWe have„ÄÇA training example„ÄÇStudents„ÄÇOpened„ÄÇThere this is the prefix„ÄÇ

 and it's paired with books„ÄÇOkayÔºå so we have our inputs„ÄÇhere and our output books„ÄÇ

So the model takes this as input„ÄÇOhoops„ÄÇÂëÉ„ÄÇAnd thisÔºå remember is„ÄÇDesired„ÄÇMoel„ÄÇPrediction„ÄÇ

So remember that our model is actually producing a probability distribution over the whole vocabulary„ÄÇ

 so what we really want is a predicted distribution in which the probability associated with the word books given this prefix is very high„ÄÇ

üòäÔºåSo to make that more concreteÔºå our model produces„ÄÇGiven„ÄÇAnd we want to„ÄÇMaximize„ÄÇThis probability„ÄÇO„ÄÇ

So we know that we want to maximize the probability of the correct continuation„ÄÇ

 given the correct continuationÔºå meaning the thing that appears in our training data„ÄÇ

 given our model„ÄÇSo remember that a loss function is a measure of how bad the model's current prediction is„ÄÇ

 so we can rewrite this as we want„ÄÇTo minimize and remember„ÄÇ

 we talked about gradient descent in the concept when we talked about gradient descent„ÄÇ

 we talked about minimizing a loss functionÔºå right„ÄÇ

 we use gradient descent to go in the direction of the negative gradient and find some sort of minimum of the loss function so we can easily turn this into a loss function„ÄÇ

 We want to minimize negative„ÄÇüòäÔºåLog probability„ÄÇSo remember again that we work in log probabilities„ÄÇ

 not raw probabilities due to many reasonsÔºå practically speaking due to numerical underflow„ÄÇ

 we don't want to deal with these super tiny numbers„ÄÇÂóØ„ÄÇAnd in our example„ÄÇ

 we want to minimize a negative log probability of books„ÄÇ

 so our loss function is going to look like negative log P of books„ÄÇGiven students open there„ÄÇ

So this is pretty straightforward„ÄÇBut before we continue here„ÄÇ

 it's important to understand why this is called a cross entropy loss„ÄÇ

 So many of you are probably familiar from previous classes„ÄÇ

 machine learning classes or so on with the concept of a cross entropy loss right So why is this called„ÄÇ

üòäÔºåThe cross interview loss„ÄÇAnd remember that the cross entropy loss intuitively gives us a measure of the distance between two probability distributions„ÄÇ

 so if my modelÔºå let's say my model gets maybe I'll draw the model to make things more clear so we have students„ÄÇ

üòäÔºåOpened„ÄÇThereÔºå remember that I have word embeddings for all these things„ÄÇ

 I combine them into single vectorÔºå and I produce a probability distributionÔºå rightÔºü

 So what does that actually look like„ÄÇIf we say all of our words are„ÄÇBooksÔºå laptoppsÔºå I don't know„ÄÇ

 Maybe you can make up some other wordsÔºå planesÔºå lampsÔºå etc cetera„ÄÇ

 So all the words in our vocabularyÔºå rightÔºå And let's say this is what our model is doing„ÄÇ

 So maybe it assigns some probability to books„ÄÇ It assigns some probability to laptops„ÄÇ

 some probability to planesÔºå a very high probability to lamps and so on„ÄÇüòäÔºåSo in our training data„ÄÇ

 we observe thatÔºå of courseÔºå the correct continuation of this is books„ÄÇ

So we treat our training distributionÔºå so maybe I will„ÄÇThis is a model„ÄÇ and let's say this color„ÄÇ

Is used for the model's predicted probability distribution„ÄÇSo next we have our data„ÄÇAnd in our data„ÄÇ

 we observe students„ÄÇOpened„ÄÇTheir books„ÄÇSo in this specific example„ÄÇ

 there is no ambiguity as to what follows students open there right so we consider this as a one hot distribution remember we talked about one hot vectors as a way of encoding words in an earlier video„ÄÇ

So we can have our same distribution hereÔºå books„ÄÇPtops„ÄÇPlanes„ÄÇLamps„ÄÇ

And this model is going to assign all of the probability„ÄÇ So P works„ÄÇG‰ª¨„ÄÇÂóØ„ÄÇThere oops„ÄÇAnd„ÄÇ

And these are all zeroÔºå so let's make this clearÔºå this is what we observe in our data„ÄÇ

The what color I not used of this„ÄÇSo intuitivelyÔºå we have a model's predicted probability distribution„ÄÇ

 and we have the ground truth probability distributionÔºå which is this one hot vectorÔºå right„ÄÇ

 essentially with one at the ground truth continuation„ÄÇ

 and we want to make the models predicted distribution as close to the ground truth distribution as possible„ÄÇ

 So remember that cross entropy loss is defined as„ÄÇüòäÔºåWe all go down here„ÄÇSorryÔºå the cross entropy„ÄÇ

Of two„ÄÇDistributions„ÄÇLet's call them P And Q„ÄÇThis essentially again quantifies„ÄÇDistance„ÄÇBetween„ÄÇ

The distributions„ÄÇSo if the cross entropy is lowÔºå then the distributions are fairly close together„ÄÇ

 if the cross entropy is highÔºå that means that they're pretty far apart„ÄÇAnd the equation for this is„ÄÇ

So in our neural language model settingÔºå remember that the different„ÄÇ

Classes that we have are the word types in our vocabulary„ÄÇOkay„ÄÇ

 and so these are the conditional probabilities that written in short form here„ÄÇ

 so this is we can treat as our ground truth distributionÔºå one for books„ÄÇ0Ôºå every„ÄÇOther„ÄÇWord type„ÄÇ

 sorryÔºå that should be 0 for every other word type„ÄÇ And this is our predicted„ÄÇConditional„ÄÇGs„ÄÇ

So this is the definition of cross entropy lossÔºå which we have ported over to our neural language model setting where P is the ground truth distribution and Q is the predicted distribution„ÄÇ

 so maybe I'll make that more clear using the same colors from above„ÄÇ

 so we used Q sorry P with this color„ÄÇQ„ÄÇ„ÅÑ„Åã„ÄÇOkay„ÄÇSo let's take a look„ÄÇ

 go back and take a look at our loss function that we came up with from before„ÄÇ

 which was just followed naturally rightÔºå we want to minimize the negative log probability of the word that appears in our training data set„ÄÇ

 books given this prefix if you look at this definition of cross entropy here„ÄÇ

 you will see that since the P term is zero for every word other than books„ÄÇ

 this simply just reduces to exactly what we had before„ÄÇGiven the prefix„ÄÇ

So hopefully this makes sense since this summation isÔºå there's only one non zero term„ÄÇ

 and that's when this W equals books„ÄÇOkayÔºå so that's a bit of intuition on„ÄÇ

What kind of loss function we're using for neural language models„ÄÇ

 but the important intuition is that you know a loss function in our case is a defining a measure of how close the model's predicted probability distribution is to what we've seen in our training data„ÄÇ

 later on in the semester we'll talk about methods where this here is not a one hot vector so you actually have to use the full definition of cross entropy there are many non-zero terms„ÄÇ

 but for now we're going to stick with the simple case„ÄÇüòäÔºåOkay„ÄÇ

 so that's all I wanted to say about the type of loss function we're using in neural language modeling and now let's quickly switch over here to the collab notebook that I want to share„ÄÇ

 so give me a second to do those„ÄÇüòä„ÄÇ

![](img/3aa8ae5afa5ccba3efd817626d99d2d3_1.png)

![](img/3aa8ae5afa5ccba3efd817626d99d2d3_2.png)

Okay„ÄÇYes„ÄÇThat's too big„ÄÇÂóØ„ÄÇOkayÔºå so now we're going to switch over to the kind of live coding exercise where I will walk you through the implementation of a very simple neural language model on a toy data set of basically I guess like four prefixes and yeah„ÄÇ

 I highly recommend for this to be productive that you open up a collab notebook of your own while I'm doing this and follow along with what I'm doing„ÄÇ

 there are a number of interesting places where you can branch off of the code that I'm writing and implement more complicated networks or training optimizers or architectures or so on so I think you'll learn a lot if you if you code this up along„ÄÇ

With with me when I'm doing it„ÄÇOkayÔºå so I'm going to click new notebook here„ÄÇU„ÄÇOkay„ÄÇ

 this is going slower than I expectedÔºå All rightÔºå let's name this Cs 685„ÄÇNlm„ÄÇ

So let's start by implementing the fixed window feed for neural network that we've seen in a previous video„ÄÇ

 so to begin I'll come up with some sample sentences so let's say my sentences are I don't know„ÄÇ

 we should make them short so maybe likeÔºåUBsÔºå sheep„ÄÇAlice runs fast„ÄÇCs 685 is fun„ÄÇ

 hopefully that's true for some of you or all of you„ÄÇI love Li„ÄÇOkayÔºå so we have these four sentences„ÄÇ

 They're all three words long„ÄÇ so we're going to„ÄÇLet's say do a fixed window neural language model where our prefix size is two„ÄÇ

 so we're going to take the first two words and try and predict the third word„ÄÇ

So maybe I'll write some stuff down in the commentsÔºå so given the first two words of each sentence„ÄÇ

We will try to predict the„ÄÇOopsThird word using a fixed window NlM„ÄÇ So this is our goal„ÄÇ

But before we do thatÔºå we have to start by doing something that you've done in homework 0„ÄÇ

 which is tokenize the input„ÄÇ So before we start„ÄÇAny fancy modeling„ÄÇ

 we have to tokenize our input and the second thing we have to do is convert the words to indices„ÄÇ

Remember because our model takes as input sequences of indices rather than sequences of text„ÄÇ

 so we need to do these two things before proceeding any further„ÄÇüòäÔºåOkayÔºå so let's do both of these„ÄÇ

I think the screen is still visible„ÄÇ Oh maybe I should increase the font size„ÄÇOf„ÄÇOkay„ÄÇ

 whatever probably hopefully it's readable„ÄÇOkayÔºå so let's start by defining an empty vocabulary so this is going to„ÄÇ

Map from word type to indexÔºå so every single unique word type in our vocabulary will be assigned a separate index„ÄÇ

 and then we'll have a list of inputs that we're going to store so it's going to be just like the sentences except each of the elements in this list is going to be a sequence of indices rather than a string„ÄÇ

So„ÄÇU„ÄÇSttores an indexified version of each sentence„ÄÇOkay„ÄÇ

 so a simple way to do this is compute the vocabulary while converting the sentences to indices on the fly so we can loop through our sentences„ÄÇ

Let's say we make a variable forÔºå I don't knowÔºå sent indexes„ÄÇ

So these are going to be the mappings for the current sentenceÔºå we're going to split on white space„ÄÇ

 so this is something you saw in homework zero„ÄÇTokenize with„ÄÇWhite space again„ÄÇ

 not the best tokenizer you a lot of you posted on piazza about things associated with you know like new line characters or punctuation being included in the word types and there are much better tokenizers„ÄÇ

 but you know this is a simple toy exampleÔºå there are only„ÄÇ

 I mean white space tokenization is exactly what we want in this case„ÄÇAll right„ÄÇ

 so we're going to loop through the words„ÄÇAnd for every word„ÄÇ

 we're first going to check if that word is in the vocabulary„ÄÇI'm sorryrry„ÄÇ

 if the word is not in the vocabulary„ÄÇ So if the word is not in the vocabulary„ÄÇ

 we want to add it to the vocabularyÔºå and we also want to assign it to a unique index„ÄÇ

 So this line of code assigns it to the current size of the vocabulary„ÄÇ

 So it ensures that every new word we add we're basically incrementing the the index that we're„ÄÇ

So the whole„ÄÇPoint of this is to add newer types to the vocabulary and pair them up with indexes and finally we are going to append the index associated with the current word that we're at to the sent IDX's variable„ÄÇ

So„ÄÇÂóØ„ÄÇVabSo this is going to if we're at the word Bob„ÄÇ

 it's going to add a mapping in the vocab dictionary from Bob to0 and then it's going to append a0 to this list that I've created here So once I've looped through all the words for this first sentence I probably have012 because these are the first three words in my vocabulary„ÄÇ

üòäÔºåActuallyÔºå I've realized that I haven't even shared any words in any of these sentences„ÄÇ

 so I guess there's 12 different„ÄÇWordsÔºå maybe we'll change that„ÄÇ so I'll make it Alice is fast„ÄÇ

 So now there's two occurrences if the word is„ÄÇÂóØ„ÄÇAll right„ÄÇ

 and once I'm done looping through all of the words in this particular sentence„ÄÇ

 I'm going to append the whole sequenceÔºå the sent IDX's list to my inputs list„ÄÇOkay„ÄÇ

 so that was a lot of stuff„ÄÇ let's see what all of this looks like„ÄÇ

 so first we can look at our vocabulary„ÄÇÂóØ„ÄÇWe need to read some of these lines So as a tip„ÄÇ

 if you want to a shortcut to run a coab cellÔºå you can type shift enter„ÄÇ

 at least that's what it is on my Mac and we see that every word type in our vocabulary Bob likes sheep Alice runs fast is blah„ÄÇ

 blah blah is assigned to a particular index in the vocabulary and the word is only appears once right because there are two tokens of is in our data set„ÄÇ

 but only one word type associated with is„ÄÇLet's also take a look just to be clear as to what the inputs object looks like„ÄÇ

So you see that it's a list of lists and each list contains a bunch of indices right rather than actual words„ÄÇ

 so we see here that ohÔºå I guess I have to run this all again to update„ÄÇYeah„ÄÇ

 so we see the first sentence Bob likes sheep is mapped to 012Ôºå the second one Alice is fastÔºå 345„ÄÇ

 the third oneÔºå CS685 is fun is 647Ôºå so note that the index associated with is occurs in two of these examples„ÄÇ

OkayÔºå so we're almost done with our preprocessing stuff and we're almost on to more fun things„ÄÇ

 But before we can actually feed this list of lists into our pytorrch models„ÄÇ

 we need to convert them into pytorch tensor objects so this is also something you saw in homework zero„ÄÇ

 we use the long tensor object for indices So the embedding module in pytorrch expects long tensor indices in the form of long tensors„ÄÇ

 So it's very easy to convert these thingsÔºå we can simply just cast them into long tensors„ÄÇ

 I'm mean in import torch because I haven't done that yet„ÄÇüòäÔºåÂëÉ„ÄÇOkayÔºå and let's just say„ÄÇOh right„ÄÇ

 so the other thing I want to doÔºå so okayÔºå two things„ÄÇOneÔºå convert to long tensors„ÄÇAnd two„ÄÇ

 define inputs and outputs„ÄÇ So right nowÔºå these are just sequencesÔºå right„ÄÇ

 So I actually want to split them up into given Bob likes the inputÔºå predict sheep the next word„ÄÇ

So let's do that first we'll call our inputs the prefixes„ÄÇ

 so we know we can get prefixes by simply just chopping off the last element of each of these lists„ÄÇ

So let's say sent and this slicing in PythonÔºå if you're not familiar with this notation„ÄÇ

 you should make sure to understand itÔºå but this„ÄÇRight here is just indexing everything in this„ÄÇ

This first sequence here except for the last the negative one element„ÄÇ

And I'm going to just loop over all of the sentences in my input„ÄÇAnd this is a list comprehension„ÄÇ

 So againÔºå for those of you who aren't familiar with Python or aren't super familiar with it„ÄÇ

 you you should read read about things like list compre list comprehensions and array slicing and so on„ÄÇ

 they're very useful concepts in general„ÄÇ OkayÔºå so similarly„ÄÇ

 I can create another list with just the label„ÄÇ So this is the the last word in each sequence„ÄÇüòä„ÄÇ

Like thisÔºüSo now just to show you what I did if I print the prefixes and the labels„ÄÇ

We so this first thing here is a prefixesÔºå so note that they are just the first two elements of each list chopped with the last element chopped off and the labels are just a sequence of the last element from each list so this is exactly what we want in our language modeling setup„ÄÇ

OkayÔºå so againÔºå to get these into long tensor format is very simpleÔºå all I do is wrap this„ÄÇ

Long tensor thing around each one„ÄÇAnd now let's say I print prefixes„ÄÇ

You'll see that I have this tensor object and contains exact same data„ÄÇ

 but I can now do all these pytorch operations to figure out what size this is and so on so now we're ready to start building up our model we have our data and input output format in the data structure this tensor thing that Pytorch expects so let's move on to the next cell where we're actually going to define the network„ÄÇ

Onto defining the network„ÄÇThis is where most of the fun stuff happens and before we start let's import just for„ÄÇ

It easier for me to type things with shorthand and so the first thing we're going to do is make a class for our neural language model„ÄÇ

And so all neural networks when you„ÄÇBuild them in Pytorch„ÄÇ

 have to inherit from this NN dot module class„ÄÇSoÔºå yeah„ÄÇOkay„ÄÇ

 so there's two things that you need to do„ÄÇIf you're going to use Pytorrch to make a neural network„ÄÇ

 so the first thing is write the init function„ÄÇThis initializes all the parameters„ÄÇOf the network„ÄÇ

 So we'll get to this in a bit„ÄÇ But rememberÔºå we've spent quite some time now in the previous videos identifying the parameters of various models and then„ÄÇ

 of courseÔºå computing the partial derivatives of our loss with respect to those parameters„ÄÇ

 The second thing we need to do is a forward function„ÄÇüòä„ÄÇ

So this defines the forward propagation computations„ÄÇ

So this is what we went through before looking at backpro right how you go from the embeddings to the final prediction of the probability distribution and we did this remember through a bunch of linear layers and softm layers and so on so that's it you just have two things to do you might be wondering what about the backpro right we spent all this time learning about how we use backpro and gradient and descent to train our models don't we have to spend time you know actually implementing those derivatives and this is the really nice thing about Pytorrch is that you don't and it computes all the derivatives for you and just one function call so we'll get to that a little later but it's you know incredibly useful you can implement super complicated networks without worrying at all about„ÄÇ

üòäÔºåmanuallyally deriving the gradient„ÄÇOkayÔºå so let's start with the in function„ÄÇÂóØ„ÄÇ

So there's a couple of things that in our fixed window neural language model we want to define So if you think about the hyperparameters of this model the things that we want to define as the people who are making this network and we don't want the model to learn it itself these are things like the dimensionality of the word embeddings the dimensionality of the hidden state so this is remember when we concatenate all the embeddings together„ÄÇ

 we then project that to a lower dimensional space using a linear layer so this is defining the size of that hidden space we may also want to define our window size for this model which we've already looked at in our training data is too but note that in a standard implementation you won't be making like static training examples like this you„ÄÇ

There be kind of incrementally constructing the window while scanning from left through left to right through a larger document„ÄÇ

 This is just for an example to make the preproces code and the training loop not super complicated„ÄÇ

OkayÔºå and the final thing we might want to do is tell the model how big the vocabulary is right it needs to know this for both the embedding parameter initialization„ÄÇ

 like how many embeddings does it need to create as well as the softm layer right„ÄÇ

 how many word types do I need to predict probabilities forÔºüüòäÔºåAl right„ÄÇ

 so the first thing I'm going to do is initialize the super class of this„ÄÇ

 the NN dot module and and anything that's higher up the base module class and then I can proceed with„ÄÇ

Creating parameters that are specific to my network„ÄÇ So I know that there„ÄÇüéº3„ÄÇ

 basically three types of parameters in this model„ÄÇ So the first is the embeddings„ÄÇ

 and we looked at this in homework 0Ôºå rightÔºå use this an N dot embedding object and it takes as arguments„ÄÇ

 I think it should„ÄÇTell me what the arguments are„ÄÇOkayÔºå maybe not„ÄÇ

 but the arguments are the number of word types in the vocabulary„ÄÇ

 which we are passing in asL vocab and the dimensionality of the embeddings„ÄÇ

 which we're saying is a de embedding„ÄÇOkayÔºå next up we have our hidden projection right so we're going from the concatenated version of these embeddings to the hidden layer„ÄÇ

 so we'll make a matrix for this or we're actually going to use Pytorrch is built in NN dot linear„ÄÇ

 which does this hidden projectionÔºå which is name it W slash Hi„ÄÇüòä„ÄÇ

So we have an NN dot linear and this the arguments here are essentially specifying the shape of the projection matrix„ÄÇ

 so we know in our case that we have thebedding time embedding dimensionality times the window size because that's how big that concatenated representation is going to be and we're going to project that to the hidden dimensionality„ÄÇ

So this thing here is the size of the concateninated representation„ÄÇMaybe I will„ÄÇConcatenated„ÄÇ

Ebeddings„ÄÇüéºTo hiddenÔºå That's the purpose of this linear layer„ÄÇ

 And the other thing we have is hidden to output probability distribution over vocab„ÄÇOkay„ÄÇ

 so we can call that self„ÄÇw sub outÔºå it's another linear layer and goes from D hidden to the vocabulary size„ÄÇ

OkayÔºå so this in our extremely simple neural language model is what we're going to„ÄÇ

 these are the parametersÔºå currently we haven't applied any nonlinearities„ÄÇ

 we could do that and maybe we can do that in our forward function„ÄÇOkay„ÄÇÂóØ„ÄÇGreat„ÄÇ

 so let's move on then to the forward function„ÄÇAnd the forward arguments always take an input„ÄÇ

 which is in our case going to correspond to the prefixes„ÄÇ

 So each input will be a batch of prefixes in our caseÔºå since we only have four prefixes„ÄÇ

 we're just going to consider a batch of all four of them in practiceÔºå of course„ÄÇ

 you would have manyÔºå many more sentences and prefixes„ÄÇ

 and so your batches would contain different prefixes per batch„ÄÇ So„ÄÇLet's go back down here„ÄÇ

In this caseÔºå four„ÄÇSo that's what is going to be in this input„ÄÇ

 So you can think of the input to this forward function as the prefixes variable that we„ÄÇ

We made up here„ÄÇAll rightÔºå so I just want to make sure this is recording properly„ÄÇ

 okay looks like it is„ÄÇÂóØ„ÄÇLet's go ahead and start„ÄÇ So this is fairly straightforward„ÄÇ

 What we're going to do is firstÔºå let's just get some statistics about the input„ÄÇPrefix„ÄÇ

Length equals input dot size„ÄÇ So againÔºå these are the dimension„ÄÇ This is the dimensionality of the„ÄÇ

The prefixes tensor that's going to be input here and now let's just go ahead and get embeddings for this input We can do this by simply calling self do embeddings on our input„ÄÇ

OkayÔºå so what does this look like and in general a good way to proceed with building your forward function is to incrementally debug it I know people like to use debuggs and stuff I„ÄÇ

Don't„ÄÇReally do that and instead just use simple print statements„ÄÇ

 which also works probably not as efficientÔºå but whatever„ÄÇ

 so let's make let's actually construct an instance of this NLM class„ÄÇüòäÔºåSo I can do that by„ÄÇAgain„ÄÇ

 this calls the init function„ÄÇ So I have to specify all of these things„ÄÇ

 Let's say my D embedding is going be„ÄÇI don't know„ÄÇ What should it be5„ÄÇThe hidden„ÄÇSay 12„ÄÇWindow size„ÄÇ

 we defined as2Ôºå and when vocabulary equals„ÄÇThe actual length of this vocabulary dictionary„ÄÇO„ÄÇ

So if I run thisÔºå it's going to execute the in function„ÄÇOkayÔºå so„ÄÇLet's make sure that it does„ÄÇ

 And there are no errors„ÄÇ OkayÔºå And nextÔºå we are just going to pass our prefixes through this forward function„ÄÇ

 So if I do and you don't even have to call the forward functionÔºå you can just„ÄÇ

Write this and it automatically calls the forward function that you've written„ÄÇ

So just to make that clearÔºå if I print the size of this embedding object„ÄÇI get four by two by5„ÄÇ

 so it's a three dimensional tensorÔºå and let's think about why its these are the this is the dimensionality of this M's variable„ÄÇ

So four we know is the number of prefixes in this tensor right we have four examples so this corresponds to the batch size two is the number of words in our prefix right the number of words in the prefix is two so in the first example our prefixes Bob likes and so we have an embedding for each of those words and five is the dimensionality of our embeddings so for each of the four prefixes and each of the two words in that prefix we have a fivedial embedding„ÄÇ

So that's how you can interpret this„ÄÇOkay„ÄÇSo now that we have thisÔºå what's the next stepÔºü

Let's just write the dimensionality out so we can have it as reference for later„ÄÇ

 So in the neural language modelÔºå next we„ÄÇWant to concatenate„ÄÇThe prefix embeddings together„ÄÇ

So here our window size is just twoÔºå so we're going to concatenate these two dimensions together and sorry these two embeddings each associated with the prefix together„ÄÇ

 So our ideal expected output is going to have a dimensionality of four by 10 right the first five dimensions of that 10 dimensional vector I'm going to correspond to the prefix embedding of the first word and the second five are going be the embeddings of the second word„ÄÇ

 So in this case we can simply just accomplish this with a reshape operation So if I say the concatet ms equals ms and the view function is a good way of reshaping your data„ÄÇ

 So I've already got this batch size which I know is for„ÄÇ

 but we're just going to use these variables in case you know at some other point„ÄÇ

 maybe a test time I want to use a batch size that's not four right I can use a variable„ÄÇüòä„ÄÇ

Number for my batch size„ÄÇSo I know the first I want to keep the four prefixes distinct from each other and if I put in negative one„ÄÇ

 that's just going to essentially squeeze the rest of the dimensions into a single vector„ÄÇ

 but in practice of course I know that this is going to be window size„ÄÇüòäÔºåWhich I did not store here„ÄÇ

 sorry„ÄÇCanËøô‰∏™„ÄÇWindow size times„ÄÇLet me just store a variable for the embedding dimensionality„ÄÇ

So dot D„ÄÇOkayÔºå so this simply takes this embeddings variable and reshapes it to a4 by 10„ÄÇ

 let's make sure this actually worked„ÄÇHopefully it did„ÄÇOkay„ÄÇ

 so I'll put some print statements along the wayÔºå so„ÄÇWe can make sense of what's going on„ÄÇ

Embbedding size„ÄÇ and yeahÔºå as a practical tipÔºå the size„ÄÇ

 the dimensionality of these intermediate variables that youre creating is one of the most useful things that you can look at because a bunch of the errors that you're going to get if you're implementing this and other more complicated networks are going to be maybe you messed up and you didn't keep track of your dimensions properly„ÄÇ

 you swapped around window size and embeing dimensionality„ÄÇ

 And now your matrix multiplications don't work and so on„ÄÇ So you should always keep track of„ÄÇüòä„ÄÇ

The size of the variables that you're creating in the forward function„ÄÇ Alright„ÄÇ

 so let's print these things out„ÄÇ WhoopsÔºå what did I call it„ÄÇOhÔºå Ds„ÄÇOkay„ÄÇ

 so we see that the embedding size was4 by 2 by5 and now the concatenated embedding size is 4 by 10„ÄÇ

 so we remove the sequence dimension and just concatenate all of the embeddings associated with the prefix together to form one 10 dimensional vector per input prefix„ÄÇ

ÂóØ„ÄÇüéºOkayÔºå and maybe we can validate thisÔºå rightÔºå So if I print the embeddings out for the first prefix in my my batch„ÄÇ

 which corresponds to„ÄÇBob likes„ÄÇÂóØ„ÄÇSo if I print this outÔºå I get these two embeddings„ÄÇ

 you can see there's two separate embeddingsÔºå one on each line„ÄÇAnd if I print out„ÄÇCan cat Ms zeroÔºü

You see there's just one 10 dimensional vector where the order here is the same as the order here is just we've squeezed these two vectors together„ÄÇ

OkayÔºå so„ÄÇNow let's get on with the next computation„ÄÇNow we project this to the hidden space„ÄÇ

And it's very easy to do thisÔºå we can just use our linear layer that we defined previously so I can say hiddens self„ÄÇ

whi„ÄÇAnd couldn cat ems„ÄÇAnd so now if I want to look at the size of this„ÄÇ

 remember that I've specified a hidden dimensionality of 12 here„ÄÇ

 So hopefully this thing is going to be„ÄÇA4 by 12„ÄÇMattrixÔºå and it is great„ÄÇ Okay„ÄÇ

 so I've gotten to this point„ÄÇ Now I'm at the hidden space„ÄÇ and finally„ÄÇüòä„ÄÇ

Project His to vocabularies„ÄÇ„Åß„Åô„ÄÇAnd againÔºå we just follow this execÔºå in factÔºå I will just copy this„ÄÇ

So we'll call this out„ÄÇOut equals w out„ÄÇHidsÔºå and we're going to print„ÄÇ

Output size equals outtop size„ÄÇAnd 11Ôºå so our vocabulary remember is 11 because there's 11 word types„ÄÇ

 we have this word is that appears twice and all the other words appear just once each„ÄÇ

 so we have a vocabulary size of 11„ÄÇüòäÔºåOkayÔºå so are we done wellÔºå if we print out„ÄÇ

We will see that these are definitely not probability distributionsÔºå right„ÄÇ

 and the reason for this is because we have not applied the softmax function„ÄÇ

So in practice to compute our loss functionÔºå which remember is the negative log probability of the target word„ÄÇ

 we're going to do a log softmax followed by the negative log likelihood„ÄÇ

 but Pytorch actually combines both of these into a single loss function called the cross entropy loss as we'll see later„ÄÇ

 but just as an example„ÄÇüéºMaybe I want toÔºå you knowÔºå show that I can get probabilities out of these„ÄÇ

 these outputs„ÄÇ so I can do this by„ÄÇ and you saw this also in„ÄÇIn homework zero„ÄÇ

 I can apply to softftmax to outs and I have to specify the dimension„ÄÇ

 so here I want to take this over the not the batch dimension but the actual the logics„ÄÇOops„ÄÇ

 I didn't print it out„ÄÇSo if I did this„ÄÇThen„ÄÇThe resulting vectorsÔºå as you can see here„ÄÇ

 this is for the first prefixÔºå you can just take my word for it that this sums up to one and you can see that all of these numbers are positive„ÄÇ

üòäÔºåActually we can„ÄÇEven„ÄÇShow that they sum up to oneÔºå so these are the sums of each of the vectors„ÄÇ

Which just enforcing that we can get probability distributions out of these if we wantÔºå but for now„ÄÇ

 we're just going to return the unnmalized probabilities„ÄÇAlso known as Los„ÄÇ

 so the logics are the values of this vector before applying the Somax„ÄÇOkay„ÄÇ

 so now that we're confident that at least the network isn't crashing right when I feed the inputs through it„ÄÇ

 I'm going to get rid of some of these print statements to make it cleaner„ÄÇüòä„ÄÇ

And we get this as outputÔºå the logics„ÄÇ And you'll see here that I have this grad function thing when I print out this out tensor„ÄÇ

 And this is what's going to allow us to perform backpro„ÄÇ The pytorrch just keeps track of„ÄÇüòä„ÄÇ

All the computations I need to do in order to compute the gradient for everything that I've defined in my forward function„ÄÇ

 so every one of these operations when I embed something or when I reshape it when I apply a linear layer„ÄÇ

 all of these have been in ptorch sort of paired up with the functions that you need to apply to compute the derivatives whenever they're applied and so we because these things are all this functionalities already there„ÄÇ

 we don't have to manually compute the derivatives ourselves„ÄÇOkay„ÄÇ

 so let's just assume that this network is workingÔºå hopefully it is„ÄÇ

 Now let's actually write a training loop to train this model„ÄÇüòäÔºåSo to do that„ÄÇ

 I need to define a couple of things„ÄÇFirstÔºå I need to define something called the number of epochs that I'm going to train my model„ÄÇ

 So let's say„ÄÇU„ÄÇSet it to 10„ÄÇ So an epoch is essentially one pass through the training data„ÄÇ

 So the number of epochs defines how many times we are going to go through our entire training data set„ÄÇ

 In this caseÔºå it's just four prefixes„ÄÇ This is aÔºå againÔºå super simple toy example„ÄÇ

 The next thing we want to do is define our learning rate„ÄÇ Remember„ÄÇ

 when we're doing gradient descentÔºå We need to define the step size„ÄÇ

 How big of a step do we want to take in the direction of the negative gradient„ÄÇüòä„ÄÇ

We can maybe we'll set it to 0 one„ÄÇ againÔºå this is a hyper parametermeter so we we can„ÄÇ

Modify this if training isn't„ÄÇConvergingÔºå etc ceÔºå we saw some of the failure cases associated with big learning rates„ÄÇ

 you might see the loss zigzagging around or going up in some cases if your learning rate is too big„ÄÇ

 so it's important to you mess with it until you see a nice smooth curve in your loss function that's decreasing„ÄÇ

So Pytorch defines this cross entropy plusÔºå you can see it here„ÄÇ

 and this applies the log Softmax function and then applies a negative log likelihood computation which accomplishes exactly what we went through on the iPad„ÄÇ

 it computes the negative log probability of the target word and averages it across all of the examples in the batch„ÄÇ

So average„ÄÇCross entropy loss over each prefix in batch„ÄÇOkay„ÄÇ

 and the last thing we need to do is select an optimizer so we went over simple gradient descent to get that we can do torch„ÄÇ

opim„ÄÇsgdstochastic gradient descent all optimizers require you to pass in the networks parameters which you can do by just doing network do parameters„ÄÇ

üòäÔºåNot too difficultÔºå and then we have to pass it in our learning rateÔºå which we specified up here„ÄÇ

OkayÔºå so„ÄÇAlrightÔºå maybe I will„ÄÇWe will use vanilla gradient descent„ÄÇYou can experiment with others„ÄÇ

Adam„ÄÇAnd stuff simply by just changing the so you have atomÔºå AM maxÔºå othersÔºå these are all different„ÄÇ

 more complex and sometimes better variants of SGD that help for particularly complicated networks„ÄÇ

OkayÔºå so next we're going to write our training loop„ÄÇSo this this loop will pass„ÄÇ

 it'll be over the number of epochs loopops for IN„ÄÇRange epochs„ÄÇ

 we're going to loop over their training set„ÄÇWe're going to compute these logicsÔºå right„ÄÇ

 I'm returning those in the forward function„ÄÇSo I will call my network on the what did I call this thing prefixes„ÄÇ

 YeahÔºå the prefixesÔºå the input tensor that I've made„ÄÇÂóØ„ÄÇ

And then I'm going to compute my loss functionÔºå The cross entropy lossÔºå very easy to do this„ÄÇ

 I will pass in the logicsÔºå and I will also pass in„ÄÇüòäÔºåThe labelsÔºå which contain the„ÄÇ

 the words that I'm supposed to predict„ÄÇOkayÔºå so I have computed the labels„ÄÇAnd no„ÄÇ

Let's update our PRs to make the loss smaller„ÄÇ So before we do this„ÄÇ

 maybe we can just print the loss„ÄÇ firstÔºå I want to see if this was working and I didn't make any dumb errors„ÄÇ

 And secondlyÔºå we should observe the loss is not changing„ÄÇ

So why isn't the loss changing because we haven't actually updated the parameters using our optimizer„ÄÇ

 So when we do thisÔºå hopefully our loss will start going down„ÄÇSo the first thing we have to do„ÄÇ

 remember is„ÄÇStep oneÔºå compute to gradient„ÄÇOkay gradient„ÄÇÂóØ„ÄÇ

Partial derivatives of loss with respect to parameters„ÄÇ

And I remember before I told you this was just one line and it's literally this simple„ÄÇ

 I just type lost stop backward and done I have the gradient I have the partial derivatives with respect to every peri and next„ÄÇ

Update perms using gradient descent„ÄÇTo do thisÔºå I take my optimizer that I've created from before and just type step„ÄÇ

ÂóØ„ÄÇI think I also want to zero the gradients„ÄÇSo this will„ÄÇStep  three„ÄÇ

 zero out the gradients for the next epoch„ÄÇAll rightÔºå let's put some spaces make this more readable„ÄÇ

So if I don't do thisÔºå the optimizer is going to accumulate the gradients across every epoch which we don't want„ÄÇ

 So after we make one stepÔºå we want to go backÔºå use our new model parameters to compute a new loss function„ÄÇ

 recompute the gradientÔºå take another step„ÄÇ So this zero grad„ÄÇ

 make sure we don't accumulate gradients across the different epochs„ÄÇAllright„ÄÇ

 so let's go ahead and also print out our loss function„ÄÇ

 So I will print out the epoch that we're on and the loss„ÄÇAnd I'll make the loss a little readable„ÄÇ

LosÔºå all rightÔºå so whoops„ÄÇWhat did I do hereÔºüNot all arguments converted during stringÔºå okay„ÄÇ

 whatever„ÄÇMaybe I will just„ÄÇOhÔºå I forgot a„ÄÇPercentage sign„ÄÇO„ÄÇGreatÔºå so we see the loss is going down„ÄÇ

 it started at 2„ÄÇ64 and now it's at 1„ÄÇ27„ÄÇ let's see if we can make it get to zero so let's train it for maybe 20 epochs„ÄÇ

All rightÔºå so it's getting down pretty close to 0Ôºå we started at 2„ÄÇ24„ÄÇ

 this is the average loss across all the elements in the prefix and we got down by Epoch 20 to 0„ÄÇ17„ÄÇ

 so huge reduction in the loss„ÄÇSo to conclude„ÄÇLet't say„ÄÇÂëÉ„ÄÇOur loss has dropped to close to zero„ÄÇ

 but are we confident that this is actually workingÔºå So we would be confident if I can put in„ÄÇ

 say Bob likes into this model and observe that it predicts sheep with a very high probability right„ÄÇ

 or if I put in Alice is and it predicts fast„ÄÇSo is this actually happening orÔºü

Is our loss low but our model still isn't predicting reasonable„ÄÇConditional probabilitiesÔºå let's see„ÄÇ

But is it actually workingÔºüLet's see„ÄÇOkayÔºå so note that up till this point I've been working entirely with the indices corresponding to the words„ÄÇ

 but now I might want to actually see like the probability that the model assigns to a particular word following the prefix so to make this easy I can define this reverse vocabulary mapping„ÄÇ

üòäÔºåP's„ÄÇFirstÔºå define„ÄÇA reverse„ÄÇüéºVocabulary mapping„ÄÇFrom I from index to word type„ÄÇ

We can call it Rev vocab and I can do this by simply iterating over all of the items in my vocab dictionary„ÄÇ

ÂóØ„ÄÇOkayÔºå did this workÔºüI think IÔºå ohÔºå whoopsÔºå I forgot a„ÄÇItemsÔºå so that should work„ÄÇ

And let's just see what this looks like to be clear„ÄÇ

So it's just the reverse of the vocab zero is mapped to BobÔºå one is mapped to likes and so on„ÄÇOkay„ÄÇ

 so let's just see what happens when I put in Bob likes„ÄÇ

 I know that's the first element of my prefix tensor so„ÄÇ

Ill just call this variable above like equals prefixes zero„ÄÇüéºSo prediction equals„ÄÇNetwork„ÄÇ

Bob likes and since my model is set up to take in two dimensional inputs right where the first dimension is the batch size„ÄÇ

 I need to feed it if I'm interested only in this bo likes prediction„ÄÇ

 I need to feed it a 2D matrix not just a single vector and so I need to make this batch size dimension one so right now where am I„ÄÇ

Down hereÔºå this bo likes variable if I print its size„ÄÇIt's just a two dimensional vectorÔºå right„ÄÇ

 because it has the indexes for Bob and likes„ÄÇ And what I'm going to do is unsqueeze„ÄÇ

Which means I create a new dimension and I put it in the first dimension and this is going to be the equivalent of my batch size and unsqueeed just makes the new dimension size one„ÄÇ

 So if I do this now you see it's a one by two and now I can pass it into my model„ÄÇüòäÔºåSo if I do this„ÄÇ

 now I get a predictionÔºå let's see what the prediction looks like„ÄÇAgain„ÄÇ

 these are the unormalized logicsÔºå so maybe to make this more readable„ÄÇ

 let's go ahead and apply the softm function„ÄÇI already imported thisÔºå rightÔºü

Softm prediction equals oneÔºå let's just call this probabilities„ÄÇCall it pros„ÄÇ we'll call this los„ÄÇ

 We'll call it this„ÄÇLogetsÔºå and I have to specify the dimension of one„ÄÇ So if we print pros„ÄÇ

Now we get the actual probability distribution„ÄÇ These are not the log probabilities„ÄÇ

 just to be clearÔºå these are the„ÄÇThe raw probabilities„ÄÇ and if you'll notice this is also a 1 by 11„ÄÇ

 so if I want to get it into if I want to flatten it„ÄÇ

 so basically do the reverse of this unseze operation I can do squeeze„ÄÇ

 which gives me just a single vector„ÄÇAnd now I can find the index that has the max probability„ÄÇ

 rightÔºå This is what the model is actually predicting so I can„ÄÇüòä„ÄÇ

rite out the AGm index equals just use the AGmax function„ÄÇAnd if I want to get„ÄÇ

 so this is going to return a tensor objectÔºå if I want to actually use this to index into my revv vocab mapping„ÄÇ

 I have to use the dot itemÔºå which will get it back into just a Python number„ÄÇSo„ÄÇLet's go ahead and„ÄÇ

üéºMake this more specific„ÄÇ So given Bob likesÔºå the model predicts„ÄÇThe word as the next word with„ÄÇ

Let's give it„ÄÇLet's say„ÄÇThe precision„ÄÇProbability„ÄÇAnd we will„ÄÇÂóØ„ÄÇThe rev vocab of the Agm index„ÄÇ

 So this is going to reverseÔºå map the Argm index into a word in our vocabulary„ÄÇ

 And let's also look at the probability of that word„ÄÇOkayÔºå hopefully this worked It did not„ÄÇGod„ÄÇ

 I'm really struggling with the O„ÄÇWhat have I done wrong hereÔºå OhÔºå I forgot parentheses„ÄÇOkay„ÄÇ

 so given Bob likesÔºå the model predicts sheep as an next word with 0„ÄÇ7851 probability„ÄÇ

 get rid this to make it easier„ÄÇAnd this is this is not greatÔºå rightÔºü

 There's only there's four prefixes in hereÔºå so I would hope that this can get closer to one„ÄÇ

 And maybe if I train for more epochsÔºå I will„ÄÇ So let's go ahead and train this for 100 epochs„ÄÇüòä„ÄÇ

OkayÔºå my loss is now at 0„ÄÇ02Ôºå so it's very low„ÄÇ I expect this to be a much higher probability„ÄÇ

 all rightÔºå98„ÄÇ93 probability„ÄÇ So you see that as a loss gets lower and lower„ÄÇ

 the probability of predicting the right continuation increases„ÄÇAll rightÔºå so just to summarizeÔºå oh„ÄÇ

 this video has gone„ÄÇMuch longer than I anticipated as usualÔºå I guessÔºå in this notebook„ÄÇ

 we've gone from brassÔºå just text of four different sentences to a fixed window neural language model that can predict the next word given any of these four prefixes„ÄÇ

 SoÔºå of courseÔºå this is not a usable modelÔºå rightÔºå It's a very simple vocabulary„ÄÇ

 We are evaluating the model on our training dataÔºå which you know„ÄÇ

 I said was one of the things you definitely should not ever do„ÄÇ this is just a demonstration„ÄÇ

 there's no expectation that this model is going to generalize to anything outside of its training set„ÄÇ

üòäÔºåAnd so yeahÔºå in practiceÔºå you would want to have a separate test set that you're evaluating your model on that it hasn't seen before„ÄÇ

OkayÔºå so with thisÔºå we've concludedÔºå we've kind of synthesized all of the concepts that we've talked about till this point right this notion of tokens and types„ÄÇ

 the idea of going from words to indices which then allow us to index into an embedding matrix which is then composed using a neural network architecture in this case a very simple one to get an output probability distribution right and how do we actually train the model to produce a better estimate of these conditional probabilities we first define a loss function that tells us model how bad the model is doing„ÄÇ

 we compute the loss on our training data we compute the partial derivatives I didn't actually get to show you much regarding this step because it's again one line„ÄÇ

 so„ÄÇüòäÔºåWe compute the we do back propagation on this one line„ÄÇ

 and we do gradient descent in this line„ÄÇAnd once we loop over our training data a bunch of times„ÄÇ

 we get a model that achieves a very low loss„ÄÇ and we see that the model is doing a pretty good job at predicting„ÄÇ

What it sees in its training dataÔºå Bob likes sheep„ÄÇOkayÔºå so for next time we're going to focus on„ÄÇ

The actual architecture that goes into the forward function here so we talked about two different ones in addition to this fixed window model„ÄÇ

 we talked about the recurrent neural language model„ÄÇ

 but for next time we're going to move to the state of the art composition function which is the transformer so we're going to start with selfattention„ÄÇ

 which is a basic building block of the transformer and then move on to how the forward propagation actually looks in the transformer this is all setting up for next week where we talk finally about the applications of largescale transformer language models for solving NLP tasks that are not language modeling you all are probably sick of just looking at language modeling by this point„ÄÇ

 but I promise soon we will advance past this but it's very important to get familiar with howÔºåüòä„ÄÇ

These models work and how they're trained and so on„ÄÇAll rightÔºå great„ÄÇ



![](img/3aa8ae5afa5ccba3efd817626d99d2d3_4.png)