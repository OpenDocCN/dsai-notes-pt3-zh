# P9ÔºöL7.1- NLP ÁöÑËøÅÁßªÂ≠¶‰π† - ShowMeAI - BV1BL411t7RV

ÂóØ„ÄÇOkayÔºå I think this may be working„ÄÇIf you can hear me„ÄÇ

 can you write something in the chat box to let me know that this is working„ÄÇÂóØ„ÄÇ



![](img/f1591e808e9bc70bdffc1770f073cdfc_1.png)

YesÔºå okayÔºå cool„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_3.png)

OkayÔºå so let's get started„ÄÇHopefullyÔºå this oh„ÄÇWhat just happened„ÄÇÂóØ„ÄÇOkayÔºå I think it's working„ÄÇOkay„ÄÇ

 please let me know in the chat box„ÄÇ if like the stream quality is lowÔºå we did test this earlier„ÄÇ

 but„ÄÇAnd we didn't have any problemsÔºå but it just said it like dropped a lot of framesÔºå so„ÄÇAnd yes„ÄÇ

 there is a delay of about 20 ish seconds„ÄÇ So hopefully that won't be too awkward„ÄÇOkay„ÄÇ

 so let's get started„ÄÇ today's topic is transfer learning using neural language models„ÄÇ

And in this lectureÔºå we're going to talk about„ÄÇAn intro to concepts underlying the current strategies by which people are using transfer learning„ÄÇ

 we're going to build up from replacing the word embedding component of the models that we've seen with actually sort of pre-training the the model parameters themselves outside of just the word embeddings„ÄÇ

OkayÔºå so before we get started with the materialÔºå just some stuff about the class logistics„ÄÇ

 your project proposals are due September 21st the overleaf template is out„ÄÇ

 a lot of you have come to our office hours and you know asking about your projects and we've given some advice if you're still uncertain about the topic that you've chosen please either send us an email or visit one of our office hours before the deadline so you can get some feedback„ÄÇ

 but all the ones that we've seen until this point seem pretty good well thought out and substantial amount of work which is what we're looking for in these projects„ÄÇ

OkayÔºå so other than thatÔºå your homework oneÔºå there's a bit of a delay on that we're trying to make it such that you get some experience using these large scale models that we are going to be talking about in this lecture in the ones next week but we also want it to not take like five hours to run a cell on coab so there's a balance here that we're trying to figure out still and it'll be out sometime next week„ÄÇ

OkayÔºå so any other questions about logistics or material or anything please put in the chat box„ÄÇ

 I am looking at the chat box now instructor video is quite fuzzy all right I'll try and look into that next time I do one of these streams I'm just happy that it's working at all honestly„ÄÇ

AlrightÔºå so our topic for today isÔºå do neural networks really need millions of labeled examples to„ÄÇ

 to solve a bunch of different NLP problems„ÄÇAnd so we've been focused primarily this semester on language modeling„ÄÇ

 which is going to form one of the key building blocks of our transfer learning setup and now we're kind of going switch over to the other side of that framework right so we have a task that we care about right like sentiment analysis or something like that„ÄÇ

 how can we actually use our huge neural language models„ÄÇ

 which as you guys know at this point have been trained using these self-supervised objectives can really benefit from a huge number of examples to learn all sorts of important and relevant linguistic properties can we actually use these to help our performance on the downstream task„ÄÇ

So this is the setting for today„ÄÇAnd we've already gone over this before„ÄÇ

 but maybe a more specific definition of transfer learning in our particular context is we have a network that's trained on a task for which it's easy to generate labels so in our case this is language modeling right and we're going to adapt it to a different task where it's hard to get labeled examples right so in sentiment analysis„ÄÇ

 I need a bunch of reviews that people have actually written along with the scores that are assigned to these reviews„ÄÇ

OkayÔºå so for those of you who are familiar withÔºå you knowÔºå the modern research and computer vision„ÄÇ

 this concept of transfer learning first really got got picked up in the computer vision community„ÄÇ

 So computer vision has this huge data set called Inet„ÄÇ

 which is like on the order of million something images„ÄÇ

 each of which has been labeled with the kind of object that is in those image„ÄÇ

 So like if there's a catÔºå then you might have a cat labelÔºå if there's a dog or a„ÄÇYou know„ÄÇ

 a bed or somethingÔºå these images would be labeled with those the objects that are the primary focus of that image„ÄÇ

And so for computer visionÔºå what people do is they take the Inet dataset„ÄÇ

 they train a large scale neural networkÔºå a convolutional neural network on this data set„ÄÇ

 and then they take the resulting network and transfer its representations and its model parameters to any other computer vision task of interest„ÄÇ

So in our caseÔºå we're going to be replacing the image image classifier component with a really large language model„ÄÇ

So just to bring back our„ÄÇA figure from earlier in the semester„ÄÇ

 we looked at this setup where we take this huge self supervised model„ÄÇ

 which in our case is a language model trained over a bunch of unlabeled text„ÄÇ

And then we transfer over its parameters to a downstream task and we'll go over more the exact terminology In this example we're looking at sentiment right so we have some labeled reviews from IMDB and we perform supervised fine tuning on our giant language model to make it kind of specialized to the task of sentiment analysis„ÄÇ

So in this lectureÔºå we'll go over the the stuff that is required to get this second step to work and some of the important ideas that have been proposed in the field to kind of further this paradigm„ÄÇ

Okay so the first paper that I want to discuss todayÔºå so if you actually did the reading„ÄÇ

 which I'm sure many of you didn'tÔºå we assigned the Elmo paper which is deep contextualized word representations as well as the Bt paper and both of these are instances of this transfer learning paradigm the Elmo paper kind of introduced the idea of using language models for many different NLP tasks„ÄÇ

 I mean along with some other workÔºå but it was the first one to actually show big improvements by using this paradigm and the Bt paper kind of built on this idea changed the model architecture from a recurrent model to a transformer changed the objective of the model so instead of predicting the next word„ÄÇ

 it predicted a masked word when it was surrounded by context on both sides„ÄÇAnd finally„ÄÇ

 it changed the way in which we actually transfer the parameters of the model over instead of just their representation„ÄÇ

 So all of this will become more clearÔºå hopefully later on in this lecture„ÄÇOkay„ÄÇ

 so I don't see any more questions in the chat box„ÄÇ

 everyone should feel free to chime in there and I will answer the questions whenever there's a natural time to pause and take them„ÄÇ

OkayÔºå so in all of the models that we've been discussing so far„ÄÇ

 the input to the model is you know the sequence of token indicesÔºå right„ÄÇ

 the index where each index corresponds to a word type from our vocabulary and then we use those indices as a kind of lookup table to index a specific word embedding that's associated with each index„ÄÇ

 so againÔºå a word embedding is a single vector that's associated with each unique word type in our vocabulary„ÄÇ

So this means that we represent words like playÔºå bankÔºå run each of these with a single vector right„ÄÇ

 and then we've looked at the role of neural networksÔºå recurrent networks„ÄÇ

 you fixed window language modelsÔºå transformers as a way to compose these vectors„ÄÇ

 these word embeddings over longer sequences so I can take you know four embeddings as input and get a single vector as output that represents that composed sequence of words„ÄÇ

So this approach has some intuitive issuesÔºå right So if I have a single word vector„ÄÇ

 a single vector per wordÔºå then I run into this issue when I try to go and apply this to a downstream task where some words have many senses and they have many ways in which they're used„ÄÇ

 So if you take the word play for example„ÄÇ I can use the word play as here like a play area for kids„ÄÇ

 I can use it as a verb like thisÔºå where it's kind of more metaphorical play to cozy up to the party base„ÄÇ

 or I could use it in the context of an actual physical play that happens in a sports game„ÄÇ

 It could also be a verbÔºå I could play baseball or something like this„ÄÇ

 and all of these different meanings of the word play are getting squashed into the single„ÄÇüòä„ÄÇ

so if I take a look at a learned word vector space„ÄÇ

 and so this one might be from something like word deveÔºå which we haven't covered in this class„ÄÇ

 but you can just think of it as a way of getting a word embedding space in which words that are similar to each other„ÄÇ

 either syntactically or semantically are close to each other distancewise in this vector space„ÄÇ

 like through the cosine distance„ÄÇSo if you look at the cosine distance from the word embedding of play to the word embeddings of all of the other words in a vocabulary„ÄÇ

 the closest ones are words like playingÔºå gamesÔºå gamesÔºå playedÔºå playersÔºå and so on„ÄÇ

 all of these words„ÄÇBut if you look closely at these embeddings„ÄÇ

 it kind of shows you that there's a lot of different types of linguistic properties that are getting kind of clustered up and mixed together here„ÄÇ

 So we have two verbs as the nearest neighbors playing and played We have nounsÔºå right gameÔºå games„ÄÇ

 playerÔºå playerÔºå football„ÄÇ And we have some adjectives like multiplayer„ÄÇAnd so„ÄÇ

The real meaning of the word play depends heavily on the context that it appears inÔºå right„ÄÇ

 That's very clear„ÄÇ If I want to differentiate between like play used in as synonym for a drama or theater versus play in the context of football„ÄÇ

 I will know that from the surrounding words and the rest of the words in that sentence„ÄÇ

So this is kind of an issue when we have something that takes a pre-trained word embedding as input right„ÄÇ

 if I have open a bank account and on the river bank„ÄÇ

 the word embedding for the word bank here in both cases„ÄÇ

 the input is going to look exactly the same rightÔºå it's going to be the same vector„ÄÇ

 but we know intuitively that these words the word bank here is being used in two different senses and really they probably shouldn't be similar vectors used here„ÄÇ

And so it would be really nice if we had a way of contextualizing these representations such that the representation for the word bank in the first sentence looks completely different than the representation for the word bank in this sentence on the river bank or phrase„ÄÇ

So this idea of contextualizing the word representations is kind of critical to the transfer learning stuff that we'll look into and then towards the end when we start talking about BRT„ÄÇ

 we'll look at not just the embedding levelÔºå but also sharing multiple parameters higher up in the network apart from just the embeddings„ÄÇ

OkayÔºå so I see a question about whyÔºå why do we use cosine similarity„ÄÇ

 I suppose referring to when we get these nearest neighbors„ÄÇ

 it depends on how this vector space is learned so for example„ÄÇ

 in word deve the dot product is used to in the objective function itself to push words that are similar to each other that occur near each other close by and push words that are just randomly sampled from the vocabulary farther away„ÄÇ

 but you could potentially set up a word embedding algorithm where other distance functions are used and in that case you could use those as well for interpreting this space„ÄÇ

 but it depends on the way in which these vectors are learned and in general„ÄÇ

 the cosine distance and measures built on the dot product are probably like the simplest ones so we tend to stick with„ÄÇ

Those„ÄÇOkayÔºå so let's see if I can do this„ÄÇ I wanted to switch over to the iPad to really kind of give some intuition on what„ÄÇ

 what the whole point of this transfer learning and contextualized embeddings are„ÄÇ

 So let's see if I can do this„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_5.png)

OkayÔºå that seemed to work pretty well„ÄÇWhoops that is not what I wanted to do„ÄÇOkayÔºå I„ÄÇWanted to„ÄÇ

Maximize the„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_7.png)

St sheet with this„ÄÇOkayÔºå I guess this is the best it's going to get„ÄÇAll rightÔºå so before I start„ÄÇ

 I will take the next question„ÄÇ when we create a word embedding using the entire text„ÄÇ

 does it take into account all the context in which the word can be used and give one embedding„ÄÇ No„ÄÇ

 So ohÔºå it depends on what kind of word embedding you're thinking of so that the old the old like 2013 style word devec approach does do what you say in which a single word is assigned to a single embedding and this embedding is updated using all of the context in which that that word occurs„ÄÇ

 but the newer ones the ones that we're going to go over a little later in this lecture don't„ÄÇ

 And so that'll be more clear later on„ÄÇOkayÔºå so let me start„ÄÇ

Think you guys can see the iPad with a sentiment analysis example to hopefully make things a little more concrete„ÄÇ

 so„ÄÇIs this showing upÔºüI'm just going to see this is showing up„ÄÇHow we used to solve„ÄÇNÔºå LP tasks„ÄÇ

So you can think about this like before the advent of pre trained word embeddings„ÄÇ

 maybe around 2013 ish„ÄÇÂóØ„ÄÇAnd let's look at the task of sentiment analysis„ÄÇ

So here we've kind of seen examples of this this task beforeÔºå rightÔºå I have a sentence„ÄÇ

 this movie is great„ÄÇI have a word embedding associated with each of these words„ÄÇI„ÄÇ

 let's say I'm going to use a recurrent neural network here„ÄÇÂëÉ„ÄÇ

And we've seen recurrent neural networks in the context of language modeling„ÄÇ

 but that doesn't mean we can't use it for any other task„ÄÇ right„ÄÇ

 I can use it for sentiment analysis using the exact same setup that we use for language modeling„ÄÇ

So remember that in a recurrent neural networkÔºå the final hidden state hereÔºå this one in the green„ÄÇ

Is then fed to the softm layer„ÄÇPredict positive in this example„ÄÇRightÔºå so this was our goal to„ÄÇ

 to get a model that can predict positive or negativeÔºå given some input„ÄÇ And back in 2013Ôºå we„ÄÇüòä„ÄÇ

Randomly„ÄÇInitialize„ÄÇThe model of parameters„ÄÇSo these are WÔºå HÔºå W„ÄÇE and the word embeddings„ÄÇ Remember„ÄÇ

 we call these the„ÄÇC sub IsÔºå and we have as many of these as we have word types in our vocabulary„ÄÇOh„ÄÇ

 I forgot our output matrixÔºå the one used in the softmax„ÄÇAnd thenÔºå we train„ÄÇsorry„ÄÇUpdate„ÄÇ

All parameters„ÄÇBy back prop„ÄÇUsing„ÄÇCross entropy loss„ÄÇFrom„ÄÇLabeled„ÄÇTraining set„ÄÇ

So we have a labeled training set and we're going to train all of the parameters of this model from scratch„ÄÇ

 they start with random values and then they get updated through gradient descent using the signal of if the model predicted that the sentence was negative it would get an error signal from softmax„ÄÇ

 and the parameters would be updated to hopefully next time predict a positive label for this sentence„ÄÇ

So this is just using the the same concepts that we've learned throughout this semester„ÄÇ

So then the issue with this is that I'm asking the model to learn all of these parameters from scratch„ÄÇ

And me use a different color„ÄÇ So all of these parameters are learned from scratch„ÄÇ

All train from scratch„ÄÇAnd this setup basically assumes that we don't have access to any other labeled or sorry„ÄÇ

 any other text at all„ÄÇ rightÔºå The model has to learn„ÄÇ and maybe I'll write this down as wellÔºå model„ÄÇ

Has to learn„ÄÇHow„ÄÇLanguage worksÔºå right„ÄÇEssentialÔºå it has to learn all of the important properties of language„ÄÇ

From„ÄÇOnly a small„ÄÇWable data set„ÄÇSo this is really hardÔºå right„ÄÇ

 What if your labeled data set is smallÔºå maybe it only contains likeÔºå I don't know„ÄÇ

1000 reviews with the sentiment labels„ÄÇ What do we do then„ÄÇ

 or do we really expect our model to learn you know„ÄÇ

 fairly complex linguistic phenomena from such a small amount of text„ÄÇ

 It seems like we can do better„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_9.png)

So the next advance in„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_11.png)

ÂëÉ„ÄÇSoÔºå around„ÄÇThe years of 2014 is to maybe 2017 was a focus on word embedding„ÄÇ So people asked„ÄÇ

 why train everything from scratch„ÄÇAnd„ÄÇHow can we„ÄÇBeverage„ÄÇLo of„ÄÇUnlabeled„ÄÇÂØπÁöÑ„ÄÇ

And the solution that we„ÄÇCame up with by weÔºå I meanÔºå the broad N LP communityÔºå not„ÄÇÂóØ„ÄÇ

You or me personally„ÄÇWas to take this same framework„ÄÇHere„ÄÇ

And pre train the embedding part of this model„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_13.png)

So the thought wasÔºå basically„ÄÇWe can„ÄÇPre train„ÄÇThe C sub I„ÄÇWith the word embeddingsÔºå right„ÄÇUsing„ÄÇ

Another objective„ÄÇThat„ÄÇTakes advantage„ÄÇOf unlabeled data„ÄÇ

So this means you know a selfsupvised data set where I can get labels for free from text„ÄÇ

 And so this era of research included things like word devec and glove„ÄÇ

 which you might have seen before we did not cover them in this class and in fact„ÄÇ

 we are not covering lots of things in this class from just even a couple of years ago„ÄÇ

 even things like variance of recurrent neural networks we're not going to cover But I wanted to give some intuition here of how the field has progressed to the point in which which we're at right now„ÄÇ

 so we have these modelsÔºå word devec and gloveÔºå which use self-supvised objectives to learn good single word embedding„ÄÇ

 static word embeddings„ÄÇ And so here instead„ÄÇOf starting„ÄÇWith a random„ÄÇWord embedding space„ÄÇWe start„ÄÇ

From„ÄÇA pre trained„ÄÇSpace in which„ÄÇWord„ÄÇEbeddings„ÄÇCapture some„ÄÇLinguistic properties of the word„ÄÇ

So hereÔºå likeÔºå we are still trainingÔºå train„ÄÇAll other parameters„ÄÇFrom scratch„ÄÇSoÔºå these are the„ÄÇÂóØ„ÄÇ

W H W E W O„ÄÇ So the important distinction here is that we're giving some information to the model that comes from outside of the current labeled the sentiment data„ÄÇ

 rightÔºå the fact that maybe movie appeared in conjunction with the wordÔºå I don't know„ÄÇ

 T or something like that a lot of times in some large scale web corpus that should be helpful for learning you know how the word movie should behave in the sentiment data„ÄÇ

 On the other handÔºå we are not pretraining the actual composition part of this networkÔºå right„ÄÇ

 So the recurrent stepÔºå the recurrent neural network is still responsible for learning how to compose these pretrained word embeddings into something that represents the meaning of the entire sentence„ÄÇ

 the movie is great„ÄÇSo we were at this stage for a few years where people were„ÄÇ

 you know trying to build up more and more fancy methods for getting better word embeddings and then around 2018„ÄÇ

 we started thinking about some of the drawbacks of this current paradigm and how we can actually extend the self supervised step to the composition function„ÄÇ

So the issuesÔºå issues„ÄÇWith„ÄÇThis set„ÄÇOur word embeddings are static„ÄÇ

 So we already looked at some examples of this from before„ÄÇA static„ÄÇOnly one vector„ÄÇPer„ÄÇWord type„ÄÇ

Regardless of context„ÄÇSo we're actually expecting our composition function to learn how to disambiguate all these different senses of a word„ÄÇ

 like figure out what role the word play is playing in in the current sentenceÔºå rightÔºü

 and this is a lot to ask if if your other parameters of your modelÔºå the WÔºå HÔºå the W E„ÄÇ

 the W O are being trained from scratch on just a labeled data setÔºå right„ÄÇ

So here the rest of the model„ÄÇIn our case„ÄÇIn this exampleÔºå I mean„ÄÇ

 the recurrent neural network is responsible„ÄÇFor learning composition„ÄÇFrom scratch„ÄÇGiven„ÄÇ

Just labeled data„ÄÇSo hopefully you can see at this point how we are asking so much from our model right we wanted to be able to properly learn how to compose different words into a meaningful representation of a sentence but if we only give it you a few thousand labeled examples of sentences it's not going to actually learn more finegrained properties of words or phrases or anything like that„ÄÇ

 it probably is just going to default to the easiest possible thing so in sentiment in our example right we have the movie is great and so this recurrent neural network doesn't actually have to learn the meaning of this phrase in in any way right it can just default to oh I observe the word embedding for great„ÄÇ

 let me forget the entire rest of the sentence and just make my final representation very indicative of the word great because if I do this then it'll be very easy for me to predict a positive label for this„ÄÇ

üòäÔºåAnd so if the model is defaulting to these kind of very simplified strategies„ÄÇ

 then it's not going to be incentivized to learn the meaning of more complicated sentences„ÄÇOkay„ÄÇ

 so now let's kind of integrate our neural language model that we've been talking about for all this time into this transfer learning framework„ÄÇ

And the idea isÔºå what if„ÄÇWe use the hidden states„ÄÇOf and neuralral language modelÔºå instead„ÄÇOf static„ÄÇ

Word and embeddings„ÄÇSo what does this meanÔºå Let's say we have the same sentenceÔºå right„ÄÇ

 The movie is great„ÄÇWe have also seen how to turn„ÄÇThis into a language modeling problemÔºå right„ÄÇ

 So we have a recurrent neuralral language model„ÄÇ So I'm gonna draw the hidden states in a different color„ÄÇ

Just to make the„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_15.png)

Intuuition more clear„ÄÇ So here„ÄÇI have these hidden states„ÄÇ

And I'm using these hidden states in a recurrent neural language model to predict the next word„ÄÇ

 rightÔºå So given the word thisÔºå I'm going to predict the word movie„ÄÇ

 given the representation of this movieÔºå I'm going to predict the word is„ÄÇ given the movie is„ÄÇ

 I'm going to predict„ÄÇGreat and so on right„ÄÇ So I have this recurrent neural language model that I can train on as much text as I can get my hands on to predict the next word„ÄÇ

 and I know that in doing soÔºå these hidden statesÔºå the ones in the blue are going to learn a lot of important information about that context that I might not necessarily expect the model to learn from just a small labeled training data set„ÄÇ

 So if I believe that these blue hidden states are capturing a lot of important linguistic information„ÄÇ

 then what is actually stopping me from using these as my word embeddings as opposed to the static word embeddings„ÄÇ

 the important point with these blue hidden states is that they've been contextualized„ÄÇ

 if I consider this hidden state here for greatÔºå this is not the same thing as the word embedding for great„ÄÇ

 right it's actually„ÄÇüòäÔºåThe entire the context that I've seen up till this point„ÄÇ

 this movie is great is being encoded into this vector and the recurrent neural sorry recurrent neural language model is probably emphasizing the word great in this representation because it's the word that appeared most recently„ÄÇ

 So I know that this this hidden state in blue has some knowledge of the word great„ÄÇ

 but also some knowledge of all of the text that preceded it„ÄÇ

 And so this is a big advantage compared to just using the static word representation for the word great„ÄÇ

üòäÔºåSo okayÔºå let me maybe move this down a little bit just to demonstrate„ÄÇWellÔºå that was cool„ÄÇOkayÔºå so„ÄÇ

 and maybe we'llÔºå we'll take this example again„ÄÇ the one from above„ÄÇSoÔºå instead of„ÄÇ

Initializing this model with„ÄÇPre trained static word embeddings„ÄÇOhÔºå wowÔºå it„ÄÇThis app is awesome„ÄÇ

 We're insteadÔºå going to„ÄÇüòäÔºåUse these hidden states from the neural language model as input„ÄÇ

So here these blue vectors are the same„ÄÇ These are the contextualized vectors„ÄÇ

 and now the model doesn't actually have to the parameters that I train from scratch„ÄÇ

 they don't have to deal too much with this process of compositionÔºå right„ÄÇ

 because the contextualized embeddings here„ÄÇContextualized„ÄÇWord embeddings„ÄÇThey„ÄÇ

 they already in incorporate a lot of theÔºå the composition that might be useful for this downstream task„ÄÇ

 So in this settingÔºå we still learn„ÄÇThe other parameters of the modelÔºå WÔºå HÔºå WÔºå E W O from scratch„ÄÇ

But we're starting with a much more powerful input representation„ÄÇ

 So this is the idea behind the ElLmo work„ÄÇOkayÔºå so at this pointÔºå I'm gonna take some questions„ÄÇ

OkayÔºå so effectivelyÔºå shouldn't this generalize to all languages it does We'll talk a little bit at the end about things like multilingual birtht„ÄÇ

 where you can apply these same concepts to many different languages That said some languages are I guess more amenable to these kinds of methods than others„ÄÇ

 know languages like Chinese have huge character sets„ÄÇ

 languages like Turkish don't have easy ways of splitting up the word boundaries because like a single sentence might just be a single string of join together segments„ÄÇ

 So yeahÔºå these concepts do generally applyÔºå but to some languages more than othersÔºå I guess„ÄÇOkay„ÄÇ

 so another question was about the bottleneck problem if we're trying to encode the context using recurrent neural networks that's true in this the notes so far„ÄÇ

 we're looking at a recurrent encoder just as an example in practice we are not going to be using a recurrent neural language model but rather huge scale transformer language model where we can get around this bottleneck I just wanted to show a very simplified example using a simple model and not go into the weeds of the transformer right now„ÄÇ

 we'll look at this when we talk about BRT„ÄÇOkayÔºå another question„ÄÇ

 If we use the hidden states for werenventsÔºå which are initialized randomly„ÄÇ

 Are't we using the randomly initialized values againÔºå YeahÔºå this is a good question„ÄÇ

 This is very important to clarifyÔºå so„ÄÇDown hereÔºå we are training this model„ÄÇ

Maybe I'll use a different color here„ÄÇThis model is trained from scratch„ÄÇ

This is called our pre training step„ÄÇAll prims„ÄÇRandomÔºå we initialized„ÄÇAnd trained„ÄÇ

With the self supervised objective„ÄÇSo importantlyÔºå here we are using millions„ÄÇ

 potentially or billions of words of text to train these parameters from scratch„ÄÇ

 right so we start with the randomly initialized values in the pretraining phase„ÄÇ

 But then when we move to the actual sentiment training phaseÔºå this fine tuning phase„ÄÇ

 we are no longer starting from random embeddingsÔºå but rather we're using these much more powerful hidden states that resulted from this huge scale pretraining process„ÄÇ

 So the idea is that we're getting around the fact that we don't have that much labeled data for„ÄÇ

 say the sentiment task by training a language model and as much text as we get„ÄÇ

 So I hope that answers your questionÔºå Kaishma But let me know if it's not clear„ÄÇ

 we're not using the randomly initialized embeddings at the„ÄÇ

Point where we're training the sentiment model„ÄÇÂóØ„ÄÇOkay„ÄÇ

 and are we indirectly using the hidden states as the word embeddings in this current setup„ÄÇ

 we are actually just directly using the hidden states as the word embeddings„ÄÇ

 And so if we have multiple hidden states associated with the wordÔºå we could have this„ÄÇ if„ÄÇ

 for exampleÔºå we have a deep recurrent neural network where we have a representation at multiple layers„ÄÇ

 we'll talk about this more when we discuss the Elmo model in detail„ÄÇ

 But there are some aggregation methods that we can use to aggregate information across multiple hidden states into a single representation„ÄÇ

 But we'll talk more about that in a bit„ÄÇOkayÔºå so just one more thing on the iPad before we switch back to the„ÄÇ

The slides„ÄÇIs idea of using language models to initialize the supervised sentiment task„ÄÇ

And one natural question you might ask„ÄÇ And so this was 2019 was„ÄÇCan we share„ÄÇMore parameters„ÄÇ

Then just the word embeddingsÔºå rightÔºå So to this point„ÄÇ

 we focused exclusively on the word embeddingsÔºå rightÔºå getting better„ÄÇ

 more powerful representations of the word embeddings„ÄÇWhat about these other parameters„ÄÇRight„ÄÇ

 what about in our current exampleÔºå the WÔºå H and the W EÔºå Can we share those also„ÄÇÂóØ„ÄÇ

And the answer isÔºå yesÔºå yesÔºå weÔºå we can„ÄÇ So how do I wantan to show this„ÄÇ Let me just copy this„ÄÇ

Language modelÔºå example over again„ÄÇSo this was our language modeling phase„ÄÇ

 And note that at every hidden state hereÔºå we are using the recurrent update equationÔºå right„ÄÇ

 So these are my H's„ÄÇAnd for this language modelÔºå I am using thisÔºå you knowÔºå the standard„ÄÇF of W H„ÄÇH„ÄÇ

 NÔºå-1 plus WÔºå EÔºå CÔºå N„ÄÇRightÔºå so I have my own parameters here„ÄÇPms„ÄÇFor language model„ÄÇ

And so what happens if I transfer over these parameters as wellÔºå right„ÄÇ

 so this means I'm actually kind of transferring over much more of the composition process than if I just use the parameters of my sorry„ÄÇ

 the hidden states of my model rightÔºå so now I'm writing rewriting these arrows in blue„ÄÇ

 which means that I'm eventually going to transfer over all of the parameters associated with these arrows to the downstream task model„ÄÇ

üòäÔºåSo let's say that I„ÄÇInitialize„ÄÇMaai„ÄÇüòîÔºåÂóØ„ÄÇSentiment„ÄÇR in N„ÄÇWith the„ÄÇLMsÔºå WÔºå H and WÔºå E„ÄÇ

 What happens hereÔºå alsoÔºå of courseÔºå the initial word embeddings„ÄÇAnd so nowÔºå all we have to do„ÄÇ

Is learn the W O matrixÔºå our output Somax matrix from scratch„ÄÇAll of the„ÄÇÂóØÂìº„ÄÇAll the other„ÄÇ

Pimeterters„ÄÇAre transferred„ÄÇAnd so in the endÔºå we have something that looks like this„ÄÇ

So hopefully last cut and paste„ÄÇÂóØ„ÄÇSo we have something like thisÔºå except I'm now going to„ÄÇWell„ÄÇ

 I shouldn't have even bothered cutting and tastinghu„ÄÇ

So all of the blue vectors here come from shared parameters or sorry„ÄÇ

 parameters that we've initialized from a neural language model„ÄÇ

 And the only thing that we're going to be training from scratch is the softmax layer„ÄÇ



![](img/f1591e808e9bc70bdffc1770f073cdfc_17.png)

![](img/f1591e808e9bc70bdffc1770f073cdfc_18.png)

And so this is kind of the current paradigm in N LP research„ÄÇ It's„ÄÇThis is current„ÄÇParaigm„ÄÇIn an LP„ÄÇ

 this was popularized by the Bt model„ÄÇWhich was one of your readings for the week„ÄÇ

And which we'll go over more very soon on the slides„ÄÇ And so just to summarize here„ÄÇWe in it„ÄÇ

 are sentiment model„ÄÇWith a pre trained language model„ÄÇ

And so remember that this pre trained language model encompasses„ÄÇEverything right„ÄÇ

 the word embedding parametersÔºå the composition network parametersÔºå like in this example„ÄÇ

 the recurrent update parametersÔºå the W HÔºå the W E And the only thing we randomly initialize is the final classifier in the softmax layer that's going to be projecting the hidden state into the two dimensional space to predict positive and negative„ÄÇ

 So we of courseÔºå can't transferred that softmax layer that parameter over from the language model„ÄÇ

 because the language models's outputs are the size of the vocabularyÔºå right„ÄÇ

 that's how many labels I have for sentiment analysis„ÄÇ

 I might just have two right positive and negative„ÄÇ So I have to learn my own parameter there„ÄÇüòä„ÄÇ

So we unit our model with this pre trained language modelÔºå and then„ÄÇBack prop„ÄÇTheer„ÄÇ

From a different task„ÄÇAnd we call thisÔºå you'll hear me refer to this as the downstream task a lot„ÄÇ

That is in reference to this the actual task that we care about and the language modeling task is called the pre training task„ÄÇ

So we back the error from a different taskÔºå the downstream task„ÄÇ



![](img/f1591e808e9bc70bdffc1770f073cdfc_20.png)

What just happened„ÄÇNoÔºå what did I do„ÄÇ„Éó„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_22.png)

OkayÔºå I don't know what I just did there into the„ÄÇPrameterters„ÄÇInto these emitters„ÄÇ

 and this whole process of starting with an existing neural language model and then kind of specializing it for the downstream task is called fine tuning„ÄÇ

SoÔºå two important„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_24.png)

ÂóØ„ÄÇTerms for you to be aware of are the downstream taskÔºå which is in this example„ÄÇ

 sentiment analysis and the process of fine tuning„ÄÇ

 which kind of takes the parametersmeter of the neural language model and specializes them to perform the task of fine tuning„ÄÇ



![](img/f1591e808e9bc70bdffc1770f073cdfc_26.png)

OkayÔºå so some questions„ÄÇ So basicallyÔºå we have changed the input of the supervised model where the word embeddings are the vectors„ÄÇ

ToÔºå I imagine that means to the hidden state„ÄÇ this is what's happening in the Elmo modelÔºå the input„ÄÇ

 the word embeddings that used to be either randomly initialized or coming from some process like Word deveec are now just the hidden states of a recurrent neural language model„ÄÇ

OkayÔºå so next questionÔºå what are some sources of oh sorry„ÄÇ

 how do we choose which hidden state value to be used„ÄÇ

 Is it the output of the pretrain model that we are using as the word embeddings for the supervised model„ÄÇ

 YesÔºå this is what happens in the Elmo model„ÄÇ in the Bt model„ÄÇ

 we're actually not doing this like just taking the hidden states„ÄÇ

 but we're rather we're basically taking all of the parameters of the neural language model„ÄÇ

 So it's not just the hidden statesÔºå but even the parameters associated with the composition process in the neural language model are being used in the paradigm of Bt So it's not just we're not taking just the hidden states„ÄÇ

 but rather starting from all of the parameters in that neural language model„ÄÇ

Okay a question on the sources of data for pretraining YeahÔºå so in general„ÄÇ

 as we'll see later on in this lectureÔºå the bigger the data set„ÄÇ

 the better gains that you see from this pretraining process so Wikipedia is very common but nowadays increasingly more people are using the commonCl„ÄÇ

 which is you can go to commonCl„ÄÇorg and check it out„ÄÇ

 it's just like a dump of the internet it has like trillion trillions of tokens of text and so really we haven't hit the limits of the amount of data that we can pretrain on and you can check out something like openaiIs GT3 for the current largest language model in existence„ÄÇ

OkayÔºå so there's a question about BertÔºå I will answer that later when we get to the Bt part of the slides„ÄÇ

 Do you fine tune the pretrained weight or just the softmax weightÔºå YeahÔºå this is a great question„ÄÇ

 So in the Elmo paradigmÔºå you only finet tune the weights that are associated with your downstream model and you don't fine tune any of the language model weights in the Bt paradigm we switched right because remember in the Bt method we are actually transferring the other parameters of this model„ÄÇ

 including these composition parametersÔºå not just the hidden statesÔºå and so in Bt„ÄÇ

 it is common to fine tune the weights of the model not just the weights associated with the softm layer And so I think that also answers other question„ÄÇ

 So the current state of the art is the pre prelearned or pretrained parameters are not fixed„ÄÇ

 they are also changed through backpro„ÄÇThe fine tuning phase„ÄÇ

 and you will get some experience doing this in homework one„ÄÇAll rightÔºå so at this pointÔºå wow„ÄÇ

 this has been going on for 52 minutes„ÄÇOkayÔºå wellÔºå let's get to these slides„ÄÇ

 and hopefully we won't take too much more time„ÄÇ

![](img/f1591e808e9bc70bdffc1770f073cdfc_28.png)

this„ÄÇI pad awayÔºå so„ÄÇHopefullyÔºå this„ÄÇThe slides are now showing„ÄÇÂóØ„ÄÇRightÔºå so what I've just described„ÄÇ

 hang onÔºå let me„ÄÇReduce the size of„ÄÇOkayÔºå so to this pointÔºå I've basically given you a very„ÄÇ

 very kind of simplified example of this how the history of transfer learning and NLP has progressed from 2013 to now and in this example we focused on this simplified sentiment analysis task where my downstream model is just a single layer recurrent neural network and my neural language model is also a recurrent neural network„ÄÇ

So now we're going to get into the actual details of the two papers that were kind of impactful in shaping the direction of this line of research and so the first one of these was this ElLmo paper which actually I was on the team that developed this so yeah was kind of awesome to be a part of that but the whole advantage of this work was to kind of integrate pre-trained neural language models into downstream tasks and really had not been at least shown to work very well before this paper„ÄÇ

So the idea here and one of the things that we haven't discussed is that if I'm using just a recurrent neural language model„ÄÇ

 my model is only aware of the tokens that occurred before the current token right and we went over this right we can't actually incorporate information about future tokens because our whole goal is to predict those tokens right this is the entire point of a language model„ÄÇ

And so to get around this because in most tasks like sentiment analysis„ÄÇ

 I actually want to have a complete encoding of the entire sentence right so this movie is great I want my token for this and movie and is to have some knowledge of the word great right and in a current in a normal recurrent neural language model„ÄÇ

 this isn't happening because I'm proceededing one token at a time„ÄÇ So in Elmo„ÄÇ

 we actually had two language models we had one that was trained left to right„ÄÇ

 which is the standard one you see an example here of open a bank So given the start of sequence token I'm predicting open given open I'm predicting this is just what we've seen before„ÄÇ

 but we also train a reverse language modelÔºå So in this case I'm starting from the last token of the sequence and I'm trying to predict the words backwards„ÄÇ

üòäÔºåessentially so given the word openÔºå I'm predicting the sorry this one is starting like over here„ÄÇ

 but in generalÔºå when I concatenate the hidden states of both of these left to right and right to left language models together„ÄÇ

 now the hidden state at every time step has some information about all of the other words in this model When we move to Bt we'll see how we can get away without using recurrent model change to the language modeling objective so that we don't have to do these two separate language models„ÄÇ

But againÔºå in ElLmoÔºå we take the hidden states from these models and we use them as replacements for word embedding„ÄÇ

 so we're applying them as essentially pre-trained embeddings and this block here„ÄÇ

 existing model architecture just represents all the parameters that we have to train from scratch using our downstream model„ÄÇ

Okay„ÄÇSo how does this ElLmo model actually workÔºå So basically we start„ÄÇ

 this is just the specific implementation of ElLmo„ÄÇOhÔºå I see„ÄÇ I have„ÄÇPop up on YouTube„ÄÇ That says„ÄÇ

The streaming quality is bad„ÄÇÂóØ„ÄÇOkayÔºå I'm just going to assume that will improve in the future„ÄÇOkay„ÄÇ

 soÔºå okayÔºå it it went away„ÄÇ I assume it's better„ÄÇ So let's say we have this context„ÄÇ

 download new games or play is like part of an existing sentence„ÄÇ

 So I have my word embeddings associated with these„ÄÇAnd„ÄÇ

I'm going to do a deep recurrent language model to„ÄÇSo this is actually streaming„ÄÇOkay„ÄÇ

 please do let me know in the chat box if the streaming quality has degraded to such a point where you can't understand me„ÄÇ

 I'm getting some messagesÔºå but I don't know if it's actually impacting the quality„ÄÇ

 So anyway in the Elmo modelÔºå we have a deep multilayed recurrent neural language model„ÄÇ

 And so what this means is instead of just having one hidden state per token„ÄÇ

 we actually have multiple hidden statesÔºå one associated with each layer of the model„ÄÇ

 We're also using a fancier composition functionÔºå not just the recurrent update„ÄÇ

 but also we're using something called an LSTMÔºå which allows us to at least theoretically take better advantage of the context„ÄÇ

 the longer context„ÄÇ So we talked about recurrent neural networks as having this issue of they have a recency biased towards the most local token„ÄÇ

They forget a lot of the distant onesÔºå the LSTM is a variant of the recurrent neural network that handles these long term dependencies better„ÄÇ

 but we didn't go into that in detail because a transformer is also a way of more effectively encoding the long term context and it's just a better model in terms of training efficiency and so on„ÄÇ

OkayÔºå so I saw a question sorry if I missed itÔºå why are we having models from both left to right and right to left in Elmo„ÄÇ

 so maybe this slide can make it more clear„ÄÇSorryÔºå so„ÄÇ

If you're thinking about the the way in which a recurrent neural network builds representations„ÄÇ

 if I have just a left to right recurrent neural network„ÄÇ

 then my hidden state for the word download doesn't include any of the other information for other words in this sentence„ÄÇ

 rightÔºå I only have seen downloadÔºå I haven't seen new games or play„ÄÇ

 But if you think about how you want to use this language model in the downstream task„ÄÇ right„ÄÇ

 I get a whole sentence as input this movie as great„ÄÇ

 And I'm no longer trying to predict the next word in sentiment analysisÔºå rightÔºå I'm just trying to„ÄÇ

 given the full sentence predict whether it's positive or negative„ÄÇSo in this case„ÄÇ

 there's no need for me to have this constraint that every word can only see the previous context and none of the future context and so this is why if I train just a left to right language model„ÄÇ

 each of these representations has seen only the words that happened that occurred in the past„ÄÇ

 but if I additionally train the reverse direction„ÄÇ

 then I have another model that is still in the language modeling setting„ÄÇ

 not cheating by seeing more words than it should but when I concatenate these two representations together„ÄÇ

 I now have a way of representing the fact that download occurs in this context of new games or play in addition to all of the words that occur in this sentence before the word download„ÄÇ

 So essentially both directions are needed to get in„ÄÇIn about the rest of the sentence„ÄÇ

 both the preceding words and the future words into the hidden state for the current time step„ÄÇ

This is more associated with how you want to use these models in the downstream task„ÄÇ

 you want to use themÔºå you get a complete sentences input„ÄÇ

 so there's no need for you to expect your your model to benefit when it's only seen like half the sentence or the prefix up till that point„ÄÇ

I hope that was clear„ÄÇ It'll become a little more clear when we talk about Bert„ÄÇ

So one question that someone asked before rightÔºå how do we choose which hidden state to use if we have multiple hidden states„ÄÇ

 we do this so in this case I have three different layers of this recurrent LO model and note that we've gone from this slide to the previous slide„ÄÇ

 these are now by LSTMs which means that we're incorporating both the left to right and right to left prediction task into this this model„ÄÇ

 so now these representations are kind of fully contextualized with both the past and the future context„ÄÇ

And so now we have these three vectors at every time step for one for each layer„ÄÇ

 and we're just going to take a weighted average of these vectors and the weights are learned through the downstream tasks so these weights are learned parameters that are back propagated into using our downstream task signal„ÄÇ

This is just one way to do this there are many other ways that have been proposed since to do this aggregation„ÄÇ

 you could do some sort of attentionÔºå you can think about it that way„ÄÇ

 you could just uniformly average and not learn any weights in our ElLMmo paper we showed that learning these weights is actually helpful and different tasks can learn different weights„ÄÇ

 but yeah all of this is like it's important to understand that general intuition„ÄÇ

 but don't worry if the exact details are kind of fuzzy we're going to spend much more time talking about the BRT model and it's offshoots which are more popular nowadays„ÄÇ

Okay so before we get into that does this mean we can't use the LLmo model for word prediction No so the ElLmo model at its core consists of two different language models that are like glued together so this right to left language model sorry this left to right language model and this right to left language model and so we can use the left to right language model for word prediction if we want if for some reason you want a model that can predict sentences backwards„ÄÇ

 you can also use the right to left language model but yeah you can use these individual components for language modeling„ÄÇ

 but the real point here is not language modeling it's to improve downstream task performance„ÄÇüòä„ÄÇ

So rightÔºå these ElLmo representationsÔºå unlike the standard word embeddings are now contextualized„ÄÇ

 they have information about the rest of the sentence in addition to just the current word and so you can think about this as instead of having just a single static word embedding associated with a particular word in my vocabulary I now have basically infinite different embeddings for a word because every words or representation depends on its context„ÄÇ

 so if I take the word play and I have10 different context in which the word play occurs using Elmo every single one of those the representation for play in those hundred context will be different from each other and that's kind of the goal of this contextualization process„ÄÇ

OkayÔºå and so do itÔºå this was kind of an impactful work because it demonstrated across„ÄÇ

A wide range of NLP tasks„ÄÇ So S is a question answering data„ÄÇ SST5 is a sentiment analysis data„ÄÇ

 we have co-reence resolutionÔºå named entity recognitionÔºå semantic role labeling„ÄÇ

 all of these different tasks just simply replacing the word embeddings in these tasks with the hidden states from this ElLmo model significantly increased the state of the art across the board„ÄÇ

 So this was kind of a positive result indicating that transfer learning in NLP has a lot of potential„ÄÇ

 and the subsequent work in B and its many offshoots have only since increased these gains„ÄÇüòä„ÄÇ

Okay so actually I think I'll probably stop here considering it's already been over an hour and that way our next lecture on Monday can focus exclusively on BRT so it'll be fairly coherent at this point if there are any questions any final questions about the content that we've covered today„ÄÇ

 so basically the whole idea of transferring word embeddings„ÄÇ

 transferring other parameters of our model using the hidden states of a language model as input to downstream model„ÄÇ

 hopefully the intuition behind a lot of this stuff is now clear„ÄÇüòäÔºåBut„ÄÇYeahÔºå do feel free to„ÄÇ

Type in any questions that you have next week we'll focus on BRT as well as how we actually apply these things to these specific tasks so S„ÄÇ

 SNLIÔºå SRL on the surface question answering and sentiment analysis are very different tasks from each other„ÄÇ

 but we show that well not we I mean the others of the BRT paper show that all of these can be solved within a single model„ÄÇ

OkayÔºå so I see one question hereÔºå having these infinite context for a single word embeing and ElLmo„ÄÇ

 isn't it making the model a little more complicated than neededÔºüÁöÑ„ÄÇ

I guess one fundamental property of Elmo is that you're not explicitly enumerating these infinite embeddings right so it's not like we have a lookup dictionary of here are all the different contexts in in the that the word play can occur in Let me have a separate embedding for each of these that I store in my memory and look up from the model is actually not very complicated right all it consists of are the parameters associated with the recurrent update equations for these language models„ÄÇ

And in your sentiment analysis exampleÔºå all I have to add on top of this is a single softmax layer on the final hidden state or the final aggregated hidden state to predict the label„ÄÇ

 So it's the models like yeahÔºå what might be complicated is the fact that we had to pretrain this language model in the forward and reverse directions on a lot of data before we can actually use it on the downstream task„ÄÇ

 but parameter wise it's not very complicated„ÄÇ And these infinite embeddings are that's just like like if you were to consider the different types of vectors that Elmo produces for a single word„ÄÇ

 they are theoretically infinite of them„ÄÇ but in practice„ÄÇ

 we're not explicitly enumerating all of obviously we can't do that right And so„ÄÇüòäÔºåBasically„ÄÇ

 we just compute these embeddings on the fly using our language model parameters for from the ElLMmo model„ÄÇ

ohÔºå I didn't see the continuation„ÄÇ There are possibilities of sentences where words can have the same usage or context„ÄÇ

 YeahÔºå but in in those those casesÔºå the Elmo would just compute those embeddings on the fly„ÄÇ

 you might consider ways of caching similar contextsÔºå but in general„ÄÇ

 that's probably not going to save you that much time„ÄÇSoÔºå yeah„ÄÇOkay„ÄÇ

 is there a reason why we don't fine tune the pretrain weights in Elmo„ÄÇ

 The main reason is because we at that point were kind of computationally constrained„ÄÇ

 So these days we have these huge GPUs„ÄÇ It's very easy to get access to GPUs with a lot of memory„ÄÇ

 but even in 2018Ôºå it wasÔºå you know„ÄÇKind of rare to have these huge scale language models and be able to fit them in memory and fitting the pre trained weights and memory as in in addition to the downstream tasks specific weights„ÄÇ

That really slows things down„ÄÇ So training becomes a lot faster when you think of that Elmo as just like a fixed feature extractor„ÄÇ

 and you're only learning a small number of parameters on top„ÄÇ

 This obviously went out the window with Bt where you're fine tuning a massive transformer language model„ÄÇ

 But a lot of this field has really benefited from advances in hardware„ÄÇ

 like better GPUus with more memory„ÄÇ So yeahÔºå I guess it's„ÄÇüòäÔºåIt's just how the„ÄÇ

 the hardware was at the time„ÄÇ And now things haveÔºå have really changed„ÄÇOkayÔºå great„ÄÇ

 So if you have other questionsÔºå you can ask them on Piazza or through the form„ÄÇ

 or you can just save them up for the start of next week next week's lecture„ÄÇüòä„ÄÇ



![](img/f1591e808e9bc70bdffc1770f073cdfc_30.png)