# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é©¬è¨è¯¸å¡CS685 ï½œ è‡ªç„¶è¯­è¨€å¤„ç†è¿›é˜¶(2020Â·å®Œæ•´ç‰ˆ) - P3ï¼šL2- ç¥ç»è¯­è¨€æ¨¡å‹ - ShowMeAI - BV1BL411t7RV

Okayy everyoneï¼Œ today we're going to be talking about neural language models so continuing the topic from the previous lecture of language modelingã€‚

 but now extending it to a more complex class of models that essentially converts the problem from counting and then normalizing as we saw with Ngram models to learning a model to predict the next word given a representation of the prefix and this is in many ways a more powerful formulationã€‚

 especially for the transfer learning paradigm that we've discussed in the first two lecturesã€‚ğŸ˜Šã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_1.png)

Okayï¼Œ so before we get into today's topicï¼Œ I wanted to go over some of the questions that came up from last timeã€‚

 so we had a question about the pass fail policy which for graduate students is technically satisfactory or unsatisfactory we are going to allow you to decide whether you want to switch from a letter grade to pass fail at any point between now until the final report deadlineã€‚

 which is December 4thï¼Œ so if you want to take this class pass failã€‚

 you should send an email to the instructor's account with your request and also make sure to send us a reminder email at the end of the semester oftentimes these emails might get lost if someone doesn't note it down then you want to make sure to reminded us of it because the end of semester it can get kind of hecticã€‚

Okayï¼Œ so other stuffï¼Œ your homework zeros do this Friday againã€‚

 if you haven't started it and you little iffy on your programming or math backgroundã€‚

 I would encourage you to start early and finally your final project groupsã€‚

 the identity of the members must be formed by this Fridayã€‚

 otherwise we'll do it for you so I guess by now you've seen the piazza post by the TA suggesting that these group assignments won't be done completely randomly if you haven't specified your group instead we've collected a bunch of information from you regarding your interestsã€‚

 your time zone etc and we'll do our best to make reasonable groups that are you know roughly within the same time zone and also sharing common research interests so it's not a huge deal if you're unable to find a group but of course if you want to pick your own group members make sure to submit the formã€‚

OrBy this Fridayã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_3.png)

Allrightï¼Œ so let's launch into today's topic so just as a quick refresher language models are basically trying to measure this joint probability of a given sequence of words so this could be a sentenceã€‚

 a phraseï¼Œ of large documentï¼Œ any piece of text and last time we looked at how we can factorize as joint probability as a product of multiple conditional probabilitybabilitiesã€‚

 so any model that's able to compute a conditional probability of a word given some number of words as a prefix is also able to compute this joint probability of a sequence via the chain rule which we went over last timeã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_5.png)

Okayï¼Œ and last time we looked at Ngram modelsï¼Œ which are again very simple methods of estimating these conditional probabilitiesã€‚

 So in essence you have a training data set of a bunch of text and for any given prefix that occurs in that training data you're going to count up first how many times that prefix occurs and then how many times a particular word occurs after that prefixã€‚

 so it's basically this count and the normalized style way of estimating these conditional probabilities and this is an example of max likelihood estimationã€‚

 So you're maximizing these probabilities on a training data setã€‚ğŸ˜Šï¼ŒOkayã€‚

 so before we get into neural language modelsï¼Œ I want to go over some of the issues with Ngri models that motivates more complex modelsã€‚

So the first issue is this problem of sparsity right so we looked at this briefly last timeã€‚

 but let's say this numerator hereï¼Œ the count of students open their W subJ is zero for a particular W subJ that means this whole probabilityã€‚

 this whole conditional probability is going to be zero and that also means that if we see at test time any documents or sentences that contain this phrase students open their W subJ the probability of that whole piece of text is going to be zero so this is obviously undesirable and we discussed a way of getting around this which is called smoothing so the intuition behind this is that for every count that I actually observe in my training data I'm going to steal some value from each of those counts and assign it to counts that I that are zero rightï¼Ÿ

ğŸ˜Šï¼ŒIf students open their W subJ is0 for a particular W subJã€‚

 I might add a small value from the observed counts to this unobserved count such that at test time the probability of some document containing this phrase students open their W subJ is no longer zeroã€‚

Okayï¼Œ there's a second problem right What if the entire prefix doesn't appear in my training data so students open there in that case the denominator is zero which is of course bad and so traditionally language modeling in Ngram models get around this using a method called back off so if students open there doesn't exist then I will back off to the bigram students opened if that still hasn't been observed in my data I'll back off to just students so at every point you're kind of getting a worse estimate right because you're ignoring a word because you had no data for it in your training data set but at the end of the day this slides you get around the problem of unobserved prefixes but both of these are kind of hacky methods right and we would love for a model to not have we would love to not have to apply these techniques tillã€‚

ğŸ˜Šï¼ŒLanguage model to get around this problem of zero counts in the numerator and denominatorã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_7.png)

Another issue is storage so remember when we were looking at that restaurant review data set we were just looking at a bigram tableã€‚

 but we had to form this table of all of the prefixes and then all of the words that follow that prefix and then we count it up you know how many times did this prefix this word occur together and then we normalized by the unigram probabilities of the prefix right that was for a bygram model but imagine now we're doing a seven gra model right so now the rows of this table all the prefixes I now have you know V to the n order rows right so I have a huge number of rows and every time I add one more word to my prefix so if I go from bigram model to a trigram model to a four gra model to five gra model the size of this table growsã€‚

ponentialallySo this really prevents me from estimating huge order ngram models even with large data sets because the I just will run out of memory and the sparsity problems that we discussed before with these zero counts become much greater in settings where you have larger n so if you remember the Shakespeare example that we went over in the last lecture with a four gra model like 99ã€‚

9% of the counts in the table were zero so you can imagine smoothing and back off et cea become much more important in these kinds of settingsã€‚

ğŸ˜Šï¼ŒSo we have basically two key issues so far right one is the sparsity issuesã€‚

 a lot of zero counts and the other is memory issues and a third issue which is kind of related to both of these things is that in engrad models all words and prefixes are treated independently right so let's look at our prefix students open there right the one we've been looking at to this point so I count up all occurrences of this particular prefix and the words that follow but this has not influenced my count of pupils open there which is a kind of paraphrase of this initial phrase scholars open there it's maybe not the exact same semantics but it's very close undergrads open there or students turn the pages of there or students attentively perus there right you can imagine you know they don't have to be identical semanticã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_9.png)

But all of these things are roughly talking about the same topics and the same actionã€‚

 and it really makes sense that if we have a modelã€‚

 it should be able to share information across these semantically similar or in some casesã€‚

 almost identical paraphrases of these prefixesã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_11.png)

So I guess at this point we'll go over two ways of representing words and first we'll start with the representation method used by engra models and many other bag of word style model so for exampleã€‚

 the naive base classifier which we're not covering in this class also relies on this assumption and commonly when you're setting up a logistic regression model for NLP you're going to also rely on this bag of words assumption and at the core it's very simple I'm going to represent every word as a vector of zeros with a single one that correspond to the index of that word so if I hadã€‚

 let's say this toy vocabulary of six words I hate love the movie film I might represent the word movie as00010 where this fifth dimension here correspondsã€‚

ğŸ˜Šï¼Œthe word movie and I might represent film as 00001 where this last dimension if it's one that means the word is film and so I might represent a phrase by adding these vectors together right so if I had the bigram movie film which of course doesn't make any senseã€‚

 but let's say I had it I would get a vector of 00011 So this is essentially the way in which we're treating words in an Ngram modelã€‚

 but importantly we're treating prefixes as kind of separate words in the vocabulary So students open there has a separate representation then pupils open there and scholars open there all of these are separate rows in my Ngramham table and none of these one hot representations are actually sharing informationã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_13.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_14.png)

Across each otherã€‚So one of the biggest problems of using a oneho representation is that all wordsã€‚

 phrasesï¼Œ eta are equally dissimilar so a movie and film right we know that they're synonymsã€‚

 but if you look at their two vectors their dot product is zeroã€‚

 their vectors are orthogonal they're no less similar to they're no less dissimilar to each other than they are to random other words like any of the other words in my vocabularyã€‚

 I hate love and all words are equally dissimilar in this vector space so I can't really share in any information on occurrences of movie and occurrences of filmã€‚

 they don't influence each otherã€‚ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_16.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_17.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_18.png)

So what we want is a representation space in which all of these you know semantically similar units of text also have similar representations So we're going to move from these one hot vectors to lowdial real valued vector so not just zeros and ones and this is where neural networks enter the picture So in a neural language model I'm going to take my prefix and then I'm going to encode it into these real valued low dimensional vectors and then I'm going to train a classifier to predict the next word from the representation that my model has learned of this prefix So in this setting I'm no longer just counting all occurrences of students open there right instead I'm actually learning a function to transform this prefix into a vector and then I'm learning a classifier to predictã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_20.png)

A word from this low dimensional representationï¼Œ so this is the shift from count based to predict prediction based models which really kind of started with at least in modern NLP with word devec which we're not covering here and now has kind of infused the rest of the research communityã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_22.png)

So at a high level in this lectureï¼Œ we're going to be focusing on the forward pass of these modelsã€‚

 which means we're just going to assume we have the parameters of a neural language model in existing language model and we're going to go through how we go from the raw text students open there to the prediction of the next word books in the next lecture we're actually going to talk about how we train these modelsã€‚

 so this is the backward pass how given a training data setã€‚

 we're going to estimate the parameters of our neural network model using the back propagation algorithm and our training objective is also going to use maximum likelihoodã€‚

 we're going to maximize the probability of our training data conditioned on the observed prefixes and the parameters of our modelã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_24.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_25.png)

Okayã€‚So let's start with wordsï¼Œ rightï¼Œ we talked about these oneho representationsã€‚

 but now we're no longer going to be using those because as we discussedï¼Œ they have many issuesã€‚

 so we're instead going to be representing words with low dimensional vectors called embeddingsã€‚

 And here I should clarify that we are representing word types as vectorã€‚

 So every word type in your vocabulary is associated with a separate vectorã€‚

 So here the word king might have you know this four dimensional representationã€‚

 This is just an example in practice the dimensionalities of these embeddings kind of ranges from I don't knowã€‚

 maybe 50 to 1000 or even more than that dimensionsï¼Œ but it's usually much smallerã€‚

 maybe an order magnitude or two smaller than the size of your vocabulary which is the dimension of one hot representationã€‚

ğŸ˜Šï¼ŒAnd these are some examples from the original word devek paperã€‚

 which essentially introduced this concept in a predict based learning framework where we see that words that are semantically similarã€‚

 So king and queen walking and upï¼Œ sorryï¼Œ man and womanã€‚

They are also close to each other in this vector space and they share certain directions in this vector space right so the relation of gender here is captured in in the vector space by the direction and distance of man and woman from each other there's also syntactic similarity that these words the word embeddings exhibits so walking and walked right they have the same stem if you remember the linguistic level of morphology that we talked about in the beginning the first video of this class here these are just two different tenses of the stem word walk and we see that the same relationship in this vector space holds for swimming and swimã€‚

ğŸ˜Šï¼ŒFinallyï¼Œ there'sï¼Œ you knowï¼Œ country and capital relations that you can see are kind of consistent for different countries and their capitalsã€‚

 So these are all just plots showing that if you're able to learnã€‚ğŸ˜Šï¼ŒPowerful word embeddingsã€‚

 then you can capture all sorts of semantic and syntactic similarities and stuff like world knowledge in the vector spaceã€‚

 and so this will enable us to share information between prefixes that contain different word typesã€‚

 but share some semantics or even syntax syntactic similaritiesã€‚ğŸ˜Šã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_27.png)

Okayï¼Œ so we're going to start with word embeddings right so for this prefix students open there I associate you know each of these words with their corresponding word embeddingã€‚

 but then we get to this process of composition right for these kinds of models I want a way of taking the sequence of three word embeddings and transforming them into a single vector that I can use for this classification problem of predicting the next wordã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_29.png)

So my goal is to predictï¼Œ say booksï¼Œ maybe books is the word that appeared in my training data setã€‚

 and I'm going to predict this from the composed representationã€‚

So there are a lot of new concepts here that I'm kind of going through pretty quicklyã€‚

 so I just want to break this down into smaller piecesã€‚

 so let's start with how this prediction process happens right so now I'm given this composed vector that represents this the meaning the semanticsã€‚

 the syntactic structure etc of this prefix students open there it's all been squash together into this lowdiional real valued vector by this neural network function So let's ignore that neural network component for the moment and just look at how we can go from one of these vectors to an actual prediction of a wordã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_31.png)

So this is a very important concept that you'll see over and over again throughout this class is that we're going to use a softmax layer to do this and at a high level what this does is it takes is input a vector and it outputs a probability distribution over the entire vocabulary or output space for a more general taskã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_33.png)

So let's look at that in a little more detail in this particular example if I have my low D representation of students open thereã€‚

 so let's say it's this like threedial vector hereã€‚

 what I want is a probability distribution over the entire vocabulary right so however many word types there are in my vocabulary and if this is a prefix that I'm observing a test time I of course want the probabilities to be pretty reasonable right so maybe a high probability to something like books and laptops and low probabilitybabilities to words that just don't make sense within this contextã€‚

ğŸ˜Šï¼ŒOkayï¼Œ and this is how we're going to be estimating these conditional probabilities in this neural language model frameworkã€‚

 so this conditional probability of any word W subi given the vector for students open there is going to be estimated through this learned neural language model so we're no longer just counting and normalizing right we're actually learning a model that predicts this distributionã€‚

ğŸ˜Šï¼ŒOkay so let's go into a little more detailï¼Œ let's say that our output vocabulary in this example consists of just four wordsã€‚

 booksï¼Œ housesï¼Œ lamps and stampsï¼Œ so of course books is like the only plausible continuation of this prefix I guess maybe houses is plausible in some situations but anyway so this is just a simplified example so we're going to start with our low-diional representation of students open there and what we want is to get a probability distribution over the four words in our output vocabulary so note that to be a valid well-form probability distribution all the elements of this vector need the sum to one and they all need to be positive so between zero and oneã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_35.png)

So every dimensionï¼Œ each dimension of this vector is associated with a particular word in my output vocabularyã€‚

 so you can interpret this as a model that producess distribution is giving a 60% probability to books being the next wordã€‚

 a 20% probability of houses and so onã€‚ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_37.png)

Okay so let's get into an actual specific example so instead of just showing you this picture of a vectorã€‚

 let's use this actual example of a three dimensional real value vector negative 2ã€‚3ã€‚9 and 5ã€‚

4 This is going to be the sample representation of my prefix students open there and we're going to call it x in our notation so this is a vector okay and let's say we also have this weight matrix W so we'll get into that in a bit but if you recall our output vocabulary has four words and this is a  three dimensional vector so we need some way of getting from a three dimensional vector to a 4 dimensional vector which is a probability distribution like we saw in the previous slideã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_39.png)

So that's where this weight matrix comes inï¼Œ it's essentially used to project this three dimensional vector into a four dimensional space and then we're going to use another function to convert the resulting four dimensional vector into a probability distribution So how do we do this projectionã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_41.png)

å—¯ã€‚Wellï¼Œ okay sorry first let us look at the specifics of what each dimension in this vector corresponds to and also we'll look at what the rows of this weight matrix intuitively mean so you can kind of interpret each dimension of this vector x as a different feature of the prefixã€‚

 remember students open there in our exampleï¼Œ this is not exactly analogous to if you're familiar with feature based like logistic regression or log linear models where you manually define features ahead of time and then assign them to specific training or test examplesã€‚

 this is not exactly the same because in this scenario the features are actually learned so these featuresã€‚

 these the values in this vector are if you remember the output ofã€‚ğŸ˜Šï¼Œneural composition functionã€‚

 but you can intuitively think of them as different encoding different like linguistic properties of the prefix and similarly oops the weight matrix if you look at that more closely you'll see that it's a 4 by3 matrix right four rows three columns and each row corresponds to the weights of each of these features for a particular word in the output vocabulary so this row corresponds to booksã€‚

 this row corresponds to housesï¼Œ this one to lamps and this one to stamps and you can kind of interpret I guess I can't highlight just a single cell here but like this 1ã€‚

2 this 0ã€‚28ã€‚94ã€‚5 these are the weights of the first dimension of the prefix representsã€‚ğŸ˜Šã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_43.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_44.png)

For each of the words in the output vocabulary so for exampleï¼Œ if this was negative 2ã€‚3ã€‚

 it looks like you know for lampsï¼Œ this is assigning kind of high importance to this this feature versus something like houses where this value is fairly lowã€‚

ğŸ˜Šï¼ŒNow you can't necessarily interpret these things in this wayï¼Œ I'm just giving you some intuitionã€‚

 but certainly the rows here correspond to specific words in your output vocabularyã€‚

 so that'll be a little more clear when we go over the actual computationã€‚

 but at a high level the dimensions of the prefix or representation correspond to features and the rows of the weight matrix correspond to feature weights for a particular word in the output vocabularyã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_46.png)

Oh let me move this thing down here right so just another cautionã€‚

 these intuitions aren't we can't easily interpret these features in the way that I've described hereã€‚

 like we can't say that you know the second dimension of this x vector corresponds to a particular linguistic property we'll get into this more when we talk about linguistic probe tasks which are specifically designed to kind of investigate what linguistic properties are encoded by these these vectorsã€‚

 but I think it's a reasonable intuition for people just getting startedã€‚ğŸ˜Šã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_48.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_49.png)

Okayï¼Œ so we talked about how we want to get from this three dimensional vector to an eventual four dimensional vectorã€‚

 which is going to form our probability distribution of the next wordã€‚So Iã€‚

 if I take this matrix vector productï¼Œ Wx get a four dimensional vector right because I've multiplied this4 by three weight matrix with this three dimensional vectorã€‚

 so the resulting matrix vector product is four dimensionsã€‚

 but let's look more closely at what this actually means to kind of connect with the intuition from the previous slideã€‚

 I know that was probably pretty confusingã€‚ğŸ˜Šï¼ŒBut let's look at how we got this 1ã€‚8 numberã€‚

So I'm multiplying each value in the first row with the corresponding dimension in my pre prefix representation right so I get 1ã€‚

2 times negative 2ã€‚3 this is the feature weight for the first dimension of the prefix representation similarly I add that to minus 0ã€‚

3 times 0ã€‚9 and 0ã€‚9 times 5ã€‚4 and so this is essentially just the dot product of the first row in this weight matrix with the prefix representation and that's how I get the first dimension of this matrix vector product so similarly minus 11ã€‚

9 results from doing the same thing with the second row of this weight matrix and so on so this hopefully clarifies why I referred to each row here as associated with a particular wordã€‚

ğŸ˜Šï¼ŒThe output vocabulary right you can see that we're using the first row here only to compute the value of the first dimension in this matrix vector product and if you remember the first dimension here is going to eventually be associated with the probability of the word booksã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so if you look at this matrix vector product hereã€‚

 you will notice that it is not a well formed valid probability distribution rightã€‚

 the numbers here do not add up to oneï¼Œ they have negative numbers and so I need to still do something to get to a probability distributionã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_51.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_52.png)

Okayï¼Œ and this is where the softmax function comes into playã€‚

 So this is a very important function that you should make sure to understand thoroughly because we'll see it over and over again through the rest of the semester it occurs and things like attention mechanisms and other things as well that will' be coveringã€‚

 So if you look at this function it takes as input a vector X and all it does is exponentiate in the numeratorã€‚

 So this is going to make all of the values positive and then it's going to normalize by the sum of this vectorã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_54.png)

The expiated vectorã€‚So that normalization step makes the sum of this vector of all the elements in the vector equal to 1ã€‚

 so this function accomplishes both of our goals right we get a vector where all the elements are positive and also all the elements sum to1ã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so in this example I kind of chopped a little bit off these numbers to make them more reasonable in the previous case hereã€‚

 we had negative 11ã€‚9 and 12ã€‚9 you wouldn't see these numbers in a normal neural language model they're fairly large and these extreme values dominate the softmax computation so I made a little more reasonable just for sake of example hereã€‚

 so if you see if you see the matrix vector product is equal to this vector applying the softmax function to this matrix vector product gives us the following vector where the first dimension is 0ã€‚

24 so 24% probability for the word booksï¼Œ 0ã€‚0006 probability for the next word whatever that was houses 0ã€‚

73 probability to stamps and 0ã€‚02 to lamps so obviously this is not a great predictionã€‚ğŸ˜Šã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_56.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_57.png)

Right it's predicting 73% probability of observing lamps given students open thereã€‚

 but this is just an exampleï¼Œ so in the next video we're going to talk about how if we have a model that's predicting something wrong like thisã€‚

 we can adjust its parametersã€‚ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_59.png)

And the parameters of this model that we've seen so far are these weights in this weight matrix hereã€‚

 how can we adjust those parameters to make the resulting prediction put more probability mass on the probability for books and less on these other wordsï¼Ÿ

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_61.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_62.png)

All rightï¼Œ so this was the the first step of theã€‚Sorryã€‚

 we were working our way backwards so this is actually the last step of neural language modeling how we go from a vectorized representation of our prefix to a prediction of the conditional probability distribution of the next word so just to sum up if we're given such a representation of a prefix we'll call it X we do the following two steps to get converted to this probability distribution we first projected to a v dimensional vector where V is the number of word types in our vocabulary using a matrix vector product as we saw on the previous slidesã€‚

 so you might see this called as a linear layerï¼Œ this is a pytorch terminology or feed forward layer fully connected layerã€‚

 there are many different names for this projection stepã€‚

 but here in all cases we're just doing a matrix vector productã€‚ğŸ˜Šã€‚

And the goal is to transform this representation that we have as input to a desired output spaceã€‚

 so in this case we want a V dimensional output spaceã€‚ğŸ˜Šã€‚

And the second step once we get this vector is to simply just apply the Somax function to get it into a probability distributionã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_64.png)

Okayï¼Œ so now that we've covered how we actually get a predicted probability distribution given a representation of the prefix now let's turn our focus to how do we compute this single representation of the prefix in the first place so as mentioned beforeã€‚

 we're going to have a neural network that acts as a composition function so it takes as input a sequence of word embeddings so here the embedding for students followed by the embedding for open followed by the embedding for there and squashes it down into a single vectorã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_66.png)

So there are many different ways you can implement this in neural network in your homework we look at element wise additionã€‚

 which means given these three vector representationsã€‚

 I'm just going to add them up element wise so I'm going to add the first dimension of each of these vectors to get this the first dimension of my composed representationã€‚

 then add up the second dimension of each of the three vectors is to get a second dimension and so onã€‚

 so this is one of the simplest composition functions element wise additionã€‚

 you can also imagine you know things like multiplication or max or something like that average is also a popular choiceã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_68.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_69.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_70.png)

But yeahï¼Œ this is just one way of composing these functions right so the problem with doing something like elementwise addition is that we ignore the order of the words in the sequence right so my representation for this prefix students open there is going to be the same as the representation for open their students or their students opened or any permutation of these words so it doesn't take into account the order of the words in the prefix which we obviously know is critical right it conveys a lot of semantic and syntactic meaning and so we want to be able to model the left or right order of these wordsã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_72.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_73.png)

So how can we get around that problem while there are many different ways we can do thisã€‚

 a simple approach is concatenationï¼Œ so we look at this in more depth in a bit but essentially I can just concatenate these three embeddings together and thatll give me if you assume each of these is a three-dimensional vector my composed representation will be nine-dimenional so this has some issues that we'll get into later and those issues kind of motivate the next class of function which are more complicated so we have feed for neural networksã€‚

 convolutional neural networks or recurrent neural networks and finally transformers the latter class of model the transformer has become the most popular neural network architecture and NLP and so our focus is going to be on transformer based architectures for most of this semesterã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_75.png)

![](img/d8e572731c1625e18efe04a4c53e19eb_76.png)

In this lecture we'll go over concatenation combined with feed forward neural networks and we'll also look into recurrent neural networks next week after we've covered back propagationã€‚

 we will start looking at the transformer attention mechanisms and then finally get to the overall transfer learning paradigm and specific implementations of that that have been proposed over the past couple of yearsã€‚

ğŸ˜Šï¼ŒThat was random roadmap in the middle of the videoã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_78.png)

Okayï¼Œ so let's look first at concatednationï¼Œ so we know that concatenation at least preserves the order of the words in the prefix right in contrast to element wise addition or mean pullingingã€‚

ğŸ˜Šï¼ŒSo let's say we have our prefix right the students open there will actually treat this in a similar way to the Ngram models in that we're going to consider a fixed window for our prefix so if we had the students open there occurring in the context of the larger piece of text as the proctor started the clock the students open their blank we're going to just discard parts of that that are outside our fixed window so in this case our windows just four words later we'll look at models that don't have this limitation and can theoretically handle unlimited context although practically that's not trueã€‚

ğŸ˜Šï¼ŒOkay and we'll call this a fixed window neural language modelã€‚

 this model was proposed initially by Yashu A Benggio in 2003ã€‚

 it's one of the readings that are on the website for this week's lectures so you should definitely check that outã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so how does it work We'll start with our word embeddings for each of the four words in the prefix right so in this visualization each word embedding is four dimensions so each of these four circles corresponds to one word embedding and the first step is just concatenate them together so if each word embedding is four dimensionsã€‚

 our concatenated representation our x vector here is 16 dimensionsã€‚ğŸ˜Šã€‚

Now an important concept in any form of deep neural networks is to add a nonlinear function into the mix and so we're going to call this a hidden layer in our terminologyã€‚

 but essentially it's a function that takes as input this concatenated representation and produces a new vector as output and we've applied a nonlinearity in the form of this function F as well as a projection matrix a linear layer you can see here we have our matrix vector product like we saw beforeã€‚

 so if you look at the dimensionality of this new vector hereï¼Œ it's actually 12 dimensionsã€‚

 not 16 dimensionï¼Œ so we've projected it down to a lower dimensional space than the initial concateninated representationã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_80.png)

So you might ask what is the purpose of thislineity F right and what is the purpose of this whole hidden layerã€‚

 Why can't I just directly apply my softmax layer to this concatenated representation there's nothing preventing you from just discarding this hidden layer and immediately proceeding with a softmax layerã€‚

 but the point of adding these nonlinearities is to be able to model nonlinear relationships between the input and the output So this is kind of hard to think about in the context of language modelingã€‚

 but a classic problem that kind of motivates the inclusion of these nonlinearities is the X or problem and so you can read more about this maybe I'll put a link to it on the scheduleã€‚

 but essentially these nonlinear functions allow for much greater representational capacity and modeling of much more complex relationships than fully linearã€‚

ğŸ˜Šï¼ŒSo the state of the art in these kinds of models actually contains stacked layers of these nonlinear projectionsã€‚

 So each one of these layers is just increasing the complexity and the types of relationships that the model can handle And so what do I mean by a nonlinear functionã€‚

 So if you look today at the most popular choices probably the most commonly used one is the rectified linear unit or Rã€‚

 So this one is very straightforwardã€‚ the function is just L of x where x is a vector is the max of0 and xã€‚

 and this is applied element wiseã€‚ So basically any element of a vector that is negative gets converted to0ã€‚

 otherwise it remains the sameã€‚ It's identity functionã€‚ other choices were popular before thisã€‚

 such as a10 h and sigmoidï¼Œ they have issues with gradient saturation thatã€‚ğŸ˜Šã€‚

Well make more sense after we get into the next lectureã€‚

 but yeah there is you know a lot of research on different types of nonlinear activation functions you might hear these referred to as activation functions when you're doing your readings as well but at the end of the day there is a couple that are kind of standard that everyone uses without the thinking too much about it and yeah the value is one of thoseã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so we took our concatenated representationã€‚

 we projected it into this hidden space using a linear layer followed by a linearityã€‚ğŸ˜Šã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_82.png)

The final step is what we just saw before we apply a softmax layer right so it's a softmax function followed by this matrix vector product with the vector is where the vector H here is the output of the hidden layer computationã€‚

So we note that we have two different weight matrices here right W sub1 is doing this projection from 16 dimensional vector to a 12 dimensional vector and so its dimensionality is 16 by 12 and or 12 by 16 depending on how you set up the computationã€‚

ğŸ˜Šï¼ŒYeahï¼Œ in this caseï¼Œ 12 by 16 and the dimensionality of this W sub2 matrix is going to be the size of the vocabulary by 12 right so when I when I multiply this matrix by the 12 dimensional vector H I'm going to get a V dimensional vector which is then projected or converted to a probability distribution via the softax functionã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so again in this lecture we are not going to look at how exactly we train the parametersã€‚

 W sub2ï¼Œ W sub1 and the word embedding C sub1 through C sub4ã€‚

 that's what we'll look at in the next lecture but note that all of these the parameters of the model are these weight matrices as well as the word embeddingsã€‚

 all of them are learned through this process of predicting the next word on a given training dataset setã€‚

ğŸ˜Šï¼ŒThat will hopefully make more sense after you watch the next videoã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_84.png)

Okayï¼Œ so let's step back and do some comparisons between this model to our NG model from the previous lectureã€‚

So first of all we don't no longer have the sparsity problem right we're not explicitly encoding this count table anymore right and we're not enumerating all possible prefixes that could be formed in the vocabulary The model size is no longer exponential in the order of the model right the window size but the model still has problems right it hasn't really addressed this fixed window thing right it's still limited to in this caseã€‚

 the four words that immediately follow the current word and scaling this model to larger windows is kind of problematic because it grows this concatenated representation right so if I wanted to include the last hundred words I'd have to concatenate 1 hundred of these embeddings together then that makes this weight matrix get a lot bigger and importantly I'm not sharing parameters betweenã€‚

ğŸ˜Šï¼Œaccross one word and another word right all of these get different weights in this weight matrix and so this can be a challenging optimization problem right if I see the word students and position 83 versus 92 do I really want to use a separate weight matrix or separate weights to handle this or do I want to share weights so the next model will look at the recurrent neural network addresses this problem and the transformer also has its own way of addressing this problem but the recurrent neural network is conceptually much simpler to explain which is why we're going to go into it briefly hereã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_86.png)

So in this modelï¼Œ we no longer have this condition of including a fixed windowã€‚

 but just for the sake of demonstrationï¼Œ we're going to just look at the same forward prefix that we've been looking at and to this point the students open thereã€‚

ğŸ˜Šï¼ŒSo just like beforeï¼Œ the first step is to get the embeddings for each of these wordsï¼Œ so againã€‚

 we have these four dimensional embeddingsï¼Œ but we're not concatenating them together anymoreã€‚ğŸ˜Šã€‚

The recurrent neural network is interesting because it proceeds in a series of steps so it processes a single wordã€‚

 the first word firstï¼Œ then it incorporates that representation with the next word so that's step twoã€‚

 then it incorporates that representation of the students with the word opened and finally the word there so in this sequential processing it kind of operates the same way that humans do when they're reading right we read left to rightã€‚

 the recurrent neural network also reads or can creates these representations left to rightã€‚ğŸ˜Šã€‚

So let's take a look at how that worksã€‚We're going to have this same concept of a hidden state except this equation looks different than it did for the fixed window language modelã€‚

 so you'll immediately notice that there are actually two weight matrices here instead of just one and we'll look into the actual roles of those in a couple of steps so we're going to first start with an initial hidden state so this is what we have before we've seen any words it's just like our base representation for any sequenceã€‚

ğŸ˜Šï¼ŒNow we're going to integrate this representation with the first word of our prefix theã€‚

So this diagram shows that how we're actually applying the two weight matricesã€‚

 W sub H and W sub E to do this integration stepï¼Œ so W sub H is responsible for projecting the previous hidden state into a new space and W sub E is responsible for modeling the word embedding of the current time stepã€‚

 so we're going to use this terminology of time steps because this model proceeds sequentially right so in the first time step we are integrating the word the the very first word with the previous hidden stateã€‚

ğŸ˜Šï¼ŒSo if you look at this equationï¼Œ W sub Hï¼Œ the matrix vector product is between W sub H and the previous hidden stateã€‚

 which for this time step1 is H sub0ï¼Œ the base representationã€‚

 and we're adding that to the matrix vector product of W sub E which is this other weight matrix multiplied by the word embedding at that time stepã€‚

 so the word embedding for theã€‚Then we apply our non linearity right this Fï¼Œ which could be the Rã€‚

 the T0 h or whateverï¼Œ and then we get H sub Tï¼Œ so in this case H sub1 our new hidden state that has incorporated one word of this contextã€‚

ğŸ˜Šï¼ŒNow that we have this representationï¼Œ we're going to integrate the next word in the sequenceã€‚

 studentsï¼Œ so at time step twoã€‚We're going to do the exact same computation except the vectors that we're using change Note that the weight matrices stay exactly the sameã€‚

 We're using W sub H again and W sub E but the matrix vector product this time is W H times H sub1 right not H0 and the matrix vector product for this term is W sub E times C sub2ã€‚

 the word embedding of studentsï¼Œ which is the word associated with the current time stepã€‚

 the second time stepã€‚ğŸ˜Šï¼ŒSo you can see here that we're actually sharing the weight parameters across different words which we were not doing in the fixed window case where we had separate weights for every single word in the fixed window here we're using the exact same two matrices for every single time step of this prefix and so our model size is not dependent on the size of the input prefix you can also see how this formulation allows you to model an arbitrarily long prefix right because I'm just proceeding one word at a time I'm using the exact same weight matrices so those are not a function of the time step and so I can just keep applying the same equation as I get one more word in the prefix so I can model has long prefix as I wantã€‚

ğŸ˜Šï¼ŒHereI'm going to integrate the third word opened and finally the fourth word there and now this representation H sub4 has information about all four words that occurred in the prefix rightã€‚

 this H sub4 is a function of every single word embedding in the prefix as we saw with this recursive equationã€‚

 rightã€‚ğŸ˜Šï¼ŒAnd so H sub4 we're going to treat as our representation of this full prefixã€‚

 so how do we get from H sub4 to a distribution over our vocabulary well we already have seen this now multiple timesã€‚

ğŸ˜Šï¼ŒRightï¼Œ I'm going to simply just use a softmax layer with a separate weight matrix at W sub2 to project this H sub4 into the vocabulary spaceã€‚

Okayï¼Œ so one thing that you might want to alwaysï¼Œ you knowï¼Œ keep in mind is what likeã€‚

 what are the dimensionalities of the different parameters that we're working with hereã€‚

 So we know our word embeddings are four dimensions eachã€‚

And what about these parameters W sub H and W sub E so in this diagram the hidden units are also four dimensions right so W sub H is is just a four by4 matrix right it's not changing the dimensionality of the vector that is getting multiplied it's making it another four dimensional vector similarly W sub E is also a four by four vector right the embedding dimensionality and the hidden dimensionality are the same this doesn't have to be the case it's just the case in this example in practice these numbers do vary and they're treated as hyperparameter so things that you have to decide beforehand before training the model things that you might want to optimize by looking at the n performance of your language modelã€‚

ğŸ˜Šï¼ŒOkayï¼Œ and what about W sub2 well the vector that this matrix vector product includes is H sub4 which is a four dimensional vector and so we want to project this four dimensionmensal vector into a size v vector right where v is the number of word types in our vocabulary so W sub2 in this equation is going to be v by4 so when you multiply this v by4 matrix with this four dimensionmensal vector you get this v dimensional vector which when we then apply the softmax to it gives us this probability distributionã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so in this lectureï¼Œ we've covered the forward pass for feed forwardward neural language model and a recurrent neural language modelã€‚

å—¯ã€‚I I realize that this is a lot of information that's been compressed into one video I'd highly encourage you to check out some of the readings that have been posted on the website if you're confused or come to office hours myself and the Ts all of us are very experienced with these kinds of models and can answer or your questionsã€‚

 hopefully well enough that you understand whatever confusions you have and againã€‚

 next time we're going be focused on the actual training process of these models so before we conclude let's just take a look at what are the advantages of this recurrent setup over the fixed window feed forward language model that we talked about previously so we've already gone through a bunch of these right we can process any input any length input right because we're operating left to right in this you know one word at a time kind of frameworkã€‚

ğŸ˜Šã€‚

![](img/d8e572731c1625e18efe04a4c53e19eb_88.png)

So we don't have to make any preconceived choices as to what the the length of the prefix that we can handle is our model size is notã€‚

It does not depend on the length of the prefix right because we're sharing parameters across the different words of the prefix and rightã€‚

 so these are the main advantages compared to the model the fixed window language model we looked at beforeã€‚

ğŸ˜Šï¼ŒSome disadvantages is that this recurrence is slowï¼Œ especially during training because wellã€‚

 this will be more clear when we talk about back propagationã€‚

 but if you only have an error signal at the output here when you're predicting the next wordã€‚

 the error that you make right if the model was assigning a low probability to books and it should have been highã€‚

 that needs to be propagated through all of the time steps that we observed in the sequence and if the sequences alongã€‚

 this computation might be slowã€‚ğŸ˜Šï¼ŒAdditionallyï¼Œ like we've talked at length about the theoretical ability of this model to handle long prefixesã€‚

 but in practice it really only encodes information about the last like a small number of words that occurred immediately previous to the word you're trying to predict so recurrent neural networks or have a hard time at least in the language modeling case looking beyond say 20 to 30 words and you really need to make them deeper train them on a huge amount of data for for them to be able to actually make use of the extended context but there are a number of optimization challenges with recurrent neural networks which have kind of shifted researchers over to transformers which at least during training areã€‚

ğŸ˜Šï¼ŒMuch faster to trainï¼Œ more parallelizable and easier to scaleã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_90.png)

Okayï¼Œ so finally we're as mentioned beforeï¼Œ going to be looking at back propagation next timeã€‚

 so how we actually train these networksï¼Œ how we get the parametersã€‚

 the weight matrices and the word embeddingsï¼Œ and then you can look out for the transformer architecture coming next weekã€‚



![](img/d8e572731c1625e18efe04a4c53e19eb_92.png)

Okayã€‚