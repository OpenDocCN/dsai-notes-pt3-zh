# P24ï¼šL19- æŽ¢æµ‹ä»»åŠ¡ - ShowMeAI - BV1BL411t7RV

Allrightï¼Œ looks like we're live hey everyoneï¼Œ so today we're going to be covering linguistic probe tasks which is a way in which you can interpret the different types of knowledge that your pre-train Bt or GPT or whatever has encoded in its hidden states so up till this point in the semester we've been mainly focusing on our pretrain models downstream performance when we use them for transfer learning right so you know looking at if I switch from BerRT to Roberta how much does my performance on some sentiment benchmark go up or some entailment benchmark or squad or something like thatã€‚

 but we might be interested in knowing more generally if our models are capturing various types of linguistic phenomenaã€‚

So that's where these probe tasks come in before we get to thatï¼Œ sorryã€‚Some stuff from last timeã€‚

 so just a reminder that the only thing left for you all to submit in this class is your final project reportã€‚

I hope that most of you have at least started working on your projectsï¼Œ but if notã€‚

 you should get it together because you have less than a month to submit this report and it's a big chunk of your gradeã€‚

ðŸ˜Šï¼ŒSo some people asked about pass fail requestï¼Œ we're just going to make December 4thã€‚

 also the deadline for pass fail request so if you want to be switched to pass failã€‚

 then send us an email on the instructor's account by December 4thã€‚

So also next week is the last week of classes and next Wednesday we're going to be having like a kind of guest lecture where PhD student Lorraine in Andrew Mcalum's lab is going to be talking aboutã€‚

 I believeï¼Œ common sense reasoning we haven't yet worked out how this is going to work on the live streamã€‚

 it might be thatã€‚Both her and I are on a call together and we stream that and I can be asking her questions and of course she can also take questions from the chatã€‚

 but yeah it's an exciting topic and there's a lot of new work going on in common sense reasoning so it should be funã€‚

Okayï¼Œ greatï¼Œ so let's move on rightã€‚

![](img/95c170530fd920441397f626e5fd3dbd_1.png)

It' a nice picture here we're interested in figuring out what kinds of knowledge has been encoded by our BRT models right so you can think of this as Btologyã€‚

 there's a paper and library called Btology that tries to help you understand what's going onã€‚ðŸ˜Šã€‚



![](img/95c170530fd920441397f626e5fd3dbd_3.png)

And so what we when we talk about like examining how BRT is workingã€‚

 we have to look at the different types of representations that we can get out of Bt right so we have of course a sequence of token level hidden states at every layer of the model so we can investigate what each one of these layers is learning that is maybe unique to that layer and not encode it as well by other layers we could also examine the attentionã€‚

 remember in a transformer we have multiple attention heads at each layer so maybe we can try and interpret what each one of those is doing and so on so there's many many different angles in which we can we can try and interpret BRTã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_5.png)

So as I was mentioningï¼Œ there's this Btology toolkit from Huging Fã€‚

 which is a popular library which we used for homework Oneã€‚

 it lets you access all of the things that you might want to analyze about BRT so this could be the hidden statesã€‚

 the attention weights and scoresï¼Œ and even the gradients with respect to different representations in the networkã€‚



![](img/95c170530fd920441397f626e5fd3dbd_7.png)

It's kind of hard to know what to do with all of those values thoughã€‚

 right Bert is a huge model if you really extract all of the hidden statesã€‚

 how are you going to proceed and analyze them right so that that's still I'm clear especially since these things are you know hundreds or thousands of dimensions each hidden state if you have you know a sequence of 512 tokensã€‚

 you're going to have 512 I would say00 dimensional vectors at every layer of the model it's just too much informationã€‚

ðŸ˜Šï¼ŒSo there's been some work on trying to prune parts of BRT at test time so you can get rid of components that are not necessary this paper are 16 heads really better than one at test time removes a large percentage of the attention heads so you can really kind of prune down your network to just the attention heads that are contributing to your performance on some downstream task I think this paper looks at machine translationã€‚



![](img/95c170530fd920441397f626e5fd3dbd_9.png)

Okay and before we get into the actual probes and visualizationsã€‚

 another tool similar to the Brtology thing is Alan NLP's interpret so this lets you look at the importance of various words in a particular example so here you see this mask language modeling inputã€‚

 the mask rush to the emergency room to see her patient and of course Bert here is asked to predict what goes into the mask token you can see it's predictions over hereã€‚

 nurseï¼Œ womanï¼Œ doctorï¼Œ etcï¼Œ but the gradient of the loss with respect to each word representation or each subword representation here can be informative when you're trying to figure out what is the most important context token for making this prediction of the mask token so here I thinkã€‚

ðŸ˜Šï¼ŒIn their simple gradient visualizationï¼Œ they just take the norm of the gradient with respect to each of the input tokensã€‚

 token embeddingsï¼Œ and you can see that the words with highest norm are emergency and herã€‚

 which makes senseã€‚Okayï¼Œ I see there's a question if we know that we can prune Bert at test timeã€‚

 what's the justification for having the heads there at training time Wellã€‚

 you can't actually train Bert to achieve the same performance with just like one head at each layerã€‚

 so the redundancy seems to be important a training timeã€‚

 but at test time you can get rid of a lot of those heads and this could be related somewhat to the stuff we talked about regarding the lottery ticket hypothesis where maybe if you have if you start training time with just you know like one head in each layer you really need like a good initialization to reach the same performance that you could get after youã€‚

 for exampleï¼Œ train model that has 12 heads per layer and then prune it down at test timeã€‚

 maybe there's you know a higher chance of you getting one head withã€‚ðŸ˜Šã€‚

with a good initialization or something like thatï¼Œ but yeahã€‚

 you need at least at this point to still train with all of the components there and then remove them at test timeã€‚

 which gives you speed ups without you significantly influencing your performanceã€‚Allrightã€‚

 so let's dig a little deeperï¼Œ there are two methods in which you can try and understand what kinds of knowledge that your models are encoding so we'll briefly talk about the first one visualization because it's actually really hard to use this in a meaningful way so probably the most prominent example of just using simple visualizations to uncover what your model is doing is this sentiment neuron work by open AI so they basically just trained a recurrent language model on what was a lot of data at that time and I guess a fairly large model size and then they manage to find just a single neuron whose weight is predictive of the sentiment of an input review So this is like kind of the ideal scenario if you're trying to interpretã€‚

Your neural networksï¼Œ you find there's just a single neuron that is that correlates reallyã€‚

 really well with some property that you care aboutã€‚ so like in this caseï¼Œ sentiment in practiceã€‚

 howeverï¼Œ this is a kind of rare result it's hard to replicate it for other parameters because there is nothing in the training process of Bt that's saying that a single neuron in isolation should be meaningful in some way or should be correlated to some property right it's way more likely that groups of neurons maybe even across different layers all contribute together to encoding some particular property and that can't be picked up by just looking at the values of each neuron in isolation and trying to draw some correlations there yeahã€‚

 this kind of visualization is not really used currently to interpret your networks just becauseã€‚

Moels are deeperï¼Œ they have way more parametersï¼Œ and it's really unlikely that you can draw interesting correlations to just a single neuronã€‚

Okayï¼Œ so like maybe expecting something like this with every model you have is unreasonable but what if you actually visualize different groups of neurons so this is a screenshot from LSTM Viz which is an interface built to visualize recurrent models and it has a lot of nice features that let you like click on different words in your input and kind of filter out neurons whose activations are below some threshold but you know just looking at this you can see that it's very hard to make sense of anything or draw any meaningful patterns from the neurons activations and you know with longer input sequences and more parametersã€‚

 more layers doing visualizations like this is just not feasible so this was from 2017 when we still had fairlyã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_11.png)

reasonably sized modelsï¼Œ but now with the explosion and model size and so on it becomesï¼Œ yeahã€‚

 really impossible to use these kinds of visualizationsã€‚

So the other way in which we can try and interpret what's going on is through linguistic probe tasks so here we're going to take some model that we want to analyzeã€‚

 let's sayï¼Œ for exampleï¼Œ we want to use BRT we're going to train classifiers on top of the representations output by Bt to predict various properties of the text that we're interested in and then we're not going to train Bt during this this process by which we're learning our classifier because Bt is what we're trying to analyze so we're going to freeze Btã€‚

 we're not going to do any gradient updates to it it's just going to be like a fixed feature extractor that is going to give us a representation for some text and we're going to learn a classifier on top of that to predict these various propertiesã€‚

ðŸ˜Šï¼ŒSo in one of the first papers that proposed these probe tasks there were a couple different tasks proposed and let's go over some of them so we're going to start with our sentence representation this thing could be derived from Bt or derived from whatever model you want to study Roberta now we're going to feed this representation as input to a classifier so this classifier could look like you know a feed for neural network it might have one layer or two layers or whatever the parameters of this classifier are trained from scratch and the parameters of the model that's giving you the sentence representation are fixed so that this is the setup for a probe task and then you're going to train your classifier on some kind of objective so here we're going to train to predict the length of a sentence right so maybeã€‚

ðŸ˜Šï¼ŒGiven Bert's CLS tokenï¼Œ can you predict the length of the sentence that CLS token representsã€‚

 so this would be a reasonable setup for a probe taskã€‚I'm going to apologize in advanceã€‚

 I think I'm really messed up the animations hereï¼Œ so we're going to just have to keep seeing this and overã€‚

 but yeah the Bt CLS representation is kept frozen and this classifiers a feed forward network train from scratchã€‚

And then of courseï¼Œ we would measure the performance of this classifier on some validation set of like sentences we didn't see during training to see if it can predict their lengths as wellã€‚

Allrightï¼Œ greatï¼Œ so another probe task that was proposed in this paper was word content So in this task we have two different types of inputs we have the sentence representation which again is derived from Bt and now we have a word representation and this could be derived from Bt it could be derived from some external word embedding like word devec or anything else that can give you a word representation and this task is called word contentã€‚

 it's basically given a sentence representation and the representation of a word predict whether that word appears in the sentence or not so God we're going to use the same setup the Bt CLS representation is kept frozen maybe we use a subword embedding for the representation of the word that we're trying to query this is also keptã€‚

ðŸ˜Šï¼ŒAt training time and again the only thing we train are the parameters of this classifierã€‚

 so that's a word content taskã€‚And finally there's a word order task so here this is a little more involved than the word content task we're going to give you as inputã€‚

 the sentence or representation and the word representation of two different words and we're going to ask if the sentence representation encodes some information about the order of those two words so whether the first word embedding here comes after the second word in the sentence that the CLS token represents so you can see how these are veryã€‚

 very simple tasks you can easily get data for this just by creating it from unlabeled text and it gives you some insight into whether Bert is encoding any of these kind of fundamental properties of languageã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_13.png)

Allrightï¼Œ we can go beyond this and try and incorporate some external labels and data sets into the probe tasks so for example we can do part of speech tagging here we're gonna to make use of the Bt's token level representations not just the CLS token so for each token in our input we can extract the final layer hidden state that we get from Bert we can feed all of these to a classifier to predict the part of speech token sorry the part of speech tag at each token of the input sentence and we can train our probe task using an existing part of speech tagging data set so we can do the same thing with named entity recognition to see if our model has picked up on the distinction between names of peopleã€‚

 placesï¼Œ objects and other non-entity words and againã€‚

 there are many data sets out there that allow you to train models forã€‚ðŸ˜Šï¼Œentity recognitionã€‚

 which we can use for our probe tasksã€‚We can get even fancier as we'll talk about a bit later and try and predict syntactic structure from our token representationsã€‚

 we could predict things like in a binary taskï¼Œ whether or not in a ground truth parse tree of a sentenceã€‚

 there is an arc between two wordsï¼Œ we can also predict like things like the path length or so onã€‚

 but there's lots of ways in which you can formal or set up a probe task to capture different types of linguistic knowledgeã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_15.png)

Another representation which is maybe even more involved than the other ones is coreferenceã€‚

 so you can say here's a sequence of tokens and here are two windows of text that are associated with mentions of you know possibly related to some entity and your task is to determine whether the two windows of text that are marked refer to the same entity or notã€‚

 so the co-reference task essentiallyï¼Œ so here you might have some sort of composition function that allows you to get a single representation out of the full sequence of token level representations for this window so that could be something like average pooling or you could even learn some model as part of your probe tasks as we'll talk about later that you want to keep your probe tasks as simple as possibleã€‚

 more complicated probe tasksã€‚ðŸ˜Šï¼ŒAnd learn to pick up on all sorts of correlationsï¼Œ whichã€‚

Might lead you astray if you're trying to interpret the findings of your probe tasksã€‚

 so we will start discussing that in a bitã€‚Okayï¼Œ so just to sum upã€‚

 we've now looked at a couple different types of probe tasks varying in how complex they areã€‚

 but the overall idea is if our classifier that we train on the probe task is able to successfully generalize to some validation set that means that the property that of interestã€‚

 whether it be part of speech or sentence length or word orderã€‚

 that information is encoded somewhere in the representationã€‚ðŸ˜Šã€‚

So compared to the visualization approach hereï¼Œ we're not getting at which neuron exactly is responsible for this property or which group of neurons it's a more higher level of conclusion we can draw that there is some subset of neurons in this model that are encoding information about this particular property So it's maybe less specific but also you can't really get the same level of interpretation from a visualization right there's just too many combinations of things that you might have to look atã€‚

ðŸ˜Šï¼ŒAnd if our classifier is unable to predict a certain property right maybe it doesn't encode part of speech very well this means that I meanã€‚

 it doesn't mean that the model doesn't have it hasn't like your burnt model hasn't learned part of speech tagsã€‚

 it just means that your probe task was unable to recover that information from your pretrain model and like if you're unable to recover this informationã€‚

 even with a pretty complicated probe networkï¼Œ that indicates that that that information is probably not readily accessible or you know encoded in a useful way for downstream tasks if you can't even recover it through a fairly large probe networkã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_17.png)

All rightï¼Œ so in generalï¼Œ probe tasks are usually formalized as classification problemsã€‚

 all the ones we've seen so far are classification problemsã€‚

 you can also have simple regression problems as wellï¼Œ and they are usually very simpleã€‚

 so things like basic linguistic propertiesï¼Œ part of speech et ceteraã€‚

 or even more basic like how long is the sentenceã€‚ðŸ˜Šï¼ŒYeahã€‚

Also important is that you can have a model that does really well on these probe tasks but does not do well on a particular downstream task of interestã€‚

 So obviously it would be good if a model that does well on some downstream task is also good at all downstream sorry what did I just say let me say that againã€‚

 a model that's good at downstream tasks is also performs strongly on your linguistic probe taskã€‚

 but you know maybe some linguistic information is just not relevant for you know some downstream task for sentiment analysis for exampleã€‚

 word order is not very important for the vast majority of examples because if I see a sentence that has the word awful in itã€‚

 I don't really need to know much about the rest of the sentence right just having the word awful there is a huge indicator that this will have negative senseã€‚

Of courseï¼Œ you can argue that that could be negated or part of a longer sequence that flips the sentiment some other wayã€‚

 but that's relatively rareã€‚

![](img/95c170530fd920441397f626e5fd3dbd_19.png)

Okayï¼Œ so againï¼Œ this is just reemphasizing the structure of a probe taskã€‚

 we're taking our BRT which is in this blue box here and we're freezing it as you see with this snowflake here so this the network that we're probing is not going to be changed at all during the training of this probe task the classifier in this red box is the only thing that's being updated and it's being updated to predict whatever property we care aboutã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_21.png)

Okayï¼Œ so let's look at some other probe tasks so here I think this is like just a high level slide we'll get into all of these more in detailã€‚

 but we can probe things like numeracy so whether the model has understood numbers and how how they can be manipulated or arithmetic and so onã€‚

 we can look at syntactic structure as I was mentioning before and also related to the problem four on your exam we can look at whether or not these models have learned facts that occur in some existing knowledge baseã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_23.png)

Okayï¼Œ in generalï¼Œ there have been a lot of papers published about probe tasks over the past couple of years and most of them have focused on Bt and Bt like models in generalã€‚

 the findings are that I mean they're kind of intuitive lower layers tend to focus more on local syntactic information while you get kind of high level semantics about different concepts or ideas that are represented in a sentence or some input text those tend to be encoded at higher up layers So you can see in this example I think this was for and Elmo or sorry an LSTM model and here we see a transformer model these are probing representations at different layers of these models for the task of part of speech tagging so which layers encode the most information about part of speech you can see withã€‚

ðŸ˜Šï¼ŒLSTMï¼Œ the part of speech tagging performance at layer one is higher by about less than a point than layer4 I think this is one of the issues with probe tasks is that you might see these differences but at different layers but they're not huge and it's hard to tell if these are meaningful right if your layer4 is still getting 96 or 97% accuracy on part of part of speech taggingã€‚

Can you really conclude that the bottom layer is learning or focusing more on local syntaxã€‚

 It's clear that the top layer is also aware of this kind of syntactic informationã€‚

 just maybe a bit lessï¼Œ soã€‚å—¯ã€‚Yeahï¼Œ so this slide also has some results for constituency parsing and co reference resolution as different probe tasksã€‚



![](img/95c170530fd920441397f626e5fd3dbd_25.png)

There was this paper last year that does a similar analysis showing that Bert kind of learns these traditional NLP pipeline components like part of speech taggingã€‚

 parsing entity recognition and semantic role labeling so this plot just shows essentially the layer at which the probe model performs bestã€‚

 and this is with a 16 layer model we see that part of speech tag and constituents and dependency parsingã€‚

 these are all they have the best performance at lower layers of the networkã€‚

 but then higher up we get things that are associated more with semantics like relation extraction or co-reference resolution or so onã€‚

 but again these differences are not hugeï¼Œ so it is a little misleading to look at a plotã€‚ðŸ˜Šã€‚

Like this and sayï¼Œ oh yeahï¼Œ Bt's lower layers are only about syntax and B's higher layers are only about semanticsã€‚

 the truth is much more murkyï¼Œ there's all of the layers are encoding a mixture of both syntax and semanticsã€‚



![](img/95c170530fd920441397f626e5fd3dbd_27.png)

Okayï¼Œ so let's and I see there have been no questions for a whileï¼Œ which is fineã€‚

 but I want to make sure I'm not ignoring anyï¼Œ you refresh thisã€‚So yeahã€‚

 let's look a little more closely at how you might investigate how well Bert has encoded these tree like parse structures of sentencesã€‚

So here we have the chef who ran out to this the chef who ran to the store was out of foodã€‚

 we're going to pass this sentence through Bert and see if it can recover the ground truth three structure of this sentenceã€‚



![](img/95c170530fd920441397f626e5fd3dbd_29.png)

å—¯ã€‚Okayã€‚Andï¼Œ you knowï¼Œ just as a refresherï¼Œ this kind of syntactic information is very useful in understandingã€‚

 you knowï¼Œ what this sentence is actually sayingï¼Œ rightã€‚

 This is a kind of strange sentence because if youï¼Œ if you read it closelyã€‚

 the chef who ran out of food wasã€‚ðŸ˜Šï¼ŒThe chef who were into the store was out of foodã€‚

 the thing that's a little confusing is where this was out of food phrase attachesï¼Œ rightã€‚

 is it attached to the chef or the store and of courseã€‚

 if you think about it more it seems clear that it's attached to the chef right the chef was out of food So he ran the store But if you're trying to figure out you know which of these two sentences is likely to follow nextã€‚

 It requires understanding this attachment rightï¼Œ was out of food modifies the chef and therefore this option two is correctã€‚

 he bought a bunch of ingredients at the store and now return to the restaurantã€‚

 you can see that this information is also encoded in the tree structure right the distance in terms of number of edges between the word chef and was is one rightã€‚

 indicating that was out of food isã€‚ðŸ˜Šï¼ŒModifying Cheï¼Œ whereas the distance from was and storeã€‚

 which is our other candidate in in the ground truth tree is four there are four edges between was and store So you can see that this kind of distance in the tree is a proxy for these attachment decisionsã€‚

Okayï¼Œ so the paper we're talking about now this Hewitt and manning paper from last yearã€‚

 they designed a method to probe these birdt embeddingsã€‚

 the token embeddings for tree structures and essentially there going to be trying to measure given two words can you predict the path length the number of edges between each of these words in the tree and can you also predict given a word what depth in the tree it is so like Che or sorry was is the root note of this treeã€‚

 something like store is far down in a lower levels of the treeã€‚

 so you can kind of think about words that are higher up as more informative in some waysã€‚



![](img/95c170530fd920441397f626e5fd3dbd_31.png)

![](img/95c170530fd920441397f626e5fd3dbd_32.png)

![](img/95c170530fd920441397f626e5fd3dbd_33.png)

![](img/95c170530fd920441397f626e5fd3dbd_34.png)

So yeah these were their two probe tasks and I should emphasize that from these probe tasks you can actually recover a tree rightã€‚

 given probe task oneï¼Œ you can get a bunch of predicted distances between every single word in the sentence and then use that to you can you can do like a minimum spanning tree to recover a predicted tree structure and then compare it to a ground truth tree structureã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_36.png)

So this is what they do and I would encourage you to look more at this paper if you're interested in all of the details I haven't even covered their probe formulationã€‚

 which is a little different than the simple classifiers that we've seen so farã€‚

 but the conclusion is to some extentï¼Œ Bt has encoded the syntactic tree structure when compared to gold treesã€‚

 this is an unlabeled attachment score which shows that Bt large is encoding more syntactic structure than Elmo or Bt baseã€‚

ðŸ˜Šï¼ŒSo yeahï¼Œ this is one way in which you can probe for more fine grain and syntactic structureã€‚



![](img/95c170530fd920441397f626e5fd3dbd_38.png)

Okayï¼Œ so let's switch over now to numeracy so we've seen I meanã€‚

 a lot of this might not be surprising now because we already saw that GT3 can do lots of impressive things regarding arithmeticã€‚

But a paper that came out before then explicitly probed Bert for its knowledge of numbersã€‚



![](img/95c170530fd920441397f626e5fd3dbd_40.png)

And so there were a couple different tasks that were proposed in this paper by Eric Wallace and some co-authors where in this first task they just put as input to Bt a list of numbers like written out in natural languageã€‚

9412111 they pass that through their BRT model they get the embeddingsï¼Œ they have some probe modelã€‚

 which here you need to actually model the sequence so they put in a recurrent model on topã€‚

 this could be a small transformer as wellï¼Œ then they predict for each token whether it's the maximum of the list or notã€‚

 so the ground truth here you can see is a one for 12ã€‚

 which is the max in this list and a0 everywhere else and they get out the predictions of the model and use this to determine if the model has learned this capability of figuring outã€‚

Which number in a sequence of numbers is the maxï¼ŸAnother task is just trying to transform natural language numbers into the actual numerical value of that numberã€‚

 So here the input is just the word9 that gets embedded through some kind of composition or pre-train model like birds or Elmo and then this is formulated as a regression so here you're asking the model to predict a single number that's close to the value of the number that's inputã€‚

 so the ground truth here is9ï¼Œ the model predicted 8ã€‚5 which is pretty closeã€‚

 we would train this using the square loss as we saw for example in problem 3 of the examã€‚ðŸ˜Šã€‚

You can get a little more complex now you can pass in two numbers and ask the model to add them and also treat this as a regression problem as well note that this is different than how GPT3 formulated these experiments in GT3 it was just a language modeling tasks so you would feed in7 plus2 as a prefix and then ask the model to generate the like character 9 the period and then the zero right so it's a different setup hereã€‚



![](img/95c170530fd920441397f626e5fd3dbd_42.png)

And so this paper found that ElLmo is actually better than BerRT at understanding numbersã€‚

 so you can see hereï¼Œ I mean this is a lotï¼Œ but let's look at the list maximum taskã€‚

 so if your numbers are from zero to 99 you can see that ElLMmo gets 98% accuracy while BRT gets 95% accuracyã€‚

 if your numbers get a little bigger like 0 to 999ã€‚

 the gap kind of widens and if the numbers are even bigger the gapï¼Œ I mean ElLMmo also deterioratesã€‚

 but BRT gets pretty bad thereã€‚And you can see similar trends for the other two tasks as wellã€‚

 So this is you knowï¼Œ might be on its face interestingã€‚

 but there's a very simple explanation for why we're seeing this and it's because in Elmo the first layer to this model is a character level convolutional neural network I don't remember if we actually covered this or notã€‚

 but Elmo does not use subword embeddingsï¼Œ it uses character level embeddings at the first layer and this makes sense right at least for this number task because Bert might be splitting something like 9989 into a bunch of different sub wordss at various locations depending on its tokenizer right so it might not be doing these splits at the right places whereas Elmo doesn'tã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_44.png)

![](img/95c170530fd920441397f626e5fd3dbd_45.png)

![](img/95c170530fd920441397f626e5fd3dbd_46.png)

Have to deal with with that as muchã€‚ And it turns out that the subword encoding is not a great way of encoding digitsã€‚

 So yeahï¼Œ this was one result that was found by this paperã€‚



![](img/95c170530fd920441397f626e5fd3dbd_48.png)

All rightï¼Œ so finally we can look at whether Bert can serve as a knowledge base so this again is following from your question four of your exam so we would like to be able to issue queries like you know where was Dante born to our model and hopefully its predictions would be actually factualã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_50.png)

So there is this Lama probe task which focuses specifically on this kind of probing so normally if you had some sort of knowledge graph you could query it by these kinds of triple so you have two entities and you have a relation and in this query you're given one of the entities and the relation and you're asked to predict the other entity so if you wanted to replicate this in a Bt like settingã€‚

 you would reformulate your query as a sentence with a mask in it right this is just like Harry Potter's social security numbers blahã€‚

 blahï¼Œ blah and then you're going to see what goes into that mask tokenã€‚ðŸ˜Šï¼ŒNowã€‚

 of course this setup has a lot of issues with subword tokenizationã€‚

 what if Florence was not just one word in the vocabï¼Œ but it was multiple subwardsã€‚

 then it becomes a little trickier to do thisï¼Œ but I believe this probe task is set up to avoid that issueã€‚



![](img/95c170530fd920441397f626e5fd3dbd_52.png)

Okayï¼Œ so yeahï¼Œ in this task they defined a bunch of templates that kind of map from one of these triple based queries to an actual natural language sentence with a mask token in itã€‚

 for example X was born in S was born in O would be the equivalent of a place of birth relationã€‚



![](img/95c170530fd920441397f626e5fd3dbd_54.png)

![](img/95c170530fd920441397f626e5fd3dbd_55.png)

And yeahï¼Œ then in some sort of ground truthï¼Œ data setï¼Œ let's say Wikipediaã€‚

 you would find sentences that have both the subject and the object in one of these triples and then mask out one of the twoã€‚

 you hear the object to kind of train this probe taskã€‚



![](img/95c170530fd920441397f626e5fd3dbd_57.png)

Allrightï¼Œ so let's look at some examples here there are a couple of different types of relationsã€‚

 so iPod touch is produced by and you have the ground truth answer Apple and you have the model's predictions So here the model gets this right but for something likeã€‚

ðŸ˜Šï¼ŒI don't knowã€‚ Let's see here a joke would want would make you want to blankã€‚

 The correct answer here is laughï¼Œ so this is notã€‚å—¯ã€‚Likeã€‚A kind of knowledge base factã€‚

But more of a common sense kind of knowledgeã€‚And here the model that they're acquiring actually puts cry as more probable than laughã€‚

 which is kind of bizarreã€‚Andï¼Œ you knowï¼Œ some of these are kind of open endedã€‚

 so typing requires blankï¼Œ the correct answer is speedï¼Œ but of course patienceã€‚

 precision accuracy are all viable here registration is a little confusingã€‚ðŸ˜Šï¼ŒAnd yeahã€‚

 so you can see that all the ones in the top half of this table are directly related to entities and their relations and some of them might still be vagueã€‚

 like Kenya maintains diplomatic relations with blank I mean they could have these relations with multiple countries and there was just one sentence that had Uganda as the object hereã€‚

 but you know all of these could be feasible or reasonable candidatesï¼Œ so this method is not perfectã€‚

 I guess is what I'm trying to sayï¼Œ but with some careful curation of your probe examplesã€‚

 you can at least get some idea of what kind of knowledge your or model is encodingã€‚ðŸ˜Šã€‚



![](img/95c170530fd920441397f626e5fd3dbd_59.png)

And I'm not going to go through this tableï¼Œ but nowadays these kinds of burntbased models are actually competitive with the symbolic knowledge basesã€‚

 the ones that encode all this knowledge like entities and relations in a graph so this requires either someone to manually construct and maintain this knowledge graph or for pretty complicated knowledge graph induction methods it's much simpler to just train a huge language model on unlabeled text and then query it in this way to recover facts and we're probably going to be seeing this more over the next couple yearsã€‚

 the standard symbolic knowledge bases replaced with these huge scale language models which I mean they obviously take a lot of compute to trainã€‚

 but they encode not only facts but they're flexible as we've seenã€‚ðŸ˜Šã€‚



![](img/95c170530fd920441397f626e5fd3dbd_61.png)

![](img/95c170530fd920441397f626e5fd3dbd_62.png)

![](img/95c170530fd920441397f626e5fd3dbd_63.png)

And you knowï¼Œ throughout the semesterï¼Œ they can be used to solve a variety of different downstream tasksã€‚

 so yeah you can probably be on the lookout for itã€‚

For more of this kind of fork over the next year or twoã€‚



![](img/95c170530fd920441397f626e5fd3dbd_65.png)

All rightï¼Œ so let's wrap up by talking about the thing that we've avoided so farã€‚

 which is the architecture and complexity of our probe networkã€‚

So one thing that we've sidesteppped is whether it's basically how how many parameters should we have in our probe network and there's kind of probes and cons for both if you have a very simple probe task and maybe you have just you know like a single softmax layer over your BRT representation this model is unlikely to be able to find information that is encoded you know in a highly nonlinear or inaccessible way whereas a complicated probe task that has multiple layers or more complex architecture might be able to uncover this information if it's like somehow deeply embedded within the representation On the other handã€‚

 if a simple probe task can recover the information that might be a sign that that information is very easily and readily accessible within the representation because even such a simpleã€‚

ðŸ˜Šï¼ŒNet could pick it out of the networkã€‚ So pros and cons to bothã€‚



![](img/95c170530fd920441397f626e5fd3dbd_67.png)

And one thing that you really need to consider and this paper came out last year when you're developing a new probe task is adding a control task and this lets you kind of account for the confound of your probe complexity because if your probe network is you know super complicated and has a lot of parametersã€‚

 it can actually learn most functions so in these control tasksã€‚

 what you do is you take the vocabulary of your model and then you randomly assign certain words to certain labelsã€‚

So let's say I'm interested in doing a part of speech tagging probe taskã€‚

 I'm going to make a control task where I select random subsets of my vocabulary and assign them to random labelsã€‚

 So here the exclamation mark and the word after are assigned a label 42 the words the and ran are assigned to label 10 Quick and dog are assigned to 15ã€‚

 These are just a random subsetsã€‚ So there's no reason why we're doing thisã€‚ðŸ˜Šã€‚

Compared to a part of speech tagging data set where cat and dog would be in the same label right they're both nouns and exclamation point and period would be in the same cluster because they're punctuation marksã€‚

 here there's no reason behind the labeling of the control taskã€‚

So you can see now I have two different tasksï¼Œ one is predict the part of speech tags given the BRT representations of say the cat ran quicklyã€‚

 this is a determiner noun verb and adverb so this is the ground truthop part of speech tag sequence that the model is predictingã€‚

 but now we also have our control tasks so we're asking the model to predict these like random assignments of various wordsã€‚

ðŸ˜Šï¼ŒSo like the and RA have the same label in the control taskã€‚

 but of course they're very different wordsï¼Œ so the idea is that a complex probe network could even learn the mapping between control task labels and the wordsã€‚

 even though they're completely randomï¼Œ given enough parametersã€‚

 the the control task could also be solvedã€‚ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_69.png)

And so rightï¼Œ thisï¼Œ I've just explained how you get the control taskã€‚

 it's just making random assignments to words in your vocabularyã€‚



![](img/95c170530fd920441397f626e5fd3dbd_71.png)

So the idea behind this method is that you have your normal probe taskã€‚

 but you also measure the performance of your control task so in this plot you see a proxy measure of the complexity of the probe task just like the number of hidden units in the probe network and on the y axis you see the accuracy of the probe task and here we're plotting two different probe task one is the part of speech tagging and the other is our control task with a random labelsã€‚

So we see with the less complex probe network with just two hidden unitsã€‚

 our part of speech tagging performance is somewhere near like 85ã€‚

 but our control task performance is way down at like 35% so there's a big gap between the part of speech tagging probe task and the control taskã€‚

 this indicates that the probe taskï¼Œ the probe network was unable to learn the random mapping of the control labels and it was able to learn the part of speech tags which indicates that that part of speech information is really helping the modelã€‚

 this simple model achieve a good accuracyã€‚ðŸ˜Šï¼ŒSo they define a selectivity as essentially the difference between the probe network and the control tasksã€‚

 so if the difference is largeï¼Œ then the model is more selective and if the difference is smallã€‚

 the model is the probe network is less selectiveã€‚ðŸ˜Šã€‚

So the idea is that you kind of want to maximize your probe task performance so you can see that with 10 hidden unitsã€‚

 I get up to like 95% accuracy and part of speech tagging and my accuracy in the control task although it's improved is still much worse it's below 70% so this might be an ideal complexity of the probe network to chooseã€‚

 you can see that if I increase the complexity of my probe network to say100 hidden unitsã€‚

 now both networksï¼Œ both probe tasks are doing about equally wellã€‚

 like the accuracy of my control taskï¼Œ which again is using completely random labels is almost equivalent to the accuracy of my part of speech tags data set which obviously has sensible and meaningful labels to usã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_73.png)

![](img/95c170530fd920441397f626e5fd3dbd_74.png)

So this is important if you are thinking about doing some probe tasks in your final projectã€‚

 make sure you do a control probeï¼Œ it's very straightforward to set up and you can make a plot like this to kind of validate your choice of probe architectureã€‚



![](img/95c170530fd920441397f626e5fd3dbd_76.png)

And yeahï¼Œ the message here is just be careful about your probe acuracies right you can have a veryã€‚

 very high accuracy on the probe task that you care aboutã€‚

 but its it might be misleading if the accuracy in a control probe is also very high with the same networkã€‚

So for part of speech taggingï¼Œ for exampleï¼Œ a single layer network is better than a network with non nonlinearity and one hidden layerã€‚

 as you can see they achieve similar acuracies on part of speech tagging but the one with only one layer is much more selective meaning the control task performs much worse than when you have an additional layer hereã€‚

ðŸ˜Šã€‚

![](img/95c170530fd920441397f626e5fd3dbd_78.png)

Okayï¼Œ so I see there's a question when doing queries in case they are not clear to Bertã€‚

 I wonder if adding a sentence describing the keywords helps Yeah that is definitely true and this is something that will probably see more of as models like GT3 become increasingly available right you know that the knowledge might be encoded in there somewhereã€‚

 but you may not know the exact way to query the model to get that knowledge out of the networkã€‚

 so you might want to do things like you know paraphrase your query or add more context to itã€‚

 retrieve information that's relevant about keywords or so on all of those are reasonable things to try Yeah so good pointã€‚

ðŸ˜Šï¼ŒOkayï¼Œ so let's wrap up here one thing you can do once you've implemented a probe task just to analyze your model is you can actually try and improve your BRT model if you see that it's lagging behind on a particular probe task and the way you can do this is simply just add your probe objective to your normal BRT objective and then basically just fine tune it so the idea is that if you add this objective during BerRT's pretraining process then that information will be encoded in a easier to represent way in the final hidden stateã€‚



![](img/95c170530fd920441397f626e5fd3dbd_80.png)

So one example of this is this Nobert paper from last year in which the idea is just adding more information about entities in the text into the Bt processing So here we have a sentence print saying purple rain these red networks are all from Bt you know self-attention computations in Nobertã€‚

 we extract the vectors that correspond to entitiesã€‚

 So prints and purple rain and then we kind of augment these representations with representations about those entitiesã€‚

 So for exampleï¼Œ purple rain albumï¼Œ film songï¼Œ I can have separate entity embeddings for each of these that have been trained through some separate processã€‚

 similarly print musician obviously the one being referenced hereã€‚

 So if I have an entity embedding for for printsï¼Œ then I can just simply add itã€‚ðŸ˜Šã€‚

to the vector for that I got out of my contextualization processã€‚

 I think in this paper they use a more complicated composition functionã€‚

 but once you you add in all of this information about these entities from some static set of entity embeddingsã€‚

 you can then apply another layer of self-atten to recontextualize the information with the new information you got from your knowledge sourceã€‚

 so this is an example of improving BRT's ability to capture relations between entities in its vectorsã€‚



![](img/95c170530fd920441397f626e5fd3dbd_82.png)

Okayï¼Œ so I think that's all I had for today and yeahï¼Œ look forward to seeing you next Mondayã€‚



![](img/95c170530fd920441397f626e5fd3dbd_84.png)