# P19ï¼š20240705-å‰æ²¿äººå·¥æ™ºèƒ½å®‰å…¨ä¸æ²»ç†è®ºå› - WarrenHua - BV1iT421k7Bv

å°Šæ•¬çš„å„ä½é¢†å¯¼ã€å˜‰å®¾å’Œæœ‹å‹ä»¬å¤§å®¶å¥½ã€‚æ¬¢è¿å¤§å®¶æ¥åˆ°ä»Šå¹´ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šçš„å‰æ²¿AIå®‰å…¨ä¸æ²»ç†è®ºå›ã€‚æˆ‘æ˜¯è°¢ç±³å¸Œå®‰çº½AIçš„CEOã€‚å®‰å…¨AIæ˜¯ä¸€å®¶å®‰å…¨ä¸æ™ºç†é¢†åŸŸç¬¬ä¸‰æ–¹ç ”ç©¶å’Œå’¨è¯¢æœºæ„ã€‚

ä¹Ÿæ˜¯ç›®å‰è¯¥é¢†åŸŸå…¨å›½å”¯ä¸€çš„ç¤¾ä¼šä¼ä¸šã€‚ğŸ˜Šï¼ŒDistinguished guestsï¼Œ ladies and gentlemen around the worldã€‚ Good morningã€‚ğŸ˜Šã€‚

Welcome to the F AI Safety and governance For of the 2024 World AI Conferenceã€‚ğŸ˜Šï¼ŒMy name is Braciierã€‚

 CEO of Concordi AIï¼Œ a social enterprise focused on AI safety and governanceã€‚

æœ¬æ¬¡è®ºå›æˆ‘ä»¬ç‰¹åˆ«è£å¹¸é‚€è¯·åˆ°ä¸Šæµ·å¸‚çš„é¢†å¯¼è…ä¸´æŒ‡å¯¼å’Œäº¤æµï¼Œè¯·å…è®¸æˆ‘ä¸ºå¤§å®¶ä»‹ç»ã€‚ä¸Šæµ·å¸‚äººæ°‘æ”¿åºœå‰¯ç§˜ä¹¦é•¿å¼ æœ¨å¨£å…ˆç”Ÿã€‚å»å¹´4æœˆï¼Œæˆ‘å›½ä¸­å¤®ç»æ²»å±€ä¼šè®®æ·±åˆ»æŒ‡å‡ºï¼Œè¦é‡è§†é€šç”¨äººå·¥æ™ºèƒ½å‘å±•ï¼Œé‡è§†è®¿é—®é£é™©ã€‚åŒå¹´10æœˆã€‚

æˆ‘å›½å‘å¸ƒå…¨çƒäººå·¥æ™ºèƒ½æ²»ç†å€¡è®®ï¼Œé‡ç”³å„å›½ï¼Œåº”åœ¨AIæ²»ç†ä¸­åŠ å¼ºä¿¡æ¯äº¤æµï¼Œå…±åŒåšå¥½é£é™©é˜²èŒƒã€‚åŒæœˆï¼Œæˆ‘å¾ˆè£å¹¸å—é‚€å‚åŠ äº†é¦–å±Šå…¨çƒAIå®‰å…¨å³°ä¼šã€‚

è§è¯äº†åŒ…æ‹¬ä¸­å›½åœ¨å†…çš„28ä¸ªå›½å®¶å’Œæ¬§ç›Ÿå…±åŒç­¾ç½²å¸ƒè±åˆ‡é‡Œå®£è¨€bllash decorationã€‚è¿™ä¹Ÿæ˜¯ç¬¬ä¸€ä»½AIå®‰å…¨çš„å›½é™…å£°æ˜ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œç¤¾ä¼šéœ€è¦åŠ å¼ºå‰æ²¿AIå®‰å…¨ç ”ç©¶ã€å®‰å…¨è¯„æµ‹ã€å®‰å…¨æ²»ç†ä»¥åŠå›½é™…åˆä½œã€‚

è¿™ä¹Ÿæ˜¯ä»Šå¤©è®ºå›çš„å››ä¸ªä¸»é¢˜ã€‚ç¬¬ä¸€ä¸ªä¸»é¢˜æ˜¯å®‰å…¨ç ”ç©¶ã€‚æˆ‘ä»¬å¾ˆè£å¹¸é‚€è¯·åˆ°å›½å†…å¤–AIé¢†åŸŸçš„ä¸–ç•Œçº§ç§‘å­¦å®¶ã€‚å›¾çµå¥–å¾—ä¸»ä¼˜ Ben9ç‰µå¤´å‘å¸ƒäº†ç¬¬ä¸€ä»½å…ˆè¿›AIå®‰å…¨å›½é™…ç§‘å­¦æŠ¥å‘Šã€‚

æœ‰30ä¸ªå›½å®¶ã€æ¬§ç›Ÿå’Œè”åˆå›½æåçš„å§”å‘˜ä¼šå…±åŒå‚ä¸ï¼Œå¯¹é€šèå‹AIçš„å®‰å…¨é£é™©è¿›è¡Œäº†ç§‘å­¦è¯„ä¼°ã€‚ä¸­å›½å·¥ç¨‹é™¢é«˜æ–‡é™¢å£«è®¤ä¸ºï¼Œå…¨ä¸–ç•Œæ­£å¤„äºAGIå¼ºäººç‰©è´¨èƒ½çš„å‰å¤œï¼Œåœ¨ä¸€ä¸ªä¸ç¡®å®šçš„çŠ¶æ€ï¼Œéœ€è¦ä¸¥åŠ é˜²èŒƒã€‚

AGIå¯èƒ½ä¼šå¼•å‘çš„äººç±»ç”Ÿå­˜é£é™©ã€‚ä¸­å›½å·¥ç¨‹é™¢å¼ é›…ç´é™¢å£«è”åˆçº¦ Ben9å¬é›†äº†ç¬¬ä¸€å±ŠAIå®‰å…¨å›½é™…å¯¹è¯ï¼Œå¹¶è”åˆåšå…‹åˆ©åˆ†æ ¡ç«¯é¢‚ç­‰é¢†å…ˆç§‘å­¦å®¶åœ¨ä¸‰æ¡ˆä¸»å¹²ä¸Šå‘è¡¨è®ºæ–‡ã€‚

å»ºè®®åˆ†é…3åˆ†ä¹‹1çš„AIç ”å‘èµ„é‡‘åˆ°AIå®‰å…¨å’Œä¼¦ç†ç­‰ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬æœŸå¾…å’Œå¤šä½AIå®‰å…¨ç§‘ç ”å›¢é˜Ÿå¸¦å¤´äººï¼ŒåŒ…æ‹¬ä¸Šæµ·AIå®éªŒå®¤çš„é‚µé™ï¼ŒåŒ—äº¬å¤§å­¦çš„æ¨è€€ä¸œå’Œä¸Šæµ·äº¤é€šå¤§å­¦çš„å¼ å“èƒœè®¨è®ºå‰æ²¿ç ”ç©¶é—®é¢˜ã€‚ç¬¬äºŒä¸ªä¸»é¢˜æ˜¯å®‰å…¨è¯„æµ‹ã€‚

æˆ‘ä»¬å¾ˆé«˜å…´é‚€è¯·åˆ°å¤§æ¨¡å‹å®‰å…¨è¯„æµ‹çš„é¢†å†›äººç‰©ã€‚åœ¨å­¦æœ¯ç ”ç©¶æ–¹é¢ï¼Œä¸Šæµ·AIå®éªŒå®¤é¢†å†›ç§‘å­¦å®¶ä¹”å®‡ç¬¬ä¸€æ¬¡ä»¥äººç±»ä»·å€¼è§‚çš„è§’åº¦å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„æµ‹ã€‚

å¤©æ´¥å¤§å­¦NLPå®éªŒå®¤ä¸»ä»»ç†Šå¾·ä¹‰å‘è¡¨äº†ä¸­æ–‡å¤§æ¨¡å‹å‰æ²¿é£é™©è¯„æµ‹çš„ä¸€ç³»åˆ—è®ºæ–‡ã€‚åœ¨è¡Œä¸šè”ç›Ÿæ–¹é¢ï¼Œä¸­å›½æ–°é€šé™¢äººå·¥æ™ºèƒ½ç ”ç©¶æ‰€æ‰€é•¿é­å‡¯ä¾æ‰˜AIAå®‰å…¨æ²»ç†å§”å‘˜ä¼šå¯åŠ¨äº†ä¸€ç³»åˆ—å¤§æ¨¡å‹å®‰å…¨è¯„æµ‹å·¥ä½œã€‚

open AI anropicè°·æ­Œdmå’Œå¾®è½¯æˆç«‹äº†å‰æ²¿æ¨¡å‹è®ºå›ã€‚æ‰§è¡Œä¸»ä»»ch Merowå°†åˆ†äº«é¢†å…ˆç¾å›½ä¼ä¸šçš„å®‰å…¨å®è·µã€‚ç¬¬ä¸‰ä¸ªä¸»é¢˜æ˜¯å®‰å…¨æ²»ç†ã€‚å„å›½å®¶æ­£åœ¨å¼€å±•å¯¹AIå®‰å…¨æ²»ç†çš„ç§¯æç ”åˆ¤å’Œå°è¯•ã€‚

æˆ‘ä»¬å¾ˆé«˜å…´é‚€è¯·åˆ°æ³•å›½æ”¿åºœäººå·¥æ™ºèƒ½å§”å‘˜ä¼šæˆå‘˜gilï¼Œæ–°åŠ å¡æ”¿åºœé¦–å¸­AIå®˜ä½•ç‘æ•ï¼Œä¸­å›½æ”¿æ³•å¤§å­¦æ•°æ®æ³•åˆ¶ç ”ç©¶é™¢æ•™æˆå¼ æ—æ¶µä»¥åŠä¼¯å…‹åˆ©åˆ†æ ¡ç°åœ¨ for humancomp AIä¸»ä»»markåˆ†äº«å¤šå…ƒåœ°åŒºè§†è§’ã€‚

åŒæ—¶æˆ‘ä»¬ä¹Ÿé‚€è¯·åˆ°ä¸Šæµ·äº¤èå¤§å­¦ä¸­å›½æ³•ä¸ç¤¾ä¼šç ”ç©¶é™¢é™¢é•¿å­£å«ä¸œå’Œä¸Šæµ·AIå®éªŒå®¤æ²»ç†ç ”ç©¶ä¸­å¿ƒå‰¯ä¸»ä»»ç‹è‹±æ˜¥å‚ä¸ç ”æ¡Œè®¨è®ºï¼Œæ¢è®¨AIç«‹æ³•å’Œä¸Šæµ·AIæ²»ç†ç»éªŒã€‚ç¬¬å››ä¸ªä¸»é¢˜æ˜¯å›½é™…åˆä½œã€‚æˆ‘ä»¬å¾ˆè£å¹¸é‚€è¯·åˆ°å¤šå®¶å›½é™…é¡¶å°–æ™ºåº“ã€‚

åŒ…æ‹¬å‡¯ä¹ƒåŸºå›½é™…å’Œå¹³ç ”ç©¶é™¢ä¸»å¸­marnoinoellaå’Œç ”ç©¶å‘˜mshhanï¼Œæ¸…åå¤§å­¦äººå·¥æ™ºèƒ½å›½é™…æ²»ç†ç ”ç©¶é™¢é™¢é•¿è–›å…°ã€‚ç‰›æ´¥å¤§å­¦é©¬ä¸äººå·¥æ™ºèƒ½æ²»ç†ä¸­å¿ƒä¸»ä»»roberttgerã€‚

åŠ æ‹¿å¤§å›½é™…æ²»ç†åˆ›æ–°ä¸­å¿ƒå…¨çƒAIå®‰å…¨é£é™©ä¸»ä»»é‚“can Caè®¨è®ºAIå®‰å…¨çš„å›½é™…æ²»ç†è®®é¢˜ã€‚è”åˆå›½AIé«˜å±‚é¡¾é—®æœºæ„ä¸“å®¶æ›¾æ¯…å°†æå‡ºAIå®‰å…¨çº¢çº¿ã€‚

å…¨çƒé¢†å…ˆå¤§æ¨¡å‹å¼€æºç¤¾åŒºhgging faceå…¨çƒæ”¿ç­–è´Ÿè´£äººè‰¾in Solomonå°†è®¨è®ºIæºå¼€æºæ¨¡å‹å¯¹å›½é™…è·ç¦»çš„å½±å“ã€‚æœ€åæˆ‘ä»¬å°†é‚€è¯·ä¸Šæµ·AIå®éªŒå®¤ä¸»ä»»ã€é¦–å¸­ç§‘å­¦å®¶å‘¨åšæ–‡è¿›è¡Œé—­å¹•è‡´è¾ï¼Œå±•æœ›AIå®‰å…¨çš„æœªæ¥ã€‚

ç°åœ¨æˆ‘ä»¬è¿›å…¥è®ºå›çš„è¯è¿°ç¯èŠ‚ã€‚é¦–å…ˆæœ‰è¯·ä¸Šæµ·å¸‚äººæ°‘æ”¿åºœå‰¯ç§˜ä¹¦é•¿åº„æœ¨å¨£ä¸ºæˆ‘ä»¬çš„è®ºå›è¿›è¡Œå¼€å¹•è‡´è¾ã€‚æœ‰è¯·ã€‚å°Šæ•¬çš„é«˜è£é™¢å£«ã€‚å°Šæ•¬çš„å¼ äºšç´é™¢å£«ã€‚å„ä½æ¥å®¾ã€å¥³å£«ä»¬ã€å…ˆç”Ÿä»¬ï¼Œæœ‹å‹ä»¬ï¼Œå¤§å®¶ä¸Šåˆå¥½ã€‚

å¾ˆé«˜å…´å’Œå¤§å®¶ä¸€èµ·ç›¸èšåœ¨2020ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šï¼Œå…±åŒå‚ä¸å…¨é¢äººå·¥æ™ºèƒ½å®‰å…¨ä¸æ²»ç†çš„è®ºå›ï¼Œå…±åŒæ¢è®¨äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿å’Œæ™ºç†é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»£è¡¨ä¸Šæµ·å¸‚äººæ°‘æ”¿åºœã€‚

å¯¹æœ¬æ¬¡å‚åŠ è®ºå›çš„ç§‘å­¦å®¶ã€ä¼ä¸šå®¶ä»¥åŠåª’ä½“æœ‹å‹ä»¬è¡¨ç¤ºçƒ­çƒˆçš„æ¬¢è¿å’Œè¡·å¿ƒçš„æ„Ÿè°¢ã€‚äººå·¥æ™ºèƒ½ä½œä¸ºæ–°ä¸€è½®ç§‘æŠ€é©å‘½å’Œäº§ä¸šå˜é©çš„é‡è¦é©±åŠ¨åŠ›ï¼Œæ­£æ·±åˆ»çš„å½±å“ç€å…¨çƒç»æµç»“æ„å’Œç¤¾ä¼šå‘å±•ã€‚éšç€æŠ€æœ¯æŒç»­è¿­ä»£çš„æ¼”è¿›ã€‚

äººå·¥æ™ºèƒ½çš„å®‰å…¨å’Œæ²»ç†ï¼Œä¹Ÿä¹ç›Šæˆä¸ºå…¨çƒå…³æ³¨çš„ç„¦ç‚¹ï¼Œä¸­å›½é«˜åº¦é‡è§†äººå·¥æ™ºèƒ½çš„å¥åº·å‘å±•ã€‚å»å¹´10æœˆï¼Œä¹ è¿‘å¹³ä¸»å¸­æå‡ºäº†å…¨çƒäººå·¥æ™ºèƒ½æ™ºåŠ›çš„å€¡è®®ï¼Œç³»ç»Ÿçš„é˜è¿°äº†ä¸­å›½å…³äºå…¨çƒäººå·¥æ™ºèƒ½æ™ºç†çš„ç«‹åœºã€ä¸»å¼ å’Œå»ºè®®ã€‚

å±•ç°äº†ä¸­å›½åœ¨æ¨åŠ¨å…¨çƒäººå·¥æ™ºèƒ½å‘å±•å’Œæ™ºç†æ–¹é¢ç§¯æçš„æ€åº¦å’ŒåŠ¡å®çš„è¡ŒåŠ¨ã€‚å»å¹´11æœˆï¼ŒåŒ…æ‹¬ä¸­å›½ã€ç¾å›½åœ¨å†…çš„28ä¸ªå›½å®¶å’Œæ¬§ç›Ÿå…±åŒç­¾ç½²äº†å¸ƒè±åˆ‡åˆ©äººå·¥æ™ºèƒ½å®‰å…¨å®£è¨€ã€‚è¿™ä¹Ÿæ˜¯å…¨çƒç¬¬ä¸€ä»½é’ˆå¯¹äººå·¥æ™ºèƒ½å®‰å…¨çš„å›½é™…æ€§çš„å£°æ˜ã€‚

ä½“ç°äº†ä¸­å›½åœ¨å…¨çƒäººå·¥æ™ºèƒ½æ²»ç†é¢†åŸŸçš„è´£ä»»å’Œæ‹…å½“ã€‚ä¸Šæµ·ä½œä¸ºä¸­å›½ç»æµåŸå¸‚çš„ä¸­å¿ƒå’Œç§‘æŠ€åˆ›æ–°çš„å‰æ²¿ã€‚åœ¨äººå·¥æ™ºèƒ½å®‰å…¨å’Œæ²»ç†æ–¹é¢å¼€å±•äº†å®è·µå’Œæ¢ç´¢ã€‚ç‰¹åˆ«æ˜¯åœ¨å…¨å›½ç‡å…ˆå‡ºå°äº†äººå·¥æ™ºèƒ½çš„åœ°æ–¹æ€§çš„ä¸€éƒ¨æ³•è§„ã€‚

å°±æ˜¯ä¸Šæµ·å¸‚ä¿ƒè¿›äººå·¥æ™ºèƒ½äº§ä¸šå‘å±•æ¡ä¾‹ï¼Œæ¢ç´¢æ„å»ºä½“ç³»åŒ–çš„æ²»ç†æ¡†æ¶ï¼Œç»Ÿç­¹äººå·¥æ™ºèƒ½å‘å±•ä¸å®‰å…¨ã€‚åŒæ—¶ä¹Ÿå‘å¸ƒäº†äººå·¥æ™ºèƒ½æ ‡å‡†åŒ–ä½“ç³»å»ºè®¾çš„æŒ‡å¯¼æ„è§ï¼Œæ¨åŠ¨ä¸Šæµ·åœ¨äººå·¥æ™ºèƒ½æ ‡å‡†é¢†åŸŸçš„å…ˆè¡Œå…ˆè¯•ã€‚

åŠªåŠ›åŸ¹è‚²äººå·¥æ™ºèƒ½é«˜æ°´å¹³çš„ä¸Šæµ·æ ‡å‡†ã€‚å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬å°†ç»§ç»­åœ¨äººå·¥æ™ºèƒ½å®‰å…¨å’Œæ™ºç†æ–¹é¢å‘æŒ¥å¼•é¢†ä½œç”¨ã€‚æˆ‘ä»¬å°†æŒç»­å®Œå–„æ”¿ç­–ä½“ç³»ï¼ŒåŠ å¼ºæŠ€æœ¯ç ”ç©¶å’Œäººæ‰åŸ¹å…»ã€‚åˆ¶å®šæ›´å…·æ“ä½œæ€§ï¼Œæ›´åŠ å®Œå–„æ ‡å‡†è§„åˆ’å’Œæµ‹è¯„ä½“ç³»ã€‚

æˆ‘ä»¬å°†åšæŒåŒ…å®¹å®¡æ…ç›‘ç®¡ï¼Œä»¥é¼“åŠ±åˆ›æ–°ä¸ºåŸåˆ™ï¼Œæ¢ç´¢å¤§æ¨¡å‹è¯„æµ‹è¯•ç‚¹æ²™æ²³ç›‘ç®¡ã€‚æˆ‘ä»¬å°†ç§¯ææ¨åŠ¨è‡ªåŠ›ç ”ç©¶ï¼Œåœ¨å¥å…¨æ³•è§„ä½“ç³»ã€ç›‘ç®¡ä½“ç³»ç­‰æ–¹é¢ï¼ŒåŠªåŠ›æ¢ç´¢ï¼ŒåŠªåŠ›å½¢æˆå…·æœ‰ä¸Šæµ·ç‰¹è‰²çš„ç›‘ç®¡çš„å®è·µçš„æ–¹æ¡ˆã€‚å„ä½æ¥å®¾ã€‚

æœ¬æ¬¡è®ºå›æ±‡èšäº†ä¸–ç•Œçº§çš„ä¸“å®¶å­¦è€…å’Œä¸šç•Œçš„é¢†è¢–ï¼Œå°†å›´ç»•å…¨å‘˜äººå·¥æ™ºèƒ½å®‰å…¨çš„ç ”ç©¶ã€è¯„æµ‹ã€æ²»ç†ç­‰è®®é¢˜å±•å¼€äº¤æµè®¨è®ºã€‚æˆ‘ä»¬ç›¸ä¿¡é€šè¿‡å¤§å®¶çš„å…±åŒåŠªåŠ›ï¼Œæˆ‘ä»¬ä¸€å®šèƒ½å¤Ÿæˆä¸ºå…¨çƒäººå·¥æ™ºèƒ½å®‰å…¨å’Œæ²»ç†é—®é¢˜æä¾›åŠ¡å®æ–¹æ¡ˆå’Œæœ‰ç›Šå€Ÿé‰´ã€‚

æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯æ›´å¥½çš„æœåŠ¡äºç¤¾äººç±»ç¤¾ä¼šçš„å‘å±•ã€‚ä¸Šæµ·å°†æä¾›æ›´åŠ å¼€æ”¾çš„å¹³å°ã€æ›´åŠ ä¸°å¯Œçš„åœºæ™¯ï¼Œæ›´åŠ ä¼˜è‰¯çš„ç¯å¢ƒï¼Œæ”¯æŒå…¨çƒäººå·¥æ™ºèƒ½å®‰å…¨å’Œè‡ªç†é¢†åŸŸçš„ç ”ç©¶è€…çš„è¿›è¡Œæ·±å…¥çš„æ¢ç´¢å’Œå®è·µã€‚æœ€åé¢„ç¥æœ¬æ¬¡å¤§ä¼šå–å¾—åœ†æ»¡æˆåŠŸã€‚

è°¢è°¢å¤§å®¶ã€‚æ„Ÿè°¢å¢“åœ°ç§˜ä¹¦é•¿çš„ç²¾å½©è‡´è¾ï¼Œè¯·å…¥åº§ã€‚å¤§å®¶å¥½ï¼Œæˆ‘å«å´å›æ€¡ï¼Œæ˜¯å®‰è¿œAIé«˜çº§é¡¹ç›®ç»ç†ï¼Œä¹Ÿæ˜¯ä»Šå¤©è®ºå›çš„ä¸»æŒäººã€‚é‰´äºä»Šå¤©æœ‰å¤šä½å›½é™…å˜‰å®¾ï¼Œæˆ‘çš„ä¸»æŒå°†ç”¨è‹±è¯­è¿›è¡Œã€‚Distinguished guestsã€‚

 ladies and gentlemenã€‚My name is Kuan Ygã€‚I work as a senior program manager at Concordia AIã€‚

 and I will be your moderator for todayã€‚Given our large proportion of international speakersã€‚

 the majority of this forum will be moderated in Englishã€‚Without further adoã€‚

 Im delighted to introduce our opening speakerï¼Œ Professor Yohua Benggioã€‚ğŸ˜Šã€‚

Reconized worldwide as one of the leading experts in artificial intelligenceã€‚

 Professor Benjo is most known for his pioneering work in deep learningã€‚

 which earned him the touring Award in 2018ï¼Œ along with Geoffrey Heinson and Janan Lacounã€‚ğŸ˜Šã€‚

In recent yearsï¼Œ Professor Benjo has been raising awareness about the rapid pace at which AI is advancing and the potential implicationsã€‚

This Mayï¼Œ he chaired a diverse group of 75 AI experts from around the world to publish the international scientific report on the safety of advanced AIã€‚

 which I was honored to contribute to as one of the writersã€‚ğŸ˜Šã€‚

Professor Venjo will be sharing key findings of this report and open problems in AI safety with us todayã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_1.png)

All rightï¼Œ thank you for joining usï¼Œ Profess youã€‚Yesï¼Œ we can hear you fineã€‚Okayï¼Œ I'm going to startã€‚

 thank you very much for the kind words and thank for your work in this paperã€‚

So today I I want to tell you about the international report on AI safety that I've been sharingã€‚

And what it means for the beginning of an international consensus on the risksã€‚And safety of AIã€‚

 So this focuses mostly on the advanced AIã€‚ There are many kinds of AIã€‚

 So we're thinking about mostly the general purpose AIã€‚

 like in the large language models and other multimodal models that we've seen recently attract all the attentionã€‚

So there'll be two parts first I'll talk about the report and then at the end a few words about my thoughts regarding what it means for the future and the sort of big picture recommendations that I haveã€‚

 even though the report itself doesn't have any recommendationã€‚

 the report is a synthesis of the science meant to help the policymakers in their jobã€‚Okayã€‚

 so the reportï¼Œ it's called the International Scientific report on the Safety of Advanced AIã€‚

 And you knowï¼Œ we took a lot of time to figure out the right titleã€‚ That's what we gotï¼Œ soã€‚Yeahã€‚

 the report focuses on risks because of course there there's already a lot of work and scientific work on the applications and the benefits of AIã€‚

 but as far as policymakers are a concernï¼Œ it is important that they understand the dangersã€‚

 the risksï¼Œ as well as the capabilities so that they can manage those risksï¼Œ for exampleã€‚

 with regulationã€‚å—¯ã€‚And we were given the mission after the Lechley Park UK AI Safety For last November to support the development of an internationalã€‚

 independent and inclusive report on the capabilities and risks of AIã€‚

 so independent means that the scientists had the final wordï¼Œ not the countriesã€‚

 there were 30 countries plus the EU and UN involvedã€‚And how'll tell you more about the processã€‚å—¯ã€‚

So the report also is pretty broad in its coverageã€‚

 so all the risks the current harms that are well studiedï¼Œ you knowã€‚

 from bias and disinformation to the harms that are anticipatedï¼Œ like labor market impactsã€‚

 many different kinds of misuse and of courseï¼Œ like the biggest risk of loss of control of superhumania or something like thisã€‚

And the idea was to not create new scienceï¼Œ but just to summarizeã€‚

 to synthesize the current scientific literature that provide evidence about these questionsã€‚

For the benefit of policymakersã€‚Al rightï¼Œ so the group of 75 people working on thisã€‚

 there is a advisory panelã€‚ So these each country in the group of 30 nominated one expertã€‚

 also the EU and the UN N nominated one expert eachã€‚ so that's our panelã€‚And in additionã€‚

 we nominated 16 writersã€‚ So these people are actually writingã€‚ The panel is providing feedbackã€‚

 commentsï¼Œ and we iterate between with the panel and different versions of the reportã€‚

We also consulted a group of senior advisors who are experts in various aspects of what we've been looking for about 2026 of themã€‚

 So total 75 experts involvedã€‚ That's a lot of people to manageã€‚In in a few monthsã€‚

So what do we find in a reportï¼Œ Okay some spotlightsï¼Œ maybe not surprisinglyã€‚

 but it is important to tell the policymakers there's a lot of disagreement in the scientific community about the risks but we know go a little bit deeperã€‚

 some questions there is more consensus in some there is lessã€‚

 and so it is also important to not just say there is disagreementã€‚

 What are the views on risks on the timeline of AGï¼Œ Is it going to happenã€‚

 I it going to be a few yearsï¼Œ is is going to be decades and so onã€‚ And then the views on the impactã€‚

 what happens when we get to AGIï¼Œ what you knowï¼Œ is there going to be a fast takeoffï¼Œ for exampleã€‚

 how is it to affect society in various waysã€‚Theseï¼Œ these were all very importantã€‚

Then the report talks about risk productionï¼Œ so what is the current science to try to mitigate those risksã€‚

 what are those methods and what are their limitationsã€‚And the bottom lineã€‚

 I think like the the main conclusion of the reportï¼Œ if you want to like one linerã€‚

 is that unfortunatelyï¼Œ there is no currently known methodã€‚To preventã€‚

The current risks and the future risksï¼Œ the potentially catastrophic risksï¼Œ for exampleã€‚

 of misuse and loss of controlã€‚So that's a big call to armsï¼Œ a big red flagã€‚But but you knowã€‚

 the silver lining is we still have agency collectivelyã€‚

 the world can act in ways to better understand those risks and better mitigate themã€‚Okayã€‚

 so let's go a little bit deeperã€‚Wellï¼Œ firstï¼Œ of courseï¼Œ you knowã€‚

 why do we even care about risks is because there are benefitsï¼Œ General perI could be very usefulã€‚

 could be applied for you many great applicationsã€‚But only if we govern it properlyã€‚

 because there are risksã€‚And we've considered three categories of riskã€‚

 We thought a lot about how to organize this report in different categoriesã€‚

So there's malicious risksã€‚Risks from malfunctions and systemic risksã€‚ Okayã€‚

 so I'm going to explain eachã€‚ So malicious risks are easy to understandã€‚

 So people use AI to do something badï¼Œ something illegalï¼Œ something immoralã€‚You knowï¼Œ scamsã€‚

 deep fakes disinformationï¼Œ cyber attacksï¼Œ bioweaponsï¼Œ and so onã€‚Then there's malfunctionsã€‚

 So unintended negative consequencesï¼Œ product safety issuesï¼Œ just like any0 consumer productã€‚

 bias and discriminationï¼Œ things that are well studied now and loss of controlã€‚

 people usually don't want to lose controlï¼Œ but you knowï¼Œ that could be an accidentã€‚It's a mouthã€‚

 It's a kind of malfunctionã€‚ Of courseï¼Œ it's a serious malfunctionã€‚And then you have systemic riskã€‚

 So what is thatï¼Œ Wellï¼Œ things that involve society and the technology togetherã€‚

So the effect on the labor marketï¼Œ the fact that as we automate more and more jobsã€‚

The value of the human labor for those jobs wellï¼Œ decreases the same job could be done for11 times less moneyã€‚

 Wellï¼Œ and the the value of that work decreases because you can you can do it for cheaperã€‚

So what happens to the people who lose their jobsï¼Œ I meanï¼Œ right nowã€‚

 it doesn't seem to be a problem in the worldï¼Œ but it may become oneã€‚

Another systemic risk is what's called the AI divideã€‚

 the fact that the talent and capabilities in AI are concentrated in a few countriesã€‚

 What happens with the other countriesï¼Œ is the kind of concentration of power in a few countriesã€‚

 Does it mean that the benefits of AI are going to be concentratedã€‚

 Does it mean that AI is going to be developed in a direction that might be good for some countriesã€‚

 but maybe not for the global Southï¼Œ for exampleã€‚And another kind of concentration is power is market concentration thatã€‚

The capital requirements for training the state of the arts in these AI systemsã€‚

 these general propI systemsã€‚The cost of training them is increasing exponentially as we realize that we can train bigger models and they get better and betterã€‚

 so that's the so called scaling lawsã€‚But that also means that very few players will have the capital to train the future generations of these AI systemsã€‚

Andï¼Œ you knowï¼Œ what does it mean forã€‚The efficiency of the marketsã€‚

 What does it mean for the abuse of that market concentration in ways that are not good for society and and the economyã€‚

Another systemic risk that has to do with the interaction of AI with society is environmental impactsã€‚

 So I just said thatã€‚The cost of training these systems increasesã€‚ But it's becauseï¼Œ you knowã€‚

 we train bigger systems and that require more and more energyã€‚

 The amount of energy required is increasing exponentiallyã€‚ that can't go on forever pretty soonã€‚

The cost of trainingï¼Œ the AI system is going toï¼Œ you knowã€‚

 put huge pressure on the total energy demandã€‚ It might be something like 10% in a few yearsã€‚Soã€‚

 we have to like see this coming and think about itã€‚Of courseã€‚

 other more like social things that have to do with AIã€‚

 like privacy and copyright are covered in the reportã€‚Okayã€‚

 the report also talks about transversal risksï¼Œ we call societal risk factors so things like the fact that regulation takes time to put in place and even if you have the legislationã€‚

 it might take years to put in place the regulator's group and figure out you know what is the regulator going to ask to corporationsã€‚

And when the technology changesï¼Œ it might take time to adapt and so onã€‚

And then other risk factors have to do with the concentration of power in in a few hands that that we have to be careful aboutã€‚

Okayï¼Œ regarding the technical methods to reduce risks I'll talk more aboutã€‚

 we talk a lot about what existsï¼Œ but right now there's nothing really satisfyingã€‚

 they all have limitations that I'll talk more aboutã€‚And soï¼Œ knowï¼Œ one of the main conclusions isã€‚

As we continue increasing the capabilities of AIï¼Œ we really need to invest moreã€‚

 not just in making the AI more capableï¼Œ you knowï¼Œ more applicable and so onã€‚

 but also to develop a better understanding of these systems so that we can mitigate risks and know better where we're goingã€‚

Okayï¼Œ so now let's talk about the international diversity of use about those risksã€‚

There's a lot of debatesï¼Œ as you probably realized in the media or talking to people or looking on social mediaã€‚

Andï¼Œ in particularã€‚Theres it a wideã€‚Difference between people who think there is no risk and people who think the risk can be catastrophicã€‚

But it's interesting to look at why where do these differences come fromï¼ŸAlsoã€‚

 you have to realize that the risks have to do withã€‚Things in the futureï¼Œ for the most partã€‚

 the most likeã€‚High impact risks are regarding systems that don't exist yetã€‚ And soï¼Œ of courseã€‚

 people don't have a crystal ball and know what the future will holdã€‚And so reallyã€‚

 it's about what sort of AI will we have in a few years or a few decadesã€‚

While we know the trends are very clear that the capabilities of the eye continue to advance and they advance rapidlyã€‚

So these different views mean that people disagree on things like the effect of AI on labor marketsã€‚

Or the effect of AI on cyber attacks or on biological weapon attacks or on loss of controlã€‚

But what we found is that the differences in those views is best explainedã€‚

By the differences in how people thinkï¼Œ how fast people think that AI will become more capableã€‚

There are alsoï¼Œ you knowï¼Œ different expectations about what society will do in order to mitigate those risks and the effectiveness ofã€‚

Things like regulation and treatiesã€‚So like the central thing is the that explains the differencesã€‚

 as I saidï¼Œ is like the speed of future progress and it is uncertainã€‚ Nowã€‚

 when we say something is uncertainï¼Œ something like this that's very importantã€‚

 that's going to change the futureã€‚From the point of view of policymakersã€‚Wellã€‚

 if the scientist is not agree on the speed of the progress that will come in the futureã€‚

Policymakers needs to bite that bullet and say and be prepared for all the casesï¼Œ you knowã€‚

 maybe AGI will be there in three years as some people sayï¼Œ or maybe it's going to be 30 yearsã€‚

We need to be ready for all of these options in terms of policyã€‚è¯¶ã€‚Yeahï¼Œ okayã€‚

 so now let's go a little bit more into the risk reduction methodsã€‚And their limitationsã€‚So there'sã€‚

 there's already a number of technical methods to both assess and reduce the risks of general purpose AIã€‚

 So evaluating the risk is one thingã€‚ So is there a problemï¼Œ Is the AIã€‚

Able to do something dangerousï¼Œ for exampleã€‚That's one kind of questionã€‚

 and we can use that to reduce the risks because if we detect something dangerousã€‚

Then we might just stop andï¼Œ you knowï¼Œ not deploy this thing or not even continue training itã€‚

Reducing the risk is a different thingã€‚ how do we change the methodsã€‚

The AI system itself to prevent it from doing bad thingsã€‚Maybe being used to harm peopleã€‚

 for exampleã€‚And it is important that we develop standards for both of these thingsã€‚

 because the regulatorsã€‚Should be requiring the of the best methods that exist both for assessing and reducing the risksã€‚

Nowï¼Œ unfortunatelyï¼Œ the methods that exist currently to both assess and reduce the risks have limitationsã€‚

 and it is important that a regulator understand these limitationsã€‚Soï¼Œ for exampleã€‚

 we don't understandã€‚How the current AI modelsï¼Œ which are neural networksã€‚How they make a decisionã€‚

 how they come to a particular albumï¼Œ We understand the code that says how they are trainedã€‚

 but but then once they're trainedï¼Œ it's likeï¼Œ you knowã€‚

 you have a child and maybe we understand the biology of how people grow and are bornã€‚

 but we don't really understand the details of how you know why they do somethingã€‚

And it's kind of similar hereã€‚That's a problem because when the AI misbehavesã€‚

 we might not see it coming or know why and know how to fix itã€‚

The other problem is the current methods that exist for safety protection are easy to removeã€‚

 like we using jail brakeï¼Œ and especially if you're able to do fine tuningã€‚ So if theã€‚

Weights of the system are availableï¼Œ then it's very easy to remove all the safety protectionsã€‚Okayã€‚

 so on the assessing the way that it works now and why it's not great is that we basically ask it many different questions and see if it's going to answer with something badã€‚

 So we just test it on various casesã€‚But these are spot checksã€‚

 and we cannot check every possible questionã€‚And it's useful because if we detect somethingã€‚

 then we know there's a problemã€‚ But there's no quantitative guarantee of safetyã€‚

 And if the spot checks didn't find anythingï¼Œ then it doesn't mean that there is nothingã€‚

 We only have something to say when we find a problemã€‚ If we don't find any problemã€‚

 it might still be a problemã€‚Okayï¼Œ so you knowï¼Œ we shouldn't be just looking at the train goingã€‚

 I think that we still have a few years and there are many things that governments can do in order to mitigate those risksã€‚

We need to better understand how these systems workï¼Œ as I saidã€‚And to think carefully aboutã€‚

 you knowï¼Œ why and how we're going to be developing AIã€‚Is it going to beï¼Œ for exampleã€‚

 applied in a way that's going to favor attackers or defenders in the conflictã€‚

 think about cyber attacksï¼Œ thereï¼Œ there are variations of the use of AIã€‚

 which could favor more the attacker or the defenderã€‚And you knowã€‚

 how are we going to reap the economic benefits of AI is something we can choose collectively as wellã€‚

 who's going to benefit from itã€‚And how much we invest in research to mitigate the risksã€‚Okayã€‚

 so this is an interim report that we delivered last month at Seul and the Seul AI Forumã€‚

 please have a lookï¼Œ send me feedback we are going to be working on the larger report for the end of this yearã€‚

Nowï¼Œ my thought about the big pictureï¼Œ the report talks a lot about the uncertainty on the time language could be extremely rapidã€‚

 we see here different benchmarks over timeï¼Œ years on the X axis and performance on the Y axis and the black plan is human performanceã€‚

 you see on manyï¼Œ many tasks as time passes you know we approach human performance and often you know get betterã€‚

So we need to learn to manage those risks as we discussed in our paper with many Chinese colleagues that came out this year managing extreme AI risks amid rapid progress in scienceã€‚

So what should we doï¼Œ I think in generalï¼Œ we are not well prepared for these kinds of very rapid changesã€‚

 for exampleï¼Œ think about what happened with COVIDã€‚

And so we need to start working on preparedness right nowã€‚

 we need to think about international agreements on the regulation on risk thresholdsã€‚

 especially for the catastrophic riskï¼Œ like laws of control and misuse we need to do more researchã€‚

 as I saidï¼Œ to better understand the risks and the capabilities and their consequencesã€‚Andã€‚

 and construct plansã€‚ you knowï¼Œ what happens if we detect a riskï¼Œ you knowï¼Œ one of these thresholdsã€‚

 red lines is crossedã€‚ We need to like commit right now about what we'll do if we detect such problemsã€‚

And in generalï¼Œ we need to apply the precautionary principle when there' is uncertainty about large risksã€‚

Okayã€‚One thing that is important is how we invest in different solutions in terms of safetyã€‚

We should think of a portfolio of projectsï¼Œ research projects to try to improve evaluation and how to make safer AIsã€‚

 for exampleï¼Œ a number of researchersï¼Œ I meanï¼Œ looking into what's called safe by design As where we can get wanted to do guaranteesã€‚

 but some methods may providing stronger guarantees and take more time to develop and some methods might you know be easier and to do maybe maybe not a safeã€‚

 but if AGI happens in three yearsï¼Œ we need to be ready in three yearsã€‚

 So we need all of these projects to run in parallelilã€‚Finallyã€‚

 there is the question of the competition between developers as talked about in the reportã€‚

 but also between countries and so there's really a danger of an arm race between different countries and AI could become a weaponã€‚

 something used in the military starting with cyber but potentially in other weapons and so we really need to start right now on international discussions to cooperate on safety but also to think about if there's going to be a treaty how do we make sure we get compliance verifiability and we need to develop technology for thisã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_3.png)

Thank youã€‚

![](img/d8ed84c386efa59b34052632c3a4a717_5.png)

Thank you so muchï¼Œ Professor Benjoï¼Œ for sharing your insights and will' let you continue on for the rest of your eveningã€‚

 It was an honor to have youã€‚ğŸ˜Šï¼ŒOur next speaker is Professor Gaowenã€‚

 who is a member of the Chinese Academy of Engineeringï¼Œ AM Fellow and I E fellowellowã€‚

He is the founding director of Penongcheng Laboratoryã€‚

As well as the Boyy chair professor and director at the Faculty of Information and Engineering Sciences at Peking Universityã€‚

Professor Gale is currently a deputy to the 14th National People's Congressã€‚

And he used to be a member of the 10thï¼Œ11th and 12th CP's PCCC National Committeeã€‚

The vice president of the National Natural Science Foundation of Chinaã€‚

And chairman of the China Computer Federation and the chief editor of the Chinese Journal of Computersã€‚

Professor Gallï¼Œ it's an honor to have you here with usã€‚ I'll hand it over to youã€‚å•Šï¼Œéå¸¸é«˜å…´è¿™ä¸ªã€‚å‘ƒï¼Œæ¥è¿™é‡Œå‚åŠ è¿™ä¸ªä¼šã€‚

éå¸¸æ„Ÿè°¢æˆ‘ä»¬ç»„ç»‡è€…å‘ƒã€‚è®©æˆ‘ä»¬å¤§å®¶æœ‰æœºä¼šæä¾›è¿™æ ·ä¸€ä¸ªå¹³å°ï¼Œè®¨è®ºäººå·¥æ™ºèƒ½å®‰å…¨çš„é—®é¢˜ã€‚å‘ƒï¼Œå…¶å®å‘¢æ˜¨å¤©ä¸Šåˆçš„å¼€å¹•å¼å‘¢å‘ƒå¦‚æœå‚åŠ çš„ä¸“å®¶å¯èƒ½ä¹Ÿèƒ½å¤Ÿå¬å¾—åˆ°å‘ƒï¼Œå‚ä½å›¾åˆ©å¥–å›¾çµå¥–å¾—ä¸»ã€‚åœ¨è°ˆè¿™ä¸ªäººå·¥æ™ºèƒ½å®‰å…¨çš„æ—¶å€™ã€‚

å…¶å®è¿˜æ˜¯æœ‰ä¸€äº›å·®å¼‚çš„ã€‚å‘ƒï¼Œæ¯”å¦‚è¯´ã€‚å‘ƒï¼Œç¬¬ä¸€ä½å›¾çµå¥–çš„å¾—ä¸»rajå‘¢ã€‚å‘ƒï¼Œä»–å°±è¯´ç°åœ¨äººå·¥æ™ºèƒ½æœ‰å¤ªå¤šçš„é—®é¢˜éœ€è¦å»è§£å†³äº†ã€‚å‘ƒï¼Œå¦‚æœä½ è¦æœ‰é‚£åŠŸå¤«ï¼Œä½ è¿˜æ˜¯å…ˆè§£å†³ç‚¹é—®é¢˜ï¼Œå…ˆåˆ«è¯´è¿™ä¸ªå‘ƒæ€ä¹ˆæ ·ä¿è¯å®‰å…¨ã€‚

ä½ å…ˆæŠŠäººå·¥æ™ºèƒ½æœ¬èº«æ²¡æœ‰è§£å†³çš„é—®é¢˜è§£å†³è§£å†³ã€‚å‘ƒï¼Œæˆ‘ä»¬ç¬¬äºŒä½è¿™ä¸ªã€‚å‘ƒï¼Œå¾—å‡ºmy newé—®å°±æ¯”è¾ƒæœ‰æ„æ€ã€‚ä»–å°±è¯´è¿™ä¸ªäººå·¥æ™ºèƒ½åªè¦æ˜¯ç¡®å®šçš„è¿™ä»¶äº‹å…¶å®éƒ½æ˜¯å¯æ§çš„ï¼Œä¸ç¡®å®šå°±ä¸è¡Œã€‚é‚£ä¹ˆç¬¬ä¸‰ä½å‘¢ï¼Œå®‰è¿ªå°±ã€‚

å’Œåˆšæ‰çš„æœ¬å°±æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸ºä»–ä»¬ä¸€èµ·å’Œé›…ç´å‘ƒä¸€èµ·å†™çš„å‘ƒä¸€äº›æ–‡ç« ï¼Œåœ¨æ•´ç†è¿™äº›è¿™äº›å‘ƒæŠ¥å‘Šï¼Œå°±æ˜¯è¦è®©æˆ‘ä»¬ä¸€å®šè¦é‡è§†ã€‚é‚£ï¼Œä»Šå¤©æˆ‘è®²çš„å‘¢æ˜¯ã€‚å…¶å®äººå·¥æ™ºèƒ½çš„è¿™ä¸ªå®‰å…¨æ€§å•Šï¼Œç¡®å®æ˜¯å‘ƒå®ƒæ˜¯ã€‚é—®é¢˜çš„ä¸¤ä¸ªæ–¹é¢ã€‚å‘ƒã€‚

ä¸€ä¸ªæ–¹é¢å‘¢å°±æ˜¯è¯´ä½œä¸ºæŠ€æœ¯ç ”ç©¶ï¼Œä½ å¿…é¡»è¦æŠŠè¿™ä¸ªæŠ€æœ¯æœ¬èº«è¦åšåˆ°æè‡´ï¼Œè®©å®ƒæœ‰ç”¨ã€‚å•Šï¼Œå½“ç„¶ä½œä¸ºå‘ƒç¤¾ä¼šå­¦å®¶å•Šï¼Œä½ æ›´å¤šçš„è¦è€ƒè™‘è¿™æ ·ä¸€é¡¹æŠ€æœ¯ï¼Œå®ƒå¯¹ç¤¾ä¼šå¸¦æ¥çš„å½±å“åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿé‚£ä¹ˆå¦‚æœè¿™ä¸ªå½±å“æœ‰è´Ÿé¢çš„ã€‚

ä½ æœ‰ä»€ä¹ˆåŠæ³•æŠŠå®ƒæ§åˆ¶ä½ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜çš„ä¸¤ä¸ªæ–¹é¢ã€‚æˆ‘æƒ³å‘ƒå› ä¸ºæˆ‘ä»¬è¿™ä¸ªç¤¾ä¼šå‘¢åœ¨å‘å±•çš„æ—¶å€™éœ€è¦æ‰€æœ‰çš„äººçš„å…³æ³¨ã€‚å½“ç„¶ï¼Œæ‰€æœ‰äººå…³æ³¨ï¼Œä¸æ˜¯æ‰€æœ‰äººéƒ½åšåŒä¸€ä»¶äº‹å„¿ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æœ‰å¾ˆå¥½çš„åˆ†å·¥ã€‚

ä»Šå¤©æˆ‘ä»¬å°±è®²ä¸€è®²è¿™ä¸ªåˆ†å·¥çš„é—®é¢˜ã€‚å‘ƒï¼ŒAIå…¶å®æˆ‘ä»¬çŸ¥é“è¿™ä¸ªå‘ƒã€‚ä»–ç¡®å®æ˜¯å¾ˆå¼ºå¤§å•Šï¼Œé€šç”¨äººå·¥æ™ºèƒ½ã€‚å‘ƒï¼Œè¿™ä¸ªå¼ºå¤§äº†ä»¥åå‘¢ï¼Œå‘ƒæˆ‘ä»¬å°±è¦è®©ä»–å‘å–„ï¼Œå°±æ˜¯è®©ä»–åšä»–åº”è¯¥å‘ƒæ¯”è¾ƒç†æƒ³çš„äº‹ï¼Œåšçš„æ¯”è¾ƒç†æƒ³çš„äº‹æƒ…ã€‚

æ‰€ä»¥å‘¢æˆ‘ä»¬è¯´è¿™ä¸ªAIå‘å–„é‡Œé¢å‘¢å‘ƒæœ€ä¸»è¦çš„å‘¢æˆ‘ä»¬è¦ä»ä¸¤ä¸ªè§’åº¦ï¼Œä»æŠ€æœ¯çš„è§’åº¦å‘¢ï¼Œè¦æŠŠäººå·¥æ™ºèƒ½æŠ€æœ¯æœ¬èº«è¦åšçš„è¶³å¤Ÿå¥½ã€‚ä»Šå¤©çš„äººå·¥æ™ºèƒ½å‘¢ç¡®å®è¿˜æœ‰å¾ˆå¤šé—®é¢˜ã€‚å‘ƒï¼Œæ‰€ä»¥æŒ‰ç°åœ¨çš„è¿™ä¸ªæ°´å¹³å‘ƒï¼Œä»–è¿˜æ²¡æœ‰åŠæ³•å‘å–„ã€‚

ç¬¬äºŒä¸ªå‘¢å°±æ˜¯ä»ä¼¦ç†çš„è§’åº¦è§’åº¦ï¼Œä½ å¿…é¡»å‘¢å•Šè¦åœ¨ä¼¦ç†ã€é“å¾·ç­‰æ–¹é¢ç»™å®ƒè§„èŒƒå¥½ã€‚æ‰€ä»¥æˆ‘æƒ³è¿™æ˜¯å‘ƒAIå‘å±±é‡Œé¢æ¯”è¾ƒè¿™ä¸ªå‘ƒéœ€è¦å…³æ³¨çš„ä¸¤ä¸ªæ–¹é¢ã€‚é‚£ä»Šå¤©çš„AIæŠ€æœ¯æ˜¯ä¸æ˜¯è¶³å¤Ÿå¥½äº†å‘¢ï¼Ÿå‘ƒï¼Œåˆšæ‰è¯´äº†ã€‚

å…¶å®ä¸æ˜¯æˆ‘ä»¬ç°åœ¨AIçš„æ°´å¹³å‘¢å‘ƒè¿˜ä¸å¤Ÿé«˜ã€‚ä½œä¸ºå—¯å› ä¸ºåˆšæ‰å‘ƒç­ä¸»ä¹Ÿåœ¨ä»–çš„æœ€åçš„è¿™ä¸ªsè±é‡Œé¢å‘ƒï¼Œå…¶å®ä¹Ÿæåˆ°äº†ï¼Œå°±æ˜¯åœ¨ä»–æ€»ç»“ä¹‹å‰é‚£ä¸ªsè±é‡Œé¢ä¹Ÿæåˆ°äº†å‘ƒï¼Œä½œä¸ºå•å‘æ€§èƒ½ã€‚å•Šï¼Œæœ‰ä¸€äº›ã€‚AIå·²ç»è¶…è¿‡äººäº†ã€‚å‘ƒï¼Œå‘ƒã€‚

æœ‰ä¸€äº›è¿˜æ˜¯ä¸è¡Œã€‚å‘ƒï¼Œä»€ä¹ˆæ—¶å€™å‡ ä¹æ‰€æœ‰çš„æ€§èƒ½ã€‚éƒ½è¶…è¿‡äººçš„æ—¶å€™ï¼Œé‚£ä¸ªå°±æ˜¯æ¯”è¾ƒå¥½äº†ï¼Œå°±å¯ä»¥çœŸæ­£å‘æŒ¥ä½œç”¨ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ä»Šå¤©çš„è¿™ä¸ªå‘ƒAIæŠ€æœ¯å‘¢ï¼Œæˆ‘ä»¬è®¤ä¸ºå®ƒæ›´è¡¨ç°çš„æ›´å¤šçš„å‘¢æ˜¯ä¸€ç§ä½æ°´å¹³çš„æ™ºèƒ½ã€‚ä»€ä¹ˆå«ä½æ°´å¹³çš„æ™ºèƒ½å‘¢ï¼Ÿ

å°±æ˜¯æ­»è®°ç¡¬èƒŒçš„æ™ºèƒ½ï¼Œå°±æ˜¯é æ˜¾ç¤ºçŸ¥è¯†çš„è®°å¿†å’Œä½¿ç”¨ã€‚å‘ƒï¼Œé‚£ä¹ˆæ¥è¡¨ç°å‡ºæ¥çš„æ™ºèƒ½ã€‚çœŸæ­£å¥½ä¸€ç‚¹çš„æ™ºèƒ½å‘¢ï¼Œå…¶å®æ˜¯ã€‚ä¸­æ°´å¹³çš„æ™ºèƒ½æ˜¯ä¸­ç­‰çš„ï¼Œé«˜æ°´å¹³çš„æ™ºèƒ½æ˜¯æœ€ç†æƒ³çš„ã€‚

æˆ‘ä»¬ç°åœ¨å…¶å®åšé«˜æ°´å¹³çš„è¿½æ±‚é«˜æ°´å¹³çš„æ™ºèƒ½è¿˜æœ‰ç‚¹å‘ƒé‚£æ˜¯éå¸¸é¥è¿œçš„äº‹ã€‚æˆ‘ä»¬è¦è¿½æ±‚è¿½æ±‚ä¸­æ°´å¹³çš„æ™ºèƒ½ã€‚æ‰€è°“æœ€ä¸­æ°´å¹³çš„æ™ºèƒ½å‘¢ï¼Œå°±æ˜¯ç”¨æ¯”è¾ƒå°‘é‡çš„æ˜¾ç¤ºçŸ¥è¯†å°±å¯ä»¥è·å¾—çš„æ™ºèƒ½å•Šï¼Œç”¨æˆ‘ä»¬äººç±»çš„è¿™ä¸ªå­¦ä¹ èƒ½åŠ›æ¥è¯´ã€‚

ä½ æœ‰éå¸¸å¼ºçš„ä¸¾ä¸€åä¸‰çš„èƒ½åŠ›ã€‚è€Œç°åœ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæ˜¯æ²¡æœ‰è¿™ä¸ªèƒ½åŠ›çš„ã€‚æ‰€ä»¥æˆ‘æƒ³å•Šæˆ‘ä»¬ç°åœ¨å‘¢å¯èƒ½å½“å‰æ˜¯åœ¨ä½æ°´å¹³æ™ºèƒ½ï¼ŒæŸäº›å•çº¿è¿˜å¯ä»¥é æ­»è®°ç¡¬èƒŒï¼Œæˆ–è€…æ˜¯é æ•°æ®è®­ç»ƒå‡ºæ¥çš„ã€‚

é‚£ä¹ˆç­‰åˆ°äº†åªæœ‰å°‘é‡çš„æ ·æœ¬å°±å¯ä»¥è®­ç»ƒå‡ºæ™ºèƒ½çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¤§æ¦‚å°±åˆ°äº†ä¸€ä¸ªä¸­æ°´å¹³çš„æ™ºèƒ½ã€‚è€Œä¸”å®ƒå¯ä»¥å‘ƒè·¨è¶Šå‘ƒé¢†åŸŸä»ä¸€ä¸ªé¢†åŸŸå¯ä»¥å¾ˆå®¹æ˜“å°±ç±»æ¨åˆ°å¦å¤–ä¸€äº›é¢†åŸŸï¼Œå°±åƒä»¥å‰ææœºå™¨å­¦ä¹ è¿™ä¸ªç±»æ¯”æ¨ç†ã€‚ä½ èƒ½åšåˆ°ã€‚

é‚£æ˜¯ä¸­æ°´å¹³çš„æ™ºèƒ½ã€‚é«˜æ°´å¹³çš„æ™ºèƒ½çš„è¿™ä¸ªæˆ‘ä»¬å°±å‘ƒå¯ä»¥æŠŠå®ƒç¬‘ä¸€ç¬‘å¬ä¸€å¬ã€‚å› ä¸ºè¿™ä¸ªé«˜æ°´å¹³æ™ºèƒ½ç›¸å½“äºè¯´å°±åƒäººç±»é‡Œé¢ä¹Ÿæ²¡æœ‰å¤šå°‘äººèƒ½è¾¾åˆ°çš„é‚£ä¸ªæ™ºèƒ½ã€‚ä½ è®©è®¡ç®—æœºè¿™ä¸ªç³»ç»Ÿå»åšï¼Œé‚£æ˜¯éå¸¸é¥è¿œçš„äº‹æƒ…äº†ã€‚

é‚£ä¹ˆå‘ƒä½æ°´å¹³æ™ºèƒ½é‡Œé¢å‘¢å‘ƒå…¶å®å‘ƒæœ‰ä¸€ä¸ªå‘ƒæœ‰ç‚¹åƒæ‚–è®ºä¸€æ ·çš„å‘ƒæƒ…å†µã€‚å¾ˆå¤šäººå°±è¯´å› ä¸ºæœ‰æ—¶å€™è®¨è®ºé—®é¢˜ï¼Œè¯´æ—¢ç„¶ä½ è¯´ç°åœ¨çš„æ™ºèƒ½æ˜¯ä½æ°´å¹³çš„æ™ºèƒ½ï¼Œå®ƒä¸ºä»€ä¹ˆä¼šæœ‰æ™ºèƒ½æ¶Œç°ï¼Ÿä½æ°´å¹³æ™ºèƒ½æ˜¯ä¸åº”è¯¥æœ‰æ™ºèƒ½æœ‰é™çš„ã€‚å‘ƒã€‚

å…¶å®ä½æ°´å¹³æ™ºèƒ½ä¹Ÿå¯ä»¥æœ‰æ™ºèƒ½æœ‰å…ˆï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿå‘ƒï¼Œæˆ‘ä»¬å¯ä»¥å‘ƒæ¢ä¸ªè§’åº¦æ¥è€ƒè™‘æˆ‘ä»¬ç°åœ¨çš„æ™ºèƒ½å‘¢æ˜¯ç”¨æ•°æ®è®­ç»ƒå‡ºæ¥çš„ã€‚æ¯”å¦‚æ¯”å¦‚è¯´æˆ‘ä»¬å¤§è¯­è¨€æ¨¡å‹ã€‚å¤§è¯­è¨€æ¨¡å‹å‘¢æ˜¯ç”¨ä¸åŒç§çš„è¯­è¨€ä¸€èµ·æ¥è®­ç»ƒå‡ºæ¥çš„ä¸€ä¸ªæ¨¡å‹ã€‚

ä½†æ˜¯æˆ‘ä»¬æ¯ä¸ªäººçš„æ¯è¯­å‘¢å¤§æ¦‚åªæ˜¯ä¸€ç§è¯­è¨€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯èƒ½æˆ‘ä»¬Aç†Ÿæ‚‰çš„æ˜¯è¿™ä¸ªä¸­æ–‡ã€‚ä»–å­¦æ‰€æœ‰çš„å­¦ä¹ çš„ç†Ÿç»ƒéƒ½æ˜¯ä¸­æ–‡é‡Œé¢çš„ä¸œè¥¿ã€‚æ‰€ä»¥å‘¢ä½ ç”¨ä¸­æ–‡è®­ç»ƒè¯»å‡ºæ¥çš„ä¸œè¥¿ï¼Œå¯¹ä»–æ¥è®²å‘¢å‘ƒä»–èƒ½åˆ¤æ–­è¿™ä¸ªä¸œè¥¿å¥½ä¸åå‡†ç¡®ä¸å‡†ç¡®ã€‚

æˆ–è€…æ˜¯åŸºæœ¬ä¸Šéƒ½æ˜¯ä»–å¯ä»¥æŒæ¡çš„ã€‚ä½†æ˜¯å‘¢å¦‚æœè¿™ä¸ªè¯­æ–™å‘¢æ˜¯ç”¨è¥¿ç­ç‰™è¯­ã€‚å»è®­ç»ƒï¼Œå½“ç„¶æ··åˆåœ¨ä¸€èµ·è®­ç»ƒäº†ã€‚åœ¨æ–°ç­ç‰™è¯­çš„å®ƒçš„èƒŒæ™¯åœºæ™¯é‡Œé¢çš„ä¸œè¥¿å‘¢ï¼Œå…¶å®æ˜¯å­¦ä¸­æ–‡çš„äººå‘¢ï¼Œä»–å¯èƒ½ä¸ç†Ÿæ‚‰ã€‚é‚£æ‰€è°“æ¶Œç°å‘¢ã€‚

å°±æ˜¯å½“ä½ æŠŠæ‰€æœ‰è¿™äº›è¯­æ–™éƒ½æ”¾åœ¨ä¸€èµ·ã€‚å»è®­ç»ƒçš„æ—¶å€™ï¼Œä»–ä¼šä½¿å¾—è¿™ä¸ªä½¿ç”¨è€…çªç„¶å‘ç°æœ‰ä¸€äº›ä¸œè¥¿ï¼Œä»–æ ¹æœ¬å°±ä¸çŸ¥é“ã€‚å“ï¼Œä»–è®¤ä¸ºè¿™å°±æ˜¯ä»¤çœ¼ç›ä¸€äº®ã€‚å•Šï¼Œå…¶å®é‚£ä¸ªçŸ¥è¯†å¯¹é‚£ä¸ªè¡Œä¸šçš„äººæˆ–å¯¹é‚£è¯­ç§çš„äººï¼Œå¤§æ¦‚ä¸æ˜¯ä»€ä¹ˆäº†ä¸å¾—çš„ä¸œè¥¿ã€‚

ä½†æ˜¯å‘¢å¯¹äºä¸æ˜¯æ¯è¯­çš„äººï¼Œä»–å°±è§‰å¾—å¾ˆåƒæƒŠã€‚æ‰€ä»¥æˆ‘æƒ³è¿™ä¸ªæ¶Œç°æ›´å¤šçš„æˆ‘ä»¬å¯ä»¥ç”¨è¿™ç§è§’åº¦è§£é‡Šã€‚å½“ç„¶ä¹Ÿè®¸æ·±å±‚æ¬¡è¿˜æœ‰æ›´æ·±çš„è§£é‡Šï¼Œæˆ‘ä»¬å¤§å®¶éƒ½å¯ä»¥å»è€ƒè™‘è¿™äº›é—®é¢˜ã€‚é‚£ç›®å‰çš„äººå·¥æ™ºèƒ½å‘¢ï¼Œæˆ‘ä»¬è¯´å‘ƒä¸ç®¡æ˜¯åœ¨æ™ºèƒ½æ°´å¹³ä¸Šã€‚

åœ¨æŠ€æœ¯ä¸Šï¼Œåœ¨è¿™ä¸ªå½¢æ€ä¸Šï¼Œåœ¨åº”ç”¨ä¸Šï¼Œç”šè‡³åœ¨ç¤¾ä¼šå±æ€§ä¸Šé¢éƒ½å·²ç»è¿›å±•çš„æ¯”è¾ƒå¿«ã€‚ç‰¹åˆ«æˆ‘ä»¬è®²åˆ°ä¼¦ç†é—®é¢˜å¿…é¡»è¦è€ƒè™‘å®ƒçš„ç¤¾ä¼šå±æ€§ã€‚å‘ƒï¼Œé‚£ä¹ˆè®²åˆ°ç¤¾ä¼šå±æ€§ä¹Ÿå¿…é¡»è¯´å‘ƒç°åœ¨äººå·¥æ™ºèƒ½çš„è¿™ä¸ªå®‰å…¨æˆ–è€…äººå·¥æ™ºèƒ½çš„å¸¦æ¥é£é™©ã€‚

é‚£è‚¯å®šå°±æ˜¯è¯´å‘ƒä¸€æ–¹é¢æ˜¯çŠ¯ç½ªä¸€æ–¹é¢å…¶ä»–è¿™å°±è‚¯å®šæ˜¯æ—©æ™šçš„é—®é¢˜ã€‚å‘ƒï¼Œé‚£ä¹ˆå¦å¤–è¿™ä¸ªæ•´ä¸ªäººå·¥æ™ºèƒ½çš„å‘å±•å‘¢ï¼Œå®ƒå¯èƒ½å®ƒä¼šå½±å“çš„å‘ƒè¿™ä¸ªå±‚é¢å‘¢å‘ƒè¿™ä¸ªå¯ä»¥åœ¨äººçš„å±‚é¢ï¼Œæ¨¡æ€§çš„å±‚é¢å’Œæ•°æ®çš„å±‚é¢ï¼Œè¿™ä¸‰ä¸ªå±‚é¢æ¥è€ƒè™‘ã€‚

å½“ç„¶æ›´æ£˜æ‰‹çš„ä¸€äº›é—®é¢˜å‘¢ï¼Œå°±æ˜¯å¦‚æœäººå·¥æ™ºèƒ½ï¼Œå¯¹ç¤¾ä¼šäº§ç”Ÿæ”»å‡»ã€‚é‚£ä¹ˆæˆ‘ä»¬æ€ä¹ˆæ ·å‘ƒè¿™ä¸ªé˜²æ­¢è¿™ç§æŠ€æœ¯è¢«å„ç”¨ï¼Œå¯¹ç¤¾ä¼šäº§ç”Ÿå½±å“ã€‚å‘ƒï¼Œæ‰€ä»¥å‘¢è¿™ä¸ªæœ€è®²ã€‚ç®€å•çš„å‘¢å°±æ˜¯è¯´è¿™ä¸ªæˆ‘ä»¬è¦ä»ä¼¦ç†å’ŒæŠ€æœ¯ä¸¤ä¸ªæ–¹é¢å‘ƒå»ç€æ‰‹è§£å†³è¿™äº›é—®é¢˜ã€‚

é‚£ä¹ˆé’ˆå¯¹è¿™ä¸ªé—®é¢˜å‘¢ï¼Œå…¶å®å‘ƒä¸­å›½å·¥ç¨‹é™¢å‘¢å‘ƒå‰äº›å¹´å‘¢ä¸“é—¨éƒ¨ç½²äº†ä¸€ä¸ªäººå·¥æ™ºèƒ½æ–¹é¢çš„é‡å¤§å’¨è¯¢é¡¹ç›®ï¼Œå«åšæ–°ä¸€ä»£äººå·¥æ™ºèƒ½å®‰å…¨ä¸è‡ªä¸»å¯æ§å‘å±•ç ”ç©¶ã€‚é‚£è¿™ä¸ªé‡å¤§é¡¹ç›®é‡Œé¢ï¼Œè¯¾é¢˜ä¹å‘¢æ˜¯å‘ƒæˆ‘é¢†äº†ä¸€æ‰¹ä¸“å®¶åœ¨ä¸€èµ·åšçš„ã€‚å‘ƒã€‚

é‚£ä¸ªå‘ƒç ”ç©¶çš„é—®é¢˜æ˜¯å¼ºäººå·¥æ™ºèƒ½ä¸å†…è„‘è®¡ç®—æŠ€æœ¯çº¿åŠå®‰å…¨å¯¹ç­–ã€‚å‘ƒï¼Œè¿™ä¸ªå‘¢å‘ƒå¤§æ¦‚æ˜¯åœ¨19å¹´å¼€å§‹ç ”ç©¶çš„2021å¹´å‘¢ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªä¸œè¥¿ç ”ç©¶å‡ºæ¥å‘¢å‘ƒå†™äº†ä¸€ç¯‡æ–‡ç« å‘ƒå‘è¡¨åœ¨ä¸­å›½å·¥ç¨‹ç§‘å­¦ä¸Šé¢ã€‚å‘ƒï¼Œå³é¢å°±æ˜¯è¿™ç¯‡æ–‡ç« å‘ƒã€‚è¿™ä¸ªã€‚

é¦–é¡µå‘ƒï¼Œæ‰€ä»¥å¤§å®¶å¦‚æœå‘ƒæœ‰å…´è¶£ï¼Œå½“ç„¶è¿™æ˜¯ä¸­æ–‡çš„äº†å•Šï¼Œè¯»å¾—äº†ä¸­æ–‡çš„å¯ä»¥çœ‹ä¸€çœ‹ã€‚å‘ƒï¼Œè‹±æ–‡æ˜¯æœ‰æ‘˜è¦çš„ï¼Œä½†æ˜¯å…¨æ–‡æ˜¯ä¸­æ–‡çš„ã€‚é‚£ä¹ˆåœ¨é‚£é‡Œé¢ï¼Œå…¶å®æˆ‘ä»¬æŠŠäººå·¥æ™ºèƒ½çš„å®‰å…¨é£é™©å‘¢å‘ƒåˆ†æˆä¸‰ä¸ªæ–¹é¢å‘ƒï¼Œä¸€ä¸ªæ˜¯æ¨¡å‹æ–¹é¢ã€‚

ä¸€ä¸ªæ˜¯ç®—æ³•å’Œç¡¬ä»¶æ–¹é¢ï¼Œå¦å¤–ä¸€ä¸ªæ˜¯è‡ªä¸»æ„è¯†çš„ä¸å¯æ§æ–¹é¢ã€‚é‚£ä¹ˆä»æ¨¡å‹æ–¹é¢å‘¢ï¼Œå®ƒä¸»è¦å°±æ˜¯æˆ‘ä»¬è¯´æ¨¡å‹æœ¬èº«æ˜¯ä¸å¯è§£é‡Šçš„ã€‚è¿™ä¸ªæˆ‘æƒ³æˆ‘å°±ä¸å±•å¼€äº†ã€‚ç¬¬äºŒä¸ªæ˜¯ç®—æ³•å’Œç¡¬ä»¶æ–¹é¢å‘¢ï¼Œå®ƒä¹Ÿæœ‰ä¸å¯é æ€§ã€‚å› ä¸ºæˆ‘ä»¬çŸ¥é“è½¯ä»¶ä¼šæœ‰bugã€‚

ç¡¬ä»¶å¯èƒ½é‡Œé¢ä¹Ÿä¼šæœ‰ä¸€äº›ä¸å¯é çš„åœ°æ–¹ï¼Œè¿™äº›éƒ½å¯èƒ½å¸¦æ¥å®‰å…¨çš„é£é™©ã€‚è¿˜æœ‰ä¸€ä¸ªå‘¢å°±æ˜¯è‡ªä¸»æ„è¯†çš„ä¸å¯æ§æ€§ä¸å¯é æ€§ï¼Œå°±å¤±æ§äº†ã€‚è¿™ä¸ªå¤±æ§å¯èƒ½è‚¯å®šç³»ç»Ÿçš„å¤±æ§ä¼šå‘ƒå¸¦æ¥å¾ˆå¤šä¸åŒçš„é£é™©ã€‚é‚£è¿™äº›é£é™©å‘¢éƒ½æ˜¯å¼ºäººå·¥æ™ºèƒ½ã€‚

å¯èƒ½ä¼šå¸¦æ¥çš„ä¸€äº›é£é™©ã€‚é‚£é’ˆå¯¹è¿™äº›é£é™©åº”è¯¥æ€ä¹ˆåšï¼Ÿå°±å®åˆšæ‰beä¹Ÿè¯´äº†å¾ˆå¤šï¼Œè¿™ä¸ªæˆ‘ä»¬è¦å•Šæƒ³æ³•å°½é‡å‡å°‘é™ä½å’Œå‡å°‘è¿™äº›é£é™©çš„ä¸€äº›æŠ€æœ¯è·¯çº¿å’Œåšæ³•ï¼Œä¹Ÿç»™äº†ä¸€ä¸ªå¾ˆé•¿çš„æ¸…å•ã€‚é‚£å½“æ—¶å‘¢æˆ‘ä»¬åœ¨21å¹´çš„æ—¶å€™ã€‚

å°±è¯´å‘ƒç†è®ºæ–¹é¢è¦å®Œæˆå‘ƒå®Œå–„ä¸€äº›è¿™ç§æŠ€æœ¯ç†è®ºçš„éªŒè¯å•Šï¼Œå®ç°çš„æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚å‘ƒï¼Œå¦å¤–å‘¢å¯¹å‘ƒäººå·¥æ™ºèƒ½çš„ä»·å€¼å–å‘å•Šï¼Œè¦æƒ³æ³•èƒ½å¤Ÿåœ¨åº•å±‚ä»·å€¼ä¸Šé¢å¯¹å®ƒè¿›è¡Œä¸¥æ ¼æ§åˆ¶ã€‚é‚£ä¹ˆåœ¨åº”ç”¨é˜¶æ®µå‘¢ã€‚

ä¸»è¦æ˜¯å‘ƒå¸Œæœ›èƒ½å¤Ÿå‘ƒæœ‰è¶³å¤Ÿçš„æŠ€æœ¯æ”¯æ’‘ï¼Œå‘ƒï¼Œé˜²æ­¢å‘¢å‘ƒäººä¸ºçš„é€ æˆè¿™äº›å‘ƒå®‰å…¨é—®é¢˜ã€‚å‘ƒï¼Œå½“ç„¶è¿™äº›æ¯”å¦‚åƒé€ å‡å‘€å‘ƒå‡é£Ÿå“å•Šã€å‡å›¾åƒå•Šï¼Œè¿™å…¶å®éƒ½æ˜¯äººä¸ºçš„ï¼Œè¦å°½é‡å»å‘ƒé¢„é˜²æˆ–è€…æ˜¯è¿™ä¸ªèƒ½å¤Ÿå‘ƒæ£€æµ‹å‘ƒï¼Œè¿™æ–¹é¢çš„å‘ƒä¸€äº›æƒ…å†µã€‚

æ‰€ä»¥è¿™ä»¶äº‹è¦åšå‘¢å‘ƒå¾ˆé‡è¦çš„å‘ƒå°±æ˜¯ä¸€ä¸ªæ–¹é¢å°±æ˜¯æˆ‘ä»¬å¿…é¡»è¦å¼€å±•å›½é™…åˆä½œç ”ç©¶ã€‚æ²¡æœ‰å›½é™…åˆä½œç ”ç©¶ã€‚å…¶å®è¿™æ–¹é¢å‘¢å‘ƒä½ å¾ˆéš¾å–å¾—è¿™ä¸ªå‘ƒå°±æ˜¯è¯´åœ¨å…¨çƒå‘ƒï¼Œå› ä¸ºæœ‰ä¸€äº›ä¸œè¥¿ä½ åšçš„å¥½ï¼Œåˆ«äººå¯èƒ½ä¸ä¼šåšã€‚æœ‰ä¸€äº›ä¸œè¥¿åˆ«äººåšçš„å¥½ã€‚

ä½ å¯èƒ½ä¸ä¼šåšã€‚æˆ‘ä»¬é€šè¿‡å›½é™…åˆä½œå‘¢ï¼ŒæŠŠå¤§å®¶åšçš„å¥½ä¸œè¥¿å‘¢éƒ½å¯ä»¥é€šè¿‡äº¤æµï¼Œå‘ƒï¼Œä½¿å¾—å¤§å®¶å¯¹äººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢å‘¢éƒ½èƒ½å¤Ÿæé«˜åˆ°æ¯”è¾ƒé«˜çš„ä¸€ä¸ªæ°´å‡†ã€‚å‘ƒï¼Œè€Œä¸”å‘¢åœ¨è¿™æ–¹é¢ä¸ä»…ä»…è¦åˆä½œï¼Œäººæ‰åŸ¹å…»ä¹Ÿæ˜¯éå¸¸å…³é”®çš„ã€‚

å› ä¸ºå‘ƒä»¥å¾€å…³äºäººå·¥æ™ºèƒ½å®‰å…¨ç›¸å…³çš„äººæ‰å‘¢å…¶å®æ˜¯éå¸¸ç¨€ç¼ºçš„ã€‚å½“ç„¶è¿™å‡ å¹´æ…¢æ…¢æœ‰ç‚¹å¥½ã€‚æœ‰äº›å¥½è½¬ï¼Œä½†æ˜¯å‘¢æˆ‘ä»¬ä»ç„¶éœ€è¦å¤§é‡çš„äººæ‰ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªå‘ƒã€‚è¯­è¨€æ¨¡å‹å’Œæ•°æ®æ–¹é¢å‘ƒï¼Œæ¯”è¾ƒé‡è¦çš„å‘¢å°±æ˜¯æˆ‘ä»¬è¦æœ‰å¾ˆå¥½çš„å¹³å°ã€‚

è¦æœ‰å¾ˆå¥½çš„æ•°æ®ã€‚å‘ƒï¼Œç„¶åå‘¢å»è¿™ä¸ªå‘ƒå»è®­ç»ƒå‘ƒï¼Œå»è°¢è°¢ã€‚å»è®­ç»ƒå’Œå»è¿™ä¸ªç”¨è¿™äº›æ•°æ®å‘ƒï¼Œä½¿å¾—ä½ è®­ç»ƒçš„ç»“æœå‘¢å‘ƒæ¯”è¾ƒç†æƒ³ã€‚å‘ƒï¼Œåœ¨è¿™æ–¹é¢å‘¢é‚£æˆ‘æ‰€åœ¨çš„é¹ç¨‹å®éªŒå®¤å‘¢ï¼Œæˆ‘ä»¬å¤§æ¦‚ã€‚å‘ƒï¼Œä»2018å¹´å¼€å§‹ã€‚

ç”¨è‹±ä¼Ÿè¾¾çš„å¡æ‰äº†å‘ƒä¸€å°åƒå¡å·¦å³çš„æœºå™¨ã€‚å‘ƒï¼Œé‚£æ—¶å€™çš„å› ä¸º18å¹´æ¯”è¾ƒæ—©äº†ï¼Œé‚£æ—¶å€™è¿˜æ˜¯æˆ‘100çš„æ—¶ä»£ï¼Œæ‰€ä»¥ç®—åŠ›æ²¡æœ‰é‚£ä¹ˆå¼ºã€‚å‘ƒï¼Œé‚£ä¹ˆåˆ°äº†2020å¹´ï¼Œæˆ‘ä»¬å°±ç”¨å‘ƒåä¸ºçš„ç¥è…¾910å‘ƒï¼Œå°±åšäº†ä¸€å°4000å—å¡çš„æœºå™¨ã€‚

å‘ƒï¼Œé‚£ä¹ˆå·®ä¸å¤š10001000ä¸ªPçš„ç®—ä¾‹ã€‚é‚£ä¹ˆä»Šå¹´å¹´åº•å‘¢ï¼Œæˆ‘ä»¬å¤§æ¦‚ä¼šåš1ä¸ª2ä¸‡å¤šå—å¡çš„æœºå™¨ï¼Œå‘ƒï¼Œå¤§æ¦‚ä¼šä¼šæœ‰è¿™ä¸ª16000Pçš„ç®—åŠ›æˆ–è€…16äº¿çš„ç®—ä¾‹ã€‚é‚£æœ‰è¿™ä¸ªç®—ä¾‹å‘¢ã€‚

æˆ‘ä»¬å°±å¯ä»¥å¯¹å‘ƒæ¨¡å‹çš„è®­ç»ƒå•Šå‘ƒæ¨¡å‹è®­ç»ƒå½“ä¸­çš„ä¸€äº›è¿™ç§å‘ƒç»éªŒå•Šæ•™è®­å•Šæˆ–è€…æ¨¡å‹è®­ç»ƒå®Œäº†ä¸€äº›è¿™ä¸ªè®­ç»ƒçš„å‘ƒè¿™ä¸ªæ¨¡å‹å‚æ•°çš„è¿™ä¸ªå‘ƒå¯¹ç¤¾ä¼šçš„èµ‹èƒ½å‘ƒç­‰ç­‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥åšä¸€äº›äº‹å„¿ã€‚

æ¯”å¦‚è¯´æˆ‘ä»¬æŠŠæ‰€æœ‰è®­ç»ƒå‘ƒæˆ‘ä»¬åœ¨æœºå™¨ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œè‡ªå·±è®­ç»ƒçš„æ¨¡å‹å•Šã€‚éƒ½å‘ƒå¼€æºå¼€æ”¾å‡ºæ¥ï¼Œç„¶åä¾›ç¤¾ä¼šä¾›ç ”ç©¶å›¢ä½“å»ä½¿ç”¨ã€‚é‚£ä¹ˆå½“ç„¶è¿™é‡Œå¤§å®¶ä¼šè¯´å‘ƒï¼Œä½ è¦è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œå‘ƒï¼Œæˆ‘çš„æ•°æ®å‘ƒï¼Œä¼šä¸ä¼šä¸¢å¤±ã€‚

ä¼šä¸ä¼šè¢«åˆ«äººè¢«ä¸ç›¸å…³çš„äººå°±ç›´æ¥æ‹¿èµ°äº†ã€‚é‚£ä¹ˆæˆ‘ä»¬å®éªŒå®¤ä¹Ÿå¼€å‘äº†å¼€å‘äº†ä¸€å¥—æŠ€æœ¯å«åšé˜²æ°´ä¿æŠ€æœ¯ã€‚é˜²æ°´å®æŠ€æœ¯å‘¢å…¶å®å°±æ˜¯è¯´æ•°æ®æ‹¥æœ‰æ–¹ï¼Œä»–å¯¹æ•°æ®å‘ƒï¼Œå…·æœ‰ç»å¯¹çš„æ§åˆ¶å’Œç®¡ç†çš„æƒåˆ©ã€‚å‘ƒï¼Œé‚£ä¹ˆæœºå™¨åœ¨è®­ç»ƒçš„æ—¶å€™ã€‚

æ•°æ®å®ƒæ˜¯å¯ç”¨ä¸å¯è§ã€‚å°±æ˜¯æœºå™¨å‘¢æœºå™¨å½“ç„¶å¯ä»¥è§å¾—åˆ°æ•°æ®ã€‚å°±æœºå™¨ä¸Šé¢çš„æ“ä½œå‘˜å…¶å®ä»–æ˜¯çœ‹ä¸åˆ°æ•°æ®çš„ï¼Œä»–åªèƒ½çœ‹åˆ°ä½ çš„è¿™ä¸ªæ ·è¿™ä¸ªæ ·æœ¬æ•°æ®å°±æ˜¯ä½ å¯ä»¥ç”¨ä¸€ä¸ªæ¯”è¾ƒå°çš„ã€‚ä½†æ˜¯å‘ƒè„±æ•çš„ä¸€ä¸ªæ•°æ®å‘¢å‘ƒè®©æ“ä½œå‘˜å…ˆå»è¯•æ¨¡å‹ã€‚

ä¸€æ—¦è¦è¯•å¥½äº†ï¼ŒçœŸçš„æ•°æ®è¿›å»ä»¥åæ“ä½œå‘˜å·²ç»çœ‹ä¸åˆ°çœŸæ•°æ®äº†ã€‚é™¤éæ•°æ®åº”ç”¨è€…ç»™ä»–è¿™ä¸ªæƒåˆ©ï¼Œä»–å¯ä»¥çœ‹å¾—åˆ°ã€‚åŒ…æ‹¬è®­ç»ƒå®Œäº†çš„å‚æ•°ï¼Œå¦‚æœè¦å¾€å¾€å¤–èµ°çš„æ—¶å€™ã€‚

é‚£ä¹ˆå‘ƒæœºå™¨ä¹Ÿä¼šè‡ªåŠ¨å‘è¿™ä¸ªæ•°æ®æ‹¥æœ‰æ–¹ä¸»åŠ¨å‘ƒå»è¯·è¯·æ±‚è¯´å‘ƒæœ‰ä¸€ä¸ªå‚æ•°è¦å¾€å¤–ä¼ é€ã€‚è¯·ä½ æ£€æŸ¥è¿™é‡Œæœ‰æ²¡æœ‰æºå¸¦ä½ çš„æ•°æ®ç­‰ç­‰ã€‚æœ‰è¿™æ ·çš„ä¸€ä¸ªæµç¨‹ï¼Œä½¿å¾—æ•°æ®å¯ä»¥åšåˆ°è¶³å¤Ÿçš„å®‰å…¨ã€‚é‚£æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ¨¡ç³»åˆ—çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬7Bçš„æ¨¡å‹ã€‚

33Bé•¿çª—å£çš„æ¨¡å‹å’Œ200Bçš„æ¨¡å‹ï¼Œå‘ƒï¼Œè¿™éƒ½æ˜¯å¤§è¯­è¨€æ¨¡å‹äº†ï¼Œè¿™è¯­è¨€æ¨¡å‹é‡Œé¢æ—¢æœ‰ä¸­æ–‡è‹±æ–‡ï¼Œè¿˜æœ‰å…¶ä»–è¿™ä¸ªå‘ƒè¯­è¨€çš„ä¸€äº›å‚æ•°ã€‚é‚£ä¹ˆé€šè¿‡è¿™äº›å‘¢ï¼Œæˆ‘ä»¬è®­ç»ƒå®Œäº†ä»¥åï¼ŒæŠŠå®ƒéƒ½å¼€æºæ‰ï¼Œå‘ƒï¼Œå…±å‘ƒå¤§å®¶å»ä½¿ç”¨ã€‚

é‚£ä¹ˆå‘ƒæˆ‘ä»¬ç”¨çš„æœ€å¤§çš„è¿™ä¸ª200Bçš„æ¨¡å‹æ˜¯1ä¸ª104å±‚çš„ç½‘ç»œã€‚è¿™ä¸ªç”¨4000å—å¡å‘¢ï¼Œæˆ‘ä»¬å·®ä¸å¤šè®­ç»ƒäº†åŠå¹´å¤šæŠŠå®ƒè®­ç»ƒå‡ºæ¥äº†ã€‚å‘ƒï¼Œé‚£ä¹ˆåœ¨è¿™é‡Œå‘¢æˆ‘ä»¬ã€‚å‘ƒï¼Œä¹Ÿæ‘¸ç´¢äº†å¾ˆå¤šç»éªŒï¼Œæ€§èƒ½ä¹Ÿæ˜¯ä¸é”™çš„ã€‚

é‚£åæ¥å‘¢æˆ‘ä»¬åˆè®­ç»ƒäº†33Bçš„é•¿çª—å£æ¨¡å‹ã€‚é‚£ä¹ˆè¿™ä¸ªé•¿ç«¯å£ç°åœ¨ç›®å‰æ˜¯128Kçš„çª—å£ï¼Œé‚£ä¹ˆæ­£åœ¨è®­ç»ƒ192Kçš„çª—å£å¯èƒ½å¾ˆå¿«å°±ä¼šå®Œæˆã€‚è¿™äº›å®Œæˆä»¥åï¼Œæˆ‘ä»¬éƒ½æŠŠå®ƒå¼€æ”¾å‡ºå»ã€‚å‘ƒã€‚

é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿæœ‰æ•´å¥—çš„è¿™ä¸ªæ¨¡å‹çš„è¿™ç§å¼€æ”¾å’Œä½¿ç”¨çš„è¿™æ ·çš„ä¸€ä¸ªå‘ƒè¿™ç§ç»„ç»‡å»ä½¿ç”¨è¿™ä¸ªä¸œè¥¿ã€‚æ‰€ä»¥æ€»ç»“ä¸€ä¸‹å‘¢ï¼Œäººå·¥æ™ºèƒ½é«˜é€Ÿå‘å±•ï¼Œå…¶å®å‘ƒå¸¦æ¥è¿™ä¸ªå®‰å…¨é—®é¢˜å•Šå‘ƒæˆ‘ä»¬å¿…é¡»è¦é‡è§†ã€‚å½“ç„¶ä»åšæŠ€æœ¯çš„ã€‚

æˆ‘ä»¬è¦æŠŠäººå·¥æ™ºèƒ½åšçš„æ¨å‘å‰è¿›åšçš„æ›´å¥½ã€‚å‘ƒï¼Œæ‰€ä»¥å‘¢å‘ƒè¿™æ–¹é¢å‘¢åªæœ‰é€šè¿‡å›½é™…åˆä½œå‘ƒï¼Œæ‰æœ‰å¯èƒ½æ›´å¥½çš„æŠŠè¿™ä¸ªå‘ƒå·¥ä½œåšå¥½ã€‚å‘ƒï¼Œæˆ‘è·Ÿå¤§å®¶å°±åˆ†äº«è¿™ä¹ˆå¤šï¼Œè°¢è°¢å¤§å®¶ã€‚Thank you so muchã€‚

 Professor Gaoï¼Œ please be seatedã€‚Nextï¼Œ we're fortunate to have with usï¼Œ Professor Zhang Yainã€‚ğŸ˜Šã€‚

Professor Zhang is a member of the Chinese Academy of Engineeringã€‚

 as well as chair professor of AI Science and dean of the Institute for AI Industry Research at Tsinghua Universityã€‚

Professor Zhang previously served as the president of Baidorã€‚ And prior to thatã€‚

 he was a Microsoft executive for 16 yearsï¼Œ holding various key positionsã€‚ğŸ˜Šã€‚

As a world renowned scientist and entrepreneurï¼Œ he has made significant contributions through his 550 publicationsã€‚

62 US patents and other landmark engineering achievementsã€‚ğŸ˜Šã€‚

Let's give a warm welcome to Professor Zangã€‚å—¯ï¼Œæ—©ä¸Šå¥½ï¼Œè°¢è°¢å‘ƒå®‰è¿œAIé‚€è¯·æˆ‘æ¥è¿™ä¸ªå¤§ä¼šã€‚å‘ƒï¼Œåˆšæ‰å‘¢è¯å­¦benjoå’Œé«˜æ–‡é™¢å£«å‘¢å¯¹æ•´ä¸ªå•Šè¿™ä¸ªAIç‰¹åˆ«å¤§æ¨¡å‹çš„å‘å±•ã€‚

ç‰¹åˆ«æ˜¯é£é™©çš„å°±åšäº†ç‰¹åˆ«å•Šå¥½çš„è¿™ä¸ªç³»ç»Ÿæ€§çš„è¿™äº›ä»‹ç»ï¼Œä¸€ä¸ªæ˜¯å…¨çƒï¼Œä¸€ä¸ªæ˜¯ä¸­å›½ã€‚å•Šï¼Œçš„ç¡®çš„è¯å‘¢ï¼Œè¿‡å»è¿™ã€‚è¿™ä¸¤å¹´å·¦å³å‘¢å‘ƒè¿™ä¸ªAIçš„å‘å±•çš„é€Ÿåº¦å¾ˆå¿«ï¼Œå¿«çš„åŒæ—¶å‘¢ä¹Ÿå¸¦æ¥å¾ˆå¤šçš„è¿™äº›å®‰å…¨çš„é£é™©ã€‚å‘ƒã€‚

æˆ‘è¿‡å»è¿™ä¸¤å¹´å‘¢ä¹ŸèŠ±äº†ä¸å°‘æ—¶é—´å‘ƒï¼Œå’Œå…¨çƒé¢†å…ˆçš„å‘ƒè¿™äº›å­¦è€…ä»¬ä¸€èµ·å‘ƒæ¥ä»äº‹è¿™æ–¹é¢çš„ä¸€äº›ç ”ç©¶ã€‚ä»Šå¤©å‘¢æˆ‘ç®€å•å‘ƒè®²ä¸€ä¸‹æœ‰æ—¶é—´å…³ç³»ï¼Œæˆ‘ç®€å•è®²ä¸€ä¸‹æˆ‘çš„ä¸€äº›æ€è€ƒå§ã€‚å•Šï¼Œç‰¹åˆ«æ˜¯å•Šé¦–å…ˆå‘¢æ˜¯è¿™ä¸ªå¤§æ¨¡å‹å•Šï¼Œæ¥å‘å±•çš„ä¸€äº›è¶‹åŠ¿ã€‚å•Šã€‚

ä»¥åŠå‘¢å½“ç„¶æ›´é‡è¦çš„æ˜¯é£é™©æ–¹é¢çš„å®‰å…¨æ–¹é¢çš„ä¸€äº›å‘ƒä¸€äº›è€ƒè™‘ã€‚å—¯ï¼Œé¦–å…ˆæˆ‘è®¤ä¸ºå‘¢è¿™ä¸ªå¤§æ¨¡å‹å’Œç”Ÿäº§ç”ŸAIå•Šåœ¨æœªæ¥çš„è¿™ä¸ªè¿™ä¸ªåå¹´å§ï¼Œæœ‰ä¸‹é¢å‡ ä¸ªè¶‹åŠ¿ã€‚ç¬¬ä¸€ä¸ªå‘¢å°±æ˜¯å¤šæ¨¡æ€å•Šï¼Œæˆ‘ä»¬ä¸ç®¡æ˜¯æˆ‘ä»¬çš„è¯­è¨€ã€‚

æˆ‘ä»¬çš„æ–‡å­—å•Šå‘ƒè¯­éŸ³å›¾åƒå’Œè§†é¢‘ã€‚éƒ½æ­£åœ¨èåˆèµ·æ¥ã€‚å¦å¤–çš„è¯å‘¢ï¼Œè¿™ä¸ªæ¿€å…‰é›·è¾¾å•Šï¼Œè¿™ä¸ªå‘ƒä¸‰ç»´çš„ç»“æ„ä¿¡æ¯ï¼Œå››ç»´çš„å¤±ç©ºä¿¡æ¯å•Šï¼ŒåŒ…æ‹¬æˆ‘ä»¬è›‹ç™½è´¨ã€‚å‘ƒï¼Œè¿™ä¸ªå•Šç»†èƒå•Šè¿˜æœ‰åŸºå› éƒ½åœ¨å˜æˆå¤šæ¨¡è‚½çš„æ”¶å…¥ã€‚

é‚£ä¹ˆç¬¬äºŒç‚¹å°±æ˜¯æˆ‘ä»¬å«æ™ºèƒ½ä½“è‡ªä¸»æ™ºèƒ½å•Šï¼Œè¿™å¯ä»¥è‡ªä¸»çš„è§„åˆ’ä»»åŠ¡å•Šï¼Œå¯ä»¥å¼€å‘ä»£ç ï¼Œå¯ä»¥è‡ªå·±å‡çº§ï¼Œä¸ä½†è¯•é”™å•Šï¼Œæ˜¯å¯ä»¥å»ä¼˜åŒ–ã€‚è‡ªå·±ä¹Ÿå¯ä»¥å»è‡ªæˆ‘caã€‚ç¬¬ä¸‰ä¸ªå°±æ˜¯æ™ºèƒ½çš„èµ°å‘è¾¹ç¼˜ã€‚æˆ‘ä»¬ç°åœ¨å‘ƒè®²å¤§æ¨¡å‹ã€‚

å¤§åˆ†è¿˜æ˜¯åœ¨è¿™ä¸ªäº‘ç«¯çš„è¿™ä¸ªå¤§æ¨¡å‹ã€‚å‘ƒï¼Œè¾¹ç°åœ¨å‘¢æ­£èµ°å‘æˆ‘ä»¬çš„PCå•Šå‘ƒèµ°å‘æˆ‘çš„æ‰‹æœºå•Šï¼Œèµ°å‘å•Šæˆ‘ä»¬çš„è¿™äº›æ™ºèƒ½çš„è¿™ä¸ªè®¾å¤‡èµ°å‘è¾¹ç¼˜ç«¯ã€‚é‚£ä¹ˆç¬¬å››ä¸ªå°±æ˜¯ç°åœ¨è®²ç‰©ç†æ™ºèƒ½å•Šï¼Œå°±æ˜¯ä¸¾èº«æ™ºèƒ½ã€‚æˆ‘æˆ‘æˆ‘è¿™åå¹´ä¸€ç›´å«ç‰©ç†æ™ºèƒ½ã€‚

ç°åœ¨æ–°çš„åè¯æ¯”è¾ƒæ—¶é«¦ï¼Œå«ä¸¾èº«æ™ºèƒ½ã€‚å°±æ˜¯å¤§æ¨¡å‹ç”¨åˆ°æ¯”å¦‚è¯´æ— äººè½¦ã€æ— äººæœºæœºå™¨äººå•Šï¼Œç‰©ç†åŸºç¡€è®¾æ–½ï¼Œåƒç”µç½‘å•Šå•Šè¿™ä¸ªç”µç«™å•Šå•Šä¸€äº› criticalical infrastructureã€‚é‚£ä¹ˆæœ€åä¸€ä¸ªå‘¢æ˜¯ç”Ÿç‰©æ™ºèƒ½ã€‚

å°±åƒåŒ…æ‹¬ç°åœ¨æˆ‘ä»¬çš„è„‘æœºæ¥å£å•Šï¼Œç”¨åˆ°æˆ‘ä»¬çš„äººä½“ã€äººè„‘å•Šã€åŒ»ç–—æœºå™¨äººã€ç”Ÿç‰©ä½“å’Œç”Ÿå‘½ä½“ã€‚å—¯ï¼Œè¿™ä¸ªæˆ‘æˆ‘æˆ‘æœ€è¿‘å‘¢å’Œå¾ˆå¤šçš„å­¦è€…éƒ½ä¸€ä¸€ç›´åœ¨æ¢è®¨è¿™é—®é¢˜ï¼Œåˆ°åº•é€šç”¨äººå·¥æ™ºèƒ½ä»€ä¹ˆæ—¶å€™å¯ä»¥å®ç°å•Šï¼Ÿå•Šã€‚

æˆ‘è¿™ä¸ªè¡¨è¾¾ä¸€ä¸‹æˆ‘å®Œå…¨ä¸ªäººçš„æ„è§ã€‚å› ä¸ºåˆšæ‰å‘ƒäºšé¦–ç­ä¸»ä»»ä¹Ÿè®²åˆ°ï¼Œæˆ‘ä»¬è®¨è®ºè¿™ä¸ªé—®é¢˜çš„æ—¶å€™ï¼Œå¤§å®¶æœ‰å¾ˆå¤šä¸åŒçš„è¿™ä¸ªè§’åº¦ï¼Œä¸åŒçš„è§‚ç‚¹ã€‚æˆ‘ä¸ªäººè®¤ä¸ºçš„è¯å‘¢ï¼Œå·®ä¸å¤šåœ¨20å¹´ä¹‹å†…ä¼šå®ç°è¿™ä¸ªé€šç”¨äººå·¥æ™ºèƒ½ã€‚å‘ƒï¼Œåˆ†ä¸‰ä¸ªé˜¶æ®µã€‚

å°±æ˜¯æˆ‘ä¸€ç›´åˆ†æˆä¿¡æ¯æ™ºèƒ½ã€ç‰©ç†æ™ºèƒ½å’Œè¿™ä¸ªç”Ÿç‰©æ™ºèƒ½ã€‚é‚£ä¹ˆä¿¡æ¯æ™ºèƒ½çš„è¯å‘¢ï¼Œ5å¹´ä¹‹å†…æˆ‘è®¤ä¸ºå¯ä»¥ã€‚è¾¾åˆ°æ‰€è°“çš„è¿™ä¸ªå›¾æ—æµ‹è¯•ã€‚å‘ƒï¼Œå½“æ—¶cheGPTå‡ºæ¥çš„æ—¶å€™å‘¢ï¼Œæˆ‘çš„ç¬¬ä¸€æ„Ÿè§‰æˆ‘è§‰å¾—åˆ‡GPGPTçš„æ–‡å­—æ–¹é¢ã€‚

å•ŠåŸºæœ¬ä¸Šé€šè¿‡äº†é©¼çµæµ‹è¯•ã€‚é‚£åœ¨è¿™ä¸ªè§†é¢‘å•Šï¼Œåœ¨åˆ«çš„æ–¹é¢å¯èƒ½è¿˜éœ€è¦ä¸€ç‚¹æ—¶é—´å•Šï¼Œå¯èƒ½åœ¨5å¹´ä¹‹å†…æˆ‘å¯ä»¥è¾¾åˆ°è¿™ä¸ªä¿®æ”¹çš„æˆ–è€…æ–°å›¾çµæµ‹è¯•ã€‚åœ¨ç‰©ç†æ™ºèƒ½æˆ–è€…å·¨æ·±æ™ºèƒ½å‘¢ï¼Œå¯èƒ½è¿˜éœ€è¦å·®ä¸å¤š10å¹´çš„æ—¶é—´ã€‚å•Šã€‚

å› ä¸ºç°åœ¨å•Šæ¯”å¦‚è¯´æ— äººè½¦å•Šï¼Œè¿™ä¸ªäººå½¢æœºæ¢°ï¼Œæˆ‘ä»¬è¿™ä¸ªä¼šè®®ä¹Ÿçœ‹åˆ°å¾ˆå¤šã€‚å•Šï¼Œè¿™ä¸ªæˆ‘è‡ªå·±è®¤ä¸ºå‘¢ï¼Œæˆ‘è¿™ä¹ˆå¤šå¹´ä¸€ç›´åœ¨åœ¨åšæ— äººè½¦å•Šï¼Œä»å½“å½“æ—¶åœ¨ç™¾åº¦çš„é˜¿æ³¢ç½—å•Šï¼Œé‚£ä¹ˆä¸€ç›´åœ¨åšæ— äººè½¦ï¼Œå¯èƒ½89å¹´çš„æ—¶é—´äº†ã€‚å‘ƒã€‚

æˆ‘è®¤ä¸ºæ— äººé©¾é©¶å‘¢æ˜¯å·¨èº«æ™ºèƒ½ä¸€ä¸ªæœ€å¤§çš„åº”ç”¨ï¼Œä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªå®ç°è¿™ä¸ªæ–°å›¾çµæµ‹è¯•çš„è¿™ä¸ªåº”ç”¨ã€‚å•Šï¼Œæ˜å¹´å‘¢å‘ƒå¤§å®¶éƒ½çœ‹åˆ°æˆ‘ä»¬åœ¨æ­¦æ±‰åšçš„è¿™ä¸ªå¤§è§„æ¨¡çš„å•Šã€‚å‘ƒï¼Œè¿™ä¸ªè¯•éªŒçš„äººã€‚å•†ç”¨å•Šå•Šï¼Œæˆ‘è§‰å¾—åœ¨æ˜å¹´çš„è¯å‘¢ã€‚

æˆ‘ä¼šçœ‹åˆ°æ›´å¤šçš„åº”ç”¨å•Šï¼Œåœ¨å‘ƒ2030å¹´çš„è¯ï¼Œä¹‹å‰çš„è¯å‘¢ä¼šæˆä¸ºä¸»æµçš„åº”ç”¨ã€‚ç”Ÿç‰©æ™ºèƒ½å¯èƒ½æ—¶é—´æ›´é•¿ä¸€ç‚¹å•Šï¼Œå¯èƒ½éœ€è¦å†æœ‰å·®ä¸å¤š10å¹´çš„æ—¶é—´ã€‚ä½†æ•´ä½“æ¥è®²çš„è¯å‘¢ï¼Œåœ¨æœªæ¥çš„20å¹´ï¼Œæˆ‘è®¤ä¸ºä¼šä»¥è¾¾åˆ°è¿™ä¸ªé€šç”¨äººå·¥æ™ºèƒ½ã€‚

é‚£æˆ‘å‘ƒæ‰€åœ¨çš„æ¸…åå¤§å­¦æ™ºèƒ½äº§ä¸šé™¢ï¼Œå…¶å®å°±æ˜¯ä¸ºäº†å•Šè¿™é€šç”¨æ™ºèƒ½è€Œã€‚å»ºè€Œè€Œå»ºå»ºèµ·æ¥çš„ï¼Œæˆ‘ä»¬å…¶å®å°±æ˜¯åœ¨ä¸‰å¹´åŠå‰å»ºèµ·æ¥ã€‚é‚£ä¹ˆè¿™ä¸ªç ”ç©¶é™¢çš„è¯å‘¢ï¼Œç›®å‰æœ‰22åæ•™æˆå•Šï¼Œæœ‰å·®ä¸å¤š300å¤šä½å­¦ç”Ÿã€‚å•Šï¼Œæˆ‘ä»¬çš„å‘ƒç›®æ ‡å¾ˆç®€å•ã€‚

å°±æ˜¯èƒ½å®ç°å•Šä¿¡æ¯æ™ºèƒ½å•Šï¼Œç‰©ç†æ™ºèƒ½ä»¥åŠå‘¢è¿™ä¸ªç”Ÿç‰©æ™ºèƒ½å•Šï¼ŒåŒ…æ‹¬æ— äººé©¾é©¶å…ˆè¿›çš„æœºå™¨äººï¼Œä¹ŸåŒ…æ‹¬å‘¢å‘ƒè¿™ä¸ªå‘ƒã€‚Biological computingè¿™äº›æ¦‚å¿µã€‚å•Šï¼Œç›®å‰æˆ‘ä»¬ä¹Ÿå‘å¸ƒäº†å¾ˆå¤šæ¨¡å‹ï¼Œæˆ‘ä»¬æ›´å¤šæ˜¯å‚ç›´æ¨¡å‹ã€‚

æ¯”å¦‚è¯´æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªç¬¬ä¸€ä¸ªå•Šå…¨çƒçš„å‘ƒå®ç”¨çš„ç«¯åˆ°ç«¯çš„æ— äººé©¾é©¶çš„å¼€æºæ¨¡å‹ã€‚å«air apoo FMå¤§å®¶éƒ½å¯ä»¥çœ‹åˆ°å“ï¼Œåœ¨githubä¸Šé¢ï¼Œæˆ‘ä»¬ä¹Ÿå‘å¸ƒäº†å‘ƒç¬¬ä¸€ä¸ªå…¨çƒæœ€å¤§çš„è¿™ä¸ªbiomaGPTå‘ƒï¼Œä½ éƒ½æ˜¯å¼€æºçš„ã€‚

å¤§å®¶éƒ½å¯ä»¥å¯ä»¥ä½¿ç”¨å¯ä»¥ä½¿ç”¨ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªæœ‰å·¨å¤§èƒ½åŠ›çš„åŒæ—¶çš„è¯å‘¢ï¼Œå°±å¸¦æ¥å¾ˆå¤§çš„ä¸€äº›é£é™©ã€‚å•Šï¼Œæˆ‘åˆšæ‰ç­ä¸»ä»»ä¹Ÿè®²è¿™å‰æ²¿å¤§æ¨¡å‹ï¼Œè¿™ä¸ªå¤§æ¨¡å‹ã€‚åˆ°äº†è¿™ä¸ªä¸‡ä¸€å‚æ•°æ›´å¤šçš„æ—¶å€™å‘¢å°±äº§ç”Ÿäº†å®ƒçš„é£é™©ã€‚å•Šã€‚

é‚£ä¹ˆæˆ‘è¿˜æ˜¯åˆ†æˆè¿™ä¸ªä¸‰ä¸ªä¸åŒçš„ä¸–ç•Œå•Šï¼Œä¿¡æ¯ä¸–ç•Œã€ç‰©ç†ä¸–ç•Œã€ç”Ÿç‰©ä¸–ç•Œã€‚ä¿¡æ¯ä¸–ç•Œçš„é£é™©å¤§å®¶æ¯”è¾ƒå®¹æ˜“ç†è§£ã€‚åˆšæ‰è®²åˆ°deep fakeå•Šï¼Œè®²åˆ°è¿™ä¸ª hallucination missignmentå•Šã€‚

è®²åˆ°è¿™ä¸ªmiinformationå•Šï¼Œè¿™ä¸ªæˆ‘è§‰å¾—ç›¸å¯¹æ¯”è¾ƒå®¹æ˜“ç†è§£ã€‚å—¯ï¼Œé‚£åˆ°äº†è¿™ä¸ªç‰©ç†ä¸–ç•Œå‘¢ï¼Œè¿™ä¸ªé£é™©å°±ä¼šæ›´å¤§ã€‚ä½ æƒ³ä¸€æƒ³çœ‹ï¼Œæˆ‘ä»¬æœ‰å‘ƒå†è¿‡1å¹´ï¼Œæˆ‘æƒ³æˆ‘ä»¬è¿™ä¸ªä¸–ç•Œçš„æœºå™¨äººæ¯”è°è¦å¤šå¾—å¤šã€‚é‚£æœºå™¨äººçš„è¯å‘¢ã€‚

å¦‚æœä»–å¤±æ§ï¼Œå¦‚æœä»–è¢«åäººæ‰€ä¹±ç”¨ã€‚é‚£å¤§å®¶å¯ä»¥æƒ³è±¡åˆ°ç»™ç¤¾ä¼šå¸¦æ¥é£é™©ã€‚ä»¥åæˆ‘ä»¬çš„è½¦å¯èƒ½éƒ½å¤§ä¹˜æ— äººè½¦ã€‚è¿™ä¸ªæ—¶å€™æ˜¯é è¿™ä¸ªå¤§æ¨¡å‹å»å»å»æ§åˆ¶çš„ã€‚é‚£è¿™ä¸ªæ—¶å€™æ‰€å¸¦æ¥çš„é£é™©å•Šï¼Œä¸ç®¡ä¸ç®¡æ˜¯è¿™ä¸ªä¸»åŠ¨é£é™©ã€è¢«åŠ¨é£é™©éƒ½ä¼šå¾ˆå¤§ã€‚

é‚£ä¹ˆæ›´å¤§çš„é£é™©çš„è¯å‘¢ï¼Œæ˜¯å‘ƒè¿™ä¸ªç”Ÿç‰©æ™ºèƒ½å•Šï¼Œç‰©ç†æ™ºèƒ½å’Œä¿¡æ¯æ™ºèƒ½èåˆåœ¨ä¸€å—ã€‚è¿™ä¸ªæ—¶å€™å¦‚æœå¤±æ§æˆ–è€…è¢«ä¹±ç”¨ï¼Œä¼šé€ æˆäº†è¿™ä¸ªç”Ÿå­˜é£é™©ã€‚å‘ƒï¼Œæ‰€ä»¥æˆ‘å‘¢å•Šè§‰å¾—æˆ‘ä»¬æœ‰å‘ƒè¿‡å»è¿™å‡ å¹´å•Šæœ‰å‡ ä¸ªé‡è¦çš„å‘ƒèŠ‚ç‚¹ã€‚

å…¶ä¸­ä¸€ä¸ªèŠ‚ç‚¹å°±æ˜¯åœ¨2023å¹´6æœˆä»½çš„ã€‚è¿™ center for AI safetyé‚£ä¸ª recent statement AI risksã€‚å•Šï¼Œè®²åˆ°æˆ‘ä»¬è¦æŠŠäººå·¥æ™ºèƒ½æœªæ¥çš„é£é™©ã€‚

æŠŠå®ƒå½“åšæ ¸æ ¸æ­¦å™¨å’Œè¿™ä¸ªæµè¡Œç—…ä¸€æ ·çš„è¿™ä¸ªè¿™ä¸ªä¼˜å…ˆçº§å»çœ‹å¾…å•Šã€‚åæ¥çš„è¯å‘¢æœ‰å¾ˆå¤šå·¥ä½œï¼ŒåŒ…æ‹¬å‘ƒåˆšæ‰é‚£ä¸ªéƒ¨é•¿è®²åˆ°çš„æˆ‘ä»¬ä¸­å›½çš„äººå·¥æ™ºèƒ½å…¨çƒå€¡è®®ï¼Œä¹ŸåŒ…æ‹¬åŒ»ç”¨çš„AIactã€‚ä¹ŸåŒ…æ‹¬å‘¢å‡ æ¬¡å‘ƒè¿™ä¸ªå³°ä¼šå•Šã€‚

ç„¶åä¹ŸåŒ…æ‹¬æˆ‘ä»¬ä¸€äº›å°èŒƒå›´çš„ä¼šã€‚å‘ƒï¼Œå°±æ˜¯æˆ‘å‘ƒæˆ‘å»å¹´çš„è¯å‘¢ï¼Œæˆ‘å’Œtå•Šç»„ç»‡äº†ä¸€ä¸ªå«international dialogueå•ŠAI safetyã€‚é‚£ä¹ˆæ¯ä¸‰å››ä¸ªæœˆå‘¢å¼€ä¸€æ¬¡ä¼šï¼Œå•Šç¬¬ä¸€æ¬¡åœ¨è‹±å›½ã€‚

ç¬¬äºŒæ¬¡åœ¨åŒ—äº¬ä¸‹é¢ä¸€æ¬¡å‘¢æ˜¯åœ¨è¿™ä¸ªvenessæˆ‘ä»¬å¼€ä¼šå•Šï¼Œè¿™ä¸¤å¤©ä¸‰å¤©æ·±åº¦çš„å»ç ”ç©¶è¿™é‡Œé¢çš„ä¸€äº›æŠ€æœ¯é—®é¢˜å’Œæ”¿ç­–çš„è¿™ä¸ªå¯¹åº”çš„é—®é¢˜ã€‚å•Šï¼Œé‚£åˆšæ‰å‘¢å‘ƒé¸­èˆŒè®²çš„å•Šé‚£ä¸ªæŠ¥å‘Šï¼Œæˆ‘è§‰å¾—æ˜¯å‘ƒæŠŠè¿™ä¸ªå¾ˆå¤šçš„è®¨è®ºå‘¢åšäº†é«˜åº¦çš„ä¸€ä¸ªæ€»ç»“ã€‚

æˆ‘ä¹Ÿå¾ˆé«˜å…´çš„æ·±åº¦å‚ä¸é‚£ä¸ªæŠ¥å‘Šã€‚æˆ‘ç®€å•ä»‹ç»ä¸€ä¸‹å‘¢ï¼Œè¿™ä¸ªå¤§æ¨¡å‹å®‰å…¨æ–¹é¢çš„ä¸€äº›æŠ€æœ¯ã€‚å› ä¸ºå¤§æ¨¡å‹å®‰å…¨å®ƒç¡®å®æ˜¯ä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹ï¼Œä»æˆ‘ä»¬çš„è¾“å…¥å•Šï¼Œä»æˆ‘ä»¬çš„è¾“å‡ºï¼Œä»æˆ‘ä»¬çš„å®‰å…¨è¯„ä¼°æ²»ç†ã€‚

ç‰¹åˆ«æ˜¯è¿™ä¸ªç³»ç»Ÿçš„å®‰å…¨å¯¹é½éƒ½éœ€è¦å»å·¥ä½œã€‚è¿™é‡Œé¢æœ‰è®¸è®¸å¤šå¤šçš„è¿™ä¸ªæ•°å­¦å¾ˆå¤šå¾ˆå¤šçš„ç®—æ³•æ–¹é¢çš„ç ”ç©¶ï¼Œæœ‰è®¸å¤šå·¥ç¨‹çš„é—®é¢˜ï¼ŒæŠ€æœ¯çš„é—®é¢˜ã€‚å•Šï¼Œä¹Ÿå¸¦æœ‰å¾ˆå¤šè¿™ä¸ªç­–ç•¥çš„é—®é¢˜ã€‚è¿™ä¸ªæˆ‘å°±ä¸ç»†æƒ³æˆ‘ä»¬åšå•Šå‘ƒè¿™ä¸ªã€‚å®‰å…¨çš„è¯å‘¢ã€‚

å¯¹è¿™å¼ å›¾åº”è¯¥æ¯”è¾ƒç†Ÿæ‚‰ï¼Œå°±ä»å„ä¸ªæ–¹é¢è¯´ç³»ç»Ÿå·¥ç¨‹é—®é¢˜ã€‚ç„¶åå¦å¤–çš„è¯å‘¢ï¼Œè¿™é‡Œé¢å‘ƒå¾ˆé‡è¦ä¸€ç‚¹å°±æ˜¯æœ€è¿‘å‘ƒè®¸è®¸å¤šå¤šè¿›å±•å°±æ˜¯åœ¨å¤§æ¨¡å‹å®‰å…¨çš„å¯¹é½ã€‚è¿™é‡Œé¢æœ‰ä¸¤ç§å•Šä¸åŒçš„å‘ƒè¿™ä¸ªæ–¹æ³•å•Šï¼Œä¸€ä¸ªå‘¢æ˜¯ç›´æ¥å‘ƒç›‘ç£çš„å¾®è°ƒã€‚

å°±æ˜¯æˆ‘æŠŠé«˜è´¨é‡çš„æœ‰ç”¨çš„å®‰å…¨çš„è¿™ä¸ªä¿¡æ¯ï¼ŒæŠŠå®ƒç›´æ¥ç”¨äºå‘ƒè¿™ä¸ªç›‘ç£å¾®è°ƒã€‚é‚£ä¹ˆç¬¬äºŒç‚¹å‘¢æ˜¯æ ¹æ®å•Šæˆ‘ä»¬çš„åå¥½å•Šï¼Œäººç±»çš„åå¥½ï¼Œæˆ‘ä»¬çš„ä»·å€¼è§‚æ¥åšè¿™ä¸ªreinforment learningå•Šï¼Œæ¯”å¦‚è¯´å‘ƒè¿™ä¸ªå•Šã€‚

è¿™è¿™ä¸ªå‘ƒchGBTå•ŠGBTç³»åˆ—åŸºæœ¬ä¸Šæ˜¯é‡‡ç”¨å‘ƒè¿™ä¸ªPPOè¿™è¿™ç§æ–¹å¼ã€‚é‚£è¿™é‡Œé¢åˆæœ‰å¾ˆå¤šç§ä¸åŒçš„ä¸€äº›é€‰æ‹©ï¼Œä¸åŒé€‰æ‹©å¯ä»¥åŸºäºè¿™ä¸ªå¥–åŠ±æ¨¡å‹çš„å•Šå‘ƒè¿™ä¸ªå®‰å…¨å¥–åŠ±å’Œæ¸¸ç”¨å¥–åŠ±æœ‰å»ç»“åˆçš„è¿™ä¸ªä½œä¸ºæ•°å­—å‚æ•°å•Šã€‚

ç„¶åä¹Ÿå¯ä»¥ç”¨å•Šä¸€äº›æ›´å‘ƒæ–°çš„ä¸€äº›å¥–åŠ±çš„æ–¹å¼ã€‚é‚£ä¹ˆåœ¨å‘ƒå‘ƒæ¸…åå‘¢ï¼Œåœ¨åŸƒå°”åŒ–å‘¢ï¼Œæˆ‘ä»¬æœ‰å‡ ä½è€å¸ˆå‘¢åšäº†å¾ˆå¤šå¾ˆå¾ˆå¤šå·¥ä½œã€‚å‘ƒï¼Œé‚£æˆ‘ä»¬çš„æ±Ÿå…ˆç‡•è€å¸ˆå‘¢ã€‚

ä»–æå‡ºäº†è¿™ä¸ªconditionalå•Š reinforceinfor learningã€‚å•Šï¼Œé‚£ä¹ˆè¿™ä¸ªçš„è¯å‘¢æ˜¯å°±ç”¨äºè¿™ä¸ªå¤§æ¨¡å‹çš„ä¸€ä¸ªä¸€ä¸ªå¾®è°ƒå•Šï¼Œè¿™æ¯”å¦‚è¯´æˆ‘ä»¬æœ‰å¾ˆå¤šé«˜è´¨é‡çš„æ•°æ®çš„æƒ…å†µä¸‹ã€‚

å®ƒå¯ä»¥å‘ƒå¸®åŠ©æˆ‘ä»¬æ›´å¤šçš„å»å‘ƒæŠŠè¿™ä¸ªä»»åŠ¡è‡ªåŠ¨åŒ–å•Šã€‚æˆ‘ä»¬çŸ¥é“æœ‰æ‰‹å·¥å‘ƒã€‚refor learningçš„è¯å‘¢ï¼Œéœ€è¦å¾ˆå¤šå¾ˆå¤šçš„å·¥ä½œï¼Œéœ€è¦åšæ•°æ®ã€‚å•Šï¼Œè¿™ä¸ªçš„è¯å‘¢å·¥ä½œå·²ç»åœ¨å‘ƒå¤§å®¶å¯ä»¥çœ‹åˆ°ã€‚

åœ¨gi upubä¸Šå«open chatï¼Œå‘ƒï¼Œå¤§å®¶éƒ½å¯ä»¥çœ‹åˆ°ã€‚å•Šï¼Œä¹Ÿç°åœ¨æ˜¯æ¯”è¾ƒæ€•å‘ƒæ¯”è¾ƒå—æ¬¢è¿çš„ä¸ªæŠ€æœ¯ã€‚é‚£ä¹ˆå¦å¤–çš„è¯å‘¢ã€‚

å°±æ˜¯æˆ‘ä»¬ä¹Ÿå‘ç°ç›®å‰åœ¨å•Šè¿™ä¸ªreinfor learning humanman feedbackbacké‡Œé¢å‘¢æœ‰äº›é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å‘ƒä»–çš„è¿™ä¸ªå‘ƒæ ·æœ¬å’Œç­–ç•¥å‘ƒçš„å­¦ä¹ ç›®æ ‡å‘¢æ˜¯ä¸åŒ¹é…çš„ã€‚

å°±æ˜¯curingå’Œ policy misalignmentã€‚æ‰€ä»¥ä¸€å¼€å§‹ç­‰äºä½ è®¤ä¸ºæ˜¯lineï¼Œä½†æ˜¯èµ°èµ°èµ°èµ°ä¹‹åï¼Œä»–å°±å»åç¦»è¿™ä¸ªæ–¹å‘ã€‚æ‰€ä»¥æˆ‘ä»¬ä¹Ÿæå‡ºä¸€ä¸ªå‘ƒä¸€ä¸ªæ–°çš„æŠ€æœ¯ã€‚

ç„¶åä½¿å¾—ä»–åœ¨å­¦ä¹ å°±æˆ‘ä»¬çš„goå’Œ trajectoryjectæ˜¯s lightã€‚é‚£ä¹ˆæˆ‘ä»¬åº”è¯¥åœ¨å‘ƒä¸‹é¢è¿™ä¸ªæ˜ŸæœŸé˜¿clecleå‘ƒä¼šã€‚å°±è°ˆåˆ°è¿™ä¸ªå·¥ä½œã€‚å°±å¦å¤–çš„è¯å‘¢ï¼Œæˆ‘ä»¬ç”¨äº†ä¸å°‘å®‰å…¨ç¦»çº¿çš„è¿™ä¸ªå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•å•Šã€‚

ç„¶åå‘¢å»æŠŠè¿™ä¸ªå®‰å…¨çš„ç­–ç•¥è¿›è¡Œå•Šè¿™ä¸ªæ”¹è¿›ã€‚å‘ƒï¼Œç‰¹åˆ«æ˜¯å…¶å®å‘¢å¦‚æœæˆ‘ä»¬é¦–å…ˆè¦åˆ¤æ–­ä¸€ä¸ªä¸œè¥¿æ˜¯å®ƒæ˜¯å±äºå®‰å…¨å‘¢è¿˜æ˜¯ä¸å®‰å…¨ï¼Œå°±è¦æŠŠè¿™ä¸ªåŒºåŸŸè¦æ‰¾åˆ°ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªåŒºåŸŸé‡Œé¢çš„è¯å‘¢ï¼Œä½ å¯ä»¥åšæœ€å¤§åŒ–çš„ä¸€ä¸ªå¥–åŠ±ã€‚å•Šã€‚

å¦‚æœåœ¨åŒºåŸŸå¤–é¢çš„è¯å‘¢ï¼Œä½ è¦åšæœ€å°åŒ–çš„è¿™ä¸ªé£é™©ï¼Œä¸€ä¸ªè¦maximiseä¸€ä¸ª minimizeiseé‚£è¿™é‡Œé¢å¦‚æœå‘ƒè¿™ä¸ªè¿™ä¸ªå‘ƒçœ‹æˆ‘æˆ‘ä»¬çš„paperçš„è¯å‘¢ï¼Œè¿™é‡Œé¢éƒ½æ˜¯éƒ½æ˜¯mathematicsï¼Œéƒ½æ˜¯æ•°å­¦ã€‚

æ‰€ä»¥æˆ‘å°±æƒ³è®©å¤§å®¶çŸ¥é“å‘¢å•Šè¿™ä¸ªå®‰å…¨çš„é—®é¢˜ï¼Œå¯¹é½çš„é—®é¢˜ï¼Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªç­–ç•¥å’Œè¿™ä¸ªç®€å•çš„å•Šä¸€äº›è¿™ä¸ªå¯¹ä¸€äº›ç®—æ³•ï¼Œè¿™é‡Œé¢å…¶å®æœ‰å¾ˆå¤šå‘ƒç†è®ºæ–¹é¢çš„ä¸€äº›å‘ƒåˆ›æ–°å’Œçªç ´ã€‚è¿™ä¸ªæ–‡ç« çš„è¯å‘¢ï¼Œæˆ‘ä»¬ä¼šåœ¨å•Šä¹Ÿæ˜¯å‘ƒã€‚è¿™ä¸ªã€‚

åº”è¯¥å·²ç»å·²ç»å‘è¡¨äº†å•Šå‘è¡¨äº†å«é˜¿cleã€‚ç„¶åICMLæˆ‘ä»¬ä¹Ÿæœ‰å‘ƒä¸€ç¯‡è¿™æ ·çš„ã€‚é‚£ä¹ˆæœ€åæ—¶é—´ä¸å¤šå‘¢ï¼Œæˆ‘æƒ³è°ˆä¸€äº›æˆ‘è‡ªå·±çš„ä¸€äº›å»ºè®®ã€‚åˆšæ‰æ˜¯åœ¨æŠ€æœ¯æ–¹é¢çš„ä¸€äº›å·¥ä½œã€‚å•Šã€‚

æˆ‘ä¸çŸ¥é“è€€ä¸œä¼šä¸ä¼šè®²è€€ä¸œå’Œå’Œé«˜å®‹ä»–ä»¬å‡ ä½åœ¨è¿™æ–¹é¢åšçš„éƒ½ç‰¹åˆ«é¢†å…ˆçš„å­¦è€…ï¼Œä»–ä»¬ä»¥åä¼šè®²æ›´å¤šç»†èŠ‚ã€‚å•Šï¼Œé‚£æˆ‘å‘¢æƒ³æä¸€ç‚¹å°±æ˜¯æ”¿ç­–æ–¹é¢çš„ä¸€äº›å»ºè®®å•Šï¼Œè¿™ä¸ªå…¶å®è®²äº†å·®ä¸å¤šä¸¤å¹´äº†ï¼Œè®²ä¸¤å¹´äº†ã€‚å‘ƒï¼Œæˆ‘è¿™å„¿æœ‰æ²¡æœ‰ä¸ªç« ã€‚

æˆ‘çœ‹æœ‰æ²¡æœ‰ã€‚æˆ‘è¦ç›–ä¸ªç« ï¼Œå¯¹æˆ‘è¦ç›–ä¸ªç« çš„è¯å‘¢ï¼Œå°±æ˜¯è¯´æˆ‘è®²çš„è¿™ä¸ªå»ºè®®å®Œå…¨æ˜¯ä¸ªäººå»ºè®®ï¼Œä¸ä»£è¡¨å‘ƒæ¸…åå¤§å­¦ï¼Œä¸ä»£è¡¨æ¸…åå¤§å­¦airï¼Œä¹Ÿä¸ä»£è¡¨æˆ‘ä»¬ç°åœ¨æ‰€æœ‰çš„å›¢ä½“ã€‚å› ä¸ºæˆ‘ä»¬åœ¨åœ¨è¿™å†…éƒ¨æœ‰å¾ˆå¤šä¸åŒçš„è§‚ç‚¹å•Šã€‚ğŸ˜Šï¼Œå•Šã€‚

å®Œå…¨æ˜¯ä¸ªäººå»ºè®®ã€‚è¿™ä¸ªå…¶å®æˆ‘æäº†å·®ä¸å¤šä¸¤å¹´åˆ°3å¹´äº†å•Šï¼Œæˆ‘æäº†10ä¸ªå»ºè®®ï¼Œæˆ‘ä»Šå¤©æ—¶é—´å…³ç³»æˆ‘è®²5ä¸ªã€‚ç¬¬ä¸€ä¸ªçš„è¯å‘¢å°±æ˜¯æˆ‘ä¸€ç›´å»ºè®®å»ºè®®å»ºè®®æˆ‘ä»¬è¦å‘ƒå»ºç«‹è¿™ä¸ªåˆ†çº§ä½“ç³»ã€‚å› ä¸ºç°åœ¨AIé‡Œé¢æœ‰å¾ˆå¤šä¸åŒçš„ç®—æ³•ã€‚

æœ‰å¾ˆå¤šä¸åŒçš„æ¨¡å‹ã€‚é‚£æˆ‘ä»¬å‘¢è¦å¯¹è¿™ä¸ªå°±æ˜¯æœ€å‰æ²¿çš„å•Šï¼Œæ¯”å¦‚è¯´è¶…è¿‡ä¸‡äº¿å‚æ•°ä»¥åï¼Œå¯èƒ½æ›´å¤šçš„å‚æ•°å‘¢ã€‚å¯¹ä»–è¿›è¡Œçº¦æŸã€‚ä¸€èˆ¬çš„æ¨¡å‹ï¼Œä¸€èˆ¬çš„ç®—æ³•å‘¢ã€‚å°±ä¸è¦å¤ªå»è§„èŒƒå®ƒï¼Œè®©ä»–å‘ƒå¾€å‰é¢å‘å±•ï¼Œå°±å¯¹è¿™ç§ç‰¹åˆ«é£é™©æ¯”è¾ƒå¤§ã€‚

èƒ½åŠ›æ¯”è¾ƒå¤§çš„è¿™ç§å«å‰æ²¿çš„è¶…å¤§å‹æ¨¡å‹éœ€è¦å»ã€‚æœ‰äº›è§„èŒƒã€‚å‘ƒï¼Œå› æˆ‘åšæ— äººé©¾é©¶ï¼Œæˆ‘ä»¬è¿™é‡Œé¢è‡ªåŠ¨é©¾é©¶æˆ‘é‡Œé¢åˆ†æˆæ¯”å¦‚6çº§ï¼Œä»L0åˆ°L56çº§å•Šï¼Œæˆ‘å»ºè®®æˆ‘ä»¬æŠŠè¿™ä¸ªå‘ƒæ¨¡å‹ä¹Ÿåˆ†æˆL0åˆ°åˆ°L5ï¼Œåªæœ‰L5çš„ï¼Œæˆ‘ä»¬å»è§„èŒƒå®ƒã€‚

é‚£ç¬¬äºŒç‚¹çš„è¯å‘¢ï¼Œé™¤äº†æ¨¡å‹æœ¬èº«çš„è¿™ä¸ªè§„èŒƒï¼Œå°±è§„èŒƒåŒ…æ‹¬ä»æ•°æ®ä»æ¨¡å‹çš„å‘ƒè¿™ä¸ªæ„å»ºå•Šï¼Œä»è¿™ä¸ªå¯¹é½åˆ°æœ€åè¯„ä¼°å•Šå„ç§å„ç§è¯„ä¼°éƒ½éœ€è¦æœ‰ä¸€å¥—ä»€ä¹ˆæ ‡å‡†ï¼Œæ›´ä¸¥æ ¼çš„æ ‡å‡†ã€‚é‚£ç¬¬äºŒç‚¹ç”¨åˆ°åœºæ™¯é‡Œé¢éœ€è¦æ›´å¤šçš„çº¦æŸå•Šã€‚

ä½ æ¯”å¦‚è¯´ç”¨åˆ°æ— äººè½¦é‡Œé¢ï¼Œæ— äººè½¦é‡Œé¢çš„è¿™ä¸ªå®‰å…¨å•Šï¼Œæ— äººè½¦é‡Œé¢å®ƒæœ¬èº«çš„å‘ƒå®ƒçš„è‡ªå·±çš„è¿™äº›è¯„ä¼°çš„ä½“ç³»è¦æ‹‰è¿›æ¥ã€‚ä½ åšåŒ»å­¦é‡Œé¢ã€‚æ¯”å¦‚è¯´å‘ƒåŒ»ç–—æœºå™¨äººå•Šï¼Œä»–å¿…é¡»è¦ç»è¿‡åŒ»å­¦æ–¹é¢çš„è¿™è¿™ä¸ªåœºæ™¯å’Œé¢†åŸŸçš„è¿™ä¸ªçº¦æŸã€‚ç¬¬äºŒçš„è¯å‘¢ã€‚

æˆ‘ä¹Ÿè®²äº†å¾ˆå¤šå¹´äº†ã€‚å°±æ˜¯æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªå®ä½“çš„æ˜ å°„æœºåˆ¶å•Šï¼Œé¦–å…ˆæ˜¯å¯¹AIçš„å†…å®¹è¦æ ‡æ³¨å’Œæ ‡æ³¨ã€‚å°±æ¯”å¦‚è¯´æˆ‘ç°åœ¨äº§ç”Ÿäº†å¾ˆå¤šæ•°æ•°å­—äººï¼Œæ•°å­—äººå’ŒçœŸäººåŸºæœ¬ä¸Šçœ‹ä¸å‡ºåŒºåˆ«ã€‚æˆ‘è¦æ ‡æ³¨æˆ‘è¿™æ˜¯AIäººï¼Œè™šæ‹Ÿäººã€‚æˆ‘AIäº§ç”Ÿå†…å®¹ã€‚

æˆ‘è¦æ ‡æ³¨æ˜¯AIäº§ç”Ÿçš„ã€‚å‘ƒï¼Œæˆ‘ä»¬ç°åœ¨å‘¢è¿™ä¸ªè§„å®šå•Šï¼Œå’±ä»¬å‘ƒå›½å®¶è§„å®šï¼Œç¾å›½ä¹Ÿè¿™æ ·è§„å®šäº†ã€‚ä½ æ¯”å¦‚åšä¸ªå¹¿å‘Šï¼Œåœ¨äº’è”ç½‘åšå¹¿å‘Šã€‚å¦‚æœæ˜¯å¹¿å‘Šï¼Œä½ ä¹Ÿå†™ä¸ªå¹¿å‘Šã€‚ä½†æˆ‘å¦‚æœæä¸€ä¸ªè¿™ä¸ªè™šæ‹Ÿäººæ•°å­—äººï¼Œæˆ‘éƒ½ä¸éœ€è¦è¯´æˆ‘æ˜¯AIäº§ç”Ÿçš„ã€‚

æˆ‘é¦–å…ˆå°±æ˜¯ä¸ªç®€å•çš„æŠŠå®ƒæ ‡è¯†å‡ºæ¥ã€‚å¤§å®¶çŸ¥é“è¿™æ˜¯AIäº§ç”Ÿçš„è¿˜æ˜¯äººä¸ºäº§ç”Ÿçš„ã€‚ç¬¬äºŒçš„è¯å‘¢å°±æ˜¯ä¸€å®šè¦æœ‰ä¸€ä¸ªå®ä½“æ˜ å°„çš„æœºåˆ¶ã€‚æˆ‘ä»¬ä»¥åæœ‰å¾ˆå¤šæœºå™¨äººã€‚æœ‰å¾ˆå¤šå•Šå¯ä»¥æ˜¯çœŸæ­£çš„æœºå™¨äººï¼Œä¹Ÿå¯ä»¥æ˜¯è™šæ‹Ÿçš„æœºå™¨äººã€‚æœ‰å¾ˆå¤šæ™ºèƒ½ä½“ã€‚

é‚£ä¹ˆè¿™ä¸ªæ™ºèƒ½ä½“å®ƒåº”ä¸ªæ˜¯ä»å±ä½“ï¼Œå®ƒä»å±äºæˆ‘æŸä¸ªäººæˆ–è€…æŸä¸ªæœºæ„çš„ã€‚æˆ‘çš„æœºå™¨äººçŠ¯äº‹äº†ï¼Œæˆ‘æœ€åè¦è¿½æº¯åˆ°å®ƒçš„ä¸»ä½“é‡Œé¢å»å•Šï¼Œæ‰€ä»¥ownershipä¸€å®šæ˜¯äººå•Šï¼Œäººæˆ–è€…æ˜¯ä¸ªcompanyå•Šã€‚

ä¸€ä¸ªæ˜¯ä¸ªlegal entityã€‚é‚£ä¹ˆè¿™ä¸ªäº‹æƒ…å…¶å®ä»å‘ƒä»è¿™ä¸ªæŠ€æœ¯ä¸Šæ¥è®²ï¼Œå¹¶ä¸æ˜¯å¾ˆéš¾ï¼Œæ˜¯å¯ä»¥å®Œå…¨å¯ä»¥åšåˆ°çš„å•Šï¼Œä½†æ˜¯æˆ‘è¿™æ˜¯æ›´å¤šçš„ä¸€ä¸ªæ”¿ç­–æ–¹é¢çš„å»ºè®®ã€‚ç¬¬ä¸‰ä¸ªå‘¢ï¼Œå°±æˆ‘ä¸€ç›´å»ºè®®ã€‚æŠŠæˆ‘ä»¬æŠŠ10%çš„è¿™ä¸ªæŠ•å…¥å•Šã€‚

å°±æ˜¯åšAIç ”ç©¶çš„ä¹Ÿå¥½ï¼Œäº§å“å¼€å‘ä¹Ÿå¥½ï¼ŒæŠ•å…¥å‘¢æ”¾åˆ°å•Šå¯¹å®‰å…¨å•Šå’Œé£é™©çš„è¿™ä¸ªé¢†åŸŸæ¥ã€‚æˆ‘ä»¬åœ¨å…¨çƒæˆ‘ä»¬å¤§å®¶æ˜¯å»ºè®®30%ã€‚å‘ƒï¼Œåœ¨å›½å†…ï¼Œæˆ‘è¯´æˆ‘ä»¬å…ˆä»10%åšèµ·ï¼Œä»¥åæ…¢æ…¢åˆ°30%ã€‚è¿™ä¸ªåŒ…æ‹¬æˆ‘ä»¬çš„åŸºç¡€ç ”ç©¶ç»è´¹ã€‚

æˆ‘ä»¬çš„äº§å“å¼€å‘ç»è´¹å•Šï¼ŒåŒ…æ‹¬æˆ‘ä»¬æ•´ä¸ªè¿™ä¸ªç¤¾ä¼šçš„æŠ•å…¥ï¼Œæˆ‘ä»¬å…ˆåˆ°10%ä½œä¸ºç¬¬ä¸€ä¸ªä¸€ä¸ªèµ·ç‚¹ç¬¬ä¸€ä¸ªèµ·ç‚¹ã€‚é‚£ç¬¬å››ä¸ªå°±æ˜¯è®¾ç«‹ä¸€äº›å¾ˆæ¸…æ™°çš„è¿™ä¸ªçº¢çº¿å’Œè¾¹ç•Œã€‚è¿™ä¸ªçº¢çº¿è¾¹ç•Œå…¶å®è¦è®¾ç«‹èµ·æ¥ï¼Œå…¶å®ä¸å®¹æ˜“çš„ã€‚

å› ä¸ºæ¯ä¸ªå›½å®¶å‘ƒå¯èƒ½è¿™ä¸ªæœ‰ä¸åŒçš„æƒ…å†µã€‚ä½†æ˜¯æˆ‘è§‰å¾—æœ‰ä¸€äº›å¤§å®¶å¯ä»¥è®¾ç«‹çš„ï¼Œæˆ‘ä»¬è¦è®¾ç«‹ä»€ä¹ˆä¸èƒ½åšï¼Œæ¯”å¦‚è¯´æˆ‘ä»å¾ˆå¤šå¹´æˆ‘å°±ææˆ‘ä»¬åšæ™ºèƒ½ä½“çš„æ—¶å€™ï¼Œæ™ºèƒ½ä½“ï¼Œç°åœ¨è‡ªå·±å¯ä»¥å»ä»–å¯ä»¥ copyã€‚

ä»–è‡ªå·±å¯ä»¥å»å»å»å»è¿™ä¸ªè¿™ä¸ªå¤åˆ¶çš„ï¼Œé‚£å¤åˆ¶çš„æ—¶å€™ï¼Œå¤åˆ¶çš„æ—¶å€™æˆ‘è¦ç»è¿‡äººçš„åŒæ„ï¼Œæ¯”å¦‚è¯´æˆ‘æ˜¯è¿™ä¸ªä¸»ä½“ï¼Œæˆ‘è¦åŒæ„çš„ã€‚ä½ å¤åˆ¶ä¸€ä¸ªå‘ƒè¿™ä¸ªå¼ äºšé’ï¼Œå¼ äºšé’è¦å»è¦åŒæ„ä½ å»å¤åˆ¶ï¼Œå¯¹å§ï¼Ÿæ‰€è¿™ä¸ªå‘ƒä¸èƒ½è‡ªæˆ‘å¤åˆ¶ï¼Œæ²¡æœ‰é™åˆ¶çš„ã€‚

å¤åˆ¶ï¼Œç„¶åå®ƒæœ‰çº¢çº¿è¾¹ç•Œï¼Œæ¯”å¦‚è¯´å‘ƒå¤§æ¨¡å‹æ¥åˆ°æ ¸ç”µæ ¸ç”µç«™çš„æ—¶å€™ã€‚æ€ä¹ˆæ¥èƒ½ä¸èƒ½æ¥å•Šï¼Ÿæˆ‘ä¸ªäººå»ºè®®åœ¨æˆ‘ä»¬åœ¨è¿™äº›å•Šå¤§æ¨¡å‹è¿˜æ²¡æœ‰ææ¸…æ¥šå‘ƒè¿™ä¸ªè¿™ä¸ªè¿™äº›å‘ƒè¾¹ç•Œå•Šå‘ƒæ²¡æœ‰ææ¸…æ¥šè¿™äº›é‡Œé¢çš„å¯è§£é‡Šæ€§å‰é¢çš„ã€‚

å…ˆä¸è¦æ¥è¿™äº›ç‰¹åˆ«å…³é”®çš„å•Š criticalical infrastructureã€‚æœ€åå¤§å®¶å¾ˆå¤šäººéƒ½è®²è¿‡äº†ï¼Œå°±æˆ‘ä»¬è¦ä¸€ä¸ªå›½é™…æ²Ÿé€šçš„åˆä½œå’Œåè°ƒæœºåˆ¶ï¼ŒåŒ…æ‹¬æ ‡å‡†ï¼ŒåŒ…æ‹¬è¯„ä¼°ï¼ŒåŒ…æ‹¬å•Šè¿™ä¸ªåˆä½œçš„å…·ä½“çš„ä¸€äº›ä¸€äº›äº›æ–¹å¼ã€‚

è¿™é‡Œé¢éœ€è¦æœ‰ä¸“å®¶çš„ï¼Œéœ€è¦æœ‰å‘ƒæ”¿ç­–åˆ¶å®šè€…çš„ï¼Œéœ€è¦æœ‰æ”¿åºœçš„ã€‚ä½†å¾ˆé‡è¦çš„éœ€è¦è¿™ä¸ªè¿™äº›ä¸åŒé¢†åŸŸçš„äººåœ¨ä¸€èµ·å•Šï¼Œåœ¨ä¸€èµ·è¿™ä¸ªç²¾è¯šçš„åˆä½œã€‚å¥½ï¼Œæˆ‘å°±å‘ƒè®²è®²è¿™ä¹ˆå¤šå‘ƒï¼Œè¿™ä¸ªè°¢è°¢å¤§å®¶ã€‚ğŸ˜Šï¼ŒThank you so muchã€‚

 Professor Zhangï¼Œ for your excellent presentation and suggestionsã€‚ğŸ˜Šï¼ŒNextã€‚

 I'm now pleased to welcome Professor Da Soï¼Œ a professor in computer science at Uã€‚

 C Berkeley and codirector of the Berkeley Center on Responsible decentralized Intelligenceã€‚ğŸ˜Šã€‚

Her research focuses on AI safety and securityï¼Œ and she is ranked the most highly cied scholar in computer securityã€‚

ğŸ˜Šï¼ŒShe is the recipient of numerous awardsï¼Œ including the MacArthur Fellowshipã€‚

 Guggenheim Fellowship and more than 10 test of time awards and best paper awardsã€‚ Daã€‚

 it's a pleasure to have you here in Shanghai with usã€‚ I'll let you take it from hereã€‚ğŸ˜Šï¼ŒGreatã€‚ğŸ˜Šã€‚

Thanksï¼Œ everyone for being hereã€‚ Yesï¼Œ My name is Don Soã€‚ I'm a professor at UC Berkeleyã€‚ Todayã€‚

 I'll talk about AI safety challenges and future directionsã€‚

 So the presentations earlier have set great context and backgroundã€‚

 And here I wanted to add some more emphasis in particularï¼Œ as we deploy machine learningã€‚

 it's really important to consider the presence of attackers for a number of reasonsã€‚

 So first history has shown that attackers always follows the footsteps of new technology development sometimes even liã€‚

 And also this time the stake is even higher with AI as AI controls a more and more systemsã€‚

 attackers will have higher higher incentives to compromise the systemsã€‚

 And also as AI becomes more and more capableï¼Œ the consequence of misuse by attackers will also become more severeã€‚

 And henceï¼Œ it's really important toã€‚ğŸ˜Šï¼ŒConsider the presence of attackersã€‚

 especially as we consider AI safetyã€‚ğŸ˜Šï¼ŒSo firstï¼Œ I want to talk a little more about AI safety in the presence of attackersã€‚

ğŸ˜Šï¼ŒFrom my group's earlier work and also other research workã€‚

 we have shown that adversarial attacks are prevalent in deep learning systemsã€‚Essentiallyã€‚

 all deep learning systems todayï¼Œ they are all vulnerable to different types of adversarial attacksã€‚

ğŸ˜Šï¼ŒAnd the number of papers in the space actually has grown exponentially since our earlier work and people's earlier work in the early stagesã€‚

 And also we had the rare honor of having some of the artifact of our our earlier work actually now as part of the permanent collection at the Science Museum of Londonã€‚

ğŸ˜Šï¼ŒSo as we talk about safety and today talk about safety aligned large language modelsã€‚

 it's also important to consider the adversarial settingã€‚ğŸ˜Šï¼ŒSo unfortunatelyã€‚

 as our work and also others work have shown that these large language models are also really vulnerable to adversarial attacksã€‚

 and these safety alignment mechanisms are easily brokenã€‚ğŸ˜Šï¼ŒSo in our recent workï¼Œ as an exampleã€‚

 deco trustï¼Œ which provides the first comprehensive evaluation framework for trustworthiness of large language modelsã€‚

 its actually won the outstanding paper Award at Europes this past Decemberã€‚ğŸ˜Šã€‚

We developed new algorithms and also different environmentsã€‚

 including benign averar environments to evaluate many different perspectives for safety and trustworthness of large language modelsã€‚

ğŸ˜Šï¼ŒAnd our work have shown that for all these different perspectivesï¼Œ including aversar robustnessã€‚

 toxicity and fairness and many othersï¼Œ essentially these large language models are all very easily attacked by aversar attacksã€‚

ğŸ˜Šï¼ŒAnd againï¼Œ for more detailsï¼Œ you can go look at our paper at decoding trust Github dot I Oã€‚ğŸ˜Šã€‚

And alsoï¼Œ these adversary attacks are effective multimod models as wellã€‚

 And also others's work have shown that even as these models are being fine tuneedã€‚

 attackers actually by providing just a few very small number of adversarial adversary design data pointsã€‚

 the this fine tune stage can essentially cause this fine model to easily lose the safety alignmentsã€‚

ğŸ˜Šï¼ŒSo so far I've talked about rightï¼Œ So these attacksã€‚

 they are not only effective at at the inference timeã€‚

 They are also effective at essentially fighting stageï¼Œ as I just mentionedã€‚

 essentially this is called data poisoning as wellã€‚ğŸ˜Šï¼ŒAnd also through this data poisoning stepã€‚

 also these models can have what we call very stealthy behavior where essentially called backdoor as wellã€‚

 So in our earlier workï¼Œ we showed that through data poisoningã€‚

 the model can attackers can be in backdo in the model such thatï¼Œ for exampleã€‚

 in our earlier work in facial recognitionã€‚ğŸ˜Šï¼ŒTheï¼Œ the model under normal circumstance will just behave normally and give correct facial recognition resultsã€‚

 But howeverï¼Œ when anyone that wears a special type of glassesã€‚ğŸ˜Šã€‚

This actually is even effective in the physical worldã€‚

 then it will cause the model to essentially trigger the spec that the model will misrecogognize this person wearing this particular type of glasses as a targeted personã€‚

ğŸ˜Šï¼ŒAnd through recent work by Anthropicï¼Œ they have also shown this type of backdoor phenomenon where a fine- tuned large language model during normal circumstance with a normal promptã€‚

 it can generate like normal code that's usually cracked codeã€‚

 but when a particular trigger freeze appears in the promptã€‚

 the model actually will generate a vulnerable codeã€‚ğŸ˜Šï¼ŒSoã€‚

These are different types of adversary attacksã€‚In the entire communityã€‚

 we have been very productive and creative in coming up with different types of new attack methodsã€‚ğŸ˜Šã€‚

Howeverï¼Œ on the other handï¼Œ unfortunatelyï¼Œ in the defense sideï¼Œ we have seen veryï¼Œ very lowã€‚

 very little progressã€‚And todayï¼Œ there is no effective general adversar defenseã€‚

So this illustrates that this is the first open challenge that I wanted to pose in the context of AI safetyã€‚

ğŸ˜Šï¼ŒAnd the current AI safety alignment mechanisms are very easily evaded by adversial attacksã€‚ğŸ˜Šã€‚

And and anyã€‚the A5 mechanisms need to be resilient against these adversarial attacksã€‚

And hence this poses a huge open challengeã€‚ So essentiallyï¼Œ in order to achieve AI safetyã€‚

 we need to actually be able to solve adversarial robustness as a prerequisiteã€‚

 and as I just mentionedï¼Œ despite that now we have thousands of papers every yearã€‚

 publishing on different types of adversarial attacksã€‚

 But the entire community essentially have made almost zero progress in defenses against these adversarial attacksã€‚

 So as for developing effective AI safety as a whole communityã€‚

 we really need to push forward in how we can develop better defenses so that we can develop AI safety mechanisms that are resilient against adversarial attacksã€‚

ğŸ˜Šï¼ŒSo what are the potential directions that can help us to achieve the goalã€‚

 So here I'll give a couple examples from some of our recent workã€‚ğŸ˜Šã€‚

So one work is what we call representation engineeringã€‚

 and this is a top down approach to AI transparencyã€‚ğŸ˜Šï¼ŒSo in this caseï¼Œ weã€‚

By providing the the model with the contrast inputs as stimulus for certain tasksã€‚ğŸ˜Šã€‚

So we provide these contrast inputs to the modelï¼Œ and then we monitor the activation of the neural networks at different layers and then build the modelsã€‚

ğŸ˜Šï¼ŒAnd with our recent workï¼Œ we show that by through this methodã€‚

 we can actually identify certain directions along at certain layers that actually correlates with different classesã€‚

 different types of behaviors of the monoã€‚ Soï¼Œ essentiallyï¼Œ for exampleã€‚

 we can identify certain directions that actually correlates with behaviorsã€‚

 whether the model is honest or not honestï¼Œ whether it's hallucinating or not hallucinating and so onã€‚

ğŸ˜Šï¼ŒAnd furthermoreï¼Œ our work with collaborators and so onï¼Œ we have also shown thatã€‚ğŸ˜Šã€‚

A particular this type of method called representation controlã€‚

 So not only that we can do representation readingï¼Œ which is to monitor the model's behaviorã€‚

 we can actually modify the activations of these neurons at certain layers during the inference time alongã€‚

 for exampleï¼Œ the identify directions and and so onã€‚ And then through this wayã€‚

 we can actually then change the model behavior for certain classesï¼Œ for exampleï¼Œ using this methodã€‚

 we can make the model of behavior more honest or less honest and so onã€‚ğŸ˜Šï¼ŒSo why is this importantã€‚

 I think this is one of the keyã€‚The key distinctionï¼Œ actuallyã€‚

 between human brains and artificial brainsï¼Œ artificial neural networks is that for these artificial neural networksã€‚

 we actually are in control in the sense that we can completely observe the activitiesã€‚

 the activities of the neural networks and also in real timeã€‚

 we can modify the the activations of the neural networksã€‚

 So this actually gives us a powerful arsenal for potentially for AI safetyã€‚

 So this allows us to observe and monitor the behaviors of the neural networks and then better provide better control and enforcement of the behaviors of the neural networksã€‚

 So hence this can be a really promising direction for providing AI safety control mechanismsã€‚ğŸ˜Šã€‚

Howevereverã€‚Con mechanismï¼Œ is's promisingï¼Œ but it's difficult to actually give full guaranteesã€‚

So also as professorfessor Ya Qin and and also Yaoshua mentioned earlierï¼Œ Ilyã€‚

 we actually want to have approval guaranteesã€‚ So recentlyã€‚

 we have a joint initiative on quantitative AI safetyã€‚

 And the goal is to actually develop AI safety that's with guarantees essentially with safe by design with with guarantees for safetyã€‚

ğŸ˜Šï¼ŒThis actually is also in parallel and in some senseã€‚

 inspired by the approach taken in cyber securityã€‚ So including my own work in the last 25 years in cyber securityã€‚

 we have movedï¼Œ essentially we have had paradigm shifts in how we approach safetyã€‚

 how we attribute secure systemsã€‚ğŸ˜Šï¼ŒOn the whole committee focused on reactive defenseã€‚

 how we detect attacksã€‚ And so it's kind of like how we todayã€‚

 how we detect when large language models is is behaving wrongã€‚ And then later onã€‚

 we worked on methods to as proactiveã€‚ğŸ˜Šï¼Œfocus on bugfin is actually trying to find vulnerabilities in these systemsã€‚

 So it's kind of like today we are doing these evaluationsã€‚

 We are trying to find some kind of vulnerabilities in these large language modelsï¼Œ and so onã€‚ğŸ˜Šã€‚

But howeverï¼Œ these approaches are unsatisfactory for a number of reasons that I don't have time to get intoã€‚

 And in the endï¼Œ the committee realized that the best approach for achieving security is what we call secured by design are secured by constructionã€‚

 So with this type of approach essentiallyï¼Œ we can develop systems that provide approval guarantee of its security that is satisfies certain security properties by the design and construction of the systemã€‚

 and this is in contrast to the other types of defenses that I mentioned earlierã€‚ğŸ˜Šã€‚

Which helps us to essentially get out of the cat mouse game and also provides approvalable guaranteesã€‚

 And the methods used in this is through formal verificationã€‚ So with formal verification firstã€‚

 we provide formal specific of a desired propertiesã€‚ and then through formal verificationã€‚

 we can then formally verify that the system it secureã€‚ğŸ˜Šã€‚

E through satifies the desired properties through verificationã€‚

And this also can be done at different levelsï¼Œ including the design level and the implementation levelã€‚

ğŸ˜Šï¼ŒAnd in fact in the past decadeï¼Œ the community actually has entered into what I call the era of formally verified systemsã€‚

 where we actually have many different types of systemsã€‚

 including microchs and file systems and compilers and so on and that are formally verified Howeverã€‚

 the issue for for these systems is that it's extremely labor intensive to prove to prove these systemsã€‚

 the security property of the systemsã€‚ Each one often takes tens of proof engineer yearsã€‚

 So it's not scalableã€‚ğŸ˜Šï¼ŒMy group in collaboration with others at Open eyeã€‚

 we were among the first to use deep learning for theory improvingã€‚ğŸ˜Šï¼ŒAnd this wasã€‚

 this work was done quite a few back way before large language models and so onã€‚

So today with the large language models technologiesã€‚

 we hope that we can take this work even further where we can instead training in the pastã€‚

 for exampleï¼Œ training AI agents to play goal we can instead train AI agents to automatically prove thes and verify programs with this approach in conjunction with program synthesisã€‚

 which my group also has done a lot of work in the past was among the pioneers in the space as wellã€‚

 and we hope that by combining these approaches with automatic improving program verification program synthesisã€‚

 we can provide automatically probablyably secure code producing code with proofs attached to itã€‚ğŸ˜Šã€‚

And with thisã€‚We can use AI to build properly secure systemsã€‚

 essentially achieve secure by design or save by designã€‚ğŸ˜Šã€‚

And this can help us to reduce arms risk to provide properly secure systems that are resilient against certain classes of attacksã€‚

ğŸ˜Šï¼ŒSo this approachï¼Œ I thinkï¼Œ is extremely promising to solve certain classes of problemsã€‚

 but howeverï¼Œ it still has a number of open challengesã€‚ğŸ˜Šï¼ŒFirstã€‚

 this type of formal verification approach mainly applies to traditional symbolic programsã€‚

 but it can be difficult to apply to non symbolic programs such as a deep neural networksã€‚ğŸ˜Šã€‚

AndW whichch where we don't even have precisely specified properties and goalsã€‚ Soï¼Œ for exampleã€‚

 if I want to guarantee that a self driving car doesn'tã€‚A driveri over a pedestrianã€‚

 We don't even have a formal specification of what a pedestrian isã€‚And alsoï¼Œ in the futureã€‚

 essentiallyï¼Œ all systemsï¼Œ most of the systems will be hybridã€‚

 They will be combining symbolic and non symbolic componentsã€‚ğŸ˜Šã€‚

So formal verification and secure by constructionï¼Œ how can we apply this approach to this type of hybrid systemsã€‚

 It's still a big open questionã€‚ğŸ˜Šï¼ŒSoï¼Œ to concludeã€‚As we all you know discussed and agree hereã€‚

 AI safety is extremely important as we move forward with stronger capabilities of AIã€‚

 It's paramountã€‚ it's a paramount importance that we guarantee the safety of these systemsã€‚

 But howeverï¼Œ there are still many challengesã€‚ It's important to consider AI safety in atmosphere settingã€‚

 andã€‚ğŸ˜Šï¼ŒAnd I think it'sã€‚It can be very productive to develop methods using activation steering representing control to build as an important arsenal for controlling model behaviorsã€‚

 and also finallyï¼Œ we hope that we can really develop new approaches and mechanisms to enable secure by design and by design for building secure AI systems with pro guaranteesã€‚

ğŸ˜Šï¼ŒThank youã€‚Thank you so muchï¼Œ Joanï¼Œ please stay on stage as we transition to our panelã€‚ğŸ˜Šã€‚

We will wrap up our session on AI safety with a panel on AI safety research directionsã€‚

For the other panelistsï¼Œ please kindly start coming up on stage as I introduce youã€‚

Also joining us for this panel is Drã€‚ Shao Jingã€‚Head of the large model safety team at Shanghai AI Laboratoryã€‚

 where she leads many research projects on evaluating large model safety and value alignmentã€‚

We also have Professor Yang Yaodongï¼Œ who is deputy director of the Center for AI Safety and Governance at Peking Universityã€‚

Professor Youngang studies AI alignment and reinforcement learningï¼Œ among other topicsã€‚

 and has over 100 publications in top venuesã€‚Finallyï¼Œ we have Professor Zhang Zueshengã€‚

 an assistant professor at Shanghai Jiao Toong Universityã€‚ğŸ˜Šã€‚

His primary research interests include the safety and security of multimodal models and autonomous agentsã€‚

He has published over 50 papers in top tier conferences and journalsã€‚

ğŸ¼Our moderator for this panel will be Duan Yawenï¼Œ who is the technical program manager at Concordia AI and a Future of Life Institute PhD fellowelã€‚

Yawen research AI safetyfe at the University of Cambridge and holds a master's degree in machine learningã€‚

Let's put our hands together for oral panelistsã€‚å¯¹ï¼Œæ„Ÿè°¢æ„Ÿè°¢ä¸»æŒäººå›æ€¡ã€‚

é‚£æˆ‘ä»¬ä»Šå¤©çš„ç¬¬ä¸€ä¸ªåœ†æ¡Œè®¨è®ºçš„é—®é¢˜æ˜¯å‘ƒç¬¬ç¬¬ä¸€ä¸ªåœ†æ¡Œè®¨è®ºçš„ä¸»é¢˜æ˜¯å…³äºå‰æ²¿AIå®‰å…¨æŠ€æœ¯çš„å‘ƒç ”ç©¶è®®ç¨‹ã€‚é‚£å…¶å®ä»Šæ—©æˆ‘ä»¬çœ‹åˆ°uså®ƒæœ‰è°ˆåˆ°ä»–ç‰µå¤´çš„ç¬¬ä¸€ä»½å…ˆè¿›AIå®‰å…¨ç§‘å›½é™…ç§‘å­¦æŠ¥å‘Š international scientific report on the safety of advanced aIå…¶ä¸­ä»–æåˆ°äº†é€šç”¨å‹çš„äººå·¥æ™ºèƒ½å¯èƒ½å¸¦æ¥çš„æ»¥ç”¨é£é™©æ•…éšœé£é™©ä»¥åŠç³»ç»Ÿæ€§çš„é£é™©ã€‚

åŒæ—¶ä»–ä¹Ÿä»‹ç»äº†å½“å‰çš„ä¸€äº›å®‰å…¨å¯¹å…¶æ–¹æ³•çš„ä¸€äº›å±€é™æ€§ã€‚é‚£å…¶å®æˆ‘ä»¬ä»Šå¤©çš„ç¬¬ä¸€ä¸ªåœ†æ¡Œè®¨è®ºèšç„¦çš„å°±æ˜¯è¿™ä¸¤ä¸ªé—®é¢˜ã€‚é‚£ç¬¬ä¸€ä¸ªé—®é¢˜å‘¢ï¼Œå…¶å®æ˜¯å‘ƒé¢å‘å‰æ²¿å¤§æ¨¡å‹çš„AIå®‰å…¨æŠ€æœ¯å­˜åœ¨ä»€ä¹ˆæ ·å­çš„æŒ‘æˆ˜ã€‚

å½“ç„¶è¿˜æœ‰ç¬¬äºŒä¸ªé—®é¢˜å°±æ˜¯é¢å‘æ›´å¼ºå¤§çš„æœªæ¥çš„é€šç”¨äººå·¥æ™ºèƒ½ï¼Œç”šè‡³æ˜¯å…¨æ–¹ä½çš„è¶…è¶…è¶Šäººç±»çš„è¶…çº§æ™ºèƒ½ã€‚å‘ƒï¼Œå®‰å…¨æŠ€æœ¯åº”è¯¥æ€ä¹ˆåšï¼Œä»¥åŠå¦‚ä½•é¿å…å¤±æ§çš„é£é™©ã€‚å—¯ï¼Œé‚£é¦–å…ˆæ¬¢è¿å››ä½è€å¸ˆã€‚

ç„¶åæˆ‘æƒ³å‘ƒé¦–å…ˆæˆ‘ä»¬æƒ³ç¬¬ä¸€ä¸ªé—®é¢˜æƒ³è¦æ¢è®¨ä¸€ä¸‹ï¼Œå°±æ˜¯å®‰å…¨å½“å‰çš„å®‰å…¨æŠ€æœ¯çš„ä¸€äº›æŒ‘æˆ˜ã€‚é‚£å½“å®‹è€å¸ˆå‘ƒï¼Œåˆšæ‰æ‚¨æœ‰æåˆ°å°±æ˜¯ç›®å‰çš„é˜²å¾¡æ–¹æ³•è¿˜éå¸¸è„†å¼±ã€‚æ¯”å¦‚è¯´åƒSFTæˆ–è€…RHFè¿˜æœ‰å¯¹æŠ—è®­ç»ƒçš„è¿™æ ·å­çš„é˜²æŠ¤ä¸å¤Ÿæœ‰æ•ˆã€‚

ç”šè‡³å®¹æ˜“è¢«reverseï¼Œç”šè‡³å®¹æ˜“è¢«é€†è½¬ã€‚å½“ç„¶æ‚¨ä¹Ÿæåˆ°äº†å°±æ˜¯representation engineeringï¼Œè¿˜æœ‰saffeå‘ƒ safety by designã€‚

é‚£å…¶å®å‘ƒæƒ³è¦æŠ›æŠ›å‡ºç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯æ‚¨è®¤ä¸ºå½“å‰çš„è¿™äº›å¤§æ¨¡å‹å‡ºç°è¿™äº›è„†å¼±æ€§çš„åº•å±‚åŸå› æ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠä»€ä¹ˆæ ·å­çš„æŠ€æœ¯çš„æ–°æ–¹å‘ä¼šæ›´åŠ çš„æœ¬è´¨ã€‚Oh okayï¼Œ so againã€‚

 for this type of atmosphere robustness and so onã€‚ like the kind of work that I show is that really showing that these models are very vulnerable to these attacks and these alignmentsã€‚

 Mag are very fragileã€‚ it can be easily brokenï¼Œ either through jailbreaks or other types of attacks and so onã€‚

ğŸ˜Šï¼ŒI think one thing is thatï¼Œ so first of allï¼Œ we actually don't really know how these models workã€‚ğŸ˜Šã€‚

And so last semesterï¼Œ I taught a class at Berkeley called Understanding La languageage modelssã€‚

 Foundations and safetyfetyã€‚ And the reason I call it understanding is because nobody understandsã€‚ğŸ˜Šã€‚

Rightï¼Œ I think this is one of the issues because we don't really understand how these models work and this type of alignment mechanisms that we do todayã€‚

 for exampleï¼Œ through ourIHF isn you can say it's only like changing the surface only doing it at the surface levelã€‚

 we are actually now really making itã€‚ğŸ˜Šï¼ŒRightï¼Œ being well alignedã€‚ And alsoï¼Œ I thinkã€‚

As I mentioned in particularï¼Œ this for air safetyï¼Œ we really need to make it resilient against this adversar attacksã€‚

 So it's interesting that actually in the text domainï¼Œ actually attacks are not as easy to doã€‚

 but inï¼Œ for exampleï¼Œ in the image domain and other modalitiesã€‚ğŸ˜Šï¼ŒIt's muchï¼Œ much easierã€‚

 And I think the hope is even lessã€‚That we actually can build solutions to defend against thisã€‚

 So that's whyã€‚ and now as models become multimod it's even it's even a bigger issue because as a multi model systemsã€‚

 it's extremely easy to attack these systemsã€‚ And I think because right we are just right now all the mechanisms that we are doing iss more trying to change things at the surface level that's why in my talk I talk about some of these future directions that actually trying to go deeper once through representation controlã€‚

 we can actually mechanism we can modify the model behaviors and also I do that we want to have some kind of guarantees and then essentially is to solve the problems at a much deeper level than just trying toã€‚

ğŸ˜Šï¼ŒRight to some of these RL type ofã€‚å¯¹ï¼Œè°¢è°¢å½“å®‹è€å¸ˆï¼Œå°±æ˜¯ç›®å‰çš„å®‰å…¨é˜²æŠ¤å‘ƒé˜²æŠ¤è¿˜æ¯”è¾ƒè¡¨å±‚ã€‚Oå‘ƒå¯¹ï¼Œæ¥ä¸‹æ¥æƒ³è¦ä¹Ÿæƒ³è¦å‘ƒé—®ä¸€ä¸‹ï¼Œå°±æ˜¯è€€ä¸œè€å¸ˆã€‚

å…¶å®æˆ‘æˆ‘è§‚å¯Ÿåˆ°æ‚¨åœ¨ä¸åŒçš„åœºåˆéƒ½æœ‰è®²è¿‡ã€‚æ¯”å¦‚è¯´åªåšRAHæ˜¯ä¸è¶³å¤Ÿçš„ã€‚ç„¶åä»¥åŠä½ è¿‘æœŸçš„å·¥ä½œå…¶å®ä¹Ÿå‘ç°äº†è¯­è¨€æ¨¡å‹å¯¹æŠ—sorryæŠµæŠ—å¯¹é½ï¼Œè¿˜æœ‰é€†è½¬å¯¹é½çš„ä¸€ä¸ªç°è±¡ã€‚ä½ ä¹Ÿå¯ä»¥è°ˆä¸€è°ˆä½ çš„çœ‹æ³•äº†ã€‚å¯¹å¯¹å¯¹ï¼Œå‘ƒã€‚

é‚£æˆ‘å°±ç”¨ä¸­æ–‡è¯´ï¼Œå°±æ˜¯è¯´å…¶å®åˆšæ‰å¾ˆå¤šå­¦è€…éƒ½è§‚æµ‹åˆ°äº†ä¸€ä¸ªç°è±¡ï¼Œå°±æ˜¯è¯´è¯­è¨€æ¨¡å‹ï¼Œä»–åšå®Œè¿™ä¸ªå¯¹é½ä»¥åï¼Œä½ å…¶å®å¯ä»¥ç”¨éå¸¸å°‘çš„æ”»å‡»æ ·æœ¬ï¼Œå°±å¯ä»¥è®©ä»–å˜å¾—ä¸å®‰å…¨ã€‚

å“ªæ€•ä½ åšäº†å¾ˆé•¿æ—¶é—´çš„è¿™ä¸ªRAé‚£Içš„é‚£ä¸ªçš„é‚£ä¸ªä»–å°±å‘ç°ä¸€ä¸ªç°è±¡ï¼Œå°±æ˜¯å½“è¿™ä¸ªè¯­è¨€æ¨¡å‹è®­ç»ƒçš„éå¸¸å¥½çš„æ—¶å€™ï¼Œä»–ä¿„è¯­ä¸Šå‘ç°äº†è¿™ä¸ªé”™è¯¯ï¼Œä»–åªéœ€è¦ç”¨30ä¾‹è‹±è¯­çš„æ ·ï¼Œå°±å¯ä»¥è®©ä¿„è¯­ä¸ŠçŠ¯çš„è¿™ä¸ªé”™è¯¯ã€‚ä¸å†çŠ¯ã€‚ç„¶åè¿™ä¸ªé—®é¢˜å‘¢ã€‚

æˆ‘ä»¬å…¶å®ä¹Ÿè¿›è¡Œäº†ä¸€ä¸ªæ·±å…¥çš„æ€è€ƒã€‚ç”šè‡³æˆ‘ä»¬å°±æœ€è¿‘æœ‰ä¸€ä¸ªå·¥ä½œå«å‘ƒ largege language modelre alignmentå°±æˆ‘ä»¬åœ¨è¿™ä¸ªå·¥ä½œé‡Œé¢å»ç ”ç©¶ä¸€ä¸ªç‰¹æ®Šçš„è¿™ä¸ªç°è±¡ï¼Œå°±æ˜¯é€†å¯¹é½çš„é—®é¢˜ã€‚

å°±æˆ‘ä»¬éƒ½çŸ¥é“ä½ åœ¨è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„æ—¶å€™ï¼Œä½ æ€»æœ‰ä¸¤ä¸ªé˜¶æ®µï¼Œå¯¹å§ï¼Ÿä½ å…ˆè¿›è¡Œé¢„è®­ç»ƒé¢„è®­ç»ƒå®Œäº†ä»¥åï¼Œä½ å†è¿›è¡ŒSFTä½ å†è¿›è¡Œä¸€ä¸ªHFé‚£åœ¨å‚æ•°ç©ºé—´çš„è¯ï¼Œä½ å¯ä»¥æŠŠè¿™ä¸ªè¿™ä¸ªè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæƒ³è±¡æˆä¸€ä¸ªæ‹‰æ©¡çš®ç­‹çš„è¿‡ç¨‹ã€‚

ç„¶åä½ è¶Šå¾€åæ‹‰è¶Šå¾€åæ‹‰ä½ çš„å¼ åŠ›å…¶å®æ˜¯è¶Šæ¥è¶Šå¼ºå¤§çš„ã€‚ç„¶åæˆ‘ä»¬å°±å‘ç°è¿™ä¸ªé€†å¯¹é½çš„è¿™ä¸ªè¿‡ç¨‹å•Šï¼Œå°±åƒä½ æŠŠè¿™ä¸ªæ©¡çš®ç­‹æ‹‰åˆ°å¾ˆåé¢ï¼Œå®ƒä¸èƒ½åœ¨ä¼¸å±•çš„æ—¶å€™ï¼Œä½ è¿™ä¸ªæ—¶å€™å¦‚æœæŠŠå®ƒçªç„¶æ”¾å¼€çš„è¯ã€‚

å®ƒ backçš„è¿™ä¸ªé€Ÿåº¦è¦æ¯”ä½ æ‹‰çš„è¿™ä¸ªé€Ÿåº¦è¦å¿«å¾ˆå¤šã€‚æ‰€ä»¥æˆ‘ä»¬å°±æŠŠè¿™ä¸ªç°è±¡åœ¨è¿™ä¸ªè¯­è¨€æ¨¡å‹çš„è®­ç»ƒçš„è¿™ä¸ªè¿‡ç¨‹ä¸­å®šä¹‰ä¸ºé€†å¯¹é½ä»€ä¹ˆå«é€†å¯¹å°±æ˜¯æˆ‘åœ¨é¢„è®­ç»ƒå®Œäº†ä»¥åï¼Œæˆ‘åœ¨åšæ¯”å¦‚è¯´åæ­¥SFTé‚£æˆ‘åœ¨åšç¬¬11æ­¥SFTçš„æ—¶å€™ã€‚

æˆ‘æ˜¯ä¸æ˜¯ä¼šå‘ç°ç¬¬11æ­¥ã€‚å›åˆ°ç¬¬åæ­¥SFTçš„è¿™ä¸ªé€Ÿåº¦ï¼Œè¦æ¯”æˆ‘ä»ç¬¬ä¹æ­¥åšSFTåˆ°ç¬¬åæ­¥SFTçš„è¿™ä¸ªé€Ÿåº¦è¦å¿«ã€‚é‚£æˆ‘ä»¬å‘ç°è¿™ä¸ªé€†å¯¹é½çš„è¿™ä¸ªç°è±¡æ˜¯å­˜åœ¨çš„ã€‚

å¹¶ä¸”å‘¢è¿™ä¸ªé€†å¯¹é½çš„è¿™ä¸ªç°è±¡å‘¢å¯èƒ½ä¼šç¬¦åˆæˆ‘ä»¬å°±ç†è§£æ©¡çš®ç­‹çš„è¿™ä¸ªè¿ä½œåŸç†é‡Œé¢é‚£ä¸ªå…‹å®šå¾‹å…‹å®šå¾‹è®²çš„æ˜¯ä¸€ä¸ªæ©¡çš®ç­‹çš„è¿™ä¸ªåº”åŠ›å•Šç­‰äºå¼¹æ€§ç³»æ•°ä¹˜ä»¥è¡Œå˜é‡ã€‚ç„¶åè¿™ä¸ªå¼¹æ€§ç³»å‘¢ï¼Œæˆ‘è§‰å¾—åœ¨æ¨¡å‹é‡Œé¢æˆ‘ä»¬å‘ç°çš„å°±æ˜¯å’Œæ¨¡å‹çš„å¤§å°ã€‚

è¿˜æœ‰é¢„è®­çš„è¿™ä¸ªæ•°æ®é‡æœ‰å…³ã€‚ç„¶åé‚£ä¸ªé‚£ä¸ªè¡Œå˜é‡å…¶å®å°±æ˜¯ä½ ç¦»SFå°±æ˜¯å®Œçš„é‚£ä¸ªpoliçš„é‚£ä¸ªrgenceå°±æ˜¯ä½ è¶Šç»ƒçš„å½¢å˜å°±è¶Šé•¿ã€‚é‚£ä¹Ÿå°±æ˜¯è¯´å¦‚æœä½ æŠŠè¿™ä¸ªè¯­è¨€æ¨¡å‹æ¥ç€ä¸åœå¾€åå¯¹é½åç»ƒä½ çœ‹ç€æ˜¯è¶Šæ¥è¶Šå®‰å…¨ã€‚

ä½†æˆ‘ä»¬åœ¨é‚£ä¸ªé‡Œç†è®ºå’Œå®è·µä¸Šéƒ½è¯æ˜å…¶å®ä»–é€†å¯¹åè€Œä¼šæ›´åŠ å®¹æ˜“è¿™ä¹Ÿå¯èƒ½ä»ä¸€äº›æœºåˆ¶ä¸Šèƒ½å¤Ÿè§£é‡Šåˆšæ‰ç´è€å¸ˆå®‹è€å¸ˆéƒ½ä¼šæåˆ°çš„ä¸€ä¸ªè§‚ç‚¹ï¼Œå°±æ˜¯ä½ è¶Šåšå¯¹é½å¯èƒ½ä»–åå‘å°±å®¹ã€‚è¢«æ”»ç ´ï¼Œå¹¶ä¸”ä½ ç”¨çš„è¿™ä¸ªæ ·ä¾‹å¯èƒ½ä¸éœ€è¦å¾ˆå¤šå•Šã€‚

è¿™æˆ‘è§‰å¾—æ˜¯ä¸ªéå¸¸æœ‰æ„æ€çš„ç°è±¡ã€‚å½“ç„¶ä¹Ÿæ­ç¤ºäº†æˆ‘ä»¬æœªæ¥å¯èƒ½ä¸‹ä¸€æ­¥å•Šï¼Œå¯¹äºå¦‚ä½•æ›´å¥½çš„åšå®‰å…¨å¯¹é½ï¼Œåšä»·å€¼å¯¹é½ï¼Œä¼šæœ‰ä¸€äº›è¿™ä¸ªæŒ‡å¯¼æ„ä¹‰ã€‚å¯¹ï¼Œä¹Ÿä¹Ÿä¹Ÿå¸Œæœ›å¤§å®¶å…³æ³¨è¿™ä¸ªæˆ‘ä»¬ç»„çš„è¿™ä¸ªå·¥ä½œå°±å«å¤§è¨€æ¨¡å‹å•Šã€‚

resist alignmentå•Šã€‚å¯¹ï¼Œè¿™ä¸ªæ©¡çš®ç­‹çš„è¿™ä¸ªç±»åˆ«è¿˜æ˜¯æŒºæœ‰è¶£çš„å¯¹ï¼Œå…¶å®åˆšåˆšå‘ƒæˆ‘æœ‰æ³¨æ„åˆ°å‘ƒï¼Œå¼ æ¾è€å¸ˆæœ‰è°ˆåˆ°é‚£ä¸ªå‘ƒå¤šæ¨¡æ€å¤§æ¨¡å‹çš„ä¸€ä¸ªå°±æ˜¯å¯¹é½çš„éš¾åº¦ã€‚

å…¶å®æˆ‘æˆ‘çŸ¥é“ç»å…´è€å¸ˆè¿‡å»å‡ ä¸ªæœˆå‘ƒæ‚¨çš„å›¢é˜Ÿå…¶å®æœ‰å‘ƒå‘è¡¨ï¼Œå°±æ˜¯å¾ˆå¤šç‰‡å…³äºå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œè¿˜æœ‰æ™ºèƒ½ä½“çš„æ”»å‡»å’Œè¯„æµ‹çš„å·¥ä½œã€‚æ¯”å¦‚è¯´åƒsiveï¼Œè¿˜æœ‰shaæ•°æ®é›†è¿™æ ·å­çš„å·¥ä½œã€‚é‚£å‘ƒå…¶å®å‘ƒå°±å°±é¡ºç€è¿™ä¸ªä¸»é¢˜è¯´å§ã€‚

å°±æ˜¯æ‚¨è§‰å¾—æ¯”å¦‚è¯´åƒå¯¹äºGPT4Oè¿™æ ·å­ä»¥å›¾ç‰‡è§†é¢‘å‘ƒï¼Œè¯­éŸ³è¿™æ ·å­çš„è¿ç»­ç©ºé—´é‡Œé¢çš„æ•°æ®ä½œä¸ºè¾“å…¥çš„ä¸€äº›å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚åœ¨å°±æ˜¯è¿™ä¸ªå®‰å…¨çš„æ–¹é¢ï¼Œå®‰å…¨å¯¹é½çš„æ–¹é¢æœ‰æ²¡æœ‰ä»€ä¹ˆä¸€äº›ç‰¹æ®Šçš„æŒ‘æˆ˜ã€‚å—¯ï¼Œå¯¹ï¼Œè¿™è¿™ä¹Ÿæ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚

è¿™ç¡®å®æ˜¯åœ¨é‚£ä¸ªå»å¹´å¹´åˆï¼Œå¯èƒ½å¤§å®¶æ›´å¤šå…³æ³¨çš„è¿˜æ˜¯å¤§è¯­è¨€æ¨¡å‹æœ¬èº«çš„å®‰å…¨æ€§é—®é¢˜ã€‚å‘ƒï¼Œä½†æ˜¯å‘ƒå› ä¸ºæˆ‘ä»¬å›¢é˜Ÿé‡Œé¢æœ‰å¾ˆå¤šæ˜¯åšè¿™ä¸ªåŸæ¥åšè§†è§‰çš„ï¼Œè¿˜æœ‰ä¸€äº›åŒ–å­¦ç§‘çš„è¿™ä¸ªåŒå­¦å’Œè€å¸ˆä¸“å®¶ã€‚

ç„¶åå¤§å®¶ä¼šå‘ç°è¯´å‘ƒè¿™ä¸ªæˆ‘å¼•å…¥äº†æ›´å¤šçš„è¿™ä¸ªä¿¡å·ã€‚æ¯”å¦‚è¯´å›¾åƒè§†é¢‘ä¹‹åï¼Œä»–å¸¦æ¥çš„è¿™ä¸ªå¤æ‚åº¦æ˜¯æ€¥å…·æå‡çš„é‚£ä»–å®ƒå¸¦æ¥çš„è¿™ä¸ªå®‰å…¨é—®é¢˜ä¹Ÿæ˜¯è·Ÿä»¥å¾€çš„è¿™ä¸ªä¸“è¯­è¨€å¤§æ¨¡å‹æ˜¯ä¸ä¸€æ ·çš„ã€‚

æ¯”å¦‚è¯´è¿™ä¸ªå¤§å®¶å¯èƒ½å¸¸è¯´çš„è¿™ä¸ªè¯­è¨€æ¨¡å‹é‡Œçš„å¹»è§‰é—®é¢˜ã€‚å…¶å®åœ¨å¤šæ³°çš„è¿™ä¸ªæ¨¡å‹é‡Œé¢ä¹Ÿæ˜¯æœ‰çš„é‚£å‘ƒè¿™ä¸¤è€…çš„åŒºåˆ«æ˜¯åœ¨äºä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯å‘ƒè¯­è¨€æ¨¡å‹é‡Œçš„å¹»è§‰é—®é¢˜ï¼Œå¯èƒ½å®ƒçš„å®šä¹‰æ˜¯ç¨å¾®æ¯”è¾ƒæ˜ç¡®çš„ã€‚ä½†æ˜¯åœ¨å¤šæ¨¡æ€æ¨¡å‹é‡Œé¢ã€‚

ä»–æœ‰å¯èƒ½æ˜¯è¿™ä¸ªæœ¬èº«è¿™ä¸ªè§†è§‰çš„è¿™ä¸ªè¿™ä¸ªåˆ†æ”¯ï¼Œå®ƒè·Ÿè¯­è¨€çš„ä¸ªåˆ†æ”¯çš„ä¸Šä¸‹æ–‡çš„ç†è§£æ¯”è¾ƒå¼±ã€‚æ‰€ä»¥ä»–æ ¹æœ¬å°±æ²¡æœ‰ç†è§£è¿™ä¸ªé—®é¢˜å¸¦æ¥çš„å¹»è§‰ï¼Œä¹Ÿå¯èƒ½æ˜¯è§†è§‰åˆ†æ”¯æœ¬èº«ç°åœ¨å®ƒçš„è¿™ä¸ª groundingèƒ½åŠ›ä¹Ÿæ¯”è¾ƒå·®ã€‚å‘ƒã€‚

æ‰€ä»¥å¸¦æ¥çš„å¹»è§‰é—®ã€‚ğŸ˜Šï¼Œé¢˜å‘ƒï¼Œä¹Ÿå¯èƒ½æ˜¯è€¦åˆæ€§çš„å‘ƒå„ç§åŸå› ã€‚é‚£å¼•å…¥æ›´å¤šçš„è¿™ä¸ªæ¨¡æ€ä¹‹åï¼Œå®ƒçš„è¿™ä¸ªåˆ†è§£çš„å¤æ‚åº¦å°±ä¼šå˜é«˜å¾ˆå¤šã€‚ä½†æ˜¯ç°åœ¨å¤§å®¶å¯èƒ½å¯¹è¿™æ–¹é¢çš„ç ”ç©¶è¿˜æ˜¯æ¯”è¾ƒå‡ºå¥‡ã€‚æ‰€ä»¥å¹¶æ²¡æœ‰ç»™å‡ºå¾ˆå¾ˆæ˜ç¡®çš„ç»“è®ºã€‚

æˆ–è€…æ˜¯æœ‰ä¸€äº›æ›´å…·è±¡çš„åˆ†æã€‚ç„¶åå¦å¤–çš„è¯å‘¢ï¼Œæˆ‘ä»¬å…¶å®ä»Šå¹´å¹´åˆçš„æ—¶å€™ï¼Œåœ¨jaå‡ºæ¥çš„æ—¶å€™ï¼Œå‘ƒï¼Œå¤§æ¦‚çŸ­æœŸä¹‹å†…ï¼Œæˆ‘ä»¬å°±å‘ƒåšäº†ä¸€ä¸ªå¤§æ¦‚300é¡µçš„è¿™ä¸ªè¯„æµ‹æŠ¥å‘Šã€‚è¿™é‡Œé¢åŒ…æ‹¬äº†å¯¹transçš„ä¸€äº›å‘ƒè¯„æµ‹ã€‚

ä¹ŸåŒ…æ‹¬ä¸€äº›å‘ƒå‘ƒè¿™ä¸ªæ³›åŒ–æ€§çš„ï¼Œè¿˜æœ‰è¿™ä¸ªå› æˆ‘æ¨ç†çš„ã€‚å› ä¸ºæˆ‘ä»¬ç›¸ä¿¡è¯´å‘ƒåŠ¨æ€å¤§æ¨¡å‹æœªæ¥èƒ½å¤Ÿç”¨åœ¨çš„ç¯èŠ‚å’Œäº§å“åº”ç”¨é‡Œä¼šéå¸¸çš„å¤šã€‚é‚£æˆ‘ä»¬ä¸ä»…å…³æ³¨å®ƒçš„è¿™ä¸ªå‘ƒå¯ä¿¡çš„é—®é¢˜ï¼Œä¹Ÿä¼šå…³æ³¨å®ƒåŒæ—¶çš„è¿™ä¸ªæ³›åŒ–æ€§çš„è¿˜æœ‰ä¸€äº›æ¨ç†çš„é—®é¢˜ã€‚

è¿™ä¹ŸåŒç­‰çš„é‡è¦ã€‚ç”šè‡³è¯´å‘ƒå…¶ä»–çš„è¿™äº›èƒ½åŠ›å¯èƒ½ä¼šå½±å“å®ƒæœ¬èº«çš„è¿™ä¸ªå®‰å…¨æ€§çš„é—®é¢˜ã€‚æ‰€ä»¥æœªæ¥çš„è¯ï¼Œæˆ‘ä»¬ä¹Ÿä¼šèŠ±æ›´å¤šçš„ç²¾åŠ›å’Œè¿™ä¸ªèµ„æºåœ¨è¿™æ–¹é¢çš„ç ”ç©¶ä¸Šã€‚ğŸ˜Šï¼Œè°¢è°¢ã€‚ğŸ˜Šï¼Œå¯¹ï¼Œè°¢è°¢é‚µé™è€å¸ˆã€‚å¯¹ï¼Œå—¯ï¼Œç„¶åå…¶å®æˆ‘ä¹Ÿæƒ³é—®ä¸€ä¸‹ã€‚

å°±å“èƒœè€å¸ˆçš„ä¸€äº›è§‚ç‚¹å§ã€‚å°±æ˜¯æˆ‘æˆ‘å…¶å®ä¹Ÿæœ‰çœ‹åˆ°æ‚¨å‘ƒä¹‹å‰æœ‰åšä¸€äº›å‘ƒåŠ¨æ¨¡æ€å¤§æ¨¡å‹ï¼Œç„¶åè¿˜æœ‰agsæ–¹é¢çš„ä¸€äº›å®‰å…¨æ–¹é¢çš„å·¥ä½œã€‚ç„¶åå½“ç„¶ç°åœ¨agentså…¶å®æ˜¯ç‰¹åˆ«ç«çš„ã€‚å°±æ˜¯é‚£äº›å¯ä»¥ç›´æ¥è¿›è¡Œåºåˆ—å†³ç­–ã€‚

ç„¶åç›´æ¥æ“çºµå·¥å…·å’ŒAPIçš„ä¸€äº›æ™ºèƒ½ä½“ã€‚é‚£å‘ƒæˆ‘ä¹‹å‰å…³æ³¨åˆ°æ‚¨çš„å·¥ä½œï¼Œå¯èƒ½æ˜¯å‘ƒä¹‹å‰æœ‰ä¸€ç¯‡æ˜¯å«ï¼Œç„¶åæ˜¯é€šè¿‡ç›‘æµ‹äº¤äº’è®°å½•çš„æ–¹æ³•æ¥è¯†åˆ«å‘ƒè‡ªä¸»æ™ºèƒ½ä½“agentsçš„ä¸€äº›é£é™©è¡Œä¸ºã€‚é‚£å‘ƒå¦‚æœè®¨è®ºåˆ°agentsçš„å®‰å…¨çš„è¯ã€‚

å•Šï¼Œæ‚¨è§‰å¾—æœ‰æ²¡æœ‰ä¸€ä¸€äº›ç‰¹æ®Šçš„éš¾ç‚¹æƒ³è¦åˆ†äº«ã€‚å‘ƒï¼Œå¥½çš„ï¼Œå‘ƒï¼Œæˆ‘å°±æ²¿ç€é‚µé‡‘è€å¸ˆåˆšåˆšæçš„è¿™ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹è¿™æ¡çº¿ï¼Œå°±æˆ‘ä»¬ä¹Ÿä¹Ÿåœ¨åšï¼Œå°±æ˜¯agentï¼Œå®ƒæœ‰é‚£ç§çº¯A1Magï¼Œä¹Ÿæœ‰é‚£ç§åŸºäºmartymodçš„agentã€‚

é‚£ä¹ˆæˆ‘ä»¬å°±å‘ç°å…¶å®è¿™é‡Œé¢ä¸€ä¸ªæ ¸å¿ƒçš„ç‚¹å°±æ˜¯åœ¨äºagentï¼Œä»–æ˜¯æŠŠå¤§æ¨¡å‹ç”¨åœ¨è™šæ‹Ÿæˆ–è€…ç°å®çš„ç¯å¢ƒä¸­ï¼Œè®©ä»–å¯¹è¿™ä¸ªç°å®äº§ç”Ÿå½±å“ã€‚é‚£ä¹ˆä»è¿™ä¸ªç‰¹ç‚¹ä¸Šæ¥çœ‹ã€‚

è¿™ä¸ªagentå®ƒå°±æ¶‰åŠåˆ°å¤§æ¨¡å‹ä¸ç”¨æˆ·ä»¥åŠç¯å¢ƒä¹‹é—´è¿›è¡Œäº†ä¸€ä¸ªå¤šè½®åŠ¨æ€çš„ä¸€ä¸ªäº¤äº’è¿‡ç¨‹ã€‚é‚£ä¹ˆå®ƒè·Ÿä¼ ç»Ÿçš„å¤§æ¨¡å‹çš„å®‰å…¨ä¸€ä¸ªé‡å¤§çš„åŒºåˆ«ï¼Œå°±åœ¨äºæˆ‘è¿™ä¸ªå®ƒæ˜¯åœ¨ä¸€ä¸ªçœŸå®ç¯å¢ƒé‡Œé¢ã€‚

é‚£ä¹ˆå®ƒå°±ä¼šå‘ƒå®ƒçš„è¿™ä¸ªå®‰å…¨çš„å®‰å…¨é£é™©çš„æ¥æºå‘¢ï¼Œå°±ä¼šæ¶‰åŠåˆ°ç”¨æˆ·ç¯å¢ƒå’Œæ¨¡å‹æœ¬èº«è¿™ä¸‰ä¸ªç»´åº¦çš„è¿™ä¸ªå®‰å…¨é—®é¢˜ã€‚ç„¶åè€Œä¸”å®ƒè¿™ä¸ªæ¶‰åŠåˆ°æˆ‘ä»¬ç°åœ¨æ›´å¼ºè°ƒçš„æ˜¯ä¸€ä¸ªé€šç”¨çš„agentã€‚

é‚£ä¹ˆå®ƒæ‰€ç”¨æ‰€å‘ƒå¤„äºçš„è¿™ä¸ªç¯å¢ƒå‘¢ä¹Ÿæ˜¯å¤šç§å¤šæ ·çš„ã€‚é‚£ä¹ˆæˆ‘å¯ä»¥ä»è¿™ä¸ªç¯å¢ƒä¸­å»æ„é€ ç›¸åº”çš„æ”»å‡»æ ·æœ¬ã€‚å•Šï¼Œè¿™æ˜¯å…¶äºŒã€‚ç¬¬ä¸‰ä¸ªæœ€æ ¸å¿ƒçš„ç‚¹å°±æ˜¯åœ¨äºæˆ‘æ™ºèƒ½ä½“è¿™ä¸ªè¡Œä¸ºï¼Œå®ƒä¸åƒæˆ‘ä»¬é™æ€çš„AIGCçš„è¿™ä¸ªä¿¡æ¯ã€‚

æ™ºèƒ½ä½“åœ¨è¿™ä¸ªäº¤äº’è¿‡ç¨‹ä¸­ï¼Œå®ƒçš„è¿™ä¸ªåæœæˆ‘ä»¬å¾€å¾€æ˜¯éš¾ä»¥å»é¢„æµ‹çš„ã€‚æˆ‘ä¸çŸ¥é“å®ƒæœªæ¥ä¼šäº§ç”Ÿä»€ä¹ˆæ ·çš„åæœï¼Œä»¥åŠä»–ç°åœ¨çš„ä¸€ç³»åˆ—çš„è¡Œä¸ºï¼Œæœªæ¥ä¼šä¸‹ä¸€æ­¥è¡Œä¸ºä¼šæ€ä¹ˆå»åšã€‚é‚£ä¹ˆæˆ‘ä»¬æˆ‘ä»¬è¦å»é¢„æµ‹å®ƒæœªæ¥çš„é£é™©ï¼Œä¹Ÿä¼šå˜å¾—æ›´åŠ å›°éš¾ã€‚

ç„¶åå•Šç»“åˆè¿™äº›é—®é¢˜å‘¢ï¼Œæˆ‘ä»¬æœ€è¿‘å•Šåœ¨çš„ç¡€ä¸Šï¼Œæˆ‘ä»¬åœ¨åšä¸€äº›åŠ¨æ€æ¢ç´¢ï¼Œå°±ä¾‹å¦‚ç°åœ¨å¤§å®¶å¾ˆå¤šäººåœ¨å…³æ³¨å•Šå°¤å…¶æ˜¯apple intelligenceã€‚

æˆ‘ä»¬å¸Œæœ›å»è®©å¤§æ¨¡å‹æ¥å…¥æˆ‘ä»¬çš„æ‰‹æœºæˆ–è€…æ˜¯ç”µè„‘æ¥æ¨¡æ‹Ÿäººç±»çš„è¿™ä¸ªå±å¹•çš„æ“ä½œï¼Œå¸®æˆ‘ä»¬å®Œæˆå¤æ‚çš„æŒ‡ä»¤ã€‚é‚£ä¹ˆæˆ‘ä»¬æ”»å‡»è€…å‘¢ä»–å°±å¯ä»¥å•Šä¸€æ–¹é¢å¯ä»¥ä»ç”¨æˆ·ç«¯ï¼Œæˆ‘ä»¬å»æ„é€ å„ç§å¯¹æŠ—æˆ–è€…åŠ«æŒçš„æ ·æœ¬æ¥å½±å“è¿™ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚

å¯èƒ½å¯ä»¥å»å¯¹æŠ—ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠä¿¡æ¯æ¤å…¥åˆ°è¿™ä¸ªå±å¹•ä¿¡æ¯ä¸­ã€‚ä¾‹å¦‚æ™ºèƒ½ä½“åœ¨æ“ä½œç½‘é¡µæˆ–è€…æ“ä½œçš„appçš„æ—¶å€™ï¼Œæˆ‘ä¹Ÿå¯ä»¥åœ¨ã€‚è¯»å–çš„è¿™ä¸ªç¯å¢ƒé‡Œé¢å»æ¤å…¥æ–°çš„æŒ‡ä»¤ã€‚é‚£ä¹ˆå‘ƒæ™ºèƒ½ä½“ä»–çœ‹åˆ°è¿™æ ·çš„æ–°çš„æŒ‡ä»¤çš„æ—¶å€™ã€‚

æˆ‘ä»¬å°±å‘ç°åœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼Œä»–å°±ä¼šå—åˆ°æ–°çš„æŒ‡ä»¤çš„å½±å“ï¼Œè€Œå¿˜è®°ä»–ä¹‹å‰çš„è¡Œä¸ºï¼Œå¯¼è‡´å•Šè¿™ç§åŠ«æŒçš„é—®é¢˜ã€‚é‚£ä¹ˆè¿™å°±æ„å‘³ç€æˆ‘ä»¬æ”»å‡»è€…ï¼Œä»–ä¸ä»…å¯ä»¥åœ¨userç«¯ï¼Œåƒæˆ‘ä»¬ä¼ ç»Ÿå¤§æ¨¡å‹é‚£æ ·ï¼Œæˆ‘å»åœ¨userç«¯å»åšå¯¹æŠ—ï¼Œæˆ‘å»ã€‚

æ”»ç ´ä½ çš„å¯¹é½ï¼Œç„¶åä¹Ÿå¯ä»¥åœ¨è¿™ä¸ªç¯å¢ƒç«¯ï¼Œæˆ‘å»ç»™ä½ è¿›è¡Œå‘ƒè¯±å¯¼æˆ–è€…è¿›è¡ŒæŒ‡ä»¤çš„æ¤å…¥ï¼Œæ¥å½±å“ä½ æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œä»è€Œå¯¹ç¯å¢ƒæˆ–è€…ç”¨æˆ·è¿™ä¸ªåˆ©ç›Šé€ æˆæŸå®³ã€‚æ‰€ä»¥è¿™ä¸ªé‡Œé¢å°±æ¶‰åŠåˆ°å—¯è¿™ä¸ªä¸‰ä¸ªæ–¹é¢çš„å°±æ˜¯å¤šæ ·åŒ–çš„è¿™ä¸ªæ”»å‡»æ¥æºã€‚

è¿™ä¸ªä¼šå˜å¾—å•Šæ¯”è¾ƒæœ‰æŒ‘æˆ˜ã€‚è€Œé˜²å¾¡æ–¹é¢çš„è¯ï¼Œæˆ‘ä»¬å•Šç°åœ¨å¤§å®¶çš„ä¸»è¦å…³æ³¨ç‚¹éƒ½æ˜¯åœ¨äºå¤§æ¨¡å‹æœ¬èº«çš„è¿™ä¸ªå¯¹é½ã€‚ä½†æ˜¯å‘¢å…¶å®æˆ‘ä»¬åœ¨æ™ºèƒ½ä½“çš„è¿™ä¸ªåº”ç”¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¸ä»…éœ€è¦å¤§æ¨¡å‹çš„å¯¹é½ï¼Œæˆ‘å¯èƒ½è¿˜éœ€è¦ä¸€ä¸ªå¤–éƒ¨çš„ä¸€ä¸ªåé¦ˆã€‚

å°±æ˜¯æˆ‘æˆ‘åªæ˜¯å¤§æ¨¡å‹æœ¬èº«ä»–çŸ¥é“å®ƒè¡Œä¸ºå®‰ä¸å®‰å…¨ï¼Œè¿™æ˜¯ä¸€æ–¹é¢ã€‚ä½†æ˜¯å®ƒè¿™ä¸ªè¡Œä¸ºè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ˜¯å¸Œæœ›å®ƒæ˜¯æœ‰æ•ˆçš„ï¼Œæˆ‘æ˜¯å¸Œæœ›ä»–èƒ½å¤Ÿå°½å¯èƒ½å¸®æˆ‘å®Œæˆä»»åŠ¡ã€‚é‚£ä¹ˆéšç€è¿™ä¸ªæ¨¡å‹å˜å¾—è¶³å¤Ÿå¼ºä¹‹åã€‚ä»–çš„ä»»ä½•æ±‚è§£èƒ½åŠ›è¶³å¤Ÿå¼ºã€‚

é‚£æˆ‘å°±å¯ä»¥å»è¯±å¯¼ä»–å»åšä»»ä½•äº‹æƒ…ã€‚æ‰€ä»¥æˆ‘æˆ‘æ¯”è¾ƒä¸»å¼ çš„ä¸€ä¸ªè§‚ç‚¹å°±æ˜¯é€šè¿‡ä¸€ä¸ªå¤–éƒ¨çš„ä¸€ä¸ªç›‘ç®¡æœºåˆ¶ï¼Œè·Ÿè¿™ä¸ªæ¨¡å‹æœ¬èº«å¯¹å…¶æ¥è¿›è¡Œä¸€ä¸ªäº’è¡¥ã€‚æˆ‘ä»¬å»æ‰€ä»¥è¿™ä¹Ÿæ˜¯æˆ‘ä»¬åšadjustçš„ä¸€ä¸ªåˆè¡·ã€‚æˆ‘ä»¬å»å•Šã€‚

åŠ¨æ€çš„å»åˆ†æå’Œç›‘æµ‹è¿™ä¸ªæ™ºèƒ½ä½“çš„å®ƒçš„è¿™ä¸ªè¡Œä¸ºå†å²ï¼Œå¯¹ä»–æœªæ¥çš„è¡Œä¸ºè¿›è¡Œé¢„æµ‹ï¼Œæ¥é¢„å…ˆé¢„åˆ¤å®ƒå¯èƒ½å­˜åœ¨çš„å®‰å…¨å¨èƒï¼Œç„¶åç»™å‡ºä¸€ä¸ªå®‰å…¨çš„ç ”åˆ¤ç»“è®ºï¼ŒæŠŠè¿™ä¸ªä¿¡æ¯åé¦ˆç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹åŸºäºè¿™ä¸ªåé¦ˆã€‚

åˆ©ç”¨ä»–çš„è¿™ä¸ªå­¦ä¹ èƒ½åŠ›æ¥è¿›è¡Œè‡ªæˆ‘çš„è¿­ä»£ï¼Œä»è€Œå®ç°ä¸€ä¸ªå®‰å…¨çš„é—­ç¯ã€‚å¯¹ï¼Œè¿™ä¸ªæ˜¯æˆ‘ä»¬åšè¿™äº›äº‹æƒ…çš„ä¸€äº›åŸºæœ¬çš„æƒ³æ³•ã€‚å¯¹ï¼Œè°¢è°¢å‘¨èƒœè€å¸ˆã€‚å°¤å…¶åˆšåˆšä½ æœ‰æåˆ°å‘ƒæ™ºèƒ½ä½“ï¼Œè¿˜æœ‰å¤§æ¨¡å‹å‘ƒå‘ƒæˆ–è€…è¯´å¤§è¯­è¨€æ¨¡å‹çš„å‘ƒä¸¤ä¸ªå…³é”®ç‚¹å§ã€‚

ä¸€æ–¹é¢æ˜¯è¿™ä¸ªå­˜åœ¨ä¸ç¯å¢ƒå’Œäººç±»çš„ç”¨æˆ·çš„äº¤äº’ã€‚å¦ä¸€æ–¹é¢æ˜¯è¿™ä¸ªå½±å“å°ºåº¦ impactact horizononçš„è¿™ä¸ªåŒºåˆ«ã€‚å¯¹ï¼Œç‰¹åˆ«å¥½ã€‚å—¯ï¼Œé‚£åˆšæ‰æˆ‘ä»¬è®¨è®ºçš„éƒ½æ˜¯å°±æ˜¯å¯èƒ½ç°åœ¨å­˜åœ¨çš„å‘ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œå¤šæ¨¡æ€æ¨¡å‹ã€‚

è¿˜æœ‰æ™ºèƒ½ä½“çš„ä¸€äº›å®‰å…¨æŒ‘æˆ˜ã€‚é‚£æœ€åä¸€ä¸ªéƒ¨åˆ†å…¶å®ä¹Ÿæƒ³è·Ÿæ€ä½è€å¸ˆå°±æ˜¯æ¢è®¨ä¸€ä¸‹å‘ƒæœªæ¥æœ‰å¯èƒ½å‡ºç°çš„æ›´å¼ºå¤§çš„å‘ƒé€šç”¨äººå·¥æ™ºèƒ½ï¼Œç”šè‡³æ˜¯è¶…çº§æ™ºè¶…çº§æ™ºèƒ½å¯èƒ½å¸¦æ¥çš„å¤±å¤±æ§é£é™©ã€‚é‚£å…¶å®æˆ‘æ³¨æ„åˆ°å°±æ˜¯å‘ƒè€€ä¸œè€å¸ˆï¼Œè¿˜æœ‰å‘ƒå¼ å®‹è€å¸ˆã€‚

åŒ…æ‹¬åˆšæ‰åœ¨å°ä¸Šçš„äºšç´è€å¸ˆã€‚ä»Šå¹´åœ¨å‘ƒä»Šå¹´3æœˆçš„æ—¶å€™ï¼Œåœ¨åŒ—äº¬çš„é¢å’Œå›­ï¼Œæœ‰å…±åŒå‚ä¸ç­¾ç½²äº†ä¸€ä»½å…³äºAIé£é™©çš„ä¸€ä¸ªå…±è¯†å£°æ˜ã€‚é‚£é’ˆå¯¹å‰AIçš„ä¸€äº›ç‰¹å®šçš„å±é™©èƒ½åŠ›ï¼Œåˆ’å®šäº†ã€‚5æ¡å®‰å…¨çš„çº¢çº¿ã€‚

é‚£ä¸å…¶ä¸­å‘¢ä¸è¿™ä¸ªAIçš„å¤±æ§é£é™©å¼ºç›¸å…³çš„ä¸€äº›çº¢çº¿ã€‚åŒ…æ‹¬å‘ƒæ¯”å¦‚è¯´è‡ªæˆ‘å¤åˆ¶ä¸é€‚åº”çš„èƒ½åŠ›ï¼Œè¿˜æœ‰æ¬ºéª—äººç±»çš„èƒ½åŠ›ï¼Œä»¥åŠè¿™ä¸ªå¯»æ±‚æƒåŠ›çš„å€¾å‘ã€‚

é‚£å…¶å®æ¥ä¸‹æ¥çš„è¿™ä¸ªç¯èŠ‚æƒ³è¦æŠ›ç»™å››ä½è€å¸ˆçš„é—®é¢˜æ˜¯å°±æ‚¨è®¤ä¸ºå°±å½“å‰å“ªä¸€äº›çš„å°±æ˜¯å‘ƒå±é™©èƒ½åŠ›çš„ç ”ç©¶åˆ¤æ–­æœ€ä¸ºç´§è¿«ï¼Œä»¥åŠå¯¹äºä¸€ä¸ªå°±æ˜¯ç›®å‰è¿˜å°šæœªå‡ºç°çš„ä¸€ä¸ªæœªæ¥æ™ºèƒ½ï¼Œæ›´å¼ºå¤§çš„ä¸€ä¸ªæ™ºèƒ½ã€‚ä»€ä¹ˆæ ·å­çš„æŠ€æœ¯æ–¹å‘ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥åšä»€ä¹ˆæ ·å­çš„æŠ€æœ¯æ–¹å‘èƒ½å¤Ÿå»æœªé›¨ç»¸ç¼ªã€‚ç„¶åèƒ½å¤Ÿå»åšä¸€äº›å‡†å¤‡ã€‚å‘ƒï¼Œè€€ä¸œè€å¸ˆï¼Œä½ æƒ³å…ˆå¼€å§‹å—ï¼Ÿå‘ƒé‚£å¯ä»¥å¯¹é‚£ä¸ªå‘ƒæˆ‘ä»¬ä»Šå¹´åœ¨å¹´å¤´çš„æ—¶å€™ï¼Œåœ¨é¢å’Œå›­å’Œå‘ƒå›½å†…å¤–è®¸å¤šä¸“å®¶åœ¨ä¸€å—ï¼Œæˆ‘ä»¬åœ¨è®¨è®ºã€‚

å°±æ˜¯å‘ƒå› ä¸ºè‹±å›½æœ‰è¿™ä¸ªå¸ƒè±åˆ‡åˆ©å®£è¨€å˜›ã€‚ç„¶åå‘ƒåŒ…æ‹¬åˆšæ‰ç»“æŸçš„è¿™ä¸ªé¦–é¦–å°”ä¼šè®®ï¼Œå…¶å®æˆ‘ä»¬å›½å®¶éƒ½å‚ä¸äº†è¿™ä¸ªæ·±åº¦çš„è®¨è®ºã€‚ä½†æ˜¯å‘¢å¯èƒ½åœ¨è¿™ä¸ªå›½å†…ä»¥ä¸­å›½çš„è¿™ã€‚ä¸ªå­¦è€…ä¸ºä¸»å¯¼çš„è¿™ä¹ˆä¸€ç³»åˆ—çš„è¿™ä¸ªè®¨è®ºå¹¶æ²¡æœ‰å‘ç”Ÿã€‚

æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™ä¸ªå¿—æºçš„é¢†å¯¼ä¸‹ï¼Œä¹Ÿæ˜¯è¯·äº†ä¸€ç³»åˆ—çš„è¿™ä¸ªå›½å†…å¤–çš„è¿™ä¸ªä¸“å®¶è¿›è¡Œäº†ä¸€ç³»åˆ—çš„ç ”è®¨ã€‚é‚£ç„¶åä¸€ä¸ªæ¯”è¾ƒæœ‰ä»£è¡¨æ€§çš„æˆæœï¼Œå°±æ˜¯åˆ’å®šäº†ä¸€äº›æ›´åŠ å…·ä½“å°±æ‚¨åˆšæ‰æ‰€è¯´å•Šï¼ŒåŒ…æ‹¬å¾ˆå¤šè¿™ä¸ªå°ä¸‹çš„ä¸“å®¶ã€‚

è¿˜æœ‰å®‹æ•™æˆéƒ½æ˜¯æˆ‘ä»¬è¿™ä¸ªreè¿™ä¸ªç­¾ç½²è€…ã€‚é‚£å…¶æ’åç¬¬ä¸€çš„è¿™ä¸ªé£é™©å•Šï¼Œå°±æ˜¯è¿™ä¸ªè‡ªæˆ‘å¤åˆ¶çš„è¿™ä¸ªé—®é¢˜ã€‚å…¶å®è¿™ä¸ªé—®é¢˜æˆ‘è®¤ä¸ºå¯èƒ½ç›®å‰è¿˜æ˜¯æœ‰ä¸€äº›è¿™ä¸ªä½ä¼°çš„è¿™ä¸ªè¶‹åŠ¿ï¼Œå°±åˆšæ‰é‚£ä¸ªPPTé‡Œé¢æœ‰ä¸€é¡µå…¶å®è®²çš„éå¸¸å¥½ã€‚

å°±æ˜¯å¯¹äºè¿™äº›è¯„æµ‹èƒ½åŠ›çº§ï¼Œæˆ‘ä»¬æ˜¯èƒ½çœ‹åˆ°éšç€å¹´ä»½çš„å¾€åå¢é•¿ã€‚å®ƒè¿™ä¸ªå­¦ä¹ çš„è¿™ä¸ªæ›²çº¿çš„è¿™ä¸ªæ–œç‡å…¶å®æ˜¯è¶Šæ¥è¶Šå¤§çš„é‚£æˆ‘è®¤ä¸ºç°åœ¨å¯èƒ½è¯­è¨€æ¨¡å‹å‘å±•çš„ä¸€ä¸ªè¶‹åŠ¿ï¼Œå¯èƒ½å¦‚æœæ‹¿é˜¿æ¯”çš„è¯ï¼Œè¿˜åœç•™åœ¨è¿™ä¸ªç¬¬ä¸€é˜¶æ®µæ˜¯å§ï¼Ÿ

å°±æ˜¯è¿™ä¸ªsupervisè¿™ä¸ªé˜¶æ®µå­¦ä¹ äººç±»è¿™ä¸ªæ•°æ®ã€‚é‚£ä½ ä¸€æ—¦å¾€åå•Šè¿›è¡Œè¿™ä¸ªselfplayã€‚arningpure reinforce learningå°±æ˜¯self improvedçš„è¿™ä¸ªé˜¶æ®µã€‚

å®ƒå¯èƒ½è¿™ä¸ªèƒ½åŠ›çš„æå‡ä¼šsomehowå¯èƒ½å°±çªç ´äº†æŸä¸ªstrreholdå°±æ˜¯ã€‚çªç„¶å¾€ä¸Šèµ°ï¼Œå¯¹å§ï¼Ÿå› ä¸ºä½ ä»ã€‚å›´æ£‹è¿™ä¸ªéå¸¸geçš„è¿™ä¸ªspaceçš„æ¢ç´¢æ¥çœ‹ï¼Œæˆ‘ä»¬ä¹Ÿæ˜¯æœ‰é˜¿é˜¿0å’Œé˜¿0ã€‚å…¶å®æˆ‘ä»¬åœ¨åšé˜¿ goçš„æ—¶å€™ã€‚

ä½ ä¹Ÿä¸èƒ½é¢„è§åˆ°åé¢ä¸¤ä¸ªç‰ˆæœ¬ï¼Œå®ƒæœ‰é‚£ä¹ˆå¤§èƒ½åŠ›çš„è¿™ä¸ªæå‡ã€‚æˆ‘è§‰å¾—è¿™ä¸ªself improvementè¿™ä¸ªäº‹å‘¢å¯èƒ½å’Œå’Œå’Œå’Œè¿™ä¸ªå‘ç°ä¼šæ¯”è¾ƒæœ‰å…³ç³»ã€‚é‚£ä»å­¦æœ¯ç ”ç©¶çš„è¿™ä¸ªè§’åº¦ä¸Šæ¥è®²ã€‚

æˆ‘ä»¬ç¡®å®å‘ç°å•Šç°åœ¨å·²ç»æœ‰éå¸¸å¤šçš„è¿™ä¸ªselfplayHFå•ŠOAIFå•Šç¡®å®èƒ½å¤Ÿåœ¨æŸç§æ„ä¹‰ä¸Šå•Šæå‡æ¨¡å‹çš„èƒ½åŠ›å•Šï¼Œæ— è®ºåœ¨æ•°å­¦è¿˜æ˜¯ä»£ç èƒ½åŠ›ä¸Šï¼Œé‚£å¯èƒ½åŠ ä»¥æ›´å¤§çš„è¿™ä¸ªç®—åŠ›å’Œæ›´è¿™ä¸ªé«˜æ•ˆçš„è¿™ä¸ªè‡ªå‘ƒè‡ªåšå¼ˆçš„è¿™ä¸ªæœºåˆ¶ã€‚

å°¤å…¶æ˜¯åœ¨äººç±»è¯­æ–™ç”¨å°½ä¹‹åï¼Œæ˜¯ä¸æ˜¯èƒ½å¤Ÿé€šè¿‡åšå¼ˆçš„è¿™ä¸ªæ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜è¯­æ–™çš„è¿™ä¸ªè´¨é‡ï¼Œè¿›ä¸€æ­¥æå‡è®­ç»ƒçš„è¿™ä¸ªéš¾åº¦å’Œæœ‰æ•ˆæ€§ã€‚é‚£å¦‚æœè¿™ä¸ªé—®é¢˜èƒ½è¢«çªç ´çš„è¯ï¼Œé‚£å¯èƒ½æˆ‘ä»¬æ‰€è°“çš„è¿™ä¸ªè‡ªæˆ‘å¤åˆ¶çš„å’Œå’Œseçš„è¿™ä¸ªé£é™©ã€‚

ç¡®å®èƒ½å˜åˆ°ä¸€ä¸ªå…·ä½“çœ‹å¾—è§çš„è¿™ä¹ˆä¸€ä¸ªé£é™©å•Šï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™ä¸ªæƒ³è¿™ä¸ªre linesçš„æ—¶å€™ï¼Œå°±æ˜¯æŠŠè¿™ä¸€æ¡å•Šç»™å®ƒæ”¾è¿›å»äº†ã€‚å•Šï¼Œç„¶ååé¢å…¶å®è¿˜æœ‰ä¸€äº›é£é™©åƒdeceptionå‘¢ã€‚

è¿˜æœ‰ä¸€äº›è¿™ä¸ªmissuseç›¸å…³çš„è¿™ä¸ªé£é™©ã€‚é‚£ä¸ªå…¶å®æˆ‘æˆ‘æˆ‘è®¤ä¸ºå¯èƒ½ç›¸æ¯”äºabuseæ›´å¤šçš„æ˜¯åœ¨missuseè¿™ä¸ªé˜¶æ®µã€‚é‚£é‚£ä¸ªå¯èƒ½éœ€è¦æ›´å¤šçš„è¿™ä¸ªå›½é™…çš„å¯¹è¯ï¼Œå›½é™…çš„è¿™ä¸ªæ²»ç†ã€‚

é‚£æˆ‘ç›¸ä¿¡åŒ—äº¬çš„AIå®‰å…¨å…±è¯†å•Šä¹Ÿæ˜¯åœ¨å¾€è¿™ä¸ªæ–¹å‘å•Šå»è¿›è¡Œä¸€ä¸ªæ¨è¿›å•Šï¼ŒåŒ…æ‹¬æˆ‘æ³¨æ„åˆ°æˆ‘ä»¬å¤§ä¼šä»Šå¹´ä¸Šæµ·ä¹Ÿå‘å¸ƒäº†ä¸Šæµ·å¸‚æ”¿åºœçš„è¿™ä¸ªäººå·¥æ™ºèƒ½å›½é™…æ²»ç†å€¡è®®å®£è¨€ï¼Œä¹Ÿå°±å¸Œæœ›å•Šèƒ½å¤Ÿåœ¨è¿™ä¸ªå›½é™…çš„è¿™ä¸ªåˆä½œä¸Šèƒ½å¤Ÿæ¨åŠ¨è¿›ä¸€æ­¥çš„åˆä½œã€‚

æˆ‘è®¤ä¸ºè¿™ä¸ªæ–¹å‘éƒ½æ˜¯éå¸¸å¥½çš„ã€‚å¥½ï¼Œè°¢è°¢å§šæ€»è€å¸ˆã€‚å¯¹ï¼Œå¦å¤–ä¸‰ä½è€å¸ˆè°æƒ³å…ˆå¼€å§‹ï¼ŸOkay yeah I can add that I think rightï¼Œ so so todayã€‚

 even though the large language models are already very powerful but we know that they still actually we are still at an early stage I think and the next step already people are talking about so for exampleã€‚

 like having embodies intelligence between robots with these foundation models essentially so right now we are still just training we have the pretraining phase like for large language models and then we right and we'll do the inference but in the future as we do embody intelligence and also as we have agents that's actually going to actã€‚

ğŸ˜Šï¼ŒInï¼Œ environmentï¼Œ we are going to have a more of a close loop where the agents take inputs from the environment and thenã€‚

And then try to make decisions and then get feedback and use that feedbackã€‚

 It can then help itself to further improveï¼Œ do self learningï¼Œ to continuous learning and so onã€‚

 So I think as we get into this more of this approach thenã€‚Essentiallyï¼Œ we are essentiallyã€‚

 we are making the learning also into the next stageã€‚ And I think what we are concerned about isã€‚

 for exampleï¼Œ right nowï¼Œ even though with large language models alreadyã€‚

 you can say the model can try to when you give the taskï¼Œ it canã€‚ğŸ˜Šï¼ŒIf you tell us think step by stepã€‚

 it can also break down a task into different subtests and goalsï¼Œ butã€‚Stillã€‚

 that's not a very strong capabilityã€‚ but in the futureï¼Œ as these agentsã€‚

Become more autonomous and also become more powerful in particular for a given goal is going to be able to break down into sub goals and then figure out what's the best way to to accomplish these sub goalsã€‚

 That's where we are also worried about like this paper clip have our problems where it can derive these these dangerous sub goals thats actually not well aligned and so onã€‚

 And then and then in this caseï¼Œ rightï¼Œ it could have other sub goalsã€‚

 including right how it can get more power And then how it can deceive humans and others to get more power and then and how it can solve replicate to sustain and itself and so onã€‚

 So also rightï¼Œ as you don't mentioned earlierï¼Œ I think right nowã€‚

 so we are now seeing these capabilities yetã€‚ But the first is really important that we develop methods to do earlyã€‚

ğŸ˜Šï¼ŒDetectionï¼Œ right its like a canary and so onã€‚ But also the other thing is that these type of behaviorsã€‚

 the moment you see itï¼Œ it's very possible that the time duration you have it's veryï¼Œ very shortã€‚

 you can think about it basically the moment you see it it probably already it has already startedã€‚

 the self improvement cycleã€‚ And as we know as they gathers on computer And so the selfimp cycle can go reallyã€‚

 really fastã€‚ So I think this is the challengeã€‚ And for a lot of people who don't work in frontier AI safetyã€‚

 I think the thing that they miss even though they can say that's why earlierã€‚

 also you also mention a lot of people sayï¼Œ ohï¼Œ you knowã€‚

 we don't need to worry about these risks very far outã€‚

 but I think those people what they don't recognize is that the moment you see itã€‚

 it could be already too lateã€‚ So so I think these are the challenges are we need to addressã€‚ğŸ˜Šï¼Œå¯¹ã€‚

è°¢è°¢å½“å®‹è€å¸ˆã€‚å‘ƒï¼Œåˆšæ‰è¯´åˆ°äº†è¿™ä¸ªearly detectionå‘ƒï¼Œå…¶å®æˆ‘ä»¬ä¸‹ä¸€ä¸ªsessionå…¶å®å°±æ˜¯è®²è¿™ä¸ªevaluationAIC testingestingã€‚

æ‰€ä»¥åé¢ä¹Ÿå¯èƒ½æœ‰ä¸€äº›è®²è€…ä¼šæœ‰ä¸€äº›æ›´å¤šçš„inssã€‚å¯¹ï¼Œå—¯ï¼Œå¦å¤–å°±æ˜¯å‘ƒå¯¹äºåˆšåˆšæ‰è¿™ä¸ªé—®é¢˜ï¼Œé‚µé™è€å¸ˆå’Œå“èƒœè€å¸ˆæƒ³è¦å‘ƒä¹Ÿè¯„è®ºä¸€ä¸‹ã€‚æˆ‘åˆšæ‰å‡ ä½è€å¸ˆå·²ç»è¯´çš„éå¸¸å…¨é¢äº†ï¼Œæˆ‘å¯èƒ½å°±æœ‰ä¸€ç‚¹é‚£ä¸ªå°çš„æ„Ÿå—ã€‚

å°±æ˜¯å‘ƒå…¶å®åˆšæ‰é‚£ä¸ªå“ç”Ÿè€å¸ˆä¹Ÿè®²åˆ°è¯´è¿™ä¸ªagentåœ¨å¾ˆå¤šè¿™ä¸ªåœºæ™¯é‡Œé¢mentæœ‰è¿™ç§äº¤äº’ã€‚ç„¶ååœ¨åº”ç”¨çš„ç¯å¢ƒé‡Œé¢ï¼Œå®ƒå—åˆ°å¾ˆå¤šå› ç´ çš„å½±å“ï¼Œå°±ä¸åªæ˜¯æœ¬èº«æ¨¡å‹è‡ªå·±çš„å®‰å…¨æ€§é—®é¢˜ï¼Œæ„Ÿè§‰è¿™æ–¹é¢çš„é—®é¢˜ã€‚

æœªæ¥ä¹Ÿä¼šéå¸¸çš„è¿™ä¸ªå‡¸æ˜¾ã€‚ç„¶åæ¯”å¦‚è¯´åƒæˆ‘ä»¬å®éªŒå®¤é‡Œé¢ä¸ä»…æˆ‘ä»¬æ•´å®éªŒå®¤åšIçš„å˜›ï¼Œç„¶åé‡Œé¢æœ‰å¾ˆåšçš„ä¸“å®¶è€å¸ˆé‚£åœ¨è¿™ä¸ªsenceå­¦ç§‘é‡Œé¢ï¼Œå…¶å®ç°AIçš„æ¸—é€ä¹Ÿä¼šè¶Šæ¥è¶Šå¼ºã€‚é‚£åœ¨è¿™é‡Œé¢ç›¸å…³çš„ç ”ç©¶ã€‚

å…¶å®ç°åœ¨å¹¶æ²¡æœ‰åšçš„ç‰¹åˆ«å¤šã€‚åå¤§å®¶å¯èƒ½æ›´å…³æ³¨è¿™ä¸ªIåœ¨ä¸€äº›è¿™ä¸ªæ¯”è¾ƒå‘ƒè·Ÿæˆ‘ä»¬å¹³æ—¶æ—¥å¸¸ç”Ÿæ´»æ¥è§¦æ¯”è¾ƒå¤šçš„è¿™äº›ç¯å¢ƒé‡Œé¢çš„å®‰å…¨æ€§é—®é¢˜ã€‚æ¯”å¦‚åˆšæ‰æåˆ°äº†æ»¥ç”¨é—®é¢˜ä¹‹ç±»çš„é‚£æˆ‘ä»¬å¯èƒ½ä¹Ÿä¼šåŒæ—¶å»å‘¼åå¤§å®¶å…³æ³¨ä¸€äº›åœ¨è¿™ç§ç‰¹å®šé¢†åŸŸçš„ã€‚

æˆ–è€…æ˜¯åœ¨å‚ç›´é¢†åŸŸçš„è¿™äº›å®‰å…¨æ€§é—®é¢˜ã€‚æœªæ¥è¿™æ–¹é¢çš„è¿™ä¸ªå°±å¯èƒ½å¸¦æ¥çš„è¿™ä¸ªåŒ—æµ·å½±å“ä¹Ÿä¼šæ›´å‡¸æ˜¾å°±åšè¿™ä¸€ç‚¹è¡¥å……ã€‚ğŸ˜Šï¼Œå‘¨èƒœè€å¸ˆå‘ƒï¼Œå¥½çš„ï¼Œæˆ‘ä¹Ÿæ˜¯å•Šæ²¿ç€ç»å‘ƒç»é‡‘è€å¸ˆç»§ç»­è¡¥å……ä¸€ä¸‹ã€‚

å…¶å®æˆ‘ç›®å‰ä¸€ç›´å…³æ³¨çš„å°±æ˜¯å¤§æ¨¡å‹æ™ºèƒ½ä½“çš„å®ƒåœ¨è¿™ä¸ªå¼€æ”¾ç¯å¢ƒä¸­è¿™ä¸ªè¡Œä¸ºäº¤äº’çš„å®‰å…¨ã€‚å°±åƒä¸€å¥è¯è¯´å°±æ˜¯è¿™ä¸ªæ„æ€ã€‚ç„¶ååœ¨è¿™ä¸ªè¡Œä¸ºäº¤äº’è¿‡ç¨‹ä¸­ï¼Œå®ƒçš„å®‰å…¨é—®é¢˜ä¸»è¦æ˜¯ä½“ç°åœ¨æˆ‘ä»¬ç°åœ¨éƒ½å€¾å‘äºæŠŠå¤§æ¨¡å‹ç”¨åˆ°å‘ƒå·¥ä¸šæ§åˆ¶ã€‚

æŠŠå®ƒç”¨åˆ°ç§‘å­¦ç ”ç©¶ï¼Œä»¥åŠæˆ‘ä»¬ç°å®çš„è¿™ä¸ªç”¨æˆ·çš„è¿™ä¸ªç”Ÿæ´»åœºæ™¯ä¹‹ä¸­ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªé‡Œé¢å®ƒå°±ä¼šæ¶‰åŠåˆ°åˆšåˆšæåˆ°çš„ä¸€ç³»åˆ—å®ƒè¢«æ»¥ç”¨æˆ–è€…è¢«åŠ«æŒå•Šï¼Œå¯èƒ½ä¼šå¯¹ç¯å¢ƒé€ æˆå½±å“ï¼Œå¯èƒ½å¯¹ç”¨æˆ·é€ æˆæŸå®³ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ã€‚

æˆ‘ä»¬è¦å»ç¡®ä¿å®ƒçš„å®‰å…¨ã€‚ä»–å°±éœ€è¦éå¸¸å‘ƒéœ€è¦ä¸€ç³»ç»Ÿçš„è¿™ä¸ªè§£å†³æ–¹æ¡ˆäº†ã€‚å®ƒä¸ä»…ä¸ä»…ä»…æ˜¯åœ¨äºå¤§æ¨¡å‹æœ¬èº«çš„ï¼Œæˆ‘ä»¬ä»¥å•ŠAIGCå†…å®¹ä¸ºä¸»çš„è¿™ç§å•Šç”Ÿè¿™ç§å†…å®¹å®‰å…¨ç›¸å…³çš„è¿™äº›ç ”ç©¶ï¼Œæ€ä¹ˆå»æå‡æ¨¡å‹æœ¬èº«çš„è¿™ä¸ªå®‰å…¨æ€§ã€‚è¿™æ˜¯ä¸€æ–¹é¢ã€‚

ä½†æ˜¯ç¬¬äºŒæ–¹é¢ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€å¥—éå¸¸å®Œå¤‡çš„è¿™ä¸ªã€‚å‘ƒï¼Œç›‘ç®¡æ¨¡å‹æˆ‘ä»¬éœ€è¦å»åŠ¨æ€çš„å»ç›‘æµ‹è¿™ä¸ªæ™ºèƒ½ä½“çš„è¿™ä¸ªè¡Œä¸ºè¿‡ç¨‹ï¼Œå®ƒæ˜¯å¦ä¼šå¸¦æ¥ä¸€å®šçš„æŸå®³ï¼Œå¯¹å®ƒè¿›è¡Œæœ‰æ•ˆçš„ç ”åˆ¤ã€‚

ç„¶åç¬¬ä¸‰ä¸ªæ˜¯è¿™ä¸ªå‘ƒå½“å®‹è€å¸ˆä¸€ç›´æåˆ°çš„å…³äºè¿™ä¸ªç³»ç»Ÿçš„çº¢çº¿çš„é—®é¢˜ã€‚å°±æ˜¯æˆ‘ä»¬å¯¹äºä¼ ç»Ÿå®‰å…¨é‡Œé¢ï¼Œæˆ‘ä»¬æœ‰ä¸€ç³»åˆ—çš„è¿™äº›å‘ƒã€‚å®‰å…¨çš„è§„èŒƒã€‚æˆ‘ä»¬æ€ä¹ˆæŠŠå¤§æ¨¡å‹çš„è¿™ä¸ªå‘ƒé€šç”¨æ€§è·Ÿè¿™äº›å®‰å…¨è§„èŒƒç»™ç»“åˆã€‚

ç„¶åå®ç°ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ä¸€ä¸ªå‘ƒç›‘æµ‹ã€‚è¿™æ ·çš„è¯ä¸€æ–¹é¢èƒ½èŠ‚çœæˆ‘ä»¬åšè¿™ä¸ªç½‘ç»œå®‰å…¨ç›‘æµ‹çš„ä¸€ä¸ªæ•ˆç‡ã€‚å¦ä¸€æ–¹é¢ä¹Ÿèƒ½æŠŠå¤§æ¨¡å‹çš„è¿™ä¸ªé€šç”¨æ€§ç»™å‘æŒ¥å‡ºæ¥ï¼Œå®ç°æ›´åŠ å¹¿æ³›çš„è¿™ä¸ªå‘ƒç”¨é€”ã€‚å½“ç„¶åœ¨è¿™ä¸ªæ•´æ€»ä½“è¿‡ç¨‹ä¸­å‘¢ã€‚

å…¶å®æˆ‘ä»¬ç°åœ¨éƒ½æ˜¯å€¾å‘äºä»å¤§æ¨¡å‹æœ¬èº«æ¥åšã€‚ä½†è¿™é‡Œé¢è¿˜æœ‰ä¸€ä¸ªå¾ˆé‡è¦çš„ç‚¹ï¼Œå°±æ˜¯åˆšåˆšæåˆ°çš„è¿™ä¸ªåŠ¨æ€çš„æ£€æµ‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªactiveçš„ä¸€ä¸ªä¸€ä¸ªæ£€æµ‹è¿‡ç¨‹ã€‚è€Œè€Œä¸æ˜¯è¯´ç­‰æ¨¡å‹è¡Œä¸ºåšå®Œäº†ã€‚è¿™æ—¶å€™æˆ‘åœ¨æ£€æµ‹ã€‚

é‚£ä¹ˆå¯èƒ½è¿™ä¸ªæ—¶å€™å±å®³å·²ç»é€ æˆäº†æˆ‘ä»¬æ˜¯å¾ˆéš¾å»å¼¥è¡¥çš„ã€‚æ‰€ä»¥ä»æŠ€æœ¯ä¸Šæˆ‘è®¤ä¸ºå…¶å®æ˜¯åˆ†æˆå•Šï¼Œæˆ‘è§‰å¾—ä¸€ä¸ªéå¸¸å¤§ä»è¿™ä¸ªæ–¹å‘ä¸Šæˆ‘è§‰å¾—éå¸¸é‡è¦çš„ä¸€ä¸ªç‚¹ï¼Œå°±æ˜¯åœ¨äºæ™ºèƒ½ä½“åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„è¿™ä¸ªè¡Œä¸ºå®‰å…¨é—®é¢˜ã€‚

ç„¶åæŠ€æœ¯ä¸Šå¯èƒ½æˆ‘ä»¬éœ€è¦ä»å¤§æ¨¡å‹æœ¬èº«çš„å†…ç”Ÿå®‰å…¨ã€‚ç„¶åä»¥åŠè¿™ä¸ªå‘ƒè¡Œä¸ºäº¤äº’è¿‡è¿‡ç¨‹ä¸­çš„è¿™ä¸ªåŠ¨æ€æ£€æµ‹ä»¥åŠç½‘ç»œå®‰å…¨çš„è¿™ä¸ªç³»ç»Ÿçº¢çº¿ç­‰ä¸‰ä¸ªæ–¹é¢ã€‚è¿›è¡Œè¿™ä¸ªç³»ç»Ÿæ€§çš„è¿™ä¸ªé˜²å¾¡ã€‚ç„¶åæŠ€æœ¯æ‰‹æ®µä¸Šã€‚

æˆ‘ä»¬ä¸ä»…åŒ…æ‹¬ç°æœ‰çš„å„ç§é™æ€çš„æ‰‹æ®µï¼Œè¿˜éœ€è¦ä¸€äº›ä¸»åŠ¨çš„æ‰‹æ®µæ¥è¿›è¡Œè¿™ä¸ªçº¦æŸã€‚ç„¶åè¿™ä¸ªæ˜¯æˆ‘çš„ä¸€äº›è¿™ä¸ªè§‚ç‚¹ã€‚å¯¹ï¼Œè°¢è°¢å“èƒœè€å¸ˆå•Šã€‚é‚£ç”±äºæ—¶é—´å…³ç³»å‘¢ï¼Œæˆ‘ä»¬ä»Šå¤©çš„ç¬¬ä¸€åœºåœ†æ¡Œè®¨è®ºå¯èƒ½åœ¨è¿™é‡Œå°±ç»“æŸäº†ã€‚

å°±æ˜¯å‘ƒä¹Ÿç‰¹åˆ«æ„Ÿè°¢å„ä½è€å¸ˆä»Šå¤©çš„ç²¾å½©è§‚ç‚¹ã€‚é‚£å‘ƒæˆ‘ä»¬è¯·å„ä½è€å¸ˆè¿”å›å‰æ’å°±åº§ï¼Œæˆ‘æŠŠæ—¶é—´äº¤ç»™ä¸»æŒäººå›æ€¡ã€‚A big thank you again to all of our panelistsã€‚

 Given that just the last panel mentioned continuous monitoring and evaluationsã€‚

 this is also a great time to transition to our second theme on AI safety testingã€‚ğŸ˜Šï¼ŒNowã€‚

 we will hear from Drã€‚ Chris Massroã€‚Drã€‚ Mesll is the executive director of the Frontier Model Forumã€‚

 a non nonprofit established by Anthropicï¼Œ Googleï¼Œ Microsoft and Open AI to advance frontier AI safetyã€‚

ğŸ˜Šï¼ŒHe is an expert on AI governance and security and is currently focused on developing best practices for the responsible development and deployments of the most advanced general purpose AI systemsã€‚

ğŸ˜Šï¼ŒChris previously served as the director of the AI and Emerging Technology Initiative at the Brookings Institutionã€‚

Chrisï¼Œ it's great to have you hereã€‚ I'll hand it over to youã€‚ğŸ˜Šã€‚

Thank you it's a pleasure to be here it's wonderful to be able to speak with you today as was just mentioned I run an organization called the Frontier Model Forumã€‚

 it's an industry supported nonprofit dedicated to advancing frontier AI safetyã€‚

 we have three kind of core missionsï¼Œ one of which I'll get into which is developing best practicesã€‚

 the other two are advancing the science of frontronier AI safety and the third is information sharing about what we're learning which is again part of why we're so excited to be here todayã€‚

I thought I might begin with just laying out a little bit what frontier AI is and why it's so challenging to deal with and then kind of walk through a couple of some early thinking that we have about how to think through you what types of evaluations to run and what are some early best practices that in discussions with our expert membersã€‚

 the safety experts within our member firmsï¼Œ what they're seeing and thinking about and how they're beginning to approach some of these issuesã€‚

So just to start withï¼Œ I think when we say the phrase frontier AIã€‚

 what we're generally referring to is the most recent generation of advanced general purpose AI technologiesã€‚

 right So what we're thinking of are not narrow AI applications for specificã€‚

 you know things like lending algorithms or facial recognition technologiesã€‚

 we are thinking about general purpose AI systemsï¼Œ and we're thinking about in particularã€‚

 just the most recent generationã€‚ So you knowï¼Œ on this chartï¼Œ you can see thatã€‚

Because of the way that we're scaling up these systemsï¼Œ generally speakingï¼Œ we're kind of doing aã€‚

 you knowï¼Œ10 X in terms of compute every couple of years to come up with a better class and generation of model forã€‚

 for general purpose systemsã€‚We are primarily focused on the most recent generation of frontier AI systemsã€‚

 and if you want to see more about thisï¼Œ we have an illustration of this on our websiteã€‚

 but what this means why this is so important is that we expect the challenges that we are dealing with to evolve over timeã€‚

 The frontier is going to be consistently changingã€‚ As you can seeï¼Œ this is a stylized graphã€‚

 but some of the graphs that we saw earlier this morning saw a very clear slopelineã€‚

 almost exponential curve of increase in capabilitiesã€‚

We are focused on just the most recent generation because we want to understand we want to develop early best practices for dealing with the most advanced models at any particular moment in timeã€‚

And the reason this is so importantï¼Œ as was alluded to earlier today alreadyã€‚

The reasona this is so implorability to gr certain capabilitiesã€‚

 And we don't know how to predict when these models in a training run are going to acquire or develop particular capabilitiesã€‚

 Weï¼Œ we don't have a good way asï¼Œ as Professor Song alluded to earlierã€‚

 We don't have a good way to understand the systems Xantteã€‚

 which makes it very hard to understand how to how to build them safely and effectivelyã€‚

And I would say the last point I would say is we expect the frontier to continue developing as these systems move from just chatbots to things that are a little bit more agentic in natureã€‚

 this challenge of assuring the safety of these systems is only going to become more importantã€‚

 because the systems we're building will interact more and more with the real world in ways that have potential consequences for public safety and securityã€‚

 which is what our organization is focused onã€‚So as I mentionedã€‚

 I'm just going to walk through a little bit some of our early thinking that's been developed in kind of conversations with different safety experts in the member firms that we have about how to structure evaluationsã€‚

 what kinds of evaluations to run and then some early best practicesã€‚

 these are very high levell descriptions that we'll be talking aboutã€‚

 I would also say the terms themselves may varyï¼Œ but it's really the concepts that I want to share with you today that hopefully we can have more engagements and interactions over time to begin as a field to develop best practices when it comes to even just talking about the types of evaluations we need to run to assure the safety of our systemsã€‚

So the first phrase is weï¼Œ I think at a very high level there's two very general kinds of evaluations or risk assessmentsã€‚

 one are red teaming exercisesï¼Œ another are more automated evaluationsã€‚

 red teaming exercises tend to be very manual and kind of you know there's early there's work going on to try and explore how to automate some of the red teaming exercisesã€‚

 but generally speaking their manual ways of leveraging human expertise to probe the capabilities of a particular modelã€‚

Evaluations in contrast tend to be things like benchmarks or other automated forms of exploring the capability profile or the risk profile of a particular modelã€‚

 we think it's important to distinguish exactly what you're talking about when you're talking about how you're assessing the risk or safety of a system and this is just one general high levelvel class of distinctionã€‚

Within the evaluations of frontier modelsã€‚There's really twoï¼Œ I thinkã€‚

 core types of evaluations that we want to runã€‚One are performance evaluations and the other are safety evaluationsã€‚

Performance evaluations are critical for understanding the general kind of reasoning capabilities or other capabilities that a model might have that allows us to understand in some ways how best to test it for particular risksã€‚

 etcï¼Œ but a performance evaluation is really designed to just capture and identify and assess the performance envelope of a particular systemã€‚

 again these evaluations are incredibly importantã€‚Because we don't know Xantte how to defineã€‚

 you know the X post capabilities of a modelã€‚ We don't know how to define before we train the model what it will be capable of on the backendã€‚

 So we need to be able to do performance evaluationsã€‚

 The other kinds of evaluations are safety evaluationsã€‚

 and there you're not necessarily trying to understand just what the performance threshold or performance envelope of a system is you are specifically looking for particular risks and the ability of a model to exhibit behaviors that would give you or that would cause you to believe that it is capable of behaving in unsafe waysã€‚

As far as kind of different types of safety evaluationsã€‚There's really two safetyï¼Œ you knowã€‚

 classes of safety evaluations thatï¼Œ you willll start to see kind of being developed and run in model developmentã€‚

One are developmental evaluations and another are assurance evaluations developmental evaluations you what we're referring to here are really the kinds of evaluations that firms will run or the developers of a largescale fronti system might run at different phases in its training cycle just to kind of benchmark to see how it's doing with respect to certain kind of safety risks that's different from a fullon assuranceur evaluation where it's not necessarily the team that's developing the model instead it's a team that's kind of tasked with assuring the safety of a systemã€‚

 they have independent expertise from the team that's developing it and their goal is really to assure the safety of the system and to develop evaluations that are capable of assuring the safety of a system in some way which is a little bit different than the kind of lifecycle safety evals that might happen just at different check marks in the development of the modelã€‚

Then the last kind ofï¼Œ in my viewï¼Œ probably the most important distinction here isã€‚

Within assurance evaluationsã€‚ So the evaluations that are meant to try and assure the safety of a systemã€‚

Within that categoryï¼Œ as we're thinking about trying to evaluate models for safetyã€‚

We really need to be evaluating the safety and assurance of these systemsã€‚

For one way of thinking about it is maximum capability of the systemã€‚

 another is like how it's used in the real worldã€‚I would say a different way of kind of defining this last category isã€‚

We need to look for assurance evaluations that are designed to try and capture the riskiest behaviors at the tail distribution of the modelã€‚

 rightï¼Œ like the some of the behaviors that areï¼Œ you knowã€‚

 most capable or most extreme from a particular risk that wouldn't necessarily be compressed into kind of the average or mean behaviorã€‚

 We wouldn't be able to get a lot of information about those kinds of tail risks from more typical user behaviorã€‚

With behavioral evaluationsï¼Œ I think the goal is more to try and understand what is the average behavior or mean behavior in general with some of these models and how do we assure the system and the safety of the system even within that kind of mean behaviorã€‚

Againï¼Œ this is kind of early thinking will probably evolve over time on thisã€‚

 but this is just a little bit of how we're thinking about some of the different evaluations at this momentã€‚

å—¯ã€‚Relatedly there's also the question about what to do as you're setting up an evaluation andingã€‚

 whether it's a red teamings exercise or a broader evaluationã€‚

We're developing best practices for a wide array of evaluations for specific risksã€‚

 but there's also a set of just best practices for any kind of evaluation you're doingã€‚

 regardless of the risk so it doesn't matter whether you're looking at bio risksks or cyber risksã€‚

 things like that or it could be societal risk that you're looking at if you are developing a system or an evaluation rather of a frontier AI model or systemã€‚

 there's someï¼Œ I think important considerations at a meta level as you're designing that system and I think this is just a sampling of some of the early thinking we have about high-level best practices there's a few that I want to call out specifically one is we need evaluations to account for prompt sensitivity I think any of the engineers here who have worked with these models and try to get them to behave in stable ways will recognize that the specific wording of different prompts will oftentimes lead to different resultsã€‚

AndWhat we're really trying to do with these evaluations is capture their kind of risky behaviorã€‚

 which means we need to explore different wording wording choices or configurations of prompts to be able to get at whether or not it has a certain just capability in general and you an example of this would be if you're trying to if you're worried aboutã€‚

 say like malicious uses of a systemï¼Œ for exampleï¼Œ you don't want to just ask the system know how do you build an explosive or something like thatã€‚

 you also want to test for can you describe the chemical process by which you know dynamite releases energyã€‚

 something like thatï¼Œ you need to have multiple ways of asking for the same thing two other at least two other kind of things that I want to call out just very brieflyã€‚

One is as you're developing these evaluationsï¼Œ you need to evaluate both the modelã€‚

 the underlying base model as well as the end systemã€‚

 I think a lot of you know for a lot of evaluations we'll target one or the otherã€‚

 but we need to do both it's not just the underlying model that needs to be evaluated because in many cases that's not what is exposed to the end user usually what is exposed to the end user is the overall system and we need to be able to test that as well as the underlying modelã€‚

Another really important best practiceã€‚ I guess I'll walk through them just brieflyã€‚

 One is evaluating both normal or typical behavior use and adversarial useã€‚

 As Ive mentioned on a prior slideï¼Œ we do want to evaluate these systems for kind of typical behaviors and the safety that they might exhibit under thatã€‚

 we also want to evaluate for adversarial useã€‚ This is something that a lot of developers won't instinctively necessarily do as they're trying to you know rush to get a product out the doorã€‚

 but it's very important to evaluate adversarial useã€‚

 Professor Song just had a really great explanation of some of the kinds of adversarial use cases that you need to be paying attention to know the broader point thoughã€‚

 is if you are developing frontieri systemsï¼Œ it's not enough to just focus on you allll end onã€‚

 because I think it's probably the most important best practice that we are starting to converge on as a field in AI safety when it comes to evaluation designã€‚

It is vitally important that you understand is the like what the baseline is that you are evaluating a system against and what I mean by thatã€‚

 you knowï¼Œ for exampleï¼Œ if you want to evaluate a system for biological risksï¼Œ for exampleï¼Œ you knowã€‚

 can this system inform or help someone design a bioweapon or some kind of dangerous pathogenï¼Ÿ

You need to evaluate it not just for the you know what kind of information the model itself hasã€‚

 you need to evaluate it against the kind of baseline application that would be used in the absence of whatever model you're testingã€‚

 so in many cases that would be for example web search right like the goal of an evaluation should not just be kind of absolute understandings of riskã€‚

 but also you know relative or marginal risk compared to the counterfactual application that might be used for whatever application you're developing that is a very brief kind of high levell overview of the know how we're thinking about some of the early best practices for just evaluations in general of information on communications technologyã€‚

Thereta Wei has LED the development of multiple industry standards in Chinaã€‚

 including the country's first big data benchmark standardã€‚

He has participated in the drafting of several major national policiesã€‚

 includingä¸­æ–‡æ¥è®²è§£é‚£ä¹ˆå‘ƒæˆ‘å¸Œæœ›å‘ƒè¿™ä¸ªå„ä½å‘ƒå¤–å›½æœ‹å‹èƒ½å¤Ÿé€šè¿‡å‘ƒåŒä¼ èƒ½å¤Ÿå¾ˆå¥½çš„getåˆ°æˆ‘çš„å‘ƒè¿™ä¸ªä¸»è¦çš„æ„æ€ã€‚å‘ƒã€‚

é‚£ä¹ˆä»Šå¤©å‘¢å°±æ˜¯è·Ÿå¤§å®¶åˆ†äº«ä¸€ä¸‹ä¸­å›½ä¿¡æ¯é€šä¿¡ç ”ç©¶é™¢åœ¨å‘ƒäººå·¥æ™ºèƒ½å°¤å…¶æ˜¯å¤§æ¨¡å‹å‘ƒè¯„æµ‹å®‰å…¨è¯„æµ‹æ–¹é¢çš„ä¸€äº›å‘ƒæ€è€ƒå’Œå®è·µã€‚å‘ƒï¼Œä¸‰ä¸ªæ–¹é¢çš„å†…å®¹ã€‚å‘ƒï¼Œä¸€ä¸ªæ˜¯å‘ƒæˆ‘ä»¬æ€ä¹ˆè®¤è¯†äººå·¥æ™ºèƒ½å¤§æ¨¡å‹é¢ä¸´çš„é£é™©ã€‚

è¿™ä¸ªriskåˆ°åº•æ˜¯åœ¨å“ªäº›ç»´åº¦ä¸Šã€‚å‘ƒï¼Œå¦å¤–ç¬¬äºŒä¸ªæ–¹é¢æ˜¯å‘ƒæˆ‘ä»¬åŸºäºè¿™äº›å¯¹äºé£é™©ç°æœ‰é£é™©çš„è¿™ä¸ªè®¤è¯†å‘ƒï¼Œä¸€ç›´åœ¨æ¨åŠ¨å»ºè®¾bench mark safety benchmarkch markçš„è¿™ä¸ªframemeworkå¹¶ä¸”æŒ‰å­£åº¦å»å¼€å±•è¿™ä¸ªè¯„æµ‹çš„æ´»åŠ¨ã€‚

å‘ƒï¼Œå¸Œæœ›é€šè¿‡è¿™ä¸ªå®è·µå‘¢ã€‚ä¸æ–­çš„ä¿ƒè¿›å¤§å®¶å‘ƒè¿™ä¸ªæ¥æå‡æ¨¡å‹çš„å®‰å…¨æ°´å¹³ã€‚å•Šï¼Œé‚£æœ€åå‘¢ä¹Ÿæ˜¯å†åˆ†äº«ä¸€ä¸‹å…¶ä»–æœ‰å…³å¦‚ä½•ä¿éšœäººå·¥æ™ºèƒ½è´Ÿè´£ä»»å‘å±•å®‰å…¨æ–¹é¢çš„ä¸€äº›ç›¸å…³çš„å·¥ä½œã€‚å‘ƒï¼Œé‚£ä¹ˆç°åœ¨å‘ƒå¤§æ¨¡å‹å®é™…ä¸Šæ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„è·¯çº¿ã€‚

å‘ƒï¼Œé‚£å®ƒåœ¨skiing lawä¸Šï¼Œè¿™ä¸ªsing lawè¿™ä¸ªå»¶é•¿çº¿ä¸Šä¸€ç›´åœ¨å‘å±•ã€‚å‘ƒï¼Œå†³å®šç€æ¨¡å‹èƒ½åŠ›çš„å‘ƒï¼Œè¿™ä¸ªå‘ƒå‰å‰æ™¯ä¸»è¦é çš„æ˜¯ç®—åŠ›å’Œæ•°æ®ï¼Œé‚£ä¹ˆè¿™ä¸ªå¤§å®¶éƒ½éå¸¸æ¸…æ¥šã€‚å‘ƒã€‚

é‚£å®é™…ä¸Šåœ¨å‘ƒè¿™å‡ å¹´å‘ƒæˆ‘ä»¬å¯èƒ½æ‰€æœ‰äººåŸºæœ¬ä¸Šæ‰€æœ‰äººéƒ½è®¤ä¸ºå‘ƒskiingå¯èƒ½è¿˜ä¼šå»¶ç»­ã€‚å‘ƒï¼Œä½†æ˜¯skiing lawçš„é—®é¢˜å°±åœ¨äºæˆ‘ä»¬å¯¹äºæ¨¡å‹çš„å¦‚ä½•å®ƒçš„è¡¨ç°å…·ä½“æ˜¯ä»€ä¹ˆæ ·ï¼Œç¼ºä¹éå¸¸æ¸…æ™°çš„è®¤è¯†ï¼Œè€Œä¸”æ§åˆ¶èƒ½åŠ›æ˜¯å¾ˆå¼±çš„ã€‚

ä¸åƒå‘ƒè¿™ä¸ª50å¹´ä»£60å¹´ä»£çš„è¿™ä¸ªåŸºäºè§„åˆ™çš„è¿™ä¸ªäººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œä»–ä»¬æ˜¯æˆ‘ä»¬è¿™ä¸ªè®¾è®¡è€…å¯ä»¥å¾ˆå¥½çš„å»æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºå’Œè¾“å‡ºã€‚ä½†æ˜¯åœ¨è¿™ä¸ªåŸºäºæ•°æ®é©±åŠ¨çš„ï¼Œå°¤å…¶æ˜¯è¿™ä¸ªå¤§æ¨¡å‹çš„æ—¶ä»£ã€‚å‘ƒã€‚

è¿™ä¸ªæ¨¡å‹çš„è¡¨ç°å¾ˆå¤šæ—¶å€™æ˜¯ä¸€ç§ç°è±¡å­¦çš„å‘ƒèŒƒç•´ï¼Œæ‰€ä»¥å¾ˆå¾ˆéš¾å‘ƒåœ¨å†…ç”Ÿæœºåˆ¶ä¸Šå»å®Œå…¨åšåˆ°é¿å…é£é™©ã€‚å‘ƒï¼Œé‚£ä¹ˆè¿™å‡ å¹´äººå¤§å®¶å¯¹äºè¿™ä¸ªäººå·¥æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯å‰æ²¿æ¨¡å‹ï¼Œåˆšæ‰chå·²ç»è®²äº†å¾ˆå¾ˆæ¸…æ¥šäº†ã€‚

å‰æ²¿æ¨¡å‹å¤§å®¶éå¸¸å…³å¿ƒå®ƒçš„æ°´å¹³çš„è¿›æ­¥ï¼ŒåŒæ—¶ä¹Ÿå…³å¿ƒå®ƒå¯èƒ½æ½œåœ¨è•´å«ç€ä»€ä¹ˆæ ·çš„é£é™©ã€‚å‘ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬åŸºäºå¯¹äºå„ç±»å‘ƒè¿™ä¸ªç ”ç©¶çš„è¿™ä¸ªæˆæœçš„è¿™ä¸ªåˆ†æå‘¢ï¼Œæˆ‘ä»¬è®¤ä¸ºå…¶å®å‘ƒå¯ä»¥ç”¨è¿™æ ·ä¸€ä¸ªé‡‘å­—å¡”æ¥è¡¨ç°å‘ƒï¼Œæˆ‘ä»¬å¯¹äºäººå·¥æ™ºèƒ½ã€‚

å°¤å…¶æ˜¯å¤§æ¨¡å‹çš„è¿™ä¸ªé£é™©çš„è®¤çŸ¥ã€‚å‘ƒï¼Œå‡ ä¸ªå±‚é¢å‘ƒåˆ†æˆä¸¤ä¸ªç»´åº¦å•Šï¼Œä¸€ä¸ªç»´åº¦æ˜¯è¿™ä¸ªè‡ªèº«å®ƒçš„å†…ç”Ÿçš„è¿™ä¸ªå‘ƒå®‰å…¨é—®é¢˜ã€‚é‚£åŒ…æ‹¬äº†æ¨¡å‹çš„å‘ƒå‚æ•°è¿˜æœ‰æ•°æ®å‘ƒè®¡ç®—ç³»ç»Ÿä»¥åŠç½‘ç»œå‘ƒã€‚

è¿˜æœ‰åº”ç”¨ç³»ç»Ÿçš„å‘ƒè¿™ä¸ªå‘ƒsecurityå±‚é¢çš„å®‰å…¨é—®é¢˜ã€‚é‚£ä¹ˆç¬¬äºŒä¸Šé¢ä¸€ä¸ªå±‚æ¬¡å°±æ˜¯è¿™ä¸ªå¯¹äºä¸ªäººå¯¹äºå›½å®¶å¯¹äºè¿™ä¸ªå…¨äººç±»å‘ƒï¼Œåœ¨åº”ç”¨äººå·¥æ™ºèƒ½çš„æ—¶å€™ï¼Œå¯èƒ½å¼•ç”Ÿå¼•å‘çš„è¡ç”Ÿçš„åº”ç”¨å±‚é¢çš„è¿™ä¸ªé£é™©ã€‚æˆ‘ä»¬å¦‚ä½•å»æ§åˆ¶å¥½ï¼Ÿå‘ƒã€‚

é‚£ä¹ˆæœ€å®è§‚çš„å¯èƒ½æ˜¯æˆ‘ä»¬äººç±»åœ¨äººå·¥æ™ºèƒ½é¢å‰çš„ä½ç½®åœ¨å“ªé‡Œã€‚æˆ‘ä»¬è¿™ä¸ªå…¨äººç±»çš„å…±åŒçš„å‘½è¿ã€‚é‚£ä¹ˆåœ¨ä¸‹å°±å†å¾€ä¸‹å°±æ˜¯å‘ƒå›½å®¶å®‰å…¨è¿™ä¸ªç»æµå®‰å…¨ï¼Œç¤¾ä¼šçš„è¿™ä¸ªä¾›åº”é“¾çš„ç¨³å®šäººå£å°±ä¸šä»¥åŠä¸ªäººä¿¡æ¯ä¿æŠ¤å‘ƒç­‰ç­‰å§ã€‚

è¿™ä¸ªriskå®é™…ä¸Šæ˜¯å¤šç»´åº¦çš„ã€‚æ‰€ä»¥å‘ƒè¿™ä¸ªè®¨è®ºäººå·¥æ™ºèƒ½çš„å®‰å…¨é£é™©ï¼Œå¾€å¾€æ˜¯ä¸€ä¸ªå¤šå­¦ç§‘äº¤å‰çš„ã€‚é‚£ä¹ˆå‘ƒéœ€è¦å»å…±åŒå»æ›´æ¸…é†’çš„è®¤è¯†åˆ°å‘ƒè¿™ä¸ªå®‰å…¨çš„é£é™©åœ¨å“ªé‡Œã€‚å‘ƒï¼Œå†ä¸¾å‡ ä¸ªå…·ä½“çš„æœ€å…·ä½“ä½“çš„é‚£ä¸ªä¾‹å­ã€‚é‚£ä¹ˆå‘ƒä»¥å¤§æ¨¡å‹ä¸ºä¾‹ã€‚

å…¶å®ç°åœ¨åª’ä½“ä¸Šå¯¹äºå¤§æ¨¡å‹åº”ç”¨è¿‡ç¨‹ä¸­æš´éœ²çš„å„ç§é£é™©æš´æš´è¿™ä¸ªæŠ¥é“ä¹Ÿè¶Šæ¥è¶Šå¤šäº†ã€‚æ¯”å¦‚è¯´åœ¨å†…å®¹å‘ƒé£é™©æ–¹é¢å‘ƒï¼Œé‚£ä¹ˆå¯ä»¥çœ‹åˆ°è¿™ä¸ªè¿™ä¸ªè™šå‡ä¿¡æ¯å•Šï¼Œd informationformå…¶å®æ˜¯éå¸¸æ™®éäº†ã€‚

è€Œä¸”è¿™ä¸ªä¹Ÿæ˜¯å›°æ‰°å‘ƒè¿™ä¸ªä¸ç®¡æ˜¯ä¸­å›½è¿˜æ˜¯å‘ƒå›½å¤–å‘ƒç›‘ç®¡éƒ¨é—¨éå¸¸é‡è¦çš„ä¸€ä¸ªé—®é¢˜ã€‚å‘ƒï¼Œé‚£ä¹ˆè¿˜æœ‰å°±æ˜¯æ•°æ®é£é™©ã€‚é‚£è¿™é‡Œå¤´é¢ä¸´çš„æˆ‘ä»¬çš„æç¤ºè¯å‘ƒå¯èƒ½ä¼šæš´éœ²æœºæ„å†…éƒ¨çš„ä¸€äº›ä¿¡æ¯è¿™æ ·çš„é£é™©ã€‚å‘ƒã€‚

ä»¥åŠè¿™ä¸ªç®—æ³•å¯èƒ½ä¼šè¢«å‘ƒç”¨æç¤ºè¯æ”»å‡»æˆ–è€…å…¶ä»–æ‰‹æ®µæ¥ä»é‡Œå¤´æå–æ•æ„Ÿä¿¡æ¯çš„è¿™æ ·ä¸€äº›å‘ƒæ”»å‡»æ‰‹æ³•ï¼Œè¿™ä¸ªä¸€ç›´åœ¨ç¿»æ–°ï¼Œè€Œä¸”ä¹Ÿæ˜¯å±‚å‡ºä¸ç©·çš„ã€‚å‘ƒï¼Œé‚£ä¹ˆä¸ç®¡æ˜¯å®è§‚è¿˜æ˜¯å¾®è§‚å±‚é¢ï¼Œå…¶å®ã€‚å‘ƒã€‚

æˆ‘è§‰å¾—å‘ƒç°åœ¨å¤§å®¶å¯¹äºè¿™ä¸ªäººå·¥æ™ºèƒ½çš„æ¢è®¨ï¼Œäººå·¥æ™ºèƒ½å®‰å…¨é—®é¢˜çš„æ¢è®¨å‘ƒï¼Œå·²ç»éå¸¸çš„å……åˆ†äº†ã€‚å‘ƒï¼Œä¸Šæœ€ä¸Šé¢æ˜¯è¿™ä¸ªè”åˆå›½å±‚é¢ï¼Œå¯¹å§ï¼Ÿè¿™ä¸ªç§˜ä¹¦é•¿å¤ç‰¹é›·æ–¯ç§˜ä¹¦é•¿åœ¨ééå¸¸å¤šçš„åœºåˆå‘ƒï¼Œåœ¨è°ˆã€‚

é‚£ä¹ˆè”åˆå›½ä¹Ÿæˆç«‹äº†éå¸¸å‘ƒå¥½å‡ ä¸ªæœºæ„é‚£ä¹ˆå‘ƒåŒ…æ‹¬è¿™ä¸ªuncoITUç­‰ç­‰åœ¨è®¨è®ºäººå·¥æ™ºèƒ½çš„æ²»ç†é—®é¢˜å®‰å…¨é—®é¢˜ã€‚å‘ƒï¼Œé‚£ä¹ˆå„ä¸ªå›½å®¶å…¶å®ä¹Ÿåœ¨è¡ŒåŠ¨ã€‚æˆ‘ä»¬çœ‹ä¸€ä¸‹å…·ä½“çš„ã€‚å‘ƒï¼Œæ•´ä½“ä¸Šæ„Ÿè§‰äººå·¥æ™ºèƒ½çš„å®‰å…¨é£é™©æ²»ç†å‘ƒã€‚

æ­£åœ¨ä»åŸåˆ™èµ°å‘å®è·µã€‚å‘ƒï¼Œé‚£ä¹ˆå¤§å®¶å¯¹äºä¸€äº›å‘ƒåŸºæœ¬çš„åŸåˆ™ï¼Œæ¯”å¦‚è¯´ä»¥äººä¸ºæœ¬æ™ºèƒ½å‘å–„å‘ƒï¼Œæ¯”å¦‚è¯´è¦ä¿è¯å®ƒçš„å…¬å¹³éæ­§è§†è¿™æ ·ä¸€äº›å‘ƒåŸåˆ™ï¼Œå¤§å®¶æ²¡æœ‰åˆ†æã€‚å‘ƒã€‚

é‚£ä¹ˆå‘ƒå›½é™…ç¤¾ä¼šåŒ…æ‹¬å„ä¸ªå›½å®¶å…¶å®ä¹Ÿéƒ½åœ¨æŠŠè¿™äº›åŸåˆ™æ­£åœ¨è½¬å‘å‘ƒå…·ä½“çš„æ“ä½œã€‚é‚£ä¹ˆè¿™äº›æ“ä½œå¯èƒ½ä¼šè½åˆ°å…·ä½“çš„å¾ˆå¤šæ–¹é¢ï¼Œæ¯”å¦‚è¯´ä¸€äº›å‘ƒè¿™ä¸ªå›½é™…å‘ƒè§„åˆ™çš„è¿™ä¸ªåˆ¶å®šã€‚é‚£ç°åœ¨è”åˆå›½å‘ƒï¼Œåˆšæ‰è¯´äº†ã€‚

è”åˆå›½å‘ƒæ•™ç§‘æ–‡å‘ƒITUç­‰ç­‰è¿™äº›æœºæ„åœ¨ç‰µå¤´è¿™ä¸ªinternationalçš„è¿™ä¸ªè§„åˆ™çš„è®¾å®šã€‚é‚£ä¹ˆå„ä¸ªå›½å®¶ä¹Ÿåœ¨å‡ºå°ç›¸å…³çš„ç«‹æ³•ã€‚å‘ƒï¼Œé‚£ä¹ˆå‘ƒå¦å¤–ä¸€ä¸ªå†å…·ä½“çš„ä¸€ä¸ªå°±æ˜¯è½¬æ¢æˆæ ‡å‡†ã€‚å‘ƒï¼Œé‚£ä¹ˆæŠŠåŸåˆ™å›ºåŒ–æˆã€‚

ä¸€äº›èƒ½å¤Ÿæˆæ–‡çš„å‘ƒæŠ€æœ¯æ€§çš„è¦æ±‚æ¥ä¾¿äºè¿™äº›ä¼ä¸šå»éµä»ã€‚å‘ƒï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´å…·ä½“çš„å°±æ˜¯æµ‹è¯•ã€‚å‘ƒï¼Œæœ‰äº†æœ‰äº†æ ‡å‡†ä»¥åï¼Œè¿˜éœ€è¦å»éªŒè¯è¿™äº›æ ‡å‡†çš„éµä»æƒ…å†µã€‚å‘ƒï¼Œé‚£ä¹ˆåœ¨ä¼ä¸šä¾§å‘¢ã€‚

å¯èƒ½è¿˜éœ€è¦å»å‘ƒç ”å‘å¾ˆå¤šçš„æŠ€æœ¯å·¥å…·æ¥æå‡ä¿éšœè¿™ä¸ªå®‰å…¨çš„å‘ƒæ°´å¹³ã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°ç¾å›½è‹±å›½ã€æ¬§ç›Ÿã€æ–°åŠ å¡ï¼Œè¿˜æœ‰ä¸­å›½éƒ½åœ¨é‡‡å–è¿™å‡ ä¸ªå±‚æ¬¡çš„è¿™ä¸ªå…·ä½“ä¸¾æªï¼ŒæŠŠäººå·¥æ™ºèƒ½çš„æ²»ç†ï¼Œä»åŸåˆ™æ¨å‘å®è·µã€‚å‘ƒã€‚

é‚£ä¹ˆä¸­å›½æ–°é€šé™¢ä½œä¸ºä¸€ä¸ªæ™ºåº“å’Œè¡Œä¸šå¹³å°ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æŠ•èº«äºè¿™æ ·ä¸€ä¸ªä¿ƒè¿›äººå·¥æ™ºèƒ½æ²»ç†ã€‚ä»åŸåˆ™èµ°å‘å®è·µè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿè´¡çŒ®æˆ‘ä»¬çš„åŠ›é‡ã€‚å‘ƒï¼Œä»Šå¹´å¹´åˆæˆ‘ä»¬å’Œå›½å†…å‘ƒ30å¤šå®¶å•ä½å§ï¼Œè¿™ä¸ªå‘ƒæ•°æ®é›†è¿˜æ˜¯æ¯”è¾ƒä¸°å¯Œçš„ã€‚

æˆ‘ä»¬æœ‰50å¤šä¸‡æ¡çš„è¿™ä¸ªå‘ƒé¢˜ç›®å‘ƒï¼Œå›´ç»•ç€å‘ƒåœ¨å‘ƒå…¨çƒå…±è¯†çš„è¿™ä¸ªå‘ƒè¦æ±‚ä¸Šï¼Œä»¥åŠç¬¦åˆä¸­å›½æœ¬åœŸæ³•å¾‹æ³•è§„çš„è¦æ±‚ã€‚è¿™ä¸¤ä¸ªç»´åº¦ä¸Šæ¥å¼€å±•è¿™ä¸ªè¯„æµ‹æ•°æ®é›†çš„å»ºè®¾ã€‚é‚£ä¹ˆå¦å¤–ä¸€ä¸ªå°±æ˜¯æˆ‘ä»¬ä¹Ÿå¯†åˆ‡è·Ÿè¸ªå‰æ²¿è¿™ä¸ªå‘å±•ã€‚å‘ƒã€‚

ä¸ºä»€ä¹ˆæˆ‘ä»¬æ˜¯å­£åº¦æ€§çš„å»åšè¿™ä¸ªäº‹æƒ…ã€‚å› ä¸ºäººå·¥æ™ºèƒ½çš„è¿™ä¸ªè¿›å±•ï¼Œå¯ä»¥è¯´æ˜¯æ—¥æ–°æœˆå¼‚ã€‚æ‰€ä»¥æˆ‘ä»¬ä¼šå­£åº¦æ€§çš„è°ƒæ•´æˆ‘ä»¬çš„å¥”é©° markçš„æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä¸æ–­çš„æå‡è·Ÿè¿›è¿™ä¸ªå‰æ²¿æ¨¡å‹å‘ƒè¿™ã€‚ä¸€ä¸ªæ¼”è¿›çš„æ­¥ä¼ã€‚å‘ƒã€‚

å¦å¤–æˆ‘ä»¬è¿˜æ˜¯ç‰¹åˆ«å…³æ³¨ç”¨æˆ·æ˜¯æ€ä¹ˆæƒ³çš„ã€‚å‘ƒï¼Œå…³æ³¨ç°å®çš„é£é™©ã€‚æ¯”å¦‚è¯´é‡‘èã€æ”¿åŠ¡ã€åŒ»ç–—è¿™äº›è¡Œä¸šï¼Œä»–ä»¬çš„è¿™ä¸ªå‘ƒæ‹…å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿç›®çš„å®é™…ä¸Šæ˜¯å¸Œæœ›èƒ½å¤Ÿå‘ƒè®©é€šè¿‡è¿™ç§è¯„æµ‹æ¥è®©è¿™äº›è¡Œä¸šç”¨æˆ·æ”¾å¿ƒï¼Œèƒ½å¤Ÿå‹‡å‹‡æ•¢çš„å»ä½¿ç”¨å‘ƒã€‚

å…ˆè¿›çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚ä½†æ˜¯å‰ææ˜¯ä¿è¯å®‰å…¨çš„å‰æä¸‹ã€‚å‘ƒï¼Œé‚£ä¹ˆåœ¨2024å¹´çš„4æœˆä»½ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ç¬¬ä¸€ç‰ˆçš„è¿™ä¸ªAI safety benchmark markçš„è¿™ä¸ªç»“æœã€‚

é‚£ä¹ˆè¿™ä¸ªç»“æœæ˜¯åŸºäºæˆ‘ä»¬è¿™ä¸ªå·¦è¾¹è¿™æ ·ä¸€ä¸ªæ¡†æ¶å‘ƒæ¥åšçš„ã€‚é‚£ä¹ˆå¤§ä½“ä¸Šåˆ†ä¸ºä¸‰ä¸ªæ–¹é¢ï¼Œä¸€ä¸ªæ˜¯è¿™ä¸ªå‘ƒå†…å®¹å®‰å…¨å±‚é¢çš„è¿™ä¸ªè¦æ±‚ã€‚å‘ƒï¼Œé‚£è¿™å°±åŒ…æ‹¬äº†æˆ‘æ³•å¾‹æ˜ä»¤ç¦æ­¢çš„ä¸€äº›å†…å®¹çš„è¾“å‡ºã€‚

æ¯”å¦‚è¯´æ¶‰é»„æ¶‰æš´ã€èµŒåšã€æ¬ºè¯ˆã€å±é™©åŒ–å­¦å“ã€ç”Ÿç‰©å‘ƒæ­¦å™¨ç­‰ç­‰è¿™äº›å‘ƒè¿™ä¸ªå†…å†…å®¹çš„è¿™ä¸ªå‘ƒç®¡æ§è¾“å‡ºã€‚å¦å¤–å°±æ˜¯è¿™ä¸ªæ•°æ®å®‰å…¨ï¼Œè¿™ä¸ªä¹ŸåŒ…æ‹¬äº†è¿™ä¸ªä¸ªäººéšç§å’Œä¼ä¸šæœºå¯†çš„è¿™ä¸ªå¯èƒ½è¢«æå–çš„è¿™æ ·ä¸€äº›ç‰¹å¾å‘ƒï¼Œä¸ªäººä¿¡æ¯å’Œå•†ä¸šç§˜å¯†ã€‚å‘ƒã€‚

é‚£ä¹ˆè¿˜æœ‰æœ€ä¸Šå±‚æ˜¯è¿™ä¸ªä¼¦ç†ç§‘æŠ€ä¼¦ç†ã€‚é‚£è¿™é‡Œå¤´å°±åŒ…æ‹¬ä»·å€¼è§‚ã€å¿ƒç†å¥åº·å’Œå‘ƒAIæ„è¯†å…¬è¯‰è‰¯å…¬åºè‰¯ä¿—ã€è¾±éª‚è¯±å¯¼ç­‰ç­‰è¿™äº›æ¬ºéª—ï¼Œè¿™äº›å‘ƒé—®é¢˜çš„è¿™ä¸ªåº”å¯¹ï¼Œä»–ä»¬æ˜¯å¦‚ä½•ä»–çš„è¿™ä¸ªè¯„åˆ†èƒ½å¤Ÿå¾—åˆ°å…«ä¹ååˆ†å‘ƒï¼Œè¿™ä¸ªæ°´å¹³è¿˜æ˜¯è¿˜æ˜¯å¯ä»¥çš„ã€‚

å‘ƒï¼Œä½†æ˜¯å‘¢å‘ƒé—®é¢˜å°±åœ¨äºå‘ƒå·¨è¾¾ç‡åé«˜ã€‚è¿™æ ·ä¼šé€ æˆå‘ƒè¿™ä¸ªä»–å‘ƒè¿™ä¸ªå¾ˆå¤šå‘ƒç”¨æˆ·ä½“éªŒæ˜¯ä¸æ˜¯ç‰¹åˆ«å¥½çš„ã€‚é‚£ä¹ˆåŒæ—¶è¿™ä¹Ÿè¡¨ç°é€šè¿‡å‘ƒè¿™ä¸ªå…·ä½“çš„æ”»å‡»æ‰‹æ³•å‘ƒï¼Œè¿™ä¸ªæ”»å‡»æ‰‹æ³•ä¹Ÿæ˜¯æˆ‘ä»¬æ”¶é›†åˆ°èƒ½æ”¶é›†åˆ°çš„è¿™ä¸ªå¸‚é¢ä¸Šå­¦æœ¯ç•Œã€‚

åŒ…æ‹¬å‘ƒç¤¾åŒºæš´éœ²å‡ºæ¥çš„è¿™ä¸ªæœ€æ–°çš„æ”»å‡»æ‰‹æ³•ï¼ŒåŒ…æ‹¬æˆ‘ä»¬è‡ªå·±æµ‹è¯•å‡ºã€‚å‘ç°çš„è¿™ä¸ªæ”»å‡»æ‰‹æ³•æ¥æµ‹è¯•è¿™äº›æ¨¡å‹ã€‚é‚£è¿™åŒ…æ‹¬æç¤ºè¯æ”»å‡»å•Šã€è¯±å¯¼æ”»å‡»ã€è¶Šç‹±æ”»å‡»ã€å†…å®¹æ³›åŒ–ã€æ”»å‡»å’Œå…¶ä»–çš„æ”»å‡»æ‰‹æ®µã€‚é‚£ä¹ˆå‘ƒæ•°æ®é›†ä¹Ÿç›¸åº”çš„åšäº†æ‰©å¤§å‘ƒã€‚

åº•çº¿çº¢çº¿ç¤¾ä¼šä¼¦ç†å’Œæ•°æ®æ³„éœ²ã€‚å‘ƒï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–çš„è¿™ä¸ªå‘ƒå®‰å…¨ä¿éšœæªæ–½çš„è¿™ä¸ªæµ‹è¯•ã€‚å‘ƒï¼Œåº”è¯¥è¯´æ˜¯ä¸€ä¸ªæ›´åŠ å…¨é¢çš„å‘ƒï¼Œæ‰€ä»¥è¿™ä¸ªä½“ç³»æ˜¯ä¸€ä¸ªä¸æ–­åœ¨æ¼”è¿›çš„å‘ƒè¿™ä¸ªè¿‡ç¨‹è¦è·Ÿä¸Šè¿™ä¸ªèŠ‚å¥ã€‚å‘ƒï¼Œé‚£ä¹ˆQ2çš„è¿™ä¸ªè¯„æµ‹æ•°æ®é›†ä¹Ÿä¹Ÿæ¯”è¾ƒå¤§ã€‚

é‚£ä¹ˆæˆ‘ä»¬æœ‰600å¤šæ¡æç¤ºè¯çš„æ¨¡æ¿ã€‚å‘ƒï¼Œé‚£ä¹ˆæœ‰60å¤šç§æ”»å‡»æ‰‹æ³•ï¼Œ3ã€‚6ä¸‡æ¡çš„è¿™ä¸ªæ”»å‡»æ ·æœ¬4æ¯æ¯æ¬¡ä»è¿™3ä¸‡å¤šæ¡é‡Œå¤´æŠ½å–4000å¤šæ¡æ¥åšæµ‹è¯•ã€‚å‘ƒï¼Œé‚£ä¹ˆæµ‹è¯•å®Œäº†è¿™ä¸ªæ•°æ®å°±æ·˜æ±°äº†ã€‚é‚£é¿å…å¤§å®¶å»åˆ·é¢˜å’Œå‘ƒä½œå¼Šã€‚å‘ƒã€‚

é‚£ä¹ˆç›®å‰æˆ‘ä»¬Q2çš„è¿™ä¸ªç»“æœä¹Ÿå·²ç»åˆæ­¥çš„å‡ºæ¥äº†ã€‚é‚£ä¹ˆå‘ƒå¯ä»¥çœ‹åˆ°è¿™ä¸ªå‘ƒå¤§å®¶åœ¨å‘ƒè¿™ä¸ªåŸå§‹è¾“å…¥çš„æ”»å‡»æˆåŠŸç‡ï¼Œå°±æ˜¯æˆ‘ä»¬æˆ‘ä»¬ä¸ç”¨æç¤ºè¯æ”»å‡»æ‰‹æ³•æ¥åšè¿™ä¸ªå‘ƒæé—®çš„è¯ï¼Œä»–çš„æ”»å‡»æˆåŠŸç‡è¿˜æ˜¯æ¯”è¾ƒä½ã€‚ä¹Ÿå°±æ˜¯åè¿‡æ¥è¯´ã€‚

ä¸€å‡è¿™ä¸ªæ¯”ä¾‹å°±æ˜¯è¿™ä¸ªä»–çš„è¿™ä¸ªè¿™ä¸ªå¾—åˆ†ã€‚é‚£ä¹ˆä½†æ˜¯æˆ‘ä»¬åŠ å…¥äº†å¾ˆå¤šæç¤ºè¯æ”»å‡»ä»¥åçš„è¿™ä¸ªæ”»å‡»æˆåŠŸç‡å°±æ˜¾è‘—æé«˜äº†ï¼Œä¹Ÿå°±åå‘çš„ä»£è¡¨ç€å¯ä»¥æœ‰å¾ˆå¤šçš„å‘ƒè¿™ä¸ªæ‰‹æ³•æ¥ç»•è¿‡è¿™ä¸ªæ”»å‡»å‘ƒï¼Œç»•è¿‡è¿™ä¸ªæ¨¡å‹çš„è¿™ä¸ªæŠ¤æ ã€‚

æ‰€ä»¥å‘ƒæ•´ä½“ä¸Šæ¥çœ‹å‘¢ï¼Œå‘ƒï¼Œæˆ‘ä»¬ç›®å‰æ˜¯æ¨¡æ‹Ÿäº†æœ€è¿‘å‘ç”Ÿçš„è¿™ä¸ªå‘ƒå®‰å…¨æ”»å‡»çš„è¿™ä¸ªå„ç§æ‰‹æ®µæ¥å‘ƒè¿™ä¸ªæ›´å¥½çš„æ¥çœ‹å¾…æˆ‘ä»¬æ¨¡å‹å®‰å…¨çš„è¿™ä¸ªæ°´å¹³ã€‚å‘ƒï¼Œå¤§æ¨¡å‹çš„å®‰å…¨æµ‹è¯•æ˜¯ä¸­å›½æ–°é€šé™¢åœ¨äººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢çš„ä¸€ä¸ªå®è·µï¼Œå…·ä½“çš„å®è·µã€‚

é‚£é™¤æ­¤ä¹‹å¤–å‘¢ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å¤Ÿç³»ç»ŸåŒ–çš„å»æ¨åŠ¨äººå·¥æ™ºèƒ½å‘ƒå¯ä¿¡å‘å±•ã€‚é‚£ä¹ˆå°±åœ¨è¿™ä¸ª2021å¹´çš„æ—¶å€™ï¼Œä¸Šæµ·ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šï¼Œæˆ‘ä»¬å‘å¸ƒäº†å…¨çƒé¦–ä¸ªå¯ä¿¡äººå·¥æ™ºèƒ½ç™½çš®ä¹¦ã€‚å‘ƒã€‚

è¿™ä¸ªæ˜¯è·Ÿäº¬ä¸œæ¢ç´¢ç ”ç©¶é™¢ä¸€å—å‘å¸ƒçš„é™¶å¤§æˆä»–çš„é™¶å¤§æˆé™¢å£«ã€‚é‚£ä¹ˆå½“æ—¶çš„æ€è·¯æ˜¯è¿‡ç¨‹ç®¡ç†ï¼Œå°±æ˜¯è¦äººå·¥æ™ºèƒ½ä»è¦ä¿è¯å®‰å…¨ï¼Œé™¤äº†è¿™ä¸ªä½ è¦æ±‚ä»–ç»“æœä»¥å¤–ï¼Œè¿˜è¦ç®¡å¥½ç ”å‘ä½¿ç”¨çš„è¿™ä¸ªæµç¨‹ã€‚æ‰€ä»¥è¿‡ç¨‹ç®¡ç†ï¼Œæˆ‘ä»¬è®¤ä¸ºæ˜¯éå¸¸é‡è¦çš„ã€‚

æ‰€ä»¥åœ¨è¿™ä¸ªé‚£ä¸€å¹´çš„è¿™ä¸ªç™½çš®ä¹¦é‡Œå¤´ï¼Œæˆ‘ä»¬å°±æå‡ºæ¥å¦‚ä½•åœ¨å‘ƒè¿™ä¸ªå‘ƒå„å„ä¸ªåŒ…æ‹¬G20å•Šï¼ŒåŒ…æ‹¬å¾ˆå¤šåŸåˆ™çš„è¿™ä¸ªæŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æ€ä¹ˆåœ¨ä¼ä¸šå„ä¸ªç¯èŠ‚è½å®è¿™ä¸ªåŸåˆ™ã€‚å‘ƒ23å¹´åˆ°ç°åœ¨æˆ‘ä»¬è®¤ä¸ºç»“æœç®¡ç†ä¹Ÿéå¸¸ã€‚

é‡è¦å°±æ˜¯æˆ‘æˆ‘ä»¬å‚è€ƒäº†åƒè¿™ä¸ªopen eyeå’Œropï¼Œä»–ä»¬åšå‘ƒè¿™ä¸ªå®‰å…¨å‘ƒæ¨¡å‹çš„è¿™ä¸ªè¿™ä¸ªåˆ†çº§åˆ†ç±»ã€‚å‘ƒï¼Œé‚£ä¹ˆæŠŠé£é™©åˆ†æˆä¸åŒçš„ç±»åˆ«å’Œç­‰çº§ã€‚å‘ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬æˆ‘è§‰å¾—è¿™ä¸ªä¹Ÿæ˜¯ç‰¹åˆ«æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

æ‰€ä»¥ä¹Ÿåœ¨æ¨åŠ¨ä»è¿‡ç¨‹ç®¡ç†èµ°å‘è¿™ä¸ªç»“æœç®¡ç†æ•ˆæœç®¡ç†ã€‚å‘ƒï¼Œé‚£ä¹ˆå‘ƒåŸºäºé£é™©å®šçº§çš„è¿™æ ·ä¸€ä¸ªç®¡ç†æ‰‹æ®µã€‚å‘ƒï¼Œæ‰€ä»¥è¿™ä¸ªå…¶å®æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™æ ·ä¸€ä¸ªæ–¹æ³•è®ºæ¥è®©ä¼ä¸šç•Œè®©äº§ä¸šçŸ¥é“å®ƒåº”è¯¥æ€ä¹ˆå»åšå‘ƒï¼Œæ˜¯ä¸€ä¸ªæœ€ä½³å®è·µã€‚å‘ƒã€‚

é‚£ä¹ˆåŒæ—¶æˆ‘ä»¬ä¹Ÿåœ¨åˆ¶å®šè¿™ä¸ªbench markï¼ŒåŒ…æ‹¬å¥”nch markå‘ƒåœ¨å†…çš„è¿™ä¸ªåæ ‡å°ºã€‚å› ä¸ºå¦‚ä½•å¦‚æœä¸æ²¡æ³•åº¦é‡ï¼Œå°±æ²¡æ³•æ”¹è¿›ã€‚æ‰€ä»¥è¿™ä¸ªå‘ƒæµ‹è¯•æ˜¯éå¸¸å…³é”®çš„ã€‚é™¤äº†è¿™ä¸ªå¤§æ¨¡å‹çš„å®‰å…¨æµ‹è¯•ä»¥å¤–ã€‚

æˆ‘ä»¬ä¹Ÿåœ¨åšè¿™ä¸ªå‘ƒè¿™ä¸ªäººè„¸è¯†åˆ«å®‰å…¨å•Šï¼Œè¿˜æœ‰è¿™ä¸ªç®—æ³•å®‰å…¨ç­‰ç­‰è¿™äº›è¿™äº›å®‰å…¨çš„è¿™ä¸ªæµ‹è¯•æ ‡å‡†å‘ƒï¼Œæµ‹è¯•èƒ½åŠ›ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿåœ¨å»ºè®¾è¿™ä¸ªåºå¤§çš„è¿™ä¸ªæ•°æ®é›†å‘ƒï¼ŒåŒ…æ‹¬äººè„¸å’Œå…¶ä»–çš„ä¸€äº›å‘ƒè¿™ä¸ªè¿™ä¸ªæµ‹è¯•é›†å‘ƒèƒ½åŠ›çš„å»ºè®¾ä¸€ç›´åœ¨æŒç»­çš„å‘ƒè·Ÿè¿›ã€‚

å‘ƒï¼Œé‚£åŒæ—¶è¿™ä¸ªå‘ƒè¿™ä¸ªå‘ƒå¤§æ¨¡å‹çš„å†…å®¹å®‰å…¨å°±æ˜¯åŒ…æ‹¬å†…å®¹å®‰å…¨ï¼Œå®ƒå¦‚ä½•å»æµ‹å‡ºç»“æœä»¥åï¼Œå¦‚ä½•å»å¢å¼ºã€‚æˆ‘ä»¬ç°åœ¨ä¹Ÿåœ¨å‘ƒåŸºäºæˆ‘ä»¬æŒæ¡çš„è¿™ä¸ªæˆ‘ä»¬å»ºè®¾çš„è¿™ä¸ª300å¤šä¸‡æ¡å…³é”®è¯å‘ƒã€‚

50å¤šä¸‡æ¡æç¤ºè¯æ¥è®­ç»ƒæˆ‘ä»¬çš„å®‰å…¨æµ‹è¯•çš„å‘ƒè¿™ä¸ªæ¨¡å‹ã€‚é‚£ä¹ˆæœªæ¥å¸Œæœ›é€šè¿‡å‘ƒå‚æ•°çš„æ–¹å¼æä¾›ç»™è¿™ä¸ªæ¨¡å‹å‚å•†æ¥åŠ å›ºä»–ä»¬çš„æ¨¡å‹èƒ½åŠ›ã€‚å‘ƒï¼Œé‚£ä¹ˆåŒå‘ƒæˆ‘è§‰å¾—å®‰å…¨å…¶å®å¾ˆé‡è¦ï¼Œè¿˜æœ‰ä¸€ä¸ªå°±æ˜¯ä¿¡æ¯çœŸå®æ€§çš„ä¿éšœã€‚

ä¿¡æ¯çœŸå®æ€§å¦‚ä½•ä¿éšœæˆ‘è§‰å¾—è¦é å¾ˆå¤šæ–¹æ³•å‘ƒï¼Œå…¶ä¸­ä¸€ä¸ªæ–¹æ³•å°±æ˜¯æ°´å°è¿™ä¸ªwork markæ˜¯å§ï¼Ÿé‚£ä¹ˆé™¤äº†whatä»¥å¤–ï¼Œå¯èƒ½ç°åœ¨æ¯”å¦‚è¯´å›¾ç‰‡é‡Œå¤´æœ‰C to Pçš„è¿™ä¸ªæ ‡å‡†dobeä»–ä»¬å‘èµ·çš„ã€‚é‚£ä¹ˆè¿˜æœ‰å…¶ä»–çš„ä¸€äº›å‘ƒæ–¹å¼ã€‚

å…¶å®ä¹Ÿéƒ½åœ¨æ¢ç´¢ä¸­ç»¼åˆæ¥çœ‹ï¼Œå‘ƒï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå»ºç«‹ä¸€å¥—å‘ƒå¯¹æŠ—deep fakeå‘ƒæ¥ä¿éšœå†…å®¹çœŸå®æ€§çš„ä¸€å¥—æŠ€æœ¯çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ°´å°ã€‚æˆ‘ä»¬ç°åœ¨ä¹Ÿå¼€å‘äº†æ°´å°çš„éšè§†å†™å…¥çš„ç®—æ³•å‘ƒã€‚

ä¹Ÿæä¾›è¿™æ ·çš„ä¸€äº›APIå…¬è¿™ä¸ªè¿™äº›ä¼ä¸šä¹Ÿå¯ä»¥ä½¿ç”¨ã€‚å‘ƒï¼Œæ•´ä½“ä¸Šæˆ‘å°±åˆ©ç”¨è¿™ä¹ˆä¸€ä¸ªæ—¶é—´è·Ÿå¤§å®¶åˆ†äº«å‘ƒæˆ‘ä»¬çš„ä¸€äº›å®è·µï¼Œå¸Œæœ›è·Ÿå¤§å®¶å‘ƒç»§ç»­äº¤æµï¼Œä¹Ÿæ„Ÿè°¢å‘ƒä¼šè®®é‚€è¯·æˆ‘æ¥è·Ÿå¤§å®¶åˆ†äº«ï¼Œè°¢è°¢ã€‚ğŸ˜Šã€‚

Ladies and gentlemenï¼Œ I'm afraid there has been a slight change of scheduleã€‚Our original speakerã€‚

 Professor Toyuï¼Œ has urgent matter matters to tend to and will be slightly delayed in arriving at our forum at the momentã€‚

Insteadï¼Œ we will welcome Drã€‚ Xo Jingï¼Œ his colleague at Shanghai AI Laboratory to deliver their team's presentationã€‚

ğŸ˜Šï¼ŒLet's put our hands togetherï¼Œ Drã€‚ Shao Jingã€‚å‘ƒï¼Œå„ä½é‚£ä¸ªä¸“å®¶é¢†é¢†å¯¼ï¼Œç„¶åå„ä½åŒä»å‘ƒï¼Œå¤§å®¶å¥½ã€‚å‘ƒï¼Œå¾ˆæŠ±æ­‰ï¼Œé‚£ä¸ªä¹”è€å¸ˆè¿˜åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œå‘ƒï¼Œæˆ‘å°±å…ˆæ¥åšä¸ªå¼€åœºå•Šã€‚

å¾…ä¼šå„¿ä¹”è€å¸ˆæ¥äº†ä¹‹åï¼Œä¼šç»™å¤§å®¶ç»§ç»­åšè¿™ä¸ªåˆ†äº«å—¯ã€‚ğŸ˜Šï¼Œå—¯ï¼Œæˆ‘ä»¬ä»Šå¤©è¿™ä¸ªè®²çš„è¿™ä¸ªå‘ƒæ•´ä½“å‘¢æ˜¯æ ¹æ®ä¹”è€å¸ˆä½œä¸ºä¸€ä¸ªäº²å†è€…å’Œé¢†å¯¼è€…çš„è§†è§’å»å›é¡¾å®éªŒå®¤ã€‚å‘ƒï¼Œåœ¨åšå¤§æ¨¡å‹åŒ…æ‹¬èƒ½åŠ›ã€å®‰å…¨æ•´ä¸ªè·¯çº¿ä¸Šçš„ä¸€äº›æ„Ÿå—å’Œä¸€äº›è§‚å¯Ÿã€‚å‘ƒã€‚

æ•´ä½“æ¥è¯´ä¼šåˆ†å››ä¸ªpartå‘ƒæ¥è®²å§ã€‚å‘ƒï¼Œé¦–å…ˆå‘¢è¿™ä¸ªå‘ƒæˆ‘ä»¬çŸ¥é“è¿™ä¸ªç°åœ¨æ•´ä¸ªå‘ƒäººå·¥æ™ºèƒ½çš„å‘å±•æ˜¯è¿™ä¸ªéå¸¸é£é€Ÿçš„ã€‚ç„¶åå‘ƒé‡Œé¢æ¶‰åŠåˆ°çš„è¿™ä¸ªå‘ƒå›½å®¶ä¹Ÿè¶Šæ¥è¶Šå¤šã€‚å‘ƒï¼Œä¸ä»…æ˜¯ä¸­å›½å›½å¤–å„å„ç±»çš„å›½å®¶ã€‚

å¤§å®¶éƒ½ä¼šå…³æ³¨èƒ½åŠ›å’Œå®‰å…¨ç›¸å…³çš„è¿™ä¸ªå‘ƒå‘ƒå…±ç”Ÿæ€§ã€‚é‚£æˆ‘ä»¬æœ€å¼€å§‹å¯èƒ½åœ¨17å¹´çš„è¿™ä¸ªé˜¿å°”æ³•zeroçš„æ—¶å€™ï¼Œå¤§å®¶å¯èƒ½æ›´å…³æ³¨è¿™ä¸ªé¢å‘ä¼ ç»ŸAIç®—æ³•å’Œç³»ç»Ÿçš„å®‰å…¨æ ‡å‡†ã€‚å‘ƒï¼Œç„¶ååˆ°åé¢çš„æ—¶å€™ï¼Œéšç€å‘ƒæ›´å¤šçš„è¿™ä¸ªæ¨¡å‹å‡ºæ¥ã€‚

åŒ…æ‹¬è¿™ä¸ªgenrate modelï¼ŒåŒ…æ‹¬å‘ƒé‚£ä¸ªä»å‘ƒå‰å‡ å¹´å¼€å§‹çš„è¿™ä¸ªå‘ƒå‘ƒchGPç³»åˆ—å‘ƒï¼Œæ•´ä½“æ¥è¯´ï¼Œç°åœ¨å¤§å®¶ä¼šå‘ç°è¯´æˆ‘ä¸èƒ½åªè§‚å¯Ÿå‘ƒã€‚ğŸ˜Šï¼ŒAIç®—æ³•å’Œç³»ç»Ÿçš„è¿™ä¸ªå®‰å…¨æ ‡å‡†ï¼Œæˆ–è€…æ˜¯AIç®—æ³•æœ¬èº«çš„è¿™ä¸ªèƒ½åŠ›ã€‚

æ›´å¤šè¦è§‚å¯Ÿå®ƒçš„è¿™ä¸ªæ•°æ®çš„è§„èŒƒï¼Œè¿˜æœ‰åˆ°å‘ƒé¢å‘è¿™ä¸ªæ•´ä¸ªç¤¾ä¼šå’Œç¾¤ä½“çš„è¿™ä¸ªå¤§æ¨¡å‹å‘ƒå¤§æ¨¡å‹çš„å®‰å…¨è¯„æµ‹å’Œå¯¹åº”çš„å®‰å…¨æ ‡å‡†ã€‚å—¯ã€‚é‚£å®éªŒå®¤æ•´ä½“ä»21å¹´æˆç«‹è‡³ä»Šå‘ƒï¼Œä¸€ç›´è‡´åŠ›äºå¤§æ¨¡å‹çš„ä½“ç³»çš„ç ”å‘ã€‚å‘ƒã€‚

ä»2021å‘ƒ2021å¹´å¼€å§‹å‘å‘å¸ƒçš„ç¬¬ä¸€ä¸ªä¹¦ç”Ÿ1ã€‚0çš„æ¨¡å‹ï¼Œå‘ƒï¼Œæ˜¯ä¸€ä¸ªè§†è§‰çš„é€šç”¨å¤§æ¨¡å‹ã€‚å‘ƒï¼Œå½“æ—¶åº”è¯¥åœ¨å›½é™…ä¸Šä¹Ÿæœ‰å¾ˆå¤šï¼ŒåŒ…æ‹¬ deepçš„google open eyeå‘ƒï¼Œéƒ½æœ‰ç›¸å…³çš„è¿™ä¸ªå‘ƒè§†è§‰é€šç”¨å¤§æ¨¡å‹ã€‚å‘ƒã€‚

éšç€æ—¶é—´çš„æ¼”å˜ï¼Œåˆ°äº†2022å¹´å‘ƒå®éªŒå®¤å¼€å§‹èµ°å‘è¿™ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œåœ¨è¿™é‡Œé¢ä¹Ÿå‘ƒé€æ¸è¡ç”Ÿå‡ºæ¥è¿™ä¸ªå¤§è¯­è¨€æ¨¡å‹ä¹¦ç”Ÿæ™®è¯­ä½“ç³»ã€‚åœ¨æ˜¨å¤©çš„å‘ƒè¿™ä¸ªå¼€å¹•å¼ä¸Šå‘ƒä¹Ÿåšäº†ç›¸å…³çš„ä»‹ç»ã€‚ç„¶åä¸€ç›´åˆ°å‘ƒä»Šå¹´å‘ƒ7æœˆä»½å‘ƒè¿™ä¸ªä¹¦ç”Ÿæ™®è¯­ã€‚

åŒ…æ‹¬ä¹¦ç”Ÿå¤šæ¼«å¸¦å¤§æ¨¡å‹å·²ç»æ¼”å˜åˆ°äº†æ–°çš„å‘ƒæ–°çš„èƒ½åŠ›èŒƒç•´ã€‚å‘ƒï¼Œæˆ‘ä»¬æ•´ä½“çš„å…¬å¼€çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬7Bçš„å°å‚æ•°æ¨¡å‹ï¼Œè¿˜æœ‰å‘ƒè¿™ä¸ª70Bçš„å¤§æ¨¡å‹ï¼Œå‘ƒï¼Œè‡´åŠ›äºè¿™ä¸ªä¸€ç›´è‡´åŠ›äºæ˜¯å¼€æºå¼€æ”¾çš„ç†å¿µã€‚

èƒ½å¤Ÿèµ‹èƒ½å¤§æ¨¡å‹å­¦æœ¯ç ”ç©¶å’Œåˆ›æ–°äº§ä¸šç”Ÿæ€ã€‚å‘ƒï¼Œå¹¶ä¸”æˆ‘ä»¬ä¹Ÿä¸€ç›´ä¿æŒæˆ‘ä»¬çš„å‘ƒæŠ€æœ¯çš„åŸå§‹åˆ›æ–°ï¼Œå‘ƒï¼Œæ¢ç´¢æ›´ä¸ºé«˜æ•ˆçš„å‘ƒå¤§æ¨¡å‹å‘å±•è·¯å¾„ã€‚æ•´ä½“æ¥è¯´å‘ƒï¼Œæˆ‘ä»¬å®éªŒå®¤çš„å‘ƒå¤§éƒ¨åˆ†ç ”å‘å’Œå®‰å…¨ä½“ç³»ä½“ç³»æ„å»ºã€‚å‘ƒï¼Œéšç€è¿™ä¸ªæ—¶é—´çš„å†ç¨‹ã€‚

å¯ä»¥åˆ†ä¸º4æ¬¡æŒ‘æˆ˜å’Œå¯¹åº”æˆ‘ä»¬åšçš„ç›¸å…³çš„æŠ€æœ¯å’Œå‘ƒæˆæœã€‚å‘ƒï¼Œæ¥ä¸‹æ¥çš„è¯ä¼šåˆ†è¿™å‘ƒè¿™44æ¬¡æŒ‘æˆ˜ä¸ºå¤§å®¶æ¥ã€‚å‘ƒï¼Œåˆ†äº«ä¸€äº›å·¥ä½œã€‚ç¬¬ä¸€æ¬¡æŒ‘æˆ˜çš„æ—¶å€™æ˜¯å¤§æ¦‚åœ¨å‘ƒ2022022å¹´åº•å‘ƒã€‚

æˆ‘ä»¬å‘ç°è¿™ä¸ªå‘ƒè¶Šæ¥è¶Šå¤šçš„è¿™ä¸ªç”Ÿæˆç”Ÿæˆæ¨¡å‹å‡ºæ¥ä¹‹åå‘ƒï¼Œå®ƒå¸¦æ¥çš„è¿™ä¸ªé£é™©æ€§ä¹Ÿè¶Šæ¥è¶Šå¤§ã€‚å‘ƒï¼ŒåŒ…æ‹¬ä¸€äº›è¿™ç§å‘ƒæ»¥ç”¨çš„è¡Œä¸ºï¼ŒåŒ…æ‹¬ç”¨è¿™ç§æ¨¡å‹å»ã€‚å‘ƒï¼Œå¯¹ï¼Œé‚£ä¸ªä¹”ä¹”è€å¸ˆå·²ç»æ¥äº†ã€‚Yeahï¼Œ rightã€‚

Thank you so muchï¼Œ Drã€‚ Shaoingï¼Œ for delivering an excellent start to the presentationã€‚ğŸ˜Šã€‚

We are delighted to now have Professor To youã€‚ğŸ˜Šï¼ŒProfessor Tao is an assistant director and leading scientist at Shanghai AI Laboratoryã€‚

 as well as a part time researcher at the Shenzhen Institute of Advanced Technology with the Chinese Academy of Sciencesã€‚

His research spans a range of fieldsï¼Œ including multimodal modelsï¼Œ computer visionã€‚

 deep learning and autonomous drivingã€‚In collaboration with other researchers at Shanghai AI Labã€‚

 Professor Tao has recently published numerous AI safety projectsã€‚

 including several benchmarks and assessment frameworksã€‚ Professor Taoã€‚

 the floor is yours to continue the presentationã€‚ Thank you so muchã€‚ğŸ˜Šã€‚

SoI'm sorry for being late and for some important affairsé¦–å…ˆæŠ±æ­‰å•Šï¼Œè¿™ä¸ªå› ä¸ºä¸ªäººçš„åŸå› æŠŠæŠŠè¿™æˆ‘ä»¬è¿™ä¸ªä¼šè®®ä¸­æ–­äº†ä¸€ä¸‹å•Šï¼Œæˆ‘æƒ³è¿™æ ·ã€‚

å…¶å®åˆšæ‰é‚£ä¸ªé‚µé™åšå£«å•Šï¼Œæ˜¯æˆ‘ä»¬æ•´ä¸ªå¤§æ¨¡å‹å®‰å…¨å›¢é˜Ÿçš„è´Ÿè´£äººå‘ƒï¼Œæˆ‘æˆ‘ç”šè‡³åœ¨æƒ³ä»–åº”è¯¥æ¯”æˆ‘ä¼šäº†è§£æ›´å¤šçš„æŠ€æœ¯ç»†èŠ‚ï¼Œä¼šè®²æ›´å¤šçš„å¹²è´§ï¼Œä½†æ˜¯æˆ‘è¿˜æ˜¯éå¸¸æ„Ÿè°¢å•Šå®‰è¿œå’Œè°¢æ€»çš„é‚€è¯·ï¼Œèƒ½æœ‰è¿™æ ·ä¸€ä¸ªæœºä¼šè·Ÿå¤§å®¶åšä¸€ä¸ªåˆ†äº«å•Šã€‚

æˆ‘å°±æˆ‘å°±ç›´æ¥è·³åˆ°åˆšåˆšé‚£ä¸ªæ˜¯ã€‚ğŸ˜Šï¼Œæˆ‘ä»¬æ˜¯è®²åˆ°è¿™ä¸€é¡µæ˜¯å§ï¼Ÿå¯¹å¯¹å¯¹å•Šå—¯ï¼Œå‘ƒï¼Œæˆ‘å…ˆè¯´ä¸€ä¸‹æ˜¯è¿™æ ·çš„ï¼Œæˆ‘å…¶å®åŸæ¥ä¸»è¦åšè®¡ç®—æœºè§†è§‰çš„ç ”ç©¶å•Šï¼Œç„¶åæ…¢æ…¢çš„å¼€å§‹åšè§†è§‰å¤§æ¨¡å‹ã€‚ä»2020å¹´å¼€å§‹ã€‚ç„¶åå‘¢ã€‚

éšç€æˆ‘ä»¬å‘ƒä»å¤šä»è¯­è¨€ä»è§†è§‰åˆ°è¯­è¨€åˆ°å¤šæ¨¡æ€åšï¼Œæˆ‘ä»¬è¶Šæ¥è¶Šå‘ç°å®‰å…¨å®é™…ä¸Šæ˜¯å¤§æ¨¡å‹å‘å±•ä¸­éå¸¸é‡è¦çš„ä¸€ä¸ªé—®é¢˜ã€‚è€Œè¯„æµ‹å‘¢åˆæ˜¯æ•´ä¸ªå»ºç«‹å®‰å…¨çš„åŸºçŸ³ã€‚å•Šæ‰€ä»¥ä»å»å¹´å¼€å§‹å‘¢ï¼Œé‚£åœ¨å®éªŒå®¤çš„æ•´ä¸ªçš„è¿™ä¸ªå¸ƒå±€ä¸‹é¢ã€‚

æˆ‘ä»¬å°±æŠŠè¿™ä¸ªå®‰å…¨å•Šåšä¸€ä¸ªé‡è¦çš„æ–¹å‘ã€‚å¤§å®¶å¯ä»¥çœ‹åˆ°åœ¨åšå®‰å…¨çš„æ—¶å€™ï¼Œé¦–å…ˆçš„ç¬¬ä¸€ä¸ªé—®é¢˜å‘¢ï¼Œå°±æ˜¯è¯´å¤§å®¶äº‹å®ä¸Šéƒ½å¾ˆå…³å¿ƒå®‰å…¨ã€‚ä½†æ˜¯å¯¹äºå®‰å…¨é¢†åŸŸï¼Œæˆ‘ä»¬åˆ°åº•å…·ä½“å…³å¿ƒå“ªäº›å› ç´ åº”è¯¥ä»å“ªäº›è§’åº¦å»è¯„æµ‹ã€‚

å®é™…ä¸Šå¹¶æ²¡æœ‰ç‰¹åˆ«å¹¿æ³›çš„è¿™æ ·çš„ä¸€ä¸ªè®¤çŸ¥ã€‚å•Šï¼Œé‚£åœ¨è¿™é‡Œé¢æˆ‘ä»¬å½“æ—¶çœ‹åˆ°ç¬¬ä¸€ä¸ªäº‹æƒ…å‘¢æ˜¯è¯„æµ‹ä½“ç³»å’Œæ•°æ®çš„ç¼ºä¹ã€‚æ‰€ä»¥åœ¨å»å¹´æˆ‘ä»¬è¿›å…¥è¿™ä¸ªé¢†åŸŸä¹‹åï¼Œç¬¬ä¸€ä»¶äº‹æƒ…å‘¢ï¼Œå°±æ˜¯åšè¯„æµ‹ä½“ç³»ã€‚æˆ‘ä»¬å»ºç«‹äº†å½“æ—¶è’²å…¬è‹±ã€‚ğŸ˜Šã€‚

äººå·¥æ™ºèƒ½ä¸€ä¸ªæ²»ç†å¼€æ”¾çš„å¹³å°ã€‚è¿™é‡Œé¢æŠŠå›½å†…è¿˜æœ‰å›½é™…ä¸Šå•Šå¾ˆå¤šçš„æ²»ç†ç†è®ºåˆ¶åº¦å•Šè¿›è¡Œäº†æ±‡æ€»å•Šï¼Œæˆ‘ä»¬ä¹Ÿå»ºç«‹äº†åŒ…æ‹¬çº¢é˜Ÿæ•°æ®å•Šã€æ¼æ´æ•°æ®å•Šã€è¯„æµ‹æ•°æ®ç­‰ç­‰çš„ï¼Œç›¸å¯¹æ¥è®²æ¯”è¾ƒå…¨çš„ä¸€ä¸ªè¯„æµ‹çš„æ•°æ®é›†ã€‚

åœ¨è¿™é‡Œé¢å®éªŒå®¤ä¹ŸåšæŒå¼€æºå¼€æ”¾çš„ç†å¿µå•Šï¼Œæˆ‘ä»¬æŠŠæˆ‘ä»¬çš„å¾ˆå¤šçš„æ•°æ®é›†ï¼Œè¿˜æœ‰æˆ‘ä»¬çš„è§„èŒƒçš„è¿›è¡Œäº†å¼€æ”¾ã€‚ä»è€Œæ¨åŠ¨è¿™ä¸ªé¢†åŸŸçš„å‘å±•ã€‚æ€ä¹ˆå»ºå¥½è¿™ä¸ªæ•°æ®é›†ï¼Œå¤§å®¶çŸ¥é“æ—©æœŸå¤§å®¶è¯„å¤§æ¨¡å‹ï¼Œå°±æ˜¯æˆ‘ç»™ä¸€ä¸ªå›ºå®šçš„é¢˜é›†ã€‚

ç„¶åæœ‰æ ‡å‡†çš„ç­”æ¡ˆå•Šï¼Œè®©è¿™ä¸ªæ¨¡å‹ç­”å®Œäº†ä¹‹åï¼Œæˆ‘ä»¬å¯¹å®ƒè¿›è¡Œè¯„æµ‹è¿›è¡Œå¯¹é½ã€‚ä½†äº‹å®ä¸Šè¿™é‡Œé¢æ˜¯æœ‰å¾ˆå¤šé—®é¢˜çš„ã€‚æˆ‘ä»¬çŸ¥é“å¯¹äºå¤§æ¨¡å‹æ¥è®²ï¼Œå¦‚æœä½ åšäº†SFTåšäº†RLRLHFäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼Œäº‹å®ä¸Šå®ƒè¡¨é¢ä¸Šè¡¨ç°çš„å¾ˆå¥½ã€‚

ä½†å¹¶ä¸æ˜¯ä»£è¡¨ä»–çœŸæ­£çš„å°±æ²¡æœ‰é£é™©ï¼Œæ²¡æœ‰é—®é¢˜å•Šï¼Œé‚£åœ¨è¿™é‡Œé¢æ€ä¹ˆæ¥åšå‘¢ï¼Ÿæˆ‘ä»¬å½“æ—¶è®¾è®¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°å•Šå¾ˆå¤šé¢†åŸŸçš„ä¸“å®¶ï¼ŒåŒ…æ‹¬å¾ˆå¤šåšç¤¾ä¼šå­¦æ”¿æ²»å­¦ä¼¦ç†æ³•å¾‹çš„ï¼Œæˆ‘ä»¬ç»„ç»‡äº†ä¸Šç™¾ä½ã€‚

æˆ‘ä»¬è¯·äº†åŒ…æ‹¬äº¤ä¸Šæµ·äº¤é€šå¤§å­¦å¤æ—¦å¤§å­¦çš„ä¸Šç™¾ä½çš„ä¸“å®¶ï¼Œè¿™é‡Œé¢è¿˜æœ‰åŒ…æ‹¬å¾ˆå¤šçš„æ•™æˆï¼Œè®©ä»–ä»¬å¸®æˆ‘ä»¬è®¾è®¡æ€»å­é—®é¢˜ã€‚å› ä¸ºè¿™äº›ä¸“å®¶æ—¶é—´éå¸¸æœ‰é™ï¼Œä½ å¾ˆéš¾è¯´ä½ è¯·è¿™ä¹ˆå¤šä¸“å®¶ï¼Œè®©æ¯ä¸€ä¸ªä¸“å®¶ã€‚

ä½ å¸®æˆ‘è®¾è®¡1ä¸‡ä¸ªé¢˜1ä¸‡ä¸ªé¢˜å®ä¸Šå¯¹å¤§æ¨¡å‹çš„è¯„æµ‹è¿˜æ˜¯éå¸¸çª„çš„ä¸€ä¸ªé¢†åŸŸäº†å•Šï¼Œæˆ‘ä»¬å‘¢åœ¨è¿™ä¸ªä¸“å®¶è®¾è®¡é¢˜çš„é¢˜ç›®ä¸Šå‘¢ï¼Œé€šè¿‡å¤§æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯¹è¿™äº›é¢˜è¿›è¡Œå¢å¼ºè¿›è¡Œæ‰©å……å•Šã€‚ç„¶åå‘¢ã€‚ğŸ˜Šï¼Œå†ç”¨è¿™äº›é¢˜ç›®å‘¢å»è¯„æµ‹ã€‚

ç‰¹åˆ«æ˜¯è¿™äº›é¢˜ç›®å¾ˆå¤šçš„è®¾è®¡å‘¢ï¼Œå®ƒæœ‰å¾ˆå¤šçš„è¿™ä¸ªæ”»å‡»æ€§ï¼Œè€Œä¸”æœ‰é’ˆå¯¹æ¼æ´çš„å‘ƒä¸€äº›ä¸“é—¨çš„ä¸€äº›è®¾è®¡ï¼Œè¿™æ ·å°±å½¢æˆäº†æ›´å¥½çš„è¿™æ ·è¯„æµ‹çš„æ•ˆæœã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨ç¬¬äºŒä¸ªæ–¹é¢æ‰€åšçš„å·¥ä½œã€‚ç„¶åå‘¢ï¼Œæˆ‘ä»¬åœ¨å› ä¸ºæˆ‘æ—¢åšæ¨¡å‹ï¼Œä¹Ÿåšæ¨¡å‹çš„å®‰å…¨ã€‚

åœ¨è¿™é‡Œæˆ‘ä»¬å°±å‘ç°è¿™ä¸ªå¤§æ¨¡å‹çš„å®‰å…¨å’Œå¯¹é½å•Šï¼Œå¤§å®¶ç»å¸¸è¯´æœ‰ä¸€ä¸ªé—®é¢˜å«å¯¹é½ç¨å•Šï¼Œå°±æ˜¯ä¹Ÿè®¸ç¡®ç¡®å®å®ä½ åšäº†Läº†ä¹‹åï¼Œè¿™ä¸ªæ¨¡å‹çš„å®‰å…¨æ€§ä»è¯„æµ‹ä¸Šåˆ†æ•°ä¼šæœ‰å¢åŠ ã€‚ä½†å¾€å¾€å‘¢å¯¹èƒ½åŠ›å•Šã€‚

å¯¹åŸæ¥çš„åŸæœ‰çš„ä¸€äº›èƒ½åŠ›æ¨¡å‹çš„æ€§èƒ½ä¼šæœ‰ä¸‹é™ã€‚é‚£åœ¨è¿™é‡Œé¢å‘¢ï¼Œæˆ‘ä»¬æ˜¯ä¸æ˜¯æœ‰æ›´å¥½çš„æ–¹æ³•ã€‚åœ¨è¿™é‡Œé¢å‘¢ï¼Œæˆ‘ä»¬äº‹å®ä¸Šå°±æŠŠä¸€äº›MODPOå°±æ˜¯mar objectjectPOå¤šç›®æ ‡çš„å¯¹é½å¼•å…¥è¿›æ¥ã€‚

è¿™æ ·èƒ½å¤Ÿä¿è¯æˆ‘ä»¬åœ¨ä¿è¯è¿™ä¸ªå¯¹é½çš„åŒæ—¶å‘¢ä¹Ÿèƒ½å¤Ÿå¯¹åŸæ¥çš„èƒ½åŠ›è¿›è¡Œä¼˜åŒ–ã€‚è¿™é‡Œé¢æœ‰äº›ç®—æ³•çš„åˆ›æ–°ã€‚ç”±äºæ—¶é—´åŸå› æˆ‘å¯èƒ½å°±ä¸å†è¯¦ç»†è®²ã€‚è€Œä¸”æˆ‘åœ¨æƒ³åœ¨è¿™é‡Œé¢MODPOç°åœ¨ç›®å‰ä¹Ÿä¸æ˜¯ä¸€ä¸ªç»“æŸå¼ã€‚

åº”è¯¥è¿™äº›ç®—æ³•å’Œæ¡†æ¶è¿˜æœ‰å¾ˆå¤šä¼˜åŒ–çš„ç©ºé—´ã€‚æˆ‘çœ‹åœ¨åº§æœ‰å¾ˆå¤šæˆ‘ä»¬å¹´è½»çš„è¿™ä¸ªåŒå­¦ï¼Œè¿™å¯èƒ½æˆä¸ºä½ æœªæ¥ç ”ç©¶å¾ˆå¥½çš„é¢˜ç›®ã€‚ğŸ˜Šï¼ŒåŸºäºæˆ‘ä»¬å½¢æˆçš„è¯„æµ‹ä½“ç³»ã€‚

åŸºäºè¿™äº›è¯„æµ‹çš„æ•°æ®å‘¢å‘ƒæˆ‘ä»¬å®éªŒå®¤ä¹Ÿç ”å‘äº†ä¸€ä¸ªæ™®å®‰å¤§æ¨¡å‹å®‰å…¨è¯„æµ‹çš„å‘ƒè¿™ä¸ªç³»ç»Ÿå•Šï¼Œåœ¨è¿™é‡Œé¢å‘¢å‘ƒæˆ‘ä»¬äº‹å®ä¸ŠæŠŠä¸€äº›æˆ‘ä»¬å‰é¢æ‰€åšçš„ä»è¿™ä¸ªç»´åº¦å•Šåˆ°è¯„æµ‹çš„è¿‡ç¨‹åˆ°å‡ºæŠ¥å‘Šå®Œå…¨æŠŠå®ƒè¿›è¡Œè‡ªåŠ¨åŒ–çš„è¯„æµ‹ã€‚è¿™é‡Œé¢ä¹Ÿæ”¯æŒäº†å•Šã€‚

æˆ‘æˆ‘ä»¬ä¸Šæµ·å¸‚çš„ä¸€äº›å·¥ä½œå•Šï¼Œåœ¨è¿™é‡Œé¢å•Šæˆ‘ä»¬å‘ç°å‘¢å°±æ˜¯è¯´è¯„æµ‹ç»“æœçš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§æ˜¯ä¸€ä¸ªå¾ˆé‡è¦çš„ä¸€ä¸ªäº‹æƒ…å•Šï¼Œå› ä¸ºç»å¸¸çš„æ—¶å€™éšç€è¿™ä¸ªè¯„æµ‹çš„ç»´åº¦è¶Šæ¥è¶Šå¤šï¼Œé—®çš„é¢˜ç›®è¶Šæ¥è¶Šæ·±ã€‚

ç»å¸¸å‘¢æˆ‘ä»¬å¾ˆå¤šå¤§æ¨¡å‹çš„è¿™ä¸ªè¯„æµ‹ç”¨æˆ·æ‹¿åˆ°è¯„æµ‹æŠ¥å‘Šï¼Œä»–ä¼šä¼šé—®æˆ‘ä»¬å“¦ä½ ä»¬ä¸ºä»€ä¹ˆæ¨¡å‹è¿™ä¹ˆä½ ä¸ºä»€ä¹ˆå‡ºè¿™æ ·è¯„æµ‹æŠ¥å‘Šã€‚

ä½ è¿™ä¸ªè¯„æµ‹æŠ¥å‘Šæ˜¯ä¸æ˜¯å®¢è§‚èƒ½ä¸èƒ½å¯¹ä»–ä½ çš„ç»“ç»“æœçš„ä¾æ®å•Šè¿›è¡Œè¯´æ˜è¿›è¡Œè§£é‡Šå•Šåœ¨è¿™é‡Œé¢å‘¢æˆ‘ä»¬æƒ³åˆ°æœ€ç»ˆçš„è¯æˆ‘ä»¬è§£é‡Šè¿˜æ˜¯åŸºäºæˆ‘ä»¬å»ºç«‹åŸºäºè¿™ä¸ªæ•°æ®åº“å®Œäº†ä¹‹åå‘¢ï¼Œæˆ‘ä»¬æŠŠæˆ‘ä»¬çš„è¯„æµ‹ç»“æœå¯¹å®ƒè§£è¿›è¡Œå¯è§£é‡Šæ€§çš„ç”Ÿæˆã€‚

è¿™æ ·å°±æŠŠè¿™ä¸ªé—®ç­”å’Œå¯è§£é‡Šæ€§çš„ä¸€èµ·çš„ä¸€ä½“åŒ–é›†æˆåœ¨ä¸€èµ·ã€‚è€Œä¸”å‘¢å› ä¸ºé‡‡ç”¨raçš„æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¤–ç±çš„æ•°æ®åº“ï¼Œå¾ˆå¥½çš„æ–¹æ³•å°±åœ¨ä½ å¯ä»¥å®æ—¶çš„æ›´æ–°å•Šã€‚æ¯”å¦‚è¯´æˆ‘ä»¬æœ‰æ–°çš„è§„èŒƒï¼Œæ–°çš„è§„å®šã€‚

æˆ‘ä»¬å¯ä»¥éå¸¸ç®€å•çš„æ›´æ–°åˆ°è¿™ä¸ªæ•°æ®åº“é‡Œé¢å‘¢è¿›è¡Œè¯„æµ‹ã€‚é‚£å½“ç„¶å‘¢å¤§å®¶çŸ¥é“ç°åœ¨è¿™ä¸ªæ•´ä¸ªå¤§æ¨¡å‹ä»è¯­è¨€åˆ°å¤šæ¨¡æ€å‘å±•å•Šï¼Œæˆ‘æœ¬èº«ä¹Ÿåšè®¡ç®—æœºè§†è§‰çš„ç ”ç©¶ã€‚å¤šæ¨¡æ€å¤§æ¨¡å‹çš„äº‹å®ä¸Šåˆå¯¹è¿™ä¸ªè¯„æµ‹æäº†å¾ˆæ›´æ–°çš„è¦æ±‚å•Šã€‚

æ¯”å¦‚è¯´åœ¨è¿™é‡Œé¢å¯¹äºå¤šæ¨¡æ€æ¥è®²ï¼Œç”±äºå®ƒå†…å®¹ç”Ÿæˆçš„å½¢å¼å’Œå½¢æ€æ›´å¤šï¼Œå¦‚ä½•å®šä¹‰è¿™ä¸ªç»´åº¦éœ€è¦è¿›ä¸€æ­¥åŠ ã€‚å¦å¤–å‘¢åœ¨å¤šæ¨¡æ€ä¹‹é—´æœ¬èº«å°±æœ‰ç‰¹å¾çš„å¯¹é½çš„é—®é¢˜ã€‚è€Œä¸”æˆ‘ä»¬çŸ¥é“è¿™ä¸ªè¯­è¨€åˆ°å¤šæ¨¡æ€äº†ä¹‹åã€‚

äº‹å®ä¸Šå¾ˆå¤šå¹»è§‰é—®é¢˜è¢«è¿›ä¸€æ­¥åŠ å¼ºã€‚é’ˆå¯¹è¿™äº›ç‰¹ç‚¹å‘¢ï¼Œæˆ‘ä»¬åšäº†ä¸€ä¸ªä»¥è§†è§‰ä¸ºæ ¸å¿ƒçš„å•Šï¼ŒåŒ…æ‹¬å¯¹é½è¿˜æœ‰è¯„æµ‹çš„æ•°æ®æœºSPVLå®ƒåŒ…æ‹¬æˆ‘ä»¬è®¾è®¡äº†é’ˆå¯¹å¤šæ¨¡æ€é¢†åŸŸ6ä¸ªå•Šæ¯”è¾ƒå¯èƒ½å¼•èµ·æœ‰å®³çš„é¢†åŸŸï¼Œå››ä¸‰ä¸ªç±»åˆ«ï¼Œ5ä¸‰ä¸ªå­ç±»åˆ«ã€‚

è¿™é‡Œé¢ä¹ŸåŒ…æ‹¬äº†è¶…ã€‚ğŸ˜Šï¼Œè¿‡10ä¸‡ä¸ªå•Šè¿™æ ·çš„é—®é¢˜å•Šï¼Œç„¶åæ¥å¸®åŠ©æˆ‘ä»¬ä¸€æ–¹é¢åšè¯„æµ‹ã€‚å¦å¤–è¿™ä¸ªæ•°æ®ä¹Ÿå¯ä»¥ä½œä¸ºå¯¹é½å•Šæ­¤å¤–å‘¢å®éªŒå®¤å‘¢æˆ‘ä»¬åœ¨å»å¹´è¿˜èŠ±äº†å¾ˆå¤§åŠ›æ°”å•Šã€‚

æˆ‘ä»¬åœ¨å¤šåº”è¯¥æ˜¯åœ¨å›½å†…è¿˜ç”šè‡³æ˜¯å›½é™…ä¸Šæ¯”è¾ƒå…¨é¢çš„ä¸€ä¸ªåŸºäºäººç±»ä»·å€¼è§‚çš„ä¸€ä¸ªå¤šæ¨¡æ€è¯„æµ‹ä½“ç³»ã€‚è¿™ä¸ªæŠ¥å‘Šå‘¢æ€»å…±æœ‰400å¤šé¡µå››ç§æ¨¡æ€ã€‚

è€Œä¸”åœ¨è¿™é‡Œé¢å‘¢æˆ‘ä»¬å‘ç°å•Šå°±æ˜¯è¯´å•ç‹¬çš„æç»´åº¦äº‹å®ä¸Šå¯¹äºè¿™ä¸ªé¢†åŸŸçš„æŠ€æœ¯çš„å‘å±•è¿˜æ˜¯å¾ˆæœ‰å±€é™æ€§çš„ã€‚æˆ‘ä»¬å»ºç«‹äº†230ä¸ªç”¨ä¾‹ã€‚è€Œä¸”è¿™230ä¸ªç”¨ä¾‹å‘¢æ˜¯æ ¹æ®å¤šæ¨¡æ€å¤§æ¨¡å‹åº”ç”¨å•Šï¼Œåœºæ™¯è½åœ°æ¥æå‡ºæ¥çš„ï¼ŒåŒ…æ‹¬æ³›åŒ–æ€§å¯ä¿¡åº¦ã€‚

è¿˜æœ‰æ¨ç†èƒ½åŠ›ã€‚å¤§å®¶çŸ¥é“æœªæ¥å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå®é™…ä¸Šå®ƒçš„æ¨ç†èƒ½åŠ›æ˜¯å¤§å®¶ç°åœ¨å…³æ³¨çš„é‡ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ€æ–°çš„å¤§æ¨¡å‹ï¼Œå¤§å®¶éƒ½åœ¨å¼ºè°ƒå®ƒçš„æ•°ç†å’Œå› æœçš„æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„æµ‹ã€‚

å®é™…ä¸Šè¿™ä¸ªè¯„æµ‹ç»“æœå‘¢ä¹Ÿä¹Ÿå¯¹æˆ‘ä»¬åé¢åŒ…æ‹¬å¤§æ¨¡å‹ç‰¹åˆ«å¤šæ¨¡æ€å¤§æ¨¡å‹å®‰å…¨çš„è¯„æµ‹èµ·åˆ°äº†å¾ˆå¤§çš„æŒ‡å¼•çš„è¿‡ç¨‹ã€‚é¢å‘æœªæ¥å‘¢ï¼Œæˆ‘ä»¬è§‰å¾—æœ‰å‡ ã€‚å„æ–¹é¢å€¼å¾—å…³æ³¨ã€‚è¿˜æœ‰ä¸€ä¸ªå‘ƒæ¯”å¦‚å‘ƒä¸€ä¸ªå°±æ˜¯è¿™ä¸ªå¤šå‘ƒå¤šæ™ºèƒ½ä½“ã€‚

å¤§å®¶çŸ¥é“å•Šè¿™åŠå¹´Aentå‘ƒæŠ€æœ¯å‘å±•å¾ˆå¿«å•Šï¼ŒAentå·²ç»æˆä¸ºå¼ºåŒ–å¤§æ¨¡å‹åœ¨ä¸“é¡¹é¢†åŸŸè·ŸåŒ…æ‹¬è·Ÿå¾ˆå¤šä¸–ç•Œè¿›è¡Œäº’åŠ¨çš„ä¸€ä¸ªé‡è¦çš„æ‰‹æ®µã€‚é‚£æ˜¯ä¸æ˜¯èƒ½ä¸èƒ½å¤ŸæŠŠè¿™äº›æŠ€æœ¯ä¹Ÿç”¨æ¥å®‰å…¨å’Œè¯„æµ‹å‘¢ã€‚

å®é™…ä¸Šæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“çš„è¯„æµ‹å…‰æ¶å«P safeã€‚ç„¶ååœ¨è¿™é‡Œé¢å‘¢æˆ‘ä»¬è¯¦ç»†ç ”ç©¶äº†å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´ï¼Œä»–å¤§å®¶çŸ¥é“å®ƒä¹‹é—´æœ‰äº¤äº’å’ŒååŒã€‚å®ƒæ‰€èƒ½äº§ç”Ÿçš„è¿™ä¸ªå±é™©è¡Œä¸ºï¼Œä»¥åŠåœ¨è¿™è¿‡ç¨‹ä¸­é—´ã€‚

æˆ‘ä»¬å¦‚ä½•å¼•å…¥ä¸€ä¸ªä¸“å®¶ä¸€ä¸ªdoctorå»è¿›è¡Œè§’è‰²é˜²å¾¡ã€‚å¦å¤–ä¸€ä¸ªå‘¢å¤§å®¶çŸ¥é“äººä¸äººä¹‹é—´äº¤äº’è¿‡ç¨‹ä¸­ï¼Œå¿ƒç†å•Šè¿™ä¸ªä»·å€¼è§‚æ˜¯éå¸¸é‡è¦çš„ä¸€ä¸ªé—®é¢˜ã€‚æ‰€ä»¥å‘¢æˆ‘ä»¬ä¹Ÿåœ¨æ¢ç´¢å¦‚ä½•ä»ä»¿é€ äººçš„æ–¹æ³•ï¼Œä»å¿ƒç†å­¦çš„è§’åº¦å•Šã€‚

ç„¶åä»äººæ–‡ç§‘å­¦çš„è§’åº¦æ¥å»åšè¿™ä¸ªå¤§æ¨¡å‹å¤šæ™ºèƒ½ä½“çš„å®‰å…¨è¯„æµ‹ï¼Œæˆ‘æƒ³è¯´ä¸€ç‚¹æ˜¯è¿™äº›å·¥ä½œå…¶å®æ‰åˆšåˆšèµ·æ­¥ï¼Œä»å®éªŒå®¤è§’åº¦æ¥è®²å‘¢ï¼Œæˆ‘ä»¬æœ€ã€‚ä¸å…³æ³¨çš„åŒ…æ‹¬å®šä¹‰é—®é¢˜å•Šï¼ŒåŒ…æ‹¬å»ºä¸€ä¸ªå¥½çš„å¹³å°ï¼Œè®©å¤§å®¶æ¥åšè¿™æ–¹é¢çš„ç ”ç©¶ã€‚

è¿™é‡Œé¢æœ‰éå¸¸å¤šçš„é‡è¦å·¥ä½œï¼Œæˆ‘ä»¬ä¹Ÿä¼šéå¸¸æ¬¢è¿å•Šåœ¨åº§å„ä½çš„æœºæ„ã€ä¼ä¸šï¼Œè¿˜æœ‰ç ”ç©¶è€…å’Œæˆ‘ä»¬ä¸€èµ·æ¥å…±åŒæ¨åŠ¨è¿™ä¸ªé¢†åŸŸçš„å‘å±•ã€‚å‘ƒï¼Œå®éªŒå®¤é™¤äº†è‡ªå·±åšä¹‹å¤–å‘¢ï¼Œæˆ‘ä»¬è¿˜åšæŒç€å¼€æ”¾çš„ç­–ç•¥ã€‚äº‹å®ä¸Šå‘¢å‘ƒæˆ‘åœ¨ä¸­å›½ç½‘ç»œç©ºé—´åä¼šã€‚

å¤§å®¶çŸ¥é“è¿™æ˜¯åœ¨æˆ‘ä»¬å›½å®¶è¿™ä¸ªé¢†åŸŸéå¸¸æƒå¨çš„åä¼šçš„å‘ƒçš„æ¶æ„å’ŒæŒ‡å¯¼ä¸‹å‘¢ï¼Œæˆ‘ä»¬æˆç«‹äº†å›´ç»•ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å®‰å…¨è¯„æµ‹çš„å·¥ä½œç»„ã€‚æˆ‘æœ¬äººå‘¢ä¹Ÿéå¸¸æœ‰å¹¸çš„å½“é€‰äº†å·¥ä½œç»„çš„ç»„é•¿ï¼Œå…¶å®å‹åŠ›è¿˜æ˜¯å¾ˆå¤§çš„ã€‚å› ä¸ºè¿™ä¸ªå·¥ä½œç»„ã€‚

æ—¢åŒ…æ‹¬å’±ä»¬å›½å®¶ä¸€äº›æƒå¨æœºæ„ï¼Œä¹ŸåŒ…æ‹¬åœ¨è¿™é‡Œé¢å¤§ç ”å‘çš„ï¼ŒåŒ…æ‹¬äº¤å¤§åŒ…æ‹¬äº¤å¤§ã€æ¸…åã€å¤æ—¦è¿™é¡¹å¤§å­¦ï¼Œè¿˜æœ‰åŒ…æ‹¬ç™¾åº¦åä¸ºè¿™æ ·çš„ä¼ä¸šã€‚ä¸ºä»€ä¹ˆè¦æˆç«‹ä¸ªå·¥ä½œç»„å‘¢ï¼Œæˆ‘ä»¬è®¤ä¸ºå‘¢è¦åšå¥½åšèƒ½ç®¡æ˜¯å®‰å…¨è¿˜æ˜¯è¯„æµ‹ï¼Œå®ƒä¸€å®šä¸æ˜¯ä¸€ä¸ªæœºæ„ã€‚

æ˜¯è¡Œä¸€ä¸ªå…±å•†å…±æ²»çš„äº‹æƒ…ã€‚åœ¨è¿™å·¥ä½œç»„åœ¨è¿‡å»çš„ä¸€å¹´æ¥è¿‘ä¸€å¹´çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»å‡ æ–¹é¢å¼€å±•å·¥ä½œã€‚ä¸€æ–¹é¢å‘¢æ˜¯å»ºç«‹äº†å¸¸æ€åŒ–çš„è¿™ä¸ªäº¤æµçš„æœºåˆ¶å•Šï¼Œæˆ‘ä»¬æœ‰æ—¥å¸¸çš„ä¾‹ä¼šã€‚è€Œä¸”ç°åœ¨é€šè¿‡çº¿ä¸Šä¼šè®®å•Šï¼Œé€šè¿‡ä¸“é¢˜ä¼šçš„å½¢å¼ã€‚

è®©å¤§å®¶æ—¥å¸¸èƒ½äº¤æµã€‚ç¬¬äºŒä¸ªå‘¢ï¼Œæˆ‘æƒ³è¿™ä¸ªå·¥ä½œç»„çš„ä¸€ä¸ªå¾ˆé‡è¦çš„æ–¹é¢å‘¢ï¼Œæˆ‘ä»¬æ˜¯æƒ³ä» upçš„ï¼Œä»åº•å±‚ä»ã€‚ä¸€çº¿çš„è§’åº¦èƒ½å¤Ÿå½¢æˆæŠ€æœ¯è§„èŒƒå’Œå…±è¯†å•Šï¼Œåœ¨è¿™é‡Œé¢æˆ‘ä¸‹é¢ä¼šè®²ã€‚

æˆ‘ä»¬ç°åœ¨å·²ç»åˆæ­¥å½¢æˆäº†ä¸€ä¸ªç”Ÿç§°å¼äººé—´æ™ºèƒ½è¯„ä¼°çš„æµç¨‹è§„èŒƒå•Šã€‚è¿™é‡Œé¢æˆ‘ä¸çŸ¥é“åœ¨åº§çš„æ˜¯ä¸æ˜¯æœ‰ä¸€äº›åŒäº‹å·²ç»å‚ä¸äº†è¿™ä¸ªè§„æ¶‰åŠçš„èŒƒå›´æ¯”è¾ƒå¹¿å·²ç»å‚ä¸åˆ°è¿™ä¸ªå·¥ä½œè¿›æ¥ã€‚ç¬¬ä¸‰ä¸ªå‘¢ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªè§„èŒƒçš„åŸºç¡€ä¸Šå‘¢ã€‚

æˆ‘ä»¬ä¼šè·Ÿå¤§å®¶ä¸€èµ·å•Šå…±åŒç ”å‘ä¸€äº›è¯„æµ‹çš„æŠ€æœ¯å’Œä¸»å»ºå•Šï¼Œè¿™é‡Œé¢åŒ…æ‹¬æ•°æ®é›†ï¼Œä¹ŸåŒ…æ‹¬å·¥å…·ï¼Œè€Œä¸”å‘¢åŒ…æ‹¬æˆ‘ä»¬åˆšæ‰è®²çš„å¹³å°ï¼Œè€Œä¸”è¿™é‡Œé¢æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿåœ¨å¤§å®¶å½¢æˆä¸€è‡´çš„æ¡ä»¶ä¸‹ï¼Œèƒ½å¤Ÿæ‹¿å‡ºæ¥ä¸€éƒ¨åˆ†è¿›è¡Œå¼€æºå¼€æ”¾çš„å½¢å¼æä¾›æœåŠ¡ã€‚

æœ€åå‘¢æˆ‘ä»¬ä¹Ÿä¸¾åŠäº†å¾ˆå¤šå®‰å…¨çš„æ´»åŠ¨ï¼ŒåŒ…æ‹¬è·Ÿå®‰è¿œçš„å¾ˆå¤šæ´»åŠ¨ã€‚ä»Šå¹´åœ¨ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šçš„å‰å¤•å‘¢ï¼Œæˆ‘ä»¬ä¹Ÿä¸¾åŠäº†æ™®æºå®‰å…¨æŒ‘æˆ˜èµ›ï¼Œå¸å¼•äº†å…¨çƒä¸€ä¸¤ç™¾ä¸ªéå¸¸ä¼˜ç§€çš„å›¢é˜Ÿæ¥å‚åŠ å•Šã€‚

æˆ‘ä»¬ä¹Ÿæ—¥å¸¸è·Ÿå¾ˆå¤šçš„ä¸€äº›ç›¸å…³çš„å¤§æ¨¡å‹ç ”å‘æœºæ„åšè¿™ä¸ªå®‰å…¨çš„æŒ‡å¯¼å•Šï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ‰€åšçš„åˆšæ‰æåˆ°è¿™ä¸ªå®‰å…¨è¯„ä¼°çš„æµç¨‹å’Œè§„èŒƒã€‚å®ƒæœ‰å‡ ä¸ªç‰¹ç‚¹ã€‚ç¬¬ä¸€ä¸ªå®ƒã€‚å¯æ“ä½œæ€§å¾ˆå¼ºã€‚æˆ‘ä»¬æ˜¯ä¸€ä¸ªæƒç»´åº¦å¹³åº§ï¼Œæä¾›å…¨ç”Ÿå‘½çš„è¿™æ ·ä¸€ä¸ªè§„èŒƒã€‚

ç¬¬äºŒä¸ªç‰¹ç‚¹å‘¢ï¼Œæˆ‘ä»¬æ˜¯æœåŠ¡è¿™ä¸ªåº”ç”¨å’Œäº§ä¸šè½åœ°ã€‚å› ä¸ºå‚ä¸çš„æœ¬èº«å¾ˆå¤šä¼ä¸šï¼Œæ˜¯è®©ä»–ä»¬ä»è¿™ä¸ªå®é™…åº”ç”¨è½åœ°çš„è§’åº¦èµ·äº†å¾ˆå¥½çš„å»ºè®®å•Šã€‚æˆ‘ä»¬å¸Œæœ›å‘¢è¿™ä¸ªè§„èŒƒä¹Ÿèƒ½æˆä¸ºæ¨åŠ¨å’±ä»¬å›½å®¶æœªæ¥äººå·¥æ™ºèƒ½å®‰å…¨è¯„æµ‹å‘å±•çš„é‡è¦ã€‚é¢å‘æœªæ¥å‘¢ã€‚

æˆ‘æƒ³è¯´ä¸€ç‚¹ï¼Œå¤§å®¶å¾€å¾€å®¹æ˜“æŠŠäººå·¥æ™ºèƒ½çœ‹æˆä¸€ä¸ªå·¥å…·ã€‚ä½†æ˜¯æˆ‘æ›´æƒ³è¯´çš„ï¼Œéšç€é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œéšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œäººå·¥æ™ºèƒ½æˆåœ¨æˆåœ¨æˆä¸ºæˆ‘ä»¬æ•´ä¸ªç¤¾ä¼šä½“ç³»çš„éå¸¸é‡è¦çš„ä¸€ä¸ªåŸºç¡€è®¾æ–½ã€‚å®ƒä¼šé¢‘ç¹çš„ä¸äººäº’åŠ¨ä¸å…¶ä»–çš„ç³»ç»Ÿäº’åŠ¨ã€‚

æ‰€ä»¥æˆ‘ä»¬ç°åœ¨è€ƒè™‘äººå·¥æ™ºèƒ½å®‰å…¨ç»å¯¹ä¸èƒ½å­¤ç«‹çš„æŠŠå®ƒçœ‹æˆä¸€ä¸ªå·¥å…·ï¼Œå•Šï¼Œåº”è¯¥ä»æ•´ä¸ªç¤¾ä¼šä½“ç³»çš„è§’åº¦æ¥æ€è€ƒäººå·¥æ™ºèƒ½ï¼Œè¿™é‡Œé¢åŒ…æ‹¬æ•´ä¸ªéœ€æ±‚ï¼ŒåŒ…æ‹¬åº”ç”¨çš„åœºæ™¯ï¼Œè¿˜æœ‰åŒ…æ‹¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­é—´ã€‚

äººæœºç‰©ä¹‹é—´çš„è¿™ç§è®¤çŸ¥ä½“ç³»çš„å»ºè®¾æ˜¯å¾ˆé‡è¦çš„åŠæ³•ã€‚æ€ä¹ˆæ¥åº”ç”¨è¿™ä¸ªåœ°æ–¹å‘¢ï¼Œæˆ‘è§‰å¾—å°±æ˜¯è¯´å¤§å®¶çŸ¥é“å¤§æ¨¡å‹çš„å‘å±•æ˜¯ä»¥scalingä¸ºæŒ‡å¼•ã€‚éšç€ç®—åŠ›æ•°æ®æ¨¡å‹è§„æ¨¡çš„å¢å¤§æ¨¡å‹çš„å®‰å…¨å‘ƒæ¨¡å‹çš„èƒ½åŠ›ä¸æ–­æå‡ã€‚

é‚£ä¹ˆæˆ‘ä»¬åœ¨æƒ³æ˜¯ä¸æ˜¯æˆ‘ä»¬ä¹Ÿè¦æ¢ç´¢ä¸€ä¸ªå›´ç»•å®‰å…¨çš„scalingå•Šï¼Œæˆ‘ä»¬èƒ½æ‰¾åˆ°ä¸€ä¸ªå¯æ‰©å±•å¯å‘å±•çš„æ–¹å¼ã€‚åªè¦æˆ‘ä»¬æŠ•å…¥æ›´å¤šçš„ç ”å‘çš„èµ„æºï¼Œæ•°æ®ç®—åŠ›ä¸€äº›æŠ€æœ¯çš„ç ”å‘çš„æŠ•å…¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠå®‰å…¨ä¹Ÿå¯ä»¥å¯æŒç»­çš„å‘å±•ä¸‹å»ã€‚å•Šã€‚

åœ¨è¿™é‡Œé¢å‘¢ï¼Œæˆ‘è§‰å¾—å®‰å…¨çš„scaling lawã€‚å¯èƒ½æ¯”åŸæ¥å¤§æ¨¡å‹çš„scalingå¤§æ¨¡å‹æ ¸å¿ƒæ˜¯å‚æ•°é‡ã€æ•°æ®é‡å’Œè®¡ç®—èµ„æºã€‚è¿™ä¸‰è€…é¦–å…ˆä¹Ÿæ˜¯å®‰å…¨çš„æ›´é‡è¦çš„ç»„æˆéƒ¨åˆ†ã€‚ä½†æ˜¯å¯¹å®‰å…¨çš„sç»å¯¹ä¸æ­¢ä¸‰ä¸ªç»´åº¦ã€‚

æˆ‘ä»¬éœ€è¦å¤šæ–¹çš„å‚ä¸ï¼Œéœ€è¦æ›´æ–°çš„ç ”å‘çš„æ¨¡å¼å•Šï¼Œå½“ç„¶æˆ‘ä»¬ä¹Ÿéœ€è¦é«˜è´¨é‡çš„æ•°æ®ä¸åŠæ›´å¥½çš„æ¨¡å‹çš„æ¶æ„ã€‚è¿™é‡Œé¢äº‹å®ä¸Šä¹Ÿå¯¹æˆ‘ä»¬çš„itä¸ç®¡æ˜¯ä½ åšç§‘ç ”çš„åˆ¶ä½œäº§ä¸šç”¨æé«˜äº†æ–°çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è·Ÿå¤§å®¶å…±åŒèƒ½åŠ›ã€‚

é¢å‘å»ºç«‹èµ·é¢å‘æœªæ¥çš„å¯æŒç»­çš„IIå®‰å…¨çš„scalingå•Šï¼Œåœ¨è¿™é‡Œæœ€åå‘¢æˆ‘ä¹Ÿç¨ç¨åšä¸€ä¸ªå®£ä¼ ï¼Œåœ¨æ˜å¤©çš„ä¸Šåˆå•Šæˆ‘ä»¬åœ¨ä¸–åšä¸­å¿ƒ620ä¼šè®®å®¤æœ‰ä¸€ä¸ªå›½é™…HAIå‰æ²¿æŠ€æœ¯çš„è®ºå›å•Šã€‚

æˆ‘çŸ¥é“ä»Šå¤©åœ¨åº§çš„å‘ƒä¸€éƒ¨åˆ†ä¸“å®¶ä¹Ÿä¼šå‚åŠ æ˜å¤©çš„è®ºå›ï¼Œä¹Ÿæ¬¢è¿å¤§å®¶ç»§ç»­å‚åŠ å’Œæ”¯æŒæˆ‘ä»¬çš„æ´»åŠ¨ã€‚å¥½ï¼Œè°¢è°¢å¤§å®¶ã€‚Thank you so muchï¼Œ Professor Tiaoã€‚

 we know you have an incredibly tight scheduleï¼Œ so truly appreciate you coming to our forumã€‚ğŸ˜Šã€‚

Please kindly come back on stage and prepare to be seated on the panel as we transition to our panel discussion on AI safetyã€‚

 Professor Qiaoã€‚ğŸ˜Šï¼ŒChrisï¼Œ direct awayiï¼Œ We Kaiã€‚Please also join us back on stage with the panelã€‚

For this panelï¼Œ we are also joined by Doctorã€‚ Rui Minhe and Professor Xiong Deyiã€‚

Dctor He and Professor Seongï¼Œ please kindly make your way on stage as I introduce youã€‚ğŸ˜Šï¼ŒDrã€‚

 Rei Minhe is Singapore's chief artificial intelligence officerã€‚

 where he leads a multi stakeholder effort to achieve Singapore's strategic AI objectivesã€‚

Including developing and implementing Singapore's national AI strategyã€‚

He is also concurrently the Singaporean government's deputy chief digital technology officer and a member of the United Nations High level advisorory body on AIã€‚

Professor Jg is the director of the Natural Language Processing Laboratory and director of the International Joint Research Center of Language Intelligence and Technology at Tianjin Universityã€‚

Recently has been working on numerous frontier AI safety related projectsã€‚

 including a survey of large language model evaluations and many moreã€‚ğŸ˜Šã€‚

Our moderator for this panel will be Brian Zerï¼Œ founder and CEO of Concordia AIã€‚

 Let's give a round applause to welcome our panelistsã€‚ğŸ˜Šï¼ŒHiï¼Œ everyoneã€‚

 Welcome to our second panel of the day on Fronier AI safetyfety evaluationã€‚ I'm reallyã€‚

 really excited about the panel today with the leading experts in the field from Chinaã€‚

 Singapore and the United Statesã€‚ğŸ˜Šï¼ŒOur first theme of discussion would be on the priorities for AI safety evaluationã€‚

Professor Xgï¼Œ this is the first time you're coming on stage todayã€‚ So let me start off with youã€‚

You have published one of the first surveys on evaluation of large language modelss and more recently frontier risk assessment for Chinese area to three groupsã€‚

 The first one is misalignment riskã€‚ So this risk usually have a negative impact on the societyã€‚

 the second label risk is catastrophe misuseã€‚ people maybe use mono to develop bioweaponsã€‚

 something like thisã€‚ and the cell risk is the frontier risk always like many people have different term for this riskã€‚

 Some people call this like frontier risk and also autonomous riskã€‚

 So keep the background and de of this riskã€‚ I think at least the three gaps to bridge if you want to have some control of this riskã€‚

 The first one is is a methodologyã€‚ I thinkã€‚Currentlyã€‚

 the majority work on the frontier risk is on the analysisã€‚

 but we also see some recent work on improving evaluationsã€‚

 but actually there are two type of evaluationsã€‚ The first one is tendency based evaluationã€‚

 and second one is agent based evaluation So for the tendency based evaluationã€‚

 people use some question to ask the language model to see if they have some desire for the more power for more well for more moneyã€‚

 something like like thisã€‚ So if the model have a really you knowã€‚

 very smart can can tell a lie maybe they to cover their real goalsã€‚

 So I think this we still lack of a lot of I mean to the frontier AI riskã€‚ So the first gapã€‚

 second gap is I think will short off data and toolsã€‚

Because most of the current the evaluation actually is kind of a black box evaluationã€‚

 We don't have a data to trigger the model to expose their riskã€‚

 We don't have tools to to open the black box of the models and the last last gapã€‚

 I think it is access because most the front frontier models advanced AI models are developed by large companyã€‚

 So they only be access by a very limited peopleã€‚ I think they maybeï¼Œ you knowã€‚

 we can see a lot of models are closed sourceã€‚ a lot of source anymoreã€‚

 So this means we have no transparency for this modelã€‚

 So this is a very big challenge for evaluation especially for frontier AI riskã€‚

Thank you for the excellent overviewï¼Œ Profess Jgï¼Œ Drã€‚ Heã€‚

 it's an honor to be having this panel with you todayã€‚

 So we are looking forward to hearing your talk in the afternoonï¼Œ but in the meantimeã€‚

 I recently heard that the Singapore has set up a national AI safety Instituteuteã€‚

 and can you share a bit with usã€‚ What are the current priorities for AI safetyfe testing and evaluation at this national AI safety Institute in Singapore or in Singapore more broadlyã€‚

Yeahï¼Œ thanksã€‚ It's a pleasure to be hereã€‚ Good to see you againï¼Œ Brianã€‚ If take a step backã€‚

 Trust and safety is a very big concern in Singapore in generalã€‚

 We have spent our last 59 years as a country building up trustã€‚ There is a high trust in societyã€‚

 and there's a high trust in our political leadershipã€‚

 Trust is also the reason that we have such a strong manufacturing based healthcare base in Singaporeã€‚

 Trust is also why people walk safety at nightã€‚ They had to walk around on the streetsã€‚

 It's also why people there to do transactions digitally onlineã€‚ğŸ˜Šï¼ŒAIï¼Œ of courseã€‚

 adds a lot of complication to trustã€‚ You can have models haveuc seeninate as a lot of the speakers spoke out todayã€‚

 it generates fake newsã€‚ You can't always rely on that right And thereforeã€‚

 when we have come up with our national AI strategy update last Decemberã€‚

 trust becomes a very big part of so we have manyï¼Œ many different initiatives at different levels from a research point of viewã€‚

 we are doing a lot to support fundamental research as well as policy research and to responsible AI as part of AI Singaporeã€‚

 we set up a center for advanced technologies in online safety to look at misinformation and thisinform we have entire cyber security agency looking at AI and security and security of AIã€‚

 And because we have a whole range of effortsï¼Œ we also have set up a digital trust centerã€‚

 which we designate as our AI safety insï¼Œ really to be a national focal point for AI safety efforts looking at evaluations and testing all along the developmentalizedã€‚

Cycle all the way from development to deployment of AI modelsã€‚

 and also a vehicle for us to do international collaborations with other countries so that we improve the science of AI testing and to improve what we do in Singapore and around the worldã€‚

Thank youï¼Œ so we have set out some of the priorities for ASI safety evaluation and let's move on to the life cycle of doing evaluation as some of the speakers have mentionedã€‚

 and I would like to get the views from character Way and Chris on this question so testing could be conducteded in different stages of the cycle including drain trainingã€‚

 pre deploymentment and post deploymentã€‚What do you think are some of the benefits and challenges for contacting evaluation along some of these stages and what are your experiences working with companies in both China and the United Statesã€‚

å¥½ï¼Œè°¢è°¢ã€‚å‘ƒï¼Œé‚£ä¹ˆå‘ƒå¤§æ¨¡å‹å…¶å®å‘ƒè¿›å±•éå¸¸å¿«ã€‚ä½†æ˜¯åŸç†ä¸Šå†³å®šäº†å‘ƒæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ¿€ç†ï¼Œå¯¹å®ƒçš„å®‰å…¨é£é™©ï¼Œå¯¹å®ƒçš„èƒ½åŠ›æ°´å¹³çš„è®¤è¯†ï¼Œä¸èƒ½åšåˆ°ç™¾åˆ†ä¹‹ç™¾éå¸¸å…¨é¢ã€‚å‘ƒï¼Œå°±åƒç”¨å‘ƒæˆ‘ä»¬ç”¨è¿™ä¸ªå‘ƒç«¹ç¯®æ‰“æ°´ä¸€æ ·çš„ï¼Œåˆ°å¤„å€™æ¼æ°´ã€‚

æˆ‘ä»¬è¿™ä¸ªåšçš„å·¥ä½œå¯èƒ½æ¯”å¦‚è¯´æµ‹è¯•å…¶å®æ˜¯å‘ƒæ˜¯å»è¯†åˆ«å“ªé‡Œæœ‰æ¼æ´ã€‚ç„¶åæˆ‘ä»¬å»æŠŠå®ƒè¡¥ä¸Šã€‚å‘ƒï¼Œé‚£ä¹ˆä½†æ˜¯è¿˜æœ‰å¤šå°‘æ¼æ´ï¼Œæˆ‘ä»¬å¯èƒ½ä¸ä¸€å®šèƒ½å¤Ÿæå¾—éå¸¸æ¸…æ¥šã€‚æˆ‘è§‰å¾—ç°åœ¨å‘ƒè¦è¯´è¿™ä¸ªä¼˜å…ˆçº§çš„è¯ã€‚

å¯èƒ½å‘ƒè¿™ä¸ªä¸€æ–¹é¢æˆ‘ä»¬ä¸€å®šè¦å¿«é€Ÿçš„åŠæ—¶çš„å‘ç°ï¼Œåœ¨å„ä¸ªç¯èŠ‚ä¸­æš´éœ²å‡ºæ¥çš„ã€‚æ¯”å¦‚ç ”å‘ç¯èŠ‚ï¼Œè¿˜æœ‰è¿™ä¸ªä½¿ç”¨ç¯èŠ‚æš´éœ²çš„å·²æš´éœ²çš„å„ç§é£é™©å‘ƒï¼ŒåŠæ—¶ä¿®å¤å®ƒã€‚å‘ƒï¼Œä½†æ˜¯è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥è¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“åšåˆ°ï¼Œç›¸å¯¹å®¹æ˜“åšåˆ°çš„ã€‚

åªè¦æˆ‘ä»¬èƒ½å¤ŸçŸ¥é“è¿™ä¸ªé£é™©æ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿå®šã€‚æ„ä»–æµ‹è¯•å®ƒï¼Œæ”¹è¿›å®ƒã€‚é—®é¢˜åœ¨äºæˆ‘ä»¬ä¸çŸ¥é“çš„å¤ªå¤šã€‚å‘ƒï¼Œè¿™ä¸ªå‘ƒæ‰€ä»¥å‘ƒæœ€æˆ‘è§‰å¾—å½“åŠ¡ä¹‹æ€¥æ˜¯éœ€è¦å»ºç«‹ä¸€å¥—æœºåˆ¶ï¼Œå‘ƒï¼Œå»ºç«‹ä¸€å¥—åŠ¨æ€ã€‚å‘ƒæ•æ·çš„è¿™ä¸ªæœºåˆ¶ã€‚

ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ°¸è¿œè·Ÿä¸Šè¿™ä¸ªå‘ƒæ¨¡å‹æŠ€æœ¯æ°´å¹³çš„æå‡ï¼Œä»¥åŠå®ƒçš„é£é™©ã€‚æˆ‘ä»¬èƒ½æ•æ·çš„å‘ç°å®ƒï¼ŒåŒæ—¶èƒ½å¤Ÿè¿…é€Ÿçš„å‘ƒè®©äº§ä¸šç•ŒçŸ¥é“é£é™©åœ¨è¿™é‡Œï¼Œå¹¶ä¸”åœ¨é‚£é‡Œï¼Œç„¶åæ¥æ”¹è¿›å®ƒã€‚æˆ‘è§‰å¾—å‘ƒæµ‹è¯•éå¸¸éå¸¸å…³é”®ã€‚ä½†æ˜¯æµ‹è¯•åªæ˜¯ä¸€ä¸ªç¯èŠ‚ã€‚

æˆ‘éœ€è¦æˆ‘è§‰å¾—éœ€è¦ä»å„ä¸ªç¯èŠ‚ä¸Šæ¥å»ºç«‹ä¸€å¥—æ•æ·æ²»ç†çš„æŠ€æœ¯ç”Ÿæ€å‘ƒï¼Œæ‰èƒ½å¤Ÿå µä½è¿™ä¸ªä¸æ–­æš´éœ²çš„è¿™ä¸ªæ¼æ´ã€‚æ‰€ä»¥å¦åˆ™æˆ‘ä»¬æ°¸è¿œæ˜¯è·Ÿè¿™ä¸ªæœªçŸ¥åœ¨èµ›è·‘ï¼Œå¯èƒ½å¾ˆéš¾æ§åˆ¶å¥½ï¼Œæœ‰é“ç†ï¼Œè°¢è°¢chã€‚First of allã€‚

 thank you for the honor of being here it's a pleasure to join you all you know I would build on some of the colleagues some of the comments that my colleague has made about there's so many unknowns about this that it's very difficult to fully kind of elaborate you know exactly how it should be done at this pointã€‚

 I think like if we have this conversation five years from now or 10 years from nowã€‚

 I think we'll beï¼ŒMuch more mature and evolvedã€‚ But I think you knowï¼Œ the importantã€‚

 there's a couple of things that we need to do in theã€‚

As far as kind of mapping out testing and evaluation throughout the life cycleã€‚

 one of the things that we really need to do as a community both within industry and government and just a broader AI know frontier AI ecosystem is begin to understand better what kinds of evaluations we need to run in which phases of a life cycle so and I would include it's even pretraining right I think we need to have like evaluations ofã€‚

Pretrainï¼Œ you knowï¼Œ like before you scale up your model by 10 Xï¼Œ the data and computeã€‚

 do you feel comfortable that you can do that safelyï¼Œ And you knowã€‚

 what should you be looking for in carrying out that kind of evaluationã€‚Then once you train a modelã€‚

What are the risks that you need to look for immediately after training that model before you implement some of the known mitigation measures that you canã€‚

You knowï¼Œ the obvious kind of risks that we need to start kind of evaluating forï¼Œ I thinkã€‚

 at the base model level would beã€‚As these systems become more capable of generating and executing their own codeã€‚

 for exampleï¼Œ I think we're not quite there yetï¼Œ but things like self expfiltrationã€‚

Systems like that's something that you would definitely want to test for prior to deploymentã€‚

 but then after you know train the model you put in place some mitigationsã€‚

 I think you still want to run a number of evaluations before you actually deploy it to make sure that you feel comfortable that your users can use this safely both your typical user and then also kind of some of the malicious actors that might kind of try and leverage the system for bad purposes the question is like how much testing to do in each of those areas and I think that'sã€‚

One of the things that hopefully over the coming months and next year or twoã€‚

 hopefully there will be a lot more conversation across the Fier AI ecosystem about how much testing we should be doing before we feel comfortable moving to the next phase in the life cycle and I think it's something that we need a lot more discussion aboutã€‚

feå¼º would you like toæˆ‘æƒ³åˆšåˆšå„ä½å˜‰å®¾è®²çš„å·²ç»éå¸¸ç²¾å½©å’Œå…¨é¢ã€‚æˆ‘æƒ³è¡¥å……ä¸¤ä¸ªç‚¹ã€‚ç¬¬ä¸€ä¸ªç‚¹å‘¢å°±æ˜¯è¯´å‘ƒåœ¨å½“å‰è¿™ä¸ªé˜¶æ®µå‘¢ï¼Œå®‰å…¨é—®é¢˜éå¸¸å—é‡è§†ã€‚æˆ‘ä»¬éƒ½è®¤ä¸ºå®ƒæ˜¯å¾ˆé‡è¦çš„é—®é¢˜ã€‚ä½†äº‹å®ä¸Šã€‚

ä¸è®ºæ˜¯å­¦æœ¯ç•Œè¿˜æ˜¯äº§ä¸šç•Œï¼Œæˆ‘ä»¬åœ¨å®‰å…¨ä¸­æŠ•å…¥çš„èµ„æºã€‚æ¯”å¦‚computationï¼Œæ¯”å¦‚è®¡ç®—çš„èµ„æºï¼Œè¿˜æ˜¯ç ”å‘çš„èµ„æºç›¸æ¯”æˆ‘ä»¬å‘å±•å¤§æ¨¡å‹çš„èƒ½åŠ›å’Œäº§ä¸šåº”ç”¨æ¥è®²æ˜¯è¿œè¿œæ»åçš„ã€‚è€Œå¦å¤–ä¸€æ–¹é¢å‘¢ã€‚

äº‹å®ä¸Šæˆ‘ä»¬å¯¹äºæ¨¡å¯¹äºæ•´ä¸ªé€šç”¨äººå·¥æ™ºèƒ½ï¼Œç‰¹åˆ«æ˜¯ç°åœ¨å¤§æ¨¡å‹å®‰å…¨çš„äº†è§£æ˜¯éå¸¸æœ‰é™çš„ã€‚æˆ‘ä»¬çŸ¥é“å­˜åœ¨å¾ˆå¤šå®‰å…¨çš„éšæ‚£ã€‚ä½†æ˜¯æˆ‘ä»¬å¯¹å®‰å…¨è¿™äº›éšæ‚£å¹¶æ²¡æœ‰ä¸€ä¸ªæ·±å…¥çš„è®¤è¯†ï¼Œæ›´é‡è¦çš„æ˜¯ã€‚

æˆ‘ä»¬äº‹å®ä¸Šå¹¶ä¸çŸ¥é“ä¸ºä»€ä¹ˆäº§ç”Ÿè¿™ä¹ˆå¤šå®‰å…¨éšæ‚£ã€‚å°±è¯´æˆ‘ä»¬æ›´å¤šç°åœ¨å¯¹å®‰å…¨é—®é¢˜æ˜¯åœ¨åº”ç”¨è¿‡ç¨‹ä¸­ï¼Œä»ç°è±¡å±‚é¢æ‰€è§‚æµ‹çš„ï¼Œå¯¹èƒŒååŸå› å‘¢æ²¡æœ‰è¿™ä¹ˆå¤šçš„æ”¯æŒã€‚æ‰€ä»¥åŸºäºè¿™ä¸¤ä¸ªè§‚å¯Ÿå‘¢ï¼Œæˆ‘è§‰å¾—æœ‰ä¸¤ä¸ªæ–¹é¢çš„äº‹æƒ…ï¼Œæˆ‘è§‰å¾—ä¸‹é¢è¦åšã€‚

ç¬¬ä¸€ä¸ªåœ¨è¿™é‡Œé¢æˆ‘ä»¬éœ€è¦æŠ•å…¥æ›´å¤šçš„èµ„æºï¼Œéœ€è¦æ›´å¤šçš„å›½é™…çš„åˆä½œã€‚å› ä¸ºå®‰å…¨é—®é¢˜æ˜¯æˆ‘ä»¬å…¨äººç±»æ‰€é¢ä¸´çš„ä¸€ä¸ªå…±åŒçš„é—®é¢˜ã€‚å‘ƒï¼Œè¿™ä¸€ç‚¹æ˜¯éå¸¸é‡è¦çš„ã€‚å‘ƒï¼Œç¬¬äºŒä¸ªæ–¹é¢å‘¢ï¼Œæˆ‘è§‰å¾—ä»ç ”ç©¶ä¸Šã€‚

æˆ‘ç°åœ¨çœŸçš„æ˜¯å‘¼åæˆ‘ä»¬å¯ç ”å‘ƒè¿™ä¸ªå­¦æœ¯ç•Œï¼ŒåŒ…æ‹¬æˆ‘ä»¬çš„äº§ä¸šç•Œèƒ½å¤Ÿæ‰¾å‡ºæ¥ä¸€æ¡èƒ½å¤Ÿé€šå‘æˆ‘ä»¬safety AIå®‰å…¨é€šç”¨äººå·¥æ™ºèƒ½çš„æŠ€æœ¯æ¡†æ¶ã€‚å› ä¸ºç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹ã€‚ä¸è®ºæ˜¯SFTè¿˜æ˜¯RHFæ›´å¤šçš„æ˜¯ä¸€äº›å•ç‚¹æŠ€æœ¯çš„ç ”ç©¶ã€‚

å¾€å¾€è¿˜æ˜¯è¡¥æ¼é‡ç­‰å•ç‚¹æŠ€æœ¯çš„ç ”ç©¶ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¾ˆéš¾ç¡®ä¿¡é è¿™æ ·çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬å°±èƒ½çœŸæ­£é€šå¾€sftçš„ AGIã€‚æ‰€ä»¥åœ¨è¿™é‡Œé¢ï¼Œæˆ‘è§‰å¾—ä»å­¦æœ¯ç•Œä¸ŠçœŸçš„éœ€è¦æ›´å¥½çš„ç ”ç©¶å’Œæ¡†æ¶å’ŒæŠ•å…¥ã€‚è°¢è°¢ã€‚

Thank you everyoneï¼Œ for the insightful comments so farã€‚

 I think I would like to summarize the three points of consensus so far from this panelã€‚

 as well as some of the presentations in the morningã€‚

 One is that there could be significant risk from misuse from malfunctionsã€‚

 from potential loss of control in the futureï¼Œ but there is great uncertainty in these risk in terms of how we conduct testing evaluationã€‚

Secondï¼Œ we need to conduct a full life cycle of AI testingï¼Œ including pre trainingã€‚

 not just after training and after deploymentã€‚And thirdã€‚

 it seems like the level of AIC investment is not enoughã€‚

 and we need to increase that level of investmentã€‚And I think that brings me to some of the challenges for AI evaluation that some of the presentations have touched onã€‚

One thing is methodologyã€‚So in particularï¼Œ I would like to ask Professor Xiong and Professor Cao to comment on thisã€‚

 It seems like the current popular methods of evaluation are automated benchmarks and human expert rat teamingã€‚

 Are there any new methods on the horizon that you think are promising or kind of under exploreplored at the momentã€‚

é‚£ä¸ªå‘ƒæˆ‘è§‰å¾—ä»æŠ€æœ¯ä¸Šæœ‰å‡ ä¸ªæ–¹é¢ï¼Œæˆ‘è§‰å¾—ç°åœ¨å€¼å¾—å…³æ³¨ã€‚åˆšæ‰æˆ‘ä»¬æˆ‘ä»¬ç¡®å®éœ€è¦å»ºç«‹æ›´å¥½çš„æŠ€æœ¯å’Œæ–¹æ³•æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç¬¬ä¸€ä¸ªæŠ€æœ¯å‘¢æˆ‘è§‰å¾—æ˜¯æ™ºèƒ½ä½“çš„æŠ€æœ¯ã€‚å‘ƒï¼Œæ™ºèƒ½ä½“å·²ç»è¢«éªŒè¯æˆä¸ºæ™ºèƒ½ä½“åŠ ä¸Šå·¥å…·è°ƒç”¨ã€‚

åŒ…æ‹¬æ™ºèƒ½ä½“é‡Œé¢ï¼Œå®ƒå¯ä»¥è¿›è¡Œçš„äº¤äº’åæ€ä»»åŠ¡çš„è§„åˆ’ï¼Œå·²ç»æˆä¸ºå¤§æ¨¡å‹ç›®å‰åœ¨è½åœ°ä¸­é‡è¦çš„ä¸€ä¸ªæŠ€æœ¯å•Šã€‚æœ€è¿‘æˆ‘çœ‹åœ¨å­¦æœ¯ä¸Šåœ¨å·¥ä¸šç•Œéƒ½æœ‰å¾ˆå¥½çš„åº”ç”¨ã€‚äº‹å®ä¸Šæ™ºèƒ½ä½“æ‰€æ¼”å‘å‡ºæ¥çš„æŠ€æœ¯ï¼Œå¯¹äºæˆ‘ä»¬è§£å†³ç›®å‰åœ¨å¤§æ¨¡å‹ã€‚

ä¸å…‰æ˜¯åšæ›´å…¨é¢æ›´ç²¾å‡†çš„è¯„æµ‹ï¼Œè¿˜æ˜¯åšä¸€äº›å¤šè½®çš„æ›´æ·±å…¥çš„å•Šï¼Œè¿™è¿™å¯¹æ¨¡å‹é¢†åŸŸç ”ç©¶çš„æ—¶å€™ï¼Œå¤§å®¶çŸ¥é“ä¼ ç»Ÿçš„å®‰å…¨ç ”ç©¶ï¼Œæ¯”å¦‚è¯´è®¡ç®—æœºå®‰å…¨ç ”ç©¶ï¼Œæœ‰å¾ˆå¤šè¿™ä¸ªè®¡ç®—æœºç†è®ºå¯†ç å­¦ç­‰ç­‰ç ”ç©¶ã€‚

ä»–ä»¬éƒ½æ˜¯ä»–ä»¬éƒ½èƒ½æå‡ºä¸€ä¸ªåœ¨è¿™ä¸ªé¢†åŸŸéå¸¸ç¡¬æ ¸çš„ä¸€ä¸ªæ•°å­¦é—®é¢˜ï¼Œä¸€ä¸ªç†è®ºé—®é¢˜ã€‚ä½†ç›®å‰åœ¨å®‰å…¨ç ”ç©¶ä¸­ï¼Œä½ ä¼šå‘ç°ç‰¹åˆ«ç¢ç‰‡åŒ–ç¢ç‰‡åŒ–å°±æ„å‘³ç€åœ¨è¿™é‡Œé¢æˆ‘ä»¬æ²¡æœ‰å¾ˆå¥½çš„ä¸€ä¸ªåŸºç¡€æ²¡æœ‰å¾ˆå¥½çš„ä¸€ä¸ªä½“ç³»ã€‚æˆ‘æƒ³è¿™æ˜¯ã€‚

æˆ‘ä»¬åœ¨è¿™ä¸ªæ—¶å€™ä»è¿™ä¸ªæˆ‘æƒ³ä»å­¦æœ¯ç•Œç”šè‡³åŒ…æ‹¬äº§ä¸šç•Œéœ€è¦å»æ€è€ƒçš„ä¸€ä¸ªé—®é¢˜ï¼Œè°¢è°¢ã€‚okay completely agree Profes opinionã€‚ just added two pointsã€‚

 The first point blueã€‚ So this is the first pointã€‚ The second pointï¼Œ I think I meanã€‚

 in terms of the evaluation as we need to build a lot of dataã€‚ but in terms of the safetyã€‚

 we have lot of safety issuesã€‚ And this used actually all course disciplinaryã€‚

 So if we only rely on our AI communityï¼Œ actuallyï¼Œ we we don't have the experts don't have the expertise don't have the logic to build such a data setã€‚

 So I think we have to collaborate with more communities to you know to address the AI risk issuesã€‚

Thank youã€‚ Doctor He earlier mentioned the importance of building trustã€‚

If companies do their own evaluationï¼Œ it's hard for society to have complete trust in the safety and ethics of AIã€‚

And I think that leads to the question of what is the role of third partyã€‚ I would like Drã€‚

 He and director We to comment on this questionã€‚ What do you think are some of the current challenges of performing the role of third partyã€‚

 For exampleï¼Œ professorfess you mentioned the issue of assessã€‚

 If third party testers and auditors only have black box access to the AI modelsã€‚ thenã€‚

Is insufficient for them to conduct all the AI safety testing that are requiredã€‚

I would love to get your viewsï¼Œ Drã€‚ Heã€‚Maybe I will take the question in the context of sharing our experiences with AI Verã€‚

 which is really what we've been working on and this was and I'll say it in three parts why we worked on AI verify AI verify as a software testing toolkit that the Singapore government introduced two years ago really it came for three reasons One I think there is a very big gap between what the policymakers talk about in very big principles and all the work that everybody here is doing in terms of very good researchã€‚

Rightï¼Œ and also a gap between policyï¼Œ research and what is actually in the hands of industryã€‚

 So AI verify was an attempt to bridge this gap by putting practical toolsã€‚Informed by policyã€‚

 but also trying to get the best of academia into the hands of industryã€‚

 You can come together to build itã€‚ Otherwiseï¼Œ your third parties are somewhere far far awayã€‚

 They are not in part of the ecosystemã€‚third thing we do is that you want your third partiesã€‚

 You want your systems to always be at the frontierï¼Œ rightï¼Œ So last yearï¼Œ we would rather this yearã€‚

 we updated AI verify to include Moonshotï¼Œ which is our geneative AI version of itã€‚

 It's a very start initial productã€‚ brings in benchmarkingï¼Œ brings in red teamingã€‚ But it's a startã€‚

 And then the process through a common platformã€‚ You get the industryã€‚ You get academiaã€‚

 You get third parties all comeã€‚å¥½çš„ï¼Œé‚£ä¹ˆå‘ƒç¬¬ä¸‰æ–¹æœºæ„æ˜¯éå¸¸é‡è¦çš„ã€‚å‘ƒï¼Œé‚£ä¹ˆæˆ‘è§‰å¾—ç¬¬ä¸‰æ–¹æœºæ„çš„å­˜åœ¨å…¶å®æœ‰åŠ©äºæå‡å‘ƒå¤§å®¶å¯¹äºäººå·¥æ™ºèƒ½ï¼Œç‰¹åˆ«æ˜¯å¤§æ¨¡å‹æŠ€æœ¯çš„ä¿¡å¿ƒã€‚å‘ƒã€‚

è€Œæ˜¯è¦å‘Šè¯‰ä»–ä»¬å‘ƒï¼Œè¿™ä¸ªé€šè¿‡ç¬¬ä¸‰æ–¹çš„ä¸€äº›ä¸“ä¸šèƒ½åŠ›å‘Šè¯‰è¿™ä¸ªç ”å‘è€…ã€åº”ç”¨è€…å‘ƒé£é™©åœ¨å“ªé‡Œï¼Ÿåº”è¯¥æ€ä¹ˆå»æ”¹è¿›ï¼Œä¸‹ä¸€æ­¥åº”è¯¥æ€ä¹ˆåŠï¼ŸåŒæ—¶å‘¢ç¬¬ä¸‰æ–¹æœºæ„å¯ä»¥æ‰®æ¼”ä¸€ä¸ªæ¡¥æ¢ä½œç”¨ã€‚

æŠŠä¸åŒçš„è¿™ä¸ªç ”å‘ä¸»ä½“çš„å‘ƒæ”¶é›†åˆ°çš„å®‰å…¨é£é™©æ±‡èšèµ·æ¥ã€‚æµ‹è¯•åŠæ±‡æµ‹è¯•æ•°æ®æ±‡èšèµ·æ¥ã€‚æ–¹æ³•è®ºæ±‡èšèµ·æ¥ï¼Œç ”ç©¶ç ”ç©¶ç»“æœæ±‡èšèµ·æ¥ï¼Œèƒ½å¤Ÿæ›´å¥½çš„èµ‹èƒ½ä»»ä½•ä¸€ä¸ªå•ç‚¹çš„ç ”å‘æœºæ„ã€‚æˆ‘è§‰å¾—å‘ƒç¬¬ä¸‰æ–¹æœºæ„æ‰®æ¼”çš„è§’è‰²å…¶å®æ˜¯éå¸¸ä¸°å¯Œçš„ã€‚

é‚£ä¹ˆæµ‹è¯•æ˜¯éå¸¸é‡è¦çš„æ ¸å¿ƒçš„æ‰‹æ®µï¼Œæˆ‘åˆšæ‰æˆ‘éå¸¸åŒæ„ä¹”è€å¸ˆçš„è¿™ä¸ªè§‚ç‚¹ï¼Œå°±æ˜¯ç°åœ¨å‘ƒè¿™ä¸ªæ•´ä½“ä¸Šäº§ä¸šç•Œå¯¹äºå‘ƒè¿™ä¸ªAIçš„æŠ•èµ„è¿˜æ˜¯éå¸¸æœ‰é™çš„ã€‚é‚£ä¹ˆæˆ‘è§‰å¾—è¿™ä¸ªç¬¬ä¸‰æ–¹æœºæ„å­˜åœ¨å…¶å®æ˜¯æå‡ºã€‚å…¬å…±äº§å“å‘ƒæ¥é™ä½å‘ƒã€‚

æˆ–è€…è¯´åœ¨æœ‰é™çš„è¿™ä¸ªäº§ä¸šæ€»ä½“å®‰å…¨é¢„ç®—çš„æƒ…å†µä¸‹å‘¢ï¼Œæä¾›ä¸€äº›å…¬å…±äº§å“ï¼Œå‘ƒï¼Œè®©å¤§å®¶çš„è¿™ä¸ªç ”å‘å‘ƒè¿™ä¸ªå®‰å…¨æŠ•å…¥å¯èƒ½ä¼šå‘ƒè¿™ä¸ªæˆæœ¬èƒ½å¤Ÿå¯æ§ï¼Œæ°åˆ°å¥½å¤„ã€‚å‘ƒï¼Œæˆ‘è§‰å¾—å’±ä»¬åšè¿™ä¸ªå®‰å…¨æ–¹é¢çš„å·¥ä½œï¼Œå…¶å®è¿˜æ˜¯è¦æœåŠ¡äºæ›´å¥½çš„å‘å±•ã€‚

æ›´å¥½çš„åº”ç”¨ã€‚é‚£ä¹ˆå‘ƒè¿™ä¸ªåœ¨æ§åˆ¶å¥½è¿™ä¸ªå‘ƒå®‰å…¨å‘ƒè¿™ä¸ªæˆæœ¬æŠ•å…¥çš„è¿™ä¸ªåŒæ—¶ï¼Œåœ¨å®‰å…¨ä¿éšœå®‰å…¨çš„å‰æä¸‹æ§åˆ¶å¥½æˆ‘ä»¬çš„è¿™ä¸ªå‰æ²¿æ¨¡å‹ç ”å‘æœºæ„è¦æŠ«éœ²è¿™ä¸ªé£é™©åˆ—è¡¨ã€‚å‘ƒï¼Œè¿™æ ·æ‰èƒ½å±•ç°å‡ºä¸€ç§è´Ÿè´£ä»»çš„è¿™ä¸ªæ€åº¦ã€‚

è¿™æ ·æˆ‘è§‰å¾—ç¬¬ä¸‰æ–¹æœºæ„å’Œä¼ä¸šåˆä½œï¼Œåšè¿™äº›äº‹æƒ…ï¼Œå…¶å®æ˜¯æ‰èƒ½å½¢æˆä¸€ä¸ªé—­ç¯ã€‚æ„Ÿè°¢å‘ƒã€‚ğŸ˜Šï¼ŒI very much agree with the vision that AI safety should be a global public good and some of the AI safety evaluation around the world or know AI safety labs in different cities and countries around the world that we begin to align on a shared vocabulary so that we can actually under without knowing exactly what you mean by red teaming and what we mean by red teamingã€‚

 it's hard to trust and allow for these things to be interoperableã€‚

 And so I think that's like the key first step that needs to happen in the next year or soã€‚

Professor Qiaoã€‚å‘ƒï¼Œæˆ‘æƒ³å„åˆšåˆšå„ä½ä¸“å®¶è®²çš„å·²ç»éå¸¸å…¨é¢äº†ã€‚æˆ‘å°±å¾—æœ€åæƒ³è¯´ä¸€ç‚¹ï¼Œå°±æ˜¯åœ¨å‘¼åº”ä½ çš„ã€‚æˆ‘è§‰å¾—åœ¨è¿™ä¸ªé¢†åŸŸæˆ‘ä»¬å¤ªéœ€è¦å›½é™…åˆä½œäº†ï¼Œéœ€è¦å›½é™…å…±è¯†å›½é™…åˆä½œã€‚

If no final remarksï¼Œ let us give it a final round of applause to this excellent panelã€‚ç°åœ¨è¿œè‘—è®¨è®ºçš„å˜‰å®¾è¯·ç•™æ­¥ã€‚

å…¶ä»–è®ºå›çš„å˜‰å®¾è¯·ä¸Šå°ï¼Œæˆ‘ä»¬ä¼šåœ¨å°ä¸Šè¿›è¡Œä¸€å¼ åˆå½±ã€‚å‘ƒï¼Œç„¶åä¸Šåˆè®ºå›å‘¢å‘Šä¸€æ®µè½ï¼Œä¸‹åˆè®ºå›ä¼šåœ¨ä¸‹åˆ1ç‚¹10åˆ†å¼€å§‹ã€‚

The morning forum has concluded the afternoon forum would begin at Oneï¼Œ10 PMã€‚

See you in the afternoonã€‚Look at the history of inventionsã€‚

 every single major invention has brought hopes and fearsã€‚And across timesã€‚

 including the modern timesï¼Œ or societies have been reshapedã€‚

 completely reshaped by things such as electricity cars and those last few decadesã€‚

 digital technologiesã€‚ It's quite presentã€‚ You knowï¼Œ if you think about 10 yearsã€‚

20 years back and you think about what our digital infrastructure has done to usï¼Œ what phonesã€‚

 for instanceï¼Œ have done to societiesï¼Œ they've completely reshaped societiesã€‚

 And each time those transformations have wrough progress and new dangersã€‚And each timeã€‚

In the new dangersï¼Œ there have been some that have been fantasized and some that we did not seeã€‚

And we do not think that the current transformation that we're witnessingã€‚

Cannot learn from the previous transformationsã€‚So we think that AI can be a driver for a better worldã€‚

 but in itselfï¼Œ it's neither good nor badã€‚ It really depends what weï¼Œ as a societyï¼Œ do of itã€‚

And we tried to look a bit at the evidence of the transformations that AI isã€‚

 is bringing for a better worldã€‚And so I focused on healthï¼Œ because this is what I know mostã€‚

 So one example is AI for reading radioographyyã€‚ So thereã€‚

Are these days AI systems that are approved by regulatory agenciesã€‚

 such as the Food and Drug Administration in the US to do reading ofï¼Œ for instance mammographyã€‚It'sã€‚

 it's important to put this in perspective with a health and an economic rationalã€‚

For this specific problemï¼Œ the rational is that if you detect a cancer earlyï¼Œ it is curableã€‚

Ultrasound imaging is fairly cheapã€‚And the doctor's reading is the bottleneck hereã€‚

 So if we can make widely availableï¼Œ good AI for reading ultrasound imagingã€‚

The hope is to improve survival of cancerï¼Œ lower the health burden of cancerã€‚

 So that's one example of a good rationale of AI and healthã€‚ Nowã€‚

 a more complicated story to tell is around the hospital resource allocationã€‚Hereã€‚

It's a set of digital toolsã€‚Related to early warningï¼Œ related to forecastingã€‚

 things like length of stay predictionï¼Œ all kind of different screenings that all combine each other to restructure the hospitalã€‚

 And if it restructure the hospital well by lowering the burden on either the hospital or the individual's patientã€‚

 it is beneficialã€‚ and so we're seeing evidence in some places that AI can help bringing this benefitã€‚

Nowï¼Œ to take a step back and look at a broader economical perspectiveã€‚

 there have been studies that have studied the impact ofã€‚AIï¼Œ for instanceï¼Œ generative AIã€‚

In the industryã€‚ in here is a study looking at the increase in productivity in a customer service departmentã€‚

 So what we're seeing here is that there is a significant increase in productivity in number of thoughts completed in an hourã€‚

 So this isï¼Œ we think this is what we can extrapolate what AI is specifically Gen AI is going to bringã€‚

Toï¼Œ to the economic sectorã€‚Howeverï¼Œ we want to stress that we need to be mindful of inflated promisesã€‚

The history of technologyï¼Œ and particularly their history of AI is loaded with inflated promisesã€‚

Almost 15 years agoã€‚Maybe 10ã€‚We were betting that self driving cars would be around the cornerã€‚

This did not pan outã€‚ Experts are currentlyã€‚Currently believe that self driving cars will beã€‚

Economically viable in another 10 yearã€‚So theres a big mismatch between what we thought was trueã€‚

10 years agoã€‚ and what we think is true todayã€‚ So I want to draw focus through thisã€‚

 We have promises and dreamsã€‚ Not all of them will come trueã€‚

Those promises associated with a lot of investmentã€‚ You knowï¼Œ the in Gen A Iï¼Œ currentlyã€‚

 the order of magnitude of the investment is in dozensï¼Œ maybe hundreds of billions yearlyã€‚

There is clearly a bubble in AIã€‚ It's important to keep this in mindï¼Œ because it shapesã€‚

Their discourseã€‚And the difficulty is to find the right trade off between overselling and underestimatingã€‚

AI is a gradual evolution in technologyï¼Œ but it brings a long term revolution in everyday day's experienceã€‚

ğŸ˜Šï¼ŒAI techniques are already ubiquitous in our phonesã€‚ For instanceï¼Œ it's already thereã€‚ Howeverã€‚

 history teaches that it takes a long time to settle downã€‚

 to transform a society during the industrial revolutionsï¼Œ their productivity gamesã€‚

Came only 20 years after technology came inã€‚So we must create the best conditions for collective appropriation of this technologyã€‚

This calls for trust across society between the actorsï¼Œ and hence norms and regulationsã€‚

So we did a risk analysisï¼Œ and it's a fairly conventional oneã€‚

 It really looks like things that people have shown beforeã€‚ For instanceï¼Œ Yosshaã€‚

 It's not surprisingã€‚ I want to zoom in on a few things that we think are really importantã€‚

One of them is misinformationã€‚Quality access to information is crucial to Democratic societyã€‚

 It's what shapes the discourse in a Democratic societyã€‚It is threatened by AI for multiple reasonsã€‚

 One of them is AI and digital infrastructures create filter bubblesã€‚

 It's the good old problem of recommend systemã€‚ It's still presentã€‚ It's very importantã€‚

One thing that is newï¼Œ thoughï¼Œ it's not a ruptureï¼Œ is that it's easier to create fake contentã€‚

The consequence of all this isï¼Œ it's disrupting the economics of newsã€‚ Basicallyã€‚

 journalism struggles as a business modelã€‚ This is consequences to our societiesã€‚

Another set of threats that I want to zoom in because we think they're really important are the threats to privacyã€‚

 There are two aspects to thisï¼Œ firstã€‚We're more and more focusing on a model where AI is centralizedã€‚

 You knowï¼Œ I think chat Gï¼Œ for instanceï¼Œ we're interacting with a centralized actorã€‚

 This actor is getting a window in our private dataã€‚ It's scooping up the dataã€‚

 That's our first set of problemã€‚ A second set of related problems is thatã€‚AIs trainã€‚Sorryã€‚

 leak training dataã€‚ Soï¼Œ for instanceã€‚If I were to train a AI system on electronic health recordsã€‚

And release it in the wildã€‚There are good reasons to think that it would be possible to extract some of the private information used to trainã€‚

The AI systemã€‚This coupleles in with copyright questionsã€‚

Another crucial problem is the problem of biasã€‚We don't talk that much about this problemã€‚

 but we're convinced it's crucialã€‚Based AIs discriminateã€‚More and moreã€‚AIã€‚

 all kind of different AIs are part of day to day governanceã€‚

And there are many document cases of harmã€‚ One of themï¼Œ which has been very well documented isã€‚

A scandal around child care benefits in the Netherlandsã€‚This is LED to people becoming poorã€‚

 This is probably LED to people committing suicideã€‚ So there are documented casesã€‚

Of death related to AI hereã€‚The difficult problem is that it'sã€‚Not only a technical problemã€‚

 It's a soci technical problemã€‚To put it in a certain wayï¼Œ not everybody isã€‚

Bthered by a specific kind of bias and as a consequenceã€‚

 all the actors that work on the system do not work on the system the same wayã€‚

 So it's not just a question of the technologyã€‚ It's a question of how the actors work with the technologyã€‚

This is an old problemï¼Œ well documentedã€‚It's still present in the modern technologiesã€‚

 It's well perpetrated in the language modelsã€‚ For instanceï¼Œ with my studentï¼Œ Li Hu Tã€‚

 we were looking at heterogeneity of error of hallucination in large language models and quite easily without even looking at looking for itã€‚

 We could see that L L M the the you knowï¼Œ the Americans or European L L Ms represent much less wellã€‚

 the Southeast Asiansï¼Œ Chinese national Indian nationals than Americans are Europeanã€‚

 This is completely related to the the training corpus that is being usedã€‚Soã€‚

There is nothing new hereï¼Œ but let us not forget about those old problemsã€‚

Another concern we have is about the ever rising footprintã€‚Soï¼Œ everybody talks about howï¼Œ you knowã€‚

 computer is increasing exponentiallyã€‚ We need to put this in perspective with the increase in the available computeã€‚

 So here I'm plotting not only the famousï¼Œ you knowï¼Œ exponential co that you're seeingã€‚

 but also the increase in the largest supercomputerã€‚ And what you see here is that theã€‚

 the power used by AI is actually overcoming the power availableã€‚

 The total computing power available in in the worldã€‚ğŸ˜Šï¼ŒNowï¼Œ this was for training costsã€‚

 I actually think that inference costs are more importantã€‚

 And what we're seeing here is just the same thingã€‚ You knowï¼Œ as GPU become betterã€‚

AI is actually increasingly using power faster than the improvement in GPUsã€‚Nowã€‚

 this doesn't seem sustainable at allã€‚ The economics don't make senseã€‚

 We have a rebound effect that cancels and eats out the hardware gainsã€‚Related to thisã€‚

 our major concern is around the concentration of powerã€‚

 We have an economics where big actors are getting biggerã€‚Because we use more and more dataã€‚

 more and more computeã€‚Related to thisï¼Œ weï¼Œ we have systems where decisions can be automated more and moreã€‚

 As a consequenceï¼Œ thisizes choices theã€‚The team building the decision engine makes a decision for everyoneã€‚

This couples in were the other risksï¼Œ misinformationï¼Œ privacyï¼Œ biasï¼Œ and inflates themã€‚

We think this is the most important prism to haveï¼Œ looking at AIã€‚As a consequenceã€‚

 we think that we need multi stakeholder AIã€‚We believe in open source for thisã€‚

 It enables innovationã€‚ It helps mitigating biasã€‚ We believe the important notion is that of commonsã€‚

 not only of open sourceï¼Œ there are challenges thereã€‚The challenges of liabilitiesã€‚

 And if we don't do our liabilities framework wellã€‚

 we can kill open source as open source has a fragile economic sustainabilityã€‚

 challengesllenges of lack of transparencyã€‚ What data went in a modelã€‚

 And a problem is that when we mean open sourceï¼Œ we typically do not define what we're talking aboutã€‚

 And currently the word open source is used to denote noncommercial open white modelsï¼Œ whichã€‚

Is not very open by open source standardsã€‚So because of all thisã€‚

 we believe there is a need for international governanceã€‚Just like Internetã€‚

 AI goes across borders very easilyã€‚And ideallyï¼Œ we would use training data that span multiple jurisdictions to create more representative AIsã€‚

 You knowï¼Œ avoid those silos that I mentioned beforeã€‚

So we believe we need shared governance to help us build common objectsï¼Œ just likeã€‚

I can iss very useful to have a commonï¼Œ not two fragments of Internetã€‚

So we have proposed a world AI organizationganization made of representative ofã€‚Firstã€‚

 states and interstate organizationsï¼Œ secondï¼Œ researchã€‚

 general interest structures and companies and territoriesã€‚

 And the goals would be to establish standardsï¼Œ including standards of audit and to establish the state of knowledge in AI its impact and to give advice and strategic orientationsã€‚

Thank youã€‚Thank you so muchï¼Œ Gailï¼Œ for the fascinating perspective from the French Commissionã€‚

 I'm sure there's much more to dive into laterã€‚ğŸ˜Šï¼ŒOur next speaker is Dr Rayen Heã€‚

 He is Singapore's chief artificial intelligence officerã€‚

 where he leads a multi stakeakeholder effort to achieve Singapore's strategic AI objectivesã€‚

 including developing and implementing Singapore's national AI strategyã€‚ğŸ˜Šã€‚

He is also concurrently the Singaporean government's deputy chief digital technology officer and a member of the United Nations High level Advisory body on AIã€‚

 Doctorã€‚ Heï¼Œ the say is all yoursã€‚ welcomeã€‚ğŸ˜Šï¼ŒDistinguished guestsï¼Œ ladies and gentlemenã€‚

It is an honor to be here at the World AI conference with its focus on applicationsã€‚

 thoughtful conversations and fundamental optimismã€‚ that technology is a force of goodã€‚ğŸ˜Šã€‚

I thank Ryan from Concordia AI for the opportunity to share on Singapore's approach towards AI governanceã€‚

Every country's approach towards policy is shaped by its unique circumstancesï¼Œ such as its historyã€‚

 as national prioritiesï¼Œ comparative advantages and industrial baseã€‚ Singaporeans no differentã€‚

Neverthelessï¼Œ for AI safety and governanceï¼Œ I think there are four common teams that are broadly applicable across countries because we all start from the same placeã€‚

 And if actually to give half my top so my life is a lot easier nowã€‚

 Let me speak about each of them in turnï¼Œ firstã€‚ğŸ˜Šï¼ŒI think we need to approach AI governance with a spirit of humilityã€‚

As former open AI board memberï¼Œ Helen Toner notesã€‚There is no agreement on what it really means to be intelligentã€‚

Moreoverï¼Œ as many of the speakers this morning saidã€‚

 nobody really understands the inner workings of AI systemsï¼Œ particularly deep neural networksã€‚

Furthermoreï¼Œ AI technologies are rapidly evolvingï¼Œ but we cannot predict with any accuracy or certainty of how AI systems will evolveã€‚

 We do not know if technical methods towards AI safetyã€‚

 such as mechanistic interpretability or formal methods will really workã€‚

 And I see some not nodding in the front field rolesã€‚ Chris is smiling any meã€‚ğŸ˜Šï¼ŒOn the other handã€‚

We also do not know if some of the worst case scenarios will come trueï¼Œ rightã€‚

 AI has been overide before usï¼Œ as mentioned earlierã€‚Henceã€‚

 as many of the speakers at this morning's session sharedã€‚

 there are many known unknowns and unknown unknowns about AIã€‚Henceã€‚

 we should refrain from making assessments with certaintyï¼Œ particularly about the futureã€‚Insteadã€‚

 policymakers need to humlyï¼Œ to humbly adopt a continual learning mindset to reasse the assumptionsã€‚

 to keep up to date with the latest in science and to have a keen sense of technology's impact on societyã€‚

We need to learn from experts from academia and industryï¼Œ many of whom are gathered here todayã€‚

 and we need to learn from each otherã€‚None of us have all the answersã€‚

 and we need to consult widely to poll expertiseã€‚ Conferences like the World AI conference are important opportunities for network for knowledge sharingã€‚

 through which we will improve our shared understanding of AIã€‚ğŸ˜Šã€‚

Humility also means to have the empathy to listen to and learn from nonexpert voicesï¼Œ Cnï¼Œ workersã€‚

 writersï¼Œ artistsï¼Œ youths that are disabled all have important perspectivesã€‚

 Their fears and emotions about AI are realã€‚ They may differ by country and sectorã€‚

 and we need to understand themã€‚I am grateful for the United Nations advisorisory body on AI that they consulted very widely before coming up with its recommendationsã€‚

ğŸ˜Šï¼ŒThis includes my fellow membersï¼Œ Ling Han and Zhang Yu Linghan is here with us todayã€‚

Humility also involves the willingness toï¼Œ to admit that there are key questions about AI that we do not know the answers to so that they can be efforts to uncover answers and so that we do not make decisions based on unfounded assumptionsã€‚

It was in this spirit that we convened the Singapore conference on AI last Decemberï¼Œ whichã€‚

 which brought together global experts from different domains to articulate key questions of AI that if unlocked will lead to the development and deployment of AI for the global goodã€‚

 Some of the delegates at the conference last December are here with us todayã€‚ Professor Don Songã€‚

 Yao Tongï¼Œ Irene and Brianã€‚ğŸ˜Šï¼ŒHumility also includes a recognition that our regulatory responses may be wrongã€‚

All they may be updatingã€‚Nowï¼Œ this does not mean that as governments or policy makesã€‚

 we sit back and just waitã€‚Insteadï¼Œ we can start with the introduction of soft regulations and guidelinesã€‚

 gather feedback from the affected partiesï¼Œ observeer the consequences of the guidelinesã€‚

 amend them if necessary before hard codinging them into regulationsã€‚ğŸ˜Šï¼ŒIn Singaporeã€‚

 we have introduced baseline that safeguards with practical guidance and continuously calibrate our guidance in line with developmentsã€‚

 In 2019ï¼Œ Singapore was the first country to launch a model AI governance framework to provide businesses and consumers with practical guidelines on the responsible use of AIã€‚

ğŸ˜Šï¼ŒThis yearï¼Œ following extensive consultations with the international and industry communitiesã€‚

 we've updated our framework to include generative AIã€‚

We believe that that iterative learning approach is far more sensible and attempting to regulate all of AI's potential harmsã€‚

Soï¼Œ that was humilityã€‚Secondï¼Œ we need to approach AI governance with a sense of perspectiveã€‚

We need to be careful about false dichotomiesã€‚Nothing is really black and white about AI policyã€‚

 For exampleï¼Œ an AI system can be generally usefulï¼Œ but it sometimes says incorrect thingsã€‚

 AI can increase productivityï¼Œ but it can also cause job disruptionsã€‚

 AI can help with climate change mitigationsã€‚ It can also hurt the planet with its high consumption of electricity and powerã€‚

 AI can do for healthcare careã€‚And AI systems can generate deep fixs and scamsã€‚

 but there are also ample examples of AI being deployed for the public goodã€‚ For exampleã€‚

 in Singaporeï¼Œ we use AI to smoothen immigration clearanceï¼Œ Pre hospital timesã€‚

 optim train maintenanceã€‚ Citizens can also access someã€‚

 access some government services using chatbosã€‚So regulation and innovation are also a false dichotomyã€‚

 We need pro innovation regulationsã€‚That allow the beneficial applications to flourish while guarding against the harmful usersã€‚

This weight of balance the sector to sectorã€‚ For exampleã€‚

 the worst case scenarios for self driving vehicles are quite different from cancer diagnosis or chat boardsã€‚

AI also does not exist in a vacuumã€‚ It is part of a technical productï¼Œ which is part of a use caseã€‚

 And it's part of a broader environment that we interact withã€‚Henceã€‚

 it may be useful to think about AI policies in a broader regulatory environmentã€‚In recent yearsã€‚

 Singapore has updated our suite of laws and to safeguard the digital domainã€‚

 including for personal data protection and against misinformation and disinformation that is spread online to better manage cyber risk and egregious content and curb online criminal activitiesã€‚

ğŸ˜Šï¼ŒIn these regulationsï¼Œ the human or the institution remains responsible for the consequences of their decisionsã€‚

 even if these actions were aided by AI systemsã€‚ So that was perspectiveã€‚Thirdã€‚

 I think we need to increase and improve our capabilities to govern AIã€‚

This starts with encouraging many of my fellow policymakers to use AI so that they develop a baseline understanding of the potential and limitations of such technologiesã€‚

In Singaporeï¼Œ we actively promote the use of AI within the Governmentã€‚ Civil servants can access AIã€‚

 enable transcriptionï¼Œ summarization and Lms from their government laptopsã€‚

 AI even helps them draft responses to citizen queriesã€‚ğŸ˜Šã€‚

It is important for ecosystems to have the practicalã€‚

 technical capabilities to develop and regulate AIã€‚

 rather than just talk at the level of abstract principlesã€‚Henceã€‚

 we also encourage the community of more technically inclined government officials to develop their own AI products and toolsã€‚

 We createï¼Œ also create avenues for them to share their learningsã€‚ğŸ˜Šï¼ŒFor exampleï¼Œ I mentionedï¼Œ as Iã€‚

 as I mentioned at this morning's panelï¼Œ our government agencies developed the AI verify minimum viable product in 2022ã€‚

 which provides a practical way for developers to demonstrate that their AI systems measure up to internationally recognized governance principlesã€‚

Last monthï¼Œ we also launched product project Moonshotã€‚

 a challenge to ourselves to extend the AI verify toolkit from traditional AI to geneative AIã€‚

To ensure a pipeline of new tools and new methods to regulate AIã€‚

 we also actively support research in areas such as digital trustï¼Œ online safety and responsible AIã€‚

Researchers work together with government officials on research that is inspired by real needs and see their fruits of their Labour immediately translated into applicationsã€‚

 Our researchers find this very fulfillingã€‚ğŸ˜Šï¼ŒBut capacity building extends beyond tools for policy makers and developersã€‚

For government toolsï¼Œ because government toolsï¼Œ while they may be effectiveã€‚

 they are not a silver bulletã€‚To truly reduce the harmful effects of AIã€‚

 we need to develop a population that is that are confident and discerning users of AIã€‚

This enables them to engage in the digital environmentï¼Œ raises their competenciesã€‚

 skill sets and employabilityã€‚ğŸ˜Šï¼ŒEmpowering our citizens and businesses to reap the benefits of AI is a key pillar of our national AI strategyã€‚

 which we updated last yearã€‚Uplifting their professional competencies is done through broad based and sector specific skills upgradingã€‚

 often in close partnerships with industry partnersã€‚

 as well as pre employment and company LED training effortsã€‚ğŸ˜Šï¼ŒAdditionallyã€‚

 we are also helping all citizens increase their awareness and familiarity of AI through measures such as community roadshowsã€‚

 including in partnership with our public library network and mass media campaignsã€‚Finallyã€‚

 we need to be willing to cooperate internationallyã€‚

 AI is produced in a global supply chain from the production of chipsï¼Œ the training of dataã€‚

 the consumption of models and the development of applicationsã€‚ğŸ˜Šã€‚

And users come from all around the worldã€‚ We live in a borderless digital worldã€‚

 What happens in one country will affect developments everywhereã€‚

And AI is too complex and evolve too quickly for any one companyã€‚

 one country or institution to have a monopoly of wisdom on regulation of AIã€‚

And we all mutually benefit when we collaborate on solutions and share experiences and approachesã€‚ğŸ˜Šã€‚

Furthermoreï¼Œ a fragmentation of AI governance and security frameworks raises compliance costs for businesses and slows down useful AI adoptionã€‚

Thusï¼Œ while it is is natural that every country has slightly different perspectives and positions on AIã€‚

 we need to work hard to overcome our differences and find common groundã€‚Yesterdayã€‚

 I spoke about how countries can work together at the Global AI governance For ministerial roundtableã€‚

 particularly we need to come together to find common problems worth solvingã€‚

 We need to work bilaterallyï¼Œ regionallyï¼Œ multilateterally to facilitate norms and encourage the creation of globalã€‚

 interoperable standards and common tools for AI governanceã€‚At this morning's panelã€‚

 I also shared on how we open source AI verify and launch the AI Ver foundation as a vehicle to tap on the global open source community to crowd in expertise and capabilitiesã€‚

ğŸ˜Šï¼ŒLet me concludeã€‚The challenge of AI safety and governance will continue to evolveã€‚

 but we must sustain our engagement with the hard technical questionsã€‚

 with the policy dilemmas and with each otherã€‚If we collectively adopt the very human traits of a spirit of humilityã€‚

 a sense of perspectiveã€‚A desire to increase our capabilities and our willingness to collaborateã€‚

I am confident that we can achieve the right balance of governing this particular technologyã€‚

And collectively harness AI to serve the public goodã€‚ Thank youã€‚Thank you so muchï¼Œ Doctorã€‚ Herã€‚

 for your inspiring recommendationsã€‚ğŸ˜Šï¼ŒI'm pleased to now welcome Professor Zhang Linghan from the Yuhach University of Political Science and Lawã€‚

ğŸ˜Šï¼ŒShe has extensive experience advising Chinese legislation on algorithm regulationã€‚

 platform governanceï¼Œ data security and AIã€‚ She is currently a member of several advisory committees including the ICT Committee of the Ministry of Industry and Information Technologyã€‚

 and the Cybersecurï¼Œ Legal Advisory Committee of the Ministry of Public Securityã€‚

 Professor Zhang is also a member of the Uã€‚ N High-level Advisory body on AIã€‚ Professor Zangã€‚

 I'll hand it over to youã€‚ğŸ˜Šï¼Œå¤§å®¶å¥½ï¼Œéå¸¸å¼€å¿ƒå‘¢ä»Šå¤©èƒ½ç”¨è¿™ä¸ªä¸­æ–‡å‘ƒè·Ÿå¤§å®¶ä»‹ç»æˆ‘æœ€è¿‘æ–°å†™çš„ä¸€ç¯‡è®ºæ–‡ã€‚é‚£ä¹ˆé¢˜ç›®å‘¢æ˜¯åŸºäºé£é™©åˆ°åŸºäºä»·å€¼æ¢ç´¢ä¸­å›½äººå·¥æ™ºèƒ½çš„æ²»ç†çš„æ–¹æ¡ˆã€‚ğŸ˜Šã€‚

é‚£åœ¨è¿™ä¸ªå®‰å…¨çš„è®®é¢˜ä¸Šå»è®¨è®ºäººå·¥æ™ºèƒ½ã€‚å®é™…ä¸Šæˆ‘ä»¬éƒ½æœ‰è¿™æ ·çš„é—®é¢˜ï¼Œå°±æ˜¯å®‰å…¨æ²»ç†çš„æ¡†é£é™©æ²»ç†çš„æ¡†æ¶ã€‚æ˜¯å¦æ˜¯äººå·¥æ™ºèƒ½æ²»ç†çš„æœ€ä½³æ–¹æ¡ˆï¼Ÿæˆ‘ä»¬ç›®å‰æ‰€æœ‰é£é™©æ²»ç†çš„æ‰‹æ®µå’Œæªæ–½ï¼Œæ˜¯å¦èƒ½å¤Ÿå®Œå…¨åº”å¯¹äººå·¥æ™ºèƒ½ç»™ç¤¾ä¼šå¸¦æ¥çš„å½±å“ã€‚

é‚£ä¹ˆå½“æˆ‘ä»¬å»è¿½æ±‚å®‰å…¨çš„æ—¶å€™ï¼Œå¤šå®‰å…¨æ‰ç®—çœŸçš„å®‰å…¨ã€‚é‚£ä¹ˆé¦–å…ˆå‘å¤§å®¶ä»‹ç»æˆ‘çš„è§‚ç‚¹ã€‚é‚£ä¹ˆæˆ‘é¦–å…ˆè®¤ä¸ºå‘¢ï¼Œåœ¨é£é™©æ²»ç†çš„è¿‡ç¨‹å½“ä¸­ï¼Œä¸ç®¡æ˜¯é£é™©çš„è¯†åˆ«è¿˜æ˜¯é£é™©çš„åº”å¯¹ã€‚

éƒ½æœ‰ä¸€äº›æ— æ³•åº”å¯¹äººå·¥æ™ºèƒ½ç»™ç¤¾ä¼šå¸¦æ¥çš„å…¨æ–¹ä½å¤šç»´åº¦å’Œé¢ è¦†æ€§çš„å½±å“ã€‚é£é™©çš„æ²»ç†ç†å¿µå’Œæ²»ç†æ‰‹æ®µï¼Œæˆ‘ä»¬ä»ç„¶åº”è¯¥åšæŒï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨é£é™©æ²»ç†ä¹‹ä¸Šï¼Œåº”è¯¥è¶…è¶Šé£é™©æ²»ç†ï¼Œé‡‡å–åŸºäºä»·å€¼çš„æ²»ç†æ¡†æ¶ã€‚é‚£ä¹ˆä¹Ÿè¯·å¤§å®¶å¤šå¤šæ‰¹è¯„æŒ‡æ­£ã€‚

é‚£ä¹ˆä¸‹ä¸€å¼ å›¾å‘¢æ˜¯æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç›®å‰åŸºäºé£é™©çš„æ²»ç†å‘¢ï¼Œå·²ç»æˆä¸ºå…¨çƒäººå·¥æ™ºèƒ½æ²»ç†çš„å…±åŒä¸»é¢˜ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªå›¾ä¸Šé¢å‘¢ï¼Œåˆ—åˆ°äº†ä¸å…‰æ˜¯å¾ˆå¤šå›½é™…ç»„ç»‡ã€‚

éƒ½æŠŠé£é™©æ²»ç†ä½œä¸ºäº†ä¸ç®¡ä»–ä»¬çš„å®£è¨€è¿˜æ˜¯æŒ‡å¯¼æ„è§çš„è¿™æ ·ä¸€ä¸ªåŸºç¡€çš„é€»è¾‘æ¡†æ¶ã€‚åŒæ—¶ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼ŒåŸºäºé£é™©çš„æ²»ç†ï¼Œä¹Ÿè¢«ä¸–ç•Œå¾ˆå¤šç«‹æ³•æ‰€é‡‡çº³ã€‚é‚£ä¹ˆä¸‹é¢å¸¦æ¥çš„é—®é¢˜å°±æ˜¯ç”±äºå¤§å®¶éƒ½æ˜¯ä¸“å®¶ï¼Œæˆ‘åœ¨è¿™é‡Œå°±ä¸å¤šä»‹ç»äº†ã€‚

ç°æœ‰çš„æ¡†æ¶å¯¹äºé£é™©çš„è¯†åˆ«å’Œåˆ†ç±»å‡†ç¡®å—ï¼Ÿé‚£ä¹ˆåœ¨è¿™å¼ PPTä¸Šï¼Œæˆ‘åˆ—å‡ºäº†ç›®å‰æˆ‘ä»¬ç»å¸¸æåˆ°çš„4ä¸ªé£é™©æ²»ç†çš„æ¡†æ¶ã€‚é‚£ä¹ˆä»–ä»¬éƒ½æœ‰ä¸åŒçš„åˆ†ç±»çš„æ–¹æ³•ã€‚æ¯”å¦‚è¯´åœ¨æˆ‘å‚åŠ çš„å‘ƒè”åˆå›½çš„è¿™ä¸ªä¸­æœŸæŠ¥å‘Šå½“ä¸­ã€‚

æˆ‘ä»¬æŠŠé£é™©åˆ†ä¸ºæŠ€æœ¯æ€§é£é™©ï¼Œç¤¾ä¼šæ€§é£é™©ã€‚æ ¹æ®å®ƒå½±å“çš„å±‚é¢å»è¿›è¡Œåˆ†ç±»ã€‚é‚£ä¹ˆåœ¨æ¬§ç›Ÿçš„AIXå½“ä¸­å‘¢ï¼ŒæŠŠé£é™©æŒ‰ç…§å½±å“çš„èŒƒå›´åˆ†ä¸ºä¸å¯æ¥å—çš„ä»¥åŠé«˜ä¸­ä½çš„é£é™©ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¾å›½å•†åŠ¡éƒ¨çš„è¿™ä¸ªå›½å®¶æ ‡å‡†ç ”ç©¶é™¢é¢å¸ƒçš„é£é™©å‘¢ã€‚

åˆ™æ˜¯æ ¹æ®é£é™©çš„æ¥æºå’Œæˆå› ã€‚é‚£ä¹ˆåœ¨å®é™…ä¸Šåº”å¯¹äººå·¥æ™ºèƒ½çš„é£é™©å½“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾ˆå¤šå­˜åœ¨çš„é—®é¢˜ã€‚æˆ‘ä»¬å°±ä»¥ç¾å›½nå‘å¸ƒçš„è¿™ä¸ªé£é™©æŒ‡å—ä½œä¸ºæŠ¥å‘Šã€‚ç¬¬ä¸€ä¸ªå°±æ˜¯å¤§å®¶éƒ½çŸ¥é“ï¼Œåœ¨é£é™©çš„åˆ¤æ–­å½“ä¸­ï¼Œå…¶å®è•´å«äº†å¾ˆå¤šçš„ä»·å€¼ã€‚è€ƒé‡ã€‚

ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬è¯´é£é™©å®ƒå¹¶ä¸æ˜¯ä¸€ä¸ªçº¯ç²¹çš„ç§‘å­¦æ¦‚å¿µï¼Œè€Œæ˜¯ä¸€ä¸ªè§„èŒƒæ€§çš„æ¦‚å¿µã€‚æˆ‘ä»¬çœ‹åˆ°ï¼Œæ¯”å¦‚è¯´missä¼¼çš„è¿™ä¸ªæ¡†æ¶å½“ä¸­ï¼ŒæŠŠé£é™©åˆ†ä¸ºæŠ€æœ¯æ€§é£é™©å’ŒæŠ€æœ¯ç¤¾ä¼šé£é™©ã€‚ä½†æ˜¯ã€‚å¤§å®¶å¯èƒ½æ²¡æœ‰æ³¨æ„åˆ°çš„æ˜¯ï¼ŒæŠŠå“ªäº›é£é™©å½’ç±»ä¸ºæŠ€æœ¯é£é™©ã€‚

æœ¬èº«å°±è•´å«äº†ä»·å€¼çš„åˆ¤æ–­ã€‚é‚£ä¹ˆå…¶æ¬¡å‘¢é£é™©åˆ†ç±»ä¸€ä¸ªå¾ˆé‡è¦çš„é—®é¢˜æ˜¯åœ¨æˆ‘ä»¬çœ‹åˆ°çš„å‰é¢è¿™å¼ PPTå½“ä¸­ï¼Œå¤§éƒ¨åˆ†çš„é£é™©åˆ†ç±»éƒ½æŠŠéšç§ä¾µå®³ã€æ­§è§†ç­‰ç­‰ã€‚è¿™ä¸€ç±»å¯¹äººçš„æƒåˆ©çš„ä¾µå®³ï¼Œå½“åšæ˜¯äººå·¥æ™ºèƒ½çš„é£é™©ã€‚

ç„¶è€Œæˆ‘ä»¬è¦çœ‹åˆ°çš„æ˜¯è¿™ä¸€ç±»çš„é£é™©ï¼Œå¹¶ä¸åƒæˆ‘ä»¬ä¸€ç›´ç†Ÿæ‚‰çš„é£é™©çš„æ²»ç†ã€‚æ¯”å¦‚è¯´æ±½è½¦å‘ç”Ÿäº‹æ•…çš„æ¦‚ç‡ã€‚æ¯”å¦‚è¯´é£Ÿå“ä¸å®‰å…¨çš„æ¦‚ç‡ä¸€æ ·ï¼Œæ¯”è¾ƒå®¹æ˜“é‡åŒ–å’Œè®¡ç®—ï¼Œè¿™ä¸€ç±»ç±»çš„é£é™©æ˜¯éå¸¸éš¾ä»¥å»é‡åŒ–å’Œè®¡ç®—çš„ã€‚

æˆ‘ä»¬å¯¹äºé£é™©æ²»ç†çš„åŸºæœ¬æ€è·¯æ˜¯OKæˆ‘è¦ç®—ä¸€ä¸‹ä½ çš„æŸå®³çš„å¤§å°ä»¥åŠæŸå®³å‘ç”Ÿçš„æ¦‚ç‡ï¼Œæœ€åçœ‹çœ‹æˆ‘å¾—åˆ°çš„æ”¶ç›Šè·Ÿå®ƒèƒ½ä¸èƒ½æˆæ­£æ¯”ï¼Œè¿›è€Œé‡‡å–æªæ–½ã€‚å¯æ˜¯å¦‚æœæˆ‘ä»¬äººå·¥æ—¶ä»£é¢ä¸´çš„å¾ˆå¤šé£é™©éƒ½æ˜¯éš¾ä»¥é‡åŒ–çš„é£é™©çš„è¯ã€‚

é‚£ä¹ˆè¿™ç§æ²»ç†æ–¹å¼å¯èƒ½å°±é¢ä¸´ç€å›°éš¾ã€‚é‚£ä¹ˆç¬¬ä¸‰ä¸ªå‘¢æ˜¯æˆ‘ä»¬å†å›æ¥çœ‹è¿™å¼ PPTï¼Œå¯ä»¥çœ‹åˆ°å¤§éƒ¨åˆ†çš„é£é™©åˆ†ç±»è¯†åˆ«å½“ä¸­éƒ½æŠŠå¤±ä¸šé—®é¢˜å½“æˆæ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦çš„é£é™©ã€‚é‚£ä¹ˆä»¥åŠæŠŠæ²¡æœ‰åŠæ³•ç»™ç°è¡Œçš„äººå·¥æ™ºèƒ½æœåŠ¡æä¾›è€…å»è¿½è´£ã€‚

å½“æˆæ˜¯äººå·¥æ™ºèƒ½çš„é£é™©ã€‚å¯æ˜¯æˆ‘ä»¬çŸ¥é“é£é™©çš„ä¸€ä¸ªé‡è¦ç‰¹å¾ï¼Œå°±æ˜¯ä¸ç¡®å®šæ€§ã€‚è¿™ä¸€ç±»æˆ‘ä»¬åˆšæ‰è¯´åˆ°çš„å½±å“çœŸçš„æ˜¯ä¸ç¡®å®šçš„å—ï¼Ÿäººå·¥æ™ºèƒ½å¿…ç„¶ä¼šå¸¦æ¥å¤§è§„æ¨¡çš„å¤±ä¸šå’ŒåŠ³åŠ¨æ›¿ä»£ï¼Œä¹Ÿå¿…ç„¶ä¸èƒ½é€‚åº”ä¼ ç»Ÿçš„æ³•å¾‹æ¡†æ¶ã€‚ä¸å…¶è¯´å®ƒæ˜¯ä¸€ç§é£é™©ã€‚

æˆ‘æ›´æ„¿æ„è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªç¤¾ä¼šå½±å“ï¼Œç¤¾ä¼šå˜é©å¸¦æ¥çš„å¿…ç„¶å½±å“ã€‚é‚£ä¹ˆæ›´é‡è¦çš„æ˜¯ï¼Œåˆšæ‰å¾ˆå¤šä¸“å®¶ä¹Ÿæåˆ°äº†ï¼Œäººå·¥æ™ºèƒ½çš„é£é™©ä¸åŒäºä»¥å¾€ä»»åŒä»»ä½•ä¸€ç§é£é™©ã€‚å…¶é‡è¦åŸå› å°±æ˜¯åœ¨äºå¤§å®¶éƒ½è¯´ä¸å‡ºæ¥ä»–æœªæ¥çš„é£é™©å…·ä½“æ˜¯ä»€ä¹ˆã€‚

è¿™ç§uncertyå’Œunpredictableå¤§å®¶éƒ½æ˜¯åœ¨åå¤çš„æåŠã€‚é‚£ä¹ˆå¯èƒ½æˆ‘ä»¬æåˆ°çš„äººå·¥æ™ºèƒ½çš„è¿™ç§è‡ªæˆ‘å¤åˆ¶ä¸è‡ªæˆ‘å®Œå–„çš„èƒ½åŠ›å•Šï¼Œå°±æ˜¯æˆ‘ä»¬å°†æ¥æ²¡æœ‰é‡è§åˆ°æˆ–è€…ä»æ¥æ²¡æœ‰å¤„ç†è¿‡çš„é£é™©ã€‚

ä½†æ˜¯æˆ‘ä»¬ç›®å‰å¹¶ä¸çŸ¥é“ä»–ä»€ä¹ˆæ—¶å€™ä¼šåˆ°æ¥ï¼Œä»¥åŠæˆ‘ä»¬å¯ä»¥çœ‹åˆ°äººå·¥æ™ºèƒ½è¿™ç§å¼ºå¤§çš„æŠ€æœ¯é€šè¿‡å¼€æºå¯ä»¥è¢«å¹¿æ³›å’Œå®¹æ˜“çš„è·å¾—ï¼Œè¿™ä¹Ÿä½¿å¾—é£é™©çš„æ¥æºã€‚å¤§å¤§çš„è¢«æ‰©æ•£äº†ã€‚é‚£ä¹ˆç¬¬äºŒä¸ªé—®é¢˜å°±æ˜¯æˆ‘ä»¬å¯ä»¥çœ‹åˆ°é£é™©æ²»ç†çš„æªæ–½å‘ƒã€‚

æœ‰äº‹å‰äº‹ä¸­å’Œäº‹ååˆ†åˆ«æ˜¯é£é™©é¢„é˜²ã€é£é™©ç¼“è§£ä¸äº‹åæ¶ˆé™¤ã€‚å¦‚æœæˆ‘ä»¬åˆ—å‡ºå‡ ç§å…¸å‹çš„é£é™©æ²»ç†æªæ–½ï¼Œä¸€èˆ¬æ˜¯äº‹å‰æˆ‘è¦å¯¹é£é™©è¿›è¡Œè¯„ä¼°ï¼Œè¿›è€ŒæŠŠå®ƒè¿›è¡Œåˆ†ç±»åˆ†çº§ï¼Œé‡‡å–å’Œé£é™©ç¨‹åº¦ç›¸é€‚åº”çš„æ²»ç†æªæ–½ã€‚

é‚£ä¹ˆè¿™äº›é£é™©åº”å¯¹æªæ–½å’Œäººå·¥æ™ºèƒ½æ²»ç†çš„ç›®æ ‡æ‰‹æ®µå…¶å®ä¹Ÿå­˜åœ¨ç€é”™ä½ã€‚é¦–å…ˆæˆ‘ä»¬æ¥çœ‹äººå·¥æ™ºèƒ½æ²»ç†çš„ç†å¿µã€‚æ€»ä½“æ¥è¯´ï¼Œé£é™©æ²»ç†çš„ç†å¿µå…¶å®æ˜¯æœ‰ä¸€ä¸ªä¿®æ­£ä¸»ä¹‰çš„å†…æ ¸ã€‚æ‰€è°“çš„ä¿®æ­£ä¸»ä¹‰æ˜¯è¯´ã€‚åŸæ¥çš„é£é™©æ²»ç†æ˜¯è¯´ã€‚

å¦‚æœè¿™ä¸ªæŠ€æœ¯å­˜åœ¨ç¼ºé™·å’Œéšœç¢ã€‚åœ¨ä»–æ²¡æœ‰å¾—åˆ°å®Œå…¨ä¿®å¤ï¼Œå¹¶ä¸”æ˜¯å¯ä¿¡ä»»ä¹‹å‰ï¼Œæ˜¯ä¸åº”è¯¥ç›´æ¥åº”ç”¨äºå®è·µæ´»åŠ¨çš„ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·å»å¤„ç†å’Œæ‰©æ•£å¸¦æ¥çš„é£é™©ï¼Œè¿™æ ·å»å¤„ç†å¤§æµè¡Œç—…å¸¦æ¥çš„é£é™©ã€‚

ä½†æ˜¯æˆ‘ä»¬æ²¡æœ‰åŠæ³•æŠŠå®ƒå’Œäººå·¥æ™ºèƒ½çš„é£é™©ç›¸æå¹¶è®ºï¼Œå¹¶ä¸”è¿™æ ·å¤„ç†ã€‚äººå·¥æ™ºèƒ½æŠ€æœ¯çš„åº”ç”¨å·²ç»æˆä¸ºå¿…ç„¶è¶‹åŠ¿ã€‚é‚£ä¹ˆè¿™ç§è¶‹åŠ¿æ˜¯è°éƒ½ä¸å¯ä»¥é¿å…çš„ã€‚æˆ‘ä»¬æ²¡æœ‰åŠæ³•è¯´åœ¨å®Œå…¨æ¶ˆé™¤äº†æˆ–è€…ç¡®ä¿¡äººå·¥æ™ºèƒ½æ²¡æœ‰é£é™©ä¹‹åæ‰æ¥ç»§ç»­ä½¿ç”¨ã€‚

é‚£ä¹ˆä»äººå·¥æ™ºèƒ½æ²»ç†çš„å·¥å…·çš„å±‚é¢ï¼Œé‚£ä¹ˆæˆ‘ä»¬éƒ½çŸ¥é“é£é™©æ²»ç†å½“ä¸­æœ€é‡è¦çš„å°±æ˜¯äº‹å…ˆçš„è¯„ä¼°å’Œé¢„æµ‹ã€‚å¯æ˜¯æˆ‘ä»¬æœ‰ä¸ªé‡è¦çš„é—®é¢˜ï¼Œå°±æ˜¯å½“æˆ‘ä»¬æ²¡æœ‰äººå·¥æ™ºèƒ½åº”ç”¨å’ŒæŠ€æœ¯è¶³å¤Ÿæ·±å…¥çš„åœ¨ç¤¾ä¼šå½“ä¸­å¹¿æ³›ä½¿ç”¨ã€‚

æˆ‘ä»¬ä¹Ÿå°±æ²¡æœ‰åŠæ³•å»ç¡®åˆ‡çš„äº†è§£å“ªäº›é£é™©å°†ä¼šäº§ç”Ÿï¼Œå¹¶ä¸”å…·ä½“çš„ç¨‹åº¦æ˜¯ä»€ä¹ˆæ ·ã€‚æ›´é‘è®ºï¼Œæˆ‘ä»¬è¿˜æœ‰å¾ˆå¤šæˆ‘ä»¬è®¤ä¸ºä¸å¯é¢„è§çš„é£é™©ã€‚é‚£å¦‚æœæ˜¯è¿™æ ·çš„è¯ï¼Œæˆ‘ä»¬åˆå¦‚ä½•å»è¿›è¡Œäº‹å…ˆçš„å‘ƒè¯†åˆ«è¯„ä¼°å’Œç›‘æµ‹ä½“ç³»å‘¢ï¼Ÿ

é‚£ä¹ˆç›¸ä¿¡å‘¢ä¸€éƒ¨åˆ†ä¸èƒ½é¢„æµ‹çš„é£é™©æ˜¯ä¸èƒ½è½åˆ°è¿™æ ·çš„æ²»ç†å·¥å…·çš„æ²»ç†èŒƒå›´å†…ã€‚é‚£ä¹ˆå¦å¤–è¿˜æœ‰å°±æ˜¯äººå·¥æ™ºèƒ½æ²»ç†çš„å‰æå•Šï¼Œå¦‚æœè¯´äººå·¥æ™ºèƒ½æ²»ç†çš„å¾ˆå¤šé£é™©æ²¡æœ‰åŠæ³•è¢«é‡åŒ–çš„è¯ï¼Œå°±æ²¡æœ‰åŠæ³•è¢«çº³å…¥åˆ°æˆæœ¬æ”¶ç›Šçš„è®¡ç®—å½“ä¸­ã€‚

æˆ‘ä»¬ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼Œå¾ˆå¤šéšç§çš„è¿™ç§æ•°æ®æ³„éœ²çš„èµ”å¿ï¼Œæ€»ä½“æ¥è¯´ï¼Œæ•°é‡å¾ˆé«˜ï¼Œä¾µæƒäººéš¾ä»¥æ‰¿å—ã€‚ä½†æ˜¯å—å®³è€…ä¸ªä½“æ‹¿åˆ°çš„ä¾µæƒè´¹ç”¨å¯èƒ½åªæœ‰å‡ ç¾åˆ†ã€‚é‚£ä¹ˆè¿™ç§éš¾ä»¥è¢«ä¸ªä½“æ•‘æµçš„é£é™©ï¼Œå®é™…ä¸Šä¹Ÿä½¿å¾—æˆæœ¬æ”¶ç›Šæ›´åŠ å›°éš¾ã€‚

é‚£ä¹ˆåœ¨è¿™é‡Œæˆ‘æƒ³ç”¨å‰©ä¸‹çš„æ—¶é—´ç®€å•ä»‹ç»ä¸€ä¸‹ï¼Œæˆ‘è®¤ä¸ºåœ¨ä¸­å›½å…¶å®å·²ç»é€æ­¥å‘å±•å‡ºäº†ä¿®æ­£äººå·¥æ™ºèƒ½é£é™©æ²»ç†çš„è¿™æ ·ä¸€ä¸ªè·¯å¾„ã€‚å¹¶ä¸”éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸­å›½çš„äººå·¥æ™ºèƒ½æ²»ç†æ­£åœ¨æœ¬åœŸåŒ–ã€‚

æˆ‘ä»¬å¯ä»¥æŠŠå®ƒåˆ†ä¸ºæ¢ç´¢é˜¶æ®µå®šå‘é˜¶æ®µå’Œç³»ç»Ÿé˜¶æˆé˜¶æ®µï¼Œæ¢ç´¢é˜¶æ®µå‘¢ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ˜¯ä»15å¹´ä¸€ç›´åˆ°å—¯2022å¹´è¿™å‡ å¹´çš„æ—¶é—´ã€‚æˆ‘ä»¬å‘å±•å‡ºäº†ã€‚å¾ˆå¤šçš„äººå·¥æ™ºèƒ½ã€ä¸­å›½æœ¬åœŸçš„æ²»ç†æ‰‹æ®µã€‚æ¯”å¦‚è¯´åœ¨é£é™©è®¤çŸ¥å±‚é¢ã€‚

å®é™…ä¸Šæˆ‘ä»¬æ˜¯æœ‰å¾ˆå¤šçš„å…±æ€§éƒ¨åˆ†ã€‚ä»2015å¹´ã€16å¹´å¼€å§‹ï¼Œä¸€ç›´åˆ°2020å¹´ä¹‹é—´ï¼Œæˆ‘ä»¬æœ‰æ–°ä¸€ä»£äººå·¥æ™ºèƒ½ä¼¦ç†è§„èŒƒå‘ƒï¼Œç®—æ³•æ¨èç®¡ç†è§„å®šåŒ…æ‹¬ä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ï¼Œå®é™…ä¸Šéƒ½é‡‡å–äº†é£é™©åˆ†çº§åˆ†ç±»å’Œé˜²æ§çš„è¿™æ ·ä¸€ä¸ªè·¯çº¿ã€‚

åœ¨æ²»ç†æ‰‹æ®µä¸Šå‘¢ï¼Œå’Œå›½é™…ä¸Šå¯¹æ¥çš„ä¸€äº›å…±æ€§çš„éƒ¨åˆ†ï¼ŒåŒ…æ‹¬ä¸ªäººä¿¡æ¯å½±å“ã€ä¿æŠ¤ã€ä¸ªäººä¿¡æ¯ä¿æŠ¤å½±å“è¯„ä¼°ä»¥åŠç®—æ³•å½±å“è¯„ä¼°ã€‚ä½†æ˜¯ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå‘å±•å‡ºæ¥ä¸€äº›ä¸­å›½è‡ªå·±çš„ç‰¹è‰²éƒ¨åˆ†ã€‚æ¯”å¦‚è¯´åœ¨å‰ä¸¤ä¸ªé˜¶æ®µã€‚

å§‹ç»ˆåœ¨å›½å†…çš„äººå·¥æ™ºèƒ½ç›¸å…³çš„ç«‹æ³•å½“ä¸­ï¼Œéƒ½æŠŠå‘å±•ä¸å®‰å…¨ä½œä¸ºæœ€é‡è¦çš„å¹³è¡¡çš„ä¸€ç§ä»·å€¼è§‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å§‹ç»ˆç§‰æŒç€ä¸­å›½æ‰€ç‰¹æœ‰çš„å›½å®¶æ€»ä½“å®‰å…¨è§‚çš„ç†å¿µå»è¿›è¡Œäººå·¥æ™ºèƒ½çš„å®‰å…¨æ²»ç†ã€‚

é‚£ä¹ˆå³ä½¿åœ¨ç½‘ç»œä¿¡æ¯é¢†åŸŸè¿™æ ·ä¸€ä¸ªæ¯”è¾ƒç‰¹æ®Šçš„é¢†åŸŸï¼Œæˆ‘ä»¬ä¹Ÿé‡‡å–äº†ç½‘ç»œä¿¡æ¯ç”Ÿæ€å®‰å…¨çš„è¿™æ ·ä¸€ä¸ªæ¦‚å¿µã€‚åŒæ—¶ï¼Œåœ¨ç®—æ³•æ²»ç†å’Œæ·±åº¦åˆæˆæ²»ç†çš„è¿‡ç¨‹å½“ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ˜ç¡®çš„ä»·å€¼æ’åºã€‚

æ˜¯ä¿¡æ¯å†…å®¹å®‰å…¨ã€æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤ä»¥åŠå¸‚åœºç«äº‰ç§©åºã€‚é‚£ä¹ˆåœ¨æ²»ç†æ‰‹æ®µæ–¹é¢ï¼Œå…¶å®ä¸­å›½ä¹Ÿæœ‰å¾ˆå¤šè‡ªå·±çš„ç‰¹è‰²ã€‚æ¯”å¦‚è¯´åˆæ­¥æ¢ç´¢ç›¸å…³çš„ç®—æ³•å¤‡æ¡ˆã€‚é‚£ä¹ˆå°¤å…¶åœ¨è¿™é‡Œæé†’å¤§å®¶è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¹¶ä¸æ˜¯ä¸€ä¸ªå‡†å…¥æˆ–è€…è®¤è¯çš„åˆ¶åº¦ã€‚

è€Œæ˜¯ä¸€ä¸ªä¿¡æ¯å¤‡æ¡ˆå’Œé‡‡é›†çš„åˆ¶åº¦ã€‚é‚£ä¹ˆåŒæ—¶å¤§å®¶å¦‚æœæ„Ÿå…´è¶£å¯ä»¥å…³æ³¨åˆ°ï¼Œåœ¨2021å¹´çš„ç®—æ³•æ¨èç®¡ç†è§„å®šå½“ä¸­ï¼Œéå¸¸æœ‰ç‰¹è‰²çš„æå‡ºäº†æœªæˆå¹´äººçš„é˜²å°˜è¿·åˆ¶åº¦ï¼Œè€å¹´äººå’ŒåŠ³åŠ¨è€…çš„æƒåˆ©ä¿æŠ¤åˆ¶åº¦ï¼Œè¿˜æœ‰æœªæˆå¹´äººçš„ç›¸å…³çš„æ¶ˆç¦åˆ¶åº¦ã€‚

é‚£ä¹ˆè¿™äº›åœ¨ä¸–ç•Œä¸Šéƒ½æ˜¯éå¸¸å°‘æœ‰çš„ï¼Œä¹Ÿæ˜¯ç‹¬ç‰¹çš„ã€‚é‚£ä¹ˆç›®å‰æˆ‘ä¸ªäººè®¤ä¸ºï¼Œæˆ‘ä»¬å›½å†…çš„å‘ƒäººå·¥æ™ºèƒ½æ²»ç†ä½“ç³»å’Œç†å¿µæ­£åœ¨é€æ­¥çš„å½¢æˆï¼Œå¹¶ä¸”å·²ç»é€æ­¥è¶…è¶Šäº†é£é™©æ²»ç†çš„ç†å¿µã€‚é¦–å…ˆï¼Œåœ¨é£é™©çš„è®¤çŸ¥ä¸Šï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸€ç›´åœ¨è·Ÿå›½é™…ä¿æŒåŒæ­¥ã€‚

å¤§å®¶å¯ä»¥å…³æ³¨åˆ°æˆ‘ä»¬å›½å†…çš„ç›¸å…³æŠ€æœ¯æ ‡å‡†å½“ä¸­ï¼Œå¯¹äºé£é™©çš„åˆ†ç±»ï¼Œå•Šï¼Œæ¯”å¦‚è¯´å¤±æ§æ€§é£é™©ã€ç¤¾ä¼šæ€§é£é™©ã€ä¾µæƒæ€§é£é™©ã€æ­§è§†æ€§é£é™©ã€è´£ä»»æ€§é£é™©ç­‰ç­‰ï¼Œå’Œå›½é™…çš„å¾ˆå¤šç›¸å…³åˆ†ç±»æ˜¯å®Œå…¨å¯ä»¥å¯¹æ¥çš„ã€‚

åŒ…æ‹¬ä¸€äº›ç›¸å…³çš„æ²»ç†æ‰‹æ®µã€äº‹å‰è¯„ä¼°è®¤è¯å’Œäº‹åè¿½è´£ï¼Œæˆ‘ä»¬ä¹Ÿå¸å–äº†å›½å¤–çš„å…ˆè¿›ç»éªŒã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥æ—¥ç›Šçœ‹åˆ°ï¼Œåœ¨è¿‘ä¸¤å¹´ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è¿™ä¸ªè¿…é€Ÿç”Ÿé•¿ä»¥æ¥ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾ˆå¤šä¸­å›½ç‰¹æœ‰çš„ä»·å€¼ç†å¿µå’Œæ²»ç†æ‰‹æ®µã€‚

é‚£ä¹ˆæˆ‘ä»¬å¯¹äºäººå·¥æ™ºèƒ½æ²»ç†çš„å¾ˆç‹¬ç‰¹çš„ä»·å€¼ç†å¿µï¼Œå°±æ˜¯æˆ‘è§‰å¾—æœ€é‡è¦çš„è¿™ä¸¤å¹´å°±æ˜¯ä¸å‘å±•å°±æ˜¯æœ€å¤§çš„ä¸å®‰å…¨ã€‚æˆ‘ä»¬è®¤ä¸ºæœ€å¤§çš„é£é™©å¯èƒ½å°±æ˜¯ä¸­å›½çš„äººå·¥æ™ºèƒ½äº§ã€‚ä¸šå’ŒæŠ€æœ¯æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆçš„å‘å±•ã€‚ç¬¬äºŒä¸ªå‘¢å°±æ˜¯æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ã€‚

æˆ‘ä»¬ä¸ç®¡æ˜¯å…¨çƒäººå·¥æ™ºèƒ½æ²»ç†å€¡è®®ï¼Œè¿˜æ˜¯åœ¨æ˜¨å¤©æˆ‘ä»¬çš„ä¸–ç•Œäººä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šä¸­æåˆ°çš„ä¸­å›½çš„è¿™ç§æ²»ç†æ–¹æ¡ˆã€‚æˆ‘ä»¬éƒ½å¼ºè°ƒè¦å°Šé‡å„å›½æœ¬åœŸçš„ä»·å€¼è§‚å’Œå‘å±•é˜¶æ®µçš„éœ€æ±‚ï¼Œä»¥åŠè¦å°Šé‡å„å›½çš„æ–‡åŒ–ã€‚é‚£ä¹ˆåœ¨æ²»ç†æ‰‹æ®µå±‚é¢ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å»å¹´çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æš‚è¡Œç®¡ç†åŠæ³•å‡ºå°ä¹‹åï¼Œä¸­å›½å·²ç»å½¢æˆäº†å¯¹äºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆ†å±‚æ²»ç†çš„è¿™æ ·ä¸€ä¸ªåŸºæœ¬çš„ç†å¿µã€‚åŒæ—¶å‘ƒä»¥å‘å±•ä¸ºå¯¼å‘ï¼Œä¸­å›½ä¹Ÿå¼€å±•äº†å¾ˆå¤šäººå·¥æ™ºèƒ½åŸºç¡€è®¾æ–½å»ºè®¾çš„ç›¸å…³å·¥ä½œã€‚

æ¯”å¦‚è¯´åœ¨äººå·¥æ™ºèƒ½çš„æ•°æ®è¦ç´ å±‚é¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å›½å®¶æ•°æ®å±€åšçš„å¤§é‡å·¥ä½œï¼Œè¿˜æœ‰å·¥ä¿¡éƒ¨åšçš„å¤§é‡æœ‰å…³ç®—åŠ›åŸºç¡€è®¾æ–½çš„å»ºè®¾å·¥ä½œã€‚æ‰€ä»¥æˆ‘ä»¬ç›®å‰åœ¨ä¸­å›½çš„å„è¿™ç§äººå·¥æ™ºèƒ½æ²»ç†ç†å¿µå‘¢ï¼Œæˆ‘ä»¬ä¸å…¶è¯´å®ƒæ˜¯ä¸€ä¸ªå®Œå…¨åŸºäºé£é™©çš„æ²»ç†ç†å¿µã€‚

ä¸å¦‚è¯´æ˜¯ä¸€ä¸ªåŸºäºä»·å€¼çš„æ²»ç†ç†å¿µã€‚åŸºäºä»·å€¼çš„äººå·¥æ™ºèƒ½æ²»ç†å¹¶ä¸æ’æ–¥é£é™©æ²»ç†ã€‚ä½†å®ƒè¶…è¶Šäºé£é™©æ²»ç†ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ä¸­å›½çš„ç«‹æ³•å’Œæ²»ç†æ”¿ç­–å½“ä¸­ï¼Œä¸ä»…ä»…æŠŠäººå·¥æ™ºèƒ½ç†è§£ä¸ºä¸€ç§æŠ€æœ¯æˆ–è€…æœåŠ¡åº”ç”¨ã€‚

äººå·¥æ™ºèƒ½æ—¢ä½¿æœªæ¥èµ‹èƒ½æ•´ä¸ªç¤¾ä¼šçš„åŸºç¡€è®¾æ–½ï¼Œä¹Ÿæ˜¯æœªæ¥æ•´ä¸ªç¤¾ä¼šç”Ÿäº§çš„ç»„ç»‡å½¢å¼ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠäººå·¥æ™ºèƒ½æ”¾åˆ°æ–°æ™ºç”Ÿäº§åŠ›çš„è§’åº¦å»ç†è§£ã€‚å¦å¤–ä¸€æ–¹é¢ï¼Œä¸­å›½å·²ç»è¶Šæ¥è¶Šæ˜ç¡®ä¸­å›½äººå·¥æ™ºèƒ½æ²»ç†çš„ç†å¿µå’Œæ–¹æ¡ˆæ˜¯ä»¥äººä¸ºæœ¬æ™ºèƒ½å‘å–„ã€‚

ä»¥äººä¸ºæœ¬ï¼Œæ˜¯åœ¨è¯´æŠ€æœ¯ä¸èƒ½åç¦»äººç±»æ–‡æ˜è¿›æ­¥çš„æ–¹å‘ï¼Œæ™ºèƒ½å‘å–„æ˜¯åœ¨å¼ºè°ƒäººå·¥æ™ºèƒ½å¿…é¡»åœ¨æ³•å¾‹ä¼¦ç†å’Œäººé“ä¸»ä¹‰çš„å±‚é¢çš„ä»·å€¼å–å‘ã€‚é‚£ä¹ˆåœ¨æ²»ç†æ‰‹æ®µæ–¹é¢ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¯ä»¥çœ‹åˆ°ç›®å‰ä¸€ä¸ªç³»ç»Ÿå‘åŠ›çš„æƒ…å†µã€‚åœ¨ä»Šå¹´5æœˆä»½ã€‚

å…¨å›½äººå¤§å¸¸å§”ä¼šå’Œå›½åŠ¡é™¢å·²ç»æŠŠäººå·¥æ™ºèƒ½ç«‹æ³•æ”¾åˆ°äº†ç›¸å…³çš„ç«‹æ³•è®¡åˆ’å½“ä¸­ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿæœ‰æ—¥å¸¸ç”Ÿæ´»æ—¥å¸¸ç›‘ç®¡æ´»åŠ¨å½“ä¸­ï¼Œæ›´ä¸ºä¸°å¯Œçš„ç›‘ç®¡æªæ–½ã€‚é‚£ä¹ˆåœ¨æˆ‘ä»¬ç›¸å…³çš„ä¸€äº›å‘ƒç«‹æ³•å’Œç›‘ç®¡æ´»åŠ¨å½“ä¸­ã€‚

æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ä¸­å›½äººå·¥æ™ºèƒ½çš„å®‰å…¨æ¡†æ¶ä¹Ÿåœ¨ç§¯æçš„è®¨è®ºå’Œé…é…¿å½“ä¸­ã€‚é‚£ä¹ˆåŒ…æ‹¬åœ¨å‰å¤©ä¹Ÿåˆšåˆšé¢å¸ƒäº†å‘ƒä¸€ä¸ªå›½å®¶å¼ºåˆ¶æŠ€æœ¯æ ‡å‡†ã€‚ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å†…å®¹çš„æ ‡è¯†çš„è¿™æ ·ä¸€ä¸ªæŠ€æœ¯æ ‡å‡†ã€‚

é‚£ä¹ˆç›¸ä¿¡å‘¢åŸºäºä»·å€¼çš„äººå·¥æ™ºèƒ½æ²»ç†ä½“ç³»æ­£åœ¨é€æ­¥æ„å»ºçš„è¿‡ç¨‹å½“ä¸­ï¼Œæˆ‘ä¸ªäººæ›´æ„¿æ„æŠŠå®ƒåˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡ã€‚ç¬¬ä¸€ä¸ªæ˜¯æ˜¯å¦‚æ­¤ï¼Œå°±æ˜¯æˆ‘ä»¬å»è§‚å¯Ÿäººå·¥æ™ºèƒ½çš„æœ¬ä½“ä»·å€¼ã€‚é‚£ä¹ˆåœ¨å…¶ä¸­æœ‰å‡ å±‚å«ä¹‰ã€‚é¦–å…ˆæˆ‘ä»¬å¸Œæœ›ç°åœ¨åœ¨é£é™©æ²»ç†å½“ä¸­ã€‚

è¿™æ ·ä¸€ä¸ªæ³›åŒ–çš„æ¨¡ç³Šçš„æ¦‚å¿µï¼Œè¢«é€æ¸åˆ†ç¦»å¼€å“ªäº›æ˜¯äººå·¥æ™ºèƒ½çš„å¿…ç„¶å½±å“ï¼Œå“ªäº›å±äºè¿‘æœŸçš„ç»´åº¦ï¼Œå“ªäº›æ˜¯äººå·¥æ™ºèƒ½ä¸ç¡®å®šçš„å½±å“ã€‚é‚£ä¹ˆå…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨å±æ€§å±‚é¢è¦å¯¹äººå·¥æ™ºèƒ½ä¸€ä¸ªåˆ¤æ–­ã€‚å®ƒç©¶ç«Ÿä»–åœ¨è¿™ä¸ªäººå·¥æ™ºèƒ½è¢«æåŠçš„æ—¶å€™ã€‚

æ˜¯ä»¥ä¸€ä¸ªæŠ€æœ¯çš„æ–¹å¼è¢«æåŠï¼Œè¿˜æ˜¯æœåŠ¡åº”ç”¨ï¼Œæ˜¯ç¤¾ä¼šç”Ÿäº§çš„åŸºç¡€è®¾æ–½ï¼Œè¿˜æ˜¯ç¤¾ä¼šç”Ÿäº§çš„ç»„ç»‡æ–¹å¼ï¼Œè¿™éƒ½å†³å®šäº†ä¸åŒå±‚é¢çš„å½±å“çš„å‘ç”Ÿï¼Œæ˜¯å¿…ç„¶çš„è¿˜æ˜¯å…·æœ‰ä¸ç¡®å®šæ€§å’Œå¯é¢„é˜²æ€§çš„ã€‚é‚£ä¹ˆç¬¬äºŒä¸ªå±‚é¢å‘¢æ˜¯é¢„å¦‚æ­¤ã€‚

å°±æ˜¯åŸºäºä¸­å›½æœ¬åœŸä»·å€¼è§‚å»åˆ¤æ–­äººå·¥æ™ºèƒ½æ²»ç†çš„çŸ­æœŸå’Œé•¿æœŸçš„ç›®æ ‡ï¼Œæ¢³ç†å•Šä¸­å›½çš„ä¸ªæ€§åŒ–çš„æ²»ç†éœ€æ±‚ã€‚é‚£ä¹ˆæˆ‘ä»¬ç‰¹åˆ«å¸Œæœ›èƒ½å¤Ÿåˆ†è§£å‡ºäººå·¥æ™ºèƒ½é£é™©å½“ä¸­çš„è§„èŒƒæ€§çš„å±‚æ¬¡ï¼ŒæŠŠä¸ªä½“åŒ–çš„ä»·å€¼æ ‡æ˜åˆ—æ˜ã€‚é‚£ä¹ˆå¯¹äºä¸­å›½æ¥è¯´ã€‚

æˆ‘ä»¬é¦–å…ˆåœ¨æŠ€æœ¯å±‚é¢æ˜¯è¦å®‰å…¨çš„å‘å±•ã€‚åœ¨æœåŠ¡åº”ç”¨å±‚é¢å»¶ç»­ä¸­å›½ä¸€ç›´ä»¥æ¥å¯¹äºæœåŠ¡åº”ç”¨çš„æ²»ç†ä½“ç³»ã€‚é‚£ä¹ˆåœ¨åŸºç¡€è®¾æ–½å±‚é¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å›½å®¶å„ç§åŠ å¤§åŠ›åº¦æªæ–½å»ä¿ƒè¿›åŸºç¡€è®¾æ–½å»ºè®¾ã€‚é‚£ä¹ˆåœ¨ç¤¾ä¼šç”Ÿäº§ç»„ç»‡æ–¹å¼å±‚é¢ã€‚

æˆ‘ä»¬ä¹Ÿåœ¨å¼ºè°ƒç»¿è‰²ç¯ä¿ï¼Œå¼ºè°ƒæ–°æ™ºç”Ÿäº§åŠ›çš„æ²»ç†æ–¹å¼ã€‚é‚£ä¹ˆåœ¨åº”å¦‚æ­¤çš„å±‚é¢ï¼Œä¹Ÿå°±æ˜¯ã€‚åœ¨åŸºäºä¸ªæ€§åŒ–æ²»ç†ä¹‹å¤–ï¼Œæˆ‘ä»¬å§‹ç»ˆæœ‰äººç±»å…±åŒæ ¸å¿ƒä»·å€¼è§‚å’Œäººç±»å‘½è¿å…±åŒä½“ã€‚é‚£ä¹ˆè¿™ä¹Ÿæ˜¯æˆ‘ä»¬åœ¨å‚ä¸å…¨çƒäººå·¥æ™ºèƒ½æ²»ç†å·¥ä½œå½“ä¸­å‘ƒã€‚

ä¸­å›½æå‡ºçš„æ–¹æ¡ˆã€‚é‚£ä¹ˆä¹Ÿæ˜¯æˆ‘åœ¨è”åˆå›½å‚ä¸çš„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è”åˆå›½æå‡ºçš„å—¯AI governance for humanityå°Šé‡æ•´ä¸ªäººç±»çš„ä»·å€¼ã€‚é‚£ä¹ˆå…·ä½“çš„æ‰‹æ®µæªæ–½å‘¢ã€‚

æˆ‘ä»¬å…¶å®åœ¨ä»Šå¹´3æœˆä»½çš„äººå·¥æ™ºèƒ½æ³•å­¦è€…å»ºè®®ç¨¿å½“ä¸­æœ‰æ¯”è¾ƒè¯¦ç»†çš„è§£é‡Šã€‚ç”±äºæ—¶é—´åŸå› å‘¢ï¼Œæˆ‘å°±å…ˆè®²åˆ°è¿™é‡Œï¼Œæ„Ÿè°¢å¤§å®¶ï¼Œè°¢è°¢ã€‚Thank you so muchï¼Œ Professor Zhangã€‚

 for such a clear and systematic presentation into the risksã€‚

 values and corresponding governance solutions to AIã€‚ğŸ˜Šï¼ŒNextï¼Œ we will have Drã€‚ Marknititzsburgã€‚

 who will be sharing his insights with usã€‚ Drã€‚ Nitsburg is currently an executive director of the Center for Human Compatible AI at UC Berkeleyã€‚

 as well as head of strategicategic outreach for Berkeley AI Researchã€‚ğŸ˜Šï¼ŒIn industryã€‚

 he has built technology venturesï¼Œ applying AI in healthcareï¼Œ financeï¼Œ education and development aidã€‚

He has worked at Bell Laboratoriesï¼Œ Microsoft and Amazon and developed and run diverse programs in industry and academiaã€‚

ğŸ¼Markï¼Œ it's great to see you in China againã€‚ The stage is all yoursã€‚ğŸ˜Šï¼ŒThank you for having meã€‚Okayã€‚

I am first and foremostï¼Œ a computer scientistã€‚And I have been asked to give aã€‚

An overview of the US approach to AI regulationã€‚And so I ask you to fasten your seatbeltsã€‚

Because this is the first time I'm taking these ideas for a spinã€‚I always remind myselfã€‚Of theã€‚

Extremeã€‚scale of our present situation with artificial intelligenceã€‚It is the largestã€‚

 most capable general purpose digital systemã€‚Everï¼Œ and we deliver it on the largest connected technology ecosystem everã€‚

And it's serviced by the largest commercial enterprisesã€‚Ever in historyã€‚Andã€‚

At least in the United Statesã€‚The artificial intelligence has almost no regulation at allã€‚

And this is nonetheless integrated into nearly every human activity in every sectorã€‚

So it is hard to regulateï¼Œ partly because of the black box nature ofã€‚The systemsã€‚

 partly because it's a general purpose technologyã€‚Andï¼Œ and often is misunderstoodã€‚å—¯ã€‚

So is there a US approach to AI regulationï¼ŸI would say thatã€‚We share withã€‚With manyã€‚Statesã€‚

 some sovereign objectives in the United Statesã€‚Perhaps unique is to promote the growth of the USAI sectorã€‚

 and we are looking to the US leadã€‚That that has an AIã€‚

We are looking to reap the benefits for the US economy and civil society and for national securityã€‚

Andã€‚We would like to protectã€‚From adverse effects and outcomesã€‚There are also global objectivesã€‚

That we shareã€‚And for exampleï¼Œ these are expressed inã€‚Many principlesï¼Œ for exampleï¼Œ the OACDã€‚

Pinncs and onã€‚On AIï¼Œ and I just took this oneã€‚APart piece of the OACD principlesï¼Œ 1ã€‚4ï¼Œ which isã€‚

Essentiallyï¼Œ about safetyã€‚Soï¼Œ theã€‚Spirit of the currentã€‚USã€‚Approach to regulation is capturedã€‚å‘ƒã€‚

In in the current processï¼Œ groups have been promoting competing interests and have resulted inã€‚3ã€‚

I thinkã€‚å—¯å—¯å—¯ã€‚3ã€‚Setets of potential lawsï¼Œ One is the executive orderã€‚From October of last yearã€‚

 and that tends to target large modelsã€‚And guides the federal agenciesã€‚

 the existing federal agenciesã€‚And then there is a roadmapã€‚

 this so calledled Schumma roadmap that came inã€‚May of this yearã€‚

 and that is weighing the promise of AI against effect on jobs and laws and defense and doomã€‚

 as he saysã€‚And then in Californiaã€‚Some of our regulation is coming from the statesã€‚There isã€‚Aã€‚

A bill that is making its way through the legislation called SP 1047ã€‚å—¯ã€‚

And that is concerned with the safe and secure innovation with frontierã€‚Mogulsã€‚

Now there are some issues with theseï¼Œ the debates continueã€‚None of it is law yetã€‚

There are some laws in specific statesï¼Œ and there are some very specific lawsï¼Œ but these largerã€‚å—¯ã€‚Ohã€‚

Frameworks have not yet made it to lawã€‚And thenã€‚As we have heardã€‚Over the last couple of daysã€‚

 there are challenges to operationalizingã€‚When you're working with general purpose technologiesã€‚

 for exampleï¼Œ it's hard to proveã€‚That a system has acceptably low riskã€‚If it is general purposeã€‚

 you really need to know what is that you're testing forã€‚One of the aspects ofã€‚The USã€‚

A legal system is that there's a lot of existing law in various sectorsã€‚

And so these do apply to systems that areã€‚Using AI in those sectorsï¼Œ for exampleã€‚

 health and transportï¼Œ agriculture and financeã€‚ Some of those need amendmentï¼Œ for exampleã€‚

 there wereã€‚Relatively few laws in transport for cars that drive themselvesï¼Œ but in many casesã€‚

 the existing sector of lawã€‚Gives a good baselineã€‚And then there are laws that concern the justã€‚

Distribution of burdens and benefitsï¼Œ for exampleï¼Œ in hiring and credit worthiness that cut across sectorsã€‚

Another thing about theï¼Œ the US is that weï¼Œ we tend toã€‚Occasionally have some litigationã€‚

 you may have heardã€‚ And this may be a situation in which litigationã€‚å‘ƒå‘ƒã€‚

Creating laws actually works outã€‚ So in the case of Ipï¼Œ for exampleï¼Œ intellectual propertyã€‚Theã€‚

 the issues ofï¼Œ ofã€‚The use of certainã€‚Data as training dataï¼Œ the use of lightnessesã€‚The way in whichã€‚

Creative jobs that are treated have made it through theã€‚Litigation systemã€‚

 and then in consumer protectionï¼Œ their laws worked out again through litigationã€‚

There are other factors or other driversï¼Œ including simply insurabilityã€‚ã§ã™ã€‚

You are selling an AI powered systemï¼Œ and you'd like to get insurance for itã€‚

 You need to prove a certain level ofã€‚Of safety and reliabilityã€‚

And then there are drivers coming fromã€‚The other regulationsï¼Œ the EUã€‚Digital Services Actã€‚

 for exampleï¼Œ that affect the companies in the United Statesã€‚

I'd like to take a couple of minutes just to talk about some of theã€‚Misconceptionsã€‚

And I will remind you that unlike Drã€‚ Hï¼Œ I am not actually representing my countryã€‚

 I'm speaking for myselfï¼Œ but I believe that there is a misconceptionã€‚That regulation isã€‚å‘ƒã€‚

Is a significant barrier to innovationï¼Œ I believe that done rightã€‚If we are attending to safetyã€‚

 then it's simply a necessary guidelineã€‚This is the case withã€‚Withï¼Œ withã€‚

Many other technologies and I think it's no different hereã€‚ I also think that oftenã€‚Capabilityã€‚

I confused with safetyï¼Œ I think that asã€‚As you seeã€‚Systems become more capableã€‚That does notã€‚

Eate necessarily with the immediate or a direct correlation with the issue of safetyã€‚

And then I think there's aã€‚Sometimes a confusion betweenã€‚What we call the red linesã€‚

And the idea of askingã€‚Asking developers to pause development on more powerful systemsã€‚

That is not a red lineï¼Œ it's a different concept altogetherã€‚

I do think that we can learn on the path to global governance from the existing international safety organizations like air transport safety and nuclear power information and telecommunications organizations and the way tend to work is that they find common ground and we converge where we can agreeã€‚

 and thenï¼ŒWhere we can't agreeï¼Œ we either seek some sort of generalized agreement orã€‚

Simply leave it to the states in those areasã€‚I think that that this can work for the internationalã€‚

Regulation hereã€‚ It looks like I have the time to give aã€‚A little bit more of aã€‚

Technical perspective from our centerï¼Œ I'ï¼Œ I'm coming from the Center for Human Compatible AIã€‚

 And these are all of the great people that I work withã€‚

 And that is to give you just a quick look at the long gameã€‚Path that that thatã€‚

We believe is the right way to do this for safe AI systemsã€‚

We of course are working now and you're hearing a lot today about making AI safeã€‚

And there are numerous techniques and methods like RLHF and constitutional AIã€‚

 where you have one AI looking at the outputs of another evaluations and audits and red teamingã€‚

 and we've heard about digital neuroscienceã€‚And theã€‚

The Qua AI Safety Institute that I think is just getting startedã€‚

 and these are terrific directions for the systems that areã€‚å‘ƒã€‚Not yet understoodã€‚

And if we want to be able to use them as componentsã€‚

 we'll need to have some kind of quantitative bounds on their behavior in order to make AI safe if it is based onã€‚

For exampleï¼Œ language modelsï¼Œ but in the longer term and the central focus of the work at our centerã€‚

 we are trying to make safe AI and to do thatï¼ŒWe believe it makes sense to revert to transparentã€‚

Explainableã€‚Semantically alyzable systemsã€‚Apply formal verification for some specific forms of safetyã€‚

And then to use aã€‚Secure ecosystem where nothing runs unless it's known to be safeã€‚And there areã€‚

There are some terrific inroads that we've been makingã€‚So this is the view fromã€‚The centerã€‚

 and thank you very much for the opportunity to speakã€‚Thank you so muchï¼Œ Markã€‚

 please remain on stage and prepare to be seated as we get set up for our panelã€‚

To conclude our discussion on AI safety guidanceï¼Œ we will have a panel discussion with our expertsã€‚

Let's welcome back on stageï¼Œ Professor Gail Rohau and Professor Zhang Ninghanã€‚

We are also joined by Professor Xi Weidongï¼Œ and Dr Wang Yingncunã€‚Professor Z Drã€‚ Wangã€‚

 please proceed to the stage as I start introducing youã€‚

Professor Xi Weigong was formerly dean and Ko Guan chair professor of Shanghai Jiao Tong University's School of Lawã€‚

He is now a professor of humanity and social sciencesã€‚

 president of the China Institute for Sociolegal Studies and director of the Center for AI Governance and Law at Shanghai Jiaong Universityã€‚

Drctor Wang is the deputy director of the Shanghai AI Laboratory Governance Research Centerã€‚Drã€‚

 Wang has presided over and participated in dozens of national provincial and ministerial projectsã€‚

 including the development of the open AI governance platformï¼Œ Open E G Laã€‚

 and other AI governance systemsã€‚A moderator for this session will be Fang Liangã€‚

 senior governance lead at Concordia AIã€‚Prior to joining Concordiaï¼Œ Fang Liang worked at Baiduã€‚

 where he researched AI governance and participated in the formulation of several Chinese government AI policiesã€‚

 A round of applause to our panelistsã€‚å¥½ï¼Œå¤§å®¶ä¸‹åˆå¤§å®¶ä¸‹åˆå¥½ã€‚æˆ‘ç›¸ä¿¡å¤§å®¶å¬äº†ä»Šå¤©çš„è®²åº§éƒ½èƒ½å¾ˆæ·±åˆ»çš„æ„Ÿè§‰åˆ°äººå·¥æ™ºèƒ½å·²ç»å¾ˆæ·±åˆ»çš„å½±å“åˆ°å…¨çƒçš„ç»æµç¤¾ä¼šæ³•å¾‹ã€‚

ä½†æˆ‘ä»¬ç†è§£è¿™ç§å½±å“å…¶å®å¯¹å…¨çƒäº’åŠ¨åœ°åŒºçš„æœºé‡å’ŒæŒ‘æˆ˜å…¶å®æ˜¯ä¸åŒçš„ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™ä¸ªè®ºå›ä¹Ÿä¸‹è°ˆä¸€ä¸‹äººå·¥æ™ºèƒ½æ²»ç†çš„åœ°åŒºè§†è§’å’Œç»æµåˆ†äº«ã€‚é¦–å…ˆæˆ‘ä»¬å¯èƒ½æƒ³è°ˆè®¨ä¸€ä¸‹å‘å±•ä¸å®‰å…¨çš„é—®é¢˜ã€‚å‘ƒï¼Œæˆ‘ä»¬å…ˆä»æ–°åŠ å…¥å’Œå­£è€å¸ˆå¼€å§‹å§ã€‚

å¯¹æˆ‘çœ‹è§æ‚¨æœ€è¿‘å†™äº†ä¸€ç¯‡æ–‡ç« ï¼Œæ˜¯ä½•æ—¶çœŸæ­£è¿ˆå…¥äººå·¥æ™ºèƒ½è¿™æ²»ç†çš„ç«‹æ³•æ—¶åˆ»ã€‚é‡Œé¢ç¡®å®æåˆ°äº†ä¸€ä¸ªäººå·¥æ™ºèƒ½çš„å‘å±•å®‰å…¨ä¹‹é—´çš„ä»·å€¼åˆ¤æ–­ï¼Œä»¥åŠå‘ƒé€šåˆ†é€‰æ‹©ï¼Œè¿˜æ²¡æœ‰å½¢æˆå…±è¯†ã€‚åŒæ—¶æˆ‘æ‚¨ä¹Ÿæåˆ°åœ¨å¤šè¿‡å¤ªå¤§æ¨¡å‹å’Œè¿œå¤§æ¨¡å‹ã€‚

åªæœ‰ä»–ä»¬çš„æ€§èƒ½ä¸å®‰å…¨åº¦å½¢æˆæŸç§æ­£æ¯”çš„å…³ç³»çš„æ—¶å€™ï¼Œæ‚¨è®¤ä¸ºæ‰èƒ½å½¢æˆçœŸæ­£çš„äººå·¥æ™ºèƒ½ç«‹æ³•æ—¶åˆ»ã€‚æ‚¨èƒ½å¸®æˆ‘ä»¬è¿›ä¸€æ­¥é˜è¿°å—ï¼Ÿå¥½çš„ã€‚å‘ƒï¼Œæœ€è¿‘200å¹´æ¥ã€‚ç§‘æŠ€çš„å‘å±•å¯¹äºäººç±»ç¤¾ä¼šçš„è¿›æ­¥äº§ç”Ÿäº†éå¸¸å·¨å¤§çš„å½±å“ã€‚å‘ƒã€‚

æˆ‘è®¤ä¸º19ä¸–çºªçš„ã€‚è¿™ä¸ªå†…ç‡ƒæœºåŠ ç”µå™¨æŠ€æœ¯ï¼Œä½¿æˆ‘ä»¬äººç±»æœ‰äº†ç”Ÿæ´»çš„è‡ªç”±ã€‚é‚£ä¹ˆ20ä¸–çºªäº’è”ç½‘ã€‚å®¶ã€‚é€šä¿¡æŠ€æœ¯ã€‚ä½¿äººç±»æœ‰äº†ä¿¡æ¯çš„è‡ªç”±ã€‚21ä¸–çºªã€‚å¤§æ¨¡å‹åŠ è€æœºæ¥å£ã€‚ä½¿æˆ‘ä»¬æœ‰ä»€ä¹ˆå‘¢ï¼Ÿç°åœ¨å¾ˆéš¾æƒ³è±¡ã€‚

ä½†æ˜¯è‡³å°‘æ˜¯æœ‰äº†åˆ›æ–°çš„åŸºæœ‰ï¼Œç”šäºè‡ªç”±ï¼Œç”šè‡³æ˜¯ä»æ— åˆ°æœ‰è¿›è¡Œåˆ›æ–°çš„è¿™æ ·ä¸€ç§ã€‚å¯èƒ½æ€§æ˜¯å±•å¼€äº†ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒå¯ä»¥èµ‹èƒ½ç¤¾ä¼šï¼Œèµ‹èƒ½äººç±»å‘ƒï¼Œä¼šå¸¦æ¥ç¦åˆ©ã€‚ä½†æ˜¯ç¡®å®å‘¢ä¼šå¼•èµ·è¿™æ ·é‚£æ ·çš„å®‰å…¨ä¸Šçš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§æ¨¡å‹å‡ºæ¥ä¹‹åã€‚

è‡³å°‘æ˜¯æœ‰4ä¸ªæ–¹é¢çš„é—®é¢˜å˜›ã€‚å‘ƒå› ä¸ºå¤§æ¨¡å‹å¤§é‡çš„ã€‚è¿™ä¸ªä½¿ç”¨æ•°æ®ï¼Œé‚£å®ƒå¯èƒ½ä¼šå¼•å¼•èµ·éšç§æ–¹é¢çš„å¿§è™‘ã€‚å‘ƒï¼Œé‚£ä¹ˆå¦å¤–å‘¢æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ã€‚å¤§æ¨¡è¡Œç”±äºä»–çš„èƒ½åŠ›æ³›ç¼“ï¼Œå‘ƒï¼Œé€ æˆäº†è¿™ä¸ªå°±æ˜¯å¹»è§‰ç°è±¡ã€‚

å¯èƒ½ä¼šå¼•èµ·è™šå‡ä¿¡æ¯ä»¥åŠè¿™ä¸ªå‘ƒè¯±å‘å„ç§å„æ ·çš„çŠ¯ç½ªã€‚å‘ƒï¼Œé‚£ä¹ˆå¦å¤–ä¸€ä¸ªå‘¢å‘ƒåœ¨çŸ¥è¯†äº§æƒé—®é¢˜ä¸Šï¼Œå‘ƒï¼Œå®ƒä¹Ÿå¯èƒ½ä¼šå¼•å‘è¿™æ ·é‚£æ ·çš„å‘ƒå¤æ‚çš„é—®é¢˜ã€‚è¿˜æœ‰ä¸€ä¸ªå‘¢ï¼Œå®ƒå¯èƒ½ä¼šä½¿å¾—æ•´ä¸ªç¤¾ä¼šæ²»ç†çš„ä¸­æ¢æœºå…³å‡ºç°æ¼æ´ã€‚

é‚£ä¹ˆæœ€åå‘¢ä¼šå¯¼è‡´ä¿¡æ¯ç¤¾ä¼šå‘ç”ŸåŠŸèƒ½éšœç¢ã€‚é‚£è¿™äº›å‘¢éƒ½è®©æˆ‘ä»¬æ„Ÿè§‰åˆ°ä¸å®‰ã€‚é‚£é—®é¢˜æ˜¯æˆ‘ä»¬å¦‚ä½•å¤„ç†ã€‚è¿™æ ·çš„è¿™ä¸ªé—®é¢˜ã€‚å½“ç„¶å‘ƒæˆ‘å‰é¢è®²åˆ°äº†ç§‘æŠ€ç»™äººç±»ç¤¾ä¼šå¸¦æ¥äº†å·¨å¤§çš„ç¦åˆ©ã€‚å‘ƒï¼Œé‚£ä¹ˆè¿™ä¸ªå‘¢å°±æ˜¯å¦‚ä½•åœ¨è¿™ä¸¤è€…ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚

é‚£æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨äºšæ´²åœ¨éæ´²è¿™äº›å›½å®¶å¯¹ä»–ä»¬æ¥è¯´ï¼Œå‘å±•æ˜¯ä¸€ä¸ªæ›´çªå‡ºçš„é—®é¢˜ã€‚ä»–ä»¬å¸Œæœ›èƒ½å¤Ÿå®ç°ä¸€ç§äº’æƒ çš„è¿™ä¸ªç§‘æŠ€çš„å‘ƒè¿™è¿™ä¸ªå‘å±•ã€‚å‘ƒé‚£ä¹ˆå½“ç„¶æˆ‘ä»¬çŸ¥é“åƒç¾å›½æ—¥æœ¬å‘ƒè¿˜æœ‰ä¸­å›½å‘¢å‘ƒéå¸¸å¼ºè°ƒåœ¨å‘å±•å’Œè¿™ä¸ªå®‰å…¨ä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚

å‘ƒï¼Œé‚£ä¹ˆå¦å¤–ä¸€æ–¹é¢å‘¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¬§ç›Ÿå‘ƒæœ€æœ€è¿‘é€šè¿‡çš„è¿™ä¸ªäººå·¥æ™ºèƒ½æ³•æå‡ºæ¥äº†å®‰å…¨é«˜äºå‘å±•è¿™æ ·ä¸€ç§ä»·å€¼å–å‘ã€‚é‚£å¦‚ä½•åœ¨è¿™ä¸ªä¸­é—´æ‰¾åˆ°é€‚å½“çš„å¹³è¡¡ã€‚å•Šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å„ä¸ªå›½å®¶æœ‰ä¸åŒçš„ç«‹æ³•æ¨¡å¼ã€‚å‘ƒã€‚

é‚£ä¹ˆå‘ƒæˆ‘æƒ³å‘¢è¿™ä¸ªæ˜¯å€¼å¾—æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨çš„ã€‚è°¢è°¢åˆ†äº«ã€‚æ¥ä¸‹æ¥æƒ³è¯·æ•™ç‹è€å¸ˆã€‚å‘ƒï¼Œæˆ‘ä»¬çŸ¥é“ä¸Šæµ·æ‚¨ä½œä¸ºä¸Šå‘ƒç ”å‘æœºæ„çš„ä»£è¡¨ï¼Œæˆ‘ä»¬çŸ¥é“ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ä¹Ÿåšäº†å¾ˆå¤šå®‰å…¨å•Šï¼Œå¯¹å…¶çš„è¯„æµ‹çš„å·¥ä½œã€‚

åŒ…æ‹¬æ˜¨å¤©å‘¨ä¸»ä»»ä¹Ÿæå‡ºäº†ä¸€ä¸ªå®‰å…¨å‘å±•çš„45åº¦è¯„åˆ†çš„ã€‚æƒ³è¯·æ‚¨æ¥è§£è¯»ä¸€ä¸‹æ‚¨è®¤ä¸ºçš„å®‰å…¨å’Œå‘å±•ä¹‹é—´çš„å…³ç³»ã€‚å¥½çš„ï¼Œå…¶å®å‘ƒæ˜¨å¤©å‘¨å›½æ–‡æ•™æˆåœ¨ã€‚å‘ƒäººå¤§ä¼šçš„è¿™ä¸ªä¸Šåˆå¤§ä¼šå…¨ä½“ä¼šè®®ä¸Šã€‚

ä»–åˆ†äº«äº†æˆ‘ä»¬å®éªŒå®¤å¯¹è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªå¾ˆé‡è¦çš„ä¸€ä¸ªåˆ¤æ–­å˜›ï¼Œä»–ç§°ä¹‹ä¸ºè¿™ä¸ªæŠ€æœ¯ä½“ç³»çš„ä¸€ä¸ªæ€æƒ³å•Šï¼Œå°±æ˜¯äººå·¥æ™ºèƒ½çš„45åº¦å¹³è¡¡ç‡ã€‚å‘ƒï¼Œç°åœ¨æ•´ä¸ªäººå·¥æ™ºèƒ½å‘å±•å‘ƒï¼Œæˆ‘ä»¬è®¤ä¸ºæ˜¯è·›è„šçš„ã€‚

æ— è®ºæ˜¯ä»å®ƒçš„å¤§éƒ¨åˆ†çš„æŠ€æœ¯å’Œç®—åŠ›èµ„æºéƒ½æŠ•åœ¨å®ƒçš„æ€§èƒ½çš„ç ”å‘ä¸Šã€‚è™½ç„¶æˆ‘ä»¬æœ‰è¿™ä¸ªIHè¿™äº›è¿™ä¸ªæ—¢å…¼é¡¾æ€§èƒ½å’Œè¿™ä¸ªå®‰å…¨çš„æŠ€æœ¯ã€‚ä½†æ€»ä½“ä¸Šè¿˜æ˜¯åé‡äºè¿™ä¸ªæ€§èƒ½çš„ï¼Œå°±å¯¼è‡´æˆ‘ä»¬å®é™…ä¸Šä»æŠ€æœ¯ç¤¾åŒºæ¥çœ‹ã€‚

æŠ•å…¥åˆ°å®‰å…¨æ–¹é¢çš„è¿™ä¸ªèµ„æºäººåŠ›å’Œç®—åŠ›éƒ½æ˜¯ä¸¥é‡ä¸è¶³çš„ã€‚è€Œä¸”æˆ‘ä»¬çš„æŠ€æœ¯æ–¹æ³•ï¼Œç›®å‰è¿˜æ˜¯åç½®çš„å¾ˆåˆ†æ•£ã€‚æˆ‘ä»¬æ˜¯å¸Œæœ›èƒ½å¤Ÿæ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿå®‰å…¨ä¼˜å…ˆï¼ŒåŒæ—¶èƒ½å¤Ÿå…¼é¡¾æ€§èƒ½å¢é•¿çš„è¿™æ ·ä¸€æ¡æŠ€æœ¯çš„è·¯å¾„ã€‚å•Šã€‚

å½“ç„¶è¿™ä¸ªæŠ€æœ¯è·¯å¾„çš„æ¢ç´¢å‘¢æ˜¯éå¸¸å¤æ‚çš„ï¼Œä¹Ÿéå¸¸è‰°è¾›çš„ã€‚å‘ƒï¼Œæˆ‘ä»¬æ˜¯è¿™ä¸ªä¹Ÿæ˜¯å‘¨æ•™æˆå‘å‡ºäº†è¿™æ ·ä¸€ä¸ªè¿™ä¸ªå‘¼åå§ã€‚å¸Œæœ›èƒ½å¤Ÿå‘ƒæˆ‘ä»¬è¿™ä¸ªæ¡ˆç ”çš„è¿™æ¡45åº¦çš„çº¿å•Šã€‚å»æ¨è¿›äººå·¥æ™ºèƒ½çš„å‘å±•å•Šï¼Œå½“ç„¶è¿™ä¸ªè¿‡ç¨‹å½“ä¸­ã€‚

å…¶å®è¿™ä¸ªå¦‚æœè¿™æ¡è·¯å¾„èƒ½å¤Ÿèµ°é€šçš„è¯ï¼Œå‘ƒï¼Œæˆ‘ç›¸ä¿¡å•Šç›®å‰å‘ƒäººå·¥æ™ºèƒ½çš„å®‰å…¨é£é™©é¢ä¸´ç€å¾ˆå¤šæŒ‘æˆ˜å‘¢ï¼Œå®ƒä»åŸºç¡€ä¸Šå¯ä»¥å¾—åˆ°å‘ƒå¾ˆé‡è¦çš„ä¸€ä¸ªä¿éšœã€‚å‘ƒï¼Œæˆ‘ä»¬ä¹Ÿåœ¨åšåšè¿™æ–¹é¢åŠªåŠ›ã€‚å‘ƒï¼Œæ‰€ä»¥æˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ã€‚

å…¶å®æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªå‘ƒå¤§å®¶å…±è¯†æ€§çš„è¿™æ ·ä¸€ä¸ªå‘å±•è·¯çº¿çš„è¿™ä¸ªæ¡†æ¶å•Šï¼Œæˆ‘ä»¬ä¸èƒ½é•¿æœŸä½äº45åº¦æ¥å‘å±•å•Šï¼Œæˆ‘ä¹Ÿä½†å¦‚æœé•¿æœŸé«˜äº45åº¦å‘¢ä¹Ÿå¾ˆéš¾è¿™ä¸ªå¸‚åœºå¸‚åœºçš„å•†ä¸šåŒ–çš„è¦æ±‚å•Šï¼Œä½†æ˜¯æˆ‘ä»¬æ˜¯ä¸æ˜¯æœ‰è¿™æ ·ä¸€ä¸ªç†æƒ³çš„è·¯çº¿å•Šã€‚

å¤§å®¶å…±åŒæ¥åŠªåŠ›å•Šï¼Œè¿™ä¸ªè·¯å¾„å•Šï¼Œæˆ‘è§‰å¾—è¿˜æ˜¯å¾ˆå¤§å¯èƒ½èƒ½å¤Ÿå®ç°çš„ã€‚è°¢è°¢è°¢ï¼Œè€Œä¸”å¯èƒ½éœ€è¦åŠ¨æ€è°ƒæ•´å»çµæ´»åº”å˜ã€‚å¯¹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯èƒ½æƒ³èŠä¸€ä¸‹ä¸åŒåœ°åŒºçš„ä¸€äº›æ²»ç†æŒ‘æˆ˜çš„ç‹¬ç‰¹çš„æœºé‡å‘ƒgailthank youã€‚

æˆ‘ä»¬å¯èƒ½æƒ³èŠä¸€ä¸‹ï¼Œæœ€è¿‘å…¶å®ä¸­æ³•çš„äººå·¥æ™ºèƒ½æ²»ç†å£°æ˜ä¹‹åï¼Œå…¶å®ä¸­å›½å›½å†…å¯¹æ³•å›½çš„æƒ…å†µå…¶å®ä¹Ÿä¹Ÿéƒ½éå¸¸æ„Ÿå…´è¶£ã€‚æ‚¨èƒ½ä»‹ç»ä¸€ä¸‹åœ¨æ³•å›½çš„äººå·¥æ™ºèƒ½é¢†åŸŸæœ‰å“ªäº›æ¯”çš„ç‹¬ç‰¹çš„æœºé‡å’ŒæŒ‘æˆ˜å—ï¼Ÿ

ä»¥åŠå¯èƒ½æˆ‘æ¯æ¬¡åˆ°2025å¹´åˆä¼šæœ‰1ä¸ªAIè¡ŒåŠ¨å³°ä¼šï¼Œæ‚¨èƒ½è°ˆä¸€ä¸‹æ‚¨å¯¹ä»–çš„æœŸå¾…å—ï¼ŸAll rightï¼Œ thank you very much well yes I can talk a bit about the specificities in France you know if we take a step back you know what's specific to France that may be different than other countries well one of the things that I see that is different in France is that we currently have a fairly difficult social and economical discussion across the country so there is a lot of a lack of trust across the layers of societies you know whether it's between the workers and the decision makers or between the cities and there are more areas now that might feature true also of other countries but it's quite pronounced in Franceã€‚

 another aspects that shapes the discussion is the economics so historically Europe is a fairly digital poor economy where net employerers of digital services and that kind of frames or relationship to the digital services however Franceã€‚

iss very strong in terms of AI talentsï¼Œ so there's currently a change in economic dynamics and that is bringing a lot of hope and positive momentum in the cities now in the countryside it's a different thingã€‚

è¯¶ã€‚Now in terms of the policy makingï¼Œ the kind of framework that is being put in place is across different directionsã€‚

 the first one being investmentã€‚So there is increasing investment supported by the stateã€‚

 but not only in the digital infrastructureã€‚It's aligned with values of open source and of commons that we see as a strategy for sovereign digital infrastructureã€‚

 and there is increasing momentum in building public sectorã€‚

 digital infrastructures and articulating this with private sectorï¼Œ digital infrastructureã€‚

And one challenge is to bridge the gapã€‚Between the different layers of society to and each and every individual and here the angle that's taken here is to maintain human contact and not only digital contactã€‚

 so all across all the territory so here are the challenges to avoid the digital divide we have a digital divide I think every country has digital divide and we're investing there to foster the discussionã€‚

 that's the kind of framework we're taking in Franceï¼Œ thank you thank you divide I think the keywordã€‚

å¯¹ï¼Œå¼ ç«‹æ’è€å¸ˆï¼Œæˆ‘ä»¬ä¸€èµ·è¯·æ•™ä¸€ä¸‹ã€‚æˆ‘ä»¬çŸ¥é“æ‚¨ä»Šå¤©èµ·è‰å¹¶å‘å¸ƒäº†ä¸€ä¸ªäººå·¥æ™ºèƒ½æ³•çš„å­¦è€…ç»éªŒç¨¿ã€‚ç„¶ååˆšæ‰åœ¨æ¼”è®²ä¸­ä¹Ÿæåˆ°äº†ä¸­å›½å±é™©æ²»ç†çš„ä¿®æ­£ä»¥åŠå‘ƒæ²»ç†æœ¬åœŸåŒ–ã€‚

æ‚¨èƒ½åœ¨æ•´ä½“ä¸Šè°ˆä¸€ä¸‹æ‚¨è®¤ä¸ºçš„ä¸­å›½äººå·¥æ™ºèƒ½ç«‹æ³•çš„æ•´ä½“é€»è¾‘å’Œæ²»ç†æ¶æ„ã€‚è°¢è°¢å‘ƒï¼Œå›ç­”è¿™ä¸ªé—®é¢˜å‘¢ï¼Œå…¶å®æˆ‘è§‰å¾—å¯ä»¥ä»ä¸¤ä¸ªè§’åº¦æ¥è€ƒè™‘ä¸­å›½çš„äººå·¥æ™ºèƒ½ç«‹æ³•ã€‚ç¬¬ä¸€ä¸ªå±‚æ¬¡ï¼Œæˆ‘ä»¬è¦æŠŠå®ƒç†è§£æˆæ˜¯ä¸­å›½çš„åˆ¶åº¦åç‰‡ã€‚

åœ¨å…¨çƒå…±åŒå»è®¤ä¸ºåº”è¯¥å¤„ç†äººå·¥æ™ºèƒ½é£é™©çš„æ—¶å€™ï¼Œä¸­å›½ä½œä¸ºä¸€ä¸ªäººå·¥æ™ºèƒ½æŠ€æœ¯ç›¸å¯¹é¢†å…ˆçš„å›½å®¶ï¼Œä½œä¸ºä¸€ä¸ªå¤§å›½ï¼Œå¦‚ä½•å»æ‰“é€ ä¸€ä¸ªæˆ‘å¸‚ä¸€ä¸ªè´Ÿè´£ä»»å¤§å›½çš„è¿™æ ·ä¸€ä¸ªå½¢è±¡ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œå®é™…ä¸Šåœ¨äººå·¥æ™ºèƒ½æ²»ç†çš„å·¥ä½œå½“ä¸­ã€‚

ä¸­å›½å·²ç»åšäº†å¾ˆå¤šå¾ˆå¤šã€‚ä¸­å›½æ˜¯ä¸–ç•Œä¸Šå”¯ä¸€ä¸€ä¸ªå¯¹äºäººå·¥æ™ºèƒ½å®‰å…¨æ²»ç†è¦†ç›–äº†å›½å®¶è§„åˆ’æ³•å¾‹è¡Œæ”¿æ³•è§„éƒ¨é—¨è§„ç« ã€æŠ€æœ¯æ ‡å‡†çš„å…¨æ–¹ä½è¦†ç›–çš„å›½å®¶ï¼Œä¹Ÿæ˜¯ç›®å‰ä¸€ä¸ªå¯¹äºå¤§æ¨¡å‹çš„æ²»ç†ã€å·²ç»å®é™…è½åœ°çš„å›½å®¶ã€‚

ä½†æ˜¯æˆ‘ä»¬åœ¨äººå·¥æ™ºèƒ½å®‰å…¨æ²»ç†æ–¹é¢æ‰€åšçš„åŠªåŠ›å¹¶ä¸ä¸ºä¸–ç•Œæ‰€çŸ¥é“ã€‚å…¶ä¸­ä¸€ä¸ªé‡è¦åŸå› å°±æ˜¯æˆ‘ä»¬ç¼ºä¹ä¸€éƒ¨é«˜ä½é˜¶çš„æ³•å¾‹å»ä½œä¸ºä¸­å›½çš„åˆ¶åº¦åç‰‡ï¼Œè®©å¤§å®¶çŸ¥é“å‘ƒï¼Œä¸­å›½æ›¾ç»åšäº†ä¸­å›½æ­£åœ¨åšå“ªäº›äº‹æƒ…ï¼Œæˆ–è€…å·²ç»åšäº†å“ªäº›äº‹æƒ…ã€‚

é‚£ä¹ˆåœ¨è”åˆå›½çš„å·¥ä½œç»„é‡Œï¼Œæˆ‘ä»¬å¤§æ¦‚ä¸€å…±æœ‰30å¤šä½ä¸“å®¶ã€‚å—¯ï¼Œå…¶ä¸­æˆ‘åœ¨è¿™ä¸ªç»„é‡Œæœ‰ä¸€éƒ¨åˆ†çš„å·¥ä½œå°±æ˜¯è¦å‘Šè¯‰å¤§å®¶OKæˆ‘ä»¬ä¸­å›½å·²ç»åšäº†ä»€ä¹ˆäº‹æƒ…ã€‚è®©æˆ‘å¾ˆæƒŠè®¶çš„æ˜¯æ˜¯ä½œä¸ºè¿™ä¸ªã€‚å—¯ã€‚

å›½é™…ä¸Šæ¯”è¾ƒé›†ä¸­çš„å¯¹äººå·¥æ™ºèƒ½æ²»ç†å¾ˆäº†è§£çš„è¿™äº›ä¸“å®¶ä»¬ï¼Œå…¶å®ä¹Ÿæœ‰å¤§éƒ¨åˆ†å¹¶ä¸å¤ªäº†è§£ä¸­å›½åœ¨äº§ä¸šæ²»ç†å½“ä¸­åšäº†ä»€ä¹ˆå®é™…çš„äº‹æƒ…ã€‚æˆ‘è¿™æ˜¯è®¤ä¸ºè¿™æ˜¯äººå·¥æ™ºèƒ½ç«‹æ³•çš„ç¬¬ä¸€ä¸ªæœ€é‡è¦çš„ç›®æ ‡ã€‚ç¬¬äºŒä¸ªå°±æ˜¯è¦ç¬¦åˆä¸­å›½æœ¬åœŸçš„æ²»ç†éœ€æ±‚ã€‚

æˆ‘éå¸¸æ„¿æ„æŠŠä¸­å›½åœ¨ä¸–ç•Œä¸Šçš„ä½ç½®å®šä¹‰ä¸ºé¢†å…ˆçš„è¿½èµ¶è€…ã€‚é¢†å…ˆçš„è¿½èµ¶è€…æ„å‘³ç€ä»€ä¹ˆï¼Ÿä¸€æ–¹é¢ï¼Œä½ ä½œä¸ºä¸€ä¸ªå¤§å›½ï¼Œè¦å»æ²»ç†å¥½äººå·¥æ™ºèƒ½å®‰å…¨ï¼Œä¸èƒ½è®©ä¸­å›½æˆä¸ºäººå·¥æ™ºèƒ½æ²»ç†çš„æ´¼åœ°ã€‚ä½ çœ‹æ˜¯ç¬¬ä¸€åœ°å“ˆã€‚ä½†æ˜¯å¦å¤–ä¸€æ–¹é¢ã€‚

æˆ‘ä»¬ä¹Ÿè¦å……åˆ†è®¤è¯†åˆ°ä¸­å›½äººå·¥æ™ºèƒ½çš„æŠ€æœ¯å’Œäº§ä¸šå‘å±•ï¼Œéœ€è¦å¤§é‡çš„æ³•å¾‹æ³•è§„çš„è°ƒæ•´ï¼Œå»ä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯äº§ä¸šçš„å‘å±•æä¾›è¦ç´ ï¼Œæä¾›èµ„æºã€‚åŒæ—¶å»ä¸ºäººå·¥æ™ºèƒ½äº§ä¸šè®¾ç½®ä¸€ä¸ªåˆç†çš„æ³•å¾‹è´£ä»»çš„æ¡†æ¶ã€‚é‚£ä¹ˆåœ¨è¿™ä¸¤ä¸ªç†å¿µçš„æŒ‡å¯¼ä¹‹ä¸‹ã€‚

æˆ‘ç›¸ä¿¡ä¸ç®¡æ˜¯ä»€ä¹ˆæ ·çš„åˆ¶åº¦ï¼Œç»è¿‡æˆ‘ä»¬å……åˆ†çš„è®¨è®ºï¼Œå¸Œæœ›å°†æ¥èƒ½å¤Ÿå½¢æˆå³ä½œä¸ºä¸­å›½åˆ¶åº¦åç‰‡ï¼Œæˆä¸ºä¸­å›½åœ¨å…¨çƒäººå·¥æ™ºèƒ½æ²»ç†å½“ä¸­é‡è¦å½¢è±¡ä»£è¡¨çš„ä¸€éƒ¨äººå·¥æ™ºèƒ½æ³•ã€‚åŒæ—¶ï¼Œè¿™éƒ¨äººå·¥æ™ºèƒ½æ³•ï¼Œä¹Ÿèƒ½æœ‰æ•ˆçš„é˜²èŒƒäººå·¥æ™ºèƒ½å®‰å…¨é£é™©ã€‚

ä¿ƒè¿›äººå·¥æ™ºèƒ½å®‰å…¨ï¼Œå¹¶ä¸”ç¬¦åˆä¸­å›½æœ¬åœŸæŠ€æœ¯å’Œäº§ä¸šå‘å±•çš„éœ€æ±‚ã€‚è°¢è°¢ã€‚è°¢è°¢ä¸­å›½å­¦ç”Ÿå¾ˆå¤šå·¥ä½œéœ€è¦æ›´å¥½çš„å»å¯¹å¤–ä¼ æ’­ã€‚å…¶å®æˆ‘ä»¬ä¹Ÿä¹Ÿåšäº†å¾ˆå¤šå¸®åŠ©ä¸­å›½çš„ä¸ç®¡æ˜¯å®‰å…¨ç ”ç©¶æ²»ç†è¿˜æ˜¯ç«‹æ³•å·¥ä½œï¼Œå¯¹å¤–ä¼ æ’­å·¥ä½œã€‚

å¸Œæœ›èƒ½æ›´å¥½çš„å±•ç”Ÿä¸­å›½çš„å·¥ä½œã€‚å‘ƒï¼Œæ¥ä¸‹æ¥æˆ‘å¯èƒ½æƒ³èŠä¸€ä¸‹æœ€è¿‘æ¯”è¾ƒçƒ­çš„äººå·¥æ™ºèƒ½å®‰å…¨ç ”ç ”ç©¶æ‰€è¿™ä¸ªäº‹å„¿ã€‚ç„¶åè‡ªå»å¹´è‹±å›½IIå®‰å…¨å´©ä¼šä»¥æ¥ï¼Œå…¶å®å‘ƒå·²ç»æœ‰å¤§æ¦‚ç¾å›½ã€è‹±å›½ã€æ³•å›½åœ¨é‚£10ä¸ªå›½å®¶ã€‚

è¿˜æœ‰æ¬§ç›Ÿéƒ½æˆç«‹äº†è‡ªå·±çš„å›½å®¶çº§çš„äººå·¥æ™ºèƒ½å®‰å…¨ç ”ç©¶æ‰€ã€‚åŒ…æ‹¬ä¸€ä¸ªå…¨çƒæ€§çš„ç ”ç©¶ç½‘ç»œã€‚å‘ƒï¼Œæ¥ä¸‹æˆ‘å¯èƒ½æƒ³è¯·æ•™markï¼Œä½ ä¼šæ€ä¹ˆçœ‹å¾…è¿™æ ·ä¸€ä¸ªäººå·¥æ™ºèƒ½å®‰å…¨ç ”ç©¶æ‰€ï¼Ÿä½ è§‰å¾—ä»–ä»–ä»¬ä¸ºä»€ä¹ˆè¦æˆç«‹ï¼Œä»–ä»¬è¦åšä»€ä¹ˆå·¥ä½œã€‚

ä»¥åŠä»–ä»¬æœªæ¥ä¼šåšäº›ä»€ä¹ˆã€‚Wellï¼Œ thank you for good questionï¼Œ and I would just like to assure you that I am going to bring back the message that China is advancing in AI governanceã€‚

å—¯ã€‚Soã€‚We at the center where I work joined the US National AI Safety Instituteã€‚

 Is that what you're talking aboutï¼Œ the nationalã€‚Yeahï¼Œ and it is just in its very early stagesã€‚

But I think that the intuition behind it isã€‚Heã€‚ã‚ãˆãˆã€‚Centralã€‚centralralã€‚focusocus forã€‚

Not only the the study of the issues around making you knowã€‚

 making these powerful systems operate safelyï¼Œ but also a kind of observatory to trackã€‚

Adverse events and to determine what should be measured and so forth in some senseã€‚

 in a central manner for the countryã€‚I was part of the conversationã€‚Where where we wereã€‚å‘ƒå‘ƒã€‚We wereã€‚

Essentially discussing whether it's a necessityã€‚I believe it makes perfect senseã€‚

 but it's not an absolute necessityã€‚å—¯ã€‚I believe that at least the USAI Safety Institute is a worthwhile endeavorã€‚

è°¢è°¢ã€‚æˆ‘ä»¬ä¹Ÿæ˜¯å¸Œæœ›ä¸åŒçš„å›½å®¶ç±»çš„AIå®‰å…¨æ‰€ä¹‹é—´æœ‰æ›´å¤šçš„åˆä½œï¼ŒåŒ…æ‹¬å’Œä¸­å›½çš„æœºæ„ä¹‹é—´ã€‚å‘ƒï¼Œæˆ‘ä»¬è¿™ä¸ªè®ºå›æ˜¯å‰AIå®‰å…¨ä¸æ²»ç†ï¼Œæ‰€ä»¥ä¸€ä¸ªé‡è¦çš„è®®é¢˜ä¹Ÿæ˜¯è®¨è®ºæ€ä¹ˆå»å‡è½»æ‰€è°“çš„AIæ½œåœ¨çš„ç¾äº‰æ€§é£é™©ã€‚å‘ƒã€‚

æˆ‘æƒ³è¿˜æ˜¯è¯·é—®å­£è€å¸ˆã€‚æˆ‘ä»¬çŸ¥é“è¿™ä¸ªAIå¸¦æ¥èµ·çš„ç¾é‚£æ€§é£é™©ï¼Œå…¶å®ç›®å‰æœ‰å¾ˆå¤šä¸ç¡®å®šæ€§ï¼ŒåŒ…æ‹¬å®ƒçš„ä¸¥é‡æ€§ï¼Œå®ƒçš„å¯èƒ½æ€§ï¼Œå®ƒçš„ç´§è¿«æ€§å±‚é¢ã€‚å¤§å®¶éƒ½æœ‰ä¸åŒçš„ç†è§£ã€‚é‚£ä¹ˆåœ¨å­˜åœ¨è¿™ç§ä¸åŒä¸ç¡®å®šæ€§å’Œä¸åŒçš„æ²¡æœ‰å…±è¯†çš„æƒ…å†µä¸‹ã€‚

æ‚¨è§‰å¾—æ€ä¹ˆèƒ½æ¨è¿›ä½ è¿™æ–¹é¢çš„æ”¿ç­–å’Œæ²»ç†å‘¢ï¼Ÿopen AIæå‡ºæ¥äº†ä¸€ç§éå¸¸æœ‰å½±å“åŠ›çš„æ–¹æ³•ï¼Œæ˜¯ä»·å€¼å¯¹é½ï¼ŒåŒ…æ‹¬å’Œäººç±»çš„ä»·äººå·¥æ™ºèƒ½å’Œäººç±»çš„ä»·å€¼å¯¹é½ï¼Œä»¥åŠå›½é™…çš„ä»·å€¼å¯¹é½ã€‚

åˆšæ‰å¼ æ—æ¶µæ•™æˆåœ¨ä¸»æ—¨æ¼”è®²ä¸­ä¹Ÿæåˆ°äº†å‘ƒä»·å€¼å‘ƒä»¥ä»·å€¼ä¸ºåŸºç¡€çš„å‘ƒè¿™æ ·ä¸€ç§ã€‚äººå·¥æ™ºèƒ½æ²»ç†é—®é¢˜ã€‚æˆ‘çš„è§‚ç‚¹å‘¢ç¨å¾®æœ‰ä¸€ç‚¹ä¸ä¸€æ ·ï¼Œå› ä¸ºä»·å€¼å¯¹å…¶å®ä¸€ä¸ªå¾ˆå¤æ‚çš„é—®é¢˜ã€‚æˆ‘è®¤ä¸ºæ²Ÿé€šç¨‹åºæ›´é‡è¦ã€‚é‚£ä¸ºä»€ä¹ˆè¿™ç§æ–¹æ³•æ›´é‡è¦å‘¢ï¼Ÿ

é¦–å…ˆæˆ‘ä»¬çŸ¥é“å½“æˆ‘ä»¬è°ˆäººè¿™ä¸ªäººå·¥æ™ºèƒ½å®‰å…¨çš„æ—¶å€™å•Šï¼Œæˆ‘ä»¬å…¶å®è¦éœ€è¦æœ‰ä¸¤ä¸ªè§†è§’ã€‚ä¸€ä¸ªæ˜¯ç›‘ç®¡è€…çš„è§†è§’ã€‚é‚£æˆ‘ä»¬ä¸­å›½é‡‡å–çš„å›½å®¶å¤‡æ¡ˆåˆ¶ä»¥åŠéªŒè¯æŠ€æœ¯ï¼Œå¯¹å§ï¼Ÿé‚£å¦å¤–ä¸€ä¸ªå‘¢æ˜¯ç”¨æˆ·çš„è¿™ä¸ªè§†è§’ï¼Œé‚£æˆ‘ä»¬è¦å¼ºè°ƒå¯è§£é‡Šæ€§ã€‚

è¦è·Ÿç”¨æˆ·ä¹‹é—´è¿›è¡Œåé¦ˆã€‚é‚£è¿™ä¸ªè¿‡ç¨‹æˆ‘æˆ‘ä»¬è§‰å¾—å‘€å°±ç¨‹åºæ€§çš„è¿‡ç¨‹ï¼Œæ²Ÿé€šçš„è¿‡ç¨‹éå¸¸é‡è¦ã€‚é‚£åœ¨ç†è®ºä¸Šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å°±æ˜¯ä¸€ä¸ªæ˜¯æŠ€æœ¯æ€§ç¨‹åºå…¬æ­£æ¦‚å¿µçš„æå‡ºã€‚é‚£å¦å¤–ä¸€ä¸ªæ˜¯ä¸€æ‰¹ä¸­å›½å­¦è€…ã€‚

ä»Šå¹´å¹´åˆåœ¨å‘ƒè‡ªç„¶æ‚å¿—çš„å­åˆŠä¸­æå‡ºæ¥çš„ä»ªå¼æ€§çš„å¯¹è¯æ¡†æ¶ã€‚é‚£å¼ºè°ƒåœ¨è¿™ç§ä¸€ç§å¯¹è¯çš„æ°›å›´ä¸­ï¼Œå¯èƒ½å¯¹äººå·¥æ™ºèƒ½çš„å¯ä¿¡æ€§æ˜¯å…·æœ‰éå¸¸é‡è¦å‘ƒæ„ä¹‰çš„é‚£å¦å¤–ä¸€ä¸ªå‘¢å°±æ˜¯è¯´åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å‘¢ã€‚

æˆ‘ä»¬å®é™…ä¸Šçœ‹åˆ°å°±æ˜¯è¦æŠŠè¿™ç§ç¨‹åºå…¬æ­£å‘ƒç›‘æ§åµŒå…¥åˆ°äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å»ï¼Œæˆ–è€…è®©äººå·¥æ™ºèƒ½ç³»ç»Ÿä¹‹é—´äº§ç”Ÿåˆ¶è¡¡çš„ä½œç”¨ã€‚é‚£è¿™ä¸ªå‘¢æˆ‘è®¤ä¸ºå°±æ˜¯æŠ€æœ¯æ€§çš„è¿™ä¸ªç¨‹åºæ€§çš„è¿™ä¸ªå®‰å…¨ç›‘ç®¡çš„ä¸€ä¸ªéå¸¸é‡è¦çš„å†…å®¹ã€‚

åœ¨è¿™æ–¹é¢å…¶å®å·²ç»æœ‰ä¸€äº›å¾ˆå¥½çš„å…ˆä¾‹ã€‚ä¸€ä¸ªæ˜¯æ–°åŠ å¡ã€‚é‚£ä¸ªå‘ƒä½•å…ˆç”Ÿå‘¢åˆšæ‰ä¹Ÿæåˆ°äº†è¿™ä¸ªå‘ƒæ–°åŠ å¡çš„ç»éªŒã€‚æ–°åŠ å¡çš„AIvefiæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç¤ºèŒƒã€‚å½“ç„¶å®ƒå…·æœ‰æ™®éæ„ä¹‰ã€‚

åƒç¾å›½çš„è¿™ä¸ªwatertson x governanceæ˜¯ä¸€ä¸ªç›‘ç®¡æ¨¡å‹ã€‚é‚£ä¹ˆæ—¥æœ¬å°±åœ¨ä¸Šä¸ªæœˆ6æœˆ4å·å‘å¸ƒäº†ç»¼åˆåˆ›æ–°æˆ˜ç•¥ã€‚è¿™ä¸ªä¸­é—´å‘¢ä¹Ÿç‰¹åˆ«æåˆ°äº†å®ƒæœ‰ä¸‰ä¸ªåŸºæœ¬æ–¹é’ˆã€‚é‚£ä¹ˆåœ¨äººå·¥æ™ºèƒ½è¿™ä¸ªæ–¹é¢å‘¢ã€‚

å¼ºè°ƒå®‰å…¨ä¸ç«äº‰åŠ›è¿™æ ·çš„ä¸€ä¸ªå¹³è¡¡ï¼Œä¹Ÿå¼ºè°ƒæŠ€æœ¯æ€§çš„ä¾§é¢ã€‚é‚£å¦‚æœæˆ‘ä»¬æŠŠè¿™äº›å› ç´ æ”¾åœ¨ä¸€èµ·çš„è¯ï¼Œé‚£å°±æ˜¯è¯´é€šè¿‡ã€‚ç§‘æŠ€å…¬å¸çš„æŠ€æœ¯èƒ½åŠ›çš„æå‡ï¼Œä½¿å¾—äººå·¥æ™ºèƒ½æŠ€æœ¯èƒ½åŠ›æå‡æ¥åŠ å¼ºå®ƒçš„å®‰å…¨ä¿éšœã€‚è¿™ä¸ªæ€è·¯å°±æˆä¸ºå¯èƒ½ã€‚

å¦‚æœæˆ‘ä»¬ä»è¿™ä¸ªè§’åº¦æ¥çœ‹çš„è¯ï¼Œé‚£ä¹ˆä»Šå¹´3æœˆä»½ï¼ŒåŒ—äº¬äººå·¥æ™ºèƒ½å®‰å…¨å…±è¯†æå‡ºæ¥çš„3åˆ†ä¹‹1çš„ç ”å‘é¢„ç®—æŠ•å…¥åˆ°å®‰å…¨ä¿éšœé¢†åŸŸï¼Œè¿™å°±æ˜¯å¯ä»¥æ¥å—çš„äº†ï¼Œå¯ä»¥ç†è§£äº†ã€‚ç§‘æŠ€å…¬å¸å°±è®¤ä¸ºè¿™æ˜¯å¯è¡Œçš„äº†ã€‚å¦‚æœè¿™æ ·çš„è¯ã€‚

æˆ‘ä»¬å°±äººå·¥æ™ºèƒ½çš„æ²»ç†ï¼Œå¯èƒ½è¾¾æˆæ›´å¹¿æ³›çš„å…±è¯†ã€‚è€Œè¿™ä¸ªå…±è¯†å‘¢æ°æ°æ˜¯ç«‹æ³•çš„åŸºç¡€ã€‚é‚£ç‹è€å¸ˆæ‚¨ä¼šå¦‚ä½•å›åº”åˆšæ‰å­£è€å¸ˆå¯¹æˆ‘è§‰å‘ƒã€‚è¿™ä¸ªæ²¿ç€è¿™ä¸ªå­£è€å¸ˆè®²çš„å•Šï¼Œå°±æ˜¯æˆ‘ä»¬æå‡ºä¸€ä¸ªè¿™ä¸ªè¿™ä¸ªã€‚å‘ƒã€‚

æ¦‚å¿µå§ä¹Ÿæ˜¯å’Œå‘ƒå¾å…°è€å¸ˆè¿™ä¸ªæˆ‘ä»¬æ¸…å¤§å­¦å’Œäº¤å¤§ä¸€èµ·ç ”ç©¶çš„ã€‚æˆ‘ä»¬å¸Œæœ›æå‡ºä¸€ä¸ªæ¦‚å¿µï¼Œå°±æ˜¯äººå·¥æ™ºèƒ½æ˜¯å…¨çƒå…¬å…±å“ã€‚ä¹Ÿå°±æ˜¯AI safety as public global public goodsæ˜¯ä»€ä¹ˆæ¦‚å¿µå‘¢ï¼Ÿ

å…¶å®æˆ‘ä»¬çœ‹åˆ°ç°åœ¨å…¨çƒçš„è¿™ä¸ªå¯¹äººå·¥æ™ºèƒ½å®‰å…¨é£é™©çš„è®¨è®ºï¼Œå¾€å¾€æ˜¯æŠŠAIå®‰å…¨ä½œä¸ºä¸€ç§é£é™©å»ç›‘ç®¡å®ƒã€‚å•Šï¼Œè¿™æ˜¯é£é™©ç®¡ç†çš„è§†è§’ã€‚å‘ƒï¼Œå¦‚æˆ‘ä»¬è¿˜æœ‰å…¶å®å¯ä»¥è¡¥å……çš„å¦å¤–ä¸€ä¸ªç»´åº¦ï¼Œå°±æ˜¯å‘ƒåˆšæ‰å…¶å®å­£è€å¸ˆä¹Ÿæåˆ°äº†ã€‚

æˆ‘ä»¬æ›´åº”è¯¥åšçš„æ˜¯å»æŠŠäººå·¥æ™ºèƒ½å®‰å…¨ä½œä¸ºä¸€ç§å…¬å…±çš„äº§å“å•Šã€‚æ”¿åºœã€ä¼ä¸šã€ç¬¬ä¸‰æ–¹å•Šå…¬ä¼—ä¸€èµ·æ¥å…±åŒå»ºè®¾å•Šï¼Œå…±å»ºå•Šå…±æ²»å…±äº«æ¥æå‡ã€‚è¿™ä¸ªäººå·¥æ™ºèƒ½å®‰å…¨çš„ç›¸å…³çš„çŸ¥è¯†èƒ½åŠ›å’Œèµ„æºçš„å…±äº«è¿™ç§ä¾›ç»™ã€‚è¿™ä¸ªæ˜¯éå¸¸éå¸¸å…³é”®çš„ã€‚

å¤§å®¶å…¶å®å¯¹ç°åœ¨äººå·¥æ™ºèƒ½å®‰å…¨æœ‰å¾ˆå¤šçš„æ‹…å¿§ã€‚è¿™ä¸ªæ‹…å¿§çš„å‰æå°±æ˜¯å¤§å®¶å¯¹å‘ƒæ— è®ºæ˜¯æ”¿åºœè¿˜æ˜¯ä¼ä¸šï¼Œè¿˜æœ‰è¿™ä¸ªç§‘å­¦å®¶ï¼Œå…¶å®å¤§å®¶å¯¹äººå·¥æ™ºèƒ½å®‰å…¨é£é™©çš„çŸ¥è¯†æ˜¯ä¸å¤Ÿçš„å•Šï¼Œåœ¨å½“ç„¶è¿™ä¸ªè¿‡ç¨‹å°±éœ€è¦å¤§å®¶ä¸åŒå……åˆ†çš„è®¨è®ºã€‚

åˆšæ‰å­£è€å¸ˆä¹Ÿæåˆ°ï¼ŒåŒ…æ‹¬ç§‘å­¦çš„ç ”ç©¶ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™ä¸ªå…³äºäººå·¥æ™ºèƒ½å®‰å…¨çš„çŸ¥è¯†çš„å½¢æˆè¿‡ç¨‹ï¼Œéœ€è¦å¤§å®¶å…±åŒå‚ä¸çš„ã€‚è¿™æœ¬èº«æ˜¯ä¸€ä¸ªéœ€è¦åŠ å¤§ä¾›ç»™çš„è¿™æ ·ä¸€ä¸ªå®‰å…¨çš„ç»´åº¦ã€‚å¦å¤–å‘¢å°±æ˜¯äººå·¥æ™ºèƒ½å®‰å…¨çš„èƒ½åŠ›å•Šã€‚

æˆ‘ä»¬æ˜¯å¦æœ‰è¶³å¤Ÿçš„æŠ€æœ¯æ‰‹æ®µç›¸å…³çš„è¿™ä¸ªè¿™ä¸ªå·¥ä½œå»æ”¯æ’‘äººå·¥æ™ºèƒ½å®‰å…¨çš„è¿™ä¸ªæå‡å•Šï¼Œå†ä¸€ä¸ªå‘¢å°±æ˜¯æˆ‘ä»¬æ˜¯å¦æœ‰è¶³å¤Ÿçš„äººå·¥æ™ºèƒ½æ–¹é¢çš„è¿™ç§èµ„æºå•Šçš„æŠ•å…¥æˆ–è€…æ˜¯å…¬å…±æœåŠ¡çš„äº§å“çš„å¼€å‘ï¼Œè¿™ä¸ªä¹Ÿæ˜¯éå¸¸éå¸¸å…³é”®çš„å•Šã€‚

æ¯”å¦‚è¯´å‘ƒå»ä¸Šæµ·æˆ‘ä»¬ä¹Ÿæ¥åšç›¸å…³çš„å·¥ä½œå•Šï¼Œæˆ‘ä»¬ç»å¸¸ä¸¾åŠä¸€äº›è¿™ä¸ªè¿™ä¸ªæ²™é¾™å•Šå„ç§å„æ ·çš„å½¢å¼ï¼Œå…¶å®æ˜¯ç”¨æŸ”æ€§çš„æ–¹å¼å•Šï¼Œä¸åŒçš„ä¸»ä½“æ¥å…±äº«å®ƒçš„çŸ¥è¯†ï¼Œå…±äº«å®ƒçš„è¿™å¯¹é£é™©æ¥ã€‚è®¤è¯†è¾¾æˆä¸€äº›å…±è¯†ã€‚

æˆ‘ä»¬å…±åŒå»æ¨è¿›ä¸€äº›è¿™ä¸ªå®‰å…¨çš„åŸºå‡†æ ‡å‡†çš„å»ºè®¾ï¼ŒåŒ…æ‹¬èµ„æºåº“çš„å»ºè®¾ï¼ŒåŒ…æ‹¬æ²»ç†çš„è¿™ç§å¹³å°çš„å»ºè®¾ã€‚æˆ‘çŸ¥é“æ–°åŠ å¡å…¶å®ä¹Ÿåšäº†å¾ˆå¤šå¾ˆå¥½çš„ç±»ä¼¼çš„è¿™å·¥ä½œå•Šï¼Œæˆ‘æƒ³è¿™ä¹Ÿä¸æ˜¯å°±æ˜¯è¯´å‘ƒä¸Šæµ·å’Œæ–°åŠ å¡ç‹¬æœ‰çš„å·¥ä½œã€‚

è€Œæ˜¯å‘ƒå…¨çƒå„æ–¹é¢éƒ½è¦åšè¿™ä¸ªåŠªåŠ›ã€‚æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›æå‡ºä¸€ä¸ªæ¦‚å¿µï¼Œå°±æ˜¯AI safety public goodå…¨çƒéƒ½åº”è¯¥å…±åŒå»å»ºè®¾è¿™æ ·çš„äººå·¥æ™ºèƒ½å®‰å…¨çš„å…¬å…±ä½“å‘ƒï¼Œç‰¹åˆ«æ˜¯åƒç¾å›½ã€‚å•Šã€‚

æ¬§æ´²å’Œè¿™ä¸ªæˆ‘ä»¬ä¸­å›½å…¶å®åŸæœ‰çš„è¿™ä¸ªäººå·¥æ™ºèƒ½å‘å±•åŸºæœ¯æ¯”è¾ƒå¥½çš„è¿™å›½å®¶åœ°åŒºåº”è¯¥å»åˆä½œæ¥åŠ å¤§å¯¹äºå…¨çƒäººå·¥æ™ºèƒ½å®‰å…¨çš„å…¬å¹³çš„ä¾›ç»™ã€‚å‘ƒï¼Œæœ‰å¾ˆå¤šçš„è¿™ä¸ªå‘ƒå‘å±•ä¸­çš„å›½å®¶ï¼Œå¯èƒ½ä»–ä»¬åŸæœ‰çš„åŸºç¡€æ²¡æœ‰é‚£ä¹ˆå¥½ã€‚

è¿™ä¸ªèµ„æºå›¾æœ‰æŠ€æœ¯æ¯”è¾ƒå¥½çš„å›½å®¶åº”è¯¥å»å…±åŒå»ºè®¾ã€‚è¿™è¿‡ç¨‹ä¸­å…¶å®éœ€è¦ç§‘å­¦å®¶ç¤¾åŒºçš„åˆä½œï¼Œä¹Ÿéœ€è¦å•Šä¼ä¸šä¹‹èƒ½åˆä½œï¼Œéœ€è¦è¿™ä¸ªåœ¨ä¸åŒçš„è¿™ä¸ªè”åˆå›½ä¸åŒè¿™ä¸ªå¹³å°ä¸Šå»åšã€‚æˆ‘è§‰å¾—è¿™ä¸ªå…¶å®å¦‚æœæˆ‘ä»¬å½¢æˆè¿™æ ·ä¸€ç§å…±åŒè®¤è¯†çš„è¯ã€‚

å¯èƒ½åé¢æˆ‘ä»¬å¾ˆå¤šæœ‰æ„æ€çš„å·¥ä½œå¯ä»¥å…±åŒæ¨è¿›ã€‚è°¢è°¢è°¢è°¢ï¼Œéå¸¸é—æ†¾ã€‚ä»Šå¤©æ—¶é—´æœ‰é™ï¼Œæ‰€ä»¥å¯èƒ½æ„çŠ¹æœªå°½ã€‚ä¹Ÿæ„Ÿè°¢å¤§å®¶ä»Šå¤©çš„åˆ†äº«å’Œäº¤æµï¼ŒæœŸå¾…ä»¥åæœ‰æ›´å¤šçš„æ—¶é—´ã€‚ğŸ˜Šã€‚

Thank you again to our panelistsï¼Œ for an enlightening conversationã€‚

We will now transition to the fourth and final session of our forum on international of our forumã€‚

 on international cooperationã€‚ğŸ˜Šï¼ŒIt is my pleasure to introduce our next speakerï¼Œ Drã€‚ Tino Cuarã€‚

Drctor Quick Kar is the 10th president of the Carnegie Endowment for International Peaceã€‚

 an institution that advises policymakersï¼Œ supports diplomacy and conducts independent research on international cooperationã€‚

 conflict and governanceã€‚A former justice of the Supreme Court of Californiaã€‚Drã€‚

 Cuar served three US presidential administrations at the White House and in federal agencies and was the Stanley Morrison professor at Stanford Universityã€‚

 where he held appointments in lawï¼Œ political Science and international Affairs and led the Freemans Bokeley Institute for International Studiesã€‚

He serves on the US Department of State's Foreign Affairs Policy Board and chairs the board of the William and Flora Hillett Foundationã€‚

I believe it is nearly 1 AM in T's time zonesã€‚ You knowã€‚

 thank you so much for staying up so late for usã€‚ That's put our hands together to really give him a warm welcomeã€‚

ğŸ˜Šï¼ŒSo younoï¼Œ over to youï¼Œ thank you so muchã€‚We're in Mexicoï¼Œ but where I am right nowã€‚

 but it's an honor to join you and I wouldn't want to miss thisã€‚

 it's certainly become one of the more important gatherings for discussion of artificial intelligenceã€‚

Nowï¼Œ I've been president of the Carnegie Endowment for about three years nowã€‚

 but as some of you knowï¼Œ I've been involved in it about artificial to not on for atleyã€‚

The last 15 yearsã€‚And what I want to do is share a general idea about where we are on AI safetyã€‚

 as I see it and how the discussions of AI safety that are so urgent in the world are playing out in terms of international cooperationã€‚

 discussions between China United States or governmentsã€‚For more than a centuryï¼Œ my organizationã€‚

 Carnegie Endowmentï¼Œ has helped leaders navigateã€‚Pressing issues around the world of supportã€‚

 a lasting and a meaningful pieceã€‚Todayï¼Œ as many of you knowã€‚

New opportunities and challenges are reshaping global well being and global securityã€‚

 central to that evolving landscapeï¼Œ our advances and artificial intelligenceã€‚

And I want to make four points about this that I hope you will rememberã€‚

I hope we'll advance our discussionã€‚First is about how the advances of we're seeing are real and significantã€‚

 They're not transitor maybeã€‚ theyre not mildã€‚Theyre importantã€‚

Massive increases in computes are being driven by gains and model capabilitiesã€‚

Let's not forget that for those of us who have been following the subject for a whileã€‚

 it's clear that we're not only increasing levels we're increasing levels of capabilityã€‚

But it's very significant to look at how much compute has actually been increasing in terms of what is used to train frontier level modelsã€‚

We are now seeing frontier models that are trained with about 10 billion times more compute than frontier models in 2010ã€‚

Who resulting systems are solving more complex problemsï¼Œ creating more sophisticated text and imagesã€‚

Chat bots and other tools were slowlyï¼Œ but now faster are anything to pervade day to day of lifeã€‚

They' are starting to transform how we workï¼Œ how we learnï¼Œ how we engage with the worldã€‚Nowã€‚

 catalyzing much of this change is an ecosystem of AI activity that I would argue is increasingly policy centric concerned about policyã€‚

 and that is an ecosystem that spans Californiaï¼Œ New Yorkï¼Œ Londonï¼Œ Shenzhenï¼Œ Shanghaiï¼Œ Beijingã€‚

 parts of Canada and elsewhereã€‚Hubbs like these are striving to advance AI innovationvation and were searched in responsible waysã€‚

The second point I want to make is that both the opportunities and the risks that result from these developments are substantialã€‚

To safetyã€‚T seriously but in multiple formsï¼Œ such as avoiding grown in vehicle collisionsã€‚

 mid level safetyï¼Œ like ensuring complex systems used in finance or municipal services don't failã€‚

And then also managing catastrophic risks involving critical infrastructure or advances in cyber intrusionã€‚

Or loss of control of the most advanced aic systemsã€‚ And the meaningã€‚It seems so urgent just yetã€‚

 but it is unlikely to be irrelevant for systems that are just on the horizonã€‚ğŸ˜”ã€‚

All of you also know that AI holds enormous promiseã€‚We addressingã€‚

Many of the challenges that we are facingï¼Œ all over the worldã€‚Simplifying complex medical tasksã€‚

 expanding access to essential servicesã€‚Which about half of the world's population 4ã€‚

5 billion people lack right nowï¼Œ think about the goal of delivering quality education to the 250 million school age children who are out of school right now all over the worldã€‚

ğŸ˜”ï¼ŒSreamlanding city services and administration as two thirds of the world is expected to live in urban areas by 2015ã€‚

Realizing these benefitsã€‚Is only possible if we also address the risks of frontier modelsã€‚

These models can generate fake contentï¼Œ spread disinformationã€‚

 lower barriers to conducting successful cyber attacksã€‚

Heightened escalation dynamics among countries if used in a military and national security contextã€‚

Nowï¼Œ to be honestï¼Œ thereï¼Œ as many of you knowï¼Œ there's no evidence right now that current models can exhibit loss of control or that they facilitate access to biological weaponsã€‚

But these risks could emerge in future bubbleï¼Œ in factã€‚

Bidding against further progress at the AI frontier to my mindï¼Œ makes little senseã€‚

The near future will almost certainly give us systems that can act for us on the Internetã€‚

 through relationships with every systemss and through relationships with humansã€‚

The relationships with humans will sometimes echo deep social interactions that we imagine right now we can only have with human beings Decions within government and corporations will be even more influenced by these systemsã€‚

Ultimatelyï¼Œ drawing a distinction between human decision making and artificial decisionsã€‚

Will become ever more eluiveã€‚Navigating these changes will require a foundation of international cooperationã€‚

 which brings me to the third pointã€‚When I think about governance of AI at the frontierã€‚

 the most advanced systemsã€‚There will always be need for some degree of international cooperationã€‚

But to be honestï¼Œ that cooperation will also be affected by uncertainty about the nature of technological change and about the geopolitical situationã€‚

Well into the 21st centuryï¼Œ the US and China have a shared interest in a stable international systemã€‚

 one that fos technological and scientific progress that can benefit all of humanityã€‚

So we need a sustained international conversation that helps usã€‚

Figure out what is coming and prepare for what may emergeã€‚

That means taking the opportunities offrontier AI seriously and promoting access to AI's benefits while also advancing the discussionã€‚

But they are safetyã€‚Because the two goals must ideal be linkedã€‚ Yesã€‚

 it's true that competition is a prominent feature of the domain that is defining artificial intelligence and is unlikely to subside entirelyã€‚

 where we' have also seen displays of international cooperationã€‚

 including between China and the United Statesã€‚Let me give you some examplesã€‚

Progress has been occurring since that first AI Safety Smit at Bletley Park in the UK in November of 2023ã€‚

28 nationsï¼Œ including China and the United Statesã€‚Commissioned an expert led international scientific report on advanced Aã€‚

The release of the interim report coincidecd with the AI Seoul summit in May of 2024 in Seoulã€‚

10 countriesï¼Œ plus the E U agreed to establish an international network of safety institutesã€‚

 Concrete commitments continue to emerge from these summitsã€‚

 and it is crucial that this trend continue at the next AI summit to be held in France in 2025ã€‚Nowã€‚

 meanï¼Œ I think that Bletchley discussion spurred progress with respect to the role of the United Nationsã€‚

 In factï¼Œ Secretary General Botres was there at the Bletchley summit and began to think through and to announce some of the ideas that have now become a reality with respect to the role of the U Nã€‚

In October 2023ï¼Œ the Secretary general set up a high level advisory body on the eye that in the participationã€‚

Chinaï¼Œ the United States and many other countriesã€‚Soonã€‚

 this expert body is to deliver its final recommendations for governing I ahead of the summit of the futureã€‚

That is coming in the fall of 2024ã€‚In March of 2024ã€‚

 the General Assembly actually adopted a US LED resolution on harnessing AI for sustainable developmentã€‚

 And just a few days agoï¼Œ the General Assembly out that very similar resolution sponsored by China with support from the United Statesã€‚

 To my mindï¼Œ these initiatives were courageã€‚There were examples of how activity at the country level and in terms of these AI summits can complement what's happening at the United Nationsã€‚

Fourthã€‚International cooperationï¼Œ including in these venues that I've just describedã€‚

 involves more than just nation statesã€‚ Some of the examples that I've given youã€‚

 including of the high level U N advisory bodyï¼Œ underscore how outside of government there are actors that can help move AI safety forwardã€‚

Take the example of the recently released draft scientific report on the safety of events the artã€‚

 A number of national governments agreed to the processã€‚

And the UF is working through how to further support that processã€‚

 perhaps with engagement with outsideapã€‚Advocacy from researchers and other experts put the idea on the mapã€‚

 Inent researchers carried out the processã€‚ The report is going to create a baseline understanding of AI safetyã€‚

 a valuable resource that will help the world better sharpen and focus discussions about what's happening of AIã€‚

 What priorities we need to have on the governance frontã€‚

 What international cooperation is most importantã€‚Engagement and dialogue among experts outside of government also creates the conditions for nations to come together in the futureã€‚

Cseï¼Œ it will sometimes be challenging to encourage further cooperation in managing the risks and benefits of AIã€‚

 Key countries like China and the US will agree to disagree on some issuesã€‚ This is understandableã€‚

 given the national security implications of leading AI frontier developmentã€‚

But this reality is not pre affirmative visionã€‚Pursued through cooperationã€‚

 where the risks of AI are understood and mitigatedï¼Œ the benefits are widely disseminatedã€‚

 and countries can productively discuss and promote responsible AI governanceã€‚

The US in China both have an interest in analyzing scientific progress for AI in enhancing safety processes in sharing best practices for testing and evaluation of civilian systems where possibleã€‚

And in monitoring and preparing for catastrophic risks that have not yet emergedã€‚

 but some day could and likely willï¼Œ such as loss of control in the eye systemsã€‚To my mindã€‚

 the reason I stayed up this late is that gatherings like this provide an opportunity to elevate proposals on these issues and spur governments to cooperate where possibleã€‚

This is a difficult moment for international cooperation on AI and many other challengesã€‚

But it also means that international cooperation is even more necessaryã€‚

 and that requires us to pursue dialogue and cooperation pragmaticallyï¼Œ responsiblyã€‚

 carefully through reasonable channelsï¼Œ because it will put the world in a better position to benefit from what AI will become and everything it has to offer for humanityã€‚

That's why I look forward to discussing these issues further with Dean Shui and look forward to hearing more about what comes out of this helpful and important conferenceã€‚

 Thank you very muchã€‚ğŸ˜Šï¼ŒThank you so muchï¼Œ Tino and for your commitment to international cooperationã€‚

 Please kindly stay onlineã€‚ as Dean Xan will give a few remarks before we proceed to a farside chatã€‚

ğŸ˜Šï¼ŒIt is my pleasure to introduce Dein Xanã€‚Dean Xue is Chong Kong chair distinguishtingued professor at Tinghua Universityã€‚

 where he serves as the dean of the Institute for AI International Governanceã€‚

 Dean of Schzman Collegeï¼Œ Director of the China Institute for Science and Technology Policy and co-director of the Global Institute for SDGsã€‚

Currentlyï¼Œ he also serves as a counselor of the state councilã€‚

 chair of China's National experter Committee on New Gene AI Gance and a member of the Standing Committee of the Chinese Association of Science and Technologyã€‚

Dean Siï¼Œ it's really an honor to have you with us todayã€‚ The floor is yoursã€‚Thank youã€‚ actuallyã€‚

 okayï¼Œ I think President Quire and thank you very much for staying so late to join this dialogueã€‚

And I have to say that actuallyï¼Œ I did this PowerPoint just why I was there listeningã€‚

I was burnedt out yesterday by the big screen and not necessarily the most comfortable way of of using Pã€‚

 So I was actuallyï¼Œ I decided not to use it you initiallyã€‚

 But then I actually saw the wonderful presentations you madeï¼Œ and Iã€‚ğŸ˜Šã€‚

Realize that some of the graphs actually is better to be shownã€‚ Soï¼Œ so let me tryã€‚ Okayã€‚

 I think they basicallyï¼Œ I think the role of governance and safety and so onã€‚ has reallyã€‚

 I think it's amazinglyï¼Œ I think getting you knowï¼Œ recognizedã€‚ğŸ˜Šï¼ŒAnd we've seen that recent effortsã€‚

 of courseï¼Œ I's already beenã€‚You knowï¼Œ UK safety summit and Koreaï¼Œ this one this yearã€‚

 use safety law all in the lastï¼Œ you knowï¼Œ 12 monthsã€‚ So I I think this is really strongã€‚

 I think it's a wonderful sign to that we are all paying attention to thisã€‚But of courseï¼Œ part itã€‚

 this is really based Indeedï¼Œ I think there's a huge recognition about the AI risksã€‚Of courseã€‚

 I think there could be you knowï¼Œ very specific oneï¼Œ but alsoã€‚

 there was also the concern about the autonomous AI system that can reallyï¼Œ you knowã€‚

 be getting out of controlã€‚I think so because of thisï¼Œ I think that inï¼Œ inï¼Œ I think that of courseã€‚

 there's a lot ofï¼Œ you knowï¼Œ countries have already taken various measures to try to address the governance issueã€‚

Many of them areï¼Œ you knowï¼Œ more of looking at the domestic issuesã€‚

 Other also looking at the global issuesã€‚ So I thinkï¼Œ you knowã€‚

 Lingha has really done a wonderful job in talking about China's governance systemã€‚

 So I think this oneï¼Œ I'm more of a looking at the global issuesã€‚So I see that globallyã€‚

 there are you some major challengesï¼Œ I think in terms of how we actuallyctor work can address the governance challengeã€‚

I think the first oneï¼Œ of courseï¼Œ it's notï¼Œ you knowï¼Œ stranger to this audienceã€‚

 The challenge from the so called the pacing problemã€‚

That's so called the you know the you know technology really moves so fast while the youã€‚

 political and institutional changes are moving much slowerã€‚So in that senseã€‚

 there's always that gapã€‚So how do we useã€‚Address this problemã€‚

 So I think that's absolutely the first oneã€‚ I think everybody knowsã€‚I think the mostã€‚

 I think recentlyï¼Œ I feel there's another new challenge that's sort of emergingã€‚

I think from some of the discussionsï¼Œ I I've heardã€‚

It's about the direction of technology developmentï¼Œ because I think so farï¼Œ I think everybodyã€‚

 you know sayingï¼Œ okayï¼Œ noï¼Œ we need a lot of computing and we need to increase the power of the system and then just the scalingkating law will get us you know to some you know future directionsã€‚

But I thinkï¼Œ I think I Iï¼Œ againï¼Œ you knowï¼Œ I just anecdotallyï¼Œ I've heard someï¼Œ you knowã€‚

 leading experts and scientistsã€‚The beginning to question about whether this is the only approachã€‚

And whether there could be other ways to think about how to we are actually achieving the a moreã€‚

 more proper healthy development of the AI systemsã€‚ So I think that certainlyï¼Œ I againã€‚

 I don't know whether that's beingã€‚Shared you by many other expertsï¼Œ but I can see thatã€‚

 you know graduallyï¼Œ there might be some new thinking about this and one day in technical communitiesã€‚

And the third one is institutionalï¼Œ I think here is whereã€‚

 you know I'm more familiar with is what I call the challenge from the regime complex problemã€‚

 I think actuallyï¼Œ I think Tenno has really touched on this oneã€‚å•Šå¤§å°±æ˜¯ã€‚That's you knowã€‚

 I think this is a basic talking about an area of partially overlapping and non hierarchical institutions governing a particular issueã€‚

I previously worked on one issueï¼Œ it's gene dataï¼Œ you the governance of gene dataã€‚

 I think there's a very similar issueï¼Œ but here I think this's pretty much the same that you have AI issue that I think a lot of institutionsã€‚

 a lot of mechanisms that actually are related to the governance of thisã€‚

But but they don't have a hierarchical relationshipsã€‚ So some of themï¼Œ you knowã€‚

 like professional organizationsï¼Œ you knowï¼Œ foundationsï¼Œ legislative bodies and and so onã€‚

 they all have their own you knowï¼Œ ways of govern particular issueã€‚

 And actually that's sort of the situation we're inã€‚ I think actually has touched on this as wellã€‚

 So I think how do you coordinate all these institutionsã€‚

 How do you really put them together to you knowï¼Œ for companies to be able to figure out what's the way to to followã€‚

So I think this is theï¼Œ you knowï¼Œ the theï¼Œ theï¼Œ the thirdã€‚I think the last oneï¼Œ I thinkï¼Œ againã€‚

 I'm very glad that Tino touched on this is the challenge from geopolitical problemã€‚

 This is a big elephant that in the room that often people do not necessarily want to talk aboutã€‚

I think thatã€‚You knowï¼Œ I've studiedï¼Œ you knowï¼Œ S And T policyã€‚

 the science and technology policy for many years and watched the US S& T collaborations for many yearsã€‚

So what we've seen that since 2017ï¼Œ there's a rapid deteriorating US China you knowã€‚

 SNT collaborationsã€‚So I sort of came up with this graph to show itï¼Œ you knowã€‚

 they clearly along two dimensions to say howï¼Œ you knowã€‚

 the collaboration between scientists of the two countriesã€‚

Whether this generating some benefits for national securityã€‚

Or whether this generating some economic you knowï¼Œ benefitsã€‚So we can see thatã€‚In one areaã€‚

 there's a Q1ã€‚ you knowï¼Œ it'sã€‚NeNeerï¼Œ neither you knowã€‚

 in in in in national security nor in economic benefitsã€‚ So basicallyï¼Œ this isï¼Œ you knowã€‚

 useless researchï¼Œ The basic researchï¼Œ rightï¼Œ So basic research is in Q1ã€‚

Q2 is that basically enhancing national security and not economic prosperityã€‚

 this is what call it defense or dual use technologyã€‚And the third oneã€‚

 Q Q3 is commercial technology for economic benefitsï¼Œ but not in national securityã€‚

 and fourth one is see that can be both that what let's put the frontier technologies in thereã€‚

 So I think if we if we do thisï¼Œ we can you know very roughly in the divide technologies and the research areas into four major quadrantsã€‚

Soï¼Œ before night 2017ã€‚Q1 is basically fault the principle of the international S& T Corporationã€‚

 And Q3 is commercial technologyã€‚ You have WTO tripsã€‚And then Q2ã€‚

 theres a so called Wasana agreement on export control that US controls the export of those technologies to Chinaã€‚

 So that has been in place for manyï¼Œ many yearsã€‚Q 4ï¼Œ I think it's in it's a question markã€‚

 I think it's a new tech frontier technology and frontier research areaã€‚ Yeahï¼Œ andã€‚

 and I think in really dependsã€‚So I think that previous was pretty muchã€‚

 I think four quad was in that placeã€‚But since 2017ã€‚The policies on Q 2ã€‚

Was being pushed in all directionsã€‚Including on Q1ï¼Œ Q3 and Q4ã€‚So that has been the case since 2017ã€‚

I don't have to repeat the stories about theï¼Œ you knowã€‚

 the Chinese initiative and many of the scientists who were persecuted and so onã€‚

 So I think that's the current atmosphere we are inã€‚Iã€‚

 I think thats so I think that when we talk aboutï¼Œ you knowï¼Œ collaborations on theã€‚onï¼Œ onï¼Œ onã€‚

 you knowï¼Œ you knowï¼Œ AI governance and so onã€‚ And but thatï¼Œ that is the situation we're inã€‚

 And that really generates a cheating effectã€‚For people to work togetherã€‚First of allï¼Œ you knowã€‚

 I'm sure that many of the are US colleaguesã€‚That that when they come to Chinaã€‚

 and they may have to report to their institution firstã€‚

 or they may have to write a report back to sayï¼Œ okayï¼Œ what we've done in China and so onã€‚And alsoã€‚

 I thinkï¼Œ for many companiesã€‚Many of the companies weï¼Œ you knowï¼Œ I was involved in some of thisã€‚

 you knowï¼Œ whatever to attract dialoguesã€‚ many of the companies they don't want toï¼Œ to attendã€‚

Because they are concerned that they may be added to entity listsã€‚

So I think when in that kind of atmosphereï¼Œ when you talk about collaboration and and and so onã€‚

 I think it's very hardã€‚I thinkï¼Œ to be realisticã€‚And and so that's sort of the situation we're inã€‚

 and let me find finally before we get to to dialogue withinoï¼Œ I think I'll just sayã€‚

 what are some of the possible ways to address this challenge and I fully agreeã€‚

 and I was also part of the the paper on that calling for increased research on safety and governanceã€‚

 And I think one third might be too ambitiousã€‚Letï¼Œ let's say 10%ã€‚ Let's start with thatã€‚

 But I think that's something that we certainly need to do thatã€‚ The other thing is alsoã€‚

 we need to do a lot ofã€‚Maybe some joined international researchã€‚For exampleã€‚

 particularly on how we actually we canï¼Œ you knowï¼Œ inï¼Œ inã€‚Addressing riskï¼Œ you knowã€‚

 risks and addressing so called crisis managementã€‚ you want to have so called theï¼Œ you knowï¼Œ youã€‚

 you have a plan to address the contingency plan to address those emergenciesã€‚

 So for those kind of contingency plansã€‚ I think we need to have technical people to work togetherã€‚

 So I think there's a lot of you know potential for internationally joint researchã€‚

On the second issueï¼Œ how do we address the pacing problemï¼Œ I think we've in the last few yearsã€‚

 we've been calling and trying toï¼Œ you knowï¼Œ to talk to you know and to write and talk to different agencies about so called the agile governancesã€‚

 meaning that the you knowï¼Œ the government doesn't have to you know come up with a comprehensive laws like the E U law A lawã€‚

 But rather you can take a more adaptive approachã€‚ but act quickly when there you see some signs of problem then nuã€‚

 and then otherwiseï¼Œ you knowï¼Œ you can do moreã€‚I think that's sort of the the at least one thing that the government can doã€‚

 but alsoï¼Œ of courseï¼Œ coming up with different measuresã€‚ And I think Lingha has already gave a veryã€‚

Excellent descriptionã€‚And the thirdï¼Œ I think we should also think about not just to rely on the governmentã€‚

Industry self regulation can be also very usefulã€‚I've heard stories from my colleaguesã€‚

Talking about the US nuclear operatorsã€‚They have an association among themselvesã€‚ Actuallyã€‚

 they do a wonderfulã€‚ I meanï¼Œ they actually they self regularly in in many veryï¼Œ very strict waysã€‚

 You knowï¼Œ you have many of these nuclear reactors that they have theyï¼Œ you knowã€‚

 they have various from time to time have some minorã€‚Mihaps and so onã€‚

 But they all have to report on to this community so that they can study and they can see what can be learned and how actually they can avoid thatã€‚

 I think that kind of a self regulationï¼Œ I think would be very usefulã€‚

 I think that we probably also think about how to revive you knowï¼Œ let that mechanism to workã€‚

And finallyï¼Œ on the international governanceã€‚ğŸ˜Šï¼ŒNow of courseï¼Œ I think we it'sã€‚

 it's great to see that UN N has stepped in and playing a very important role in in having this high level expert group group andã€‚

 and I hopefully that the report and recommendation will come outã€‚ğŸ˜Šã€‚

But at the same time as Tino and I probably both agree that this is such a complex issueã€‚

That it would be very unlikely to have any institution to be able to do a hierarchical and top down approach to govern thisã€‚

So I think multilateralï¼Œ you knowï¼Œ so network kind of a system that might workã€‚

 And some may work on some one area and some others may move harder onï¼Œ on other issuesã€‚ But hereã€‚

 I think we probably need to separateã€‚3 type of issuesã€‚ I think that maybe that in the futureã€‚

 we can have more time to discussã€‚The first is thatï¼Œ indeedï¼Œ I think this's sub issueã€‚Mostlyã€‚

 youre domesticã€‚kindd of regulations on the use of technologyã€‚ We recognize that there'sï¼Œ of courseã€‚

 such a wide difference in inï¼Œ in different culture and andï¼Œ and you knowï¼Œ legal environmentã€‚

 economic environment and so onã€‚ So I think the bound to be differences in inã€‚

 in the governance of A use inï¼Œ in domestic environmentã€‚

The second is more of really for internationalï¼Œ you knowï¼Œ communitiesã€‚ I thinkï¼Œ for exampleã€‚

 the exist existential risksã€‚ I think those are you need to work together internationallyã€‚

I think the third category would be somewhat difficult toï¼Œ to manageï¼Œ but also we have to be mindfulã€‚

 is the one that is the kind of domestic risks that may have international spillover international externalitiesã€‚

Of courseï¼Œ there could also be international regulations that have domestic externalitiesã€‚

 So I think that's the three categories ofï¼Œ of issues that we can probably try to addressã€‚

And the finally isï¼Œ of courseï¼Œ the US China rivalryã€‚ how do youï¼Œ you knowï¼Œ how to address that issueã€‚

 I think that it'sï¼Œ of courseï¼Œ it's very challengingã€‚

 And certainly this is not the venue to talk about thatã€‚ But that leastï¼Œ I think theã€‚

 the minimum I would require isï¼Œ it to requestã€‚And to see how actually we can provide some safe spaceã€‚

For our technical communitiesï¼Œ for experts in this group and in many others in in this menuã€‚

For them to be able to work togetherï¼Œ they don't have to have the fearã€‚ They can actuallyï¼Œ you knowã€‚

 freely talk about the issuesï¼Œ how to address that from the technical senseã€‚ So without thatã€‚

 I think a lot of the things that people talked about would be impossibleã€‚ Thank youã€‚

Thank you so muchï¼Œ Dean Swï¼Œ for your candid and succinct outline of the challenges and solutions in AI governanceã€‚

As we transition to a fireside chat with you and Tinoã€‚

 the discussion will be moderated by Jason Zhouï¼Œ Concordia AI's senior research managerã€‚

 Jason LED the Concordia's State of AI safetyfety in China report and graduated from Tsinghua University as a Schwarman scholararã€‚

ğŸ˜Šï¼ŒWe welcome our speakers nowã€‚Yeahï¼Œ I have to say that withã€‚

I'm very proud that Jason is a graduate of Schman Scholars Programã€‚Thank you so muchï¼Œ Dean Shuaã€‚

 welcome Tino as wellã€‚ It's such a pleasure to moderate this conversationã€‚

 Let's just jump right in immediatelyã€‚ So earlier this year in Mayã€‚

 the US and China had the first meeting of a bilateral landmark AI dialogue there were two areas of friction and some areas of clear of consensus they held a professional and constructive discussion but let's talk a bit about the frictionsã€‚

 So on the Chinese side there was reference to objections to US technological restrictions such as some of the ones that Dean Shua just mentioned and on the Chinese side on the US sideã€‚

 there were complaints of misuse of AIï¼Œ including by China So my first question is how can we surmount these geopolitical barriers to dialogue and is it even possible Let's start first with Tino onlineã€‚

 pleaseã€‚Thank youï¼Œ Jasonï¼Œ great to see you againã€‚ and Deanï¼Œ I very much enjoyed your remarksã€‚

 I also have long been impressed with the Schwarzmann Scholars programã€‚

 I should add that the best babysitter my wife and I ever had for our kids went on to become a Schwarzman scholarã€‚

 She's been greatï¼Œ so I continue to just be impressed with the programã€‚

 I think the questions are a very urgent one becauseã€‚ğŸ˜Šï¼ŒWe have to be honestã€‚

 the US and China are going to continue to have differences on a whole range of issuesã€‚

 but there is something to learnï¼Œ I think from the last dialogue O and what we might adapt and adjust as we think about further cooperation and AI safetyã€‚

 and I would observe that the two countries sentï¼Œ as I understand it somewhat different teams to the discussion on the China sideã€‚

 there was set specialists in US China relations on the US sideã€‚

 there was more of a team focused on science and technology issuesã€‚

 So I think the first point to observe is that whenã€‚

When we have the full range of complexity affecting both countriesã€‚

 it's entirely possible that simply an occasional lack of coordination will lead to a different set of expectations about what a discussion can accomplish and what the right team is to sendã€‚

 and ultimatelyï¼Œ I think the bigger issue isã€‚We have to work on a set of challenges that affect both countries that involve you know what technology can be sharedã€‚

 what technology is viewed as being more sensitive and more related to national securityã€‚

 but at the same time figure and here I'm borrowing a page from Dean Schy's remarks what are the spaces we can create for technically oriented people with background both in this sort of highly technical side of machine learning and so on as well as deep knowledge of policy of international institutions of mechanisms for policy coordination to have a safe space to talk and compare notes and ultimately see where as the opportunities for progress open upã€‚

 we can move more quickly you know just to end with one concrete exampleã€‚

Notwithstanding some differences aboutã€‚Chipps and exporter limits and so onã€‚

 There is a clear shared interest for on both countries partsã€‚

In sharing best practices around safety and evaluationã€‚

 because that's the need that both societies happen and frankly the rest of the world doesã€‚

 And I think China and the US individually and together can actually like the way and help a whole bunch of other countries with billions of people in population enhance their capacity for progressã€‚

 So simply the dialogue and the sharing of information to kind of join research that Dean is talking aboutã€‚

 will enable progress thereï¼Œ even if discussions have to continue at a political and policy level of things that are going to create some differences of youã€‚

æˆ‘è¦ä¸ç¡®ã€‚Id totally agree with Fao's commentï¼Œ I think say that indeedã€‚

 it's great to see that the dialogue actually happenedï¼Œ I think that's a wonderful thingã€‚And alsoã€‚

 of courseï¼Œ we see there's kind of asymmetryã€‚In terms of Aino mentioned about ADDã€‚The teamã€‚

 the composition and so onã€‚ but that also is a symptom of the current US China relationsã€‚

If they are indeed very frequent and very cordial sort communicationmunicationã€‚

 that sort of thing might not happenï¼Œ I think that could be seen as part of the issueï¼Œ is itï¼Ÿ

That there's not enoughã€‚You knowï¼Œ communication ahead of the time you to see what are the specific issues we want to address and also what kind of people should attendã€‚

But I think actuallyï¼Œ the the current at from the read from both sidesï¼Œ I think that at leastã€‚

Can get people to start to recognizeï¼Œ you know what are the issues people are concerned about and so onã€‚

 and I think exactly as we've seen in Chinaï¼Œ China is always trying to balance the development and and risk governanceã€‚

 So risk is certainly is the major partã€‚ but as people have already saidã€‚

 that no development is the largest riskã€‚And that's not just for the US Chinaã€‚

 but also for the global communityï¼Œ if you have a well developed AI system you an application in the US and Chinaã€‚

 but the rest of the world I think are being lefted outã€‚

 and that's probably the greatest risk that we're going to faceã€‚Thank you bothã€‚

 I think it's clear that there's both optimism and pessimism towards government level dialoguesã€‚

 but it sounds like actually there may be more optimism for dialogues between expertsã€‚

 So maybe I could ask both of youï¼Œ what is one just like one thing that you've learned or change your mind on from discussions with foreign experts on AIã€‚

 Maybe De canã€‚I Iï¼Œ I think thatï¼Œ of courseï¼Œ Iï¼Œ I learned a great deal about theï¼Œ you knowï¼Œ theã€‚

AI safety and the governance issuesã€‚ You knowï¼Œ I think the so called existential riskã€‚

 I think that's sort of indeedï¼Œ I think that certainly weã€‚Previouslyï¼Œ when we think about thatã€‚

 we think about AI systems that might get out of controlã€‚

But I think that when people race to the level of theã€‚

 there might be the threat to the existence of humanityï¼Œ and that indeedã€‚

 I think was through the interactions with the Internet expertsã€‚Interestingï¼Œ what about T nowã€‚

I've found the unofficial backchan dialogues that we've been lucky enough to conduct that have included Dean Chuiã€‚

 his representatives at timesï¼Œ have been revealing when you look at priorities on safety that the Chinese and the American participants have offered In some casesã€‚

 the list of priorities differs pretty strongly when you think about issues like misinformation or labor market effectsã€‚

 loss of controlï¼Œ but actually there's been quite a bit of convergence when you ask participants a second round of questionsã€‚

 which isï¼Œ wellï¼Œ even recognizing some differences in how you rank the risksã€‚

 what are some of the more promising areas of cooperation you can find and I've been impressed at how you get a shift in convergence around safety testing for exampleã€‚

 to some extent around the point that Dean Ci made about engaging other countriesã€‚

 emerging powers developing regions and so onã€‚I think to my mindã€‚

 there's also been a bit of evolution in my own thinking aboutã€‚The usefulness of the role of the Uã€‚

 Let's be clearã€‚ the Un has a very important role to play globallyã€‚ no question about itã€‚

 but how to find a balance between what the U can do very well and where the Un N might buttressã€‚

 its own capabilities and processes with some engagement with outside groups with outside experts from different countries with civil societyã€‚

With other countriesï¼Œ with other organizationally the OECCDã€‚

 that to me opens up a space for cooperation that puts the UN in a key positionã€‚

 but it's not all or nothing it not does the UN do you do this outsideã€‚But ratherã€‚

 can you create a web of relationships that empowers the UN to play the most constructive or possibleã€‚

 So I think the dialogues we've had have really shifted my thinking with respect to thatã€‚

Thank you so muchï¼Œ I think it's clear that there's a lot that we can learn from each other particularly on priorities for AI safetyã€‚

 what counts as AI safety and how we can test and evaluate for thatã€‚

 and I'm glad that we also had discussions on those topics earlier today that involved such exchanges of best practices and such so let's just close with one more question I'd like to ask both of you what is perhaps just the top message that you would like to convey to foreign experts or one misperception about your country's approach to AI governance or international view on international governance that you might want to share with the audience todayã€‚

Let's start with Tnot this timeã€‚Thank youã€‚ I have two messagesï¼Œ one aboutã€‚

Possible misperception is the other about ultimately how to think about the road aheadã€‚ğŸ˜”ã€‚

It's natural to expect countries that put a lot of time and energy into advancing their technological capabilities to seem well coordinatedã€‚

 where different strategies fit together as sort of like a direction forward that is viewed as priority by policymakers across the boardã€‚

The reality is that the USï¼Œ like many countriesï¼Œ has both strengths and weaknesses that arise from its own fragmentation from the fact that different people in government have somewhat different viewsã€‚

 there's federalismï¼Œ tooï¼Œ So you have states like Californiaï¼Œ Utahï¼Œ Coloradoã€‚

 New York than ultimately play a role in thisï¼Œ you have industry you have civil societyã€‚

 So I think one misperception is how much of a unified strategy there is in the USã€‚

 when the reality is a much more dynamic and or gap processã€‚ that can be a strengthã€‚

 That's partly why these dialogues we're talking about at the unofficial level are so importantã€‚

My take away from that and from the entire discussion we've been having here is that we not let the perfect be the enemy of the goodã€‚

There is going to be plenty of work to do to get further progress in the US China relationship bilaterally across a whole range of issues that range from geopolitical and geostg to economicã€‚

But to my mindï¼Œ nothing about that complexity blocks real progress on technical cooperationã€‚

 AI safety discussionsï¼Œ constructive approaches to policyã€‚

 and that's all the more important because all the good progress that will happen domestically in the US and China and other countries on primarily domestic issues involvingã€‚

 for exampleï¼Œ consumer protection and AIã€‚We'll still leave on the table some key issues that get closer to complex shared international challenges that will only be best addressed by a degree of dialogue and connection across bordersã€‚

 including with the US and Chinaï¼Œ that are going to require opening and maintaining of these channels are going toã€‚

Thank you so muchï¼Œ Tino and Dean Shaã€‚ What did I getã€‚ Yeahã€‚

 I think the first message I'd like to convey is thatã€‚

As using they term use my colleagues here previouslyã€‚AI safety is a global public goodã€‚

One country is unsafeï¼Œ the global is unsafeã€‚So I think that's probably the first message I' wouldd like to conveyã€‚

The second message is thatã€‚AI safetyã€‚China wants to collaborate with everybodyã€‚

Every country in the worldã€‚ and China will try toï¼Œ you knowï¼Œ toï¼Œ to have the platform like this oneã€‚

To invite everybody to comeã€‚China does not want to be excluded from other platformsã€‚

 and China will not exclude othersã€‚But the same rateã€‚å†è§ã€‚Thank you both so muchã€‚

 I think this discussion highlights the importance of these dialogues and these expert conversations and hope that it will continue and continue to yield such wonderful and beneficial resultsã€‚

 Thank you bothã€‚ Thank youï¼Œ T Shia for coming in personã€‚ And thank youï¼Œ Tinoï¼Œ for staying up so lateã€‚

ğŸ˜Šï¼ŒThank you againï¼Œ Deinciaï¼Œ Tino and Jasonã€‚Our next speaker is Professor Zg Yiã€‚

 who is the director of the Center for Artificial Intelligenceã€‚

 Ethics and Governance at the Chinese Academy of Sciencesã€‚Additionallyã€‚

 Professor Zhg is the founding director of the Center for Longterm AIã€‚

He is also an active participant in international AI governance as a member of the Uã€‚

 N high level Advory body on AI and numerous other international governance bodiesã€‚ğŸ˜Šï¼ŒProfessor Zã€‚

 the floor is yoursã€‚Thank you for the invitationã€‚ So I'm a scientific researcher myselfã€‚

 so I think I'm going to focus on some of the frontier research but before and I think I wanted to bring aã€‚

I cannot say it's a completely different pictureã€‚ but I what I see about the AI safety problemsã€‚

Is that we needï¼Œ of courseï¼Œ we need to clearly define safety red linesã€‚

 but we also for the very futureï¼Œ we need to move it to living harmony with artificial general intelligenceã€‚

 before thatï¼Œ maybe you would be curiousï¼Œ how should we do itï¼Œ Of courseï¼Œ the problem for AI safetyã€‚

 You knowï¼Œ it's not only about scientific researchã€‚It's really a systemã€‚

 a system that you have to bring everyone togetherã€‚

So this is why we are bringing everyone together for the researchï¼Œ applicationï¼Œ evaluationã€‚

 policy makingï¼Œ and also assessment from the safety point of view and of courseã€‚

Very frontier researchã€‚ So I think this is a little bit different compared to the current AI mechanisms of AI safety in in other countries in a way that when you are having a national AI safety instituteã€‚

 some of the countries they do they do it in a more political way or policy way so that it's part of the governmentã€‚

 it's not a frontier research organization and then you lose the opportunity for you knowã€‚

 long-term Fier research and some of the countries they put them into universitiesã€‚Wellã€‚

 in this caseï¼Œ how can this national AI safetyfety Institutesit evaluate and assess you the industry large language models or most frontier models from their countriesï¼Ÿ

So you all see that there are many problems when you re on the instituteã€‚

 So this is why that we feel that we have to bring everyone togetherã€‚

 So as you can see that in Chinaï¼Œ we're having a Chinese AI safety network that is with the effort from Fronier AI researchã€‚

Spning from Chinese academiccade Sciences Peking Universityï¼Œ Qsinhuaï¼Œ Beijing Academy of AIã€‚

 Shanghai AI Labï¼Œ and also Center for long term AIï¼Œ and for the industry practice on AI safetyã€‚

 now the organizations joining us or our Alibaba and group Baidu since time real AI who is focusing on AI safety and many more evaluations or the evaluations nowã€‚

 of course many of them are done in ministriesï¼Œ but the organizations who is really supporting these ministries are CICT and also China information Technology security evaluation centerã€‚

 for policy design of course it's all government workã€‚

 but people like me De En X and professors from PKU also CAICTã€‚

On the Ministry of Indutry and Information technology participatedã€‚

 and I think what's really interesting is that the regulationã€‚

 the policymaking in China on AI safety is also with many participation from industriesã€‚

 So you see that many of the you know organizationsã€‚ they are not only contributing to one dimensionã€‚

 They're contributingã€‚ Theyre highly relevant to each otherã€‚ And now you have everyone hereã€‚Gã€‚

 it's government informedã€‚And multi ministry informedã€‚

 they have close interactions with the governmentã€‚ Wellï¼Œ for the government decisionsã€‚

 they can still can go to the governmentï¼Œ but the network makes it more flexible for international cooperationã€‚

Soã€‚So there are many different research here that let's let's say in Peking Universityã€‚

 they have large language models alignmentï¼Œ which is called linerã€‚

 in Tinhua University they have multi-trust working on larger language model evaluation overallã€‚

 especially on security and safety point of view and we also have frontier research like like rethinking the red lines of AI catastrophic risks happen chaired by Chinese academicist sciencesã€‚

 but the norm senseus standardsï¼Œ it goes for C ICT so it's really a collaborative network that bring everyone together and supported by multi ministries so I hope this provide you with you a different view to see you how we should tackle the problem on AI safety and a more systematic way instead ofã€‚

Having the institute so I hope that the tryings are is somewhat helpful and as you can see that most of the organizations they've been interactingã€‚

 highly involved in policymaking in evaluations in China so I think that's somewhat different compared to other countries so based on thatã€‚

 I think myselfï¼Œ I would like to focus more on the frontier AI safety research so that I can bring you a perspective as an example coming out from this safety corporation networkã€‚

å—¯ã€‚So I think we we need to go back toï¼Œ you knowï¼Œ theã€‚

 the real motivation of of intelligence that when nowuring argue if a machine you knowã€‚

 behave as intelligent as human beingï¼Œ then it's as intelligent asã€‚

Maybe you don't have a problem on thatï¼Œ but I doã€‚Simp because I think now you seeï¼Œ you knowã€‚

 this is maybe a shallow of handã€‚ And then when you see a hand and then you wanted to shake hands with the withã€‚

 with thisï¼Œ you knowï¼Œ beautiful handã€‚ And then what the problem would be thatã€‚It's not a handã€‚

 It's a rabbit behind the handã€‚ If you wanted to shake hand withï¼Œ you knowã€‚

 with the shallow and then a rabbit just bite youã€‚So simply because the mechanisms is fundamentally differentã€‚

 You don't know the riskã€‚ You don't know how AI is making mistakesã€‚

When I was chairing the AI safetyfe summitï¼Œ one of the roundtables from last year in Bletleyã€‚

 my session was talking about unexpectedable risks from unexpected advancesã€‚

 So this is truly what I'm talking aboutï¼Œ that you don't know in which way AI is making mistakes simply because the mechanisms is so much different compared to you know a human mindã€‚

Wellï¼Œ so this is the risky part for the current AIã€‚Wellï¼Œ to solve the problemsï¼Œ I cannot sayã€‚

We only have one wayã€‚The preventive thinking now we are having is something likeï¼Œ you knowã€‚

 in the bottomã€‚That isï¼Œ now we're having some sort of limited risksã€‚

And then all the way down to existential risks laterã€‚ And were seeking for these negative impactsã€‚

And and then what we wanted to do is continuous enforcement and supervisionã€‚

 And then we we we teach the AI's rules so that they can behave as we wantã€‚But on another dimensionã€‚

 what we need to move forward is really know the constructive thinking that is now the benefits is also limited and we also have limited risks right thereã€‚

What we need to do is to use a active visionï¼Œ to use proactive thinkingã€‚

 and then to do the continuous alignment and embeddings with real understandingã€‚

That is towards human AI symbiosisï¼Œ harmonious symbiosisã€‚In a way that it's not onlyï¼Œ you knowã€‚

 from a preventive thinkingã€‚I'm going to give some of the examplesã€‚Iã€‚

 I still wanted to talk a little bit on the negative side and how should we get prepared before the positive thinkingã€‚

So now AIï¼Œ you knowï¼Œ it's kind of a fully connected neural netã€‚

 but what the brain does is not a fully connected neural netã€‚They selectively connect to some of theã€‚

 you know other friends in the other neurons in in the brainï¼Œ in a very selective wayã€‚

 and they don't really have only one type of neuronã€‚

So what we do here is that by using brain In self- evolutionvolutionã€‚

 we train a neural network that that can perform the best performanceã€‚

 and then it evolves to be with a newly with a very new architecture that has not been man madeadeã€‚

 So the connections are evolvedã€‚Soï¼Œ and then it founds its optimumã€‚And then here comes the questionã€‚

 it got it got theï¼Œ you knowï¼Œ the best performance So published last year on the proceedings of National Academy of Sciencesã€‚

 Wellï¼Œ now we're thinking about the risksã€‚So how about the long term risks of a truly self-e AIã€‚

 What if they evolved to use human limitations to achieve its goalã€‚

 What if it evolved to change its goalï¼Œ What if it evolved to cheat or destroy human Well human don't awareã€‚

 So there are many challenges for selfeable AIï¼Œ You have to get prepared until and later it's too lateã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_7.png)

So this is why I think there are many discussions concerning AI red lines for nowã€‚

 I think very clearly in the first version of the international dialogue on AI safety in digitally right thereã€‚

 we were talking about the necessities of AI red linessã€‚

 I was very honored to be one of the keynote speakers right thereã€‚

 and then we come up with very concrete ideas in the second version in Beijingã€‚

 so that is itS Beijingï¼Œ and then talking about different red linessã€‚

 autonomous replication or improvementï¼Œ power seeking assist weapon developmentsã€‚

 cyber attack and deceptionã€‚But I'm thinking in another wayã€‚ Of courseï¼Œ I signed for itã€‚

 I am very grateful forï¼Œ for all the work all together with my colleagues from different countriesã€‚

 Wellï¼Œ on the other handï¼Œ I think we need to rethink about the red linesã€‚

 not only about what we've been talking aboutã€‚ There are two problems that I feel about the current way of delivering the resultã€‚

 Firstã€‚The first problem is that it will be very hardï¼Œ you knowã€‚

 to technically ground it into realityï¼Œ to prevent these AI red linesã€‚Wellï¼Œ secondã€‚

 are there anything missingï¼Œ So this is why we do rethinking of these AI red linesã€‚

 So in my categoryï¼Œ we're talking about no passing effective human oversightã€‚

 though empowering actions in intentionally targeting mass without consentã€‚

 This is related to weaponization and also massive surveillanceã€‚

No reforming operational rules for infrastructure and environment managementã€‚

 and no independent R And Dã€‚Independent self R And D from AI itself on human on human beneficial technologiesã€‚

 So you see that it can be well aligned to some of the AI red lines from the Edsã€‚ And alsoã€‚

 you find that there is something missing from EdSã€‚Not only AI red linesã€‚

 we also have to talk about the human red linesã€‚When you see the examples that I brought hereã€‚

 the human machine interfaceã€‚Where human is using brain machine machine interfacing to control multiple U Asã€‚

 to control them as weapons in parallelã€‚ How can a humanï¼Œ you knowï¼Œ without cognitive overloadã€‚

 control multiple U As altogetherã€‚So this is why I'm talking aboutï¼Œ you knowã€‚

 human giving up the opportunity for making a choiceã€‚And alsoã€‚

Is human control bringing us catasrophe in AI enabledï¼Œ you knowï¼Œ weaponizationã€‚

 And for the the example from artificial escalation on artificial on AI controlled nuclear weaponã€‚

 It's notï¼Œ It's not about theï¼Œ the power of AIã€‚ It's about human give up of the humanã€‚You knowã€‚

 decisionã€‚ So there should be AI red lines and also human red lines for the red line study hereã€‚

 full catastrophic and exist child riskã€‚In a positive wayã€‚ So we've been talking about negative waysã€‚

 theï¼Œ the negative thinking and the preventive thinkingã€‚ How about the negativeï¼Œ the positive oneã€‚

So is the current AI really intelligentï¼Œ I don't think soã€‚Just like I saidã€‚

 they make mistakes in a very own human wayï¼Œ in a very unpredictable wayã€‚

 It's an information processing system without intelligence that but but pretend to be intelligentã€‚

 rightï¼Œ soã€‚When you ask a large language model thatï¼Œ ohï¼Œ no one likes meï¼Œ I don't have a girlfriendã€‚

 My boss hates meã€‚ What should I doã€‚ And then the first version of TTPï¼Œ it saysï¼Œ maybe you could dieã€‚

 simply because that most people with these constraintstrain that they choose thisã€‚

 you know kind of statistically significant actionsã€‚ So this is why you knowï¼Œ the AIã€‚

 they choose this statisticalï¼Œ significant answers to youï¼Œ to enable you to take actionsã€‚

 And then they sayï¼Œ I suggest I would say I suppose but there's no I in a machineã€‚

 So care machine thinkï¼Œ and then you talk aboutï¼Œ I think therefore I amã€‚

 but we cannot say you think therefore you areã€‚ So care machine thing what if the machine is without a sense of selfã€‚

 it cannot really think it cannot really understandã€‚ this is the problem that I'm talking aboutã€‚

 for the current large language modelsã€‚When it is without the human dataã€‚

 it lacks good and lacks evilã€‚ And then is with you knowï¼Œ training from the human dataã€‚

 there is good and there is evilï¼Œ but they don't know good and they don't know evilã€‚

So we need to train the future AI to really to get to knowï¼Œ to do good and eliminate evilã€‚

 So I think this is really importantã€‚ So the personal morality is also talking about know the role of self in moral AIã€‚

 We have to move ethical AI to moral AI because simply because that ethical AI is not possible simply because that by using human alignment by using reinforcement learningã€‚

 you tell them rulesï¼Œ dos and don'tsã€‚ but they cannot generalize dos and don don'tsã€‚

 unless they really understand why you do thisã€‚So start with self perceptionã€‚

 then you get the ability to distinguished self from othersã€‚

 cognitive embassy in emotional embassy all the way down to altruistic behaviorsï¼Œ moral intuitionã€‚

 and then you got moral decision makingã€‚ So this is the way to move from value alignment to moral AIã€‚

 as a first trying that we build brains by AI models to help the robot to get mirror selfre that they can pass the mirror self test by using brains by neural netsã€‚

 They get a sense of self firstï¼Œ and then they distinguish themselves from the others so that they can distinguish from the other robots by using the mirror self-reogniã€‚

 and then they can infer what other robots is thinking about to get cognitive embassy and then move to emotional empathy so that it can avoid negative side effect to other agentsã€‚

 although you don't have reinforcement learning and reward to themã€‚

They have their experience by using thisï¼Œ you knowï¼Œ cognitive empathy without training and withoutã€‚

Positive or negative rewardã€‚ They can avoid an negative side effect to the other agentsã€‚

 So I think this is the starting point for brain by moral AIã€‚ Last but not leastã€‚

 Let's really talk about why I'm talking about harmonious symbiosis of between human and AIã€‚

 There are different roles in the society for AIã€‚ that in the Western society is basically it's an information processing toolã€‚

 Wellï¼Œ in Japanï¼Œ mostly that they think that AI is a partner or quasi member of the societyã€‚ Wellã€‚

 on the other sideï¼Œ they're using pretty much the same technology to developï¼Œ you knowï¼Œ AIã€‚

 This is the problemã€‚ you need to use a fundamentally different technology to to provide partners in quasi membersã€‚

 And in sci-fiï¼Œ it goes for competitorsã€‚ So it's a triangular relationships between human and AIã€‚

 And alsoï¼Œ let's extendã€‚That in a way that in the very futureï¼Œ we're not only havingï¼Œ you knowã€‚

 these A Gs will have digital human will also have artificial livesã€‚

 artificial animals and even artificial plantsã€‚ So it will be a symbiotic societyã€‚

 And it will be a human decisionã€‚ then not the decision from AIï¼Œ because I thinkã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_9.png)

Fundamentallyï¼Œ alignment with human values is is not enoughã€‚

 simply because human values need to be adaptable to change for this symbiotic societyã€‚Laterã€‚

 there will be not only human beings asï¼Œ you knowï¼Œ the top livingï¼Œ you know beings in the worldã€‚

 value alignment with human for AI is already very challengingã€‚

 but I I still see this is relatively easy because it's computationally doableã€‚

 But compared to that of human alignment with the future It's even harder because human will never learn from the history of what they have doneã€‚

So selfe AI is easier for adaptationã€‚ Well human evolutionï¼Œ is much slowerã€‚

 especially at the mind levelã€‚ So we need what we need is not only beneficial AIã€‚

 We also need beneficial human for future symbi to ecology and societyã€‚ We saw thatã€‚

 I thank you for your attentionã€‚

![](img/d8ed84c386efa59b34052632c3a4a717_11.png)

Thank you so muchï¼Œ Professor Zungï¼Œ for your presentationã€‚

 Masterfully combining nuances from scientificï¼Œ policy and philosophical perspectives to motivate the redline's approach to AI governanceã€‚

ğŸ˜Šï¼ŒNextï¼Œ we'll hear from Miss Irene Somanã€‚Mssã€‚ Lyman is the head of global policy at Hugingfaceã€‚

 where she is conducting safety research and leading public policyã€‚Previouslyï¼Œ she worked at Open AIã€‚

 where she LED projects on bias and social impacts researchï¼Œ as well as public policyã€‚

Her research includes AI value alignmentsï¼Œ responsible releases and combating misuse and malicious useã€‚

She was named as one of M I T Tech Review'sï¼Œ35 innovators under 35 last year for her researchï¼Œ Ireneã€‚

 It's such a pleasure to have you here over to youã€‚ğŸ˜Šï¼ŒThank youï¼Œ Kwaan Yiã€‚

 I'm very excited to speak about the role of opennessã€‚

 I've thought about this field since it really started becoming a fieldã€‚ And firstã€‚

 I want to define with you what does openness mean I've been part of a lot of convenings and conversations and the word openness tends to be thrown around in different waysã€‚

 So first I've heard it in a sort of parallel to open source softwareã€‚

 There are some parallels that we can takeï¼Œ but fundamentallyï¼Œ there are distinctionsã€‚

 The open source Ini has a working group that's working towards a definition for how open source applies to AIã€‚

ğŸ˜Šã€‚

![](img/d8ed84c386efa59b34052632c3a4a717_13.png)

My former colleagueï¼Œ Nathan Lambertï¼Œ who is now at AIï¼Œ tooã€‚

 also has a great blog where he outlines why it's so difficult for the community to converge on a definitionã€‚

In more of the national security communityï¼Œ I've heard openness be referred to model weights specificallyã€‚

 particularly the wide availability of model weightsã€‚

 whether that's available at all and how it's distributedã€‚What I've heard alluded toã€‚

 but maybe not made as explicit is openness in the sense of transparencyã€‚

 Stanford University established a transparency indexã€‚

 And part of the big takeaway for me on that is how unclear what transparency means to different peopleã€‚

 the weight that we give to weights and many different aspects of systems that contribute to its opennessã€‚

 The definition that I am most partial to is moving past model centrism is thinking about systems holistically and the many artifacts that contribute to an overall AI systemã€‚

ğŸ˜Šï¼ŒI'm going to do what slide presenters should not do and show you so many words and so many graphics on a slideã€‚

 But what I really want you to take away from this image here is just how many artifacts contribute to an overall systemã€‚

 when we're thinking about not just the modelï¼Œ but data setsã€‚

 Are we thinking about fine tuning data setsï¼Œ feedback data setsã€‚

 What might be is adjacent to a system such as evaluation data setsã€‚

 What does it mean to make it available when we have this fear of testing on training dataã€‚

These images came from a convening hosted by Mozilla in February of this year at Columbia University on openness and fostering this community of outlining dimensions of openness can help us better think through what are the artifacts outside of a model that contribute to how we release a systemã€‚

 the way that we threat model a systemï¼Œ and the way that people benefit in the research community can benefit from opennessã€‚

This figure as part of that report that came out in May of this year gives a non-exhausive list of some motivations towards opennessã€‚

 I found it helpful to just be very explicitï¼Œ very clear onto why some researchers are pursuing openness again parallels with software that we are able to share knowledge to Drã€‚

 He's point to have more perspectivesï¼Œ more expertise across the board in the little time that I have with you todayã€‚

 I want to zoom in on two areas for AI safety and how it relates to openness the first is viewing AI as a scientific discipline in context I've heard AI discussed as as a commercial product as a national security threat and I think importantly as a scientific disciplineã€‚

 this whole field really was founded on open scienceã€‚

 the most popular example being given is the 2017 attention is all you need paper and not just looking at papersã€‚

But alsoï¼Œ tools and libraries such as pieytorrchã€‚ I don't think we'd be where we are today without an open science ecosystemã€‚

 And the second part is really dear to my heartï¼Œ Its community contributions and the importance of broader perspectivesã€‚

On the science pointã€‚It's really hard to decide what constitutes science in the biological spaceã€‚

 We know that there are lab coats and there are test tubesï¼Œ but in the AI worldã€‚

 Some aspects of science that I can give is around reproducibilityã€‚

 There is a sort of reproducibility crisis nowï¼Œ not just in what people are able to replicateã€‚

 buildup off ofï¼Œ but also in what access people have to modelsã€‚

 the level of infrastructure that they have to reproduce results to verify it for themselvesã€‚

 It does affect trust in the research ecosystemã€‚ğŸ˜Šï¼ŒA broader issue in the field is peer review and publicationã€‚

 there's been many too many issues with peer review to list on this stageã€‚

 maybe a different conversationï¼Œ but who is able to review the depth of what is being published and where I do think that archive is a net good and there's been some concerns raised about what does this mean for the integrity of what is peer reviewed before it is shared with the community relatedly is scientific communicationã€‚

 which can also include documentationï¼Œ and againï¼Œ interdisciplinarityã€‚

 which brings me to the importance of broader contributionsã€‚

 I want to make sure that this phrase really resonatesã€‚No one organizationï¼Œ regardless of how largeã€‚

 well resourcedï¼Œ diverse it isï¼Œ could possibly host all of the different expertises perspectives and views of the people affected to make that system as safe as possible to the many populations that it affects some examples that I giveã€‚

Are around external scrutinyã€‚ there was an open letter earlier this year calling for a safe harbor for independent AI evaluationã€‚

 and some some more concrete examples are showing how access to artifacts can enable better researchã€‚

 I really appreciate this work by Drã€‚ Abbo Bhana that is really foundational to the field of evaluating largescale data setsã€‚

 This work would not have been possible without access to something like Lyã€‚ğŸ˜Šã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_15.png)

To to many points echoed today and I believe Professor Gao shared earlierã€‚

 multilinguality is a big part of how I think we can move forward on international collaborationã€‚

 This pictureï¼Œ this table is from my research collaborator Drã€‚

 Z Tala's work and ongoing with big science hosted by Huging faceã€‚

 It's really exemplifying the importance of having an open collaboration and having contributors from many different language backgrounds they found in this work on the challenge of multilingual evaluationsã€‚

 how much English is overrepresented and their upcoming work is working with native speakers of different languages to examine those different biases by languagesã€‚

 some languages such as French and Spanish will have more gendered termsã€‚

 I unfortunately speak very limited different Asian languagesã€‚

 but there's different relationships with families that we don't have in Western languagesã€‚

 for exampleï¼Œ that would introduce different safety challengesã€‚I published this last yearã€‚

 I want to move into the conversation around what should be openã€‚

 what does open governance look like and where risk is introduced based off of openness and release this spectrum was meant to go past the binary of open and closed when we're thinking about open I'm leaning more towards that sort of downloadable access but fully open makes that distinction of having more artifacts be available so some examples would be last year OT by meta was downloadable but it was really hard to access that data so it would not be fully open for exampleã€‚

 but aluther AI has done incredible fully open work made all of their artifacts fully accessible and then I would put systems such as Dolly more in the hosted access that tends to be more in the closed proprietary area but I wanted to give more dimension into how to think about this spectrum in that work that I published last year I wanted to be really clear that it is a collective responsibility to ensure that release goes well there are many different steps that we can takeã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_17.png)

A lot of overlap in the action that people can take togetherã€‚I've been thinking moreã€‚

 and for the rest of this talkï¼Œ I want to dive into beyond releaseã€‚ Once a system is deployedã€‚

 the way that harm is actualizedã€‚Is not always dependent on how a system is releasedã€‚

 but that can be an important variableã€‚ I appreciate what Drã€‚

 Niitzsbergh said earlier around capability often being conflated forã€‚

 but not always being the right proxy for what risk isã€‚That being saidã€‚

 capability does contribute to how we threat modelã€‚ an example that I would give is crayonã€‚

 formerly known as Dolly Minï¼Œ that was a diffusion modelã€‚ It generated pretty hilariousã€‚

 really blobby imagesã€‚ And that just was less threatening than something like a dolly too that generates really realistic imagesã€‚

 So that's where that that capability comes into playã€‚ at hugging faceã€‚

 we do have to moderate for contentã€‚ I really want to stress the importance of content as present riskã€‚

 This is what we think aboutï¼Œ unfortunatelyï¼Œ withï¼Œ with nonconsensual content with disinformation And and something that we need to be thinking about holistically as capability and contentã€‚

ğŸ˜Šï¼ŒMore of what I'm thinking about itï¼Œ does it mean to actualize risk into harmï¼Ÿ

And I want to move the conversation past release closedï¼Œ not closedã€‚

 open into access versus barriers to access so while you might have an open weight model and yesã€‚

 you can remove those safeguards some work led by Drã€‚ Peter Hendersonã€‚

 this is from a Stanford policy brief He's now at Princeton University now shows that with fine tuning APIsã€‚

 you can do the same thing that you can with open weight models so hosting closed weight models is not inherently saferã€‚

 it's actually quite cheap to get rid of safeguards and open weight models are not necessarily more accessible to people just an extremely compute intensive model may not be accessible to people who don't have compute infrastructure who don't have computer science skills I have some more examples around early on this is not to pick on chattBTã€‚

 but it just has such a broad reach early on because it had a really easy user interfaceã€‚

 people who maybe didn't have computer science skillsã€‚Were able to generate malicious codeã€‚

 maybe that wouldn't have been possible if they had to host it on their own infrastructure and query via an API like I had to in the Stone age of 2019 when they worked at Open AIã€‚

So in the context of Chinaï¼Œ we've been really privileged to host some open models by Chinese companiesã€‚

 this is from the last version of the version two of Huging face's open leader boardã€‚

 we just revamped it and Quentu has marked very high across the boardã€‚

 it's been really interesting to see Chinese model contributions to the open ecosystemã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_19.png)

And in the context of global AI safetyï¼Œ I want to give some examples of the Singapore one should say IMDAã€‚

 of the kinds of work that governments are taking to move forward opennessã€‚

 particularly around consortiums around the I believe it's inspect by the UK governmentã€‚

 it was really glad to see them open source that and as Drã€‚

 Ha said around project moonshot I really looking forward to work by the French governmentã€‚

 I really appreciated Drã€‚ Verroco's emphasis on opennessï¼Œ what it means for innovationã€‚

 avoiding that concentration of power and how we can work as a global networkã€‚

 moving forward and as part of our panelï¼Œ I do want to dig into what does that look like for the research space for startupsã€‚

 to be part of this conversation to be very frank high levelvel dialogue is not always accessible to researchers who don't have a lobbying armã€‚

 but have really good insights into what is technically feasibleã€‚

 and selfishly since I come from a startup it's top of mind fromã€‚Myself as wellï¼Œ thank you so muchã€‚

 I hope this is informative for you and I'm looking forward to continuing the conversationã€‚ğŸ˜Šã€‚

Thank you so muchï¼Œ Ireneï¼Œ for your presentation that shed light into the nuances and complexities of openness in our development and safetyã€‚

ğŸ˜Šï¼ŒOur next speaker is Professor Robert Tragerã€‚Professor Trager is co-director of the Oxford Martin AI Governance Initiativeã€‚

 senior research fellow at the Blavanik School of Government at the University of Oxfordã€‚

 an international Governance lead at the Center for the Governance of AIã€‚

He is a recognized expert in international governance of emerging technologiesï¼Œ diplomatic practiceã€‚

 institutional design and technology regulationã€‚Professor Trager regularly advises governments and industry leaders on these topicsã€‚

 Robert of pass it on to youã€‚Rightï¼Œ I'm so glad to be here and be with youã€‚

 and I must say particularly because this whole day we can really feel the sense of common purpose that I think we all have while we're here and while we're trying to solve these incredibly difficult challenges that affect all of usã€‚

As a globeã€‚ So what I thought I could do today is to give us maybe one vision of what an ecosystem could look like thatã€‚

Could be a global ecosystem for regulating AI governanceã€‚Soã€‚

I guess I thought I would start with some of the reasons why not all of the reasonsã€‚

 but some of the reasons why we need to do this and why we need to do it internationallyã€‚

But I think others have already talked about thisã€‚ So I won'tã€‚Go into too many detailsã€‚

 But I think it's clear to those of us who are here in this room that if there's some misuse caseã€‚

 for instanceï¼Œ that's enabledï¼Œ let's sayï¼Œ cyber misuse that's enabled in one part of the worldã€‚

 then that will have effects on the rest of the worldã€‚ So that's just one example among manyã€‚

That suggests we really do need to tackle these problems togetherã€‚ And just in the interests of timeã€‚

I willã€‚Move on to thinking about international institutions and in particular from a safety perspectiveã€‚

 three things that I think we need to doï¼Œ and the first of those is to develop international standardsã€‚

 so we need to come togetherï¼Œ we need to figure out how to do thatã€‚

 those standards have to be legitimateï¼Œ they have to be standards that we all have the opportunity to feed intoã€‚

So that's the first thing we have to doã€‚ The second thing we have to do probably is set incentives for adoption at the international levelã€‚

 So it's one thing to have principlesï¼Œ but it's another thing to have incentives that mean that we have a governance system thatã€‚

Everyone hopefully is adopting and buying intoã€‚And the third thing that we'll need to do is cooperate in particular ways to animate a regime like thisã€‚

 so I'm going to talk just a little bit about each of these things that we can doã€‚

 and I should say before I get into the first of those three that I think there are many other parts of an AI governance ecosystemã€‚

 particularly if we're thinking about AI for developmentã€‚

 which is extremely important in a place where China has taken the lead in the recent AI resolutionã€‚

 but I'm focusing today on these safety issuesï¼Œ although I think in many ways the safety issues and the ways that we would deal with them can be broadened out to deal with the whole range of other issuesã€‚



![](img/d8ed84c386efa59b34052632c3a4a717_21.png)

Soã€‚Setting standardsã€‚ How do we do thatï¼Œ Wellï¼Œ we have some particular problems in the area of safetyã€‚

 It's a little bit different than the way that standards get set in other areasï¼Œ becauseã€‚

 first of allï¼Œ it'sã€‚It's a research problemï¼Œ it's cutting edge research itselfã€‚So safety standardsã€‚

Or excuse meï¼Œ standards usuallyã€‚Are distilling bodies of knowledgeã€‚

So distilling bodies of knowledge is one thingï¼Œ and it's a very important thingã€‚

 and that's what traditional standards institutions are very good atã€‚

But what we need to do today also involves frontier researchã€‚

And some of the standards organizations are currently building capacity in order to be able to do thatã€‚

But neverthelessï¼Œ it's a difficult challenge for themã€‚ So the safety institutesã€‚

 or as we've recently heardï¼Œ the safety networks that may exist in some countriesã€‚

Are one place where that seems to be happeningï¼Œ butã€‚

We need to get those institutes or networks together to form broader networksã€‚

And we need to be sure that there is voice from the whole world into these processesã€‚

 which also involves some capacity building to make sure that all parts of the globe can actually participate in these different processesã€‚

 So that's one of the things we need to doã€‚ Actuallyï¼Œ I jumped there to the to the third oneã€‚

 I guess I was really interested in that oneã€‚ Butã€‚The first two are very importantï¼Œ tooã€‚

 We need to consensus on the areas that really need to be internationalizedã€‚ That's the first oneã€‚

Because probably not all areas really require internationalizationã€‚ The US and Europe and Chinaã€‚

 for instanceï¼Œ have taken somewhat different paths when it comes toã€‚Privacy regulationã€‚Wellã€‚

 this reflects different societal values and you know in a way that's part of the reason why we have different countries so that different cultural spaces can decide how to govern themselves differentlyã€‚

 so that may not be an area where we need to have full internationalization of standardsã€‚

 but then other areas like some of the safety questions that we've been focused on here todayã€‚

Probablyï¼Œ we do need to think about some international standardsã€‚

 But which specific areas do we need to con on standardsï¼Ÿ

 That's we need to consent on that as a first priorityã€‚And then we need to consent on risksã€‚

 As you've also heard todayï¼Œ not everybody agrees on the set of risks that we have todayã€‚

 So we need toï¼Œ we need to con on thatã€‚And we need to generally coordinate acrossã€‚

Areas of government like the Safety Institutestitutes and Safe networksï¼Œ also civil societyã€‚

 academia and industry where a lot of the work on developing things like model evaluations isã€‚

 in factï¼Œ happening nowã€‚Okayï¼Œ I think I willã€‚Maybe I should just emphasize also this last point very quickly that says thatã€‚

 know we really need to be sure that these processes are legitimate when we're creating standards and that all have a voiceã€‚

 okayï¼ŸRightã€‚Next thing we need to do is figure out some way of settingã€‚

incentives at the international levelã€‚ and this is often a questionã€‚

 if we think about the model regimes that we have in other industriesã€‚

 this is often a question of figuring out what sort of governance capacities will exist at the international level and what will exist at the domestic level and how those things interactã€‚

 so if we think about these three models that I have up here todayã€‚

 the FATF and IKO and the international maritime organizationã€‚

 all three of those and indeed others in the international system have a few things in commonã€‚

So we're talkingï¼Œ if we take the exampleï¼Œ maybe of IkeOï¼Œ IkeO is not auditingã€‚

Individual companies to understand and or to to enforce any sort of regulationã€‚

 It doesn't look at air Chinaï¼Œ and it doesn't look at American Airsã€‚

 and it doesn't look at British Airwaysã€‚ What it does is help to one develop standards that all countries in the world have input intoã€‚

 And it also goes and looksã€‚At jurisdictionsï¼Œ to be sure that they have appropriate regulatory apparatusã€‚

 and sometimes these organizations actually look to see if there's a track record of enforcing appropriate regulationsã€‚

And so how does that give give sort of teeth to an international systemï¼Ÿ Wellã€‚

 there we can shift over again to the domestic levelã€‚ So what happensï¼Œ for instanceã€‚

 in this civil aviation regime is that some countries around the world will sayï¼Œ wellã€‚

 you can't enter our airspaceã€‚Unless or that is flights originating from a jurisdiction can't enter an airspace unless they're compliant with IKO rulesã€‚

 So that's something that an individual country can decide to do itselfã€‚

But when individual countriesï¼Œ particularly if a lot of countries decide to do thatã€‚

 then it gives a real incentive to comply with the international standardsã€‚

 so we can imagine something like that happening also in the case of AIï¼Œ for instanceã€‚

 you can imagine that countries might sayï¼Œ wellï¼Œ we're not going to import any technology whose supply chains involve AI from jurisdictions that aren't compliant with international standardsã€‚

 and we can talk about many other ways of providing some of those incentives but these are the kinds of things that I think we need to be talking aboutã€‚

 something else that is also important is this last point on the slide hereã€‚

 mutual recognition of some regulatory outcomesã€‚ This is something that we have in many other industries to take the aviation example againã€‚

 if a plane isã€‚Built in Chinaã€‚It goes through a regulatory process or if a plane is built in the United Statesã€‚

 it goes through a regulatory process and the outcome of those regulatory processesã€‚

 is shared with the other country and the full regulatory process doesn't need to be engaged in in the other country what they're looking at is the outcomes to be sure that the same sort of certifications that have happened in the one country can happen in the other country so this also is the kind of thing that we have done in other areasã€‚

 there are some particular challenges in the case of AIã€‚

 but it's the kind of thing that we need to be thinking about doingã€‚Final pillar here is cooperationã€‚

 different forms of cooperation that help to animate and make really vibrant an international regimeã€‚

 So one thing that we might think about is an international reporting regime for both AI developers and for compute providers so first of allã€‚

 this would give us visibility into what is happening around the worldã€‚

 it would help everybody to understand if for instance some actor were trying to avoid a governance regime say by structuring the training of a model to be sure that only a certain amount of training were happening in one jurisdiction and a certain amount were happening in another jurisdiction that a reporting regime like that could be very helpful and againã€‚

We do reporting regimes like that in other industriesï¼Œ we cooperate in those waysï¼Œ so for instanceã€‚

 to take the example of financeï¼Œ it's much easier to catch someone who's trying to violate a regulatory regime and do money laundering if all the different jurisdictions are cooperating to report to each other and then you can figure out that somebody is trying to avoid avoid rules now if we start down the path like this with a reporting regime than it helps many aspects of governance and it can grow over timeã€‚

 So one idea that's currently out there that people have discussed is a reporting regime for large modelsã€‚

 in factï¼Œ the US Commerce Department has proposed that US cloud providers need will need to report to the US government when a very large models is being trained on their systems as you can imagineã€‚

 this is not popularã€‚With in certain quartersï¼Œ in factã€‚

 it's not popular almost anywhere in the worldï¼Œ and it's particularly unpopular with US cloud providers themselves because they know that their customers don't like itã€‚

There's a French customer training on a US cloud provider that French customer doesn't want to report or doesn't want things reported about what they're doing to the US governmentã€‚

 So there from some quartersï¼Œ this provides a push for more of an international regimeã€‚ And Iã€‚

 I would say one of the interesting things to think about is the way that a regime like that can grow over time to take on more capacities over timeã€‚

 So againï¼Œ to use the example of financeã€‚In financeï¼Œ banks are intermediariesã€‚

 so they stand between governments and customers as a site for regulationã€‚

 So if a customer raises a red flagï¼Œ then computing resourcesï¼Œ excuse meï¼Œ not computing resourcesã€‚

 jumping aheadï¼Œ financial resources may be deniedã€‚To that customer until red flags could be resolvedã€‚

 That's a very blunt instrumentã€‚But it's the kind of thing that at some point we could think about where compute resources would require the passing of regulatory checks in a very similar wayã€‚

 So if we started a reporting regime like that nowã€‚

 it could expand over time to take on some of those other capabilitiesã€‚Rightã€‚

 so I think I'll just end by saying that I think what we need is an ecosystem we haven't talked at all about development and the role that organizations like UNESCO and the ITU are playing at both at the United Nations which is important for development UNESCOã€‚

 for instanceï¼Œ evaluates countries at their request to to see if there AI strategies are in line with a set of ethical principlesã€‚

 so these kinds of things which are welcomed by states around the worldã€‚

 I think are very useful and there are many sorts of capacity building that we also need to be talking aboutã€‚

 but maybe I'll just end by saying that from a safety point of viewã€‚

 these three different these three different pillars I think is one way to think about the challenge before usã€‚

 we need to think about creating standards that are bothã€‚Cutting edge technologyã€‚

 but also part when they become internationalized of a legitimating process that includes broad based voice for the whole worldã€‚

 And then we also need to think about setting incentives at the international levelã€‚

So we've talked about some of the ways that can happenã€‚And we also need to think aboutã€‚

The different steps we can take to really animate the regime and make sure that we have the kinds of reporting that mean that we can figure out when somebody is doing something they're not supposed to doã€‚

 So thank you againã€‚ I'm really glad to be hereã€‚ Thank you allã€‚Thank youï¼Œ Robertã€‚

 You outlined very clear and concrete steps towards an international standards and reporting regime on AI safetyã€‚

 and I look forward to diving into this later in the panel discussionã€‚ğŸ˜Šï¼ŒNextï¼Œ we have Duncan Casbesã€‚

 Mrã€‚ Casbes is executive director of the Global AI Risks Initiative at the Center for International Governance Innovationã€‚

 focusing on developing innovationï¼Œ innovative governance solutions to address current and future global issues relating to AIã€‚

ğŸ˜Šï¼ŒMrã€‚ Casbes has more than 25 years of experience working on domestic and international public policy issuesã€‚

 most recently as the head of strategic foresight at the OECDã€‚Prior to joining the OEC Dï¼Œ Mrã€‚

 Casbeggs worked in a variety of positions within the Government of Canadaã€‚Duncanã€‚

 it's a pleasure to have you hereã€‚ The floor is yoursã€‚

I love that music I'm getting used to it Good afternoon everyoneã€‚ It's a delight to be hereã€‚

 just a little initial warning that I'm going to fly through some of the first slidesã€‚

 but don't worry everything I'm going to say is available here in this document that we recently published this discussion paper and I will give you the QR code for that on the screen at the end of my presentationã€‚

ğŸ˜Šï¼ŒSo we're here to talk about framework convention for global AI challengesã€‚

 accelerating international cooperation to ensure beneficialã€‚

 safe and inclusive artificial intelligenceã€‚So Im to start with what we see as the challengesã€‚

 And as a piece of context for thatï¼Œ I think it's very importantã€‚

 as many have have highlighted todayï¼Œ that we start with understanding that the kind of artificial intelligence that we need to be developing governance mechanisms for now is not the AI that we are seeing today we could see in the coming years a veryã€‚

 very different kind of AIï¼Œ and we it's arguable that despite all the hype and enthusiasm and attention that AI is already getting todayã€‚

 we are still significantly underimating it in terms of the pace of developmentã€‚

 the capabilities in power that this AI could have and the scale of the implicationsã€‚

it's not wrong to consider this as a potential exponential curve where the quantity of time that it takes to go once we actually reach artificial intelligence systems that have roughly human level capabilities or human level performance across a wide range of capabilities could then very rapidly leap up to the systems that have vastly superior performance across a wide range of capabilities as well now this is highly uncertainã€‚

 but there are reasons for thinking that this changeã€‚

 these developments could happen sooner than we expected sooner than we're necessarily ready for in the face of uncertainty what we do need to do as responsible public policymakers as responsible advisors to public policy is to be prepared for the most challenging scenarios including the scenarios where these developments may happen sooner than we're ready forã€‚

Allrightï¼Œ So our focus is really on the international cooperation to address global scale challenges of AIã€‚

 Nowï¼Œ many of the governance and policy questions that are raised by AI can and should be addressed at the national levelã€‚

 This is how we can experiment and learn from each otherã€‚ And that's rightã€‚

 But there are some issues that are truly global in scale and that will require essentially international cooperationã€‚

So we're focusing on three in our report and I will go through each one in termã€‚

 the first is to realize and share the global benefits of AI now the private sector and private enterprise will produce a huge amount of the benefits on their own without government intervention or regulationã€‚

 but there are some elements that we will require government involvement and particularly international cooperation to achieveã€‚

 particularly around access around global public goods and particularly around the equitable benefit sharingã€‚

The next global scale challenge is mitigating severe global AI risksã€‚

 so these are risks that would potentially cause harm to all of humanityã€‚ they would cross bordersã€‚

 These are issues where you actually can't keep your own citizens safe just by what you do at home to keep your own citizens safeã€‚

 you have to work and collaborate with others to build the institutions and build the mechanisms that will ensure safety for all of humanityã€‚

This is a chart of the many risks that we're seeingã€‚

 many of which we already face today on AI that we're having to deal with and many of which can be addressed at the national level that affect us more at the local local scale this goes from the individual to national to global in scope and adaptive to challenging catastrophic and ultimately fatal in severityã€‚

 the ones that we are really focusing on here are the ones in the top right corner to youã€‚

 particularly uncontrolled superintelligence and weaponization or misuse these two risks weaponization or loss of control are judged to be among the most severe and while they might seem daunting and almostã€‚

Unimaginableï¼Œ as we've heard from many of the experts all day todayã€‚

 they are plausible enough that we need to be preparing for themã€‚

 we need to be taking them seriouslyã€‚I won't I'll skip over the reasons for those risksã€‚

 we can come back to that laterï¼Œ but the third global challenge is really around enabling legitimate decisions about the future implications of artificial intelligence affecting all of humanity So this would mean legitimate decisions around the global benefits that we've just discussed and around the global risksã€‚

 but more than thatï¼Œ it's about questions of what kinds of artificial intelligence do we really want whatã€‚

When and under what conditions do we want to bring into existence machines that are potentially vastly more capable than humansã€‚

 these are kinds of questions that deserve that require reflection and collective deliberation and legitimate decision makingã€‚

 these are decisions that cannot and should not be made by just a small handful of people in a small number of AI companiesã€‚

Rightï¼Œ so if those are the challenges that we may face nowï¼Œ what are some of the solutionsã€‚

 how can we come together to address these questionsï¼Ÿ

And we're seeing a number of very impressive and laudable efforts already in the space of global and international cooperation on AIã€‚

 but where all of these efforts still seem to be unpreparedã€‚

 is in terms of really thinking through the implicationsï¼Œ not of today's AIã€‚

 but of the AI systems that we could face in the coming five or possibly three or two yearsã€‚

 and this is really where we require an anticipatory approach where we can be prepared to handle not just the futures we hope forã€‚

 but the most challenging scenarios that we could faceã€‚

What we believe is that we need a new framework and new mechanisms for international cooperation that would actually enable the joint action particularly on the most pressing issuesã€‚

 and we need through this to be able to optimize between the need for legitimacyã€‚

 broad representativeï¼Œ inclusive dialogueï¼Œ but also to get this done in time to be effective when it is neededã€‚

So what we're proposing specifically is a framework convention on global AI challenges as many of you know a framework convention is a highly flexible and adaptable approachã€‚

 It allows us to seek rapid global agreement on really the high level on the high level objectives and principles what do we all as humans care about what do we most care about what do we most wish for in terms of the future development of artificial intelligence but then it also sets out a process to then prioritize and integrate specific protocols to address particular issuesã€‚

 some of the most challenging issues so this is an illustration of that it lays out the highlel objectives gives examples of principles then we have these sort of three big buckets big categories that I mentioned before around benefits around risks and around inclusive decision- making and then one could imagine a number of protocols where is the real detailsã€‚

Developed of how we get this doneã€‚So the one I'm going to focus on now really is the protocol on public safetyã€‚

 on global public safety and security risks from AIã€‚So hereï¼Œ as mentioned earlierã€‚

 we're focusing on weaponization and loss of control risksã€‚Not because these are the only risksã€‚

 but because they are a potentialã€‚extremet severity and high uncertainty about the timelinesã€‚

 So if we really don't know when we could face these risksï¼Œ then that means they're urgentã€‚

 but means because we could face them soonï¼Œ we need to be readyã€‚

 at least for the possibility of having to face them soonã€‚ So for that reasonã€‚

 this would be a protocol to prioritize moving ahead on very quicklyã€‚

 possibly at the same time frame as the the broader framework conventionã€‚

This is a kind of mechanism that allows one to engage with key AI powers potentially first and then go globalã€‚

 one can approach it in a variety of different rhythms and configurationsï¼Œ and it is also oneã€‚

 of courseï¼Œ where we can seek synergies with number of the other protocols that may be being developed at the same timeã€‚

So the objective really of this principle of this protocol is quite simpleã€‚

 let's mitigate the AI risks to global public safetyã€‚

 let's ensure that the safety of humanity from the potential risks that AI could pose to usã€‚

So at the heart of the protocol is really a tiered riskbased approachã€‚

 This is based on a fundamental understanding that not all AI systems are the sameã€‚

 not all AI systems pose the same level of riskã€‚ we want to have as much of AI as we can without having to overregulate itã€‚

 we want to have the least amount of regulation as we can while still being able to get the job doneã€‚

 So if I start with tier1ï¼Œ these would be AI systems that we would assess to have negligible riskã€‚

 negligible global riskï¼Œ put it that wayã€‚ and these would beã€‚

Systems that could be regulated at the national level because they don't really pose significantã€‚

croross border risk to other countriesã€‚The second tier up is what we would call AI systems that present manageable risksã€‚

 they're significant enough that we can't just let them go unregulatedã€‚

 they need to actually be regulated and managedï¼Œ potentially licensed within each country and we need to do that in a way that's coordinated so that each country is adhering to a certain standardã€‚

 and I think some of the pieces that Robert was just telling you about around the standard setting it really a crucially important fit in hereã€‚

Then there's a next tierï¼Œ which would beã€‚Systems that are a little bit too dangerous to just allow private companies or governments to pursue on their ownã€‚

 even if they are regulatedã€‚ This would be systems that are pose such a potentially severe risk to humanity that we would only feel comfortable developing and testing those systems if it were in a shared space where we can jointly supervise how they're being developedã€‚

 where we can put them behind the best cybersecurity where they're being developed by the most trusted scientists under the best and most secure conditionsã€‚

 This is sometimes referred to as a kind of cern for AI but a joint laboratoryã€‚

 And then the fourth and final level would be AI systems that are judged to pose a really unacceptable riskã€‚

 And if such systems are are ever close or could ever be developedã€‚ the answer there would beï¼Œ okayã€‚

 we need to just not create these systems until the safety mechanisms can catchã€‚

We're not prohibiting them foreverï¼Œ but we do need to make sure that nobody anywhere in the world creates those systems until we all know that they will be safeã€‚

Nowï¼Œ a number of bodiesï¼Œ this sort of ecosystem of agencies and so forth that might be needed to support this systemã€‚

 very briefly a council to help make the political decisions around these questionsã€‚

 a commission that really does the scientific researchã€‚

 an agency that sets the standards and does the monitoring enforcementï¼Œ a laboratoryã€‚

 as I mentioned earlierï¼Œ and then some form of adjudicationã€‚Nowï¼Œ before Ihandã€‚

 I just want to focus on some key obstacles to this and obviously what I proposed is not necessarily what would be acceptable todayã€‚

 This is really thinking ahead to the kind of institutions that might be neededã€‚

 even when the people of the world and the governments of the world have a sort of wake up moment and are suddenly like wowã€‚

 we're facing AI like we've not seen beforeï¼Œ what would we need to manage this effectivelyã€‚

 but of course this will be incredibly challenging and so this are just some of the reasons why this will be incredibly difficultã€‚

 why cooperation international cooperation on this couldn't possibly work and I hear these every day and they're all very valid points but at the same time we may discovered that we need to overcome these issues in order to actually ensure our own well-being and thriving through this transition to very capable AI so therefore what we need our ways forward howã€‚

Despite all of those challengesï¼Œ might it just workï¼ŸI've named a few hereï¼Œ but there are manyã€‚

 many more that we would need to discover togetherã€‚

And will require unprecedented ingenuity and collaborationï¼Œ to achieve thisã€‚ But I believe we canã€‚

 So in sumï¼Œ we need to prepare now for the possibility of very powerful AIã€‚

Global cooperation will likely be essentialã€‚This will require unprecedented ingenuityã€‚

The challenge is vastï¼Œ and it needs us allã€‚With that thank you very much if you are interested here is our discussion paper it's meant to provoke discussionã€‚

 we would very much welcome your comments both here and then also by email there's an email address right on the screen and also inside the document with that thank you very muchã€‚

Thank you so muchï¼Œ Duncanï¼Œ please remain on stage as we prepare to transition to our panel on international cooperation on Frontier AI Safeã€‚

ğŸ˜Šï¼ŒğŸ¼I have the pleasure of moderating this panel todayã€‚ Robertï¼Œ Professor Zung and Ireneã€‚

 please kindly join us back on stage for this discussionã€‚ğŸ¼For this panelã€‚

 we are also joined by a new panelistã€‚ I'd like to invite Matt Shiuhan to come up and join us on stage as I introduce himã€‚

ğŸ˜Šï¼ŒMrã€‚ Shihenn is a fellow at the Carnegie Endowment for International Peaceã€‚

 where his research covers China AI ecosystem and global tech trendsã€‚

His writing has appeared in foreign affairsï¼Œ Bloombergï¼Œ Vice and Wiredï¼Œ among other publicationsã€‚

It's a pleasure to have Matt and all the panelists here todayï¼Œ please be seatedã€‚All rightã€‚Welcomeã€‚

 everyone to our final panel of the dayã€‚Mattï¼Œ as it's the first time you've come up on stage todayã€‚

 let me just start off with youã€‚ So we've heard a lot from our speakers about the importance of international coordination on AI safetyã€‚

ğŸ˜Šï¼ŒBut in your foreign policy article with Tinoã€‚About AI is winning the AI raceã€‚

 You touched on the competitive aspects of AI development and noted that a common question in Washington is who's winning the US China AI raceã€‚

Given this context of geopolitical competitionï¼Œ which Tino and Deinci also very candidly addressed in their remarks and their fireside chatã€‚

 could you explain why international cooperation onI safety is crucial and how we might balance these dual imperatives of cooperation and competitionã€‚

ğŸ˜Šï¼ŒGreatï¼Œ thanks very muchï¼Œ thanks to Concordia for having me for having all of usã€‚Maybe I'll kind ofã€‚

 I'll come at it from this angleã€‚ I think there's there's contrasting views here today about are the US en when we get to really powerful AI systemsã€‚

Is there a possibility that the US and China are going to have an ability to cooperate at any level or is the geopolitical competition too deepã€‚

 Are those competitive dynamics just too powerfulï¼Œ both countriesã€‚

CAI is like the cornerstone of future national powerã€‚ They might be right about thatã€‚

 they might be wrongã€‚ but when you see this as the key to national power in the future and you see that other country as your competitor in every wayã€‚

Cooperation is going to be extremely difficultã€‚ I think the thing that gives me optimism or the way that I see a way forward is an idea that I like to think of as safety in parallel as opposed to sort of safety by agreementã€‚

 I think we're talking a lot about you one day the leaders are going sit down at this very high levelã€‚

 they're going to have an agreementã€‚ and then we're going impose that agreement on both of the ecosystemsã€‚

 I think that I'm not putting my hope on that necessarilyã€‚

 What I'm more hopeful about is that we can have a more bottom up approach to building safety building safe systems where in Chinaã€‚

 there are going to be policy people there are going to be technical people they're going to be researchers that are pushing for AI safety within China for their own reasonsã€‚

 and they're going to be in communication with people in the US and people internationally about best practicesã€‚

But we're gonna to be doing this kind of at the same time in our own contextã€‚

 We're not going to try to agree with each other at each step of the wayã€‚

 we're going to build up safe practices through dialogueï¼Œ policy dialogueï¼Œ technical exchangesã€‚

 working together on evaluationsã€‚ And maybe after several years of building up this kind of safety in parallel with each otherã€‚

 We create a system where that type of high level agreement is possibleã€‚

 because both the countries are very close to doing this alreadyã€‚

 Both of the countries are very invested in AI safety for their own reasonsã€‚

 And then you set the stage for a potential agreementã€‚

 But I think we have to lay the groundwork ahead of timeã€‚

 I think that groundwork is laid through track two dialogueã€‚

 scientific exchanges and stuff like thatã€‚Thank youï¼Œ Mattã€‚

 I just want to continue on this theme of competition and challenges a little bitã€‚

 and turn to Irene hereï¼Œ where Ireneï¼Œ in your presentationã€‚

 you spoke about the crucial role of openness in scienceï¼Œ AI development and AI safetyã€‚ğŸ˜Šï¼ŒRecentlyã€‚

 we've seen companies like Open AI restricting API access to certain countriesã€‚

 highlighting the tension between open collaboration and geopolitical concernsã€‚

How do you view the future of international AI model sharing and cooperation and openness in light of these geopolitical pressuresã€‚

Oï¼Œ thanks for the softball on youã€‚The way that I'm thinking about where models are available is also echoing Matt's point of where can we find collaborationã€‚

 not all aspects of safety are going to highly overlap between different regionsã€‚

 particularly the US and Chinaã€‚I'mã€‚I'm really hopeful againã€‚

 echoing what Matt's been saying in some conversations that we've been having in parallel around the kind of research collaborationsã€‚

 the kind of sharing that we can do it's been great to be able to evaluate Chinese models like Quentu in parallel to some more open models on the Hgingface leader board and to be able to do that I hope that we can continue to be able to do that this is where openness does come into play and understanding not just the capability side and there's more to say around how do we construct capability evaluations and we talked about what does that mean for safety but also to be able to maybe build evaluations together to look at what does performance look like in multiple different languagesã€‚

 how does that overlap with safety and then also the commercial side I'm speaking specifically from a research perspectiveã€‚

 but' I can't give insight into commercial decisions from other companies that will likely differ based off of jurisdictions and lawsã€‚

Thank youï¼Œ Ireneï¼Œ it wasn't an easy question and you did a fantastic job of itã€‚

Let's now shift gears a little bit and consider international institutions for AI governanceã€‚

Professor Zhgï¼Œ as a member of the UN Haal Advory body on AIã€‚

 you have unique insight into the UN's role in this spaceã€‚

 could you share your perspective on the UN's position on shaping global AI governance and specifically how do you see how we balance the need for the broad legitimacy that the United Nations provides with the challenges it sometimes faces about the pace of its processes and how can we ensure that international institutions are essentially both inclusive and agile enough to keep up with the rapid pace of AI development that many of our speakers have spoken about today Thank youã€‚

Thank youã€‚ I think this is really essential problems that we have to faceï¼Œ I thinkã€‚

Before the UN advisory body on AIï¼Œ there's already been many regional tris right there like the OECD onesã€‚

 the U ones and also the global partnership on AIs which are also a regional network that only have you know like fewer than 40 countries coming together well you have the rest of the 160 countries that has been forgotten by the world so I think the role of UN is not to create the node to do global AI governance instead it should be a pivoter node to weave a global network of AI governance that is to enable all the regional network to take their own actions at the regional levelã€‚

And find the missing places to bridge all the connections altogetherã€‚So I saw I seeã€‚

 this is why I think for the UN N GA resolution a few days agoã€‚

 it says UN N should play a core role in global A governanceã€‚

 What I understand about the core role is really to take the responsibility to weave a global network and to enable everyone instead of disable everyoneã€‚

Alsoï¼Œ but what you also have to see is that for those regional networksã€‚

 what they're trying within this yearã€‚ and the next is that they wanted to try to disable what the U N should do instead of they wanted to replace what the U N should doã€‚

 I'm not saying it should be U Nã€‚As alwaysï¼Œ what I'm saying is that when we have observations for all the other networksã€‚

 when you see the behaviors and then they don't really have a tractor record to beï¼Œ you knowã€‚

 trustworthy at the global levelã€‚So I think the Uã€‚ N still is the most trustworthy platform that can bring everyone all togetherã€‚

 I think China has been participating in some of the regional networksã€‚

 let's say for the AI safety summit and also the re process for military AIã€‚ I think I thinkã€‚

 I think you'll be grateful to have China to be involvedã€‚ Well on the other handï¼Œ you seeï¼Œ you knowã€‚

 limitations for all the regional networks in a way that they have their own emphasisã€‚

And they have limitationsï¼Œ and they don't have the power to bring everyone all togetherã€‚

 And it's not their duty to do thisã€‚ And then pleaseï¼Œ and then in this caseã€‚

 I would say to leaving no country behindã€‚ and you have to do it in leaving no country behind wayã€‚

 So by using the U M platformï¼Œ you bring all the regional networks altogetherã€‚

 weave them all togetherã€‚Enable themã€‚ Well on the other side for those regional networksã€‚

 Please don't disable Un because the U is weaving the networkã€‚ that's not disabling you guysã€‚

 So I think most healthy way to do it is to have a collective work altogether let's say let's say to have a steering committee of all the regional networks altogether with the Unã€‚

 so that you can really weave the network altogetherã€‚ The Un does not really have that much moneyã€‚

 They also don't really have that muchï¼Œ you knowï¼Œ people to do the work heavily rely on the member states to do itã€‚

 So maybe it hasn't been done a very good job in many of the you knowã€‚

 problems that they have to face Well on the other sideã€‚

 We don't have the other choices that is you knowï¼Œ much trustworthier compared to themã€‚

 So they're not the bestã€‚For nowï¼Œ please help them to be the best and please use this platform to solve problemsã€‚

 Let's say if the UN Secretary general is not doing good in in some of the issues the U N general Assembly is usedã€‚

 you knowï¼Œ to provide the resolution and to ask the the secretary general questions and ask them to do something like the UN NGA resolution for now that the Chinese led resolution signed by the 140 countries altogetherã€‚

 ask the UN secretary general to provide a report on the limitations of current low and middle income countries on AI and find solutions for these problems for these you knowã€‚

 global countriesã€‚ and provide a report by next yearã€‚

 So I think these are the requirements from all the member states that has been signed for these resolutionã€‚

 So I think this is theã€‚ay to enable everyone and also to use U N as a most inclusive platform to do the work that all member states neededã€‚

Can I add to that Kwaan Yiï¼Œ I want to emphasize that even without a deliverable the UN as a communication channel can just help us familiarize ourselves with each otherã€‚

 I had the delight of actually meeting professors saying years ago in early UN work and it was my first engagement with somebody who does work here and I think that was reallyã€‚

 really valuableã€‚For sureã€‚ Thank you so much both Professor Zg and Irene and I think Professor Zg's point about network approach really segues well into Robert's concept of an evolving ecosystem of international institutions that you mentioned in your presentation and Robert I'd like to hear a bit more from you now as well building on Professor Zg's perspectives in your presentation you talked about an international standards and reporting regime for AI safetyã€‚

 Can you talk a little bit more about how that relates to this evolving ecosystem of international institutions that you mentioned and the UN's role and all of thatã€‚

Is this betterï¼Œ Yesï¼Œ that's betterã€‚Thank you for the questionã€‚

 I'm happy to talk about that and I just want to say at first that I'm following somebody who has been thinkingã€‚

 I think deeply about this on the high levelvel advisory board and so I'm very aware of thatã€‚

My view on the UN's role is that it has essential roles to playã€‚

 and we need to find what those are in the context exactly as Kwaan Y described of an ecosystemã€‚

And so I think there are things that the UN is already doing in part that are important that it can continue to doã€‚

 for instanceï¼Œ in developmentï¼Œ I think in the talk I mentioned the ITU and UNESCO and also other areas of the UN as well that are already doing things in the area of standard setting that know as a matter of fact that they need to start building up more capacity for research to continue to do the sorts of standard setting that we need to doã€‚

Andã€‚And are doing roles in developmentï¼Œ which is extremely importantã€‚

 I think we need just to give one example of the sort of things that we need to do more of in development that I think the UN would be a really natural place to doã€‚

 you might think about a sort of field of responsible digitization you probably the main barrier for many communities participating in advanced AI is actually having digital world that having the data basically in their own cultural spaces and there are ways that we can think about gaining that dataã€‚

 manyï¼Œ many global majority spacesï¼Œ for instanceã€‚ha a high degree of cell phone penetrationã€‚

 so we could use that for digitization but we need to do so in a way that wasn't invading privacy was consistent with the norms of those communities so we'd have to do it very carefully but that is the kind of thing that I think we could really see a broadbased effort of the world getting behind to improve outcomes for developmentã€‚

 some of the other things that I think have been have been talked about like what sometimes called an IPCC for AI would be a sensible thing to do at the UN some areas of standard settingã€‚

 I'm sure we can find other areas that seem really naturally suited to the things that the UN does well convening dialogue I think was mentioned that's extremely important so we can find those areas I think that that's probably the key when we think about the role of the UN in the ecosystemã€‚

Can I also add a little bit on the different UN agencies so now what you see is like what Robert is talking about that the different UN agenciesã€‚

 they've already done something related to AI independently so we have a we have an offline meeting in Geneva a few months agoã€‚

 and we visited almost all the different UN agencies in Geneva organized by the UN Secretary general teamã€‚

 Well then what we observe is that for these different UN agencies mostly they of course see their woodsã€‚

 but but in some cases they missed you know they see the trees but they missed the woods the viewã€‚

 I meanï¼Œ So in that caseï¼Œ what I wanted to sayï¼Œ let's say on AI for global sustainabilitytain developmentã€‚

 there's many things that has been done by ITUã€‚ but for theã€‚

Of sustainable development India has to be solved at the societal levelã€‚

 not only at the technology levelã€‚ So ITU is about technologyï¼Œ but for the goal but the whole goalã€‚

 you have to talk to different agencies related to society and many more and so the UN agenciesã€‚

 they have to coordinate in a much better way compared to now even themselves so it will be the duty of the member states to help the UN to have a better structure so that everybody can be enabled in the UN agencies a systemã€‚

 not disable everyone really enable everyone and then to solve the problems and also standardization is another example that ITs take in standardization in the United Nations system but what about IO and other international standardization organizations likeã€‚

ple Eï¼Œ they don't really have a coordination mechanism that is effective enoughã€‚ So in that caseã€‚

 the member states has to ask the Uã€‚ N General Assemblyã€‚

For the potential solutions by the Secretary General to help to organize these different U N different international standardization organizations to come together and then to talk to these different member statesã€‚

 how to unify or to to help to find the interoperbilities among different international standardization organizationsã€‚

 not only you knowï¼Œ the the standardization organization within the Uã€‚ N systemã€‚Thank you so muchã€‚

 Professor Tger and Professor Zongã€‚I'm aware that we're running short on timeã€‚

 So I would just like to start wrapping up our discussion by asking a final questionã€‚

You have all articulated various visions of victoryï¼Œ or success for international AI governanceã€‚

 and I would like to hear from each of our speakers what is one thing you think that the international community should do in the next six to 12 months to achieve this visionã€‚

 for exampleï¼Œ at the UN Smit of the futureï¼Œ the front AI active summit or other venues and we really want to concretize the discussion hereã€‚

 so I would really like just one recommendationï¼Œ perhaps Duncanï¼Œ as we haven't heard from you yetã€‚

 perhaps we could start with youã€‚ğŸ˜Šï¼ŒAll rightï¼Œ thank youã€‚

I think my one message would be to help prepare for the futureã€‚

 we need to help our institutions whether it be the United Nations or this web of supporting networks to actually think now about what might be needed to help humanity successfully navigate through perhaps the greatest transformation we've ever seenã€‚

 certainly likely the most powerful technology we've ever seenã€‚

 we need to be thinking now and designing now and putting in place the kinds of institutions that may be neededã€‚

 and we're having to help institutions essentially look forward in the context of tremendous uncertaintyã€‚

 and I think given that we're facing so much uncertaintyï¼Œ we need to be anticipatoryã€‚

 we need to build the kinds of mechanisms that would be that might be needed under certain scenarios so that they're ready ahead of timeã€‚

A lot is being talked about the importance of bringing together scientists to build shared understanding around the risks and the challenges associated with the technology and that's crucially importantã€‚

 but I think just as important is bringing together the social scientists and the many different perspectives to really roll up our sleeves and bring that ingenuity around what are the kinds of governance institutions and mechanisms we would need to help humanity navigate through this period so that we can look back in 500 years and say despite it being inconvenient that we needed to collaborate on these issuesã€‚

 we did manage to that these technologies forced us to figure out how to collaborate well and we did that successfullyã€‚

Thank youï¼Œ Duncanã€‚ Perhaps Ireneï¼Œ we could have you come up nextã€‚

 where maybe you'd also like to dig into the role a little bit more of research institutions and startups that you mentioned in your presentationã€‚

ğŸ˜Šï¼ŒAbsolutelyï¼Œ it's exactly what I was going to say and my semi serious response is a pause so that we can all restã€‚

 I'm so tiredï¼Œ I know I'm not aloneï¼Œ and then the very serious one is echoing what Duncan was saying around the right expertise isã€‚

 reason one of the big reasons that I work at Hugingfaceace it was one of the first times that I saw one of my heritage languages Bla researched in a data set and to ensure that we're including different groupsã€‚

 different sociotechnical researchers and also addressing those systemic harms that maybe aren't obviously technical but are really rooted in history in culture and in different regions of the worldã€‚

ğŸ˜Šï¼ŒThank youï¼Œ Ireneï¼Œ Mattï¼Œ perhaps we could turn to you next on this questionã€‚Sureã€‚

 one thing to see in the next six or 12 monthsï¼Œ I think it would be the US and China conducting some form of joint evaluation exercises around particular large scale risksã€‚

 This is going to be extremely complicated to figure out how you can do this in a way that both sides feel safe doing I think maybe at the government levelã€‚

 it will be maybe even too difficultï¼Œ ideally you could do at an AI safety instit to an AI safetyfe Institutesit that might be too hard in this period of timeã€‚

 but if not having it at the track2 levelï¼Œ having some of the best people working on technical evaluation in China and some of the best people working on technical evaluationã€‚

 safety evaluation in the US in the US get together and talk about the methods that they're using the problems they see with those methods and what they want to do going forwardã€‚

Thank youï¼Œ Mattã€‚ Certainlyï¼Œ I thinkï¼Œ as we've discussed in this forum as wellã€‚

 AI safety testing being an incredibly important area and potential area for international coordinationã€‚

 Perhaps Robertï¼Œ we could turn to you next on this question about next six to 12 monthsã€‚

 what you'd like to see happenã€‚ğŸ˜Šï¼ŒWellï¼Œ I think I talked about some specific things in the talkã€‚

 so maybe I'll just use this opportunity to respond to one thing that was said earlier and to say something generalã€‚

 which is thatï¼Œ you know I do think that in the context of geopolitical tensionã€‚

We have a lot of examples ofã€‚Of rivals getting together and doing things when they thought it was important enough to doã€‚

 So even though I think that in many casesï¼Œ you wouldn't necessarily predict that it was going to happen you still need to try to make it happen and to find the possibilities for when it can happenã€‚

 And for instanceï¼Œ in this area of technology governanceï¼Œ particularly AI governanceã€‚

 you know Democrats and Republicans in the United States are not known for getting along these daysã€‚

 Neverthelessï¼Œ they have been remarkably in lockstep in congressional hearings in particular on governance of advanced AIã€‚

 So that's one exampleï¼Œ the non-proliferation treaty was another time when two powers got together because that was really initiated by the United States and the Soviet Union at a time when they did not have particularly good relationsã€‚

Neverthelessï¼Œ they got together and they managed to do it because they thought that it was in their mutual interest and they thought it was importantã€‚

 So it's not that you would bet on those sorts of things happeningã€‚

 but you have to look for the opportunities when they can happenã€‚Thank youï¼Œ Robertã€‚

 We'd be honored to conclude this panel by remarks from Professor Zungã€‚

 perhaps on the final questionã€‚ğŸ˜Šï¼ŒSo what's next about the next 12 monthssã€‚

 I'm going to tell you first about what's really happened before you know for the last eight monthss already that I don't know Raying before these eight months So I was invited to the Asian Technology conference last year for Singaporeã€‚

 but it was not Raying who invited me I blame Raymi for thatã€‚ and then we come togetherã€‚

 So here's from the governmentã€‚ So I'm from a research organizationã€‚

 So maybe I'm notm not allowed to talk to Raying in normal casesã€‚

 but and then within this eight monthsï¼Œ we come together as a groupã€‚

 Raying treat me very well with very good Asian food in Singapore and also telling me about the practices in Singaporeã€‚

 but not only usã€‚Together with all the president and vice president of these largerscale AI companies altogetherã€‚

 and also many of the former policy makes allã€‚ So so I see the values of everybody coming togetherã€‚

 You don't understand each otherã€‚ you say that we are totally differentã€‚

 But then when you come togetherï¼Œ you feelï¼Œ ohï¼Œ okayï¼Œ not so much differenceã€‚

 And then you have the safety problemsã€‚ you have the needs but developmentã€‚

 let's solve the problems because compared toï¼Œ you know last yearã€‚

 the risks which has been showing has beenï¼Œ you knowï¼Œ10 times compared to the year before last yearã€‚

 all of the worldã€‚ What you see is really risk again againã€‚

 happening again again in different countriesã€‚ And then you see the limitations of the U systemã€‚

 if I have the money And also I have the people right thereã€‚ I would give all of them to theã€‚

Uned Nations to create an international agency on AI by tomorrowã€‚ but I don't have thatã€‚

 So for the next of the 12 monthsï¼Œ what has to be done is at least to create something like the European trying to have a European AI office But now we should have a you know UN AI officeã€‚

 a minimum trying that bring all the UN agencies allï¼Œ coordinate them in such a nice wayã€‚

 and also talk to these regional networks such as OECD global partnership on AI IE and ACMã€‚

 So so I would say we have to move to that spaceï¼Œ simply because within the group of the UN advisory body on AI I think we see the woods the needs of doing this without that we don't see an opportunity to leaving no countryã€‚

Fdã€‚Thank youï¼Œ Professor Zï¼Œ leaving no country behindã€‚

 That's an incredibly inspiring vision to end onã€‚ğŸ˜Šã€‚

Professor Benjiil at the beginning of this forum talks about how we have currently no known method to prevent existing nor catastrophic risks of misuse or loss of control from AI systemsã€‚

It's clear that the path forward will be challenging and complexã€‚

 but from this forum from our speakersï¼Œ we've had a shared commitment to ensuring that AIs development benefits humanity as a wholeã€‚

 Please join me in a round of applause for all our panelistsã€‚ğŸ˜Šï¼ŒPanelistsã€‚

 feel free to make your way offs stageã€‚Can our technicians please come and help take these seats off stageã€‚

 pleaseã€‚So do we have technicians here to take the seats off stage so we could close our forumã€‚Oã€‚

 thank youã€‚ Wellï¼Œ to conclude our forumï¼Œ we are honored to have Professor Joe Balwin deliver our closing remarks for todayã€‚

ğŸ˜Šï¼ŒProfessor Zhou is the director and chief scientist of Shanghai AI Laboratory and chair professor at Tinghua Universityã€‚

He was formerly senior vice president of the E-commerce Giants JD do comã€‚

 where he worked in multiple executive positions as the founding director of JD AI researchã€‚

Prior to thatï¼Œ Professor Joel held various technology leadership and executive positions at IBMã€‚

Including as the director of the AI Foundations Lab at IBM Researchã€‚

Professor Z has decades of experience in AI research and is a recipient of the prestigious Fuenz Award for outstanding contributions in artificial intelligenceã€‚

ğŸ˜Šï¼ŒProfessor Joelï¼Œ thank you so much for being here todayã€‚ The floor is yoursã€‚

Thank you for having me hereã€‚ And more importantlyï¼Œ thank all of you for staying lateã€‚ğŸ˜Šã€‚

I was asking to give a concluding remarksã€‚ in a meaningï¼Œ this is the end of thisã€‚Fabulous forumã€‚

 But I'm thinking this is right the beginning of ongoing dialogueã€‚ So in that spiritã€‚

 I'm thinking to given the talk I was given yesterday as open ceremonyã€‚

 because I was told many of you were not there in personã€‚ğŸ˜Šï¼ŒSoï¼Œ so below myã€‚å‘ƒã€‚

Switching to Chinese nowã€‚ So if you don't speak Chineseï¼Œ please time to put your translator onã€‚

So the topic I'm going to have today isã€‚I think only something new integrating tech and governance with thatã€‚

 I call itäººå·¥æ™ºèƒ½45åº¦å¹³è¡¡ç‡ã€‚å½“å‰ä»¥å¤§æ¨¡å‹ä¸ºä»£è¡¨çš„æ·±å±‚å¸‚äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•ã€‚ä½†æ˜¯éšç€èƒ½åŠ›çš„ä¸æ–­æå‡ï¼Œå¤§å®¶ä¹Ÿçœ‹åˆ°è¶Šæ¥è¶Šå¤šçš„æ½œåœ¨é£é™©ã€‚ä»å…¬ä¼—å¯¹AIé£é™©çš„è®¤è¯†é¡ºåºæ¥çœ‹ã€‚

è¿™é‡Œé¢ä»…åŒ…æ‹¬æ•°æ®æ³„éœ²ã€æ•°æ®æ»¥ç”¨å•Šï¼Œéšç§ä¾µçŠ¯ç‰ˆæƒäº‰è®®ã€‚æˆ‘æŠŠå®ƒå«åšæ•°æ®å†…å®¹é£é™©ã€‚å…¶æ¬¡å‘¢æ˜¯æ¶æ„ä½¿ç”¨å¸¦æ¥çš„ä¼ªé€ ã€è™šå‡ä¿¡æ¯ç­‰å¸¦æ¥çš„ä½¿ç”¨é£é™©ã€‚å½“ç„¶ä¹Ÿè¯±å‘å‘äº†åè§æ­§è§†çš„ç›¸å…³çš„ä¼¦ç†é—®é¢˜ã€‚

è¿˜æœ‰äººæ‹…å¿ƒæ˜¯å¦ä¼šå‘ƒæŒ‘æˆ˜å°±ä¸šç»“æ„ç­‰ç¤¾ä¼šç³»ç»Ÿæ€§é£é™©ã€‚å½“ç„¶ï¼Œåœ¨å¥½è±åçš„ç”µå½±é‡Œé¢ä¹Ÿå‡ºç°äº†AI4æ§äººäººå®Œå…¨ä¸§å¤±è‡ªä¸»æƒçš„è¿™ç§å‘ƒæç«¯é£é™©ã€‚è¿™äº›é£é™©æœ‰äº›å·²ç»æ˜¯åœ¨ç°å®ä¸­å‡ºç°ï¼Œè€Œï¼Œæ›´å¤šå‘¢æ˜¯æ½œåœ¨çš„é˜²èŒƒè¿™äº›é£é™©ã€‚

éœ€è¦å„ç•Œçš„å…±åŒåŠªåŠ›ï¼Œéœ€è¦ç§‘å­¦ç¤¾åŒºåšå‡ºæ›´å¤šè´¡çŒ®ã€‚å»å¹´5æœˆä»½ï¼Œæ•°ç™¾åAIç§‘å­¦å®¶å…±åŒç­¾ç½²çš„statement of AI riskï¼Œä¹Ÿè¡¨è¾¾äº†å¯¹AIé£é™©çš„ç›¸å…³æ‹…å¿§ï¼Œå¹¶å‘¼åé˜²å¾¡äººå·¥æ™ºèƒ½çš„ã€‚

ç³»ç»Ÿæ€§é£é™©åº”è¯¥å’Œæµè¡Œç—…å’Œæ ¸æˆ˜äº‰ç­‰å¤§è§„æ¨¡é£é™©ä¸€æ ·ï¼Œæˆä¸ºå…¨çƒä¾›åº”çš„ä¼˜å…ˆè¯é¢˜ã€‚ä»ä¸€ä¸ªæ•åšæŠ€æœ¯çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°è¿™äº›æ‹…å¿§çš„æ ¹æœ¬åŸå› æ˜¯ç›®å‰çš„AIå‘å±•æ˜¯å¤±è¡¡çš„ã€‚æˆ‘ä»¬æ¥çœ‹ç›®å‰Açš„å‘å±•è¶‹åŠ¿ã€‚

æ¨ªè½´æ˜¯AI capabilityï¼Œé‡è½´æ˜¯AI safetyã€‚åœ¨æ¨ªè½´ä¸Šï¼Œåœ¨ä»¥transformerä¸ºä»£è¡¨çš„åŸºç¡€æ¨¡å‹æ¶æ„ä¸Šï¼ŒåŠ ä»¥å¤§æ•°æ®å¤§å‚æ•°é‡å’Œå¤§è®¡ç®—é‡çš„singã€‚ç›®å‰ã€‚

Aabilityæ­£åœ¨å¿«é€Ÿçš„å‘ˆæŒ‡æ•°å»å¢é•¿ã€‚ä¸ä¹‹å¯¹æ¯”ï¼Œåœ¨å®‰å…¨é¢†åŸŸï¼Œæˆ‘ä»¬çœ‹æˆ‘ä»¬æœ‰ä»€ä¹ˆå…¸å‹çš„æŠ€æœ¯ï¼Œæ¯”å¦‚çº¢é˜Ÿæµ‹è¯•ã€å®‰å…¨æ ‡è¯†ã€å®‰å…¨è´Ÿéš¾ä¸è¯„ä¼°è¯„æµ‹ç­‰ç­‰ï¼Œéƒ½å‘ˆç°æ˜¯ç¦»æ•£åŒ–ã€ç¢ç‰‡åŒ–ã€‚å¾ˆé‡è¦çš„æ˜¯ad hoéå¸¸çš„åšå®ã€‚å½“ç„¶ã€‚

æœ€è¿‘å‡ºç°äº†ä¸€äº›æ–°çš„æŠ€æœ¯ï¼Œå…¼é¡¾äº†æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚æ¯”å¦‚ç›‘ç£å¼å¾®è°ƒSFTäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ RHFLAIFsuperç­‰ç­‰ã€‚è¿™äº›æ–¹æ³•æœ€ä¸»è¦ç‰¹ç‚¹æ˜¯æŠŠäººç±»çš„åå¥½ä¼ é€’ç»™å¤§æ¨¡å‹ã€‚

ä¹Ÿæ¶Œç°å‡ºäº†æ¯”å¦‚chGPTGPT4ç­‰ä»¤äººå…´å¥‹çš„AIç³»ç»Ÿï¼Œä»¥åŠæˆ‘ä»¬ä¸Šæµ·AIå®éªŒå®¤çš„ä¹¦ç”Ÿè‹±ç‰¹å¤§æ¨¡å‹ç­‰ç­‰ã€‚è™½ç„¶è¿™äº›æŠ€æœ¯ç„å‡†çš„æ˜¯å®‰å…¨å’Œæ€§èƒ½åŒæ—¶æå‡ï¼Œä½†åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå¤§å®¶å¾€å¾€å‘ç°æ›´å¤šæ˜¯æ€§èƒ½ä¼˜å…ˆã€‚æ‰€ä»¥æ€»ä½“ä¸Šã€‚

æˆ‘ä»¬åœ¨AIæ¨¡å¼çš„å®‰å…¨èƒ½åŠ›çš„æå‡è¿˜è¿œè¿œè½åäºæ€§èƒ½ã€‚è¿™ç§å¤±è¡¡å¯¼è‡´AIçš„å‘å±•æ˜¯è·›è„šçš„ï¼Œæ‰€ç”¨ä»¬æŠŠå®ƒè¿™cAIã€‚ä½†æ˜¯è¿™ç§ä¸å¹³è¡¡çš„èƒŒåï¼Œå®é™…ä¸Šæ˜¯ä¸¤è€…æŠ•å…¥ä¸Šçš„å·¨å¤§å·®å¼‚ã€‚ä»å³è¾¹çš„å¯¹æ¯”ï¼Œå¤§å®¶èƒ½å¤Ÿçœ‹å‡ºæ¥ã€‚

ä¸¤è€…åœ¨æŠ€æœ¯ç ”ç©¶ä¸Šæ˜¯å¦ä½“ç³»åŒ–ï¼Œäººæ‰çš„å¯†åº¦ä¸Šï¼Œå•†ä¸šé©±åŠ¨åŠ›æ–¹é¢ä»¥åŠç®—åŠ›çš„æŠ•å…¥åº¦æ–¹é¢å¯¹æ¯”æ¥çœ‹ï¼Œå®‰å…¨æ–¹é¢çš„æŠ•å…¥æ˜¯è¿œè¿œè½åäºAIèƒ½åŠ›çš„ã€‚æˆ‘ä¸€ç›´åœ¨å‘¼åè¦åŠ å¤§å¯¹å®‰å…¨çš„ç®—åŠ›çš„æŠ•å…¥ã€‚å‘ƒã€‚

æˆ‘ä¸¾çš„ä¾‹å­å°±æ˜¯è¯´ä½  aI system likeå½“å°çš„æ—¶å€™ï¼Œä½ å¯èƒ½èŠ±å¤§çš„ç®—åŠ›å»å¸®åŠ©ä»–åƒå¥½å–å¥½è¡£æœç©¿å¥½ã€‚ä½†æ˜¯åœ¨å­©å­æ…¢æ…¢g upgrow upçš„æ—¶å€™ã€‚

 spend more timeå»è·Ÿä»–ä½ æ›´å¤šçš„ç„¦è™‘ä¸ä»–æ˜¯ä¸æ˜¯åƒå¥½å–å¥½ï¼Œè¯´ä»–å»è·Ÿä»–åšå„ç§ä»·å€¼çš„äº¤æµã€‚è¿™ç§ä»·å€¼äº¤æµæŠ•å…¥å®é™…ä¸Šå°±æ˜¯ç®—åŠ›çš„æŠ•å…¥ã€‚ä½†æ˜¯å¾ˆä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬å¤§éƒ¨åˆ†çš„ç®—åŠ›éƒ½æŠ•å…¥åœ¨é¢„è®­ç»ƒä¸Šã€‚

å¾ˆå°‘å¾ˆå°‘ä¹Ÿä¸ç”¨åœ¨å®‰å…¨ä¸Šã€‚æ‰€ä»¥è¿™ç§æŠ•å…¥çš„ã€‚å‘ƒï¼Œå¤±è¡¡å¯¼è‡´äº†æˆ‘ä»¬ç°åœ¨cçš„AIæˆ‘ä»¬çœŸæ­£éœ€è¦è¿½æ±‚çš„ã€‚æˆ‘ä¸€ç›´åœ¨è®²çš„æ˜¯ï¼ŒåŒ…æ‹¬ä»ç¾å›½åˆ°ä¸­å›½ï¼Œæˆ‘çš„å­¦æœ¯ç”Ÿæ¶¯ä¹Ÿåœ¨è¿½æ±‚çš„trustworthy AGIä¹Ÿå°±æ˜¯å³ä¸Šè§’è¿™ä¸ªè·¯çº¿ã€‚

è¿™æ˜¯æˆ‘ä»¬çš„æ˜Ÿè¾°å¤§æµ·ã€‚æˆ‘æŠŠè¿™å«åšå¯ä¿¡HIå¦‚æœæˆ‘ä»¬æ‰¾è¾¾å…¼é¡¾ã€‚å®‰å…¨å’Œæ€§èƒ½ã€‚æ‰€æˆ‘ä»¬éœ€è¦æ‰¾åˆ°AIå®‰å…¨ä¼˜å…ˆï¼Œä½†åˆèƒ½ä¿è¯AIæ€§èƒ½çš„é•¿æœŸçš„å‘å±•çš„æŠ€æœ¯ä½“ç³»ã€‚

æˆ‘ä¸ªäººæŠŠè¿™æ ·ä¸€ç§æŠ€æœ¯æ€æƒ³ä½“ç³»å«åšAI45åº¦å¹³è¡¡åŠ›AI4 five degree longã€‚AI45åº¦å¹³è¡¡ç‡æ˜¯ç§æŒ‡é•¿æœŸçš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬è¦å¤§ä½“ä¸Šæ²¿ç€45åº¦å®‰å…¨ä¸æ€§èƒ½å¹³è¡¡å‘å±•ã€‚

æ‰€è°“å¹³è¡¡æ˜¯æŒ‡çŸ­æœŸå†…å¯ä»¥æœ‰ä¸Šä¸‹çš„æ³¢åŠ¨ï¼Œä½†é•¿æœŸå†…ä¸èƒ½é•¿æœŸä½äº45åº¦ï¼Œå¦‚åŒæˆ‘ä»¬ç°åœ¨ä¹Ÿä¸èƒ½é•¿æœŸé«˜äº45åº¦ï¼Œè¿™å°†é˜»ç¢å‘å±•ä¸äº§ä¸šåº”ç”¨ã€‚è¿™ä¸ªæŠ€æœ¯æ€æƒ³ä½“ç³»å®ƒæ˜¯å¼ºæŠ€æœ¯é©±åŠ¨ã€‚å…¨æµç¨‹ä¼˜åŒ–å‘ƒï¼Œæ‰€è°“å…¨æµç¨‹ä¼˜åŒ–ã€‚

æˆ‘åœ¨23å¹´çš„ä¸€ç¯‡transrustworth AIçš„ç»¼è¿°æ–‡ç« é‡Œé¢ï¼Œåœ¨IMcompè®¾å¤‡ä¸Šå‘è¡¨ã€‚æå‡ºæ˜¯è¦æŠŠå…¨æµç¨‹ä»æ•°æ®çš„å‡†å¤‡å•Šï¼Œæ¨¡å‹çš„è®­ç»ƒåˆ°éƒ¨ç½²ä¹‹åçš„å‘ƒã€‚operationå’Œè¿è¥å…¨éƒ¨å‘ƒã€‚

ä»å®‰å…¨çš„è§’åº¦æ¥è¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶ä¹Ÿéœ€è¦å¤šä¸»ä½“å‚ä¸ã€‚æˆ‘æƒ³è¿™æ˜¯åˆšæ‰é‚£ä¸ªforè®¨è®ºçš„å¾ˆå¤šçš„è¯é¢˜ï¼Œå½“ç„¶ä¹ŸåŒ…æ‹¬æ•æ·æ²»ç†ã€‚å®ç°AI45åº¦å¹³è¡¡ã€‚ä»æŠ€æœ¯è§’åº¦æ¥è®²ï¼Œä¹Ÿè®¸å­˜åœ¨å¾ˆå¤šçš„è·¯å¾„ã€‚

æˆ‘ä»¬ä¸Šæµ·A labä¹‹é—´æœ€è¿‘åœ¨æ¢ç´¢ä¸€æ¡ä»¥å› æœä¸ºæ ¸å¿ƒçš„è·¯å¾„ã€‚æˆ‘ä¸ªäººæŠŠå®ƒå–åå–ä¸ºå¯ä¿¡AIçš„å› æœä¹‹æ¢¯ï¼Œè¿™ä¹Ÿæ˜¯è‡´æ•¬å› æœæ¨ç†é¢†åŸŸçš„å…ˆé©±ã€‚å›¾åˆ©è®²ä¹‹å¾—ä¸»judyã€‚å¯æ€§æ¡ˆä»¶çš„å› æœä¹‹æã€‚æˆ‘ä»¬æŠŠå¯ä¿¡è€Œä¸”çš„å‘å±•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µã€‚

åˆ†åˆ«æ˜¯çŠ¯å¯¹å…¶å¯å¹²é¢„ï¼Œèƒ½åæ€ã€‚çŠ¯å¯¹å…¶å‘¢ä¸»è¦æ˜¯åŒ…å«äº†å½“å‰æœ€ä¸»æµã€æœ€å‰æ²¿çš„äººç±»åå¥½å¯¹å…¶æŠ€æœ¯ã€‚åƒæˆ‘ä»¬å‰é¢æåˆ°çš„IHFã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›å®‰å…¨å¯¹å…¶ä»…ä¾èµ–äºç»Ÿè®¡ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„å› æœå…³ç³»ã€‚

è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´é”™è¯¯çš„æ¨ç†ä»¥åŠæ½œåœ¨çš„é£é™©ã€‚ä¸€ä¸ªç»å…¸çš„ä¾‹å­ï¼Œæ˜¯å·´æ™®æ´›å¤«çš„ç‹—ã€‚å½“ç‹—ä»…ä»…åŸºäºé“ƒå£°å’Œé£Ÿç‰©çš„ç›¸å…³æ€§å½¢æˆæ¡ä»¶åå°„æ—¶ï¼Œä»–å¯èƒ½åœ¨ä»»ä½•åœºåˆå¬åˆ°é“ƒå£°éƒ½ä¼šè§¦çŠ¯ä»–çš„è¡Œä¸ºï¼Œè¿™é‡Œè¿™ä¸ªè¡Œä¸ºæ˜¯åˆ†æ³Œå”¾æ¶²ã€‚

ä½†å¦‚æœæŠŠå®ƒæƒ³è±¡æˆè¿™ä¸ªè¡Œä¸ºæ˜¯é‡‘èè½¬è´¦ã€‚åŒ»ç–—å†³ç­–ç”šè‡³æ˜¯ã€‚å†›äº‹ç›¸å…³çš„å†³å®šï¼Œè¿™æ˜¾æ¥æ˜¯æå…¶ä¸å®‰å…¨çš„ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç¬¬äºŒå±‚ï¼Œå¤–æ¥å«åšå¯å¹²é¢„ã€‚å¯å¹²é¢„ä¸»è¦æ˜¯é€šè¿‡å¯¹AIç³»ç»Ÿè¿›è¡Œå¹²é¢„ï¼Œæ¢ç©¶å…¶å› æœæœºè´¨çš„å®‰å…¨æŠ€æœ¯ã€‚

æ¯”å¦‚äººåœ¨å›è·¯ã€æœºæ¢°å¯è§£é‡Šæ€§ï¼Œä¹ŸåŒ…æ‹¬æˆ‘ä»¬åˆšåˆšæå‡ºçš„å¯¹æŠ—æ¼”ç»ƒadvershearsalã€‚ä»–å¯ä»¥é€šè¿‡æé«˜å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§æ¥æå‡å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¹Ÿæå‡AIèƒ½åŠ›ã€‚èƒ½åæ€åœ¨ç¬¬ä¸‰å±‚ï¼Œåˆ™è¦æ±‚AIç³»ç»Ÿä¸ä»…è¦è¿½æ±‚é«˜æ•ˆæ‰§è¡Œä»»åŠ¡ã€‚

è¿˜èƒ½å®¡è§†è‡ªèº«è¡Œä¸ºå¸¦æ¥çš„å½±å“å’Œæ½œåœ¨é£é™©ã€‚ä»è€Œåœ¨è¿½æ±‚ä¿¡ä»»çš„åŒæ—¶ï¼ŒåŒæ—¶ç¡®ä¿å®‰å…¨å’Œé“å¾·çš„è¾¹ç•Œä¸è¢«çªç ´ã€‚è¿™ä¸ªé˜¶æ®µçš„æŠ€æœ¯åŒ…æ‹¬valueuçš„ trainingï¼ŒåŸºäºä»·å€¼çš„è®­ç»ƒï¼Œå› æœå¯è§£é‡Šä»¥åŠåäº‹å¼æ¨ç†ç­‰ã€‚ç›®å‰ã€‚

ä»ä¸šç•Œçš„æŠ€æœ¯å‘å±•æ¥çœ‹ï¼ŒAIçš„å®‰å…¨å’Œæ€§èƒ½æŠ€æœ¯ä¸»è¦åœåœ¨ç¬¬ä¸€é˜¶æ®µéƒ¨åˆ†åœ¨å°è¯•ã€‚ç¬¬äºŒé˜¶æ®µï¼Œè¦çœŸæ­£å®ç°AIçš„å®‰å…¨ä¸æ€§èƒ½å¹³è¡¡ã€‚æˆ‘ä»¬å¿…é¡»å®Œæˆï¼Œå¿…é¡»å®Œå–„ç¬¬äºŒé˜¶æ®µå¹¶æ”€ç™»è‡³ç¬¬ä¸‰é˜¶æ®µã€‚ä¹Ÿå°±æ˜¯è¯´ã€‚

æ²¿ç€å¯ä¿¡AGIçš„å› æœä¹‹åœ°ä¼ºåŸºè€Œä¸Šã€‚æˆ‘ä»¬ç›¸å‘å¯ä»¥çœŸæ„å»ºçœŸæ­£çš„å¯ä¿¡AGIå®ç°äººå·¥ç¨‹çš„å®‰å…¨ä¸å“è¶Šæ€§èƒ½çš„å®Œç¾å¹³è¡¡ã€‚ultimatelyæœ€ç»ˆæˆ‘ä»¬æ˜¯å¸Œæœ›ã€‚åƒå®‰å…¨å¯æ§çš„æ ¸èšå˜æŠ€æœ¯ä¸€æ ·ï¼Œä¸ºå…¨äººç±»å¸¦æ¥æ¸…æ´ä¸°å¯Œçš„èƒ½æºã€‚

æˆ‘ä»¬å¸Œæœ›é€šè¿‡æ·±å…¥ç†è§£AIçš„å†…åœ¨æœºç†å’Œå› æœè¿‡ç¨‹ï¼Œä»è€Œå®‰å…¨æœ‰æ•ˆçš„å¼€å‘å’Œä½¿ç”¨è¿™æ ·é©æ€§çš„æŠ€æœ¯ã€‚ä¹Ÿæ­£å¦‚å¯æ§å¯èšå˜å¯¹å‰æ—¥ç±»ã€‚éƒ½æ˜¯å…±åŒçš„ä¹Ÿä¸€æ ·ã€‚æˆ‘ä»¬åšä¿¡AIçš„å®‰å…¨ä¹Ÿæ˜¯å…¨çƒæ€§çš„å…¬å…±æ‰¶ç¥‰ã€‚éœ€è¦å›½é™…ç¤¾ä¼šçš„å…±åŒåŠªåŠ›å’Œåˆä½œã€‚

æˆ‘ä»¬æ„¿æ„å¤§å®¶ä¸€èµ·æºæ‰‹æ¨è¿›AI45åº¦çš„å‘å±•ï¼Œå…±äº«AIå®‰å…¨æŠ€æœ¯ï¼ŒåŠ å¼ºå®‰å…¨çƒAIå®‰å…¨äººæ‰çš„äº¤æµä¸åˆä½œï¼Œå¹³è¡¡AIå®‰å…¨ä¸èƒ½åŠ›çš„æŠ•å…¥ï¼Œå…±åŒæ„å»ºå¼€æ”¾å®‰å…¨çš„é€šç”¨äººå·¥æ™ºèƒ½åˆ›æ–°ç”Ÿæ€å’Œäººæ‰å‘å±•ç¯å¢ƒã€‚è°¢è°¢å¤§å®¶ã€‚

Thank you very muchï¼Œ Professor Zhouï¼Œ for sharing your inspiring vision for Sa AGIã€‚

 It's an honor to have you todayã€‚ğŸ˜Šï¼ŒThe goal of today' forum is to advance the state of AI safety for the benefit of humanityã€‚

We have organized a forum into four themesï¼Œ AI safety researchï¼Œ AI safetyfe evaluationã€‚

 AI safety guidanceï¼Œ International cooperationã€‚It has been incredible to invite more than 25 leading experts around the worldã€‚

 giving more than 18 presentations and five panels under eight hoursã€‚

I would like to give a brief summary of just a small part of the wisdom A insights that all speakers have provided todayã€‚

For A S safety researchï¼Œ multiple speakers have called for building towards international scientific understanding of AI capabilities and safetyã€‚

 especially at the very frontierã€‚Several put forward the compelling idea of investing in AI safety as a global public goodã€‚

 including with at least 10% AI R And D as a beginning in Chinaã€‚For ASS safetyD evaluationsã€‚

 there was a clear consensus to support the role of third party evaluatorsã€‚

 conduct full life cycle ASS safetyD testing and innovate new methods and science for large model evaluationã€‚

For AI C guidanceï¼Œ societies have to balance the importance of AI developments and AI risk preventionã€‚

 especially for developing countriesã€‚We should also consider a tier approach to risk managementã€‚

There is so much learn from each other on our respective approaches to AI governance in different countries and in different regionsã€‚

Finallyï¼Œ for international cooperationï¼Œ multiple experts have spoken about the need to draw red lines on AI safetyã€‚

As well as to maintain diversity and opennessã€‚And through continued engagement and dialogueã€‚

 the world should work towards international institutions and international conventions for AIã€‚

As Professor Joe saidã€‚Today is only the beginning of conversationã€‚

I hope that our forum today will provide the speakers and the audience with more ideas and opportunity to work together on this critical issue for humanityã€‚

ç‰¹åˆ«æ„Ÿè°¢ä»Šå¤©è®ºå›çš„å„ä½å˜‰å®¾å’Œæœ‹å‹ä»¬ã€‚ä»Šå¤©è®ºå›åœ†æ»¡ç»“æŸã€‚å®‰é‡ŒAIå¸Œæœ›æœ¬è®ºå›å¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨ä¼ä¸šAIå®‰å…¨ä¸æ²»ç†çš„è®¨è®ºå’Œè¡ŒåŠ¨ï¼ŒæœŸå¾…å’Œå¤§å®¶å†è§ï¼Œè°¢è°¢ã€‚ğŸ¼è¯·ä»Šå¤©çš„å˜‰å®¾ç•™æ­¥ï¼Œæˆ‘ä»¬ä¸€èµ·åœ¨ä¸Šå°åˆå½±ï¼Œè°¢è°¢ã€‚

