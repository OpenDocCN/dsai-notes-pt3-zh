# P2ï¼šL1.1.1- è¯¾ç¨‹æ¦‚è¿°ç¬¬ 1 éƒ¨åˆ†ï¼šåŠ¨æœºå’Œä¸»é¢˜ - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let's now get started then with the contents I prepared for this weekã€‚

 I thought I would start with a short teaser what you will be able to do after this courseã€‚

 Of courseï¼Œ it's a long journeyï¼Œ1515 weeks in the futureã€‚

 but at some point you will be able to accomplish something really cool with deep learning things you will learn in this classã€‚

 So here are a few examples of class projects that students worked on in the previous semestersã€‚

 For exampleï¼Œ in this projectï¼Œ students converted audio signals into spectrogramsï¼Œ for exampleã€‚

 spoken textã€‚ðŸ˜Šã€‚

![](img/f5ce1c357294d049e6ea46c9734c8cb7_1.png)

![](img/f5ce1c357294d049e6ea46c9734c8cb7_2.png)

And then they applied convolution neural networks to classify different texts and yeahã€‚

 to extract language out of the audio clipsã€‚ So in this caseã€‚

 it was actually not language I just see it was finger snapping and here singingã€‚

 So a distinguishing between different audio inputsã€‚

 So was one example of a class project from last semesterã€‚



![](img/f5ce1c357294d049e6ea46c9734c8cb7_4.png)

Another one wasï¼Œ yeah working with 3D convolutional networksã€‚

 So this is a 3D version of the so-called MnesT data setã€‚

 which you will be seeing a lot in this classï¼Œ or at least in the introductory lectures later onã€‚

 because it's a yeah simple data set to get started with neural networksã€‚And yeahï¼Œ in this projectã€‚

The students yeah worked with FRMI or yeah magnetic resonance imaging data so like brain scans and so forth and yeah classifying different types of yeah brain scans So that was another interesting project or also yeah students worked with different types of generative adversarial networks which will also be covered at the end of yeah this class where you will be able to generate new data or also mixed data from different data sources So hereã€‚



![](img/f5ce1c357294d049e6ea46c9734c8cb7_6.png)

The studentsï¼Œ yeah mixed artistic paintings or photographsã€‚

 artistic inputs with portrait of a photo modelã€‚ and then the output was basically here shown on the righthand sideã€‚

 like a portrait of a person mixed with a different styles So this would be an example of style transferã€‚

 Yeahï¼Œ why did I pick these three projects and it was kind of arbitrarily It was just somethingã€‚

 Yeah so I looked at the projects were last semester to be honest and looked at which ones had yeah nice figuresã€‚

 So in that way it looked nicer on slidesã€‚ but of course you are free to work on whatever you like for your class project and I will talk more about that laterã€‚

 I don't want to overwhelm you with too many things at the beginningã€‚

Just wanted to show you some examples of things you will be able to accomplish at the end of the semesterã€‚



![](img/f5ce1c357294d049e6ea46c9734c8cb7_8.png)

Yeah also if you're interested a little bit about my research so I'm working a lot on machine learning and deep learning so also yeah just compiled an overview here of projects I worked on so yeah just to also introduce myself and what I'm interested in So yeah last year for example I worked on rank consistent auto regression networks we call that method coral which is forã€‚

Oh yeahã€‚You can think of it as classification of ordinal inputs so if you have class labels that are order and we want to sort them or predict the right order of the labels and also the numeric value associated with it that for that we developed networks here applied to age classification or we worked on face privacy we call this method privacy net where we can hide a facial attributes for exampleã€‚

 age and gender and race and so forth from the input images for protecting one's privacyã€‚

Also yeah collaborated with people from Nvidiaï¼Œ it was more like a review articleã€‚

 we wrote about the latest trends in the realm of Pythonï¼Œ machine learning and deep learningã€‚

 in particular the focus on GPU memory and that's also something we will be talking more about later when we talk about the tools that we will be using for this classã€‚

Yeahï¼Œ with a student of mineï¼Œ I also brought another review article here onã€‚

Machine learning andI based approaches for bioactive ligand discoveryã€‚

 So yeah one of my students is working on yeah small ligand discovery and synthesis also using generative models and generative deep learning models for yeah in the context of molecular molecular synthesis and design and yeah another student of mine is working on few short learning so few short learning is a branch of deep learning that is concerned with learning from small data sets most of the time people use meta learning or transfer learning we will be talking more about transfer learning later in this courseã€‚

 we won't be covering few short learning though I may ask though my student maybe to give a small guest lectureã€‚

If he has time later this semester and Zhgji was working also on this paperã€‚

 he is also our TA in this semester so if you are interested you can ask Zhongji more about different few short learning approaches and he would be very excited to chat more about you more with you about that I think so during office hours if you have questions about few short learning I think he would be excited to talk more about it because he's always excited to talk about itã€‚

Yeah and lastly I'm also yeah working on some traditional machine learning methods so this was in a collaboration where we used not deep learningã€‚

 but yeah traditional machine learning methods in this case nearest neighbor methods for yeah also predictions related to computational biologyã€‚

 So here this was concerned with the structure of GPR which is a G protein coupled receptor that is yeah very important protein or protein receptorã€‚

 it's a binding to small molecules in humans and yeah most most drug targets are actually targeting GCRs but here this was more likeã€‚

Yeahï¼Œ I have fundamental computational biology researchã€‚

 analyzing the structural yeah composition of these proteinsã€‚ So this is just a little bit about meã€‚

 So you can probably see a thes that I like working on deep learning and also have some interest in computationally computational biology applicationsã€‚

 So these two are basically my main research areas and things I'm really excited aboutã€‚



![](img/f5ce1c357294d049e6ea46c9734c8cb7_10.png)

Okayï¼Œ but now let's talk more about the courseã€‚ So yeahï¼Œ for this courseï¼Œ I planned lots of topicsã€‚

 so mainly deep learning and gene ad knownã€‚

![](img/f5ce1c357294d049e6ea46c9734c8cb7_12.png)

Networks like yeahï¼Œ like the cost title suggestsã€‚And I structured this cross into five partsã€‚

 So here are partsã€‚1ï¼Œ2ï¼Œ3ã€‚ And on the next slideï¼Œ I have some more parts of the remaining twoã€‚Soã€‚

 firstï¼Œ in the introductionã€‚That's where we are right nowã€‚

 I wanted to give you a brief overview of this courseã€‚

 and yeah also introduce machine learning and deep learningã€‚

 That's what we are going to do this weekã€‚Then I want to also briefly briefly talk about the history of deep learningã€‚

 And I think that's interestingï¼Œ becauseã€‚That helps you understanding like where the things and motivations are coming fromã€‚

 because yeahï¼Œ deep learningï¼Œ the term deep learning is relatively newã€‚

 it emerged about 10 years agoï¼Œ but it has a long history because yeah deep learning you can think of it as a fancy term for neural networksã€‚

And neural networks have been around for at least 60ã€‚

70 years and are yeah some ideas that emerged very early on that motivated the development of different ideas later on and we will be covering a lot of things related to neural networks So in this this lecture you can think of it as the big picture overviewã€‚

So we will then just briefly cover the history and then later when we are introducing different topics in this lectureã€‚

 we will do this step by step and relate it back to the history and also motivate why we learn about them and why they are usefulã€‚

Yeahï¼Œ and then we will talk about one of the early methods of yeah machine learningã€‚

 a single layer neural networkã€‚ So the perceptron algorithmï¼Œ it's a very traditional algorithmã€‚

 It's not very yeah commonly used nowadays anymoreã€‚

 but I think it's this like a easygo introduction to the problem of classificationã€‚

 So classificationifyingã€‚Oopsã€‚Classifying thingsï¼Œ putting things into different categoriesã€‚And yeahã€‚

 I think that will be a good introduction to get started with the topicã€‚

 and then we will have a small part two hereã€‚Which is concerned with the mathematical and computational foundationsã€‚

 So with thatï¼Œ I mean like introducing some mathematical necessities like linear algebraã€‚

 So indeed deep learningï¼Œ linear algebra is usually usedã€‚To express things more compactlyã€‚

 technically we can or we could use deep learning without linear algebraã€‚

 but it would be very yeah hard to write it down and also slow to implement because when we use deep learning in practice the computing libraries that we use they use or they rely on linear algebra computational routines that help us executing certain computations more efficiently compared to let's say a Python for loop So linear algebra is like in that wayã€‚

 very important for deep learning we won't be covering or needing any advanced linear algebra conceptsã€‚

 just simple yeah vector dot products and matrix multiplicationsã€‚ That's it basicallyã€‚

But I I think it's still worthwhile yeahï¼Œ covering this in a separate lecture because yeahã€‚

 laying down the groundwork for the later lectures properly makes everything later on a little bit easierã€‚

 I thinkã€‚Then we will be talking about gradient descentã€‚ That'sï¼Œ yeah a calculus topicã€‚

 That's gradient descent is the main method forã€‚Yeahï¼Œ parameterizing or training neural networksã€‚

And then after this is more like a refresher after covering this topicã€‚

 we will talk about automatic differentiation with Pytorchã€‚

 So automatic differentiation is yeah calculus on the computerã€‚

 you can think of it like that and we will be using a tool called Pytorchã€‚

 which is a library for Ya linear algebra automatic differentiation and then also neural network training or deep learning and it also allows us to implement things on the GPU to make things more efficientã€‚

 So I will also explain them hereã€‚In lecture 7ï¼Œ how you can use cluster and cloud computing resourcesã€‚

 it will be a relatively short part though because yeah the main the main topic is deep learning of course computational aspects are necessary but for this introductory class you don't have to be an expert programmer and or yeah user of computers you should be familiar with certain things on your computer and certain programming aspects but we are not here in machine learning engineering more like giving a conceptual overview so you will get by with some free resources that I will talk about in this lectureã€‚

 but if you are interested you can of course also use more advanced resources for exampleã€‚

 or campusesï¼Œ HTCC and so forthã€‚But it won't be required for this classã€‚ Yeahã€‚

 and then after the mathematical and computational foundationsã€‚

 we will be talking then finally about neural networksã€‚ So in this part 3ã€‚

 I will lay the groundwork for yeahï¼Œ deep learningã€‚

 So we will talk or we start with logistic regressionã€‚

 which you can think of a single layer neural networkã€‚ So this is basically an extension of thisã€‚

Yeahï¼Œ single network that we talked about earlierã€‚ that is nowã€‚

Differentiable and using the logistic regression as a starterï¼Œ we will add additional hidden layersã€‚

 making this a deep networkï¼Œ which is also called multilayer perceptronã€‚

 and then we will learn how we can train such a multilay perceptron using the back propagation algorithm then parts here parts 10 to 12 are more like tricks for training deep neural networksã€‚

 for exampleã€‚Regularization techniques to avoid overfitting input normalization and weight initializationã€‚

 it's just yeah making training neural networks more robust and faster and then also talking about learning rates and yeah some advanced optimization algorithmsã€‚

 So like fancier versions of gradient in descent essentially And these are really kind of necessary to make neural networks work well in practiceã€‚

 these topics may not sound super excitingï¼Œ especially like 10 and 11 but they are super useful or importantã€‚

 even yet to make neural networks work wellã€‚

![](img/f5ce1c357294d049e6ea46c9734c8cb7_14.png)

And then we will get to the interesting parts in this courseï¼Œ or I would say the more advanced partsã€‚

 so here in part4 we will then be talking about deep learninging for computer vision and language modelingã€‚

 so we will spend a lot of time on convolutional networksï¼Œ So this is one big topicã€‚

And then we will also talk about recurrent neural networksã€‚ They are for language modelingã€‚

 so convolution networks are more for image modelingã€‚

 although you can also use the one dimensional convolution network for textã€‚But yeahã€‚

 text will be more focused on in lecture 15 and these will kind of also lay the groundwork for the deep generative models that we will be talking aboutã€‚

So in terms of deep generative models we will be talking about auto encodersã€‚

 so-called variational auto encodersã€‚ then we will talk about generative adversary networksã€‚

 You may already have heard of them as GNsã€‚ So just the long form of writing G generative adversary networkã€‚

 then this is also a very big topicã€‚ we will have aã€‚2 lectures on thatã€‚

 So one introduction and then one on some more advanced Gsã€‚ for exampleï¼Œ the bastein Gã€‚

 and then also how we can evaluate and compare different Gs to each other because now in this partã€‚

We are focused on predictionã€‚Oopsã€‚Prediction andã€‚And in the second part hereï¼Œ we are focused onã€‚

Generatingã€‚Thinksï¼Œ so it's a little bit differentã€‚ It's a little bit trickier to ever these modelsã€‚

 So we will have a lecture on thatã€‚And then I also plan to cover some aspects about recurrent neural networks forative modelingã€‚

 for exampleï¼Œ generating new text on using or in a sequence to sequence contextã€‚

 So here in lecture 15ï¼Œ I will first try also to focus only on the prediction partsã€‚

 but we will be revisiting this topic also for yeah generating new data on text and then also going into a more advanced topicã€‚

Adding the so called attention mechanism to R and ends and then also explaining self attention in the context of transformersã€‚

 which are yeahï¼Œ underlying the models that you have heard about in the media probably one is called bird or Gã€‚

2 and GP3ã€‚ So these are the building blocks of these modelã€‚ So we'll also talk about thoseï¼Œ soã€‚

I't want to make this too crowded hereï¼Œ but this part will be essentiallyã€‚For imagesã€‚

And these two last parts hereã€‚Will be for textã€‚ So we will have both generative models for images and for textã€‚



![](img/f5ce1c357294d049e6ea46c9734c8cb7_16.png)