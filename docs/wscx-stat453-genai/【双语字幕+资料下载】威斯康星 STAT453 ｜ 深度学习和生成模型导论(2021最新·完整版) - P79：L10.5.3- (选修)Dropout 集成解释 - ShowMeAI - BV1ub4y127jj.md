# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P79ï¼šL10.5.3- (é€‰ä¿®)Dropout é›†æˆè§£é‡Š - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ now let's look at other explanation for why dropout might perform wellã€‚

 and this is in the context of ensemble methodsï¼Œ so we can essentially see dropout as an ensemble methodã€‚

 as I will show you in the next couple of slidesã€‚And yeahï¼Œ what is an ensemble methodã€‚

 So students who have taken 451ï¼Œ we talked there about ensemble methods a lotã€‚ we talked aboutã€‚

 for instanceï¼Œ majority votingï¼Œ beggingï¼Œ random forestsï¼Œ boostingã€‚

 You don't have to know any of that for this class hereï¼Œ But in a nutshellã€‚

 what is an ensemble methodã€‚It's essentially a combination of multiple models where you average the results of the modelsã€‚

 So you can think of it intuitivelyã€‚ let's say you have to make an important financial decisionã€‚

 let's say yeah you want to make or you want to buy something or invest let's say you want to invest into somethingã€‚

 and it's a very important decision So you consider asking an expertã€‚

 So the expert here is your modelï¼Œ and it might give you a prediction or some adviceã€‚That might beã€‚

 yeahï¼Œ a good advice if it's an expert and expertã€‚Now is the topic very wellã€‚ howeverï¼Œ in practiceã€‚

 it's usually an even better idea to ask a couple of experts and then considerã€‚

 yeah the shared opinion of the expertsã€‚ So it's not always better than let's say the opinion of a single personã€‚

 especially if yeah the committee doesn't agree with a single person and the single person knows something the committee doesn't knowã€‚

 but on averageï¼Œ usually asking committee of multiple people is a better idea than just relying on the opinion of a single person right so in that wayã€‚

 using multiple models and then averaging the predictions or taking the majority voteã€‚

Its also often a better idea than yeah using a single modelã€‚ Why don't we do that always thenï¼Œ Yeahã€‚

 wellï¼Œ because it's usually very expensive to train modelsï¼Œ especially in deep learningã€‚

 So in deep learningï¼Œ I would say most of the time we want to focus on a single model because it's computationally cheaper and we also often care about yeah improving a model in generalã€‚

 And then let's say in productionï¼Œ you can always yeah train multiple of these models and then combine themã€‚

To make the predictions even more robustï¼Œ in any caseã€‚

 talking about the ensemble approach now why dropout can be seen as an ensemble methodã€‚



![](img/a04da2e34492efeefd42353113fafb08_1.png)

So you can think of yeahï¼Œ drop out the procedure as having a different model for each mini batchã€‚

 rightï¼Œ so because you randomly drop notes each forward passã€‚

 each mini batch will see a slightly different model andã€‚Essentiallyã€‚

 what we do is we sample over2 to the power of H modelsï¼Œ by H is the number of hidden unitsã€‚

 If we only consider a hidden layerï¼Œ if we have multiple hidden layersï¼Œ wellã€‚

 this becomes even largerã€‚ but if you have alreadyï¼Œ let's sayã€‚

Well if you only have a hidden layer with 10 unitsã€‚

 you already have 10 to the power of 10 possible combinationsã€‚

Of hidden layers that you may sample during each forward passã€‚So in that wayï¼Œ itã€‚

 it can be seen as a model ensembleã€‚Except that there is one restrictionã€‚

And the restriction is that we have a time dimensionã€‚ So we don't have these models in parallelã€‚

 rightï¼Ÿ So what I mean is if we haveã€‚Say hidden thereã€‚

 a network like that and another network like thatã€‚We it beã€‚ğŸ˜”ï¼ŒIm runningning out of spaceã€‚

Let's say each of these is one multiceptronã€‚ I'm not connecting all the units hereã€‚

 but let's say in this first oneï¼Œ first fo passï¼Œ we drop this one and the other oneã€‚

 we drop this oneã€‚ And then here we drop these onesã€‚ So we have three different networks hereã€‚

 but we don't use all of them in parallelï¼Œ rightï¼Œ We usuallyã€‚Because it's a training for for loopã€‚

 weã€‚Go through them one at a timeï¼Œ So weã€‚Have during the first followed passï¼Œ maybe this modelã€‚

 during the second passï¼Œ this fall this model and during the third pass this modelã€‚

 So that's essentially a time constraintsï¼Œ a restrictionã€‚Andã€‚

We can also see this restriction as weight sharingï¼Œ becauseã€‚The second modelã€‚Receive or willã€‚

 the first model will update the weightsï¼Œ and then the second modelã€‚

Will be depending on the weight updates from the first modelï¼Œ rightã€‚

 It's just like how the regular training worksï¼Œ because each iteration you update the weights and each consequent model will work with the weights from the previous backward passã€‚

 So in this wayï¼Œ there's like this weight sharing between the different forward passesã€‚

So there's the weight sharing over these modelsã€‚ And yeahã€‚

 we can see that as a type of regularizationï¼Œ like a constrainedã€‚

 an additional piece of information or constraint that we addã€‚ So in that wayã€‚

 we can see this weight sharing as aï¼Œ yeahï¼Œ as a regularization andã€‚Stillï¼Œ we haveã€‚ meanwhileã€‚

 we have the weight sharing during trainingï¼Œ we could technicallyã€‚

Create all these different models after training during inference and then average over all these modelsã€‚

I meanï¼Œ there's nothing that prevents us from doing thatã€‚

 The only problem with that is if we even only consider a small case with 10 to the power of 10 combinationsã€‚

 This is like a very large combination of modelsã€‚ So this is veryï¼Œ veryï¼Œ very expensiveã€‚

 And this is something we technically wouldn't want to do in practice because yeahï¼Œ it'sã€‚

 its just way too expensiveã€‚

![](img/a04da2e34492efeefd42353113fafb08_3.png)

But yeahï¼Œ let's just for a secondï¼Œ continue with this thought experiment and assume we have nowã€‚Yeahã€‚

 created all these models for or during inferenceã€‚ Soï¼Œ for instanceã€‚

 if we had a hidden layer with 10 unitsï¼Œ we have these 10 to the2 to the power of 10 modelsã€‚

 Let's call thatã€‚And we have M models nowã€‚ So how do we average the predictions of these models for simplicityã€‚

 Think of a binary classification caseã€‚ And in this caseã€‚

 it's essentially averaging the look likelihoodsã€‚Of the predictionsã€‚

 which is essentially what you probably know as the geometric meanï¼Œ soã€‚For instanceã€‚

 if we have a P probability Sc P for given test data point Iã€‚ So if this's test data point I hereã€‚

 then we multiply all these probabilities for the M models and take thisã€‚To the power of one over Mã€‚

 And yeahï¼Œ I was just rewriting this to show you this isï¼Œ yeahï¼Œ essentially averaging likelihoodsã€‚

 soã€‚So sum over the lock likelihood terms hereã€‚ and then times1 over Mã€‚

 And here I'm just adding the exponentã€‚ So I was just taking considering the lock of this here and this one un the un does itã€‚

 So it's essentially the same computationã€‚ and here inside you can think of it as averaging the lock likelihoodsã€‚

 So essentially what this is its we are computing the geometric meanã€‚

So that would be one way we can combine the predictionsã€‚And yeahï¼Œ if we have multiple classes moreã€‚

Yeahï¼Œ more than binary classificationã€‚ we also want to normalize these so that they sum up to one probabilitiesã€‚

 because we have multiple classesï¼Œ rightï¼Œ if we have multiple classesã€‚

 we have multiple probabil scores and ideallyï¼Œ we want them to sum up to oneã€‚

 and the class label can then be obtained by considering the class with the highest probabilityã€‚



![](img/a04da2e34492efeefd42353113fafb08_5.png)

Yeahï¼Œ but this still doesn't solve our problem that this is very computationally expensive because we we have to consider2 to the power of 10 models right for averagingã€‚

 And here this is also assuming that we only have one hidden layer with only 10 units if we have a hidden layerã€‚

 let'd sayï¼Œ with 64 unitsã€‚ Then yeah this would be really infeasibleã€‚So what do we do about thatã€‚

 How can we address this problemï¼Œ So actuallyï¼Œ the regular dropout technique that we discussed earlierã€‚

Essentially is already computing this geometric meanã€‚

 or essentially it's approximating this geometric meanã€‚ because yeahã€‚

 the scaling factor that I mentionedï¼Œ the 1 minus p that we use to scale the data after training when we use the model for testingã€‚

 This is essentially an approximation of this geometric meanã€‚

 So we don't have to create all these different modelsã€‚

 we only consider the last model after trainingã€‚ So just the model that comes out out of training and then this scaling will essentiallyã€‚

Compute an approximated version of that geometric meanã€‚

 as they mentioned in the original dropout paperã€‚And they also argueï¼Œ essentiallyï¼Œ if youã€‚

Have a linear modelã€‚Then the geometric mean would be actually exactly like this scaled version hereã€‚

 So essentially we are approximating the geometric mean in dropout of a model ensembleã€‚

 And this is essentially the explanation of dropout why it might work wellã€‚

 because essentially we can think of it as a model ensembleã€‚



![](img/a04da2e34492efeefd42353113fafb08_7.png)