# P79ÔºöL10.5.3- (ÈÄâ‰øÆ)Dropout ÈõÜÊàêËß£Èáä - ShowMeAI - BV1ub4y127jj

YeahÔºå now let's look at other explanation for why dropout might perform well„ÄÇ

 and this is in the context of ensemble methodsÔºå so we can essentially see dropout as an ensemble method„ÄÇ

 as I will show you in the next couple of slides„ÄÇAnd yeahÔºå what is an ensemble method„ÄÇ

 So students who have taken 451Ôºå we talked there about ensemble methods a lot„ÄÇ we talked about„ÄÇ

 for instanceÔºå majority votingÔºå beggingÔºå random forestsÔºå boosting„ÄÇ

 You don't have to know any of that for this class hereÔºå But in a nutshell„ÄÇ

 what is an ensemble method„ÄÇIt's essentially a combination of multiple models where you average the results of the models„ÄÇ

 So you can think of it intuitively„ÄÇ let's say you have to make an important financial decision„ÄÇ

 let's say yeah you want to make or you want to buy something or invest let's say you want to invest into something„ÄÇ

 and it's a very important decision So you consider asking an expert„ÄÇ

 So the expert here is your modelÔºå and it might give you a prediction or some advice„ÄÇThat might be„ÄÇ

 yeahÔºå a good advice if it's an expert and expert„ÄÇNow is the topic very well„ÄÇ howeverÔºå in practice„ÄÇ

 it's usually an even better idea to ask a couple of experts and then consider„ÄÇ

 yeah the shared opinion of the experts„ÄÇ So it's not always better than let's say the opinion of a single person„ÄÇ

 especially if yeah the committee doesn't agree with a single person and the single person knows something the committee doesn't know„ÄÇ

 but on averageÔºå usually asking committee of multiple people is a better idea than just relying on the opinion of a single person right so in that way„ÄÇ

 using multiple models and then averaging the predictions or taking the majority vote„ÄÇ

Its also often a better idea than yeah using a single model„ÄÇ Why don't we do that always thenÔºå Yeah„ÄÇ

 wellÔºå because it's usually very expensive to train modelsÔºå especially in deep learning„ÄÇ

 So in deep learningÔºå I would say most of the time we want to focus on a single model because it's computationally cheaper and we also often care about yeah improving a model in general„ÄÇ

 And then let's say in productionÔºå you can always yeah train multiple of these models and then combine them„ÄÇ

To make the predictions even more robustÔºå in any case„ÄÇ

 talking about the ensemble approach now why dropout can be seen as an ensemble method„ÄÇ



![](img/a04da2e34492efeefd42353113fafb08_1.png)

So you can think of yeahÔºå drop out the procedure as having a different model for each mini batch„ÄÇ

 rightÔºå so because you randomly drop notes each forward pass„ÄÇ

 each mini batch will see a slightly different model and„ÄÇEssentially„ÄÇ

 what we do is we sample over2 to the power of H modelsÔºå by H is the number of hidden units„ÄÇ

 If we only consider a hidden layerÔºå if we have multiple hidden layersÔºå well„ÄÇ

 this becomes even larger„ÄÇ but if you have alreadyÔºå let's say„ÄÇ

Well if you only have a hidden layer with 10 units„ÄÇ

 you already have 10 to the power of 10 possible combinations„ÄÇ

Of hidden layers that you may sample during each forward pass„ÄÇSo in that wayÔºå it„ÄÇ

 it can be seen as a model ensemble„ÄÇExcept that there is one restriction„ÄÇ

And the restriction is that we have a time dimension„ÄÇ So we don't have these models in parallel„ÄÇ

 rightÔºü So what I mean is if we have„ÄÇSay hidden there„ÄÇ

 a network like that and another network like that„ÄÇWe it be„ÄÇüòîÔºåIm runningning out of space„ÄÇ

Let's say each of these is one multiceptron„ÄÇ I'm not connecting all the units here„ÄÇ

 but let's say in this first oneÔºå first fo passÔºå we drop this one and the other one„ÄÇ

 we drop this one„ÄÇ And then here we drop these ones„ÄÇ So we have three different networks here„ÄÇ

 but we don't use all of them in parallelÔºå rightÔºå We usually„ÄÇBecause it's a training for for loop„ÄÇ

 we„ÄÇGo through them one at a timeÔºå So we„ÄÇHave during the first followed passÔºå maybe this model„ÄÇ

 during the second passÔºå this fall this model and during the third pass this model„ÄÇ

 So that's essentially a time constraintsÔºå a restriction„ÄÇAnd„ÄÇ

We can also see this restriction as weight sharingÔºå because„ÄÇThe second model„ÄÇReceive or will„ÄÇ

 the first model will update the weightsÔºå and then the second model„ÄÇ

Will be depending on the weight updates from the first modelÔºå right„ÄÇ

 It's just like how the regular training worksÔºå because each iteration you update the weights and each consequent model will work with the weights from the previous backward pass„ÄÇ

 So in this wayÔºå there's like this weight sharing between the different forward passes„ÄÇ

So there's the weight sharing over these models„ÄÇ And yeah„ÄÇ

 we can see that as a type of regularizationÔºå like a constrained„ÄÇ

 an additional piece of information or constraint that we add„ÄÇ So in that way„ÄÇ

 we can see this weight sharing as aÔºå yeahÔºå as a regularization and„ÄÇStillÔºå we have„ÄÇ meanwhile„ÄÇ

 we have the weight sharing during trainingÔºå we could technically„ÄÇ

Create all these different models after training during inference and then average over all these models„ÄÇ

I meanÔºå there's nothing that prevents us from doing that„ÄÇ

 The only problem with that is if we even only consider a small case with 10 to the power of 10 combinations„ÄÇ

 This is like a very large combination of models„ÄÇ So this is veryÔºå veryÔºå very expensive„ÄÇ

 And this is something we technically wouldn't want to do in practice because yeahÔºå it's„ÄÇ

 its just way too expensive„ÄÇ

![](img/a04da2e34492efeefd42353113fafb08_3.png)

But yeahÔºå let's just for a secondÔºå continue with this thought experiment and assume we have now„ÄÇYeah„ÄÇ

 created all these models for or during inference„ÄÇ SoÔºå for instance„ÄÇ

 if we had a hidden layer with 10 unitsÔºå we have these 10 to the2 to the power of 10 models„ÄÇ

 Let's call that„ÄÇAnd we have M models now„ÄÇ So how do we average the predictions of these models for simplicity„ÄÇ

 Think of a binary classification case„ÄÇ And in this case„ÄÇ

 it's essentially averaging the look likelihoods„ÄÇOf the predictions„ÄÇ

 which is essentially what you probably know as the geometric meanÔºå so„ÄÇFor instance„ÄÇ

 if we have a P probability Sc P for given test data point I„ÄÇ So if this's test data point I here„ÄÇ

 then we multiply all these probabilities for the M models and take this„ÄÇTo the power of one over M„ÄÇ

 And yeahÔºå I was just rewriting this to show you this isÔºå yeahÔºå essentially averaging likelihoods„ÄÇ

 so„ÄÇSo sum over the lock likelihood terms here„ÄÇ and then times1 over M„ÄÇ

 And here I'm just adding the exponent„ÄÇ So I was just taking considering the lock of this here and this one un the un does it„ÄÇ

 So it's essentially the same computation„ÄÇ and here inside you can think of it as averaging the lock likelihoods„ÄÇ

 So essentially what this is its we are computing the geometric mean„ÄÇ

So that would be one way we can combine the predictions„ÄÇAnd yeahÔºå if we have multiple classes more„ÄÇ

YeahÔºå more than binary classification„ÄÇ we also want to normalize these so that they sum up to one probabilities„ÄÇ

 because we have multiple classesÔºå rightÔºå if we have multiple classes„ÄÇ

 we have multiple probabil scores and ideallyÔºå we want them to sum up to one„ÄÇ

 and the class label can then be obtained by considering the class with the highest probability„ÄÇ



![](img/a04da2e34492efeefd42353113fafb08_5.png)

YeahÔºå but this still doesn't solve our problem that this is very computationally expensive because we we have to consider2 to the power of 10 models right for averaging„ÄÇ

 And here this is also assuming that we only have one hidden layer with only 10 units if we have a hidden layer„ÄÇ

 let'd sayÔºå with 64 units„ÄÇ Then yeah this would be really infeasible„ÄÇSo what do we do about that„ÄÇ

 How can we address this problemÔºå So actuallyÔºå the regular dropout technique that we discussed earlier„ÄÇ

Essentially is already computing this geometric mean„ÄÇ

 or essentially it's approximating this geometric mean„ÄÇ because yeah„ÄÇ

 the scaling factor that I mentionedÔºå the 1 minus p that we use to scale the data after training when we use the model for testing„ÄÇ

 This is essentially an approximation of this geometric mean„ÄÇ

 So we don't have to create all these different models„ÄÇ

 we only consider the last model after training„ÄÇ So just the model that comes out out of training and then this scaling will essentially„ÄÇ

Compute an approximated version of that geometric mean„ÄÇ

 as they mentioned in the original dropout paper„ÄÇAnd they also argueÔºå essentiallyÔºå if you„ÄÇ

Have a linear model„ÄÇThen the geometric mean would be actually exactly like this scaled version here„ÄÇ

 So essentially we are approximating the geometric mean in dropout of a model ensemble„ÄÇ

 And this is essentially the explanation of dropout why it might work well„ÄÇ

 because essentially we can think of it as a model ensemble„ÄÇ



![](img/a04da2e34492efeefd42353113fafb08_7.png)