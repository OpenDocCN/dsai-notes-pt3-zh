# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P116ÔºöL14.3.1.2- PyTorch ‰∏≠ÁöÑ VGG16 - ShowMeAI - BV1ub4y127jj

![](img/9c0c719111bb824ca05ef070abb9e2b7_0.png)

AlrightÔºå let's now take a look at the code example of VGG16 that I implemented here I should mention that I don't want to rerun this year from scratch during yeah this video because it took one and a half hours to train it's a large network so in that way we will just take a look at the results so I will of course yeah share this with you should find the link another video„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_2.png)

YeahÔºå so what we have here is all the usual importsÔºå watermarkÔºå torch torch vision and so forth„ÄÇ

Then my helper files as usualÔºå they are I think identical to what I used to last week„ÄÇ

 I can't remember making a modification to themÔºå so exactly the same like that we used for AlexNe„ÄÇ

 the only difference here is really that I have a different architecture„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_4.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_5.png)

So defining our hyperparile settingsÔºå random seatd shouldn't be a hyperparile but you have to set it to some things I'm setting it to one to3 batch size 256 and we are training for 50 epochs and I was using a GPU for that because otherwise it took too long so you could for instance run this on Google Colllap on the GPU„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_7.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_8.png)

If you run things on the GPU„ÄÇThis function doesn't really work that well anymore because I actually cut this from the videos because it was too long„ÄÇ

 but just to briefly mention why so when you train things on the GPU there are different types of algorithms used for convolutions„ÄÇ

 So there'sÔºå of courseÔºå the convolution that we talked about in the lecture„ÄÇ

 but in code people don't implement it this wayÔºå they are usually more efficient approximations of that like a fast fourier transform based ones and depending on what computer you use and what graphics card you use„ÄÇ

Different approximation algorithms are usedÔºå although these approximations are pretty good„ÄÇ

 they are approximating the convolution veryÔºå very well„ÄÇ

 there are tiny differences after the decimal point„ÄÇ And if you have a lot of tiny differences„ÄÇ

 they can add up„ÄÇ and then sometimes you will„ÄÇFind that results are different when you run them again„ÄÇ

 because not only are different algorithms automatically chosen„ÄÇBased on the computer„ÄÇ

 But also when you run it multiple times„ÄÇ So NviDdia is actually running some automatic way to guess which algorithm might be a good choice at a given time„ÄÇ

 So that is why there are sometimes slight differences„ÄÇ

 It used to that you could set this to a deterministic setting„ÄÇ but for some reason„ÄÇ

 it complains now that this deterministic setting doesn't really work anymore„ÄÇ But any case„ÄÇ

 you don't have to worry about that„ÄÇ It's very normal in deep learning that you get different results if you run things again„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_10.png)

I meanÔºå they will be slightly different„ÄÇ They will not be very different„ÄÇ It's just like very„ÄÇ

 very small differences„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_12.png)

Any caseÔºå let's not worry about this okay so„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_14.png)

Talking about the data now„ÄÇ So here we' are working with a Cy 10 dataset„ÄÇWhich is 32 by 32„ÄÇ But yeah„ÄÇ

 I„ÄÇResize thisÔºå because V GÔºå if you„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_16.png)

Look at this figure again„ÄÇ it was originally developed for 224 times 234 inputs„ÄÇ

 and there are just too many layers for small inputs because you have the size of the layers„ÄÇ

 and at that that pointÔºå if you have half the input size„ÄÇ

 you will already have only3 by three or4 by4 layers here„ÄÇ And if you make it even smaller„ÄÇ

 it would be one by one„ÄÇEven smaller if you have a here or e one by one and then try to half it„ÄÇ

 So that wayÔºå we can't have inputs that are too small„ÄÇ It actually works with 32 by 32„ÄÇ

 but the results were not as good„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_18.png)

So it was fasterÔºå of courseÔºå but„ÄÇWith this version as slightly upscaleÔºå I get actually85% accuracy„ÄÇ

So what I'm doing here is I'm upscaling these images from 3 32 to 7070 then I do a slight random crop here so the random crop is to avoid overfitting or not avoid overfitting but to reduce the overfitting to make it a little bit less sensible or sensitive to exact pixel locations and then I'm converting it to a tensor and then normalizing it so that they will the pixels will be sent at zero across the channels and have a standard deviation of one„ÄÇ

So that's the usual procedure„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_20.png)

AgainÔºå for testingÔºå I don't do any random crop I am centre cropping„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_22.png)

Then„ÄÇThe rest is exactly the same that we talked about for Alexnet last week„ÄÇ

 So here's the interesting part„ÄÇ That's the VG G 16 architecture„ÄÇ Yeah„ÄÇ

 I just had some notes for myself for calculating the paddingÔºå but„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_24.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_25.png)

We probably don't need that here„ÄÇHow I implemented it in different blocks„ÄÇ

 So I have one block that is usually the convolutions and then stopped or the final layer in that block is the max pooling layer that reduces the size by half„ÄÇ

 So these„ÄÇPreserve the size„ÄÇ It's the same convol with the padding such that the input equals the output size and max pudding reduces the size„ÄÇ

 And then againÔºå hereÔºå this consserves the size„ÄÇ This reduces the size by half because of the stride„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_27.png)

Then here„ÄÇAgainÔºå we haveÔºå yeahÔºå we have„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_29.png)

Another block„ÄÇWith max pooling„ÄÇAnother block„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_31.png)

Max pullinging„ÄÇAnd another block„ÄÇAnd max pooling„ÄÇ And then these are what I call my feature extractor layers„ÄÇ

 And then we have a classifier layer„ÄÇ This is really if I go back to the figure„ÄÇ

 this is really this last part here in light blue„ÄÇ This is the fully connected part„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_33.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_34.png)

So here a linear layerÔºå a fully connected layerÔºå essentially another one and another one„ÄÇ

 So you have dropout I should probably yeah this is one deep dropout version„ÄÇ

 So if you want to add dropout for other layersÔºå you have to I would recommend using dropout 2D„ÄÇ

ActuallyÔºå I recallÔºå I forgot about that recently and I used a regular dropout somewhere because I for some reason forgot to type 2 D and I was wondering why I couldn't notice any difference„ÄÇ

AlrightÔºå yeah here I'm using the gaminging uniform weights„ÄÇ

 Is skip here because that's what was used in the original paper„ÄÇ

 Then I have an adaptive average poolingÔºå which will in this case„ÄÇ

 make things equal to the height and width here so that I chose„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_36.png)

To be 3 by3Ôºå so„ÄÇBecause for the linear layerÔºå you have to know the inputÔºå height and width„ÄÇ

Because you need to know the number of features„ÄÇ We know always know the number of channelsÔºå right„ÄÇ

 because it's coming from hereÔºå but sometimes it's hard to know what's the width and height to compute the number of features„ÄÇ

So usually what we do is we have here the feature map and then average pulling„ÄÇ

 And then what we do is we„ÄÇFlattenÔºå this is essentially„ÄÇFlattening operation„ÄÇ

 This is essentially the same that you do when you work with multilay perceptrons and Mness„ÄÇ

 So you basically flatten the inputÔºå which is a imageÔºå you flatten this to be a long vector„ÄÇ

 That's the the same operation here„ÄÇ And for thatÔºå you have to know the number of parameterss because you need to know the number of weights right for the multilay perceptron layer„ÄÇ

 the linear layer„ÄÇSo how would you get this information hereÔºå I'm using adaptive average pooling„ÄÇ

So adaptive average pudding is an operation where you can determine the input and output size and it will either add the strides or the padding such that this dimension will be met„ÄÇ

 So if the input is so where am I here if the input to this one is smaller than three by3„ÄÇ

 it will add padding if it's larger than three by3„ÄÇ

 it will not add padding and do some strides to reach that size that we desire„ÄÇÂóØ„ÄÇYeahÔºå and„ÄÇ

If you don't want to do adaptive average pooling and you want to know the number of„ÄÇ

Sizes that come out of the last blockÔºå for example„ÄÇ

 one way would of course to use the equation that I showed you and just calculated by hand„ÄÇ

That is a valid approach that could be a potential exam questionÔºå but we don't have an exam anymore„ÄÇ

 so don't worry about it maybe something for the quizÔºå but in practice„ÄÇ

 yeah people would write forward or backward hooks I think I explained it so you would use a forward hook here I explained it to you when we talked about the Pytoch API but to be honest„ÄÇ

Even that is something most people won't do because it's just too much work to write a hook function„ÄÇ

 I meanÔºå I wouldn't do it„ÄÇ The simpleÔºå brute false way would be to just implement a print statement temporarily„ÄÇ

 So what you would do is you would just have something that's lazy wayÔºå the lazy but efficient way„ÄÇ

 something like x does the sizeÔºå print the size here of x„ÄÇThen you would run this„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_38.png)

HereÔºå when you do the trainingÔºå it will output the size„ÄÇ

 It will probably crash if you don't have the right dimensions„ÄÇ But then you know the sizeÔºå right„ÄÇ

 you knowÔºå the height and width thenÔºå and then you can„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_40.png)

Remove it„ÄÇAt the desired height and width here„ÄÇ And you are good to go„ÄÇ So in practiceÔºå most people„ÄÇ

 to be honestÔºå are just inserting„ÄÇ if you want to know the sizeÔºå just import the„ÄÇPrint statement„ÄÇ

 And that is how you find out the brute force way„ÄÇ Okay„ÄÇ

 so this is essentially the energy G architecture„ÄÇ so you can see„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_42.png)

They are essentially conversion layers„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_44.png)

Followed always by max pooling that halfalves the input size„ÄÇ We have many of those„ÄÇ

 then these classification layers here„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_46.png)

ÂóØ„ÄÇMiao„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_48.png)

And then„ÄÇWe have here the training„ÄÇ So I' am initializing it for 10 classes„ÄÇ

 If we look back to the slidesÔºå it's 1000 because imagenetÔºå the dataset„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_50.png)

That they used here had 1000 classes„ÄÇ We have Cypher 10 with 10 classes only„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_52.png)

I use SGD with momentumÔºå and there's our learning rate scheduler that reduces the learning hell rate by a factor of 10„ÄÇ

 Sore dividing it by 10 if„ÄÇThe validation accuracy doesn't improve„ÄÇ

 So this is just my training script that I used for Alex 92„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_54.png)

Al rightÔºå so here is then the training the same as with Alexnet„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_56.png)

Can say see it trains pretty quicklyÔºå35%Ôºå50%Ôºå53% accuracy„ÄÇ So it's slowly climbing up„ÄÇ at some point„ÄÇ

 it stops„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_58.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_59.png)

Around hereÔºå it's improving a little bitÔºå but it takes longerÔºå more epochs„ÄÇ So I do that„ÄÇ So it's„ÄÇ

 sometimesÔºå you knowÔºå it's very useful to look at this„ÄÇ If you run an expensive network like Widge G„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_61.png)

It might be taking a few hours„ÄÇ and you probably want to take a look at the beginning„ÄÇ

 whether it's even worthwhile training it for 50 epochsÔºå right„ÄÇ

 So if you notice that the loss doesn't go downÔºå maybe in the first four epochs or something like that„ÄÇ

 or you notice the accuracy doesn't improve„ÄÇ Then I would just stop the training and maybe change some parameters before you just waste one and a half hours waiting until it's finishes„ÄÇ

 So that is why I'm printing this during the training„ÄÇ There's a tool called„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_63.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_64.png)

Tensor board„ÄÇThat can create visualizations during training„ÄÇ

 We are not talking about this in this class because I think we already have enough tools for you to learn about„ÄÇ

 So there's already enough code going on„ÄÇ But you're very welcome to check this out at some point„ÄÇ

 It's also yeahÔºå nice for visualizing things„ÄÇ But yeahÔºå hereÔºå I'm trying to keep things simple„ÄÇ

 So not too many tools at once„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_66.png)

Sorry enough you have to learn in a way„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_68.png)

But at the endÔºå I always find it still helpful to take a look at visualizations here„ÄÇ

 I have make them of courseÔºå with Meprolip so that you can see them during training only after training but yeah what you can see is the loss goes down pretty nicely so it kind of converges here after 50 epochs maybe it would still approve a little bit but looking at the validation performance here so in orange you can see maybe maybe it would go up slightly more but you can already see after epoch1 there' a huge amount of overfitting so to reduce that overfitting maybe adding some dropout 2D might help„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_70.png)

You can actually this may be a good exercise„ÄÇ you can actually insert a regular dropout and you will see with a regular dropod that you won't reduce overfitting too much„ÄÇ

 but if you use dropod 2 D that helps actually more with convolutional networks„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_72.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_73.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_74.png)

OkayÔºå so this is that and then here just the visualization looking at some examples you can see most of that looks correct so P is predicted T is on the true label„ÄÇ

 you can see this one is wrong de and frorogÔºå it's hard to tell„ÄÇ

 I mean if you look at this Cypher 10 has such a low resolution that even we have difficulties telling what's in these images I think„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_76.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_77.png)

AlrightÔºå so here's a confusion matrix„ÄÇ So that looks actually interesting„ÄÇ

 You can see dog and cat are often misclassified here„ÄÇ

 and that is kind of reasonable because cats and dogs are both animals„ÄÇ So a cat in a docker„ÄÇ

 for exampleÔºå very different from an airplane„ÄÇ So dog and airplane is rarely confused whereas dog and cat is more often confused in the grand scheme of things that's kind of reasonable„ÄÇ

 I would sayÔºå so you can actually see all the animals„ÄÇ That's very interesting„ÄÇ

 So you can see all the animals here in this square are often„ÄÇüòäÔºåYeah„ÄÇ

 misclassified compared to ships and trucks and like automobiles„ÄÇ So here you have things„ÄÇ

 and then you have the animals„ÄÇ animals among each other are harder to classify compared to other objects like airplanes and automobiles„ÄÇ

 AlrightÔºå so this is VG G16„ÄÇ maybe one more thing„ÄÇ I have VG G16 standardized„ÄÇ

 was just toing around with it„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_79.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_80.png)

To see if proper standardization can improve thingsÔºå but it didn't turn out to be true„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_82.png)

Just to show you theÔºå I would sayÔºå proper way„ÄÇ So it's scrolling up again„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_84.png)

Here I'm just using 0„ÄÇ5„ÄÇ5„ÄÇ5Ôºå which will scale the pixels such they are between minus1 and1 center at 0„ÄÇ

So using 0„ÄÇ5„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_86.png)

But„ÄÇWe can actually use the proper standard deviation and feature pixel mean by computing them„ÄÇ

 So here I just have a function„ÄÇAdded to it that computes or approximates„ÄÇ

 actually because it's faster„ÄÇ that approximates the mean and standard deviation for each channel for each color channels„ÄÇ

 That's a redÔºå green and blue1„ÄÇ So they are around „ÄÇ5„ÄÇ

And then here the standard deviations there are 0„ÄÇ25 instead of 0„ÄÇ

5 in what I assumed in the previous„ÄÇCoodeÔºå but if I use these„ÄÇProper ones„ÄÇInstead of just using 0„ÄÇ5„ÄÇ

 I find that it doesn't really make any difference in all„ÄÇ So performanceÔºå I think„ÄÇ

 was pretty much the same„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_88.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_89.png)

ActuallyÔºå worse82„ÄÇ OhÔºå actuallyÔºå it's an interesting drop here„ÄÇ It's probably due to overfitting„ÄÇ

 more overfitting„ÄÇ

![](img/9c0c719111bb824ca05ef070abb9e2b7_91.png)

YeahÔºå soÔºå but you can see in this caseÔºå I didn't gain anything from doing this other standardization„ÄÇ

 maybe it's actually training betterÔºå though you can see it's dropping more here„ÄÇ

 could be because the schedule is triggered at that point„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_93.png)

It doesnn't have to be because of choosing the mean and side deviation could be just coincidence„ÄÇ

All right„ÄÇ So there was a long video probably„ÄÇ So let me wrap this up in the next video„ÄÇ

 we will talk about residual networksÔºå which are a little bit more interesting than just adding more layers to it„ÄÇ



![](img/9c0c719111bb824ca05ef070abb9e2b7_95.png)