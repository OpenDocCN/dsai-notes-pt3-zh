# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P142ï¼šL17.4- å˜åˆ†è‡ªç¼–ç å™¨æŸå¤±å‡½æ•° - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let me now give you the brief picture overview of the loss function of the variational auto encoderã€‚



![](img/c021d3119325b31b7ff4031823e44158_1.png)

So overall it's about minimizing elbow the evidence lower bound so if you have not heard about elbow or the evidence lower bond before don't worry about it too much for this class So for here it's not really that important that would be something for a follow up class or yeah maybe you would encounter this in a class on the relational Bayesian methods or something like that soã€‚

For hereï¼Œ maybe it's not that importantã€‚ but overallã€‚

 we are optimizing or minimizing a loss that consists ofã€‚Yeahã€‚

 minimizing the expected negative likelihood of the data and then the Colbe lab divergence termã€‚

 which is the difference essentially between this latent space distribution and a standard multivariate Gaussian distributionã€‚

So technicallyï¼Œ you would be maximizing the lock likelihood of the dataã€‚

 but as we discussed before when we talked about logistic regression a long time agoã€‚

 maximizing the lock likelihood is the same as minimizing the negative lock likelihoodã€‚

 So that's why I've written it like this with this negative sign hereã€‚Yeahï¼Œ but againã€‚

 the details are not that importantã€‚ Likeï¼Œ if you have not encountered this beforeã€‚

 don't worry about it too muchã€‚ The big picture is that this isã€‚A reconstructionconstructã€‚Last termã€‚

 it's essentially the reconstruction between all the difference between input and output distribution hereã€‚

And the K divergence is making sure that this one here has is essentially standard multivaried Gaussian distributionã€‚

Nowã€‚If you would look at some implementations of variational out encodersï¼Œ what you might find isã€‚

That usually for the reconstructionist peopleã€‚Either use the binary cross entropyã€‚All the meanã€‚

Squared errorã€‚Both work and practiceã€‚Personallyã€‚I would say it makes more sense to use the mean squarearrowã€‚

Becauseã€‚So people use the B CE when theyã€‚Normalize the images to 0ï¼Œ1 rangeã€‚ And they also have the 0ã€‚

1 range hereã€‚The pixelsã€‚Originally image pixels are between 0 and 255 and you can normalize them in 01 range and in fact this is done automatically in Pytorch when we call the2 tenor method in the data transform so we will see that in the code exampleã€‚

å—¯ã€‚And then you can have hereã€‚Sigmoid activationï¼Œ logistic sigmoidã€‚

 and then you will also automatically ensure that the outputs are in a 01 rangeã€‚Andã€‚

It might be tempting then to use a binary cross entropyã€‚But keep in mindã€‚

 this is not a benoulli distributionã€‚ I meanï¼Œ this is not0 or1ï¼Œ the valueã€‚Of the pixelã€‚

 it's between 0 and 1ã€‚And franklyï¼Œ I don't know why some people would use BE for thatã€‚

 I would use the BE if that would be a moivate benoli distribution with I have values like zeros and onesã€‚

For the inputï¼Œ But we have values between 0 and 1ã€‚ So to meï¼Œ it makes more sense to use the M Cã€‚

And also in practiceï¼Œ when you do some comparisonsã€‚

 you will find probably So it's at least what I found that if you use the B CEã€‚

 the images would look a little bit blurrierï¼Œ could be becauseã€‚

It's kind of forcing the pixels to be more in the centerã€‚



![](img/c021d3119325b31b7ff4031823e44158_3.png)

Of the distributionã€‚In any caseï¼Œ so yeahï¼Œ whatã€‚

![](img/c021d3119325b31b7ff4031823e44158_5.png)

I also want to show youï¼Œ let me go to the next partã€‚

 maybe I wanted to show you that the cross entropy is not symmetricã€‚

So this is also one issue where I think it's probably not a good idea to use the binary cross entropy as the reconstructionnessã€‚

Becauseã€‚The loss is different depending on the pixel valuesã€‚What I mean by thatã€‚

 if what I'm showing you on the left hand side is a plotã€‚

 where I plotted the binary cross entropy lossã€‚For values of the true and the predicted labelã€‚æ˜¯ã€‚Xingã€‚

ğŸ˜”ï¼Œ0ero is probably a bad exampleï¼Œ but let's say 0ã€‚2ã€‚Hereã€‚

 what I mean is when we consider cross entropyï¼Œ that the true value is 0ã€‚2ã€‚

And the predicted value is pointuneï¼Œ So that's how we computeã€‚It should be a minusã€‚Across entropyã€‚

 Oh yeahï¼Œ for referenceï¼Œ I have it also hereã€‚ So recall the binary cross entropy from the logistic regression classã€‚

 We have a probabilityã€‚Hereã€‚And look the probability hereã€‚Usually in the logistic regression contextã€‚

This was either either a one or a0ã€‚Now we have values between 0 and1ã€‚Soï¼Œ if I haveã€‚

Pixel in my input image of 0ã€‚2 and I predict a pixel of 0ã€‚2ã€‚ I would get a lossã€‚It's around 0ã€‚5 hereã€‚

Nowï¼Œ here what I have is I have an input pixel of 0ã€‚4ã€‚ğŸ˜”ï¼ŒAnd a predicted pixel value of 24ï¼Œ as wellã€‚

I can see thisã€‚The loss is higherã€‚ Its aroundï¼Œ I don't knowï¼Œ6ã€‚5ï¼Œ something like thatã€‚ So you can seeã€‚

Even though I reconstruct the right pixel ã€‚4 is the reconstructionï¼Œ and  point4 is theã€‚

Original labelã€‚I get a higher loss now compared as if it was 02ï¼Œ so it's not symmetricã€‚

 I get different losses depending on what my pixel values areã€‚We justã€‚Which doesn'tã€‚

 which doesn't really make senseã€‚ So if you have a means square error lossã€‚Hereã€‚

This is said say my0 hereã€‚We would have0 everywhereã€‚Cause the MEï¼Œ there is no difference between theã€‚

Poã€‚4 and the ã€‚4ã€‚ So the M E would be 04 minus-ã€‚4ã€‚Squadï¼Œ which would beã€‚0ã€‚

 that makes more sense to meï¼Œ at leastã€‚ Well here's another exampleã€‚

 Consider we have the true pixel value ã€‚8ã€‚And the reconstructed pixel value is 0ã€‚7ã€‚

 So we are off by 0ã€‚1ã€‚We would get a loss of 0ã€‚28ã€‚If I have now 0ã€‚9 years for reconstructionï¼Œ stillã€‚

 I'm off by only 0ã€‚1ã€‚Now I get a ã€‚08ã€‚Rightï¼Œ so for some reasonã€‚It penalizes this ã€‚7 more than this 0ã€‚

9ã€‚Cause it's not symmetricï¼Œ same as it'll have  0ã€‚2 here as a true pixelã€‚ and I put it 0ã€‚1 hereï¼Œ 0ã€‚3ã€‚

 I will find these different valuesã€‚Yeahï¼Œ this is why I would suggest for the variation out encoder toã€‚

 if you have input pixels between 0 and 1 orã€‚Me between  minus1 and 1 doesn't matterã€‚

If you have input pixels on a continuous rangeï¼Œ I would suggest using the ME instead of the binary cross entropy for the reconstruction termã€‚



![](img/c021d3119325b31b7ff4031823e44158_7.png)

Allrightï¼Œ soã€‚This wasã€‚

![](img/c021d3119325b31b7ff4031823e44158_9.png)

Now the first termsï¼Œ if I go back one more slideã€‚So this was the first term that we discussed the reconstruction usã€‚

 So I suggest using the mean squarearrow for thatã€‚

![](img/c021d3119325b31b7ff4031823e44158_11.png)

So the mean here is over the examples in the batchã€‚For if you only have one training exampleã€‚

 you can think of it as the sum square error between all the pixelsã€‚

 So it's essentially a Euclidean distanceã€‚ so you wouldã€‚For each pair of pixelsã€‚

 compare the differenceã€‚Square itã€‚ and then you sum them all upï¼Œ and you take the square rootã€‚

 So you have the original dimension backï¼Œ and then you can average that over theã€‚Btchã€‚

 and that would be then your mean square errorã€‚And I will show youï¼Œ of courseã€‚

 in the code example how that looks likeï¼Œ I think that will also make things more clearã€‚

The second term is the KL divergence termã€‚So this is measuring the difference between the distribution here and the latent spaceã€‚

The sportã€‚So we have our mean vector and our block var vectorã€‚So it shouldï¼Œ I meanã€‚

 I'm only showing it as one vectorï¼Œ but there should be technically two vectorsã€‚

 the mean vector and the logbar vector here I'm just writing a sustainability deviationã€‚å—¯ã€‚Soï¼Œ we areã€‚

Minimizing the difference betweenã€‚This distribution of the latent space and a standard normal distributionã€‚

So we use the K divergence term for thatã€‚Andã€‚It can be written as follows as shown hereã€‚

 I'm not showing you here the derivationã€‚ I'm just showing you how you can write thatã€‚



![](img/c021d3119325b31b7ff4031823e44158_13.png)

I have two slides laterï¼Œ I have the derivation hereã€‚

 but I don't want to go into through this in detail if you're interestedã€‚

 I actually got this from this link here I was not writing this myselfã€‚

 but if you're interested in studying this you can find it here so you derive the KL lossã€‚

For the very short encoderã€‚

![](img/c021d3119325b31b7ff4031823e44158_15.png)

Going back to this one hereï¼Œ what's more interesting is to take a quick look at this so you can seeã€‚

We have thisã€‚One plus hereã€‚Plus this lock barï¼Œ This is from the Lobar trickã€‚And yeahã€‚

 you can see this would be minimized ifã€‚We have one minusã€‚The variance oneã€‚

 If we have a variance of oneã€‚ So we wouldã€‚Minimize this if the variance is actually oneã€‚So lock ofã€‚

1 should be also0 thenï¼Œ rightã€‚And if this is 0ã€‚ So if we have oneã€‚Plus 0ï¼Œ-0ï¼Œ minusã€‚-1ã€‚Give us 0ã€‚

 This is when our K divergence term is minimizedã€‚So what we want hereã€‚Iã€‚

This mean vectorton to be close to 0 and the variance to be close to 0ã€‚closeose to oneã€‚

 which happens if this lock wire term is close to 0ã€‚

So that's essentially what the KL divergence term forces theseï¼Œ thisã€‚And this vector to beã€‚And thenã€‚

 if this isã€‚Trueï¼Œ I meanï¼Œ if our mean vector is 0 and our variance vector is 1ã€‚ So this is oneã€‚

And this is 0ã€‚Thenã€‚What we haveã€‚I that it's our distributionã€‚Will beã€‚ğŸ˜”ã€‚

It's the abnormal distributionã€‚That is our goal for this latent representationã€‚ Alrightã€‚

 so this is essentially in a nutshellï¼Œ the variation or and loss functionã€‚



![](img/c021d3119325b31b7ff4031823e44158_17.png)

Againï¼Œ if you're interestedï¼Œ you can take a look at the loss derivationã€‚And now in the next videoã€‚

 I will finally show you how this all comes together in a code example where we have the sampling aspectã€‚

 the logva trick and the loss functionï¼Œ everything together in one code exampleã€‚

Where I will show you that we can train this auto encoder to yeah generate handwritten digitsã€‚

 but since handwritten digits are by nowï¼Œ I think a little bit boringã€‚

 we will also use a second data set and apply this concept of face imagesã€‚



![](img/c021d3119325b31b7ff4031823e44158_19.png)