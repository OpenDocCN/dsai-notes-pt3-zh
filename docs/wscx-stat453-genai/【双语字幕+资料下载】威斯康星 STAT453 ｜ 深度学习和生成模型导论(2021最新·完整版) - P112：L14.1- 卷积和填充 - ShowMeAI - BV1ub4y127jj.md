# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P112ÔºöL14.1- Âç∑ÁßØÂíåÂ°´ÂÖÖ - ShowMeAI - BV1ub4y127jj

OkayÔºå let's now talk about patting„ÄÇ So padding is a mechanism that allows us to control the output size„ÄÇ

 in addition to choosing the strideite„ÄÇ So before we talked about only the simple case of a stride that equals one„ÄÇ

 and that means if we have an input image„ÄÇ and then we have our kernel„ÄÇ

 then a stride of one means we move„ÄÇOne position to the right„ÄÇ And then we do the convolution„ÄÇ

 And also then later one position to the bottom„ÄÇ So we move by one pixel each time„ÄÇ

 So the whole kernelÔºå we can choose a stride of two„ÄÇ So in that wayÔºå we move by two pixels„ÄÇ

 each time„ÄÇ And this would essentially approximately„ÄÇShrink the size of the output by half„ÄÇ

 So the output with a straight of two will be approximately only half as large as the input„ÄÇ

So the padding is kind of like an opposite to that„ÄÇ it allows us to make the output larger„ÄÇSo„ÄÇ

 let's assume weÔºå we have„ÄÇThis image hereÔºå and„ÄÇWe have this outputÔºå So input„ÄÇAnd output„ÄÇ

So then againÔºå we have our„ÄÇColonnel„ÄÇSlilightdes over„ÄÇThe image input„ÄÇ

 the kernel is not really important here„ÄÇ per seÔºå I meanÔºå it kind of is„ÄÇ

 But so what we are doing now is we are adding a row if we have a padding of one„ÄÇ

This would be patting with oneÔºå one on each side„ÄÇ You can also actually control this more finely„ÄÇ

 you can say„ÄÇ1 for the top„ÄÇ1 for the right sideÔºå one for the bottom and one for the left„ÄÇ

 So we have a patting of one on each side„ÄÇ that means we add one row of pixels on each side„ÄÇSo„ÄÇ

 it's essentially„ÄÇAnying another„ÄÇRule of pixels„ÄÇAnd essentiallyÔºå usuallyÔºå we set these to0„ÄÇ

 So they are usually just zeros„ÄÇ So we are adding in this„ÄÇKind of empty„ÄÇ

artificialific border around hereÔºå which helps us to control the output size„ÄÇ because now„ÄÇ

 instead of having the kernel like here in the center„ÄÇ

 the kernel would start actually here if we have a two back2 kernelÔºå for example„ÄÇ and then we„ÄÇ

Depending on the right would either slide by one or two positions to the right„ÄÇ So by that„ÄÇ

 we can kind of make the output slightly larger„ÄÇ Of courseÔºå this is like a patting of one„ÄÇ

 We can also generalize that have other values like patting of two rows around here„ÄÇ

 And if you have a non quadratic imageÔºå you can also„ÄÇCompensate for thatÔºå by also making„ÄÇ

The padding only on the leftÔºå on the right hand side„ÄÇ

So this is like the basic concept behind padding„ÄÇ it's essentially just adding a border around the image„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_1.png)

I believe I already showed you this equation last week„ÄÇ

 This is the equation for computing the output size of a convolution layer„ÄÇ So last timeÔºå though„ÄÇ

 we regarded padding as p equals 0„ÄÇ So we didn't have any padding„ÄÇ So this term canceled last time„ÄÇ

 But yet now we can consider„ÄÇThis equation with a padding that is non0„ÄÇ

 So this whole equation allows us essentially to compute the output size for a given input size„ÄÇ

 or in particular if„ÄÇOhÔºå the output is the output„ÄÇwidthth„ÄÇ

 so we can do the same thing with width and height„ÄÇ and the same would also apply for the height„ÄÇ

We computed as the input width„ÄÇTime a two times the patting amount minus the kernel size„ÄÇ

Divided by the strite„ÄÇAnd then we are rounding this down„ÄÇ So this is the floor operation„ÄÇ

 And then we add a one to it„ÄÇ So this is how we compute the output size„ÄÇ

 So let's take a look at some of the examples of that„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_3.png)

YeahÔºå so there's a relatively nice article called a guide to convolution arithmetic for deep learning„ÄÇ

 So I have some visualizations from that article here„ÄÇ

 and we will also revisit this article another time later on when we talk about deconvolution or the so-called transposed convolution in the context of convolutional autoentas„ÄÇ

 But for now let's focus on the regular convolutions„ÄÇ So in the left hand„ÄÇüòäÔºåUpper corner„ÄÇ

 I'm showing you an input of4 by4„ÄÇInput„ÄÇWith a stride of one and no padding resulting in a two by two output when I plug in these values in the previous equation„ÄÇ

 So that's actually an animation so you can actually see how it looks like when we do the convolution so you can see how„ÄÇ

 yeah how that works„ÄÇSo we just move by one pixel position„ÄÇ

 And there you can see we actually lose one pixel on each side„ÄÇ That is becauseÔºå yeahÔºå we„ÄÇ

 we don't go over the edges„ÄÇ So in that way„ÄÇThe output will be by two rows and two columns shorter than the input„ÄÇ

So in the the next oneÔºå which one is itÔºüOr the upper right one„ÄÇ So here I have a paning of two„ÄÇ

And also here„ÄÇ So each side has a padding of two„ÄÇ And you can see now the kernel slides over the padding as well„ÄÇ

 So the paddings these are usually just zeroÔºå so they don't contribute anything to the computation rightÔºü

 So when you compute the convolutionÔºå it's in a way await weighted sum and these are zeros„ÄÇ

 So nothing really happens„ÄÇ you don't you don't change any value so„ÄÇWhat I mean is„ÄÇWhen we have„ÄÇ

This case in the upper left corner„ÄÇOnly these values contributeÔºå because these are essentially„ÄÇÂóØ„ÄÇ

X times w as wellÔºå but the x is equal to0Ôºå so which kind of doesn't do anything„ÄÇ

 So here in this caseÔºå it will only focus on this corner here„ÄÇYeah„ÄÇ

 and this is how you can then control the output size a little bit so that you don't lose„ÄÇ

Pixels on each side„ÄÇ So this is like an extreme case here„ÄÇ The output is even bigger than the input„ÄÇ

YeahÔºå that is also kind of an artifact of the fact that kernel sizes4 by4„ÄÇ

 I why did I pick this particular exampleÔºå That's just because there wasn' an animation for that one„ÄÇ

 Usually in practiceÔºå it's more common to use3 by three or5 by5 kernels„ÄÇLast one left lower corner„ÄÇ

 and this is Str of two„ÄÇ So if you had or were unsure how the Str of two works now as an example„ÄÇ

So in this caseÔºå also we get an output of two hereÔºå the input is5 now5 by5„ÄÇResulting in output of2„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_5.png)

All right so„ÄÇThere are two„ÄÇMain terms I briefly wanted to mention in Pythtorarch„ÄÇ

 they are not commonly used„ÄÇ It's more like I would say„ÄÇ

 a more traditional wording that was kind of used in„ÄÇTsorflow„ÄÇ

 at least when I used Tensorflow back then„ÄÇ So there's something called a valid convolution and the same convolution„ÄÇ

 These terms come from more traditional computer vision„ÄÇ

 and a valid convolution means essentially no padding„ÄÇ And this can have as a consequence„ÄÇ

 the shrinking feature map„ÄÇ So if I go back one slide„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_7.png)

So„ÄÇThis one here and this one hereÔºå These would be valet convolutions„ÄÇ

 So that means we are not going over the edge„ÄÇOff the input„ÄÇWhy validÔºå I don't know why valid„ÄÇ

 but probably it's really referring to the fact that we stay within our boundaries here„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_9.png)

The other oneÔºüIs called same convolution„ÄÇSame convolution means that we usually choose a padding such that the input and the output size are the same„ÄÇ

 So the output size equals the input size On the previous slide„ÄÇ

 I don't have any particular example of a valid sorry of a same convolution I should have maybe added one for the same convolution„ÄÇ

 we would have an example where the input size as exactly the same size as the output notice it's not always possible„ÄÇ

 So in this caseÔºå if we have to have a 4 by4 kernelÔºå it's not possible to maintain this size„ÄÇ

 if I would have one row of padding also even then I wouldn't be able to get the same input size„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_11.png)

![](img/21258d29e8d14413af95bf7210a989e9_12.png)

All right„ÄÇYeahÔºå and this is maybe also why common kernel size conventions are3 by 3„ÄÇ

5 by5 and7 by 7 so that you can actually have a same convolution„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_14.png)

So how do we choose pitting so that we get same convolutionÔºü

So you can do this by rearranging things if youÔºå for simplicityÔºå now ignore the flow operation„ÄÇ

With a stride„ÄÇ So if you only use a stride of one can cancel that and it simplifies to this form„ÄÇ

 So we just remove the floor and stride„ÄÇAnd then yeah„ÄÇ

 you can just rearrange that and bring P to the left hand sideÔºå the O to the right hand side„ÄÇ

 and then divide by 2„ÄÇSo we can rearrange things and then we can simplify things„ÄÇ

 we want the input to be the same size as the output right„ÄÇ

 so because we want the input to be the same size of the outputÔºå we can cancel those„ÄÇ

So what remains is this one and this one allows us then to choose the straight sorry„ÄÇ

 the kernel size such that we have the same convolutionÔºå for instanceÔºå if I have„ÄÇOf 4 by four kernel„ÄÇ

 then I would have one4 minus„ÄÇ1 divided by2Ôºå which is an uneven number of 1„ÄÇ5„ÄÇ So I can seeÔºå okay„ÄÇ

 there is no way I can have„ÄÇPtting such that I have the same convolution„ÄÇ The only solution„ÄÇ

 maybe let me clarify„ÄÇ There is actually a solution where you add one„ÄÇ

Ro to the bottom but two rows to the topÔºå for exampleÔºå there would be one one solution„ÄÇ

 But if you want to have a symmetric paddingÔºå then there is no good way for that„ÄÇHowever„ÄÇ

 if you use a kernel that is 5Ôºå you have5 minus1„ÄÇDivided by 2„ÄÇWhich is42Ôºå rightÔºü So in this way„ÄÇ

 you can choose a padding of two if my kernel„ÄÇ

![](img/21258d29e8d14413af95bf7210a989e9_16.png)

Size here was„ÄÇ5Ôºå then we would have ended up with a5 output„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_18.png)

OkayÔºå so this is just patting in a nutshell in the next slide or next video„ÄÇ

 I will talk about spatial dropout and spatial patch on„ÄÇ



![](img/21258d29e8d14413af95bf7210a989e9_20.png)

![](img/21258d29e8d14413af95bf7210a989e9_21.png)