# P38ÔºöL5.6- ÁêÜËß£Ê¢ØÂ∫¶‰∏ãÈôç - ShowMeAI - BV1ub4y127jj

YeahÔºå now equipped with a knowledge of what a function derivative is„ÄÇ

 Let us use the idea from care colours to train a linear regression model using a method that is called or related to„ÄÇ

YeahÔºå now equipped with the knowledge of what a function derivative or gradient is„ÄÇ

 let's now take a look at training a linear regression model using these concepts„ÄÇYeah„ÄÇ

 now equipped with the knowledge of what a function derivative and gradient is„ÄÇ

 Let's not train a linear regression model„ÄÇ So this technique in the context of linear regression is„ÄÇ

YeahÔºå now equipped her with a knowledge of what a function derivative and gradient is„ÄÇ

 Let's now train a linear regression model„ÄÇ So for this„ÄÇ

 we will be using a technique called gradient descent„ÄÇ

 which is yet related to computing function derivatives and gradients„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_1.png)

So consider the linear regression model here again where the activation function is just the identity function„ÄÇ

So this is just like a regular linear regression model and we have one output here„ÄÇ

 So this is our prediction and for yeah fitting or finding these weights we want to minimize the loss or the prediction error of linear regression for this„ÄÇ

 we formulate a convex loss function in the case of least squares a linear regression„ÄÇThat's the„ÄÇ

Some squared errorÔºå we can also consider the mean squared error doesn't matter„ÄÇ

So either one would work„ÄÇ So for this squared error loss„ÄÇ

 what we have is we have the prediction minus the actual value that we want to predict„ÄÇ

 So we consider the difference between the two squared„ÄÇ So why the squared„ÄÇ

 I mean this way outliers will be more heavily penalized„ÄÇAnd also„ÄÇ

It doesn't matter whether one or the others on the left or on the right handend side in any case„ÄÇ

 so if we would plot this nowÔºå so by the wayÔºå I should say this is over the training examples„ÄÇ

 So if we have a data set with n training examplesÔºå this would go from„ÄÇFirst„ÄÇ

 until the last training example„ÄÇ So the loss on the training dataset set„ÄÇ

So now yeah I plotted the loss here on the at the bottom as a function of one of the weights„ÄÇ

 so you notice there are usually oh sorry there are usually multiple weights here we are looking only at one of the weights because it had simpler to look at„ÄÇ

 but you can also yeah generalize this concept to multiple dimensions„ÄÇ

So what you can see is this bowl shape or parabola shape„ÄÇ

And the loss is minimum for a certain value of the weights„ÄÇ

 So if the value of the weight is too largeÔºå you can see the loss goes up or„ÄÇ

Maybe it's easier to see it„ÄÇ like if we go to the right of the weight is larger„ÄÇ

 you can see it has a higher loss in the same way for this„ÄÇW here„ÄÇ That is too small„ÄÇ

 So there is so called„ÄÇGbal„ÄÇ‰Ω†‰ª¨ÁöÑËêå„ÄÇThat we want to find„ÄÇ

 So we want to find this global minimum for the weights so that our prediction error or loss will be minimized„ÄÇ

 We will do that forÔºå of courseÔºå all the weights„ÄÇ So if you think of this in three dimensions„ÄÇ

 So if we have this w1 and a second weightÔºå this would be a like a bowl like a serial„ÄÇÂóØ„ÄÇ

Like a serial bowl or something like that where you want to reach the bottom of the serial bowl bowl„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_3.png)

But yeahÔºå it's easier to think of it as maybe one dimension„ÄÇYeah„ÄÇ

 let's now see what happens if we use the concept of gradient descent„ÄÇ

 So what we do is we have to start with a certain weightÔºå let's say„ÄÇThis waitÔºå this is a one„ÄÇ

Starting point„ÄÇ So in practice for a linear regression„ÄÇ

 we usually initialize the weights to all zeros or small random numbers„ÄÇ

 So if I had initialized the weight with value 0„ÄÇWe would be over here„ÄÇ and you can see alsoÔºå yeah„ÄÇ

 the loss would be pretty large here at zero„ÄÇ We want to find basically this global„ÄÇGlobal minimum„ÄÇ

That's our goal„ÄÇ So if you think backÔºå if you have seen the optional video on„ÄÇThe derivatives„ÄÇ

 So this is like almost like theÔºå yeah squared function that I talked about„ÄÇ So x squaredÔºå rightÔºü So„ÄÇ

 and we can compute„ÄÇDerivative of this function„ÄÇ So the derivative of this function with respect to W1„ÄÇ

 For exampleÔºå if we consider our starting weight„ÄÇ So this is the way we currently consider or we start with„ÄÇ

 Let's say this's a small random number„ÄÇ We can compute the function derivative at this point„ÄÇ

 So here„ÄÇ So and then„ÄÇUsing thisÔºå we can„ÄÇSo I should say actually the derivative should point upwards here in this direction„ÄÇ

 but this is this arrow here is the negative„ÄÇThe negative slope„ÄÇ So this arrow here is the negative„ÄÇ

Slope„ÄÇAnd we can use this negative slope in multiple dimension„ÄÇ

 you can think of it as a gradient instead of a derivative„ÄÇ

 we can use this function gradient to guide the weights into the direction of the global minimum„ÄÇ

The slope„ÄÇWould tell us how steep the function is„ÄÇ So at a given pointÔºå how steep the function is„ÄÇ

But yeahÔºå we are interested in the negative slopeÔºå which is„ÄÇ

 yeah also giving us the steepness of the function„ÄÇ But instead of pointing upwards„ÄÇ

 its's pointing downwardsÔºå it's pointing towards the minimum that we want to reach„ÄÇ

So we can use then this value this negative slope or negative gradient to guide the function downwards„ÄÇ

Towards this global minimum„ÄÇ So we make small steps towards the global minimum„ÄÇOne step at a time„ÄÇ

 And for thatÔºå we haveÔºå as I've showed youÔºå I think in the second video of this lecture„ÄÇ

 we had a so called learning rate„ÄÇEtter and so the loading rate E together with how steep it is guides how much we update„ÄÇ

So you can see here at the bottomÔºå it becomes a bit flatter„ÄÇ rightÔºü

 So the function is not as steep anymore„ÄÇ So in these cases„ÄÇ

 we also make smaller steps because the update really depends on the steepness and also on the learning rate„ÄÇ

 So the learning rate would all scale these„ÄÇYeahÔºå slopes by a constant„ÄÇ

 So the learning rate is usually a constant that scales all of these slopes„ÄÇ

 That is like an optional step you don't have to use„ÄÇLning rate„ÄÇ

 but in practice indeed deep learningÔºå it's yeah kind of required to have a learning rate„ÄÇ

 And also there will be extensive tuning into what is a good learning rate„ÄÇ So this is something„ÄÇ

Where you would have to try different learning rates and see what performs better„ÄÇ

 but this is something that will be done later in this class„ÄÇ

 and I will probably also make a homework about that where you have to play around with us a little bit„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_5.png)

So if you have a learning rate that is too largeÔºå what might happen is that you might overshoot so if you choose a learning rate that is very large„ÄÇ

 you may make an updateÔºå so let's say you make an update here„ÄÇ

 this is maybe a little bit exaggerated so you move from this weight position to this one„ÄÇ

And then the next update is a large update here„ÄÇSo in this caseÔºå the update has been too large„ÄÇ

 So we are overshooting the global minimum and„ÄÇDepending on yeah what what you find here„ÄÇ

 you may overshoot even more„ÄÇ And then you kind of lose the weight„ÄÇ So your loss will increase„ÄÇ

 You can see that happening in practice if you use a learning rate of this too large„ÄÇ

 So notice here alsoÔºå I'm only again„ÄÇAlways looking at one weight„ÄÇ

 but there are yeah weights in multiple dimensions„ÄÇ

 so in that way there is a lot of possibility something like that can happen if we use a large learning rate so it's actually better to make like smaller baby steps to watch at towards reaching this global minimum however„ÄÇ

 if we make the learning rate too small it will just take a very long time until we reach this bottom so we make all these little baby steps it will take a lot of iterations or a lot of updates to reach this global minimum in practice for a convex loss function like this one where we have a global minimum„ÄÇ

That is easy to reach because everything is convex„ÄÇ Then it's not a problem„ÄÇ

 usually to have a small learning rate„ÄÇ We have to just be patient in practice for deep learning„ÄÇ

 This would be„ÄÇA problem because the algorithm can get stuck„ÄÇ So in deep learning„ÄÇ

 we usually have loss functions that are not convex„ÄÇ so we can have a loss function„ÄÇ

 Let me draw you an example„ÄÇYou can have a loss function„ÄÇ that is somewhat„ÄÇLet's sayÔºå like„ÄÇ

 like this„ÄÇSo if we have then a wait where we start hereÔºå let's say we go down here„ÄÇ

 then it goes down hereÔºå okayÔºå this would actually be a good caseÔºå sorry„ÄÇThat'sÔºå let's start here„ÄÇ

 So if we start hereÔºå we make small stepsÔºå then we may up may end up in thisÔºå in this pit here„ÄÇ

 This is a local„ÄÇMinimumÔºå so actuallyÔºå I made it relatively steep„ÄÇ

 So you would probably end up in this one eventually anyways„ÄÇ but let me make a„ÄÇMaybe a flatter„ÄÇone„ÄÇ

Like something like this„ÄÇSo what could happen is if you have a very small learning rate„ÄÇ

 you go from here into this pit and then you get stuck here„ÄÇ

Because the learning rate is not large enough at this point here to jump over thisÔºå for example„ÄÇ

 so in this case having a too small learning rate can also be a little bit bad„ÄÇ

 but yet in practice we will later talk about algorithms that have a so-called momentum term and yet to overcome these little challenges also„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_7.png)

OkayÔºå so but moving on„ÄÇYeahÔºå here have the overview of the online mode again„ÄÇ

 just for reference to just show you what we want to get at now„ÄÇ

 So what we want to do is we want to find this global minimum„ÄÇ

 So just pay attention to the right hand side here„ÄÇ This is the stochastic gradient in descent„ÄÇ

 There's also a batch gradient in descent„ÄÇ I will say a few words about that„ÄÇ

 But here for simplicityÔºå consider the case where we update one training point at a time„ÄÇ

So in this caseÔºå what we do is we compute the gradient of the loss function here so you can see we compute the gradients here„ÄÇ

And we have this for W and the biasÔºå and in that caseÔºå it would be just the partial derivative„ÄÇ

Not a gradient„ÄÇBut let's use the same symbolÔºå the nur for everything„ÄÇ It'sÔºå I think„ÄÇ

 simpler compared to switching between different symbols„ÄÇ

 So then our update is based on the current weight and then the learning rate„ÄÇ

 So this is the learning rate again„ÄÇTimes the negative gradient„ÄÇ you can see there's a negative now„ÄÇ

 the negative sign„ÄÇ So this is because we want to go towards the global minimum and not upwards„ÄÇ

 rightÔºå so we don't want to make the loss larger„ÄÇ We want to make the loss smaller„ÄÇ

 So we take a step into the direction of the negative gradient„ÄÇ

 So here we use actually these gradients to update„ÄÇSo what are these valuesÔºå So wheres„ÄÇ

This value and this value come fromÔºå Where these values come from„ÄÇ So for thatÔºå we can„ÄÇ

 yeah derive our loss functionÔºå rightÔºå so we can derive our loss function with respect to W and with respect to B„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_9.png)

YeahÔºå let's not derive the loss function with respect to W„ÄÇ

 So here we are only taking a look at one of the weights W JÔºå just to keep it simpler„ÄÇ

 We could also do this for the weight vector„ÄÇ but yeahÔºå recall the„ÄÇGraient of the loss„ÄÇ

Would be just a collection of these individual partial derivativesÔºå right„ÄÇ

 So we would have a partial derivative of the loss with respect to W1„ÄÇW2„ÄÇUp to„ÄÇüòîÔºåWM„ÄÇ

 so there would be a vector„ÄÇ So here we are only taking a look at one of these„ÄÇ for example„ÄÇ

 the first one„ÄÇ let's call it W J some of the arbitrary entries here in this„ÄÇGraadient victim„ÄÇ

So we are considering here the sum squaredarrow us so in the previous slide I showed you for simplicity the stochastic version where we only update or compute the gradient based on one training example at a time„ÄÇ

 which would be an approximation of this overall loss function right so if I only consider one data point without the sum So if I ignore the sum this one would be only an approximation of the overall loss function on the training set so here using the whole training set it's actually more accurate„ÄÇ

This is also sometimes called just gradient incent or batch„ÄÇGraian decent if we use„ÄÇThe whole„ÄÇ

Training setÔºå where we have„ÄÇI equals1 up to n„ÄÇAnd then we can also consider though a smaller case where we only have a mini batch„ÄÇ

 so„ÄÇIf we have a mini batchÔºå if this isÔºå let's sayÔºå a mini batch of size K and K is smaller„ÄÇ

Then and this would be the mini batch version„ÄÇ This goes back to what I explained at the very beginning of this lecture where we have this online mode„ÄÇ

 mini batch mode and batch mode„ÄÇ how we compute the green„ÄÇ It's the same concept„ÄÇ

 except that here we have this sum„ÄÇEverywhereÔºå but you can also ignore the sum„ÄÇ

 and then it would be for the online mode„ÄÇAll rightÔºå so what we do now„ÄÇIs let'sÔºå yeah„ÄÇ

 we are drivingr no the loss with respect to the weight„ÄÇ Let's take a look here„ÄÇ

 This is just the sum square error loss that I've written down again„ÄÇ

 Now we are yeah differentiating this loss with respect to W J„ÄÇ

 So first step is I'm expanding this term hereÔºå the prediction„ÄÇ

 So if you recall how we compute the prediction it's„ÄÇThe weights„ÄÇ

Times the feature values plus the biasÔºå rightÔºü So here I'm ignoring the bias„ÄÇ

 Let's say it's implicitly included in the weights„ÄÇ

 So we are only looking at everything without the bias just for simplicity„ÄÇ

 I've actually included it hereÔºå but maybe ignore this for now„ÄÇÂóØ„ÄÇYeahÔºå and then„ÄÇ

We have the activation function„ÄÇAnd the activation function was just„ÄÇAn identity function„ÄÇ

 So I'm just using this one hereÔºå writing it more compactly with using vector notation W transpose X using„ÄÇ

M the notation here„ÄÇSo„ÄÇThat's what I'm what I have here„ÄÇ So I just expandedend this one„ÄÇ

And now we are using the chain rule to compute the derivative„ÄÇ

 So the chain rule is the outside derivative times the inner derivative„ÄÇ

 So here notice we so the auto function would be„ÄÇThis one basic„ÄÇ So this is the inner in red„ÄÇ

 And then theÔºå the square here would be the outta function„ÄÇ

 So what I'm doing is I'm deriving now the outer  one„ÄÇ This is this part here„ÄÇ

 This is the outta derivative„ÄÇCan you write a stone„ÄÇUsing the power rule„ÄÇ

 And then on the left hand side here„ÄÇThat's„ÄÇIn a derivative for the outer derivative„ÄÇ

 we are basically done„ÄÇ so we don't have to do anything„ÄÇOn the left hand side„ÄÇ

 So the left hand side is„ÄÇDonone now let's take the a look at the inaderative„ÄÇ

 So if we take a look at this inneraderativeÔºå we seeÔºå okayÔºå there is another„ÄÇ

Chain rule application requiredÔºå rightÔºå because againÔºå we haveÔºå yeahÔºå different things going on here„ÄÇ

 We have„ÄÇThe activationation function„ÄÇ And then there is the value inside of the activation function„ÄÇ

So let's now consider the chain rule again when we compute the alta and the innerad derivative„ÄÇ

So the way I've written it here is againÔºå on the left hand sideÔºå the outar„ÄÇDeerivative„ÄÇ

So this would be whatÔºå would be the„ÄÇAto derivative„ÄÇ

The derivative of this activation function with respect to its input„ÄÇ

 so the input is this term W transpose x„ÄÇAnd„ÄÇThen on the right hand side„ÄÇ

 I have the derivative of the inner part„ÄÇ So this would be the inner part„ÄÇ notice„ÄÇ

We cancel the minus y because this is just a constant„ÄÇ So this goes away„ÄÇAlrightÔºå so„ÄÇ

Looking at this hereÔºå now this termÔºå what if we compute the derivative of this„ÄÇ

 this whole term with respect to WÔºå the result would be xÔºå rightÔºü So because we compute it„ÄÇ

The derivative with respect to WJ„ÄÇSo the live of this term„ÄÇWith respect to„ÄÇW„ÄÇLet's say WJ„ÄÇ

Because W is a vector„ÄÇWould be„ÄÇOops„ÄÇXgÔºå because all the other values in that vector would be constant„ÄÇ

SoÔºå this is what we have„ÄÇHereÔºå so this' is what we have left over if we derive this part„ÄÇNow„ÄÇ

 we are left still with the outer derivative to do„ÄÇ So the outer derivative„ÄÇ

Have we computed the derivative of sigma with respect to its input as one„ÄÇ So why is that„ÄÇ

 Because we have the identity function hereÔºå rightÔºü So the derivative of this function„ÄÇSoÔºå let me„ÄÇ

Wite the stone„ÄÇThis would be oneÔºå right„ÄÇ So this goes away„ÄÇ So what we are left here with is just„ÄÇ

X J„ÄÇ So the derivative„ÄÇOf the loss function with respect to the weight is„ÄÇWhy hat„ÄÇ

 So if I would condense this againÔºå actually it would be two times y hat minus y„ÄÇTimes X J„ÄÇ

 And this would be the derivative of this loss function with respect to this W J„ÄÇ

 And in each iterationÔºå when we do the update or the training„ÄÇ

 we can compute this value and update the weight valuesÔºå rightÔºå so we can„ÄÇ

Use this term now to update the weight„ÄÇ So if we have the„ÄÇWÔºå we updated by„ÄÇItself„ÄÇPlus„ÄÇThen„ÄÇ

 the learning rate„ÄÇTimes the negative version of this whole part„ÄÇ

 This would be how we update the weights in each round to minimize this loss function„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_11.png)

So here I have an alternative notation of this whole thing„ÄÇ

 This is actually what people do in practice„ÄÇSo usually to make this a little bit more stable with in terms of the learning rate so that the learning rate doesn't depend on the size of the training set„ÄÇ

 we actually compute„ÄÇOr use the mean squared error„ÄÇ So here we have no a one over n„ÄÇ

 So just abridging„ÄÇ the rest is all the sameÔºå except that I know have this one over n„ÄÇ

 but also notice that it's not only one over n„ÄÇ it's over„ÄÇ2 m„ÄÇAnd again„ÄÇ

 this is really to make things more convenient because then when we use the power rule„ÄÇ So if I„ÄÇ

Use the power rule here„ÄÇ The two goes in frontÔºå rightÔºü So I have two over to n„ÄÇ

 and then the two cancel„ÄÇ And then I'm just left with„ÄÇ1 over n„ÄÇ

And this is a little bit more convenient convenient„ÄÇ

 So this one is usually the one that is used in practice because then if I use this one for updating the weights„ÄÇ

 I can choose a learning rate that is independent of the size of the training set or independent of the size of the mini batches„ÄÇ

 Otherwise if I change my data set or my mini batch size„ÄÇ

 I have to find the learning rate every time from scratch again„ÄÇActually„ÄÇ

 it's important to find the learning rate for each new dataset set anyways„ÄÇ

 but if we play around with differentÔºå different mini batch sizesÔºå it would be more robust if we„ÄÇ

YeahÔºå use the one over„ÄÇ and„ÄÇ this is usually what we do in practice„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_13.png)

YeahÔºå so here's how gradient in the sand looks like for a loss function with„ÄÇYeah„ÄÇ

 so here's again how gradient descent looks like using a surface plot„ÄÇ

 So here we have again the mean square error lossÔºå and we are looking at two weights now and the global minimum is here in the center„ÄÇ

Now suppose we start with the weight hereÔºå let's say this is a one„ÄÇStarting weight„ÄÇ

 And the lines hereÔºå these contraours indicate how large the loss is„ÄÇ SoÔºå of courseÔºå yeah„ÄÇ

 the closer you get to the local minimumÔºå the smaller the loss„ÄÇ And that's where we want to get at„ÄÇ

 We want to get to the global minimum„ÄÇNowÔºå if we use the batch grade in descent„ÄÇ

 where we derive the loss function based on„ÄÇThe whole training set„ÄÇ

Let me just quickly write a down here again„ÄÇ So if we compute„ÄÇ

The derivative of this one with respect to the whole yeah or considering the whole data set„ÄÇ

 then we will make direct leaps towards the global minimum„ÄÇ

 So each update will be perpendicular here to the contour lines„ÄÇ

If we use something like stochastic gradientding descent so that is if we only update based on one data point at a time or based on a mini batch„ÄÇ

 then the updates will be a little bit noisier„ÄÇ

![](img/8509f09386937833c2e551d9587e74ce_15.png)

So instead of going straight to the centreÔºå there will be like this little zigzag for linear regression„ÄÇ

 that is probably not ideal because it's yeah why would we want to do thatÔºå It's„ÄÇ

 there's no point doing that„ÄÇ if we can do these direct steps make these direct steps„ÄÇ

 Why would we do one step at a time„ÄÇ Maybe one advantage is if we have a veryÔºå veryÔºå very„ÄÇ

 very large data set„ÄÇ Then this will yeah train the linear regression model quicker because„ÄÇ

You will be having more updates per epoch„ÄÇ So if you have batch grain decent„ÄÇ

 you only have one update per training epoch and with mini batch grain descent„ÄÇ

 you have more updates per epoch„ÄÇHoweverÔºå each update itself is noisier„ÄÇ again„ÄÇ

 for linear regressionÔºå it probably doesn't make sense to use mini batch gradient descent„ÄÇ

But later when we talk about deep learning we have non convex loss function„ÄÇ

 so we probably won't be reaching the global minimum anyway„ÄÇ

 and in this way using these little stochastic gradient decent updates„ÄÇ

 I say are actually better than these complete gradient updates„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_17.png)

YeahÔºå also one more„ÄÇNote about data normalization„ÄÇ So I think I mentioned it before that it is important to normalize the input data so that the features are all on the same scale„ÄÇ

 So this is not a problemÔºå for exampleÔºå if we have something like iris where everything is already on centimeter scales„ÄÇ

 but it's still helpful to make these values smaller because there will be certain weight initialization schemes„ÄÇ

We will talk about this in a separate lectureÔºå how to initialize these weights as well„ÄÇ

But if we haveÔºå let's sayÔºå data sets where we have features on„ÄÇYeahÔºå different scales„ÄÇ

 let's say one is something likeÔºå I don't know„ÄÇ income and the one is age„ÄÇ So have„ÄÇ

 let's say age and income is your features and income is a dollar amount„ÄÇ

 So let's say you have something likeÔºå I knowÔºå50000 or something per year„ÄÇ

 and the age is maybe 23 or something like that„ÄÇ So these are very different scale„ÄÇ

 So one scale is from maybe 10000 to 10 millionÔºå whereas age is betweenÔºå let's say 0 and 100„ÄÇ

 So in this wayÔºå what happens well what the problem is is that these„ÄÇ

Contour lines will be more like skewedÔºå so they will be more like ellipick„ÄÇ

 and then it will be harder to update because remember we are computing the gradient of the loss so that considers all the directions„ÄÇ

And then we may make these steps that are not directly pointing towards them„ÄÇI mean„ÄÇ

 they are pointing towards the minimum in a way in one directionÔºå but in the other direction„ÄÇ

 we make maybe a„ÄÇLarger leap„ÄÇ So actuallyÔºå maybe if weÔºå weÔºå we overshoot basically„ÄÇ

 and then we also end up zigzaing„ÄÇ So in this wayÔºå it helps actually initializing the weights with small random numbers„ÄÇ

 but also normalizing the inputs„ÄÇ about the weight initializationÔºå we will have a separate lecture„ÄÇ

 talking about different strategies for initializing the weights„ÄÇ WellÔºå Of course„ÄÇ

 for a simple case like linear regressionÔºå it doesn't really matter that much„ÄÇ

 but it will matter when we talk about deep neural networks„ÄÇ

So you can really think of deep neural network training as applying the same concept that we talked about in this lecture exactly the same concept„ÄÇ

 but to a more complicated networkÔºå but we will also be computing the derivative of the loss with respect to the weights„ÄÇ

 but in the network we have multiple layers and multiple weightsÔºå but except that it's very„ÄÇ

 very similar„ÄÇ

![](img/8509f09386937833c2e551d9587e74ce_19.png)

AlrightÔºå so then yeah to end this lectureÔºå let's talk about training a single layer neural network„ÄÇ

 so an extension of linear regression for classificationÔºå so the adelineÔºå the adaptive linear neuron„ÄÇ



![](img/8509f09386937833c2e551d9587e74ce_21.png)