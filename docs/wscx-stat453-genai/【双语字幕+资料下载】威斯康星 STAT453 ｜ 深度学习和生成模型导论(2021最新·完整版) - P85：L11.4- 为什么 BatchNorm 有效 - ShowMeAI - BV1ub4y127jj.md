# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P85ÔºöL11.4- ‰∏∫‰ªÄ‰πà BatchNorm ÊúâÊïà - ShowMeAI - BV1ub4y127jj

YeahÔºå in the previous videos we discussed how batchchome works„ÄÇ

 Let's not discuss why batchcheome works„ÄÇ I can't give you the definitive answer here„ÄÇ

 but at least I pool together some theories and some practical or empirical evidence to kind of support certain claims„ÄÇ

 So I think that might be interesting if you are really interested in studying batchchon„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_1.png)

So originallyÔºå so the paper that proposed beome mentioned this internal Covariate shift„ÄÇ

 so the authors of the original beome paper said that yeah batchomization accelerates training by reducing this internal covariate shift„ÄÇ

What is this internal covariage of what does it meanÔºå So it's essentially saying„ÄÇ

That the layer in input distribution changes„ÄÇ So this shift iss a shift in the distribution of these inputs to a given layer„ÄÇ

 So it's essentially a feature shift„ÄÇ So the features„ÄÇ

 if you think of the inputs to a given layer as the featuresÔºå the distribution of these„ÄÇ

Features shift over time during the course of training„ÄÇ

So that's one theory that happens during a regular training and batchche norm helps preventing it by renormalizing these inputs„ÄÇ

HoweverÔºå theres yet no guarantee for that and there's also no strong evidence for this theory„ÄÇ

 I will show you a paper that kind of debunks this theory„ÄÇ

Another theory that's like more what I'm kind of sometimes speculating is„ÄÇThat yeah„ÄÇ

 beon provides additional parameters that might help„ÄÇ However„ÄÇ

 I think this is also maybe not a reasonable theory because it is something you can simply test by inserting or making the previous layer a little bit larger„ÄÇ

 adding some parameters to it„ÄÇ And I don't think I think at least someone would have tested that already„ÄÇ

 So that's probably not the case„ÄÇ Another idea maybe that it makes these layers a little bit more independent of each other„ÄÇ

 SoÔºå for instanceÔºå if you think about one layer producing some nonsense outputs or something„ÄÇ

 yeah it's veryÔºå so let's say you have a late layer in the network„ÄÇThey are very close to the output„ÄÇ

 It kind of depends on the previous layers of giving it good input„ÄÇ

 So if you screw up one of the early layersÔºå wellÔºå then the output layer„ÄÇ

 well all the other layers are really also depending on that„ÄÇ

 So we are kind of that's like a domino effect„ÄÇ you have like if you screw up one layer„ÄÇ

 you screw up the other layersÔºå too„ÄÇ And the same thing can happen during both the forward and the backwardboard pass„ÄÇ

 rightÔºå because„ÄÇInThe forward passÔºå you go from the left to the right and in the backward pass„ÄÇ

 you go from the right to the left„ÄÇ So in that wayÔºå if something is wrong in one given layer„ÄÇ

 it will affect all the other layers„ÄÇ And maybe with a batch norm„ÄÇ

 this kind of helps yeah decoupling the layers a little bit in terms of if one layer screws up the other layer a little bit more robust to that„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_3.png)

But that's also just one of the many theories„ÄÇSo here I found a paper from 2019„ÄÇ

 I mean it might already be outdatedÔºå I don't knowÔºå but I found it interesting„ÄÇ

 so I included it here„ÄÇThis paper is called How Does batchch normalization help optimization„ÄÇ

So here they in the abstract basically state that vgeome is good for stabilizing the training„ÄÇ

 but also the exact reasons are still poorly understood„ÄÇ

And then they mention the internal covariate shift as one of the potential reasons„ÄÇ

 but they demonstrate that such a distributional shift„ÄÇ

What distributional stability of the layers has little to do with the success of petm„ÄÇ

 So they are essentially saying the covariate shift or preventing covariate shift is not really why petome works so well„ÄÇ

YeahÔºå insteadÔºå what they find is that„ÄÇIt makes the optimization landscape significantly smoother„ÄÇ

 so essentially making or creating a smoother loss surface„ÄÇAnd this helps then with yeah„ÄÇ

 making the training more stable and it allows us to yeah have larger learning rates and have faster training because we have more stable behavior of the gradients„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_5.png)

So I took some visualizations from the paper shown here„ÄÇ

I don't want to go into too much detail about this paper„ÄÇ

 I mean if you are interested you are very welcome to read it so here I only a few takeaways„ÄÇ

 so what they say is essentially that batchome enables faster convergence by allowing larger learning rates I found that interesting so let's focus on the left„ÄÇ

Plot first„ÄÇThat's the training accuracy here„ÄÇAnd„ÄÇThis is the number of steps„ÄÇ so instead of epochs„ÄÇ

 they have steps here so that's the steps you can think of it as iterations or mini batch updates doesn't really matter whether we have epochs here or mini batches plotted„ÄÇ

 it really just represents the the trainingÔºå the course of the training„ÄÇSo we have 15000 updates„ÄÇ

 and you can see there are four graphs„ÄÇSo the red onesÔºå a standard network„ÄÇ

With learning rate 01 and 05„ÄÇ And just lookingÔºå let's use the red for this one„ÄÇ

 So just looking at this red curve hereÔºå you can see„ÄÇThe 0„ÄÇ1 trains relatively well„ÄÇ

 it reaches 10% at some pointÔºå however the large oneÔºå the 0„ÄÇ5 doesn't train so well„ÄÇSo it doesn't„ÄÇ

 So the learning rate is essentially too large„ÄÇSoÔºå what we have then is„ÄÇThe same„ÄÇ1 and „ÄÇ5„ÄÇ

 now with batch norm„ÄÇNowÔºå if you look at itÔºå the one„ÄÇWith„ÄÇPoint1„ÄÇTrains actually fasterÔºå also with 0„ÄÇ

5„ÄÇIt also trains at all„ÄÇ I meanÔºå the other one with 05Ôºå when we had the standard one„ÄÇ

 it fell apart hereÔºå rightÔºå So batch essentially allows us to also have larger learning rates„ÄÇ

 In this caseÔºå the larger learning rate is„ÄÇNot better than the 0„ÄÇ1 learning rateÔºå but it is stable„ÄÇ

 It is training„ÄÇWhat the interesting takeaway here isÔºå thoughÔºå is that„ÄÇOverall„ÄÇ

 the training with petome is faster„ÄÇEven though we reach the same performance in the end and the training set„ÄÇ

 what is nice here is that bem trains faster„ÄÇNow on the right hand side that's what we really care about that's the test accuracy How does the test accuracy look like So here I assume they are testing or they using the same test set for so they train the model and then they use the same test set to make this plot where this is really like the training steps here„ÄÇ

So„ÄÇYeahÔºå what we can see here is during training„ÄÇ So firstÔºå on also againÔºå with batch norm„ÄÇ

 we see that the accuracy„ÄÇIncreases faster„ÄÇ In the end„ÄÇ

 I would say there is a little difference between the red and the blue and there is a slight difference„ÄÇ

 So you can say maybe beon performs better„ÄÇAnd you can see alsoÔºå pet is stable worth a„ÄÇ

5 learning rate„ÄÇI wouldn't sayÔºå I mean„ÄÇI wouldn't say the training is faster„ÄÇ

 or convergence is faster because it allows us large learning rates because we can see the smaller learning rate still works better„ÄÇ

 but what I would say is it's making our training more robust so it's maybe requiring less time to find good hyperpermeter settings to get good results„ÄÇ

Which is kind of nice„ÄÇ So if it just stabilizebils strain„ÄÇ

 this is something I would always want to doÔºå rightÔºå Why would we not want to patch on„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_7.png)

Then„ÄÇAlrightÔºå so next day looked also into this covariate shift here„ÄÇ

 Let me take a look at this figure to refresh my memory here„ÄÇYesÔºå so what they are showing here is„ÄÇ

 again„ÄÇThe trainingÔºå the course of the training and the training set accuracy„ÄÇ

And on the right hand sideÔºå these are the distributions of the activations in the different layers„ÄÇ

 So they look at layer 2Ôºå layer 9 and layer 13 of a deep neural network of VG G network„ÄÇ

 We will talk about VG G in the con network lecture„ÄÇ So what we can see is for standard„ÄÇ

That the distributions„ÄÇAcross layers are slightly differentÔºå so„ÄÇ„ÅÇ„ÄÇ

Can't really read these numbers here„ÄÇ it's very small„ÄÇ

 but these should be ordered by increasing size„ÄÇ So I think this corresponds here to the to the steps„ÄÇ

So the steps that we see here„ÄÇSo this at the bottom is the later training in each„ÄÇ

You can think of each be curve or each histogram here as the activation„ÄÇ

 the distribution of the activations at that given step„ÄÇ

So what you can see is they are all kind of in line here„ÄÇ So there's not much distributional shift„ÄÇ

 I would say hereÔºå maybe there's someÔºå some shift you can see„ÄÇ

 it's maybe I'm not sure it's hard to see that the„ÄÇ

The mean of the distribution during the course of the training„ÄÇ

 during the steps shifts a little bit to the left„ÄÇ for instance„ÄÇ

 but we can also see this a little bit hereÔºå rightÔºå So in that wayÔºå this one is the one with beome„ÄÇ

 It's notÔºå I would sayÔºå that much different„ÄÇNowÔºå what they did is they produced a batch variant by adding noise to it„ÄÇ

 So here you can clearly see on that„ÄÇThe distribution shiftsÔºå so you can see„ÄÇHere„ÄÇ

 this peak is very different from this peak„ÄÇ so you can see a huge shift of the distribution„ÄÇ

 So I hear this one„ÄÇ HoweverÔºå when they then do the training„ÄÇ

So you can see here on the left hand sideÔºå we then do the training„ÄÇBoth the standard„ÄÇ sorryÔºå the„ÄÇ

Bchm and the noisy batchmÔºå both„ÄÇPerform really well„ÄÇ

 So even though there is covariate shift or there is no covariate shift„ÄÇ

 it doesn't seem like it affects the performance that much„ÄÇ So even let's say„ÄÇ

 if batch norm fixes this covariate shift„ÄÇIÔºå they think it's not really the explanation why it results in such a good performance„ÄÇ

 because then you would expect for this case hereÔºå a very poor performanceÔºå but that's not the case„ÄÇ

 So that's just one observation from this paper here„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_9.png)

OkayÔºå now let me list a few more papersÔºå I don't want to go into these in too much detail„ÄÇ

 but if you are interested you are yeah encouraged to read more it's of course not required for this class„ÄÇ

 but yeah if you're really interested in batch doesn't hurt to read those okay so„ÄÇ

Just in chronological orderÔºå there is the theory that it reduces covariate shift„ÄÇ

 So this is the original paper„ÄÇ Then we just saw the paper that says that it makes the optimization landscape smoother„ÄÇ

 There's another paper I found where they say that„ÄÇ

The reason why batchome works so well is that batch normalization implicitly discourages single direction„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_11.png)

Reliance„ÄÇThen another paper from 2018 also says that it acts as an implicit regularizer and that is then helping with a generalization accuracy„ÄÇ

And another paper was not so positive about bem„ÄÇ It says essentially that bem causes exploding gradients„ÄÇ

So yeahÔºå we will actually talk about these skip connections in a future lecture tooÔºå by the way„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_13.png)

YeahÔºå let's get now to the practical advice section of this video„ÄÇ

 So there are also slight modifications of batchnoÔºå so one is putting batchn before the activation„ÄÇ

 So this is the original version of batchome that was discussed on the previous slides of previous video and also presented in the original paper So here the workflow is as follows First we compute the net inputs„ÄÇ

And then yeahÔºå we apply batch normÔºå Then we apply the activation function„ÄÇ

 and then we compute the next layer inputs„ÄÇ HoweverÔºå we can also put batch norm after the activation„ÄÇ

 So what we do is we essentially„ÄÇFlip the order here„ÄÇ

 We first apply the activation function and then use batchome„ÄÇ So we can see net inputs„ÄÇ

 applying activation functionÔºå batchomeÔºå and then computing the next layer inputs„ÄÇÂóØ„ÄÇ

This could make more sense in practice if you think about it asÔºå let's say for simplification„ÄÇ

 Let's say you use batchome and batchome learns to standardize„ÄÇYou are„ÄÇ

Fature activation distribution et say at mean 0„ÄÇRightÔºå so you have now0 min„ÄÇFeatures„ÄÇ

 but then you apply the activation function„ÄÇLet's say on the left hand side„ÄÇ

 we apply the activation function then„ÄÇIt would kind of change this distribution before it gets to the next layer„ÄÇ

 rightÔºå because let's say we have mean 0 for a given feature„ÄÇ And if you useÔºå for example„ÄÇ

 a logistic sigmoid function„ÄÇThen the mean will change from 0 to „ÄÇ5Ôºå rightÔºü

 So the next layer receives inputs that are at point centre at 05 and not centred at 0„ÄÇ rightÔºü

 So that way„ÄÇThe sigmoid would re remove also the negative values„ÄÇ So in that way„ÄÇ

 maybe it makes more sense to put pet norm after the activation personallyÔºå I haven't found any„ÄÇ

 yeahÔºå big difference„ÄÇ I haven't benchmarked this extensively„ÄÇ I tried it here and there„ÄÇ

 I didn't notice a big difference„ÄÇ I did some searching online„ÄÇ

 that was repository that I' found where they did some experiment or experiments„ÄÇ

 multiple ones and found that before and after„ÄÇ

![](img/ddf5484a85844b4b00ff0a05549f327c_15.png)

Did make or have a difference„ÄÇ And they foundÔºå for instance„ÄÇ

 that putting beome after the activation indeed resulted in a in a better accuracy„ÄÇSoÔºå I mean„ÄÇ

 why not It may make sense just to put it always after the activation and not worry about it„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_17.png)

AlrightÔºå some more practical consideration„ÄÇ So batch normÔºå I should also say„ÄÇ

 only works really well if we have reasonable batch sizes„ÄÇ So if we make the batch sizes too small„ÄÇ

 then batchome doesn't work very well anymore„ÄÇ So that might be explained by the fact thatÔºå yeah„ÄÇ

 these mean and standard deviation statistics become very noisy if we have small batch sizesÔºå right„ÄÇ

So here I found a paper this is actually a paper about an alternative to batch norm it's more for convolutional networks but it's called yeah groupnm„ÄÇ

 we can revisit this maybe later we don't have to talk about groupn here„ÄÇ

 it's really just something that doesn't work for multilayer perceptionceptance it's only for convolution networks but what I found interesting was here this figure„ÄÇ

Ignoring the group norm plot hereÔºå what you can see is they did an experiment with different batch sizes„ÄÇ

 and here's the percent arrow„ÄÇAnd„ÄÇThey have it yet 32„ÄÇ Unfortunately„ÄÇ

 they are not showing larger onesÔºå but you can see 16 and„ÄÇ32 both workÔºå but if you make it smaller„ÄÇ

So if you make the batch size smallerÔºå like 8Ôºå4 or2Ôºå then batchome becomes really bad„ÄÇ

 so get a very large arrow here„ÄÇ So just a practical considerationÔºå if you use batchome„ÄÇ

 it's yeah highly recommended to use a batch size greater than 16 in practice I would even say bigger than 6 32 even 64„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_19.png)

So here's some further reading resources„ÄÇ If the previous ones were not enough„ÄÇ

 if you are interested„ÄÇ yeahÔºå like I saidÔºå we can't cover by everything in this course„ÄÇ

 This is an introductory course„ÄÇ So I have to be very selective with topics„ÄÇ

 but if you are interested some„ÄÇAdditional papers that you might find interesting„ÄÇ

 maybe in the context of your project you can try one or the other paper and see whether that helps„ÄÇ

 so one is conditional batchomeÔºå which is essentially including class information so having different parameters for the different classes in the dataset which kind of makes sense„ÄÇ

YeahÔºå then there's a paper on measuring the effects of data parallelism on neural network training„ÄÇ

 So here what I just found interesting is it's not directly related to batch none„ÄÇ

 but it's related to the previous slide regarding the batch sizes„ÄÇ

So sometimes in practice people say don't use a large batch size„ÄÇ So let's go back„ÄÇ

 So there are two theoriesÔºå one theory or one not theory but recommendation is to use large batch sizes because large batch sizes help you to utilize the GPU better because you have large matrix modifications and then the training will be faster and it's it's better Some people say choosing smaller batch sizes is better because it helps with generalization because then maybe the loss gets„ÄÇ

YeahÔºå we less likely to get trapped in local mini on the lost surface„ÄÇHoweverÔºå yeah„ÄÇ

 this is very controversial here was just a paper I found interesting where they say basically they don't find any evidence that large batch sizes degrade the performance„ÄÇ

ÂóØ„ÄÇYeahÔºå then„ÄÇAnother one here is„ÄÇAn alternative„ÄÇTo batch normalization that might I have not tried it myself„ÄÇ

 but it sounded interesting„ÄÇ It just came out this year„ÄÇ I saw that this year„ÄÇ

 I haven't even read this yet because I have such a huge list of papers I find interesting„ÄÇ

 and I want to read„ÄÇ but there's only so many hours in the dayÔºå but that sounded pretty appealing„ÄÇ

 So they say that they present a pluck in replacement for batch normalization called exponential moving average normalization„ÄÇ

 which improves the performance of existing student teacher based self„ÄÇ

And semi super based learning techniquesÔºå amateur may whether it applies also to regular networks„ÄÇ

 but could be worthwhile looking into„ÄÇ So I just had that on my reading list and I thought I just included here„ÄÇ

And the last one I have here is„ÄÇÂóØ„ÄÇIt's actually without batch normalization„ÄÇ So here„ÄÇ

 so usually people say batch normalization is very important to reach good performance„ÄÇ

 And here in this recent paperÔºå also from 2021Ôºå they demonstrated that also„ÄÇ

 you can actually get good performance without batch normalizationÔºå so„ÄÇ

They mentioned that previous work was already successful in demonstrating that you can train without batch normalization get good performance„ÄÇ

 but these models do not match the test aacies of the best normalized networks and are often unstable for large learning rates or strong data augmentation„ÄÇ

 So while it is possible to get good performanceÔºå it's not as good as with patch batch normalization and„ÄÇ

The models are more unstable„ÄÇBut in this workÔºå theyÔºå yeah„ÄÇ

 they have an alternative to make them stable„ÄÇ So they call that adaptive gradient clipping„ÄÇ

 So gradient clipping is a technique„ÄÇThat if you have a very large gradient„ÄÇ

 you clip it at a certain value so that it doesn't cause problems like expding gradient problems„ÄÇ

 And they say by having adaptiveÔºå an adaptive version of that gradient clipping„ÄÇ

 they can train a network without batch normalization and get a good performance„ÄÇ But yeahÔºå again„ÄÇ

 this is a very recent paper„ÄÇTime has to tell whether this is really also working well across other architectures„ÄÇ

 but that might be also something cool to look into„ÄÇ maybe in the context of your class project„ÄÇ

 So againÔºå I'm just throwing out some ideas or interesting reading material here„ÄÇ

 You don't have to read that„ÄÇ Of course„ÄÇ I'm justÔºå yeahÔºå sharing it because it sounds interesting„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_21.png)

![](img/ddf5484a85844b4b00ff0a05549f327c_22.png)

AllrightÔºå so one last thing„ÄÇI mentioned here these batch sizes„ÄÇ

 and you may have noticed they are all powers of  twoÔºå rightÔºå2 to the power of 2Ôºå3Ôºå4Ôºå5Ôºå6„ÄÇ

 and so forth„ÄÇ So why is thatÔºå Why do people usually choose batch sizes in powers of  two„ÄÇ

 So that has something to do with how GPus work„ÄÇ Theres this„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_24.png)

Just brieflyÔºå there this paradigmÔºå single instructionÔºå multiple data„ÄÇ So let's like„ÄÇ

You have multiple data pointsÔºå or let's say multiple values in a vector„ÄÇ

 and you have a single instructionÔºå let's say like addition„ÄÇ

 and you can distribute this instruction to apply it to everything in a vector„ÄÇ

 So that's like how on a certain levelÔºå parallelism works on a CPU and GPU„ÄÇSo„ÄÇAnd„ÄÇüòîÔºåOn GPUus„ÄÇ

 the number of processing qua is usually a power of two„ÄÇ

 That's just how I don't know the chip design works„ÄÇNow„ÄÇ

 you can take that and take it or make it an advantage by choosing a batch size that is also a power of two„ÄÇ

 because then you canÔºå you better balance out how these cores are used„ÄÇ For instance„ÄÇ

 if you have 32 columns in a matrixÔºå you can„ÄÇSo let's say you have your design matrix„ÄÇ

 you would do a transpose on this matrix„ÄÇ and then these columns„ÄÇ

 you can map them to dot products to each of the processing core„ÄÇ So if you have 16 processing cores„ÄÇ

 for instanceÔºå you can run two dot products on these 16 cores right So if you have only 31 columns„ÄÇ

 then one core„ÄÇWould be finished fasterÔºå but it has to wait until the other ones finished anyway„ÄÇ

 so you don't fully utilize your CPUU GPU„ÄÇSo in this case„ÄÇ

 it's just so that the GPU is better utilized„ÄÇ People prefer these powers of two when youre choosing batch sizes„ÄÇ

But yeahÔºå this is also anecdotal„ÄÇ You don't have toÔºå of course„ÄÇ

 choose batch sizes that are powers of two„ÄÇ Its just„ÄÇYou know what it is„ÄÇ

 It's actually also making life simpler because there are so many hyperparameter to tune and to consider„ÄÇ

 And if you knowÔºå also would consider values between values powers of  two„ÄÇ

 then you would even have more hyperparmeter to considerÔºå right„ÄÇ

 So if you say I only tune powers of 2„ÄÇ So you have 32 batch sizeÔºå64Ôºå1 28 and so forth„ÄÇ

Then this is less than saying I will consider 25Ôºå30Ôºå35Ôºå40 and so forthÔºå rightÔºå So in this way„ÄÇ

 it's also one way to keep things simpler for hyper permit tuning„ÄÇ And yeahÔºå of course„ÄÇ

 it utilizes the CPU and GP better„ÄÇ but in practice„ÄÇDoes make such a huge difference„ÄÇ

 It's not like you get any significant speed up through that„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_26.png)

OkayÔºå so that was enough about batch normÔºå let's now talk also about weight initialization„ÄÇ



![](img/ddf5484a85844b4b00ff0a05549f327c_28.png)