# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P145ï¼šL17.7- PyTorch ä¸­çš„ VAE æ½œåœ¨ç©ºé—´ç®—æ³• - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ I'm not sure how that happenedï¼Œ but we are already at 65 minutes in this recordingã€‚

 So we only have 10 minutes leftã€‚ So things took longer than I expectedï¼Œ like alwaysã€‚ Soï¼Œ but yeahã€‚

 we only really have one more topic left for today that I wanted to talk about because that's more like a fun topicã€‚

 So this topic is on manipulating the latent space or doing some latent space arithmeticã€‚

 We are goingï¼Œ in particularï¼Œ going to focus on the variational auto encoder that we have trained in the previous videoã€‚

 on the setup a face images and what we are going to do as we are going to take some of these face images and making the people smile moreã€‚

ğŸ˜Šã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_1.png)

So let me give you the brief outline of what we are going to do next in a code example soã€‚

Imagine we have this latent space hereã€‚ Soï¼Œ of courseï¼Œ if you recall the previous videoã€‚

 we had a two dimensional space at 200 dimensional spaceã€‚

 and here I'm only showing a two dimensional spaceã€‚

 This is just because it's easier to draw this in a two dimensional space because I can't draw in a 200 dimensional spaceã€‚

 So but the concept is the sameã€‚ what we are going to do is first we are going to compute a smile vectorã€‚

 So we are going to average the embeddings of all the people that are smilingã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_3.png)

We actually know who is smiling because when I go back to my notebook hereã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_5.png)

Notebook 2ã€‚ we or I told you that there are these 40 targetã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_7.png)

Vectctors or attributesã€‚ sorryï¼Œ in setup Aã€‚ So this is provided in this data setã€‚

 And these are these 40 binary vectorsã€‚ and one of them is smilingã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_9.png)

So we are going to focus on all images where smiling is trueã€‚And thenï¼Œ we are going to average themã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_11.png)

Oopsï¼Œ we are going to average them in this latent spaceã€‚

 and then we are going to do the same thing with people who are not smilingã€‚

 So people without a smileï¼Œ we are calling this the no smile vectorã€‚Then what we do is we takeã€‚Thisã€‚

ğŸ˜”ï¼ŒSmile vector and subtract the no smile vector and get this difference vector here in greenã€‚

So this is a different vector pointing from no smile to smileã€‚And then given a face imageã€‚

 let's call that also here these is the embeddingã€‚ So let's say we have the embedding of a face imageã€‚

 Let's call that original face imageã€‚What we are going to do is to this embedding of the original phase imageã€‚

 we are going to add this different vector multiplied by scaling vector hereã€‚ Alã€‚

 it's a hyperparameterã€‚ It's kind of determining how muchã€‚Of the smile vector we want to addã€‚

And then we obtain this new vectorï¼Œ which yeahï¼Œ is basically aã€‚

The person here that is smiling more if alpha is numberï¼Œ let's say larger than 0ã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_13.png)

Okayã€‚Yeahï¼Œ so let's do this and see how that worksã€‚ So hereã€‚

 I have a code example where we are working withã€‚The model from notebook 2ã€‚

 the variation of auto encoder trained on setup Aã€‚ So I'm importing the only three functions this timeã€‚

 One is just the data lataã€‚ One is for computing the average paces and one is for plotting these modified facesã€‚

So this is the same as beforeï¼Œ just the data loaddownã€‚And here we are now also loading the labelsã€‚

 So the labels are 40 dimensional vectors with these 40 binary attributesã€‚This yearã€‚

And here I have a function to compute the average phasesã€‚

 So I'm starting here in this notebook that are two parts in this notebook1 is the image manipulation in the original spaceã€‚

 I just want to motivate why we want to do the latent space arithmetic and why we don't manipulate the original face images like in the original space because yeah in the original space it doesn't work quite so well and it will show an example So here this function is computing the average face image with a given feature and without a given feature and this feature here I'm choosing is smiling because I'm using the feature and X 31 31 this is really like smiling here from this listã€‚

So let me just show you how I computed thisï¼Œ how I implemented this compute average faces functionã€‚

 So if we go to data hereã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_15.png)

Should have it somewhere hereã€‚Data onï¼Œ compute average facesã€‚

 So what I'm doing here is I'm just initializing these to 0ã€‚Thenï¼Œ I'm iteratingã€‚Overã€‚All the imagesã€‚

Laterï¼Œ I will do the same thing with embeddingï¼Œ but for nowï¼Œ we don't do any embeddingã€‚ We justã€‚

 can ignore this partã€‚ We are just averagingï¼Œ soã€‚If encoding function is noneã€‚

 I just call embeddings the imagesã€‚When you read here embeddingsã€‚

 this is just the original images for now because I don'tï¼Œ I don't set encoding function to anythingã€‚

 so I don'tã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_17.png)

Have it specified hereã€‚I have it to noneã€‚This is because later we can reuse the same function with the actual embeddingsã€‚

 but for nowï¼Œ we are not using embeddingsã€‚ So what I'm doing here is now I'm averagingã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_19.png)

Soï¼Œ I'm averagingã€‚The ones with the featureã€‚Soã€‚There is something called index image with feature feature Fe means featureã€‚

 And this is corresponding to all the images in the batch that have that featureã€‚

So that's how I find it outã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_21.png)

It's maybe a little bit complicated to wrap your head around thisã€‚ like best just by looking atã€‚

 it might be yeah interestingã€‚ Wellï¼Œ might help to play around with thatã€‚

 But like I mentioned beforeï¼Œ the label vector here is is actually a matrixã€‚

 It's a 256 times 40 dimensional matrixã€‚ So for each imageï¼Œ we have a vector of 40 binary attributesã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_23.png)

And then this one hereã€‚ So what I'm doing is I'm selecting allã€‚Imagesï¼Œ and then I only getã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_25.png)

The position of my feature indexã€‚ And I set the feature index toã€‚31 to smilingã€‚

And this is either 0 or1ã€‚ So 0 means no smilingï¼Œ and one means smilingã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_27.png)

And then I'm converting it to a bullï¼Œ a boolean vectorã€‚

 which is either true on faults or true true or falsesã€‚ And this will be then essentiallyã€‚

Index of true and falseï¼Œ a maskã€‚ And then I'm selectingï¼Œ soã€‚

This will be giving me an indicator which imagesã€‚Smiling at which I'm not smilingã€‚

 It will be eed like true faultsï¼Œ faultï¼Œ faultsï¼Œ trueï¼Œ falseï¼Œ faultsï¼Œ trueã€‚

 And here now I'm selectingã€‚From my imagesï¼Œ from the batchï¼Œ which one are smilingã€‚

 And this one means here this wggly thingã€‚ The tilde means not smilingã€‚ So I'm selecting smilingã€‚

 And here I'm selecting not smilingã€‚ So I'm just adding upã€‚

 So I'm summing them over the batch and then add that to my average image with featureã€‚

 So these are the smilingã€‚ And these are the not smiling onesã€‚ I' just adding them upã€‚ğŸ˜Šã€‚

And then I'm also counting the number of images with feature and counting the number without featureã€‚

So the number of smiling and the numberï¼Œ not smilingã€‚

 So and this for loop goes through the whole epochã€‚ in the endï¼Œ when this whole thing is completedã€‚

 I have the sum of all smiling ones and all not smiling imagesã€‚

And then I divide by the number respectivelyï¼Œ and this gives me the averageã€‚

 And then I'm returning these averagesã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_29.png)

Okayï¼Œ so let's take a look at how the results look likeã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_31.png)

Soï¼Œ this isã€‚The averageã€‚Image of all peopleï¼Œ smilingã€‚It's it looks likes on the average smilingã€‚

And this is the averageï¼Œ not smilingã€‚You can see there's a differenceï¼Œ rightã€‚ So this small smilingã€‚

 this is not smiling at allã€‚ So given a face imageã€‚Can we manipulate this face imageï¼ŸThatã€‚

This person is now more smilingã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_33.png)

Let'll many moreã€‚Soï¼Œ smilingnessã€‚So I'm computing the different vector hereã€‚

Here it's more like a difference matrixã€‚And thenï¼Œ I'mã€‚In the top rowï¼Œ I'm addingï¼Œ smilingã€‚

 and at the bottom rowï¼Œ I'm removingï¼Œ smiling for the top row actually looks pretty goodã€‚

 so we can actually make this of a person smile more for the bottomï¼Œ it doesn't work quite so wellã€‚ğŸ˜Šã€‚

So this is done in the original spaceã€‚ So we can actuallyï¼Œ just by adding this average faceã€‚

 we can make this person smile moreã€‚ It works actually quite surprisingly wellã€‚ You will findã€‚

 though it won't work with all theã€‚Different attributesï¼Œ you can play aroundã€‚ I meanã€‚

 instead of using smiling hereï¼Œ you can play around and useã€‚Any of these hereï¼Œ rightã€‚

 So we will find for some it will work better or notã€‚

 So we could technically do this in the original space like I'm showing you hereã€‚

 But yet now we are doing the same thing in the latent spaceã€‚ Usuallyã€‚

 if you have a good high resolution model that can produce good reconstructionsã€‚

 you will find in latent spaceï¼Œ you can do way more fun thingsã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_35.png)

![](img/ee666c6018756e5084ebb2ab7329b7da_36.png)

So this againï¼Œ my auto encoderï¼Œ I'm not training it hereã€‚

 I'm just loading it like before I'm loading it for my second notebookã€‚And now I'm doing thisã€‚

 computing the face or the averages of the smiling and not smilingã€‚

 using my latent dimension and I'm using here this encoding functionã€‚

 So if encoding is set to a functionã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_38.png)

It'sã€‚Yeahï¼Œ and it's encoding my imagesã€‚ So I'm using now the actual embeddingsã€‚

 So recall from beforeï¼Œ this is my auto encoder hereã€‚ So from before we have this encoding functionã€‚

 which is calling encoderã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_40.png)

Then doing the repapaterization and so forthã€‚Soï¼Œ hereï¼Œ I'm encodingã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_42.png)

![](img/ee666c6018756e5084ebb2ab7329b7da_43.png)

And then what I'm doing is I'm computing the difference vector again between with and without featureã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_45.png)

And now I'm plotting and you can seeï¼Œ of courseï¼Œ the reconstruction is not so greatã€‚ but yeahã€‚

 we can actually make the person smile moreã€‚ So alpha is how much of the smiling vector I'm addingã€‚

 So you can see person smiling more and more and moreã€‚

 And also for this reference here now I'm so the bottom is subtracting the difference vectorã€‚

 you can see the person will smile lessã€‚ So that's how much I subtract above his addition and at the bottom is subtractionã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_47.png)

To can see here we can make a person smile more and less by manipulating things in the latent spaceã€‚

 Nowï¼Œ I also have that yeahï¼Œ with and without glassesã€‚ it's also funã€‚ğŸ˜Šã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_49.png)

![](img/ee666c6018756e5084ebb2ab7329b7da_50.png)

So you can seeï¼Œ I can give the person glassesã€‚ And here I can remove glasses since the person doesn't have any glasses to begin withã€‚

 doesn't really workã€‚ But you can seeï¼Œ actuallyï¼Œ it's doing something hereã€‚ But yeahï¼Œ itã€‚

 it's quite fun So we can give the person hereã€‚ glassesã€‚ Alsoã€‚

 And you can also do this with hair colour and other thingsã€‚ Okayï¼Œ so this wasï¼Œ I hopeã€‚

 kind of little fun coat exampleã€‚ And this isï¼Œ yeahï¼Œ the lecture on variational autoenrsã€‚ğŸ˜Šã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_52.png)

![](img/ee666c6018756e5084ebb2ab7329b7da_53.png)

![](img/ee666c6018756e5084ebb2ab7329b7da_54.png)

![](img/ee666c6018756e5084ebb2ab7329b7da_55.png)

It wentnt longer than I expected initiallyï¼Œ but hopefully not to too longã€‚ And in the next lectureã€‚

 we will talk about alternativeã€‚

![](img/ee666c6018756e5084ebb2ab7329b7da_57.png)

Late in variable modelsï¼Œ a so called generative adversarial networkã€‚



![](img/ee666c6018756e5084ebb2ab7329b7da_59.png)

All rightï¼Œ so that's it then for todayã€‚