# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P50ï¼šL8.0- é€»è¾‘å›å½’ã€è¯¾ç¨‹æ¦‚è¿°ã€‘ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hiï¼Œ everyoneã€‚ I hope you had a good weekend and everything went smoothly with the homework one submissionã€‚

 I noticed that there were yeah still some challenges around the understanding of single layer neural networksã€‚

 So todayï¼Œ we will have another chance to take a look at itã€‚ So todayã€‚

 I want to talk about logistic regression as a single layer neural networkã€‚ And that really helpsã€‚

 I think with understanding the basic concept of a forward and a backward passã€‚

 And then we will also in this lecture generalize this concept from binary classification to multiclass classificationã€‚

 because yeahï¼Œ there will be also later onï¼Œ more multiclass stuff going on because in real world scenariosã€‚

 we often have data sets where we have more than two classesã€‚ So in this lectureã€‚

 we will talk about logistic regression as a binary classifierã€‚

 and then multinial logistic regression also known as softm regressionã€‚

 which will be a good data for then understanding multi layerer perceptanceã€‚

 which is the topic of the next lectureã€‚ All rightï¼Œ because there are lots of things to talk aboutã€‚

 Let's get startedã€‚ğŸ˜Šï¼ŒYeah let's briefly recap what we have done in the previous lectures so one thing we did in the previous lectures was implementing the perceptionceptron that's already long time ago it had convergence issues if data was not or the classes were not linearly separable but we solved that convergence issue by using Adeline which was based on a mean squared error loss function and gradient in descent and then last week we conceptualized gradient in descent using computation graphs so if you recall that is how Adeline looked likeã€‚

Andã€‚Here is a dataset set example that couldï¼Œ for exampleï¼Œ not be easily solved by perceptronã€‚

 I meanï¼Œ a perceptron would also be able to come up with a decision boundaryã€‚

 but it would never convergeï¼Œ which is a little bit trickier to handle in practice because the decision boundary would jump back and forth if we train it here because we have these yeah data points like this one which are outliers So in this case or this one you can never get a perfect classification with 100% accuracy if you have a linear decision boundary in this case and a perceptron would have the challenge that it would flip back and forthã€‚



![](img/dde61643984eeac0ddc9007e2ed05284_1.png)

So now we kind of continue with the concept of Adeline but we make some improvementsã€‚

 so one of these improvements is to use yeah loss function called cross entropy instead of the meansquad error and another improvement is to extend this neuron model to multiclass classification so for that we will be using multiple output notes and the so-called softmax activation function I briefly sketched it here on the left hand in the left hand corner we will talk about this in more detail later in this lectureã€‚

But essentially we will have multiple outputs here and that will allow us to yeah classify data sets that have more than two classesã€‚

 So here's an example of the iris data set where we have three flower classes iris versic colorã€‚

 bikinica and sattoa before we could only do binary classification Now you can see that could be three decision boundariesã€‚

 So one way would be here this way we have three linear lines or same model the data looks a little bit different it could also look like this hereã€‚

 those are both so-called soft max regression models which we will be talking about todayã€‚



![](img/dde61643984eeac0ddc9007e2ed05284_3.png)

So yeahï¼Œ just to give you a big picture overview a view of all the topicsã€‚

 it looks like a lot in nine topicsã€‚ Howeverï¼Œ I think they will be relatively yeah shortã€‚

 so I hope they will be short like shorter videos like 10 minutes maxã€‚

 but we will see videos always turn out to be later longer in practice than you might think when you make them the slidesã€‚

 So the topics I have in mind for today are basically discussing logistic regression as an artificial neuronã€‚

 So showing you how logistic regression relates to the concept ofã€‚Edelineã€‚

And then we will introduce the loss function of logistic regressionã€‚

 which is the negative look likelihoodã€‚And then alsoã€‚

 I will show you how we can use gradient descentã€‚Which we talked about beforeã€‚

 we will use gradient descent to train the logistic regression modelã€‚

 And then here there's just some terminology called the logics and cross entropyã€‚

 These are some terms you will find in deep learningã€‚ Yeahã€‚

 and a lot of deep learning concepts actuallyã€‚ So logics and cross entropy are likeï¼Œ I would sayã€‚

Jargonã€‚In deep learningã€‚ So we will just briefly see what these terms meanã€‚And yeahã€‚

 then I will show you a logistic regression code exampleã€‚ So all this stuff hereï¼Œ this is about theã€‚

 I would sayï¼Œ regularã€‚Wellï¼Œ let's call it binaryã€‚Bnoimã€‚Logisticã€‚Reggressionã€‚

 so that's the original logistic regression for binary classificationï¼Œ only for two classesã€‚

 And then this one here or the second partã€‚Would beã€‚Multiclassã€‚Or also called multinoialã€‚Oopsã€‚

Logisticã€‚Regressionã€‚And another word for multinoial logistic regression is softmax regressionã€‚

So we will talk about thatã€‚ and for that so here at first I will yeah introduce the model for thatã€‚

 we will need something called oneH encodingï¼Œ which I will then introduce it' just like a format that we can use to represent categorical dataã€‚

And then I will also introduce the multi category cross entropisï¼Œ which is an extension from theã€‚

B cross be hereã€‚Then we will talk about the softmax regression learning ruleã€‚

 essentially gradient descent againï¼Œ4ã€‚Softmax regressionã€‚

 And then I will also show you a code exampleã€‚And then in the next lecture on Thursdayã€‚

 what we will do is we will just add a hidden layer to the softm regression modelã€‚

 and then we have a multi layer perceptron or a first small deep neural networkã€‚ All rightã€‚

 so with thatï¼Œ let's get started with logistic regression as an artificial neuronã€‚



![](img/dde61643984eeac0ddc9007e2ed05284_5.png)