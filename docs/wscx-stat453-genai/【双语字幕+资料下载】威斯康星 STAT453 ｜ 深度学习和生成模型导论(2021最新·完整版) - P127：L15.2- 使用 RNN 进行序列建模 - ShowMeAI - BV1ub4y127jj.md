# P127ÔºöL15.2- ‰ΩøÁî® RNN ËøõË°åÂ∫èÂàóÂª∫Ê®° - ShowMeAI - BV1ub4y127jj

AllrightÔºå let's now talk about how we can modify multi layer perceptrons to capture sequence information so in particular we are going to talk about sequence modeling with recurrent neural networks„ÄÇ

So but before we get to thatÔºå how can we tell whether our model already uses sequence information„ÄÇ

 so for instanceÔºå if you think of logistic regression or multilay perceptrons„ÄÇ

 do these types of models actually use sequence informationÔºü

So the answer is no and how can you know I meanÔºå there are two ways or two types of sequence information that might be encoded in our training set„ÄÇ

 One type is across the training example access and the other type is across the feature axis so maybe to illustrate that let's revisit the IIS data and now talking about IIS is a little bit boring„ÄÇ

 but I think it's a nice simple data set to illustrate these types of problems„ÄÇSo in Iris„ÄÇ

 what we had we hadÔºå let's call it setal length„ÄÇS width„ÄÇPaal length and petal width„ÄÇ

 So we have this tabular data set„ÄÇAnd we have„ÄÇThe training example like 1Ôºå2Ôºå3Ôºå4Ôºå up to 150„ÄÇNo„ÄÇ

When you have this data set„ÄÇAnd„ÄÇSo that let's say this is your data set and you split it into a training set and a test set„ÄÇ

And then„ÄÇYou train the model on this training set„ÄÇ Its call it train„ÄÇTest„ÄÇYou can shuffleÔºå actually„ÄÇ

All the records in the test set„ÄÇSo you assumeÔºå of courseÔºå alsoÔºå this is shuffled because you want„ÄÇ

before you split in iris usually you have the 51st floor are sattoa„ÄÇ

 the second 50 R rea and the third 50 R versicaÔºå so let's assume you split them in a way that they are equally distributed„ÄÇ

NowÔºå in the training and test set„ÄÇ But nowÔºå given the test set„ÄÇ

 you can actually shuffle all the records in the test set„ÄÇ

 And when you evaluate your model on the test setÔºå you should still get the same performance„ÄÇ

 So this is kind of like a way of saying the model doesn't really use any sequence information„ÄÇ

 It regards the considers the data as so-called I I D„ÄÇ I I D means„ÄÇ

That the data is independent and identically distributed„ÄÇ So this means that each training record„ÄÇ

Is independent of each other„ÄÇ So it has been sampled independently„ÄÇ

 And also it's from the same distribution„ÄÇ So distribution of flowers or ir flowers so„ÄÇ

How to tell whether your model uses sequence information across the training access is really by„ÄÇ

 let's say„ÄÇDoing this thought experiment of shuffling the test set„ÄÇ And you canÔºå of courseÔºå yeah„ÄÇ

 probably tell that whether you shuffle the test set or not„ÄÇ

The performance of the model should be exactly the same on the test„ÄÇ

 if you use multilay perceptrons or logistic regression„ÄÇ

Another type of sequence information might be encoded in the featuresÔºå right so„ÄÇ

The order here of the fions„ÄÇSo what you can also think about is if what happens if you swap columns„ÄÇ

 So let's say you use this this original ir data set with these columns here„ÄÇ

Then you train the model on this datasetÔºå and then you test it„ÄÇ Let's say you get 90% accuracy„ÄÇNow„ÄÇ

 let's say just out of funÔºå you are swapping these two columns here„ÄÇ

 like a sample length and pedal width„ÄÇ Now in your modified data set„ÄÇ

 you have let's say petal width here and sample length here„ÄÇAnd then again„ÄÇ

 you split this the same way into training and test like you did before using the same records for each data set„ÄÇ

NowÔºå if you train the model and test itÔºå you should get exactly the same 90% accuracy„ÄÇ

 You can try this in practice you will find the model performance should be exactly the same„ÄÇ

 and this is because let's say a multilay perceptrons and logistic regression„ÄÇ

They don't use sequence information„ÄÇAnd across the features„ÄÇ

 they regard the features as independent here„ÄÇ So in that way„ÄÇ

 they don't have to occur in a certain order„ÄÇ And if the features occur in a certain order„ÄÇ

 this information is ignored„ÄÇThis is becauseÔºå I mean„ÄÇ

 you can simply test this by swapping columns and then training the model on the dataset set with the swap columns and testing it on a dataset set with swap columns„ÄÇ

 and you will find there is no difference„ÄÇ

![](img/1bc3390e4881acb0aa3b84999f84804e_1.png)

This can be a problem thoughÔºå when you think back of our back of words model„ÄÇ

So the back of words model had this vocabulary„ÄÇAnd it essentially gets rid of the word order in each training example in the feature vector„ÄÇ

 So if you think of„ÄÇAn example here where I have written down just a spontaneous sentence saying the movie my friend has not seen is good„ÄÇ

This isÔºå of courseÔºå different than the movie my friend has seen is not good„ÄÇ

So two different meaningsÔºå so„ÄÇFirst meaning is in the first sentence that the friend has not seen a movie which is good„ÄÇ

 And the second oneÔºå it has seen the friend has seen the movieÔºå but the movie is not good„ÄÇ

 So here we have a good movie„ÄÇ and here we have a not„ÄÇ let's say„ÄÇA bad movie„ÄÇ

 So two different sentences„ÄÇ But if we would use the back of words model where we have the text as the word frequency„ÄÇ

This would get lostÔºå because both„ÄÇBoth sentences would result in the exactly same feature vector„ÄÇ

So in this caseÔºå we have this„ÄÇBetween in the inputs here and the features„ÄÇ

 we have this ordering information that it really depends on„ÄÇ3 depends on which order words occur„ÄÇ

 So not seen„ÄÇIs very different from„ÄÇNot good„ÄÇ Like theÔºå the ordering really matters here„ÄÇSo„ÄÇ

 and recurrent neural networks can help us capture this ordering information„ÄÇ



![](img/1bc3390e4881acb0aa3b84999f84804e_3.png)

Yeah and yeah some examples of sequence dataÔºå for exampleÔºå text classification„ÄÇ

 which is something we will be focusing on in this lecture„ÄÇ

 especially at the end when I show you a pyarch example„ÄÇ

 So here in the text data set you can think of each so the time dimension over the words So you have text„ÄÇ

 let's say T1 T2 like it is a little confusing because it's T for time let's say document T1 document D2 and each document is a text So you have the„ÄÇ

Time dimension over the wordsÔºå and then you have also had different training examples„ÄÇ

 so each document would be one training example for example„ÄÇAnd yeah„ÄÇ

 also something like speech recognition there you have a sequence of sounds or language translation„ÄÇ

 which translates from one sequence into another sequence„ÄÇThere's also yeah stock market prediction„ÄÇ

 which is a common and popular problem„ÄÇ So in stock market prediction„ÄÇ

 you can think of maybe each each stock at a data point as a training example„ÄÇ

 So you have stock  oneÔºå stock2 and then you have the prices or you have a feature vector„ÄÇ

 let's say a feature vector„ÄÇCould be the priceÔºå but it could also be the sentiment or some news information„ÄÇ

 someÔºå some type of feature vector„ÄÇ It's called F1„ÄÇF2Ôºå F3„ÄÇone„ÄÇTo„ÄÇ3„ÄÇ

 so you have a feature vector for each time step right hereÔºå the time dimension„ÄÇWould beÔºå let's say„ÄÇ

The time of the day or something„ÄÇOr the date day itself or the month or something like that„ÄÇ

 So the time dimension would be here really like literally time whereas in text data„ÄÇ

 the time dimension would be just the order of the words So time using time is a little bit loose so we can also just more think of it as as sequence„ÄÇ

Another example here would be DNA or sequence modeling„ÄÇ

 so instead of just thinking of text and words you can also think of a DNA sequence so here it's by character„ÄÇ

For example„ÄÇ

![](img/1bc3390e4881acb0aa3b84999f84804e_5.png)

AlrightÔºå so these are just a few examples„ÄÇ NowÔºå let's get to the part where we actually talk about how an R and N looks like„ÄÇ

 So previouslyÔºå we worked with so called„ÄÇFeed forward neural networks„ÄÇSoÔºå our„ÄÇÂì¶Â•ΩÂêß„ÄÇ

Milaier perceptrons or conversion networksÔºå logistic regression„ÄÇ

 they were all cases of feet forward neural networksÔºå so we had usually an input vector„ÄÇ

 feature vector xÔºå then some in a multiai perceptron case or conversion case„ÄÇ

 some hidden layers and then outputs„ÄÇSo now in the recurrent neural network setup up„ÄÇ

 that's why it's called recurrent„ÄÇ we have this recurrent edge„ÄÇ

So what's new here is we have a time step T and we get a feature vector at a time step T„ÄÇ

 give it to the hidden stateÔºå and then it results in an output at time step T but„ÄÇ

In addition to thatÔºå instead of for this hidden layer only receiving the input here„ÄÇ

 but it additionally is receivesÔºå it's also receiving the input„ÄÇFrom the previous time steps„ÄÇ

 for exampleÔºå t minus1 and so forth„ÄÇ So it's also youre receiving previous information from previous time steps„ÄÇ



![](img/1bc3390e4881acb0aa3b84999f84804e_7.png)

So it's maybe a little bit more clear to show it like this„ÄÇ

 And this is also how it's usually implemented„ÄÇ It's the unfolded state of this single layer recurrent neural network I was showing you on the previous slide„ÄÇ

 So this is again what I showed you on the previous slide and we have this recurrent edge and we can actually unroll this„ÄÇ

NetworkÔºå so if we have a sequence„ÄÇLet's say consisting of three time steps„ÄÇ

 then we can enroll it like this„ÄÇSo given a time step T„ÄÇLet's focus„ÄÇ Yeah„ÄÇ

 let's focus on this timecept T in the center„ÄÇSo it receives the feature vector at time T„ÄÇ

Which results in the hidden stateÔºå and this results in the output„ÄÇ But in addition to that„ÄÇ

 this time step also receives„ÄÇThe input from the previous layer„ÄÇnot layer„ÄÇ

 sorry from the previous time step„ÄÇYou can see there is this time connection there are now two inputs„ÄÇ

 One input is from the sequence input from the X„ÄÇ This is our feature vector here„ÄÇAt time step T„ÄÇ

And this is the„ÄÇHidden„ÄÇState„ÄÇOf the previous„ÄÇTime step„ÄÇWhich is some T -1„ÄÇ

So here this allows the network being aware of the order of the sequenceÔºå rightÔºüAnd then this„ÄÇ

 this one here is passed then in the next step to t plus 1 and so forth„ÄÇ

So here on the right hand sideÔºå also bothÔºå both they are„ÄÇ

Equivalent so same network is just a different way of showing the network„ÄÇ

 this is just showing the compact notation with this recurrent edge„ÄÇ

 but in practice usually we use the unfolded version„ÄÇ

 the right hand side it's just a different way of showing it and that's also how we would implement it in code„ÄÇ



![](img/1bc3390e4881acb0aa3b84999f84804e_9.png)

So„ÄÇHere„ÄÇüòîÔºåThis was showing you in the previous slide a single layer recurrent neural network„ÄÇ

 but of courseÔºå we can also extend this concept to multi layer recurrent neural networks„ÄÇ

 I will also show you some examplesna and code„ÄÇ So here we have now two hidden layers„ÄÇ

So the same concept applies that for each hidden layerÔºå we have this recurrent edge here„ÄÇ

 So when we unfold it again„ÄÇYou can see this one here is receiving now the input from its previous hidden layer in the same time step and also the input from the same layer from t -1„ÄÇ

YeahÔºå and this is the general setup of how a recurrent neural network looks like„ÄÇ



![](img/1bc3390e4881acb0aa3b84999f84804e_11.png)

YeahÔºå so just to emphasize againÔºå each hidden unit receives two inputs„ÄÇ So if we focus on this unit„ÄÇ

 it receives an input from here and from here„ÄÇ

![](img/1bc3390e4881acb0aa3b84999f84804e_13.png)

AlrightÔºå so this is the general setup of how a recurrent neural network looks like and in the next video I want to show you also that we can use this architecture for different types of sequence modeling tasks and then after that video I will show you how the back propagation algorithm works for this type of model„ÄÇ



![](img/1bc3390e4881acb0aa3b84999f84804e_15.png)