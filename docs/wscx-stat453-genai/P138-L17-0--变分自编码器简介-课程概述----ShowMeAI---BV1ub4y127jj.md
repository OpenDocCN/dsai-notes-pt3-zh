# P138ï¼šL17.0- å˜åˆ†è‡ªç¼–ç å™¨ç®€ä»‹ã€è¯¾ç¨‹æ¦‚è¿°ã€‘ - ShowMeAI - BV1ub4y127jj

Yeah hi everyoneï¼Œ so last week we learned about auto encoders auto encoders are special types of neural networks that consists of two partsã€‚

 an encoder that takes an input and decompresses it into a smaller dimensional representation and the decoder which takes the smaller dimensional representation and yet reconstructs the inputã€‚

I showed you then an example where I was drawing a random coordinate from this lower dimensional spaceã€‚

 and I was using the decoder to yeah create a new image that is somewhat similar to the training dataã€‚

 So in that wayï¼Œ we can think of it as generating new dataã€‚Howeverã€‚

 when we talk about higher dimensional spacesã€‚ So if the lower dimensional space is moreã€‚

 let's say than more than two dimensionsï¼Œ this method doesn't work very well because yeah there are several challenges with thatã€‚

 I will go into more detail about that in this lectureã€‚ So in this lecture overallã€‚

 I want to talk aboutã€‚Particular type of auto encoder called the variational autoenrã€‚

 and this variational auto encoder is better for sampling new data so you can think of the variational auto encoder as a generative model that we can use to sample from a distribution and generate new dataã€‚

So in that wayï¼Œ it's kind of like a modified version of that auto encoderã€‚

 particularly well suited for yeah creating new dataã€‚

 And before I go into too much detail about how that worksã€‚

 let me give you the lecture overview and then we will learn step by step how this variational auto encoder worksã€‚

 and I will show you alsoï¼Œ of courseï¼Œ some code examplesã€‚ againï¼Œ the Mist code exampleã€‚

 but also more interestinglyï¼Œ we will take a look at a data set consisting of face images and then we will also learn how we can make people smileã€‚

 Alrightï¼Œ let's get startedã€‚Allrightï¼Œ So yeahï¼Œ these are our topics for today and really don't to worryã€‚

 I know it looks like a lot 8 topicsï¼Œ but I'm really keeping it short this timeã€‚ I know that yeahã€‚

 our lecture days are limitedã€‚ There are only a few days left in the semesterã€‚

 And you are all busy withï¼Œ yeahï¼Œ working on your class projects and other classes as wellã€‚

 I am aware of thatã€‚ So I don't want to make things too complicated at the end of the semester and just give you big picture overview of what a variational autoenr is hereã€‚

 And I only made 25 slidesã€‚ I'm very confident that I'm going to be able to finish this lecture within our regular 75 minute lecture timeã€‚

 And yeahï¼Œ I also have code examples of courseã€‚ So with that lecture hereã€‚

 I'm hoping to give you a big overviewï¼Œ big picture overview of how a variational autoencor worksã€‚ğŸ˜Šã€‚

And I will also provide you then with some future references if you are interested because yeah there is also a lot of mathematical underpinnings behind variation autos which we can't cover here in this lectureã€‚

 but some of you may be interested in thatï¼Œ so I will also provide further studying resourcesã€‚



![](img/33bc47ec5a3cc3af90377ed00d48ad35_1.png)

Alrightï¼Œ so in the next video we will start with the overview of what a variational auto encoder isã€‚



![](img/33bc47ec5a3cc3af90377ed00d48ad35_3.png)