# P156ÔºöL19.2.2- Âú® PyTorch ‰∏≠ÂÆûÁé∞Â≠óÁ¨¶ RNN(‰ª£Á†ÅÁ§∫‰æã) - ShowMeAI - BV1ub4y127jj

All rightÔºå let's now take a look at a code implementation regarding the character R and N that we talked about in the previous video„ÄÇ

 So in the previous videoÔºå it gave you some overview of the LSTM and the LSTM cell„ÄÇ

And I prepared two notebooks„ÄÇ One is based on using„ÄÇ

 I can maybe just scroll down for now on using the LSDM class„ÄÇ and one is based on using„ÄÇ

The LSTM cell classÔºå personallyÔºå I think for this type of model„ÄÇ

 it makes more sense to use the LSTM cell„ÄÇ It's a little bitÔºå I thinkÔºå more intuitive„ÄÇ

So going back to what I talked about in the previous video„ÄÇ So here that's the LSDM cell class„ÄÇ

 And we willÔºå we will only consider„ÄÇOne layer„ÄÇ I meanÔºå we can easily extend it for multiple layers„ÄÇ

 but we will only have one layerÔºå so„ÄÇLet's blend that part out„ÄÇAnd„ÄÇWe will essentially„ÄÇ

 so if you consider this partÔºå we will receive one initial hidden state and one initial cell state„ÄÇ

Together with that input token like thisÔºå it will receive one input token hidden„ÄÇStateÔºå cell state„ÄÇ

 and then produce one outputÔºå and this output will go to a fully connected layer to do the character prediction„ÄÇ

Then we will move to the next input„ÄÇ So hereÔºå the next inputÔºå it will receive„ÄÇ

The hidden stayed from the previous time step and the cell stayed from the previous type step„ÄÇ

Together with a new characterÔºå and then it will produce again„ÄÇ

 an output that goes to a fully connected layer to predict the next character„ÄÇ

Then we will go on here againÔºå it will receive receive the hidden state and the cell state from the previous time step„ÄÇ

The current times of inputÔºå output something and so forth„ÄÇ So that's how the NTM cell class works„ÄÇ

AlrightÔºå But before we get to this partÔºå the LSTM cell classÔºå let's start at the top„ÄÇ

 Let me even'cause it's quite fast to run„ÄÇ Let me even„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_1.png)

Run this from scratch„ÄÇ We haven't done this in a long time„ÄÇ AlrightÔºå soÔºå I meanÔºå from scratch„ÄÇ

 I meanÔºå by executing it as we go here„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_3.png)

So we start by implementing oh by just importing some libraries„ÄÇ

 So here I didn't use any helper files I try to keep everything in the notebook because the code here is relatively short„ÄÇ

And simple„ÄÇSo here I have some hypoparmeters like the text portion size„ÄÇ

 So how long a typical text portion is„ÄÇThe number of iteration for iterations for training„ÄÇ

 So here we don't use epochs„ÄÇ We just use iterations„ÄÇÂóØ„ÄÇLearning rateÔºå the size of the embedding„ÄÇ

And the size of the hidden layer„ÄÇOkayÔºå let's execute that„ÄÇI' runningning this on a CPU„ÄÇ

I actually had problems running this on the GPU„ÄÇ I think there's a buck in Pytorch„ÄÇ

 so it runs fine on the CPUÔºå but on the GPU it will restart the kernel And if I run this in my terminal without to put an notebook„ÄÇ

 it gives a segmentation fold and my suspicion is that„ÄÇYeahÔºå it's a back in Pyth and„ÄÇ

Because it's maybe related to the fact that it's loading the data too fast„ÄÇ And there's some„ÄÇ

 it's trying to„ÄÇAccess a memory in the GPU that is not free yet„ÄÇ So actuallyÔºå this shouldn't happen„ÄÇ

 So IÔºå I believe there's a bug somewhere„ÄÇBut anywaysÔºå it's quite fast so we can„ÄÇRun this on the CPU„ÄÇ

OkayÔºå so we use this as our character set from the string„ÄÇPython library„ÄÇ

 So string is like standard library in Python for string related things„ÄÇ

And we will use all printable characters„ÄÇ So a bunch of themÔºå actuallyÔºå how many are there„ÄÇIt same„ÄÇ

100„ÄÇSo we will want use a set of 100 printable characters So numbersÔºå lower case„ÄÇ

 upper case letters and special characters„ÄÇYeahÔºå so actuallyÔºå yeah„ÄÇ So we will use„ÄÇ

Has the data set the CoVd 19 FAQ from the University of Wisconsin websiteÔºü

I went actually to that websiteÔºå and extracted all„ÄÇOn the FAQ questions here„ÄÇ So IÔºå as you can see„ÄÇ

 there was a lot of„ÄÇTextÔºå so I just copied everything into a text file„ÄÇ So this is our training set„ÄÇ

 Its all Covid 19 related„ÄÇQuestions on our university website„ÄÇÂóØ„ÄÇOkay„ÄÇ

 so here I'm just opening and loading the text file„ÄÇSo in totalÔºå we have„ÄÇ

84000 characters in our textitesÔºå like a small bookÔºå almost„ÄÇok„ÄÇ

So here I have a function for getting a random portion of the text of„ÄÇSize„ÄÇText lengthÔºå so„ÄÇ

Do I have this text portionÔºå sorry„ÄÇMy text portion size 200Ôºå so it gets„ÄÇ

Text of the portion size 200 from the whole text hereÔºå randomly„ÄÇ

So this will be our yeah training batch„ÄÇOkayÔºå so yeah those random portions„ÄÇ

 so you can see some letters are chopped off„ÄÇ but for all our simple case hereÔºå it's good enough„ÄÇ

 So againÔºå this is not perfect„ÄÇ So you may in real world application„ÄÇ

 you may want to implement the function that it has like a complete sentences or something like that„ÄÇ

Should't just keep things simple„ÄÇ This is just a simple function just getting 200 characters at a time„ÄÇ

Then I have a function converting the characters here to tensrs„ÄÇ

So here this is just getting the index for the characters„ÄÇ So if we have 100 characters„ÄÇ

 it gets the index rightÔºå So a would be index 10 and so forth„ÄÇ

 So it's converting the strings into numbers that we can work with in Pythtorch„ÄÇAnd„ÄÇ

This is putting those things together„ÄÇ So this is for drawing a random sample for training„ÄÇ

 So this is just getting a random text portion in string format„ÄÇ

 This is for converting a string to integers and this does both„ÄÇ It's getting the random portion„ÄÇ

 right„ÄÇConverts that into integers„ÄÇAnd then it also gets our labels„ÄÇ

 So the labels are the inputs shifted by one character because here our task isÔºå yeah„ÄÇ

 to predict the next character in the sequenceÔºå rightÔºå So if I do that and draw my random sample„ÄÇ

 So my random samples are 0Ôºå94Ôºå24 and so forth„ÄÇ And you can see„ÄÇThe target is shifted just by one„ÄÇ

 rightÔºå Because if we are here and we want to predict the next characterÔºå the next character is 94„ÄÇ

 And from hereÔºå the next character is 24 and so forth in interteger representation„ÄÇ So this is our„ÄÇ

Or batch of features„ÄÇAnd this is ourÔºå wellÔºå these are our labels„ÄÇKim„ÄÇ

So here's now our R R N implementation„ÄÇSo I just have something to keep track of the hidden size„ÄÇ

 This is our embedding layer that goes from the integer„ÄÇ

 the character integer to the embedding vectorÔºå a real value vector of size„ÄÇ Let me see of size„ÄÇ100„ÄÇ

 and the hidden dimension is 128„ÄÇOkayÔºå so we have the embedding size and then the LDM cell„ÄÇ

Takes vectors of sizeÔºå128„ÄÇAnd has a hidden size of1 sorryÔºå of 100 and has a hidden size of 128„ÄÇ

 So if I go back to my slides„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_5.png)

Maybe using this representation here„ÄÇ So our text we had here a one hot encoding„ÄÇThis isÔºå yeah„ÄÇ

 when we want to compute the loss„ÄÇWe use actually just an integer here„ÄÇ

 So here this would be the integer 2Ôºå the integer 0Ôºå and integer 1„ÄÇ if you look at this here„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_7.png)

RightÔºå so this one inÔºå in this figure would correspond to an SÔºå for example„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_9.png)

And then the embedding layer will output„ÄÇ100 dimensional„ÄÇ

Vectome and the hidden layer will be a 128 dimensional vector„ÄÇSo let's see how we use that actually„ÄÇ

 in the forward pass„ÄÇSo in the forward passÔºå we first put the character through the embedding„ÄÇ

 So this will be„ÄÇAccepting batch size and bedding dimensionality„ÄÇ

 we use only one character at a time„ÄÇ So it will be one times thebedding dimensionality„ÄÇ

 which is in our case 100„ÄÇThen we give to the R and NÔºå which is an LCDM cell„ÄÇ

 We give the embedded vectorÔºå which is„ÄÇThe 1 times 100Ôºå together with a hidden„ÄÇ

State and the cell state from the previous iteration„ÄÇSo thisÔºå if we are here„ÄÇEssentially„ÄÇ

 or maybe use the other representation again„ÄÇ So if we„ÄÇLet's say the first step here„ÄÇ

So we are currently running here the this R and N„ÄÇ It will get the hidden state and the cell state from the previous iteration„ÄÇ

 This is these two„ÄÇ And theseÔºå we provide them via„ÄÇBy the forward pass as input„ÄÇ

 So these will go into this one and these will return a new set of hidden and cell states„ÄÇ

 So here these are the inputsÔºå and then they return these outputs hereÔºå these two for the next round„ÄÇ

And then this is computing our„ÄÇLodges for the softmÔºå for the cross entropis„ÄÇSo„ÄÇThis one output here„ÄÇ

 This is essentially this one through a fully connected layer„ÄÇ So this will be„ÄÇ

It's clearer like this„ÄÇSo itÔºå this will be„ÄÇLike this„ÄÇ

So we'll be giving us one output where here we have„ÄÇA fully connected there in between„ÄÇ

Gives us one output„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_11.png)

OkayÔºå and we return the output because we use it for computing the loss„ÄÇAnd also„ÄÇ

 when we want to take a look at the text so that we can generate some text„ÄÇThe next character„ÄÇ

And then alsoÔºå when we want to generate textÔºå of course„ÄÇ

 we have to feed the output back into the input„ÄÇ So if I go back„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_13.png)

There's too many slides here„ÄÇ When I go back hereÔºå this visualizationÔºå this is for training„ÄÇ

 but for generating textÔºå we feed the output here so we get this input„ÄÇ

 produce an output and the output gets fed to the next time step is input so that we can generate new text„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_15.png)

YeahÔºå anything else„ÄÇ YeahÔºå just the dimensionality for reference„ÄÇÂóØ„ÄÇAnything out„ÄÇ

This is not right here„ÄÇ I think this should be„ÄÇDefinitely not this one„ÄÇ It should be the„ÄÇ

Number of characters„ÄÇOhÔºå but here I said hidden say output size would be yeahÔºå the output size„ÄÇOkay„ÄÇ

YeahÔºå and we would„ÄÇ So we one more thing„ÄÇ We have this initialization of the zero state„ÄÇ

 So here we have to start somewhereÔºå rightÔºå so„ÄÇIf I go back to my„ÄÇVisualization„ÄÇHere„ÄÇ„Åù„ÅÜ„ÄÇ

Me rid of one of those„ÄÇSo here we don't have initial input„ÄÇ input the input is here„ÄÇ

 so we have to have some some zero state here„ÄÇ and this is here my my zeros just„ÄÇ

 just some initial state„ÄÇOkayÔºå and alrightÔºå then let's get started„ÄÇ So actuallyÔºå the output size„ÄÇ

 I should mentionÔºå this is the same size as the input size„ÄÇYeahÔºå so let's initialize the R and N„ÄÇ

 So as input size is the length of the number of characters„ÄÇ it's 100„ÄÇ

So the output size would be also 100„ÄÇ And in betweenÔºå we have the embedding and hidden dimension„ÄÇ

I'm using atom„ÄÇJust simple„ÄÇHere's an evaluation function„ÄÇ

 Let me get to the evaluation function in a second„ÄÇ Let me first„ÄÇBundus„ÄÇThenÔºå run this„ÄÇ

Can already execute this„ÄÇ But I will talk about this after„ÄÇ

 after talking about the main training loopÔºå because the evaluation function here„ÄÇIt's actually used„ÄÇ

Resuced on„ÄÇHereÔºå so it's just one tiny part of the training function„ÄÇ

So let's talk about the big picture training function first„ÄÇ So I'm just iterating 5000 times„ÄÇ

5000 steps„ÄÇThenÔºå I have„ÄÇMy initialization here„ÄÇ So this is for initializing my zero state„ÄÇ

 So this is for initializing here these initial states„ÄÇ I can actually make this biggerÔºå right„ÄÇ

So this is for initializing these zero states here„ÄÇThis for each iteration„ÄÇ

 So each so each iteration will go through one text portion of size 200„ÄÇ So for each text portion„ÄÇ

 we initialize that as0Ôºå this is our beginning of the text„ÄÇWhat we do is we set the loss to 0„ÄÇ

Draw a random sample„ÄÇSo againÔºå the random sample will be like thisÔºå just some text„ÄÇ

 some random text portion„ÄÇAnd the target shifted by one value here„ÄÇWe put it on the GPU„ÄÇIn our case„ÄÇ

 nothing happens because we use the CPU„ÄÇThen for each character in the text portion size„ÄÇ

This is where we do the step putting it through the model„ÄÇ So here„ÄÇ

 this is just for making the dimensional nuityÔºå rightÔºå because this is„ÄÇJust one single value„ÄÇ

 But as you recallÔºå we want„ÄÇOne times sorryÔºå batch size times 1„ÄÇ So it should be a 2 d tensor„ÄÇ

 So we are doing unsqueed adding an empty dimension„ÄÇWe provide the hidden from here„ÄÇ

 the cell state from hereÔºå these initial cell states„ÄÇ So in the first round„ÄÇ

 these will be the initial ones„ÄÇ But then we are also outputting themÔºå right„ÄÇ

 So it will feed by right back„ÄÇ So in an acceleration„ÄÇ

These will be used from the previous iterations that will feed right backÔºå and we are„ÄÇ

Computing the lossÔºå we are just adding the loss here„ÄÇ

So we are computing the loss between the outputs and the targets one at a time„ÄÇ

And then we just normalize by the text portion size„ÄÇ So we haveÔºå we haveÔºå it's just the average„ÄÇ

 the mean„ÄÇMean over the batch sizeÔºå if you willÔºå because„ÄÇWe addÔºå let's sayÔºå200 losses„ÄÇ

 and then we divide by 200 just averaging„ÄÇ it's„ÄÇJust so that„ÄÇ

Works better with the learning rate so we can change the takes portion sizes and shouldn't have to worry about changing the learning rate„ÄÇ

Then we call backwardÔºå make a stepÔºå an update stepÔºå and that is it„ÄÇ

 So here we just have some logging„ÄÇIn some more loggingÔºå it will create a P D F with a plot„ÄÇ

 the loss plot so we can take a look at it during training„ÄÇThere's also a tool called Tensor board„ÄÇ

 which is actually pretty usefulÔºå but we already have so many things to talk about and code examples and everything I don't want to make it more complicated„ÄÇ

 so we are just using matpllip here„ÄÇBut yeahÔºå the in one last interesting part before I run this is the evaluation function„ÄÇ

 So instead of just printing out the loss„ÄÇ in addition to thatÔºå I'm also evaluating the model„ÄÇ

 So what do I mean by thatÔºå I'm letting it generate text„ÄÇ So let's take a look„ÄÇ

This is my evaluation function„ÄÇSo„ÅÜ„ÄÇWe initialize it to the zero stateÔºå then we„ÄÇ

Build up some hidden state„ÄÇ We are priming thisÔºå essentially„ÄÇ

So what that means is we are providing some prime„ÄÇCharacter„ÄÇ

 this is like some starting character here„ÄÇ placeholder A„ÄÇ I actually use T H„ÄÇ

 So all texts will start with T H that we are generating„ÄÇ It's justÔºå I mean„ÄÇ

 it's arbitrary could be anything„ÄÇSo all texts will start with TÔºå H„ÄÇAnd then here we are priming it„ÄÇ

 We are building up some hidden states so that the model stabilizes„ÄÇ So we are doing this for„ÄÇ

The letters in the prime range„ÄÇ So we only have two lettersÔºå rightÔºå we can have actually more„ÄÇ

 We can have some real words„ÄÇSpills up a„ÄÇOh„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_17.png)

Sa state„ÄÇ So it's justÔºå it's just essentially running in this caseÔºå through two of theseÔºå rightÔºå1Ôºå2„ÄÇ

 And then we get to this part„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_19.png)

AndÔºå for each„ÄÇIn the prediction lengthÔºå we are generating text of size 100„ÄÇ

We are just running it as before„ÄÇ So againÔºå there's nothing special yet„ÄÇ

 It's just running the model similar to how we run it in the training loop„ÄÇThe new part nowÔºå though„ÄÇ

 is„ÄÇThis part„ÄÇSoÔºå here„ÄÇCould I've also actually written it simpler like this„ÄÇ If it' just a division„ÄÇ

So we are dividing by a temperature„ÄÇ So what is the temperatureÔºå SoÔºå first of all„ÄÇ

Outputs are our logics„ÄÇ So if we go back to our model here„ÄÇThese are just here„ÄÇ

 we don't use any softm because the softm is usually used in a cross entropy function„ÄÇSo„ÄÇ

 here we have„ÄÇJust our loits„ÄÇAnd then we computeÔºå I mean„ÄÇ

 we are not really computing the softm as a normalization factor in the softmÔºå right„ÄÇ

 So we are usually normalizing by the sum of all of these for each class here„ÄÇ We are lazy„ÄÇ

 We don't do that„ÄÇ We just take E to the power of„ÄÇSo essentiallyÔºå it's E to the power of„ÄÇ

TheLos divided by the temperature„ÄÇAnd the higher the temperature is„ÄÇSo maybe the other way around„ÄÇ

 if I have los and I have a small numberÔºå like the temperature is usually a value between 0„ÄÇAnd one„ÄÇ

 andÔºå I should sayÔºå whyÔºå what's the temperature hereÔºå this like„ÄÇ

I think it's inspired by energy based modelsÔºå but which is turn in turn inspired by the Bsman distribution„ÄÇ

But it's essentially„ÄÇHow sharp our distribution is in this case„ÄÇ So if we have a small value here„ÄÇ

 like 01„ÄÇAll the values will become larger„ÄÇYesÔºå we will get larger values„ÄÇ If we have a value1„ÄÇ1 1„ÄÇ0„ÄÇ

 they will be softer„ÄÇSo„ÄÇWe have here that a real main interesting part„ÄÇ

 We will here have this multinoial functionÔºå which is essentially a function that randomly samples from a multinoial distribution„ÄÇ

Using theseÔºå you can think of them as weights or probabilities and„ÄÇThis is on„ÄÇ

The number of samples we draw„ÄÇSo here we are drawing one„ÄÇone„ÄÇCharacter„ÄÇFrom the predictions„ÄÇ

 So outputs will be the lodges for all 100 charactersÔºå because we have„ÄÇÂì¶„ÄÇ1 are different characters„ÄÇ

 So logics will be„ÄÇÂì¶„ÄÇValuesÔºå if we have soft nextÔºå these would be all the probabilities corresponding„ÄÇ

Corresponding to these characters„ÄÇAnd here we are randomly samplingÔºå so„ÄÇIf we so we expect„ÄÇ

 expectation would be sampling„ÄÇThe characterÔºå although we would most often sample the character corresponding to the highest probability„ÄÇ

And„ÄÇWe can soften this by having a 1„ÄÇ0„ÄÇ This is a standard setting temperature of 1„ÄÇ0„ÄÇ

 If if we use no temperatureÔºå this would be or regular valuesÔºå But if we use a smaller value„ÄÇ

 let's say„ÄÇ1„ÄÇ this will be a sharper distribution„ÄÇ So the most„ÄÇ

The character with the highest value will be sampled more often„ÄÇ So even more oftenÔºå so„ÄÇ

We can actually set the temperature veryÔºå very small„ÄÇ

And then it will essentially be like not sampling at all„ÄÇ

 It will always sample the character with the highest probabilityity„ÄÇ in that wayÔºå it's„ÄÇ

More like deterministic„ÄÇ So you can think of it like thatÔºå the higher the temperature„ÄÇ

 the more diversity in the output in the generated text you will get„ÄÇ

So if you want to have more diverse textÔºå youÔºå you can lower the temperature„ÄÇ It'sÔºå it's kind of„ÄÇ

 you can increase the temperature„ÄÇ If you want to have no randomnessÔºå you lower the temperature„ÄÇ

 Higher temperature means more diversity„ÄÇ You can think of it also as„ÄÇ

 as kind of like heat in a biological system where youÔºå if you heat up the„ÄÇThe system„ÄÇ

 you have more kinetic energy„ÄÇ Everything is symbolic„ÄÇWiggling around and things like that„ÄÇ

So here we have like  point8„ÄÇ It's like a trade off„ÄÇ but it's a hyper parametermeterÔºå if you have„ÄÇ

Higher temperatureÔºå there will be more mistakes„ÄÇNot also more diverse text„ÄÇ It's like a trade off„ÄÇ

Okay„ÄÇCan play around with that if you like when you do the training„ÄÇ

 but let's just use the ones that I use to begin with„ÄÇ So it's running now„ÄÇ

 it gets a pretty high loss„ÄÇ And this is the initial text„ÄÇ Notice that it starts with a T H„ÄÇ

 But the rest is like Gib gok„ÄÇ It's nothing reasonable in here is' just an arbitrary text„ÄÇBut yeah„ÄÇ

 it already finished the next iteration„ÄÇ Let's take a look at that„ÄÇStill not anything useful„ÄÇ

 You can see it learned somethingÔºå rightÔºå So you can seeÔºå okayÔºå these„ÄÇThis is an actual word here„ÄÇ

So it's learning something„ÄÇ Let's take a look at the next one„ÄÇOhÔºå okayÔºå we have„ÄÇSome real words here„ÄÇ

Kind of learning something„ÄÇÊ≤°Êúâ„ÄÇYou can see students„ÄÇSo againÔºå this is trained on the Covid FAQ„ÄÇWhich„ÄÇ

Its more mostly readable text„ÄÇK„ÄÇüòîÔºåYeahÔºå it's gettingÔºå it's getting a little bit better„ÄÇI mean„ÄÇ

 of courseÔºå this is like not real text„ÄÇ It's not a very sophisticated iron„ÄÇ And it's just one layer„ÄÇ

 very simpleÔºå very small text„ÄÇ AlsoÔºå I meanÔºå it just learned for one minuteÔºå right so„ÄÇ

But you can see that it actually„ÄÇSeems to learn something If you think about it started from this„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_21.png)

Had then went to this„ÄÇ And nowÔºå it's kind of„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_23.png)

Talking about„ÄÇSa a beger„ÄÇBdges app notification system„ÄÇ So it'sÔºå it's kind of learning thingsÔºå right„ÄÇ

Wisconsin„ÄÇCovid-19 testing„ÄÇYeahÔºå it will have to run way longer to get some sensible inputs„ÄÇ Again„ÄÇ

 there are also many hyperparmeter to tune can have more layers„ÄÇ

 You can change the temperature and these types of things„ÄÇTo get better results„ÄÇ But I think„ÄÇ

 feel likeÔºå I meanÔºå given that I'm on the CPU just running it for two minutes„ÄÇ

 it's quite cool that it can„ÄÇüòä„ÄÇ

![](img/49a9862e64b36005a3bc6d464085a858_25.png)

![](img/49a9862e64b36005a3bc6d464085a858_26.png)

YeahÔºå can create text that is not total nonsense„ÄÇ I meanÔºå it's total nonsense nonsenseÔºå to be honest„ÄÇ

 but„ÄÇWe can identify individual words„ÄÇ So that's actually quite cool„ÄÇ OkayÔºå so let me„ÄÇ

 let me stop it right here„ÄÇ was a long video„ÄÇ And in the next videoÔºå we will„ÄÇüòä„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_28.png)

Take a next stepÔºå talking about how we can outfit our ends with the so called attention mechanism„ÄÇ



![](img/49a9862e64b36005a3bc6d464085a858_30.png)