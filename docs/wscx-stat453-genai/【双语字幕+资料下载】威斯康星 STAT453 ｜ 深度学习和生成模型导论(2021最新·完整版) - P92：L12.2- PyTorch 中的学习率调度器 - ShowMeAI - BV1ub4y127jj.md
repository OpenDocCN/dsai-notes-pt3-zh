# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P92ÔºöL12.2- PyTorch ‰∏≠ÁöÑÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Âô® - ShowMeAI - BV1ub4y127jj

YesÔºå in the previous video we discussed what learning rate decay is and why it might be useful„ÄÇ

 Let's now see how we can do this in practice in Pytorch„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_1.png)

So firstÔºå the manual wayÔºå the tedious wayÔºå we can define our own function to do learning rate decay in pyr„ÄÇ

 So here I defined a function called adjust learning rate„ÄÇ

 and this is implementing the exponential decay that we talked about in the last video So what it is doing is it's every 10th epoch So this line may read a little bit counter intuitivetuitively because„ÄÇ

This modulus operator is essentially doing a division and then returning the remainder of that division so„ÄÇ

If I have a number that is not divisible by 10Ôºå it will return the remainder„ÄÇ

And here it's essentially saying if there' is no remainderÔºå then do this„ÄÇ

 So it's essentially doing something every 10 epochs„ÄÇSo if the number is divisible by 10Ôºå do this„ÄÇ

 And what I'm doing here is I'm essentially implementing together„ÄÇExponential decay„ÄÇ

 And I'm applying it to the learning rateÔºå so„ÄÇI implemented this in a way here that looks a little bit complicated„ÄÇ

 that is because there are different optimizers and some optimizers have yet different learning rates for different parameter groups and so forth„ÄÇ

 so this will essentially take care of updating all the learning rates that are relevant„ÄÇAlright„ÄÇ

 so this would be the tedious way„ÄÇ And then you can call this function after every epoch„ÄÇ

 So if you have a follow so far epoch„ÄÇIn„ÄÇRinge„ÄÇApoch or something like that„ÄÇAnd then for each„ÄÇ

 I'm not writing it allÔºå but for each mini batchÔºå essentially„ÄÇ

Then you do your training and you would have this function„ÄÇFor each epochÔºå but not for each mini„ÄÇ

 So would you would put it„ÄÇEssentiallyÔºå hereÔºå like come adjust„ÄÇLearning rate and so forth„ÄÇ

 I can't write down here because I have my color bar here„ÄÇ

 but you would put essentially the the function here adjust learning rate with the optimizers input„ÄÇ

 the epoch numberÔºå the initial learning rate and the decay rate and then it will update every 10th epoch„ÄÇ

 HoweverÔºå this is just the the manual way„ÄÇ So you can see how you could implement it yourself„ÄÇ

 if you have an idea for a new learning rate„ÄÇ But in practiceÔºå if you want to just use an existing„ÄÇ

Common way of doing learning grade decay„ÄÇ You can use one of the build in functions in Pythtorch„ÄÇ

 So for thatÔºå I recommend going to this website„ÄÇ They have a selection of different„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_3.png)

Learning rate schedulers here' is just an example„ÄÇ that is a very generic one„ÄÇ

 So here I chose a screenshot of this particular example because it came with a practical example„ÄÇ

 so I didn't have to retype it„ÄÇSo how this works is essentially that you initialize the scheduler„ÄÇ

 So here it's this a Lada learning rate scheduler„ÄÇ you give it the optimizer and then these values„ÄÇ

And then you iterate over the epochs and then for each epoch you call this dot step essentially„ÄÇ

 and then you have your train function and radiation function that we usually use in this class too„ÄÇ

 So essentially you there are two steps only one step is initializing it„ÄÇ

 and the second step is yeah essentially using it and that's it„ÄÇ

And you can do that with any type of scheduler they have on that website„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_5.png)

So here I'm showing you now how that works with the exponential learning grade schedule„ÄÇ

So here I have first an initialization of a model„ÄÇThen I initialize my optimizer here„ÄÇ

 I'm using a stochastic gradient decent optimizerÔºå and then I'm initializing my scheduler„ÄÇ So here„ÄÇ

 notice I'm giving it„ÄÇThe optimizer as input„ÄÇ

![](img/16a73732b5dc404fdee7ea2860473d02_7.png)

And then I have my main training loop„ÄÇ So 5 epochs set my model into training mode„ÄÇ

 and then I iterate over the mini batches„ÄÇ And this is our„ÄÇTyplicical model trainingÔºå so„ÄÇ

We call the modelÔºå compute the lossÔºå set the gradients to 0Ôºå do back propagation„ÄÇ

Yeah do the gradient So here this is the applying the gradient„ÄÇ So backward„ÄÇ

 we'll compute the gradients here we are doing the gradient update„ÄÇ Here's some logging„ÄÇ

 And then notice after the epoch that's where we call the scheduler step„ÄÇ

 but you can also actually put it up here if you want to„ÄÇ so it doesn't really matter„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_9.png)

AlrightÔºå let's now talk about my favorite way of learning rate decay so I'm not using learning rate decay that often because I usually find that Adam an optimizer that we will be discussing later works pretty well out of the box but yeah sometimes I compare it to SGD and learning rate decay and I find it actually yeah this method works actually best compared to other methods I've tried and yeah I got this from a paper in 2016 on deep residual learning which is the paper that originally proposed the Renet which we will be talking about also yeah next week it's among the most popular convolutional in your networks here„ÄÇ

But yeah here I'm not sure if they proposed this learning rate decay method for the first time„ÄÇ

 but yeah that's at least I got the inspiration for that So what they do is they use regular SGD with a mini batch size of 256 and then they start with a learning rate of 0„ÄÇ

1 which is actually quite large for SGD but yeah they start with 0„ÄÇ

1 and then divide by 10 every time then when the error plateaus so essentially if they do the training and notice that the loss doesn't further go down I mean of course there's some wiggle room so„ÄÇ

If it staysÔºå let's sayÔºå within a certain marginÔºå then they divide the learning rate by 10„ÄÇSo yeah„ÄÇ

 that's the whole trick„ÄÇ And they also actually use„ÄÇWeight decay of 0„ÄÇ0001„ÄÇ

 So weight decay we discussed this in the previous lecture that's essentially L2 regularization and they use a momentum of 0„ÄÇ

9 and momentum I will explain in the next video„ÄÇ

![](img/16a73732b5dc404fdee7ea2860473d02_11.png)

But let me show you how this works in practice in PythtorchÔºå so I prepared a code example„ÄÇ

 so I put it here as scheduler I as an iyth or Jupiter notebook and I will share that on the repository like always so I will include a link„ÄÇ

Another video„ÄÇ And yeahÔºå so what I'm doing here is I'm initializing the multi layer perception from last week where we used batch norm and dropout and I'm now using a S GD with a learning rate of 0„ÄÇ

1„ÄÇ And here this is the„ÄÇLearning rate scheduler from Pyrage that can be used similar to what they proposed on in the previous slide„ÄÇ

 where they have basically the„ÄÇ

![](img/16a73732b5dc404fdee7ea2860473d02_13.png)

Monitoring of the plateauing error and the divide by 10 every time the error plateauus„ÄÇ

 So this is essentially this learning rate scheduler hereÔºå so„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_15.png)

I am providing it with the optimizer and the factor is essentially by how much we divide itÔºå so 0„ÄÇ

1 means times 0„ÄÇ1 and times 0„ÄÇ1 is essentially dividing by 10 like they described in the paper„ÄÇ

 but yeah you can of course change that it's another hyperparmeter„ÄÇThen theres a mode„ÄÇ

 So the mode it really depends on where or what you apply this to„ÄÇ

 So you can apply this to anything you like during trainingÔºå so„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_17.png)

HereÔºå they say„ÄÇErorÔºå it's not quite clear what they mean by error„ÄÇ

 It could be either the loss or the classification error„ÄÇ personallyallyÔºå I tried different things„ÄÇ

 and I find it usually works best if I do this on the error on the validation set or accuracy on the validation set„ÄÇ

 so I usually like to work with accuracy instead of error because I don't know it's more positive„ÄÇ

 It sounds better like having a high accuracy sounds better than having a low error for some reason I don't know I like accuracy„ÄÇ

 It's maybe also because in the context of psychic learn„ÄÇ

 everything is an accuracy and I really like psychic learn„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_19.png)

It's for those who haven't taken 451Ôºå it's a package for traditional machine learning„ÄÇ

 which has a very nice API anyways so„ÄÇHere I have a modified version of my train model function that we used in previous lectures„ÄÇ

 I will show you the relevant parts in the next slide„ÄÇAnd„ÄÇüòîÔºåI'm applying this reduce L or„ÄÇ

L R on plateau method scheduler to the validation set accuracy and„ÄÇEssentially„ÄÇ

 I'm saying I want to maximize the validation at accuracy„ÄÇ So every time it plateaus„ÄÇ

 I want to divide the learning rate by 10„ÄÇ So I'mÔºå I'm saying maxÔºå because for accuracy„ÄÇ

 the higher the better„ÄÇ if I use the mini batchche lawsÔºå I would put„ÄÇMin here„ÄÇRightÔºå so„ÄÇEssentially„ÄÇ

I tried it with many different things„ÄÇTraining set lossÔºå that training set accuracy„ÄÇ

 the validation set accuracyÔºå the validation set loss„ÄÇ

 and from all my experience I find that using it with a validation accuracy happens for me at least to work best„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_21.png)

AlrightÔºå so and here are the relevant parts from this„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_23.png)

Oh„ÄÇ

![](img/16a73732b5dc404fdee7ea2860473d02_25.png)

Train model function„ÄÇ So here I zoomed in„ÄÇ againÔºå you can find it next to this notebook in the Github repository„ÄÇ

 I will include the link at the bottom„ÄÇAnd„ÄÇYeahÔºå essentiallyÔºå we have„ÄÇThe loop over the epoox„ÄÇ

 then the loop over the mini batches here I„ÄÇJust left out all the code because it would be too long otherwise„ÄÇ

 And then I'm just saying if we use it schedule so this can is by default none„ÄÇ but we can„ÄÇ

 of courseÔºå like I showed on the previous slide„ÄÇPviide here the„ÄÇScheduller„ÄÇ

 So if we have a scheduler and it's set to validation accuracy„ÄÇ

 it will essentially just do this step based on„ÄÇThe validation accuracy which we collect during training so I have a validation accuracy I keep track of during training I do this for logging purposes so I can do a plot later on and so here I do it on the latest validations set accuracy if it hasn't improved substantially compared to the previous iterations so I think it's for over three iterations then it will divide the learning rate by a factor of 10 but you can also set„ÄÇ

A number„ÄÇTo includeÔºå likeÔºå how often doesÔºå does it have to stagnate before you divide by 10„ÄÇ

 So there are a lot of options that you may also want to choose„ÄÇ

 But I found just this works well for me„ÄÇ

![](img/16a73732b5dc404fdee7ea2860473d02_27.png)

YeahÔºå related to learning rate schedulers and optimize us„ÄÇ

 I also wanted to say something about saving models in Pytch„ÄÇ

 We haven't talked about saving models in Pytch yet„ÄÇ but yeah„ÄÇ

 it's kind of an important topic right So if you train a model„ÄÇ

 let's say for a certain number of iterations and you want to use it another day for making predictions it's of course yeah important to save that model so you don't have to retrain it from scratch every time right but also sometimes you just want to train it longer„ÄÇ

 let's say you train it for 100 epochs and you find the loss has not quite converged yet let me train it longer So instead of just rerunning the training from scratch you can in that case then just let's say lot your model from let's say a previous round and then continue running it„ÄÇ

So saving a model in Pytrch is actually quite simpleÔºå so there are two ways of doing it„ÄÇ

 but I will only show you the recommended way of doing it„ÄÇ

 so the recommended way is using TorchSa on the model state dictionary„ÄÇ

 so the model state dictionary contains all the model settings and also all the model parameters„ÄÇ

However then also if you use an optimizer that has a stateÔºå for example„ÄÇ

 if you use SGD and change the learning rateÔºå the learning rate would be a state essentially„ÄÇ

 so you can also save that so here you can do optimizer do state di and then it will also save that it will all be saved as I think they are actually pickle files but I'm actually not sure I think they could be Yal files I honestly forgot I just call them do P files pytorch files„ÄÇ

And yeahÔºå these contain the settings for your modelÔºå essentially„ÄÇ

And then if you want to load your modelÔºå you can use„ÄÇAlso you have to do into steps„ÄÇ

 you have to first initialize your model„ÄÇBecause here really you are only„ÄÇ

 youre only saving the parameters of the modelÔºå like the weights and everything and„ÄÇ

Here you can or here youÔºå for instanceÔºå initialize your model„ÄÇ

 It has to be the same model that you saved„ÄÇ And then what you do is you„ÄÇ

Have like this load state dict„ÄÇ So it's easy to remember because this is called state dict here„ÄÇ

 And this is essentially just load state di„ÄÇ And thenÔºå but you have to do an inner torch load here„ÄÇ

 which is loading this pytorrch file„ÄÇ So if you do thatÔºå it will load the model„ÄÇAnd„ÄÇüòî„ÄÇ

It will essentially load the weights„ÄÇ So in this static the weights„ÄÇ

 it will load the weights into that model here„ÄÇAnd that's all there is to itÔºå essentially„ÄÇ

If you have an optimizerÔºå thoughÔºå that you saved and you want to reuse„ÄÇ

 you do the same thing here for the optimizer„ÄÇ For instanceÔºå you just initialize a blank optimizer„ÄÇ

 Let's say S GD optimizer with a model parameters and then„ÄÇYou load the state tick„ÄÇ

 so if you had used a learning rate scheduler that changed the learning rate„ÄÇ

 it will then update the learning rate so you don't have to remember or check manually„ÄÇ

And then yeah also for the schedulerÔºå if you use the exponential„ÄÇSchedulularÔºå it willÔºå of course„ÄÇ

 yeah change over timeÔºå rightÔºå So in that way„ÄÇYou can alsoÔºå then„ÄÇYet„ÄÇ

 Lordless scheduler with its parameterss„ÄÇAlrightÔºå so and that is essentially how you save and load models in Pythtorch„ÄÇ



![](img/16a73732b5dc404fdee7ea2860473d02_29.png)