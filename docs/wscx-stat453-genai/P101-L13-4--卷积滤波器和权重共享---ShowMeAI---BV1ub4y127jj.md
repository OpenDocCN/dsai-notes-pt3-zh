# P101ÔºöL13.4- Âç∑ÁßØÊª§Ê≥¢Âô®ÂíåÊùÉÈáçÂÖ±‰∫´ - ShowMeAI - BV1ub4y127jj

YeahÔºå let's now discuss the concept of the convolutional filters and the weight sharing principle that I briefly mentioned in the previous video in more detail here„ÄÇ

 so basically I'm just stepping through the process of applying a filter a little bit more slowly now„ÄÇ

 so I mentioned in the previous video there's this concept of weight sharing„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_1.png)

So we have a feature detectorÔºå which we also call„ÄÇFilter„ÄÇCronnel„ÄÇ

 I actually like the term feature detector because it kind of describes what it is doing here„ÄÇ

 so we slide this feature detector over the inputs to generate a feature map„ÄÇSo here„ÄÇ

 this is what I'm illustrating in the center here„ÄÇ we haveÔºå for instance„ÄÇ

 here an emnes handwritten digitÔºå for instanceÔºå the digit5 here„ÄÇ

And then we have a three by three feature detector or kernel„ÄÇ and then we are we compute this„ÄÇ

Feature map here„ÄÇAnd„ÄÇWhen we apply„ÄÇThis kernel to a region in or on this image or in this image„ÄÇ

 we call this region in the image the receptive field„ÄÇ

 So these 9 pixels that we are overlaying with this feature detector are called the receptive field„ÄÇ

So I do that and then compute this value here„ÄÇ How is this value computed This is in the feature map„ÄÇ

 So this is computed simply by computing the weighted sum„ÄÇ So the kernel„ÄÇ

You can think of it as a weight matrix„ÄÇLet's make it a little bit bigger„ÄÇ don't have much space here„ÄÇ

 to be honest„ÄÇLet's do it like thisÔºå so„ÄÇThis is my„ÄÇColonel here„ÄÇAnd this is a 3 by31„ÄÇ

 So I have a weight„ÄÇ let's say W 1Ôºå W 2Ôºå W 3„ÄÇW4Ôºå W5„ÄÇ6„ÄÇ7Ôºå8Ôºå9„ÄÇ

And this is overlaid with a receptive field„ÄÇ So the receptive field is the part of the image that lies below here„ÄÇ

 So it is also a3 by3„ÄÇMatrixÔºå these are the pixelsÔºå rightÔºå So in this caseÔºå it's call them x 1„ÄÇ3e„ÄÇ

Up to„ÄÇX 9„ÄÇAnd then we do just a multiplication„ÄÇ So we are waiting„ÄÇ

We are applying a weight to each pixel value hereÔºå and that is how we obtain this part here„ÄÇ

So why this weight sharingÔºå well one advantage of this weight sharing really is that we require fewer permits if we had a feature detector for each a separate one for each region here„ÄÇ

 that would be computationally very expensive„ÄÇI meanÔºå we would have a lot of weight„ÄÇ

 and then we would essentially have a fully connected not lot of fully connected„ÄÇ

 but we are getting closer to a fully connected network then„ÄÇ So in that way„ÄÇ

 that would be pretty expensive or expensive„ÄÇModel and the other one„ÄÇ

 the other reason is there's a idea behind using just the same same filter„ÄÇ

 sliding it over the image„ÄÇ And the idea is essentially that if we have a feature detector that„ÄÇ

 for instanceÔºå can detect edges„ÄÇ I will show you examples later„ÄÇ

 But let's say the feature detector can detect whether there's an edge or something„ÄÇ then„ÄÇ

The same picture at detector is useful in different parts of the image„ÄÇ So let's say„ÄÇ

I have a feature detector and the goal or itÔºå it learns to extract certain edges because that's associated with this number„ÄÇ

 so„ÄÇYou would use then the same feature detector in in several regions„ÄÇ

 there's no need to develop a new feature detector if the goal is to detect edges„ÄÇ

 I will show you in a later video what the network really learns„ÄÇ

 So it's learning certain types of feature detectors and I will show you a few examples of that but„ÄÇ

YeahÔºå just to illustrate now the concept of how we apply this filter„ÄÇ

 so I actually copy and pasteed the same slide and so you can see here what I'm doing is from going from here„ÄÇ

 I move it by one pixel row to the right here right and then I'm moving it to the right and so forth and then you can see here„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_3.png)

I'm computing these pixels one at a time„ÄÇAnd then this is my feature map„ÄÇ

 I can then either apply the sub sampling„ÄÇWhat we also call pooling„ÄÇ

Or I can apply another convolutional operationÔºå for instance„ÄÇ

 I could have another convolutional layer that is another„ÄÇ3 by3„ÄÇ And then„ÄÇProject this into the next„ÄÇ

Feature map and so forth„ÄÇ So by thatÔºå the network is yeah„ÄÇ

 consecutively extracting this feature from an image so„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_5.png)

Going here one more time„ÄÇ So it's going this direction from left to right„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_7.png)

And then when we completed this rowÔºå then we go here to the next rowÔºå and then we go„ÄÇ

Through this again and again till we are finished with this pictureÔºå essentially„ÄÇ

So this was explaining how we use a single feature detector„ÄÇ

 So this is a single feature detector that we slide over the image„ÄÇ So we get one feature map„ÄÇ

And this might be a feature detector that the network learns for detecting edges„ÄÇ

 but there might also be some other feature detector that might be learned for other types of information from the image„ÄÇ

 for exampleÔºå in this caseÔºå there might be a feature detector learned to„ÄÇLet's say to„ÄÇ

 to recognize horizontal edges„ÄÇ So we would have to have a different feature detector„ÄÇ

 a different filter for that„ÄÇ For instanceÔºå if this particular one learns this here„ÄÇ

 another filter may be„ÄÇLet's say we make this a green one„ÄÇIfAnother filter is applied„ÄÇ

 it would also slide the same way„ÄÇTo recognize these horizontal edges„ÄÇ

 So how can we use multiple feature detectors in a convolution network„ÄÇ

 So here's just an illustration of that„ÄÇ It's essentially„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_9.png)

Very simpleÔºå we just apply them independently„ÄÇ so here I'm using multiple feature detectors to create multiple feature maps„ÄÇ

So„ÄÇBut I'm showing here at the top that is exactly the same thing I showed you before where we apply this kernel yet to this image here„ÄÇ

And we had this3 by three weight matrixÔºå which is a yeah filter or kernel„ÄÇNow I have another one„ÄÇ

 the blue one here„ÄÇ It's the same concept„ÄÇ I'm again sliding it over here„ÄÇ

 The only difference is that the weights will be different„ÄÇ

 So the green and the blue one they have different weights„ÄÇ That's the only difference„ÄÇ

 and I can have maybe a third one here„ÄÇ and this yeah third one again has different weights„ÄÇ

 So that way I can obtain three different feature maps by just using three different filters„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_11.png)

NowÔºå one thing I already briefly mentioned in the previous video that's the size before and after applying these convolutions„ÄÇ

 so the convolution is essentially this operation of going from here to here and filling out this feature map that is essentially the convolution„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_13.png)

And what is the size essentially going from hereÔºüTo here„ÄÇ So I showed you in the linenette„ÄÇ They had„ÄÇ

 what was it„ÄÇHonestlyÔºå I forgot„ÄÇ But let's say we have a 30Ôºå32 by 32 input image„ÄÇ

 I think that's what they had in this lineette 5„ÄÇ

![](img/cb11494dad89daa5b2fc6e773c6245c1_15.png)

And then they head a5 by5„ÄÇColonnelÔºå so what would be the output size of thatÔºü

So the output size going from„ÄÇLet'd sayÔºå the handwritten„ÄÇLetter to the feature map„ÄÇ

So this was 32 by 32 and this one„ÄÇApplies 5 by 5„ÄÇColonnelÔºå what would be the output size„ÄÇ

 So the output size would be 28„ÄÇ Why would it be 28„ÄÇ

 So there's actually a formula that you can use to compute the output size„ÄÇSo W is the input width„ÄÇ

 So that's the„ÄÇYeah the width of„ÄÇThe input image„ÄÇThen K is the kernel width„ÄÇ that is„ÄÇSoÔºå let's yeah„ÄÇ

K here width of the kernel„ÄÇ That's 5 in this case„ÄÇ So we have„ÄÇ32„ÄÇ5Ôºå and then padding„ÄÇ

 So what is paddingÔºå I will discuss patting actually in the next lecture„ÄÇ

 because there are already too many concepts„ÄÇTo discuss right nowÔºå one thing at a timeÔºå right„ÄÇ

 So penning is when you would essentially add zeros around your image„ÄÇ We will again„ÄÇ

 discuss this in a„ÄÇNext lecture„ÄÇ So in this caseÔºå you can ignore padding„ÄÇ We don't have padding here„ÄÇ

 And then there is a stride„ÄÇ What does the stride mean„ÄÇ

 The strideite means by how many pixels we move the kernels„ÄÇ

 if we only move the kernel here by one row„ÄÇBy one pixel rowÔºå then we have a stride of one„ÄÇ

 If we move the kernel by two rows„ÄÇ So it's maybe easier to show if I make a new drawing So if„ÄÇ

These are mine„ÄÇMy pixel roses„ÄÇI can„ÄÇHave this Cor link here and like a three by3 kernel„ÄÇ

And then if I move it„ÄÇTo the right by one row„ÄÇI'm hereÔºå but I could have also moved it by„ÄÇ2 rows„ÄÇ

 So that would be a stride of two„ÄÇ So the stride is really by how many rows we move to the right„ÄÇ

 and the same also it's actually columns„ÄÇ sorryÔºå by how many columns we move to the right„ÄÇ

 But the same thing also applies to the rows„ÄÇ How many rows we move to the bottom in each„ÄÇ

When we finished basically processing the column in the next rowÔºå how many„ÄÇRoseÔºå we shift down„ÄÇ

 So for simplicityÔºå we had a convolution with„ÄÇStriide equals1„ÄÇ

 So we were moving only one position to the right and then one position to the bottom„ÄÇSo in our case„ÄÇ

 we had a stride of one„ÄÇSo we can actually use a Str of twoÔºå again„ÄÇ

 that is a topic I will cover in the next lecture and that would essentially reduce the size„ÄÇAnyways„ÄÇ

 now putting everything togetherÔºå32„ÄÇ-5Ôºå divided by1„ÄÇThat is 27 plus 1„ÄÇ That should be 28Ôºå rightÔºü

 So this is why the output image is 28„ÄÇ That happens reallyÔºå because we are„ÄÇWe are cutting off right„ÄÇ

 so we are not allowing anything where the kernel overlaps„ÄÇ

 So when we move that to the right at some pointÔºå it will reach the the edge here and there's no no overlap allowed„ÄÇ

 If we want to allow an overlapÔºå that's where we use the padding„ÄÇ

 which is a topic what we discussed next lecture„ÄÇ But yeah„ÄÇ

 that gives you an idea that there's a smallÔºå change or difference between the input and the output size„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_17.png)

So now what about the number of trainable parameters for the convolutional layer of the kernel essentially„ÄÇ

 so I will actually show you a yeah full code example at the end of the lecture where we will train a convolution network„ÄÇ

 but just for illustration purposes consider here this little snippet where I was initializing a convolutional layer„ÄÇ

 So this is my input image hereÔºå an endlessnes image of size„ÄÇ28 by 28„ÄÇ And there's one„ÄÇChannel„ÄÇ

 because it's gray scaleÔºå rightÔºå So the format here is channel height„ÄÇWith so 1Ôºå28Ôºå28„ÄÇ

So when I initialize this layerÔºå what we have to specify is the number of input channels„ÄÇ

 So the number of input channels in the input to that layer„ÄÇ So in this case„ÄÇ

 it's one that's why I'm specifying one and then I have to specify how many feature maps I want to get out of it right So if I have my kernel then and that produces then these„ÄÇ

Could actually make that larger„ÄÇ It produces these„ÄÇFeature maps at each position„ÄÇ

 So how many feature maps do I wantÔºå So 1Ôºå2„ÄÇThreeÔºå yeah specified that I wanted to have eight and in practice usually we add more and more channels„ÄÇ

 the deeper we go in the network againÔºå I will show you that in the concrete full code example„ÄÇ

Here I want eight output feature mapsÔºå these are essentially like these feature maps„ÄÇ

 just eight of them instead of three and I specify a kernel size of5 by 5 with a stride 1 by one„ÄÇ

 So5 by5 that is always like height and width and also for the stride it's the horizontal and the vertical stride so I just have a stride of one here„ÄÇ

And then when I look at the weight matrix that is associated with this convolutional2 D layer„ÄÇ

 can see„ÄÇThe number of parametersÔºå this is 5 by 5 because the I have a5„ÄÇBy 5„ÄÇCoronnel„ÄÇ

 and then I I have actually„ÄÇ58 of these 5 by 5 kernelsÔºå because I want to produce8 feature maps„ÄÇ

 rightÔºå So I would have 1Ôºå2„ÄÇ3„ÄÇüòîÔºå4Ôºå5Ôºå6Ôºå7„ÄÇ8 of these kernels that I applied to the same input image to create 8 feature maps„ÄÇ

 That's why we have„ÄÇ8ight times 5 times 5 parameters„ÄÇFor the weight matrix„ÄÇ

 But then there's also the bias„ÄÇ So there's one bias for each output„ÄÇ

Fature map that we add to all the values in the output feature map„ÄÇ

 So the number of parameters here is 8 times 1 times 5 times 5 plus 8„ÄÇ

 That's the number of trinable parameters„ÄÇ And actuallyÔºå this is pretty smallÔºå rightÔºü

 So if you consider a fully connected layer that is applied to an emest image„ÄÇFully connected layer„ÄÇ

 This would be 28 times 28„ÄÇPlusÔºå whatever number„ÄÇ So in the next layer we want„ÄÇ

 So if we say the nextÔºå so if we have a fully connected network like this„ÄÇAnd we owe a next layer„ÄÇ

 let's sayÔºå hasÔºå I don't know„ÄÇ10 units only„ÄÇ And the first layer is the input imageÔºå784„ÄÇ

 Then I already have for a fully connected networkÔºå7840 weights„ÄÇ

 That's a much bigger number than this number hereÔºå rightÔºü So in this case„ÄÇActually„ÄÇ

 usually conversion networks are very memory efficient compared to a fully connected network„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_19.png)

AlrightÔºå moving on just a small site nodeÔºå one attractive aspect about convolutional networks is also that they have certain yeah to a certain extent some translation„ÄÇ

 rotation and scale invarianceÔºå but it is not a true invariance„ÄÇ

 so invariance here means that it does matter where the object is in that image where it is located„ÄÇ

 but in fact in reality it does matter a little bit„ÄÇ

Because even though we use the same feature detector„ÄÇ

 so whether the feature detector is applied here or hereÔºå it will compute the same output in a sense„ÄÇ

 the same value because it's a modification between the inputs and the weights„ÄÇ

 but where it puts this value mattersÔºå it depends really on where this object is located in that image„ÄÇ

 so to some extent it's not fully invariant to the position rotation and the scale of that object„ÄÇ

 it's only I would say invariant to a small extent it's called also a variant„ÄÇ

But if you do or if you use a lot of convolutions in successionÔºå if you have a very deep network„ÄÇ

 it will essentially become more and more invariant to the yeah position of the object because it would essentially further„ÄÇ

Let's sayÔºå summarize this information into a smaller„ÄÇInto a smaller region of feature map„ÄÇ

 and then in the smaller feature map later onÔºå this detected object may map into the same position„ÄÇ

whether it's located here or whether it's located here„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_21.png)

AlrightÔºå yeahÔºå lastlyÔºå maybe one more note„ÄÇ what also is helpful for improving the invaris are these max pooling layers„ÄÇ

 NowadaysÔºå honestlyÔºå not all networks use max pooling anymore„ÄÇ

 you can actually get pretty good performance without max pooling„ÄÇ

 some max pooling against this subampling„ÄÇ we will see some architectures or multiple architectures in next week's lecture„ÄÇ

 So there will you will also see some with and some without max pooling„ÄÇ

 but traditionally max pooling there was also operation that was used between each convolution layer to reduce„ÄÇ

The size of the feature maps„ÄÇ SoÔºå for instanceÔºå how max pulling works is it is essentially„ÄÇ

Just retrieving the largest element in the feature map„ÄÇ So if this is„ÄÇ

 let's say you are your feature mapÔºå this whole thing is your feature map„ÄÇ

 and you have a three by3 max pooling„ÄÇÂóØ„ÄÇI would call it max pullinging map or something like that or max pulling on detector„ÄÇ

So if you apply 3 by3 max pooling hereÔºå it willÔºå again„ÄÇ

 similar to the convolution slide over the image„ÄÇHere we have a stride of 3 by 3„ÄÇ

 so it will slide by 3„ÄÇPixels instead of a stride of one and what it will do„ÄÇ

 Usually people use a stride of 2Ôºå3Ôºå4 and things like that„ÄÇ But here for illustration purposes„ÄÇ

 we have a three by3 and what it will do is for each oneÔºå it will retrieve the maximum„ÄÇ

 the highest value„ÄÇ That's why it's called max pooling„ÄÇ So in this case„ÄÇ

 if I look at this first region here this first receptive field„ÄÇ

I will take this 8 because it's the largest value„ÄÇ and I put it into my„ÄÇFeature map„ÄÇ

 and then I move it by three pixels to the right„ÄÇ so I'm now„ÄÇHereÔºå and then again„ÄÇ

 I look at the largest value„ÄÇ It's a 5„ÄÇ I will put it here„ÄÇ And then here in the left corner„ÄÇ

 there's actually a tie„ÄÇ It's a 6 into 6„ÄÇ So largest value is a 6„ÄÇ It will put it here„ÄÇ

 And then here the three„ÄÇAnother version of pooling is mean pooling or average pooling„ÄÇ

 but that is not so commonly used as max poolÔºå max pullinging actually„ÄÇ

Also quite useful because it can help a little bit also with us local invariance„ÄÇ

 because it really here doesn't matter whether the8 is located here or here or here„ÄÇ So whether„ÄÇ

The object isÔºå let's sayÔºå more slightly to the left or to the right here in this image„ÄÇ

 it would be kind of invariant to that„ÄÇ so in that way„ÄÇ

 it can help a little bit with this local invariance„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_23.png)

YeahÔºå one note also that pooling layers usually don't have any learnnable parameters„ÄÇ Of course„ÄÇ

 people are creative„ÄÇ There are actually some learnable pooling layers with parameters„ÄÇ

 but the common ones like min average poolÔºå they don't have any weight parameters or learnnable parameters„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_25.png)

AlrightÔºå so this wasÔºå yeah„ÄÇExplanation of how convolutional filters and the weight sharing concept works in the next video I want to briefly do some nitpicking go over cross correlation versus convolution maybe also helps a little bit again with understanding how these filters work and then we will briefly talk about scene ends and back propagation„ÄÇ



![](img/cb11494dad89daa5b2fc6e773c6245c1_27.png)