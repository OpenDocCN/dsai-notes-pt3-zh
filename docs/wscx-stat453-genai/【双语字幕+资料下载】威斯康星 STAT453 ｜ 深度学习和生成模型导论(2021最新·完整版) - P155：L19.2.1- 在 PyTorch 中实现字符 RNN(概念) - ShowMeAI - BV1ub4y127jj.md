# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P155ï¼šL19.2.1- åœ¨ PyTorch ä¸­å®ç°å­—ç¬¦ RNN(æ¦‚å¿µ) - ShowMeAI - BV1ub4y127jj

All rightã€‚ So we are now slowly getting to our code implementation of the character R and N N Pytorã€‚

 And like I mentioned at the end of the previous videoï¼Œ we will split this into two partsã€‚

 So this video will be on some conceptualã€‚ğŸ˜Šï¼ŒYeahï¼Œ aspects about the implementationã€‚

 some I would say things that help us understanding the implementationã€‚

 And then the next video will be the actual code exampleã€‚

 So there's one thing I wanted to talk aboutã€‚ that's the LSTM classã€‚

 We have used that before when we implemented the R and N for classificationã€‚ But yeahã€‚

 there was a question also on Piazza and I made this ugly drawing trying to better explain the inputs and outputsã€‚

 And I realized this is like one of the really obscure things in Pytorchã€‚

 It's not so obvious what's going on there because yeah there are lots of inputs and outputs to the LSDM And I wanted to yeah explain this again for everyone And spoilerã€‚

 there's also an LSTM cell class that I also wanted to explainã€‚

 especially the difference between the LSTM cell and the LSDM classã€‚



![](img/c36ddb5a550ae3b5fe75c08c784a6c89_1.png)

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_2.png)

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_3.png)

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_4.png)

Pytorchã€‚ But yeah one thing at a timeã€‚ So we used this LSTM class before when we implemented the R and N for classificationã€‚

 and yeahï¼Œ there are lots of things going on with that oneã€‚ So here just looking at the exampleã€‚

 they initialize the LSDMã€‚ So there are three valuesã€‚ if we look up the values here in the usageã€‚

 It's the input sizeï¼Œ the number of expected featuresã€‚

 essentially the hidden size and the number of layersã€‚

 So usually usually but previously we used only one hidden layerã€‚

 but we can also of course use multiple hidden layersã€‚

 I will have a visualization showing you how that looks like in the next slideã€‚



![](img/c36ddb5a550ae3b5fe75c08c784a6c89_6.png)

But we are focusing on thatã€‚ So we haveï¼Œ let's sayï¼Œ initialized our LSTM and thenã€‚Hereã€‚

 it's getting usedï¼Œ and it receivesã€‚2 inputsï¼Œ soã€‚The inputã€‚

 and then the second input is this tuple here consisting of H 0 and C 0ã€‚ So what areã€‚H 0 and C 0ã€‚

 These are our initialã€‚Hidden state and cell state hereã€‚

So here they use random numbers for everythingï¼Œ but usually we initialize the initial states with zerosã€‚

But hereï¼Œ it doesn't really matter hereã€‚ They just want to try or ill try illustrating howã€‚

 how inputs might look like from the dimension perspectiveã€‚ So whatï¼Œ what they essentially look likeã€‚

 what dimensions we needã€‚ In any caseï¼Œ So there are these inputs and also as outputï¼Œ there'sã€‚

An output hereï¼Œ And also a tupleã€‚With what called H N and C N hereï¼Œ where this isã€‚

The hidden state of the lastã€‚Time step and the cell stayed after the last time stepã€‚

So I had this relatively ugly drawing and piazzaã€‚ I actually searched on the internet a little bit to find a better visualization and I found the following here on stack overflowã€‚

 which illustrates this nicely what everything here is soã€‚



![](img/c36ddb5a550ae3b5fe75c08c784a6c89_8.png)

The input hereã€‚Which goes into the R N can yeah really think of it as your data inputã€‚

Where is this wholeã€‚So maybe I should use a different colorã€‚

 This whole thing should represent your LSTMï¼Œ whereã€‚Hereï¼Œ this is the depthï¼Œ the number of layersã€‚

 soã€‚This would beã€‚Layer 1ã€‚Layer 2ï¼Œ andã€‚Yeahï¼Œ3ã€‚ So previously in the code examplesã€‚

 we only used one layerã€‚ But of courseï¼Œ yeahï¼Œ we can have multiple layersã€‚ It's justã€‚

 you can see that it'sã€‚Just one layer on top of each otherã€‚ rightï¼Œ So previouslyï¼Œ we only hadã€‚

In the actual code example in the lectureï¼Œ we only had this this one hidden layerã€‚

 but we also talked about that we can have an arbitrary number of hidden layers similar to how we can have an arbitrary number of layers in a convolutional network or an multilayer perceptronã€‚

Oã€‚So this is our input here at the bottomã€‚Rightï¼Œ and now H 0 and C 0ã€‚ me also use a different colourã€‚

 maybe yellowã€‚ So these areã€‚Here alongã€‚Initial states that go into the Estianã€‚

So here the two is for the number of layersã€‚Okayï¼Œ and thenã€‚The outputsã€‚From the R and Nï¼Œ this isã€‚

Also for each time stepã€‚ So there's one output for each time stepï¼Œ similar to how there's oneã€‚

Input for each time stepã€‚In addition to thatï¼Œ it also returnsã€‚Hereã€‚The hidden state and cell stateã€‚

For the lastã€‚Time stepã€‚So technicallyï¼Œ what we could do is we could haveï¼Œ let's sayï¼Œ another RNã€‚Callã€‚

 let' sayï¼Œ some input sequence 2ã€‚Togetherï¼Œ select we had it here together with now here this H Nã€‚

And seeï¼Œ And if we wanted toï¼Œ for exampleï¼Œ if we want to reuse that partï¼Œ for exampleã€‚Alrightï¼Œ soã€‚

But this yeahï¼Œ this is conceptually what we get outï¼Œ soã€‚This and thisï¼Œ these are our outputsã€‚Andã€‚

This is our input hereã€‚

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_10.png)

Nowï¼Œ the LSTM cell classesã€‚It's kind of like part of the LSTMã€‚ It's a smaller unit like LSTM cell isã€‚

 is only a small unitï¼Œ and we can actually use both either the LSTM or the LSTM cell for implementing the character R and N and I haveã€‚

Use bothï¼Œ and I will also provide you with code examples for bothã€‚ but to be honestã€‚

 I think using the LSDM cell class is a little bit more naturalã€‚Becauseuse the way we computeã€‚

The labelsï¼Œ it's a little bit more easy to do thatã€‚



![](img/c36ddb5a550ae3b5fe75c08c784a6c89_12.png)

And alsoï¼Œ for generating dataã€‚So the LSTM cellï¼Œ how it worksã€‚

 is that it is just representing one small unit of what you have seen beforeã€‚

 so the LSTM cell is naturally only one layer instead of setting the number of layersã€‚

 we would have to stack them on top of each otherã€‚Andã€‚

Using the figure that I showed you before in the red boxï¼Œ this is essentially whatã€‚

Represents the LSDM cellã€‚ So it only receives one characterize the inputã€‚

Then for the one layer of the hiddenã€‚The initial hidden and cell stateï¼Œ and it only outputsã€‚

One output hereã€‚And then the hidden and cell set for the next stateã€‚

 So why that is more useful is in a way for computing the lossï¼Œ essentially that you canã€‚

Get one thing at a timeï¼Œ essentiallyï¼Œ instead of runningã€‚The whole thingï¼Œ andã€‚

If we would use the LSDM classï¼Œ we would have to provide one character at a time as input and it's kind of wasteful doing that soã€‚

Handling wiseï¼Œ I find the LSTM cell a little bit more easy to use for this purposeã€‚ Butï¼Œ of courseã€‚

 you can use bothã€‚ You canã€‚

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_14.png)

You can also for the character earn endï¼Œ for exampleï¼Œ run this character on in hereã€‚Withã€‚ğŸ˜”ï¼ŒOnly oneã€‚

 one input hereï¼Œ multiple layersã€‚ you can also run it like that by only having an input of one inputã€‚



![](img/c36ddb5a550ae3b5fe75c08c784a6c89_16.png)

But if youï¼Œ if we want to do this in the first placeï¼Œ we can also just use the LSDM cell and useã€‚

 use that oneã€‚ It'sï¼Œ it's this smaller unit hereã€‚Essentialï¼Œ if we wasï¼Œ would only use one layerã€‚

 we would thenã€‚Use this output hereï¼Œ put it to a fully connected layer to get the class label predictions and then compute the loss and then go on to the next character and so forthã€‚

And here we are going to do that in the code example in the next videoã€‚

So one more thing we are talking here about the many too many where we haveã€‚

Or we want to create textã€‚ So predicting the next wordã€‚

 if you are interested in another type of many too many architectureï¼Œ for instanceã€‚

 language translation I found they have actually a great tutorial on the Pythtorch website so we are not going to talk about language to language translation because yeahã€‚

 theres already a great tutorial and it would be kind of boring for me just to reproduce this tutorial So maybe if you're interestedã€‚

 you can just check it out and work through it step by step So the next video we willã€‚



![](img/c36ddb5a550ae3b5fe75c08c784a6c89_18.png)

Oopsï¼Œ in the next videoï¼Œ we will focus on the character level R an N generating new textã€‚

 and we will be using this LSDM cell classã€‚

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_20.png)

![](img/c36ddb5a550ae3b5fe75c08c784a6c89_21.png)