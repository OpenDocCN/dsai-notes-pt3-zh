# P77ÔºöL10.5.1- Dropout ËÉåÂêéÁöÑ‰∏ªË¶ÅÊ¶ÇÂøµ - ShowMeAI - BV1ub4y127jj

YeahÔºå noÔºå we are finally getting to the more interesting part of this lectureÔºå talking about dropout„ÄÇ

 Or at least that's what I find a little bit more interesting because it's a method specifically developed for neural networks and not just an extension of generalized linear models„ÄÇ

üòäÔºåSo to keep things a little bit organizedÔºå I split this topic into four sub topics„ÄÇ However„ÄÇ

 don't worryÔºå these will be probably very short videos„ÄÇ So there are not many slides per topic„ÄÇ

 but I thought it's just simpler to organize it this way„ÄÇ So in this first video„ÄÇ

 I want to just give you the brief big picture overview the main concept behind dropod„ÄÇ

 and then we will talk about the co adaptation technique not a technique interpretation of dropod„ÄÇ

 And then we will talk about the ensemble method interpretation„ÄÇ

 And then we will see how we can do that in Pytorch„ÄÇ



![](img/5850120663c2da5ecd72d6a4e41c42ba_1.png)

So for referenceÔºå if you're interestedÔºå you can later also check out these papers„ÄÇ

 They are pretty nicely written and not too complicated„ÄÇSo these two papersÔºå as far as I know„ÄÇ

 are the original papers proposing dropout„ÄÇ So one goes back to 2012„ÄÇ

 It's entitled Iroving neural networks by Preventing Co adaptation of feature detectors„ÄÇ

 So like I mentionedÔºå we will see what this core adaptation interpretation means„ÄÇ later„ÄÇ

 And that's also another„ÄÇInterpretationÔºå it goes back to this second paper Dout„ÄÇ

 a simple way to prevent neural networks from overfitting„ÄÇ

 This is like the more like the ensemble interpretation„ÄÇ But yeah„ÄÇ

 we will talk about this in more detail„ÄÇ I just wanted to give you these references for these methods„ÄÇ

 So if you want to check out more detailsÔºå you can find them here„ÄÇ



![](img/5850120663c2da5ecd72d6a4e41c42ba_3.png)

So what is the main concept behind dropout„ÄÇ So in a nutshellÔºå it's yeah about dropping notes„ÄÇ

 So suppose you have a multi data perception here„ÄÇ That's a very small one„ÄÇ

Essentially it's doing training randomly dropping node„ÄÇ So for exampleÔºå hereÔºå as shown„ÄÇ

 you would randomly drop this node with a certain probability so„ÄÇ

If you say the dropout probability is 0„ÄÇ5 like 50%Ôºå then during training for each note„ÄÇ

 you would say with 50% probabilityÔºå I will just delete it during the forward pass„ÄÇ

And in each training forward passÔºå it will beÔºå yeahÔºå different nodes that will be deleted„ÄÇ

Because it's a probability„ÄÇ So you randomly create different models„ÄÇ So one„ÄÇ

Forward path might delete„ÄÇThis oneÔºå another forward path would have these two deleted„ÄÇ

Another forward past may have„ÄÇThis one deleted and so forth„ÄÇ So why that is useful„ÄÇ

 There are a couple of theoriesÔºå and we will talk about this„ÄÇ

 here' just like the big picture overview how it works„ÄÇ I meanÔºå like what the mainÔºå what„ÄÇ

 what's happening basically„ÄÇ And then why that helps that is a topic for the next video„ÄÇ



![](img/5850120663c2da5ecd72d6a4e41c42ba_5.png)

YeahÔºå now the question isÔºå how can we implement that efficiently„ÄÇ

 So how can we implement a dropout without having to make any major modifications to our neural network architecture like deleting nodes or something like that„ÄÇ

 So how can we just do it conveniently„ÄÇ So one way would be by implementing a beulli sampling procedureÔºü

 So for thatÔºå we can define a drop probability P„ÄÇSo for exampleÔºå this could be 50%„ÄÇWowÔºå let's see„ÄÇ5„ÄÇ

 that's our drop probability„ÄÇ So with that probability for each given node„ÄÇ

 the it's the probability that we drop that node„ÄÇNow„ÄÇWe create a vector here„ÄÇ

 So it's like a random vectorÔºå a random sample from a uniform distribution in the range 0 and 1„ÄÇ

 So this vector will have the same number of elements as the size of the hidden layer that we want to apply dropout to„ÄÇ

 So dropout is only applied to hidden layers„ÄÇ It's not applied to the output layer„ÄÇ

So now let's say we have done that„ÄÇ We haveÔºå let's sayÔºå five units in our hidden layer„ÄÇ

 So we have five numbers between„ÄÇ0 and1„ÄÇI'm just writing down some arbitrary numbers here„ÄÇOops„ÄÇ

Like this„ÄÇAnd„ÄÇNowÔºå that is when we apply basically a benoli sampling where we say for each value in that vector for each value V„ÄÇ

We set this value to 0„ÄÇ if this value is smaller than a drop probabilityÔºå otherwise we set it to1„ÄÇ

So we are modifying this now that it is a binary vector„ÄÇ So this one is smaller than 05„ÄÇ

 So we set it to„ÄÇOneÔºå this oneÔºå ohÔºå sorryÔºå should be0„ÄÇ This one is greater„ÄÇSo we set it to one„ÄÇ

 This one is smaller„ÄÇ so we set it to 0„ÄÇ this one it's on the border„ÄÇ we have defined smaller„ÄÇ

 so let's set it to one„ÄÇAnd then this one would be also„ÄÇ0Ôºå so we have now this binary vector„ÄÇ

And then we can simply model or multiply the binary vector with our activations a„ÄÇ

 So these are the activations in our hidden layer„ÄÇ And then now we have effectively cancelled activations and„ÄÇ

YeahÔºå that's it„ÄÇ That's how we can apply dropout during training„ÄÇ



![](img/5850120663c2da5ecd72d6a4e41c42ba_7.png)

OkayÔºå so but there is one little thing we have to adjust if we want to use our model for inference„ÄÇ

 like making predictions on new data or even computing the test set accuracy„ÄÇSoÔºå yeah„ÄÇ

 the reason is we don't want to have any randomness when we make real world predictionsÔºå right„ÄÇ

 So it would be a little bit awkward if let's say we predict someÔºå let's say credit scores„ÄÇ

 but sometimes random nodes get dropped„ÄÇ So the customer would on different days get different credit scores„ÄÇ

 although nothing else has changed„ÄÇ So that wayÔºå we don't want to have any randomness during testing„ÄÇ

 So the dropboard is only applied during training„ÄÇSo what effectively„ÄÇ

 what it means we don't do any removal of the nodes during testing„ÄÇ However„ÄÇ

 there's one additional thing we have to do when we want to remove the dropping of the notess and that has to do with the scale of the activations„ÄÇ

 So assume I have a multi layer perceptron„ÄÇWith multiple hidden layers„ÄÇ

 let's call that hidden layer H1 and this hidden layer H 2„ÄÇ

 And they are fully connected to each otherÔºå like always„ÄÇ

So now if I drop one of the nodes though in H1 by a dropoutÔºå let's say I do it like this„ÄÇ Well„ÄÇ

 let's say„ÄÇLet's say it like this„ÄÇ Each value here is the same„ÄÇ The weights are all one„ÄÇ

 and the activations are all one just to keep things simple„ÄÇSo„ÄÇIf this is the case„ÄÇ

The activation at that given note here is 1 times 1 plus 1 times 1„ÄÇPlus1 times 1Ôºå it's 3„ÄÇRight„ÄÇ

 so if I have the„ÄÇSome over„ÄÇThe widths„ÄÇAnd„ÄÇActivations from the previous layerÔºå I sum that up„ÄÇ

 the net input„ÄÇ Let's say the net input is 3 because all the weights and all the activations are one just for simplicity now„ÄÇ

If I drop one of these nodes during using dropout„ÄÇThen now I only have two inputs„ÄÇ So the value„ÄÇ

Now is only two instead of threeÔºå because I only have two incoming„ÄÇ

Activations when computing the net input„ÄÇOkayÔºå so that meansÔºå I mean„ÄÇ

 this doesn't have any or is not a problem during trainingÔºå Of courseÔºå I mean„ÄÇ

 this is just like some arbitrary numbers I'm showing you here„ÄÇ

 but now if you don't do any dropout during testing„ÄÇAll the activations will be larger right„ÄÇ

 so during testingÔºå we don't have any dropout„ÄÇSoÔºå the values„ÄÇ

Are actually larger than during training„ÄÇ During trainingÔºå we had a value of let's say2„ÄÇ

 and during testingÔºå we will have a value of 3„ÄÇHowever„ÄÇ

Everything in the network is kind of has learned to work with these smaller values„ÄÇ So the threshold„ÄÇ

 let's sayÔºå in the last layer for the prediction would not really work for„ÄÇ

For these larger values nowÔºå because yeahÔºå we have all now throughout the whole network„ÄÇ

 larger values because we don't drop any node„ÄÇ and that could be awkward„ÄÇ

 So what we do is we just scale these activations„ÄÇ That's like a very simple trick to deal with this problem„ÄÇ

 So let's say„ÄÇWe have a drop probably of 0„ÄÇ5Ôºå and we have an activation of threeÔºå one let's say„ÄÇ

We had two during training„ÄÇBecause this one node got randomly dropped„ÄÇ I mean„ÄÇ

 this is just a simplification„ÄÇ And then doing testing„ÄÇ

 the value here that comes in here is 3 because there is no dropping of any node„ÄÇ

 So we have now this problem that this too large„ÄÇ So how can we do it get it smaller„ÄÇ

 we can just scale it by one minus the drop probability„ÄÇ So in this caseÔºå okay„ÄÇ

 this doesn't make quite sense because this is not an even number„ÄÇLet's„ÄÇ

 let's do it proper properly here„ÄÇSorryÔºå so let's do a network where we have 1Ôºå2Ôºå3Ôºå4 and oneÔºå2„ÄÇ

3 nodes here„ÄÇ same thing applies during trainingÔºå we drop 50%Ôºå so we drop„ÄÇTwo nodes randomly„ÄÇ

 So the value during training we get is2Ôºå but during testing„ÄÇWe don't„ÄÇDrop anything„ÄÇ

 So the value we may get at this note here is no 4„ÄÇSo how do we scale it„ÄÇ

 We can scale it by  one minus the probabilityÔºå for exampleÔºå can have then 4 times 1 minus„ÄÇ

5 which is 4 times 0„ÄÇ5 which is essentially 2 So in this way we scale all the predictions during training testing sorry so that's like a simple trick to deal with these activations that might be too large during testing and yeah this is essentially how dropout works„ÄÇ



![](img/5850120663c2da5ecd72d6a4e41c42ba_9.png)

So in the next videoÔºå I want to talk about the core adaptation interpretation„ÄÇ

 So one way to interpretÔºå drop out why it might work in practice„ÄÇ

 Then there's an alternative interpretation looking at„ÄÇDropout as an ensemble method„ÄÇ

 And then I will show you how we can do dropout in pyrch„ÄÇ

 And there's also a concept for that called inverted dropoutÔºå which will be interesting„ÄÇ

 It's a slight modification of what I just talked about„ÄÇ It's called inverted dropout„ÄÇ

 I will call talk about that when we yeah talk about the implementation„ÄÇ



![](img/5850120663c2da5ecd72d6a4e41c42ba_11.png)