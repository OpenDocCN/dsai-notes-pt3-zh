# P56ÔºöL8.6- Â§öÈ°π Logistic ÂõûÂΩíÔºèSoftmax ÂõûÂΩí - ShowMeAI - BV1ub4y127jj

YeahÔºå let's now generalize the concept of logistic regression to multiple classes„ÄÇ

 This is also known as softmax regression or sometimes also called multiomial logistic regression„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_1.png)

So for this we will consider a relatively simple data setÔºå the M list data set„ÄÇ

 I already introduced this in the introductory lecturesÔºå so these are 60„ÄÇ

00 hundredwritten digits in the training set„ÄÇAnd yeah it's a nice and balanced data set„ÄÇ

 so balanced means that there's the same number of training examples per class„ÄÇ

 so in this case we have 10 classesÔºå the handwritten digits0 to9 and there are 6000 digits per class„ÄÇ

 Here's an example of how these digits look like„ÄÇAnd each of these training examples here„ÄÇ

Has an image dimension of 28 times 28 times 1„ÄÇ So 28 for the height and 28 for the width„ÄÇ

 So this is the number of pixels„ÄÇ So we have in totalÔºå784 pixels or features per training example„ÄÇ

In PytchÔºå we will be using a format so called N C HW„ÄÇ

 which is yeah how we represent a mini batch or an image batch„ÄÇ So here„ÄÇIn this case„ÄÇ

 we have 128 examples in that mini batchÔºå so the 128 is for this n here„ÄÇ

Then we have a colour channel„ÄÇ We have only one colour channel because these are black and white images„ÄÇ

 So there are no redÔºå green and blue colours„ÄÇAnd thenÔºå we have„ÄÇThe heightÔºå so 28 pixels„ÄÇ

And for the width„ÄÇWe have also 28 pixels„ÄÇ This is usually how we read in a data set into Pytorch„ÄÇ

 and we will be also looking at code examples for that„ÄÇSo yeahÔºå just to recap„ÄÇ

 we have 60000 training examples„ÄÇAnd then the corresponding labelsÔºå and then we have„ÄÇ

For the test images and test labels 10000 each„ÄÇ so that's just how the dataset set looks like„ÄÇYeah„ÄÇ

 for softm regressionÔºå we will actually be concatenating„ÄÇThese images„ÄÇ

Or these features in the image to a long vectorÔºå like we discussed in the introductory lecture„ÄÇ

 So yeahÔºå here's just an example of how„ÄÇ

![](img/7bddc4e3a5fe43463115092b77a6bea5_3.png)

Kind of diverse„ÄÇ These handwritings are„ÄÇ So these are all„ÄÇThe letter 7 sorry the number 7„ÄÇ

 So illustrations of different sevens„ÄÇ So different people writing down a 7„ÄÇ

 and you can see it varies a lot„ÄÇ So personally where I grew up in Germany„ÄÇ

 we usually write a 7 like this with this cross line here in the US„ÄÇ

 it's more common to write it like thisÔºå but you can see„ÄÇ

They are also very interesting flavors of thisÔºå for exampleÔºå looks like a C almost here„ÄÇ

 this one is actually interesting because yeah in GermanyÔºå we write a one as follows like this„ÄÇ

 This is a one in Germany in the US one is usually written as this one So but you can see just by looking at these handwritings„ÄÇ

 it's sometimes maybe not even clear what the number is„ÄÇ

 So if I wouldn't tell you that these are different illustrations of7„ÄÇ

 Some people might think these might be once„ÄÇ and yeah„ÄÇ

 this actually I don't even know it could be a C if you have data set of letters in digits„ÄÇ

 for example„ÄÇ

![](img/7bddc4e3a5fe43463115092b77a6bea5_5.png)

So yeahÔºå like I mentioned in the context of Pythtor and when we work with unstructured data„ÄÇ

 also later with convolutional networksÔºå we would be working with the formatÔºå the NCHW„ÄÇAs follows„ÄÇ

 So this would be„ÄÇ4 dimensional tensor„ÄÇThink I introduced that in the first lecture of this course„ÄÇ

 so a long time ago„ÄÇ So yeahÔºå here's how it would look like in Pytorrch„ÄÇ

 where we have this four dimensional tensor„ÄÇÂóØ„ÄÇActuallyÔºå in this caseÔºå its a three dimensional ten„ÄÇ

 so I only represent one single image„ÄÇThat's a single image here at the bottom„ÄÇ

Representing the number threeÔºå and you can see there are different values from 0 to 0„ÄÇ5„ÄÇ

 I think the highest one would be approaching one„ÄÇ and that is because I normalized„ÄÇ

These pixels by dividing them by 255„ÄÇ And this is becauseÔºå yeah„ÄÇ

 these values between or around 0 or in this caseÔºå between 0 and1 work better with gradient in decent„ÄÇ

 it would be even betterÔºå for exampleÔºå to normalize these images such that they are centered at 0 so you can„ÄÇ

 for exampleÔºå yeahÔºå subtractor 0„ÄÇ5„ÄÇ If you have the range between 0 and1Ôºå you can subtractor 0„ÄÇ

5 and then will be ranging between„ÄÇinus„ÄÇ52„ÄÇ5And this would work even better„ÄÇ

 what people say that would work better with gradient in descent because now it's centered at0 and you can have positive and negative inputs equally„ÄÇ

 HoweverÔºå in practiceÔºå personallyÔºå I for these simple dataset setsÔºå I don't notice a big difference„ÄÇ

 especially if we work with logistic regression„ÄÇ Sometimes I put more effort into normalizing the input images„ÄÇ

 for exampleÔºå doing the standard normal standardization the„ÄÇWhat people call the st normal„ÄÇData„ÄÇ

 So it's giving your data the properties of a standard normal distribution„ÄÇ ButÔºå of course„ÄÇ

 if your data is not normally distributed from begin with„ÄÇ

 it will also not be normally distributed after normalizing„ÄÇ So what I mean is„ÄÇFor example„ÄÇ

 if you haveÔºå it's called that X primeÔºå if you have a given feature„ÄÇX„ÄÇYou subjecttract„ÄÇ

The mean of the feature and then divided by the standard deviation„ÄÇ

 So this would be called standardization„ÄÇWell some people just call that ZCO normalization„ÄÇÂóØ„ÄÇ

This is also recommendedÔºå we can do that here tooÔºå but since the images of the features„ÄÇ

 sorry the features are all equalÔºå they are all from 0 to 25 255„ÄÇFor pixels„ÄÇ

 it wouldn't make a big difference with„ÄÇWe use yeah„ÄÇ

 this standardization or just scale them between the range minus 0„ÄÇ5 and „ÄÇ5„ÄÇ

 So that's like a minor detail„ÄÇ

![](img/7bddc4e3a5fe43463115092b77a6bea5_7.png)

AlrightÔºå so but I am getting sidetracked here what I wanted to mention is the main point here is what we are doing now is we are concatenating„ÄÇ

 So if I go back we are concatenating the rows so„ÄÇ

![](img/7bddc4e3a5fe43463115092b77a6bea5_9.png)

If you consider„ÄÇFortunatelyÔºå there's a line wrapping here if you consider this here as the first row„ÄÇ

Then I can maybe write this down first all„ÄÇOs„ÄÇFirst„ÄÇüòîÔºåRu„ÄÇüòîÔºåThen„ÄÇüòîÔºåSecond„ÄÇRu„ÄÇüòîÔºåThird„ÄÇüòîÔºåR„ÄÇüòî„ÄÇ

And so forthÔºå if we concatenate these rowsÔºå we get a one long vector here„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_11.png)

That's how it looks like„ÄÇ So here I've concateated these rows„ÄÇ just to illustrate this again„ÄÇ

 Let me jump back one more time„ÄÇ So here in greenÔºå this would be really the first row on in this image here in blue„ÄÇ

 this would be„ÄÇ

![](img/7bddc4e3a5fe43463115092b77a6bea5_13.png)

Second row and the third row here in red would be this one„ÄÇ if we concatenate them by thatÔºå I mean„ÄÇ

 if we make one long vector„ÄÇ So the green one firstÔºå then„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_15.png)

The blue 1„ÄÇAnd then the red oneÔºå we can use that as input for our logistic regression model„ÄÇ

 And in the case of the Ms data setÔºå we have more than two classes„ÄÇ

 So we are going to use softmax regression„ÄÇ YeahÔºå to just briefly recap on before I introduce the softmax model here is a drawing of the regular logistic regression model for the binary classification case„ÄÇ

 So as inputÔºå we have a feature vector„ÄÇAnd then we have this weight vector„ÄÇ

 So we compute the dot product between the two at the bias unitÔºå which gives us the net input„ÄÇ

 and then we put the net input through a non nonlinear activation functionÔºå the logistic„ÄÇ

Sick might function„ÄÇWhich we defined as follows„ÄÇAnd„ÄÇThen if so what it represents is the probability„ÄÇ

So this present represents the probability that a training example„ÄÇBelongs to„ÄÇClas„ÄÇOne„ÄÇ

 if we wanted to compute the probability that it belongs to class 0Ôºå we canÔºå yeahÔºå we could„ÄÇ

We could compute it as„ÄÇOops„ÄÇ1 minus thisÔºå right„ÄÇ1 minus this probability„ÄÇ This would give us„ÄÇ

The probability that it belongs to„ÄÇClass 0„ÄÇBut back to this one here now„ÄÇ

 So we have a probability and„ÄÇIt belongs to class1„ÄÇ

 and then we can say if this probability is greater„ÄÇF P is greater than „ÄÇ5 than class„ÄÇone„ÄÇAnd„ÄÇ

If it's smallerÔºå then„ÄÇPot5„ÄÇClass„ÄÇ0„ÄÇSo how can we generalize this now to multiple classes so for that we can actually extend this model„ÄÇ

 so here is a drawing of a model that would work with a multiclass classification data where we have more than two classes so just to jump back one more time„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_17.png)

![](img/7bddc4e3a5fe43463115092b77a6bea5_18.png)

So this model here that I'm circling in greenÔºå this can actually be thought of as a model that is embedded in this bigger model here„ÄÇ

 So if we have the same inputs hereÔºå it's the same as before„ÄÇ

 our feature vector of dimensionality T M„ÄÇ

![](img/7bddc4e3a5fe43463115092b77a6bea5_20.png)

We can think of„ÄÇEverything I'm drawing now in green„ÄÇAss the same model that I showed you before„ÄÇ

 rightÔºå So if you look at thisÔºå it's the same model„ÄÇ I mean„ÄÇ

 it's of course a little bit skewed because these lines are a little bit different„ÄÇ

 but it's essentially the same model„ÄÇ And so this would be one logistic regression model„ÄÇ

 And for that oneÔºå we have a feature vector„ÄÇOf dimensionality M„ÄÇ so we can have the feature vector„ÄÇ

 Let's write it one„ÄÇ1„ÄÇOops„ÄÇit like thisÔºå1Ôºå1„ÄÇ1Ôºå2Ôºå and„ÄÇWe„ÄÇüòîÔºåLet's say three if we have three classes„ÄÇ

SoÔºå if we have three featuresÔºå let's do M to a bit more general„ÄÇ So if we have M features„ÄÇ

 we have this feature vector here„ÄÇAnd noticeÔºå noÔºå I meanÔºå we have„ÄÇ

Multiple of these logistic regression models„ÄÇ The second model would be this one hereÔºå right„ÄÇ

 is's another logistic regression modelÔºå same inputs„ÄÇAnd for this one„ÄÇ

 we can also have a feature vector for this oneÔºå we could have the feature vector„ÄÇ2Ôºå1„ÄÇ2Ôºå2 and2 m„ÄÇ

And then there's a third model hereÔºå rightÔºüSo it's a third model„ÄÇAnd for that„ÄÇ

 we also have a feature vector„ÄÇ we can write it as on W3Ôºå1Ôºå W23Ôºå2 and W 3Ôºå3„ÄÇSo if I call this„ÄÇ

Feature vectorÔºå let's say let's call it W1„ÄÇAnd this is„ÄÇW2„ÄÇAnd„ÄÇüòîÔºåThis is„ÄÇW 3„ÄÇRight„ÄÇ

 so I have now three feature vectors„ÄÇ So if I„ÄÇStack them„ÄÇ I can stack them to make a feature matrix„ÄÇ

 So this matrix here„ÄÇ so I can write this matrix as„ÄÇI think yeahÔºå I would need to transpose this„ÄÇüòî„ÄÇ

So„ÄÇWe would have W1„ÄÇTranspose„ÄÇW 2„ÄÇTpose and„ÄÇview3 transport„ÄÇ

So this would be my feature matrix of dimensionity H times M„ÄÇ

 where H is the number of output nodes here„ÄÇSo this would be one way I could generalize my logistic regression model„ÄÇ

Then we can yeah compute also the class membership probabilities here„ÄÇ

 However notice now the class membership probabilities work a little bit differently so because we don't have a binary classification problem now„ÄÇ

 but each node is still binary„ÄÇ So here let's say green„ÄÇ

 this would be the probability that this is class 1„ÄÇIn blue„ÄÇ

 this is that probability that is class 2„ÄÇBut now each probability works like this„ÄÇ

That it is the probability that this is the class„ÄÇ So the probability that is class 3 versus not being class 3 so„ÄÇ

What can happen is that or what will probably likely happen is that these probabilities don't sum up to one„ÄÇ

 SoÔºå for exampleÔºå for a given inputÔºå if we have the digit 3Ôºå we may get a„ÄÇProbability that this is„ÄÇ

Pot3Ôºå wellÔºå let's say„ÄÇ33%„ÄÇBut this is in class  one„ÄÇThat's a„ÄÇ40%„ÄÇThat this is class 2Ôºå and then„ÄÇ

70% that this is class 3„ÄÇSo„ÄÇBut yeahÔºå these don't sum up to one„ÄÇ SoÔºå for example„ÄÇ

 if the probability is 40% as is class 2Ôºå the probability that this is not class2 would be 60%„ÄÇ

And for this oneÔºå for the first oneÔºå the probability that it would be class„ÄÇ3 would be 67%„ÄÇ

In any caseÔºå these probabilities down some up to one„ÄÇ if we also take a look at this one„ÄÇ

 these down some up to oneÔºå it can be a little bit weird„ÄÇ So if we have a number„ÄÇ

 let's say the digit„ÄÇ3Ôºå how can it be 33%ÔºüNumber oneÔºå40%Ôºå number two and 70% probability number3„ÄÇ

 it doesn't really„ÄÇMake intuitive sense„ÄÇI think„ÄÇThis would make sense if we have a data set when the classes are not mutually exclusive„ÄÇ

 for exampleÔºå if you have something like a vehicle„ÄÇ

 there's a higher probability that an object in an image is a vehicle there's maybe also probability greater than 50% that this is a car and then maybe a probability„ÄÇ

 a certain probability that iss a certain brand of a car and so forth„ÄÇ

 So where we don't have mutually exclusive classes„ÄÇ

 but in the case of digits it would be more intuitive if these probabilitybil sum up to one and for that we have this multinoial logistic regression model or softmax regression model„ÄÇ

 So what I showed you in the previous case was more like a concatenation of individual logistic regression models here would be the real multinoial logistic regression model where we assume„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_22.png)

That the classes are mutually exclusive„ÄÇ So what is the difference between this slide and the previous slide„ÄÇ

 So if I go back one more time in the previous slide„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_24.png)

It's hard to see nowÔºå but we had one sigmoid activation in each node here„ÄÇ

 So I'm getting rid now of this sigmoid activation„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_26.png)

And have a so called„ÄÇSoft max activation„ÄÇ And also notice that I'm drawing it a little bit differently before I was drawing it inside here because they were all independent from each other now„ÄÇ

The softmax activation is actually not independent for each node„ÄÇ

 So all the activation go into the same softmax activation„ÄÇAnd then„ÄÇTheres some computation going on„ÄÇ

 I will explain to you how that works in the next video and what comes out are these„ÄÇ

Activations that now sum up to one„ÄÇ So these sum up to one„ÄÇ SoÔºå for example„ÄÇ

 the first one may be 10% probability that it belongs to class 1„ÄÇThen that might be 20%„ÄÇ

For class 2 and„ÄÇ70% for class 3 and so forth„ÄÇAnd how can we then convert it into a class table„ÄÇ

 We would take the arc mix„ÄÇ So the arc mix is„ÄÇThe positionÔºå the index position of the highest value„ÄÇ

 If I scan these three positionsÔºå let's say position„ÄÇ1Ôºå position 2„ÄÇAnd position 3Ôºå the highest index„ÄÇ

70%„ÄÇ This would be 3„ÄÇ So this would return class A 3 if I apply the Archs„ÄÇ

If this is unclear we will walk through this step by step in the next video so this was just the introduction of how the softmax regression model looks like like it's basically here the concatetnation of individual logistic regression models and in the next videos we will talk about softmax and this arcmax and how we also compute the loss function„ÄÇ



![](img/7bddc4e3a5fe43463115092b77a6bea5_28.png)