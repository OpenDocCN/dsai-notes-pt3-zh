# P24ÔºöL3.5- ÊÑüÁü•Âô®ËÉåÂêéÁöÑÂá†‰ΩïÁõ¥Ëßâ - ShowMeAI - BV1ub4y127jj

YeahÔºå as the last video for this weekÔºå I want to briefly talk about the geometric intuition behind the perceptron„ÄÇ

 So this isÔºå I thinkÔºå particularly interesting because the perceptron rule is so simple„ÄÇ And yeah„ÄÇ

 here we will briefly go over why it actually works at all„ÄÇSo if you recall„ÄÇ

 this is the perceptual learning algorithm„ÄÇThe slide is just copied from again earlier video„ÄÇ

 So it's just for recapping what's going on„ÄÇ So if the prediction is correct„ÄÇ

 nothing happens so we don't need to update the decision boundary„ÄÇ HoweverÔºå if it's incorrect„ÄÇ

 there are two scenariosÔºå scenario A and scenario B„ÄÇ So scenario A is if the output„ÄÇ

 the prediction is 0 classable 0Ôºå while the target is classable1„ÄÇ

And scenario B is if the prediction is one and the class labelÔºå the actual one is 0„ÄÇ

 So in both casesÔºå we make a mistakeÔºå and then we have to update the perceptron„ÄÇ

 So in the first case in this caseÔºå how we update the decision boundary is by„ÄÇEing„ÄÇ

The input vector so the feature vector to the weight vector„ÄÇ

 So why do we have to add the input vector to the weight vector and why does it help at allÔºü

 So in this videoÔºå I want to briefly go over the intuition behind that„ÄÇ

So here also yeah encode just again the outline of the learning algorithm if it's more cleared this way„ÄÇ

 so we initialize the weights with all zeros or we can also use all small random numbers doesn't really matter and then for every training at we iterate over the training examples and then we compute the prediction and then yeah update if we make a mistake so here we compute thearrow and then update so that is the basic concept behind the perceptionceptron that we talked about before„ÄÇ

Now regarding the geometric intuitionÔºå so imagine you have these two classes here on class 0 and class 1„ÄÇ

 and here that's the black line here is the decision boundary„ÄÇ

So the weight vector turns out to be perpendicular to the decision boundary„ÄÇ Can you think ofÔºå yeah„ÄÇ

 the reason why the weight vector would be perpendicular to the decision boundary„ÄÇ

 So perpendicular here means 90 degree angle„ÄÇ Why is there a 19 degree angle between the weight vector and the decision boundary„ÄÇ

 maybe pass the video for a few minutes and think about it a little bit„ÄÇ

 Maybe you yeah get the idea if not it don't worry it's a little bit„ÄÇ

 I would say it's not immediately obvious„ÄÇAlrightÔºå so the intuition behind it is why the decision boundary in the weight vector have this 90 degree angle is because if you think of our threshold if we„ÄÇ

Compute or look at the dot product„ÄÇ Let's ignore the bias for now„ÄÇ

 if we just look at the dot product of the weights and the feature vectorÔºå we make a decision„ÄÇIt 0„ÄÇ

 So whether it's greater or smaller than 0„ÄÇSo if it's small or equal to 0Ôºå the net input so„ÄÇÂóØ„ÄÇ

They didn't put it„ÄÇIf it's smaller or equal to 0Ôºå we predict or return classable 1„ÄÇ

 If it's greater than 0Ôºå we return 1„ÄÇ So everything kind of hinges upon the0 here„ÄÇ

 And if we write the dot product a little bit differently as the length„ÄÇ

Or magnitudes of the two vectors„ÄÇAnd then the angle between themÔºå the cosine of the angle„ÄÇ

 and we have the following equation you probably have seen that from bienia algebra classes so„ÄÇ

In order„ÄÇSo in order now to have the stock product0„ÄÇ like if we want to have it 0„ÄÇ Yeah„ÄÇ

 what we would have is to have at least„ÄÇOne of these two terms0„ÄÇ But this is not the caseÔºå right„ÄÇ

 becauseÔºå yeahÔºå the vector is not 0 unless it's allÔºå values are 0Ôºå but„ÄÇYeahÔºå so in this case„ÄÇ

 that would be one scenario where we might have a case of0„ÄÇ

 another case is where the decision boundary„ÄÇSoÔºå the„ÄÇ

Angle here between the weight vector and the decision boundary„ÄÇ

 When this term here becomes 0 and when is this term 0Ôºå the term is a 0„ÄÇ

 If we have a 90 degree angle here between the twoÔºå rightÔºå because the cosine„ÄÇOf 90 degree is 0„ÄÇ

 So by setting this part here this part to 0Ôºå we get a weight vector that actually has some length„ÄÇ

And that has then angle between the decision boundary and the weight vector of 90 degree„ÄÇYes„ÄÇ

 so what else does that meanÔºå So if we think about„ÄÇThe 90 degrees a little bit more„ÄÇ

 So what we can deduce then is that there will be an„ÄÇAngleÔºå smaller than 90 degree on this„ÄÇ

Site here of the decision boundaryÔºå so„ÄÇBy thatÔºå I mean if you have a training example or a data point that is somewhere yeah on the right side of the decision boundary„ÄÇ

 let's say it can be here„ÄÇSo if we have a feature vector that is hereÔºå can also be„ÄÇ

Here doesn't really matter„ÄÇ So anywhere where we have a decision„ÄÇYesÔºå so what else does this mean„ÄÇ

 So let's focus in a little bit more on the 90 degrees here that we just talked about„ÄÇ

 So now every input vector on this side here on the right hand side will have an angle with a weight vector that is smaller than 90 degrees„ÄÇ

 So againÔºå in blueÔºå this is still„ÄÇWait„ÄÇüòîÔºåViectctor here„ÄÇ

And now if I take a training example and put it anywhere on the right side of the decision boundary„ÄÇ

 for example hereÔºå So if we look at this oneÔºå it has this angle that is smaller than 90 degree„ÄÇ

 if we put it hereÔºå same thing is true„ÄÇ or if we put it here outside„ÄÇ

 maybe it doesn't really matter AlsoÔºå we have always this 90 degree angle„ÄÇ And also„ÄÇ

 you note that it doesn't really matter how large the weight vector is if we scale the late weight vector by a vector of 100 or 1000„ÄÇ

 it doesn't matterÔºå it will be„ÄÇStill then the angle„ÄÇ

 because whether this is longer or not doesn't really matter„ÄÇ

 the same is true for the training examples„ÄÇ if they are super large„ÄÇ

 if let's say one training example is somewhere here„ÄÇIt's still a 90 degree angle or smallerÔºå sorry„ÄÇ

 smaller than 90 degree angle„ÄÇ So the magnitudes of the weights and features doesn't really matter whether we scale them or not„ÄÇ

 however„ÄÇThe updating the learning can be a little bit faster if they are scaled„ÄÇ

Which is why I had the standardization in in the code example„ÄÇ But yeah„ÄÇ

 so there's one takeaway that the angle is smaller than 90 degree on this side„ÄÇ

 And on the left side hereÔºå it would be of courseÔºå yeah greater than 90 degree if we have a value here„ÄÇ

 that would be greater than 90 degree„ÄÇ So everything again hinges upon the 90 degrees here„ÄÇNow„ÄÇ

 why is that again interestingÔºå So let's now focus on the actual prediction case before I just laid out where„ÄÇ

 yeahÔºå how the data set and the weight vector are oriented„ÄÇ Now„ÄÇ

 let's take a look at a prediction case„ÄÇ So we have a correct side and a wrong sideÔºå so„ÄÇ

Let's assume we have an input vector with class table 1„ÄÇAnd like I've shown you before„ÄÇ

 a weight vector for the correct prediction must be somewhere such that the angle is smaller than 90 degrees to make a correct prediction„ÄÇ

 Let me go back to just show you again what I meanÔºå so„ÄÇNotice here we have all the ones on this side„ÄÇ

 and this is our„ÄÇWeight vector here„ÄÇ So to make a correct prediction that is predicting a new training example like this one as class 1„ÄÇ

 it has to be smaller than 90 degree„ÄÇ It has to be in within the circle here right„ÄÇ

 so that's what we are looking at now„ÄÇ So we have here on this correct side„ÄÇ

 So if you think of this as the decision boundary between the wrong and the correct side„ÄÇ

 we have to have an angle between the input vector and the weight vector that is smaller than 90 degrees„ÄÇ

So if we are within the 90 degreesÔºå yeahÔºå then we make a correct prediction because the dot product then will be positive greater than0 because„ÄÇ

YeahÔºå because this term will be greater than0„ÄÇYeah„ÄÇ

 now let's take a look at what happens if we make a wrong prediction„ÄÇSo here in red„ÄÇ

 I have now a weight vector that is not ideal„ÄÇ so that is a weight vector where when we make a prediction„ÄÇ

 where we make a wrong prediction because„ÄÇThe angle is not smaller than 90 degrees„ÄÇ It'sÔºå in fact„ÄÇ

 larger than 90 degrees now„ÄÇ So if it's between a value between„ÄÇLarger than 90 up to 270„ÄÇ

 we will make a wrong prediction because now the value and the cosine of that angle larger than 90 and smaller than 270 would be negative„ÄÇ

So we would be smaller than0„ÄÇ so we would make class  zero predictionÔºå which is what happens here„ÄÇ

So we have an angle that is greater than 190 degrees„ÄÇ

 and then we would predict the wrong label we would predict„ÄÇClass level 0„ÄÇ

 although input vector here„ÄÇWe assume that it still has class A1„ÄÇHow do we make this correctÔºå then„ÄÇ

 How can we correct this prediction That is by moving the weight vector to the side of yeah„ÄÇ

 closer to the„ÄÇside of the training example so that the angle is smaller than 90 degree„ÄÇ

 So how do we do thatÔºå So in the perceptionron algorithm„ÄÇ

 how we do that is we add the feature vector„ÄÇ So the input vector we added„ÄÇTo the weight back„ÄÇ

So here the dash line hereÔºå that is„ÄÇThe input vector„ÄÇSo here that's the same input vector„ÄÇ

 I'm just taking the input vector and adding it to my weight vector„ÄÇ So maybe doing it step by step„ÄÇ

 So what we have is we have our input vector and we have our wrong weight vector„ÄÇ

 What I'm going to do now is I'm taking„ÄÇThis input vector„ÄÇ And I'm adding it to the weight vector„ÄÇ

Like this„ÄÇ And then my new„ÄÇW directoror will be like thisÔºå right„ÄÇ

 So then we will have this smaller than 90 degree angleÔºå which is shown here„ÄÇ

 So by adding the weight vectorÔºå we can yeah make the decision boundaryÔºå correct for this case„ÄÇ

So this isÔºå if I go back hereÔºå this is exactly what happens in scenario a„ÄÇ

 So we predict the output is 0Ôºå although the target is one„ÄÇ So this is when we„ÄÇIf I go back here„ÄÇ

 that is what we have„ÄÇ We have a label of oneÔºå and then we predict 0 because we are on the right wrong side here„ÄÇ

What we do is then we add the input vector to the weight vector„ÄÇ

 which is what I've shown you in the previous slide and by that we can correct the decision boundary and here if you want or likely you can also play through the same scenario if the prediction and the targets are flipped so the same concept applies there too„ÄÇ

YeahÔºå to summarize the perceptron has many shortcomings„ÄÇ

 So there are many problems with a perceptronÔºå which is why it's not commonly used anymore in practice„ÄÇ

 Some people still may use it in certain applications where computational performance matters because one advantage of the perceptron is it's very simple to implement and yeah very fast„ÄÇ

 HoweverÔºå yeahÔºå nowadaysÔºå many problems that we have in the real world cannot be solved by this perceptron„ÄÇ

 which is why we have yeah fancy deep learning methods„ÄÇ

 So let me go over the shortcomings of the perceptron„ÄÇSo first of allÔºå it's a linear classifier„ÄÇ

 so you cannot have nonlinear decision boundaries if you think back of the XO problem„ÄÇ

 it cannot solve the XO problem orifier„ÄÇFor example„ÄÇ

 just have a another simple problem where we have two classes like„ÄÇLike thisÔºå for example„ÄÇ

 class0 and class„ÄÇOne like this„ÄÇAround it„ÄÇ So if we have these concentric circles„ÄÇ

 the perceptionron cannot classify these very well because it can only do a linear decision boundary like this or like this„ÄÇ

 And that is not helpful what we need is we haveÔºå we need a non nonlinear decision boundary like„ÄÇ

ThisÔºå for example„ÄÇSo that is one shortcoming Another shortcoming is that it is a binary classifier so it can only classify two classes„ÄÇ

 but yeah of course there are also extensions where you can use multiple perceptrons There's a method called one versus rest or one versus all or one versus one So actually two methods that I can just write them down one„ÄÇ

RersusÔºå rest„ÄÇ Sometimes it's also called one rosesÔºå all„ÄÇWhere you can just use multiple perceptrons„ÄÇ

Or multiple binary classifiers„ÄÇ And one is called one versusus1„ÄÇTo have multi class classification„ÄÇÂóØ„ÄÇ

YeahÔºå but it is like one single perceptron cannot do multi class classification because it's just returning either a 0 and a one„ÄÇ

Another shortcoming is it does not converge if classes are not linearly separable„ÄÇ

 So what does that mean So that is exactly the problem related to yeah the convergence theorem that we have not covered„ÄÇ

 but in the convergence theorem itself it only converges if the classes are linearly separable to let me draw that out also„ÄÇ

 So if we have a case„ÄÇAgainÔºå likeÔºå let's say like this„ÄÇIt will find a decision boundary like this„ÄÇ

 howeverÔºå if I have a data point over hereÔºå then there is no decision boundary such that I can classify everything correctly so it will learn let's say this decision boundary but then it encounters this data point so if we keep iterating and then it thinks okay I have to move the decision boundary further to the right like here but then it gets these examples wrong so then it thinks okay I have to move it back to the left so it will keep on moving the decision boundary back and forth and it will never really stop updating„ÄÇ

So in that caseÔºå that is also a problem„ÄÇ if you want to have the final model„ÄÇ IÔºå it's fluctuating„ÄÇ

 It's likeÔºå the longer you run the algorithmÔºå the yeahÔºå the more solutions you will get„ÄÇ

 So it will never really finish updating„ÄÇÂóØ„ÄÇYeahÔºå and then also„ÄÇ

Another thing is there are multiple solutions possible if„ÄÇThe classes are separable„ÄÇ

 So now let's assume you have a problem that is„ÄÇLet me do is a little prettier„ÄÇ

 Let's say no you have a problem where you can linearly separate these classesÔºå let's say„ÄÇLike this„ÄÇ

 So one possible decision boundary is like this„ÄÇ Another one is like this„ÄÇ One is like this„ÄÇ

 So there are multiple solutionsÔºå and it depends onÔºå yeahÔºå where you start with your weight„ÄÇ

 Which one is the best one or which one is picked by the perceptioncept one before it stops updating„ÄÇ

 And yeahÔºå this is also kind of annoying or can be annoying„ÄÇ

 There are multiple solutions So that I would say there is a best one„ÄÇ

 The best one might be the one that is just in the center„ÄÇRather than something that fitsÔºå let's say„ÄÇ

 the data very closely here to the left or very closely to the right„ÄÇ

 because one might argue that putting the decision boundary in the center helps with reducing overfitting„ÄÇ

 for example„ÄÇBut yeahÔºå that is also something the perceptioncept is not capable of„ÄÇ AndÔºå and yeah„ÄÇ

 later lecturesÔºå the methods we will be usingÔºå they are a little bit better with that„ÄÇAllright„ÄÇ

 so that is just the other conclusions regarding the perceptionceptronÔºå like one last thing„ÄÇ

 like a little fun fact„ÄÇSo yeahÔºå back in the dayÔºå the perceptron was used or people tried to use it to detect tanks and photographs„ÄÇ

So here on the left hand sideÔºå there would be tank„ÄÇ and here there would be no tank„ÄÇ and they used„ÄÇ

 yeahÔºå the perceptron on images to classify whether there is a tank in the photograph or not„ÄÇ

 HoweverÔºå like we just saidÔºå the perceptron is very limited„ÄÇ So why would that work at all„ÄÇ

 People found that it workedÔºå But there was like a little gotcha„ÄÇ

 a little mistake or conceptual mistake„ÄÇSo because people didn't first think about it very well„ÄÇ

 So what happened was that all the forest pictures were darker than the pictures with the tanks„ÄÇ

 And the perceptron simplyÔºå yeahÔºå recognized that darker photographs means no tank and brighter photographs means tank„ÄÇ

 Of courseÔºå it had no idea whether there was actually a tank or not„ÄÇ

 So if you would have a dark photograph with a tank„ÄÇ It wouldÔºå of courseÔºå get that wrong„ÄÇ Yeah„ÄÇ

 and this was like a little fun fact about the perceptionceptron„ÄÇ

 I got that from this interview with Marvin MinskyÔºå who was„ÄÇ

One of the authors of this perceptron book I mentioned where they kind of„ÄÇ

 yeah said bad things about that perceptron that it was limited„ÄÇ

 So here is like a little excerpt from this interview„ÄÇSoÔºå it's basically„ÄÇÂóØ„ÄÇ

It was looking for a scene at a scene of a forest in which there were camouflage tanks in one picture and no camouflage tanks in the other picture„ÄÇ

And the perceptronÔºå after a little training made 100% corrected distinctions between these two different sets of photographs„ÄÇ

Then they were embarrassed a few hours later to discover that the two rolls of film had been developed differently„ÄÇ

 And so these pictures were just a little darker than all of the pictures„ÄÇ

 and the perception was just measuring the total amount of light in the scene„ÄÇ

 So it was just counting the pixel values„ÄÇThe pixel brightnesses„ÄÇ

But it was very clever of the perceptionceptron to find some way of making this distinction„ÄÇ Yeah„ÄÇ

 I think I would admit that it was actually kind of exploiting the fact that photographs were brighter and darker„ÄÇ

 but of courseÔºå it had nothing to do with whether there was a tank in the picture or not„ÄÇ

 it was just how they developed the film„ÄÇ All rightÔºå with thatÔºå I want to end this lecture„ÄÇ

 which was a little bit long„ÄÇ I will in the future try to keep that chart„ÄÇ

 But I didn't want to split this topic over two weeks„ÄÇ So in the next week„ÄÇ

 we will cover some basic linear algebra„ÄÇ I meanÔºå just some ground rules that we will be using„ÄÇ

 And then hopefully also slowly get into implementing more sophisticated methods in patrontorch„ÄÇ



![](img/574931c4b6ab59660c98db6e5b4061c3_1.png)

![](img/574931c4b6ab59660c98db6e5b4061c3_2.png)