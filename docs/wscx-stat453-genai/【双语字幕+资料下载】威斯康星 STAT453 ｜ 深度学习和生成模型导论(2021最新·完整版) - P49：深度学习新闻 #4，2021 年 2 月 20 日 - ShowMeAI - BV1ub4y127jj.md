# P49ï¼šæ·±åº¦å­¦ä¹ æ–°é—» #4ï¼Œ2021 å¹´ 2 æœˆ 20 æ—¥ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hiï¼Œ everyoneã€‚ I hope you had a nice weekã€‚ I just noticed we already completed week4ã€‚

 which meansï¼Œ yeahï¼Œ one fourthth of the semester is already overã€‚ time is running so fastã€‚

 I hope you like the content on Pytorch and the cloud computing resources from last weekã€‚

 So in today's stuff in the news sectionï¼Œ I just wanted to yeah give you a brief update again on what's happening in the deep learning worldã€‚

 Alsoï¼Œ coincidentallyï¼Œ there were some interesting tools around Pytorchã€‚ðŸ˜Šï¼ŒAnd also someã€‚

 yeah interesting news regarding cloud computing and its implications on the environment and also yeahã€‚

 maybe some not so nice applications of of deep learning regarding yeah ethical AI and these types of thingsã€‚

 So yeahï¼Œ there's a variety of news again this weekã€‚ And yeahï¼Œ let's dive in and talk about itã€‚

Yeah let's start with a fun library I saw this week called Free Wired Freely wired neural networksã€‚

 So this is a Pythtor extension library for creating optimized freely wired neural networks to run on Kudaã€‚

 the GPU essentiallyã€‚So you have seen already in the lecture how yeah multi layerer perceptionceptrons work fully connected layersã€‚

 So where each unit in one layer is connected to every unit in the following layerã€‚

 and we will discuss that also in more detail next week in the courseã€‚ but yeahã€‚

 here what's different is that we can arbitrarily connect these notessã€‚ For exampleï¼Œ considerã€‚

In this drawing on the left hand sideï¼Œ consider this case of this unitã€‚

 let's say this is the input layerã€‚This is the firstã€‚Didn't theyã€‚

And then we have the second hidden layerã€‚So we have this unit hereï¼Œ for exampleã€‚

 that is only connected to one other unit in a typical connected context what we would see is we would see also a connection to this one hereã€‚

 but here here it's more arbitrarily connected or you can also see this input here is connected to this unit and this unit as you would expect in a fully connected layerã€‚

 but then there's also a connection to a consequent layerã€‚

We will see later some flavors of that also in the context of convolution networks and residual connectionsã€‚

 it's also something that is popular and transformersã€‚

 but here it's really more arbitrary than in other architecturesã€‚

 So its just a tool for allowing you to create these types of architecturesã€‚

 I thought that was like a cool thingã€‚ if you want to run some crazy experiments that might might be something yeah interesting to look atã€‚

So where would this be usefulï¼Ÿ Soï¼Œ for exampleï¼Œ one thing that came to mind is this paper exploring randomly wired neural networks for image recognitionã€‚

 So that is a paper I saw like two years agoï¼Œ approximatelyã€‚

 And here the researchers has proposed a method forã€‚Yeahã€‚

 randomly connecting yeah neural networks or getting these random connectionsã€‚

 I think also involving convolutional errorssã€‚ So in this way they used when I recall correctlyã€‚

 they used evolutionary algorithms or genetic algorithms to learn these by yeah optimizing fitness function and they found that this architectureã€‚

 this randomly wired oneã€‚Performs better for exampleï¼Œ than existing one based on Resnetã€‚

 So Resnet is also something we will talk about in this courseã€‚ The Resnet architecture achieved 77ã€‚

1% accuracy on this dataset its imagenet and this randomly wired one achieved a higher accuracyã€‚

 though it seems arbitraryã€‚ So when I read that paper like two years agoã€‚

 I thought really that's maybe the time when these hand designed convolution networks are going out of fashion where people start using these automatically wired onesã€‚

 Howeverï¼Œ I must say personallyï¼Œ I haven't seen anyã€‚Yeah randomly wired network sinceã€‚

 yeah since I read this paper actuallyï¼Œ So maybe this is because it's hard to implement Also it's alsoã€‚

 I would say computationally more intensiveã€‚ And maybe there's also less logic behindã€‚

 So for regular CNN architectureï¼Œ you can maybe make some arguments why it's supposed to perform better with a randomly wired oneã€‚

 it's maybe a little bit more like a black box and people don't like thatã€‚ But yeahï¼Œ againã€‚

 with this tool I showed you in the previous slideã€‚

 there might be an opportunity to explore this further because now it's maybe easier to implement such randomly wired networksã€‚

Yeahï¼Œ also this week we got a new computer vision dataset setã€‚ So in this new dataã€‚

 the researchers compiled 1ã€‚5 million images from 565 classesã€‚

 So what's novel or interesting about this data is that it is focused on object categories that are important to humansã€‚

 at least that's what the researchers aim forã€‚Yeahã€‚

 so this paper will be actually published in three daysã€‚

 so it's today the 20th and the paper is already available on the website of PNSã€‚

If you want to take a closer look at itã€‚And also the researchers provide code examples regarding pretrained convolutional networks trained on the dataset and also how you can load this dataã€‚

 There are some other interesting aspects from the dataï¼Œ how the data looks like the statisticsã€‚

 the basic statisticsï¼Œ but then also how they collected this data setã€‚

 So I wanted to just briefly also walk through this because I thought that was interestingã€‚

So here's an example of some of the 565 classes man house carï¼Œ woman phone bedã€‚

 so things that are relevant to humansã€‚ And so here they also looked at these categories for exampleã€‚

 60% of things all labeled are artificial 40% naturalã€‚ So for exampleã€‚

 a car would be artificial and the tree would be natural and so forthã€‚

 So they have some interesting yeah analysis hereã€‚Alsoï¼Œ regarding the number of images note thatã€‚

 yeah the number of images is different for each classã€‚

 so it's not a balanced data set it's imbalancedã€‚Which makes certain things like the evaluation a bit more challengingã€‚

 So instead ofï¼Œ for exampleï¼Œ using the regular accuracyã€‚

 something like the class balanced or balanced accuracy might be something more worthwhile or not worthwhileã€‚

 but more reasonable to use hereã€‚ So some students who took the 4ï¼Œ51ã€‚Class last semesterã€‚

 we talked about the balanced accuracy that would be maybe one measure that would be also appropriate hereã€‚

 So and also notice that these images have different aspect ratios and resolutionsã€‚

 which is also shown hereã€‚ So if you work with the dataset setã€‚

 you also have to make sure of course that yeahï¼Œ you find a common denominator for the input sizesã€‚

 if you work with certain architecturesï¼Œ which are restricted to certain input sizesã€‚

But that being saidï¼Œ let's skip to the more interesting part how they collected this data setã€‚

 So what they did is they try to kind of filter for labels that occur frequently in large text corpesã€‚

 So it's like a proxy for saying how important class or word is or knownã€‚

 and they also had this concreteness ratingã€‚ So this was something yeah quite interestingã€‚

 So here they employed yeah data from human observersã€‚ Soã€‚

How that works is that they asked people to rate on a scale from  one to 5 how concrete a certain noun isã€‚

 So in terms of how easily can you visualize the conceptã€‚ So as an exampleï¼Œ for exampleã€‚

 con consider on one end strawberryã€‚If I tell you the word strawberryã€‚

 I think you can easily visualize thisï¼Œ so it would have a very high concretenessã€‚

But when I mention a noun like hopeï¼Œ for exampleï¼Œ which is a little bit less concreteã€‚

 So here it's kind of harder to visualize how hope looks like as aï¼Œ as an objectã€‚ So in this wayã€‚

 it would have a very low of concreteness ratingã€‚ So by using these types of ratingsã€‚

 they also filtered for very concreteï¼Œ very concrete things like itemsï¼Œ andã€‚

From that they extracted in the top 3ï¼Œ500 words based on frequency and correctnessã€‚

 they had a certain index forulating calculating thatã€‚Yeahã€‚

 and then they had each now describing basic level category and excluding subordinate categoriesã€‚

 so making sure it's not too hierarchicalï¼Œ like picking the yeahï¼Œ common denominator categoriesã€‚

And also merging synonymsï¼Œ for exampleï¼Œ automobile and car into one wordã€‚

And after they basically extracted these class labels they were interested inã€‚

What they then did is they downloaded corresponding images from ImageNeï¼Œ Flickr and Bingã€‚

Then they also performed duplicate removal to make sure there are no duplicatesã€‚

 so here they used principal component analysisï¼Œ so they did decomposed the images with principal component analysis and then when I remember correctly what they did is yeah they looked at the factor loadings of the PCAã€‚

And then looked at the correlation between these vector loadingsã€‚

 So I believe they did that in a pairwise fashion to filter out then duplicatesã€‚Alsoã€‚

 what they did is here they manually excluded misclassifications from all categories here they don't say anything specific here they say something with a 4% error rate and in the paper they say basically that they sampled 100 images randomly from each classã€‚

And then theyï¼Œ as a human looked at how many of these images are incorrectly labeledã€‚

 So if you look at them and you seeï¼Œ okayï¼Œ this label label doesn't make sense if the rate was more than 4%ã€‚

 like four more than four images out of the 100ï¼Œ then they looked at all the images from this category and cleaned that outã€‚

And otherwiseï¼Œ I think they left it as isã€‚ so you can see or can think of it as in the worst case scenarioã€‚

 you can only have a classifier that is as good as 96% accuracy considering there are up to 4% mislabelled images in this dataset setã€‚

 but it's still reasonable goodã€‚And then they merged these images from the different databasesã€‚

 removed the duplicates againã€‚ So now checking for duplicates from imagenet versus the Flr and Bing imagesã€‚

Then theyï¼Œ yeahï¼Œ prune the data setï¼Œ removing images or some of the imagesã€‚ So that theseï¼Œ yeahã€‚

 labeled or sorry that the examples are approximately between 7 and 5000 per classã€‚

And then yeah they obtained this final datasetã€‚ So 94% of the images are actually from Imnetã€‚

 which is a big numberï¼Œ5% are only from Bing and 1% is only from Flickrã€‚

 I actually don't recall exactly how imagenet images were collectedã€‚

 I think they were collected from Google but there might be also just an overlap between imagenet and Bing and that's why there's such a high number of imagenet images compared to the other imagesã€‚

 but yeahï¼Œ exactly why theres such a high imagenet percentage I don't knowã€‚

 but yeah so it's an interesting dataset's yeah just another cool dataset that can be used for yeah testing your modelsã€‚

Yeahï¼Œ moving from larger scale image data sets to large scale modelsã€‚

 So there's also common or popular discussion nowadays that these AI or deep learning models become more and more expensive to train soã€‚

Here was an article called the billion dollarll AI problem that just keeps scalingã€‚ Yeahã€‚

 luckily we haven't reached the point where you model training costs 1 billionã€‚

 but it can easily end up costing multiple million dollars even nowadays when you consider these big modelsã€‚

 So for exampleï¼Œ here is a chart from the megaron modelã€‚From Nvidiaã€‚

 So here what they did is they trained on or they had a system called Celineã€‚

And this consisted of 280 DGX A 100 systems where each of these systemsï¼Œ if you would buy themã€‚

 would cost $200ï¼Œ000ã€‚ and then if you considerï¼Œ let's say 15% on top of it for a networking cost like connecting everythingã€‚

And then also 20% for storing thisã€‚ Yeahï¼Œ this would be easily or if you would consider the this priceã€‚

 it would easily end up being like $75 million just to buy the systemã€‚ And also in this articleã€‚

 they did a calculation like how expensive would it be to run this model if you would rent the hardware on AWS so the Amazon on server or service it would also easily be like around 80 million if you just rent this computational resource for three yearsã€‚

 So in this way also I should say electricity is not includedã€‚

 it's becoming more and more expensive to train modelsã€‚ So for this modelã€‚

 they really used like this huge systemã€‚Where they had around 3000 GPusã€‚

 So 3000 gus and the model had around 200 billion parametersã€‚

 So things become more and more expensiveï¼Œ which can be a problemã€‚

 So also regarding yeah parallelizationï¼Œ there are two techniquesã€‚

So also it's kind of computation an interesting problemã€‚

 One is a data parallelism where you split up the batchã€‚

 So you make the batch larger and larger and larger and then you split it up onto multiple GPUus where each GPU receives fraction of that original mini batch and then each of the GPU computes the gradient and then you combine the gradients by averaging over these gradients to update the modelsã€‚

 but there is also a limit for yeah doing that theres like limiting diminishing returnsã€‚

 as you would sayï¼Œ if you have high and higher data parallelismã€‚

 So also newer methods consider tensor parallelism where you can efficientlyã€‚Computeï¼Œ let's sayã€‚

 matrix multiplications across different GPusã€‚ That can also be a little bit trickyï¼Œ thoughã€‚

 because you have to consider the connection between the GPusã€‚ You have to make sure they areã€‚

 yeah connected with fast connections like infinity band and these types of thingsã€‚

 So it's actually not a trivial problemã€‚ So if you have more GPSusã€‚

 It's not necessary saying that it's easy to parallelizeã€‚

So it's just an interesting research problemï¼Œ but yeahã€‚

 one problem with that is really environmental friendliness alsoã€‚



![](img/165d2a908592e15b2955520f44c6d7df_1.png)

Yesï¼Œ speaking of environmental friendlinessï¼Œ a recent study showed that federated learning can lead to reduced carbon emissionsã€‚

 So what is federated learningï¼Œ Feerated learning means essentially computing things on multiple devices and then gathering the results from these devicesã€‚

 For exampleï¼Œ these could be cell phones or different data centersã€‚Soï¼Œ for exampleã€‚

 you can think of if youï¼Œ I'm not sure if you remember this when I was a kidï¼Œ there wasã€‚

Something called folding at home for PlayStationã€‚ and people could sign up for this serviceã€‚

 And then if you had idle yeah idle cycles on your PlayStationã€‚

 these were used by let's say some research center to help with protein folding or another example would be our H TC condor on campus where we haveã€‚

 yeah where we can use different computers like desktops in different offices if they are idleã€‚

 which is actually pretty coolã€‚ And they found if you do that it can lead to lower let's say carbon emissionsã€‚

 And yeahï¼Œ likely this is due becauseã€‚Of the heatingã€‚

 because if you have computers in different locations and they are only used a little bitã€‚

 you may not need any extra heatingã€‚ sorry cooling for preventing the heat exposure or overheating of the device compared to let's say a big data center where you have to pump in constantly like lots of energy to keep it coolã€‚

 And yeahï¼Œ also just the numbers were just impressiveã€‚ I meanï¼Œ these data centers nowadaysã€‚

 they use20 a terW hours per yearã€‚ compared to average US homeã€‚

 which only uses 10000 kiloWs per hourã€‚ And I just looked up what a terat isã€‚

 a ter is I wasn't actually sure I was I not sure if it's 1 millionã€‚ It's actually 1 billion kiloWã€‚

It's like a huge amount of energy they useã€‚ So if this federated learning can helpï¼Œ why notï¼Ÿ Howeverã€‚

 the researchers also noted that it there's like a little caveatã€‚

 So it's actually not quite clear because federated learning can also be less efficientã€‚For exampleã€‚

 if you consider prolonged training times due to distributed databasesã€‚

 So if you want to train a model using multiple devicesã€‚

 but you have your database sitting in a location that has to be accessedã€‚

 well that will probably make training longerã€‚ So you have you have to keep your device running for longer timeã€‚

 but then alsoã€‚You will have eventually then some more data data transfer via wfiã€‚

 which also takes energyã€‚ And then also you have to consider that these devices may be less efficientã€‚

 like using the same energyï¼Œ but being less efficientã€‚ One example would beï¼Œ for exampleã€‚

 the new what is it M1 chipï¼Œ the arm chips in the Macboxã€‚

 they have the same speed as the top and inter CPUusã€‚ Howeverï¼Œ they are more efficientã€‚

 So in that wayï¼Œ you have to also consider certain devices are not as efficientã€‚

 So if you use federated learningï¼Œ this could also be maybe yeah less efficientã€‚

 But I think it's still interesting that people started looking at that because yeahã€‚

T we also have to be aware of the environment and be resourceful when we do deep learningã€‚



![](img/165d2a908592e15b2955520f44c6d7df_3.png)

And related to thatï¼Œ I also saw there another new startup I think it's pronounced new realityã€‚

 but I'm not exactly sureã€‚ So also one trend is really that people work on making chips for deep learning more efficientã€‚

 So this would be yet another company that focuses on yeah deep learning chips or instead of GPUs like having specialized chips for deep learning andã€‚

InThis articleï¼Œ they said basically the chip can perform inference with 15 times higher performance per dollar than the competitionã€‚

 Not that they don't say that the chip is faster than GPusã€‚

 really here the focus is on the performance per dollarã€‚ but it's not a bad thingã€‚ I meanã€‚

 maybe this chip overall is slowerã€‚ but you can see 15 times higher performance per dollarã€‚

 So per dollarï¼Œ it's not clear whether this means like manufacturing cost or also usage cost in terms of electricityã€‚

 but I bet it's a mix of bothã€‚ So it's probably a chip that is smallerã€‚

 less powerful than a GPU for exampleï¼Œ but probably more energy efficientã€‚ but well seeã€‚



![](img/165d2a908592e15b2955520f44c6d7df_5.png)

Yeahï¼Œ so here I have some news which is I thinkï¼Œ yeah not such a good use of AI or deep learningã€‚

 I find this always questionable like these types of applicationsã€‚

 but I think it's still yeah important to talk about itã€‚

 So here this is about a system trained to rate yeah applicants during job interviewsã€‚

I think also when people develop these systemsï¼Œ So the aim is really to make things more objectiveã€‚

 like if you think of it as a humanï¼Œ you have probably some biases towards certain candidates and if you have an AI systemã€‚

 you may be hoping that this is lessï¼Œ let's say it's more objectiveã€‚

 less subjective and probably also scales better so you can have more applicants but yeahã€‚

 so I think I'm not sure but I think these are all developed in good faithã€‚

 but they can also yeah be very problematicã€‚ So here was a questionable use of AI forã€‚

Job applicationsã€‚ I also recommend you to read this full articleã€‚ It was a very interesting analysisã€‚

 It's a short articleï¼Œ but veryï¼Œ very well writtenï¼Œ soã€‚Yeahã€‚

 essentially here the researchers developed an AI to rate job applicantsã€‚

 so they trained the AI on videos from more than 12ï¼Œ000 people from different agesã€‚

 gender and ethnic backgrounds andã€‚

![](img/165d2a908592e15b2955520f44c6d7df_7.png)

An additional 2500 people rated how they perceived them in terms of personality dimensions and so forthã€‚

And I think it's not quite clearï¼Œ but I think how this works is basicallyã€‚

 they had people rating these job applicants and thenã€‚

Collected these scores and then trained the AI on these scoresã€‚ And that way you can think of itã€‚

 maybe the model yeah just mimics what the humans are doing during the ratingã€‚

 So they had like these different rating terms like opennessï¼Œ conscientiousnessï¼Œ extraversionã€‚

 agreeableness and neuroticism and these were like other different labelsã€‚For the personal traitsã€‚

 Yeahï¼Œ and then the researchers employed actors to do mock interviewsã€‚

 So keeping everything constant like speech and content of the text during the interviewã€‚

 and they also repeated that multiple timesã€‚ So make sure that making sure that the results are stable and then they for exampleã€‚

 varied different aspectsã€‚ So hereï¼Œ for exampleï¼Œ they the actress was wearing glasses and comparing the results to not wearing glassesã€‚

 And you can see in blue is the original without glassesã€‚



![](img/165d2a908592e15b2955520f44c6d7df_9.png)

And then in yellow with glasses and you can see based on these ratings by the AI that the squs for opennessã€‚

 conscientiousnessï¼Œ extratroversionï¼Œ agreeableness were less when the person was wearing glasses So this is also one of the reasons why I'm not wearing glasses when I record my lecture No I'm just kidding Now I think this is like a little bit problematic because I think wearing glasses or not shouldn't influence any decision right so this is maybe something that is not going so well here with this AI and even worseã€‚



![](img/165d2a908592e15b2955520f44c6d7df_11.png)

So where the results were even more differentï¼Œ they had a person doing a mock interview with and without a bookshelf in the background where everything else was constantã€‚

 So and you can see that with a bookshelfã€‚ The squash improved or lotã€‚

 So this is also a very questionable result by the eyeã€‚

 So there shouldn't be such a difference whether someone has a bookshelf in the background or notã€‚

 And yeahï¼Œ this is maybe also a reason why I have a bookshelf in my background hereã€‚

 Now I'm just again kiddingã€‚ But yeahï¼Œ so this is againï¼Œ it's actually a serious topicã€‚

 I think this is something when you develop such systemsï¼Œ you should make sure thatã€‚

Such problems don't occur because I think this is incredibly yeahï¼Œ unfair because reallyï¼Œ I meanã€‚

 it really shouldn't have anything to do with your skills as an applicantã€‚

 whether what type of background you have and things like thatã€‚ So yeahï¼Œ this is someã€‚

 some very problematic resultã€‚ And I think in generalã€‚

 maybe this is an area where AI shouldn't be employedã€‚



![](img/165d2a908592e15b2955520f44c6d7df_13.png)

Yesï¼Œ so why does this problem happenã€‚ So there was one comment by Kaarina Swwikeã€‚

 which I think hits the nail on the headã€‚ It's the fundamental problem with face recognition by machine learning is that we never know exactly which pattern in an image these machines are responding toã€‚

 So yeahï¼Œ deep learning can sometimes or often be more like a black boxã€‚

 So you never know really which parts of the image the machine learning system is using rightã€‚

 because you provide the whole image with background in everythingã€‚ So do you really knowã€‚

What the machine learning is paying attention toã€‚I don't know if that solves the problemã€‚

 but if you think of Zoomï¼Œ for exampleï¼Œ where you can easily switch these backgroundsã€‚

 having these videos with data augmentationï¼Œ swapping different backgrounds may help at least with this bookshelf situationã€‚

 but there of courseï¼Œ more issues to be addressed hereã€‚

 but yeah this is something if you develop systems you should probably be aware of that and test also things like that and make sure that yeah these these things are filtered out and not causing problemsã€‚

Especially in a real world applicationã€‚So yeahï¼Œ related to looking into what deep learning doesã€‚

 how it behavesã€‚ So I also saw another interesting tool on called cockpitã€‚ It's also for Pytorchã€‚

 So it's a practical debugging tool for training deep neural networksã€‚

 I think it might be something that could be helpful in the class projectï¼Œ for instanceã€‚

So the authors say when engineers train deeping modelsï¼Œ they are very much flying blindã€‚

 So here they offer a collection of instruments to look into the inner workingsã€‚

Of machine learning modelsã€‚ I don't knowã€‚ I somehow screwed up this sentence hereã€‚

 So it's really useful for troubleshooting during trainingã€‚ And you can find a tool here on Githubã€‚

Just to show you one example hereï¼Œ so here they focused on the problem of learning rate tuning so usually when you want to find a good learning rateã€‚

 you usually just try different things and see what works and yeah tune it by hand and see you make it larger and smaller and see whether it's better or not and then sometimes you also employ learning rate schedulers which decrease the learning rate over timeã€‚

 we will discuss that in a future lecture actually in two weeksã€‚

But here's an example where the researchers looked at the loss curveã€‚

 So they have mini batch training loss here plotted on the Y axisã€‚

 So that's the loss per mini batch over the different iterationsã€‚ and orange and blueã€‚

 these two lines are corresponding to different learning rate settingsã€‚Nowã€‚

 if they use a look at the lost landscape here in two dimensionsï¼Œ againï¼Œ it's kind of hard to seeã€‚

 but here in orangeï¼Œ they have one settingï¼Œ the orange one and in blueï¼Œ the other oneã€‚So hereã€‚

 what they want to illustrate is that the orange oneã€‚Is only updating a little bitã€‚

 and then it stopsã€‚ So the learning rate is likely too smallã€‚

 and the blue one has a learning rate that is is too largeã€‚ You can see it's overshootingã€‚

So here would be the global minimumã€‚Global minimumã€‚ So they are overshooting hereã€‚

 So the blue learning rate is too large and the orange one is too smallã€‚ and they offer some toolsã€‚

 for exampleï¼Œ this alpha distribution hereã€‚Ass a debugging toolã€‚

 So this is like what they call a normalized step length and0 would be where you would directly be stepping into the right direction of theã€‚

 yeahï¼Œ of the minimumï¼Œ the global minimumï¼Œ or I think it is local minimum in that contextã€‚

It wasn't actually quite clearã€‚ I think the paperï¼Œ I think the tool looks greatã€‚

 the paper wasn't my favorite to readï¼Œ it wasn't very well writtenï¼Œ maybe it wasn't well writtenã€‚

 I didn't understand it properlyã€‚ I don't know add a little bit of a hard time understanding certain things thereã€‚

 but what you want what theoretically would expect is stepping into this yeah into this direction directlyã€‚

 if you have a learning rate that does that it would be optimalã€‚And with this toolã€‚

 you can see whether you are too smallï¼Œ like in the orange case or too largeã€‚

 and then you want to adjust it more to the middleï¼Œ Surpriinglyï¼Œ thoughã€‚

 when they did some experiments on different data sets hereã€‚

 they found that the ones that resulted in the best generalization performance in terms of test accuracy were slightly above the zeroã€‚

 So here they found so these linesã€‚These lines here correspond the solid lines on theã€‚

Vertical lines hereï¼Œ they correspond to the models that have the best performanceã€‚

 And you can see those areã€‚At least yeahï¼Œ I would say 0 or larger than0 in terms of this normalized step length hereã€‚

 They have the mean normalized step lengthï¼Œ soã€‚But in any caseï¼Œ thisï¼Œ I thinkã€‚

 might be a useful tool if you are tuning the learning rateã€‚

 you can just maybe see where you are here compared to the0 and then maybeã€‚

Focus on changing it to be somewhere in this region hereã€‚

 But this is something I haven't tried personallyã€‚ So this is also something that's new to meã€‚

 I usually like many other peopleï¼Œ tune the learning rate by handã€‚

 but it might be something useful to look at in the futureã€‚ So with thatã€‚

 I think that's all the news I haveã€‚ I think it's a long videoã€‚ But yeahï¼Œ with thatã€‚

 let me end this videoã€‚ And I will see you back in class next weekã€‚



![](img/165d2a908592e15b2955520f44c6d7df_15.png)