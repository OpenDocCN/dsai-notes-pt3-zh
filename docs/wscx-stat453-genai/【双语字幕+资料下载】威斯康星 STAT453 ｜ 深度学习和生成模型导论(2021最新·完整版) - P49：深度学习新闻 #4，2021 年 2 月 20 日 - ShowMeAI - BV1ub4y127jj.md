# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P49ÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞Èóª #4Ôºå2021 Âπ¥ 2 Êúà 20 Êó• - ShowMeAI - BV1ub4y127jj

YeahÔºå hiÔºå everyone„ÄÇ I hope you had a nice week„ÄÇ I just noticed we already completed week4„ÄÇ

 which meansÔºå yeahÔºå one fourthth of the semester is already over„ÄÇ time is running so fast„ÄÇ

 I hope you like the content on Pytorch and the cloud computing resources from last week„ÄÇ

 So in today's stuff in the news sectionÔºå I just wanted to yeah give you a brief update again on what's happening in the deep learning world„ÄÇ

 AlsoÔºå coincidentallyÔºå there were some interesting tools around Pytorch„ÄÇüòäÔºåAnd also some„ÄÇ

 yeah interesting news regarding cloud computing and its implications on the environment and also yeah„ÄÇ

 maybe some not so nice applications of of deep learning regarding yeah ethical AI and these types of things„ÄÇ

 So yeahÔºå there's a variety of news again this week„ÄÇ And yeahÔºå let's dive in and talk about it„ÄÇ

Yeah let's start with a fun library I saw this week called Free Wired Freely wired neural networks„ÄÇ

 So this is a Pythtor extension library for creating optimized freely wired neural networks to run on Kuda„ÄÇ

 the GPU essentially„ÄÇSo you have seen already in the lecture how yeah multi layerer perceptionceptrons work fully connected layers„ÄÇ

 So where each unit in one layer is connected to every unit in the following layer„ÄÇ

 and we will discuss that also in more detail next week in the course„ÄÇ but yeah„ÄÇ

 here what's different is that we can arbitrarily connect these notess„ÄÇ For exampleÔºå consider„ÄÇ

In this drawing on the left hand sideÔºå consider this case of this unit„ÄÇ

 let's say this is the input layer„ÄÇThis is the first„ÄÇDidn't they„ÄÇ

And then we have the second hidden layer„ÄÇSo we have this unit hereÔºå for example„ÄÇ

 that is only connected to one other unit in a typical connected context what we would see is we would see also a connection to this one here„ÄÇ

 but here here it's more arbitrarily connected or you can also see this input here is connected to this unit and this unit as you would expect in a fully connected layer„ÄÇ

 but then there's also a connection to a consequent layer„ÄÇ

We will see later some flavors of that also in the context of convolution networks and residual connections„ÄÇ

 it's also something that is popular and transformers„ÄÇ

 but here it's really more arbitrary than in other architectures„ÄÇ

 So its just a tool for allowing you to create these types of architectures„ÄÇ

 I thought that was like a cool thing„ÄÇ if you want to run some crazy experiments that might might be something yeah interesting to look at„ÄÇ

So where would this be usefulÔºü SoÔºå for exampleÔºå one thing that came to mind is this paper exploring randomly wired neural networks for image recognition„ÄÇ

 So that is a paper I saw like two years agoÔºå approximately„ÄÇ

 And here the researchers has proposed a method for„ÄÇYeah„ÄÇ

 randomly connecting yeah neural networks or getting these random connections„ÄÇ

 I think also involving convolutional errorss„ÄÇ So in this way they used when I recall correctly„ÄÇ

 they used evolutionary algorithms or genetic algorithms to learn these by yeah optimizing fitness function and they found that this architecture„ÄÇ

 this randomly wired one„ÄÇPerforms better for exampleÔºå than existing one based on Resnet„ÄÇ

 So Resnet is also something we will talk about in this course„ÄÇ The Resnet architecture achieved 77„ÄÇ

1% accuracy on this dataset its imagenet and this randomly wired one achieved a higher accuracy„ÄÇ

 though it seems arbitrary„ÄÇ So when I read that paper like two years ago„ÄÇ

 I thought really that's maybe the time when these hand designed convolution networks are going out of fashion where people start using these automatically wired ones„ÄÇ

 HoweverÔºå I must say personallyÔºå I haven't seen any„ÄÇYeah randomly wired network since„ÄÇ

 yeah since I read this paper actuallyÔºå So maybe this is because it's hard to implement Also it's also„ÄÇ

 I would say computationally more intensive„ÄÇ And maybe there's also less logic behind„ÄÇ

 So for regular CNN architectureÔºå you can maybe make some arguments why it's supposed to perform better with a randomly wired one„ÄÇ

 it's maybe a little bit more like a black box and people don't like that„ÄÇ But yeahÔºå again„ÄÇ

 with this tool I showed you in the previous slide„ÄÇ

 there might be an opportunity to explore this further because now it's maybe easier to implement such randomly wired networks„ÄÇ

YeahÔºå also this week we got a new computer vision dataset set„ÄÇ So in this new data„ÄÇ

 the researchers compiled 1„ÄÇ5 million images from 565 classes„ÄÇ

 So what's novel or interesting about this data is that it is focused on object categories that are important to humans„ÄÇ

 at least that's what the researchers aim for„ÄÇYeah„ÄÇ

 so this paper will be actually published in three days„ÄÇ

 so it's today the 20th and the paper is already available on the website of PNS„ÄÇ

If you want to take a closer look at it„ÄÇAnd also the researchers provide code examples regarding pretrained convolutional networks trained on the dataset and also how you can load this data„ÄÇ

 There are some other interesting aspects from the dataÔºå how the data looks like the statistics„ÄÇ

 the basic statisticsÔºå but then also how they collected this data set„ÄÇ

 So I wanted to just briefly also walk through this because I thought that was interesting„ÄÇ

So here's an example of some of the 565 classes man house carÔºå woman phone bed„ÄÇ

 so things that are relevant to humans„ÄÇ And so here they also looked at these categories for example„ÄÇ

 60% of things all labeled are artificial 40% natural„ÄÇ So for example„ÄÇ

 a car would be artificial and the tree would be natural and so forth„ÄÇ

 So they have some interesting yeah analysis here„ÄÇAlsoÔºå regarding the number of images note that„ÄÇ

 yeah the number of images is different for each class„ÄÇ

 so it's not a balanced data set it's imbalanced„ÄÇWhich makes certain things like the evaluation a bit more challenging„ÄÇ

 So instead ofÔºå for exampleÔºå using the regular accuracy„ÄÇ

 something like the class balanced or balanced accuracy might be something more worthwhile or not worthwhile„ÄÇ

 but more reasonable to use here„ÄÇ So some students who took the 4Ôºå51„ÄÇClass last semester„ÄÇ

 we talked about the balanced accuracy that would be maybe one measure that would be also appropriate here„ÄÇ

 So and also notice that these images have different aspect ratios and resolutions„ÄÇ

 which is also shown here„ÄÇ So if you work with the dataset set„ÄÇ

 you also have to make sure of course that yeahÔºå you find a common denominator for the input sizes„ÄÇ

 if you work with certain architecturesÔºå which are restricted to certain input sizes„ÄÇ

But that being saidÔºå let's skip to the more interesting part how they collected this data set„ÄÇ

 So what they did is they try to kind of filter for labels that occur frequently in large text corpes„ÄÇ

 So it's like a proxy for saying how important class or word is or known„ÄÇ

 and they also had this concreteness rating„ÄÇ So this was something yeah quite interesting„ÄÇ

 So here they employed yeah data from human observers„ÄÇ So„ÄÇ

How that works is that they asked people to rate on a scale from  one to 5 how concrete a certain noun is„ÄÇ

 So in terms of how easily can you visualize the concept„ÄÇ So as an exampleÔºå for example„ÄÇ

 con consider on one end strawberry„ÄÇIf I tell you the word strawberry„ÄÇ

 I think you can easily visualize thisÔºå so it would have a very high concreteness„ÄÇ

But when I mention a noun like hopeÔºå for exampleÔºå which is a little bit less concrete„ÄÇ

 So here it's kind of harder to visualize how hope looks like as aÔºå as an object„ÄÇ So in this way„ÄÇ

 it would have a very low of concreteness rating„ÄÇ So by using these types of ratings„ÄÇ

 they also filtered for very concreteÔºå very concrete things like itemsÔºå and„ÄÇ

From that they extracted in the top 3Ôºå500 words based on frequency and correctness„ÄÇ

 they had a certain index forulating calculating that„ÄÇYeah„ÄÇ

 and then they had each now describing basic level category and excluding subordinate categories„ÄÇ

 so making sure it's not too hierarchicalÔºå like picking the yeahÔºå common denominator categories„ÄÇ

And also merging synonymsÔºå for exampleÔºå automobile and car into one word„ÄÇ

And after they basically extracted these class labels they were interested in„ÄÇ

What they then did is they downloaded corresponding images from ImageNeÔºå Flickr and Bing„ÄÇ

Then they also performed duplicate removal to make sure there are no duplicates„ÄÇ

 so here they used principal component analysisÔºå so they did decomposed the images with principal component analysis and then when I remember correctly what they did is yeah they looked at the factor loadings of the PCA„ÄÇ

And then looked at the correlation between these vector loadings„ÄÇ

 So I believe they did that in a pairwise fashion to filter out then duplicates„ÄÇAlso„ÄÇ

 what they did is here they manually excluded misclassifications from all categories here they don't say anything specific here they say something with a 4% error rate and in the paper they say basically that they sampled 100 images randomly from each class„ÄÇ

And then theyÔºå as a human looked at how many of these images are incorrectly labeled„ÄÇ

 So if you look at them and you seeÔºå okayÔºå this label label doesn't make sense if the rate was more than 4%„ÄÇ

 like four more than four images out of the 100Ôºå then they looked at all the images from this category and cleaned that out„ÄÇ

And otherwiseÔºå I think they left it as is„ÄÇ so you can see or can think of it as in the worst case scenario„ÄÇ

 you can only have a classifier that is as good as 96% accuracy considering there are up to 4% mislabelled images in this dataset set„ÄÇ

 but it's still reasonable good„ÄÇAnd then they merged these images from the different databases„ÄÇ

 removed the duplicates again„ÄÇ So now checking for duplicates from imagenet versus the Flr and Bing images„ÄÇ

Then theyÔºå yeahÔºå prune the data setÔºå removing images or some of the images„ÄÇ So that theseÔºå yeah„ÄÇ

 labeled or sorry that the examples are approximately between 7 and 5000 per class„ÄÇ

And then yeah they obtained this final dataset„ÄÇ So 94% of the images are actually from Imnet„ÄÇ

 which is a big numberÔºå5% are only from Bing and 1% is only from Flickr„ÄÇ

 I actually don't recall exactly how imagenet images were collected„ÄÇ

 I think they were collected from Google but there might be also just an overlap between imagenet and Bing and that's why there's such a high number of imagenet images compared to the other images„ÄÇ

 but yeahÔºå exactly why theres such a high imagenet percentage I don't know„ÄÇ

 but yeah so it's an interesting dataset's yeah just another cool dataset that can be used for yeah testing your models„ÄÇ

YeahÔºå moving from larger scale image data sets to large scale models„ÄÇ

 So there's also common or popular discussion nowadays that these AI or deep learning models become more and more expensive to train so„ÄÇ

Here was an article called the billion dollarll AI problem that just keeps scaling„ÄÇ Yeah„ÄÇ

 luckily we haven't reached the point where you model training costs 1 billion„ÄÇ

 but it can easily end up costing multiple million dollars even nowadays when you consider these big models„ÄÇ

 So for exampleÔºå here is a chart from the megaron model„ÄÇFrom Nvidia„ÄÇ

 So here what they did is they trained on or they had a system called Celine„ÄÇ

And this consisted of 280 DGX A 100 systems where each of these systemsÔºå if you would buy them„ÄÇ

 would cost $200Ôºå000„ÄÇ and then if you considerÔºå let's say 15% on top of it for a networking cost like connecting everything„ÄÇ

And then also 20% for storing this„ÄÇ YeahÔºå this would be easily or if you would consider the this price„ÄÇ

 it would easily end up being like $75 million just to buy the system„ÄÇ And also in this article„ÄÇ

 they did a calculation like how expensive would it be to run this model if you would rent the hardware on AWS so the Amazon on server or service it would also easily be like around 80 million if you just rent this computational resource for three years„ÄÇ

 So in this way also I should say electricity is not included„ÄÇ

 it's becoming more and more expensive to train models„ÄÇ So for this model„ÄÇ

 they really used like this huge system„ÄÇWhere they had around 3000 GPus„ÄÇ

 So 3000 gus and the model had around 200 billion parameters„ÄÇ

 So things become more and more expensiveÔºå which can be a problem„ÄÇ

 So also regarding yeah parallelizationÔºå there are two techniques„ÄÇ

So also it's kind of computation an interesting problem„ÄÇ

 One is a data parallelism where you split up the batch„ÄÇ

 So you make the batch larger and larger and larger and then you split it up onto multiple GPUus where each GPU receives fraction of that original mini batch and then each of the GPU computes the gradient and then you combine the gradients by averaging over these gradients to update the models„ÄÇ

 but there is also a limit for yeah doing that theres like limiting diminishing returns„ÄÇ

 as you would sayÔºå if you have high and higher data parallelism„ÄÇ

 So also newer methods consider tensor parallelism where you can efficiently„ÄÇComputeÔºå let's say„ÄÇ

 matrix multiplications across different GPus„ÄÇ That can also be a little bit trickyÔºå though„ÄÇ

 because you have to consider the connection between the GPus„ÄÇ You have to make sure they are„ÄÇ

 yeah connected with fast connections like infinity band and these types of things„ÄÇ

 So it's actually not a trivial problem„ÄÇ So if you have more GPSus„ÄÇ

 It's not necessary saying that it's easy to parallelize„ÄÇ

So it's just an interesting research problemÔºå but yeah„ÄÇ

 one problem with that is really environmental friendliness also„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_1.png)

YesÔºå speaking of environmental friendlinessÔºå a recent study showed that federated learning can lead to reduced carbon emissions„ÄÇ

 So what is federated learningÔºå Feerated learning means essentially computing things on multiple devices and then gathering the results from these devices„ÄÇ

 For exampleÔºå these could be cell phones or different data centers„ÄÇSoÔºå for example„ÄÇ

 you can think of if youÔºå I'm not sure if you remember this when I was a kidÔºå there was„ÄÇ

Something called folding at home for PlayStation„ÄÇ and people could sign up for this service„ÄÇ

 And then if you had idle yeah idle cycles on your PlayStation„ÄÇ

 these were used by let's say some research center to help with protein folding or another example would be our H TC condor on campus where we have„ÄÇ

 yeah where we can use different computers like desktops in different offices if they are idle„ÄÇ

 which is actually pretty cool„ÄÇ And they found if you do that it can lead to lower let's say carbon emissions„ÄÇ

 And yeahÔºå likely this is due because„ÄÇOf the heating„ÄÇ

 because if you have computers in different locations and they are only used a little bit„ÄÇ

 you may not need any extra heating„ÄÇ sorry cooling for preventing the heat exposure or overheating of the device compared to let's say a big data center where you have to pump in constantly like lots of energy to keep it cool„ÄÇ

 And yeahÔºå also just the numbers were just impressive„ÄÇ I meanÔºå these data centers nowadays„ÄÇ

 they use20 a terW hours per year„ÄÇ compared to average US home„ÄÇ

 which only uses 10000 kiloWs per hour„ÄÇ And I just looked up what a terat is„ÄÇ

 a ter is I wasn't actually sure I was I not sure if it's 1 million„ÄÇ It's actually 1 billion kiloW„ÄÇ

It's like a huge amount of energy they use„ÄÇ So if this federated learning can helpÔºå why notÔºü However„ÄÇ

 the researchers also noted that it there's like a little caveat„ÄÇ

 So it's actually not quite clear because federated learning can also be less efficient„ÄÇFor example„ÄÇ

 if you consider prolonged training times due to distributed databases„ÄÇ

 So if you want to train a model using multiple devices„ÄÇ

 but you have your database sitting in a location that has to be accessed„ÄÇ

 well that will probably make training longer„ÄÇ So you have you have to keep your device running for longer time„ÄÇ

 but then also„ÄÇYou will have eventually then some more data data transfer via wfi„ÄÇ

 which also takes energy„ÄÇ And then also you have to consider that these devices may be less efficient„ÄÇ

 like using the same energyÔºå but being less efficient„ÄÇ One example would beÔºå for example„ÄÇ

 the new what is it M1 chipÔºå the arm chips in the Macbox„ÄÇ

 they have the same speed as the top and inter CPUus„ÄÇ HoweverÔºå they are more efficient„ÄÇ

 So in that wayÔºå you have to also consider certain devices are not as efficient„ÄÇ

 So if you use federated learningÔºå this could also be maybe yeah less efficient„ÄÇ

 But I think it's still interesting that people started looking at that because yeah„ÄÇ

T we also have to be aware of the environment and be resourceful when we do deep learning„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_3.png)

And related to thatÔºå I also saw there another new startup I think it's pronounced new reality„ÄÇ

 but I'm not exactly sure„ÄÇ So also one trend is really that people work on making chips for deep learning more efficient„ÄÇ

 So this would be yet another company that focuses on yeah deep learning chips or instead of GPUs like having specialized chips for deep learning and„ÄÇ

InThis articleÔºå they said basically the chip can perform inference with 15 times higher performance per dollar than the competition„ÄÇ

 Not that they don't say that the chip is faster than GPus„ÄÇ

 really here the focus is on the performance per dollar„ÄÇ but it's not a bad thing„ÄÇ I mean„ÄÇ

 maybe this chip overall is slower„ÄÇ but you can see 15 times higher performance per dollar„ÄÇ

 So per dollarÔºå it's not clear whether this means like manufacturing cost or also usage cost in terms of electricity„ÄÇ

 but I bet it's a mix of both„ÄÇ So it's probably a chip that is smaller„ÄÇ

 less powerful than a GPU for exampleÔºå but probably more energy efficient„ÄÇ but well see„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_5.png)

YeahÔºå so here I have some news which is I thinkÔºå yeah not such a good use of AI or deep learning„ÄÇ

 I find this always questionable like these types of applications„ÄÇ

 but I think it's still yeah important to talk about it„ÄÇ

 So here this is about a system trained to rate yeah applicants during job interviews„ÄÇ

I think also when people develop these systemsÔºå So the aim is really to make things more objective„ÄÇ

 like if you think of it as a humanÔºå you have probably some biases towards certain candidates and if you have an AI system„ÄÇ

 you may be hoping that this is lessÔºå let's say it's more objective„ÄÇ

 less subjective and probably also scales better so you can have more applicants but yeah„ÄÇ

 so I think I'm not sure but I think these are all developed in good faith„ÄÇ

 but they can also yeah be very problematic„ÄÇ So here was a questionable use of AI for„ÄÇ

Job applications„ÄÇ I also recommend you to read this full article„ÄÇ It was a very interesting analysis„ÄÇ

 It's a short articleÔºå but veryÔºå very well writtenÔºå so„ÄÇYeah„ÄÇ

 essentially here the researchers developed an AI to rate job applicants„ÄÇ

 so they trained the AI on videos from more than 12Ôºå000 people from different ages„ÄÇ

 gender and ethnic backgrounds and„ÄÇ

![](img/165d2a908592e15b2955520f44c6d7df_7.png)

An additional 2500 people rated how they perceived them in terms of personality dimensions and so forth„ÄÇ

And I think it's not quite clearÔºå but I think how this works is basically„ÄÇ

 they had people rating these job applicants and then„ÄÇ

Collected these scores and then trained the AI on these scores„ÄÇ And that way you can think of it„ÄÇ

 maybe the model yeah just mimics what the humans are doing during the rating„ÄÇ

 So they had like these different rating terms like opennessÔºå conscientiousnessÔºå extraversion„ÄÇ

 agreeableness and neuroticism and these were like other different labels„ÄÇFor the personal traits„ÄÇ

 YeahÔºå and then the researchers employed actors to do mock interviews„ÄÇ

 So keeping everything constant like speech and content of the text during the interview„ÄÇ

 and they also repeated that multiple times„ÄÇ So make sure that making sure that the results are stable and then they for example„ÄÇ

 varied different aspects„ÄÇ So hereÔºå for exampleÔºå they the actress was wearing glasses and comparing the results to not wearing glasses„ÄÇ

 And you can see in blue is the original without glasses„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_9.png)

And then in yellow with glasses and you can see based on these ratings by the AI that the squs for openness„ÄÇ

 conscientiousnessÔºå extratroversionÔºå agreeableness were less when the person was wearing glasses So this is also one of the reasons why I'm not wearing glasses when I record my lecture No I'm just kidding Now I think this is like a little bit problematic because I think wearing glasses or not shouldn't influence any decision right so this is maybe something that is not going so well here with this AI and even worse„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_11.png)

So where the results were even more differentÔºå they had a person doing a mock interview with and without a bookshelf in the background where everything else was constant„ÄÇ

 So and you can see that with a bookshelf„ÄÇ The squash improved or lot„ÄÇ

 So this is also a very questionable result by the eye„ÄÇ

 So there shouldn't be such a difference whether someone has a bookshelf in the background or not„ÄÇ

 And yeahÔºå this is maybe also a reason why I have a bookshelf in my background here„ÄÇ

 Now I'm just again kidding„ÄÇ But yeahÔºå so this is againÔºå it's actually a serious topic„ÄÇ

 I think this is something when you develop such systemsÔºå you should make sure that„ÄÇ

Such problems don't occur because I think this is incredibly yeahÔºå unfair because reallyÔºå I mean„ÄÇ

 it really shouldn't have anything to do with your skills as an applicant„ÄÇ

 whether what type of background you have and things like that„ÄÇ So yeahÔºå this is some„ÄÇ

 some very problematic result„ÄÇ And I think in general„ÄÇ

 maybe this is an area where AI shouldn't be employed„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_13.png)

YesÔºå so why does this problem happen„ÄÇ So there was one comment by Kaarina Swwike„ÄÇ

 which I think hits the nail on the head„ÄÇ It's the fundamental problem with face recognition by machine learning is that we never know exactly which pattern in an image these machines are responding to„ÄÇ

 So yeahÔºå deep learning can sometimes or often be more like a black box„ÄÇ

 So you never know really which parts of the image the machine learning system is using right„ÄÇ

 because you provide the whole image with background in everything„ÄÇ So do you really know„ÄÇ

What the machine learning is paying attention to„ÄÇI don't know if that solves the problem„ÄÇ

 but if you think of ZoomÔºå for exampleÔºå where you can easily switch these backgrounds„ÄÇ

 having these videos with data augmentationÔºå swapping different backgrounds may help at least with this bookshelf situation„ÄÇ

 but there of courseÔºå more issues to be addressed here„ÄÇ

 but yeah this is something if you develop systems you should probably be aware of that and test also things like that and make sure that yeah these these things are filtered out and not causing problems„ÄÇ

Especially in a real world application„ÄÇSo yeahÔºå related to looking into what deep learning does„ÄÇ

 how it behaves„ÄÇ So I also saw another interesting tool on called cockpit„ÄÇ It's also for Pytorch„ÄÇ

 So it's a practical debugging tool for training deep neural networks„ÄÇ

 I think it might be something that could be helpful in the class projectÔºå for instance„ÄÇ

So the authors say when engineers train deeping modelsÔºå they are very much flying blind„ÄÇ

 So here they offer a collection of instruments to look into the inner workings„ÄÇ

Of machine learning models„ÄÇ I don't know„ÄÇ I somehow screwed up this sentence here„ÄÇ

 So it's really useful for troubleshooting during training„ÄÇ And you can find a tool here on Github„ÄÇ

Just to show you one example hereÔºå so here they focused on the problem of learning rate tuning so usually when you want to find a good learning rate„ÄÇ

 you usually just try different things and see what works and yeah tune it by hand and see you make it larger and smaller and see whether it's better or not and then sometimes you also employ learning rate schedulers which decrease the learning rate over time„ÄÇ

 we will discuss that in a future lecture actually in two weeks„ÄÇ

But here's an example where the researchers looked at the loss curve„ÄÇ

 So they have mini batch training loss here plotted on the Y axis„ÄÇ

 So that's the loss per mini batch over the different iterations„ÄÇ and orange and blue„ÄÇ

 these two lines are corresponding to different learning rate settings„ÄÇNow„ÄÇ

 if they use a look at the lost landscape here in two dimensionsÔºå againÔºå it's kind of hard to see„ÄÇ

 but here in orangeÔºå they have one settingÔºå the orange one and in blueÔºå the other one„ÄÇSo here„ÄÇ

 what they want to illustrate is that the orange one„ÄÇIs only updating a little bit„ÄÇ

 and then it stops„ÄÇ So the learning rate is likely too small„ÄÇ

 and the blue one has a learning rate that is is too large„ÄÇ You can see it's overshooting„ÄÇ

So here would be the global minimum„ÄÇGlobal minimum„ÄÇ So they are overshooting here„ÄÇ

 So the blue learning rate is too large and the orange one is too small„ÄÇ and they offer some tools„ÄÇ

 for exampleÔºå this alpha distribution here„ÄÇAss a debugging tool„ÄÇ

 So this is like what they call a normalized step length and0 would be where you would directly be stepping into the right direction of the„ÄÇ

 yeahÔºå of the minimumÔºå the global minimumÔºå or I think it is local minimum in that context„ÄÇ

It wasn't actually quite clear„ÄÇ I think the paperÔºå I think the tool looks great„ÄÇ

 the paper wasn't my favorite to readÔºå it wasn't very well writtenÔºå maybe it wasn't well written„ÄÇ

 I didn't understand it properly„ÄÇ I don't know add a little bit of a hard time understanding certain things there„ÄÇ

 but what you want what theoretically would expect is stepping into this yeah into this direction directly„ÄÇ

 if you have a learning rate that does that it would be optimal„ÄÇAnd with this tool„ÄÇ

 you can see whether you are too smallÔºå like in the orange case or too large„ÄÇ

 and then you want to adjust it more to the middleÔºå SurpriinglyÔºå though„ÄÇ

 when they did some experiments on different data sets here„ÄÇ

 they found that the ones that resulted in the best generalization performance in terms of test accuracy were slightly above the zero„ÄÇ

 So here they found so these lines„ÄÇThese lines here correspond the solid lines on the„ÄÇ

Vertical lines hereÔºå they correspond to the models that have the best performance„ÄÇ

 And you can see those are„ÄÇAt least yeahÔºå I would say 0 or larger than0 in terms of this normalized step length here„ÄÇ

 They have the mean normalized step lengthÔºå so„ÄÇBut in any caseÔºå thisÔºå I think„ÄÇ

 might be a useful tool if you are tuning the learning rate„ÄÇ

 you can just maybe see where you are here compared to the0 and then maybe„ÄÇ

Focus on changing it to be somewhere in this region here„ÄÇ

 But this is something I haven't tried personally„ÄÇ So this is also something that's new to me„ÄÇ

 I usually like many other peopleÔºå tune the learning rate by hand„ÄÇ

 but it might be something useful to look at in the future„ÄÇ So with that„ÄÇ

 I think that's all the news I have„ÄÇ I think it's a long video„ÄÇ But yeahÔºå with that„ÄÇ

 let me end this video„ÄÇ And I will see you back in class next week„ÄÇ



![](img/165d2a908592e15b2955520f44c6d7df_15.png)