# P113ï¼šL14.2- ç©ºé—´ä¸¢å¤±å’Œ BatchNorm - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let's now learn how we can translate familiar concepts such as dropout and batchno to the convolutional settingsã€‚

 So these are also called spatial dropout and batchno and it sounds fancycier than it really is because you will see it's pretty straightforward and there's not much work required to make these things work for the two dimensional setting where with 2Dã€‚

 I mean two dimensional imagesã€‚So before we talked about dropout in the multilay perceptron contextã€‚

 So why do we even have to invent a new version of dropout or why do we have to modify it in the first placeã€‚

 So that is becauseï¼Œ I meanï¼Œ you canï¼Œ of courseï¼Œ use the regular dropout you learned about beforeã€‚

 but there's a good argumentã€‚

![](img/3de170346d52b5c63fca42854e817a53_1.png)

By Thompson here in this paper I linked below that the problem with regular dropout is that in scene endsã€‚

 you usually you have like images where you slide your kernels over the image and usually the adjacent pixels are likely highly correlated to each otherã€‚

 So if youã€‚Like a remove half of the pixels and a given in a givenã€‚Receptive fieldã€‚

 It doesn't really changeã€‚The output too muchï¼Œ except it's maybe like a little bit of a scaling differenceã€‚

 You can also think of itã€‚ for instanceï¼Œ if you have an inputã€‚ I meanã€‚

 we are not put masking anything in the inputï¼Œ but just conceptual itã€‚

 And let's think about it like thatã€‚ You have a phase imageã€‚Andã€‚Hereã€‚

 there are lots of pixels corresponding to the eyeã€‚ If you consider one pixel hereã€‚There areã€‚

Other pixels closely next to itã€‚ and all these pixels together represent an eyeã€‚

 So it doesn't really change that much if we mask half of these pixelsã€‚ So hereã€‚

 the argument is to instead of masking dropping out individual positions in a feature mapã€‚

 we are dropping out the channelsã€‚ So usually in the later stages of the networkã€‚

 these channels represent higher higher or larger concepts as we've talked about in the last last texture where we have bigger picture concepts like a channel represents the eye that was detected one the mouthã€‚

 one the nose and so forthã€‚ So here the idea is really to drop these higher order features in in that wayã€‚

 dropping entire feature maps instead of individual pixelsã€‚

 So essentially dropout to the is dropout applied to the channels rather than the pixelã€‚

So you would drop an entire channels instead of pixelsã€‚ And that's all there is to itï¼Œ soã€‚



![](img/3de170346d52b5c63fca42854e817a53_3.png)

How do we do that in Pywachï¼Œ It's pretty straightforwardã€‚ So instead of saying drop out 1 Dã€‚

 you are now saying drop out 2 dã€‚ And that's essentially itã€‚

 So heres an example showing you how that looks likeã€‚ So eachï¼Œ each box here represents one channelã€‚

 So here I have aã€‚Just some random example input with three channelsã€‚ and you can seeã€‚

Two of these channels are zeroed outã€‚ That's how spatial dropout worksã€‚ And that'sï¼Œ yeahã€‚

 all there is to itã€‚ It's not very complicatedã€‚

![](img/3de170346d52b5c63fca42854e817a53_5.png)

The same with batch norms instead of using batch norm 1 dï¼Œ which we used earlierã€‚

When we talked about multi layer perceptrons of fully connected layers for the convolution layersã€‚

 we use batchome 2Dï¼Œ' shown hereã€‚So just to briefly recapï¼Œ I don't want to explainã€‚ yeahã€‚

 batch again because we have a video for thatã€‚ But in the regular batch norm in the 1 D versionã€‚

 we were computing things for each featureã€‚ So we were computing the scma and better for each feature over the batch dimensionã€‚

 So if n is myã€‚Bch size hereã€‚We have an input that is two dimensionalã€‚ It is n times Mã€‚

 where let's say M is the number ofã€‚Featuresã€‚Soã€‚This is my input dimensionã€‚ So we had usuallyã€‚

 let's sayï¼Œ a table where we have different featuresã€‚ let's call themã€‚If oneã€‚2 and F3ã€‚

 So we have three features here andã€‚Qingã€‚ðŸ˜”ï¼ŒThis is oneã€‚Bch dimensionã€‚ So we were computing for eachã€‚

Featureå—¯ã€‚Gamma and betaã€‚ So they were allã€‚Differentenceã€‚Gams andetusã€‚Let's use yellowã€‚So we hadã€‚

 if we had three featuresï¼Œ we had three gammas and three betasã€‚

 Now we extend this concept here to the two dimensional case where we computeã€‚

These for inputs that are four dimensionalï¼Œ rightï¼Œ because we have nowã€‚The batch sizeã€‚

 we have the channelsã€‚ We have the heightï¼Œ and we have the widthã€‚So we compute the batch now overã€‚

The number of inputsï¼Œ height and widthã€‚ So in that senseï¼Œ we we combine theseã€‚ So weã€‚

 we average essentially over these andã€‚We have then the sameã€‚

 So the number of gammas and beta is the one corresponding to the number of channelsã€‚

 So if we have 64 channelsï¼Œ we have 64 gammas and 64 betas in that senseã€‚

 So we are computing it overã€‚The number of fetchesï¼Œ height and width hereã€‚



![](img/3de170346d52b5c63fca42854e817a53_7.png)

So yeahï¼Œ here is essentially a summary of what I just said just for your recordsã€‚

 if you want to look at it againï¼Œ Yeahï¼Œ and here's how we can do that in Pytorchã€‚

 So if this is my convolutional layer hereï¼Œ I just define the number ofã€‚



![](img/3de170346d52b5c63fca42854e817a53_9.png)

Inputã€‚Channels and the number ofã€‚

![](img/3de170346d52b5c63fca42854e817a53_11.png)

Outputã€‚Channels and then oopsã€‚ and then I applyã€‚

![](img/3de170346d52b5c63fca42854e817a53_13.png)

Btchome to that hereï¼Œ the number of parametersï¼Œ the number of bes and gammas of batchome is the number of output channelsã€‚

 So we have to put a 192 here because I have 192 output channelsã€‚ So I will have 192ã€‚Bes and 192ã€‚



![](img/3de170346d52b5c63fca42854e817a53_15.png)

Gmsã€‚All rightï¼Œ that is it in the next videoï¼Œ I will briefly recap that there are different architectures around thereã€‚

 and then we will talk about VGG16 and Resnetã€‚

![](img/3de170346d52b5c63fca42854e817a53_17.png)