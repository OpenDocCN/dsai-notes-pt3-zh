# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P71ÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞Èóª #6Ôºå2021 Âπ¥ 3 Êúà 7 Êó• - ShowMeAI - BV1ub4y127jj

YeahÔºå hiÔºå everyone„ÄÇ So this weekÔºå I have a bunch of handson stuff related to neural network training„ÄÇ

 SoÔºå first of allÔºå there was the new Pytorch releaseÔºå which I want to briefly talk about„ÄÇ And also„ÄÇ

 I saw a really cool article regarding tips for training neural networks„ÄÇ

 And since I'm currently grading your class projects„ÄÇ

 I thought it might be a good opportunity to go through some of these tips and tricks for yeah„ÄÇ

 helping with neural network training„ÄÇ So with that„ÄÇ

 let me get started with a stuff in the news for this week„ÄÇüòäÔºåYeah„ÄÇ

 probably the coolest thing that happened in the context of deep learning this week was that there was a new pytorch released this week„ÄÇ

 So I usually every like that if there's a new version that comes out„ÄÇ

 it's maybe every half a year or soÔºå because yeahÔºå there are usually a lot of cool new features and improvements that are useful in practice„ÄÇ

 And since this class is also focused on PytorchÔºå I thought it might be worth while mentioning some of these new additions„ÄÇ

üòäÔºåSo theres an article that they put together with the highlights in this new 1„ÄÇ8 release„ÄÇ

 So if you're interested this one summarizes most of the relevant changes„ÄÇ

 a more detailed list listing all the changes and additions is on Github so you can go here if you want the more detailed list„ÄÇ

 but it's a long list I think there were about 700 commits like feature changes of feature editions and things like that„ÄÇ

If I were to highlight a fewÔºå I have these three as I would sayÔºå my personal highlights„ÄÇ

 So one is that they knowÔºå yeahÔºå make it easier to use„ÄÇM D GPus„ÄÇ

 So competition is always good for businessÔºå right„ÄÇ

 So it's good that also other cards other than than videodia are supportedÔºå which I think„ÄÇ

It's nice for those people who haveÔºå let's sayÔºå a gaming PC or something like that with an M D graphics card„ÄÇ

 So now you can also use those graphics cards„ÄÇ So this is supported by a library called Rom„ÄÇ

 It's like KudaÔºå but for AM D GPUus„ÄÇThere was support for AM M D GPUus before„ÄÇ

 but it was like a little bit of a hassle you had to compile Pytorch yourself and was not so easy„ÄÇ

 And now what's new is that they are„ÄÇMaking the binaries directly available from the installer menu so you can now also yeah set it up more conveniently„ÄÇ

Now it's also possible to fit larger models onto GPUs without any external libraries„ÄÇ

 so recall last week we talked about fair scale and Microsoft Deeppe„ÄÇ

Which were two libraries Yeah providing functions to distribute a model across multiple GPus„ÄÇ

 So there were already something like a data parallel in Pythrch„ÄÇ So one is„ÄÇData parallel„ÄÇ

And one is distributed„ÄÇData perroll„ÄÇ Let's just call it a D P„ÄÇ And these were methods for splitting„ÄÇ

Many batches across multiple GPus and then training in parallel„ÄÇ

 But this wouldn't help with a problem where a model is too big for a given GPU„ÄÇ

 One method that we discussed was this checkpointing„ÄÇ

 but the checkpointing would be using a single GPU„ÄÇ

 Now there's a way that you can conveniently put a single model onto multiple GPus and then run it in parallel„ÄÇ

 I will show you an example„ÄÇ So that's actually pretty cool„ÄÇ

 So you don't need any external libraries for that„ÄÇ

 There are now some features that yeah support it directly in Pytorch„ÄÇ

And then also what's kind of nice or what will be nice in practice is that the torch Ln Allk module got extended„ÄÇ

 I think in this release it was even created„ÄÇ I was using a pre release version„ÄÇ

 so I'm not sure if it was actually created here or just modified„ÄÇ but in any case„ÄÇ

 so there are no additional„ÄÇYeahÔºå linear algebra functions that are usually only Ny„ÄÇ

 So now we don't have to switch so often between Ny and Pi to and we train on neural networks„ÄÇ

 If you need something like determinants or eigenvalues and so forth„ÄÇ

 So that's also making life more convenient„ÄÇ

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_1.png)

So regarding the pywa fineries for M D GPU support„ÄÇ I checked actuallyÔºå the installer menu„ÄÇ

 So good news isÔºå like I saidÔºå it's available nowÔºå it does not seem to be available yet though for Mac and Windows only for Linux right now„ÄÇ

 But I think stillÔºå this is pretty exciting because I'm pretty sure these will be also added over time„ÄÇ

 So it's probably just a matter of time„ÄÇ So things are developing in a nice direction where„ÄÇüòä„ÄÇ

You can now yeahÔºå directly install„ÄÇThe wheel my a pip for„ÄÇ

Rockm AMD support if you are on Linux at leastÔºå and I'm pretty sure Mac and Windows will probably also follow some time„ÄÇ



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_3.png)

By the wayÔºå I updated one of my computers with Pych 1„ÄÇ8 the other day„ÄÇ And yeahÔºå this„ÄÇ

 I was just looking at this„ÄÇ I was really surprised about the size„ÄÇ It's now 2„ÄÇ48 GB„ÄÇ

 so it's kind of impressive what's all in that library„ÄÇ

 So one thing I I'm not sure if I mentioned that in classÔºå but„ÄÇüòäÔºåSo when you install Pythr„ÄÇ

 the GPU or kuda versionÔºå it will bring its own kudaÔºå which is done to make things more convenient„ÄÇ

 So before that„ÄÇWhen I usedÔºå I meanÔºå like that was like 2015 things like that when I used Tensorflow before Kuda was bundled„ÄÇ

 you had to yeah install kuda on your graphics card and then link it when you install it„ÄÇ

 and that was usually very flak„ÄÇ It took a lot of effort to somehow make sure that„ÄÇYeah„ÄÇ

 the library is using the right kuda version on your computer„ÄÇ So this way„ÄÇ

 buttonling the installer here with the ka version the Quda toolkit here„ÄÇ

Makes it a little bit more convenient to install that„ÄÇ

 So you don't have to manually blink the the Pytoch library with Kuda and K D and N„ÄÇ

 But that also has the downside that this is rather large„ÄÇ I was just noticing it„ÄÇAnyways„ÄÇ

 so regarding the distributed training that I mentioned„ÄÇ

 where you can run multiple one model on multiple GPUus„ÄÇ

 So there are some MI additions that are listed here„ÄÇ There were a few more„ÄÇ

So they have the pipeline parallelism that I mentioned last week where yeah you just put a model on different you use a sequential model and then you put different parts of that model on different GPUus„ÄÇ

 and there's a utility to make that more convenient„ÄÇ I will show you in the next slide„ÄÇ

 So that looks like we already saw yeah last week a version using the fair scale„ÄÇ

 And there are also for data for distributed data parallel„ÄÇ there are also some additions„ÄÇAnd also„ÄÇ

 theres zero redundancy optimizerÔºå making the optimizers more efficient„ÄÇ So last week„ÄÇ

 we also mentioned briefly these types of things„ÄÇ and they also got added directly to„ÄÇYeah„ÄÇ

 to Pytorrch to the main library„ÄÇSo here here's just a brief overview of this model parallelism using multiple GPUus it's kind of related to yeah the pipeline version„ÄÇ

 So how that works is that you put different parts of a model„ÄÇ

 So if you split your model into four partsÔºå let's say you have four layers„ÄÇ

So here the F represents the forward pass„ÄÇ So let's say you„ÄÇRun the first layer on GPU„ÄÇ

 or you keep it on GPU 0„ÄÇ This one on GPU 1Ôºå GPU 2 and GPU 3„ÄÇ So this way„ÄÇ

 you avoid exceeding the memory of GPU 0 because you have each layer on a different GPU„ÄÇI mean„ÄÇ

 for a single or simple multi layer perceptionÔºå that's probably„ÄÇNot necessary„ÄÇ

 but if you later have larger networksÔºå I will show you an example that might make sense„ÄÇÂóØ„ÄÇYeah„ÄÇ

 and so here we have then one layer per GPU„ÄÇ and then FÔºå I think„ÄÇ

 is supposed to stand for forward pass„ÄÇ And then you yeahÔºå you have the backward pass similarlyÔºå but„ÄÇ

One downside of this approach is that you can seeÔºå so„ÄÇ

This one runs So the second GPU needs the results from this GPUÔºå rightÔºå So then„ÄÇIt's kind of„ÄÇ

GPU0 is keeping or staying idle while GPU1 runs and so forth„ÄÇSo in that way„ÄÇ

 where everything here under the area under the curve here„ÄÇ

This is like idle time for some of the GPus„ÄÇ So in this way you don't utilize three out of the four GPUus„ÄÇ

 and that it is basically kind of inefficient because things are sitting there„ÄÇ

 So one improved version is to use these types of micro batchches„ÄÇ So this is like illustrated here„ÄÇ

 So this already starts running while these finish up basically on different micro batchches„ÄÇ

 and then they are passed to the other GPU„ÄÇ And that way you kind of run certain things somewhat in parallel„ÄÇ

 Of courseÔºå theres still this bubble hereÔºå what they call bubble where there is inefficiency„ÄÇ

 but this is at least small improvement over the regular model parallelism„ÄÇ

 So this is kind kind of called pipeline execution„ÄÇ I don't know why„ÄÇThe word pipeline is used„ÄÇ

 but it might be because the toolÔºå I think the original one was called a G pipe„ÄÇ but yeah„ÄÇ

In any caseÔºå if you want to learn more about thisÔºå you can visit this link here and it will bring you directly to this website where I got this from„ÄÇ



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_5.png)

ActuallyÔºå I was kind of curious and wanted to try it and practice yesterday„ÄÇ

So I implemented a VG G16 network„ÄÇ It would actually fit onto my into a single GPU„ÄÇ

 But just for the sake of the exampleÔºå I just chose that model because it had different blocks„ÄÇ

 or I yeah coded it up in different blocks„ÄÇ And then I was putting it onto„ÄÇ

Or into this pipeline here„ÄÇSo what I did is I was writing up these blocks„ÄÇ So this is one„ÄÇ

Block in my modelÔºå I have five of these blocks„ÄÇAnd one block is one convolutional layerÔºå then a relu„ÄÇ

 another convolutional layerÔºå another re in the max pooling layer„ÄÇ

 We will talk more about that in the convolutional network section„ÄÇ

 But so you can what the main takeaways you can see here is that there are multiple things in one block„ÄÇ

 it can be pretty large if you have maybe even more channels„ÄÇ This would be I mean„ÄÇ

 not really large too large for your single GPU„ÄÇ but againÔºå it's just for the sake of this example„ÄÇ

 So at the bottom now I'm showing you how I use this new pipe in torch distributed pipeline sync„ÄÇ

There are actually a few more„ÄÇSet up steps that I'm skipping here because they didn't fit on the slide„ÄÇ

 But if you' are interested in checking out this code that I wasÔºå yeah playing around with„ÄÇ

 I put it here up on Gitthub so you can actually check that out and see the full code and how it looks like„ÄÇ

 So what I'm doing here is„ÄÇI'm putting the first block on the first GPU„ÄÇ So Kuda 0 is the first GPU„ÄÇ

 I was actually running something else on GPU 1„ÄÇ So I was skipping it and I was putting things on GPU2 here„ÄÇ

 GPU 3Ôºå and then the classifier back on to GPU 0„ÄÇ Why did I do that„ÄÇ

So one of the reasons is that you have to have the data„ÄÇ

And the code that you're running on the same GPU„ÄÇ So if you have your input data on GPU 0„ÄÇ

 you should also have your first block on GPU0„ÄÇ And then if you have your class labels for computing the accuracy on GPU 0„ÄÇ

 Then you also want to have the output of the model on GPU 0„ÄÇ That is what I was doing here„ÄÇ

 So I didn't have to rewrite any of my other codeÔºå I could just plug that in into my existing code„ÄÇ

So then yeah I'm using so I'm having these blocks now on different GPUs it has nothing to do with the pipe yet and what I'm doing is I'm using the sequential in torch to merge these blocks into one model and then I'm using just this model inside the pipeline here with8 microbetches and then I'm providing it here for Adam or to Adam as the parameters and this is essentially it and then this one would actually then run„ÄÇ

The pipeline mechanismÔºå like putting it every everything together and work kind of smoothly„ÄÇAnd yeah„ÄÇ

 it worked well in practice„ÄÇ I must say the training time in this case was about half as slow compared to running everything on a single GPU„ÄÇ

 But againÔºå speed is not the goal here„ÄÇ So we are not using this pipeline mechanism to speed up the model„ÄÇ

 We are using it because we assume that this single model would not fit onto a single GPU„ÄÇ

 So this way we make possible a training of models that are exceeding a regular GPU memory„ÄÇ

 So I think this is actually pretty cool and I will„ÄÇ

Pretty surely use that in one of my research projects„ÄÇ



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_7.png)

YeahÔºå okayÔºå there was one more modification I had to make to the code„ÄÇ

 So the modification is in these two lines„ÄÇ So this whole thing is my training follow over the epochs and over the mini batches„ÄÇ

 And here I had to add just this line because now the model is not returning the logicts„ÄÇ

 It's returning R ref„ÄÇReferenceÔºå a reference object to this R PCC thingy„ÄÇ And I you just had to„ÄÇ

 yeah„ÄÇGet the local valueÔºå and then everything else should work„ÄÇ Again„ÄÇ

 the full code example is here if you are interested to checkÔºå check it out„ÄÇ



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_9.png)

AlrightÔºå so this was just a few highlights from this Pytoch 1„ÄÇ

 a release some other topics I saw this paper on archive this week virus AmnesIS„ÄÇ

 So I just thought it might be a cool new fun simple data for benchmarking yeah deep learning models„ÄÇ

 So if you are just interesting in developing a new model anyone to see how well it performs and different data sets Amness is one Scipher is one nowadays also imagenet and things like that so but I thought this is here a interesting approach because I haven't seen anything like that before„ÄÇ

So„ÄÇThis is an emist like dataset set„ÄÇ So if people say something ems„ÄÇ

 they usually mean a data that is approximately 10 classes and 50000 or 60000 training images„ÄÇ

And hereÔºå what they have is a data set concerning virus data or malware so„ÄÇThere are 10 classes here„ÄÇ

 So class label 0 and 9„ÄÇ There's only one class that is not malware„ÄÇ It's called bannerware„ÄÇ

 iss like the good stuff here„ÄÇ that's just like a regular ex file„ÄÇ I guess putty„ÄÇ

 I think it's I think it's a terminal„ÄÇProgram for WindowsÔºå but I'm not sure anyways„ÄÇ

 And then there are„ÄÇSome other viruses hereÔºå I think X windows„ÄÇAnd„ÄÇInstead„ÄÇ

 so they had like a C Z file and some information about that„ÄÇ And instead of putting that„ÄÇ

Up as the CV fileÔºå what they did is they„ÄÇI meanÔºå it's at least what I understood is they„ÄÇ

Converted to grayscale images using the intermediate net B PM text format„ÄÇ

 And then they created AskI raw images and some did something else to make it or to create Jpe files„ÄÇ

 So how it looks like is this one„ÄÇ So you can see basically these yeahÔºå JpeEC images of these files„ÄÇ

 And this one would beÔºå for exampleÔºå the good one and all the other ones are examples of malware„ÄÇ

 I haven't seen like approach like this where people yeahÔºå kind of convert text„ÄÇ I mean„ÄÇ

 code is essentially text„ÄÇ or hereÔºå I think they used„ÄÇThe check sums of the files„ÄÇ But yeah„ÄÇ

 there was an interesting idea like convertingÔºå let's say text or a string into an image so that they can use convolutional networks„ÄÇ

 I'm actually not sure if that makes it better„ÄÇ So if you have a text file and was some weird„ÄÇ

Input format that doesn't look like a tableular datasetÔºå but also not like anything else„ÄÇ

 not sure whetherre converting into a JPEG really is meaningful„ÄÇ

 but I thought that was interesting they actually got pretty good performance or acuracies on that dataset„ÄÇ

 so I think it's probably working„ÄÇ

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_11.png)

YeahÔºå I also saw this cool article this week called Simple considerations for Simple People Building fancy neural networks„ÄÇ

 So this is an article highlighting some of the things you should think about when building a model or also when debuing a model„ÄÇ

 And since you are now working on your class projects„ÄÇ

 I thought it was a nice summary really that I wanted to share with you like explaining going over some tips in practice„ÄÇ

 So the first step would be putting aside machine learning and simply focusing on your data„ÄÇ

 So before you start applying a model to your data„ÄÇ

 just take a look at your data and get a feeling for it„ÄÇ for example„ÄÇ

 looking at whether the labels are balanced like getting a feeling for that„ÄÇ

 whether they're balanced or not like on the same ratio for the class labels„ÄÇüòäÔºåSame proportions„ÄÇ

 So and are the gold labels that you are there gold labels that you don't agree with„ÄÇ

 So gold labels hereÔºå they mean ground truth„ÄÇ So are the test set or training set labels that are provided making sense„ÄÇ

 Are there are some maybe that are wrong„ÄÇ Do you agree with them„ÄÇThen also„ÄÇ

 it's always useful to know how the data was obtainedÔºå whetherÔºå for example„ÄÇ

 there could be possible sources of noise in this process„ÄÇ So it can tell you an anecdote„ÄÇ

 for exampleÔºå there was forÔºå I like it was like one or two years ago„ÄÇ

 There was a face image data set from IBM„ÄÇ I think it was called the I„ÄÇF or something like that„ÄÇ

It was yeahÔºå a phase data set shared by I IBM„ÄÇ and I applied to get access to this data set„ÄÇ

 and I was very excited to get it„ÄÇ but then„ÄÇüòäÔºåI wrote I read the readme file and was actually pretty disappointed because they shared a lot of yeah attributes that can be used for developing machine learning systems„ÄÇ

 but then all these attributes were not assigned by a human like not labeled they were actually predicted using another machine learning model So if you develop a machine learning model based on labels predicted by another model you would assume you can never actually do better than an existing model and that I find not very useful for the development of machine learning methods necessarily so in a way it's also good to think a little bit about or find out how labels were obtained„ÄÇ

And yeahÔºå then also thinking about the preprocessing steps„ÄÇ

 maybe there are some things that were not ideal„ÄÇs just good to yeah think about these how diverse are the examples And also what rulebased algorithm would perform decently on this problem„ÄÇ

 So likeÔºå can you think of a rule-based algorithm in terms of simple decisions that you might think of that can actually perform well and then you can implement these simple rules as a baseline because you want to see that the model that you are training performs better than any simple baseline„ÄÇ

 for exampleÔºå So for exampleÔºå rule-based algorithm on email spam data could be„ÄÇ

If the subject header is capitalized in all capsÔºå then it's spamÔºå something like that„ÄÇ

 something simpleÔºå maybe„ÄÇ

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_13.png)

YeahÔºå so while the previous slide focused on understanding the data set so the second recommendation is about yeah understanding how difficult the task is„ÄÇ

 so how well would standard baselines perform on that taskÔºå for example„ÄÇ

 a classification task So if you are working on a classification task it's always a good idea„ÄÇ

To also include a simple baseline like logistic regression as a linear classifier because let's say you suppose you develop a fancy neural network„ÄÇ

 let's say a convolutional network that gets 95% accuracy on MNS„ÄÇBy itself„ÄÇ

95% might sound super impressiveÔºå but then let's say you run logistic regression„ÄÇ

 And usually if you do that on Amness„ÄÇYou would see you will get around 93% performance already well accuracy already„ÄÇ

 so 93% accuracy and 95% accuracy that's not very different in that way„ÄÇ

 maybe the CNNN is not as good as you might think So having a baseline like logistic regression is actually a good idea because maybe even you find if you train CNNN or multi perceptron it's like only 90% And if you only know the number 90% accuracy and you tell someone I have a model that can predict let's saywritten digits and Mist with 90% and a person doesn't know Ms just looks at some examples person might be super impressed but then„ÄÇ

If you think about a logistic regression as a linear model already being able to do that„ÄÇ

 then it's actually not that impressive after all„ÄÇYeahÔºå so just going through these points here„ÄÇ

 how would a random predictor performÔºå especially in classification problems and the data sets can be unbalanced I'm actually a little bit confused by this point I think the main point here is asking how well a random predictor might perform that is actually something I also always ask myself So if you have a binaryclass problem yeah the a random prediction would be 50% right and for three classes it's 33% so it's always good to keep that in mind by a random prediction might look like„ÄÇ

ÂóØ„ÄÇWhat I'm a little bit confused about is this point if the data set is unbalanced„ÄÇ I mean„ÄÇ

 this shouldn't really affect the random predictorÔºå rightÔºå so because„ÄÇIf you haveÔºå let's say„ÄÇ90%„ÄÇ

Sam and 20%„ÄÇNot„ÄÇSmÔºå and this is your data set„ÄÇBinary data„ÄÇ

 the random prediction predict predictor would be 50% accurate right because for each email it would either randomly predict spam or not spam so you get a 50% accuracy And if the data is balanced„ÄÇ

 it's also 50% for random predictionÔºå so it should always be 50% if you have a binary data set or around 50% let's say„ÄÇ

I think what the person here was more interested in in terms of data set can be unbalanced is majority„ÄÇ

Class„ÄÇPredicctor„ÄÇBecause then it gets interestingÔºå for exampleÔºå on a balanced binary data set„ÄÇ

 a majority class predictor would get 50% accuracyÔºå but if you have a data set like that„ÄÇ

 where you have 80% spam and 20% not spam„ÄÇThen some classifier that would always predict the majority class if the test has these tables„ÄÇ

 you would already get„ÄÇ80% accuracyÔºå just by always predicting spam„ÄÇ So in that case„ÄÇ

 if you develop a machine learning system that getsÔºå let's say„ÄÇ82% accuracy„ÄÇ

 that's not very impressive because by always predicting the majority classÔºå you can already get 80%„ÄÇ

 So that's actually also something worthwhile to think about when you develop a model„ÄÇ

So then the next thing isÔºå what would the loss look like for a random predictor„ÄÇ

So this is also related to„ÄÇOne of the quiz questions I had last week recall when I asked you to compute the cross entropy for the worst case scenario which was yeah infinity„ÄÇ

 but let's say if you have a case in where you always yeah predict randomly so random classifier„ÄÇ

Would have a loss of„ÄÇ1 over the number of class labelsÔºå right„ÄÇ

So this would be the maximum or the worst case„ÄÇ and if you don't have an infinity a0 in the lock„ÄÇ

The totally random prediction would be„ÄÇActuallyÔºå it should probably let's sayÔºå number of classes„ÄÇ

Times minus lock one over the number of classes„ÄÇ So this would be a totally random prediction„ÄÇ

 I think it's for binary„ÄÇ it's around what would it be like 2„ÄÇ3 something„ÄÇ

 but I'm not sure I would have to double check„ÄÇBut it's always good to double check these numbers when you do training and you see the loss gets stuck at a certain point„ÄÇ

To see what would be the loss for a random prediction„ÄÇ And this way„ÄÇ

 you can find out whether the model was learning something at all or not„ÄÇYeah„ÄÇ

 some other points are what are the best metrics for measuring performanceÔºå for example„ÄÇ

 precision recall accuracy„ÄÇYeahÔºå and„ÄÇyeahÔºå the last one is also interesting„ÄÇ

 Are there architectures in my neural network tool box that would be good to model the inductive bias of the data„ÄÇ

 Inductive biasÔºå we briefly talked about this„ÄÇ It's like one of these machine learning jargon terms„ÄÇ

For exampleÔºå yeah that goes actually back to the previous point understanding the data set„ÄÇ

 so for instanceÔºå if you have time series data it might be better to use a recurrent neural network or if you have image data it might be good to use a conversion network and so forth„ÄÇ

 so also youre understanding your data set„ÄÇ

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_15.png)

YeahÔºå assume now you understand the data set and also the difficulty of the task„ÄÇ

But you find that the model is not performing wellÔºå for example„ÄÇ

 the loss that you observed during training the cross entropy loss is the one that you would expect for a random prediction„ÄÇ

 So there might be something wrong with your code and that might be a good opportunity for doing some model debugging to see whether there is some error„ÄÇ

 So one common thing is usually if you have a learning rate that is way too large or way too small„ÄÇ

 then you will probably see that yeah the model is not learning well„ÄÇIf you„ÄÇPersonally„ÄÇ

 that's what I try first„ÄÇ If you tweak the learning rateÔºå though„ÄÇ

 but and you still find things are not performing well„ÄÇ

 what I also like to do is I try to overfit a small batch of examples„ÄÇ

 There was actually really helpful in a recent research project where we had actually an issue where we used this technique to diagnose it„ÄÇ

 so„ÄÇWhen you only do training on a single mini batch„ÄÇ

 what you would expect is that the model should overfi to this mini batch„ÄÇ

 and you should get like a small loss like around 0„ÄÇ So how you can do that is just by adding„ÄÇ

When you do in the data loadÔºå if you do like for„ÄÇLet's say„ÄÇBtch and„ÄÇLabels„ÄÇIn„ÄÇData lot„ÄÇ

When you do that and after the first„ÄÇTraining loopÔºå I would add a break at the end„ÄÇ So in this way„ÄÇ

 you only do it onceÔºå and then you do it for multiple epochs„ÄÇ And this way„ÄÇ

 you can try to overfit to a single mini batch„ÄÇ And you should get around zero loss„ÄÇ If not„ÄÇ

 there is something weirdÔºå usually„ÄÇ So here are achieved from that article„ÄÇ few things to look into„ÄÇ

For exampleÔºå one could be you forgot to call model evil in the valuation mode„ÄÇ

 We will talk more about this„ÄÇ actually when we talk about dropout this week„ÄÇ

 But yeah that would be one case or when you have„ÄÇWell you forgot to model0 grad„ÄÇ

 so you will otherwise yeah accumulate if you doÔºå if you don't do that„ÄÇ

 you will accumulate the gradients from the previous rounds„ÄÇ

 And then if you update that could also be some issues because you don't do gradients you do something with accumulated gradients„ÄÇ

Or there's maybe something wrong with the preprosing of the inputs„ÄÇ

 maybe you forgot to normalize them„ÄÇAnother common one is using the wrong arguments in the loss„ÄÇ

 So recall when we said that the cross entropy loss in Pywach yeah works with the logicits„ÄÇ

 So if you provide it with the softm probabilities it will give you very weird results„ÄÇ

 so that's also one thing to check for„ÄÇWell here initialization doesn't break symmetry that is related to the question I asked you when I said why don't we initialize all the weights in a neural network to0 that is because we have symmetry that was actually a nice piazza post by one of the students who answer this question so I recommend you to check out this post because yeah it was very well written and essentially explaining the problem very well so that is actually a symmetry if we initialize the network with all zeros or all small numbers that are the same So what we want is we want to have small randomly different numbers to initialize the network to break this symmetry„ÄÇ

Or here another one is some parameters are never called during the forward pass„ÄÇ

 so if you don't use them during the forward passÔºå they won't be updated because they are not part of the computation graph„ÄÇ

Another one that happens often to me is if I have two models simple in a single script„ÄÇ

Actually happened to me when I was implementing this pipeline parallelism„ÄÇ So I had my atom„ÄÇ

OptimizerÔºå or let's sayÔºå I think weÔºå yeahÔºå we haven't covered Adam„ÄÇ

 let's say we I had my SGD optimizer„ÄÇ We will cover Adam later„ÄÇ

 and you pass it the model parameters like this„ÄÇAnd„ÄÇI had two models in that notebook„ÄÇ

 I had just called it model„ÄÇ And the other 1Ôºå I called model„ÄÇParallel„ÄÇ

But when I was training the parallel modelÔºå I used„ÄÇSlow model parameters in the SGD„ÄÇ

 So it was not actually updating the right model parameters„ÄÇ And then the training didn't work„ÄÇ

 And I was wonderingÔºå why is this model not training„ÄÇ And then I sawÔºå ohÔºå I„ÄÇ

 I didn't give it the right parameters„ÄÇYeahÔºå another one here is the learning rate is taking funky values like zero„ÄÇ

 so there's something called learning rate schedule„ÄÇ



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_17.png)

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_18.png)

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_19.png)