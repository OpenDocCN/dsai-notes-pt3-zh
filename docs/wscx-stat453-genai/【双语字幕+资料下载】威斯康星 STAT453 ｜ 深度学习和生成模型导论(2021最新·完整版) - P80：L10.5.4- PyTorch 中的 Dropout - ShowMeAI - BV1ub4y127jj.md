# P80ï¼šL10.5.4- PyTorch ä¸­çš„ Dropout - ShowMeAI - BV1ub4y127jj

Al rightï¼Œ finallyï¼Œ let's talk about how we can implement dropout in practiceã€‚

 That is how we use drop out in Pytorchã€‚

![](img/3c5cd0a856eb936b143add6a293ab378_1.png)

So but before I can do thatï¼Œ I have to explain one more thingã€‚ It's called inverted dropoutã€‚

 but don't worry that's actually a very simple conceptã€‚ It is hopefully simple to explain tooã€‚

So and thereï¼Œ I wanted to explain that becauseï¼Œ in factã€‚

 nowadays most frameworks implement inverted dropoutï¼Œ including Pywachã€‚

 So what you are going to use in Pywach is called inverted dropoutã€‚So what is inverted robotã€‚

 So if you recall in an earlier videoï¼Œ I mentioned that we are scaling the activations during testing by a factor of 1 minus P rightã€‚

 because during trainingã€‚Withã€‚We dropã€‚Notesï¼Œ rightã€‚ And in testingï¼Œ this is in regular robotã€‚

 we scaleã€‚Activationsã€‚Byeã€‚ðŸ˜”ï¼ŒF of1 minus pã€‚And that is because during trainingã€‚

 when we drop the notesï¼Œ if you think of the net inputsã€‚They will beã€‚

Smaller than the net inputs during testingï¼Œ assuming all the net inputs are positive for simplicityã€‚

 And so because other units in the network expect these activations on a certain scaleã€‚

 the scale of the activations during testing would be too large if we don't drop any nodeï¼Œ rightã€‚

 Because during testingï¼Œ we don't drop any nodeï¼Œ But then we have to adjust the magnitude of these activationsã€‚

 So we're scaling by 1 minus P where P is theã€‚Dropã€‚Probabilityã€‚Okayã€‚

 so this is what we do in regular dropout now in inverted dropoutã€‚

 we still drop notes during trainingï¼Œ but we what we don't do is we don't scale the activations during testing anymoreã€‚

Soï¼Œ insteadã€‚So instead of doing this during testingï¼Œ the scalingï¼Œ we do it already during trainingã€‚

Soï¼Œ we doã€‚During training this scaling by a factor of 1 minus pã€‚

 and then we don't have to do anything during testing because then the activations will be on the expected scale during testing automatically if we already scale during trainingã€‚

So why do we do thatï¼Œ why do we do the inverted dropoutï¼ŸI'm not 100% sureã€‚

 but I believe this trend was started back then in 2015 in Tensorflow thats was that at least was the first time I've seen itã€‚

 So I've seen it in Tensorflowã€‚ And I think my theory is that if you have a big companyï¼Œ let's sayã€‚

 like Googleã€‚You do a lot of predictions rightï¼Œ you have millions or billions of people using let's sayã€‚

 Google search and if your Google search engine relies on a neural network with dropoutã€‚

 it would be very expensive to scaleã€‚The predictions for each userã€‚ I meanã€‚

 the scanning is a simple operationã€‚ It's just one multiplicationï¼Œ rightã€‚

 But if you have a billion search queriesï¼Œ you have toã€‚

Think about having 1 billion additional operations right So that wayã€‚

 I think to save computational resources during prediction timeï¼Œ during inferenceã€‚

 people are scaredd during training so that they don't have to do anything during testingã€‚

 that's at least my theoryã€‚In any case both dropout or inverted dropout should give you exactly the same resultsã€‚

 it's just a matter of whether we perform the scaling during training or during testingã€‚

 and it happens that most frameworks nowadaysï¼Œ including Pywach use actually inverted dropoutã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_3.png)

Yeahï¼Œ and here's a code snippet illustrating how we can use dropout in Pytoch in the sequential layerã€‚

 I have the full code example also in a Jupyter notebookã€‚ If you are interested to take a look at itã€‚

 I have a link somewhere later in my slidesï¼Œ but I also will post it again on canvas like alwaysã€‚

 So but here here's the essential part for dropoutã€‚

So here what I have is a multi layer perception with two hidden layersã€‚And okayã€‚

 this is the first layerï¼Œ first hidden layerã€‚ and this is theã€‚Second hit layerã€‚ and I'm just addingã€‚

 you can see I'm just adding dropout hereã€‚After the activationsã€‚So why after the activationsï¼Œ Wellã€‚

 because we we want to zero out the activationsï¼Œ rightï¼Œ But in the case of Reluã€‚

 it doesn't really matter whether we have it before or after Relu becauseã€‚If the input to Red is 0ã€‚

 then the output of Red is also0ï¼Œ rightï¼Œ because recallã€‚How re looks like it looks likeã€‚Thisã€‚

 howeverï¼Œ it would make a difference if we haveã€‚Let's say logistic syigmoidã€‚

 because for the logistic syigmoidã€‚At0ï¼Œ we we have this S shaped curveï¼Œ rightã€‚

I'm not good at drawing thisã€‚We have this S shaped curve for the logistic sigmoidã€‚

 And if the input is 0ï¼Œ then we will get actually a pretty large output of 0ã€‚5ã€‚ So usuallyã€‚For Reluã€‚

 it doesn't matter whether we have drop out before or afterï¼Œ but for other activation functionsã€‚

 it might matterã€‚ So it's always good to have drop out after the activation just to be consistentã€‚

Any caseã€‚ So yeahï¼Œ this is how we use dropoutã€‚ So the drop probability is just some hyperparmeter I also provided here as inputã€‚

 Yeahï¼Œ one more thing maybe to mention here is we don't use dropout for the output layerã€‚

 Why is thatï¼Ÿ Wellï¼Œ because it doesn't really make senseï¼Œ rightï¼Œ if we haveï¼Œ let's sayã€‚

10 class labelsã€‚ And I drop three of the nodes corresponding to the class labels that would be a little bit awkwardã€‚

 rightï¼Œ So we don't want to drop any class label predictionsã€‚

 We only want to drop hidden activationsã€‚Yeahï¼Œ this is essentially how you can modify a neural network to use dropoutã€‚

Alrightï¼Œ so alsoï¼Œ I maybe my not thing to mentionï¼Œ you don't have to use the same probability here and thereã€‚

 You can use different probabilities alsoã€‚

![](img/3c5cd0a856eb936b143add6a293ab378_5.png)

Yeahï¼Œ one more modification you have to make in the code though is you have to make pay attention that you use model train and model Eval when you do the model training and the testing so I mentioned this earlier when we talked about Pythch that this is importantã€‚

 I always do that no matter whether I have dropout or not just to make sure that theres no unwanted side effect or no unwanted behaviorã€‚

So why is that requiredï¼ŸThis will essentially set the model or tell the model that it's currently in training modeã€‚

 So it will automatically use dropoutã€‚ So during training modeã€‚

 it will actually use these drop out thingsã€‚ Howeverï¼Œ during model evaluationï¼Œ during testing hereã€‚

 it will essentiallyã€‚

![](img/3c5cd0a856eb936b143add6a293ab378_7.png)

![](img/3c5cd0a856eb936b143add6a293ab378_8.png)

![](img/3c5cd0a856eb936b143add6a293ab378_9.png)

Skip dropout rightï¼Œ because we don't want to use dropout during testing or predictionã€‚

 So when we set our model to Eval modeï¼Œ Eval stands for evaluationï¼Œ then it will not use dropoutã€‚

 So it can essentially just skip dropout or you can also think of just setting the drop probability to 0ã€‚

0ã€‚ So it won't drop anythingã€‚ So this is importantã€‚

 So you have to really make sure you use train and evalã€‚ And not only if you use dropboardã€‚

 I would always recommend doing that because yeahï¼Œ it doesn't hurt rightã€‚

 So it's always a good practice to using thoseã€‚

![](img/3c5cd0a856eb936b143add6a293ab378_11.png)

![](img/3c5cd0a856eb936b143add6a293ab378_12.png)

![](img/3c5cd0a856eb936b143add6a293ab378_13.png)

![](img/3c5cd0a856eb936b143add6a293ab378_14.png)

You have the full code example for that here on GithHub if you want to check it outã€‚

 so here I train this multilay perceptionceptron on MNã€‚

 so at the top I have no dropout so you can see the training loss on the left side as the training loss for 50 epochs it goes downã€‚

And here on the right hand sideï¼Œ this is the accuracy on the training and validation setã€‚

 and you can see the training set accuracy goes up like expected during training and the validation accuracy also goes up but what you can see here is that at some point you will have a overfitting gap hereã€‚

 So the model without dropout starts to overfit at some point here at the bottom I added 50% dropoutã€‚

 So this is exactly the model I was showing you here with 50% dropout now So you can see the accuracy improvesã€‚

 but here we don't have any overfitting anymoreã€‚ So we are reducing the overfitting I should say there is one little caveat though if I computed the test set accuracyã€‚

 This was like 97ã€‚5% approximately and this was only 96ã€‚5%ã€‚

 So maybe I was doing too much dropout here at the bottomã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_16.png)

![](img/3c5cd0a856eb936b143add6a293ab378_17.png)

Cause I reduced overfittingï¼Œ but at the same timeï¼Œ I also yeahã€‚

 maybe added too much of a regularization penalty here using dropout so that also that yeah the performance overall role is a bit lowerã€‚

 But yeah and essentially this is how dropout worksï¼Œ you can play around with thatã€‚ And yeahã€‚

 with thatï¼Œ maybe some more practical tipsã€‚ So I recommend not using dropout if your model does not overfitã€‚

 So I would maybe just train the model and see how it behavesã€‚ if it doesn't overfitã€‚

 then there's no need to use use dropoutã€‚ But if you see that there's overfittingã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_19.png)

![](img/3c5cd0a856eb936b143add6a293ab378_20.png)

Like you can see hereï¼Œ then it's worthwhile actually adding dropoutã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_22.png)

In practiceï¼Œ alsoï¼Œ the creators of dropout recommend essentiallyã€‚

 if you observe that your model does not overfitï¼Œ you should increase the capacity of your modelã€‚

 adding more parameters until it overfis and then adding drop out to itã€‚ So essentiallyã€‚

 they are arguing thatã€‚It's better to design a model that overfis and then add drop out compared to designing a model that does not overfitã€‚

 So overallï¼Œ they expect to see thatã€‚You will get a better performance if you make a model largerã€‚

 such as it overfits and then use dropout to reduce the overfittingã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_24.png)

Yeahï¼Œ there's also you may have already had this idea while listening to this lectureã€‚

 there's also a related ideaï¼Œ randomly dropping weightsã€‚

 So instead of dropping the activations in dropoutã€‚

 we can also actually randomly drop dropping weightsã€‚

 So if you had this idea as a potential research idea I have to unfortunately disappoint youã€‚

 Someone has already worked on thisã€‚ It's called droprop connectã€‚

So it has it's more like a general generalization of dropboard because yeahï¼Œ you can think of itã€‚

 it can if youã€‚Let's sayï¼Œ if you considerã€‚Let'd say this note hereã€‚

 if you drop this weight and this weightï¼Œ both weightsï¼Œ if you drop bothã€‚

 then it's essentially the same as dropping this activationã€‚

 So that way you can think of drop connect as a generalization of drop outã€‚ but yeahã€‚

 in a more general senseï¼Œ it's just dropping weight randomlyã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_26.png)

So the paper for that is can be found hereã€‚Unfortunatelyï¼Œ or I'm not sure if it's unfortunatelyã€‚

 unfortunately it doesn't really matterã€‚ but it happens that this is something I find that works not so well in practiceã€‚

 I long time agoï¼Œ I tried it onceã€‚ I found that dropout works betterã€‚

 and also I really don't see it often being used in practiceã€‚ So I think it's justã€‚

 less popular than dropoutï¼Œ maybe because it doesn't yeah perform so well as drop outã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_28.png)

Yeah to close this lectureï¼Œ I have a reading recommendation for youã€‚ the original dropout paperã€‚

 It's actually a really nice paperï¼Œ veryï¼Œ I would say intuitive and nicely written and very accessibleã€‚

 It doesn't have any or not much mathematical jargonï¼Œ which is niceã€‚

So if you want to practice reading research papers it's actually a good one to start with and yeah also happy Women's Day if you're watching this today so today is the International Women's Day and with that let me close this lecture and next time we will talk about some weight initialization techniques and also then we will talk about learning rates later and different optimizers so essentially other techniques for improving neural network trainingã€‚



![](img/3c5cd0a856eb936b143add6a293ab378_30.png)