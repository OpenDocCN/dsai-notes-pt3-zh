# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P80ÔºöL10.5.4- PyTorch ‰∏≠ÁöÑ Dropout - ShowMeAI - BV1ub4y127jj

Al rightÔºå finallyÔºå let's talk about how we can implement dropout in practice„ÄÇ

 That is how we use drop out in Pytorch„ÄÇ

![](img/3c5cd0a856eb936b143add6a293ab378_1.png)

So but before I can do thatÔºå I have to explain one more thing„ÄÇ It's called inverted dropout„ÄÇ

 but don't worry that's actually a very simple concept„ÄÇ It is hopefully simple to explain too„ÄÇ

So and thereÔºå I wanted to explain that becauseÔºå in fact„ÄÇ

 nowadays most frameworks implement inverted dropoutÔºå including Pywach„ÄÇ

 So what you are going to use in Pywach is called inverted dropout„ÄÇSo what is inverted robot„ÄÇ

 So if you recall in an earlier videoÔºå I mentioned that we are scaling the activations during testing by a factor of 1 minus P right„ÄÇ

 because during training„ÄÇWith„ÄÇWe drop„ÄÇNotesÔºå right„ÄÇ And in testingÔºå this is in regular robot„ÄÇ

 we scale„ÄÇActivations„ÄÇBye„ÄÇüòîÔºåF of1 minus p„ÄÇAnd that is because during training„ÄÇ

 when we drop the notesÔºå if you think of the net inputs„ÄÇThey will be„ÄÇ

Smaller than the net inputs during testingÔºå assuming all the net inputs are positive for simplicity„ÄÇ

 And so because other units in the network expect these activations on a certain scale„ÄÇ

 the scale of the activations during testing would be too large if we don't drop any nodeÔºå right„ÄÇ

 Because during testingÔºå we don't drop any nodeÔºå But then we have to adjust the magnitude of these activations„ÄÇ

 So we're scaling by 1 minus P where P is the„ÄÇDrop„ÄÇProbability„ÄÇOkay„ÄÇ

 so this is what we do in regular dropout now in inverted dropout„ÄÇ

 we still drop notes during trainingÔºå but we what we don't do is we don't scale the activations during testing anymore„ÄÇ

SoÔºå instead„ÄÇSo instead of doing this during testingÔºå the scalingÔºå we do it already during training„ÄÇ

SoÔºå we do„ÄÇDuring training this scaling by a factor of 1 minus p„ÄÇ

 and then we don't have to do anything during testing because then the activations will be on the expected scale during testing automatically if we already scale during training„ÄÇ

So why do we do thatÔºå why do we do the inverted dropoutÔºüI'm not 100% sure„ÄÇ

 but I believe this trend was started back then in 2015 in Tensorflow thats was that at least was the first time I've seen it„ÄÇ

 So I've seen it in Tensorflow„ÄÇ And I think my theory is that if you have a big companyÔºå let's say„ÄÇ

 like Google„ÄÇYou do a lot of predictions rightÔºå you have millions or billions of people using let's say„ÄÇ

 Google search and if your Google search engine relies on a neural network with dropout„ÄÇ

 it would be very expensive to scale„ÄÇThe predictions for each user„ÄÇ I mean„ÄÇ

 the scanning is a simple operation„ÄÇ It's just one multiplicationÔºå right„ÄÇ

 But if you have a billion search queriesÔºå you have to„ÄÇ

Think about having 1 billion additional operations right So that way„ÄÇ

 I think to save computational resources during prediction timeÔºå during inference„ÄÇ

 people are scaredd during training so that they don't have to do anything during testing„ÄÇ

 that's at least my theory„ÄÇIn any case both dropout or inverted dropout should give you exactly the same results„ÄÇ

 it's just a matter of whether we perform the scaling during training or during testing„ÄÇ

 and it happens that most frameworks nowadaysÔºå including Pywach use actually inverted dropout„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_3.png)

YeahÔºå and here's a code snippet illustrating how we can use dropout in Pytoch in the sequential layer„ÄÇ

 I have the full code example also in a Jupyter notebook„ÄÇ If you are interested to take a look at it„ÄÇ

 I have a link somewhere later in my slidesÔºå but I also will post it again on canvas like always„ÄÇ

 So but here here's the essential part for dropout„ÄÇ

So here what I have is a multi layer perception with two hidden layers„ÄÇAnd okay„ÄÇ

 this is the first layerÔºå first hidden layer„ÄÇ and this is the„ÄÇSecond hit layer„ÄÇ and I'm just adding„ÄÇ

 you can see I'm just adding dropout here„ÄÇAfter the activations„ÄÇSo why after the activationsÔºå Well„ÄÇ

 because we we want to zero out the activationsÔºå rightÔºå But in the case of Relu„ÄÇ

 it doesn't really matter whether we have it before or after Relu because„ÄÇIf the input to Red is 0„ÄÇ

 then the output of Red is also0Ôºå rightÔºå because recall„ÄÇHow re looks like it looks like„ÄÇThis„ÄÇ

 howeverÔºå it would make a difference if we have„ÄÇLet's say logistic syigmoid„ÄÇ

 because for the logistic syigmoid„ÄÇAt0Ôºå we we have this S shaped curveÔºå right„ÄÇ

I'm not good at drawing this„ÄÇWe have this S shaped curve for the logistic sigmoid„ÄÇ

 And if the input is 0Ôºå then we will get actually a pretty large output of 0„ÄÇ5„ÄÇ So usually„ÄÇFor Relu„ÄÇ

 it doesn't matter whether we have drop out before or afterÔºå but for other activation functions„ÄÇ

 it might matter„ÄÇ So it's always good to have drop out after the activation just to be consistent„ÄÇ

Any case„ÄÇ So yeahÔºå this is how we use dropout„ÄÇ So the drop probability is just some hyperparmeter I also provided here as input„ÄÇ

 YeahÔºå one more thing maybe to mention here is we don't use dropout for the output layer„ÄÇ

 Why is thatÔºü WellÔºå because it doesn't really make senseÔºå rightÔºå if we haveÔºå let's say„ÄÇ

10 class labels„ÄÇ And I drop three of the nodes corresponding to the class labels that would be a little bit awkward„ÄÇ

 rightÔºå So we don't want to drop any class label predictions„ÄÇ

 We only want to drop hidden activations„ÄÇYeahÔºå this is essentially how you can modify a neural network to use dropout„ÄÇ

AlrightÔºå so alsoÔºå I maybe my not thing to mentionÔºå you don't have to use the same probability here and there„ÄÇ

 You can use different probabilities also„ÄÇ

![](img/3c5cd0a856eb936b143add6a293ab378_5.png)

YeahÔºå one more modification you have to make in the code though is you have to make pay attention that you use model train and model Eval when you do the model training and the testing so I mentioned this earlier when we talked about Pythch that this is important„ÄÇ

 I always do that no matter whether I have dropout or not just to make sure that theres no unwanted side effect or no unwanted behavior„ÄÇ

So why is that requiredÔºüThis will essentially set the model or tell the model that it's currently in training mode„ÄÇ

 So it will automatically use dropout„ÄÇ So during training mode„ÄÇ

 it will actually use these drop out things„ÄÇ HoweverÔºå during model evaluationÔºå during testing here„ÄÇ

 it will essentially„ÄÇ

![](img/3c5cd0a856eb936b143add6a293ab378_7.png)

![](img/3c5cd0a856eb936b143add6a293ab378_8.png)

![](img/3c5cd0a856eb936b143add6a293ab378_9.png)

Skip dropout rightÔºå because we don't want to use dropout during testing or prediction„ÄÇ

 So when we set our model to Eval modeÔºå Eval stands for evaluationÔºå then it will not use dropout„ÄÇ

 So it can essentially just skip dropout or you can also think of just setting the drop probability to 0„ÄÇ

0„ÄÇ So it won't drop anything„ÄÇ So this is important„ÄÇ

 So you have to really make sure you use train and eval„ÄÇ And not only if you use dropboard„ÄÇ

 I would always recommend doing that because yeahÔºå it doesn't hurt right„ÄÇ

 So it's always a good practice to using those„ÄÇ

![](img/3c5cd0a856eb936b143add6a293ab378_11.png)

![](img/3c5cd0a856eb936b143add6a293ab378_12.png)

![](img/3c5cd0a856eb936b143add6a293ab378_13.png)

![](img/3c5cd0a856eb936b143add6a293ab378_14.png)

You have the full code example for that here on GithHub if you want to check it out„ÄÇ

 so here I train this multilay perceptionceptron on MN„ÄÇ

 so at the top I have no dropout so you can see the training loss on the left side as the training loss for 50 epochs it goes down„ÄÇ

And here on the right hand sideÔºå this is the accuracy on the training and validation set„ÄÇ

 and you can see the training set accuracy goes up like expected during training and the validation accuracy also goes up but what you can see here is that at some point you will have a overfitting gap here„ÄÇ

 So the model without dropout starts to overfit at some point here at the bottom I added 50% dropout„ÄÇ

 So this is exactly the model I was showing you here with 50% dropout now So you can see the accuracy improves„ÄÇ

 but here we don't have any overfitting anymore„ÄÇ So we are reducing the overfitting I should say there is one little caveat though if I computed the test set accuracy„ÄÇ

 This was like 97„ÄÇ5% approximately and this was only 96„ÄÇ5%„ÄÇ

 So maybe I was doing too much dropout here at the bottom„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_16.png)

![](img/3c5cd0a856eb936b143add6a293ab378_17.png)

Cause I reduced overfittingÔºå but at the same timeÔºå I also yeah„ÄÇ

 maybe added too much of a regularization penalty here using dropout so that also that yeah the performance overall role is a bit lower„ÄÇ

 But yeah and essentially this is how dropout worksÔºå you can play around with that„ÄÇ And yeah„ÄÇ

 with thatÔºå maybe some more practical tips„ÄÇ So I recommend not using dropout if your model does not overfit„ÄÇ

 So I would maybe just train the model and see how it behaves„ÄÇ if it doesn't overfit„ÄÇ

 then there's no need to use use dropout„ÄÇ But if you see that there's overfitting„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_19.png)

![](img/3c5cd0a856eb936b143add6a293ab378_20.png)

Like you can see hereÔºå then it's worthwhile actually adding dropout„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_22.png)

In practiceÔºå alsoÔºå the creators of dropout recommend essentially„ÄÇ

 if you observe that your model does not overfitÔºå you should increase the capacity of your model„ÄÇ

 adding more parameters until it overfis and then adding drop out to it„ÄÇ So essentially„ÄÇ

 they are arguing that„ÄÇIt's better to design a model that overfis and then add drop out compared to designing a model that does not overfit„ÄÇ

 So overallÔºå they expect to see that„ÄÇYou will get a better performance if you make a model larger„ÄÇ

 such as it overfits and then use dropout to reduce the overfitting„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_24.png)

YeahÔºå there's also you may have already had this idea while listening to this lecture„ÄÇ

 there's also a related ideaÔºå randomly dropping weights„ÄÇ

 So instead of dropping the activations in dropout„ÄÇ

 we can also actually randomly drop dropping weights„ÄÇ

 So if you had this idea as a potential research idea I have to unfortunately disappoint you„ÄÇ

 Someone has already worked on this„ÄÇ It's called droprop connect„ÄÇ

So it has it's more like a general generalization of dropboard because yeahÔºå you can think of it„ÄÇ

 it can if you„ÄÇLet's sayÔºå if you consider„ÄÇLet'd say this note here„ÄÇ

 if you drop this weight and this weightÔºå both weightsÔºå if you drop both„ÄÇ

 then it's essentially the same as dropping this activation„ÄÇ

 So that way you can think of drop connect as a generalization of drop out„ÄÇ but yeah„ÄÇ

 in a more general senseÔºå it's just dropping weight randomly„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_26.png)

So the paper for that is can be found here„ÄÇUnfortunatelyÔºå or I'm not sure if it's unfortunately„ÄÇ

 unfortunately it doesn't really matter„ÄÇ but it happens that this is something I find that works not so well in practice„ÄÇ

 I long time agoÔºå I tried it once„ÄÇ I found that dropout works better„ÄÇ

 and also I really don't see it often being used in practice„ÄÇ So I think it's just„ÄÇ

 less popular than dropoutÔºå maybe because it doesn't yeah perform so well as drop out„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_28.png)

Yeah to close this lectureÔºå I have a reading recommendation for you„ÄÇ the original dropout paper„ÄÇ

 It's actually a really nice paperÔºå veryÔºå I would say intuitive and nicely written and very accessible„ÄÇ

 It doesn't have any or not much mathematical jargonÔºå which is nice„ÄÇ

So if you want to practice reading research papers it's actually a good one to start with and yeah also happy Women's Day if you're watching this today so today is the International Women's Day and with that let me close this lecture and next time we will talk about some weight initialization techniques and also then we will talk about learning rates later and different optimizers so essentially other techniques for improving neural network training„ÄÇ



![](img/3c5cd0a856eb936b143add6a293ab378_30.png)