# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P54ï¼šL8.4- Logits å’Œäº¤å‰ç†µ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ since we talked so much about logistic regression now I thought that might be a good opportunity now to introduce two terms logicits and cross entropy because that's what I will also use yeah quite often later in this class and that's because also it's a very common these are two common terms in the deep learning literature I sometimes refer to it as the deep learning jargon because yeah we sometimes use these terms a little bit differently in statistics compared to deep learning so I just wanted to briefly clarify how they relate to the concepts we just discussed if this doesn't make complete senseã€‚

 I mean we will be using these terms later and you will then have something to refer back toã€‚



![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_1.png)

Okayï¼Œ soã€‚In deep learningï¼Œ when we have multi layerer neural networks that's say I will just draw a very simple one hereã€‚

We will actually talk about them on Thursdayã€‚ So if we have a multi layer perceptioncept like thisã€‚

Everything is connected to everything hereã€‚ It's a fully connected layerã€‚

 And then we have this one output node andã€‚In this networkã€‚

 we also have net inputs similar to Adeline or logistic regressionï¼Œ but now we have two net inputsã€‚

 one is hereï¼Œ let's call that Z1 and one is here at Z2ã€‚ we will talk more about that on Thursdayã€‚

But yeahï¼Œ indeed deep learningï¼Œ it's very common to call this here the net inputs that come before the output to call themã€‚

ğŸ˜”ï¼ŒThe lodgesã€‚In statisticsï¼Œ the logics have a specific meaningã€‚

 This is the logarithm of the odds or lockotsã€‚Andã€‚In the context of logistic regressionã€‚

 which we just coveredï¼Œ these los are naturally the net inputsã€‚So where does that come fromã€‚

 This is the inverse of our logistic sigmite functionã€‚Oopsã€‚That someoneã€‚

Logistic sigitete function hereã€‚ justï¼Œ I just abbreviated S Pã€‚ So the lock of P over 1 minus Pã€‚

 this function is the inverse ofã€‚This functionã€‚And yeahï¼Œ so indeed deep learningï¼Œ thoughã€‚

It does not necessarily any relationship between the lock os and yeah the net inputs of the last there Its just likeã€‚

 I think it's derivedã€‚ the term is derived from logistic regressionã€‚

 People call that logicits in the context of logistic regression and then just generalize this wording to arbitrary multilayer perceptrons where we may not even have a sigmoid functionã€‚

 as we will see laterã€‚ So you can just think of it as the net inputs of the odd of the last layer basicallyã€‚



![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_3.png)

So here here just for a referenceã€‚ So here on the left hand sideã€‚

 the logistic sigoid function and here on the right hand sideï¼Œ the logicitã€‚

 So the logicit function is the inverse of the logistic sigoid functionã€‚

 So basically how it looks likeã€‚ I don't think I have to go into do that into that in much detail because we have seen that beforeã€‚

 and this is essentially just flippedã€‚

![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_5.png)

So there is another concept that is maybe a little bit confusing but it's exactly what we've covered before actually we had a seminar at UW last week where we also yeah it was briefly mentioned coincidentally there was like a question whether it's the same the negative log likelihood and cross entropy so yeah the negative log likelihood and the binary cross entropy equivalent and in practice in deep learning people just say cross entropy multicategory cross entropy which would be multi-class version of the negative log likelihood which we will cover later in this lecture when we talk about the softm functionã€‚

Soã€‚Just to keep it briefï¼Œ the negative look likelihood that we just covered a few videos ago is the same as what people call the binary cross entropyã€‚

 They were just formulated in different contextsã€‚ So negative look likelihood comes more like fromã€‚

 I think it's likeã€‚It's probably from a statistics contextã€‚ I don't know the first paperã€‚

Or reference that mentioned thatã€‚ But this is something usually I see in statistics papers and the binary cross entropyã€‚

Thing has originated from the field of information theoryã€‚All computer scienceã€‚

So we have actually seen that or not the cross entropyã€‚

 but we have seen the self entropy or just entropy in statistics 4ï¼Œ5ï¼Œ1 for those who took this classã€‚

In fall semester where we had used the entropy function in the context of the information theory and decision trees that we used the lock to instead of the natural algorithmã€‚

 but yeahï¼Œ it's kind of somewhat related if you have taken any class where you talkedï¼Œ for exampleã€‚

 about the KL divergenceã€‚Or coolbe labla divergenceã€‚

 which measures the difference between two distributionsã€‚ The K L divergence is essentiallyã€‚

The crossã€‚Entropyã€‚Mus theã€‚Self entropyã€‚Of course you don't have to know that it's just like a fun tidbit here the only thing you have to know is or should know because it's useful to know is that the negative look likelihood is the same as the binary cross entropyã€‚

 this is like a useful thing to know and this is yeah what we've discussed in the previous videos and there is also a multicatego version that is the multicategory cross entropy which is just a generalization of the binary cross entropy to multiple classesã€‚

Soï¼Œ in order to make thatã€‚Negative look likelihood or binary cross entropy work for multiple classesã€‚

We assume a so-called one hot encoding where the class tables are either0 or one for some reason it was cut off hereã€‚

 But this is something weï¼Œ of courseï¼Œ haven't discussed yetã€‚ And if this doesn't make sense yetã€‚

 we will actually discuss that after the logistic regression code exampleã€‚

 when I will introduce the multinial logistic regression modelã€‚ So againã€‚

 all I wanted to say here is the logicits and deep learning usually refer to the net inputs of the layerã€‚

That just comes before the outputã€‚And the term binary cross entropy and negative look likelihood are essentially the sameã€‚

 All rightï¼Œ so in the next video I will show you a logistic regression code exampleã€‚

 and then we will take a look at this multi category cross entropyã€‚



![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_7.png)

![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_8.png)