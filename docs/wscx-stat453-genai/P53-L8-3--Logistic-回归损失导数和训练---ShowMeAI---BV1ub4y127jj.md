# P53ÔºöL8.3- Logistic ÂõûÂΩíÊçüÂ§±ÂØºÊï∞ÂíåËÆ≠ÁªÉ - ShowMeAI - BV1ub4y127jj

Yeah let's now take a look at how we train a logistic regression model„ÄÇ

 by that I really mean now computing the gradients of the loss function with respect to the weight and bias parameters in the logistic regression model so that we can then use gradient descent to update the model and then in a later video we will see how we implement that encode to the actual practical training„ÄÇ



![](img/e27658390d5d2e3c3ac7e6c8b59df939_1.png)

So yeahÔºå let's take a look at the logistic syigmoid function again„ÄÇ so here on the left hand side„ÄÇ

 that's the logistic syigmoid activation function that we talked about before where we have the net input on the x axis and the output on the y axis„ÄÇ

 it has this syigmoided shape„ÄÇAnd you can already see„ÄÇJust by looking at this function„ÄÇ

 what the gradients might be„ÄÇ So you can see here the function on the ends it's relatively flat„ÄÇ

 so the gradient should be also very flat like0 around that region and the steepest part is here„ÄÇ

 so this should also be where the gradient is the largest or let's say the partial derivative with respect to the input is the largest„ÄÇ

Now„ÄÇüòîÔºåYeahÔºå on the left on the right hand sideÔºå I was yeah„ÄÇ

 computing the derivative of the sigmoid function with respect to its input„ÄÇSoÔºå yeahÔºå if youÔºå yeah„ÄÇ

 use the simple calculus rules„ÄÇ So it's how you would compute the derivative of this partÔºå right„ÄÇ

 because you have the derivative of the„ÄÇWhat is it called numeratorÔºü

Which is E to the power of minus zÔºå the one cancels„ÄÇ

 And then you have this square term in the denominator„ÄÇAnd when you solve that„ÄÇ

 I'm not doing it here„ÄÇ So when you solve thatÔºå if you're interested„ÄÇ

 I've done that in the Python machine learning bookÔºå I think it's in chapter 12 somewhere„ÄÇ

 but I can also share that if you are really curious about that„ÄÇ

 but you can probably also find examples of on the Internet„ÄÇ It's quite involved„ÄÇ

 It will take you a few sheets of paperÔºå I guess„ÄÇBut what the result is is this one here on the right hand side that's the derivative of the logistic sigmoid function„ÄÇ

 And you can see it's relatively simpleÔºå rightÔºå it's just the sigmoid function times one minus the sigmoid function right So that's what I mean„ÄÇ

 it has nice derivatives„ÄÇ That's actually one little yeah not so nice part about this activation function or it's derivative„ÄÇ

 we will talk about that in a future lecture„ÄÇ So what's not so nice about it is actually that yeah these these parts here are of the derivative are actually 0„ÄÇ

 So here we don't have any yeah power to update the function„ÄÇ So if these terms are0„ÄÇ

 this might be a problem when we have multilayer neural networks and then yeah have a chain rule where part of the chain rule is0 because then the whole„ÄÇ

Loss gradient becomes 0„ÄÇ but this is something we will talk about later„ÄÇ

 So let's now just focus on how this function looks like„ÄÇ So in the right lower side here„ÄÇ

 I have the derivative of this logistic sigite function„ÄÇ

 And you can see like by S with our intuition hereÔºå this part„ÄÇWhere has e is 0„ÄÇ

That's whether gradient is the highest or the partial derivative is the highest„ÄÇ

 You can see the partial derivative is 0„ÄÇ25Ôºå the largest value„ÄÇ

 And then for very negative and very positive inputs the derivative goes down to yeah 0„ÄÇ It's yeah„ÄÇ

 you can see that here where it's also very flatÔºå right„ÄÇ

 So this is how the logistic sigmoid derivative looks like„ÄÇ



![](img/e27658390d5d2e3c3ac7e6c8b59df939_3.png)

So and here is how the loss function looks like„ÄÇüòîÔºåSoÔºå at the bottom„ÄÇHere„ÄÇThis isÔºå againÔºå my„ÄÇNegative„ÄÇ

LÔºå likelihood„ÄÇLoss I talked about in the previous video„ÄÇ and meÔºå like we talked about before„ÄÇ

 there are two partsÔºå reallyÔºå of this loss function„ÄÇ

 You can really think of it as the part when the class label is one and the part when the class label is 0„ÄÇ

 So here we really talk about the„ÄÇTrue label in the datasetÔºå the label of the training example„ÄÇ

 And this is yeah also the true label„ÄÇ So these are the true labels provided in the training dataset„ÄÇ

 AgainÔºå we only computeÔºå I meanÔºå this might trivialÔºå but just to recap it„ÄÇ

 We only compute the loss function„ÄÇ during trainingÔºå not during prediction„ÄÇ

 right when we have a test set or something and do a prediction„ÄÇ we don't use the loss„ÄÇ

 It's really just to find the weight parameters of the model„ÄÇ

 So we are given these true labels from the training set„ÄÇSo what happens ifÔºüThe true label is one„ÄÇ

 let's let's consider this scenario of the true label is one„ÄÇ

 We have a one here right on on the bottom and we have one here and here if we have a one here on the right hand side„ÄÇ

Then the whole term here becomes0Ôºå and then we have0 times something„ÄÇ So this whole term cancels„ÄÇ

 So if the true label is oneÔºå we really only focus on this part here„ÄÇIf the true label is one„ÄÇ

 then our loss function is minus lock„ÄÇAnd I can just write this part as aÔºå the activation„ÄÇ

Its minus lock of the activation„ÄÇNo„ÄÇIf my label is0„ÄÇThen„ÄÇ

Let me erase this as maybe a little bit messy here„ÄÇWe set minus log of a„ÄÇ

 and for this right hand partÔºå if y is 0Ôºå then this part will be 0Ôºå0 times something cancels„ÄÇ

Here we have a 0„ÄÇ So this will be one„ÄÇ So if„ÄÇThe true label is0Ôºå then the loss function is„ÄÇ

Minus lock„ÄÇ1 minus„ÄÇP„ÄÇNow let's take a look at the shape of this curves here„ÄÇIf we let me use„ÄÇ

 I should have maybe used yellow minus lock„ÄÇ1 minus a„ÄÇ So it matches this orange or yellow line here„ÄÇ

 So you can see for the value here of the sigmoid function„ÄÇ

 So this is really like the logistic sigmoidÔºå the class membership probability„ÄÇ

 And recall this one is for„ÄÇMaybe use maybe the black color„ÄÇThis is for„ÄÇThe class table equals one„ÄÇ

 so the class membership probability for one„ÄÇLast one„ÄÇ so this is really like this probability here„ÄÇ

So if what we want so what we want is„ÄÇThis probability to be high if the true label is1„ÄÇ

And we want this probability to be low if the probability is 0„ÄÇ so„ÄÇ

 but we can also write this as1 minus„ÄÇY equal equal0„ÄÇThis is it's easier to think about„ÄÇÂóØ„ÄÇSo let's„ÄÇ

 let's take a look at now what happens if this probability here is high„ÄÇ

 So if this whole term is highÔºå this means„ÄÇIf we have a low probabilityÔºå that this is indeed class 0„ÄÇ

SoÔºå if we have a„ÄÇTerm hereÔºå we have a low probability for this one„ÄÇ

This one will have the whole term with a1 minus included will have a large probability„ÄÇ

 and you can see„ÄÇWe have a high loss in this caseÔºå so if we have a probability for class1„ÄÇ

 so really like the class„ÄÇOne probability„ÄÇIf this is high at oneÔºå then we have a very large loss„ÄÇ

 you can see thatÔºå so the loss approaches infinity„ÄÇIf we have a high probability„ÄÇVice versa„ÄÇThelo„ÄÇ

Approaches0 if we have a low probability rightÔºå so you can see that„ÄÇOn the closer„ÄÇ

 I get to a low probability„ÄÇThe closer I get to a low probability that the classable is one„ÄÇ

The smaller the lossÔºå because a low probability for class 1 means a high probability for class 0„ÄÇ

 which is what we want„ÄÇSoÔºå here„ÄÇIf the trueÔºå if the true label is indeed zero„ÄÇ

 we want a low probability for class 1„ÄÇSo here we have a zero loss„ÄÇ

 if we have a zero probability for class 1 because then the probability for y equals0 will be large because of this relationship here„ÄÇ

The same way nowÔºå let's take a look at„ÄÇLet's use the blue color„ÄÇ

 the case when the class table is one the same way when we have a low probability for class membership one„ÄÇ

We have a high loss it approaches infinityÔºå rightÔºüBecauseuse„ÄÇI mean„ÄÇ

 we will never really have a probability of 0„ÄÇ We will hopefully perhaps something like„ÄÇVery„ÄÇ

 very small because otherwise it will crash our lock function„ÄÇ or yeah„ÄÇ so we will have„ÄÇ

Lock of minus log of„ÄÇ0Ôºå right„ÄÇ So will be a very small number„ÄÇ And then this approach oops„ÄÇ

This loss approaches infinityÔºå basically„ÄÇIf we have a low probability and we have a high loss„ÄÇ sorry„ÄÇ

 we have a high„ÄÇIf we have a high probabilityÔºå the loss will be very low„ÄÇ That's what I want to say„ÄÇ

 So the takeaway here is reallyÔºå that is how our loss function looks like„ÄÇ

 What we want is we want to have a high probability for the right class and a wrong probability„ÄÇ

 like a low probability„ÄÇFor the right class or a high probability for the wrong class„ÄÇ consequently„ÄÇ

 will result in a very large loss„ÄÇ And you can see it's like steeply increasing here„ÄÇ

 So that's the takeaway„ÄÇ So there's a large penalty for making a wrong prediction„ÄÇ



![](img/e27658390d5d2e3c3ac7e6c8b59df939_5.png)

YeahÔºå how does the learning rule now look likeÔºå I mean„ÄÇ

 this is the same gradient descent rule that we used before when we trained a linear regression model or add a line using gradient descent for that one„ÄÇ

 what we needed was the derivative of the loss function„ÄÇWith respect to the weights„ÄÇ

 So that's the partial derivative of the loss functionÔºå with respect to the weight„ÄÇ

 And we also compute the partial derivative of the loss function with respect to the bias„ÄÇ

 It's the same concept„ÄÇSo in order to do thatÔºå we can compose it into three terms„ÄÇ

 It's the same as we done we have done for adeline and linear regression„ÄÇ

So we compute the derivative of the loss with respect to the output of the activation function„ÄÇ

Then we compute the derivative of the activation function with respect to the input for Adeline„ÄÇ

 this was just one because the activation function was yeah identity function„ÄÇ

And then what we also compute is the derivative of the net input with respect to the weight„ÄÇ

 And that was just xÔºå rightÔºå X J similar to Adeline„ÄÇSo hereÔºå the bottom„ÄÇI've written this out„ÄÇ

 So all these three terms„ÄÇ So this oneÔºå by the wayÔºå this is yeahÔºå the negative look likelihood„ÄÇ

What loss that we talked about in the previous slideÔºå and if you look at this„ÄÇ

 the only thing that's really different from Adeline is„ÄÇThat these and these values areÔºå of course„ÄÇ

 different because we have a different loss function in a line„ÄÇWe had the MEÔºå right„ÄÇ

 where the derivative„ÄÇ I saw the ME wasÔºå for example„ÄÇWritten like this„ÄÇ

Let's say like this with a one half in front„ÄÇ And we can also do the one over nÔºå if you like„ÄÇBut„ÄÇ

OkayÔºå so let's do it„ÄÇ And then„ÄÇWe bring the to up front for the derivativeiveÔºå so it's„ÄÇ2 over to n„ÄÇ

 So the two cancels„ÄÇSoÔºå we just have„ÄÇ1 over„ÄÇAnd„ÄÇTimes„ÄÇYeahÔºå just my my mine„ÄÇ sorryÔºå it should be„ÄÇ

Y hat minus yÔºå so that would be the derivative of our MSE„ÄÇNowÔºå of courseÔºå we don't have an MSE„ÄÇ

 We have this negative log Ne for which we have the following derivative with respect to the activation function„ÄÇ

With respect to its input and„ÄÇThis one for Adeline„ÄÇWhat's one„ÄÇ

And this one is the same because the net input is the same„ÄÇ

Now the only thing we have to do really is to put those togetherÔºå right„ÄÇ

 we have to just multiply them using the chain rule so it's the same like for Adeline for Adeline just looked simpler„ÄÇ

 but yeah for Adeline we just had one over n„ÄÇTimes way head„ÄÇThat's y times„ÄÇX JÔºå rightÔºü

 So there was one for L line for the logistic one„ÄÇ It may look more complicated because we have now this term in this term„ÄÇ

 HoweverÔºå theres something nice that's happening„ÄÇ

![](img/e27658390d5d2e3c3ac7e6c8b59df939_7.png)

SoÔºå we can actually„ÄÇI'm not sure what I've done hereÔºå we can actually simplify this„ÄÇThis part„ÄÇ

 and this partÔºå when we put them together„ÄÇIt simplifies„ÄÇTo this one here„ÄÇ

 So you can work out the math if you like„ÄÇ But it's yeah a simpleÔºå very simple derivative of here„ÄÇ

 So you can see that these cancel each other nicely„ÄÇ And this is why I said„ÄÇ

That the logistic regression or the sigmoid functions has nice derivatives„ÄÇThat's because„ÄÇ

Things cancel here nicely„ÄÇAnd then we just add the x to it and you can see actually this looks exactly like the Adeline learning rule right„ÄÇ

 This looks exactly like the linear regression one right„ÄÇ

 isn't that quite cool So here is just from the previous slides that we had on aline and linear regression So this was the stochastic gradient descent learning rule here where we had the initial weights„ÄÇ

 the follow loop over the training epochs and then the follow loop of the training examples that could also be for mini batchs then we compute the prediction„ÄÇ

üòä„ÄÇ

![](img/e27658390d5d2e3c3ac7e6c8b59df939_9.png)

The gradients here„ÄÇAnd then we do the update hereÔºå and then we have the learning rate etta here„ÄÇ

We can use exactly the same„ÄÇRule for logistic regression Also note there was also a discussion on Piazza„ÄÇ

We can actually writeÔºå rewrite this because I think I've done it the other way around in code„ÄÇ

 We can actually rewrite this as„ÄÇOf courseÔºå like y hat minus yÔºå right„ÄÇ

' the same thing because with minus and minus„ÄÇ

![](img/e27658390d5d2e3c3ac7e6c8b59df939_11.png)

And that is what I had in the previous slide„ÄÇ If I go backÔºå I have a minus y instead of y minus a„ÄÇ

 So I should say that a is„ÄÇ

![](img/e27658390d5d2e3c3ac7e6c8b59df939_13.png)

Equal to y head here„ÄÇ so that's the output of the logistic regression model„ÄÇÊâÄ‰ª•„ÄÇüòî„ÄÇ

And my head are the same thing here„ÄÇSo let me write just a here„ÄÇüòîÔºåSo„ÄÇBut I mean„ÄÇ

 we can also write thisÔºå maybe it's if it's more clear„ÄÇI can't erase more„ÄÇ

Let's maybe you write this as„ÄÇA minus„ÄÇWei„ÄÇüòîÔºåExtreme„ÄÇAnd this one should be just„ÄÇ„ÅàÊÄù„ÅÑ„Åæ„Åô„ÄÇüòî„ÄÇ

So this is exactly the same learning rule as for Analine„ÄÇ

 What's the difference now why is this logistic regression Well the big difference here is that a is computed differently right So a is now computed by„ÄÇ

Non nonlinear functionÔºå whereas for linear regressionÔºå it was or Adeline„ÄÇ

 it was just computed as the identity function„ÄÇ So in the case of Adeline or linear regression„ÄÇ

 this was the net input„ÄÇSoÔºå but yeahÔºå that isÔºå again„ÄÇ

 why I said it has nice derivatives because we don't even have to change our learning rule„ÄÇ

 The only thing we have to change is how why head or„ÄÇBasicallyÔºå a is computed„ÄÇ



![](img/e27658390d5d2e3c3ac7e6c8b59df939_15.png)

AlrightÔºå so few last things about this„ÄÇ So here we can think of this as the whole computation graph again„ÄÇ

 this is our logistic regression computation graph„ÄÇ We haven't talked about this output yet right so„ÄÇ

We do the training„ÄÇHereÔºå so here's„ÄÇWhere we compute the error„ÄÇ So we have a true„ÄÇLet's say„ÄÇüòî„ÄÇ

A true labeled„ÄÇIt's called that true level y„ÄÇThat goes in here„ÄÇAnd„ÄÇüòîÔºåHere„ÄÇüòî„ÄÇ

What comes out is our A or Y hat„ÄÇ Let's call it Y hat„ÄÇ

It's actually a probabilityba that it's maybe more clear to call it a„ÄÇ So we have our a„ÄÇAnd oh while„ÄÇ

True why„ÄÇ And from that oneÔºå we compute the negative log likelihood„ÄÇ Comp the gradients„ÄÇ

 the gradients or the pressure of the loss with respect to each„ÄÇWeight and also the bias„ÄÇ

 So you am actually not showing the bias I just see„ÄÇ so it should be technically also a bias unit„ÄÇ

So we compute that oneÔºå and then we update the weights and the bias units with that one„ÄÇ

And this right part here this comes after training„ÄÇ this is just for the prediction„ÄÇ

 So for that one we can use a threshold function similar to Allline„ÄÇ

 so in logistic regression we can use the following threshold function„ÄÇ

Here to compute the real Y hat„ÄÇ So I should have maybe be more consistent„ÄÇ

 say Y hat is the predicted class label and a is the probability„ÄÇ class me probability„ÄÇ

 I will be more yeah careful of that when I write the code for the next video anyways„ÄÇ

 so we can say with a threshold function of the probability„ÄÇSo this is aÔºå if this is greater than 0„ÄÇ

5„ÄÇThen return class label1„ÄÇAnd otherwiseÔºå return class label 0„ÄÇ Actually„ÄÇ

 we can do it even more computationally efficient during prediction„ÄÇ

 We can even skip the activation function„ÄÇ we can just operate with Z„ÄÇ

 We can say if the net input is greater than point or if it's greater than0Ôºå then the label is one„ÄÇ

 Otherwise it's 0„ÄÇ Why is that„ÄÇ

![](img/e27658390d5d2e3c3ac7e6c8b59df939_17.png)

If we take a look at the„ÄÇLogistic sigmoid function again„ÄÇ So here that's the net input at 0„ÄÇSo at 0„ÄÇ

 the output is 0„ÄÇ5„ÄÇ so we know so weÔºå we can say everything above hereÔºå above 0„ÄÇ5 is class label„ÄÇ1„ÄÇ

 say Y hat„ÄÇClass A1 and below here„ÄÇY hat S class„ÄÇ0Ôºå but we know„ÄÇIf we have the threshold „ÄÇ5„ÄÇ

 it corresponds to a net input of 0„ÄÇ so we can also just say if the net input is greater than0 greater than 0„ÄÇ

 then it's class label 1Ôºå if it's smaller than 0Ôºå then it's class label 0„ÄÇ

 So that's just what I wanted to say about logistic regression„ÄÇ



![](img/e27658390d5d2e3c3ac7e6c8b59df939_19.png)

YeahÔºå in the next videoÔºå I want to talk a little bit about the logics and cross entropy„ÄÇ

 These are just two„ÄÇI would say„ÄÇJargon words about or regarding logistic regression logicits and cross entropy„ÄÇ

 I will just briefly discuss what those mean„ÄÇ And then I will show you logistic regression in a code example„ÄÇ

 And if everything in this video goes a little bit fast and confusing„ÄÇ

 I think the code example will probably clarify that because then we will put everything together„ÄÇ

 we will„ÄÇDo the forward passÔºå compute the gradients and update the weights„ÄÇ

 So what I mean is we will really„ÄÇ

![](img/e27658390d5d2e3c3ac7e6c8b59df939_21.png)

Implement this whole stochasticÔºå gradientÔºå decent learning rule in code and that I hope clarifies everything„ÄÇ



![](img/e27658390d5d2e3c3ac7e6c8b59df939_23.png)