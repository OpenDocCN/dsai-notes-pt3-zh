# P117ï¼šL14.3.2.1- ResNet æ¦‚è¿° - ShowMeAI - BV1ub4y127jj

Yesï¼Œ so now that we discussed the VG G16 architecture in the previous videosã€‚

 the natural question is can we even add more layers and expect to improve the performance furtherï¼Ÿ

 Wellï¼Œ technically yesï¼Œ but noï¼Œ so technically adding more layers should we would expect it to helpã€‚

But in practiceï¼Œ there might be problems like the vanishing gradient problem that we talked aboutã€‚

 So if we have a multiplicationã€‚In thereï¼Œ I meanï¼Œ we have the multi variableable chain ruleï¼Œ rightã€‚

 So we have the chain ruleï¼Œ which is based onã€‚It's essentially a product of different thingsã€‚

 And then if only one of those is a very small numberï¼Œ let's say even early onã€‚

 that it will essentially cancel out all the remaining signals in the back propagationã€‚

 So having small values inã€‚Back propagation somewhereã€‚ if we use it in a productã€‚

 can really cause big issuesã€‚So we can't just add more layers and expect then the performance to perform betterã€‚

 That's what's also experimentally observedã€‚ So people were thinking about thatï¼Œ I meanã€‚

 different solutions to this type of problemã€‚ One is using the so-called residual networkã€‚

 So the residual network relies on so-called skip connectionsã€‚

 which can essentially learn to skip certain layers if these layers are detrimental to the performance of the networkã€‚

 So essentially it's CNNnNs with a simple trickã€‚So let's take a look how that looks likeã€‚

 And in this videoï¼Œ I will just go over the yeahï¼Œ broad concept and in the nextã€‚



![](img/a4443987499377a6e50aef71e14e91a0_1.png)

Videoï¼Œ I will show you a code implementationã€‚So yeahï¼Œ just for referenceã€‚

Revisiting our visualization hereï¼Œ that's where residual nets are locatedã€‚ There's another one hereã€‚

 a bigger oneã€‚We are talking about a small oneï¼Œ34ã€‚ I meanï¼Œ this's also an 18 versionï¼Œ but yeahã€‚

 it is not as good as this oneã€‚ We could have talked about this oneï¼Œ but it is three times largerã€‚

 So 34 is referring to the number of layersã€‚ This is kind of a good sweet spot for a simple problemã€‚

 We could have used 18ï¼Œ maybeï¼Œ but yeah here we are using 34ã€‚

 it's slightly better than VG G16 also hereã€‚But has far fewer parametersã€‚

 you can see this is far smallerï¼Œ it's maybe around maybe 25 million or something that is 35 its bigger than this oneã€‚

 whereas VG G16 was at least maybe three or five times as largeã€‚



![](img/a4443987499377a6e50aef71e14e91a0_3.png)

Alrightï¼Œ so here's a screenshot from the paperã€‚ So the paper is called deep residual learning for image recognitionã€‚

 It's from 2016ã€‚ So it's not newï¼Œ of courseï¼Œ but it's still very popular architectureã€‚

 There are also variants of thatï¼Œ called whiteï¼Œ like white residualã€‚

Networks that are still very popularã€‚And they're just broader layersã€‚

 but essentially the concept behind residual networks is really powerfulã€‚

 It's even something called a dense netï¼Œ which is taking this even further and connecting each layer with each other layerã€‚

 But anywaysï¼Œ let's stay here with the residual networks for now one for this video soã€‚

Here's an outline of how this whole thing looks likeï¼Œ soã€‚Suppose there is an input Xã€‚

 but this is only an input to a given layerã€‚ I meanã€‚

 we have more of these blocks in the whole networkã€‚ So this doesn't have to be the image inputã€‚

 This can be the input to a given layerã€‚ Let's say let's call that layerã€‚Inputã€‚And then after thatã€‚

 we have a weight layerï¼Œ for exampleï¼Œ a convolutional layerã€‚A re functionã€‚Another weight layerã€‚

And then we have this plus hereã€‚ So the plus here is a addition operationã€‚ So it's addingã€‚

This X hereã€‚Backï¼Œ soã€‚It's written hereï¼Œ so we are adding thisã€‚To whatever comes out of hereã€‚

 So let's call that oneã€‚F of xã€‚And here we have this inputã€‚ So we have this plus operationã€‚

 which is f of x plus xã€‚Rightï¼Œ and then after thatï¼Œ there's another reã€‚ So essentiallyï¼Œ we haveã€‚

This so calledã€‚Skipã€‚Connectionã€‚Why is it called residual networkï¼Œ Wellã€‚

 I guess it's called like that because if weï¼Œ if we maybe talk about what comes out of here as let's sayã€‚

Aã€‚The output of this oneï¼Œ it's called itã€‚è¯¶ã€‚Thenï¼Œ we have aã€‚us x equals f of xã€‚

 if I just rearrange that and maybe you can think of it as learning the residual between the output and the inputã€‚

 but this is like really more conceptual is maybe why they thought of the name residualã€‚

 but it's only speculatingï¼Œ but I guess that's what inspired the name residual networkã€‚

 But what's more important here has really that it's learningã€‚

Or it's having this weird setup where we add this inputã€‚Backã€‚

I have more visualizations of that to illustrate this furtherã€‚

 and then I will tell you why this might possibly help because essentially yeahã€‚

 it's actually already written here itã€‚Can result in learning the identity functionã€‚ So the networkã€‚

Can technically learnã€‚To skip thisã€‚ So if you think of the signal that is being back propagatedã€‚

 if this is a bad signalã€‚Like a0 or something like thatã€‚You still have this identityã€‚

 which is a simple plusï¼Œ so it can still just maintain the signalï¼Œ rightï¼Œ So instead ofã€‚

Having cancelled all all the gradientsï¼Œ if there's a zero in here or somewhereã€‚

 what happens is that we still would have this identity hereã€‚



![](img/a4443987499377a6e50aef71e14e91a0_5.png)

So here is a visualization of the big picture of the residual networkã€‚ So yeahã€‚

 what you can see here is thatã€‚This is 34 layer plane network for reference BG G 19ã€‚

And in this 34 layer plane networkã€‚They have justã€‚Many different layers hereã€‚

 but they found that this doesn't really result in a good performanceã€‚

The performance of this one is actually pretty poorã€‚ So in order to fix thatã€‚

 they added these skip connectionsã€‚ So you drop it's the 34 layer residual network and these yeah skip connections help with improving the performanceã€‚

 So here yeahï¼Œ it was essentially an ablation study what you callã€‚Applationã€‚

Study where you remove one part of the network and see if it still performs wellã€‚

 So it's same like ablation studies in yeahï¼Œ let's say biological fieldsã€‚other fields of scienceã€‚

 So hereã€‚With and without residual networksï¼Œ I'm actually not showing you the table becauseï¼Œ yeahã€‚

 we have many things to talk aboutã€‚ I don't want to make the tables too longã€‚ sorryã€‚

 the video too longã€‚ but if you're interestedï¼Œ you can refer to the original paperã€‚

 What's more interesting is againï¼Œ let's talk about how one of these residual blocks look likeã€‚

 like zooming inã€‚

![](img/a4443987499377a6e50aef71e14e91a0_7.png)

Soã€‚Here I'm writing essentially the residual block that I showed you earlierã€‚

 So let's think of the output here asã€‚Aã€‚ğŸ˜”ï¼ŒL plus 2ã€‚

I shouldn't have made this so large because then no I don't have space to fit it inã€‚ğŸ˜”ï¼ŒAlrightã€‚

 so this isï¼Œ let's call this A L plus 2 L plus 2 because it'sã€‚Two layers after what is hereã€‚

 And let's call this thenã€‚AL at a given layerï¼Œ and this is two layers laterï¼Œ rightã€‚should meã€‚

Like thisã€‚Thisã™ã€‚Meerã€‚ğŸ˜”ï¼Œ1ï¼Œ and it'sã€‚hereã€‚Leirã€‚ğŸ˜”ï¼Œ2 notice that the skip connection only spans one and a half layerssã€‚

So that is because before hereï¼Œ we also set the reã€‚ So this is the activationã€‚

And if we would have the red loop before hereã€‚Weã€‚It actually would end up applyingã€‚

Two activations in a rowã€‚ So in that wayï¼Œ it'sã€‚Set up like thisã€‚Okayï¼Œ soã€‚Orï¼Œ essentiallyã€‚Yeahã€‚

 that'sï¼Œ that's what I meanã€‚ Okayï¼Œ so essentially what you can think of here is asã€‚

Z plus is the valueã€‚Comes out of hereã€‚Seeã€‚ğŸ˜”ï¼ŒPlus 2ã€‚And this one would beï¼Œ essentiallyã€‚å—¯ã€‚ğŸ˜”ï¼ŒOh plus1ã€‚

And yeahï¼Œ so here this would be Zã€‚L plusã€‚Just writing down some notation hereã€‚

 and this is the basic setupï¼Œ so you have a convolã€‚Optionallyï¼Œ a batchomeï¼Œ they use batchome inã€‚

Resion networkï¼Œ you don't have to use petaginumï¼Œ but it performs betterã€‚

R activation another convolution patch normã€‚ And then you addã€‚Bothtï¼Œ you add those togetherã€‚

You add this oneã€‚With this oneã€‚And thenï¼Œ you applyã€‚The render activation hereã€‚

That is essentially how a simple skip connection looks likeã€‚Alrightã€‚

 now expanding this one or in particularï¼Œ this oneã€‚ So this one is the one here againï¼Œ Z Lã€‚It's trueã€‚

Now we can expand that by taking a look atã€‚A L plus 1ã€‚Then we apply a weightã€‚

 can think of the convolution at batchome here as our weightsã€‚ we add a biasã€‚But yeahã€‚

 I'm kind of simplifying itã€‚ it would be a convolution operation and then batchcheome where we have also a gamma and a betaã€‚

 which I'm ignoring hereï¼Œ let's just assume we don't have batchaginome only a weight and a bias for simplicity so this can be understoodã€‚

Is this operationã€‚ So we are essentially adding this oneã€‚ And now you can think of itã€‚ Okayã€‚

 what happens ifã€‚å—¯ã€‚The weights and the bias are 0ã€‚ So if I haveã€‚Z weights hereã€‚And0 biasã€‚

 the network decides to learn thatã€‚ What will happen is it will essentially skipï¼Œ rightï¼Œ it willã€‚

Skip that layerã€‚ so then we will only haveã€‚This term hereã€‚

So all of that would be essentially canceledï¼Œ so we only remain with this termã€‚

 So what I'm writing down hereã€‚ So in that senseï¼Œ the network learns an identity mapping here between the input to the layerã€‚

 So let's call thatã€‚å—¯ã€‚Ohã€‚Andã€‚ğŸ˜”ï¼ŒThis oneï¼Œ yeahï¼Œ this one is essentially shortcuted if this isã€‚

IfWhatever comes out of hereã€‚Is0ï¼Œ0 plusã€‚A Lï¼Œ and then we have the reluã€‚

So it's essentially learning how to skip this whole partã€‚

 And this is the main idea behind residual networksï¼Œ soã€‚I meanï¼Œ we we don't decide what to skip hereã€‚

 The network has the option to learn how to skip somethingã€‚ So just by applying back propagationã€‚

 it might happen that it learns to skip certain layers if they are badã€‚

 So that way you are building a very large networkã€‚

 but it's actually maybe not using all of these layersã€‚

 it's essentially sometimes maybe learning to skip certain layers if they are bad for performanceã€‚



![](img/a4443987499377a6e50aef71e14e91a0_9.png)

Soã€‚Yeahï¼Œ so we assumeï¼Œ thoughï¼Œ thatã€‚That dimension of our valueã€‚Aã€‚

L here has the same dimension as this L plusã€‚To hereï¼Œ rightï¼Ÿ Because otherwiseã€‚

 we wouldn't be able to add thoseã€‚ We can only add things if they have the same dimensionã€‚

 The strong assumptionã€‚ So if we have the setupï¼Œ this means we always have to make sure we apply theã€‚

Same convolution with the appropriate amount of padding and that we also cannot reduce the size of the layersã€‚

 rightï¼Œ so that can be a little bit of a problem because then our net network has always the same inputs when we have 224 times 224 images as inputã€‚

 and we always have these skip connectionsã€‚ The last dayï¼Œ we'll still have 24 times 24 sized imagesã€‚

Rightï¼Œ so that wayï¼Œ it's also not very usefulã€‚ So in practiceï¼Œ there's also a wayã€‚



![](img/a4443987499377a6e50aef71e14e91a0_11.png)

We can reduce itã€‚ And it's by having a special version of that shortcut only using yeah convolution and batch normã€‚

 So there are some alternative residual blocks that can also be used Yeah toï¼Œ to reduce the sizeã€‚

 And that wayï¼Œ it's almost as if we wouldã€‚Have one layerã€‚ So this is essentially kind of likeã€‚

If you ignore this partï¼Œ it's this togetherï¼Œ it's kind of like a single layer instead of having two layersã€‚

So yeahï¼Œ that's a trick you can reduce the size withã€‚All rightï¼Œ soã€‚

That's essentially how residual networksã€‚Work and in the next videoã€‚

 I will show you a code implementation of thatã€‚

![](img/a4443987499377a6e50aef71e14e91a0_13.png)