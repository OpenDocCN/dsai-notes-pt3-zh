# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P39ï¼šL5.7- è®­ç»ƒè‡ªé€‚åº”çº¿æ€§ç¥ç»å…ƒ (Adaline) - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ so the last couple of videos included a lot of mathematical detailsã€‚

 which may not be super excitingã€‚ So let's now take a look at the more hands on example and do somethingã€‚

 yeah more hands on training and adaptive linear neuron and Pythonã€‚

 So here I will just give you the overview of how things relate to the Alineã€‚ So recall the Alineã€‚ğŸ˜Šã€‚



![](img/9196609a4baf3856bac9916a9102ebf3_1.png)

That is what we covered briefly in the history of deep learning lectureã€‚

 that is a model by Wirow and Hf in developed in 90 and around the 1960sã€‚

 So actually whats the physical deviceï¼Œ but you can of course also nowadays implemented in software and this is a nicely differentiable neuro model so the perceptron model that we talked about before is not differentiable because of this threshold functionã€‚

 So this Analine you can think of it as an advancement of this perceptron that is now also converging even if the data is not linearly separableã€‚



![](img/9196609a4baf3856bac9916a9102ebf3_3.png)

Soï¼Œ yeahï¼Œ just for comparisonsã€‚ So how the perception looked likeã€‚ So recall in the perceptionã€‚

 you we compute the net input firstï¼Œ and then we pass the net input through a threshold functionã€‚

 and then threshold function produces our outputã€‚ So the output isã€‚The predicted class tableã€‚

 which can be either0 or oneã€‚And ifã€‚The predictionã€‚

Matches the actual label for that given training exampleã€‚ Then we don't do anythingï¼Œ butã€‚If itã€‚

Doesn't match if it's differentã€‚Then we have this error term that we computedã€‚

 So yeah the difference between the two and we use that to update the weightsã€‚

 so we update the weightsï¼Œ which then will change the decision boundary to make the prediction correctã€‚

So that is how the perceptron worksï¼Œ but a threshold function is not differentiableã€‚

 so here we couldn't use calculusã€‚ There was just this rule that was developed by I would sayã€‚

 thinking hard about this problemï¼Œ so it was more like an empirical update rule now we can actually improve that by using calculusã€‚

So at the bottom is the adelineã€‚ So here you can think of this part here as theã€‚Maybe base modelã€‚

 And this isï¼Œ yeahï¼Œ like linear regression worksã€‚ So againã€‚

 we also compute the net input similar to the perceptronã€‚ Now we have as an activation functionã€‚

 We have this identity functionï¼Œ which isã€‚Not doing anythingã€‚ So if it's not doing anythingã€‚

 If my activation function is just the identity functionï¼Œ why do I write it down in the first placeã€‚

 That's because yeahï¼Œ next week or the week after thatã€‚

 we will be talking about non linear activation functionsã€‚ We will be talking aboutã€‚

Our functions that areï¼Œ for exampleï¼Œ a sigoidal functionï¼Œ likeï¼Œ likeã€‚This oneï¼Œ for exampleã€‚

 so we will be covering other types of activation functionsã€‚

 and we will need them to develop multilayer networks because this will help our multilayer network toã€‚

 yeah produce nonlinear decision boundaries for solving complex problems because if we would be using linear activation functions in a multi layerer neural networkã€‚

 thenã€‚The multilayer network would be just a combination of linear functions and the sum of linear functions is also just the linear functions so we wouldn't gain anything so later on we will be talking about nonlinear activation functions and here you can think of it as a place that we will replace that later I'm just including it here in the notation because it's a more general notation that would apply to also nonlinear activation functions okayã€‚

Now consider this linear regression base in order to turn this linear regression model into an adaptive linear neuron into the Adeline modelã€‚

 all we have to do is now adding a threshold function because the adeline model is a classifierã€‚

Classifier for classifying data points similar to the perceptionã€‚ Howeverï¼Œ note the difference hereã€‚

 really between the perception and the airlineline isã€‚Where we compute the error soã€‚

For the perceptionceptronï¼Œ we compute the error after the threshold function and in the add lineã€‚

 we compute the error before the threshold functionã€‚

 So here we don't have the problem that things are not differentiable because we compute it before the threshold functionã€‚

 The threshold function still is not differentiableã€‚But we don'tã€‚

 yeah worry about that because we compute the gradients before thatã€‚

 So in that way it's not a hindrance and it will become more clear when I show you the code exampleã€‚



![](img/9196609a4baf3856bac9916a9102ebf3_5.png)

So here's just conceptually how it looks like when we fit an adeline modelã€‚

 So here what I'm just doing is I'm showing you the class label on the y axissã€‚ So for binaryã€‚

Classificationï¼Œ where we have twoã€‚Clasification where we have two class tables of 1 and 0ã€‚

 I have plotted them hereï¼Œ so the class as can be either one or0ã€‚

 And then we have only one feature value for simplicity hereã€‚And yeahã€‚

 we want to predict that and the linear neuron is a linear modelã€‚

 so essentially what it will learn will fit a linear line similar to linear regressionã€‚

 so this would be very similar to linear regression rightã€‚

 except now the difference is really that in linear regression we usually have continuous valuesã€‚

 not only zeros and ones and we don't have the threshold functionã€‚

So in the headdeline after fitting this modelï¼Œ we apply this threshold function let's say at 0ã€‚

5 between 0 and 1ã€‚ and we sayã€‚Thenï¼Œ if somethingã€‚If something is bigger than a certain valueã€‚

 So if theã€‚It's maybe a little bit unfortunate that I wrote class table here because this is this thresholdã€‚

 if the predictionï¼Œ let's sayã€‚Predictionã€‚If the prediction is greater than 05ï¼Œ produce glass table 1ã€‚

Andã€‚ğŸ˜”ï¼ŒOtherwiseã€‚Produce class label 0ã€‚ So in that wayã€‚

 we turn the continuous prediction into a class labelã€‚ againã€‚

 this will become clear when I show you the code exampleã€‚So I prepared some code examplesã€‚

 let me pause this video and do this in a fresh videoã€‚

 so there will be another video where I will go over these code examplesã€‚



![](img/9196609a4baf3856bac9916a9102ebf3_7.png)