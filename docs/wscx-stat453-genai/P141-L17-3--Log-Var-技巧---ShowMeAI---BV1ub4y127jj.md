# P141ï¼šL17.3- Log-Var æŠ€å·§ - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ before we get to the code example hereï¼Œ I want to talk about two more thingsã€‚

 one is the loss function and one is the lockvaricã€‚

 which I will talk about in this particular video And yeah it's kind of like a chicken egg problem What do we want to talk about first you want to see the code example first and I explain the details or should I explain the details and then we talk about the code example think in this particular orderã€‚

Briefly talking about the lockat and then revisiting it in the code example might be easierã€‚

 So let me start with this oneã€‚ And soã€‚What this is about is the fact that we are training a variation ought encoderã€‚

 which is still a neural network for which we want to use back propagationã€‚But yeahã€‚

 if we have a probability distributionï¼Œ how do we back propagate with a probability distributionã€‚

 rightï¼Œ or if we have a random variableï¼Œ how do we deal with that using back propagationã€‚Soã†ã€‚

This is where the lockbaric will come into playã€‚ but let me just briefly recap something from the previous videoã€‚

 So just yeahï¼Œ to recap the previous video we discussed that we take the input data setã€‚

 and when we train the variation of encoderï¼Œ it will happen such thatã€‚



![](img/0c8d49081edadc39eaedd6f912aad7d2_1.png)

The latent representation will resemble or will be according to a standard multivariate Gaussian distributionã€‚

 So distribution of this latent space will follow a standard multivariate Gaussian distributionã€‚

So here's just for referenceï¼Œ againï¼Œ the probability density function andã€‚In this particular caseã€‚

 I was drawing a 2D Gaussian and we have a mean vector and a covariance matrixã€‚

 and we said that because it's a standard normal distributionã€‚It's 0 mean and unit varianceã€‚

 and theres no interaction between the featuresã€‚ So we essentially have a varianceã€‚

Vecto that is just oneï¼Œ1 in each dimensionã€‚

![](img/0c8d49081edadc39eaedd6f912aad7d2_3.png)

Yeahï¼Œ and then when we sampleã€‚ So this is maybe a little bit tricky part toï¼Œ yeahã€‚

 wrap our heads around this factã€‚ So we sample in the following wayã€‚ So in the networkï¼Œ we will haveã€‚

A fully connected layer that will learn this mean vectorã€‚And there will be a fully connected layerã€‚

That will learn this variance vectorï¼Œ at least in theoryï¼Œ and how we sampleã€‚

Is then we use this epsilon that we sample from aã€‚Standard normal distributionã€‚

 So with no interaction between these featuresã€‚ So we just draw for each dimension a value from the randomtum standard normal distributionã€‚

And we multiply this value with our standard activation here and add the mean vectorã€‚

 and this is how we get a new sample soã€‚Essentiallyï¼Œ weã€‚Encodeã€‚ So if we have Xï¼Œ we encodeã€‚

 let's call thisã€‚å—¯ã€‚How can we call itï¼Œ let's just call it the mean vectorã€‚Andã€‚ğŸ˜”ï¼ŒZä¸ªã€‚ğŸ˜”ï¼ŒThe verytoã€‚

When we put our exampleã€‚Through the encoderã€‚We get this toã€‚Back to usã€‚

 So that's what we are learning in the networkã€‚ And thenã€‚We will have the step where weã€‚

Have have the mean plus the star deviation vector times this epsilonã€‚

 this epsilon is sampled independently though it's sampled independently from our standard normal distribution with the identity matrix is the covariance matrixã€‚

 and then we put it in hereã€‚And only this part is part of the networkã€‚Thenï¼Œ we haveã€‚Oopsã€‚The decoderã€‚

And then what comes out of here isã€‚X primeã€‚And like I mentioned beforeã€‚ğŸ˜”ã€‚

We compute the distance or difference between those two to make sure the reconstruction is goodã€‚

 And here for this slant spaceï¼Œ which is essentially represented by this mean vector and the variance vectorã€‚

 we want this to beã€‚Normmal distributedã€‚ so we have this KL divergence termã€‚

 I will talk about this again in the next videoï¼Œ so we have these two objectivesã€‚

 but yeah in essenceï¼Œ we have here the mean vector and the variance vector as part of the networkã€‚

Againï¼Œ if this looks a little bit abstract right now in the code exampleã€‚

 you will exactly see how that worksã€‚All rightï¼Œ so but instead of using this variance vector here in the networkã€‚

 we are using the lock varianceã€‚

![](img/0c8d49081edadc39eaedd6f912aad7d2_5.png)

Why are we doing thatï¼ŸThis will allow us for positive and negative valuesã€‚ So in this wayã€‚

 if we only had a variance vector thereï¼Œ we could only have positive valuesã€‚

 and it's kind of a bit trickyã€‚ I would say to learn this during black propagation to make the learning a bit more stableã€‚

 betterã€‚ we will use the lock of this variance vectorã€‚And thenï¼Œ we will sampleã€‚

As follows as shown here on the bottomã€‚ So instead of sampling what I showed you beforeã€‚

 instead of sayingã€‚We draw the sampleã€‚Like thisã€‚We now replace this one by E to the power of lockã€‚

Of the very inspectorï¼Œ divided by2ã€‚Why is it this one hereï¼ŸYeahã€‚

 this is just yeah you can see that if you just yeah rearrange the termsã€‚

 so this one you can bring the two outside right so it's the same as two times lock of the variances sorryã€‚

 lock of the san deviationã€‚And if you want to then write just this lock of the s deviationationã€‚

 you can cancel the tombã€‚ So on the left hand side you have lock variance divided by twombã€‚And thenã€‚

If you takeã€‚This to the power so the exponential E to the power of rockã€‚This one should beã€‚

 of courseï¼Œ this oneï¼Œ rightã€‚ And we know that this one is the same asã€‚Lookã€‚ğŸ˜”ï¼ŒSigmaã€‚Squatã€‚

So we know that this is essentially thenã€‚Lookï¼Œ whatever this isã€‚squared divided by2ã€‚

 So this is why well how we end up with this sampling procedureã€‚

 and this allows us to only yeah store the the lock of the variance vectorã€‚Lookã€‚Baranceï¼Œ so okayã€‚

 so this is how we would implement thatï¼Œ because thenã€‚



![](img/0c8d49081edadc39eaedd6f912aad7d2_7.png)

If I go back one more slideã€‚å—¯ã€‚Pink here instead of having this hereã€‚

 we will have a vector that we call lookã€‚Varsã€‚This isè€ã€‚

This vector to have positive and negative valuesã€‚Which is also true for the mean vector hereã€‚



![](img/0c8d49081edadc39eaedd6f912aad7d2_9.png)

Okayï¼Œ so if this was abstract in the code exampleï¼Œ I hope this will become more clearã€‚

 so in the next video let me briefly talk about the loss function in the variation of our encodeta and then we will finally see our first code exampleã€‚



![](img/0c8d49081edadc39eaedd6f912aad7d2_11.png)