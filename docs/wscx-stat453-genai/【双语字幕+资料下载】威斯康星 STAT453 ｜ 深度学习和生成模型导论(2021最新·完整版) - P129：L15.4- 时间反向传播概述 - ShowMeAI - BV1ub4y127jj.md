# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P129ï¼šL15.4- æ—¶é—´åå‘ä¼ æ’­æ¦‚è¿° - ShowMeAI - BV1ub4y127jj

All rightï¼Œ let's now take a brief look under the hood of a recurrent neural networkã€‚

 So here we are going to talk about be propagation through timeã€‚

 This is essentially the be propagation algorithm we have seen earlierã€‚

Except that we now take a look at how it might work if we have this time dimension and also don't worryã€‚

 I won't ask any detailed questions about that in the quiz because yeah this is a complicated topic and here we are only really briefly looking at it so you don't have to know the details here it's just like to illustrate briefly that back propagation also applies to recurrent neural networksã€‚



![](img/c969bd6cc14afbf10e02176ae6238ddf_1.png)

So here I'm showing a figure that is similar or looks similar to what I've shown you before in an earlier videoã€‚

 So here we have a recurrent neural network with a single hidden layerã€‚On the left hand sideã€‚

 this is the compact notation where we only seeã€‚The input and the output hereã€‚

 and then we have the hidden layer here with this recurrent edgeã€‚Howeverï¼Œ compared to beforeã€‚

 what's different now is that I have added these weight matrices that we use hereã€‚

 So there are three weight matricesï¼Œ let's say1ï¼Œ2ï¼Œ and 3ã€‚That's let'srated like thisï¼Œ oneï¼Œ2 and3ã€‚

What we can see here is that one matrix connects the input to the hidden layerã€‚

 and then there's one mate matrix connecting the hidden layer to the output layerã€‚

These are weight matrices that you would findã€‚ So one and twoã€‚

 they would you would find them in a regular multi layererã€‚Perceptronã€‚Howeverï¼Œ now in theã€‚Are an Nã€‚

We haveã€‚3ï¼Œ we have basically all the ones from the multicione plus this weight matrix3 hereã€‚

 which is the one from the previous hidden stateã€‚ So to summarize in a R and N hereï¼Œ we have twoã€‚

Matrices for the hidden layerï¼Œ One is connectingã€‚The input hereã€‚Soï¼Œ the inputã€‚

To the hidden there and the other one is connecting the previous hidden there to the current hidden thereã€‚

 which is this H H hereã€‚So on the right hand side is the unfolded versionã€‚

 and you can see so that we haveã€‚Ohï¼Œ you can see also that we are reusing these matrices so at each time step we use the same matrix here for the input connected to the hidden stateã€‚

 and then we also use the same matrices here for each on time step so the same matrix connecting the hidden state to the next hidden stateã€‚

And thenï¼Œ we also haveã€‚This yearã€‚So what's really new compared to a multi layer percept is that we haveã€‚

These in greenã€‚ So this is newï¼Œ and this is newã€‚ These are weight matrices that we did not have in the multi layerer perceptron beforeã€‚



![](img/c969bd6cc14afbf10e02176ae6238ddf_3.png)

So how do we computeï¼ŸNowï¼Œ the hidden stateï¼Œ I meanã€‚

This is essentially very similar to computing the regular inputï¼Œ except that we have noã€‚2ï¼Œ yeahã€‚

 weight matrices and two inputsï¼Œ soã€‚Let's consider this case when we compute the net input for this hidden state hereã€‚

At time step Tã€‚So what we do is we like before when we computed the net input in a multiivatorceptronã€‚

 we multiply this weight matrix hereã€‚With this input hereã€‚And that gives us the hidden stateã€‚

We also may write this differentlyã€‚Like thisï¼Œ rightã€‚

 So we talked about this before in the linear algebra lectureã€‚

 I don't know why I've written this in a different orderï¼Œ but it's the same thingã€‚å—¯ã€‚Yeahï¼Œ and thenã€‚

This is one inputï¼Œ The other one is H Hã€‚From the previous hidden stateï¼Œ t -1ï¼Œ soã€‚

Maybe to use different colorsã€‚This one isï¼Œ of courseã€‚

 this one for So this is an net input for this hidden layerï¼Œ and thenã€‚

This part is computing this partã€‚And this part here isã€‚Computing this partã€‚

And then we add Osoya Allabias hereã€‚So the bias is for this hidden state hereã€‚Alrightã€‚

 so this is how itã€‚Looks like how we compute the net input and then to compute the activationã€‚

 we would just use an activation function like 10 H or the sigmoid or Relu functionã€‚And yeahã€‚

 this is how we compute the net input nowã€‚How do we compute the net input for the outputã€‚

 This is exactly like what we do for a multi layer perceptronã€‚ So we have one weight matrix hereã€‚

 the hidden state and then the biasã€‚ And then againï¼Œ we can use a activation functionï¼Œ for exampleã€‚

Softm activation or the sigoid activationã€‚ if we have a binary output or yeah or just a linear layerã€‚

 if it's a regression output and so forthã€‚ So here this is really the same that we would do for a Moperceptronã€‚



![](img/c969bd6cc14afbf10e02176ae6238ddf_5.png)

Now what about the loss So yeah it really depends on what type of task or sequence modeling task we talk aboutã€‚

 So if you only want to predict one label for a given textã€‚

 you technically don't need these losses hereã€‚ you only need a lossã€‚

A single loss from the last one hereã€‚ Some people argue it might be good thoughã€‚

 to keep the intermediate losses as wellã€‚ it helps training the earlier layersã€‚

 So I think I mean depending on who you ask some people keep these losses some don't if you when you compute also many to one sequencing problem and it is different though if you have many too many then you want to have multiple losses so let's take a and look at just the general caseã€‚

 so you can have different lossesã€‚Hereï¼Œ a loss for each time step and when you then want to compute the overall lossã€‚

 you can just sum them up so that would be the overall lossã€‚Yeahã€‚

 so this would be just looking at the lossã€‚ by the wayã€‚

 theres a paper on back propagation through timeï¼Œ what it does and how to do itã€‚

 So here we are really just briefly scratching the surfaceã€‚

 So this might be some resource to consultã€‚And also there are probably many different tutorials on the internet also for doing that in detail for this classã€‚

 given that we have several generative modeling topics still to discuss on here we are not going to spend too much time on these detailsã€‚



![](img/c969bd6cc14afbf10e02176ae6238ddf_7.png)

But yetï¼Œ soonnana to highlight one of the issues that areã€‚Apparentlyã€‚

 when we use spec propagation through timeï¼Œ let's just briefly focus on how the gradient of a particular loss at time step T is computed with respect to thisã€‚

W H H hidden matrixã€‚ So hereï¼Œ let's assume we have lossã€‚T at that layer hereã€‚

 So what will happen is that we do the back propagationï¼Œ rightã€‚

 so we would compute the partial derivative root or gradient of this loss at the time step with respect toã€‚

Weiã€‚ğŸ˜”ï¼ŒTï¼Œ so with respect to this oneã€‚ and then we would compute the gradient ofã€‚Wã€‚ğŸ˜”ã€‚

Tea with respect toã€‚Hã€‚ğŸ˜”ï¼ŒTï¼Œ so with respect to that oneã€‚ So here we covered this partã€‚ But then yeahã€‚

 so this is essentially what we would do for a regular multilay perceptronã€‚

But then on on top of thatï¼Œ we haveã€‚This term hereï¼Œ whichã€‚

Is essentially the through time step where we have time steps from K1 to Tã€‚

 So from up to the very beginningã€‚ So we haveã€‚Partial the reservoir gradient of H T with respect to the one in the very beginningã€‚

 So summing them upã€‚

![](img/c969bd6cc14afbf10e02176ae6238ddf_9.png)

Andã€‚Those themselvesï¼Œ we won't go into too much detail hereï¼Œ but those themselves are a product hereã€‚

 and the product is essentially what could cause problems like vanishing or exploding gradient problemsã€‚

 so don't worry about the details here too muchã€‚ so really the I would say the main point here is not really doing this by handã€‚

 because usually we have autograd implemented in Pywat and stuff like thatã€‚

 So we would not have to implement these types of things by handã€‚



![](img/c969bd6cc14afbf10e02176ae6238ddf_11.png)

But this highlights the main issue that if we have a lot of multiplicationsã€‚

 we can have these vanishing or exploding gradient problems if we have very large or very small numbers that we multiply multiple timesã€‚

 So in the next video I will talk about the so-called longï¼Œ shorter memoryã€‚

 which is one approach to yeah to mitigating this exploding or vanishing gradient problemsã€‚



![](img/c969bd6cc14afbf10e02176ae6238ddf_13.png)

![](img/c969bd6cc14afbf10e02176ae6238ddf_14.png)