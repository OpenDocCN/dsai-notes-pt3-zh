# P32ÔºöL5.0- Ê¢ØÂ∫¶‰∏ãÈôç„ÄêËØæÁ®ãÊ¶ÇËø∞„Äë - ShowMeAI - BV1ub4y127jj

YeahÔºå hi everyone„ÄÇ So we had a little excursion last time talking about linear algebra„ÄÇ Today„ÄÇ

 there will be another little excursion talking about some concepts in calculus„ÄÇ

 because they will be useful forÔºå yeah fitting neurons with gradient descent„ÄÇ So today„ÄÇ

 the lecture will be on gradient in descent„ÄÇ and I will have some optional sections on„ÄÇ

 some calculus topics as a little refresher„ÄÇ But yeah„ÄÇ

 because I think there's a lot of stuff to cover today„ÄÇ

 I will keep the introduction short and just dive in„ÄÇüòä„ÄÇ



![](img/20ba561c26dab43aa06f65958ffadb52_1.png)

YesÔºå so the goal here in this lecture is to improve upon the perceptron that we covered last week„ÄÇ

 So in this lectureÔºå we are going to talk about or learn about a neural network model for which that training always converges„ÄÇ

 So even if the data is not linearly separable„ÄÇ here's an overview of the big picture goals„ÄÇ

 So our goals overall„ÄÇ So like I just said in this lecture„ÄÇ

 we are focusing on a learning rule that is more robust than the perceptron„ÄÇ

 so that it always convergesÔºå even if the data is not linearly separable„ÄÇ



![](img/20ba561c26dab43aa06f65958ffadb52_3.png)

But yeah that is not the end goal of this course„ÄÇ Of course we want to learn about more sophisticated models compared to single layer neural networks„ÄÇ

 So what we really care about is also how we can make or create more complex classifiers that can solve more complicated problems than just binary or linearly separate problems because if I have something„ÄÇ

Like that„ÄÇ where I have two classes„ÄÇ I meanÔºå this is relatively trivialÔºå right„ÄÇ

 So the perceptionceptron again had the shortcoming„ÄÇ If data point is over there„ÄÇ

 there is no linear decision boundary such that there is no mistake„ÄÇ So that way„ÄÇ

 it would never converge„ÄÇ But even let's sayÔºå if we have a learning rule that converges like this„ÄÇ

 this is a relatively trivial problem„ÄÇ And in practice„ÄÇ

 we are yeah interested in more complicated problems„ÄÇ

 So we need to also find a way to combine multiple neurons„ÄÇ because„ÄÇ

These combinations of multiple neurons or deep neural networks can help us to learn such more complicated decision boundaries„ÄÇ

 and then with that we can solve yeahÔºå also complicated real world problems„ÄÇ

 there are many like objective detection and let's say on classifying arbitrarybitary things„ÄÇ

So in that way that is something we will then start tackling in the next lecture„ÄÇ

 So first here we lay the groundwork talking about the learning rule and then we will apply the same learning rule to the combination of neurons and also we will then learn how we can extend this to multiple categories„ÄÇ

 It will also be next lecture next week„ÄÇSo we will learn also how we can classify yeah problems where we have more than two classes„ÄÇ

 which is also importantÔºå of courseÔºå rightÔºå if you think about„ÄÇ

 let's say you you build an application for a birdwacher to classify birds outside„ÄÇ

 So there are more than just two bird species right So in that way we will also learn about methods that can handle multiple classes„ÄÇ

 And then yeah more towards the end of the courseÔºå we will then also learn how to do fancier things„ÄÇ

 not just classificationÔºå but also like things like generating new images and new text and things like that„ÄÇ

But yeah all these types of things are based on the same learning algorithm and extensions of this learning algorithm„ÄÇ

 it's not like a fundamentally different though and in this lecture here we will learn the core principle of this learning algorithm that we will be using for the rest of the semester„ÄÇ



![](img/20ba561c26dab43aa06f65958ffadb52_5.png)

So and yeah also the good news is after this lecture here there won't be any new mathematical concepts so in this lecture I will talk a little bit about calculus but that is about it when it comes to the mathematics of this course„ÄÇ

 of course there will be mathematical concepts or applications of the concepts we discussed like the linear algebra stuff and calculus stuff in this lecture„ÄÇ

 but there won't be any new fancier concepts so after this lecture you can relax there won't be any yeah new mathematical complicated things going on„ÄÇ

üòäÔºåSo everything also in deep learning will be extensions and applications of these basic concepts in linear algebra and calculus that we are covering now„ÄÇ



![](img/20ba561c26dab43aa06f65958ffadb52_7.png)

So MIÔºå just to give you the big picture overview of things I have in mind for this lecture„ÄÇ

So first we are going to talk about the different learning modes„ÄÇ

 there's something called online batch and mini batch learning„ÄÇ

It is a general concept that applies to all types of single layer neural networks and multilayer neural networks„ÄÇ

 convol networks and everything basically in deep learning and MIa after just this big overview here„ÄÇ

 we will talk about the relationship between the perceptioncept and linear regression„ÄÇ

And then we will learn about an iterative training algorithm for linear regression„ÄÇ

 You probably know the closed form solution for linear regression these the matrix formulations and such the short con dense formula for solving for the weights and the bias unit directly here we will talk about an iterative learning algorithm because it helps us understand„ÄÇ

How we can train neural networks„ÄÇ It's kind of very closely related„ÄÇ Yeah„ÄÇ

 I will have a little calculus refresher„ÄÇ This will be optional„ÄÇ

 So you notice that there are seven sections this time„ÄÇ it may be a little bit long„ÄÇ

 So this is also really just optional„ÄÇ's I thought it might be good for some of those who have taken calculus a long time ago„ÄÇ

 or maybe not quite sure about calculus anymore„ÄÇ So in that wayÔºå it's like a little refresher„ÄÇ

 but if you are very familiar with calculusÔºå you don't have to watch these videos So in that way it will be a shorter lecture„ÄÇ

 it's also something these optional things that I probably wouldn't have covered in the inper class because yeah„ÄÇ

 this is more like a prerequisite„ÄÇ So in this wayÔºå I wouldn't want to make the lecture in person too long„ÄÇ

 but if you likeÔºå you can watch themÔºå but youre not required to„ÄÇ

 So it's it's just some background information some optional stuff„ÄÇÂóØ„ÄÇYeahÔºå after that„ÄÇ

 we will be talking then about gradient descent That isÔºå the learning algorithm that I„ÄÇ

Briefly outlined in the previous video„ÄÇAnd then we will be training a linear neuron Adeline„ÄÇ

 and I will also show you then echo code example in Pythtorch of how we can train such a linear neuron using the concept of gradient descent Allright„ÄÇ

 so then let's get started with the first video on online batch and mini batch mode in the next video„ÄÇ



![](img/20ba561c26dab43aa06f65958ffadb52_9.png)