# P99ï¼šL13.2- å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ but before we dive into convolutional neural networksã€‚

 I actually wanted to motivate the problem of computer vision and image classification first because it's actually a pretty hard task and I think you don't maybe fully appreciate the good performance of convolutional networks before you thought about how difficult computer vision and image classification actually isã€‚



![](img/bb6cd4c4edd9a5991c27fa84e0ce212e_1.png)

So for instanceï¼Œ if you take a look look at this cat image here soã€‚It's the same catã€‚

 And you as a humanï¼Œ you probably have a easy time telling that this is the same cat in the same cat pictureã€‚

 rightï¼Œ But for a computer or for let's sayï¼Œ standard neural networkã€‚

 it's actually a pretty difficult taskã€‚ So why is thatã€‚I meanã€‚

 if you remember how you used multi layer perceptronsï¼Œ you were essentially concatenating theseã€‚

Image rose into a one long vectorï¼Œ rightï¼Œ soã€‚If I haveï¼Œ let's say this is the firstã€‚Rowã€‚

 and this is the second rowã€‚ And this is the third rowã€‚

 So usually how that would look like it would create thisã€‚Longã€‚ğŸ˜”ã€‚

Beectctor as input to the multi layer perceptionronï¼Œ thenã€‚You have yourã€‚Firstã€‚

 hidden layer and everything is fully connectedï¼Œ rightï¼Œ Soï¼Œ and I want to draw all of itã€‚

 but it would beã€‚Prettyã€‚ğŸ˜”ï¼ŒDense networkã€‚ So there are some other challenges with thatã€‚

 But if you recall how you would compute the activation hereï¼Œ it's essentiallyã€‚Awaittedã€‚

Some of the weights and the inputs plus bias hereã€‚ So in that wayã€‚

 you are essentially summing up the feature valuesï¼Œ rightï¼Œ Then if you have an image like thatã€‚

It's the same image as the one on the right hand sideï¼Œ but everything is darker soã€‚

What happens here is that all the activations will actually be lower compared to this one hereã€‚

 if you haveï¼Œ because if image is darkerï¼Œ the pixel values are lowerã€‚ So that wayã€‚

 also when the lighting and contrast and everything like that changesï¼Œ it will severely affectã€‚

The network performanceã€‚ So one thingï¼Œ of courseï¼Œ it's important to normalize the imagesã€‚ But alsoã€‚

 even if you take careï¼Œ let's say of the lighting and contrastï¼Œ Other issues areã€‚

 it might happen sometimes that the image is just like a little bit un distortistortedï¼Œ rightã€‚

 So here you canï¼Œ as the human still tellï¼Œ it's the same catï¼Œ rightã€‚

But if you take a look at this sum again soã€‚Nowï¼Œ for instanceã€‚Let's say on the left hand sideã€‚

 when you take a look at this pixel regionï¼Œ this pixel region might be contained hereã€‚Higher up hereã€‚

 rightã€‚ So if I have my vectorsã€‚ but then the same region is lower here on on this sideã€‚

 so it might be actually somewhere hereã€‚ So in that caseï¼Œ it would fall here andã€‚Here and hereã€‚

 there are vastly different weights involved when you compute the activationsï¼Œ rightã€‚

 So everything has every every line here is a different weightã€‚

 So you would compute totally different activations againã€‚

 depending on where the object is located in the imageã€‚

 So image classification is actually a pretty hard task if you think about it in the context of a multilayer perceptronã€‚

 So how do we make sureã€‚That we get good performanceã€‚ Of courseï¼Œ yeahï¼Œ for thatã€‚

 one solution would be the convolutional networkï¼Œ But before convolutional networks were inventedã€‚

 what were some other strategiesã€‚ So a classic strategy wasï¼Œ for exampleã€‚

 doing feature extraction manuallyã€‚ So by thatï¼Œ I meanï¼Œ instead of providing the input imageã€‚



![](img/bb6cd4c4edd9a5991c27fa84e0ce212e_3.png)

As the input to the networkï¼Œ for instanceï¼Œ instead of having iris flower as the image inputã€‚

 what people would do is they would study these flowers or think about these flowers and think about what are some features that we can extract from these flowersã€‚

 and then we can just work with these manually extracted featuresã€‚

 So in the case of the iris flowersï¼Œ these features would beï¼Œ for exampleï¼Œ the sepal lengthã€‚

 sepal width petal length and petal widthã€‚ So here you essentially had a domain expertã€‚

 Someone who really understands these flowers and might have a hypothesis and sayï¼Œ hey maybeã€‚

These different iris classesï¼Œ they differï¼Œ yeah by the dimensionsã€‚ But another person alsoï¼Œ I meanã€‚

 might say it is maybe the colourã€‚ So you could alsoï¼Œ for instanceã€‚

 think about describing the colour as a potential featureã€‚ But yeahã€‚

 the the bottom line is we would use these hand engineered featuresã€‚

 we would think about the image instead of just giving it as input and extract some features manuallyã€‚



![](img/bb6cd4c4edd9a5991c27fa84e0ce212e_5.png)

And another traditional approach would be alsoï¼Œ for instanceï¼Œ for face imagesã€‚To yeahã€‚

 to get these or extract these facial landmarksã€‚ So that is also a traditional technique where peopleã€‚

 instead of when they did face recognition provided the full images inputã€‚

 they would develop an algorithm that maps these points to the eyesã€‚

 the nose and the mouth and then do a comparison based on these facial key pointsã€‚

 that's another hand engineeredï¼Œ I would sayï¼Œ hand engineered feature techniqueã€‚



![](img/bb6cd4c4edd9a5991c27fa84e0ce212e_7.png)

Yeahï¼Œ and also one other traditional technique it's also kind of beneficial though still for convolution networks is to preprocess the images so for instanceã€‚

 if you think back of the MT dataset in the Mnes datasetã€‚

 all the digits were already kind of cleaned from backgroundã€‚

 if you imagine someone is writing on piece of paper the piece of paper background was removed it was just the image of the digit and also they were all pretty centered in the center of the imageã€‚

 but if you take a look at real worldorld imagesã€‚Usually you don't just have the object of interest in that image you have a lot of other stuff thereã€‚

 so for instance here you have like this cat in the park or forest hereã€‚

And in traditional computer visionï¼Œ people would manually extract or also develop some background removal tools to to first extract the image before doing classification because that makes the task simplerã€‚

 so basically yeah extractingï¼Œ centering and cropping and things like thatã€‚Of courseã€‚

 this can still be beneficial for convolal networksï¼Œ especially if the data set is smallã€‚ nowadaysã€‚

 howeverï¼Œ if you have a really large data setï¼Œ millions of examplesã€‚

Conary networks can also perform well on these if all the other things are includedã€‚

 because if you have a large amount of imagesï¼Œ then the network will learn how to focus on the important partsã€‚

 For instanceï¼Œ if the task is let's say animal classificationï¼Œ it willã€‚Learn how toï¼Œ yeahã€‚

 focus on the animal region in these imagesã€‚ It will essentially learnã€‚The it will learnã€‚

How to extract these objects there will be an implicit feature extraction inside the conversion network we will see that later in this lecture I will show you some examples of how conversion networks learn and how they are able to learn these feature extractors essentially so in that sense it's also why we call deep learning sometimes feature learning or automatic feature learning soã€‚

Deep learninging is essentially capable of implicitly learning how to extract features instead of doing this manual feature extractionã€‚



![](img/bb6cd4c4edd9a5991c27fa84e0ce212e_9.png)

So in the next video I want to then now finally talk about how these conversion networks workã€‚

 so I will zoom in into one of the would say earliest architectures and then describe to you all these components step by stepã€‚



![](img/bb6cd4c4edd9a5991c27fa84e0ce212e_11.png)