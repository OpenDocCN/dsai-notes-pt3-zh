# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P53ï¼šL8.3- Logistic å›å½’æŸå¤±å¯¼æ•°å’Œè®­ç»ƒ - ShowMeAI - BV1ub4y127jj

Yeah let's now take a look at how we train a logistic regression modelã€‚

 by that I really mean now computing the gradients of the loss function with respect to the weight and bias parameters in the logistic regression model so that we can then use gradient descent to update the model and then in a later video we will see how we implement that encode to the actual practical trainingã€‚



![](img/e27658390d5d2e3c3ac7e6c8b59df939_1.png)

So yeahï¼Œ let's take a look at the logistic syigmoid function againã€‚ so here on the left hand sideã€‚

 that's the logistic syigmoid activation function that we talked about before where we have the net input on the x axis and the output on the y axisã€‚

 it has this syigmoided shapeã€‚And you can already seeã€‚Just by looking at this functionã€‚

 what the gradients might beã€‚ So you can see here the function on the ends it's relatively flatã€‚

 so the gradient should be also very flat like0 around that region and the steepest part is hereã€‚

 so this should also be where the gradient is the largest or let's say the partial derivative with respect to the input is the largestã€‚

Nowã€‚ğŸ˜”ï¼ŒYeahï¼Œ on the left on the right hand sideï¼Œ I was yeahã€‚

 computing the derivative of the sigmoid function with respect to its inputã€‚Soï¼Œ yeahï¼Œ if youï¼Œ yeahã€‚

 use the simple calculus rulesã€‚ So it's how you would compute the derivative of this partï¼Œ rightã€‚

 because you have the derivative of theã€‚What is it called numeratorï¼Ÿ

Which is E to the power of minus zï¼Œ the one cancelsã€‚

 And then you have this square term in the denominatorã€‚And when you solve thatã€‚

 I'm not doing it hereã€‚ So when you solve thatï¼Œ if you're interestedã€‚

 I've done that in the Python machine learning bookï¼Œ I think it's in chapter 12 somewhereã€‚

 but I can also share that if you are really curious about thatã€‚

 but you can probably also find examples of on the Internetã€‚ It's quite involvedã€‚

 It will take you a few sheets of paperï¼Œ I guessã€‚But what the result is is this one here on the right hand side that's the derivative of the logistic sigmoid functionã€‚

 And you can see it's relatively simpleï¼Œ rightï¼Œ it's just the sigmoid function times one minus the sigmoid function right So that's what I meanã€‚

 it has nice derivativesã€‚ That's actually one little yeah not so nice part about this activation function or it's derivativeã€‚

 we will talk about that in a future lectureã€‚ So what's not so nice about it is actually that yeah these these parts here are of the derivative are actually 0ã€‚

 So here we don't have any yeah power to update the functionã€‚ So if these terms are0ã€‚

 this might be a problem when we have multilayer neural networks and then yeah have a chain rule where part of the chain rule is0 because then the wholeã€‚

Loss gradient becomes 0ã€‚ but this is something we will talk about laterã€‚

 So let's now just focus on how this function looks likeã€‚ So in the right lower side hereã€‚

 I have the derivative of this logistic sigite functionã€‚

 And you can see like by S with our intuition hereï¼Œ this partã€‚Where has e is 0ã€‚

That's whether gradient is the highest or the partial derivative is the highestã€‚

 You can see the partial derivative is 0ã€‚25ï¼Œ the largest valueã€‚

 And then for very negative and very positive inputs the derivative goes down to yeah 0ã€‚ It's yeahã€‚

 you can see that here where it's also very flatï¼Œ rightã€‚

 So this is how the logistic sigmoid derivative looks likeã€‚



![](img/e27658390d5d2e3c3ac7e6c8b59df939_3.png)

So and here is how the loss function looks likeã€‚ğŸ˜”ï¼ŒSoï¼Œ at the bottomã€‚Hereã€‚This isï¼Œ againï¼Œ myã€‚Negativeã€‚

Lï¼Œ likelihoodã€‚Loss I talked about in the previous videoã€‚ and meï¼Œ like we talked about beforeã€‚

 there are two partsï¼Œ reallyï¼Œ of this loss functionã€‚

 You can really think of it as the part when the class label is one and the part when the class label is 0ã€‚

 So here we really talk about theã€‚True label in the datasetï¼Œ the label of the training exampleã€‚

 And this is yeah also the true labelã€‚ So these are the true labels provided in the training datasetã€‚

 Againï¼Œ we only computeï¼Œ I meanï¼Œ this might trivialï¼Œ but just to recap itã€‚

 We only compute the loss functionã€‚ during trainingï¼Œ not during predictionã€‚

 right when we have a test set or something and do a predictionã€‚ we don't use the lossã€‚

 It's really just to find the weight parameters of the modelã€‚

 So we are given these true labels from the training setã€‚So what happens ifï¼ŸThe true label is oneã€‚

 let's let's consider this scenario of the true label is oneã€‚

 We have a one here right on on the bottom and we have one here and here if we have a one here on the right hand sideã€‚

Then the whole term here becomes0ï¼Œ and then we have0 times somethingã€‚ So this whole term cancelsã€‚

 So if the true label is oneï¼Œ we really only focus on this part hereã€‚If the true label is oneã€‚

 then our loss function is minus lockã€‚And I can just write this part as aï¼Œ the activationã€‚

Its minus lock of the activationã€‚Noã€‚If my label is0ã€‚Thenã€‚

Let me erase this as maybe a little bit messy hereã€‚We set minus log of aã€‚

 and for this right hand partï¼Œ if y is 0ï¼Œ then this part will be 0ï¼Œ0 times something cancelsã€‚

Here we have a 0ã€‚ So this will be oneã€‚ So ifã€‚The true label is0ï¼Œ then the loss function isã€‚

Minus lockã€‚1 minusã€‚Pã€‚Now let's take a look at the shape of this curves hereã€‚If we let me useã€‚

 I should have maybe used yellow minus lockã€‚1 minus aã€‚ So it matches this orange or yellow line hereã€‚

 So you can see for the value here of the sigmoid functionã€‚

 So this is really like the logistic sigmoidï¼Œ the class membership probabilityã€‚

 And recall this one is forã€‚Maybe use maybe the black colorã€‚This is forã€‚The class table equals oneã€‚

 so the class membership probability for oneã€‚Last oneã€‚ so this is really like this probability hereã€‚

So if what we want so what we want isã€‚This probability to be high if the true label is1ã€‚

And we want this probability to be low if the probability is 0ã€‚ soã€‚

 but we can also write this as1 minusã€‚Y equal equal0ã€‚This is it's easier to think aboutã€‚å—¯ã€‚So let'sã€‚

 let's take a look at now what happens if this probability here is highã€‚

 So if this whole term is highï¼Œ this meansã€‚If we have a low probabilityï¼Œ that this is indeed class 0ã€‚

Soï¼Œ if we have aã€‚Term hereï¼Œ we have a low probability for this oneã€‚

This one will have the whole term with a1 minus included will have a large probabilityã€‚

 and you can seeã€‚We have a high loss in this caseï¼Œ so if we have a probability for class1ã€‚

 so really like the classã€‚One probabilityã€‚If this is high at oneï¼Œ then we have a very large lossã€‚

 you can see thatï¼Œ so the loss approaches infinityã€‚If we have a high probabilityã€‚Vice versaã€‚Theloã€‚

Approaches0 if we have a low probability rightï¼Œ so you can see thatã€‚On the closerã€‚

 I get to a low probabilityã€‚The closer I get to a low probability that the classable is oneã€‚

The smaller the lossï¼Œ because a low probability for class 1 means a high probability for class 0ã€‚

 which is what we wantã€‚Soï¼Œ hereã€‚If the trueï¼Œ if the true label is indeed zeroã€‚

 we want a low probability for class 1ã€‚So here we have a zero lossã€‚

 if we have a zero probability for class 1 because then the probability for y equals0 will be large because of this relationship hereã€‚

The same way nowï¼Œ let's take a look atã€‚Let's use the blue colorã€‚

 the case when the class table is one the same way when we have a low probability for class membership oneã€‚

We have a high loss it approaches infinityï¼Œ rightï¼ŸBecauseuseã€‚I meanã€‚

 we will never really have a probability of 0ã€‚ We will hopefully perhaps something likeã€‚Veryã€‚

 very small because otherwise it will crash our lock functionã€‚ or yeahã€‚ so we will haveã€‚

Lock of minus log ofã€‚0ï¼Œ rightã€‚ So will be a very small numberã€‚ And then this approach oopsã€‚

This loss approaches infinityï¼Œ basicallyã€‚If we have a low probability and we have a high lossã€‚ sorryã€‚

 we have a highã€‚If we have a high probabilityï¼Œ the loss will be very lowã€‚ That's what I want to sayã€‚

 So the takeaway here is reallyï¼Œ that is how our loss function looks likeã€‚

 What we want is we want to have a high probability for the right class and a wrong probabilityã€‚

 like a low probabilityã€‚For the right class or a high probability for the wrong classã€‚ consequentlyã€‚

 will result in a very large lossã€‚ And you can see it's like steeply increasing hereã€‚

 So that's the takeawayã€‚ So there's a large penalty for making a wrong predictionã€‚



![](img/e27658390d5d2e3c3ac7e6c8b59df939_5.png)

Yeahï¼Œ how does the learning rule now look likeï¼Œ I meanã€‚

 this is the same gradient descent rule that we used before when we trained a linear regression model or add a line using gradient descent for that oneã€‚

 what we needed was the derivative of the loss functionã€‚With respect to the weightsã€‚

 So that's the partial derivative of the loss functionï¼Œ with respect to the weightã€‚

 And we also compute the partial derivative of the loss function with respect to the biasã€‚

 It's the same conceptã€‚So in order to do thatï¼Œ we can compose it into three termsã€‚

 It's the same as we done we have done for adeline and linear regressionã€‚

So we compute the derivative of the loss with respect to the output of the activation functionã€‚

Then we compute the derivative of the activation function with respect to the input for Adelineã€‚

 this was just one because the activation function was yeah identity functionã€‚

And then what we also compute is the derivative of the net input with respect to the weightã€‚

 And that was just xï¼Œ rightï¼Œ X J similar to Adelineã€‚So hereï¼Œ the bottomã€‚I've written this outã€‚

 So all these three termsã€‚ So this oneï¼Œ by the wayï¼Œ this is yeahï¼Œ the negative look likelihoodã€‚

What loss that we talked about in the previous slideï¼Œ and if you look at thisã€‚

 the only thing that's really different from Adeline isã€‚That these and these values areï¼Œ of courseã€‚

 different because we have a different loss function in a lineã€‚We had the MEï¼Œ rightã€‚

 where the derivativeã€‚ I saw the ME wasï¼Œ for exampleã€‚Written like thisã€‚

Let's say like this with a one half in frontã€‚ And we can also do the one over nï¼Œ if you likeã€‚Butã€‚

Okayï¼Œ so let's do itã€‚ And thenã€‚We bring the to up front for the derivativeiveï¼Œ so it'sã€‚2 over to nã€‚

 So the two cancelsã€‚Soï¼Œ we just haveã€‚1 overã€‚Andã€‚Timesã€‚Yeahï¼Œ just my my mineã€‚ sorryï¼Œ it should beã€‚

Y hat minus yï¼Œ so that would be the derivative of our MSEã€‚Nowï¼Œ of courseï¼Œ we don't have an MSEã€‚

 We have this negative log Ne for which we have the following derivative with respect to the activation functionã€‚

With respect to its input andã€‚This one for Adelineã€‚What's oneã€‚

And this one is the same because the net input is the sameã€‚

Now the only thing we have to do really is to put those togetherï¼Œ rightã€‚

 we have to just multiply them using the chain rule so it's the same like for Adeline for Adeline just looked simplerã€‚

 but yeah for Adeline we just had one over nã€‚Times way headã€‚That's y timesã€‚X Jï¼Œ rightï¼Ÿ

 So there was one for L line for the logistic oneã€‚ It may look more complicated because we have now this term in this termã€‚

 Howeverï¼Œ theres something nice that's happeningã€‚

![](img/e27658390d5d2e3c3ac7e6c8b59df939_7.png)

Soï¼Œ we can actuallyã€‚I'm not sure what I've done hereï¼Œ we can actually simplify thisã€‚This partã€‚

 and this partï¼Œ when we put them togetherã€‚It simplifiesã€‚To this one hereã€‚

 So you can work out the math if you likeã€‚ But it's yeah a simpleï¼Œ very simple derivative of hereã€‚

 So you can see that these cancel each other nicelyã€‚ And this is why I saidã€‚

That the logistic regression or the sigmoid functions has nice derivativesã€‚That's becauseã€‚

Things cancel here nicelyã€‚And then we just add the x to it and you can see actually this looks exactly like the Adeline learning rule rightã€‚

 This looks exactly like the linear regression one rightã€‚

 isn't that quite cool So here is just from the previous slides that we had on aline and linear regression So this was the stochastic gradient descent learning rule here where we had the initial weightsã€‚

 the follow loop over the training epochs and then the follow loop of the training examples that could also be for mini batchs then we compute the predictionã€‚

ğŸ˜Šã€‚

![](img/e27658390d5d2e3c3ac7e6c8b59df939_9.png)

The gradients hereã€‚And then we do the update hereï¼Œ and then we have the learning rate etta hereã€‚

We can use exactly the sameã€‚Rule for logistic regression Also note there was also a discussion on Piazzaã€‚

We can actually writeï¼Œ rewrite this because I think I've done it the other way around in codeã€‚

 We can actually rewrite this asã€‚Of courseï¼Œ like y hat minus yï¼Œ rightã€‚

' the same thing because with minus and minusã€‚

![](img/e27658390d5d2e3c3ac7e6c8b59df939_11.png)

And that is what I had in the previous slideã€‚ If I go backï¼Œ I have a minus y instead of y minus aã€‚

 So I should say that a isã€‚

![](img/e27658390d5d2e3c3ac7e6c8b59df939_13.png)

Equal to y head hereã€‚ so that's the output of the logistic regression modelã€‚æ‰€ä»¥ã€‚ğŸ˜”ã€‚

And my head are the same thing hereã€‚So let me write just a hereã€‚ğŸ˜”ï¼ŒSoã€‚But I meanã€‚

 we can also write thisï¼Œ maybe it's if it's more clearã€‚I can't erase moreã€‚

Let's maybe you write this asã€‚A minusã€‚Weiã€‚ğŸ˜”ï¼ŒExtremeã€‚And this one should be justã€‚ãˆæ€ã„ã¾ã™ã€‚ğŸ˜”ã€‚

So this is exactly the same learning rule as for Analineã€‚

 What's the difference now why is this logistic regression Well the big difference here is that a is computed differently right So a is now computed byã€‚

Non nonlinear functionï¼Œ whereas for linear regressionï¼Œ it was or Adelineã€‚

 it was just computed as the identity functionã€‚ So in the case of Adeline or linear regressionã€‚

 this was the net inputã€‚Soï¼Œ but yeahï¼Œ that isï¼Œ againã€‚

 why I said it has nice derivatives because we don't even have to change our learning ruleã€‚

 The only thing we have to change is how why head orã€‚Basicallyï¼Œ a is computedã€‚



![](img/e27658390d5d2e3c3ac7e6c8b59df939_15.png)

Alrightï¼Œ so few last things about thisã€‚ So here we can think of this as the whole computation graph againã€‚

 this is our logistic regression computation graphã€‚ We haven't talked about this output yet right soã€‚

We do the trainingã€‚Hereï¼Œ so here'sã€‚Where we compute the errorã€‚ So we have a trueã€‚Let's sayã€‚ğŸ˜”ã€‚

A true labeledã€‚It's called that true level yã€‚That goes in hereã€‚Andã€‚ğŸ˜”ï¼ŒHereã€‚ğŸ˜”ã€‚

What comes out is our A or Y hatã€‚ Let's call it Y hatã€‚

It's actually a probabilityba that it's maybe more clear to call it aã€‚ So we have our aã€‚And oh whileã€‚

True whyã€‚ And from that oneï¼Œ we compute the negative log likelihoodã€‚ Comp the gradientsã€‚

 the gradients or the pressure of the loss with respect to eachã€‚Weight and also the biasã€‚

 So you am actually not showing the bias I just seeã€‚ so it should be technically also a bias unitã€‚

So we compute that oneï¼Œ and then we update the weights and the bias units with that oneã€‚

And this right part here this comes after trainingã€‚ this is just for the predictionã€‚

 So for that one we can use a threshold function similar to Alllineã€‚

 so in logistic regression we can use the following threshold functionã€‚

Here to compute the real Y hatã€‚ So I should have maybe be more consistentã€‚

 say Y hat is the predicted class label and a is the probabilityã€‚ class me probabilityã€‚

 I will be more yeah careful of that when I write the code for the next video anywaysã€‚

 so we can say with a threshold function of the probabilityã€‚So this is aï¼Œ if this is greater than 0ã€‚

5ã€‚Then return class label1ã€‚And otherwiseï¼Œ return class label 0ã€‚ Actuallyã€‚

 we can do it even more computationally efficient during predictionã€‚

 We can even skip the activation functionã€‚ we can just operate with Zã€‚

 We can say if the net input is greater than point or if it's greater than0ï¼Œ then the label is oneã€‚

 Otherwise it's 0ã€‚ Why is thatã€‚

![](img/e27658390d5d2e3c3ac7e6c8b59df939_17.png)

If we take a look at theã€‚Logistic sigmoid function againã€‚ So here that's the net input at 0ã€‚So at 0ã€‚

 the output is 0ã€‚5ã€‚ so we know so weï¼Œ we can say everything above hereï¼Œ above 0ã€‚5 is class labelã€‚1ã€‚

 say Y hatã€‚Class A1 and below hereã€‚Y hat S classã€‚0ï¼Œ but we knowã€‚If we have the threshold ã€‚5ã€‚

 it corresponds to a net input of 0ã€‚ so we can also just say if the net input is greater than0 greater than 0ã€‚

 then it's class label 1ï¼Œ if it's smaller than 0ï¼Œ then it's class label 0ã€‚

 So that's just what I wanted to say about logistic regressionã€‚



![](img/e27658390d5d2e3c3ac7e6c8b59df939_19.png)

Yeahï¼Œ in the next videoï¼Œ I want to talk a little bit about the logics and cross entropyã€‚

 These are just twoã€‚I would sayã€‚Jargon words about or regarding logistic regression logicits and cross entropyã€‚

 I will just briefly discuss what those meanã€‚ And then I will show you logistic regression in a code exampleã€‚

 And if everything in this video goes a little bit fast and confusingã€‚

 I think the code example will probably clarify that because then we will put everything togetherã€‚

 we willã€‚Do the forward passï¼Œ compute the gradients and update the weightsã€‚

 So what I mean is we will reallyã€‚

![](img/e27658390d5d2e3c3ac7e6c8b59df939_21.png)

Implement this whole stochasticï¼Œ gradientï¼Œ decent learning rule in code and that I hope clarifies everythingã€‚



![](img/e27658390d5d2e3c3ac7e6c8b59df939_23.png)