# P110ï¼šæ·±åº¦å­¦ä¹ æ–°é—» #9ï¼Œ2021 å¹´ 3 æœˆ 27 æ—¥ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hiï¼Œ everyoneã€‚ I hope you are doing well and are well prepared for the exam on Tuesdayã€‚

 I just want to clarify againï¼Œ these stuff in the news videos are not topics of the examã€‚

 so you can just watch them and entirely forget about them if you wantã€‚

 And you don't have to watch them at allï¼Œ of courseã€‚ So todayï¼Œ because we have an exam on Tuesdayã€‚

 and I have another talk on Wednesdayã€‚ I will try to keep it rather shortã€‚

 I will mainly introduce a new selfsvised learning technique I foundã€‚ðŸ˜Šã€‚

Then there is something about some of the challenges in deep learning related to causal relationships that I wanted to share with youã€‚

 And there are also a few tools I found really coolã€‚ for instanceï¼Œ a Pytoch profilerã€‚ All rightã€‚

 So with thatï¼Œ let us get startedã€‚Yeah so the first cool thing I wanted to share with you is a cool podcast I discovered this week so I'm not listening to as many podcasts anymore as I once did because sometimes I think it's good to give your brain a breakã€‚

 especially like if you work and study a lot and it's good sometimes just to maybe do sometimes nothing and give your brain some restã€‚

 but if you're doing something boring let's say commuting or household chores like cleaning up at home then yeah sometimes it might be nice to make this a little bit more interesting by listening to some yeah interesting podcastã€‚

So this podcast is about machine learningï¼Œ just came out this weekã€‚

 it's by Peter Abiil who is yeah a very well-known researcher in deep learning and its a style of interview podcasts and there's only one episode outã€‚

 but in this first episode he was interviewing Andre Cappatiï¼Œ who is the director of AI at Teslaã€‚

And they talked aboutï¼Œ yeahã€‚Mainly machine learning and deep learning and how it is like to work at Teslaã€‚

 So I found this very interesting because they were also talking about yeah how deep learning is used in practiceã€‚

 so at Tesla they for instance use deep learning for their autopilotã€‚

 which is some sort of semi automatic self-driving or semi semi self-driving car capability so whether car can drive automatically on highways but they also have recently extended this version to a beta version that can drive alsoã€‚

In yeahï¼Œ non highway situationsã€‚ And they are mainly using cameras for thatã€‚

 like regular cameras mounted on the carã€‚ and they are using under the hoodï¼Œ for instanceã€‚

 convolutional neural networksã€‚ alsoï¼Œ and one of the interesting takeaways from that interview was thatã€‚

Andria Patty mentioned thatã€‚For instanceï¼Œ it's a better bang for the buckã€‚

 If you want to improve your model if you collect more data and you areã€‚

 let's say labeling your dataã€‚ So paying attention how you are label your data and also what type of data you collect that might improve your model more or the performance more then let's say fine tuning or trying out a different modelã€‚

 So sometimesã€‚Just focusing more on the data side can also give you reallyï¼Œ really good resultsã€‚

 It is like a very common pattern I've seen in industryã€‚ For instanceï¼Œ I recall also Andrewngã€‚

 who is now he was a professor at Stanfordï¼Œ who also was the cofounder of Cora and had these very popular machine learning classes that I also took and really enjoyed onlineã€‚

 So he also has a startup companyã€‚ I think it's beyond startupã€‚ Nowï¼Œ It's a relatively big companyã€‚

 focused on working with industry partners andã€‚ðŸ˜Šï¼ŒAlso from things he talks aboutã€‚

 I gathered that yeah a focus is also really on the dataã€‚

 it's very important to get good quality data and if you want to improve your models yet just focusing on the data is is really important compared to just tuning the modelã€‚

 which is very different from academiaï¼Œ for instanceï¼Œ in academiaã€‚

 we usually work with benchmark datas because usually when you look at paper or papersï¼Œ it's likeã€‚

About improving on a certain benchmark dataset like Amsï¼Œ Cypher 10 Inet and many othersã€‚

 So you are using the same data set and just want to see which model performs betterã€‚

 But once you leave the academic situation and you go into industry and you want to develop real productsã€‚

 it's then more about yeah also collecting additional data and not just tuning the model but really yeah collecting more and betterã€‚

 more informative data to improve the performance of your applicationã€‚Okayã€‚

 this was like a long tangentã€‚ Just wanted mentionã€‚ this was actually a pretty cool podcast I likedã€‚

 So if you' are interestedï¼Œ yeahï¼Œ feel free to check that outã€‚ðŸ˜Šã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_1.png)

Yeahï¼Œ so this week I also discovered a new self-supvised learning method I found particularly interestingã€‚

 So if you recall self-supervised learning is essentially about leveraging the feature information or the structure of the data for a supervised learning task usually that's done by creating some label information from the data so you can apply selfsvised learning to un data and then train in the supervised fashion by creating labels here this is a little bit more focused on I would say the structure of the data So theres no I would say explicit label information created here it's more about yeah the structure so this method is called Ba twins and the paper is titled self-supvised learning by a redundancy reductionã€‚

So how that works is that essentially they run a network twiceã€‚ So focusing on this figure hereã€‚

 let's say this part is one networkï¼Œ and then they create a copy of that network soã€‚

Let's say the lower partï¼Œ it looks the same as the upper part except the input is differentã€‚

 so but here it's really just an identical copy of the networkã€‚

 it's the same as running the same network twice and this setup in general is sometimes also called Siameseã€‚

Networkï¼Œ it's also popular in other contextsï¼Œ for exampleï¼Œ traditional face recognition and so forthã€‚

So in any caseï¼Œ so what the goal here is essentially is to learn feature representations that ignore small modifications of that imageã€‚

 for instanceï¼Œ changes in the brightness or slight color perturbations or slight rotations of that imageã€‚

 So what they do is theyã€‚Provide andã€‚Let's say this is the originalã€‚The original imageã€‚

And let's say this is the distorted version of that image where youï¼Œ for exampleã€‚

 make everything a little bit brighter you change the brightnessã€‚

 then you run this through a convolutional networkã€‚

 so you run both of them through a convolutional networkï¼Œ the same convolution networkã€‚

 and it will create a feature representationã€‚And thenã€‚

You have the feature representation of the distorted one up here and the one of the original one hereã€‚

 it could also be toã€‚Different types of distorted imagesï¼Œ for instanceã€‚

 this could be slightly rotated to the leftã€‚ and this one could be slightly rotated to the rightã€‚

 For instanceï¼Œ the the point is here really that you have two images that are similarã€‚Andã€‚

Then you get these feature representations here at the end and you compute the cross correlation matrix and you then try to make the cross correlation matrix similar to an identity matrix where you have the ones in the diagonalã€‚

 And so with that you are trying to learn essentially onã€‚Yeahï¼Œ you're trying to learnã€‚

That these feature representation vectors should be similar to each otherã€‚

 so it should ignore the network should ignore essentially the small perturbations in that imageã€‚

 So yeahï¼Œ hereï¼Œ I summarize it essentially that they run the original and the distorted image through the same networkã€‚

 computer correlation matrixã€‚ and then they have an objective function or loss function that forces this correlation matrix to be close to an identity matrixã€‚

Yeahï¼Œ and that is essentially then forcing the representation vectors of similar examples to be similarã€‚

So here is the codeï¼Œ the Pythtor style pseudocode for thatã€‚

 I found it actually nice that they included that in the paperã€‚

 so here that makes it yeah easy to implement soã€‚Here they produce two augmented version of the imageã€‚

Then compute the representation vector where F would be then the neural networkã€‚And thenã€‚

 they computeã€‚Or they normalizeï¼Œ and then they compute the correlation cross correlation between the twoã€‚

 and then they compute the difference between the cross correlation and then identity matrixã€‚

 They apply squareã€‚ So they square the results because yeah toã€‚

So it doesn't matter whether it's positive or negativeã€‚

 which directions it doesn't matter which which way it isï¼Œ rightï¼ŸAnd thenã€‚They multiply it by lambdaã€‚

 the difference the of diagonalsã€‚ So I guess it's a hyperparmeter like a scaling how how much penalty you would assignã€‚

 I'm not sure if that actually is really necessary because to some extentã€‚

 I would think that the learning rate already takes care of that by scaling the gradientsã€‚

 but I guess it's a little bit more control over the penalty hereã€‚ and then they sum upã€‚

The oftagon notess hereã€‚And essentiallyï¼Œ you want to minimize the difference between those andã€‚

The identity matrixã€‚And yeahï¼Œ this is then the loss that they optimizeã€‚ And that's all they doã€‚

 There is no class table informationã€‚ and they train that methodã€‚ and thenã€‚

When they obtain these feature representation vectorsï¼Œ they train a linear modelã€‚

 so they say linear evaluationï¼Œ usually that's something like logistic regressionã€‚

 some simple linear generalized linear modelï¼Œ and when they apply then the linear evaluation or let's say logistic regression trained on the feature vectorsã€‚

They get a pretty good accuracy hereï¼Œ which is shown hereã€‚ So let me maybe draw thatã€‚

 So if you have your input imageï¼Œ let's call that Xã€‚ it goes through the neural networkã€‚

 the convolutional layersã€‚ And then there's this feature representation vectorã€‚ and theyã€‚

 let's say produce all the feature representation vectors for the whole image netã€‚ So you have theseã€‚

 you treat them as fixedã€‚ So you haveã€‚For the whole imagenetï¼Œ these feature representation vectorsã€‚

 and they are your areã€‚Exs yourï¼Œ let'sã€‚The x the feature input X for the logistic regression modelã€‚

 So it'sã€‚You wanted it hereã€‚So it's called it x 1 for the first training exampleã€‚

 And let's say x 3 for the third training exampleï¼Œ X 2 for the second training exampleã€‚

 But then you also useã€‚The original class labelã€‚From Imnetã€‚And this will beã€‚

This whole thing will be your training setï¼Œ your new training set for Loã€‚Regressionã€‚

It's essentially a traditional supervised classification problemã€‚ So whyï¼Œ why would you do thatã€‚

 This is reallyï¼Œ yeahï¼Œ it's really testing how much information about the imagesã€‚

Is captured in these vectorsã€‚ It's a way of evaluating the con network feature extraction in that senseã€‚

 So here the hypothesis is essentiallyã€‚By forcingã€‚The network to ignore these distortionsã€‚

 you can produce essentially representation vectors that are really representative of the image and then the linear classifier can classify them well and you can seeã€‚

This method when you train it in that manner with a logistic regression on these feature representations gets a 73ã€‚

2% top 1 accuracy on INe and 91% top5 accuracy and there are also some other methods for comparison this is not the best methods so notice there are two methods here at least that are better but it's very competitive it's performing really well here and what I like about it is it's pretty simple if you look at this it's a pretty simple training method and yeah is sometimes also good it's actually pretty cool I thinkã€‚

Yeahï¼Œ so one thing that is great in our day and age is that we have all these technologies now that make communication online easierã€‚

 but yeah one unfortunate downside of that is it's also making cyber bullying easier and other abusive things on the internetã€‚

 So it's actually nice to see that theres now like workshop focusing on issues like that to improve the identification of hateful memesã€‚

 for instanceï¼Œ So I think this workshop was organized by Facebook I'm not sure anymore by Facebook AI research I think but I would have to double checkã€‚

So here are a few examples of that and essentially this is a competition where everyone can participate and develop models and it's a workshop where you're then invited to also yeahã€‚

Write a paper about your model if it's performing wellã€‚

 I wish I would have known about that earlier because then I think we could have made this a class project it would have been coolã€‚

 but yeah it's it was just announced last weekã€‚ it's a little bit late in the semesterã€‚

 but if someone is interested in working on that as aI project this would be an interesting applicationã€‚

 for instance also of convolutional networksã€‚So one thing that is new about this is it's a multila problem I mean not new in the general senseã€‚

 but in the context of our class we haven't worked with multilabel problemsã€‚

 but it's essentially pretty straightforward soã€‚Let's say you have your output layerã€‚

Of a neural networkï¼Œ convol network or multi layer perceptionceptronã€‚

 usually what we used was a soft mixã€‚Function such that these outputsã€‚

That they sum up to one to a probability of oneã€‚ So we have a eachã€‚

 each note is a class membership probabilityï¼Œ rightï¼Œ So the first one is aã€‚Let's say for class 0ã€‚

 given the inputï¼Œ this is for class1ï¼Œ given the input Xï¼Œ and this one is forã€‚Class 2ã€‚

 given an input Xï¼Œ so usuallyã€‚We assume these were exclusiveï¼Œ mutually exclusive classesã€‚

 for exampleï¼Œ in MNTï¼Œ each digit can only be one class right so it makes sense if we have three membership probabilities for let's say digit 1ã€‚

 digit 2 and digit 3 that they sum up to1ã€‚So if for instanceã€‚

 if you have 90% probability for label 1ï¼Œ then it's maybe 5% for label 2 and 5% for label 3ã€‚

 but yeah in a multila problem it's not a requirement soã€‚A data point can have multiple classesã€‚

 for instanceã€‚Here for each memeï¼Œ one task is to predictã€‚The protected categoryï¼Œ for instanceï¼Œ raceã€‚

 disabilityï¼Œ religionï¼Œ nationality and sexã€‚ So a meme couldï¼Œ for instanceã€‚

 target multiple things at onceã€‚ So here you are not constrained of having probability summing up to oneã€‚

 So for instanceï¼Œ you can say for class1ï¼Œ it could beï¼Œ for instanceï¼Œ a 90% probabilityã€‚

 but it could also have a 95% percent probability that it has in additionï¼Œ also label number twoã€‚

 so it can have multiple labelsã€‚How do you achieve thatï¼Ÿ

 So the only thing you really have to do is you just change softm by the logisticã€‚Sigmoid functionã€‚

 So in that wayï¼Œ you don't constrain the network to have these probabilities summing up to oneã€‚

 but essentiallyï¼Œ yeahï¼Œ this would be a very similarã€‚Approachã€‚

 you can use conversion networks for thatã€‚ So yeahï¼Œ if someone is interested hereã€‚

 the important datesï¼Œ Here's a link to this workshopã€‚ I just found that interestingã€‚ I wishï¼Œ yeahã€‚

 we would have known about that earlier because then it would have been a cool class projectã€‚

 But nowï¼Œ yeahï¼Œ I don't want toï¼Œ you spend already so much time working on itã€‚ I don't want toã€‚

Proposed this as the new class projectï¼Œ but if someone is interestedã€‚

 that would be an interesting site projectã€‚

![](img/152bfaa7984ce3e5ad1f90b55f160a3e_3.png)

Yeah so there was an interesting paper recently towards causal representation layer learning it's not a new paper I mean it's relatively new but it's from last month but I have a huge backlog of stuff in the news items that I discover and haven't had a chance to discuss yetã€‚

Soï¼Œ yeahï¼Œ so essentiallyï¼Œ this is about pushing deep learning more towards yeah causal representation learningã€‚

 So one shortcomingã€‚Nowadays is that deep learning or the in generalã€‚

 the current state of supervised learning and yeah predictive modeling is more reliant or is essentially reliant onã€‚

ID data that means independent and identically distributed data where one data point doesn't really influence the other and deep learning systems like classifiersã€‚

 typical classifiers are essentially just learning statistical correlations between the input data and the output dataã€‚

 so you're essentially not learning a system that can really understand the relationship between the data in a causal way it's more like exploiting correlationsã€‚

So for instanceï¼Œ why would it be useful to learn causal relationshipsï¼Ÿ

One aspect that is also kind of mentioned in that paper is it can make models more robust towards unexpected situationsã€‚

 for instanceã€‚If you haveã€‚Let's say a self driving carã€‚ And remember when we hadã€‚

 I showed you that before this adversarial attack where they had a laser beam in or in front of a street signã€‚

 and it was fooling the classifier into thinking that the street sign means something differentlyã€‚

 If there would be a better causal understanding that might be avoidedã€‚So in that wayã€‚

 I think certain adversarial attacks could be mitigated if the network had a better understanding of causal or relationshipsã€‚

 but also it can make training cheaperï¼Œ for instanceã€‚

 let's say you train a classifier to detect objects and one of the classes is let's say predicting whether something is a chair to sit onã€‚

So usually if you want to make this really robustï¼Œ you would have to take pictures of that same chair and include it into the training data set from different anglesã€‚

 a human thoughã€‚A human doesn't really need to see the same chair from different angles to recognize that this is a chairã€‚

 So that wayï¼Œ if the network has a better understandingã€‚

 maybe what the what what makes a chair a chairï¼Œ then in that wayã€‚

 we would maybe require fewer training examplesã€‚ And this would also help withã€‚

Reurpurposing models so that you don't have to train the model from scratch on each new dataset sets and that way it could be more effective maybe to train a model on one dataset and then apply it to anotherã€‚

 for instanceï¼Œ if you think of reinforcement learning also in a grana scheme where you let's say learn an agentã€‚

To play age of empiresã€‚ It's a strategy gameã€‚ Then maybe this agent could be applied also to play Starcraftã€‚

 which is another strategy game withoutï¼Œ let's sayã€‚

 requiring learning from scratch because the games are relatively similarã€‚ So in that wayã€‚

 it could also help with transfer learning and repurposing modelsã€‚ But yeahï¼Œ it'sï¼Œ it's a challengeã€‚

 It's something where people don't have a solution for yetã€‚

 So this paper was mainly highlighting the challenges and proposing someã€‚Potential future directionsã€‚

 but yeahï¼Œ the main challenges essentially areã€‚Whether the data even reveals causal relationships and also then how can we infer these abstract causal variablesã€‚

 so I currently I think there are no solutions yetã€‚

 but people yeah started to think about it more actively and I think that is like an interesting area of research to keep an eye on in the futureã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_5.png)

Alrightï¼Œ let's wrap it up with some of the cool tools I discovered this weekã€‚

 So one of them is classify AI with the rather clever name AIã€‚ Anywaysï¼Œ So hereã€‚

 this tool is essentially a image and notttaã€‚ So there are some other tools that existã€‚

 I shared some of them with you in the context of the class projects beforeã€‚ But yeahã€‚

 this is a new1ï¼Œ I thinkï¼Œ and it looked pretty coolã€‚ It's an open source toolã€‚ andã€‚ðŸ˜Šï¼ŒYeahã€‚

 so it is providing capabilities for annotating images convenientlyï¼Œ for instanceã€‚

 also for object detectionã€‚ So at the bottom here this is a video I hope it plays yeah so it's just an example that shows you how convenient it is to label different objectsã€‚

 So if you are interested in that check it outã€‚ this is freeã€‚ It's an open source toolã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_7.png)

Another thing I wanted to talk about is yeah making your code more efficient so this week I actually spent quite some time making some of my code more efficient because training was relatively slow and I had hundreds or thousands of lines of code and I was just finding or trying to find the bottleneck Why is the training so slow Eventually I found out my mistakeã€‚

Or the inefficiencyï¼Œ is's kind of summarized hereã€‚ So I was computing three thingsï¼Œ every epochã€‚

I was or actually six thingsï¼Œ I was computing the validation set mean absolute error mean square error and the validation set accuracyã€‚

 and then I was computing the same mean absolute error mean square errorã€‚

And accuracy for the training setã€‚So I did that each epoch and each time the way I implemented these functions was by iterating over the dataset because they are too large to load into memoryã€‚

 so I was just iterating over the dataset the same way you would iterate over the batches when you do the training and I did that every epochã€‚

Now that was very inefficient because it takes a lot of time if you have like half a million images in your data set iterating over the training set in each epoch that can easily like take two minutes or something and if something takes two minutes and you train for let's say 200 epochs that's 400 minutes right so it's a lot of time extra timeã€‚

And yeahï¼Œ the downside of that was well not the downsideã€‚

 but the problem I had here was that I did this in a very inefficient wayã€‚

 for instance I computed here the train mean absolute  error and mean squared errorã€‚

Using iteration over the training modelï¼Œ and then I had the same thing here tooã€‚

So how I solved this problem was essentially writing one function that computes M AEã€‚

 ME and accuracy in one one goã€‚ So I don't have to call this function in this function separatelyã€‚

 It was just writing a function that can do both at onceã€‚

 So I would essentially save 400 minutes if one of these takesã€‚

2 minutes each epoch when I was training for 200 epochsã€‚ and essentiallyã€‚

 I also decided just not to track these statistics during training only for the validation setã€‚

 And alsoï¼Œ it saved me a lot of timeã€‚ then another 400 minutesã€‚ So in that wayï¼Œ sometimesã€‚

 yeah thinking aboutã€‚What you're doing in the code can be useful to make your code more efficientã€‚

 especially if you don't really need the training statistics during trainingã€‚ If it's enoughã€‚

 for instanceï¼Œ to plot the final training accuracy and the final validation accuracy to assess overfittingã€‚

 Alrightï¼Œ so long story shortã€‚The main point I wanted to make here is I was spending a lot of time analyzing my coat and trying to find bottlenecksã€‚

 And then yeahï¼Œ on Thursday or soï¼Œ I saw that as' a new toolã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_9.png)

A Pytoch profiler that just came outã€‚ And this is a tool that kind of promises to make this more easy to identify bottlenecks in your codeã€‚

 And I thought that might might be interesting to youã€‚

 So this is based also on a tool called Tensor board and Tensor board is a tool visualization tool that you can use during training to attract different thingsã€‚

Like performance metrics and so forth during trainingã€‚Soã€‚This also got me into a rabbit holeï¼Œ soã€‚

I found out about this Pytoch profileï¼Œ which I think is really coolã€‚

 that might be something worthwhile trying in the futureã€‚ And then from there I sawï¼Œ okayã€‚

 this looks like visual studio code right and then I also saw actually that last month visual studio code released some new capabilities or was updated to add an integration for Tensor board before you had to use Tensor board separately for instanceã€‚

 in a web browser now you can use it directly in visual Studio code so you can have your code and Tensor board side by side and also one of the cool things about Tensor board is it can instead of also in just in addition to just tracking training statistics it can also visualizeã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_11.png)

The neural network graphã€‚ So that is also a helpful debugging technique to make sure the network looks like you expect it to look like that you don't have any weird connections between layers that you didn't intend to connectã€‚

 So that is a cool tool to check out as wellã€‚ Tensorboardã€‚ It's also freeï¼Œ of courseã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_13.png)

And another tool that I saw recently is a tool for comparing different experimentsã€‚

 so previously this tensor board is more for tracking during trainingã€‚

 this is when you are training and you want to look at the loss function during training for exampleã€‚

 it's easy way to visualize the lossã€‚

![](img/152bfaa7984ce3e5ad1f90b55f160a3e_15.png)

In class the code I provide to you is usually that we use Matprolib at the end of the trainingã€‚

 but Tensorboard is really useful during training I didn't want to teach that to you in class because yeah we have already so many things to talk about and I would say this is more like somethingã€‚

That you want to use once you are more familiar with a basic training because yeahã€‚

 it's just another tool and I didn't want to overburden you with too many toolsã€‚

 but I recommend to check that out at some pointã€‚

![](img/152bfaa7984ce3e5ad1f90b55f160a3e_17.png)

This tool here is a little bit differentï¼Œ it's more like for comparing models after trainingã€‚

It's actually I looked at the how you use itã€‚ It's actually super simpleã€‚

 So you only add one or two lines of code and it will write a certain python dictionary to to your hard driveã€‚

 and it will do it such that you can import it into this tool and it will allow you to compare between different runsã€‚

 So if you run the model with different hyperparmeterã€‚

 It will essentially help you to visualize which one is the best hyperparameter configuration and so forthã€‚

 So actually a pretty nice tool here tooã€‚ Of courseï¼Œ this is just one of the manyï¼Œ manyï¼Œ many toolsã€‚

 There are other ones like there'sï¼Œ for exampleï¼Œ I think one is called Hyaï¼Œ one is called M flowã€‚

There are many tools for doing thatã€‚ but yeahï¼Œ this one looked actually pretty coolã€‚ Alsoã€‚

 just wanted to share this because yeahï¼Œ it's also free and also something you may want to check out at one pointã€‚



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_19.png)

Yeahï¼Œ lastlyï¼Œ so I will be giving a talk on Wednesday if someone is interestedã€‚

 so they gave me free tickets for studentsã€‚ I shared them with you on Piazza so someone is interestedã€‚

 my particular talk would be on March 31st 10ï¼Œ40 AM central timeã€‚ðŸ˜Šï¼ŒI think that is11ã€‚

For the in our time zoneã€‚ No waitï¼Œ this is our time zoneã€‚ sorryã€‚

 So this is our time zone central timeã€‚ Now I used to live in Michigan when we were on Eastern Timeã€‚

 So yeahï¼Œ we are in central timeã€‚ So yeahï¼Œ if someone is interested But in generalã€‚

 lots of interesting talks hereï¼Œ you can check it out hereã€‚

 So and because there is an exam on Tuesday that I still have to prepare and I also have to prepare for this talkã€‚

 I will nowï¼Œ yeahï¼Œ end this stuff in the news section and yeahï¼Œ good luck for the exam on Tuesdayã€‚

