# P61ï¼šæ·±åº¦å­¦ä¹ æ–°é—» #5ï¼Œ2021 å¹´ 2 æœˆ 27 æ—¥ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hi everyoneã€‚ I hope you had a nice weekã€‚ So last week we discussed a lot of topics related to computational challenges with deep learningã€‚

 So what do we do if we have these larger models we both require more data and more GPUus and also more time to train these models So coincideally this week I found a lot of news related to the topic of making the training of deep learning models more efficient So that is by spreading out the computation across multiple devices in a process called federated learningã€‚

 but then this also brings privacy concernsã€‚ So there was also some interesting news related to yeah privacy protectionã€‚

And lastlyï¼Œ I find personally the most exciting topic is how we can train these deep neural networks still on a single GPU because yeahã€‚

 truth is most of us only have access to one or few GPUusã€‚

 So wouldn't it be nice if we could train just simply things on multiple GPUs like hundreds of them but yeah in real life it's not so easy So luckily there are still a bunch of techniques that allow us to train deep neural networksã€‚

 even the large ones on single GPUusã€‚ So I will also discuss this topic at the end of today's news sessionã€‚

 but yeah because there are so many topics I wanted to cover todayã€‚

 Let me just dive in and get startedã€‚Yeahï¼Œ let's start with Feerated learningã€‚

 So last week I explained Feerated learning is yeah learning on or using multiple devices like splitting up a computation across multiple devicesã€‚

 So here was an interesting paper by researchers from Appleã€‚

 So this paper was entitled Federated evaluation and tuning for ondevvised personalizationã€‚

 system design and Applicã€‚So does this essentially Apple's undevvised machine learning system for federated evaluation and tuningã€‚

 So federated learning is not newã€‚ I meanï¼Œ a lot of people and companies use federated learning that is using multiple machines to achieve somethingã€‚

So but usually what other companies do is they use federated learning to tune a global neural networkã€‚

So imagineï¼Œ for exampleï¼Œ you have a server with your deep neural network modelã€‚

 and this is let's say an image classifier or something like thatã€‚

 So then this global model can learn from yeahï¼Œ individual peopleã€‚

 So if you have a cell phone or smartphoneï¼Œ it can access this model on the serverã€‚

 And by if you label your dataï¼Œ you can provide training data for this modelã€‚

 So there's usually one global modelã€‚I meanï¼Œ not everyone is using that approachã€‚

 but many people are used this approach where you essentially have or keep one global modelã€‚

 In contrast here in this paperï¼Œ Apple describes a systemã€‚Whereã€‚There are global parametersã€‚

But the model is trained locallyã€‚ So all the user data remains inaccessible to the server sideã€‚

 So that means here they focus on protecting your privacyã€‚ So not sending any user data to a serverã€‚

 So all the training happens locallyã€‚ So you may get some parameters from the global modelã€‚

 but essentially you will get a personalized model on your your phoneï¼Œ for exampleã€‚

 without sharing your personal data with a serverï¼Œ which I find actually pretty cool soã€‚

Here's just a sketch of how this worksï¼Œ whereã€‚There are three types of information being sharedã€‚

 The red arrow is the task configuration and attachmentsã€‚

 information then in green the task results and telemetry and then on blue these on device records and here this one the left box represents the end user device and you can see for the blue onesã€‚

 the blue ones are the user dataï¼Œ the on device records they never really leave the device so on the right hand side is the developer interface So what the developer interfaceã€‚

Has access to is yeahï¼Œ the task configuration and the task resultsï¼Œ because you as a developerã€‚

 you still want to see whether theï¼Œ yeahï¼Œ the the learning model learns wellã€‚ howeverã€‚

 you don't want to see any user data because that would be than a privacy implication or problem soã€‚



![](img/09613e6476d680e6ab682955b4cff563_1.png)

Here's an example of that where they show how this can be used for tuning for news personalization so they haveã€‚

 for exampleï¼Œ different modelsï¼Œ let's call them run one and run two and yeah so here they involve different parameters and different metricsã€‚

From an AB experimentation resultã€‚ Soï¼Œ for exampleã€‚

 which model should be used on a local system and they only have access to information such as how much things improved for exampleã€‚

 So here they have a delta percent so they can see basically that run2 improved the percent the daily article views increase it by 1ã€‚

87ã€‚ And if your metric is to maximize the number of article viewsï¼Œ if that's your metricã€‚

 then you can measure this by just looking at the results you don't have to have access to what types of articles the user readsã€‚



![](img/09613e6476d680e6ab682955b4cff563_3.png)

Yes on the previous slide I highlighted these fundamentally different approaches to federated learning one is keeping one global model on the server and then sending user data to that serverã€‚

 which sounds from a privacy perspective a little bit I would say questionable the second approach sounded a little bit more privacy friendly where you do the training on the device and your user data never leaves the serverã€‚

 however the first approach with a global model is actually not that bad if you take some precautionsã€‚

So there's one area of research called differentiable privacyã€‚ Let me write this downã€‚

 differentiableã€‚Privacyï¼Œ so this deals with or this field develops methods for yeah adding some noise to a data set such that you can't identify yeah people from this dataã€‚

 So there was one interesting article by Microsoft but of courseã€‚This is a broad fieldã€‚

 Many people are working on thisã€‚ I just highlight this article because it was just in the news so but be aware that it's not the only approach with thisã€‚

But againï¼Œ just to highlight the problemã€‚ So if you share data with a server or with anyoneã€‚

 essentiallyï¼Œ even if you don'tï¼Œ let's sayï¼Œ have the names of the persons in that dataã€‚

 it may be possible to identify users or people from this dataã€‚

 So there was this $1 million Netflix price that has been quite some year agoã€‚

 I think maybe a decade ago evenã€‚It was this $1 million price when Netflix had a data set that they shared on Kegggel and they asked people to develop recommendation systems and the best recommendation systemã€‚

 the best movie recommenderã€‚Or the person who developed this or the team would gain $1 millionã€‚

So in order to facilitate this competitionï¼Œ they shared a data setã€‚ It had the titleã€‚

The title of the movieï¼Œ the user IDï¼Œ the date of the rating and the rating itselfã€‚

 So the user ID you can think of it here it is not identifying a particular person It's just for helping to make sure if two movies are rated whether they are rated by the same person or notã€‚

 but you still don't know who the person isã€‚Actuallyï¼Œ I never really had Netflixã€‚

 but I think they don't have these five star ratings anymoreã€‚ I think it's working differently nowã€‚

 I'm honestly not sureï¼Œ but I think they don't do ratings anymoreï¼Œ butã€‚

Probably back in the day they did anywaysã€‚ So the thing hereã€‚

 what happened is that people use the IMDB movie review databaseã€‚

And using this IMDP movie database and looking at this Netflix data here on the left hand sideã€‚

 they were able to identify people in this datasetã€‚ So how that worked was theyã€‚

By MDB by I think by email or usernameï¼Œ they could identify the personã€‚

 and then they were just matching the date of the rating and the rating itselfã€‚

To the records in IMDBã€‚ and by thatï¼Œ they could identify the user ID hereã€‚

To whom it belongs by just matching the date of rating and ratingã€‚

 If you do that for a couple of moviesï¼Œ it becomes very easy to identify certain users by user I Dã€‚

 And then you couldï¼Œ yeahï¼Œ you can see basically all the movies that the person watched and how they rated that movieã€‚

 And that isï¼Œ I thinkï¼Œ a big privacy violationã€‚ if you haven't agreed to if you are Netflix user and you haven't agreed to share that information of all the movies you watchedã€‚

Then yeahï¼Œ that might be a privacy validationã€‚ Maybe you only have a subset on IM DBã€‚

 but you don't want certain people to see what other movies you watchã€‚

 Then this would be problematicã€‚ So differentiable privacy deals with this problemï¼Œ how you canã€‚

Kind of keep the utility of a dataset set So making the dataset still usableï¼Œ but avoidingã€‚

The identification of individualsã€‚ So it's essentially the broad yeahã€‚

 the broad approach is essentially adding or synthesizing noise and adding noise to this dataset so that the dataset has broadly the same characteristicsã€‚

So if you wouldï¼Œ for exampleï¼Œ compute the average ratingï¼Œ the average rating may not changeã€‚

 but there's some noise that for a user forgiven userã€‚

 some of the rating is a little bit higher and lower than they were in real so in that way it's not so easy to go to MDB and unambiguously identify these usersã€‚

Yeahï¼Œ and what's new hereï¼ŸIs that Microsoft released a tool set called smart noiseã€‚

 So in this smart noise yeah contains some Python API and other things to make this more easy to use in practice soã€‚

There is also a sample Gitthub repository where you can take a look at at some examplesã€‚

 and I want to talk too much about the individual techniquesã€‚

 So they it's basically a framework for implementing different techniquesã€‚

 but what I found interesting isã€‚

![](img/09613e6476d680e6ab682955b4cff563_5.png)

From this list here of the techniques they implement that many of these techniques they involve GNsã€‚

 so GNs are generative adversarial networksï¼Œ we will cover them later in this class and they are used essentially to learn the training data distribution and then generating samplesã€‚

 new samples from that training set distribution so we will cover that later in this class I just found it interesting that most techniques for differentiable privacy seem to employ generative adversarial networks nowadays or at least the techniques they implemented in this smart noise approachã€‚



![](img/09613e6476d680e6ab682955b4cff563_7.png)

Yeahï¼Œ related to the topic of data augment I've seen this article this week entitledHums are trying to take bias out of facial recognition programsã€‚

It's not working itã€‚So here in this articleï¼Œ the authors mention that one likely reason for this bias in face recognition is the lack of diversity in the data setsã€‚

 and they say that one common mitigation approach is to provide algorithms with data sets that represent all groups equally and fairlyã€‚

So there was a paper called1 label 1 billion faces usage and consistency of Ra C in computer vision by Zid Khan and Yuun Fu who looked at this problemã€‚

 So they look at the problem whether a diverse data sets can really help and they say yeah it only it can work but only for a very stereotypical sense of fairness so essentially it doesn't really work So heres a quote what they say is the people in the images appear to fit racial stereotypes For example an algorithm was more likely to label in an individual in an image as white if that person had blonde hair so yeah using even a more diverse data doesn't really help with a biased problem so it's still like yeah having these stereotype steps and stuff like that soã€‚

There needs to be more work done on developing systems that are more fairã€‚

 so it's not as easy as making a data more diverseã€‚



![](img/09613e6476d680e6ab682955b4cff563_9.png)

Yeah an interesting yet unrelated topic is AutoMLï¼Œ which stands for automatic machine learningã€‚

 What is automatic machine learning automatic machine learning is about finding yeah good machine learning algorithms and higher parametermeter settings and sometimes also preprocess steps given a certain problemã€‚

So traditionally we humansï¼Œ we try out different algorithms and typerometer settings and yeah data normalization steps and so forth to see what works well on a given data AutoMl is type is a type of approach of automating this process so it's less workforce for humans it's some system on top of machine learning that yeah learns this how to do it well basically so it's basically automating machine learning in a wayã€‚

A specific flavor of OML is neural architecture searchã€‚

 this is specific to neural networks where OML can be broaderã€‚

 it can also involve yeah traditional machine learning algorithmsã€‚

 a neural architecture search is a specific subfield of OTML for neural networksã€‚

 sometimes also it's abbreviated asna for neural architecture searchã€‚So here in this articleã€‚

 the article is entitled introducingtro model search an open source platform for Find optimal machine learning modelsã€‚

 There are a couple of other yeah open source platforms for OMl and I had like a little bit of a hard time seeing what's new hereã€‚

 but I think what's new is really that they yeah approach this problem a little bit differentlyã€‚

 I meanï¼Œ the method is slightly different than other methodsã€‚

 So methods I've seen before they're usually based on reinforcement learningã€‚

 evolutionary algorithms or and or combin notatorial search and here it's also looking like a combination of thoseã€‚

Where they haveã€‚Multiple trainers are trained asynchronously and then there is some beam search going on looking at the results and then considering the best modelsã€‚

 they mutate them so making small changes to the best performing ones which is kind of reminding me of how evolutionary algorithms work and they also talk about knowledge distillation and having some transfer learning hereã€‚

 or not literally transfer learning more sorry more like weighted transferã€‚So hereã€‚

 when I recall correctly what they do is they transfer the weights from well performing models to new models they want to exploreã€‚

 so instead of starting from scratchã€‚

![](img/09613e6476d680e6ab682955b4cff563_11.png)

Yeahï¼Œ so the results look actually pretty goodã€‚ So compared toã€‚Previous methodsã€‚

 So here the dotted lines are the previous methodsã€‚ They found that their methods hereã€‚

 they perform the new method performs better the model search methodã€‚ Soï¼Œ yeahã€‚

 my thought is really that this is fundamentallyï¼Œ the building blocks are not that novelã€‚

 But in the way they are put togetherã€‚ This is a system that performs really wellã€‚

 So I think that's the takeaway that this is reallyã€‚If you want to use it in practiceã€‚

 if you want to use something that performs really wellï¼Œ this might be a good approachã€‚

Yeah what I found interesting thoughï¼Œ when I read thisï¼Œ they said in a recent paperã€‚

 we demonstrated the capabilities of model search in speech domain by discovering something I think I cut it off but yeah so this is from this 2021 article and I said in a recent paper I looked at up I looked at the paper it's from 2019 so it's actually not that recent anymore I still think this is an interesting approach so if you everã€‚

Want to attempt neural architecture search this might be one of the approaches to tryã€‚

 I should warn you though neural architecture search is of course veryã€‚

 very expensive because yeah I mean training a single deep neural network is already expensive and here in this approach imagine you have to train multiple ones asynchronously and yeah this is yet another computational challenge I would say for people who don't have access to hundreds or thousands of GPUusã€‚



![](img/09613e6476d680e6ab682955b4cff563_13.png)

Yesï¼Œ speaking of computational scaling or training large scale models on multiple GPUusã€‚

 so I saw this paper hereï¼Œterrappipeï¼Œ token level pipeline parallelism for training large scale language modelsã€‚

Which is kind of interestingï¼Œ so it's related to the G3 language model which has 175 billion parametersmeterã€‚

 and they found a way that they can train it five times more efficiently on 48 large GPUusã€‚

 So they are these transformer models and there are different ways you can make them more efficient soã€‚

One isã€‚Partetatiitioning the operations hereï¼Œ that's in subfigu Bã€‚

Where I think this means that they just split the computationã€‚

 Let's say the left and the right hand side andã€‚Yeahï¼Œ pass it on to multiple devicesã€‚

 So he had device 1 and2ã€‚ I think there has actually a label problem in that figure when I understand correctlyã€‚

 this should be actually part2 here everywhereã€‚ Yeahã€‚

 but here it's splitting up the computation across multiple devices by splitting itã€‚

 splitting the layer basicallyã€‚Another approach is this micro parallelism or micro micro batchchbased pipeline where they split up the batchesã€‚

 the mini batches into micro batches if I understand it this correctly here So they split it up into also multiple devices this way and here the novelties essentially that they have another method for parallelism where they split up the input the sequence input into tokensã€‚

 I mean this is what you would do always you would always split it up into tokenã€‚

 but then they distribute these tokens onto across different devicesã€‚

 So each device may have a small number of tokens only to considerã€‚

 so you have parallelism across these tokensï¼Œ which is I think the new approach hereã€‚



![](img/09613e6476d680e6ab682955b4cff563_15.png)

Yeahï¼Œ but now let's get to the real interesting part because this concerns training large scale models on a single GPUã€‚

 so I saw this tweet hereï¼Œ a screenshot of a talk by Zilvane Guar who worked at fast APã€‚

Yeah now let's get to the real interesting part training large scale models on a single GPUã€‚

 so I saw this tweet here from a talk by Zilvin Google who works at Fast AIã€‚

 which is a company yeah focusing on education around deep learning and developing awesome friendly APIs for Pytorchã€‚

Soã€‚Here in this talk Zilva and Google summarize the main steps or main approaches for yeah making large model training possible on a single GPUã€‚

 one obvious one that is the easiest also is to reduce the batch size So the input batch size because then if you reduce the batch size you have smaller matrix multiplication So that helps with yeah memory constraints because usually the yeah the problem with GPUus is that you have a fixed size memoryã€‚

 So usually commonï¼Œ I meanï¼Œ not all of them but common cards betweenã€‚

 let's say 12 and 20 gigytes of Ramã€‚And this can be quite limitingï¼Œ especially for these big modelsã€‚

 And yeahï¼Œ a bottleneck is usually matrix multiplicationã€‚

 especially if you have fully connected layers like the linear layers in Pytorrchã€‚

 So reducing the batch size can helpã€‚Another approach is gradient accumulation is kind of related to the batch sizeã€‚

 So if you have very small batchesï¼Œ what will happen is that your updates will be very noisyã€‚

 So remember when we talked about online learning and stochastic gradient descentã€‚

 we will also talk more about that when we talk about choosing learning rates and things like batch normalizationã€‚

 in any caseï¼Œ gradientã€‚Accumulation is an approach where you essentially run backward in Pyr twiceã€‚

 So you accumulate the gradients from multiple mini batches before you do the updateï¼Œ basicallyã€‚

 So in that wayï¼Œ it allows you to use smaller mini batchesã€‚

Another approach is gradient checkpointingã€‚ I actually made a brief figure about thatã€‚

 I will explain it to you in the next slideï¼Œ and then there are also these approaches like zero which has something to do with optimizer and then also shard it data distributed data parallel I will also yeah explain to you how that works in a separate slideã€‚

 and there's also model parallelism and pipeline parallelismã€‚

 These are related approaches where you put parts of the model onto separate GPusã€‚

 So imagine you have a model with a lot of parameters and they don't fit into one GPU And this way you simply yeah split the model across different GPUusã€‚

 I also have a small slide about that because I found yeah some interesting new tools that allow you to do that very convenientlyã€‚



![](img/09613e6476d680e6ab682955b4cff563_17.png)

So here is a visualization of gradient checkpointingï¼Œ I made it for a notebookã€‚

 I tried it all in practice earlier this yearã€‚So here this drawingï¼Œ I should sayã€‚

 is inspired by another post thoughï¼Œ that I recommend you to check out to read a more in detailed coverage of that if you're interestedã€‚

So we talked a little bit about gradientding descent and we have only done it for smaller models like softm regressionã€‚

 but of course the same concept applies if we have multilayer perceptrons and convolutional networks with multiple layersã€‚

 so usually we compute first the forward passã€‚And then we have a loss functionã€‚ Let's sayã€‚

 this is our outputï¼Œ a lossã€‚ So for thatï¼Œ you needã€‚The forward passã€‚

 but then also the signal from the class labelã€‚ So let's say here you have the class label information and your prediction and you use that when you back propagate to your compute the gradientsã€‚

So traditionally what happens is that you keep all these orange nodesã€‚

 the computations from these nodes in memoryï¼Œ so for different gradients you will need these individual steps from the forward pass and if you just run PyToch regularly for exampleã€‚

 it will keep all these computations in memory because you will need them to update them so for exampleã€‚

 just in a very broad conceptwise mannerï¼Œ so in order to update this node during back propagation you need these two nodesã€‚

And then if you want to update this nodeï¼Œ you need these two orange nodesã€‚While you're doing thatã€‚

 thoughï¼Œ you don'tã€‚ you don't need this partã€‚ You will need it at the next stepï¼Œ rightã€‚

 But in that wayã€‚Keeping it in memory is actually efficient in the given moment because you will need the values laterã€‚

 but if you have memory limit limitationï¼Œ this might be something where you want to offload this or you want to not keep it in memory soã€‚



![](img/09613e6476d680e6ab682955b4cff563_19.png)

Graadient checkpointing how it works is as followsã€‚ So you willã€‚Compute the forward passã€‚

 but then you delete all this intermediate information because right nowã€‚

 you are only focusing on updating this data pointã€‚ So you only use this in this computationã€‚

 You forget about this because let's say you can't keep it in memoryã€‚ it's too bigã€‚But thenã€‚

 when youã€‚Update the second node hereã€‚For exampleï¼Œ then in that way you recompute these nodesã€‚

 for example and you also recompute these these onesã€‚

 So the blue ones are the ones that are recomputedã€‚ it's wasteful to recompute them every timeã€‚

 So essentially when you use gradient checkpointingã€‚

 the model training becomes slower because you have to recompute thingsã€‚

 but it still helps you dealing with the memory limitations because you don't keep everything in memoryã€‚

 So in both slidesã€‚

![](img/09613e6476d680e6ab682955b4cff563_21.png)

So here this is the regular approachã€‚ Everything in orange is always kept in memoryã€‚

 So you can see with gradient and checkpointing hereï¼Œ you keep fewer things in memoryã€‚

 only the orange things in memoryï¼Œ but you need to recomputeã€‚

 So that's a tradeoff basically between memory and computational efficiencyã€‚ it's slowerï¼Œ but againã€‚

 it helps you with memory limitationsã€‚

![](img/09613e6476d680e6ab682955b4cff563_23.png)

![](img/09613e6476d680e6ab682955b4cff563_24.png)

Yeahï¼Œ so the remaining things on the slide are shing zero then model parallelism and pipeline parallelismã€‚

 so it sounded like based on the tweet that it was for single GPU useã€‚

 unfortunately it requires multiple GPUusã€‚But stillï¼Œ it'sï¼Œ I thinkã€‚

 a cool selection of things that we can use to make the model training more efficientã€‚

 So I dont want to also discuss this in nitty gritty detail because that would be a lecture in itselfã€‚

 But just to give you a big picture overviewã€‚ So0 redundancy optimizerã€‚

 that's a technique developed byï¼Œ I think it was developed by Microsoftã€‚

 which developed this deep speed libraryã€‚ So there's a tutorial in this deep speed library that discusses what0 does in more detailã€‚

ðŸ˜Šï¼ŒBut the broad overview is that it is essentially about memory optimization using 16 bit floating point operationsã€‚

 and yeahï¼Œ what's nice about it is in contrast to some other things that make model training more efficientã€‚

 Ze does not require any major modification of your model codeã€‚ So in this wayã€‚

 it's more like a wrapper around your modelï¼Œ but it doesn't require you to modify the model significantly by yourselfã€‚

So yeahï¼Œ what it doesï¼Œ it's also then reducing memory consumptionã€‚

 One is by yeah the 16 bit trainingï¼Œ but it's also partitioning on the different statesï¼Œ weightsã€‚

 gradients and optimizer states across the available GPUus and CPUUusã€‚

 So that's why yeah you need also multiple GPUã€‚ Howeverã€‚

 that might be something worthwhile to consider if you have multiple GPUus and memory constraintsã€‚



![](img/09613e6476d680e6ab682955b4cff563_26.png)

So related to that there's also zero offloadï¼Œ which is yeah kind of related to offloading modelsã€‚

 So here they even say that it works for models up to 13 billion parameters and they basically say that one of the bottlenecks is also using optimizers like Adam So Adam is something we will be covering soon in the lecturesã€‚

 which is Adam is still my favorite optimizer it's actually super nice it's very robust toã€‚Yeahã€‚

 for it's very robust for training deep neural networks it almost always converges compared to SGD and momentum where you need a little bit more fiddlingã€‚

 But we will talk more about that in classã€‚And they say they developed this implementationã€‚

 deepD speed CPU atï¼Œ which is an implementation of Adam that is5 to seven times faster than the standard implementationã€‚

 which I also found interestingã€‚

![](img/09613e6476d680e6ab682955b4cff563_28.png)

Soã€‚Yeahï¼Œ related to all these techniquesï¼Œ also to the shardingã€‚

 I also saw a tweet by the Pytorch developers so involving a library called fair scaleã€‚

 which is developed by Facebook AI Researchã€‚And this is a pytwach extension for efficient large scale trainingã€‚

 and it uses this fully sharded dataï¼Œ a data parallel approachã€‚

 So sharding is essentially yeah also a form of splitting the weightsã€‚ So againã€‚

 here has an article if you are interested in more detail how sharding workã€‚

 So this article describes sharding and here on the left hand side this is yeah combination betweenã€‚

Parallel data trainã€‚Distributing the mini batches across GPSus and the shardingã€‚

 So it's a new thing that just gotã€‚Added to this libraryï¼Œ I haven't tried it yetã€‚

 but it also looks like another yetã€‚Yeahï¼Œ cutting edge interesting approachã€‚

 So it's actually based on the Deep speed Microsoft Research Libraryã€‚ But I think yeahã€‚

 the goal here in fair scale is to make it a little bit more easier to use within Pytorchã€‚

 So I looked at some things that fair scale providesã€‚

 So what I also found interesting was this pipeline parallelism in fair scaleã€‚

 I found this particularly interesting because it looks like super easy to useã€‚ Soã€‚



![](img/09613e6476d680e6ab682955b4cff563_30.png)

Heres a problem where you haveï¼Œ for exampleï¼Œ the model that doesn't fit into a single GPUã€‚

 and let's say you have a second GPU that you could useã€‚ technicallyã€‚

 what you could do is all the other tricks I mentioned beforeã€‚

 like reducing the batch size and so forth hereï¼Œ what you could do is you can just put also different parts of the model on different GPU is simple very similar to the model parallelismã€‚

 except that we use Pwach pipeline hereã€‚ So the Pwach sequential pipelineã€‚ We discussed thisã€‚

 I think when we discussed theã€‚Pytoch API in lecture 5ã€‚

So where I showed you how you can have multiple layers in a sequential APIã€‚

 so here you can also use the sequential API for the distributed trainingã€‚

 So if you have multiple layersï¼Œ let's say layer A and a layer B are two layersã€‚

 C and D are two more layersï¼Œ you canã€‚Put them all into your sequential pipeline and then it will automatically take care of itã€‚

 distributing it across devicesã€‚ So hereï¼Œ across two devicesï¼Œ two GPUusã€‚

 and you can also say how balanced these are so you maybe can say if you have a stronger GPU put more onto the first GPU than the other GPU and so forthã€‚

 and then also yeah there's a chunk perimeterã€‚Soï¼Œ yeahã€‚

 that is something I will probably try out pretty soonã€‚ I haven't played around with that yetã€‚

 but it looked super cool because it's yeah simple to useã€‚ But yeahï¼Œ if you have one GPUã€‚

 this would of course be a bottleneckã€‚ but so it would still be a problemã€‚So in that wayã€‚

 I would rather consider reducing the batch size and gradient checkpointing if you have a single GPUã€‚

 Allrightï¼Œ so it was probably a very long video I will just stop it at this point and I will see you back in class on Mondayã€‚



![](img/09613e6476d680e6ab682955b4cff563_32.png)