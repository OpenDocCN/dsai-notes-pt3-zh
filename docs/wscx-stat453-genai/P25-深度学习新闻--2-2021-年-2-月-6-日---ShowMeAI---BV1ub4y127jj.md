# P25ï¼šæ·±åº¦å­¦ä¹ æ–°é—» #2ï¼Œ2021 å¹´ 2 æœˆ 6 æ—¥ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hiï¼Œ everyoneã€‚ I can't believe we already completed the second week of this semesterã€‚

 but in the meantimeï¼Œ there have been a lot of interesting things happening in the deep learning and artificial intelligence worldã€‚

 So here I'm back with stuff in the news number tooã€‚

 Some of the interesting things that happen this weekã€‚

 So we will be mainly seeing things from my perspective things I have been reading aboutã€‚

 But I also had to trim down the listã€‚ yeahï¼Œ quite a bit because there hell always lots of interesting research papers and news articles coming outã€‚

 So but with thatï¼Œ let me not waste too much time on the introduction and get started with the stuff in the newsã€‚

ðŸ˜Šã€‚

![](img/829a5200a794f56317f2f6a8221e5d48_1.png)

Yeahï¼Œ one of the things that piqued my interest this week was this article entitled Seu Research Explo Crazy idea of Automating AI paper reviewsã€‚

So paper reviews that's a veryï¼Œ I would say sensitive topic in the deep learning and machine learning community right nowã€‚

 because at yeah these top conferences where people submit papers like ICML andã€‚

New reps there are usually aroundï¼Œ I don't knowï¼Œ5 to 10000ã€‚

 let's say 10000 submissions every year nowadaysã€‚ and it's yet pretty challenging to find or to maintain good quality reviewsã€‚

 I'm also a review for both of these conferences and get about like 3 to 5 papers to review also the timeline is very shortã€‚

 And yeahï¼Œ if you have 10000 submissionsï¼Œ you have to find also 10000 reviews every one of those reviewing three papers or moreã€‚

 and it's really hard to yeah keep up with these numbersã€‚

 So I think that was what motivated these people toã€‚Work on this ideaã€‚

 I would say I'm not honestly convinced this is a good thing to work on or even try to automate paper reviews with with AIã€‚

 Maybe it's good for checking general formats like beyond the yeah conventionalã€‚

Quality checks like whether the paper has or has a right page limit or right template and things like thatã€‚

 maybe AI can be used for some quality checksã€‚ I'm a little bit skeptical whether it can be used to really make or should make decisions on the quality or regarding the quality of a paperã€‚

Nonethelessï¼Œ I think this is an interesting project from a language perspectiveï¼Œ like assessing howã€‚

Far deep learning has come in terms of understanding human language and contents of more comprehensive textsã€‚

And also what I found like as a general interesting takeaway are these most frequently mentioned qualities of a good reviewã€‚

 so these are things to yeah assess whether a reviewã€‚

 peer review is good or not and this is actually something I'm thinking about adopting as a rubric for our class projects because like I mentioned before there will be a peer review for our class projects at the end of the semester and I will of course share more details later on during the semesterã€‚

 but this might be a good rubric for me also to assess whether the peer reviews you write about the reports of other students might be good peer reviewsã€‚

 so something like decisivenessï¼Œ comprehensivenessï¼Œ justificationã€‚

 accuracy and kindness kindness is also very important because yeah reviews can sometimes be grumpy and I think it's usually not very not very nice and not very fair to write grumpier reviewsã€‚

Alrightï¼Œ so yeah this model they proposed is based on BRT and bart Bart is a denoing out encoder it's related to BRT and it this like for pretraining sequence to sequence models we will talk more about sequence to sequence models yeah in part5 of this course at the end of the semester so sequence to sequence means that the input to the model is a sequence and the output is also a sequence instead of just a single prediction so instead of just predicting a single class label you are outputting also a sequenceã€‚

 for example textã€‚A simple example also of yeah sequence to sequence models would beã€‚

 for example language translation where you translate from one language to the other so here they yeah involve of course human annotations so they annotate the paper with these statistics I think some stands for summary or actually posted it here summary MOT is for motivation S UBs for substance so they tag humans tag the article with these different aspects then they train a tagger that can do the same thingã€‚

Then there is some post processing going on and then a human has to evaluate the resultsã€‚

 So if you are interested in this project or want to read moreã€‚

 there's the paper here on archive and they also posted yeah the code on Githubs if you want to play around with us if you are curiousã€‚

Alrightï¼Œ moving onï¼Œ you have probably never wondered what a night made of spaghetti would look likeã€‚

 So this is not new newsã€‚ It's a a bit olderã€‚ It's like a month old or something that's like a machine learning project called Dal Eã€‚

It's like I think a meshup of Wall E and the artist Daliã€‚

 So in that way here what the researchers did is they trained another language modelï¼Œ a G3ã€‚

 12 billion parametermeter model and in this doll E model here which is based on G3 or the concepts of G3 but the researchers do is they train a model onã€‚

Images and descriptions of imagesã€‚ And then so these are the inputs so you can think of it as Inet for exampleã€‚

 and then you can query the model and it gives you outputs and the outputs outputs are meshups from the text inputã€‚

 So for exampleï¼Œ you can type in spaghetti night or night made of spaghetti you can type that in and the model will produce new images that have both concepts in commonã€‚

 for exampleï¼Œ night and spaghetti and here are some some of the results of nights made out of spaghettiã€‚

 so these are images that don't existã€‚ This is something the model is synthesizedã€‚Unfortunatelyã€‚

 yeahï¼Œ the paper is not available and the code is also not availableï¼Œ but there is a block articleã€‚

 if you want to read more about this projectï¼Œ I thought it might be also a fun project especially if you are interested in generative modelsã€‚

Yeahï¼Œ and while the DalE paper and code are currently not available yetã€‚

One part of delE is availableï¼Œ this a so-called clip model or approachã€‚

 so C stands for contrastive language image pretrainingã€‚

 So this is something that DlE uses under the hoodã€‚

 It's also based on yeah GT3 I think so and what clip does its you can think of it as yeah and almost like an image classifier in that senseã€‚

 but it is doing something cleverã€‚So clip is based on zero shot transfer so here zero shot means classifying something where you don't have any training example forã€‚

 So as I mentioned earlier I'm also yeah working a lot in the area of few short learning with my student ju and in a few short learning we have this setting likeã€‚

En shotã€‚K wayï¼Œ few short learningã€‚ So the Nï¼Œ in end shot few short learning stands for the number of examples per class in case the number of classesã€‚

 Soï¼Œ for exampleï¼Œ if you have aã€‚Let's sayï¼Œ a5ã€‚Short10ã€‚Way few short learning problemã€‚

 That means you have 10 different classesï¼Œ but only five examples per classï¼Œ a very small problemã€‚

 And it's very challenging to yeah develop deep learning systems that can work with such small dataset setsã€‚

 So zero shot is even more extremeã€‚ it really means you have zero examplesã€‚ So in this caseã€‚

 the model should predict somethingï¼Œ even though it may have never seen this class beforeã€‚

 So how would you approach thisã€‚ So for thatã€‚The researchers use natural language supervision and multimodal learningã€‚

 so multimodal learning stands for a means using different data sourcesã€‚

 So here they use image data and text dataã€‚ So that's where the natural language supervision part comes inã€‚

 So in particular they are predicting novel image classes based on word embeddings So when they train the data or the modelã€‚

 they use both descriptions of an image and the image itself and then if they see a new class they can use yeah word embeddings or the descriptions to predict the classã€‚

So here's an example of thatï¼Œ so this is from the paper or the blog post I think that I linked here so what they do is here they compare their model with a regular residual net 101 this is something we will also talk about later in class that's a convolution network trained on I think it's yet they are it's trained on imageNeã€‚

And I should sayã€‚Noï¼Œ nonene of thats clipã€‚Thing is fine tuned to these dataset setsã€‚

 So it's trained on a different dataset and then applied to dataset setsã€‚

 It has not been fine tuned toã€‚ So this restnet here was trained on imagenetï¼Œ and it reaches 76ã€‚

2% accuracy prediction accuracyã€‚ The split modelï¼Œ which was trained on a different dataset also reaches 76ã€‚

2% accuracy on this datasetã€‚ But what's more impressive is it also achieves good performance on different types of imagenets that were not yeah contained in this training setã€‚

 So in that wayã€‚This clip model yeah generalizes better to other types of datasets by this concept of this contrast of language image freetrainã€‚

 So that is also I thinkï¼Œ an interesting idea or approach and you can also find more about this and the paper here and the code is also available here and this I think can be potentially really useful if for exampleã€‚

 you yeah develop for exampleï¼Œ let's say a self-driving car and you have a large data and you want to look for specific videos or clips or different images like I want to have a image of let's say a pedestrian crossing a busy intersection So in this way you can have maybe this model finding youã€‚

Qury very efficiently in the large data setã€‚ And why is that interestingï¼Œ Becauseï¼Œ yeahã€‚

 you can imagine you don't have to train the model on all possible queries if you have these word embeddingsã€‚

 you can search for something that is similar to your input word embeddingã€‚

 and it pulls out the corresponding imageã€‚So in that wayã€‚

 I think this is actually a very interesting approachã€‚Yeahã€‚

 and here is a big picture overview of how the method worksã€‚

 It has been some time since yeah lecture  one wayï¼Œ I think I briefly mentioned itã€‚

 but contrast of pretrainï¼Œ that's a form ofã€‚Self supervisedã€‚

Learning and here in this particular caseï¼Œ what they do is they train a model on text descriptions and imagesã€‚

 So in the example I gave earlier about self-subervised learning the researchers used an image and then a transformed version of this image and the model had to predict whether it's the same image or not compared to a image that is differentã€‚

 So like being able to distinguish different images from each other and modified versions of the same imageã€‚

And here in this caseï¼Œ it's kind of a similar conceptã€‚

 but here it's based on text data and image dataï¼Œ so they train this text encoder on the test text description and the image decoder on the image description so they produce then a so calledled embedding and thenã€‚

Based on the text and the image embeddingï¼Œ the model learns like the relationshipã€‚

 So it should be basically let's say for theï¼Œ I think that's what it means for the diagonalã€‚

 if you have an image from a certain class from this class and a description that matches this classã€‚

 It should give a high relationships score or something like thatã€‚

 So you would train the model basically to associate the right description with the right imageã€‚

 then you can if you have a data set of descriptions and imagesã€‚

 you can shuffle them and it should give you a low score if you have a description that does not match your image and so forthã€‚

 So let's say you pretrain your modelï¼Œ then you can create a data classifier from text labelã€‚

 So here what they do is theyã€‚Andbed the class label in a text descriptionï¼Œ for exampleï¼Œ hereã€‚

 a photo of aã€‚Planin carï¼Œ dogï¼Œ birdï¼Œ and so forthã€‚And then yeahï¼Œ you produce these text embeddingsã€‚

 and now imagine you have a new image that you want to classifyã€‚ you don't know the class neighborã€‚

 that is what you want to find outã€‚ So this a zero shot predictionã€‚

 This means this class label doesn't have to be in necessarily the training setã€‚

So you don't have to have an example of that if you have the right embeddingsã€‚In that wayã€‚

 what they have here is they have this imageã€‚ They go give it to the image encoderï¼Œ and thenã€‚

You have an embeddingï¼Œ and then you would pull the text description embedding that has the highest scoresã€‚

 In this caseï¼Œ a photo of a docï¼Œ for exampleã€‚ So in this caseï¼Œ youã€‚

From the text and codingding and the imagery would then pull out the class from that oneã€‚Yeahã€‚

 that is on a high level how this clip method worksã€‚

 I mentioned the model G multiple times G 2 and G 3ã€‚ unfortunatelyï¼Œ G 3 is not available yetã€‚

 So this is a model byã€‚Open AI for G 2ï¼Œ I'm actually not sure whether it's already availableã€‚

 I haven't followed it very closelyã€‚ Howeverï¼Œ theres I also saw now an initiative to train an open source version of that modelã€‚

So here they say our primary goal is to replicate GT3 sized modelã€‚

Models and open source to the public for freeã€‚ So they train the models onã€‚

The data set called the pileï¼Œ which is 825 GB modelã€‚

 including like YouTube descriptions papers from Pubmed and so forthã€‚

 And it is like a 200 parameter modelã€‚ It's very bigã€‚ Soï¼Œ yeahã€‚

 it's big initiative because these models for usï¼Œ normal people are really out of reachã€‚

 You really need a big company to be able to train such a modelã€‚

 And I remember when the first bird or G models came outã€‚

 I think it was estimated like the cost was estimated to be like around $100000 just to train the modelã€‚

 And I think for this G3ï¼Œ which is magnet to it's biggerã€‚

 It's probably in the realm of almost  a million dollars to just train the modelã€‚

 Sos it's actually good then that someone maybeã€‚Does that in a young in a way that is then also available and free for the public Alright so moving on I also saw it's not directly related to deeping but I also saw yeah this news article today so it's about the yeah popularity of programming languages and tools so this is by O'reilly and they based their analysis based on queries on their website and books and things like that like mostly search terms on their website and what people look forã€‚

So yeahï¼Œ unsurprisinglyï¼Œ Python isï¼Œ of courseï¼Œ the first placeã€‚

 Also it all or it or it's growing againã€‚ So it's still not stagnatingã€‚ So people yeahã€‚

 use more and more pythonï¼Œ which I think isï¼Œ yeahï¼Œ is good to knowã€‚

 So you know where things are headedã€‚What I also found interesting is Scalaï¼Œ So Scalaã€‚

 I thought personally that might be the next hot thing when I was in grad school when I was in grad schoolã€‚

 everyone started talking about Scala at some point which is language kind of related to jascript or I think it runs in the JVM the ja virtual machine personally I have to say I never really used it and now seeing that there's this big decline I'm also glad that I didn't waste my time learning a new programming language because that's also always a trade off it's like the tradeoff between doing researchã€‚

 getting things done and doing things like learning new languages So there's like a trade off you want to always be up to date with current technology and what people are doing because that's how you keep up to date and can produce cutting edge work but then also you don't want to waste your time by pursuing things that are maybe not worthwhile So for now it seems like Python is still a good choice for machine learning and deep learningã€‚

Alrightï¼Œ so another interesting project I saw was this approach or method byã€‚Facebook AI researchã€‚

 where they are teaching AI to manipulate objects using visual demosã€‚ So it'sã€‚

 yeah currently very challenging still to train robotsã€‚

 There's a field called reinforcement learningï¼Œ As you recall from the introductory lecturesã€‚

 So in this wayï¼Œ robots learn by trial and errorã€‚ So you train a robot to do something by having it fail a lot of timesã€‚

And there's another approach toã€‚Reinforcement learning called inverse reinforcement learningã€‚

 which is like learning based on a demonstrationã€‚ Howeverï¼Œ traditionallyã€‚

 it was kind of also not very easy because you had to have a virtual environment for that where you can simulate this behavior that you want your agent to mimicã€‚

 So here a Facebook AI research proposed a model based inverse reinforcement learningã€‚

Approach using visual demonstrations on a physical robotã€‚

 So that means the robot can learn by looking at something on a video or series of images instead of having this virtual environmentã€‚

 for exampleï¼Œ a video game context a 3D worldï¼Œ you can show the robot images of a task that a human performsã€‚

 for exampleï¼Œ hereï¼Œ a human is yeah grabbing a bottle and moving a bottle around if you show the reinforcement learning agent the videoã€‚

 it canï¼Œ to some extentï¼Œ learn to perform this behavior as wellã€‚ So there's a videoã€‚

 I can't unfortunately play it in the slidesã€‚ but if you are interestedï¼Œ yeahã€‚

 you can find out more hereã€‚And this blog post addressã€‚

 and there's also the code available on Gitthubã€‚lastlyï¼Œ the last topic for today is about TNã€‚

 tea distributed stochastic neighbor embeddingï¼Œ and this is a very popular technique in deep learning to visualize high dimensional data setsã€‚

Yeahï¼Œ I have an image of Mistã€‚ So Mnitï¼Œ as you knowã€‚

 is a high dimensional dataset right So for Mness for these  hundredwritten0 digitsã€‚

 you have 28 times 28 pixelsï¼Œ which is 784 dimensionsã€‚

 So you have a very high dimensional data set and you can' really easily visualize that in a scatterttleboard for exampleã€‚

 So you can use Tniï¼Œ thoughï¼Œ to reduce the dimensionality of this dataset into down to two dimensionsã€‚

 and yeah the interesting aspect about a Tni is that it preserves the relationship between the high dimensional space between the objects in a high dimensional space and in the new lower dimensional spaceã€‚

 So here isã€‚The result ofã€‚A Tney dimensionality reduction into two dimensionsã€‚

 let's say you have two features now x1 and x2ã€‚ And you can see that the Tney algorithm takes this high dimensional 784 dimensional Mnes embeddings andã€‚

Puts them into this 2D space such that the relationship between the numbers is still preservedã€‚

 You can see all the zeros here cluster together or the seventh cluster together or the fourth fourthã€‚

Clued together and so forthã€‚ So you can see it kind of preserves the relationship or yeahï¼Œ theã€‚

 the relative meaning of these numbers byã€‚Yeahï¼Œ when when it is projecting those into a higher dimensional lower dimensional space and you can also think of it as principal component analysis analysisã€‚

 but principal component analysis analysis is a tool it a linear transformationã€‚ so PCAã€‚

It's a lineariumã€‚Transformationã€‚And this method hereã€‚

 the Tsney method is not just a linear transformationã€‚ It's a little bit more involved than thatã€‚

 I actually taught Tsney in statistics 4ï¼Œ79ã€‚I have actually some slides I can uploadã€‚

 I had a full lecture on thatã€‚ I don't want to go into too much detail hereã€‚

 but it's a very cool method'sã€‚Yeahï¼Œ really usefulã€‚

 But one kind of downside of that is that you have to tune the hyper parametersmeter in order to make it perform wellã€‚

 And usually you don't know because you don't know theï¼Œ yeah labels alwaysã€‚

 you don't know what a good setting isï¼Œ soã€‚There was a paper I saw this weekã€‚

 it's about initialization is critical for preserving the global structure in both Tsney and UMP so here the researchers say that it's really important that you choose the right parameter when you use Ts the right hyperparmeter and one of those is the random start like how you start Tney the initialization and here they found that TN will perform much much better if you use actually PC as an initialization you basically initialize TN with PCAã€‚

Reult so in that way they found that for exampleï¼Œ when they compared TN with PCA initialization and the random initialization that TE performed muchã€‚

 much better across everything hereï¼Œ almost everything I think the way they measure that enough I recall correctlyã€‚

Is they measure the correlation between the distances in the original space and the reduced spaceã€‚

 So if I go backï¼Œ they would compute the distance between two images in the high dimensional spaceã€‚

And then they would compute it in the low dimensional space in the two dimensional space and then check the correlation for these different approaches and they found basically that using Tney with PCA really preserves the distance between objects in the low dimensional spaceã€‚

Yet here as you can see from the title there's another method called UMPã€‚

 this is also a very good method and it's recently even become more popular than TN because sometimes it's even better at preserving this structureã€‚

So yeahï¼Œ with that this isï¼Œ yeah Tneyï¼Œ I think you may probably want to use it at some point in your class projectsã€‚

 It's really useful for visualizing high dimensionional data setsï¼Œ but in practiceã€‚

 it's not really only used for visualizing dataset setsã€‚

 It's usually also used for understanding what a deep neural network doesï¼Œ for exampleã€‚

You would apply that to an embedding of a neural network to really understand what information is con contained in the networkã€‚

 So if you have a big conversion networkã€‚ So you usually haveã€‚Layers that become smaller in sizeã€‚

 the deeper the network becomesï¼Œ and then researchersï¼Œ for exampleã€‚

 would take the later embeddings and then apply TN to see how well it separates different classes in the slower embedding and so forthã€‚

Yeahï¼Œ just like a last thing here about hisneyã€‚ So in psychic learn on the randomã€‚Sorryã€‚

 the initialization scheme is a random initialization and yeah here you if you want to use T name practice according to this research paperã€‚

 you want to change that to PCA So instead of using the default random you would want to use PCA because yeah it performs better according to this paper on the previous slide and also there is a discussion right now on the psychic learn issue tracker whereã€‚

It's proposed to change this default behavior so that it will also use PCA in by default and then also there are some other tips so if you want to use Tney I would recommend you to check out this issue here where there's a more in- depth yeah discussion and with that I think it's already a long video so enough news for this week I will see you on Tuesday with the next lectureã€‚



![](img/829a5200a794f56317f2f6a8221e5d48_3.png)