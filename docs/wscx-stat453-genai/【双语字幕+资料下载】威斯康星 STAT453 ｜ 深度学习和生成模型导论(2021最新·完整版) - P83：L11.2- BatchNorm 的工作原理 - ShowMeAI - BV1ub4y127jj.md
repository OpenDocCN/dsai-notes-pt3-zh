# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P83ï¼šL11.2- BatchNorm çš„å·¥ä½œåŸç† - ShowMeAI - BV1ub4y127jj

Okayï¼Œ let's now talk about extending the input normalization to the hidden layers using the technique called batch normalizationã€‚

 I will just show you in this video how it worksã€‚ and then in the next video I will show you how we can use it in Pytorarchã€‚

 and then I will show you or explain to you why it worksã€‚

At least we will discuss some of the theories trying to explain how it worksã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_1.png)

Alrightï¼Œ so batch normalization or in short batch norm goes back to a paper published in 2015 called batch normalization accelerating deep network training by reducing internal covariate shift So internal covariator shift that's like yeah I don't know fancy word for just saying that the feature distributions shift during the course of the training internally in the hidden layerss we will explain or think about this more when I go to the video where I offer some theories why batch normalization works well in practiceã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_3.png)

So here on just the I would sayï¼Œ the short version of batch normalizationï¼Œ what it isã€‚

 it's about normalizing the hidden layer inputs instead of just normalizing the inputs to the networkã€‚

 we normalize yeah also internally the inputs to each hidden layer and this helps yeah with exploding and vanishing gradient problems so that the gradient sound become too large or too smallã€‚

And overallï¼Œ it can increase the stabilityã€‚During trainingã€‚ So in that wayã€‚

 training progresses more smoothlyï¼Œ and alsoï¼Œ the convergence rate improvesã€‚

 That means it may be that we need fewer epochs toï¼Œ yeahã€‚

 get the same loss that we would achieve if we don't use batchã€‚ So usually with batchiaã€‚

 the networks train fasterã€‚And how you can think of it is as an additional normalization layerã€‚

 And there are also some additional trainable parameters involvedã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_5.png)

Yeahï¼Œ so in the next couple of slidesï¼Œ I'm going to walk you through the procedure of batch normalizationã€‚

 So just for context on let's suppose we have a multi layer perceptronã€‚

 So let's say we focus in on this activation here and the second hidden layerã€‚

 the first activation in the second hidden layerã€‚And we are actually looking at the net input of that activationã€‚

 So remember how we compute the activationã€‚ So it's usually by applyingã€‚

Activation function to the net inputã€‚What type of activation function doesn't really matter hereã€‚

 it could be logistic sigmoidï¼Œ 10 H or Relu should work with any type of activation functionã€‚Alrightã€‚

 so yeahï¼Œ we are focusing on this hidden layer net input corresponding to the activation here now and also now suppose we have a mini batchã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_7.png)

So we have a mini batch such that the net input of a given training example at layer 2 is written as followssã€‚

 so I have now this index I here for the index of the training example in that mini batch so if I have I equals one this would be the first example in that mini batch and just of the first simplicity I will actually ignore the layer index in the next couple of slides just so that the other notation is a bit simpler to readã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_9.png)

So yeahï¼Œ here that's the first step of patronomeã€‚ There are two stepsã€‚

 So the first step is to normalize the net inputsã€‚ This is essentially the same as the standardization that I explained earlier for yeahã€‚

 the inputsã€‚ So what's going on here is we are computing the meanã€‚

Over a feature So the J is the feature index againã€‚Soã€‚

You can actually use batchome for any type of inputã€‚

 So we will also see there is a two dimensional version for that for convolutional networks later onã€‚

 But here we are just focusing on the regular one dimensional version where you have a feature vectorã€‚

 for instanceã€‚ So let's say we haveã€‚Yeahï¼Œ the J featureã€‚ And if I go backã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_11.png)

So if you consider this activation hereï¼Œ what are the featuresã€‚

 So the features are all essentially all the previousã€‚Layer activationsï¼Œ rightï¼Œ So all theseã€‚Go intoã€‚

That activationã€‚ So all of these here are the featuresã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_13.png)

offã€‚This activation hereã€‚So J really is the index over the activations from the previous layersã€‚

 or we can also think of it as the feature index or previousã€‚Which are the previousã€‚Layersã€‚

Mctivationsã€‚Yeahï¼Œ so we compute from that the meanã€‚Andã€‚ğŸ˜”ï¼ŒThe varianceã€‚So that isï¼Œ yeahã€‚

 nothing that should be new to youã€‚ It's simpleï¼Œ yeahï¼Œ simple calculation hereã€‚

 And then we standardizeã€‚ So this is like what we had in a previous video as standardizationã€‚

And this is essentially it's in that wayï¼Œ step one of batchome is similar to the input standardizationã€‚

Except now we are looking at a hidden layerã€‚ So instead of lookingã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_15.png)

Instead of looking at standardizing Xï¼Œ we are now standardizing these A's from the previous layerã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_17.png)

So after step one comes step2ï¼Œ so one more thing to say about step1ã€‚

 so in practice we modify this standardization slightly such that we don't bump into yeah division zero issues if the variance is zero if there's no varianceã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_19.png)

So for numerical stabilityï¼Œ we add a small value epsilon hereã€‚

 So here we have the variance plus this little epsilonã€‚

 And then we take the square root instead of dividing by the the directlyã€‚

 That's just like a small computational trick to prevent division by0  errorssã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_21.png)

Nowï¼Œ step 2ï¼Œ So step 2 isã€‚Yeah on the pre activation scalingã€‚ What do I mean by pre activationationã€‚

 So this is the valueã€‚That is computed before the activation is computedã€‚

 and it will become more clear in in the next slides where I have step one and step  two connected to each otherã€‚

Soã€‚This is essentially what we have done in the previous slideã€‚ So if I go backã€‚

 this is the end of step 1ã€‚ and then fromã€‚Step 1ï¼Œ we go here to step 2ã€‚Where we apply a scalingã€‚

 So there is a gamma J and a beta Gã€‚And both of these are learnable parameters that are also learned using back propagationã€‚

So similar to the weights in the networkï¼Œ these are also updated via a propagationã€‚

So what do these doï¼Œ I meanï¼Œ essentially there is a scaling parameter hereã€‚

And there is a shift perimeterã€‚Rightï¼Œ so technicallyï¼Œ it canã€‚

Learn to undo what we just did in step 1ï¼Œ rightï¼Ÿ So it essentially canã€‚Undo itsï¼Œ ifã€‚åŸä¹ˆã€‚I Jï¼Œ sorryã€‚

 if that happensã€‚To be equal toï¼Œ let's sayï¼Œ this termã€‚And beta J happens to be equal toã€‚This oneã€‚

 I meanã€‚These are learnables that may happen so the network can actually learnã€‚To undo step 1ã€‚

 the skatingã€‚A sitizationã€‚Whether it does that or notï¼Œ it yeah it really dependsã€‚

 but I'm just saying here the possibility exists that it can undo thisã€‚But in that wayã€‚

What's happening here is it's something that is a little bit more flexible than just regular standardizationã€‚

 So hereï¼Œ step 1 would be a regular standardization of the hidden layer activationsã€‚

 and step 2 is a little bit moreï¼Œ I would sayï¼Œ flexibleï¼Œ soã€‚Technicallyã€‚

 this whole step 1 instead step 2 thing can just simplify to regular standardization if this is given or it could be something differentã€‚

 And in practiceï¼Œ it happens thatã€‚This setupï¼Œ the step 1 and step 2 setup works better than just step 1 aloneã€‚

 Itï¼Œ it just happensã€‚ Soã€‚ and yeahï¼Œ one theoryã€‚Could beã€‚ it's just my theory that I'm coming up withã€‚

 but one simple explanation could beï¼Œ for exampleï¼Œ it might perform better because yeahã€‚

 we just have additional learnable parametersã€‚ It's essentially as if we have moreï¼Œ yeahã€‚

 more parameters in the networkã€‚ So the network has a higher capacity maybeã€‚ but yeahã€‚

 we will look at some other theories in a future videoã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_23.png)

Alrightï¼Œ yeahã€‚ so as I saidï¼Œ so this controls the mean and this controls the spread or scale soã€‚

From this one and this oneï¼Œ basicallyã€‚å—¯ã€‚And like I also already saidï¼Œ technicallyï¼Œ emotionalallyã€‚

 I could learn to perform standardization with0 minute unit variance ifï¼Œ yeah ifã€‚These are the sameã€‚

 andã€‚

![](img/3df80e31d97767fe8d752cb737fe5b1e_25.png)

These are the sameã€‚So here is step1 and step 2ï¼Œ connected and summarizedã€‚So yeahã€‚

 I'm not showing you the numerically stable versionï¼Œ but you probably knowï¼Œ I meanã€‚

 we can also do itã€‚Doesn't really matterã€‚It's just for yeahï¼Œ keeping things simpleã€‚

 So here's the whole procedure of how it would look likeã€‚ here I'm applying it to bothã€‚

This layer and this layerã€‚ So the colors should match hereã€‚ So let's say x is our inputã€‚

 Then we first compute the net inputã€‚ So if we are hereï¼Œ we are computing the net inputã€‚

 and then this would be step oneï¼Œ where we standardizeã€‚The net inputã€‚

Then what we do is we compute this pre activation scalingï¼Œ so this is a scalingã€‚

 and then we compute the activationã€‚So the activation in this case would be computed byã€‚

Sigma or let me write the stone properlyï¼Œ a1ã€‚So be sigmaã€‚A primeã€‚Oneï¼Œ rightï¼Ÿ

And then once we have thatï¼Œ we go to the next inputã€‚ So this is the next net inputã€‚ Then againã€‚

 we do step oneã€‚ So here we canã€‚Just at oneã€‚And then we doã€‚Sep 2ã€‚ğŸ˜”ï¼ŒAnd then we compute againã€‚

activationationã€‚This should be the one hereã€‚Sigmaã€‚è¯¶ã€‚So yeahï¼Œ this is essentially itã€‚ And yeahã€‚

 this is how patchome worksã€‚ do you have anything moreã€‚ Yeahã€‚

 one more thing to consider is that this one also makes the bias redundantï¼Œ rightã€‚

 So I'm not showing the bias unit hereã€‚

![](img/3df80e31d97767fe8d752cb737fe5b1e_27.png)

In this networkï¼Œ but technicallyï¼Œ you will have a biased unitã€‚B1 here here B2ã€‚Hereã€‚ğŸ˜”ã€‚

B3 that gets added to the net inputã€‚Rightï¼Œ so that is something that you are familiar with andã€‚

The if you think about how the bias worksï¼Œ rightï¼Ÿ So if I I have a simple case hereï¼Œ the net inputã€‚

 it'sã€‚m it likeã€‚ğŸ˜”ï¼ŒWï¼Œ just for a simplified caseã€‚So let's say I'm computing the net input for theã€‚

Second layerï¼Œ this would be weights from the first layerã€‚

Multiped by the activation from the first layerã€‚ here layer 1 and layer 1ï¼Œ and then plusã€‚The biasã€‚

 So I'm only considering you a simple case omitting the indexã€‚ Otherwiseï¼Œ I would have another indexã€‚

 But yeahï¼Œ you know what I meanã€‚ So I would have the bias added to it rightã€‚

 But this kind of becomes L redundant because essentially the bias could be already included in this be or the be would essentiallyã€‚

 if you leave outã€‚This bias hereï¼Œ the beta would essentially take the role of that bias if we compute petomeã€‚

 So in that senseï¼Œ you can yes skip the bias when you define the layersã€‚

 I will show you that in the code exampleï¼Œ how we can do thatã€‚ I meanï¼Œ it it doesn't really matterã€‚

 butã€‚Whether we do it or notï¼Œ it should work both ways in practiceï¼Œ but it's justã€‚

 I would say a little bit cleaner to not use the bias because it's redundantã€‚

 I will show you in the next video in the code example how that looks like so you can actually by an argumentã€‚

 say bias equals faultsã€‚Yeahï¼Œ and also note that now when we use batch norm batch norm has learnable parametersã€‚

 So if we use batch norm in a given layer it has we have an additional two vectors that have the same dimension as the biasve right So if we have we use batch norm here in this layer we will have twoã€‚

Forã€‚ğŸ˜”ï¼ŒDimenssional vectorsï¼Œ like this bias vector here would also be four dimensionalï¼Œ rightã€‚

 because there's one bias for eachã€‚Hden layer activationã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_29.png)

Alrightï¼Œ so I thinkï¼Œ yeahï¼Œ this is all I have about batchomeã€‚

 This is how batchome works In my slides I actually had also not sureã€‚

 it was actually like 10 slides or so how we do back propagation with batchome but I promised you not to torture you with these nitty gritty details and mathã€‚

 the medical details because that's not super important because we use autograd and practice anyway I think it's in this course also more important to understand the bigger picture concepts And then if you're interested you can study those things later on So yeah also if we would start like going through these slides it would take another half an hour I would rather complete this lecture today so that next lecture we can talk about optimizes and move on to convolution networksã€‚

 I hope you don't mindã€‚ So in that way next video I will show you batchome in Pytorchã€‚



![](img/3df80e31d97767fe8d752cb737fe5b1e_31.png)