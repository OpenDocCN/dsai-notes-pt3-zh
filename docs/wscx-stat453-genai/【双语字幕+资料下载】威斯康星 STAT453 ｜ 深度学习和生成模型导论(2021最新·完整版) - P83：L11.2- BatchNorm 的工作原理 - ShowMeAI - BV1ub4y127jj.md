# P83ÔºöL11.2- BatchNorm ÁöÑÂ∑•‰ΩúÂéüÁêÜ - ShowMeAI - BV1ub4y127jj

OkayÔºå let's now talk about extending the input normalization to the hidden layers using the technique called batch normalization„ÄÇ

 I will just show you in this video how it works„ÄÇ and then in the next video I will show you how we can use it in Pytorarch„ÄÇ

 and then I will show you or explain to you why it works„ÄÇ

At least we will discuss some of the theories trying to explain how it works„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_1.png)

AlrightÔºå so batch normalization or in short batch norm goes back to a paper published in 2015 called batch normalization accelerating deep network training by reducing internal covariate shift So internal covariator shift that's like yeah I don't know fancy word for just saying that the feature distributions shift during the course of the training internally in the hidden layerss we will explain or think about this more when I go to the video where I offer some theories why batch normalization works well in practice„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_3.png)

So here on just the I would sayÔºå the short version of batch normalizationÔºå what it is„ÄÇ

 it's about normalizing the hidden layer inputs instead of just normalizing the inputs to the network„ÄÇ

 we normalize yeah also internally the inputs to each hidden layer and this helps yeah with exploding and vanishing gradient problems so that the gradient sound become too large or too small„ÄÇ

And overallÔºå it can increase the stability„ÄÇDuring training„ÄÇ So in that way„ÄÇ

 training progresses more smoothlyÔºå and alsoÔºå the convergence rate improves„ÄÇ

 That means it may be that we need fewer epochs toÔºå yeah„ÄÇ

 get the same loss that we would achieve if we don't use batch„ÄÇ So usually with batchia„ÄÇ

 the networks train faster„ÄÇAnd how you can think of it is as an additional normalization layer„ÄÇ

 And there are also some additional trainable parameters involved„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_5.png)

YeahÔºå so in the next couple of slidesÔºå I'm going to walk you through the procedure of batch normalization„ÄÇ

 So just for context on let's suppose we have a multi layer perceptron„ÄÇ

 So let's say we focus in on this activation here and the second hidden layer„ÄÇ

 the first activation in the second hidden layer„ÄÇAnd we are actually looking at the net input of that activation„ÄÇ

 So remember how we compute the activation„ÄÇ So it's usually by applying„ÄÇ

Activation function to the net input„ÄÇWhat type of activation function doesn't really matter here„ÄÇ

 it could be logistic sigmoidÔºå 10 H or Relu should work with any type of activation function„ÄÇAlright„ÄÇ

 so yeahÔºå we are focusing on this hidden layer net input corresponding to the activation here now and also now suppose we have a mini batch„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_7.png)

So we have a mini batch such that the net input of a given training example at layer 2 is written as followss„ÄÇ

 so I have now this index I here for the index of the training example in that mini batch so if I have I equals one this would be the first example in that mini batch and just of the first simplicity I will actually ignore the layer index in the next couple of slides just so that the other notation is a bit simpler to read„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_9.png)

So yeahÔºå here that's the first step of patronome„ÄÇ There are two steps„ÄÇ

 So the first step is to normalize the net inputs„ÄÇ This is essentially the same as the standardization that I explained earlier for yeah„ÄÇ

 the inputs„ÄÇ So what's going on here is we are computing the mean„ÄÇ

Over a feature So the J is the feature index again„ÄÇSo„ÄÇ

You can actually use batchome for any type of input„ÄÇ

 So we will also see there is a two dimensional version for that for convolutional networks later on„ÄÇ

 But here we are just focusing on the regular one dimensional version where you have a feature vector„ÄÇ

 for instance„ÄÇ So let's say we have„ÄÇYeahÔºå the J feature„ÄÇ And if I go back„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_11.png)

So if you consider this activation hereÔºå what are the features„ÄÇ

 So the features are all essentially all the previous„ÄÇLayer activationsÔºå rightÔºå So all these„ÄÇGo into„ÄÇ

That activation„ÄÇ So all of these here are the features„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_13.png)

off„ÄÇThis activation here„ÄÇSo J really is the index over the activations from the previous layers„ÄÇ

 or we can also think of it as the feature index or previous„ÄÇWhich are the previous„ÄÇLayers„ÄÇ

Mctivations„ÄÇYeahÔºå so we compute from that the mean„ÄÇAnd„ÄÇüòîÔºåThe variance„ÄÇSo that isÔºå yeah„ÄÇ

 nothing that should be new to you„ÄÇ It's simpleÔºå yeahÔºå simple calculation here„ÄÇ

 And then we standardize„ÄÇ So this is like what we had in a previous video as standardization„ÄÇ

And this is essentially it's in that wayÔºå step one of batchome is similar to the input standardization„ÄÇ

Except now we are looking at a hidden layer„ÄÇ So instead of looking„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_15.png)

Instead of looking at standardizing XÔºå we are now standardizing these A's from the previous layer„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_17.png)

So after step one comes step2Ôºå so one more thing to say about step1„ÄÇ

 so in practice we modify this standardization slightly such that we don't bump into yeah division zero issues if the variance is zero if there's no variance„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_19.png)

So for numerical stabilityÔºå we add a small value epsilon here„ÄÇ

 So here we have the variance plus this little epsilon„ÄÇ

 And then we take the square root instead of dividing by the the directly„ÄÇ

 That's just like a small computational trick to prevent division by0  errorss„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_21.png)

NowÔºå step 2Ôºå So step 2 is„ÄÇYeah on the pre activation scaling„ÄÇ What do I mean by pre activationation„ÄÇ

 So this is the value„ÄÇThat is computed before the activation is computed„ÄÇ

 and it will become more clear in in the next slides where I have step one and step  two connected to each other„ÄÇ

So„ÄÇThis is essentially what we have done in the previous slide„ÄÇ So if I go back„ÄÇ

 this is the end of step 1„ÄÇ and then from„ÄÇStep 1Ôºå we go here to step 2„ÄÇWhere we apply a scaling„ÄÇ

 So there is a gamma J and a beta G„ÄÇAnd both of these are learnable parameters that are also learned using back propagation„ÄÇ

So similar to the weights in the networkÔºå these are also updated via a propagation„ÄÇ

So what do these doÔºå I meanÔºå essentially there is a scaling parameter here„ÄÇ

And there is a shift perimeter„ÄÇRightÔºå so technicallyÔºå it can„ÄÇ

Learn to undo what we just did in step 1Ôºå rightÔºü So it essentially can„ÄÇUndo itsÔºå if„ÄÇÂéü‰πà„ÄÇI JÔºå sorry„ÄÇ

 if that happens„ÄÇTo be equal toÔºå let's sayÔºå this term„ÄÇAnd beta J happens to be equal to„ÄÇThis one„ÄÇ

 I mean„ÄÇThese are learnables that may happen so the network can actually learn„ÄÇTo undo step 1„ÄÇ

 the skating„ÄÇA sitization„ÄÇWhether it does that or notÔºå it yeah it really depends„ÄÇ

 but I'm just saying here the possibility exists that it can undo this„ÄÇBut in that way„ÄÇ

What's happening here is it's something that is a little bit more flexible than just regular standardization„ÄÇ

 So hereÔºå step 1 would be a regular standardization of the hidden layer activations„ÄÇ

 and step 2 is a little bit moreÔºå I would sayÔºå flexibleÔºå so„ÄÇTechnically„ÄÇ

 this whole step 1 instead step 2 thing can just simplify to regular standardization if this is given or it could be something different„ÄÇ

 And in practiceÔºå it happens that„ÄÇThis setupÔºå the step 1 and step 2 setup works better than just step 1 alone„ÄÇ

 ItÔºå it just happens„ÄÇ So„ÄÇ and yeahÔºå one theory„ÄÇCould be„ÄÇ it's just my theory that I'm coming up with„ÄÇ

 but one simple explanation could beÔºå for exampleÔºå it might perform better because yeah„ÄÇ

 we just have additional learnable parameters„ÄÇ It's essentially as if we have moreÔºå yeah„ÄÇ

 more parameters in the network„ÄÇ So the network has a higher capacity maybe„ÄÇ but yeah„ÄÇ

 we will look at some other theories in a future video„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_23.png)

AlrightÔºå yeah„ÄÇ so as I saidÔºå so this controls the mean and this controls the spread or scale so„ÄÇ

From this one and this oneÔºå basically„ÄÇÂóØ„ÄÇAnd like I also already saidÔºå technicallyÔºå emotionalally„ÄÇ

 I could learn to perform standardization with0 minute unit variance ifÔºå yeah if„ÄÇThese are the same„ÄÇ

 and„ÄÇ

![](img/3df80e31d97767fe8d752cb737fe5b1e_25.png)

These are the same„ÄÇSo here is step1 and step 2Ôºå connected and summarized„ÄÇSo yeah„ÄÇ

 I'm not showing you the numerically stable versionÔºå but you probably knowÔºå I mean„ÄÇ

 we can also do it„ÄÇDoesn't really matter„ÄÇIt's just for yeahÔºå keeping things simple„ÄÇ

 So here's the whole procedure of how it would look like„ÄÇ here I'm applying it to both„ÄÇ

This layer and this layer„ÄÇ So the colors should match here„ÄÇ So let's say x is our input„ÄÇ

 Then we first compute the net input„ÄÇ So if we are hereÔºå we are computing the net input„ÄÇ

 and then this would be step oneÔºå where we standardize„ÄÇThe net input„ÄÇ

Then what we do is we compute this pre activation scalingÔºå so this is a scaling„ÄÇ

 and then we compute the activation„ÄÇSo the activation in this case would be computed by„ÄÇ

Sigma or let me write the stone properlyÔºå a1„ÄÇSo be sigma„ÄÇA prime„ÄÇOneÔºå rightÔºü

And then once we have thatÔºå we go to the next input„ÄÇ So this is the next net input„ÄÇ Then again„ÄÇ

 we do step one„ÄÇ So here we can„ÄÇJust at one„ÄÇAnd then we do„ÄÇSep 2„ÄÇüòîÔºåAnd then we compute again„ÄÇ

activationation„ÄÇThis should be the one here„ÄÇSigma„ÄÇËØ∂„ÄÇSo yeahÔºå this is essentially it„ÄÇ And yeah„ÄÇ

 this is how patchome works„ÄÇ do you have anything more„ÄÇ Yeah„ÄÇ

 one more thing to consider is that this one also makes the bias redundantÔºå right„ÄÇ

 So I'm not showing the bias unit here„ÄÇ

![](img/3df80e31d97767fe8d752cb737fe5b1e_27.png)

In this networkÔºå but technicallyÔºå you will have a biased unit„ÄÇB1 here here B2„ÄÇHere„ÄÇüòî„ÄÇ

B3 that gets added to the net input„ÄÇRightÔºå so that is something that you are familiar with and„ÄÇ

The if you think about how the bias worksÔºå rightÔºü So if I I have a simple case hereÔºå the net input„ÄÇ

 it's„ÄÇm it like„ÄÇüòîÔºåWÔºå just for a simplified case„ÄÇSo let's say I'm computing the net input for the„ÄÇ

Second layerÔºå this would be weights from the first layer„ÄÇ

Multiped by the activation from the first layer„ÄÇ here layer 1 and layer 1Ôºå and then plus„ÄÇThe bias„ÄÇ

 So I'm only considering you a simple case omitting the index„ÄÇ OtherwiseÔºå I would have another index„ÄÇ

 But yeahÔºå you know what I mean„ÄÇ So I would have the bias added to it right„ÄÇ

 But this kind of becomes L redundant because essentially the bias could be already included in this be or the be would essentially„ÄÇ

 if you leave out„ÄÇThis bias hereÔºå the beta would essentially take the role of that bias if we compute petome„ÄÇ

 So in that senseÔºå you can yes skip the bias when you define the layers„ÄÇ

 I will show you that in the code exampleÔºå how we can do that„ÄÇ I meanÔºå it it doesn't really matter„ÄÇ

 but„ÄÇWhether we do it or notÔºå it should work both ways in practiceÔºå but it's just„ÄÇ

 I would say a little bit cleaner to not use the bias because it's redundant„ÄÇ

 I will show you in the next video in the code example how that looks like so you can actually by an argument„ÄÇ

 say bias equals faults„ÄÇYeahÔºå and also note that now when we use batch norm batch norm has learnable parameters„ÄÇ

 So if we use batch norm in a given layer it has we have an additional two vectors that have the same dimension as the biasve right So if we have we use batch norm here in this layer we will have two„ÄÇ

For„ÄÇüòîÔºåDimenssional vectorsÔºå like this bias vector here would also be four dimensionalÔºå right„ÄÇ

 because there's one bias for each„ÄÇHden layer activation„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_29.png)

AlrightÔºå so I thinkÔºå yeahÔºå this is all I have about batchome„ÄÇ

 This is how batchome works In my slides I actually had also not sure„ÄÇ

 it was actually like 10 slides or so how we do back propagation with batchome but I promised you not to torture you with these nitty gritty details and math„ÄÇ

 the medical details because that's not super important because we use autograd and practice anyway I think it's in this course also more important to understand the bigger picture concepts And then if you're interested you can study those things later on So yeah also if we would start like going through these slides it would take another half an hour I would rather complete this lecture today so that next lecture we can talk about optimizes and move on to convolution networks„ÄÇ

 I hope you don't mind„ÄÇ So in that way next video I will show you batchome in Pytorch„ÄÇ



![](img/3df80e31d97767fe8d752cb737fe5b1e_31.png)