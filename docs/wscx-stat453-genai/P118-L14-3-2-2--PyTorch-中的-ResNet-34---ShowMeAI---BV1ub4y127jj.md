# P118ï¼šL14.3.2.2- PyTorch ä¸­çš„ ResNet-34 - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let me know show you how we can implement residual networks in Pytorchã€‚

 So I will show you two notebooksã€‚ First is a naive implementation I have made myselfã€‚

 And then I will show you a more sophisticated implementation of Resnet 34 from the Pytorch communityã€‚

 So in this first notebook now I will show you what we talked about in the last video these two different types of yeah residual blocks and then in the next notebook I will show you this residual network with 34 layersã€‚

 So yeahï¼Œ I'm not going to rerun this notebookï¼Œ it didn't take too longã€‚

 but yeah why waiting if if it's not necessaryã€‚ So I will show you just other results soã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_1.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_2.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_3.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_4.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_5.png)

Yeahï¼Œ hereï¼Œ it's just the boiler plate importing torch and nuyã€‚ the usual stuff for the data setã€‚

 So I'm not using the helper functions here because yeahï¼Œ it's really just very simpleã€‚ So Iã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_7.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_8.png)

Didn't really focus on abstracting thingsã€‚ I just coded the residual blocksã€‚

 and this is a self contained notebook in that wayã€‚

 So here I'm using the Mnes data just for simplicityã€‚

 because just wanted to have a dataset doesn't really matter which oneã€‚

 because this is not going to be a good conversion network It's just like more of a proof of concept how the residual block worksã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_10.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_11.png)

So the data set really doesn't matter that much hereã€‚

So here I'm implementing now this yeah residual blockã€‚

 the one where the input has the same dimension as hereï¼Œ theã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_13.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_14.png)

Output from the residual partã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_16.png)

So how does it look likeã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_18.png)

that one soã€‚I implemented it using the torch module class just a regular confifin that I'm implementing hereã€‚

 I have a confin with two residual blocksï¼Œ and each of those is one residual blockã€‚

 So you can see that this convolution here represents this oneã€‚

 So here I' amm starting with one channel for output channelsã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_20.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_21.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_22.png)

Thenï¼Œ I have bechnomã€‚Then I have Reouã€‚ By the wayï¼Œ I haven't really explained what that1ï¼Œ1 meansã€‚

 I think I have used that before in some other codeã€‚ So in place equals trueã€‚ This just means thatã€‚

Pyto doesn't have to make a copy of that array internallyã€‚ So we could do something like that I meanã€‚

 not here insideï¼Œ but in generalï¼Œ we could do something likeã€‚Like thisã€‚Justã€‚

 let's write it like thisã€‚ So this will create a newã€‚A new tensor Xã€‚ and then overr this tensor Xã€‚

 So it's essentially over writing this tensor Xã€‚ But for a brief moment in timeã€‚

 when this gets executedï¼Œ there are two arraysã€‚ If this is an existing oneï¼Œ if I have someã€‚

Previous computation hereã€‚ So this previous computation created xã€‚ And then when I'm calling thisã€‚

 it willã€‚Take an X and create a new version while X is still in memoryã€‚

 So for a brief moment in timeï¼Œ I have two arrays in memoryã€‚ it's not a big deal at allã€‚

 but you have toï¼Œ I meanï¼Œ under the hoodï¼Œ allocate memory in the GPU and stuff like thatã€‚

 So it's kind of a little bit more expensive to do that compared to doing an in place operation and in place operation is essentially modifying something in place without creating a new arrayã€‚

 So it's slightly more efficientã€‚ It's not always possible to do thatã€‚ But yeahï¼Œ if you can do thatã€‚

 it's actually niceã€‚ So its essentially the difference betweenï¼Œ let's sayã€‚

 writing this and x plus equals one in that senseï¼Œ you are directly modifying something whereas here you are having creating a copy and then assigning the copy to itã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_24.png)

Anywaysï¼Œ it's a little tangentã€‚ The results are exactly the sameã€‚ whether you do this or thisã€‚

 And in practiceï¼Œ you probably won't notice any difference anywayã€‚ So it does not really matterã€‚

 But yeahï¼Œ why not doing itï¼Ÿ Allrightï¼Œ so small tangent So we have convolution batch norm and then this reã€‚

 This is this part is really the first three and then we have another convolution and a batch normã€‚

 which is this partã€‚ noticeice that I'm going from one to4ï¼Œ and then back from4 to1ã€‚

 It seems kind of weird why am I doing that Yeahï¼Œ it's really like to match the dimensionsã€‚

 Otherwiseï¼Œ I will have more channels here then I have as an input So I would have four channels hereã€‚

 and then one input channel and it doesn't really work if we add them because then it's not an identity anymore okayã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_26.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_27.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_28.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_29.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_30.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_31.png)

Yeahï¼Œ and we also have one fully connected layerã€‚ This this just to make this classifierã€‚So yesã€‚

 and how I'm implementing thisã€‚ So here I was justã€‚Defining or initializing these layersï¼Œ the blocksã€‚

 And here I'm calling themã€‚ So in the forward pass is really where things happenã€‚ So Iã€‚

Save X as the shortcut hereã€‚So I'm saving this hereã€‚Then Im calling my blockã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_33.png)

So this part I highlighted hereï¼Œ this is really calling the whole blockï¼Œ rightã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_35.png)

It's this whole blockã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_37.png)

And actuallyï¼Œ I could haveã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_39.png)

These are kind of redundant I could have used the sameã€‚ Okayï¼Œ but thenï¼Œ of courseã€‚

 the weights are differentã€‚ Anywayï¼Œ sorryï¼Œ so I call my block hereã€‚

 and then I have my re function and the re function is applied to x plus the shortcutã€‚

 So this part really isã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_41.png)

This partã€‚ So I'mï¼Œ I'm adding insideï¼Œ and then I'm applying the re So the reã€‚

 that is what is shown hereã€‚ and here I have this additionã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_43.png)

Rightï¼Œ so this is essentially one residual blockã€‚ And then I'm repeating itã€‚

 So why am I not using one hereï¼Œ Wellï¼Œ then it would be the same layer It's that wouldn't work reallyã€‚

Okayã€‚But the the shape is the sameã€‚ It's justï¼Œ we have different weightsï¼Œ rightã€‚

 So it's just like having two convolutional layers after each otherã€‚

And then we have this linear hereï¼Œ which is turning this into a classifierã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_45.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_46.png)

Alrightï¼Œ soã€‚So the linear Le has output the nu classesã€‚ and here I am justã€‚Flatening itã€‚

 So I'm assuming what comes out ofã€‚å—¯ã€‚This block 2 has a dimensionity 28 times 28 784ã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_48.png)

Yeahï¼Œ and then I'm running this just pasting my convenience functions that I usually have in my helper functionã€‚

 it's a slightly simpler versionca I'm not plotting anything I just want to show you that this actually runsã€‚

 doesn't get great performance becauseï¼Œ of courseï¼Œ it's a very naive implementationã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_50.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_51.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_52.png)

There's also only the training accuracyï¼Œ the test accuracy is 92%ã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_54.png)

So what I show what I mean is how do I know that this is the actual numberã€‚ I meanã€‚

 I can think about itã€‚ I can look at thisã€‚ but like I explained in the previous videoã€‚

 what I can also do isã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_56.png)

I meanï¼Œ what most people do is just print X sizeã€‚ then you can oopsã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_58.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_59.png)

Let run everything hereã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_61.png)

Thenã€‚Can run the trainingã€‚ And then you will see the sizeã€‚

 Of course you don't want to complete it because it's annoying to have it hereã€‚ So I just stopped itã€‚

 I can seeï¼Œ ohï¼Œ it's 1ï¼Œ282 times 28ã€‚ And that is what I can then copy and paste and thenã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_63.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_64.png)

And go here and put it in hereã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_66.png)

Rightï¼Œ so that'sã€‚Where this number comes fromã€‚ And it is also where this number comes fromã€‚Okayã€‚

In practiceï¼Œ if you don't want to think about it too hard and you are debugging thingsã€‚ I meanã€‚

 it doesn't hurt toã€‚ And so the print statementï¼Œ it's what everyone is doingã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_68.png)

Okayã€‚So we trained thatã€‚ Noï¼Œ of courseï¼Œ I interrupted itï¼Œ butã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_70.png)

Suppose it trainedï¼Œ I meanï¼Œ it trained beforeã€‚ so if I fixed it it would trainã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_72.png)

Nowï¼Œ the second partï¼Œ now focusing on the more interesting part where we have this resizing hereã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_74.png)

So I am implementing this a bit differently nowï¼Œ using a reusable unitã€‚ I call that a residual blockã€‚

 So I am implementing my residual block hereï¼Œ and this one is implemented the sameã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_76.png)

Wï¼Œ now it's a little bit more generalã€‚ I have something called channels hereã€‚

 This is the input channelsï¼Œ or let's say the first number of channelsï¼Œ the output channelsã€‚

And then here I have1 and2ã€‚ So I am going from 0 to 1 to 2ã€‚Andã€‚I canï¼Œ I meanã€‚

 I'm not defining what these numbers areã€‚ I'm defining them later but I'm calling thisã€‚

 so I can maybe briefly skip aheadã€‚ So I'm using this residual blockã€‚

 actually in my convolutional network hereã€‚ So I'm using it hereã€‚ And here I'm defining the channelsã€‚

 I'm going from 1 to 4 to 8ã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_78.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_79.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_80.png)

Soã€‚Yeahï¼Œ so that's what I'm doing hereã€‚ So I'm going from0ã€‚Also 1 to 4ï¼Œ sorryï¼Œ1ï¼Œ2 4 to 8ã€‚

And then I have my shortcutï¼Œ which goes also from 0ã€‚ sorryï¼Œ from 1 to 8ã€‚ Otherwiseã€‚

 I wouldn't be able to edit because ifï¼Œ let's say this is one channelã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_82.png)

Outcomesã€‚8 channelsã€‚ Then this also has to be 8 channelsã€‚ otherwise I can't editã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_84.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_85.png)

So that's what's going on thereã€‚Soã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_87.png)

My residual block is scrolling up again so we can see everythingã€‚ So my residual block 1ï¼Œ2ï¼Œ3ã€‚

 This is really this partï¼Œ these three first blocksã€‚

 and then like before these second blocks are this and this Now the difference isã€‚

 yeah that I have different numbers of channels and I can also reduce the sizeã€‚Rightã€‚

 so here I have a stride of twoã€‚ so that will reduce the sizeã€‚ I have to do a stride of two hereã€‚

 too to match these dimensionsã€‚ So here I have to be a little bit more careful that the dimensions match alsoã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_89.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_90.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_91.png)

Yeahï¼Œ and then as before I have my block I have a shortcut and both the block plus shortcutã€‚

 they go into my re functionã€‚ So this is what I'm showing you hereã€‚

 this residual block is really this whole thing hereã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_93.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_94.png)

Yeah and then I'm using my residual blockï¼Œ I'm initializing one residual block and another so I have a network with two residual blocksã€‚

 the first one goes from 1 to 8 and the second one from 8 to 32 and yeah the number of the sizes here is seven times7 times 32 so it's because we are also havinglving the dimensions here half and half approximatelyã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_96.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_97.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_98.png)

So going from 28 times 28 to 14 times 14 and from 14 times 14 to 7 times 7ã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_100.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_101.png)

Yeahï¼Œ this is essentially itã€‚So that's how we implement thisã€‚ So yeahã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_103.png)

Then we are training itã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_105.png)

Tins hereã€‚ and yeah it performs much better than our previous implementationã€‚ But againã€‚

 the goal of residual networks is really to go deep in the network in terms of the number of layersã€‚

 So here we only have two layersã€‚ Soï¼Œ I meanï¼Œ this is probably not a great network to use for other datasets here we are just using Mnã€‚

 So if we want to use theï¼Œ I would say sophisticated dataã€‚

 I'm actually only using C 10 because it's simple to a lotã€‚ But if you want to use a different dataã€‚

 Resnet 34 is a good choiceã€‚ So this is the oneï¼Œ the the deep one hereã€‚

 it performs pretty well going back hereï¼Œ it gets actually pretty good performance onã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_107.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_108.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_109.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_110.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_111.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_112.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_113.png)

An imagenet of one accuracy better than the Gï¼Œ for exampleã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_115.png)

And how does that workï¼Œ So it's the same conceptã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_117.png)

Thanksï¼Œ Seanï¼Œ sorryï¼ŒShown hereï¼Œ except moreï¼Œ I would sayï¼Œ more sophisticated implementation of thatã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_119.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_120.png)

So I could have implemented it by handï¼Œ but there's always the chance to make mistakes at some pointã€‚

 So why not using what's already implementedã€‚ So here this isï¼Œ againï¼Œ I'm using my helper functionsã€‚

 that's again the same that I explained for VG G16ã€‚ So everything is the same as for VG G16ã€‚

 So I don't have to discuss everything againã€‚ the only new part here is really this partï¼Œ the modelã€‚

 So here I actually copied the code from this websiteï¼Œ which is an implementationã€‚ yepã€‚

 the official Pyr implementation which has different versions of resnet wide resnetã€‚

 regular ressonnetï¼Œ18 layersï¼Œ34 layersï¼Œ100 layersï¼Œ1 or 52 layers and so forthã€‚ I grabbedã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_122.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_123.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_124.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_125.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_126.png)

The code that is used to yeah initialize all of these networksã€‚

 So they have written some code that can be reused for different types of residual networksã€‚

 So here was copying it and simplifying it a little bitã€‚ So it's not that longã€‚

 And then they have something they call the bottleneckã€‚

 It's kind of similar to what I call the residual blockã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_128.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_129.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_130.png)

But we have hereã€‚ and then I meanï¼Œ it's relatively complicatedã€‚

 I have to admit it would take me also a couple of us to really understand how that is implementedã€‚

 The most important thing is that it worksã€‚ Many people are using it So I'm kind of trusting that this is indeed workingã€‚

 So they have like a make layer method here or function here that creates these layersã€‚

 It's a little bit more sophisticated than than my versionã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_132.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_133.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_134.png)

So and then in the forward methodï¼Œ you have these different layersã€‚

 So each layer has also multiple convolutional layersã€‚ That's how you get the number 34ã€‚ And yeahã€‚

 we can also use the torch flatten function hereã€‚ That's actually something I should also maybe use more oftenã€‚

 It's a more recent thingã€‚ So I could actually technically replaceã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_136.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_137.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_138.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_139.png)

Replace that one by flattenã€‚So hereï¼Œ that could be replaced by flattenã€‚

 but I still need to know this number anywaysï¼Œ because I have to put it here for the number of parametersã€‚

 So even though we can put a torch flatten hereï¼Œ it'sï¼Œ it's not that much simplerã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_141.png)

Yeahã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_143.png)

Soã€‚Yepï¼Œ that is essentially itã€‚ So here I would have to know still this number in this millionaire the 512ã€‚

 So I could technically also write this as torchã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_145.png)

Not viewã€‚-1ï¼Œ512ã€‚ I think blocks expansion here is oneã€‚

 This is only used for the other types of networksï¼Œ other residual net soã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_147.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_148.png)

Could technically also write it like thisã€‚ But yeahï¼Œ we have this nicer flattenã€‚

 thing what's nice about flatten is everyone knows what flatten that it has a meaning that is more intuitiveã€‚

 maybe than saying view-1 or somethingã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_150.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_151.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_152.png)

Oã€‚ðŸ˜”ã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_154.png)

Yeahï¼Œ and here it's also the same code that I used for the V Gã€‚ And now it's trainingã€‚ Actuallyã€‚

 we are using Cypher 10 hereã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_156.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_157.png)

Let me open this one againã€‚So here I have 70 by 70 imagesã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_159.png)

Scraning upï¼Œ sorryã€‚ Yeahï¼Œ I have made it larger because otherwise the performance was very poorã€‚

 I meanï¼Œ all these types of networks are really implemented for bigger data setsï¼Œ not Cypha 10ã€‚

 I'm just using Cypha 10 because then we don't have to download a separate dataset if you want to reproduce these resultsã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_161.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_162.png)

And I showed you how you can use your own data set tooï¼Œ So in the wayã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_164.png)

Shouldn't be an issue for youï¼Œ but if you have questionsã€‚

 you can always ask I'm happy to help with thatã€‚So here with Resnetï¼Œ we get approximately 48%ã€‚

 which is not much better than what we got with midji G16 here it's alsoã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_166.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_167.png)

Kind of the sameã€‚ But noticeï¼Œ even though I use large images hereï¼Œ it was at leastã€‚

 at least faster to run 62 minutes versus 90 minutesã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_169.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_170.png)

Okayï¼Œ as if Iï¼Œ if I would have made the images smaller here the same sizeã€‚

 it would have probably finished in like 30 or 40 minutesã€‚ Also overfittingã€‚

 So here might be a case for adding more dropoutã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_172.png)

So here we only haveã€‚Do we have actually dropletsã€‚

![](img/6fc0b4c4fba26a77a100c96c79d18809_174.png)

Notï¼Œ not reallyã€‚ we only have petonï¼Œ so maybe could be added to could be adding dropã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_176.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_177.png)

Okayï¼Œ so some the results get spurret and frog wrongã€‚ What was the one that this one got wrongã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_179.png)

Deer and fck interestingã€‚ So yeahï¼Œ animal classes are still confusingã€‚ You can also pro yeahã€‚

 you can see againï¼Œ the square where it makes misclassifications between different animalsã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_181.png)

Againï¼Œ the tech cats and dogsã€‚ And yeahï¼Œ this is resnet implemented hereã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_183.png)

Honestlyï¼Œ if you implement networksï¼Œ you don't have to implement things from scratch unless it's for educational purposes like for learning thingsã€‚

 usually when you find a paper or read a paper with an interesting implementation thing you want to try usually what people do is they would go on Gitthub and search for the original authors providing the code for that paper and then adopting this codeã€‚

 so you would technically not run it one to oneï¼Œ you have to probably make some modifications so that it works for your dataã€‚

 but usually in practice once we are working with these more complicated datasã€‚

 theres no it doesn't make sense to implement this Re 34ã€‚

 let's say completely from scratch it's only another source of making errorsã€‚

 I mean it's useful here as a thought exercise to do it with a simple case with two layersã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_185.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_186.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_187.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_188.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_189.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_190.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_191.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_192.png)

Where you haveã€‚Simple implementation with two layersï¼Œ yet maybe makes sense to do thatã€‚

 But if you go deeper Renet 34ï¼Œ maybe use something that isï¼Œ yeahï¼Œ someone has implementedã€‚

 saves you lots of time and pain in that wayã€‚ Alrightï¼Œ so okayï¼Œ this is resnetã€‚

 I think we are already at the 75 minutesã€‚ So we will continue next week withã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_194.png)

Yeahï¼Œ the all convolutional networkã€‚ I already implemented this somewhere hereã€‚

 and then we will also talk about transfer learningã€‚ I have to still implement it anywayã€‚

 No I have it here already soã€‚We will talk about transfer learning also next weekï¼Œ All rightã€‚



![](img/6fc0b4c4fba26a77a100c96c79d18809_196.png)