# P94ÔºöL12.4- AdamÔºöÁªìÂêàËá™ÈÄÇÂ∫îÂ≠¶‰π†ÁéáÂíåÂä®Èáè - ShowMeAI - BV1ub4y127jj

YesÔºå so in the previous video we talked about adding this momentum term„ÄÇ

 which is essentially a velocity term that helps dampening the oscillations in stochastic gradient descent„ÄÇ

 but it also can help with overcoming like flat regions on the loss surfaceÔºå for instance„ÄÇ

 saddle points or local minima„ÄÇSo in addition to this momentum term we learn now about a slightly related also slightly different concept called adaptive learning rates„ÄÇ

 so adaptive learning rates are essentially about accelerating and deaccerating the learning rate at the right moment„ÄÇ

 so speeding the learning up when we are going into the right direction and slowing it down when we change directions„ÄÇ

And then we will see how we can combine this with momentum learning and the combination of both there's an algorithm that does that it's called Adam„ÄÇ

 so Adam is essentially a combination of adaptive learning and momentum learning„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_1.png)

Yeah there are many different flavors of adaptive learning rates out there and discussing them all would be a little bit of out of the scope of this course„ÄÇ

 however yeah just to go over the main concepts I will show you a simple example and yeah the key takeaway like I said in the previous slide is that we decrease the learning rate if the gradient changes the direction and we increase the learning rate if the gradient stays consistent„ÄÇ

SoÔºå for instanceÔºå if we„ÄÇDo these updates„ÄÇ and they are all going to the sameÔºå roughly same direction„ÄÇ

 Then we accelerate becauseÔºå yeahÔºå in this wayÔºå it's probably likely the correct direction„ÄÇ

 So in that wayÔºå we can just speed it up and converge faster„ÄÇHowever„ÄÇ

 if we have something where we change the directionÔºå for example„ÄÇ

 So let's say we have an update like that and there's another change„ÄÇ

 then it will slow down the update so that it isÔºå if it's a wrong direction„ÄÇ

 there's maybe some noise so that it is not heading too much into this wrong directionÔºå for example„ÄÇ

 So if every time there's a change in directionÔºå it will essentially slow down„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_3.png)

YeahÔºå so how does it workÔºå How can we use an adaptive learning rate There are essentially two main steps for doing that„ÄÇ

 so step one would be initializing a so called local gain for each weight and if you look at this„ÄÇ

HereÔºå when we compute the Dlta WI J term and„ÄÇIf you onlyÔºå if you„ÄÇRemove this one„ÄÇ

 This would essentially look like the regular„ÄÇDelta term that we compute„ÄÇ

 So this is essentially our gradient„ÄÇOf the loss with respect to that weight that we want to update times the learning rate„ÄÇ

 So this is the learning rate„ÄÇBut the difference now is that we add this gain and the ske„ÄÇ

As you can see it's also index IJÔºå so it's the gain associated with that particular weight and in that way you can think of this one as a learning rate just for this particular weight so you can have unique learning rates for different weights or different directions in that way„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_5.png)

So now after initializing this local gain„ÄÇWe have a second step„ÄÇ

 so this is what we then do during learning„ÄÇ we are modifying the gain„ÄÇ

 so similar to modifying the weightsÔºå we modify the scan„ÄÇ

 so we either increase it if it's consistent or otherwise if it's inconsistent so if we change the direction„ÄÇ

 then we decrease or yeah we yeah we change the dampen the gain essentially„ÄÇSo like I'm saying here„ÄÇ

 multiplying by a factor has a larger impact if the gains are large compared to adding a term addition to in contrast to this edition„ÄÇ

And„ÄÇThat means„ÄÇ so how this is set upÔºå it means that if we are going into the right direction„ÄÇ

 we are slowly„ÄÇAccelerating„ÄÇ SoÔºå so this is if we are consistent and we are adding this beta term„ÄÇ

 and we make it a„ÄÇBigger by a small value here by betaÔºå let's say beta is 0„ÄÇ9„ÄÇ We increase by 0„ÄÇ9„ÄÇ

In each„ÄÇRoundÔºå essentially„ÄÇBut if we change the directionÔºå we slow down faster by multiplying„ÄÇ

By this term hereÔºå for instance„ÄÇ So hereÔºå for instanceÔºå we will multiply a beta is „ÄÇ9„ÄÇ

 We will multiply this by 0„ÄÇ1„ÄÇ And this can have a yes a stronger dampening effect when we change the directions„ÄÇ

 This is kind of intuitiveÔºå becauseÔºå yeah„ÄÇIf weÔºå let's say drive a car and we see the road is clear ahead„ÄÇ

 we start to accelerateÔºå but we don't go crazy and stomp on the gas„ÄÇ

 we start accelerating in a reasonable manner„ÄÇAnd let's say we want to make a turn or something like that„ÄÇ

 or we are in the turnÔºå usually we accelerate a lot before a turn„ÄÇ

 we don't slowly accelerate in most casesÔºå at least I meanÔºå if it's a very sudden turn„ÄÇ

 if we have to avoid an obstacle or something like that„ÄÇ

 we have to push the brakes pretty hard to turn left or right to avoid the obstacle„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_7.png)

AlrightÔºå so there is a particular implementation of an adaptive learning rate that was very popular for yeah„ÄÇ

 for a time„ÄÇ it is called RMS Pro„ÄÇ And there I think it's from a cost Jo H taught„ÄÇ

 you can find references to that on the InternetÔºå but there is no official paper of that or corresponding to that„ÄÇ

 It's something Jo Hinton„ÄÇWho was one of the persons popularizing and working on yeah neural networks throughout„ÄÇ

 yeahÔºå the last 60 or so yearsÔºå 50 years„ÄÇYeahÔºå it has been mentioned in one of his talks and people started using that and it performed pretty well„ÄÇ

 it's essentially a modification of RropÔºå which was published in 2000 around 2000„ÄÇAnd yeah„ÄÇ

 it's also relatively similar to another concept called Ada Delta„ÄÇ

So the main idea behind this arm is prop by mentioning it because it will be relevant when we talk about the atom algorithm„ÄÇ

So the main concept is we divide the learning rate by an exponentially decreasing moving average of the squared gradients„ÄÇ

 So it's essentially a modified version of this adaptive learning rate that I showed you on the previous slide„ÄÇ

 So the previous slide was a very simple type of adaptive learning rate„ÄÇ

 This is a little bit more sophisticated„ÄÇBecause yeah„ÄÇ

 it takes also into account that gradients can vary widely in the magnitude„ÄÇ

 so some weights or gradients are larger than others„ÄÇ

And the arm S and arm S Pro stands for root means squaredÔºåca it'sÔºå yeah related to these„ÄÇ

Squared gradients„ÄÇAnd it also has an effectÔºå this dampening effect„ÄÇ

 in addition to the adaptive learning„ÄÇ it also has this a dampening effect of the„ÄÇ

A momentum on the oscillations„ÄÇBut in practiceÔºå yeah„ÄÇ

 people thought it might work better than using momentumÔºå so this arm S prop„ÄÇ

People found works better than just momentum„ÄÇ but yeahÔºå nowadays„ÄÇ

 people use the combination of atom or people use atom„ÄÇ

 which is a combination of arm S prop and momentum„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_9.png)

OkayÔºå but let's talk about this arm S prop first before we go into the Adamom algorithm„ÄÇ

So there is now a mean square term here„ÄÇ That is the moving average of the squared gradient of each weight„ÄÇ

 It's kind of in a wayÔºå similar to momentum„ÄÇ But we have now„ÄÇ

 instead of just considering the moving averageÔºå we have the moving average„ÄÇ

Of these squared gradients„ÄÇThere's a bitter term„ÄÇSo T isÔºå againÔºå the time step„ÄÇFor instance„ÄÇ

 the iteration„ÄÇAnd WI J is the weight we want to update„ÄÇSo we have the mean squared gradient„ÄÇ

Of that weight at a given time step„ÄÇ And this is computed by multiplying a beta with„ÄÇ

The mean square value from the previous iteration„ÄÇSoÔºå and then on top of that„ÄÇ

 we add these squared gradients for the current iteration„ÄÇ So this is„ÄÇThe mean term here„ÄÇRight„ÄÇ

 so this is something we keep from the previous rounds„ÄÇ

 And this is only for the current round for time step T„ÄÇ

 So it's essentially the gradient of the loss or partial derivativeertive of the loss with respect to the weight should be a„ÄÇ

Pel here„ÄÇAnd„ÄÇWe square those essentiallyÔºå so they are always positive values„ÄÇYeah„ÄÇ

 and then this is how we compute the mean square value„ÄÇ

 and then we use that mean square value to scale„ÄÇThe gradients at that update„ÄÇ

 So here we are just computing„ÄÇ This means square term„ÄÇ And here we are applying it„ÄÇ

 How do we apply thisÔºå So if you look at this againÔºå this is like the„ÄÇRegular„ÄÇGreat incent update„ÄÇ

 rightÔºå We take a„ÄÇStep into the directionÔºå the negative direction of the gradient„ÄÇ

 So we subtract the gradient times the learning rate„ÄÇBut nowÔºå in addition to doing that„ÄÇ

 we scale by this mean square term„ÄÇAnd we take the square root„ÄÇ

 so it has the same unit as the weights„ÄÇSo and this is essentially it it's very similar to what I showed you before the adaptive learning rate where we have a gain that we modify except that this is a certain type of gain that is working a little bit differently„ÄÇ

And yetÔºå we have the small epsilon to avoid division by zero  errorss„ÄÇ In a way„ÄÇ

 it's also somewhat similar to the momentum term„ÄÇ So when we look at atom„ÄÇ

 we will see there are two thingsÔºå the momentum term in this term„ÄÇ

 and they are actually also themselves very similar„ÄÇExcept this is like a scaling factor in a way„ÄÇ

 and the momentum is something that we add„ÄÇ

![](img/8a480effebe949863a966439a53a0acf_11.png)

On top„ÄÇSo that's then now talk about this Adam because I mentioned it so many times it stands for adaptive moment estimation Yeah and it's probably the most widely used optimization algorithm in deep learning as of today„ÄÇ

 I use it a lot because I find it just works very well out of the box and I always find I get almost always the same or even better performance as with SGD plus momentum and learning rate schedulers but I need way less tuning„ÄÇ

I meanÔºå hereÔºå I mean tuning of the learning rate and the strength of the scheduler„ÄÇSo like I said„ÄÇ

 it's a combination of momentum and RMS S prop and to yeahÔºå to make this a little bit easier to read„ÄÇ

I was so also should say if you want to read the original paperÔºå which contains way more detail„ÄÇ

 this is the paper where it was proposed from 2014„ÄÇ So here you can find also more information„ÄÇ

 So here I'm rewriting this slightly to make this a little bit„ÄÇ

 I would say easier to see how it is related to momentum„ÄÇSoÔºå here„ÄÇOn on top„ÄÇ

 this is the original definition of the momentum that we defined in„ÄÇTheLast video„ÄÇ

And I'm just replacing this delta W by this m T„ÄÇ So I'm just using a different notation here„ÄÇ

 I'm just changing the notationÔºå I'm not changing any concept„ÄÇ I'm just sayingÔºå okay„ÄÇ

 let's call this one now at time step T so the momentum term M for a momentum at time step T and here the this is the current time step and this is the time step for the next round essentially„ÄÇ

So here this is the rewritten version of that„ÄÇBut you can see it's slightly different now„ÄÇ

 because we have„ÄÇThis modified versionÔºå where we have„ÄÇAlpha„ÄÇ

 but instead of using the learning rate hereÔºå we use  one minus alpha„ÄÇ

 So it's not exactly like the same like the original momentum term„ÄÇ it's slightly different„ÄÇ

 So we have this one minus alpha„ÄÇInstead of the atta and the original momentum„ÄÇ

 But you can see how similar it isÔºå rightÔºå So we have both on alpha times MÔºå which we have here„ÄÇ

 and we have plus„ÄÇHere we have the gradientÔºå the same thingÔºå except again„ÄÇ

 the only difference here we have the learning rate and here we have one minus alpha„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_13.png)

OkayÔºå so here I was just rewriting the momentum term from the previous slide exactly the same thing„ÄÇ

 just carrying it over and now we have this arm S prop term that I showed you a few slides ago„ÄÇ

 so here we had the better term for multiplying it with a mean square I'm just abbreviating here as R„ÄÇ

And then we have1 minus better times the squared gradients„ÄÇ

 So this is essentially the same I also showed you before„ÄÇ NowÔºå I'm just using R„ÄÇ So it's shorter„ÄÇ

 And then for atomÔºå we combine both„ÄÇ we have momentum„ÄÇ

So we have the learning rate times the momentum term„ÄÇ It's essentially a velocity„ÄÇ

 and we scale that momentum term by this„ÄÇR S prop term„ÄÇ So squared with R plus this epsilon„ÄÇ

And this is essentially how Adamom works„ÄÇ So we have„ÄÇThe momentum„ÄÇTerm and here„ÄÇY„ÄÇüòîÔºåAs„ÄÇCh„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_15.png)

Okay on„ÄÇThere is a slight„ÄÇI would say modification of that in the paper„ÄÇ

 So here that's the original paper„ÄÇ how it is defined in the paper„ÄÇ

 And what I haven't contained or included in the slides for simplicity was this bias corrected first moment estimate„ÄÇ

 and the bias corrected second raw moment estimate„ÄÇ So here the difference is essentially that„ÄÇ

I should also sayÔºå I'm calling it alpha and beta„ÄÇ

![](img/8a480effebe949863a966439a53a0acf_17.png)

So here they have be 1 and be 2„ÄÇ So if we go backÔºå I call that alpha just because I was„ÄÇ

 I don't know„ÄÇ I found it simplerÔºå but this is essentially better  one„ÄÇAnd hereÔºå this is„ÄÇBetter too„ÄÇ

In the paperÔºå I just found it somehow inin intuitivetu to say alpha and better because wehausa used alpha before when we talked about momentum„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_19.png)

So yeahÔºå they just correct this term by having„ÄÇAnother scaling by so they scale the momentum to by dividing by1 minus beta to the power of t„ÄÇ

 where t is the time step essentiallyÔºå and they do the same thing for this V term„ÄÇ



![](img/8a480effebe949863a966439a53a0acf_21.png)

YeahÔºå and this is essentially itÔºå so in the next video I will show you how we can use these optimization algorithms in Pytorch„ÄÇ

üòä„ÄÇ

![](img/8a480effebe949863a966439a53a0acf_23.png)