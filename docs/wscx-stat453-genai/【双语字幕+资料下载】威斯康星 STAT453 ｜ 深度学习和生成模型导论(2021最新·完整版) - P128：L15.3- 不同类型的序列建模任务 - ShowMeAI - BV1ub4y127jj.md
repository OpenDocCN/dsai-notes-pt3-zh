# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P128ï¼šL15.3- ä¸åŒç±»å‹çš„åºåˆ—å»ºæ¨¡ä»»åŠ¡ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ so in the previous video I showed you the big picture overview of how a recurrent neural network looks likeã€‚

 and now I want to show you that we can use this architecture for different types of sequence modeling tasksã€‚



![](img/964e5934239eacaf54bdebad125ce02d_1.png)

So on this slide I have an overview of the four common sequence modeling tasksã€‚

 and I will walk through them step by step in the next couple of slidesã€‚



![](img/964e5934239eacaf54bdebad125ce02d_3.png)

So starting with this many to one settingï¼Œ that is when we have multiple inputs and one outputã€‚

 So the input data is a sequenceï¼Œ but the output is a fixed size vector or valueï¼Œ not a sequenceã€‚

 one example would be sentiment analysis where the input could be some text and the output could be a class labelã€‚

 whether the text is yeah positive or negative in terms of the sentimentï¼Œ for instanceã€‚

 if you think back of the movie review example that I mentioned earlierã€‚

 So each input could be a written review and the output could be whether the review thinks the movie is positive or negativeã€‚



![](img/964e5934239eacaf54bdebad125ce02d_5.png)

Another one would be one to manyï¼Œ so this is another type of sequence modeling task where you could I meanã€‚

 this is a little bit more beyond just standard R andNsã€‚

 It's more like something that you could use together with CN andNsã€‚

 so it's also of course possible to combine both networksã€‚So hereã€‚

The input data would be standard inputã€‚ Soï¼Œ for exampleï¼Œ an imageã€‚

 and the output would be a sequenceã€‚ And one example would be image captioning where here the input isã€‚

 let's say an image of somethingã€‚ and then the output is the text description matching that imageã€‚

 for instanceï¼Œ if you have a picture of someone who's playing tennisã€‚ you can have aã€‚Let's sayã€‚

An image like thatã€‚ And as a personã€‚Withã€‚ğŸ˜”ï¼ŒTennis racket or something like thatã€‚

 And the description based on that image could be the sequence of textã€‚

 saying a person playing tennis or something like thatã€‚



![](img/964e5934239eacaf54bdebad125ce02d_7.png)

Another type of task is many to manyï¼Œ and there are actually two flavors of thatã€‚

 So here in general in the many to many setupï¼Œ the inputs and the outputs are sequences andã€‚

It could be a direct or a delayed set up hereã€‚ So on the left hand sideã€‚

 this is like a direct one where we have an input here and an output thereã€‚

And then for the next element in the sequence and input and an output and so forthã€‚

 and this could beï¼Œ for instanceï¼Œ a video captioningã€‚ So for instanceã€‚

 if you have a video which consists of multiple frames where each frame is an imageã€‚

 So if I go back one timeã€‚ So each frame would be a one to many taskï¼Œ but when you think of a videoã€‚

 you have multiple of these framesï¼Œ it would be then generating a description for each inputã€‚



![](img/964e5934239eacaf54bdebad125ce02d_9.png)

![](img/964e5934239eacaf54bdebad125ce02d_10.png)

And a delayed task would beï¼Œ for exampleï¼Œ translating one language into another languageã€‚

 So you could have a sentence in English and then youï¼Œ let's sayï¼Œ translated to Germanã€‚So hereã€‚

 this would not be a good case for a direct setup becauseã€‚You don't want to translate each wordã€‚

You don't want to translate the sentence word by word right because usually if you think of like just taking a dictionary and translating word to word from one language and the otherã€‚

 it usually doesn't give you very readable results because it's more complicated than that because different languages have different grammar rules so you can't just translate word by wordã€‚

 you have to first read the whole sentence and then you can generate the translationã€‚



![](img/964e5934239eacaf54bdebad125ce02d_12.png)

Alrightï¼Œ so this was just a quick overview of the different sequence modeling tasks in the next video we will take a look at back propagation through timeã€‚

 That is how we learn the parameters in the recurrent neural network then we will yeah talk about an improvement of the standard recurrent neural network setup and then we will take a look at examples here in these examplesã€‚

will be many to one examples using word R andNs where we will train R andN classifier and yeah laterã€‚

 later in this class Im planning to also have something on generating textã€‚

 maybe we have to see how much time we have maybe also language translationã€‚



![](img/964e5934239eacaf54bdebad125ce02d_14.png)

![](img/964e5934239eacaf54bdebad125ce02d_15.png)

Right rightï¼Œ so next let's talk about Beck propagation through timeã€‚



![](img/964e5934239eacaf54bdebad125ce02d_17.png)