# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P66ÔºöL9.3.2- PyTorch Part 2Ôºè3 ‰∏≠ÁöÑÂ§öÂ±ÇÊÑüÁü•Âô®(Jupyter Notebook) - ShowMeAI - BV1ub4y127jj

AllrightÔºå let's now take a look at some code examples„ÄÇ so I prepared multiple notebooks„ÄÇ

 One is a multiar preceptron from scratch implementation just to practice my calculus skills„ÄÇ

1 is Pytor sigmoid and means code error implementation that I showed you on the slides„ÄÇ

 And this one is the Pytorch„ÄÇWith a soft Pyth implementation with the softm activation and across entropys„ÄÇ

I will only walk you through this one because they are all fundamentally very similar„ÄÇ And yeah„ÄÇ

 I don't want to make the lectures too long„ÄÇ So let's get started„ÄÇ So here again„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_1.png)

![](img/6785be20b4e65afee85a3f103de7402c_2.png)

Just the boilerplate stuffÔºå all the imports„ÄÇHere are some hyperparameter settings„ÄÇ

 I set the batch size to 100„ÄÇ It's kind of arbitrary„ÄÇ

 usually people like to use also values like powers of2Ôºå like 32Ôºå64Ôºå128„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_4.png)

![](img/6785be20b4e65afee85a3f103de7402c_5.png)

I use 100 epochs and yeah here the Mnes data set So the Ms data set is or how we loaded is how we did it also in the Somax lecture„ÄÇ

 so there's nothing new here„ÄÇ I will have a separate video explaining how you can set up these data load for your own data„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_7.png)

![](img/6785be20b4e65afee85a3f103de7402c_8.png)

![](img/6785be20b4e65afee85a3f103de7402c_9.png)

![](img/6785be20b4e65afee85a3f103de7402c_10.png)

So now here we are just regarding it as the standard Mnes dataset that is already included in torch vision„ÄÇ

 So what I want to focus on is really here the multi layer perceptron implementation„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_12.png)

![](img/6785be20b4e65afee85a3f103de7402c_13.png)

SoÔºå here„ÄÇReallyÔºå what's new compared to softmax regression is that we have this hidden layer„ÄÇ

 So if I remove this hidden layer„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_15.png)

![](img/6785be20b4e65afee85a3f103de7402c_16.png)

For a second„ÄÇAnd just right„ÄÇNoumb features here„ÄÇOops„ÄÇ

Then this is the same as the softmax regression code„ÄÇ

The only maybe small difference that you might notice is that„ÄÇOkay„ÄÇ

 I would also have to remove this one„ÄÇCance the not„ÄÇSo the only difference is that I'm using small„ÄÇ

 random weights„ÄÇ I asked you why we don't„ÄÇImplement or why we don't initialize the weights to zeros in multi episodecyclron„ÄÇ

 So that's something for you to think about„ÄÇ But yeah here„ÄÇWe use a standard normal„ÄÇ

Distribution and we scale it„ÄÇ So the mean„ÄÇ So there are two parameters for this„ÄÇ

 the mean and the standard deviation„ÄÇ So usually a standard normal distributionÔºå as you might know„ÄÇ

 has mean0 and unit variance„ÄÇ So standard deviation of one here„ÄÇ

 we use a smaller value01 just making the distribution a bit narrowÔºå closer to 0„ÄÇ

 So that's because we don't want to have yeah two large or two extreme values because recall sigmoid activations can chaturate„ÄÇ

So then we have very small gradient„ÄÇ So it's actually good to have them not too big„ÄÇ So yeah„ÄÇ

 that is what we do here„ÄÇ The whole point really is just having small random weights center at 0 later in later lectures„ÄÇ

 we will also learn about other ways to initialize the weights„ÄÇBut yeahÔºå let me undo that„ÄÇ Otherwise„ÄÇ

 the code won't work„ÄÇ So now„ÄÇWhat it's different from softm is really that we have this hidden layer here in between„ÄÇ

 right between the input and output layer„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_18.png)

![](img/6785be20b4e65afee85a3f103de7402c_19.png)

Alright„ÄÇ soÔºå but there's nothing really new„ÄÇ I meanÔºå I wish I could tell you something exciting here„ÄÇ

 but actuallyÔºå it's very similar to softm„ÄÇ So it's actually maybe a good sign„ÄÇ

 It's not too complicatedÔºå I hope„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_21.png)

![](img/6785be20b4e65afee85a3f103de7402c_22.png)

YeahÔºå just use Probu here„ÄÇ You don't have to do that so you can actually skip this step„ÄÇ

 You don't need that because if you use the cross entropy loss„ÄÇ

 Pythrch will already compute that softmax implicitly automatically„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_24.png)

YeahÔºå and when we initialize here the multiceptronÔºå the number of input features is 28 by 28„ÄÇ

 that's the dimension of MNT„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_26.png)

And I use a hidden there with 100 units„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_28.png)

That's a hypoparameter can change that number and see whether it performs better or worse„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_30.png)

Yeah we use also the stochastic gradient descent„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_32.png)

ÏóÖÌä∏ÏóêÏÑú„ÄÇNotice now„ÄÇ So this is So I have a separate function for computing the sets for each epochÔºå so„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_34.png)

![](img/6785be20b4e65afee85a3f103de7402c_35.png)

How I implement that is I am iterating over the data order„ÄÇAnd compute the loss and add it up„ÄÇ

So this is instead of computing it on the whole dataset because the dataset might be too large to load it into memory„ÄÇ

 So in M for MS it might workÔºå but it's just a good practice to implement a function that does it also incrementally notice this is just a compute loss function that I used to compute the loss on the training set„ÄÇ

 This is not a training function„ÄÇ but there was a good question on Piazza I used something similar before no Grd I before used on the manual version like enable enable gradient faults„ÄÇ

This is the same thing„ÄÇ So here I disable the gradient„ÄÇ or I'm saying we don't need a gradient„ÄÇ

 So I'm saying don't build this computation graph„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_37.png)

Why do I not have to have a computation graph„ÄÇ So here I'm just computing the loss„ÄÇ

 So there is no backward wordÔºå so it would be wasteful„ÄÇ Actually„ÄÇ

 this should be just net would be better„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_39.png)

![](img/6785be20b4e65afee85a3f103de7402c_40.png)

YeahÔºå also how I'm computing the losses actuallyÔºå I can show it to you also here„ÄÇ So here„ÄÇ

 how I'm computing the lossesÔºå I'm using the negative log likelihood loss„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_42.png)

![](img/6785be20b4e65afee85a3f103de7402c_43.png)

And then I'm calling lock Probu„ÄÇ This is how we do it„ÄÇ let's sayÔºå theoreticallyÔºå mathematically„ÄÇ

And I've done that in my from scratch and the other implementation„ÄÇ

 but in practice it's actually better to use F cross entropyÔºå a functional cross entropy version„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_45.png)

![](img/6785be20b4e65afee85a3f103de7402c_46.png)

So actuallyÔºå let me modify this code to show you the recommended way„ÄÇ

 I did that just for comparison purposesÔºå but„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_48.png)

It's actually better„ÄÇTo have it like this„ÄÇHope I don't delete lead it in a way that it doesn't work afterwards„ÄÇ

 What we will see will be a surprise„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_50.png)

Okay„ÄÇüòîÔºåSo hereÔºå I call it cost„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_52.png)

AlrightÔºå so yeahÔºå we don't need to call softmax ourselves becauseÔºå yeah„ÄÇ

 Pyrs already doing it for us„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_54.png)

![](img/6785be20b4e65afee85a3f103de7402c_55.png)

But except that the training is exactly the same„ÄÇ we haveÔºå we call forward here„ÄÇ

 So that gives us the outputsÔºå the logicsÔºå compute the cross entropy where this one does the logof Max for us„ÄÇ

And we set the gradients from the previous round to 0Ôºå called backwards to do the back propagation„ÄÇ

And this is just for logging purposes„ÄÇMaybe I should put it down here„ÄÇ

 It's like just for keeping track„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_57.png)

So we can make the plot„ÄÇAnd this is the model update„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_59.png)

Yet he am computing the loss over the whole training set„ÄÇ This is just for plotting„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_61.png)

So this is using this function here„ÄÇI will actually show you another video after that where I reflect this a little bit into Python scripts„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_63.png)

![](img/6785be20b4e65afee85a3f103de7402c_64.png)

AlrightÔºå yeahÔºå I„ÄÇCommon problem„ÄÇ I forgot to execute these things„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_66.png)

OkayÔºå it's training„ÄÇ So this is training on the CPU„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_68.png)

I compiled Pyr for the M1 MacÔºå So it might be a bit faster than on a normal CPU„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_70.png)

![](img/6785be20b4e65afee85a3f103de7402c_71.png)

But yeahÔºå it's still simple enough to just run it on a CPU„ÄÇ You don't need a GPU for thatÔºå really„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_73.png)

RightÔºå you might take a while now„ÄÇ You have seen the results on the slides alreadyÔºå right„ÄÇ

 So the loss and things like that„ÄÇ So let me„ÄÇ

![](img/6785be20b4e65afee85a3f103de7402c_75.png)

Past this video„ÄÇ I can show you the result in the beginning of the next video„ÄÇ

 and I will show you also how I would personally run code in Python scripts„ÄÇ

 So using Jupyter notebook is super nice for teaching because we have everything in one notebook„ÄÇ

 But like I saidÔºå if our code becomes more complicated„ÄÇ

 It might be nice to put some of the code into Python scripts where it's sometimes easier to debug And also you can then reuse some of your functions„ÄÇ

 So let me pause this videoÔºå I will make another video showing you the Python scripts„ÄÇ



![](img/6785be20b4e65afee85a3f103de7402c_77.png)