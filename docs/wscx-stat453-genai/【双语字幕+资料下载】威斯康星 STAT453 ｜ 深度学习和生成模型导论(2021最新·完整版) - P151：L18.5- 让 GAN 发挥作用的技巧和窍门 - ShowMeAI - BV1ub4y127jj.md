# P151ÔºöL18.5- ËÆ© GAN ÂèëÊå•‰ΩúÁî®ÁöÑÊäÄÂ∑ßÂíåÁ™çÈó® - ShowMeAI - BV1ub4y127jj

YeahÔºå so instead of making more slidesÔºå I thought it might be more fun to do this video on tips and tricks to make ends work a little bit more interactive„ÄÇ

 That isÔºå yeahÔºå I will walk you through a list of very nice list of tips and tricks„ÄÇ

 And then we will see in the code that we seen last video„ÄÇ How or if I used these tricks„ÄÇ OkayÔºå so„ÄÇüòä„ÄÇ

This list is based on Gitub repository by Sit Chintala„ÄÇ

 sumit Chintla is a researcher at Facebook AI research and yeah is' also one of the main Pyt developers and also back in 2016 worked on the Vas Daen G which is a very popular version of the G„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_1.png)

So„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_3.png)

Here in this listÔºå there areÔºå I think 17 tips„ÄÇ so I wanted to walk through them step by step„ÄÇ

 It says that this list is no longer maintained and he says he is not sure whether it's still relevant in 2020„ÄÇ

 but actually most of the tips are still very useful„ÄÇ So they are really useful starter tips for GNs„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_5.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_6.png)

Even though the list is not maintained any more„ÄÇ So let's look at them one at a time„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_8.png)

So normalizing the inputsÔºå normalize images between -1 and one range and using 10 H in the last layer of the generator output„ÄÇ

 So that is something I actually did„ÄÇ So here I normalized the images in-11 range and then I also had my 10 h here„ÄÇ

 By the wayÔºå I was writing this code before I looked at the list„ÄÇ

 So everything is just a per coincidence„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_10.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_11.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_12.png)

based on something I've heard before that seems to work well in practice„ÄÇSo then in G papers„ÄÇ

 the loss function is to optimize„ÄÇIt as followsÔºå but in practice we do the maximization and then we flip the labels„ÄÇ

When training the generatorÔºå this is for theÔºå the whole thing is for the generator„ÄÇ

 the modified loss function„ÄÇ And this is also what we extensively talked about„ÄÇ So when we„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_14.png)

Go back here„ÄÇ This isÔºå essentially„ÄÇWhat we talked about when we flipped this one minus into just„ÄÇ

 yeahÔºå the output„ÄÇ And then we also flipped the labels that was right here„ÄÇ So yeah„ÄÇ

 we are also using that trick„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_16.png)

That was also in the code„ÄÇUsing a spherical z„ÄÇSo instead of sampling the noise from a uniform distribution„ÄÇ

 we are sampling from aÔºå yeahÔºå Gaussian distribution„ÄÇ And as„ÄÇ

 that's also something I it when we revisit„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_18.png)

Training code hereÔºå so„ÄÇI think it was at the bottom somewhere training again„ÄÇ So yeah„ÄÇ

 we used a random normal distribution and not„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_20.png)

Uniform distribution„ÄÇAlrightÔºå next„ÄÇUsing beon„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_22.png)

So when using batch normm it's so I'm actually not using batch normm in this code„ÄÇ

 but I will show you in the next video my code for the setup a face image dataset set where I was writing a convolutional again and there I used batchn„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_24.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_25.png)

And hereÔºå the trick or tip is about not mixing the real and generated imagesÔºå so„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_27.png)

Here the recommendation is keeping them separate„ÄÇ So training the discriminator on a patch of real and a patch of generated instead of mixing real and generated„ÄÇ

 And that is also something we did„ÄÇ So when we go back to the code here„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_29.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_30.png)

Âì¶„ÄÇRight„ÄÇHereÔºå so we have fake images„ÄÇAnd we have„ÄÇNow the real images all appear here„ÄÇ

 real images and fake imagesÔºå and we feed them to the discriator separately„ÄÇ

 So first we get the real images and then the fake images we don't mix them together„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_32.png)

OkayÔºå next„ÄÇAvoiding sparse gradients using leaky redo„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_34.png)

SoÔºå yeahÔºå like we talked about beforeÔºå the Relu can have these dead neuronsÔºå that's„ÄÇLecture„ÄÇ

 I think lecture 5Ôºå6Ôºå something like thatÔºå or was it later could have been9 when we talked about activation functions and we talked about the debt re problem„ÄÇ

 and yeah if we have a generator that should generate something„ÄÇ

 maybe using a regular re is not a great idea„ÄÇSo using relu here„ÄÇ

 actually in both and G and D the generate and this screenator might be a good idea so„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_36.png)

Let's check whether we use that„ÄÇYepÔºå I have leaky red here„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_38.png)

Next for dawn samplingÔºå use average pooling„ÄÇ OkayÔºå this is something I have not done„ÄÇ

 So this is actually it will be in my next„ÄÇ Let me just double check„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_40.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_41.png)

NoÔºå I don't have„ÄÇDone that„ÄÇOkayÔºå that might be„ÄÇThat might be something to consider to improve my code using average pullinging and for up samplingling con transpo plus strand that is something I used„ÄÇ

 Pix shuffle might also be an additional interesting trickÔºå which we may find here in this paper„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_43.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_44.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_45.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_46.png)

Using softÔºå noisy labelsÔºå that is something I have tried in the past„ÄÇ it worked a little bit better„ÄÇ

 I think so„ÄÇWasn't„ÄÇThat much better about slightly better„ÄÇ

 I haven't done this in this code because yeahÔºå I didn't„ÄÇ didn't keep in mind everything„ÄÇ

 But if you want to play around with thatÔºå it's another interesting thing to do„ÄÇ So instead of using„ÄÇ

1Ôºå for realÔºå we use random numbers between0„ÄÇ7 and1„ÄÇ2 to make the labels a bit softer or not softer„ÄÇ

 but to yeahÔºå instead of having these fixed numbersÔºå having some uncertainty around them„ÄÇ

 And then for the fake ones to use the numbers between 0 and 03„ÄÇ Actually when I did that„ÄÇ

 I had like a slide„ÄÇ So I only had the soft labelsÔºå not the noisy labels„ÄÇ

 So I only had like instead of1Ôºå I had a 0„ÄÇ9 instead of 0Ôºå I had a 0„ÄÇ1 and it helped a little bit„ÄÇ

 So I haven't tried this range before or this range0„ÄÇ3„ÄÇ So it might be something interesting to try„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_48.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_49.png)

And there's also another thing here making the labels noisy for the discriminator by occasionally flipping them„ÄÇ

 I also heard this works very well in practice to improve the discriminator so it doesn't become too good„ÄÇ

 So you kind of shake it up sometimes„ÄÇ and I also have not tried that yet it might be another interesting thing to try„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_51.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_52.png)

Yeah using DC again when you can so I okay I mean I intentionally didn't use it here to keep things simple„ÄÇ

 just having the simple regular again with the fully connected layers„ÄÇ

 but in the next code example where we work with face images I will use a DC again So nowadays also like I mentioned before„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_54.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_55.png)

In the lecture„ÄÇIt's just called again because nowadays when DC again was new everyone used the abbreviation„ÄÇ

 DC again to distinguish it from the original again„ÄÇ

 but nowadays convolutionalgan are so common that we don't say DC againÔºå we just say again„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_57.png)

Using stability tricks from reinforcement learningÔºå so I'm not a big reinforcement learning person„ÄÇ

 so I haven't used these tricks„ÄÇBut what might be useful is keeping checkpoints„ÄÇ

From the generator and discrimininatorÔºå that is like saving them occasionally every few epochs„ÄÇ

 And then alsoÔºå yeah swapping them sometimes„ÄÇ So if things go bad„ÄÇ

 swapping in the old versions could also be useful„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_59.png)

NowÔºå yeahÔºå regarding the optimizer„ÄÇAdam rules„ÄÇ SoÔºå yeahÔºå Adam is usually working„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_61.png)

Most of the timeÔºå well out of the box„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_63.png)

And another recommendation is using SGD for the discrimininator and at for the generator„ÄÇ

I think this is due to the fact that momentum may not be ideal for the discriinator because you wanted to react quickly„ÄÇ

Same actually also for the generatorÔºå but in practice„ÄÇ

 I still find that training actually both with Adamom is even better„ÄÇ I tried this actually„ÄÇ

 and it didn't work so wellÔºå so I switched back to using Adamom for both the the screenator and the generator„ÄÇ

 But againÔºå this is something you have to try and practice„ÄÇ Sometimes it may work better„ÄÇ

 sometimes it may work worse„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_65.png)

Trick failures early„ÄÇOkayÔºå so just„ÄÇChecking things if the discrimininator loss goes to 0„ÄÇ

 then that's not good„ÄÇ The discriminator is too strong and then may the generator may not be able to learn anything useful„ÄÇ

 Maybe you have to„ÄÇYeahÔºå see well how you can address that„ÄÇChecking the norms„ÄÇ

So if the norms of the of the gradient norms are too largeÔºå then it might also not be good„ÄÇ

 So some people also use something like a gradient penalty„ÄÇ we haven't talked about this yet„ÄÇ

 It might beÔºå I meanÔºå there are so many infiniteÔºå many things to talk about„ÄÇ

 but it's another thing to keep in mind„ÄÇWhen things are working„ÄÇ

 the discgraator has a low variance and goes down over time versus having a huge variance in spaniking„ÄÇ

 Let's take a look„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_67.png)

YeahÔºå it goes downÔºå I meanÔºå goes up and down and then kind of stabilizes looking actually there are no spikes„ÄÇ

 actually looking at the value or also the variance is relatively small compared to the generator so that kind of looks like it's a hearing to what we would expect here„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_69.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_70.png)

One thing about that is„ÄÇThis value here is aroundÔºå I would say maybe 0„ÄÇ5„ÄÇ6 something„ÄÇ

 And if you think about„ÄÇA random prediction around 0„ÄÇ5„ÄÇ and then you take the lock of 0„ÄÇ5„ÄÇ

On minus- log „ÄÇ5Ôºå it should be around „ÄÇ69Ôºå something like that„ÄÇ

 So it's actually kind of like a random prediction„ÄÇ So it's„ÄÇ

 it's actually quite good that it is in that range here„ÄÇ So for binary prediction„ÄÇ5 managed of „ÄÇ

5 should be around„ÄÇ69„ÄÇSo in that wayÔºå we can seeÔºå okayÔºå this is actually close to„ÄÇ69„ÄÇ

 So it's kind of like a random prediction here„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_72.png)

Which is goodÔºå okay„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_74.png)

If loss of generator steadily decreasesÔºå then it's fullying„ÄÇD with garbage„ÄÇSoÔºå yeahÔºå okay„ÄÇ

 we don't have this problem here„ÄÇ I will show you actually a failure case in the next video„ÄÇ

 Let me just double check on that„ÄÇ I can show it already„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_76.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_77.png)

It's not going down„ÄÇ It's rather going upÔºå but yeah„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_79.png)

There was something interesting happening there we will revisit it in next video„ÄÇK„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_81.png)

Don't balance the loss via statistics unless you have a good reason to„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_83.png)

So yeahÔºå that isÔºå don't try to find a number of generator number of the updates„ÄÇÂóØ„ÄÇ

It usually doesn't work so well in practice it's kind of hard just to find the good recommendation if you recall the original G paper head a hypoparmetera for the number of discriator updates before updating the generator and let's go to the paper„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_85.png)

Just screenshot of that algorithm somewhere„ÄÇHere„ÄÇYeahÔºå the K steps„ÄÇ So here they used K equals  one„ÄÇ

 one might be tempted to have multiple discriminator updates before updating the generator„ÄÇ But yeah„ÄÇ

 apparentlyÔºå this is probably not a great idea„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_87.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_88.png)

It's hard„ÄÇ We've all tried it„ÄÇAlsoÔºå yeahÔºå following a more principled approach„ÄÇLike„ÄÇ

 checking the loss of the losses„ÄÇToo large„ÄÇ Or if the loss is very large train„ÄÇ

Dis screener until it goes down with the not for the generator to large train it until it goes down instead of doing a fixed number of updates„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_90.png)

If you have labelsÔºå use them„ÄÇSo if you have labels available train„ÄÇ

 it is greater to also classify the samples„ÄÇSo it's kind of like a auxiliary gun„ÄÇ Actually„ÄÇ

 last yearÔºå we worked in the paper„ÄÇ We actually did that here„ÄÇ This is we used the cycle again„ÄÇ

 It's more of the advanced conceptsÔºå which we won cover in this class„ÄÇ but part of itÔºå we also„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_92.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_93.png)

Had a G here„ÄÇ I meanÔºå this is like an interesting setup„ÄÇ We have an auto encoder plus a again„ÄÇ

 So there's also the G aspect that we have a discriator here„ÄÇ And this is the generator„ÄÇ

 But it happens also in auto encoder„ÄÇ So it's kind of like a hybrid„ÄÇ

And here we also had attribute classifier and an auxiliary face match„ÄÇ

 which are kind of like auxiliary ones„ÄÇ So this is really the auxiliary one„ÄÇ

 And this is an additional constraint here„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_95.png)

So it's alsoÔºå yeahÔºå might be a good idea if you have label information to include that as well„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_97.png)

It also brings me to the topic of how we evaluate Gs„ÄÇ It's a kind of a tricky question„ÄÇ

 It's still an active research problem„ÄÇ We haven't really talked about it„ÄÇ

 One is called fresht inception distance„ÄÇAnd it's kind of also„ÄÇ

 it's based on essentially comparing so many of these„ÄÇ

Metrics are based on comparing the distribution of the training data to the distribution of the generated data to see how similar the distributions are and sometimes people also use pre-trained models„ÄÇ

 for instance you can train let's say model on let's say MN a classifier on MN and and you do the classification on the original Mnes data and then you do the classification on the generated dataset and you expect approximately that so the better the model is the better the classifier prediction should be because if you train the classifier on the original dataset and then you show a garbage it probably won't perform as well the distribution of predictions will be different„ÄÇ

From the training set distribution predictions„ÄÇSo that that is kind of one way to kind of get a feeling of how good the results are„ÄÇ

 But yeahÔºå there are many other metrics which are also slightly out of the scope of this class„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_99.png)

ÂóØ„ÄÇAdd noise to the inputs decay over time„ÄÇSo adding some noise„ÄÇ It's kind ofÔºå I think„ÄÇYeah„ÄÇ

 this is like adding noise to the way of the generator and to the inputs„ÄÇ

 it's kind of like adding if you think back of the denoising out and code we talked about„ÄÇ

 it's kind of like that adding some noise to the input images„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_101.png)

Train the discriator more„ÄÇ So it's not sure„ÄÇ So yeah„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_103.png)

ThereÔºå that is like going back to the tricky part that we also had here in the paper whether we should train the disc screenator what times the a generator„ÄÇ

 It's a not sure thing„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_105.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_106.png)

Bched discrimination„ÄÇ I actually forgot what that is„ÄÇ SorryÔºå should look this up again„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_108.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_109.png)

Discreet variables„ÄÇSo yeahÔºå so conditional againÔºå is another topic we haven't talked about„ÄÇ

So what you can also do is you can concatenate„ÄÇThe target variable with the input and then also feed it through the network is in a conditional setting„ÄÇ

 and then it also allows you to reconstruct so„ÄÇIf youÔºå I meanÔºå there areÔºå two ways through that„ÄÇ

 some people concatenated with the input„ÄÇAnd„ÄÇThen you concatenated it with a generated output„ÄÇ

And you check the„ÄÇSorry you check the reconstruction also So there's more like for an autoenr setting„ÄÇ

 but you include essentially labeling information and that can also help you to generate particular data points of a particular class if you're interested in that and here it appears that it is also maybe a general trick that helps making Gs perform better„ÄÇ

 I haven't done experiments extensively with with thatÔºå but it also goes back I meanÔºå not without it„ÄÇ

 but it also goes back to our case here where we add„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_111.png)

Add this to the input the target labels essentiallyÔºå we provide both essentially„ÄÇ

 and this is for a different context because we want to switch the attributes of the image„ÄÇ But yeah„ÄÇ

 apparentlyÔºå maybe it can also help stabilizing G training„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_113.png)

Using dropout both during training and testing„ÄÇI have used only dropout during training„ÄÇ

Using it during testing is an interesting idea might be something worth trying„ÄÇ

 And now this is Orri it„ÄÇ By the wayÔºå17 is also my favorite and lucky number„ÄÇ

 I always like to have the 17 in soccer back then„ÄÇLong time ago„ÄÇ but yeah„ÄÇ so anyways„ÄÇ

 So that is just maybe an interesting idea of things to try some„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_115.png)

Inial things that work well with Gs notice that this is not longer maintainedÔºå but many of these„ÄÇ



![](img/f309bc3f2c8659e3a7371e3e82493fef_117.png)

Tips are still very relevant in my opinion„ÄÇ AlrightÔºå so the next video„ÄÇ

 I will then talk about our D C again„ÄÇ

![](img/f309bc3f2c8659e3a7371e3e82493fef_119.png)

![](img/f309bc3f2c8659e3a7371e3e82493fef_120.png)