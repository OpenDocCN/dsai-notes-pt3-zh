# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P150ï¼šL18.4- åœ¨ PyTorch ä¸­ç”Ÿæˆæ‰‹å†™æ•°å­—çš„ GAN - ShowMeAI - BV1ub4y127jj

All rightï¼Œ let's now implement a again in Ptorchã€‚ So here we will start with a simple G with fully connected layers trained to generate handwritten digitsã€‚

And I hope this will make these types of things clear that I tried to explain in the previous video it's veryã€‚

 it's a lot of mathematical notationï¼Œ it's maybe a little bit overwhelmingã€‚

 but you will see in the code example is actually quite straightforwardã€‚I implemented several GNsã€‚

 honestly it took me quite some time to get a good GN working and they're also not greatã€‚

 so theyre just simplegans so they are not that greatã€‚

 but even then it took me quite some time because training GNs is quite challengingã€‚

So I will in the next video go over some tips and tricks for thatã€‚

 but even if you consider these tips and tricksï¼Œ it can still be hard compared to let's sayã€‚

 just training a classifierã€‚So one of the reasons isï¼Œ so hereã€‚

 let's say go over our regular imports and everythingã€‚

 But one of the reason is that we have now two neural networks that we have to trainã€‚

 So before we only hadï¼Œ for instanceï¼Œ one learning rate for training a classifierã€‚ now we haveã€‚

2 modelsã€‚ So we need also two learning rates and can also be hard to find the goodï¼Œ a good kind ofã€‚

I would say ratio between the learning gradesã€‚Let me make this a little bit bigger hereã€‚

Before we only had to tune1ï¼Œ now we have to tune2ï¼Œ the generator learning rate and the discriminator learning rateã€‚

 and also we have to find the right relationshipã€‚ for instanceã€‚

 it could be that this has to be much larger than this one or other way aroundã€‚ So againã€‚

 this is something that has to be tuned and that just yeah increases the complexityã€‚Alrightã€‚

 so this is just a general setup beside having now two learning ratesã€‚ So here we have the data setã€‚

 againï¼Œ nothing specialã€‚ I'm using the endless data set and hereã€‚

I'm normalizing the images between -1 and1ã€‚Worked a little bit better than 0ï¼Œ1ã€‚

And we are only using the training set hereã€‚ we are not using the test set for M because GANs are also unsupervised algorithmsã€‚

 so we only need we don't need labels or something like thatã€‚

 You could technically merge training and test data into one single data set so you have more dataã€‚

 but I was lazy hereï¼Œ so I'm just using the training setã€‚For simplicityã€‚

Just checking that everything loads correctlyã€‚ Now I'm using this torch vision U dot make grid functionã€‚

 which is a pretty nice function to make visualization quickly soã€‚Okayã€‚



![](img/5049c02cf602d62e3419b206a736cae4_1.png)

Here it'sã€‚Ploottting 64 images in an 8 by 8 gridit pretty convenientlyã€‚ It's justï¼Œ yeahã€‚

 basically oneï¼Œ two lines of codeï¼Œ or three lines of codeã€‚ So hereã€‚

 this is a function I will also use later to visualize the resultsã€‚

 It's a quite useful function for just making visualizations quicklyã€‚

 So here's just how some of these headwritten digits look likeã€‚



![](img/5049c02cf602d62e3419b206a736cae4_3.png)

![](img/5049c02cf602d62e3419b206a736cae4_4.png)

![](img/5049c02cf602d62e3419b206a736cae4_5.png)

Okayï¼Œ so here's the interesting partï¼Œ the modelã€‚Soï¼Œ we haveã€‚Now a generator enter a discriminatorã€‚

 so I implement this as photosï¼Œ I use the sequential API for the generator and also the sequential API for the discriminatorã€‚

So this is a fully connected canã€‚ So I have a linear layerã€‚ Here have a leakquire and a dropoutã€‚

 I tried different types of dropout with and withoutã€‚ it worked better with with with dropoutã€‚

 But while it's another thing to tuneã€‚ It's kind of trickyã€‚å—¯ã€‚Yeahï¼Œ andã€‚Like I saidã€‚

 we are using fully connected layers and because I normalized my input images to be on the -11 rangeã€‚

 we have also a 10 h here so that the pixel outputs are also in the  -1 in one range that they are comparable to the input rangeã€‚

 So this is the output from our generatorã€‚ So the output here are our generated images essentiallyã€‚

The discriminator is just a classifierï¼Œ soã€‚It receives an input imageã€‚

 We just flatten it so that it goes into our fully connected layerã€‚Which has the sizeï¼Œ imageã€‚

 hide image width and cut channelsã€‚ the number of featuresã€‚ if we flatten an imageã€‚

Has been a long time ago but this is something you probably remember from the multi layerer perceptron classã€‚

And then we have a leaky re hereï¼Œ dropboard and output layer with one output node because we are using the binary cross entropyã€‚

 We have a binary classification problem hereã€‚ We could have two output nodes and use the multi category cross entropyã€‚

But hereï¼Œ for hereï¼Œ I for simplicityï¼Œ I'm using the binary crossenttropyã€‚

Which is essentially like a regular logistic regressionã€‚Okayï¼Œ soã€‚For eachã€‚

 for both the generator and for the discriminatorï¼Œ I have a forward functionã€‚

 I have the generator forward here and the discriminator forwardã€‚ So the generator forwardã€‚

The generator will receive an input image from noiseã€‚ So the way I implemented my training functionã€‚

 because I implemented it with con auto generators in mindã€‚I have in the training functionã€‚

 something that creates a noise vector that is in the shapeã€‚Color channelsï¼Œ height widthã€‚

So I'm just flattening it hereã€‚ That is just a vectorã€‚And then this noise vectorã€‚

 This is a noise vector from a random normal distributionã€‚It goes into the generatorã€‚

 So this part is executingã€‚This yearï¼Œ this pipelineï¼Œ which will produce a new image generated imageã€‚

 And since the last day is a fully connected layerï¼Œ I'm just reshaping it that it outputsã€‚Alsoã€‚

 the dimensions colour channelsï¼Œ image height and image widthã€‚

 So what goes into here is also colour channels height and widthã€‚

 which is why I'm flattening this hereã€‚So let me put this maybe hereã€‚Z hasã€‚I mentionedã€‚Cï¼Œ H Wã€‚

And the output also has the dimension Cï¼Œ HW across also the batch sizeã€‚

 so we can maybe also add that hereã€‚So Nï¼Œ Nï¼Œ Cï¼Œ Hï¼Œ Wã€‚Okayã€‚

 and the discriminmat is a little bit simpler hereã€‚ it's just receiving the input imageã€‚

 which has the flattening here and then returns the logicits for the predictions that this is a real imageã€‚

Okayï¼Œ so then we initialize so this is our modelã€‚ I will show you the training loop in a few momentsã€‚

 then one more thing is yeah we are initializing our model and then here we are initializing our optimizersã€‚

Here is one important aspectã€‚So first I'm using Adam for bothã€‚

 Some people recommend using S GD for I think the generatorï¼Œ I tried different thingsã€‚

 This happened to beã€‚The best working version that I could getã€‚

 So you may find better settings in the next video I will go over some tricksã€‚

 but this happened to work well for meã€‚ took me a long time to find anything that worked at allã€‚

 anywaysï¼Œ so one important thing here is that of courseã€‚

 we use the generator learning rate for the generator and the discriator learning rate for the discriinatorã€‚

 but the more important thing is that we also use the right parameters hereã€‚

So we have an optimizer for the generator that should only update the generator parametersã€‚

And the disc itã€‚Hereã€‚Should only update the discriator parameters if we had something like thisã€‚Wellã€‚

 this would not work well in practiceï¼Œ because then it wouldã€‚When we take a stepã€‚

Here for the optimizerï¼Œ it would update both the discriator and the disc generatorã€‚

 And this is not what we wantã€‚ So becauseã€‚If I go back to my slides a long time agoã€‚

 but in the first videoï¼Œ I think talked about thisã€‚Might be the second videoã€‚ So we trainã€‚

Let us screen it up firstã€‚While we keep the generator frozen whenï¼Œ when it gets thereã€‚Fake imagesã€‚

 So we only train the discriatorï¼Œ and then we freeze the discriator and only to train the generatorã€‚

 So in order to achieve thatã€‚We have these two different optimizers where we for one optimizerã€‚

 only train the generator where here the discriator remains frozen and here we only train the discriator where the generator remains frozenã€‚

And then we have the training function hereï¼Œ so I have this in a helper file because then I can reuse it across my different notebooksã€‚

 Let me open this hereã€‚So should probably at the bottom because it was the last thing I implementedã€‚

 So here this is my trainingã€‚ So this is a training I'm using hereã€‚

 So let me now explain how the training looks likeã€‚ First of allã€‚

 we are using binary cross entropy with logicsã€‚ Why it's because we haveã€‚

The output here is the logicsã€‚Andã€‚Thenï¼Œ we are samplingã€‚So we are sampling in eachã€‚So for sorryã€‚

 we are generating here a fixed noise vectorã€‚ I'm calling it fixed noise because we are reusing that one to monitor the progress during trainingã€‚

 This will become clear after the epochsã€‚ So for nowï¼Œ maybe ignore this partã€‚

 let's focus here on the epochcateeration during the trainingã€‚

 So for each epoch and the number of epochs we want to trainã€‚This is the same as beforeã€‚

 like in any neural network we trained beforeã€‚ Now here we just get the batch size as a variable so it will be easier to read andã€‚

Hereï¼Œ we are only concerned with the real imagesã€‚ We don't need the labelsã€‚ Usuallyã€‚

 we had something like the labels hereï¼Œ but we are not using themã€‚ So I'm just using an underscoreã€‚

 in Pythonã€‚ The underscore is just like a indicator that this variable is not usedã€‚

So here we have the real imagesã€‚ and now we create our real labelsã€‚ So the real labels are onceã€‚

 So here realã€‚Real is equal to oneã€‚And fake is equal to 0ã€‚So we create our real labels hereã€‚

Now we also generate fake imagesã€‚ This is for the discriminator trainingã€‚

So we create them from noiseï¼Œ rightï¼Œ so here I have a random normal distributionã€‚

I have my batch sizeã€‚ So if I haveï¼Œ let's sayï¼Œ64 real images I'm creating now 64 fake images as wellã€‚

And these are from a noise vectorã€‚ That's the latent dimension hereã€‚ So if I go back to my slidesã€‚

 this is this noiseã€‚Sorryï¼Œ this noise vector here sampledã€‚From aã€‚Random normal distributionã€‚

 You can also use a random uniform distributionï¼Œ but practically in practice random normalã€‚

 is that Gaussian works betterã€‚ So this isï¼Œ let's say if we have a 100 dimensional embeddingã€‚

 this would be a 100 dimensional vectorã€‚ So by defaultï¼Œ I said itã€‚To 100ï¼Œ I thinkï¼Œ hereã€‚

I didn't set it hereï¼Œ let me seeã€‚å“¦ã€‚Yeahï¼Œ set the latent dimension to 100 hereã€‚

This will create a 100 dimensional vectorã€‚ And here I'm reshaping it to an image formatã€‚

 So it's formatã€‚Andã€‚So this is sorryã€‚ I was actually hereã€‚

 but the fixed size vector is created the same wayã€‚ So it's a format N color channels H Wã€‚

Where what's the other oneã€‚Hereã€‚Soæˆ‘ã€‚We are sampling from this random normal distribution with 100 latent sizeã€‚

 This is our color channels because I implemented that with the convolutional autoenr generator firstã€‚

 and then I did the fully connected oneã€‚But this one gets reshapedã€‚This gets reshapedã€‚Hereã€‚

 so that it will also work with a fully connected layerã€‚ So you don't have to worry about thisã€‚ Okayã€‚

 now we are generating these fake images from our noise vectorã€‚We are calling generator forwardã€‚

 This will generate our fake imagesã€‚And then we will create our fake labelsï¼Œ which are0ã€‚Soã€‚

 fake relasã€‚Nbel is oneã€‚And fake labelï¼Œ this label is 0ï¼Œ rightã€‚This is our fake labelã€‚

 and we will also create our flipped fake labelsã€‚ So this is our trick that we discussedã€‚

Where was itï¼Œ we discussed thatã€‚Hereï¼Œ where we had our flippedã€‚Labelsã€‚So the flipped labelsã€‚

 flipped fake labels are the real labelsï¼Œ soã€‚Hereã€‚ğŸ˜”ï¼Œå•Šã€‚Okayï¼Œ let's do this realã€‚å¯ä»¥ä¸ã€‚ğŸ˜”ã€‚

Faith label hereã€‚Think label is oneã€‚This is that we use for fooling the discriinator when we train the generatorã€‚

Nowï¼Œ we are training the discriminator hereã€‚Firstã€‚Like alwaysï¼Œ we0 the previous gradients of theã€‚

Model of the discriminatorã€‚This will only do that for the discreteator onesã€‚And hereï¼Œ so hereï¼Œ againã€‚

 we generated the data for trainingã€‚ And here we carry out the actual trainingã€‚

 So what we do is we nowã€‚Get the predictionsã€‚ These are the lotsã€‚

And the logicits would be in the formã€‚N timesã€‚å—¯ã€‚Sometimes oneï¼Œ rightã€‚ So we convert that just to nã€‚

Instead of having n times 1ï¼Œ we just convert to nï¼Œ it will be just a vectorã€‚

Instead of an end by one matrixï¼Œ just easier to handleã€‚ We are just removing the last imageã€‚

 I could have also done a squeeze hereã€‚It's maybe easierã€‚ğŸ˜”ï¼ŒIt doesn't matterã€‚Okayã€‚Then hereï¼Œ we haveã€‚

The lossã€‚The loss for the real imagesã€‚ So we want the disc screenator to predict realã€‚

 So how we compute the loss is as the loss function again is if we don't specify anythingã€‚

 it's the binary cross entropy with logicsã€‚ So we haveã€‚

Prediction logics that's from the discriminatorã€‚ And this is the real labeledã€‚ So these are the onesã€‚

 So weï¼Œ these are the labelsï¼Œ the onesã€‚ And we want this also to become oneã€‚ So this is the lossã€‚

To predict oneï¼Œ given that the labels are oneã€‚So the real imagesã€‚And then we do the fake images hereã€‚

 it's the sameã€‚As beforeï¼Œ exceptã€‚Nowï¼Œ thatã€‚We haveã€‚

The predictions for the fake images they were obtained on the fake images hereã€‚

 And here I have the detaach because otherwise it will influence the generator gradientã€‚

 It's complaining about thatã€‚ So I'm just detaching it from the generator graphã€‚

 because I could haveã€‚Probablyï¼Œ yeahï¼Œ noï¼Œ it's fineã€‚So yeahï¼Œ it's fake imagesã€‚

Retaching it from the graphã€‚ So I'm not optimizing the generator hereï¼Œ rightã€‚

 The generator is frozenã€‚And then I'm computing the loss hereã€‚

The fake prediction with the fake labelsã€‚Andã€‚These are so fake labels are0ã€‚

 and we want to make them 02ã€‚So againï¼Œ we are just using binary cross entropy between labelsã€‚

 we just want toã€‚Make these labels similar to these labelsï¼Œ the onesã€‚

 and we want to make the fake prediction similar to the fake labelsï¼Œ which are the zerosã€‚Thenã€‚

 I meanï¼Œ we could technically back propagate here and hereã€‚

 But instead of calling backwardwater twiceï¼Œ it's more efficient to justã€‚

Add them together here and then call back what onceã€‚

So I'm just combining the loss on the real labels and the fake labelsã€‚

 So I'm combining that from here and hereï¼Œ adding it upã€‚ It's actually not necessary to have the 0ã€‚5ã€‚

 but why not And then call the backwardã€‚ and then we update the discriminatorã€‚

 So this whole thing is training the discriminatorã€‚Nowï¼Œ we are training the generatorã€‚

So we are now using the flipped labelsã€‚Soã€‚Here we obtain now the predictions on the fake imagesã€‚

Rightï¼Œ same as beforeã€‚ So this is the sameã€‚As beforeã€‚ But now we are using the the gradientsã€‚

For the generatorã€‚We want to keep the gradientsï¼Œ so I'm not detaching anything hereã€‚

And then I'm computing the loss between the fake predictions and the flipped fake labelsã€‚

 So the flipped fake labels are oneï¼Œ rightï¼Œ If I go up againï¼Œ they are oneã€‚And hereã€‚The predictionã€‚

So the discreator will output if it's a good discreminator will output to 0ã€‚But we want to fool itã€‚

 so we wanted to output a oneã€‚But we are not training the discriminator hereã€‚

 We are training the generator here onlyã€‚ That's why we are using optimize the generator hereï¼Œ rightã€‚

And this is also why I'm not detaching anything hereã€‚ It's the disclaimer here remains frozenã€‚

 We are only updating the generatorï¼Œ because when I go backã€‚Hereã€‚

We have only the generator parameters in our optimizer for the generatorã€‚

 so we are only updating the generator hereï¼Œ not the discriminator and the generator will be updated such that in the next iteration it will be better at fooling the discriminator here to output onces because the ones will be then similar to these ones hereã€‚

 So it's trying to make these similarã€‚Alrightï¼Œ so this is essentially how things workã€‚

And then the rest is just logging purposesã€‚Just printing some resultsï¼Œ saving some resultsã€‚ Ohã€‚

 maybe one more thing I wanted to mention isã€‚I mentioned earlier I had this fixed noise hereï¼Œ rightï¼Ÿ

 So why am I using the fixed noiseï¼Œ I'm actually using that in each epochã€‚ So if I scroll down againã€‚

 So in each epoch hereï¼Œ I am using my fixed noiseã€‚And make a predictionã€‚

And then I'm saving my image gridit to just the lists I haveã€‚List hereã€‚

 just I call this images from noise per epochã€‚ and I'm appending to this listã€‚

 and then we can take a look at the generated images during trainingã€‚Okayï¼Œ so let me go to myã€‚

Training hereã€‚ So this is trainingã€‚So you can see the generator discriminator losses hereã€‚

 so they start out pretty balanced and they keep kind of balancedã€‚

 but honestly looking at this is not very informative except that you don't want to have one going to0 and one going really highã€‚

 but other than that it's really hard to tell just by looking at these numbers whether it's training well or not you want them to be not going crazyã€‚

 you don't want to like I said going them going to really infinitely high or become zeroã€‚

 but except that usually there's not that much information you can gain from looking at this okay trained for 100poã€‚

And then yeahï¼Œ here's a loss functionã€‚ So sorryï¼Œ loss plotã€‚

 I saved everything in this locked dictionaryã€‚ and then we have the is greaterator loss per batch and generate loss per batchã€‚

 which I'm showing you hereã€‚ So kind ofã€‚They are pretty stable hereï¼Œ actuallyï¼Œ that's actually goodã€‚

å—¯ã€‚Os somehowå¥½è¯¶ã€‚å•Šã€‚I don't think I can fix it here because I didn't run itã€‚

 but it should have been epochã€‚ so that's why it's not showingã€‚This year was 100 epochsï¼Œ anywaysã€‚å—¯ã€‚

And then the visualizationï¼Œ iss the more interesting partã€‚

 So here I'm visualizing these generated images per epochã€‚ and then the last oneã€‚

I'm not sure if this is necessaryã€‚ğŸ˜”ï¼ŒCould haveã€‚ğŸ˜”ï¼ŒAdd it X rowã€‚Plusï¼Œ5 hereã€‚Anywaysã€‚Wass just lazyã€‚

 just copy and pastingã€‚ So this is just train So plotting from the fixed noiseã€‚ these imagesã€‚

 So you can see the generated images at epoch 0ã€‚ So at the beginningã€‚

 the generator is not able to produce anything usefulã€‚



![](img/5049c02cf602d62e3419b206a736cae4_7.png)

![](img/5049c02cf602d62e3419b206a736cae4_8.png)

You see that hereã€‚Then after Epoch 5ï¼Œ things look a a little bit betterã€‚



![](img/5049c02cf602d62e3419b206a736cae4_10.png)

U book 10ã€‚15ã€‚ğŸ˜”ã€‚

![](img/5049c02cf602d62e3419b206a736cae4_12.png)

20 so forthã€‚ you can see it becomes better and betterã€‚ Now you can even see these7sã€‚ This is a9ã€‚

 So actually things look very goodã€‚ So let's scroll down to the bottomã€‚



![](img/5049c02cf602d62e3419b206a736cae4_14.png)

![](img/5049c02cf602d62e3419b206a736cae4_15.png)

Okay this is last epochã€‚

![](img/5049c02cf602d62e3419b206a736cae4_17.png)

So you can see I didn't really improve that muchã€‚ You have now issues like here and hereã€‚

 So training it for longer might notã€‚

![](img/5049c02cf602d62e3419b206a736cae4_19.png)

Improveï¼Œ but it's actually given that this is a very simpleï¼Œ fully connected Gã€‚

Images look quite reasonableã€‚ So not all of them look greatï¼Œ of courseï¼Œ butã€‚Wellï¼Œ it's not terribleã€‚

 I meanï¼Œ itï¼Œ it's learning somethingï¼Œ rightï¼Œ if you compare to the firstã€‚



![](img/5049c02cf602d62e3419b206a736cae4_21.png)

Apochsã€‚

![](img/5049c02cf602d62e3419b206a736cae4_23.png)

Hereï¼Œ it is learning somethingï¼Œ rightï¼Ÿ

![](img/5049c02cf602d62e3419b206a736cae4_25.png)

So yeahï¼Œ this is our first G and training Gs is tricky to be honestã€‚

 so in the next video I will go over some tips and tricks for training GNsã€‚



![](img/5049c02cf602d62e3419b206a736cae4_27.png)