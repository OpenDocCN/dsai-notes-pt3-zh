# P37ÔºöL5.5- (ÈÄâ‰øÆ)ÂæÆÁßØÂàÜÂ§ç‰π† 2- Ê¢ØÂ∫¶ - ShowMeAI - BV1ub4y127jj

All rightÔºå let's not talk about gradientsÔºå the derivative of multi„ÄÇAll right„ÄÇ

 let's now talk about gradientsÔºå the derivatives of multivariable functions„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_1.png)

So here we are now interested in functions that have multiple inputs„ÄÇ

 so note though that the term multivariable and multivariate are sometimes used interchangeably„ÄÇ

 so multivariate is usually a more common termÔºå but here we really mean multivaririable because multivariate can mean multiple outputs here we are currently only focusing on a case where we have multiple inputs and a single output so for example„ÄÇ

 if you consider linear regression you can have multiple inputs like the features of your dataset„ÄÇ

 So if you have multiple a train example with multiple features„ÄÇ

 or think of the iris data set where you have s length„ÄÇ

 S width a pital length and pital width there would be multiple inputs„ÄÇ

 but the output there would be only one output for example„ÄÇ

 a continuous value like for regression or class label for the prediction„ÄÇÂóØ„ÄÇSoÔºå yeahÔºå in this case„ÄÇ

 we are focusing now on the multi variableable functions„ÄÇ

 So consider a function where we have multiple inputs„ÄÇ if we only had one input„ÄÇ

We would usually write the derivative as„ÄÇFollows with a„ÄÇSmaller letter D here„ÄÇNow„ÄÇ

 since we have multiple variablesÔºå we have actually a vector as the derivative„ÄÇ

 this is also the so called gradient we use this upside down triangleÔºå the Nala here„ÄÇ

 So this is I think it's spelled like this Nala So here we have multiple things going on now in this neck„ÄÇ

So each row is a partial derivative„ÄÇ So what is a partial derivative„ÄÇ

 It's essentially like the full derivative„ÄÇ However„ÄÇ

 it's for a function where we have multiple input arguments„ÄÇ

 And if we compute the partial derivative of F with respect to xÔºå what we do is we set„ÄÇ

Y and Z here or with multipo„ÄÇOnce we said all of the all of these„ÄÇAs constants„ÄÇ

 So we treat them as regular numbersÔºå not as variables„ÄÇ While we compute this one„ÄÇ

 And then once we have computed this partial derivativeÔºå we then focus on the second entry„ÄÇ

 the partial derivative of F with respect to y while„ÄÇConsidering x and z as constants and so forth„ÄÇ

 So we compute each derivative one at a time„ÄÇ the rules are the same as for the full derivative„ÄÇ

 So really the big difference here is only that we treat the other variables as constantsÔºå but yeah„ÄÇ

 that is essentially a very similar concept„ÄÇ

![](img/f2777a0e00499d4d9a6c873abcd3c252_3.png)

So here's an example to illustrate that„ÄÇ So consider this function here x squared y plus y so that two inputs x and y for this function here„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_5.png)

So when we then compute the gradient of this functionÔºå we have two partial derivatives„ÄÇ

 the partial derivative of x with respect to the partial derivative of f with respect to x and the partial derivative of f with respect to y„ÄÇ

 so we can compute them separately„ÄÇ We don't have to intermix them„ÄÇ We can do one at a time„ÄÇ

 So let's start with the partial derivative„ÄÇ use the same color maybe„ÄÇ

Partial derivative root of F with respect to xÔºå so„ÄÇWhat we do is„ÄÇWe canÔºå we treat why„ÄÇAs a constant„ÄÇ

 So you can think of it as a regular number„ÄÇ So it's essentially like a computing X„ÄÇ

Times 1 plus  oneÔºå something like that„ÄÇSo and if you think of the power rule„ÄÇ

 what happens is the two goes up up front and then minus-1„ÄÇ So it will be just„ÄÇTo xÔºå and„ÄÇ

This one will stayÔºå right„ÄÇ So if this was a two or somethingÔºå this would„ÄÇStay„ÄÇ

 So here also the Y stays„ÄÇ So we have2 x Y„ÄÇAnd then plus the derivative of a constant is 0„ÄÇ

 So plus 0„ÄÇ So the derivative would be 2 x y for this one„ÄÇ

 So using the power rule and the constant rule„ÄÇNowÔºå we have computed this partial derivative to XÔºå Y„ÄÇ

 Now we can compute the partial„ÄÇDivative for the second entry„ÄÇFor„ÄÇ

The partial derivative derivative for F with respect to y„ÄÇ So that's what I have done here„ÄÇ

 Notice instead of xÔºå we are focusing no on y„ÄÇSo x here„ÄÇ X is no„ÄÇA constant„ÄÇ

And if this this basically then staysÔºå rightÔºå So we have then„ÄÇJust two x squared as the constant„ÄÇ

 So it's like if you consider a case where we have something like2 x and we compute the f of x„ÄÇ

We compute the derivative of D F with respect to x„ÄÇSo this would be two right„ÄÇ So in the same way„ÄÇ

 if this is a constantÔºå the y goes awayÔºå it just staysÔºå and the one is also a constant„ÄÇ sorry„ÄÇ

The y here is the variable„ÄÇ So the derivative of just the variable is oneÔºå rightÔºü

 So then the derivative of the whole part would be x squared plus 1„ÄÇAlrightÔºå so we have both„ÄÇ

 and then we just arrange them in the vector hereÔºå and this is No gradient„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_7.png)

OkayÔºå now let's consider the case where we have a composite function like the following one„ÄÇ

 Notice that we have two inputs„ÄÇSo we have G and H„ÄÇTwo inputs here„ÄÇ

But they both rely on the same input argument here„ÄÇ

So there's an x that is shared by both functions here„ÄÇ

So remember from the regular chain rule that I explained in the last video„ÄÇ

 if we have a function nesting like this one and we want to compute the derivative„ÄÇ

 we can do this by computing the derivative of the auto part„ÄÇTimes the derivative of„ÄÇThe inner part„ÄÇ

Now for the two inputs yeah we have the following case„ÄÇ

 they depend they have a dependency they depend both on x„ÄÇ so it's a little bit trickier„ÄÇ

 so we actually combine them because we have this dependency here„ÄÇ

 so we still use the chain rule hereÔºå but we combine the results„ÄÇ

 So if we want to compute the derivative of this function with respect to x the inner argument„ÄÇ

 so note that we are not computing we are not computing partial derivative of G„ÄÇ

Of F with respect to G and„ÄÇFor example„ÄÇH separately as the gradientÔºå because both really depend on x„ÄÇ

 I mean we this we do this and this as part of itÔºå but it's not the end result compared to the previous slide where we had a gradient where we had different input arguments„ÄÇ

So„ÄÇHereÔºå like I just saidÔºå we have these partial terms with respect to gene„ÄÇ

 So let me use different colors„ÄÇThis partÔºå this is one partÔºå and„ÄÇThis steep„ÄÇüòîÔºåOther partÔºå however„ÄÇ

 both share the same xÔºå so the derivative of this function with respect to xÔºå the one argument„ÄÇ

 So in fact we have one argument in a way here„ÄÇWe„ÄÇCombine them here with„ÄÇ

Where is my yellow with this plus symbol here so„ÄÇLet me clear„ÄÇ This is maybe more clear like that„ÄÇ

 We still use the chain rule„ÄÇ Let's maybe focus on„ÄÇOn this partÔºå for this part„ÄÇ

 we still use the chain rule like here„ÄÇ So we have an inner and an auto part„ÄÇ

 So the auto part is a partial derivative F with respect to G„ÄÇ here„ÄÇ

 we use the partial symbol because theres G and H„ÄÇHoweverÔºå there's only one input argument to G„ÄÇ

 So here we use a little D„ÄÇ If you don't do this like consistently like that„ÄÇ

 I think no one would blame you if we just use partial symbols that would be fine too„ÄÇ

 but yeah this would be the proper way„ÄÇSoÔºå this would be„ÄÇThe derivative root for this part„ÄÇ And then„ÄÇ

For the second partÔºå we have a partial derivative root of f with respect to H so„ÄÇ

 and then the inner part so„ÄÇLet me do this one more time„ÄÇ So this is„ÄÇOne part„ÄÇ

 this is the other part„ÄÇAnd here we have the chain rule for each of them„ÄÇ

 the outta  one and the inner one„ÄÇHere we also have the auto  one„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_9.png)

And„ÄÇAlrightÔºå let's take a look at an example to apply this in practice„ÄÇYeah„ÄÇ

 consider the following function where we have two input functionsÔºå G and H„ÄÇ

 So this is just for reference what I had on the previous slide and the function is G squared H plus H„ÄÇ

Where G is also a function that takes as input xÔºå where this is 3 x„ÄÇ So the function G is 3 x„ÄÇ

 and then H also is a function taking input x„ÄÇ And so this function is x squared„ÄÇAllrightÔºå so„ÄÇ

Doing it one at a timeÔºå so let's focus on this part firstÔºå which is here„ÄÇ

So here this is the partial derivative of x with respect to G„ÄÇ So if we„ÄÇTake a look at this„ÄÇ

And differentiated with respect to G„ÄÇUse the power rule„ÄÇ So this goes front and goes away„ÄÇ

 And this is a constant because it's a partial derivative„ÄÇ H is a constant„ÄÇ So this also goes away„ÄÇ

 So what the result is this to G H„ÄÇSoÔºå now„ÄÇAnd let's take a look at the inner part„ÄÇ

 So the inner part of GÔºå so„ÄÇLet me use this color„ÄÇ So G is 3 x„ÄÇ So we are looking at this one now„ÄÇ

 So the derivative of 3 x is just 3Ôºå rightÔºå So if we derive G with respect to its only input„ÄÇ

 So there is only one inputÔºå then we don't have to treat anything as a constant because there is nothing else except x„ÄÇ

 and the derivative of that is 3„ÄÇNow let's take a look„ÄÇ So we solved basically this in this part„ÄÇ

 Now let's take a look at„ÄÇThe other part hereÔºå the partial derivative of F with respect to H„ÄÇ

 the other argument„ÄÇSo if we treatÔºå So if we compute„ÄÇPartial derivative of F with respect to H„ÄÇ

 And we treat everything that is not H as a constant„ÄÇ So this is a constant„ÄÇ So in this way„ÄÇ

If G squared is a constant„ÄÇThen the derivative of this part with respect to H would be G squared„ÄÇ

 rightÔºå So we have„ÄÇ3 squared„ÄÇAnd this is then oneÔºå rightÔºå the derivative of a variable is just one„ÄÇ

AlrightÔºå so G squared plus1„ÄÇhave it somewhere here„ÄÇRightÔºå so here we have it„ÄÇAnd then the last part„ÄÇ

 the derivative of H its with respect to its input„ÄÇXÔºå so which which color„ÄÇ

Do we have left here this oneÔºå maybe„ÄÇSo let's compute this terminal on the derivative of H with respect to x and„ÄÇ

H is x squaredÔºå that derivative should be„ÄÇ2 x using the power rule„ÄÇ So that's2 x„ÄÇ

 And now we just bring everything together„ÄÇ So we have the first part and the second part„ÄÇ

So I don't have any colour left that I haven't used yet„ÄÇ

 So let me just use black Well this left part here„ÄÇWe bring„ÄÇThose together„ÄÇ

 So this this is basically„ÄÇThis part and then for the right part„ÄÇHereÔºå we bring„ÄÇThose those together„ÄÇ

 and this is this part„ÄÇ And then if we just simplify this„ÄÇ

 So if you take a look at this and you expand these termsÔºå it simplifies as follows„ÄÇ

So this would be how we use the multi variable„ÄÇ

![](img/f2777a0e00499d4d9a6c873abcd3c252_11.png)

Chimru„ÄÇYeahÔºå I don't want to go overboardÔºå but we can also yeah go one step further and write this more compactly using vector notation„ÄÇ

 So again the same function here F with two inputs two input functions G and H which in turn take both x as input So if we compute the derivative this is what we have written down before and we can also write this more compactly using vector notation„ÄÇ

We writing it as gradient of f with respect to its inputs and then times the the derivative of a vector V„ÄÇ

 So the vector vÔºå we define it as a vector containing those two inputs here„ÄÇAnd then„ÄÇ

 the derivative would be„ÄÇThe derivative of the vector would be containing„ÄÇ

Both the derivative of G with respect to x and the derivative of D with respect of H with respect to x„ÄÇ

 So this is more like a compact form of writing down these partial terms„ÄÇ

 So basically this term and this term„ÄÇ So you know the partial termsÔºå but the inner terms„ÄÇ

 So these are„ÄÇButÔºå you know„ÄÇIn other routes„ÄÇAnd now then putting it together„ÄÇWhere this comes from„ÄÇ

 I meanÔºå if you then consider the gradient of the function with respect to its inputs„ÄÇ

 so that's the gradient we discussed„ÄÇSo the partial„ÄÇTermsÔºå and then„ÄÇHere the innerrums„ÄÇ

And if you look at it now„ÄÇ So if you use or compute the dot productÔºå rightÔºå So in orderÔºå yeah„ÄÇ

 in order to compute the dot productÔºå what we have is„ÄÇLet me use this color„ÄÇThis times this„ÄÇPlus„ÄÇ

 this times thisÔºå right„ÄÇ So this is how we usually compute our product„ÄÇ So it expands to this one„ÄÇ

So it's just like a little yeah convenience thing to write things more compact„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_13.png)

YeahÔºå there's one more concept to cover the Jacobovian matrix„ÄÇ

 So consider now a case where F is a vector„ÄÇ So you have a vector F where F is a function„ÄÇ

 and there could be multiple inputsÔºå so„ÄÇLet's consider the expanded form here„ÄÇ So this vector„ÄÇ

 So there's F 1Ôºå F2 F 3Ôºå F MÔºå different functions„ÄÇ Each of them has the inputs X 1Ôºå x2 x 3Ôºå x M„ÄÇ

 So this could beÔºå for exampleÔºå a case forÔºå yeah„ÄÇPerceptron where you have a net input function„ÄÇ

 and it takes multiple features as input„ÄÇSo the Jacobian of this„ÄÇ

This vector here is written down hereÔºå so the Jacobian is a matrix where all the combinations are considered„ÄÇ

 so the partial derivative of f1 with respect to x1„ÄÇ

 the partial derivative of f1 with x2 with respect to x2„ÄÇ

 the partial derivative of F2 with respect to x1 and so forthÔºå Yeah„ÄÇ

 so you can see all the different combinations here„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_15.png)

All rightÔºå I here I see„ÄÇ I have highlighted one thing just yeah to highlight this is the gradient of function 1„ÄÇ

 for exampleÔºå with respect to its inputsÔºå if you recall the gradient that we discussed earlier„ÄÇ

 So each each row of these of this Jacobco matrix would be basically a gradient of a function or a function gradient„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_17.png)

YeahÔºå okay„ÄÇ so second order derivatives„ÄÇ So let's talk about second order derivatives„ÄÇ

 Now I'm just kidding„ÄÇ so we won't be talking about second order derivatives because we won't be needing it for this class„ÄÇ

 ActuallyÔºå second order derivatives can be useful„ÄÇ Some people have done some research working with them in deep learning„ÄÇ

 but so farÔºå it has not been very fruitful„ÄÇ I meanÔºå the resultsÔºå I I mean„ÄÇ

 there are proof of concepts that it conceptual works„ÄÇ

 but it is very slow and there's not really any advantage over using first order methods„ÄÇ so„ÄÇ

Despite some research effortsÔºå as as„ÄÇFor as I know„ÄÇ

 no one is really seriously using second auto gradient methods yetÔºå I mean„ÄÇ

 at least while at that point when I'm recording this„ÄÇ

 maybe in the future there will be a method that is vastly superior compared to our current methods but so far it has not been done very prominently or very I wouldn't say successfully because the research in itself was successful„ÄÇ

 but it's not very practicalÔºå I would say allright so with that let's not worry about second auto methods then let's go back to the more exciting parts through the machine learning and deep learning parts„ÄÇ

 So the next two videos I will be wrapping up on this lecture on gradient descent and then training a single layer neural network„ÄÇ



![](img/f2777a0e00499d4d9a6c873abcd3c252_19.png)

![](img/f2777a0e00499d4d9a6c873abcd3c252_20.png)