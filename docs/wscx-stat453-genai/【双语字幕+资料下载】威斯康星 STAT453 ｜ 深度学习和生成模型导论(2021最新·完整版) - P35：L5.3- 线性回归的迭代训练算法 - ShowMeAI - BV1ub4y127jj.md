# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P35ï¼šL5.3- çº¿æ€§å›å½’çš„è¿­ä»£è®­ç»ƒç®—æ³• - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ so in this videoï¼Œ I want to now talk about this iterative training algorithm that I mentioned in the previous videoã€‚

 so we will be talking about training a linear regression using an iterative algorithmã€‚

 This is also the first part and we will come back to that later on when we talk about gradient descentã€‚

 here I just want to give the general outlineã€‚ So one way we can use an iterative algorithm to fit least square linear regression models is by using brute force So this would be what I call or think of as a very naive way to fit a linear regression model or any type of neural networkã€‚

So what we can do is we can start by initializing parameters to all zerosã€‚

 so the weights and the bias or small random numbersã€‚

 and then we have a follow loop here for K roundsã€‚ So let me use a different color for itã€‚

 So for K roundsï¼Œ we can then choose another random set of weightsã€‚And then yeahã€‚

 we look at the predictions of the linear regression modelã€‚ And now if the model performs betterã€‚

We keep those weightsã€‚And then yeahï¼Œ we go back hereã€‚ So this is like a iteration K timesã€‚

 Every time we find better waysï¼Œ we keep thoseã€‚If the weights are worseã€‚

 as with the model with the weightsã€‚Performs worseã€‚ we actually discard themã€‚

 So we only keep the weights if they are actually better than beforeã€‚ And yeahï¼Œ if you do that manyã€‚

 manyï¼Œ many timesï¼Œ this approach is actually a guaranteed to find the optimal solution rightã€‚

 because yeahï¼Œ you do this manyï¼Œ manyï¼Œ many timesã€‚ And if the model can only improveã€‚

 eventually just by luckï¼Œ you will find yeah great set of weightsï¼Œ the optimal weightsã€‚

 But as you can think or as you can imagineï¼Œ this would be a veryã€‚

 very inefficient and terribly slow way of fitting a linear regression modelã€‚

 So I would not recommend doing this in practiceï¼Œ although in practice it worksã€‚

 it's it's not a good way to fit a linear regression modelã€‚Yesï¼Œ so luckilyã€‚

 there is a better way for fitting a linear regression model iterativelyã€‚

 So what we can do is we can analyze what effect of change a parameter has on the predictive performance of the model so we can take a look at the square  error loss and see when we change the weight and the bias in a certain way how it affects these  errorss and then we can make a small change so we can change the weight and bias a little bit in the direction that improves the performanceã€‚

 So if we understand the relationship between the weight and the loss we can change the weight such that the loss goes down so we have a smaller error and we can then do this several times in small steps until the loss is not further decreasingã€‚

 So one by one we will change the weight such that loss decreasesã€‚

It turns out that this is actually the the online mode that we talked aboutã€‚ yeahã€‚

 just written a little bit differentlyã€‚ So on the left hand sideï¼Œ we haveï¼Œ againã€‚

 the online mode for the perceptron learning ruleã€‚So where we iterate over the training epochsã€‚

 and then for every training exampleï¼Œ we compute the predictionï¼Œ compute the errorã€‚

 and then do the update hereã€‚Soã€‚For linear regressionã€‚

 there is an algorithm called stochastic gradient descentã€‚

 So stochastic gradient descent is the analog of the perceptron learning rule for convex loss functions hereã€‚

 we can also use it for non convex loss functions for neural networks that will be something we will cover in the next lectureã€‚

 So here we will focus on the linear regression model and then at the end of this lecture also at on the adaptive linear neuronã€‚

 So what is similar and what is differentã€‚ So again so we have the same weight initialization hereã€‚

Let me use a different color for things that are the sameã€‚Weight initialization is the sameã€‚

 we also iterate over the training epochsï¼Œ then we also iterate over the training examples in a data setã€‚

 we also compute the predictions the same way because here like I mentioned in the previous videoã€‚

 the perceptron and the linear regression model they both compute the net input here except here we have the threshold function and the perceptron and let me use right here that would be the threshold functionã€‚

 the case of the perceptron and here this would be an identity functionã€‚And this is the thresholdã€‚

And yet inside hereï¼Œ this is the net input this isã€‚Oopsã€‚å¯¹ã€‚

I don't know what it is with curly bracketsï¼Œ but Iã€‚Can't draw cur bracketsã€‚ Okayï¼Œ so yeahï¼Œ Zã€‚

 the net inputs on they are computed exactly the same wayã€‚

 This function is a identity function in the case of linear regression and a threshold function in the case of the perceptronã€‚

Nowã€‚For the perception oneï¼Œ we compute thisï¼Œ this error here by just subtracting the predicted value from the actual class table in linear regressionã€‚

 We have continuous valuesï¼Œ but it's very similarã€‚ So also hereã€‚

 we have a subtraction for the weightsã€‚ we multiply this by xã€‚By the input feature vectorã€‚

 and for the bias unitï¼Œ we don'tã€‚ So if you're wondering what these symbols here meanã€‚

 So this labla symbolï¼Œ this stands for the gradientã€‚

So here we are computing the gradient of the loss the gradient of the loss Lã€‚

 is cur L L here with respect to the weights hereã€‚ So this is theã€‚Gadientã€‚Offã€‚

 when let me spell this outã€‚Grariientã€‚offã€‚The lossã€‚Functionã€‚Withã€‚Respectã€‚Toã€‚ğŸ˜”ï¼ŒWhoopsï¼Œ weightsã€‚

And thenã€‚Here at the bottomï¼Œ this is the sayï¼Œ also also the gradient of the loss with respect to the bias unit here or derivativeã€‚

 soã€‚Yeahï¼Œ so you can see this is closely related to what's going on in the perceptionceptronã€‚

And in step Cï¼Œ nowï¼Œ we do the updateã€‚So the update is a little bit different hereã€‚On theã€‚

Left hand side hereï¼Œ we have the current weightã€‚And then we multiply the error with a feature vectorã€‚

We don't do this on the right hand sideã€‚ We have done something like multiplying with the feature vector up hereã€‚

 But here now what we do is we we addã€‚Another term hereã€‚ and this term Eta is a scalarã€‚

 It's a we are scaling somethingï¼Œ and this is the so called learning rateã€‚ So let's call thatã€‚

The learningã€‚It's just a scaling term and in the parentheses hereï¼Œ this is the negative gradientã€‚

 so I was wrote it down hereï¼Œ so we have the negative gradientã€‚

 We now update the parameters by the negative gradientã€‚

So it's kind of similar to what's going on in the perceptronã€‚

 except that the way we get this gradient is based on calculusã€‚ and in the next couple of villageã€‚

 I will yeah give you a brief calculus refresher to explain where this comes fromã€‚

 So here we are really computing the negative gradient of the squared error loss with respect to the weights and the bias and in the perceptron case this is it looks very similar it's a little bit different because it's not depending on calculus here we have the threshold function and depending on how fresh your calculus skills areã€‚

 you know that for non-smooth functions like threshold functions we can't compute derivativesã€‚

 So in that wayã€‚It's somewhat related and very similarï¼Œ but there's a littleï¼Œ little detailã€‚Okayã€‚

 so yeahï¼Œ one more slide I just see I have hereã€‚ So on the left hand sideã€‚

 we or I showed you vectorized implementation of this linear regression online modeã€‚

So where we had vectorï¼Œ so x here is a vector for exampleï¼Œ and the gradient is a vectorã€‚

 but we can also kind of yeah unroll this using a for loopï¼Œ so this way we don't yeah need gradientsã€‚

 we can just talk about partial derivativesã€‚So in order to simplify this as a fall loopã€‚

 we would just look at oneã€‚Feature at a time or one wait at a timeã€‚So if we have a data setã€‚

Where our dimensionality of the inputs is Mã€‚ So we have M featuresã€‚ What we can do is then for eachã€‚

Weï¼Œ so the number of weights is equal to the featuresï¼Œ rightï¼Œ Because if I have my inputs X 1ï¼Œ x 2ã€‚

 x 3ï¼Œ then here they go into myã€‚That input function in each input is associated with a weightï¼Œ rightã€‚

So I'm not just drawing the bias hereï¼Œ but yeah each input is associated with the weightã€‚

 so we have M weightsï¼Œ and I can for each weight separately compute the partial derivative of the loss with respect to that weightã€‚

So it's an analogec of this one hereã€‚Not using any linear algebra now just using partial derivatives instead of gradients and then I can do also the update the same wayã€‚

 So here this is just the for loop version I think yeah this is maybe conceptually a bit easier because partial derivatives are maybe easier to think about as compared to gradients I mean it's the same thing but instead of yeah talking about gradients we could also talk about partial derivativesã€‚

 however like I explained in the previous lecture vectorized implementations are faster which is why we usually use gradients and vector based implementationsã€‚

So the learning rule from the previous slide now is called stochastic gradientdescent I was showing you this learning ruleã€‚

 but I didn't show you where this learning rule came from So where how did I derive this learning rule So in order to understand that yeah there's some little cculus required I think most of you already took cculus classes because that was the prerequisite for this class if you a little bit rusty I have two bonus videos I will be recording after this video so you canã€‚

Maybe you refresh your calculus skillsï¼Œ but it's not necessaryã€‚

 so you can also jump ahead to this video where I will explain where the learning rule comes from and if you like you can watch these two videos where I go over some of the calculus conceptsã€‚



![](img/187c74e4ca8bb2d2bb568f7d8632b615_1.png)

![](img/187c74e4ca8bb2d2bb568f7d8632b615_2.png)