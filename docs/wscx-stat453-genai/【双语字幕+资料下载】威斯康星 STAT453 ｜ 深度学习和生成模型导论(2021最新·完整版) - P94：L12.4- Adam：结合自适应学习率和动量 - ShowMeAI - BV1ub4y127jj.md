# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P94ï¼šL12.4- Adamï¼šç»“åˆè‡ªé€‚åº”å­¦ä¹ ç‡å’ŒåŠ¨é‡ - ShowMeAI - BV1ub4y127jj

Yesï¼Œ so in the previous video we talked about adding this momentum termã€‚

 which is essentially a velocity term that helps dampening the oscillations in stochastic gradient descentã€‚

 but it also can help with overcoming like flat regions on the loss surfaceï¼Œ for instanceã€‚

 saddle points or local minimaã€‚So in addition to this momentum term we learn now about a slightly related also slightly different concept called adaptive learning ratesã€‚

 so adaptive learning rates are essentially about accelerating and deaccerating the learning rate at the right momentã€‚

 so speeding the learning up when we are going into the right direction and slowing it down when we change directionsã€‚

And then we will see how we can combine this with momentum learning and the combination of both there's an algorithm that does that it's called Adamã€‚

 so Adam is essentially a combination of adaptive learning and momentum learningã€‚



![](img/8a480effebe949863a966439a53a0acf_1.png)

Yeah there are many different flavors of adaptive learning rates out there and discussing them all would be a little bit of out of the scope of this courseã€‚

 however yeah just to go over the main concepts I will show you a simple example and yeah the key takeaway like I said in the previous slide is that we decrease the learning rate if the gradient changes the direction and we increase the learning rate if the gradient stays consistentã€‚

Soï¼Œ for instanceï¼Œ if weã€‚Do these updatesã€‚ and they are all going to the sameï¼Œ roughly same directionã€‚

 Then we accelerate becauseï¼Œ yeahï¼Œ in this wayï¼Œ it's probably likely the correct directionã€‚

 So in that wayï¼Œ we can just speed it up and converge fasterã€‚Howeverã€‚

 if we have something where we change the directionï¼Œ for exampleã€‚

 So let's say we have an update like that and there's another changeã€‚

 then it will slow down the update so that it isï¼Œ if it's a wrong directionã€‚

 there's maybe some noise so that it is not heading too much into this wrong directionï¼Œ for exampleã€‚

 So if every time there's a change in directionï¼Œ it will essentially slow downã€‚



![](img/8a480effebe949863a966439a53a0acf_3.png)

Yeahï¼Œ so how does it workï¼Œ How can we use an adaptive learning rate There are essentially two main steps for doing thatã€‚

 so step one would be initializing a so called local gain for each weight and if you look at thisã€‚

Hereï¼Œ when we compute the Dlta WI J term andã€‚If you onlyï¼Œ if youã€‚Remove this oneã€‚

 This would essentially look like the regularã€‚Delta term that we computeã€‚

 So this is essentially our gradientã€‚Of the loss with respect to that weight that we want to update times the learning rateã€‚

 So this is the learning rateã€‚But the difference now is that we add this gain and the skeã€‚

As you can see it's also index IJï¼Œ so it's the gain associated with that particular weight and in that way you can think of this one as a learning rate just for this particular weight so you can have unique learning rates for different weights or different directions in that wayã€‚



![](img/8a480effebe949863a966439a53a0acf_5.png)

So now after initializing this local gainã€‚We have a second stepã€‚

 so this is what we then do during learningã€‚ we are modifying the gainã€‚

 so similar to modifying the weightsï¼Œ we modify the scanã€‚

 so we either increase it if it's consistent or otherwise if it's inconsistent so if we change the directionã€‚

 then we decrease or yeah we yeah we change the dampen the gain essentiallyã€‚So like I'm saying hereã€‚

 multiplying by a factor has a larger impact if the gains are large compared to adding a term addition to in contrast to this editionã€‚

Andã€‚That meansã€‚ so how this is set upï¼Œ it means that if we are going into the right directionã€‚

 we are slowlyã€‚Acceleratingã€‚ Soï¼Œ so this is if we are consistent and we are adding this beta termã€‚

 and we make it aã€‚Bigger by a small value here by betaï¼Œ let's say beta is 0ã€‚9ã€‚ We increase by 0ã€‚9ã€‚

In eachã€‚Roundï¼Œ essentiallyã€‚But if we change the directionï¼Œ we slow down faster by multiplyingã€‚

By this term hereï¼Œ for instanceã€‚ So hereï¼Œ for instanceï¼Œ we will multiply a beta is ã€‚9ã€‚

 We will multiply this by 0ã€‚1ã€‚ And this can have a yes a stronger dampening effect when we change the directionsã€‚

 This is kind of intuitiveï¼Œ becauseï¼Œ yeahã€‚If weï¼Œ let's say drive a car and we see the road is clear aheadã€‚

 we start to accelerateï¼Œ but we don't go crazy and stomp on the gasã€‚

 we start accelerating in a reasonable mannerã€‚And let's say we want to make a turn or something like thatã€‚

 or we are in the turnï¼Œ usually we accelerate a lot before a turnã€‚

 we don't slowly accelerate in most casesï¼Œ at least I meanï¼Œ if it's a very sudden turnã€‚

 if we have to avoid an obstacle or something like thatã€‚

 we have to push the brakes pretty hard to turn left or right to avoid the obstacleã€‚



![](img/8a480effebe949863a966439a53a0acf_7.png)

Alrightï¼Œ so there is a particular implementation of an adaptive learning rate that was very popular for yeahã€‚

 for a timeã€‚ it is called RMS Proã€‚ And there I think it's from a cost Jo H taughtã€‚

 you can find references to that on the Internetï¼Œ but there is no official paper of that or corresponding to thatã€‚

 It's something Jo Hintonã€‚Who was one of the persons popularizing and working on yeah neural networks throughoutã€‚

 yeahï¼Œ the last 60 or so yearsï¼Œ 50 yearsã€‚Yeahï¼Œ it has been mentioned in one of his talks and people started using that and it performed pretty wellã€‚

 it's essentially a modification of Rropï¼Œ which was published in 2000 around 2000ã€‚And yeahã€‚

 it's also relatively similar to another concept called Ada Deltaã€‚

So the main idea behind this arm is prop by mentioning it because it will be relevant when we talk about the atom algorithmã€‚

So the main concept is we divide the learning rate by an exponentially decreasing moving average of the squared gradientsã€‚

 So it's essentially a modified version of this adaptive learning rate that I showed you on the previous slideã€‚

 So the previous slide was a very simple type of adaptive learning rateã€‚

 This is a little bit more sophisticatedã€‚Because yeahã€‚

 it takes also into account that gradients can vary widely in the magnitudeã€‚

 so some weights or gradients are larger than othersã€‚

And the arm S and arm S Pro stands for root means squaredï¼Œca it'sï¼Œ yeah related to theseã€‚

Squared gradientsã€‚And it also has an effectï¼Œ this dampening effectã€‚

 in addition to the adaptive learningã€‚ it also has this a dampening effect of theã€‚

A momentum on the oscillationsã€‚But in practiceï¼Œ yeahã€‚

 people thought it might work better than using momentumï¼Œ so this arm S propã€‚

People found works better than just momentumã€‚ but yeahï¼Œ nowadaysã€‚

 people use the combination of atom or people use atomã€‚

 which is a combination of arm S prop and momentumã€‚



![](img/8a480effebe949863a966439a53a0acf_9.png)

Okayï¼Œ but let's talk about this arm S prop first before we go into the Adamom algorithmã€‚

So there is now a mean square term hereã€‚ That is the moving average of the squared gradient of each weightã€‚

 It's kind of in a wayï¼Œ similar to momentumã€‚ But we have nowã€‚

 instead of just considering the moving averageï¼Œ we have the moving averageã€‚

Of these squared gradientsã€‚There's a bitter termã€‚So T isï¼Œ againï¼Œ the time stepã€‚For instanceã€‚

 the iterationã€‚And WI J is the weight we want to updateã€‚So we have the mean squared gradientã€‚

Of that weight at a given time stepã€‚ And this is computed by multiplying a beta withã€‚

The mean square value from the previous iterationã€‚Soï¼Œ and then on top of thatã€‚

 we add these squared gradients for the current iterationã€‚ So this isã€‚The mean term hereã€‚Rightã€‚

 so this is something we keep from the previous roundsã€‚

 And this is only for the current round for time step Tã€‚

 So it's essentially the gradient of the loss or partial derivativeertive of the loss with respect to the weight should be aã€‚

Pel hereã€‚Andã€‚We square those essentiallyï¼Œ so they are always positive valuesã€‚Yeahã€‚

 and then this is how we compute the mean square valueã€‚

 and then we use that mean square value to scaleã€‚The gradients at that updateã€‚

 So here we are just computingã€‚ This means square termã€‚ And here we are applying itã€‚

 How do we apply thisï¼Œ So if you look at this againï¼Œ this is like theã€‚Regularã€‚Great incent updateã€‚

 rightï¼Œ We take aã€‚Step into the directionï¼Œ the negative direction of the gradientã€‚

 So we subtract the gradient times the learning rateã€‚But nowï¼Œ in addition to doing thatã€‚

 we scale by this mean square termã€‚And we take the square rootã€‚

 so it has the same unit as the weightsã€‚So and this is essentially it it's very similar to what I showed you before the adaptive learning rate where we have a gain that we modify except that this is a certain type of gain that is working a little bit differentlyã€‚

And yetï¼Œ we have the small epsilon to avoid division by zero  errorssã€‚ In a wayã€‚

 it's also somewhat similar to the momentum termã€‚ So when we look at atomã€‚

 we will see there are two thingsï¼Œ the momentum term in this termã€‚

 and they are actually also themselves very similarã€‚Except this is like a scaling factor in a wayã€‚

 and the momentum is something that we addã€‚

![](img/8a480effebe949863a966439a53a0acf_11.png)

On topã€‚So that's then now talk about this Adam because I mentioned it so many times it stands for adaptive moment estimation Yeah and it's probably the most widely used optimization algorithm in deep learning as of todayã€‚

 I use it a lot because I find it just works very well out of the box and I always find I get almost always the same or even better performance as with SGD plus momentum and learning rate schedulers but I need way less tuningã€‚

I meanï¼Œ hereï¼Œ I mean tuning of the learning rate and the strength of the schedulerã€‚So like I saidã€‚

 it's a combination of momentum and RMS S prop and to yeahï¼Œ to make this a little bit easier to readã€‚

I was so also should say if you want to read the original paperï¼Œ which contains way more detailã€‚

 this is the paper where it was proposed from 2014ã€‚ So here you can find also more informationã€‚

 So here I'm rewriting this slightly to make this a little bitã€‚

 I would say easier to see how it is related to momentumã€‚Soï¼Œ hereã€‚On on topã€‚

 this is the original definition of the momentum that we defined inã€‚TheLast videoã€‚

And I'm just replacing this delta W by this m Tã€‚ So I'm just using a different notation hereã€‚

 I'm just changing the notationï¼Œ I'm not changing any conceptã€‚ I'm just sayingï¼Œ okayã€‚

 let's call this one now at time step T so the momentum term M for a momentum at time step T and here the this is the current time step and this is the time step for the next round essentiallyã€‚

So here this is the rewritten version of thatã€‚But you can see it's slightly different nowã€‚

 because we haveã€‚This modified versionï¼Œ where we haveã€‚Alphaã€‚

 but instead of using the learning rate hereï¼Œ we use  one minus alphaã€‚

 So it's not exactly like the same like the original momentum termã€‚ it's slightly differentã€‚

 So we have this one minus alphaã€‚Instead of the atta and the original momentumã€‚

 But you can see how similar it isï¼Œ rightï¼Œ So we have both on alpha times Mï¼Œ which we have hereã€‚

 and we have plusã€‚Here we have the gradientï¼Œ the same thingï¼Œ except againã€‚

 the only difference here we have the learning rate and here we have one minus alphaã€‚



![](img/8a480effebe949863a966439a53a0acf_13.png)

Okayï¼Œ so here I was just rewriting the momentum term from the previous slide exactly the same thingã€‚

 just carrying it over and now we have this arm S prop term that I showed you a few slides agoã€‚

 so here we had the better term for multiplying it with a mean square I'm just abbreviating here as Rã€‚

And then we have1 minus better times the squared gradientsã€‚

 So this is essentially the same I also showed you beforeã€‚ Nowï¼Œ I'm just using Rã€‚ So it's shorterã€‚

 And then for atomï¼Œ we combine bothã€‚ we have momentumã€‚

So we have the learning rate times the momentum termã€‚ It's essentially a velocityã€‚

 and we scale that momentum term by thisã€‚R S prop termã€‚ So squared with R plus this epsilonã€‚

And this is essentially how Adamom worksã€‚ So we haveã€‚The momentumã€‚Term and hereã€‚Yã€‚ğŸ˜”ï¼ŒAsã€‚Chã€‚



![](img/8a480effebe949863a966439a53a0acf_15.png)

Okay onã€‚There is a slightã€‚I would say modification of that in the paperã€‚

 So here that's the original paperã€‚ how it is defined in the paperã€‚

 And what I haven't contained or included in the slides for simplicity was this bias corrected first moment estimateã€‚

 and the bias corrected second raw moment estimateã€‚ So here the difference is essentially thatã€‚

I should also sayï¼Œ I'm calling it alpha and betaã€‚

![](img/8a480effebe949863a966439a53a0acf_17.png)

So here they have be 1 and be 2ã€‚ So if we go backï¼Œ I call that alpha just because I wasã€‚

 I don't knowã€‚ I found it simplerï¼Œ but this is essentially better  oneã€‚And hereï¼Œ this isã€‚Better tooã€‚

In the paperï¼Œ I just found it somehow inin intuitivetu to say alpha and better because wehausa used alpha before when we talked about momentumã€‚



![](img/8a480effebe949863a966439a53a0acf_19.png)

So yeahï¼Œ they just correct this term by havingã€‚Another scaling by so they scale the momentum to by dividing by1 minus beta to the power of tã€‚

 where t is the time step essentiallyï¼Œ and they do the same thing for this V termã€‚



![](img/8a480effebe949863a966439a53a0acf_21.png)

Yeahï¼Œ and this is essentially itï¼Œ so in the next video I will show you how we can use these optimization algorithms in Pytorchã€‚

ğŸ˜Šã€‚

![](img/8a480effebe949863a966439a53a0acf_23.png)