# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P33ÔºöL5.1- Âú®Á∫ø„ÄÅÊâπÂ§ÑÁêÜÂíåÂ∞èÊâπÈáèÊ®°Âºè - ShowMeAI - BV1ub4y127jj

Yes so in this video I want to talk about the different ways we can train a neural network„ÄÇ

 so there's the online modeÔºå the batch mode and the mini batch mode and they refer essentially to how we utilize our data when we train a neural network model so basically how in which water we read the data regarding the training epochs and also how we update our model So these are general concepts that apply to any type of neural network So just to briefly recap our perceptron model here„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_1.png)

So here on the left hand sideÔºå this is just again a sketch for our perceptionceptron where we compute the net inputs and then we update the weights„ÄÇ

So hereÔºå I mainly want to highlight what's going on again under the hood so if we have a data set„ÄÇ

 let's call the data D„ÄÇWithin labels„ÄÇAnd sorryÔºå with end training examples„ÄÇ

But also in labeled where each training example has a feature vector„ÄÇX and class table Y„ÄÇ

So when we start training on this data setÔºå what we do is we initialize the parameters of the perceptron„ÄÇ

SoÔºå for exampleÔºå we can set the weights to all zeros„ÄÇAnd we can set the bias unit also to a 0„ÄÇ

So this is nothing new„ÄÇ This is just like a recap„ÄÇ And then for every training epochÔºå remember„ÄÇ

 a training epoch is„ÄÇA one cycle„ÄÇOver„ÄÇThe training setÔºå as I explainedÔºå I think it was last week„ÄÇ

trainingining set„ÄÇ But if you have questionsÔºå always„ÄÇ

 please free of feel free to ask on Piazza I'm always happy to yeah repeat these concepts because yeah„ÄÇ

 they are quite important„ÄÇ So please don't hesitate if you have any question about what a training apo is to ask because that's kind of important to make sure everyone yeah knows what we are talking about here„ÄÇ

üòäÔºåSo so for every training epochÔºå there could be multiple ones we can have an arbitrary number of training epochs in practice it really depends on the data set and deep learning later on„ÄÇ

 it's not uncommon to train something for 100 or thousand0 epochs even it really depends on how difficult a problem is„ÄÇ

And so for every training hypothesis or for loopÔºå then we„ÄÇ

Go through each training example one at a time„ÄÇ so there two follow loops„ÄÇ

 one is over the number of training epochs and one is over the data points in each round„ÄÇ

Maybe just to„ÄÇDraw this again„ÄÇ So if this is my in pinkÔºå if this is my data set a D„ÄÇ

I have these different data points here„ÄÇAnd this inner for loop here„ÄÇ let me use a different color„ÄÇ

 maybe green„ÄÇSo this follow up hereÔºå what it is really doing is it's just going from one training point to the next one„ÄÇ

 So one at a time„ÄÇThat's what's going on here„ÄÇ and the outer oneÔºå the training apoch1„ÄÇ

 once we completed going through the data set„ÄÇWe start at at the beginning again and go through the dataset again„ÄÇ

So that's really what's going on here„ÄÇ So for each data point in the perceptionron what we are doing is we are computing the prediction„ÄÇ

 So my head it is our prediction while yeah the net input„ÄÇ

 and then we have this threshold function or activation function„ÄÇ

 which in this case is the threshold function„ÄÇThen we computed the error„ÄÇ and then based on that„ÄÇ

 we update„ÄÇ So we update here the weight vectorÔºå and we also update the bias vector„ÄÇ

 So that are two updates„ÄÇ Notice that the updates happen„ÄÇAfter each training data pointÔºå so„ÄÇAgain„ÄÇ

 if I have here in greenÔºå if I go to the first example„ÄÇBased on thatÔºå I compute Y hat„ÄÇ

Compute the  error„ÄÇAnd then„ÄÇUpdate„ÄÇWÔºå the weight vector and the bias unit„ÄÇIf I have done that„ÄÇ

 I will go to the second training example and do the same thing again„ÄÇ

 so I update after each data point„ÄÇ

![](img/351cb4b77800bdb89f1639a447ab466e_3.png)

So„ÄÇAnd this is called the„ÄÇOnline mode„ÄÇ That is a little bit confusing„ÄÇ It's justÔºå I think„ÄÇ

 a historical thing that it was called online mode„ÄÇ Notice often textbooks Theres a„ÄÇ

Usually a dash between the on and the line„ÄÇ This is like old machine learning textbooks„ÄÇ

 So I think that's probably actually related to the worldwide web„ÄÇ I'm not quite sure„ÄÇ

 but I can imagine it is like when the internet was new„ÄÇ

 you collected it one data point at a time and the advantage is really if you have a model with online training„ÄÇ

 you can update after each data point„ÄÇ So if you have a model on a web application for each new customer„ÄÇ

 you can update the model„ÄÇ You don't have to collect multiple customersÔºå for example„ÄÇ

 to make an update„ÄÇ So usually after each data pointÔºå you can make an update if you want to„ÄÇYeah„ÄÇ

 so this is just a summary of what I had on the previous slide without any mathematical symbols„ÄÇ

 just basically for each training apochÔºå we iterate through the data pointsÔºå compute the output„ÄÇ

 calculate the error and update the weights and the bias„ÄÇ

 So this is the general concept that applies to any kind kind of neural network„ÄÇ

 not only the perceptron„ÄÇ We have seen that in the context of the perceptron„ÄÇ

 but this is something you can use also„ÄÇAs we will see later for linear regression„ÄÇ

 logistic regressionÔºå multi ofceptronsÔºå convolutional networksÔºå and so forth„ÄÇÂóØ„ÄÇAnd„ÄÇ

There are then some variant of that„ÄÇ One is called the batch mode mode and one is the mini batch mode„ÄÇ

 which are essentially modifications of that„ÄÇ and they have also certain advantages which we will go over in the next couple of slides„ÄÇ

 So this is one way you can utilize your dataÔºå but there are two other ways„ÄÇ

 one is called the batch mode and the other one is called the mini batch mode just a little spoiler here in deep learning„ÄÇ

 actuallyÔºå the mini batch mode is the most common one„ÄÇ

 and I will also mention briefly then later why that is„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_5.png)

So the batch modeÔºå let's take a look at the batch mode„ÄÇ NoÔºå next to the online mode„ÄÇ

 So the difference here really is that„ÄÇWe collect„ÄÇHow we would update the model before we actually update it after each data point„ÄÇ

 So we still use the same number of data points in a batch mode„ÄÇ

 But instead of updating after each data pointÔºå we„ÄÇCollect some information first„ÄÇ

 and then we do the update„ÄÇ So the update is only really after each epoch„ÄÇ So again„ÄÇ

 let's take a look at the online mode on the left hand side„ÄÇ So for each training epoch„ÄÇ

 we go over the data points and then can see here inside each after each data point we update on the right hand side„ÄÇ

 I have the batch mode„ÄÇ It's a little bit different„ÄÇ

 We still have a full loop over the training epochs„ÄÇHoweverÔºå the difference now is that we„ÄÇ

Let's say initialize this placeholder here„ÄÇ it's I call it deelta that for change„ÄÇ

 So this is some placeholder hereÔºå which I have for the weights and for the bias„ÄÇ's let's call that„ÄÇ

A placeholder„ÄÇThis is also a placeholder„ÄÇAnd then like on the left hand side here„ÄÇ

I am iterating through the data points„ÄÇ AlsoÔºå the same thing here„ÄÇ

 I am computing the output or the prediction„ÄÇ same thing as on in the online mode„ÄÇ Also„ÄÇ

 I calculate the errorÔºå same thing here„ÄÇ So that's all the same„ÄÇ However„ÄÇ

 the difference really is here„ÄÇIn this stepÔºå so you can see here I'm updating the weight and the bias directly here I'm only updating my placeholders„ÄÇ

 so I'm just accumulating some error terms or some information and I keep yeah I keep accumulating that„ÄÇ

Until I itated over every data point in the data„ÄÇAnd then based on that„ÄÇ

 after I finished iterating through the current dataset set here„ÄÇ

Then I update the weight vector on the bias here„ÄÇ So you can see I'm now using my placeholder„ÄÇ

Here and hereÔºå to update these on„ÄÇPermetertersÔºå I just see there a missing letter„ÄÇ

 It should be be up front here„ÄÇ So I'm updating both„ÄÇThe weight and the bias using the placeholders„ÄÇ

 which have some accumulated information here„ÄÇ So what is the advantage of doing„ÄÇ

 let's say online and batch mode„ÄÇ So online mode is„ÄÇAn update„ÄÇ YeahÔºå one update at a time„ÄÇ

 so it can be a little bit noisier„ÄÇ The batch mode is a little bit moreÔºå I would sayÔºå robust„ÄÇ

 So you collect statistics or information from the whole data set before you do one update„ÄÇ

 The disadvantage is it's a slower mode of updating because you do only one update for each training epoch„ÄÇ

 And on the left hand sideÔºå you do multiple updatesÔºå each epoch„ÄÇ So on the left hand side„ÄÇ

 you can think of it as maybe as a faster learning because there are more updates on the right hand side„ÄÇ

They have fewer updatesÔºå each update takes longer because you have to cycle through the whole data set before you make an update„ÄÇ

 but the update will be more accurate„ÄÇI will also show you some visualizations of that how that works really in the context of„ÄÇ

 let's sayÔºå linear regression or real real case scenario„ÄÇThere's noÔºå I would say„ÄÇ

 I would say neither of the two are very common in deep learningÔºå though„ÄÇ

 and I will also tell you why later on it will become obviousÔºå I guess„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_7.png)

In practiceÔºå there's something called a mini batch modeÔºå which is the most common one„ÄÇ

I had one more note here about the online mode I just see yeah„ÄÇ

 I highlighted it here in practice we usually also shuffle the data set prior to each epoch to prevent cycles„ÄÇ

So sometimes it can happen thatÔºå for exampleÔºå in the perceptionceptron case or some other case that the yeah algorithm gets stuck flipping back and forth„ÄÇ

 So empiricallyÔºå it's actually better to shuffle the data set prior to each epoch„ÄÇ

 You can also think of it if you haveÔºå for exampleÔºå the iris data set„ÄÇIf you just load it„ÄÇ

 the classes will be sorted by class labelÔºå so you have 50 examples from class 0„ÄÇ

 50 examples from class 1 and then class 2Ôºå and it is also not ideal because let's say„ÄÇ

This is my Irish dataset set„ÄÇAll these are class table 0„ÄÇ and then you have class table 1„ÄÇ

 and then class table2„ÄÇ So if you don't shuffleÔºå the problem is the algorithm will only update based on„ÄÇ

 on the zeros„ÄÇ Maybe the zeros are easy„ÄÇSo there's nothing happening and then it sees a completely new class„ÄÇ

 and then it only focuses on getting these rightÔºå but then maybe it's getting some other ones wrong again and it's usually better to have more a better yeah shuffling a little bit more diversity that you have zeros ones and twos like intermixed with each other and also yeah to prevent flipping back and forth„ÄÇ

 it's just good to shuffle it shuffle doesn't cost much„ÄÇ

 So in that way it's it's not a bad thing to do anyways„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_9.png)

Well I should say the online mode is sometimes also called„ÄÇstochastic mode„ÄÇ

 or in in the context of gradient descentÔºå which we will cover later in this course„ÄÇ

 it is also called stochastic gradient descent„ÄÇThere's another online mode let's call that online mode version2 that is also quite common or used to be quite common„ÄÇ

 So here the difference is really that you don't iterate over the epochs„ÄÇ

 you just say for a given number of iterations„ÄÇYou do the updates„ÄÇ

 so for a given number of iterations„ÄÇ you say pick at random some data point from the data set and then you proceed with computing the prediction„ÄÇ

 the error and the update„ÄÇ So here really the difference you don't iterate over the epochs„ÄÇ

 The difference also then here is on the left hand side you guarantee that each data point is used exactly once in each round whereas on the right- hand side a data point can be used multiple times and the same number of iterations„ÄÇ

 So for exampleÔºå if you have a data set of size N and you have n iterations„ÄÇ

 it can be that some data points are used multiple times whereas certain other data points are not used at all on the right hand side„ÄÇ

 this is usually what people call the true stochastic mode„ÄÇ

This is something you find sometimes in statistics papers because it's easier with that one to yeah to derive certain proofs like theoretical proofs„ÄÇ

 it's mathematically a little bit easier to handle than the left oneÔºå but in practice so„ÄÇIn practice„ÄÇ

 this is usually used more often because you get better results with that one because you guarantee that each data point is used in the data and each data point is used the same number of times it's kind of more making better use of the data in that way„ÄÇ

AlsoÔºå yeahÔºå this is also not truly stochasticÔºå I would say because in order for it to be truly stochastic you would be sampling from the distribution here„ÄÇ

 we still have a a fixed size training setÔºå but this is just yeah knitt picking actually„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_11.png)

Now let's get to the most common mode of learningÔºå so this is the mini batch mode„ÄÇ

 which is a mix between the online mode and the batch mode„ÄÇ

 so similar to the previous two modes what we do is we initialize our parameters that we want to update the weights and the bias and then we also iterate over the training epochs but now what's new is we also iterate over mini batches„ÄÇ

So what is a mini batchÔºå A mini batch is„ÄÇA chunk of the data sets„ÄÇ So if this is my„ÄÇTining„ÄÇSet„ÄÇ

 you can think of a mini batch as„ÄÇYeahÔºå a smaller chunk of the training set„ÄÇ

 So this would be mini batch„ÄÇ1Ôºå mini batch 2„ÄÇ3„ÄÇFor„ÄÇSo what is the size of mini beds„ÄÇ

 usually it really depends usually common ones you will find in deep learning something like 32Ôºå64„ÄÇ

1 at 28256Ôºå sometimes also 512„ÄÇ It really depends on yeahÔºå also the problem„ÄÇ

 It's like a hyperparmeter or something you have to try out and see what works„ÄÇ

 So there are some papers suggesting smaller batch sizes can accelerate learning because it's noisier and the noise can help making updates a little bit faster So you have more iterations larger batch size use or make better use of the GPU„ÄÇ

But yeah it's really it's really something that depends on practice and we will also talk more about that when we talk about the technique later on called batch normalization In any case„ÄÇ

 so this mini batch here it's then essentially used like batch gradient descent„ÄÇ

 everything down here„ÄÇ‰∏ç isÂÆû„ÄÇSimilar‰∫Ü„ÄÇTo the batch„ÄÇMoode„ÄÇ

 except that you perform the batch mode on this smaller chunknelÔºå so„ÄÇ

Before the batch mode was done on this whole training set„ÄÇ

 now the batch is done at on one chunk at a time„ÄÇ So you again initialize these placeholders and then for each data point in the mini batch„ÄÇ

 you compute the predictionÔºå calculate the errorÔºå update the placeholderÔºå and then„ÄÇYeah„ÄÇ

 then you update the weight and bias unit here„ÄÇ So this wayÔºå it would be similar to the batch mode„ÄÇ

 except that you use one smaller mini batch at a time and„ÄÇ

This is usually the most common mode in deep learning„ÄÇ So as a question for you„ÄÇ

 do you have any ideas why that is„ÄÇ

![](img/351cb4b77800bdb89f1639a447ab466e_13.png)

YeahÔºå maybe pause the video for a second and think about it and see if you have some ideas before I read out note my notes you're on the right hand side„ÄÇ

 So yeahÔºå I've written down three of the reasons why I think that mini batch is much more useful than„ÄÇ

 let's say the online mode and the batch mode in deep learning„ÄÇ

 So one would mean that when you are choosing such a subsetÔºå such a mini batch„ÄÇ

Instead of just one example at a timeÔºå we can take advantage of the vectorization that we talked about in the previous lecture„ÄÇ

 So in the previous lectureÔºå we had seen that we can compute things efficiently„ÄÇSo when we had„ÄÇ

 for example„ÄÇA training matrix containing multiple entry entries multiplied by the weight vector„ÄÇ

 So if we had thatÔºå we could process multiple training examples at a time„ÄÇ

 which is then faster than in a fall loop executing this multiplication with a vector here„ÄÇ

 So instead of using a vector times this a weight matrix and doing this multiple times with a fall loop is yeah„ÄÇ

 of courseÔºå it's lower than doing it one time with this matrix multiplication here„ÄÇ

 So using this vectorization„ÄÇM„ÄÇEverything or makes the processing or iterating of an epoch faster„ÄÇ

In the mini batch modeÔºå compared to the online mode„ÄÇAlso„ÄÇ

 having fewer updates than in an online modeÔºå like including more examples in each update makes„ÄÇ

Things a little bit more robustÔºå so it reduces the noise a little bit„ÄÇOn the other hand„ÄÇ

 we could if these two argumentsÔºå if you consider theseÔºå then we can sayÔºå okay„ÄÇ

 why don't we use the full batch modeÔºå the regular batch mode then right because that would be even faster and it would be even more robust because updates are less noisy„ÄÇ

 So and practice actually firstÔºå the data would be too large a common data„ÄÇ

 So this one on a deep learning data wouldn't be feasible„ÄÇ

 We can't just have millions or thousands even of data points here because then we would have„ÄÇ

Sa if we have 1000Ôºå if we have a 10000 data pointÔºå training said we have a matrix that is 10000 times„ÄÇ

 let's say maybe also 10000 for the features„ÄÇEn deep learning„ÄÇ So x„ÄÇWould be a 10000 times 10„ÄÇ

000 times matrix and W would be then also a 10Ôºå000 times something matrix and that would probably blow up the computer memory„ÄÇ

even if that would work„ÄÇActuallyÔºå the noise is quite good for learning and deep learning„ÄÇ

 We will see that later„ÄÇ it helps the algorithm to escape socal local minima„ÄÇ

 So in this way actually the mini batch mode also the little bit of noise is good„ÄÇ

 so in the online mode there's maybe be too much noise like from one training point at a time„ÄÇ

 but having a little bit of noise like having these small batches actually helps„ÄÇ

Yeah and also compared to the full batch modeÔºå like I said here in 0„ÄÇ3„ÄÇ

 we have more updates in the mini batch mode for each iteration over the epoch„ÄÇIn the batch mode„ÄÇ

 we onlyÔºå yeah have this iteration of the epochs and then only„ÄÇUpdate once„ÄÇ So we apply this only„ÄÇ

Once after each epoC in the mini batch modeÔºå we do that multiple times depending on how many mini batchs we have in each epoch„ÄÇ

 so in that way we have more updates than in the batch mode which can also help with learning„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_15.png)

Things faster„ÄÇAllrightÔºå so in the next video I will briefly go over the relation between the perceptron and linear regression„ÄÇ

 So if things were a little bit I would say yeah a little bit weird out of context this is something that we will be using for the rest of this course so we will be using mostly the mini batch mode later on when we train neural networks so here I was just explaining the different ways we can process a data and this will become relevant when we talk about stochastic gradient descent so with this lecture it's like usually a chicken egg problem there are many things or concepts one depends on the other and we have to start somewhere so now I outlined different ways we can process the data and then we will see in stochastic gradient descent and gradient descent how this actually applies to the actual updates„ÄÇ



![](img/351cb4b77800bdb89f1639a447ab466e_17.png)