# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P57ÔºöL8.7.1- OneHot ÁºñÁ†ÅÂíåÂ§öÁ±ªÂà´‰∫§ÂèâÁÜµ - ShowMeAI - BV1ub4y127jj

YeahÔºå in the previous videos I showed you how the Somax regression model looks like„ÄÇ

 Now let's talk about the multi category cross entropy loss that we will need for training such a model„ÄÇ

And for thatÔºå we will also need the one hot encoding scheme for representing the class ables in a way that is compatible with a multi categoryeg cross entropys„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_1.png)

So yeahÔºå this is just a recap a slide I already showed you in the previous video„ÄÇ

 It's how the basic setup looks like where we have an input feature vector„ÄÇ

 and then we have multiple sets of feature of sorry of weight vectors„ÄÇ

 we have multiple sets of weight vectors„ÄÇ If we have H classesÔºå we have H„ÄÇWeight vectors„ÄÇ

 would wecombine into a matrix W„ÄÇSo we discussed that before„ÄÇ I don't need to recap thisÔºå I think„ÄÇ

 But yeahÔºå what we are focusing on now is how we„ÄÇComputers softne activation that comes from these different net inputs„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_3.png)

So here's an overview of how the Somax activation function looks like„ÄÇ

 It's fundamentally not very complicated„ÄÇ It's essentially a normalization function„ÄÇ

 So it's in a wayÔºå you can think of it as a generalization of the logistic Sigoid function for making the probabilities to sum up to one„ÄÇ

So„ÄÇLet's say we have a class A TÔºå So the„ÄÇPossible class tables are12 H„ÄÇAnd„ÄÇWith a softm activation„ÄÇ

 we can compute the probability that a given output belongs to class T„ÄÇSo if I go back here„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_5.png)

This is„ÄÇIt's say Z1Ôºå Z2 up to Z H„ÄÇ We have H net inputs here„ÄÇ And from that one„ÄÇ

 we can compute this a1Ôºå which is essentially the probability that„ÄÇClass label belongs to class 1„ÄÇ

 given„ÄÇThe input vector to or here in this caseÔºå that's bright as as„ÄÇZ1 is the net input„ÄÇ

 because here we have written this as a functionÔºå a softm activation function„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_7.png)

And how this works is there are two parts that it's essentially an exponential function„ÄÇ

 So you have E to the power of the net input as the numerator„ÄÇ So I is the index„ÄÇOver„ÄÇThe training„ÄÇ

Examples„ÄÇAnd„ÄÇHere in the numeratorÔºå we sum over all exponential terms so„ÄÇÂóØ„ÄÇ

We have these exponential termsÔºå JÔºå also 12 H„ÄÇAnd if we have H class tables„ÄÇ

 we have H activations and H net inputs„ÄÇ So here we are summing over H sorry„ÄÇ

 J here we are summing from J to H„ÄÇSo this is yeah essentially a normalization term„ÄÇ

 So if we apply this the probability„ÄÇWill be smaller than one between 0 and 1„ÄÇ

 And the probabilities from all the net inputs will sum up to  one„ÄÇ so consequently„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_9.png)

If I go back one more time hereÔºå so the sum„ÄÇLet me write this on the right hand side„ÄÇ

 the sum of all these„ÄÇ

![](img/6d5708ed91a29326a0e189ec448b1c76_11.png)

I„ÄÇShould be one after this normalization function„ÄÇSo yeah„ÄÇ

 this is essentially how this softm activation works„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_13.png)

Now let's talk about the loss function„ÄÇSo before we can talk about the loss function„ÄÇ

 there's one more thingÔºå we have to talk about the oneHt encoding„ÄÇ

 which is yeah an encoding scheme for categorical variables and we can also apply this to the class label variable So you can use that in different contexts„ÄÇ

 you can do that for encoding features„ÄÇ you have probably seen that in the context of traditional machine learning„ÄÇ

 but yeah we can also apply this one hot encoding scheme to the class label variable or the class label vector„ÄÇ

 which is essentially also categorical variable„ÄÇSo imagine we have a data set that consists of yeah different class labels here„ÄÇ

 Each row represents one training exampleÔºå So we have the first training example with class label 0„ÄÇ

 the second one with class label1 and class label 3 and then class label2 and the possible class tables are 3 So let's say we have class labels on„ÄÇ

in betweenÔºå it's a set between os„ÄÇ0Ôºå1Ôºå2Ôºå and3 these are possible class labels„ÄÇ

Now we can convert this into a one hot encoding on the right hand side here„ÄÇWe have now„ÄÇA column„ÄÇ

For each possible valueÔºå so class 0Ôºå1Ôºå2Ôºå and 3„ÄÇAnd„ÄÇEach row still represents one training example„ÄÇ

 So here this is still the first training example„ÄÇ The second row is still the second training example„ÄÇ

 The third row is the third„ÄÇ and this is the fourth„ÄÇ I can maybe write this down„ÄÇ

So this still refers to the training examplesÔºå 1Ôºå2Ôºå3Ôºå and 4„ÄÇAnd then„ÄÇÂóØ„ÄÇWe can look at all these for„ÄÇ

Features here„ÄÇ So for training example 1Ôºå let's focus on training example 1„ÄÇ

So training example1 has class A 0Ôºå rightÔºå So it has a one and indicator one in the column class 0 and all the other„ÄÇ

Columns are set to 0„ÄÇ So hereÔºå this is a 0Ôºå0 and 0 because it's class 0„ÄÇ

 So the true class has a one in its position and the other ones are all 0„ÄÇ And consequently„ÄÇ

 if we look at class number 2 here„ÄÇThat's why training example two„ÄÇThere this one has class A 1„ÄÇ

 and it has a one in the second column in belonging to class 1„ÄÇAnd then„ÄÇ

Let's do one more third training example„ÄÇ This is class 3„ÄÇ It has a one here in the last column„ÄÇ

 So this is how the one hot encoding works„ÄÇ You can think of it as an indicator variable„ÄÇ

 So you have like a one in the right columnÔºå and the other ones are 0„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_15.png)

Yeah and here's now how the multicatego cross entropy looks like for the age different class labels„ÄÇ

 so we have class labels 12 H and essentially it's a generalization of the binary cross entropy„ÄÇ

 So if you recall the binary cross entropy is essentially the negative lock likelihood„ÄÇ

 which we discussed in the vast videos about logistic regression now so if you recall what we had was then„ÄÇ

This„ÄÇNegative lock of the activation„ÄÇ That's the same thing here„ÄÇ So we also have a negative lock„ÄÇ

 But there's one more thing„ÄÇ Now we have this Y I Y J here and the Y J„ÄÇIs either one or0 right„ÄÇ

 according to the one hot encodingÔºå this is either a1 or a 0„ÄÇ

 So this whole equation assumes a1 hot encoding„ÄÇI will show you a concrete example in the next slide to further clarify that„ÄÇ

 but yeahÔºå just to focus in on this overview here„ÄÇAlsoÔºå IÔºå againÔºå I is over the training examples„ÄÇ

 So let's only focus on this inner term here„ÄÇ So what we have is we have for this y either a 1 or a 0„ÄÇ

And„ÄÇThen we areÔºå we are summing over this„ÄÇNegative locks for these different activations„ÄÇ

 So the activationsÔºå each activation is a value„ÄÇBetween 0 and 1„ÄÇ So we apply„ÄÇ

 So it should be actually larger than 0„ÄÇ So we apply„ÄÇMinus lock„ÄÇOf this term between 0 and 1„ÄÇ

And then we sum over all of these for the output nodeÔºå we have H output nodes and we sum over those„ÄÇ

So„ÄÇIf we use the one hott encodingÔºå we have h classesÔºå but only one class„ÄÇ

 the true label for one class is only one rightÔºå according to the one hot encoding„ÄÇ

 all the other ones are0„ÄÇAnd I will clarify this with a concrete example because I think it's hard to visualize if you don't have a concrete example in mind„ÄÇ

 but yeah one more thing then here this first sum is then summing over the training examples„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_17.png)

AlrightÔºå so let's take a look at a more concrete example„ÄÇ or okayÔºå I more slide in between„ÄÇ

 And then I will show you the concrete example„ÄÇ So here's just the analogy to the binary cross entropy at the top„ÄÇ

 So at the topÔºå you have the binary cross entropy„ÄÇSo yeahÔºå you have also the negative term„ÄÇ

 like bring it insideÔºå if you like„ÄÇ So you have this negative term„ÄÇ

 negative cross negative look likelihood„ÄÇ and this term is essentially the same as this term„ÄÇ

If the classable is one„ÄÇRightÔºå so this is a one„ÄÇ this is a one that's the same„ÄÇAnd this one„ÄÇIs„ÄÇ

When we had a class table 0„ÄÇ So if we have a class table 0Ôºå then we have1 minus-0Ôºå then this is a 1„ÄÇ

So essentially we have also this term hereÔºå1 minus a„ÄÇ

 we don't have that here here we have minus lock for all these classes„ÄÇ

 but essentially it's the same thingÔºå it's just a generalization because if we have a 0„ÄÇ8 here„ÄÇ

For the activationÔºå then we would have a 0„ÄÇ2 here for 1 minus the activation„ÄÇ

 right because I have to sum up to1„ÄÇWe set with a softm function„ÄÇ it's the same thing„ÄÇ

 All the activations sum up to one„ÄÇ So if we have„ÄÇ2 class labelsÔºå0 and1„ÄÇ

 If our class labels are either„ÄÇ0 and1„ÄÇThese equations are identical„ÄÇBecauseuse„ÄÇ

It's just a different way of writing this here„ÄÇ we write this as the sum but the sum is for binary classification„ÄÇ

 basically the sum between those two termsÔºå so it's essentially exactly the same thing„ÄÇ

 but now with writing it like this using the one hot encoding we can generalize this to more than two classes„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_19.png)

So here's a concrete example„ÄÇSo let's consider this exampleÔºå where we have„ÄÇ1„ÄÇTraining examples„ÄÇ

Each row here represents one training example„ÄÇNumber one„ÄÇBut two„ÄÇ3 and„ÄÇüòîÔºåNumber4„ÄÇ

And the column index represents which class label it is„ÄÇ So we have three possible classes„ÄÇ

 So the first column is„ÄÇClass„ÄÇ1„ÄÇThen this is class 2Ôºå and this is class 3„ÄÇSo we can see class 1„ÄÇ

 the true label is class sorryÔºå training example 1Ôºå the true label is class 1„ÄÇ

Because the one is in the first column„ÄÇ This one has the one in the second column„ÄÇ So it's class„ÄÇ2„ÄÇ

 this should be class 3 because the one is in the third columnÔºå and this is also class 3„ÄÇ

Here on the right hand sideÔºå I generated some arbitrary softmax outputs„ÄÇ

So notice that all the columns should sum up to one„ÄÇ So the sum of the columns„ÄÇShould be one„ÄÇ

 in each case„ÄÇYou can double check this with a calculator„ÄÇ if you like„ÄÇ

 they should all sum up to one because we have mutually exclusive classes„ÄÇ

 This is what we get when we apply the softm outputs„ÄÇ

 I will also show you a code example how we can actually compute that in Pytorch„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_21.png)

But let's continue with this example„ÄÇSo now let's compute the loss„ÄÇSo here at the bottom„ÄÇ

 so we are focusingÔºå oh let's say we are focusing on the first training example„ÄÇ

 training example one„ÄÇSo this corresponds to this first row in the„ÄÇActation matrix„ÄÇ

And now what we are computing isÔºå we are computing„ÄÇThe negative lock of the activation„ÄÇSo„ÄÇLet me use„ÄÇ

Let me use blue here„ÄÇ So we compute one„ÄÇTimes the lock of 0„ÄÇ37„ÄÇAnd then 0 times the log of 0„ÄÇ31„ÄÇ

And0 times„ÄÇ31„ÄÇSoÔºå this is what I'm„ÄÇWhat I've written down here„ÄÇ So -1„ÄÇTimes lock„ÄÇÂóØ„ÄÇ3Ôºå7„ÄÇ

 and then the other terms here„ÄÇSoÔºå this is„ÄÇThis is this one here that we are computing when we are summing„ÄÇ

For the first training exampleÔºå we get a loss of 0„ÄÇ96„ÄÇ969692„ÄÇ

 So this is our loss for the first training example„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_23.png)

OkayÔºå I hope that makes sense so we can then do the same thing for the other training examples„ÄÇ

 So here I use a colorÔºå a different color for each training example„ÄÇSo here„ÄÇ

 this one is computed here then„ÄÇWe have the red ones„ÄÇFor that„ÄÇWe have this box hereÔºå and then lastly„ÄÇ

For this roleÔºå we have this box here„ÄÇ I don't have to go over the computation againÔºå I think„ÄÇ

 because it's the same as for the first training example„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_25.png)

YeahÔºå and then one more stepÔºå we can put everything together„ÄÇ So now the outer sum„ÄÇ

So we are summing overall„ÄÇThese losses hereÔºå and the sum„ÄÇOf these isÔºå sorry„ÄÇ

 it should be actually the meanÔºå right„ÄÇ1 over„ÄÇÂóØ„ÄÇSoÔºå the mean 093„ÄÇOtherwise„ÄÇ

 the sum would be four times that„ÄÇOkayÔºå so„ÄÇUsually like I said„ÄÇ

 it's more stable in a deep learning context in practice if we compute the mean instead of the sum for the mini batch because then it's easier to find a good learning rate because let's say you find a good learning rate for a given mini batch and then you make the mini batch larger the loss will then be also if you make mini batch 10 times larger the loss will also be 10 times larger so you have to decrease the learning rate by a factor of 10 to get approximately the same performance so it's usually I find it easier to deal with the mean instead of the sum„ÄÇ

OkayÔºå and yeahÔºå I will show you now a code example how we can do this in Pythtorch„ÄÇ

 but this will be more like exactly what we have done in the slides and practice there are a few more tricks that will be the later shown when we talk about a Som implementation„ÄÇ

 but let me make a pause now and let me record this code example in a separate video because I think this video is probably already pretty long„ÄÇ



![](img/6d5708ed91a29326a0e189ec448b1c76_27.png)

![](img/6d5708ed91a29326a0e189ec448b1c76_28.png)