# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P64ÔºöL9.2- ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ - ShowMeAI - BV1ub4y127jj

YeahÔºå let us now get to these nonlinear activation functions So why are we interested in these nonlinear activation functions Yeah„ÄÇ

 together with the hidden layersÔºå they allow us to model complex nonlinear decision boundaries„ÄÇ

 so with that we can solve complicated problems complicated classification problemsÔºå for instance„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_1.png)

So but before we take a look at these nonlinear activation functions more closely„ÄÇ

 let us briefly recap the Pytorch APIÔºå So this is something I just copy and pasted from lectureture 5„ÄÇ

Where we already had a multi multilayer perceptionron just to illustrate how the Pythtorch API works„ÄÇ

 So on the left hand sideÔºå yeahÔºå this is like the regular approach„ÄÇ So where we have in it„ÄÇ

Constructor„ÄÇ And here this is a multi layerer perceptron with two hidden layers„ÄÇ

 I called them linear 1 and linear 2 because linearÔºå because the layer is called linear„ÄÇ

 it's computing the net inputÔºå rightÔºå So these are our„ÄÇLayers that we use and one output layer„ÄÇ

 So this is essentially the setup that I showed you earlier in the slides„ÄÇ So if I can just go back„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_3.png)

So this is essentially this setup where we have one hidden layerÔºå second hidden layer„ÄÇ

 and an output layer„ÄÇ

![](img/1ad76e568e4f9cdba813be05e8c1f3bb_5.png)

AlrightÔºå so this is how it looks like„ÄÇ and in the forward method„ÄÇWe use these actual layers„ÄÇ

 So we apply the first hidden layer„ÄÇ Then we have our linear„ÄÇActation here is' a re function„ÄÇ

 We will„ÄÇRevisit this in a few slides„ÄÇ Then another„ÄÇNe inputÔºå another activation function„ÄÇ

 another net input„ÄÇ And then usually we would compute the softm„ÄÇ in Pyth„ÄÇ recall„ÄÇ

 we use this cross entropy lossÔºå which already yeah computes the softm for us„ÄÇ

 So we don't have to do it ourselves„ÄÇ So here we apply this logofm function„ÄÇOn the logics„ÄÇ

 So there's also a softm function„ÄÇ But here yeahÔºå we computers based on the„ÄÇ

 we use the lock softmax because it's numerically more stable if we were to use„ÄÇ

The negative look likelihoodless„ÄÇThis is really the only if we want to use the negative log likelihood loss„ÄÇ

 otherwise we would just use softmax if we are interested in the probabilitiesÔºå to be honest„ÄÇ

 now looking at thisÔºå I don't know why I used log Somax„ÄÇ

 I think in the big code example when I created lecture 5 I had the negative log likelihood here„ÄÇ

So also technically you don't have to compute the probabilities within this class„ÄÇ

 you can do this separately if you care about because technically you never have to use the probability if you use the cross entropy function and Pythtorch for optimization„ÄÇ

 but again this will become more clear when we look at the concrete code example in the next video„ÄÇ

On the right hand sideÔºå the main difference is that I'm using the sequential API„ÄÇ

 which can be a little bit more would say easier to read more compact„ÄÇ

 So it's exactly the same network except that here we define it and it sequential can actually also run it So we don't have to put it explicitly in the forward method it will already it for us if we call here My network„ÄÇ

 I just call it my network„ÄÇ So it is in a way a little bit more convenient„ÄÇ

 So what's more convenient in particular is that we don't have to„ÄÇ

Yeah define these steps in the order these are used ourselves and we can also just read right now from here how these layers are executed„ÄÇ

 they are executed from top to bottom„ÄÇ This is also the case hereÔºå but for example„ÄÇ

 if I want to know what the linear one isÔºå I have to go up and look it up here„ÄÇ

 So it's a little bit moreÔºå I would sayarrow prone whereas on the right hand side„ÄÇ

 we define it and we also directly use it in that order„ÄÇ

 So it's a little bit mores it's a little bit saferÔºå I would say„ÄÇSo yeah„ÄÇ

 so the flow is we apply a linear layerÔºå a non nonlinear activationÔºå a linear layer„ÄÇ

 non nonlinear activationÔºå and a linear layer„ÄÇAnd that will produce our logics„ÄÇ

 And then if we interested in thatÔºå the probabilities„ÄÇ But again„ÄÇ

 Pyrage and cross entropy already applies the softm for usÔºå so we don't have to do it ourselves„ÄÇ

So technicallyÔºå we can skip the softmax step and don't have to use that„ÄÇ

It's just like I usually like to include the probabilities because I sometimes also print out the results from a model and analyze it with plots and probability plots and stuff like that„ÄÇ

 so in that way I sometimes find it helpful to have the probabilities available so I usually save my results but you don't have to worry about this really„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_7.png)

AlrightÔºå so with thatÔºå we can then solve the X O problem using these non nonlinear activation functions„ÄÇ

 So the non nonlinear activation functions let us„ÄÇ

![](img/1ad76e568e4f9cdba813be05e8c1f3bb_9.png)

Make complex decision boundariesÔºå like I just said„ÄÇ So here„ÄÇ

 this is like a toy data set I made just two features„ÄÇX 1 and x 2Ôºå just for simplicityÔºå two classes„ÄÇ

 these orange ones and these blue dots„ÄÇSo the orange squares and the Budts on the left hand side here„ÄÇ

Just for funÔºå I was applying a„ÄÇMulil layer perceptron with one hidden layer and a linear activation function„ÄÇ

 So recall the linear activation function looks like that„ÄÇSo you can see„ÄÇThat on the left hand side„ÄÇ

 that it's a linear decision boundaryÔºå even though we have a hidden layer„ÄÇ by the way„ÄÇ

 I don't want to open the code notebook right now because then I have to switch the screen again„ÄÇ

 But if you're interested after watching this video„ÄÇ

 you can double check here under this link that this is indeed the code is indeed correct„ÄÇ

 so you can reproduce these results if you don't believe meÔºå but in any case so you can see„ÄÇ

On the left hand sideÔºå this is a linear decision boundaryÔºå even though we have a hidden layer„ÄÇ

 so what happened hereÔºüSo because in logistic regression„ÄÇ

 we had a non nonlinear activation function and no hidden layer„ÄÇ It was a linear decision boundary„ÄÇ

 NowÔºå we have a hidden layer„ÄÇAnd no non linear activation„ÄÇ

 We also have a decision boundary that is linear„ÄÇ So let's recap logistic regression„ÄÇNo„ÄÇ

 hidden there„ÄÇPlus„ÄÇNo linear activation„ÄÇGives us a linear boundary„ÄÇMlP„ÄÇWith„ÄÇWith„ÄÇüòîÔºålinnyang„ÄÇ

Activation„ÄÇPlus„ÄÇHidden there„ÄÇAlsoÔºå linear boundary„ÄÇSo from thatÔºå we can deduce neither„ÄÇThe„ÄÇ

Hidden layerÔºå which we have in the multideceptionronÔºå nor the„ÄÇ

None nonlinear activation function alone are sufficient„ÄÇFor making a nonlinear decision boundary„ÄÇ

 they are necessaryÔºå but they are not sufficient for making a nonlinear decision boundary„ÄÇ In fact„ÄÇ

 we need both„ÄÇ We need both the hidden layers and the nonlinear activation functions to make a nonlinear decision boundary„ÄÇ

 So on the right hand side„ÄÇI have now the same multiap perceptron with one hidden layer„ÄÇ

 the same number of weightsÔºå the only difference is that I'm now using a non nonlinear activation function here I'm using the relative function„ÄÇ

And you rememberÔºå this is very simple„ÄÇ It's actually just threshold at 0„ÄÇ

 So if the input is negativeÔºå the output is 0Ôºå otherwiseÔºå it's an identity function„ÄÇ

 So it's almost an identity functionÔºå but not quite„ÄÇ And this is„ÄÇSuffic„ÄÇ

 together with the hidden layers to make a nonlinear decision bar„ÄÇ

 You can see this decision bar is now nonlinear„ÄÇ You can see it's solving this X O problemÔºå so„ÄÇ

It can now classify these data points correctly„ÄÇSo why is this working and why is this not workingÔºü

 So why is the linear decision on the linear activation function not sufficient and produces a linear decision region„ÄÇ

 That's because if you think about it„ÄÇEven though we have a hidden layer„ÄÇ

If we have a linear activation functionÔºå what happens is that we have essentially a combination of multiple linear functions and the combination of multiple linear functions is still a linear function„ÄÇ

 So if we don't use nonlinear activation functions„ÄÇ

 then we don't really gain anything by using a hidden layer„ÄÇ

 so we need actually both hidden layers and nonlinear activation functions to produce these complex decision boundaries„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_11.png)

ActuallyÔºå there are way more than just relu functions„ÄÇ Redlu just happens to be popular because it's„ÄÇ

It's quite simple and quite fast to compute and has also some other nice properties„ÄÇ

 which I will also briefly talk about in a few minutes„ÄÇ SoÔºå but yeahÔºå traditionallyÔºå these are also„ÄÇ

 yeah one of the most popular activation functions in multi perceptioncepts with thatÔºå I mean„ÄÇ

The logistic sigmoid that we already encountered in the context of logistic regression„ÄÇ

 So back in the dayÔºå I would say even the sigmoid was maybe the most popular activation function in multi epicirons„ÄÇ

But againÔºå it has this problem that these gradients saturate here„ÄÇ

Another very popular activation function for a while was the 10 H function„ÄÇ It's also a soidal„ÄÇ

So both are smodal functions„ÄÇ It's also smodal functionsÔºå So S shaped„ÄÇ

But this hyperbolic tangent function„ÄÇLooks a little bit different„ÄÇ

 So you can see this one the logistic sigmoid function is centered around 0Ôºå and the output is 0„ÄÇ5„ÄÇ

Whereas this one is centered at 0Ôºå and the output„ÄÇAt 0 is also0„ÄÇ

 So it's producing positive and negative valuesÔºå which can be an advantage„ÄÇThere' is also a heart t„ÄÇ

 which is essentially very similar to 10 hÔºå except that it's threshold hereÔºå similar to Relu„ÄÇ

So what is the advantage ofÔºå let's sayÔºå the 10 h over the sigmoid activationÔºü

So the average of 10 Hs reallyda that we have this centering„ÄÇAt 0„ÄÇ

So that we have positive and negative values and you can also see it's steeper„ÄÇ

 So here the it's steeper than this one„ÄÇ so we have larger gradients„ÄÇ

It has also a very simple derivative 1-10 H„ÄÇ recall for the logistic sigmoid„ÄÇ The derivative was„ÄÇ

Like this„ÄÇSo itself times 1 minus itself„ÄÇ So the„ÄÇThe relative is slightly smaller because it'sÔºå yeah„ÄÇ

 multiplying„ÄÇ2 numbersÔºå smaller than„ÄÇOne with each other„ÄÇ So where series of 1 minus this one„ÄÇÂóØ„ÄÇ

But then yeahÔºå you have also the squared here„ÄÇ So it's actually not that different okay„ÄÇYesÔºå so„ÄÇ

 but both have the problem that if you make a wrong prediction„ÄÇ

 it's both if you make a right or wrong predictionÔºå but in both cases„ÄÇ

You hear and hear So you saturate it so that that can be a problem if you make a wrong prediction„ÄÇ

 you end up with a very small partial derivative„ÄÇWithÔºå with respect to its well derivative„ÄÇ

 with respect to its input„ÄÇ And then when you compute the partial derivatives in the chain rule„ÄÇ

 then yeahÔºå you will get very small gradients and the learning will be very slow„ÄÇ

 which can be a disadvantage„ÄÇOr maybe one more thing about why it's good to have„ÄÇ

Negative and positive values that just gives you more combinations„ÄÇ

 So imagine you initialize your weights fromÔºå let's say a small or from a random normal distribution„ÄÇ

 standard normal distributionÔºå let's say or a scale standard normal distribution„ÄÇ

 so you initialize your weights such that they are centered at0„ÄÇ

 So you can have positive and negative starting weights„ÄÇ

 And if you also use this 10h which can have positive and negative values„ÄÇ

 you get just more combinations of possible values„ÄÇ

 whether you combine a positive with a negative number„ÄÇ

 a negative with a positive number to a negative numbers or to positive numbers„ÄÇ

 you have four different ways you can combine these signs„ÄÇ

 whereas if you have a activation function that can only produce positive numbers„ÄÇ

 you a little bit more limited„ÄÇSo I would say that 10 H is a little bit more expressive„ÄÇ

 It allows you a little bit more„ÄÇ YeahÔºå expressivityÔºå if that's a word„ÄÇBut yeah„ÄÇ

 I think also in practice when I use them and I recall„ÄÇ

 I only saw a minor difference using one over the other„ÄÇ

 really if you use a re function that gives you a better bang for the buck usually„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_13.png)

So here are some more non nonlinear activation functionsÔºå including the Redou function„ÄÇ

 So the Redlu function here„ÄÇ that's something you have seen before„ÄÇ It's„ÄÇ

 I would say still the most widely used activation function in deep learning„ÄÇ

 I think it's maybe 10 years old by nowÔºå something like that„ÄÇ But when you look at recent papers„ÄÇ

 People still use a Reddo a lot„ÄÇ It has really nice properties„ÄÇ It's simple to computeÔºå really„ÄÇ

 And you have always„ÄÇYeahÔºå that derivative„ÄÇ The derivative is one„ÄÇ

 So if you use the chain rule and your derivative is oneÔºå if the inputs are positiveÔºå then„ÄÇYet„ÄÇ

 you don't diminish the product in the„ÄÇGeneralÔºå okayÔºå but it can also be0Ôºå which can be a problem„ÄÇ

 So if you have negative inputs to this activation functionÔºå yourÔºå your output would be 0„ÄÇ

 which will then basically cancel the weight update for that corresponding weight corresponding to this activation or connected to this activation„ÄÇ

 So that can be a problem if you always have very negative input„ÄÇ So there is a problem called„ÄÇDying„ÄÇ

Neurons or dead„ÄÇR loose that happens usually when you have somehow updated the weights such that you will never come into the positive region anymore„ÄÇ

 And then yeahÔºå you will never be able to update your weights again because the derivative will always be zero„ÄÇ

 that can be a problem„ÄÇ HoweverÔºå in practiceÔºå some people argue it can also be an advantage because it can help with pruning let's a unnecessary neurons„ÄÇ

 like if you have an excessive number of neurons„ÄÇ this way you can get rid of some of them and it may help with preventing overfitting„ÄÇ

A version of Re that some people find„ÄÇTo perform sometimes a little bit better is the leaky relu„ÄÇ

 which doesn't have the problem of these dying neurons„ÄÇ So hereÔºå the difference is that we have„ÄÇ

 So if we look at the simplified notationÔºå the„ÄÇPace ofwise linear I function here„ÄÇÂóØ„ÄÇ

What you can see here or the Pwise function„ÄÇ sorry„ÄÇ

 what you can see here is that the only difference is that we have now this alpha here„ÄÇ

 which is a slope„ÄÇ If the input is smaller than 0„ÄÇ So for the negative region here„ÄÇ

 we have now a slope„ÄÇ what value we can choose for the slopeÔºå it's a hyper parametermeter right„ÄÇ

 So hyper„ÄÇPermence is something that youÔºå as the practitionerÔºå haveÔºå has to choose„ÄÇ

 So there's no way„ÄÇYouÔºå it's say can know what's a good value if it's a habit right„ÄÇ

 It's something you have to try out and practice and change and see what performs better„ÄÇ

 I have seen all kinds of values for this negative slope here or for the slope in the negative region„ÄÇ

So in Kas that's API for TensorflowÔºå I believe they use 0„ÄÇ3 as the default value in Pyr„ÄÇ

 I don't know exactly the default„ÄÇ I usually specify myself„ÄÇ I usually use something like 0„ÄÇ1 or 0„ÄÇ

01 but yeah there are different values that work well for different problems„ÄÇ

 in practice you will only see a slight small difference„ÄÇ

 So it's not something if your network let's say doesn't perform very well than choosing a different value here probably won't make a big difference„ÄÇ

 So you have probably bigger problems to fix„ÄÇBut it can give you maybe1Ôºå2„ÄÇ

3 percentage points in terms of accuracy if you are lucky„ÄÇ

I said hyperpar cannot be like automatically learned from gradient descentÔºå however people designed„ÄÇ

Parameterterized version of the leaky Relu„ÄÇ It's called Perlu parameterized Relu basically And here this is essentially the same as buff„ÄÇ

 but here the people made it made alpha a trainable parameter„ÄÇ

 so it's a parameter that can be also updated with gradient descent„ÄÇSo in practice„ÄÇ

 I honestly never really„ÄÇI have never really seen this being used„ÄÇ

 I'm not sure if this is really that useful„ÄÇThere's also an L an exponential linear unit„ÄÇ

 So it's getting around this kink here„ÄÇ It's like more like a smooth version here in this kk„ÄÇ

 So there are different typesÔºå reallyÔºå manyÔºå many different types of non nonlinear activation functions„ÄÇ

 There's alsoÔºå I recall there's a„ÄÇSluÔºå I've seen that quite often recently„ÄÇ

 It's I think it stands for self normalizing exponential linear unit„ÄÇ

 So it has some nice properties also„ÄÇ But yeahÔºå againÔºå there are lots of different flavors„ÄÇ

 Usually people still use the relu a lot because it just performs well„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_15.png)

YeahÔºå related to the topic of nonlinear activation functions I saw this paper here last year called Sth adversarial training„ÄÇ

 I bookedmarked it because I knew it would become handy when I teach a class on the burning so„ÄÇ

Adversarial trainingÔºå just briefly what is adversarial training„ÄÇ

 adversarial training is yeah exploiting deep neural networks or more like fooling deep neural networks it's if you have let's say an image or some data point and your network makes a prediction„ÄÇ

 let's say you have an image of a catÔºå it predicts CA„ÄÇ

There's some way you can exploit the network by just changing the image very slightly„ÄÇ

 making like a few pixel changesÔºå but you find these pixel changes that are able to fool the network to„ÄÇ

 let's say nowÔºå think that the image is a dock instead of a cat„ÄÇ

 So it's like exploiting weaknesses in the network„ÄÇSo like the authors say„ÄÇ

 it is commonly believed that networks cannot be both accurate and robust„ÄÇ

So usually if you want to make your network more robust towards these advers and examples„ÄÇ

 you usually trade it off by yeah by suffering in terms of accuracy„ÄÇ

 so the accuracy is usually lower in these more robust networks„ÄÇSo like I say„ÄÇ

 gaining robustness means losing accuracyÔºå HoweverÔºå yeahÔºå let me just read it„ÄÇ

 our key observation is that the widely used relu activation function significantly weakens adversarial training due to its non smoothth nature„ÄÇ

 So it sounds like yeah the Relu is like a little bit of a disadvantage for adversarial training„ÄÇüòä„ÄÇ

HenceÔºå we propose smooth adversarial training in which we replace Redlu with its smooth approximations to strengthen adversarial training„ÄÇ

 So the authors argue that if you replace this Redlu here in red with this non smooth„ÄÇ

Point here with a smooth versionÔºå for exampleÔºå the permit soft plus function„ÄÇ Then yeah„ÄÇ

 you can strengthen it because now maybe it has something to do with the gradients because here you have this big difference whereas here you have also something in between„ÄÇ

 So now this is maybe helping with this adversarial training„ÄÇ So based on the experiments„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_17.png)

YeahÔºå and the second reason why I like this paper is because they have a nice summary of the smooth approximations of the non smooth relu function„ÄÇ

 So on the left hand sideÔºå this is how the activation functions look like„ÄÇ

 they call it here the forward path„ÄÇ And then here are the derivatives for the backward path on the right hand side„ÄÇ

 So yeahÔºå you can see these are all slightly different„ÄÇ

 but they are all kind of approximating the the relu in some way„ÄÇüòäÔºåIn practice yeah in practice„ÄÇ

 this has a big implication apparently for the adversarial robustnessÔºå but also like I said„ÄÇ

 choosing different activation functions that are relatively similar„ÄÇ

 it doesn't make a big impact in practiceÔºå so also what they see here„ÄÇ

 it's just one percentage pointÔºå so one percentage point between Relu and the others„ÄÇ

So it helps a little bit„ÄÇ TheseÔºå some of these others perform actually betterÔºå a little bit better„ÄÇ

 It's not hugeÔºå but it's a little bit better„ÄÇ So it'sÔºå in a wayÔºå it doesn't hurt using them„ÄÇ

 But on the other handÔºå you can see there's a huge advantage in terms of adversarial robustness from 33% up to„ÄÇ

42% is almost there 10%„ÄÇ10% difference in terms of adversarial robustness„ÄÇ So given thatÔºå I mean„ÄÇ

 it's not much work to just replace Relu with one of these non other non linear activation functions„ÄÇ

Given that's not much workÔºå why not doing itÔºå rightÔºü So it's actually quite interesting„ÄÇ



![](img/1ad76e568e4f9cdba813be05e8c1f3bb_19.png)

YeahÔºå and because activation functions are quite boring„ÄÇ

 I have a quite fun visualization here I just saw today coincidentally so that's these dance moves of deep learning activation functions„ÄÇ

 With that I want to enter this video and then in the next video we will talk a little bit more about multilayer perceptrons and in particular coding it up in pywach and training it„ÄÇ

üòä„ÄÇ

![](img/1ad76e568e4f9cdba813be05e8c1f3bb_21.png)

![](img/1ad76e568e4f9cdba813be05e8c1f3bb_22.png)