# P90ï¼šL12.0- æ”¹è¿›åŸºäºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–ã€è¯¾ç¨‹æ¦‚è¿°ã€‘ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hi everyoneã€‚ I hope you had a nice weekendã€‚ So today is the last time I want to dedicate a whole lecture to just yeah techniques for improving generalization performance in neural networksã€‚

 So in the upcoming lecturesï¼Œ I will yeah talk about new neural architecturesï¼Œ for instanceã€‚

 convolal networksï¼Œ recurrent neural networksï¼Œ autoencos generative adversarial networks transformers and so forthã€‚

 So yeah today is really the last time we will just talk about improving generalization performance and in particularã€‚

 the topics I have in mind are yeah optimization relatedã€‚

 So there's one topic called learning rate schedulesã€‚

 So it's how we can improve stochastic gradient in descent learning by decreasing the learning rate over time during the course of trainingã€‚

 and the other topic is yeah replacing stochastic gradient in descent by modified versions of stochastic gradient in descent For instanceã€‚

 there is a momentum term that we can addã€‚ğŸ˜Šï¼ŒAnd there's also something called adaptive learning ratesã€‚

 So we will learn about an algorithm called atomï¼Œ which combines adaptive learning rates and momentum to yeah make learningã€‚

 I would sayï¼Œ easierã€‚ so you have to do less fiddling with hyperparmetersã€‚

 but also it can converge fasterã€‚So I should say thoughã€‚

 everything we talked about so far in terms of these tricks like batchomeï¼Œ dropout and so forthã€‚

 activation functions todayï¼Œ the optimization algorithmsï¼Œ they also apply to all the architecturesã€‚

 we are going to cover in future like justã€‚ So it's all kind of relevantã€‚

 I thought it's just like yeah nice to get that out of the wayã€‚

 And then when you learn about new architectures you are already familiar with these tricks and you don't have to think about what these different terms mean when I show you some implementations of those that make use of these tricksã€‚

 So with that yeahï¼Œ let's get startedã€‚ And yeahï¼Œ and by the wayã€‚

 if you are watching this on Wednesdayã€‚ also yeah happy send pertex dayã€‚Yeahã€‚

 in the past couple of weeksï¼Œ we already covered a bunch of topics related to improving neural network trainingã€‚

 For exampleï¼Œ we talked about regularization to reduce overfitting examples of that included L2 regularizationã€‚

 what we call also weight decayï¼Œ or yeah dropout and then last week we discussed how we normalize our inputs and hidden layer activations for instanceã€‚

 yeahï¼Œ the input standardization and batch normalizationã€‚

And there were some other tricks like weight initialization schemes like Saer Gode and cainghaã€‚

 which can also improve neural network trainingã€‚ So in this lecture we will yet discuss tricks related to the optimization algorithmã€‚

 so there will be a section on the learning rate decayï¼Œ momentum learning and adaptive learningã€‚

I should say these are all techniques that concern your first order methods for optimizationã€‚

 stochastic gradient descentï¼Œ so they are essentially all modifications of stochastic gradient descentã€‚

If you are familiar with more advanced optimization techniquesã€‚

 you may wonder why we don't talk about second order methodsã€‚For exampleã€‚

 if you consider the Hassian matrix and yeah the partial the second order partial derivativesã€‚

 So the reason isï¼Œ yeahï¼Œ peopleï¼Œ many people tried using those techniquesã€‚

 but it just turns out they don't work very well in practiceã€‚ They are either way too expensiveã€‚

 but also don't really result in faster training in terms of your epochs andã€‚Better performanceã€‚

 So usuallyï¼Œ even maybe if you find that it requires fewer epochsã€‚

 these methods are computation is still so expensive that you don't gain anything by using second order methodsã€‚

 Or at least I haven't seen up to this day convincingã€‚Yeahã€‚

 method that really is widely used in practiceã€‚But maybeï¼Œ I meanï¼Œ neverï¼Œ never say neverã€‚

 So it doesn't mean there won't be any technique inï¼Œ in the future that may work wellã€‚

 It's just like that we don't have them yetã€‚

![](img/8c643593ef9e0c66f00d3c66917fecb2_1.png)

So the topics we are going to cover today are these sixã€‚

 so first I want to talk about learning rate decayã€‚

 so a very simple topic concerning the learning rate and yeah reducing it over the course of training to reduce the noise and stochastic gradient in descentã€‚

 I will show you then how we can do that in Pythtorchã€‚And thenï¼Œ we will talk aboutã€‚

A momentum term that we can add to our gradients to help accelerate gradient descent in the right moments and slow it down in other moments or steer it better in that wayã€‚

Yeahï¼Œ there is also a topic of adaptive learning ratesï¼Œ so adaptive learning ratesã€‚

 especially coupled with momentum are commonly used nowadays there's one algorithm called atom which is probably the most widely used optimization algorithm I use it all the time because it works very well out of the boxã€‚

 you can get good results with stochastic gradientdescent and momentum tooã€‚

 but in my experience at least it requires more hyperparmeter searching over the learning rate and atom is usually more robust and works often better out of the box it requires less work in a senseã€‚

I will then show you how we can use these algorithms in Python and there are also so many other algorithms besides let's say atom and momentum that have been emerging in the recent yearsã€‚

 I have a huge list of these topics that I find also sometimes relatively interesting where I do some reading here and there about yeah latest trendsã€‚

 so I will make a separate video outlining some of these advanced studying topics So you don't have to yeahã€‚

You won't have to know these things in detailï¼Œ but for those who are interested to in reading a little bit moreã€‚

 let's say beyond the scope of the classï¼Œ I will just include some references here that you may want to check out in the future and on your ownã€‚

 for exampleã€‚

![](img/8c643593ef9e0c66f00d3c66917fecb2_3.png)

Alrightï¼Œ so let's get started then with the first topicï¼Œ learning rate Deã€‚



![](img/8c643593ef9e0c66f00d3c66917fecb2_5.png)