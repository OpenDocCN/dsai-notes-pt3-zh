# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P96ï¼šL12.6- å…³äºä¼˜åŒ–ç®—æ³•çš„é™„åŠ ä¸»é¢˜å’Œç ”ç©¶ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ I wanted to end this lecture with some additional topics because yeah over the years I bookedmarked so many interesting articlesã€‚

 I didn't even have a chance to read them all there are hundreds of interesting articles related to optimization but yeah if you're interested I have a small selection of things here that you may find interesting So let's start with Adam So going back to the original paperã€‚

 the Adam paper from 2014ã€‚

![](img/20946313d5e9eab28bf7adbcb9836673_1.png)

So you can see there are several other methods that they looked at and this is from 2014 nowadays there are maybe at least five times more methods out there that people suggest to use compared to regular SGDã€‚

So one thing for example that we also didn't talk about is SGD Nessterof which is a modified version of SGD with momentumã€‚

 It's like this Nessterov momentum in theory it may work better than the regular momentum I also wanted to cover it in this lecture actually I made the slides for that but then I removed them because I noticed otherwise the lecture will be just too long and most people in practice use the regular momentum again anyways so SGd with the regular momentum is also among the best algorithms even though it's the simplest one and I will explain to you in a few slides why so Adam like I said is cool because it doesn't require as much tuning but also SGD is also in terms of generalization performance really goodã€‚

Soã€‚To make this pointï¼Œ so this plotï¼Œ this is looking at the training cost or training lossã€‚

 and you can see in this paper they find that Adam really performs best compared to other methods when it comes to the training cost or lossã€‚

Howeverï¼Œ a training low training cost doesn't imply that yeah final model generalizes well to new dataã€‚

 maybe it's just overfitting So what I mean is of course we want to achieve a low training cost but that alone is not enough we can end up with a low training cost but then if we have a high degree of overfitting then the resulting model still isn't so goodã€‚

 for exampleï¼Œ compared to a model that has a higher training loss but lower overfitting and may have a better generalization performanceã€‚

So to explain you more in detail what I meanï¼Œ here's a visualization from a paper from I actually don't know which year I think it was 2017 I should have included it here butã€‚



![](img/20946313d5e9eab28bf7adbcb9836673_3.png)

Here they also looked at the different optimization algorithmsã€‚

 They looked at the trainingarrow hereã€‚And the test errorã€‚

 So what is interesting is that they find theyve Adam theã€‚

What optimized one like hyperpar tuning and the default oneã€‚ Most people I said use the default oneã€‚

 but in this caseï¼Œ the default one performs really badã€‚ the tuned one performs better hereã€‚ But yeahã€‚

 in contrast to the previous slide that I can't explain in contrast to the previous slideã€‚

 they find that on the training set S GD performs better than Adamã€‚ So HB is heavy ball methodã€‚

 it's another methodã€‚ But this is like a little bit different from what I've shown you in the previous slideã€‚

Nonethelessï¼Œ the takeaway here is more like what's happening on the test setã€‚

 so on the test set that is usually what we care about here they also find that regular SGD performs better than Adamomã€‚

And this is not unique to this single paperï¼Œ this is something that has been observed by other peoples as well that SGDã€‚

 if it's tuned wellï¼Œ it has usually the best generalization performance one possible explanation for that is that SGD is just noisier and because of this noise it's maybe easier to avoid certain sharp minima or it's easier to it's say first of all wiggle out of some local minima but then also maybe to don't find such a good loss so if you find a loss that is very low you may end up overfitting in that case maybe having a higher training loss is maybe sometimes not as badã€‚

Okayï¼Œ so that is one point why people still sometimes prefer S GD compared to more fancy algorithm algorithms like here listed hereã€‚



![](img/20946313d5e9eab28bf7adbcb9836673_5.png)

å—¯ã€‚Yeahï¼Œ there was also a fun paper where theyã€‚Essentially suggest switching from atom to S GD over the course of trainingã€‚

 so they say despite superior training outcomesï¼Œ adaptive optimization methods such as Adam Edgrad or R SPp have been found to generalize poorly compared to S GDã€‚

 So it's essentially what I said in the previous slide that regular S GD can usually yield better generalization performanceã€‚

Then they say these methods tend to perform well in the initial portion of the trainingã€‚

 but are all performed by S GD in the later stages of trainingã€‚

 We investigate a hybrid strategy that begins with training an adaptive methodã€‚

 So using Adam and then switching to S GD when appropriateã€‚

So here they have essentially compared S GD and atã€‚ So using SGD firstã€‚ and then sorryã€‚

 using at first and then switching to S GDã€‚

![](img/20946313d5e9eab28bf7adbcb9836673_7.png)

So yeah here is also againï¼Œ a plot showing the test error showing that with S GDã€‚

 you can end up with a lower test error or a better generalization performance compared to the orange here to atomã€‚



![](img/20946313d5e9eab28bf7adbcb9836673_9.png)

Here's a block articleã€‚Also listing all the strengths and weaknesses of these different algorithms like taking an objective view so here they didn't develop a particular method because usually when you read a paper where people propose a new methodã€‚

 the new method is of course better than the previous methods but here this is someone just summarizing these different methods and coming up with some takeaways so yeah in terms of the state memoryã€‚

 how many memoryã€‚That requires an addition on your GPU because you have additional parametersã€‚

 but then also the hyperparmeter that you need to tuneã€‚So of courseï¼Œ SGD is the simplest oneã€‚

 when we add an additional number of parameters hereã€‚And also tunable parametersã€‚

 I don't know where the second one comes fromã€‚ Yeahï¼Œ one is the learning rate and the other one isã€‚

 of courseï¼Œ the momentum termã€‚ but the betterã€‚ But yeahï¼Œ I was just wonderingï¼Œ okayï¼Œ anywaysã€‚

 so yeahï¼Œ so here they also say that it's the best forã€‚

Generalization but requires extensive trainingï¼Œ so requires more tuning for the learning rateã€‚

 but also more epochsã€‚And then this one accelerates and overcomes weaknessesã€‚So on forthï¼Œ andã€‚Hereã€‚

 with Adamï¼Œ they also say it generalizes worseã€‚Thenï¼Œ SGDã€‚So yeahï¼Œ this isã€‚

Interesting other algorithm atom Wã€‚ So it improves atom in terms of generalizationã€‚

And but requires more state memory it's the same as this oneã€‚

 So it's in that way not much worse than regular Adamã€‚

 but Adam W is also something I've recently seen people using It's a little bit better than the regular atomã€‚

 But yeahï¼Œ if you're interestedï¼Œ you can read this article for more details for more detailed comparison of different methodsã€‚



![](img/20946313d5e9eab28bf7adbcb9836673_11.png)

So here was what an interesting take onã€‚Adam againã€‚

 so people often use Adam and say Adam performs just well across many different architectures and here this person argues this may be because architectures evolved to make Adam the best optimizerã€‚

So hereï¼Œ the person says that it is known that Adam will not always give you the best performance yet most of the time people know that they can use it withoutã€‚

It's defaultã€‚With it yeah so you don't need to change the default parameters and usually you can get good performance on your problem just out of the box so that is also what I observed in practice but here the argument is maybe it's not because Adam worked so well it's because Adam yeah worked so well in the past and now with these new architectures people just keep using Adam and the new architectures that evolved expect that people use Adam so Adam is essentially a good fit for these architectures because these architectures were evolved with Adam in mind so there's like this evolution kind of bias hereã€‚

So hereï¼Œ the person also says usually people try new architecturesã€‚

 keeping the optimization algorithm fixedã€‚ And most of the time the algorithm of choice is Adamomã€‚

 This happens because as explained above Adam is the default optimizerã€‚

 It's like a chicken egg problem in a wayã€‚

![](img/20946313d5e9eab28bf7adbcb9836673_13.png)

So yeahï¼Œ and recently and that was saw that a couple of weeks ago actually in a talk this figureã€‚

 So there was also some additional independent evaluation of these different optimization algorithms and here essentially the argument is really it does not matter what type of optimization algorithm you useã€‚

So on the left hand sideï¼Œ this is a Resnet 18 a residual networkã€‚

 which we will talk about yeah this in next weekï¼Œ so it's a type of convolutional network and here this is a multilay perceptionceptronã€‚

And they train this on Cypher 10ã€‚ So both methodsã€‚ And hereï¼Œ this is the accuracy on the test setã€‚

Andã€‚Sorryï¼Œ the blue one is the test training set and the red one is a test setã€‚

 Let me write this downã€‚Traamã€‚ğŸ˜”ï¼ŒTest and you can see the training set accuracy for both convolutional networksã€‚

And fully connected networks multilay perceptrons here are essentially the same the training set acricaciesã€‚

 Howeverï¼Œ the test at accuracy hereï¼Œ that's a huge differenceï¼Œ soã€‚

Archiecture make can make a huge difference in terms of the generalization performanceã€‚

 even though let's say the training accuracy remains the sameã€‚

 So architecture choice is very importantã€‚ Howeverï¼Œ in contrast to the architecture choiceã€‚

 so here they trainedã€‚VG G 13ï¼Œ which is a different convolution networkã€‚ So for this training hereã€‚

 they used differentã€‚Optimization algorithmsï¼Œ for exampleï¼Œ S GDï¼Œ S GD with momentumã€‚

 S GD with nest of momentum or atom here and you can see that while the training performance is slightly differentã€‚

 maybe the test performance is almost identical you can see that there's almost virtually no difference in the test performance maybe one or two percent points and yeah here really what we can see is thatã€‚

Honestlyï¼Œ the type of optimization algorithm that we use doesn't matter that much as we might thinkã€‚

 It's reallyï¼Œ if you want toï¼Œ let's sayï¼Œ get better performanceã€‚

 it's probably better to look at different architectures rather than different optimization algorithmsã€‚



![](img/20946313d5e9eab28bf7adbcb9836673_15.png)

That being saidï¼Œ there was recently also a new optimizer that was pretty popular or became very popular very quick let's say I saw that in several discussion forumsã€‚

 so there's this atbel optimizer which is essentially a modification of Adamom soã€‚

Here on the left hand side is the regular atomã€‚Optimizer and on the right hand side is this addbel optimizer and in blueã€‚

Alsoï¼Œ I can also use blue here in the blue fontï¼Œ you canã€‚See the little changes to itã€‚

 I don't want to discuss this in too much detail because I want to wrap up this lectureã€‚

 It was long enoughã€‚ But yeahï¼Œ this is another interesting new optimizerã€‚ And of courseã€‚

 you can see it performs better than the other onesã€‚ And blue hereã€‚

 That's the other belief optimizerã€‚ and it outperformsã€‚

 Yeah other optimizers using VG G11 and Renet 34ï¼Œ which are both conversion networks that we will talk aboutã€‚

 And we will also start using Cy 10 thenã€‚

![](img/20946313d5e9eab28bf7adbcb9836673_17.png)