# P54ÔºöL8.4- Logits Âíå‰∫§ÂèâÁÜµ - ShowMeAI - BV1ub4y127jj

YeahÔºå since we talked so much about logistic regression now I thought that might be a good opportunity now to introduce two terms logicits and cross entropy because that's what I will also use yeah quite often later in this class and that's because also it's a very common these are two common terms in the deep learning literature I sometimes refer to it as the deep learning jargon because yeah we sometimes use these terms a little bit differently in statistics compared to deep learning so I just wanted to briefly clarify how they relate to the concepts we just discussed if this doesn't make complete sense„ÄÇ

 I mean we will be using these terms later and you will then have something to refer back to„ÄÇ



![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_1.png)

OkayÔºå so„ÄÇIn deep learningÔºå when we have multi layerer neural networks that's say I will just draw a very simple one here„ÄÇ

We will actually talk about them on Thursday„ÄÇ So if we have a multi layer perceptioncept like this„ÄÇ

Everything is connected to everything here„ÄÇ It's a fully connected layer„ÄÇ

 And then we have this one output node and„ÄÇIn this network„ÄÇ

 we also have net inputs similar to Adeline or logistic regressionÔºå but now we have two net inputs„ÄÇ

 one is hereÔºå let's call that Z1 and one is here at Z2„ÄÇ we will talk more about that on Thursday„ÄÇ

But yeahÔºå indeed deep learningÔºå it's very common to call this here the net inputs that come before the output to call them„ÄÇ

üòîÔºåThe lodges„ÄÇIn statisticsÔºå the logics have a specific meaning„ÄÇ

 This is the logarithm of the odds or lockots„ÄÇAnd„ÄÇIn the context of logistic regression„ÄÇ

 which we just coveredÔºå these los are naturally the net inputs„ÄÇSo where does that come from„ÄÇ

 This is the inverse of our logistic sigmite function„ÄÇOops„ÄÇThat someone„ÄÇ

Logistic sigitete function here„ÄÇ justÔºå I just abbreviated S P„ÄÇ So the lock of P over 1 minus P„ÄÇ

 this function is the inverse of„ÄÇThis function„ÄÇAnd yeahÔºå so indeed deep learningÔºå though„ÄÇ

It does not necessarily any relationship between the lock os and yeah the net inputs of the last there Its just like„ÄÇ

 I think it's derived„ÄÇ the term is derived from logistic regression„ÄÇ

 People call that logicits in the context of logistic regression and then just generalize this wording to arbitrary multilayer perceptrons where we may not even have a sigmoid function„ÄÇ

 as we will see later„ÄÇ So you can just think of it as the net inputs of the odd of the last layer basically„ÄÇ



![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_3.png)

So here here just for a reference„ÄÇ So here on the left hand side„ÄÇ

 the logistic sigoid function and here on the right hand sideÔºå the logicit„ÄÇ

 So the logicit function is the inverse of the logistic sigoid function„ÄÇ

 So basically how it looks like„ÄÇ I don't think I have to go into do that into that in much detail because we have seen that before„ÄÇ

 and this is essentially just flipped„ÄÇ

![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_5.png)

So there is another concept that is maybe a little bit confusing but it's exactly what we've covered before actually we had a seminar at UW last week where we also yeah it was briefly mentioned coincidentally there was like a question whether it's the same the negative log likelihood and cross entropy so yeah the negative log likelihood and the binary cross entropy equivalent and in practice in deep learning people just say cross entropy multicategory cross entropy which would be multi-class version of the negative log likelihood which we will cover later in this lecture when we talk about the softm function„ÄÇ

So„ÄÇJust to keep it briefÔºå the negative look likelihood that we just covered a few videos ago is the same as what people call the binary cross entropy„ÄÇ

 They were just formulated in different contexts„ÄÇ So negative look likelihood comes more like from„ÄÇ

 I think it's like„ÄÇIt's probably from a statistics context„ÄÇ I don't know the first paper„ÄÇ

Or reference that mentioned that„ÄÇ But this is something usually I see in statistics papers and the binary cross entropy„ÄÇ

Thing has originated from the field of information theory„ÄÇAll computer science„ÄÇ

So we have actually seen that or not the cross entropy„ÄÇ

 but we have seen the self entropy or just entropy in statistics 4Ôºå5Ôºå1 for those who took this class„ÄÇ

In fall semester where we had used the entropy function in the context of the information theory and decision trees that we used the lock to instead of the natural algorithm„ÄÇ

 but yeahÔºå it's kind of somewhat related if you have taken any class where you talkedÔºå for example„ÄÇ

 about the KL divergence„ÄÇOr coolbe labla divergence„ÄÇ

 which measures the difference between two distributions„ÄÇ The K L divergence is essentially„ÄÇ

The cross„ÄÇEntropy„ÄÇMus the„ÄÇSelf entropy„ÄÇOf course you don't have to know that it's just like a fun tidbit here the only thing you have to know is or should know because it's useful to know is that the negative look likelihood is the same as the binary cross entropy„ÄÇ

 this is like a useful thing to know and this is yeah what we've discussed in the previous videos and there is also a multicatego version that is the multicategory cross entropy which is just a generalization of the binary cross entropy to multiple classes„ÄÇ

SoÔºå in order to make that„ÄÇNegative look likelihood or binary cross entropy work for multiple classes„ÄÇ

We assume a so-called one hot encoding where the class tables are either0 or one for some reason it was cut off here„ÄÇ

 But this is something weÔºå of courseÔºå haven't discussed yet„ÄÇ And if this doesn't make sense yet„ÄÇ

 we will actually discuss that after the logistic regression code example„ÄÇ

 when I will introduce the multinial logistic regression model„ÄÇ So again„ÄÇ

 all I wanted to say here is the logicits and deep learning usually refer to the net inputs of the layer„ÄÇ

That just comes before the output„ÄÇAnd the term binary cross entropy and negative look likelihood are essentially the same„ÄÇ

 All rightÔºå so in the next video I will show you a logistic regression code example„ÄÇ

 and then we will take a look at this multi category cross entropy„ÄÇ



![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_7.png)

![](img/c56a2035cc8f7b9e12f5cbdda3fecb67_8.png)