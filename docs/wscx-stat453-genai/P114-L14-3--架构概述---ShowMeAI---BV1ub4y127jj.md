# P114ï¼šL14.3- æž¶æž„æ¦‚è¿° - ShowMeAI - BV1ub4y127jj

All rightï¼Œ before we dive deeper into B G G 16ã€‚ No pun intendednet here and Renetã€‚

 let me just briefly recap one or two of the slides I showed you last weekã€‚

 So here was a paper from 2016 that was highlighting some common architectures back in the dayã€‚

 That's already five years agoã€‚ But yeahï¼Œ it highlights really how fast this field is developingã€‚

 So Alex Net was developed in 2012ã€‚ We talked about it lastã€‚ðŸ˜Šã€‚



![](img/c248769f21018736d9f0f37ad20715ea_1.png)

Last lecture and now we can see there are manyï¼Œ many more and this is also not a comprehensive overviewã€‚

 these are only a few of the common selected onesã€‚Good news is that yeah it is not growing exponentiallyã€‚

 I think so many people still work with VGG and Renet in 2021 because they are really good basic base architectures is a backbone for yeah methods on top of itã€‚

 Howeverï¼Œ there are also some additional architectures not listed here that are popular nowadays for instanceã€‚

 So also I should say these are only for classificationã€‚

 We have also another set of architectures for different tasksã€‚

 but yeah other common ones are top of my headï¼Œ maybe mobile net wide residual networksã€‚

 efficient and efficient that here densenetã€‚ðŸ˜Šï¼ŒHighway networksã€‚ But yeahï¼Œ anywaysï¼Œ so these areã€‚

Still a good overviewã€‚ If you want to get started with experimenting with different architecturesã€‚

 I would yeah recommend considering those tooã€‚ Soï¼Œ for instanceã€‚

 also a good takeaway from this one here is thatã€‚

![](img/c248769f21018736d9f0f37ad20715ea_3.png)

It's not only about the number of parametersã€‚ So you can see VG G6 is relatively largeã€‚

 but it doesn't perform as wellï¼Œ for instanceï¼Œ compared to inception networks or here on the residual networksã€‚

 So here these add certain tricks for instanceï¼Œ we will talk about residual networksã€‚

 They have a residual connection which can help with making or having more layers without having such a problem from vanishing gradientsã€‚

But then yeahï¼Œ you also have inceptionï¼Œ which has interesting way of combining different convolutionsã€‚

 So convolutions of different sizesã€‚ I originally plan to talk about it moreã€‚ but yeahã€‚

 due to the interest of timeã€‚ We will be skipping inceptionã€‚ But yeahã€‚

 they are also interesting tricks like combining different convolution sizes into one layer and then also having auxiliary losses like having intermediate losses in the networkã€‚

 So these kind of tricks can also help improving the architecture because we you G 16 is essentially just a convolutional network with 16 layersã€‚

Which are pretty largeï¼Œ but it's not all about thatã€‚ It's also about certain tricksã€‚ for exampleã€‚

 hereï¼Œ Resnet 152 has 152 layersã€‚ It's not possible to have just 152 layers in a regular VG G context because then we would have vanishing gradient problems Recentlyã€‚

 there were some effortsï¼Œ thoughï¼Œ to yeahï¼Œ also developã€‚

Very deep networks without residual connectionsã€‚ And there was also interestingly normalize a free resnetã€‚

 which also gets rid of patch normï¼Œ for exampleã€‚ So there are still interesting developments happeningã€‚

 But yeahï¼Œ againï¼Œ these are the basic architecturesã€‚

 And in the next video I will start by discussing the Vji G16 architectureã€‚



![](img/c248769f21018736d9f0f37ad20715ea_5.png)

![](img/c248769f21018736d9f0f37ad20715ea_6.png)