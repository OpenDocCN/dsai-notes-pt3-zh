# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P41ÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞Èóª #3Ôºå2021 Âπ¥ 2 Êúà 13 Êó• - ShowMeAI - BV1ub4y127jj

YeahÔºå hiÔºå everyone„ÄÇ So all good things come in threes„ÄÇ and I'm back with stuff in the news number3„ÄÇ

 Last weekÔºå I talked a lot about language models for deep learning„ÄÇ And this week„ÄÇ

 my news section will be more focused on computer vision models„ÄÇ

 which is also a little bit closer to my research area because there happened a lot of interesting stuff around computer vision this week„ÄÇ

 there are lots of things to talk about„ÄÇ So let me get started and dive in„ÄÇüòäÔºåYeah„ÄÇ

 let's start with a good old fashioned computer vision application image recognition„ÄÇ

 So here the title of the paper is high performance„ÄÇ

 La scale image recognition without normalization„ÄÇ So here the researchers has developed a deep neural network that can achieve new image classification state of the art results without batch norm„ÄÇ

 So if you encounter this work here SotaÔºå this usually stands for„ÄÇState of„ÄÇ

The art and means that you achieve a state of the art performance in terms of the best possible performance or among the best possible performances compared to other cutting edge state of the art models„ÄÇ

 So hereÔºå what was the novelty„ÄÇ So hereÔºå the novelty is that they achieved this state of the art performance without batch normalization„ÄÇ

 So batch normalization is something we will be covering later in this semester„ÄÇ

 And it's a technique that has been used in the lastÔºå I would sayÔºå4 or five years to getÔºå yeah„ÄÇ

 stable and good performing models„ÄÇIt's like a type of data normalization technique„ÄÇ

 and we will come this in more detail later in class„ÄÇ

 So here what's new is basically that they achieve the good performance now without batch normalization„ÄÇ

 Basically making models simple again„ÄÇ So what did they use here„ÄÇ

 So here they developed an adaptive gradienting clipping technique„ÄÇ

To overcome the instabilities that arise from not using batchome„ÄÇ

 So there have been also some other models recently in a last couple of months or years that achieved state of the art performance without batchome„ÄÇ

 but they were usually more unstable„ÄÇ So more unstable towards little fluctuations in the data set and so forth„ÄÇ

 So here they achieved really good performance„ÄÇWithout this normalization technique„ÄÇ

 so they developed so calledled normalizer free residual networks„ÄÇ

 So restnets or residual networks are also something we will be covering in the semester in the convolal network lecture„ÄÇ

 So we will be likely also revisiting this paper then„ÄÇ

What is also interesting is how they evaluated the performance„ÄÇ

 So here I have a screenshot from the paper on the Y axis„ÄÇ you see the imagenet top one accuracy„ÄÇ

So recall from the introductory lecturesÔºå I I mentioned the challenges with Inet where you can have multiple possible labels„ÄÇ

 So in this way usually people also call or compute the so-called top 5% accuracy because yeah if there is something let's say you have a bus or a car in an image there are multiple possible labels„ÄÇ

 for exampleÔºå it could be a vehicle car automobile and stuff like that„ÄÇ

 So there are multiple possible ways to yeah have a correct label and sometimes there are also multiple objects in an image here yeah„ÄÇ

 for some reason I mean the researchers yeah use the top1 accuracy„ÄÇ

 I think top five accuracy both are yeah important things to look at in any case„ÄÇ

 but it's interesting here„ÄÇThe X axis so here on the X axis the researchers also focused on computational efficiency„ÄÇ

 so here they call it the training latency as I think it's seconds per step on a TU„ÄÇ

 so this is like the tensa processing unit equivalent to a GPU or similar to a GPU and they looked at a batch size of 32 per device so they used multiple devices„ÄÇ

 multiple TUus in parallel„ÄÇYeahÔºå but what is interestingÔºå though„ÄÇ

 is to think of it as the just like the efficiency„ÄÇ So the right hand side„ÄÇ So here it's more„ÄÇ

Expensive„ÄÇAnd this one here on the left hand side would be cheaper„ÄÇ

 make cheaper to train faster to run„ÄÇSo and why is that interesting to look at„ÄÇ

 it's because usually you can also think of it as if you make a model larger„ÄÇ

 you have more parameterssÔºå you will likely get a better performance„ÄÇBut then at the same time„ÄÇ

 also these networks become more expensiveÔºå so depending on your application you are interested also sometimes in smaller models because you cannot run these large models„ÄÇ

 for exampleÔºå on a cell phone or iPhone or something like that where they have on device machine learning algorithms„ÄÇ

But then also at the other handÔºå you otherwiseÔºå still can make the argumentÔºå the together the model„ÄÇ

 the better the performance„ÄÇ And you want to also see whether the method just works for yeah smaller models„ÄÇ

 So here they have different flavors of the method„ÄÇ F 0 to f 5„ÄÇ

 I think these are just different sizes of the model„ÄÇ And you can see„ÄÇIn terms of accuracy„ÄÇ

 their method is yeah better than all the other methods here and these are actually pretty good methods„ÄÇ

 it's like efficient methodsÔºå but it's called efficient net for exampleÔºå Lambdanet„ÄÇ

 I think the stands for data efficient image transformer and so forth so you can see for all types of yeah model sizes their method performs better than these alternatives but again these are different architectures we will talk more about the convolution network architectures later in this course what was interesting really here is that yeah you can also achieve good performance without using batch normalization„ÄÇ

YeahÔºå here's another interesting thing I found this week„ÄÇ

 It's not so much a news article but a Reddit discussion from the Reddit machine learning subdit„ÄÇ

 which has sometimes really interesting and useful insightful discussions„ÄÇ

 So here someone asked about deep learning theory like wondering about the recent advances of or in the theory of deep learning„ÄÇ

 like certain understandings how we think of deep learning„ÄÇ So in the last couple of years„ÄÇ

 there were some„ÄÇHypotheses that were particularly popular„ÄÇ For example„ÄÇ

 the lottery ticket hypothesisÔºå which is essentially aboutÔºå yeahÔºå saying that just by chance„ÄÇ

If you train a large overpoorized network with lots of combinations„ÄÇ

 just by chance there is a high probability that at least one of them will yeah have optimal performance just because you considered so many variations or combinations that just one of them performs well by chance„ÄÇ

 there's a lot of evidence in favor of this hypothesis„ÄÇ

 but I also recall like a couple of weeks ago or months ago„ÄÇ

 I think what was 2020 there was a paper like giving evidence against this lottery ticket hypothesis And in this way I'm actually not sure in how far this hypothesis is still a valid hypothesis anyways„ÄÇ

The other one is the double decentscent hypothesisÔºå which is also particularly interesting„ÄÇ

 So here at the bottomÔºå I have a screenshot of this hypothesis from a different blog post article because it was just a nice image„ÄÇ

 so„ÄÇWhat is shown here is the test and the training error„ÄÇ So in contrast to the previous slide„ÄÇ

 the lower is better„ÄÇ So in the previous slideÔºå we had accuracy„ÄÇ

 so the higher was better here it's the lower is better„ÄÇ And here we haveÔºå for exampleÔºå the view„ÄÇ

 the expected viewÔºå the classical statistics viewpointÔºå so„ÄÇHereÔºå you can„ÄÇ

Think of it in a classical statistics viewpoint that you start with a model„ÄÇThat has a high error„ÄÇ

 Why is the error high„ÄÇ YeahÔºå that is because„ÄÇUsually your model is too simple to capture the complexity in the data„ÄÇ

 so your model doesn't have enough parameters and then it's too simple and cannot achieve a good classification performance because„ÄÇ

 for exampleÔºå think you have think of a nonlinear decision boundary that you need to classify the data„ÄÇ

 but you only have a linear model in this way the model is too simple to capture„ÄÇ

The patterns in the data„ÄÇ And as you make your model more and more complex„ÄÇ

 maybe think of adding more layers to a neural network„ÄÇ

 then your training error will go down and the test error will also go down„ÄÇ

 but at some point the test error will go up again because you have now so many parameters that your model starts overfitting„ÄÇ

 So in this way your model will perform worse with a given size„ÄÇ

 So here they use the residual network„ÄÇ we will talk about residual networks also later in this class„ÄÇ

YesÔºå so this is the classic yeahÔºå statistical viewpoint with„ÄÇLet's maybeÔºå yeah„ÄÇ

 let's consider the modern modern view„ÄÇ The modern view is usually the more data you have the larger sorry the the larger the accuracy„ÄÇ

 but also the smaller the errorÔºå basically So but the reality is that theres usually this bump„ÄÇ

 That is what researchers has yeahÔºå found outÔºå I think in the last couple of years„ÄÇ

 So if you look at it in practiceÔºå you start„ÄÇWith a high accuracyÔºå the accuracyÔºå sorryÔºå high error„ÄÇ

 the error goes down„ÄÇBut then yeahÔºå it goes up again for some reason„ÄÇ So„ÄÇ

 and then there is this bump„ÄÇ and then it goes down againÔºå which is a little bit weird„ÄÇ

 So people call that the double descent hypothesis because you go first down„ÄÇ

 then you go up and then you go down again„ÄÇ So right now„ÄÇ

 I think it's also still a discussion whether this is a valid thing or whether this is maybe due to accidental„ÄÇ

Experimental errors like someoneÔºå maybe add some little bug in their code or something like that„ÄÇ

 But so farÔºå all the evidence shows that this is indeed a phenomenon that can be observed„ÄÇ And yeah„ÄÇ

 that was another interesting hypothesis that is currently still under discussion„ÄÇ

 So if you're interested in more of these types of things„ÄÇCheck out this subd„ÄÇ

 there are way more hypotheses listed below there in the thread so people discuss these types of things„ÄÇ

 and I think it's just interesting if you are curious about these things„ÄÇYeah„ÄÇ

 another interesting research article I saw was this one he entitled Reming biased data to I fairness and E„ÄÇ

 soundss like a relatively straightforward approach„ÄÇ

 like if you have unfair system and this is like also a very important topic due to your data sets sometimes and other reasons machine learning systems can be unfair towards certain„ÄÇ

SubpopulationsÔºå for exampleÔºå and here the researchers tested the idea that removing biased data„ÄÇ

 whether this can improve the fairness and maybe even the accuracy of the system and what they found what was quite interesting is that they achieved a low discrimination of almost 0% when they trained the model on the biased data so by just changing the data said they were able to reduce the discrimination of the model„ÄÇ

But what was also interesting is that they said„ÄÇWhile observed in higher accuracy compared to models trained on the full data„ÄÇ

 so basically they removed biased data points and not only did they improve the fairness of the machinery model„ÄÇ

 but they also achieved a higher accuracy„ÄÇSo and according to the authorsÔºå for example„ÄÇ

 other methods that are being used to make machine learning more fair usually achieve this fairness at the expense of accuracy„ÄÇ

 so usually when people want to make a machine learning model more fair„ÄÇ

 the accuracy goes down and then here they found yeah the opposite you make it more fair and the accuracy goes up which which sounds like a win- win situation„ÄÇ

So here' just a brief overview of what they didÔºå so they started with the biased training data and they had a feature extractor generated similar pairs„ÄÇ

 so similar pair in terms of you have a data point from so maybe let me step back take a step back so they evaluated it on eight data sets here„ÄÇ

where they had multiple attributes and they had also a sensitive attributeÔºå for example„ÄÇ

 the sex of a person or the race of a person and what they wanted to predict isÔºå for example„ÄÇ

 different things like incomeÔºå credit worthinessÔºå exam scores and so forth„ÄÇAnd theyÔºå for example„ÄÇ

 generated similar pairs withÔºå for exampleÔºå someone who had„ÄÇ

 or where some instances where the sensitive attributes were different„ÄÇ

 but everything else was the same„ÄÇSo they found individuals treated unfairly„ÄÇ

 and they trained algorithms based on this biased modelÔºå they then rank these„ÄÇ

Instances by influence of how much influence these instances had on the model„ÄÇ And then here„ÄÇ

 they sorted the training dataÔºå removed the data points and trained„ÄÇ

Model on this the biased dataset set measured the discrimination„ÄÇ

 And then if the discrimination decreasedÔºå they did another round and removed more data points so they kept going until no improvement could be made and they found by by this simple approach they could yeah improve both the fairness and the accuracy„ÄÇ

 which it sounds like a good idea„ÄÇBut I'm not an expert in this area„ÄÇ

 but it sounds like something that yeah should be explored more and maybe also applied to some deep learning models„ÄÇ

 so I found this interesting„ÄÇRelated to also the training data influence„ÄÇ

 I saw this article by the Google AI research teamsÔºå so they had a block article„ÄÇ

 but in the block article is's also a link to the paper to the original paper„ÄÇ

 so they call their method trace inÔºå which is the simple method for estimating the training data influence„ÄÇ

So what they do is during trainingÔºå they record the changes in the prediction caused by an individual training example„ÄÇ

 so they train a model and then look whether the training performance is yeah how much the training and performance is influenced by this particular data point and this technique is especially then useful for detecting outliers„ÄÇ

And also what you can do with this technique is you can explain predictions so to get some insights into the AI model from the training examples rather than the features so most techniques for interpretability usually focus on the features it's basically about saying okay this and this attribute in the training set is important for making predictions here they look at it from the perspective of training examples like which training examples have a large influence on the model and I can imagine if you think back of the previous slide this model on fairness„ÄÇ

 I think those two methods might be also interesting to combine in a way that might be a good or interesting thing to look at„ÄÇ

These influential training examples and how they maybe influence the fairness„ÄÇ

Might be a future research direction anywaysÔºå or could be something interesting for your class project„ÄÇ

 In any caseÔºå so here's a figure from this blog post„ÄÇSo here on the X axis„ÄÇ

 they have the updates during trainingÔºå so the different model updates on the Y axis is the loss and here they take a look at the class zucchini„ÄÇ

And they noticeÔºå for exampleÔºå if they provide a training example that contains zucchinis„ÄÇ

 then the loss goes down as expected„ÄÇ So you go down with a loss„ÄÇ but then for example„ÄÇ

 if you encounter a training example seat beltt which has nothing to do with the zucchini then the training loss for zucchini goes up„ÄÇ

 So if you want to improve the zucchini classification then you shouldn't show it seat beltt samples but then of course your problem is probably a multiclass classification problem where you also have seat belts So it's like a tradeoff right should you show the seat belt„ÄÇ

 I mean then you make maybe the seat beltt classification better but then you lose on the zucchini classification„ÄÇ

 So in this way„ÄÇThey give it here another seatbelt„ÄÇ It goes up„ÄÇAnd„ÄÇ

Here goes down me doing some otherÔºå I don't think they show every training example„ÄÇ

 they only show the most influential onesÔºå the proponents and opponents and seatbel„ÄÇ

 it goes up againÔºå zucchini it goes down by a lot and so forth and sunlasses it's interesting for sunlasses also the loss goes down on zucchini„ÄÇ

Yeah and so here they can kind of better understand what influence the data or data example have„ÄÇ

 for example hereÔºå one seat belt has more influence than another seat beltt and this way you may also find interesting weird outliers in a dataset„ÄÇ

YeahÔºå another interesting research article was related to generative generative adversary networks„ÄÇ

 So this is also something we will be covering towards the end of the class„ÄÇ

 and this might be us in generalÔºå though on interesting application or idea for your class project„ÄÇ

 So there were some projects in the past called this person does not exist where people used„ÄÇ

Genative„ÄÇAdverilial„ÄÇNetworks to generate faces of people that don't exist in real life„ÄÇ

 So for exampleÔºå this person does not exist in real lifeÔºå it's made up by the model„ÄÇ

 And if you go to this website and refresh it every time you will see a different face that does not exist in the real life„ÄÇ

And alsoÔºå similarlyÔºå theres a„ÄÇWebsite called this cat does not exist where people did the same thing for cat images„ÄÇ

 So yeahÔºå you can„ÄÇYeahÔºå generate cat images„ÄÇ Yeah„ÄÇ and now researchers applied the same idea to genome research„ÄÇ

 So he was an„ÄÇArticle linking to a research article that was recently published where the researchers described that in our work„ÄÇ

 we apply a similar concept to genetic data to automatically learn its structure and for the first time„ÄÇ

 produce high quality realistic genomes„ÄÇ So why is that important or interesting„ÄÇ

Currently there's a lot of research being done on DNA and genomes„ÄÇ

 but one problem is really like privacy because yeah genome is unique for a given person„ÄÇ

 so if you do research on genomic dataÔºå you kind of have to be careful about how you handle the data if you have realistic genomes that are totally realistic but don't belong to a certain person then I can imagine this can make certain types of research easier for example„ÄÇ

 if if you train large scale classification models for a different genes and you have a data and your model learns from this data„ÄÇ

The model will con turn contain some information about the data„ÄÇ So it's kind ofÔºå yeah„ÄÇ

 a little bit tricky to make this model available for people or to people who might find it useful because maybe someone would be able to extract some personal information from that model„ÄÇ

 if you train a model on this„ÄÇFake DNAÔºå and it works well on real DNA as well„ÄÇ

 I can imagine this can help with some of these privacy issues around that„ÄÇ



![](img/f978af83cc38bc20a97a5bfd62e47432_1.png)

YeahÔºå one last thing about computer vision or related to computer vision„ÄÇ

 which might also be useful as inspiration for a class project„ÄÇ

So I also saw this project where a companyÔºå I think it's called Reback„ÄÇ

 a company trained computer vision model to estimate the price of a handbag„ÄÇ

So here based on the article an interview with a person„ÄÇ

 they trained it on millions of data points and it took like six year to develop six years to develop this app„ÄÇ

 but yeah it sounds like a fun project where you can take a picture of your handbag and then it estimates the price So it's like a simple straightforward computer vision project I think not too complicated but I guess the trick here was to really get a lot of pictures of handbags and make it accurate So that might be also like an inspiration for a class project of course not with handbags but maybe with something else„ÄÇ

 So yeah these days people maybe sell a lot of cell phones or laptops or something like that So in that way might be also interesting of course it's not only then based on the picture but also some other statistics like processor speed each of the computer and stuff but yeah I think this is also maybe an interesting fun class project to develop a simple computer vision app that can take pictures of something and then yeah estimate„ÄÇ



![](img/f978af83cc38bc20a97a5bfd62e47432_3.png)

The resale value„ÄÇ AlrightÔºå with thatÔºå this is all I have for this news week„ÄÇ

 and I wanted to also wish you a happy Luna New Year„ÄÇ Hope you have a nice weekend„ÄÇ

 and I will see you back in class on Tuesday„ÄÇüòä„ÄÇ

![](img/f978af83cc38bc20a97a5bfd62e47432_5.png)