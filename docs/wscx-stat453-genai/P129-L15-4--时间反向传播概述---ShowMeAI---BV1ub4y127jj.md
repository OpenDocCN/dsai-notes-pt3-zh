# P129ÔºöL15.4- Êó∂Èó¥ÂèçÂêë‰º†Êí≠Ê¶ÇËø∞ - ShowMeAI - BV1ub4y127jj

All rightÔºå let's now take a brief look under the hood of a recurrent neural network„ÄÇ

 So here we are going to talk about be propagation through time„ÄÇ

 This is essentially the be propagation algorithm we have seen earlier„ÄÇ

Except that we now take a look at how it might work if we have this time dimension and also don't worry„ÄÇ

 I won't ask any detailed questions about that in the quiz because yeah this is a complicated topic and here we are only really briefly looking at it so you don't have to know the details here it's just like to illustrate briefly that back propagation also applies to recurrent neural networks„ÄÇ



![](img/c969bd6cc14afbf10e02176ae6238ddf_1.png)

So here I'm showing a figure that is similar or looks similar to what I've shown you before in an earlier video„ÄÇ

 So here we have a recurrent neural network with a single hidden layer„ÄÇOn the left hand side„ÄÇ

 this is the compact notation where we only see„ÄÇThe input and the output here„ÄÇ

 and then we have the hidden layer here with this recurrent edge„ÄÇHoweverÔºå compared to before„ÄÇ

 what's different now is that I have added these weight matrices that we use here„ÄÇ

 So there are three weight matricesÔºå let's say1Ôºå2Ôºå and 3„ÄÇThat's let'srated like thisÔºå oneÔºå2 and3„ÄÇ

What we can see here is that one matrix connects the input to the hidden layer„ÄÇ

 and then there's one mate matrix connecting the hidden layer to the output layer„ÄÇ

These are weight matrices that you would find„ÄÇ So one and two„ÄÇ

 they would you would find them in a regular multi layerer„ÄÇPerceptron„ÄÇHoweverÔºå now in the„ÄÇAre an N„ÄÇ

We have„ÄÇ3Ôºå we have basically all the ones from the multicione plus this weight matrix3 here„ÄÇ

 which is the one from the previous hidden state„ÄÇ So to summarize in a R and N hereÔºå we have two„ÄÇ

Matrices for the hidden layerÔºå One is connecting„ÄÇThe input here„ÄÇSoÔºå the input„ÄÇ

To the hidden there and the other one is connecting the previous hidden there to the current hidden there„ÄÇ

 which is this H H here„ÄÇSo on the right hand side is the unfolded version„ÄÇ

 and you can see so that we have„ÄÇOhÔºå you can see also that we are reusing these matrices so at each time step we use the same matrix here for the input connected to the hidden state„ÄÇ

 and then we also use the same matrices here for each on time step so the same matrix connecting the hidden state to the next hidden state„ÄÇ

And thenÔºå we also have„ÄÇThis year„ÄÇSo what's really new compared to a multi layer percept is that we have„ÄÇ

These in green„ÄÇ So this is newÔºå and this is new„ÄÇ These are weight matrices that we did not have in the multi layerer perceptron before„ÄÇ



![](img/c969bd6cc14afbf10e02176ae6238ddf_3.png)

So how do we computeÔºüNowÔºå the hidden stateÔºå I mean„ÄÇ

This is essentially very similar to computing the regular inputÔºå except that we have no„ÄÇ2Ôºå yeah„ÄÇ

 weight matrices and two inputsÔºå so„ÄÇLet's consider this case when we compute the net input for this hidden state here„ÄÇ

At time step T„ÄÇSo what we do is we like before when we computed the net input in a multiivatorceptron„ÄÇ

 we multiply this weight matrix here„ÄÇWith this input here„ÄÇAnd that gives us the hidden state„ÄÇ

We also may write this differently„ÄÇLike thisÔºå right„ÄÇ

 So we talked about this before in the linear algebra lecture„ÄÇ

 I don't know why I've written this in a different orderÔºå but it's the same thing„ÄÇÂóØ„ÄÇYeahÔºå and then„ÄÇ

This is one inputÔºå The other one is H H„ÄÇFrom the previous hidden stateÔºå t -1Ôºå so„ÄÇ

Maybe to use different colors„ÄÇThis one isÔºå of course„ÄÇ

 this one for So this is an net input for this hidden layerÔºå and then„ÄÇ

This part is computing this part„ÄÇAnd this part here is„ÄÇComputing this part„ÄÇ

And then we add Osoya Allabias here„ÄÇSo the bias is for this hidden state here„ÄÇAlright„ÄÇ

 so this is how it„ÄÇLooks like how we compute the net input and then to compute the activation„ÄÇ

 we would just use an activation function like 10 H or the sigmoid or Relu function„ÄÇAnd yeah„ÄÇ

 this is how we compute the net input now„ÄÇHow do we compute the net input for the output„ÄÇ

 This is exactly like what we do for a multi layer perceptron„ÄÇ So we have one weight matrix here„ÄÇ

 the hidden state and then the bias„ÄÇ And then againÔºå we can use a activation functionÔºå for example„ÄÇ

Softm activation or the sigoid activation„ÄÇ if we have a binary output or yeah or just a linear layer„ÄÇ

 if it's a regression output and so forth„ÄÇ So here this is really the same that we would do for a Moperceptron„ÄÇ



![](img/c969bd6cc14afbf10e02176ae6238ddf_5.png)

Now what about the loss So yeah it really depends on what type of task or sequence modeling task we talk about„ÄÇ

 So if you only want to predict one label for a given text„ÄÇ

 you technically don't need these losses here„ÄÇ you only need a loss„ÄÇ

A single loss from the last one here„ÄÇ Some people argue it might be good though„ÄÇ

 to keep the intermediate losses as well„ÄÇ it helps training the earlier layers„ÄÇ

 So I think I mean depending on who you ask some people keep these losses some don't if you when you compute also many to one sequencing problem and it is different though if you have many too many then you want to have multiple losses so let's take a and look at just the general case„ÄÇ

 so you can have different losses„ÄÇHereÔºå a loss for each time step and when you then want to compute the overall loss„ÄÇ

 you can just sum them up so that would be the overall loss„ÄÇYeah„ÄÇ

 so this would be just looking at the loss„ÄÇ by the way„ÄÇ

 theres a paper on back propagation through timeÔºå what it does and how to do it„ÄÇ

 So here we are really just briefly scratching the surface„ÄÇ

 So this might be some resource to consult„ÄÇAnd also there are probably many different tutorials on the internet also for doing that in detail for this class„ÄÇ

 given that we have several generative modeling topics still to discuss on here we are not going to spend too much time on these details„ÄÇ



![](img/c969bd6cc14afbf10e02176ae6238ddf_7.png)

But yetÔºå soonnana to highlight one of the issues that are„ÄÇApparently„ÄÇ

 when we use spec propagation through timeÔºå let's just briefly focus on how the gradient of a particular loss at time step T is computed with respect to this„ÄÇ

W H H hidden matrix„ÄÇ So hereÔºå let's assume we have loss„ÄÇT at that layer here„ÄÇ

 So what will happen is that we do the back propagationÔºå right„ÄÇ

 so we would compute the partial derivative root or gradient of this loss at the time step with respect to„ÄÇ

Wei„ÄÇüòîÔºåTÔºå so with respect to this one„ÄÇ and then we would compute the gradient of„ÄÇW„ÄÇüòî„ÄÇ

Tea with respect to„ÄÇH„ÄÇüòîÔºåTÔºå so with respect to that one„ÄÇ So here we covered this part„ÄÇ But then yeah„ÄÇ

 so this is essentially what we would do for a regular multilay perceptron„ÄÇ

But then on on top of thatÔºå we have„ÄÇThis term hereÔºå which„ÄÇ

Is essentially the through time step where we have time steps from K1 to T„ÄÇ

 So from up to the very beginning„ÄÇ So we have„ÄÇPartial the reservoir gradient of H T with respect to the one in the very beginning„ÄÇ

 So summing them up„ÄÇ

![](img/c969bd6cc14afbf10e02176ae6238ddf_9.png)

And„ÄÇThose themselvesÔºå we won't go into too much detail hereÔºå but those themselves are a product here„ÄÇ

 and the product is essentially what could cause problems like vanishing or exploding gradient problems„ÄÇ

 so don't worry about the details here too much„ÄÇ so really the I would say the main point here is not really doing this by hand„ÄÇ

 because usually we have autograd implemented in Pywat and stuff like that„ÄÇ

 So we would not have to implement these types of things by hand„ÄÇ



![](img/c969bd6cc14afbf10e02176ae6238ddf_11.png)

But this highlights the main issue that if we have a lot of multiplications„ÄÇ

 we can have these vanishing or exploding gradient problems if we have very large or very small numbers that we multiply multiple times„ÄÇ

 So in the next video I will talk about the so-called longÔºå shorter memory„ÄÇ

 which is one approach to yeah to mitigating this exploding or vanishing gradient problems„ÄÇ



![](img/c969bd6cc14afbf10e02176ae6238ddf_13.png)

![](img/c969bd6cc14afbf10e02176ae6238ddf_14.png)