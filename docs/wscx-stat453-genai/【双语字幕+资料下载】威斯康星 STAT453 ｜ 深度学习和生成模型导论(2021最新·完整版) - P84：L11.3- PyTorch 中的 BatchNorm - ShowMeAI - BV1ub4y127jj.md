# P84ï¼šL11.3- PyTorch ä¸­çš„ BatchNorm - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let's now talk about how we can use batch norm in practiceã€‚ And alsoã€‚

 we will talk briefly about how batch norm behaves during inference becauseï¼Œ yeahã€‚

 you know that during trainingï¼Œ we have mini batchesã€‚

 but that's not necessarily the case if we want to use our network for predictionã€‚

 Sometimes we only have a single data pointsã€‚ So how would that workã€‚So but yeahï¼Œ firstã€‚

 let me show you how patronon works when we use Pytorchã€‚



![](img/663f13830f5f43202285f587c1f50d44_1.png)

I've actually just used the code from last week where I implemented dropout and just made a slight modification where I removed dropout and added a batch layerã€‚

 So I was just accept that reusing the same codeã€‚ So I don't want to walk through this in detailã€‚

 but you can find the code here on Giubã€‚ And yeah here I'm showing youã€‚The main codeã€‚

 the multiceptron that we have to modify andã€‚Yeah so what we have here is1ã€‚

2 hidden layers and the output layer at the bottom and first notice that I'm using again flatten that's because I was working with MN and Mnes is on n times 1 times 28ã€‚

Times 28 dimensionalï¼Œ and flatten will essentially flatten this to a vector to n times 784 dimensional vector where n is the batch sizeã€‚

 and then it will work with a fully connected linear layer hereã€‚

So yeah I here insert batchome after the linear layer notice that there is a 1 dã€‚

 It may be confusing Why is there a 1 d That's because there is a slightly different version of batchome for convolutional networksã€‚

 we will discuss this in the convolutional network lecture where this would be called beome 2 d for the convolution networksã€‚

 So to keep them apartï¼Œ This is called beome 1Dã€‚ This is essentially just the beome that we discussed here in the previous videosã€‚

 So the 1 d is just yet to keep them apartã€‚Yeah and he am doing the same thing in the second hidden thereã€‚

Alsoï¼Œ yeah notice that I set the bias to fault because yet it would be redundant rightã€‚

 because if we compute the net input as let's say the weight times the feature plus B and then in batch we had this plus Bita valuesã€‚

 in that way the bias becomes kind of redundantï¼Œ so it's not necessaryã€‚

 but you can have it there it doesn't really matterã€‚

 I ran it with and without it I didn't notice any difference in this caseã€‚Okayï¼Œ so this is howï¼Œ yeahã€‚

 batchome looks like a full code example can be found hereï¼Œ but there's really nothing yeahã€‚

 nothing really to talk about because it's just two lines of code hereã€‚



![](img/663f13830f5f43202285f587c1f50d44_3.png)

Yeahï¼Œ so I was also just for fun running some experimentsã€‚

Without the bias that I just showed you and having patched on before the activationã€‚

 that's usually how yeah that was originally how it was proposed in the paperã€‚

But sometimes people also nowadaysï¼Œ it's even more common to have it after the activationã€‚

 I will talk more about that in the next videoã€‚ I found some benchmarksã€‚

 some more sophisticated benchmarks I wanted to show youã€‚ So but here in this caseã€‚

 when I ran the code of the multilay perceptioncept that I just showed youï¼Œ I enabled the biasã€‚

 So I'm actually not showing it hereï¼Œ I don't know why I deleted itã€‚ but by defaultã€‚

 bias is true if you don't set anythingã€‚ andã€‚I found it was the same performanceã€‚

I then also inserted peton after the activation instead of before the activation like here or hereã€‚

 So I now instead of here having it before the activationï¼Œ I now have itã€‚

After the activation in both casesã€‚ and I also didn't notice anyã€‚Yeahï¼Œ any difference really hereã€‚å—¯ã€‚

Yeahï¼Œ and then I also ran experiment with dropoutã€‚ in this caseã€‚ Alsoã€‚

 I didn't notice much of a differenceï¼Œ except now the network was not overfitting anymoreã€‚

 The test accuracy for both dropboard cases was slightly lower compared to no dropoutã€‚

 I think I use just too much dropoutã€‚ But I could at least see there was no overfitting anymoreã€‚

 But yeahï¼Œ the comparison here isã€‚å—¯ã€‚Insertingã€‚Ptch on before the activation and then drop outã€‚

 and thenã€‚After the activation and then drop outã€‚ And I also did not notice any difference here in practiceã€‚

 peopleï¼Œ nowadaysï¼Œ it's more common to actually recommend if you use dropout to recommend having beome after the activationã€‚

 And yeah one little fun memory aid to remember that is if youã€‚ðŸ˜Šï¼ŒConsider this caseã€‚So we have Peonã€‚

Then you have the activation and then you have dropoutã€‚ You may call it badã€‚

 and it might be better to have beome after the activationã€‚

 That's typically a little bit more commonã€‚ In this caseï¼Œ I didn't notice any differenceã€‚

 It may make a small difference in other types of networks like convolution networksã€‚ So I wouldã€‚

 if use drop outã€‚ I would probably go with this variantã€‚ But yeahï¼Œ of courseï¼Œ it's somethingã€‚

 It's a hyperparmeter essentially to experiment withã€‚



![](img/663f13830f5f43202285f587c1f50d44_5.png)

Allrightï¼Œ so I have one more thing about dropout and Pythtorchã€‚

So when we look again at our training function hereã€‚

 this is exactly the same training function that I used last week in dropoutã€‚ but againã€‚

 this is again highlightingã€‚Train and evil are important here that we during trainingã€‚

 set our model into training modeï¼Œ because that's where beome will compute the running mean and the running varianceã€‚

 I will talk about this in the next slideã€‚ So hereã€‚

Bchome will actually yeah compute some running statistics during trainingã€‚

And these running statistics are then used in the evaluation mode when we evaluate a model on new dataã€‚

 So during an evaluationï¼Œ you have to imagine that you are mimicking the inference scenario and an inference you may only have a single data point rightã€‚

 so let's say you have the Google search engine and there's just one user running a query and you have a network that has batch So you have to normalizeã€‚

But you don't have a batch of usersã€‚ So only for one userã€‚ So how do we deal with that scenarioã€‚



![](img/663f13830f5f43202285f587c1f50d44_7.png)

So there are two ways to deal with that scenarioã€‚ The easy one would be to useã€‚

A global training set mean and varianceã€‚ So you would compute these meansã€‚

For the features and the variances for the featuresï¼Œ for the whole training setã€‚

 that's something you would also usually do or could do when you compute the input standardizationã€‚

But this is actually not very common in practiceã€‚ for some reasonsã€‚

 So what's more common is to use an exponentially weighted average or a moving average is just a different name for thatã€‚

 So usuallyã€‚Practice people keep a moving average of both the mean and the variance during trainingã€‚

 So you can think of it as also as the running meanã€‚ how it's computed is by having a momentum termã€‚

 It's usually a small value like 0ã€‚1ã€‚ And this is multiplied by the running mean from the previousã€‚

Epochã€‚sorryï¼Œ previous mini batchã€‚And then what you so you have this this termã€‚

 this is like the running mean times the momentum termã€‚ This is a 0ã€‚1 valueã€‚

 And then you have 1 minus the momentumã€‚ This is like a 0ã€‚9 valueï¼Œ thenã€‚Plusï¼Œ yeahï¼Œ plusã€‚

9 times the current sample meanã€‚ So that's the miniã€‚Bch meanã€‚

 and you this do the same thing also for the running variancesã€‚So hereï¼Œ essentiallyã€‚

 this is just like a moving average or a running meanã€‚And you do the same thing for the varianceã€‚

 That's what you keepã€‚ And then during inferenceï¼Œ you use that one to scale the data point that you do a prediction onã€‚

 You yourself don't have to do thatã€‚ Yeah yourselfï¼Œ by the wayï¼Œ by using model evilã€‚

 it will actually happen automaticallyã€‚

![](img/663f13830f5f43202285f587c1f50d44_9.png)

![](img/663f13830f5f43202285f587c1f50d44_10.png)

But yeahï¼Œ here's just like the explanation what's gone happen under the hoodã€‚Okayï¼Œ so yeahã€‚

 that is how batchchome works in Pytorchã€‚

![](img/663f13830f5f43202285f587c1f50d44_12.png)

And in the next videoï¼Œ I want to briefly go over yeahï¼Œ some veryï¼Œ yeahï¼Œ briefã€‚

 a brief rundown of all the types of literature that try to explain how batchaginome worksã€‚



![](img/663f13830f5f43202285f587c1f50d44_14.png)