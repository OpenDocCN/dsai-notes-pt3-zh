# P60ÔºöL8.9- ‰ΩøÁî® PyTorch ÁöÑ Softmax ÂõûÂΩí‰ª£Á†ÅÁ§∫‰æã - ShowMeAI - BV1ub4y127jj

As the last video for this lectureÔºå let me now show you a code example implementing the softm regression model from scratch„ÄÇ

 using this vectorized form that I showed you to show you that it indeed works„ÄÇ Actually„ÄÇ

 I have two code notebooks here„ÄÇ One is the from scratch implementation using a simpler Irish data set„ÄÇ

 And then I will alsoÔºå because I mentioned that before in the lecture„ÄÇ

 I will also show you the Mist example because I'm sure you were all waiting for it„ÄÇ Al right„ÄÇ

 so let's execute the boiler platelate here first„ÄÇGet it getting it out of the way„ÄÇ And here„ÄÇ

 So the data loading„ÄÇThere's nothing really new going on„ÄÇ

 It's the same code I used for these toy datasets and the simple iris data I used before„ÄÇ

 so I don't want to really spend too much time on discussing this again„ÄÇ

 It's essentially the same only that this time we are using three classes instead of two„ÄÇ

So we have now these circlesÔºå upside down triangles and the squares„ÄÇ

 So left hand side is the training set yeahÔºå and the right hand side is the test set„ÄÇ

In the other notebook I will show you also the Pytorch data loader„ÄÇ

 So that is then usually how we load data because in deep learning we usually don't use CV data sets„ÄÇ

 so we usually don't do this procedure„ÄÇ We usually yeah use the data lot I will show you in the next„ÄÇ

Could not book how that looks like„ÄÇYeahÔºå so„ÄÇStarting with our low level from scratch implementation„ÄÇ

 computing them gradients manually„ÄÇ So here I just defined some helper functions„ÄÇ

 one that converts the class label to a one hot format„ÄÇ

 a softmax function and the cross entropy as the loss function know that you don't have to implement any of that when you use Pytorch regularly„ÄÇ

 I meanÔºå the regular version of how people implement code in Pytorch because„ÄÇPyarch will„ÄÇ

Do the one hot encoding yeah internally kind of implicitly when we compute the loss„ÄÇ

 I will show you how that would look like later„ÄÇ And also„ÄÇ

 it has the softm and cross entropy function already implemented„ÄÇ

 so you don't have to worry about this yourself„ÄÇBut yeah I'm just doing it here to really show you that the mathematical concepts from this lecture really translate one to one into code and it actually all works„ÄÇ

So similar to logistic regressionÔºå we start with defining a class„ÄÇ So in the init constructor method„ÄÇ

 we construct the weights and the bias„ÄÇ So we initialize them here to all zeros and notice that weights in contrast to logistic regression in this such mixed regression context„ÄÇ

 it's now„ÄÇMatrixÔºå rightÔºå So we have the matrix numb classesÔºå numb features„ÄÇ So is„ÄÇÂóØ„ÄÇWe used age„ÄÇ

 I thinkÔºå for the number of classesÔºå H times m matrix„ÄÇ

 And because we have also two net inputs for the two outputsÔºå we also have now the bias as a vector„ÄÇ

 rightÔºå So this is H dimensional vector now„ÄÇSo age is the number of classes„ÄÇAnd here„ÄÇ

 here's the forward method„ÄÇ So the forward method is computing the net inputs„ÄÇ

 So I call them logicts„ÄÇ But yeahÔºå these are„ÄÇ you can think of them as the net inputs„ÄÇ It's just„ÄÇ

 yeahÔºå the„ÄÇMultulplication between the inputs and the weights plus the biofactor„ÄÇ

 So this would be an age dimensional„ÄÇNet inputs„ÄÇ So these are our Z values here„ÄÇ



![](img/7df04f90925eeb810ea669cb8002200f_1.png)

Then I'm applying the softmax to get the probabilities„ÄÇ So the probabilities here with that„ÄÇ

 I mean these unitsÔºå aÔºå A 1 and a2 are activations„ÄÇ So we can also call them the activations„ÄÇ

 And these are our„ÄÇAnd I'm returning both here„ÄÇ and yeah here that is the interesting part„ÄÇ

 the backward methodÔºå that is where we compute our gradients„ÄÇ and yeah„ÄÇ

 instead of doing everything with a for loop for each weight individually„ÄÇ

 we can actually use the vectorized implementation I showed you and let me move this over„ÄÇ

 I'm implementing it really one to by one here or I mean like literally it's not making any change we use our x transpose„ÄÇ



![](img/7df04f90925eeb810ea669cb8002200f_3.png)

Times„ÄÇSo with a matrix modification times y minus probability„ÄÇ So Pro is really the activations here„ÄÇ

So this part is essentially this part and the parentheses and then also the transpose again„ÄÇ

 and this is exactly the same as I'm showing you on the slide„ÄÇFor the bias unit„ÄÇ

 it's a little bit simpler because the derivative doesn't include the X termsÔºå so we can just„ÄÇ

So this is a vectorÔºå rightÔºå so we can just compute this sum„ÄÇAlright„ÄÇYeahÔºå this is it„ÄÇ

 So here in the predict labels thingÔºå I'm just applying the arcm to get the index of the highest probability in the activations„ÄÇ

 So that's also what I explained to you earlier„ÄÇ

![](img/7df04f90925eeb810ea669cb8002200f_5.png)

Can actually move that back„ÄÇAlrightÔºå so this isÔºå yeahÔºå allÔºå all that's new here„ÄÇ really„ÄÇ

 the rest here is the same„ÄÇ So the evaluate functionÔºå that's the same as„ÄÇ

The one for logistic regression and the training is also the same„ÄÇ

 Notice the only difference here for the train function„ÄÇ What I did is train method„ÄÇ

 What I did is I added this one hot encoding here to get the one hot encoded class tables„ÄÇBut again„ÄÇ

 yeahÔºå we call forwardÔºå and we call backward to get the gradients„ÄÇ

 and then we update the weights by applying the negative gradients of minus„ÄÇ

So minus the gradients times the learning rate„ÄÇ I'm also normalizing by the size of the mini batches because I told you that it's yeah„ÄÇ

 easier to find a good learning rate this way„ÄÇAnd here this is just for logging again to show our yeah accuracy and loss during training„ÄÇ

 But yeahÔºå againÔºå this is actually not too complicatedÔºå rightÔºå I mean„ÄÇ

 if we look at the backward methodÔºå it's really just two lines of code here„ÄÇ

 if we use the linear algebra compact vectorized form„ÄÇAesha actually mention this is not so trivial„ÄÇ

 of courseÔºå to think about what should go in here„ÄÇ It looks very simple„ÄÇ

 but it might take you hours and hours to think about it„ÄÇ So personally in the first pass„ÄÇ

 if I would ever implement something from scratchÔºå I would implement it at Python 4 loops and then go to the linear algebra„ÄÇ

YeahÔºå implementation„ÄÇ I think that's more maybe easier for me„ÄÇ

 Other people may find it easier to think about in these matrix matrix matrix modification terms„ÄÇ

 I personallyÔºå it's not myÔºå my quite a my thing„ÄÇ I like things simpler„ÄÇ

 even they can be mobile robustÔºå but I'm using thisÔºå of courseÔºå because it's„ÄÇMore compact„ÄÇ

 It's very information dense„ÄÇ Lots of things going on here„ÄÇ It's more compactÔºå though„ÄÇ

 and computationally more efficient„ÄÇBut againÔºå alsoÔºå I should say in practice„ÄÇ

 no one really implements this by themselves anymore because we can use Pytors auto grant„ÄÇ

 which has some computational facilities to do this automatically„ÄÇ AlrightÔºå so let's train this then„ÄÇ

AlrightÔºå so we can see after the first eÔºå we already got a pretty good training accuracy„ÄÇ

 It actually goes downÔºå which is interesting„ÄÇ But yeahÔºå more interestinglyÔºå the„ÄÇ

Cost or the loss here goes also down„ÄÇ So I think this is really due to noise that the training accuracy goes down„ÄÇ

 It's probably overfitting here„ÄÇ But yeahÔºå we see the loss goes down„ÄÇIt's our result„ÄÇ

 And let's take a look at„ÄÇHow it looks like„ÄÇSo here we can see the loss goes down„ÄÇ

 maybe training it more would be even better„ÄÇAnd then it's compute test accuracyÔºå it's 80%„ÄÇ

Let's take a look at how it looks like„ÄÇSo yeahÔºå I'm using function decision regions„ÄÇ

 We have seen that briefly in 36451„ÄÇ if you recall that„ÄÇ

 but you don't have to know this or remember this because in practice„ÄÇ

 we will actually never really work with two dimensional data sets„ÄÇ

 So we never really have yeah the luxury of plotting it„ÄÇ I'm just using a very simple dataset„ÄÇ

 which is why it's possible here„ÄÇSo here's how the decision regions would look like„ÄÇ So you can see„ÄÇ

YeahÔºå the softm regression classifier is able to deal with three classes by classifying them relatively well„ÄÇ

 It's not great in this region here„ÄÇ could have maybe been improved if„ÄÇ

The line would be more like this„ÄÇ but yeahÔºå it's hard to tell„ÄÇ

 it's actually not a trivial problem for a linear classifier„ÄÇ

A non nonlineara classifier may have an easier time here with in this region„ÄÇÂóØ„ÄÇNow„ÄÇ

 but this is also not the point„ÄÇ Now let's take a look at how we would implement it using Pytor's module API„ÄÇ

 So the module API makes many things easierÔºå like I said beforeÔºå so we can use this linear layer„ÄÇ

 which is our yeah net input computation here„ÄÇ So actually we don't need all of this part in a real application„ÄÇ

I'm just doing this part so that we can compare this implementation to our from scratch implementation because above I also used zeros„ÄÇ

 so if I go up„ÄÇI initialized the weights to zeros„ÄÇ So otherwiseÔºå if I don't do that„ÄÇ

Here at the bottom„ÄÇIt will have small random numbers„ÄÇ

 The small random numbers are good for multi layer networksÔºå but yetÔºå it would make it hard to„ÄÇ

 of courseÔºå compare than this implementation with our above if we have different starting weights„ÄÇ

 so„ÄÇYeahÔºå we just add thisÔºå but in a real caseÔºå you would just run it like this„ÄÇAlright„ÄÇ

 but we will also talk more about weight initialization in a later lecture„ÄÇ

 I think like I would say maybe in in two weeksÔºå I will have a lecture on how we can initialize weights and what these random weights are„ÄÇ

 And there's alsoÔºå there are some computational tricks to choosing good weights„ÄÇAll right„ÄÇYeah„ÄÇ

 the forward method is now very simple„ÄÇ We can use the„ÄÇOops„ÄÇThe net input computation here„ÄÇ

 via the linear layerÔºå which we defined here„ÄÇ And then here„ÄÇ

 the softm is our softm activation function„ÄÇ We use that from Pytorch now„ÄÇ So F is from Pytch„ÄÇ

 I think I„ÄÇIported it at the very top„ÄÇ Let me steam„ÄÇYeahÔºå so here this is the torch functional API„ÄÇ

 So an N stands for neural networkÔºå and this is the functional API instead of typing it all the time„ÄÇ

 instead of writingÔºå let's say this dot„ÄÇSoftmax and do the same for other functions„ÄÇ

 People usually implement the function API S F just to make it shorter„ÄÇ

 So you only have to type F softmÔºå basically„ÄÇSo that's just a convention when you work with Pythal code„ÄÇ

Okay„ÄÇSo yeahÔºå and that is basically it„ÄÇ we are using the stochastic gradientding descent optr here„ÄÇ

 optimizing the parameters of that model„ÄÇAnd yeahÔºå this is all„ÄÇ all there is to itÔºå really„ÄÇ

When you use patrony„ÄÇAnd training it nowÔºå you can see test accuracy is also 80%„ÄÇ And actually„ÄÇ

 if you would scroll upÔºå you look at this decision region plot and„ÄÇThis one„ÄÇ

 it looks almost identical„ÄÇ S some slight numerical difference„ÄÇ If I think if you look at„ÄÇ

These values 0„ÄÇ558 and minus0„ÄÇ12„ÄÇLetMe just copy themÔºå maybe„ÄÇTheres some slide„ÄÇMaybe not great„ÄÇ

 really even„ÄÇ YeahÔºå thisÔºå this is one iss a little bit different„ÄÇ You can see„ÄÇ

 but it's only like three digits after the decimal point„ÄÇ This is just a numerical rounding error„ÄÇ

 I would sayÔºå because we implemented lots of things from scratchÔºå like softm and„ÄÇ

The cross entropy function and things like that„ÄÇ And we have the vanilla implementations„ÄÇ

 There are also some numerically more stable implementations of softmax by adding or multiplying the nominator denominator and numerator by C So adding lock C look constant to the exponent that would make it more stable„ÄÇ

 but then it would change the numbers slightly think these differences are because of that„ÄÇ Yeah„ÄÇ

 of courseÔºå in practice you don't want to implement softmax and cross entropy yourself because it's numerically not as stable as not as fast as the implementations that are already provided„ÄÇ

 but yeahÔºå overallÔºå we get exactly the same results„ÄÇ I meanÔºå with small rounding differences„ÄÇNow„ÄÇ

 this is how we do Som regression and Pythrch„ÄÇ Let's now take a look at the same thing for the Mnes data set„ÄÇ

 because yeahÔºå I already showed you the Mnes data set in the lecture„ÄÇ And so just for completeness„ÄÇ

 we will actually be working with this more when we talk about multilay perceptioncepts in the extension„ÄÇ

So what's new now is I'm using a data loader and this is usually how we load images in Pytorch on other types of data„ÄÇ

 So here are just some settings for my model and here the Ms data is already provided in a library let me screw up again in a library called Torch vision it comes with number of datasets So if you install Pytorch it will also if you use the installer via the website it will automatically install Torch vision so you don't have to install it separately it usually is included in the command that you executed when you install Pythtorch„ÄÇ

So„ÄÇWe use this em data set from Torch vision from the dataset sets sub moduleule„ÄÇ

 This is just specifying where to store the data„ÄÇ so it will create„ÄÇ



![](img/7df04f90925eeb810ea669cb8002200f_7.png)

![](img/7df04f90925eeb810ea669cb8002200f_8.png)

If I go hereÔºå it will create a data folder in next to my notebook hereÔºå because I call it data„ÄÇ

 but you can actually change this to whatever you can change it to desktop„ÄÇ

Data or something like that really doesn't matter just where it keeps the data so temporarily„ÄÇ

 So if you go here insideÔºå you will see it will„ÄÇ

![](img/7df04f90925eeb810ea669cb8002200f_10.png)

Keep„ÄÇüòîÔºåCopy of that„ÄÇ

![](img/7df04f90925eeb810ea669cb8002200f_12.png)

ÂØπÂ§¥„ÄÇAll right„ÄÇThen it has a test test and a training set„ÄÇ So train true test„ÄÇ this is for testing„ÄÇ

 if you say train faults„ÄÇ I will actually show you„ÄÇ

 it's a bit unfortunate that Mnes doesn't have a validation split„ÄÇ

 but I will show you in the multi layer perceptioncept lectures how we also can create a validation split for that one„ÄÇ

 This one is kind of required its„ÄÇBecause Mnes is images and they come in a„ÄÇ

 I think it's a P I L Python imaging library format„ÄÇ and you want to convert them to Pyth ten source„ÄÇ

 so„ÄÇThis one is converting the yeah image representation in Python to a Pythage tensor representation„ÄÇ

 but we will also see later we can use other data augmentation here like randomly flipping or cropping images„ÄÇ

 so we will talk about this also more„ÄÇWhen we talk about convolution networksÔºå also„ÄÇ

 I should say that2 tensor normalizesÔºå it normalizes„ÄÇThe pixels to 0Ôºå1 range„ÄÇ

 So it's dividing them by 255„ÄÇSo I should mention that„ÄÇ

 So we don't have to do our data normalization ourselves„ÄÇ Of course„ÄÇ

 we could actually do something better„ÄÇ We could standardize them for colour images„ÄÇ

 Sometimes people standardize using the mean„ÄÇAnd the start deviations from Imnet„ÄÇ

We could technically also do that for colour images for here„ÄÇ

 something like endlessnes it doesn't matter at allÔºå to be honest„ÄÇ

 there's probably no difference in the performance of the model because it's a very simple optimization task here with stochastic gradient descent„ÄÇ

 AllrightÔºå so„ÄÇThis is defining the training data setÔºå this is defining the test dataset set„ÄÇ

 the difference is really that we set train fault and here to true„ÄÇ

 we only need to download at once right so I don't have a download here„ÄÇThenÔºå we„ÄÇUsing the data„ÄÇ

 we define data load„ÄÇ So here„ÄÇI am defining a data lot for the training set„ÄÇ I can set my batch size„ÄÇ

 I set it here above„ÄÇ That's my mini batch size„ÄÇ I set it to 256„ÄÇ

And then whether we want to shuffle or not„ÄÇ So this is shuffling before each epoch„ÄÇ

 shuffling the data set„ÄÇ notice that„ÄÇÂóØ„ÄÇHereÔºå I'm not shuffling the test set because it doesn't really matter„ÄÇ

 I'm loading the test set in batches„ÄÇ It's also not really necessary because we don't do any training and stochastic grain incent on the test set„ÄÇ

 But inÔºå let's say if we work with real image data sets like imagenet„ÄÇ

 the test set can be quite large and it might not fit into memory„ÄÇ So we can also load it one by one„ÄÇ

 BasicallyÔºå that's what I'm doing here„ÄÇHere just for completeness„ÄÇ

 I'm this is more like for verifying that everything looks good„ÄÇ

So here I'm just having a follow loop over„ÄÇThe images and labels in the training order just to make sure everything looks okay„ÄÇ

And you can see it's loading the images in this„ÄÇNÔºå HÔºå sorryÔºå N CÔºå H W format the patch size„ÄÇ

 the number of colour channels„ÄÇ we only have one colour channel because M this is black and white„ÄÇ

 And then we have the height and the width here„ÄÇ and the labelsÔºå of course„ÄÇ

 are a vector because yeahÔºå thiss just the numbers„ÄÇAnd I should also tell youÔºå okay„ÄÇ

These labels are not one hot encoded„ÄÇSo you can see that they are just the numbersÔºå the class labels„ÄÇ

 the 1 class labels„ÄÇIt's not necessary to1 hot encode them because Pytoch does it automatically when we call the cross entropy loss„ÄÇ

 AllrightÔºå so here is now my implementation„ÄÇ This is the same as in in this from scratch here„ÄÇ

 So there's nothing really different„ÄÇSame thing„ÄÇYeahÔºå so also notice„ÄÇ So yeahÔºå the accuracy„ÄÇ

 I'm computing it using a for loop over the data load„ÄÇI don't have to go through this„ÄÇ

 I think this is something you could copy and paste if you need it„ÄÇÂóØ„ÄÇYeah„ÄÇ

 so the interesting part what I wanted to show you is because I don't want to make this video too long I mean I could discuss everything„ÄÇ

 but if you have questionsÔºå maybe ask on PiazzaÔºå we can discuss this more„ÄÇ

 I don't want to spend too much time on things that are not that important right now„ÄÇYeah„ÄÇ

 so the important parts are here that we are concatetnating„ÄÇ

So that goes back to what I've showed you„ÄÇHereÔºå so this would be how I am„ÄÇ

Image looks like it's 3D tensorÔºå one image is a 3D tensor 28 by 28„ÄÇ and then the color dimension„ÄÇ

 So if you don't think of the color dimensionÔºå it would be a matrixÔºå a 28 by 28 matrix„ÄÇ

 but we need a feature vector is input„ÄÇ So we would concatennate it to a very long vector a 28 times 28„ÄÇ

 So 784 dimensional vector„ÄÇ So that's what I'm doing here„ÄÇ So I'm„ÄÇConcatenating it to a vector„ÄÇ

 I can maybe show you„ÄÇthat would it look like for one given„ÄÇLet's sayÔºå we take„ÄÇOne image„ÄÇ

So this would be the one colour channel and 28 by 28„ÄÇ ActuallyÔºå you can alsoÔºå I meanÔºå okay„ÄÇIt's fine„ÄÇ

 I think so view„ÄÇmin-1„ÄÇ28„ÄÇüòîÔºåTo8„ÄÇ so you can see now it's a one long vector basically„ÄÇSo in this case„ÄÇ

 I think it's a matrix because we have the first dimensionÔºå but„ÄÇSo about the same thing applies„ÄÇ

 so here„ÄÇYeahÔºå we do that for the„ÄÇSo actuallyÔºå this would be for the batch dimension one more time„ÄÇ

 SorryÔºå images„ÄÇSo this should be 25Ôºå 256„ÄÇTimes„ÄÇ7Ôºå84 dimensional„ÄÇMatrix„ÄÇ

So this is for the is or basicallyÔºå this is like how our design matrix looks like it's then„ÄÇ

And n times„ÄÇM dimensional matrix now where M is the featuresÔºå and is the batches„ÄÇIt got rid of„ÄÇ

 notice it got rid of the color channel here„ÄÇ there is no colour channel because we only define two a's„ÄÇ

If we do it like thisÔºå it would have a colour channel„ÄÇ but yeahÔºå we don't want a colour channel here„ÄÇ

YeahÔºå and then we call our modelÔºå known as when we use Pythrch„ÄÇ

 we don't use dot forward because it's actually better to just call it like like this„ÄÇ

 it will internally call forwardÔºå but it will do some extra checks before it calls forward„ÄÇ

 as I explained in an earlier lecture„ÄÇAnd now the interesting part is how we compute the cross entropy cost or lost„ÄÇ

Here„ÄÇAnd notice that is important that it uses the logicts as input„ÄÇ

So it's not using the probabilities„ÄÇ It's using the logics„ÄÇ

 This was what I explained in the in a previous video„ÄÇ This is like the slightly confusing part„ÄÇ

 I plain explained it inÔºå in this„ÄÇOther previous video„ÄÇ

 So we have to pay attention that we provide the los and not the probabilities here„ÄÇYeah„ÄÇ

 and then we zero the gradients to the backward„ÄÇWe are justÔºå yeah„ÄÇAveraging the costsÔºå here„ÄÇ

Don't know why I did that„ÄÇDon't need that„ÄÇ OhÔºå I okayÔºå I see„ÄÇI have„ÄÇ

I compute the average cost over the epochÔºå I see„ÄÇYeahÔºå so then I'm plotting it„ÄÇOkay„ÄÇ

 I need to execute this part„ÄÇSo this cost is the average cost per epoch„ÄÇ

I could technically compute the cost by„ÄÇCalling forward on doing„ÄÇÂì¶„ÄÇThis one for the whole data set„ÄÇ

But it would be more workÔºå right„ÄÇ So you would have to load the whole data set„ÄÇ

 So I'm just computing the average„ÄÇIt might take a while„ÄÇIt's not too badÔºå thoughÔºå it's like„ÄÇPoin„ÄÇÂóØ„ÄÇ

int4 minuteÔºå04 minutes„ÄÇPer„ÄÇüòîÔºåÈÇ£‰∏ç„ÄÇüòîÔºåIt's not even„ÄÇ I don't know if these numbers make sense„ÄÇ

 One that makes maybe sense„ÄÇAlrightÔºå so„ÄÇNowÔºå yeahÔºå this is how the loss looks like„ÄÇ

 You can see it goes down as expected„ÄÇ Let's take a quick look at test accuracy„ÄÇ92„ÄÇ

16 iss actually pretty goodÔºå right„ÄÇ I mean given that this is an image data set and we only have a linear classifier„ÄÇ

 we don't even use a neural networkÔºå I mean a multilayer neural network„ÄÇ

 we already get a pretty good performance„ÄÇ And personally„ÄÇ

 I also recommend you when you work on classification tasks„ÄÇ

 always to run also logistic regression classifier or softm regression classifier as a baseline just to get a feeling of how difficult the classification problem is„ÄÇ

 right So if you train let's say convolutional network and you've got only 94% with a„ÄÇ

Convolutional networkÔºå you would sayÔºå okayÔºå this is such a complicated convolutional network model„ÄÇ

 Why does it only get 94% when I get already 92% with logistic regression„ÄÇ

 Maybe my model with 94% isn't that goodÔºå after all„ÄÇ

So it's always a good thing to run also logistic regression as a baseline„ÄÇYeah„ÄÇ

 here's just an example„ÄÇLearning three random images from the dataÔºå7Ôºå2Ôºå1Ôºå and 0„ÄÇ

 And then here I'm doing my predictions„ÄÇSo calling this one on the feature vectors and then the arc max next to get the class labels„ÄÇ

 rightÔºå because„ÄÇThis one would give us just the probabilities„ÄÇ

You can see so we are taking the arc mix„ÄÇ So for the first oneÔºå let's take a look„ÄÇ

Should be this one should be the highest0Ôºå1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6Ôºå7„ÄÇ

This is the highest it actually yeah yeah should be the highest one„ÄÇ So in this way„ÄÇ

 we compute the class tables here just to show that these are indeed correctÔºå alrightÔºå so„ÄÇWith that„ÄÇ

 yeahÔºå this is all I think I have for this from implementation„ÄÇ and then in the next lecture„ÄÇ

 I will talk about multi layer perceptionceptrons„ÄÇ

![](img/7df04f90925eeb810ea669cb8002200f_14.png)