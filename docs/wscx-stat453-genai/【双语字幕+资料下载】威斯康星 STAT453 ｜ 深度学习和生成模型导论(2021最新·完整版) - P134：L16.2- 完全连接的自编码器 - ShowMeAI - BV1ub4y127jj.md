# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P134ÔºöL16.2- ÂÆåÂÖ®ËøûÊé•ÁöÑËá™ÁºñÁ†ÅÂô® - ShowMeAI - BV1ub4y127jj

All rightÔºå let's now talk about the auto encoders„ÄÇ Now„ÄÇ Finally„ÄÇ

 we are starting with fully connected auto encoders becauseÔºå yeah„ÄÇ

 they are the simplest type of auto encoder„ÄÇAnd you can think of such an autota as an hourlass shaped multilayer perceptron„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_1.png)

So here's a visualization of a very simple type of auto encode are fully connected„ÄÇOuter encoder„ÄÇ

 So you can really think of it as a multi day upceptron„ÄÇ So here„ÄÇLet's start by„ÄÇ

Let's start by looking„ÄÇOnly8„ÄÇWhat I circled hereÔºå this is the so called encode quota part„ÄÇ

EssentiallyÔºå this is a fully connected layer„ÄÇ So you can think of it in the context of torch by torch„ÄÇ

 you can think of it as a„ÄÇLinear layer„ÄÇFor the connected layer with here of five inputs and two outputs„ÄÇ

And then on the other side hereÔºå we have„ÄÇA decoder„ÄÇ

And the decoder is essentially also fully connected layer„ÄÇNowÔºå with the opposite„ÄÇ

2 inputs and five outputs„ÄÇ So it's essentially reversed„ÄÇ It's going back into the original dimension„ÄÇ

And„ÄÇIn betweenÔºå we have something that we call„ÄÇThe hidden units are sometimes we also use the word embedded space or latent space„ÄÇ

 a latent representationÔºå for instanceÔºå or a bottleneck because yeah it's it's like a bottleneck here and„ÄÇ

The outputs would be the reconstructed inputs„ÄÇ So what we are doing is we are projecting„ÄÇThis„ÄÇ

Features into a smaller dimensional space„ÄÇ And then we are reconstructing them„ÄÇ

I will show you in the next couple of slides why that's necessary here it's just the basic setup„ÄÇ

 I'm just showing how it looks like„ÄÇAnd if we would have it set up like this„ÄÇ

 it would be identical to the principal component analysis that I showed you in the previous video„ÄÇ

Except with their difference„ÄÇThat hereÔºå there is no orthogonality constraints„ÄÇ So there is no„ÄÇ

Explit way to ensure that feature1 here and feature 2 in the embedded space are orthogonal„ÄÇ

 so that would be different from the principal component analysis„ÄÇ

 principalnc component analysis has the constraint that the features are orthogonal because we are extracting eigenvectors„ÄÇ

Another difference here is„ÄÇI said that„ÄÇ We have„ÄÇA fully connected layer and a fully connected layer„ÄÇ

 but of courseÔºå you know that would not be I meanÔºå it would be kind of boring because then we only have a linear transformation here and in practice it would be more powerful to have a nonlinear transformation or the ability to yeah„ÄÇ

 learn nonlinear transformations„ÄÇSo in practiceÔºå we would have connected to this fully connected layer here when it's„ÄÇ

I'm not trying to visualize this exactlyÔºå but this should wÔºå this is an activation„ÄÇFunction„ÄÇHere„ÄÇ

 that should symbolize an activation function„ÄÇ And similarlyÔºå here„ÄÇ

 we would have an activation function after the fully connected layer„ÄÇ So„ÄÇ

 what I mean is we would have tor dot an n dot linear and then let's say torch„ÄÇBut„ÄÇRu„ÄÇAnd„ÄÇ

 and I think it's an un functionalal„ÄÇFal„ÄÇRelo„ÄÇYou could also see its a smaller case in a little„ÄÇ

So we would have linear layer„ÄÇNon linear activation functionÔºå just like in a multireceptron„ÄÇ

 then againÔºå a linear layer here„ÄÇ And then we would have another„ÄÇActivation functionÔºå let's say„ÄÇ

Tochdot thickm„ÄÇI will explain to you why sigma here when we talk about the implementation later„ÄÇ

I don't want to explain too any things on this one slide because it's already a little bit crowded„ÄÇ

 But the bottom line is we have„ÄÇA fullyly connected layerÔºå a non nonlinear activation„ÄÇ

 fully connected layerÔºå non nonlinear activationÔºå and I'm showing you here something where we only have two units in the center„ÄÇ

 but this is like arbitrary„ÄÇ It could also be more than two„ÄÇAndÔºå of course„ÄÇ

 it could also be more than„ÄÇOne fully connected layer and only nonlinear activation I could have multiple fully connected layers the same way you can have a multi layer perceptron with one hidden layer„ÄÇ

 two hidden layersÔºå3 hidden layers and so forth„ÄÇ So the number of hidden layers is also arbitrary here I just try to keep it simple to fit it also onto the slide„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_3.png)

So„ÄÇYeahÔºå like I said beforeÔºå if we don't use the nonlinear activation function„ÄÇ

 this would be similar to principle component analysis„ÄÇ

 but in practice we would use nonlinear activation functions„ÄÇ

 so it's not like principal component analysisÔºå it's actually more powerful than principal component analysis because we can learn nonlinear transformations„ÄÇ

YeahÔºå the one missing piece I didn't mention in the previous slide is what the point is of having the inputs and then the outputs„ÄÇ

 which are the reconstructed inputs„ÄÇ This is so that we can learn this transformation„ÄÇ

So what what we are doing is we are computing the difference between the inputs and outputs„ÄÇ

 and we use that for back propagation so that the auto can auto encoder can learn to yeah„ÄÇTo a good„ÄÇ

Compression of the data„ÄÇ So what happens is„ÄÇThe auto encoder goes from this large dimensional input to this small„ÄÇ

Representation to this embedded space map this is smaller„ÄÇLet's sayÔºå this is large„ÄÇThis is smaller„ÄÇ

 and then it's large again„ÄÇAnd„ÄÇHow do we learn that„ÄÇ

 How do we know that this is a good representationÔºå I mean„ÄÇ

We could just arbitrarily come up with two numbers„ÄÇ

 How do we know it represents our original data well„ÄÇ

 How do we know that it's a good way of compressing this data„ÄÇSoÔºå so this is why we„ÄÇ

Have to project it back here„ÄÇ So we are projecting back into the original space„ÄÇ

 and then we can compare so then we can compare the outputs to the inputs„ÄÇ

 And what we want is we want them to be similar„ÄÇAnd if they are similar„ÄÇ

 so if I have input data and I'm able to reconstruct this output data„ÄÇ

 or I have my output similar to the inputÔºå that means that my embedded space here„ÄÇ

 my latent representation has to have important information captured about my input data because if this in the center would not contain any useful information„ÄÇ

 then we would not be able to reconstruct the inputs via the outputs„ÄÇ

 So this is really the setup is to ensure that the auto quota is indeed able to retain the most useful information in this embedded space„ÄÇ

And there are different loss functions for that„ÄÇ AndÔºå of courseÔºå yeah„ÄÇ

 the simplest one would be just the L2 difference or mean squared error„ÄÇ So we are„ÄÇ

Just computing for each input here„ÄÇ So we have five inputs and also five outputs„ÄÇ

 We would be compare comparing them one by one„ÄÇSo let me use a different colourÔºå maybe„ÄÇ

Which colour should I useÔºå It seems pinkÔºüSo we would compare„ÄÇThis oneÔºå this one„ÄÇThis one„ÄÇThis one„ÄÇüòî„ÄÇ

This one„ÄÇSum them up„ÄÇHereÔºå and then we could also average like one over„ÄÇM„ÄÇ

 let's say M for the dimensionalityÔºå like the mean squared error„ÄÇIf we would put it„ÄÇas one over M„ÄÇ

Okay„ÄÇYeahÔºå this is how the basic setup of an auto encoder looks like„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_5.png)

YeahÔºå so here have a question that if we can achieve the same thing that we can achieve with an out encoder„ÄÇ

 just using PCAÔºå which is essentially a linear transformation kind of matrix factorization that is very efficient more efficient„ÄÇ

 in fact than let's say using back propagation with stochastic gradient descent to train the out encoder„ÄÇ

 why do we bother about out encoders in the first place well„ÄÇI meanÔºå like I said before„ÄÇ

 principle component analysis is a linear transformation and out ands can be more powerful than that„ÄÇ

 we can learn non nonlinearar transformations and for instanceÔºå if we work with image data„ÄÇ

 we can swap out the fully connected layersÔºå let's say with convolution layers and so forth„ÄÇ

 So we have just moreÔºå let's sayÔºå opportunities to make good models for different types of data that is more complicated„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_7.png)

AlrightÔºå so also a potential applications of this auto encoder„ÄÇ So in practiceÔºå of course„ÄÇ

 there is no point„ÄÇ And let's say just reconstructing the image becauseÔºå well„ÄÇ

 if we already have the input imageÔºå why would we be interested in that reconstruction„ÄÇ So here„ÄÇ

 the reconstruction in the regular auto encoder is only used for computing the„ÄÇ

Means squaredarrow loss for learning„ÄÇThe embedding space here„ÄÇAnd after trainingÔºå we can„ÄÇ

 after the auto coder has been trainedÔºå we can„ÄÇYeahÔºå we can remove this part„ÄÇ

 I will show you also in a code example later how that works„ÄÇ

 We can remove that part and then only use that embedding„ÄÇAs our extracted features„ÄÇ So„ÄÇ

 for instanceÔºå some peopleÔºå if we have a large data set may train„ÄÇOutten on a large dataset„ÄÇ

 generate all the embeddings and then try out whether„ÄÇ

 let's say training a classifier on top of it helps like using traditional machine learning like support vector machines„ÄÇ

 Can neighborers random forests„ÄÇ Of course you can also yeah use this directly like a multilayer perceptron with one hidden layer you have let's say you make this bigger and then you have the output layer„ÄÇ

 you can have a multilayer perceptron but if you don't want that you can also yet train the autoen on a large data where where you don't maybe have label information So you can apply this to a large unlabeled data because autoenderrs don't require label information whereas a normal multilayer perceptron would require that„ÄÇ

 So for instanceÔºå you can„ÄÇSimilar to transfer learning„ÄÇ

 train your auto and code on a very large data set„ÄÇ

Then it learns how to extract good features and let's say you only have a you have only labels for a subset of that data for that„ÄÇ

 then you can useÔºå let's sayÔºå traditional methods and train those on the embeddings„ÄÇ

You can also use the latent spaceÔºå so„ÄÇThe lower dimensional space for visualization purposes„ÄÇ

 If it's two dimensionalÔºå for example„ÄÇ SoÔºå for instance„ÄÇ

 if you want to do exploratory data analysis in a S plot„ÄÇBut of course„ÄÇ

 it's not restricted to do dimensions„ÄÇ you can also have a larger„ÄÇ So for instance„ÄÇ

 if you want to be doing clustering or something like that and the original input space is too large for computing pairwise similarities„ÄÇ

 you may also want to considerÔºå yeah using this for dimensionality reduction„ÄÇ HoweverÔºå I must say„ÄÇ

That in a real world applicationÔºå I would probably not recommend an auto quota for dimensionality reduction„ÄÇ

 for non nonlinear transform dimensionality reduction„ÄÇ There are better techniquesÔºå for instance„ÄÇ

 if you are interested in„ÄÇVisualizationÔºå let's sayÔºå in exploratory data analysis„ÄÇ

Converting your data into a two dimensional space for visualization is a technique called T distributed stochastic neighbor embedding Tni„ÄÇ

 Or there's also another technique called U mapÔºå which can be also used for dimensionality reduction„ÄÇ

 These are a little bit moreÔºå I would sayÔºå robust„ÄÇ So these are better techniques for dimensionality reduction„ÄÇ

 But yeahÔºå O codes are still very good techniques for„ÄÇ

Let's say learning on unlabeled large unname data and then having something as an input to traditional machine learning and classifier„ÄÇ

 but of course„ÄÇThere are many more interesting aspects and applications of auto encoderss„ÄÇ

 which we are not covering this first lectureÔºå but in the next lectureÔºå for example„ÄÇ

 we will be talking about variational out encoders and this out encoder here would yeah form the basic yeah basic model behind this variational out encoder would so learning about this out encoder here will help us understand how variational out encoder works next lecture„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_9.png)

So here's an implementation of a simple autoer encoderÔºå just using fully connected layers„ÄÇ

 So using applying it to the„ÄÇEmist data set„ÄÇ So on the left hand sideÔºå I have a hand written digit„ÄÇ

 the handwritten digit 7„ÄÇIn MnesÔºå it's a 28 times 28 dimensional image„ÄÇ

 So I'm reshaping this to a 784 dimensional feature vector„ÄÇ

 And this goes into my fully connected layer that is followed by a leaky relu„ÄÇ

So this fully connected layer compresses the input image from 784 to 32 dimensions„ÄÇ

 so it doesn't have to be two dimensions„ÄÇ it can also have more than two dimensions„ÄÇOf courseÔºå and„ÄÇ

Then I'm havingÔºå or I'm using another fully connected layer that converts the 32 dimensional„ÄÇHidden„ÄÇ

Space here back into a 784 dimensional representation and this fully connected layer is followed by a syigmoid activation„ÄÇ

 Why sigmoid activationÔºå that's because I normalize the input images„ÄÇ

 the pixels such that they are range between 0 and 1„ÄÇ

 So the syigmoid will also output values in range between 0 and 1„ÄÇ

 So I'm applying the sigmoid to get the same values as the input has„ÄÇ

This are the same range of pixel values„ÄÇSo this will also be 0 between 0 and 1„ÄÇ

 And then I can compute the mean squared error or minimize the mean squared error between„ÄÇ

The pixels here„ÄÇSo here is the or here a snapshot of the original am„ÄÇ

Images and at the bottom are the reconstructed ones„ÄÇ and you can see they are not perfect„ÄÇ

 there are some artifacts hereÔºå but overall it looks pretty good„ÄÇ

 So our very simple auto encoder is able to retain enough information in this 30 dimensional space to reconstruct the original images„ÄÇ

 And if you think about itÔºå 32 is approximately approximately like 20 times smaller than„ÄÇ

About 700 dimensional input„ÄÇ So I was able to here reduce the side2 fold„ÄÇ

If you're interested in this code it's here I have it here on GitHub I won't walk you through this code because in the next videos I want to show you a convolutional auto encodeder which is slightly more interesting and I implemented some code for this class also to make this more interesting so we will be focusing one more interesting example of a convolutional auto encodeder„ÄÇ

 but if you're interested here you can also find the code for the simple auto encodeder which is essentially very similar to a multilayer perceptron„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_11.png)

Okay so in the next video let me then introduce some concepts about convolutional autoenrs„ÄÇ

 it involves transposed convolutions and deconvolutions„ÄÇ

 or it's just a different name for the same thing and then we will after this next video we will implement this in Pytch„ÄÇ



![](img/7dd83d16ce2e06647e981f1004671470_13.png)