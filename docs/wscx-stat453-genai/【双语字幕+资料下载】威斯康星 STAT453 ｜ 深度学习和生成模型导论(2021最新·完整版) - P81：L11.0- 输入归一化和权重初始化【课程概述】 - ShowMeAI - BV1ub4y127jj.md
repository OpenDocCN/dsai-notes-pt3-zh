# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P81ï¼šL11.0- è¾“å…¥å½’ä¸€åŒ–å’Œæƒé‡åˆå§‹åŒ–ã€è¯¾ç¨‹æ¦‚è¿°ã€‘ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hiï¼Œ everyoneã€‚ So last weekï¼Œ we fell into a little bit of a rabbit hole when I started talking about different ways we can improve neural network training and the generalization performanceã€‚

 Of courseï¼Œ we can't cover all the topics I outlined on that mind mapã€‚ Howeverã€‚

 I think there are a few important ones that are really worth talking aboutã€‚ So in this lectureã€‚

 I want to talk about batch normalization and also a little bit about different weight initialization schemesã€‚

 which are also kind of important to make neural networks work well in practiceã€‚ğŸ˜Šã€‚

And then in the next lectureï¼Œ I want to briefly talk about optimization algorithmsã€‚

 And then yeah we will move on and talk about cooler things like convolutional networks and recurt neural networksã€‚

 But for nowï¼Œ let's talk about batch normalization because that's really a good technique for making the network train better and fasterã€‚

All rightï¼Œ yeahï¼Œ just to recap again today we are talking about tricks for improving deep neural network trainingã€‚

 So today in particular we will talk about feature and input normalization and focus on a topic called batch norm and then I will also talk about weight initialization schemes when we initialize the model weights and there are two particular techniques we are going to take a look at there's the Xavier Got initialization scheme and the caing he initialization schemeã€‚

Yeahï¼Œ and then in the next lecture we will talk about optimization algorithms for improving gradient descent learningã€‚

 for instanceï¼Œ as R Mepro Edaggradï¼Œ Adamï¼Œ and I will also provide you with a list of manyã€‚

 many more algorithmsï¼Œ but yeah personally I think stochastic gradient descent with momentum and Adam are still the most yeah useful or at least most popular onesã€‚



![](img/b49d7e51f6ec3aa2b77261781938312a_1.png)

Alrightï¼Œ so as an overviewï¼Œ things we are going to cover todayã€‚ So I structured it into seven partsã€‚

 Some of these parts will be very shortï¼Œ but I try to yeah structure it as always so that they go by topic and that it is somewhat organizedã€‚

 So firstï¼Œ we will just briefly talk about input normalizationã€‚

 Then we will switch to batch normalizationã€‚ I will show you how we do batch normalization in Pytorchã€‚

 And then we will have a discussion of how why batch normalization worksã€‚

 So I will explain here how it works like the techniques the procedureã€‚ and thenã€‚

We will revisit this here and discuss why it may have a positive effect in practiceã€‚

 so it's still a controversial topic so its still not clear and I will highlight some of the theories then we will switch topics in this section here and talk about these different weight initialtization schemes like why we care about that to different examples and then also how it works in Python Alright so with that let's get started with input normalizationã€‚



![](img/b49d7e51f6ec3aa2b77261781938312a_3.png)