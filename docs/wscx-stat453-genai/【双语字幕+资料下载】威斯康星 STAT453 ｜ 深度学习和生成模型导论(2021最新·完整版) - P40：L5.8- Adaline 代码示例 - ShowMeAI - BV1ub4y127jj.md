# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P40ÔºöL5.8- Adaline ‰ª£Á†ÅÁ§∫‰æã - ShowMeAI - BV1ub4y127jj

YeahÔºå now let me show you a code example of„ÄÇLinear regression and Adeline trained with gradient descent„ÄÇ

 So I prepared two coat files„ÄÇ One is called linear rigor for regression G D„ÄÇ So G D here„ÄÇ

 this would be the regular gradient descent version„ÄÇ

 And then I will train the Adeline model with stochastic gradient descent„ÄÇ However„ÄÇ

 note you can also do it the other way around„ÄÇ You can train the linear regression model with stochastic gradient descent„ÄÇ

 And you can train the Adeline model with gradient descent„ÄÇ So it's kind of interchangeable„ÄÇ

 And if you likeÔºå you can play around with that later on„ÄÇüòäÔºåSo let's do one thing at a time„ÄÇ

 So I have my watermark here again just to show you what software versions I'm using here„ÄÇ

 And then now here the linear regression model with gradient in descent„ÄÇ

 So what we are doing is we are training this model here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_1.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_2.png)

So the training is up to this point and then the threshold function is only used later on when we do the prediction of the class labels that's after training so for training we don't need the threshold function„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_4.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_5.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_6.png)

All right„ÄÇAnd yeah again like know that they are very similar„ÄÇ

 So here on the left yeah on the left hand side I have an overview of the code that I'm implementing below„ÄÇ

 so now these two are exactly what I implemented in these two notebooks I was just screenshoting it to show you that they are indeed the same I have them on the left hand side as linear regression on the right hand side as Adeline and they are exactly the same the same code except after training we can for Adeline then here use a threshold function to produce a class labels„ÄÇ

 but now that these are indeed the same codes that I used here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_8.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_9.png)

So we will go over these codesÔºå of courseÔºå in this notebook„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_11.png)

AlrightÔºå so we initialize some stuff that we will be using some libraries„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_13.png)

I prepared a toy data set here„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_15.png)

Just some simple data set containing two featuresÔºå x1 and x2„ÄÇ

 So tail is showing the last five lines of the data set of the CB file„ÄÇ

 and there are 1000 training points„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_17.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_18.png)

So„ÄÇYeahÔºå we have two featuresÔºå x1 and x2 and one output value Y„ÄÇ So this is a regression problem„ÄÇ

 So it's a continuous output value here„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_20.png)

So okayÔºå let's start by„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_22.png)

Creating here our design matrix and Pytorch„ÄÇ So here we will have a design matrix consisting of the two features and then the class table as a separate tensor„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_24.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_25.png)

I'm also shuffling the data setÔºå so I'm shuffling it and dividing it into a training and a test set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_27.png)

So I'm again using this index trick that I explained to you earlier later we will be using so called data loads when we use more complicated data sets like large image databases„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_29.png)

Where we have then some automatic functions in Pyr that do these steps for us„ÄÇHere„ÄÇ

 I think it's sometimes good to just see how you would do that conceptually„ÄÇ

 like seeing or looking at the code to see what we are doing here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_31.png)

So what I'm doing here is I'm creating these shuffle indices so let me show you how they look like„ÄÇ

 So these shuffle indices are just the indices 1 to 1000 shuffled and then I use them to select the training data„ÄÇ

 So first I'm actually using it to shuffle the data set So here this is just the shuffled indices I select all the data points in the shuffled way„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_33.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_34.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_35.png)

For both the training and the for bothÔºå sorryÔºå the features and the labels„ÄÇSo note that here„ÄÇ

 because we use the same shuffle indexÔºå they are shuffled in a way that is consistent because otherwise if we use different shuffle indices„ÄÇ

 X might be shuffle differently than y„ÄÇ And this is also why we just can't use simply this shuffle function that is implemented in Ny or Pytor because the shuffle function would shuffle them individually and then features won't match to the corresponding„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_37.png)

Labeled anymore„ÄÇ So what I mean is so this oneÔºå this entry here might be then belonging to train data point 99998 by excellent„ÄÇ

 if we just shuffle them individuallyÔºå so this is why we use a consistent shuffle index„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_39.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_40.png)

Then I select 70% of the data„ÄÇ So this is just a size hope„ÄÇHow many data points are 70%„ÄÇ

 So let me this should be 700 like that rightÔºå likeÔºå let me double check that„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_42.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_43.png)

So yeahÔºå700„ÄÇ But this is just a general implementation that would work with any size of data set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_45.png)

And yeahÔºå this is for how then I'm selecting the training and the test set„ÄÇ So the first„ÄÇ70%„ÄÇ

 the first seven data points will be trainingÔºå and the remaining 70% will be for testing and the same thing for the labels„ÄÇ

 And then here I'm normalizing„ÄÇ So this procedure is also called„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_47.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_48.png)

Sanditization„ÄÇIt's kind of yeah giving the data the properties of a standard normal distribution„ÄÇ

 it won't be a standard normal distributionÔºå so we don't change the shape of the distribution„ÄÇ

 but it will have the same parameters that means it will have zero mean and unit variance„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_50.png)

AlrightÔºå so after we've done thatÔºå we now implement our linear regression model that is where it becomes more interesting„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_52.png)

So we use this class based implementation because this is also something that we will be doing in Pytorch later for complex models„ÄÇ

 So Pytorch gives us a lot of convenience functions like convolutional layers„ÄÇ

 fully connected layers and so forthÔºå but we still apply the same concept by saying what our parameters are and then how we use all parameters in which order„ÄÇ

 The backward function is later something Pytorch will implement for us automatically„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_54.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_55.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_56.png)

So Pythch will actually create the backward function for us„ÄÇ

 we don't have to do it ourselves here we do it ourselves because this is like a very low level implementation where I do things from scratch„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_58.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_59.png)

In PythÔºå later we can just use a fully connected layer and the backward function will be done automatically„ÄÇ

But I think here for nowÔºå it's maybe easier or not easier„ÄÇ

 but at least like better for your understanding if we walk through this step by step„ÄÇ

And do it ourselves before we rely on automatic ways of doing that„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_61.png)

So first we define the number of features in the initialization method„ÄÇ

And then we assign the weights of our model„ÄÇ We just use zero weights„ÄÇ

 Can we also small random numbers for this model„ÄÇ It does not matter„ÄÇ We initialize our weights„ÄÇ

 We needÔºå we need to know the number of featuresÔºå rightÔºü So we use that information here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_63.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_64.png)

BecauseÔºå yeahÔºå if I screw upÔºå the number of weights depends on the number of input features„ÄÇ

 of course„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_66.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_67.png)

Let me go down againÔºå we have a separate bias unit because it's computational a bit easier to handle so we don't have to modify the input features when we get them from the training set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_69.png)

So we also initial to 0„ÄÇ We only have one bias unitÔºå so there's only one value„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_71.png)

And then here we compute the net inputs„ÄÇ So that is similar to the perceptron that we've used before„ÄÇ

 So we multiply xÔºå the features and the weights„ÄÇAnd we add the bias„ÄÇ

 So I'm writing this in a complicated way you can technicallyÔºå I think you could write it also as x„ÄÇ

Its„ÄÇSelf„Éà weights„ÄÇPlus„ÄÇOf bias it should do the same thing„ÄÇ

 I' not sure why I was using it So in a complicated way„ÄÇ

 I think I just wanted to show you that I was using Pyth here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_73.png)

YeahÔºå so then we have the activation„ÄÇ So the activations are just the in inputs because the activation function is a linear function„ÄÇ

 So I could also„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_75.png)

Could have done something like that„ÄÇ self deviation„ÄÇFunction X„ÄÇXÔºå and then„ÄÇUse this one„ÄÇ

 social yourself„ÄÇAnd then„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_77.png)

Just„ÄÇTo this basically„ÄÇ So it would be the same thing should also work„ÄÇ Does not really matter„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_79.png)

Okay„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_81.png)

And we get the activation„ÄÇ So now the view here is just to make sure that the shape is the same shape„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_83.png)

That a we sorry that it is like a single vector„ÄÇ So it's„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_85.png)

One dimension„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_87.png)

Not two dimensional„ÄÇIt just for convenience„ÄÇ And then now in the backward method„ÄÇ

 what we are doing is„ÄÇWe are computing the gradient„ÄÇ This is what we have done on this in the slides„ÄÇ

 So rememberÔºå the gradient is„ÄÇWhy so the gradient of the loss function with respect to Y head„ÄÇ

 So starting at the back„ÄÇ So we're doing in two steps„ÄÇ So this is„ÄÇ

So if our loss function it write down againÔºå if our loss function is y„ÄÇMus Y hat„ÄÇSquaredÔºå then„ÄÇ

The derivative is two times yÔºå minus y„ÄÇHeadÔºå rightÔºå sorryÔºå that should be the other way around„ÄÇ

This way„ÄÇAnd then this should be the derivative„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_89.png)

All rightÔºå so that's what we have here„ÄÇAnd then we compute the gradient of y head with respect to the weights„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_91.png)

Which was x and the gradient of y had with respect to the bias is -1„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_93.png)

And then we put these two things together„ÄÇSo using the chain ruleÔºå the inner times al„ÄÇ

 this is what we have done in the slides„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_95.png)

So we multiply the weights as the transpose with Y hat„ÄÇ So this is a little bit tricky„ÄÇ

 all these types of thingsÔºå the view and things like that„ÄÇ

 This is really like to make the dimensions match„ÄÇ So later on we won't have issues with that when we use Pyto automatic functions because they will take care of of it for us„ÄÇ

 if we do things manually we always have to make sure that the dimensions are right„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_97.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_98.png)

And thenÔºå because we compute„ÄÇOr we have gradient in descent based on the whole training set„ÄÇ

 we divide by the size of the data set„ÄÇ This is basically for the mean square error instead of the sum squared error„ÄÇ

 So this is our„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_100.png)

The loss of the sorryÔºå the gradient of the loss with respect to the weight here„ÄÇ

 And this is the partial derivative of the loss with respect to the bias„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_102.png)

And then we return the negative gradients„ÄÇ So this is exactly what we have done in the slides„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_104.png)

Now„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_106.png)

We have a method where we compute the follow pathÔºå the predictions„ÄÇ

 and then we can compute the negative gradients„ÄÇ and these gradients will be used then for updating„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_108.png)

The weights to minimize the loss function„ÄÇ So here we have„ÄÇ

 we have the training and evaluation functionsÔºå so„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_110.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_111.png)

I will check something yeah„ÄÇ So here we have„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_113.png)

The loss function„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_115.png)

And yeahÔºå that's the mean„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_117.png)

Means„ÄÇE lost here„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_119.png)

And here we are training our model„ÄÇ So now we have this photo„ÄÇ This is„ÄÇBch gradient„ÄÇDent„ÄÇ

 So we have the training functionÔºå which takes our modelÔºå the input data„ÄÇ

 the number of epochs and the learning rate„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_121.png)

So we also take yeahÔºå keep track of the cost or loss„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_123.png)

Typed it as cost here„ÄÇ but you can think of it as a loss„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_125.png)

ÂóØ„ÄÇYeahÔºå so what we do is we iterate then over the epochs„ÄÇ So for each epoch„ÄÇ

 we compute the predictions„ÄÇ So this will be a vector„ÄÇ

 My head will be a vector because the predictions are for the whole dataset set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_127.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_128.png)

Then we compute the negative gradients using our backward model method„ÄÇ

And then we update the weights and the bias„ÄÇ So like we discussed in the slidesÔºå this isÔºå yeah„ÄÇ

 the learning rate times the negative gradient„ÄÇLet me see if I can bring up the slides here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_130.png)

Â±Å„ÄÇSome‰πà it's not„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_132.png)

ÂóØ„ÄÇJust want to show you where we are„ÄÇ YeahÔºå we are at this step here right nowÔºå where we update„ÄÇ

 basically so„ÄÇWe compute the negative gradients here in step B„ÄÇ

 and then we update right now with these negative gradients„ÄÇ So that's what is' going on here„ÄÇ

 The learning rate times the negative gradient„ÄÇ So that's how we update the weights„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_134.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_135.png)

And this is it„ÄÇ And then we go up here„ÄÇ So hereÔºå this is just for logging that we are just logging„ÄÇ

We are just logging„ÄÇThe loss„ÄÇCurrent loss„ÄÇAnd„ÄÇüòîÔºåPrint some outputs„ÄÇ

 We print which epoch we are currently in„ÄÇ and the means quite error loss„ÄÇ

 because then we can see whether it decreases or not„ÄÇ Actually„ÄÇ

 we don't have to compute that one because„ÄÇWe already had that from the first iteration It like a little bit wasteful„ÄÇ

 I don't know„ÄÇWhy I did that actually„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_137.png)

YeahÔºå so„ÄÇI think yeahÔºå because otherwiseÔºå this would be the first I see now„ÄÇ

 because this would be the the zeroth epoch„ÄÇ And this is after we updated„ÄÇ

 So I could actually technically remove that„ÄÇAnd then show it to you with„ÄÇ

Startuding at the zeroth epoch„ÄÇ But then we won't know what the last„ÄÇGradient update looks like„ÄÇ

 rightÔºå Because when we update the weights and then we don't compute the last prediction with updated weights„ÄÇ

 we don't know how the final model looks like„ÄÇ So this way„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_139.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_140.png)

In the last iterationÔºå we know also how the final model looks like„ÄÇ I meanÔºå we can do it other way„ÄÇ

 otherÔºå in other waysÔºå we could have this actually„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_142.png)

Let me see we could actually comment this out„ÄÇ and then outside the for loopÔºå we could„ÄÇ

When the epochs here have finishedÔºå we can compute„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_144.png)

The predictionÔºå but it was just fewer codeÔºå fewer lines of code and a little bit simplerÔºå alright„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_146.png)

Because hereÔºå everything is fast anyways„ÄÇ alrightÔºå that's„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_148.png)

To find it„ÄÇ And let's now run this model„ÄÇ So here I am now calling this train function„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_150.png)

OopsÔºå I have an error„ÄÇ OkayÔºå I have not„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_152.png)

Not executed this„ÄÇ So we have to define this and define this„ÄÇ Now we can„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_154.png)

Still not train it„ÄÇActation function is not defined„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_156.png)

I don't know why this should not be defined„ÄÇ Oh yeahÔºå it should be itself„ÄÇ

This is also something I just added so you don't have to use this activation function„ÄÇ

 not doing anything anyways„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_158.png)

OkayÔºå so now it was training here„ÄÇ OkayÔºå you can see we start with a relatively high lossÔºå like 1532„ÄÇ

 notice that let me scroll up again to the data setÔºå these are relatively large values„ÄÇ

 which is why the mean squared error loss is so large„ÄÇ

 if I would divide all these values by a factor of 10„ÄÇ

 These mean square error losses would also be smaller by a factor of 10„ÄÇ So the absolute number„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_160.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_161.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_162.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_163.png)

ItDoesn't really matter„ÄÇ So you don't have to really pay attention to the absolute number„ÄÇ

 What you want to see is the relative decrease that it goes down over time here„ÄÇ

 you can see the longer we train the smaller the means square error becomes„ÄÇ So it's training here„ÄÇ

 And you can see there's a point where it doesn't go down anymore„ÄÇ it stays at 371„ÄÇ So that means„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_165.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_166.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_167.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_168.png)

It kind of convergesÔºå and it has probably reached a global minimum„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_170.png)

So okayÔºå's what we see here„ÄÇ So let's plot the cost nowÔºå because remember„ÄÇ

 I was keeping track of the costÔºå which is the loss over each epoch„ÄÇ It's just like to„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_172.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_173.png)

YeahÔºå not con The loss for the„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_175.png)

Stochastic grand deworth„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_177.png)

The one over the epochÔºå but it doesn't really matter the terms are useÔºå usually used interchangeably„ÄÇ

 Where was I YeahÔºå here„ÄÇ So here I have the cost we can also call it lossÔºå actually„ÄÇ

 it's maybe easier„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_179.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_180.png)

Let's call that loss and„ÄÇIÔºå I think I have the loss function that is called loss„ÄÇ

 which is why it was confusing„ÄÇ OkayÔºå let's leave it at cost here„ÄÇAlright„ÄÇ

 so we compute the cost or inice it to an empty list here„ÄÇ And then in each iterationÔºå we„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_182.png)

Add the current loss to it„ÄÇ So this will be a list of all the losses over the training„ÄÇ

 So this will be a list of these values here so we can plot that and look take a look at how it looks like„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_184.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_185.png)

YeahÔºå so we can see it starts out very large and then it goes down smoothly until it converges„ÄÇ

 So here at that pointÔºå we can say the model has converged„ÄÇ

 It has probably reached the global minimum of the loss function„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_187.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_188.png)

AlrightÔºå so now after we train the modelÔºå we can now use it for prediction„ÄÇ So let's now„ÄÇ

Predic on the training set and the test set„ÄÇ So this is similar to the training just for reference„ÄÇ

 let's compute the mean squared error on the training set and then to the test set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_190.png)

AlrightÔºå so you can see here the training set MSE is lower than the test set MSE„ÄÇ

 which means the model is overfitting a little bit to the training setÔºå but it's not too bad„ÄÇ

 I would say„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_192.png)

ÂóØ„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_194.png)

NowÔºå let's also compare that with the analytical solution„ÄÇ So if IÔºå if you recall„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_196.png)

Somewhere hereÔºå I showed you„ÄÇIn the slidesÔºå the analytical„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_198.png)

SolutionÔºå I don't know where I headed it„ÄÇüòî„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_200.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_201.png)

I think it was yeah here„ÄÇ So we have this analytical solution where we get the optimal weights using these matrix the matrix inverse here times x transpose y„ÄÇ

 this is the analytical solution„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_203.png)

So let's compare our gradientd descent solution with the analytical solution to see whether they match„ÄÇ

 So ideallyÔºå they should be very close„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_205.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_206.png)

So here are the model weights from our„ÄÇOwn model from our gradient descent model„ÄÇ

 So we see for the weights feature  one weight is 0„ÄÇ36 and feature 2 weight is „ÄÇ379 approximately„ÄÇ

 and the bias is minus 0„ÄÇ54„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_208.png)

NowÔºå here is the analytical solution where I implemented what I had on the slides„ÄÇ

 but I just showed you„ÄÇ So when we execute thatÔºå you can see this one and this one„ÄÇ

 they are very close and also these are virtually in identical and also the bias unit is identical so we can see„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_210.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_211.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_212.png)

This gradient descent implementation worksÔºå and it gave us yeahÔºå the global minimum here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_214.png)

YeahÔºå an ungraded homework exercise just for you so Mike the task for you is modify the training function such that the data set is shuffled prior to each epoch do you see a difference Yes or no„ÄÇ

 so you can try this out in practice but you can also just think about it whether you would see a difference whether you shuffle prior to each epoch or not and yeah try to come up with an explanation for your observation or intuition and then maybe post this on piazza just to check your understanding whether shuffling would make a difference here yes or no„ÄÇ

So you can try that out in practiceÔºå but also you would be able„ÄÇ

 I think to answer this without even changing the code„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_216.png)

AlrightÔºå now let's take a look at the„ÄÇAdd a line model with stochastic gradient descent„ÄÇ

 So we could also implement it with gradient descent„ÄÇ It doesn't really matter„ÄÇ

 But here we used a stochastic gradient descent a mini batch mode because this is something we will also be using later on when we implement deep neural networks„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_218.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_219.png)

So againÔºå these steps are all the same„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_221.png)

OkayÔºå just yeahÔºå using a different data set„ÄÇ NowÔºå we are using the iris data set where we have four features„ÄÇ

 x 1Ôºå x2Ôºå x 3 and x 4„ÄÇ So that's the se lengthÔºå Sple width petal length and petal width„ÄÇ

 this is our class tableÔºå the predicted class table„ÄÇ So they are actually„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_223.png)

Simplified it so that there are only two class labels„ÄÇ So let me see„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_225.png)

So this should be classable 0 yeah the first data points„ÄÇ

 So I'm only using data point 50 to 150 hereÔºå I think yeah„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_227.png)

SoÔºå here I'm„ÄÇYeahÔºå I'm applying class label one„ÄÇ So here I'm selecting„ÄÇ

 So what I'm doing precisely is I'm selecting only data points 50 to 150„ÄÇ

 And then this would be class label 1„ÄÇTo 1 and2Ôºå but I changed them to 0 or 1„ÄÇ

 Otherwise our function would not work„ÄÇ So I binaryize our class ables„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_229.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_230.png)

This is just to make this problem work for Adeline and also that same would be true for the perception later we will be seeing models that can also deal with multiple class labels„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_232.png)

So hereÔºå this is exactly the same code as beforeÔºå where I'm just using„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_234.png)

The shuffle indices now use 70% again for training and 30% for testing„ÄÇ

 and I'm also standardizing the features„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_236.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_237.png)

Just to visualize how the data looks like„ÄÇ So this is our data set to classes„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_239.png)

Blue dots and orange dots here„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_241.png)

And this is for the training setÔºå how the training set looks like„ÄÇ Sorry„ÄÇ

 the test set looks like above is the training set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_243.png)

And now we are implementing our Adeide model„ÄÇ Note that this is exactly the same as before„ÄÇ

 So I don't have to go over this again„ÄÇ So this is exactly„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_245.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_246.png)

Like the same code„ÄÇ And so here I have made screenshots side by side„ÄÇ

 So you can see if you look at it closelyÔºå there is no difference between the two„ÄÇ

 So how we got that again is by let me see„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_248.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_249.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_250.png)

By deriving„ÄÇWhat did I have„ÄÇWas a long lecture„ÄÇHere„ÄÇ

 so this is basically what we do in the backward passÔºå where we compute„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_252.png)

ÂóØ„ÄÇSeeÔºå so where we compute the derivativeÔºå so„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_254.png)

This is the outer derivative„ÄÇ And these are the inner derivatives„ÄÇ So if I go back„ÄÇ

 we have the out derivativeÔºå This is„ÄÇThis part hereÔºå where we have the power rule„ÄÇ

 we bring the two up front here„ÄÇ So this is this part here„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_256.png)

And then„ÄÇThis is the inner part„ÄÇ This is hereÔºå this xÔºå basically for„ÄÇThe weightsÔºå that is the x„ÄÇ

 And for the bias unitÔºå it's just -1„ÄÇ because if we would compute the derivative of„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_258.png)

This one with respect to the bias unitÔºå this would be one„ÄÇsorryrryÔºå why are we here So hereÔºå sorry„ÄÇ

 if we compute the partial derivative of this net input with respect to the bias unit„ÄÇÂóØ„ÄÇ

This one will be„ÄÇLet me ride the stone somewhere„ÄÇSo we have an W transpl x plus w„ÄÇ

 If we derive with respect to B„ÄÇ So this would be a constant„ÄÇ So this goes away and a derivative of„ÄÇ

This one with respect to B is oneÔºå rightÔºü So in this oneÔºå in this caseÔºå the derivative would be one„ÄÇ

 but we are also interested in the negative gradient„ÄÇ so it would be-1„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_260.png)

Alright„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_262.png)

ÂóØ„ÄÇYeahÔºå allright„ÄÇ So and then we„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_264.png)

To find the training and evaluationeration functionsÔºå this is the same as before„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_266.png)

Computer the loss„ÄÇ We have our training functionÔºå the same as before„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_268.png)

Because nothing really changes because we don't use our threshold function yet„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_270.png)

Let's see„ÄÇ our model trains„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_272.png)

Not defined„ÄÇ YeahÔºå I am„ÄÇForgetting to execute them step by step here„ÄÇ

 which is an issue that can happen in Jupiter notebooks„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_274.png)

AlrightÔºå so nowÔºå yeah we see we start again with a very large„ÄÇRoss nowÔºå ohÔºå sorry„ÄÇ

 I mentioned it was the same„ÄÇ but one difference is now we're using knee batch gradient descent So batch gradient descent„ÄÇ

 So reallyÔºå the difference is that„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_276.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_277.png)

We have now these mini batches hereÔºå so before„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_279.png)

Maybe just showing that side by side„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_281.png)

before we are just iterating over the epochs here hereÔºå we iterate over the epochs and inside„ÄÇ

 we also iterate over the mini batch„ÄÇ So I'm creating a mini batch„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_283.png)

HereÔºå sorryÔºåmi„ÄÇShufflingÔºå and I'm creating multiple mini batchs here by splitting this„ÄÇ

Into mini batches of a certain size„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_285.png)

So I'm using a size 10„ÄÇ So I'm splitting it into multiple vectors where each„ÄÇ

Vectctor will contain1 attend0 training examples„ÄÇ And then for each„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_287.png)

Mini batch„ÄÇ Im doing my forward pass„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_289.png)

And so forth„ÄÇ So now I have the mini batch training so that I have„ÄÇIf I have 1 of 50 so 10Ôºå1Ôºå2Ôºå3Ôºå4Ôºå5„ÄÇ

6Ôºå7Ôºå I have 7„ÄÇMini batchs in each iterationÔºå because here I have 700 training examples„ÄÇ

So I can divide them into„ÄÇ10 mini batches with„ÄÇ70 training examples each„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_291.png)

So we start out with a large meanqua arrowÔºå and then it you can see it goes down„ÄÇ

 but it is now now a little bit noisier„ÄÇ You can see it goes down here„ÄÇAnd down„ÄÇ

 But then it goes up a little bit„ÄÇ sorryÔºå and you can see then it's a little bit noisy at these updates„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_293.png)

YeahÔºå and then overall per epochÔºå we can see that„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_295.png)

Things go down„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_297.png)

ÂæóÎûçÂæóÂæó‰∫Ü„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_299.png)

So at some point hereÔºå we have converged„ÄÇ so we can see it's fluctuating a little bit„ÄÇ

 but it's not going down significantly„ÄÇ It's going a little bit up and down„ÄÇ

 So it's a little bit noisier than the gradient descent„ÄÇ

 The mini batch gradient descent is usually a little bit noisier„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_301.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_302.png)

All right„ÄÇSo time to evaluate it„ÄÇ So it's easier actually to look at it in a plot compared to these lists„ÄÇ

 This is something if you train complicated neural network models„ÄÇ

 it might be helpful to print it on a command line on a server if you don't have any plotting functions available„ÄÇ

 but after training it's usually good to at least take a look at this„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_304.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_305.png)

Because here it's a little bit more clearer to see so you can see it goes down very steeply and then you can see it fluctuates so it's not converging completely because yeah it's more stochastic so there's more noise„ÄÇ

 but overall we can say it kind of converged maybe we could have changed the learning rate making the learning rate a little bit smaller and later on we will also learn about something called learning rate schedule us where we make the learning rate smaller over time so that we converge better also„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_307.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_308.png)

AlrightÔºå so let's take a look at the model weights and the bias„ÄÇ

From the training and then compare to our analytical solution againÔºå so we can see the weights are 0„ÄÇ

07 and 0„ÄÇ4 hereÔºå 0„ÄÇ488„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_310.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_311.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_312.png)

And yeahÔºå if we look at our analytical solutionÔºå it's„ÄÇCledÔºå you can see it's quite close„ÄÇ

 but it's not as close as before before we used gradient descent instead of stochastic gradient descent„ÄÇ

 so before it was a little bit more accurate„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_314.png)

So but you can change this code also to do gradient descent later on„ÄÇ

 when we talk about multi layer networksÔºå yeahÔºå regular gradient descent wouldn't really be very useful„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_316.png)

So it's actually good to use stoy gradientnesscent here as a practice already„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_318.png)

AlrightÔºå so let's take a look at the prediction accuracy now„ÄÇ

 So how good is our classifier that we trained„ÄÇ So for this oneÔºå we now need the threshold function„ÄÇ

 So we set the threshold at 0„ÄÇ5„ÄÇ So this is if I go back to my slides„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_320.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_321.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_322.png)

So this is what I'm doing right now hereÔºå so for a given valueÔºå if the value exceeds 0„ÄÇ5„ÄÇ

 the prediction exceeds 0„ÄÇ5Ôºå we assign at class value class level 1 if it's below 0„ÄÇ

5 we assign at class level 0 so this is what I'm implementing here if my predictions are greater than 0„ÄÇ

5„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_324.png)

We predict once„ÄÇ So I have actually prepared a„ÄÇBe of once„ÄÇ

 So this is one limitation of the torch where function„ÄÇ So how the torch where function works„ÄÇ

 If you remember from the perceptronÔºå is if you have a value„ÄÇSo if you meet this condition here„ÄÇ

 then you return this value„ÄÇ If you don't meet this conditionÔºå you return the second value„ÄÇ

 So I'm preparing these vectors already as output„ÄÇ So I don't have to recreate them„ÄÇ

 So this is the ones„ÄÇ and this is the zeros„ÄÇ And now I'm also then computing„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_326.png)

The mean„ÄÇ So what is the meanÔºå It's basically the number of„ÄÇ

 So we are computing the sum of the number of ones on the correct predictions here„ÄÇ

Cause I have the equal sign„ÄÇ So it's maybe going onÔºå or there's a lot of things going on in one step„ÄÇ

 I could make„ÄÇMaybe a separate stepÔºå predicted„ÄÇLabels and sign that„ÄÇHere might be easier to read„ÄÇ

 and then„ÄÇHave would like this„ÄÇItMight be easier to read„ÄÇSo hereÔºå firstÔºå I'mÔºå like I said before„ÄÇ

 I'm creating these„ÄÇPredicted labels„ÄÇAnd then Im checking how many of the prelude labels match my true labels„ÄÇ

 So this will be a vector of zeros and ones„ÄÇAnd then I sum up all the onces and divide by the number of„ÄÇ

Data point„ÄÇ So this is what the mean is doing„ÄÇ So it's yeahÔºå basically telling me on average„ÄÇ

 how many are correct„ÄÇSo it will be a value between 0 and 1„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_328.png)

YeahÔºå and I do the same thing here for the test set„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_330.png)

So let's execute that„ÄÇ There we get 90% training accuracy and 96% „ÄÇ6% test accuracy„ÄÇ

 It's a small data setÔºå usually you will find that in a real world case„ÄÇ

 the test that accuracy is usually lower than the training set accuracy because of overfitting and stuff like that„ÄÇ

 But this is a very small data set„ÄÇ And this might be just due to chance„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_332.png)

So just to illustrate in what this value here is„ÄÇ So this a vector of zeros and one„ÄÇ or sorry„ÄÇ

 truths and faults„ÄÇ But you can actually convert it to integers„ÄÇ and then it should be zeros and one„ÄÇ

 and then you can compute„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_334.png)

ÔºåSomeÔºå so this gives you the number of one„ÄÇAnd then if I divide it by the number of training data points„ÄÇ

 which is let's say„ÄÇThey both do size„ÄÇSo this is my 90% here„ÄÇ so I can then multiply it by 100„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_336.png)

That's my 90% training accuracy„ÄÇAlrightÔºå so let's take a look at the decision boundary„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_338.png)

So this is the same code as from the perceptron class or lecture„ÄÇ and we can see now„ÄÇ

 so on the left hand sideÔºå this is for the training set and on the right hand side„ÄÇ

 it's for the test set„ÄÇ It's actually doing better than the perceptron„ÄÇ

 It's something this would be a problem where the perceptron wouldn't be able to solve it because here you can see now this is the case where the classes are not linearly separable„ÄÇ

 So there is no decision boundary where you get perfect accuracy„ÄÇ

 So a perceptron here would have really problems„ÄÇ It would change„ÄÇ

 It would like have a decision model like this and like thisÔºå this it would flip back and forth„ÄÇ

 but this model converges„ÄÇ you saw that based on the„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_340.png)

Training And also itsÔºå it's kind of finding a good boundary here„ÄÇ And this is for the test„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_342.png)

So yeahÔºå this is it for this lecture„ÄÇ in the next lecture„ÄÇ

 we will be taking a look at how we compute or how we can compute these gradients automatically in Pythtor„ÄÇ

 So next lectureÔºå we will see„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_344.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_345.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_346.png)

How Pytorch computes this one automatically so that we don't have to do this by handÔºå because„ÄÇ

 I meanÔºå right now„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_348.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_349.png)

What we had here„ÄÇWhere was itÔºü

![](img/5ca7be5140adcc9be61cf008e9bde772_351.png)

What we had here was relatively simpleÔºå a very simple function„ÄÇ But even for that„ÄÇ

 we needed a lot ofÔºå yeah not a lot of steps„ÄÇ I meanÔºå you can do it in fewer steps„ÄÇ

 But it' is kind of complicated when we have multiple layers and nonlinear activation functions and stuff like that„ÄÇ

 And it would be really tedious for every neural network network architecture to do that from scratch„ÄÇ

 So Pytor actually implements some functionality to„ÄÇ



![](img/5ca7be5140adcc9be61cf008e9bde772_353.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_354.png)

Computers backward function automatically by analyzing the forward function„ÄÇ Allright„ÄÇ

 we will see how that works then next week„ÄÇ

![](img/5ca7be5140adcc9be61cf008e9bde772_356.png)

![](img/5ca7be5140adcc9be61cf008e9bde772_357.png)