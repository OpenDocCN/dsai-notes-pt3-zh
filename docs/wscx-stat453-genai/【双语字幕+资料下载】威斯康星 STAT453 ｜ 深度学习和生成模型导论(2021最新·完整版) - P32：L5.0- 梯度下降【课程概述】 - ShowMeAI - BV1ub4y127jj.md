# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P32ï¼šL5.0- æ¢¯åº¦ä¸‹é™ã€è¯¾ç¨‹æ¦‚è¿°ã€‘ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hi everyoneã€‚ So we had a little excursion last time talking about linear algebraã€‚ Todayã€‚

 there will be another little excursion talking about some concepts in calculusã€‚

 because they will be useful forï¼Œ yeah fitting neurons with gradient descentã€‚ So todayã€‚

 the lecture will be on gradient in descentã€‚ and I will have some optional sections onã€‚

 some calculus topics as a little refresherã€‚ But yeahã€‚

 because I think there's a lot of stuff to cover todayã€‚

 I will keep the introduction short and just dive inã€‚ğŸ˜Šã€‚



![](img/20ba561c26dab43aa06f65958ffadb52_1.png)

Yesï¼Œ so the goal here in this lecture is to improve upon the perceptron that we covered last weekã€‚

 So in this lectureï¼Œ we are going to talk about or learn about a neural network model for which that training always convergesã€‚

 So even if the data is not linearly separableã€‚ here's an overview of the big picture goalsã€‚

 So our goals overallã€‚ So like I just said in this lectureã€‚

 we are focusing on a learning rule that is more robust than the perceptronã€‚

 so that it always convergesï¼Œ even if the data is not linearly separableã€‚



![](img/20ba561c26dab43aa06f65958ffadb52_3.png)

But yeah that is not the end goal of this courseã€‚ Of course we want to learn about more sophisticated models compared to single layer neural networksã€‚

 So what we really care about is also how we can make or create more complex classifiers that can solve more complicated problems than just binary or linearly separate problems because if I have somethingã€‚

Like thatã€‚ where I have two classesã€‚ I meanï¼Œ this is relatively trivialï¼Œ rightã€‚

 So the perceptionceptron again had the shortcomingã€‚ If data point is over thereã€‚

 there is no linear decision boundary such that there is no mistakeã€‚ So that wayã€‚

 it would never convergeã€‚ But even let's sayï¼Œ if we have a learning rule that converges like thisã€‚

 this is a relatively trivial problemã€‚ And in practiceã€‚

 we are yeah interested in more complicated problemsã€‚

 So we need to also find a way to combine multiple neuronsã€‚ becauseã€‚

These combinations of multiple neurons or deep neural networks can help us to learn such more complicated decision boundariesã€‚

 and then with that we can solve yeahï¼Œ also complicated real world problemsã€‚

 there are many like objective detection and let's say on classifying arbitrarybitary thingsã€‚

So in that way that is something we will then start tackling in the next lectureã€‚

 So first here we lay the groundwork talking about the learning rule and then we will apply the same learning rule to the combination of neurons and also we will then learn how we can extend this to multiple categoriesã€‚

 It will also be next lecture next weekã€‚So we will learn also how we can classify yeah problems where we have more than two classesã€‚

 which is also importantï¼Œ of courseï¼Œ rightï¼Œ if you think aboutã€‚

 let's say you you build an application for a birdwacher to classify birds outsideã€‚

 So there are more than just two bird species right So in that way we will also learn about methods that can handle multiple classesã€‚

 And then yeah more towards the end of the courseï¼Œ we will then also learn how to do fancier thingsã€‚

 not just classificationï¼Œ but also like things like generating new images and new text and things like thatã€‚

But yeah all these types of things are based on the same learning algorithm and extensions of this learning algorithmã€‚

 it's not like a fundamentally different though and in this lecture here we will learn the core principle of this learning algorithm that we will be using for the rest of the semesterã€‚



![](img/20ba561c26dab43aa06f65958ffadb52_5.png)

So and yeah also the good news is after this lecture here there won't be any new mathematical concepts so in this lecture I will talk a little bit about calculus but that is about it when it comes to the mathematics of this courseã€‚

 of course there will be mathematical concepts or applications of the concepts we discussed like the linear algebra stuff and calculus stuff in this lectureã€‚

 but there won't be any new fancier concepts so after this lecture you can relax there won't be any yeah new mathematical complicated things going onã€‚

ğŸ˜Šï¼ŒSo everything also in deep learning will be extensions and applications of these basic concepts in linear algebra and calculus that we are covering nowã€‚



![](img/20ba561c26dab43aa06f65958ffadb52_7.png)

So MIï¼Œ just to give you the big picture overview of things I have in mind for this lectureã€‚

So first we are going to talk about the different learning modesã€‚

 there's something called online batch and mini batch learningã€‚

It is a general concept that applies to all types of single layer neural networks and multilayer neural networksã€‚

 convol networks and everything basically in deep learning and MIa after just this big overview hereã€‚

 we will talk about the relationship between the perceptioncept and linear regressionã€‚

And then we will learn about an iterative training algorithm for linear regressionã€‚

 You probably know the closed form solution for linear regression these the matrix formulations and such the short con dense formula for solving for the weights and the bias unit directly here we will talk about an iterative learning algorithm because it helps us understandã€‚

How we can train neural networksã€‚ It's kind of very closely relatedã€‚ Yeahã€‚

 I will have a little calculus refresherã€‚ This will be optionalã€‚

 So you notice that there are seven sections this timeã€‚ it may be a little bit longã€‚

 So this is also really just optionalã€‚'s I thought it might be good for some of those who have taken calculus a long time agoã€‚

 or maybe not quite sure about calculus anymoreã€‚ So in that wayï¼Œ it's like a little refresherã€‚

 but if you are very familiar with calculusï¼Œ you don't have to watch these videos So in that way it will be a shorter lectureã€‚

 it's also something these optional things that I probably wouldn't have covered in the inper class because yeahã€‚

 this is more like a prerequisiteã€‚ So in this wayï¼Œ I wouldn't want to make the lecture in person too longã€‚

 but if you likeï¼Œ you can watch themï¼Œ but youre not required toã€‚

 So it's it's just some background information some optional stuffã€‚å—¯ã€‚Yeahï¼Œ after thatã€‚

 we will be talking then about gradient descent That isï¼Œ the learning algorithm that Iã€‚

Briefly outlined in the previous videoã€‚And then we will be training a linear neuron Adelineã€‚

 and I will also show you then echo code example in Pythtorch of how we can train such a linear neuron using the concept of gradient descent Allrightã€‚

 so then let's get started with the first video on online batch and mini batch mode in the next videoã€‚



![](img/20ba561c26dab43aa06f65958ffadb52_9.png)