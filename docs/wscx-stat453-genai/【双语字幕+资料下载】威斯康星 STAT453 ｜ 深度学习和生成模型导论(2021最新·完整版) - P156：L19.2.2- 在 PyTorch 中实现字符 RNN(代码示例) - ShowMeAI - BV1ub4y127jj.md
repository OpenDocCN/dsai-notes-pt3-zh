# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P156ï¼šL19.2.2- åœ¨ PyTorch ä¸­å®ç°å­—ç¬¦ RNN(ä»£ç ç¤ºä¾‹) - ShowMeAI - BV1ub4y127jj

All rightï¼Œ let's now take a look at a code implementation regarding the character R and N that we talked about in the previous videoã€‚

 So in the previous videoï¼Œ it gave you some overview of the LSTM and the LSTM cellã€‚

And I prepared two notebooksã€‚ One is based on usingã€‚

 I can maybe just scroll down for now on using the LSDM classã€‚ and one is based on usingã€‚

The LSTM cell classï¼Œ personallyï¼Œ I think for this type of modelã€‚

 it makes more sense to use the LSTM cellã€‚ It's a little bitï¼Œ I thinkï¼Œ more intuitiveã€‚

So going back to what I talked about in the previous videoã€‚ So here that's the LSDM cell classã€‚

 And we willï¼Œ we will only considerã€‚One layerã€‚ I meanï¼Œ we can easily extend it for multiple layersã€‚

 but we will only have one layerï¼Œ soã€‚Let's blend that part outã€‚Andã€‚We will essentiallyã€‚

 so if you consider this partï¼Œ we will receive one initial hidden state and one initial cell stateã€‚

Together with that input token like thisï¼Œ it will receive one input token hiddenã€‚Stateï¼Œ cell stateã€‚

 and then produce one outputï¼Œ and this output will go to a fully connected layer to do the character predictionã€‚

Then we will move to the next inputã€‚ So hereï¼Œ the next inputï¼Œ it will receiveã€‚

The hidden stayed from the previous time step and the cell stayed from the previous type stepã€‚

Together with a new characterï¼Œ and then it will produce againã€‚

 an output that goes to a fully connected layer to predict the next characterã€‚

Then we will go on here againï¼Œ it will receive receive the hidden state and the cell state from the previous time stepã€‚

The current times of inputï¼Œ output something and so forthã€‚ So that's how the NTM cell class worksã€‚

Alrightï¼Œ But before we get to this partï¼Œ the LSTM cell classï¼Œ let's start at the topã€‚

 Let me even'cause it's quite fast to runã€‚ Let me evenã€‚



![](img/49a9862e64b36005a3bc6d464085a858_1.png)

Run this from scratchã€‚ We haven't done this in a long timeã€‚ Alrightï¼Œ soï¼Œ I meanï¼Œ from scratchã€‚

 I meanï¼Œ by executing it as we go hereã€‚

![](img/49a9862e64b36005a3bc6d464085a858_3.png)

So we start by implementing oh by just importing some librariesã€‚

 So here I didn't use any helper files I try to keep everything in the notebook because the code here is relatively shortã€‚

And simpleã€‚So here I have some hypoparmeters like the text portion sizeã€‚

 So how long a typical text portion isã€‚The number of iteration for iterations for trainingã€‚

 So here we don't use epochsã€‚ We just use iterationsã€‚å—¯ã€‚Learning rateï¼Œ the size of the embeddingã€‚

And the size of the hidden layerã€‚Okayï¼Œ let's execute thatã€‚I' runningning this on a CPUã€‚

I actually had problems running this on the GPUã€‚ I think there's a buck in Pytorchã€‚

 so it runs fine on the CPUï¼Œ but on the GPU it will restart the kernel And if I run this in my terminal without to put an notebookã€‚

 it gives a segmentation fold and my suspicion is thatã€‚Yeahï¼Œ it's a back in Pyth andã€‚

Because it's maybe related to the fact that it's loading the data too fastã€‚ And there's someã€‚

 it's trying toã€‚Access a memory in the GPU that is not free yetã€‚ So actuallyï¼Œ this shouldn't happenã€‚

 So Iï¼Œ I believe there's a bug somewhereã€‚But anywaysï¼Œ it's quite fast so we canã€‚Run this on the CPUã€‚

Okayï¼Œ so we use this as our character set from the stringã€‚Python libraryã€‚

 So string is like standard library in Python for string related thingsã€‚

And we will use all printable charactersã€‚ So a bunch of themï¼Œ actuallyï¼Œ how many are thereã€‚It sameã€‚

100ã€‚So we will want use a set of 100 printable characters So numbersï¼Œ lower caseã€‚

 upper case letters and special charactersã€‚Yeahï¼Œ so actuallyï¼Œ yeahã€‚ So we will useã€‚

Has the data set the CoVd 19 FAQ from the University of Wisconsin websiteï¼Ÿ

I went actually to that websiteï¼Œ and extracted allã€‚On the FAQ questions hereã€‚ So Iï¼Œ as you can seeã€‚

 there was a lot ofã€‚Textï¼Œ so I just copied everything into a text fileã€‚ So this is our training setã€‚

 Its all Covid 19 relatedã€‚Questions on our university websiteã€‚å—¯ã€‚Okayã€‚

 so here I'm just opening and loading the text fileã€‚So in totalï¼Œ we haveã€‚

84000 characters in our textitesï¼Œ like a small bookï¼Œ almostã€‚okã€‚

So here I have a function for getting a random portion of the text ofã€‚Sizeã€‚Text lengthï¼Œ soã€‚

Do I have this text portionï¼Œ sorryã€‚My text portion size 200ï¼Œ so it getsã€‚

Text of the portion size 200 from the whole text hereï¼Œ randomlyã€‚

So this will be our yeah training batchã€‚Okayï¼Œ so yeah those random portionsã€‚

 so you can see some letters are chopped offã€‚ but for all our simple case hereï¼Œ it's good enoughã€‚

 So againï¼Œ this is not perfectã€‚ So you may in real world applicationã€‚

 you may want to implement the function that it has like a complete sentences or something like thatã€‚

Should't just keep things simpleã€‚ This is just a simple function just getting 200 characters at a timeã€‚

Then I have a function converting the characters here to tensrsã€‚

So here this is just getting the index for the charactersã€‚ So if we have 100 charactersã€‚

 it gets the index rightï¼Œ So a would be index 10 and so forthã€‚

 So it's converting the strings into numbers that we can work with in Pythtorchã€‚Andã€‚

This is putting those things togetherã€‚ So this is for drawing a random sample for trainingã€‚

 So this is just getting a random text portion in string formatã€‚

 This is for converting a string to integers and this does bothã€‚ It's getting the random portionã€‚

 rightã€‚Converts that into integersã€‚And then it also gets our labelsã€‚

 So the labels are the inputs shifted by one character because here our task isï¼Œ yeahã€‚

 to predict the next character in the sequenceï¼Œ rightï¼Œ So if I do that and draw my random sampleã€‚

 So my random samples are 0ï¼Œ94ï¼Œ24 and so forthã€‚ And you can seeã€‚The target is shifted just by oneã€‚

 rightï¼Œ Because if we are here and we want to predict the next characterï¼Œ the next character is 94ã€‚

 And from hereï¼Œ the next character is 24 and so forth in interteger representationã€‚ So this is ourã€‚

Or batch of featuresã€‚And this is ourï¼Œ wellï¼Œ these are our labelsã€‚Kimã€‚

So here's now our R R N implementationã€‚So I just have something to keep track of the hidden sizeã€‚

 This is our embedding layer that goes from the integerã€‚

 the character integer to the embedding vectorï¼Œ a real value vector of sizeã€‚ Let me see of sizeã€‚100ã€‚

 and the hidden dimension is 128ã€‚Okayï¼Œ so we have the embedding size and then the LDM cellã€‚

Takes vectors of sizeï¼Œ128ã€‚And has a hidden size of1 sorryï¼Œ of 100 and has a hidden size of 128ã€‚

 So if I go back to my slidesã€‚

![](img/49a9862e64b36005a3bc6d464085a858_5.png)

Maybe using this representation hereã€‚ So our text we had here a one hot encodingã€‚This isï¼Œ yeahã€‚

 when we want to compute the lossã€‚We use actually just an integer hereã€‚

 So here this would be the integer 2ï¼Œ the integer 0ï¼Œ and integer 1ã€‚ if you look at this hereã€‚



![](img/49a9862e64b36005a3bc6d464085a858_7.png)

Rightï¼Œ so this one inï¼Œ in this figure would correspond to an Sï¼Œ for exampleã€‚



![](img/49a9862e64b36005a3bc6d464085a858_9.png)

And then the embedding layer will outputã€‚100 dimensionalã€‚

Vectome and the hidden layer will be a 128 dimensional vectorã€‚So let's see how we use that actuallyã€‚

 in the forward passã€‚So in the forward passï¼Œ we first put the character through the embeddingã€‚

 So this will beã€‚Accepting batch size and bedding dimensionalityã€‚

 we use only one character at a timeã€‚ So it will be one times thebedding dimensionalityã€‚

 which is in our case 100ã€‚Then we give to the R and Nï¼Œ which is an LCDM cellã€‚

 We give the embedded vectorï¼Œ which isã€‚The 1 times 100ï¼Œ together with a hiddenã€‚

State and the cell state from the previous iterationã€‚So thisï¼Œ if we are hereã€‚Essentiallyã€‚

 or maybe use the other representation againã€‚ So if weã€‚Let's say the first step hereã€‚

So we are currently running here the this R and Nã€‚ It will get the hidden state and the cell state from the previous iterationã€‚

 This is these twoã€‚ And theseï¼Œ we provide them viaã€‚By the forward pass as inputã€‚

 So these will go into this one and these will return a new set of hidden and cell statesã€‚

 So here these are the inputsï¼Œ and then they return these outputs hereï¼Œ these two for the next roundã€‚

And then this is computing ourã€‚Lodges for the softmï¼Œ for the cross entropisã€‚Soã€‚This one output hereã€‚

 This is essentially this one through a fully connected layerã€‚ So this will beã€‚

It's clearer like thisã€‚So itï¼Œ this will beã€‚Like thisã€‚

So we'll be giving us one output where here we haveã€‚A fully connected there in betweenã€‚

Gives us one outputã€‚

![](img/49a9862e64b36005a3bc6d464085a858_11.png)

Okayï¼Œ and we return the output because we use it for computing the lossã€‚And alsoã€‚

 when we want to take a look at the text so that we can generate some textã€‚The next characterã€‚

And then alsoï¼Œ when we want to generate textï¼Œ of courseã€‚

 we have to feed the output back into the inputã€‚ So if I go backã€‚



![](img/49a9862e64b36005a3bc6d464085a858_13.png)

There's too many slides hereã€‚ When I go back hereï¼Œ this visualizationï¼Œ this is for trainingã€‚

 but for generating textï¼Œ we feed the output here so we get this inputã€‚

 produce an output and the output gets fed to the next time step is input so that we can generate new textã€‚



![](img/49a9862e64b36005a3bc6d464085a858_15.png)

Yeahï¼Œ anything elseã€‚ Yeahï¼Œ just the dimensionality for referenceã€‚å—¯ã€‚Anything outã€‚

This is not right hereã€‚ I think this should beã€‚Definitely not this oneã€‚ It should be theã€‚

Number of charactersã€‚Ohï¼Œ but here I said hidden say output size would be yeahï¼Œ the output sizeã€‚Okayã€‚

Yeahï¼Œ and we wouldã€‚ So we one more thingã€‚ We have this initialization of the zero stateã€‚

 So here we have to start somewhereï¼Œ rightï¼Œ soã€‚If I go back to myã€‚Visualizationã€‚Hereã€‚ãã†ã€‚

Me rid of one of thoseã€‚So here we don't have initial inputã€‚ input the input is hereã€‚

 so we have to have some some zero state hereã€‚ and this is here my my zeros justã€‚

 just some initial stateã€‚Okayï¼Œ and alrightï¼Œ then let's get startedã€‚ So actuallyï¼Œ the output sizeã€‚

 I should mentionï¼Œ this is the same size as the input sizeã€‚Yeahï¼Œ so let's initialize the R and Nã€‚

 So as input size is the length of the number of charactersã€‚ it's 100ã€‚

So the output size would be also 100ã€‚ And in betweenï¼Œ we have the embedding and hidden dimensionã€‚

I'm using atomã€‚Just simpleã€‚Here's an evaluation functionã€‚

 Let me get to the evaluation function in a secondã€‚ Let me firstã€‚Bundusã€‚Thenï¼Œ run thisã€‚

Can already execute thisã€‚ But I will talk about this afterã€‚

 after talking about the main training loopï¼Œ because the evaluation function hereã€‚It's actually usedã€‚

Resuced onã€‚Hereï¼Œ so it's just one tiny part of the training functionã€‚

So let's talk about the big picture training function firstã€‚ So I'm just iterating 5000 timesã€‚

5000 stepsã€‚Thenï¼Œ I haveã€‚My initialization hereã€‚ So this is for initializing my zero stateã€‚

 So this is for initializing here these initial statesã€‚ I can actually make this biggerï¼Œ rightã€‚

So this is for initializing these zero states hereã€‚This for each iterationã€‚

 So each so each iteration will go through one text portion of size 200ã€‚ So for each text portionã€‚

 we initialize that as0ï¼Œ this is our beginning of the textã€‚What we do is we set the loss to 0ã€‚

Draw a random sampleã€‚So againï¼Œ the random sample will be like thisï¼Œ just some textã€‚

 some random text portionã€‚And the target shifted by one value hereã€‚We put it on the GPUã€‚In our caseã€‚

 nothing happens because we use the CPUã€‚Then for each character in the text portion sizeã€‚

This is where we do the step putting it through the modelã€‚ So hereã€‚

 this is just for making the dimensional nuityï¼Œ rightï¼Œ because this isã€‚Just one single valueã€‚

 But as you recallï¼Œ we wantã€‚One times sorryï¼Œ batch size times 1ã€‚ So it should be a 2 d tensorã€‚

 So we are doing unsqueed adding an empty dimensionã€‚We provide the hidden from hereã€‚

 the cell state from hereï¼Œ these initial cell statesã€‚ So in the first roundã€‚

 these will be the initial onesã€‚ But then we are also outputting themï¼Œ rightã€‚

 So it will feed by right backã€‚ So in an accelerationã€‚

These will be used from the previous iterations that will feed right backï¼Œ and we areã€‚

Computing the lossï¼Œ we are just adding the loss hereã€‚

So we are computing the loss between the outputs and the targets one at a timeã€‚

And then we just normalize by the text portion sizeã€‚ So we haveï¼Œ we haveï¼Œ it's just the averageã€‚

 the meanã€‚Mean over the batch sizeï¼Œ if you willï¼Œ becauseã€‚We addï¼Œ let's sayï¼Œ200 lossesã€‚

 and then we divide by 200 just averagingã€‚ it'sã€‚Just so thatã€‚

Works better with the learning rate so we can change the takes portion sizes and shouldn't have to worry about changing the learning rateã€‚

Then we call backwardï¼Œ make a stepï¼Œ an update stepï¼Œ and that is itã€‚

 So here we just have some loggingã€‚In some more loggingï¼Œ it will create a P D F with a plotã€‚

 the loss plot so we can take a look at it during trainingã€‚There's also a tool called Tensor boardã€‚

 which is actually pretty usefulï¼Œ but we already have so many things to talk about and code examples and everything I don't want to make it more complicatedã€‚

 so we are just using matpllip hereã€‚But yeahï¼Œ the in one last interesting part before I run this is the evaluation functionã€‚

 So instead of just printing out the lossã€‚ in addition to thatï¼Œ I'm also evaluating the modelã€‚

 So what do I mean by thatï¼Œ I'm letting it generate textã€‚ So let's take a lookã€‚

This is my evaluation functionã€‚Soã†ã€‚We initialize it to the zero stateï¼Œ then weã€‚

Build up some hidden stateã€‚ We are priming thisï¼Œ essentiallyã€‚

So what that means is we are providing some primeã€‚Characterã€‚

 this is like some starting character hereã€‚ placeholder Aã€‚ I actually use T Hã€‚

 So all texts will start with T H that we are generatingã€‚ It's justï¼Œ I meanã€‚

 it's arbitrary could be anythingã€‚So all texts will start with Tï¼Œ Hã€‚And then here we are priming itã€‚

 We are building up some hidden states so that the model stabilizesã€‚ So we are doing this forã€‚

The letters in the prime rangeã€‚ So we only have two lettersï¼Œ rightï¼Œ we can have actually moreã€‚

 We can have some real wordsã€‚Spills up aã€‚Ohã€‚

![](img/49a9862e64b36005a3bc6d464085a858_17.png)

Sa stateã€‚ So it's justï¼Œ it's just essentially running in this caseï¼Œ through two of theseï¼Œ rightï¼Œ1ï¼Œ2ã€‚

 And then we get to this partã€‚

![](img/49a9862e64b36005a3bc6d464085a858_19.png)

Andï¼Œ for eachã€‚In the prediction lengthï¼Œ we are generating text of size 100ã€‚

We are just running it as beforeã€‚ So againï¼Œ there's nothing special yetã€‚

 It's just running the model similar to how we run it in the training loopã€‚The new part nowï¼Œ thoughã€‚

 isã€‚This partã€‚Soï¼Œ hereã€‚Could I've also actually written it simpler like thisã€‚ If it' just a divisionã€‚

So we are dividing by a temperatureã€‚ So what is the temperatureï¼Œ Soï¼Œ first of allã€‚

Outputs are our logicsã€‚ So if we go back to our model hereã€‚These are just hereã€‚

 we don't use any softm because the softm is usually used in a cross entropy functionã€‚Soã€‚

 here we haveã€‚Just our loitsã€‚And then we computeï¼Œ I meanã€‚

 we are not really computing the softm as a normalization factor in the softmï¼Œ rightã€‚

 So we are usually normalizing by the sum of all of these for each class hereã€‚ We are lazyã€‚

 We don't do thatã€‚ We just take E to the power ofã€‚So essentiallyï¼Œ it's E to the power ofã€‚

TheLos divided by the temperatureã€‚And the higher the temperature isã€‚So maybe the other way aroundã€‚

 if I have los and I have a small numberï¼Œ like the temperature is usually a value between 0ã€‚And oneã€‚

 andï¼Œ I should sayï¼Œ whyï¼Œ what's the temperature hereï¼Œ this likeã€‚

I think it's inspired by energy based modelsï¼Œ but which is turn in turn inspired by the Bsman distributionã€‚

But it's essentiallyã€‚How sharp our distribution is in this caseã€‚ So if we have a small value hereã€‚

 like 01ã€‚All the values will become largerã€‚Yesï¼Œ we will get larger valuesã€‚ If we have a value1ã€‚1 1ã€‚0ã€‚

 they will be softerã€‚Soã€‚We have here that a real main interesting partã€‚

 We will here have this multinoial functionï¼Œ which is essentially a function that randomly samples from a multinoial distributionã€‚

Using theseï¼Œ you can think of them as weights or probabilities andã€‚This is onã€‚

The number of samples we drawã€‚So here we are drawing oneã€‚oneã€‚Characterã€‚From the predictionsã€‚

 So outputs will be the lodges for all 100 charactersï¼Œ because we haveã€‚å“¦ã€‚1 are different charactersã€‚

 So logics will beã€‚å“¦ã€‚Valuesï¼Œ if we have soft nextï¼Œ these would be all the probabilities correspondingã€‚

Corresponding to these charactersã€‚And here we are randomly samplingï¼Œ soã€‚If we so we expectã€‚

 expectation would be samplingã€‚The characterï¼Œ although we would most often sample the character corresponding to the highest probabilityã€‚

Andã€‚We can soften this by having a 1ã€‚0ã€‚ This is a standard setting temperature of 1ã€‚0ã€‚

 If if we use no temperatureï¼Œ this would be or regular valuesï¼Œ But if we use a smaller valueã€‚

 let's sayã€‚1ã€‚ this will be a sharper distributionã€‚ So the mostã€‚

The character with the highest value will be sampled more oftenã€‚ So even more oftenï¼Œ soã€‚

We can actually set the temperature veryï¼Œ very smallã€‚

And then it will essentially be like not sampling at allã€‚

 It will always sample the character with the highest probabilityityã€‚ in that wayï¼Œ it'sã€‚

More like deterministicã€‚ So you can think of it like thatï¼Œ the higher the temperatureã€‚

 the more diversity in the output in the generated text you will getã€‚

So if you want to have more diverse textï¼Œ youï¼Œ you can lower the temperatureã€‚ It'sï¼Œ it's kind ofã€‚

 you can increase the temperatureã€‚ If you want to have no randomnessï¼Œ you lower the temperatureã€‚

 Higher temperature means more diversityã€‚ You can think of it also asã€‚

 as kind of like heat in a biological system where youï¼Œ if you heat up theã€‚The systemã€‚

 you have more kinetic energyã€‚ Everything is symbolicã€‚Wiggling around and things like thatã€‚

So here we have like  point8ã€‚ It's like a trade offã€‚ but it's a hyper parametermeterï¼Œ if you haveã€‚

Higher temperatureï¼Œ there will be more mistakesã€‚Not also more diverse textã€‚ It's like a trade offã€‚

Okayã€‚Can play around with that if you like when you do the trainingã€‚

 but let's just use the ones that I use to begin withã€‚ So it's running nowã€‚

 it gets a pretty high lossã€‚ And this is the initial textã€‚ Notice that it starts with a T Hã€‚

 But the rest is like Gib gokã€‚ It's nothing reasonable in here is' just an arbitrary textã€‚But yeahã€‚

 it already finished the next iterationã€‚ Let's take a look at thatã€‚Still not anything usefulã€‚

 You can see it learned somethingï¼Œ rightï¼Œ So you can seeï¼Œ okayï¼Œ theseã€‚This is an actual word hereã€‚

So it's learning somethingã€‚ Let's take a look at the next oneã€‚Ohï¼Œ okayï¼Œ we haveã€‚Some real words hereã€‚

Kind of learning somethingã€‚æ²¡æœ‰ã€‚You can see studentsã€‚So againï¼Œ this is trained on the Covid FAQã€‚Whichã€‚

Its more mostly readable textã€‚Kã€‚ğŸ˜”ï¼ŒYeahï¼Œ it's gettingï¼Œ it's getting a little bit betterã€‚I meanã€‚

 of courseï¼Œ this is like not real textã€‚ It's not a very sophisticated ironã€‚ And it's just one layerã€‚

 very simpleï¼Œ very small textã€‚ Alsoï¼Œ I meanï¼Œ it just learned for one minuteï¼Œ right soã€‚

But you can see that it actuallyã€‚Seems to learn something If you think about it started from thisã€‚



![](img/49a9862e64b36005a3bc6d464085a858_21.png)

Had then went to thisã€‚ And nowï¼Œ it's kind ofã€‚

![](img/49a9862e64b36005a3bc6d464085a858_23.png)

Talking aboutã€‚Sa a begerã€‚Bdges app notification systemã€‚ So it'sï¼Œ it's kind of learning thingsï¼Œ rightã€‚

Wisconsinã€‚Covid-19 testingã€‚Yeahï¼Œ it will have to run way longer to get some sensible inputsã€‚ Againã€‚

 there are also many hyperparmeter to tune can have more layersã€‚

 You can change the temperature and these types of thingsã€‚To get better resultsã€‚ But I thinkã€‚

 feel likeï¼Œ I meanï¼Œ given that I'm on the CPU just running it for two minutesã€‚

 it's quite cool that it canã€‚ğŸ˜Šã€‚

![](img/49a9862e64b36005a3bc6d464085a858_25.png)

![](img/49a9862e64b36005a3bc6d464085a858_26.png)

Yeahï¼Œ can create text that is not total nonsenseã€‚ I meanï¼Œ it's total nonsense nonsenseï¼Œ to be honestã€‚

 butã€‚We can identify individual wordsã€‚ So that's actually quite coolã€‚ Okayï¼Œ so let meã€‚

 let me stop it right hereã€‚ was a long videoã€‚ And in the next videoï¼Œ we willã€‚ğŸ˜Šã€‚



![](img/49a9862e64b36005a3bc6d464085a858_28.png)

Take a next stepï¼Œ talking about how we can outfit our ends with the so called attention mechanismã€‚



![](img/49a9862e64b36005a3bc6d464085a858_30.png)