# 【双语字幕+资料下载】威斯康星 STAT453 ｜ 深度学习和生成模型导论(2021最新·完整版) - P26：L4.0- 深度学习的线性代数【课程概述】 - ShowMeAI - BV1ub4y127jj

Yeah， hi everyone。 I hope last week's lecture on Perceptrons was a nice easygoing introduction into the overall concept of supervised learning。

 So but yeah， of course， we want to solve more complex problems。

 especially in deep learning here in this class So in this lecture I'm gonna tell you a little bit more about yeah。

 the linear algebra conventions for deep learning because yeah deep learning kind of heavily relies on basic concepts of linear algebra for making the notation a little bit more convenient。

 but then also to implement things in code more efficiently But yeah。

 no worries these will be relatively simple， straightforward applications of linear algebra。

 So here I just want to lay the groundwork so that we can then。

Later talk about how we yeah represent multi layerer neural networks that can solve more complex problems such as the yeah X problem and many other things like image classification。

 So yeah in this lecture I want to briefly talk about the use of tensrs in deep learning than how we work with pytorch in terms of yeah using or working with tensrs So this will be then the concept of multidimensional arrays and then I will talk a little bit about vectors matrices and then the concept of broadcasting。

 So this third section will be particularly about the broadcasting which makes the work with vectors matrices and tensrs more convenient。

Then lastly， I will end this lecture with some notational conventions for neural networks。

 so there's some little I would say discrepancy between textbook notations where we have linear transformations and textbooks as way back to times the inputs and then in computational tools like Pythtorch it's usually the other way around to make things a little bit more convenient from a computational perspective。

 So here I just want to yeah give you an overview so then things will become easier later on in this lecture when we talk about certain concepts like convolutal networks and recurrent neural networks where these things heavily rely on high dimensional tensors So but with that yeah。

 let me then get started with the first video of this week。

