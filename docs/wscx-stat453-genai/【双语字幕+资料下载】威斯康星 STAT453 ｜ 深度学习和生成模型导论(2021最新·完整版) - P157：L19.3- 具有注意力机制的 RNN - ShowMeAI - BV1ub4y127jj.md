# P157ÔºöL19.3- ÂÖ∑ÊúâÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑ RNN - ShowMeAI - BV1ub4y127jj

Alright so for this video I have a fun idea so some people complained that RNNs and LSTMs are not complicated enough„ÄÇ

 so in this video we are going to make them a little bit more complicated„ÄÇ

 so in particular we are going to talk about recurrent neural networks with a socalled attention mechanism that helps at dealing better with long sequences when we have tasks such as language translation„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_1.png)

So just to yeah recap how language translation looks like„ÄÇ

 So this is a many too many architecture for language translation„ÄÇ

 So we have an input sentence and this input sentence gets parsed gets parsed into one hidden state so„ÄÇ

This whole part isÔºå essentially„ÄÇEncoding the input sentence„ÄÇ

 which is why it's also sometimes called as the encoder„ÄÇPart of the RnN„ÄÇ

 similar to how you call the first layers of an auto encoder„ÄÇ

 the encoder because it's compressing the input into oneÔºå let's say hidden state„ÄÇAnd then„ÄÇWe have„ÄÇ

This„ÄÇDeder part of the R NÔºå which takes in the hidden stateÔºå and then produces„ÄÇ

The translated sentence„ÄÇIf you are interested in playing around with an example„ÄÇ

 I haven't made one for this classÔºå but theres an excellent tutorial here on the PitWatch website that you can check out„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_3.png)

But before I show you how the attention mechanism works„ÄÇ

 let me give you an example of a language translation task„ÄÇ

 So here I'm translating one input sentence to an output sentence„ÄÇ

 So the input sentence is the English sentence today is a great dayÔºå and I'm translating it to„ÄÇ

Hotaist and Goar attack„ÄÇ So I'm not sure if you can guess which language it is„ÄÇ

 maybe a fun question for Piazza„ÄÇ In any caseÔºå one thing„ÄÇ

In this particular sentence is that it actually translates word by word„ÄÇ

 so if you would take a dictionary„ÄÇYou could technically just translate itÔºå whoops„ÄÇWord for word„ÄÇ

 So if you have a sentence like thatÔºå you could almost use the many to many R N that we learned about in the previous lecture„ÄÇ

 the„ÄÇOne where we have the inputs„ÄÇThen„ÄÇThe hidden states„ÄÇThenÔºå the outputs„ÄÇAnd we technically„ÄÇ

Could just have the setup like thisÔºå like a„ÄÇMany too many set up where„ÄÇ

We are translating word by word„ÄÇBut this doesn't work for all sentences„ÄÇ

This is why we have this delete set upÔºå where we usually„ÄÇDon't have these parts„ÄÇ

And this one goes into„ÄÇThe decoder„ÄÇAnd the decota then produces the outputs just based on the last state„ÄÇ

 because for some sentencesÔºå theres not this one to one correlation between input and output work„ÄÇ

 So for instanceÔºå heres another example sentence its if you have ever studied a foreign language„ÄÇ

 you have probably encountered a false friend at some point and here„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_5.png)

This is the translation„ÄÇ So one thing that happens isÔºå for instanceÔºå this oneÔºå these two words„ÄÇ

Become one word„ÄÇ It's called fpairÔºå and„ÄÇAnother thing I can spot here is that the you you have and here also„ÄÇ

 it translates one time to this word„ÄÇSo actuallyÔºå the combination of these two„ÄÇ

 which are farther apart„ÄÇAnd then„ÄÇFor this Monday is no corresponding word really„ÄÇAnd„ÄÇüòî„ÄÇ

This one corresponds to this one„ÄÇ So in this wayÔºå hereÔºå it's not so obvious„ÄÇ

 Like we don't have this one to one mapping„ÄÇ So things might be shifted a little bit around in that sentence„ÄÇ

 depending on the grammar„ÄÇ So it's not„ÄÇ

![](img/4afda3b49113df2fe91c660d308afc56_7.png)

Super straightforward to use an architectureÔºå like„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_9.png)

The original one that I had before here because we can't just map input to output„ÄÇ

Which is why we have this set up this many too many set up where we have„ÄÇ

The input sentence that gets passed into this hidden representation„ÄÇ

 And then we have something that creates the output so the„ÄÇEncodeder„ÄÇAnd the decoder network„ÄÇHowever„ÄÇ

 then one challenge is really that the network has to compress all the information into this one hidden state right„ÄÇ

 So essentially it's like reading the whole sentence here„ÄÇ

 it's reading the complete sentence and after reading the sentence„ÄÇ

 it's starting to translate So how do we make sure that the network even remembers right„ÄÇ

 So if it reaches this pointÔºå it has to contain all the information of the whole sentence and this can be actually quite challenging„ÄÇ

 I meanÔºå even for instanceÔºå for you if you think about a sentence like„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_11.png)

![](img/4afda3b49113df2fe91c660d308afc56_12.png)

Like this it's maybe easy because it's just four words so you can memorize them and then translate them„ÄÇ

 but let's say you have a longer sentence like that and I read this out loud and then I tell you in one minute you have to translate it but you're not allowed to look at the whole sentence anymore So in that way I think also you would have a challenge memorizing all the words in that sentence because it's a lot of words„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_14.png)

![](img/4afda3b49113df2fe91c660d308afc56_15.png)

So the same for the R and NÔºå it will probably be challenged to have all the necessary information in the hidden state„ÄÇ

 the longer the sentence becomes„ÄÇ

![](img/4afda3b49113df2fe91c660d308afc56_17.png)

So this is why researchers worked on this so called attention mechanism to address this issue of having poor performance on long sentences„ÄÇ

So„ÄÇHereÔºå this is going back to the paper by Baranau and cos and„ÄÇAs far as I know„ÄÇ

 this is the first paper that really proposed this attention mechanism„ÄÇ

 but I may be wrong there may have been other papersÔºå but this is at least the one„ÄÇ

 the first one I am aware of„ÄÇAnd„ÄÇThe key idea here is essentially that it's a mechanism that allows the model to automatically soft search for parts of a source sentence„ÄÇ

That are„ÄÇRelevant for predicting a target wordÔºå so„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_19.png)

If I go back one more time to my sentence hereÔºå So let's say it's currently at that point translating this sentence„ÄÇ

 oopsÔºå it's allowed to look„ÄÇ

![](img/4afda3b49113df2fe91c660d308afc56_21.png)

![](img/4afda3b49113df2fe91c660d308afc56_22.png)

Like in the original sentenceÔºå which words are most„ÄÇ

Most important it it has access to the whole sentence at each time step„ÄÇEssentialÔºå that's the idea„ÄÇ

 And some words are more important than others„ÄÇ SoÔºå for instance„ÄÇ

 this might have a very low attentionÔºå like 0 and„ÄÇThis one is kind of relevant„ÄÇ

 It has maybe attention close to one„ÄÇ if we have an attention value between 0 and 1„ÄÇ

 So we have high and low attention for different words„ÄÇ So that at each time step the the model„ÄÇ

We be allowed to take a look at different parts of the sentence„ÄÇ that's like the new idea here„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_24.png)

And„ÄÇAt the bottom here is an illustration of how they evaluated the performance„ÄÇ

 So there is something called the thing it's called blue scoreÔºå and this is a score for computing„ÄÇ

 how good the quality of the translation is„ÄÇSo we won't go into too much detail of how that works for here for this lecture„ÄÇ

Just assume a higher blue score is better„ÄÇBetter if it's high„ÄÇAnd„ÄÇ

What you can see here is the sentence length„ÄÇ So they did experiments with different sentence lengths„ÄÇ

 And you can see that for„ÄÇThese two networksÔºå these are„ÄÇAbbreviated R NÔºå inkÔºå30 and 50„ÄÇ So these are„ÄÇ

These are„ÄÇEssentiallyÔºå encoder networks like regular„ÄÇ they are actually traditional R and N networks„ÄÇ

 So it's just like a shortcut for these networks„ÄÇ and the 30 and 50 is how much access they have„ÄÇ

To the inputÔºå I think„ÄÇAnd you can see that the performance yet drops„ÄÇ

Ds dramatically for sentence lengthsÔºå larger than 30„ÄÇ

So you can see it gets good performance in this regionÔºå but if you get longer sentences„ÄÇLer than 50„ÄÇ

WordsÔºå then the performance declines„ÄÇ But when they use this„ÄÇTension mechanism here„ÄÇ they„ÄÇ

 they get good performanceÔºå even for longer sentences„ÄÇ So here the takeaway is that„ÄÇ

They find that the performance for longer sentences is better if they use this so called attention mechanism„ÄÇ

 So how does the attention mechanism workÔºå That is what I will show you in the next couple of slides„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_26.png)

YeahÔºå so as I mentioned beforeÔºå for each wordÔºå there will be a certain context provided„ÄÇ

 so the network will be allowed to look at the input sentence and„ÄÇ

See which words are important for translating a given word so„ÄÇSummarise„ÄÇ

 it would be assigning an attention weight to each word„ÄÇ

So that we know how much attention the model should pay to each word„ÄÇ

 so for each word the networks learns a certain context„ÄÇ

 so what is for this particular word what is the context of words that is relevant for translating thisÔºü



![](img/4afda3b49113df2fe91c660d308afc56_28.png)

So here's a figure from the paper„ÄÇOutlining the whole mechanism in one slide„ÄÇ

 So one thing to notice is that there are„ÄÇ2 R ends„ÄÇ So this is all my annotation„ÄÇ

 But this is what I got out of this figure„ÄÇ So there are two R Ns„ÄÇ First of all„ÄÇ

 they have a bidirectional R N„ÄÇ We haven't talked about bidirectional R ends„ÄÇ

 but it's a very simple concept„ÄÇ It's just running the„ÄÇRecurrent neural network forward and backward„ÄÇ

 or you can also think of it running the recurrent neural network twice one time in the regular order and one time by having the sentence„ÄÇ

Put in in the reverse order„ÄÇSo„ÄÇIf we have the sequences or the input sentence„ÄÇX1 to X T„ÄÇ

 This is our input sentence„ÄÇ So one„ÄÇR and N runs this forward like a regular R n from x1Ôºå x 2„ÄÇ

 x 3 and to x T„ÄÇ So it produces these hidden states corresponding to it„ÄÇ

 And then you can also have a„ÄÇThe same concept running backwards„ÄÇ So you just reverse the orderÔºå so„ÄÇ

This is shown at the bottom„ÄÇ So you start with X T when you go to x 3Ôºå TÔºå2 and 1„ÄÇ

 So you are just going backwards essentially„ÄÇSo yeahÔºå this is a bidirectional RN„ÄÇ

It produces these hidden states„ÄÇ And from the hidden states„ÄÇ

 there will be these attention weights that will be computed„ÄÇ I will„ÄÇ

Show you how that works in the next couple of slides„ÄÇ But here's just the overview„ÄÇ

 So we have a bi direction R N„ÄÇThen we compute these attention weights„ÄÇAnd these go into our„ÄÇ

R and N that does the translation now„ÄÇ So I call this the regular R and N because it's not bidirectional„ÄÇ

 It'sÔºå it's more like a regular R and N works like we talked about before„ÄÇ

 except now that it also receives„ÄÇThese attention weights as input„ÄÇ

 So not only the attention weightsÔºå the attention weights weighted„ÄÇ So hereÔºå in this case„ÄÇ

 the attention weight weighted with this„ÄÇWith this stuff hereÔºå so with this hidden state„ÄÇ

 I will show you how this gets computed in the next couple of slides„ÄÇAnd then yeah„ÄÇ

 it also receives the hidden state from the previous slide„ÄÇ

 So here this is maybe a little bit confusingÔºå but it's the S„ÄÇ

 the S is just a letter that they used to denote the hidden state of this other of the second R N because here we already used the word the letter H and they didn't want to use the same letter„ÄÇ

 I guess for here and here because then it would be a little bit also confusing So here S„ÄÇIt's also„ÄÇ

 the hidden„ÄÇHidden stateÔºå but no the hidden state of the second are in„ÄÇ

And then these are the translated words„ÄÇ

![](img/4afda3b49113df2fe91c660d308afc56_30.png)

So here in this illustration I'm trying to simplify or at least try to explain what's going on in the previous figure in a little different way„ÄÇ

 So I haven't mentioned before what we do now with the hidden states of the bidirectional R andN„ÄÇ

 so when we have the bidirectional RN I mentioned we are running it forward and backward to obtain these hidden representations„ÄÇ

 but now we have two sets of hidden activationsÔºå one from the forward passÔºå it's called that F1„ÄÇ

And at this point„ÄÇ and then we also have the backward  one„ÄÇ let's call it B1„ÄÇ

 So how do we combine the two„ÄÇ So in practiceÔºå it's very simple„ÄÇ we just concatenate those„ÄÇ

 So we have just a hidden state that is twice the size of a regular hidden state from just the forward pass„ÄÇ

 because yeahÔºå we are just concatenating those„ÄÇ So the concatenator„ÄÇ Then let's call that H1„ÄÇ

 This is our hidden activation„ÄÇAnd yeah here I'm trying to illustrate how the attention mechanism works„ÄÇ

 It combines multiple things„ÄÇ It combines these hidden states„ÄÇFrom the bidirectional R N„ÄÇ

And the activationÔºå I sorryÔºå the attention weights„ÄÇ So these are„ÄÇThe attention„ÄÇWait„ÄÇ

 send these are the hidden„ÄÇActivations„ÄÇOf the bidirectional R N„ÄÇ And this is the hidden„ÄÇActation„ÄÇ

SoLet's call this decoder„ÄÇOops„ÄÇDecoder„ÄÇR NÔºå okay„ÄÇ„Åù„ÅÜ„ÄÇFor a regular RnN„ÄÇ

 what happens is that we provide the original word as input if you think back of our word RnN at the very beginning of this lecture„ÄÇ

 we would provide a word X1 here as input and then we let's say predict the next word for the many too many one for language translation„ÄÇ

 we wouldn't have any input hereÔºå we would just use the hidden state from the previous time step when we are in the decoding part„ÄÇ

So hereÔºå what's new is compared to a regular word R and N where we get this x1Ôºå the word is input„ÄÇ

 we now take in„ÄÇA context vector„ÄÇ So this context vector provides information about the whole sequence„ÄÇ

 not just x1Ôºå the whole sequenceÔºå the whole input sequence in a regular Rn N„ÄÇ

 This would be encoded in the previous hidden state„ÄÇSo if we at time step1Ôºå this would be of course„ÄÇ

0Ôºå but now imagine we areÔºå let's say at the fifth time step„ÄÇ

 so the fifth time step would combine in the hidden state or the previous four time steps„ÄÇ

But it's like a very compressed version and information might get lost„ÄÇ

And for each word also in the sequence„ÄÇDifferent other words might be important„ÄÇ

 So it's kind of hard for a given time step to really ensure that right now it captures the important information for this current word„ÄÇ

 But then for the next oneÔºå it captures other important information„ÄÇ

 So it's easy to to forget what information was important from a previous time step and so forth„ÄÇ

 So to solve this problemÔºå we have this context vector that allows us to take a look at the whole input sequence„ÄÇ

 So to illustrate this again„ÄÇRemove this„ÄÇAnnotationÔºå we have this C1 here and this C1„ÄÇIs computed by„ÄÇ

Taking all the attentions multiplied by each hidden stateÔºå or„ÄÇThis multipliedÔºå plus„ÄÇThis multiplied„ÄÇ

Plus these multiplied„ÄÇ So it might happen that a word is not very important„ÄÇ So in this case„ÄÇ

 if the word is not importantÔºå what would happen is that the attention weight would be„ÄÇ

 let's say close to 0Ôºå then it would notÔºå let's say use this word„ÄÇ

 Other words might be more importantÔºå so the attention weight is larger„ÄÇ

And here's how the context vector is computed„ÄÇ It'sÔºå yeahÔºå summing over„ÄÇ

The tension weights alpha multiplied by these hidden activations„ÄÇ That's how„ÄÇ

How we compute this context vector„ÄÇ NowÔºå how do we compute this attention weight itselfÔºå I mean„ÄÇ

 we talked about the hidden activationÔºå rightÔºå So the hidden activation is„ÄÇ

Just the concatenation of the forward and backward hidden activation„ÄÇ NowÔºå how do we compute this„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_32.png)

Attention weight„ÄÇSo the attention weight is computed by using another neural networkÔºå for instance„ÄÇ

Mtilayer perceptron„ÄÇSo„ÄÇLet's maybe before we get to thatÔºå take a look at this current time step S1„ÄÇ

 So what does the current time step„ÄÇGet as information„ÄÇ So it gets as information„ÄÇ

 essentially this information from the hidden state hidden activations from the previous time step„ÄÇ

 And then we also have this„ÄÇHid an activation from the current time stepÔºå rightÔºå x1Ôºå so this is„ÄÇ

Previous hidden„ÄÇActation of„ÄÇPrevious„ÄÇTime„ÄÇStep„ÄÇAnd this is the hidden„ÄÇActation of the current„ÄÇ

Time step and„ÄÇThis network here gets takes both as input„ÄÇ

 the information from the current time step and the information from the previous time step and then outputs a score E„ÄÇ

 This is a while„ÄÇUnormalized„ÄÇAttention weight„ÄÇAnd this unmalized attention rate is put through„ÄÇOh„ÄÇ

 softftm„ÄÇFunction to normalize it so that the attention weights„ÄÇÂóØ„ÄÇSome up to oneÔºå right„ÄÇ

 So after softmÔºå as you are familiar with„ÄÇWe would have„ÄÇIt should be„ÄÇüòîÔºåShould be over with T„ÄÇ

 I guess„ÄÇSoÔºå they should sum up„ÄÇTo one„ÄÇAnd yeahÔºå so this is how we compute these attention weights„ÄÇ

And yeahÔºå there might be larger and smaller attention weightsÔºå given how important each word is„ÄÇ

 So it'sÔºå it's a form the attention weight depends on both„ÄÇThe current time input and„ÄÇ

The previous hidden state„ÄÇ So the attention ratess might be different if we look at a different word„ÄÇ

 if we have maybe attention rates that say that this is„ÄÇt8Ôºå1 and „ÄÇ1 for the current word„ÄÇ

For for this context vector here„ÄÇIt might be different if we compute C2„ÄÇ

 it might happen that this word hereÔºå the first one is not that important„ÄÇ

 the second word is more important and so forthÔºå so at each time step might be different words that are important„ÄÇ

So but yeahÔºå the bottom line is that this attention mechanism allows the network to kind of get information from the whole input sequence at each time step in addition to just this compressed information from this hidden state„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_34.png)

AlrightÔºå so to finish it upÔºå here's another figure from the paper where they visualize the attention weights since the attention weights are values between„ÄÇ

0 and 1Ôºå my guessÔºå is the white ones here correspond to a high attention rate of one and the black ones here correspond to a low attention rate of 0„ÄÇ

So I guess this is„ÄÇ1Ôºå and this is a0 here„ÄÇSoÔºå here you have„ÄÇActuallyÔºå notÔºå I think„ÄÇ

They translated from English to French„ÄÇMight be wrong„ÄÇ I might be misremembering„ÄÇ

 So let's say this is our input„ÄÇAnd this is„ÄÇOutput„ÄÇ

And then you can see for each word how much attention the network pays„ÄÇWhen translatingÔºå so„ÄÇ

When translatingÔºå let's say the L hereÔºå it pays most attention to theÔºå but also something to s„ÄÇ

And so forth„ÄÇ And you can see for some„ÄÇLikeake„ÄÇThe number here 1992Ôºå 1992„ÄÇ

 it really only pays attention„ÄÇ You can see that„ÄÇIt only pays attention„ÄÇTo this number„ÄÇ

 which makes senseÔºå right„ÄÇSo in this wayÔºå what you can see is the strong value„ÄÇ

Attention value in the diagonalÔºå which means that it's kind of like a one to one translation in that way„ÄÇ

 but not always there are some cases where it pays attention to words that are also far apart in that sentence right so this is the attention mechanism and in the next video we will build on this idea„ÄÇ

By taking away the R and N partÔºå because it turns out we just went from a very complicated model„ÄÇ

 The R and N added attentionÔºå but actually we don't need the R N at all„ÄÇ

 We can maybe also get good results with just attention„ÄÇ And that is the topic of the next video„ÄÇ



![](img/4afda3b49113df2fe91c660d308afc56_36.png)