# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P30ï¼šL4.4- ç¥ç»ç½‘ç»œçš„ç¬¦å·çº¦å®š - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ in this videoï¼Œ I want to briefly go over the notational conventions regarding linear algebra and deep learningã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_1.png)

Soã€‚This is something we have seen beforeï¼Œ from the perceptra lectureã€‚So at the topã€‚

 there is a perceptronã€‚ And now imagine we do inferenceã€‚

 So inference in the context of deep learning means basically predicting the label of input feature vectorã€‚

 So in the perceptronï¼Œ assume we have one training example as input or let's sayã€‚Testã€‚

 test data point or test example as inputã€‚So how we would do that is as followsã€‚

 we would have x transpose W plus Bï¼Œ the bias unit and then get the net input and then we give it to the activation functionã€‚

 which was the threshold functionï¼Œ and then we get our predictionã€‚So here this partã€‚

Is doing this linear algebra computationï¼Œ x transpo dotw plus Bã€‚So now this is for one data pointã€‚

 We can actually also extend that to multiple data pointsã€‚

 So if we want to do that for multiple data points like n training examples or nã€‚Test examplesã€‚

 we can use a design matrix for representing the data and n times M dimensional design matrixã€‚ So Nã€‚

Its the numberã€‚Of examplesã€‚Andã€‚We would write it as followsã€‚ So here x is n times Mã€‚W is m times 1ã€‚

B is just as scalar as a1ã€‚And then the output should be n times M dot M times1ã€‚

 that should be n times 1ã€‚Yï¼Œ in times1ï¼Œ that should be our outputã€‚ So in this way we have a vectorã€‚

 So each value in the vector is the net input for the corresponding test exampleã€‚

 So that is just how we can process more data points at the same time with still a single operation hereã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_3.png)

Nowï¼Œ in deep learningï¼Œ we usually have these neural networks with hidden layout representationsã€‚

We will learn about that a little bit later in this courseï¼Œ actuallyï¼Œ like I think next week alreadyã€‚

 So we haveï¼Œ let's say one data point is input hereã€‚ That's the feature vector andã€‚Featuresã€‚And thenã€‚

 we will haveã€‚A number of outputsã€‚ So let's say Hã€‚Is the number of hiddenã€‚Units and a hidden layerã€‚

 And then we have also multiple hidden layerã€‚ so we can have another hidden layer and another one and so forthã€‚

 So in this wayï¼Œ what I want to highlight here is that whereas in the perceptronï¼Œ we have aã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_5.png)

![](img/b5b959b0efe1a59facc6c66c335a8c4c_6.png)

Single output hereã€‚Hereï¼Œ we can have multiple outputsã€‚So how can we deal with such aã€‚

 yeah scenario using linear algebraã€‚ So what we can do is we can now have a weight matrixã€‚

 So before I talked about the design matrix for the inputs nowã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_8.png)

There's oneã€‚Inputã€‚Vectctor 1 data pointï¼Œ but multiple outputsã€‚ So this way we can use a matrix hereã€‚

 So this matrix would beã€‚H times m dimensionalã€‚Soã€‚ğŸ˜”ï¼ŒLet's write that downã€‚å—¯ã€‚H times Mã€‚

 So H rows and M featuresï¼Œ whereas the features are corresponding to the M hereã€‚ So the Mã€‚

Matches hereã€‚ So what we do is we put the W in frontã€‚The x secondã€‚ So this would beã€‚

 if I write this downã€‚H times M dotã€‚M times 1ã€‚And B should be then also the same dimension as Hã€‚

Cause there is one bias unit for eachã€‚Computation hereã€‚ So the output should be Hm M dot M times 1ã€‚

H times1ï¼Œ H times one dimension output vectorã€‚It's also what I've written down hereã€‚ I just seeã€‚

 So yeahï¼Œ so in this caseï¼Œ this is how we can deal with multiple outputs and we will learn how this works alsoã€‚

 yeahï¼Œ shortly later on next weekã€‚ Okayï¼Œ let's now put both concepts togetherã€‚ So we have multipleã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_10.png)

Trainingã€‚Examples and multipleã€‚Outputsã€‚So we have now a matrix for the weights and a design matrix for the inputsã€‚

 So let's write it like thisã€‚ So W has a dimensionalityã€‚H times Mï¼Œ like on the previous slideã€‚And xã€‚

Ha's the dimensionality n times Mã€‚ it's a design matrixï¼Œ rightã€‚

 But now if you want to multiply those twoï¼Œ you notice that they are not compatibleï¼Œ rightã€‚

 The inner dimensions don't matchã€‚ So that's why we have the transpose hereã€‚ So get meã€‚Write itã€‚

 likeã€‚Thisï¼Œ so this has a dimensional teamã€‚On M timesã€‚And nowï¼Œ so those dimensions matchã€‚Mã€‚

Is H times one dimensionalã€‚So if I multiply thoseã€‚ So these match inner our onesã€‚ So we have nã€‚

 So this one isã€‚H times n plus h times 1ã€‚ So the resultingã€‚Resulting matrixï¼Œ if weã€‚Look at this oneã€‚

The dimensionality of that one should beã€‚Eachã€‚ğŸ˜”ï¼ŒTimesã€‚ğŸ˜”ï¼ŒNï¼Œ howeverã€‚

 it is usually nice for each layer to resemble the input dimensions of the previous layerï¼Œ rightã€‚

 So the original inputï¼Œ you can think of x as the original input wasã€‚N times Mã€‚

 So it would be nice if we have for this one as input to the next layerã€‚

 the same near dimensionalityï¼Œ like the same orderingã€‚ I meanï¼Œ not exactly the same dimensionalityã€‚

 but the in the first dimensionï¼Œ the inputsï¼Œ the number of input should be the sameã€‚

 we are carrying over the same number of training examplesã€‚ So what we would want isã€‚

For a for the outputs to have the functionalityality n times somethingï¼Œ So n timesã€‚Hã€‚

 where H is the hidden dimensionã€‚So in order to achieve thatï¼Œ this is why we have the transposeã€‚

 So with this transposeï¼Œ we have then the n times H dimensionã€‚ So it's usually niceã€‚

 What I mean is it's nice to have the training examples as the rows alwaysã€‚ Nowã€‚

 if you think of thisã€‚What's going on hereï¼Œ you can think of it as a linear transformation that is happening to xã€‚

 So instead of the original M featuresï¼Œ we now have H features and H could be larger or smaller than Mã€‚

 So they are in deep learning both some networks make make it larger and some make it smallerã€‚

So there's a linear transformationï¼Œ like I saidï¼Œ but it's not necessary completely trueã€‚

 So if this is a nonlinear functionï¼Œ then it's not a linear transformationã€‚

 It's a linear transformation that goes through a nonlinear activation functionã€‚

 but we will talk about this activation function next lectureã€‚Soï¼Œ but yeahã€‚

 this is just like the overall notationã€‚ Also in textbooksã€‚

 you may notice that there is no transposeã€‚ So I have a short note about thatã€‚

 And the reason is in textbooks in older textbooksï¼Œ especiallyï¼Œ they haveã€‚Not the n times Nã€‚

 n times M design matrixã€‚ They have a M times n design matrixã€‚

 So they have the columns as and rows switchedï¼Œ which is a little bit confusingã€‚

 I just want to note that in case you stumble upon itã€‚

So usually modern deep learninging has intense and dimensional inputsã€‚

Sometimes it makes things more inconvenient from a linear algebra perspective because youll notice here we have these transposeã€‚

 if you have it the other way around you don't need the transposesã€‚

 but yeah there's always a trade offã€‚

![](img/b5b959b0efe1a59facc6c66c335a8c4c_12.png)

Alrightï¼Œ so why also this W X notationï¼Œ Why not X Wï¼Œ actually in the next videoã€‚

 I will show you that in Pytorchï¼Œ it's the other way aroundã€‚Traditionallyï¼Œ thoughã€‚

 this is convenient because its there's some intuition behind it in terms of linear algebra if you think of traditional methodsã€‚

 how you write things in linear algebraï¼Œ it's that you have a transformation matrix that you apply to a vectorã€‚

 so the vector would be here your feature vectorã€‚And this makes it sometimes or for some peopleã€‚

 easier to think aboutã€‚Linear algebra in a geometry contextã€‚ So here this is just an identity matrixã€‚

 so nothing is going to happenã€‚Soã€‚Technicallyï¼Œ theseã€‚Diaagonalsï¼Œ I think I have a slide on thatã€‚

 These diagonals are for scalingã€‚Or this diagnosis for scaling values in the vector andã€‚

The other diagonal is forã€‚Translation so moving thingsã€‚

And it's like easy to see so why the eigen sorry by the identity matrix here is not doing anything because the scaling is here 1 and 1ã€‚

 so scaling something by one doesn't change it and translation by0 doesn't change anything either So in this wayã€‚

The original inputs are preservedã€‚ So alsoï¼Œ if you think of it as a dot productã€‚æ²¡ã€‚

Remove the notation hereã€‚ If you think of it as a dot productã€‚So one times 0 with this oneã€‚

It's basically1 times x1 plus0 times x2ï¼Œ which is x1ï¼Œ rightã€‚

 And here0 times x1 is 0 plus 1 times x2 is x2ã€‚ So nothing is going on hereã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_14.png)

Yeahï¼Œ he actually has a slide on thatã€‚ He has like a summaryã€‚Of what I just meantã€‚ So you can alsoã€‚

 yeahï¼Œ write it as followsã€‚ So whereasã€‚If you you can be compose this into two separate operations vector edition with this scalr hereã€‚

 and then you can think of this first one as scaling the x coordinateã€‚And thenï¼Œ you can think ofã€‚

This oneã€‚Is scaling the y coordinateã€‚ So like I said beforeï¼Œ these are for scalingã€‚

 and then the other ones are movingã€‚

![](img/b5b959b0efe1a59facc6c66c335a8c4c_16.png)

The vectorsã€‚ So these are for moving the vectorsã€‚Yeahï¼Œ hereï¼Œ I just have a example of the scalingã€‚

 So for exampleï¼Œ if you have a matrix that has a three hereï¼Œ it would scale 3ã€‚ So sorryã€‚

 scale the X X by 3ã€‚ So it's kind of stretching it hereã€‚

And so forth here it scaling or stretching the y axis by a factor of  twoã€‚Andã€‚

This one here at the bottom is doing both stretching and scalingã€‚

 and you can maybe as an optional workï¼Œ think about the translationã€‚

 But in the context of deep learningï¼Œ I think it might be helpful sometimes to think of it as a series of transformationsã€‚

 But yeahï¼Œ also one of the reasons why we use a linear algebra is more like for making notations more compactã€‚

 and we'll show you in the next video how this is achieved in pytorchã€‚



![](img/b5b959b0efe1a59facc6c66c335a8c4c_18.png)

![](img/b5b959b0efe1a59facc6c66c335a8c4c_19.png)