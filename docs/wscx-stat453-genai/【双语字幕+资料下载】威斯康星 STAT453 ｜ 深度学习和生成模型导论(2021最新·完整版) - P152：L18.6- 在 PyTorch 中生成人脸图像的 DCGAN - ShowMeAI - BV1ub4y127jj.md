# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P152ï¼šL18.6- åœ¨ PyTorch ä¸­ç”Ÿæˆäººè„¸å›¾åƒçš„ DCGAN - ShowMeAI - BV1ub4y127jj

All rightï¼Œ so the last topic in this already very long G lecture is on implementing a D G for generating face images and Pytorchã€‚

And just when I looked at the paper againï¼Œ actuallyï¼Œ I just noticed it's also so much Ch till I hearã€‚

 the author of the G tips and tricks that we discussed in the previous videoã€‚Soã€‚Essentiallyã€‚

 the only thing that is different compared to our previous Mist G is that the dataset set isã€‚

 of courseï¼Œ different and that we are now usingã€‚

![](img/ee921d44bc17001fe894df15e01e9953_1.png)

AConvolutionalganï¼Œ instead of fully connected layersã€‚So let's go through this step by stepã€‚

 again our boiler platelate hereã€‚ again notice we have two running rates like beforeã€‚Nowï¼Œ we haveã€‚

Slip a data setã€‚

![](img/ee921d44bc17001fe894df15e01e9953_3.png)

Instead of Mnessã€‚And here's an example of some of the imagesï¼Œ how they look likeã€‚



![](img/ee921d44bc17001fe894df15e01e9953_5.png)

So it's an image data set consisting of celebrities collected from Googleï¼Œ I thinkã€‚

 from the Google image searchã€‚

![](img/ee921d44bc17001fe894df15e01e9953_7.png)

Yeahï¼Œ so and this is our deep conutal G hereã€‚Los of layers for the generatorã€‚

 the con trans post layers because we go from laed dimension by default 100 100 dimensional noise vectorã€‚

We go upã€‚Sample it upï¼Œ soã€‚That it has 64 times 64 dimensionalã€‚

 So I should mention I resized if I go back up againï¼Œ I resize the images hereã€‚

 So I first crop them and then I resize them to 64 times 64ã€‚

 it's easier to train again with smaller images of course there are GNs with bigger images and if you look there are multiple approaches to thatã€‚

 some people train thegan still on lower images and then use super resolution methods some others do that directly but yes you can see these have much higher quality it's just hard to do that and especially with regulargans there are many advanced GNs which we can't all cover in this classã€‚

 but for regular conclusion again like this one it's easier to work with smaller image sizesã€‚

 it also depends really on how large your data set is how many images you have how diverse the images are in terms of colors and view angles and things like thatã€‚



![](img/ee921d44bc17001fe894df15e01e9953_9.png)

![](img/ee921d44bc17001fe894df15e01e9953_10.png)

![](img/ee921d44bc17001fe894df15e01e9953_11.png)

Many many factors to considerã€‚

![](img/ee921d44bc17001fe894df15e01e9953_13.png)

So here I found 64 times 64 worked much better than 128 times 128ï¼Œ for exampleã€‚



![](img/ee921d44bc17001fe894df15e01e9953_15.png)

Okayï¼Œ so it's just a snapshotã€‚

![](img/ee921d44bc17001fe894df15e01e9953_17.png)

Of some randomly sampled peopleã€‚And thenï¼Œ here on onã€‚Generator againã€‚

 So here I'm alternating between transpose convolutionã€‚Or just transpose convolutionã€‚

 I don't use any pooling or something like thatã€‚ It's just or no up samplingling is's just transpose a convolution for making it largerã€‚

Then batchomeï¼Œ likirelu transposeï¼Œ batchomeï¼Œ Liquireluï¼Œ same thingï¼Œ same thingã€‚

 And then the last one with a 10 H function hereã€‚So that we get-1 at1 pixel rangesã€‚

And the discriminator is a little bit and not that much simplerï¼Œ but maybe it's a little bit too bigã€‚

 It's also one aspect of the discriminator too good that it's also then challenging for the generatorã€‚

 soã€‚They could have been a bit too bigï¼Œ but it happened to workã€‚

 So here the disc screenator has a convolutional layerï¼Œ leakqui reelloï¼Œ convolution layerï¼Œ batchomeã€‚

 leakki reloï¼Œ and so forthã€‚ So I'm done sampling with the convolution here instead of using maxpoã€‚

Yeahã€‚And thenï¼Œ there's a flattenã€‚To get from So we don't use any fully connected layerã€‚

 So here this is similar to what we discussed in a convolutional lectureã€‚

 When we said we can actually get rid of the lastã€‚å—¯ã€‚Fully connected layerã€‚ if we make the dimensionsã€‚

 So such hereï¼Œ if I have 64 times 64 inputs at this pointï¼Œ when you do the mathã€‚

 you will have four times 4 feature mapsã€‚ and then I'm applying a convolutional layer with kernel size 4ã€‚

 So from 4 to 4ï¼Œ it will go to 1 to 1ã€‚And thenï¼Œ I go fromã€‚8 channels to one channelã€‚

 So the output will be a 1 by one by one tensorã€‚ And then I'm flattening it into a single valueã€‚

 which is myã€‚Probability that this is a real imageã€‚Because we are using binary cross entropyã€‚

And this looks like before a little bit simpler now compared to ourã€‚

Mnes scan because I wrote all the training function such that it works with imagesã€‚

 And then when I had my Mnes scanï¼Œ I had to adjust a little bit to do the reshaping because a fully connected layer works with back tos not with images right so I had to just do this back and forth here to make it work with my training codeã€‚

 but you could also always write more elegant training code to have an if else statementã€‚

 whether it's a convolution layer or fully connected layerï¼Œ but I yet I didn't do that muchã€‚



![](img/ee921d44bc17001fe894df15e01e9953_19.png)

Work because I felt like the code is already complicated enoughã€‚Hard to read alreadyã€‚Okayã€‚

 so I'm using Adam again for bothã€‚ I tried using SGD for the the screenï¼Œ but it didn't work so wellã€‚



![](img/ee921d44bc17001fe894df15e01e9953_21.png)

Requires also more learningï¼Œ great tuningã€‚ And already it took me a long time to get this running wellã€‚

So yesï¼Œ my training function the same as beforeã€‚ we can take a look at this againã€‚ if you likeã€‚

 I mean there's nothing new so nothing has changedã€‚



![](img/ee921d44bc17001fe894df15e01e9953_23.png)

But there was one thing I wanted to discuss brieflyã€‚

 So one thing I was wondering when I was going over the tips in the previous video that I was thinking of one additional tip that isã€‚

Whether we should sample the noise separately for the discriminator and the generatorã€‚ So hereã€‚

I'm getting the generated fake imagesã€‚ I was wonderingï¼Œ So here I'm using them againã€‚

 So sorry here I'm using them for the the screenator and hereã€‚For the generatorã€‚

 I'm using them againã€‚ So I was wonderingã€‚Instead of having the noise hereã€‚

Like for the screenator and the generatorï¼Œ we could maybe haveã€‚So it's pretty clear if I put it hereã€‚

 we could have it hereã€‚Fake images formã€‚The discreteerã€‚

 but then use different ones for fooling the discreteerã€‚

Find this is maybe also interesting thing to try outã€‚ Actuallyï¼Œ I haven't tried asã€‚

 I could run this and see if it performs better becauseã€‚The rationale could be hereã€‚

 we are training the discriminator to recognize these images as fakeã€‚Hereã€‚

 and then we are training the generatorã€‚sTo basically fool the discer with these imagesã€‚

But I'm wondering if it's just back and forthï¼Œ because we use the same batchã€‚

 whether it might be better to use a fresh batchï¼Œ soã€‚Instead of using the same one for bothã€‚

 having these two different ones might be another trick to tryã€‚

 I haven't tried this might be something else to considerã€‚ Maybe it doesn't make a differenceã€‚

 I don't knowï¼Œ okayã€‚Soï¼Œ but except that the whole training function is still the sameã€‚



![](img/ee921d44bc17001fe894df15e01e9953_25.png)

Going back to my codeã€‚training itã€‚

![](img/ee921d44bc17001fe894df15e01e9953_27.png)

And see kind of stays okayã€‚ So nothingã€‚ So the discscriptator doesn't go to 0ï¼Œ which is badã€‚



![](img/ee921d44bc17001fe894df15e01e9953_29.png)

Or yeahï¼Œ0 loss for the screenã€‚ That means it's way too goodã€‚Andã€‚ğŸ˜”ï¼ŒNothing unusualã€‚

 It's training for long timeï¼Œ20 epochsã€‚

![](img/ee921d44bc17001fe894df15e01e9953_31.png)

Sorryï¼Œ one hour almostã€‚ I meanï¼Œ for the grand scheme of things for deepeningã€‚

 it's actually pretty fastã€‚ But if you try to code the lectureã€‚



![](img/ee921d44bc17001fe894df15e01e9953_33.png)

So that I can talk about itã€‚ I was likeã€‚Yeahï¼Œ a little bit on time constraintsã€‚

 I was happy that it finished so fast because I also had to try different hyperparmetersã€‚ Yeahã€‚

 so it'sï¼Œ it's a lot of work actually to do these codesã€‚

 So I talk to some people during office hours and some students that it' kind of takes a long time to work on the projectã€‚

 But yeahï¼Œ I can totally understand you because also for research or these even these simple class examplesã€‚

 it takes a long timeã€‚ it's just the nature of deep learningã€‚ğŸ˜Šï¼ŒSo yeah plotting thingsï¼Œ I can seeã€‚

 that's interestingã€‚ The generator goes upã€‚ the is great Sã€‚Kind of low herere actuallyï¼Œ to be honestã€‚

 approaching 0ã€‚ But when I look at the resultsï¼Œ they looked actually quite reasonableã€‚



![](img/ee921d44bc17001fe894df15e01e9953_35.png)

So first epoch can see these don't really look like face imagesã€‚ but as we go furtherï¼Œ epoch 5ã€‚

 epoch 10ï¼Œ soã€‚

![](img/ee921d44bc17001fe894df15e01e9953_37.png)

![](img/ee921d44bc17001fe894df15e01e9953_38.png)

Not all of them look realisticï¼Œ but this personã€‚

![](img/ee921d44bc17001fe894df15e01e9953_40.png)

Maybe this personnelï¼Œ to some extentï¼Œ start looking more realisticã€‚ I meanï¼Œ of courseã€‚

 you can see they are generatedã€‚ They are not that greatï¼Œ but they are also not terribleï¼Œ rightã€‚



![](img/ee921d44bc17001fe894df15e01e9953_42.png)

![](img/ee921d44bc17001fe894df15e01e9953_43.png)

![](img/ee921d44bc17001fe894df15e01e9953_44.png)

So yeahï¼Œ it could be betterã€‚ The results could be betterã€‚ This person looks kind of realisticã€‚

 but yeahï¼Œ given that this is very simple codeï¼Œ I only trained 20 epochsã€‚

 It doesn't look too terribleï¼Œ in my opinionã€‚ Actuallyã€‚

 just wanted to show you one that actually failedã€‚

![](img/ee921d44bc17001fe894df15e01e9953_46.png)

![](img/ee921d44bc17001fe894df15e01e9953_47.png)

![](img/ee921d44bc17001fe894df15e01e9953_48.png)

So the difference between the 1 I just showed you and this one is only that here I initially had a regular reã€‚

And when I train it with a regular reã€‚It was first going fineã€‚ And then we had this huge spike hereã€‚

 and then suddenlyã€‚

![](img/ee921d44bc17001fe894df15e01e9953_50.png)

Things looked very differentã€‚ And now looking at the results firstï¼Œ it looks the same as beforeã€‚

 rightã€‚

![](img/ee921d44bc17001fe894df15e01e9953_52.png)

![](img/ee921d44bc17001fe894df15e01e9953_53.png)

Looks okayishã€‚ But then suddenly there's some mod collapse or something like thatã€‚

 You can see they all look the sameã€‚ So it looks like mode collapseã€‚



![](img/ee921d44bc17001fe894df15e01e9953_55.png)

Whereã€‚ğŸ˜”ã€‚

![](img/ee921d44bc17001fe894df15e01e9953_57.png)

Theer can be easily probably fooled by the generator or something like thatã€‚



![](img/ee921d44bc17001fe894df15e01e9953_59.png)

Ger is also very noisyã€‚ It generates the same type of imageã€‚



![](img/ee921d44bc17001fe894df15e01e9953_61.png)

Kind ofã€‚ So againï¼Œ it was also interestingã€‚ Okayï¼Œ so that is a deep convolutionã€‚ Againï¼Œ it's overallã€‚

 if you look at the codeï¼Œ it's a pretty complicated topicï¼Œ rightï¼Œ we have aã€‚



![](img/ee921d44bc17001fe894df15e01e9953_63.png)

Lot of training codeã€‚ But that's just yeahï¼Œ the nature of deep learningã€‚ To be honestã€‚

 it also takes me time to write thisï¼Œ butã€‚

![](img/ee921d44bc17001fe894df15e01e9953_65.png)

I remember when I was a student when this was all kind of new back thenã€‚

 I didn't learn this actually in class because yeahï¼Œ was finished with classes when Gs came outã€‚

 but was reading the paper looking at some early code examplesã€‚

 I think it was the yeah back in the day code was much harder to read compared to Pythtorã€‚

 and it also took me a long time to kind of figure out how the code worksã€‚ Now it's for me easierã€‚

 I can write these things myselfã€‚ but it I would say it's still one of the more complicatedã€‚

Models compared toï¼Œ let's sayï¼Œ classifierã€‚ So if this looks all complicated to youã€‚

 you just need to give it some timeã€‚ You have to maybe implement it yourself or take some codeã€‚

 modify it a little bitï¼Œ maybe apply some of the tricksã€‚ So you couldï¼Œ for instanceã€‚

 apply the trick whereã€‚You flip the labels for the discriminator or occasionally and just get a bit of feeling for the code and where things are andã€‚

Yeah then with some time you will also become pretty confident reading codeã€‚

 but yeah it really takes time so I can imagine just taking one class is not enough to really get confident doing thisã€‚

 it takes probably years and what's very important is really working on the projects where where you just work by yourself I mean not totally by yourself you have your team you can ask me for feedback many students or several students by an office hours where I usually help with a coding parts of their errors but ultimately you need to spend time with the code as no kind of textbook or anything like that that helps you really understanding everything it is really through interaction most of it and also doing some search on the internet and this is how everyone who's doing coding learns it's a processã€‚

 it takes practiceã€‚All rightï¼Œ so with thatï¼Œ let me then end this already long lecture on generative adversial networksã€‚



![](img/ee921d44bc17001fe894df15e01e9953_67.png)