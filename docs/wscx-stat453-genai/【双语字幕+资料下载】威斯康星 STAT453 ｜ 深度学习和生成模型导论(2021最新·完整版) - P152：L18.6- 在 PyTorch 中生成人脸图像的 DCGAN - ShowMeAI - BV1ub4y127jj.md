# P152ÔºöL18.6- Âú® PyTorch ‰∏≠ÁîüÊàê‰∫∫ËÑ∏ÂõæÂÉèÁöÑ DCGAN - ShowMeAI - BV1ub4y127jj

All rightÔºå so the last topic in this already very long G lecture is on implementing a D G for generating face images and Pytorch„ÄÇ

And just when I looked at the paper againÔºå actuallyÔºå I just noticed it's also so much Ch till I hear„ÄÇ

 the author of the G tips and tricks that we discussed in the previous video„ÄÇSo„ÄÇEssentially„ÄÇ

 the only thing that is different compared to our previous Mist G is that the dataset set is„ÄÇ

 of courseÔºå different and that we are now using„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_1.png)

AConvolutionalganÔºå instead of fully connected layers„ÄÇSo let's go through this step by step„ÄÇ

 again our boiler platelate here„ÄÇ again notice we have two running rates like before„ÄÇNowÔºå we have„ÄÇ

Slip a data set„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_3.png)

Instead of Mness„ÄÇAnd here's an example of some of the imagesÔºå how they look like„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_5.png)

So it's an image data set consisting of celebrities collected from GoogleÔºå I think„ÄÇ

 from the Google image search„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_7.png)

YeahÔºå so and this is our deep conutal G here„ÄÇLos of layers for the generator„ÄÇ

 the con trans post layers because we go from laed dimension by default 100 100 dimensional noise vector„ÄÇ

We go up„ÄÇSample it upÔºå so„ÄÇThat it has 64 times 64 dimensional„ÄÇ

 So I should mention I resized if I go back up againÔºå I resize the images here„ÄÇ

 So I first crop them and then I resize them to 64 times 64„ÄÇ

 it's easier to train again with smaller images of course there are GNs with bigger images and if you look there are multiple approaches to that„ÄÇ

 some people train thegan still on lower images and then use super resolution methods some others do that directly but yes you can see these have much higher quality it's just hard to do that and especially with regulargans there are many advanced GNs which we can't all cover in this class„ÄÇ

 but for regular conclusion again like this one it's easier to work with smaller image sizes„ÄÇ

 it also depends really on how large your data set is how many images you have how diverse the images are in terms of colors and view angles and things like that„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_9.png)

![](img/ee921d44bc17001fe894df15e01e9953_10.png)

![](img/ee921d44bc17001fe894df15e01e9953_11.png)

Many many factors to consider„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_13.png)

So here I found 64 times 64 worked much better than 128 times 128Ôºå for example„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_15.png)

OkayÔºå so it's just a snapshot„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_17.png)

Of some randomly sampled people„ÄÇAnd thenÔºå here on on„ÄÇGenerator again„ÄÇ

 So here I'm alternating between transpose convolution„ÄÇOr just transpose convolution„ÄÇ

 I don't use any pooling or something like that„ÄÇ It's just or no up samplingling is's just transpose a convolution for making it larger„ÄÇ

Then batchomeÔºå likirelu transposeÔºå batchomeÔºå LiquireluÔºå same thingÔºå same thing„ÄÇ

 And then the last one with a 10 H function here„ÄÇSo that we get-1 at1 pixel ranges„ÄÇ

And the discriminator is a little bit and not that much simplerÔºå but maybe it's a little bit too big„ÄÇ

 It's also one aspect of the discriminator too good that it's also then challenging for the generator„ÄÇ

 so„ÄÇThey could have been a bit too bigÔºå but it happened to work„ÄÇ

 So here the disc screenator has a convolutional layerÔºå leakqui reelloÔºå convolution layerÔºå batchome„ÄÇ

 leakki reloÔºå and so forth„ÄÇ So I'm done sampling with the convolution here instead of using maxpo„ÄÇ

Yeah„ÄÇAnd thenÔºå there's a flatten„ÄÇTo get from So we don't use any fully connected layer„ÄÇ

 So here this is similar to what we discussed in a convolutional lecture„ÄÇ

 When we said we can actually get rid of the last„ÄÇÂóØ„ÄÇFully connected layer„ÄÇ if we make the dimensions„ÄÇ

 So such hereÔºå if I have 64 times 64 inputs at this pointÔºå when you do the math„ÄÇ

 you will have four times 4 feature maps„ÄÇ and then I'm applying a convolutional layer with kernel size 4„ÄÇ

 So from 4 to 4Ôºå it will go to 1 to 1„ÄÇAnd thenÔºå I go from„ÄÇ8 channels to one channel„ÄÇ

 So the output will be a 1 by one by one tensor„ÄÇ And then I'm flattening it into a single value„ÄÇ

 which is my„ÄÇProbability that this is a real image„ÄÇBecause we are using binary cross entropy„ÄÇ

And this looks like before a little bit simpler now compared to our„ÄÇ

Mnes scan because I wrote all the training function such that it works with images„ÄÇ

 And then when I had my Mnes scanÔºå I had to adjust a little bit to do the reshaping because a fully connected layer works with back tos not with images right so I had to just do this back and forth here to make it work with my training code„ÄÇ

 but you could also always write more elegant training code to have an if else statement„ÄÇ

 whether it's a convolution layer or fully connected layerÔºå but I yet I didn't do that much„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_19.png)

Work because I felt like the code is already complicated enough„ÄÇHard to read already„ÄÇOkay„ÄÇ

 so I'm using Adam again for both„ÄÇ I tried using SGD for the the screenÔºå but it didn't work so well„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_21.png)

Requires also more learningÔºå great tuning„ÄÇ And already it took me a long time to get this running well„ÄÇ

So yesÔºå my training function the same as before„ÄÇ we can take a look at this again„ÄÇ if you like„ÄÇ

 I mean there's nothing new so nothing has changed„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_23.png)

But there was one thing I wanted to discuss briefly„ÄÇ

 So one thing I was wondering when I was going over the tips in the previous video that I was thinking of one additional tip that is„ÄÇ

Whether we should sample the noise separately for the discriminator and the generator„ÄÇ So here„ÄÇ

I'm getting the generated fake images„ÄÇ I was wonderingÔºå So here I'm using them again„ÄÇ

 So sorry here I'm using them for the the screenator and here„ÄÇFor the generator„ÄÇ

 I'm using them again„ÄÇ So I was wondering„ÄÇInstead of having the noise here„ÄÇ

Like for the screenator and the generatorÔºå we could maybe have„ÄÇSo it's pretty clear if I put it here„ÄÇ

 we could have it here„ÄÇFake images form„ÄÇThe discreteer„ÄÇ

 but then use different ones for fooling the discreteer„ÄÇ

Find this is maybe also interesting thing to try out„ÄÇ ActuallyÔºå I haven't tried as„ÄÇ

 I could run this and see if it performs better because„ÄÇThe rationale could be here„ÄÇ

 we are training the discriminator to recognize these images as fake„ÄÇHere„ÄÇ

 and then we are training the generator„ÄÇsTo basically fool the discer with these images„ÄÇ

But I'm wondering if it's just back and forthÔºå because we use the same batch„ÄÇ

 whether it might be better to use a fresh batchÔºå so„ÄÇInstead of using the same one for both„ÄÇ

 having these two different ones might be another trick to try„ÄÇ

 I haven't tried this might be something else to consider„ÄÇ Maybe it doesn't make a difference„ÄÇ

 I don't knowÔºå okay„ÄÇSoÔºå but except that the whole training function is still the same„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_25.png)

Going back to my code„ÄÇtraining it„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_27.png)

And see kind of stays okay„ÄÇ So nothing„ÄÇ So the discscriptator doesn't go to 0Ôºå which is bad„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_29.png)

Or yeahÔºå0 loss for the screen„ÄÇ That means it's way too good„ÄÇAnd„ÄÇüòîÔºåNothing unusual„ÄÇ

 It's training for long timeÔºå20 epochs„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_31.png)

SorryÔºå one hour almost„ÄÇ I meanÔºå for the grand scheme of things for deepening„ÄÇ

 it's actually pretty fast„ÄÇ But if you try to code the lecture„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_33.png)

So that I can talk about it„ÄÇ I was like„ÄÇYeahÔºå a little bit on time constraints„ÄÇ

 I was happy that it finished so fast because I also had to try different hyperparmeters„ÄÇ Yeah„ÄÇ

 so it'sÔºå it's a lot of work actually to do these codes„ÄÇ

 So I talk to some people during office hours and some students that it' kind of takes a long time to work on the project„ÄÇ

 But yeahÔºå I can totally understand you because also for research or these even these simple class examples„ÄÇ

 it takes a long time„ÄÇ it's just the nature of deep learning„ÄÇüòäÔºåSo yeah plotting thingsÔºå I can see„ÄÇ

 that's interesting„ÄÇ The generator goes up„ÄÇ the is great S„ÄÇKind of low herere actuallyÔºå to be honest„ÄÇ

 approaching 0„ÄÇ But when I look at the resultsÔºå they looked actually quite reasonable„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_35.png)

So first epoch can see these don't really look like face images„ÄÇ but as we go furtherÔºå epoch 5„ÄÇ

 epoch 10Ôºå so„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_37.png)

![](img/ee921d44bc17001fe894df15e01e9953_38.png)

Not all of them look realisticÔºå but this person„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_40.png)

Maybe this personnelÔºå to some extentÔºå start looking more realistic„ÄÇ I meanÔºå of course„ÄÇ

 you can see they are generated„ÄÇ They are not that greatÔºå but they are also not terribleÔºå right„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_42.png)

![](img/ee921d44bc17001fe894df15e01e9953_43.png)

![](img/ee921d44bc17001fe894df15e01e9953_44.png)

So yeahÔºå it could be better„ÄÇ The results could be better„ÄÇ This person looks kind of realistic„ÄÇ

 but yeahÔºå given that this is very simple codeÔºå I only trained 20 epochs„ÄÇ

 It doesn't look too terribleÔºå in my opinion„ÄÇ Actually„ÄÇ

 just wanted to show you one that actually failed„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_46.png)

![](img/ee921d44bc17001fe894df15e01e9953_47.png)

![](img/ee921d44bc17001fe894df15e01e9953_48.png)

So the difference between the 1 I just showed you and this one is only that here I initially had a regular re„ÄÇ

And when I train it with a regular re„ÄÇIt was first going fine„ÄÇ And then we had this huge spike here„ÄÇ

 and then suddenly„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_50.png)

Things looked very different„ÄÇ And now looking at the results firstÔºå it looks the same as before„ÄÇ

 right„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_52.png)

![](img/ee921d44bc17001fe894df15e01e9953_53.png)

Looks okayish„ÄÇ But then suddenly there's some mod collapse or something like that„ÄÇ

 You can see they all look the same„ÄÇ So it looks like mode collapse„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_55.png)

Where„ÄÇüòî„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_57.png)

Theer can be easily probably fooled by the generator or something like that„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_59.png)

Ger is also very noisy„ÄÇ It generates the same type of image„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_61.png)

Kind of„ÄÇ So againÔºå it was also interesting„ÄÇ OkayÔºå so that is a deep convolution„ÄÇ AgainÔºå it's overall„ÄÇ

 if you look at the codeÔºå it's a pretty complicated topicÔºå rightÔºå we have a„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_63.png)

Lot of training code„ÄÇ But that's just yeahÔºå the nature of deep learning„ÄÇ To be honest„ÄÇ

 it also takes me time to write thisÔºå but„ÄÇ

![](img/ee921d44bc17001fe894df15e01e9953_65.png)

I remember when I was a student when this was all kind of new back then„ÄÇ

 I didn't learn this actually in class because yeahÔºå was finished with classes when Gs came out„ÄÇ

 but was reading the paper looking at some early code examples„ÄÇ

 I think it was the yeah back in the day code was much harder to read compared to Pythtor„ÄÇ

 and it also took me a long time to kind of figure out how the code works„ÄÇ Now it's for me easier„ÄÇ

 I can write these things myself„ÄÇ but it I would say it's still one of the more complicated„ÄÇ

Models compared toÔºå let's sayÔºå classifier„ÄÇ So if this looks all complicated to you„ÄÇ

 you just need to give it some time„ÄÇ You have to maybe implement it yourself or take some code„ÄÇ

 modify it a little bitÔºå maybe apply some of the tricks„ÄÇ So you couldÔºå for instance„ÄÇ

 apply the trick where„ÄÇYou flip the labels for the discriminator or occasionally and just get a bit of feeling for the code and where things are and„ÄÇ

Yeah then with some time you will also become pretty confident reading code„ÄÇ

 but yeah it really takes time so I can imagine just taking one class is not enough to really get confident doing this„ÄÇ

 it takes probably years and what's very important is really working on the projects where where you just work by yourself I mean not totally by yourself you have your team you can ask me for feedback many students or several students by an office hours where I usually help with a coding parts of their errors but ultimately you need to spend time with the code as no kind of textbook or anything like that that helps you really understanding everything it is really through interaction most of it and also doing some search on the internet and this is how everyone who's doing coding learns it's a process„ÄÇ

 it takes practice„ÄÇAll rightÔºå so with thatÔºå let me then end this already long lecture on generative adversial networks„ÄÇ



![](img/ee921d44bc17001fe894df15e01e9953_67.png)