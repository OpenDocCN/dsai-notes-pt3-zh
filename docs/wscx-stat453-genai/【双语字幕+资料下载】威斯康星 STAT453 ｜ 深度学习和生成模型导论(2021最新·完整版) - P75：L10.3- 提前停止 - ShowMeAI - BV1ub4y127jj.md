# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P75ï¼šL10.3- æå‰åœæ­¢ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ let's talk about some other ways we can use forã€‚Yeahã€‚

 let's talk about some other ways for dealing with overfittingã€‚ For instanceã€‚

 if collecting more data is not feasible and if data augmentation only gets used so farã€‚

So one way would be reducing the network's capacity by other meansã€‚

I decided yeah not to do an extensive slide on that because we discussed several techniques already in that overview video I made with a mind map and also to be honest the really most exciting topic here is dropoutsã€‚

 and I want to spend too much time on other techniques at this pointã€‚

 I will briefly talk about early stopping though and then in the next video I will also talk about2 regularization which are two ways that allow us to yeah reduce the capacity So capacity is essentially interesting term it refers really to the ability of the network to fit complex functions so like you can think of it as the complexity of the decision boundaryã€‚

 So adding a penalty against complexity that's essentially what L2 regularization is it helps reducing the capacity but also let's say adding noise my dropout also reduces the capacity in a wayã€‚

So all these topics are kind of relatedã€‚ So in this videoã€‚

 we will mainly focus on early stopping and in the next video will explain regularizationã€‚



![](img/a2d7402b9548231ad0c815815d3e7568_1.png)

Yeahï¼Œ and as far as early stopping is concernedã€‚ So there's a general strategy hereã€‚

 So this is usually something I always recommend when you work in deep learningã€‚

 you split your dataset set into three partsï¼Œ a large training set and the second largest chunk should be yourã€‚

Data set for testing and then a smaller validation set that you use for model tuning and the test set should only ideally used once and the very end once you yeahã€‚

 you found your modelï¼Œ you are happy with it by tuning it on the validation setã€‚

 then you use your test set to evaluate the final performance of this modelã€‚And yeahã€‚

 the validation set is essentially for tuning the modelã€‚ But then you can also use itï¼Œ of courseã€‚

 for early stoppingã€‚ That's part of the tuning essentiallyã€‚ So last weekã€‚

 I showed you some real dataï¼Œ like theã€‚

![](img/a2d7402b9548231ad0c815815d3e7568_3.png)

Trainingã€‚ğŸ˜”ï¼ŒOver the epochs and the accuracyï¼Œ we also did the same for the lossã€‚

 So you can also do that for the loss hereï¼Œ just for the sake of keeping things simpleã€‚

 I'm just showing you an idealized drawing of thatã€‚

So early stopping is essentially yeah a method for looking at the training and validation set performance and you from there you try to deduce a good point for stopping the trainingã€‚

 for instanceï¼Œ you may see that the validation accuracy goes upã€‚

So the validation set serves as a way of estimating the generalization performanceã€‚

 you can say the generalization performance improvesã€‚

Up to a certain point where you find that the validation set performance goes down againã€‚

 So in that wayï¼Œ training longer may make your performance worseã€‚ Howeverã€‚

 there might be some exceptions sometimes like we've seen in the double descent paper last weekã€‚

 but in general there might be something like this where you find that if you stop the training early actually get a better performanceã€‚

In practiceï¼Œ this method is not super common anymoreã€‚

 I mean it's not totally uncommon to do that but I would say first try it's either L2 regularization or even better using dropout to improve generalization performance and once you use both techniques and you still find that early stopping helps then I would probably consider early stopping but I would first consider the other two techniques I'm going to present in the next two videosã€‚

But againï¼Œ this is yeah also a useful technique to know aboutã€‚

 So in the next video I want to show you L2 regularizationã€‚

 which is essentially a penalty against complexityã€‚ and then we will get to the main topic dropoutã€‚



![](img/a2d7402b9548231ad0c815815d3e7568_5.png)

![](img/a2d7402b9548231ad0c815815d3e7568_6.png)