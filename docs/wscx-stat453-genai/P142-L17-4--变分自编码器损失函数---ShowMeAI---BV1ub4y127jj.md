# P142ÔºöL17.4- ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÊçüÂ§±ÂáΩÊï∞ - ShowMeAI - BV1ub4y127jj

AlrightÔºå let me now give you the brief picture overview of the loss function of the variational auto encoder„ÄÇ



![](img/c021d3119325b31b7ff4031823e44158_1.png)

So overall it's about minimizing elbow the evidence lower bound so if you have not heard about elbow or the evidence lower bond before don't worry about it too much for this class So for here it's not really that important that would be something for a follow up class or yeah maybe you would encounter this in a class on the relational Bayesian methods or something like that so„ÄÇ

For hereÔºå maybe it's not that important„ÄÇ but overall„ÄÇ

 we are optimizing or minimizing a loss that consists of„ÄÇYeah„ÄÇ

 minimizing the expected negative likelihood of the data and then the Colbe lab divergence term„ÄÇ

 which is the difference essentially between this latent space distribution and a standard multivariate Gaussian distribution„ÄÇ

So technicallyÔºå you would be maximizing the lock likelihood of the data„ÄÇ

 but as we discussed before when we talked about logistic regression a long time ago„ÄÇ

 maximizing the lock likelihood is the same as minimizing the negative lock likelihood„ÄÇ

 So that's why I've written it like this with this negative sign here„ÄÇYeahÔºå but again„ÄÇ

 the details are not that important„ÄÇ LikeÔºå if you have not encountered this before„ÄÇ

 don't worry about it too much„ÄÇ The big picture is that this is„ÄÇA reconstructionconstruct„ÄÇLast term„ÄÇ

 it's essentially the reconstruction between all the difference between input and output distribution here„ÄÇ

And the K divergence is making sure that this one here has is essentially standard multivaried Gaussian distribution„ÄÇ

Now„ÄÇIf you would look at some implementations of variational out encodersÔºå what you might find is„ÄÇ

That usually for the reconstructionist people„ÄÇEither use the binary cross entropy„ÄÇAll the mean„ÄÇ

Squared error„ÄÇBoth work and practice„ÄÇPersonally„ÄÇI would say it makes more sense to use the mean squarearrow„ÄÇ

Because„ÄÇSo people use the B CE when they„ÄÇNormalize the images to 0Ôºå1 range„ÄÇ And they also have the 0„ÄÇ

1 range here„ÄÇThe pixels„ÄÇOriginally image pixels are between 0 and 255 and you can normalize them in 01 range and in fact this is done automatically in Pytorch when we call the2 tenor method in the data transform so we will see that in the code example„ÄÇ

ÂóØ„ÄÇAnd then you can have here„ÄÇSigmoid activationÔºå logistic sigmoid„ÄÇ

 and then you will also automatically ensure that the outputs are in a 01 range„ÄÇAnd„ÄÇ

It might be tempting then to use a binary cross entropy„ÄÇBut keep in mind„ÄÇ

 this is not a benoulli distribution„ÄÇ I meanÔºå this is not0 or1Ôºå the value„ÄÇOf the pixel„ÄÇ

 it's between 0 and 1„ÄÇAnd franklyÔºå I don't know why some people would use BE for that„ÄÇ

 I would use the BE if that would be a moivate benoli distribution with I have values like zeros and ones„ÄÇ

For the inputÔºå But we have values between 0 and 1„ÄÇ So to meÔºå it makes more sense to use the M C„ÄÇ

And also in practiceÔºå when you do some comparisons„ÄÇ

 you will find probably So it's at least what I found that if you use the B CE„ÄÇ

 the images would look a little bit blurrierÔºå could be because„ÄÇ

It's kind of forcing the pixels to be more in the center„ÄÇ



![](img/c021d3119325b31b7ff4031823e44158_3.png)

Of the distribution„ÄÇIn any caseÔºå so yeahÔºå what„ÄÇ

![](img/c021d3119325b31b7ff4031823e44158_5.png)

I also want to show youÔºå let me go to the next part„ÄÇ

 maybe I wanted to show you that the cross entropy is not symmetric„ÄÇ

So this is also one issue where I think it's probably not a good idea to use the binary cross entropy as the reconstructionness„ÄÇ

Because„ÄÇThe loss is different depending on the pixel values„ÄÇWhat I mean by that„ÄÇ

 if what I'm showing you on the left hand side is a plot„ÄÇ

 where I plotted the binary cross entropy loss„ÄÇFor values of the true and the predicted label„ÄÇÊòØ„ÄÇXing„ÄÇ

üòîÔºå0ero is probably a bad exampleÔºå but let's say 0„ÄÇ2„ÄÇHere„ÄÇ

 what I mean is when we consider cross entropyÔºå that the true value is 0„ÄÇ2„ÄÇ

And the predicted value is pointuneÔºå So that's how we compute„ÄÇIt should be a minus„ÄÇAcross entropy„ÄÇ

 Oh yeahÔºå for referenceÔºå I have it also here„ÄÇ So recall the binary cross entropy from the logistic regression class„ÄÇ

 We have a probability„ÄÇHere„ÄÇAnd look the probability here„ÄÇUsually in the logistic regression context„ÄÇ

This was either either a one or a0„ÄÇNow we have values between 0 and1„ÄÇSoÔºå if I have„ÄÇ

Pixel in my input image of 0„ÄÇ2 and I predict a pixel of 0„ÄÇ2„ÄÇ I would get a loss„ÄÇIt's around 0„ÄÇ5 here„ÄÇ

NowÔºå here what I have is I have an input pixel of 0„ÄÇ4„ÄÇüòîÔºåAnd a predicted pixel value of 24Ôºå as well„ÄÇ

I can see this„ÄÇThe loss is higher„ÄÇ Its aroundÔºå I don't knowÔºå6„ÄÇ5Ôºå something like that„ÄÇ So you can see„ÄÇ

Even though I reconstruct the right pixel „ÄÇ4 is the reconstructionÔºå and  point4 is the„ÄÇ

Original label„ÄÇI get a higher loss now compared as if it was 02Ôºå so it's not symmetric„ÄÇ

 I get different losses depending on what my pixel values are„ÄÇWe just„ÄÇWhich doesn't„ÄÇ

 which doesn't really make sense„ÄÇ So if you have a means square error loss„ÄÇHere„ÄÇ

This is said say my0 here„ÄÇWe would have0 everywhere„ÄÇCause the MEÔºå there is no difference between the„ÄÇ

Po„ÄÇ4 and the „ÄÇ4„ÄÇ So the M E would be 04 minus-„ÄÇ4„ÄÇSquadÔºå which would be„ÄÇ0„ÄÇ

 that makes more sense to meÔºå at least„ÄÇ Well here's another example„ÄÇ

 Consider we have the true pixel value „ÄÇ8„ÄÇAnd the reconstructed pixel value is 0„ÄÇ7„ÄÇ

 So we are off by 0„ÄÇ1„ÄÇWe would get a loss of 0„ÄÇ28„ÄÇIf I have now 0„ÄÇ9 years for reconstructionÔºå still„ÄÇ

 I'm off by only 0„ÄÇ1„ÄÇNow I get a „ÄÇ08„ÄÇRightÔºå so for some reason„ÄÇIt penalizes this „ÄÇ7 more than this 0„ÄÇ

9„ÄÇCause it's not symmetricÔºå same as it'll have  0„ÄÇ2 here as a true pixel„ÄÇ and I put it 0„ÄÇ1 hereÔºå 0„ÄÇ3„ÄÇ

 I will find these different values„ÄÇYeahÔºå this is why I would suggest for the variation out encoder to„ÄÇ

 if you have input pixels between 0 and 1 or„ÄÇMe between  minus1 and 1 doesn't matter„ÄÇ

If you have input pixels on a continuous rangeÔºå I would suggest using the ME instead of the binary cross entropy for the reconstruction term„ÄÇ



![](img/c021d3119325b31b7ff4031823e44158_7.png)

AllrightÔºå so„ÄÇThis was„ÄÇ

![](img/c021d3119325b31b7ff4031823e44158_9.png)

Now the first termsÔºå if I go back one more slide„ÄÇSo this was the first term that we discussed the reconstruction us„ÄÇ

 So I suggest using the mean squarearrow for that„ÄÇ

![](img/c021d3119325b31b7ff4031823e44158_11.png)

So the mean here is over the examples in the batch„ÄÇFor if you only have one training example„ÄÇ

 you can think of it as the sum square error between all the pixels„ÄÇ

 So it's essentially a Euclidean distance„ÄÇ so you would„ÄÇFor each pair of pixels„ÄÇ

 compare the difference„ÄÇSquare it„ÄÇ and then you sum them all upÔºå and you take the square root„ÄÇ

 So you have the original dimension backÔºå and then you can average that over the„ÄÇBtch„ÄÇ

 and that would be then your mean square error„ÄÇAnd I will show youÔºå of course„ÄÇ

 in the code example how that looks likeÔºå I think that will also make things more clear„ÄÇ

The second term is the KL divergence term„ÄÇSo this is measuring the difference between the distribution here and the latent space„ÄÇ

The sport„ÄÇSo we have our mean vector and our block var vector„ÄÇSo it shouldÔºå I mean„ÄÇ

 I'm only showing it as one vectorÔºå but there should be technically two vectors„ÄÇ

 the mean vector and the logbar vector here I'm just writing a sustainability deviation„ÄÇÂóØ„ÄÇSoÔºå we are„ÄÇ

Minimizing the difference between„ÄÇThis distribution of the latent space and a standard normal distribution„ÄÇ

So we use the K divergence term for that„ÄÇAnd„ÄÇIt can be written as follows as shown here„ÄÇ

 I'm not showing you here the derivation„ÄÇ I'm just showing you how you can write that„ÄÇ



![](img/c021d3119325b31b7ff4031823e44158_13.png)

I have two slides laterÔºå I have the derivation here„ÄÇ

 but I don't want to go into through this in detail if you're interested„ÄÇ

 I actually got this from this link here I was not writing this myself„ÄÇ

 but if you're interested in studying this you can find it here so you derive the KL loss„ÄÇ

For the very short encoder„ÄÇ

![](img/c021d3119325b31b7ff4031823e44158_15.png)

Going back to this one hereÔºå what's more interesting is to take a quick look at this so you can see„ÄÇ

We have this„ÄÇOne plus here„ÄÇPlus this lock barÔºå This is from the Lobar trick„ÄÇAnd yeah„ÄÇ

 you can see this would be minimized if„ÄÇWe have one minus„ÄÇThe variance one„ÄÇ

 If we have a variance of one„ÄÇ So we would„ÄÇMinimize this if the variance is actually one„ÄÇSo lock of„ÄÇ

1 should be also0 thenÔºå right„ÄÇAnd if this is 0„ÄÇ So if we have one„ÄÇPlus 0Ôºå-0Ôºå minus„ÄÇ-1„ÄÇGive us 0„ÄÇ

 This is when our K divergence term is minimized„ÄÇSo what we want here„ÄÇI„ÄÇ

This mean vectorton to be close to 0 and the variance to be close to 0„ÄÇcloseose to one„ÄÇ

 which happens if this lock wire term is close to 0„ÄÇ

So that's essentially what the KL divergence term forces theseÔºå this„ÄÇAnd this vector to be„ÄÇAnd then„ÄÇ

 if this is„ÄÇTrueÔºå I meanÔºå if our mean vector is 0 and our variance vector is 1„ÄÇ So this is one„ÄÇ

And this is 0„ÄÇThen„ÄÇWhat we have„ÄÇI that it's our distribution„ÄÇWill be„ÄÇüòî„ÄÇ

It's the abnormal distribution„ÄÇThat is our goal for this latent representation„ÄÇ Alright„ÄÇ

 so this is essentially in a nutshellÔºå the variation or and loss function„ÄÇ



![](img/c021d3119325b31b7ff4031823e44158_17.png)

AgainÔºå if you're interestedÔºå you can take a look at the loss derivation„ÄÇAnd now in the next video„ÄÇ

 I will finally show you how this all comes together in a code example where we have the sampling aspect„ÄÇ

 the logva trick and the loss functionÔºå everything together in one code example„ÄÇ

Where I will show you that we can train this auto encoder to yeah generate handwritten digits„ÄÇ

 but since handwritten digits are by nowÔºå I think a little bit boring„ÄÇ

 we will also use a second data set and apply this concept of face images„ÄÇ



![](img/c021d3119325b31b7ff4031823e44158_19.png)