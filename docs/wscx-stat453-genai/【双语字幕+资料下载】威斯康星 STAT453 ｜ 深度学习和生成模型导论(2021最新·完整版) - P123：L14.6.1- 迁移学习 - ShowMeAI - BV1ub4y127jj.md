# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P123ï¼šL14.6.1- è¿ç§»å­¦ä¹  - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ all good things and all lectures come to an endã€‚

 So this is the last topic in the series on conversion network architecturesï¼Œ soã€‚

This is not really a topic specific to convol networksã€‚

 but I think right now is a good time to talk about it because there are some nice pretrained computer vision models available that could be useful for thisã€‚

 So the topic is transfer learningã€‚ And I think transfer learning is particularly useful for your class projectsã€‚

 So the key key idea here is that in a convol networkï¼Œ we have this feature extraction partã€‚ğŸ˜Šã€‚



![](img/9141fdfe10df5f24e1287669bc317fd9_1.png)

å•Šã€‚W extractionã€‚ Let's write it like thisã€‚ And then we have these fully connected layersã€‚

Or also convolution layers asï¼Œ as you remember from the previous two videosã€‚

 But let's call them for fully con connected layers orã€‚Linear layersï¼Œ these are usually the onesã€‚

 the multi layers perceptioncept part that we use for classification or or classifier partã€‚

 And the idea in transfer learning is that these con layers hereã€‚

 this automatic feature extraction pipeline might be useful for other related tasks as wellã€‚ Soã€‚

 for instanceï¼Œ I know that a lot of you are working on projectsã€‚

 class projects related to Covid 19 prediction from chest X ray dataã€‚ So there might beã€‚

An hypothesis that let's say a network trained on a large medicalã€‚X ray chest X ray data setã€‚

 not related to Covid 19ï¼Œ more generalï¼Œ might be also useful for Covid 19 detection if it's fine tuned on the dataã€‚

 So usually we in deep burning have these large benchmark dataset setsã€‚

 but they are usually only used forï¼Œ of courseã€‚Comparing and benchmarking different models in a real applicationã€‚

 you usually have a much smaller data setã€‚ So the question isã€‚

 can we leverage these large amounts of dataã€‚Train the network on those and then fine tune in those networks on our smaller target datasetã€‚

So the idea is that the feature extraction layers may be maybe beã€‚Generally usefulã€‚

 And then we can use pre trained modelsã€‚ For instanceã€‚

 models pre traineded on imagenet freeze the weightsã€‚ So that meansã€‚

When we are freezing these weightsï¼Œ keeping them fixedã€‚

 not updating them and only training the last few layers like the fully connectedã€‚Layersã€‚

 the moular perceptron partã€‚So this would be in in the nutshellã€‚

 this would be transfer learning related to thatã€‚ there's also the approach that you train the whole network on a given data and then fine tune the whole network to the smaller datasetã€‚

 so it's essentially just a special case of transfer learning where you don't freeze the weightsã€‚



![](img/9141fdfe10df5f24e1287669bc317fd9_3.png)

So here has a screenshot from an older paperã€‚ it's on a large scale video classification with convolutional networksã€‚

 but this kind of is a nice summary of whatã€‚Options we have when we use transfer learningï¼Œ soã€‚

Focusing on this hereã€‚ So they evaluated the performance by training a conversion network from scratchã€‚

So here they are talking about the accuracyï¼Œ they got a 41% accuracyã€‚Onã€‚

Pretrain a network on a large dataset and then only fine tuned the last layerã€‚

 Yet top layer means the output layerã€‚ They got a 64% accuracyï¼Œ fine tuning the top three layersã€‚

 So last three layersï¼Œ they got the best accuracyï¼Œ65% and fine tuning all layersã€‚They got 62%ã€‚

 So hereï¼Œ based on thisï¼Œ of courseï¼Œ it's just one experiment or one scenario based on this case hereã€‚

 they got or they found out that tuning three layersã€‚Gives the best resultsã€‚ Butï¼Œ of courseã€‚

 your mileage may varyã€‚ It really depends on the network architectureã€‚

 on the data set and many other thingsã€‚ So it's not generalã€‚

Case that this is always true and practice it's just another hyperparmeterã€‚

 So how many layers to fine tune is just another hyperparmeter to considerã€‚But yeahã€‚

 you can also see evenã€‚So what is also interesting when you look at these two hereã€‚

 that train from scratch isã€‚Much worseï¼Œ let's sayï¼Œ than pretrain the network on a large dataset set and then fine tuning itã€‚

 even if you don't freeze any layersã€‚I will show you in the next video how we can do this in Pyã€‚

 So for nowï¼Œ if this is very abstractï¼Œ we will get more concrete in the next videoã€‚



![](img/9141fdfe10df5f24e1287669bc317fd9_5.png)

So here's what I'm going to show you in the next videoï¼Œ so because we learned already about V G16ã€‚

 it's probably the easiest one to explainã€‚

![](img/9141fdfe10df5f24e1287669bc317fd9_7.png)

But of courseï¼Œ this is a original conceptã€‚ So what we are going to do in the next video is we are going to freeze the convolution layersã€‚

And then I will show you how we can replaceã€‚Although I should say firstã€‚

 we use the pre trained version that has been trained on imagenetï¼Œ then we freeze these layersã€‚

And replace the output layers and then fine tune these on a different datasetã€‚

 We are going to use Cypher 10 but of course it could be any datasetã€‚

 it's just because Cypha 10 is already in Pywa so you don't have to download anythingã€‚

 So I'm always using Cypha 10 to just make the code simpler or smaller I showed you before how you can use your own datasetã€‚

 but I don't want you to always have to download a dataset when you run the code examples from classã€‚

 So that is why we use Cypher 10ã€‚ but of course this is a very general conceptã€‚ soã€‚



![](img/9141fdfe10df5f24e1287669bc317fd9_9.png)

There are besides VG G16ï¼Œ also other models availableã€‚

 So VG G16 might not be the best model we have learnedã€‚ it's actually not the best performing oneã€‚

 It's also very expensiveã€‚ So for instanceï¼Œ for your class projects except if you want to get a good performance the wide resnet is actually pretty good and Inception version 3 is also pretty good in terms of performanceã€‚

 but they are also slow to runã€‚ the good one is actually mobile net it's actually pretty fast well performingform networkã€‚

 and of courseï¼Œ our residual netsã€‚ but yeah the wide residual net could can be sometimes betterã€‚

 not always when I tested it on Cypher 10ï¼Œ it actually was worse than the regular renetã€‚

 So also your mileage may vary thereã€‚ So okay so oh yeah one more thing when we do transfer learningã€‚

 I also wanted to warn youã€‚

![](img/9141fdfe10df5f24e1287669bc317fd9_11.png)

You have to double check how the people who pretrain the network and provide it to youã€‚

 how they normalize the data so you can of course train or pretrain the networks yourselfã€‚

 but in Pythtorarch there are some of these models available that have been pretrained on imageNe if you want to use those you have to be careful to use the same data normalization that they used and for imagenet they used this normalization so usually before we used 0ã€‚

50ã€‚5 and 0ã€‚5 because we observed that whether we do that or compute the exact mean and cell deviation doesn't make any difference in practice but because they use these parameterss we also have to use the same parameters when we apply to a different data set to make sure our datas on the same scale as the network might expectã€‚



![](img/9141fdfe10df5f24e1287669bc317fd9_13.png)

Alrightï¼Œ so in the next video I will show you then concretely how that looks like in Pytorchã€‚



![](img/9141fdfe10df5f24e1287669bc317fd9_15.png)