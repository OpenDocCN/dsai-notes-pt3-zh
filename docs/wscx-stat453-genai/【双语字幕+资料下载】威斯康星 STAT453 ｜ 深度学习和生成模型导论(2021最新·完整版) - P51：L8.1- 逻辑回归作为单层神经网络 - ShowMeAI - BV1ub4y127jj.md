# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P51ï¼šL8.1- é€»è¾‘å›å½’ä½œä¸ºå•å±‚ç¥ç»ç½‘ç»œ - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let's dive in into the topic of logistic regression by viewing it as a single layer neural networkã€‚



![](img/afcfdf7c4d85c778b97d544b8ea331ac_1.png)

And yeahï¼Œ this is why I emphasized the identity function in Adeline so muchã€‚

Because if we look at this figure hereã€‚This is a single layer neural network where we have the inputs xã€‚

 the weights Wï¼Œ and then we compute the net inputã€‚Here and the activation hereã€‚

And then an output here where weï¼Œ for exampleï¼Œ could apply a threshold functionã€‚But for trainingã€‚

 we can ignore the threshold function for trainingã€‚

 we usually yeah use the output from the activationã€‚

 So in add line this activation function was an identity functionã€‚So this was in Adelineã€‚

And in Adelineï¼Œ then we use the mean squared error as a loss function between we computed it between the activation and the true class tablesã€‚

So here aï¼Œ this is the activation hereï¼Œ and then we had some true class ables y and computed the mean squared errorã€‚

 Nowï¼Œ the only difference really between a line and logistic regression is that we have a different activation functionã€‚

 So one difference is that we now use this logistic sigmoid function instead of the identity functionã€‚

 So inã€‚A alineï¼Œ againï¼Œ we had yet just the identity function Zã€‚ So input Z output Zã€‚

 Now we have thisã€‚Non nonlinear functionã€‚ So this a nonlinearã€‚Actation functionã€‚

I will show you in the next slides how it looks likeã€‚

And another difference between logistic regression and Adeline is that we now have a different loss functionã€‚

 I will cover the loss function in logistic regression in the next videoã€‚

 though so that we can keep this video a bit shorterã€‚



![](img/afcfdf7c4d85c778b97d544b8ea331ac_3.png)

Alrightï¼Œ so here is how the logistic sigmoid function looks likeã€‚

 So that's just from the previous slideã€‚So what I showed you on the right hand side is this one I meanã€‚

 there are two forms you can write it as usually this one is the more common form in deep learning or in deep learning contextã€‚

 we will be using this activation function also in the context of multilaycepttranslator So how you get from here to here oops from the left hand side to the right hand side is just by by dividing by E to the power of zã€‚

 So if you divide thisã€‚By e to the above of zã€‚You get the right hand because yepï¼Œ this would be oneã€‚

 This would be oneï¼Œ and this part will beã€‚E minus Zï¼Œ rightã€‚Becauseã€‚Of this relationshipã€‚ Alrightï¼Œ soã€‚

Here how it looks likeã€‚ So we have the activation here plotted on the Y axis and the net inputã€‚

 which is passed to the activation function on the x axisã€‚

 So this is really the input to the activation function hereã€‚And you can see that yeahã€‚

 it's centered around0ã€‚ so it's centered around 0ã€‚ and if the input is 0ã€‚Thenï¼Œ the output will beã€‚

Point5ã€‚And it saturates here at oneã€‚ So the maximum value is1 it approaches 1ã€‚

 and here on the other handï¼Œ it approaches 0ã€‚ So it's a function between0 and1ã€‚Alrightã€‚

 so but you can see you have to have very negative or very positive valuesã€‚

 So this is yeah how this function looks likeã€‚

![](img/afcfdf7c4d85c778b97d544b8ea331ac_5.png)

So yeahï¼Œ I will explain to you how the logistic regression loss function works like in the next video hereã€‚

 I just wanted to summarize a few thingsï¼Œ and it may be that you have to think a bit about this because it can look a little bit complicated at first glanceã€‚

 but essentially it's not that complicatedã€‚ So let me just use this notation here to summarize the logistic regression model where H you can think of it as the hypothesis in traditional machine learning or you can more simple terms Think of it as the modelã€‚

 This is our logistic regression modelï¼Œ which receives as inputã€‚Net inputã€‚

And then you have this logistic sigmoid activation function that I showed you in the previous slideã€‚

Whi returns yeah a value between 0 and 1ã€‚ and you can actually think of this whole model here as a model that computes the posterior probabilityã€‚

 the probability of class label Yï¼Œ given a feature vector xã€‚ So y is the class labelã€‚And x is theã€‚

Pitchonã€‚hoopsï¼Œ like toã€‚So you can think of it as this andã€‚Nowï¼Œ let's say we haveã€‚Yeahã€‚

 we have class tableã€‚1ï¼Œ so let'sï¼Œ let's look at a concrete exampleã€‚

 Let's consider the Iis data set where we have class Na1ã€‚Sã€‚Let's say it's a tooã€‚

Now for simplified case for a simplified scenarioï¼Œ think of it as a binary classification problemã€‚

 we haveã€‚1 is thetosa and 0 Sã€‚Wy columnï¼Œ these two classesã€‚Nowã€‚

If I want to compute the probability that a given feature vector belongs to Ctoa to class 1ã€‚

 I can write it as this Pã€‚Y equalsï¼Œ let's say one because it's shorter given xã€‚

 And this is computed asã€‚It's this oneã€‚ And the probabilityã€‚Of y equals 0 is computed as xã€‚

 which we can write asã€‚1 minus h of xã€‚So these are the feature vectors you these axisã€‚Noã€‚ğŸ˜”ï¼ŒSoã€‚

Imagine the probability for the tourismã€‚Iã€‚Pot8ï¼Œ then equivalentlyï¼Œ the probability forã€‚

Versy color would be pointï¼Œ2ã€‚Rightï¼Œ so if this is indeed trueã€‚

If the flower is indeed versica as a toerï¼Œ then this would be actually pretty goodã€‚

 So let's say we assume we have a true labelã€‚Let's assume thatã€‚True labelã€‚It'sï¼Œ actuallyã€‚Wenã€‚

Then it would be actually pretty good to get a accuracy of 80%ï¼Œ the higherï¼Œ the better thoughã€‚

 so we want to actually have a high probabilityã€‚But if the true labelã€‚Is 0ã€‚

 then a high probability would actually be bad if theã€‚True label is0ï¼Œ what we wantã€‚

Is actually a high probability forã€‚Piã€‚ğŸ˜”ï¼ŒWhyï¼Œ so I can just maybe useã€‚Different colourã€‚ what we wantã€‚

Here is we want to have a high probability for this caseã€‚ then if the true label is indeed0ã€‚

 then we want to have a 80%ï¼Œ maybe for this caseã€‚ So that is what I'm summarizing here at the bottom of this slideã€‚

 So if we have a binaryã€‚Classification problem with possible class labels 0 and1ã€‚

 What we want is these probabilities to be close to one for the respective classã€‚

 So if the class labelï¼Œ the true class label is 0ï¼Œ then we want the probability for y equals 0ã€‚

To be close to oneã€‚If the true label is oneï¼Œ then we want the probability for this scenario to be close to oneã€‚

So in this caseï¼Œ it's yeahï¼Œ it's depending which probability we want to be maximized depends on the true label hereã€‚

 So we will look at that howï¼Œ how we can do that withã€‚Loss function in the next videoã€‚

 but this is like the basic setupã€‚ So we want a high probabilityã€‚Givenï¼Œ yeahï¼Œ the certain classã€‚

 So vice versaï¼Œ we can also think of it like this if we haveã€‚Ass a tools are labelã€‚

We want this one to be largeï¼Œ which gives us the scenario for class1ã€‚ So if we have a sattoaã€‚

Case make this one notchã€‚Make this oneã€‚Smallã€‚Howeverã€‚

 let me go erase that because we don't have that much spaceã€‚ if we have a true labelï¼Œ worthyy colourã€‚

Thenï¼Œ make this one smallã€‚And make this oneã€‚Larchã€‚So this is essentially what we want to achieveã€‚

 and I will show you how we can do that in the next videoã€‚



![](img/afcfdf7c4d85c778b97d544b8ea331ac_7.png)