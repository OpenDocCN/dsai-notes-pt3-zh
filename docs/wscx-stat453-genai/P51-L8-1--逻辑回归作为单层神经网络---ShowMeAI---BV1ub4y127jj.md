# P51ÔºöL8.1- ÈÄªËæëÂõûÂΩí‰Ωú‰∏∫ÂçïÂ±ÇÁ•ûÁªèÁΩëÁªú - ShowMeAI - BV1ub4y127jj

AlrightÔºå let's dive in into the topic of logistic regression by viewing it as a single layer neural network„ÄÇ



![](img/afcfdf7c4d85c778b97d544b8ea331ac_1.png)

And yeahÔºå this is why I emphasized the identity function in Adeline so much„ÄÇ

Because if we look at this figure here„ÄÇThis is a single layer neural network where we have the inputs x„ÄÇ

 the weights WÔºå and then we compute the net input„ÄÇHere and the activation here„ÄÇ

And then an output here where weÔºå for exampleÔºå could apply a threshold function„ÄÇBut for training„ÄÇ

 we can ignore the threshold function for training„ÄÇ

 we usually yeah use the output from the activation„ÄÇ

 So in add line this activation function was an identity function„ÄÇSo this was in Adeline„ÄÇ

And in AdelineÔºå then we use the mean squared error as a loss function between we computed it between the activation and the true class tables„ÄÇ

So here aÔºå this is the activation hereÔºå and then we had some true class ables y and computed the mean squared error„ÄÇ

 NowÔºå the only difference really between a line and logistic regression is that we have a different activation function„ÄÇ

 So one difference is that we now use this logistic sigmoid function instead of the identity function„ÄÇ

 So in„ÄÇA alineÔºå againÔºå we had yet just the identity function Z„ÄÇ So input Z output Z„ÄÇ

 Now we have this„ÄÇNon nonlinear function„ÄÇ So this a nonlinear„ÄÇActation function„ÄÇ

I will show you in the next slides how it looks like„ÄÇ

And another difference between logistic regression and Adeline is that we now have a different loss function„ÄÇ

 I will cover the loss function in logistic regression in the next video„ÄÇ

 though so that we can keep this video a bit shorter„ÄÇ



![](img/afcfdf7c4d85c778b97d544b8ea331ac_3.png)

AlrightÔºå so here is how the logistic sigmoid function looks like„ÄÇ

 So that's just from the previous slide„ÄÇSo what I showed you on the right hand side is this one I mean„ÄÇ

 there are two forms you can write it as usually this one is the more common form in deep learning or in deep learning context„ÄÇ

 we will be using this activation function also in the context of multilaycepttranslator So how you get from here to here oops from the left hand side to the right hand side is just by by dividing by E to the power of z„ÄÇ

 So if you divide this„ÄÇBy e to the above of z„ÄÇYou get the right hand because yepÔºå this would be one„ÄÇ

 This would be oneÔºå and this part will be„ÄÇE minus ZÔºå right„ÄÇBecause„ÄÇOf this relationship„ÄÇ AlrightÔºå so„ÄÇ

Here how it looks like„ÄÇ So we have the activation here plotted on the Y axis and the net input„ÄÇ

 which is passed to the activation function on the x axis„ÄÇ

 So this is really the input to the activation function here„ÄÇAnd you can see that yeah„ÄÇ

 it's centered around0„ÄÇ so it's centered around 0„ÄÇ and if the input is 0„ÄÇThenÔºå the output will be„ÄÇ

Point5„ÄÇAnd it saturates here at one„ÄÇ So the maximum value is1 it approaches 1„ÄÇ

 and here on the other handÔºå it approaches 0„ÄÇ So it's a function between0 and1„ÄÇAlright„ÄÇ

 so but you can see you have to have very negative or very positive values„ÄÇ

 So this is yeah how this function looks like„ÄÇ

![](img/afcfdf7c4d85c778b97d544b8ea331ac_5.png)

So yeahÔºå I will explain to you how the logistic regression loss function works like in the next video here„ÄÇ

 I just wanted to summarize a few thingsÔºå and it may be that you have to think a bit about this because it can look a little bit complicated at first glance„ÄÇ

 but essentially it's not that complicated„ÄÇ So let me just use this notation here to summarize the logistic regression model where H you can think of it as the hypothesis in traditional machine learning or you can more simple terms Think of it as the model„ÄÇ

 This is our logistic regression modelÔºå which receives as input„ÄÇNet input„ÄÇ

And then you have this logistic sigmoid activation function that I showed you in the previous slide„ÄÇ

Whi returns yeah a value between 0 and 1„ÄÇ and you can actually think of this whole model here as a model that computes the posterior probability„ÄÇ

 the probability of class label YÔºå given a feature vector x„ÄÇ So y is the class label„ÄÇAnd x is the„ÄÇ

Pitchon„ÄÇhoopsÔºå like to„ÄÇSo you can think of it as this and„ÄÇNowÔºå let's say we have„ÄÇYeah„ÄÇ

 we have class table„ÄÇ1Ôºå so let'sÔºå let's look at a concrete example„ÄÇ

 Let's consider the Iis data set where we have class Na1„ÄÇS„ÄÇLet's say it's a too„ÄÇ

Now for simplified case for a simplified scenarioÔºå think of it as a binary classification problem„ÄÇ

 we have„ÄÇ1 is thetosa and 0 S„ÄÇWy columnÔºå these two classes„ÄÇNow„ÄÇ

If I want to compute the probability that a given feature vector belongs to Ctoa to class 1„ÄÇ

 I can write it as this P„ÄÇY equalsÔºå let's say one because it's shorter given x„ÄÇ

 And this is computed as„ÄÇIt's this one„ÄÇ And the probability„ÄÇOf y equals 0 is computed as x„ÄÇ

 which we can write as„ÄÇ1 minus h of x„ÄÇSo these are the feature vectors you these axis„ÄÇNo„ÄÇüòîÔºåSo„ÄÇ

Imagine the probability for the tourism„ÄÇI„ÄÇPot8Ôºå then equivalentlyÔºå the probability for„ÄÇ

Versy color would be pointÔºå2„ÄÇRightÔºå so if this is indeed true„ÄÇ

If the flower is indeed versica as a toerÔºå then this would be actually pretty good„ÄÇ

 So let's say we assume we have a true label„ÄÇLet's assume that„ÄÇTrue label„ÄÇIt'sÔºå actually„ÄÇWen„ÄÇ

Then it would be actually pretty good to get a accuracy of 80%Ôºå the higherÔºå the better though„ÄÇ

 so we want to actually have a high probability„ÄÇBut if the true label„ÄÇIs 0„ÄÇ

 then a high probability would actually be bad if the„ÄÇTrue label is0Ôºå what we want„ÄÇ

Is actually a high probability for„ÄÇPi„ÄÇüòîÔºåWhyÔºå so I can just maybe use„ÄÇDifferent colour„ÄÇ what we want„ÄÇ

Here is we want to have a high probability for this case„ÄÇ then if the true label is indeed0„ÄÇ

 then we want to have a 80%Ôºå maybe for this case„ÄÇ So that is what I'm summarizing here at the bottom of this slide„ÄÇ

 So if we have a binary„ÄÇClassification problem with possible class labels 0 and1„ÄÇ

 What we want is these probabilities to be close to one for the respective class„ÄÇ

 So if the class labelÔºå the true class label is 0Ôºå then we want the probability for y equals 0„ÄÇ

To be close to one„ÄÇIf the true label is oneÔºå then we want the probability for this scenario to be close to one„ÄÇ

So in this caseÔºå it's yeahÔºå it's depending which probability we want to be maximized depends on the true label here„ÄÇ

 So we will look at that howÔºå how we can do that with„ÄÇLoss function in the next video„ÄÇ

 but this is like the basic setup„ÄÇ So we want a high probability„ÄÇGivenÔºå yeahÔºå the certain class„ÄÇ

 So vice versaÔºå we can also think of it like this if we have„ÄÇAss a tools are label„ÄÇ

We want this one to be largeÔºå which gives us the scenario for class1„ÄÇ So if we have a sattoa„ÄÇ

Case make this one notch„ÄÇMake this one„ÄÇSmall„ÄÇHowever„ÄÇ

 let me go erase that because we don't have that much space„ÄÇ if we have a true labelÔºå worthyy colour„ÄÇ

ThenÔºå make this one small„ÄÇAnd make this one„ÄÇLarch„ÄÇSo this is essentially what we want to achieve„ÄÇ

 and I will show you how we can do that in the next video„ÄÇ



![](img/afcfdf7c4d85c778b97d544b8ea331ac_7.png)