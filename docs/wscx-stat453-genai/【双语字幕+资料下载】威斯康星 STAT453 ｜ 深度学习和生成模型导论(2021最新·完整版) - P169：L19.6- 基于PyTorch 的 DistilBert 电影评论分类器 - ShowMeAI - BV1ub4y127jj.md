# P169ï¼šL19.6- åŸºäºŽPyTorch çš„ DistilBert ç”µå½±è¯„è®ºåˆ†ç±»å™¨ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ so this was a very long lecture on sequence modeling with recurrent neural networks and transformersã€‚

 So we talked about R ends with attentionï¼Œ then we talked about selfatten and multihead attentionã€‚

 which are concepts found in the original transformer model and then we talked about some popular transform models such as Bã€‚

 GT and barRTã€‚ So in this last video I wanted to show you a code implementation of BRTã€‚

 I know that yeahï¼Œ this lecture has been about generating sequences soã€‚Howeverã€‚

 as we also have learnedï¼Œ the birdRT model is more or better for discriminative modeling like prediction tasksã€‚

 whereas GPT would be a better model for generating textã€‚So Bertã€‚

 because I just find this model also interestingï¼Œ I wanted to show you a simple example using our familiar movie review classifier data setã€‚

And then we can compare to the performance of the LSDM model that we trained in Ncture 15ã€‚

But of courseï¼Œ please feel free to experiment more with usã€‚

 There is essentially a company who is developing an open source library that everyone uses when working with Transers the company is called Huging phase andã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_1.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_2.png)

Yeahï¼Œ they have a lot of tutorialsã€‚ So I was just focusing on classificationã€‚

 but they have many otherã€‚Tuttorials availableã€‚ So if you are interestedã€‚

 so you can find also sequence generation tasks and so farï¼Œ like question answering and yeahã€‚

 different different types of thingsï¼Œ language modelingã€‚ So please feel free also to explore moreã€‚

 I wanted to just because it was a long video or lecture alreadyã€‚

 I wanted to just finish this with a shorter example showing you how to use this resourceã€‚

 the transformer library from hugging faceã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_4.png)

So here we are going to train a bird modelã€‚ And I was actually implementing bird models on this movie review data set or fine tuning itã€‚

Essentialï¼Œ I couldn't get really good performance with thatã€‚ So I changed that toã€‚Using distill birdã€‚

 So distill bird is a smallerï¼Œ faster and computationally cheaper version of birdtã€‚

 So they essentially took Btï¼Œ the original bird model and distilled it down to a smaller sizeã€‚

 So it has 40% less periã€‚ So it can make this a little bit larger hereã€‚ So it hasã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_6.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_7.png)

40% less parametersã€‚And run 60% faster while achieving approximately 95% of the original performance of bird measured on a language understanding taskã€‚

So againï¼Œ I just wanted to show you in general how we can import these models from the Transformer libraryã€‚

 which we can install here via Pip install Transers so in for this particular example I was using the version 1 4ã€‚

6ã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_9.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_10.png)

All rightï¼Œ so and I also structured this whole notebook in a similar way that I structured previouslyã€‚

 for instanceï¼Œ the LSDM notebook that we had for movie review classificationã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_12.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_13.png)

So I'm importing a couple of things hereï¼Œ there's a tokenizer and a classifier modelã€‚

 so this is just for preprocessing the text and then this is the main for loading the main distill bird modelã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_15.png)

The tokenizer hereï¼Œ when I understood correctly based on the documentationã€‚

 actually the same tokenizer that they use for the regular bird modelï¼Œ they call it distill birdã€‚

 but you could technically also just use bird tokenizerã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_17.png)

Just a few settings hereã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_19.png)

I'm only training for three epochs because it'sï¼Œ it's a slowã€‚ I it's a big modelï¼Œ rightã€‚

 It's it was it 300 million parametersã€‚ So it takes a while to trainã€‚

 So that's why I only trained for three epochs because yeahï¼Œ I didn't want to wait foreverã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_21.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_22.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_23.png)

Yeahã€‚So actually 3 million parametersmeterï¼Œ I thinkï¼Œ is the original bird modelã€‚ So that is 40%ã€‚

 that it's more around 17hã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_25.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_26.png)

So first I'm loading the dataï¼Œ so remember I had like a preprocessed version of that dataset that we used also in our LSTM code before just for convenience I'm using that it's a CV fileã€‚

 so here this is just downloading and then unziiping the CV file from my book directory where I have this preprocessed versionã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_28.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_29.png)

And heres how it looks like the reviews as text and then the sentiment here as yeahã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_31.png)

That's the class tableï¼Œ whether it's positive or negativeã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_33.png)

We have 50ï¼Œ000 movie reviews and I'm splitting this into a training validation and test setã€‚

 so I use the first 35ï¼Œ000 for training and I use 5000 for validation and the remaining 10ã€‚

000 for testingã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_35.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_36.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_37.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_38.png)

Then I'm loading the tokenizer from the pretrained model hereï¼Œ So why using a pretrained tokenizerã€‚

 I think this includes the generation of the word embeddingsã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_40.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_41.png)

So the vocabulary and everythingã€‚Nowã€‚This is very convenientï¼Œ so we can just apply thatã€‚

 So now it would then process those other wordsã€‚I reviewss the same way it processed the once when it was doing the unlabeled trainingã€‚

 So just to recapï¼Œ let me go into the lecture slidesã€‚

 So there are two steps I should have maybe said that earlierã€‚

 We are in this notebook only focusing on the pre trainingã€‚ So if I go back to the main conceptsã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_43.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_44.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_45.png)

So yeah there are two main approachesï¼Œ the two training approaches for the transformers so first there's this pretraining on the large unlabeled dataset and then there's the training for the downstream tasks on a smaller labeled dataset So in our case we are loading the pre-trained one so the people at hugging phase prepared this pretrained model which we are loading and then in our particular case we are just doing the training for the downstream tasks on this small labeled movie review dataset and we are using the fine tuning approach where we update the whole bird modelã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_47.png)

Okayï¼Œ so here we are creating then ourã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_49.png)

Encoingsã€‚Socodings for the textï¼Œ I think this includes also the word embeddingsã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_51.png)

So the context size is 512ã€‚ So that's also one thing to keep in mindã€‚

 what it will do is it will truncate soã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_53.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_54.png)

If we have a movie review that has fewer than 512 wordsã€‚

 it will do a padding if it's longer than 512 wordsï¼Œ it will truncateã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_56.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_57.png)

So then everything will be the same sizeï¼Œ512ã€‚ Then we set up our data set hereã€‚

So this is just a Pwach data set from yeahï¼Œ from the main Pytch libraryã€‚

 So it's something kind of familiarã€‚ So now we work with encodings and labelsã€‚

 and yes a specific way to pre processã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_59.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_60.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_61.png)

Or arrange the encodingsã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_63.png)

So it's essentially from the keyï¼Œ like the I Iï¼Œ the word ID to the encodingã€‚Sorryï¼Œ yeahã€‚No sorryã€‚

 this includes the labelsï¼Œ rightï¼Œ rightï¼Œ okayã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_65.png)

Yeahï¼Œ and thenã€‚We construct our data set from the encodings and the labelsã€‚

 So this gets our encodings and the labelsã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_67.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_68.png)

Let's see the encodings in the system labelsã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_70.png)

Labels are just the zeros and onesï¼Œ then I'm setting up my data loadus Not I'm only using batch size of 16 because the bird model is quite large and I got memory problems on my GPU when I was increasing thatã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_72.png)

And now we are loading the pretrain model hereï¼Œ so we are using the distill bird for sequence classificationã€‚

From yeah pre from pretrainedã€‚ So we are using an uncased modelã€‚ So it's case insensitiveã€‚That wasã€‚

 yeahï¼Œ something I triedã€‚ It's just a simple modelã€‚ I think they also have caseï¼Œ sensitive onesã€‚

But since we have a small data setï¼Œ that might be the better option here using the atom optimizerã€‚

And then we are doing the trainingã€‚ Okayï¼Œ so training is very unspectacularã€‚ So this isï¼Œ by the wayã€‚

 this's just the accuracy functionã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_74.png)

Have carried this over from our previous code examplesã€‚

 I made a few modifications just about this part hereã€‚ This is for loading the dataï¼Œ butã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_76.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_77.png)

Except that it' this is from our LTM notebook earlierã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_79.png)

Where we also should have a accuracy function hereã€‚Just took it from here from our previous codeã€‚

 modified it a little bit for our encoder transformerã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_81.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_82.png)

And yeahã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_84.png)

That is then computing the accuracy in the same wayã€‚Nowã€‚

 not also that the outputs when we run the forward path of our modelã€‚ so the bird modelã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_86.png)

Gets the input Is and then the attention maskã€‚Andã€‚The outputs areã€‚Two thinks it's a tupleã€‚

 one is the loss and one are the losï¼Œ so it's computing already the loss inside so we don't have to worry aboutã€‚

 let's say using cross entropy loss during training alsoã€‚

And then we get the predicted labels by looking them at the maximumã€‚Rodget hereã€‚

 So this is similar to a whileã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_88.png)

A LDM classifierã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_90.png)

Or any other classifier we trained beforeã€‚Okayã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_92.png)

So here we are training againï¼Œ so this is for preparing the input data setï¼Œ so getting the input IDsã€‚

 the attention maskï¼Œ and then the labels and these all go together as input to the BRT modelã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_94.png)

So if it's the input Iï¼Œ sorry I said earlier that the tokenizer includes the word embedding that's not trueã€‚

 this is then happening in the model itself if it's just in getting the ID so the conversion to the embeddings must have happened in the word model itself ifã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_96.png)

Go back to our slidesã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_98.png)

So I don't have a specific bird slide hereï¼Œ I thinkã€‚ but if we look at the general transformer hereã€‚

 So these input embeddings are probably happening inside the bird model together with positional encodingã€‚

 So in birdã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_100.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_101.png)

This is stupidTã€‚In B this look like thisï¼Œ if you recallï¼Œ So these embeddingsã€‚

Probably happen in inside the modelã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_103.png)

The Yerã€‚Okayã€‚Because this is just the Isã€‚Yeahï¼Œ then the backward passã€‚

 we set the gradients from the previous round to 0ï¼Œ perform the backward pass on the lossã€‚Againã€‚

 the loss is returned by the bird modelï¼Œ so the modelï¼Œ when we call it with a forward passã€‚

 will return a tupleï¼Œ which consists of loss and logicitsã€‚

And we use the loss for the backward pass hereï¼Œ we don't really use the logicsã€‚

 We only use the logicits later here for computing the predicted labels in our accuracy functionã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_105.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_106.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_107.png)

Yeahï¼Œ and then it's trainingã€‚ this is just our boiler plate for keeping track of the trainingã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_109.png)

And yeahï¼Œ it's take some timeã€‚ It's 20 minutes per epochã€‚ It's muchï¼Œ much slower than our LSTMã€‚

 but it's also muchï¼Œ much larger than our LSDMã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_111.png)

In terms of the number of parametersï¼Œ and in the endï¼Œ it gets pretty good performanceã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_113.png)

So three epochs took about one hourï¼Œ gets 99% training accuracy and about 92% test accuracyã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_115.png)

Now let's compare that to our LSTM model that we trained before in lectureture 15ï¼Œ soã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_117.png)

That wasã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_119.png)

The with the pe sequencesï¼Œ where we had a more efficient way of peing sequencesã€‚

 So when I compare it with this modelï¼Œ we got around 89% accuracyã€‚

 So let me scroll down to the bottomã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_121.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_122.png)

So it trains much fasterã€‚ You can see it's only a fewã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_124.png)

Half a minuteï¼Œ approximately less than half a minuteã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_126.png)

For one epochã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_128.png)

Andã€‚ðŸ˜”ï¼Œ15 epochs was just in five minutes or four minutesï¼Œ it's impressiveã€‚ So 89% accuracyã€‚

 it's actually not too badã€‚ Soï¼Œ but yeah using our pretrained bird modelã€‚

 we get even better performance out of thisï¼Œ which is also interestingã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_130.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_131.png)

So yeahï¼Œ in that caseï¼Œ that's how the bird model worksã€‚

 It is how we can use a pretrain model and fine unitã€‚

 So the key idea is essentially using the tokenizerã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_133.png)

To get the encodingsã€‚The so called encodingã€‚ And then we can useã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_135.png)

This is also getting all attention masksï¼Œ and then we can use the pretrain model hereã€‚

 distill bird for sequence classificationã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_137.png)

And just train this further for our target data setã€‚ So I did some more experimentsï¼Œ soã€‚

To further improve the performanceï¼Œ I looked into what they recommended in the tutorialã€‚

 so I made a few changesã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_139.png)

So one of the changes was usingã€‚Adom Wï¼Œ which is an atom that supports decoupled weight decayã€‚

 that's2 regularizationï¼Œ soã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_141.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_142.png)

Let meã€‚ So here's from the paperã€‚ So there's a paper corresponding to that Adam with weight decay hereã€‚

 if you're interestedã€‚ And this is just from the screenshot explaining how they perform the at weight decayã€‚

 but that's not get cited tracked hereã€‚ So the difference is I'm now using Adam with weight decay hereã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_144.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_145.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_146.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_147.png)

Before I just used the regular itemã€‚And alsoï¼Œ what I'm doing is I'm using a running rate scheduler with a linear running rate scheduleã€‚

And this is something I got from the hugging face repositoryã€‚ So let meï¼Œ yeahã€‚ so you can see thatã€‚

 So I got that from the transformerã€‚Libraryï¼Œ that's also the something they recommended with these settingsã€‚

And when I train thatï¼Œ then with these two changes using Adam W and theã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_149.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_150.png)

Linear scheduleï¼Œ which I update after each iterationã€‚I got even better performanceï¼Œ I got 93%ã€‚3ã€‚

 it's about 1% better test accuracyã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_152.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_153.png)

Another thing I found really cool is what they also have in the hug face repository is they haveã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_155.png)

A so calledã€‚Trainer classã€‚ So this is essentially the same as beforeã€‚ So I'm usingã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_157.png)

Againï¼Œ distill birdtã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_159.png)

For classification with usï¼Œ they also have an Wã€‚ I use just the oneï¼Œ I think in Pytorchã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_161.png)

Maybe their is even betterï¼Œ Im not exactly sure what the differences areã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_163.png)

Between their Adam W and the one and Pytorchã€‚ But my guess is either they implemented it before it was implemented in Pytorch or they have like a small change to it that makes it maybe more specific or better for transformers in any caseã€‚

 So this is all the same setupï¼Œ also the same tokenizerã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_165.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_166.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_167.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_168.png)

But now the difference is after loading the modelã€‚ So maybe I should have left it up hereã€‚Sorryã€‚ðŸ˜”ã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_170.png)

Because I'm not using the one from Pythtorchï¼Œ I'm using the Arrsen now So before I use the one in Pythrchã€‚

 I'm using the Ars snowï¼Œ it doesn't really matterã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_172.png)

And yeah here I'm using this trainer class from the repositoryã€‚ So this is actually very convenientã€‚

 They pack a lot of functionality in thereã€‚ they have a per device training batch sizeã€‚

 So when I was running this on my machineï¼Œ which had multiple GPUusã€‚

 it was utilizing the different GPUus for efficiency and you can also specify directly the scheduler and everythingã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_174.png)

So these are the arguments for the trainerã€‚ And then you can specify the trainer with these argumentsã€‚

 You can provide a training set and a meditation setã€‚

 So you can just use that one instead of having ourã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_176.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_177.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_178.png)

å—¯ã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_180.png)

Yopsã€‚I'm not sure why it swap swaps hereã€‚ I want to show both at the same timeã€‚

 somehow it doesn't like it right nowã€‚ But instead of defining our training loop here ourselvesã€‚

 we could also useã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_182.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_183.png)

Use their trainer hereï¼Œ and then it trains for a couple of timesã€‚

 gives you some output for the different stepsï¼Œ or different losses hereã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_185.png)

You can see the loss goes downã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_187.png)

Andï¼Œ yeahã€‚It's my accuracy and evaluation and you get even slightly better performance than with the one I trained with my own schedulerã€‚

Okayï¼Œ so this was just a very quick video on using distillber from Haging faceã€‚

 They have way more models hereã€‚ You can see there's a huge range of modelsã€‚

 They also have G models hereã€‚ if you are interested in the more generative modeling partsã€‚

 This is usually a model that is better for sequence generationã€‚



![](img/154b688314fa0550cc46ed42fc16e4d9_189.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_190.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_191.png)

But yeahï¼Œ there's lotsï¼Œ lots of stuff hereã€‚ if you're interested in language modelsã€‚ So with thatã€‚

 let me end this lectureï¼Œ which was already probably a little bit too longã€‚ but yeahã€‚

 I hope that was usefulã€‚

![](img/154b688314fa0550cc46ed42fc16e4d9_193.png)