# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P169ÔºöL19.6- Âü∫‰∫éPyTorch ÁöÑ DistilBert ÁîµÂΩ±ËØÑËÆ∫ÂàÜÁ±ªÂô® - ShowMeAI - BV1ub4y127jj

YeahÔºå so this was a very long lecture on sequence modeling with recurrent neural networks and transformers„ÄÇ

 So we talked about R ends with attentionÔºå then we talked about selfatten and multihead attention„ÄÇ

 which are concepts found in the original transformer model and then we talked about some popular transform models such as B„ÄÇ

 GT and barRT„ÄÇ So in this last video I wanted to show you a code implementation of BRT„ÄÇ

 I know that yeahÔºå this lecture has been about generating sequences so„ÄÇHowever„ÄÇ

 as we also have learnedÔºå the birdRT model is more or better for discriminative modeling like prediction tasks„ÄÇ

 whereas GPT would be a better model for generating text„ÄÇSo Bert„ÄÇ

 because I just find this model also interestingÔºå I wanted to show you a simple example using our familiar movie review classifier data set„ÄÇ

And then we can compare to the performance of the LSDM model that we trained in Ncture 15„ÄÇ

But of courseÔºå please feel free to experiment more with us„ÄÇ

 There is essentially a company who is developing an open source library that everyone uses when working with Transers the company is called Huging phase and„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_1.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_2.png)

YeahÔºå they have a lot of tutorials„ÄÇ So I was just focusing on classification„ÄÇ

 but they have many other„ÄÇTuttorials available„ÄÇ So if you are interested„ÄÇ

 so you can find also sequence generation tasks and so farÔºå like question answering and yeah„ÄÇ

 different different types of thingsÔºå language modeling„ÄÇ So please feel free also to explore more„ÄÇ

 I wanted to just because it was a long video or lecture already„ÄÇ

 I wanted to just finish this with a shorter example showing you how to use this resource„ÄÇ

 the transformer library from hugging face„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_4.png)

So here we are going to train a bird model„ÄÇ And I was actually implementing bird models on this movie review data set or fine tuning it„ÄÇ

EssentialÔºå I couldn't get really good performance with that„ÄÇ So I changed that to„ÄÇUsing distill bird„ÄÇ

 So distill bird is a smallerÔºå faster and computationally cheaper version of birdt„ÄÇ

 So they essentially took BtÔºå the original bird model and distilled it down to a smaller size„ÄÇ

 So it has 40% less peri„ÄÇ So it can make this a little bit larger here„ÄÇ So it has„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_6.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_7.png)

40% less parameters„ÄÇAnd run 60% faster while achieving approximately 95% of the original performance of bird measured on a language understanding task„ÄÇ

So againÔºå I just wanted to show you in general how we can import these models from the Transformer library„ÄÇ

 which we can install here via Pip install Transers so in for this particular example I was using the version 1 4„ÄÇ

6„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_9.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_10.png)

All rightÔºå so and I also structured this whole notebook in a similar way that I structured previously„ÄÇ

 for instanceÔºå the LSDM notebook that we had for movie review classification„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_12.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_13.png)

So I'm importing a couple of things hereÔºå there's a tokenizer and a classifier model„ÄÇ

 so this is just for preprocessing the text and then this is the main for loading the main distill bird model„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_15.png)

The tokenizer hereÔºå when I understood correctly based on the documentation„ÄÇ

 actually the same tokenizer that they use for the regular bird modelÔºå they call it distill bird„ÄÇ

 but you could technically also just use bird tokenizer„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_17.png)

Just a few settings here„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_19.png)

I'm only training for three epochs because it'sÔºå it's a slow„ÄÇ I it's a big modelÔºå right„ÄÇ

 It's it was it 300 million parameters„ÄÇ So it takes a while to train„ÄÇ

 So that's why I only trained for three epochs because yeahÔºå I didn't want to wait forever„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_21.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_22.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_23.png)

Yeah„ÄÇSo actually 3 million parametersmeterÔºå I thinkÔºå is the original bird model„ÄÇ So that is 40%„ÄÇ

 that it's more around 17h„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_25.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_26.png)

So first I'm loading the dataÔºå so remember I had like a preprocessed version of that dataset that we used also in our LSTM code before just for convenience I'm using that it's a CV file„ÄÇ

 so here this is just downloading and then unziiping the CV file from my book directory where I have this preprocessed version„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_28.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_29.png)

And heres how it looks like the reviews as text and then the sentiment here as yeah„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_31.png)

That's the class tableÔºå whether it's positive or negative„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_33.png)

We have 50Ôºå000 movie reviews and I'm splitting this into a training validation and test set„ÄÇ

 so I use the first 35Ôºå000 for training and I use 5000 for validation and the remaining 10„ÄÇ

000 for testing„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_35.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_36.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_37.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_38.png)

Then I'm loading the tokenizer from the pretrained model hereÔºå So why using a pretrained tokenizer„ÄÇ

 I think this includes the generation of the word embeddings„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_40.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_41.png)

So the vocabulary and everything„ÄÇNow„ÄÇThis is very convenientÔºå so we can just apply that„ÄÇ

 So now it would then process those other words„ÄÇI reviewss the same way it processed the once when it was doing the unlabeled training„ÄÇ

 So just to recapÔºå let me go into the lecture slides„ÄÇ

 So there are two steps I should have maybe said that earlier„ÄÇ

 We are in this notebook only focusing on the pre training„ÄÇ So if I go back to the main concepts„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_43.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_44.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_45.png)

So yeah there are two main approachesÔºå the two training approaches for the transformers so first there's this pretraining on the large unlabeled dataset and then there's the training for the downstream tasks on a smaller labeled dataset So in our case we are loading the pre-trained one so the people at hugging phase prepared this pretrained model which we are loading and then in our particular case we are just doing the training for the downstream tasks on this small labeled movie review dataset and we are using the fine tuning approach where we update the whole bird model„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_47.png)

OkayÔºå so here we are creating then our„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_49.png)

Encoings„ÄÇSocodings for the textÔºå I think this includes also the word embeddings„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_51.png)

So the context size is 512„ÄÇ So that's also one thing to keep in mind„ÄÇ

 what it will do is it will truncate so„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_53.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_54.png)

If we have a movie review that has fewer than 512 words„ÄÇ

 it will do a padding if it's longer than 512 wordsÔºå it will truncate„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_56.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_57.png)

So then everything will be the same sizeÔºå512„ÄÇ Then we set up our data set here„ÄÇ

So this is just a Pwach data set from yeahÔºå from the main Pytch library„ÄÇ

 So it's something kind of familiar„ÄÇ So now we work with encodings and labels„ÄÇ

 and yes a specific way to pre process„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_59.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_60.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_61.png)

Or arrange the encodings„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_63.png)

So it's essentially from the keyÔºå like the I IÔºå the word ID to the encoding„ÄÇSorryÔºå yeah„ÄÇNo sorry„ÄÇ

 this includes the labelsÔºå rightÔºå rightÔºå okay„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_65.png)

YeahÔºå and then„ÄÇWe construct our data set from the encodings and the labels„ÄÇ

 So this gets our encodings and the labels„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_67.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_68.png)

Let's see the encodings in the system labels„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_70.png)

Labels are just the zeros and onesÔºå then I'm setting up my data loadus Not I'm only using batch size of 16 because the bird model is quite large and I got memory problems on my GPU when I was increasing that„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_72.png)

And now we are loading the pretrain model hereÔºå so we are using the distill bird for sequence classification„ÄÇ

From yeah pre from pretrained„ÄÇ So we are using an uncased model„ÄÇ So it's case insensitive„ÄÇThat was„ÄÇ

 yeahÔºå something I tried„ÄÇ It's just a simple model„ÄÇ I think they also have caseÔºå sensitive ones„ÄÇ

But since we have a small data setÔºå that might be the better option here using the atom optimizer„ÄÇ

And then we are doing the training„ÄÇ OkayÔºå so training is very unspectacular„ÄÇ So this isÔºå by the way„ÄÇ

 this's just the accuracy function„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_74.png)

Have carried this over from our previous code examples„ÄÇ

 I made a few modifications just about this part here„ÄÇ This is for loading the dataÔºå but„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_76.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_77.png)

Except that it' this is from our LTM notebook earlier„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_79.png)

Where we also should have a accuracy function here„ÄÇJust took it from here from our previous code„ÄÇ

 modified it a little bit for our encoder transformer„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_81.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_82.png)

And yeah„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_84.png)

That is then computing the accuracy in the same way„ÄÇNow„ÄÇ

 not also that the outputs when we run the forward path of our model„ÄÇ so the bird model„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_86.png)

Gets the input Is and then the attention mask„ÄÇAnd„ÄÇThe outputs are„ÄÇTwo thinks it's a tuple„ÄÇ

 one is the loss and one are the losÔºå so it's computing already the loss inside so we don't have to worry about„ÄÇ

 let's say using cross entropy loss during training also„ÄÇ

And then we get the predicted labels by looking them at the maximum„ÄÇRodget here„ÄÇ

 So this is similar to a while„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_88.png)

A LDM classifier„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_90.png)

Or any other classifier we trained before„ÄÇOkay„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_92.png)

So here we are training againÔºå so this is for preparing the input data setÔºå so getting the input IDs„ÄÇ

 the attention maskÔºå and then the labels and these all go together as input to the BRT model„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_94.png)

So if it's the input IÔºå sorry I said earlier that the tokenizer includes the word embedding that's not true„ÄÇ

 this is then happening in the model itself if it's just in getting the ID so the conversion to the embeddings must have happened in the word model itself if„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_96.png)

Go back to our slides„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_98.png)

So I don't have a specific bird slide hereÔºå I think„ÄÇ but if we look at the general transformer here„ÄÇ

 So these input embeddings are probably happening inside the bird model together with positional encoding„ÄÇ

 So in bird„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_100.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_101.png)

This is stupidT„ÄÇIn B this look like thisÔºå if you recallÔºå So these embeddings„ÄÇ

Probably happen in inside the model„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_103.png)

The Yer„ÄÇOkay„ÄÇBecause this is just the Is„ÄÇYeahÔºå then the backward pass„ÄÇ

 we set the gradients from the previous round to 0Ôºå perform the backward pass on the loss„ÄÇAgain„ÄÇ

 the loss is returned by the bird modelÔºå so the modelÔºå when we call it with a forward pass„ÄÇ

 will return a tupleÔºå which consists of loss and logicits„ÄÇ

And we use the loss for the backward pass hereÔºå we don't really use the logics„ÄÇ

 We only use the logicits later here for computing the predicted labels in our accuracy function„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_105.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_106.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_107.png)

YeahÔºå and then it's training„ÄÇ this is just our boiler plate for keeping track of the training„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_109.png)

And yeahÔºå it's take some time„ÄÇ It's 20 minutes per epoch„ÄÇ It's muchÔºå much slower than our LSTM„ÄÇ

 but it's also muchÔºå much larger than our LSDM„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_111.png)

In terms of the number of parametersÔºå and in the endÔºå it gets pretty good performance„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_113.png)

So three epochs took about one hourÔºå gets 99% training accuracy and about 92% test accuracy„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_115.png)

Now let's compare that to our LSTM model that we trained before in lectureture 15Ôºå so„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_117.png)

That was„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_119.png)

The with the pe sequencesÔºå where we had a more efficient way of peing sequences„ÄÇ

 So when I compare it with this modelÔºå we got around 89% accuracy„ÄÇ

 So let me scroll down to the bottom„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_121.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_122.png)

So it trains much faster„ÄÇ You can see it's only a few„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_124.png)

Half a minuteÔºå approximately less than half a minute„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_126.png)

For one epoch„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_128.png)

And„ÄÇüòîÔºå15 epochs was just in five minutes or four minutesÔºå it's impressive„ÄÇ So 89% accuracy„ÄÇ

 it's actually not too bad„ÄÇ SoÔºå but yeah using our pretrained bird model„ÄÇ

 we get even better performance out of thisÔºå which is also interesting„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_130.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_131.png)

So yeahÔºå in that caseÔºå that's how the bird model works„ÄÇ

 It is how we can use a pretrain model and fine unit„ÄÇ

 So the key idea is essentially using the tokenizer„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_133.png)

To get the encodings„ÄÇThe so called encoding„ÄÇ And then we can use„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_135.png)

This is also getting all attention masksÔºå and then we can use the pretrain model here„ÄÇ

 distill bird for sequence classification„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_137.png)

And just train this further for our target data set„ÄÇ So I did some more experimentsÔºå so„ÄÇ

To further improve the performanceÔºå I looked into what they recommended in the tutorial„ÄÇ

 so I made a few changes„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_139.png)

So one of the changes was using„ÄÇAdom WÔºå which is an atom that supports decoupled weight decay„ÄÇ

 that's2 regularizationÔºå so„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_141.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_142.png)

Let me„ÄÇ So here's from the paper„ÄÇ So there's a paper corresponding to that Adam with weight decay here„ÄÇ

 if you're interested„ÄÇ And this is just from the screenshot explaining how they perform the at weight decay„ÄÇ

 but that's not get cited tracked here„ÄÇ So the difference is I'm now using Adam with weight decay here„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_144.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_145.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_146.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_147.png)

Before I just used the regular item„ÄÇAnd alsoÔºå what I'm doing is I'm using a running rate scheduler with a linear running rate schedule„ÄÇ

And this is something I got from the hugging face repository„ÄÇ So let meÔºå yeah„ÄÇ so you can see that„ÄÇ

 So I got that from the transformer„ÄÇLibraryÔºå that's also the something they recommended with these settings„ÄÇ

And when I train thatÔºå then with these two changes using Adam W and the„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_149.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_150.png)

Linear scheduleÔºå which I update after each iteration„ÄÇI got even better performanceÔºå I got 93%„ÄÇ3„ÄÇ

 it's about 1% better test accuracy„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_152.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_153.png)

Another thing I found really cool is what they also have in the hug face repository is they have„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_155.png)

A so called„ÄÇTrainer class„ÄÇ So this is essentially the same as before„ÄÇ So I'm using„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_157.png)

AgainÔºå distill birdt„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_159.png)

For classification with usÔºå they also have an W„ÄÇ I use just the oneÔºå I think in Pytorch„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_161.png)

Maybe their is even betterÔºå Im not exactly sure what the differences are„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_163.png)

Between their Adam W and the one and Pytorch„ÄÇ But my guess is either they implemented it before it was implemented in Pytorch or they have like a small change to it that makes it maybe more specific or better for transformers in any case„ÄÇ

 So this is all the same setupÔºå also the same tokenizer„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_165.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_166.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_167.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_168.png)

But now the difference is after loading the model„ÄÇ So maybe I should have left it up here„ÄÇSorry„ÄÇüòî„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_170.png)

Because I'm not using the one from PythtorchÔºå I'm using the Arrsen now So before I use the one in Pythrch„ÄÇ

 I'm using the Ars snowÔºå it doesn't really matter„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_172.png)

And yeah here I'm using this trainer class from the repository„ÄÇ So this is actually very convenient„ÄÇ

 They pack a lot of functionality in there„ÄÇ they have a per device training batch size„ÄÇ

 So when I was running this on my machineÔºå which had multiple GPUus„ÄÇ

 it was utilizing the different GPUus for efficiency and you can also specify directly the scheduler and everything„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_174.png)

So these are the arguments for the trainer„ÄÇ And then you can specify the trainer with these arguments„ÄÇ

 You can provide a training set and a meditation set„ÄÇ

 So you can just use that one instead of having our„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_176.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_177.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_178.png)

ÂóØ„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_180.png)

Yops„ÄÇI'm not sure why it swap swaps here„ÄÇ I want to show both at the same time„ÄÇ

 somehow it doesn't like it right now„ÄÇ But instead of defining our training loop here ourselves„ÄÇ

 we could also use„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_182.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_183.png)

Use their trainer hereÔºå and then it trains for a couple of times„ÄÇ

 gives you some output for the different stepsÔºå or different losses here„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_185.png)

You can see the loss goes down„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_187.png)

AndÔºå yeah„ÄÇIt's my accuracy and evaluation and you get even slightly better performance than with the one I trained with my own scheduler„ÄÇ

OkayÔºå so this was just a very quick video on using distillber from Haging face„ÄÇ

 They have way more models here„ÄÇ You can see there's a huge range of models„ÄÇ

 They also have G models here„ÄÇ if you are interested in the more generative modeling parts„ÄÇ

 This is usually a model that is better for sequence generation„ÄÇ



![](img/154b688314fa0550cc46ed42fc16e4d9_189.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_190.png)

![](img/154b688314fa0550cc46ed42fc16e4d9_191.png)

But yeahÔºå there's lotsÔºå lots of stuff here„ÄÇ if you're interested in language models„ÄÇ So with that„ÄÇ

 let me end this lectureÔºå which was already probably a little bit too long„ÄÇ but yeah„ÄÇ

 I hope that was useful„ÄÇ

![](img/154b688314fa0550cc46ed42fc16e4d9_193.png)