# P116ï¼šL14.3.1.2- PyTorch ä¸­çš„ VGG16 - ShowMeAI - BV1ub4y127jj

![](img/9c0c719111bb824ca05ef070abb9e2b7_0.png)

Alrightï¼Œ let's now take a look at the code example of VGG16 that I implemented here I should mention that I don't want to rerun this year from scratch during yeah this video because it took one and a half hours to train it's a large network so in that way we will just take a look at the results so I will of course yeah share this with you should find the link another videoã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_2.png)

Yeahï¼Œ so what we have here is all the usual importsï¼Œ watermarkï¼Œ torch torch vision and so forthã€‚

Then my helper files as usualï¼Œ they are I think identical to what I used to last weekã€‚

 I can't remember making a modification to themï¼Œ so exactly the same like that we used for AlexNeã€‚

 the only difference here is really that I have a different architectureã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_4.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_5.png)

So defining our hyperparile settingsï¼Œ random seatd shouldn't be a hyperparile but you have to set it to some things I'm setting it to one to3 batch size 256 and we are training for 50 epochs and I was using a GPU for that because otherwise it took too long so you could for instance run this on Google Colllap on the GPUã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_7.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_8.png)

If you run things on the GPUã€‚This function doesn't really work that well anymore because I actually cut this from the videos because it was too longã€‚

 but just to briefly mention why so when you train things on the GPU there are different types of algorithms used for convolutionsã€‚

 So there'sï¼Œ of courseï¼Œ the convolution that we talked about in the lectureã€‚

 but in code people don't implement it this wayï¼Œ they are usually more efficient approximations of that like a fast fourier transform based ones and depending on what computer you use and what graphics card you useã€‚

Different approximation algorithms are usedï¼Œ although these approximations are pretty goodã€‚

 they are approximating the convolution veryï¼Œ very wellã€‚

 there are tiny differences after the decimal pointã€‚ And if you have a lot of tiny differencesã€‚

 they can add upã€‚ and then sometimes you willã€‚Find that results are different when you run them againã€‚

 because not only are different algorithms automatically chosenã€‚Based on the computerã€‚

 But also when you run it multiple timesã€‚ So NviDdia is actually running some automatic way to guess which algorithm might be a good choice at a given timeã€‚

 So that is why there are sometimes slight differencesã€‚

 It used to that you could set this to a deterministic settingã€‚ but for some reasonã€‚

 it complains now that this deterministic setting doesn't really work anymoreã€‚ But any caseã€‚

 you don't have to worry about thatã€‚ It's very normal in deep learning that you get different results if you run things againã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_10.png)

I meanï¼Œ they will be slightly differentã€‚ They will not be very differentã€‚ It's just like veryã€‚

 very small differencesã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_12.png)

Any caseï¼Œ let's not worry about this okay soã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_14.png)

Talking about the data nowã€‚ So here we' are working with a Cy 10 datasetã€‚Which is 32 by 32ã€‚ But yeahã€‚

 Iã€‚Resize thisï¼Œ because V Gï¼Œ if youã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_16.png)

Look at this figure againã€‚ it was originally developed for 224 times 234 inputsã€‚

 and there are just too many layers for small inputs because you have the size of the layersã€‚

 and at that that pointï¼Œ if you have half the input sizeã€‚

 you will already have only3 by three or4 by4 layers hereã€‚ And if you make it even smallerã€‚

 it would be one by oneã€‚Even smaller if you have a here or e one by one and then try to half itã€‚

 So that wayï¼Œ we can't have inputs that are too smallã€‚ It actually works with 32 by 32ã€‚

 but the results were not as goodã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_18.png)

So it was fasterï¼Œ of courseï¼Œ butã€‚With this version as slightly upscaleï¼Œ I get actually85% accuracyã€‚

So what I'm doing here is I'm upscaling these images from 3 32 to 7070 then I do a slight random crop here so the random crop is to avoid overfitting or not avoid overfitting but to reduce the overfitting to make it a little bit less sensible or sensitive to exact pixel locations and then I'm converting it to a tensor and then normalizing it so that they will the pixels will be sent at zero across the channels and have a standard deviation of oneã€‚

So that's the usual procedureã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_20.png)

Againï¼Œ for testingï¼Œ I don't do any random crop I am centre croppingã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_22.png)

Thenã€‚The rest is exactly the same that we talked about for Alexnet last weekã€‚

 So here's the interesting partã€‚ That's the VG G 16 architectureã€‚ Yeahã€‚

 I just had some notes for myself for calculating the paddingï¼Œ butã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_24.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_25.png)

We probably don't need that hereã€‚How I implemented it in different blocksã€‚

 So I have one block that is usually the convolutions and then stopped or the final layer in that block is the max pooling layer that reduces the size by halfã€‚

 So theseã€‚Preserve the sizeã€‚ It's the same convol with the padding such that the input equals the output size and max pudding reduces the sizeã€‚

 And then againï¼Œ hereï¼Œ this consserves the sizeã€‚ This reduces the size by half because of the strideã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_27.png)

Then hereã€‚Againï¼Œ we haveï¼Œ yeahï¼Œ we haveã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_29.png)

Another blockã€‚With max poolingã€‚Another blockã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_31.png)

Max pullingingã€‚And another blockã€‚And max poolingã€‚ And then these are what I call my feature extractor layersã€‚

 And then we have a classifier layerã€‚ This is really if I go back to the figureã€‚

 this is really this last part here in light blueã€‚ This is the fully connected partã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_33.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_34.png)

So here a linear layerï¼Œ a fully connected layerï¼Œ essentially another one and another oneã€‚

 So you have dropout I should probably yeah this is one deep dropout versionã€‚

 So if you want to add dropout for other layersï¼Œ you have to I would recommend using dropout 2Dã€‚

Actuallyï¼Œ I recallï¼Œ I forgot about that recently and I used a regular dropout somewhere because I for some reason forgot to type 2 D and I was wondering why I couldn't notice any differenceã€‚

Alrightï¼Œ yeah here I'm using the gaminging uniform weightsã€‚

 Is skip here because that's what was used in the original paperã€‚

 Then I have an adaptive average poolingï¼Œ which will in this caseã€‚

 make things equal to the height and width here so that I choseã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_36.png)

To be 3 by3ï¼Œ soã€‚Because for the linear layerï¼Œ you have to know the inputï¼Œ height and widthã€‚

Because you need to know the number of featuresã€‚ We know always know the number of channelsï¼Œ rightã€‚

 because it's coming from hereï¼Œ but sometimes it's hard to know what's the width and height to compute the number of featuresã€‚

So usually what we do is we have here the feature map and then average pullingã€‚

 And then what we do is weã€‚Flattenï¼Œ this is essentiallyã€‚Flattening operationã€‚

 This is essentially the same that you do when you work with multilay perceptrons and Mnessã€‚

 So you basically flatten the inputï¼Œ which is a imageï¼Œ you flatten this to be a long vectorã€‚

 That's the the same operation hereã€‚ And for thatï¼Œ you have to know the number of parameterss because you need to know the number of weights right for the multilay perceptron layerã€‚

 the linear layerã€‚So how would you get this information hereï¼Œ I'm using adaptive average poolingã€‚

So adaptive average pudding is an operation where you can determine the input and output size and it will either add the strides or the padding such that this dimension will be metã€‚

 So if the input is so where am I here if the input to this one is smaller than three by3ã€‚

 it will add padding if it's larger than three by3ã€‚

 it will not add padding and do some strides to reach that size that we desireã€‚å—¯ã€‚Yeahï¼Œ andã€‚

If you don't want to do adaptive average pooling and you want to know the number ofã€‚

Sizes that come out of the last blockï¼Œ for exampleã€‚

 one way would of course to use the equation that I showed you and just calculated by handã€‚

That is a valid approach that could be a potential exam questionï¼Œ but we don't have an exam anymoreã€‚

 so don't worry about it maybe something for the quizï¼Œ but in practiceã€‚

 yeah people would write forward or backward hooks I think I explained it so you would use a forward hook here I explained it to you when we talked about the Pytoch API but to be honestã€‚

Even that is something most people won't do because it's just too much work to write a hook functionã€‚

 I meanï¼Œ I wouldn't do itã€‚ The simpleï¼Œ brute false way would be to just implement a print statement temporarilyã€‚

 So what you would do is you would just have something that's lazy wayï¼Œ the lazy but efficient wayã€‚

 something like x does the sizeï¼Œ print the size here of xã€‚Then you would run thisã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_38.png)

Hereï¼Œ when you do the trainingï¼Œ it will output the sizeã€‚

 It will probably crash if you don't have the right dimensionsã€‚ But then you know the sizeï¼Œ rightã€‚

 you knowï¼Œ the height and width thenï¼Œ and then you canã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_40.png)

Remove itã€‚At the desired height and width hereã€‚ And you are good to goã€‚ So in practiceï¼Œ most peopleã€‚

 to be honestï¼Œ are just insertingã€‚ if you want to know the sizeï¼Œ just import theã€‚Print statementã€‚

 And that is how you find out the brute force wayã€‚ Okayã€‚

 so this is essentially the energy G architectureã€‚ so you can seeã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_42.png)

They are essentially conversion layersã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_44.png)

Followed always by max pooling that halfalves the input sizeã€‚ We have many of thoseã€‚

 then these classification layers hereã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_46.png)

å—¯ã€‚Miaoã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_48.png)

And thenã€‚We have here the trainingã€‚ So I' am initializing it for 10 classesã€‚

 If we look back to the slidesï¼Œ it's 1000 because imagenetï¼Œ the datasetã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_50.png)

That they used here had 1000 classesã€‚ We have Cypher 10 with 10 classes onlyã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_52.png)

I use SGD with momentumï¼Œ and there's our learning rate scheduler that reduces the learning hell rate by a factor of 10ã€‚

 Sore dividing it by 10 ifã€‚The validation accuracy doesn't improveã€‚

 So this is just my training script that I used for Alex 92ã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_54.png)

Al rightï¼Œ so here is then the training the same as with Alexnetã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_56.png)

Can say see it trains pretty quicklyï¼Œ35%ï¼Œ50%ï¼Œ53% accuracyã€‚ So it's slowly climbing upã€‚ at some pointã€‚

 it stopsã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_58.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_59.png)

Around hereï¼Œ it's improving a little bitï¼Œ but it takes longerï¼Œ more epochsã€‚ So I do thatã€‚ So it'sã€‚

 sometimesï¼Œ you knowï¼Œ it's very useful to look at thisã€‚ If you run an expensive network like Widge Gã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_61.png)

It might be taking a few hoursã€‚ and you probably want to take a look at the beginningã€‚

 whether it's even worthwhile training it for 50 epochsï¼Œ rightã€‚

 So if you notice that the loss doesn't go downï¼Œ maybe in the first four epochs or something like thatã€‚

 or you notice the accuracy doesn't improveã€‚ Then I would just stop the training and maybe change some parameters before you just waste one and a half hours waiting until it's finishesã€‚

 So that is why I'm printing this during the trainingã€‚ There's a tool calledã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_63.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_64.png)

Tensor boardã€‚That can create visualizations during trainingã€‚

 We are not talking about this in this class because I think we already have enough tools for you to learn aboutã€‚

 So there's already enough code going onã€‚ But you're very welcome to check this out at some pointã€‚

 It's also yeahï¼Œ nice for visualizing thingsã€‚ But yeahï¼Œ hereï¼Œ I'm trying to keep things simpleã€‚

 So not too many tools at onceã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_66.png)

Sorry enough you have to learn in a wayã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_68.png)

But at the endï¼Œ I always find it still helpful to take a look at visualizations hereã€‚

 I have make them of courseï¼Œ with Meprolip so that you can see them during training only after training but yeah what you can see is the loss goes down pretty nicely so it kind of converges here after 50 epochs maybe it would still approve a little bit but looking at the validation performance here so in orange you can see maybe maybe it would go up slightly more but you can already see after epoch1 there' a huge amount of overfitting so to reduce that overfitting maybe adding some dropout 2D might helpã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_70.png)

You can actually this may be a good exerciseã€‚ you can actually insert a regular dropout and you will see with a regular dropod that you won't reduce overfitting too muchã€‚

 but if you use dropod 2 D that helps actually more with convolutional networksã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_72.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_73.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_74.png)

Okayï¼Œ so this is that and then here just the visualization looking at some examples you can see most of that looks correct so P is predicted T is on the true labelã€‚

 you can see this one is wrong de and frorogï¼Œ it's hard to tellã€‚

 I mean if you look at this Cypher 10 has such a low resolution that even we have difficulties telling what's in these images I thinkã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_76.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_77.png)

Alrightï¼Œ so here's a confusion matrixã€‚ So that looks actually interestingã€‚

 You can see dog and cat are often misclassified hereã€‚

 and that is kind of reasonable because cats and dogs are both animalsã€‚ So a cat in a dockerã€‚

 for exampleï¼Œ very different from an airplaneã€‚ So dog and airplane is rarely confused whereas dog and cat is more often confused in the grand scheme of things that's kind of reasonableã€‚

 I would sayï¼Œ so you can actually see all the animalsã€‚ That's very interestingã€‚

 So you can see all the animals here in this square are oftenã€‚ðŸ˜Šï¼ŒYeahã€‚

 misclassified compared to ships and trucks and like automobilesã€‚ So here you have thingsã€‚

 and then you have the animalsã€‚ animals among each other are harder to classify compared to other objects like airplanes and automobilesã€‚

 Alrightï¼Œ so this is VG G16ã€‚ maybe one more thingã€‚ I have VG G16 standardizedã€‚

 was just toing around with itã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_79.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_80.png)

To see if proper standardization can improve thingsï¼Œ but it didn't turn out to be trueã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_82.png)

Just to show you theï¼Œ I would sayï¼Œ proper wayã€‚ So it's scrolling up againã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_84.png)

Here I'm just using 0ã€‚5ã€‚5ã€‚5ï¼Œ which will scale the pixels such they are between minus1 and1 center at 0ã€‚

So using 0ã€‚5ã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_86.png)

Butã€‚We can actually use the proper standard deviation and feature pixel mean by computing themã€‚

 So here I just have a functionã€‚Added to it that computes or approximatesã€‚

 actually because it's fasterã€‚ that approximates the mean and standard deviation for each channel for each color channelsã€‚

 That's a redï¼Œ green and blue1ã€‚ So they are around ã€‚5ã€‚

And then here the standard deviations there are 0ã€‚25 instead of 0ã€‚

5 in what I assumed in the previousã€‚Coodeï¼Œ but if I use theseã€‚Proper onesã€‚Instead of just using 0ã€‚5ã€‚

 I find that it doesn't really make any difference in allã€‚ So performanceï¼Œ I thinkã€‚

 was pretty much the sameã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_88.png)

![](img/9c0c719111bb824ca05ef070abb9e2b7_89.png)

Actuallyï¼Œ worse82ã€‚ Ohï¼Œ actuallyï¼Œ it's an interesting drop hereã€‚ It's probably due to overfittingã€‚

 more overfittingã€‚

![](img/9c0c719111bb824ca05ef070abb9e2b7_91.png)

Yeahï¼Œ soï¼Œ but you can see in this caseï¼Œ I didn't gain anything from doing this other standardizationã€‚

 maybe it's actually training betterï¼Œ though you can see it's dropping more hereã€‚

 could be because the schedule is triggered at that pointã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_93.png)

It doesnn't have to be because of choosing the mean and side deviation could be just coincidenceã€‚

All rightã€‚ So there was a long video probablyã€‚ So let me wrap this up in the next videoã€‚

 we will talk about residual networksï¼Œ which are a little bit more interesting than just adding more layers to itã€‚



![](img/9c0c719111bb824ca05ef070abb9e2b7_95.png)