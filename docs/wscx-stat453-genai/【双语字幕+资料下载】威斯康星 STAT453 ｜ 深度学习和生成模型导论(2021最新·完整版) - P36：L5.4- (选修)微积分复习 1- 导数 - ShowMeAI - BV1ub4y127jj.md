# P36ÔºöL5.4- (ÈÄâ‰øÆ)ÂæÆÁßØÂàÜÂ§ç‰π† 1- ÂØºÊï∞ - ShowMeAI - BV1ub4y127jj

YesÔºå in the next two videosÔºå I want to give you a brief calculus refresher if you need it„ÄÇ

 So that's optional„ÄÇ You don't have to watch the next two videos„ÄÇ

 If you are very familiar with calculusÔºå it will beÔºå I thinkÔºå too trivial for you„ÄÇ But if you don't„ÄÇ

 if you„ÄÇYesÔºå so in the next two videos I want to give you a brief calculus refresher talking about derivatives and gradients„ÄÇ

 and like I said these videos are optionalÔºå So if you are very familiar with calculus„ÄÇ

 then I think this might be too easy for you„ÄÇ but for those who are yeah you want to refresh their calculus„ÄÇ

 this is like a brief summary of the main parts that we will need for the gradient descent algorithm„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_1.png)

YeahÔºå so here is an overview of what a derivative is„ÄÇ So a derivative of a function„ÄÇ

 you can think of it as the rate of change of that function or the so-called slope„ÄÇ

 So you have a very simple exampleÔºå a function F which is equal to 2 x„ÄÇ

 So if we have an input let's pick an arbitrary number here on the x axiss„ÄÇ

 So if I have the number3 here„ÄÇSo„ÄÇThe input is 3„ÄÇThen the output would be2 times x or2 times a in this case„ÄÇ

2 times 3Ôºå that would be 6Ôºå so the output would be 6„ÄÇNow let's pick a second point„ÄÇ

Yeah move on the X X by Delta a units„ÄÇ So then we arrive at„ÄÇThis point here„ÄÇ

 So if I look at these valuesÔºå let's move by four points and we arrive by at 7 here„ÄÇ

 And then the output corresponding to that would be two times the output would be„ÄÇ14„ÄÇSo yeah„ÄÇ

 there is between these two points„ÄÇ there is a change„ÄÇHereÔºå going onÔºå the change is„ÄÇFrom 6 to 14„ÄÇ

 So this is by„ÄÇ8 units„ÄÇAnd the change on the x axis is from 3 to 7Ôºå it's only„ÄÇF units„ÄÇ

 so we can then compute the rate of change or the slope by having this8 divided by 4 equal to 2„ÄÇ

 So the slope of this function of 2 x is2„ÄÇ So almost more formally heres the equation how we can compute the slope so we pick the function on the right hand side„ÄÇ

 So this is the input on the right hand side here„ÄÇAnd then this is the input here on the left hand side„ÄÇ

 and let me use a different color„ÄÇAnd of courseÔºå then the output here„ÄÇ

 this is corresponding to this one and the output of this one is corresponding to this one„ÄÇ

 So we are plugging in the difference of the height„ÄÇ So the 14„ÄÇYou write the stone„ÄÇ

Let's do it in black 14 minus-6„ÄÇAnd then divide by„ÄÇ

 then we have a and then this second value Dlta a minus B„ÄÇWhich would in our caseÔºå three„ÄÇPlus 4„ÄÇ-3„ÄÇ

Soom„ÄÇOkay now yeahÔºå So actuallyÔºå I can simplify that„ÄÇ So we have a„ÄÇAnd then we have the second value„ÄÇ

 Delta a minus a„ÄÇMaybe it's simpler to see it like this„ÄÇ So these two belong together„ÄÇ It's a and„ÄÇ

F of a and these two or these belong together because it's„ÄÇFor this point hereÔºå a plus delta a„ÄÇ

 which we plug in hereÔºå but we can actually simplify that right because we have a minus a so we can actually„ÄÇ

Cancel that„ÄÇ And then we have this Sla form hereÔºå which is essentially then„ÄÇÂóØ„ÄÇ14 minus-6 divided by„ÄÇ

4Ôºå which is„ÄÇ8 divided by 4„ÄÇ So this is like more the formal way of how we compute the slope„ÄÇ

 And in this caseÔºå the slope of this function is 2„ÄÇ

 It tells us by if we change the function by one unitÔºå So the input of the function„ÄÇ

 if we change it by one unitÔºå the output changes by two units because the other slope is2„ÄÇ

 So it gives us an idea of how fast the function grows here with respect to the inputs„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_3.png)

YeahÔºå so on the previous slideÔºå I showed you a veryÔºå very big leap on the X axiss„ÄÇ

 Usually when we think of function derivativesÔºå we think of a very small change in the input„ÄÇ

 So we think about what what happens if deelta x„ÄÇ So the change on the input scale goes to 0 is like infinitely small„ÄÇ

 So here on the right hand side„ÄÇThis is againÔºå the slope from the previous slide„ÄÇ

 where I just used the letter X now instead of a„ÄÇ And here these are two different ways of yeah denoting that we are computing a derivative„ÄÇ

 So if we have a functionÔºå F of xÔºå then F prime of x would be the derivative of that function„ÄÇ

 So when I recall from my calculus class a long time agoÔºå I think this was called Larangerange„ÄÇ

Notation„ÄÇWhereas this one arm is the leibs„ÄÇNottationÔºå you don't have to memorize these names„ÄÇ

 but yeahÔºå there there are just two different ways of denoting the same thing„ÄÇ

Theip notation will be a little bit more useful later in the context of deep learning when we talk about yeah partial derivatives of functions and also yeah gradients„ÄÇ

In any caseÔºå let's take a look at an example deriving a function or computing the derivative more formally„ÄÇ

 so let's consider again our function 2 x here„ÄÇSo we are nowÔºå yeahÔºå starting with a simple„ÄÇ

 just a formal notation from above„ÄÇ Now let's plug in„ÄÇ let's plug in this functionÔºå so„ÄÇ

If we have x and deelta xÔºå the output would be„ÄÇWhat would be the output should be2 x„ÄÇ

 So two times x plus deelta xÔºå right„ÄÇSo this one„ÄÇ And yeahÔºå hereÔºå I seeÔºå I just expanded it already„ÄÇ

 So I'm just expanding this to„ÄÇÂóØ„ÄÇ2 x plus 2 Delta x„ÄÇ So that's what I have here„ÄÇ

 And then this one isÔºå yeahÔºå2 xÔºå rightÔºü So that's where this one comes from„ÄÇ

So what I've done here is really just plugging in theÔºå the function into the equation here„ÄÇ

 Now I can see„ÄÇI have a 2 x here and a 2 x here„ÄÇ And thenÔºå I meanÔºå this is minus-2 x actually„ÄÇ

 so I can cancel those„ÄÇ So this one„ÄÇAnd this one cancels„ÄÇ

 So what I have is two delta x divided by Delta x„ÄÇ So this can also be canceled„ÄÇ

 And then what remains is the two„ÄÇ And that is how we derive or compute the the root of this function„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_5.png)

YeahÔºå let's do this again„ÄÇ using a slightly more complicated function„ÄÇ

 Let's take a look at x squared„ÄÇ So how can we derive x squared using the same concept as before„ÄÇ

So exactly the same notation now plugging in x squared instead of to x„ÄÇ

So what we get here is for the first part is for F of x plus delta x„ÄÇ

This is if we consider x squared„ÄÇ this would be then x plus theta x squared„ÄÇ Then if I expand this„ÄÇ

 this would be 2 xÔºå sorry„ÄÇ3X squared plus 2„ÄÇXÔºå delta X„ÄÇPlusÔºå on deelta X square„ÄÇ

So this is what I've written down here„ÄÇSo if I have now this formÔºå I can simplify it because yeah„ÄÇ

 if we pay attention theres again x squared minus x squaredÔºå so we can cancel those„ÄÇ

 so what remains is 2 x delta x plus delta x squared„ÄÇSoÔºå now„ÄÇWhat we do is we looking at this„ÄÇ

 we can also further simplify itÔºå rightÔºüSo we can divideÔºå actuallyÔºå by„ÄÇDelta x„ÄÇ

 So if I divide by Delta xÔºå this one goes away„ÄÇThis one goes away and this one goes away so what„ÄÇ

What statess is 2 x plus deelta x and also considering„ÄÇ

That we are interested in a very small chain of x„ÄÇWhen deelta x goes to 0„ÄÇ

 so you can think of it as 0„ÄÇ So this kind of also goes away„ÄÇ So the derivative is basically2 x„ÄÇ

 So deriving x squaredÔºå the derivative would be 2 x„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_7.png)

YesÔºå so conceptually what we were doing in the previous slide when we computed the derivative of x squared is we approximated the slope by a second between two points„ÄÇ

So just to draw what that how that looks like„ÄÇ So if we consider this function f of x x squared„ÄÇ

 So we have this yeah parabola here„ÄÇ so now let's consider a concrete point here„ÄÇ

 Let's consider this one here„ÄÇ and the slope of this of the function at this point would be this tangent line here„ÄÇ

 this red line„ÄÇ how we approximate it is it is by taking on two points„ÄÇ

 So we took the original point„ÄÇ and then we added„ÄÇDelta x to itÔºå right plus Delta x to x„ÄÇPlus„ÄÇ

 delta x of we„ÄÇAddit Delta x to xÔºå let's say„ÄÇThis is Delta X„ÄÇ So we are here on the right hand side„ÄÇ

 Now we have the second point„ÄÇ So this is f of x plus delta x„ÄÇAnd then there's the secondant line„ÄÇ

 the line„ÄÇYeahÔºå between these two points„ÄÇ And we used that line to approximate the tangent„ÄÇ

 And you can see ifÔºå yeahÔºå the difference is relatively large between the two pointsÔºå right„ÄÇ

 between if basicallyÔºå if theta x is very largeÔºå then you can see this is not a very good approximation„ÄÇ

 this is why we usually take a look at„ÄÇWhen that x goes to0 at the limit so in this case what we would have is we would maybe have a smaller distance here„ÄÇ

 it's still bigÔºå but otherwise I wouldn't be able to draw it and you can see if I now connect this point and this point„ÄÇ

 this is a better approximation of the red line and if I go and make this smaller and smaller the better I will yeah approximate this tangent line here So this is basically conceptually how yeah this symbolic derivative root of computation works when we consider this small rate of change here„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_9.png)

YeahÔºå here I made a cheatee sheet for you with the most common derivatives„ÄÇ

 the most useful ones to memorize„ÄÇI don't want to go over all over all of these„ÄÇ but yeah„ÄÇ

 they are in the slides for your convenience„ÄÇ So if you need that as a cheat sheet that's in the slides then„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_11.png)

I hear more derivative rules„ÄÇ so the main onesÔºå the sum ruleÔºå the difference rule„ÄÇProduct rule„ÄÇ

 quotient ruleÔºå reciprocurro rule„ÄÇ These are actuallyÔºå you don't have to memorize these„ÄÇ

 You can actually derive them from the product rule„ÄÇ For exampleÔºå if you consider the quotient rule„ÄÇ

You can actually rewrite that as„ÄÇF of x„ÄÇTimes G X minus1Ôºå and then yeahÔºå work with this one„ÄÇ

So you don't need to memorize the quotient of reciprochcal rule„ÄÇ

 you can actually work with a product rule„ÄÇÂóØ„ÄÇYeahÔºå so then the chain rule„ÄÇ

 that is actually the most important one„ÄÇ So for deep learning„ÄÇ I meanÔºå all of them are important„ÄÇ

 but the chain rule is the main one that kind of drives deep learning„ÄÇ

 So you will see later when we talk about gradient descent„ÄÇ

 it is essentially just an application of the chain rule when we compute the derivative of the loss function with respect to the weights„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_13.png)

YesÔºå so here is a visual explanation of this decomposition of the nested function using the chain rule„ÄÇ

 So using colour hereÔºå it maybe be easier to see„ÄÇ So let's assume we have a function„ÄÇ

Capital F of x that consists of two nested functions„ÄÇ F and G here„ÄÇ

 So how we compute this is we first compute the inner part and then we compute the auto part„ÄÇ

 So this is just the regular function„ÄÇ So this is visualized here in this graph„ÄÇ

 So we have a computation graph here„ÄÇ So this is more like a computation graph view where we first pass x into G„ÄÇ

 then we get some output and this output of the inner part then goes into the auto part F„ÄÇ

 and then this gives us the result Z„ÄÇSo when we then compute the derivative of this functionÔºå we„ÄÇ

 yeahÔºå we use the chain rule and the chain rule is yeah two things„ÄÇ it's two parts„ÄÇ

 the auto derivative times the inader„ÄÇ So we can think of it as the„ÄÇAllter„ÄÇDivative„ÄÇAnd here the„ÄÇ

You know„ÄÇDivative„ÄÇLet me compute here and„ÄÇVisualallyÔºå alsoÔºå what happens is yeah„ÄÇ

 we can do this separately so we can„ÄÇCompute hereÔºå in this caseÔºå the inner derivative„ÄÇSo„ÄÇ

 we pass x to„ÄÇG and computer derivative of that„ÄÇ And here that's the auto derivative„ÄÇ

 So here we are drivingring F„ÄÇAnd then once we have these two resultsÔºå we can multiply them together„ÄÇ

 and that is the derivative of the whole function„ÄÇ

![](img/dacb0f27562105e9bb7affcdb79090fe_15.png)

YeahÔºå one of the nice things of Pytorch is that it will build such computation graphs automatically if we define variables and do some computation with those„ÄÇ

 So Pytch will literally keep a computation graph in the background then and there are utilities then for computing derivatives automatically„ÄÇ

 So Pytch can these derivatives automatically using a package inside which is called autograd„ÄÇ

 So automatic gradient computation we will see that in the next week„ÄÇ

 So usually it's not required that we derive things by hand when we work in Pytorch„ÄÇ

 there are automatic ways of doing that„ÄÇ but of course it's useful to know how yeah derivatives and gradients work because we have to I think it's a good idea to have an understanding of how these types of things work before you use packages that can do them automatically„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_17.png)

yeah and here I just wanted to show you in text when we write down derivatives„ÄÇ

 we will mostly be using the lipibnet notation„ÄÇ So instead of writing this nested thing here„ÄÇ

 we will use the labibnet notationÔºå which I find personally a little bit easier to read„ÄÇ

 especially when we have more complicated function with a lot of nesting going on„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_19.png)

YeahÔºå so here's just an example applying the chain rule in practice on nested function„ÄÇ

 So here we have the nested function lock of square root of x„ÄÇSo first„ÄÇ

 it's easy just to take them into two parts so„ÄÇThe auto derivative„ÄÇTimes„ÄÇüòîÔºåIn a derivative here„ÄÇSo„ÄÇ

 and then we can derive them separately„ÄÇ So now let's just focus on„ÄÇThe Alta 1 first„ÄÇ

 so the derivative of the autota function is derivative of log g is 1 over G and G is just a placeholder for square root of x„ÄÇ

 So the derivative of the alta function is1 over square root of x„ÄÇAnd thenÔºå for the inner one„ÄÇ

So we have the derivative root of square root of x„ÄÇ I'm just rewriting it here for convenience„ÄÇ

 And then we can use the power rule here„ÄÇBring the one half up frontÔºå and then a -1„ÄÇIn the„ÄÇExponent„ÄÇ

 so it's -1 half„ÄÇ And that is just rewritten as one over2 square root x„ÄÇ

 So this is the inner derivative„ÄÇ And then we just combine them because the chain rule is really just sorry the„ÄÇ

Enner times the alta„ÄÇ So here I'm just putting these two parts together„ÄÇ

 So the overall derivative is one over2 x here„ÄÇ

![](img/dacb0f27562105e9bb7affcdb79090fe_21.png)

YeahÔºå and we can also use the chain rule for arbitraryrbitrarily long function composition„ÄÇ

 So consider this function f of x hereÔºå which consists of five nested functionsÔºå1Ôºå2Ôºå3Ôºå45 So yeah„ÄÇ

 we can also expand this using the chain rule when we compute the derivative„ÄÇ

 So if we compute the derivative„ÄÇOf the function of of the auto function F here„ÄÇ

 if we also the the root of the whole thing here„ÄÇWe can decompose that into smaller partsÔºå and then„ÄÇ

Computute all of them separately using the general rule and multiply them to compute the derivative basically„ÄÇ



![](img/dacb0f27562105e9bb7affcdb79090fe_23.png)