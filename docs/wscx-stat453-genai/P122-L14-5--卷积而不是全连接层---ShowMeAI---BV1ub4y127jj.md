# P122ï¼šL14.5- å·ç§¯è€Œä¸æ˜¯å…¨è¿æ¥å±‚ - ShowMeAI - BV1ub4y127jj

All rightï¼Œ nowï¼Œ just for funï¼Œ I will show you how we can replace a fully connected layer with a convolution layerã€‚

 And because it's so much funï¼Œ there are actually two ways of doing thatã€‚

 So we will have twice the funã€‚ All rightï¼Œ let's start with the first oneï¼Œ soã€‚ğŸ˜Šã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_1.png)

On the left hand sideï¼Œ I'm showing you for reference a fully connected layer that we have seen beforeã€‚

 So in order to fit it onto the slideï¼Œ this fully connected layer has exactly four inputs and two outputsã€‚

 Soï¼Œ yeahï¼Œ you rememberï¼Œ of courseï¼Œ how we compute these rightï¼Œ soã€‚Let's start with a green oneã€‚

 So hi greenã€‚All the connections in greenã€‚ These are one way vectorã€‚ Let's call that W 1ã€‚

 And the output here isã€‚Yeahï¼Œ the green dotã€‚ And we multiply the weight vectorï¼Œ of courseã€‚

 with the inputs hereã€‚The input Xã€‚ And then we also have some bias unitã€‚ The bias unit is not shownã€‚

 but it'sã€‚Adddedï¼Œ Why notã€‚ So that would be added hereã€‚ And yeahã€‚

 this is how we compute the first outputã€‚ So let's call that maybe output 1ã€‚

 And then we have output 2ï¼Œ and we computed the same wayã€‚

 except now that we have these yellow connectionsï¼Œ such as or W 2ã€‚And actuallyã€‚

 we can do the same thing exactly the same computation using a convolutional layerã€‚

 How does that workï¼Œ Soï¼Œ yeahï¼Œ we would arrange our for inputs as an imageï¼Œ let's say as aã€‚

2 by two image hereã€‚Rightï¼Œ and then we can have aã€‚let's focus on the green oneã€‚ nowã€‚

 we can have a kernelã€‚That has oneã€‚Input channelã€‚Counnel with a one input channelï¼Œ and2 by 2ã€‚

Colonnel size and widthã€‚ So the kernel is2 by2 with one input channelã€‚Andã€‚Here this would be O W 1ã€‚

 The first kernelï¼Œ let's say the green one hereã€‚Allrightï¼Œ so this is one kernelã€‚

And the second kernel could be this W2 hereï¼Œ which wasï¼Œ againã€‚

 a kernel with one input size or one input channelã€‚ sorryï¼Œ and a2 by 2 kernel sizeã€‚

 And if you recall how we computeã€‚The convolution forï¼Œ let's sayã€‚

The receptive field here doest a wellã€‚Receptiveã€‚Fieldï¼Œ and this is hereã€‚In greenã€‚

 this is our Oh sorryï¼Œ This is our outputï¼Œ of courseï¼Œ here in greenã€‚This is our kernelã€‚

 So this one is essentiallyï¼Œ you can think of this operationï¼Œ essentially asã€‚

Similar to a dot productï¼Œ essentiallyï¼Œ if youï¼Œ you multiply thoseï¼Œ rightï¼Œ it's the weightedã€‚

 weighted inputã€‚ and then you compute the sumã€‚ãã†ã§ã™ã€‚Some over theseã€‚ And then you add the biasã€‚

 And it's exactly the same computation as this oneã€‚

 And I will show you in the next slide with a code implementation that it gives you exactly the same resultsã€‚

 So in the grand ski grandã€‚Big picture scheme hereã€‚

 This would be a case where we would have a two kernels with one input channel and a kernel size of2 by2ã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_3.png)

Alrightï¼Œ so it's maybe more complicated than it seemsã€‚

 so let's just take a look at a code example to maybe for those who prefer code examples to have another look at thisã€‚

So here againï¼Œ this is our fully connected on setupï¼Œ soã€‚I'm defining here my inputsã€‚

 I'm just using some made up numbersï¼Œ1ï¼Œ2ï¼Œ3ï¼Œ4ã€‚And I'm shaping them here such that they are shaped in this N Cã€‚

 HW format that we usually use for convolution networksã€‚ So we have oneã€‚Bch size of oneã€‚

One input channelï¼Œ height is 2ï¼Œ width is 2ã€‚Then I'm defining my fully connected layer hereã€‚Forã€‚

Inputsã€‚2ã€‚ğŸ˜”ï¼ŒOutputsã€‚Here are my weightsã€‚So on the previous slideï¼Œ this would be OW1ã€‚Hereã€‚

 so I can actually use the same colorsï¼Œ maybeã€‚So let meï¼Œcause this was greenã€‚

 Let me use the exact same colours Soã€‚Green1ã€‚And this isã€‚Your orangerange oneã€‚And the biasã€‚

B2 is the yellow oneã€‚And hereï¼Œ the green one that's theã€‚1ã€‚Andã€‚

Now I'm just assigning them to the weight and bias in the fully connected layer here and the linear layer Why am I doing that hereã€‚

 That is because when I call this function hereï¼Œ it will use random weights and I want to compare this to a convolution networkã€‚

 So in that way I'm using fixed weights and the fixed bias unit just for yeahã€‚

 just for showing you that I can do the same thing with a conversion network so that the numbers are not a random hereã€‚

Yeahï¼Œ then I am calling here the fully connected layer on my inputsã€‚

 I'm reshaping this to a long vector of courseï¼Œ so I'm making so this is a2 by2 right 2 by 2 I'm just reshaping it such that is a long vector here with four units and then you can see the results of 14ã€‚

9 and 19ã€‚

![](img/1d35b1664d39b2ece6898b81db03493f_5.png)

Now I can do the same thing here with my convolutional setupã€‚Soã†ã€‚

The kernel size I'm going to use is2 by 2 because it's2 by2 here andã€‚

I am just getting rid of the first dimension hereã€‚ This is all I'm doingï¼Œ because in the inputs hereã€‚

 I have it on the left hand sideã€‚ For referenceï¼Œ I'm just getting rid of these dimensionsã€‚

 So the first squeezeï¼Œ squeeze dimension 0 will get rid of the first oneã€‚ So first dimensionã€‚

This one will get rid of the second dimension so that I have met  two by twoã€‚ I meanã€‚

 these dimensions were emptyï¼Œ rightï¼Œ These were essentially just theseã€‚Square bracketsã€‚

 I essentially was just getting rid of these square bracketsã€‚å—¯ã€‚Yeahã€‚

 and then I'm initializing my convolutional layer hereï¼Œ one input channelï¼Œ two output channelsã€‚

 So I have essentially two kernelsã€‚And the kernel sizes2 by2ã€‚And hereã€‚Just printing out the sizesã€‚

And hereã€‚I'm assigning the weights that I had on the previous nightã€‚Soã€‚

I'm assigning these weights hereã€‚To my convolutional weightsã€‚Why am I doing thatã€‚

 It's not cheating or something I'm just trying to use the same weightsã€‚

 Because when I initialize thatï¼Œ I willï¼Œ of courseï¼Œ get random weightsã€‚

 And I don't want to do this experiment with random weights because then it's hard to compareï¼Œ rightã€‚

 So we have to use the same weightsã€‚ So here I'm just using the same weights and the same biasã€‚

 And then here I'm calling my convolutionã€‚ And you can seeã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_7.png)

And get exactly the same results as with my fully connected layerï¼Œ exceptï¼Œ of courseã€‚

 the difference is that they are differently arranged because here we haveã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_9.png)

Channels with a fully connected layerï¼Œ we don't have channelsã€‚

 inputs and output channels so that dimension might look differentã€‚

 but the results are exactly the sameã€‚ So this is one way you can replace a fully connected layer with a convolutional layerã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_11.png)

Another wayï¼Œ this is the second way would be to take theseã€‚These fully connectedã€‚ sorryã€‚

 this vector to here and stack itã€‚ So we stack it with four channelsã€‚And then you can useã€‚

4 kernel sorryï¼Œ two kernels with four channels eachã€‚ So you haveã€‚Nowã€‚

 still two kernels because we will have two output maps or yeah two output feature mapsã€‚

 where can now useã€‚Two kernelsã€‚Withã€‚Forã€‚Channelsã€‚To implement the same as we have implemented hereã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_13.png)

Now going back hereï¼Œ I had two kernelsã€‚ğŸ˜”ï¼ŒWithã€‚ğŸ˜”ï¼Œ1 channelã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_15.png)

Eachã€‚Now I have to con with four channel eachã€‚ The difference really is only that I have them stacked hereã€‚

 whereas here I have them arranged in this two by two fashionã€‚

 Both ways would result in equivalent results compared to the fully connected layerã€‚

 If you don't believe meã€‚ here's another exampleã€‚ So here I'm now having my con layer with four input channels to output channelsã€‚

 So this is essentially the fourã€‚

![](img/1d35b1664d39b2ece6898b81db03493f_17.png)

![](img/1d35b1664d39b2ece6898b81db03493f_18.png)

Channels to kernels set upã€‚Using my same weight so I can compare them and we will again get exactly the same resultsã€‚

 So these are two different ways we can replace a fully connected layer with conversion layersã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_20.png)

So just revisiting ourï¼Œ all convolution network where we had this adaptive poolingã€‚ So we can nowã€‚

 for instanceï¼Œ instead of using adaptive poolingã€‚Could of courseï¼Œ use a fully connected layerã€‚

 but we could also use our new idea of just using a convolutional layer here so to make this all convolutional network even more convolutionalã€‚

 So here now I will set the kernel size to 8 by 8 because we found out in the previous code notebook that here at this stage the feature maps will be 8 by 8 64 by 8 by8ã€‚

 So 64 the channels 8 by8 is theã€‚height and widthã€‚ So that's why I'm setting it to8ã€‚

 And this if you run thisï¼Œ it will give you exactlyï¼Œ well noã€‚

 it will not give you exactly the same results as the adaptive pulling because now we have more parametersã€‚

 but it will run it whiteï¼Œ even give you better resultsã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_22.png)

And here here's last thing for this video here is a comparisonã€‚

 a direct comparison between the fully connected implementation of a network and the fully the fully convolutional version of thatã€‚

 So on the left hand sideã€‚I have a convolution to D setup with 64 input channels and 64 output channelsã€‚

And then I'm flattening itã€‚ So if these are 64ã€‚Times 8 times 8ã€‚

Here I'm flattening this so that this gets combined into theã€‚Number ofã€‚Featuresã€‚

That's what I have hereã€‚Zã€‚Number of featuresã€‚ And then my output is the number of classesã€‚

 So this is my last layerã€‚That that's how we would usually do itã€‚ But if you haveï¼Œ for some reasonã€‚

 and aversion against fully connected layersï¼Œ you could achieve the same thing using convolution layersã€‚

 by the wayï¼Œ I alsoã€‚Tried that a few years agoï¼Œ just for fun with more serious networksã€‚

 There's usually no no benefitã€‚ I thought back in the dayã€‚

 it might be easier for the implementation in Kuda to achieve better performance with thatã€‚

 because you could probably leverage better parallelism with multiple GPU courseã€‚ But in practiceã€‚

 I found almost zero differenceã€‚ whether we use fully connected or convolutional layers in the last layerã€‚

 So this is here reallyï¼Œ what I'm showing you is really just more like a toy experiment orã€‚

An example that we have the ability to do that to achieve the equivalent resultsã€‚

 but it's not necessaryã€‚ so you don't have to worry about that or you don't have to do thatã€‚

 I'm just like illustrating this hereã€‚If you wanted toï¼Œ I meanï¼Œ it's more likeï¼Œ I think itã€‚

 it could be technically be helpful to understand how convolutions workã€‚ But againã€‚

 you can even think of this video as entirely optionalã€‚Soã€‚Here on the right hand side nowã€‚

 I'm showing you the equivalent implementation whereã€‚

I am setting my con layer to the 64 input channelsã€‚It's like before andã€‚The output is the 64 classesã€‚

 So hereï¼Œ this is reallyã€‚The number of classes in the drawing hereã€‚We only had two classesã€‚

 but let's say in Cypher 10ï¼Œ we would have 10 classesã€‚Alrightï¼Œ soã€‚That's what I setting it toã€‚

 And this is then hereï¼Œ the 8 by 8ï¼Œ because we hadã€‚64 by 8 by 8 in the previous layerã€‚

 or were assuming thatã€‚ And for this to workï¼Œ this has to have the size ofã€‚

The feature map of the previous layerï¼Œ because we want to have a oneã€‚1 by one output hereã€‚

And you can run this and it will workã€‚And it should give you equivalent results to the left hand side implementationã€‚

 If you try that in practice and you find that results might be slightly differentã€‚

 that might have something to do with a random weight initializationã€‚

 because every time you call a layerï¼Œ it will create random weightsã€‚

 So this makes it hard to compareã€‚ But in theoryï¼Œ it should give you the same resultsï¼Œ because as weã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_24.png)

Seen beforeã€‚

![](img/1d35b1664d39b2ece6898b81db03493f_26.png)

Here it's exactlyï¼Œ yeah producing the same resultsã€‚ Okayï¼Œ so these were just someã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_28.png)

Fun videos now are not funï¼Œ but I would say thought experiments how we could technicallyã€‚

Get rid of non convolution layersã€‚ If we have an aversion against fully connected or pooling layersã€‚

 And in the next videoï¼Œ I willã€‚Go back to a more important topicã€‚

 transfer learning which I think will be very useful for your class projects because it helps us leveraging related data setsã€‚



![](img/1d35b1664d39b2ece6898b81db03493f_30.png)