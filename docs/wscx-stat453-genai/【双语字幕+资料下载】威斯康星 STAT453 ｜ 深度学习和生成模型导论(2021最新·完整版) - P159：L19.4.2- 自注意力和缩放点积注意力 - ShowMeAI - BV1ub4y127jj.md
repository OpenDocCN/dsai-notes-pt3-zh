# P159ÔºöL19.4.2- Ëá™Ê≥®ÊÑèÂäõÂíåÁº©ÊîæÁÇπÁßØÊ≥®ÊÑèÂäõ - ShowMeAI - BV1ub4y127jj

So in the previous videoÔºå we talk about a basic form of self attention„ÄÇ

 Now we are going to get a little bit more sophisticated and talk about the self attention mechanism that is used in the attention is all you need paper„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_1.png)

SoÔºå yeahÔºå just to recapÔºå this is what we looked at at the in the previous video where we had„ÄÇ

The self attention mechanism defined as follows where we had this input sequence„ÄÇ

 which were embeddingsÔºå word embeddings„ÄÇ Then we computed the dot products here„ÄÇ

Between one particular input and all the other inputs„ÄÇAnd then here„ÄÇ

 we normalize those with the softmax and computed the output as the„ÄÇAttention weighted inputs„ÄÇ

 So for each input for each input AIÔºå we will get a vector„ÄÇ And if we have„ÄÇLet's say T words„ÄÇ

 We will get„ÄÇT„ÄÇüòîÔºåMres from one„ÄÇ2„ÄÇEssentialentlyÔºå a matrix„ÄÇTension matrix„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_3.png)

OkayÔºå but yeah we are now kind of extending this concept of this basic attention using the attention that is used in the paper attention is all you need„ÄÇ

 So how does that one look like„ÄÇ

![](img/95f5761979ffd1fb738f0dfee9498c14_5.png)

So first of allÔºå noticed that there was one big problem with the basic version of self-atten and the problem was that it did not involve any learnable parameters„ÄÇ

 So the previous version of self-attenÔºå the basic form was actually not very useful for learning a language model because how do we update this right So if we want to develop a language model that is for instance„ÄÇ

 supposed to translate text how do we make it better if there are no learnable parameters right so here we are now introducing three trainable weight matrices that are multiplied with the input sequence embeddings the Xs that we have before so„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_7.png)

NowÔºå instead of let me go back„ÄÇ So instead of just computing the stop product here between the input and the query„ÄÇ

We now involve weight matrices„ÄÇ In factÔºå we will use three different types of weight matrices we call the one WQ„ÄÇ

 which corresponds to the query„ÄÇ

![](img/95f5761979ffd1fb738f0dfee9498c14_9.png)

K for the key and V for the value„ÄÇ So when we now compute this matrix modification between„ÄÇ

or the word embedding XÔºå which is a vectorÔºå and this matrix W QÔºå we get the so called query„ÄÇ

For this oneÔºå we get the so called key„ÄÇ So these are vector to us„ÄÇ and the value is also vector to„ÄÇ

 This is the value between a matrix W V„ÄÇAnd the input X I„ÄÇSo what's new is now that we have„ÄÇ

Modified versions of this word embedding„ÄÇAnd these are weight matrices that can be updated with back propag„ÄÇ

 for instance„ÄÇ

![](img/95f5761979ffd1fb738f0dfee9498c14_11.png)

So here's a drawing of how this self attention mechanism„ÄÇLooks like for a particular input„ÄÇ

 So let's consider the one in the middle hereÔºå x2 wordÔºå the second word in the sentence„ÄÇ

So this currentÔºå we can consider this as the current input„ÄÇ we call this the„ÄÇCan't query„ÄÇSo I mean„ÄÇ

 queryrry is a bit ambiguous because we we also have the queue here„ÄÇ

 but let's consider this as our current input here„ÄÇ

 and we compute these three things that I showed you on the previous slide by matrix modificationification„ÄÇ

 So if I go back it's just a matrix modification to compute these three things and we do this actually for all the inputs„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_13.png)

![](img/95f5761979ffd1fb738f0dfee9498c14_14.png)

So we also do this here and here„ÄÇSo from word 12 up to the last wordÔºå the teeth word„ÄÇ

Now we are also computing the attention values here„ÄÇAs a dot product„ÄÇHowever„ÄÇ

Here we are now computing it instead of computing it between the sequences on x1 and x on let's say I„ÄÇ

Instead of computing it like this„ÄÇWe computedÔºå actuallyÔºå between„ÄÇThe query and the key„ÄÇ

 So these are just modified versions of that„ÄÇ So here we are regarding„ÄÇQ 2 as the current input„ÄÇ

 So we are using Q2 here everywhere„ÄÇ This is our query„ÄÇAnd then we use the key„ÄÇ

So maybe I should use different colors„ÄÇSo for the blue one hereÔºå we use„ÄÇBluook key„ÄÇ

For green one here for itselfÔºå we use green one„ÄÇAnd for this oneÔºå we use this oneÔºå but„ÄÇThis„ÄÇ

Query is all the same„ÄÇ The keys are differentÔºå but the queries are the same„ÄÇ

 So why the terms query key and valueÔºå ThisÔºå I thinkÔºå comes from the field of databases„ÄÇ

 So here it's not really that relevant„ÄÇ It's just„ÄÇDifferent different names for different parts of this computation here„ÄÇ

And you canÔºå yeahÔºå think of this„ÄÇ This is also known as„ÄÇ

The stock product here has multiplicative attention„ÄÇ

 there's also the other types of attention like additive attention„ÄÇ

 but here it's like a form of yeah form of multiplicative attention and essentially for each query for each here for each query the model learns which key value input it should attend to so„ÄÇ

Continuing with this model hereÔºå when we compute this„ÄÇ

 there is also a normalization via the soft mix„ÄÇAnd then these are added up to form this a2„ÄÇ

 which is„ÄÇThe context aware embedding of the input„ÄÇ So if I go back„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_16.png)

This is similar essentially to what happens here right so when we have our context aware embedding of the input X I„ÄÇ

 now we have the same thing except the computation is a little bit fancier because we involve these three matrices„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_18.png)

I will also show youÔºå I meanÔºå there is also a scaled version of that„ÄÇ but one step at a time„ÄÇ

 So here is the not scaled version„ÄÇ So here„ÄÇIn the centerÔºå we haveÔºå againÔºå a soft mix„ÄÇ

 So the soft mix of these dot products„ÄÇSo that these attention weightsÔºå these A's„ÄÇ

Some up to one just like before„ÄÇ And then what's newestÔºå we multiply this here by the value„ÄÇ

So we do that for all the T values„ÄÇ we are summing over them here„ÄÇThis will be a vector„ÄÇ

 so this will be the vector„ÄÇ4„ÄÇThe second word„ÄÇA 2 corresponding to x 2„ÄÇ

 we would do the same thing also for x 1 and X T also up to X T„ÄÇ So we would repeat this process„ÄÇ

 but we would each time swap the query then byÔºå let's say query„ÄÇ1Ôºå the first wordÔºå and then also„ÄÇ

Teethward„ÄÇAnd so forth„ÄÇ But notice usÔºå what's cool about this is we could do this all in parallel„ÄÇ

 There is no sequential nature of that„ÄÇ so we could all compute these in parallel„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_20.png)

YetÔºå to explain a little bit better what was going on in the previous slide„ÄÇ

 I made a copy of the previous slide and added some annotation about the different dimensions of the different parts in that figure„ÄÇ

 So let's walk through this from left to rightÔºå so„ÄÇHere X IÔºå X1„ÄÇ

 this is a word a word embedding vectorÔºå so you can think of it as a one times the E dimensional matrix or de dimensional vector where the E is„ÄÇ

 yeahÔºå the embedding size and the original attentions all you need paper„ÄÇ

 they used 512 as the embedding sizeÔºå but of courseÔºå this is a hyperparameter„ÄÇ

 It's something you can choose it's arbitrary in that way can have 256 or 1024 or some other number is the word embedding size as we have seen also when we worked with RNs„ÄÇ

Then we have our matrices hereÔºå our WsÔºå the query key and valueÔºå and they have to haveÔºå of course„ÄÇ

 the same dimensionÔºå I mean the same number of rows as we have columns here for the matrix multiplication„ÄÇ

So we have D E here everywhere and then the output size or the number of columns is DvÔºå Dq and D„ÄÇ

 and in the original paperÔºå they had Dq equaling D„ÄÇ I mean„ÄÇ

 of course thats also necessary for the stock product hereÔºå right you have to match the dimensions„ÄÇ

And in the original transform paperÔºå they also had D Q equal to Dv„ÄÇSo this isÔºå yeah„ÄÇ

 we will see later why that isÔºå it's because we have also yeah certain stacking going on and things like that„ÄÇ

 So that's determining our output size„ÄÇÂóØ„ÄÇOkayÔºå what elseÔºüYeahÔºå so„ÄÇThese will beÔºå of courseÔºå scalers„ÄÇ

 these top productsÔºå rightÔºå because it's a modification between two vectors„ÄÇ So our soft mix here„ÄÇ

 this will give us„ÄÇOur essentiallyÔºå our scaled attention weight that we then„ÄÇ

Multiply by this value vectorÔºå which is a one times„ÄÇDV dimensional vector„ÄÇAnd then we sum that up„ÄÇ

 So the output size of this one would be one times Dv„ÄÇSoÔºå that's„ÄÇYeah„ÄÇ

 just annotated what what these parts are„ÄÇ now you know what is the vector and what's the matrix if that wasn't clear before„ÄÇ

 but yeahÔºå this isÔºå I thinkÔºå maybe just another summary of the previous slide„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_22.png)

Here's an example„ÄÇ

![](img/95f5761979ffd1fb738f0dfee9498c14_24.png)

Of these attention visualizations„ÄÇ So againÔºå this one here is just for one„ÄÇOne wordÔºå rightÔºü

 So this is just for„ÄÇThe second word„ÄÇ So it's the attention„ÄÇBetoÔºå for the second word„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_26.png)

So the previous slideÔºå we had„ÄÇLooked at the second word as the input„ÄÇ but of course„ÄÇ

 we have also other words in the input„ÄÇ So we would repeat this whole process for every input element„ÄÇ

 So here on the left hand side„ÄÇWe consider the first word„ÄÇ and here on the right hand side„ÄÇ

 we consider the last wordÔºå whereas here we consider the second word but„ÄÇ

This might be a misleading visualization„ÄÇ It looks like there's a sequential part it„ÄÇ

 That's not true„ÄÇ All of these can be computed„ÄÇIn parallel„ÄÇ

So we don't have to wait for one to finish before we can compute the next so these are all parallel computations„ÄÇ

 which is one nice aspect about these transformer models or the self attention mechanism in the transformer model„ÄÇ

And then so we get an attention aware embedding for each of the words„ÄÇ And then these essentially„ÄÇ

 you can think of it as a matrixÔºå now an attention matrix„ÄÇWhere each row corresponds to„ÄÇ

 yeah to the em beddingÔºå attention and bedding for each word„ÄÇSo it shouldÔºå if we have a T here„ÄÇ

 this should actually be„ÄÇ

![](img/95f5761979ffd1fb738f0dfee9498c14_28.png)

T„ÄÇOkay„ÄÇYeahÔºå and here is just like more like compact notation for that„ÄÇ So now consider„ÄÇ

For the inputsÔºå we represent the inputs as a T times the E dimensional matrix„ÄÇ

 where the is are embedding size again„ÄÇ and T is the input sequence size„ÄÇ

So I'm just summarizing everything here in the center„ÄÇ So now„ÄÇ

Instead of doing these steps individually here„ÄÇCan just write this as one matrix multiplication„ÄÇSo„ÅÜ„ÄÇ

We now haveÔºå let's sayÔºå the matrix Q„ÄÇ So we instead of having Q1 hereÔºå Q1Ôºå Q1 and so forth„ÄÇ

 So these resultsÔºå we can just summarize that as„ÄÇA matrix at T times D Q dimensional matrix„ÄÇ

K times T times d and t times D B„ÄÇ and then we can compute this attention matrix which is t times t dimensional matrix„ÄÇ

 So this is oneÔºå maybe one disadvantage of the self-at approach is that this is kind of large if you have a large input sequence because yeah„ÄÇ

 it's like a pairwise in a wayÔºå a pairwise similarity score here So it's n squared what t squared the big O of that„ÄÇ

 So it's not the mostÔºå I would say memory efficient approach„ÄÇ but well„ÄÇ

At leastt we can compute things in parallelÔºå so„ÄÇWe have now kindly a kind of slightly modified version of the dot product that I showed you before„ÄÇ

 We call that a scale dot productÔºå which is because of this 1„ÄÇ I will explain that at the next slide„ÄÇ

 I'm just focusing again on this whole thing here„ÄÇ So now we have Q times„ÄÇK as a matrix modification„ÄÇ

 Then we take the soft max„ÄÇ So we have Q terms T„ÄÇQ times k is the matrix modification„ÄÇ

 And we have this scaling here„ÄÇ and we'll talk about it in the next slide set„ÄÇ

 and then we have the softm„ÄÇAnd then we multiply by the matrix of V„ÄÇ

 So this is just the more compact form of what I've shown you on the previous slides„ÄÇ

 and then we will get this T times TV dimensional attention matrix„ÄÇO„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_30.png)

So what is this scaling factor here in the denominator„ÄÇ

 So this is just to prevent these individual dot products„ÄÇ So if you do a matrix modification„ÄÇ

 you can also think of it as multiple dot products between these Q and k so„ÄÇ

To prevent these to become too large„ÄÇWe scale them because if you think of the softm function„ÄÇ

 If you have a very negative input to the softmax functionÔºå it will be 0„ÄÇ

 If you have a very large inputÔºå a very positive large inputÔºå it will be close to one in the softm„ÄÇ

So„ÅÜ„ÄÇIn order to render a very sharp distribution of values in the softm„ÄÇ

 we have this scaling factor because if you maybe just think back of the logistic syigmoid„ÄÇ

 which is essentially similar to the softm„ÄÇExcept that soft mix incorporates all the other onesÔºå but„ÄÇ

It's kind of like a s model thing„ÄÇ If you have values close to 1Ôºå close to 0„ÄÇ

 there's like this saturationÔºå rightÔºå So you want to prevent values from being too extreme„ÄÇ I mean„ÄÇ

 the same concept This isÔºå of courseÔºå not a softmÔºå but the same„ÄÇConcept applies to the softm„ÄÇ

 So by scalingÔºå we prevent the softm from having a distribution that is too sharp„ÄÇYeah„ÄÇ

 and here just to our reference is the visualization of the scaled dot product extension from the attention is all you need paper„ÄÇ

 so here again they are justmarizing summarizing that visually„ÄÇ

 the matrix modification and the scaling here„ÄÇOptional mask we will talk about the mask later when we talk about the transformer here in these steps there is no masks„ÄÇ

 so the transformer also consists of an encoder and a decoder and the decoder has a mask So here can actually ignore it and then we have the softm and then we have a matrix modification with B so this is essentially summarizing this one„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_32.png)

OkayÔºå yeah„ÄÇ okay„ÄÇ This was self attention and the scared dot product attention„ÄÇ In the next video„ÄÇ

 I will talk about the multi head attention„ÄÇAnd then we will be one step closer to the transform model„ÄÇ



![](img/95f5761979ffd1fb738f0dfee9498c14_34.png)