# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P159ï¼šL19.4.2- è‡ªæ³¨æ„åŠ›å’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› - ShowMeAI - BV1ub4y127jj

So in the previous videoï¼Œ we talk about a basic form of self attentionã€‚

 Now we are going to get a little bit more sophisticated and talk about the self attention mechanism that is used in the attention is all you need paperã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_1.png)

Soï¼Œ yeahï¼Œ just to recapï¼Œ this is what we looked at at the in the previous video where we hadã€‚

The self attention mechanism defined as follows where we had this input sequenceã€‚

 which were embeddingsï¼Œ word embeddingsã€‚ Then we computed the dot products hereã€‚

Between one particular input and all the other inputsã€‚And then hereã€‚

 we normalize those with the softmax and computed the output as theã€‚Attention weighted inputsã€‚

 So for each input for each input AIï¼Œ we will get a vectorã€‚ And if we haveã€‚Let's say T wordsã€‚

 We will getã€‚Tã€‚ğŸ˜”ï¼ŒMres from oneã€‚2ã€‚Essentialentlyï¼Œ a matrixã€‚Tension matrixã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_3.png)

Okayï¼Œ but yeah we are now kind of extending this concept of this basic attention using the attention that is used in the paper attention is all you needã€‚

 So how does that one look likeã€‚

![](img/95f5761979ffd1fb738f0dfee9498c14_5.png)

So first of allï¼Œ noticed that there was one big problem with the basic version of self-atten and the problem was that it did not involve any learnable parametersã€‚

 So the previous version of self-attenï¼Œ the basic form was actually not very useful for learning a language model because how do we update this right So if we want to develop a language model that is for instanceã€‚

 supposed to translate text how do we make it better if there are no learnable parameters right so here we are now introducing three trainable weight matrices that are multiplied with the input sequence embeddings the Xs that we have before soã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_7.png)

Nowï¼Œ instead of let me go backã€‚ So instead of just computing the stop product here between the input and the queryã€‚

We now involve weight matricesã€‚ In factï¼Œ we will use three different types of weight matrices we call the one WQã€‚

 which corresponds to the queryã€‚

![](img/95f5761979ffd1fb738f0dfee9498c14_9.png)

K for the key and V for the valueã€‚ So when we now compute this matrix modification betweenã€‚

or the word embedding Xï¼Œ which is a vectorï¼Œ and this matrix W Qï¼Œ we get the so called queryã€‚

For this oneï¼Œ we get the so called keyã€‚ So these are vector to usã€‚ and the value is also vector toã€‚

 This is the value between a matrix W Vã€‚And the input X Iã€‚So what's new is now that we haveã€‚

Modified versions of this word embeddingã€‚And these are weight matrices that can be updated with back propagã€‚

 for instanceã€‚

![](img/95f5761979ffd1fb738f0dfee9498c14_11.png)

So here's a drawing of how this self attention mechanismã€‚Looks like for a particular inputã€‚

 So let's consider the one in the middle hereï¼Œ x2 wordï¼Œ the second word in the sentenceã€‚

So this currentï¼Œ we can consider this as the current inputã€‚ we call this theã€‚Can't queryã€‚So I meanã€‚

 queryrry is a bit ambiguous because we we also have the queue hereã€‚

 but let's consider this as our current input hereã€‚

 and we compute these three things that I showed you on the previous slide by matrix modificationificationã€‚

 So if I go back it's just a matrix modification to compute these three things and we do this actually for all the inputsã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_13.png)

![](img/95f5761979ffd1fb738f0dfee9498c14_14.png)

So we also do this here and hereã€‚So from word 12 up to the last wordï¼Œ the teeth wordã€‚

Now we are also computing the attention values hereã€‚As a dot productã€‚Howeverã€‚

Here we are now computing it instead of computing it between the sequences on x1 and x on let's say Iã€‚

Instead of computing it like thisã€‚We computedï¼Œ actuallyï¼Œ betweenã€‚The query and the keyã€‚

 So these are just modified versions of thatã€‚ So here we are regardingã€‚Q 2 as the current inputã€‚

 So we are using Q2 here everywhereã€‚ This is our queryã€‚And then we use the keyã€‚

So maybe I should use different colorsã€‚So for the blue one hereï¼Œ we useã€‚Bluook keyã€‚

For green one here for itselfï¼Œ we use green oneã€‚And for this oneï¼Œ we use this oneï¼Œ butã€‚Thisã€‚

Query is all the sameã€‚ The keys are differentï¼Œ but the queries are the sameã€‚

 So why the terms query key and valueï¼Œ Thisï¼Œ I thinkï¼Œ comes from the field of databasesã€‚

 So here it's not really that relevantã€‚ It's justã€‚Different different names for different parts of this computation hereã€‚

And you canï¼Œ yeahï¼Œ think of thisã€‚ This is also known asã€‚

The stock product here has multiplicative attentionã€‚

 there's also the other types of attention like additive attentionã€‚

 but here it's like a form of yeah form of multiplicative attention and essentially for each query for each here for each query the model learns which key value input it should attend to soã€‚

Continuing with this model hereï¼Œ when we compute thisã€‚

 there is also a normalization via the soft mixã€‚And then these are added up to form this a2ã€‚

 which isã€‚The context aware embedding of the inputã€‚ So if I go backã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_16.png)

This is similar essentially to what happens here right so when we have our context aware embedding of the input X Iã€‚

 now we have the same thing except the computation is a little bit fancier because we involve these three matricesã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_18.png)

I will also show youï¼Œ I meanï¼Œ there is also a scaled version of thatã€‚ but one step at a timeã€‚

 So here is the not scaled versionã€‚ So hereã€‚In the centerï¼Œ we haveï¼Œ againï¼Œ a soft mixã€‚

 So the soft mix of these dot productsã€‚So that these attention weightsï¼Œ these A'sã€‚

Some up to one just like beforeã€‚ And then what's newestï¼Œ we multiply this here by the valueã€‚

So we do that for all the T valuesã€‚ we are summing over them hereã€‚This will be a vectorã€‚

 so this will be the vectorã€‚4ã€‚The second wordã€‚A 2 corresponding to x 2ã€‚

 we would do the same thing also for x 1 and X T also up to X Tã€‚ So we would repeat this processã€‚

 but we would each time swap the query then byï¼Œ let's say queryã€‚1ï¼Œ the first wordï¼Œ and then alsoã€‚

Teethwardã€‚And so forthã€‚ But notice usï¼Œ what's cool about this is we could do this all in parallelã€‚

 There is no sequential nature of thatã€‚ so we could all compute these in parallelã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_20.png)

Yetï¼Œ to explain a little bit better what was going on in the previous slideã€‚

 I made a copy of the previous slide and added some annotation about the different dimensions of the different parts in that figureã€‚

 So let's walk through this from left to rightï¼Œ soã€‚Here X Iï¼Œ X1ã€‚

 this is a word a word embedding vectorï¼Œ so you can think of it as a one times the E dimensional matrix or de dimensional vector where the E isã€‚

 yeahï¼Œ the embedding size and the original attentions all you need paperã€‚

 they used 512 as the embedding sizeï¼Œ but of courseï¼Œ this is a hyperparameterã€‚

 It's something you can choose it's arbitrary in that way can have 256 or 1024 or some other number is the word embedding size as we have seen also when we worked with RNsã€‚

Then we have our matrices hereï¼Œ our Wsï¼Œ the query key and valueï¼Œ and they have to haveï¼Œ of courseã€‚

 the same dimensionï¼Œ I mean the same number of rows as we have columns here for the matrix multiplicationã€‚

So we have D E here everywhere and then the output size or the number of columns is Dvï¼Œ Dq and Dã€‚

 and in the original paperï¼Œ they had Dq equaling Dã€‚ I meanã€‚

 of course thats also necessary for the stock product hereï¼Œ right you have to match the dimensionsã€‚

And in the original transform paperï¼Œ they also had D Q equal to Dvã€‚So this isï¼Œ yeahã€‚

 we will see later why that isï¼Œ it's because we have also yeah certain stacking going on and things like thatã€‚

 So that's determining our output sizeã€‚å—¯ã€‚Okayï¼Œ what elseï¼ŸYeahï¼Œ soã€‚These will beï¼Œ of courseï¼Œ scalersã€‚

 these top productsï¼Œ rightï¼Œ because it's a modification between two vectorsã€‚ So our soft mix hereã€‚

 this will give usã€‚Our essentiallyï¼Œ our scaled attention weight that we thenã€‚

Multiply by this value vectorï¼Œ which is a one timesã€‚DV dimensional vectorã€‚And then we sum that upã€‚

 So the output size of this one would be one times Dvã€‚Soï¼Œ that'sã€‚Yeahã€‚

 just annotated what what these parts areã€‚ now you know what is the vector and what's the matrix if that wasn't clear beforeã€‚

 but yeahï¼Œ this isï¼Œ I thinkï¼Œ maybe just another summary of the previous slideã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_22.png)

Here's an exampleã€‚

![](img/95f5761979ffd1fb738f0dfee9498c14_24.png)

Of these attention visualizationsã€‚ So againï¼Œ this one here is just for oneã€‚One wordï¼Œ rightï¼Ÿ

 So this is just forã€‚The second wordã€‚ So it's the attentionã€‚Betoï¼Œ for the second wordã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_26.png)

So the previous slideï¼Œ we hadã€‚Looked at the second word as the inputã€‚ but of courseã€‚

 we have also other words in the inputã€‚ So we would repeat this whole process for every input elementã€‚

 So here on the left hand sideã€‚We consider the first wordã€‚ and here on the right hand sideã€‚

 we consider the last wordï¼Œ whereas here we consider the second word butã€‚

This might be a misleading visualizationã€‚ It looks like there's a sequential part itã€‚

 That's not trueã€‚ All of these can be computedã€‚In parallelã€‚

So we don't have to wait for one to finish before we can compute the next so these are all parallel computationsã€‚

 which is one nice aspect about these transformer models or the self attention mechanism in the transformer modelã€‚

And then so we get an attention aware embedding for each of the wordsã€‚ And then these essentiallyã€‚

 you can think of it as a matrixï¼Œ now an attention matrixã€‚Where each row corresponds toã€‚

 yeah to the em beddingï¼Œ attention and bedding for each wordã€‚So it shouldï¼Œ if we have a T hereã€‚

 this should actually beã€‚

![](img/95f5761979ffd1fb738f0dfee9498c14_28.png)

Tã€‚Okayã€‚Yeahï¼Œ and here is just like more like compact notation for thatã€‚ So now considerã€‚

For the inputsï¼Œ we represent the inputs as a T times the E dimensional matrixã€‚

 where the is are embedding size againã€‚ and T is the input sequence sizeã€‚

So I'm just summarizing everything here in the centerã€‚ So nowã€‚

Instead of doing these steps individually hereã€‚Can just write this as one matrix multiplicationã€‚Soã†ã€‚

We now haveï¼Œ let's sayï¼Œ the matrix Qã€‚ So we instead of having Q1 hereï¼Œ Q1ï¼Œ Q1 and so forthã€‚

 So these resultsï¼Œ we can just summarize that asã€‚A matrix at T times D Q dimensional matrixã€‚

K times T times d and t times D Bã€‚ and then we can compute this attention matrix which is t times t dimensional matrixã€‚

 So this is oneï¼Œ maybe one disadvantage of the self-at approach is that this is kind of large if you have a large input sequence because yeahã€‚

 it's like a pairwise in a wayï¼Œ a pairwise similarity score here So it's n squared what t squared the big O of thatã€‚

 So it's not the mostï¼Œ I would say memory efficient approachã€‚ but wellã€‚

At leastt we can compute things in parallelï¼Œ soã€‚We have now kindly a kind of slightly modified version of the dot product that I showed you beforeã€‚

 We call that a scale dot productï¼Œ which is because of this 1ã€‚ I will explain that at the next slideã€‚

 I'm just focusing again on this whole thing hereã€‚ So now we have Q timesã€‚K as a matrix modificationã€‚

 Then we take the soft maxã€‚ So we have Q terms Tã€‚Q times k is the matrix modificationã€‚

 And we have this scaling hereã€‚ and we'll talk about it in the next slide setã€‚

 and then we have the softmã€‚And then we multiply by the matrix of Vã€‚

 So this is just the more compact form of what I've shown you on the previous slidesã€‚

 and then we will get this T times TV dimensional attention matrixã€‚Oã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_30.png)

So what is this scaling factor here in the denominatorã€‚

 So this is just to prevent these individual dot productsã€‚ So if you do a matrix modificationã€‚

 you can also think of it as multiple dot products between these Q and k soã€‚

To prevent these to become too largeã€‚We scale them because if you think of the softm functionã€‚

 If you have a very negative input to the softmax functionï¼Œ it will be 0ã€‚

 If you have a very large inputï¼Œ a very positive large inputï¼Œ it will be close to one in the softmã€‚

Soã†ã€‚In order to render a very sharp distribution of values in the softmã€‚

 we have this scaling factor because if you maybe just think back of the logistic syigmoidã€‚

 which is essentially similar to the softmã€‚Except that soft mix incorporates all the other onesï¼Œ butã€‚

It's kind of like a s model thingã€‚ If you have values close to 1ï¼Œ close to 0ã€‚

 there's like this saturationï¼Œ rightï¼Œ So you want to prevent values from being too extremeã€‚ I meanã€‚

 the same concept This isï¼Œ of courseï¼Œ not a softmï¼Œ but the sameã€‚Concept applies to the softmã€‚

 So by scalingï¼Œ we prevent the softm from having a distribution that is too sharpã€‚Yeahã€‚

 and here just to our reference is the visualization of the scaled dot product extension from the attention is all you need paperã€‚

 so here again they are justmarizing summarizing that visuallyã€‚

 the matrix modification and the scaling hereã€‚Optional mask we will talk about the mask later when we talk about the transformer here in these steps there is no masksã€‚

 so the transformer also consists of an encoder and a decoder and the decoder has a mask So here can actually ignore it and then we have the softm and then we have a matrix modification with B so this is essentially summarizing this oneã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_32.png)

Okayï¼Œ yeahã€‚ okayã€‚ This was self attention and the scared dot product attentionã€‚ In the next videoã€‚

 I will talk about the multi head attentionã€‚And then we will be one step closer to the transform modelã€‚



![](img/95f5761979ffd1fb738f0dfee9498c14_34.png)