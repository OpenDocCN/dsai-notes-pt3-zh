# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P131ÔºöL15.7- PyTorch ‰∏≠ÁöÑ RNN ÊÉÖÊÑüÂàÜÁ±ªÂô® - ShowMeAI - BV1ub4y127jj

AlrightÔºå so this is going to be the last video in this lecture on recurrent neural networks„ÄÇ

 so we are now talking about implementing an R andR class R classifier in Pythtor„ÄÇ

 so the many to one word R andN that we just talked about in the previous video so it will be essentially this whole process from going from a text to building a vocabulary then this integer representation and then using an embedding to get this input to the R and N which will be when you recall this one many to one R andN where we have one sentence as an input and one classable as the output„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_1.png)

![](img/49104266c97f1f4f9793724450b9f7d0_2.png)

![](img/49104266c97f1f4f9793724450b9f7d0_3.png)

![](img/49104266c97f1f4f9793724450b9f7d0_4.png)

So I have actually two codes here„ÄÇ One is an LCSDM and one is a packed LSDM„ÄÇ

 The packed LCTM is just a more implement more efficient implementation„ÄÇ So let's just talk about„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_6.png)

MeI just„ÄÇ let's talk about the regular LDM firstÔºå the regular approach„ÄÇ So it took yeah„ÄÇ

 a couple of minutes to run this„ÄÇ So I I'm not going rerun this„ÄÇ

 We are just taking a look at the resultsÔºå and I will upload this„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_8.png)

![](img/49104266c97f1f4f9793724450b9f7d0_9.png)

![](img/49104266c97f1f4f9793724450b9f7d0_10.png)

To githubÔºå if you want to play around with that„ÄÇ So we are going to use„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_12.png)

Torch or pyrch and torch text„ÄÇSo in particularÔºå any version of Torch text that is 0„ÄÇ9 or higher„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_14.png)

Also notice I don't have any helper files here because yeah this is like a tricky concept to explain„ÄÇ

 I wanted to keep everything in this notebook to make it a little bit more yeah easy to explain by walking you through this instead of visiting different files„ÄÇ

 but yeah you could technically also use these helper files when code becomes larger„ÄÇ

 So here the training function will be very simple„ÄÇ

 I don't have any fancy training function this time„ÄÇ

 just to keep things simple because I think an RnN is already complicated enough„ÄÇ

 So compared to a convolutional networkÔºå these R ends are actually really tricky to implement at least in my opinion„ÄÇ

 I actually very much prefer working with convolutional networks„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_16.png)

So„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_18.png)

Here are some general settingsÔºå so we are going to use the vocabulary size capped at 20000„ÄÇ

The learning rateÔºå batch sizeÔºå number of epoes is something familiar to you„ÄÇ

 The embedding dimension that we will use is 1 or 28„ÄÇ The hidden dimension will be 256„ÄÇ

 This is like on„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_20.png)

After that„ÄÇAnd then the number of classes is 2„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_22.png)

So firstÔºå we are going to download the datasetÔºå the IMDB movie review dataset„ÄÇ So here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_24.png)

In thisÔºå I in this partÔºå we are downloading it from my book becauseÔºå yeahÔºå just for simplicity„ÄÇ

 because this will save us some pre processing steps„ÄÇ

 There is actually an I M D B dataset set implemented in torch text„ÄÇBut hereÔºå I'm also„ÄÇ

Explaining to you how you can use an LSDM on your own dataset„ÄÇ So it's basically two steps in one„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_26.png)

And we are using this because it's a CV format that is already„ÄÇ

 I would say easy to use so we can skip all the pre processingsing steps of this particular dataset set so that we can more focus on how torch text works„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_28.png)

So this is just on downloading it„ÄÇFrom the St repositorium„ÄÇThen this is unziping it„ÄÇ

And then this is loading it into up„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_30.png)

Panda's data frameÔºå because it's just a little simpler„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_32.png)

Thenhan using anything else„ÄÇ So this is a CV file„ÄÇ So we are using pandas to take a look at it„ÄÇ

 So it hasÔºå I think 50000 entries„ÄÇ So yeahÔºå these are the first five„ÄÇ The review is the text„ÄÇ

 the input text and the sentiment is the class label„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_34.png)

![](img/49104266c97f1f4f9793724450b9f7d0_35.png)

It's kind of to personallyÔºå to meÔºå a common gotcha to have the wrong names later in the code„ÄÇ

So here I'm using something called a text column name and label column name as the name for these„ÄÇ

 because„ÄÇYou will see that later„ÄÇYou have to provide„ÄÇ

Kind of like attribute access to these features in the training loop and the same for the labelsÔºå so„ÄÇ

Depending on the names of your columns in the pans data frame„ÄÇ

 your training loop might look different„ÄÇ You have to rewrite itÔºå and I find this very tedious„ÄÇ

 so I would give my panda data frame if I have different data sets always these names just to keep it simple„ÄÇ

 it could be something elseÔºå but I find it also capital letters„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_37.png)

![](img/49104266c97f1f4f9793724450b9f7d0_38.png)

Useful„ÄÇ So it reminds me what this means or what this is„ÄÇ

 it's just easier to see it that screams out„ÄÇ OkayÔºå this is a column name here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_40.png)

And this is important„ÄÇ AlrightÔºå so I'm just giving it the generic name„ÄÇ

 text column name and the generic nameÔºå label column name here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_42.png)

AlrightÔºå and then I'm deleting this again„ÄÇ So here I was just essentially loading it„ÄÇ

 renaming it and saving it again„ÄÇ And then here this is just for taking a look at it again that it looks okay and then I'm deleting it because I'm not using it here anymore„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_44.png)

![](img/49104266c97f1f4f9793724450b9f7d0_45.png)

![](img/49104266c97f1f4f9793724450b9f7d0_46.png)

Then we are going now to prepare this data set with torch text as input to the R and N„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_48.png)

For thatÔºå we are going to use a library that is called Spacey„ÄÇ

 Spacey is a very popular natural language processing library for Python„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_50.png)

And in particularÔºå we are using the tokenizer„ÄÇSo by default„ÄÇ

 it would use tokenizer splitting on white spacesÔºå but I heard from people working with real world datas that sometimes these are not very robust to„ÄÇ

 let's say weird characters„ÄÇ So also HL characters and things like that„ÄÇ

 And this tokenizer also gets rid of certain formatting things you find in HL like these on„ÄÇ

These symbols here and so forth„ÄÇ So it's like a little bit more sophisticated than just splitting on white space„ÄÇ

 So what this is doing is it's splitting yeahÔºå a text„ÄÇInto white space so that one token is one word„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_52.png)

And we are using here this„ÄÇEnglish language„ÄÇ it'sÔºå I think it's stands for English core„ÄÇ

 web and something„ÄÇ I'm not this is not for something„ÄÇ I'm just saying something„ÄÇ

 I forgot what this meansÔºå but this is essentially a library or not a library dictionary for English words and web things encountered on„ÄÇ

 and on websites and stuff like that„ÄÇ So this is usually useful„ÄÇ If youÔºå yeah„ÄÇ

 just work with a data like scrape from somewhere on the Internet„ÄÇIf you are just running this„ÄÇ

 it may be that you encounter will encounter an error„ÄÇ So you have to run this one firstÔºå which will„ÄÇ

 you have to run this on the command I this will download this„ÄÇYeahÔºå dictionary here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_54.png)

And yetÔºå to install a space hereÔºå I recommend CondaÔºå but you can also lose Pip„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_56.png)

Pip install„ÄÇNowÔºå like I said before in the previous videoÔºå things have changed in Torch text„ÄÇ

 so we are going to use the old way called Torch text„ÄÇt legacy„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_58.png)

![](img/49104266c97f1f4f9793724450b9f7d0_59.png)

But if you want to convert this to the new oneÔºå I actually spent a lot of time yesterday„ÄÇ

 and then it was not working very well„ÄÇ So I„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_61.png)

![](img/49104266c97f1f4f9793724450b9f7d0_62.png)

We use the old way again„ÄÇ but if you are interested hereÔºå there's a tutorial for migration„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_64.png)

On the slides„ÄÇSo to migrate to them newer way that it doesn't use this legacy thing„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_66.png)

AlrightÔºå so we are now defining a field for the text„ÄÇ So this will be our featuresÔºå our tokens„ÄÇ

 So thisÔºå this will be„ÄÇ So if we haveÔºå for instance on„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_68.png)

OopsÔºå if we have this text hereÔºå this will be something like a list containing of„ÄÇ

Containing these consisting of these words„ÄÇSplit by„ÄÇApproximately something like the white space„ÄÇ

 but a little bit fancier than that„ÄÇ But each entry in the list will be one wordÔºå essentially„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_70.png)

Using this tokenizer„ÄÇAl right„ÄÇ The second one is the label„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_72.png)

So this is for designating this label„ÄÇWhich will be a integer„ÄÇ LongÔºå long is just a 604 bit integer„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_74.png)

Here we are providing these fields„ÄÇAnd„ÄÇThis is„ÄÇSo we are providing these fields which are the text that we have defined here and the label that we have defined here„ÄÇ

 and we are using a tabular data set which will read our CV file and then parsing out these things„ÄÇ

So that we will can then load them as a data load„ÄÇ So hereÔºå this is why it's so here„ÄÇ

 this is important„ÄÇ This name has to be„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_76.png)

![](img/49104266c97f1f4f9793724450b9f7d0_77.png)

The name that is actually here„ÄÇ And personallyÔºå it'sÔºå that is why I always make mistakes„ÄÇ So I„ÄÇ

 let's sayÔºå don't rename it„ÄÇ If I don't rename itÔºå I would have to put in like„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_79.png)

Oops„ÄÇI would have to put in review here„ÄÇAnd then sentiment here„ÄÇ And then later„ÄÇ

 I have to also use these words„ÄÇ And if I have a predefined training loop„ÄÇ

 I would have to rewrite certain thingsÔºå depending on what data set I use„ÄÇ

 This is why at the beginningÔºå I rename the column„ÄÇ

 So I can always leave it like it is right here now„ÄÇ The only thing„ÄÇI have to changesÔºå of course„ÄÇ

 the path to my dataset„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_81.png)

AlrightÔºå And in this caseÔºå it doesn't have a header„ÄÇ So you also have to check in this case„ÄÇ

 there is no header row„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_83.png)

![](img/49104266c97f1f4f9793724450b9f7d0_84.png)

There are only the colour namesÔºå but there's noÔºå no particular header rule„ÄÇ OhÔºå waitÔºå Sorry„ÄÇ

 there is skip header„ÄÇ There's no header rule„ÄÇ It's skipping the headerÔºå okay„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_86.png)

![](img/49104266c97f1f4f9793724450b9f7d0_87.png)

So it's skippingÔºå skipping these„ÄÇ SorryÔºå what I meant is it has a header„ÄÇ It has these column names„ÄÇ

 and it's skipping those„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_89.png)

Okay„ÄÇSo this is now the way we process our CV file into a data set„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_91.png)

Next„ÄÇWe want to have a training and a test dataset set„ÄÇ So I'm using this split function„ÄÇ

 I'm splitting this dataset set into two a training and a test dataset set„ÄÇ Actually„ÄÇ

 I try to split it into 3 directly„ÄÇ It should technically support that„ÄÇ So I had something like„ÄÇ7„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_93.png)

Pot1 and„ÄÇ2„ÄÇ But it gave me some very weird results„ÄÇ I think it's a bug„ÄÇ

 So because what happened was I had a validationation set like this„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_95.png)

Train dataÔºå validation data and test data„ÄÇ For some reason„ÄÇ

 I don't know why the validation data was much bigger than a test dataÔºå which should be the opposite„ÄÇ

 And I tried many things„ÄÇ It seems to be a bug„ÄÇ So I do it in two steps„ÄÇ First step is„ÄÇIm splitting„ÄÇ

The training data or the data center for training dataÔºå 80% and 20% test data„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_97.png)

So I'm double checking hereÔºå so the dataset consists of 50000 data pointsÔºå40000 will be for training„ÄÇ

10000 will be for testing„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_99.png)

And then I split the training data further into a training data set again and validation data„ÄÇ

 So in totalÔºå what I will have is 34000 training examples„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_101.png)

6000 validation examples and 10000 test examples„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_103.png)

Just to make sure everything looks okayÔºå we are now taking a look at the first training example„ÄÇ

 So 0 index0 is the first training example„ÄÇ and this is how it looks like„ÄÇSo„ÄÇ

 text column name that is„ÄÇThe tokenized„ÄÇ So this is using the tokenizer„ÄÇ

 This is the tokenized review text„ÄÇ You can see it keeps punctuation„ÄÇ It keeps numbers„ÄÇ

So for some unknown reasonÔºåa seven years agoÔºå I watched blah blahÔºå blahÔºå So you can seeÔºå yeah„ÄÇ

 this is now the tokenized text„ÄÇ This is from using the spacey tokenizer„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_105.png)

And alsoÔºå there's the can name here„ÄÇ This is like the„ÄÇThis should be the class label„ÄÇ

Cre you not sure why this is„ÄÇNot an integer„ÄÇ LaterÔºå actuallyÔºå doesn't cause any problems„ÄÇ

 So it seems to be okayÔºå but feel like this should be without quotation marksÔºå but anyways„ÄÇAllright„ÄÇ

 so„ÄÇNowÔºå we have the„ÄÇData sets the training dataÔºå validation setÔºå data and the test data„ÄÇ

What we are now doing is we are building the vocabulary„ÄÇSo hereÔºå build vocab„ÄÇ

I' am setting a maximum size for the vocabulary because we are to prevent overfitting„ÄÇ

 So we are only using the most used wordsÔºå the 20000 most frequent words„ÄÇ

 So I define the vocabulary size here or somewhere at at the top can play around with that„ÄÇ

 can use 10002500Ôºå30000 depends a little bit on how big the training set is„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_107.png)

![](img/49104266c97f1f4f9793724450b9f7d0_108.png)

And howÔºå yeahÔºå how diverse the data points the texts areÔºå how long the texts are„ÄÇ

 But 20000 is a good number to start with„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_110.png)

SoÔºå we are building„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_112.png)

This now„ÄÇAnd for some reasonÔºå it's also called build wall cap here„ÄÇ I think I'm doing it right„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_114.png)

So vocabulary sizeÔºå what we all find is its 20„ÄÇThousand and2Ôºå not 20000„ÄÇ

 And this is because we have the unknown word„ÄÇToken„ÄÇ

 So if we encounter an unknown word that it will not crash and then answer the ones for patting„ÄÇ

 And we have two classesÔºå0 and 1„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_116.png)

![](img/49104266c97f1f4f9793724450b9f7d0_117.png)

Here I'm now showing the just to look at whether it makes sense„ÄÇ Actually„ÄÇ

 I can see there is something that I feel like shouldn't be there„ÄÇ as is this break character here„ÄÇ

I thoughtÔºå to be honestÔºå the spacey tokenizer would be a bit more robust that it would not have these types of things„ÄÇ

 but oh well„ÄÇHappens„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_119.png)

It's probably not perfect„ÄÇËØ¥„ÄÇHere we are looking at the  trend0 most frequently encountered„ÄÇ

Words or or tokens„ÄÇ So the is very frequent„ÄÇ comm frequent point punctuation and so forth„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_121.png)

But yeahÔºå this kind of bothers me„ÄÇ This shouldn't be here„ÄÇ So we would in an real world application„ÄÇ

 probably have to deal with that using a different tokenizer or maybe just stripping it out before we pass it to the tokenizer or something like that„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_123.png)

![](img/49104266c97f1f4f9793724450b9f7d0_124.png)

AlrightÔºå next tokens corresponding to the first 10 indices just to look at those„ÄÇ

 So we have a vocabulary that is of size 2000„ÄÇ If I go back to my„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_126.png)

![](img/49104266c97f1f4f9793724450b9f7d0_127.png)

My slides„ÄÇ So we have this vocabulary„ÄÇAnd we are now looking 1Ôºå2Ôºå3Ôºå4Ôºå5„ÄÇ

 The different integer the strings correspond to„ÄÇ So the first entry in my vocabulary is this unknown  one„ÄÇ

Second one is the paddingÔºå and then the command point and A OÔºå F and so forth„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_129.png)

And just for I just making sure things work technically„ÄÇ

 so we will use that data for making predictions„ÄÇ But later generallyÔºå we can also just take this„ÄÇ

Vocabulary for the text field and convert any word into this interteject corresponding to the dictionary„ÄÇ

The the according to this vocabularyÔºå the word the is at index position 2Ôºå So 0Ôºå1Ôºå2„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_131.png)

So here we are putting it in and get the number 2„ÄÇ So this is all we are here currently„ÄÇ

 this is not necessary„ÄÇ We are just investigating what's going on just to make sure things look okay„ÄÇ

 OhÔºå yeahÔºå I see„ÄÇ SoÔºå yeahÔºå we have now this class label vocabulary„ÄÇ And I mentioned earlier„ÄÇ

 that shouldn't be strings„ÄÇ I meanÔºå this is probably because„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_133.png)

![](img/49104266c97f1f4f9793724450b9f7d0_134.png)

YeahÔºå we could have„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_136.png)

Here we could have put the word pause or neck for positive or negative„ÄÇ

 This is what's original in the movie review„ÄÇDatabase„ÄÇ So here I inÔºå in my book„ÄÇ

 I converted it to 1 and 0 already and„ÄÇThe code thinks these are yeahÔºå strings„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_138.png)

Kind of funny„ÄÇ So it's mapping 1 to 0 and 0 to 1„ÄÇ So we would have to keep that in mind when we later do the prediction„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_140.png)

So we could have just used words like pause and negative and stuff like that„ÄÇ We could also„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_142.png)

YeahÔºå we could also change that if we wanted to„ÄÇ it's justÔºå yeahÔºå it's justÔºå I think„ÄÇ

 alphabetical order or something like that„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_144.png)

Well it can't be alphabeticalÔºå I think it's just„ÄÇWhat it has encountered first or something„ÄÇ

 Or maybe it's even random„ÄÇJust have to remember that the„ÄÇ

String  one in our label column corresponds to the class label 0 and and in the tensor later„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_146.png)

Then here is frequency count of the vocabulary„ÄÇ sorry of the training data points„ÄÇ

So we have approximately 60000 corresponding to 0Ôºå and this one is actually negative„ÄÇ0 is negative„ÄÇ

 and one is positive and 70000 positive ones„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_148.png)

On„ÄÇOne more thing I wanted to say is let'sÔºå yeah here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_150.png)

I am building the vocabulary only on the training dataÔºå not on the whole dataÔºå because as usual„ÄÇ

 we pretend that the validation and test data are unseen that there are new data sets„ÄÇ

 That's what we use for evaluating our model during training„ÄÇ we pretend we don't know these„ÄÇ

 It's like independent data„ÄÇ So we are only building„ÄÇThe voculary based on the known data„ÄÇ

 the training data„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_152.png)

AlrightÔºå so you can see it's really complicated to implement an R and are lots of steps involved„ÄÇ

 it's way more complicated than a convolutional network„ÄÇ

 So if you don't really understand everything the first time it's totally normalÔºå it's just„ÄÇ

 it is very complicated„ÄÇ People study this for many months to before they become comfortable using those things„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_154.png)

Now we are implementing the training validation and test load„ÄÇ

 we use something called the bucket iteratorÔºå which is a special iterator in Pytorrch torch text„ÄÇ

 which will group„ÄÇThe batches such that the sentences have„ÄÇSimilar length„ÄÇ

 And that reduces the number of pedding that is required„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_156.png)

OkayÔºå here now we are testing whether those work„ÄÇ ActuallyÔºå these data load us and you„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_158.png)

Can see what I'm just doing is I'm like before in my other code examples„ÄÇ

 I'm just doing for batch and train order„ÄÇ and then I„ÄÇPrint these„ÄÇ and then I do a break„ÄÇ

 So it only shows the first batch„ÄÇ I just want to see ifÔºå if it runsÔºå okay„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_160.png)

So you can see for the first batchÔºå it's actually pretty largeÔºå I should say„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_162.png)

The first value here is the sentence length„ÄÇAnd the second one is the batch size„ÄÇ

 So this is a little bit different from the conversion networksÔºå where we had the batch size first„ÄÇ

This is what makes I feel like everything also a bit complicated to understand„ÄÇ

 It's like that the sentence length here is„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_164.png)

First„ÄÇAnd the sentence length is„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_166.png)

These integers hereÔºå this here„ÄÇSo I a better one here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_168.png)

YeahÔºå thisÔºå this is basically the sentence„ÄÇ WellÔºå this is for one wordÔºå sorry„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_170.png)

ÂóØ„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_172.png)

YeahÔºå I don't have a good one here in the lecture notesÔºå but„ÄÇSo if you concatenate these together„ÄÇ

 this would be the sentence length„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_174.png)

And then this is the batch size„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_176.png)

I think yeah„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_178.png)

![](img/49104266c97f1f4f9793724450b9f7d0_179.png)

Alright„ÄÇ so now the R and N here„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_181.png)

SoÔºå the R and N„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_183.png)

OkayÔºå sorryÔºå let's just have to„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_185.png)

Because a bit bigger here„ÄÇ So the R and N„ÄÇLooks like as followsÔºå we're actually using an LTM„ÄÇ

 So there are„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_187.png)

Several things now„ÄÇ we are using this embedding that provides the input to the LSDM„ÄÇ

Then the EsteM itself„ÄÇAnd then the„ÄÇOutput layer„ÄÇHidden there„ÄÇ sorryÔºå the output layer„ÄÇ

 it's not a hidden there„ÄÇThisÔºå you can think of of the LSDM„ÄÇ you can think of as the hidden layer„ÄÇ

WhereasÔºå yeahÔºå okayÔºå the embedding comes before that„ÄÇ it's like preparing the input„ÄÇFor the hidden„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_189.png)

ActuallyÔºå in the slidesÔºå I said it's this oneÔºå but then there would be another matrix„ÄÇ

 So it's technically a little bit confusing„ÄÇ I should have actually not done that„ÄÇ

 I should have showed you„ÄÇIt is a embedding that comes before that that prepares these xs„ÄÇ And this„ÄÇ

 this W technically belongs to the R and N hidden layer„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_191.png)

If we would implement this one here„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_193.png)

And the SDDM has a more complicated setupÔºå as you recall from last time„ÄÇ

 So it has all these different types of hidden layers here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_195.png)

![](img/49104266c97f1f4f9793724450b9f7d0_196.png)

Okay„ÄÇÂóØ„ÄÇNo„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_198.png)

What we have here is the embedding that converts„ÄÇThe word into the real value vector„ÄÇ

 If I go back to my slides againÔºå this is giving us this matrix here„ÄÇ So this goes from text„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_200.png)

![](img/49104266c97f1f4f9793724450b9f7d0_201.png)

To this interteger vectorÔºå through these embeddings„ÄÇ

And then the LSDM takes in this embeddings and produces the hidden activations„ÄÇ

And then this is just like a classification layerÔºå like in a multi layer perception or the last layer of a convolutional layer„ÄÇ

 This goes from the hidden dimension„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_203.png)

To the class labelsÔºå the output dim is the number of class labels„ÄÇNowÔºå these are defined here„ÄÇ

 We can't use easily a sequential like we did before„ÄÇ

 because there's a little bit the output looks a little bit different„ÄÇSo I'm also„ÄÇ

 maybe I should say you can technical use an R and N instead of the LSDM„ÄÇ

 but you will find the performance is relatively poor„ÄÇ So you„ÄÇProbably want to use an SDDM„ÄÇ

 but if you want to do some experimentsÔºå you can actually use the R and N instead of the LSTM„ÄÇ

 The LSTM is essentiallyÔºå yeahÔºå what I showed you before in the last2Ôºå two videos ago„ÄÇ

 the LSDM hidden layer„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_205.png)

Okay„ÄÇThe forward pass gets a text„ÄÇ So this gets ready the text„ÄÇ This goes through the embedding„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_207.png)

![](img/49104266c97f1f4f9793724450b9f7d0_208.png)

And then the embedding is the input to the LSDM„ÄÇ So I call it R nÔºå but's the LSDM„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_210.png)

And it outputs„ÄÇThe outputÔºå the hidden and the cell state„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_212.png)

So„ÄÇÂóØ„ÄÇmJust looking for a good slide here„ÄÇ So we have this many21 here„ÄÇAnd„ÄÇThe„ÄÇ

Should probably focus on manyÔºå one too many or many too many„ÄÇ OkayÔºå so let's„ÄÇ

 let's consider this many too many here„ÄÇ When I call the LSDMÔºå it will output„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_214.png)

![](img/49104266c97f1f4f9793724450b9f7d0_215.png)

So the orange  one you can think of asÔºå as the LCDMÔºå it will output„ÄÇ

Something that goes to the output„ÄÇ I think Id a better slide somewhere hereÔºå YeahÔºå here„ÄÇ

 So it will output the y„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_217.png)

And it will output the hidden state for the next hidden layer„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_219.png)

So this is the output is the y„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_221.png)

This is the y„ÄÇHidden is what goes here in this arrow„ÄÇ What goes to the next one„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_223.png)

And cell state is specific to the LSDM„ÄÇ That's this LSTM state„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_225.png)

Okay„ÄÇSo„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_227.png)

ÂóØ„ÄÇWe scroll here„ÄÇSoÔºå we have„ÄÇCell state that is output here„ÄÇ rightÔºü So this one hereÔºå Then we have„ÄÇ

 this is the y here to next layer where it says to next layerÔºå this would be the y„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_229.png)

This would be„ÄÇThe output„ÄÇAnd H TÔºå this would be the hidden„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_231.png)

And sellÔºå this would be the C„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_233.png)

It is complicated„ÄÇ So if that does make sense immediatelyÔºå it is a complicated concept„ÄÇSo„ÄÇ

 since we have„ÄÇLet me find a better representation yet again„ÄÇSince we have many to one„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_235.png)

We are not going to use these here„ÄÇ So we are not going to use this one and this oneÔºå so„ÄÇThis one„ÄÇ

 we are not going to use„ÄÇ This one is not„ÄÇ we are not going to use„ÄÇ

 and this one is kind of also included in this output„ÄÇSo to make things simplerÔºå we are going to use„ÄÇ

 actually„ÄÇLet me run arrow„ÄÇ So what we are going to use here is we are going to use the hidden output from the last one„ÄÇ

 which is this hidden here„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_237.png)

![](img/49104266c97f1f4f9793724450b9f7d0_238.png)

And thenÔºå we will provide„ÄÇOr own hidden there„ÄÇOf our own output here„ÄÇ instead of using this green 1„ÄÇ

 we will have our own„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_240.png)

![](img/49104266c97f1f4f9793724450b9f7d0_241.png)

Fooully connected here or fully connected thereÔºå this is„ÄÇWhat we are doing here„ÄÇ

 So we are computing our own output„ÄÇ We are„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_243.png)

Removing thisÔºå it'sÔºå let's say too complicated„ÄÇ We don't want to use allÔºå all these green ones„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_245.png)

We are going to only use„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_247.png)

The hidden oneÔºå which is the output of this orange here of the last orange„ÄÇ

 and then have our own fully connected layer to get this output here„ÄÇ This is what's going on here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_249.png)

AlrightÔºå I hope this makes somewhat sense„ÄÇSo„ÄÇIf we look at the„ÄÇ

Sizes we go from sentence length to batch size„ÄÇ This is like what we had before„ÄÇ

 sentence length and batch sizeÔºå the matrix of our input„ÄÇThen this goes into the embedding layer„ÄÇ

 which produces a sentence lengthÔºå batch sizeÔºå embedding dimension„ÄÇ This is our„ÄÇ

 I don't know why it is arranged like thatÔºå but it is our„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_251.png)

Let me delete this„ÄÇ So weÔºå so I don't save it later when I export it for you„ÄÇ yeah„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_253.png)

So this will beÔºå this is the this is the embedding„ÄÇThis is the embedding matrix„ÄÇ

 This is the embedding for one training example„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_255.png)

We have some„ÄÇ we have multiple because we have a batch size and the batch size dimensions between the embedding and the sentence length„ÄÇ

 So the sentence length length would be the rows here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_257.png)

![](img/49104266c97f1f4f9793724450b9f7d0_258.png)

HereÔºå it's also the rows„ÄÇ and the columns is hereÔºå not the embedding dimensions„ÄÇ It's the batch size„ÄÇ

 So we have sent in flks batch size and embedding„ÄÇ So you have to think there would be an additional dimension here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_260.png)

![](img/49104266c97f1f4f9793724450b9f7d0_261.png)

![](img/49104266c97f1f4f9793724450b9f7d0_262.png)

And then this goes sentence lengthÔºå batch sizeÔºå hidden dimension„ÄÇ This is„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_264.png)

The dimensionity of a hidden there„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_266.png)

We choseÔºå I think 256„ÄÇ and then we„ÄÇWhat we output get isÔºå there's only one„ÄÇ

So hidden is one because it's the last oneÔºå only the last one„ÄÇ

The dimension is batch size and hidden dimension„ÄÇ This is usually what would go into the next hidden layer„ÄÇ

And we want to give it to our weÔºå like I saidÔºå before we make our own output layer„ÄÇ

 Thiss a fully connected layerÔºå this one„ÄÇSo we are removing here with squeeze„ÄÇ

 We are removing this one„ÄÇ So we make this a batch size times hidden dimension„ÄÇ

 And this is something you have seen before all the time when we use the multilay perceptron and a convol network„ÄÇ

This squeeze is justÔºå we are saying remove the first dimension„ÄÇ

 the one here so that it is compatible with our linear layer here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_268.png)

AgainÔºå this is complicated stuff„ÄÇ So if that doesn't make sense„ÄÇ

 you don't have to memorize any of thisÔºå I can totally understand if this is complicated„ÄÇ

 to be honestÔºå I also spent several hours just implementing this„ÄÇ it's it's not easy„ÄÇ

 it's complicated and„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_270.png)

![](img/49104266c97f1f4f9793724450b9f7d0_271.png)

If you really want to work with textÔºå of courseÔºå watching this one lecture gives you just the introduction„ÄÇ

 It's normal to spend weeks or months or professionals spend years really doing all these things There are many„ÄÇ

 many aspects to working with text„ÄÇ this is just the introduction so don't feel bad if this looks a little bit complicated to you„ÄÇ

 it naturally takes time to work with this and you to get a better grasp of what's going on here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_273.png)

OÔºå but moving onÔºå as saw we„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_275.png)

Iitialize now our recurrent neural network„ÄÇ the input dimension is equal to our vocabulary size is a 20002„ÄÇ

 So we use that here in hour to create our embedding matrix„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_277.png)

![](img/49104266c97f1f4f9793724450b9f7d0_278.png)

![](img/49104266c97f1f4f9793724450b9f7d0_279.png)

Then the embedding dimensionÔºå we had something like 1 of28„ÄÇAnd the hidden dimension is 256„ÄÇ

 and number of classes we have„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_281.png)

2Ôºå we set it to two„ÄÇ So if I scroll up to the top„ÄÇWe set it to„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_283.png)

Two hereÔºå we in a dimension 256 embedding 128„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_285.png)

We could technically use just one class and then use logistic sigmoid instead of softm function„ÄÇ

 and then we could use the binary cross entrobus instead of the regular cross entropios in Pytorch„ÄÇ

 I did that at firstÔºå but then I was thinking maybe you would like to use this code here as a template for some other classification problem„ÄÇ

 That is not a binary one„ÄÇ And then you would have to rewrite everything„ÄÇ So I„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_287.png)

![](img/49104266c97f1f4f9793724450b9f7d0_288.png)

![](img/49104266c97f1f4f9793724450b9f7d0_289.png)

![](img/49104266c97f1f4f9793724450b9f7d0_290.png)

![](img/49104266c97f1f4f9793724450b9f7d0_291.png)

Implemented it here with two output notesÔºå although it's redundant„ÄÇ

 I implemented it so that you can adopt it to your own dataset set„ÄÇ

 So I thought it's more useful in this wayÔºå so you don't have to rewrite any code if you want to use that for your project„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_293.png)

![](img/49104266c97f1f4f9793724450b9f7d0_294.png)

AlrightÔºå so now„ÄÇLet's get„ÄÇ So this is initializing the model„ÄÇ I'm just using Adamom„ÄÇ Now„ÄÇ

 let's get to the part„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_296.png)

WellÔºå we have the training„ÄÇ So here's the accuracy function for computing the accuracy so„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_298.png)

![](img/49104266c97f1f4f9793724450b9f7d0_299.png)

YeahÔºå so yeah we are just computing the accuracy„ÄÇ And here that's the training„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_301.png)

![](img/49104266c97f1f4f9793724450b9f7d0_302.png)

InterestingÔºå I should have„ÄÇ yeahÔºå I could have also done it like that„ÄÇ But so to be clear here„ÄÇ

 how this works is„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_304.png)

I have batch index and batch data„ÄÇ So here I did it a little bit differently„ÄÇ So yeah„ÄÇ

 already you unrolled thisÔºå it seemed to work„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_306.png)

![](img/49104266c97f1f4f9793724450b9f7d0_307.png)

But yesÔºå so here I have batch index and batch data„ÄÇ So it's the training loop„ÄÇ

 I iterating over the eposÔºå setting my model to train mode„ÄÇ

 and then I'm iterating over the data loadÔºå and it gives me two things the text„ÄÇ

 which is batch data dot text column name„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_309.png)

And labels which dispatched data dot label column name„ÄÇThis is why I earlier renamed the columns„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_311.png)

Then I'm providing either logicts„ÄÇ This is the output from the model„ÄÇ

 So we give the text to the modelÔºå and it will do the embedding for us„ÄÇ

And then run it through the SDDM„ÄÇAnd ought come the logicits„ÄÇ

 which is just like the logicits in the multi layer perceptron or the convol network that we have seen before„ÄÇ

 And we have the labels here„ÄÇ The labels is all sentiment„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_313.png)

And this is exactly how we've seen that before„ÄÇ There's nothing new here„ÄÇ And yeahÔºå it trains„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_315.png)

ActuallyÔºå I was training itÔºå and I noticedÔºå okayÔºå it's not really workingÔºå right„ÄÇ

 you can see it doesn't really work„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_317.png)

![](img/49104266c97f1f4f9793724450b9f7d0_318.png)

And I was like really frustrated because I spent many hours implementing this„ÄÇ

 and then it didn't work„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_320.png)

But then for some reasonÔºå it picked up training here„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_322.png)

And the performance got really good„ÄÇ So at the endÔºå I had a test accuracy of 84%„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_324.png)

Then also to make sure I should have used the same dictionary„ÄÇ I don't know why I didn't do that for„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_326.png)

YeahÔºå the tokenise anyway„ÄÇ So here is just an example„ÄÇ This I took actually modified it slightly„ÄÇ

 This is based on„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_328.png)

Based on the tutorial here„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_330.png)

Took it from here„ÄÇ So modified it a little bit for this code„ÄÇ

 This is just like an example of the things we have to do in order to put something into our text„ÄÇ

 If we have new text„ÄÇ So let's say I have my model and I have a new text like this is such an awesome movie„ÄÇ

 I really love it„ÄÇüòä„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_332.png)

![](img/49104266c97f1f4f9793724450b9f7d0_333.png)

And I want to know whether it what the probability that this is a positive review is„ÄÇ

It turns out it's 98% positiveÔºå which is what we would expect„ÄÇSo what did I do here„ÄÇ

 I put the model in evaluation mode„ÄÇ I tokenized the textÔºå so„ÄÇYeah„ÄÇ

 so I'm tokenizing a text using this LP tokenizer„ÄÇ I should have maybe used the same as above this web dictionary„ÄÇ

 but it worked just fine„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_335.png)

ThenÔºå I'm getting the„ÄÇInterjo„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_337.png)

Representation„ÄÇThe length I don't need here„ÄÇ I need it for my other model„ÄÇ

 This is why I think I've just left it in here„ÄÇ Then I'm converting it to a long ten tensor and put it onto the GPU„ÄÇ

 It's where my model is„ÄÇ If it was on the GPUÔºå otherwiseÔºå it will be on the CPUÔºå if device is CPU„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_339.png)

Just an optional step if you' trained on the GPU„ÄÇYeahÔºå so this' is the index tensor„ÄÇ

And this index sensorsor will go to the embedding„ÄÇ So the embedding will not do the text into„ÄÇ

The embedding it will be two stepsÔºå so we have to prepare the indices„ÄÇYeahÔºå and then„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_341.png)

That's the softm because we don't have any softm in our model ourselves„ÄÇAnd yeah„ÄÇ

 this is just this predict sentiment function for„ÄÇPutting in arbitrary text„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_343.png)

And here is the same phone„ÄÇ the computingÔºå the probability that something is negative„ÄÇ It's just my„ÄÇ

1 minus thisÔºå right„ÄÇSo hereÔºå I really hate this movie„ÄÇ It is a really bad movie on sucks„ÄÇ

 and you can see„ÄÇModel also detects that this is indeed a negative one„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_345.png)

AlrightÔºå so this is how it essentially works„ÄÇ So this is the LSTMÔºå the regular LSTM„ÄÇ

 It's pretty complicatedÔºå as you have seen„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_347.png)

![](img/49104266c97f1f4f9793724450b9f7d0_348.png)

You can actually make it even co more complicated using this packed approach„ÄÇ

 So if you're interested in thatÔºå there is imp very good explanation here„ÄÇ So essentially„ÄÇ

 it's about„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_350.png)

Ordering„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_352.png)

The sentences and the batches to minimize the number of computations„ÄÇ

 because we have to do a paddingÔºå rightÔºü And if you just shuffle your data set„ÄÇ

 there will be randomly long and short sentences togetherÔºå but„ÄÇ

It is inefficient because for some batchesÔºå you have to pad a lot„ÄÇ

 just because there are one or two long sentences„ÄÇ So this packed approach„ÄÇ

 what we we will do is it will look at the whole training set and organizeise it such that it minimizes the number of padding required„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_354.png)

So there are a few changes that you have to make for that„ÄÇSo„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_356.png)

I highlighted them here„ÄÇ so you have to include the length in the tokenizer and so forth„ÄÇ

 So some changes„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_358.png)

![](img/49104266c97f1f4f9793724450b9f7d0_359.png)

I think here we„ÄÇThe some„ÄÇSting within the batches also required„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_361.png)

![](img/49104266c97f1f4f9793724450b9f7d0_362.png)

YeahÔºå and then here that was what take took me a long time to figure outÔºå you have to„ÄÇ

Use this R N dot pe dot padded sequences and then provide also the text length„ÄÇ

 took me literally hours to figure that out„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_364.png)

![](img/49104266c97f1f4f9793724450b9f7d0_365.png)

Was kind of frustratingÔºå but„ÄÇAnywaysÔºå okayÔºå so„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_367.png)

And then I had to modify this a little bit in the accuracy function„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_369.png)

In any caseÔºå long story shortÔºå it trained„ÄÇ It trained actually very well„ÄÇ

 I didn't expect it to train that well„ÄÇ I'm not sure if there's a buck„ÄÇ

 but it trained so good actually or so well that it got„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_371.png)

![](img/49104266c97f1f4f9793724450b9f7d0_372.png)

![](img/49104266c97f1f4f9793724450b9f7d0_373.png)

99% test accuracy„ÄÇ So essentiallyÔºå this should be just the same as this one„ÄÇ

 but more efficient because it puts„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_375.png)

Or organize it so that we minimize the pedding„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_377.png)

And it is also faster because of thatÔºå you can see if I go here for one epo takes only 0„ÄÇ33 minutes„ÄÇ

 whereas here„ÄÇ

![](img/49104266c97f1f4f9793724450b9f7d0_379.png)

![](img/49104266c97f1f4f9793724450b9f7d0_380.png)

It takes almost more than two times as much„ÄÇAnd yeah you can see it gets 99% test accuracy„ÄÇ

 I took a look at this„ÄÇ I can't find any mistake or bug„ÄÇ

 I think it's just a great model here that trains well„ÄÇ

 It could be that there's a bug somewhere because this is a bit suspiciously good„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_382.png)

![](img/49104266c97f1f4f9793724450b9f7d0_383.png)

![](img/49104266c97f1f4f9793724450b9f7d0_384.png)

But yeah well I take it 99% doesn't sound too bad So yeah„ÄÇ

 so here you have two templates and based on what I've seen actually I think this one looked really good so here you will have some additional resources if you are really interested in working with text but again this is a very complicated topic and I don't blame you if this looks all complicated to you so you are just learning about it probably for the first time„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_386.png)

![](img/49104266c97f1f4f9793724450b9f7d0_387.png)

People who work with natural language processing spend manyÔºå many„ÄÇYeah daysÔºå weeks„ÄÇ

 months learning these types of things„ÄÇ So here I gave you the overview„ÄÇ I hope it's it's useful„ÄÇ

 Next lectureÔºå we will now then finally get to the generative modelling part„ÄÇ

 So now I gave you the introduction to the general machine learning and deep learning concepts in the next lecture„ÄÇ

 we will take a look at deep learning models forÔºå yeahÔºå generating new dataÔºå auto encodes„ÄÇ

 variational auto encodes generative adversarial networks„ÄÇüòäÔºåAnd if we have time„ÄÇ

 maybe also transform us„ÄÇ All rightÔºå then see you on Thursday„ÄÇ



![](img/49104266c97f1f4f9793724450b9f7d0_389.png)