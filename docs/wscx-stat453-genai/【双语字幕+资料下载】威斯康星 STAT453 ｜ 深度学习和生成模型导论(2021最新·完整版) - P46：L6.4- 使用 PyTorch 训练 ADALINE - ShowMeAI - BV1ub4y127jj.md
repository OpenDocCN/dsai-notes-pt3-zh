# P46ï¼šL6.4- ä½¿ç”¨ PyTorch è®­ç»ƒ ADALINE - ShowMeAI - BV1ub4y127jj

Okayï¼Œ now in this videoï¼Œ let me explain how we train Analine using the concept of automatic differentiation and Pytorchã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_1.png)

So I prepared a code notebook here where I have three different implementations of Adeline first is the manual implementation that we saw last week then I have an implementation using the Gr function that I just explained in the previous video and then I show you an even more automatic way using the backward function that Pyrge automatically creates based on a forward function and that is also a topic I will then dive in more in the next video so here I want to just show you how it works and the next video I explain a little bit moreã€‚

Alrightï¼Œ so starting again with watermarkï¼Œ checking all versions hereã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_3.png)

I can also make this a little bit biggerã€‚ So here againã€‚

 this is our yeah adeline model that we talked about extensively last weekã€‚

 where we have multiple inputsï¼Œ the weightsï¼Œ the net input functionã€‚

 the activation function is just a yeah identity functionã€‚

 and then the threshold function for predictionã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_5.png)

![](img/13f8af674fef34195c068fc27c3da2eb_6.png)

Alrightï¼Œ let's import some libraries we will be using the gra function I explained in the previous videoã€‚

 and we will also make use of the functional APIã€‚ Againã€‚

 the functional API will be a more discussion topic in more detail in the next videoã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_8.png)

![](img/13f8af674fef34195c068fc27c3da2eb_9.png)

![](img/13f8af674fef34195c068fc27c3da2eb_10.png)

So we will be working with the IRS data again because it's simpleã€‚

 so we can then focus more on the code rather than on the dataã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_12.png)

So here we have no load of the data setã€‚ And this is exactly the same as we have done in the previous week weekã€‚

 So if you are unsure how this worksï¼Œ this is exactly the same code as last weekã€‚

 So you can go to the video from last weekã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_14.png)

![](img/13f8af674fef34195c068fc27c3da2eb_15.png)

![](img/13f8af674fef34195c068fc27c3da2eb_16.png)

So everything is explained in that video last weekã€‚ alright soã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_18.png)

Alsoï¼Œ just to recapï¼Œ this is exactly the same code that we were using last weekã€‚

 So here this is our adeline implementation where we first initialized the weights and the biasã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_20.png)

![](img/13f8af674fef34195c068fc27c3da2eb_21.png)

We have our forward function that computes the net inputsï¼Œ and then the activationã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_23.png)

And then we have the backwardwork function where we computed the gradient by ourselvesã€‚

 So what I mean by that is we derived it in the slidesï¼Œ we had slide on how we derived that gradientã€‚

 and then here this would be the equivalent code implementation of thatã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_25.png)

![](img/13f8af674fef34195c068fc27c3da2eb_26.png)

Yeahï¼Œ we have some training and evaluation wrappers just to make things a bit more convenient to look at so we have a loss function then we compute where we can plot the loss function during training So here in our training function again like in last week we have a cost list where we compute the cost which is actually yeah the loss over the EpoCã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_28.png)

![](img/13f8af674fef34195c068fc27c3da2eb_29.png)

![](img/13f8af674fef34195c068fc27c3da2eb_30.png)

![](img/13f8af674fef34195c068fc27c3da2eb_31.png)

![](img/13f8af674fef34195c068fc27c3da2eb_32.png)

Or mini batchã€‚ actuallyã€‚ So let me seeã€‚ So what we do is we iterate over the epochsã€‚

 So for each epochï¼Œ we shuffle the data setï¼Œ and then we create the mini batchesã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_34.png)

![](img/13f8af674fef34195c068fc27c3da2eb_35.png)

![](img/13f8af674fef34195c068fc27c3da2eb_36.png)

And for each mini batch hereï¼Œ we perform the forward pass that is like predicting yeahã€‚

 the class labelsã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_38.png)

And then we compute the backward passã€‚ So here this is computing the negative gradientsã€‚

 the negative gradient of the loss with respect to the weights and the biasã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_40.png)

And then we update the weightsã€‚ So we update byã€‚Yeahï¼Œ we have the original weightã€‚

 and we update it by adding the negative gradient multiplied with the learning rateã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_42.png)

Why for some reasonï¼Œ commented that outã€‚ Yeahï¼Œ I think it was just too much outputã€‚

 And I wanted to keep this notebook short because there will be other codesã€‚

 and it was too long other ways too detailedã€‚ We don't need that level of detail hereã€‚ Yeahã€‚

 we are just printingã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_44.png)

The number of epochs and the loss for each epochã€‚ So let'sã€‚Deefine itã€‚

 So note this is not running any code because it's just setting up the functions hereã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_46.png)

![](img/13f8af674fef34195c068fc27c3da2eb_47.png)

And then here we are defining or initializing the modelã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_49.png)

So x strain sizeã€‚ This is the sameã€‚ I could have used shapeã€‚ This is the sameï¼Œ same as shapeã€‚

 So it's the number of featuresã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_51.png)

![](img/13f8af674fef34195c068fc27c3da2eb_52.png)

It's the same asã€‚Speã€‚One the of featuresã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_54.png)

Then yeahï¼Œ here's our dataã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_56.png)

input the number of epoch 20 learning rate ã€‚01ï¼Œ a random seat so we can reproduce these resultsã€‚

 So that means if someone else like you runs this codeï¼Œ you should get get exactly the same resultsã€‚

And then the mini batch sizeï¼Œ how many mini batchs we use in each iterationã€‚

 Not the random seat here is only used for shuffling the data setï¼Œ rightï¼Œ If I go up againã€‚

 this is being used when we here shuffle the data setã€‚ So if you change the random seatã€‚

 you might get some different resultsã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_58.png)

![](img/13f8af674fef34195c068fc27c3da2eb_59.png)

![](img/13f8af674fef34195c068fc27c3da2eb_60.png)

![](img/13f8af674fef34195c068fc27c3da2eb_61.png)

Alrightï¼Œ so let's do thisã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_63.png)

So this is training and this is super fastã€‚ so you can see the outputs immediately hereã€‚

 Usually if for deep learningï¼Œ it would take maybe it depends reallyã€‚

 but it could take 10 minutes perpo and take an hour paypo depends on the dataã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_65.png)

So let's take a look at the lossã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_67.png)

Okayï¼Œ this isã€‚Convergingï¼Œ it's not converged yetï¼Œ but doesn't really matter because hereã€‚

 the point is really explaining the automatic gradient computation in the next codeã€‚I want toã€‚

 why I'm showing you this is so that you can compare the automatic way of Pytorch of doing this with this oneã€‚

 and you will see it's exactly the same resultã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_69.png)

So just to show you that our conceptual thing thing that we did manually last week is actually correctã€‚

 And well other way around that Pytharch is actually correctã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_71.png)

Alrightï¼Œ so here that's the predictionsã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_73.png)

For computing the test and training accuraciesã€‚ Alsoï¼Œ it's essentially the same concept as last weekã€‚

 So nothing new hereã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_75.png)

Noã€‚After we just reapped yeah the manual implementation of the endline that we talked about last weekã€‚

 now let's do this semi automatically or semi manually using this auto griddã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_77.png)

A PI from Pytorchã€‚ So there will be only very subtle changesã€‚

 So this one is exactly the same as beforeã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_79.png)

![](img/13f8af674fef34195c068fc27c3da2eb_80.png)

Nowï¼Œ the only difference is in the train methodã€‚ Soï¼Œ let me scrollã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_82.png)

To the relevant partã€‚ So notice everything here is the sameã€‚

 What has changed is now how we compute the gradients hereã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_84.png)

So now we use this gra function to compute the gradients of the lossã€‚

With respect to the model weightsã€‚And then we retain the graphã€‚

 remember from last video this is if we need againï¼Œ gradientsã€‚

 we need to retain it one more time so because we want to compute the bias here tooã€‚

 and then here we don't care because in the next round it will be constructed from scratch the graph so here we don't need to retain the graphã€‚

Why the -1ã€‚ So that is because we want to have the negative gradientï¼Œ because then weã€‚

 we add the negative gradient to the model weightã€‚ We could also just skip this stepã€‚ Of courseã€‚

 rightï¼Œ we can do it like this and addã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_86.png)

![](img/13f8af674fef34195c068fc27c3da2eb_87.png)

On minus here osã€‚Same thingã€‚But just to keep it consistent with the implementation that we had beforeã€‚

 our manual implementationã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_89.png)

I wanted to make it as similar as possibleã€‚Alrightï¼Œ soã€‚

The only difference to before is now that how we compute the gradients notice that there is no backward functionã€‚

 nowã€‚ If I scroll up againï¼Œ show it to you againã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_91.png)

![](img/13f8af674fef34195c068fc27c3da2eb_92.png)

In the previous timeï¼Œ we had backward here to compute the gradients where backward was ourã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_94.png)

Yeah manual way of computing the gradientsã€‚ Now we do it automaticallyã€‚ Seeã€‚

 this is actually because of the scrolling is why I commented on the logging because otherwise it would be a lot of stuff to scroll hereã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_96.png)

![](img/13f8af674fef34195c068fc27c3da2eb_97.png)

Alrightï¼Œ back to theã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_99.png)

Thing hereã€‚ So yeahï¼Œ here hereï¼Œ this is the difference instead of using backward with all manual gradientsã€‚

 we now use this Gr functionã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_101.png)

Except that everything should be the sameï¼Œ I made a small modification here to the loggingã€‚

Notice that I use with torch nograd because when we just do some logging hereã€‚

 we don't do any model training hereã€‚ we don't need to construct the graphã€‚

 it would be computationally wasteful to build the graph because otherwise it will create the graph in our forward method So here this one because we have set requires gradient into true if this is set to true every time this is usedã€‚

 it will create this computation graphã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_103.png)

![](img/13f8af674fef34195c068fc27c3da2eb_104.png)

![](img/13f8af674fef34195c068fc27c3da2eb_105.png)

Okayï¼Œ soã€‚Hereï¼Œ the computation graph gets destroyed because we don't have retain graph to trueã€‚

 So every time we do the fall loop hereï¼Œ it makes a new graphã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_107.png)

Here we don't call Gr right So here it would create a graphï¼Œ but we don't need this graphã€‚

 and it would be just computationally wastefulã€‚ It's just a good habit if we don't need a graph if we don't need to compute gradients here for loggingã€‚

 then we can use this with torch no gradientã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_109.png)

Context and everything that is indentedï¼Œ soã€‚Everything that is below hereã€‚

Does not construct a computation graphã€‚ It's just to save computational resourcesã€‚

 Here it's such a simple codeã€‚ It doesn't matterï¼Œ but it does matter for deeper neural networksã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_111.png)

Alrightï¼Œ so defining itã€‚ then againï¼Œ same as beforeï¼Œ we initialized the modelã€‚

 notice that thes no outline line 2 instead of a line 1ï¼Œ then yeahï¼Œ training the modelã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_113.png)

![](img/13f8af674fef34195c068fc27c3da2eb_114.png)

So we can see againï¼Œ the loss goes downï¼Œ upï¼Œ up upï¼Œ and let's take a look the plotã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_116.png)

Should look exactly like beforeã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_118.png)

Let's compute the training accuracy and test accuracyã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_120.png)

Alsoï¼Œ the same as beforeã€‚ you can actually double checkã€‚ These are exactly the same numbers asã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_122.png)

Once yearã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_124.png)

Alrightï¼Œ so this is now doing things more convenientï¼Œ rightï¼Œ So because you can think of itã€‚

 if I scroll up againï¼Œ I don't want to scroll that muchï¼Œ but I think it's useful here in this caseã€‚

 So if this forward one would be a very longï¼Œ complicated function using multiple layers and stuff like thatã€‚

 You can already see how it's convenient to not implement this backward method by hand by deriving that by handã€‚

 rightï¼Œ I meanï¼Œ it's a good exercise stillï¼Œ but it is also very error prone for deep neural networksã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_126.png)

![](img/13f8af674fef34195c068fc27c3da2eb_127.png)

![](img/13f8af674fef34195c068fc27c3da2eb_128.png)

![](img/13f8af674fef34195c068fc27c3da2eb_129.png)

So it's better to rely on these automatic functionsã€‚ Howeverã€‚

 there is an even more convenient way to do this that I want to show you nowã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_131.png)

So this is usually how peopleï¼Œ most people use Pythchã€‚

 so you can actually use this so calledled linear layer hereã€‚ So this is as I explained last weekã€‚

 this is computing the net inputã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_133.png)

![](img/13f8af674fef34195c068fc27c3da2eb_134.png)

![](img/13f8af674fef34195c068fc27c3da2eb_135.png)

I have nowï¼Œ an additional stepã€‚So here I have this0ã€‚So what's going on hereã€‚

Usually when we use torch do and n dot linearã€‚It's thinking we want to implement some multilay neural network because that's what most people do in deep learningã€‚

And then it will initialize the weights to small random numbersã€‚

 This would be also totally fine for our aline hereã€‚ Howeverã€‚

 to make our adeline co implementation here more comparable to the previous two codes I showed you before where we used0 weightsã€‚

 I also want to use0 weightsã€‚ So I'm settingã€‚These weights to 0 hereã€‚

 So just to show you what I meanï¼Œ that meanã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_137.png)

Just do thisã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_139.png)

Alrightï¼Œ let me just useã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_141.png)

Like thisã€‚ And thenã€‚Should haveï¼Œ of courseï¼Œ signed it to somethingã€‚

 You can see that these should be small random valuesã€‚ See thatã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_143.png)

![](img/13f8af674fef34195c068fc27c3da2eb_144.png)

And if I set them to 0ã€‚They will beã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_146.png)

Oopsã€‚Alrightï¼Œ this is the problemã€‚ you have to detach itã€‚ It doesn't like it if you haveã€‚

Variable definedã€‚ And you want to modify it like in placeã€‚Leaf variable means a leafã€‚

 It's like an end pointï¼Œ and it does ni it if you modify itã€‚With an inla operationã€‚

 because it's also error problemsã€‚ This is usually something you don't want to do in a networkã€‚

 So it's kind of warning youã€‚Soï¼Œ you have toã€‚Detach it from the graph hereã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_148.png)

There we goã€‚ So now I set it to 0ã€‚ Also notice I didn'tï¼Œ I didn't doã€‚This hereã€‚

 equal to because there's a convention in Pythã€‚ there are these so called in place operationsã€‚

 These with underscoreï¼Œ they do an operation in placeã€‚ So there's no return valueï¼Œ actuallyã€‚

It just takes the existing vector and overwrites itã€‚

 This is also done for computational efficiency because imagine you have a very large vectorã€‚

And then you want to override it with all zerosã€‚ You would have to memory briefly create two vectorsã€‚

 You have the original oneï¼Œ then the0 vector and the0 vector overrites the original oneã€‚

 So in a brief moment in timeï¼Œ you would have two vectors in memoryã€‚

 and it would take twice as much memoryã€‚ So if you do this with large matricesã€‚

 it can in certain GPUus be yeahï¼Œ problemã€‚ I meanï¼Œ it's just wastefulã€‚ So in this caseã€‚

 these underscore operationsã€‚Modify something in placeã€‚ But yeahï¼Œ I'm getting sidetric hereã€‚

 Let's go back to what's going on hereã€‚ So here I'm now defining the forward pass passã€‚ Oh sorryã€‚

 the weights used in the forward pass using this linearã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_150.png)

![](img/13f8af674fef34195c068fc27c3da2eb_151.png)

Rpperï¼Œ we talked about this briefly last weekï¼Œ and thenã€‚So I'm signing it to self dot linearã€‚

 And then I'm using it in the forward method hereã€‚ So here I'm computing the net inputs usingã€‚

Self dot linearã€‚Then I'm computing the activationsã€‚

 activations that's nothing and identity functionsã€‚ I'm just over writing itã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_153.png)

![](img/13f8af674fef34195c068fc27c3da2eb_154.png)

And then I'm returning itã€‚Alrightï¼Œ so here now I'm training it againã€‚ noticeã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_156.png)

The only things I define areã€‚This weight layer hereã€‚And this for what method hereã€‚

 I'm not defining anything elseã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_158.png)

In the train functionï¼Œ this is fundamentally very similar to beforeã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_160.png)

Except now see I'm computing the loss hereï¼Œ so I'm computing loss function I'm using here the MSE lossã€‚

 I could use my own lossOSï¼Œ but like I mentioned if there is a function that is already implemented in Pythtã€‚

 I recommend using that one over your own implementation because it's usually more efficientã€‚

They use some tricks under to also C plus plus code to implement things more efficientlyã€‚ So hereã€‚

We are using this M Elosã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_162.png)

Andã€‚Then we are resetting the gradients using zero gradã€‚And calling backwardã€‚

 the are multiple steps that are new now that are happening hereï¼Œ soã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_164.png)

Calling forward to predict the outputsï¼Œ the class labelsï¼Œ So compute outputsã€‚

 then it's actually the class labels is the net inputsã€‚Because it's before the threshold functionã€‚

 Let me scroll up one more timeã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_166.png)

So we are come we are here thisï¼Œ this value hereã€‚ we are computing this valueã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_168.png)

![](img/13f8af674fef34195c068fc27c3da2eb_169.png)

Alrightï¼Œ where was a So we're computing this value Y hatã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_171.png)

Ohï¼Œ this is the previous oneï¼Œ sorryã€‚Mmmã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_173.png)

Okayï¼Œ hereã€‚Forwardï¼Œ so we are computing this my head valueã€‚ Then we compute the predictionarrowã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_175.png)

Using the means Scott arrow hereã€‚Notice that Im resetting gradients from the previous iterationã€‚

 so this will be running multiple timesã€‚And this isã€‚

How Pythrarch works is got great attribute that will be set for these D variables after each iteration and we are resetting it otherwise you would be accumulating the gradients So usually it's not the case in deep learning we usually compute the gradientsã€‚

 update the weights then do the next round compute the gradientsï¼Œ update the way update the weightsã€‚

 but there are some applications where we want to for exampleã€‚

 not update the weights after each epochï¼Œ for exampleã€‚

 we can do two forward passes and then update the weights soã€‚This would be possibleã€‚ We couldã€‚

 for exampleï¼Œ skip0ing the gradientã€‚ weï¼Œ we couldï¼Œ technicallyï¼Œ for certain research experimentsã€‚

 accumulate the experimentsã€‚So this is why Pyrch has this implementation to allow certain researchers to do some more flexible researchã€‚

 but it also as a normal userï¼Œ forces us to remember to zero the gradientsã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_177.png)

![](img/13f8af674fef34195c068fc27c3da2eb_178.png)

So here the so we are also usingï¼Œ should say an optimizer stochastic gradient descent that is more automatically than what we have done beforeã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_180.png)

So predictionï¼Œ computing the lossï¼Œ zeroing the gradients from the previous roundã€‚

 calling backward that computes our gradientsã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_182.png)

And thenï¼Œ updating the weightsã€‚So this is usually a typical Pytch workflowã€‚ That is what people doã€‚

 usually in practice and what we do when we do implementing neural networksã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_184.png)

In the previous roundï¼Œ we had it manuallyã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_186.png)

Soï¼Œ we hadã€‚Computed forwardã€‚ And then we had our loss functionã€‚

 But then we computed here the negative gradientsã€‚ And we did stuff hereã€‚ the update automaticallyã€‚

 This is whatã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_188.png)

In our code information below is equal to optimize stepã€‚

So how does Opr know that we want to update the weight and the biasï¼Ÿ Wellã€‚

 that is because we feed it with the parameters I will show youã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_190.png)

Hereï¼Œ weã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_192.png)

We provided here see with model parametersã€‚ So there's also a conceptã€‚

 If you use these functions like Torchd linearï¼Œ these will be registered as model parameters in this module hereã€‚

 soã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_194.png)

![](img/13f8af674fef34195c068fc27c3da2eb_195.png)

Hereï¼Œ this will automatically contain the model parametersã€‚ Let me actually show it to youã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_197.png)

![](img/13f8af674fef34195c068fc27c3da2eb_198.png)

So here we haveï¼Œ where was it first scroll upï¼Œ we haven't actually defined it yetï¼Œ sorryã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_200.png)

So let me execute this part firstï¼Œ and then I will show you more detailsã€‚Alrightï¼Œ soã€‚

I already ran thisã€‚ so everything should I can actually also add it's fineã€‚ So here you can seeã€‚

 okayï¼Œ maybe can't because it's a generatorã€‚So you can see the addã€‚2 entriesã€‚

1 is actually the weightsï¼Œ and one is the biasã€‚ So they are registered under the parametersã€‚

 So these are really the values that we have as model dot F dot weightsã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_202.png)

Ohï¼Œ what was itã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_204.png)

How did we save it one second or linearï¼Œ we call it linearï¼Œ not Fã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_206.png)

![](img/13f8af674fef34195c068fc27c3da2eb_207.png)

Soï¼Œ you can seeã€‚These are corresponding to this oneã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_209.png)

And this one is corresponding to this finalã€‚So this is how the optimizer knows what to update when we call stepã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_211.png)

I can maybe also show you model linearã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_213.png)

Waitï¼Œ there should be a grã€‚So this is the gradientã€‚It's the gradient from training itã€‚

 So from the backward passï¼Œ and if we call backward an next roundã€‚

 it will add to this gradient it will growã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_215.png)

Soã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_217.png)

This oneã€‚Should actuallyã€‚Oopsï¼Œ make it 0ã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_219.png)

Since execute thisã€‚That doesn't workã€‚ Oh it'sï¼Œ it's because it's defined outside thisã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_221.png)

Whatite this function hereï¼Œ That's a bit unfortunateã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_223.png)

Yã€‚ðŸ˜”ï¼ŒWhat would be a tricky to show it to you hereã€‚ Maybe I canã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_225.png)

can do it differently I canã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_227.png)

Do it here beforeã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_229.png)

Afterã€‚ðŸ˜”ã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_231.png)

Alrightï¼Œ it's actually noneã€‚ Oh okayã€‚ Noï¼Œ it's something elseã€‚ It'sï¼Œ I think the first roundã€‚

 So yeahï¼Œ I can see firstï¼Œ it's this and then after 0ï¼Œ this and then it's this and thisã€‚

You can see how it's computedï¼Œ then0 to compute 0ã€‚ if I don't do this oneã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_233.png)

![](img/13f8af674fef34195c068fc27c3da2eb_234.png)

It will just grow larger and largerã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_236.png)

Can see that maybe not because it's a positive or negative very you can see how how large it becomesã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_238.png)

That's actually not goodã€‚So hereï¼Œ the model wouldn't learn anything usefulï¼Œ I guessã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_240.png)

Let's seeã€‚ Yeahï¼Œ you can see it's not you're learning anything usefulã€‚ So let's fix thatã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_242.png)

![](img/13f8af674fef34195c068fc27c3da2eb_243.png)

Alrightï¼Œ let's fix this hereã€‚And run it properlyã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_245.png)

![](img/13f8af674fef34195c068fc27c3da2eb_246.png)

Alrightã€‚And you can see this is the same as beforeã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_248.png)

When I compute the test and training accuracyï¼Œ look at these value values 92ã€‚86% and 93ã€‚33%ã€‚

 and this is exactly the same accuracy Let me screw up to Oã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_250.png)

![](img/13f8af674fef34195c068fc27c3da2eb_251.png)

Manual implementationï¼Œ exactlyã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_253.png)

The same number hereã€‚ So you can see Pytorch is performing exactly the same thing we do manuallyã€‚

 So our manual derivatives are correctã€‚ and vice versa Pytorch is also correctã€‚

 So I will talk about this more in the slides explaining this againã€‚

 But I think if this is still unclearã€‚ maybe focus on this partã€‚

 So this is really how we use Pytorch like forward computeute the loss0 the gradients backward updateã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_255.png)

![](img/13f8af674fef34195c068fc27c3da2eb_256.png)

![](img/13f8af674fef34195c068fc27c3da2eb_257.png)

![](img/13f8af674fef34195c068fc27c3da2eb_258.png)

![](img/13f8af674fef34195c068fc27c3da2eb_259.png)

And this is essentially a Pyth in a nutshellã€‚ And we can use this API for all types of modelsã€‚

 So the only difference is really here when we define theã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_261.png)

![](img/13f8af674fef34195c068fc27c3da2eb_262.png)

Weight parameters and the forward at passã€‚ So this is the only differenceã€‚

 The training loop is essentially always the sameã€‚

![](img/13f8af674fef34195c068fc27c3da2eb_264.png)

![](img/13f8af674fef34195c068fc27c3da2eb_265.png)

Alrightï¼Œ then let me stop this video and then go back into the slides and explain to you a little bit more about the Pythtorch APIã€‚



![](img/13f8af674fef34195c068fc27c3da2eb_267.png)