# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P46ÔºöL6.4- ‰ΩøÁî® PyTorch ËÆ≠ÁªÉ ADALINE - ShowMeAI - BV1ub4y127jj

OkayÔºå now in this videoÔºå let me explain how we train Analine using the concept of automatic differentiation and Pytorch„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_1.png)

So I prepared a code notebook here where I have three different implementations of Adeline first is the manual implementation that we saw last week then I have an implementation using the Gr function that I just explained in the previous video and then I show you an even more automatic way using the backward function that Pyrge automatically creates based on a forward function and that is also a topic I will then dive in more in the next video so here I want to just show you how it works and the next video I explain a little bit more„ÄÇ

AlrightÔºå so starting again with watermarkÔºå checking all versions here„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_3.png)

I can also make this a little bit bigger„ÄÇ So here again„ÄÇ

 this is our yeah adeline model that we talked about extensively last week„ÄÇ

 where we have multiple inputsÔºå the weightsÔºå the net input function„ÄÇ

 the activation function is just a yeah identity function„ÄÇ

 and then the threshold function for prediction„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_5.png)

![](img/13f8af674fef34195c068fc27c3da2eb_6.png)

AlrightÔºå let's import some libraries we will be using the gra function I explained in the previous video„ÄÇ

 and we will also make use of the functional API„ÄÇ Again„ÄÇ

 the functional API will be a more discussion topic in more detail in the next video„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_8.png)

![](img/13f8af674fef34195c068fc27c3da2eb_9.png)

![](img/13f8af674fef34195c068fc27c3da2eb_10.png)

So we will be working with the IRS data again because it's simple„ÄÇ

 so we can then focus more on the code rather than on the data„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_12.png)

So here we have no load of the data set„ÄÇ And this is exactly the same as we have done in the previous week week„ÄÇ

 So if you are unsure how this worksÔºå this is exactly the same code as last week„ÄÇ

 So you can go to the video from last week„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_14.png)

![](img/13f8af674fef34195c068fc27c3da2eb_15.png)

![](img/13f8af674fef34195c068fc27c3da2eb_16.png)

So everything is explained in that video last week„ÄÇ alright so„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_18.png)

AlsoÔºå just to recapÔºå this is exactly the same code that we were using last week„ÄÇ

 So here this is our adeline implementation where we first initialized the weights and the bias„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_20.png)

![](img/13f8af674fef34195c068fc27c3da2eb_21.png)

We have our forward function that computes the net inputsÔºå and then the activation„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_23.png)

And then we have the backwardwork function where we computed the gradient by ourselves„ÄÇ

 So what I mean by that is we derived it in the slidesÔºå we had slide on how we derived that gradient„ÄÇ

 and then here this would be the equivalent code implementation of that„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_25.png)

![](img/13f8af674fef34195c068fc27c3da2eb_26.png)

YeahÔºå we have some training and evaluation wrappers just to make things a bit more convenient to look at so we have a loss function then we compute where we can plot the loss function during training So here in our training function again like in last week we have a cost list where we compute the cost which is actually yeah the loss over the EpoC„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_28.png)

![](img/13f8af674fef34195c068fc27c3da2eb_29.png)

![](img/13f8af674fef34195c068fc27c3da2eb_30.png)

![](img/13f8af674fef34195c068fc27c3da2eb_31.png)

![](img/13f8af674fef34195c068fc27c3da2eb_32.png)

Or mini batch„ÄÇ actually„ÄÇ So let me see„ÄÇ So what we do is we iterate over the epochs„ÄÇ

 So for each epochÔºå we shuffle the data setÔºå and then we create the mini batches„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_34.png)

![](img/13f8af674fef34195c068fc27c3da2eb_35.png)

![](img/13f8af674fef34195c068fc27c3da2eb_36.png)

And for each mini batch hereÔºå we perform the forward pass that is like predicting yeah„ÄÇ

 the class labels„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_38.png)

And then we compute the backward pass„ÄÇ So here this is computing the negative gradients„ÄÇ

 the negative gradient of the loss with respect to the weights and the bias„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_40.png)

And then we update the weights„ÄÇ So we update by„ÄÇYeahÔºå we have the original weight„ÄÇ

 and we update it by adding the negative gradient multiplied with the learning rate„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_42.png)

Why for some reasonÔºå commented that out„ÄÇ YeahÔºå I think it was just too much output„ÄÇ

 And I wanted to keep this notebook short because there will be other codes„ÄÇ

 and it was too long other ways too detailed„ÄÇ We don't need that level of detail here„ÄÇ Yeah„ÄÇ

 we are just printing„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_44.png)

The number of epochs and the loss for each epoch„ÄÇ So let's„ÄÇDeefine it„ÄÇ

 So note this is not running any code because it's just setting up the functions here„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_46.png)

![](img/13f8af674fef34195c068fc27c3da2eb_47.png)

And then here we are defining or initializing the model„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_49.png)

So x strain size„ÄÇ This is the same„ÄÇ I could have used shape„ÄÇ This is the sameÔºå same as shape„ÄÇ

 So it's the number of features„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_51.png)

![](img/13f8af674fef34195c068fc27c3da2eb_52.png)

It's the same as„ÄÇSpe„ÄÇOne the of features„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_54.png)

Then yeahÔºå here's our data„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_56.png)

input the number of epoch 20 learning rate „ÄÇ01Ôºå a random seat so we can reproduce these results„ÄÇ

 So that means if someone else like you runs this codeÔºå you should get get exactly the same results„ÄÇ

And then the mini batch sizeÔºå how many mini batchs we use in each iteration„ÄÇ

 Not the random seat here is only used for shuffling the data setÔºå rightÔºå If I go up again„ÄÇ

 this is being used when we here shuffle the data set„ÄÇ So if you change the random seat„ÄÇ

 you might get some different results„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_58.png)

![](img/13f8af674fef34195c068fc27c3da2eb_59.png)

![](img/13f8af674fef34195c068fc27c3da2eb_60.png)

![](img/13f8af674fef34195c068fc27c3da2eb_61.png)

AlrightÔºå so let's do this„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_63.png)

So this is training and this is super fast„ÄÇ so you can see the outputs immediately here„ÄÇ

 Usually if for deep learningÔºå it would take maybe it depends really„ÄÇ

 but it could take 10 minutes perpo and take an hour paypo depends on the data„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_65.png)

So let's take a look at the loss„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_67.png)

OkayÔºå this is„ÄÇConvergingÔºå it's not converged yetÔºå but doesn't really matter because here„ÄÇ

 the point is really explaining the automatic gradient computation in the next code„ÄÇI want to„ÄÇ

 why I'm showing you this is so that you can compare the automatic way of Pytorch of doing this with this one„ÄÇ

 and you will see it's exactly the same result„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_69.png)

So just to show you that our conceptual thing thing that we did manually last week is actually correct„ÄÇ

 And well other way around that Pytharch is actually correct„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_71.png)

AlrightÔºå so here that's the predictions„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_73.png)

For computing the test and training accuracies„ÄÇ AlsoÔºå it's essentially the same concept as last week„ÄÇ

 So nothing new here„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_75.png)

No„ÄÇAfter we just reapped yeah the manual implementation of the endline that we talked about last week„ÄÇ

 now let's do this semi automatically or semi manually using this auto gridd„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_77.png)

A PI from Pytorch„ÄÇ So there will be only very subtle changes„ÄÇ

 So this one is exactly the same as before„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_79.png)

![](img/13f8af674fef34195c068fc27c3da2eb_80.png)

NowÔºå the only difference is in the train method„ÄÇ SoÔºå let me scroll„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_82.png)

To the relevant part„ÄÇ So notice everything here is the same„ÄÇ

 What has changed is now how we compute the gradients here„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_84.png)

So now we use this gra function to compute the gradients of the loss„ÄÇ

With respect to the model weights„ÄÇAnd then we retain the graph„ÄÇ

 remember from last video this is if we need againÔºå gradients„ÄÇ

 we need to retain it one more time so because we want to compute the bias here too„ÄÇ

 and then here we don't care because in the next round it will be constructed from scratch the graph so here we don't need to retain the graph„ÄÇ

Why the -1„ÄÇ So that is because we want to have the negative gradientÔºå because then we„ÄÇ

 we add the negative gradient to the model weight„ÄÇ We could also just skip this step„ÄÇ Of course„ÄÇ

 rightÔºå we can do it like this and add„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_86.png)

![](img/13f8af674fef34195c068fc27c3da2eb_87.png)

On minus here os„ÄÇSame thing„ÄÇBut just to keep it consistent with the implementation that we had before„ÄÇ

 our manual implementation„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_89.png)

I wanted to make it as similar as possible„ÄÇAlrightÔºå so„ÄÇ

The only difference to before is now that how we compute the gradients notice that there is no backward function„ÄÇ

 now„ÄÇ If I scroll up againÔºå show it to you again„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_91.png)

![](img/13f8af674fef34195c068fc27c3da2eb_92.png)

In the previous timeÔºå we had backward here to compute the gradients where backward was our„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_94.png)

Yeah manual way of computing the gradients„ÄÇ Now we do it automatically„ÄÇ See„ÄÇ

 this is actually because of the scrolling is why I commented on the logging because otherwise it would be a lot of stuff to scroll here„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_96.png)

![](img/13f8af674fef34195c068fc27c3da2eb_97.png)

AlrightÔºå back to the„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_99.png)

Thing here„ÄÇ So yeahÔºå here hereÔºå this is the difference instead of using backward with all manual gradients„ÄÇ

 we now use this Gr function„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_101.png)

Except that everything should be the sameÔºå I made a small modification here to the logging„ÄÇ

Notice that I use with torch nograd because when we just do some logging here„ÄÇ

 we don't do any model training here„ÄÇ we don't need to construct the graph„ÄÇ

 it would be computationally wasteful to build the graph because otherwise it will create the graph in our forward method So here this one because we have set requires gradient into true if this is set to true every time this is used„ÄÇ

 it will create this computation graph„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_103.png)

![](img/13f8af674fef34195c068fc27c3da2eb_104.png)

![](img/13f8af674fef34195c068fc27c3da2eb_105.png)

OkayÔºå so„ÄÇHereÔºå the computation graph gets destroyed because we don't have retain graph to true„ÄÇ

 So every time we do the fall loop hereÔºå it makes a new graph„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_107.png)

Here we don't call Gr right So here it would create a graphÔºå but we don't need this graph„ÄÇ

 and it would be just computationally wasteful„ÄÇ It's just a good habit if we don't need a graph if we don't need to compute gradients here for logging„ÄÇ

 then we can use this with torch no gradient„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_109.png)

Context and everything that is indentedÔºå so„ÄÇEverything that is below here„ÄÇ

Does not construct a computation graph„ÄÇ It's just to save computational resources„ÄÇ

 Here it's such a simple code„ÄÇ It doesn't matterÔºå but it does matter for deeper neural networks„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_111.png)

AlrightÔºå so defining it„ÄÇ then againÔºå same as beforeÔºå we initialized the model„ÄÇ

 notice that thes no outline line 2 instead of a line 1Ôºå then yeahÔºå training the model„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_113.png)

![](img/13f8af674fef34195c068fc27c3da2eb_114.png)

So we can see againÔºå the loss goes downÔºå upÔºå up upÔºå and let's take a look the plot„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_116.png)

Should look exactly like before„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_118.png)

Let's compute the training accuracy and test accuracy„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_120.png)

AlsoÔºå the same as before„ÄÇ you can actually double check„ÄÇ These are exactly the same numbers as„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_122.png)

Once year„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_124.png)

AlrightÔºå so this is now doing things more convenientÔºå rightÔºå So because you can think of it„ÄÇ

 if I scroll up againÔºå I don't want to scroll that muchÔºå but I think it's useful here in this case„ÄÇ

 So if this forward one would be a very longÔºå complicated function using multiple layers and stuff like that„ÄÇ

 You can already see how it's convenient to not implement this backward method by hand by deriving that by hand„ÄÇ

 rightÔºå I meanÔºå it's a good exercise stillÔºå but it is also very error prone for deep neural networks„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_126.png)

![](img/13f8af674fef34195c068fc27c3da2eb_127.png)

![](img/13f8af674fef34195c068fc27c3da2eb_128.png)

![](img/13f8af674fef34195c068fc27c3da2eb_129.png)

So it's better to rely on these automatic functions„ÄÇ However„ÄÇ

 there is an even more convenient way to do this that I want to show you now„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_131.png)

So this is usually how peopleÔºå most people use Pythch„ÄÇ

 so you can actually use this so calledled linear layer here„ÄÇ So this is as I explained last week„ÄÇ

 this is computing the net input„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_133.png)

![](img/13f8af674fef34195c068fc27c3da2eb_134.png)

![](img/13f8af674fef34195c068fc27c3da2eb_135.png)

I have nowÔºå an additional step„ÄÇSo here I have this0„ÄÇSo what's going on here„ÄÇ

Usually when we use torch do and n dot linear„ÄÇIt's thinking we want to implement some multilay neural network because that's what most people do in deep learning„ÄÇ

And then it will initialize the weights to small random numbers„ÄÇ

 This would be also totally fine for our aline here„ÄÇ However„ÄÇ

 to make our adeline co implementation here more comparable to the previous two codes I showed you before where we used0 weights„ÄÇ

 I also want to use0 weights„ÄÇ So I'm setting„ÄÇThese weights to 0 here„ÄÇ

 So just to show you what I meanÔºå that mean„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_137.png)

Just do this„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_139.png)

AlrightÔºå let me just use„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_141.png)

Like this„ÄÇ And then„ÄÇShould haveÔºå of courseÔºå signed it to something„ÄÇ

 You can see that these should be small random values„ÄÇ See that„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_143.png)

![](img/13f8af674fef34195c068fc27c3da2eb_144.png)

And if I set them to 0„ÄÇThey will be„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_146.png)

Oops„ÄÇAlrightÔºå this is the problem„ÄÇ you have to detach it„ÄÇ It doesn't like it if you have„ÄÇ

Variable defined„ÄÇ And you want to modify it like in place„ÄÇLeaf variable means a leaf„ÄÇ

 It's like an end pointÔºå and it does ni it if you modify it„ÄÇWith an inla operation„ÄÇ

 because it's also error problems„ÄÇ This is usually something you don't want to do in a network„ÄÇ

 So it's kind of warning you„ÄÇSoÔºå you have to„ÄÇDetach it from the graph here„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_148.png)

There we go„ÄÇ So now I set it to 0„ÄÇ Also notice I didn'tÔºå I didn't do„ÄÇThis here„ÄÇ

 equal to because there's a convention in Pyth„ÄÇ there are these so called in place operations„ÄÇ

 These with underscoreÔºå they do an operation in place„ÄÇ So there's no return valueÔºå actually„ÄÇ

It just takes the existing vector and overwrites it„ÄÇ

 This is also done for computational efficiency because imagine you have a very large vector„ÄÇ

And then you want to override it with all zeros„ÄÇ You would have to memory briefly create two vectors„ÄÇ

 You have the original oneÔºå then the0 vector and the0 vector overrites the original one„ÄÇ

 So in a brief moment in timeÔºå you would have two vectors in memory„ÄÇ

 and it would take twice as much memory„ÄÇ So if you do this with large matrices„ÄÇ

 it can in certain GPUus be yeahÔºå problem„ÄÇ I meanÔºå it's just wasteful„ÄÇ So in this case„ÄÇ

 these underscore operations„ÄÇModify something in place„ÄÇ But yeahÔºå I'm getting sidetric here„ÄÇ

 Let's go back to what's going on here„ÄÇ So here I'm now defining the forward pass pass„ÄÇ Oh sorry„ÄÇ

 the weights used in the forward pass using this linear„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_150.png)

![](img/13f8af674fef34195c068fc27c3da2eb_151.png)

RpperÔºå we talked about this briefly last weekÔºå and then„ÄÇSo I'm signing it to self dot linear„ÄÇ

 And then I'm using it in the forward method here„ÄÇ So here I'm computing the net inputs using„ÄÇ

Self dot linear„ÄÇThen I'm computing the activations„ÄÇ

 activations that's nothing and identity functions„ÄÇ I'm just over writing it„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_153.png)

![](img/13f8af674fef34195c068fc27c3da2eb_154.png)

And then I'm returning it„ÄÇAlrightÔºå so here now I'm training it again„ÄÇ notice„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_156.png)

The only things I define are„ÄÇThis weight layer here„ÄÇAnd this for what method here„ÄÇ

 I'm not defining anything else„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_158.png)

In the train functionÔºå this is fundamentally very similar to before„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_160.png)

Except now see I'm computing the loss hereÔºå so I'm computing loss function I'm using here the MSE loss„ÄÇ

 I could use my own lossOSÔºå but like I mentioned if there is a function that is already implemented in Pytht„ÄÇ

 I recommend using that one over your own implementation because it's usually more efficient„ÄÇ

They use some tricks under to also C plus plus code to implement things more efficiently„ÄÇ So here„ÄÇ

We are using this M Elos„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_162.png)

And„ÄÇThen we are resetting the gradients using zero grad„ÄÇAnd calling backward„ÄÇ

 the are multiple steps that are new now that are happening hereÔºå so„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_164.png)

Calling forward to predict the outputsÔºå the class labelsÔºå So compute outputs„ÄÇ

 then it's actually the class labels is the net inputs„ÄÇBecause it's before the threshold function„ÄÇ

 Let me scroll up one more time„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_166.png)

So we are come we are here thisÔºå this value here„ÄÇ we are computing this value„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_168.png)

![](img/13f8af674fef34195c068fc27c3da2eb_169.png)

AlrightÔºå where was a So we're computing this value Y hat„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_171.png)

OhÔºå this is the previous oneÔºå sorry„ÄÇMmm„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_173.png)

OkayÔºå here„ÄÇForwardÔºå so we are computing this my head value„ÄÇ Then we compute the predictionarrow„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_175.png)

Using the means Scott arrow here„ÄÇNotice that Im resetting gradients from the previous iteration„ÄÇ

 so this will be running multiple times„ÄÇAnd this is„ÄÇ

How Pythrarch works is got great attribute that will be set for these D variables after each iteration and we are resetting it otherwise you would be accumulating the gradients So usually it's not the case in deep learning we usually compute the gradients„ÄÇ

 update the weights then do the next round compute the gradientsÔºå update the way update the weights„ÄÇ

 but there are some applications where we want to for example„ÄÇ

 not update the weights after each epochÔºå for example„ÄÇ

 we can do two forward passes and then update the weights so„ÄÇThis would be possible„ÄÇ We could„ÄÇ

 for exampleÔºå skip0ing the gradient„ÄÇ weÔºå we couldÔºå technicallyÔºå for certain research experiments„ÄÇ

 accumulate the experiments„ÄÇSo this is why Pyrch has this implementation to allow certain researchers to do some more flexible research„ÄÇ

 but it also as a normal userÔºå forces us to remember to zero the gradients„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_177.png)

![](img/13f8af674fef34195c068fc27c3da2eb_178.png)

So here the so we are also usingÔºå should say an optimizer stochastic gradient descent that is more automatically than what we have done before„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_180.png)

So predictionÔºå computing the lossÔºå zeroing the gradients from the previous round„ÄÇ

 calling backward that computes our gradients„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_182.png)

And thenÔºå updating the weights„ÄÇSo this is usually a typical Pytch workflow„ÄÇ That is what people do„ÄÇ

 usually in practice and what we do when we do implementing neural networks„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_184.png)

In the previous roundÔºå we had it manually„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_186.png)

SoÔºå we had„ÄÇComputed forward„ÄÇ And then we had our loss function„ÄÇ

 But then we computed here the negative gradients„ÄÇ And we did stuff here„ÄÇ the update automatically„ÄÇ

 This is what„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_188.png)

In our code information below is equal to optimize step„ÄÇ

So how does Opr know that we want to update the weight and the biasÔºü Well„ÄÇ

 that is because we feed it with the parameters I will show you„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_190.png)

HereÔºå we„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_192.png)

We provided here see with model parameters„ÄÇ So there's also a concept„ÄÇ

 If you use these functions like Torchd linearÔºå these will be registered as model parameters in this module here„ÄÇ

 so„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_194.png)

![](img/13f8af674fef34195c068fc27c3da2eb_195.png)

HereÔºå this will automatically contain the model parameters„ÄÇ Let me actually show it to you„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_197.png)

![](img/13f8af674fef34195c068fc27c3da2eb_198.png)

So here we haveÔºå where was it first scroll upÔºå we haven't actually defined it yetÔºå sorry„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_200.png)

So let me execute this part firstÔºå and then I will show you more details„ÄÇAlrightÔºå so„ÄÇ

I already ran this„ÄÇ so everything should I can actually also add it's fine„ÄÇ So here you can see„ÄÇ

 okayÔºå maybe can't because it's a generator„ÄÇSo you can see the add„ÄÇ2 entries„ÄÇ

1 is actually the weightsÔºå and one is the bias„ÄÇ So they are registered under the parameters„ÄÇ

 So these are really the values that we have as model dot F dot weights„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_202.png)

OhÔºå what was it„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_204.png)

How did we save it one second or linearÔºå we call it linearÔºå not F„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_206.png)

![](img/13f8af674fef34195c068fc27c3da2eb_207.png)

SoÔºå you can see„ÄÇThese are corresponding to this one„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_209.png)

And this one is corresponding to this final„ÄÇSo this is how the optimizer knows what to update when we call step„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_211.png)

I can maybe also show you model linear„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_213.png)

WaitÔºå there should be a gr„ÄÇSo this is the gradient„ÄÇIt's the gradient from training it„ÄÇ

 So from the backward passÔºå and if we call backward an next round„ÄÇ

 it will add to this gradient it will grow„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_215.png)

So„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_217.png)

This one„ÄÇShould actually„ÄÇOopsÔºå make it 0„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_219.png)

Since execute this„ÄÇThat doesn't work„ÄÇ Oh it'sÔºå it's because it's defined outside this„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_221.png)

Whatite this function hereÔºå That's a bit unfortunate„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_223.png)

Y„ÄÇüòîÔºåWhat would be a tricky to show it to you here„ÄÇ Maybe I can„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_225.png)

can do it differently I can„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_227.png)

Do it here before„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_229.png)

After„ÄÇüòî„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_231.png)

AlrightÔºå it's actually none„ÄÇ Oh okay„ÄÇ NoÔºå it's something else„ÄÇ It'sÔºå I think the first round„ÄÇ

 So yeahÔºå I can see firstÔºå it's this and then after 0Ôºå this and then it's this and this„ÄÇ

You can see how it's computedÔºå then0 to compute 0„ÄÇ if I don't do this one„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_233.png)

![](img/13f8af674fef34195c068fc27c3da2eb_234.png)

It will just grow larger and larger„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_236.png)

Can see that maybe not because it's a positive or negative very you can see how how large it becomes„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_238.png)

That's actually not good„ÄÇSo hereÔºå the model wouldn't learn anything usefulÔºå I guess„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_240.png)

Let's see„ÄÇ YeahÔºå you can see it's not you're learning anything useful„ÄÇ So let's fix that„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_242.png)

![](img/13f8af674fef34195c068fc27c3da2eb_243.png)

AlrightÔºå let's fix this here„ÄÇAnd run it properly„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_245.png)

![](img/13f8af674fef34195c068fc27c3da2eb_246.png)

Alright„ÄÇAnd you can see this is the same as before„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_248.png)

When I compute the test and training accuracyÔºå look at these value values 92„ÄÇ86% and 93„ÄÇ33%„ÄÇ

 and this is exactly the same accuracy Let me screw up to O„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_250.png)

![](img/13f8af674fef34195c068fc27c3da2eb_251.png)

Manual implementationÔºå exactly„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_253.png)

The same number here„ÄÇ So you can see Pytorch is performing exactly the same thing we do manually„ÄÇ

 So our manual derivatives are correct„ÄÇ and vice versa Pytorch is also correct„ÄÇ

 So I will talk about this more in the slides explaining this again„ÄÇ

 But I think if this is still unclear„ÄÇ maybe focus on this part„ÄÇ

 So this is really how we use Pytorch like forward computeute the loss0 the gradients backward update„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_255.png)

![](img/13f8af674fef34195c068fc27c3da2eb_256.png)

![](img/13f8af674fef34195c068fc27c3da2eb_257.png)

![](img/13f8af674fef34195c068fc27c3da2eb_258.png)

![](img/13f8af674fef34195c068fc27c3da2eb_259.png)

And this is essentially a Pyth in a nutshell„ÄÇ And we can use this API for all types of models„ÄÇ

 So the only difference is really here when we define the„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_261.png)

![](img/13f8af674fef34195c068fc27c3da2eb_262.png)

Weight parameters and the forward at pass„ÄÇ So this is the only difference„ÄÇ

 The training loop is essentially always the same„ÄÇ

![](img/13f8af674fef34195c068fc27c3da2eb_264.png)

![](img/13f8af674fef34195c068fc27c3da2eb_265.png)

AlrightÔºå then let me stop this video and then go back into the slides and explain to you a little bit more about the Pythtorch API„ÄÇ



![](img/13f8af674fef34195c068fc27c3da2eb_267.png)