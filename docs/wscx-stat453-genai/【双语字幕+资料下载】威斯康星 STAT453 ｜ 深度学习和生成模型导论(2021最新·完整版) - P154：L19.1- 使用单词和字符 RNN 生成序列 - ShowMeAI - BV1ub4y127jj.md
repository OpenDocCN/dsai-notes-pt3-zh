# P154ÔºöL19.1- ‰ΩøÁî®ÂçïËØçÂíåÂ≠óÁ¨¶ RNN ÁîüÊàêÂ∫èÂàó - ShowMeAI - BV1ub4y127jj

Al rightÔºå so in this videoÔºå let me illustrate the main big picture concepts behind using RNs for sequence generation„ÄÇ

 In particularÔºå we will be briefly talking about many to many R ends for generating texts„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_1.png)

So recall this figure here that I showed you a couple of weeks ago when we talked about recurrent neural networks for text classification„ÄÇ

 So we had this slide on the different types of sequence modeling tasks„ÄÇ and previously we used„ÄÇ

There's many to one approach where we had text input„ÄÇ

 So many inputs and had one output sentiment label„ÄÇ

 whether the movie review was positive or negative„ÄÇ So it was essentially a classifier and„ÄÇ

The classifier worked on a word levelÔºå so we had an embedding vector that took each word as an input and was converting it into yeah into continuous vector„ÄÇ

TodayÔºå we are going to focus on„ÄÇThe manyÔºå too many architecture for generating texts„ÄÇ

So there's also a related many to many architecture here„ÄÇ

 this could be for instance used for language translation and we will talk a little bit more about language translation when we talk about R ends with attention„ÄÇ

 but in this video we are going to focus on the many too many approach first that is really like taking one input and producing one output at each time step„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_3.png)

So and when we talk about generating texts with recurrent neural networks„ÄÇ

 there are two main approaches„ÄÇ One is a so called character level R and NÔºå also just known as„ÄÇ

Character„ÄÇR and n enter word levelÔºå R and nÔºå which we also just call word„ÄÇR and N„ÄÇ

 And the difference is really what we„ÄÇProvide as token as one input token„ÄÇ

 So focusing on the left subfi here„ÄÇSo for let's say a word R and N„ÄÇ

 we could have one word would be the input„ÄÇ So for each time step hereÔºå the input would be„ÄÇ

A single wordÔºå for instanceÔºå let's say I„ÄÇLike„ÄÇHing„ÄÇAnd when we train this word R andN„ÄÇ

 what we want to do is we want to predict during training the next word so„ÄÇ

Let's say I like hiking a lot or something with a sentence that would continue like this„ÄÇHing a lot„ÄÇ

 So what we would do is in the first time stepÔºå we have this I is inputÔºå and it should predict„ÄÇLike„ÄÇ

 and then„ÄÇSecond input is likeÔºå it should predict„ÄÇOr output hikingÔºå And then„ÄÇI input is a hiking„ÄÇ

 and then it should„ÄÇProvide an AÔºå and then„ÄÇSo forth„ÄÇ so we would continue like that„ÄÇ

 so we provide one word as inputÔºå and then it should learn how to predict the next word„ÄÇ

 This is usually how a word level RN is trained„ÄÇFor the character level or an N„ÄÇ

It's trained slightly differently„ÄÇ but following the same conceptÔºå let me erase that„ÄÇ

 So instead of giving one word as an input at each positionÔºå we would just use one character„ÄÇ So„ÄÇ

 for exampleÔºå I„ÄÇÂóØ„ÄÇSpace„ÄÇLike an L and so forth„ÄÇ So in that wayÔºå it predicts one character at a time„ÄÇ

 So we would predict the next character„ÄÇ So IÔºå if I haveÔºå I like„ÄÇHiking„ÄÇ

 so I would be the first partÔºå and it predicts the white space„ÄÇHere„ÄÇ

 and then the next one would be the white spaceÔºå and then the L„ÄÇ

And then the L and the next letter would be the I and so forth so„ÄÇFundamentallyÔºå yeah„ÄÇ

 the differences between using a word and a or a character level are an n„ÄÇ at the end of this video„ÄÇ

 I have just a small comparison slideÔºå like listing the advantages and disadvantages of using„ÄÇ

Each of those two approaches„ÄÇ So let's focus for nowÔºå maybe on the character or an N„ÄÇ

 And I will tell you at the end of this„ÄÇVideo what the advantages are of using a character RnN and what the advantages are of using a word RnN„ÄÇ

But for simplicityÔºå we are focusing now on the character Arn andÔºå not on the word aren'n it„ÄÇ

So now assume we have this character R and N„ÄÇ This is for training„ÄÇ

 we predict the next word and if we have a sentence like that„ÄÇ

 we know the next label right because we have this letter and then we know all the following letters And that way it is it is a flavor of self-supervised learning where we generate our labels so we don't have to have labels for this training task because the labels are essentially the structure of the sentence right because we we know the next word and„ÄÇ

HereÔºå the prediction is essentially the next word„ÄÇI will have another slide that maybe makes us a little bit more clear„ÄÇ

 so for nowÔºå stay with me so what we are doing here is we are always providing the next ver as input during training or sorry the next character„ÄÇ

But„ÄÇFor testing„ÄÇ So how do we then generate a new text with thatÔºå rightÔºü

 So we want to generate a new text and not just regenerate an existing text„ÄÇ

 So how we do that is we provide a random letter as input„ÄÇThen it predicts the next letter„ÄÇ Usually„ÄÇ

 there is also some sampling involved„ÄÇ We are sampling with a certain probability„ÄÇ

 So we would consider all the top predicted letters„ÄÇ Let's say the„ÄÇwith a certain probability„ÄÇ

 and then we would randomly sample the weighted sampling of these predicted letters and then new type letter is the input for the next one„ÄÇ

And then againÔºå we predict characters and we„ÄÇTake one of themÔºå and provide„ÄÇAnd is input to the next„ÄÇ

Position and so forth„ÄÇ And with thatÔºå we can yeahÔºå predict different or generate different characters and sequences„ÄÇ

 And I think this will also become more clear when we take a look at the code example„ÄÇ

 So essentiallyÔºå the network learns„ÄÇThe probability that certain letters occur after each other„ÄÇ

 and if we train it like that„ÄÇIt will at some pointÔºå be able toÔºå yeahÔºå to generate„ÄÇ

Realistic text with a certain amount ofÔºå yeah spelling errors„ÄÇ But yeah„ÄÇ

 we will see about that in the code example to make this concept a bit more clear„ÄÇ

 I have a slightly different slide here„ÄÇ That is maybe a little bit moreÔºå I would sayÔºå concrete„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_5.png)

So this is a character R and N here again„ÄÇ And just assume it's processing„ÄÇThe textÔºå test„ÄÇ

So it's trained to predict the next character„ÄÇSo here I have the inputs„ÄÇFirst one is TÔºå then E„ÄÇ

 and then S„ÄÇAnd these letters are represented as a one hot encoding„ÄÇ So for the letter T„ÄÇ

The last position is a one for the letter EÔºå the first and for the letter S„ÄÇ

 it's the center position HereÔºå we have only a one hot encoding„ÄÇVectctor of three elements„ÄÇ

But the number of elements hereÔºå the size of this would be equal to the number of possible characters„ÄÇ

 SoÔºå for instanceÔºå if we only would consider all lowercase English„ÄÇAlphabet lettersÔºå it would be„ÄÇ

 let's sayÔºå24„ÄÇ if we consider lower and upper caseÔºå it would be 48 And then if we have punctuation„ÄÇ

 like period comma colonÔºå semicolonÔºå it would be 52 and so forth„ÄÇ So the size of the„ÄÇ

When including a vector here really depends on how many characters we consider„ÄÇAnd this size here„ÄÇ

Of this mono ending vector is equal to the size of the output here„ÄÇ So the output layer„ÄÇ

 this what's shown here is„ÄÇA vector of the predicted„ÄÇSoft mix„ÄÇProbabilities„ÄÇ

So these are the probabilitiesÔºå the predicted probabilities for each letter„ÄÇSo in this case„ÄÇ

 position 1 was the letter E rightÔºå So it has 70% probability that this is an E„ÄÇ

 20% that this is an S and 10% that this is a T„ÄÇ correct that has an E„ÄÇ So in this case„ÄÇ

 the network is making good prediction„ÄÇSo because the highest probability is the correct letter„ÄÇ

For the second oneÔºå so it receives an EÔºå it should predict an S„ÄÇ

So the probability that this is an E is 20%„ÄÇ The probability that this is an„ÄÇThis is an S is 60%„ÄÇ

 and the probability that this is in T is 20%„ÄÇ so also in this case„ÄÇ

The highest probability is for the is corresponding to the correct letter„ÄÇ So that's also good„ÄÇ

 And for the last oneÔºå yeahÔºå this is also good„ÄÇ So this isÔºå of courseÔºå just a toy example„ÄÇ

 but here essentially the network is trained to output a high probability for the correct word„ÄÇ

And then we can use the Archrcm„ÄÇTo convert this spec into a letter from going from this probability here to to the actual letter„ÄÇ

 we couldÔºå for exampleÔºå use an arcm„ÄÇSo it would give us the index index corresponding to the letter in let's say„ÄÇ

 a vector or dictionary„ÄÇA few more things so usually we use an embedding layer„ÄÇ

 we have talked about this in the RNN classification lecture we use usually an embedding layer to embed the inputs so there's usually a certain dimensionality of that embedding that comes out of it and then we also usually choose the size of the hidden layer and here it just happens that the hidden layer has three values but it's just a coincidence because it fit nicely here into this box but of course the hidden dimension is arbitrary it could be for example 64128„ÄÇ

200Ôºå211Ôºå whatever you like„ÄÇ it'sÔºå this is similar to the R and N for classification„ÄÇ So„ÄÇ

 so these parts are really the same that we have seen before when we talked about R and N classifiers„ÄÇ

The new part is really that the output here is„ÄÇSoftmax probability vector corresponding to the 100 codingding of the input„ÄÇ

 So essentially yeah we are trying to predict letters now rather than class labels„ÄÇ

 So the letters here are essentially our class labels and in that sense„ÄÇ

 since we are predicting something that is in in the data it's kind of like yeah kind of like selfsvised learning because we are generating our labels here ourselves by just„ÄÇ

 yeah using the inputs the text„ÄÇYeahÔºå and this isÔºå yeahÔºå theÔºå the broader concept„ÄÇ

 And like I mentioned before during the generation„ÄÇ

 So once we train the network and when we use it for inference for generating new textÔºå we sample„ÄÇ

We sample fromÔºå from this output vector here„ÄÇ So it's not guaranteed that we pick the letter E as input„ÄÇ

For the next„ÄÇToken„ÄÇLet us do after train doing testing because if we would guarantee that„ÄÇ

 then of course it would just memorize textÔºå but we want to usually have some variety in our text„ÄÇ

 so there's a certain probabilityÔºå so the probability is 70% to sample that E for example„ÄÇ

 whereas ass a smaller probability let's say 20% to also choose a different letter„ÄÇ

So we can also in practiceÔºå I meanÔºå there are different ways„ÄÇ

 you can also say only consider the top five letters or top three letters and sample them with a given probability and so forth„ÄÇ

 So there are different ways for doing that„ÄÇBut yeah during the generation of new text we usually don't just pick the one with the highest probability„ÄÇ

 we usually pick the one with the highest probability most often„ÄÇ

 but we also occasionally pick another letter and this will become hopefully more clear in the code example where we implement something like that„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_7.png)

YeahÔºå so here are just„ÄÇË¶ÅÂóØ„ÄÇFor referenceÔºå what I talked aboutÔºå howÔºå how we„ÄÇ

Work with this character R N„ÄÇ So during trainingÔºå what we do is usually we ignore the prediction„ÄÇ

 I meanÔºå the prediction is only used for computing the loss„ÄÇ

 but we don't we don't during training feed it into here during prediction we take„ÄÇ

This from the original input text„ÄÇ

![](img/b7ec30ba44477de018eb505c069c33a5_9.png)

And during„ÄÇTo inference stageÔºå we usually then use the prediction as the next input„ÄÇ

 This allows us then to create text with a variety that it's not always the same text„ÄÇ

Yeah I wanted to highlightÔºå of courseÔºå this works with both word and character level RNs„ÄÇ

 We talked about character level RNs when I„ÄÇ

![](img/b7ec30ba44477de018eb505c069c33a5_11.png)

Show you this„ÄÇ ButÔºå of courseÔºå this would also work with word level rNs„ÄÇ The differenceÔºå though„ÄÇ

 would be that the size here of this mono encoding would be the same size as the number of words that can occur„ÄÇ

 if you consider the English vocabularyÔºå there are millions of words„ÄÇ

 usually so you restrict that So that doesn't become too large„ÄÇ But even if you have a vocabulary„ÄÇ

 let's sayÔºå of 20000 wordsÔºå it would be way larger than the number of characters„ÄÇ

 which is one of the challenges of working with word r ns„ÄÇ

 So this wasn't a problem that much when we talked about the classifier„ÄÇ

 when we implemented the RN classifier„ÄÇCause yeahÔºå I meanÔºå for the em bedding„ÄÇ

 it still works fine if we have a 20000„ÄÇMeocabulary size„ÄÇ

 But the problem is really when we have these outputs here„ÄÇ

 like we compute the label over these 20000„ÄÇPossible words and compute the cross entropy between the two right that would be a little bit more challenging because these could be all very small values and there might be no large value among them and so forth„ÄÇ

 So there's there's a field of machine learning called energy based models which kind of borrows ideas from physics like how to make two vectors or how to assess whether or how similar to vectors are„ÄÇ

 but yeahÔºå this is a different topic for maybe a separate lecture„ÄÇ but„ÄÇ

The fact that in work level RnNs these vectors are very large„ÄÇ

 it's usually easier to train a character level RnN for generating new texts„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_13.png)

So let me summarize things here„ÄÇ So for character embeddings„ÄÇ

 we usually only have the 24 letters if we consider lower case only if we have up and lower cases different letters„ÄÇ

 we would of course have 48„ÄÇAnd in generalÔºå it requires less memory compared to word embeddings„ÄÇAnd„ÄÇ

And the losses also to have yesÔºå smaller output layers„ÄÇ

The disadvantage of using character RnNs over word RNNs is that it can more easily create weird nonsense words because each character is input so it can put together weird character combinations that don't correspond to real words whereas if you use word level RnN„ÄÇ

 I mean you can still have a weird word orderÔºå but each word is kind of fixed so that way you don't at least have a spellingarrows in the words„ÄÇ

And one other disadvantage of character level R and Ns is that they are worse at capturing long distance dependencies„ÄÇ

 That is simply because you haveÔºå or we have more tokensÔºå rightÔºü So if each character has a token„ÄÇ

 So if we have the sentenceÔºå I„ÄÇLike„ÄÇüòîÔºåHiking or something like that„ÄÇ So if I look at this„ÄÇ

 these are only two tokens apart„ÄÇ If I consider them as words„ÄÇ But if I consider each character„ÄÇ

 I have„ÄÇ1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6Ôºå7Ôºå8Ôºå8 sequence positions„ÄÇAre between these two so in that way it becomes more challenging to capture the relationship because this might get lost when we even if we have an LSDM„ÄÇ

 these types of long sequence relationships might get lost„ÄÇWe will talk about„ÄÇThe tension mechanism„ÄÇ

 which can help with longer sequencesÔºå but we will take a look at that in a word R and n context„ÄÇ

 For nowÔºå since it's the simpler case in the next video„ÄÇ

 I want to yeah implement a character R n in Pytorch because that's also easier to train than a word R and N and„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_15.png)

I will first show you maybe the concepts in the next video because there are also some„ÄÇ

 I would say technical details that are worth noting„ÄÇ

 so I will first show you the big picture concepts how to implement this using LSTMs and LSTM cells and then I will show you in another video the actual code implementation„ÄÇ



![](img/b7ec30ba44477de018eb505c069c33a5_17.png)