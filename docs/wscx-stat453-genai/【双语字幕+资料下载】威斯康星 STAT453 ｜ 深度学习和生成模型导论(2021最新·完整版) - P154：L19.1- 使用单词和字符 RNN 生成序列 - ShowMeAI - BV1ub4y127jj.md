# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P154ï¼šL19.1- ä½¿ç”¨å•è¯å’Œå­—ç¬¦ RNN ç”Ÿæˆåºåˆ— - ShowMeAI - BV1ub4y127jj

Al rightï¼Œ so in this videoï¼Œ let me illustrate the main big picture concepts behind using RNs for sequence generationã€‚

 In particularï¼Œ we will be briefly talking about many to many R ends for generating textsã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_1.png)

So recall this figure here that I showed you a couple of weeks ago when we talked about recurrent neural networks for text classificationã€‚

 So we had this slide on the different types of sequence modeling tasksã€‚ and previously we usedã€‚

There's many to one approach where we had text inputã€‚

 So many inputs and had one output sentiment labelã€‚

 whether the movie review was positive or negativeã€‚ So it was essentially a classifier andã€‚

The classifier worked on a word levelï¼Œ so we had an embedding vector that took each word as an input and was converting it into yeah into continuous vectorã€‚

Todayï¼Œ we are going to focus onã€‚The manyï¼Œ too many architecture for generating textsã€‚

So there's also a related many to many architecture hereã€‚

 this could be for instance used for language translation and we will talk a little bit more about language translation when we talk about R ends with attentionã€‚

 but in this video we are going to focus on the many too many approach first that is really like taking one input and producing one output at each time stepã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_3.png)

So and when we talk about generating texts with recurrent neural networksã€‚

 there are two main approachesã€‚ One is a so called character level R and Nï¼Œ also just known asã€‚

Characterã€‚R and n enter word levelï¼Œ R and nï¼Œ which we also just call wordã€‚R and Nã€‚

 And the difference is really what weã€‚Provide as token as one input tokenã€‚

 So focusing on the left subfi hereã€‚So for let's say a word R and Nã€‚

 we could have one word would be the inputã€‚ So for each time step hereï¼Œ the input would beã€‚

A single wordï¼Œ for instanceï¼Œ let's say Iã€‚Likeã€‚Hingã€‚And when we train this word R andNã€‚

 what we want to do is we want to predict during training the next word soã€‚

Let's say I like hiking a lot or something with a sentence that would continue like thisã€‚Hing a lotã€‚

 So what we would do is in the first time stepï¼Œ we have this I is inputï¼Œ and it should predictã€‚Likeã€‚

 and thenã€‚Second input is likeï¼Œ it should predictã€‚Or output hikingï¼Œ And thenã€‚I input is a hikingã€‚

 and then it shouldã€‚Provide an Aï¼Œ and thenã€‚So forthã€‚ so we would continue like thatã€‚

 so we provide one word as inputï¼Œ and then it should learn how to predict the next wordã€‚

 This is usually how a word level RN is trainedã€‚For the character level or an Nã€‚

It's trained slightly differentlyã€‚ but following the same conceptï¼Œ let me erase thatã€‚

 So instead of giving one word as an input at each positionï¼Œ we would just use one characterã€‚ Soã€‚

 for exampleï¼Œ Iã€‚å—¯ã€‚Spaceã€‚Like an L and so forthã€‚ So in that wayï¼Œ it predicts one character at a timeã€‚

 So we would predict the next characterã€‚ So Iï¼Œ if I haveï¼Œ I likeã€‚Hikingã€‚

 so I would be the first partï¼Œ and it predicts the white spaceã€‚Hereã€‚

 and then the next one would be the white spaceï¼Œ and then the Lã€‚

And then the L and the next letter would be the I and so forth soã€‚Fundamentallyï¼Œ yeahã€‚

 the differences between using a word and a or a character level are an nã€‚ at the end of this videoã€‚

 I have just a small comparison slideï¼Œ like listing the advantages and disadvantages of usingã€‚

Each of those two approachesã€‚ So let's focus for nowï¼Œ maybe on the character or an Nã€‚

 And I will tell you at the end of thisã€‚Video what the advantages are of using a character RnN and what the advantages are of using a word RnNã€‚

But for simplicityï¼Œ we are focusing now on the character Arn andï¼Œ not on the word aren'n itã€‚

So now assume we have this character R and Nã€‚ This is for trainingã€‚

 we predict the next word and if we have a sentence like thatã€‚

 we know the next label right because we have this letter and then we know all the following letters And that way it is it is a flavor of self-supervised learning where we generate our labels so we don't have to have labels for this training task because the labels are essentially the structure of the sentence right because we we know the next word andã€‚

Hereï¼Œ the prediction is essentially the next wordã€‚I will have another slide that maybe makes us a little bit more clearã€‚

 so for nowï¼Œ stay with me so what we are doing here is we are always providing the next ver as input during training or sorry the next characterã€‚

Butã€‚For testingã€‚ So how do we then generate a new text with thatï¼Œ rightï¼Ÿ

 So we want to generate a new text and not just regenerate an existing textã€‚

 So how we do that is we provide a random letter as inputã€‚Then it predicts the next letterã€‚ Usuallyã€‚

 there is also some sampling involvedã€‚ We are sampling with a certain probabilityã€‚

 So we would consider all the top predicted lettersã€‚ Let's say theã€‚with a certain probabilityã€‚

 and then we would randomly sample the weighted sampling of these predicted letters and then new type letter is the input for the next oneã€‚

And then againï¼Œ we predict characters and weã€‚Take one of themï¼Œ and provideã€‚And is input to the nextã€‚

Position and so forthã€‚ And with thatï¼Œ we can yeahï¼Œ predict different or generate different characters and sequencesã€‚

 And I think this will also become more clear when we take a look at the code exampleã€‚

 So essentiallyï¼Œ the network learnsã€‚The probability that certain letters occur after each otherã€‚

 and if we train it like thatã€‚It will at some pointï¼Œ be able toï¼Œ yeahï¼Œ to generateã€‚

Realistic text with a certain amount ofï¼Œ yeah spelling errorsã€‚ But yeahã€‚

 we will see about that in the code example to make this concept a bit more clearã€‚

 I have a slightly different slide hereã€‚ That is maybe a little bit moreï¼Œ I would sayï¼Œ concreteã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_5.png)

So this is a character R and N here againã€‚ And just assume it's processingã€‚The textï¼Œ testã€‚

So it's trained to predict the next characterã€‚So here I have the inputsã€‚First one is Tï¼Œ then Eã€‚

 and then Sã€‚And these letters are represented as a one hot encodingã€‚ So for the letter Tã€‚

The last position is a one for the letter Eï¼Œ the first and for the letter Sã€‚

 it's the center position Hereï¼Œ we have only a one hot encodingã€‚Vectctor of three elementsã€‚

But the number of elements hereï¼Œ the size of this would be equal to the number of possible charactersã€‚

 Soï¼Œ for instanceï¼Œ if we only would consider all lowercase Englishã€‚Alphabet lettersï¼Œ it would beã€‚

 let's sayï¼Œ24ã€‚ if we consider lower and upper caseï¼Œ it would be 48 And then if we have punctuationã€‚

 like period comma colonï¼Œ semicolonï¼Œ it would be 52 and so forthã€‚ So the size of theã€‚

When including a vector here really depends on how many characters we considerã€‚And this size hereã€‚

Of this mono ending vector is equal to the size of the output hereã€‚ So the output layerã€‚

 this what's shown here isã€‚A vector of the predictedã€‚Soft mixã€‚Probabilitiesã€‚

So these are the probabilitiesï¼Œ the predicted probabilities for each letterã€‚So in this caseã€‚

 position 1 was the letter E rightï¼Œ So it has 70% probability that this is an Eã€‚

 20% that this is an S and 10% that this is a Tã€‚ correct that has an Eã€‚ So in this caseã€‚

 the network is making good predictionã€‚So because the highest probability is the correct letterã€‚

For the second oneï¼Œ so it receives an Eï¼Œ it should predict an Sã€‚

So the probability that this is an E is 20%ã€‚ The probability that this is anã€‚This is an S is 60%ã€‚

 and the probability that this is in T is 20%ã€‚ so also in this caseã€‚

The highest probability is for the is corresponding to the correct letterã€‚ So that's also goodã€‚

 And for the last oneï¼Œ yeahï¼Œ this is also goodã€‚ So this isï¼Œ of courseï¼Œ just a toy exampleã€‚

 but here essentially the network is trained to output a high probability for the correct wordã€‚

And then we can use the Archrcmã€‚To convert this spec into a letter from going from this probability here to to the actual letterã€‚

 we couldï¼Œ for exampleï¼Œ use an arcmã€‚So it would give us the index index corresponding to the letter in let's sayã€‚

 a vector or dictionaryã€‚A few more things so usually we use an embedding layerã€‚

 we have talked about this in the RNN classification lecture we use usually an embedding layer to embed the inputs so there's usually a certain dimensionality of that embedding that comes out of it and then we also usually choose the size of the hidden layer and here it just happens that the hidden layer has three values but it's just a coincidence because it fit nicely here into this box but of course the hidden dimension is arbitrary it could be for example 64128ã€‚

200ï¼Œ211ï¼Œ whatever you likeã€‚ it'sï¼Œ this is similar to the R and N for classificationã€‚ Soã€‚

 so these parts are really the same that we have seen before when we talked about R and N classifiersã€‚

The new part is really that the output here isã€‚Softmax probability vector corresponding to the 100 codingding of the inputã€‚

 So essentially yeah we are trying to predict letters now rather than class labelsã€‚

 So the letters here are essentially our class labels and in that senseã€‚

 since we are predicting something that is in in the data it's kind of like yeah kind of like selfsvised learning because we are generating our labels here ourselves by justã€‚

 yeah using the inputs the textã€‚Yeahï¼Œ and this isï¼Œ yeahï¼Œ theï¼Œ the broader conceptã€‚

 And like I mentioned before during the generationã€‚

 So once we train the network and when we use it for inference for generating new textï¼Œ we sampleã€‚

We sample fromï¼Œ from this output vector hereã€‚ So it's not guaranteed that we pick the letter E as inputã€‚

For the nextã€‚Tokenã€‚Let us do after train doing testing because if we would guarantee thatã€‚

 then of course it would just memorize textï¼Œ but we want to usually have some variety in our textã€‚

 so there's a certain probabilityï¼Œ so the probability is 70% to sample that E for exampleã€‚

 whereas ass a smaller probability let's say 20% to also choose a different letterã€‚

So we can also in practiceï¼Œ I meanï¼Œ there are different waysã€‚

 you can also say only consider the top five letters or top three letters and sample them with a given probability and so forthã€‚

 So there are different ways for doing thatã€‚But yeah during the generation of new text we usually don't just pick the one with the highest probabilityã€‚

 we usually pick the one with the highest probability most oftenã€‚

 but we also occasionally pick another letter and this will become hopefully more clear in the code example where we implement something like thatã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_7.png)

Yeahï¼Œ so here are justã€‚è¦å—¯ã€‚For referenceï¼Œ what I talked aboutï¼Œ howï¼Œ how weã€‚

Work with this character R Nã€‚ So during trainingï¼Œ what we do is usually we ignore the predictionã€‚

 I meanï¼Œ the prediction is only used for computing the lossã€‚

 but we don't we don't during training feed it into here during prediction we takeã€‚

This from the original input textã€‚

![](img/b7ec30ba44477de018eb505c069c33a5_9.png)

And duringã€‚To inference stageï¼Œ we usually then use the prediction as the next inputã€‚

 This allows us then to create text with a variety that it's not always the same textã€‚

Yeah I wanted to highlightï¼Œ of courseï¼Œ this works with both word and character level RNsã€‚

 We talked about character level RNs when Iã€‚

![](img/b7ec30ba44477de018eb505c069c33a5_11.png)

Show you thisã€‚ Butï¼Œ of courseï¼Œ this would also work with word level rNsã€‚ The differenceï¼Œ thoughã€‚

 would be that the size here of this mono encoding would be the same size as the number of words that can occurã€‚

 if you consider the English vocabularyï¼Œ there are millions of wordsã€‚

 usually so you restrict that So that doesn't become too largeã€‚ But even if you have a vocabularyã€‚

 let's sayï¼Œ of 20000 wordsï¼Œ it would be way larger than the number of charactersã€‚

 which is one of the challenges of working with word r nsã€‚

 So this wasn't a problem that much when we talked about the classifierã€‚

 when we implemented the RN classifierã€‚Cause yeahï¼Œ I meanï¼Œ for the em beddingã€‚

 it still works fine if we have a 20000ã€‚Meocabulary sizeã€‚

 But the problem is really when we have these outputs hereã€‚

 like we compute the label over these 20000ã€‚Possible words and compute the cross entropy between the two right that would be a little bit more challenging because these could be all very small values and there might be no large value among them and so forthã€‚

 So there's there's a field of machine learning called energy based models which kind of borrows ideas from physics like how to make two vectors or how to assess whether or how similar to vectors areã€‚

 but yeahï¼Œ this is a different topic for maybe a separate lectureã€‚ butã€‚

The fact that in work level RnNs these vectors are very largeã€‚

 it's usually easier to train a character level RnN for generating new textsã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_13.png)

So let me summarize things hereã€‚ So for character embeddingsã€‚

 we usually only have the 24 letters if we consider lower case only if we have up and lower cases different lettersã€‚

 we would of course have 48ã€‚And in generalï¼Œ it requires less memory compared to word embeddingsã€‚Andã€‚

And the losses also to have yesï¼Œ smaller output layersã€‚

The disadvantage of using character RnNs over word RNNs is that it can more easily create weird nonsense words because each character is input so it can put together weird character combinations that don't correspond to real words whereas if you use word level RnNã€‚

 I mean you can still have a weird word orderï¼Œ but each word is kind of fixed so that way you don't at least have a spellingarrows in the wordsã€‚

And one other disadvantage of character level R and Ns is that they are worse at capturing long distance dependenciesã€‚

 That is simply because you haveï¼Œ or we have more tokensï¼Œ rightï¼Ÿ So if each character has a tokenã€‚

 So if we have the sentenceï¼Œ Iã€‚Likeã€‚ğŸ˜”ï¼ŒHiking or something like thatã€‚ So if I look at thisã€‚

 these are only two tokens apartã€‚ If I consider them as wordsã€‚ But if I consider each characterã€‚

 I haveã€‚1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ï¼Œ7ï¼Œ8ï¼Œ8 sequence positionsã€‚Are between these two so in that way it becomes more challenging to capture the relationship because this might get lost when we even if we have an LSDMã€‚

 these types of long sequence relationships might get lostã€‚We will talk aboutã€‚The tension mechanismã€‚

 which can help with longer sequencesï¼Œ but we will take a look at that in a word R and n contextã€‚

 For nowï¼Œ since it's the simpler case in the next videoã€‚

 I want to yeah implement a character R n in Pytorch because that's also easier to train than a word R and N andã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_15.png)

I will first show you maybe the concepts in the next video because there are also someã€‚

 I would say technical details that are worth notingã€‚

 so I will first show you the big picture concepts how to implement this using LSTMs and LSTM cells and then I will show you in another video the actual code implementationã€‚



![](img/b7ec30ba44477de018eb505c069c33a5_17.png)