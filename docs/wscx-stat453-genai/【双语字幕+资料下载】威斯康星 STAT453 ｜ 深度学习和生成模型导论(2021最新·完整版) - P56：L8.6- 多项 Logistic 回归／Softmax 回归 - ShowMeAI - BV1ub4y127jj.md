# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P56ï¼šL8.6- å¤šé¡¹ Logistic å›å½’ï¼Softmax å›å½’ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ let's now generalize the concept of logistic regression to multiple classesã€‚

 This is also known as softmax regression or sometimes also called multiomial logistic regressionã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_1.png)

So for this we will consider a relatively simple data setï¼Œ the M list data setã€‚

 I already introduced this in the introductory lecturesï¼Œ so these are 60ã€‚

00 hundredwritten digits in the training setã€‚And yeah it's a nice and balanced data setã€‚

 so balanced means that there's the same number of training examples per classã€‚

 so in this case we have 10 classesï¼Œ the handwritten digits0 to9 and there are 6000 digits per classã€‚

 Here's an example of how these digits look likeã€‚And each of these training examples hereã€‚

Has an image dimension of 28 times 28 times 1ã€‚ So 28 for the height and 28 for the widthã€‚

 So this is the number of pixelsã€‚ So we have in totalï¼Œ784 pixels or features per training exampleã€‚

In Pytchï¼Œ we will be using a format so called N C HWã€‚

 which is yeah how we represent a mini batch or an image batchã€‚ So hereã€‚In this caseã€‚

 we have 128 examples in that mini batchï¼Œ so the 128 is for this n hereã€‚

Then we have a colour channelã€‚ We have only one colour channel because these are black and white imagesã€‚

 So there are no redï¼Œ green and blue coloursã€‚And thenï¼Œ we haveã€‚The heightï¼Œ so 28 pixelsã€‚

And for the widthã€‚We have also 28 pixelsã€‚ This is usually how we read in a data set into Pytorchã€‚

 and we will be also looking at code examples for thatã€‚So yeahï¼Œ just to recapã€‚

 we have 60000 training examplesã€‚And then the corresponding labelsï¼Œ and then we haveã€‚

For the test images and test labels 10000 eachã€‚ so that's just how the dataset set looks likeã€‚Yeahã€‚

 for softm regressionï¼Œ we will actually be concatenatingã€‚These imagesã€‚

Or these features in the image to a long vectorï¼Œ like we discussed in the introductory lectureã€‚

 So yeahï¼Œ here's just an example of howã€‚

![](img/7bddc4e3a5fe43463115092b77a6bea5_3.png)

Kind of diverseã€‚ These handwritings areã€‚ So these are allã€‚The letter 7 sorry the number 7ã€‚

 So illustrations of different sevensã€‚ So different people writing down a 7ã€‚

 and you can see it varies a lotã€‚ So personally where I grew up in Germanyã€‚

 we usually write a 7 like this with this cross line here in the USã€‚

 it's more common to write it like thisï¼Œ but you can seeã€‚

They are also very interesting flavors of thisï¼Œ for exampleï¼Œ looks like a C almost hereã€‚

 this one is actually interesting because yeah in Germanyï¼Œ we write a one as follows like thisã€‚

 This is a one in Germany in the US one is usually written as this one So but you can see just by looking at these handwritingsã€‚

 it's sometimes maybe not even clear what the number isã€‚

 So if I wouldn't tell you that these are different illustrations of7ã€‚

 Some people might think these might be onceã€‚ and yeahã€‚

 this actually I don't even know it could be a C if you have data set of letters in digitsã€‚

 for exampleã€‚

![](img/7bddc4e3a5fe43463115092b77a6bea5_5.png)

So yeahï¼Œ like I mentioned in the context of Pythtor and when we work with unstructured dataã€‚

 also later with convolutional networksï¼Œ we would be working with the formatï¼Œ the NCHWã€‚As followsã€‚

 So this would beã€‚4 dimensional tensorã€‚Think I introduced that in the first lecture of this courseã€‚

 so a long time agoã€‚ So yeahï¼Œ here's how it would look like in Pytorrchã€‚

 where we have this four dimensional tensorã€‚å—¯ã€‚Actuallyï¼Œ in this caseï¼Œ its a three dimensional tenã€‚

 so I only represent one single imageã€‚That's a single image here at the bottomã€‚

Representing the number threeï¼Œ and you can see there are different values from 0 to 0ã€‚5ã€‚

 I think the highest one would be approaching oneã€‚ and that is because I normalizedã€‚

These pixels by dividing them by 255ã€‚ And this is becauseï¼Œ yeahã€‚

 these values between or around 0 or in this caseï¼Œ between 0 and1 work better with gradient in decentã€‚

 it would be even betterï¼Œ for exampleï¼Œ to normalize these images such that they are centered at 0 so you canã€‚

 for exampleï¼Œ yeahï¼Œ subtractor 0ã€‚5ã€‚ If you have the range between 0 and1ï¼Œ you can subtractor 0ã€‚

5 and then will be ranging betweenã€‚inusã€‚52ã€‚5And this would work even betterã€‚

 what people say that would work better with gradient in descent because now it's centered at0 and you can have positive and negative inputs equallyã€‚

 Howeverï¼Œ in practiceï¼Œ personallyï¼Œ I for these simple dataset setsï¼Œ I don't notice a big differenceã€‚

 especially if we work with logistic regressionã€‚ Sometimes I put more effort into normalizing the input imagesã€‚

 for exampleï¼Œ doing the standard normal standardization theã€‚What people call the st normalã€‚Dataã€‚

 So it's giving your data the properties of a standard normal distributionã€‚ Butï¼Œ of courseã€‚

 if your data is not normally distributed from begin withã€‚

 it will also not be normally distributed after normalizingã€‚ So what I mean isã€‚For exampleã€‚

 if you haveï¼Œ it's called that X primeï¼Œ if you have a given featureã€‚Xã€‚You subjecttractã€‚

The mean of the feature and then divided by the standard deviationã€‚

 So this would be called standardizationã€‚Well some people just call that ZCO normalizationã€‚å—¯ã€‚

This is also recommendedï¼Œ we can do that here tooï¼Œ but since the images of the featuresã€‚

 sorry the features are all equalï¼Œ they are all from 0 to 25 255ã€‚For pixelsã€‚

 it wouldn't make a big difference withã€‚We use yeahã€‚

 this standardization or just scale them between the range minus 0ã€‚5 and ã€‚5ã€‚

 So that's like a minor detailã€‚

![](img/7bddc4e3a5fe43463115092b77a6bea5_7.png)

Alrightï¼Œ so but I am getting sidetracked here what I wanted to mention is the main point here is what we are doing now is we are concatenatingã€‚

 So if I go back we are concatenating the rows soã€‚

![](img/7bddc4e3a5fe43463115092b77a6bea5_9.png)

If you considerã€‚Fortunatelyï¼Œ there's a line wrapping here if you consider this here as the first rowã€‚

Then I can maybe write this down first allã€‚Osã€‚Firstã€‚ğŸ˜”ï¼ŒRuã€‚ğŸ˜”ï¼ŒThenã€‚ğŸ˜”ï¼ŒSecondã€‚Ruã€‚ğŸ˜”ï¼ŒThirdã€‚ğŸ˜”ï¼ŒRã€‚ğŸ˜”ã€‚

And so forthï¼Œ if we concatenate these rowsï¼Œ we get a one long vector hereã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_11.png)

That's how it looks likeã€‚ So here I've concateated these rowsã€‚ just to illustrate this againã€‚

 Let me jump back one more timeã€‚ So here in greenï¼Œ this would be really the first row on in this image here in blueã€‚

 this would beã€‚

![](img/7bddc4e3a5fe43463115092b77a6bea5_13.png)

Second row and the third row here in red would be this oneã€‚ if we concatenate them by thatï¼Œ I meanã€‚

 if we make one long vectorã€‚ So the green one firstï¼Œ thenã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_15.png)

The blue 1ã€‚And then the red oneï¼Œ we can use that as input for our logistic regression modelã€‚

 And in the case of the Ms data setï¼Œ we have more than two classesã€‚

 So we are going to use softmax regressionã€‚ Yeahï¼Œ to just briefly recap on before I introduce the softmax model here is a drawing of the regular logistic regression model for the binary classification caseã€‚

 So as inputï¼Œ we have a feature vectorã€‚And then we have this weight vectorã€‚

 So we compute the dot product between the two at the bias unitï¼Œ which gives us the net inputã€‚

 and then we put the net input through a non nonlinear activation functionï¼Œ the logisticã€‚

Sick might functionã€‚Which we defined as followsã€‚Andã€‚Then if so what it represents is the probabilityã€‚

So this present represents the probability that a training exampleã€‚Belongs toã€‚Clasã€‚Oneã€‚

 if we wanted to compute the probability that it belongs to class 0ï¼Œ we canï¼Œ yeahï¼Œ we couldã€‚

We could compute it asã€‚Oopsã€‚1 minus thisï¼Œ rightã€‚1 minus this probabilityã€‚ This would give usã€‚

The probability that it belongs toã€‚Class 0ã€‚But back to this one here nowã€‚

 So we have a probability andã€‚It belongs to class1ã€‚

 and then we can say if this probability is greaterã€‚F P is greater than ã€‚5 than classã€‚oneã€‚Andã€‚

If it's smallerï¼Œ thenã€‚Pot5ã€‚Classã€‚0ã€‚So how can we generalize this now to multiple classes so for that we can actually extend this modelã€‚

 so here is a drawing of a model that would work with a multiclass classification data where we have more than two classes so just to jump back one more timeã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_17.png)

![](img/7bddc4e3a5fe43463115092b77a6bea5_18.png)

So this model here that I'm circling in greenï¼Œ this can actually be thought of as a model that is embedded in this bigger model hereã€‚

 So if we have the same inputs hereï¼Œ it's the same as beforeã€‚

 our feature vector of dimensionality T Mã€‚

![](img/7bddc4e3a5fe43463115092b77a6bea5_20.png)

We can think ofã€‚Everything I'm drawing now in greenã€‚Ass the same model that I showed you beforeã€‚

 rightï¼Œ So if you look at thisï¼Œ it's the same modelã€‚ I meanã€‚

 it's of course a little bit skewed because these lines are a little bit differentã€‚

 but it's essentially the same modelã€‚ And so this would be one logistic regression modelã€‚

 And for that oneï¼Œ we have a feature vectorã€‚Of dimensionality Mã€‚ so we can have the feature vectorã€‚

 Let's write it oneã€‚1ã€‚Oopsã€‚it like thisï¼Œ1ï¼Œ1ã€‚1ï¼Œ2ï¼Œ andã€‚Weã€‚ğŸ˜”ï¼ŒLet's say three if we have three classesã€‚

Soï¼Œ if we have three featuresï¼Œ let's do M to a bit more generalã€‚ So if we have M featuresã€‚

 we have this feature vector hereã€‚And noticeï¼Œ noï¼Œ I meanï¼Œ we haveã€‚

Multiple of these logistic regression modelsã€‚ The second model would be this one hereï¼Œ rightã€‚

 is's another logistic regression modelï¼Œ same inputsã€‚And for this oneã€‚

 we can also have a feature vector for this oneï¼Œ we could have the feature vectorã€‚2ï¼Œ1ã€‚2ï¼Œ2 and2 mã€‚

And then there's a third model hereï¼Œ rightï¼ŸSo it's a third modelã€‚And for thatã€‚

 we also have a feature vectorã€‚ we can write it as on W3ï¼Œ1ï¼Œ W23ï¼Œ2 and W 3ï¼Œ3ã€‚So if I call thisã€‚

Feature vectorï¼Œ let's say let's call it W1ã€‚And this isã€‚W2ã€‚Andã€‚ğŸ˜”ï¼ŒThis isã€‚W 3ã€‚Rightã€‚

 so I have now three feature vectorsã€‚ So if Iã€‚Stack themã€‚ I can stack them to make a feature matrixã€‚

 So this matrix hereã€‚ so I can write this matrix asã€‚I think yeahï¼Œ I would need to transpose thisã€‚ğŸ˜”ã€‚

Soã€‚We would have W1ã€‚Transposeã€‚W 2ã€‚Tpose andã€‚view3 transportã€‚

So this would be my feature matrix of dimensionity H times Mã€‚

 where H is the number of output nodes hereã€‚So this would be one way I could generalize my logistic regression modelã€‚

Then we can yeah compute also the class membership probabilities hereã€‚

 However notice now the class membership probabilities work a little bit differently so because we don't have a binary classification problem nowã€‚

 but each node is still binaryã€‚ So here let's say greenã€‚

 this would be the probability that this is class 1ã€‚In blueã€‚

 this is that probability that is class 2ã€‚But now each probability works like thisã€‚

That it is the probability that this is the classã€‚ So the probability that is class 3 versus not being class 3 soã€‚

What can happen is that or what will probably likely happen is that these probabilities don't sum up to oneã€‚

 Soï¼Œ for exampleï¼Œ for a given inputï¼Œ if we have the digit 3ï¼Œ we may get aã€‚Probability that this isã€‚

Pot3ï¼Œ wellï¼Œ let's sayã€‚33%ã€‚But this is in class  oneã€‚That's aã€‚40%ã€‚That this is class 2ï¼Œ and thenã€‚

70% that this is class 3ã€‚Soã€‚But yeahï¼Œ these don't sum up to oneã€‚ Soï¼Œ for exampleã€‚

 if the probability is 40% as is class 2ï¼Œ the probability that this is not class2 would be 60%ã€‚

And for this oneï¼Œ for the first oneï¼Œ the probability that it would be classã€‚3 would be 67%ã€‚

In any caseï¼Œ these probabilities down some up to oneã€‚ if we also take a look at this oneã€‚

 these down some up to oneï¼Œ it can be a little bit weirdã€‚ So if we have a numberã€‚

 let's say the digitã€‚3ï¼Œ how can it be 33%ï¼ŸNumber oneï¼Œ40%ï¼Œ number two and 70% probability number3ã€‚

 it doesn't reallyã€‚Make intuitive senseã€‚I thinkã€‚This would make sense if we have a data set when the classes are not mutually exclusiveã€‚

 for exampleï¼Œ if you have something like a vehicleã€‚

 there's a higher probability that an object in an image is a vehicle there's maybe also probability greater than 50% that this is a car and then maybe a probabilityã€‚

 a certain probability that iss a certain brand of a car and so forthã€‚

 So where we don't have mutually exclusive classesã€‚

 but in the case of digits it would be more intuitive if these probabilitybil sum up to one and for that we have this multinoial logistic regression model or softmax regression modelã€‚

 So what I showed you in the previous case was more like a concatenation of individual logistic regression models here would be the real multinoial logistic regression model where we assumeã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_22.png)

That the classes are mutually exclusiveã€‚ So what is the difference between this slide and the previous slideã€‚

 So if I go back one more time in the previous slideã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_24.png)

It's hard to see nowï¼Œ but we had one sigmoid activation in each node hereã€‚

 So I'm getting rid now of this sigmoid activationã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_26.png)

And have a so calledã€‚Soft max activationã€‚ And also notice that I'm drawing it a little bit differently before I was drawing it inside here because they were all independent from each other nowã€‚

The softmax activation is actually not independent for each nodeã€‚

 So all the activation go into the same softmax activationã€‚And thenã€‚Theres some computation going onã€‚

 I will explain to you how that works in the next video and what comes out are theseã€‚

Activations that now sum up to oneã€‚ So these sum up to oneã€‚ Soï¼Œ for exampleã€‚

 the first one may be 10% probability that it belongs to class 1ã€‚Then that might be 20%ã€‚

For class 2 andã€‚70% for class 3 and so forthã€‚And how can we then convert it into a class tableã€‚

 We would take the arc mixã€‚ So the arc mix isã€‚The positionï¼Œ the index position of the highest valueã€‚

 If I scan these three positionsï¼Œ let's say positionã€‚1ï¼Œ position 2ã€‚And position 3ï¼Œ the highest indexã€‚

70%ã€‚ This would be 3ã€‚ So this would return class A 3 if I apply the Archsã€‚

If this is unclear we will walk through this step by step in the next video so this was just the introduction of how the softmax regression model looks like like it's basically here the concatetnation of individual logistic regression models and in the next videos we will talk about softmax and this arcmax and how we also compute the loss functionã€‚



![](img/7bddc4e3a5fe43463115092b77a6bea5_28.png)