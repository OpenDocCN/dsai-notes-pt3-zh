# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P110ÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞Èóª #9Ôºå2021 Âπ¥ 3 Êúà 27 Êó• - ShowMeAI - BV1ub4y127jj

YeahÔºå hiÔºå everyone„ÄÇ I hope you are doing well and are well prepared for the exam on Tuesday„ÄÇ

 I just want to clarify againÔºå these stuff in the news videos are not topics of the exam„ÄÇ

 so you can just watch them and entirely forget about them if you want„ÄÇ

 And you don't have to watch them at allÔºå of course„ÄÇ So todayÔºå because we have an exam on Tuesday„ÄÇ

 and I have another talk on Wednesday„ÄÇ I will try to keep it rather short„ÄÇ

 I will mainly introduce a new selfsvised learning technique I found„ÄÇüòä„ÄÇ

Then there is something about some of the challenges in deep learning related to causal relationships that I wanted to share with you„ÄÇ

 And there are also a few tools I found really cool„ÄÇ for instanceÔºå a Pytoch profiler„ÄÇ All right„ÄÇ

 So with thatÔºå let us get started„ÄÇYeah so the first cool thing I wanted to share with you is a cool podcast I discovered this week so I'm not listening to as many podcasts anymore as I once did because sometimes I think it's good to give your brain a break„ÄÇ

 especially like if you work and study a lot and it's good sometimes just to maybe do sometimes nothing and give your brain some rest„ÄÇ

 but if you're doing something boring let's say commuting or household chores like cleaning up at home then yeah sometimes it might be nice to make this a little bit more interesting by listening to some yeah interesting podcast„ÄÇ

So this podcast is about machine learningÔºå just came out this week„ÄÇ

 it's by Peter Abiil who is yeah a very well-known researcher in deep learning and its a style of interview podcasts and there's only one episode out„ÄÇ

 but in this first episode he was interviewing Andre CappatiÔºå who is the director of AI at Tesla„ÄÇ

And they talked aboutÔºå yeah„ÄÇMainly machine learning and deep learning and how it is like to work at Tesla„ÄÇ

 So I found this very interesting because they were also talking about yeah how deep learning is used in practice„ÄÇ

 so at Tesla they for instance use deep learning for their autopilot„ÄÇ

 which is some sort of semi automatic self-driving or semi semi self-driving car capability so whether car can drive automatically on highways but they also have recently extended this version to a beta version that can drive also„ÄÇ

In yeahÔºå non highway situations„ÄÇ And they are mainly using cameras for that„ÄÇ

 like regular cameras mounted on the car„ÄÇ and they are using under the hoodÔºå for instance„ÄÇ

 convolutional neural networks„ÄÇ alsoÔºå and one of the interesting takeaways from that interview was that„ÄÇ

Andria Patty mentioned that„ÄÇFor instanceÔºå it's a better bang for the buck„ÄÇ

 If you want to improve your model if you collect more data and you are„ÄÇ

 let's say labeling your data„ÄÇ So paying attention how you are label your data and also what type of data you collect that might improve your model more or the performance more then let's say fine tuning or trying out a different model„ÄÇ

 So sometimes„ÄÇJust focusing more on the data side can also give you reallyÔºå really good results„ÄÇ

 It is like a very common pattern I've seen in industry„ÄÇ For instanceÔºå I recall also Andrewng„ÄÇ

 who is now he was a professor at StanfordÔºå who also was the cofounder of Cora and had these very popular machine learning classes that I also took and really enjoyed online„ÄÇ

 So he also has a startup company„ÄÇ I think it's beyond startup„ÄÇ NowÔºå It's a relatively big company„ÄÇ

 focused on working with industry partners and„ÄÇüòäÔºåAlso from things he talks about„ÄÇ

 I gathered that yeah a focus is also really on the data„ÄÇ

 it's very important to get good quality data and if you want to improve your models yet just focusing on the data is is really important compared to just tuning the model„ÄÇ

 which is very different from academiaÔºå for instanceÔºå in academia„ÄÇ

 we usually work with benchmark datas because usually when you look at paper or papersÔºå it's like„ÄÇ

About improving on a certain benchmark dataset like AmsÔºå Cypher 10 Inet and many others„ÄÇ

 So you are using the same data set and just want to see which model performs better„ÄÇ

 But once you leave the academic situation and you go into industry and you want to develop real products„ÄÇ

 it's then more about yeah also collecting additional data and not just tuning the model but really yeah collecting more and better„ÄÇ

 more informative data to improve the performance of your application„ÄÇOkay„ÄÇ

 this was like a long tangent„ÄÇ Just wanted mention„ÄÇ this was actually a pretty cool podcast I liked„ÄÇ

 So if you' are interestedÔºå yeahÔºå feel free to check that out„ÄÇüòä„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_1.png)

YeahÔºå so this week I also discovered a new self-supvised learning method I found particularly interesting„ÄÇ

 So if you recall self-supervised learning is essentially about leveraging the feature information or the structure of the data for a supervised learning task usually that's done by creating some label information from the data so you can apply selfsvised learning to un data and then train in the supervised fashion by creating labels here this is a little bit more focused on I would say the structure of the data So theres no I would say explicit label information created here it's more about yeah the structure so this method is called Ba twins and the paper is titled self-supvised learning by a redundancy reduction„ÄÇ

So how that works is that essentially they run a network twice„ÄÇ So focusing on this figure here„ÄÇ

 let's say this part is one networkÔºå and then they create a copy of that network so„ÄÇ

Let's say the lower partÔºå it looks the same as the upper part except the input is different„ÄÇ

 so but here it's really just an identical copy of the network„ÄÇ

 it's the same as running the same network twice and this setup in general is sometimes also called Siamese„ÄÇ

NetworkÔºå it's also popular in other contextsÔºå for exampleÔºå traditional face recognition and so forth„ÄÇ

So in any caseÔºå so what the goal here is essentially is to learn feature representations that ignore small modifications of that image„ÄÇ

 for instanceÔºå changes in the brightness or slight color perturbations or slight rotations of that image„ÄÇ

 So what they do is they„ÄÇProvide and„ÄÇLet's say this is the original„ÄÇThe original image„ÄÇ

And let's say this is the distorted version of that image where youÔºå for example„ÄÇ

 make everything a little bit brighter you change the brightness„ÄÇ

 then you run this through a convolutional network„ÄÇ

 so you run both of them through a convolutional networkÔºå the same convolution network„ÄÇ

 and it will create a feature representation„ÄÇAnd then„ÄÇ

You have the feature representation of the distorted one up here and the one of the original one here„ÄÇ

 it could also be to„ÄÇDifferent types of distorted imagesÔºå for instance„ÄÇ

 this could be slightly rotated to the left„ÄÇ and this one could be slightly rotated to the right„ÄÇ

 For instanceÔºå the the point is here really that you have two images that are similar„ÄÇAnd„ÄÇ

Then you get these feature representations here at the end and you compute the cross correlation matrix and you then try to make the cross correlation matrix similar to an identity matrix where you have the ones in the diagonal„ÄÇ

 And so with that you are trying to learn essentially on„ÄÇYeahÔºå you're trying to learn„ÄÇ

That these feature representation vectors should be similar to each other„ÄÇ

 so it should ignore the network should ignore essentially the small perturbations in that image„ÄÇ

 So yeahÔºå hereÔºå I summarize it essentially that they run the original and the distorted image through the same network„ÄÇ

 computer correlation matrix„ÄÇ and then they have an objective function or loss function that forces this correlation matrix to be close to an identity matrix„ÄÇ

YeahÔºå and that is essentially then forcing the representation vectors of similar examples to be similar„ÄÇ

So here is the codeÔºå the Pythtor style pseudocode for that„ÄÇ

 I found it actually nice that they included that in the paper„ÄÇ

 so here that makes it yeah easy to implement so„ÄÇHere they produce two augmented version of the image„ÄÇ

Then compute the representation vector where F would be then the neural network„ÄÇAnd then„ÄÇ

 they compute„ÄÇOr they normalizeÔºå and then they compute the correlation cross correlation between the two„ÄÇ

 and then they compute the difference between the cross correlation and then identity matrix„ÄÇ

 They apply square„ÄÇ So they square the results because yeah to„ÄÇ

So it doesn't matter whether it's positive or negative„ÄÇ

 which directions it doesn't matter which which way it isÔºå rightÔºüAnd then„ÄÇThey multiply it by lambda„ÄÇ

 the difference the of diagonals„ÄÇ So I guess it's a hyperparmeter like a scaling how how much penalty you would assign„ÄÇ

 I'm not sure if that actually is really necessary because to some extent„ÄÇ

 I would think that the learning rate already takes care of that by scaling the gradients„ÄÇ

 but I guess it's a little bit more control over the penalty here„ÄÇ and then they sum up„ÄÇ

The oftagon notess here„ÄÇAnd essentiallyÔºå you want to minimize the difference between those and„ÄÇ

The identity matrix„ÄÇAnd yeahÔºå this is then the loss that they optimize„ÄÇ And that's all they do„ÄÇ

 There is no class table information„ÄÇ and they train that method„ÄÇ and then„ÄÇ

When they obtain these feature representation vectorsÔºå they train a linear model„ÄÇ

 so they say linear evaluationÔºå usually that's something like logistic regression„ÄÇ

 some simple linear generalized linear modelÔºå and when they apply then the linear evaluation or let's say logistic regression trained on the feature vectors„ÄÇ

They get a pretty good accuracy hereÔºå which is shown here„ÄÇ So let me maybe draw that„ÄÇ

 So if you have your input imageÔºå let's call that X„ÄÇ it goes through the neural network„ÄÇ

 the convolutional layers„ÄÇ And then there's this feature representation vector„ÄÇ and they„ÄÇ

 let's say produce all the feature representation vectors for the whole image net„ÄÇ So you have these„ÄÇ

 you treat them as fixed„ÄÇ So you have„ÄÇFor the whole imagenetÔºå these feature representation vectors„ÄÇ

 and they are your are„ÄÇExs yourÔºå let's„ÄÇThe x the feature input X for the logistic regression model„ÄÇ

 So it's„ÄÇYou wanted it here„ÄÇSo it's called it x 1 for the first training example„ÄÇ

 And let's say x 3 for the third training exampleÔºå X 2 for the second training example„ÄÇ

 But then you also use„ÄÇThe original class label„ÄÇFrom Imnet„ÄÇAnd this will be„ÄÇ

This whole thing will be your training setÔºå your new training set for Lo„ÄÇRegression„ÄÇ

It's essentially a traditional supervised classification problem„ÄÇ So whyÔºå why would you do that„ÄÇ

 This is reallyÔºå yeahÔºå it's really testing how much information about the images„ÄÇ

Is captured in these vectors„ÄÇ It's a way of evaluating the con network feature extraction in that sense„ÄÇ

 So here the hypothesis is essentially„ÄÇBy forcing„ÄÇThe network to ignore these distortions„ÄÇ

 you can produce essentially representation vectors that are really representative of the image and then the linear classifier can classify them well and you can see„ÄÇ

This method when you train it in that manner with a logistic regression on these feature representations gets a 73„ÄÇ

2% top 1 accuracy on INe and 91% top5 accuracy and there are also some other methods for comparison this is not the best methods so notice there are two methods here at least that are better but it's very competitive it's performing really well here and what I like about it is it's pretty simple if you look at this it's a pretty simple training method and yeah is sometimes also good it's actually pretty cool I think„ÄÇ

YeahÔºå so one thing that is great in our day and age is that we have all these technologies now that make communication online easier„ÄÇ

 but yeah one unfortunate downside of that is it's also making cyber bullying easier and other abusive things on the internet„ÄÇ

 So it's actually nice to see that theres now like workshop focusing on issues like that to improve the identification of hateful memes„ÄÇ

 for instanceÔºå So I think this workshop was organized by Facebook I'm not sure anymore by Facebook AI research I think but I would have to double check„ÄÇ

So here are a few examples of that and essentially this is a competition where everyone can participate and develop models and it's a workshop where you're then invited to also yeah„ÄÇ

Write a paper about your model if it's performing well„ÄÇ

 I wish I would have known about that earlier because then I think we could have made this a class project it would have been cool„ÄÇ

 but yeah it's it was just announced last week„ÄÇ it's a little bit late in the semester„ÄÇ

 but if someone is interested in working on that as aI project this would be an interesting application„ÄÇ

 for instance also of convolutional networks„ÄÇSo one thing that is new about this is it's a multila problem I mean not new in the general sense„ÄÇ

 but in the context of our class we haven't worked with multilabel problems„ÄÇ

 but it's essentially pretty straightforward so„ÄÇLet's say you have your output layer„ÄÇ

Of a neural networkÔºå convol network or multi layer perceptionceptron„ÄÇ

 usually what we used was a soft mix„ÄÇFunction such that these outputs„ÄÇ

That they sum up to one to a probability of one„ÄÇ So we have a each„ÄÇ

 each note is a class membership probabilityÔºå rightÔºå So the first one is a„ÄÇLet's say for class 0„ÄÇ

 given the inputÔºå this is for class1Ôºå given the input XÔºå and this one is for„ÄÇClass 2„ÄÇ

 given an input XÔºå so usually„ÄÇWe assume these were exclusiveÔºå mutually exclusive classes„ÄÇ

 for exampleÔºå in MNTÔºå each digit can only be one class right so it makes sense if we have three membership probabilities for let's say digit 1„ÄÇ

 digit 2 and digit 3 that they sum up to1„ÄÇSo if for instance„ÄÇ

 if you have 90% probability for label 1Ôºå then it's maybe 5% for label 2 and 5% for label 3„ÄÇ

 but yeah in a multila problem it's not a requirement so„ÄÇA data point can have multiple classes„ÄÇ

 for instance„ÄÇHere for each memeÔºå one task is to predict„ÄÇThe protected categoryÔºå for instanceÔºå race„ÄÇ

 disabilityÔºå religionÔºå nationality and sex„ÄÇ So a meme couldÔºå for instance„ÄÇ

 target multiple things at once„ÄÇ So here you are not constrained of having probability summing up to one„ÄÇ

 So for instanceÔºå you can say for class1Ôºå it could beÔºå for instanceÔºå a 90% probability„ÄÇ

 but it could also have a 95% percent probability that it has in additionÔºå also label number two„ÄÇ

 so it can have multiple labels„ÄÇHow do you achieve thatÔºü

 So the only thing you really have to do is you just change softm by the logistic„ÄÇSigmoid function„ÄÇ

 So in that wayÔºå you don't constrain the network to have these probabilities summing up to one„ÄÇ

 but essentiallyÔºå yeahÔºå this would be a very similar„ÄÇApproach„ÄÇ

 you can use conversion networks for that„ÄÇ So yeahÔºå if someone is interested here„ÄÇ

 the important datesÔºå Here's a link to this workshop„ÄÇ I just found that interesting„ÄÇ I wishÔºå yeah„ÄÇ

 we would have known about that earlier because then it would have been a cool class project„ÄÇ

 But nowÔºå yeahÔºå I don't want toÔºå you spend already so much time working on it„ÄÇ I don't want to„ÄÇ

Proposed this as the new class projectÔºå but if someone is interested„ÄÇ

 that would be an interesting site project„ÄÇ

![](img/152bfaa7984ce3e5ad1f90b55f160a3e_3.png)

Yeah so there was an interesting paper recently towards causal representation layer learning it's not a new paper I mean it's relatively new but it's from last month but I have a huge backlog of stuff in the news items that I discover and haven't had a chance to discuss yet„ÄÇ

SoÔºå yeahÔºå so essentiallyÔºå this is about pushing deep learning more towards yeah causal representation learning„ÄÇ

 So one shortcoming„ÄÇNowadays is that deep learning or the in general„ÄÇ

 the current state of supervised learning and yeah predictive modeling is more reliant or is essentially reliant on„ÄÇ

ID data that means independent and identically distributed data where one data point doesn't really influence the other and deep learning systems like classifiers„ÄÇ

 typical classifiers are essentially just learning statistical correlations between the input data and the output data„ÄÇ

 so you're essentially not learning a system that can really understand the relationship between the data in a causal way it's more like exploiting correlations„ÄÇ

So for instanceÔºå why would it be useful to learn causal relationshipsÔºü

One aspect that is also kind of mentioned in that paper is it can make models more robust towards unexpected situations„ÄÇ

 for instance„ÄÇIf you have„ÄÇLet's say a self driving car„ÄÇ And remember when we had„ÄÇ

 I showed you that before this adversarial attack where they had a laser beam in or in front of a street sign„ÄÇ

 and it was fooling the classifier into thinking that the street sign means something differently„ÄÇ

 If there would be a better causal understanding that might be avoided„ÄÇSo in that way„ÄÇ

 I think certain adversarial attacks could be mitigated if the network had a better understanding of causal or relationships„ÄÇ

 but also it can make training cheaperÔºå for instance„ÄÇ

 let's say you train a classifier to detect objects and one of the classes is let's say predicting whether something is a chair to sit on„ÄÇ

So usually if you want to make this really robustÔºå you would have to take pictures of that same chair and include it into the training data set from different angles„ÄÇ

 a human though„ÄÇA human doesn't really need to see the same chair from different angles to recognize that this is a chair„ÄÇ

 So that wayÔºå if the network has a better understanding„ÄÇ

 maybe what the what what makes a chair a chairÔºå then in that way„ÄÇ

 we would maybe require fewer training examples„ÄÇ And this would also help with„ÄÇ

Reurpurposing models so that you don't have to train the model from scratch on each new dataset sets and that way it could be more effective maybe to train a model on one dataset and then apply it to another„ÄÇ

 for instanceÔºå if you think of reinforcement learning also in a grana scheme where you let's say learn an agent„ÄÇ

To play age of empires„ÄÇ It's a strategy game„ÄÇ Then maybe this agent could be applied also to play Starcraft„ÄÇ

 which is another strategy game withoutÔºå let's say„ÄÇ

 requiring learning from scratch because the games are relatively similar„ÄÇ So in that way„ÄÇ

 it could also help with transfer learning and repurposing models„ÄÇ But yeahÔºå it'sÔºå it's a challenge„ÄÇ

 It's something where people don't have a solution for yet„ÄÇ

 So this paper was mainly highlighting the challenges and proposing some„ÄÇPotential future directions„ÄÇ

 but yeahÔºå the main challenges essentially are„ÄÇWhether the data even reveals causal relationships and also then how can we infer these abstract causal variables„ÄÇ

 so I currently I think there are no solutions yet„ÄÇ

 but people yeah started to think about it more actively and I think that is like an interesting area of research to keep an eye on in the future„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_5.png)

AlrightÔºå let's wrap it up with some of the cool tools I discovered this week„ÄÇ

 So one of them is classify AI with the rather clever name AI„ÄÇ AnywaysÔºå So here„ÄÇ

 this tool is essentially a image and nottta„ÄÇ So there are some other tools that exist„ÄÇ

 I shared some of them with you in the context of the class projects before„ÄÇ But yeah„ÄÇ

 this is a new1Ôºå I thinkÔºå and it looked pretty cool„ÄÇ It's an open source tool„ÄÇ and„ÄÇüòäÔºåYeah„ÄÇ

 so it is providing capabilities for annotating images convenientlyÔºå for instance„ÄÇ

 also for object detection„ÄÇ So at the bottom here this is a video I hope it plays yeah so it's just an example that shows you how convenient it is to label different objects„ÄÇ

 So if you are interested in that check it out„ÄÇ this is free„ÄÇ It's an open source tool„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_7.png)

Another thing I wanted to talk about is yeah making your code more efficient so this week I actually spent quite some time making some of my code more efficient because training was relatively slow and I had hundreds or thousands of lines of code and I was just finding or trying to find the bottleneck Why is the training so slow Eventually I found out my mistake„ÄÇ

Or the inefficiencyÔºå is's kind of summarized here„ÄÇ So I was computing three thingsÔºå every epoch„ÄÇ

I was or actually six thingsÔºå I was computing the validation set mean absolute error mean square error and the validation set accuracy„ÄÇ

 and then I was computing the same mean absolute error mean square error„ÄÇ

And accuracy for the training set„ÄÇSo I did that each epoch and each time the way I implemented these functions was by iterating over the dataset because they are too large to load into memory„ÄÇ

 so I was just iterating over the dataset the same way you would iterate over the batches when you do the training and I did that every epoch„ÄÇ

Now that was very inefficient because it takes a lot of time if you have like half a million images in your data set iterating over the training set in each epoch that can easily like take two minutes or something and if something takes two minutes and you train for let's say 200 epochs that's 400 minutes right so it's a lot of time extra time„ÄÇ

And yeahÔºå the downside of that was well not the downside„ÄÇ

 but the problem I had here was that I did this in a very inefficient way„ÄÇ

 for instance I computed here the train mean absolute  error and mean squared error„ÄÇ

Using iteration over the training modelÔºå and then I had the same thing here too„ÄÇ

So how I solved this problem was essentially writing one function that computes M AE„ÄÇ

 ME and accuracy in one one go„ÄÇ So I don't have to call this function in this function separately„ÄÇ

 It was just writing a function that can do both at once„ÄÇ

 So I would essentially save 400 minutes if one of these takes„ÄÇ

2 minutes each epoch when I was training for 200 epochs„ÄÇ and essentially„ÄÇ

 I also decided just not to track these statistics during training only for the validation set„ÄÇ

 And alsoÔºå it saved me a lot of time„ÄÇ then another 400 minutes„ÄÇ So in that wayÔºå sometimes„ÄÇ

 yeah thinking about„ÄÇWhat you're doing in the code can be useful to make your code more efficient„ÄÇ

 especially if you don't really need the training statistics during training„ÄÇ If it's enough„ÄÇ

 for instanceÔºå to plot the final training accuracy and the final validation accuracy to assess overfitting„ÄÇ

 AlrightÔºå so long story short„ÄÇThe main point I wanted to make here is I was spending a lot of time analyzing my coat and trying to find bottlenecks„ÄÇ

 And then yeahÔºå on Thursday or soÔºå I saw that as' a new tool„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_9.png)

A Pytoch profiler that just came out„ÄÇ And this is a tool that kind of promises to make this more easy to identify bottlenecks in your code„ÄÇ

 And I thought that might might be interesting to you„ÄÇ

 So this is based also on a tool called Tensor board and Tensor board is a tool visualization tool that you can use during training to attract different things„ÄÇ

Like performance metrics and so forth during training„ÄÇSo„ÄÇThis also got me into a rabbit holeÔºå so„ÄÇ

I found out about this Pytoch profileÔºå which I think is really cool„ÄÇ

 that might be something worthwhile trying in the future„ÄÇ And then from there I sawÔºå okay„ÄÇ

 this looks like visual studio code right and then I also saw actually that last month visual studio code released some new capabilities or was updated to add an integration for Tensor board before you had to use Tensor board separately for instance„ÄÇ

 in a web browser now you can use it directly in visual Studio code so you can have your code and Tensor board side by side and also one of the cool things about Tensor board is it can instead of also in just in addition to just tracking training statistics it can also visualize„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_11.png)

The neural network graph„ÄÇ So that is also a helpful debugging technique to make sure the network looks like you expect it to look like that you don't have any weird connections between layers that you didn't intend to connect„ÄÇ

 So that is a cool tool to check out as well„ÄÇ Tensorboard„ÄÇ It's also freeÔºå of course„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_13.png)

And another tool that I saw recently is a tool for comparing different experiments„ÄÇ

 so previously this tensor board is more for tracking during training„ÄÇ

 this is when you are training and you want to look at the loss function during training for example„ÄÇ

 it's easy way to visualize the loss„ÄÇ

![](img/152bfaa7984ce3e5ad1f90b55f160a3e_15.png)

In class the code I provide to you is usually that we use Matprolib at the end of the training„ÄÇ

 but Tensorboard is really useful during training I didn't want to teach that to you in class because yeah we have already so many things to talk about and I would say this is more like something„ÄÇ

That you want to use once you are more familiar with a basic training because yeah„ÄÇ

 it's just another tool and I didn't want to overburden you with too many tools„ÄÇ

 but I recommend to check that out at some point„ÄÇ

![](img/152bfaa7984ce3e5ad1f90b55f160a3e_17.png)

This tool here is a little bit differentÔºå it's more like for comparing models after training„ÄÇ

It's actually I looked at the how you use it„ÄÇ It's actually super simple„ÄÇ

 So you only add one or two lines of code and it will write a certain python dictionary to to your hard drive„ÄÇ

 and it will do it such that you can import it into this tool and it will allow you to compare between different runs„ÄÇ

 So if you run the model with different hyperparmeter„ÄÇ

 It will essentially help you to visualize which one is the best hyperparameter configuration and so forth„ÄÇ

 So actually a pretty nice tool here too„ÄÇ Of courseÔºå this is just one of the manyÔºå manyÔºå many tools„ÄÇ

 There are other ones like there'sÔºå for exampleÔºå I think one is called HyaÔºå one is called M flow„ÄÇ

There are many tools for doing that„ÄÇ but yeahÔºå this one looked actually pretty cool„ÄÇ Also„ÄÇ

 just wanted to share this because yeahÔºå it's also free and also something you may want to check out at one point„ÄÇ



![](img/152bfaa7984ce3e5ad1f90b55f160a3e_19.png)

YeahÔºå lastlyÔºå so I will be giving a talk on Wednesday if someone is interested„ÄÇ

 so they gave me free tickets for students„ÄÇ I shared them with you on Piazza so someone is interested„ÄÇ

 my particular talk would be on March 31st 10Ôºå40 AM central time„ÄÇüòäÔºåI think that is11„ÄÇ

For the in our time zone„ÄÇ No waitÔºå this is our time zone„ÄÇ sorry„ÄÇ

 So this is our time zone central time„ÄÇ Now I used to live in Michigan when we were on Eastern Time„ÄÇ

 So yeahÔºå we are in central time„ÄÇ So yeahÔºå if someone is interested But in general„ÄÇ

 lots of interesting talks hereÔºå you can check it out here„ÄÇ

 So and because there is an exam on Tuesday that I still have to prepare and I also have to prepare for this talk„ÄÇ

 I will nowÔºå yeahÔºå end this stuff in the news section and yeahÔºå good luck for the exam on Tuesday„ÄÇ

