# P123ÔºöL14.6.1- ËøÅÁßªÂ≠¶‰π† - ShowMeAI - BV1ub4y127jj

YeahÔºå all good things and all lectures come to an end„ÄÇ

 So this is the last topic in the series on conversion network architecturesÔºå so„ÄÇ

This is not really a topic specific to convol networks„ÄÇ

 but I think right now is a good time to talk about it because there are some nice pretrained computer vision models available that could be useful for this„ÄÇ

 So the topic is transfer learning„ÄÇ And I think transfer learning is particularly useful for your class projects„ÄÇ

 So the key key idea here is that in a convol networkÔºå we have this feature extraction part„ÄÇüòä„ÄÇ



![](img/9141fdfe10df5f24e1287669bc317fd9_1.png)

Âïä„ÄÇW extraction„ÄÇ Let's write it like this„ÄÇ And then we have these fully connected layers„ÄÇ

Or also convolution layers asÔºå as you remember from the previous two videos„ÄÇ

 But let's call them for fully con connected layers or„ÄÇLinear layersÔºå these are usually the ones„ÄÇ

 the multi layers perceptioncept part that we use for classification or or classifier part„ÄÇ

 And the idea in transfer learning is that these con layers here„ÄÇ

 this automatic feature extraction pipeline might be useful for other related tasks as well„ÄÇ So„ÄÇ

 for instanceÔºå I know that a lot of you are working on projects„ÄÇ

 class projects related to Covid 19 prediction from chest X ray data„ÄÇ So there might be„ÄÇ

An hypothesis that let's say a network trained on a large medical„ÄÇX ray chest X ray data set„ÄÇ

 not related to Covid 19Ôºå more generalÔºå might be also useful for Covid 19 detection if it's fine tuned on the data„ÄÇ

 So usually we in deep burning have these large benchmark dataset sets„ÄÇ

 but they are usually only used forÔºå of course„ÄÇComparing and benchmarking different models in a real application„ÄÇ

 you usually have a much smaller data set„ÄÇ So the question is„ÄÇ

 can we leverage these large amounts of data„ÄÇTrain the network on those and then fine tune in those networks on our smaller target dataset„ÄÇ

So the idea is that the feature extraction layers may be maybe be„ÄÇGenerally useful„ÄÇ

 And then we can use pre trained models„ÄÇ For instance„ÄÇ

 models pre traineded on imagenet freeze the weights„ÄÇ So that means„ÄÇ

When we are freezing these weightsÔºå keeping them fixed„ÄÇ

 not updating them and only training the last few layers like the fully connected„ÄÇLayers„ÄÇ

 the moular perceptron part„ÄÇSo this would be in in the nutshell„ÄÇ

 this would be transfer learning related to that„ÄÇ there's also the approach that you train the whole network on a given data and then fine tune the whole network to the smaller dataset„ÄÇ

 so it's essentially just a special case of transfer learning where you don't freeze the weights„ÄÇ



![](img/9141fdfe10df5f24e1287669bc317fd9_3.png)

So here has a screenshot from an older paper„ÄÇ it's on a large scale video classification with convolutional networks„ÄÇ

 but this kind of is a nice summary of what„ÄÇOptions we have when we use transfer learningÔºå so„ÄÇ

Focusing on this here„ÄÇ So they evaluated the performance by training a conversion network from scratch„ÄÇ

So here they are talking about the accuracyÔºå they got a 41% accuracy„ÄÇOn„ÄÇ

Pretrain a network on a large dataset and then only fine tuned the last layer„ÄÇ

 Yet top layer means the output layer„ÄÇ They got a 64% accuracyÔºå fine tuning the top three layers„ÄÇ

 So last three layersÔºå they got the best accuracyÔºå65% and fine tuning all layers„ÄÇThey got 62%„ÄÇ

 So hereÔºå based on thisÔºå of courseÔºå it's just one experiment or one scenario based on this case here„ÄÇ

 they got or they found out that tuning three layers„ÄÇGives the best results„ÄÇ ButÔºå of course„ÄÇ

 your mileage may vary„ÄÇ It really depends on the network architecture„ÄÇ

 on the data set and many other things„ÄÇ So it's not general„ÄÇ

Case that this is always true and practice it's just another hyperparmeter„ÄÇ

 So how many layers to fine tune is just another hyperparmeter to consider„ÄÇBut yeah„ÄÇ

 you can also see even„ÄÇSo what is also interesting when you look at these two here„ÄÇ

 that train from scratch is„ÄÇMuch worseÔºå let's sayÔºå than pretrain the network on a large dataset set and then fine tuning it„ÄÇ

 even if you don't freeze any layers„ÄÇI will show you in the next video how we can do this in Py„ÄÇ

 So for nowÔºå if this is very abstractÔºå we will get more concrete in the next video„ÄÇ



![](img/9141fdfe10df5f24e1287669bc317fd9_5.png)

So here's what I'm going to show you in the next videoÔºå so because we learned already about V G16„ÄÇ

 it's probably the easiest one to explain„ÄÇ

![](img/9141fdfe10df5f24e1287669bc317fd9_7.png)

But of courseÔºå this is a original concept„ÄÇ So what we are going to do in the next video is we are going to freeze the convolution layers„ÄÇ

And then I will show you how we can replace„ÄÇAlthough I should say first„ÄÇ

 we use the pre trained version that has been trained on imagenetÔºå then we freeze these layers„ÄÇ

And replace the output layers and then fine tune these on a different dataset„ÄÇ

 We are going to use Cypher 10 but of course it could be any dataset„ÄÇ

 it's just because Cypha 10 is already in Pywa so you don't have to download anything„ÄÇ

 So I'm always using Cypha 10 to just make the code simpler or smaller I showed you before how you can use your own dataset„ÄÇ

 but I don't want you to always have to download a dataset when you run the code examples from class„ÄÇ

 So that is why we use Cypher 10„ÄÇ but of course this is a very general concept„ÄÇ so„ÄÇ



![](img/9141fdfe10df5f24e1287669bc317fd9_9.png)

There are besides VG G16Ôºå also other models available„ÄÇ

 So VG G16 might not be the best model we have learned„ÄÇ it's actually not the best performing one„ÄÇ

 It's also very expensive„ÄÇ So for instanceÔºå for your class projects except if you want to get a good performance the wide resnet is actually pretty good and Inception version 3 is also pretty good in terms of performance„ÄÇ

 but they are also slow to run„ÄÇ the good one is actually mobile net it's actually pretty fast well performingform network„ÄÇ

 and of courseÔºå our residual nets„ÄÇ but yeah the wide residual net could can be sometimes better„ÄÇ

 not always when I tested it on Cypher 10Ôºå it actually was worse than the regular renet„ÄÇ

 So also your mileage may vary there„ÄÇ So okay so oh yeah one more thing when we do transfer learning„ÄÇ

 I also wanted to warn you„ÄÇ

![](img/9141fdfe10df5f24e1287669bc317fd9_11.png)

You have to double check how the people who pretrain the network and provide it to you„ÄÇ

 how they normalize the data so you can of course train or pretrain the networks yourself„ÄÇ

 but in Pythtorarch there are some of these models available that have been pretrained on imageNe if you want to use those you have to be careful to use the same data normalization that they used and for imagenet they used this normalization so usually before we used 0„ÄÇ

50„ÄÇ5 and 0„ÄÇ5 because we observed that whether we do that or compute the exact mean and cell deviation doesn't make any difference in practice but because they use these parameterss we also have to use the same parameters when we apply to a different data set to make sure our datas on the same scale as the network might expect„ÄÇ



![](img/9141fdfe10df5f24e1287669bc317fd9_13.png)

AlrightÔºå so in the next video I will show you then concretely how that looks like in Pytorch„ÄÇ



![](img/9141fdfe10df5f24e1287669bc317fd9_15.png)