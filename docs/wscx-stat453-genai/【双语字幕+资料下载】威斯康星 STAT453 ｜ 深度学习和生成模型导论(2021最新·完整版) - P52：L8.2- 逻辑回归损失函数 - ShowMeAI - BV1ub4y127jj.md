# P52ÔºöL8.2- ÈÄªËæëÂõûÂΩíÊçüÂ§±ÂáΩÊï∞ - ShowMeAI - BV1ub4y127jj

AllrightÔºå let's now take a look at how we compute the loss function for the logistic regression model„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_1.png)

The so called negative look likelihoodness„ÄÇSo here„ÄÇ

 that's just the slide I already showed you at the end of the previous video„ÄÇ

And I just want to recap some of the main aspects here„ÄÇ

 So there are two things going on on this slide„ÄÇ So one is how we compute this probability„ÄÇ

 And here at the bottom is what we want this probability to be„ÄÇ

So let's take a look at what we want first in this case„ÄÇ So we want this probability„ÄÇ

 the class membership probability for class 0 to be„ÄÇ1Ôºå to be maximized„ÄÇIf the true label is indeed 0„ÄÇ

 So this is on the left hand sideÔºå this is the class membership probability„ÄÇ We want this to be„ÄÇ

As high as possible„ÄÇ4 equal 0Ôºå if the true labels indeed 0„ÄÇRightAnd vice versa„ÄÇ

 we want the class membership probability here for class1„ÄÇTo be as high as possible„ÄÇ

 also approaching one„ÄÇIf the true label is oneÔºå rightÔºå so„ÄÇ

Given us what we want is we want to maximize the probability for the given labelÔºå for the true label„ÄÇ

NowÔºå how is this probability computed„ÄÇThat is what is shown here„ÄÇ So here again„ÄÇ

 this is how we compute these probabilitiesÔºå so„ÄÇThe class membership pro for y equals one„ÄÇ So here„ÄÇ

 this is not really the true label„ÄÇ You think ofÔºå you can think of it as„ÄÇum„ÄÇ

As this one in this function hereÔºå so„ÄÇWe compute the class membership probability for y equals 1„ÄÇ

So this one here at the bottom„ÄÇBy just the activation„ÄÇ So this is simply the activation„ÄÇ

 So the logistic sigmoid„ÄÇApplied to the„ÄÇNet input„ÄÇAnd for the second case„ÄÇ

 So this one can be computed by one minus the activation„ÄÇSoÔºå this would be„ÄÇThis one here„ÄÇOkay„ÄÇ

 so this is just like a brief recap„ÄÇ Now we can actually rewrite this piecewise function here„ÄÇ

 how we compute this class membership probability using this more compact notation„ÄÇ So this combines„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_3.png)

Or this essentially summarizes this piecewise function in one line or one equation„ÄÇ

 So how it works is yeahÔºå let's just plug in some numbers„ÄÇ Or first„ÄÇ

 let's let me clarify what what this a here means„ÄÇ So this a is yeah the activation„ÄÇ

 it's just instead of writing H of xÔºå it's just abbreviated with a letter a„ÄÇ

 It's like nicer to look at or look simpler„ÄÇNowÔºå let's consider the case where we have the class membership probability for class 1„ÄÇ

Given the feature vector X„ÄÇSoÔºå if„ÄÇY equals1„ÄÇThen„ÄÇYeahÔºå this term„ÄÇRemains as aÔºå and„ÄÇThis one„ÄÇüòî„ÄÇ

Nothing happens here„ÄÇ This one becomes one right and 1 minus-1 is 0 so„ÄÇYeahÔºå this cancels„ÄÇ

Well becomes 0 essentially„ÄÇ and something to the power of0 is  oneÔºå rightÔºü So this whole term here„ÄÇ

Becomes one„ÄÇ So this whole equation here reduces two just„ÄÇÂóØ„ÄÇRightÔºå so a is equal to H of x„ÄÇ

So this one here„ÄÇ So we satisfy this first term here„ÄÇ So this„ÄÇ

 I'm just showing you right now that this function indeed summarizes the piece of s linear function„ÄÇ

 So now consider the case„ÄÇWellÔºå oops„ÄÇY equals 0Ôºå given the feature x to x„ÄÇSo here now consider„ÄÇ

 let me erase this a little bit so we have more space„ÄÇSo we are looking now„ÄÇüòîÔºåThis one hereÔºå so„ÄÇ

Where y is equal to 0„ÄÇ So if y is equal to 0Ôºå then this is a 0„ÄÇ So this whole„ÄÇThing becomes one„ÄÇAnd„ÄÇ

What we have is this becomes a 0„ÄÇ So this term here becomes one„ÄÇSo what we have„ÄÇ

 then what remains is„ÄÇ1 minus aÔºå rightÔºå which is this one„ÄÇ

 So you can now see that this equation here at the bottom summarizes this piece ofwise linear function here„ÄÇ

Just a different notation„ÄÇ

![](img/f59ece0231083208879ddbd6ecc91d4f_5.png)

NowÔºå yeahÔºå what we want is we want to maximize the class membership probabilities„ÄÇ So„ÄÇ

 and usually in a data setÔºå we not only have one single data pointÔºå we have„ÄÇAnd data points„ÄÇ

 So this is the size„ÄÇO„ÄÇTraining„ÄÇSaid„ÄÇAnd yetÔºå we want to maximize the class membership probabilities for all these examples in the training set„ÄÇ

So you can think of it asÔºå yeahÔºå the probabilities here multiplied„ÄÇ So the conditional probabilities„ÄÇ

 and we want to findÔºå yeahÔºå the„ÄÇModel parameterstersÔºå because„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_7.png)

YeahÔºå it depends here on our model right„ÄÇ So the probability here„ÄÇ

 the prediction probability here depends on our model parameters„ÄÇ

 that's how we compute these probabilities„ÄÇ So we want to find good parameters that maximize the class membership probability for the right class„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_9.png)

So this concept is also known as maximum likelihood estimation„ÄÇ

 So the we want to maximize the likelihoodÔºå the probability of observing the data given the model parameters„ÄÇ

And yeahÔºå you probably know this from other statistics classes„ÄÇ

 so I don't want to go into too much detail about maximum likelihood estimation here„ÄÇ

 So just assume now we want to maximize this term here„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_11.png)

So here what I'm doing is I'm just rewriting this with we the simpler notation I showed you before„ÄÇ

So if I go back to slides„ÄÇ

![](img/f59ece0231083208879ddbd6ecc91d4f_13.png)

So„ÄÇThis notation here at the bottom„ÄÇ

![](img/f59ece0231083208879ddbd6ecc91d4f_15.png)

That's what I'm doing here„ÄÇ So I'm replacing this„ÄÇRight„ÄÇ

 this is something you have seen before on the previous slide I just showed you„ÄÇ

 I'm replacing this now with this more compact notation here where„ÄÇBefore I I rebb it as a„ÄÇ

 I don't know why I expanded it here again„ÄÇ I I think I just wanted to show you how it's computed„ÄÇ

Where a is we have the logistic regression modelÔºå where this is sigma is logistic activation function and Z is the net input„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_17.png)

All right„ÄÇSo„ÄÇYou can think of it also as the loss function„ÄÇ then so as the logistic loss„ÄÇ

 which we computers follows the probability over the whole training set„ÄÇ

And I said we want to maximize thisÔºå rightÔºå We want to maximize the class membership probabilities„ÄÇ

In practiceÔºå it's actually easier to maximize the natural lock of this equation„ÄÇ

 which is also called the lock likelihood function„ÄÇSo why is that if we apply the lock here„ÄÇ

 then we can replace the product„ÄÇBy a sum„ÄÇAndAlso here in this multiplication„ÄÇ

 we can replace it with a sum and numerically on a computer this is just more stable to implement it more more stable when we use„ÄÇ

 for exampleÔºå gradient descent„ÄÇBut you can think of it also as maximizing the„ÄÇ

L likelihood also maximizes the likelihood„ÄÇ So it's like the same kind of the same thing as if you maximize one„ÄÇ

 you also maximize the other„ÄÇ So in that wayÔºå this is why we can do that„ÄÇ

So we here at the bottom is the„ÄÇLock and was just taking the lock of this party at the at the top„ÄÇ

 So we have now„ÄÇWhy is the true label times lock the activation„ÄÇ

So this is the cast membership probability„ÄÇSo„ÄÇThose made for completenessÔºå this would be„ÄÇY given X„ÄÇ

 andÔºå of courseÔºå also the model parametersÔºå But I'm omitting the model parameters here just for simplicity„ÄÇ

And yeahÔºå this is the lock likelihood loss„ÄÇ

![](img/f59ece0231083208879ddbd6ecc91d4f_19.png)

I meanÔºå it's not„ÄÇQuite the likelihood innessÔºå because there's one more modification„ÄÇ So in practice„ÄÇ

 alsoÔºå when you recall from the pytor code that I showed you„ÄÇ

 we have the stochastic gradient descent„ÄÇAnd in stochastic„ÄÇ

 gradient descent that me bright us may be known„ÄÇSchastic„ÄÇüòîÔºåGrilliant„ÄÇSentÔºå we want to minimize„ÄÇ

So this is process when we„ÄÇMinimize something„ÄÇ If we wanted to maximize something„ÄÇ

 we would have to use stochastic gradient„ÄÇesscent„ÄÇThat is like the other way around„ÄÇ we want to„ÄÇ

 we would maximize something„ÄÇSo what we don't want to„ÄÇ

 so if we think of it maybe as a loss function soÔºå or as a function„ÄÇ

 So grain and descent would be like this where we go down this hill„ÄÇAnd great essencecent would be„ÄÇ

This wayÔºå where we go up that hill„ÄÇNowÔºå in Ptor or many other libraries„ÄÇ

 it just happens just for simplicity„ÄÇTheres an implementation of gradient descent where we minimize something„ÄÇ

 So in that way„ÄÇIfWe would just give it a look likelihood function as the loss function to optimize„ÄÇ

 then yeahÔºå it would actually max minimize this lossÔºå which would actually very bad„ÄÇ

 So we we would get as very small class membership probabilities for the correct labels„ÄÇ

 So what we can do now is just to turn around this problem„ÄÇ So instead of„ÄÇÂóØ„ÄÇ

Maximizing the look likelihoodÔºå we can minimize the negative look likelihood„ÄÇ

 just putting a negative here in front„ÄÇ And then we turn this maximization problem into a minimization problem„ÄÇ

 And then we can just use the stochastic gradienting descent implemented in Pytorch„ÄÇüòä„ÄÇ

So it's just like a little trick we do by flipping around the sign here and another trick we can do in code is that we usually add a one over end scaling factor„ÄÇ

Here to scale yeah over the number of data points in the training set or the batch size„ÄÇ

 the mini batch size„ÄÇ So last time we also talked about mini batches„ÄÇ so we can also excuse me„ÄÇ

 so we can also consider the mini batch size here„ÄÇ

![](img/f59ece0231083208879ddbd6ecc91d4f_21.png)

That it makes the training yeahÔºå numerically more stable„ÄÇYeah„ÄÇ

 so do just have a little checkpoint here to summarize„ÄÇ so doing logistic regression by that„ÄÇ

 I mean a training logistic regression„ÄÇIs actually similar to what we have done before in Adeline in Adeline„ÄÇ

 recallÔºå we minimize the mean squared errors„ÄÇHereÔºå but yeah„ÄÇ

 the difference is that in logistic regressionÔºå we maximize the likelihood„ÄÇ

Because it's numerically more stable instead of maximizing the likelihood„ÄÇ

 we can also maximize the lock likelihood„ÄÇBut likea mentioned„ÄÇ

 the tools that we usually have available are developed for stochastic gradient descent so that we don't have to rewrite those tools„ÄÇ

 we can just yeah turn this maximization problem into a minimization problem so we can minimize the negative look likelihood„ÄÇ

 which is the same as maximizing the look likelihood„ÄÇSo yeah„ÄÇ

 it was probably a lot of information in this short video In the next video„ÄÇ

 I will talk a little bit more about this training and how the loss function actually looks like„ÄÇ

 I think this will probably clarify some yeah thoughts that you might have„ÄÇ

 So if you have some questions right now maybe don't think about it too hard„ÄÇ

 maybe watch the next video and then if you have questions then yeah maybe think about it more„ÄÇ

 but I think the next video could potentially clarify some points„ÄÇ



![](img/f59ece0231083208879ddbd6ecc91d4f_23.png)