# P82ï¼šL11.1- è¾“å…¥è§„èŒƒåŒ– - ShowMeAI - BV1ub4y127jj

Alrightï¼Œ let's start by talking about normalizing inputs to improve gradient indesscentã€‚

 So essentiallyï¼Œ input normalizationã€‚

![](img/c9773462c19c0c6c8ab584d7ecb9a2e9_1.png)

Yeahï¼Œ so this is just a brief recap why we normalize inputs for gradient in descentã€‚

 We already talked about this a little bit earlier when we talked about gradient descent and stochastic gradient in descentã€‚

 I think it was in lecture 5ã€‚So yeahï¼Œ just to recapã€‚

 why do we normalize on those weights So that is really to find a good learning rate and to have aã€‚

 I would sayï¼Œ more stable trainingã€‚So suppose for simplicityã€‚

 we have here shown a surface part of a convex loss functionã€‚

 So here this is like a convex function shown hereã€‚

 So usually when we work with deep neural networksï¼Œ that's not the caseã€‚

 that's not a convex function anymoreï¼Œ but just for simplicityï¼Œ assume here it's convexã€‚

 and we have aï¼Œ let's sayï¼Œ a linear modelã€‚Let's say linear regression with two weights just to keep things simpleã€‚

Ignoring the bias alsoã€‚ nowï¼Œ technicallyï¼Œ when we have something like gradient descent and this is fullyã€‚

Circularï¼Œ let's sayï¼Œ in this lower case hereï¼Œ we would usually take steps perpendicular to these contoursã€‚

Howeverï¼Œ when we have stochastic gradient descentï¼Œ also these steps won't beã€‚Yeahã€‚

 perpendicular anymoreã€‚ And especially this will be exaggerated if we haveã€‚These elliptical lossesã€‚

 So where we may overshoot in one dimensions instead ofï¼Œ let's say going in hereã€‚

 we take a step perpendicularï¼Œ butã€‚Instead of stopping here and then taking this step hereã€‚

 we overshoot and then we go this zigzaggingã€‚ And then also if we have stochastic gradient sentences even more exaggeratedã€‚

So why why this weird also zigzagging can be amplified is when we have features on different scalesã€‚

Soï¼Œ for instanceï¼Œ imagine a data set where we haveï¼Œ let's sayã€‚The age of a person fromã€‚

10 to 50 years and then let's say the income of a personã€‚Let's sayï¼Œ fromï¼Œ I don't knowï¼Œ10000 toã€‚

Let's sayï¼Œ500000ã€‚US dollars or something like thatã€‚ So these are on vastly different scalesï¼Œ rightã€‚

And remember how we do gradient in descentã€‚ So usually what we do is we have the weight updateã€‚

 let's say weight 1ï¼Œ we would update it byã€‚We  one minus the partial derivative root of the loss with respect to that weightã€‚

 and then times a learning red alphaï¼Œ rightï¼Ÿ So in this partï¼Œ this computation involves usually Xã€‚

 the input itselfï¼Œ for exampleï¼Œ it could beã€‚Y head minus yã€‚Timesï¼Œ so let's say xã€‚

X1 is the input featureã€‚ So hereï¼Œ the input feature kind of scalesã€‚The whole thingã€‚

 And then if we have input features on very different scalesï¼Œ then wellã€‚

 finding a good learning rate would be hardã€‚ we would have toï¼Œ yeah choose a learning rate for eachã€‚

Weighed individuallyï¼Œ and that would be yeahï¼Œ a lot of hassleã€‚ Soï¼Œ for instanceã€‚

 because if we consider no W 2ã€‚Were2 minusã€‚How farã€‚ğŸ˜”ï¼ŒLike thisï¼Œ soã€‚é‚£'sã€‚This one and this oneã€‚

 these learning ratesï¼Œ we usually use the same learning rate and this really only works well if the features are on the same scale right because if this one is like1 thousand times largerã€‚

 we may have to use a thousand times smaller learning rate to get a good behaviorã€‚

Another thing is also that most optimization algorithms really work well if the features are centered at zero and have only like a smallã€‚

 yeah small values around zeroã€‚ One issue is also always when we have very large or very small numbers and computational context because then we also have a computational instability issuesã€‚

Soï¼Œ oneã€‚Common theme that works pretty well in practice is Z square standardizationã€‚

 So people just call it standardizationã€‚ It's essentially giving your data or features the properties of a standard normal distributionã€‚

 I meanï¼Œ the data after applying this won't be normal if it wasn't normal beforeã€‚

 but it has the properties of a standard normal distributionï¼Œ which is zero mean and unit varianceã€‚

 So unit variance is a fancy word for saying that the standard deviation is one or the variance is oneã€‚

Soã€‚ğŸ˜Šï¼ŒAgainï¼Œ if the data is not normal to begin withï¼Œ this won't changeã€‚

 This won't really change the distributionï¼Œ how it looks likeã€‚

 it's just people say it has the parameters of the normal distribution because essentially it will haveã€‚

0ï¼Œ mean endã€‚Unit variancesï¼Œ But just think of it asã€‚

Here as centering the data with standard deviation of variance 1ã€‚So how it works is for each featureã€‚

 So let's say this is for a particular data pointã€‚So we want to scale data pointã€‚Iã€‚

 let's say this is like an iris flower eyeï¼Œ and we want to scale the J feature where J might be something like let's say Salã€‚

Lengthã€‚And how we do that is we subtract the feature inã€‚ So here the feature meanã€‚This would be overã€‚

æˆ‘ä¸å“¦ã€‚Flowers1 over Nï¼Œ rightã€‚And let me do this properly hereã€‚



![](img/c9773462c19c0c6c8ab584d7ecb9a2e9_3.png)

oneæˆ‘ã€‚

![](img/c9773462c19c0c6c8ab584d7ecb9a2e9_5.png)

Go over inã€‚A equals1ã€‚Inï¼Œ and then xã€‚Hã€‚ğŸ˜”ï¼Œ so we will just computeã€‚The mean across this featureã€‚

And here we do the same thing for the standard deviationã€‚

And this is essentially it this usually helps with a better behaviorã€‚

 so we will then usually have a more elliptical sorry a more symmetric loss surfaceã€‚

 but again this only applies really if we have a very simple generalized linear model like linear regression in this caseã€‚

 for exampleï¼Œ if we have something else like a deep neural networkã€‚

 this won't be a convex loss function anymoreï¼Œ but still it helps to standardize the inputs it still yeah recommended in practiceã€‚



![](img/c9773462c19c0c6c8ab584d7ecb9a2e9_7.png)

Okayï¼Œ so this is one way we can standardize the inputsã€‚

 but now when we talk about deep neural networksã€‚ So let's say we have the inputs hereï¼Œ x1 and x2ã€‚

 and then we have activations a1 a2ï¼Œ a3ï¼Œ and then let's say we have two output unitsã€‚

 Let's call it01 and02ã€‚ Okayï¼Œ so previously on the previous slideã€‚

 we talked about standardizing these things here rightï¼Œ So the inputsã€‚

 Now what about the activationsã€‚Might also be helpful to standardize the activations that go then into theseã€‚

Output into the output layerï¼Œ rightã€‚So that might also be helpful to standardize the activationsã€‚

 the hidden layer activationsï¼Œ because you can really think of these AC as the activationsã€‚

 You can think of them as the inputs to the next layerã€‚ Theyre essentially just like xã€‚

 except that these are the inputs to the next layerã€‚ So in the next videoã€‚

 I want to talk about one technique called batchnobã€‚

Which is essentially a variant of this standardization for yes standardizing hidden activationsã€‚

 It's coming with a twist There will be a learnnable or two learnnable parameters for thatã€‚

 but essentially you can think of it as yes standardization of the hidden layer activationsã€‚

 so that will be the topic of the next videoã€‚

![](img/c9773462c19c0c6c8ab584d7ecb9a2e9_9.png)

![](img/c9773462c19c0c6c8ab584d7ecb9a2e9_10.png)