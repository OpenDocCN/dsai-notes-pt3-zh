# P55ÔºöL8.5- PyTorch ‰∏≠ÁöÑÈÄªËæëÂõûÂΩí - ‰ª£Á†ÅÁ§∫‰æã - ShowMeAI - BV1ub4y127jj

YeahÔºå before we move on and talk about the multi class example of logistic regression„ÄÇ

 the so called softmax regressionÔºå let's just wrap up everything we here talked about in the previous four videos in a code example„ÄÇ

 So I prepared a code notebook„ÄÇ You can find it on Gith„ÄÇ I will share the link on canvas„ÄÇüòä„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_1.png)

Where I have implemented two things„ÄÇ So the first thing is logistic regression from scratch„ÄÇ

 just yeahÔºå from the ground upÔºå low level implementation of logistic regression„ÄÇ

 really like implementing„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_3.png)

The learning rule that we talked about here„ÄÇAnd then I will show you how we can do that using the Pythr utilities„ÄÇ

 the module API that we talked about last week where we have this N dot linear layer and so forth„ÄÇ

 This is something we would do in practice when we haveÔºå of courseÔºå more complicated networks„ÄÇ

 but I thought from scratch implementation is still useful from the learning perspective so you get a better feeling of how logistic regression works„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_5.png)

SoÔºå let's get started„ÄÇSoÔºå of courseÔºå first my watermarkÔºå you can ignore the torch version„ÄÇ

 this torch version doesn't exist„ÄÇ actuallyÔºå I yesterday„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_7.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_8.png)

Compiled torch from sources to test something„ÄÇ So this should also work on torch of 1„ÄÇ

7 so no worries can ignore that„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_10.png)

YeahÔºå so„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_12.png)

BÁöÑ„ÄÇPo of matpotletÔºå which we'll be using NyÔºå we will be using„ÄÇ

 And then the torch stuff we will be using„ÄÇ So just some inputs to get them out of the way„ÄÇ

 we will be using some simple toy data set„ÄÇ That's likeÔºå yeahÔºå just„ÄÇ

 I think that is what I gave you also for the homeworkÔºå to be honest„ÄÇ I have to double check„ÄÇ

 I made multiple of these toy data sets„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_14.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_15.png)

ButÔºå that's just a simple binary classification dataset set„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_17.png)

Now here is my logistic regression implementation„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_19.png)

So how does it workÔºü So I'm using the roughly same way how we would implement that using Pytorch with a module API„ÄÇ

 So here I am implementing forward and backward manually„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_21.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_22.png)

These are the two methods where forward isÔºå yeahÔºå the forward pass and backward is the gradient descent computing„ÄÇ

YeahÔºå the gradient of the loss function„ÄÇI actually let me modify this„ÄÇ

 Let's call that loss with respect to„ÄÇ So W RT means with respect to„ÄÇ I think it makes it clearer„ÄÇ

 So that's done„ÄÇGraing of the loss with respect to the outputÔºå and so forth„ÄÇ

We'll just modify it everywhere here„ÄÇAnd then„ÄÇLet's do it step by step here„ÄÇ

 So then here in the init functionÔºå this is like the function or method that get ex gets executed when we instantiate a new object from this class„ÄÇ

 What we have is the weights and the bias unit„ÄÇSo the weights we initialize them to to zeros„ÄÇ

 so we use the zeros function to initialize them to zeros„ÄÇ It's a vectorÔºå a oneÔºå it's a row vector„ÄÇ

 basically a one times nu features vectorÔºå so matrix„ÄÇ so it's essentially a row vector so„ÄÇ

Numb features is the number of features in the dataset„ÄÇ

 So the number of weights is equal to the number of features just like an Adeline„ÄÇ

 and we only have one bias unit„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_24.png)

AlrightÔºå so the forward methodÔºå yeahÔºå this is computing the net inputÔºå rightÔºå So this is„ÄÇ

The net input can also call it Z„ÄÇCan„ÄÇüòîÔºåWe could technically call it an a„ÄÇ

 but you know what we mean rightÔºå I meanÔºå I can rewrite this maybe„ÄÇDoesn't really matter„ÄÇ

I can just leave it like that„ÄÇ I think I'm underestimating you„ÄÇ

 I think it's probably all clear to you„ÄÇ I don't have to use these different words„ÄÇ alright„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_26.png)

SorryÔºå so„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_28.png)

The backward method isÔºå yeah computing these gradient terms„ÄÇ

 This is going back to what we've done here„ÄÇ So here I'm computing the gradient of the loss with respect to the output„ÄÇ

 So this is essentially this part„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_30.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_31.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_32.png)

Here„ÄÇThen the grain of the loss with respect to the weightÔºå this is„ÄÇThis part here„ÄÇ

Where I put those two things together so„ÄÇThis should probably be with respect to the the and Z„ÄÇ

 the net inputÔºå this may be clearer instead of saying output„ÄÇ

So this is matrix modificationification between„ÄÇX„ÄÇAnd this term„ÄÇ

 I just rearranged that to make the dimensions matchÔºå which is why I have the transpose„ÄÇ you„ÄÇ

 to be honestÔºå don't have to memorize these details„ÄÇ

 You will probably find out by tinkering with the code if you ever implement something from scratch because here this is really for one weight„ÄÇ

 And here I have that vector implementation for multiple weightsÔºå right because we have„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_34.png)

The number of weights equal to the number of features„ÄÇ It's a bit more complicated„ÄÇ

 but the concept is the same„ÄÇ So this line here really is this part here„ÄÇ

I can actually also maybe go to this part here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_36.png)

ThisÔºå this part hereÔºå this„ÄÇIs essentially this one here„ÄÇ

 the gradient of the loss with respect to the weight„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_38.png)

And then the same for the biasÔºå which would correspond to this part here„ÄÇ

Here this is the threshold functionÔºå predict labels„ÄÇ

 So I'm calling forward that gives me the probabilities aÔºå so a is the probabilities Pros„ÄÇ

 I call them Pros because in the lecture I said also it was a bit complicated when we had Y hat and it was confusing so„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_40.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_41.png)

Referd to a as the probabilities„ÄÇThat's the sigmoid activation here„ÄÇ

And this is the threshold function where if the probabilities are greater than 0„ÄÇ5„ÄÇ

 return class label 1„ÄÇ OtherwiseÔºå return class label 0„ÄÇ I couldÔºå howeverÔºå also„ÄÇ

Do it as with the net inputsÔºå I could say if the net inputs„ÄÇ

Let's call them 0 are greater than 0 than return 1„ÄÇ OtherwiseÔºå0„ÄÇ This is because„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_43.png)

What we discussed here„ÄÇSo here's the threshold function„ÄÇ And because„ÄÇThe probabilities with that „ÄÇ5„ÄÇ

 the net input is 0„ÄÇ So either we can operate directly on the net input or the probabilities here„ÄÇ

 So you have that with the probabilitiesÔºå but doesn't really matter that much„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_45.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_46.png)

OkayÔºå so here's just an evaluation functionÔºå so this is not super interesting„ÄÇ

Here I'm predicting the labelsÔºå which in in turn uses this method here„ÄÇ

 which computes forward and then the threshold function here„ÄÇSo we can call it a threshold„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_48.png)

Fction„ÄÇAnd thenÔºå in order to compute the accuracyÔºå we„ÄÇSome„ÄÇOver the correct predictions„ÄÇ

 So every time the class label matches the„ÄÇThe predicted labels here„ÄÇ

Matches the actual class tables every time they match„ÄÇWe sum it up„ÄÇ

 So this is a number of correct predictions and then divide by the size of the training set„ÄÇ

 So it's a value between 0 and 1„ÄÇ It's the proportion of correct predictions„ÄÇ Why do I do float here„ÄÇ

 It's because the class labels are integers„ÄÇ And yeahÔºå in Pythage„ÄÇ

 it's a little bit tricky to compute or compare things like floatsÔºå The labels are floats here„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_50.png)

And then this is Integer„ÄÇ I think I could have omitted this here„ÄÇ

 It's maybe a little bit complicated„ÄÇ I could have was think done„ÄÇThat„ÄÇ

 and it would also have the same effect„ÄÇSoÔºå but anyways„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_52.png)

OkayÔºå anything else„ÄÇ YeahÔºå this is a sigoid functionÔºå which you have seen a lot of times„ÄÇ

 So there is this function here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_54.png)

YeahÔºå the loss function„ÄÇ That's the negative look likelihoodÔºå which we have„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_56.png)

Right here at the bottomÔºå that's the„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_58.png)

We'll just take a cost function„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_60.png)

And here's the training functionÔºå so here„ÄÇAgainÔºå let me jump back to this one„ÄÇWe are right here now„ÄÇ

 So for every training apoch„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_62.png)

For E in the number of epochsÔºå we compute the outputsÔºå which is here„ÄÇ That's the probabilities„ÄÇ

 the A's„ÄÇ So this is really this should be an a actually„ÄÇ So remember„ÄÇ

 this was copied from the a line should have„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_64.png)

Probably adjusted it to an A here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_66.png)

But you probably know what I mean„ÄÇ Then we compute the gradients„ÄÇ So these are under B„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_68.png)

Here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_70.png)

Then we update the weightsÔºå which is C this part here„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_72.png)

And this is it„ÄÇ And this is just forÔºå yeahÔºå logging„ÄÇ

 just keeping track of everything like looking at the results and the loss score„ÄÇ

 which can be useful„ÄÇ So let's actually do that„ÄÇ Let's„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_74.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_75.png)

YeahÔºå do that here„ÄÇ What I'm doing is I'm converting the numpey„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_77.png)

A racese to torch tensors„ÄÇ because if you go up here„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_79.png)

You can seeÔºå I„ÄÇUse nuy here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_81.png)

So to load of data„ÄÇ So here I'm just converting them to torch tensrs„ÄÇ

 Then I'm initializing my logistic regression model„ÄÇ So this will be actually step  one initializing„ÄÇ

And then Im calling train train would be everything under two here„ÄÇ

And then I'm printing the model parametersÔºå the weights in the bias just to take a look at them„ÄÇ

 and I'm training for 30 epochs„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_83.png)

With a learning rate of 01„ÄÇSo I think that's the mistake I always make I talk through this„ÄÇ

 but I usually forget to execute this„ÄÇ OkayÔºå now we are„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_85.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_86.png)

Running the codeÔºå okayÔºå we've got an arrow here„ÄÇOkayÔºå I yeahÔºå I changed the„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_88.png)

Name one second„ÄÇ Let me fix that„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_90.png)

AlrightÔºå so it's training„ÄÇ You can see it's blazing fast„ÄÇ

 It starts with already a very high accuracy after the first epochÔºå to be honest„ÄÇ So it's yeah„ÄÇ

 it's actually training super fast„ÄÇ The cost is very low„ÄÇ

 but you can see if I do more epochs that goes even lower„ÄÇ

 the accuracy goes up to 100% here even but you can see even though the training accuracyies or 100%„ÄÇ

 the loss still goes down„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_92.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_93.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_94.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_95.png)

I could have trained more and would go maybe down further„ÄÇ

 but that at that point might not be necessary„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_97.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_98.png)

Might be overfitting„ÄÇÂóØ„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_100.png)

Wanted to execute this part„ÄÇOkayÔºå theres some message here„ÄÇ

 I think this message comes from Ny because I recently installed a very recent version of Ny„ÄÇ

 and it must be something in Matpl here„ÄÇ You can actually ignore that that this is not„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_102.png)

Arrow message that is just a warning„ÄÇ So if you see warning or deprecation warning„ÄÇ

That means things are still working„ÄÇ so don't worry about them„ÄÇ But in future„ÄÇ

 if there's an even newer version of a software library in PythonÔºå this may not work anymore so„ÄÇHere„ÄÇ

 this is nothing we are doing wrong„ÄÇ This is probably something inside Matpro lip„ÄÇ

 and I'm sure they are already aware and working with that„ÄÇ so don't worry about this warning„ÄÇ

 Everything still works fine„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_104.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_105.png)

There will be probably a new met product version that addresses this issue at some point„ÄÇ

 but everything is fine„ÄÇ YeahÔºå you can see here is how the lockÔºå yeah„ÄÇ

 the negative look likelihood loss looks like„ÄÇ So it should be the negative„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_107.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_108.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_109.png)

Not likelihood loss to be precise„ÄÇ And then over the number of epos„ÄÇ

 you can see maybe it will go down further„ÄÇ You can see it's actually going steep down„ÄÇ

 But whether it's actually helpingÔºå I don't know because we already get 100% training accuracy„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_111.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_112.png)

So let's take a look at the test accuracyÔºå it's only 96%„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_114.png)

For completenessÔºå we will also take a look at the„ÄÇYeahÔºå it this year„ÄÇ So yeah„ÄÇ

 we can see actually here on the training setÔºå the decision model looks quite good„ÄÇ

So it's dividing both classes class0 and class1 on the test that yeah„ÄÇ

 it's not performing so well because there's this one outlier pointÔºå right so here„ÄÇ

This one is not classified correctly„ÄÇ So if this decision body would go a little bit more to the right„ÄÇ

 then we might get this one correct„ÄÇ but at this point„ÄÇ

 this model only see a training that I doesn't know of this point here on the right hand side„ÄÇ

 So in this caseÔºå I think training the model further wouldn't really help because this is already doing a good job here actually maybe putting a little bit more to the right would help actually because then would be more here„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_116.png)

Ê≤°Êúâ„ÄÇüòîÔºåAnywaysÔºå this is just an exampleÔºå of course I just implemented this quickly in a way that I didn't really find tune in anything„ÄÇ

 and this is a toy data set anywayÔºå so don't over interpretterpret this Okay so the goal here was really to show you if I scroll up again to show you how we implement this from scratch like the most interesting part I would say is the backward function which is a little bit different from the Adeline function„ÄÇ

 the rest should be all the same as in the Adeline implementation so it's not that new to you I hope„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_118.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_119.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_120.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_121.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_122.png)

NowÔºå I want to show youÔºå thoughÔºå how we yeahÔºå use the module API to implement this logistic regression module so here„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_124.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_125.png)

This should do exactly the same thingÔºå but now I'm relying on in build or build in pytorch functions„ÄÇ

 So I'm using this self dot linear here to refer to my torch dot in end dot linear layer„ÄÇ

 This is a layer that computes the net input„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_127.png)

HoweverÔºå note that by defaultÔºå this will initialize the weights to small random numbers„ÄÇ Later on„ÄÇ

 this will be very usefulÔºå but it is not that useful to compare it to our model above because if you recall above I initialized my weights and bias to 0„ÄÇ

 So I want to do the same here„ÄÇ so I can compare the two models„ÄÇ

 So what I'm doing is manually assigning„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_129.png)

0 to the weightsÔºå and by unit„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_131.png)

ÂóØ„ÄÇYeahÔºå and then we have the forward method„ÄÇ So here the forward method„ÄÇ

 we compute the logicits via the self to linearÔºå which is this fully connected layer or linear layer„ÄÇ

 and then we have built in py function that computes the sigmoid activation function„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_133.png)

So you can see a logistic regression model is actually super simpleÔºå actuallyÔºå we can also„ÄÇ

 it would train just fine if we have this one„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_135.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_136.png)

Just for comparison person reasons I have this oneÔºå but you can see how simple it is„ÄÇ

 and it will already implement the backward function for us„ÄÇ We don't have to do it ourselves„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_138.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_139.png)

Here we are using the stochastic gradient descent„ÄÇÂóØ„ÄÇOptimizerÔºå so this will update the weights then„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_141.png)

So this will compute the gradients via other the backward and update the weights„ÄÇ

 So we are just specifying what weights we want to train„ÄÇ This is the model parametersus here„ÄÇ

 Model 2„ÄÇ So this is„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_143.png)

Model 2 or one logistic regression model 2 Model 1 was the from scratch implementation„ÄÇ right„ÄÇ

 so here is just an accuracy function to compute the prediction accuracy„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_145.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_146.png)

We are training also for 30 epochs„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_148.png)

Like beforeÔºå what Im doing is I am casting the nuy„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_150.png)

A raised to a tensorÔºå I've already done thatÔºå so it's probably redundant„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_152.png)

And now here' is my training loop in Pytorch„ÄÇSoÔºå for each epoch„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_154.png)

I compute the outputs„ÄÇ These are the probabilities„ÄÇ This is from the forward pass„ÄÇ This is„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_156.png)

These probabilities here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_158.png)

Then I use this cross entropyÔºå binary cross entropy function„ÄÇ

 and this is essentially why I spent some time in the last lectureÔºå last video here when we had it„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_160.png)

When we had this topic hereÔºå why I spend some time Yeah just telling you that the negative log likelihood and binary cross entropy are equivalent„ÄÇ

 same thing„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_162.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_163.png)

Here I'm saying reduction sumÔºå because that's what we have done here„ÄÇ It's a sum„ÄÇ

 or we can also do the average„ÄÇ ActuallyÔºå if I change thisÔºå it will be the average or„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_165.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_166.png)

Mean by the default„ÄÇ But since we implemented above the sum2Ôºå and I have the sum hereÔºå I'm also„ÄÇ

 for comparison reasons use the sum here in practiceÔºå I think using the mean is more stable„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_168.png)

For with regard to learning rates and different batch sizes„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_170.png)

YeahÔºå and then this is really like what we have done in the last week when we trained Adeline using these automatic things„ÄÇ

 we„ÄÇHave the followingÔºå we compute the loss„ÄÇI„ÄÇTraditionally use the term cost„ÄÇ

 but I think loss is more natural now nowadays than we„ÄÇYeahÔºå set the optimizer„ÄÇ0 gra„ÄÇ

 which will yeah reset the gradients from the previous round„ÄÇ

 Otherwise the gradients will accumulateÔºå I I explained last week„ÄÇ

Then we compute the gradients for the current round and then update the weights„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_172.png)

And this is essentially it„ÄÇ Let's run this„ÄÇAlrightÔºå error„ÄÇ Model 2 is not defined„ÄÇ See„ÄÇ

 I always forget to execute things„ÄÇ AlrightÔºå let's execute this one„ÄÇ and then this one„ÄÇ Yeah„ÄÇ

 it trains„ÄÇ OkayÔºå so and you can see if you recallÔºå these have the same values as we've seen before„ÄÇ

 I can just scroll up in a second and just to show you„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_174.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_175.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_176.png)

And also take a look at these weight parameterss„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_178.png)

Can just copy and paste them„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_180.png)

So yeah you can see they are exactly the sameÔºå so we learned exactly the same using Pyhar„ÄÇ

 which gives us confidenceÔºå I hope that we understand gradient descent correctly and implemented„ÄÇ

 yeahÔºå the gradientding here correctly„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_182.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_183.png)

So it's the same as in Pytorch„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_185.png)

Allright„ÄÇYeahÔºå and this is it essentially„ÄÇ So evaluating the modelÔºå same tester accuracy„ÄÇ



![](img/eb410a541f0e309c0b9d6e6b05498cf9_187.png)

Because it's the same weights and biasÔºå so it should be the same decision boundary„ÄÇ

 which we can see here„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_189.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_190.png)

YeahÔºå so this is how we implement logistic regression„ÄÇ

 I hope that was helpful like understanding a little bit better how these different concepts here in the slides work„ÄÇ

 You may also want to work through this more slowly„ÄÇ I think that might be also helpful„ÄÇ but yeah„ÄÇ

 that's it„ÄÇ So the next video we will then talk about how we can generalize these concepts to multiple classes„ÄÇ

 That means like more than two classes„ÄÇ

![](img/eb410a541f0e309c0b9d6e6b05498cf9_192.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_193.png)

![](img/eb410a541f0e309c0b9d6e6b05498cf9_194.png)