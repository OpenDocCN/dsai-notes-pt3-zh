# P167ï¼šL19.5.2.6- BART- ç»“åˆåŒå‘å’Œè‡ªå›å½’transformer - ShowMeAI - BV1ub4y127jj

Yesï¼Œ so there's virtually an endless number of language modelsã€‚ Of courseã€‚

 we can't cover all of themã€‚But yesï¼Œ so one moreï¼Œ because that's an interesting oneã€‚

 It's called Bartã€‚ And what's interesting about it is it is combining both the concepts from GT and Btã€‚

 So Bart stands for bidirectional and autoregressive transformersã€‚

 So bidirectional for Bt and the autoregressive one for GTã€‚ It's about combining those twoã€‚ğŸ˜Šã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_1.png)

So the ideaï¼Œ I meanï¼Œ this model is from 2019ã€‚ So of courseï¼Œ it's notï¼Œ I would sayã€‚

 state of the art anymore in terms of performanceï¼Œ but I think the main idea is still interestingã€‚

 So this is by Facebookï¼Œ AI research combining both the Google bird and Openme I G andã€‚

The argument or the thinking behind it is that birdss bidirectional natureï¼Œ the auto encode natureã€‚

 is good for downstream tasks like classificationï¼Œ which require information about the whole sentence so if you have classification tasksã€‚

 for exampleï¼ŒIt's not necessary an advantage toï¼Œ yeahï¼Œ predict one word at a timeã€‚

 It's more about understanding the wholeï¼Œ the whole sentence at onceï¼Œ in a senseã€‚

The downside of something like bird isï¼Œ it's not so good for generation tasksï¼Œ thoughã€‚

 where the generated word should only depend on the previously generated wordã€‚ For instanceã€‚

 it's a question answering or something like that where you generate one word at a timeã€‚ So bird isã€‚

So in in a nutshellï¼Œ bird is better at classificationï¼Œ but not so good at generating thingsã€‚

On the other handï¼Œ GP's Uniidirectional autoag approach is better for generating textã€‚

 but then it's not so good at tasks that require the whole input sequence as input like classificationã€‚

And the argument here is that part is the best of both worlds because it combines bothã€‚Noã€‚

Bt directionional bidirectional encodeder and the GT Unidirectional decoderã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_3.png)

So here's a figure from the paper like illustrating that conceptã€‚

Bd is essentially a bird encoder plus GPT decoderï¼Œ plus plus some additional noise transformations that they came up withã€‚

Yesï¼Œ so for referenceï¼Œ that's the bird model hereï¼Œ the bidirectional encoderã€‚ So if you recallã€‚

 it's about masking andã€‚Yeah essentially masking tokens and the task is to predict what words or tokens go in there in these gapsã€‚

 So these gaps recall they could be either masks or they could be random words or it could be the same wordã€‚

 but the task is that the model has to predict what goes in thereã€‚The G modelã€‚

It's essentially an decoderï¼Œ so it's predicting the next words if you have the start of the sequenceã€‚

 the next tokenï¼Œ let's say is an Aã€‚ And then for the token Aï¼Œ it should predict B and so forthã€‚

 So it's predicting the input one at a timeã€‚And part is combining those twoã€‚

 So you have this bydirectional encoder on the leftï¼Œ which feeds into this autoregressive decoderã€‚

And I'm not sure it's a couple of videos to goã€‚ But if you think about thisã€‚

 this should maybe remind you of somethingã€‚ So it has been quite some time agoã€‚

 but it's essentially what they are doing hereã€‚

![](img/50bdc4c07944a3d458b08422d300e1cb_5.png)

My opinionï¼Œ this is essentially reconstructing the original transformer modelï¼Œ rightã€‚

 because you also have in the original transformer modelã€‚

 So this is from the attentions all you need paperã€‚ You also have thisã€‚

En quarter where you have the whole Texas inputã€‚ And then you have thisã€‚Deder hereï¼Œ where you haveã€‚

Everything shifted by one positionï¼Œ so you predict the next workã€‚Of courseã€‚

 it's not identical to Bartï¼Œ but because in Bartï¼Œ they haveï¼Œ in addition to thatã€‚addition hereã€‚

 they have like this noise transformationã€‚ So here this is essentially an end quarterã€‚

 but is more like a denoising auto encoder conceptã€‚It's essentially an the concept is very similarã€‚

 if you think back of the lecture on denoing out endsã€‚

 you essentially perturb this one and try to reconstruct back the original oneã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_7.png)

So here is an example of these different noise transformations they haveã€‚ So that's from the paperã€‚

 So they haveã€‚A taskã€‚ So forï¼Œ for exampleï¼Œ if this is the originalï¼Œ So this isã€‚ABCã€‚

 let's say these are tokens and these D and E are second sentence also tokensï¼Œ So they haveã€‚

 for exampleï¼Œ token masking that's similar to Btï¼Œ they also have sentence promoting where they yeah permute these sentences so they permute one and two they have document rotation So I think in this way what do they do hereã€‚

Yeahï¼Œ they rearrange things or sentence one and two go together token sorryã€‚

 they make new sentences essentially hereã€‚ I think also it could mean that they are rearranging all parts of the documentã€‚

They have token deletionã€‚ So going from Aï¼Œ B Cï¼Œ Now they only have aï¼Œ C and Eï¼Œ so they delete itã€‚

B And D and text in filling where the yeahï¼Œ modelï¼Œ essentially this looksã€‚Similarã€‚To talkken maskingã€‚

 So I'm honestly notã€‚Quite sure what theã€‚Differencesã€‚

 it looks like combination between token masking and token lesionã€‚ In any caseã€‚

 they say they have differentã€‚Methods hereï¼Œ different methods forã€‚

We are having noise transformation and the task is to reconstruct back the original inputã€‚

It's similarï¼Œ likeã€‚The noise in out encode where you optimize the reconstruction is where you want to reconstruct back the originalã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_9.png)

So here are someï¼Œ yeahï¼Œ studies on the performance of these noise transformationsã€‚

 So we have the base model hereã€‚And then with token maskingï¼Œ with token deletionã€‚

 with text and fillingï¼Œ now you can seeã€‚So they only try one noise transformation at a timeã€‚

 And you can see theyã€‚Perform almost equally wellã€‚ like get really good performance with only having one or twoã€‚

 Yeahï¼Œ in this caseï¼Œ two of these methods combinedã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_11.png)

å—¯ã€‚Yeahï¼Œ so for fine tuã€‚ So this is pre trainingã€‚ So the pre training is forã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_13.png)

Doing these noise transformations and then reconstructing the inputã€‚

 And now for the fine tuning for the multi for the multiple downstream tasksã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_15.png)

They have these two setupsã€‚ Soï¼Œ for instanceï¼Œ on the left hand sideï¼Œ this is for classificationã€‚

 So here you would essentiallyã€‚Predict a labelã€‚ And forã€‚Tasksï¼Œ so this is like aã€‚Clraificationã€‚

 or they call itã€‚Discimminativeã€‚Taskã€‚And hereï¼Œ this is what be a generative task because it's a generating textã€‚

So this would beï¼Œ in this caseï¼Œ language translation or machine translationã€‚

 It's just a word for having a computer doing language translationã€‚

 And here they have to have a small modificationã€‚ So this is the originalã€‚Partt modelã€‚

 where they have the pre trained encodederï¼Œ pretrain decoderã€‚ And for language translationã€‚

 they add anotherã€‚Randomly initialized end quarter that is then fine tunedã€‚ So instead ofã€‚

Feeding the original word embeddings to the encoderï¼Œ they feedã€‚

The word embeddings to the additional encoderï¼Œ and this puts outã€‚

Ebeddings that then go into this pretrained and quarterã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_17.png)

Yeahï¼Œ and this model performs pretty wellã€‚ So here are performances for discriminative tasksã€‚

 like classification tasksã€‚Andã€‚They perform almost as wellã€‚ in in these casesï¼Œ they perform bestã€‚

 So the bolt numbers are the bestã€‚ in this oneã€‚ alsoï¼Œ in some casesï¼Œ other methods are betterã€‚

But they are very competitiveã€‚ So Roberta is a type of bird model that isã€‚

 I think it's larger than Btã€‚ It's a very large modelã€‚

 Another model that we probably don't cannot discuss in this lecture because yeahã€‚

Just too many things to discussï¼Œ but yeahï¼Œ it's another bird type modelã€‚

 there are plenty of them out thereã€‚And the argument here is that partã€‚

 even though it has this unidirectal nature with a GTã€‚

 it can still perform competitively with be models that are onlyã€‚By directionã€‚

 So they are arguing that the barRT modelï¼Œ even though it is also yeah having this unidirectional autoregressive decoder is still good for discreative tasksã€‚

 But then in addition to discreminative tasksï¼Œ it's also good at generative tasksã€‚

 which is something that bird models wouldï¼Œ for exampleï¼Œ not be able to do all good atã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_19.png)

So here they actually perform best they reach the state of the artã€‚

Compared to some specialized bird modelsã€‚Yeahã€‚So moreã€‚More tasks hereã€‚

 So this is for Eli 5 abstractive question answeringã€‚ This is for conversational response generationã€‚

And this is text summarizationã€‚Alrightã€‚ so this was the part model and the best of both worlds combining B and Gã€‚

 So last and the last video aboutã€‚

![](img/50bdc4c07944a3d458b08422d300e1cb_21.png)

These popular transform modelsï¼Œ let me say a few additional thingsã€‚



![](img/50bdc4c07944a3d458b08422d300e1cb_23.png)