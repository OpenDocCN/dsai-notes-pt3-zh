# P71ï¼šæ·±åº¦å­¦ä¹ æ–°é—» #6ï¼Œ2021 å¹´ 3 æœˆ 7 æ—¥ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hiï¼Œ everyoneã€‚ So this weekï¼Œ I have a bunch of handson stuff related to neural network trainingã€‚

 Soï¼Œ first of allï¼Œ there was the new Pytorch releaseï¼Œ which I want to briefly talk aboutã€‚ And alsoã€‚

 I saw a really cool article regarding tips for training neural networksã€‚

 And since I'm currently grading your class projectsã€‚

 I thought it might be a good opportunity to go through some of these tips and tricks for yeahã€‚

 helping with neural network trainingã€‚ So with thatã€‚

 let me get started with a stuff in the news for this weekã€‚ðŸ˜Šï¼ŒYeahã€‚

 probably the coolest thing that happened in the context of deep learning this week was that there was a new pytorch released this weekã€‚

 So I usually every like that if there's a new version that comes outã€‚

 it's maybe every half a year or soï¼Œ because yeahï¼Œ there are usually a lot of cool new features and improvements that are useful in practiceã€‚

 And since this class is also focused on Pytorchï¼Œ I thought it might be worth while mentioning some of these new additionsã€‚

ðŸ˜Šï¼ŒSo theres an article that they put together with the highlights in this new 1ã€‚8 releaseã€‚

 So if you're interested this one summarizes most of the relevant changesã€‚

 a more detailed list listing all the changes and additions is on Github so you can go here if you want the more detailed listã€‚

 but it's a long list I think there were about 700 commits like feature changes of feature editions and things like thatã€‚

If I were to highlight a fewï¼Œ I have these three as I would sayï¼Œ my personal highlightsã€‚

 So one is that they knowï¼Œ yeahï¼Œ make it easier to useã€‚M D GPusã€‚

 So competition is always good for businessï¼Œ rightã€‚

 So it's good that also other cards other than than videodia are supportedï¼Œ which I thinkã€‚

It's nice for those people who haveï¼Œ let's sayï¼Œ a gaming PC or something like that with an M D graphics cardã€‚

 So now you can also use those graphics cardsã€‚ So this is supported by a library called Romã€‚

 It's like Kudaï¼Œ but for AM D GPUusã€‚There was support for AM M D GPUus beforeã€‚

 but it was like a little bit of a hassle you had to compile Pytorch yourself and was not so easyã€‚

 And now what's new is that they areã€‚Making the binaries directly available from the installer menu so you can now also yeah set it up more convenientlyã€‚

Now it's also possible to fit larger models onto GPUs without any external librariesã€‚

 so recall last week we talked about fair scale and Microsoft Deeppeã€‚

Which were two libraries Yeah providing functions to distribute a model across multiple GPusã€‚

 So there were already something like a data parallel in Pythrchã€‚ So one isã€‚Data parallelã€‚

And one is distributedã€‚Data perrollã€‚ Let's just call it a D Pã€‚ And these were methods for splittingã€‚

Many batches across multiple GPus and then training in parallelã€‚

 But this wouldn't help with a problem where a model is too big for a given GPUã€‚

 One method that we discussed was this checkpointingã€‚

 but the checkpointing would be using a single GPUã€‚

 Now there's a way that you can conveniently put a single model onto multiple GPus and then run it in parallelã€‚

 I will show you an exampleã€‚ So that's actually pretty coolã€‚

 So you don't need any external libraries for thatã€‚

 There are now some features that yeah support it directly in Pytorchã€‚

And then also what's kind of nice or what will be nice in practice is that the torch Ln Allk module got extendedã€‚

 I think in this release it was even createdã€‚ I was using a pre release versionã€‚

 so I'm not sure if it was actually created here or just modifiedã€‚ but in any caseã€‚

 so there are no additionalã€‚Yeahï¼Œ linear algebra functions that are usually only Nyã€‚

 So now we don't have to switch so often between Ny and Pi to and we train on neural networksã€‚

 If you need something like determinants or eigenvalues and so forthã€‚

 So that's also making life more convenientã€‚

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_1.png)

So regarding the pywa fineries for M D GPU supportã€‚ I checked actuallyï¼Œ the installer menuã€‚

 So good news isï¼Œ like I saidï¼Œ it's available nowï¼Œ it does not seem to be available yet though for Mac and Windows only for Linux right nowã€‚

 But I think stillï¼Œ this is pretty exciting because I'm pretty sure these will be also added over timeã€‚

 So it's probably just a matter of timeã€‚ So things are developing in a nice direction whereã€‚ðŸ˜Šã€‚

You can now yeahï¼Œ directly installã€‚The wheel my a pip forã€‚

Rockm AMD support if you are on Linux at leastï¼Œ and I'm pretty sure Mac and Windows will probably also follow some timeã€‚



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_3.png)

By the wayï¼Œ I updated one of my computers with Pych 1ã€‚8 the other dayã€‚ And yeahï¼Œ thisã€‚

 I was just looking at thisã€‚ I was really surprised about the sizeã€‚ It's now 2ã€‚48 GBã€‚

 so it's kind of impressive what's all in that libraryã€‚

 So one thing I I'm not sure if I mentioned that in classï¼Œ butã€‚ðŸ˜Šï¼ŒSo when you install Pythrã€‚

 the GPU or kuda versionï¼Œ it will bring its own kudaï¼Œ which is done to make things more convenientã€‚

 So before thatã€‚When I usedï¼Œ I meanï¼Œ like that was like 2015 things like that when I used Tensorflow before Kuda was bundledã€‚

 you had to yeah install kuda on your graphics card and then link it when you install itã€‚

 and that was usually very flakã€‚ It took a lot of effort to somehow make sure thatã€‚Yeahã€‚

 the library is using the right kuda version on your computerã€‚ So this wayã€‚

 buttonling the installer here with the ka version the Quda toolkit hereã€‚

Makes it a little bit more convenient to install thatã€‚

 So you don't have to manually blink the the Pytoch library with Kuda and K D and Nã€‚

 But that also has the downside that this is rather largeã€‚ I was just noticing itã€‚Anywaysã€‚

 so regarding the distributed training that I mentionedã€‚

 where you can run multiple one model on multiple GPUusã€‚

 So there are some MI additions that are listed hereã€‚ There were a few moreã€‚

So they have the pipeline parallelism that I mentioned last week where yeah you just put a model on different you use a sequential model and then you put different parts of that model on different GPUusã€‚

 and there's a utility to make that more convenientã€‚ I will show you in the next slideã€‚

 So that looks like we already saw yeah last week a version using the fair scaleã€‚

 And there are also for data for distributed data parallelã€‚ there are also some additionsã€‚And alsoã€‚

 theres zero redundancy optimizerï¼Œ making the optimizers more efficientã€‚ So last weekã€‚

 we also mentioned briefly these types of thingsã€‚ and they also got added directly toã€‚Yeahã€‚

 to Pytorrch to the main libraryã€‚So here here's just a brief overview of this model parallelism using multiple GPUus it's kind of related to yeah the pipeline versionã€‚

 So how that works is that you put different parts of a modelã€‚

 So if you split your model into four partsï¼Œ let's say you have four layersã€‚

So here the F represents the forward passã€‚ So let's say youã€‚Run the first layer on GPUã€‚

 or you keep it on GPU 0ã€‚ This one on GPU 1ï¼Œ GPU 2 and GPU 3ã€‚ So this wayã€‚

 you avoid exceeding the memory of GPU 0 because you have each layer on a different GPUã€‚I meanã€‚

 for a single or simple multi layer perceptionï¼Œ that's probablyã€‚Not necessaryã€‚

 but if you later have larger networksï¼Œ I will show you an example that might make senseã€‚å—¯ã€‚Yeahã€‚

 and so here we have then one layer per GPUã€‚ and then Fï¼Œ I thinkã€‚

 is supposed to stand for forward passã€‚ And then you yeahï¼Œ you have the backward pass similarlyï¼Œ butã€‚

One downside of this approach is that you can seeï¼Œ soã€‚

This one runs So the second GPU needs the results from this GPUï¼Œ rightï¼Œ So thenã€‚It's kind ofã€‚

GPU0 is keeping or staying idle while GPU1 runs and so forthã€‚So in that wayã€‚

 where everything here under the area under the curve hereã€‚

This is like idle time for some of the GPusã€‚ So in this way you don't utilize three out of the four GPUusã€‚

 and that it is basically kind of inefficient because things are sitting thereã€‚

 So one improved version is to use these types of micro batchchesã€‚ So this is like illustrated hereã€‚

 So this already starts running while these finish up basically on different micro batchchesã€‚

 and then they are passed to the other GPUã€‚ And that way you kind of run certain things somewhat in parallelã€‚

 Of courseï¼Œ theres still this bubble hereï¼Œ what they call bubble where there is inefficiencyã€‚

 but this is at least small improvement over the regular model parallelismã€‚

 So this is kind kind of called pipeline executionã€‚ I don't know whyã€‚The word pipeline is usedã€‚

 but it might be because the toolï¼Œ I think the original one was called a G pipeã€‚ but yeahã€‚

In any caseï¼Œ if you want to learn more about thisï¼Œ you can visit this link here and it will bring you directly to this website where I got this fromã€‚



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_5.png)

Actuallyï¼Œ I was kind of curious and wanted to try it and practice yesterdayã€‚

So I implemented a VG G16 networkã€‚ It would actually fit onto my into a single GPUã€‚

 But just for the sake of the exampleï¼Œ I just chose that model because it had different blocksã€‚

 or I yeah coded it up in different blocksã€‚ And then I was putting it ontoã€‚

Or into this pipeline hereã€‚So what I did is I was writing up these blocksã€‚ So this is oneã€‚

Block in my modelï¼Œ I have five of these blocksã€‚And one block is one convolutional layerï¼Œ then a reluã€‚

 another convolutional layerï¼Œ another re in the max pooling layerã€‚

 We will talk more about that in the convolutional network sectionã€‚

 But so you can what the main takeaways you can see here is that there are multiple things in one blockã€‚

 it can be pretty large if you have maybe even more channelsã€‚ This would be I meanã€‚

 not really large too large for your single GPUã€‚ but againï¼Œ it's just for the sake of this exampleã€‚

 So at the bottom now I'm showing you how I use this new pipe in torch distributed pipeline syncã€‚

There are actually a few moreã€‚Set up steps that I'm skipping here because they didn't fit on the slideã€‚

 But if you' are interested in checking out this code that I wasï¼Œ yeah playing around withã€‚

 I put it here up on Gitthub so you can actually check that out and see the full code and how it looks likeã€‚

 So what I'm doing here isã€‚I'm putting the first block on the first GPUã€‚ So Kuda 0 is the first GPUã€‚

 I was actually running something else on GPU 1ã€‚ So I was skipping it and I was putting things on GPU2 hereã€‚

 GPU 3ï¼Œ and then the classifier back on to GPU 0ã€‚ Why did I do thatã€‚

So one of the reasons is that you have to have the dataã€‚

And the code that you're running on the same GPUã€‚ So if you have your input data on GPU 0ã€‚

 you should also have your first block on GPU0ã€‚ And then if you have your class labels for computing the accuracy on GPU 0ã€‚

 Then you also want to have the output of the model on GPU 0ã€‚ That is what I was doing hereã€‚

 So I didn't have to rewrite any of my other codeï¼Œ I could just plug that in into my existing codeã€‚

So then yeah I'm using so I'm having these blocks now on different GPUs it has nothing to do with the pipe yet and what I'm doing is I'm using the sequential in torch to merge these blocks into one model and then I'm using just this model inside the pipeline here with8 microbetches and then I'm providing it here for Adam or to Adam as the parameters and this is essentially it and then this one would actually then runã€‚

The pipeline mechanismï¼Œ like putting it every everything together and work kind of smoothlyã€‚And yeahã€‚

 it worked well in practiceã€‚ I must say the training time in this case was about half as slow compared to running everything on a single GPUã€‚

 But againï¼Œ speed is not the goal hereã€‚ So we are not using this pipeline mechanism to speed up the modelã€‚

 We are using it because we assume that this single model would not fit onto a single GPUã€‚

 So this way we make possible a training of models that are exceeding a regular GPU memoryã€‚

 So I think this is actually pretty cool and I willã€‚

Pretty surely use that in one of my research projectsã€‚



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_7.png)

Yeahï¼Œ okayï¼Œ there was one more modification I had to make to the codeã€‚

 So the modification is in these two linesã€‚ So this whole thing is my training follow over the epochs and over the mini batchesã€‚

 And here I had to add just this line because now the model is not returning the logictsã€‚

 It's returning R refã€‚Referenceï¼Œ a reference object to this R PCC thingyã€‚ And I you just had toã€‚

 yeahã€‚Get the local valueï¼Œ and then everything else should workã€‚ Againã€‚

 the full code example is here if you are interested to checkï¼Œ check it outã€‚



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_9.png)

Alrightï¼Œ so this was just a few highlights from this Pytoch 1ã€‚

 a release some other topics I saw this paper on archive this week virus AmnesISã€‚

 So I just thought it might be a cool new fun simple data for benchmarking yeah deep learning modelsã€‚

 So if you are just interesting in developing a new model anyone to see how well it performs and different data sets Amness is one Scipher is one nowadays also imagenet and things like that so but I thought this is here a interesting approach because I haven't seen anything like that beforeã€‚

Soã€‚This is an emist like dataset setã€‚ So if people say something emsã€‚

 they usually mean a data that is approximately 10 classes and 50000 or 60000 training imagesã€‚

And hereï¼Œ what they have is a data set concerning virus data or malware soã€‚There are 10 classes hereã€‚

 So class label 0 and 9ã€‚ There's only one class that is not malwareã€‚ It's called bannerwareã€‚

 iss like the good stuff hereã€‚ that's just like a regular ex fileã€‚ I guess puttyã€‚

 I think it's I think it's a terminalã€‚Program for Windowsï¼Œ but I'm not sure anywaysã€‚

 And then there areã€‚Some other viruses hereï¼Œ I think X windowsã€‚Andã€‚Insteadã€‚

 so they had like a C Z file and some information about thatã€‚ And instead of putting thatã€‚

Up as the CV fileï¼Œ what they did is theyã€‚I meanï¼Œ it's at least what I understood is theyã€‚

Converted to grayscale images using the intermediate net B PM text formatã€‚

 And then they created AskI raw images and some did something else to make it or to create Jpe filesã€‚

 So how it looks like is this oneã€‚ So you can see basically these yeahï¼Œ JpeEC images of these filesã€‚

 And this one would beï¼Œ for exampleï¼Œ the good one and all the other ones are examples of malwareã€‚

 I haven't seen like approach like this where people yeahï¼Œ kind of convert textã€‚ I meanã€‚

 code is essentially textã€‚ or hereï¼Œ I think they usedã€‚The check sums of the filesã€‚ But yeahã€‚

 there was an interesting idea like convertingï¼Œ let's say text or a string into an image so that they can use convolutional networksã€‚

 I'm actually not sure if that makes it betterã€‚ So if you have a text file and was some weirdã€‚

Input format that doesn't look like a tableular datasetï¼Œ but also not like anything elseã€‚

 not sure whetherre converting into a JPEG really is meaningfulã€‚

 but I thought that was interesting they actually got pretty good performance or acuracies on that datasetã€‚

 so I think it's probably workingã€‚

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_11.png)

Yeahï¼Œ I also saw this cool article this week called Simple considerations for Simple People Building fancy neural networksã€‚

 So this is an article highlighting some of the things you should think about when building a model or also when debuing a modelã€‚

 And since you are now working on your class projectsã€‚

 I thought it was a nice summary really that I wanted to share with you like explaining going over some tips in practiceã€‚

 So the first step would be putting aside machine learning and simply focusing on your dataã€‚

 So before you start applying a model to your dataã€‚

 just take a look at your data and get a feeling for itã€‚ for exampleã€‚

 looking at whether the labels are balanced like getting a feeling for thatã€‚

 whether they're balanced or not like on the same ratio for the class labelsã€‚ðŸ˜Šï¼ŒSame proportionsã€‚

 So and are the gold labels that you are there gold labels that you don't agree withã€‚

 So gold labels hereï¼Œ they mean ground truthã€‚ So are the test set or training set labels that are provided making senseã€‚

 Are there are some maybe that are wrongã€‚ Do you agree with themã€‚Then alsoã€‚

 it's always useful to know how the data was obtainedï¼Œ whetherï¼Œ for exampleã€‚

 there could be possible sources of noise in this processã€‚ So it can tell you an anecdoteã€‚

 for exampleï¼Œ there was forï¼Œ I like it was like one or two years agoã€‚

 There was a face image data set from IBMã€‚ I think it was called the Iã€‚F or something like thatã€‚

It was yeahï¼Œ a phase data set shared by I IBMã€‚ and I applied to get access to this data setã€‚

 and I was very excited to get itã€‚ but thenã€‚ðŸ˜Šï¼ŒI wrote I read the readme file and was actually pretty disappointed because they shared a lot of yeah attributes that can be used for developing machine learning systemsã€‚

 but then all these attributes were not assigned by a human like not labeled they were actually predicted using another machine learning model So if you develop a machine learning model based on labels predicted by another model you would assume you can never actually do better than an existing model and that I find not very useful for the development of machine learning methods necessarily so in a way it's also good to think a little bit about or find out how labels were obtainedã€‚

And yeahï¼Œ then also thinking about the preprocessing stepsã€‚

 maybe there are some things that were not idealã€‚s just good to yeah think about these how diverse are the examples And also what rulebased algorithm would perform decently on this problemã€‚

 So likeï¼Œ can you think of a rule-based algorithm in terms of simple decisions that you might think of that can actually perform well and then you can implement these simple rules as a baseline because you want to see that the model that you are training performs better than any simple baselineã€‚

 for exampleï¼Œ So for exampleï¼Œ rule-based algorithm on email spam data could beã€‚

If the subject header is capitalized in all capsï¼Œ then it's spamï¼Œ something like thatã€‚

 something simpleï¼Œ maybeã€‚

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_13.png)

Yeahï¼Œ so while the previous slide focused on understanding the data set so the second recommendation is about yeah understanding how difficult the task isã€‚

 so how well would standard baselines perform on that taskï¼Œ for exampleã€‚

 a classification task So if you are working on a classification task it's always a good ideaã€‚

To also include a simple baseline like logistic regression as a linear classifier because let's say you suppose you develop a fancy neural networkã€‚

 let's say a convolutional network that gets 95% accuracy on MNSã€‚By itselfã€‚

95% might sound super impressiveï¼Œ but then let's say you run logistic regressionã€‚

 And usually if you do that on Amnessã€‚You would see you will get around 93% performance already well accuracy alreadyã€‚

 so 93% accuracy and 95% accuracy that's not very different in that wayã€‚

 maybe the CNNN is not as good as you might think So having a baseline like logistic regression is actually a good idea because maybe even you find if you train CNNN or multi perceptron it's like only 90% And if you only know the number 90% accuracy and you tell someone I have a model that can predict let's saywritten digits and Mist with 90% and a person doesn't know Ms just looks at some examples person might be super impressed but thenã€‚

If you think about a logistic regression as a linear model already being able to do thatã€‚

 then it's actually not that impressive after allã€‚Yeahï¼Œ so just going through these points hereã€‚

 how would a random predictor performï¼Œ especially in classification problems and the data sets can be unbalanced I'm actually a little bit confused by this point I think the main point here is asking how well a random predictor might perform that is actually something I also always ask myself So if you have a binaryclass problem yeah the a random prediction would be 50% right and for three classes it's 33% so it's always good to keep that in mind by a random prediction might look likeã€‚

å—¯ã€‚What I'm a little bit confused about is this point if the data set is unbalancedã€‚ I meanã€‚

 this shouldn't really affect the random predictorï¼Œ rightï¼Œ so becauseã€‚If you haveï¼Œ let's sayã€‚90%ã€‚

Sam and 20%ã€‚Notã€‚Smï¼Œ and this is your data setã€‚Binary dataã€‚

 the random prediction predict predictor would be 50% accurate right because for each email it would either randomly predict spam or not spam so you get a 50% accuracy And if the data is balancedã€‚

 it's also 50% for random predictionï¼Œ so it should always be 50% if you have a binary data set or around 50% let's sayã€‚

I think what the person here was more interested in in terms of data set can be unbalanced is majorityã€‚

Classã€‚Predicctorã€‚Because then it gets interestingï¼Œ for exampleï¼Œ on a balanced binary data setã€‚

 a majority class predictor would get 50% accuracyï¼Œ but if you have a data set like thatã€‚

 where you have 80% spam and 20% not spamã€‚Then some classifier that would always predict the majority class if the test has these tablesã€‚

 you would already getã€‚80% accuracyï¼Œ just by always predicting spamã€‚ So in that caseã€‚

 if you develop a machine learning system that getsï¼Œ let's sayã€‚82% accuracyã€‚

 that's not very impressive because by always predicting the majority classï¼Œ you can already get 80%ã€‚

 So that's actually also something worthwhile to think about when you develop a modelã€‚

So then the next thing isï¼Œ what would the loss look like for a random predictorã€‚

So this is also related toã€‚One of the quiz questions I had last week recall when I asked you to compute the cross entropy for the worst case scenario which was yeah infinityã€‚

 but let's say if you have a case in where you always yeah predict randomly so random classifierã€‚

Would have a loss ofã€‚1 over the number of class labelsï¼Œ rightã€‚

So this would be the maximum or the worst caseã€‚ and if you don't have an infinity a0 in the lockã€‚

The totally random prediction would beã€‚Actuallyï¼Œ it should probably let's sayï¼Œ number of classesã€‚

Times minus lock one over the number of classesã€‚ So this would be a totally random predictionã€‚

 I think it's for binaryã€‚ it's around what would it be like 2ã€‚3 somethingã€‚

 but I'm not sure I would have to double checkã€‚But it's always good to double check these numbers when you do training and you see the loss gets stuck at a certain pointã€‚

To see what would be the loss for a random predictionã€‚ And this wayã€‚

 you can find out whether the model was learning something at all or notã€‚Yeahã€‚

 some other points are what are the best metrics for measuring performanceï¼Œ for exampleã€‚

 precision recall accuracyã€‚Yeahï¼Œ andã€‚yeahï¼Œ the last one is also interestingã€‚

 Are there architectures in my neural network tool box that would be good to model the inductive bias of the dataã€‚

 Inductive biasï¼Œ we briefly talked about thisã€‚ It's like one of these machine learning jargon termsã€‚

For exampleï¼Œ yeah that goes actually back to the previous point understanding the data setã€‚

 so for instanceï¼Œ if you have time series data it might be better to use a recurrent neural network or if you have image data it might be good to use a conversion network and so forthã€‚

 so also youre understanding your data setã€‚

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_15.png)

Yeahï¼Œ assume now you understand the data set and also the difficulty of the taskã€‚

But you find that the model is not performing wellï¼Œ for exampleã€‚

 the loss that you observed during training the cross entropy loss is the one that you would expect for a random predictionã€‚

 So there might be something wrong with your code and that might be a good opportunity for doing some model debugging to see whether there is some errorã€‚

 So one common thing is usually if you have a learning rate that is way too large or way too smallã€‚

 then you will probably see that yeah the model is not learning wellã€‚If youã€‚Personallyã€‚

 that's what I try firstã€‚ If you tweak the learning rateï¼Œ thoughã€‚

 but and you still find things are not performing wellã€‚

 what I also like to do is I try to overfit a small batch of examplesã€‚

 There was actually really helpful in a recent research project where we had actually an issue where we used this technique to diagnose itã€‚

 soã€‚When you only do training on a single mini batchã€‚

 what you would expect is that the model should overfi to this mini batchã€‚

 and you should get like a small loss like around 0ã€‚ So how you can do that is just by addingã€‚

When you do in the data loadï¼Œ if you do like forã€‚Let's sayã€‚Btch andã€‚Labelsã€‚Inã€‚Data lotã€‚

When you do that and after the firstã€‚Training loopï¼Œ I would add a break at the endã€‚ So in this wayã€‚

 you only do it onceï¼Œ and then you do it for multiple epochsã€‚ And this wayã€‚

 you can try to overfit to a single mini batchã€‚ And you should get around zero lossã€‚ If notã€‚

 there is something weirdï¼Œ usuallyã€‚ So here are achieved from that articleã€‚ few things to look intoã€‚

For exampleï¼Œ one could be you forgot to call model evil in the valuation modeã€‚

 We will talk more about thisã€‚ actually when we talk about dropout this weekã€‚

 But yeah that would be one case or when you haveã€‚Well you forgot to model0 gradã€‚

 so you will otherwise yeah accumulate if you doï¼Œ if you don't do thatã€‚

 you will accumulate the gradients from the previous roundsã€‚

 And then if you update that could also be some issues because you don't do gradients you do something with accumulated gradientsã€‚

Or there's maybe something wrong with the preprosing of the inputsã€‚

 maybe you forgot to normalize themã€‚Another common one is using the wrong arguments in the lossã€‚

 So recall when we said that the cross entropy loss in Pywach yeah works with the logicitsã€‚

 So if you provide it with the softm probabilities it will give you very weird resultsã€‚

 so that's also one thing to check forã€‚Well here initialization doesn't break symmetry that is related to the question I asked you when I said why don't we initialize all the weights in a neural network to0 that is because we have symmetry that was actually a nice piazza post by one of the students who answer this question so I recommend you to check out this post because yeah it was very well written and essentially explaining the problem very well so that is actually a symmetry if we initialize the network with all zeros or all small numbers that are the same So what we want is we want to have small randomly different numbers to initialize the network to break this symmetryã€‚

Or here another one is some parameters are never called during the forward passã€‚

 so if you don't use them during the forward passï¼Œ they won't be updated because they are not part of the computation graphã€‚

Another one that happens often to me is if I have two models simple in a single scriptã€‚

Actually happened to me when I was implementing this pipeline parallelismã€‚ So I had my atomã€‚

Optimizerï¼Œ or let's sayï¼Œ I think weï¼Œ yeahï¼Œ we haven't covered Adamã€‚

 let's say we I had my SGD optimizerã€‚ We will cover Adam laterã€‚

 and you pass it the model parameters like thisã€‚Andã€‚I had two models in that notebookã€‚

 I had just called it modelã€‚ And the other 1ï¼Œ I called modelã€‚Parallelã€‚

But when I was training the parallel modelï¼Œ I usedã€‚Slow model parameters in the SGDã€‚

 So it was not actually updating the right model parametersã€‚ And then the training didn't workã€‚

 And I was wonderingï¼Œ why is this model not trainingã€‚ And then I sawï¼Œ ohï¼Œ Iã€‚

 I didn't give it the right parametersã€‚Yeahï¼Œ another one here is the learning rate is taking funky values like zeroã€‚

 so there's something called learning rate scheduleã€‚



![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_17.png)

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_18.png)

![](img/c90346fb7fe09d21c2cb8bbf34c9dd33_19.png)