# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P109ÔºöL13.9.3- PyTorch ‰∏≠ÁöÑ AlexNet - ShowMeAI - BV1ub4y127jj

All rightÔºå in this videoÔºå let's now talk about Alex Nt trained on Cypher 10„ÄÇ

 So just to recap how Alexnett looks likeÔºå It's this architecture where we have„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_1.png)

Its input 96 channelsÔºå then 256 going from 90s„ÄÇ So the input imagesÔºå first of all„ÄÇ

 are 224 times 224 times3„ÄÇThe firstÔºå after the first conversionÔºå it's 96 channels„ÄÇThen 256 channels„ÄÇ

386Ôºå I think there should be 384 like I mentioned I think this was a typoÔºå then another 384„ÄÇ

 256 and then these fully connected layers and you can see these fully connected layers are really huge so there's 40964096 and then 1000 class tables I shortened the first layer of it from 96 to 64 but overall I try to keep the same architecture that I implemented notice that we are not training it on imagenet we are training it only on Cypher 10 images which are smaller so„ÄÇ

And that wayÔºå I made some small adjustments to yeah account for the smaller size of the input images„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_3.png)

SoÔºå here„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_5.png)

Everything should be the same as before„ÄÇ I'm actually not rerunning this now because this might take a while„ÄÇ

 I will show you at the end how long it took to train this architectureÔºå so„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_7.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_8.png)

I renders on a GPUÔºå also„ÄÇSo first of allÔºå I let me only go through the changes that are different from the Lyette that I showed you before„ÄÇ

 One change is that we now have Cypher 10 images instead of amist images„ÄÇ

 So what I'm doing here also is I'm making the Cypher 10 images larger than they really are„ÄÇ

 So Cypher 10 is 32 times 32„ÄÇ But I'm making them largerÔºå like 70 times 70„ÄÇ

 because otherwise I get problems with the dimensions„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_10.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_11.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_12.png)

Because otherwiseÔºå we go back to the overview here„ÄÇ Otherwise here at this point„ÄÇ

 the height and width will be too small„ÄÇEssentiallyÔºå they will be nonexistentÔºå basically„ÄÇ

 So in that way„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_14.png)

I have to resize the input images to make this network architecture work on Cypher 10 and what I'm doing is also to make the network a little bit more robust towards overfitting„ÄÇ

 All I'm doing is I'm doing a random crop„ÄÇSo I'm randomly cropping a 64 by 64 region from the 70 times 70 input„ÄÇ

 that's during training„ÄÇ And every time I'll be a different random crop„ÄÇThen yeah„ÄÇ

 the regular totensor thing„ÄÇ And then here I'm normalizing the channels„ÄÇ

The color channels to be the pixels to be centered at 0„ÄÇ

 you could technically also compute the standard deviation and mean from the actual image data„ÄÇ

 but here what I'm doing is I'm just yeah normalizing them in a very simple way that the pixels are centered at 0 and have in the range between -1 and1 that is the same as I did with MN before„ÄÇ

 but now I have three color channels So that's why I'm doing it like this if I just go back to my MN1 I only had one color channels so I only had 0„ÄÇ

5 here„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_16.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_17.png)

All right„ÄÇNotice that I'm only doing this random crop for the training„ÄÇ

 I don't want any randomness when I apply my model toÔºå let's say„ÄÇ

 new data during prediction and validation and test data„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_19.png)

I' mimic mimicking new data because I use that data to evaluate how well my model might perform on new data„ÄÇ

 and it wouldn't make sense to have some random cropping for new data„ÄÇ if you have„ÄÇ

 let's say customerÔºå let's say at the airport or you want to do face recognition to see whether the person at the airport is the same as in the passport or something like that„ÄÇ

 you do don't want to just do random„ÄÇCrops and predictions„ÄÇ you just want to do center crop„ÄÇ

 So the random crop is just to make the model more robust towards small perturbations„ÄÇÂóØ„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_21.png)

YeahÔºå so here we use then the center crop instead of this random crop„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_23.png)

The rest is all the same as before„ÄÇI have my images my batches are 256 in sizeÔºå three color channels„ÄÇ

64 pixels high and 64 pixels wide„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_25.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_26.png)

And also have 10 classes„ÄÇHere's the Alexnet architectureectÔºå so„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_28.png)

Can maybe be„ÄÇHighlight the blocks„ÄÇ So this is one convolutional block„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_30.png)

And there's another one„ÄÇJust separating them out here„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_32.png)

SoÔºå these are„ÄÇThe convolutional box hereÔºå I only have max poolingÔºå essentially„ÄÇOhÔºå sorry„ÄÇ

 what I did was not ideal„ÄÇ I should have done it like this max pullinging„ÄÇ

 comfortably re max pullinging„ÄÇNoticed here is„ÄÇNo pulling in between„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_34.png)

So yeahÔºå just yeahÔºå separated them in two„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_36.png)

Blocks„ÄÇYou can't see it here might be visible here so you can see„ÄÇMax pullingÔºå max pullinging„ÄÇ

 Max pullingingÔºå but no pullinging here„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_38.png)

Try to implement that here„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_40.png)

So that is how the Alexnet looks like„ÄÇ That is now the feature extractor part„ÄÇ

 So this whole part here is the feature extractor part„ÄÇ So I call it again featuresÔºå so„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_42.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_43.png)

It's essentially that part„ÄÇUp to„ÄÇUp to here„ÄÇAnd then the right side here is the multily perceptioncept part„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_45.png)

So hereÔºå I call it again classifier„ÄÇ I also have an adaptive average pooling here„ÄÇ

 So what that will do is it will„ÄÇTake whatever„ÄÇSize that isÔºå whatever size comes out of here„ÄÇAnd„ÄÇ

Poulls it such by averaging such that the output feature met will be 6 by 6„ÄÇ

 So in that way I can rely on that that what comes in here in the linear is 256 times 6 times 6 because here I know that the number of channels that comes out of here is 20256„ÄÇ

 So if I go back hereÔºå that's 256Ôºå and I know the size is 6 by6„ÄÇ

 I'm saying it by average pooling So if I give it larger images if I give it images that are I 150 times 150„ÄÇ

 It will also work the same way„ÄÇ I don't have to adjust my code„ÄÇ However„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_47.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_48.png)

If I have images that are smallerÔºå than whatever I have as input„ÄÇLike the original Se 10„ÄÇ

 this will not work because„ÄÇAt this stageÔºå we may have a three by3 or something like that„ÄÇ

 and this doesn't upscale„ÄÇ It only downtscalesÔºå but it doesn't upscale„ÄÇ

 So what I'm saying is if I would change it to 30 times 30 everything we have to also change that to something like smaller than 30„ÄÇ

 would you get the idea if this is smaller than 70 times 170„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_50.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_51.png)

This will fail because what comes out of here is not 3 6 by 6„ÄÇ It might be 3 by 3Ôºå for instance„ÄÇSo„ÄÇ

 adaptive average pullingdding will„ÄÇDownsize something to a common size„ÄÇ

 And this will allow me then also to receive larger inputs„ÄÇSo in that way„ÄÇ

 the architecture is not so picky towards the exact resolution„ÄÇ

If I wanted to train it'd say on a different data set„ÄÇOkayÔºå so„ÄÇHereÔºå that's my fully connected part„ÄÇ

 So I'm using dropboard in between Did they use dropod actuallyÔºå or did I„ÄÇ

 I think I just edit it because I had the overfitting„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_53.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_54.png)

YeahÔºå I think so„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_56.png)

I don't think they had it in the paperÔºå but I would have to double check„ÄÇ I can't see it here„ÄÇ

 I would have to double check in the paperÔºå but I am pretty sure that something I did because I was having massive issues with overfitting„ÄÇ

 I will show you next„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_58.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_59.png)

AlrightÔºå so here we have the features„ÄÇThen we have this average pooling that brings everything down to 6 by 6„ÄÇ

 And then hereÔºå this is my„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_61.png)

Fully connected 1„ÄÇ I could have used a flatteningÔºå actuallyÔºå could have used„ÄÇThe where is it„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_63.png)

Could have used the flatten„ÄÇBut wellÔºå I didn't„ÄÇ I think the flatten is also relatively new„ÄÇ

 So I sometimes forget to use it because it's just recently added„ÄÇAnd then I'm calling my classifier„ÄÇ

 which gives me my predictions„ÄÇ And that isÔºå then hereÔºå everything is the same as before„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_65.png)

I'm using againÔºå the same SGD with momentum„ÄÇRunning rate scheduler„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_67.png)

Same thing„ÄÇ But now it takes much longer to train„ÄÇ So it tookÔºå I was running it for 200 epochs„ÄÇ

 and it took long time„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_69.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_70.png)

Took approximatelyÔºå yeahÔºå three hoursÔºå almost„ÄÇAnd what was also interesting is„ÄÇ

 so usually what I do is when I train the networkÔºå I take a look at at this output here„ÄÇ

 So it's maybe also as a tip for your homework 3„ÄÇ When you train the network„ÄÇ

 you want to see that the validation goes up and you want to see that the loss goes down here it's going up„ÄÇ

 I would give it maybe sometimes a little bit more timeÔºå sometimes it goes a little bit up„ÄÇ

 but you want to see at least maybe45Ôºå6 epochs„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_72.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_73.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_74.png)

That the loss decreases„ÄÇ if you see it does not increase or it even increases„ÄÇ

 I would actually stop the training because then it's usually that you have a learning rate that is too large„ÄÇ

Or other issues„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_76.png)

So but yeah here I sawÔºå okayÔºå it's training well„ÄÇ and then I sawÔºå okayÔºå what's going onÔºå68Ôºå66„ÄÇ

67 what something' is weird here„ÄÇ And somehow I honestly almost wanted to stop it here„ÄÇ

 but I was busy with other things and I just let it continue training because even down to 62„ÄÇ

 I didn't stare at it the whole time because I was like 25 minutes„ÄÇ

 I was doing something else in the meantime„ÄÇ And then I was quite surprised when I looked at the plot„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_78.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_79.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_80.png)

I had actually this double descent phenomenon that I talked about in a previous lecture„ÄÇ So„ÄÇ

 first of allÔºå the loss went down and then wind went up again and then with this double descent over the epoch„ÄÇ

 So the epochwise double descentÔºå I saw it went down again„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_82.png)

And then it stayed here„ÄÇ So you can also see the same thing for the training validation accuracy„ÄÇ

 It improvesÔºå then it comes worseÔºå and then it improves again„ÄÇ But overall„ÄÇ

 you can also see the huge degree of overfitting 20% that's I had already drop out„ÄÇ

 but it didn't help that much„ÄÇ And that's yeah still overfitting„ÄÇ

 So one thing that might help is maybe adding more dropout or also adding more data augmentation„ÄÇ so„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_84.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_85.png)

If you go here„ÄÇIf you go hereÔºå instead of just random cropping„ÄÇ

 you could also yeah do something with a colour jitta and rotation and things like that that might also help with the overfitting„ÄÇ

 So in practiceÔºå if you find overfitting like that„ÄÇ But yeah„ÄÇ

 I was a little bit under time pressure to get this code finished for the lecture„ÄÇ

 And it already took a long time„ÄÇ So I didn't want to rerun everything to reduce overfitting„ÄÇ

 might be an interesting exerciseÔºå if you are interested to try this out„ÄÇ Okay„ÄÇ

 so then Im just looking at some results„ÄÇ So I visualize them„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_87.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_88.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_89.png)

So notice here I'm doingÔºå I'm using this unnmalized function that I implemented somewhere in my helper function„ÄÇ

 I think I have it here„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_91.png)

YouSee„ÄÇüòîÔºåLet me maybe double check where I implemented that and let me scroll up„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_93.png)

Unorized data sets„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_95.png)

Right here„ÄÇ So here the un normalmalizing is essentially undoing my normalization with this 0„ÄÇ5„ÄÇ

 So here I do the normalization„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_97.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_98.png)

And hereÔºå Im undoing my normalization so that I can plot the images„ÄÇ So I'm essentially multiplying„ÄÇ

By the standard deviation„ÄÇAnd then adding the mean„ÄÇ And I do that for every channel„ÄÇ

 That's just a very compact way of writing this„ÄÇ You don't have to understand this in detail„ÄÇ

 It's just very efficient„ÄÇ I meanÔºå I wanted to just write this in one lineÔºå instead„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_100.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_101.png)

Writing too much code here„ÄÇ AlrightÔºå so essentiallyÔºå this will anize„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_103.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_104.png)

So I have to provide the information that I used for normalising„ÄÇ So I provide them„ÄÇ

The same way And just as the same thing as the normalizingÔºå but inverse like„ÄÇReeversing it„ÄÇAll right„ÄÇ

 so„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_106.png)

Then I give it also the class dictionary with the names so that in my plot hereÔºå my show examples„ÄÇ

 but I can actually see the namesÔºå the predictions„ÄÇ can see actuallyÔºå And this looks cool here„ÄÇ

 it's funny„ÄÇ it actually it gets dogs and cats wrong„ÄÇ

 So the predict label is dog and the true labor as a cat„ÄÇ

 which I think justifies the fact that so many people work on cats versus dog classifiers because it's actually a challenging problem„ÄÇ

 Okay„ÄÇüòä„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_108.png)

And then againÔºå my confusion matrix here just to look at what they get„ÄÇCurrently wrong„ÄÇ

 You can see it's kind of interesting„ÄÇ The dog versus cats is a category that is often almost often wrong in this data set„ÄÇ

 It's kind of funny„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_110.png)

AlsoÔºå ship an airplaneplan here„ÄÇAnd frorog and cat OhÔºå fck and cat„ÄÇ I meanÔºå not that similar„ÄÇ

 but okay„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_112.png)

YeahÔºå so that is how Alex networks„ÄÇ And it's essentially overallÔºå it's the same as Lyette„ÄÇ

 except that we have no color channels and the network architecture isÔºå of courseÔºå bigger„ÄÇ

 So if I can screw up againÔºå takes longer to train and it'sÔºå yeah I can see it's„ÄÇ

 it's much bigger than our Lyette before which was just yeahÔºå smaller network„ÄÇ Allright„ÄÇ

 that's it then for the lecture„ÄÇ I mean okayÔºå let me just briefly go to this one because I have it„ÄÇ

 So here I„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_114.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_115.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_116.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_117.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_118.png)

Trained another CNN„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_120.png)

If you're interestedÔºå try to get better accuracy„ÄÇBut it is just aÔºå yeah„ÄÇ

 see an end with patch norm and leaky Relu and to drop out trying to reduce the overfitting„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_122.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_123.png)

But yeahÔºå you can see it was not that much better„ÄÇ It gets someÔºå somewhat better accuracy„ÄÇ

 It trains only 3030 minutes instead of three hoursÔºå which is also an improvement„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_125.png)

Doesn't have this double descentÔºå but it's still overfitting by a lot„ÄÇ

 It gets 10% training accuracyc thoughÔºå which is quite surprising„ÄÇ So okayÔºå yeahÔºå it's another„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_127.png)

NetworkÔºå it gets confuses a deal with an airplane„ÄÇ It's also interesting„ÄÇ Also„ÄÇ

 the cat where the stock is wrong here again„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_129.png)

YeahÔºå also cat with stock„ÄÇ It's apparently a challengingÔºå challenging category„ÄÇ Alright„ÄÇ

 so that's it then for this lecture in the next lecture„ÄÇ

 we will take a more detailed look into different neural network architectures„ÄÇ I meanÔºå these two„ÄÇ

 Lynette and Alexnet„ÄÇ

![](img/eafc166425d5eda00acc7c3a2db181bf_131.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_132.png)

Ire really like beginner architectures„ÄÇ They are quite old„ÄÇ

 but there are more powerful architectures„ÄÇ And yeahÔºå that's the topic of the next lecture„ÄÇ



![](img/eafc166425d5eda00acc7c3a2db181bf_134.png)