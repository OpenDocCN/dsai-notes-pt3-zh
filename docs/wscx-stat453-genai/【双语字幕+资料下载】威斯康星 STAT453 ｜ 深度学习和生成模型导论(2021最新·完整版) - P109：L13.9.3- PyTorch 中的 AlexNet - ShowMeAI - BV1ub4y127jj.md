# P109ï¼šL13.9.3- PyTorch ä¸­çš„ AlexNet - ShowMeAI - BV1ub4y127jj

All rightï¼Œ in this videoï¼Œ let's now talk about Alex Nt trained on Cypher 10ã€‚

 So just to recap how Alexnett looks likeï¼Œ It's this architecture where we haveã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_1.png)

Its input 96 channelsï¼Œ then 256 going from 90sã€‚ So the input imagesï¼Œ first of allã€‚

 are 224 times 224 times3ã€‚The firstï¼Œ after the first conversionï¼Œ it's 96 channelsã€‚Then 256 channelsã€‚

386ï¼Œ I think there should be 384 like I mentioned I think this was a typoï¼Œ then another 384ã€‚

 256 and then these fully connected layers and you can see these fully connected layers are really huge so there's 40964096 and then 1000 class tables I shortened the first layer of it from 96 to 64 but overall I try to keep the same architecture that I implemented notice that we are not training it on imagenet we are training it only on Cypher 10 images which are smaller soã€‚

And that wayï¼Œ I made some small adjustments to yeah account for the smaller size of the input imagesã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_3.png)

Soï¼Œ hereã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_5.png)

Everything should be the same as beforeã€‚ I'm actually not rerunning this now because this might take a whileã€‚

 I will show you at the end how long it took to train this architectureï¼Œ soã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_7.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_8.png)

I renders on a GPUï¼Œ alsoã€‚So first of allï¼Œ I let me only go through the changes that are different from the Lyette that I showed you beforeã€‚

 One change is that we now have Cypher 10 images instead of amist imagesã€‚

 So what I'm doing here also is I'm making the Cypher 10 images larger than they really areã€‚

 So Cypher 10 is 32 times 32ã€‚ But I'm making them largerï¼Œ like 70 times 70ã€‚

 because otherwise I get problems with the dimensionsã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_10.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_11.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_12.png)

Because otherwiseï¼Œ we go back to the overview hereã€‚ Otherwise here at this pointã€‚

 the height and width will be too smallã€‚Essentiallyï¼Œ they will be nonexistentï¼Œ basicallyã€‚

 So in that wayã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_14.png)

I have to resize the input images to make this network architecture work on Cypher 10 and what I'm doing is also to make the network a little bit more robust towards overfittingã€‚

 All I'm doing is I'm doing a random cropã€‚So I'm randomly cropping a 64 by 64 region from the 70 times 70 inputã€‚

 that's during trainingã€‚ And every time I'll be a different random cropã€‚Then yeahã€‚

 the regular totensor thingã€‚ And then here I'm normalizing the channelsã€‚

The color channels to be the pixels to be centered at 0ã€‚

 you could technically also compute the standard deviation and mean from the actual image dataã€‚

 but here what I'm doing is I'm just yeah normalizing them in a very simple way that the pixels are centered at 0 and have in the range between -1 and1 that is the same as I did with MN beforeã€‚

 but now I have three color channels So that's why I'm doing it like this if I just go back to my MN1 I only had one color channels so I only had 0ã€‚

5 hereã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_16.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_17.png)

All rightã€‚Notice that I'm only doing this random crop for the trainingã€‚

 I don't want any randomness when I apply my model toï¼Œ let's sayã€‚

 new data during prediction and validation and test dataã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_19.png)

I' mimic mimicking new data because I use that data to evaluate how well my model might perform on new dataã€‚

 and it wouldn't make sense to have some random cropping for new dataã€‚ if you haveã€‚

 let's say customerï¼Œ let's say at the airport or you want to do face recognition to see whether the person at the airport is the same as in the passport or something like thatã€‚

 you do don't want to just do randomã€‚Crops and predictionsã€‚ you just want to do center cropã€‚

 So the random crop is just to make the model more robust towards small perturbationsã€‚å—¯ã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_21.png)

Yeahï¼Œ so here we use then the center crop instead of this random cropã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_23.png)

The rest is all the same as beforeã€‚I have my images my batches are 256 in sizeï¼Œ three color channelsã€‚

64 pixels high and 64 pixels wideã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_25.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_26.png)

And also have 10 classesã€‚Here's the Alexnet architectureectï¼Œ soã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_28.png)

Can maybe beã€‚Highlight the blocksã€‚ So this is one convolutional blockã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_30.png)

And there's another oneã€‚Just separating them out hereã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_32.png)

Soï¼Œ these areã€‚The convolutional box hereï¼Œ I only have max poolingï¼Œ essentiallyã€‚Ohï¼Œ sorryã€‚

 what I did was not idealã€‚ I should have done it like this max pullingingã€‚

 comfortably re max pullingingã€‚Noticed here isã€‚No pulling in betweenã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_34.png)

So yeahï¼Œ just yeahï¼Œ separated them in twoã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_36.png)

Blocksã€‚You can't see it here might be visible here so you can seeã€‚Max pullingï¼Œ max pullingingã€‚

 Max pullingingï¼Œ but no pullinging hereã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_38.png)

Try to implement that hereã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_40.png)

So that is how the Alexnet looks likeã€‚ That is now the feature extractor partã€‚

 So this whole part here is the feature extractor partã€‚ So I call it again featuresï¼Œ soã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_42.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_43.png)

It's essentially that partã€‚Up toã€‚Up to hereã€‚And then the right side here is the multily perceptioncept partã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_45.png)

So hereï¼Œ I call it again classifierã€‚ I also have an adaptive average pooling hereã€‚

 So what that will do is it willã€‚Take whateverã€‚Size that isï¼Œ whatever size comes out of hereã€‚Andã€‚

Poulls it such by averaging such that the output feature met will be 6 by 6ã€‚

 So in that way I can rely on that that what comes in here in the linear is 256 times 6 times 6 because here I know that the number of channels that comes out of here is 20256ã€‚

 So if I go back hereï¼Œ that's 256ï¼Œ and I know the size is 6 by6ã€‚

 I'm saying it by average pooling So if I give it larger images if I give it images that are I 150 times 150ã€‚

 It will also work the same wayã€‚ I don't have to adjust my codeã€‚ Howeverã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_47.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_48.png)

If I have images that are smallerï¼Œ than whatever I have as inputã€‚Like the original Se 10ã€‚

 this will not work becauseã€‚At this stageï¼Œ we may have a three by3 or something like thatã€‚

 and this doesn't upscaleã€‚ It only downtscalesï¼Œ but it doesn't upscaleã€‚

 So what I'm saying is if I would change it to 30 times 30 everything we have to also change that to something like smaller than 30ã€‚

 would you get the idea if this is smaller than 70 times 170ã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_50.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_51.png)

This will fail because what comes out of here is not 3 6 by 6ã€‚ It might be 3 by 3ï¼Œ for instanceã€‚Soã€‚

 adaptive average pullingdding willã€‚Downsize something to a common sizeã€‚

 And this will allow me then also to receive larger inputsã€‚So in that wayã€‚

 the architecture is not so picky towards the exact resolutionã€‚

If I wanted to train it'd say on a different data setã€‚Okayï¼Œ soã€‚Hereï¼Œ that's my fully connected partã€‚

 So I'm using dropboard in between Did they use dropod actuallyï¼Œ or did Iã€‚

 I think I just edit it because I had the overfittingã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_53.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_54.png)

Yeahï¼Œ I think soã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_56.png)

I don't think they had it in the paperï¼Œ but I would have to double checkã€‚ I can't see it hereã€‚

 I would have to double check in the paperï¼Œ but I am pretty sure that something I did because I was having massive issues with overfittingã€‚

 I will show you nextã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_58.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_59.png)

Alrightï¼Œ so here we have the featuresã€‚Then we have this average pooling that brings everything down to 6 by 6ã€‚

 And then hereï¼Œ this is myã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_61.png)

Fully connected 1ã€‚ I could have used a flatteningï¼Œ actuallyï¼Œ could have usedã€‚The where is itã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_63.png)

Could have used the flattenã€‚But wellï¼Œ I didn'tã€‚ I think the flatten is also relatively newã€‚

 So I sometimes forget to use it because it's just recently addedã€‚And then I'm calling my classifierã€‚

 which gives me my predictionsã€‚ And that isï¼Œ then hereï¼Œ everything is the same as beforeã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_65.png)

I'm using againï¼Œ the same SGD with momentumã€‚Running rate schedulerã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_67.png)

Same thingã€‚ But now it takes much longer to trainã€‚ So it tookï¼Œ I was running it for 200 epochsã€‚

 and it took long timeã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_69.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_70.png)

Took approximatelyï¼Œ yeahï¼Œ three hoursï¼Œ almostã€‚And what was also interesting isã€‚

 so usually what I do is when I train the networkï¼Œ I take a look at at this output hereã€‚

 So it's maybe also as a tip for your homework 3ã€‚ When you train the networkã€‚

 you want to see that the validation goes up and you want to see that the loss goes down here it's going upã€‚

 I would give it maybe sometimes a little bit more timeï¼Œ sometimes it goes a little bit upã€‚

 but you want to see at least maybe45ï¼Œ6 epochsã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_72.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_73.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_74.png)

That the loss decreasesã€‚ if you see it does not increase or it even increasesã€‚

 I would actually stop the training because then it's usually that you have a learning rate that is too largeã€‚

Or other issuesã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_76.png)

So but yeah here I sawï¼Œ okayï¼Œ it's training wellã€‚ and then I sawï¼Œ okayï¼Œ what's going onï¼Œ68ï¼Œ66ã€‚

67 what something' is weird hereã€‚ And somehow I honestly almost wanted to stop it hereã€‚

 but I was busy with other things and I just let it continue training because even down to 62ã€‚

 I didn't stare at it the whole time because I was like 25 minutesã€‚

 I was doing something else in the meantimeã€‚ And then I was quite surprised when I looked at the plotã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_78.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_79.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_80.png)

I had actually this double descent phenomenon that I talked about in a previous lectureã€‚ Soã€‚

 first of allï¼Œ the loss went down and then wind went up again and then with this double descent over the epochã€‚

 So the epochwise double descentï¼Œ I saw it went down againã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_82.png)

And then it stayed hereã€‚ So you can also see the same thing for the training validation accuracyã€‚

 It improvesï¼Œ then it comes worseï¼Œ and then it improves againã€‚ But overallã€‚

 you can also see the huge degree of overfitting 20% that's I had already drop outã€‚

 but it didn't help that muchã€‚ And that's yeah still overfittingã€‚

 So one thing that might help is maybe adding more dropout or also adding more data augmentationã€‚ soã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_84.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_85.png)

If you go hereã€‚If you go hereï¼Œ instead of just random croppingã€‚

 you could also yeah do something with a colour jitta and rotation and things like that that might also help with the overfittingã€‚

 So in practiceï¼Œ if you find overfitting like thatã€‚ But yeahã€‚

 I was a little bit under time pressure to get this code finished for the lectureã€‚

 And it already took a long timeã€‚ So I didn't want to rerun everything to reduce overfittingã€‚

 might be an interesting exerciseï¼Œ if you are interested to try this outã€‚ Okayã€‚

 so then Im just looking at some resultsã€‚ So I visualize themã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_87.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_88.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_89.png)

So notice here I'm doingï¼Œ I'm using this unnmalized function that I implemented somewhere in my helper functionã€‚

 I think I have it hereã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_91.png)

YouSeeã€‚ðŸ˜”ï¼ŒLet me maybe double check where I implemented that and let me scroll upã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_93.png)

Unorized data setsã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_95.png)

Right hereã€‚ So here the un normalmalizing is essentially undoing my normalization with this 0ã€‚5ã€‚

 So here I do the normalizationã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_97.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_98.png)

And hereï¼Œ Im undoing my normalization so that I can plot the imagesã€‚ So I'm essentially multiplyingã€‚

By the standard deviationã€‚And then adding the meanã€‚ And I do that for every channelã€‚

 That's just a very compact way of writing thisã€‚ You don't have to understand this in detailã€‚

 It's just very efficientã€‚ I meanï¼Œ I wanted to just write this in one lineï¼Œ insteadã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_100.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_101.png)

Writing too much code hereã€‚ Alrightï¼Œ so essentiallyï¼Œ this will anizeã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_103.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_104.png)

So I have to provide the information that I used for normalisingã€‚ So I provide themã€‚

The same way And just as the same thing as the normalizingï¼Œ but inverse likeã€‚Reeversing itã€‚All rightã€‚

 soã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_106.png)

Then I give it also the class dictionary with the names so that in my plot hereï¼Œ my show examplesã€‚

 but I can actually see the namesï¼Œ the predictionsã€‚ can see actuallyï¼Œ And this looks cool hereã€‚

 it's funnyã€‚ it actually it gets dogs and cats wrongã€‚

 So the predict label is dog and the true labor as a catã€‚

 which I think justifies the fact that so many people work on cats versus dog classifiers because it's actually a challenging problemã€‚

 Okayã€‚ðŸ˜Šã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_108.png)

And then againï¼Œ my confusion matrix here just to look at what they getã€‚Currently wrongã€‚

 You can see it's kind of interestingã€‚ The dog versus cats is a category that is often almost often wrong in this data setã€‚

 It's kind of funnyã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_110.png)

Alsoï¼Œ ship an airplaneplan hereã€‚And frorog and cat Ohï¼Œ fck and catã€‚ I meanï¼Œ not that similarã€‚

 but okayã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_112.png)

Yeahï¼Œ so that is how Alex networksã€‚ And it's essentially overallï¼Œ it's the same as Lyetteã€‚

 except that we have no color channels and the network architecture isï¼Œ of courseï¼Œ biggerã€‚

 So if I can screw up againï¼Œ takes longer to train and it'sï¼Œ yeah I can see it'sã€‚

 it's much bigger than our Lyette before which was just yeahï¼Œ smaller networkã€‚ Allrightã€‚

 that's it then for the lectureã€‚ I mean okayï¼Œ let me just briefly go to this one because I have itã€‚

 So here Iã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_114.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_115.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_116.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_117.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_118.png)

Trained another CNNã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_120.png)

If you're interestedï¼Œ try to get better accuracyã€‚But it is just aï¼Œ yeahã€‚

 see an end with patch norm and leaky Relu and to drop out trying to reduce the overfittingã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_122.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_123.png)

But yeahï¼Œ you can see it was not that much betterã€‚ It gets someï¼Œ somewhat better accuracyã€‚

 It trains only 3030 minutes instead of three hoursï¼Œ which is also an improvementã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_125.png)

Doesn't have this double descentï¼Œ but it's still overfitting by a lotã€‚

 It gets 10% training accuracyc thoughï¼Œ which is quite surprisingã€‚ So okayï¼Œ yeahï¼Œ it's anotherã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_127.png)

Networkï¼Œ it gets confuses a deal with an airplaneã€‚ It's also interestingã€‚ Alsoã€‚

 the cat where the stock is wrong here againã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_129.png)

Yeahï¼Œ also cat with stockã€‚ It's apparently a challengingï¼Œ challenging categoryã€‚ Alrightã€‚

 so that's it then for this lecture in the next lectureã€‚

 we will take a more detailed look into different neural network architecturesã€‚ I meanï¼Œ these twoã€‚

 Lynette and Alexnetã€‚

![](img/eafc166425d5eda00acc7c3a2db181bf_131.png)

![](img/eafc166425d5eda00acc7c3a2db181bf_132.png)

Ire really like beginner architecturesã€‚ They are quite oldã€‚

 but there are more powerful architecturesã€‚ And yeahï¼Œ that's the topic of the next lectureã€‚



![](img/eafc166425d5eda00acc7c3a2db181bf_134.png)