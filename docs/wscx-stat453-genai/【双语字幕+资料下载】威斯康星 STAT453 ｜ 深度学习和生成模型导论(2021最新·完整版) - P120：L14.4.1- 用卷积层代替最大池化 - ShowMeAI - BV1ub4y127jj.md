# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P120ÔºöL14.4.1- Áî®Âç∑ÁßØÂ±Ç‰ª£ÊõøÊúÄÂ§ßÊ±†Âåñ - ShowMeAI - BV1ub4y127jj

YesÔºå so in the previous videos we talked about the VGG16 architecture and the residual network„ÄÇ

 Now I want to talk about a topic that is somewhat related to convolutional neural network architectures„ÄÇ

 it's not super important in terms of let's say implementing better performing convolutional networks„ÄÇ

 but I think it really helps like solidifying the understanding of how convolutions work„ÄÇ

 So the first topic I want to talk about is replacing max pooling with convolutional errors„ÄÇ

 and then in the with video after that I want to talk about replacing„ÄÇ

Fully connected layers by convolutional layers„ÄÇ So both videos will be essentially about simplifying a neural network architecture by essentially getting rid of max pudding and getting rid of fully connected layers„ÄÇ

 It won't result necessarily in a better performanceÔºå but it isÔºå I think„ÄÇ

 just an interesting thought experiment„ÄÇ

![](img/e7d0429cf947e56233b7cbdee0b404dc_1.png)

So the first video here or the this two video series it will be focused on replacing max pooling„ÄÇ

 So there's action architecture that is called the all convolutional network„ÄÇ

 It comes from a paper that is already seven years old„ÄÇ It's entitled Striving for simplicity„ÄÇ

 The all convolutional network„ÄÇ So here the authors proposed replacing max pooling with a convolution layer that has a stride of2„ÄÇ

 And this is sometimes also called strideed convolution„ÄÇüòäÔºåSo nowÔºå traditional„ÄÇ

Neurural network or convolution network„ÄÇ We have usually a convolution layer with„ÄÇStriide equals1„ÄÇ

 And then we have max pullinging„ÄÇUsually to buy two max pooling„ÄÇAlso with a slide of two„ÄÇAnd then„ÄÇ

 we have„ÄÇA convolutional„ÄÇLayer again with spread of oneÔºå and we„ÄÇContinue like that„ÄÇ

 And usually the convolutional layersÔºå they preserve the size because we have a straight of one„ÄÇ

And the max pooling will reduce the size twofold„ÄÇ It will half the size„ÄÇ

 So the size that comes out of it is one half„ÄÇ if we haveÔºå let's say„ÄÇ

 a two by two max pooling with a strip of two„ÄÇAnd„ÄÇYou can maybe also write the stone„ÄÇ

So the kernel size is usually2 by 2 and„ÄÇJust's right by two„ÄÇ

 And this helps us also yeah achieving a little bit of this location in variance„ÄÇ However„ÄÇ

 it's not essential to have thatÔºå so„ÄÇYou can technically get rid of this and then just increase the stride here by two or here„ÄÇ

 and you will have the same effect that you reduce the size by one half„ÄÇSo„ÄÇ

I don't want to go into too much detail hereÔºå but if you look at the experimental results when I recall correctly„ÄÇ

 they found that the performance slightly decreases when you get rid of this max poolingÔºå however„ÄÇ

 how you can get an even better performance than before is by replacing this max pooling by convolution layer„ÄÇ

 that mean get rid of this two here first„ÄÇSo we have a one here„ÄÇ

 and then you can have a convolutional layer here„ÄÇWith„ÄÇStriide equals 2„ÄÇ

 And this will also have this one over half effect that it will yeah decrease the size by half„ÄÇ

 but instead of using the max pullinging„ÄÇYou have the convolution which has weight parameters and you can think of it as something like a learnable pooling at least that's what they argue here in that paper„ÄÇ

 So it will be of course a little bit more expensive because you have no more parameters but in that way you can also simplify the network in terms of yeah having it look s away just saying okay we only use convolutions we don't use anything else if that's desirable of course you still need the activation function but you don't need pooling layers for example„ÄÇ

I recall there was a talk by Jo Hinton„ÄÇ I don't recall„ÄÇExact words„ÄÇ

 But I remember Jo Hinton said something along the lines of that Max pullinging was one of the biggest mistakes in computer vision in terms of convolal networks„ÄÇ

 personallyÔºå I think it's actually not that bad„ÄÇ Max pullinging works quite well„ÄÇ

 Yes not necessarily urge to get rid of this„ÄÇ So it's here more like a thought experiment how you can„ÄÇ

Potentially simplify a neural network architecture„ÄÇ

And then you talking about simplifying so you can also get rid of the fully connected layer using convolutions„ÄÇ

 I will talk more about that in the next video and another way to get rid of fully connected layers is by using global average pooling so„ÄÇ



![](img/e7d0429cf947e56233b7cbdee0b404dc_3.png)

I'm not sure if this was the paper that kind of proposed that in the first place„ÄÇ I guess it was not„ÄÇ

 but this was nonetheless a nice figure„ÄÇ So usually we use these fully connected layers„ÄÇ

 like shown here on the left hand sideÔºå we use these fully connected layers to map from the convolutional layer onto the number of desired class labels„ÄÇ

 So usually we have a fixed number of class labels„ÄÇAnd„ÄÇ

In order to get from the other feature map size to the number of class table„ÄÇ

 we use these fully connected layers„ÄÇSo on the first fully connected here layer here would have would have as the input size„ÄÇ

 the reshaped feature app size„ÄÇ So things this will become more clear when I show you the code example„ÄÇ

 This is something you have seen before„ÄÇ It's not new„ÄÇ

 It may look a little bit different because I haven't shown that inÔºå in a figure here„ÄÇ

 but this is essentially when we reshape„ÄÇSo that it matches„ÄÇ

The size here in the fully connected layer„ÄÇAnd„ÄÇWhat comes out of here„ÄÇ

 let's call that H like some hidden dimension„ÄÇ It goes into the next fully connected layer„ÄÇ

 And here the output is then the number„ÄÇOf classes„ÄÇOf courseÔºå yeah„ÄÇ

 this fully connected part is usually very intensive in terms of lots of parameters„ÄÇ

You can simplify this by using global average pooling„ÄÇSo here what you do is essentially„ÄÇ

First of allÔºå you assume that the number of channels„ÄÇIn the last„ÄÇCon layer is equal to the number of„ÄÇ

Number of„ÄÇClasses„ÄÇAnd„ÄÇGlobal average pooling works essentially like that„ÄÇ

 You have heard about average poolingÔºå which is just taking the average value and global here means of the whole feature map„ÄÇ

 So you would technically just use the whole feature map here„ÄÇAnd then average over all these values„ÄÇ

And this will be then the single value here„ÄÇ And then you do the same thing for the next one„ÄÇ

 So you just average over the whole feature maps„ÄÇ And that is also a way to get rid of fully connected layers„ÄÇ

And by the wayÔºå here on the left hand sideÔºå this is actually not necessary to have two fully connected layers„ÄÇ

You could technicallyÔºå also reshape„ÄÇInto this one and this one takes out the number of class tables„ÄÇ

It can be good to have a fully connected layer„ÄÇ It's like additional parameters to learn„ÄÇ

 but it's also not essential„ÄÇ You can just have all the parameters learned in this convolutional network„ÄÇ

And or con part„ÄÇ and then have the global average pooling„ÄÇ

 And I will show you next code example how that works„ÄÇ And in the next„ÄÇ

Video I will also show you how we can replace that„ÄÇBy an equivalentÔºå convolutional operation„ÄÇ

 actuallyÔºå there are two ways to do that„ÄÇSo personallyÔºå again„ÄÇ

 I would say there is no reason to get rid of the fully connected layers„ÄÇ

 They are just fine and many or even most architectures use fully connected layers„ÄÇ

 but if you wanted toÔºå you could technically also get rid of it„ÄÇ Allright„ÄÇ

 let me show you in the next video a code example of an implementation of this all con network„ÄÇ

 and I think this will probably make certain things a little bit more clear that I just yeah was drawing here„ÄÇ

 Allright„ÄÇ

![](img/e7d0429cf947e56233b7cbdee0b404dc_5.png)

![](img/e7d0429cf947e56233b7cbdee0b404dc_6.png)

![](img/e7d0429cf947e56233b7cbdee0b404dc_7.png)