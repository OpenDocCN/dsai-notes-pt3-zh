# P10ÔºöL1.5- Êú∫Âô®Â≠¶‰π†Á¨¶Âè∑Âíå‰∏ìÊúâÂêçËØç - ShowMeAI - BV1ub4y127jj

Yeah‰∫Ü„ÄÇYeah now let me go over some of the necessary machine learning notation and jargon„ÄÇ

 something that we will be using in this class over and over again and that is also commonly used in the machine learning and deep learning communities„ÄÇ

So yeahÔºå on revisiting the term supervissed learningÔºå What is supervissed learning„ÄÇ

 It's about learning a function that maps some input X„ÄÇTo some output Y„ÄÇ

 where x are the features and y are the targets„ÄÇ SoÔºå yeah„ÄÇ

 like you remember from the previous videosÔºå the featuresÔºå or we also call them„ÄÇObservations„ÄÇ

They are usually vectors or yeah for exampleÔºå the flower dimensionsÔºå sal length„ÄÇ

 sal width petal length and petal width or pixels of an image and so forthÔºå by the way„ÄÇ

 if I write a vectorÔºå I would then if I do or use handwriting„ÄÇ

 use this a little error to denote a vector and I will try to use if I don't forget an underscore and a capital letter to refer to a matrix„ÄÇ

And XÔºå if I don't use any of this would be a scalar value„ÄÇ So hereÔºå though„ÄÇ

 I was just writing x as a scalarÔºå but features in practice can be also vectors„ÄÇ So anyways„ÄÇ

 we are getting ahead of ourselves here a little bit„ÄÇ

 So the next term here on this slide is structured dataÔºå which we discussed us in the previous video„ÄÇ

 So the term structured data usually refers to data in the format of yeah tables can also also think of it as„ÄÇ

Tular„ÄÇüòîÔºåData usually comes in the form of databasesÔºå like if you know SQL databasesÔºå for example„ÄÇ

 or if you think of Excel spreadsheets or CSV files„ÄÇ

 this would be a typical example of a structured data„ÄÇThen yeah unstructured data„ÄÇ

 this is usually what we refer to as the raw dataÔºå for exampleÔºå imagesÔºå the pixels and image„ÄÇ

 audio signals or text sentences like documentsÔºå and this is usually something that yeah deep learning works with„ÄÇ

So and before deep learningÔºå we usually needed extensive feature engineeringÔºå as I explained before„ÄÇ

 where we extracted features from raw data sources„ÄÇSo here on the right hand side I„ÄÇ

Just wanted to show you another example of yeah raw image input versus„ÄÇ

Something that requires feature engineering„ÄÇ SoÔºå for exampleÔºå here you have a portrait of image„ÄÇ

 And let's say the task is„ÄÇFace recognition where you want to„ÄÇMatch multiple faces„ÄÇ

 So here I don't have multiple people where we want to compare whether it's the same person or not„ÄÇ

 So we don't have really face recognition matching„ÄÇ However„ÄÇ

 what I'm illustrating here is that we can extract features from this image„ÄÇ

 So here this is a traditional method„ÄÇ using like facial key point extraction„ÄÇ

 So or extracting so-called face landmarks„ÄÇAnd on this would be a feature engineering step like simplifying the image such that it only now contains these key points„ÄÇ

 I think there are around 67 or so key points„ÄÇ So we reduced the complexity of the dataset by having a high dimensional data set„ÄÇ

 So if this isÔºå for exampleÔºå200 by 200 pixel image„ÄÇWe would have„ÄÇ40000 pixels or 40000 features„ÄÇ

 including background and everything„ÄÇ and we here we just simplified it and only have„ÄÇ

 I think it's 67 features I could be wrong you would have to double check„ÄÇ

 so here this is actually a function I implemented based on„ÄÇLirry called Deup„ÄÇ

 which implements this face extraction„ÄÇHere I'm just yeah I implemented a Python Reper function to make it a little bit simpler to use any case„ÄÇ

 so here this would be an example of manual feature engineering„ÄÇ

 like reducing the complexity of the problem by extracting a smaller number of features from this raw input„ÄÇ

OkayÔºå so moving on so here on this slide some more terminology„ÄÇ

 so a training set we usually refer to it as D and a training set comes in pairs of these observations the so-called training examples or features and the target labels and like I said I will be using this error notation„ÄÇ

 if I do handwriting to refer to vectors so in if I don't do handwriting like in the slides using computer font I will make them bold„ÄÇ

And yeahÔºå for scalrsÔºå I won't use„ÄÇBt front„ÄÇThen a training data consists of n training examples„ÄÇ

 so that's the number of examples in a training set„ÄÇAnd I will be using the square„ÄÇ

Brackt notation here as a superscript to refer to the training data points index„ÄÇ

 So if I write something like„ÄÇ10„ÄÇThen this means that's the 10th yeahÔºå training feature„ÄÇ

Feature vector in the training setsÔºå I'm referring to the10th training example to the feature vector of the 10th training example„ÄÇ

So„ÄÇThen in practiceÔºå so usually the data setÔºå the training data comes from some data generating distributionion„ÄÇ

 something could be a natural physical phenomenon that has created our data and so forth„ÄÇ

 so there exists some unknown function that associates our feature vectors with some label„ÄÇ

And the goal in machine learning is yeah in supervised learning is to yeah predict the labels„ÄÇ

 So how we do that is by approximating this unknown function„ÄÇ and yeah„ÄÇ

 traditionally this is called the hypothesis nowadays I think the hypothesis term is a little bit dated we can just call that model we don't have to use fancy terms like hypothesis we can just call that a machine learning model So it's like a more common way to refer to it„ÄÇ

 So you can think of the machine learning model as a function that approximate approximates this unknown function„ÄÇ

 So it willÔºå for exampleÔºå in the classification example return predicted label So we can write the predicted label as y head for example or we can just call it target target t or output O always maybe a little bit tricky because it almost looks like a zero but yeah these are all equivalent notations„ÄÇ

 So in this way is age you can think of„ÄÇIt is the machine learning model„ÄÇYeah„ÄÇ

 and classification again„ÄÇ So we have the modelÔºå which here is a function that maps some M dimensional input vector„ÄÇ

 So this would be our features mapping those„ÄÇYeah to some targets here„ÄÇ

Where the targets in the term in the context of classifications are class tables„ÄÇ

 so we can have up to k class tables„ÄÇ So usually K is larger or larger equal to two„ÄÇ

 We have at least two classes in the case of the iris dataset set that I've shown you earlier„ÄÇ

 There were actually three possible classes on Cosa„ÄÇVsy color„ÄÇAnd oricaÔºå for example„ÄÇ But yeah„ÄÇ

 of course„ÄÇReally depends on the dataset set„ÄÇ there is no limit to the number of classes you can have„ÄÇ

 AlsoÔºå yeah in regressionÔºå we have a input vector that is m dimensional„ÄÇ

 and we map that to a continuous to real valued number„ÄÇYeahÔºå more about the feature vector„ÄÇ

 So I said before the feature vector is n dimensional„ÄÇ so if you think back„ÄÇOf the Iis example„ÄÇ

 where we had the Sepal„ÄÇleength„ÄÇS Paul„ÄÇWith„ÄÇPol„ÄÇleength„ÄÇAnd pital„ÄÇWithÔºå so we had„ÄÇFor features„ÄÇ

And thenÔºå if I look„ÄÇAt the training at at the index„ÄÇLet's say„ÄÇfocuscus in on the„ÄÇThirdhir„ÄÇüòî„ÄÇ

Training exampleÔºå I would write it down as„ÄÇth3„ÄÇAnd then it's a vector„ÄÇ

 So it's a third feature vector„ÄÇ So in this caseÔºå we have a vector of consisting of„ÄÇFor values„ÄÇ

 So this is what I'm showing you here as the vector representation„ÄÇSo yeah„ÄÇ

 we have an m dimensional vectorÔºå and I will use the subscript„ÄÇ

Notation to refer to the feature index„ÄÇ So if I write something like„ÄÇ3 here„ÄÇ

 this would refer to the thirdÔºå the 123 to the petal length of the third„ÄÇTraining example„ÄÇ

Now in Python we start zero indexingÔºå so if we do Python coding„ÄÇ

 it might be a little bit more complicated because we should we start indexing at0 in contrast to r„ÄÇ

 So in in Python notation this would be actually the fourth„ÄÇTraining example and the fourth feature„ÄÇ

 like petal width„ÄÇYeahÔºå moving on and now here I have a feature vector again„ÄÇ

 just as on the previous slide„ÄÇ HoweverÔºå in practiceÔºå we usually have multiple feature vectors„ÄÇ

 So one feature vector per training example„ÄÇ So if you think back of the iris data set where we had up to 150„ÄÇ

Training examples„ÄÇ Let's use the letter N here over the rows to referÔºå yeahÔºå to the training„ÄÇ

Data points„ÄÇ And then for each oneÔºå we had a feature that's called it feature 1Ôºå feature 2„ÄÇ

 feature 3 and feature 4„ÄÇSo we have this table here„ÄÇ So you can also use„ÄÇLet's see„ÄÇüòîÔºåDot dot dot„ÄÇ

 feature„ÄÇÂóØ„ÄÇSo„ÄÇThen each data pointÔºå So each„ÄÇObservation is a feature vector„ÄÇ

 So in linear algebra notationÔºå we usually yeah write a vector as a column„ÄÇ However„ÄÇ

 we arrange the vector here as like a row vector„ÄÇ So a row vector would be referring to one data point as one observation„ÄÇ

 So we use a transpose on that one to„ÄÇI'll refer to this one here as the first„ÄÇYeah„ÄÇ

 data point in the table„ÄÇ It's a little bit confusing the way I've written it here„ÄÇ

 because now I'm using the sub„ÄÇ This is just because I have the trans post here„ÄÇ

 And then it's a little bitÔºå yeahÔºå a little bit„ÄÇSquiedÔºå so actually„ÄÇ

 I could have written it as transpose„ÄÇOneÔºå for example„ÄÇ

 it would have been clear or maybe so maybe ignore this notation here on the right hand side„ÄÇ

 I have the more YeahÔºå extended notation where I'm referring„ÄÇ

With a superscript to the data point index„ÄÇSo this would be all the first data point in the first row„ÄÇ

And then the subscript refers to the feature index„ÄÇSo if I pickÔºå for exampleÔºå this case here„ÄÇ

 this would beÔºå what is maybeÔºå with unfortunateÔºå Let me use this one here„ÄÇ This is the„ÄÇ

Second feature of the n data point„ÄÇ So this would be if I go right hereÔºå so would be this one here„ÄÇ

 So this scalar value„ÄÇ So usually machinery will use a design matrix I will use bold letters and if I handr this I will try to use an underscore code to make sure to indicate that this is a matrix because with handwriting it can be unclear if I do an x it's unclear if I don't yeah deote it further whether it's a vector or matrix and so forth So again for vectors I will use this notation and for matrices this notation and I may forget„ÄÇ

 but I will try not to„ÄÇOkay yeah just to summarize this is a structured data set how it would look like in machine learning in the first couple of lectures we will actually be using with structured data before we go into working with unstructured data so in the first couple of lectures just as a warm up using simpler deep learning related methods like single layer neural networks and then multilayer neural networks that are fully connected„ÄÇ

 they will be working with also structured data and then when we talk about convolutional networks and recurrent neural networks which are the real innovations in deep learning then we will be using unstructured data„ÄÇ

AlrightÔºå so yeahÔºå just to recap M is the„ÄÇPeture„ÄÇÂóØÔºå numberumber„ÄÇSo we have up to M features„ÄÇ

 and N is the„ÄÇYeah„ÄÇNumber„ÄÇOf training„ÄÇExamplesÔºå so I think the reason why I left the blank here„ÄÇ

 I wanted to ask you on that as an exercise„ÄÇ So in that way„ÄÇI already did the exercise for you„ÄÇ

 So hereÔºå the row„ÄÇ I think it was what I was referring to„ÄÇYeahÔºå training examples„ÄÇ

Where the columns are along„ÄÇFeatures„ÄÇAnd this is our one class label column„ÄÇAll right„ÄÇ

 so it was not very excitingÔºå but yeah I was just summarizing how we think of a data set in the context of machine learning„ÄÇ

üòäÔºåNowÔºå if we have image dataÔºå how do we deal with that„ÄÇ

 How does it fit into our yeah concept of representing data„ÄÇ

 So images are actually unstructured dataÔºå as I mentioned before„ÄÇ

 but we can actually convert it into a structured data set and if you think of image like this„ÄÇ

 this is actually a 28 by„ÄÇ28 imageÔºå which should be 700„ÄÇ84 dimensionalÔºå so we have 784 pixels„ÄÇ

Pixels are usually in the range between 0 and 255Ôºå however here„ÄÇAssume I normalized the data„ÄÇ

 we will talk more about why data normalization is useful„ÄÇ

I normalize it such that the pixel values are in the range between 0 and1„ÄÇ

 So we have a pixel like several hundred„ÄÇOf this„ÄÇThere are 84 of these pixels here„ÄÇ

 so you can actually all„ÄÇCan see this actually lower resolution one pixel here and one pixel here and so forth„ÄÇ

 So I thinkÔºå yeahÔºå you know what I mean by pixel„ÄÇ So we can actually use that as input to a traditional machine learning method in terms of„ÄÇ

Thinking of this as a structured data set by concatetnating the rows so we can make a long feature vector out of this image because„ÄÇ

 I meanÔºå this is a matrixÔºå rightÔºü So it's like„ÄÇÂóØ„ÄÇ28 by 28 matrix„ÄÇ

 but we can convert this into a vector by just concatetnating„ÄÇ So take the first row„ÄÇ

Let's see this first row here„ÄÇAnd then we have the second row„ÄÇAnd just add it here to the first row„ÄÇ

And then„ÄÇThird row„ÄÇHeading it here„ÄÇ So and if I keep doing thatÔºå I get a very long vectorÔºå in fact„ÄÇ

 a 784 dimensional vector„ÄÇ and that is what I am showing you here in the center„ÄÇ

 we're probably already wondering what these numbers are„ÄÇ and these are the pixel values„ÄÇ So the 0„ÄÇ

0 refers to a white pixel„ÄÇ So you can see in the beginning here„ÄÇThe image is all white„ÄÇ

 So we have all the white 0„ÄÇ0s here„ÄÇ and then yeah you have these gray scale and black pixels which are about here„ÄÇ

 So we kind of converted this unstructured data set into a feature vector that we can then use with traditional methods„ÄÇ

 and we will also yeah be doing that in„ÄÇA context of deep learning using simpler methods like multi layerer perceptrons„ÄÇ

 fully connected neural networksÔºå so we will start with a simple approach and then build up to it and then later use the unstructured data directly„ÄÇ

AlrightÔºå so yeahÔºå like I said before convolutional neural networks„ÄÇ

 which will be a topic yet later on in this courseÔºå convolutional neural networksÔºå they use„ÄÇ

The image data directly and usually the format for that is also a matrix more precisely it's actually a three or four dimensional tensor„ÄÇ

Here so there are two two common representations„ÄÇ one is called NCHW„ÄÇ

 We will talk more about that later when the time comes„ÄÇ

 but maybe if you are curious what it stands forÔºå H stands for the height„ÄÇ

W stands for the width of an image„ÄÇ So we have here the H times W„ÄÇ So this would be a matrix„ÄÇ

 HoweverÔºå there's usually also a color channel„ÄÇSo C stands for color in this case„ÄÇ

 we don't have color because it's a black and white image„ÄÇ So in this case it's so there's a one„ÄÇ

 So actually I'm showing you here the dimensions„ÄÇ So 28 by 28 height times width and then the color channel and then usually in deep learning what we do is we bundle a bunch of images together as input for the neural network„ÄÇ

 So in we have something called a batch size„ÄÇ So here I have 128 images where each image is„ÄÇ

3D tensor„ÄÇ So in that wayÔºå if I consider all of thatÔºå it's a 4D tensor„ÄÇ So in that way„ÄÇ

We are representing the data as ten source and this is why we need some basic linear algebra for this course while I mean we won't be using fancy things just simple matrix multiplications and dot products and I will also talk more about that later so right now I don't want to go into too much detail because convolutional networks that is topic far ahead a few weeks ahead and I don't want to talk too much about this here„ÄÇ

 I have to be honestly a little bit careful with the time because like you noticed I always try I try to avoid it but I always end up going on tangents and talking about things in too much detail too early I think so let me move on then„ÄÇ

Yeah here lastly machine learning jargon part2Ôºå so somewhat terms we will be using„ÄÇ

 I think this is more like useful as a cheat sheet„ÄÇ

 like something you can maybe refer to later when certain terms are unclear so when we say for example training a model„ÄÇ

That is the same as sayingÔºå for exampleÔºå fitting a model or parameterizing a model or learning from data„ÄÇ

Then he had the word training example„ÄÇ it's synonymous to sayingÔºå for example„ÄÇ

 training record or training instance or training sample„ÄÇHoweverÔºå yeahÔºå for example„ÄÇ

 when I teach other statistics classesÔºå I usually use often as staians we use the term sample„ÄÇ

 but I find sample can be a little ambiguousÔºå especially in the context of deep learning because if we say training sample it's not clear really whether we are referring to a single training data point or multiple training data points„ÄÇ

 So for example when I was writing my python machine learning book in 2015 in the first edition„ÄÇ

 I used the term training sample as I rememberÔºå I mean it has been six years ago„ÄÇ

 but I think I used training sample throughoutÔºå and then sometimes I noticed it was a little bit confusing do I mean now a single data point or do I mean the training data as a training sample So I data also went back and changed everything to saying training example„ÄÇ

 if I refer to a simple or single training example and then also plural I would say training examples„ÄÇ

IThink this is a little bit more clear„ÄÇ And this is also something most people in machine learning and deep learning nowadays use„ÄÇ

 So the term training example„ÄÇThen yeah feature feature is also synonymous to observation or predictor„ÄÇ

 variableÔºå independent variableÔºå inputÔºå attributeÔºå Covariate„ÄÇ

 So in other statistics classes we are often using the termÔºå for example„ÄÇ

 predictor and things like that or even covariate so here in machine learning and deep learning we usually use the term feature„ÄÇ

Then yeah we have the term targetÔºå which is synonymous to outcome ground truth„ÄÇ

 output response variableÔºå dependent variable in a specific context of classification„ÄÇ

 We also say class label or yeahÔºå just label„ÄÇSo alsoÔºå these are all the same thing„ÄÇ

 And then yeah lastly here output prediction„ÄÇ And this is what the model produces„ÄÇ This is different„ÄÇ

 Yeah from the target„ÄÇ So the target is we want to predict the target„ÄÇ So this is the ground„ÄÇ

Truth or something that is provided in the data set and the prediction or output is the thing that the model returns„ÄÇ

 and we want to usually match the target„ÄÇOkayÔºå so this is it forja jargon and the next video I want to briefly talk about a little bit more about the tools that we will be using in this course„ÄÇ



![](img/a5f179f50ae9a5cb408d8f689cdbf576_1.png)

![](img/a5f179f50ae9a5cb408d8f689cdbf576_2.png)