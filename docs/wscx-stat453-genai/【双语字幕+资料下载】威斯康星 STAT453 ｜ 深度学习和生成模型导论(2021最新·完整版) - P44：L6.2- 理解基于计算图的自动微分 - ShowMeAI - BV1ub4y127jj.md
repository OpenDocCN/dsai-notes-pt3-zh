# P44ÔºöL6.2- ÁêÜËß£Âü∫‰∫éËÆ°ÁÆóÂõæÁöÑËá™Âä®ÂæÆÂàÜ - ShowMeAI - BV1ub4y127jj

YeahÔºå in order to understand automatic differentiation„ÄÇ

 I think it's helpful to take a look at computation graphs„ÄÇ

So computation graphs are a graph of a computation or calculation that we define or execute in Pythtor„ÄÇ

So in PythgenÔºå in particularÔºå when we implement neural network modelsÔºå there will be two methods„ÄÇ

 One is the forward„ÄÇMethodÔºå and one is the„ÄÇBackward method„ÄÇ So in the forward method„ÄÇ

This one constructs„ÄÇThe graphÔºå and then backwardÔºå essentiallyÔºå computes„ÄÇGras by walking backwardÔºå so„ÄÇ

Forward graph is constructing a graph„ÄÇ If we have a graph with multiple inputsÔºå some computations„ÄÇ

 This is like the forward method„ÄÇ and in the backward methodÔºå based on the output we„ÄÇ

Compute the derivatives of the outputÔºå or the loss„ÄÇWith respect to these inputsÔºå for example„ÄÇ

 the weights that we want to update„ÄÇ So in this lectureÔºå I want to in this video„ÄÇ

 I want to briefly go over yeahÔºå what such a computation graph is and how we work with it conceptually before we see how Pytorch handles that„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_1.png)

So yeahÔºå in the context of deep learning and pythtorch„ÄÇ

 it is helpful to think about neural networks as computation graphs because neural networks are really just a chain of computations like nested computations„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_3.png)

YesÔºå so that we have a concrete example„ÄÇ Let's take a look at a activation function that is not a linear activation function„ÄÇ

 So if you recall the Adeline model„ÄÇWhat we had is or what we had was the computation„ÄÇ

That computed the net inputÔºå which was then passed on to a threshold function„ÄÇ So the net input„ÄÇ

Let's consider one weightÔºå1 feature was computed as follows„ÄÇAnd then we had an Adeline„ÄÇ

This activation functionÔºå which was just an identity function„ÄÇ So this one was just„ÄÇ

An identity function„ÄÇAnd now we are taking a look at a function where the sigma is not an identity function„ÄÇ

 So we are now taking a look at to make it a little bit more interesting at this re function„ÄÇ

So in this caseÔºå now what we have is the sigma„ÄÇIs the activation function is this„ÄÇ

Relu function that I have written down here„ÄÇ So the relu function in the context of a neural network usually takes also the net input„ÄÇ

 so„ÄÇThis is our net input„ÄÇ You can think of it as the net input„ÄÇ

And we will revisit the Relu function also in a later lecture and also talk about why it is so commonly used in deep learning„ÄÇ

 So for right nowÔºå just think of it as an activation function„ÄÇIt is almost like a linear function„ÄÇ

 but not quite„ÄÇ So the shape of this function is as follows So on the„ÄÇIn the plot here„ÄÇ

 I'm showing you how it looks like for different inputsÔºå so„ÄÇHere we have then input to the function„ÄÇ

 for exampleÔºå the net input Z„ÄÇ And here we have the output on the y axis„ÄÇ

 and the re function is defined„ÄÇYes„ÄÇAlmost like the identity function„ÄÇ So it returns Z„ÄÇThe input„ÄÇ

If z is greater than 0 and 0 otherwise„ÄÇ So it really is„ÄÇThis region„ÄÇAn identity„ÄÇFunction„ÄÇAnd here„ÄÇ

NotÔºå it's not doing anything here on the left hand side„ÄÇ So it'sÔºå it's clipped„ÄÇ

 You can think of it as a„ÄÇA clipping here„ÄÇSo why is that useful in the context of deep learning„ÄÇ

 we will talk about this moreÔºå but it is essentially yet to have a nonlinearity in the multilay internet network because if we have multiple linear functions„ÄÇ

 regular linear functionsÔºå because the combination of linear functions is' also linear functions„ÄÇ

 so in a sense our network won't be able to approximate nonlinear functions„ÄÇ

But that would be going a little bit too far ahead„ÄÇ We will talk about this more in detail here„ÄÇ

 We just care about the computation„ÄÇ So also not for this re function„ÄÇ

This one would be not differentiable in a mathematical sense„ÄÇ Yeah„ÄÇ

 just to briefly redaw the red function here for reference„ÄÇ So how it looks like is that it is 0„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_5.png)

If the input is smaller than0 and„ÄÇThe identity function of the input is larger than 0„ÄÇ

 So here we have this little kink at 0„ÄÇ And now for the left hand sideÔºå because there is no slope„ÄÇ

 rightÔºå we can immediately see„ÄÇThe derivative of this function is 0„ÄÇ On the right hand side„ÄÇ

 it's an identity function„ÄÇSoÔºå if the function is„ÄÇThisÔºå then the derivative of this function„ÄÇ

Would be oneÔºå rightÔºü So slope 1„ÄÇSo on the right hand side„ÄÇ

 we have a slope of one on the left hand sideÔºå we have a slope of 0„ÄÇ

 but here at this kink we have a problem nowÔºå if you recall from calculus classes„ÄÇ

 this would be a function that is not differentiable„ÄÇ

 So in this case we have a derivative of 0 if the input is smaller than0„ÄÇ

 a derivative of one if the input is larger than0„ÄÇBut otherwiseÔºå if the input is exactly 0„ÄÇ

 then the derivative does not exist„ÄÇ SoÔºå yeahÔºå what do we do now„ÄÇ

 So in computer science or in the computational contextÔºå we are not that picky„ÄÇ

 We just make a small adjustment„ÄÇSo we adjust the derivative as follows that it is0 if z is smaller or equal to 0„ÄÇ

 So that's just adding this smaller or equal to here to fix this issue„ÄÇ And otherwiseÔºå sorry„ÄÇ

 this should be actually a one„ÄÇIt's oneÔºå if z is greater than 0„ÄÇ

 Notice that we could have also otherwise written it the other way round„ÄÇ

 We could also write it as smaller and„ÄÇSorry„ÄÇLarger or equal„ÄÇ

 It doesn't really matter because it's so rare that we hit exactly 0 anyways that this will probably never happen in practice anyways„ÄÇ

 becauseÔºå yeahÔºå usually we work with floating point numbers„ÄÇ

 like lots of digits of the decimal point„ÄÇ So it's very rare to hit a exact zero anyways„ÄÇ

 So even though it's not preventable„ÄÇIn a mathematical senseÔºå we make in the computational sense„ÄÇ

 this little adjustment and define the derivatives as follows„ÄÇ OkayÔºå but yeah„ÄÇ

 let's move on to the computation graph„ÄÇ just to summarize„ÄÇ

 Now we have the following activation function„ÄÇ It just a summary„ÄÇ It's a multivari function„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_7.png)

Where we have three inputsÔºå we have„ÄÇThe features„ÄÇ So here we only have one feature for simplicity„ÄÇ

So let actually it annotated here„ÄÇ one featureÔºå one training example for simplicity„ÄÇ

 We have the bias unit„ÄÇAnd the weight parameterÔºå also for simplicity„ÄÇ

 only one weight parameter corresponding to the one training examples feature„ÄÇ So as you recall„ÄÇ

 we usually con calculate the net input„ÄÇFor example„ÄÇ

 in Adeline and then we would pass it to an activation functionÔºå in this case„ÄÇ

 we use this re function„ÄÇSo„ÄÇRight now we don't even have to think about yeah neural networks if you don't want to here„ÄÇ

 you can just think of it as a simple computation that we want to represent using a computation graph„ÄÇ

 so how would we do thatÔºü

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_9.png)

So here would be a computation graph of this function„ÄÇ

 So I use some intermediate variables to make this a little bit easier to read„ÄÇ

 So we define this computation as U and we define this computation as V„ÄÇSo we have three inputs„ÄÇ

 like I saidÔºå input1Ôºå the biasÔºå input2Ôºå the featureÔºå input 3Ôºå the weight„ÄÇ

 What we do is the first part is we compute u here right so this is multiplying x and w„ÄÇ

 So this is this multiplication here„ÄÇ So we define as a placeholder here„ÄÇ U is equal to W X„ÄÇ

And then what we do is we add the bias unit hereÔºå rightÔºå So we have U plus B„ÄÇ

So we have this operation here„ÄÇWhich gives us another variable V„ÄÇAnd then V goes through this„ÄÇYeah„ÄÇ

 activation function„ÄÇAlsoÔºå I could have used Z because this is like really the net input but„ÄÇ

For some reasonÔºå I used to V here shouldn't really matter„ÄÇ

 just think of it as a computation here that we then represent as such a graph„ÄÇSo yeah„ÄÇ

 in the context of deep learningÔºå we are usually interested in computing the partial derivative of the loss function with respect to the weights or the bias unit„ÄÇ

 as we've seen in the Adeline lecture last week and we can actually decompose that into smaller subs„ÄÇ

 rightÔºüUsing the chain ruleÔºå we can also decompose it into the partial derivative of the loss with respect to the activation and times the derivative of the activation„ÄÇ

With respect to the weightsÔºå and so decomposing it„ÄÇ

So here we have a simpler computation graph I haven't included a loss function here„ÄÇ

 So what we are actually looking at here is only the activation„ÄÇ So we are in this example„ÄÇ

 only walking through this part„ÄÇ So the partial derivative of the activation with respect to the weights„ÄÇ

 which we can also further decompose into substep using the chain rule„ÄÇSo that's what I'm doing here„ÄÇ

 but that's what I'm going to show you in the next couple of slidesÔºå so here I have the first step„ÄÇ

 so computing the derivative of the activation with respect to the input of the activation„ÄÇ

So here this is the term the derivative of a with respect to B„ÄÇ

 that's the first step and using this computation graph we can really make this super easy like computing this whole part by doing it in smaller steps„ÄÇ

So the next step would beÔºå for exampleÔºå computing the derivative of V with respect to B„ÄÇ

 So because we are interested in both on usually also the bias„ÄÇ

 because the bias is also like the weight a model parameterÔºå which we can„ÄÇThen alsoÔºå write us on„ÄÇ

Phos„ÄÇSoÔºå this part here would be„ÄÇSo this whole part here„ÄÇWould be„ÄÇThis one here„ÄÇ

 what we are looking at right now„ÄÇ So the next step„ÄÇIs computing„ÄÇ

Partial derivative of we with respect to„ÄÇB„ÄÇ

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_11.png)

Its sniffing in here„ÄÇYeah then we can do this also for the other parts so what's so nice about a computation graph is that we can do it step by step right so we can for each computation do a back track here and annotate our graph„ÄÇ

ÂóØ„ÄÇAnd then we can put everything together„ÄÇ So if we're interested in the derivative of a of the output with respect to the input„ÄÇ

 we just need to use the chain ruleÔºå rightÔºå so we can then write this as„ÄÇBe„ÄÇüòîÔºåB on times„ÄÇÂóØ„ÄÇÁ¨¨‰∏Ä„ÄÇ

A times the„ÄÇWe„ÄÇSo this is a thing mathematicians usually don't like„ÄÇ

 but we can actually cancel these„ÄÇ So this is why we get this one using the chain rule„ÄÇ

And we can also do this here at the bottomÔºå rightÔºå so we can do this„ÄÇFor this part„ÄÇ

 So the derivative of a with respect to„ÄÇW would be„ÄÇPutting all these together„ÄÇÂóØÔºåDA„ÄÇ

Maybe and you can technically see how things„ÄÇCancel right„ÄÇ So this is how we get this one„ÄÇ

But technicallyÔºå this is not a right way to do„ÄÇ I meanÔºå we shouldn't cancel things hereÔºå but„ÄÇI use„ÄÇ

 use it all the time as a memoryÔºå I would say memory bridgeier or something„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_13.png)

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_14.png)

AlrightÔºå so yeah this is what I've done here if you want to step through that in the slides„ÄÇ

 so I was just putting things together here„ÄÇ

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_16.png)

YeahÔºå let's now take a look at a concrete example with some actual numbers„ÄÇ

 So assume the bias unit is one„ÄÇ The weights are 2Ôºå and we have an input of„ÄÇth3e„ÄÇ

So the intermediate value U would be then 6Ôºå right„ÄÇBecause two times 3 is 6Ôºå and then 6 plus 1 is 7„ÄÇ

 And for the radio functionÔºå because it's positiveÔºå it's an identity function„ÄÇ So it's also 7„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_18.png)

NowÔºå let's differentiate„ÄÇ So the derivative of the real function if„ÄÇ

The input is greater than 0 is  oneÔºå rightÔºå because it's an identity function„ÄÇ

 So that the root of this part here would be one„ÄÇ

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_20.png)

Then yeahÔºå let's compute a derivative here and here„ÄÇ So if we look at V„ÄÇ So V„ÄÇ

 you can think of it as a sum of two functions here„ÄÇ

 So the derivatives can be then computed separately„ÄÇ So if we look atÔºå let's say the upper part here„ÄÇ

 and let's take a look at this one„ÄÇ the derivative of V with respect to B„ÄÇ So in this case„ÄÇ

 U is a constant„ÄÇ So we can ignore that and the derivative of B would be one„ÄÇ So there should be„ÄÇ

One and similarlyÔºå the other way around„ÄÇAt the bottom hereÔºå B becomes a constant„ÄÇ

 and the derivative of v with respect to U is then also oneÔºå rightÔºå So we can also put a one here„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_22.png)

Alright moving on so we have a derivative hereÔºå so what do we have hereÔºå so this is yeah just wx„ÄÇ

 if we compute the derivative of U with respect to W„ÄÇ

 then the derivative would be x and x is what is x x is3 so here we should have a three„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_24.png)

And yeahÔºå then putting everything together here„ÄÇThe partial derivative of the activation with respect to W would be3 times 1 times 1„ÄÇ

 it's equal to3 because yeah we are combining with a chain rule„ÄÇThis here„ÄÇ and similarly„ÄÇ

 we can combine„ÄÇ yeahÔºå the results for„ÄÇFor this part hereÔºå too„ÄÇSo in this wayÔºå the„ÄÇ

 the result here would be also yeah one because it's one times one„ÄÇAlright„ÄÇ

 so this is how we use computation graphs„ÄÇ and I will show you in the next video how we can implement this in Pytorch where it computes these derivatives for us„ÄÇ

 Of courseÔºå this example is a trivial one„ÄÇ It's very simple right it's something you can do in a few seconds by hand„ÄÇ

 But if you rethink of computational graphs as new networks that are more complicated with many layers and yeah lots of input features and stuff it would be tedious to code it all up in Pytorch by yourself or in nuy„ÄÇ

 So that is why there's this automatic differentiation that I will show you„ÄÇ

Where Pytoch will construct this graph here under the hoodÔºå like we can actually print out the graph„ÄÇ

 but we don't have to„ÄÇ It will do it automatically for us„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_26.png)

YeahÔºå just some more computation graphs that will be relevant or that are kind of related to deploying just to complete this video here„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_28.png)

So here here we have an example of a graph with a single path„ÄÇüòî„ÄÇ

So you can maybe also think of it as a as a simple aline or logistic regression model„ÄÇ

 which we will be covering next lectureÔºå or you can think of it as a a line as maybe simpler when you think of the sigma here as an identity function„ÄÇ

So this would be the full graph based on what I've shown you earlier„ÄÇ So in the previous slide„ÄÇ

 I showed you only„ÄÇThis part here now also think of the output where we compute the loss„ÄÇFor example„ÄÇ

 in Adeline the mean squared error between the prediction OÔºå the output and the actual class label„ÄÇ

So if we have a case like that„ÄÇThen we compute the derivative root of the loss with respect to the weights„ÄÇ

 and this can be decomposed into the loss with respect to the output„ÄÇ

 So this would be this part here„ÄÇ then the output with respect to the activation„ÄÇ

 So this would be this part and then the activation with respect to the weights„ÄÇ

 this would be this part„ÄÇ And this is what we had in the last„ÄÇSlightdes as an example„ÄÇ

 So this would be a simple caseÔºå a graph with a single path„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_30.png)

So in practiceÔºå there will be oscillatorÔºå some more yeah complicated constructs„ÄÇ for example„ÄÇ

 imagine a graph with weight sharing„ÄÇ It is kind of related to what's going on in convolutional networks that will be something that will be covered later„ÄÇ

 Here's a simple example„ÄÇImagine we have a case„ÄÇ We have a single input„ÄÇ

 and then we have a weight parameter that is shared„ÄÇ

 So that's the same weight It goes here to compute some activation„ÄÇ

 So these could be different functions„ÄÇ A1 and a2 could be different functions„ÄÇ

And then we compute some output„ÄÇAnd then also here the loss like on the previous slide„ÄÇ

 So if we have a case like thatÔºå we have to use the multi variable chain rule because yeah„ÄÇ

 there are no two paths like there's someÔºå some weight sharing going on„ÄÇ

 So if we want to update the WÔºå it depends really„ÄÇOn this path here and this pathÔºå rightÔºü

 So we have to combine them„ÄÇ So we have the multivariable chain„ÄÇ We compute the loss„ÄÇ So here„ÄÇ

 the the red of this is the same„ÄÇ You can use blue here„ÄÇ So you can see this is„ÄÇThe same here„ÄÇ

And thenÔºå we have„ÄÇ2 different derivatives„ÄÇ So we have„ÄÇThis one here„ÄÇAnd„ÄÇüòîÔºåSo here at the bottom„ÄÇ

 we would haveÔºå ohÔºå for some reason I haven't written it down„ÄÇüòîÔºåthis would be this one here„ÄÇ

And then which color didn't I OkayÔºå let's use this one and then just derivative here and then I'm running out of colour„ÄÇ

 I me use black and this one here and then putting it together with a multivariable chain rule„ÄÇüòî„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_32.png)

AllÔºå so that would be a case where we have weight sharing„ÄÇ

So here would be an example of a multi layerer perceptronÔºå which we will be covering next week„ÄÇ

 So here we also have multiple paths if we want to update certain weights„ÄÇ

 So this is a multi layerer perceptron with two hidden layers„ÄÇ

 You can see there now two levels of activations„ÄÇ One level is here„ÄÇAnd one of is hereÔºå so„ÄÇ

This is the first„ÄÇHidden layer„ÄÇ And this is the second„ÄÇHidden„ÄÇThereir„ÄÇAnd again„ÄÇ

 this part is like the same as in the previous slideÔºå the loss„ÄÇ

Let's say we are interested here in computing the derivative of the loss with respect to this weight here in W1„ÄÇ

1 in the first hidden layer„ÄÇ So in order to do thatÔºå we compute the loss„ÄÇ So the sorryÔºå the yeah„ÄÇ

 compute the loss„ÄÇ but then also the derivative of the loss with respect to the output that would be„ÄÇ

This part here„ÄÇAnd then we go hereÔºå this would be this part„ÄÇAnd then we go here„ÄÇüòî„ÄÇ

Which would be this part„ÄÇSo we are skipping across some parts later„ÄÇ and next week„ÄÇ

 we will do this step by step in more detail„ÄÇAnd then so I'm skipping here a lot of steps„ÄÇ

 so because this could also be decomposed as sub stepss with the net input and so forth„ÄÇ

 but let's not do it here„ÄÇAnd then„ÄÇWe have this part„ÄÇ

 which is the derivative root of this activationÔºå the a1 with respect to this weight„ÄÇ

 Let me use the green color here„ÄÇSoÔºå this is„ÄÇHereÔºå so we are computing„ÄÇWith respect to„ÄÇÈóÆÈ¢ò„ÄÇAlright„ÄÇ

 howeverÔºå note that there's a second pathÔºå rightÔºå so we can alsoÔºå let's use red here„ÄÇ

 Let's go the second path„ÄÇ So it's againÔºå the same length one as like this one„ÄÇ But we have now„ÄÇAlso„ÄÇ

 the second path„ÄÇOhÔºå sorryÔºå said was wrong„ÄÇThe second path would be this one„ÄÇAnd this oneÔºå see„ÄÇ

 it's actually not so easy to see what is going into here so„ÄÇ

This would be the second path if I use a different color„ÄÇ Let's say pink for the forward here„ÄÇ

 So you can see this oneÔºå this one and this one is one path„ÄÇ and the other one is this one„ÄÇ

And this oneÔºå and this one„ÄÇ So they both go in there„ÄÇ So the second path would be„ÄÇ

Then the same as before and the end here„ÄÇ But then this one is a little bit different because it's now the second unit here„ÄÇ

And then this one is also the second unit„ÄÇ This is this connection here„ÄÇ

And then we compute this part hereÔºå which this is the same„ÄÇSo in that case„ÄÇ

 you can see actually things can become quite complicated if we have a multi layer network And now imagine coding this by hand„ÄÇ

 like computing the derivativesÔºå implementing them in code„ÄÇ It's very  error prone„ÄÇ

 It's important to„ÄÇOn the big pictureÔºå you understand what's going on„ÄÇ

 But we are implementing this in by hand„ÄÇ And this is only for one wayÔºå it would be very error prone„ÄÇ

 which is why we actually use deep learning frameworks that can do this automatically for us„ÄÇ

 So in the deep learning frameworkÔºå we only have to provide the forward computation„ÄÇ

And then the backward computation is yet derived automatically because you canÔºå yeah„ÄÇ

 you can implement simple rules to do that automatically„ÄÇ

 you just have to be careful implementing these rules once and then you can always apply them to any arbitrary computations„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_34.png)

OkayÔºå so in the next video I will show you how we can do this automatic differentiation in Pytorch and then in the next videos I will also do this in a more yeah comprehensive example using the Aline that we talked about last week and then we will also take a closer look at the Pytorch API the convenience functions al right„ÄÇ



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_36.png)