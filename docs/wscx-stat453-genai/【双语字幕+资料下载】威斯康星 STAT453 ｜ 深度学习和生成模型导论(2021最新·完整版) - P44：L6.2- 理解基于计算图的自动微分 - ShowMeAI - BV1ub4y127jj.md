# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P44ï¼šL6.2- ç†è§£åŸºäºè®¡ç®—å›¾çš„è‡ªåŠ¨å¾®åˆ† - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ in order to understand automatic differentiationã€‚

 I think it's helpful to take a look at computation graphsã€‚

So computation graphs are a graph of a computation or calculation that we define or execute in Pythtorã€‚

So in Pythgenï¼Œ in particularï¼Œ when we implement neural network modelsï¼Œ there will be two methodsã€‚

 One is the forwardã€‚Methodï¼Œ and one is theã€‚Backward methodã€‚ So in the forward methodã€‚

This one constructsã€‚The graphï¼Œ and then backwardï¼Œ essentiallyï¼Œ computesã€‚Gras by walking backwardï¼Œ soã€‚

Forward graph is constructing a graphã€‚ If we have a graph with multiple inputsï¼Œ some computationsã€‚

 This is like the forward methodã€‚ and in the backward methodï¼Œ based on the output weã€‚

Compute the derivatives of the outputï¼Œ or the lossã€‚With respect to these inputsï¼Œ for exampleã€‚

 the weights that we want to updateã€‚ So in this lectureï¼Œ I want to in this videoã€‚

 I want to briefly go over yeahï¼Œ what such a computation graph is and how we work with it conceptually before we see how Pytorch handles thatã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_1.png)

So yeahï¼Œ in the context of deep learning and pythtorchã€‚

 it is helpful to think about neural networks as computation graphs because neural networks are really just a chain of computations like nested computationsã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_3.png)

Yesï¼Œ so that we have a concrete exampleã€‚ Let's take a look at a activation function that is not a linear activation functionã€‚

 So if you recall the Adeline modelã€‚What we had is or what we had was the computationã€‚

That computed the net inputï¼Œ which was then passed on to a threshold functionã€‚ So the net inputã€‚

Let's consider one weightï¼Œ1 feature was computed as followsã€‚And then we had an Adelineã€‚

This activation functionï¼Œ which was just an identity functionã€‚ So this one was justã€‚

An identity functionã€‚And now we are taking a look at a function where the sigma is not an identity functionã€‚

 So we are now taking a look at to make it a little bit more interesting at this re functionã€‚

So in this caseï¼Œ now what we have is the sigmaã€‚Is the activation function is thisã€‚

Relu function that I have written down hereã€‚ So the relu function in the context of a neural network usually takes also the net inputã€‚

 soã€‚This is our net inputã€‚ You can think of it as the net inputã€‚

And we will revisit the Relu function also in a later lecture and also talk about why it is so commonly used in deep learningã€‚

 So for right nowï¼Œ just think of it as an activation functionã€‚It is almost like a linear functionã€‚

 but not quiteã€‚ So the shape of this function is as follows So on theã€‚In the plot hereã€‚

 I'm showing you how it looks like for different inputsï¼Œ soã€‚Here we have then input to the functionã€‚

 for exampleï¼Œ the net input Zã€‚ And here we have the output on the y axisã€‚

 and the re function is definedã€‚Yesã€‚Almost like the identity functionã€‚ So it returns Zã€‚The inputã€‚

If z is greater than 0 and 0 otherwiseã€‚ So it really isã€‚This regionã€‚An identityã€‚Functionã€‚And hereã€‚

Notï¼Œ it's not doing anything here on the left hand sideã€‚ So it'sï¼Œ it's clippedã€‚

 You can think of it as aã€‚A clipping hereã€‚So why is that useful in the context of deep learningã€‚

 we will talk about this moreï¼Œ but it is essentially yet to have a nonlinearity in the multilay internet network because if we have multiple linear functionsã€‚

 regular linear functionsï¼Œ because the combination of linear functions is' also linear functionsã€‚

 so in a sense our network won't be able to approximate nonlinear functionsã€‚

But that would be going a little bit too far aheadã€‚ We will talk about this more in detail hereã€‚

 We just care about the computationã€‚ So also not for this re functionã€‚

This one would be not differentiable in a mathematical senseã€‚ Yeahã€‚

 just to briefly redaw the red function here for referenceã€‚ So how it looks like is that it is 0ã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_5.png)

If the input is smaller than0 andã€‚The identity function of the input is larger than 0ã€‚

 So here we have this little kink at 0ã€‚ And now for the left hand sideï¼Œ because there is no slopeã€‚

 rightï¼Œ we can immediately seeã€‚The derivative of this function is 0ã€‚ On the right hand sideã€‚

 it's an identity functionã€‚Soï¼Œ if the function isã€‚Thisï¼Œ then the derivative of this functionã€‚

Would be oneï¼Œ rightï¼Ÿ So slope 1ã€‚So on the right hand sideã€‚

 we have a slope of one on the left hand sideï¼Œ we have a slope of 0ã€‚

 but here at this kink we have a problem nowï¼Œ if you recall from calculus classesã€‚

 this would be a function that is not differentiableã€‚

 So in this case we have a derivative of 0 if the input is smaller than0ã€‚

 a derivative of one if the input is larger than0ã€‚But otherwiseï¼Œ if the input is exactly 0ã€‚

 then the derivative does not existã€‚ Soï¼Œ yeahï¼Œ what do we do nowã€‚

 So in computer science or in the computational contextï¼Œ we are not that pickyã€‚

 We just make a small adjustmentã€‚So we adjust the derivative as follows that it is0 if z is smaller or equal to 0ã€‚

 So that's just adding this smaller or equal to here to fix this issueã€‚ And otherwiseï¼Œ sorryã€‚

 this should be actually a oneã€‚It's oneï¼Œ if z is greater than 0ã€‚

 Notice that we could have also otherwise written it the other way roundã€‚

 We could also write it as smaller andã€‚Sorryã€‚Larger or equalã€‚

 It doesn't really matter because it's so rare that we hit exactly 0 anyways that this will probably never happen in practice anywaysã€‚

 becauseï¼Œ yeahï¼Œ usually we work with floating point numbersã€‚

 like lots of digits of the decimal pointã€‚ So it's very rare to hit a exact zero anywaysã€‚

 So even though it's not preventableã€‚In a mathematical senseï¼Œ we make in the computational senseã€‚

 this little adjustment and define the derivatives as followsã€‚ Okayï¼Œ but yeahã€‚

 let's move on to the computation graphã€‚ just to summarizeã€‚

 Now we have the following activation functionã€‚ It just a summaryã€‚ It's a multivari functionã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_7.png)

Where we have three inputsï¼Œ we haveã€‚The featuresã€‚ So here we only have one feature for simplicityã€‚

So let actually it annotated hereã€‚ one featureï¼Œ one training example for simplicityã€‚

 We have the bias unitã€‚And the weight parameterï¼Œ also for simplicityã€‚

 only one weight parameter corresponding to the one training examples featureã€‚ So as you recallã€‚

 we usually con calculate the net inputã€‚For exampleã€‚

 in Adeline and then we would pass it to an activation functionï¼Œ in this caseã€‚

 we use this re functionã€‚Soã€‚Right now we don't even have to think about yeah neural networks if you don't want to hereã€‚

 you can just think of it as a simple computation that we want to represent using a computation graphã€‚

 so how would we do thatï¼Ÿ

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_9.png)

So here would be a computation graph of this functionã€‚

 So I use some intermediate variables to make this a little bit easier to readã€‚

 So we define this computation as U and we define this computation as Vã€‚So we have three inputsã€‚

 like I saidï¼Œ input1ï¼Œ the biasï¼Œ input2ï¼Œ the featureï¼Œ input 3ï¼Œ the weightã€‚

 What we do is the first part is we compute u here right so this is multiplying x and wã€‚

 So this is this multiplication hereã€‚ So we define as a placeholder hereã€‚ U is equal to W Xã€‚

And then what we do is we add the bias unit hereï¼Œ rightï¼Œ So we have U plus Bã€‚

So we have this operation hereã€‚Which gives us another variable Vã€‚And then V goes through thisã€‚Yeahã€‚

 activation functionã€‚Alsoï¼Œ I could have used Z because this is like really the net input butã€‚

For some reasonï¼Œ I used to V here shouldn't really matterã€‚

 just think of it as a computation here that we then represent as such a graphã€‚So yeahã€‚

 in the context of deep learningï¼Œ we are usually interested in computing the partial derivative of the loss function with respect to the weights or the bias unitã€‚

 as we've seen in the Adeline lecture last week and we can actually decompose that into smaller subsã€‚

 rightï¼ŸUsing the chain ruleï¼Œ we can also decompose it into the partial derivative of the loss with respect to the activation and times the derivative of the activationã€‚

With respect to the weightsï¼Œ and so decomposing itã€‚

So here we have a simpler computation graph I haven't included a loss function hereã€‚

 So what we are actually looking at here is only the activationã€‚ So we are in this exampleã€‚

 only walking through this partã€‚ So the partial derivative of the activation with respect to the weightsã€‚

 which we can also further decompose into substep using the chain ruleã€‚So that's what I'm doing hereã€‚

 but that's what I'm going to show you in the next couple of slidesï¼Œ so here I have the first stepã€‚

 so computing the derivative of the activation with respect to the input of the activationã€‚

So here this is the term the derivative of a with respect to Bã€‚

 that's the first step and using this computation graph we can really make this super easy like computing this whole part by doing it in smaller stepsã€‚

So the next step would beï¼Œ for exampleï¼Œ computing the derivative of V with respect to Bã€‚

 So because we are interested in both on usually also the biasã€‚

 because the bias is also like the weight a model parameterï¼Œ which we canã€‚Then alsoï¼Œ write us onã€‚

Phosã€‚Soï¼Œ this part here would beã€‚So this whole part hereã€‚Would beã€‚This one hereã€‚

 what we are looking at right nowã€‚ So the next stepã€‚Is computingã€‚

Partial derivative of we with respect toã€‚Bã€‚

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_11.png)

Its sniffing in hereã€‚Yeah then we can do this also for the other parts so what's so nice about a computation graph is that we can do it step by step right so we can for each computation do a back track here and annotate our graphã€‚

å—¯ã€‚And then we can put everything togetherã€‚ So if we're interested in the derivative of a of the output with respect to the inputã€‚

 we just need to use the chain ruleï¼Œ rightï¼Œ so we can then write this asã€‚Beã€‚ğŸ˜”ï¼ŒB on timesã€‚å—¯ã€‚ç¬¬ä¸€ã€‚

A times theã€‚Weã€‚So this is a thing mathematicians usually don't likeã€‚

 but we can actually cancel theseã€‚ So this is why we get this one using the chain ruleã€‚

And we can also do this here at the bottomï¼Œ rightï¼Œ so we can do thisã€‚For this partã€‚

 So the derivative of a with respect toã€‚W would beã€‚Putting all these togetherã€‚å—¯ï¼ŒDAã€‚

Maybe and you can technically see how thingsã€‚Cancel rightã€‚ So this is how we get this oneã€‚

But technicallyï¼Œ this is not a right way to doã€‚ I meanï¼Œ we shouldn't cancel things hereï¼Œ butã€‚I useã€‚

 use it all the time as a memoryï¼Œ I would say memory bridgeier or somethingã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_13.png)

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_14.png)

Alrightï¼Œ so yeah this is what I've done here if you want to step through that in the slidesã€‚

 so I was just putting things together hereã€‚

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_16.png)

Yeahï¼Œ let's now take a look at a concrete example with some actual numbersã€‚

 So assume the bias unit is oneã€‚ The weights are 2ï¼Œ and we have an input ofã€‚th3eã€‚

So the intermediate value U would be then 6ï¼Œ rightã€‚Because two times 3 is 6ï¼Œ and then 6 plus 1 is 7ã€‚

 And for the radio functionï¼Œ because it's positiveï¼Œ it's an identity functionã€‚ So it's also 7ã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_18.png)

Nowï¼Œ let's differentiateã€‚ So the derivative of the real function ifã€‚

The input is greater than 0 is  oneï¼Œ rightï¼Œ because it's an identity functionã€‚

 So that the root of this part here would be oneã€‚

![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_20.png)

Then yeahï¼Œ let's compute a derivative here and hereã€‚ So if we look at Vã€‚ So Vã€‚

 you can think of it as a sum of two functions hereã€‚

 So the derivatives can be then computed separatelyã€‚ So if we look atï¼Œ let's say the upper part hereã€‚

 and let's take a look at this oneã€‚ the derivative of V with respect to Bã€‚ So in this caseã€‚

 U is a constantã€‚ So we can ignore that and the derivative of B would be oneã€‚ So there should beã€‚

One and similarlyï¼Œ the other way aroundã€‚At the bottom hereï¼Œ B becomes a constantã€‚

 and the derivative of v with respect to U is then also oneï¼Œ rightï¼Œ So we can also put a one hereã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_22.png)

Alright moving on so we have a derivative hereï¼Œ so what do we have hereï¼Œ so this is yeah just wxã€‚

 if we compute the derivative of U with respect to Wã€‚

 then the derivative would be x and x is what is x x is3 so here we should have a threeã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_24.png)

And yeahï¼Œ then putting everything together hereã€‚The partial derivative of the activation with respect to W would be3 times 1 times 1ã€‚

 it's equal to3 because yeah we are combining with a chain ruleã€‚This hereã€‚ and similarlyã€‚

 we can combineã€‚ yeahï¼Œ the results forã€‚For this part hereï¼Œ tooã€‚So in this wayï¼Œ theã€‚

 the result here would be also yeah one because it's one times oneã€‚Alrightã€‚

 so this is how we use computation graphsã€‚ and I will show you in the next video how we can implement this in Pytorch where it computes these derivatives for usã€‚

 Of courseï¼Œ this example is a trivial oneã€‚ It's very simple right it's something you can do in a few seconds by handã€‚

 But if you rethink of computational graphs as new networks that are more complicated with many layers and yeah lots of input features and stuff it would be tedious to code it all up in Pytorch by yourself or in nuyã€‚

 So that is why there's this automatic differentiation that I will show youã€‚

Where Pytoch will construct this graph here under the hoodï¼Œ like we can actually print out the graphã€‚

 but we don't have toã€‚ It will do it automatically for usã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_26.png)

Yeahï¼Œ just some more computation graphs that will be relevant or that are kind of related to deploying just to complete this video hereã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_28.png)

So here here we have an example of a graph with a single pathã€‚ğŸ˜”ã€‚

So you can maybe also think of it as a as a simple aline or logistic regression modelã€‚

 which we will be covering next lectureï¼Œ or you can think of it as a a line as maybe simpler when you think of the sigma here as an identity functionã€‚

So this would be the full graph based on what I've shown you earlierã€‚ So in the previous slideã€‚

 I showed you onlyã€‚This part here now also think of the output where we compute the lossã€‚For exampleã€‚

 in Adeline the mean squared error between the prediction Oï¼Œ the output and the actual class labelã€‚

So if we have a case like thatã€‚Then we compute the derivative root of the loss with respect to the weightsã€‚

 and this can be decomposed into the loss with respect to the outputã€‚

 So this would be this part hereã€‚ then the output with respect to the activationã€‚

 So this would be this part and then the activation with respect to the weightsã€‚

 this would be this partã€‚ And this is what we had in the lastã€‚Slightdes as an exampleã€‚

 So this would be a simple caseï¼Œ a graph with a single pathã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_30.png)

So in practiceï¼Œ there will be oscillatorï¼Œ some more yeah complicated constructsã€‚ for exampleã€‚

 imagine a graph with weight sharingã€‚ It is kind of related to what's going on in convolutional networks that will be something that will be covered laterã€‚

 Here's a simple exampleã€‚Imagine we have a caseã€‚ We have a single inputã€‚

 and then we have a weight parameter that is sharedã€‚

 So that's the same weight It goes here to compute some activationã€‚

 So these could be different functionsã€‚ A1 and a2 could be different functionsã€‚

And then we compute some outputã€‚And then also here the loss like on the previous slideã€‚

 So if we have a case like thatï¼Œ we have to use the multi variable chain rule because yeahã€‚

 there are no two paths like there's someï¼Œ some weight sharing going onã€‚

 So if we want to update the Wï¼Œ it depends reallyã€‚On this path here and this pathï¼Œ rightï¼Ÿ

 So we have to combine themã€‚ So we have the multivariable chainã€‚ We compute the lossã€‚ So hereã€‚

 the the red of this is the sameã€‚ You can use blue hereã€‚ So you can see this isã€‚The same hereã€‚

And thenï¼Œ we haveã€‚2 different derivativesã€‚ So we haveã€‚This one hereã€‚Andã€‚ğŸ˜”ï¼ŒSo here at the bottomã€‚

 we would haveï¼Œ ohï¼Œ for some reason I haven't written it downã€‚ğŸ˜”ï¼Œthis would be this one hereã€‚

And then which color didn't I Okayï¼Œ let's use this one and then just derivative here and then I'm running out of colourã€‚

 I me use black and this one here and then putting it together with a multivariable chain ruleã€‚ğŸ˜”ã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_32.png)

Allï¼Œ so that would be a case where we have weight sharingã€‚

So here would be an example of a multi layerer perceptronï¼Œ which we will be covering next weekã€‚

 So here we also have multiple paths if we want to update certain weightsã€‚

 So this is a multi layerer perceptron with two hidden layersã€‚

 You can see there now two levels of activationsã€‚ One level is hereã€‚And one of is hereï¼Œ soã€‚

This is the firstã€‚Hidden layerã€‚ And this is the secondã€‚Hiddenã€‚Thereirã€‚And againã€‚

 this part is like the same as in the previous slideï¼Œ the lossã€‚

Let's say we are interested here in computing the derivative of the loss with respect to this weight here in W1ã€‚

1 in the first hidden layerã€‚ So in order to do thatï¼Œ we compute the lossã€‚ So the sorryï¼Œ the yeahã€‚

 compute the lossã€‚ but then also the derivative of the loss with respect to the output that would beã€‚

This part hereã€‚And then we go hereï¼Œ this would be this partã€‚And then we go hereã€‚ğŸ˜”ã€‚

Which would be this partã€‚So we are skipping across some parts laterã€‚ and next weekã€‚

 we will do this step by step in more detailã€‚And then so I'm skipping here a lot of stepsã€‚

 so because this could also be decomposed as sub stepss with the net input and so forthã€‚

 but let's not do it hereã€‚And thenã€‚We have this partã€‚

 which is the derivative root of this activationï¼Œ the a1 with respect to this weightã€‚

 Let me use the green color hereã€‚Soï¼Œ this isã€‚Hereï¼Œ so we are computingã€‚With respect toã€‚é—®é¢˜ã€‚Alrightã€‚

 howeverï¼Œ note that there's a second pathï¼Œ rightï¼Œ so we can alsoï¼Œ let's use red hereã€‚

 Let's go the second pathã€‚ So it's againï¼Œ the same length one as like this oneã€‚ But we have nowã€‚Alsoã€‚

 the second pathã€‚Ohï¼Œ sorryï¼Œ said was wrongã€‚The second path would be this oneã€‚And this oneï¼Œ seeã€‚

 it's actually not so easy to see what is going into here soã€‚

This would be the second path if I use a different colorã€‚ Let's say pink for the forward hereã€‚

 So you can see this oneï¼Œ this one and this one is one pathã€‚ and the other one is this oneã€‚

And this oneï¼Œ and this oneã€‚ So they both go in thereã€‚ So the second path would beã€‚

Then the same as before and the end hereã€‚ But then this one is a little bit different because it's now the second unit hereã€‚

And then this one is also the second unitã€‚ This is this connection hereã€‚

And then we compute this part hereï¼Œ which this is the sameã€‚So in that caseã€‚

 you can see actually things can become quite complicated if we have a multi layer network And now imagine coding this by handã€‚

 like computing the derivativesï¼Œ implementing them in codeã€‚ It's very  error proneã€‚

 It's important toã€‚On the big pictureï¼Œ you understand what's going onã€‚

 But we are implementing this in by handã€‚ And this is only for one wayï¼Œ it would be very error proneã€‚

 which is why we actually use deep learning frameworks that can do this automatically for usã€‚

 So in the deep learning frameworkï¼Œ we only have to provide the forward computationã€‚

And then the backward computation is yet derived automatically because you canï¼Œ yeahã€‚

 you can implement simple rules to do that automaticallyã€‚

 you just have to be careful implementing these rules once and then you can always apply them to any arbitrary computationsã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_34.png)

Okayï¼Œ so in the next video I will show you how we can do this automatic differentiation in Pytorch and then in the next videos I will also do this in a more yeah comprehensive example using the Aline that we talked about last week and then we will also take a closer look at the Pytorch API the convenience functions al rightã€‚



![](img/c273fc7aee7e7b35a44ac71fa81dfb7b_36.png)