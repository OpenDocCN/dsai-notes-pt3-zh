# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P118ÔºöL14.3.2.2- PyTorch ‰∏≠ÁöÑ ResNet-34 - ShowMeAI - BV1ub4y127jj

AlrightÔºå let me know show you how we can implement residual networks in Pytorch„ÄÇ

 So I will show you two notebooks„ÄÇ First is a naive implementation I have made myself„ÄÇ

 And then I will show you a more sophisticated implementation of Resnet 34 from the Pytorch community„ÄÇ

 So in this first notebook now I will show you what we talked about in the last video these two different types of yeah residual blocks and then in the next notebook I will show you this residual network with 34 layers„ÄÇ

 So yeahÔºå I'm not going to rerun this notebookÔºå it didn't take too long„ÄÇ

 but yeah why waiting if if it's not necessary„ÄÇ So I will show you just other results so„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_1.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_2.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_3.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_4.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_5.png)

YeahÔºå hereÔºå it's just the boiler plate importing torch and nuy„ÄÇ the usual stuff for the data set„ÄÇ

 So I'm not using the helper functions here because yeahÔºå it's really just very simple„ÄÇ So I„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_7.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_8.png)

Didn't really focus on abstracting things„ÄÇ I just coded the residual blocks„ÄÇ

 and this is a self contained notebook in that way„ÄÇ

 So here I'm using the Mnes data just for simplicity„ÄÇ

 because just wanted to have a dataset doesn't really matter which one„ÄÇ

 because this is not going to be a good conversion network It's just like more of a proof of concept how the residual block works„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_10.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_11.png)

So the data set really doesn't matter that much here„ÄÇ

So here I'm implementing now this yeah residual block„ÄÇ

 the one where the input has the same dimension as hereÔºå the„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_13.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_14.png)

Output from the residual part„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_16.png)

So how does it look like„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_18.png)

that one so„ÄÇI implemented it using the torch module class just a regular confifin that I'm implementing here„ÄÇ

 I have a confin with two residual blocksÔºå and each of those is one residual block„ÄÇ

 So you can see that this convolution here represents this one„ÄÇ

 So here I' amm starting with one channel for output channels„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_20.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_21.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_22.png)

ThenÔºå I have bechnom„ÄÇThen I have Reou„ÄÇ By the wayÔºå I haven't really explained what that1Ôºå1 means„ÄÇ

 I think I have used that before in some other code„ÄÇ So in place equals true„ÄÇ This just means that„ÄÇ

Pyto doesn't have to make a copy of that array internally„ÄÇ So we could do something like that I mean„ÄÇ

 not here insideÔºå but in generalÔºå we could do something like„ÄÇLike this„ÄÇJust„ÄÇ

 let's write it like this„ÄÇ So this will create a new„ÄÇA new tensor X„ÄÇ and then overr this tensor X„ÄÇ

 So it's essentially over writing this tensor X„ÄÇ But for a brief moment in time„ÄÇ

 when this gets executedÔºå there are two arrays„ÄÇ If this is an existing oneÔºå if I have some„ÄÇ

Previous computation here„ÄÇ So this previous computation created x„ÄÇ And then when I'm calling this„ÄÇ

 it will„ÄÇTake an X and create a new version while X is still in memory„ÄÇ

 So for a brief moment in timeÔºå I have two arrays in memory„ÄÇ it's not a big deal at all„ÄÇ

 but you have toÔºå I meanÔºå under the hoodÔºå allocate memory in the GPU and stuff like that„ÄÇ

 So it's kind of a little bit more expensive to do that compared to doing an in place operation and in place operation is essentially modifying something in place without creating a new array„ÄÇ

 So it's slightly more efficient„ÄÇ It's not always possible to do that„ÄÇ But yeahÔºå if you can do that„ÄÇ

 it's actually nice„ÄÇ So its essentially the difference betweenÔºå let's say„ÄÇ

 writing this and x plus equals one in that senseÔºå you are directly modifying something whereas here you are having creating a copy and then assigning the copy to it„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_24.png)

AnywaysÔºå it's a little tangent„ÄÇ The results are exactly the same„ÄÇ whether you do this or this„ÄÇ

 And in practiceÔºå you probably won't notice any difference anyway„ÄÇ So it does not really matter„ÄÇ

 But yeahÔºå why not doing itÔºü AllrightÔºå so small tangent So we have convolution batch norm and then this re„ÄÇ

 This is this part is really the first three and then we have another convolution and a batch norm„ÄÇ

 which is this part„ÄÇ noticeice that I'm going from one to4Ôºå and then back from4 to1„ÄÇ

 It seems kind of weird why am I doing that YeahÔºå it's really like to match the dimensions„ÄÇ

 OtherwiseÔºå I will have more channels here then I have as an input So I would have four channels here„ÄÇ

 and then one input channel and it doesn't really work if we add them because then it's not an identity anymore okay„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_26.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_27.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_28.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_29.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_30.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_31.png)

YeahÔºå and we also have one fully connected layer„ÄÇ This this just to make this classifier„ÄÇSo yes„ÄÇ

 and how I'm implementing this„ÄÇ So here I was just„ÄÇDefining or initializing these layersÔºå the blocks„ÄÇ

 And here I'm calling them„ÄÇ So in the forward pass is really where things happen„ÄÇ So I„ÄÇ

Save X as the shortcut here„ÄÇSo I'm saving this here„ÄÇThen Im calling my block„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_33.png)

So this part I highlighted hereÔºå this is really calling the whole blockÔºå right„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_35.png)

It's this whole block„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_37.png)

And actuallyÔºå I could have„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_39.png)

These are kind of redundant I could have used the same„ÄÇ OkayÔºå but thenÔºå of course„ÄÇ

 the weights are different„ÄÇ AnywayÔºå sorryÔºå so I call my block here„ÄÇ

 and then I have my re function and the re function is applied to x plus the shortcut„ÄÇ

 So this part really is„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_41.png)

This part„ÄÇ So I'mÔºå I'm adding insideÔºå and then I'm applying the re So the re„ÄÇ

 that is what is shown here„ÄÇ and here I have this addition„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_43.png)

RightÔºå so this is essentially one residual block„ÄÇ And then I'm repeating it„ÄÇ

 So why am I not using one hereÔºå WellÔºå then it would be the same layer It's that wouldn't work really„ÄÇ

Okay„ÄÇBut the the shape is the same„ÄÇ It's justÔºå we have different weightsÔºå right„ÄÇ

 So it's just like having two convolutional layers after each other„ÄÇ

And then we have this linear hereÔºå which is turning this into a classifier„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_45.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_46.png)

AlrightÔºå so„ÄÇSo the linear Le has output the nu classes„ÄÇ and here I am just„ÄÇFlatening it„ÄÇ

 So I'm assuming what comes out of„ÄÇÂóØ„ÄÇThis block 2 has a dimensionity 28 times 28 784„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_48.png)

YeahÔºå and then I'm running this just pasting my convenience functions that I usually have in my helper function„ÄÇ

 it's a slightly simpler versionca I'm not plotting anything I just want to show you that this actually runs„ÄÇ

 doesn't get great performance becauseÔºå of courseÔºå it's a very naive implementation„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_50.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_51.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_52.png)

There's also only the training accuracyÔºå the test accuracy is 92%„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_54.png)

So what I show what I mean is how do I know that this is the actual number„ÄÇ I mean„ÄÇ

 I can think about it„ÄÇ I can look at this„ÄÇ but like I explained in the previous video„ÄÇ

 what I can also do is„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_56.png)

I meanÔºå what most people do is just print X size„ÄÇ then you can oops„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_58.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_59.png)

Let run everything here„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_61.png)

Then„ÄÇCan run the training„ÄÇ And then you will see the size„ÄÇ

 Of course you don't want to complete it because it's annoying to have it here„ÄÇ So I just stopped it„ÄÇ

 I can seeÔºå ohÔºå it's 1Ôºå282 times 28„ÄÇ And that is what I can then copy and paste and then„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_63.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_64.png)

And go here and put it in here„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_66.png)

RightÔºå so that's„ÄÇWhere this number comes from„ÄÇ And it is also where this number comes from„ÄÇOkay„ÄÇ

In practiceÔºå if you don't want to think about it too hard and you are debugging things„ÄÇ I mean„ÄÇ

 it doesn't hurt to„ÄÇ And so the print statementÔºå it's what everyone is doing„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_68.png)

Okay„ÄÇSo we trained that„ÄÇ NoÔºå of courseÔºå I interrupted itÔºå but„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_70.png)

Suppose it trainedÔºå I meanÔºå it trained before„ÄÇ so if I fixed it it would train„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_72.png)

NowÔºå the second partÔºå now focusing on the more interesting part where we have this resizing here„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_74.png)

So I am implementing this a bit differently nowÔºå using a reusable unit„ÄÇ I call that a residual block„ÄÇ

 So I am implementing my residual block hereÔºå and this one is implemented the same„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_76.png)

WÔºå now it's a little bit more general„ÄÇ I have something called channels here„ÄÇ

 This is the input channelsÔºå or let's say the first number of channelsÔºå the output channels„ÄÇ

And then here I have1 and2„ÄÇ So I am going from 0 to 1 to 2„ÄÇAnd„ÄÇI canÔºå I mean„ÄÇ

 I'm not defining what these numbers are„ÄÇ I'm defining them later but I'm calling this„ÄÇ

 so I can maybe briefly skip ahead„ÄÇ So I'm using this residual block„ÄÇ

 actually in my convolutional network here„ÄÇ So I'm using it here„ÄÇ And here I'm defining the channels„ÄÇ

 I'm going from 1 to 4 to 8„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_78.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_79.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_80.png)

So„ÄÇYeahÔºå so that's what I'm doing here„ÄÇ So I'm going from0„ÄÇAlso 1 to 4Ôºå sorryÔºå1Ôºå2 4 to 8„ÄÇ

And then I have my shortcutÔºå which goes also from 0„ÄÇ sorryÔºå from 1 to 8„ÄÇ Otherwise„ÄÇ

 I wouldn't be able to edit because ifÔºå let's say this is one channel„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_82.png)

Outcomes„ÄÇ8 channels„ÄÇ Then this also has to be 8 channels„ÄÇ otherwise I can't edit„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_84.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_85.png)

So that's what's going on there„ÄÇSo„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_87.png)

My residual block is scrolling up again so we can see everything„ÄÇ So my residual block 1Ôºå2Ôºå3„ÄÇ

 This is really this partÔºå these three first blocks„ÄÇ

 and then like before these second blocks are this and this Now the difference is„ÄÇ

 yeah that I have different numbers of channels and I can also reduce the size„ÄÇRight„ÄÇ

 so here I have a stride of two„ÄÇ so that will reduce the size„ÄÇ I have to do a stride of two here„ÄÇ

 too to match these dimensions„ÄÇ So here I have to be a little bit more careful that the dimensions match also„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_89.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_90.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_91.png)

YeahÔºå and then as before I have my block I have a shortcut and both the block plus shortcut„ÄÇ

 they go into my re function„ÄÇ So this is what I'm showing you here„ÄÇ

 this residual block is really this whole thing here„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_93.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_94.png)

Yeah and then I'm using my residual blockÔºå I'm initializing one residual block and another so I have a network with two residual blocks„ÄÇ

 the first one goes from 1 to 8 and the second one from 8 to 32 and yeah the number of the sizes here is seven times7 times 32 so it's because we are also havinglving the dimensions here half and half approximately„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_96.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_97.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_98.png)

So going from 28 times 28 to 14 times 14 and from 14 times 14 to 7 times 7„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_100.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_101.png)

YeahÔºå this is essentially it„ÄÇSo that's how we implement this„ÄÇ So yeah„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_103.png)

Then we are training it„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_105.png)

Tins here„ÄÇ and yeah it performs much better than our previous implementation„ÄÇ But again„ÄÇ

 the goal of residual networks is really to go deep in the network in terms of the number of layers„ÄÇ

 So here we only have two layers„ÄÇ SoÔºå I meanÔºå this is probably not a great network to use for other datasets here we are just using Mn„ÄÇ

 So if we want to use theÔºå I would say sophisticated data„ÄÇ

 I'm actually only using C 10 because it's simple to a lot„ÄÇ But if you want to use a different data„ÄÇ

 Resnet 34 is a good choice„ÄÇ So this is the oneÔºå the the deep one here„ÄÇ

 it performs pretty well going back hereÔºå it gets actually pretty good performance on„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_107.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_108.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_109.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_110.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_111.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_112.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_113.png)

An imagenet of one accuracy better than the GÔºå for example„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_115.png)

And how does that workÔºå So it's the same concept„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_117.png)

ThanksÔºå SeanÔºå sorryÔºåShown hereÔºå except moreÔºå I would sayÔºå more sophisticated implementation of that„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_119.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_120.png)

So I could have implemented it by handÔºå but there's always the chance to make mistakes at some point„ÄÇ

 So why not using what's already implemented„ÄÇ So here this isÔºå againÔºå I'm using my helper functions„ÄÇ

 that's again the same that I explained for VG G16„ÄÇ So everything is the same as for VG G16„ÄÇ

 So I don't have to discuss everything again„ÄÇ the only new part here is really this partÔºå the model„ÄÇ

 So here I actually copied the code from this websiteÔºå which is an implementation„ÄÇ yep„ÄÇ

 the official Pyr implementation which has different versions of resnet wide resnet„ÄÇ

 regular ressonnetÔºå18 layersÔºå34 layersÔºå100 layersÔºå1 or 52 layers and so forth„ÄÇ I grabbed„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_122.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_123.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_124.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_125.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_126.png)

The code that is used to yeah initialize all of these networks„ÄÇ

 So they have written some code that can be reused for different types of residual networks„ÄÇ

 So here was copying it and simplifying it a little bit„ÄÇ So it's not that long„ÄÇ

 And then they have something they call the bottleneck„ÄÇ

 It's kind of similar to what I call the residual block„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_128.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_129.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_130.png)

But we have here„ÄÇ and then I meanÔºå it's relatively complicated„ÄÇ

 I have to admit it would take me also a couple of us to really understand how that is implemented„ÄÇ

 The most important thing is that it works„ÄÇ Many people are using it So I'm kind of trusting that this is indeed working„ÄÇ

 So they have like a make layer method here or function here that creates these layers„ÄÇ

 It's a little bit more sophisticated than than my version„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_132.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_133.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_134.png)

So and then in the forward methodÔºå you have these different layers„ÄÇ

 So each layer has also multiple convolutional layers„ÄÇ That's how you get the number 34„ÄÇ And yeah„ÄÇ

 we can also use the torch flatten function here„ÄÇ That's actually something I should also maybe use more often„ÄÇ

 It's a more recent thing„ÄÇ So I could actually technically replace„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_136.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_137.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_138.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_139.png)

Replace that one by flatten„ÄÇSo hereÔºå that could be replaced by flatten„ÄÇ

 but I still need to know this number anywaysÔºå because I have to put it here for the number of parameters„ÄÇ

 So even though we can put a torch flatten hereÔºå it'sÔºå it's not that much simpler„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_141.png)

Yeah„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_143.png)

So„ÄÇYepÔºå that is essentially it„ÄÇ So here I would have to know still this number in this millionaire the 512„ÄÇ

 So I could technically also write this as torch„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_145.png)

Not view„ÄÇ-1Ôºå512„ÄÇ I think blocks expansion here is one„ÄÇ

 This is only used for the other types of networksÔºå other residual net so„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_147.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_148.png)

Could technically also write it like this„ÄÇ But yeahÔºå we have this nicer flatten„ÄÇ

 thing what's nice about flatten is everyone knows what flatten that it has a meaning that is more intuitive„ÄÇ

 maybe than saying view-1 or something„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_150.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_151.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_152.png)

O„ÄÇüòî„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_154.png)

YeahÔºå and here it's also the same code that I used for the V G„ÄÇ And now it's training„ÄÇ Actually„ÄÇ

 we are using Cypher 10 here„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_156.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_157.png)

Let me open this one again„ÄÇSo here I have 70 by 70 images„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_159.png)

Scraning upÔºå sorry„ÄÇ YeahÔºå I have made it larger because otherwise the performance was very poor„ÄÇ

 I meanÔºå all these types of networks are really implemented for bigger data setsÔºå not Cypha 10„ÄÇ

 I'm just using Cypha 10 because then we don't have to download a separate dataset if you want to reproduce these results„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_161.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_162.png)

And I showed you how you can use your own data set tooÔºå So in the way„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_164.png)

Shouldn't be an issue for youÔºå but if you have questions„ÄÇ

 you can always ask I'm happy to help with that„ÄÇSo here with ResnetÔºå we get approximately 48%„ÄÇ

 which is not much better than what we got with midji G16 here it's also„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_166.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_167.png)

Kind of the same„ÄÇ But noticeÔºå even though I use large images hereÔºå it was at least„ÄÇ

 at least faster to run 62 minutes versus 90 minutes„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_169.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_170.png)

OkayÔºå as if IÔºå if I would have made the images smaller here the same size„ÄÇ

 it would have probably finished in like 30 or 40 minutes„ÄÇ Also overfitting„ÄÇ

 So here might be a case for adding more dropout„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_172.png)

So here we only have„ÄÇDo we have actually droplets„ÄÇ

![](img/6fc0b4c4fba26a77a100c96c79d18809_174.png)

NotÔºå not really„ÄÇ we only have petonÔºå so maybe could be added to could be adding drop„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_176.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_177.png)

OkayÔºå so some the results get spurret and frog wrong„ÄÇ What was the one that this one got wrong„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_179.png)

Deer and fck interesting„ÄÇ So yeahÔºå animal classes are still confusing„ÄÇ You can also pro yeah„ÄÇ

 you can see againÔºå the square where it makes misclassifications between different animals„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_181.png)

AgainÔºå the tech cats and dogs„ÄÇ And yeahÔºå this is resnet implemented here„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_183.png)

HonestlyÔºå if you implement networksÔºå you don't have to implement things from scratch unless it's for educational purposes like for learning things„ÄÇ

 usually when you find a paper or read a paper with an interesting implementation thing you want to try usually what people do is they would go on Gitthub and search for the original authors providing the code for that paper and then adopting this code„ÄÇ

 so you would technically not run it one to oneÔºå you have to probably make some modifications so that it works for your data„ÄÇ

 but usually in practice once we are working with these more complicated datas„ÄÇ

 theres no it doesn't make sense to implement this Re 34„ÄÇ

 let's say completely from scratch it's only another source of making errors„ÄÇ

 I mean it's useful here as a thought exercise to do it with a simple case with two layers„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_185.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_186.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_187.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_188.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_189.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_190.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_191.png)

![](img/6fc0b4c4fba26a77a100c96c79d18809_192.png)

Where you have„ÄÇSimple implementation with two layersÔºå yet maybe makes sense to do that„ÄÇ

 But if you go deeper Renet 34Ôºå maybe use something that isÔºå yeahÔºå someone has implemented„ÄÇ

 saves you lots of time and pain in that way„ÄÇ AlrightÔºå so okayÔºå this is resnet„ÄÇ

 I think we are already at the 75 minutes„ÄÇ So we will continue next week with„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_194.png)

YeahÔºå the all convolutional network„ÄÇ I already implemented this somewhere here„ÄÇ

 and then we will also talk about transfer learning„ÄÇ I have to still implement it anyway„ÄÇ

 No I have it here already so„ÄÇWe will talk about transfer learning also next weekÔºå All right„ÄÇ



![](img/6fc0b4c4fba26a77a100c96c79d18809_196.png)