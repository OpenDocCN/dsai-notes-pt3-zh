# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P2ÔºöL1.1.1- ËØæÁ®ãÊ¶ÇËø∞Á¨¨ 1 ÈÉ®ÂàÜÔºöÂä®Êú∫Âíå‰∏ªÈ¢ò - ShowMeAI - BV1ub4y127jj

AlrightÔºå let's now get started then with the contents I prepared for this week„ÄÇ

 I thought I would start with a short teaser what you will be able to do after this course„ÄÇ

 Of courseÔºå it's a long journeyÔºå1515 weeks in the future„ÄÇ

 but at some point you will be able to accomplish something really cool with deep learning things you will learn in this class„ÄÇ

 So here are a few examples of class projects that students worked on in the previous semesters„ÄÇ

 For exampleÔºå in this projectÔºå students converted audio signals into spectrogramsÔºå for example„ÄÇ

 spoken text„ÄÇüòä„ÄÇ

![](img/f5ce1c357294d049e6ea46c9734c8cb7_1.png)

![](img/f5ce1c357294d049e6ea46c9734c8cb7_2.png)

And then they applied convolution neural networks to classify different texts and yeah„ÄÇ

 to extract language out of the audio clips„ÄÇ So in this case„ÄÇ

 it was actually not language I just see it was finger snapping and here singing„ÄÇ

 So a distinguishing between different audio inputs„ÄÇ

 So was one example of a class project from last semester„ÄÇ



![](img/f5ce1c357294d049e6ea46c9734c8cb7_4.png)

Another one wasÔºå yeah working with 3D convolutional networks„ÄÇ

 So this is a 3D version of the so-called MnesT data set„ÄÇ

 which you will be seeing a lot in this classÔºå or at least in the introductory lectures later on„ÄÇ

 because it's a yeah simple data set to get started with neural networks„ÄÇAnd yeahÔºå in this project„ÄÇ

The students yeah worked with FRMI or yeah magnetic resonance imaging data so like brain scans and so forth and yeah classifying different types of yeah brain scans So that was another interesting project or also yeah students worked with different types of generative adversarial networks which will also be covered at the end of yeah this class where you will be able to generate new data or also mixed data from different data sources So here„ÄÇ



![](img/f5ce1c357294d049e6ea46c9734c8cb7_6.png)

The studentsÔºå yeah mixed artistic paintings or photographs„ÄÇ

 artistic inputs with portrait of a photo model„ÄÇ and then the output was basically here shown on the righthand side„ÄÇ

 like a portrait of a person mixed with a different styles So this would be an example of style transfer„ÄÇ

 YeahÔºå why did I pick these three projects and it was kind of arbitrarily It was just something„ÄÇ

 Yeah so I looked at the projects were last semester to be honest and looked at which ones had yeah nice figures„ÄÇ

 So in that way it looked nicer on slides„ÄÇ but of course you are free to work on whatever you like for your class project and I will talk more about that later„ÄÇ

 I don't want to overwhelm you with too many things at the beginning„ÄÇ

Just wanted to show you some examples of things you will be able to accomplish at the end of the semester„ÄÇ



![](img/f5ce1c357294d049e6ea46c9734c8cb7_8.png)

Yeah also if you're interested a little bit about my research so I'm working a lot on machine learning and deep learning so also yeah just compiled an overview here of projects I worked on so yeah just to also introduce myself and what I'm interested in So yeah last year for example I worked on rank consistent auto regression networks we call that method coral which is for„ÄÇ

Oh yeah„ÄÇYou can think of it as classification of ordinal inputs so if you have class labels that are order and we want to sort them or predict the right order of the labels and also the numeric value associated with it that for that we developed networks here applied to age classification or we worked on face privacy we call this method privacy net where we can hide a facial attributes for example„ÄÇ

 age and gender and race and so forth from the input images for protecting one's privacy„ÄÇ

Also yeah collaborated with people from NvidiaÔºå it was more like a review article„ÄÇ

 we wrote about the latest trends in the realm of PythonÔºå machine learning and deep learning„ÄÇ

 in particular the focus on GPU memory and that's also something we will be talking more about later when we talk about the tools that we will be using for this class„ÄÇ

YeahÔºå with a student of mineÔºå I also brought another review article here on„ÄÇ

Machine learning andI based approaches for bioactive ligand discovery„ÄÇ

 So yeah one of my students is working on yeah small ligand discovery and synthesis also using generative models and generative deep learning models for yeah in the context of molecular molecular synthesis and design and yeah another student of mine is working on few short learning so few short learning is a branch of deep learning that is concerned with learning from small data sets most of the time people use meta learning or transfer learning we will be talking more about transfer learning later in this course„ÄÇ

 we won't be covering few short learning though I may ask though my student maybe to give a small guest lecture„ÄÇ

If he has time later this semester and Zhgji was working also on this paper„ÄÇ

 he is also our TA in this semester so if you are interested you can ask Zhongji more about different few short learning approaches and he would be very excited to chat more about you more with you about that I think so during office hours if you have questions about few short learning I think he would be excited to talk more about it because he's always excited to talk about it„ÄÇ

Yeah and lastly I'm also yeah working on some traditional machine learning methods so this was in a collaboration where we used not deep learning„ÄÇ

 but yeah traditional machine learning methods in this case nearest neighbor methods for yeah also predictions related to computational biology„ÄÇ

 So here this was concerned with the structure of GPR which is a G protein coupled receptor that is yeah very important protein or protein receptor„ÄÇ

 it's a binding to small molecules in humans and yeah most most drug targets are actually targeting GCRs but here this was more like„ÄÇ

YeahÔºå I have fundamental computational biology research„ÄÇ

 analyzing the structural yeah composition of these proteins„ÄÇ So this is just a little bit about me„ÄÇ

 So you can probably see a thes that I like working on deep learning and also have some interest in computationally computational biology applications„ÄÇ

 So these two are basically my main research areas and things I'm really excited about„ÄÇ



![](img/f5ce1c357294d049e6ea46c9734c8cb7_10.png)

OkayÔºå but now let's talk more about the course„ÄÇ So yeahÔºå for this courseÔºå I planned lots of topics„ÄÇ

 so mainly deep learning and gene ad known„ÄÇ

![](img/f5ce1c357294d049e6ea46c9734c8cb7_12.png)

Networks like yeahÔºå like the cost title suggests„ÄÇAnd I structured this cross into five parts„ÄÇ

 So here are parts„ÄÇ1Ôºå2Ôºå3„ÄÇ And on the next slideÔºå I have some more parts of the remaining two„ÄÇSo„ÄÇ

 firstÔºå in the introduction„ÄÇThat's where we are right now„ÄÇ

 I wanted to give you a brief overview of this course„ÄÇ

 and yeah also introduce machine learning and deep learning„ÄÇ

 That's what we are going to do this week„ÄÇThen I want to also briefly briefly talk about the history of deep learning„ÄÇ

 And I think that's interestingÔºå because„ÄÇThat helps you understanding like where the things and motivations are coming from„ÄÇ

 because yeahÔºå deep learningÔºå the term deep learning is relatively new„ÄÇ

 it emerged about 10 years agoÔºå but it has a long history because yeah deep learning you can think of it as a fancy term for neural networks„ÄÇ

And neural networks have been around for at least 60„ÄÇ

70 years and are yeah some ideas that emerged very early on that motivated the development of different ideas later on and we will be covering a lot of things related to neural networks So in this this lecture you can think of it as the big picture overview„ÄÇ

So we will then just briefly cover the history and then later when we are introducing different topics in this lecture„ÄÇ

 we will do this step by step and relate it back to the history and also motivate why we learn about them and why they are useful„ÄÇ

YeahÔºå and then we will talk about one of the early methods of yeah machine learning„ÄÇ

 a single layer neural network„ÄÇ So the perceptron algorithmÔºå it's a very traditional algorithm„ÄÇ

 It's not very yeah commonly used nowadays anymore„ÄÇ

 but I think it's this like a easygo introduction to the problem of classification„ÄÇ

 So classificationifying„ÄÇOops„ÄÇClassifying thingsÔºå putting things into different categories„ÄÇAnd yeah„ÄÇ

 I think that will be a good introduction to get started with the topic„ÄÇ

 and then we will have a small part two here„ÄÇWhich is concerned with the mathematical and computational foundations„ÄÇ

 So with thatÔºå I mean like introducing some mathematical necessities like linear algebra„ÄÇ

 So indeed deep learningÔºå linear algebra is usually used„ÄÇTo express things more compactly„ÄÇ

 technically we can or we could use deep learning without linear algebra„ÄÇ

 but it would be very yeah hard to write it down and also slow to implement because when we use deep learning in practice the computing libraries that we use they use or they rely on linear algebra computational routines that help us executing certain computations more efficiently compared to let's say a Python for loop So linear algebra is like in that way„ÄÇ

 very important for deep learning we won't be covering or needing any advanced linear algebra concepts„ÄÇ

 just simple yeah vector dot products and matrix multiplications„ÄÇ That's it basically„ÄÇ

But I I think it's still worthwhile yeahÔºå covering this in a separate lecture because yeah„ÄÇ

 laying down the groundwork for the later lectures properly makes everything later on a little bit easier„ÄÇ

 I think„ÄÇThen we will be talking about gradient descent„ÄÇ That'sÔºå yeah a calculus topic„ÄÇ

 That's gradient descent is the main method for„ÄÇYeahÔºå parameterizing or training neural networks„ÄÇ

And then after this is more like a refresher after covering this topic„ÄÇ

 we will talk about automatic differentiation with Pytorch„ÄÇ

 So automatic differentiation is yeah calculus on the computer„ÄÇ

 you can think of it like that and we will be using a tool called Pytorch„ÄÇ

 which is a library for Ya linear algebra automatic differentiation and then also neural network training or deep learning and it also allows us to implement things on the GPU to make things more efficient„ÄÇ

 So I will also explain them here„ÄÇIn lecture 7Ôºå how you can use cluster and cloud computing resources„ÄÇ

 it will be a relatively short part though because yeah the main the main topic is deep learning of course computational aspects are necessary but for this introductory class you don't have to be an expert programmer and or yeah user of computers you should be familiar with certain things on your computer and certain programming aspects but we are not here in machine learning engineering more like giving a conceptual overview so you will get by with some free resources that I will talk about in this lecture„ÄÇ

 but if you are interested you can of course also use more advanced resources for example„ÄÇ

 or campusesÔºå HTCC and so forth„ÄÇBut it won't be required for this class„ÄÇ Yeah„ÄÇ

 and then after the mathematical and computational foundations„ÄÇ

 we will be talking then finally about neural networks„ÄÇ So in this part 3„ÄÇ

 I will lay the groundwork for yeahÔºå deep learning„ÄÇ

 So we will talk or we start with logistic regression„ÄÇ

 which you can think of a single layer neural network„ÄÇ So this is basically an extension of this„ÄÇ

YeahÔºå single network that we talked about earlier„ÄÇ that is now„ÄÇ

Differentiable and using the logistic regression as a starterÔºå we will add additional hidden layers„ÄÇ

 making this a deep networkÔºå which is also called multilayer perceptron„ÄÇ

 and then we will learn how we can train such a multilay perceptron using the back propagation algorithm then parts here parts 10 to 12 are more like tricks for training deep neural networks„ÄÇ

 for example„ÄÇRegularization techniques to avoid overfitting input normalization and weight initialization„ÄÇ

 it's just yeah making training neural networks more robust and faster and then also talking about learning rates and yeah some advanced optimization algorithms„ÄÇ

 So like fancier versions of gradient in descent essentially And these are really kind of necessary to make neural networks work well in practice„ÄÇ

 these topics may not sound super excitingÔºå especially like 10 and 11 but they are super useful or important„ÄÇ

 even yet to make neural networks work well„ÄÇ

![](img/f5ce1c357294d049e6ea46c9734c8cb7_14.png)

And then we will get to the interesting parts in this courseÔºå or I would say the more advanced parts„ÄÇ

 so here in part4 we will then be talking about deep learninging for computer vision and language modeling„ÄÇ

 so we will spend a lot of time on convolutional networksÔºå So this is one big topic„ÄÇ

And then we will also talk about recurrent neural networks„ÄÇ They are for language modeling„ÄÇ

 so convolution networks are more for image modeling„ÄÇ

 although you can also use the one dimensional convolution network for text„ÄÇBut yeah„ÄÇ

 text will be more focused on in lecture 15 and these will kind of also lay the groundwork for the deep generative models that we will be talking about„ÄÇ

So in terms of deep generative models we will be talking about auto encoders„ÄÇ

 so-called variational auto encoders„ÄÇ then we will talk about generative adversary networks„ÄÇ

 You may already have heard of them as GNs„ÄÇ So just the long form of writing G generative adversary network„ÄÇ

 then this is also a very big topic„ÄÇ we will have a„ÄÇ2 lectures on that„ÄÇ

 So one introduction and then one on some more advanced Gs„ÄÇ for exampleÔºå the bastein G„ÄÇ

 and then also how we can evaluate and compare different Gs to each other because now in this part„ÄÇ

We are focused on prediction„ÄÇOops„ÄÇPrediction and„ÄÇAnd in the second part hereÔºå we are focused on„ÄÇ

Generating„ÄÇThinksÔºå so it's a little bit different„ÄÇ It's a little bit trickier to ever these models„ÄÇ

 So we will have a lecture on that„ÄÇAnd then I also plan to cover some aspects about recurrent neural networks forative modeling„ÄÇ

 for exampleÔºå generating new text on using or in a sequence to sequence context„ÄÇ

 So here in lecture 15Ôºå I will first try also to focus only on the prediction parts„ÄÇ

 but we will be revisiting this topic also for yeah generating new data on text and then also going into a more advanced topic„ÄÇ

Adding the so called attention mechanism to R and ends and then also explaining self attention in the context of transformers„ÄÇ

 which are yeahÔºå underlying the models that you have heard about in the media probably one is called bird or G„ÄÇ

2 and GP3„ÄÇ So these are the building blocks of these model„ÄÇ So we'll also talk about thoseÔºå so„ÄÇ

I't want to make this too crowded hereÔºå but this part will be essentially„ÄÇFor images„ÄÇ

And these two last parts here„ÄÇWill be for text„ÄÇ So we will have both generative models for images and for text„ÄÇ



![](img/f5ce1c357294d049e6ea46c9734c8cb7_16.png)