# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P25ÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞Èóª #2Ôºå2021 Âπ¥ 2 Êúà 6 Êó• - ShowMeAI - BV1ub4y127jj

YeahÔºå hiÔºå everyone„ÄÇ I can't believe we already completed the second week of this semester„ÄÇ

 but in the meantimeÔºå there have been a lot of interesting things happening in the deep learning and artificial intelligence world„ÄÇ

 So here I'm back with stuff in the news number too„ÄÇ

 Some of the interesting things that happen this week„ÄÇ

 So we will be mainly seeing things from my perspective things I have been reading about„ÄÇ

 But I also had to trim down the list„ÄÇ yeahÔºå quite a bit because there hell always lots of interesting research papers and news articles coming out„ÄÇ

 So but with thatÔºå let me not waste too much time on the introduction and get started with the stuff in the news„ÄÇ

üòä„ÄÇ

![](img/829a5200a794f56317f2f6a8221e5d48_1.png)

YeahÔºå one of the things that piqued my interest this week was this article entitled Seu Research Explo Crazy idea of Automating AI paper reviews„ÄÇ

So paper reviews that's a veryÔºå I would say sensitive topic in the deep learning and machine learning community right now„ÄÇ

 because at yeah these top conferences where people submit papers like ICML and„ÄÇ

New reps there are usually aroundÔºå I don't knowÔºå5 to 10000„ÄÇ

 let's say 10000 submissions every year nowadays„ÄÇ and it's yet pretty challenging to find or to maintain good quality reviews„ÄÇ

 I'm also a review for both of these conferences and get about like 3 to 5 papers to review also the timeline is very short„ÄÇ

 And yeahÔºå if you have 10000 submissionsÔºå you have to find also 10000 reviews every one of those reviewing three papers or more„ÄÇ

 and it's really hard to yeah keep up with these numbers„ÄÇ

 So I think that was what motivated these people to„ÄÇWork on this idea„ÄÇ

 I would say I'm not honestly convinced this is a good thing to work on or even try to automate paper reviews with with AI„ÄÇ

 Maybe it's good for checking general formats like beyond the yeah conventional„ÄÇ

Quality checks like whether the paper has or has a right page limit or right template and things like that„ÄÇ

 maybe AI can be used for some quality checks„ÄÇ I'm a little bit skeptical whether it can be used to really make or should make decisions on the quality or regarding the quality of a paper„ÄÇ

NonethelessÔºå I think this is an interesting project from a language perspectiveÔºå like assessing how„ÄÇ

Far deep learning has come in terms of understanding human language and contents of more comprehensive texts„ÄÇ

And also what I found like as a general interesting takeaway are these most frequently mentioned qualities of a good review„ÄÇ

 so these are things to yeah assess whether a review„ÄÇ

 peer review is good or not and this is actually something I'm thinking about adopting as a rubric for our class projects because like I mentioned before there will be a peer review for our class projects at the end of the semester and I will of course share more details later on during the semester„ÄÇ

 but this might be a good rubric for me also to assess whether the peer reviews you write about the reports of other students might be good peer reviews„ÄÇ

 so something like decisivenessÔºå comprehensivenessÔºå justification„ÄÇ

 accuracy and kindness kindness is also very important because yeah reviews can sometimes be grumpy and I think it's usually not very not very nice and not very fair to write grumpier reviews„ÄÇ

AlrightÔºå so yeah this model they proposed is based on BRT and bart Bart is a denoing out encoder it's related to BRT and it this like for pretraining sequence to sequence models we will talk more about sequence to sequence models yeah in part5 of this course at the end of the semester so sequence to sequence means that the input to the model is a sequence and the output is also a sequence instead of just a single prediction so instead of just predicting a single class label you are outputting also a sequence„ÄÇ

 for example text„ÄÇA simple example also of yeah sequence to sequence models would be„ÄÇ

 for example language translation where you translate from one language to the other so here they yeah involve of course human annotations so they annotate the paper with these statistics I think some stands for summary or actually posted it here summary MOT is for motivation S UBs for substance so they tag humans tag the article with these different aspects then they train a tagger that can do the same thing„ÄÇ

Then there is some post processing going on and then a human has to evaluate the results„ÄÇ

 So if you are interested in this project or want to read more„ÄÇ

 there's the paper here on archive and they also posted yeah the code on Githubs if you want to play around with us if you are curious„ÄÇ

AlrightÔºå moving onÔºå you have probably never wondered what a night made of spaghetti would look like„ÄÇ

 So this is not new news„ÄÇ It's a a bit older„ÄÇ It's like a month old or something that's like a machine learning project called Dal E„ÄÇ

It's like I think a meshup of Wall E and the artist Dali„ÄÇ

 So in that way here what the researchers did is they trained another language modelÔºå a G3„ÄÇ

 12 billion parametermeter model and in this doll E model here which is based on G3 or the concepts of G3 but the researchers do is they train a model on„ÄÇ

Images and descriptions of images„ÄÇ And then so these are the inputs so you can think of it as Inet for example„ÄÇ

 and then you can query the model and it gives you outputs and the outputs outputs are meshups from the text input„ÄÇ

 So for exampleÔºå you can type in spaghetti night or night made of spaghetti you can type that in and the model will produce new images that have both concepts in common„ÄÇ

 for exampleÔºå night and spaghetti and here are some some of the results of nights made out of spaghetti„ÄÇ

 so these are images that don't exist„ÄÇ This is something the model is synthesized„ÄÇUnfortunately„ÄÇ

 yeahÔºå the paper is not available and the code is also not availableÔºå but there is a block article„ÄÇ

 if you want to read more about this projectÔºå I thought it might be also a fun project especially if you are interested in generative models„ÄÇ

YeahÔºå and while the DalE paper and code are currently not available yet„ÄÇ

One part of delE is availableÔºå this a so-called clip model or approach„ÄÇ

 so C stands for contrastive language image pretraining„ÄÇ

 So this is something that DlE uses under the hood„ÄÇ

 It's also based on yeah GT3 I think so and what clip does its you can think of it as yeah and almost like an image classifier in that sense„ÄÇ

 but it is doing something clever„ÄÇSo clip is based on zero shot transfer so here zero shot means classifying something where you don't have any training example for„ÄÇ

 So as I mentioned earlier I'm also yeah working a lot in the area of few short learning with my student ju and in a few short learning we have this setting like„ÄÇ

En shot„ÄÇK wayÔºå few short learning„ÄÇ So the NÔºå in end shot few short learning stands for the number of examples per class in case the number of classes„ÄÇ

 SoÔºå for exampleÔºå if you have a„ÄÇLet's sayÔºå a5„ÄÇShort10„ÄÇWay few short learning problem„ÄÇ

 That means you have 10 different classesÔºå but only five examples per classÔºå a very small problem„ÄÇ

 And it's very challenging to yeah develop deep learning systems that can work with such small dataset sets„ÄÇ

 So zero shot is even more extreme„ÄÇ it really means you have zero examples„ÄÇ So in this case„ÄÇ

 the model should predict somethingÔºå even though it may have never seen this class before„ÄÇ

 So how would you approach this„ÄÇ So for that„ÄÇThe researchers use natural language supervision and multimodal learning„ÄÇ

 so multimodal learning stands for a means using different data sources„ÄÇ

 So here they use image data and text data„ÄÇ So that's where the natural language supervision part comes in„ÄÇ

 So in particular they are predicting novel image classes based on word embeddings So when they train the data or the model„ÄÇ

 they use both descriptions of an image and the image itself and then if they see a new class they can use yeah word embeddings or the descriptions to predict the class„ÄÇ

So here's an example of thatÔºå so this is from the paper or the blog post I think that I linked here so what they do is here they compare their model with a regular residual net 101 this is something we will also talk about later in class that's a convolution network trained on I think it's yet they are it's trained on imageNe„ÄÇ

And I should say„ÄÇNoÔºå nonene of thats clip„ÄÇThing is fine tuned to these dataset sets„ÄÇ

 So it's trained on a different dataset and then applied to dataset sets„ÄÇ

 It has not been fine tuned to„ÄÇ So this restnet here was trained on imagenetÔºå and it reaches 76„ÄÇ

2% accuracy prediction accuracy„ÄÇ The split modelÔºå which was trained on a different dataset also reaches 76„ÄÇ

2% accuracy on this dataset„ÄÇ But what's more impressive is it also achieves good performance on different types of imagenets that were not yeah contained in this training set„ÄÇ

 So in that way„ÄÇThis clip model yeah generalizes better to other types of datasets by this concept of this contrast of language image freetrain„ÄÇ

 So that is also I thinkÔºå an interesting idea or approach and you can also find more about this and the paper here and the code is also available here and this I think can be potentially really useful if for example„ÄÇ

 you yeah develop for exampleÔºå let's say a self-driving car and you have a large data and you want to look for specific videos or clips or different images like I want to have a image of let's say a pedestrian crossing a busy intersection So in this way you can have maybe this model finding you„ÄÇ

Qury very efficiently in the large data set„ÄÇ And why is that interestingÔºå BecauseÔºå yeah„ÄÇ

 you can imagine you don't have to train the model on all possible queries if you have these word embeddings„ÄÇ

 you can search for something that is similar to your input word embedding„ÄÇ

 and it pulls out the corresponding image„ÄÇSo in that way„ÄÇ

 I think this is actually a very interesting approach„ÄÇYeah„ÄÇ

 and here is a big picture overview of how the method works„ÄÇ

 It has been some time since yeah lecture  one wayÔºå I think I briefly mentioned it„ÄÇ

 but contrast of pretrainÔºå that's a form of„ÄÇSelf supervised„ÄÇ

Learning and here in this particular caseÔºå what they do is they train a model on text descriptions and images„ÄÇ

 So in the example I gave earlier about self-subervised learning the researchers used an image and then a transformed version of this image and the model had to predict whether it's the same image or not compared to a image that is different„ÄÇ

 So like being able to distinguish different images from each other and modified versions of the same image„ÄÇ

And here in this caseÔºå it's kind of a similar concept„ÄÇ

 but here it's based on text data and image dataÔºå so they train this text encoder on the test text description and the image decoder on the image description so they produce then a so calledled embedding and then„ÄÇ

Based on the text and the image embeddingÔºå the model learns like the relationship„ÄÇ

 So it should be basically let's say for theÔºå I think that's what it means for the diagonal„ÄÇ

 if you have an image from a certain class from this class and a description that matches this class„ÄÇ

 It should give a high relationships score or something like that„ÄÇ

 So you would train the model basically to associate the right description with the right image„ÄÇ

 then you can if you have a data set of descriptions and images„ÄÇ

 you can shuffle them and it should give you a low score if you have a description that does not match your image and so forth„ÄÇ

 So let's say you pretrain your modelÔºå then you can create a data classifier from text label„ÄÇ

 So here what they do is they„ÄÇAndbed the class label in a text descriptionÔºå for exampleÔºå here„ÄÇ

 a photo of a„ÄÇPlanin carÔºå dogÔºå birdÔºå and so forth„ÄÇAnd then yeahÔºå you produce these text embeddings„ÄÇ

 and now imagine you have a new image that you want to classify„ÄÇ you don't know the class neighbor„ÄÇ

 that is what you want to find out„ÄÇ So this a zero shot prediction„ÄÇ

 This means this class label doesn't have to be in necessarily the training set„ÄÇ

So you don't have to have an example of that if you have the right embeddings„ÄÇIn that way„ÄÇ

 what they have here is they have this image„ÄÇ They go give it to the image encoderÔºå and then„ÄÇ

You have an embeddingÔºå and then you would pull the text description embedding that has the highest scores„ÄÇ

 In this caseÔºå a photo of a docÔºå for example„ÄÇ So in this caseÔºå you„ÄÇ

From the text and codingding and the imagery would then pull out the class from that one„ÄÇYeah„ÄÇ

 that is on a high level how this clip method works„ÄÇ

 I mentioned the model G multiple times G 2 and G 3„ÄÇ unfortunatelyÔºå G 3 is not available yet„ÄÇ

 So this is a model by„ÄÇOpen AI for G 2Ôºå I'm actually not sure whether it's already available„ÄÇ

 I haven't followed it very closely„ÄÇ HoweverÔºå theres I also saw now an initiative to train an open source version of that model„ÄÇ

So here they say our primary goal is to replicate GT3 sized model„ÄÇ

Models and open source to the public for free„ÄÇ So they train the models on„ÄÇ

The data set called the pileÔºå which is 825 GB model„ÄÇ

 including like YouTube descriptions papers from Pubmed and so forth„ÄÇ

 And it is like a 200 parameter model„ÄÇ It's very big„ÄÇ SoÔºå yeah„ÄÇ

 it's big initiative because these models for usÔºå normal people are really out of reach„ÄÇ

 You really need a big company to be able to train such a model„ÄÇ

 And I remember when the first bird or G models came out„ÄÇ

 I think it was estimated like the cost was estimated to be like around $100000 just to train the model„ÄÇ

 And I think for this G3Ôºå which is magnet to it's bigger„ÄÇ

 It's probably in the realm of almost  a million dollars to just train the model„ÄÇ

 Sos it's actually good then that someone maybe„ÄÇDoes that in a young in a way that is then also available and free for the public Alright so moving on I also saw it's not directly related to deeping but I also saw yeah this news article today so it's about the yeah popularity of programming languages and tools so this is by O'reilly and they based their analysis based on queries on their website and books and things like that like mostly search terms on their website and what people look for„ÄÇ

So yeahÔºå unsurprisinglyÔºå Python isÔºå of courseÔºå the first place„ÄÇ

 Also it all or it or it's growing again„ÄÇ So it's still not stagnating„ÄÇ So people yeah„ÄÇ

 use more and more pythonÔºå which I think isÔºå yeahÔºå is good to know„ÄÇ

 So you know where things are headed„ÄÇWhat I also found interesting is ScalaÔºå So Scala„ÄÇ

 I thought personally that might be the next hot thing when I was in grad school when I was in grad school„ÄÇ

 everyone started talking about Scala at some point which is language kind of related to jascript or I think it runs in the JVM the ja virtual machine personally I have to say I never really used it and now seeing that there's this big decline I'm also glad that I didn't waste my time learning a new programming language because that's also always a trade off it's like the tradeoff between doing research„ÄÇ

 getting things done and doing things like learning new languages So there's like a trade off you want to always be up to date with current technology and what people are doing because that's how you keep up to date and can produce cutting edge work but then also you don't want to waste your time by pursuing things that are maybe not worthwhile So for now it seems like Python is still a good choice for machine learning and deep learning„ÄÇ

AlrightÔºå so another interesting project I saw was this approach or method by„ÄÇFacebook AI research„ÄÇ

 where they are teaching AI to manipulate objects using visual demos„ÄÇ So it's„ÄÇ

 yeah currently very challenging still to train robots„ÄÇ

 There's a field called reinforcement learningÔºå As you recall from the introductory lectures„ÄÇ

 So in this wayÔºå robots learn by trial and error„ÄÇ So you train a robot to do something by having it fail a lot of times„ÄÇ

And there's another approach to„ÄÇReinforcement learning called inverse reinforcement learning„ÄÇ

 which is like learning based on a demonstration„ÄÇ HoweverÔºå traditionally„ÄÇ

 it was kind of also not very easy because you had to have a virtual environment for that where you can simulate this behavior that you want your agent to mimic„ÄÇ

 So here a Facebook AI research proposed a model based inverse reinforcement learning„ÄÇ

Approach using visual demonstrations on a physical robot„ÄÇ

 So that means the robot can learn by looking at something on a video or series of images instead of having this virtual environment„ÄÇ

 for exampleÔºå a video game context a 3D worldÔºå you can show the robot images of a task that a human performs„ÄÇ

 for exampleÔºå hereÔºå a human is yeah grabbing a bottle and moving a bottle around if you show the reinforcement learning agent the video„ÄÇ

 it canÔºå to some extentÔºå learn to perform this behavior as well„ÄÇ So there's a video„ÄÇ

 I can't unfortunately play it in the slides„ÄÇ but if you are interestedÔºå yeah„ÄÇ

 you can find out more here„ÄÇAnd this blog post address„ÄÇ

 and there's also the code available on Gitthub„ÄÇlastlyÔºå the last topic for today is about TN„ÄÇ

 tea distributed stochastic neighbor embeddingÔºå and this is a very popular technique in deep learning to visualize high dimensional data sets„ÄÇ

YeahÔºå I have an image of Mist„ÄÇ So MnitÔºå as you know„ÄÇ

 is a high dimensional dataset right So for Mness for these  hundredwritten0 digits„ÄÇ

 you have 28 times 28 pixelsÔºå which is 784 dimensions„ÄÇ

 So you have a very high dimensional data set and you can' really easily visualize that in a scatterttleboard for example„ÄÇ

 So you can use TniÔºå thoughÔºå to reduce the dimensionality of this dataset into down to two dimensions„ÄÇ

 and yeah the interesting aspect about a Tni is that it preserves the relationship between the high dimensional space between the objects in a high dimensional space and in the new lower dimensional space„ÄÇ

 So here is„ÄÇThe result of„ÄÇA Tney dimensionality reduction into two dimensions„ÄÇ

 let's say you have two features now x1 and x2„ÄÇ And you can see that the Tney algorithm takes this high dimensional 784 dimensional Mnes embeddings and„ÄÇ

Puts them into this 2D space such that the relationship between the numbers is still preserved„ÄÇ

 You can see all the zeros here cluster together or the seventh cluster together or the fourth fourth„ÄÇ

Clued together and so forth„ÄÇ So you can see it kind of preserves the relationship or yeahÔºå the„ÄÇ

 the relative meaning of these numbers by„ÄÇYeahÔºå when when it is projecting those into a higher dimensional lower dimensional space and you can also think of it as principal component analysis analysis„ÄÇ

 but principal component analysis analysis is a tool it a linear transformation„ÄÇ so PCA„ÄÇ

It's a linearium„ÄÇTransformation„ÄÇAnd this method here„ÄÇ

 the Tsney method is not just a linear transformation„ÄÇ It's a little bit more involved than that„ÄÇ

 I actually taught Tsney in statistics 4Ôºå79„ÄÇI have actually some slides I can upload„ÄÇ

 I had a full lecture on that„ÄÇ I don't want to go into too much detail here„ÄÇ

 but it's a very cool method's„ÄÇYeahÔºå really useful„ÄÇ

 But one kind of downside of that is that you have to tune the hyper parametersmeter in order to make it perform well„ÄÇ

 And usually you don't know because you don't know theÔºå yeah labels always„ÄÇ

 you don't know what a good setting isÔºå so„ÄÇThere was a paper I saw this week„ÄÇ

 it's about initialization is critical for preserving the global structure in both Tsney and UMP so here the researchers say that it's really important that you choose the right parameter when you use Ts the right hyperparmeter and one of those is the random start like how you start Tney the initialization and here they found that TN will perform much much better if you use actually PC as an initialization you basically initialize TN with PCA„ÄÇ

Reult so in that way they found that for exampleÔºå when they compared TN with PCA initialization and the random initialization that TE performed much„ÄÇ

 much better across everything hereÔºå almost everything I think the way they measure that enough I recall correctly„ÄÇ

Is they measure the correlation between the distances in the original space and the reduced space„ÄÇ

 So if I go backÔºå they would compute the distance between two images in the high dimensional space„ÄÇ

And then they would compute it in the low dimensional space in the two dimensional space and then check the correlation for these different approaches and they found basically that using Tney with PCA really preserves the distance between objects in the low dimensional space„ÄÇ

Yet here as you can see from the title there's another method called UMP„ÄÇ

 this is also a very good method and it's recently even become more popular than TN because sometimes it's even better at preserving this structure„ÄÇ

So yeahÔºå with that this isÔºå yeah TneyÔºå I think you may probably want to use it at some point in your class projects„ÄÇ

 It's really useful for visualizing high dimensionional data setsÔºå but in practice„ÄÇ

 it's not really only used for visualizing dataset sets„ÄÇ

 It's usually also used for understanding what a deep neural network doesÔºå for example„ÄÇ

You would apply that to an embedding of a neural network to really understand what information is con contained in the network„ÄÇ

 So if you have a big conversion network„ÄÇ So you usually have„ÄÇLayers that become smaller in size„ÄÇ

 the deeper the network becomesÔºå and then researchersÔºå for example„ÄÇ

 would take the later embeddings and then apply TN to see how well it separates different classes in the slower embedding and so forth„ÄÇ

YeahÔºå just like a last thing here about hisney„ÄÇ So in psychic learn on the random„ÄÇSorry„ÄÇ

 the initialization scheme is a random initialization and yeah here you if you want to use T name practice according to this research paper„ÄÇ

 you want to change that to PCA So instead of using the default random you would want to use PCA because yeah it performs better according to this paper on the previous slide and also there is a discussion right now on the psychic learn issue tracker where„ÄÇ

It's proposed to change this default behavior so that it will also use PCA in by default and then also there are some other tips so if you want to use Tney I would recommend you to check out this issue here where there's a more in- depth yeah discussion and with that I think it's already a long video so enough news for this week I will see you on Tuesday with the next lecture„ÄÇ



![](img/829a5200a794f56317f2f6a8221e5d48_3.png)