# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P158ï¼šL19.4.1- åœ¨æ²¡æœ‰ RNN çš„æƒ…å†µä¸‹ä½¿ç”¨æ³¨æ„åŠ›ï¼šä¸€ç§åŸºæœ¬çš„è‡ªæˆ‘æ³¨æ„åŠ›å½¢å¼ - ShowMeAI - BV1ub4y127jj

All rightï¼Œ in the previous videoï¼Œ we talked about recurrent neural networks with attentionã€‚

 So this attention mechanism helps the recurrent neural networks to deal better with long sequencesã€‚

 Nowï¼Œ we have doing something crazy and remove the recurrent neural network partã€‚

 So we are going to take a look at a model that just uses attention without the R N partã€‚ğŸ˜Šã€‚

So this particular type of attention we are going to look at is also called selfatten and this is the foundation behind or one of the main principles behind the so-called transformer networks that we are also taking a look at so these transformer models are currently state of the art models for long sequence modeling and working with text dataã€‚

So since there are lots of small topics to introduce instead of making one very long videoã€‚

 I decided to split this up further into subsectionsã€‚

 so in this video we are going to take a look at a broad conceptã€‚

And then introduce a very basic form of self attention just for educational purposes to understand the underlying principle behind self attentionã€‚

 and then we are going to take a look at the more sophisticated form that is found in this in the original transformer modelã€‚

And this original transform model also has a concept called multi head attentionã€‚

 And after we cover theseï¼Œ we will thenã€‚Take a look at how these these concepts are combinedã€‚

Into the transformer modelã€‚And I will also introduceã€‚ä¸€ä¸‰ã€‚Interesting insights about thatã€‚

 And also talk about some popular nowadaysï¼Œ popular flavors of thatã€‚And finallyã€‚

 we will end with the implementation of transform models in pythtorchã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_1.png)

Okayï¼Œ soã€‚This is here just a recap of what we covered in the previous videoã€‚ So we had thisã€‚

R an end with the attention mechanismã€‚ and how this worked was that we hadã€‚For each generated wordã€‚

 So we had thisã€‚Orn n hereï¼Œ we call this R N number oneã€‚ And we have this bidirectional R nã€‚

So for each time stepï¼Œ the R N here was creating an output wordã€‚

 and in addition to just receiving the previous hidden stateã€‚

It was also receiving this context vectorï¼Œ which was depending on the whole sequence input hereã€‚

 So we hadã€‚The soil sequence hereã€‚ So the hidden representations hereã€‚

And then we multiplied them by theseã€‚Attention waits hereã€‚

So the attention weights were a normalized version of values computed by a neural networkã€‚

 So this was what we covered in the previous videoã€‚ So the key idea was that weï¼Œ we hadã€‚

The whole sequence in a weighted formï¼Œ as inputã€‚

![](img/9c2432cfe755e6d9aeb4c5d977423b15_3.png)

Now we are going to remove all sequential parts from that modelã€‚

 So were getting rid of all the sequential partsã€‚ we don't use any recurrenceï¼Œ no convolutionã€‚

 nothing like thatï¼Œ nothing that really is specific for processing input sequentially andã€‚

We are going to work towards this so called transformer modelã€‚

 which only relies on the self attention mechanismã€‚

 and the self attention mechanism processes the whole sequence all at onceã€‚

 And this is actually also great for parallelizationã€‚ actuallyã€‚

 transform models are pretty expensive to trainï¼Œ but they are better at utilizingã€‚

OrMiple GP U because you can train in parallel withã€‚R and Nï¼Œ you generate one thing at a timeï¼Œ rightã€‚

 And so you can't run these in parallel because you have toï¼Œ for this not to compute thisã€‚

 you have to have finished this partã€‚ So transform us a little bit better or a lot better at parallellying computationsã€‚

And similar to the R and Nï¼Œ the many to many oneã€‚ we will also have an encode and a decoder partã€‚

 But instead of here using RN or LSTMsï¼Œ we use something called stacked attention layersã€‚

 So this is what we are working towards2ã€‚ This is the big picture like adding to these partsã€‚

 and we are going to do this one step at a timeã€‚

![](img/9c2432cfe755e6d9aeb4c5d977423b15_5.png)

The basic foundation for these slides is this paperã€‚

 it's called attentionten is All You and this was yeah the foundational groundbreaking paper in 2017 that introduced the original transformoma architecture which outperformed any other method out there up to this pointã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_7.png)

So and since thenï¼Œ since 2018ï¼Œ the field of natural language processing with transformers has grown tremendouslyã€‚

 so you can see it starts also relatively smallï¼Œ so hereã€‚When the y axisã€‚

 this is the number of parameters inã€‚In millionsï¼Œ I thinkã€‚Yiã€‚

It's always unfortunate when people don't label the Y axisï¼Œ but Iã€‚

Think what they meant here in this article wasã€‚Parameters and millionsã€‚And this one would be 8ã€‚3ã€‚

Billionã€‚Parametersï¼Œ so you can see there's a huge growth curveã€‚Tms of the sizes of these modelsã€‚

 but also a huge growth in popularityã€‚ So I don't have here in this videoã€‚

 but there are also papers like review papers that show the number of citationsã€‚

 often transformers are citedã€‚ and how many models are out there and it's also an exponential growthã€‚

 So it's a veryï¼Œ very popular fieldã€‚ But of courseã€‚

 this is something that is not feasible for normal human being to train 8ã€‚3 billion parametersã€‚

 Then nowaday is also models or research groups focusing on developing small transform modelsã€‚

 In any caseï¼Œ this is just like big picture showing that transform models are interestingã€‚

 there are many different flavorsã€‚ We are talking about this foundational one attention is all you need and if you are interestedã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_9.png)

![](img/9c2432cfe755e6d9aeb4c5d977423b15_10.png)

You can follow up with yeah some other models later I will also briefly talk about GT2 and the BRT modelã€‚

 which are also kind of foundational modelsï¼Œ the main concept behind that because they are they are using cell supervised learning techniques that have been also then adopted in other types of transformersã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_12.png)

Okayï¼Œ coming backã€‚Toã€‚The self attention mechanismã€‚ So before I talk about the self attention mechanism that is used in the transformersã€‚

 I wanted to cover a very basic form of that just to yeah introduce the topic slowlyã€‚

So this very basic formï¼Œ we can think of it as a procedure consisting of three stepsã€‚

 so the first step is deriving the attention weightsã€‚

 which are a form of similarity or compatibility between a current inputã€‚

 So current input in the sequenceï¼Œ one sequence elementï¼Œ you can think of it asã€‚A wordã€‚

In the sentence and all other inputsï¼Œ soã€‚The similarity between a given word and all the other words in the sentenceã€‚

So once we I will show you how we derive the weights in the next slideã€‚ So once we have the weightsã€‚

 we normalize them via the softmax functionã€‚ This is similar to what we have done in the R and Nã€‚

 by the wayï¼Œ when we computed the normalized attention weightsã€‚And then we will instead step4 hereã€‚

 compute the attention value from the normalized weightsã€‚And the corresponding inputsã€‚

So this whole thing looks very similar to what I've shown you before with the R and Nã€‚

 So the R and N attention mechanismã€‚ So also hereï¼Œ we have as the attentionã€‚Value hereã€‚

We have awaited some hereã€‚Soã€‚This yearï¼Œ ex Janeã€‚Is an input a wordã€‚

We assume we have T words in our sentenceã€‚ and so for each wordã€‚We haveã€‚And attention weightã€‚

So let's call this word J to J Tã€‚ So we have T words in our sentence and thenã€‚Noã€‚

 it's a little bit unfortunate because I was hiding thisã€‚Soã€‚Word Jã€‚ And this one hereã€‚

 you can see this is I Jã€‚Soï¼Œ the I is for the Iã€‚Worktã€‚So this is an attention weightã€‚

For the relationship between the I word and word Jã€‚ And you use that to compute thisã€‚

Attention value for the Ih input for the I word in the sentenceã€‚

 So it's maybe a little bit denseier in terms of informationã€‚ So let's look at this step by stepã€‚

 and I will also show you how these attention weights these as here are computedã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_14.png)

So here at the topï¼Œ I haveï¼Œ againï¼Œ what I've showed you on the previous slide where we compute the output corresponding to the I input to the I wordã€‚

 So if I every time I write input hereï¼Œ I inputï¼Œ I meanã€‚For instanceã€‚

 a sentence and I input would be the I wordã€‚Andã€‚How do we now compute these attention ratesã€‚

 So in this simpleï¼Œ very basic form of self attentionï¼Œ just for introductory purposesã€‚

 let's assume we compute this as the Dutch productã€‚Betweenã€‚Hereã€‚The I input word and word Jã€‚

 So let's sayã€‚Wtã€‚å–‚ã€‚Andã€‚ğŸ˜”ï¼ŒWt Gã€‚And then we repeat that for all the inputs in the sentenceã€‚

 for all the wordsï¼Œ the T wordsã€‚Soï¼Œ we getã€‚äºŒç³»ã€‚1ã€‚E I2 Eã€‚3 up to E myã€‚åœã€‚And when we computeã€‚

The normalized form using the soft next functionã€‚So then all theseã€‚

The normalized ones will sum up to oneã€‚

![](img/9c2432cfe755e6d9aeb4c5d977423b15_16.png)

And these will be then our attention ratesã€‚Yeahï¼Œ so to summarize the previous slidesã€‚

 here is a visual representation of what we have just talked aboutã€‚Soï¼Œ assume we haveã€‚

This input sequence hereã€‚ So here the input sequenceï¼Œ you can think of it as a sentenceã€‚

And each x hereï¼Œ each vector represents a wordã€‚So I said vectorï¼Œ because this is an embeddingã€‚

 So an embedding of the wordã€‚ And yeahï¼Œ we have talked about this in the context of R and ends where weã€‚

 for instanceï¼Œ convert the word into an integer index and then we retrieve the embedding from an embedding matrixã€‚

 So the embedding is essentially just a continuous valued vectorã€‚For each particular wordã€‚

And then we compute in step oneï¼Œ the similarityã€‚With let's call that the queryï¼Œ the current inputã€‚

Let's call itã€‚ğŸ˜”ï¼ŒQueredã€‚Andã€‚Hereï¼Œ for instanceï¼Œ Iã€‚Could be oneã€‚ So the firstã€‚First word hereã€‚

 for instanceï¼Œ So we would do that with every wordï¼Œ but we would startï¼Œ let's sayã€‚

 with the first wordï¼Œ then walk through step 1ï¼Œ step 2ï¼Œ step 3ã€‚

 and then we would move on to the second word and do the same thing with step 1ï¼Œ step 2 and step 3ã€‚

 So hereã€‚The output is AI equals  one for the first stepã€‚ And then for the second stepï¼Œ it's 2ã€‚

 And then we would stack them all upã€‚ so we wouldã€‚Getï¼Œ essentiallyã€‚Matrix hereã€‚

I'm getting a little bit ahead of myself hereï¼Œ so expanding one thing at a time so we use this or we can use a dot product to compute the similarity why a dot product well it's just one way we we can compute this compatibility or similarity between the vectors we could also consider other functions like cosine similarity it's essentially in just a normalized dot productã€‚

 but let's say keep things simple's we use a dot product so we compute the dot product here betweenã€‚

The queryï¼Œ X I and each other word in the sentenceï¼Œ rightï¼Œ So notice it's x1 hereï¼Œ X2ã€‚

 So for each oneï¼Œ we compute thisã€‚Similarityï¼Œ which is a scalarã€‚ rightï¼Œ So eachã€‚

 each thing here is a scalar single numberã€‚And then we put that through the softm function so that they are normalizedã€‚

 we have now our normalized attention scores hereã€‚Which are then values between 0 and 1ã€‚

 and they sum up to oneã€‚I'm sorryï¼Œ it should beã€‚T hereã€‚Okayï¼Œ fromã€‚I equals 12 Tã€‚umã€‚Anywaysï¼Œ soã€‚

 and then we sum them up hereã€‚ So we have theã€‚Attention valuesï¼Œ which isã€‚A vectorã€‚Rightã€‚

 because x X J is select as xj are our inputs wordsã€‚ So we are going here from X J to X Tã€‚

 So we are going over all the inputs hereã€‚ So we are now waitinging the inputsã€‚With thisã€‚

Correspondingã€‚Aï¼Œ so what we are doing is we weight this inputï¼Œ and thenã€‚We addedã€‚

To this weighted input and add it to this weighted input and add it to this weighted inputã€‚

And then this gives us a vectorï¼Œ rightï¼Œ because we are adding themã€‚In this vectorã€‚Iã™ã€‚

Essentially just like a word embeddingï¼Œ except that it contains no information about the whole sequenceã€‚

 so this original word embedding here only contains information about the word itselfã€‚

So no matter where the word is in this sentence and no matter what the sentence looks like to start with theã€‚

The word that' sayï¼Œ the wordã€‚Helloã€‚Would always have the same embedding when we put it into the modelã€‚

 that's also in R and Nï¼Œ when we have the embeddingï¼Œ we always have the same valueã€‚

 no matter where it is in the sentenceã€‚If it's the first wordï¼Œ the second wordï¼Œ the last wordã€‚

 and no matter what the other words areã€‚ But nowï¼Œ in contrast hereï¼Œ oh wellã€‚Our output hereã€‚

If we have the queryã€‚S word 1ã€‚It is also a representation of this wordã€‚ Let's say helloã€‚

Except it contains information about hello in the context of all the other wordsï¼Œ rightã€‚

 Because we have thisã€‚This waiting step here going onã€‚

 So we have now a more powerful context to where a benddding vectorã€‚Soã€‚What we did is essentiallyã€‚

 in terms ofã€‚Yeahï¼Œ extracting information instead of just considering each word individually we we now haveã€‚

Representations of words aware of its contextã€‚ So this is like what I call a very simple basic form of self attentiontentionã€‚

 This isï¼Œ of courseï¼Œ not what is used in the transformerï¼Œ but it's just to introduce the topicã€‚

 So in then in the next video we will look at the more sophisticated versionã€‚ But I thinkï¼Œ yeahã€‚

 this one kind of summarizes the whole conceptã€‚ I the whole conceptï¼Œ butã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_18.png)

![](img/9c2432cfe755e6d9aeb4c5d977423b15_19.png)

One of the main ideas behind attentionã€‚Like deriving the contextã€‚ All rightï¼Œ so in the next videoã€‚

 let's take a look at the more sophisticated version thenã€‚



![](img/9c2432cfe755e6d9aeb4c5d977423b15_21.png)

![](img/9c2432cfe755e6d9aeb4c5d977423b15_22.png)