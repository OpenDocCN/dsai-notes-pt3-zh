# P158ÔºöL19.4.1- Âú®Ê≤°Êúâ RNN ÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®Ê≥®ÊÑèÂäõÔºö‰∏ÄÁßçÂü∫Êú¨ÁöÑËá™ÊàëÊ≥®ÊÑèÂäõÂΩ¢Âºè - ShowMeAI - BV1ub4y127jj

All rightÔºå in the previous videoÔºå we talked about recurrent neural networks with attention„ÄÇ

 So this attention mechanism helps the recurrent neural networks to deal better with long sequences„ÄÇ

 NowÔºå we have doing something crazy and remove the recurrent neural network part„ÄÇ

 So we are going to take a look at a model that just uses attention without the R N part„ÄÇüòä„ÄÇ

So this particular type of attention we are going to look at is also called selfatten and this is the foundation behind or one of the main principles behind the so-called transformer networks that we are also taking a look at so these transformer models are currently state of the art models for long sequence modeling and working with text data„ÄÇ

So since there are lots of small topics to introduce instead of making one very long video„ÄÇ

 I decided to split this up further into subsections„ÄÇ

 so in this video we are going to take a look at a broad concept„ÄÇ

And then introduce a very basic form of self attention just for educational purposes to understand the underlying principle behind self attention„ÄÇ

 and then we are going to take a look at the more sophisticated form that is found in this in the original transformer model„ÄÇ

And this original transform model also has a concept called multi head attention„ÄÇ

 And after we cover theseÔºå we will then„ÄÇTake a look at how these these concepts are combined„ÄÇ

Into the transformer model„ÄÇAnd I will also introduce„ÄÇ‰∏Ä‰∏â„ÄÇInteresting insights about that„ÄÇ

 And also talk about some popular nowadaysÔºå popular flavors of that„ÄÇAnd finally„ÄÇ

 we will end with the implementation of transform models in pythtorch„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_1.png)

OkayÔºå so„ÄÇThis is here just a recap of what we covered in the previous video„ÄÇ So we had this„ÄÇ

R an end with the attention mechanism„ÄÇ and how this worked was that we had„ÄÇFor each generated word„ÄÇ

 So we had this„ÄÇOrn n hereÔºå we call this R N number one„ÄÇ And we have this bidirectional R n„ÄÇ

So for each time stepÔºå the R N here was creating an output word„ÄÇ

 and in addition to just receiving the previous hidden state„ÄÇ

It was also receiving this context vectorÔºå which was depending on the whole sequence input here„ÄÇ

 So we had„ÄÇThe soil sequence here„ÄÇ So the hidden representations here„ÄÇ

And then we multiplied them by these„ÄÇAttention waits here„ÄÇ

So the attention weights were a normalized version of values computed by a neural network„ÄÇ

 So this was what we covered in the previous video„ÄÇ So the key idea was that weÔºå we had„ÄÇ

The whole sequence in a weighted formÔºå as input„ÄÇ

![](img/9c2432cfe755e6d9aeb4c5d977423b15_3.png)

Now we are going to remove all sequential parts from that model„ÄÇ

 So were getting rid of all the sequential parts„ÄÇ we don't use any recurrenceÔºå no convolution„ÄÇ

 nothing like thatÔºå nothing that really is specific for processing input sequentially and„ÄÇ

We are going to work towards this so called transformer model„ÄÇ

 which only relies on the self attention mechanism„ÄÇ

 and the self attention mechanism processes the whole sequence all at once„ÄÇ

 And this is actually also great for parallelization„ÄÇ actually„ÄÇ

 transform models are pretty expensive to trainÔºå but they are better at utilizing„ÄÇ

OrMiple GP U because you can train in parallel with„ÄÇR and NÔºå you generate one thing at a timeÔºå right„ÄÇ

 And so you can't run these in parallel because you have toÔºå for this not to compute this„ÄÇ

 you have to have finished this part„ÄÇ So transform us a little bit better or a lot better at parallellying computations„ÄÇ

And similar to the R and NÔºå the many to many one„ÄÇ we will also have an encode and a decoder part„ÄÇ

 But instead of here using RN or LSTMsÔºå we use something called stacked attention layers„ÄÇ

 So this is what we are working towards2„ÄÇ This is the big picture like adding to these parts„ÄÇ

 and we are going to do this one step at a time„ÄÇ

![](img/9c2432cfe755e6d9aeb4c5d977423b15_5.png)

The basic foundation for these slides is this paper„ÄÇ

 it's called attentionten is All You and this was yeah the foundational groundbreaking paper in 2017 that introduced the original transformoma architecture which outperformed any other method out there up to this point„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_7.png)

So and since thenÔºå since 2018Ôºå the field of natural language processing with transformers has grown tremendously„ÄÇ

 so you can see it starts also relatively smallÔºå so here„ÄÇWhen the y axis„ÄÇ

 this is the number of parameters in„ÄÇIn millionsÔºå I think„ÄÇYi„ÄÇ

It's always unfortunate when people don't label the Y axisÔºå but I„ÄÇ

Think what they meant here in this article was„ÄÇParameters and millions„ÄÇAnd this one would be 8„ÄÇ3„ÄÇ

Billion„ÄÇParametersÔºå so you can see there's a huge growth curve„ÄÇTms of the sizes of these models„ÄÇ

 but also a huge growth in popularity„ÄÇ So I don't have here in this video„ÄÇ

 but there are also papers like review papers that show the number of citations„ÄÇ

 often transformers are cited„ÄÇ and how many models are out there and it's also an exponential growth„ÄÇ

 So it's a veryÔºå very popular field„ÄÇ But of course„ÄÇ

 this is something that is not feasible for normal human being to train 8„ÄÇ3 billion parameters„ÄÇ

 Then nowaday is also models or research groups focusing on developing small transform models„ÄÇ

 In any caseÔºå this is just like big picture showing that transform models are interesting„ÄÇ

 there are many different flavors„ÄÇ We are talking about this foundational one attention is all you need and if you are interested„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_9.png)

![](img/9c2432cfe755e6d9aeb4c5d977423b15_10.png)

You can follow up with yeah some other models later I will also briefly talk about GT2 and the BRT model„ÄÇ

 which are also kind of foundational modelsÔºå the main concept behind that because they are they are using cell supervised learning techniques that have been also then adopted in other types of transformers„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_12.png)

OkayÔºå coming back„ÄÇTo„ÄÇThe self attention mechanism„ÄÇ So before I talk about the self attention mechanism that is used in the transformers„ÄÇ

 I wanted to cover a very basic form of that just to yeah introduce the topic slowly„ÄÇ

So this very basic formÔºå we can think of it as a procedure consisting of three steps„ÄÇ

 so the first step is deriving the attention weights„ÄÇ

 which are a form of similarity or compatibility between a current input„ÄÇ

 So current input in the sequenceÔºå one sequence elementÔºå you can think of it as„ÄÇA word„ÄÇ

In the sentence and all other inputsÔºå so„ÄÇThe similarity between a given word and all the other words in the sentence„ÄÇ

So once we I will show you how we derive the weights in the next slide„ÄÇ So once we have the weights„ÄÇ

 we normalize them via the softmax function„ÄÇ This is similar to what we have done in the R and N„ÄÇ

 by the wayÔºå when we computed the normalized attention weights„ÄÇAnd then we will instead step4 here„ÄÇ

 compute the attention value from the normalized weights„ÄÇAnd the corresponding inputs„ÄÇ

So this whole thing looks very similar to what I've shown you before with the R and N„ÄÇ

 So the R and N attention mechanism„ÄÇ So also hereÔºå we have as the attention„ÄÇValue here„ÄÇ

We have awaited some here„ÄÇSo„ÄÇThis yearÔºå ex Jane„ÄÇIs an input a word„ÄÇ

We assume we have T words in our sentence„ÄÇ and so for each word„ÄÇWe have„ÄÇAnd attention weight„ÄÇ

So let's call this word J to J T„ÄÇ So we have T words in our sentence and then„ÄÇNo„ÄÇ

 it's a little bit unfortunate because I was hiding this„ÄÇSo„ÄÇWord J„ÄÇ And this one here„ÄÇ

 you can see this is I J„ÄÇSoÔºå the I is for the I„ÄÇWorkt„ÄÇSo this is an attention weight„ÄÇ

For the relationship between the I word and word J„ÄÇ And you use that to compute this„ÄÇ

Attention value for the Ih input for the I word in the sentence„ÄÇ

 So it's maybe a little bit denseier in terms of information„ÄÇ So let's look at this step by step„ÄÇ

 and I will also show you how these attention weights these as here are computed„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_14.png)

So here at the topÔºå I haveÔºå againÔºå what I've showed you on the previous slide where we compute the output corresponding to the I input to the I word„ÄÇ

 So if I every time I write input hereÔºå I inputÔºå I mean„ÄÇFor instance„ÄÇ

 a sentence and I input would be the I word„ÄÇAnd„ÄÇHow do we now compute these attention rates„ÄÇ

 So in this simpleÔºå very basic form of self attentionÔºå just for introductory purposes„ÄÇ

 let's assume we compute this as the Dutch product„ÄÇBetween„ÄÇHere„ÄÇThe I input word and word J„ÄÇ

 So let's say„ÄÇWt„ÄÇÂñÇ„ÄÇAnd„ÄÇüòîÔºåWt G„ÄÇAnd then we repeat that for all the inputs in the sentence„ÄÇ

 for all the wordsÔºå the T words„ÄÇSoÔºå we get„ÄÇ‰∫åÁ≥ª„ÄÇ1„ÄÇE I2 E„ÄÇ3 up to E my„ÄÇÂÅú„ÄÇAnd when we compute„ÄÇ

The normalized form using the soft next function„ÄÇSo then all these„ÄÇ

The normalized ones will sum up to one„ÄÇ

![](img/9c2432cfe755e6d9aeb4c5d977423b15_16.png)

And these will be then our attention rates„ÄÇYeahÔºå so to summarize the previous slides„ÄÇ

 here is a visual representation of what we have just talked about„ÄÇSoÔºå assume we have„ÄÇ

This input sequence here„ÄÇ So here the input sequenceÔºå you can think of it as a sentence„ÄÇ

And each x hereÔºå each vector represents a word„ÄÇSo I said vectorÔºå because this is an embedding„ÄÇ

 So an embedding of the word„ÄÇ And yeahÔºå we have talked about this in the context of R and ends where we„ÄÇ

 for instanceÔºå convert the word into an integer index and then we retrieve the embedding from an embedding matrix„ÄÇ

 So the embedding is essentially just a continuous valued vector„ÄÇFor each particular word„ÄÇ

And then we compute in step oneÔºå the similarity„ÄÇWith let's call that the queryÔºå the current input„ÄÇ

Let's call it„ÄÇüòîÔºåQuered„ÄÇAnd„ÄÇHereÔºå for instanceÔºå I„ÄÇCould be one„ÄÇ So the first„ÄÇFirst word here„ÄÇ

 for instanceÔºå So we would do that with every wordÔºå but we would startÔºå let's say„ÄÇ

 with the first wordÔºå then walk through step 1Ôºå step 2Ôºå step 3„ÄÇ

 and then we would move on to the second word and do the same thing with step 1Ôºå step 2 and step 3„ÄÇ

 So here„ÄÇThe output is AI equals  one for the first step„ÄÇ And then for the second stepÔºå it's 2„ÄÇ

 And then we would stack them all up„ÄÇ so we would„ÄÇGetÔºå essentially„ÄÇMatrix here„ÄÇ

I'm getting a little bit ahead of myself hereÔºå so expanding one thing at a time so we use this or we can use a dot product to compute the similarity why a dot product well it's just one way we we can compute this compatibility or similarity between the vectors we could also consider other functions like cosine similarity it's essentially in just a normalized dot product„ÄÇ

 but let's say keep things simple's we use a dot product so we compute the dot product here between„ÄÇ

The queryÔºå X I and each other word in the sentenceÔºå rightÔºå So notice it's x1 hereÔºå X2„ÄÇ

 So for each oneÔºå we compute this„ÄÇSimilarityÔºå which is a scalar„ÄÇ rightÔºå So each„ÄÇ

 each thing here is a scalar single number„ÄÇAnd then we put that through the softm function so that they are normalized„ÄÇ

 we have now our normalized attention scores here„ÄÇWhich are then values between 0 and 1„ÄÇ

 and they sum up to one„ÄÇI'm sorryÔºå it should be„ÄÇT here„ÄÇOkayÔºå from„ÄÇI equals 12 T„ÄÇum„ÄÇAnywaysÔºå so„ÄÇ

 and then we sum them up here„ÄÇ So we have the„ÄÇAttention valuesÔºå which is„ÄÇA vector„ÄÇRight„ÄÇ

 because x X J is select as xj are our inputs words„ÄÇ So we are going here from X J to X T„ÄÇ

 So we are going over all the inputs here„ÄÇ So we are now waitinging the inputs„ÄÇWith this„ÄÇ

Corresponding„ÄÇAÔºå so what we are doing is we weight this inputÔºå and then„ÄÇWe added„ÄÇ

To this weighted input and add it to this weighted input and add it to this weighted input„ÄÇ

And then this gives us a vectorÔºå rightÔºå because we are adding them„ÄÇIn this vector„ÄÇI„Åô„ÄÇ

Essentially just like a word embeddingÔºå except that it contains no information about the whole sequence„ÄÇ

 so this original word embedding here only contains information about the word itself„ÄÇ

So no matter where the word is in this sentence and no matter what the sentence looks like to start with the„ÄÇ

The word that' sayÔºå the word„ÄÇHello„ÄÇWould always have the same embedding when we put it into the model„ÄÇ

 that's also in R and NÔºå when we have the embeddingÔºå we always have the same value„ÄÇ

 no matter where it is in the sentence„ÄÇIf it's the first wordÔºå the second wordÔºå the last word„ÄÇ

 and no matter what the other words are„ÄÇ But nowÔºå in contrast hereÔºå oh well„ÄÇOur output here„ÄÇ

If we have the query„ÄÇS word 1„ÄÇIt is also a representation of this word„ÄÇ Let's say hello„ÄÇ

Except it contains information about hello in the context of all the other wordsÔºå right„ÄÇ

 Because we have this„ÄÇThis waiting step here going on„ÄÇ

 So we have now a more powerful context to where a benddding vector„ÄÇSo„ÄÇWhat we did is essentially„ÄÇ

 in terms of„ÄÇYeahÔºå extracting information instead of just considering each word individually we we now have„ÄÇ

Representations of words aware of its context„ÄÇ So this is like what I call a very simple basic form of self attentiontention„ÄÇ

 This isÔºå of courseÔºå not what is used in the transformerÔºå but it's just to introduce the topic„ÄÇ

 So in then in the next video we will look at the more sophisticated version„ÄÇ But I thinkÔºå yeah„ÄÇ

 this one kind of summarizes the whole concept„ÄÇ I the whole conceptÔºå but„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_18.png)

![](img/9c2432cfe755e6d9aeb4c5d977423b15_19.png)

One of the main ideas behind attention„ÄÇLike deriving the context„ÄÇ All rightÔºå so in the next video„ÄÇ

 let's take a look at the more sophisticated version then„ÄÇ



![](img/9c2432cfe755e6d9aeb4c5d977423b15_21.png)

![](img/9c2432cfe755e6d9aeb4c5d977423b15_22.png)