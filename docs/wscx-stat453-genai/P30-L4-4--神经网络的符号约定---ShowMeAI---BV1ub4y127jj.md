# P30ÔºöL4.4- Á•ûÁªèÁΩëÁªúÁöÑÁ¨¶Âè∑Á∫¶ÂÆö - ShowMeAI - BV1ub4y127jj

YeahÔºå in this videoÔºå I want to briefly go over the notational conventions regarding linear algebra and deep learning„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_1.png)

So„ÄÇThis is something we have seen beforeÔºå from the perceptra lecture„ÄÇSo at the top„ÄÇ

 there is a perceptron„ÄÇ And now imagine we do inference„ÄÇ

 So inference in the context of deep learning means basically predicting the label of input feature vector„ÄÇ

 So in the perceptronÔºå assume we have one training example as input or let's say„ÄÇTest„ÄÇ

 test data point or test example as input„ÄÇSo how we would do that is as follows„ÄÇ

 we would have x transpose W plus BÔºå the bias unit and then get the net input and then we give it to the activation function„ÄÇ

 which was the threshold functionÔºå and then we get our prediction„ÄÇSo here this part„ÄÇ

Is doing this linear algebra computationÔºå x transpo dotw plus B„ÄÇSo now this is for one data point„ÄÇ

 We can actually also extend that to multiple data points„ÄÇ

 So if we want to do that for multiple data points like n training examples or n„ÄÇTest examples„ÄÇ

 we can use a design matrix for representing the data and n times M dimensional design matrix„ÄÇ So N„ÄÇ

Its the number„ÄÇOf examples„ÄÇAnd„ÄÇWe would write it as follows„ÄÇ So here x is n times M„ÄÇW is m times 1„ÄÇ

B is just as scalar as a1„ÄÇAnd then the output should be n times M dot M times1„ÄÇ

 that should be n times 1„ÄÇYÔºå in times1Ôºå that should be our output„ÄÇ So in this way we have a vector„ÄÇ

 So each value in the vector is the net input for the corresponding test example„ÄÇ

 So that is just how we can process more data points at the same time with still a single operation here„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_3.png)

NowÔºå in deep learningÔºå we usually have these neural networks with hidden layout representations„ÄÇ

We will learn about that a little bit later in this courseÔºå actuallyÔºå like I think next week already„ÄÇ

 So we haveÔºå let's say one data point is input here„ÄÇ That's the feature vector and„ÄÇFeatures„ÄÇAnd then„ÄÇ

 we will have„ÄÇA number of outputs„ÄÇ So let's say H„ÄÇIs the number of hidden„ÄÇUnits and a hidden layer„ÄÇ

 And then we have also multiple hidden layer„ÄÇ so we can have another hidden layer and another one and so forth„ÄÇ

 So in this wayÔºå what I want to highlight here is that whereas in the perceptronÔºå we have a„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_5.png)

![](img/b5b959b0efe1a59facc6c66c335a8c4c_6.png)

Single output here„ÄÇHereÔºå we can have multiple outputs„ÄÇSo how can we deal with such a„ÄÇ

 yeah scenario using linear algebra„ÄÇ So what we can do is we can now have a weight matrix„ÄÇ

 So before I talked about the design matrix for the inputs now„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_8.png)

There's one„ÄÇInput„ÄÇVectctor 1 data pointÔºå but multiple outputs„ÄÇ So this way we can use a matrix here„ÄÇ

 So this matrix would be„ÄÇH times m dimensional„ÄÇSo„ÄÇüòîÔºåLet's write that down„ÄÇÂóØ„ÄÇH times M„ÄÇ

 So H rows and M featuresÔºå whereas the features are corresponding to the M here„ÄÇ So the M„ÄÇ

Matches here„ÄÇ So what we do is we put the W in front„ÄÇThe x second„ÄÇ So this would be„ÄÇ

 if I write this down„ÄÇH times M dot„ÄÇM times 1„ÄÇAnd B should be then also the same dimension as H„ÄÇ

Cause there is one bias unit for each„ÄÇComputation here„ÄÇ So the output should be Hm M dot M times 1„ÄÇ

H times1Ôºå H times one dimension output vector„ÄÇIt's also what I've written down here„ÄÇ I just see„ÄÇ

 So yeahÔºå so in this caseÔºå this is how we can deal with multiple outputs and we will learn how this works also„ÄÇ

 yeahÔºå shortly later on next week„ÄÇ OkayÔºå let's now put both concepts together„ÄÇ So we have multiple„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_10.png)

Training„ÄÇExamples and multiple„ÄÇOutputs„ÄÇSo we have now a matrix for the weights and a design matrix for the inputs„ÄÇ

 So let's write it like this„ÄÇ So W has a dimensionality„ÄÇH times MÔºå like on the previous slide„ÄÇAnd x„ÄÇ

Ha's the dimensionality n times M„ÄÇ it's a design matrixÔºå right„ÄÇ

 But now if you want to multiply those twoÔºå you notice that they are not compatibleÔºå right„ÄÇ

 The inner dimensions don't match„ÄÇ So that's why we have the transpose here„ÄÇ So get me„ÄÇWrite it„ÄÇ

 like„ÄÇThisÔºå so this has a dimensional team„ÄÇOn M times„ÄÇAnd nowÔºå so those dimensions match„ÄÇM„ÄÇ

Is H times one dimensional„ÄÇSo if I multiply those„ÄÇ So these match inner our ones„ÄÇ So we have n„ÄÇ

 So this one is„ÄÇH times n plus h times 1„ÄÇ So the resulting„ÄÇResulting matrixÔºå if we„ÄÇLook at this one„ÄÇ

The dimensionality of that one should be„ÄÇEach„ÄÇüòîÔºåTimes„ÄÇüòîÔºåNÔºå however„ÄÇ

 it is usually nice for each layer to resemble the input dimensions of the previous layerÔºå right„ÄÇ

 So the original inputÔºå you can think of x as the original input was„ÄÇN times M„ÄÇ

 So it would be nice if we have for this one as input to the next layer„ÄÇ

 the same near dimensionalityÔºå like the same ordering„ÄÇ I meanÔºå not exactly the same dimensionality„ÄÇ

 but the in the first dimensionÔºå the inputsÔºå the number of input should be the same„ÄÇ

 we are carrying over the same number of training examples„ÄÇ So what we would want is„ÄÇ

For a for the outputs to have the functionalityality n times somethingÔºå So n times„ÄÇH„ÄÇ

 where H is the hidden dimension„ÄÇSo in order to achieve thatÔºå this is why we have the transpose„ÄÇ

 So with this transposeÔºå we have then the n times H dimension„ÄÇ So it's usually nice„ÄÇ

 What I mean is it's nice to have the training examples as the rows always„ÄÇ Now„ÄÇ

 if you think of this„ÄÇWhat's going on hereÔºå you can think of it as a linear transformation that is happening to x„ÄÇ

 So instead of the original M featuresÔºå we now have H features and H could be larger or smaller than M„ÄÇ

 So they are in deep learning both some networks make make it larger and some make it smaller„ÄÇ

So there's a linear transformationÔºå like I saidÔºå but it's not necessary completely true„ÄÇ

 So if this is a nonlinear functionÔºå then it's not a linear transformation„ÄÇ

 It's a linear transformation that goes through a nonlinear activation function„ÄÇ

 but we will talk about this activation function next lecture„ÄÇSoÔºå but yeah„ÄÇ

 this is just like the overall notation„ÄÇ Also in textbooks„ÄÇ

 you may notice that there is no transpose„ÄÇ So I have a short note about that„ÄÇ

 And the reason is in textbooks in older textbooksÔºå especiallyÔºå they have„ÄÇNot the n times N„ÄÇ

 n times M design matrix„ÄÇ They have a M times n design matrix„ÄÇ

 So they have the columns as and rows switchedÔºå which is a little bit confusing„ÄÇ

 I just want to note that in case you stumble upon it„ÄÇ

So usually modern deep learninging has intense and dimensional inputs„ÄÇ

Sometimes it makes things more inconvenient from a linear algebra perspective because youll notice here we have these transpose„ÄÇ

 if you have it the other way around you don't need the transposes„ÄÇ

 but yeah there's always a trade off„ÄÇ

![](img/b5b959b0efe1a59facc6c66c335a8c4c_12.png)

AlrightÔºå so why also this W X notationÔºå Why not X WÔºå actually in the next video„ÄÇ

 I will show you that in PytorchÔºå it's the other way around„ÄÇTraditionallyÔºå though„ÄÇ

 this is convenient because its there's some intuition behind it in terms of linear algebra if you think of traditional methods„ÄÇ

 how you write things in linear algebraÔºå it's that you have a transformation matrix that you apply to a vector„ÄÇ

 so the vector would be here your feature vector„ÄÇAnd this makes it sometimes or for some people„ÄÇ

 easier to think about„ÄÇLinear algebra in a geometry context„ÄÇ So here this is just an identity matrix„ÄÇ

 so nothing is going to happen„ÄÇSo„ÄÇTechnicallyÔºå these„ÄÇDiaagonalsÔºå I think I have a slide on that„ÄÇ

 These diagonals are for scaling„ÄÇOr this diagnosis for scaling values in the vector and„ÄÇ

The other diagonal is for„ÄÇTranslation so moving things„ÄÇ

And it's like easy to see so why the eigen sorry by the identity matrix here is not doing anything because the scaling is here 1 and 1„ÄÇ

 so scaling something by one doesn't change it and translation by0 doesn't change anything either So in this way„ÄÇ

The original inputs are preserved„ÄÇ So alsoÔºå if you think of it as a dot product„ÄÇÊ≤°„ÄÇ

Remove the notation here„ÄÇ If you think of it as a dot product„ÄÇSo one times 0 with this one„ÄÇ

It's basically1 times x1 plus0 times x2Ôºå which is x1Ôºå right„ÄÇ

 And here0 times x1 is 0 plus 1 times x2 is x2„ÄÇ So nothing is going on here„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_14.png)

YeahÔºå he actually has a slide on that„ÄÇ He has like a summary„ÄÇOf what I just meant„ÄÇ So you can also„ÄÇ

 yeahÔºå write it as follows„ÄÇ So whereas„ÄÇIf you you can be compose this into two separate operations vector edition with this scalr here„ÄÇ

 and then you can think of this first one as scaling the x coordinate„ÄÇAnd thenÔºå you can think of„ÄÇ

This one„ÄÇIs scaling the y coordinate„ÄÇ So like I said beforeÔºå these are for scaling„ÄÇ

 and then the other ones are moving„ÄÇ

![](img/b5b959b0efe1a59facc6c66c335a8c4c_16.png)

The vectors„ÄÇ So these are for moving the vectors„ÄÇYeahÔºå hereÔºå I just have a example of the scaling„ÄÇ

 So for exampleÔºå if you have a matrix that has a three hereÔºå it would scale 3„ÄÇ So sorry„ÄÇ

 scale the X X by 3„ÄÇ So it's kind of stretching it here„ÄÇ

And so forth here it scaling or stretching the y axis by a factor of  two„ÄÇAnd„ÄÇ

This one here at the bottom is doing both stretching and scaling„ÄÇ

 and you can maybe as an optional workÔºå think about the translation„ÄÇ

 But in the context of deep learningÔºå I think it might be helpful sometimes to think of it as a series of transformations„ÄÇ

 But yeahÔºå also one of the reasons why we use a linear algebra is more like for making notations more compact„ÄÇ

 and we'll show you in the next video how this is achieved in pytorch„ÄÇ



![](img/b5b959b0efe1a59facc6c66c335a8c4c_18.png)

![](img/b5b959b0efe1a59facc6c66c335a8c4c_19.png)