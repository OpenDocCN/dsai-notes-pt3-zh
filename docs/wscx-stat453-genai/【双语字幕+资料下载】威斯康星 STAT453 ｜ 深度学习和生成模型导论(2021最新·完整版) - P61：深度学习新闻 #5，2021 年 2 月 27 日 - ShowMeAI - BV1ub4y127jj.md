# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂ®ÅÊñØÂ∫∑Êòü STAT453 ÔΩú Ê∑±Â∫¶Â≠¶‰π†ÂíåÁîüÊàêÊ®°ÂûãÂØºËÆ∫(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P61ÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞Èóª #5Ôºå2021 Âπ¥ 2 Êúà 27 Êó• - ShowMeAI - BV1ub4y127jj

YeahÔºå hi everyone„ÄÇ I hope you had a nice week„ÄÇ So last week we discussed a lot of topics related to computational challenges with deep learning„ÄÇ

 So what do we do if we have these larger models we both require more data and more GPUus and also more time to train these models So coincideally this week I found a lot of news related to the topic of making the training of deep learning models more efficient So that is by spreading out the computation across multiple devices in a process called federated learning„ÄÇ

 but then this also brings privacy concerns„ÄÇ So there was also some interesting news related to yeah privacy protection„ÄÇ

And lastlyÔºå I find personally the most exciting topic is how we can train these deep neural networks still on a single GPU because yeah„ÄÇ

 truth is most of us only have access to one or few GPUus„ÄÇ

 So wouldn't it be nice if we could train just simply things on multiple GPUs like hundreds of them but yeah in real life it's not so easy So luckily there are still a bunch of techniques that allow us to train deep neural networks„ÄÇ

 even the large ones on single GPUus„ÄÇ So I will also discuss this topic at the end of today's news session„ÄÇ

 but yeah because there are so many topics I wanted to cover today„ÄÇ

 Let me just dive in and get started„ÄÇYeahÔºå let's start with Feerated learning„ÄÇ

 So last week I explained Feerated learning is yeah learning on or using multiple devices like splitting up a computation across multiple devices„ÄÇ

 So here was an interesting paper by researchers from Apple„ÄÇ

 So this paper was entitled Federated evaluation and tuning for ondevvised personalization„ÄÇ

 system design and Applic„ÄÇSo does this essentially Apple's undevvised machine learning system for federated evaluation and tuning„ÄÇ

 So federated learning is not new„ÄÇ I meanÔºå a lot of people and companies use federated learning that is using multiple machines to achieve something„ÄÇ

So but usually what other companies do is they use federated learning to tune a global neural network„ÄÇ

So imagineÔºå for exampleÔºå you have a server with your deep neural network model„ÄÇ

 and this is let's say an image classifier or something like that„ÄÇ

 So then this global model can learn from yeahÔºå individual people„ÄÇ

 So if you have a cell phone or smartphoneÔºå it can access this model on the server„ÄÇ

 And by if you label your dataÔºå you can provide training data for this model„ÄÇ

 So there's usually one global model„ÄÇI meanÔºå not everyone is using that approach„ÄÇ

 but many people are used this approach where you essentially have or keep one global model„ÄÇ

 In contrast here in this paperÔºå Apple describes a system„ÄÇWhere„ÄÇThere are global parameters„ÄÇ

But the model is trained locally„ÄÇ So all the user data remains inaccessible to the server side„ÄÇ

 So that means here they focus on protecting your privacy„ÄÇ So not sending any user data to a server„ÄÇ

 So all the training happens locally„ÄÇ So you may get some parameters from the global model„ÄÇ

 but essentially you will get a personalized model on your your phoneÔºå for example„ÄÇ

 without sharing your personal data with a serverÔºå which I find actually pretty cool so„ÄÇ

Here's just a sketch of how this worksÔºå where„ÄÇThere are three types of information being shared„ÄÇ

 The red arrow is the task configuration and attachments„ÄÇ

 information then in green the task results and telemetry and then on blue these on device records and here this one the left box represents the end user device and you can see for the blue ones„ÄÇ

 the blue ones are the user dataÔºå the on device records they never really leave the device so on the right hand side is the developer interface So what the developer interface„ÄÇ

Has access to is yeahÔºå the task configuration and the task resultsÔºå because you as a developer„ÄÇ

 you still want to see whether theÔºå yeahÔºå the the learning model learns well„ÄÇ however„ÄÇ

 you don't want to see any user data because that would be than a privacy implication or problem so„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_1.png)

Here's an example of that where they show how this can be used for tuning for news personalization so they have„ÄÇ

 for exampleÔºå different modelsÔºå let's call them run one and run two and yeah so here they involve different parameters and different metrics„ÄÇ

From an AB experimentation result„ÄÇ SoÔºå for example„ÄÇ

 which model should be used on a local system and they only have access to information such as how much things improved for example„ÄÇ

 So here they have a delta percent so they can see basically that run2 improved the percent the daily article views increase it by 1„ÄÇ

87„ÄÇ And if your metric is to maximize the number of article viewsÔºå if that's your metric„ÄÇ

 then you can measure this by just looking at the results you don't have to have access to what types of articles the user reads„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_3.png)

Yes on the previous slide I highlighted these fundamentally different approaches to federated learning one is keeping one global model on the server and then sending user data to that server„ÄÇ

 which sounds from a privacy perspective a little bit I would say questionable the second approach sounded a little bit more privacy friendly where you do the training on the device and your user data never leaves the server„ÄÇ

 however the first approach with a global model is actually not that bad if you take some precautions„ÄÇ

So there's one area of research called differentiable privacy„ÄÇ Let me write this down„ÄÇ

 differentiable„ÄÇPrivacyÔºå so this deals with or this field develops methods for yeah adding some noise to a data set such that you can't identify yeah people from this data„ÄÇ

 So there was one interesting article by Microsoft but of course„ÄÇThis is a broad field„ÄÇ

 Many people are working on this„ÄÇ I just highlight this article because it was just in the news so but be aware that it's not the only approach with this„ÄÇ

But againÔºå just to highlight the problem„ÄÇ So if you share data with a server or with anyone„ÄÇ

 essentiallyÔºå even if you don'tÔºå let's sayÔºå have the names of the persons in that data„ÄÇ

 it may be possible to identify users or people from this data„ÄÇ

 So there was this $1 million Netflix price that has been quite some year ago„ÄÇ

 I think maybe a decade ago even„ÄÇIt was this $1 million price when Netflix had a data set that they shared on Kegggel and they asked people to develop recommendation systems and the best recommendation system„ÄÇ

 the best movie recommender„ÄÇOr the person who developed this or the team would gain $1 million„ÄÇ

So in order to facilitate this competitionÔºå they shared a data set„ÄÇ It had the title„ÄÇ

The title of the movieÔºå the user IDÔºå the date of the rating and the rating itself„ÄÇ

 So the user ID you can think of it here it is not identifying a particular person It's just for helping to make sure if two movies are rated whether they are rated by the same person or not„ÄÇ

 but you still don't know who the person is„ÄÇActuallyÔºå I never really had Netflix„ÄÇ

 but I think they don't have these five star ratings anymore„ÄÇ I think it's working differently now„ÄÇ

 I'm honestly not sureÔºå but I think they don't do ratings anymoreÔºå but„ÄÇ

Probably back in the day they did anyways„ÄÇ So the thing here„ÄÇ

 what happened is that people use the IMDB movie review database„ÄÇ

And using this IMDP movie database and looking at this Netflix data here on the left hand side„ÄÇ

 they were able to identify people in this dataset„ÄÇ So how that worked was they„ÄÇ

By MDB by I think by email or usernameÔºå they could identify the person„ÄÇ

 and then they were just matching the date of the rating and the rating itself„ÄÇ

To the records in IMDB„ÄÇ and by thatÔºå they could identify the user ID here„ÄÇ

To whom it belongs by just matching the date of rating and rating„ÄÇ

 If you do that for a couple of moviesÔºå it becomes very easy to identify certain users by user I D„ÄÇ

 And then you couldÔºå yeahÔºå you can see basically all the movies that the person watched and how they rated that movie„ÄÇ

 And that isÔºå I thinkÔºå a big privacy violation„ÄÇ if you haven't agreed to if you are Netflix user and you haven't agreed to share that information of all the movies you watched„ÄÇ

Then yeahÔºå that might be a privacy validation„ÄÇ Maybe you only have a subset on IM DB„ÄÇ

 but you don't want certain people to see what other movies you watch„ÄÇ

 Then this would be problematic„ÄÇ So differentiable privacy deals with this problemÔºå how you can„ÄÇ

Kind of keep the utility of a dataset set So making the dataset still usableÔºå but avoiding„ÄÇ

The identification of individuals„ÄÇ So it's essentially the broad yeah„ÄÇ

 the broad approach is essentially adding or synthesizing noise and adding noise to this dataset so that the dataset has broadly the same characteristics„ÄÇ

So if you wouldÔºå for exampleÔºå compute the average ratingÔºå the average rating may not change„ÄÇ

 but there's some noise that for a user forgiven user„ÄÇ

 some of the rating is a little bit higher and lower than they were in real so in that way it's not so easy to go to MDB and unambiguously identify these users„ÄÇ

YeahÔºå and what's new hereÔºüIs that Microsoft released a tool set called smart noise„ÄÇ

 So in this smart noise yeah contains some Python API and other things to make this more easy to use in practice so„ÄÇ

There is also a sample Gitthub repository where you can take a look at at some examples„ÄÇ

 and I want to talk too much about the individual techniques„ÄÇ

 So they it's basically a framework for implementing different techniques„ÄÇ

 but what I found interesting is„ÄÇ

![](img/09613e6476d680e6ab682955b4cff563_5.png)

From this list here of the techniques they implement that many of these techniques they involve GNs„ÄÇ

 so GNs are generative adversarial networksÔºå we will cover them later in this class and they are used essentially to learn the training data distribution and then generating samples„ÄÇ

 new samples from that training set distribution so we will cover that later in this class I just found it interesting that most techniques for differentiable privacy seem to employ generative adversarial networks nowadays or at least the techniques they implemented in this smart noise approach„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_7.png)

YeahÔºå related to the topic of data augment I've seen this article this week entitledHums are trying to take bias out of facial recognition programs„ÄÇ

It's not working it„ÄÇSo here in this articleÔºå the authors mention that one likely reason for this bias in face recognition is the lack of diversity in the data sets„ÄÇ

 and they say that one common mitigation approach is to provide algorithms with data sets that represent all groups equally and fairly„ÄÇ

So there was a paper called1 label 1 billion faces usage and consistency of Ra C in computer vision by Zid Khan and Yuun Fu who looked at this problem„ÄÇ

 So they look at the problem whether a diverse data sets can really help and they say yeah it only it can work but only for a very stereotypical sense of fairness so essentially it doesn't really work So heres a quote what they say is the people in the images appear to fit racial stereotypes For example an algorithm was more likely to label in an individual in an image as white if that person had blonde hair so yeah using even a more diverse data doesn't really help with a biased problem so it's still like yeah having these stereotype steps and stuff like that so„ÄÇ

There needs to be more work done on developing systems that are more fair„ÄÇ

 so it's not as easy as making a data more diverse„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_9.png)

Yeah an interesting yet unrelated topic is AutoMLÔºå which stands for automatic machine learning„ÄÇ

 What is automatic machine learning automatic machine learning is about finding yeah good machine learning algorithms and higher parametermeter settings and sometimes also preprocess steps given a certain problem„ÄÇ

So traditionally we humansÔºå we try out different algorithms and typerometer settings and yeah data normalization steps and so forth to see what works well on a given data AutoMl is type is a type of approach of automating this process so it's less workforce for humans it's some system on top of machine learning that yeah learns this how to do it well basically so it's basically automating machine learning in a way„ÄÇ

A specific flavor of OML is neural architecture search„ÄÇ

 this is specific to neural networks where OML can be broader„ÄÇ

 it can also involve yeah traditional machine learning algorithms„ÄÇ

 a neural architecture search is a specific subfield of OTML for neural networks„ÄÇ

 sometimes also it's abbreviated asna for neural architecture search„ÄÇSo here in this article„ÄÇ

 the article is entitled introducingtro model search an open source platform for Find optimal machine learning models„ÄÇ

 There are a couple of other yeah open source platforms for OMl and I had like a little bit of a hard time seeing what's new here„ÄÇ

 but I think what's new is really that they yeah approach this problem a little bit differently„ÄÇ

 I meanÔºå the method is slightly different than other methods„ÄÇ

 So methods I've seen before they're usually based on reinforcement learning„ÄÇ

 evolutionary algorithms or and or combin notatorial search and here it's also looking like a combination of those„ÄÇ

Where they have„ÄÇMultiple trainers are trained asynchronously and then there is some beam search going on looking at the results and then considering the best models„ÄÇ

 they mutate them so making small changes to the best performing ones which is kind of reminding me of how evolutionary algorithms work and they also talk about knowledge distillation and having some transfer learning here„ÄÇ

 or not literally transfer learning more sorry more like weighted transfer„ÄÇSo here„ÄÇ

 when I recall correctly what they do is they transfer the weights from well performing models to new models they want to explore„ÄÇ

 so instead of starting from scratch„ÄÇ

![](img/09613e6476d680e6ab682955b4cff563_11.png)

YeahÔºå so the results look actually pretty good„ÄÇ So compared to„ÄÇPrevious methods„ÄÇ

 So here the dotted lines are the previous methods„ÄÇ They found that their methods here„ÄÇ

 they perform the new method performs better the model search method„ÄÇ SoÔºå yeah„ÄÇ

 my thought is really that this is fundamentallyÔºå the building blocks are not that novel„ÄÇ

 But in the way they are put together„ÄÇ This is a system that performs really well„ÄÇ

 So I think that's the takeaway that this is really„ÄÇIf you want to use it in practice„ÄÇ

 if you want to use something that performs really wellÔºå this might be a good approach„ÄÇ

Yeah what I found interesting thoughÔºå when I read thisÔºå they said in a recent paper„ÄÇ

 we demonstrated the capabilities of model search in speech domain by discovering something I think I cut it off but yeah so this is from this 2021 article and I said in a recent paper I looked at up I looked at the paper it's from 2019 so it's actually not that recent anymore I still think this is an interesting approach so if you ever„ÄÇ

Want to attempt neural architecture search this might be one of the approaches to try„ÄÇ

 I should warn you though neural architecture search is of course very„ÄÇ

 very expensive because yeah I mean training a single deep neural network is already expensive and here in this approach imagine you have to train multiple ones asynchronously and yeah this is yet another computational challenge I would say for people who don't have access to hundreds or thousands of GPUus„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_13.png)

YesÔºå speaking of computational scaling or training large scale models on multiple GPUus„ÄÇ

 so I saw this paper hereÔºåterrappipeÔºå token level pipeline parallelism for training large scale language models„ÄÇ

Which is kind of interestingÔºå so it's related to the G3 language model which has 175 billion parametersmeter„ÄÇ

 and they found a way that they can train it five times more efficiently on 48 large GPUus„ÄÇ

 So they are these transformer models and there are different ways you can make them more efficient so„ÄÇ

One is„ÄÇPartetatiitioning the operations hereÔºå that's in subfigu B„ÄÇ

Where I think this means that they just split the computation„ÄÇ

 Let's say the left and the right hand side and„ÄÇYeahÔºå pass it on to multiple devices„ÄÇ

 So he had device 1 and2„ÄÇ I think there has actually a label problem in that figure when I understand correctly„ÄÇ

 this should be actually part2 here everywhere„ÄÇ Yeah„ÄÇ

 but here it's splitting up the computation across multiple devices by splitting it„ÄÇ

 splitting the layer basically„ÄÇAnother approach is this micro parallelism or micro micro batchchbased pipeline where they split up the batches„ÄÇ

 the mini batches into micro batches if I understand it this correctly here So they split it up into also multiple devices this way and here the novelties essentially that they have another method for parallelism where they split up the input the sequence input into tokens„ÄÇ

 I mean this is what you would do always you would always split it up into token„ÄÇ

 but then they distribute these tokens onto across different devices„ÄÇ

 So each device may have a small number of tokens only to consider„ÄÇ

 so you have parallelism across these tokensÔºå which is I think the new approach here„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_15.png)

YeahÔºå but now let's get to the real interesting part because this concerns training large scale models on a single GPU„ÄÇ

 so I saw this tweet hereÔºå a screenshot of a talk by Zilvane Guar who worked at fast AP„ÄÇ

Yeah now let's get to the real interesting part training large scale models on a single GPU„ÄÇ

 so I saw this tweet here from a talk by Zilvin Google who works at Fast AI„ÄÇ

 which is a company yeah focusing on education around deep learning and developing awesome friendly APIs for Pytorch„ÄÇ

So„ÄÇHere in this talk Zilva and Google summarize the main steps or main approaches for yeah making large model training possible on a single GPU„ÄÇ

 one obvious one that is the easiest also is to reduce the batch size So the input batch size because then if you reduce the batch size you have smaller matrix multiplication So that helps with yeah memory constraints because usually the yeah the problem with GPUus is that you have a fixed size memory„ÄÇ

 So usually commonÔºå I meanÔºå not all of them but common cards between„ÄÇ

 let's say 12 and 20 gigytes of Ram„ÄÇAnd this can be quite limitingÔºå especially for these big models„ÄÇ

 And yeahÔºå a bottleneck is usually matrix multiplication„ÄÇ

 especially if you have fully connected layers like the linear layers in Pytorrch„ÄÇ

 So reducing the batch size can help„ÄÇAnother approach is gradient accumulation is kind of related to the batch size„ÄÇ

 So if you have very small batchesÔºå what will happen is that your updates will be very noisy„ÄÇ

 So remember when we talked about online learning and stochastic gradient descent„ÄÇ

 we will also talk more about that when we talk about choosing learning rates and things like batch normalization„ÄÇ

 in any caseÔºå gradient„ÄÇAccumulation is an approach where you essentially run backward in Pyr twice„ÄÇ

 So you accumulate the gradients from multiple mini batches before you do the updateÔºå basically„ÄÇ

 So in that wayÔºå it allows you to use smaller mini batches„ÄÇ

Another approach is gradient checkpointing„ÄÇ I actually made a brief figure about that„ÄÇ

 I will explain it to you in the next slideÔºå and then there are also these approaches like zero which has something to do with optimizer and then also shard it data distributed data parallel I will also yeah explain to you how that works in a separate slide„ÄÇ

 and there's also model parallelism and pipeline parallelism„ÄÇ

 These are related approaches where you put parts of the model onto separate GPus„ÄÇ

 So imagine you have a model with a lot of parameters and they don't fit into one GPU And this way you simply yeah split the model across different GPUus„ÄÇ

 I also have a small slide about that because I found yeah some interesting new tools that allow you to do that very conveniently„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_17.png)

So here is a visualization of gradient checkpointingÔºå I made it for a notebook„ÄÇ

 I tried it all in practice earlier this year„ÄÇSo here this drawingÔºå I should say„ÄÇ

 is inspired by another post thoughÔºå that I recommend you to check out to read a more in detailed coverage of that if you're interested„ÄÇ

So we talked a little bit about gradientding descent and we have only done it for smaller models like softm regression„ÄÇ

 but of course the same concept applies if we have multilayer perceptrons and convolutional networks with multiple layers„ÄÇ

 so usually we compute first the forward pass„ÄÇAnd then we have a loss function„ÄÇ Let's say„ÄÇ

 this is our outputÔºå a loss„ÄÇ So for thatÔºå you need„ÄÇThe forward pass„ÄÇ

 but then also the signal from the class label„ÄÇ So let's say here you have the class label information and your prediction and you use that when you back propagate to your compute the gradients„ÄÇ

So traditionally what happens is that you keep all these orange nodes„ÄÇ

 the computations from these nodes in memoryÔºå so for different gradients you will need these individual steps from the forward pass and if you just run PyToch regularly for example„ÄÇ

 it will keep all these computations in memory because you will need them to update them so for example„ÄÇ

 just in a very broad conceptwise mannerÔºå so in order to update this node during back propagation you need these two nodes„ÄÇ

And then if you want to update this nodeÔºå you need these two orange nodes„ÄÇWhile you're doing that„ÄÇ

 thoughÔºå you don't„ÄÇ you don't need this part„ÄÇ You will need it at the next stepÔºå right„ÄÇ

 But in that way„ÄÇKeeping it in memory is actually efficient in the given moment because you will need the values later„ÄÇ

 but if you have memory limit limitationÔºå this might be something where you want to offload this or you want to not keep it in memory so„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_19.png)

Graadient checkpointing how it works is as follows„ÄÇ So you will„ÄÇCompute the forward pass„ÄÇ

 but then you delete all this intermediate information because right now„ÄÇ

 you are only focusing on updating this data point„ÄÇ So you only use this in this computation„ÄÇ

 You forget about this because let's say you can't keep it in memory„ÄÇ it's too big„ÄÇBut then„ÄÇ

 when you„ÄÇUpdate the second node here„ÄÇFor exampleÔºå then in that way you recompute these nodes„ÄÇ

 for example and you also recompute these these ones„ÄÇ

 So the blue ones are the ones that are recomputed„ÄÇ it's wasteful to recompute them every time„ÄÇ

 So essentially when you use gradient checkpointing„ÄÇ

 the model training becomes slower because you have to recompute things„ÄÇ

 but it still helps you dealing with the memory limitations because you don't keep everything in memory„ÄÇ

 So in both slides„ÄÇ

![](img/09613e6476d680e6ab682955b4cff563_21.png)

So here this is the regular approach„ÄÇ Everything in orange is always kept in memory„ÄÇ

 So you can see with gradient and checkpointing hereÔºå you keep fewer things in memory„ÄÇ

 only the orange things in memoryÔºå but you need to recompute„ÄÇ

 So that's a tradeoff basically between memory and computational efficiency„ÄÇ it's slowerÔºå but again„ÄÇ

 it helps you with memory limitations„ÄÇ

![](img/09613e6476d680e6ab682955b4cff563_23.png)

![](img/09613e6476d680e6ab682955b4cff563_24.png)

YeahÔºå so the remaining things on the slide are shing zero then model parallelism and pipeline parallelism„ÄÇ

 so it sounded like based on the tweet that it was for single GPU use„ÄÇ

 unfortunately it requires multiple GPUus„ÄÇBut stillÔºå it'sÔºå I think„ÄÇ

 a cool selection of things that we can use to make the model training more efficient„ÄÇ

 So I dont want to also discuss this in nitty gritty detail because that would be a lecture in itself„ÄÇ

 But just to give you a big picture overview„ÄÇ So0 redundancy optimizer„ÄÇ

 that's a technique developed byÔºå I think it was developed by Microsoft„ÄÇ

 which developed this deep speed library„ÄÇ So there's a tutorial in this deep speed library that discusses what0 does in more detail„ÄÇ

üòäÔºåBut the broad overview is that it is essentially about memory optimization using 16 bit floating point operations„ÄÇ

 and yeahÔºå what's nice about it is in contrast to some other things that make model training more efficient„ÄÇ

 Ze does not require any major modification of your model code„ÄÇ So in this way„ÄÇ

 it's more like a wrapper around your modelÔºå but it doesn't require you to modify the model significantly by yourself„ÄÇ

So yeahÔºå what it doesÔºå it's also then reducing memory consumption„ÄÇ

 One is by yeah the 16 bit trainingÔºå but it's also partitioning on the different statesÔºå weights„ÄÇ

 gradients and optimizer states across the available GPUus and CPUUus„ÄÇ

 So that's why yeah you need also multiple GPU„ÄÇ However„ÄÇ

 that might be something worthwhile to consider if you have multiple GPUus and memory constraints„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_26.png)

So related to that there's also zero offloadÔºå which is yeah kind of related to offloading models„ÄÇ

 So here they even say that it works for models up to 13 billion parameters and they basically say that one of the bottlenecks is also using optimizers like Adam So Adam is something we will be covering soon in the lectures„ÄÇ

 which is Adam is still my favorite optimizer it's actually super nice it's very robust to„ÄÇYeah„ÄÇ

 for it's very robust for training deep neural networks it almost always converges compared to SGD and momentum where you need a little bit more fiddling„ÄÇ

 But we will talk more about that in class„ÄÇAnd they say they developed this implementation„ÄÇ

 deepD speed CPU atÔºå which is an implementation of Adam that is5 to seven times faster than the standard implementation„ÄÇ

 which I also found interesting„ÄÇ

![](img/09613e6476d680e6ab682955b4cff563_28.png)

So„ÄÇYeahÔºå related to all these techniquesÔºå also to the sharding„ÄÇ

 I also saw a tweet by the Pytorch developers so involving a library called fair scale„ÄÇ

 which is developed by Facebook AI Research„ÄÇAnd this is a pytwach extension for efficient large scale training„ÄÇ

 and it uses this fully sharded dataÔºå a data parallel approach„ÄÇ

 So sharding is essentially yeah also a form of splitting the weights„ÄÇ So again„ÄÇ

 here has an article if you are interested in more detail how sharding work„ÄÇ

 So this article describes sharding and here on the left hand side this is yeah combination between„ÄÇ

Parallel data train„ÄÇDistributing the mini batches across GPSus and the sharding„ÄÇ

 So it's a new thing that just got„ÄÇAdded to this libraryÔºå I haven't tried it yet„ÄÇ

 but it also looks like another yet„ÄÇYeahÔºå cutting edge interesting approach„ÄÇ

 So it's actually based on the Deep speed Microsoft Research Library„ÄÇ But I think yeah„ÄÇ

 the goal here in fair scale is to make it a little bit more easier to use within Pytorch„ÄÇ

 So I looked at some things that fair scale provides„ÄÇ

 So what I also found interesting was this pipeline parallelism in fair scale„ÄÇ

 I found this particularly interesting because it looks like super easy to use„ÄÇ So„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_30.png)

Heres a problem where you haveÔºå for exampleÔºå the model that doesn't fit into a single GPU„ÄÇ

 and let's say you have a second GPU that you could use„ÄÇ technically„ÄÇ

 what you could do is all the other tricks I mentioned before„ÄÇ

 like reducing the batch size and so forth hereÔºå what you could do is you can just put also different parts of the model on different GPU is simple very similar to the model parallelism„ÄÇ

 except that we use Pwach pipeline here„ÄÇ So the Pwach sequential pipeline„ÄÇ We discussed this„ÄÇ

 I think when we discussed the„ÄÇPytoch API in lecture 5„ÄÇ

So where I showed you how you can have multiple layers in a sequential API„ÄÇ

 so here you can also use the sequential API for the distributed training„ÄÇ

 So if you have multiple layersÔºå let's say layer A and a layer B are two layers„ÄÇ

 C and D are two more layersÔºå you can„ÄÇPut them all into your sequential pipeline and then it will automatically take care of it„ÄÇ

 distributing it across devices„ÄÇ So hereÔºå across two devicesÔºå two GPUus„ÄÇ

 and you can also say how balanced these are so you maybe can say if you have a stronger GPU put more onto the first GPU than the other GPU and so forth„ÄÇ

 and then also yeah there's a chunk perimeter„ÄÇSoÔºå yeah„ÄÇ

 that is something I will probably try out pretty soon„ÄÇ I haven't played around with that yet„ÄÇ

 but it looked super cool because it's yeah simple to use„ÄÇ But yeahÔºå if you have one GPU„ÄÇ

 this would of course be a bottleneck„ÄÇ but so it would still be a problem„ÄÇSo in that way„ÄÇ

 I would rather consider reducing the batch size and gradient checkpointing if you have a single GPU„ÄÇ

 AllrightÔºå so it was probably a very long video I will just stop it at this point and I will see you back in class on Monday„ÄÇ



![](img/09613e6476d680e6ab682955b4cff563_32.png)