# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P77ï¼šL10.5.1- Dropout èƒŒåçš„ä¸»è¦æ¦‚å¿µ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ noï¼Œ we are finally getting to the more interesting part of this lectureï¼Œ talking about dropoutã€‚

 Or at least that's what I find a little bit more interesting because it's a method specifically developed for neural networks and not just an extension of generalized linear modelsã€‚

ğŸ˜Šï¼ŒSo to keep things a little bit organizedï¼Œ I split this topic into four sub topicsã€‚ Howeverã€‚

 don't worryï¼Œ these will be probably very short videosã€‚ So there are not many slides per topicã€‚

 but I thought it's just simpler to organize it this wayã€‚ So in this first videoã€‚

 I want to just give you the brief big picture overview the main concept behind dropodã€‚

 and then we will talk about the co adaptation technique not a technique interpretation of dropodã€‚

 And then we will talk about the ensemble method interpretationã€‚

 And then we will see how we can do that in Pytorchã€‚



![](img/5850120663c2da5ecd72d6a4e41c42ba_1.png)

So for referenceï¼Œ if you're interestedï¼Œ you can later also check out these papersã€‚

 They are pretty nicely written and not too complicatedã€‚So these two papersï¼Œ as far as I knowã€‚

 are the original papers proposing dropoutã€‚ So one goes back to 2012ã€‚

 It's entitled Iroving neural networks by Preventing Co adaptation of feature detectorsã€‚

 So like I mentionedï¼Œ we will see what this core adaptation interpretation meansã€‚ laterã€‚

 And that's also anotherã€‚Interpretationï¼Œ it goes back to this second paper Doutã€‚

 a simple way to prevent neural networks from overfittingã€‚

 This is like the more like the ensemble interpretationã€‚ But yeahã€‚

 we will talk about this in more detailã€‚ I just wanted to give you these references for these methodsã€‚

 So if you want to check out more detailsï¼Œ you can find them hereã€‚



![](img/5850120663c2da5ecd72d6a4e41c42ba_3.png)

So what is the main concept behind dropoutã€‚ So in a nutshellï¼Œ it's yeah about dropping notesã€‚

 So suppose you have a multi data perception hereã€‚ That's a very small oneã€‚

Essentially it's doing training randomly dropping nodeã€‚ So for exampleï¼Œ hereï¼Œ as shownã€‚

 you would randomly drop this node with a certain probability soã€‚

If you say the dropout probability is 0ã€‚5 like 50%ï¼Œ then during training for each noteã€‚

 you would say with 50% probabilityï¼Œ I will just delete it during the forward passã€‚

And in each training forward passï¼Œ it will beï¼Œ yeahï¼Œ different nodes that will be deletedã€‚

Because it's a probabilityã€‚ So you randomly create different modelsã€‚ So oneã€‚

Forward path might deleteã€‚This oneï¼Œ another forward path would have these two deletedã€‚

Another forward past may haveã€‚This one deleted and so forthã€‚ So why that is usefulã€‚

 There are a couple of theoriesï¼Œ and we will talk about thisã€‚

 here' just like the big picture overview how it worksã€‚ I meanï¼Œ like what the mainï¼Œ whatã€‚

 what's happening basicallyã€‚ And then why that helps that is a topic for the next videoã€‚



![](img/5850120663c2da5ecd72d6a4e41c42ba_5.png)

Yeahï¼Œ now the question isï¼Œ how can we implement that efficientlyã€‚

 So how can we implement a dropout without having to make any major modifications to our neural network architecture like deleting nodes or something like thatã€‚

 So how can we just do it convenientlyã€‚ So one way would be by implementing a beulli sampling procedureï¼Ÿ

 So for thatï¼Œ we can define a drop probability Pã€‚So for exampleï¼Œ this could be 50%ã€‚Wowï¼Œ let's seeã€‚5ã€‚

 that's our drop probabilityã€‚ So with that probability for each given nodeã€‚

 the it's the probability that we drop that nodeã€‚Nowã€‚We create a vector hereã€‚

 So it's like a random vectorï¼Œ a random sample from a uniform distribution in the range 0 and 1ã€‚

 So this vector will have the same number of elements as the size of the hidden layer that we want to apply dropout toã€‚

 So dropout is only applied to hidden layersã€‚ It's not applied to the output layerã€‚

So now let's say we have done thatã€‚ We haveï¼Œ let's sayï¼Œ five units in our hidden layerã€‚

 So we have five numbers betweenã€‚0 and1ã€‚I'm just writing down some arbitrary numbers hereã€‚Oopsã€‚

Like thisã€‚Andã€‚Nowï¼Œ that is when we apply basically a benoli sampling where we say for each value in that vector for each value Vã€‚

We set this value to 0ã€‚ if this value is smaller than a drop probabilityï¼Œ otherwise we set it to1ã€‚

So we are modifying this now that it is a binary vectorã€‚ So this one is smaller than 05ã€‚

 So we set it toã€‚Oneï¼Œ this oneï¼Œ ohï¼Œ sorryï¼Œ should be0ã€‚ This one is greaterã€‚So we set it to oneã€‚

 This one is smallerã€‚ so we set it to 0ã€‚ this one it's on the borderã€‚ we have defined smallerã€‚

 so let's set it to oneã€‚And then this one would be alsoã€‚0ï¼Œ so we have now this binary vectorã€‚

And then we can simply model or multiply the binary vector with our activations aã€‚

 So these are the activations in our hidden layerã€‚ And then now we have effectively cancelled activations andã€‚

Yeahï¼Œ that's itã€‚ That's how we can apply dropout during trainingã€‚



![](img/5850120663c2da5ecd72d6a4e41c42ba_7.png)

Okayï¼Œ so but there is one little thing we have to adjust if we want to use our model for inferenceã€‚

 like making predictions on new data or even computing the test set accuracyã€‚Soï¼Œ yeahã€‚

 the reason is we don't want to have any randomness when we make real world predictionsï¼Œ rightã€‚

 So it would be a little bit awkward if let's say we predict someï¼Œ let's say credit scoresã€‚

 but sometimes random nodes get droppedã€‚ So the customer would on different days get different credit scoresã€‚

 although nothing else has changedã€‚ So that wayï¼Œ we don't want to have any randomness during testingã€‚

 So the dropboard is only applied during trainingã€‚So what effectivelyã€‚

 what it means we don't do any removal of the nodes during testingã€‚ Howeverã€‚

 there's one additional thing we have to do when we want to remove the dropping of the notess and that has to do with the scale of the activationsã€‚

 So assume I have a multi layer perceptronã€‚With multiple hidden layersã€‚

 let's call that hidden layer H1 and this hidden layer H 2ã€‚

 And they are fully connected to each otherï¼Œ like alwaysã€‚

So now if I drop one of the nodes though in H1 by a dropoutï¼Œ let's say I do it like thisã€‚ Wellã€‚

 let's sayã€‚Let's say it like thisã€‚ Each value here is the sameã€‚ The weights are all oneã€‚

 and the activations are all one just to keep things simpleã€‚Soã€‚If this is the caseã€‚

The activation at that given note here is 1 times 1 plus 1 times 1ã€‚Plus1 times 1ï¼Œ it's 3ã€‚Rightã€‚

 so if I have theã€‚Some overã€‚The widthsã€‚Andã€‚Activations from the previous layerï¼Œ I sum that upã€‚

 the net inputã€‚ Let's say the net input is 3 because all the weights and all the activations are one just for simplicity nowã€‚

If I drop one of these nodes during using dropoutã€‚Then now I only have two inputsã€‚ So the valueã€‚

Now is only two instead of threeï¼Œ because I only have two incomingã€‚

Activations when computing the net inputã€‚Okayï¼Œ so that meansï¼Œ I meanã€‚

 this doesn't have any or is not a problem during trainingï¼Œ Of courseï¼Œ I meanã€‚

 this is just like some arbitrary numbers I'm showing you hereã€‚

 but now if you don't do any dropout during testingã€‚All the activations will be larger rightã€‚

 so during testingï¼Œ we don't have any dropoutã€‚Soï¼Œ the valuesã€‚

Are actually larger than during trainingã€‚ During trainingï¼Œ we had a value of let's say2ã€‚

 and during testingï¼Œ we will have a value of 3ã€‚Howeverã€‚

Everything in the network is kind of has learned to work with these smaller valuesã€‚ So the thresholdã€‚

 let's sayï¼Œ in the last layer for the prediction would not really work forã€‚

For these larger values nowï¼Œ because yeahï¼Œ we have all now throughout the whole networkã€‚

 larger values because we don't drop any nodeã€‚ and that could be awkwardã€‚

 So what we do is we just scale these activationsã€‚ That's like a very simple trick to deal with this problemã€‚

 So let's sayã€‚We have a drop probably of 0ã€‚5ï¼Œ and we have an activation of threeï¼Œ one let's sayã€‚

We had two during trainingã€‚Because this one node got randomly droppedã€‚ I meanã€‚

 this is just a simplificationã€‚ And then doing testingã€‚

 the value here that comes in here is 3 because there is no dropping of any nodeã€‚

 So we have now this problem that this too largeã€‚ So how can we do it get it smallerã€‚

 we can just scale it by one minus the drop probabilityã€‚ So in this caseï¼Œ okayã€‚

 this doesn't make quite sense because this is not an even numberã€‚Let'sã€‚

 let's do it proper properly hereã€‚Sorryï¼Œ so let's do a network where we have 1ï¼Œ2ï¼Œ3ï¼Œ4 and oneï¼Œ2ã€‚

3 nodes hereã€‚ same thing applies during trainingï¼Œ we drop 50%ï¼Œ so we dropã€‚Two nodes randomlyã€‚

 So the value during training we get is2ï¼Œ but during testingã€‚We don'tã€‚Drop anythingã€‚

 So the value we may get at this note here is no 4ã€‚So how do we scale itã€‚

 We can scale it by  one minus the probabilityï¼Œ for exampleï¼Œ can have then 4 times 1 minusã€‚

5 which is 4 times 0ã€‚5 which is essentially 2 So in this way we scale all the predictions during training testing sorry so that's like a simple trick to deal with these activations that might be too large during testing and yeah this is essentially how dropout worksã€‚



![](img/5850120663c2da5ecd72d6a4e41c42ba_9.png)

So in the next videoï¼Œ I want to talk about the core adaptation interpretationã€‚

 So one way to interpretï¼Œ drop out why it might work in practiceã€‚

 Then there's an alternative interpretation looking atã€‚Dropout as an ensemble methodã€‚

 And then I will show you how we can do dropout in pyrchã€‚

 And there's also a concept for that called inverted dropoutï¼Œ which will be interestingã€‚

 It's a slight modification of what I just talked aboutã€‚ It's called inverted dropoutã€‚

 I will call talk about that when we yeah talk about the implementationã€‚



![](img/5850120663c2da5ecd72d6a4e41c42ba_11.png)