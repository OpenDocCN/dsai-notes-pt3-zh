# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P37ï¼šL5.5- (é€‰ä¿®)å¾®ç§¯åˆ†å¤ä¹  2- æ¢¯åº¦ - ShowMeAI - BV1ub4y127jj

All rightï¼Œ let's not talk about gradientsï¼Œ the derivative of multiã€‚All rightã€‚

 let's now talk about gradientsï¼Œ the derivatives of multivariable functionsã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_1.png)

So here we are now interested in functions that have multiple inputsã€‚

 so note though that the term multivariable and multivariate are sometimes used interchangeablyã€‚

 so multivariate is usually a more common termï¼Œ but here we really mean multivaririable because multivariate can mean multiple outputs here we are currently only focusing on a case where we have multiple inputs and a single output so for exampleã€‚

 if you consider linear regression you can have multiple inputs like the features of your datasetã€‚

 So if you have multiple a train example with multiple featuresã€‚

 or think of the iris data set where you have s lengthã€‚

 S width a pital length and pital width there would be multiple inputsã€‚

 but the output there would be only one output for exampleã€‚

 a continuous value like for regression or class label for the predictionã€‚å—¯ã€‚Soï¼Œ yeahï¼Œ in this caseã€‚

 we are focusing now on the multi variableable functionsã€‚

 So consider a function where we have multiple inputsã€‚ if we only had one inputã€‚

We would usually write the derivative asã€‚Follows with aã€‚Smaller letter D hereã€‚Nowã€‚

 since we have multiple variablesï¼Œ we have actually a vector as the derivativeã€‚

 this is also the so called gradient we use this upside down triangleï¼Œ the Nala hereã€‚

 So this is I think it's spelled like this Nala So here we have multiple things going on now in this neckã€‚

So each row is a partial derivativeã€‚ So what is a partial derivativeã€‚

 It's essentially like the full derivativeã€‚ Howeverã€‚

 it's for a function where we have multiple input argumentsã€‚

 And if we compute the partial derivative of F with respect to xï¼Œ what we do is we setã€‚

Y and Z here or with multipoã€‚Once we said all of the all of theseã€‚As constantsã€‚

 So we treat them as regular numbersï¼Œ not as variablesã€‚ While we compute this oneã€‚

 And then once we have computed this partial derivativeï¼Œ we then focus on the second entryã€‚

 the partial derivative of F with respect to y whileã€‚Considering x and z as constants and so forthã€‚

 So we compute each derivative one at a timeã€‚ the rules are the same as for the full derivativeã€‚

 So really the big difference here is only that we treat the other variables as constantsï¼Œ but yeahã€‚

 that is essentially a very similar conceptã€‚

![](img/f2777a0e00499d4d9a6c873abcd3c252_3.png)

So here's an example to illustrate thatã€‚ So consider this function here x squared y plus y so that two inputs x and y for this function hereã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_5.png)

So when we then compute the gradient of this functionï¼Œ we have two partial derivativesã€‚

 the partial derivative of x with respect to the partial derivative of f with respect to x and the partial derivative of f with respect to yã€‚

 so we can compute them separatelyã€‚ We don't have to intermix themã€‚ We can do one at a timeã€‚

 So let's start with the partial derivativeã€‚ use the same color maybeã€‚

Partial derivative root of F with respect to xï¼Œ soã€‚What we do isã€‚We canï¼Œ we treat whyã€‚As a constantã€‚

 So you can think of it as a regular numberã€‚ So it's essentially like a computing Xã€‚

Times 1 plus  oneï¼Œ something like thatã€‚So and if you think of the power ruleã€‚

 what happens is the two goes up up front and then minus-1ã€‚ So it will be justã€‚To xï¼Œ andã€‚

This one will stayï¼Œ rightã€‚ So if this was a two or somethingï¼Œ this wouldã€‚Stayã€‚

 So here also the Y staysã€‚ So we have2 x Yã€‚And then plus the derivative of a constant is 0ã€‚

 So plus 0ã€‚ So the derivative would be 2 x y for this oneã€‚

 So using the power rule and the constant ruleã€‚Nowï¼Œ we have computed this partial derivative to Xï¼Œ Yã€‚

 Now we can compute the partialã€‚Divative for the second entryã€‚Forã€‚

The partial derivative derivative for F with respect to yã€‚ So that's what I have done hereã€‚

 Notice instead of xï¼Œ we are focusing no on yã€‚So x hereã€‚ X is noã€‚A constantã€‚

And if this this basically then staysï¼Œ rightï¼Œ So we have thenã€‚Just two x squared as the constantã€‚

 So it's like if you consider a case where we have something like2 x and we compute the f of xã€‚

We compute the derivative of D F with respect to xã€‚So this would be two rightã€‚ So in the same wayã€‚

 if this is a constantï¼Œ the y goes awayï¼Œ it just staysï¼Œ and the one is also a constantã€‚ sorryã€‚

The y here is the variableã€‚ So the derivative of just the variable is oneï¼Œ rightï¼Ÿ

 So then the derivative of the whole part would be x squared plus 1ã€‚Alrightï¼Œ so we have bothã€‚

 and then we just arrange them in the vector hereï¼Œ and this is No gradientã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_7.png)

Okayï¼Œ now let's consider the case where we have a composite function like the following oneã€‚

 Notice that we have two inputsã€‚So we have G and Hã€‚Two inputs hereã€‚

But they both rely on the same input argument hereã€‚

So there's an x that is shared by both functions hereã€‚

So remember from the regular chain rule that I explained in the last videoã€‚

 if we have a function nesting like this one and we want to compute the derivativeã€‚

 we can do this by computing the derivative of the auto partã€‚Times the derivative ofã€‚The inner partã€‚

Now for the two inputs yeah we have the following caseã€‚

 they depend they have a dependency they depend both on xã€‚ so it's a little bit trickierã€‚

 so we actually combine them because we have this dependency hereã€‚

 so we still use the chain rule hereï¼Œ but we combine the resultsã€‚

 So if we want to compute the derivative of this function with respect to x the inner argumentã€‚

 so note that we are not computing we are not computing partial derivative of Gã€‚

Of F with respect to G andã€‚For exampleã€‚H separately as the gradientï¼Œ because both really depend on xã€‚

 I mean we this we do this and this as part of itï¼Œ but it's not the end result compared to the previous slide where we had a gradient where we had different input argumentsã€‚

Soã€‚Hereï¼Œ like I just saidï¼Œ we have these partial terms with respect to geneã€‚

 So let me use different colorsã€‚This partï¼Œ this is one partï¼Œ andã€‚This steepã€‚ğŸ˜”ï¼ŒOther partï¼Œ howeverã€‚

 both share the same xï¼Œ so the derivative of this function with respect to xï¼Œ the one argumentã€‚

 So in fact we have one argument in a way hereã€‚Weã€‚Combine them here withã€‚

Where is my yellow with this plus symbol here soã€‚Let me clearã€‚ This is maybe more clear like thatã€‚

 We still use the chain ruleã€‚ Let's maybe focus onã€‚On this partï¼Œ for this partã€‚

 we still use the chain rule like hereã€‚ So we have an inner and an auto partã€‚

 So the auto part is a partial derivative F with respect to Gã€‚ hereã€‚

 we use the partial symbol because theres G and Hã€‚Howeverï¼Œ there's only one input argument to Gã€‚

 So here we use a little Dã€‚ If you don't do this like consistently like thatã€‚

 I think no one would blame you if we just use partial symbols that would be fine tooã€‚

 but yeah this would be the proper wayã€‚Soï¼Œ this would beã€‚The derivative root for this partã€‚ And thenã€‚

For the second partï¼Œ we have a partial derivative root of f with respect to H soã€‚

 and then the inner part soã€‚Let me do this one more timeã€‚ So this isã€‚One partã€‚

 this is the other partã€‚And here we have the chain rule for each of themã€‚

 the outta  one and the inner oneã€‚Here we also have the auto  oneã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_9.png)

Andã€‚Alrightï¼Œ let's take a look at an example to apply this in practiceã€‚Yeahã€‚

 consider the following function where we have two input functionsï¼Œ G and Hã€‚

 So this is just for reference what I had on the previous slide and the function is G squared H plus Hã€‚

Where G is also a function that takes as input xï¼Œ where this is 3 xã€‚ So the function G is 3 xã€‚

 and then H also is a function taking input xã€‚ And so this function is x squaredã€‚Allrightï¼Œ soã€‚

Doing it one at a timeï¼Œ so let's focus on this part firstï¼Œ which is hereã€‚

So here this is the partial derivative of x with respect to Gã€‚ So if weã€‚Take a look at thisã€‚

And differentiated with respect to Gã€‚Use the power ruleã€‚ So this goes front and goes awayã€‚

 And this is a constant because it's a partial derivativeã€‚ H is a constantã€‚ So this also goes awayã€‚

 So what the result is this to G Hã€‚Soï¼Œ nowã€‚And let's take a look at the inner partã€‚

 So the inner part of Gï¼Œ soã€‚Let me use this colorã€‚ So G is 3 xã€‚ So we are looking at this one nowã€‚

 So the derivative of 3 x is just 3ï¼Œ rightï¼Œ So if we derive G with respect to its only inputã€‚

 So there is only one inputï¼Œ then we don't have to treat anything as a constant because there is nothing else except xã€‚

 and the derivative of that is 3ã€‚Now let's take a lookã€‚ So we solved basically this in this partã€‚

 Now let's take a look atã€‚The other part hereï¼Œ the partial derivative of F with respect to Hã€‚

 the other argumentã€‚So if we treatï¼Œ So if we computeã€‚Partial derivative of F with respect to Hã€‚

 And we treat everything that is not H as a constantã€‚ So this is a constantã€‚ So in this wayã€‚

If G squared is a constantã€‚Then the derivative of this part with respect to H would be G squaredã€‚

 rightï¼Œ So we haveã€‚3 squaredã€‚And this is then oneï¼Œ rightï¼Œ the derivative of a variable is just oneã€‚

Alrightï¼Œ so G squared plus1ã€‚have it somewhere hereã€‚Rightï¼Œ so here we have itã€‚And then the last partã€‚

 the derivative of H its with respect to its inputã€‚Xï¼Œ so which which colorã€‚

Do we have left here this oneï¼Œ maybeã€‚So let's compute this terminal on the derivative of H with respect to x andã€‚

H is x squaredï¼Œ that derivative should beã€‚2 x using the power ruleã€‚ So that's2 xã€‚

 And now we just bring everything togetherã€‚ So we have the first part and the second partã€‚

So I don't have any colour left that I haven't used yetã€‚

 So let me just use black Well this left part hereã€‚We bringã€‚Those togetherã€‚

 So this this is basicallyã€‚This part and then for the right partã€‚Hereï¼Œ we bringã€‚Those those togetherã€‚

 and this is this partã€‚ And then if we just simplify thisã€‚

 So if you take a look at this and you expand these termsï¼Œ it simplifies as followsã€‚

So this would be how we use the multi variableã€‚

![](img/f2777a0e00499d4d9a6c873abcd3c252_11.png)

Chimruã€‚Yeahï¼Œ I don't want to go overboardï¼Œ but we can also yeah go one step further and write this more compactly using vector notationã€‚

 So again the same function here F with two inputs two input functions G and H which in turn take both x as input So if we compute the derivative this is what we have written down before and we can also write this more compactly using vector notationã€‚

We writing it as gradient of f with respect to its inputs and then times the the derivative of a vector Vã€‚

 So the vector vï¼Œ we define it as a vector containing those two inputs hereã€‚And thenã€‚

 the derivative would beã€‚The derivative of the vector would be containingã€‚

Both the derivative of G with respect to x and the derivative of D with respect of H with respect to xã€‚

 So this is more like a compact form of writing down these partial termsã€‚

 So basically this term and this termã€‚ So you know the partial termsï¼Œ but the inner termsã€‚

 So these areã€‚Butï¼Œ you knowã€‚In other routesã€‚And now then putting it togetherã€‚Where this comes fromã€‚

 I meanï¼Œ if you then consider the gradient of the function with respect to its inputsã€‚

 so that's the gradient we discussedã€‚So the partialã€‚Termsï¼Œ and thenã€‚Here the innerrumsã€‚

And if you look at it nowã€‚ So if you use or compute the dot productï¼Œ rightï¼Œ So in orderï¼Œ yeahã€‚

 in order to compute the dot productï¼Œ what we have isã€‚Let me use this colorã€‚This times thisã€‚Plusã€‚

 this times thisï¼Œ rightã€‚ So this is how we usually compute our productã€‚ So it expands to this oneã€‚

So it's just like a little yeah convenience thing to write things more compactã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_13.png)

Yeahï¼Œ there's one more concept to cover the Jacobovian matrixã€‚

 So consider now a case where F is a vectorã€‚ So you have a vector F where F is a functionã€‚

 and there could be multiple inputsï¼Œ soã€‚Let's consider the expanded form hereã€‚ So this vectorã€‚

 So there's F 1ï¼Œ F2 F 3ï¼Œ F Mï¼Œ different functionsã€‚ Each of them has the inputs X 1ï¼Œ x2 x 3ï¼Œ x Mã€‚

 So this could beï¼Œ for exampleï¼Œ a case forï¼Œ yeahã€‚Perceptron where you have a net input functionã€‚

 and it takes multiple features as inputã€‚So the Jacobian of thisã€‚

This vector here is written down hereï¼Œ so the Jacobian is a matrix where all the combinations are consideredã€‚

 so the partial derivative of f1 with respect to x1ã€‚

 the partial derivative of f1 with x2 with respect to x2ã€‚

 the partial derivative of F2 with respect to x1 and so forthï¼Œ Yeahã€‚

 so you can see all the different combinations hereã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_15.png)

All rightï¼Œ I here I seeã€‚ I have highlighted one thing just yeah to highlight this is the gradient of function 1ã€‚

 for exampleï¼Œ with respect to its inputsï¼Œ if you recall the gradient that we discussed earlierã€‚

 So each each row of these of this Jacobco matrix would be basically a gradient of a function or a function gradientã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_17.png)

Yeahï¼Œ okayã€‚ so second order derivativesã€‚ So let's talk about second order derivativesã€‚

 Now I'm just kiddingã€‚ so we won't be talking about second order derivatives because we won't be needing it for this classã€‚

 Actuallyï¼Œ second order derivatives can be usefulã€‚ Some people have done some research working with them in deep learningã€‚

 but so farï¼Œ it has not been very fruitfulã€‚ I meanï¼Œ the resultsï¼Œ I I meanã€‚

 there are proof of concepts that it conceptual worksã€‚

 but it is very slow and there's not really any advantage over using first order methodsã€‚ soã€‚

Despite some research effortsï¼Œ as asã€‚For as I knowã€‚

 no one is really seriously using second auto gradient methods yetï¼Œ I meanã€‚

 at least while at that point when I'm recording thisã€‚

 maybe in the future there will be a method that is vastly superior compared to our current methods but so far it has not been done very prominently or very I wouldn't say successfully because the research in itself was successfulã€‚

 but it's not very practicalï¼Œ I would say allright so with that let's not worry about second auto methods then let's go back to the more exciting parts through the machine learning and deep learning partsã€‚

 So the next two videos I will be wrapping up on this lecture on gradient descent and then training a single layer neural networkã€‚



![](img/f2777a0e00499d4d9a6c873abcd3c252_19.png)

![](img/f2777a0e00499d4d9a6c873abcd3c252_20.png)