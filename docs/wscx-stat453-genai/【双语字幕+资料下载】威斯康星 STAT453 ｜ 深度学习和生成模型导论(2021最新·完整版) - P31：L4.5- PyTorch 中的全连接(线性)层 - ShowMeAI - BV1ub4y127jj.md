# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P31ï¼šL4.5- PyTorch ä¸­çš„å…¨è¿æ¥(çº¿æ€§)å±‚ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ in this last video of this lectureï¼Œ I want to give you a brief teaser of Ptos convenience functionsã€‚

 the linear or fully connected layerï¼Œ but also like to just wrap up the conventions regarding linear algebraã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_1.png)

So yeahï¼Œ what do I mean by fully connected or linear layerã€‚ So if we have a multi layerã€‚Networkã€‚

 like shown here soã€‚This part here would be a linear transformation if we don't consider the activation functionã€‚

 and you can also think of this as a fully connected layerã€‚ So this is sometimes called fullyã€‚

Connectedã€‚ğŸ˜”ï¼ŒLairã€‚Or sometimes people also call it a dense layerã€‚In Pyã€‚

 it's called a linear layer because it's a linear transformationã€‚

 so you can also think of it as a linearã€‚Lairã€‚ğŸ˜”ï¼ŒAnd in the context of kas and Tensorflowã€‚

 people call it aã€‚Denseã€‚ğŸ˜”ï¼ŒLayerï¼Œ so all these things are equivalentã€‚

 So another dense layer or fully connected layer would be from hereã€‚

To here so neural network really isï¼Œ yeahï¼Œ a concatenation of multiple of these fully connected layersã€‚

 which are then interspersed with theseã€‚Non nonlinearã€‚Actation functionsï¼Œ for exampleã€‚

 So you can do this manyï¼Œ many timesã€‚But also multi layer networks are topic for a different time for a different lecture next weekã€‚

 just outlying already how that relates to linear algebra hereã€‚ So how we thenã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_3.png)

Yeahï¼Œ implement a fully connected layerã€‚ and Pythtch is very simpleã€‚

 There's actually a function called torch and end dot linearã€‚ So let's start with the data set hereã€‚

 So let's assume I'm just creating some random dataã€‚

 but let's assume this is some valid training dataã€‚So this iså—¯ã€‚Design matrix with dimensionalityã€‚

 What do I have here 10 times 5ã€‚10 times 5 input matrixã€‚

 And then when we initialize this linear layerã€‚We give it the number of features and the numberã€‚

 So that's the input features and the number of output featuresã€‚ So here we have five featuresã€‚

 like that's our design matrixï¼Œ the n times M matrixã€‚ And let's say we want three output featuresã€‚

 the output featuresã€‚ if I go back one slide areã€‚

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_5.png)

These same what I highlightedã€‚Wellï¼Œ hereï¼Œ you can think of it as these hereï¼Œ the net inputsã€‚

 So that's basically the N number of outputsã€‚

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_7.png)

And they are computed via this linear transformationã€‚All rightï¼Œ soã€‚Then each of these linear layersã€‚

 when you initialize it hereã€‚Has attached as an attributeï¼Œ a weight matrix and a bias vectorã€‚

So the weight matrixï¼Œ you can see it here isã€‚H times M or in this caseã€‚Three times five dimensionalã€‚

 rightã€‚ So you have three rows and five columns of  three by five dimensionalã€‚ So you can seeã€‚

That this5 here matches with this5ã€‚So that's the number of input featuresã€‚

 And the bias unit is equal to the number of output featuresã€‚ So in this caseã€‚

 it's a vector with three valuesï¼Œ soã€‚

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_9.png)

Because each output has a bias unit attached to itã€‚

 So here we would have five inputs in the next slight exampleã€‚

 if I would modify it here and three outputsã€‚

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_11.png)

Alrightï¼Œ soï¼Œ and notice hereï¼Œ these are also small random valuesã€‚

 We will talk about it later why it's useful to have random values here instead of all zerosã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_13.png)

Noã€‚ğŸ˜”ï¼ŒHere I'm printing the dimensions for the inputï¼Œ the weight and the biasã€‚ So let's 10 by 5ã€‚

3 by 5 and 3ã€‚Now I'm applying this fully connected layerã€‚Soï¼Œ I'm applying it to thisã€‚Xã€‚

 which is 10 times 5ã€‚And the Wï¼Œ which is  three times 5ã€‚And the outputã€‚ So there is also plus Bã€‚

 So This is a matrix medication plus Bï¼Œ which is three dimensional and the outputã€‚Isã€‚

What is it 10 times  three dimensional A is a 10 times  three dimensional matrixã€‚

 So how does that workï¼Ÿ So what is going on hereï¼Œ So you can already probably see there must be some transpose here for the Wã€‚

 rightï¼Œ so that the5 goes in front the and the three goes hereã€‚

 And then maybe it's compatible with this xã€‚ But is it really what's going onã€‚ So yeahã€‚

 that's what's going onã€‚ So yeahï¼Œ I have a summary of thatã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_15.png)

So I also have to stare for itã€‚ or I have to stare at it for a few secondsã€‚

 and let me see what I've written down hereã€‚So yeahã€‚

 based on Pytorch we have another convention hereï¼Œ recall in the last videoã€‚

 I mentioned the convention where the Wï¼Œ the transformation matrix or weight matrix is in front of X and Pytorarch is it's after x and I think this makes sense not from a geometry perspective but from a data flow perspective because in this way we have to use fewer transposes and also it's kind of symbolizing the way that data flows through the network so we start with xã€‚

 and then we multiply it by a weight matrix W and then we get a so we have X doã€‚

W resulting in A and stuff like thatã€‚ So it's more like this linearã€‚Thenard flow here actuallyã€‚

 I'm writing always aï¼Œ but it's the net inputã€‚ so it depends on whether we apply the activation function or notã€‚

 actually I have it hereã€‚ No worries then alright soã€‚

If we have an input with one training example like this x hereï¼Œ this vectorã€‚

 then we can use the notation where x is in frontã€‚ if we transpose Wï¼Œ then the dimensions will matchã€‚

 rightã€‚ So here in this caseã€‚It's a1 times m dimensionalã€‚

Vectctor where W is an M times h to dimensional vectorã€‚ So the resultã€‚W will be just one valueã€‚

 The result will be then one times h dimensional vectorã€‚Andã€‚If we have n inputsã€‚

 then we can also keep x in frontã€‚ We can also transpose Wã€‚ Then in this caseã€‚

 what will happen is thatã€‚We have an n times M dimensional one here hereã€‚

 the same M times H dimensional oneã€‚ And here we will have an n times H dimensional oneã€‚ So this wayã€‚

 what is nice about this convention is we can keep the same operations hereï¼Œ whether this isã€‚

Multiple or only one data pointã€‚ So this is actually quite convenient from a computational perspectiveã€‚

 So if we have codeï¼Œ we don't have to change much aroundã€‚

 And if you don't believe me that this is the way Pyr does it here is the source code if you want to look at itã€‚

 so this is like the common convention in Pytorarch how the linear transformation happensã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_17.png)

So yeahï¼Œ just to concludeï¼Œ I meanï¼Œ there are multiple different ways we can compute this linear transformationã€‚

 What's really important is thinking about itï¼Œ like always think about how the dot products are computed when writing and implementing matrix modificationã€‚

 So becauseï¼Œ yeahï¼Œ sometimes it's easy to make mistakes soã€‚

Things may compute because dimensions matchï¼Œ but it's not computing what you wanted it to computeã€‚

 So it's always important to write down what the dimensions are and what you're computing and what you expectã€‚

 Yeahï¼Œ the output isã€‚Because yeahï¼Œ also theoretical intuition and conventionã€‚

 like having the W in front as a transformation matrix does not always match up with practical convenienceã€‚

 like when we write things in codeã€‚Soã€‚Here I've written down some rules that you might find useful when you're reading textbooks and things are not the same as let's say in code so you can easily late between those concepts for exampleã€‚

 if you have two matrixes A and B multiplying those is the same as B transpose a transpose and then taking the auto transpose hereã€‚

 So that's the same thing And also here these are the same So I was just writing down some rules here that help you maybe be navigating switches between code and textbooks I think you probably won't read many textbooks because there are not many deep learning textbooks really yetã€‚

 but maybe in the future sometimestime but also in papers people use all kinds of different conventionsã€‚

 So I think this is like a handy thing to keep in mindã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_19.png)

And yeahï¼Œ also just to summarize traditional versus Pywach conventionsã€‚Soï¼Œ yeahã€‚

 there are multiple ways we can compute this linear transformationã€‚ Like I mentionedã€‚

 we can have this weight matrix up frontã€‚So this would result thenã€‚In H times one dimensional vectorã€‚

Whenã€‚X is a feature vector M times 1 dimensionalã€‚ This is the same using the rule as I that I showed you in the previous slide as writing it like thisã€‚

 Sam thingã€‚So it gives us a wait for the same input gives us the same outputã€‚

Another way we can write that that is how Pythr does that is putting the x in frontã€‚

 but yeah we are transposing W hereã€‚ this gives or this assumes a different input thoughã€‚

 this assumes the one times m dimensional input and yeah this is actually my preferred representation because this is kind of easy when we go back and forth between one or multiple training examples because now here at the bottom would be the cases where we have n training examplesã€‚

 So if we want to use it the traditional way we have to use true transpos which is more workã€‚

 So this is the usual case in deep learning we have usually many inputs and many outputs So we only have to use one transpose it's shorter and this is also the wayã€‚

Pyroch implements itã€‚Soæ‰€æœåˆ°ã€‚Pyarch conventionã€‚ So just to sum it up for this lectureã€‚ Soï¼Œ yeahã€‚

 as a little und homework experimentï¼Œ you mayã€‚

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_21.png)

Yeahï¼Œ you canï¼Œ if you have extra timeï¼Œ revisit the perceptron Ny codeã€‚

 and without even running the codeï¼Œ just thinking about itã€‚

 Can you tell if the perceptron could predict the class tablesã€‚If we feed it in arrayï¼Œ I'm actuallyã€‚

Underigning it in an unfortunate wayã€‚ without running the codeã€‚

 can you tell if the perceptron could predict the class labels if we feed an array of multiple training examples at onceã€‚

 So if we haveã€‚A design matrixã€‚Ofã€‚Dimenssalityã€‚And times M for testingã€‚

Would it be able to run after trainingï¼Ÿ So if yesï¼Œ whyï¼Œ if notã€‚

 what change to the code have do you have to make So you can think of thisã€‚

 And then you can actually run the codeï¼Œ right with design matrix is input for prediction and see whether your intuition was correct and also feel free to open a discussion on Piazza about thatã€‚

So yeahï¼Œ run and verify your initionã€‚ and then how about the train methodsã€‚

Can we also have some parallelism through matrix modifications in the train methodã€‚

 having multiple training examplesï¼Œ so does it make senseã€‚Withoutï¼Œ let's sayã€‚

 fundamentally changing the perception learning ruleï¼Œ would that make senseã€‚

 So it's also another thing to think aboutã€‚

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_23.png)

Allrightï¼Œ soã€‚Next lectureï¼Œ then we will talk about yeah more like a deep learning topic that is not like fundamental linear algebraã€‚

 We will talk about a better learning algorithm for neural networksã€‚

 So we learned about the perceptioncept rule next last weekã€‚

 but this is actually not a very good learning rule and we will yeah develop a better learning rule next lectureã€‚



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_25.png)