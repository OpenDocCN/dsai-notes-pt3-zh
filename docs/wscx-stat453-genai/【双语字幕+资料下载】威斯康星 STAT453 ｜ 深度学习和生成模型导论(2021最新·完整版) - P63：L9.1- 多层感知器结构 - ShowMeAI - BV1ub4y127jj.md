# P63ï¼šL9.1- å¤šå±‚æ„ŸçŸ¥å™¨ç»“æž„ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ now it's finally time that we talk about multi layer perceptionceptsã€‚

 So what is a multi layer perceptionceptronã€‚ So multi layer perceptioncepts are fully connectedã€‚

 feet forward neural networks with one or more hidden layersã€‚

 So what does the fully connected mean hereã€‚ So fully connected means that if you have a given layerã€‚

 each unit in that layer is connected toã€‚All the other units in the next layerã€‚ Soï¼Œ for instanceã€‚

If you have a layerï¼Œ let's say like thisï¼Œ an input layerã€‚ And then you have another layerã€‚

 let's say with only one unitï¼Œ thenã€‚Each input is connected to that oneã€‚

 It's actually similar to logistic regression in that wayã€‚ So in a logistic regression contextã€‚

 we also had yeahï¼Œ everything in the input layout being connected to the next layerã€‚

 Or if we had multiple outputsï¼Œ we had alsoã€‚This setupã€‚ And this is actually similar to aï¼Œ yeahã€‚

 multi layer perceptronï¼Œ exceptã€‚Where in soft mixed regressionã€‚We had an inputã€‚And an output layerã€‚

LetMake maybe one moreã€‚Nowï¼Œ what we have is we haveã€‚A hidden layerã€‚

 So that means there is another output layerï¼Œ for exampleï¼Œ like thatã€‚

 So instead of being soft mixed regressionï¼Œ we have no multi perceptronã€‚

 And in the minimal in the middle hereï¼Œ we have a hidden layerã€‚

 And now this one becomes a new output layerã€‚ So againï¼Œ everything is connected hereã€‚

 So that's why it's called fully connectedã€‚Yeahï¼Œ and the main difference betweenã€‚

A multiceptron and soft mixed regression is that we have this hidden layer hereã€‚Alsoã€‚

 we use a learning algorithm called be propagationã€‚

 which is actually yeah just gradient descent using the chain ruleã€‚

 it's essentially very similar to what we have done in the last lecture when we talked about softm regressionã€‚

Except againï¼Œ that we have no hidden layerã€‚Alrightï¼Œ so yeahï¼Œ another term here is feed forwardã€‚

 So feed forward refers to the fact that we go into one direction fromã€‚

 yeah from left to right in this particular drawingã€‚ But essentially from the input to the outputã€‚

 So that means feed forwardã€‚ laterï¼Œ we will see other networks that are not feed forwardã€‚

 for exampleï¼Œ there are recurrent neural networks where we haveã€‚Recurrentã€‚Layer where yeahã€‚

 you go essentially backwards as a time dimensionã€‚I should also sayã€‚

Not all networks are fully connectedã€‚So later in this courseã€‚

 we will also see convolutional networks which are not fully connectedã€‚Alrightï¼Œ but yeahï¼Œ Imã€‚

I'm way ahead of my slides hereã€‚ I just see I explained almost everything already because yeahã€‚

 I made actually some nice figuresã€‚ So let's take a look at these figuresã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_1.png)

So here I'm showing you a multi layer perceptron with two hidden layersã€‚

 So we have on the left hand sideï¼Œ the input layerï¼Œ then a hidden layerã€‚Another hidden thereã€‚

 and then the outputã€‚Layerï¼Œ and because we only have one output unit similar to logistic regression and different from softm regressionã€‚

 this one would beã€‚A binary classification problemã€‚ And so for the outputã€‚

 we can actually use a sigmoid function similar to yeah logistic regressionã€‚Other than thatã€‚

 there is nothing really new hereã€‚ I meanï¼Œ this is just like softm regressionã€‚

 except that we have now these hidden layersï¼Œ rightï¼Œ So in that wayã€‚

 there's actually not that much to explain becauseï¼Œ yeahã€‚

 we have also the same way these weights connecting each unit between the layersã€‚

And when we then compute the partial derivatives for yeah training this networkã€‚

 we can also use the chain ruleï¼Œ the multivariable chain ruleã€‚ So for instanceã€‚

 if we would want to compute the derivativeï¼Œ the partial derivative of this loss function or the loss here with respect of with respect to one of the weightsã€‚

Then yeahï¼Œ we would also use the chain rule to go backwards and then the multivariable chain rule to combine the output from different unitsã€‚

 So what I mean is if we have the output hereã€‚Then we go backwardã€‚ So we are interestedã€‚ Againã€‚

 we are interested in this weight W 11ã€‚Just as an exampleã€‚ So I'm just highlighting all the pathsã€‚

 So there's one path that goes hereã€‚ You can see it hereï¼Œ rightï¼Œ And the other path is this one hereã€‚

So in in purpleï¼Œ what I'm showing you is this one here at the topã€‚And thenï¼Œ inã€‚

Here for the multiparable channelï¼Œ this isã€‚This path hereã€‚ So there are two pathsï¼Œ actuallyã€‚Yeahã€‚

 andï¼Œ and that is actually itã€‚ That's how we would train or update this weightã€‚

 Notice that there areï¼Œ of courseï¼Œ multiple weights and actually multiple weight layersã€‚

So here we haveï¼Œ for exampleï¼Œ the weight in the second layerã€‚ We will train those alsoï¼Œ of courseã€‚

 So in this caseï¼Œ yeahï¼Œ the rule would be a little bit of differentã€‚ So in this caseã€‚

 let me just take a lookã€‚ We haveã€‚Let me write this downï¼Œ maybe so we have the partial derivativeã€‚

With respect to this oneï¼Œ which should beã€‚Los with respect to the outputï¼Œ and thenã€‚

The output with respect toolã€‚The oneã€‚2ï¼Œ rightï¼Œ then a1ã€‚Torueï¼Œ with respect toã€‚We were saying1ï¼Œ1ï¼Œ2ã€‚ðŸ˜”ã€‚

So in this wayï¼Œ you can see this path here is actuallyã€‚Wellï¼Œ let's seeã€‚Thisã€‚

 this party is actually sharedã€‚Withã€‚ðŸ˜”ï¼ŒThis path hereï¼Œ rightã€‚

 So actually you can reuse if you compute this one firstï¼Œ you can reuse this partã€‚

In order to compute this part for back propagating furtherã€‚ So alsoã€‚

 when you implement this in practiceï¼Œ yeahï¼Œ there are some tricks to avoid recomputing things or gradients hereã€‚

 But fundamentallyï¼Œ that's the same concept that applied to soft mixed regressionã€‚

 So here really what I wanted to say there' is nothing newï¼Œ reallyã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_3.png)

Yeahï¼Œ here's a figure we looked at earlier in this courseã€‚

 these different types of learning algorithms and this algorithm that we use for training the weights and multi perceptioncept that I just showed youã€‚

Based on the partial derivatives or gradients that's yeah called back propagationã€‚

 that's one type of learningã€‚ And yet it remains still as the most precise learning algorithmã€‚

 So there was this paper in 2020 where the authors proposed an an alternative algorithm that may be more similar to what's going on in the brainã€‚

 but the back propagation algorithm is still the most efficient one for training neural networksã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_5.png)

So now let's talk a little bit about the nomenclatureã€‚ So here that's the same figure as beforeã€‚

 And notice that I'm using these roundã€‚Bracktets or parentheses hereã€‚

 So before when we had something like thisã€‚With a square bracketsã€‚

 this must usually referring to theã€‚First activation for the Iã€‚Trainingã€‚ðŸ˜”ï¼ŒExampleã€‚Hereã€‚

 the round parenthesesï¼Œ they don'tã€‚ or this doesn't mean this is the first training exampleã€‚

 This really means here the first hidden layerã€‚ So the round braces hereã€‚ sorryã€‚

 the parentheses refer to theã€‚Hiddenã€‚Layer indexã€‚So here we have two hidden layersã€‚

 So you can see this is the first hidden layerã€‚ And then here we have the second hidden layerã€‚

 And I could alsoï¼Œ instead of writing output hereï¼Œ I could also write activationã€‚With aã€‚

Prenhees here 3ï¼Œ because the way the output is computedã€‚

 if we use a sigmoid function is the same way as these activations are computedã€‚ Soï¼Œ for exampleã€‚

 if you have one activation hereã€‚Let's say the one in the firstã€‚Hidden thereã€‚

 this is really computed as the activationã€‚Based onï¼Œ let's sayï¼Œ some net input Zã€‚

 where Z is computed as yeahï¼Œ the net inputã€‚Hereï¼Œ so I'm not showingï¼Œ I should sayã€‚

 I'm not showing the biases so that should alsoã€‚Be a biasã€‚For eachã€‚Actationã€‚

So I was not drawing it because the figure was already so crowdedã€‚ Seeã€‚

 it's a little bit messy So let me remove this againã€‚

 but there should be a bias also for each activationã€‚Okayã€‚And this isã€‚For this unit hereã€‚

 But now if we take a look at this oneã€‚So for this oneï¼Œ we would compute it the same wayã€‚

Except now that weï¼Œ instead ofï¼Œ so we still have Zã€‚ So let's just use the same matter Zã€‚

 But Z is now notï¼Œ of courseï¼Œ involving the x directlyã€‚ It's based onã€‚The previous layer activationsã€‚

 So in this caseï¼Œ you can think of it as the previous layerã€‚Activationsã€‚Soã€‚W transposeã€‚Plus Bã€‚

 I'm not using indices here for the weightsï¼Œ but this would beï¼Œ of courseã€‚

 these would be the weights from the previous layers respectivelyã€‚

 So here I was just focusing on what the inputs are for thisã€‚Second one hereã€‚

 we have the activations of the previous layerï¼Œ which act as the inputs to that layerã€‚

 if that makes sense soã€‚For this layerï¼Œ we have the excess S inputã€‚And for this layerã€‚

 we have the activations from the previous layers inputã€‚

Actually the main point here I also wanted to make is how we count the layers so it's quite common I've seen that couple of times referring to the input layer as the first layer in the network then the first hidden layer is the layer number two the second hidden layer is the layer number three and the output layer would be layer number4 we could do that but then yeah it would be a little bit weird when we talk about logistic regression because then logistic regression is a two layer network which is kind of weird because usually you think or we think of logistic regression as a single layer neural networkã€‚

So what I mean isã€‚Yeahï¼Œ if we count like thatï¼Œ if we thenã€‚Remove these hidden layersã€‚

 We have an input layer and an output layerï¼Œ like in logistic regressionã€‚

 This would be a two layer networkã€‚ And that is a little bit weirdã€‚ So in this wayï¼Œ we usually useã€‚

The in DC is a little bit differentlyã€‚ So this is like the input layer sometimes also called the zeroth layer orã€‚

Just input layer and the first hidden layer is the first layerã€‚

 the second hidden layer is the second layer and the output layer in this case would be the third layerã€‚

 So that's a little bit more intuitive if we think of logistic regression as single layer neural networksã€‚

So yeahï¼Œ like I saidï¼Œ we can use the sigmoidï¼Œ the logistic sigmoid function here everywhere as activation functionã€‚

Soã€‚Then it would be really like logistic regressionã€‚

 except that we have these hidden layers in betweenã€‚

 But the loss function could also be the negative look likelihood that we talked about orã€‚

We can also call it binaryã€‚Cross entropyã€‚Which is here equivalent toã€‚The negativeã€‚Lookã€‚Likelihoodã€‚

 actually for classification in neural networksï¼Œ that's also the most common loss functionã€‚

And that isï¼Œ againï¼Œ why we spend so much time on logistic regression and softm regressionã€‚

 Because fundamentallyï¼Œ things are very similarã€‚

![](img/d3197a7b20a46c45854b70a38749b6da_7.png)

Yeahï¼Œ and also like in softm regressionï¼Œ if we have multiple output unitsã€‚

Instead of using sigmoid functions hereï¼Œ we can use the softmax functionã€‚ But yeahã€‚

 I don't have to explain how softmax works againã€‚ if you forgot how softmax worksï¼Œ then yeahã€‚

 we have lecture 8ã€‚ So you the nice thing about lecture recordings is that you can revisit lecture 8ã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_9.png)

Yeah now an interesting question is is this really deep learning I mean multilayer perceptrons have been around maybe for 50 years alreadyã€‚

 so is this really deep learning if we have the same multilayer perceptron that has been around for such a long time and yeah how is it different from a neural network onces the differenceï¼Ÿ

Yeahï¼Œ now the interesting question isï¼Œ is this really deep learningï¼Œ I meanã€‚

 multi layerer perceptrons have been aroundï¼Œ I would sayï¼Œ for 50 yearsï¼Œ approximately soã€‚Yeahã€‚

 can we really call that multi layerer perceptionceptron we just saw deep learningã€‚

 So according to the recent trendsï¼Œ when I look at recent papersã€‚

 it's more and more common to call these multi layerer perceptrons deep learningã€‚

So here is just a figure I saw recently in a paper I don't want to reference this paper in this case because I think I don't know I don't want to blame anyone for calling this deep that I want to call out certain people hereã€‚

 but here in this paper I read for example there was this figure of a multilayer perceptron where there were only 10 inputs very small numbers of hidden units and then one outputã€‚

So output unitã€‚ So here they call it yeahï¼Œ deep learning andã€‚

Essentially proposed this deep survival modelã€‚ So yeahï¼Œ like I saidã€‚

 it's more and more common to call these multilayer perceptrons deep learningã€‚

 traditionally when the term deep learning was coinedï¼Œ like I mentioned in the introductory lecturesã€‚

 it usually you referred to the yeah feature learning by using certain tricks that are a little bit about beyond the regular multilayer perceptronsã€‚

 where we had certain algorithmic improvements of neural networks that make the learning more efficientã€‚

 But yeahï¼Œ nowadaysã€‚A multi layer perceptronï¼Œ I would sayï¼Œ is already deep learningï¼Œ in a senseã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_11.png)

Yeahï¼Œ another very important point is that now in a multilayer perceptron the loss is not convex anymoreã€‚

 so linear regressionï¼Œ adelineï¼Œ logistic regression and soft mix regression we had convex loss function so for exampleã€‚

 for linear regression if we recap or the loss look like let's say with respect to one weight because it's easier to drawã€‚

 it would be something like this bowel shape hereã€‚Where let's say we start with an arbitrary weight value hereã€‚

 or let's say we start with weight 0 hereã€‚ So the loss would beã€‚So very high when we startã€‚

 and then we go down with gradient descent till we reach this global minimumã€‚Yeahï¼Œ nowã€‚

 in the case of multi layer perceptronanceï¼Œ it's not that simple anymoreã€‚

 we don't have such a nice convex loss function where there's only a path downward to the global minimumã€‚

So in factï¼Œ these are highly non convex lost surfaces nowã€‚

 so I found a nice paper that is about visualizing the lost landscapes of neural networksã€‚

 So here this is for if you have or consider two weights in a neural networkã€‚

 So if we have let's sayã€‚W1 and W 2ã€‚Let's say W 1ã€‚1 W2 1ã€‚ And let's say in the first hidden layerã€‚

 because there are manyï¼Œ many weights in the neural networkã€‚ but you can seeã€‚

That this loss is highly complexã€‚ There are manyï¼Œ many local minima going on hereã€‚ And hereã€‚

 I think that should be the globalã€‚Minimumã€‚So if we find the values of the weights that we are here at this point for this lossã€‚

 So hereï¼Œ the loss valueï¼Œ let's sayã€‚The higherã€‚ So this is the maximumï¼Œ the higherï¼Œ the largerã€‚

 the loss and the lowerï¼Œ the lower the lossã€‚ So if we are at this global minimum hereã€‚

 then we would have minimized the loss function with respect to the weightsï¼Œ howeverã€‚

Depending on what our starting weights areã€‚ So technique typically in the context of neural network trainingã€‚

 we start with small random weightsã€‚ So one ray reason isã€‚

Why we start with small random weights is that we want to try different configurationsã€‚Soã€‚

 for exampleï¼Œ if we have configurationã€‚Of weights we may start hereã€‚ We could start here or hereã€‚

 depending on what our initial weights areã€‚ And it's not uncommon that we get stuck in a local minimumã€‚

 So we reach one of these local minimaï¼Œ and then the algorithm gets stuckã€‚ in practiceã€‚

 it's actuallyã€‚Almostï¼Œ I would say impossible to find the global minimumã€‚

 So you never know whether you reach the global minimumã€‚

 Usually common knowledge is that you will end up in some of the local minimaã€‚ And it's not uncommonã€‚

 because of that to start the net neural network training multiple times worth different random seatsã€‚

 So sometimes you have a better random seat for a given network than other timesã€‚

 So sometimes you will notice that you will have a better performance just by changing the random seatã€‚

 So in papersï¼Œ it's that also not uncommon toï¼Œ let's sayï¼Œ run a network training 5 or 10 timesã€‚

 then take maybe the best three runs and then average over them and report the standard deviation and things like thatã€‚

Yeahï¼Œ so we deal now with this highly non convex lost surface and in the next couple of lectures we will also talk about tricks for learning rate choosing learning ratesã€‚

 choosing initial weights and choosing optimization algorithms that can help jumping out of these local minima once we get stuck so there are certain yeah tricks and improvements to make the learning more efficient compared to the traditional stochastic gradient descent versionã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_13.png)

Yeahï¼Œ and beyond choosing good starting weights and good learning rates and good optimizersã€‚

 it's also very important to choose good activation functions and combinations of activation and loss functionsã€‚

So yeah hereï¼Œ I'm just want to illustrate a problem with certain activation and loss functionsã€‚

 and in the next videosï¼Œ we will talk aboutï¼Œ let's sayï¼Œ alternative activation functionsã€‚

 which are nowadays a little bit better for training multi layer perceptronsã€‚

 So here for this exampleï¼Œ consider we have a logistic sigma activation and the mean squared error function loss functionã€‚

 Soã€‚One problem we may encounter is that we may end up with very flat gradientsã€‚

And especially then if the output is very wrongã€‚ But if you think about itã€‚

 if we make a very wrong predictionï¼Œ we want to actually have large gradients because then we can update the weights more if the gradients are very flatã€‚

 then there's almost zero yeah weight updateã€‚ and then we can't fix our problem or the wrong predictionã€‚

 So recall how the logistic sigmoid function looks likeã€‚Soï¼Œ if we have aã€‚

Logistic Sigoid function like this with a net inputã€‚Thenã€‚Recall it was like this S shaped functionã€‚

 And let's sayã€‚The true class labelã€‚Is oneã€‚But we return a very low probabilityï¼Œ likeï¼Œ let's sayã€‚

 oopsã€‚The probability class membership probability is 0ã€‚001 or something like thatã€‚ rightã€‚

 So if you look at thisã€‚A lowï¼Œ very negative net input and a low probability will resultï¼Œ of courseã€‚

 in these very flat gradientsã€‚Soã€‚If you then look at the derivative of the loss function with respect to the weightsã€‚

 So here we haveã€‚The derivative of the mean squared errorã€‚With respectï¼Œ let's say to the activationã€‚

 it's called it just sigmaï¼Œ just sigma hereã€‚And thenã€‚This isã€‚

partialial derivative loopops of sigma with respect to the net inputã€‚And thenã€‚ðŸ˜”ï¼ŒThe last termã€‚

 rememberï¼Œ isã€‚The derivative of the net input with respect to the weightã€‚ So here we areã€‚

 again using the chain ruleã€‚ But if we have theseã€‚The derivative of the sigmoid hereã€‚ And now Cã€‚

 combined with the MSEï¼Œ doesn't cancel so nicelyã€‚ So when we had the binary cross entropy and the sigmoid activationã€‚

 things canceled nicely that we ended up with this niceã€‚Learning ruleï¼Œ rightã€‚

So we had this nice learning ruleã€‚ where we didn't have this problemã€‚ But if we useï¼Œ for exampleã€‚

 a means square error function combined with these sigmoid activationsã€‚

 we end up with this derivative here thatã€‚Has the problemã€‚Thatã€‚ðŸ˜”ï¼ŒIt has this term itselfã€‚ Nowã€‚

 if this termã€‚It's very smallã€‚ So if we make a very wrong predictionã€‚ let's say 0ã€‚001ã€‚

 maybe even smallerã€‚ let's say 0ã€‚000001ã€‚ Then we multiply this very small value with all the other values hereã€‚

 So the whole derivative will become very low or very smallã€‚

 And then we will almost update our weights by nothingã€‚ So in that wayã€‚

 we can't fix this wrong predictionã€‚ So it's kind of also important to think a little bit aboutã€‚

What types of activation functions we combine with which types of loss functionsã€‚

Here's another point I madeï¼Œ but hereï¼Œ this point is really I was just saying if you have a multilay networkã€‚

 of courseï¼Œ you would use a softm activation if you have mutually exclusive classes instead of sigmoid activations in the last layerã€‚

 but the same problem would still existã€‚ so that doesn't really solve all problem I just wanted to add this as a small noteã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_15.png)

Alrightï¼Œ so the next video I will show you then different types of activation functions that are a little bit yeah better than logistic sigmoid functions in the multilayer perceptronã€‚

 but MIA as an ungraded homework exercise or piazza discussion point I wanted to I wanted you to think about little aspect about multilayer networksã€‚

 So I have a question here for you what happens if we initialize the multilayer perceptron to all zero weightsã€‚

So before I mentioned we use different small random weights to escape these local minima or just to have different starting pointsã€‚

 so we may end up finding better local minima if we run this multiple times with different random starting weightsã€‚

 but now consider the case where we set all the weights to0 it's justã€‚

Unrelated to these local minimaï¼Œ I just wonder if you can think of what happens if we choose all the same values for the weightsã€‚

 because you may thinkï¼Œ okayï¼Œ why do we use small random valuesã€‚

 Why don't we use just small numbers and have the same number for each weight or have all the weights at 0ã€‚



![](img/d3197a7b20a46c45854b70a38749b6da_17.png)

So that's maybe something to think aboutã€‚ And thenã€‚In the next videoã€‚

 we will talk about linear activation functionsã€‚

![](img/d3197a7b20a46c45854b70a38749b6da_19.png)