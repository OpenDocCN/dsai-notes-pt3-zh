# P153ÔºöL19.0- Áî®‰∫éÂ∫èÂàóÂà∞Â∫èÂàóÂª∫Ê®°ÁöÑ RNN ÂíåËΩ¨Êç¢Âô® - ShowMeAI - BV1ub4y127jj

Yeah hi everyone„ÄÇ So for some reason the semester went by far more quickly than I thought„ÄÇ

 or at least far more quickly than it felt like and yeah we are already here at our last lecture and of course„ÄÇ

 this lectureÔºå like I mentioned will be optional because I know you're working very hard right now on your class projects and'm yeah also really looking forward to watching your class project presentations and reading your reports this week and next week So yeah„ÄÇ

 like I saidÔºå there won't be any quiz or any grading related to this lecture so you can watch it now next week sometime later in summer or maybe not at all if you're not interested in this topic So the topic is recurrent neural networks and transform us for sequence generation So we covered recurrent neural networks before but only talking about classification now we are focusing on generating new data with recurrent neural networks then we will add an attention mechanism„ÄÇ

üòäÔºåTo improveÔºå yeahÔºå the sequence generation process with recurrent neural networks„ÄÇ

 And then we will actually take away the recurrent neural network part and just take a look at attention„ÄÇ

 which is the yeahÔºå the basis behind these transformer networksÔºå which have become very popular„ÄÇ

 especially in the last 1Ôºå2 years„ÄÇ So with thatÔºå yeahÔºå let's„ÄÇ

 let's get started with our last lecture„ÄÇAlrightÔºå so here is an outline of the six broad topics I have in mind for today„ÄÇ

 FirstÔºå we will start with a general discussion about sequence generation with recurrent neural networks Before in an early lecture„ÄÇ

 we talked about recurrent neural networks for classification Now we will learn that we can also use them to generate sequences„ÄÇ



![](img/821c6a0e498995258a60a276c6a09a5c_1.png)

![](img/821c6a0e498995258a60a276c6a09a5c_2.png)

![](img/821c6a0e498995258a60a276c6a09a5c_3.png)

![](img/821c6a0e498995258a60a276c6a09a5c_4.png)

![](img/821c6a0e498995258a60a276c6a09a5c_5.png)

And in particularÔºå we will be taking a look at a so called character RnN„ÄÇ

 a character RnN is a an RnN that takes in one character at a timeÔºå for example„ÄÇ

 letters in the alphabet and can then yeah generate new text from that„ÄÇ



![](img/821c6a0e498995258a60a276c6a09a5c_7.png)

![](img/821c6a0e498995258a60a276c6a09a5c_8.png)

![](img/821c6a0e498995258a60a276c6a09a5c_9.png)

Then we will learn about a concept called attention Or that was formulated with or four recurrent neural networks„ÄÇ

 so„ÄÇ

![](img/821c6a0e498995258a60a276c6a09a5c_11.png)

![](img/821c6a0e498995258a60a276c6a09a5c_12.png)

That at each time stepÔºå the recurrent neural network can pay attention to certain parts of the sequence„ÄÇ

 and this was particularly useful for dealing with long sequences„ÄÇ



![](img/821c6a0e498995258a60a276c6a09a5c_14.png)

![](img/821c6a0e498995258a60a276c6a09a5c_15.png)

So then we will learn what some concept called or the paper was at least called attention is all you need and this is about using just this attention or self-atten mechanism without using an RN and the researchers have shown that just using the attention mechanism can get you a really good performance and you actually don't need the recurrent neural network for that so this gave rise to the so-called transformer models that we will then talk about and then I will also show you„ÄÇ



![](img/821c6a0e498995258a60a276c6a09a5c_17.png)

![](img/821c6a0e498995258a60a276c6a09a5c_18.png)

![](img/821c6a0e498995258a60a276c6a09a5c_19.png)

![](img/821c6a0e498995258a60a276c6a09a5c_20.png)

![](img/821c6a0e498995258a60a276c6a09a5c_21.png)

![](img/821c6a0e498995258a60a276c6a09a5c_22.png)

![](img/821c6a0e498995258a60a276c6a09a5c_23.png)

As one code exampleÔºå how we can use transformers and pythtorch„ÄÇ So okay„ÄÇ

 these are our six broad topics„ÄÇ Let's start at the top with the next video and talk about sequence generation with recurrent neural networks„ÄÇ



![](img/821c6a0e498995258a60a276c6a09a5c_25.png)

![](img/821c6a0e498995258a60a276c6a09a5c_26.png)