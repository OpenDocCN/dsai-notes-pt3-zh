# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P119ï¼šæ·±åº¦å­¦ä¹ æ–°é—» #10ï¼Œ2021 å¹´ 4 æœˆ 3 æ—¥ - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ hi everyoneã€‚ So lots of exciting things happened again this weekã€‚

 So it's tricky to be selective and have to keep the number of topics small and manageableã€‚

 So for this weekï¼Œ since we talked about convolutional neural networks this weekã€‚

 I selected a few topics related to that topicã€‚ So one is a paper on analyzing the bias of different methods and also introducing certain type of mist data set for analyzing biasesã€‚

 And then I will also talk about a new convolutional network architecture that was introduced this weekã€‚

 which might be also interesting to you and also interesting in the context of your class projectsã€‚

 So with that let's get startedã€‚ğŸ˜Šï¼ŒYeah previously we discussed that there can be big issues with bias when we apply deep learning to certain types of problemsã€‚

 for instanceï¼Œ this is something I took from a previous video when we discussed this particular system where people trained a deep neural network to judge job applicants and in particular the big issue here was that the system was rating job applicants substantially differentlyã€‚

If there was a bookshelf in the backgroundï¼Œ andï¼Œ of courseï¼Œ that's something that should not happenã€‚



![](img/c692551d51cc035e9aba15429f1b754d_1.png)

So this weekï¼Œ there was a paper onï¼Œ yeahï¼Œ evaluating different systems for mitigating such biasã€‚

 So there are different techniques that were developedã€‚ And the question is essentiallyã€‚

 how good are they really soã€‚The learning of these inappropriate biases can cause deep learning models essentially perform really badly on minority groups that's something we want to prevent or avoid and these methods that have been developed the question is essentially do they really work and how useful are they if you recall there was also a paper that I presented a few weeks ago where someone essentially said yesã€‚

 these methods work but only like in a very stereotypical sense of fairnessï¼ŸBut in any caseã€‚

 so I think yeah one or there are multiple problemsã€‚

 one is essentially does it even make sense or is it a good idea to even apply this model to a given type of problem the other one is then what methods can we use to yeah let's say reduce the bias and then also how can we assess that really whether these methods really work so that like these three different levels of questions we should askã€‚

Soã€‚Here what the authors did is they tried to improve the evaluation protocol and also yeah provide a new data setã€‚

 they call it the biased MSnes dataia set and in particular they evaluated seven different methods and their code and data sets are available here on Gitthubã€‚

Soã€‚Here's an overview from the paper how they kind of describe the problem of analyzing the biasesã€‚

 So there are different types of data setsã€‚ There was this Q and A data setã€‚

 and there's also the setupã€‚A data setï¼Œ which we may be using later in this class in the context of generative adversarial networksã€‚

 also or variational autosï¼Œ I will also make a few examplesã€‚But yeahï¼Œ essentiallyã€‚

 this Q And A data setï¼Œ it'sï¼Œ it has difficultã€‚Types of biases includedã€‚

And the ease of analysis is not goodï¼Œ thoughã€‚ So itï¼Œ even though this is a maybe good data set forã€‚

Having a challenging task for neural network in terms of biasesã€‚

 it's not so easy to analyze and the S a data set and the colored Mnes dataã€‚

 which is essentially a version of Mness with colorsã€‚ they are both easy to analyzeã€‚

 but the biases to detect are too simple to really I would say or what they say is that simple to really assess the different methods for mitigating biasã€‚

 So they develop this bias Mnes data set which they say is easy to analyze and also provides an adequate difficulty of these biasesã€‚



![](img/c692551d51cc035e9aba15429f1b754d_3.png)

So the task in this biased M data set is as followsã€‚

 it's also about yeah recognizing 10 different digits from0 to 9ã€‚ and these include 9 no sorryã€‚

7 different sources of biasã€‚ So one is the background colorã€‚

 the color of the target digit to be classifiedã€‚ the position of the digitã€‚

So they have nine grid locationsã€‚The distractor shapeã€‚

 So they have other shapes in that image to distract from the predictionã€‚

 the color of these distractorsï¼Œ the type of texture and the texture colourã€‚

 So that's essentially backgroundã€‚Alsoï¼Œ in a wayï¼Œ I meanï¼Œ they have the background colorã€‚

 but then they also have these background texturesã€‚ So here's an exampleã€‚Of how this might look likeã€‚

 So in the top row is the digit oneã€‚ So we can see it hereã€‚ And they sayï¼Œ for instanceã€‚

That in this exampleï¼Œ the one is most often green and is most often placed on a purple backgroundã€‚

 and it often occurs or co occurs with right angled triangles like you can see hereã€‚

 but sometimes they have a different colorï¼Œ for exampleã€‚Making it redã€‚ So thisï¼Œ in this caseã€‚

 when it's greenï¼Œ it would be the majorityã€‚ And if it's another colorã€‚

 it would be the minority class or groupã€‚ So that wayï¼Œ for each digitï¼Œ they haveã€‚

Something that is common againï¼Œ among the majority class and not so common in the minority classã€‚

 for instanceï¼Œ changing the color or changing the background and these types of things So with that they try to assess how how sensitive the network is towards these things that are not the digit itself So how sensitive is the network towards changing the color of the distractor or the shape of the distractor and so forth and how sensitive with the network towards the placementã€‚

 whether it's in the center here or on upper part and things like thatã€‚

 so they are different types of distractions that they try to analyze whether the network is yeah focusing on the digit itself or other things like color and so forthã€‚



![](img/c692551d51cc035e9aba15429f1b754d_5.png)

So here are some results from that paperã€‚ I must say it's a little bit tricky to interpretã€‚

 so I hope I understand this correctlyã€‚ it was not very or entirely clear from the paperã€‚

 but based on my interpretation let me walk you through this So first let's take a look at this MMD plot hereã€‚

 So they define MMD as the majority minority differenceã€‚

Measuring the difference between majority of minority groups so they compute the accuracy of the majority classã€‚

 for exampleï¼Œ if you consider from the previous slide the Amnesty digitã€‚

 the green one that would be the majority class and then the minority class would be when it's redã€‚

 for exampleã€‚å—¯ã€‚Soï¼Œ yeahï¼Œ so they haveã€‚The MMD here for different types of biasesã€‚

 the seven different biases that we talked about in the previous slide and they focus on one bias at a time that they call the explicit biasã€‚

 and that is the one that the model is addresseding and the implicit biases are the ones that are not addressed and here the box spot is over when I insert correctly over the different methodsã€‚

 So we will take a look at it afterwardsï¼Œ but there are seven different methods listed down hereã€‚

 So the box spot is across the different methodsã€‚ Soã€‚For instanceï¼Œ what you can see here is methodsã€‚

Treat background color as an explicit biasã€‚ You can see on averageï¼Œ or the the median here is lowerã€‚

Compared to the implicit biasã€‚ so implicit biases are the ones not treatedã€‚

 and they look at it for the different types of biases asï¼Œ as the explicit biasã€‚

 And you can see in all casesï¼Œ the explicit bias or theã€‚The median here of the explicit biasã€‚

Is lower than the mediumn of the implicit ones so the methods are essentially working to some extent in terms of lowering the bias of a target variable that you specifyã€‚

 for instanceï¼Œ distractor shapeï¼Œ distractor color and so forth for digit position we see there is not much of a difference it could be because convolutional networks are naturally somewhat equivalent to the position and people also often naturally use data augmentation already for random croppingã€‚

 the inputsã€‚But other than thatï¼Œ you can see there's a large difference between explicit and implicit biasã€‚

 so the methods are working to some extentï¼Œ but also at the other hand they are far away from zeroã€‚

 so they are not working very well and you can see for the ones that are implicitã€‚

 the implicit biases they still exist and there's a huge difference between 30 and 40% is like a huge differenceã€‚

Between the majority and minority classesã€‚So looking at these methods in more detailã€‚Hereã€‚

 so what we can see here is seven methodsã€‚ This is SDDMã€‚

 This is the what they call the standard methodã€‚And this is just the baseline modelã€‚

 You can think of it as the the baselineï¼Œ and then they haveã€‚7 other methods they compared toã€‚

 So let's say the up W T hereã€‚ So for this upWTï¼Œ they are kind of was drawing over itã€‚

 but there's this explicit biasã€‚ you can see that so now the box but I also should also say when I understand it correctlyã€‚

 the box plotï¼Œ now the range of the box plot is over these different types of implicit biasesã€‚

 So the range is now over implicit biases and you can seeã€‚Across these different biasesã€‚Mã€‚

In this caseï¼Œ you take a look at one explicit biasã€‚It'sã€‚Close to 0ï¼Œ but the implicit biasesã€‚

 the other ones are still highã€‚ So how I think they analyzed it is they considered all these biasesã€‚

Like a fall loop over the biasesã€‚ And then they treated each one as the explicit bias while treating all the remaining six ones as the implicit onesã€‚

 and then averaging over thatã€‚ So that's howï¼Œ on averageï¼Œ the method performsã€‚When you wrote itã€‚

What explicit and implicit biases areï¼Œ and you can seeã€‚

This method is pretty good at dealing with these explicit biases if you specify itã€‚

 but all the others are still a big issueï¼Œ actually even bigger than in the standard modelã€‚

 So in that wayï¼Œ if you use that methodï¼Œ you may be fixing the explicit bias that you specifyã€‚

 but you make everything else worseã€‚And overallï¼Œ there's no realã€‚

Method that can deal with all the implicit biasesã€‚ You can seeï¼Œ yeahã€‚

 there are big issues everywhereã€‚ Maybe this method hereï¼Œ it's for the explicit and implicit biasesã€‚

 the lowestã€‚ I think reallyï¼Œ the tricky part is the implicit biases because that's something that can be lurking in your data setã€‚

 explicitlicit biasesï¼Œ you can be awareã€‚ even if it existsï¼Œ you can maybe tell peopleï¼Œ okayã€‚

 this is something to look out forã€‚ But implicit biasesï¼Œ think that's maybe an even bigger issueã€‚

 But yeahï¼Œ you can see all these methodsã€‚They are not super yeahï¼Œ robust to these types of issuesã€‚

 On the right hand sideï¼Œ there is a different evaluationã€‚ They call it the IOSMã€‚

 which is the improvement over the standard modelã€‚And this measures the difference in the group's accuracy compared to the standard modelã€‚

 And also here yeahï¼Œ you can see that for most casesï¼Œ these models or methodsã€‚Have issuesã€‚

 But for setup Aï¼Œ there are some methods where there is really low moral difference for these four types of biasesã€‚

 So there are methods that address this very well on simple dataset sets like setup Aã€‚

 But then when they look at thisã€‚Bed Msï¼Œ these issues still occur because you can see L and L and I am for these are these twoã€‚

 So these still have issuesã€‚ So yeahï¼Œ with thatï¼Œ you can see maybe the biased Mnes data set is also something to consider when developing methods because the setup A data set on theã€‚

Right hand side might be too simpleã€‚Alrightï¼Œ but yeahã€‚

 this is I think still an active area of researchã€‚ I'm also no expert in this field I just thought this might be an interesting paper to look at in a new data set for evaluating potential methods for mitigating biasã€‚



![](img/c692551d51cc035e9aba15429f1b754d_7.png)

Alrightï¼Œ let's now talk about a convolutional neural network architectureã€‚

 So in class we only talked about a few of them because we have timing constraintsã€‚ but yeahã€‚

 there are always more interesting ones And one I mentioned was efficient netsã€‚

 So just to recap or not recap but just to introduce them because we haven't even talked about them yetã€‚

 there was yeah the efficient net architecture or family of architectures introduced about two years agoã€‚

 so that's not news we will get to the news in the next slideï¼Œ but essentiallyã€‚

This is an approach for building efficient neural network architecturesã€‚

It's about scaling up convol networks using a compound coefficientã€‚

 So traditionally there are three different ways we can scale convol networksã€‚

 one is yeah by adding more layersï¼Œ one is by making the layers wider and one is by changing the image resolution usually if we increase the image resolution the networks don't perform so well So we have to do also changes to the layers and the number of layers and the width of the layers soã€‚

The idea behind efficiency efficient net is to analyze how the scaling kind of relates to each otherã€‚

 So here they are coming up with a certain formula to do the scaling of these three components and what theyã€‚

Found is thatï¼Œ yeahï¼Œ by their approachï¼Œ they can achieve a better accuracy with fuel parameters and also having networks that work or run faster than reference networks when they apply the scalingã€‚

 So here on the left hand sideã€‚Its a comparisonã€‚ So you haveï¼Œ for exampleã€‚

 renet 152 here is a big networkï¼Œ dense netï¼Œ inception and so forthã€‚ So here they areã€‚

Ass the number of parametersã€‚ So on the rightï¼Œ these are bigger of course andã€‚

On the Y axis is the performance on imagenetï¼Œ soã€‚You can see that this efficient net isã€‚

Smaller in terms of the number of parameters than other reference architectures appearã€‚

 but it's also fast sorryï¼Œ better performingã€‚ So looking at the best performing networkã€‚Might beã€‚

 yeahï¼Œ this oneã€‚ Let's look at the best performing network that is not efficient It might be this oneã€‚

 So you can see thatã€‚This one might be comparable here beforeï¼Œ if I go straightï¼Œ so you can seeã€‚

They develop a much more efficient architecture that reaches the same accuracy as this a mobile net hereã€‚

 So that is actually a really interesting approachï¼Œ that's efficient netã€‚



![](img/c692551d51cc035e9aba15429f1b754d_9.png)

Yeah the news here is though there is an efficient net version to noã€‚

 which was released also on April 1ï¼Œ they were two April 1 papersã€‚ and yeahã€‚

 you have to be always a little bit careful with April 1ã€‚

 like you probably remember from my car network that might be sometimes not a true network it might be just a fun thing April's full joke but yeahã€‚

 in this case it seems to be a legit paper and it's actually a pretty cool oneã€‚

 it's extending efficient net to perform even betterã€‚ So hereã€‚

They introduced new operations such as the fused and B convolutionã€‚

 We won't go into too much detail about thatï¼Œ but it's essentially about progressively increasing the image size during training soã€‚

Usually when you increase the image sizeï¼Œ the performance degradesï¼Œ there's also more overfittingã€‚

 it's probably just due to the fact that there are more pixels nowã€‚

And what they do is during trainingï¼Œ they scale the resolution progressivelyã€‚

 And then while they are doing thatï¼Œ they are adaptively adjusting the regularization using both dropout and data augmentationã€‚

 So here they have nowã€‚A way toã€‚Train with larger images while adaptively reducing the overfittingã€‚

 which I thought is a cool ideaï¼Œ tooã€‚ And you can seeã€‚This is theã€‚Actuallyã€‚

 all of them are efficient net version to architecturesã€‚å—¯ã€‚

And you can see they all perform better than much better than the efficient net version one hereã€‚

 So it's actually a huge improvementã€‚ It'sã€‚83 to 874% points on INe and while still being efficientã€‚



![](img/c692551d51cc035e9aba15429f1b754d_11.png)

Yesï¼Œ since we recently talked about different optimization algorithmsã€‚

 and we also had a quiz an exam question on thatï¼Œ I just wanted to introduce a new oneã€‚

 So there's now Matt Grtï¼Œ which does not stand for Matt Grt studentã€‚ but yeahï¼Œ it'sã€‚

 it's an modificationificationï¼Œ essentially of E Grt and Adamã€‚ Soã€‚ğŸ˜Šã€‚

They say that Adam doesn't quite reach the goal of being a general purpose deep learning optimizerã€‚

 and the mat Grd method is directly designed to address these issuesã€‚ So Adam and SD with momentumã€‚

 they work well across different types of problemsï¼Œ but not all problemsã€‚

 So it's a little hard to see here on the left hand sideï¼Œ This is Adam hereã€‚

And on the right hand sideã€‚Adam is up here in the top performing form regionã€‚And for SGDã€‚Let meã€‚ğŸ˜”ã€‚

Underline this for SGDã€‚Plusï¼Œ momentumã€‚I thinkã€‚It's up here and up hereï¼Œ and you can seeã€‚

Sometimes M is betterï¼Œ sometimes S GD is betterï¼Œ and they have now currently in a new methodã€‚

 met Grdï¼Œ which is always performing wellï¼Œ it's alwaysã€‚Somewhereã€‚Up thereã€‚

 So that is maybe another interesting thing to check outã€‚ So I haven't tried this in practice yetã€‚

 And yeahï¼Œ but if you are interestedï¼Œ it's another interesting thing to tryï¼Œ they haveã€‚

The code on Gitthubï¼Œ I think it's also linked in that paperã€‚

 So I haven't looked into detail into this paperï¼Œ but it might be another interesting yeah method to considerã€‚

So yeahï¼Œ last week I briefly mentioned this new tool for tracking machine learning experiments calledã€‚

Aim aimim is for some reasonï¼Œ not on this list hereã€‚

 but yeah here is an interesting list of different experiment tracking toolsã€‚ Last weekã€‚

 I also mentioned M L flow and tensa boardã€‚ but yeahï¼Œ here are a few more of theseã€‚

 So this graphic was made by the people at Ds hubï¼Œ which is also on hereã€‚

 So you should take this graphicï¼Œ maybe with a grain of saltã€‚

 but it still maybe an interesting tableã€‚Where all you can find differentã€‚Yeahã€‚

 alternatives for tracking machine learning experimentsã€‚

 which may be useful to you if you're working on your class project and you want to I would say take a more organized approach compared to let's say tracking the performances of your models in an Excel spreadsheet or using Matprolip personally I'm not extensively using these methods with a colleague we are using MLlflow right nowã€‚

 I sometimes use Tensor board but often I also don't use either of themã€‚

 I think others are also pretty goodã€‚ it's really just a matter of taste and how big your experiments are and if you are doing a lot of experiments a lot of the time then of course it might be worthwhile investing in some of these it's essentially with everything like it really depends to find the sweet spot between notã€‚

Using something that is kind of overkill for what you're trying to doã€‚

 but also if you are doing something frequently and often and in a large scaleã€‚

Before reinventing the wheel and writing your own tool for trackingã€‚

 maybe look whether there is something out there already that can solve your problemã€‚Alrightã€‚

 so with thatï¼Œ that's it for this weekï¼Œ next weekï¼Œ we will be wrapping up the lecture on transfer learning and then we will talk about recurrent neural networksã€‚



![](img/c692551d51cc035e9aba15429f1b754d_13.png)