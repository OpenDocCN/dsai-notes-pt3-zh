# P121ÔºöL14.4.2- PyTorch ‰∏≠ÁöÑÂÖ®Âç∑ÁßØÁΩëÁªú - ShowMeAI - BV1ub4y127jj

All rightÔºå let me now show you how we can implement this all convolutional network that I discussed in the previous video„ÄÇ



![](img/01aeb81254f41e05749e6dc46c33a23a_1.png)

So here I have a code notebook I will of course share as usual and also as usual I won run this because it took like 45 minutes to run this whole notebook„ÄÇ

 and yeahÔºå there was actually an interesting question on Piazza regarding the runtime of the homework„ÄÇ

So a student was asking whether it's normal that it takes like 8 minutes to run the notebook„ÄÇ Yeah„ÄÇ

 that is totally normal in deep learning„ÄÇ It's actually pretty fast eaten„ÄÇ So yeahÔºå deep learning is„ÄÇ

 of courseÔºå a little bit different than traditional statistics where we usually work with small data sets„ÄÇ

 like little tables of data„ÄÇ So here we have really large data sets and complicated models with many„ÄÇ

 manyÔºå many parameters„ÄÇ So yeahÔºå it takes usually a long time„ÄÇ So some people„ÄÇ

Actually use hundreds or thousands of GPUus for multiple weeks to train these cutting edge„ÄÇ

 vision and language models„ÄÇ So you can actually get good performance also with efficient models that only run a few hours„ÄÇ

 But honestlyÔºå if you have a real world data„ÄÇ Sometimes hereÔºå you have to be patient„ÄÇ

 Sometimes it takes a few days or so„ÄÇ So this model here is relatively simple„ÄÇ

 we have the Cypher 10 data set„ÄÇ So luckilyÔºå it only takes 45 minutes„ÄÇ But yeah„ÄÇ

 that is still a long time„ÄÇ and we probably want don't want to wait 45 minutes here during the video until it finishes training„ÄÇ

 So I'm not going to rerun this„ÄÇ but yeahÔºå I will share the results„ÄÇ

So everything here is exactly the same as in the VG G 16 notebook„ÄÇ

 so I don't have to recap all of that„ÄÇ The only new part is really here„ÄÇ

 this code of the all convolutional network can make this maybe a little bit bigger„ÄÇ



![](img/01aeb81254f41e05749e6dc46c33a23a_3.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_4.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_5.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_6.png)

YeahÔºå I'm seeing„ÄÇ I was writing this very ver both„ÄÇ So you actually„ÄÇ

 if you have the same height and widthÔºå just a small common here„ÄÇ

 if you have the same height and width for kernel size in stride„ÄÇ

 you can also replace this by just the three„ÄÇ and this just by by oneÔºå for example„ÄÇ So here„ÄÇ

 the first value always stands for the height„ÄÇ And this is for the width„ÄÇ

 And this is also for the height and for the width„ÄÇSo yeah„ÄÇ

 because it's all about simplifying convolution networks here you will only see convolutional layers and re layers and the beome layer„ÄÇ

 no max pooling and no fully connected layer so„ÄÇYou can furtherÔºå of course„ÄÇ

 simplify it by not using batchchome„ÄÇ I think batchchnom was not even around not even invented or around when this architecture was proposed„ÄÇ

 but I didn't get really good results without batchchome„ÄÇ So I just added it„ÄÇ

And the rest is just like the paper that I showed you this striving for a simplicity paper„ÄÇBut yeah„ÄÇ

 except that I added a patch norm„ÄÇ YeahÔºå and all I can tell you here is that we have convolutions and„ÄÇ

Here we have always like one convolution that increases the number of channels„ÄÇ

 but it keeps the input in the output size the same„ÄÇ

 So this is like the same convolution that we talked about„ÄÇ I used a patting of one to achieve that„ÄÇ

 So if the input is 70 pixelsÔºå the output will also be 70 pixels„ÄÇ

And then so here there' is always a con that increases the channels„ÄÇ

Because each channel can be thought of a feature map from a different kernel or feature extractor„ÄÇ

That is how the network yeah learns to extract different features„ÄÇ

And then there is always a convolution that keeps the same number of channels„ÄÇ

And this one has the stride of2„ÄÇ And this isÔºå these are the equivalents of the max pooling with kernel size of2 by2 and stride of 2„ÄÇ

 So this is here„ÄÇ This is the one for reducing the height and width„ÄÇ



![](img/01aeb81254f41e05749e6dc46c33a23a_8.png)

And then we haveÔºå againÔºå one that increases the channels„ÄÇWith a straight of one„ÄÇ

 So it keeps the input and output height„ÄÇ And then we haveÔºå again„ÄÇ

 one that here with a Str of2 just reduces the height and width by a factor of 2„ÄÇ

And we keep doing that a couple of times„ÄÇ And then in the endÔºå we have 10 classes„ÄÇ

 right in Cypher 10„ÄÇWe have 10 classesÔºå so in my last convolution layer„ÄÇ

 I have now the number of output channels equal to the number of classes„ÄÇAnd„ÄÇIn PytjÔºå there is„ÄÇ

 for some reasonÔºå no global average poolingÔºå global average pooling is essentially just computing the mean over the channels„ÄÇ

 So so for each height and width for each channel you would compute the mean„ÄÇ

 you could technically implement that very in a very simple way„ÄÇ

 So that's probably why they didn't have a global average pooling layer in Pytorch„ÄÇ

 but you can also just implementing averaging layer„ÄÇ

 you can also use this adaptive average pooling 2 D with an input of one It has the exact same effect as this global average pooling„ÄÇ

So adaptive average pullingdding is a layer that is quite versatile„ÄÇ

 So what it can do is it will produce the size that you desire„ÄÇ So if you put a two here„ÄÇ

It will produce  two by two outputs„ÄÇ If you put a one hereÔºå it willÔºå produce the one by one outputs„ÄÇ

 If I go back to my lecture here„ÄÇ

![](img/01aeb81254f41e05749e6dc46c33a23a_10.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_11.png)

So„ÄÇThis global average pullingdding will be the same as this adaptive average pullingddings„ÄÇ

 It will take the whole feature mapÔºå produce the size of one here„ÄÇ So that is what we are doing now„ÄÇ

 So we are reducing this one to a size of one„ÄÇ I actually don't even know„ÄÇ



![](img/01aeb81254f41e05749e6dc46c33a23a_13.png)

I probably should have prepared thisÔºå but I don't know what the size before this is„ÄÇ It's probably„ÄÇ

 I don't knowÔºå maybe 8 by 8 or something like that„ÄÇYou can actually double check that by„ÄÇ

Removing this part„ÄÇAnd thenÔºå just doing a„ÄÇPrint exc size„ÄÇ

 and then you can find it out if you don't want to do the math„ÄÇ

But I'm not doing that now because then it will crash the output hereÔºå so„ÄÇ

But that is how you can find out at home„ÄÇ You canÔºå I can just make a copyÔºå to be honest„ÄÇ

 Let's do that„ÄÇ Why not„ÄÇ

![](img/01aeb81254f41e05749e6dc46c33a23a_15.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_16.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_17.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_18.png)

SoÔºå it's 8 by 8Ôºå like I thought„ÄÇ So it was actually a pretty good guess„ÄÇ Alright„ÄÇ

 so back to the code here„ÄÇ So yeahÔºå so we have now this global average pooling or here called adaptive average pooling instead of the fully connected layer that we had in VG G16„ÄÇ

 And then yeah I'm training the network using very simple setup the same as for VG G16„ÄÇ

 It's training„ÄÇ It's a very simple network„ÄÇ So it doesn't really train that long„ÄÇ

 It doesn't have so many parameters„ÄÇ

![](img/01aeb81254f41e05749e6dc46c33a23a_20.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_21.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_22.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_23.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_24.png)

I meanÔºå not as long as the VDG16„ÄÇ So the VD G16Ôºå I couple check here„ÄÇIt was probably long time„ÄÇüòî„ÄÇ

90 minutes„ÄÇSoÔºå this only took„ÄÇ

![](img/01aeb81254f41e05749e6dc46c33a23a_26.png)

Half the timeÔºå then„ÄÇ41 minutes„ÄÇ It doesn't get the same good performance„ÄÇ It has 80%„ÄÇ

 I think this one has 84Ôºå85%„ÄÇ So it's not as goodÔºå but twice as efficient„ÄÇ AlrightÔºå so yeah„ÄÇ

 this is how our all convolution network works without max pooling and without fully connected layer„ÄÇ

 in the next videoÔºå I will show you how we can alternatively replace fully connected layers by convolutional layers„ÄÇ

 So here we learned how to replace it by average„ÄÇ

![](img/01aeb81254f41e05749e6dc46c33a23a_28.png)

![](img/01aeb81254f41e05749e6dc46c33a23a_29.png)

PulingÔºå but average pool doesn't have any parameters„ÄÇReplacing the fully connected layer„ÄÇ

 but it's not equivalent„ÄÇ It'sÔºå of courseÔºå different because now we replace it by a version without permits„ÄÇ

 in The next videoÔºå I will show you how we couldÔºå technically„ÄÇ

 if we wanted to replace the fully connected layer„ÄÇ



![](img/01aeb81254f41e05749e6dc46c33a23a_31.png)