# P136ÔºöL16.4- PyTorch ‰∏≠ÁöÑÂç∑ÁßØËá™Âä®ÁºñÁ†ÅÂô® - ShowMeAI - BV1ub4y127jj

AlrightÔºå let's now take a look at how we can implement a convolutional alender in Pytorch„ÄÇ

 So I have like always a couple of helper functionsÔºå which I will go over later„ÄÇ

 let me hide this view„ÄÇ So we have more space in the center for the notebook here„ÄÇ

 So we start as usual by importing watermark„ÄÇ So you know which pytorch version I used„ÄÇ

 We are later also going to use metpotlip for visualizing some outputs and also the training loss as usual„ÄÇ

 So the helper functions are some related to plotting„ÄÇ

 the data order is exactly the same that we used before„ÄÇ

 and this is the domestic setting that we also used before„ÄÇ

 I slightly modified the training function„ÄÇ I will show you the modification later„ÄÇ

 but except that yeah I try to keep it very simple„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_1.png)

![](img/f7a8252ead9e202d89946be554f92300_2.png)

![](img/f7a8252ead9e202d89946be554f92300_3.png)

So we are going to yeah train for 20 epochsÔºå batch size of 32Ôºå just some settings here„ÄÇ

Here I'm getting my data set„ÄÇ NowÔºå notice we are not using any validation data points„ÄÇ

We don't compute anything like accuracy here„ÄÇ So here we are only looking at the reconstruction between the input and outputs„ÄÇ

 and for thatÔºå we just use the yeah training set„ÄÇSo„ÄÇJust testing that the data lot work„ÄÇ

 you can notice validation set is empty now„ÄÇYepÔºå so here that is our main model„ÄÇIn factÔºå okay„ÄÇ

 these are some helper functions for my main model„ÄÇ But yeah hereÔºå these are my main„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_5.png)

I mean onceÔºå so„ÄÇI think you can see that„ÄÇ So here this is my autor class„ÄÇ

 I will explain the other functions in a few moments„ÄÇ The other classes I showed you„ÄÇ

 So we are implementing our autor using„ÄÇ2 sequential parts„ÄÇ

 That is because that makes it later easier to reuse the encoder as a feature extractor and to use the decoder as„ÄÇ

Something where we can reconstruct images from a latent representation„ÄÇ

 So I like to keep those two separate„ÄÇ It's easier to read in my opinionÔºå but also in that way„ÄÇ

 we can use them in a certain way„ÄÇAs you will see laterÔºå so„ÄÇ

Here I just implemented conversion layersÔºå a couple of them„ÄÇ So going from one channel to 32„ÄÇ

 So the M has only its gray scale„ÄÇ so we only have one input channelÔºå I go to 32„ÄÇColonnal size3 by 3„ÄÇ

 that's just the usual stuff„ÄÇ Then I go from 32 to 64 and then I have just two conversion layers„ÄÇ

That keep the channels„ÄÇ You could probably increase the channels„ÄÇ This is just reducing the size by2„ÄÇ

 This is not doing anything except having more permit in my network might be redundant„ÄÇ

 You could be able to remove that„ÄÇ Then we are flattening„ÄÇThis so this is„ÄÇ

 I don't know exactly the sizeÔºå but this is something times something times 64„ÄÇWhich isÔºå yeahÔºå then„ÄÇ

ReshapedÔºå flattened into a linear layer„ÄÇ So it will be all combined into one dimension„ÄÇ

 like you are familiar with before from convolution networks„ÄÇ So in this caseÔºå we get„ÄÇ

A 3136 pixel vector as the output from here„ÄÇ And then I am compressing it into a two dimensional space„ÄÇ

 So only two hidden features„ÄÇ It's very small„ÄÇ This is„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_7.png)

Let me go to my slides„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_9.png)

ËØ¥ÁöÑ‰∫ãÂÆû„ÄÇHereÔºå here„ÄÇ So this is the fully connected one„ÄÇ But for some reasonÔºå I„ÄÇ

I should have probably put something in„ÄÇHere„ÄÇSo this in between would be only2„ÄÇ

 two pixels two dimensional„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_11.png)

O„ÄÇPick„ÄÇSo this is my encoder going from the 700„ÄÇ84 pixel Mnis to a two dimensional representation„ÄÇ

 so we are reducing the size here by 392 by a factor of 392Ôºå which is actually very impressive„ÄÇ

 in my opinion„ÄÇAnd then we have the decoder and the decoder„ÄÇGoes backwards„ÄÇ

 So it takes the two dimensional hereÔºå the two dimensional representation and„ÄÇ

Projects it into a 3136 dimensional representation„ÄÇAnd then I'm reshaping this so that it has„ÄÇ

 yeah the dimensionity of the convolution as before„ÄÇ So here the output now I remember„ÄÇ

 so the output here would be 7 times 7 times 64 or 64 times 7 times 7„ÄÇ64 times 7 times 7„ÄÇ

That's where we get this value from„ÄÇOops„ÄÇüòîÔºåAnd now we are going backwards„ÄÇ

 Were going from 3136 to 64 times 7 times 7„ÄÇ Then I have my transpose convolution„ÄÇ

Nikki Reou transferversese conversion„ÄÇ So here I'm essentially with this one„ÄÇ

 I'm undoing this one with this oneÔºå I'm undoing this one and so forth„ÄÇ And then I go from 64 to 32„ÄÇ

Like the opposite of this one„ÄÇ And then I have the opposite of this one„ÄÇHowever„ÄÇ

 due to how things work with padding and everythingÔºå I tried to get 28 by 28„ÄÇ

 try different paddings and thinking about it where to pa and„ÄÇIt didn't work out„ÄÇ

 so what I get is either something like 27 or 29Ôºå I'm not able to get 28 just because of rounding in the padding„ÄÇ

So this is why I have this trim class„ÄÇ So this trim„ÄÇ

Class I implemented here is just removing one pixel„ÄÇ So from 29Ôºå it's trimming it to 28„ÄÇ in that„ÄÇ

 And so I'm going from 29 to 28„ÄÇ So I have the original size as my input„ÄÇ

And then I have a sigoid here to get a pixels in the range 01Ôºå because„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_13.png)

Not showing it hereÔºå but by defaultÔºå if I don't use anythingÔºå the input images„ÄÇ

 let me go maybe to my helper data lot function„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_15.png)

YeahÔºå so by defaultÔºå if I don't specify the train transform in myÔºå this is actually say for 10„ÄÇ

A this„ÄÇ So yeahÔºå if I„ÄÇDon't specify anything for my train transforms„ÄÇ I will just use this one„ÄÇ

 And as you knowÔºå it will normalize the pixels into a 0Ôºå1 range„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_17.png)

And I want to compare my input to my output pixels„ÄÇ

 So I also want my output pixels in a 0 and one range as as you knowÔºå Sigmoid will accomplish that„ÄÇ

 So if you alternatively normalize your inputs for -1 to 1 pixel range„ÄÇ

 you could use a 10 H functioner technically„ÄÇOkayÔºå and then in the forward method„ÄÇ

 I' am just defining my encodeder and decoderÔºå so just putting together what I have here„ÄÇ

And then I'm initializing itÔºå I'm using Adamom here for simplicity„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_19.png)

And that I'm calling my train function„ÄÇ Let's take a look at this train function„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_21.png)

So okayÔºå this is from the previous lecturesÔºå the train classifier„ÄÇNow we have a slight modification„ÄÇ

 I can't„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_23.png)

like this„ÄÇ So the train auto encoder functions is almost identical to this train classifier function except„ÄÇ

 of course we don't compute the cross entropy„ÄÇ so we compute the means square error loss„ÄÇ

 So I have a MSE here if no loss functions specified„ÄÇ

 some people like to train auto encoders with a binary cross entropy„ÄÇ

 but I don't I I don't like this idea because it's not symmetric„ÄÇIn any caseÔºå so„ÄÇ

If you have questions about thatÔºå I'm also happy to discuss the small piazza„ÄÇ

 I have some visualizations to show it to youÔºå but I don't want to make the lecture too long if you're interested I can show it to you but it's not essential„ÄÇ

 so we have now the mean squared error loss here„ÄÇBy default„ÄÇ

 if we don't specify anything and this is between the los and the features right„ÄÇ

 so we don't use any class labels„ÄÇ That's the main difference compared to the classifier Here we are comparing the los which are the reconstructed images with the original images„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_25.png)

AlrightÔºå so this is all that's new„ÄÇ All that boiler plate here is the same as before„ÄÇ

 It's just for except that we don't compute the accuracy„ÄÇ Of course„ÄÇ

 we are just computing plotting the loss„ÄÇ We don't have any accuracy here„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_27.png)

And yeahÔºå this is essentially itÔºå it's pretty simple training function„ÄÇ

 It looks maybe more complicated than it isÔºå but it's just„ÄÇThe class functionÔºå simplified„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_29.png)

AlrightÔºå so„ÄÇNow here I'm training it„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_31.png)

So you can see there's a big jumpÔºå and then it only trains slowly„ÄÇ

 So the first iteration already minimizes it a lot„ÄÇWhich is good„ÄÇIt train like fall„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_33.png)

8ight minutes on a GPU„ÄÇ And then here's a loss„ÄÇ Maybe I can see maybe training it longer would have helped a little bit„ÄÇ

 but yeahÔºå I was lazyÔºå just trained it for eight minutes„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_35.png)

And you can see the results look quite blurry„ÄÇ So you can„ÄÇ So at the top„ÄÇ

 So here I have a function on the in the top row„ÄÇ These are the original images„ÄÇ

 And at the bottom are the reconstructed versions„ÄÇ and you can see„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_37.png)

It's all very blurry„ÄÇSo„ÄÇWhy is thatÔºå Why is the quality so bad compared to what Ive showed you hereÔºü

 So the reason is I'm using only a two dimensional representation here„ÄÇ

 I actually forgot what I used toÔºå I used something higher dimensional„ÄÇ

 So here for the fully connected one I used a 32 dimensional„ÄÇ

 I think I did the same thing for the convolutional one here„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_39.png)

![](img/f7a8252ead9e202d89946be554f92300_40.png)

ÂóØ„ÄÇSo here I just want to see what happens if I use it to„ÄÇTwo dimensional1„ÄÇ It's an extreme reduction„ÄÇ

 So it's kind of impressive that it can reconstruct anything at all from just two pixelsÔºå right„ÄÇ

 But then you can also see it makes mistakes here for the4„ÄÇ

 it also thinks it's a 9 because 4 and 9 are sometimes very similar„ÄÇ And yeahÔºå So it's not perfect„ÄÇ

 you would get muchÔºå muchÔºå much better results„ÄÇ if you wouldÔºå for exampleÔºå change this number here„ÄÇ

 if I go up„ÄÇ if you'd change this numberÔºå let's say to 100 or let's say„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_42.png)

64 and„ÄÇ64Ôºå you would get much better results„ÄÇ I just wanted to show you the extreme case of having two„ÄÇ

 because„ÄÇA two dimensional space we can visualize„ÄÇOkayÔºå so because I have some more visualizations„ÄÇ

 So this is how the reconstruction looks like„ÄÇ Now here I have a visualization of the two dimensional space„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_44.png)

![](img/f7a8252ead9e202d89946be554f92300_45.png)

The embedded space for all the training data points„ÄÇSo you can see it's kind of a mess„ÄÇ

 but what you can see is that similar numbers cluster together„ÄÇ

 So I have added the class table information by color so you can see the oranges here are all the all the ones„ÄÇ

The dark blues are all the zerosÔºå and they all cluster together becauseÔºå yeah„ÄÇ

 they are somewhat similar„ÄÇ And the auto encodeta is able to capture this similarity in this two dimensional space„ÄÇ

 which is kind of interesting„ÄÇ But you can also see„ÄÇThatÔºå for exampleÔºå some are overlapping 8 and 9„ÄÇ

 you can see they are overlapping here„ÄÇ three is also buried somewhere here„ÄÇ so it's it's not great„ÄÇ

 certain things are overlapping a lotÔºå so„ÄÇIn that way„ÄÇ

 if you sample a data point here where things overlapÔºå well„ÄÇ

 it's unclear which one it would reconstructÔºå rightÔºå So in that way„ÄÇ

 it loses the information between these classes„ÄÇ It can also maybe then help explain„ÄÇ

 I think the four might be also burnt hereÔºå it might explain that if we have a four here in reconstruct a 9 because„ÄÇ

They are overlapping here„ÄÇAnd in the next lectureÔºå we will talk about variational auto encodes„ÄÇ

 which fix this problem a little bit better„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_47.png)

So there it will be a little bit better organized in this space„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_49.png)

YeahÔºå and here I'm just using the decoder„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_51.png)

So maybe I can show you the plot latent space function briefly„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_53.png)

So that helps you understand maybe how IÔºå I did that„ÄÇSo that is plot latent space„ÄÇ

 So here I'm technically just using a data load iterating over the data load„ÄÇ

 and here I'm using only the encoder you can see model dot encoder„ÄÇBased on the images„ÄÇ

 So features here are the images„ÄÇFrom my data setÔºå and then I'm producing these embeddings„ÄÇ

 and then I'm here's just some plotting code for plotting these two dimensional embeddings„ÄÇ But here„ÄÇ

 seeÔºå I'm only using the encoder part„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_55.png)

And if I can go back here hereÔºå I have another visualization hereÔºå I'm using the decoder part„ÄÇ

 so what I'm doing here is I am reconstructing an image„ÄÇSo„ÄÇI'm taking one point hereÔºå let's say„ÄÇ2„ÄÇ

5-2„ÄÇ5„ÄÇ So if I go hereÔºå it should be„ÄÇSomewhere here in the center and you can see it reconstructs this9 here from from from this vector so this is my input vector and it will reconstruct the 9 I'm just sampling from here„ÄÇ

 So here it looks like a pretty dense spaceÔºå but you can think of it as that we have now a method for reconstructing or generating new data so I could sample any point here any I can put in some random values and by inputting these random values I will„ÄÇ

Be able to generate data„ÄÇ And if I take something that is not in my dataset like a point here„ÄÇ

 it I actually don't know what will happen„ÄÇ It will create some data„ÄÇ I wish I could show you now„ÄÇ

 but I would have to run this on my laptop„ÄÇ This will probably take more than 20 minutes„ÄÇ

 But yeah if you're interestedÔºå you can just put in some random values a homework exercise or something and see what comes out„ÄÇ

 And you will see the results won't be great„ÄÇ and you maybe get some fantasy numbers that don't exist„ÄÇ

 And in the next lecture we will see a better method for doing that there's a concept called a variational auto encoder So here this is just a basic introduction in the next lecture„ÄÇ

 I will show you a modification of the auto encoder„ÄÇ

 which is better at or is more designed for sampling from a certain distribution to generate new data„ÄÇ

 AllrightÔºå so this is it for this code example in the last video I will go over some other types of auto encoders„ÄÇ



![](img/f7a8252ead9e202d89946be554f92300_57.png)

![](img/f7a8252ead9e202d89946be554f92300_58.png)

![](img/f7a8252ead9e202d89946be554f92300_59.png)

![](img/f7a8252ead9e202d89946be554f92300_60.png)

And then we will end this lecture for today„ÄÇ

![](img/f7a8252ead9e202d89946be554f92300_62.png)