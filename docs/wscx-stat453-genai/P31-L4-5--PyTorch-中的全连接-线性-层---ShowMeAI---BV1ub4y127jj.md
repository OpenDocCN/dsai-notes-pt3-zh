# P31ÔºöL4.5- PyTorch ‰∏≠ÁöÑÂÖ®ËøûÊé•(Á∫øÊÄß)Â±Ç - ShowMeAI - BV1ub4y127jj

YeahÔºå in this last video of this lectureÔºå I want to give you a brief teaser of Ptos convenience functions„ÄÇ

 the linear or fully connected layerÔºå but also like to just wrap up the conventions regarding linear algebra„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_1.png)

So yeahÔºå what do I mean by fully connected or linear layer„ÄÇ So if we have a multi layer„ÄÇNetwork„ÄÇ

 like shown here so„ÄÇThis part here would be a linear transformation if we don't consider the activation function„ÄÇ

 and you can also think of this as a fully connected layer„ÄÇ So this is sometimes called fully„ÄÇ

Connected„ÄÇüòîÔºåLair„ÄÇOr sometimes people also call it a dense layer„ÄÇIn Py„ÄÇ

 it's called a linear layer because it's a linear transformation„ÄÇ

 so you can also think of it as a linear„ÄÇLair„ÄÇüòîÔºåAnd in the context of kas and Tensorflow„ÄÇ

 people call it a„ÄÇDense„ÄÇüòîÔºåLayerÔºå so all these things are equivalent„ÄÇ

 So another dense layer or fully connected layer would be from here„ÄÇ

To here so neural network really isÔºå yeahÔºå a concatenation of multiple of these fully connected layers„ÄÇ

 which are then interspersed with these„ÄÇNon nonlinear„ÄÇActation functionsÔºå for example„ÄÇ

 So you can do this manyÔºå many times„ÄÇBut also multi layer networks are topic for a different time for a different lecture next week„ÄÇ

 just outlying already how that relates to linear algebra here„ÄÇ So how we then„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_3.png)

YeahÔºå implement a fully connected layer„ÄÇ and Pythtch is very simple„ÄÇ

 There's actually a function called torch and end dot linear„ÄÇ So let's start with the data set here„ÄÇ

 So let's assume I'm just creating some random data„ÄÇ

 but let's assume this is some valid training data„ÄÇSo this isÂóØ„ÄÇDesign matrix with dimensionality„ÄÇ

 What do I have here 10 times 5„ÄÇ10 times 5 input matrix„ÄÇ

 And then when we initialize this linear layer„ÄÇWe give it the number of features and the number„ÄÇ

 So that's the input features and the number of output features„ÄÇ So here we have five features„ÄÇ

 like that's our design matrixÔºå the n times M matrix„ÄÇ And let's say we want three output features„ÄÇ

 the output features„ÄÇ if I go back one slide are„ÄÇ

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_5.png)

These same what I highlighted„ÄÇWellÔºå hereÔºå you can think of it as these hereÔºå the net inputs„ÄÇ

 So that's basically the N number of outputs„ÄÇ

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_7.png)

And they are computed via this linear transformation„ÄÇAll rightÔºå so„ÄÇThen each of these linear layers„ÄÇ

 when you initialize it here„ÄÇHas attached as an attributeÔºå a weight matrix and a bias vector„ÄÇ

So the weight matrixÔºå you can see it here is„ÄÇH times M or in this case„ÄÇThree times five dimensional„ÄÇ

 right„ÄÇ So you have three rows and five columns of  three by five dimensional„ÄÇ So you can see„ÄÇ

That this5 here matches with this5„ÄÇSo that's the number of input features„ÄÇ

 And the bias unit is equal to the number of output features„ÄÇ So in this case„ÄÇ

 it's a vector with three valuesÔºå so„ÄÇ

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_9.png)

Because each output has a bias unit attached to it„ÄÇ

 So here we would have five inputs in the next slight example„ÄÇ

 if I would modify it here and three outputs„ÄÇ

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_11.png)

AlrightÔºå soÔºå and notice hereÔºå these are also small random values„ÄÇ

 We will talk about it later why it's useful to have random values here instead of all zeros„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_13.png)

No„ÄÇüòîÔºåHere I'm printing the dimensions for the inputÔºå the weight and the bias„ÄÇ So let's 10 by 5„ÄÇ

3 by 5 and 3„ÄÇNow I'm applying this fully connected layer„ÄÇSoÔºå I'm applying it to this„ÄÇX„ÄÇ

 which is 10 times 5„ÄÇAnd the WÔºå which is  three times 5„ÄÇAnd the output„ÄÇ So there is also plus B„ÄÇ

 So This is a matrix medication plus BÔºå which is three dimensional and the output„ÄÇIs„ÄÇ

What is it 10 times  three dimensional A is a 10 times  three dimensional matrix„ÄÇ

 So how does that workÔºü So what is going on hereÔºå So you can already probably see there must be some transpose here for the W„ÄÇ

 rightÔºå so that the5 goes in front the and the three goes here„ÄÇ

 And then maybe it's compatible with this x„ÄÇ But is it really what's going on„ÄÇ So yeah„ÄÇ

 that's what's going on„ÄÇ So yeahÔºå I have a summary of that„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_15.png)

So I also have to stare for it„ÄÇ or I have to stare at it for a few seconds„ÄÇ

 and let me see what I've written down here„ÄÇSo yeah„ÄÇ

 based on Pytorch we have another convention hereÔºå recall in the last video„ÄÇ

 I mentioned the convention where the WÔºå the transformation matrix or weight matrix is in front of X and Pytorarch is it's after x and I think this makes sense not from a geometry perspective but from a data flow perspective because in this way we have to use fewer transposes and also it's kind of symbolizing the way that data flows through the network so we start with x„ÄÇ

 and then we multiply it by a weight matrix W and then we get a so we have X do„ÄÇ

W resulting in A and stuff like that„ÄÇ So it's more like this linear„ÄÇThenard flow here actually„ÄÇ

 I'm writing always aÔºå but it's the net input„ÄÇ so it depends on whether we apply the activation function or not„ÄÇ

 actually I have it here„ÄÇ No worries then alright so„ÄÇ

If we have an input with one training example like this x hereÔºå this vector„ÄÇ

 then we can use the notation where x is in front„ÄÇ if we transpose WÔºå then the dimensions will match„ÄÇ

 right„ÄÇ So here in this case„ÄÇIt's a1 times m dimensional„ÄÇ

Vectctor where W is an M times h to dimensional vector„ÄÇ So the result„ÄÇW will be just one value„ÄÇ

 The result will be then one times h dimensional vector„ÄÇAnd„ÄÇIf we have n inputs„ÄÇ

 then we can also keep x in front„ÄÇ We can also transpose W„ÄÇ Then in this case„ÄÇ

 what will happen is that„ÄÇWe have an n times M dimensional one here here„ÄÇ

 the same M times H dimensional one„ÄÇ And here we will have an n times H dimensional one„ÄÇ So this way„ÄÇ

 what is nice about this convention is we can keep the same operations hereÔºå whether this is„ÄÇ

Multiple or only one data point„ÄÇ So this is actually quite convenient from a computational perspective„ÄÇ

 So if we have codeÔºå we don't have to change much around„ÄÇ

 And if you don't believe me that this is the way Pyr does it here is the source code if you want to look at it„ÄÇ

 so this is like the common convention in Pytorarch how the linear transformation happens„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_17.png)

So yeahÔºå just to concludeÔºå I meanÔºå there are multiple different ways we can compute this linear transformation„ÄÇ

 What's really important is thinking about itÔºå like always think about how the dot products are computed when writing and implementing matrix modification„ÄÇ

 So becauseÔºå yeahÔºå sometimes it's easy to make mistakes so„ÄÇ

Things may compute because dimensions matchÔºå but it's not computing what you wanted it to compute„ÄÇ

 So it's always important to write down what the dimensions are and what you're computing and what you expect„ÄÇ

 YeahÔºå the output is„ÄÇBecause yeahÔºå also theoretical intuition and convention„ÄÇ

 like having the W in front as a transformation matrix does not always match up with practical convenience„ÄÇ

 like when we write things in code„ÄÇSo„ÄÇHere I've written down some rules that you might find useful when you're reading textbooks and things are not the same as let's say in code so you can easily late between those concepts for example„ÄÇ

 if you have two matrixes A and B multiplying those is the same as B transpose a transpose and then taking the auto transpose here„ÄÇ

 So that's the same thing And also here these are the same So I was just writing down some rules here that help you maybe be navigating switches between code and textbooks I think you probably won't read many textbooks because there are not many deep learning textbooks really yet„ÄÇ

 but maybe in the future sometimestime but also in papers people use all kinds of different conventions„ÄÇ

 So I think this is like a handy thing to keep in mind„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_19.png)

And yeahÔºå also just to summarize traditional versus Pywach conventions„ÄÇSoÔºå yeah„ÄÇ

 there are multiple ways we can compute this linear transformation„ÄÇ Like I mentioned„ÄÇ

 we can have this weight matrix up front„ÄÇSo this would result then„ÄÇIn H times one dimensional vector„ÄÇ

When„ÄÇX is a feature vector M times 1 dimensional„ÄÇ This is the same using the rule as I that I showed you in the previous slide as writing it like this„ÄÇ

 Sam thing„ÄÇSo it gives us a wait for the same input gives us the same output„ÄÇ

Another way we can write that that is how Pythr does that is putting the x in front„ÄÇ

 but yeah we are transposing W here„ÄÇ this gives or this assumes a different input though„ÄÇ

 this assumes the one times m dimensional input and yeah this is actually my preferred representation because this is kind of easy when we go back and forth between one or multiple training examples because now here at the bottom would be the cases where we have n training examples„ÄÇ

 So if we want to use it the traditional way we have to use true transpos which is more work„ÄÇ

 So this is the usual case in deep learning we have usually many inputs and many outputs So we only have to use one transpose it's shorter and this is also the way„ÄÇ

Pyroch implements it„ÄÇSoÊâÄÊêúÂà∞„ÄÇPyarch convention„ÄÇ So just to sum it up for this lecture„ÄÇ SoÔºå yeah„ÄÇ

 as a little und homework experimentÔºå you may„ÄÇ

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_21.png)

YeahÔºå you canÔºå if you have extra timeÔºå revisit the perceptron Ny code„ÄÇ

 and without even running the codeÔºå just thinking about it„ÄÇ

 Can you tell if the perceptron could predict the class tables„ÄÇIf we feed it in arrayÔºå I'm actually„ÄÇ

Underigning it in an unfortunate way„ÄÇ without running the code„ÄÇ

 can you tell if the perceptron could predict the class labels if we feed an array of multiple training examples at once„ÄÇ

 So if we have„ÄÇA design matrix„ÄÇOf„ÄÇDimenssality„ÄÇAnd times M for testing„ÄÇ

Would it be able to run after trainingÔºü So if yesÔºå whyÔºå if not„ÄÇ

 what change to the code have do you have to make So you can think of this„ÄÇ

 And then you can actually run the codeÔºå right with design matrix is input for prediction and see whether your intuition was correct and also feel free to open a discussion on Piazza about that„ÄÇ

So yeahÔºå run and verify your inition„ÄÇ and then how about the train methods„ÄÇ

Can we also have some parallelism through matrix modifications in the train method„ÄÇ

 having multiple training examplesÔºå so does it make sense„ÄÇWithoutÔºå let's say„ÄÇ

 fundamentally changing the perception learning ruleÔºå would that make sense„ÄÇ

 So it's also another thing to think about„ÄÇ

![](img/4b942bcc44a75ebfe6655aa5c8e647f3_23.png)

AllrightÔºå so„ÄÇNext lectureÔºå then we will talk about yeah more like a deep learning topic that is not like fundamental linear algebra„ÄÇ

 We will talk about a better learning algorithm for neural networks„ÄÇ

 So we learned about the perceptioncept rule next last week„ÄÇ

 but this is actually not a very good learning rule and we will yeah develop a better learning rule next lecture„ÄÇ



![](img/4b942bcc44a75ebfe6655aa5c8e647f3_25.png)