# P100ÔºöL13.3- Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂü∫Á°Ä - ShowMeAI - BV1ub4y127jj

AllrightÔºå let's now talk about the basic concepts behind convolutional neural networks„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_1.png)

So I showed you this figure before it's from an article you can find here if you're interested„ÄÇ

 I thought that was actually a very nice summary figure of the different assumptions that neural networks make„ÄÇ

 SoÔºå for instanceÔºå the multilayer perceptron assumes that the features are independent„ÄÇ

 So that's the basic assumption behind the multilayer perceptron architecture„ÄÇ I just see„ÄÇActually„ÄÇ

 this should be fully connectedÔºå usually in a fully connected or multiceptron anyways„ÄÇ

 So another one is that we have a time dependency or sequence„ÄÇ

 And that is something we will talk about when we talk about a recurrent in neural networks„ÄÇ

 So if you think about text„ÄÇ So here we have„ÄÇEssentiallyÔºå a sequence dependency„ÄÇ

 if you want to analyze text from left to right or something like that„ÄÇ

 or you want to do stock market predictions on something where you have a sequential component to the data that would be something for recurrent neural networks for convolutional neural networks„ÄÇ

 we have this locality assumption„ÄÇSo this is what we are going to talk about and specifically this is for two dimensional convolal networks„ÄÇ

 You can technically also use one dimensional convolal networks to analyze text but here we are going to focus on the two dimensional case for simplicity that is image analysis„ÄÇ

 So here imagine this is our input image here and how what the conversion network is doing„ÄÇ

It's assuming essentially that these patches hereÔºå the pixels in a certain neighborhood„ÄÇ

 they are connected to each other or they are related to each other„ÄÇ And it makes sense for instance„ÄÇ

 if I draw a face as let's say my my imageÔºå you know that if you take this region„ÄÇ

 these are all the pixels for the eye that have a relationship to each other„ÄÇ So that way„ÄÇ

 these pixels here in the eye regionÔºå they are not independentÔºå they have some„ÄÇ

 yeah some local dependencyÔºå right„ÄÇSoÔºå and that is what the CNNN is trying to capture„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_3.png)

So here is an illustration of one of the earliest convolutional neural networks„ÄÇ

 at least that's the oneÔºå I think that was the first published version of it„ÄÇ

 So this is yeah by Yan Lekun and coaus back in 1989„ÄÇ

 So it's a long time ago actually but this is still relevant„ÄÇ

 It's still I would say the main architecture for image classificationÔºå I mean„ÄÇ

 not this particular architectureÔºå but the convolutional neural network architecture in particular„ÄÇ

 I think this one is actually called Lynette„ÄÇ5Ôºå there might be other variants of lineard 5„ÄÇ

 And I will yeah also show you how to implement that in Pytht and how to apply to M N„ÄÇ But yeah„ÄÇ

 let's focus on the big picture here firstÔºå so„ÄÇHere„ÄÇThe authors used input images„ÄÇ

 handwritten digitsÔºå similar to Mnes„ÄÇ So heres an example of how they look like„ÄÇ

 And the goal back then wasÔºå yeahÔºå here in that caseÔºå handwritten zip code recognition„ÄÇ

And the inputs were 256 times 256„ÄÇ I think that's 16 by 16 smaller than Nest„ÄÇAnd you can see„ÄÇ

 like I showed you on the previous slideÔºå there are these„ÄÇ

Image patches or these patch regions that are analyzed„ÄÇ

 And you can see what is going on here is that„ÄÇWell what's illustrated is that piglets from this patch region go into„ÄÇ

Here into some will become a pixel in the next layer„ÄÇ So if you have multiple layers like„ÄÇInput„ÄÇ

Hidden layer 1Ôºå hidden layer 2Ôºå hidden layer 3„ÄÇ And then hereÔºå the output„ÄÇ

Then you can see there is always a patch that is analyse and it goes into the next layer„ÄÇ

 and the same thing is going on„ÄÇHereÔºå so that wayÔºå there's some feature extraction going on„ÄÇ And„ÄÇ

 in factÔºå also this is only aesthetic image„ÄÇ What you would do is you would slide this patch here over the image„ÄÇ

 So you would start in the left corner and then move it to the right and so forth„ÄÇ So you„ÄÇ

EssentiallyÔºå move this„ÄÇÂóØ„ÄÇYou move this around like this from left to right and then from top to bottom„ÄÇ

But this will become clear„ÄÇ I will show youÔºå of courseÔºå additionalÔºå yeah„ÄÇ

 illustrations where this will be moreÔºå I would say clear here„ÄÇ

 This is a very old figure and some things are maybe not so easy to see here„ÄÇ Another thing„ÄÇ

 for instanceÔºå to highlight here is„ÄÇThat there are twoÔºå two of these„ÄÇ and actuallyÔºå in fact„ÄÇ

 there are 12„ÄÇ So then they are dotted between„ÄÇ So there would be 12 of these so called yeah feature maps„ÄÇ

 And let me just maybe forward because I have another illustration of the same architecture that is a little bit more clear„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_5.png)

YesÔºå so here is another illustration of that architectureÔºå except that yeah„ÄÇ

 the inputs a little bit larger„ÄÇ They are 32 by 32„ÄÇ

 It's from a slightly newer paper where they apply this to document recognition instead of just zip code recognition„ÄÇ

 So here this also includes letters and not only digits„ÄÇ But hereÔºå againÔºå the same concept applies„ÄÇ

 And I think this is a little bit easier to yeah to interpret„ÄÇ And this isÔºå yeah„ÄÇ

 this is than lineette 5„ÄÇ And I will show you a code implementation in Pytage„ÄÇ

 I yeah just implemented that also in code„ÄÇSo„ÄÇWhat's going on here is essentially that we have these what we call convolutions„ÄÇ

 this operation where you take a patch from the input„ÄÇAnd then project it up to here„ÄÇ

 And how does this computeÔºå I willÔºå of courseÔºå explain it to you for now„ÄÇ

 Just assume somehow these pixels get„ÄÇSummarized into a single pixelÔºå which goes into the next map„ÄÇ

 These are the so called feature maps„ÄÇSo here I have oneÔºå2Ôºå3Ôºå4Ôºå5Ôºå6„ÄÇ6 of these„ÄÇ

Fhiatureips thats this number here„ÄÇAnd„ÄÇThe input is 32 by 32 and if I do this if I start on the left corner„ÄÇ

ThenÔºå compute the first„ÄÇPixel here„ÄÇ and then I move„ÄÇ

This by one pixel to the right and then compute the next pixel here„ÄÇ

And then move this again to the right and compute the next pixel„ÄÇ My output would be 28 by 28„ÄÇ

 because that's just because of the cutoffÔºå because if the kernel here in this case is 3 by 3„ÄÇ

 or I think it might be actually bigger„ÄÇ You cut off something on the left and on the right hand side„ÄÇ

 But this is a detail that we will discuss„ÄÇin a later videoÔºå I think this is actually a 5 by5 corner„ÄÇ

 then it makes sense here„ÄÇ So if we have a 5 by5 cornerÔºå we would on each sideÔºå cut off two pixels„ÄÇ

 So we lose two pixels of the input„ÄÇ So which is why it's 28 by 28 and not 32 by 32„ÄÇ But again„ÄÇ

 we will discuss that in a future video„ÄÇHow exactly things got calculated and cut off„ÄÇ

 So here that just the big picture is that we go from the input to these feature maps„ÄÇ

 and then we go to another„ÄÇFeature maps thing hereÔºå another and another and so forth„ÄÇ

 So we extract these feature maps„ÄÇ And what you can see is„ÄÇFirst of allÔºå this is„ÄÇ

Here a step where we add channels„ÄÇ SoÔºå in factÔºå there are now6„ÄÇChannels hereÔºå1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6„ÄÇ

And we do that by using differentÔºå different of these feature detectors„ÄÇ Let's call that„ÄÇFicon„ÄÇ

Detectctor„ÄÇAnd we have six of these feature detectors in each feature detector we slide„ÄÇ

Over the image„ÄÇ So let's say we have this red feature detector that we slide over the image to produce this feature map„ÄÇ

 And then we do the same here with the green 1„ÄÇThe green one would be„ÄÇAnother feature detector„ÄÇ

 then it produces this feature map„ÄÇ And just to see meÔºå we have six of these„ÄÇ

 And here there is a so called sub samplingling operation„ÄÇ We will see later„ÄÇ

 This is also called max pooling„ÄÇ And hereÔºå what we do is we just reduce the size„ÄÇ So this is like„ÄÇ

So this first is the convolution and the sub sampling„ÄÇ you can think of as„ÄÇLet's sayÔºå size„ÄÇReduction„ÄÇ

And then again we do another convolution„ÄÇ Now we have 16 feature maps„ÄÇ

Then we do another sub samplinglingÔºå another size reduction„ÄÇAnd then„ÄÇWe end up with this year„ÄÇ

 and this yearÔºå they get now reshaped so we can say reshape„ÄÇIn if you will see the code later„ÄÇ

 it will also become way more clear„ÄÇ So that's just the reshape operation to reshape that into„ÄÇ

vectorctor„ÄÇAnd„ÄÇFor instanceÔºå when we have this reshaped  one„ÄÇ

 we can then use a fully connected layer and another fully connected layer to compute the class labels„ÄÇ

 the outputsÔºå so„ÄÇ

![](img/bad6d1f0de5b0df685212facc6c301a2_7.png)

I illustrated this here in this slide„ÄÇ This is essentially the mainÔºå I would say„ÄÇ

 the main idea behind convolutional networks„ÄÇ Besides using these local filters„ÄÇ

 it's that we have automatic feature extractor„ÄÇ This is essentially the deep learning part about it„ÄÇ

 So instead of having a human manually extracting features from input images„ÄÇ

 Theres now this automatic feature extractorÔºå the convolutional layers„ÄÇ

So the convolutional errors are doing automatic feature extraction that is learned by using back propagation„ÄÇ

 And then in the endÔºå part of the convolution network is here this„ÄÇFully connected part„ÄÇ

 Or you can think of it as the multi„ÄÇThere„ÄÇPerceptron„ÄÇPart„ÄÇ

 and this is then essentially just like a classifier„ÄÇSo here when it gets these fully connected„ÄÇ

sorryÔºå the vectorÔºå the reshaped vector„ÄÇ This is essentially then here like a regular classification„ÄÇ

 And this is a feature vector that was automatically extracted using these convolutional layers„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_9.png)

YeahÔºå and each convolutional layer hereÔºå you can think of it as one hidden layer in this network„ÄÇ

 So if we are counting the number of layersÔºå we have five networksÔºå five layers in this architecture„ÄÇ

 So the first convolution here„ÄÇ So it's the first convolutional layer here„ÄÇThe sub sampling there„ÄÇ

 it's essentially just„ÄÇIt's also sometimes called pooling„ÄÇIt doesn't have any weight„ÄÇ Per us„ÄÇSo„ÄÇ

 there is no real„ÄÇLearning going on„ÄÇ So we usually don't count those as layers„ÄÇ

 We only count the layers that have weights„ÄÇSoSo in this caseÔºå first layer„ÄÇSecond layer„ÄÇ

 the other convolution layer„ÄÇ And then we have1Ôºå2„ÄÇTo fully connected layers„ÄÇ So that's3„ÄÇ

For these are essentially hidden layers„ÄÇAnd then we have this output layer„ÄÇ This is another one„ÄÇ

 It's output fifth layer„ÄÇ That's why the network here has five layers„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_11.png)

YeahÔºå so here is an annotation of everything I talked about in a previous slide„ÄÇ So yeah„ÄÇ

 the this first number here represents the number of channels„ÄÇ

 this the second and third number represents the height and width„ÄÇ So the size„ÄÇ So let's say height„ÄÇ

Whi„ÄÇLike I saidÔºå this last part is a multi perceptron part„ÄÇThe sub samplingampling nowadays„ÄÇ

 is usually called pooling„ÄÇAnd yeahÔºå these are the so called„ÄÇFeature detectors nowadays„ÄÇ

 also we call them kernels or filters really depends on kind of who the author is and who let's say wrote the libraries So some machine learning or deep learning libraries use the word kernel„ÄÇ

 some use the term filter and nowadays they are pretty much interchangeable„ÄÇ

I most often happen to use kernelÔºå but I also recently try to use the fill a term filter„ÄÇ

 more often it's a little bit more modern„ÄÇHere's one more„ÄÇA note about the Scussian connections„ÄÇSo„ÄÇ

 this is essentially„ÄÇWhat they did here is they used fully connected layer together with a minquadarrow loss this was usually called a Gaussian connection and nowadays„ÄÇ

 as you know in multilayer perceptrons it's more common to use the fully connected layer with the softm activation and the cross entropy loss„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_13.png)

So here is a summary of the three main concepts behind convolutional networks so based on what I just showed you in the previous slides„ÄÇ

 so there's one aspect that's the sparse connectivity so that means that a single element in the feature map is connected to only a small patch of pixels which is very different from fully connected neural networks so just to illustrate what I mean again so if I have my input image here„ÄÇ

ThenÔºå I have my own„ÄÇüòîÔºåMy feature map here„ÄÇIn the next there„ÄÇ And we use these kernels„ÄÇ

 like I explained toÔºå So tocent essentially focus on a small region and then project this as a„ÄÇ

Let's say activation into the next layer„ÄÇ That is what we do in the convolution network„ÄÇ

 If you recall in the multi layer perceptionceptronÔºå we didn't have this sparse connectivity„ÄÇ

 We had a full connectivity„ÄÇ That means„ÄÇWhen we had an input image„ÄÇ

 we were usually reshaping it into a long vectorÔºå rightÔºüThis one here„ÄÇ

Would become like a long vector„ÄÇ And then how this worked was for all the inputs„ÄÇ

 So the whole thing when became a activation in the next layer„ÄÇ So the whole„ÄÇ

Maybe do this differentlyÔºå so„ÄÇIn the next activation in the next layerÔºå this is our next layer„ÄÇ

 all the„ÄÇAll the inputs are connected so in that sense„ÄÇ

 it would be similar to using a kernel that has the same size as the whole image and then projecting it here right so if we had a full connection so sparse connectivity„ÄÇ

 that's one aspect behind convolution networksÔºå but then there's also the aspect of parameter sharing„ÄÇ

So what that means is that we use the sameÔºå exactly the same filter here or kernel„ÄÇ

Or feature detector that we slide over the image„ÄÇ So once I compute this one here„ÄÇ

 I move it by one position to the right„ÄÇ Let's say like this„ÄÇ

 And then I compute it for the for the next position„ÄÇ But I'm not creating a new filter„ÄÇ

 I'm using the same filter and just move it by one position up to the right„ÄÇ So that way„ÄÇ

 I'm doing a parameter sharing here„ÄÇ I'm reusing that„ÄÇ againÔºå in a multi layerer perceptron„ÄÇ

 we would have„ÄÇA completely different set of weights for each hidden activationÔºå right„ÄÇ

 So for this second oneÔºå I would have a completely new„ÄÇSet of weights„ÄÇ

 which is not the case for the convolutional networkÔºå for the convolutional network„ÄÇ

 we just slide the same weights over by one position and reuse that„ÄÇ

So this is why you can think of it as a very general purpose feature extractor that should work for all regions in that image„ÄÇ

But„ÄÇYeahÔºå what we have is we have multiple of these feature detectors so we can have multiple to produce these different feature maps„ÄÇ

 rightÔºå so there could be three feature maps so I can have three of these filters„ÄÇ

 but for each feature map I'm reusing the same one that I'm sliding here„ÄÇYeah„ÄÇ

 and another aspect is that we have many layers„ÄÇ so when we do the convolution here„ÄÇ

 we construct feature maps and we do this manyÔºå many times and over time this is essentially over many layers„ÄÇ

UsuallyÔºå what we do is we„ÄÇCreate more channels when we go from one to the next layer„ÄÇ

 And this is essentially to extract patterns„ÄÇ And it willÔºå I will show you an example later„ÄÇ

 What effect that Ha has is that it is„ÄÇBasicallyÔºå essentially„ÄÇ

Trying to extract local patterns from the global onesÔºå so„ÄÇYou have a big picture I mean„ÄÇ

 big in terms of the size of the pictures relatively large„ÄÇ But then yeah„ÄÇ

 over or the course over the trainingÔºå it will from this global pictureÔºå extract local images„ÄÇ

 So information about the eye and the nose and things like that„ÄÇ

 So it will instead of just regarding this whole thing as one imageÔºå it will then have one„ÄÇ

Feature map that represents„ÄÇLet's sayÔºå I and one that represents„ÄÇns and so forth„ÄÇ So in that way„ÄÇ

 it's learning based on more local patterns from this global image„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_15.png)

OkayÔºå so this was really the big picture overview a view„ÄÇ

 I can imagine there was a lot of new stuff going on in this video so things might be unclear instead of maybe rewatching this video I recommend you maybe for now to keep going and watch the next video because I will go into a little bit more detail about the convolutional filters and the weight sharing and I think this might clarify some questions you have and if things are still unclear after that then maybe revisit these two videos but right now I would say„ÄÇ

If things in this video were unclearÔºå keep going before you rewatch this one because the next video really might clarify some things„ÄÇ



![](img/bad6d1f0de5b0df685212facc6c301a2_17.png)