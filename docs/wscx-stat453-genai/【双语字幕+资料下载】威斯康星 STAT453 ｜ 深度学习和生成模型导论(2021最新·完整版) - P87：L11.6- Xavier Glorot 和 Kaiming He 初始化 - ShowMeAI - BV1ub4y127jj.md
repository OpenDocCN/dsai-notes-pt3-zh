# P87ÔºöL11.6- Xavier Glorot Âíå Kaiming He ÂàùÂßãÂåñ - ShowMeAI - BV1ub4y127jj

YeahÔºå let's now take a look at two of the most common weight initialization schemes for deep neural networks„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_1.png)

So the first one we start with is called Xavier Goid initialization„ÄÇ

 sometimes people just sayaver initializationÔºå sometimes people say gloid initialization„ÄÇ

 so the fact or the name comes from the name of the first author of the paper that proposed this method„ÄÇ

So usually this type of initialization is used in connection with a 10 h function„ÄÇ

 the hyperbolic tangent activation function„ÄÇ And here recall this is also a sigmoidal activation function similar to the logistic sigmoid„ÄÇ

 except that the output is centered at0 and„ÄÇSo„ÄÇWhere we have in the logistic sigmoid„ÄÇ

 something like thisÔºå I'm really not good at drawing this„ÄÇIt's try again where the output is„ÄÇAt 0„ÄÇ

5 for the logistic sigmoidÔºå then we have for the 10 H„ÄÇSomething more like„ÄÇ

Like this where we have one and minus-1 here and„ÄÇThe if you recall„ÄÇ

 the partial derivative of that function with respect to this input is one„ÄÇ

 So instead of 025 for the logistic sigmoid for the 10 HÔºå the derivative„ÄÇ

At the highest point here in in the center„ÄÇIs one„ÄÇ So in this case„ÄÇ

 we have less of a yeah vanishing gradient in problem compared to the logistic sigmoid„ÄÇ

 but of course„ÄÇIt still has the problem of this saturation near extreme valuesÔºå so„ÄÇ

Here at this point and here at this pointÔºå we have still this very small or0 gradient„ÄÇ

 which is still a problem„ÄÇ And yeah this Xavier initialization can act as a small improvement to prevent these extreme values„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_3.png)

So how does it workÔºå So it'sÔºå yeahÔºå essentially a two step procedure„ÄÇ

 The first step is to initialize the weights from a Gaussian or random uniform distribution„ÄÇ

And then the weights in the second step are scaled proportional to the number of inputs to that given layer„ÄÇ

And therefore for the first hidden thereÔºå the number of inputs would be the number of features in the data„ÄÇ

And then yeahÔºå in the second layerÔºå that would be the number of units in the first hidden layer and so forth„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_5.png)

So here's how it looks likeÔºå so„ÄÇassume that the weight is initialized from a Gaussian distribution with mean 0 into small variance„ÄÇ

And„ÄÇLet's say this is our yeahÔºå our weight matrix from the Scussian distribution„ÄÇ

 Well random could also be a random normal uniform distribution„ÄÇ sorry„ÄÇ

 and then you scale it by a factor of the square root of one over M„ÄÇL-1„ÄÇ So what is M„ÄÇ

 M is the feature number of features and L-1 is the layer index„ÄÇ

 So L-1 means the number of features in the previous layer„ÄÇ So if I have„ÄÇSet up like„ÄÇThisÔºå and then„ÄÇ

Everything isÔºå of courseÔºå connected to each other„ÄÇSo if I initialize the weights here„ÄÇ

I want to initialize these weights here„ÄÇ they are initialized based on the number of features here„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_7.png)

YeahÔºå and if you didn't initialize the bias units to all zeros„ÄÇ

 you can also yeah include those in the scalingÔºå but yeah it's fine to initialize the bias units to all zeros I just saying if you don't„ÄÇ

 then I recommend also including those in that scaling„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_9.png)

YeahÔºå what is the rationale behind applying this scaling factor hereÔºå So yeah„ÄÇ

 that goes back to making an assumption that„ÄÇLet's say when we compute the net input„ÄÇ

 we have a multiplication between the weights and the activations from the previous layer„ÄÇ

 And you can think of them as yeah independentÔºå rightÔºå SoÔºå and then„ÄÇ

If you have an increasing sample size„ÄÇThen the variance of so hereÔºå I mean„ÄÇ

 if you have increasing number of units in the previous layer„ÄÇ

 So that's what I'm thinking here of the sample size then„ÄÇ

The sum increases because you have independent variablesÔºå right„ÄÇ

 And then you' are just adding up the variances„ÄÇ So you can think of it as the variance of the sum of independent variables is the sum of the variances„ÄÇ

 And then yeahÔºå we have„ÄÇA scaling by one over MÔºå where M is the number of„ÄÇThe samplesÔºå one let's say„ÄÇ

Maybe maybe the number of features would be more correct here„ÄÇüòîÔºåAnd„ÄÇHere„ÄÇ

 the square root is to consider the standard deviation„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_11.png)

AlrightÔºå so yeahÔºå here's just a veryÔºå very brief sketch of what I meant„ÄÇ

 So if we have the variance of the net input„ÄÇ So just focusing on one unit here„ÄÇ

 this can be expressed as the sum over„ÄÇThe weights times the activation from the previous layer„ÄÇ

 so that's nothing new„ÄÇAnd yeahÔºå we can„ÄÇThe variance of the sum is the sum of the variancesÔºå right„ÄÇ

 so we can rewrite this and here I'm just extending it into these these are independent variables„ÄÇ

And„ÄÇThen essentiallyÔºå cause we have a sum over these values and they are the same for all the different„ÄÇ

PositionsÔºå rightÔºå so I can actually„ÄÇSay instead of summing over these„ÄÇ

 I can just say it's M times this product in a way„ÄÇ So this is where the M comes fromÔºå right„ÄÇ

 So we are then scaling it back by one over M„ÄÇ

![](img/56116244f36bd69a28d42e7729f21f2b_13.png)

![](img/56116244f36bd69a28d42e7729f21f2b_14.png)

And since yeahÔºå the square root is farther standard deviation here„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_16.png)

AlrightÔºå soÔºå but don't worry about it too much„ÄÇ It's essentially a scaling factor„ÄÇ

 You can think of it more broadly as a scaling factor that accounts for the number of features that goes into a given layer„ÄÇ

 So that's the main message here that we take take into consideration the number of„ÄÇFeatures„ÄÇ

 and this is sometimes also called the number of features from the previous layer„ÄÇ

 It's sometimes also called fan in„ÄÇ And I have no idea why it's called like that„ÄÇ

 It's another term that is commonly used„ÄÇ There's actually a term fan in„ÄÇ and there's also the term„ÄÇ

Fan out„ÄÇ I only know everyone is using this like since 10 years ago or something„ÄÇ

 but I never understood where this name comes from„ÄÇAlrightÔºå so in practice„ÄÇ

 also sometimes if you look at some initialization schemesÔºå sometimes people„ÄÇAlso„ÄÇ

 include here the number of output units„ÄÇ for exampleÔºå they would write it as„ÄÇ

L here L is the output number of features that goes out from that given layer„ÄÇ

 So that's also some timeÔºå something things people sometimes do„ÄÇ But yeahÔºå in practice„ÄÇ

 I think fanine is more common than having both„ÄÇ

![](img/56116244f36bd69a28d42e7729f21f2b_18.png)

YeahÔºå so here's a visualization from theavavier initialization paper„ÄÇ

 So what they are showing here is a network with a 10h activation and without Xavavier initialization just for reference now so here at the top this is a histogram„ÄÇ

 a normalized histogram showing the activation value„ÄÇ

For the different layers in the network and you can see that in the early layers like one and2 the activations are more uniform„ÄÇ

 spread oddÔºå whereas for the later layers they are largely zero„ÄÇ

 it just happens that how the signal propagates through the network that we will see in later layers„ÄÇ

 the activations are more centered at zero and here consequently„ÄÇWhen we do back propagation„ÄÇ

 we have this multivariable chain ruleÔºå rightÔºå So the we we go usually from the right to the left„ÄÇ

 if I have a networkÔºå let's say like this„ÄÇThat's the forward passÔºå and then in the backward pass„ÄÇ

I start with Lair„ÄÇ5 and then multiply things until I reach layer1 when I do the updates„ÄÇ

 So what you can see here is for layer 5sÔºå we have back propagated gradients that look reasonable in a range at say between minus0„ÄÇ

1 and 0„ÄÇ1„ÄÇ but then the further back I go I get this vanishing gradient problem„ÄÇ

 You can see that the early layers almost get yeah0 gradients most of the time„ÄÇ

Which can then be a problem in this senseÔºå the network will mostly only update the later layers„ÄÇ

 but almost ignore the earlier layers„ÄÇ so this can be a kind of yeah not good if you want to train a neural network well„ÄÇ

 so„ÄÇ

![](img/56116244f36bd69a28d42e7729f21f2b_20.png)

Then here's a visualization showing what Ive showed you on the previous slide„ÄÇ

 and here a version with the yeah Xvier initialization„ÄÇSo you can see at the bottom„ÄÇ

 this looks much better if they use xxavaviia initialization„ÄÇ

 you can see that all the gradients are in a reasonable range for all the different layers„ÄÇ

 So in this wayÔºå that's actually pretty niceÔºå it fixes this issue that some layers low and better than others„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_22.png)

AllrightÔºå so„ÄÇThere's another initialization scheme„ÄÇ

 So Xavier initialization scheme was assuming that you use the 10 h activation function„ÄÇ

 There's also something called a Hu initialization„ÄÇ

 So this comes from the fact that the first author on that paper was name or his name is Kaiing he„ÄÇ

And„ÄÇYeahÔºå like I said previouslyÔºå we assume that the activations had a zero mean or mean  zero„ÄÇ

 which is yeah reasonable„ÄÇWhen we use the 10 H activation because it's centered at zero„ÄÇBut for„ÄÇ

 yeahÔºå for ReluÔºå it's different because the activations are not centerd at 0 anymoreÔºå right„ÄÇ

 if we have a relu function„ÄÇI will only have positive valuesÔºå right„ÄÇ

 because everything looks almost like a leaky roll„ÄÇ

Cause everything here on the left side left from 0„ÄÇWill be set to 0Ôºå right„ÄÇ So that way„ÄÇ

 we don't have the activation centered at0 anymore„ÄÇ

 So this paper proposes a method that works better with re units where we don't have the weights centered at 0„ÄÇ

 or sorryÔºå the activation centered at 0„ÄÇ And there's some complicated math in that paper if you're interested„ÄÇ

 you can check that out„ÄÇ But the bottom line is„ÄÇThat we would add just a scaling factor of two here„ÄÇ

 So inÔºå in totalÔºå we would add aÔºå sometimes people call that gain„ÄÇ We would add a gain of„ÄÇÂóØ„ÄÇ

Square root of two„ÄÇ And we can just put it inside here„ÄÇ So essentially„ÄÇ

 it's just adding a scaling factor„ÄÇ And this addresses„ÄÇ yeah„ÄÇ

 the issue with having the activations not center at 0 in the case of relu„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_24.png)

Ohs sorry it„ÄÇ Okay„ÄÇ so yeahÔºå so you don't have toÔºå I would say„ÄÇ

 worry about these types of things too much if you justÔºå yeahÔºå use regular neural networks„ÄÇ

 So there are reasonable defaults in Pytorch„ÄÇ So this video was more like illustrating that„ÄÇ

Different weight internationalization schemes exist„ÄÇBut in practiceÔºå I mean„ÄÇ

 it is a good idea to choose a good initialization schemeÔºå but yeah„ÄÇ

 mostly frameworks handle this automatically pretty well these days„ÄÇ

 so I will actually in the next video show you how this is done in Pythtorch and how we can change the initialization scheme in Pythtorch„ÄÇ



![](img/56116244f36bd69a28d42e7729f21f2b_26.png)