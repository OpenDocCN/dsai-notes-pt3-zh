# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å¨æ–¯åº·æ˜Ÿ STAT453 ï½œ æ·±åº¦å­¦ä¹ å’Œç”Ÿæˆæ¨¡å‹å¯¼è®º(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P87ï¼šL11.6- Xavier Glorot å’Œ Kaiming He åˆå§‹åŒ– - ShowMeAI - BV1ub4y127jj

Yeahï¼Œ let's now take a look at two of the most common weight initialization schemes for deep neural networksã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_1.png)

So the first one we start with is called Xavier Goid initializationã€‚

 sometimes people just sayaver initializationï¼Œ sometimes people say gloid initializationã€‚

 so the fact or the name comes from the name of the first author of the paper that proposed this methodã€‚

So usually this type of initialization is used in connection with a 10 h functionã€‚

 the hyperbolic tangent activation functionã€‚ And here recall this is also a sigmoidal activation function similar to the logistic sigmoidã€‚

 except that the output is centered at0 andã€‚Soã€‚Where we have in the logistic sigmoidã€‚

 something like thisï¼Œ I'm really not good at drawing thisã€‚It's try again where the output isã€‚At 0ã€‚

5 for the logistic sigmoidï¼Œ then we have for the 10 Hã€‚Something more likeã€‚

Like this where we have one and minus-1 here andã€‚The if you recallã€‚

 the partial derivative of that function with respect to this input is oneã€‚

 So instead of 025 for the logistic sigmoid for the 10 Hï¼Œ the derivativeã€‚

At the highest point here in in the centerã€‚Is oneã€‚ So in this caseã€‚

 we have less of a yeah vanishing gradient in problem compared to the logistic sigmoidã€‚

 but of courseã€‚It still has the problem of this saturation near extreme valuesï¼Œ soã€‚

Here at this point and here at this pointï¼Œ we have still this very small or0 gradientã€‚

 which is still a problemã€‚ And yeah this Xavier initialization can act as a small improvement to prevent these extreme valuesã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_3.png)

So how does it workï¼Œ So it'sï¼Œ yeahï¼Œ essentially a two step procedureã€‚

 The first step is to initialize the weights from a Gaussian or random uniform distributionã€‚

And then the weights in the second step are scaled proportional to the number of inputs to that given layerã€‚

And therefore for the first hidden thereï¼Œ the number of inputs would be the number of features in the dataã€‚

And then yeahï¼Œ in the second layerï¼Œ that would be the number of units in the first hidden layer and so forthã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_5.png)

So here's how it looks likeï¼Œ soã€‚assume that the weight is initialized from a Gaussian distribution with mean 0 into small varianceã€‚

Andã€‚Let's say this is our yeahï¼Œ our weight matrix from the Scussian distributionã€‚

 Well random could also be a random normal uniform distributionã€‚ sorryã€‚

 and then you scale it by a factor of the square root of one over Mã€‚L-1ã€‚ So what is Mã€‚

 M is the feature number of features and L-1 is the layer indexã€‚

 So L-1 means the number of features in the previous layerã€‚ So if I haveã€‚Set up likeã€‚Thisï¼Œ and thenã€‚

Everything isï¼Œ of courseï¼Œ connected to each otherã€‚So if I initialize the weights hereã€‚

I want to initialize these weights hereã€‚ they are initialized based on the number of features hereã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_7.png)

Yeahï¼Œ and if you didn't initialize the bias units to all zerosã€‚

 you can also yeah include those in the scalingï¼Œ but yeah it's fine to initialize the bias units to all zeros I just saying if you don'tã€‚

 then I recommend also including those in that scalingã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_9.png)

Yeahï¼Œ what is the rationale behind applying this scaling factor hereï¼Œ So yeahã€‚

 that goes back to making an assumption thatã€‚Let's say when we compute the net inputã€‚

 we have a multiplication between the weights and the activations from the previous layerã€‚

 And you can think of them as yeah independentï¼Œ rightï¼Œ Soï¼Œ and thenã€‚

If you have an increasing sample sizeã€‚Then the variance of so hereï¼Œ I meanã€‚

 if you have increasing number of units in the previous layerã€‚

 So that's what I'm thinking here of the sample size thenã€‚

The sum increases because you have independent variablesï¼Œ rightã€‚

 And then you' are just adding up the variancesã€‚ So you can think of it as the variance of the sum of independent variables is the sum of the variancesã€‚

 And then yeahï¼Œ we haveã€‚A scaling by one over Mï¼Œ where M is the number ofã€‚The samplesï¼Œ one let's sayã€‚

Maybe maybe the number of features would be more correct hereã€‚ğŸ˜”ï¼ŒAndã€‚Hereã€‚

 the square root is to consider the standard deviationã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_11.png)

Alrightï¼Œ so yeahï¼Œ here's just a veryï¼Œ very brief sketch of what I meantã€‚

 So if we have the variance of the net inputã€‚ So just focusing on one unit hereã€‚

 this can be expressed as the sum overã€‚The weights times the activation from the previous layerã€‚

 so that's nothing newã€‚And yeahï¼Œ we canã€‚The variance of the sum is the sum of the variancesï¼Œ rightã€‚

 so we can rewrite this and here I'm just extending it into these these are independent variablesã€‚

Andã€‚Then essentiallyï¼Œ cause we have a sum over these values and they are the same for all the differentã€‚

Positionsï¼Œ rightï¼Œ so I can actuallyã€‚Say instead of summing over theseã€‚

 I can just say it's M times this product in a wayã€‚ So this is where the M comes fromï¼Œ rightã€‚

 So we are then scaling it back by one over Mã€‚

![](img/56116244f36bd69a28d42e7729f21f2b_13.png)

![](img/56116244f36bd69a28d42e7729f21f2b_14.png)

And since yeahï¼Œ the square root is farther standard deviation hereã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_16.png)

Alrightï¼Œ soï¼Œ but don't worry about it too muchã€‚ It's essentially a scaling factorã€‚

 You can think of it more broadly as a scaling factor that accounts for the number of features that goes into a given layerã€‚

 So that's the main message here that we take take into consideration the number ofã€‚Featuresã€‚

 and this is sometimes also called the number of features from the previous layerã€‚

 It's sometimes also called fan inã€‚ And I have no idea why it's called like thatã€‚

 It's another term that is commonly usedã€‚ There's actually a term fan inã€‚ and there's also the termã€‚

Fan outã€‚ I only know everyone is using this like since 10 years ago or somethingã€‚

 but I never understood where this name comes fromã€‚Alrightï¼Œ so in practiceã€‚

 also sometimes if you look at some initialization schemesï¼Œ sometimes peopleã€‚Alsoã€‚

 include here the number of output unitsã€‚ for exampleï¼Œ they would write it asã€‚

L here L is the output number of features that goes out from that given layerã€‚

 So that's also some timeï¼Œ something things people sometimes doã€‚ But yeahï¼Œ in practiceã€‚

 I think fanine is more common than having bothã€‚

![](img/56116244f36bd69a28d42e7729f21f2b_18.png)

Yeahï¼Œ so here's a visualization from theavavier initialization paperã€‚

 So what they are showing here is a network with a 10h activation and without Xavavier initialization just for reference now so here at the top this is a histogramã€‚

 a normalized histogram showing the activation valueã€‚

For the different layers in the network and you can see that in the early layers like one and2 the activations are more uniformã€‚

 spread oddï¼Œ whereas for the later layers they are largely zeroã€‚

 it just happens that how the signal propagates through the network that we will see in later layersã€‚

 the activations are more centered at zero and here consequentlyã€‚When we do back propagationã€‚

 we have this multivariable chain ruleï¼Œ rightï¼Œ So the we we go usually from the right to the leftã€‚

 if I have a networkï¼Œ let's say like thisã€‚That's the forward passï¼Œ and then in the backward passã€‚

I start with Lairã€‚5 and then multiply things until I reach layer1 when I do the updatesã€‚

 So what you can see here is for layer 5sï¼Œ we have back propagated gradients that look reasonable in a range at say between minus0ã€‚

1 and 0ã€‚1ã€‚ but then the further back I go I get this vanishing gradient problemã€‚

 You can see that the early layers almost get yeah0 gradients most of the timeã€‚

Which can then be a problem in this senseï¼Œ the network will mostly only update the later layersã€‚

 but almost ignore the earlier layersã€‚ so this can be a kind of yeah not good if you want to train a neural network wellã€‚

 soã€‚

![](img/56116244f36bd69a28d42e7729f21f2b_20.png)

Then here's a visualization showing what Ive showed you on the previous slideã€‚

 and here a version with the yeah Xvier initializationã€‚So you can see at the bottomã€‚

 this looks much better if they use xxavaviia initializationã€‚

 you can see that all the gradients are in a reasonable range for all the different layersã€‚

 So in this wayï¼Œ that's actually pretty niceï¼Œ it fixes this issue that some layers low and better than othersã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_22.png)

Allrightï¼Œ soã€‚There's another initialization schemeã€‚

 So Xavier initialization scheme was assuming that you use the 10 h activation functionã€‚

 There's also something called a Hu initializationã€‚

 So this comes from the fact that the first author on that paper was name or his name is Kaiing heã€‚

Andã€‚Yeahï¼Œ like I said previouslyï¼Œ we assume that the activations had a zero mean or mean  zeroã€‚

 which is yeah reasonableã€‚When we use the 10 H activation because it's centered at zeroã€‚But forã€‚

 yeahï¼Œ for Reluï¼Œ it's different because the activations are not centerd at 0 anymoreï¼Œ rightã€‚

 if we have a relu functionã€‚I will only have positive valuesï¼Œ rightã€‚

 because everything looks almost like a leaky rollã€‚

Cause everything here on the left side left from 0ã€‚Will be set to 0ï¼Œ rightã€‚ So that wayã€‚

 we don't have the activation centered at0 anymoreã€‚

 So this paper proposes a method that works better with re units where we don't have the weights centered at 0ã€‚

 or sorryï¼Œ the activation centered at 0ã€‚ And there's some complicated math in that paper if you're interestedã€‚

 you can check that outã€‚ But the bottom line isã€‚That we would add just a scaling factor of two hereã€‚

 So inï¼Œ in totalï¼Œ we would add aï¼Œ sometimes people call that gainã€‚ We would add a gain ofã€‚å—¯ã€‚

Square root of twoã€‚ And we can just put it inside hereã€‚ So essentiallyã€‚

 it's just adding a scaling factorã€‚ And this addressesã€‚ yeahã€‚

 the issue with having the activations not center at 0 in the case of reluã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_24.png)

Ohs sorry itã€‚ Okayã€‚ so yeahï¼Œ so you don't have toï¼Œ I would sayã€‚

 worry about these types of things too much if you justï¼Œ yeahï¼Œ use regular neural networksã€‚

 So there are reasonable defaults in Pytorchã€‚ So this video was more like illustrating thatã€‚

Different weight internationalization schemes existã€‚But in practiceï¼Œ I meanã€‚

 it is a good idea to choose a good initialization schemeï¼Œ but yeahã€‚

 mostly frameworks handle this automatically pretty well these daysã€‚

 so I will actually in the next video show you how this is done in Pythtorch and how we can change the initialization scheme in Pythtorchã€‚



![](img/56116244f36bd69a28d42e7729f21f2b_26.png)