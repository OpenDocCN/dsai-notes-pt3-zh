# P37ï¼šL37-Why Taylor Series actually work - The Taylor Inequality - ShowMeAI - BV1tq4y1U7Cv

If I have a power series or a Taylor series in specificã€‚

 the power series is only as good as its remainders are small that if you want to approximate something with the first end terms of a seriesã€‚

 wellï¼Œ how bad is the remainder Now well in the past we've seen the alternating series remainder and the integral test remainder in this videoã€‚

 we're going to focus on the Taylor series and talk about an inequality that bounds the remainder for a Taylor seriesã€‚

ðŸ˜¡ï¼ŒAnd indeedï¼Œ the fact that we're going to be able to take the remainder for our Taylor series and control it in a really nice way is really what makes the Taylor series so powerfulã€‚

 Otherwiseï¼Œ it would just be some random expressionã€‚ But why was it usefulã€‚ Wellã€‚

 it's going to be useful because remainder is going to be small in an appropriate senseã€‚ Soã€‚

 for instanceï¼Œ this is the infinite sumï¼Œ But I can sort of divide the infinite sum into two different sub portionsã€‚

 There's an initial finite portionã€‚ So the sum from I equal to1 up to nï¼Œ some fixed number Nã€‚ðŸ˜Šã€‚

And then the infinite tailï¼Œ which is the N all the way to infinityã€‚ So two different regionsã€‚

 And the first of themï¼Œ I'm going to call my Tayloror polynomial now polynomialã€‚

 because that's finiteã€‚ And the second is my tailor's remainderã€‚

 And then what the goal of this video is to take that remainder and argue that in an appropriate senseã€‚

 this can be considered smallï¼Œ at least small when your X values are near aã€‚

 the center of your seriesã€‚ðŸ˜Šï¼ŒSo let's try to graph a few things and get a sense of what's going on hereã€‚

 The first point that I want to illustrate is that the remainder really is a function of xã€‚

 So here I've given a generic F of xã€‚ And I'm going to put its first tater approximation on thisã€‚

 This is T1ã€‚ And it's just a linear approximationï¼Œ as we could have done back in cculus 1ã€‚

Then the remainder is going to beï¼Œ wellï¼Œ the difference between these two things iss this little strip here where I take the F of x and I subtract off the first Tayloror polynomialã€‚

 and that height that difference is my remainderã€‚Nowï¼Œ if I vary my Xã€‚

 where I'm asking my remainders atï¼Œ likeï¼Œ if I move this alongã€‚Then by remainder gets biggerã€‚

 it's a function of xã€‚ And as we can sort of see hereã€‚

 it's a function of x where close to the centerï¼Œ the value of x equal to a in this caseã€‚

 x equal to 0ã€‚ The remainder is pretty smallã€‚ But the further that you go awayã€‚

 the remainder appears to be getting bigger and bigger and bigger And geã€‚

 this is generally the case that as you go away from the centerã€‚

 your remainder has the possibility of getting bigger and bigger and biggerã€‚

 So whatever my formula for the remainder isï¼Œ it better depend on the difference between x and aã€‚

 it better depend on the magnitude of x minus aã€‚But what else does my remainder depend onã€‚

 I've put up a new F of x and a new linear approximationï¼Œ a new T1 and a new remainderã€‚ Nowã€‚

 we already know that as I change the x valuesï¼Œ my remainder is going to changeã€‚

 But what if I changed the curvature of the function itselfã€‚As in my F of is a concave up functionã€‚

 but a fairly shallow one right nowã€‚ Watch what happens if I make my F ofex a sort of more aggressively concave 1ã€‚

 is in I take this and I transform itã€‚ Nowï¼Œ watch the remainder as this movesã€‚

 The remainder gets bigger and bigger and biggerï¼Œ when my function gets sort of more and more curvy and and shallow in a senseã€‚

 Nowï¼Œ Concavity is what the second derivative tells us aboutã€‚ðŸ˜Šï¼ŒThat isã€‚

 I've done my first approximation hereã€‚ my linear approximation happens to be a horizontal line in this exampleã€‚

 but the second derivative of the concavity of the function is going to make very large differences in terms of how bad the remainder isã€‚

Now I had to program this so I know that my actual function is one plus kx squaredã€‚

 when I animated itï¼Œ I sent my K to be a bunch of different valuesã€‚

The first taner polynomial is just always oneã€‚ The derivative of one plus x squared is just going be 0 at x equal to 0ã€‚

 So it's the function value plus 0 So just oneã€‚And then the remainder is the difference between the function and the first Taylorlor polynomial isã€‚

 and it's this K x squaredã€‚ Okayï¼Œ so in some sort of loose senseã€‚

 we're thinking that the remainder depends on xã€‚ And if I'm doing a linear approximationã€‚

 it depends on the concavity or in other wordsï¼Œ the second derivativeã€‚All rightã€‚

 so let's actually see the full formulaã€‚ This is Taylor's inequalityã€‚

 Imagine that that n plus one derivative is boundedã€‚ It's all less than some fixed number Mã€‚

 or at least it's all less than that M in some regionï¼Œ and x minus a less than Dã€‚

 some number that's as close as you might need it to beã€‚ So in this narrow regionã€‚

 the second derivative is constrained to be some particular value or in generalã€‚

 the n plus one derivative is constrained to be less than Mã€‚ Then if you have thatã€‚ What do you getã€‚

 Wellï¼Œ the remainder is less than that M that you computed that bound divided by M plus one factorial times the x minus a to the power of M plus1ã€‚

 And indeedï¼Œ that's a remainderã€‚ So it depends on this distanceï¼Œ the x minus aã€‚

 It depends on that bound on the n plus one derivativeã€‚

 And it depends on n is because of the n plus one on the bottom here is N gets largerã€‚

 This remainder is getting smaller and smaller and smallerã€‚ So that's the general caseã€‚

 in our specific case here where we are doing the firstã€‚ðŸ˜Šï¼ŒTaylor polynomialã€‚

 that meant that our M was equal to oneã€‚ So we were considering one plus oneã€‚

 which was the second derivativeã€‚ And the second derivative in this example is just going to beã€‚

 wellï¼Œ twice Kã€‚ You do one derivative takes the x squared down to a2 k X and then goes away to 2 Kã€‚

 So M would be 2 k in this particular exampleã€‚ Indeedã€‚

 it generally looks like that bound times in our caseã€‚

 the remainder we compute exactly was x squaredã€‚ But in generalã€‚

 the distance between x minus a to the power of n plus1ï¼Œ if n was oneã€‚

 that would be x minus a squaredã€‚ So this general formula does seem to align with this lower dimensional when we have hereã€‚

 And indeed it's true in generalï¼Œ although I won't prove it more than this in this particular videoã€‚

 So now that we have the statement for Taylor's inequalityï¼Œ what can we actually do with itã€‚ Wellã€‚

 let's return to the study of the Taylor series for E to the Xã€‚ Nowã€‚

 when I first introduced Taylor seriesã€‚ we talked about the Taylor series for E to the Xã€‚

 But there was a little detail you may have missedã€‚ What I said was thatã€‚ðŸ˜Šã€‚

If there is a power series that converges on some intervalã€‚

 then that power series better have the coefficients given by the Taylor seriesã€‚

 In the case of E to the Xï¼Œ the coefficients are one over and factoriaã€‚

So the whole thing was conditional on the fact that if you had a representationã€‚

 then it tells you what the representation wasï¼Œ but does E to X even have a power series to begin withï¼Ÿ

So let's go out and compute the remainder and see whether the remainder is good or whether the remainder is badã€‚

 Nowï¼Œ I'm going to restrict myself to the situation where the x is less than D in terms of magnitudeã€‚

 So I'm considering some region as the Taylor's inequality ask us to doã€‚

 We look at it for some particular regionã€‚ Okayï¼Œ nowã€‚

 if I take n plus one derivativeros of E to the Xã€‚ I just get right back to E to the X and take as many as you wishã€‚

 Always E to the Xã€‚So then if your x is less than Dï¼Œ your E to the x is less than E to the Dã€‚

 In other wordsï¼Œ this e to the x is some smaller number than E to the Dã€‚ That's your Mã€‚

 That is your bound that you can put on the n plus one derivative in this particular regionã€‚Okayã€‚

 put that into my formulaã€‚ So what do I getï¼Œ My n remainder is less than e to the D that boundã€‚

 And then I multiply it by the x minus aã€‚ So just x in this caseã€‚

 absolute value to power impulse plus 1 divided out by m plus 1ã€‚Nowã€‚

 what happens to this remainder is my n gets largeã€‚ if I take more and more termsã€‚ Wellã€‚

 either the D is just sum number doesn't matter in the numeratorã€‚

 you have a polynomial X to the power of n plus  oneã€‚ That would be a bigger polynomialã€‚

 but it's polynomialã€‚ And in the denominatorï¼Œ you have effect factorialã€‚

One of the things we know of our sort of hierarchy of functions is factorials are going to dominate polynomials and certainly dominate a constant like E to the Dã€‚

 So as n gets largeï¼Œ this entire remainder is going to 0 as n goes to infinityã€‚

We should just say my remainder is actually reallyï¼Œ really goodã€‚ That's what you wantã€‚

 That's your best case scenarioã€‚ It saysï¼Œ if I take enough termsã€‚

 My remainder can be as small as you wishï¼Œ no matter how small you need it to be for your specific applicationã€‚

 I just need to take enough values of Nã€‚ And that my remainder is good enough for youã€‚

 This is incredibly powerfulã€‚ It'sï¼Œ it's so powerful that I will say that E to the x is now equal to its Taylor seriesã€‚

 It's equal to the sum of the x to the n over the n factorialã€‚

 The Taylor series that we computed up beforeã€‚ðŸ˜Šï¼ŒThe first video we just computed it and we saidï¼ŒHeyã€‚

 look at this seriesï¼Œ isn't that lovelyï¼Œ but now we know that it's a series where the difference between the series and the functionã€‚

 the remainder goes to zero if you take enough terms and that's compelling enough that we will now say the function E to the X is not just similar toã€‚

 but in fact is actually equal to this particular Taylor seriesã€‚

This inequality is what makes all the magic worseã€‚ Nowï¼Œ I can use E0 D Xã€‚

 And if you do the same thing for sine and cosine and all the other sort of traditional tailor seriesã€‚

 you can get all the functions that you wantã€‚ The remainders are going to be going to0ã€‚

 And then no matter what level of uncertainty you needï¼Œ you could always use a tailor approximationã€‚

 just by taking n to be sufficiently largeã€‚ So that is incredibly powerfulã€‚ðŸ˜Šã€‚

