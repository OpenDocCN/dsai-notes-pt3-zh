# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÊñØÂù¶Á¶èENGR108 ÔΩú Áü©ÈòµËÆ∫‰∏éÂ∫îÁî®Á∫øÊÄß‰ª£Êï∞(2020¬∑ÂÆåÊï¥Áâà) - P16ÔºöL5.2- GramSchmidtÊ≠£‰∫§Âåñ - ShowMeAI - BV17h411W7bk

![](img/3af9e911005a889fef7bc743f4921988_0.png)

Now we're going to look at a very famous algorithm called the Graham Schmidt algorithm„ÄÇ

 the name stands for two mathematiciansÔºå Graham and Schmidt„ÄÇ



![](img/3af9e911005a889fef7bc743f4921988_2.png)

So we'll take a look at it and its it has to do with linear independence and in fact it is an algorithm I mean for right now its main purpose is to detect whether a set of vectors is linearly dependent or linearly independent we're actually going to see later that it is enormously useful in a lot of practical applications but that's not going to come up for a while so right now it is again i'm sorry just kind of weird and abstract I should also say another thing about it you know we describe one algorithm already that's the Ka means algorithm and now we're going to describe Gham Schmidt I guess I'm not one who thinks that„ÄÇ

Students should be forced to implement these things by themselves I mean I don't know I mean I suppose there's a point to it I don't but it will describe it and of course you know when you're doing this in practice you're going to rely on on implementations written by others so so the point of it is not that you're going to go out and code this yourself the point is you should understand what it is and how it works and what it does but when you actually use it you're going to call you know some routine written by someone else„ÄÇ

Okay„ÄÇSo here's what it is„ÄÇFor nowÔºå we'll see that it's a lot more than thisÔºå but for now„ÄÇ

 here's what it is„ÄÇ It is simply an algorithm that checks if a set of vectors is linearly independent„ÄÇ

 That's it„ÄÇ We're gonna to see it has a lot of other uses later„ÄÇ

 In fact you know in the whole class is about two or three algorithms and this is they're basically all related to it's all it's all this one this is gra Sch okay so here it is we're just going to jump right in and like I said right now it's kind of abstract„ÄÇ

 but that it'll you know it'll emerge later in the book that it has a lot of uses Okay so we're given a set of vectors and they're n vectors and we have K of them„ÄÇ

And what it's going to do is it's going to process them the vectors one at a time and in the end it is going to will be we'll be able to use this to determine if those vectors are linearly independent or linearly dependent right to remind you linearly dependent means that there's a linear combination of them that zero with the coefficient not Paul being zero a linear independent means that's not the case Okay„ÄÇ

 so here's what it does„ÄÇ the first step is it takes„ÄÇ

The I vector AI and it subtracts from AI oh it maintainsÔºå I should say it maintains a set of vectors„ÄÇ

1Ôºå Q2Ôºå Q3 up to Qk unless the algorithm terminates early„ÄÇ

 it generates a set of Qs which are orthoormal oh I should mention that the the notation Q is this is just a convention typically a set of Qs are orthoormal vectors right by the way„ÄÇ

 same same for U's and Vs and it is just convention so okay but in this case it's following the convention„ÄÇ

So the vectorsÔºå it's going to generate a set of vectors Q1Ôºå Q2Ôºå which are orthoormal„ÄÇ

 so here's what's going to happen„ÄÇYou're going to take AI and you are going to subtract from it the inner product of Q1„ÄÇ

And AIÔºå that's a number times Q1 and so on„ÄÇ NowÔºå what you'll recognize„ÄÇ

 you may remember this from last timeÔºå in factÔºå is that„ÄÇIf„ÄÇ

If AI is a linear combination of Q1 up to QI minus1Ôºå then in fact„ÄÇ

All of this is equal to AI and Q I is zeroÔºå so if QI turns out to be zero„ÄÇ

 that means that AI is a linear combination of Q1 up to a minus1Ôºå Q subi minus„ÄÇ

Now what we do is againÔºå we check if QY tilde is zeroÔºå if it is„ÄÇ

 we quit and because in that case it'll turn out that these vectors are linearly dependent„ÄÇÂóØ„ÄÇIf not„ÄÇ

 if it's non zeroÔºå we simply divide by the norm and we normalize and we get QI from QI tildeÔºå okayÔºü

So if this algorithm runs all the way to the end and does not quit as in step two here of the algorithm„ÄÇ

 then that's going to tell us that those vectors are linearly independent„ÄÇÂóØ„ÄÇ

If it stops if it stops in algorithm in step JÔºå then it's going to tell us that vector Aj is actually a linear combination of the previous ones a1 up to Aj minus1 and that means that our set of vectors is linearly dependent okay so that's the algorithm now the one that's actually used numerically in practice is mathematically equivalent but it's not quite the same as this I mean it generates exactly the same vectors and things like that but it's slightly different„ÄÇ

OkayÔºå well let's walk through an example„ÄÇHere's two vectorsÔºå which are two„ÄÇ

 two vectors They're a1 and a2„ÄÇ This gray circle is showing you all points of norm1„ÄÇ

 So it's a circle basically of radius 1 Okay and so let's see so that would tell us for example„ÄÇ

 the norm of a1 is about one point whatever3 I'm making that up that's about it Okay so let's run Gm Schchmidt well we grab a1 and the first thing we do is we subtract off from a1's subline combination of the previous cues but we have no previous cues so we subtract nothing and so that tells us that Q1 this is the first step right Q1„ÄÇ

There we go sorry about thatÔºå so Q and Tilde is in fact just nothing but A1„ÄÇOkay„ÄÇ

 so now that's the first step we check if it's0„ÄÇ It is definitely not zero„ÄÇ And so we normalize it„ÄÇ

 And that means we shrink it to get Q1„ÄÇ And you see now it's green„ÄÇ

 And you can see that it's it's arrowÔºå the tip of the arrow you knowÔºå shows is on that unit circle„ÄÇ

 It's what it's called and that means it now has norm1„ÄÇ

 So that that's Q1 Okay next step now it gets interesting right because now we go over to here„ÄÇ

 And what happens is we take a2 and we subtract off from a2„ÄÇ

 a multiple of Q1 to make the result orthogonal to Q1„ÄÇ And so the correct thing to„ÄÇ

To subtract is minus Q1 transpose A2 Q1„ÄÇ And that's this vector here„ÄÇ

 And we end up with this red vector hereÔºå which is Q2 tilde„ÄÇ

 Now notice that Q2 Tilde is orthogonal to Q1 because that's actually what we're doing is we're slowly producing and we're generating a set of orthoormal vectors okay„ÄÇ

‰∫Æ„ÄÇAt this point they're just orthoÔºå they're not normal yet and so what we do here is we check if Q2 tilde is zero it is not zero and therefore we scale it to make it have length1 and we end up with there we go„ÄÇ

 there's Q1 and Q2 and so this is extremely boring„ÄÇ

 there's no reason to have any interest in because there's nothing just to be completely open about it„ÄÇ

There's nothing practical we can do with this YesÔºå so yeahÔºå so I'm just I'm admit„ÄÇ

 that's all I'm admitventing it„ÄÇ OkayÔºå so so the point is if you run like the„ÄÇ

If you run Graham Schmidt on these two vectorsÔºå A1 and A2„ÄÇThat's what comes out Q1 and Q2„ÄÇ

And you could well ask who cares and I would say you don't know the answer is you don't know yet„ÄÇ

 you'll know later that this actually can do all sorts of super cool practical stuff„ÄÇ

 but for the moment this is all just weird and abstract whatever so just trust me all right„ÄÇSo„ÄÇüòä„ÄÇ

We can analyze this a bitÔºå I'm not going to go into it in too much detail because this is the kind of thing that's best done by you kind of you reading through the book carefully and stuff like that„ÄÇ

 but I will say just a few things about how the algorithmÔºå what it does„ÄÇ

So the first thing and I think our example already kind of gave us hint is thees that come out of the gram Schchmidt algorithm are orthoormal okay and the reason we can check suppose it's true for I minus1 and we'll show this by recursion in that orthogonalization step you are actually subtracting exactly the coefficient you need to of the previous cues to make it orthogonal to all of them okay so when you finish Q I Tilde is orthogonal to all the previous ones that's how that works now to see that all we do is we take an inner product with Qj so Qj transpose Q I Tilde is equal to and this is Qy Tilde here and I'm just going to simply work it out and see what we get but over here we get things like Qj Tilde Q1 well that is going be that's going to be zero right so the only one that comes out„ÄÇ

As we end up with QJ transpose sorry the inner product of QJ and AI and then again QJ and AI and dis subtracting me get zero so it's orthogonal and what that says is that the new„ÄÇ

It says Qitility is orthogonal to all the previous ones„ÄÇ

 but if I scale it by a number because to make its norm oneÔºå it's still orthogonal„ÄÇ

 so the cues are orthogonal„ÄÇAnd of courseÔºå the normalization step guarantees that the norm one okay„ÄÇ

 so it says that the QY's that come out of the back end of the Gham Schchmidt algorithm are orthoormal„ÄÇ

OkayÔºå now if it hasn't if the algorithm has not terminated before iteration I„ÄÇ

 then it turns out AI is a linear combination of Q1 through QY and we can even see that because we can actually very explicitly write down what the linear combination is it is„ÄÇ

Literally the norm of QY tillilde times QI plus and then again this inner product these are inner product coefficients„ÄÇ

 it's the one you've already seen before in a north normal basis expansion„ÄÇ

 So what we do is we we get that that's very explicit So AI is literally a linear combination of Q1 up to QI Okay In fact„ÄÇ

 well that's one of the ways we could say it is that each each AI is a linear combination of Q1 up to Q I okay„ÄÇ

And it turns out the reverse is true too each QI is a linear combination of A1 through AI„ÄÇ

 and again you can see that by inductionÔºå I'm not going to go into the details there„ÄÇ

 but's that's the case„ÄÇOkayÔºå we get to early termination if it terminates in step J„ÄÇ

 what that says is that Q J tilde is equal to0Ôºå that means that when you took AI and you AJ and when you subtracted the appropriate linear combination of Q1 up to QJ minus1 from it„ÄÇ

 you got zero and that tells you actually that that equality holds now„ÄÇ

Now now we'll notice that each of these Qs here is a linear combination of„ÄÇThe AsÔºå a1 up to Aj -1„ÄÇ

 So this is a linear combination of linear combinations„ÄÇ

 linear combination of linear combination is againÔºå a linear combination„ÄÇ

 And that says that when it stopsÔºå it says that AJ is a linear combination of a1 up to Aj -1„ÄÇ

s that is one of the definitions of being dependent„ÄÇ

 So what that says is that the Graham Schchmidt algorithmÔºå if it terminates early„ÄÇ

 then it terminates because the A's you started with were linearly dependent„ÄÇ

I want to say a little bit about the complexity of the Gham Schchmidt algorithm about how long does it take to you know basically how many flops does it take to actually carry this out Well let's figure it out in iteration I you have to evaluate I minus1 inner products right and that costs each inner product costs two n minus1 flops and there's I minus1 so that's that okay so that's the total number of flops to compute Qit you have to subtract subtract off various things and that costs you two n I minus1 flops„ÄÇ

Then it costs you three n flops to calculate the norm and then divide QI by that norm to get QI„ÄÇ

 so that's three n flops„ÄÇAnd the total is now we add this plus this plus that and we have to now sum that from I equals1 to K and you get something horrible that looks like that„ÄÇ

 Now you could work out what that is in fact it's exactly this and I'll tell you why I'm going use a formula which you can just remember or look up or go to Wikipedia to find out or something like that„ÄÇ

 and it's this it's the sum from I equals 1 to K of I minus1 is actually k times k 1 over2„ÄÇ

 In other wordsÔºå the following sumÔºå you knowÔºå if you can check it if k equals5 and this says you should add up0 plus 1 plus2 plus 3 plus 4 and you will find out that that is 5 times 4 about about two which is 10 and it's correct I mean„ÄÇ

 maybe I misspoke it but„ÄÇIt's correct so anywayÔºå so you get some horrible thing like this now what you have to remember about flop counts is„ÄÇ

It's extremely crude„ÄÇ So we're going to drop what are what we call non- dominantinant terms right So for example„ÄÇ

 you know when you work this outÔºå the biggest term you're possibly going to get is going to be this4 n times the K squared divided by two that's two nk squared all the other terms have a coefficient of either n or k which is smaller and we're interested in the case where n and K are reasonably large„ÄÇ

 So we just drop them and so this is completely standard in a flop count and you would say that Gm Schchmidt has a complexity of two N K squared flops„ÄÇ

 and the truth is it's not quite right„ÄÇ but the whole flop count model is so approximate that this is just fine because' you we're pretending that multiplies and adds and divisions and things like that„ÄÇ

 even a square root because we have to calculate a norm and we're kind of pretending that these all costs the same which they don't so you get two„ÄÇ

K squared that's interesting right because it tells you that it is„ÄÇ

It is quadratic in the number of vectors that you apply it toÔºå and it's linearÔºå you would say„ÄÇ

 in the dimension n„ÄÇ

![](img/3af9e911005a889fef7bc743f4921988_4.png)

![](img/3af9e911005a889fef7bc743f4921988_5.png)