# 【官方教程】ChatGLM-6B 微调：P-Tuning，LoRA，Full parameter - P1 - ChatGLM - BV1fd4y1Z7Y5

嗯好嘞，那我们就开始今天的内容啊，很高兴能够来给大家做这么一个报告，那今天这个报告的主要内容呢，就是教大家从手把手的去翻译to，我们的chat gm6B这样一个模型，我们主要会包括这么几个部分。

首先我会带大家回忆一下啊，回顾一下这个JIA模型，它的这个结构和运训练的过程，接下来我们会讲翻译器，我们首先会涉及一些FP的基础知识，包括混合精度和这个零冗余的优化器，然后我们会讲两种高效微调方法。

就是PETA和LORA，最后我们也会介绍全量微调，然后全量微调的话就需要算力稍微多一点，我们也会讲到，用word I/O来部署一个可以和用户交互的前端，让大家玩的也比较方便，OK那我们就直入主题。

首先我们来回顾一下这个gm模型，gm这个模型呢它和GPTT一样，是一个语言模型，跟它的预期训练过程是和GPTT有所不同的，我们可以看到从右边这个cash mask来看，它的前缀，也就是说。

我们会把假设我们输出这个语段是X1到X6，那么我们预算的过程呢，我们会随机的max掉一些部分，和BERT不一样的是，我们会mars掉一个较为长的sequence，而和这个不一样的地方在于。

我们会采取一个bidirectional的，前面会做一个bidirectional encoding，也就是说从右边这个tention mask，你可以看到呃，这些前缀是可以互相做attention的。

然后我们会将整个的这个mask掉的span，替换成一个mask token，不管它原来有多长，不管它原来有多长，然后我们会有一个by the way，有一个to two dimensional。

The positional equecoding，也就是说我们会用第一个position表示这个token，在原训练的位置，然后还有第二个position表示token。

在他这个被骂死掉了sequence中的一个相对位置，然后我们在训练的过程中，我们就要求模型去predict，每一个这个被骂死掉的语段是什么，而我们会把这个语段做一个random shaffle。

就是随机的随机打乱他们的顺序，比如说我们正常的顺序是X1X2X三，XX5X六，那么我们会把X3给max掉，把X5X6给max掉，mass掉之后换成一个master token，然后在生成的时候呢。

我们用position to呃，我们用position one，就是这个五来提示它要生成的是position啊，在原序列中position five这个位置，然后我们用这个三来提示它。

调成原序列中第三个位置的这个token，从而让模型去学习生成，而在生成的第一个sequence之后，第二个sequence，就可以attend到他之前自己生成的这个sequence。

这就是它的PRETRAINING的过程，而在我们实际翻译的过程当中呢，我们会先给一个pad，然后在pad后面加一个must token，然后加一个sop token，这个大家在代码里面可以看到。

也就是说我们把相当于是我们认为它在mask，最后一个地方变成一个这样一个逮捕权的位置，然后后面有一个sob gm series，有很多一系列的这个模型，首先就是这个最基础的JI模型。

其次是今年EPLER2023，我们有有一个这个gm，130B的这么一个大模型，当然这个大模型它一翻译，就无论是做高效微调还是材料微调，它都需要呃非常大的算力，就是说并不是每个人的电脑上都可以翻译的了。

因此我们也推出了这个gm6B的模型，就是让每一个同学，他自己电脑上有一个消费级的显卡，就可以去做这个高效微调，如果你有几张这个好一点显卡，那就可以做全量微调，那这就是关于GLM模型的一个简单的回顾。

接下来我们就是只来介绍一下，我们可以怎样去真正的去跑起来它，那第一步呢我们就下载一个chat point，然后我们可以验证一下关下载对了没有，接下来就是我们的翻译清明过程呃，首先来说一下今天的这个演示。

他用了什么环境，我们的演示的GPU主要是这个RTX3090啊，这是一个消费级CPU，但是呢还有24G显存，其实也不一定需要这么多，如果用我们介绍的这个PETINT方法，其实七个GB就已经够了啊。

当然是在4B的量化的情况下，如果你不做量化的话，得十几个GB嗯，如果你做全量微调的话，我们经过尝试，在八张3090上好像是跑不起来呃，但是四张A100就可以跑起来了，大概是每张卡需要50多个GB。

通过做全量微调，我们也会做演示，但是这个不是重点，然后我们演示的这个这个融资镜像是NVIDIA，Pad torch，这个这样这个镜像，如果如果大家在有一个LINUX机器的话。

可以起一个tourist container，这样的话会非常方便的帮助你配置环境，就省去了很多配置的过程，还有一个要提示大家的是，刚才我说到A180G版本，如果用四张，好像A100的话。

你需要用80G的，如果用八张的话，应该40G也可以，嗯好的继续，然后就是大家一定要注意，如果你用这个镜像的话，要要要给他换个圆，不然的话他会去那个NNGC上面去下载，非常的慢。

而且他的这个keep confp的位置，跟普通的还不太一样，所以建议大家可以直接用这三条命令来了，快速放原一下，OK接下来就是下载chat point，下载check point的话。

我们可以看一下hanging facebook，也就是他这个如果你去，呃GITHUB上你点开这个链接，你会发现有这么一个可以再点跳转到HBASE里面，这里有一个模型介绍。

然后你可以看到有很多的这个space，都在用chat gm6B，然后呢你可以看到这里有很多文件，它用IOS来管理，就是说如果最简单的方法，其实就是刚才我说的呃。

首先你第一步就是呃你要去下载get iOS，因为我们用get iOS来进行管理，下载网上就可以点一下哦，我是不是没有把课件发给大家，这样我我我把课件发给大家了，这样的话大家可以同步的操作，好的。

我发一个群文档，好好的，现在大家在聊天区里应该可以看到，我发送了一个文件，这个文件就是这个课件，大家就因为我很多，发现我很多东西就直接做链接，这样大家点起来比较方便。

大家可以点一下这get started，然后A哪去。

![](img/516b4d6a63046c575afd05bf52048c07_1.png)

这个started里面呢做了一个比较详细的介绍，就是介绍了哪些嗯，就是你如果各种操作系统都可以去按这个，get fp呃，但是我们也是主要是在LINUX里，我也没有试过其他多的系统。

但如果你要从HUGFP上下载的话。

![](img/516b4d6a63046c575afd05bf52048c07_3.png)

你会发现它特别慢，就是你你这get on之后呢，它会卡在这单卡在这，其实卡在最后一行是一个预期行为，因为他在LFS运行的过程当中，并不会有显示内容，所以说如果你打算从哈利内存上下载。

你可以用b w n int来去monitor一下，这个intro traffic，我们来简单演示一下。



![](img/516b4d6a63046c575afd05bf52048c07_5.png)

如果你想要他给face话，它大概下载是一个什么样的状态，比如说啊我们现在的demo，我们先进行一个文件夹demo，OK现在他是公，然后我们去不用这个东西，那我们还是先建一个TM。

诶有同学说没有看到发送的课件，我是在这个聊天里发了一份呀，那我在文档里再发一分呃，文档，嗯好啊，大家看到了，有的好的，那那你再检查一下前面的这个聊天啊，那我那我发了就OK了，呃好的呃。

我们首先去假设我们去克隆这个东西，然后你会发现他似乎卡在最后一行，然后你就可以看一下他现在下载啊，接下载还是挺快的，你看到它大概是18个G，就是它在它它它在下载，只是它没有回血，OK来了，往这边点上。

大家看的更清楚啊，就是你可以看到它在下载，但他没有回血，这是hugin face啊，今天hugon face连连连接比较好，你可以看到它是20多，20多个。



![](img/516b4d6a63046c575afd05bf52048c07_7.png)

20多兆，大概如果hugin base的连接不太好的话，这个就这个方法就可以比较麻烦。

![](img/516b4d6a63046c575afd05bf52048c07_9.png)

然后另外一种办法，另外一种办法是什么呢，就是嗯手动下载，手动下载这个也是在，嗯然后再传一下好，我再发一份手动下载这个东西呢，它会有有有有有这么一个方法，就是说去用这种方式来套路，我们演示一下啊。



![](img/516b4d6a63046c575afd05bf52048c07_11.png)

你可以啊，我们先把他杀了啊，我们，那这种方法你看他告诉你GLFP去IP model，所以他一下就做完了，然后我们进去看一下啊，这些文件你发现它都是假的。



![](img/516b4d6a63046c575afd05bf52048c07_13.png)

只有135是吧，就非常非常的小，显然是不对的，这个时候我们需要手动去下载这个文件。

![](img/516b4d6a63046c575afd05bf52048c07_15.png)

那怎么做呢，呃如果你看到这个官方的REPO里面，它会提供一个清华云的链接。

![](img/516b4d6a63046c575afd05bf52048c07_17.png)

就是这个链接，但是你有一种办法就是手动一个下载，然后再把它一个个传到服务器上，如果你用服务器或者是用什么东西，但这回非常的慢，因为他至少走了一遍电脑，其次很麻烦，你可能会下载丢之类的。

所以我们推荐大家直接用一个现成的工具。

![](img/516b4d6a63046c575afd05bf52048c07_19.png)

就是用这个东西，就是叫清华飘down的人，这个工具，这个工具呢可以帮助你们，把这个清华学的这个东西呢快速下载下载，我们还是拿这个举个例子。



![](img/516b4d6a63046c575afd05bf52048c07_21.png)

我们把这个仓库弄了1LORA的时候，尽量尽量只用SSH，因为HTTP呢很可能不稳定，所以你很难预期你不用多长时间。



![](img/516b4d6a63046c575afd05bf52048c07_23.png)

![](img/516b4d6a63046c575afd05bf52048c07_24.png)

你DK当然可以了。

![](img/516b4d6a63046c575afd05bf52048c07_26.png)

但是有好几文件比较麻烦，或者说这种话，这种玩法的话就可以直接把这些东西都搞定，然后呢你就跑一下这个东西，它就会自动帮你把这个链接里面的东西都搞定。



![](img/516b4d6a63046c575afd05bf52048c07_28.png)

啊然后它就会自动帮你一个都下载下来啊，就是这样一个过程。

![](img/516b4d6a63046c575afd05bf52048c07_30.png)

具体就下载我就不演示，就是。

![](img/516b4d6a63046c575afd05bf52048c07_32.png)

然后呢就是假设你刚才已经完成这些步骤呃，现当然因为这下载大概需要个十几分钟，所以肯定不能在课上演示呃。



![](img/516b4d6a63046c575afd05bf52048c07_34.png)

就是大家可以先下载下来，然后你需要克隆官方的源代码，这个克隆官方源代码呢嗯你需要克隆，下载之后，你需要手动安装一下torch呃，不要不要自动安装，因为自动安装的话。

他很可能跟你的这个扩大版本和你的系统啊，主要是你不打版本，它一般会默认下载一个CPU的东西，所以不太方便，你可以点一下这个pad previous versions。

然后previous versions里面呢，你你你就可以在里面去找找你的这个适合，懂得为什么这么慢，我起个proxy，啊关于LORA的问题，我们一会会我们一会详细的讲，现在我一会一定会想到这个问题。

呃先打不开了，现在先不管这个，一会儿再看，就是说总之你要下载一个适合你的torch知识，然后去install requirements，我们来举个例子，就是把这仓扩容下来了。



![](img/516b4d6a63046c575afd05bf52048c07_36.png)

![](img/516b4d6a63046c575afd05bf52048c07_37.png)

啊那个下载方法呢当然也可以啊。

![](img/516b4d6a63046c575afd05bf52048c07_39.png)

我只是我只是演示一种可行的方法，各种方法都是可以的。

![](img/516b4d6a63046c575afd05bf52048c07_41.png)

都是可以的，然后我们将它下载下来之后呃，官方参数还有点大小。

![](img/516b4d6a63046c575afd05bf52048c07_43.png)

下载完之后你就可以直接去跑一下来了，我们来演示一下。

![](img/516b4d6a63046c575afd05bf52048c07_45.png)

啊下载下载比较慢，呃为什么，嗯好吧，那我们用一个之前下载好的这么一个例子。

![](img/516b4d6a63046c575afd05bf52048c07_47.png)

好现在这是一个vs后的视频，下载好了之后呢。

![](img/516b4d6a63046c575afd05bf52048c07_49.png)

直接去笔记这个web demo，web demo里面你只需要改两行代码，第一potter，第二是model，如果你不改的情况下，它原来是这样啊，这个东西它会从华为P站重新下载一遍，所以会比较比较麻烦。

所以说你可以把它改成你自己的这个，模型的相对路径，然后你就可以直接跑一下啊。

![](img/516b4d6a63046c575afd05bf52048c07_51.png)

不过刚才我我现在说的是这个command line interface demo，我们cl demo cl demo也是一样。



![](img/516b4d6a63046c575afd05bf52048c07_53.png)

就是把这两个地方改改完之后，OK那我们先看一下显卡，OK行领卡通过了，那就不用指定这个divide ordin，就直接运行就好，然后他就呃怎么会是这样呢。



![](img/516b4d6a63046c575afd05bf52048c07_55.png)

给我看，哦我忘改这个东西。

![](img/516b4d6a63046c575afd05bf52048c07_57.png)

我如果忘改了，它就会重新下载。

![](img/516b4d6a63046c575afd05bf52048c07_59.png)

![](img/516b4d6a63046c575afd05bf52048c07_60.png)

那他如果你指定对话，他会直接开始下载chat房子，啊然后就可以说你好，然后孩子开始推理，比如说学习不好，它是一个类似于打字机这样的一个效果，OK我们可以看一下它它的显存占用的情况。

嗯就是这么一个全量推理，我应该没开量化好看呃，没开量化对这么一个直接的推理，大概是这么打，小3090完全可以，然后你如果如果有显存不够的话，你可以看下量化试试，这就是这个CLI的效果。

但是CLI它不是很酷，所以我们可以试一下web demo，同样可你需要先specify，你的这个活动估计。



![](img/516b4d6a63046c575afd05bf52048c07_62.png)

Web demo，它其实就是呢怪的LO，这么一个非常适合给深度学习的东西，搭前端这么一个前后端应该说的一个框架。



![](img/516b4d6a63046c575afd05bf52048c07_64.png)

然后你会发现一件事情，就是如果你用vs code来写这个东西，它会自动检测，然后给你开端口转发，这个端口转发了之后呢，他就会为自动把你本地的一个请求，转发到这样服务器上的这个7861端口上。



![](img/516b4d6a63046c575afd05bf52048c07_66.png)

这样的话我这个后端其实是跑在服务器上，然后你就可以，这样直接推理这个东西，就是你如果你打算拿它打一个什么前端的话，你一种比较简单策略就是直接在web deo上面改。



![](img/516b4d6a63046c575afd05bf52048c07_68.png)

改完之后呢，加一个反弹就OK了。

![](img/516b4d6a63046c575afd05bf52048c07_70.png)

那现在就是已经用GRADIO和CLI这两部分，大家应该已经知道怎么做了，接下来就是可以去翻译Q，那翻译Q我们首先会介绍两个前置知识，一个是mixed position。

一个zero mix position是什么意思呢，我们这个一个计算机的基础知识就是浮点数，相信大家都非常理解HQE的浮点数标准，FP32312位浮点数，也就是一个单精度浮点数，那它有八个整数。

八个这个指数位和32的尾数位，它可以用来表示这个范围那个数字，但是呢大家如果学过浮点数的乘法，它是怎么计算的，你会发现它有很多步骤，在现在处理器里面，它也需要多个周期，或或和挺多个周期才能把它算出来。

所以说比较麻烦，或者说速度比较慢，而且你会发现它很大，它一个一个一个浮点数据需要四个百，还比较费心，费存储空间，那么我们会发现一个事，就是机器学习里面很多东西，它并不需要那么高的精度。

所以说我们就有另外两种半精度浮点数，一个叫FP16，一个叫braint float16，FP16呢，就是FPE的这个16位的浮点数标准，还有五个这个指数，为何把十个尾数为它的范围呢，你会发现非常的小。

就是说他最大只能表示到那个6万的水平，然后最小也只能是还有这个五一负八的那个数，所以说我们有些人就觉得你啊，或者说或者说我们我们发现了这个，很多时候在machine learning里面。

它并不需要这么高的精度，相相比来说，它更需要的是一个比较大的范围，比如说底下的精度没有那么高，没关系，但是我表示范围一定要大，所以我们就有了这个bert fold16，也就是说它很大。

可以翻译38的3E，33×10的38次方这么大呃，小的话也可以非常小，这样的话它既不容易发生下翼，也不容易发生上移，然训练的过程中呢就不容易崩，那么我们论文还有什么好处呢，首先直观来说它的显存省了一半。

对话就是由32变成16减空间省，其次是你可以看到这个NVIDIA，他告诉你，如果你用这个半精度，它这个速度就是它的一个翻译倍，所以就就是非常的快，你训练起来呢就会更加方便，尤其大模型参数量最大的时候。

当然如果你用一点80年就更快了，那么这样做有什么问题呢，他会丢进度对吧，那丢进度就是说我们怎么解决这个问题呢，或者说我们所有的精组队有必要吗，我们会发现一件事，就是说嗯在模型的梯度更新的过程当中。

有一些梯度它非常非常的小，比如说它小于这个1×10的27，-27次方，加上小于1×10的-27次方，那这个东西可以说对训练没有什么影响啊，我记得有一篇paper是media他们讨论过这个事情。

但是也有些梯度呢，呃它的这个大小它是介于这个，比如说十的好像是十的-17次方，和十的二次方之间，这样一个梯度的这个大小啊，具体数可能不准确，但是大概是说有一个已经低于了这个FP16，它的表示范围。

但是对模型的训练仍然有影响，有这么一类的这么一个梯度，所以说我们还不能把所有的参数都给变成了，Fp16，但是我们还想要FP16乘显存算得快的特点，那怎么办呢，我们就做这样一个事情。

首先呃大家应该知道这个这个假设，我们用ADAM这个优化器，但它会有momentum，就是一级特征和variance2级特征这两个东西，我们把PREMATTER，momentum和variance。

这三个参数都用AP32来表示，那这样的好处就是说，如果我们在训练中出现了一个，非常非常小的gradient，那么它还是可以在这个FP32的精度上，被体现出来，就是他对这个模型的加减。

会在FP32这个精度上被体现出来，然后呢我们拿到规定的之后，我们把它变成FPC2，在更新上，那你说这个我懂得问题是啥呢，你可能觉得哎，那这个很小的东西，不是在这一步规定已经丢掉了吗。

那这个时候就有一个技术叫DYNDYNAMIC，这个lost sky什么意思呢，就是说如果你的，我们在保证loss不发生上亿的情况下，尽可能让loss变变得大一些，这样的话一个小的数。

就比如说原来loss是比如说1亿，就比如说呃这个1亿负-18，然后呢我们加了一个比如说4096的lost sky，那他这个时候就会变成1亿负十五，比如说这样的话，它就更不容易发生下移。

下移就意思是说它太小了，以至于直接表示成零了，那么我们在ford pass的过程当中，for pass过程当中，我们首先会把这样一个呃，这parameter我们先把它变成half half的。

意思就是说把它变成半进度的，然后呢前向传播和data也做成半径度的，前向传播的过程中得出这个半径的activation，然后在backward的过程中使用这个DIMDYNAMIC。

Lost skating，也就是说把loss变大一个范围，这样的话呢一些原来会发生下跌的东西，由于它变大了就不会下移，这样的话我们就可以取得一个更好的训练精度，得到这么一个gradient之后呢。

它是一个放大了的gradient，我们再把它转化成相应的这个正常的FP32，然后做FP32下载累加，这样一些原来就发生下意识的东西，就不会发生下一，原因是我们自i dynamic loss里面。

把一把它变大变大了之后，我们在转换的时候，它就不会变成呃它但这一步规定就不会变成零，这样加下来就会对模型有一个实际的效果，OK这个就是mixed position这么一个基本思想。

接下来还有一个比较比较常见的，或者说一个非常重要的技术是lora dance，这个工作他说的是什么呢，他是一个可以帮你省选组织工作，我们为什么需要ZIDANCY呢。

我们已经事实上我们已经有了这个data parallel，和model parallel的这么一个技术，这int parallel这个大家相信大家已经有很多了，就是多就是多卡训练。



![](img/516b4d6a63046c575afd05bf52048c07_72.png)

比如说你有啊，比如说你有八张卡，那么原来你在一张卡上，假设它bert size，你能跑到四，那么由你的显存不够了，所以说呢你你没办法在一张卡商用，跟通过gash size，或者说你模型参数也不能减对话。

那你就可以用八张卡，那么你就一张卡好，四个batch size，跑这个ford pass，ford pass之后呢，你可以在每张卡上做一个单独的background。

backward之后你得到gradient，然后你做一个or reduce通信操作，把所有的gradient取一个均值，这样每一个每一个节点就拿到了这个规定均值，然后呢，在每一个节点上。

自己用自己的优化器去更新它的参数啊，这是data parallel的一个基本思想，然后mona parallel的意思是说呃，当然这个MODEPL有很很很多，很多方法也比较简单的策略。

就是说我们比如说瓦库懂得ky head，它这个分块嘛，我们把它分块到不同的显卡的预算，然后呢在这个four pass中，一些地方我们会做这个ORIGDUCE，还有一些地方就做identity。

就是说一些地方做复制，一些地方做这个ORRODUCE，然后在backward pass过程当中呢，我们原来做复制的那部分，我们做ORRODUCE，然后原来做ORRODUCE那部分我们做复制。

就是这个这个的话大家可以去看一下呃，如果我没记错的话，19年我pad micron那pad air他讲的会比较清楚，嗯是的呃，那个周周同学说的这个是是是是没什么问题啊，但是也也不一定分成多个数据集。

其实我们一般一般的策略是说，我们把它，比如说就是比如说现在我们bert pad by13，32，然后一张卡都能跑四个，那我们就把这个32呗拆到四张卡上去跑，CPU能跑呃，会非常非常的慢。

不建议用CPU跑，而且也需要很大的内存，你的内存我不我不太确定啊，嗯然后就是MODEPARALLEL，但是这个MODEPARALLE的问题是在于，它的这个这个通信太慢。

然后data parallel呢就是它的通信虽然很小，因为只需要通信的记录，但是呢它有非常非常大的冗，就非常非常大的冗余，但冗余是什么呢，就是model state，我们知道广告看海这张图。

你有一个parameter，嗯好的，一会儿我可以给大家找一下，然后发发发的那个发发发到聊天里，嗯好的，就是你假设你有一个parameter，一个parameter。

你就需要从它的momentum variant，所以model state其实要比PARMETER用的多，那在你的显存方面用的用料用料更多，所以说呢嗯你如果用DP的话。

你的这个optimizer在每张卡上都会存一遍，那这显然是很不很不高效的对吧，然后mp的话，它确实把它彻底partition了，但是呢它的通信是非常的tense，所以说没那么方便。

那么这是一张比较直观的图，让大家看一下家庭表示1bert base，那么你的这个呃这个memory就去哪，你发现那个model它这并不是这个。

并不是占了主导的company method占的比报告多的多，然后activation这样的动力啊，当然大家应该也是能activation check point int这种方法。

但是今天这不是我的重点，所以zero这个paper或者是zero的工作，他提出，就是说我们可以把这些重复的部分给它拆开，那我们用图来说一下这个事情，这个图片上的指纹他说的是什么呢。

假设杯赛杯赛就是刚才说的deep data erro来了，data parallel的每一个GPU拥有一套完整的参数，然后有完整的gradient，一米多高。

然后呢你还有这个完整的optimized safe，这图画的也很形象，也就是，这图画的形象，也就是说这个这个off matters state，它占了一个很主导的地位，那如果你用这个stage one。

也就是把这个alt matter state给拆开了，这样的话呢，就把alt meter state加上你N个GPTU给它拆成N份，然后呢你如果用stage two。

就是把这个这个optimizer和gradient都拆开了，这样的GRADI对话也拆开，如果你用这个c three，就是把这个premier给拆开，那可以看到右边那个显存，它是降的非常非常的厉害。

OK那这这有这样的东西呢，我们就可以去做我们实际上的微调了，然后我们就介绍一种高效微调做法，这个pad0b two p p int two呢，它的做法可以，大家可以先回忆一下这个PROMPTUNE。

PROMPTATION做法是什么呢，就是说你的输入就是就是我们把它叫pad，那你的输入可以在它前面或者后面添加一些，啊添加，我们可以在它的前面或者后面，添加一些这个连续的embedding。

你正常的情况下，我们会把一个token过一下这个bedding层，然后变成一个embedding，然后我们添加一些连续的MBEDDING，然后我们在每次训练的时候呢，就只训这些添加了连续领带领。

那这个效果呢其实在很多这个比较大的模型上，其实效果是还可以的，但是呢它它它有很大的局限性，局限性，就是它的效果可能跟直接去翻译，性能差的还比较多，所以我就有这么一种JING确的方法，他的做法是什么呢。

我们除了在前面加这个LEARNABLE的，这个bedding之外，我们在每个层都加入这样的这么一个LEARNABLE，The parameter，也就是说这些parameter呢。

我们不用原来的attention计算，我们直接放在这，而其他的这些地方呢就是蓝色表示，就是原来该怎么算怎么算，我们也不会对它计算梯度，那它的一个好处是它会大大节约你的这个呃。

这个memory和训练的时间，以及它会有一个比较好的performance，那我们来看一下performance，就是p two b two pad paper中的一个performance呃。

可以看到这个peta v two呃，跟跟直接的翻译器来说可以取，可以说是取得了一个非常好的效果，虽然可能还差几个点呃，可能还差几个点，但是在这但像这种的话，他偶尔还能还能开源高一些。

就是说就是说他非常非常的接近，这个效果是非常好，今天我们来做这个事例呢，就是chat gm6B官方仓库中的这么一个例子，就是一个广告性能数据集，我们来翻译的这个例子。

就是说我们的输入是这样一个content，就是说我们描述这个这个衣服是啥东西，比如说上衣版型，然后呢它是这样一点，就是把衣服的属性抽象也生成进去，要求语言模型的输出的，就是把它变成一段人类可读的广告词。

嗯在刚才克隆的这个chat，这个mob的官方仓库之后，你需要下载这么一些dependence，下载完dependency之下载一下这个数据集，接下来我们就直接跑一下。



![](img/516b4d6a63046c575afd05bf52048c07_74.png)

好的时候，这个这个这个代码呢，基本上就是瞬间就可以上手呃，第一个是如果有多张卡的话，记得配一下他在哪张卡上跑，然后这个COUNTIZATIONBERT这个框架。

ZATIONBERT可以配你的这个量化程度，如果你看quantization的话，它性能会稍微慢一点，可以把刚才这个例子杀掉，然后我们比如说直接啊对了，还有一件事进程你需要配置数据集的位置。

然后他就会下载模型做这么一个翻译，这个时间如果开了这个框架的群，pad为四的话，大概需要三个小时，嗯是的呃，这个这个群里这个群里面提问，说他有灾难性遗忘，是的，他肯定是灾难性遗忘，因为我们并没有去。

因为因为这个问题，我们已经要求他，就是说他完全把它condition在这个广告数据集上，所以它原来东西肯定是灾难性遗忘呃，还有这个问题是他做去训练，一会儿就过拟合了，这肯定是这样，因为我的数据集很小。

这个模型的参数量又很大，所以肯定是非常容易过拟合。

![](img/516b4d6a63046c575afd05bf52048c07_76.png)

呃至于如何避免过顶盒这个问题，其实各位各位同学，你们如果参加这个比赛的话，你应该自己思考这个问题，因为你毕业过拟合的方法有很多。



![](img/516b4d6a63046c575afd05bf52048c07_78.png)

你可以做一些TRIBU的数据增广，或者是你你就simply少训练一些epoch，或者是说你自己去整一点额外的数据集之类的，都是一些可以减轻过拟合的方法，呃好然后他宽带这个会很很花时间在这等。

呃然后如果你想要快训练快一点。

![](img/516b4d6a63046c575afd05bf52048c07_80.png)

可以把它直接去掉，如果把这个框架内成倍的四去掉，那么哦说解释一下每个参数的含义，呃好吧，我们一会一会来做这个事情，我记住把这个去掉的话，训练可以快一些，但它显显显存也可以乘相对元高。

但是13GB如果硬盘3090的话，也是无所谓，这是PT5，然后一会呃我看看啊，没事，我们直接直接Emo吧。



![](img/516b4d6a63046c575afd05bf52048c07_82.png)

就用一个我已经选好chat point呃，用他那个默认的位置是这个output杠，我改了一下Q4，那么我们其他会在这个位置，然后如果你想要evaluate的话，这是我学习。

这其实就是直接把这个复制的意思嗯，然后你可以指定一个test file，test file的话也是你需要一个proint color呃，我为了这个老师有点看跳来跳去的，看不清楚，见面呃。

现在可以看清楚吗，OK好那继续问题的话，一会儿一会儿QA的时候启动回答吧，我看现在论文比较多，比如说刚才我们用了，我们换张卡。



![](img/516b4d6a63046c575afd05bf52048c07_84.png)

这样的话好，他现在已经开始去了，呃啊不对，他现在做token int啊。

![](img/516b4d6a63046c575afd05bf52048c07_86.png)

对说到这里，那就直接在这解释一下含义吧，就是说我看我先把它恢复一下，或者删掉东西好，没删掉，朱雀可以跟那个evil Eva对比一下，如果没有，Do predict，Do train。

意思告诉他现在你要训练还是多推理，然后是trint validation bile，这两个名字很明确，Problem column，Response，column的意思是说你的这个训练文件里面。

你这个训练文件里面，它the prompt response分别是由哪些键决定的，Overhead cash。



![](img/516b4d6a63046c575afd05bf52048c07_88.png)

overhead cash的意思是说，你可以看到他现在在token ize，这个整个数据集对话，这个事大概需要一分钟的时间。



![](img/516b4d6a63046c575afd05bf52048c07_90.png)

这个是这个大约一分钟的时间，但是显然是没必要的，因为你在假设你在debug的过程中，你会反复的把模型删掉，然后再重启杀掉再重启，每次都做一个头盔单词是非常耗费时间的，所以如果你alright cash。

那它就会忽略这个cash的存在，如果你想要比如说我就针对这一个数据集，我要反复的这个我要反复的去翻译Q，然后或者说反复的比bug，但是呢我的速度没有变，那你就可以把这个你就跑一次，然后呃或者不用跑一次。

你把它删掉，就可以把它删掉的话呢，这个东西它就会首先去检查你的chat在不在，如果有cash，它就不会重跑，token im就会很大程度上节约的第八个时间，OK就是说你可以把这行删掉。

如果你的个训练数据没有变，训练数据变了之后，一定要把它加回来，把这个chat重新刷掉，不然的话你会发现你一直在先救出去，Over output directory，就是说如果你在100多的时候。

还有个output directory1把，这个开源了之后，应该如我记得应该是呃，他他会把原来的那个OPPO给出来了，max seven name和max toget bmx a max source。

max source then的意思是说，假设现在我们把prompt映射成这，我们这个prompt的长度，在token neint之后不能超过多少个token。

如果超过了这个max source token，它就会直接截断呃，max ta也是一样的，这样的话好处就是说，假设你使用一个纯天然的这么一个数据集，它里面的这个token的长度可能会非常非常长。

甚至有些token他一下，就比如说你爬的时候没弄好，一下一下就很棒了，特别特别的长，啊特别特正常，那这个时候你可能会一下刷显存，或者是说你没法做batch，你不太好跑这个bert organizer。

所以说你必须把它限制成一个固定的长度，这样我们在软int跟net的时候，就可以把它全部都pad pad int一样的，就是说如果这个长度没有到64会做T。

然后per device train bad size就很很明白，就是每个device上面会放多少个呗，会放多少个sample，然后gradient commodation set。

这个是在做这种高调微调或者单卡微调，比较重要，你可以看到pad burbchat是一个扰，噪声非常非常大，如果你直接这么训练的话，可能不能收敛，可能不收敛，或者说可能效果很差。

所以说呢我们就模拟一个大fp cise the模型，Down beside，就是说我们让他做16次的这个ford pass，然后呢把这16次梯度举起来，就好像还有一个bash做的for pass啊。

做的这个一样，这个我我好像没没没还注意这个参数是干嘛，max day就是你训练多少，然后lock lock step就是你可以看到他打log这句话，房子什么时候打。

lo save step就是多少存一下，这两个意思很明确，这个参数是PEACHING，刚才我们看到P图，int里面增加了那个token的数量，O，然后一百一百的话，我我就我就不跑了，直接看一下。

画出来这个效果是什么，它就会把你这个test file里面读出来，然后让模型推理，推理完之后呢，就给你打来了，输出输出到输出到一个TXT里面去，当然这个我应该是改了一小部分的，这个一般代码是吗。

我改了翻译啊，对我改了一下，就是我把他打了一下，他，我记得他原来的代码是只会打label和PREDICINT，label就是说这个ground truth的这个输出的。

ground truth predicint就是模型的输出，然后我加了个print，这样大家就可以更更清楚的看到这个呃。



![](img/516b4d6a63046c575afd05bf52048c07_92.png)

prompt是什么，然后label的意思是说，你这个训练记忆本来要求的是什么，然后这个predict是模型的输出，就是evaluate这些东西的一个简单的功能，接下来是fp parameter啊。

LORA放在后面，为什么呢，因为LORA我们用了一个community仓库，用了另外一个仓库，我们节省切换参数的时间，我们就直接讲一下FPFPAMETER的话，一定要用刚才deep fp。

它就实现了刚才的zero，然后我们来看一下fp pad parameter。

![](img/516b4d6a63046c575afd05bf52048c07_94.png)

OK这是这是也是这个chat gl6B的仓库。

![](img/516b4d6a63046c575afd05bf52048c07_96.png)

它的首先你对specify一个model name or pad，就是你这个模型这个chat房子位置。



![](img/516b4d6a63046c575afd05bf52048c07_98.png)

然后事实上你用四张卡就可以跑起来嗯，可以试一下，我单卡一下，这个是在A板上做一个实验，哦现在空余的卡，比如说现在有的是，比如说3567都是空余的，比如说那我们就把它改成3567啊。



![](img/516b4d6a63046c575afd05bf52048c07_100.png)

对还有一件事情是提醒一下大家，如果用DC的话，不要使用这个could available devices，做diet ordinal，你应该用include这个参数来做。



![](img/516b4d6a63046c575afd05bf52048c07_102.png)

诶然后再再换一个，OK然后，OK这他就开始起deep，然后他让他来做这个不为的devices，嗯然后他开始下载了。



![](img/516b4d6a63046c575afd05bf52048c07_104.png)

这个有一个小问题是我发现呃，如果你直接去跑这个代码的话，可能会遇到一个问题，就是至少在我这遇到一个问题，就是，就是它在加载这个model和token int的时候，可能会有一个一个进程。

把另一个进程的这个东西给抢了，这时候你可以加一个log，让他们不要一起下载，这样的话一个下载就不会这个问题。



![](img/516b4d6a63046c575afd05bf52048c07_106.png)

我试了一下八个3090好像起来了。

![](img/516b4d6a63046c575afd05bf52048c07_108.png)

我刚才是在这该是四张80之间，100强做这个实验，这是可以可以做到的。

![](img/516b4d6a63046c575afd05bf52048c07_110.png)

然后还有一些常见问题，第一个是有时候突然变成突然出来了。

![](img/516b4d6a63046c575afd05bf52048c07_112.png)

1broken pad，这个东西直接冲泡它应该就没有问题，然后按如果他contention。

![](img/516b4d6a63046c575afd05bf52048c07_114.png)

就是说结果他比如说报告这个模型，看缺少了什么什么方法，或者说文件什么什么不存在，结果确定文件存在的话，很可能是嗯，还有可能是你这个他同时下载的问题。



![](img/516b4d6a63046c575afd05bf52048c07_116.png)

Ok，好接下来是LORA，LORA的方法呢就也是非常的简单，他他说的是什么呢，他说的这个这个图的意思是说，你原来位置都不动，然后我加一个低质矩阵进来，然后用这个低质矩阵来模拟这么一个模拟。

这么一个他梯度，更就是你额外额外的这个delta，更新的这么一个额外更新的这么一个效果，那好呃，我简单说一下它的这个等价的原理是什么，虽然实现不一定总是这样，但是它的它的意思是说。

假设你原来有这么一个PRECIATIONW，那么然后对原来这个X来说，你的做法就是你直接把拿这个X去给他投影，就OK了，呃就是假设你就是矩阵乘法WX，然后你翻译的作用效果是什么呢。

其实就是在原来的位置上面，又加了一个新的delta，delta其实就是你翻译Q学习来了个差值，但是呢如果你直接去学，因为它比较随机，所以基本上都是满质，但是LORA他发现这个东西你不买也没有关系。

所以说呢你他就假设这个德德W永远是，比如说AB这样一个形式，它是一个致R的矩阵，那至R的矩阵我们可以把它分解成这么一个A，什么BAA是这个一维度乘以R，然后R乘以第二维度。

然后W等于W0加AB这样一个结果，也就是说原来他更新的这个W呢，那你就用AB来了，其中这个R呢是远远的低于，远远的低于这个这个D和K的大小，这样的话我们可以省很多很多的绩效。

那我们来看看paper中的一个效果，那就是说LORA的这个效果，比如在TPT3上来说，它可以跟这个pad thon是呃可以匹敌的，然后就是在chat电脑简历上怎么能搞多少啊。

这个我们可以直接用一个community，说明他做了这么一个仓库，这仓库叫zero l p，它也是比较好用。



![](img/516b4d6a63046c575afd05bf52048c07_118.png)

它它的原始代码是要在一个叫做CHINESE草包pad。

![](img/516b4d6a63046c575afd05bf52048c07_120.png)

数据集上面去翻译这个chat，661我们来直接看下代码呃。

![](img/516b4d6a63046c575afd05bf52048c07_122.png)

然后另一个事情是，大家如果关注chat这个没有一段时间的话，会发现我们的模型权重和代码更新了好几版，也不是好几笔，记得更新过几次呃，是删掉了这个呃SETK的依赖，然后也删减了词表。

删减词表是为了让小节约显存，然后删掉SETK的依赖，是发现，因为FPTK的这个依赖跟conformers有一些，或者说它的兼容性太好，经常会遇到兼容性问题，所以说我们就调整一下。

然后这个作者呢就觉得我们商用太频繁，所以他就把我们这个早期的版本，留了一个早期来了，所以说它的代码是基于他或者410，早期版本强大的，然后他也在早期版本的模型上做了一些修改，这样可以这样支持LORA。

然后我们就嗯还是在在他这个基础上，我们改改代码，让他也能去调1ADJ这个对线，当然我我觉得这个T键只是一个例子，就是大家无论自己要翻译用什么样的data set，其实你都可以仿照这个流程来做类似的操作。

因为翻译任务无误，无无外乎就是你需要一个prompt，然后对这个prompt要求模型，去识别这个prompt中的模型，然后在希望这个模式之后，把里面的一些信息提取出来，做一个response。

所以说而且你会发现，现在有很多基于chat globe的就是开源代码，所以说呃你你你要去假设，你要去用这个开源代码来获得，基本上是一样的，第一步是简单读一下代码，知道他的行为是什么。

你不需要把每个模块假设你只需要看用一下，你越快，你不需要把这个文化说得很明白，不需要把模块弄的很明白，接下来就是如果你成为的快，如果你要说如果你想要调整实现的话，你还可以验验仔细看一下，但是呃对。

然后就是你你什么需求就改一改，改一改之后你si chat看一下你的输出对不对，输出对不对，然后你看一下这个最终看一下原来的效果，你就你其实知道你这个evaluate，它关键其实你看的是EVATE。

跑出来跟你是不是一样，OK那我们就来看一下，拿LORA去翻译，听chat gm6B，假设还是做这么一个这么一个B站的电脑set。



![](img/516b4d6a63046c575afd05bf52048c07_124.png)

OK我们还是克隆一下这种LP。

![](img/516b4d6a63046c575afd05bf52048c07_126.png)

![](img/516b4d6a63046c575afd05bf52048c07_127.png)

好这个是好，我们先来看一下刚才跑的那个模型，太好了，什么什么程度什么程度的，这个是在四张80GA100上去跑，这个这个全量的翻译，请我们来看一下显示，他大概用了58个G的显存啊，四张卡，然后。

他如果全量它翻译成5000个step的话，只需要41分钟，你可懂得为什么这么快呢，刚才3000个step还需要几个小时，为什么知识需要40多分钟。

因为那3000个step里面是做了规定accumulation。

![](img/516b4d6a63046c575afd05bf52048c07_129.png)

也就是说你ford16次才会step4就会相对慢一些，但这个全量的话，它没它规定，HUMIDATION是一，也就是说每一个forward都会直接有一个backward，OK我们直接把他杀了。

然后我们搞一下在3090上。

![](img/516b4d6a63046c575afd05bf52048c07_131.png)

![](img/516b4d6a63046c575afd05bf52048c07_132.png)

我们首先去克隆一下这个电脑set啊。

![](img/516b4d6a63046c575afd05bf52048c07_134.png)

不是克隆一下这个代码。

![](img/516b4d6a63046c575afd05bf52048c07_136.png)

哦哦不是这个，这个的意思是说它需要下载一段时间啊，这个我也提前下载好，这个它的规模跟原跟普通的那个chat这样牛逼，是相当的，然后我们需要LORA。



![](img/516b4d6a63046c575afd05bf52048c07_138.png)

是这。

![](img/516b4d6a63046c575afd05bf52048c07_140.png)

这个是ZOLP这个项目，它里面包括了这个这个用LORA去翻译，应用chat这个6B的代码。

![](img/516b4d6a63046c575afd05bf52048c07_142.png)

OK利用这个时间我们正好看一下这个，因为这时间我正好看一下这个训练的问题，有是否有必要全量微调，这个其实，呃我觉得这个问题你可以自己试，因为对于不同的任务来说就是不太一样，你对于一些任务来说。

你这个拿LORA试一下就已经非常好了，事实上对于很多这个简单的问答，比如说你想你想让他去加一些额外的知识，你LORA看一下就会非常的好，但是你也可以试一下前两微调。

我觉得这个这个是一个非常practice问题，呃增量与训练，今天我们可能并不会讨论这个问题，什么是算哪一种，如果把max sequence land去掉，会发生什么。

哦哦刚才说看到说这个LORA上面有15G显存，那么它pad显卡还好，那pad显卡上可以跑PTUNE，可以跑量化版本的PETA，翻译镜下载游任务一般需要多少数据量呃，这个也是与任务差别很大。

而且由于你的这个翻译性的步数，和你最终翻译性能的目的是什么，如果你只是希望它有一个增量的一个，比如说你就是还希望它保持它原始能力，再稍微增增加一点知识，不要让他灾难性遗忘，那你其实用少量的数据翻译。

就一少量的次数就可以了，但如果你想要让他去彻底学会一个新任务，我觉得有时候几千条就可以，有时候得几万条的时候得几10万条，这个这个对于不同的任务来说，差别其实挺大的。

然后PTUNING和LORA两种翻译方式区别大嘛，怎么快速选择，是用哪个翻译方式，我觉得你都可以试一下，我我我之前主要用的是p to me too，呃我开了quantization b4。

需要跑17个小时，呃我觉得是有可能的，因为1080ti它的计算速，它的这个计算效果和3090应该是差一些，请问怎么把3091百三十B微调成6B的呃，这个应该不是今天能解答的问题。

然后跑p two v two的原始能力丢失，想试试LORA可不可以8G跑LORA，你可以试一下，把它宽大之后能不能用8G跑多少，但是呃一个问题是，你说原始能丢失，我觉得这个可能跟P兄弟好多啊。

没有必然关系，因为你做翻译听的话，他原始能力几乎肯定是要丢失很多的，你你做翻译的过程当中，如果你想要保持原始能力的话，你其实是需要拿到我的预训练的这些，这个就是或者说指令微调的时候，用到这个数据集。

然后你要把数据加入你的这个，我们一开始指令微调那个数据集里面去，然后你再做翻译，Q但是你现在没有办法拿到这些数据，所以说你肯肯定是容易出现这样一个问题，呃16GB显存肯定是不可以的。

事实上八张3090应该都跑不起来，全联微调多机对话卡，前两微调怎么配置，这个这个其实就直接跑这个文件就可以了。



![](img/516b4d6a63046c575afd05bf52048c07_144.png)

就是你只需要把比如说刚才他一开始用的这个，比如说number of node，你就直接在这配GPTU的数量，然后这个这个东西可以直接跑，不怎么需要配置，你只需要改好你的model path。

然后改好的数据集的位置，然后设定好这样东西就可以直接跑，不怎么需要配置，然后就是调参的事情，说int4int4模型，用p two02微调出输出的胡言乱语，我我我前我前两天试了一下，拿int4。

好像p q int调完之后是正常的，至少我这边是正常，这个关于6B和130B的性能，这个现在130B还在内测，微调后回答重复甚至一直重复，直接卡死呃，首先为这个这个事情是容易出现的，就是微调微调之后。

就是微调之后，因为它会破坏一下它的原始能力，所以微调之后，他在做你原你微调数据集上那个分布的语调，肯定会效果比较好，但是如果你问他，比如说你好，他可能啥也不会，他开始胡言乱语。

因为你在微调的过程中让他彻底，你怎么回答你好，或者怎么进行对话一些问题，那嗯嗯嗯，最难的方法是，不要去在微调之后去问这些out of distribution的数据，其次是说如果他真的一直重复卡死。

你可以试着给它加一个，比如说max max output than，比如说你觉得正常的这个output，可能二二百个头跟你差不多了，那你可以比如说给他设置一个max then，回复，最多回500个投屏。

max new投屏设置一下，这个就可以，int字，有成功案例吗，呃有的一会我可以看一下1。4的成功案例。



![](img/516b4d6a63046c575afd05bf52048c07_146.png)

就这个这这些东西，就是拿int4微调的模型生产出来，就是这个东西就是开了宽带，最近被四这么一个微调，然后它生生成的这个，你可以看到它的predict还是比较合理，就是至少看起来是很可以。

呃少量数据回答之后对稍作修改问题，泛化能力很差，和训练方式有关系吗，我觉得少量数据呃不太能指望他发挥能力很好，就是拿手量数据做微调，肯定发挥能力不会特别好，而且这个比赛里面他的这个我看了这几个chat。

好像他对泛化能力都没有特别高的要求，因为这几个track基本上都是你的，假设是微调类型的chat，那你基本上用于测试的数据和用于训练的数据，都是在同一个TRIBUTION里面，哇好多问题啊。

我觉得我还得稍微推推进度好，这边搞定，只有LP搞定，我们开一个新的cod窗口。

![](img/516b4d6a63046c575afd05bf52048c07_148.png)

报告。

![](img/516b4d6a63046c575afd05bf52048c07_150.png)

就是刚才克隆下载这个ZLB，OK他给了一个这样一个训练的ipad notebook，首先呢我们就直接看一下它的。



![](img/516b4d6a63046c575afd05bf52048c07_152.png)

它的这个整体的思路，他首先整了一些把这个chint pa的数据集以前close，然后从里面读东西，读游戏之后，他把他做了一个拆分，你是用这么大的chat才拆成了这个chk size。

拆成这样chat之后呢，他的意思是说，你要下载一下它这个用这个早期版本的chat，这样B他在上面做了一些代码的修改，然后你配一个，然后他用的是他跟face的这个pad这个库，它它里面有有有有有继承了。

这个LORA的一些东西，然后定义的这个一这个trainer和叠加C，然后connection的话，这个其实大同小异，几乎几乎所有的这个这个，这个这个东西都差不多，然后是清浅存一些东西，然后他就直接去。

但是我发现他就有一个问题，就是说他这个仓库里面如果你直接跑的话，他不太会打log，我也我也不知道为什么，所以说，好的，这是什么心。



![](img/516b4d6a63046c575afd05bf52048c07_154.png)

我一直跑到就行，所以说我对他这个做了一些修改，就是说首先比如就比如说，我们要去替我们这样一个advertise，这样一个数据集，那么，我们我们我们首先可以把它转成，他所要的这个格式。

他要的这个格式就是这样这样这样一些格式，然后你可以把它转一下，或者你也可以不这样做，你就可以直接在这个读读取数据的地方，直接改一下都可以啊，我应该是直接改一下读取数据这个地方。

首先因为我觉得在ipad notebook里面跑，并不是特别方便，因为你需要保证一直都连着，或者是vs code给你后台挂着，非常容易非常容易，所以说我就单独拿了一个这个train文件出来了。

主要代码就是复制了他这里面代码，然后还是一样的下载模型下载training，但是它load data set这部分你就不用不用下载，然后你可以把这个data set，改成我们自己的这个data set。

就是把china打来了，他刚才就变大赛改了，改完之后呢，他在龙队加赛的过程当中，原来用的是这个CSP，你可以把CFP改成JASON，然后它的PREPROCESS参数其实就是只指导了他。

可能把这个example变成相应的这个prompt呃，呃说错了，把把咱把这个example变成input和output，就是他要要要用这个东西，然后你再直接训练就可以。



![](img/516b4d6a63046c575afd05bf52048c07_156.png)

就在伊卡错了，有人在用。

![](img/516b4d6a63046c575afd05bf52048c07_158.png)

![](img/516b4d6a63046c575afd05bf52048c07_159.png)

嗯然后就是说我的建议是，把他这个训练的这个周int notebook给给改了，改到一个Python Python文件夹，改到PYTH文件里面，然后它的influence的话，我我试了一下。

如果拿LORA去翻译two这个A键数据集，它的效果也是挺好的，就是这是他的一个输出，这个东西应该是翻译int了十几个小时吧，它的我翻译性能一炮非常的大啊，这个翻译听的这个数量非常的大，就是打了这么多。

但是其实没有必要，当时我只是直接暴力来了来了来了来了四个，这个医保应该不用这么多，也可以，应该是几个小时就差不多，然后他他的做法也是一样，先做一遍map，就是把它token int之后，然后就继续。

所以LORA代码的话，其实可以参考我的建议是参考这个这个文件，然后把它改到一个Python里面去。

![](img/516b4d6a63046c575afd05bf52048c07_161.png)

然后数据处理的部分就是像刚才说的，你可以改成我们自己对，然后influence的话也可以稍微魔改一下，它的这个influence代码就可以了，就是用LORA来跑一个方法好，那接下来还有一些时间。

我们就来，呃在在尽可能的回答一下大家的这个，大家的问题啊，预训练过程，这个不太好讲，就是不太能说，OK然后哇好我我我，哇哇好快，说好我就先先先随便找一个看吧，因为真的是问题很多，OK就是说，说微调训练。

比如说我拿数据集，只有两条数据，这个实时微调近似实时是什么意思，我觉得如果你用这么几条数据去调的话，他很可能出现就是一下就就是憨憨的训练，他刚才大家说的这灾难性遗忘。

或者是说就是很快就不太知道别的东西了，就是如果你要打算，还保留它原来对话能力进行微调，我觉得一方面是你可以用这个只有高效微调，另一方面是你可以有稍微多一点的数据，或者说你的场景。

你这个场景下载对话数据可以稍微多一点，呃说这个训练说这个输出训练数据量减少了，但是训练时间还是六个小时，是的诶，调整了max step之后还是六个小时，就是因为我们现在用了step作为单位。

我觉得调整max step之后，时间会会会减少的，调整max step之后应该会减少，就是时间跟你数据集大小，如果你用map max deep这种策略的话，跟你数据集大小没有直接的关系。

还是看你走了多少个stable，如果只是想要去呃，现在学习流程想快速走几遍，快速走几遍的话，你可以试着把这个pad device，这个gradient umulation stable弄小一点。

但这样效果会差一点，但是你可以如果你只是想快速的话，你可以把gradient acumulation stable弄小一点，推理带max token，是的，推理代码里的呃推理。

因为呃我们直接看下推理代码，就是它的呃好吧，不太好找，就是他的那个推理代码里的，意思是说，你的这个history还要把它编编码成一个字符串，然后他就把它问答问答RN这样的一个形式。

你可以看到这个model chat gm，这个GPT8文件里面是有有写这个东西，所以肯定是把整个history都加上去的，呃instruction比较复杂的时候容易胡言乱语。

这个我觉得很可能是你的训练的数据量，或者stable不够，可以试着呃找更多的数据来训练，或者是把step稍微提升一点，让它在训练集上拟合的更彻底一点，呃全量微调，为什么需要40个G的显存。

可以把这个zero stage开得更高，我只开到了二，如果你们可以在自己的计算资源上，试着开得更高一点，试一下，因为我觉得zero stage three它的速度稍微减慢。

微调数据集是否增加prompt，就是你在微调的时候，可以给它加一个统一的prompt，这个有些情况下会显著提升它的效果，但有些情况下也没有，我觉得你可以，这这是一个值得知识，可可以试的内容。

就比如说像就比如说像VIKA，他在反进入拉满的时候，他就加了一个前置的pad，华为NPU，这个我没有做过测试，OK我是不是差不多到了刚才刚才的这个位置，哇又死了几条，嗯好那现在已经3。05了。

我再回答几条问题，说这个训练集测试集是怎么分配，我觉得如果把所有东西都放在训练集里面的话，看看你你在测你的测试会有一些问题，你可能不知道哪，怎么什么数据去凭他的这个效果，所以肯肯定要留一些。

你就留一小条测试来做测试，但是这个测试肯定就跟很多打榜那种测试，不一样，肯定不是说你去衡量他的，比如说blue呀或者什么，你很可能就是设置的东西，就是你自己去看一下，stable加大会有过度遗忘。

这种情况下，其实最理想的情况下是，你要你拿到所有的这个preaching数据，然后把它加到数据里面，不是说错了，不是PRETRAIN，就是instruction，翻译tune和对话翻译器的数据。

你把它带到这个里面是最理想，但是但是你如果没有这部分数据的话，我觉得这这个这个并不是很容易解决这个问题，6B模型啊，不肯定不是只有encoder，它的它的前缀来说是双向的，双双向数据编码。

但是它的后面都是AUTOGRAVE的生成，呃说做分类任务的时候，输出的分类标签能不能输出分类概率，那你如翻译听的时候，你也可以把，就是说如果你想输出分类标签的话，那你做法显然就是去做这个。

比如说prompt，然后它的output就是那个token或者token sequence，如果你想让它输出这个分类的概率的话，你可以你可以把它呃加一个新的head。

你把原来的vocabulary在后面再接一个head，或者把它去掉，然后你换一个换一个head，这样你就可以像原来一样做成就结果，或者说你可以像原来一样输出largest。

然后过去把largest for of max这样一个简单的操作，提升专业知识的数据要怎么准备，问答形式呃，这个如果你打算把它调成一个专业术语问答，问答机器人的话，我觉得你需要准备嗯。

相当多的专业专业知识的问答，然后你可以给他做一些简单的数据增广，然后翻译听一下试试，但是翻译听完之后的效果很可，很大程度上很可能就是还是局限于这样一个，就是局限于你的专业领域。

如果你问一些out of distribution的问题，他可能就不太会回答了，max target live需要数据，其实平均长度是5000左右，要怎么截断呃，当当你的长度超过我们训练的长度的话。

可能会有这个下降，可能会有这个性能性能性能下降，截断的长度，我觉得我不知道你这5000是5000个token，还是说它本身的就是字符串的长度是5000，结果字符串的长度是5000的。

encode完之后应该会这个token int，它的效率还是很高的，可能可能会很短嗯，阶段的长度你在考虑到你硬件的需求下，可能是肯定是越长越好，呃history越来越长会对如果你有多轮的数据呢。

history作为一条history会越来越长，会对微调效果有什么影响，就是比如说你有一个history，那你微调的做法很可，假设你有一个dialogue，我我我举个例子，比如说你有个带logo。

比如说比如说Q1A1代码怎么QA，然后Q2A2Q3A3，比如说你这么一个数据，那那你做法是第一次给pad Q1，然后让他输出A1，第二次呢就是用sequence，那用这个history模式给三个。

让他输出A2，第三次是给这个让他输出A3，就是这样一个模型，而不是不不能把它一口气都放进去啊，不能比如说一口气把这三个放进去，然后他数数A3，这样的话A1和A2是没有训到，如何让具体稳定的输出呃。

如果你希望他有一个稳定的输出，你可以用一些比如说像large processor之类的东西，把他其他的不允许输出，lodge都ban掉，这是最简单的办法，你不需要做任何的额外处理。

直接把largest ban掉，它就可以解决大输出，其他比如说你输出分类任务的话，对如果用，如果你是用知识库的话，我觉得你应该加一些retrieve的模块在里面，呃翻译听原始能力是否一定丢失。

这个与与你的与你的这个翻译性能的时候，用的数据是有关系的，如果你直接用一个新的数据来翻译的话，原始能力肯定会有比较明显的丢失，但是你比如说你减少一些发行的step，或者是你加一些明显的pattern。

或者加一些FP都可能缓解这个事情，但是对于不同的数据集来说差别很大，呃关于我发现好几个人问了这个，关于6亿用了多大的数据集规模，这个不太好说，但是如果你打算比如说你训练个代码生成模型。

或者是你要训练一个专业问答的话，对话数据的话肯定是显然是基本上是越多越好，越多它泛化性会效果越好，很多时候我们自己训练的时候发现他化器不好，因为数据不够，也是数据，数据加上去。

它很自然就会提升泛化的问题，就会解决泛化的问题，然后需要准备多少量级的数据，就比如说你代码上面的这个任务，我觉得还是有一定难度的，但是你可以先用用，比如说用个几万条，几万条试一下。

看看它性能的效果怎么样，还有就比如说你想把它做成一个代码生成的，这么一个模型，我觉得里面有很多可能是工程上的问题，需要需要考虑，比如说你的prompt里放什么pad，怎么放出的。

这些问题都会比较明显的影响到的生成效果呃，his story多长的知识，其实呃如果你去看这个mod，这个chat gl a modelint p y这个文件里面。

其实它它并不区别你history知识多长，只要你的history嗯，没有超过他的这个这个最大的这个context，这个window应该应该是2048，我记得如果你没超过这个的话，他应该是要多长有多长。

即使超过的话也没有关系，他会把离你当前对话较远的history都截断掉了，还要把离你对话对话较远的history截断掉，然后离就保留比较近的那些呃，P two two，同一个问题换一种问法就答不对了。

我觉得这是很常见的，就是这个因为因为你的，因为你的这个假设的训练数据比较有限的话，确实不可能保证模型一定会泛化到，泛化到这个其他的地方去，呃刚才这个这个有些同学问这个，说就是说到130币用。

用了多少才能才能单卡的显存都是不够的，对对，如果模型量很大的话，肯定是要上，肯定肯定是要用这个MODEPARALLEL之类的，呃中文搜索相关任务，这个你可以可以考虑去看一些，比如说像呃。

比如说像chat gm，Long chat，参考一下的那种思路，我觉得如果你要做搜索或者是知识库类的问题，你加一些retrieve的模块是非常重要的啊，Presequence that。

presequence that就对应于这个PETUNING，它的那个前缀，呃说高并发测试数据大的显存崩掉，怎么机制轻松显存，呃按理说你的这个是你的这个tensor，他离开之后。

他的garbage collection会自动做这个事情，如果你想要去显示的，去把这显显存清除掉，你可以再比如说你这个cash kb，你的kv cash用完之后，你可以手动把它delete掉。

然后twitch chat empty cash，这样可以试一下这种操作，但是我其实呃试了一些，就比如说让他去连续回答几千个几万的问题，我好像我好像觉得代码中没有没有这个bug，好像它是不会爆显存。

呃embedding知识库微调账号是怎么选择，我觉得如果比如说这个比赛他有这个MING赛道啊，就知识就是与知识问答这种赛道，我觉得嗯我觉得一种比较简单的做法，可能就是你你直接你肯定要加一些知识库。

因为你很难把这些知识，在不破坏它模型能力的情况下，全部给它注入进去，gm6B的代码是开源的吗，是的，代码是完全开源的，它的整个模型的代码都是开源的，你可以在GITHUB上面看到哈，根pad上面也有。

咱们直接把QA的A作为分类的label，呃我觉得如果你要做QA的话，A肯定不是做stable，应该是做的是这个fighting的response，多个场景，每个场景微调都不同。

这种需要部署多少个chat，这样的牛逼，我觉得如果每个场景它差异非常大的话，应该每个场景都需要微调一个，呃instruction tuning的chat这样说明怎么做。

呃刚才给的这个zero NO p这个数据，他的这个默认训练代码，其实就是试图的拿这个这个china pak数据集，做instruction tune，所以你可以看下这个代码。

它的做法就是非常经典的一种instruction input，output这样的形式，同样大小的数据集参数量大嗯，呃其实参数最优的这个参数，其实是跟你数据集是有一个线性的关系的。

就是说如果你的数据集太太小了，然后你有很大的参数，那可能几个set不下载就过拟合了，所以说参数越大，其实你要对于模型，你要求的数据集其实也是要求有约的，那如果我把比如说把文档in bed。

这个比比如说你有一篇很长的文档，那第一种做法就是把它拆成很多段，然后embed，然后用户来了一个输出，你拿这个输出去查EBEDDING，查出来那个段落，把这个段落变成这样。

拆了这样六臂的这么一个输出的prompt，你把这个拿EBELDING查出来了，就变成prompt，然后拿这个prompt去呃，去年开的这个六一，从抽取信息肯定没办法直接把它变得进去呃。

LORALORA的话，它就是像刚才说他会他会做一个增量，增量参数，就是一个低质的增量参数在上面呃，啊我的天又刷到这，嗯嗯你说的哦，呃130，130B的这个分布式步数，这个可以参考，我记得这代码是开源的。

就是一一百三十B的部署代码。

![](img/516b4d6a63046c575afd05bf52048c07_163.png)

在130B的仓库里应该有。

![](img/516b4d6a63046c575afd05bf52048c07_165.png)

对这个仓库里面应该做了，怎么怎么用这个fp transformer部署，对这个influence，the best transformers里面应该有讲这个部署。



![](img/516b4d6a63046c575afd05bf52048c07_167.png)

如果拿LORA训练知识扮演prompt，要怎么设置背景知识，还重复，如果背景知识重复，就是说，如果你想让它变成一个特别专一的，这么一个模型，那你显然你可以把背景知识都略掉，让它自己就直接学这个。

比如说从角色到对话这么一个东西，但是我觉得如果你这么做的话，问题就是他肯定会把对话的知识彻底丢掉，天津对话就彻底完蛋了，所以说我觉得你假设他有一个背景知识，你还是应该重复一下呃，训练模型做混淆简历。

比如说被速度四简历呃，就这类型的问题呃，如果你想要让模型在保持对话能力的情况下，呃去训练这种东西，我觉得是非常难的，因为你想让模型去学习VC6的解密，其实需要比较大量的相关训练数据。

而这个训练数据加进去，模型的对话能力肯定彻底废掉，呃强化数学推理能力进行微调，呃同样跟刚才其实是其实是类似的，这个数据推理能力，如果你想把它做成一个专一专一的模型，让他彻底抛弃对话能力的话。

你你可以选一个数据集作为微调，但是大概率的结果就是它对话能力彻底废掉，OK现在已经3。20了，OK3。2十了，那我们就请这个下载下一位老师来做分享，我今天的分享就到这里。



![](img/516b4d6a63046c575afd05bf52048c07_169.png)