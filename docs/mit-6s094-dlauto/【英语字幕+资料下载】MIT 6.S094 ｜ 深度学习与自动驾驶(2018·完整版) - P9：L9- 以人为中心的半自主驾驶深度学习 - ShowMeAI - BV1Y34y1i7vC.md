# „ÄêËã±ËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëMIT 6.S094 ÔΩú Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™Âä®È©æÈ©∂(2018¬∑ÂÆåÊï¥Áâà) - P9ÔºöL9- ‰ª•‰∫∫‰∏∫‰∏≠ÂøÉÁöÑÂçäËá™‰∏ªÈ©æÈ©∂Ê∑±Â∫¶Â≠¶‰π† - ShowMeAI - BV1Y34y1i7vC

All rightÔºå so the human side„ÄÇOf AI„ÄÇSo how do we turnÔºüThis camera back in on the human„ÄÇ

So we've been talking about perceptionÔºå how to detect cats and dogs„ÄÇPedestrians„ÄÇ

 lanes how to steer a vehicle based on the external environment„ÄÇ

The thing that's really fascinating and severely understudied is the human side„ÄÇ

So you know you talk about the TeslaÔºå we have cameras and 17 Teslas driving around Cambridge„ÄÇ

Because Te is one of the only vehiclesÔºå allowing you„ÄÇTo„ÄÇTo experience in the real way on the road„ÄÇ

 the interaction between the human and the machine„ÄÇ

And the thing that we don't have that deep learning needs on the human side of semi autotonnomous vehicles and fully autonomous vehicles is video of drivers„ÄÇ

 That's what we're collecting„ÄÇThat's what my work is in„ÄÇ

 is looking at billions of video frames of human beings driving 60 miles an hour plus on the highway in their semi autotonnoommous Tesla„ÄÇ

What are the things that we want to know about the humanÔºü

If if we are a deep learning therapist and we try to tear apartÔºå break„ÄÇ

 break apart the different things we can detect from this raw set of pixels„ÄÇ

We can look here from the green to red is the different detection problems„ÄÇ

 the different computer vision detection problemsÔºå green means it's less challenging„ÄÇIt's feasible„ÄÇ

 even under poor lighting conditionsÔºå variable poseÔºå noisyÔºå environmentÔºå poor resolution„ÄÇRed„ÄÇ

Means it's really hard no matter what you do„ÄÇThat's starting on the left with face detection„ÄÇ

 body poseÔºå one of the best studied and one of the easier computer vision problems„ÄÇ

We have huge data sets for these„ÄÇAnd then there is microcicos„ÄÇ

 the slight tremors of the eye that happened in one„ÄÇAt a rate of a thousand times a second„ÄÇAll right„ÄÇ

 let's look at„ÄÇBut firstÔºå why do we even careÔºüAbout the human in the car„ÄÇSo one is trust„ÄÇ

 the stress part is so you think about it„ÄÇTo build trust„ÄÇ

 the car needs to have some awareness of the biological thing it's carrying insideÔºå the human inside„ÄÇ

 you kind of assume the car knows about you because you're like sitting there controlling it„ÄÇ

But if you think about itÔºå almost every single car on the road today has no sensors with which it's perceiving you„ÄÇ

 It knows some cars have a pressure sensor on the steering wheel„ÄÇ

And a pressure sensor or some kind of sensor detecting that you're sitting in the seat„ÄÇ

 That's the only thing it knows about you„ÄÇThat'sÔºå that's it„ÄÇ So how is the car supposed to„ÄÇ

This same car is driving 70 miles an hour on the highway autonomously„ÄÇ

How is it supposed to build trust with you if it doesn't perceive you that's one of the critical things here„ÄÇ

 So if I'm constantly advocating somethingÔºå it' that we should have a driver facing camera in every car„ÄÇ

And that despite the privacy concernsÔºå now you have a camera on your phone and you don't have as much of a privacy concern there is despite the privacy concerns„ÄÇ

 the safety benefits are hugeÔºå the trust benefits are huge„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_1.png)

So let's start with the easy oneÔºå body poseÔºå detecting body pose„ÄÇWhy do we care„ÄÇ

 So there is seatbelt design„ÄÇThere's these dummies crashed as dummies with which are used to design the safety system„ÄÇ

 the passive safety systems in our carsÔºå and they make certain assumptions about body shapesÔºå male„ÄÇ

 femaleÔºå child body shapesÔºå but they also make assumptions about the position of your body in the seat„ÄÇ

They have the optimal positionÔºå the position they assume you take„ÄÇ The reality is„ÄÇIn a Tesla„ÄÇ

When the car is driving itself„ÄÇThe variabilityÔºå if you remember the catÔºå the deformable cat„ÄÇ

 you start doing a little bit more of that„ÄÇ you start to reach back in the back seat in your purse„ÄÇ

 your bagÔºå for your cell phoneÔºå these kinds of things„ÄÇ and that's when the crashes happened„ÄÇ

 and we need to know how often that happensÔºå the car needs to know that you're in that position„ÄÇ

That's critical for that very serious moment when the actual crash happens„ÄÇHow do you doÔºü

This is deep learning classÔºå rightÔºå So this deep learning you're the rescue„ÄÇ

Whenever you have these kind of tasks of detectingÔºå for exampleÔºå body poses„ÄÇ

 you're detecting points of the shouldersÔºå points of the headÔºå5Ôºå10 points along the arms„ÄÇ

 the skeleton„ÄÇHow do you do thatÔºå You have a CNN con neural network„ÄÇ

That takes this input image and takes an output„ÄÇ It's a regressor„ÄÇ

 It gives an X Y position of the whatever you're looking for„ÄÇ

 the left shoulder of the right shoulderÔºå and then you have a cascade of regressors„ÄÇ

That give you all these points to give you the shouldersÔºå the armsÔºå and so on„ÄÇAnd thenÔºå you have„ÄÇ

Through time on every single frameÔºå you make that prediction„ÄÇAnd then you optimize„ÄÇYou know„ÄÇ

 you knowÔºå you knowÔºå you know you can make certain assumptions about physics that you can't„ÄÇ

 your arm can't be in this place in the in one frame„ÄÇ and then the next frame be over here„ÄÇ

 It moves smoothly through space„ÄÇ So under those constraintsÔºå you can then minimize the error„ÄÇ

The temporal error from frame to frameÔºå or you can just dump all the frames as if there are different channels„ÄÇ

 like RGB is three channels„ÄÇ You could think of those channels in time„ÄÇ

 You can dump all those frames together in what are called 3D convolutional neural networks„ÄÇ

 You dump them all together„ÄÇ and then you estimate the body pose in all the frames at once„ÄÇ

And there is some data sets for sports„ÄÇ

![](img/2e59b1469b0badf387c16c287812cb16_3.png)

And we're building our own„ÄÇ I don't know who that guy is„ÄÇLet'sÔºå let's fly through this a little bit„ÄÇ

 So what's called gaze classification„ÄÇ Ga is another worth glanceÔºå right„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_5.png)

It's a classification problem„ÄÇHere's one of the TAs for this class„ÄÇAgain„ÄÇ

 not here because he's marriedÔºå had to be home„ÄÇI know his priorities are at„ÄÇ This is on camera„ÄÇ

 should be here„ÄÇThere's five camerasÔºå this is what we're recording in the Tesla„ÄÇ

 this is the Tesla vehicle„ÄÇüòäÔºåThere's in the in the bottom right„ÄÇ

 there's a blue icon that lights up automatically detected if it's operating under autopilot„ÄÇ

 That means the car is currently driving itself„ÄÇ There's five camerasÔºå one of the forward roadway„ÄÇ

 One of the instrument clusterÔºå one of the center stacksÔºå steering wheelÔºå his face„ÄÇ

 And then its it's a classification problem„ÄÇ You dumped the raw pixels into a convolution in neural network„ÄÇ

 have six classesÔºå forward roadway„ÄÇ You're predicting where the person is looking„ÄÇ

 Forward roadway leftÔºå right„ÄÇCenter stackÔºå instrument clusterÔºå rearview mirror„ÄÇ

And you give it millions of frames for every classÔºå simple„ÄÇAnd it does„ÄÇIncredible well at predicting„ÄÇ

Where the driver is looking And the process is the same for majority of the driver' state problems that have to do with the face„ÄÇ

 The face has so much information where you're lookingÔºå emotionÔºå drowsiness„ÄÇ

 So different degrees of frustration„ÄÇ Ill fly through those as well„ÄÇ But the process is the same„ÄÇ

 There's some preproces„ÄÇ So this is in the wild data„ÄÇ There's a lot of crazy light going on„ÄÇ

 There's noiseÔºå There's vibration from the vehicle„ÄÇ So firstÔºå you have to video stabilization„ÄÇ

 You have to remove all of that vibrationÔºå all of that noiseÔºå as best as you can„ÄÇ There's a lot of„ÄÇüòä„ÄÇ

AlgorithmsÔºå non neural network algorithms„ÄÇBoringÔºå but they work for removing the removing the noise„ÄÇ

 removing the effects of sudden light variations and the vibrations of the vehicle„ÄÇ

 There's the automated calibration„ÄÇ So you have to estimate theÔºå the frame of the camera„ÄÇ

 the position of the camera„ÄÇAnd estimate the identity of the person you're looking at„ÄÇ

The more you can specialize the network to the identity of the person and the identity of the car the person is riding in„ÄÇ

 the better the performance for the different driver state classification„ÄÇ

So you personalize the networkÔºå you have a background model that works on everyone and you specialize each individual„ÄÇ

 this is transfer learningÔºå you specialize each individual network to that one individual„ÄÇAllright„ÄÇ

 there is face frontilizationÔºå fancy name for the fact that no matter where they're looking„ÄÇ

 you want to transfer that face to the eyes the nose of the exact same position in the image„ÄÇ

 that wayÔºå if you want to look at the eyes„ÄÇAnd you want to study the subtle movement of the eyes„ÄÇ

 the subtle blinkingÔºå the dynamics of the eyelidÔºå the velocity of the eyelid„ÄÇ

 it's always in the same place so you can really focus in„ÄÇ

 remove all effects of any other motion of the head„ÄÇAnd then you just„ÄÇ

 this is the beauty of deep learningÔºå rightÔºå You don't„ÄÇ There is some pre processing because this is„ÄÇ

You knowÔºå real world dataÔºå but you just dump to raw pixels in„ÄÇYou don't throw all pixels in and„ÄÇ

 and predict whatever you need„ÄÇ What do you need„ÄÇ One is emotion„ÄÇ You can have„ÄÇ So we had we had„ÄÇ

 we had a study where people used a crappy and a good„ÄÇBoysbased navigation system„ÄÇ

 So the crappy one got them really frustratedÔºå and they self-reed that as a frustrating experience or not on scale 1 to 10„ÄÇ

 that gives us ground truth„ÄÇ But it had a bunch of people use this system„ÄÇ

 And you know they put themselves as frustrated or not„ÄÇ

 And so then we can predict we can train a con in your network to predict„ÄÇ

 is this person frustrated or not„ÄÇ I think we've seen a video of that„ÄÇ

 turns out smiling is a strong indication of frustration„ÄÇ

 You can also predict drowsiness in this way„ÄÇGayze estimation in this wayÔºå cognitive load„ÄÇ

 I'll briefly look at that„ÄÇ And the process is all the same„ÄÇ You detect the face„ÄÇ

 You find the landmark points in the face for the face alignmentÔºå face frontalilization„ÄÇ

And then you dump the raw pixels in for classification„ÄÇ Step 5Ôºå you can use SVMs there„ÄÇ

 or you can use what everyone uses nowÔºå con your neural networks„ÄÇ

This is the one part where CNNNs have still struggled to compete„ÄÇIs the alignment problem„ÄÇ

This is why I talked about the cascade of regressors„ÄÇIs„ÄÇFinding the landmarks„ÄÇOn„ÄÇThe the eyebrows„ÄÇ

 the noseÔºå the jaw lineÔºå the mouth„ÄÇThere's certain constraints there„ÄÇ

 And so algorithms that can utilize those constraints effectively can often perform better than end to end regressors that just don't have any concept of what a face is shaped like„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_7.png)

And there's huge data sets„ÄÇ And we're a part of of the awesome community that's building those data sets for face alignment„ÄÇ

OkayÔºå so this isÔºå againÔºå the T A in this younger form„ÄÇThis is in live in the car„ÄÇ

 real time system predicting where theyre looking„ÄÇ

![](img/2e59b1469b0badf387c16c287812cb16_9.png)

This is taking slow steps towards the„ÄÇExciting direction that machine learning is headed„ÄÇ

 which is unsupervised learning„ÄÇThe less you have to have humans look through the data and annotate that data„ÄÇ

 the more power these machine learning algorithms getÔºå right„ÄÇCurrently„ÄÇ

 supervised learning is what's needed„ÄÇ You need human beings to label a cat and label a dog„ÄÇ But if„ÄÇ

 if you can only have a human being label 1%„ÄÇOne10th of a percent of a data„ÄÇOnly the hard cases„ÄÇ

 So the machine can come to the human and be likeÔºå I don't know„ÄÇ

 I don't know what I'm looking at at these pictures because of the partial light occlusions„ÄÇ

 We're not good at dealing with occlusions„ÄÇWhether it's your own arm or because of light conditions„ÄÇ

 we're not good with„ÄÇCrazy light drown out the image„ÄÇ

 This is what Google self driving car actually struggled with when they're trying to use their vision sensors„ÄÇ

 moving out of frame„ÄÇ So just all kinds of occlusions are really hard„ÄÇ4 computer vision algorithms„ÄÇ

And in those caseÔºå we want a machine to step in and say and pass that image onto to the human and be like„ÄÇ

 help me out with this„ÄÇAnd the other corner cases isÔºå so in drivingÔºå for example„ÄÇ

 90 plus percent of the timeÔºå all you're doing is staring forward at the road in the same way„ÄÇ

 That's where the machine shines„ÄÇ That's where machine annotation„ÄÇ

 automated annotation shines because it's seen that face for hundreds of millions of frames already in that exact position so it can do all the hard work of annotation for you it's in the transition away from those positions that it needs a little bit help just to make sure that this person just start looking away from the road to the rear view and you bring those points up so there's using optical flow„ÄÇ

 putting the optical flow in the convolution in neural network„ÄÇ

You use that to predict when when something has changed and when something has changed„ÄÇ

 you bring that to the machine for annotation„ÄÇ All of this is to build a giant„ÄÇBillions of frames„ÄÇ

 annotated data set of ground truth„ÄÇ I want to train your driver state algorithms„ÄÇAnd in this way„ÄÇ

 you can control on the X axis is the fraction of frames the human has to annotateÔºå0% on the left„ÄÇ

10% on the right„ÄÇ And then the accuracy trade off„ÄÇ the more the human an isÔºå the higher the accuracy„ÄÇ

 you approach 100% accuracy„ÄÇBut you can still do pretty good„ÄÇ

 This is for the gaze classification task„ÄÇWhen„ÄÇWith an 84% 84 fold to almost two orders in magnitude reduction in human annotation„ÄÇ

 This is the future of machine learning„ÄÇAnd hopefullyÔºå one dayÔºå no human annotation„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_11.png)

And the result„ÄÇIs millions of images like thisÔºå video frames„ÄÇSame thing„ÄÇ driver frustration„ÄÇ

 This is what I was talking about„ÄÇ The frustrated driver is the one that's on the bottom„ÄÇ

So a lot of movement of the eyebrows and a lot of smiling„ÄÇ and that's true subject„ÄÇ

 that's the subject„ÄÇAnd the happyÔºå the satisfiedÔºå when don't say happy„ÄÇ

 the satisfied driver is cold and stoic„ÄÇ And that's true for subject after subject„ÄÇ

 because driving is a boring experience„ÄÇ And you want it to stay that way„ÄÇ YesÔºå question„ÄÇüòäÔºåGreat„ÄÇ

 greatÔºå great questionÔºå they're not„ÄÇËÅäÂ§©„ÄÇAbsolutelyÔºå that's a great question„ÄÇ There is a„ÄÇ

 so this is cars owned by MIT„ÄÇ There is somebody in the back„ÄÇ‰ª¨ÂÆ∂„ÄÇSo the comment was„ÄÇ

 my emotions might then have nothing to do with the driving experience„ÄÇ Yes„ÄÇ

 and let me continue that comment is your emotions are often„ÄÇ

You're an actor on the stage for others with your emotion„ÄÇ So when you're alone„ÄÇ

 you might not express emotion„ÄÇ You're really expressing emotion oftentimes for others„ÄÇ

 Like your' frustrated likeÔºå what the heck„ÄÇ That's for for the passenger„ÄÇ

 And that that's absolutely right„ÄÇ So one of the cool things„ÄÇWe're doing as I said„ÄÇ

 we now have over a billion video frames in the Tesla„ÄÇ

 We study collecting huge amounts of data in the Tesla„ÄÇ And it emotion is complex thingÔºå right„ÄÇ

 In this caseÔºå we can we know the ground truth frustrated they were in naturalistic data when it's just people driving around„ÄÇ

 We don't know how theyre really feeling at the moment„ÄÇ

 We're not asking them to like enter in an app„ÄÇ How are you feeling right now„ÄÇ

But we do know certain things like weÔºå we know that people sing a lot„ÄÇ

That has to be a paper at some point„ÄÇ It's awesome„ÄÇ People love singing„ÄÇüòä„ÄÇ

So that doesn't happen in this kind of data because there's somebody sitting in the car„ÄÇ

 And I think the expression of a frustration is also the same„ÄÇÈóÆ‰Ω†Âêß„ÄÇdrive„ÄÇYesÔºå so„ÄÇYeah„ÄÇ

 the question isÔºå yeah or the comment is that the data set„ÄÇ

 the solo data is probably going to be very different from a data that's nonsolo with a passenger„ÄÇ

 and it's very true„ÄÇ the tricky thing about driving and this is why it's a huge challenge for cell driving cars for the external facing sensors and for the internal facing sensors„ÄÇ

 analyzing human behavior is like 99„ÄÇ9% of driving is the same thing it's really boring„ÄÇ

 So finding the interesting bits is actually pretty complicated„ÄÇSo that has to do with emotion„ÄÇ

 that has to do with„ÄÇ So singing is easy to find„ÄÇ So we can track the mouth pretty well„ÄÇ

 So whenever you're talking or singingÔºå we can find that„ÄÇ

 But how do you find the subtle expressions of emotionÔºå It's hard„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_13.png)

When you're solo„ÄÇAnd cognitive load„ÄÇThat's„ÄÇThat'sÔºå that's a fascinating thing„ÄÇ I mean„ÄÇ

 similar to emotionÔºå it's„ÄÇNowÔºå it's a little more concrete„ÄÇ

In the sense that there's a lot of good science and ways to measure cognitive load„ÄÇ

Cognitive workloadÔºå How occupied your mind is„ÄÇMental workload is another term used„ÄÇ

And so the the window to the soulÔºå theÔºå the cognitive workload soul isÔºå is the eyes„ÄÇ So pupil„ÄÇ

 So first of allÔºå the eyes move in two different ways„ÄÇ TheyÔºå they move a lot of ways„ÄÇ

 But two major ways is scos„ÄÇ These are these ballistic movements„ÄÇ

 They jump around whenever you look around the room„ÄÇ

 they're actually just jumping around when you readÔºå their eyes are jumping around„ÄÇ And when„ÄÇüòäÔºåLike„ÄÇ

 if follow you just follow this bottle with your eyes„ÄÇ

 that your eyes are actually gonna move smoothlyÔºå smooth pursuit„ÄÇ

 Somebody actually just told me today thats probably has to do with our hunting background or as animals„ÄÇ

II don't know how that helps„ÄÇ Like frogs track flies really well„ÄÇ So you have to likeÔºå I don't know„ÄÇ

 AnywayÔºå The point is there are smooth pursuit movements where the eyes move smoothly„ÄÇ

 And those are all indications of certain aspects of cognitive load„ÄÇ And„ÄÇ

 and then there is very subtle movementsÔºå which are almost imperceptible for computer vision„ÄÇ

 And these are microcicos„ÄÇ These are tremors of the eye„ÄÇ

Here work from here from Bill Freeman magnifying those subtle movements„ÄÇ

 These are taken at 500 frames a second„ÄÇAnd so cognitive load„ÄÇWhen the pupil„ÄÇ

 that black dot in the middleÔºå just in case we don't know what a pupil is in the middle of the eye„ÄÇ

 when it gets largerÔºå that's an an indicator of high cognitive load„ÄÇ

 But it also gets larger when the light is dim„ÄÇ So there's like this complex interplay„ÄÇ

 So we can't rely in the wildÔºå outside in the car or just in general outdoors on using the pupil size„ÄÇ

 even though pupil size have been used effectively in a lab to measure cognitive load„ÄÇ

 it can't be reliably used in the car„ÄÇ and the same with blinks„ÄÇThe when„ÄÇ

 when there's higher cognitive loadÔºå your blink rate decreases and your blink duration shortens„ÄÇOkay„ÄÇ

I think Im just repeating the same thing over and over„ÄÇ

 But you could imagine how we can predict cognitive loadÔºå right„ÄÇWe extract video of the eye„ÄÇ

Here is the primary eye of the person the system is observing„ÄÇHappens to be the same T A once again„ÄÇ

We take the sequence of 100Ôºå ohÔºå it's 90 images„ÄÇ So that's 6 secondsÔºå16 frames a second„ÄÇ

15 frames a second„ÄÇAnd we dumped that into a 3D convolution neural network„ÄÇAgain„ÄÇ

 that means it's 90 channels„ÄÇOf it's not 90 framesÔºå gray scale„ÄÇAnd then the prediction is„ÄÇ

 one of three„ÄÇClass is of cognitive load„ÄÇLow cognitive load„ÄÇ

 medium cognitive load and high cognitive load„ÄÇ And there's ground truth for that„ÄÇ

 because we have people over 500 different people do different tasks of various cognitive load„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_15.png)

And after some frontalilizationÔºå againÔºå where you see the eyes are trans„ÄÇ

 no matter where the person is looking„ÄÇThe image of the face is transposed in such a way that the eyes„ÄÇ

 the corners of the eyes remain always in the same position„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_17.png)

After the frontalilization„ÄÇWe find the I active appearance modelsÔºå find 39 points„ÄÇ

Of the eye of the eyelids„ÄÇThe iris and four points on the pupil„ÄÇ

Putting all of that into a 3D CNNN modelÔºå They positioned I sequence on the left3D CNNNN model in the middle„ÄÇ

 cognitive vote prediction on the right„ÄÇThis codeÔºå by the way„ÄÇIt's freely available online„ÄÇ

All you have to doÔºå dump a webcam„ÄÇFrom the video streamÔºå CNN runs and faster than real time„ÄÇ

 predicts cognitive load„ÄÇSame process as detecting the identity of the face„ÄÇ

 same process as detecting where the driver is lookingÔºå same process detecting emotion„ÄÇ

 and all of those require very little hyperparameter tuning on the convolutional neural networks„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_19.png)

They only require„ÄÇHuge amounts of data„ÄÇ

![](img/2e59b1469b0badf387c16c287812cb16_21.png)

And why do we care about detecting what the driver is doingÔºü And I think Eric has mentioned„ÄÇThis is„ÄÇ

On the oh manÔºå this is the comeback of the slide„ÄÇI was criticize this for this being a very cheesy slide„ÄÇ

In„ÄÇIn the path towards full automation„ÄÇWe're likely to take gradual steps towards that„ÄÇ

I can't it's enough of that„ÄÇ this is better„ÄÇAnd especially given that theÔºå this is„ÄÇGiven today„ÄÇ

 our new president„ÄÇThis is pickup truck country„ÄÇÂïä„ÄÇThis is manually controlled vehicle country for quite a little while„ÄÇ

 Will like control„ÄÇAnd control„ÄÇBeing given to somebody else to the machine will be a gradual process„ÄÇ

 is a gradual process of that machine earning trust„ÄÇAnd through that process„ÄÇ

 the machine like the TeslaÔºå like the BMWÔºå like the Mercedes„ÄÇ

 the Volvo that's now playing with these ideas„ÄÇIt's going to need to see what the human is doing„ÄÇ

And for that„ÄÇTo see what the human is doing„ÄÇWe have billions of miles of forward facing data„ÄÇ

 What we need is billions of miles of driver facing dataÔºå as well„ÄÇ

We're in the process of collecting that„ÄÇAnd this is a pitch„ÄÇFor automakers„ÄÇ

And everybody to buy cars that have a driver facing camera„ÄÇAndÔºå let meÔºå sort of„ÄÇCloseÔºå so I said„ÄÇ

 we need a lot of data„ÄÇBut I think this class has been„ÄÇAnd through your own research„ÄÇ

 you'll find that we're in the very early stages of discovering the power of deep learning„ÄÇ

For exampleÔºå you knowÔºå isÔºå is recently„ÄÇLike Jan Lacoon said„ÄÇThat it seems„ÄÇ

That the deeper the networkÔºå the better the results in a lot of really important cases„ÄÇ

Even though the data is not increasing„ÄÇSo why does a deeper network give better resultsÔºü

This is a mysterious thing we don't understand„ÄÇ there's these hundreds of millions of parameters„ÄÇ

And from them is emerging some kind of structureÔºå some kind of representation of the knowledge that we're giving it„ÄÇ

One of my favorite examples of this emergent concept is Conway's game of life„ÄÇ

For those of you who know what this is„ÄÇWill probably criticize me for it being as cheesy as the stairway slide„ÄÇ

 But I think itÔºå it's actually such a simple and brilliant example of„ÄÇ

 of how like a neuron in a neural network is a really simple computational unit„ÄÇüòä„ÄÇ

And then incredible power emerges when you just combine a lot of them in a network„ÄÇ

 And in the same wayÔºå theÔºå this is called a cellular automata„ÄÇüòäÔºåThat is a weird pronunciation„ÄÇAnd„ÄÇ

 and the ruleÔºå it it's every single cell is operating under a simple rule„ÄÇ

 You can think of it as a cellÔºå living and dying„ÄÇIt'sÔºå it's„ÄÇ

 it's filled in black when it's alive and whiteÔºå when it's dead„ÄÇ And when it has two orÔºå it'„ÄÇ

 if it's alive and it has two or three neighborsÔºå it survives to the next time slot„ÄÇOtherwise„ÄÇ

 it dies„ÄÇAnd if it has exactly three neighborsÔºå it's dead„ÄÇIt comes back to life„ÄÇ

 It has exactly three neighbors„ÄÇ That's a simple rule„ÄÇ whatever you can just imagine„ÄÇ

 It's just simple„ÄÇ All is' doing is operating under this very local„ÄÇA processÔºå same as a neuron„ÄÇ

It's's or in the way we're currently training neural networks in this local gradient„ÄÇ

 we're optimizing over a local gradientÔºå the same local rules„ÄÇ

 And what happens if you run this system„ÄÇOperating under the really local rules where you get on the right„ÄÇ

It's not„ÄÇAgainÔºå you have to go home„ÄÇHopefullyÔºå no drugs involved„ÄÇBut you have to open up your mind„ÄÇ

And and see how amazing that isÔºå because what happens is„ÄÇüòäÔºåIt'sÔºå it's a local computational unit„ÄÇ

That knows very little about the worldÔºå but somehow really complex patterns emerge„ÄÇ

And we don't understand why„ÄÇIn factÔºå under different rulesÔºå incredible patterns emerge„ÄÇ

 and it feels like it's living creatures like communicating„ÄÇ like when you just watch itÔºå not„ÄÇ

 not these examples„ÄÇ This isÔºå this is the original they get like complex and interesting„ÄÇ

 But even these examplesÔºå this complex geometric patterns emerge„ÄÇ It's incredible„ÄÇ

 We don't understand why„ÄÇ same when neural networks„ÄÇ We don't understand why„ÄÇ

 and we need to in order to see how these networks will be able to reason„ÄÇüòäÔºåOkayÔºå so what's nextÔºü

I encourage you to read the deep learning book„ÄÇIt's available onlineÔºå deeplearningbook„ÄÇorg„ÄÇ

As I mentioned to a few peopleÔºå you shouldÔºå WellÔºå first„ÄÇ

 theres a ton of amazing papers every day coming out an archive„ÄÇIÔºå I'll put these links up„ÄÇ

 but there is a lot of good collections of strong paper list of papers„ÄÇ

 There is the literally awesome listÔºå the awesome deep learning papers on Github„ÄÇ

 It's calling itself awesomeÔºå but it happens to be awesome„ÄÇüòä„ÄÇ

And there is a lot of blogs that are just amazing„ÄÇThat's how I recommend you learn machine learning is on blogs„ÄÇ

And if you're interested in the application of deep learning in the automotive space„ÄÇ

 you can come do research in our group„ÄÇJust email me„ÄÇAnywayÔºå we have three winners„ÄÇJeffrey Hugh„ÄÇ

 Michael Gm„ÄÇAnd„ÄÇHow do you are you here„ÄÇHeyÔºå how are you say name„ÄÇNo this„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_23.png)

Chris„ÄÇAll„ÄÇSo my name is Florida„ÄÇP„ÄÇÂà∞ÁöÑÊòØ„ÄÇOhÔºå I see„ÄÇW anywayÔºå here„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_25.png)

SoÔºå he„ÄÇAchievveed as the stunning speed„ÄÇOf„ÄÇSo IÔºå IÔºå this is kind of incredible„ÄÇ

 So I didn't know what kind of speed we're going to be able to achieve„ÄÇ

 I thought 73 was unbeatable because we played with it for a while„ÄÇ We couldn't achieve 73„ÄÇ

 We designed a deterministic algorithm that was able to achieve 74Ôºå I believe„ÄÇüòäÔºåMeaning like„ÄÇ

 it's cheating with a cheating algorithm that got 74„ÄÇ And so folks have come up„ÄÇ

With algorithms that have done have beaten 73 and then 74„ÄÇ So this is really incredible„ÄÇ

 And the other two guys„ÄÇ So all three of you get a free term at the Uaci cell drivingiv car engineering degree„ÄÇ

 Thanks to all those guys for giving it„ÄÇ given that award and bring in their army of brilliant So they„ÄÇ

 they have people who are obsess about cell driving cars„ÄÇ And we've received„ÄÇüòä„ÄÇ

Over 2000 submissions for this competition„ÄÇ A lot of them from those guys„ÄÇ

 and they're just brilliant„ÄÇ So it'sÔºå it's it's really„ÄÇüòä„ÄÇ

Exciting to have such a big community of deep learning folks working in this field„ÄÇ So this is„ÄÇüòä„ÄÇ

For the rest of eternityÔºå well we're gonna change this up a little bit„ÄÇ

 But this is actually the three„ÄÇNeural networksÔºå the three three winning neural networks running side by side„ÄÇ

 And you can see the number of cars passed there„ÄÇ The first place on the left„ÄÇ

Second place and third place„ÄÇ AndÔºå in factÔºå the third place is almost no„ÄÇ

 second place is winning currently„ÄÇBut that just tells you that„ÄÇThe random nature of competition„ÄÇ

 sometimes you winÔºå sometimes you lose„ÄÇSoÔºå there is„ÄÇ

 the actual evaluation process runs through a lot of a lot of iterations and takes to medium evaluation„ÄÇ



![](img/2e59b1469b0badf387c16c287812cb16_27.png)

With thatÔºå let me thank you guys so much for„ÄÇ WellÔºå waitÔºå waitÔºå waitÔºå got question„ÄÇ

would be winning networks at all„ÄÇYeahÔºå so all three guys wrote me a note about how their networks work„ÄÇ

 I did not read that noteÔºå so I'll post this tells you how crazy this has been„ÄÇ

 I'll post the winning networks„ÄÇTo online„ÄÇ andÔºå and I encourage you to continue competing„ÄÇ

And continue submitting networks„ÄÇ This will run for a while„ÄÇ And we're working on a journal paper„ÄÇ

For this„ÄÇFor„ÄÇThis game„ÄÇWe're trying to find the optimal solutions„ÄÇOkay„ÄÇ

 so this is the first time I've ever taught a class„ÄÇAnd the first time„ÄÇ

 obviously teaching this classÔºå and so thank you so much for being a part of it„ÄÇThank youÔºå Eric„ÄÇ

you didn't get a shirtÔºå please come backÔºå please come down and get a shirt„ÄÇ

Just write your email on the note on the index note„ÄÇThank you„ÄÇ

