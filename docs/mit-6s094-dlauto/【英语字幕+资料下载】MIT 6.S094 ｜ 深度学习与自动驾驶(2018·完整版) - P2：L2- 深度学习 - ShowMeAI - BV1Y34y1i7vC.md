# „ÄêËã±ËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëMIT 6.S094 ÔΩú Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™Âä®È©æÈ©∂(2018¬∑ÂÆåÊï¥Áâà) - P2ÔºöL2- Ê∑±Â∫¶Â≠¶‰π† - ShowMeAI - BV1Y34y1i7vC

Thank you everyoneÔºå for braving the cold and the snow to be here„ÄÇ

This is6S 094 deep learning for cell driving cars„ÄÇAnd it's a course where we cover the topic of deep learning„ÄÇ

Which is a set of techniques that have taken a leap in the last decade for our understanding of what artificial intelligence systems are capable of doing„ÄÇ

And self driving carsÔºå which is„ÄÇSystems that can take these techniques and integrate them„ÄÇ

In a meaningfulÔºå profound wayÔºå enter our daily lives in a way that transforms society„ÄÇ

So that's why both of these topics are extremely important and extremely exciting„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_1.png)

My name is Le Friedman„ÄÇAnd I'm joined by an amazing team of engineers in Jack Towilliger„ÄÇ

 Julia KiinsbergerÔºå Dan BrownÔºå Michael GlaerÔºå Lee DingÔºå Spencer DoddÔºå and Benedict Jenk„ÄÇ

 among many others„ÄÇWe build autonomous vehicles here at MIT„ÄÇ

 not just ones that perceive and move about the environmentÔºå but ones that interact„ÄÇ

 communicate and earn the trustÔºå and understanding of human beings inside the car„ÄÇ

 the drivers and the passengers and the human beings outside the car„ÄÇ

 the pedestrians and the other drivers and cyclists„ÄÇThe website for this courseÔºå selfdivingcars„ÄÇmt„ÄÇ

iduÔºå if you have questionsÔºå email deepcars@ mit„ÄÇidu„ÄÇSlackÔºå deep D MIT„ÄÇFor registered MIT students„ÄÇ

 you have to register on the website„ÄÇAnd„ÄÇBy midnightÔºå FridayÔºå January 19„ÄÇ

Build a neural network and submit it to the competition that achieves the speed of 65 m per hour on the new deep traffic 2„ÄÇ

0„ÄÇIt's much harder and much more interesting than last year's for those of you who participated„ÄÇ

There's three competitions in this classÔºå deep trafficÔºå se fuseÔºå deep crash„ÄÇ

There's guest speakers that come from WaymoÔºå Google„ÄÇTesla„ÄÇ

And those are starting new autonomous vehicle startups in voyage„ÄÇUtonomy„ÄÇAnd„ÄÇAurora„ÄÇ

In the news a lot today from Cs„ÄÇAnd we have shirts„ÄÇ

For those of you who brave the snow and continue to do so„ÄÇTowards the end of the class„ÄÇ

 there will be free shirts„ÄÇ YesÔºå I saidÔºå free in shirts in the same sentence„ÄÇ You should be here„ÄÇ

OkayÔºå firstÔºå the deep traffic competition„ÄÇThere's a lot of updates„ÄÇ

 and we'll cover those on Wednesday„ÄÇ It's a deeper reinforce learning competition„ÄÇ Last year„ÄÇ

 we received over 18000 submissions„ÄÇüòäÔºåThis yearÔºå we're going to go bigger„ÄÇüò°„ÄÇ

Not only can you control one car within your networkÔºå you can control up to 10„ÄÇ This is multi agent„ÄÇ

 deeper enforcement learning„ÄÇ This is super cool„ÄÇüòä„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_3.png)

SecondÔºå psych fuseÔºå dynamic driving scene segmentation competition„ÄÇWhere you're given„ÄÇThe raw video„ÄÇ

The kinematics of the vehicles and the movement of the vehicle„ÄÇ

The state of the art segmentation for the training setÔºå you're given ground truth labels„ÄÇ

 pixel level labelsÔºå scene segmentation„ÄÇAndd optical flow„ÄÇ And with those pieces of data„ÄÇ

 you're tasked to try to perform better than the state of the art in image based segmentation„ÄÇ

Why is this critical and fascinating and an open research problem„ÄÇBecause„ÄÇ

Robots that act in in this world in the physical space not only must interpret„ÄÇ

 use these deep learning methods to interpret the spatial visual characteristics of a scene„ÄÇ

 they must also interpretÔºå understand and track the temporal dynamics of the scene„ÄÇ

 This competition is about temporal propagation of information„ÄÇ not just scene segmentation„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_5.png)

You must understand the space and time„ÄÇAnd finally„ÄÇ

Deep crash where we use deep reinforcement learning to slam cars thousands of times here at MI T at the gym„ÄÇ

You're given data on 100 runs where a car knowing nothing is using a monocular camera as a single input driving over 30 miles an hour through a scene that has very little control through„ÄÇ

 very little capability to localize itselfÔºå it must act very quickly in that scene„ÄÇ

 you're given 10 runs to learn anything„ÄÇWe'll discuss this in the coming weeksÔºå this competition„ÄÇ

Will result in four submissions„ÄÇThat we evaluate everyone's in simulation„ÄÇ

 But the top four submissions we put head to head at the gym„ÄÇ And until there is a winner declared„ÄÇ

 we keep sing cars„ÄÇAt 30 miles an hour„ÄÇDeep crash and also on the website as from last year and on GitHub„ÄÇ

 there's deep Tesla„ÄÇWhich is using the large scale naturalistic driving data that we have to train a neural network to do end to end steering that takes in monocular video from the forward roadway and produces steering commands that steering commands for the car„ÄÇ

Lectures today we'll talk about deep learningÔºå tomorrow we'll talk about autonomous vehicles„ÄÇ

 deep parallells on Wednesday„ÄÇDriving scene understandingÔºå so segmentation„ÄÇThat's Thursday„ÄÇOn Friday„ÄÇ

 we have Sasha Arnu„ÄÇThe director of engineering at Waymo„ÄÇ

 Waymo is one of the companies that's truly taking huge strides in fully autonomous vehicles„ÄÇ

 Theyre taking the fully L 4 L 5 autonomous vehicle approach„ÄÇ

 And it's fascinating to learn He is also the head of perception for them„ÄÇ

To learn from him what kind of problems they're facing and what kind of approach they're taking on„ÄÇ

 We have Amelia FrizoliÔºå who one of last year's speakersÔºå Sir Dsh Carman„ÄÇ

 said Emillia is the smartest person he knows„ÄÇSo Amelia Frzo is the CT of autonomy„ÄÇ

 an autonomous vehicle„ÄÇcompany that was just acquired by Delphi„ÄÇFor a large sum of money„ÄÇ

 and they're doing a lot of incredible work in Singapore and here in Boston„ÄÇüòäÔºåNext Wednesday„ÄÇ

 we are going to talk about the topic of our research and my personal fascination is deep learning for driver state sensing„ÄÇ

 understanding the humanÔºå perceiving everything about the human being inside the car and outside the car„ÄÇ

One talk I'm really excited about is Oliver Cameron on Thursday„ÄÇ

He is now the CEO of Autonomous Vehicle startup Voyage„ÄÇ

 he was previously the director of the selfdriving Car Program for Uudacity„ÄÇ

 he will talk about how to start a selfdriving car company For those„ÄÇ

 he said that MIT folks and entrepreneursÔºå if you want to start a one yourself„ÄÇ

 he'll tell you exactly how it's super cool„ÄÇAnd then Sterling Anderson„ÄÇ

Who was the director previously Al Tesla Autopilo team and now is the cofounder of Aurora„ÄÇ

The the self drivingriving car„ÄÇStartup that I mentioned that has now partnered with Nvidia and many others„ÄÇ

 So why self driving cars„ÄÇThis class is about applying data driven learning methods to the problem of autonomous vehicles„ÄÇ

Why self driving cars are fascinating and an interesting problem space„ÄÇQuite possiblyÔºå in my opinion„ÄÇ

 this is the first wide reaching and profound integration of personal robots in society„ÄÇ

Wide reachingÔºå because there's 1 billion cars on the roadÔºå Even a fraction of that will change„ÄÇ

The face of transportation and how we move about this world„ÄÇProfound„ÄÇ

 and this is an important point that's not always understood„ÄÇIs there is an intimate„ÄÇ

Connection between a human and a vehicle when there's a direct transfer of control„ÄÇ

 it's a direct transfer of control that takes that his or her life into the hands of an artificial intelligence system„ÄÇ

I showed a few click„ÄÇQu clips here„ÄÇ You can Google first time with Tesla Autopilot on YouTube and watch people perform that transfer of control„ÄÇ

 There's something magical about a human and a robot working together„ÄÇüòä„ÄÇ

That will transform what artificial intelligence is in the 21st century„ÄÇ

And this particular autonomous system„ÄÇAI systemÔºå cell driving cars„ÄÇ

Is on the scale and profound the life critical nature of it is profound in a way that it will truly test the capabilities of AI„ÄÇ

There is a personal connection that will argue throughout these lectures that we cannot escape considering the human being„ÄÇ

That autonomous vehicle must not only perceive and control its movement through the environment„ÄÇ

 It must also perceive everything about the human driver and the passenger and interact„ÄÇ

 communicate and build trust with that driver„ÄÇ

![](img/a98e904adf8b8abf573097b7201b7b40_7.png)

Because„ÄÇIn my viewÔºå as I will argue throughout this course„ÄÇ

 an autonomous vehicle is more of a personal robot than it is a perfect perception control system because perfect perception and control to this world full of humans„ÄÇ

Is extremely difficult and could be 2Ôºå3Ôºå4 decades away„ÄÇFull autonomy„ÄÇ

Autonomous vehicles are going to be flawedÔºå They're going to have flaws„ÄÇ

 and we have to design systems that are effectively caught that effectively transfer control to human beings when they can't handle the situation„ÄÇ

 And that transfer of control„ÄÇIs a fascinating opportunity for AI„ÄÇBecause„ÄÇThe obstacle avoidance„ÄÇ

 perception„ÄÇOf obstacles and obstacle avoidance is the easy problem„ÄÇ

It's the safe problem going 30 miles an hourÔºå navigating through streets of Boston„ÄÇIs easy„ÄÇ

It's when you have to get to work and you're late„ÄÇOr you're sick of the person in front of you that you want to go in the opposing lane and speed up„ÄÇ

That's human natureÔºå and we can't escape it„ÄÇOur artificial intelligence systems can't escape human nature„ÄÇ

 they must work with it„ÄÇWhat's shown here is one of the algorithms we'll talk about next week for cognitive load„ÄÇ

Where we take the raw 3D convolution neural networksÔºå take in the eye region„ÄÇ

 the blinking and the pupil movement to determine the cognitive load of the driver„ÄÇ

 we'll see how we can detect everything about the driverÔºå where they're lookingÔºå emotion motion„ÄÇ

 cognitive loadÔºå body pose estimation„ÄÇDrowsiness„ÄÇTheÔºå the„ÄÇ

 the movement towards full autonomy is so difficult„ÄÇ

I would argue that it almost requires human level of intelligence„ÄÇThat the„ÄÇAs I saidÔºå2Ôºå3„ÄÇ

 four decade out journey for artificial intelligence researchers to achieve full autonomy will require achieving„ÄÇ

 solving some of the problemsÔºå fundamental problems of creating intelligence„ÄÇAnd„ÄÇ

That's something we'll discuss in much more depth in a broader view in two weeks for the artificial general intelligence course„ÄÇ

Well we have Andrei Corpay from TeslaÔºå Ray KurzweilÔºå Mark Reyberg„ÄÇFrom Boston Dynamics„ÄÇ

Who asks for the dimensions of this room because he's bringing robots„ÄÇNothing else was told to me„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_9.png)

It'll be a surprise„ÄÇSo that is why I argue the human centered artificial intelligence approach„ÄÇ

In every algorithm of the design considers the human„ÄÇFor autonomous vehicle on the left„ÄÇ

 the perceptionÔºå sceneÔºå understanding and the control problem„ÄÇ

 as we'll explore through the competitions and the assignments of this course„ÄÇ

Can handle 90 and increasing percent of the casesÔºå but it's the 101„ÄÇ

1% of the cases as we get better and better that we have to„ÄÇ

 we're not able to handle through these methods„ÄÇ And that's where the human perceiving the human is really important„ÄÇ

 This is the video from last year„ÄÇOf Arc de Triomph„ÄÇ Thank you„ÄÇ I didn't know it last year„ÄÇ

 I know now„ÄÇThat is one of millions of cases where human to human interaction„ÄÇ

Is the dominant driver not the basic perception control problemÔºüSo why deep learning in this spaceÔºü

Because deep learning is a set of methods„ÄÇThat do well from a lot of data„ÄÇ

And to solve these problems where human life is at stake„ÄÇ

 we have to be able to have techniques that learn from dataÔºå learn from real world data„ÄÇ

 This is the fundamental reality of artificial intelligence systems that operate in the real world„ÄÇ

 They must learn from real world data„ÄÇWhether that's un left for the perceptionÔºå the control side„ÄÇ

Or on the right for the humanÔºå the perception and the communication„ÄÇ

 interaction and collaboration with the human and the human robot interaction„ÄÇOkay„ÄÇ

So what is deep learningÔºüIt's a set of techniques„ÄÇ If you allow me the definition of intelligence being the ability to accomplish complex goals„ÄÇ

Then I would argue„ÄÇDefinition of understanding may be reasoning„ÄÇ

Is the ability to turn complex information into simpleÔºå usefulÔºå actionable information„ÄÇ

And that is what deep learning does„ÄÇDeep learning is representation learning„ÄÇOr feature learning„ÄÇ

 if you will„ÄÇIt's able to take raw information„ÄÇRaw„ÄÇ

 complicated information that's hard to do anything with and construct hierarchical representations of that information to be able to do something interesting with it„ÄÇ

It is the branch of artificial intelligenceÔºå which is most capable and focused on this task„ÄÇ

Forming representations from dataÔºå whether it's supervised or unsupervised„ÄÇ

 whether it's with the help of humans or notÔºå it's able to construct„ÄÇStructure„ÄÇ

 find structure in the data such that you can„ÄÇExtract simpleÔºå usefulÔºå actionable information„ÄÇ

On the left„ÄÇFor me in Goodfe's book„ÄÇIs the basic example of image classification„ÄÇ

The input of the image„ÄÇOn the bottom were the raw pixelsÔºå and as we go up the stack„ÄÇ

 as we go up the layersÔºå higher and higher order representations are formed„ÄÇ

From edges to contours to cornersÔºå to object parts„ÄÇ and then finally„ÄÇ

 the full object semantic classification of what's in the image„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_11.png)

![](img/a98e904adf8b8abf573097b7201b7b40_12.png)

This is representation learning„ÄÇA favorite example for me„ÄÇIs„ÄÇOne from four centuries ago„ÄÇ

Our place in the universe„ÄÇAnd representing that place in the universe„ÄÇ

 whether it's relative to Earth or relative to the sun„ÄÇOn the left is our current belief„ÄÇ

 On the right is the one that is held widely four centuries ago„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_14.png)

![](img/a98e904adf8b8abf573097b7201b7b40_15.png)

Representation matters because what's on the right is much more complicated than what's on the left„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_17.png)

![](img/a98e904adf8b8abf573097b7201b7b40_18.png)

You can think of in a simple case here when the task is to draw a line that separates green triangles in blue circles in the Cartesian coordinate space on the left„ÄÇ

 the task is much more difficultÔºå impossible to do well„ÄÇ

 On the right is trivial in polar coordinates„ÄÇThis transformation is exactly which we need to learn„ÄÇ

 This is representation learning„ÄÇSo you can take the same task of having to draw a line that separates the blue curve and the red curve on the left„ÄÇ

If we draw a straight lineÔºå it's going to be a high„ÄÇ

 there's no way to do it with zero error with 100% accuracy„ÄÇShown on the right is our best attempt„ÄÇ

But what we can do with deep learning with a single hidden layer network done here„ÄÇIts form the„ÄÇ

 the topologyÔºå the mapping of the space in such a way in the middle that allows for a straight line to be drawn that separates the blue curve and the red curve„ÄÇ

The learning of the function in the middle„ÄÇIs what we're able to achieve with deep learning„ÄÇ

It's taking rawÔºå complicated information and making it simple„ÄÇActionableÔºå useful„ÄÇ

And the point is that this kind of ability to learn from raw sensor information means that we can do a lot more with a lot more data„ÄÇ

So deep learning gets better with more data„ÄÇ

![](img/a98e904adf8b8abf573097b7201b7b40_20.png)

And that's important for real world applications„ÄÇWhere edge cases are everything„ÄÇ

This is us driving with two perception control systems„ÄÇ One is in Tesla vehicle with the autopilot„ÄÇ

Version 1 system that's using a monocular camera to perceive the external environment and produce control decisions„ÄÇ

 and our own neural network running on the Jescent T X 2 that's taking in the same with a monocular camera and producing control decisions„ÄÇ

And the two systems argueÔºå and when they disagreeÔºå they raise up a flag to say that this is an edge case that needs human intervention„ÄÇ

There is covering such edge cases using machine learning„ÄÇIs the main problem„ÄÇ

 of artificial intelligence and when applied to the real worldÔºå it is the main problem to solve„ÄÇOkay„ÄÇ

 so what are neural networks„ÄÇInspired very loosely„ÄÇ

 And I'll discuss about the key difference between our own brains and artificial brains„ÄÇüòä„ÄÇ

Because there's a lot of insights in that difference„ÄÇ

But inspired loosely by the biological neural networks here is a simulation of a thalamoccortical brain network„ÄÇ

 which is only3 million neurons„ÄÇ476 million synapses„ÄÇ The full human brain is a lot more than that„ÄÇ

100 billion neurons„ÄÇ1000 trillion„ÄÇSynapses„ÄÇ

![](img/a98e904adf8b8abf573097b7201b7b40_22.png)

There's an inspirational music with this one that I didn't realize was here„ÄÇIt should make you think„ÄÇ

Artificial neural networksÔºå okayÔºå let's just it play„ÄÇüòä„ÄÇ

The human neural network is 100 billion neuronsÔºå rightÔºå 1000 trillion synapses„ÄÇ

One of the state of the art neural networks is ResNe 152Ôºå which has 60 million synapses„ÄÇ

That's a difference„ÄÇOf about a seven order of magnitude difference„ÄÇ

 the human brains have 10 million times more synapses than artificial neural networks„ÄÇPlus„ÄÇ

 or minus one order of magnitudeÔºå depending on the network„ÄÇ

So what's the difference between a biological neuron and artificial neuronÔºü

The topology of the human brain have no layers„ÄÇ Ne networkss are stacked in layers„ÄÇ They're fixed„ÄÇ

 for the most part„ÄÇThere is chaosÔºå very little structure in our human brain in terms of how neurons are connected„ÄÇ

 they're connected often to 10Ôºå000 plus other neurons„ÄÇ

 The number of synapses from individual neurons that are input into the neuron is huge„ÄÇ

They're asynchronous„ÄÇ The human brain works asynchronously„ÄÇ

 artificial neural networks work synchronously„ÄÇThe learning algorithm for artificial neural networks„ÄÇ

 the only one„ÄÇThe best one„ÄÇIs back propagation„ÄÇAnd we don't know„ÄÇHow human brains learn„ÄÇ

Processing speed„ÄÇThis is one of the„ÄÇThe only benefits we have with artificial neural networks is artificial neurons are faster„ÄÇ

But they're also extremely power and efficient„ÄÇAnd„ÄÇ

There is a division into stages of training and testing with neural networks with biological neural networks„ÄÇ

 as you're sitting here todayÔºå they're always learning„ÄÇThe only profound similarity„ÄÇ

The inspiring one„ÄÇThe captivating one is that both are distributed computation at scale„ÄÇ

There is an emergent„ÄÇAspect to neural networks where the basic element of computationÔºå a neuron„ÄÇ

Is simpleÔºå is extremely simple„ÄÇ But when connected togetherÔºå beautifulÔºå amazing„ÄÇüòä„ÄÇ

Powerful approximators can be formed„ÄÇA neural network is built up with these computational units where the inputs„ÄÇ

 there's a set of edges with weights on them„ÄÇThesThe weights are multiplied by this input signal„ÄÇ

A bias is added with a nonlinear function that determines whether the network gets activated or not„ÄÇ

 WellÔºå the neuron gets activated or not visualized here„ÄÇ

And these neurons can be combined in a number of ways„ÄÇ They can form a feed forward neural network„ÄÇ

 or they can„ÄÇFeed back into itself to form„ÄÇTo have state memory„ÄÇIn recurrent neural networks„ÄÇ

 the ones on the left„ÄÇAre the ones that are most successful for most applications in computer visionÔºü

The ones in the right are very popular and specific when temporal dynamics or dynamics time series of any kind are used„ÄÇ

 In factÔºå the ones in the right are much closer to the way our human brains are than the ones on the left„ÄÇ

But that's why they're really hard to train„ÄÇOne beautiful aspect„ÄÇ

Of this emergent power for multiple neurons being connected together is the universal property that with a single hidden layer„ÄÇ

 these networks„ÄÇCan learn any function„ÄÇ Learn to approximate any function„ÄÇ

Which is an important property to be aware of„ÄÇBecause„ÄÇ

The limits here are not in the power of the networks„ÄÇ

 The limits in the is in the methods by which we construct them and train them„ÄÇ

What kinds of machine learningÔºå deep learning are thereÔºüWe can separate it into two categories„ÄÇ

Memorizers„ÄÇNow the approaches that essentially memorize patterns in the data„ÄÇ

And approaches that we can loosely say are beginning to reason to generalize over the data with minimal human input on top on the left are the quote unquote„ÄÇ

  teachersÔºå is how much human input in blue is needed to make the methods successful for supervised learning„ÄÇ

 which is what most of deep learning successes come from„ÄÇ

 where most of the data is annotated by human beings„ÄÇThe human is at the core of the success„ÄÇ

 most of the data that's part of the training needs to be annotated by human beings„ÄÇ

With some additional successes coming from augmentation methods that extend that„ÄÇ

Extend the data based on which these networks are trained„ÄÇ

And the semiupvised reinforcement learning and unsupervised methods that we'll talk about later in the course„ÄÇ

That's where the near term successes we hope are„ÄÇ And with the unsupervised learning approaches„ÄÇ

 that's where the true excitement about the possibilities of artificial intelligence lie„ÄÇ

 being able to make sense of our world with minimal input from humans„ÄÇ

So we can think of two kinds of„ÄÇDeep learning impact spaces„ÄÇ

 One is a special purpose intelligence is taking a problemÔºå formalizing it„ÄÇ

 collecting enough data on it and being able to solve a particular„ÄÇCase that provides value„ÄÇ

Of particular interest here is a network that estimates apartment costs in the Boston area so you could take the number of bedrooms„ÄÇ

 the square feet in the neighborhood and provide as output the estimated cost„ÄÇ

On the right is the actual data„ÄÇOf apartment cost„ÄÇ We're actually standing in a„ÄÇ

In an area that has over $3000 for a studio apartment„ÄÇSome of you may be feeling that pain„ÄÇ

And then there's general purpose intelligence„ÄÇOr something that feels like approaching general purpose intelligence„ÄÇ

 which is reinforcement and unsupervised learning„ÄÇHere from Madri Apathhi's Pong the Pixels„ÄÇ

 a system that takes in 80 by 80 pixel image and when no other information is able to beat„ÄÇ

 is able to win at this gameÔºå no information except a sequence of images„ÄÇ

 raw sensory information the same wayÔºå the same kind of information that human beings take in from the visual audio touch sensory data„ÄÇ

 the very low level data and be able to learn to win in this very simplistic in this very artificially constructed world„ÄÇ

 but nevertheless a world where no feature learning is performed„ÄÇ

 only raw sensory information is used to win„ÄÇWith very sparse minimal human input„ÄÇ

We'll talk about that on Wednesday„ÄÇWith deep reinforcement learning„ÄÇSoÔºå but for now„ÄÇ

 we'll focus on supervised learning„ÄÇWhere there is input data„ÄÇ

 there is a network we're trying to train„ÄÇA learning system„ÄÇ

 and there's a correct output that's labeled by human beings„ÄÇ

That's the general training process for a neural networkÔºå input data„ÄÇ

 labels and the training of that network that model„ÄÇ

 so that in the testing stage on new input data that has never seen before it's tasked with producing guesses and is evaluated based on that„ÄÇ

For autonomous vehiclesÔºå that means being released either in simulation or in the real world to operate„ÄÇ

And how they learn how neural networks learn is given the forward path of taking the input data„ÄÇ

 whether it's from the training stage„ÄÇIn the training stageÔºå the taking the input data„ÄÇ

 producing a predictionÔºå and then given that there's ground truth in the training stage„ÄÇ

 we can have a measure of error based on a loss function that then punishesÔºåThe synapses„ÄÇ

 the connectionsÔºå the parameters that were involved with making that wrong prediction„ÄÇ

And it back propagates the error„ÄÇThrough those weights„ÄÇ

 we'll discuss that in a little bit more detail in a bit here„ÄÇSo what can we do with deep learning„ÄÇ

 You can do one to one mapping„ÄÇReallyÔºå you can think of input as being anything„ÄÇ

 it can be a number of vector numbersÔºå a sequence of numbersÔºå a sequence of vector of numbers„ÄÇ

 anything you can think of from images to video to audio to text can be represented in this way„ÄÇ

 and the output can the same be a single numberÔºå or it can be imagesÔºå video textÔºå audio„ÄÇ

1 to one mapping on the bottomÔºå one to manyÔºå many to oneÔºå many to many„ÄÇ

 and many to many with different starting points for the data„ÄÇAsynchronous„ÄÇ

Some quick terms that will come upÔºå deep learning is the same as neural networks„ÄÇ

It's really deep neural networksÔºå large neural networks„ÄÇ

It's a subset of machine learning that has been extremely successful in the past decade„ÄÇ

Multilayer perceptronÔºå deep neural networkÔºå recurrent neural networkÔºå long short term memory network„ÄÇ

 LSTMÔºå convolution neural networkÔºå and deep belief networks„ÄÇ

 all of these will come up through the slides„ÄÇAnd there is specific operations„ÄÇ

 layers within these networks of convolutionÔºå poolingÔºå activation and back propagation„ÄÇ

 this concept that we'll discuss„ÄÇIn this class„ÄÇActivation functions„ÄÇ There's a lot of variance„ÄÇ

On the left is the activation function in the left column and the x axis is the input and the y axis is the output„ÄÇ

The sigmoid functionÔºå the output„ÄÇIf the font is too smallÔºå the output is not centered at zero„ÄÇ

For the 10 H functionÔºå it's centered at zeroÔºå but it still suffers from vanishings„ÄÇ

Danished ingredients is when the valueÔºå the input is low or high„ÄÇTheÔºå the output of the network„ÄÇ

 as you see in the right column thereÔºå the derivative of the function is very low„ÄÇ

 So the learning rate is very low„ÄÇFor rail you„ÄÇIt's also not zero centered„ÄÇ

 but it does not suffer from vanished ingredients„ÄÇBack propagation is the process of learning„ÄÇ

 it's the way we take go from errorÔºå compute is the loss function on the bottom right of the slide„ÄÇ

 taking the actual output of the network with a forward passÔºå subtracting it from the ground truth„ÄÇ

 squarearing dividing way to and using that loss function„ÄÇ

 then back propagate through to construct a gradient to backproagate the error to the weights that we're responsible for making either a correct or an incorrect decision„ÄÇ

So the subasks to thatÔºå there's a forward passÔºå there's a backward pass„ÄÇ

 and a fraction of the weight's gradient subtracted from the weightÔºå that's it„ÄÇ

That process is modularÔºå so it's local to each individual neuron„ÄÇ

 which is why it's extremely we're able to distribute it across multiple„ÄÇAcross the GPU„ÄÇ

 parallellyzed across the GPU„ÄÇSo learning for a neural network„ÄÇ

These competition units are extremely simple„ÄÇ They're extremely simple to then correct when they make an error„ÄÇ

 when they're part of a larger network that makes an error„ÄÇ

And all that boils down to is essentially an optimization problem where the objective utility function is the loss function and the goal is to minimize it„ÄÇ

 and we have to update the parametersÔºå the weights and the synapses and the biases to decrease that loss function„ÄÇ

And that loss function is highly nonlinear„ÄÇDepending on the activation functions„ÄÇ

 different propertiesÔºå different issues ariseÔºå there's vanish ingredientss„ÄÇFor sigmoid„ÄÇ

Where the learning can be slow„ÄÇThere's dying ras„ÄÇWhere the derivative is exactly zero„ÄÇFor inputs„ÄÇ

 less than  zero„ÄÇThere are solutions to this like leak Urales and a bunch of details that you may discover when you try to win the deep traffic competition„ÄÇ

But for the most partÔºå these are the main activation functions„ÄÇAnd it's the choice of the„ÄÇ

Neo network designerÔºå which one works best„ÄÇTheir saddle points„ÄÇ

 all the problems from numerical nonlinear optimization that arise come up here„ÄÇ

It's hard to break symmetry„ÄÇAnd stochastic gradient descent without any kind of tricks to it„ÄÇ

Can take a very long time to arrive at the minimumma„ÄÇ

One of the biggest problems in all the machine learning and certainly deep learning is overfitting„ÄÇ

You can think of the blue dots in a plot here„ÄÇAs the data to which we want to fit a curve„ÄÇ

We want to design a learning system that approximates to the regression of this data„ÄÇ

So in green is a sign curveÔºå simpleÔºå fits well„ÄÇAnd then there's a ninth degree polynomial„ÄÇ

 which fits even better in terms of the errorÔºå but it clearly overfits this data if there's other data„ÄÇ

That it has not seen yet that it has to fit„ÄÇIt's likely to produce a high error„ÄÇ

 So it's overfitting the training set„ÄÇThis is a big problem for small data sets„ÄÇ

And so we have to fix that with regularization„ÄÇ Regularization is a set of methodologies that prevent overfitting„ÄÇ

Learning the training too well in order and then to not be able to generalize to the testing stage„ÄÇ

And overfitting the main symptom is the error decreases in training set„ÄÇ

 but increases in the test set„ÄÇSo there's a lot of techniques in traditional machine learning that deal with this cross validations and so on„ÄÇ

 but because of the cost of training for neural networks„ÄÇ

It's traditional to use what's called a validation set„ÄÇ

So you create a subset of the training that you keep away for which you have the ground truth and use that as a representative of the testing set„ÄÇ

Perform early stoppage or more realistically just save a checkpoint often„ÄÇ

To see how as the training evolves„ÄÇThe performance changes on the validation set„ÄÇ

 and so you can stop when the performance in the validation set is getting a lot worse„ÄÇ

 it means you're overtraining on the training set„ÄÇIn practiceÔºå of course„ÄÇ

 we run training much longer and see when what is the best performing what is the best performing snapshot checkpoint of the network„ÄÇ

Dropout is another very powerful regularization technique where we randomly remove part of the network„ÄÇ

 randomly remove some of the nodes in the network„ÄÇAlong with its incoming and outgoing edges„ÄÇ

So what that really looks like is a probability of keeping a node„ÄÇ

And in many deep learning frameworks today„ÄÇIt comes with a dropout layer„ÄÇ

 So it's essentially a probability that's usually greater than 0„ÄÇ5 that a node will be kept„ÄÇ

For the input layerÔºå the probability should be much higher or more effectively what works well is just adding noise„ÄÇ

 What's the point hereÔºå you want to create enough diversity in the training data such that it is generalizable to the testing„ÄÇ

And as you'll see with deep traffic competitionÔºå there's L2 and L1 penalty„ÄÇWeightÔºå decay„ÄÇ

 weight penalty„ÄÇWhere there's a penalization on the weights that get too large„ÄÇ

 the L2 penalty keeps the weights small unless the air derivative is huge„ÄÇ

And produces a smoother model and prefers to distribute when there is two similar inputs it prefers to put half the weights on each„ÄÇ

 distribute the weights as opposed to putting the weight on one of the edges„ÄÇ

Makes the network more robust„ÄÇone penalty has the one benefit that for really large weights„ÄÇ

 they're allowed to stay„ÄÇ So it allows for a few weights to remain very large„ÄÇ

These are the regularization techniquesÔºå and I wanted to mention them because they're useful to some of the competitions here in the course„ÄÇ

And I recommend to go to to TensorFlow playground„ÄÇTo play around with some of these parameters„ÄÇ

Where you get to online in the browserÔºå play around with different inputsÔºå different features„ÄÇ

 different number of layers and regularization techniques„ÄÇ

And to build your intuition about classificationÔºå regression problems„ÄÇ

 given different input data sets„ÄÇSo what changedÔºå why over the past many decadesÔºü

Neural networks that have gone through two winters are now again dominating the artificial intelligence community„ÄÇ

CPUsÔºå GPUs„ÄÇAicsÔºå so computational power has skyrocketed„ÄÇFrom Moore's Law to GPUs„ÄÇ

There is huge data setÔºå including IageNe and others„ÄÇThere is researchÔºå bad propagation„ÄÇIn the 80s„ÄÇËØ∂„ÄÇ

The convolutional neural networksÔºå LSTMsÔºå there's been a lot of interesting breakthroughs about how to design these architectures„ÄÇ

 how to build them such that they're trainable efficiently using GPUs„ÄÇ

There is the software infrastructure from being able to share the data Gi„ÄÇ

To being able to train networks and share code and effectively view neural networks as a stack of layers as opposed to having to influence stuff from scratch with Tensorflow pietorrch and other deep learning frameworks and there's huge financial backing from Google„ÄÇ

 Facebook and so on„ÄÇDeep learning„ÄÇIs„ÄÇIn order to understand why„ÄÇ

It works so well and where its limitations areÔºå we need to understand where our own intuition comes from about what is hard and what is easy„ÄÇ

The important thing about computer visionÔºå which is a lot of what this course is about„ÄÇ

 even it's in deeper reinforcement learning formulation„ÄÇ

Is that visual perception for us human beings was formed 540 million years ago„ÄÇ

 that's 540 million years worth of data„ÄÇAn abstract thought is only formed about 100000 years ago„ÄÇ

That's several orders of magnitudeÔºå less data„ÄÇSo we can make with neural networksÔºå predictions„ÄÇ

That seemed trivial„ÄÇThe the trivial to us human beings„ÄÇ

But completely challenging and wrong to neural networks„ÄÇ

 here on the left showing a prediction of a dog with a little bit of a distortion of noise added to the image„ÄÇ

 producing the image on the rightÔºå a neural network is confidently 99% plus accuracy predicting that it's an ostrich„ÄÇ

And there's all these problems that has to deal withÔºå whether it's in computer vision data„ÄÇ

 whether it's in text dataÔºå audioÔºå all of this variation arises in vision„ÄÇ

 its illumination variability„ÄÇThe set of pixels and the numbers look completely different„ÄÇ

 depending on the lighting conditionsÔºå it's the biggest problem in driving is lighting conditions„ÄÇ

 lighting variabilityÔºå pose variation objects need to be learned from every different perspective„ÄÇ

 I'll discuss that for when sensing the driverÔºå most of the deep learning work that's done on the face on the human is done on the frontal face or semifrontal face„ÄÇ

 there's very little work done on the full 360 pose variability that a human being can take on„ÄÇ

Intraclass variability for the classification problemÔºå for the detection problem„ÄÇ

 there is a lot of different kinds of objects for catsÔºå dogsÔºå carsÔºå bicyclistsÔºå pedestrians„ÄÇ

So that brings us to object classificationÔºå and I'd like to take you through where deep learning has taken big strides for the past several years leading up to this year to 2018„ÄÇ

So let's start at object classificationÔºå is' when you take a single image and you have to say one class that's most likely to belong in that image„ÄÇ

The most famous variant to that is the Inet competitionÔºå INe challengellge„ÄÇ

 the Inet data set is a data set of 14 million images with 21„ÄÇ

000 categories and for say the category of fruitÔºå there's a total of 188„ÄÇ

000 images of fruit and there is 1Ôºå200 images of Grnny Smith apples„ÄÇ

 it gives you a sense of what we're talking about here„ÄÇ

So this has been the source of a lot of interesting breakthroughs in deep learning and a lot of the excitement in deep learning is first„ÄÇ

 the big successful networkÔºå at least„ÄÇOne that became famous in deep learning is Alex in 2012 that took a leap of a significant leap in performance on the Inet challengellge„ÄÇ

So he was one of the first neural networks that was successfully trained on the GPU and achieved an incredible performance boost over the previous year on the Inet Challge„ÄÇ

 the challenge isÔºå and I'll talk about some of these networks is to give in a single image„ÄÇ

 give five guesses and you have five guesses to guess for one of them to be correct„ÄÇ

The human annotation is the question often comes upÔºå so how do you know the ground truthÔºü

Human level performance is 5„ÄÇ1% accuracy on this task„ÄÇ

But the way the annotation for IageNet is performed is there's a Google search where you pull the images already labeled for you„ÄÇ

 and then the annotation that on mechanical TurkÔºå other humans perform is just binary„ÄÇ

 is this a cat or not a catÔºüSo they're not tasked with performing the very high resolution semantic labeling of the image„ÄÇ

OkayÔºå so through from 2012 with Alex andette to today„ÄÇ

And the big transition in 2018 of the Inet challengellge leaving Stanford and going to Kegel„ÄÇ

It's sort of a monumental step because in 2015 with the Reant network was the first time that the human level of performance was exceeded„ÄÇ

And I think this is„ÄÇA very important„ÄÇMap of where deep learning isÔºå for particular„ÄÇ

 what I would argue is a toy exampleÔºå despite the fact that it's 14 million images„ÄÇ

So we're developing state of ER techniques here and the next stage„ÄÇ

 as we are now exceeding human level performance and this task is how to take these methods into the real world to perform scene perception„ÄÇ

 to perform driver state perception„ÄÇIn 2016 and 2017„ÄÇ

 CU Image and SCNe has a very unique new addition to the previous formulations that has achieved an accuracy of 2„ÄÇ

2% error„ÄÇ2„ÄÇ25% error on the Inet classification challengeen is an incredible result„ÄÇOkay„ÄÇ

 so you have this image classification architecture that takes in a single image and produces convol and takes it through pooling convolution„ÄÇ

 and at the end fully connected layers and performs a classification task or regression task and you can swap out that layer to perform any kind of other task„ÄÇ

 including with the recur neural networks of image captioning and so on„ÄÇ

 or localization of bonding boxesÔºå or you can do„ÄÇFully convolution networksÔºå which we'll talk about„ÄÇ

On Thursday„ÄÇWhich is when you take a image as an input and produce an image as an output„ÄÇ

 but where the output image in this case is a segmentation„ÄÇ

Is where a color indicates what the object isÔºå the category of the object„ÄÇ

 So it's pixel level segmentationÔºå every single pixel in the image is assigned a class„ÄÇ

 a category where that pixel belongs to„ÄÇThis is the kind of task that's overlaid on top of other sensor information coming for the car in order to„ÄÇ

Perceive the external environment„ÄÇYou can continue to extract information from images in this way to produce image to image mapping„ÄÇ

 for exampleÔºå to colorize images and take from grayscale images to color images„ÄÇ

Or you can use that kind of heat map information to localize objects in the image„ÄÇ

 So as' as opposed to just classifying that this is an image of a cowÔºå RCNN„ÄÇ

 fast and faster RCNN and a lot of other localization networks„ÄÇ

Allow you to propose different candidates for where exactly the cow is located in this image and thereby being able to perform object detection„ÄÇ

 not just object classification„ÄÇIn 2017Ôºå there's been a lot of cool applications of these architectures„ÄÇ

 one of which is back on removal„ÄÇAgainÔºå mapping from image to image„ÄÇ

 ability to remove an background from selfies„ÄÇOf humans or human like„ÄÇPictures„ÄÇfaceaces„ÄÇ

The references with some incredible animations are in the bottom of the slide„ÄÇ

 and the slides are now available online„ÄÇ

![](img/a98e904adf8b8abf573097b7201b7b40_24.png)

Picks to picksÔºå HD„ÄÇThere's been a lot of work in Gs„ÄÇIn generative artsarial networks„ÄÇ

 in particular in driving„ÄÇGNs have been used to generate examples that generate examples from source data„ÄÇ

 whether that's from raw dataÔºå or in this caseÔºå with Ps to Ps„ÄÇ H D is taking„ÄÇ

Carse semantic labeling of the imagesÔºå pixel levelÔºå and producing photoreisticÔºå high definition„ÄÇ

Images of the forward roadway„ÄÇ This is an exciting„ÄÇ

Possibility for being able to generate a variety of cases for cell driving cars for autonomous vehicles to be able to learn„ÄÇ

 to generateÔºå to augment the dataÔºå and be able to change the way different roads look„ÄÇ

 road conditionsÔºå to change the way vehicles look cyclists pedestrians„ÄÇ



![](img/a98e904adf8b8abf573097b7201b7b40_26.png)

Then we can move on to recurringcurren neural networks„ÄÇ

 Everything I've talked about was one to one mapping from image to image or image to number„ÄÇ

 recur neural networks work with sequences„ÄÇWe can use sequences„ÄÇTo generate handwriting„ÄÇ

To generate text captions from an image„ÄÇBased on the localizations„ÄÇ

 the various detections in that image„ÄÇWe can provide video description generation„ÄÇ So taking a video„ÄÇ

And combining convolution neural networks with recurrent neural networks„ÄÇ

 using convolution neural networks to extract features frame to frame„ÄÇ

 and using those extractive features to input into the RNNs to then generate a labeling„ÄÇ

A description of what's going on in the video„ÄÇA lot of exciting approaches for autonomous systems„ÄÇ

 especially in dronesÔºå where the time to make a decision„ÄÇIs short„ÄÇ

 same with the RC car traveling 30 miles an hourÔºå attentional mechanisms for steering the attention of the network have been very popular for the localization task and for just saving how much interpretation of the image„ÄÇ

 how many pixels need to be considered in the classification task„ÄÇSo we can steer„ÄÇ

 we can model the way a human being looks around an image to interpret it and use the network to do the same„ÄÇ

And we can use that kind of steering„ÄÇTo draw images as well„ÄÇFinally„ÄÇ

 the big breakthroughs in 2017 came from„ÄÇThisÔºå the pong pixelsÔºå the reinforcement learning„ÄÇ

 using sensory dataÔºå raw sensory dataÔºå and use reinforcement learning methods„ÄÇ

 deep are all methods of which you'll talk about on Wednesday„ÄÇ

 I'm really excited about the underlying methodology of deep traffic and deep crash is using neural networks„ÄÇ

As the appromators inside reinforcement learning approaches„ÄÇ

 So Alpha Go in 2016 has achieved a monumental task„ÄÇ

That when I first started artificial intelligence was told to me it was impossible for the asked them to accomplish„ÄÇ

 which is to win at the game of go against the top human player in the world„ÄÇHowever„ÄÇ

 that method was trained on human expert positions„ÄÇ

The AlGo system was trained on previous games played by human experts„ÄÇ

And an incredible accomplishment„ÄÇAlpha goes  zero in 2017„ÄÇ

It was able to beat alphaphago and many of its variants„ÄÇBy playing itself„ÄÇFrom zero information„ÄÇ

So no knowledge of human experts„ÄÇNo gamesÔºå no training dataÔºå very little human input„ÄÇ

And what more it was able to generate moves that were surprising to human experts„ÄÇ

I think its Einstein that said that intelligence„ÄÇThat the key mark of intelligence is imagination„ÄÇ

I think it's beautiful to see an artificial intelligence system come up with something that surprises human experts„ÄÇ

Truly surprises„ÄÇFor the gambling junkiesÔºå deep stack and a few other variants„ÄÇ

Had been used in 2017 to win a heads up poker„ÄÇ AgainÔºå another incredible result„ÄÇ

 I was always told artificial intelligence would be impossible for any machine learning method to achieve„ÄÇ

And was able to beat a professional playerÔºå and several competitors have come along since„ÄÇ

We're yet to be able to win in a tournament settingÔºå so multiple players„ÄÇ

 for those of you familiar heads up poker is one on oneÔºå it's a muchÔºå much smaller„ÄÇ

 easier space to solve„ÄÇThere's a lot more humidity humid dynamics going on from when there's multiple players„ÄÇ

 but that's the task for 2018„ÄÇAnd the drawbacks is one of my favorite videosÔºåüòäÔºåShow it often„ÄÇ

Of coast runners„ÄÇFor these deep reinforcement learning approachesÔºå the„ÄÇ

Learning of the reward functionÔºå the definition of the reward function change controls how the actual„ÄÇ

System behavesÔºå and this will come„ÄÇThis would be extremely important for us with autonomous vehicles„ÄÇ

Here the boat is tasked with gaining the highest number of points„ÄÇ

And it figures out that it does not need to raceÔºå which is the whole point of the game in order to gain points„ÄÇ

 But insteadÔºå pick up green„ÄÇCircleles that regenerate themselves over and over„ÄÇ

This is the counterintuitive„ÄÇBehavior of a system that„ÄÇ

Would not be expected when you first designed the reward function„ÄÇ And this is a very formal„ÄÇ

 simple system„ÄÇ NeverthelessÔºå is extremely difficult to come up with a reward function that makes it operate in the way you expect it to operate„ÄÇ

 Very applicable for„ÄÇAutomous vehicless„ÄÇ AndÔºå of courseÔºå in the perception side„ÄÇ

 as I mentioned with the ostrich and the dog„ÄÇA little bit of noise with 99„ÄÇ6% confidence„ÄÇ

 we can predict that the noise up top is a RobinÔºå a cheetahÔºå Ar modilloÔºå Leserpanda„ÄÇ

 these are outputs from actual state of the art and neural networks„ÄÇ

Taking in the noise and producing a confident prediction„ÄÇ

It should build our intuition to understand that we don'tÔºå that the visual characteristics„ÄÇ

 the spatial characteristics of an image do not necessarily convey the level of hierarchy necessary to function in this world„ÄÇ

In a similar wayÔºå with a dog in the ostrich and everything in an ostrich„ÄÇ

 a network confidently with a littleer of noise can make the wrong prediction„ÄÇ

Thinking a school bus is an ostrich and a speaker is an ostrich„ÄÇThey're easily fooled„ÄÇBut not really„ÄÇ

 because they performed the task that they were trained to do well„ÄÇSoÔºå we have to„ÄÇ

Make sure we keep our intuition„ÄÇOptimized to the way machines learnÔºå not the way humans have learned„ÄÇ

Over the 540 million years of data that we've gained through developing the eye the revolution„ÄÇ

The current challenges we're taking onÔºå firstÔºå transfer learning„ÄÇ

There is a lot of success in transfer learning between domains that are very close to each other„ÄÇ

 so image classification from one domain to the next„ÄÇ

There's a lot of value forming representations of the way scenes look in natural scenes look in order to do scene segmentation„ÄÇ

 the driving caseÔºå for exampleÔºå but we're not able to do any„ÄÇ

Any bigger leaps in the way we perform transfer learningÔºü

The biggest challenge for deep learning is to generalizeÔºå generalize across domains„ÄÇ

It lacks the ability to reason in the way that we've defined understanding previously„ÄÇ

 which is the ability to turn complex information into simpleÔºå useful information„ÄÇ

Convert domain specific„ÄÇComplicated sensory information that doesn't relate to the initial training set„ÄÇ

That's the open challenge for deep learning„ÄÇTrain on very little data and then go and reason and operate in the real world„ÄÇ

Right now neural networks are very inefficientÔºå they acquire big data„ÄÇThey require supervised data„ÄÇ

 which means they need humanÔºå costly human input„ÄÇThey're not fully automated„ÄÇ

 despite the fact that the feature learning incredibly„ÄÇ

 the big breakthrough feature learning is performed automatically„ÄÇ

 you still have to do a lot of design of the actual architecture of the network and all the different hyperparameter tuning needs to be performed human input„ÄÇ

Perhaps a little bit more educated human input in form of PhD studentsÔºå postdocsÔºå faculty„ÄÇ

Is required tune these hyperparametersÔºå but nevertheless human input is still necessary„ÄÇ

They cannot be left alone„ÄÇFor the most part„ÄÇThe award defining the awardÔºå as we saw with Coast R„ÄÇ

 is extremely difficult„ÄÇFor systems that operate in the real worldÔºå transparency„ÄÇ

Quite possibly is not an important oneÔºå but neural networks currently are a black box„ÄÇ

 for the most partÔºå they're not able to accept through a few successful visualization methods that visualize different aspects of the activations„ÄÇ

 they're not able to reveal to us humans why they work or where they fail„ÄÇ

And this is a philosophical question for autonomous vehicles that we may not care as human beings if the system works well enough„ÄÇ

But I would argue that itll be a long time before systems work well enough where we don't care„ÄÇ

We'll care and we'll have to work together with these systemsÔºå and that's where transparency„ÄÇ

 communicationÔºå collaboration is critical„ÄÇ and edge casesÔºå it's all about edge cases„ÄÇIn robotics„ÄÇ

 in autonomless vehicles„ÄÇThe 99„ÄÇ9% of driving is really boring„ÄÇ It's the same„ÄÇ

 especially highway drivingÔºå traffic driving„ÄÇIt's the sameÔºå the obstacle avoidance„ÄÇ

 the car followingÔºå the lane centeringÔºå all these problems are trivialÔºå it's the edge cases„ÄÇ

 the trillions of edge cases that need to be generalized over on a very small amount of training data„ÄÇ

So againÔºå I return to why deep learning„ÄÇI mentioned a bunch of challenges„ÄÇAnd this is an opportunity„ÄÇ

It's an opportunity„ÄÇTo come up with„ÄÇTechniques„ÄÇThat operates successfully in this world„ÄÇ

 So I hope the competitions we present in this class and the autonomous vehicle domain will give you some insight and an opportunity to apply in some of these cases or open research problems„ÄÇ

With semantic segmentation of external perception with control of the vehicle and deep traffic„ÄÇ

And with deep crash of control of the vehicle in under actuated high speed conditions and the driver' state perception„ÄÇ

So with thatÔºå I wanted to introduce deep learning to you today before we get to the fun tomorrow of autonomous vehicles„ÄÇ

So we'd like to thank NvidiaÔºå GoogleÔºå Autolive„ÄÇToyota and at the risk of setting off people's phones„ÄÇ

 AmazonÔºå Alexa Auto„ÄÇ

![](img/a98e904adf8b8abf573097b7201b7b40_28.png)