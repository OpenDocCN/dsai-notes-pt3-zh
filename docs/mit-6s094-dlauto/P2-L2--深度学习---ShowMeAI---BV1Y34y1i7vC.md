# P2ï¼šL2- æ·±åº¦å­¦ä¹  - ShowMeAI - BV1Y34y1i7vC

Thank you everyoneï¼Œ for braving the cold and the snow to be hereã€‚

This is6S 094 deep learning for cell driving carsã€‚And it's a course where we cover the topic of deep learningã€‚

Which is a set of techniques that have taken a leap in the last decade for our understanding of what artificial intelligence systems are capable of doingã€‚

And self driving carsï¼Œ which isã€‚Systems that can take these techniques and integrate themã€‚

In a meaningfulï¼Œ profound wayï¼Œ enter our daily lives in a way that transforms societyã€‚

So that's why both of these topics are extremely important and extremely excitingã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_1.png)

My name is Le Friedmanã€‚And I'm joined by an amazing team of engineers in Jack Towilligerã€‚

 Julia Kiinsbergerï¼Œ Dan Brownï¼Œ Michael Glaerï¼Œ Lee Dingï¼Œ Spencer Doddï¼Œ and Benedict Jenkã€‚

 among many othersã€‚We build autonomous vehicles here at MITã€‚

 not just ones that perceive and move about the environmentï¼Œ but ones that interactã€‚

 communicate and earn the trustï¼Œ and understanding of human beings inside the carã€‚

 the drivers and the passengers and the human beings outside the carã€‚

 the pedestrians and the other drivers and cyclistsã€‚The website for this courseï¼Œ selfdivingcarsã€‚mtã€‚

iduï¼Œ if you have questionsï¼Œ email deepcars@ mitã€‚iduã€‚Slackï¼Œ deep D MITã€‚For registered MIT studentsã€‚

 you have to register on the websiteã€‚Andã€‚By midnightï¼Œ Fridayï¼Œ January 19ã€‚

Build a neural network and submit it to the competition that achieves the speed of 65 m per hour on the new deep traffic 2ã€‚

0ã€‚It's much harder and much more interesting than last year's for those of you who participatedã€‚

There's three competitions in this classï¼Œ deep trafficï¼Œ se fuseï¼Œ deep crashã€‚

There's guest speakers that come from Waymoï¼Œ Googleã€‚Teslaã€‚

And those are starting new autonomous vehicle startups in voyageã€‚Utonomyã€‚Andã€‚Auroraã€‚

In the news a lot today from Csã€‚And we have shirtsã€‚

For those of you who brave the snow and continue to do soã€‚Towards the end of the classã€‚

 there will be free shirtsã€‚ Yesï¼Œ I saidï¼Œ free in shirts in the same sentenceã€‚ You should be hereã€‚

Okayï¼Œ firstï¼Œ the deep traffic competitionã€‚There's a lot of updatesã€‚

 and we'll cover those on Wednesdayã€‚ It's a deeper reinforce learning competitionã€‚ Last yearã€‚

 we received over 18000 submissionsã€‚ðŸ˜Šï¼ŒThis yearï¼Œ we're going to go biggerã€‚ðŸ˜¡ã€‚

Not only can you control one car within your networkï¼Œ you can control up to 10ã€‚ This is multi agentã€‚

 deeper enforcement learningã€‚ This is super coolã€‚ðŸ˜Šã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_3.png)

Secondï¼Œ psych fuseï¼Œ dynamic driving scene segmentation competitionã€‚Where you're givenã€‚The raw videoã€‚

The kinematics of the vehicles and the movement of the vehicleã€‚

The state of the art segmentation for the training setï¼Œ you're given ground truth labelsã€‚

 pixel level labelsï¼Œ scene segmentationã€‚Andd optical flowã€‚ And with those pieces of dataã€‚

 you're tasked to try to perform better than the state of the art in image based segmentationã€‚

Why is this critical and fascinating and an open research problemã€‚Becauseã€‚

Robots that act in in this world in the physical space not only must interpretã€‚

 use these deep learning methods to interpret the spatial visual characteristics of a sceneã€‚

 they must also interpretï¼Œ understand and track the temporal dynamics of the sceneã€‚

 This competition is about temporal propagation of informationã€‚ not just scene segmentationã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_5.png)

You must understand the space and timeã€‚And finallyã€‚

Deep crash where we use deep reinforcement learning to slam cars thousands of times here at MI T at the gymã€‚

You're given data on 100 runs where a car knowing nothing is using a monocular camera as a single input driving over 30 miles an hour through a scene that has very little control throughã€‚

 very little capability to localize itselfï¼Œ it must act very quickly in that sceneã€‚

 you're given 10 runs to learn anythingã€‚We'll discuss this in the coming weeksï¼Œ this competitionã€‚

Will result in four submissionsã€‚That we evaluate everyone's in simulationã€‚

 But the top four submissions we put head to head at the gymã€‚ And until there is a winner declaredã€‚

 we keep sing carsã€‚At 30 miles an hourã€‚Deep crash and also on the website as from last year and on GitHubã€‚

 there's deep Teslaã€‚Which is using the large scale naturalistic driving data that we have to train a neural network to do end to end steering that takes in monocular video from the forward roadway and produces steering commands that steering commands for the carã€‚

Lectures today we'll talk about deep learningï¼Œ tomorrow we'll talk about autonomous vehiclesã€‚

 deep parallells on Wednesdayã€‚Driving scene understandingï¼Œ so segmentationã€‚That's Thursdayã€‚On Fridayã€‚

 we have Sasha Arnuã€‚The director of engineering at Waymoã€‚

 Waymo is one of the companies that's truly taking huge strides in fully autonomous vehiclesã€‚

 Theyre taking the fully L 4 L 5 autonomous vehicle approachã€‚

 And it's fascinating to learn He is also the head of perception for themã€‚

To learn from him what kind of problems they're facing and what kind of approach they're taking onã€‚

 We have Amelia Frizoliï¼Œ who one of last year's speakersï¼Œ Sir Dsh Carmanã€‚

 said Emillia is the smartest person he knowsã€‚So Amelia Frzo is the CT of autonomyã€‚

 an autonomous vehicleã€‚company that was just acquired by Delphiã€‚For a large sum of moneyã€‚

 and they're doing a lot of incredible work in Singapore and here in Bostonã€‚ðŸ˜Šï¼ŒNext Wednesdayã€‚

 we are going to talk about the topic of our research and my personal fascination is deep learning for driver state sensingã€‚

 understanding the humanï¼Œ perceiving everything about the human being inside the car and outside the carã€‚

One talk I'm really excited about is Oliver Cameron on Thursdayã€‚

He is now the CEO of Autonomous Vehicle startup Voyageã€‚

 he was previously the director of the selfdriving Car Program for Uudacityã€‚

 he will talk about how to start a selfdriving car company For thoseã€‚

 he said that MIT folks and entrepreneursï¼Œ if you want to start a one yourselfã€‚

 he'll tell you exactly how it's super coolã€‚And then Sterling Andersonã€‚

Who was the director previously Al Tesla Autopilo team and now is the cofounder of Auroraã€‚

The the self drivingriving carã€‚Startup that I mentioned that has now partnered with Nvidia and many othersã€‚

 So why self driving carsã€‚This class is about applying data driven learning methods to the problem of autonomous vehiclesã€‚

Why self driving cars are fascinating and an interesting problem spaceã€‚Quite possiblyï¼Œ in my opinionã€‚

 this is the first wide reaching and profound integration of personal robots in societyã€‚

Wide reachingï¼Œ because there's 1 billion cars on the roadï¼Œ Even a fraction of that will changeã€‚

The face of transportation and how we move about this worldã€‚Profoundã€‚

 and this is an important point that's not always understoodã€‚Is there is an intimateã€‚

Connection between a human and a vehicle when there's a direct transfer of controlã€‚

 it's a direct transfer of control that takes that his or her life into the hands of an artificial intelligence systemã€‚

I showed a few clickã€‚Qu clips hereã€‚ You can Google first time with Tesla Autopilot on YouTube and watch people perform that transfer of controlã€‚

 There's something magical about a human and a robot working togetherã€‚ðŸ˜Šã€‚

That will transform what artificial intelligence is in the 21st centuryã€‚

And this particular autonomous systemã€‚AI systemï¼Œ cell driving carsã€‚

Is on the scale and profound the life critical nature of it is profound in a way that it will truly test the capabilities of AIã€‚

There is a personal connection that will argue throughout these lectures that we cannot escape considering the human beingã€‚

That autonomous vehicle must not only perceive and control its movement through the environmentã€‚

 It must also perceive everything about the human driver and the passenger and interactã€‚

 communicate and build trust with that driverã€‚

![](img/a98e904adf8b8abf573097b7201b7b40_7.png)

Becauseã€‚In my viewï¼Œ as I will argue throughout this courseã€‚

 an autonomous vehicle is more of a personal robot than it is a perfect perception control system because perfect perception and control to this world full of humansã€‚

Is extremely difficult and could be 2ï¼Œ3ï¼Œ4 decades awayã€‚Full autonomyã€‚

Autonomous vehicles are going to be flawedï¼Œ They're going to have flawsã€‚

 and we have to design systems that are effectively caught that effectively transfer control to human beings when they can't handle the situationã€‚

 And that transfer of controlã€‚Is a fascinating opportunity for AIã€‚Becauseã€‚The obstacle avoidanceã€‚

 perceptionã€‚Of obstacles and obstacle avoidance is the easy problemã€‚

It's the safe problem going 30 miles an hourï¼Œ navigating through streets of Bostonã€‚Is easyã€‚

It's when you have to get to work and you're lateã€‚Or you're sick of the person in front of you that you want to go in the opposing lane and speed upã€‚

That's human natureï¼Œ and we can't escape itã€‚Our artificial intelligence systems can't escape human natureã€‚

 they must work with itã€‚What's shown here is one of the algorithms we'll talk about next week for cognitive loadã€‚

Where we take the raw 3D convolution neural networksï¼Œ take in the eye regionã€‚

 the blinking and the pupil movement to determine the cognitive load of the driverã€‚

 we'll see how we can detect everything about the driverï¼Œ where they're lookingï¼Œ emotion motionã€‚

 cognitive loadï¼Œ body pose estimationã€‚Drowsinessã€‚Theï¼Œ theã€‚

 the movement towards full autonomy is so difficultã€‚

I would argue that it almost requires human level of intelligenceã€‚That theã€‚As I saidï¼Œ2ï¼Œ3ã€‚

 four decade out journey for artificial intelligence researchers to achieve full autonomy will require achievingã€‚

 solving some of the problemsï¼Œ fundamental problems of creating intelligenceã€‚Andã€‚

That's something we'll discuss in much more depth in a broader view in two weeks for the artificial general intelligence courseã€‚

Well we have Andrei Corpay from Teslaï¼Œ Ray Kurzweilï¼Œ Mark Reybergã€‚From Boston Dynamicsã€‚

Who asks for the dimensions of this room because he's bringing robotsã€‚Nothing else was told to meã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_9.png)

It'll be a surpriseã€‚So that is why I argue the human centered artificial intelligence approachã€‚

In every algorithm of the design considers the humanã€‚For autonomous vehicle on the leftã€‚

 the perceptionï¼Œ sceneï¼Œ understanding and the control problemã€‚

 as we'll explore through the competitions and the assignments of this courseã€‚

Can handle 90 and increasing percent of the casesï¼Œ but it's the 101ã€‚

1% of the cases as we get better and better that we have toã€‚

 we're not able to handle through these methodsã€‚ And that's where the human perceiving the human is really importantã€‚

 This is the video from last yearã€‚Of Arc de Triomphã€‚ Thank youã€‚ I didn't know it last yearã€‚

 I know nowã€‚That is one of millions of cases where human to human interactionã€‚

Is the dominant driver not the basic perception control problemï¼ŸSo why deep learning in this spaceï¼Ÿ

Because deep learning is a set of methodsã€‚That do well from a lot of dataã€‚

And to solve these problems where human life is at stakeã€‚

 we have to be able to have techniques that learn from dataï¼Œ learn from real world dataã€‚

 This is the fundamental reality of artificial intelligence systems that operate in the real worldã€‚

 They must learn from real world dataã€‚Whether that's un left for the perceptionï¼Œ the control sideã€‚

Or on the right for the humanï¼Œ the perception and the communicationã€‚

 interaction and collaboration with the human and the human robot interactionã€‚Okayã€‚

So what is deep learningï¼ŸIt's a set of techniquesã€‚ If you allow me the definition of intelligence being the ability to accomplish complex goalsã€‚

Then I would argueã€‚Definition of understanding may be reasoningã€‚

Is the ability to turn complex information into simpleï¼Œ usefulï¼Œ actionable informationã€‚

And that is what deep learning doesã€‚Deep learning is representation learningã€‚Or feature learningã€‚

 if you willã€‚It's able to take raw informationã€‚Rawã€‚

 complicated information that's hard to do anything with and construct hierarchical representations of that information to be able to do something interesting with itã€‚

It is the branch of artificial intelligenceï¼Œ which is most capable and focused on this taskã€‚

Forming representations from dataï¼Œ whether it's supervised or unsupervisedã€‚

 whether it's with the help of humans or notï¼Œ it's able to constructã€‚Structureã€‚

 find structure in the data such that you canã€‚Extract simpleï¼Œ usefulï¼Œ actionable informationã€‚

On the leftã€‚For me in Goodfe's bookã€‚Is the basic example of image classificationã€‚

The input of the imageã€‚On the bottom were the raw pixelsï¼Œ and as we go up the stackã€‚

 as we go up the layersï¼Œ higher and higher order representations are formedã€‚

From edges to contours to cornersï¼Œ to object partsã€‚ and then finallyã€‚

 the full object semantic classification of what's in the imageã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_11.png)

![](img/a98e904adf8b8abf573097b7201b7b40_12.png)

This is representation learningã€‚A favorite example for meã€‚Isã€‚One from four centuries agoã€‚

Our place in the universeã€‚And representing that place in the universeã€‚

 whether it's relative to Earth or relative to the sunã€‚On the left is our current beliefã€‚

 On the right is the one that is held widely four centuries agoã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_14.png)

![](img/a98e904adf8b8abf573097b7201b7b40_15.png)

Representation matters because what's on the right is much more complicated than what's on the leftã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_17.png)

![](img/a98e904adf8b8abf573097b7201b7b40_18.png)

You can think of in a simple case here when the task is to draw a line that separates green triangles in blue circles in the Cartesian coordinate space on the leftã€‚

 the task is much more difficultï¼Œ impossible to do wellã€‚

 On the right is trivial in polar coordinatesã€‚This transformation is exactly which we need to learnã€‚

 This is representation learningã€‚So you can take the same task of having to draw a line that separates the blue curve and the red curve on the leftã€‚

If we draw a straight lineï¼Œ it's going to be a highã€‚

 there's no way to do it with zero error with 100% accuracyã€‚Shown on the right is our best attemptã€‚

But what we can do with deep learning with a single hidden layer network done hereã€‚Its form theã€‚

 the topologyï¼Œ the mapping of the space in such a way in the middle that allows for a straight line to be drawn that separates the blue curve and the red curveã€‚

The learning of the function in the middleã€‚Is what we're able to achieve with deep learningã€‚

It's taking rawï¼Œ complicated information and making it simpleã€‚Actionableï¼Œ usefulã€‚

And the point is that this kind of ability to learn from raw sensor information means that we can do a lot more with a lot more dataã€‚

So deep learning gets better with more dataã€‚

![](img/a98e904adf8b8abf573097b7201b7b40_20.png)

And that's important for real world applicationsã€‚Where edge cases are everythingã€‚

This is us driving with two perception control systemsã€‚ One is in Tesla vehicle with the autopilotã€‚

Version 1 system that's using a monocular camera to perceive the external environment and produce control decisionsã€‚

 and our own neural network running on the Jescent T X 2 that's taking in the same with a monocular camera and producing control decisionsã€‚

And the two systems argueï¼Œ and when they disagreeï¼Œ they raise up a flag to say that this is an edge case that needs human interventionã€‚

There is covering such edge cases using machine learningã€‚Is the main problemã€‚

 of artificial intelligence and when applied to the real worldï¼Œ it is the main problem to solveã€‚Okayã€‚

 so what are neural networksã€‚Inspired very looselyã€‚

 And I'll discuss about the key difference between our own brains and artificial brainsã€‚ðŸ˜Šã€‚

Because there's a lot of insights in that differenceã€‚

But inspired loosely by the biological neural networks here is a simulation of a thalamoccortical brain networkã€‚

 which is only3 million neuronsã€‚476 million synapsesã€‚ The full human brain is a lot more than thatã€‚

100 billion neuronsã€‚1000 trillionã€‚Synapsesã€‚

![](img/a98e904adf8b8abf573097b7201b7b40_22.png)

There's an inspirational music with this one that I didn't realize was hereã€‚It should make you thinkã€‚

Artificial neural networksï¼Œ okayï¼Œ let's just it playã€‚ðŸ˜Šã€‚

The human neural network is 100 billion neuronsï¼Œ rightï¼Œ 1000 trillion synapsesã€‚

One of the state of the art neural networks is ResNe 152ï¼Œ which has 60 million synapsesã€‚

That's a differenceã€‚Of about a seven order of magnitude differenceã€‚

 the human brains have 10 million times more synapses than artificial neural networksã€‚Plusã€‚

 or minus one order of magnitudeï¼Œ depending on the networkã€‚

So what's the difference between a biological neuron and artificial neuronï¼Ÿ

The topology of the human brain have no layersã€‚ Ne networkss are stacked in layersã€‚ They're fixedã€‚

 for the most partã€‚There is chaosï¼Œ very little structure in our human brain in terms of how neurons are connectedã€‚

 they're connected often to 10ï¼Œ000 plus other neuronsã€‚

 The number of synapses from individual neurons that are input into the neuron is hugeã€‚

They're asynchronousã€‚ The human brain works asynchronouslyã€‚

 artificial neural networks work synchronouslyã€‚The learning algorithm for artificial neural networksã€‚

 the only oneã€‚The best oneã€‚Is back propagationã€‚And we don't knowã€‚How human brains learnã€‚

Processing speedã€‚This is one of theã€‚The only benefits we have with artificial neural networks is artificial neurons are fasterã€‚

But they're also extremely power and efficientã€‚Andã€‚

There is a division into stages of training and testing with neural networks with biological neural networksã€‚

 as you're sitting here todayï¼Œ they're always learningã€‚The only profound similarityã€‚

The inspiring oneã€‚The captivating one is that both are distributed computation at scaleã€‚

There is an emergentã€‚Aspect to neural networks where the basic element of computationï¼Œ a neuronã€‚

Is simpleï¼Œ is extremely simpleã€‚ But when connected togetherï¼Œ beautifulï¼Œ amazingã€‚ðŸ˜Šã€‚

Powerful approximators can be formedã€‚A neural network is built up with these computational units where the inputsã€‚

 there's a set of edges with weights on themã€‚ThesThe weights are multiplied by this input signalã€‚

A bias is added with a nonlinear function that determines whether the network gets activated or notã€‚

 Wellï¼Œ the neuron gets activated or not visualized hereã€‚

And these neurons can be combined in a number of waysã€‚ They can form a feed forward neural networkã€‚

 or they canã€‚Feed back into itself to formã€‚To have state memoryã€‚In recurrent neural networksã€‚

 the ones on the leftã€‚Are the ones that are most successful for most applications in computer visionï¼Ÿ

The ones in the right are very popular and specific when temporal dynamics or dynamics time series of any kind are usedã€‚

 In factï¼Œ the ones in the right are much closer to the way our human brains are than the ones on the leftã€‚

But that's why they're really hard to trainã€‚One beautiful aspectã€‚

Of this emergent power for multiple neurons being connected together is the universal property that with a single hidden layerã€‚

 these networksã€‚Can learn any functionã€‚ Learn to approximate any functionã€‚

Which is an important property to be aware ofã€‚Becauseã€‚

The limits here are not in the power of the networksã€‚

 The limits in the is in the methods by which we construct them and train themã€‚

What kinds of machine learningï¼Œ deep learning are thereï¼ŸWe can separate it into two categoriesã€‚

Memorizersã€‚Now the approaches that essentially memorize patterns in the dataã€‚

And approaches that we can loosely say are beginning to reason to generalize over the data with minimal human input on top on the left are the quote unquoteã€‚

  teachersï¼Œ is how much human input in blue is needed to make the methods successful for supervised learningã€‚

 which is what most of deep learning successes come fromã€‚

 where most of the data is annotated by human beingsã€‚The human is at the core of the successã€‚

 most of the data that's part of the training needs to be annotated by human beingsã€‚

With some additional successes coming from augmentation methods that extend thatã€‚

Extend the data based on which these networks are trainedã€‚

And the semiupvised reinforcement learning and unsupervised methods that we'll talk about later in the courseã€‚

That's where the near term successes we hope areã€‚ And with the unsupervised learning approachesã€‚

 that's where the true excitement about the possibilities of artificial intelligence lieã€‚

 being able to make sense of our world with minimal input from humansã€‚

So we can think of two kinds ofã€‚Deep learning impact spacesã€‚

 One is a special purpose intelligence is taking a problemï¼Œ formalizing itã€‚

 collecting enough data on it and being able to solve a particularã€‚Case that provides valueã€‚

Of particular interest here is a network that estimates apartment costs in the Boston area so you could take the number of bedroomsã€‚

 the square feet in the neighborhood and provide as output the estimated costã€‚

On the right is the actual dataã€‚Of apartment costã€‚ We're actually standing in aã€‚

In an area that has over $3000 for a studio apartmentã€‚Some of you may be feeling that painã€‚

And then there's general purpose intelligenceã€‚Or something that feels like approaching general purpose intelligenceã€‚

 which is reinforcement and unsupervised learningã€‚Here from Madri Apathhi's Pong the Pixelsã€‚

 a system that takes in 80 by 80 pixel image and when no other information is able to beatã€‚

 is able to win at this gameï¼Œ no information except a sequence of imagesã€‚

 raw sensory information the same wayï¼Œ the same kind of information that human beings take in from the visual audio touch sensory dataã€‚

 the very low level data and be able to learn to win in this very simplistic in this very artificially constructed worldã€‚

 but nevertheless a world where no feature learning is performedã€‚

 only raw sensory information is used to winã€‚With very sparse minimal human inputã€‚

We'll talk about that on Wednesdayã€‚With deep reinforcement learningã€‚Soï¼Œ but for nowã€‚

 we'll focus on supervised learningã€‚Where there is input dataã€‚

 there is a network we're trying to trainã€‚A learning systemã€‚

 and there's a correct output that's labeled by human beingsã€‚

That's the general training process for a neural networkï¼Œ input dataã€‚

 labels and the training of that network that modelã€‚

 so that in the testing stage on new input data that has never seen before it's tasked with producing guesses and is evaluated based on thatã€‚

For autonomous vehiclesï¼Œ that means being released either in simulation or in the real world to operateã€‚

And how they learn how neural networks learn is given the forward path of taking the input dataã€‚

 whether it's from the training stageã€‚In the training stageï¼Œ the taking the input dataã€‚

 producing a predictionï¼Œ and then given that there's ground truth in the training stageã€‚

 we can have a measure of error based on a loss function that then punishesï¼ŒThe synapsesã€‚

 the connectionsï¼Œ the parameters that were involved with making that wrong predictionã€‚

And it back propagates the errorã€‚Through those weightsã€‚

 we'll discuss that in a little bit more detail in a bit hereã€‚So what can we do with deep learningã€‚

 You can do one to one mappingã€‚Reallyï¼Œ you can think of input as being anythingã€‚

 it can be a number of vector numbersï¼Œ a sequence of numbersï¼Œ a sequence of vector of numbersã€‚

 anything you can think of from images to video to audio to text can be represented in this wayã€‚

 and the output can the same be a single numberï¼Œ or it can be imagesï¼Œ video textï¼Œ audioã€‚

1 to one mapping on the bottomï¼Œ one to manyï¼Œ many to oneï¼Œ many to manyã€‚

 and many to many with different starting points for the dataã€‚Asynchronousã€‚

Some quick terms that will come upï¼Œ deep learning is the same as neural networksã€‚

It's really deep neural networksï¼Œ large neural networksã€‚

It's a subset of machine learning that has been extremely successful in the past decadeã€‚

Multilayer perceptronï¼Œ deep neural networkï¼Œ recurrent neural networkï¼Œ long short term memory networkã€‚

 LSTMï¼Œ convolution neural networkï¼Œ and deep belief networksã€‚

 all of these will come up through the slidesã€‚And there is specific operationsã€‚

 layers within these networks of convolutionï¼Œ poolingï¼Œ activation and back propagationã€‚

 this concept that we'll discussã€‚In this classã€‚Activation functionsã€‚ There's a lot of varianceã€‚

On the left is the activation function in the left column and the x axis is the input and the y axis is the outputã€‚

The sigmoid functionï¼Œ the outputã€‚If the font is too smallï¼Œ the output is not centered at zeroã€‚

For the 10 H functionï¼Œ it's centered at zeroï¼Œ but it still suffers from vanishingsã€‚

Danished ingredients is when the valueï¼Œ the input is low or highã€‚Theï¼Œ the output of the networkã€‚

 as you see in the right column thereï¼Œ the derivative of the function is very lowã€‚

 So the learning rate is very lowã€‚For rail youã€‚It's also not zero centeredã€‚

 but it does not suffer from vanished ingredientsã€‚Back propagation is the process of learningã€‚

 it's the way we take go from errorï¼Œ compute is the loss function on the bottom right of the slideã€‚

 taking the actual output of the network with a forward passï¼Œ subtracting it from the ground truthã€‚

 squarearing dividing way to and using that loss functionã€‚

 then back propagate through to construct a gradient to backproagate the error to the weights that we're responsible for making either a correct or an incorrect decisionã€‚

So the subasks to thatï¼Œ there's a forward passï¼Œ there's a backward passã€‚

 and a fraction of the weight's gradient subtracted from the weightï¼Œ that's itã€‚

That process is modularï¼Œ so it's local to each individual neuronã€‚

 which is why it's extremely we're able to distribute it across multipleã€‚Across the GPUã€‚

 parallellyzed across the GPUã€‚So learning for a neural networkã€‚

These competition units are extremely simpleã€‚ They're extremely simple to then correct when they make an errorã€‚

 when they're part of a larger network that makes an errorã€‚

And all that boils down to is essentially an optimization problem where the objective utility function is the loss function and the goal is to minimize itã€‚

 and we have to update the parametersï¼Œ the weights and the synapses and the biases to decrease that loss functionã€‚

And that loss function is highly nonlinearã€‚Depending on the activation functionsã€‚

 different propertiesï¼Œ different issues ariseï¼Œ there's vanish ingredientssã€‚For sigmoidã€‚

Where the learning can be slowã€‚There's dying rasã€‚Where the derivative is exactly zeroã€‚For inputsã€‚

 less than  zeroã€‚There are solutions to this like leak Urales and a bunch of details that you may discover when you try to win the deep traffic competitionã€‚

But for the most partï¼Œ these are the main activation functionsã€‚And it's the choice of theã€‚

Neo network designerï¼Œ which one works bestã€‚Their saddle pointsã€‚

 all the problems from numerical nonlinear optimization that arise come up hereã€‚

It's hard to break symmetryã€‚And stochastic gradient descent without any kind of tricks to itã€‚

Can take a very long time to arrive at the minimummaã€‚

One of the biggest problems in all the machine learning and certainly deep learning is overfittingã€‚

You can think of the blue dots in a plot hereã€‚As the data to which we want to fit a curveã€‚

We want to design a learning system that approximates to the regression of this dataã€‚

So in green is a sign curveï¼Œ simpleï¼Œ fits wellã€‚And then there's a ninth degree polynomialã€‚

 which fits even better in terms of the errorï¼Œ but it clearly overfits this data if there's other dataã€‚

That it has not seen yet that it has to fitã€‚It's likely to produce a high errorã€‚

 So it's overfitting the training setã€‚This is a big problem for small data setsã€‚

And so we have to fix that with regularizationã€‚ Regularization is a set of methodologies that prevent overfittingã€‚

Learning the training too well in order and then to not be able to generalize to the testing stageã€‚

And overfitting the main symptom is the error decreases in training setã€‚

 but increases in the test setã€‚So there's a lot of techniques in traditional machine learning that deal with this cross validations and so onã€‚

 but because of the cost of training for neural networksã€‚

It's traditional to use what's called a validation setã€‚

So you create a subset of the training that you keep away for which you have the ground truth and use that as a representative of the testing setã€‚

Perform early stoppage or more realistically just save a checkpoint oftenã€‚

To see how as the training evolvesã€‚The performance changes on the validation setã€‚

 and so you can stop when the performance in the validation set is getting a lot worseã€‚

 it means you're overtraining on the training setã€‚In practiceï¼Œ of courseã€‚

 we run training much longer and see when what is the best performing what is the best performing snapshot checkpoint of the networkã€‚

Dropout is another very powerful regularization technique where we randomly remove part of the networkã€‚

 randomly remove some of the nodes in the networkã€‚Along with its incoming and outgoing edgesã€‚

So what that really looks like is a probability of keeping a nodeã€‚

And in many deep learning frameworks todayã€‚It comes with a dropout layerã€‚

 So it's essentially a probability that's usually greater than 0ã€‚5 that a node will be keptã€‚

For the input layerï¼Œ the probability should be much higher or more effectively what works well is just adding noiseã€‚

 What's the point hereï¼Œ you want to create enough diversity in the training data such that it is generalizable to the testingã€‚

And as you'll see with deep traffic competitionï¼Œ there's L2 and L1 penaltyã€‚Weightï¼Œ decayã€‚

 weight penaltyã€‚Where there's a penalization on the weights that get too largeã€‚

 the L2 penalty keeps the weights small unless the air derivative is hugeã€‚

And produces a smoother model and prefers to distribute when there is two similar inputs it prefers to put half the weights on eachã€‚

 distribute the weights as opposed to putting the weight on one of the edgesã€‚

Makes the network more robustã€‚one penalty has the one benefit that for really large weightsã€‚

 they're allowed to stayã€‚ So it allows for a few weights to remain very largeã€‚

These are the regularization techniquesï¼Œ and I wanted to mention them because they're useful to some of the competitions here in the courseã€‚

And I recommend to go to to TensorFlow playgroundã€‚To play around with some of these parametersã€‚

Where you get to online in the browserï¼Œ play around with different inputsï¼Œ different featuresã€‚

 different number of layers and regularization techniquesã€‚

And to build your intuition about classificationï¼Œ regression problemsã€‚

 given different input data setsã€‚So what changedï¼Œ why over the past many decadesï¼Ÿ

Neural networks that have gone through two winters are now again dominating the artificial intelligence communityã€‚

CPUsï¼Œ GPUsã€‚Aicsï¼Œ so computational power has skyrocketedã€‚From Moore's Law to GPUsã€‚

There is huge data setï¼Œ including IageNe and othersã€‚There is researchï¼Œ bad propagationã€‚In the 80sã€‚è¯¶ã€‚

The convolutional neural networksï¼Œ LSTMsï¼Œ there's been a lot of interesting breakthroughs about how to design these architecturesã€‚

 how to build them such that they're trainable efficiently using GPUsã€‚

There is the software infrastructure from being able to share the data Giã€‚

To being able to train networks and share code and effectively view neural networks as a stack of layers as opposed to having to influence stuff from scratch with Tensorflow pietorrch and other deep learning frameworks and there's huge financial backing from Googleã€‚

 Facebook and so onã€‚Deep learningã€‚Isã€‚In order to understand whyã€‚

It works so well and where its limitations areï¼Œ we need to understand where our own intuition comes from about what is hard and what is easyã€‚

The important thing about computer visionï¼Œ which is a lot of what this course is aboutã€‚

 even it's in deeper reinforcement learning formulationã€‚

Is that visual perception for us human beings was formed 540 million years agoã€‚

 that's 540 million years worth of dataã€‚An abstract thought is only formed about 100000 years agoã€‚

That's several orders of magnitudeï¼Œ less dataã€‚So we can make with neural networksï¼Œ predictionsã€‚

That seemed trivialã€‚The the trivial to us human beingsã€‚

But completely challenging and wrong to neural networksã€‚

 here on the left showing a prediction of a dog with a little bit of a distortion of noise added to the imageã€‚

 producing the image on the rightï¼Œ a neural network is confidently 99% plus accuracy predicting that it's an ostrichã€‚

And there's all these problems that has to deal withï¼Œ whether it's in computer vision dataã€‚

 whether it's in text dataï¼Œ audioï¼Œ all of this variation arises in visionã€‚

 its illumination variabilityã€‚The set of pixels and the numbers look completely differentã€‚

 depending on the lighting conditionsï¼Œ it's the biggest problem in driving is lighting conditionsã€‚

 lighting variabilityï¼Œ pose variation objects need to be learned from every different perspectiveã€‚

 I'll discuss that for when sensing the driverï¼Œ most of the deep learning work that's done on the face on the human is done on the frontal face or semifrontal faceã€‚

 there's very little work done on the full 360 pose variability that a human being can take onã€‚

Intraclass variability for the classification problemï¼Œ for the detection problemã€‚

 there is a lot of different kinds of objects for catsï¼Œ dogsï¼Œ carsï¼Œ bicyclistsï¼Œ pedestriansã€‚

So that brings us to object classificationï¼Œ and I'd like to take you through where deep learning has taken big strides for the past several years leading up to this year to 2018ã€‚

So let's start at object classificationï¼Œ is' when you take a single image and you have to say one class that's most likely to belong in that imageã€‚

The most famous variant to that is the Inet competitionï¼Œ INe challengellgeã€‚

 the Inet data set is a data set of 14 million images with 21ã€‚

000 categories and for say the category of fruitï¼Œ there's a total of 188ã€‚

000 images of fruit and there is 1ï¼Œ200 images of Grnny Smith applesã€‚

 it gives you a sense of what we're talking about hereã€‚

So this has been the source of a lot of interesting breakthroughs in deep learning and a lot of the excitement in deep learning is firstã€‚

 the big successful networkï¼Œ at leastã€‚One that became famous in deep learning is Alex in 2012 that took a leap of a significant leap in performance on the Inet challengellgeã€‚

So he was one of the first neural networks that was successfully trained on the GPU and achieved an incredible performance boost over the previous year on the Inet Challgeã€‚

 the challenge isï¼Œ and I'll talk about some of these networks is to give in a single imageã€‚

 give five guesses and you have five guesses to guess for one of them to be correctã€‚

The human annotation is the question often comes upï¼Œ so how do you know the ground truthï¼Ÿ

Human level performance is 5ã€‚1% accuracy on this taskã€‚

But the way the annotation for IageNet is performed is there's a Google search where you pull the images already labeled for youã€‚

 and then the annotation that on mechanical Turkï¼Œ other humans perform is just binaryã€‚

 is this a cat or not a catï¼ŸSo they're not tasked with performing the very high resolution semantic labeling of the imageã€‚

Okayï¼Œ so through from 2012 with Alex andette to todayã€‚

And the big transition in 2018 of the Inet challengellge leaving Stanford and going to Kegelã€‚

It's sort of a monumental step because in 2015 with the Reant network was the first time that the human level of performance was exceededã€‚

And I think this isã€‚A very importantã€‚Map of where deep learning isï¼Œ for particularã€‚

 what I would argue is a toy exampleï¼Œ despite the fact that it's 14 million imagesã€‚

So we're developing state of ER techniques here and the next stageã€‚

 as we are now exceeding human level performance and this task is how to take these methods into the real world to perform scene perceptionã€‚

 to perform driver state perceptionã€‚In 2016 and 2017ã€‚

 CU Image and SCNe has a very unique new addition to the previous formulations that has achieved an accuracy of 2ã€‚

2% errorã€‚2ã€‚25% error on the Inet classification challengeen is an incredible resultã€‚Okayã€‚

 so you have this image classification architecture that takes in a single image and produces convol and takes it through pooling convolutionã€‚

 and at the end fully connected layers and performs a classification task or regression task and you can swap out that layer to perform any kind of other taskã€‚

 including with the recur neural networks of image captioning and so onã€‚

 or localization of bonding boxesï¼Œ or you can doã€‚Fully convolution networksï¼Œ which we'll talk aboutã€‚

On Thursdayã€‚Which is when you take a image as an input and produce an image as an outputã€‚

 but where the output image in this case is a segmentationã€‚

Is where a color indicates what the object isï¼Œ the category of the objectã€‚

 So it's pixel level segmentationï¼Œ every single pixel in the image is assigned a classã€‚

 a category where that pixel belongs toã€‚This is the kind of task that's overlaid on top of other sensor information coming for the car in order toã€‚

Perceive the external environmentã€‚You can continue to extract information from images in this way to produce image to image mappingã€‚

 for exampleï¼Œ to colorize images and take from grayscale images to color imagesã€‚

Or you can use that kind of heat map information to localize objects in the imageã€‚

 So as' as opposed to just classifying that this is an image of a cowï¼Œ RCNNã€‚

 fast and faster RCNN and a lot of other localization networksã€‚

Allow you to propose different candidates for where exactly the cow is located in this image and thereby being able to perform object detectionã€‚

 not just object classificationã€‚In 2017ï¼Œ there's been a lot of cool applications of these architecturesã€‚

 one of which is back on removalã€‚Againï¼Œ mapping from image to imageã€‚

 ability to remove an background from selfiesã€‚Of humans or human likeã€‚Picturesã€‚faceacesã€‚

The references with some incredible animations are in the bottom of the slideã€‚

 and the slides are now available onlineã€‚

![](img/a98e904adf8b8abf573097b7201b7b40_24.png)

Picks to picksï¼Œ HDã€‚There's been a lot of work in Gsã€‚In generative artsarial networksã€‚

 in particular in drivingã€‚GNs have been used to generate examples that generate examples from source dataã€‚

 whether that's from raw dataï¼Œ or in this caseï¼Œ with Ps to Psã€‚ H D is takingã€‚

Carse semantic labeling of the imagesï¼Œ pixel levelï¼Œ and producing photoreisticï¼Œ high definitionã€‚

Images of the forward roadwayã€‚ This is an excitingã€‚

Possibility for being able to generate a variety of cases for cell driving cars for autonomous vehicles to be able to learnã€‚

 to generateï¼Œ to augment the dataï¼Œ and be able to change the way different roads lookã€‚

 road conditionsï¼Œ to change the way vehicles look cyclists pedestriansã€‚



![](img/a98e904adf8b8abf573097b7201b7b40_26.png)

Then we can move on to recurringcurren neural networksã€‚

 Everything I've talked about was one to one mapping from image to image or image to numberã€‚

 recur neural networks work with sequencesã€‚We can use sequencesã€‚To generate handwritingã€‚

To generate text captions from an imageã€‚Based on the localizationsã€‚

 the various detections in that imageã€‚We can provide video description generationã€‚ So taking a videoã€‚

And combining convolution neural networks with recurrent neural networksã€‚

 using convolution neural networks to extract features frame to frameã€‚

 and using those extractive features to input into the RNNs to then generate a labelingã€‚

A description of what's going on in the videoã€‚A lot of exciting approaches for autonomous systemsã€‚

 especially in dronesï¼Œ where the time to make a decisionã€‚Is shortã€‚

 same with the RC car traveling 30 miles an hourï¼Œ attentional mechanisms for steering the attention of the network have been very popular for the localization task and for just saving how much interpretation of the imageã€‚

 how many pixels need to be considered in the classification taskã€‚So we can steerã€‚

 we can model the way a human being looks around an image to interpret it and use the network to do the sameã€‚

And we can use that kind of steeringã€‚To draw images as wellã€‚Finallyã€‚

 the big breakthroughs in 2017 came fromã€‚Thisï¼Œ the pong pixelsï¼Œ the reinforcement learningã€‚

 using sensory dataï¼Œ raw sensory dataï¼Œ and use reinforcement learning methodsã€‚

 deep are all methods of which you'll talk about on Wednesdayã€‚

 I'm really excited about the underlying methodology of deep traffic and deep crash is using neural networksã€‚

As the appromators inside reinforcement learning approachesã€‚

 So Alpha Go in 2016 has achieved a monumental taskã€‚

That when I first started artificial intelligence was told to me it was impossible for the asked them to accomplishã€‚

 which is to win at the game of go against the top human player in the worldã€‚Howeverã€‚

 that method was trained on human expert positionsã€‚

The AlGo system was trained on previous games played by human expertsã€‚

And an incredible accomplishmentã€‚Alpha goes  zero in 2017ã€‚

It was able to beat alphaphago and many of its variantsã€‚By playing itselfã€‚From zero informationã€‚

So no knowledge of human expertsã€‚No gamesï¼Œ no training dataï¼Œ very little human inputã€‚

And what more it was able to generate moves that were surprising to human expertsã€‚

I think its Einstein that said that intelligenceã€‚That the key mark of intelligence is imaginationã€‚

I think it's beautiful to see an artificial intelligence system come up with something that surprises human expertsã€‚

Truly surprisesã€‚For the gambling junkiesï¼Œ deep stack and a few other variantsã€‚

Had been used in 2017 to win a heads up pokerã€‚ Againï¼Œ another incredible resultã€‚

 I was always told artificial intelligence would be impossible for any machine learning method to achieveã€‚

And was able to beat a professional playerï¼Œ and several competitors have come along sinceã€‚

We're yet to be able to win in a tournament settingï¼Œ so multiple playersã€‚

 for those of you familiar heads up poker is one on oneï¼Œ it's a muchï¼Œ much smallerã€‚

 easier space to solveã€‚There's a lot more humidity humid dynamics going on from when there's multiple playersã€‚

 but that's the task for 2018ã€‚And the drawbacks is one of my favorite videosï¼ŒðŸ˜Šï¼ŒShow it oftenã€‚

Of coast runnersã€‚For these deep reinforcement learning approachesï¼Œ theã€‚

Learning of the reward functionï¼Œ the definition of the reward function change controls how the actualã€‚

System behavesï¼Œ and this will comeã€‚This would be extremely important for us with autonomous vehiclesã€‚

Here the boat is tasked with gaining the highest number of pointsã€‚

And it figures out that it does not need to raceï¼Œ which is the whole point of the game in order to gain pointsã€‚

 But insteadï¼Œ pick up greenã€‚Circleles that regenerate themselves over and overã€‚

This is the counterintuitiveã€‚Behavior of a system thatã€‚

Would not be expected when you first designed the reward functionã€‚ And this is a very formalã€‚

 simple systemã€‚ Neverthelessï¼Œ is extremely difficult to come up with a reward function that makes it operate in the way you expect it to operateã€‚

 Very applicable forã€‚Automous vehiclessã€‚ Andï¼Œ of courseï¼Œ in the perception sideã€‚

 as I mentioned with the ostrich and the dogã€‚A little bit of noise with 99ã€‚6% confidenceã€‚

 we can predict that the noise up top is a Robinï¼Œ a cheetahï¼Œ Ar modilloï¼Œ Leserpandaã€‚

 these are outputs from actual state of the art and neural networksã€‚

Taking in the noise and producing a confident predictionã€‚

It should build our intuition to understand that we don'tï¼Œ that the visual characteristicsã€‚

 the spatial characteristics of an image do not necessarily convey the level of hierarchy necessary to function in this worldã€‚

In a similar wayï¼Œ with a dog in the ostrich and everything in an ostrichã€‚

 a network confidently with a littleer of noise can make the wrong predictionã€‚

Thinking a school bus is an ostrich and a speaker is an ostrichã€‚They're easily fooledã€‚But not reallyã€‚

 because they performed the task that they were trained to do wellã€‚Soï¼Œ we have toã€‚

Make sure we keep our intuitionã€‚Optimized to the way machines learnï¼Œ not the way humans have learnedã€‚

Over the 540 million years of data that we've gained through developing the eye the revolutionã€‚

The current challenges we're taking onï¼Œ firstï¼Œ transfer learningã€‚

There is a lot of success in transfer learning between domains that are very close to each otherã€‚

 so image classification from one domain to the nextã€‚

There's a lot of value forming representations of the way scenes look in natural scenes look in order to do scene segmentationã€‚

 the driving caseï¼Œ for exampleï¼Œ but we're not able to do anyã€‚

Any bigger leaps in the way we perform transfer learningï¼Ÿ

The biggest challenge for deep learning is to generalizeï¼Œ generalize across domainsã€‚

It lacks the ability to reason in the way that we've defined understanding previouslyã€‚

 which is the ability to turn complex information into simpleï¼Œ useful informationã€‚

Convert domain specificã€‚Complicated sensory information that doesn't relate to the initial training setã€‚

That's the open challenge for deep learningã€‚Train on very little data and then go and reason and operate in the real worldã€‚

Right now neural networks are very inefficientï¼Œ they acquire big dataã€‚They require supervised dataã€‚

 which means they need humanï¼Œ costly human inputã€‚They're not fully automatedã€‚

 despite the fact that the feature learning incrediblyã€‚

 the big breakthrough feature learning is performed automaticallyã€‚

 you still have to do a lot of design of the actual architecture of the network and all the different hyperparameter tuning needs to be performed human inputã€‚

Perhaps a little bit more educated human input in form of PhD studentsï¼Œ postdocsï¼Œ facultyã€‚

Is required tune these hyperparametersï¼Œ but nevertheless human input is still necessaryã€‚

They cannot be left aloneã€‚For the most partã€‚The award defining the awardï¼Œ as we saw with Coast Rã€‚

 is extremely difficultã€‚For systems that operate in the real worldï¼Œ transparencyã€‚

Quite possibly is not an important oneï¼Œ but neural networks currently are a black boxã€‚

 for the most partï¼Œ they're not able to accept through a few successful visualization methods that visualize different aspects of the activationsã€‚

 they're not able to reveal to us humans why they work or where they failã€‚

And this is a philosophical question for autonomous vehicles that we may not care as human beings if the system works well enoughã€‚

But I would argue that itll be a long time before systems work well enough where we don't careã€‚

We'll care and we'll have to work together with these systemsï¼Œ and that's where transparencyã€‚

 communicationï¼Œ collaboration is criticalã€‚ and edge casesï¼Œ it's all about edge casesã€‚In roboticsã€‚

 in autonomless vehiclesã€‚The 99ã€‚9% of driving is really boringã€‚ It's the sameã€‚

 especially highway drivingï¼Œ traffic drivingã€‚It's the sameï¼Œ the obstacle avoidanceã€‚

 the car followingï¼Œ the lane centeringï¼Œ all these problems are trivialï¼Œ it's the edge casesã€‚

 the trillions of edge cases that need to be generalized over on a very small amount of training dataã€‚

So againï¼Œ I return to why deep learningã€‚I mentioned a bunch of challengesã€‚And this is an opportunityã€‚

It's an opportunityã€‚To come up withã€‚Techniquesã€‚That operates successfully in this worldã€‚

 So I hope the competitions we present in this class and the autonomous vehicle domain will give you some insight and an opportunity to apply in some of these cases or open research problemsã€‚

With semantic segmentation of external perception with control of the vehicle and deep trafficã€‚

And with deep crash of control of the vehicle in under actuated high speed conditions and the driver' state perceptionã€‚

So with thatï¼Œ I wanted to introduce deep learning to you today before we get to the fun tomorrow of autonomous vehiclesã€‚

So we'd like to thank Nvidiaï¼Œ Googleï¼Œ Autoliveã€‚Toyota and at the risk of setting off people's phonesã€‚

 Amazonï¼Œ Alexa Autoã€‚

![](img/a98e904adf8b8abf573097b7201b7b40_28.png)