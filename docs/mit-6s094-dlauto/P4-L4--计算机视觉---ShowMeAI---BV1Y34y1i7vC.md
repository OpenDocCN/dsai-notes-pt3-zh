# P4ï¼šL4- è®¡ç®—æœºè§†è§‰ - ShowMeAI - BV1Y34y1i7vC

Today we'll talk about how to make machines see computer visionã€‚And we will presentï¼Œ thank youã€‚

 whoever said yesã€‚And today we will present a competition that unlike deep trafficã€‚

Which is designed to explore ideasã€‚Teach you about concepts of deep reinforcement learningã€‚Sgfuseã€‚

 the deep dynamic driving scene segmentation competition that I'll present today is at the very cutting edgeã€‚

Whoever does well in this competition is likely to produceã€‚

A publication or ideas that would lead the world in the area of perceptionã€‚

Perhaps together with the people running this classï¼Œ perhaps on your ownã€‚

 and I encourage you to do soã€‚

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_1.png)

Even more casts todayï¼Œ computer visionã€‚Todayï¼Œ as it standsã€‚Is deep learningã€‚

Majority of the successes in how we interpret form representations understand images and videos utilize to a significant degree neural networksã€‚

 the very ideas we've been talking about that applies for supervisedã€‚

 unsupervised and reinforcement learningã€‚And for the supervised caseã€‚

 this is just the focus of todayã€‚The process is the sameã€‚The data is essentialã€‚

 there's annotated data where the human provides the labels that serves as the ground truth and the training processã€‚

 then the neural networkã€‚Gsã€‚Through that dataã€‚Learning to map from the raw sensory inputã€‚

To the ground truth labels and then generalize over the testing data setã€‚

And the kind of raw senses we're dealing with are numbersã€‚

I'll say this again and again that for human visionã€‚For us hereã€‚

 we take for granted this particular aspect of our ability is to take in raw sensory information through our eyes and interpret itã€‚

But it's just numbersã€‚ that's something whether you're an expert computer vision person or new to the fieldã€‚

 you have to always go back to meditate onã€‚is what kind of things the machine is givenã€‚

 what is the data that its tasked to work with in order to perform the task you're asking it to doã€‚

 perhaps the data it's given is highly insufficient to do what you wanted to do that's the question that'll come up again and again are images enough to understand the world around youã€‚

And given these numbersã€‚They set of numbers sometimes with one channelï¼Œ sometimes with three RGBã€‚

 where every single pixel have three different colorsã€‚The task is to classifyã€‚Or regressã€‚

 producing continuous variableã€‚Or one of a set of glass labelsã€‚As beforeã€‚

We must be careful about our intuition of what is hard and what is easing computer visionã€‚

Let's take a step back to the inspirationã€‚For neural networksï¼Œ our own biological neural networksã€‚

Because the human vision system and the computer vision system is a little bit more similar in these regardsã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_3.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_4.png)

The structure of the human visual cortex is in layersã€‚

And his information passes from the eyes to theã€‚To the parts of the brain that makes sense of the raw sensor informationã€‚

 higher and higher order representations have formedã€‚This is the inspirationã€‚

 the idea behind using deep neural networks for imagesã€‚

Higher and higher order representations have formed through the layersã€‚They're early layersã€‚

Taking in the very raw sensory information and extracting edgesã€‚Connecting those edgesã€‚

 forming those edges to form more complex featuresã€‚

 and finally into the higher order semantic meaning that we hope to get from these imagesã€‚

In computer visionï¼Œ deep learning is hardã€‚I'll say this againã€‚

 the illumination variability is the biggest challengeã€‚

Or at least one of the biggest challenges in drivingã€‚For visible light camerasã€‚Poose variabilityã€‚

The objectsã€‚As I'll also discuss about some of the advances from Jeff Hton and the capsule networksã€‚

 the ideaã€‚With the neural networksï¼Œ as they are currently used for computer visionã€‚

 are not good with representing variable posesã€‚These objects in images and this 2T plane of color and textureã€‚

Look very different numerically when the object is rotatedã€‚

And the object is mangled and shaped in different waysï¼Œ the deformableable truncated catã€‚

Interclass variability for the classification taskã€‚

 which would be an example today throughout to introduce some of the networks over the past decade that have received success and some of the intuition and insight that made those networks workã€‚

 classificationã€‚There is a lot of variability inside the classes and very little variability between the classesã€‚

All of these are cats at topã€‚ All of those are dogs at bottomã€‚ They look very differentã€‚

 And the otherï¼Œ I would say the second biggest problem in driving perceptionã€‚

 visible light camera perception is occlusion when part of the object is occluded due to the three dimensionalã€‚

Nature of our worldï¼Œ some objects in front of othersã€‚And they occlude the background objectã€‚

And yet we're still tasked with identifying the object when only part of it is visibleã€‚

 and sometimes that part told you there's cats is very hardly visible hereã€‚

 we're tasked with classifying a cat when just an ear is visibleï¼Œ just the legã€‚

And on a philosophical levelï¼Œ as we'll talk about the motivation for our competition hereã€‚

 here's a cat dressed as a monkey eating a banana on a philosophical levelã€‚

Most of us understand what's going on in the sceneã€‚In factã€‚

 a neural network today successfully classified thisã€‚Image this video as a catã€‚But the contextã€‚

 the humor of the situationã€‚ and in factï¼Œ you could argue it's a monkeyã€‚Is missingã€‚

And what else is missing is the dynamic informationï¼Œ the temporal dynamics of the sceneã€‚

That's what's missing in a lot of the perception work that has been done to date in the autonomous vehicle space in terms of visible light camerasã€‚

 And we're looking to expand on thatã€‚ That's what Syg views is all aboutã€‚

Image classification pipelineï¼Œ there's a bin with different categories inside each classï¼Œ catï¼Œ dogã€‚

 mug hatã€‚Those binsï¼Œ there's a lot of examples of each and you're tasked with when a new example comes along you've never seen before to put that image in a binã€‚

 it's the same as the machine learning task beforeã€‚

And everything relies on the data that's been ground truthï¼Œ that have been labeled by human beingsã€‚

EmNist is a toy dataset set of Hanin digitsï¼Œ often used as examplesã€‚And cocoï¼Œ Sa farã€‚

 Inet places and a lot of other incredible data setsï¼Œ rich data sets of 100 thousandsã€‚

 millions of images out there represent scenesï¼Œ people's faces and different objectsã€‚

 Those are all ground truth data for testing algorithms and forã€‚ðŸ˜Šã€‚

Competing architectures to be evaluated against each otherã€‚CF 10ï¼Œ one of the simplestã€‚

Almost toy dataset sets of tiny icons with 10 categoriesï¼Œ of airplane automobileï¼Œ birdcatã€‚

 deer dog frogï¼Œ horse ship and truckã€‚It's commonly used to explore some of the basic convolution neural networks we'll discussã€‚

 So let's come up with a very trivial classifier to explain the concept of how we could go about itã€‚

In factï¼Œ this isã€‚Maybe if you start to think about how to classify an imageã€‚

 if you don't know any of these techniquesï¼Œ this is perhaps the approach you would take is see would subtract imagesã€‚

 So in order to know that an image of a cat is different than the image of a dogã€‚

 you have to compare them when given those two imagesï¼Œs what's the way you compare themã€‚

One way you could do it is you just subtract itã€‚And then sum all the pixel wise differences in the imageã€‚

 just subtract the intensity of the image pixel by pixel sum it upã€‚If thatã€‚

 if that difference is really highï¼Œ that means the images are very differentã€‚Using that metricã€‚

 we can look at C far 10 and use it as a classifier sayingï¼Œ based on this difference functionã€‚

 I'm going to find one of the 10 bins for a new image thatã€‚That is has the lowest differenceã€‚

Find an image in this data that is most like the image I have and put it in the same bin as that image is inã€‚

So there's 10 classesï¼Œ if we just flip a coinï¼Œ the accuracy of our classifier would be 10%ã€‚

Using our image difference classifierï¼Œ we can actually do pretty goodï¼Œ much better than randomã€‚

 much better than 10%ã€‚We can do 35ï¼Œ38% accuracyã€‚That's a classifierï¼Œ we have our firstã€‚Classifierã€‚

Kier his neighborsã€‚Let's take our classifier to our whole new level instead of comparing it to justã€‚

Trying to find one image that's the closest in our data setã€‚

 we try to find K closest and say what class do the majority of them belong toã€‚

 and we take that K and increase it from one to two to three to four to fiveã€‚

And see how that changes the problemã€‚With seven nearest neighborsã€‚

 which is the optimal under this approach for CFR 10ã€‚We achieved 30% accuracyã€‚

Human level is 95% accuracyã€‚And with convolution neural networksï¼Œ we get very close to 100%ã€‚That'sã€‚

Where neural networks shineï¼Œ this very taskã€‚Of binning imagesã€‚

 it all starts at this basic computational unitã€‚Signal inï¼Œ each of the signals are weighedã€‚Summedã€‚

 bias addedã€‚And put an input into a nonlinear activation function that produces an outputã€‚

The nonlinear activation function is keyã€‚All of these put togetherã€‚And more and more hidden layersã€‚

Form a deep neural networkï¼Œ and that deep neural network is trained as we've discussed by taking a forward pass on examples of ground truth labelsã€‚

 seeing how close those labels are to the real ground truth and then punishing the weights that resulted in the incorrect decisions and rewarding the weights that resulted in correct decisionsã€‚

For the case of 10 examplesï¼Œ the output of the networkã€‚It's 10 different valuesã€‚å—¯ã€‚

The input being handwritten digits from zero to nineï¼Œ theres 10 of thoseã€‚

And we wanted our network to classify what is in this image of a handwritten digitã€‚Its  zeroï¼Œ1ï¼Œ2ã€‚

3 through9ã€‚The way it's often done is there's 10 outputs of the networkã€‚

And each of the neurons on the output is responsible for getting really excited when its number is calledã€‚

And everybody else is supposed to be not excitedï¼Œ thereforeã€‚

The number of classes is the number of outputsï¼Œ that's how it's commonly doneã€‚

And you assign a class to the input image based on the highest the neuron which produces the highest outputã€‚

But that's for a fully connected network that we've discussed on Mondayã€‚

There is in deep learning a lot of tricks that make things workã€‚

 that make training much more efficient on largeã€‚Class problems where there's a lot of classes on large data setsã€‚

When the representation that the neural network is tasked with learning is extremely complexã€‚

 and that's where convolutional neural networks step inï¼Œ the trick they use is spatial invarianceã€‚

They use the idea thatã€‚A cat in the top left corner of an image is the same as a cat in the bottom right corner of an imageã€‚

So we can learn the same features across the imageã€‚That's where the convolution operation steps inã€‚

Instead of the fully connected networksï¼Œ here there's a third dimensionã€‚Of depthã€‚

So the blocks in this neural networkã€‚As input take 3D volumes and this output produce 3D volumesã€‚

They take a slice of the imageã€‚A window and slide it acrossã€‚Applying the same exact weightsã€‚

 and we'll go through an exampleï¼Œ the same exact weights as in the fully connected network on the edges that are used to map the input to the outputã€‚

 here are used to map the slice of an imageï¼Œ this window of an image to the outputã€‚

And you can make several many of suchã€‚Convolutional filtersï¼Œ many layersï¼Œ many different options ofã€‚

What kind of features you look for in an imageï¼Œ what kind of window you slide across in order to extract all kinds of thingsã€‚

All kinds of edgesï¼Œ all kinds of higher order patterns in the imagesã€‚

The very important thing is the parameters on each of these filtersï¼Œ these subset of the imageã€‚

 these windowsï¼Œ are sharedã€‚If the feature that defines a cat is useful in the top left cornerã€‚

 it's useful in the top right cornerï¼Œ it's useful in every aspect of the imageã€‚

 this is the trick that makes convolutional neural networks save a lot ofã€‚

A lot of parameters reduce parameters significantlyã€‚Is the reuseã€‚

 the spatial sharing of features across the space of the imageã€‚

The depth of these 3D volumes is the number of filtersã€‚The stride is the skip of the filterã€‚

 the step sizeï¼Œ how many pixels you skip when you apply the filter to the inputã€‚And the paddingã€‚

Is the paddingï¼Œ the zero padding on the outside of the input to a convolutional layerã€‚

Let's go through an exampleã€‚So on the left hereã€‚And the slides are now available onlineã€‚

 you can follow them alongï¼Œ and I'll step through this exampleã€‚

On the left here is an input volume of three channelsã€‚The left column is the inputã€‚

 the three squares there are the three channelsã€‚And there's numbers inside those channelsã€‚

And then we have a filter in redã€‚Two of themã€‚Two channels of filtersã€‚With a biasã€‚ And weã€‚

 those filters are 3 by 3ã€‚ Each one of them isã€‚Size three by threeã€‚

 And what we do is we take those three by three filtersã€‚That are to be learnedã€‚

 these are our variablesï¼Œ our weights that we have to learnã€‚

And then we slide it across an image to produce the output on the rightï¼Œ the greenã€‚

So by applying the filters in the redï¼Œ there's two of them and within each oneã€‚

 there's one per every input channelã€‚We go from the left to the rightã€‚

From the input volume on the left to the output volume green on the rightã€‚And you can lookã€‚

 you can pull up the slides yourselfï¼Œ not if you can't see the numbers on the screenã€‚

 but the operationsã€‚Are performed on the input to produce the single value that's highlighted there in the green and the outputã€‚

And we slide this convolutionï¼Œ no filter along the imageã€‚With the stride in this caseã€‚Of twoã€‚

 skippingï¼Œ skipping alongã€‚They sum to the rightï¼Œ the two channel output in greenã€‚That's itã€‚

 that's the convolutional operationï¼Œ that's what's called the convolutional layer in neural networksã€‚

And the parameters hereï¼Œ besides the biasï¼Œ are the red values in the middleã€‚

 That's what we're trying to learnã€‚And there's a lot of interesting tricks we'll discuss today on top of thoseã€‚

 but this is at the coreï¼Œ this is the spatially invariant sharing of parameters that make convolution neural networks able to efficiently learn and find patterns and imagesã€‚

To build your intuition a little bit moreã€‚About convolutionã€‚

 here's an input image on the left and on the rightã€‚

 the identity filter produces the output you see on the rightã€‚

 and then theres different ways you can different kinds of edges you can extractã€‚

We the resulting activation maps seen on the rightï¼ŸSo when applying the filtersã€‚

Those edge detection filtersã€‚To the image on the leftã€‚

 you produce in white are the parts that activate the convolutionã€‚The results of these filtersã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_6.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_7.png)

And so you can do any kind of filterï¼Œ that's what we're trying to learnï¼Œ any kind of edgeã€‚

Any kind of pattern you can move along in this window in this way that's shown hereã€‚

 you slide along the image and you produce the output you see on the rightã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_9.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_10.png)

And depending on how many filters you have in every levelã€‚

 you have many of such slices that you see on the rightï¼Œ the input on the leftã€‚

 the output on the rightï¼Œ if you haveã€‚

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_12.png)

Dozens of filtersï¼Œ you would have dozens of images on the rightã€‚

 each with different results that show where each of the individual filter patterns were foundã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_14.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_15.png)

And we learn what patterns are useful to look for in order to perform the classification taskã€‚

That's the task for the neural network to learn these filtersã€‚

And the filters have higher and higher orderã€‚Of representationã€‚

Going from the very basic edges to the high semanticã€‚Meaning that spans entire imagesã€‚

And the ability to span images can be done in several waysã€‚

 but traditionally has been successfully done through max poolingï¼Œ through poolingã€‚

Of taking the output of a convolutional operationã€‚And reducing the resolution of thatã€‚Byã€‚

By condensing that information byï¼Œ for exampleï¼Œ taking the maximum valuesï¼Œ the maximum activationsã€‚

Thereforefoï¼Œ reducing the spatial resolutionï¼Œ which has detrimental effectsã€‚

 as we'll talk about in the scene segmentationï¼Œ but it's beneficial for finding higher order representations in the images that bring images togetherã€‚

 that bring features together to form an entity that we're trying to identify and classifyã€‚Okayã€‚

 so that formsã€‚A convolution neural networkï¼Œ such convolutional layers stacked on top of each other is the only addition to a neural network that makes for a convolutional neural networkã€‚

And then at the endï¼Œ the fully connected layers or any kind of other architectures allow us to apply particular domainsã€‚

Let's take ImageNe as a case studyã€‚An imagenetã€‚The data setï¼Œ an imageNeï¼Œ the challengeã€‚

 the task is classificationã€‚As I mentioned in the first lectureï¼Œ Inet is a data setã€‚

One of the largest in the world of images with 14 million imagesï¼Œ 21ï¼Œ000 categoriesã€‚

And a lot of depth to many of the categoriesï¼Œ as I mentionedï¼Œ 1ï¼Œ200 Grannny Smith applesã€‚

TheseThese allow the neural networks to learn the rich representations in both poseã€‚

 lighting variability and intraclass class variation for the particular thingsã€‚Particular classesã€‚

Like Grny Smith applesã€‚So let's look through the various networksï¼Œ let's discuss themã€‚

 let's see the insights it started with Alexnetï¼Œ the first really big successful GPU trained neural network on IageNet that's achieved a significant boost over the previous yearã€‚

And moved on to VGGNeã€‚GoogleNeã€‚Agu Lynetteã€‚ResNeï¼Œ CU imageï¼Œ and SNeã€‚In 2017ã€‚Againã€‚

 the numbers will show for the accuracy are based on the top five error rateã€‚

 you get five guesses and it's a one or zeroã€‚ if you if one of the five is correctã€‚

 you get a one for that particular guessï¼Œ otherwise it's a zeroã€‚And human error is 5ã€‚

1 when a human tries to achieve the same tries to perform the same task as the machinist task we doingã€‚

 the error is 5ã€‚1ï¼Œ the human annotation is performed on the images based on binary classificationã€‚

 Granny Smithï¼Œ Apple or notï¼Œ cat or notï¼Œ the actual task that the machine has to perform and that the human competing has to performã€‚

 is given an image is provided one of the many classesã€‚Under thatï¼Œ human errors is 5ã€‚1%ã€‚

 which was surpassed in 2015 by Resnetã€‚To achieve 4% errorã€‚So let's start withã€‚AlexNetã€‚

 I'll zoom in on the later networksã€‚ They have some interesting insightsã€‚

 but AlexNet and VGGNet both followed a very similar architectureã€‚Very uniform throughout its depthã€‚

Viji Gnet in 2014ã€‚Is convolutionï¼Œ convolution poolingï¼Œ convolutionï¼Œ poolingï¼Œ convolution poolingã€‚

 and fully connected layers at the endã€‚There's a certain kind of beautiful simplicityã€‚

 uniformity to these architectures because you can just make it deeper and deeper and makes it very amenable to implementation in a layer stack kind of way in any of the deep learning frameworksã€‚

It's clean and beautiful to understand in the case of EGGnet 16 or 19 layers with 138 million parametersã€‚

 not many optimizations on these parametersï¼Œ thereforeã€‚

 the number of parameters is much higher than the networks that followed itã€‚

 despite the layers not being that largeã€‚GoogleNet introduced the inception moduleã€‚

 starting to do some interesting things with the small modules within these networksã€‚

 which allow for the training to be more efficient and effectiveã€‚

The idea behind the inception module shown hereã€‚With the previous layer on bottomã€‚

And the convolutional layer here with the inception moduleã€‚On topã€‚

 produced on top is it used the idea that different size convolutions provide different value for the networkã€‚

Smaller convolutions are able to capture or propagate forward features that are very localã€‚

 a high resolution in textureã€‚Larger convolutions are better able to represent and capture and catch highly abstracted featuresã€‚

 higher order featuresã€‚So the idea behind the inception module is to sayï¼Œ wellã€‚

 as opposed to choosing in a hyperparameter tuning process or architecture design processã€‚

 choosing which convolution size we want to go withã€‚

 why not do all of them well several together in the case of the GoogleNe modelã€‚

 there is the one by oneï¼Œ three by three and five by five convolutionsã€‚

With the old trusty friend of Max Poing still left in there as wellã€‚

Which has lost favor more and more over time for the image classification taskã€‚

And the result is there's fewer parameters that are required if you pickã€‚

The placing of these inception modules correctlyï¼Œ the number of parameters required to achieve a higher performance is much lowerã€‚

Resnetã€‚One of the most popular still to dateã€‚Architectures that we'll discuss in scene segmentation as wellã€‚

Came up and used the idea of a residual blockã€‚The initialã€‚Inspiring observationã€‚

 which doesn't necessarily hold trueï¼Œ as it turns outã€‚ But that network depthã€‚

Increases representation powerã€‚ So these residual blocks allow you to have much deeper networksã€‚

 And I'll explain why in a second hereï¼Œ butã€‚The thought was they work so well because the networks are much deeperã€‚

The key thing that makes these blocks so effective is the same idea that's reminiscent ofcurren neural networksã€‚

That I hope we get a chance to talk aboutã€‚The training of them is much easierã€‚

They take a simple blockï¼Œ repeat it over and overã€‚And they pass the input along without transformationã€‚

Along with the ability to transform itï¼Œ to learn the filtersï¼Œ learn the weightsã€‚So you're allowed toã€‚

You allow every layer to not only take on the processing of previous layersã€‚

 but to take in the raw transform data and learn something newã€‚

The ability to learn something new allows you to have much deeper networksã€‚

And the simplicity of this block allows for more effective trainingã€‚The state of the art in 2017ã€‚

 the winner is Squeeing Exitation Networksã€‚That unlike the previous year was C image which Chimpley took ensemble methods and combined a lot of successful approaches to take a marginal improvementã€‚

 SC netã€‚Got a significant improvementã€‚At least in percentagesï¼Œ I think is a 25% reduction in errorã€‚

From 4% to 3%ã€‚Something like that by using a very simple idea that I think is important to mentionã€‚

 a simple insightã€‚It added a parameter to each channel in the convolutional layerirã€‚

In the convolutional blockã€‚So the network can now adjust the waiting on each channelã€‚

Based for each feature mapï¼Œ based on the contentï¼Œ based on the input to the networkã€‚

This is kind of a takeaway to think aboutï¼Œ about any of the networks who talk about any of the architecturesã€‚

Is a lot of times recurrent neural networks and convolutional neural networks have tricks that significantly reduce the number of parametersã€‚

The bulkï¼Œ the sort of low-han fruitï¼Œ they use spatial invarianceã€‚

 the temporal invariance to reduce the number of parameters to represent the input dataã€‚

 but they also leave certain things not paraized they don't allow the network to learn it allowing in this case the network to learn the waitinging on each of the individual channels so each of the individual filters is something that you learn as along with the filters makes a huge boostã€‚

 the cool thing about this is it's applicable to any architectureï¼Œ this kind of blockã€‚

 this kind of what the squeeze and excitation blockã€‚Is applicable to any architectureã€‚

And because obviously it just simply parameterizes the ability to choose which filter you go with based on the contentã€‚

 it's a subtle but crucial thingï¼Œ I think it's pretty cool and for future research it inspires to think about what else can be parameterized in neural networksã€‚

 what else can be controlled as part of the learning processã€‚

 including hiring higher order hyperparametersã€‚Which aspects of the training and the architecture of the network can be part of the learningã€‚

 This is what this network conspiresã€‚Another network has been in development since the '90s ideasã€‚

But Jeff Hntonï¼Œ but really received been published on and received significant attention in 2017ã€‚

 that I won't go into detail hereï¼Œ we are going to release an online only video about capsule networksã€‚

It's a little bit too technicalï¼Œ but they inspire a very important pointã€‚

That we should always think about with deep learningã€‚Whenever it's successfulã€‚

 is to think about whatï¼Œ as I mentioned with the cat eating a banana on a philosophical and the mathematical levelã€‚

 we have to consider what assumptions these networks make and what through those assumptions they throw awayã€‚

So neural networks with convolution neural networks due to their spatial invarianceã€‚

Throw away information about the relationshipã€‚Betweenã€‚

TheThe hierarchies between the simple and the complex objectsã€‚

 So the face on the left and the face on the right looks the same to a convolution neural networkã€‚

 the presence of eyes and noseã€‚And mouthã€‚Is the essential aspect of what makes the classification task work for convolution network where it will fire and say this is definitely a phaseã€‚

But the spatial relationship is lostï¼Œ is ignoredï¼Œ which meansã€‚

There's a lot of implications to this butã€‚For things like pose variationã€‚That information is lostã€‚

 we're throwing that away completely and hoping that the pooling operation that's performed in these networks is able to sort of mesh everything together to come up with the features that are firing of the different parts of the face to then come up with the total classification that it's a face without representing really the relationship between these features at the low level and the high level at the low level the hierarchy at the simple and the complex levelã€‚

This is a super exciting field now that hopefully will spark developments of how we design neural networks that are able to learn the rotationalã€‚

 the orientation in variances as wellã€‚Okayï¼Œ so as I mentionedã€‚

You take these convolution neural networksï¼Œ chop off the final layerã€‚

In order to apply to a particular domainã€‚And that is what we'll do with fully convolutionary neural networksã€‚

 the ones that we task to segment the image at a pixel levelã€‚As a reminderã€‚

 these networks through the convolutional processã€‚Really producing a heat mapã€‚

 different parts of the network are getting excited based on the different aspects of the imageã€‚

 and so it can be used to do the localization of detectingã€‚

 not just classifying the image but localizing the objectã€‚And they could do so at a pixel levelã€‚

So the convolutional layers are during the encoding processï¼Œ they're taking the richã€‚

 raw sensory information in the image and encoding them into an interpretable set of features representation that can then be used for classificationã€‚

 but we can also then use a decoderï¼Œ upsample that information and produce a map like thisã€‚

Fully convolution neural networkï¼Œ segmentationï¼Œ semantic scene segmentationï¼Œ image segmentationã€‚

 the goal is toï¼Œ as opposed to classify the entire imageï¼Œ you classify every single pixelã€‚

 it's pixel level segmentationï¼Œ you color every single pixel with what that pixelã€‚

 what object that pixel belongs to in this 2D space of the imageã€‚

The 2D projection in the image of a three dimensional worldã€‚

So the thing is there's been a lot of advancement in the last threeã€‚Yearsã€‚

But it's still an incredibly difficult problemã€‚If if you thinkï¼Œ if you think aboutã€‚

The amount of data that's used for training and the task of pixel level of megapixels here of millions of pixels that are tasked with having assigned a single labelã€‚

 it's an extremely difficult problemã€‚Why is this interestingã€‚

 important problem to try to solve as opposed to bounny boxes around catsï¼ŸWellã€‚

 it's whenever precise boundaries of objects are importantã€‚

 certainly medical applications when looking at imaging and detecting in particularï¼Œ for exampleã€‚

 detecting tumors in medical imaging of different organsã€‚And in driving in roboticsã€‚

When objects are involvedï¼Œ it's a dense scene involved with vehiclesï¼Œ pedestrian cyclistsã€‚

 we need to be able to not just have a loose estimate of where objects areã€‚

 we need to be able to have the exact boundariesï¼Œ and then potentially through data fusion fusing sensors togetherã€‚

 fusing this rich textual information about pedestrian cyclists and vehicles to light our data that's providing us the three dimensionmenal map of the worldã€‚

 or have both these semantic meaning of the different objects and their exact three- dimensional locationã€‚

A lot of this work successfullyã€‚A lot of the work in the semantic segmentation started with fully convolutionary networks for semantic segmentation paperã€‚

 FCNï¼Œ that's where the name of FCN came from in November 2014ã€‚ Now go through a few papers hereã€‚

To give you some intuition where the field is goneã€‚

And how that takes us to se fuse the segmentation competitionã€‚

So FCN repurposed the imagenet pretrained netsï¼Œ the nets that were trained to classify wasn't an imageã€‚

The entire imageã€‚And chopped off the fully connected layers and then added decoder parts that upsampled the imageã€‚

To produce a heat map here shown with a tabby catï¼Œ a heat map of where the cat is in the imageã€‚

It's a much lowerï¼Œ much coarser resolution than the input imageã€‚One8 at bestã€‚

Skip connections to improve coarseness of upslingï¼Œ there's a few tricksã€‚

If you do the most naive approachï¼Œ the upsampling is going to be extremely coarse because that's the whole point of the neural networkã€‚

 the encoding part is you throw away all the useless data to the most essential aspects that represent that imageã€‚

 so you're throwing away a lot of information that's necessary to then form a high resolution imageã€‚

So there's a few tricks where you skip a few of the finalã€‚

Pooling operations to go in similar way into a residual block to go to the outputã€‚

 produce higher and higher resolution heat map at the endã€‚Signnet in 2015ã€‚

Applied this to the driving context and really taking it to Kitty data set and have shown a lot of interesting results and really explored the encoder decoder formulation of the problemã€‚

Really solidifying this the place of the encoder decoder framework for the segmentation taskã€‚

Dilated convolutionï¼Œ I'm taking you through a few components which are critical here to the state of the artã€‚

Dated convolutionsã€‚So the convolution operationã€‚As the pooling operationã€‚

Reduces resolution significantlyã€‚And dilated convolution has a certain kind of grdtting as visualized there that maintains the localã€‚

 high resolution texturesã€‚While still capturing the spatial window necessaryã€‚

It's called dilated convolutional layerã€‚And that's in a 2015 paper proved to be much better at upsampling a high resolution imageã€‚

Deep lab with a beeã€‚V1ï¼Œ V2ï¼Œ now V3ã€‚Added conditional random fieldsã€‚

Which is the final piece of the state of the art puzzle hereã€‚

 a lot of the successful networks today that do segmentationï¼Œ not allã€‚Do post process using CRFsã€‚

 conditional random fieldsï¼Œ and what they do is they smooth the segmentationã€‚

 the upsampled segmentation that results from the FCN by looking at the underlying image intensitiesã€‚

So that's the key aspects of the successful approaches todayã€‚

 you have the encoded decoder framework of a fully accomplished neural networkã€‚

 it replaces the fully connected layers with the convolutional layersã€‚

Deconvolutional errors and as the years progressed from 2014 to todayã€‚As usualã€‚

 than underlying networksã€‚From AlexNet to VGGNe and to now Resnet have been one of the big reasons for the improvements of these networks to be able to perform the segmentationã€‚

So naturallyï¼Œ they mirror the imagenet challenge performance in adapting these networksã€‚

 So the state of the art uses resnet or similar networksï¼Œ conditional random fieldsã€‚

For smoothing based on the input image intensities and the dilatedã€‚

Convolution that maintains the computational costï¼Œ but increases the resolution of the upsseamplingã€‚

Throughout the intermediate feature mapsã€‚And that takes us to the state of the art that we usedã€‚

To produce the imagesã€‚å—¯ã€‚To produce the images for the competitionã€‚

Bresnet DUC for Dance up sampling convolutionã€‚Instead of bilinear up samplingã€‚

 you make the ups learnableã€‚You learn upscaling filtersã€‚That's on the bottomã€‚

 that's really the key part that made it workã€‚There should be a theme hereã€‚

 sometimes the biggest addition that could be done is paraizing one of the aspects of the network that they've taken for grantedã€‚

 letting the network learn that aspectã€‚And the otherã€‚I'm not sure how important it is to the successã€‚

 but it's a cool little additionï¼Œ is a hybrid dilated convolutionã€‚

As I showed that visualization where the convolution is spread apart a little bit in the inputã€‚

 from the input to the outputã€‚The steps of that dilated convolution filterã€‚When they're changedã€‚

 it produces a smoother result because when it's kept the sameã€‚

Their certain input pixels get a lot more attention than othersã€‚

So losing that favoritism is what it's achieved by using a variable different dilation rateã€‚

Those are the two tricksï¼Œ but really the biggest one is the parameterization of the upscaling filtersã€‚

Okayï¼Œ so that's what we use to generate that data and that's what we provide you the code with if you're interested in competing in psychFsã€‚

The other aspect here that everything we talked about from the classification to the segmentation to making sense of images is the information about timeã€‚

 the temporal dynamics of the scene is thrown awayã€‚

And for the driving context for the robotics contestã€‚

 and what we'd like to do is sefuse for the segmentationã€‚

 dynamic scene segmentation context of when you try to interpret what's going on in the scene over time and use that informationã€‚

Time is essentialã€‚The movement of pixels is essential through timeã€‚

 that understanding how those objects move in a 3D spaceã€‚

Through the 2D projection of an image is fascinating and there's a lot of set of open problems thereã€‚

So flow is what's very helpful toï¼Œ as a starting point to help us understand how these pixels move flowã€‚

Optical flowï¼Œ dense optical flowsï¼Œ the computation that our bestã€‚

Our best approximation of where each pixel in image oneã€‚

And moved in the temporal following image after thatï¼Œ there's two images and 30 frames a secondã€‚

 there's one image at times 0ï¼Œ the other is 33ã€‚3 milliseconds laterã€‚

 and the dense optical flow is our best estimate of how each pixel in the input image moved to in the output imageã€‚

The optical flow for every pixel produces a direction of where we think that pixel moved and the magnitude of how far moved that allows us to take information that we detected about the first frameã€‚

And try to propagate it forwardã€‚ This is the competition is to try to segment an image and propagate that information forwardã€‚

For manual annotationã€‚Of an imageï¼Œ so this kind of coloring book annotation where you color every single pixel in the state of the art data setã€‚

For driving cityscapes that it takesã€‚ 1ã€‚5 hoursï¼Œ 90 minutes to do that coloringã€‚

 it's 90 minutes per imageã€‚That's extremely long timeã€‚

 that's why there doesn't exist today a data setï¼Œ and in this class we're going to create oneã€‚

Of segmentation of these images through timeã€‚Through videoã€‚

So long videos where every single frame is fully segmentedã€‚

That's still an open problem that we need to solveã€‚Flow is a piece of thatã€‚And we also provide youã€‚

This compute that state of the art flow using flowlowNe 2ã€‚0ã€‚So flowNe 1ã€‚0 in May 2015ã€‚

Use neural networks to learn the optical flowï¼Œ the dense optical flowã€‚

And it did so with two kinds of architecturesï¼Œ flow net Sï¼Œ flow net simpleï¼Œ and flow net coreã€‚

 flown net Cã€‚The simple one is simply taking the two images so what's the task here there's two images and you want to produce from those two images they follow each other in time 33ã€‚

3 milliseconds apart and your task is is the output to produce the dense optical flow so for the simple architecture you just stack them together each are RGB so it produces six channel input to the network there's a lot of convolution and finally it's the same kind of process as the fully convolution neural networks to produce the optical flow then there is flownet correlationã€‚

Architectureï¼Œ where you perform some convolutions separately before using a correlation layer to combine the feature mapsã€‚

Both are effectiveã€‚In different data sets and different applicationsï¼Œ so flowlowNe 2ã€‚

0 in December of 2016 is one of the state of the art frameworksã€‚

Code bases that we use to generate the data I showã€‚Combines the flown at S and flown Sã€‚

It improves over the in flow netï¼Œ producing a smoother flow field preserves the fine motion detail along the edges of the objectsã€‚

And it runs extremely efficientlyï¼Œ depending on the architectureï¼Œ there's a few variantsã€‚

 either  eight to 140 frames a secondã€‚And the process there is essentially one that's common across various applicationsã€‚

 deep learning is stacking these networks togetherã€‚The very interesting aspect here thatã€‚

We're still exploring and again applicable in all of deep learning in this case it seemed that there was a strong effect in taking sparseã€‚

 small multiple data set and doing the trainingï¼Œ the order of which those datas were used for the training process mattered a lotã€‚

That's very interestingã€‚

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_17.png)

So using FlowNe 2ã€‚0ï¼Œ here's the data set we're making available for psychfuse the competitionã€‚

Cars that MIT died to use ssych views Firstï¼Œ the original videoã€‚Us driving in high definitionã€‚

1080p in a 8K360 videoã€‚Original video driving around Cambridgeã€‚Then we're providing the ground truthã€‚

For a training setã€‚For that training set for every single frameï¼Œ 30 frames a secondã€‚

 we're providing the segmentationï¼Œ frame to frame to frameã€‚Segmented on mechanical turkã€‚

We're also providing the outputã€‚Of the network that I mentionedã€‚

 the state of the art segmentation networkã€‚That's pretty damn close to the ground truthã€‚

But still notã€‚And our task isï¼Œ this is the interesting thing isã€‚

Our task is to take the output of this networkã€‚Well there's two optionsã€‚

 one is to take the output of this networkã€‚And use other networks to help you propagate the information better so what this segmentationã€‚

 the output of this network does is it only takes a frame by frame by frame it's not using the temporal information at all so the question is can we figure out a wayã€‚

 can we figure out tricks to use temporal information to improve this segmentation so it looks more like this segmentationã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_19.png)

And we're also providing the optical flow from frame to frame to frameã€‚

 So the optical flow based on flow at 2ã€‚0 of how each of the pixels movedã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_21.png)

Okayã€‚And that forms a psyfuse competitionï¼Œ 10ï¼Œ000 imagesã€‚And the task is to submitã€‚Codeã€‚

 we have starter code in Python and on GitHubã€‚To take in the original videoã€‚

 take in for the training setï¼Œ the ground truthï¼Œ the segmentation from the state of the artã€‚

 segmentation networkï¼Œ the optical flow from the state of the artï¼Œ optical flowã€‚

Network and taking that together to improve the stuff on the bottom leftã€‚

 the segmentation to try to achieve the ground truth on the top rightã€‚



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_23.png)

Okayï¼Œ with thatï¼Œ I'd like to thank you tomorrow at 1 pm is Waymo in Stataã€‚321ï¼Œ23ï¼Œ the next lectureã€‚

 next week will be on deep learning for a sense in the human understanding the humanã€‚

 and we will release online only lecture on capsule networks and GANsï¼Œ general adversarial networksã€‚

 thank you very muchã€‚

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_25.png)