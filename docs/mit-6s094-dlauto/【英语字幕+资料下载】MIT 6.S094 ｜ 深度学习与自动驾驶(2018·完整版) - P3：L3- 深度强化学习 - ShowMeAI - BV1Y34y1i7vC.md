# „ÄêËã±ËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëMIT 6.S094 ÔΩú Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™Âä®È©æÈ©∂(2018¬∑ÂÆåÊï¥Áâà) - P3ÔºöL3- Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π† - ShowMeAI - BV1Y34y1i7vC

TodayÔºå we will talk about deep reinforcement learning„ÄÇ



![](img/731ed94783f15549cd0d4044b14ddb26_1.png)

The question we would like to explore„ÄÇIt's to which degree we can teach systems to act„ÄÇ

 to perceive and act in this world from data„ÄÇSo let's take a step back and think of what is the full range of tasks that an artificial intelligence system needs to accomplish„ÄÇ

 Here's the stack from top to bottomÔºå top to inputÔºå bottom output„ÄÇThe environment at the top„ÄÇ

 the world that the agent is operating in„ÄÇSensed by sensors„ÄÇ

 taking in the world outside and converting it to raw data interpretable by machines„ÄÇCensor data„ÄÇ

And from that raw sensor dataÔºå you extract features„ÄÇ

You extract structure from that data such that you can input itÔºå make sense of itÔºå discriminate„ÄÇ

 separateÔºå understand the data„ÄÇAnd as we discussedÔºå you form higher and higher order representations„ÄÇ

 a hierarchy of representations based on which the machine learning techniques can then be applied„ÄÇ

Once the machine learning techniquesÔºå the understandingÔºå as I mentioned„ÄÇ

 converts the data into features into higher order representations and into simpleÔºå actionable„ÄÇ

 useful information„ÄÇWe aggregate that information into knowledge„ÄÇ

 We take the pieces of knowledge extracted from the data„ÄÇ

Through the machine learning techniques and to build a taxonomy„ÄÇA library of knowledge„ÄÇ

And with that knowledgeÔºå we reasonÔºå an agent is has to reason„ÄÇTo aggregate„ÄÇ

To connect pieces of data seen in the recent past or the distant past„ÄÇ

To make sense of the world it's operating in„ÄÇAnd finally„ÄÇ

 to make a plan of how to act in that world based on its objectives„ÄÇ

 based on what it wants to accomplish„ÄÇ As I mentioned„ÄÇ

 a simple but commonly accepted definition of intelligence„ÄÇ

Is a system that's able to accomplish complex goals„ÄÇ

So a system that's operating in an environment in this world must have a goal„ÄÇ

 must have an objective functionÔºå a reward function„ÄÇ And based on that„ÄÇ

 it forms a plan and takes action„ÄÇAnd because it operates in many cases in the physical world„ÄÇ

 it must have tools„ÄÇEffectors with which it applies the actions to change something about the world„ÄÇ

That's the full stack„ÄÇOf an artificial dince system that acts in the world„ÄÇAnd the question is„ÄÇ

What kind of task can such a system take onÔºüWhat kind of task can an artificial intelligence system learn as we understand AI todayÔºü

We will talk about the advancement of deeper enforcementment learning approaches in some of the fascinating ways it's able to take much of this stack and treat it as an end to end learning problem„ÄÇ

But we look at games„ÄÇ We look at simpleÔºå formalized worldsÔºå while it's still impressive„ÄÇ

 beautiful and unprecedented accomplishments„ÄÇüòäÔºåIt's nevertheless formal tasks„ÄÇ

Can we then move beyond games and into expert tasksÔºüOf medical diagnosis„ÄÇOf design„ÄÇ

And into natural languageÔºå and finallyÔºå the human level tasks of emotionÔºå imagination„ÄÇConsciousness„ÄÇ

Let's once again review the stack in practicality„ÄÇIn the tools we have„ÄÇ

The input for robots operating in the worldÔºå from cars to humanoid to drones„ÄÇIts LiDarÔºå camera„ÄÇ

 radarÔºå GPSÔºå stereo camerasÔºå audioÔºå microphoneÔºå networking for communication„ÄÇ

 and the various ways to measure kinematics„ÄÇWell I'm you„ÄÇThe raw sensory data„ÄÇIt's then processed„ÄÇ

 Feature are formedÔºå representations are formed and multiple higher and higher order representations„ÄÇ

 That's what deep learning gets us before neural networksÔºå before the advent of„ÄÇ

Before the recent successes of neural networks to go deeper and therefore be able to form high order representations of the data that was done by experts„ÄÇ

 by human experts TodayÔºå networks are able to do that„ÄÇ

 that's the representation piece and on top of the representation piece„ÄÇ

 the final layers these networks are able to accomplish the supervised learning tasks„ÄÇ

 the generative tasks„ÄÇAnd the unsupervised clustering tasks„ÄÇThrough machine learning„ÄÇ

 that's what we talked about a little in lecture one and will continue tomorrow and Wednesday„ÄÇ

That supervised learning„ÄÇAnd you can think about the output of those networks as simpleÔºå clean„ÄÇ

 usefulÔºå valuable information„ÄÇ That's the knowledge„ÄÇAnd that knowledge„ÄÇ

It can be in the form of single numbersÔºå it could be regressionÔºå continuous variables„ÄÇ

 it could be a sequence of numbers„ÄÇ It can be imagesÔºå audio sentencesÔºå text„ÄÇSpeech„ÄÇ

 once that knowledge is extracted and aggregatedÔºå how do we connect it in multireutional ways„ÄÇ

 form hierarchies of ideasÔºå connect ideasÔºüThe trivial silly example is connecting imagesÔºå activity„ÄÇ

 recognition and audioÔºå for example„ÄÇIf it looks like a duck quacks like a duck and swims like a duck„ÄÇ

 we do not„ÄÇCurrently have approaches that effectively integrate this information„ÄÇ

To produce a higher confidence estimate that isÔºå in factÔºå a duck and the planning piece„ÄÇ

The task of taking the sensor informationÔºå fusing the sensor informationÔºå and making action„ÄÇ

 control and longer term plans based on that informationÔºå as we'll discuss today„ÄÇ

Are more and more amenable to the learning approachÔºå to the deep learning approach„ÄÇ

 but to date have been the most successful and non-learning optimization based approaches„ÄÇ

 like with the several of the guest speakers we have„ÄÇ

 including the creator of this robot Atlas in Boston Dynamics„ÄÇSo the question„ÄÇ

 how much of the stack can be learned end to end from the input to the output„ÄÇ

 we know we can learn the representation and the knowledgeÔºå from the representation and to knowledge„ÄÇ

 even with the kernel methods of SVM„ÄÇAndÔºå certainly„ÄÇWith neural networks„ÄÇ

 mapping from representation to information„ÄÇHas been where the primary success in machine learning over the past three decades has been„ÄÇ

Mapping from raw sensory data to knowledge„ÄÇThat's where the success„ÄÇ

 the automated representation learning of deep learning„ÄÇ

Has been a success going straight from raw data to knowledge„ÄÇ

The open question for us today and beyond is if we can expand the red box there of what can be learned end to end„ÄÇ

 from sensory data to reasoningÔºå so aggregatingÔºå forming higher representations of the extracted knowledge„ÄÇ

And forming plans and acting in this world from the raw sensory data„ÄÇ

We will show the incredible fact that we're able to do learn exactly what's shown here and to end with deeper enforcement learning on trivial tasks„ÄÇ

üòäÔºåIn a generalizable wayÔºå the question is whether that can then move on to real world tasks of autonomous vehicles of humanoid„ÄÇ

 roboticsÔºå and so on„ÄÇThat's the open question„ÄÇ So today let's talk about reinforcement learning„ÄÇ

 There's three types of machine learning„ÄÇSupervised„ÄÇ

Unsupervised are the categories at the extremes relative to the amount of human input that's required for supervised learning„ÄÇ

 every piece of data that's used for teaching these systems is first labeled by human beings and unsupervised learning on the right is no data is labeled by human beings„ÄÇ

In between is some sparse input from humansÔºå semisupervised learning is when only part of the data is provided by humans ground truth„ÄÇ

 and the rest must be inferreed generalized by the system„ÄÇ

 and that's where reinforcement learning falls„ÄÇReinforcement learning„ÄÇ

That's shown there with the cats„ÄÇAs I saidÔºå every successful presentation must include cats„ÄÇ

They're supposed to be Pavlov's cats„ÄÇAnd ringing a bell and every time they ring a bell„ÄÇ

 they're given them food and they learn this process„ÄÇThe goal of reinforcement learning is to learn„ÄÇ

From sparse reward dataÔºå from learnÔºå from sparse supervised data„ÄÇ

 and take advantage of the fact that in simulation or in the real world„ÄÇ

 there is a temporal consistency to the worldÔºå there is a temporal dynamics that follows from state to state to state through time„ÄÇ

 and so you can propagate information even if the information that you received about the supervision„ÄÇ

 the ground truth is sparseÔºå you can follow that information back through time to infer something about the reality of what happened before then„ÄÇ

 even if your reward signals are weak„ÄÇSo it's using the fact that the physical world evolvevolves through time in some sort of predictable way to take sparse information and generalize it over the entirety of the experience that's being learned„ÄÇ

So we apply this to two problems„ÄÇ TodayÔºå we'll talk about deep traffic„ÄÇAs a methodology„ÄÇ

 as a way to introduce deeper enforcement learning„ÄÇ

 So deep traffic is a competition that we ran last year„ÄÇAnd expanded significantly this year„ÄÇ

And I'll talk about some of the details and how the folks in this room can on your smartphone today or if you have a laptop„ÄÇ

 train an agent while I'm talkingÔºå training a neural network in the browser„ÄÇ

 some of the things we've added are we've added the capability we've now turned it into a multi-agent deeper reinforcement learning problem where you can control up to 10 cars with a neural network„ÄÇ

Perhaps less significantÔºå but pretty cool is the ability to customize the way the agent looks„ÄÇ

So you can upload and people haveÔºå to an absurd degreeÔºå have already begun doing so„ÄÇ

 uploading different images instead of the car that's shown there„ÄÇ

As long as it maintains the dimensionsÔºå shown here is a SpaceX rocket„ÄÇ

The competition is hosted on the websiteÔºå self driving cars that MITDduu sdeep trafficff we'll return to this later„ÄÇ

The code is on GitHub with some more informationÔºå a starter code„ÄÇ

And a paper describing some of the fundamental insights„ÄÇ

That will help you win at this competition is an archive„ÄÇ

So from supervised learning in lecture 1 to today„ÄÇSupervised learning we can think of as memorization„ÄÇ

Of ground truth data in order to form representations that generalizes from that ground truth„ÄÇ

Reinforcement learning is we can think of as a way to brute force propagate that information„ÄÇ

 the sparse information„ÄÇThrough„ÄÇTime„ÄÇTo assign quality reward to state that does not directly have a reward„ÄÇ

To make sense of this world when the rewards at sparse but are connected through time„ÄÇ

You can think of that as reasoning„ÄÇSo the connection through time„ÄÇIs modeled„ÄÇ

In most reinforcement learning approaches„ÄÇVery simply that there's an agent taking an action in a state and receiving a reward„ÄÇ

And the agent operating in an environment executes an action„ÄÇ

 receives an observed state and new state and receives their award„ÄÇ

 This process continues over and over„ÄÇüòäÔºåIn some examplesÔºå we can think of any of the video games„ÄÇ

 some of which we'll talk about todayÔºå like an Atari breakout as the environment„ÄÇ

 the agent is the paddle„ÄÇEach action that the agent takes has an influence on the evolution of the environment„ÄÇ

And the success is measured by some reward mechanismÔºå in this caseÔºå points are given by the game„ÄÇ

And every game„ÄÇIt has a different point scheme that must be converted„ÄÇ

 normalized into a way that's interpretable by the system„ÄÇAnd the goal is to maximize those points„ÄÇ

 maximize the reward„ÄÇThe continuous problem of cart pole balancing„ÄÇ

 the goal is to balance the pole on top of a moving cart„ÄÇThe state is the angleÔºå the angular speed„ÄÇ

 the positionÔºå the horizontal velocity„ÄÇThe actions are the horizontal force applied to the cart„ÄÇ

 and the reward is one at each time step if the poll is still upright„ÄÇAll the first person shooters„ÄÇ

 the video gamesÔºå and all Starcraft„ÄÇThe strategy games„ÄÇIn case of first person shooter and doom„ÄÇ

 what is the goalÔºå the environment is the gameÔºå the goal is to eliminate all opponents„ÄÇ

 the state is the raw game pixels coming inÔºå the actions is moving upÔºå downÔºå leftÔºå rightÔºå and so on„ÄÇ

 and the reward is positive when eliminating an opponent„ÄÇAnd negative when the agent is eliminated„ÄÇ

Industrial robotics„ÄÇBeen packing with a robotic arm„ÄÇ

 the goal is to pick up a device from a box and put it into a container„ÄÇ

 the state is the raw pixels of the real world that the robot observes„ÄÇ

 the actions or the possible actions of the robot the different degrees of freedom and moving through those degrees moving the different auators to realize the position of the arm and the reward is positive when placing a device successfully and negative otherwise„ÄÇ

Everything could be modeled in this way„ÄÇMarkov decision process as a state as zero action A zero and reward received„ÄÇ

 a new state is achievedÔºå again action and reward stateÔºå action reward state„ÄÇ

 until a terminal state is reached„ÄÇAnd the major components of reinforcement learning is a policy„ÄÇ

Some kind of plan of what to do in every single stateÔºå what kind of action to perform„ÄÇ

A value function„ÄÇAs some kind of sense of what is a good state to be in„ÄÇ

 of what is a good action to take in a state„ÄÇAnd sometimes a model„ÄÇ

That the agent represents the environment with„ÄÇSome kind of sense of the environment it's operating in„ÄÇ

 the dynamics of that environment that's useful for making decisions about actions„ÄÇ

Let's take a trivial example„ÄÇA grid world of 3 by 4Ôºå12 squares„ÄÇ When you start at the bottom left„ÄÇ

And our tasked with walking about this world to maximize reward„ÄÇ

The reward that the top right is a plus one and a one square below that is a negative one„ÄÇ

 and every step you take„ÄÇIs a punishment or is a negative reward of 0„ÄÇ04Ôºü

So what is the optimal policy in this worldÔºüNowÔºå when everything is deterministic„ÄÇ

Perhaps this is the policy„ÄÇWhen you start at the bottom leftÔºå wellÔºå because every step hurts„ÄÇ

 every step has a negative rewardÔºå then you want to take the shortest path to the maximum square with the maximum reward„ÄÇ

When the state space is non deterministic„ÄÇAs presented before with a probability of 0„ÄÇ8„ÄÇ

 when you choose to go upÔºå you go upÔºå but with probability 0„ÄÇ1„ÄÇYou go left and point1Ôºå you go right„ÄÇ

UnfairÔºå againÔºå much like life„ÄÇThat would be the optimal policy„ÄÇ

What is the key observation here that every single state in the space must have a planÔºü

Because you can'tÔºå because then a nondeministic aspect of the control„ÄÇ

 you can't control where you're going to end up„ÄÇ So you must have a plan for every place„ÄÇ

 That's the policyÔºå having an action an optimal action to take in every single state„ÄÇNow„ÄÇ

 suppose we change the reward structure„ÄÇ And for every step we take„ÄÇ

 there's a negative reward is's a negative too„ÄÇ So it really hurts„ÄÇ

 There's a high punishment for every single step we take„ÄÇ So no matter what„ÄÇ

We always take the shortest pathÔºå the optimal policy is to take the shortest path to the only spot on the board that doesn't result in punishment„ÄÇ

If we decrease the reward of each step to negative 0„ÄÇ1„ÄÇThe policy changes„ÄÇ

Where there is some extra degree of wanderingÔºå encouraged„ÄÇ

And as we go further and further in lowering the punishment as before to negative 0„ÄÇ04„ÄÇ

 more wandering and more wandering is allowed„ÄÇAnd when we finally turn the reward„ÄÇInto positive„ÄÇ

 So every step„ÄÇ„Åà„ÄÇEvery step increases their reward„ÄÇThen there's a significant incentive to„ÄÇ

 to stay on the board without ever reaching the destination„ÄÇKind of like college for a lot of people„ÄÇ

So the value functionÔºå the way we think about the value of a state or the value of anything in the environment„ÄÇ

Is the reward we're likely to receive in the future„ÄÇ

And the way we see the reward we're likely to receive is we discount the future award because we can't always count on it„ÄÇ

Here gammaÔºå further and further out into the future„ÄÇ

 more and more discounts decreases the importance of the reward received„ÄÇ

And the good strategy is taking the sum of these rewards and maximizing it„ÄÇ

 maximizing the disccoed future awardÔºå that's what reinforcement learning hopes to achieve„ÄÇ

And with Q learning„ÄÇWe use„ÄÇAny policy to estimate the value of taking an action in a state„ÄÇ

So off policyÔºå forget policy„ÄÇWe move about the world and use the Belllman equation here on the bottom„ÄÇ

To continuously update our estimate of how good a certain action is in a certain state„ÄÇ

So we don't need this this allows us to operate in a much larger state space in a much larger action space„ÄÇ

 we move about this world through simulation or in the real world„ÄÇ

 taking actions and updating our estimate of how good certain actions are over time„ÄÇ

The new state at the left is the is the updated value„ÄÇ

 The old state is the starting value for the equation„ÄÇ

 And we update that old state estimation with the sum„ÄÇ

Of the reward received by taking action action A in state S„ÄÇ

And the maximum reward that's possible to be received in the following states„ÄÇDiscounted„ÄÇ

That update is decreased with the learning rateÔºå the higher the learning rateÔºå the more value we„ÄÇ

 theÔºå the faster we learnÔºå the more value we assign to new information„ÄÇThat's simple„ÄÇ That's it„ÄÇ

 That's Q learning„ÄÇ This simple update rule allows us to„ÄÇTo explore the world„ÄÇ and as we explore„ÄÇ

Get more and more information about what's good to do in this world„ÄÇ

 And there's always a balance in the various problem spaces we'll discuss„ÄÇ

 There's always a balance between exploration and exploitation„ÄÇ

As you form a better and better estimate of the Q function of what actions are good to take„ÄÇ

You start to get a sense of what is the best action to take„ÄÇBut it's not a perfect sense„ÄÇ

 It's still an approximationÔºå and so there's value of exploration„ÄÇ

 but the better and better you estimate becomes the less and less exploration has a benefit„ÄÇ

 so usually want to explore a lot in the beginning and less and less so towards the end and when we finally release the system out into the world and wish it to operate its best then we have it operate as a greedy system„ÄÇ

 always taking optimal action according to the Q value function„ÄÇ

And everything I'm talking about now is parameterized and are parameters that are very important for winning the deep traffic competition„ÄÇ

Which is using this very algorithm with a neural network at its score„ÄÇ

So for simple table representation of a Q function where the y axis is state for statesÔºå S1234„ÄÇ

 and the x axis is actionsÔºå a12Ôºå3Ôºå4„ÄÇWe can think of this table as randomly initiated or initiated„ÄÇ

 initiatedized in any kind of way that's not representative of actual reality and as we move about this world and we take actions we update this table with a bellman equation shown up top and here slides now are online„ÄÇ

 you can see a simple pseudocode algorithm of how to update it of how to run this bellman equation and over time the approximation becomes the optimal Q table The problem is when that Q table becomes exponential in size„ÄÇ

When we take in raw sensor informationÔºå as we do with camerasÔºå with deep crash„ÄÇOr with deep traffic„ÄÇ

 taking the full grid space and taking that informationÔºå the raw grid pixels of deep traffic„ÄÇ

 and when you take the arcade games here that are taking the raw pixels of the game„ÄÇ

Or when we take goÔºå the game of goÔºå when it's taking the unitsÔºå the board„ÄÇ

 the raw state of the board„ÄÇAs the inputÔºå the potential state space„ÄÇ

 the number of possible combinatorial variations of what states as possible is extremely large„ÄÇ

 larger than we can certainly hold the memory and larger than we can ever be able to accurately approximate through the Belmon equation over time„ÄÇ

üò°ÔºåTo simulation„ÄÇThrough to the simple update of the Bellammy equation„ÄÇ

So this is where deep reinforcement learning comes in„ÄÇNeural networks are really good appromators„ÄÇ

 They're really good at exactly this task of learning this kind of Q table„ÄÇüòä„ÄÇ

So as we started with supervised learningÔºå Re networks help us memorize patterns using supervised„ÄÇ

Ground truth data„ÄÇ And we move to reinforcement learning that hopes to propagate„ÄÇ

Outcomes to knowledge„ÄÇDeep learning allows us to do so on much larger state spacesÔºå a much larger„ÄÇ

Action spaces„ÄÇWhich means it's generalizable„ÄÇ It's much more capable to deal with the raw stuff of sensory data„ÄÇ

 which means it's much more capable to deal with a broad variation of real world applications„ÄÇ

And it does so because it's able to learn the representationsÔºå as we discussed„ÄÇOn Monday„ÄÇ

The understanding comes from converting the raw sensor information into simple„ÄÇ

 useful information based on which the action in this particular state can be taken in the same exact way„ÄÇ

 so instead of the Q tableÔºå instead of this Q function we plug in a neural network where the input is the state space„ÄÇ

 no matter how complex„ÄÇAnd the output is a value for each of the actions that you could take„ÄÇ

Input is the state„ÄÇ outputput is the value of the function„ÄÇ It's simple„ÄÇThis is Deep Q NetworkÔºå DQN„ÄÇ

At the core of the success of Deep MindÔºå a lot of the cool stuff you see about video games„ÄÇ

 DQN or variants of DQN are at play„ÄÇThis is what at first with a nature paper„ÄÇDe mind„ÄÇ

The success came of playing the different gamesÔºå including Atari„ÄÇgames„ÄÇ

So how are these things trainedÔºüVery similar to supervised learning„ÄÇThe bellman equation up top„ÄÇ

Takes the reward„ÄÇAnd the discounted expected award from future states„ÄÇ

The loss function here for neural networkÔºå the neural network learns with a loss function„ÄÇ

 It takes the reward received at the current state„ÄÇ

Does the forward pass through a neural network to estimate the value of the future stateÔºüOf the best„ÄÇ

Action to taken in the future state„ÄÇAnd then subtract that„ÄÇ

From the forward pass through the network for the current state in action„ÄÇ

So you take the difference between what your Q estimatorÔºå the neural network„ÄÇ

Believes the value of the current state is„ÄÇAnd what it more likely is to be„ÄÇ

Based on the value of the future states that are reachable based on the actions you can take„ÄÇ

Here's the algorithm„ÄÇInput is the state output is the Q value for each actionÔºå or in this diagram„ÄÇ

 input is the state in actionÔºå and the output is the Q value„ÄÇIt's very similar architectures„ÄÇ

 So given a transition of SÔºå A RÔºå S prime„ÄÇS current State taking an action receiving a reward in achieving S prime State„ÄÇ

TheThe update is do a feed forward pass through the network for the current state„ÄÇ

 do a feed forward pass for each of the possible actions taken in the next state„ÄÇ

 and that's how we compute the two parts of the loss function„ÄÇ

And update the weights using bad propagation„ÄÇAgainÔºå laws function„ÄÇ

 back propagation is how the network is trained„ÄÇThis has actually been around for much longer than deep mind„ÄÇ

A few tricks made it really work„ÄÇExperience replay is the biggest one„ÄÇ

So as the games are played through simulationÔºå or if it's a physical systemÔºå as it acts in the world„ÄÇ

It's actually collecting the observations into a library of experiences and that training is performed by randomly sampling the library in the past„ÄÇ

By randomly sampling the previous experiences and batches„ÄÇ

So you're not always training on the naturalÔºå continuous evolution of the system„ÄÇ

 You're training on randomly picked batches of those experiences„ÄÇ That's a huge„ÄÇ

seems like a subtle trickÔºå but it's a really important one„ÄÇ

So the system doesn't overfi a particular evolution of the game of the simulation„ÄÇAnother important„ÄÇ

AgainÔºå subtle trick as in a lot of deep learning approaches„ÄÇ

 the subtle tricks make all the difference„ÄÇIs fixing the target network„ÄÇFor the loss function„ÄÇ

 if you notice„ÄÇYou have to use the neural networkÔºå theÔºå the single neural network„ÄÇ

 the D Q I network to estimate the value of the current state„ÄÇAn action pair„ÄÇAnd the next„ÄÇ

 so you're using it multiple times„ÄÇAnd as you perform that operationÔºå you're updating the network„ÄÇ

 which means the target function inside that loss function is always changing„ÄÇ

 so the very nature of your loss function is changing all the time as you're learning„ÄÇüò°„ÄÇ

And that's a big problem for stability„ÄÇThat can create big problems to the learning process„ÄÇ

 So this little trick is to fix the network and only update it„ÄÇEveryÔºå sayÔºå thousand steps„ÄÇ

So as you train the networkÔºå the network that's used to compute the target function inside the loss function is fixed„ÄÇ

It produces a more stable computational loss function„ÄÇ

 So the ground doesn't shift under you as you're trying to find a minimal„ÄÇFor the loss function„ÄÇ

 the loss function doesn't change in unpredictableÔºå difficult to understand ways„ÄÇAnd reward clipping„ÄÇ

Which is always true with general systems that are operating seeking to operate in a generalized way is for very„ÄÇ

 for these various games„ÄÇThe points are differentÔºå some points are lowÔºå some points are high„ÄÇ

 some go positive and negativeÔºå and they're all normalized to a point where the good points or the positive points are a one and negative points are a negative one that's reward clipping„ÄÇ

 simplify the reward structure„ÄÇAnd because a lot of the games are 30 FPS or 60 FPS and the actions„ÄÇ

It's not valuable to take actions at such a high rate inside of these particularly Atari games„ÄÇ

 that you only take an action every four steps while still taken in the frames as part of the temporal window to make decisions„ÄÇ

TricksÔºå but hopefully it gives you a sense„ÄÇOf the kind of things necessary for both„ÄÇ

Seminnal papers like this one„ÄÇ And for the more important accomplishment of winning deep traffic„ÄÇ

s the tricks make all the differenceÔºüHere on the bottom is„ÄÇ

The circle is when the technique is used and excellent it's not looking at replay and target„ÄÇ

 fixed target network and experience replay when both are used for the game of breakoutÔºå riverade„ÄÇ

 Seaquest and space invadersÔºå the higher the numberÔºå the better it isÔºå the more points achieved„ÄÇ

So when it gives you a sense that when replay and target both gives significant improvements in the performance of the system„ÄÇ

Order of magnitude improvementsÔºå two orders of magnitude for breakout„ÄÇAnd here is„ÄÇ

Sudocode of implementing DQNÔºå the learning„ÄÇThe key thing to noticeÔºå and you can look to the slides„ÄÇ

Is„ÄÇthe loopÔºå the wild loop of playing through the games and selecting the actions to play„ÄÇ

Is not part of the training„ÄÇ It'sÔºå it's part of the saving„ÄÇThe observationsÔºå the state action reward„ÄÇ

 next state observations and saving them into replay memory into that library„ÄÇ

 and then you sample randomly from that replay memory to then train the network„ÄÇ

Based on the loss function„ÄÇAnd with probability up up top with the probabilityÔºå Epsilon„ÄÇ

 selected random actionÔºå that epsilon is the probability of exploration that decreases at something you'll see in deep traffic as well„ÄÇ

Is the rate at which that exploration decreases over time through the training process„ÄÇ

 you want to explore a lot first„ÄÇ

![](img/731ed94783f15549cd0d4044b14ddb26_3.png)

And less and less over time„ÄÇSo this algorithm that's being able to accomplish„ÄÇIn 2015„ÄÇ

 and since a lot of incredible things„ÄÇThings that made„ÄÇThe AI world„ÄÇThink that we„ÄÇ

We're onto something„ÄÇThat general AI is within reach„ÄÇ

It's for the first time that raw sensor information was used to create a system that acts and makes sense of the world„ÄÇ

 makes sense of the physics of the world enough to be able to succeed in it from very little information„ÄÇ

But these games are trivial„ÄÇEven though there is a lot of them„ÄÇ



![](img/731ed94783f15549cd0d4044b14ddb26_5.png)

This DQN approach has been able to outperform a lot of the Atari games„ÄÇ

 That's what's been reported onÔºå outperform the human level performance„ÄÇBut again„ÄÇ

 these games are trivial„ÄÇWhat I think„ÄÇAnd perhaps biasedÔºå I am biased„ÄÇ

 but one of the greatest accomplishments of artificial intelligence in the last decade„ÄÇüòä„ÄÇ

At least from the philosophical or the research perspective„ÄÇIs„ÄÇAlpha goes 0„ÄÇFirstÔºå alphago„ÄÇ

 and then alphago 0„ÄÇIs Deep Mind system that beat the best in the world in the game of go„ÄÇ

So what's the game of Go„ÄÇIt's simple„ÄÇI won't get into the rulesÔºå but basically it's a 19 by 19 board„ÄÇ

Shown on the bottom of the slide„ÄÇFor the bottom row of the tableÔºå for a board of 19 by 19„ÄÇ

The number of legal game positions is two times 10 to the power of 170„ÄÇ

It's a very large number of possible positions to consider„ÄÇ And at any one time„ÄÇ

 especially the game evolvesÔºå the number of possible moves is huge„ÄÇMuch larger than in chess„ÄÇ

So that's why AI„ÄÇThe community thought that this game is not solvable„ÄÇUntil 2016„ÄÇ

When Alpha Go used to used human expert position play„ÄÇTo seed in a supervised way„ÄÇ

 reinforcement learning approach„ÄÇAnd I'll describe it in a little bit of detail in the couple of slides here„ÄÇ

To beat the best in the world„ÄÇAnd then alpha goes0Ôºå that is„ÄÇThe accomplishment of the decade„ÄÇ

For me in AI„ÄÇIs being able to play„ÄÇWith no„ÄÇTraining data on human expert„ÄÇGames„ÄÇ

And beat the best in the world in an extremely complex game„ÄÇ This is not aari„ÄÇ

This is a much higher order difficulty game and the quality of players that is competing in is much higher„ÄÇ

 and it's able to extremely quickly hear„ÄÇTo achieve a rating that's better than alpha go„ÄÇ

And better than the different variant of Algo and certainly better than the best of the human players in 21 days„ÄÇ

Of self play„ÄÇSo how does it workÔºüAll of these approachesÔºå muchÔºå much like the previous ones„ÄÇ

 the traditional ones that are not based on deep learning„ÄÇAre using Monte Carlo T searchÔºå MCTS„ÄÇ

Which is„ÄÇWhen you have such a large state spaceÔºå you start at a board and you play„ÄÇ

And you choose moves„ÄÇWith some exploitation exploration balancing„ÄÇ

Choosing to explore totally new positions or to go deep in the positions you know are good until the bottom of the game is reached until the final state is reached„ÄÇ

 And then you back propagate the quality of the choices you made leading to that position„ÄÇ

 And in that wayÔºå you learn the value of„ÄÇOf board positions and play„ÄÇ

That's been used by the most successful go playing„ÄÇEngines before and alphagos„ÄÇ

But you might be able to guessÔºå what's the difference with Algo versus the previous approaches„ÄÇ

They use the neural network„ÄÇAs the intuitionÔºå quote unquoteÔºå to what are the good states„ÄÇ

 what are the good next board positions to exploreÔºüAnd the key thingsÔºå again„ÄÇ

 the tricks make all the difference„ÄÇThat made alpha go zero„ÄÇ

Work and work much better than Alphago is first because there was no expert play instead of human games„ÄÇ

Alphago used that very same Monte Carlo T search algorithmÔºå MCTSÔºå to do an intelligent look ahead„ÄÇ

Based on the neural network prediction of what are the good states to take„ÄÇ

It checked that instead of human expert playÔºå it checked how good indeedÔºå are those states„ÄÇ

It's a simple look ahead action that does the ground truth„ÄÇ

 that does the target correction that produces the loss function„ÄÇ

 The second part is the multitask learning or what's not called multitask learning is the network is quote unquote  two headed„ÄÇ

In the sense that first it outputs the probability of which move to take the obvious thing and it's also producing a probability of winning„ÄÇ

 and there's a few ways to combine that information and continuously train both parts of the network„ÄÇ

 depending on the choice taken so you want to take the best choice in the short term and achieve the positions that are highly likelihood of winning for the player that' whose turn it is„ÄÇ

And another big step„ÄÇIs that they updated from 2015Ôºå they updated the state of the art architecture„ÄÇ

 which are now the architecture that one imagenet is residual networksÔºå RasNe for IageNe„ÄÇThat's it„ÄÇ

 and those little changes made all the difference„ÄÇSo that takes us through deep traffic and the $8 billion hours stuck in traffic„ÄÇ

America's past time„ÄÇ So we tried to simulate„ÄÇDriving the behavior layer of driving„ÄÇ

 so not the immediate controlÔºå not the motion planning„ÄÇ

 but beyond that on top on top of those control decisions„ÄÇ

 the human interpretable decisions of changing lane or speeding up slowing down„ÄÇ

 modeling that in a micro traffic simulation framework that's popular in traffic engineering„ÄÇ

 the kind of shown here„ÄÇWe applied deeper reinforce learning to that„ÄÇWe call it deep traffic„ÄÇ

The goal is to achieve the highest average speed over a long period of time„ÄÇ

Weaving in and out of traffic„ÄÇFor students hereÔºå the requirement is to follow the tutorial and achieve a speed of 65 miles an hour„ÄÇ

And if you really want to achieve a speed over 70 miles an hourÔºå which is what's required to win„ÄÇ

And perhaps upload your own image„ÄÇTo make sure you look good doing it„ÄÇWhat you should do„ÄÇ

 clear instructions to competeÔºå read the tutorial„ÄÇYou can change parameters in the code box on that website„ÄÇ

 cars„ÄÇmtDu s deep trafficÔºå click the white button that says apply code„ÄÇ

 which applies the code that you write„ÄÇ these are the parameters that you specify for the neural network„ÄÇ

It applies those parametersÔºå creates the architecture that you specify„ÄÇ

 and now you have a network written in JavaScript living in the browser ready to be trained„ÄÇ

 Then you click the blue button that says run training„ÄÇAnd that trains the network„ÄÇ

Much faster than what's actually being visualized in the browser„ÄÇ

1 thousand times faster by evolving the gameÔºå making decisionsÔºå taking in the grid space„ÄÇ

 as I'll talk about here in a secondÔºå the speed limit is 80 miles an hour„ÄÇ

Based on the various adjustments we made to the game„ÄÇ

Reaching 80 miles an hour is certainly impossibleÔºå on average„ÄÇ

And reaching some of the speeds that we've achieved last year is muchÔºå muchÔºå much more difficult„ÄÇ

FinallyÔºå when you're happy and the training is done„ÄÇSubmit the model to competition„ÄÇ

For those super eagerÔºå dedicated studentsÔºå you can do so every five minutes„ÄÇ

And to visualize your submission„ÄÇYou can click the request visualization specifying the custom image and the color„ÄÇ

OkayÔºå so here's the simulationÔºå speed limit 80 miles an hour„ÄÇCars 20 on the screen„ÄÇ

 One of them is a red one„ÄÇ in this case„ÄÇs that one is controlled by neural network„ÄÇ

s speed it's allowed„ÄÇ the actions are speed upÔºå slow downÔºå change lanesÔºå leftÔºå right„ÄÇ

 or stay exactly the same„ÄÇThe other cars are pretty dumb„ÄÇ They speed upÔºå slow downÔºå turn leftÔºå right„ÄÇ

 but they don't have a purpose in their existence„ÄÇ They do so randomly„ÄÇ

Or at least purpose has not been discovered„ÄÇThe roadÔºå the car is a speed„ÄÇ The road is a grid space„ÄÇ

An occupancy grid that specifies„ÄÇWhen it's emptyÔºå it's set to„ÄÇAbeÔºå meaning that„ÄÇ

The grid value is whatever speed is achievable if you are inside that grid„ÄÇ

And when there's other cars that are going slowÔºå the value in that grid is the speed of that car„ÄÇ

 That's the state space„ÄÇ That's the state representation„ÄÇ And you can choose how much„ÄÇ

 what slice of that state space you take in„ÄÇ That's the input to the neural network„ÄÇ

For visualization purposesÔºå you can choose normal speed or fast speed for watching the network operate„ÄÇ

And there's display options to help you build intuition about what the network takes in and what space that car is operating in„ÄÇ

 the default is no extra information is addedÔºå then there's the learning input„ÄÇ

 which visualizes exactly which part of the road serves as the input to the network„ÄÇ

 then there is the safety system which I'll describe in a little bit„ÄÇ

 which is all the parts of the road the car is not allowed to go into because it would result in a collision„ÄÇ

 and that JavaScript would be very difficult to animate„ÄÇAnd the full map„ÄÇHere's a safety system„ÄÇ

 You could think of this system as„ÄÇACC basic radarÔºå ultrasonic sensors„ÄÇ

 helping you avoid the obvious collisions to obviously the detectable objects around you and the task for this red car for this neural network is to move about this space„ÄÇ

Is to move about the space under the constraints of the safety system„ÄÇ

The red shows all the parts that the grid is not able to move into„ÄÇ

So the goal for the car is to not get stuck in traffic„ÄÇ

Is make big sweeping motions to avoid crowds of cars„ÄÇThe input like DQN is the state space„ÄÇ

 the output is the value of the different actionsÔºå and based on the epsilon parameter through training and through inference evaluation process„ÄÇ

You choose how much exploration you want to do„ÄÇ These are all parameters„ÄÇ

The learning is done in the browser„ÄÇOn your own computer„ÄÇUtilizing only this CPU„ÄÇThe action space„ÄÇ

 there's five giving you some of the variables hereÔºå perhaps you go back to the slides to look at it„ÄÇ

The brain quoteÔºå unquote„ÄÇIs the thing that takes in the stateÔºüAnd the reward„ÄÇ

Takes a forward pass to the state and produces the next action„ÄÇ

The brain is where the neural network is containedÔºå both for the training and the evaluation„ÄÇ

The learning input can be controlled in width„ÄÇForward length and backward length„ÄÇ

 lane side number of lanes to the side that you see patches ahead is the patches ahead that you see„ÄÇ

 patches behind as patches behind that you see„ÄÇNew this year can control the number of agents„ÄÇ

They are controlled by the neural network„ÄÇAnywhere from one to 10„ÄÇ

And the evaluation is performed exactly the same way„ÄÇ

 You have to achieve the highest average speed for the agents„ÄÇ

The very critical thing here is the agents are not aware of each other„ÄÇ

So they're not jointly planning„ÄÇThe network is trained under the joint objective of achieving the average speed for all of them„ÄÇ

But the actions are taking in a greedy way for each„ÄÇ

It's very interesting what can be learned in this way„ÄÇ

Because these kinds of approaches are scalable to an arbitrary number of cars„ÄÇ

 and you can imagine us plopping down the best cars from this class together„ÄÇAnd having them compete„ÄÇ

In this wayÔºå the best neural networks„ÄÇBecause they're full in their greedy operation„ÄÇ

The number of networks that can concurrently operate is fully scalable„ÄÇThere's a lot of parameters„ÄÇ

The temporal window„ÄÇThe layersÔºå the many layers types that can be added„ÄÇ

 Here is a fully connected layer with 10 neuronsÔºå The activation functions„ÄÇ

 All of these things can be customizedÔºå as is specified in the tutorial„ÄÇThe final layer„ÄÇ

 a fully connected layer with„ÄÇI'll put a five regression„ÄÇ

 given the value of each of the five actions„ÄÇAnd there's a lot of more specific parameters„ÄÇ

 some of which I've discussed„ÄÇFrom gamma to epsilon„ÄÇTo experience replay size„ÄÇ

To learning rate and temporal window„ÄÇThe optimizerÔºå the learning rateÔºå momentumÔºå batch size„ÄÇ

 L2 L1 decay K for regularization and so on„ÄÇ there's a big white button that says apply code that you press that kills all the work you've done up to this point„ÄÇ

 so be careful doing it„ÄÇ you should be doing it only at the very beginning„ÄÇ

If you happen to leave your computer running and training for several daysÔºå as folks have done„ÄÇ

 the blue training buttonÔºå you press and it trains based on the parameters you specify„ÄÇ

And the network state gets shipped to the main simulation from time to time„ÄÇ

 So the thing you see in the browser as you open up the website is running the same network that's being trained and regularly it updates that network so it's getting better and better„ÄÇ

 even if the training takes weeks for youÔºå it's constantly updating the network you see on the left„ÄÇ

 So if the car for the network that you're training is just standing in place and not moving„ÄÇ

That's probably„ÄÇTime to restart and change the parametersÔºå maybe add a few layers to your network„ÄÇ

Number of iterations is certainly an important parameter to control„ÄÇ

And the evaluation is something we've done a lot of work on since last year to remove the degree of randomness„ÄÇ

 to remove the„ÄÇThe incentive to submit the same code over and over again to hope to produce a higher evaluation score„ÄÇ

The method for evaluation is to collect the average speed over 10 runs„ÄÇ

 about 45 seconds of game eachÔºå not minutesÔºå 45 simulated seconds„ÄÇAnd there is 50s of those„ÄÇ

 And we take the median speed of the 500 runs„ÄÇ It's done server side„ÄÇ

 So extremely difficult to cheat„ÄÇ I urge you to try„ÄÇYou can try it locally„ÄÇ

 There's a start evaluation runÔºå but that one doesn't count„ÄÇ

 That's just for you to feel better about your network„ÄÇThat„ÄÇ

 that should produce a result that's very similar to the one we produce on the server„ÄÇ

It's to build your own intuition„ÄÇAnd as I saidÔºå we significantly reduced the influence of randomness„ÄÇ

 so the scoreÔºå the speed you get for the network you design should be very similar with every evaluation„ÄÇ

Loading and savingÔºå if the network is huge and you want to switch computers„ÄÇ

 you can save the networkÔºå it saves both the architecture of the network and the weights on the network„ÄÇ

 and you can load it back in„ÄÇObviouslyÔºå when you load it in„ÄÇ

 it's not saving any of the data you've already done„ÄÇ

 You can't do transfer learning or JavaScriptscript in the browser yet„ÄÇSubmitting your network„ÄÇ

 submit model to competitionÔºå and make sure you run training firstÔºå otherwise„ÄÇ

It will be the way to initiate randomly and it will not do so well„ÄÇ

You can resubmit as often you like and the highest score is what counts„ÄÇ

The coolest part is you can load your custom imageÔºå specify colorsÔºå and request to visualization„ÄÇ

We have not yet shown the visualizationÔºå but I promise you it's going to be awesome again„ÄÇ

 read the tutorialÔºå change the parameters in the code boxÔºå click apply codeÔºå run training„ÄÇ

 everybody in this room on the way homeÔºå on the train hopefully not in your car should be able to do this in the browser„ÄÇ

And then you can visualizeÔºå request visualization because it's an expensive process„ÄÇ

 You have to want it for us to do it because we have to run in serverside„ÄÇCompetition link is there„ÄÇ

 GitHub starter code is thereÔºå and the details for those that truly want to win is in the archive paper„ÄÇ

So the question„ÄÇThat will come up throughout is whether these reinforcement learning approaches are at all„ÄÇ

 or rather if action planning control is amenable to learning„ÄÇCertainly in the case of driving„ÄÇ

 we can't do what Alpha goes zero did„ÄÇWe can't learn from scratch from self play„ÄÇ

 because that would result in millions of crashes in order to learn„ÄÇTo avoid the crashes„ÄÇ

Unless we're working like we are a deep crash on the RC car or we're working in simulation„ÄÇ

So we can look at expert data„ÄÇ We can look at driver dataÔºå which we have a lot of and learn from„ÄÇ

 It's an open questionÔºå where this is applicable„ÄÇ

![](img/731ed94783f15549cd0d4044b14ddb26_7.png)

To date„ÄÇAnd I bring up two companies because they're both guest speakers„ÄÇ

DRL is not involved in the most successful robots operating in the real world„ÄÇ

In the case of Boston dynamicsy„ÄÇMost of the perceptionÔºå control and planningÔºå like in this robot„ÄÇ

 does not involve learning approaches except with minimal addition on the perception side„ÄÇ

Best of our knowledge„ÄÇAnd certainly the same is true with Wayma as the speaker on Friday will talk about„ÄÇ

 deep learning is used a little bit in perception on top„ÄÇ

 but most of the work is done from the sensors and the opt basedÔºå the model based approaches„ÄÇ

 trajectory generation and optimizing which trajectory trajectory is best to avoid collisions„ÄÇ

DR is not involved„ÄÇAnd coming back and back again„ÄÇThe unexpected local pockets of higher war„ÄÇ

 which arise in all of these situations when applied in the real world„ÄÇSo for the cat video„ÄÇ

 that's pretty short where the cats are ringing the bell and they're learning that the ring in the bell is mapping to food„ÄÇ

I urge you to think about how that can evolve over time in unexpected ways that may not have a desirable effect„ÄÇ

Where the final award is in the form of food„ÄÇAnd„ÄÇThe intended effect is to ring the bell„ÄÇ



![](img/731ed94783f15549cd0d4044b14ddb26_9.png)

That's where AI safety comes in for the artificial General intelligencetellig course in two weeks„ÄÇ

 that's something we'll explore extensively„ÄÇIt's how these reinforcement learning planning algorithms will evolve in ways that're not expected„ÄÇ

And how we can constrain themÔºå how we can design reward functions that result in safe operation„ÄÇ

So I encourage you to come to the talk on Friday at 1 pm as a reminderÔºå it's at 1 pm not 7 p„ÄÇ

m in StataÔºå 32123Ôºå and to the awesome talks in two weeks from Boston Dynamics to Ray Kurzweil and so on for AGI„ÄÇ



![](img/731ed94783f15549cd0d4044b14ddb26_11.png)

NowÔºå tomorrowÔºå we'll talk about computer vision and psych views„ÄÇ Thank youÔºå everybody„ÄÇ



![](img/731ed94783f15549cd0d4044b14ddb26_13.png)