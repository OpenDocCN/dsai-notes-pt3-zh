# P7ÔºöL7- Áî®‰∫éÂÖ®È©æÈ©∂‰ªªÂä°Á´ØÂà∞Á´ØÂ≠¶‰π†ÁöÑÂç∑ÁßØÁ•ûÁªèÁΩëÁªú - ShowMeAI - BV1Y34y1i7vC

![](img/a3ea2eb7da7b24a9c2c132cab561a980_0.png)

All rightÔºå welcome back everyone„ÄÇSound okayÔºå all right„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_2.png)

So today we willÔºå we talked a little bit about„ÄÇNeural networks started to talk about neural networks yesterday„ÄÇ

TodayÔºå will continue„ÄÇTo talk about neural networks that work with images„ÄÇ

 convolutional neural networks and see how those„ÄÇTypes of networks can help us drive a car„ÄÇ

If we have timeÔºå we'll cover a simple illustrative case study of detecting„ÄÇTraffic lights„ÄÇ

The problem of detecting greenÔºå yellowÔºå red„ÄÇIf we can't teach our neural networks to do that„ÄÇ

 we're in trouble„ÄÇBut it's a good clear illerative case study of a three class classification problem„ÄÇ

OkayÔºå next there's„ÄÇDeep Tesla„ÄÇHereÔºå looped over and over in a very short g„ÄÇ

This is actually running live on a website right now we'll show it towards the end of the lecture„ÄÇ

 this once againÔºå just like deep traffic is a neural network that learns to steer a vehicle based on the video of the forward roadway„ÄÇ

And once againÔºå doing all of that in the browserÔºå using jascript„ÄÇ

So you'll be able to train your own very network to drive using real world data„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_4.png)

I'll explain how„ÄÇYou will also have a tutorial„ÄÇAnd code„ÄÇ

Briefly describe today at the end of the lectureÔºå there's time„ÄÇ

How to do the same thing in Tensorflow„ÄÇ So if you want to build a network that's bigger„ÄÇDeeper„ÄÇ

 and you want to utilize GPU to train that network„ÄÇ You want to not do it in your browser„ÄÇ

 You want to do it offline using Tensorflow and having a powerful GPU on your computer„ÄÇ

 And we'll explain how to do that„ÄÇComputer vision„ÄÇSo we talked about„ÄÇVanilla machine learning„ÄÇ

 where there is no or the size yesterday„ÄÇWhere the size of the input is small„ÄÇFor the most part„ÄÇ

The number of neurons in the case the neural networks is on the order of 10Ôºå 100Ôºå1000„ÄÇ

When you think of imagesÔºå images are a collection of pixels„ÄÇ

One of the most iconic images from computer vision and the bottom left there is Lenna„ÄÇ

I encourage you to Google it and figure out the story behind that image„ÄÇ

Is quite shocking when I found out recently„ÄÇSo once againÔºå computer vision„ÄÇIs these days„ÄÇ

Dominated by data driven approaches„ÄÇBy machine learning„ÄÇWhere all of the same methods„ÄÇ

That are used on other types of data are used on images where the input is just a collection of pixels„ÄÇ

 and pixels are„ÄÇNumbers from 0 to 2Ôºå55Ôºå discrete values„ÄÇ

So we can think exactly what we've talked about previously„ÄÇ

 We can think of images in the same exact way„ÄÇ It's just numbers„ÄÇ

And so we can do the same kind of thingÔºå we can do supervised learning where you have an input image„ÄÇ

 an output label„ÄÇThe input image here is a picture of a womanÔºå the label might be woman„ÄÇ

On supervised learningÔºå same thing„ÄÇ We'll look at that briefly as well„ÄÇ

Is clustering images into categories„ÄÇAgain semiupervised and reinforcement learning„ÄÇ In fact„ÄÇ

 the Atari Game we talked about yesterday„ÄÇDo some preproces on the images„ÄÇ

 They're doing computer vision„ÄÇ They're using convolutional neural networksÔºå as we'll discuss today„ÄÇ

And the pipeline for supervised learning is again the same„ÄÇ There's raw data in the form of images„ÄÇ

 There's labels on those images„ÄÇWe perform a machine learning algorithm„ÄÇ

 performance feature extraction„ÄÇIt trains given the inputs and outputs„ÄÇ

On the images and the labels of those images construct the model and then test that model„ÄÇ

 and we get a metric and accuracy„ÄÇ Ac is the term that's used to often describe how well a model performs„ÄÇ

It's a percentage„ÄÇI apologize for the constant presence of cats throughout this course„ÄÇ

 I assure you this course is about drivingÔºå not cats„ÄÇBut images are numbers„ÄÇSo for us„ÄÇ

 we take it for granted„ÄÇWe're really good at looking at and converting visual perception as human beings„ÄÇ

 converting visual perception into semantics„ÄÇWe see this image and we know it's a cat„ÄÇ

But a computer only sees numbers„ÄÇRGB values for colored image„ÄÇ

There's three values for every single pixel„ÄÇFrom 0 to 255„ÄÇAnd so given that image„ÄÇ

 we can think of two problems„ÄÇ One is regression and the other is classification„ÄÇ

Regression is when given an imageÔºå we want to produce a real valued output back„ÄÇ

 So if we have an image of the forward roadwayÔºå we want to produce a value for the steering wheel angle„ÄÇ

And if you have an algorithm that's really smart„ÄÇIt can take any image of the forward roadway and produce the perfectly correct steering angle that drives the car safely across the United States„ÄÇ

üòäÔºåWe'll talk about how to do that and where that fails„ÄÇClassification is when the input again„ÄÇ

 is an image and the output„ÄÇIs a class labelÔºå a discrete class label„ÄÇUnderneath itÔºå though„ÄÇ

 often is still a regression problem„ÄÇ And what's produced is a probability that this particular image belongs to a particular category„ÄÇ

And we use a threshold to chop off„ÄÇThe outputs associated with low probabilities„ÄÇ

And take the labels associated with the high probabilities and convert it into a discrete classification„ÄÇ

I mentioned this yesterdayÔºå but it bears saying againÔºå computer vision is hard„ÄÇ

We once again take it for granted„ÄÇ as human beings„ÄÇ

 we're really good at dealing with all of these problems„ÄÇ There's viewpoint variation„ÄÇ

 The object looks totally different in terms of the numbers of the behind the images„ÄÇ

 in terms of the pixels when viewed from a different angleÔºå viewpoint variation„ÄÇObjects„ÄÇ

 when you're standing far away from them are up close are totally different size„ÄÇ

 We're good at detecting that they're different size It's still the same object as human beings„ÄÇ

 but that's still a really hard problem because those sizes can vary drastically„ÄÇ

We talked about occlusions and deformations with cats„ÄÇWell understood problem„ÄÇ

There's background clutter„ÄÇYou have to separate the object of interest from the background„ÄÇ

And given the three dimensional structure of our world„ÄÇ

 there's a lot of stuff often going on in the background„ÄÇThe clutter„ÄÇ

There are inter class variation that's often greater than inter class variation„ÄÇ

 meaning objects of the same type often have more variation than the objects that you're trying to„ÄÇ

Se them from„ÄÇThere's the hard one for drivingÔºå illumination„ÄÇLight is the way we perceive things„ÄÇ

 The reflection of light off the surface„ÄÇAnd the source of that light changes the way that object appears„ÄÇ

 And we have to be robust to all of that„ÄÇSo the image classification pipeline„ÄÇIt's the same„ÄÇ

 as I mentioned„ÄÇThere's categories„ÄÇIts a classification problems„ÄÇ So there's categories of catÔºå dog„ÄÇ

 mugÔºå hat„ÄÇ And you have a bunch of examplesÔºå image examples of each of those categories„ÄÇ

 And so the input„ÄÇIt's just those images paired with the category„ÄÇAnd you train„ÄÇTo map„ÄÇ

 to estimate a function that maps from the images to the categories„ÄÇFor all of thatÔºå you need data„ÄÇ

A lot of it„ÄÇThere isÔºå unfortunately„ÄÇThere's a growing number of data sets„ÄÇ

 but they're still relatively small„ÄÇWe get excited„ÄÇ There are millions of images„ÄÇ

 but there are not billions or trillions of images„ÄÇAnd these are„ÄÇ

The data sets that you will see if you read academic literature most often„ÄÇEmist„ÄÇ

 the one that's been beaten to death„ÄÇAnd then we use as well in this course„ÄÇ

Is a data set of handwritten digits„ÄÇWhere the categories is zero to9„ÄÇImagenet„ÄÇ

 one of the largest image data sets„ÄÇFully labeled image data sets in the world„ÄÇ

Has images with a hierarchy of categories from Wardnet„ÄÇ

And what you see there is a labeling of what images associated with which words are present in the data„ÄÇ

Car 10 and Car 100 are tiny images„ÄÇThat are used to prove in a very efficient and quick way Offhand that your algorithm that you're trying to publish on or trying to impress the world with works well„ÄÇ

It's small„ÄÇ It's a small data set„ÄÇ C far 10 means there's 10 categories„ÄÇ

And places is a data set of natural scenesÔºå woodsÔºå nature„ÄÇCityÔºå so on„ÄÇ

 So let's look at C far1 as a data set of 10 categoriesÔºå airplaneÔºå automobileÔºå birdÔºå catÔºå so on„ÄÇ

They're shown there with sample images as the rows„ÄÇ

And so let's build a classifier that's able to take images from one of these 10 categories and tell us what„ÄÇ

Is shown in the image„ÄÇ So how do we do that„ÄÇOnce againÔºå all the algorithm sees as numbers„ÄÇ

So we have to try„ÄÇTo haveÔºå at the very coreÔºå we have to have an operator for comparing two images„ÄÇ

So given an imageÔºå and I want to say if it's a cat or a dog„ÄÇ

 I want to compare it to images of cats and compare it to images as dogs and see which one matches better„ÄÇ

 So there has to be a comparative operator„ÄÇOkayÔºå so one way to do that is take the absolute difference between the two images„ÄÇ

 pixel by pixel„ÄÇTake the difference between each individual pixel„ÄÇ

Shown on the bottom of the slide for a4 by4 image„ÄÇAnd then we sum that pixel Y„ÄÇ

 Pix Y is absolute difference until a single number„ÄÇ So if the image is totally different„ÄÇ

 pixel wise„ÄÇThere'll be a high number„ÄÇ If it's the same imageÔºå the number will be 0„ÄÇOh„ÄÇ

 it's the absolute valueÔºå tooÔºå of the difference„ÄÇAnd that's called L1 distance„ÄÇDoesn't matter„ÄÇ

When we speak of distanceÔºå we usually mean L2 distance„ÄÇAnd soÔºå if we„ÄÇTry to„ÄÇ

 so we can build a classifier that just„ÄÇUses this operator to compare to every single image in the data and say„ÄÇ

 I'm gonna pick the„ÄÇI'm going to pick the category that's the closest using this comparative operator„ÄÇ

I'm going to find„ÄÇ I have a picture of a cat„ÄÇ and I'm going look through the data set and find the image that's the closest to this picture„ÄÇ

And say that is the category that this picture belongs to„ÄÇSo if we just flip the coin„ÄÇ

And randomly pick which category an image belongs to will get that accuracy„ÄÇWould be on averageÔºå 10%„ÄÇ

Tandom„ÄÇThe accuracy we achieve with our„ÄÇBrilliant„ÄÇImage difference algorithm that just goes to the data set and finds the closest one„ÄÇ

Is 38%„ÄÇIt is pretty good„ÄÇ It's way above 10%„ÄÇSo you can think about this operation of looking through the data and finding the closest image as„ÄÇ

What's called Kaier's neighbors or K in that case is„ÄÇ

Meaning you find the one closest neighbor to this image that you're asking a question about„ÄÇ

And accept the label from that image„ÄÇ it could do the same thingÔºå increasing K„ÄÇ

KInc k to 2 means you take the two nearest neighbors„ÄÇ

You find the two closest in terms of pixelix Y image difference images to this particular query image„ÄÇ

And find which category do those belong toÔºüWhat's shown up top on the left is the data set we're working with„ÄÇ

 redÔºå greenÔºå blue„ÄÇWhat's shown in the middle is the one nearest neighbor classifierÔºå meaning„ÄÇ

This is how you segment the entire space of different things that you can compare„ÄÇ

And if a point falls into any of these regionsÔºå it will be immediately associated with the nearest neighbor algorithm to belong to that image„ÄÇ

ToÔºå to that region„ÄÇWith five nearest neighborsÔºå there's immediately an issue„ÄÇ

The issue is that theres white regionsÔºå there's tide breakers„ÄÇWhere your five closest neighbors„ÄÇ

Are from various categoriesÔºå so it's unclear what you belong to„ÄÇSo if we„ÄÇ

 this is a good example of parameter tuningÔºå you have one parameter k„ÄÇ

And you have to your task as a machineÔºå as a teacher of machine learning„ÄÇ

You have to teach this algorithm how to do your learning for you„ÄÇ

Is to figure out that parameter that's called parameter tuning or hyperparameter tuning„ÄÇ

 as it's called in neural networks„ÄÇAnd so on the bottom right of the slide is on the x axis is k as we increase it from 0 to 100„ÄÇ

And on the Y axis is classification accuracy„ÄÇ It turns out that the best K for this data set is 7„ÄÇ

7 years neighbors„ÄÇ with thatÔºå we get a performance of„ÄÇ30%„ÄÇHuman level performance„ÄÇ

And I should say that the way that number isÔºå we get that number„ÄÇ

 as we do with a lot of the machine learning pipeline„ÄÇProcess is you separate the data„ÄÇ

Into the parts of the data set you use for training and another part they use for testing„ÄÇ

You're not allowed to touch the testing part„ÄÇAs's cheating„ÄÇ

You construct your model of the world on the training data„ÄÇ

And you use what's called cross validation„ÄÇWill you take a small partÔºü

Of the training data shown fold 5 there in yellow„ÄÇTo leave that part out from the training„ÄÇAnd then„ÄÇ

 use it„ÄÇAs part of the hyperparameter tuning„ÄÇAs you train„ÄÇ

 figure out with that yellow part fold fiveÔºå how well you're doing„ÄÇ

And then you choose a different fold and see how well you're doing and keep playing with parameters„ÄÇ

 never touching the test part„ÄÇAnd when you're ready„ÄÇ

 you run the algorithm of test data to see how well you really do„ÄÇ

 how well it really generalizes Yes question„ÄÇIntuition„ÄÇOr do you just have to run„ÄÇ

So the question wasÔºå is there a good way to„ÄÇIs there any good intuition behind what a good K is„ÄÇ

 There's general rules for different data setsÔºå but usually you just have to run through it„ÄÇ

 grid searchÔºå brute force„ÄÇYesÔºå question„ÄÇGoodÔºå good questionÔºå yes„ÄÇYeah„ÄÇYesÔºå the question was„ÄÇ

 is each pixel one number or three numbersÔºüFor majority of computer vision throughout its history use grayscale images„ÄÇ

 So it was one number„ÄÇ But RGB is three numbers„ÄÇ And there' are sometimes a depth valueÔºå too„ÄÇ

 So it's four numbers„ÄÇ So it's„ÄÇIf you have a ste vision camera that gives you the depth information of the pixels„ÄÇ

 that's a fourth„ÄÇAnd then if you stack two images togetherÔºå that could be six„ÄÇIn general„ÄÇ

 everything we work with will be three numbers for a pixel„ÄÇIt wasÔºå yes„ÄÇ

 so the question was for the absolute valueÔºå it was just one number exactly right„ÄÇ

 so in that case those was gray scale imagesÔºå so it's not RGB images„ÄÇSo that'sÔºå you know„ÄÇ

 this algorithm is pretty good if we use the best„ÄÇwe optimize the hyperparameters of this algorithm„ÄÇ

 choose K of 7Ôºå seems to work well for this particular CFFR  to data setÔºå okayÔºå we get 30% accuracy„ÄÇ

It's impressiveÔºå higher than 10%„ÄÇHuman beings perform at about 94„ÄÇ

 slightly above 94% accuracy for C far1„ÄÇ So given an imageÔºå it's a tiny image„ÄÇ I should clarify„ÄÇ

 it's like it's a little icon„ÄÇGiven that imageÔºå human beings able to determine accurately one of the 10 categories with 94% accuracy„ÄÇ

 and the currently state of the art convolution neural networks is 95„ÄÇ It's 95 point„ÄÇ4% accuracy„ÄÇ

And believe it or notÔºå it's a heated battle„ÄÇBut the most importantÔºå the critical fact here is„ÄÇ

It's recently surpassed humans„ÄÇAnd certainly surpass the Can nearest neighbor's algorithm„ÄÇSo„ÄÇ

How does this workÔºüLet's briefly look back„ÄÇIt's allÔºå it all still boils down to this little guyÔºå the„ÄÇ

 the neuron that sums the weights of its inputsÔºå adds a bias„ÄÇ

 produces an output facing on an activationÔºå a smooth activation function„ÄÇüòäÔºåAs question„ÄÇ

The question wasÔºå do you take a picture of a cat so you know it's a catÔºü

But that's not encoded anywhere„ÄÇLikeÔºå you have to write that down somewhere„ÄÇ So you have„ÄÇ

 you have to write as a caption„ÄÇ This is my cat„ÄÇ And then the unfortunate thing„ÄÇ

 given the Internet and how witty it isÔºå you can't trust the captions and images„ÄÇ

Because maybe you're just being clever and it's not a cat at allÔºå it's a dog dressed as a cat„ÄÇ

Yes question„ÄÇSorryÔºå CNN do better than what„ÄÇYeahÔºå so the question was„ÄÇ

 do coal networks generally do better than nearest neighborsÔºå There's very few problems on which„ÄÇ

Neural networks don't do better„ÄÇ YesÔºå they almost always do better„ÄÇ

 except when you have almost no data„ÄÇSoÔºå you need data„ÄÇ

And convolutional neural networks isn't some specialÔºå magical thing„ÄÇ

 It's just neural networks with someÔºå with some cheating up front that I'll explain some tricks to try to reduce the size and make it capable to deal with images„ÄÇ

üòäÔºåSo againÔºå yeahÔºå the input isÔºå in this case that we looked at classifying an image of a number„ÄÇ

As opposed to doing some fancy convolutional tricksÔºå we just take the„ÄÇ

 the entire 28 by 28 pixel image„ÄÇ That's 700 and„ÄÇ84 pixels as the input„ÄÇ

 that's 784 neurons in the inputÔºå15 neurons on the hidden layer and 10 neurons in the output„ÄÇNow„ÄÇ

 everything we'll talk about has the same exact structureÔºå nothing fancy„ÄÇ

There is a forward pass the network where you take an input image and produce an output classification„ÄÇ

And there's a backward path of the network through back propagation„ÄÇ

Where you adjust the weights when your prediction doesn't match„ÄÇThe ground truth output„ÄÇ

And learning just boils down to optimization„ÄÇ It's just optimizing a smooth function„ÄÇ

Differentiable function„ÄÇThat's defined as the loss function„ÄÇ

 That's usually as simple as a squared error„ÄÇBetween the true output and the one you actually got„ÄÇ

So what's the differenceÔºå what are convolutional neural networksÔºüComlish on neural networksÔºå take„ÄÇ

Take inputs that have some spatial consistencyÔºå have some meaning to the spatial„ÄÇ

Has some spatial meaning in themÔºå like images„ÄÇ There's other other things„ÄÇ

 You can think of the dimension of time„ÄÇAnd you can input audio signal into a conution on neural network„ÄÇ

And so the input is usually for every single layerÔºå that's a convolutional layer„ÄÇ

 The input is a 3D volumeÔºå and the output is a 3D volume„ÄÇ

I'm simplifying because you can call it 4D too„ÄÇIt's 3D„ÄÇ there's heightÔºå width and depth„ÄÇ

So that's an image„ÄÇThe height and the width is the width and the height of the image„ÄÇAnd then„ÄÇ

 the depth„ÄÇFor gray scale image is one„ÄÇFor an RGB image is three„ÄÇFor a1 frame video„ÄÇ

Of grayscale imagesÔºå the depth is 10„ÄÇIt's just a volumeÔºå a 3D menstrual matrix of numbers„ÄÇAnd„ÄÇ

The only thing that a convolutional layer does„ÄÇIs take a 3D volume input„ÄÇ

Produce the 3D volumes output and has some smooth function„ÄÇOperating on the inputs„ÄÇ

 on the sum of the inputs„ÄÇThat may or may not be a parameter that you tune that you try to optimize„ÄÇ

That's it„ÄÇSo Lego pieces that you stack together in the same way as we talked to Bob before„ÄÇ

So what are the types of layers that a convolution neural network haveÔºå There's inputs„ÄÇ So„ÄÇ

 for exampleÔºå a color image of 32 by 32„ÄÇWill be a volume of 32 by 32 by 3„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_6.png)

A convolutional layer„ÄÇTakes advantage of„ÄÇThe spatial relationships„ÄÇOf the input neurons„ÄÇ

And a commvolutional layer„ÄÇIt's the same exact neuron as for a fully connected network„ÄÇ

 The regular network we talked about before„ÄÇ But it just has a narrower receptive field„ÄÇ

 It's more focused„ÄÇTheTheÔºå the inputs to a neuron on the convolutional layer„ÄÇ

Come from a specific region from the previous layer„ÄÇAnd the parameters on each filter„ÄÇ

 you could think of this as a filter because you slide it across the entire image„ÄÇ

And those parameters are shared„ÄÇSo supposed to taking the for„ÄÇ

 if you think of about two layers as opposed to connecting every single pixel on in the first layer to every single neuron in the following layer„ÄÇ

 you only„ÄÇConnect„ÄÇThe neurons and the input layer that are close to each otherÔºå to the output layer„ÄÇ

And thenÔºå you enforce„ÄÇThe weight to be„ÄÇTied together„ÄÇSpaly„ÄÇAnd what that results in is a filter„ÄÇ

Every single layer on the outputÔºå you could think of as a filter„ÄÇThat gets excitedÔºå for example„ÄÇ

 for an edge„ÄÇAnd when it sees this particular kind of edge in the imageÔºå it'll get excited„ÄÇ

 it'll get excited in the top left of the image and top rightÔºå top bottom leftÔºå bottom right„ÄÇ

The assumption there is that a powerful feature for detecting a cat„ÄÇIs just as important„ÄÇ

 no matter where in the image it is„ÄÇAnd this allows you to cut away a huge number of connections„ÄÇ

Between neurons„ÄÇBut it still boils down on the right„ÄÇAs a neuronÔºå that sums„ÄÇ

A collection of inputs and applies weights to them„ÄÇThe tuable„ÄÇ

The spatial arrangement of the output volume volume„ÄÇ

Relative to the input volume is controlled by three things„ÄÇThe number of filters„ÄÇ

So for every single quote unquoteÔºå filterÔºå you'll get an extra layer on the output„ÄÇSo if the input„ÄÇ

 let's talk about the very first layerÔºå the input is 32 by 32 by 3„ÄÇ It's an RGB image of 32 by 32„ÄÇ

If the number of filters is 10„ÄÇThen the resulting depth„ÄÇThe resulting number of„ÄÇ

The stacked channels in the output will be 10„ÄÇStriide„ÄÇIs given„ÄÇ

Is the step size of the filter that you slide along the image„ÄÇOftentimesÔºå there's just one„ÄÇOr three„ÄÇ

And that directly reduces„ÄÇThe sizeÔºå the spatial sizeÔºå the width and the height of the output image„ÄÇ

And then there is a convenient thing that's often done is padding„ÄÇThe image on the outside was zeros„ÄÇ

So that the input and the output have the same height and width„ÄÇSo this is a visualization of„ÄÇ

Convolution„ÄÇAnd I encourage you to kind of maybe offlineÔºå think about what's happening„ÄÇIt's„ÄÇ

Similar to the way human vision works„ÄÇCrudelyÔºå so if there's any experts in the audience„ÄÇ

So the input here on the left„ÄÇIs a collection of numbersÔºå0Ôºå1Ôºå2„ÄÇAnd a filter„ÄÇWell„ÄÇ

 there is two filters„ÄÇShown as W 1 and WÔºå W 0 and W 1„ÄÇ

Those filters shown in red are the different weights applied on those filters„ÄÇ

And each of the filters have a depth„ÄÇJust like the inputÔºå a depth of three„ÄÇ

So there's three of them in each column„ÄÇAnd soÔºå let's see„ÄÇ‰∏çË∑ü„ÄÇYeah„ÄÇAnd so you slide that filter„ÄÇ

Along the image„ÄÇKeeping the weights the same„ÄÇ This is the sharing of the weights„ÄÇ

And so your first filterÔºå you pick the weights„ÄÇ This is an optimization problem„ÄÇ

 You pick the weights in such a way that it firesÔºå gets excited„ÄÇ

 at useful features and doesn't fire for not useful features„ÄÇ

 And then this is the second filter that fires for useful features and not„ÄÇüòä„ÄÇ

And produces a signal on the output„ÄÇDepending on a positive number„ÄÇ

 meaning there's a strong feature in that region and a negative numberÔºå if there isn't„ÄÇ

But the filter is the same„ÄÇ This allows for drastic reduction in the parameters„ÄÇ

 And so you can deal with„ÄÇInputs that are 100 by 1 thousand pixel imageÔºå for exampleÔºå or video„ÄÇ

There's a really powerful concept thereÔºå the spatial„ÄÇThe spatial sharing of weights„ÄÇ

 that means there's a spatial invariance to the features you're detecting„ÄÇ

 it allows you to learn from arbitrary images„ÄÇIt can you so you don't have to be concerned about preprocessing the images in some clever way„ÄÇ

 You just give the raw image„ÄÇThere's another operation„ÄÇPoulling„ÄÇ

It's a way to reduce the size of the layers„ÄÇByÔºå for exampleÔºå in this case„ÄÇ

 max pooling for taking a collection of outputs and choosing max1 and summarizing those„ÄÇ

Collection of pixels such that the output of the pooling operation is much smaller than the input„ÄÇ

Because the justification there is„ÄÇThat you don't need a high resolution„ÄÇ

Loization of exactly where which pixel got is important in the imageÔºå according toÔºå you know„ÄÇ

 you don't need to know exactly which pixels associated with the cat ear„ÄÇOr you knowÔºå a cat face„ÄÇ

As long as you kind of knowÔºå it's around that part„ÄÇ

 and that reduces a lot of complexity in in the operations as question„ÄÇ

The question was when is it too much poolingÔºå when do you stop poolingÔºüSo„ÄÇ

So pooling is a very crude operation that doesn't have any„ÄÇ

One thing you need to know is it doesn't have any parameters that are learnable„ÄÇ

 So you can't learn anything„ÄÇClever about the about pooling„ÄÇYou're just pickingÔºå in this case„ÄÇ

 max poolÔºå so you're picking the largest number„ÄÇSo you're reducing the resolution„ÄÇ

 You're losing a lot of information„ÄÇThere's an argument that you're not„ÄÇ

 losing that much information as long as you're not pool the entire image into a single value„ÄÇBut„ÄÇ

You're gaining training efficiency„ÄÇ You're gaining the memory size„ÄÇ

 the reducing the size of the networkÔºå so„ÄÇIt'sÔºå it's definitely a thing that people debate„ÄÇ

 And some it's a parameter that you play with„ÄÇTo see what works for you„ÄÇOkayÔºå so„ÄÇ

How does this thing look like as a wholeÔºå a convolution in neural own networkÔºå The input is an image„ÄÇ

There's usually a convolutional layer„ÄÇThere is a pooling operationÔºå another convolutional layer„ÄÇ

 another pooling operationÔºå and so on„ÄÇüòäÔºåAt the very endÔºå if the task is classification„ÄÇ

You have a stack of convolutional layers and pooling layers„ÄÇ

 There are several fully connected layers„ÄÇ So you go from those„ÄÇ

The spatial convolutional operations to fully connecting every single neuron in a layer to the following layer and you do this so that by the end you'll have a collection of neurons„ÄÇ

 each one is associated with a particular class„ÄÇ so in what we looked at yesterday as the input is an image of a number 0 through 9 the output here would be 10 neurons so you boil down that image with the collection of convolution neural„ÄÇ

Convolutional layers„ÄÇWith one or two or three fully connected layers at the end that all lead to 10 neurons„ÄÇ

 And each of those neurons job is to give„ÄÇGet fired up„ÄÇ

When it sees a particular number and for the other ones to produce a low probability„ÄÇAnd so„ÄÇ

 this kind of process„ÄÇIs how you„ÄÇHave the 95 percentile accuracy on the C4 10 problem„ÄÇ

 This here is Inet data set that I mentioned is how you take this image of a leopard„ÄÇ

Of a container ship„ÄÇAnd produce a probability that that is a container ship or a leopard„ÄÇ

Also shown there are the outputs of the other nearest„ÄÇNearest neurons in terms of their confidence„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_8.png)

NowÔºå you can use the same exact operation by chopping off„ÄÇThe fully connected layer at the end„ÄÇ

 and as opposed to mapping from image to a prediction of what's contained in the image„ÄÇ

 you map from the image to another image„ÄÇAnd you can train that image to be one that gets„ÄÇExcited„ÄÇ

Spatly„ÄÇMeaning it gives you a high close to one value for areas of the image that contain the object of interest„ÄÇ

And then a low number for areas of image that are unlikely to contain that image„ÄÇAnd so from this„ÄÇ

 you can go on the leftÔºå an original image of a woman and a horse„ÄÇ

To a segmented image of knowing where the woman is and where the horse is and where the background is„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_10.png)

The same process can be done for detecting the object„ÄÇ

So you can segment the scene into a bunch of interesting objects„ÄÇCandidates„ÄÇFor interesting objects„ÄÇ

 and then go through those candidates„ÄÇ1 by one and perform the same kind of classification as in the previous step„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_12.png)

Where it's just an input is an imageÔºå and the output is a classification„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_14.png)

And through this process of hopping around an image„ÄÇ

 you can figure out exactly where is the best way to segment the cow out of the image„ÄÇ

 It's called object detection„ÄÇOkayÔºå so how canÔºå how can these magical convolution neural networks help us in driving„ÄÇ

üòäÔºåThis is a video„ÄÇOf the forward roadway„ÄÇFrom a data that we'll look at„ÄÇ

That we've collected from O Tesla„ÄÇ But firstÔºå let me look at driving„ÄÇBriefly„ÄÇ

The general driving task„ÄÇFrom the human perspective„ÄÇOn average„ÄÇ

 an American driver in the United States drives 1010000 miles a year„ÄÇA little more for rural„ÄÇ

 a little less for urban„ÄÇThere is about 30Ôºå000 fatal crashes and 32 plus„ÄÇSometimes this high as 38„ÄÇ

000 fatalities a year„ÄÇThis includes car occupants„ÄÇPedestrians„ÄÇBcyclists„ÄÇAnd motorcycle riders„ÄÇ

This may be a surprising factÔºå but„ÄÇIn a class on self driving cars„ÄÇ

 we should remember that so ignore the 59„ÄÇ9% that's other„ÄÇ

The most popular cars in the United States are pickup trucks„ÄÇForour F1 seriesÔºå Shevy Silvervaaddo„ÄÇ

 Ram„ÄÇIt's an important point that„ÄÇWe're still married to our„ÄÇTo wanting to be in control„ÄÇAnd so„ÄÇ

One of the interesting cars that we look at„ÄÇAnd the car that is the data set that we provide to the class is collected from is a Tesla„ÄÇ

It's the one that comes at the intersection of the For F 150 and the cute little Google self drivingriving car on the right„ÄÇ

It's fast„ÄÇ It allows you to have a feeling of control„ÄÇ

 but it can also drive itself for hundreds of miles on the highwayÔºå if need be„ÄÇ

It allows you to press a button and the car takes over„ÄÇ

It's a fascinating trade off of transferring control from the human to the car„ÄÇ

It's a transfer of trust„ÄÇAnd it's a chance for us to study the psychology„ÄÇ

Of human beings as they relate to machines„ÄÇAt 60 plus miles an hour„ÄÇ

In case you're not aware a little summary of human beings„ÄÇWe're distracted things„ÄÇWould like to text„ÄÇ

 use the smartphone„ÄÇWatch videosÔºå groomÔºå talk to passengers„ÄÇEatÔºå drink„ÄÇTexting„ÄÇ

169 billion texts were sent in the US every single month„ÄÇIn 2014„ÄÇOn averageÔºå five seconds„ÄÇ

Our I spent off the road while texting„ÄÇ5 seconds„ÄÇThat's the opportunity for automation to step in„ÄÇ

More than thatÔºå there's what NTA refers as the4D's„ÄÇDrunkÔºå drugÔºå distracted and drowsy„ÄÇ

 each one of those opportunities for automation to step in„ÄÇDtrong driving„ÄÇ

Stands to benefit significantly from automationÔºå perhaps„ÄÇSo the milesÔºå let's look at the miles„ÄÇ

 the dataÔºå there's 3 trillionÔºå3 million„ÄÇMillion„ÄÇ3 million million miles driven every year„ÄÇ

And Tesla Autopilot„ÄÇOur case study for this class and as human beings„ÄÇ

Is driven on full autopilot mode„ÄÇ So it driving by itself„ÄÇ300 million miles as of December 2016„ÄÇ

And the fatalities for„ÄÇHuman controlled vehicles„ÄÇIs one in 90 million„ÄÇ

 so about 30 plus000 fatalities a year„ÄÇAnd currently in TeslaÔºå onto Tesla Autopilot„ÄÇ

 there's one fatality„ÄÇThere's a lot of ways you can tear that statistic apart„ÄÇ

 but it's one to think about„ÄÇAlreadyÔºå perhaps„ÄÇAutomation results in safer driving„ÄÇThe thing is„ÄÇ

 we don't understand automation„ÄÇBecause we don't have the data„ÄÇ We don't have the data„ÄÇ

On the forward roadway videoÔºå we don't have the data on the driver„ÄÇ

And we just don't have that many cars on the road todayÔºå that drives themselves„ÄÇ

So we need a lot of data„ÄÇWe'll provide some of it to you in the class„ÄÇ

And as part of our research at MI I TÔºå we're collecting huge amounts of it„ÄÇ

Of cars driving themselves„ÄÇAnd what we collecting that data is how we get to understanding„ÄÇ

So talking about theÔºå the data„ÄÇAnd what will world„ÄÇBe doing training our algorithms on„ÄÇ

Here is a Tesla Model SÔºå Model XÔºå we have instrumented 17 of them„ÄÇ

Have collected over 5000 hours and 70000 miles„ÄÇ And I'll talk about the cameras that we put in„ÄÇ

In them„ÄÇWe're collecting video of the forwardward roadway„ÄÇ

 This is a highlight of a trip from Boston to Florida of one of the people driving in Tesla„ÄÇüòä„ÄÇ

What's also shown in blue is the amount of time that Apolllo was engaged„ÄÇCurrently zero minutes„ÄÇ

 and then it grows and grows„ÄÇFor prolonged periods of timeÔºå so hundreds of miles„ÄÇ

 people engage autopilot„ÄÇOut of 1„ÄÇ3 billion miles driven in TeslaÔºå300 million are in autopilot„ÄÇ

You do the mathÔºå whatever that isÔºå 25%„ÄÇSo we are collecting data of the forward roadway of the driver„ÄÇ

 We have two cameras on the driver„ÄÇ what we're providing with a class is„ÄÇ

Epics of time of the forward roadway„ÄÇFor privacy considerations„ÄÇCameras use the record„ÄÇ

Are you your a regular webcamÔºüThe workhor of the computer vision community„ÄÇThe C920„ÄÇ

And we have some special lenses on top of it„ÄÇ NowÔºå what's special about these webcams„ÄÇ

Nothing that costs 70 bucks can be that goodÔºå right„ÄÇ

The what's special about them is that they do on board compression„ÄÇAnd allow you to„ÄÇ

Collect huge amounts of dataÔºå and use reasonably sized„ÄÇ

Storage capacity to store that data and train your algorithms on„ÄÇSo what on the„ÄÇ

Soll driving side do we have to work with„ÄÇHow do we build a self driving carÔºüTheres the sensors„ÄÇ

RadarÔºå LiarÔºå vision„ÄÇAudio„ÄÇAll looking outsideÔºå helping you detect the objects in the external environment to localize yourself and so on„ÄÇ

 And there's a sensorus facing insideÔºå visible light cameraÔºå audio again„ÄÇ

 and infrared camera to help detect pupils„ÄÇ

![](img/a3ea2eb7da7b24a9c2c132cab561a980_16.png)

So we can decompose the cell driving car task into four steps„ÄÇLooccalization„ÄÇAnsweringÔºå where am I„ÄÇ

Seecen understanding„ÄÇUsing the texture of the information of the scene around„ÄÇ

To interpret the identity of the different objects„ÄÇIn the scene„ÄÇ

And the semantic meaning of those objectsÔºå of their movement„ÄÇThere's movement planning„ÄÇ

 Once you figured all that outÔºå findÔºå found all the pedestriansÔºå found all the other cars„ÄÇ

 How do I navigate„ÄÇThrough this mazeÔºå a clutter of objects in a safe and legal way„ÄÇ

And there's driver stateÔºå how do I detect using video or other informationÔºü

Video of the driver detect information about their emotional state or their distraction level„ÄÇ

 is question„ÄÇYesÔºå that's a real time figure from Lidar„ÄÇ Lidar is the sensor that provides you the„ÄÇ

 the 3D point cloud„ÄÇOf the external scene„ÄÇSo Lidar is a technology used by„ÄÇ

Most folks were working with cell driving cars to give you a strong ground truth of the objects„ÄÇ

It's probably the best sensor we have for getting 3D information„ÄÇ

The least noisy 3D information about the external environment„ÄÇQuestion„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_18.png)

So autopilot is always changingÔºå one of the most amazing things about this vehicle is that the updates to autopilot come in the form of software„ÄÇ

 so the amount of time it's available it changesÔºå has become more conservative with time„ÄÇBut in this„ÄÇ

 this is one of the earlier versionsÔºå and it shows the second line„ÄÇIn yellow„ÄÇ

 it shows how often the autopilo was availableÔºå but not turned on„ÄÇ

 So it was the total driving time was 10 hours„ÄÇ Autopilo was available 7 hours and was engaged an hour„ÄÇ

 This particular person is„ÄÇA responsible drive because what you see or„ÄÇIs more cautious driver„ÄÇ

 What you see is it's raining„ÄÇ Autopilo is still availableÔºå but„ÄÇ

The comment was that you shouldn't trust that one fatality number as an indication of safety because the drivers elect to only engage the system when they are when it's safe to do so„ÄÇ

 it's totally open- there's a lot bigger arguments for that number than just that one„ÄÇTheÔºå theÔºå the„ÄÇ

 the question is„ÄÇWhether that's a bad thing„ÄÇ So maybe we can trust human beings to engageÔºå you know„ÄÇ

Despite the poorly filmed YouTube videosÔºå despite the hype in the media„ÄÇ

You're still a human being riding a 60 miles an hour in a metal box with your life and the line„ÄÇ

 You won't engage the system„ÄÇUnlessÔºå you knowÔºå it's completely safe„ÄÇ

 unless youve built up a relationship with itÔºå it's not all the stuff you see where a person gets in the back of a Tesla and starts sleeping or just playing chess or whatever„ÄÇ

 That's all for YouTube„ÄÇ The reality isÔºå when it's just you in the car„ÄÇ

 It's still your life in the line„ÄÇ And so you're gonna do the responsible thing„ÄÇ

 Unless perhaps you're a teenager and so on„ÄÇ But that never changesÔºå no matter what you're in„ÄÇ„Åù„ÄÇ

The question wasÔºå what do you need to see or sense about the external environment to be able to successfully drive„ÄÇ

 do you need lane markings do you need other what are the landmarks based on which you do the localization of the navigation„ÄÇ

 and that depends on the sensors„ÄÇSo with a Google self driving car in sunny California„ÄÇ

 it depends on Lidar toÔºå in a high resolution wayÔºå map the environment„ÄÇ

In order to be able to localize itself based on Lidar and LidarÔºå now„ÄÇ

 I don't know the details of exactly where Ldar fails„ÄÇBut it's not good with rain„ÄÇ

 It's not good with snow„ÄÇIt's not good when the environment is changing„ÄÇ

 So what snow does is it changes the visualÔºå the appearance„ÄÇ

 the reflective texture of the surfaces around us human beings are still able to figure stuff out„ÄÇ

 but a car that's relying heavily on LiDar won't be able to localize itself using the landmarks previously as detected because they look different now with snow„ÄÇ

Computer vision can help us„ÄÇWith lanes or„ÄÇFollowing a car„ÄÇ

 the two landmarks that we use staying in a lane is following a car in front of you or staying between two lanes„ÄÇ

 That's the nice thing about our roadways is they're designed for human eyes so you can use computer vision for lanes and for cars in front to follow them„ÄÇ

And there is radar that's a crude but reliable source of distance information that allows you to not collide with metal objects„ÄÇ

So all of that togetherÔºå depending on what you want to rely on moreÔºå gives you a lot of information„ÄÇ

 The question isÔºå when it's the domestic complexity„ÄÇOf real life occurs„ÄÇ

 how reliable will it be in the urban environmentÔºå and so on„ÄÇSoÔºå localization„ÄÇ

How can deep learning helpÔºå So firstÔºå let's just quick„ÄÇSummary of visual oometry„ÄÇ It's using„ÄÇ

A monocular or stereo input of video images„ÄÇTo determine your orientation in the world„ÄÇ

The orientation in this case of a vehicle to in the frame of the world„ÄÇ

And all you have to work with is a video of the forward Worldway„ÄÇAnd with stereo„ÄÇ

 you get a little extra information of how far away different objects are„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_20.png)

And so this is where one of our speakers on Friday will talk about his expertiseÔºå Slam„ÄÇ

 simultaneous localization and mapping„ÄÇ This is a very well studied and understood problem„ÄÇ

Of detecting unique features in the external scene„ÄÇ

And localizing yourself based on the trajectory of those„ÄÇThose unique features„ÄÇ

 when the number of features is high enoughÔºå it becomes an optimization problem„ÄÇYou know„ÄÇ

 this particular lane moved a little bit from frame to frameÔºå you can track that information„ÄÇ

And fuse everything together in order to be able to estimate your trajectory through the three dimensional space„ÄÇ

You also have other sensors to help you outÔºå you have GPS„ÄÇWhich is„ÄÇPretty accurateÔºå not perfect„ÄÇ

 but pretty accurate„ÄÇ It's another signal to help you localize yourself„ÄÇYou also have IM U„ÄÇ

 Acceleroerometer tells you the your acceleration„ÄÇ

![](img/a3ea2eb7da7b24a9c2c132cab561a980_22.png)

From the gyrocope at the accelerometerÔºå you have the6 degree of freedom of movement information about how„ÄÇ

The moving objectÔºå the car„ÄÇI's navigating through space„ÄÇSo you can do that„ÄÇ

You could do using the old school way of optimization„ÄÇGiven a unique set of features„ÄÇ

 like Sift features„ÄÇAnd that step involves„ÄÇWith stereo input„ÄÇOn distorting and rectifying the images„ÄÇ

 you have two images„ÄÇ You have toÔºå from the two imagesÔºå compute the depth map„ÄÇ

 So for every single pixelÔºå Comp the your estimateÔºå best estimate of the depth of that pixel„ÄÇ

 the three dimensional„ÄÇPositionÔºå relative„ÄÇTo the camera„ÄÇThen you compute„ÄÇ

 that's where you compute the disparity map„ÄÇ That's what that's called the„ÄÇ

From which you get the distance„ÄÇThen you detect„ÄÇUniqueÔºå interesting features in the scene„ÄÇ

 Sift is a popular one„ÄÇHas a popular algorithm for detecting unique features„ÄÇ and then you over time„ÄÇ

 track those features„ÄÇ and that tracking is what allows you to„ÄÇ

 through the vision alone to get information about your trajectory through the three dimensional space„ÄÇ

And you estimate that trajectory„ÄÇThere's a lot of assumptionsÔºå assumptions that bodies are rigid„ÄÇ

So you have to figure out if a large object passes right in front of you„ÄÇ

 you have to figure out that that what that was„ÄÇYou have to figure out the mobile objects in the scene„ÄÇ

And those that are stationary„ÄÇOr you can cheat what we'll talk about„ÄÇAnd do it using neural networks„ÄÇ

 end to end„ÄÇNowÔºå what does end to end meanÔºå And this will come up a bunch of times throughout this class and today„ÄÇ

 end to end means„ÄÇAnd I refer to it as cheating because it takes away a lot of the hard work of hand engineering features„ÄÇ

You take the raw input of whatever sensors„ÄÇIn this case„ÄÇ

 it's taking stereo input from a stereo vision cameraÔºå so two images„ÄÇ

 a sequence of two images coming from a ste vision camera„ÄÇAnd the output„ÄÇ

Is a estimate of your trajectory through space„ÄÇ So as opposed to doing the hard work of slam or detecting unique features of localizing yourself„ÄÇ

 of tracking those features and figuring out what your trajectory isÔºå you simply train the network„ÄÇ

With some ground truth that you have from a more accurate sensor like LdDar„ÄÇ

And you train it on a set of inputsÔºå their are stereo vision inputs„ÄÇ

 and outputs is the trajectors to space„ÄÇYou have a separate convolution in neural networks for the velocity and for the orientation„ÄÇ

And this works pretty well„ÄÇUnfortunatelyÔºå not quite well„ÄÇAnd John Lennon will talk about that„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_24.png)

SM is one of the places where deep learning has not been able to outperform the previous approaches„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_26.png)

Where where deep learning really helps is the scene understanding part„ÄÇ

Is interpreting the objects in the scene„ÄÇIt's detecting the various„ÄÇParts of the scene„ÄÇ

 segmenting them and with optical flowÔºå determining their movement„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_28.png)

So previous approaches for detecting objects„ÄÇLike the traffic signal classification detection that we have the TensorF tutorial for„ÄÇ

Or to use„ÄÇP like features or other types of features that are hand engineered from the images„ÄÇNow„ÄÇ

 we can use conution neural networks toÔºå to replace„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_30.png)

The extraction of those features„ÄÇAnd there's a tense flow implementation„ÄÇOf segmentnet„ÄÇ

Which is taking the exact same neural network that I talked about„ÄÇ It's the same thing„ÄÇ

 just the beauty is you just apply similar types of networks to different problems„ÄÇ

 And depending on the complexityÔºå the problem can get quite amazing performance„ÄÇ In this case„ÄÇ

 we convolutionize in the networkÔºå meaning the output is an image„ÄÇ input is an image„ÄÇ

 A single monocular image„ÄÇ The output is a„ÄÇüòäÔºåSegmented image where the colors indicate your best pixel by pixel estimate of what object is in that part„ÄÇ

 This has noÔºå is not using any spatial information„ÄÇ It's not using any temporal information„ÄÇ

 So it's it's processing every single frame separately„ÄÇ

 and it's able to separate the road from the trees from the pedestriansÔºå other cars„ÄÇAnd so on„ÄÇ

This is intended to lie on top„ÄÇOf a radar slash Liar type of technology that's giving you the three dimensional or stereo vision„ÄÇ

 Three dimensional information about the scene„ÄÇ You're sort of painting that scene with the identity of the objects that are in it„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_32.png)

Your best estimate of it„ÄÇThis is something„ÄÇI'll talk about tomorrow„ÄÇIts recurring neural networks„ÄÇ

 and we can use recurring neural networks that work with temporal data„ÄÇ

To process video and also process audio„ÄÇIn this case„ÄÇ

We can process what's shown on the bottom is a spectrogram of audio for a wet road and a dry road„ÄÇ

You can look at that spectrogram as an image„ÄÇAnd process it in a temporal way using recurren neural networks„ÄÇ

 just slide it across and keep feeding it to a network„ÄÇ

And it does incredibly well on the simple tasksÔºå certainly„ÄÇOf dry roadÔºå versus red road„ÄÇ

This is importantÔºå a subtleÔºå but very important„ÄÇTask„ÄÇ

 and there's many like it to know that the road the textureÔºå the quality„ÄÇ

The characteristics of the roadÔºå wetness being a critical one„ÄÇ when it's not raining„ÄÇ

 but the road is still wetÔºå that information is very important„ÄÇOkayÔºå so for movement planning„ÄÇ

The same kind of approach„ÄÇÂëÉ„ÄÇOn the right is work from one of our other speakersÔºå Sirsh Kaman„ÄÇ

The same approach we're using for theÔºå to solve traffic„ÄÇ

Through friendly competition is the same that we can use for what Chris Gerdys does with his race cars„ÄÇ

 for planning trajectories in high speed movement along„ÄÇüòäÔºåComplex„ÄÇCve„ÄÇ

So we can solve that problem using optimization„ÄÇSolve the control problem using optimization„ÄÇ

 or we can use it with reinforcement learning„ÄÇBy running„ÄÇTens of millions„ÄÇ

 hundreds of millions of time through that simulation of taking that curve and learning which trajectory doesn't both optimizes the the speed at which you take the turn and the safety of the vehicle„ÄÇ

Exactly the same thing that you're using for traffic„ÄÇAnd for driver state„ÄÇ

 this is what we will talk about next week„ÄÇIs all the fun face stuffÔºå eyesÔºå faceÔºå emotion„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_34.png)

This is„ÄÇWe have video of the driverÔºå video of the driver's bodyÔºå video of the driver's face„ÄÇ

On the left is one of the TAs in his younger days„ÄÇStill looks the same„ÄÇThere he is„ÄÇ

So that's in that particular case„ÄÇYoure„ÄÇDoing one of the easier problems„ÄÇ

 which is one of detecting where the head and the eyes are positioned„ÄÇ

 the head and eye pose in order to determine what's called„ÄÇThe gaze of the driver„ÄÇ

 where the driver is lookingÔºå glance„ÄÇAnd so shown and we'll talk about these problems„ÄÇ

From the left to the rightÔºå on the left and green are the easier problems on the red or the harder from the computer vision aspect„ÄÇ

So on the left is body poseÔºå head poseÔºå the larger the objectÔºå the easier it is to detect„ÄÇ

 and the orientation of it is the easier to detect„ÄÇAnd then there is pupil diameter„ÄÇ

 detecting the pupil„ÄÇThe characteristicsÔºå the positionÔºå the size of the pupil„ÄÇ

 and there's microcicadesÔºå things that happen at 1 millisecond frequencyÔºå the tremors of the eye„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_36.png)

All important information for„ÄÇTo determine the state of the driver„ÄÇ

Some are possible with computer vision„ÄÇ Some are not„ÄÇThis is something that we'll talk about„ÄÇ

 think on Thursday„ÄÇIs the detection of where the driver is looking„ÄÇ

 So with this is a bunch of the cameras that we have in the Tesla is„ÄÇ

Dan driving a Tesla and detecting exactly where of one of six regions„ÄÇ

 we've converted into a classification problem of left right rear view mirrorÔºå instrument cluster„ÄÇ

 center stack or forward roadway„ÄÇ So we have to determine out of those six categories which direction is the driver looking at„ÄÇ

 This is important for driving„ÄÇ We don't care exactly the X Y Z position of where the driver is looking at„ÄÇ

 We care that they're looking at the road or not„ÄÇ Are they look at their cell phone in their lab„ÄÇ

 or they looking at the forward roadway„ÄÇ And we'll be able to answer that pretty effectively using commvolution of neural networks„ÄÇ

You can also look at emotion„ÄÇUsing CNNs to extract„ÄÇAgainÔºå converting emotion„ÄÇ

 the complex world of emotion into a binary problem of frustrated versus satisfied„ÄÇ

This is a video of drivers interacting with a voice navigation system„ÄÇ If you've ever used one„ÄÇ

 you knowÔºå it may be a source of frustration from folks„ÄÇAnd so this is self reported„ÄÇ

 This is one of the hardÔºå you knowÔºå driver emotion„ÄÇ

 If you're in what's called effective computing is the field of studying emotion„ÄÇ

From the computational side„ÄÇIf youreÔºå if you know thatÔºå if you're working in that field„ÄÇ

 you know that the annotation side of emotion is a really challenging one„ÄÇ

 So getting the ground truth ofÔºå wellÔºå okayÔºå so this guy is smiling„ÄÇSo can I label that as happyÔºü

Or he's frowningÔºå as Zemin saidÔºå most effective computing folks do just that„ÄÇIn this case„ÄÇ

 with selfre ask people how frustrated they were on a scale of one to 10„ÄÇDan up top reported a a one„ÄÇ

 so not frustrated„ÄÇ He's satisfied with the interaction„ÄÇ and the other driver reported it as a9„ÄÇ

 He was very frustrated with interaction„ÄÇ NowÔºå what you notice is there is a very cold„ÄÇ

 stoic look on Dan's faceÔºå which is an indication of happiness„ÄÇAnd„ÄÇIn the case of frustration„ÄÇ

 the driver is smiling„ÄÇ So this is sort of a good reminder that we can't trust our own human instincts and engineering features and engineering the ground truth„ÄÇ

 We have to„ÄÇüòäÔºåTrust the data„ÄÇTrust the ground truth„ÄÇ

That we believe is closest reflection of the actual semantics of what's going on in the scene„ÄÇOkay„ÄÇ

 so enter and drivingÔºå getting to theÔºå the project and the tutorial„ÄÇ

So if driving is like a conversation„ÄÇAndÔºå thank you„ÄÇ

For someone to clarify that this is a actic triumphph in Paris this video„ÄÇ

If if driving is like a natural language conversation that we can think of end to end driving„ÄÇ

As skipping the entire touring test components and treating it„ÄÇ

As an enter a natural language generation„ÄÇSo what we do is we take as inputÔºå the external sensors„ÄÇ

 and an output„ÄÇThe control of the vehicle„ÄÇAnd the magic happens in the middle„ÄÇ

We replaced that entire step„ÄÇWithin your own network„ÄÇ

The Ts told me to not include this image because it's the cheesiest I've ever seen„ÄÇI apologize„ÄÇ

Thank you„ÄÇ Thank you„ÄÇÂïä„ÄÇI regret nothing„ÄÇSo this is to show our path to self driving cars„ÄÇ

 but it to explain a point that we have a large data set of ground truth„ÄÇ

If we were to formulate the driving task is simply taking external images„ÄÇ

And producing steering commandsÔºå acceleration of breaking commands„ÄÇ

 that we have a lot of ground truth„ÄÇWe have„ÄÇA large number of drivers on the road every day„ÄÇ

Driving and therefore collecting our ground truth for us because they're an interested party in producing the steering commands that keep them alive„ÄÇ

And thereforeÔºå if we were to record that dataÔºå it becomes ground truth„ÄÇ

 So if it's possible to learn thisÔºå what we can do is we can collect data for the manually controlled vehicles and use that data to train an algorithm to to„ÄÇ

They control a self driving vehicle„ÄÇOkayÔºå so one of the first folks that did this is NviDdia„ÄÇ

 where they actually train in an external image„ÄÇThe image of the forward roadway„ÄÇ

And a neural networkÔºå a convolution neural networkÔºå a simple vanillaÔºå convolution neural network„ÄÇ

 I'll briefly outline„ÄÇTake an image inÔºå take a steeringÔºå produce a steering command out„ÄÇ

 and they're able to successfully„ÄÇüòäÔºåTo some degreeÔºå learn to navigate basic turns„ÄÇ

 curves and even stop or make sharp turns at a T intersection„ÄÇSo thisÔºå this network is simple„ÄÇ

 There is input on the bottomÔºå outputll up top„ÄÇ The input is a 66 by 200 pixel image„ÄÇRGB„ÄÇ

Shown on the left„ÄÇOr shown on the left is the raw input„ÄÇ

 and then you crop it a little bit and resize it down„ÄÇ66 by 200„ÄÇ

That's what we have in the code as well„ÄÇIn the two versions of the code we provide for you„ÄÇ

 both that runs in the browser and in TensorF„ÄÇIt has a few layers„ÄÇA few convolutional layers„ÄÇ

A few fully connected layersÔºå and an output„ÄÇThis is a regression network„ÄÇ

 It's producing not a classification of cat versus dog„ÄÇ It's producing a steering command„ÄÇ

 How do I turn the steering wheel„ÄÇ That's it„ÄÇThe rest is magic„ÄÇAnd we train it on„ÄÇHuman input„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_38.png)

What we have here is a project„ÄÇIs an implementation of this system in Comnet JS that runs in your browser„ÄÇ

This is the tutorial to follow and the project toÔºå to take on„ÄÇSoÔºå unlike the„ÄÇDeep traffic game„ÄÇ

This is reality„ÄÇ This is a realÔºå real input from real vehicles„ÄÇSo you can go to this link„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_40.png)

Demo went wonderfully yesterday„ÄÇ So let's seeÔºå maybe two for two„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_42.png)

there's a tutorial and then the actual gameÔºå the actual simulation is on deep Tesla JS„ÄÇ I apologize„ÄÇ

Everyone's going there nowÔºå aren't they„ÄÇDoes it work on a phone„ÄÇIt does„ÄÇGreat„ÄÇAgain„ÄÇ

 similar structure up top is the visualization of the loss function as the network is learning„ÄÇ

 and it's always training„ÄÇNext is the input for the layout of the network„ÄÇ Theres the„ÄÇ

The specificationÔºå the inputÔºå 200 by 66„ÄÇThere's a convolutional there„ÄÇThere's a pooling layer„ÄÇ

 and the output is the regression layer„ÄÇA single neuron„ÄÇThis is a tiny version„ÄÇDeep tinyÔºå right is„ÄÇ

As a tiny version of„ÄÇThe Nvidia architecture„ÄÇ

![](img/a3ea2eb7da7b24a9c2c132cab561a980_44.png)

And then you can visualize the operation of this network„ÄÇUnreal video„ÄÇ

The actual wheel value that produced by the driver by the autopilot system„ÄÇIs in blue„ÄÇ

 and the output of the network is in white„ÄÇIn what's indicated by green is the cropping of the image that is then resized to produce the 66 by 200 input to the network„ÄÇ

So once againÔºå amazinglyÔºå this is running in your browser„ÄÇ

Training on real world video so you can get in your car today„ÄÇ

 input it and maybe teach and you'll know to drive like you„ÄÇ

We have the code in commnet JS and Tensorflow to do that and a tutorial„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_46.png)

WellÔºå let me briefly describe some of the work here„ÄÇSo„ÄÇThe input to the network is a single image„ÄÇ

 This is for deep Tesla JSÔºå single image„ÄÇ And the output is a steering wheel value between negative 20 and 20„ÄÇ

 That's in degrees„ÄÇ

![](img/a3ea2eb7da7b24a9c2c132cab561a980_48.png)

We record„ÄÇLike I saidÔºå thousands of hoursÔºå but we provide publicly 10 video clips of highway driving from O Tesla„ÄÇ

Half are driven by autopilotÔºå half are driven by human„ÄÇ

The wheel value is extracted from a perfectly synchronized„ÄÇCanant„ÄÇ

 we are collecting all of the messages from can„ÄÇWhich contains steering wheel value„ÄÇ

 And that's synchronized with the video„ÄÇ We cropÔºå extract the windowÔºå the green 1 I mentioned„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_50.png)

And then provide that as input to the network„ÄÇSo this is a slight difference from deep traffic with the red car weaving through traffic because there's the messy reality of real world lighting conditions„ÄÇ

üòäÔºåAnd your taskÔºå for the most partÔºå in this simple steering task„ÄÇIs to stay inside the lane„ÄÇ

And say the lane markings„ÄÇIn an end to end wayÔºå learn to do just that„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_52.png)

So Comnet JS is a JavaScript implementation of CNNs of convolution neural networks„ÄÇIt supports„ÄÇ

ReallyÔºå arbitrary networks„ÄÇI meanÔºå all neural networks are simple„ÄÇ

 but because it runs in jascript it's not utilizing GPU„ÄÇThe larger the network„ÄÇ

 the more it's going to be weighed down computationally„ÄÇNowÔºå unlike the previous„ÄÇ

 unlike deep trafficÔºå this isn't the competition„ÄÇ but if you are a student registered for the course„ÄÇ

 you still do have to submit the code„ÄÇ You still have to submit your ownÔºå your own car„ÄÇ

As part of the classÔºåQuestion„ÄÇSo the question was the amount of data that's neededÔºü

Is there general rules of thumb for the amount of data needed for a particular task certainly in driving„ÄÇ

 for exampleÔºüIt's a good question„ÄÇYou generally have toÔºå like I said„ÄÇ

 and you'll know where' the good memors„ÄÇ So you have to just have„ÄÇ

Every case represented in the training set that you're interested in„ÄÇAs much as possible„ÄÇ

 So that means„ÄÇIn generalÔºå if you want a pictureÔºå you want to classify difference in cats and dogs„ÄÇ

You want to have at least 1000 cats and 100 dogsÔºå and then you do really well„ÄÇ

The problem with driving is twofold„ÄÇ One is that most of the time driving looks the same„ÄÇ

And the stuff you really care about is when driving looks differentÔºå it's all the edge cases„ÄÇ

So what we're not good with the neural networks is generalizing from the common case to the edge cases to the outliers„ÄÇ

 So avoiding a crashÔºå just because you can stay in the highway for thousands of hours successfully„ÄÇ

 doesn't mean you can avoid a crash when somebody runs in front of you on the road„ÄÇ

And the other part we're driving is the accuracy you have to achieve is really high„ÄÇ So for„ÄÇ

For Ca versuss dogÔºå life doesn't depend on your error„ÄÇ

On your ability to steer a car inside of the laneÔºå youd better be„ÄÇVery close to 100% accurate„ÄÇ

There's a box for designing the network„ÄÇThere's a visualization of the metrics measuring the performance of the network as it trains„ÄÇ

There is a visualizationÔºå a layer visualization of what features the network is extracting at every convolutional layer and every fully connected layer„ÄÇ

There is ability to restart the training„ÄÇVisualize the„ÄÇVisualized in network„ÄÇ

 performing on real video„ÄÇThere is„ÄÇThe input layer„ÄÇThe convolutional layers„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_54.png)

The video visualization„ÄÇ

![](img/a3ea2eb7da7b24a9c2c132cab561a980_56.png)

![](img/a3ea2eb7da7b24a9c2c132cab561a980_57.png)

An interesting tidbit on the bottom right is a barcode„ÄÇThat will has ingeniously designed„ÄÇ

How do I clearly explain why this is so coolÔºå It's a way„ÄÇTo through video„ÄÇ

 synchronize multiple streams of data together„ÄÇ So it's very easy for those who have worked with multimodal data„ÄÇ

 where there are several streams of data for the for them to become unsynchronized„ÄÇ

 especially when a big component of training in neural network is shuffling the data„ÄÇ

 So you have to shuffle the data in clever ways„ÄÇ So you're not overfitting any one little aspect of the video and yet maintain the data perfectly synchronized„ÄÇ

 So what you did is doing the hard work of connecting the the steering wheel and and the video is actually putting the steering wheel on top of the video as a barcode„ÄÇ

üòäÔºåThe final result is you can watch the network operate„ÄÇAnd over timeÔºå it learns more and more„ÄÇ

To steer correctlyÔºå I'll fly through this a little bit in the interest of time„ÄÇ

 Just kind of summarize some of the things that you can play with in terms of tutorials and let you guys go„ÄÇ



![](img/a3ea2eb7da7b24a9c2c132cab561a980_59.png)

This is the same kind of process and driving with Tensorflow„ÄÇ So we have code available in Github„ÄÇ

We just put up on my GiHub on deep Tesla that takes in a single video or an arbitrary number of videos„ÄÇ

 trains on them and produces the visualization that compares the steering wheel„ÄÇ

 the actual steering wheelÔºå and the predicted steering wheel„ÄÇThe steering wheel„ÄÇ

 when it agrees with the human driver or the autopilot system lighting up as green and when it disagrees„ÄÇ

 lighting up as redÔºå hopefully not too often„ÄÇAgain„ÄÇ

 this is some of the details of how that's exactly done in TensorF„ÄÇ

 This is vanilla convolution neural networksÔºå specifying a bunch of layers„ÄÇConvolutional layers„ÄÇ

 a fully connected layerÔºå train the modelÔºå so you iterate over the batches of images„ÄÇ

Run the model over a test set of images„ÄÇAnd get this result„ÄÇWe have a tutorial„ÄÇ

Or IPyon notebook in a tutorial up„ÄÇThis is perhaps the best way to get started with convolution neural networks in terms of our class is looking at the simplest„ÄÇ

Iage classification problem of traffic light classification„ÄÇ

 So we have these images of traffic lights„ÄÇWe did the hard work of detecting them for you„ÄÇ

So now you have to figure out you have to build a composition network„ÄÇ It gets„ÄÇ

ItFures out the concept of color and gets excited when it sees redÔºå yellow or green„ÄÇ

If anyone has questions„ÄÇWelcom those„ÄÇ You can stay after class if you have any concerns with Docker with„ÄÇ

Tenssorflow with„ÄÇHow to win trafficÔºå deep trafficÔºå just stay after class or come by Friday„ÄÇ

5 to sevenÔºå see you guys tomorrow„ÄÇ

![](img/a3ea2eb7da7b24a9c2c132cab561a980_61.png)