# „ÄêËã±ËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëMIT 6.S094 ÔΩú Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™Âä®È©æÈ©∂(2018¬∑ÂÆåÊï¥Áâà) - P6ÔºöL6- Èù¢ÂêëËøêÂä®ËßÑÂàíÁöÑÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π† - ShowMeAI - BV1Y34y1i7vC

All rightÔºå hello everybodyÔºå welcome backÔºå glad you came back„ÄÇToday„ÄÇWe will unveil the first tutorial„ÄÇ

 the first project„ÄÇThis is deep traffic code named deep traffic where your task is to solve the„ÄÇ

Traffic problem using deeper reinforce learning„ÄÇAnd I'll talk about what's involved in designing a network there„ÄÇ

How you submit your own network and how you participate in the competition„ÄÇAs I said„ÄÇ

 the winner gets a very special prize„ÄÇTo be announced later„ÄÇWhat is machine learningÔºüSeveral types„ÄÇ

 there's supervised learning„ÄÇ As I mentioned yesterday„ÄÇ

 that's what meant usually when you discuss aboutÔºå you talk about machine learning and talk about its successes„ÄÇ

 Supervised learning requires a data set„ÄÇWellÔºå you know the ground truth„ÄÇ

 you know the inputs and the outputs„ÄÇAnd you provide that to a machine learning algorithm in order to learn the mapping between the inputs and the outputs in such a way that you can generalize to further examples in the future„ÄÇ

On supervised learning„ÄÇIs the other side„ÄÇWhen you know absolutely nothing about the outputs„ÄÇ

 about the truth„ÄÇOf the data that you're working withÔºå All you get is data„ÄÇ

And you have to find underlying structure„ÄÇUnderlying representation of the data that's meaningful for you to„ÄÇ

 to accomplish a certain taskÔºå whatever that is„ÄÇThere's semiupervised data„ÄÇOr only parts„ÄÇ

 usually a very small amount„ÄÇIs hasÔºå is labeled„ÄÇ There's ground truth available for just a small fraction of it„ÄÇ

 if you think of„ÄÇImages that are out there on the Internet„ÄÇAnd then you think about IageNe„ÄÇ

 a data set where every image is labeled„ÄÇThe size of that image that data set„ÄÇ

It's a tiny subset of all the images„ÄÇOn the available online„ÄÇ

But that's the task we're dealing with as human beings„ÄÇ

 as people interested in doing machine learning„ÄÇIs„ÄÇHow to expand the size of that„ÄÇ

Of the part of our data that we know something confidently about„ÄÇ

And reinforcement learning sits somewhere in between„ÄÇIt it's semi superupvised learning„ÄÇWhere„ÄÇ

There's an agent that has to exist in the world„ÄÇAnd that agent„ÄÇ

Knows the inputs that the world provides„ÄÇBut knows very little about that world„ÄÇ

 except through occasional„ÄÇTime delayed rewards„ÄÇ This is what it's like to be human„ÄÇ

This is what life is about„ÄÇYou don't know what's good and bad„ÄÇ You kind of have to just live it„ÄÇ

 And every once in a while„ÄÇYou find out that all that stuff you did last week was pretty bad idea„ÄÇ

That's reinforcement learning„ÄÇ That's semi supervised„ÄÇ

In the sense that only a small subset of the data comes with some ground truth„ÄÇ

 some certainty that you have to then extract knowledge from„ÄÇSo first„ÄÇ

 at the core of anything that's that works currently in terms of practical in a practical sense„ÄÇ

 there has to be some ground truth„ÄÇ There has to be some truth„ÄÇThat we can hold on to„ÄÇ

 as we try to generalize„ÄÇAnd that's supervised learning„ÄÇ even as in reinforcement learning„ÄÇ

 the only thing we can count on is that truth that comes in a form of a reward„ÄÇ

So the standard supervised learning pipeline is you have some raw data„ÄÇThe inputs„ÄÇYou„ÄÇGround truth„ÄÇ

 the labelsÔºå the outputs that matches to the inputs„ÄÇThen you run a certain any kind of algorithm„ÄÇ

 whether that's a neural network„ÄÇOr another prepros processing algorithm that extracts the features from that data set„ÄÇ

You could think of a picture of a face that algorithm could„ÄÇExtract the noseÔºå the eyes„ÄÇ

 the corners of the eyesÔºå the pupil„ÄÇOr even lower level features in that image„ÄÇAfter that„ÄÇ

 we insert those features„ÄÇInto a„ÄÇIn a modelÔºå a machine learning model„ÄÇWe train that model„ÄÇ

Then we as weÔºå whatever that algorithm isÔºå as we pass it through that training process„ÄÇ

 we then evaluate„ÄÇAfter we've seen this one particular example„ÄÇHow much better are we at other tasksÔºü

And as we repeat this loopÔºå the model learns to perform better and better at generalizing from the raw data to the labels that we have„ÄÇ

And finallyÔºå you get to release that model into the wild to actually do prediction„ÄÇ

On data A is never seen before„ÄÇThat you don't know about„ÄÇAnd the task there„ÄÇIs to predict the labels„ÄÇ

OkayÔºå so neural networks is what this class is about„ÄÇ

It's one of the machine learning algorithms that has proven to be very successful„ÄÇ

And the building blockÔºå the computational building block of a neural networkÔºå is a neuron„ÄÇ

A perceptron„ÄÇIs a type of neuron„ÄÇIt's the original old school neuron„ÄÇWhere the output is binary„ÄÇ

 a0 or1„ÄÇIt's not real valued„ÄÇAnd the process„ÄÇTheir perceptron goes through„ÄÇIs it has multiple inputs„ÄÇ

And a single output„ÄÇThe inputsÔºå each of the inputs have weights on them shown here on the left as „ÄÇ7„ÄÇ

6Ôºå1„ÄÇ4„ÄÇThose weights are applied to the inputsÔºå and a perceptron the inputs are ones or zeros„ÄÇBinary„ÄÇ

And those weights are applied„ÄÇJustÔºå and then sum together„ÄÇA bias on each neuron is then added on top„ÄÇ

And„ÄÇA threshold„ÄÇThere's a test whether that summed value plus the bias is below or above a threshold„ÄÇ

 If it's above a thresholdÔºå produces a oneÔºå If it's below a thresholdÔºå it produces a 0Ôºå simple„ÄÇ

It's one of the only things we understand about neural networks„ÄÇConfidently„ÄÇ

 we can prove a lot of things about this neuron„ÄÇFor example„ÄÇWhatÔºå we know„ÄÇ

Is that a neuron can approximate approximate a Nn gate„ÄÇAnd navigategate„ÄÇIs„ÄÇA logical operation„ÄÇ

 a logical function that takes this input„ÄÇIt has two inputsÔºå A and B here on the diagram on the left„ÄÇ

And the table shows what that function is„ÄÇ when the inputs are  zerosÔºå0Ôºå1Ôºå in any order„ÄÇ

 the output is a one„ÄÇOtherwiseÔºå it's a0„ÄÇThe cool thing about an aggregategate is that it's a universal„ÄÇ

üòäÔºåGay„ÄÇThat you can build up any computer you have„ÄÇWhere you phone in your pocket today can be built out of just N gates„ÄÇ

 So it's aÔºå it's a functionally complete„ÄÇ You could build any logical function out of them„ÄÇ

 If you stack them together in arbitrary ways„ÄÇ The problem with N gates and computers„ÄÇ

Is they built from the bottom upÔºå You have to design these circuits of N gates„ÄÇ

So the cool thing here is with the perceptron„ÄÇWe can learn„ÄÇThis magical man„ÄÇ

 we can learn this function„ÄÇüòäÔºåSo let's go through what how we can do that„ÄÇ

How a perceptron can perform the Nand operation„ÄÇHere's the four examplesÔºå if we put„ÄÇ

The weights of negative2 on each of the inputs„ÄÇAnd a bias of three on the neuron„ÄÇ

And if we perform that same operation„ÄÇOf summing the weightsÔºå times the inputs„ÄÇPlusÔºå the bias„ÄÇ

In the top leftÔºå we get„ÄÇWhen the inputs are zeros and there's sum to the biasÔºå we get a three„ÄÇThat's„ÄÇ

 that's a positive numberÔºå which means the output of a perceptron will be a one„ÄÇOn the top right„ÄÇ

 when the input is a 0 and a 1„ÄÇThat sum is still a positive number„ÄÇ AgainÔºå produces a one„ÄÇAnd so on„ÄÇ

 But when the inputs are both onesÔºå then„ÄÇThe output is a negative oneÔºå less than 0„ÄÇSoÔºå while„ÄÇ

 this is„ÄÇSimpleÔºå it's really important to think about„ÄÇIt'sÔºå it's a sort of„ÄÇThe one„ÄÇ

Basic computational truth you can hold on to as we talk about some of the magical things neural network can do„ÄÇ

Because if you compare a circuit of Nan gates„ÄÇIn a circuit of neurons„ÄÇThe difference„ÄÇ

 while a circuit of neurons„ÄÇWhich is what we think of as a neural network„ÄÇ

Can perform the same thing as the circuit of nagates„ÄÇWhat it can also do is it can learn„ÄÇ

It can learn the arbitrary logical functions„ÄÇThat a arbitrary circuit of N gates can represent„ÄÇ

 but it doesn't require the human designer„ÄÇWe can evolve„ÄÇIf you will„ÄÇSo one of the key aspects here„ÄÇ

 one of the key drawbacks of perceptron„ÄÇIs it not very smooth in its' output„ÄÇ

As we change the weights„ÄÇOn the inputsÔºå and we change the bias and we tweak it a little bit„ÄÇ

It's very likely that when you get„ÄÇIt's very easy to make the neuron output a0 instead of a 1 or one instead of a0„ÄÇ

So when we start stacking many of these togetherÔºå it's hard to control the output of the thing as a whole„ÄÇ

NowÔºå the essential step„ÄÇThat makes a neural network„ÄÇWork that a circular perceptrons doesn't„ÄÇ

Is if the output is made smooth„ÄÇIs made continuous with an activation function„ÄÇ

And so instead of using a step functionÔºå like a perceptron does„ÄÇShown there on the leftÔºå we use„ÄÇ

Any kind of smooth functionÔºå a sigmoid„ÄÇWhere the output can change gradually„ÄÇ

As you change the weights and the bias„ÄÇAnd this is a basic but critical step„ÄÇAnd soÔºå learning„ÄÇ

Is generally the process of adjusting those weights gradually and seeing how it has an effect on the rest of the network„ÄÇ

 You just keep tweaking weights here and there and seeing how much closer you get to the ground truth„ÄÇ

And if you get farther away„ÄÇYou just adjust the weights in the opposite direction„ÄÇ

That's neural networks in a nutshell„ÄÇThere is what we' will mostly talk about today is feed forward neural networks„ÄÇ

On the left„ÄÇGoing from inputs to outputs„ÄÇWith no loops„ÄÇThere is also„ÄÇ

These amazing things called recurrent neural networks„ÄÇThey're amazing because they have memory„ÄÇ

They have a memory of state„ÄÇ They remember„ÄÇThe temporal dynamics of the data that went through„ÄÇ

But the painful thing„ÄÇIs that they're really hard to train„ÄÇ

Today we'll talk about feed for neural networks„ÄÇSo let's look at this example„ÄÇYes„ÄÇ

An example of stacking a fewÔºå a few of these neurons together„ÄÇLet's think of the task„ÄÇ

The basic tasks now famous using the classification of numbers„ÄÇYou have an image of a number„ÄÇ

 handwritten number„ÄÇAnd your task is given that image„ÄÇTo say what number is in that image„ÄÇNow„ÄÇ

 what is an imageÔºå An image is a collection of pixelsÔºå In this caseÔºå 28 by 28 pixels„ÄÇ

 that's a total of 784 numbers„ÄÇ Those numbers are from 0 to 255„ÄÇAnd on the left of the network„ÄÇ

 the size of that input„ÄÇDespite the diagram is 784 neurons„ÄÇThat's the input„ÄÇ

Then comes the hidden layer„ÄÇIt's called the hidden layerÔºå because it has„ÄÇIt has no interaction„ÄÇ

With the input or the output„ÄÇIt is simply a block used„ÄÇ

The it's at the core of the computational power of neural networks is the hidden layer„ÄÇ

It's tasked with forming a representation„ÄÇOf the data in such a way that it maps from the inputs to the outputs„ÄÇ

In this caseÔºå there is 15 neurons in a hidden layer„ÄÇThere is 10 values on the output„ÄÇ

Corresponding to each of the numbers„ÄÇThere are several ways you could„ÄÇ

 you can build this kind of network„ÄÇ And this is what the magic of neural networks is you can do it in a lot of ways„ÄÇ

 You only really need four outputs to represent to values 0 through 9„ÄÇBut in practice„ÄÇ

 it seems that having 10 outputs works better„ÄÇAnd how do these work whenever the input is a5„ÄÇ

The output neuron in charge of fiveÔºå gets really excited„ÄÇ and output a value that's close to one„ÄÇ

From zero to one„ÄÇClose to one„ÄÇ And then the other ones„ÄÇGet output of value„ÄÇ

 hopefully that it's close to 0„ÄÇAnd when they don't„ÄÇ

We adjust the weights in such a way that they get closer to 0 and closer to one„ÄÇ

 depending on whether it's the correct neuron associated with the picture„ÄÇ

We'll talk about the details of this training process more tomorrow when it's more relevant„ÄÇBut the„ÄÇ

What we've discussed just now is the forward pass„ÄÇThrough to the network„ÄÇ

It's the pass when you take the inputsÔºå apply the weightsÔºå sum them togetherÔºå add the bias„ÄÇ

 produce the output„ÄÇAnd check which of the outputs produces the highest confidence of the number„ÄÇ

Then once those probabilities for each of the numbers is provided„ÄÇWe determine„ÄÇThe gradient„ÄÇ

That's used to punish or reward the weights that resulted in either the correct or the incorrect decisions„ÄÇ

 And that's called back propagation„ÄÇ We step backwards through the network„ÄÇ

 applying those punishments or rewards„ÄÇBecause of the smoothness of the activation functions„ÄÇ

 that is a mathematically efficient operation„ÄÇThat's where the G GPUs step in„ÄÇ

So far example of numbers„ÄÇThe ground truth„ÄÇFor number six„ÄÇLooks like the following„ÄÇIn the slides„ÄÇ

Y of x equals to„ÄÇA10 dimensional vector„ÄÇWhere only one of them„ÄÇTheÔºå the sixth„ÄÇValues of one„ÄÇ

 The rest are 0„ÄÇThat's„ÄÇThe ground truth that comes with the image„ÄÇThe loss function here„ÄÇ

The basic loss function is the squared error„ÄÇY of x is the ground truth„ÄÇ

And a is the output of the neural network„ÄÇResulting from the forward pass„ÄÇ

So when you input that number of a of a6 and the outputs„ÄÇWhatever it outputsÔºå that's a„ÄÇ

 a tenal vector„ÄÇAnd it's summed over the inputs„ÄÇTo produce the squared error„ÄÇ

That's our loss function„ÄÇ The loss functionÔºå the objective function„ÄÇ That's what's used to determine„ÄÇ

How much to reward or punish the back propagated weights throughout the networkÔºü

And the basic operation„ÄÇOf optimizing that loss functionÔºå of minimizing that loss function„ÄÇ

Is done with various variants ofÔºå of gradient descent„ÄÇIt's hopefully a somewhat smooth function„ÄÇ

But it's a highly nonlinear function„ÄÇThis is why we can't prove much about neural networks„ÄÇIs it„ÄÇ

 it's a highlyÔºå high dimensionalÔºå highly nonlinear function„ÄÇ that's hopefully smooth enough„ÄÇ

Were the gradient descent„ÄÇConfined„ÄÇIt's way to at least a good solution„ÄÇ

And there has to be some stochastic element there„ÄÇThat„ÄÇ

doesn't get that jumps around to ensure that it doesn't get stuck in a local minima„ÄÇ

Of this very complex function„ÄÇOkayÔºå that's supervised learning„ÄÇ There''s inputsÔºå there's outputs„ÄÇ

 ground truth„ÄÇIt's our comfort zone„ÄÇWe're pretty confident we know what's going on„ÄÇ

All you have to do is just you have this data setÔºå you train and„ÄÇ

You train a network on that data and you can evaluate it„ÄÇ

 you can write a paper and try to beat a previous paperÔºå It's great„ÄÇ

The problem is when you then use that neural network to create„ÄÇ

An intelligent system that you put out there in the world„ÄÇ

And now that system no longer is working with your data set„ÄÇ

It has to exist in this world that's maybe very different„ÄÇFrom the ground truth„ÄÇ

So the takeaway from supervised learning„ÄÇIs that neural networks is a great memorizationÔºü

But in a sort of philosophical wayÔºå they might not be great at generalizing„ÄÇAt reasoning beyond„ÄÇ

The specific flavor of data set that they were trained on„ÄÇThe hope for reinforcement learning„ÄÇ

Is that we can extend the knowledge we gain in a supervised way„ÄÇTo the huge world outside„ÄÇ

Where we don't have the ground truth of how to act„ÄÇ

Of what does how good a certain state is or how bad a certain state is„ÄÇ

 This is a kind of brute force reasoning„ÄÇ And I'll talk about„ÄÇKind of what I mean there„ÄÇ

 But it feels like it's closer to reasoning as opposed to memorization„ÄÇ

 That's a good way to think of supervised learning as memorization„ÄÇYou're just studying for an exam„ÄÇ

 And as many of you knowÔºå that doesn't mean you're going to be successful in life just because you get an A„ÄÇ

And so„ÄÇA reinforcement learning agent„ÄÇOr just any agent„ÄÇA human being„ÄÇOr any machine„ÄÇ

Existing in this world„ÄÇCan operate in the following way„ÄÇFrom the perspective of the agent„ÄÇ

 it can execute an action„ÄÇIt can receive an observation„ÄÇResulting from that action„ÄÇ

In a form of a new stateÔºå and they can receive a reward or a punishment„ÄÇYou can break down every our„ÄÇ

 You can„ÄÇ You can break down our existence in this way„ÄÇ Simplistic view„ÄÇ

But it's a convenient one on the computational side„ÄÇAnd from the environment side„ÄÇ

Their environment receives the action„ÄÇAmidst the observation„ÄÇ So your action changes the world„ÄÇ

 ThereforeÔºå that world has to change„ÄÇAnd then tell you about it„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_1.png)

And give your reward or punishment for it„ÄÇSo„ÄÇThat's the„ÄÇAgainÔºå one of the most fascinating things„ÄÇ

 I'll try to convey why this is fascinating a little bit later on„ÄÇIs the work of deep mind„ÄÇOnatari„ÄÇ

This is Atari breakout„ÄÇA game„ÄÇWhere a paddle has to move around„ÄÇThat's the world it's existing in„ÄÇ

 It's a paddle„ÄÇ The agent is a paddle„ÄÇAnd there's a bouncing ball„ÄÇ

 And you're trying to move your actions to rightÔºå move rightÔºå move left„ÄÇ

In you trying to move in such a way that the ball doesn't get past you„ÄÇ

And so here is a human level performance of that agent„ÄÇ

And so what does this panelddle have to do that's to operate in this environment„ÄÇ That's to act„ÄÇ

 move leftÔºå move right„ÄÇEach action changes the state of the world as may seem obvious„ÄÇ

 but moving right changes visually„ÄÇThe state of the worldÔºå in fact„ÄÇ

 what we're watching now on the slides„ÄÇIs the world changing before your eyes for this little guy„ÄÇ

And it gets rewards or punishments„ÄÇRewardsÔºå it gets in the form of points„ÄÇ

 They're racking up points in the top left of the of the video„ÄÇAnd then when the ball gets passed„ÄÇ

 the paddle„ÄÇIt gets punished„ÄÇBy dyingÔºå quote unquote„ÄÇ

 and that's the number of lives there has left going from 5 to 4 to 3„ÄÇDown to 0„ÄÇ

And so the goal is to select at any one moment„ÄÇThe action that maximizes future award„ÄÇ

Without any knowledge of what a reward is„ÄÇIn a greater sense of the word„ÄÇ

 all you have is an instantaneous reward of punishment„ÄÇ

Instantaneous response to the world to your actions„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_3.png)

And this can be modeled as a Markov decision process„ÄÇMark our decision process„ÄÇ

Is a mathematically convenient construct„ÄÇIt has no memory„ÄÇAll you get is you have a state„ÄÇ

That you're currently inÔºå you perform an actionÔºå you get a reward„ÄÇ

 and you find yourself in a new state„ÄÇ And that repeats over and over„ÄÇYou start you from state 0„ÄÇ

 you go to state 1„ÄÇ You once againÔºå repeat an actionÔºå get a rewardÔºå go to the next state„ÄÇOkay„ÄÇ

 that's the formulation that we're operating in„ÄÇWhen you're in a certain state„ÄÇ

 you have no memory of what happened„ÄÇTwo states go„ÄÇEverything is operating on the instantaneous„ÄÇ

Instantaneously„ÄÇAnd so what are the major components of a reinforcement learning agent„ÄÇ

 there's a policy„ÄÇThats that's the agentÔºå the function broadly defined of an agent's behavior„ÄÇ

That means„ÄÇThat includes the knowledge of howÔºå for any given state„ÄÇ

 what is an action that I will take„ÄÇWith some probability„ÄÇ

Value function is how good each state and action are in any particular state„ÄÇAnd there's a model„ÄÇNow„ÄÇ

 this is„ÄÇA little„ÄÇA subtle thing that is actually the biggest problem with everything you'll see today is the model is how we represent the environment„ÄÇ

And what you'll see today is some amazing things that neural networks can achieve„ÄÇ

On a relatively simplistic model of the world„ÄÇ And to question whether that model can extend to the real world where human lives are at stake in the case of driving„ÄÇ

So let's look at the simplistic worldÔºå a robot in a room„ÄÇYou start at the bottom left„ÄÇ

 your goal is to get to the top right„ÄÇYour possible actions are going up downÔºå left and right„ÄÇNow„ÄÇ

 this world could be deterministicÔºå which means when you takeÔºå when you go upÔºå you actually go up„ÄÇ

Or it could be non deterministic„ÄÇAs human life is„ÄÇIts when you go upÔºå sometimes you go right„ÄÇ

So in this caseÔºå if you choose to go upÔºå you move up 80% of the timeÔºå you move left 10% of the time„ÄÇ

 and you move rightÔºå10% of the time„ÄÇAnd when you get to the top rightÔºå you get a reward of plus one„ÄÇ

 and you get to the second block from thatÔºå42Ôºå you get negative oneÔºå you get punished„ÄÇ

And every time you take a stepÔºå you get a slight punishment„ÄÇA „ÄÇ04„ÄÇOkayÔºå so the question is„ÄÇ

If you start the bottom left„ÄÇIs this a good solution„ÄÇ

 Is this a good policy by which you exist in the worldÔºüAnd it is if the world is deterministic„ÄÇ

Whenever you choose to go upÔºå you'll go up„ÄÇWhen ever choose to go rightÔºå you go right„ÄÇ

But if the actions are stochasticÔºå that's not the case„ÄÇIn what I described previously„ÄÇWith „ÄÇ8 up„ÄÇ

 and„ÄÇProbability of 01Ôºå going left and right„ÄÇThis is the optimalum policy„ÄÇNow„ÄÇ

 if we punish every single step with a negative 2 as opposed to negative 004„ÄÇ

So every time you take a stepÔºå it hurts„ÄÇYou're going to try to get to a positive block as quickly as possible„ÄÇ

And that's what this policy says„ÄÇ I'll walk through a negative one if I have to„ÄÇ as long as I get„ÄÇ

 stop getting the negative 2„ÄÇNowÔºå if the reward for each step is a negative „ÄÇ1„ÄÇ

You might choose to go around that negative1 block„ÄÇSlight detour to avoid the pain„ÄÇ

And then you might take an even longer detour as the reward for each step goes up or the punishment goes down„ÄÇ

 I guess„ÄÇAnd then„ÄÇÂì¶„ÄÇAnd then there's an actual positive reward for every step you take„ÄÇThen„ÄÇ

 you'll never„ÄÇYou'll avoid going to the finish line„ÄÇüò°ÔºåYou'll just wander the world„ÄÇWe saw that„ÄÇ

With the coast racer yesterdayÔºå the boat that chose not to finish the race because it was having too much fun getting points in the middle„ÄÇ

So„ÄÇLet's look at the world that this agent is operating in„ÄÇAs a value function„ÄÇ

That value function depends on a reward„ÄÇThe word that comes in the future„ÄÇ

And that reward is discounted because the world is stochastic„ÄÇ We don't„ÄÇ We can't expect the reward„ÄÇ

To come along to us in the way that„ÄÇWe hope it does based on the policy„ÄÇ

 based on the way we choose to act„ÄÇAnd so there's a gamma there thatÔºå over time„ÄÇ

As their reward is farther and farther into the futureÔºå discounts„ÄÇThat reward„ÄÇ

Diminishes the impact of that future award in your evaluation of the current state„ÄÇ

And so your goal is„ÄÇTo develop a strategy that maximizes the discounted future awardÔºå this sum„ÄÇ

 this discounted sum„ÄÇAnd reinforcement learning„ÄÇTheres a lot of approaches for coming up„ÄÇ

With a good policy„ÄÇA near optimalÔºå an optimal policy„ÄÇThere's a lot of fun math there„ÄÇ

You could try to construct a model that optimizes some estimate of this world„ÄÇ

You can try in a Monte Carlo way through just„ÄÇSimulate that world and see how it unrolls„ÄÇ

And as it unrollsÔºå you try to compute the optimal policy„ÄÇ

Or what we'll talk about today is Q learning„ÄÇIt's„ÄÇAn off policy approach„ÄÇWhere the policy„ÄÇ

Is estimated as we go along„ÄÇThe policy„ÄÇIs represented as a Q function„ÄÇThe Q function„ÄÇ

Shown there on the left isÔºå I apologize for the equations I lied„ÄÇTherell be some equations„ÄÇ

The input to the Q function is a state at time TÔºå S T and an action that you choose to take in that state A T„ÄÇ

And your goal is in that state to choose an action which maximizes the reward is the next step„ÄÇ

And what Q learning does„ÄÇ And I'll describe the process„ÄÇ

Is it's able to approximate through experience„ÄÇThe optimal Q function„ÄÇ

The optimal function that tells you how to act in any state of the world„ÄÇYou just have to„ÄÇLive it„ÄÇ

 You have to simulate this world„ÄÇYou have to move about it„ÄÇYou have to explore„ÄÇ

In order to see every possible stateÔºå try every different action„ÄÇGet rewardedÔºå get punished„ÄÇ

And figure out what is the optimal thing to do„ÄÇThat's done using„ÄÇThis Bowman equation„ÄÇOn the left„ÄÇ

 the output is the new state„ÄÇThe estimate„ÄÇThe Q function estimate of the new state for new action„ÄÇ

And thatsÔºå this is the update rule at the core of Q learning„ÄÇYou take the estimateÔºå the old estimate„ÄÇ

An ad based on the learning rate„ÄÇAlpha from 0 to 1„ÄÇThey update the evaluation of that state„ÄÇ

Based on your new reward that you received at that time„ÄÇSo you've arrived in a certain state S T„ÄÇ

 You try to do an action„ÄÇAnd then you got a certain reward and you update your estimate of that state„ÄÇ

An action pair based on this rule„ÄÇWhen the learning rate is 0Ôºå you don't learn„ÄÇ when alpha is0„ÄÇ

You never change„ÄÇYour worldviewÔºå based on the new new incoming evidence„ÄÇWhen alpha is one„ÄÇ

You every time change„ÄÇYour evaluationÔºå your world evaluation based on the new evidence„ÄÇ

And that's the key ingredient to reinforcement learning„ÄÇFirstÔºå you exploreÔºå then you exploit„ÄÇFirst„ÄÇ

 you explore in a non greedy wayÔºå and then you get greedy„ÄÇ

 you figure out what's good for you and you keep doing it„ÄÇ

So if you want to learn an Atara game firstÔºå you try every single action every stateÔºå you screw up„ÄÇ

 get punishedÔºå get rewarded„ÄÇ and eventually you figure out what's actually the right thing to do„ÄÇ

 and you just keep doing it„ÄÇ And that's how you win„ÄÇAgainst theÔºå the greatest world„ÄÇ

 the greatest human players in the world in the game of GoÔºå for exampleÔºå as we'll talk about„ÄÇüòä„ÄÇ

And the way you do that is you have an epsilon greedy policy that over time„ÄÇWith a probability„ÄÇ

Of 1 minus epsilonÔºå you perform an optimal greedy action with a probability of epsilon„ÄÇ

 You perform a random action„ÄÇRandom action being explore„ÄÇAnd so as epsilon goes down from1 to 0„ÄÇ

 you explore less and less„ÄÇSo the algorithm here is really simple„ÄÇOn the bottom of the slide there„ÄÇ

It's the algorithm versionÔºå the pseudocode version of the equation„ÄÇThe Bellman equation updateate„ÄÇ

You initialize your estimate of state action pairs„ÄÇArbitrarilyÔºå a random number„ÄÇ Now„ÄÇ

 this is an important point„ÄÇWhen you start playing or living or doing whatever you're doing and whatever you're doing with reinforcement learning or driving„ÄÇ

You have no preconceived notion of what's good and bad„ÄÇ It's random„ÄÇOr however„ÄÇ

 you choose to initialize it„ÄÇ And the fact that it learns anything is amazing„ÄÇ

I want you to remember that„ÄÇThat's one of the amazing things about„ÄÇThe Q learning at all„ÄÇ

 And then the deep neural network version of Q learning„ÄÇThe algorithm repeats the following step„ÄÇ

 You step into the worldÔºå observe in an initial state„ÄÇ you select„ÄÇAnd actionction A„ÄÇSo that action„ÄÇ

 if you're exploring will be a random actionÔºå if you're greedily pursuing the best action you can„ÄÇ

 it will be the action that maximizes the Q function„ÄÇYou observe an reward after you take the action„ÄÇ

And a new state that you find yourself in„ÄÇ And then you update your estimate„ÄÇ

Of the previous state you are in having given taken that action using that Belman equation update„ÄÇ

And repeat this over and over„ÄÇAnd so there on the bottom of the slide is„ÄÇA summary of„ÄÇLife„ÄÇYesÔºå he„ÄÇ

The Q function„ÄÇYesÔºå yes„ÄÇ yeah„ÄÇ It's a single„ÄÇThe question wasÔºå is the Q function a single valueÔºü

 And yesÔºås it's just a single continuous value„ÄÇSo the question wasÔºå how do you model the worldÔºüSo„ÄÇ

The way you model„ÄÇ So let's start this very simplistic world of Atari„ÄÇPaddle„ÄÇ

 you think you model it as a paddle that can move left and right„ÄÇ and there's some blocks„ÄÇ

 and you model the physics of theÔºå the ball„ÄÇThat requires a lot of expert knowledge in that particular game„ÄÇ

 So you sit there handcrafting this model„ÄÇThat's hard to doÔºå even for a simplistic game„ÄÇ

The other model you could take„ÄÇIs looking at this world in the way that humans do visuallyÔºå so„ÄÇ

Take the model in as a set of pixels„ÄÇJust the model is all the pixels of the world„ÄÇ

 You know nothing about paddles or balls or physics or colors and points„ÄÇ

 they're just pixels coming in„ÄÇThat seems like a ridiculous model of the world„ÄÇ

 but it seems to work for Atari„ÄÇ It seems to work for human beings„ÄÇ When you're born„ÄÇ

 you see there's light coming into your eyes„ÄÇAnd you don't have any„ÄÇAs far as as far as we know„ÄÇ

 you don't come with an instruction when you're born„ÄÇYou knowÔºå there's people in the world„ÄÇ

 and there is„ÄÇThere's good guys and bad guysÔºå and this is how you walk„ÄÇ No„ÄÇ

 all you get is light sound„ÄÇAnd the other sensors„ÄÇAnd„ÄÇ

And you get to learn about the every single thing you think of as„ÄÇ

The way you model the world is a learned representation„ÄÇ

 And we'll talk about how a neural network does that„ÄÇ It learns to represent the world„ÄÇ

But if we have to hand model the world„ÄÇIt's an impossible task„ÄÇIt'ss that's the question„ÄÇ

 If we have to hand model the world„ÄÇThen that world better be a simplistic one„ÄÇ

That's a great question„ÄÇ So the question wasÔºå what is the robustness of this model if„ÄÇ

 if the way you represent the world is at all even slightly different from the way you thought that world is„ÄÇ

That's not that well„ÄÇStudy as far as I'm awareÔºå you I mean„ÄÇ

 it's already amazing that if you constructÔºå if you have a certain input of the world„ÄÇ

 if you have a certain model of the worldÔºå you can learn anything' is already amazing„ÄÇ

The question isÔºå and it's an important one is we'll talk a little bit about it„ÄÇ

 not about the world modelÔºå but the reward function„ÄÇ If the reward function is slightly different„ÄÇ

The real reward function of life or of driving or of coast runner is different than what you expected it to be„ÄÇ

 What's theÔºå what's the negative there„ÄÇ YeahÔºå it'sÔºå it's it could be hugeÔºå so„ÄÇ

There's another question or noÔºå never mindÔºå yep„ÄÇSorryÔºå can you ask that again„ÄÇYes„ÄÇ

 you can change it over„ÄÇ So the question wasÔºå do you change the alpha value over time and you certainly should change the alpha value over time„ÄÇ

 yeah„ÄÇelse„ÄÇSo the question wasÔºå what is the„ÄÇThe complex interplay of the epsilon function with the Q learning update„ÄÇ

That's 100% fine tunedÔºå hand tuned to the particular learning problem„ÄÇ So you certainly want to„ÄÇ

The more complexÔºå theÔºå theÔºå the larger the number of states in the world and the larger the number of actions„ÄÇ

 the longer you have to„ÄÇWait before you decrease epsilon to zeroÔºå but you have to play with it„ÄÇ

AndIt's one of the parameters you have to play withÔºå unfortunatelyÔºå and there's quite a few of them„ÄÇ

Which is why you can't just drop a reinforcement learning agent into the world„ÄÇOh„ÄÇ

 the effect in that senseÔºå NoÔºå noÔºå it's just a coin flip„ÄÇAnd ifÔºå if that epsilon is 0„ÄÇ

5 half the timeÔºå you're gonna take a random action„ÄÇ So it's noÔºå there's no specific„ÄÇ

 It's not like you'll take the best action„ÄÇAnd then with some probability„ÄÇ

 take the second bass and so on„ÄÇ I meanÔºå you can certainly do that„ÄÇ

 But then the simple formulation that works is you just take a random action because you don't want to have a preconceived notion of what's a good action to try when you're exploring„ÄÇ

 The whole point is you try crazy stuff„ÄÇIf it's a simulation„ÄÇOkayÔºå so good question„ÄÇ

So representation matters„ÄÇThis is the question about how we represent the world„ÄÇ

 So we can think of this world of breakoutÔºå for example„ÄÇOf the Saari game„ÄÇ

As a paddle that moves left and right„ÄÇAnd the exact position of the different things you can hit„ÄÇ

 construct this complex modelÔºå this expert driven model that has to fine tune it to this particular problem„ÄÇ

But„ÄÇIn practiceÔºå the more complex this model gets the worse that Bellman equation update„ÄÇ

 that value the trying to construct a Q function for every single combination of state and actions becomes too difficult because that function is too sparse and huge„ÄÇ

 So if you think„ÄÇOf„ÄÇBecause of looking at this world in a general wayÔºå in the way human beings would„ÄÇ

 is a collection of pixels visually„ÄÇIf you just take in a pixel„ÄÇ

 this game is a collection of 84 by 84 pixelsÔºå an imageÔºå an RGB image„ÄÇ

And then you look at not just the current image„ÄÇBut look at the temporal„ÄÇTrajectory of those images„ÄÇ

 So likeÔºå if there's a ball movieÔºå you want to know about that movement„ÄÇ So you look at„ÄÇ4 images„ÄÇ

 So current image and three images back„ÄÇAnd say they're gray scale with 256 gray levels„ÄÇ

That size of the Q tableÔºå that the Q value function„ÄÇIt has to learn„ÄÇIs„ÄÇWhatever that number is„ÄÇ

 but it's certainly larger than the number of atoms in the universe„ÄÇThat's a large number„ÄÇ

 So you have to run the simulation long enough to touch at least a few times„ÄÇ

Most of the states in that Q table„ÄÇSo„ÄÇAs Elon Musk saysÔºå you may need to runÔºå you know„ÄÇ

 we live in a simulation„ÄÇYou may runÔºå have to run a universe just to to„ÄÇ

 to compute the the the Q function in this case„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_5.png)

So that's where deep learning steps in„ÄÇAs instead of„ÄÇMoodeling the world„ÄÇAs a Q table„ÄÇYou estimate„ÄÇ

 you try to learn that function„ÄÇAnd so the takeaway from supervised learning„ÄÇ

 if you remember that it's good at memorizing or good at memorizing data„ÄÇ

The hope for reinforcement learning„ÄÇWith a QÔºå Q learning„ÄÇIs that we can extend„ÄÇ

The occasional rewards we get to generalize over the operation„ÄÇ

 the actions you take in that world leading up to the rewards„ÄÇ

And the hope for deep learning is that we can move this reinforcement learning system„ÄÇ

Into a world that doesn't need to beÔºå they can be defined arbitrarily„ÄÇCan include„ÄÇ

All the pixels of an Atari game can include all the pixels sentd by a drone or a robot or a car„ÄÇ

But stillÔºå it needs a formalized definition of that worldÔºå which is much easier to do„ÄÇ

When you're able to take in sensorsÔºå like an image„ÄÇSo deep Q learningÔºå a deep version„ÄÇ

So instead of learning a Q tableÔºå a Q function„ÄÇWe tryÔºå and estimating that„ÄÇQ prime„ÄÇ

We try to learn it„ÄÇUsing machine learning„ÄÇSo it tries to learn some parameters„ÄÇThis huge„ÄÇ

Complex function„ÄÇWe try to learn it„ÄÇAnd the way we do that„ÄÇAs we have a neural network„ÄÇ

 the same kind that I showed that learned the numbers to map from an image to a classification of that image into a number„ÄÇ

 the same kind of network is used to take an a state„ÄÇAnd an action and produce a Q value„ÄÇNow„ÄÇ

 here's the amazing thing„ÄÇThatÔºå without knowing anything„ÄÇIn the beginning„ÄÇAs I saidÔºå with a Q table„ÄÇ

 it's initialized randomly„ÄÇThe Q function„ÄÇThis deep network knows nothing in the beginning„ÄÇ

All it knows„ÄÇIs in the simulated worldÔºå the awards you get for a particular game„ÄÇ

 So you have to play time and time again and see„ÄÇTheÔºå the rewards you get for every single„ÄÇ

Iteration of the game„ÄÇBut in the beginningÔºå it knows nothing„ÄÇ

And it's able to learn to play better than human beings„ÄÇThis is a deep mind paper„ÄÇ

Playing at with deeper reinforcement learning„ÄÇFrom 2013„ÄÇ

That's one of the key things that got everybody excited about their role of deep learning and artificial intelligence„ÄÇ

Is that„ÄÇUsing a comb neural networkÔºå which I'll talk about tomorrow„ÄÇ But it's„ÄÇ

 it's a vanilla networkÔºå like any other like I talked about earlier todayÔºå just a regular network„ÄÇ

It takes the raw pixelsÔºå as I said„ÄÇAnd estimates that Q function from the raw pixels is able to play at many of those games better than a human being„ÄÇ

And the loss function that I mentioned previously„ÄÇSo„ÄÇAgainÔºå very vanilla loss function„ÄÇ

 very simple objective function„ÄÇTheÔºå the first one youll probably implement„ÄÇ

 we have a tutorial and tensorflow„ÄÇSquared error„ÄÇ So we take this Belman equation„ÄÇ

Where the estimate is  Q„ÄÇThe Q function estimate of state and action„ÄÇIs the maximumÔºü

Rewards you get for taking any of the actions„ÄÇThat takes you to any of the future states„ÄÇAnd you„ÄÇ

Try to take that actionÔºå observeer the result of that action„ÄÇAnd if the target is different„ÄÇ

That your learned targetÔºå the learnedÔºå what the function has learned is the expected reward in that case is different than what you actually got„ÄÇ

You adjust itÔºå you adjust the weights in the network„ÄÇAnd this is exactly the process by which we„ÄÇ

Learn how to exist in this pixel world„ÄÇSoÔºå you're mapping states„ÄÇAnd actions„ÄÇSo a Q value„ÄÇ

The algorithm is as follows„ÄÇThis is how we train it„ÄÇ We're given a transition„ÄÇ

S current state action taken in that state are the rewards you get„ÄÇ And S prime is what you„ÄÇ

 the state you find yourself in„ÄÇAnd so we replaced the basic update rule in the previous pseudocode„ÄÇ

By„ÄÇTaking a forward pass„ÄÇThrough the networkÔºå given that S state„ÄÇWe look at what„ÄÇ

The predicted Q value is of that action„ÄÇWe then do another forward pass through that network and see what we actually get„ÄÇ

And then if we're totally off„ÄÇÊØè„ÄÇPunishÔºå we back propagate the weights in a way that next time willll make less of that mistake„ÄÇ

And you repeat this process„ÄÇAnd this is a„ÄÇYou're playing your this is a simulation„ÄÇ

You're learning against yourself„ÄÇAnd againÔºå the same rule applies here„ÄÇ

 exploration versus exploitation„ÄÇYou start out with an epsilon of„ÄÇZero or1„ÄÇYou you're„ÄÇ

Mostly exploring„ÄÇAnd then you move towards an epsilon of 0„ÄÇAnd with theari breakout„ÄÇ

 this is the deep mind paper result„ÄÇ is training epics on the X axis on the Y axis is the average action value and the average reward per episode„ÄÇ

I'll show why it's kind of an amazing resultÔºå but it's messy because there's a lot of tricks involved„ÄÇ

üòäÔºåSo it's not just putting in a bunch of pixels of a game and getting an agent that knows how to win at that game„ÄÇ

 There's a lot of preprocess and playing with the data required„ÄÇSo which is unfortunate„ÄÇ

 because the truth is messier„ÄÇThenhan the hope„ÄÇBut one of the critical tricks needed is called experience replay„ÄÇ

So as opposed to letting an agent„ÄÇ So you're learning this big network that learn that tries to build a model of what's good to do in the world and what's not„ÄÇ

And you're learning as you go„ÄÇSo with with experience replay„ÄÇ

 you're keeping a track of all the things you did„ÄÇ And every every once in a while„ÄÇ

 you look back into your memory and pull out some of those old experiences„ÄÇ

 the old good old times and train on those again„ÄÇüòäÔºåAs opposed to letting the agent„ÄÇ

Run itself into some local optima where it tries to learn a very subtle aspects of the game that actually in the global sense„ÄÇ

 doesn't get you farther to winning the game„ÄÇVery much like life„ÄÇSo here's the algorithm„ÄÇ

 deep Q learning algorithm„ÄÇCo code„ÄÇWe initialized the replay memory„ÄÇ Again„ÄÇ

 there this is this little trick that's required„ÄÇIt's keeping a track of stuff that's happened in the past„ÄÇ

We initialized the action value function Q with random weights„ÄÇAnd observe initial state„ÄÇ again„ÄÇ

 same thing„ÄÇ Select in action„ÄÇWith a probability epsilonÔºå explore„ÄÇOtherwise„ÄÇ

 choose the best one based on the estimate provided by the neural network„ÄÇ

And then carry out the actionÔºå observe their award„ÄÇAnd store that experience in the replay memory„ÄÇ

And then sampled random transition from replay memory„ÄÇSoÔºå with a certain probability„ÄÇ

You bring those old timers back to get yourself out of the local mini„ÄÇ

And then you train the Q network„ÄÇUsing theÔºå the difference between„ÄÇYour„ÄÇ

What you actually got and your estimate„ÄÇYou repeat this processÔºå over and over„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_7.png)

So here's what you can do after 10 minutes of training„ÄÇOn the leftÔºå so that's very little training„ÄÇ

What you get is a paddle that„ÄÇLearns hardly anythingÔºå and it just keeps dying„ÄÇ

 If you look at it goes from five to four to3 to2 to oneÔºå those are the number of lives left„ÄÇ

Then after two hours of training and a single GPU„ÄÇIt learns to win itÔºå win the„ÄÇYou know„ÄÇ

 not die rackup points„ÄÇAnd learns to avoid the ball from passing„ÄÇPassing the paddleÔºå which is great„ÄÇ

 That's human level performanceÔºå really„ÄÇBetter than some humansÔºå you know„ÄÇ

 but it still dies sometimes„ÄÇ So it's very human level„ÄÇAnd then after four hours„ÄÇ

 it does something really amazing„ÄÇüòäÔºåIt figures out„ÄÇHow to win the game in a very lazy way„ÄÇ

 which is drill„ÄÇI hole through the blocks up to the top and get the ball stuck up there„ÄÇ

 and then it does all the hard work for you„ÄÇ That minimizes the probability of the ball getting past your paddle because it's just stuck in in the blocks up top„ÄÇ

 So that might be something that you wouldn't even figure out to do yourself„ÄÇAnd that's an„ÄÇ

I need to sort of pause here to clearly explain what's happeningÔºå the input to this algorithm„ÄÇ

Is just the pixels of the game„ÄÇIt's the same thing that human beings take in„ÄÇ

When you take the visual perception„ÄÇAnd it's able to learn„ÄÇUnder this constrained„ÄÇ

Definition of what is a reward and a punishment It's able to learn„ÄÇTo get a high reward„ÄÇ

That's general artificial intelligence„ÄÇA very small example of itÔºå but it's general„ÄÇ

 It's general purpose„ÄÇ It knows nothing about games„ÄÇ It knows nothing about paddles or physics„ÄÇ

 It's just taking sensory input of the game„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_9.png)

And theyve did the same thing for a bunch of different games in Atari„ÄÇAnd„ÄÇ

What's shown here in this plot on the X axis and is a bunch of different games from Atari and on the Y axis is a percentile where 100% is about the best that human beings can do„ÄÇ

 meaning it's the score that human beings would get„ÄÇ So everything about there in the middle„ÄÇ

 Everything to the left of that is far exceeding human level performance„ÄÇ

And below that is on par or worse than human level performance„ÄÇ So you can learn all so many boxing„ÄÇ

pinball„ÄÇAll of these gamesÔºå and it doesn't know anything about any of the individual games„ÄÇ

 It's just taking them pixels„ÄÇ It's just as if you put a human being behind any of these games„ÄÇAnd„ÄÇ

Ask them to learn to be„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_11.png)

Beat the game„ÄÇAnd there's been a lot of improvements in this algorithm recently„ÄÇ YesÔºå question„ÄÇNope„ÄÇ

 noÔºå there's no„ÄÇ So the question wasÔºå do they customize the model for game for a particular game And no„ÄÇ

 the point you couldÔºå of course„ÄÇ But the point is it doesn't need to be a customized for the game„ÄÇ

 but„ÄÇBut the important thing is„ÄÇThat it's still only on Atari games„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_13.png)

So the questionÔºå whether this is transferable to driving„ÄÇPerhaps not„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_15.png)

RightÔºå you play the game where you do or notÔºå you don't have the YeahÔºå you play one step of the game„ÄÇ

SeeÔºå you take action in a state„ÄÇAnd then you observe that„ÄÇ So you have the simulation„ÄÇ I mean„ÄÇ

 that's really„ÄÇThat's one of the biggest problems here is you require the simulation to in order to get the ground truth„ÄÇ

Yes„ÄÇSo that's a great question or a comment„ÄÇ the comment was that for a lot of these situations„ÄÇ

The reward function might not change at allÔºå depending on your actions„ÄÇ The awards are really„ÄÇ

Most of the time delayed 10Ôºå 20Ôºå 30 steps down the line„ÄÇ

 which is why it is amazing that this works at all„ÄÇThat it's learningÔºå it's learning locally„ÄÇ

And through that process of simulation of hundreds of thousand times runs through the game„ÄÇ

 it's able to learn„ÄÇüòäÔºåWhat to do now such that I get a reward later„ÄÇItÔºå it's if you just pause„ÄÇ

 look at the math of itÔºå it's very simple math„ÄÇAnd look at the resultÔºå it's incredible„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_17.png)

So there's a lot of improvementsÔºå this one„ÄÇCalled the General reinforcement Learning architectureitecture or gorilla„ÄÇ

The cool thing about this in the simulated worldÔºå at least„ÄÇ

 is that you can run deeper reinforce learning in distributed way„ÄÇ

 You could do both the simulation in distributed way„ÄÇ

 You could do the learning in the distributed way„ÄÇüòäÔºåYou canÔºå you can generate experiences„ÄÇ

 which is what this kind of diagram shows„ÄÇ You can either from human beings„ÄÇOr from simulation„ÄÇSo„ÄÇ

 for exampleÔºå the way that„ÄÇThe way that Alpha GoÔºå the Deep Mind team has beat the game of Go is they learn from both expert games and by playing itself„ÄÇ

So you can do this in a distributed wayÔºå and you can do the learning a distributed way so you can scale„ÄÇ

 And in this particular caseÔºå the gorilla„ÄÇHas achieved a better result than the DQN network„ÄÇ

That's part of their nature paper„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_19.png)

OkayÔºå so let me now get to„ÄÇDriving for a second here„ÄÇ whereÔºå Where does reinforcement learning„ÄÇ

Where reinforcement learning can step in„ÄÇAnd help„ÄÇSo this is back to the open question that I asked yesterday„ÄÇ

 is driving closer to chess„ÄÇOr to everyday conversation„ÄÇChes„ÄÇ

 meaning it can be formalized in a simplistic wayÔºå and then we could think about it as an obstacle avoidance problem„ÄÇ

 And once the obstacle avoidance is solved„ÄÇYou just navigate that constrained space„ÄÇ

 you choose to move leftÔºå you choose to move right in laneÔºå you choose to speed up or slow down„ÄÇWell„ÄÇ

 if it's a game like chessÔºå which we'll assume for today„ÄÇAs opposed to for tomorrowÔºå for today„ÄÇ

We're going to go with the one on the left„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_21.png)

And we're going to look at deep traffic„ÄÇHere's this„ÄÇGame„ÄÇA simulation„ÄÇ

Where the goal is to achieve the highest average speed you can„ÄÇOn this seven lane highway„ÄÇ

Full of cars„ÄÇAnd so as a side note for studentsÔºå the requirement is they have to follow the tutorial that I'll present a link for at the end of this presentation„ÄÇ

And what they have to do is achieve a speed„ÄÇ build a network that achieves a speed of 65 m an hour or higher„ÄÇ

There is a leaderboard„ÄÇAnd you get to submit the model you come up with with a simple click of a button„ÄÇ

 So all of this runs in the browserÔºå which is also another amazing thing„ÄÇAnd then you immediately„ÄÇ

 or relatively so„ÄÇMake your way up to leader board„ÄÇSo let's lookÔºå let's zoom in„ÄÇ What is this„ÄÇ

 what is this worldÔºå two dimensional world of traffic is„ÄÇ

What does it look like for the intelligence systemÔºü

We destize that world into a grid shown here on the left„ÄÇ That's the representation of the state„ÄÇ

 There are  seven lanesÔºå and every single lane is broken up into blocks spatially„ÄÇ

And if there's a car in that blockÔºå the length of a car is about three blocks„ÄÇ

Three of those grid blocks„ÄÇThen that grid is seen is occupied„ÄÇAnd then the red car is you„ÄÇ

That's the thing that's running in the intelligent agent„ÄÇ

There is on the left is the current speed of the red car„ÄÇActually she says MIT on top„ÄÇ

And then you also have a account of how many cars you passed„ÄÇAnd if your network sucks„ÄÇ

 then that number is going to get to be negative„ÄÇYou can also change with a drop down„ÄÇ

 the simulation speed from normal on the left to fast on the right„ÄÇ So normal is„ÄÇSoÔºå you know„ÄÇ

 the the fast speeds up the replay of the simulation„ÄÇThe one on the left normal„ÄÇ

 it feels a little more like real driving„ÄÇTheÔºå there is a drop down for different display options„ÄÇ

 The default is none in terms of stuff you show on the road„ÄÇThen there is the learning input„ÄÇ

Which is the while the whole space is Destized„ÄÇYou can choose what your car sees„ÄÇ

And that's you could choose how far head it sees behindÔºå how far to the left and right it sees„ÄÇ

And so by choosing the learning input to visualize the learning input„ÄÇ

 you get to see what you set that input to be„ÄÇThen there is„ÄÇThe safety system„ÄÇ

This is a system that protects you from yourself„ÄÇThe way we've made this game„ÄÇ

Is that it operates under something similar„ÄÇIf you have some intelligence„ÄÇ

 if you drive and you have adaptive cruise control in in your carÔºå it operates in the same way„ÄÇ

 when it gets close to the car in frontÔºå it slows down for you„ÄÇ

And it doesn't let you run the car to the left of youÔºå to the right of you off the road„ÄÇ

So conss the movement„ÄÇCpabilities of your car in such a way that you don't hit anybody„ÄÇ

Because then it would have to simulate collisionsÔºå and it would just be a mess„ÄÇ

So it protects you from that„ÄÇ And so you can choose to visualize that quoteÔºå unquoteÔºå safety system„ÄÇ

With the visualization box„ÄÇ And then you can also choose to visualize the full map„ÄÇ

This is the full occupancy map that you getÔºå if you would like to„ÄÇProvide his input to the network„ÄÇ

NowÔºå that input for every single gridÔºå that it's a number„ÄÇ It's not just a 0Ôºå1„ÄÇ

 whether there's a car in there„ÄÇIt's„ÄÇThe maximum speed limitÔºå which is 80 m hour„ÄÇ'tÔºå't go„ÄÇ

 don't get crazyÔºå80 miles an hour is the speed limit„ÄÇThat blockÔºå when it's empty„ÄÇ

 is set to the 85 milesÔºå80 miles an hour„ÄÇAnd when it's occupied„ÄÇ

 it's set to the number that's the speed of the car„ÄÇAnd then the blocks that you„ÄÇ

 the red car is occupying is said to the number to a very large number„ÄÇ

 much higher than the speed limit„ÄÇSo safety system here shown in red„ÄÇ

Are the parts of the grid that are not that your car can't move into questionÔºüÊàëÂ°û„ÄÇYep„ÄÇYes„ÄÇYes„ÄÇ

 the question was„ÄÇWhat was the third option I just mentionedÔºå and it's youÔºå the red card itself„ÄÇ

You yourselfÔºå the blocks underneath that car are set to a really high number„ÄÇ

 It's a way for the algorithm to knowÔºå for the learning algorithm to know„ÄÇ

That these blocks are special„ÄÇSoÔºå safety system„ÄÇShows red here if the car can't move into those blocks„ÄÇ

So any„ÄÇIn terms of„ÄÇWhen when it lights up redÔºå it means the car can't speed up anymore in front of it„ÄÇ

 And when the blocks to the left or to the rightÔºå light up is red„ÄÇ

 that means you can't change lanes to the left or right„ÄÇOn the right of the slide„ÄÇYou're free to go„ÄÇ

 free to do whatever you want„ÄÇ That's what that indicates„ÄÇ is all the blocks are yellow„ÄÇ

Safety system says you're free to choose any of the five actions and the five actions are move left„ÄÇ

 move rightÔºå stay in placeÔºå accelerate or slow down„ÄÇAnd those actions are given us input„ÄÇ

That action isÔºå is that's what's produced by theÔºå what's called hereÔºå the brain„ÄÇ

The brain takes in the current state as inputÔºå the last reward and produces and and learns„ÄÇ

 uses that reward„ÄÇTo train the network„ÄÇThrough backward„ÄÇFunction thereÔºå it's back propagation„ÄÇ

And then„ÄÇAsk the brainÔºå given the current stateÔºå to give it the next action„ÄÇWith a forward pass„ÄÇ

 the forward functionÔºå you don't need to know the operation of this function in particular„ÄÇ

 This is not something you need to worry aboutÔºå but you canÔºå if you want„ÄÇ

 you can customize this learning step„ÄÇThere isÔºå by the wayÔºå what I'm describing now„ÄÇ

 there's just a few lines of code right there in the browser„ÄÇ

That you can change and it immediately with the press of a button changes the simulation or the design of the network„ÄÇ

 you don't need to have any special hardwareÔºå you don't need to do anything special„ÄÇ

 and the tutorial cleanly outlines exactly all of these steps„ÄÇ

But it's kind of amazing that you can design a deep neural network that's part of the reinforcement learning agent„ÄÇ

So it's a deep Q learning agent„ÄÇRight there in the browser„ÄÇSo you can choose the lane side variable„ÄÇ

 which controls how many„ÄÇLaanes to the side you see„ÄÇ So when that value is 0Ôºå you only look forward„ÄÇ

 When that value is oneÔºå you have one lane to the leftÔºå one value to the right„ÄÇ It's really the lane„ÄÇ

 the radius of your perception system Pates ahead is how far ahead you look„ÄÇ

 Pates behind is how far behind you look„ÄÇAnd soÔºå for exampleÔºå hereÔºå the lane side equals 2„ÄÇ

 That means it looks 2 to the leftÔºå2 to the right„ÄÇObviouslyÔºå if to to the right is off road„ÄÇ

It provides a value of 0 in those blocks„ÄÇIf we set the patches behind to be 10„ÄÇ

 it looks 10 patches back behind starting at the one patch back is starting from the front of the car„ÄÇ

The scoring„ÄÇFor the evaluationÔºå for the competition„ÄÇ

Is your average speed over a predefined period of time„ÄÇAnd so the method we do„ÄÇ

 we use to collect at speed is we we run the agent 10 runsÔºå about 30 simulated minutes of game each„ÄÇ

And take the median speed of the 10 runs„ÄÇThat's the score„ÄÇThis is done serverside„ÄÇAnd„ÄÇ

So so given that we've gotten someÔºå for thisÔºå for this code recently gotten some publicity online„ÄÇ

 unfortunately„ÄÇThis might be a dangerous thing to sayÔºå is no cheating possible„ÄÇ

But because it's done SursideÔºå and we did this is JavaScript and it runs in the browser„ÄÇ

 it's hopefully sandboxed„ÄÇSo we can't do anything tricky„ÄÇBut we dare you to try„ÄÇ

You can try it locally to get an estimate„ÄÇAnd there's a button that says evaluate„ÄÇ

 and it gives you a score right back of how well you're doing with the current network„ÄÇ

That button is„ÄÇStart evaluation run„ÄÇ You press the button„ÄÇ It does a progress bar„ÄÇ

 and it gives you the average speed„ÄÇYou can there's a code box„ÄÇ

Where you modify all the variables I mentioned and the tutorial describes this in detail„ÄÇ

And then once you're readyÔºå you modify a few thingsÔºå you can press apply codeÔºå it restarts„ÄÇIt„ÄÇ

Kills all the training that you've done up to this point or resets it„ÄÇAnd starts the training again„ÄÇ

SoÔºå save often„ÄÇAnd there's a save button„ÄÇSo training is done a separate thread„ÄÇIn web workers„ÄÇ

Which are exciting things that allow you to„ÄÇAllow jascript to run amazingly inÔºå inÔºå in know„ÄÇ

On multiple CPU cores in a parallel way„ÄÇSo the simulation that scores this or the sorry„ÄÇ

 the training is done a lot faster than real time„ÄÇ1000 frames a second„ÄÇ100 movement steps a second„ÄÇ

This is all in JavaScript„ÄÇAnd the next day gets shipped to the main simulation from time to time as the training goes on„ÄÇ

So all you have to do is press run training„ÄÇAnd it trainsÔºå and the car behaves better over time„ÄÇ

Maybe I'll actually show it in the browser„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_23.png)

Ëøô„ÄÇLet's see if it workÔºå wellÔºå is it going to mess upÔºüWell good„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_25.png)

Why is it being weird„ÄÇWhat can possibly go wrong„ÄÇSo here's the game„ÄÇWhen it starts„ÄÇ

This is running live in the browser„ÄÇArtificial intelligenceÔºå ladies and gentlemen„ÄÇIn the browser„ÄÇ

 a neural network„ÄÇ So currently it's not very good„ÄÇ It's driving at two miles an hour„ÄÇ

And watching everybody pass„ÄÇSo„ÄÇWhat's being shown live is the loss function„ÄÇWhich is pretty poor„ÄÇ

 So in order to trainÔºå like I saidÔºå thousand frames a second„ÄÇYou just press the run training button„ÄÇ

And pretty quicklyÔºå learns based on the network you specify in the code box„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_27.png)

How to and based on the input and all the things that I mentioned„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_29.png)

Training finished„ÄÇIt learns how to do a little better„ÄÇWe on purpose„ÄÇ

 put in a network that's not very good in there„ÄÇ So right nowÔºå we won'tÔºå on average„ÄÇ

 be doing that well„ÄÇBut it does better than standing there in place„ÄÇ

And then you could do the start evaluation run„ÄÇTo simulate the network much faster than real time„ÄÇ

 to see how well it does„ÄÇ This is a similar evaluation step that we take„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_31.png)

When determininging where you stand on the leaderboardÔºå the current average speed„ÄÇ

In that 10 run simulation is 56„ÄÇ56„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_33.png)

Ms per hour„ÄÇNow„ÄÇI may be logged in„ÄÇ Maybe not„ÄÇ if you're logged in„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_35.png)

You click submitmit your codeÔºå if you're not logged inÔºå it says you're not logged in„ÄÇ

 please log in to submit your code„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_37.png)

And then all you have to do is log in„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_39.png)

is the most flawless demo of my life„ÄÇAnd then you press submit model again in success„ÄÇOh man„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_41.png)

Thank you for your submission„ÄÇ And so now my submission is entered as Lex and the leaderboard and my 56„ÄÇ

56 or whatever it was„ÄÇüòäÔºåSo I dare all of you to try to beat that„ÄÇSoÔºå to„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_43.png)

AsÔºå as you play around with stuffÔºå if you want to save the code„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_45.png)

You could do so by pressing the save code button saves the various JavaScript configurations„ÄÇ

 and that saves the network layout to file„ÄÇAnd then you can load from file as well„ÄÇ danger„ÄÇ

 it overrides the code for you„ÄÇAnd you press the submit button to submit the model to competition„ÄÇ

Make sure that you train the network„ÄÇ We don't train it for you„ÄÇ You bring usÔºå you submit a model„ÄÇ

 and you have to press train„ÄÇAnd it gets evaluated in time„ÄÇ it enters a queue to get evaluated„ÄÇ

This is public facingÔºå so the queue can grow pretty big„ÄÇAnd it goes to that queueÔºå evaluates it„ÄÇ

 And then depending on where you standÔºå you get added to the leaderboardÔºå showing the top 10 entries„ÄÇ

You can resubmit often„ÄÇAnd only the highest score counts„ÄÇOkayÔºå we're using we're using code„ÄÇNow„ÄÇ

 implementation of neural networks„ÄÇDone in just JavaScript„ÄÇBy Andre Carpathy from Stanford„ÄÇ

 now Open AI„ÄÇComdomnet JS is a library„ÄÇAnd what's being visualized there„ÄÇ

 it's also being visualized in the game is the inputs to the network„ÄÇ In this caseÔºå it's 135 inputs„ÄÇ

You canÔºå you can also specify not just the„ÄÇNot just how far head behind you see to the left and to the right„ÄÇ

 you can specify how far back in time you look as well„ÄÇ

And so what's visualized there is the input to the networkÔºå135„ÄÇAnd neurons„ÄÇ and then the output„ÄÇ

 a regression similar to the kind of output we saw with numbers where there's 10 outputs saying if it's a 0„ÄÇ

1Ôºå0 9 hereÔºå the output is one of the five actions„ÄÇ LeftÔºå rightÔºå Stay in placeÔºå speed up or slow down„ÄÇ

The cognitivenet JS settings„ÄÇIs you can select the number of inputs„ÄÇ

If you want to mess with this stuffÔºå this is all the stuff you don't need to mess with because we're ready to give you the variables of laneside and patches ahead and so on„ÄÇ

 You could select the number of actions„ÄÇThe temporal window„ÄÇAnd the network size„ÄÇÂóØ„ÄÇSo the„ÄÇ

 the network definition here„ÄÇIs the„ÄÇThis is the inputÔºå the size of the input„ÄÇAgain„ÄÇ

 all this is in the tutorial„ÄÇ just to give you a little outline„ÄÇ

 there is a the first fully connected layer has 10 neurons„ÄÇWith radio activation functions„ÄÇ

Same kind of smooth„ÄÇFunction that we talked about before„ÄÇAnd the regression layer for the output„ÄÇ

And there's a bunch of other messy options that you can play with if you dare„ÄÇ

But those aren't the ones I mentioned before it's really the important ones„ÄÇ

 it's selecting the number of layersÔºå the size of those layers„ÄÇ

 you get to build your own very neural network that drives„ÄÇ

And the actual learning is done with a backward propagation„ÄÇ And then the„ÄÇ

That returns the action by doing a forward path to the network„ÄÇ

In case you're interested in this kind of stuffÔºå there is an amazingly cool„ÄÇüòäÔºåCode editor„ÄÇ

That's a monoco editor„ÄÇThat itÔºå it just works„ÄÇ It does some auto completion„ÄÇ

 So you get to play with it„ÄÇ makes everything very convenient in terms of code editing„ÄÇ

A lot of this visualization of the game„ÄÇAnd the and the simulation and the simulation we talk about tomorrow is done in in the browser using HTML L 5 canvas„ÄÇ

 So here is a simple specification of a blue box with canvas„ÄÇ And it is very„ÄÇ

Efficient and easy to work with„ÄÇAnd the thing that„ÄÇA lot of us are excited about a very subtle one„ÄÇ

 but that you can„ÄÇNot just run„ÄÇSo with the V 8 engineÔºå jascript has become super fast„ÄÇ You can„ÄÇ

 you could train neural networks in the browser„ÄÇ That's already amazing„ÄÇ And then with Web workers„ÄÇ

 as long as you have Chrome„ÄÇüòäÔºåA modern browser„ÄÇIs you can run multiple processes in separate threads„ÄÇ

So we could do a lot of stuff„ÄÇ You can do visualization separately„ÄÇ

 and you can train in separate threads„ÄÇIt's very cool„ÄÇOkay„ÄÇ

 so the tutorial is CARS that MITDDU and deep traffic„ÄÇ

 we won't put these links on the website for a little bit„ÄÇ

We got put on the front page of hacker news„ÄÇWhich„ÄÇWe don't want those to leak out„ÄÇ

 especially with the claims that you can't cheat„ÄÇAnd while it's pretty efficient in terms of running so everything is running on your machine„ÄÇ

 client side„ÄÇIt's stillÔºå you have to pull some images here and pull some of the code„ÄÇ

 So the tutorials on cars„ÄÇ MT IDU slash deep traffic and the simulation is deep traffic J S„ÄÇ

 So cars M T IU slash deep traffic J S„ÄÇ I encourage you to go thereÔºå play with the network„ÄÇ

 submit your code„ÄÇAnd win the very special prizeÔºå it it is a pretty cool one„ÄÇ

 but we're still working on it„ÄÇüòä„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_47.png)

there is a prizeÔºå I swear„ÄÇAllrightÔºå so's„ÄÇLet's„ÄÇTake a pause and think about what we talked about today„ÄÇ

So the very best of deep reinforcement learning„ÄÇIs„ÄÇThe most exciting accomplishment„ÄÇI think„ÄÇ

Is when the gameÔºå when I first started„ÄÇAndÔºå as a freshmanÔºå took intro to artificial intelligence„ÄÇ

It was said that it's a game that's impossible for machines to beat because of the combinatorial complexity„ÄÇ

 it's just a sheer number of optionsÔºå it's so much more complex than chess„ÄÇAnd so„ÄÇ

The most amazing accomplishment of deep reinforcement learning to me is the design of Algo„ÄÇüòä„ÄÇ

When for the first timeÔºå the world champion and go was beaten„ÄÇBy deep mindsÔºå alphaphago„ÄÇ

And the way they did itÔºå and this isÔºå I thinkÔºå very relevant to driving„ÄÇIs you start„ÄÇ

By creating first in a supervised wayÔºå training a policy network„ÄÇSo you take expert games„ÄÇ

To to construct a network„ÄÇFirstÔºå so you don't play against yourself„ÄÇ

The agent doesn't play against itself„ÄÇBut they learn from expert games„ÄÇ

So theres some human ground truth„ÄÇ this human ground truth represents reality„ÄÇ So for driving„ÄÇ

 this is important„ÄÇ we have a well we're starting to get a lot of data where video of drivers is being recorded so we can learn on that data before we then run the agents through simulation where it learns much larger magnitudes of data sets through simulation„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_49.png)

And they did just that„ÄÇNowÔºå as a reminder„ÄÇThat when you let an agent„ÄÇDrive itself„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_51.png)

I is probably one of my favorite videos of all time„ÄÇ But I just recently saw this„ÄÇ

 I could just watch this for hoursÔºå but it's a reminder that„ÄÇüòä„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_53.png)

![](img/48a6f6ece54e22cf179eb2b7e849c01a_54.png)

You can't„ÄÇYou can't trust„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_56.png)

Your first estimates of a reward function„ÄÇ

![](img/48a6f6ece54e22cf179eb2b7e849c01a_58.png)

To be those that are safe and productive for our society„ÄÇ

 when you're talking about an intelligent system that gets to operate in the real world„ÄÇ

 this is just as clear of a reminder of that as there is„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_60.png)

![](img/48a6f6ece54e22cf179eb2b7e849c01a_61.png)

![](img/48a6f6ece54e22cf179eb2b7e849c01a_62.png)

So„ÄÇAgainÔºå all the references are available online for these slidesÔºå we'll put up the slides„ÄÇ



![](img/48a6f6ece54e22cf179eb2b7e849c01a_64.png)

I imagine you might haveÔºå if you want to come down and talk to us for questions for the either Docker or JavaScript question„ÄÇ

So the question wasÔºå what is the visualization you're seeing in deep traffic„ÄÇ

 you're seeing a car move aboutÔºå how is it movingÔºå it's moving based on the latest snapshot of the network you trained„ÄÇ

So it's just visualizing for you just for funÔºå the network you trained most recently„ÄÇOkayÔºå so if you„ÄÇ

 if people have questionsÔºå stick around afterwards„ÄÇJust details on Docker and„ÄÇAnd yeah„ÄÇ

Do you want to do it offlineÔºå we're going to check up„ÄÇ

