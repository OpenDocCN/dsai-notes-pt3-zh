# „ÄêËã±ËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëMIT 6.S094 ÔΩú Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™Âä®È©æÈ©∂(2018¬∑ÂÆåÊï¥Áâà) - P4ÔºöL4- ËÆ°ÁÆóÊú∫ËßÜËßâ - ShowMeAI - BV1Y34y1i7vC

Today we'll talk about how to make machines see computer vision„ÄÇAnd we will presentÔºå thank you„ÄÇ

 whoever said yes„ÄÇAnd today we will present a competition that unlike deep traffic„ÄÇ

Which is designed to explore ideas„ÄÇTeach you about concepts of deep reinforcement learning„ÄÇSgfuse„ÄÇ

 the deep dynamic driving scene segmentation competition that I'll present today is at the very cutting edge„ÄÇ

Whoever does well in this competition is likely to produce„ÄÇ

A publication or ideas that would lead the world in the area of perception„ÄÇ

Perhaps together with the people running this classÔºå perhaps on your own„ÄÇ

 and I encourage you to do so„ÄÇ

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_1.png)

Even more casts todayÔºå computer vision„ÄÇTodayÔºå as it stands„ÄÇIs deep learning„ÄÇ

Majority of the successes in how we interpret form representations understand images and videos utilize to a significant degree neural networks„ÄÇ

 the very ideas we've been talking about that applies for supervised„ÄÇ

 unsupervised and reinforcement learning„ÄÇAnd for the supervised case„ÄÇ

 this is just the focus of today„ÄÇThe process is the same„ÄÇThe data is essential„ÄÇ

 there's annotated data where the human provides the labels that serves as the ground truth and the training process„ÄÇ

 then the neural network„ÄÇGs„ÄÇThrough that data„ÄÇLearning to map from the raw sensory input„ÄÇ

To the ground truth labels and then generalize over the testing data set„ÄÇ

And the kind of raw senses we're dealing with are numbers„ÄÇ

I'll say this again and again that for human vision„ÄÇFor us here„ÄÇ

 we take for granted this particular aspect of our ability is to take in raw sensory information through our eyes and interpret it„ÄÇ

But it's just numbers„ÄÇ that's something whether you're an expert computer vision person or new to the field„ÄÇ

 you have to always go back to meditate on„ÄÇis what kind of things the machine is given„ÄÇ

 what is the data that its tasked to work with in order to perform the task you're asking it to do„ÄÇ

 perhaps the data it's given is highly insufficient to do what you wanted to do that's the question that'll come up again and again are images enough to understand the world around you„ÄÇ

And given these numbers„ÄÇThey set of numbers sometimes with one channelÔºå sometimes with three RGB„ÄÇ

 where every single pixel have three different colors„ÄÇThe task is to classify„ÄÇOr regress„ÄÇ

 producing continuous variable„ÄÇOr one of a set of glass labels„ÄÇAs before„ÄÇ

We must be careful about our intuition of what is hard and what is easing computer vision„ÄÇ

Let's take a step back to the inspiration„ÄÇFor neural networksÔºå our own biological neural networks„ÄÇ

Because the human vision system and the computer vision system is a little bit more similar in these regards„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_3.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_4.png)

The structure of the human visual cortex is in layers„ÄÇ

And his information passes from the eyes to the„ÄÇTo the parts of the brain that makes sense of the raw sensor information„ÄÇ

 higher and higher order representations have formed„ÄÇThis is the inspiration„ÄÇ

 the idea behind using deep neural networks for images„ÄÇ

Higher and higher order representations have formed through the layers„ÄÇThey're early layers„ÄÇ

Taking in the very raw sensory information and extracting edges„ÄÇConnecting those edges„ÄÇ

 forming those edges to form more complex features„ÄÇ

 and finally into the higher order semantic meaning that we hope to get from these images„ÄÇ

In computer visionÔºå deep learning is hard„ÄÇI'll say this again„ÄÇ

 the illumination variability is the biggest challenge„ÄÇ

Or at least one of the biggest challenges in driving„ÄÇFor visible light cameras„ÄÇPoose variability„ÄÇ

The objects„ÄÇAs I'll also discuss about some of the advances from Jeff Hton and the capsule networks„ÄÇ

 the idea„ÄÇWith the neural networksÔºå as they are currently used for computer vision„ÄÇ

 are not good with representing variable poses„ÄÇThese objects in images and this 2T plane of color and texture„ÄÇ

Look very different numerically when the object is rotated„ÄÇ

And the object is mangled and shaped in different waysÔºå the deformableable truncated cat„ÄÇ

Interclass variability for the classification task„ÄÇ

 which would be an example today throughout to introduce some of the networks over the past decade that have received success and some of the intuition and insight that made those networks work„ÄÇ

 classification„ÄÇThere is a lot of variability inside the classes and very little variability between the classes„ÄÇ

All of these are cats at top„ÄÇ All of those are dogs at bottom„ÄÇ They look very different„ÄÇ

 And the otherÔºå I would say the second biggest problem in driving perception„ÄÇ

 visible light camera perception is occlusion when part of the object is occluded due to the three dimensional„ÄÇ

Nature of our worldÔºå some objects in front of others„ÄÇAnd they occlude the background object„ÄÇ

And yet we're still tasked with identifying the object when only part of it is visible„ÄÇ

 and sometimes that part told you there's cats is very hardly visible here„ÄÇ

 we're tasked with classifying a cat when just an ear is visibleÔºå just the leg„ÄÇ

And on a philosophical levelÔºå as we'll talk about the motivation for our competition here„ÄÇ

 here's a cat dressed as a monkey eating a banana on a philosophical level„ÄÇ

Most of us understand what's going on in the scene„ÄÇIn fact„ÄÇ

 a neural network today successfully classified this„ÄÇImage this video as a cat„ÄÇBut the context„ÄÇ

 the humor of the situation„ÄÇ and in factÔºå you could argue it's a monkey„ÄÇIs missing„ÄÇ

And what else is missing is the dynamic informationÔºå the temporal dynamics of the scene„ÄÇ

That's what's missing in a lot of the perception work that has been done to date in the autonomous vehicle space in terms of visible light cameras„ÄÇ

 And we're looking to expand on that„ÄÇ That's what Syg views is all about„ÄÇ

Image classification pipelineÔºå there's a bin with different categories inside each classÔºå catÔºå dog„ÄÇ

 mug hat„ÄÇThose binsÔºå there's a lot of examples of each and you're tasked with when a new example comes along you've never seen before to put that image in a bin„ÄÇ

 it's the same as the machine learning task before„ÄÇ

And everything relies on the data that's been ground truthÔºå that have been labeled by human beings„ÄÇ

EmNist is a toy dataset set of Hanin digitsÔºå often used as examples„ÄÇAnd cocoÔºå Sa far„ÄÇ

 Inet places and a lot of other incredible data setsÔºå rich data sets of 100 thousands„ÄÇ

 millions of images out there represent scenesÔºå people's faces and different objects„ÄÇ

 Those are all ground truth data for testing algorithms and for„ÄÇüòä„ÄÇ

Competing architectures to be evaluated against each other„ÄÇCF 10Ôºå one of the simplest„ÄÇ

Almost toy dataset sets of tiny icons with 10 categoriesÔºå of airplane automobileÔºå birdcat„ÄÇ

 deer dog frogÔºå horse ship and truck„ÄÇIt's commonly used to explore some of the basic convolution neural networks we'll discuss„ÄÇ

 So let's come up with a very trivial classifier to explain the concept of how we could go about it„ÄÇ

In factÔºå this is„ÄÇMaybe if you start to think about how to classify an image„ÄÇ

 if you don't know any of these techniquesÔºå this is perhaps the approach you would take is see would subtract images„ÄÇ

 So in order to know that an image of a cat is different than the image of a dog„ÄÇ

 you have to compare them when given those two imagesÔºås what's the way you compare them„ÄÇ

One way you could do it is you just subtract it„ÄÇAnd then sum all the pixel wise differences in the image„ÄÇ

 just subtract the intensity of the image pixel by pixel sum it up„ÄÇIf that„ÄÇ

 if that difference is really highÔºå that means the images are very different„ÄÇUsing that metric„ÄÇ

 we can look at C far 10 and use it as a classifier sayingÔºå based on this difference function„ÄÇ

 I'm going to find one of the 10 bins for a new image that„ÄÇThat is has the lowest difference„ÄÇ

Find an image in this data that is most like the image I have and put it in the same bin as that image is in„ÄÇ

So there's 10 classesÔºå if we just flip a coinÔºå the accuracy of our classifier would be 10%„ÄÇ

Using our image difference classifierÔºå we can actually do pretty goodÔºå much better than random„ÄÇ

 much better than 10%„ÄÇWe can do 35Ôºå38% accuracy„ÄÇThat's a classifierÔºå we have our first„ÄÇClassifier„ÄÇ

Kier his neighbors„ÄÇLet's take our classifier to our whole new level instead of comparing it to just„ÄÇ

Trying to find one image that's the closest in our data set„ÄÇ

 we try to find K closest and say what class do the majority of them belong to„ÄÇ

 and we take that K and increase it from one to two to three to four to five„ÄÇ

And see how that changes the problem„ÄÇWith seven nearest neighbors„ÄÇ

 which is the optimal under this approach for CFR 10„ÄÇWe achieved 30% accuracy„ÄÇ

Human level is 95% accuracy„ÄÇAnd with convolution neural networksÔºå we get very close to 100%„ÄÇThat's„ÄÇ

Where neural networks shineÔºå this very task„ÄÇOf binning images„ÄÇ

 it all starts at this basic computational unit„ÄÇSignal inÔºå each of the signals are weighed„ÄÇSummed„ÄÇ

 bias added„ÄÇAnd put an input into a nonlinear activation function that produces an output„ÄÇ

The nonlinear activation function is key„ÄÇAll of these put together„ÄÇAnd more and more hidden layers„ÄÇ

Form a deep neural networkÔºå and that deep neural network is trained as we've discussed by taking a forward pass on examples of ground truth labels„ÄÇ

 seeing how close those labels are to the real ground truth and then punishing the weights that resulted in the incorrect decisions and rewarding the weights that resulted in correct decisions„ÄÇ

For the case of 10 examplesÔºå the output of the network„ÄÇIt's 10 different values„ÄÇÂóØ„ÄÇ

The input being handwritten digits from zero to nineÔºå theres 10 of those„ÄÇ

And we wanted our network to classify what is in this image of a handwritten digit„ÄÇIts  zeroÔºå1Ôºå2„ÄÇ

3 through9„ÄÇThe way it's often done is there's 10 outputs of the network„ÄÇ

And each of the neurons on the output is responsible for getting really excited when its number is called„ÄÇ

And everybody else is supposed to be not excitedÔºå therefore„ÄÇ

The number of classes is the number of outputsÔºå that's how it's commonly done„ÄÇ

And you assign a class to the input image based on the highest the neuron which produces the highest output„ÄÇ

But that's for a fully connected network that we've discussed on Monday„ÄÇ

There is in deep learning a lot of tricks that make things work„ÄÇ

 that make training much more efficient on large„ÄÇClass problems where there's a lot of classes on large data sets„ÄÇ

When the representation that the neural network is tasked with learning is extremely complex„ÄÇ

 and that's where convolutional neural networks step inÔºå the trick they use is spatial invariance„ÄÇ

They use the idea that„ÄÇA cat in the top left corner of an image is the same as a cat in the bottom right corner of an image„ÄÇ

So we can learn the same features across the image„ÄÇThat's where the convolution operation steps in„ÄÇ

Instead of the fully connected networksÔºå here there's a third dimension„ÄÇOf depth„ÄÇ

So the blocks in this neural network„ÄÇAs input take 3D volumes and this output produce 3D volumes„ÄÇ

They take a slice of the image„ÄÇA window and slide it across„ÄÇApplying the same exact weights„ÄÇ

 and we'll go through an exampleÔºå the same exact weights as in the fully connected network on the edges that are used to map the input to the output„ÄÇ

 here are used to map the slice of an imageÔºå this window of an image to the output„ÄÇ

And you can make several many of such„ÄÇConvolutional filtersÔºå many layersÔºå many different options of„ÄÇ

What kind of features you look for in an imageÔºå what kind of window you slide across in order to extract all kinds of things„ÄÇ

All kinds of edgesÔºå all kinds of higher order patterns in the images„ÄÇ

The very important thing is the parameters on each of these filtersÔºå these subset of the image„ÄÇ

 these windowsÔºå are shared„ÄÇIf the feature that defines a cat is useful in the top left corner„ÄÇ

 it's useful in the top right cornerÔºå it's useful in every aspect of the image„ÄÇ

 this is the trick that makes convolutional neural networks save a lot of„ÄÇ

A lot of parameters reduce parameters significantly„ÄÇIs the reuse„ÄÇ

 the spatial sharing of features across the space of the image„ÄÇ

The depth of these 3D volumes is the number of filters„ÄÇThe stride is the skip of the filter„ÄÇ

 the step sizeÔºå how many pixels you skip when you apply the filter to the input„ÄÇAnd the padding„ÄÇ

Is the paddingÔºå the zero padding on the outside of the input to a convolutional layer„ÄÇ

Let's go through an example„ÄÇSo on the left here„ÄÇAnd the slides are now available online„ÄÇ

 you can follow them alongÔºå and I'll step through this example„ÄÇ

On the left here is an input volume of three channels„ÄÇThe left column is the input„ÄÇ

 the three squares there are the three channels„ÄÇAnd there's numbers inside those channels„ÄÇ

And then we have a filter in red„ÄÇTwo of them„ÄÇTwo channels of filters„ÄÇWith a bias„ÄÇ And we„ÄÇ

 those filters are 3 by 3„ÄÇ Each one of them is„ÄÇSize three by three„ÄÇ

 And what we do is we take those three by three filters„ÄÇThat are to be learned„ÄÇ

 these are our variablesÔºå our weights that we have to learn„ÄÇ

And then we slide it across an image to produce the output on the rightÔºå the green„ÄÇ

So by applying the filters in the redÔºå there's two of them and within each one„ÄÇ

 there's one per every input channel„ÄÇWe go from the left to the right„ÄÇ

From the input volume on the left to the output volume green on the right„ÄÇAnd you can look„ÄÇ

 you can pull up the slides yourselfÔºå not if you can't see the numbers on the screen„ÄÇ

 but the operations„ÄÇAre performed on the input to produce the single value that's highlighted there in the green and the output„ÄÇ

And we slide this convolutionÔºå no filter along the image„ÄÇWith the stride in this case„ÄÇOf two„ÄÇ

 skippingÔºå skipping along„ÄÇThey sum to the rightÔºå the two channel output in green„ÄÇThat's it„ÄÇ

 that's the convolutional operationÔºå that's what's called the convolutional layer in neural networks„ÄÇ

And the parameters hereÔºå besides the biasÔºå are the red values in the middle„ÄÇ

 That's what we're trying to learn„ÄÇAnd there's a lot of interesting tricks we'll discuss today on top of those„ÄÇ

 but this is at the coreÔºå this is the spatially invariant sharing of parameters that make convolution neural networks able to efficiently learn and find patterns and images„ÄÇ

To build your intuition a little bit more„ÄÇAbout convolution„ÄÇ

 here's an input image on the left and on the right„ÄÇ

 the identity filter produces the output you see on the right„ÄÇ

 and then theres different ways you can different kinds of edges you can extract„ÄÇ

We the resulting activation maps seen on the rightÔºüSo when applying the filters„ÄÇ

Those edge detection filters„ÄÇTo the image on the left„ÄÇ

 you produce in white are the parts that activate the convolution„ÄÇThe results of these filters„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_6.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_7.png)

And so you can do any kind of filterÔºå that's what we're trying to learnÔºå any kind of edge„ÄÇ

Any kind of pattern you can move along in this window in this way that's shown here„ÄÇ

 you slide along the image and you produce the output you see on the right„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_9.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_10.png)

And depending on how many filters you have in every level„ÄÇ

 you have many of such slices that you see on the rightÔºå the input on the left„ÄÇ

 the output on the rightÔºå if you have„ÄÇ

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_12.png)

Dozens of filtersÔºå you would have dozens of images on the right„ÄÇ

 each with different results that show where each of the individual filter patterns were found„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_14.png)

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_15.png)

And we learn what patterns are useful to look for in order to perform the classification task„ÄÇ

That's the task for the neural network to learn these filters„ÄÇ

And the filters have higher and higher order„ÄÇOf representation„ÄÇ

Going from the very basic edges to the high semantic„ÄÇMeaning that spans entire images„ÄÇ

And the ability to span images can be done in several ways„ÄÇ

 but traditionally has been successfully done through max poolingÔºå through pooling„ÄÇ

Of taking the output of a convolutional operation„ÄÇAnd reducing the resolution of that„ÄÇBy„ÄÇ

By condensing that information byÔºå for exampleÔºå taking the maximum valuesÔºå the maximum activations„ÄÇ

ThereforefoÔºå reducing the spatial resolutionÔºå which has detrimental effects„ÄÇ

 as we'll talk about in the scene segmentationÔºå but it's beneficial for finding higher order representations in the images that bring images together„ÄÇ

 that bring features together to form an entity that we're trying to identify and classify„ÄÇOkay„ÄÇ

 so that forms„ÄÇA convolution neural networkÔºå such convolutional layers stacked on top of each other is the only addition to a neural network that makes for a convolutional neural network„ÄÇ

And then at the endÔºå the fully connected layers or any kind of other architectures allow us to apply particular domains„ÄÇ

Let's take ImageNe as a case study„ÄÇAn imagenet„ÄÇThe data setÔºå an imageNeÔºå the challenge„ÄÇ

 the task is classification„ÄÇAs I mentioned in the first lectureÔºå Inet is a data set„ÄÇ

One of the largest in the world of images with 14 million imagesÔºå 21Ôºå000 categories„ÄÇ

And a lot of depth to many of the categoriesÔºå as I mentionedÔºå 1Ôºå200 Grannny Smith apples„ÄÇ

TheseThese allow the neural networks to learn the rich representations in both pose„ÄÇ

 lighting variability and intraclass class variation for the particular things„ÄÇParticular classes„ÄÇ

Like Grny Smith apples„ÄÇSo let's look through the various networksÔºå let's discuss them„ÄÇ

 let's see the insights it started with AlexnetÔºå the first really big successful GPU trained neural network on IageNet that's achieved a significant boost over the previous year„ÄÇ

And moved on to VGGNe„ÄÇGoogleNe„ÄÇAgu Lynette„ÄÇResNeÔºå CU imageÔºå and SNe„ÄÇIn 2017„ÄÇAgain„ÄÇ

 the numbers will show for the accuracy are based on the top five error rate„ÄÇ

 you get five guesses and it's a one or zero„ÄÇ if you if one of the five is correct„ÄÇ

 you get a one for that particular guessÔºå otherwise it's a zero„ÄÇAnd human error is 5„ÄÇ

1 when a human tries to achieve the same tries to perform the same task as the machinist task we doing„ÄÇ

 the error is 5„ÄÇ1Ôºå the human annotation is performed on the images based on binary classification„ÄÇ

 Granny SmithÔºå Apple or notÔºå cat or notÔºå the actual task that the machine has to perform and that the human competing has to perform„ÄÇ

 is given an image is provided one of the many classes„ÄÇUnder thatÔºå human errors is 5„ÄÇ1%„ÄÇ

 which was surpassed in 2015 by Resnet„ÄÇTo achieve 4% error„ÄÇSo let's start with„ÄÇAlexNet„ÄÇ

 I'll zoom in on the later networks„ÄÇ They have some interesting insights„ÄÇ

 but AlexNet and VGGNet both followed a very similar architecture„ÄÇVery uniform throughout its depth„ÄÇ

Viji Gnet in 2014„ÄÇIs convolutionÔºå convolution poolingÔºå convolutionÔºå poolingÔºå convolution pooling„ÄÇ

 and fully connected layers at the end„ÄÇThere's a certain kind of beautiful simplicity„ÄÇ

 uniformity to these architectures because you can just make it deeper and deeper and makes it very amenable to implementation in a layer stack kind of way in any of the deep learning frameworks„ÄÇ

It's clean and beautiful to understand in the case of EGGnet 16 or 19 layers with 138 million parameters„ÄÇ

 not many optimizations on these parametersÔºå therefore„ÄÇ

 the number of parameters is much higher than the networks that followed it„ÄÇ

 despite the layers not being that large„ÄÇGoogleNet introduced the inception module„ÄÇ

 starting to do some interesting things with the small modules within these networks„ÄÇ

 which allow for the training to be more efficient and effective„ÄÇ

The idea behind the inception module shown here„ÄÇWith the previous layer on bottom„ÄÇ

And the convolutional layer here with the inception module„ÄÇOn top„ÄÇ

 produced on top is it used the idea that different size convolutions provide different value for the network„ÄÇ

Smaller convolutions are able to capture or propagate forward features that are very local„ÄÇ

 a high resolution in texture„ÄÇLarger convolutions are better able to represent and capture and catch highly abstracted features„ÄÇ

 higher order features„ÄÇSo the idea behind the inception module is to sayÔºå well„ÄÇ

 as opposed to choosing in a hyperparameter tuning process or architecture design process„ÄÇ

 choosing which convolution size we want to go with„ÄÇ

 why not do all of them well several together in the case of the GoogleNe model„ÄÇ

 there is the one by oneÔºå three by three and five by five convolutions„ÄÇ

With the old trusty friend of Max Poing still left in there as well„ÄÇ

Which has lost favor more and more over time for the image classification task„ÄÇ

And the result is there's fewer parameters that are required if you pick„ÄÇ

The placing of these inception modules correctlyÔºå the number of parameters required to achieve a higher performance is much lower„ÄÇ

Resnet„ÄÇOne of the most popular still to date„ÄÇArchitectures that we'll discuss in scene segmentation as well„ÄÇ

Came up and used the idea of a residual block„ÄÇThe initial„ÄÇInspiring observation„ÄÇ

 which doesn't necessarily hold trueÔºå as it turns out„ÄÇ But that network depth„ÄÇ

Increases representation power„ÄÇ So these residual blocks allow you to have much deeper networks„ÄÇ

 And I'll explain why in a second hereÔºå but„ÄÇThe thought was they work so well because the networks are much deeper„ÄÇ

The key thing that makes these blocks so effective is the same idea that's reminiscent ofcurren neural networks„ÄÇ

That I hope we get a chance to talk about„ÄÇThe training of them is much easier„ÄÇ

They take a simple blockÔºå repeat it over and over„ÄÇAnd they pass the input along without transformation„ÄÇ

Along with the ability to transform itÔºå to learn the filtersÔºå learn the weights„ÄÇSo you're allowed to„ÄÇ

You allow every layer to not only take on the processing of previous layers„ÄÇ

 but to take in the raw transform data and learn something new„ÄÇ

The ability to learn something new allows you to have much deeper networks„ÄÇ

And the simplicity of this block allows for more effective training„ÄÇThe state of the art in 2017„ÄÇ

 the winner is Squeeing Exitation Networks„ÄÇThat unlike the previous year was C image which Chimpley took ensemble methods and combined a lot of successful approaches to take a marginal improvement„ÄÇ

 SC net„ÄÇGot a significant improvement„ÄÇAt least in percentagesÔºå I think is a 25% reduction in error„ÄÇ

From 4% to 3%„ÄÇSomething like that by using a very simple idea that I think is important to mention„ÄÇ

 a simple insight„ÄÇIt added a parameter to each channel in the convolutional layerir„ÄÇ

In the convolutional block„ÄÇSo the network can now adjust the waiting on each channel„ÄÇ

Based for each feature mapÔºå based on the contentÔºå based on the input to the network„ÄÇ

This is kind of a takeaway to think aboutÔºå about any of the networks who talk about any of the architectures„ÄÇ

Is a lot of times recurrent neural networks and convolutional neural networks have tricks that significantly reduce the number of parameters„ÄÇ

The bulkÔºå the sort of low-han fruitÔºå they use spatial invariance„ÄÇ

 the temporal invariance to reduce the number of parameters to represent the input data„ÄÇ

 but they also leave certain things not paraized they don't allow the network to learn it allowing in this case the network to learn the waitinging on each of the individual channels so each of the individual filters is something that you learn as along with the filters makes a huge boost„ÄÇ

 the cool thing about this is it's applicable to any architectureÔºå this kind of block„ÄÇ

 this kind of what the squeeze and excitation block„ÄÇIs applicable to any architecture„ÄÇ

And because obviously it just simply parameterizes the ability to choose which filter you go with based on the content„ÄÇ

 it's a subtle but crucial thingÔºå I think it's pretty cool and for future research it inspires to think about what else can be parameterized in neural networks„ÄÇ

 what else can be controlled as part of the learning process„ÄÇ

 including hiring higher order hyperparameters„ÄÇWhich aspects of the training and the architecture of the network can be part of the learning„ÄÇ

 This is what this network conspires„ÄÇAnother network has been in development since the '90s ideas„ÄÇ

But Jeff HntonÔºå but really received been published on and received significant attention in 2017„ÄÇ

 that I won't go into detail hereÔºå we are going to release an online only video about capsule networks„ÄÇ

It's a little bit too technicalÔºå but they inspire a very important point„ÄÇ

That we should always think about with deep learning„ÄÇWhenever it's successful„ÄÇ

 is to think about whatÔºå as I mentioned with the cat eating a banana on a philosophical and the mathematical level„ÄÇ

 we have to consider what assumptions these networks make and what through those assumptions they throw away„ÄÇ

So neural networks with convolution neural networks due to their spatial invariance„ÄÇ

Throw away information about the relationship„ÄÇBetween„ÄÇ

TheThe hierarchies between the simple and the complex objects„ÄÇ

 So the face on the left and the face on the right looks the same to a convolution neural network„ÄÇ

 the presence of eyes and nose„ÄÇAnd mouth„ÄÇIs the essential aspect of what makes the classification task work for convolution network where it will fire and say this is definitely a phase„ÄÇ

But the spatial relationship is lostÔºå is ignoredÔºå which means„ÄÇ

There's a lot of implications to this but„ÄÇFor things like pose variation„ÄÇThat information is lost„ÄÇ

 we're throwing that away completely and hoping that the pooling operation that's performed in these networks is able to sort of mesh everything together to come up with the features that are firing of the different parts of the face to then come up with the total classification that it's a face without representing really the relationship between these features at the low level and the high level at the low level the hierarchy at the simple and the complex level„ÄÇ

This is a super exciting field now that hopefully will spark developments of how we design neural networks that are able to learn the rotational„ÄÇ

 the orientation in variances as well„ÄÇOkayÔºå so as I mentioned„ÄÇ

You take these convolution neural networksÔºå chop off the final layer„ÄÇ

In order to apply to a particular domain„ÄÇAnd that is what we'll do with fully convolutionary neural networks„ÄÇ

 the ones that we task to segment the image at a pixel level„ÄÇAs a reminder„ÄÇ

 these networks through the convolutional process„ÄÇReally producing a heat map„ÄÇ

 different parts of the network are getting excited based on the different aspects of the image„ÄÇ

 and so it can be used to do the localization of detecting„ÄÇ

 not just classifying the image but localizing the object„ÄÇAnd they could do so at a pixel level„ÄÇ

So the convolutional layers are during the encoding processÔºå they're taking the rich„ÄÇ

 raw sensory information in the image and encoding them into an interpretable set of features representation that can then be used for classification„ÄÇ

 but we can also then use a decoderÔºå upsample that information and produce a map like this„ÄÇ

Fully convolution neural networkÔºå segmentationÔºå semantic scene segmentationÔºå image segmentation„ÄÇ

 the goal is toÔºå as opposed to classify the entire imageÔºå you classify every single pixel„ÄÇ

 it's pixel level segmentationÔºå you color every single pixel with what that pixel„ÄÇ

 what object that pixel belongs to in this 2D space of the image„ÄÇ

The 2D projection in the image of a three dimensional world„ÄÇ

So the thing is there's been a lot of advancement in the last three„ÄÇYears„ÄÇ

But it's still an incredibly difficult problem„ÄÇIf if you thinkÔºå if you think about„ÄÇ

The amount of data that's used for training and the task of pixel level of megapixels here of millions of pixels that are tasked with having assigned a single label„ÄÇ

 it's an extremely difficult problem„ÄÇWhy is this interesting„ÄÇ

 important problem to try to solve as opposed to bounny boxes around catsÔºüWell„ÄÇ

 it's whenever precise boundaries of objects are important„ÄÇ

 certainly medical applications when looking at imaging and detecting in particularÔºå for example„ÄÇ

 detecting tumors in medical imaging of different organs„ÄÇAnd in driving in robotics„ÄÇ

When objects are involvedÔºå it's a dense scene involved with vehiclesÔºå pedestrian cyclists„ÄÇ

 we need to be able to not just have a loose estimate of where objects are„ÄÇ

 we need to be able to have the exact boundariesÔºå and then potentially through data fusion fusing sensors together„ÄÇ

 fusing this rich textual information about pedestrian cyclists and vehicles to light our data that's providing us the three dimensionmenal map of the world„ÄÇ

 or have both these semantic meaning of the different objects and their exact three- dimensional location„ÄÇ

A lot of this work successfully„ÄÇA lot of the work in the semantic segmentation started with fully convolutionary networks for semantic segmentation paper„ÄÇ

 FCNÔºå that's where the name of FCN came from in November 2014„ÄÇ Now go through a few papers here„ÄÇ

To give you some intuition where the field is gone„ÄÇ

And how that takes us to se fuse the segmentation competition„ÄÇ

So FCN repurposed the imagenet pretrained netsÔºå the nets that were trained to classify wasn't an image„ÄÇ

The entire image„ÄÇAnd chopped off the fully connected layers and then added decoder parts that upsampled the image„ÄÇ

To produce a heat map here shown with a tabby catÔºå a heat map of where the cat is in the image„ÄÇ

It's a much lowerÔºå much coarser resolution than the input image„ÄÇOne8 at best„ÄÇ

Skip connections to improve coarseness of upslingÔºå there's a few tricks„ÄÇ

If you do the most naive approachÔºå the upsampling is going to be extremely coarse because that's the whole point of the neural network„ÄÇ

 the encoding part is you throw away all the useless data to the most essential aspects that represent that image„ÄÇ

 so you're throwing away a lot of information that's necessary to then form a high resolution image„ÄÇ

So there's a few tricks where you skip a few of the final„ÄÇ

Pooling operations to go in similar way into a residual block to go to the output„ÄÇ

 produce higher and higher resolution heat map at the end„ÄÇSignnet in 2015„ÄÇ

Applied this to the driving context and really taking it to Kitty data set and have shown a lot of interesting results and really explored the encoder decoder formulation of the problem„ÄÇ

Really solidifying this the place of the encoder decoder framework for the segmentation task„ÄÇ

Dilated convolutionÔºå I'm taking you through a few components which are critical here to the state of the art„ÄÇ

Dated convolutions„ÄÇSo the convolution operation„ÄÇAs the pooling operation„ÄÇ

Reduces resolution significantly„ÄÇAnd dilated convolution has a certain kind of grdtting as visualized there that maintains the local„ÄÇ

 high resolution textures„ÄÇWhile still capturing the spatial window necessary„ÄÇ

It's called dilated convolutional layer„ÄÇAnd that's in a 2015 paper proved to be much better at upsampling a high resolution image„ÄÇ

Deep lab with a bee„ÄÇV1Ôºå V2Ôºå now V3„ÄÇAdded conditional random fields„ÄÇ

Which is the final piece of the state of the art puzzle here„ÄÇ

 a lot of the successful networks today that do segmentationÔºå not all„ÄÇDo post process using CRFs„ÄÇ

 conditional random fieldsÔºå and what they do is they smooth the segmentation„ÄÇ

 the upsampled segmentation that results from the FCN by looking at the underlying image intensities„ÄÇ

So that's the key aspects of the successful approaches today„ÄÇ

 you have the encoded decoder framework of a fully accomplished neural network„ÄÇ

 it replaces the fully connected layers with the convolutional layers„ÄÇ

Deconvolutional errors and as the years progressed from 2014 to today„ÄÇAs usual„ÄÇ

 than underlying networks„ÄÇFrom AlexNet to VGGNe and to now Resnet have been one of the big reasons for the improvements of these networks to be able to perform the segmentation„ÄÇ

So naturallyÔºå they mirror the imagenet challenge performance in adapting these networks„ÄÇ

 So the state of the art uses resnet or similar networksÔºå conditional random fields„ÄÇ

For smoothing based on the input image intensities and the dilated„ÄÇ

Convolution that maintains the computational costÔºå but increases the resolution of the upsseampling„ÄÇ

Throughout the intermediate feature maps„ÄÇAnd that takes us to the state of the art that we used„ÄÇ

To produce the images„ÄÇÂóØ„ÄÇTo produce the images for the competition„ÄÇ

Bresnet DUC for Dance up sampling convolution„ÄÇInstead of bilinear up sampling„ÄÇ

 you make the ups learnable„ÄÇYou learn upscaling filters„ÄÇThat's on the bottom„ÄÇ

 that's really the key part that made it work„ÄÇThere should be a theme here„ÄÇ

 sometimes the biggest addition that could be done is paraizing one of the aspects of the network that they've taken for granted„ÄÇ

 letting the network learn that aspect„ÄÇAnd the other„ÄÇI'm not sure how important it is to the success„ÄÇ

 but it's a cool little additionÔºå is a hybrid dilated convolution„ÄÇ

As I showed that visualization where the convolution is spread apart a little bit in the input„ÄÇ

 from the input to the output„ÄÇThe steps of that dilated convolution filter„ÄÇWhen they're changed„ÄÇ

 it produces a smoother result because when it's kept the same„ÄÇ

Their certain input pixels get a lot more attention than others„ÄÇ

So losing that favoritism is what it's achieved by using a variable different dilation rate„ÄÇ

Those are the two tricksÔºå but really the biggest one is the parameterization of the upscaling filters„ÄÇ

OkayÔºå so that's what we use to generate that data and that's what we provide you the code with if you're interested in competing in psychFs„ÄÇ

The other aspect here that everything we talked about from the classification to the segmentation to making sense of images is the information about time„ÄÇ

 the temporal dynamics of the scene is thrown away„ÄÇ

And for the driving context for the robotics contest„ÄÇ

 and what we'd like to do is sefuse for the segmentation„ÄÇ

 dynamic scene segmentation context of when you try to interpret what's going on in the scene over time and use that information„ÄÇ

Time is essential„ÄÇThe movement of pixels is essential through time„ÄÇ

 that understanding how those objects move in a 3D space„ÄÇ

Through the 2D projection of an image is fascinating and there's a lot of set of open problems there„ÄÇ

So flow is what's very helpful toÔºå as a starting point to help us understand how these pixels move flow„ÄÇ

Optical flowÔºå dense optical flowsÔºå the computation that our best„ÄÇ

Our best approximation of where each pixel in image one„ÄÇ

And moved in the temporal following image after thatÔºå there's two images and 30 frames a second„ÄÇ

 there's one image at times 0Ôºå the other is 33„ÄÇ3 milliseconds later„ÄÇ

 and the dense optical flow is our best estimate of how each pixel in the input image moved to in the output image„ÄÇ

The optical flow for every pixel produces a direction of where we think that pixel moved and the magnitude of how far moved that allows us to take information that we detected about the first frame„ÄÇ

And try to propagate it forward„ÄÇ This is the competition is to try to segment an image and propagate that information forward„ÄÇ

For manual annotation„ÄÇOf an imageÔºå so this kind of coloring book annotation where you color every single pixel in the state of the art data set„ÄÇ

For driving cityscapes that it takes„ÄÇ 1„ÄÇ5 hoursÔºå 90 minutes to do that coloring„ÄÇ

 it's 90 minutes per image„ÄÇThat's extremely long time„ÄÇ

 that's why there doesn't exist today a data setÔºå and in this class we're going to create one„ÄÇ

Of segmentation of these images through time„ÄÇThrough video„ÄÇ

So long videos where every single frame is fully segmented„ÄÇ

That's still an open problem that we need to solve„ÄÇFlow is a piece of that„ÄÇAnd we also provide you„ÄÇ

This compute that state of the art flow using flowlowNe 2„ÄÇ0„ÄÇSo flowNe 1„ÄÇ0 in May 2015„ÄÇ

Use neural networks to learn the optical flowÔºå the dense optical flow„ÄÇ

And it did so with two kinds of architecturesÔºå flow net SÔºå flow net simpleÔºå and flow net core„ÄÇ

 flown net C„ÄÇThe simple one is simply taking the two images so what's the task here there's two images and you want to produce from those two images they follow each other in time 33„ÄÇ

3 milliseconds apart and your task is is the output to produce the dense optical flow so for the simple architecture you just stack them together each are RGB so it produces six channel input to the network there's a lot of convolution and finally it's the same kind of process as the fully convolution neural networks to produce the optical flow then there is flownet correlation„ÄÇ

ArchitectureÔºå where you perform some convolutions separately before using a correlation layer to combine the feature maps„ÄÇ

Both are effective„ÄÇIn different data sets and different applicationsÔºå so flowlowNe 2„ÄÇ

0 in December of 2016 is one of the state of the art frameworks„ÄÇ

Code bases that we use to generate the data I show„ÄÇCombines the flown at S and flown S„ÄÇ

It improves over the in flow netÔºå producing a smoother flow field preserves the fine motion detail along the edges of the objects„ÄÇ

And it runs extremely efficientlyÔºå depending on the architectureÔºå there's a few variants„ÄÇ

 either  eight to 140 frames a second„ÄÇAnd the process there is essentially one that's common across various applications„ÄÇ

 deep learning is stacking these networks together„ÄÇThe very interesting aspect here that„ÄÇ

We're still exploring and again applicable in all of deep learning in this case it seemed that there was a strong effect in taking sparse„ÄÇ

 small multiple data set and doing the trainingÔºå the order of which those datas were used for the training process mattered a lot„ÄÇ

That's very interesting„ÄÇ

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_17.png)

So using FlowNe 2„ÄÇ0Ôºå here's the data set we're making available for psychfuse the competition„ÄÇ

Cars that MIT died to use ssych views FirstÔºå the original video„ÄÇUs driving in high definition„ÄÇ

1080p in a 8K360 video„ÄÇOriginal video driving around Cambridge„ÄÇThen we're providing the ground truth„ÄÇ

For a training set„ÄÇFor that training set for every single frameÔºå 30 frames a second„ÄÇ

 we're providing the segmentationÔºå frame to frame to frame„ÄÇSegmented on mechanical turk„ÄÇ

We're also providing the output„ÄÇOf the network that I mentioned„ÄÇ

 the state of the art segmentation network„ÄÇThat's pretty damn close to the ground truth„ÄÇ

But still not„ÄÇAnd our task isÔºå this is the interesting thing is„ÄÇ

Our task is to take the output of this network„ÄÇWell there's two options„ÄÇ

 one is to take the output of this network„ÄÇAnd use other networks to help you propagate the information better so what this segmentation„ÄÇ

 the output of this network does is it only takes a frame by frame by frame it's not using the temporal information at all so the question is can we figure out a way„ÄÇ

 can we figure out tricks to use temporal information to improve this segmentation so it looks more like this segmentation„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_19.png)

And we're also providing the optical flow from frame to frame to frame„ÄÇ

 So the optical flow based on flow at 2„ÄÇ0 of how each of the pixels moved„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_21.png)

Okay„ÄÇAnd that forms a psyfuse competitionÔºå 10Ôºå000 images„ÄÇAnd the task is to submit„ÄÇCode„ÄÇ

 we have starter code in Python and on GitHub„ÄÇTo take in the original video„ÄÇ

 take in for the training setÔºå the ground truthÔºå the segmentation from the state of the art„ÄÇ

 segmentation networkÔºå the optical flow from the state of the artÔºå optical flow„ÄÇ

Network and taking that together to improve the stuff on the bottom left„ÄÇ

 the segmentation to try to achieve the ground truth on the top right„ÄÇ



![](img/503a9d7d80235ad0b54499bb7a8b6d3a_23.png)

OkayÔºå with thatÔºå I'd like to thank you tomorrow at 1 pm is Waymo in Stata„ÄÇ321Ôºå23Ôºå the next lecture„ÄÇ

 next week will be on deep learning for a sense in the human understanding the human„ÄÇ

 and we will release online only lecture on capsule networks and GANsÔºå general adversarial networks„ÄÇ

 thank you very much„ÄÇ

![](img/503a9d7d80235ad0b54499bb7a8b6d3a_25.png)