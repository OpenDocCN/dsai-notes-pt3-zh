# P10：L10- 机器学习简介 - ShowMeAI - BV1Y34y1i7vC

The video you're watching now is in 360 resolution is not great。

 but we wanted to try something different。So if you're on a desktop or laptop。

 you can pen around with your mouse， or if you're in a phone or tablet。

 you should be able to just move your device to look around Of course it's best viewed with a VR headset。

The video that follows is a guest lecture on machine learning that I gave an MIT Sloan course on the business of artificial intelligence。

The lecture is non technical and intended to build intuition about these ideas amongst the business students in the audience。

The room was a half circle， so we thought， why not film the lecture in 360？

We recorded a screencast of the slides and pasted it into the video so that the slides are more crisp。

Let me know what you think and remember it's an experiment。So this course is talking about。

The broad context， the impact of artificial intelligence， the global， there's global。

 which is the global impact of artificial intelligence， This is the business。

 which is when you have to take these fun research ideas that I'll talk about today。

 A lot of them are cool ontoy examples。 when you bring them to reality， you face real challenges。

 which is what I would like to really highlight today。😊。

That's the business part when you want to make real impact。

 when you're going to make these technologies a reality。

 So I'll talk about how amazing the technology is for a nerd like me。

 but also talk about how when you take that into the real world， what are the challenges you face。

 So machine learning。Which is the technology at the core of artificial intelligence。

We will talk about the promise。The excitement that I feel about it。

 the limitations will bring it down a little bit。 What are the real capabilities of the technology。

We for the first time。Really， as a civilization， exploring the meaning of intelligence。It is。

 if you pause for a second and just think， you know。

 maybe if many of you want to make money out of this technology， many of you want to save lives。

 help people， but also on the philosophical level， we get to explore what makes us human。

 So while I'll talk about the lowle technologies， also think about the incredible opportunity here。

 we get to almost psychoanalyze ourselves by trying to build versions of ourselves in the machine。

All right， so here's the open question。How powerful is artificial intelligence。

 How powerful is machine learning that lies at the core of artificial intelligence。

 Is it simply a helpful tool， a special purpose tool to help you solve simple problems if you're which is what it currently is。

Currently， machine learning， artificial intelligence is a way if you can formally define the problem。

 you can formally define the tools you're working with。

 you can formally define the utility function where you want to achieve those tools。

 as long as you can define those things， we can come up with algorithms that can solve them。

 as long as you have the right kind of data， which is what I'll talk about， data is key。

And the question is。Into the future。Can we break past this very narrow definition of what machine learning can give us。

 which is solve specific problems。To something bigger。

 to where we approach the general intelligence as we exhibit as human beings When we're born。

 we know nothing and we learn quickly from very little data。The right answer。As we don't know。

 we don't know what are the limitations of the technology。What kind of machine learning are there？

They're several flavors。 The first two is what's really the first is what's achieved success today。

 superupvised learning。 what I'm showing here on the left of the slide is the teachers is the data that is fed to the system and on the right is the students。

 which is the system itself for machine learning。So theres supervised learning when everybody talks about machine learning today。

 for the most part， they're referring to supervised learning。

 which means every single piece of data that is used to train the model is seen by human eyes and those human eyes。

With an accompanying brain， label that data。In a way that makes it useful to the machine。 This is。

 this is critical because that's one， the blue box， the human is really costly。

 So whenever every single piece of data that needs to be that's used to train the machine needs to be seen by a human。

 you need to pay for that human。And second， you're limited to just the time。

s the amount of data necessary to label what it means to exist in this world is humongous。Augmented。

 supervised learning is when you get machine to really to help you a little bit。

 There's a few tricks there， but still still only tricks。 It's still the human is at the core of it。

 And the promise。Of future research that we're pursuing that I'm pursuing and perhaps in the applications if we get to discuss or some of the speakers here get to discuss they're pursuing in semisupervised and reinforcement learning。

 where the human starts to play a smaller and smaller role in how much they get to annotate。

 they have to annotate the data。And the dream of the sort of wizards of the dark arts of deep learning are all excited about unsupervised learning。

 that has very few actual successes in application in the real world today。 But it is。😊。

The idea that you can build a machine that doesn't require a human teacher。

A human being to teach you anything。Is fills us。Artificial intelligence researchers with excitement。

There's a theme here。Machine learning is really simple。The learning system in the middle。

There's a training stage where you teach it something。 All you need is some data。Input data。

And you need to teach it。The correct output for that input data。So you have to have a lot of。

Pearairs of input data and correct output。There'll be a theme of cats throughout this presentation。

 So if you want to teach us a system。Difference between a cat and a dog。

 You need a lot of images of cats。 And you need to tell it that this is a cat。

This bonding box here and the image is a cat。You have to give it a lot of images of dogs and tell it。

 okay， well， this in this， in these pictures， there are dogs。And then， then， there's a。

Sppell only mistake on the second stage is the testing stage。

When you actually give it new input data as never seen before。

And you hope that it has given for cab versus dog enough data to guess。

 is this new image that I've never seen before。A cat or a dog。Now。

 one of the open questions you want to keep in mind。Is what in this world？

Can we not model in this way， What activity， what task， what goal。My。

 I offer to you that there's nothing you can't model in this way。So， let's think about。What。

In terms of machine learning。Can be。So it starts small。 What could be modeled in this way。

First on the bottom of the slide left is one to one mapping where the input is an image of a cat and the output is a label that says cat or dog。

You can also do one to many where the image。The input is an image of a cat。

 and the output is a story about that cat。A captioning of the image。You can， first of all。

 you can do it the other way， many to one mapping。Where you give it a story about a cat。

 and it generates an image。There's many to many， this is Google Translate。

 we translate a sentence from one language to another。And there's various flavors of that。 again。

 same theme here， input data。Provided with correct output。 and then。

Let it go into the wild where it runs on。Input data hasn't seen before to provide guesses。

And it's as simple as this。 whatever you can convert into one of the following four things， numbers。

Vectctor of numbers。 So a bunch of numbers。A sequence of numbers where the temporal dynamics matters。

 So like audio， video or the sequence， the ordering matters or a sequence of vector numbers。

 Just a bunch of numbers。 If you can convert it into numbers。

And I propose to you that there's nothing you can't convert into numbers。呃。

If you can converge numbers， you can have a system learn to do it。

 And the same thing with the output， generate numbers， vectors and numbers。

 sequence of numbers or sequence of vectors and numbers。First， is there any questions at this point？

We， we have a lot of fun slides to get through。But I'll pause every once in a while to make sure we're on the same page here。

 So what kind of input are we talking about just to fly through it， images。

 So faces or medical applications for looking looking at scans of different parts of the body to determine if to diagnose any kind of medical conditions。

 text， So conversations， your texts， article blog posts for sentiment analysis。

 question and answering， So you ask it a question where the output you hope is answers。

 sounds of voice recognition， any kind of anything you could tell from audio。Time series data。

 So financial data， stock market， can use it to predict。Anything you want about the stock market。

 including whether to buy or sell。If you're curious， it doesn't work quite well。

As a machine learning application。Physical world， So cars or any kind of object。

 any kind of robot that exists in this world。 So location of where I am。

A location of where other things are， the actions of others that could be all input。

 all of it can be converted to numbers。And the correct output， same thing， classification。

 a bunch of numbers。 classification is saying is this a cat or a dog。

 regressionion is saying to what degree I turn a steering wheel sequence， generating audio。

 generating video， generating stories， captioning， text， images。

 generating anything you could think of as numbers。

 And at the core of it is a bunch of data agnostic。Machine learning algorithms。

There's traditional ones near neighbors na Bay support。Support vector machines。A lot of them。Our。

Limited， and I'll describe how。 and then there's neural networks。

There's nothing special and new about neural networks。 And I'll describe。

Exactly the very subtle thing that is。Powerful， that's always been there all along。

 And certain things have now been able to unlock that power about neural networks。

 But it's still just the flavor of a machine learning algorithm。

And the inspiration from neural networks， as Jonathan showed last time。

 is our human brain is perhaps why the media， perhaps why the hype。

Is captivated by the idea of neural networks is because you immediately jump to this feeling like because there's this mysterious structure to them that scientists don't understand artificial neural networks I'm referring to。

And the biological ones， we don't understand them， And the similarity。

Capttivates our minds and we think， well。This approach is perhaps as limited as our as limitless as our own human mind。

But the comparison ends there。In fact， artificial neuron。 there are artificial neural networks。

Are much simpler computational units at the core of everything。Is this neuron。

This is a computational unit。That does。A very， two， very simple operations on the left side。

 it takes。A set of numbers as inputs。It applies weights to those inputs。Sums them together。

Applies a little bias。And provides an output。Somewhere between0 and one。

So you can think of it as a computational entity。That gets excited when it sees certain inputs。

And gets totally turned off when it gets other kinds of inputs。 So maybe this neuron。With a with  a 。

7。6，1。4 weights。It gets really excited when it sees pictures of cats。

And totally doesn't care about dogs。Some of us are like that。So that's the job of this neuron。

It to detect cats。Now， what， the way you build。An artificial neural network。

 the way you release the power that I'll talk about in the following slides about the applications of what could be achieved。

Is just stacking a bunch of these together。Think about it。这事实。But this is a extremely simple。

 computational unit。There so you need to sort of pause whenever we talk about the following slides and think。

That。There， there's a few slides that I'll show that say neural networks are amazing。

I want you to think back to this slide。That everything is built on top of these really simple addition operations with a simple nonlinear function applied at the end。

Just a tiny math operation。We stacked them together in a feed forward way。

 So there's a bunch of layers。 And when people talk about deep neural networks。

 it means there's a bunch of those layers。And then there's re neural networks that are also a special flavor that's able to have memory。

😊，So as opposed to just pushing input into output directly。

 It's also able to do stuff on the inside in a loop where it remembers things。

 This is useful for natural language processing， for audio processing， whenever the sequence is。

Not the length of the sequence is not defined。Okay， slide number one， in terms of neural networks。

 they are amazing。This is， this is perhaps for the math nerds。But also。

 I want you to use your imagination。 there's a universality to neural networks。

 means that this simple computational unit on the left is an input on the right is the output of this network。

With just a single hidden layer， it's called a hidden layer because it sits there in the middle of the input and the output layers。

A single hidden layer。With some number of nodes。Can represent any function。Any function。That means。

Anything you want to build in this world， Everyone in this room。Can be represented。

With a neural network with a single hidden layer。So the power， and this is just one he layer。

 The power of these things is limitless。 The problem， of course。Is how do you find the network？

So how do you build a network that as as clever as many of the people in this room。

 But the fact that that you can。Build such a network is incredible。 It is amazing。

 I want you to think about that。And the way you train a network， so。It's born as a blank slate。

 some random weights assigned to the edges。 Again， a network is represented the numbers at the core。

 the parameters at the core of this network are the numbers on each of those arrows。

 each of those edges。And you start knowing nothing。 This is a baby network。

And the way you teach it something。Unfortunately， currently， as I said。

 in a supervised learning mechanism， you have to give it pairs of input and output。

 You have to give it pictures of cats。And labels on those pictures， saying that they're cats。

And the basic fundamental operation of learning。Is when。You compute。The measure of an error。

And you back propagate it to the network。 What I mean。Everything's easier with cats。 I apologize。

Apo to many cats。And。So the input here is a cat。 and the neural network we trained。

 It's just guessing。 It doesn't know。 It's like， I don't know。I guessing cat。Well。

 it happens to be right。 So we have to， this is the measure of error。 Yes， you got it right。

 And you have to back propagate that error。You have to reward the network for doing a good job。

And all you do， what I mean by reward。 There's weights on each of those edges。

 And so the node that individual neurons that were responsible that back to that cat neuron。

 that cat neuron needs to be rewarded for seeing the cat。

So you just increase the weights on the neurons that were associated with producing the correct answer。

Now you give it a picture of a dog and the neural networkss his cat。Well， that's an incorrect answer。

 So no， there's a high error needs to be back propagated to the network。

 So the weights they're responsible with classifying this this picture as a cat need to be punished。

 They need to be decreased。肾保。And you just repeat this process over and over。

 It's just what we do as kids when we're first learning， and you know， for the most part。

 that we have two， we're also supervised learning machines in the sense that we have our parents。

And we have the environment， the world that teaches about what's correct and what's incorrect。

And we back propagate this error and reward through our brain to learn。The problem is。

As human beings， we don't need too many examples。 And I'll talk about some of the drawbacks of these approaches。

 We don't need too many examples。 You fall off your bike once or twice。

 and you learn how to ride the bike。 Unfortunately。

 neural networks need tens of thousands of times when they fall off the bike in order to learn how to not do it。

 That's one of the limitations。And one key thing I didn't mention here。

Is when we refer to the input data。It's。When we refer to input data。

 we usually refer to sensory data， raw data。 We have to represent that data in some clever way。

In some deeply clever way。Where we can reason about it。

Whether it's in our brains or in the neural network。And a very simple example here。

To illustrate why representation of data matters。So the way you represent the data can make the discrimination of one class from another。

 a cat versus dog， either incredibly difficult or incredibly simple。

 Here is a visualization of the same kind of data in Cartesian coordinates and p coordinates。

On the right， you can just draw a simple line to separate the two。

What you want is a system that's able to learn。The polar coordinate representation versus the Cartesian representation automatically。

And this is where deep learning has stepped in。And revealed the incredible power of this approach。

Which。Deep learning is the smallest circle there。Is a type of representational learning。

Machine learning is the bigger， second to the biggest。 So this class is about the biggest circle。

 AI includes robotics includes all the fun things that are built on learning。 And I'll discuss while。

 machine learning， I think will close this entire circle into one。 But for now。

 AI is the biggest circle than the subset of that is machine learning and a smaller subset of that is representation learning。

So deep learning is not only able to say， given a few examples of cats and dogs that discriminate between a cat and a dog。

It's able to represent what it means to be a cat。It's so it's able to automatically determine。

What are the fundamental units。At the low level and the high level， talking about this very Plato。

 what it means to represent a cat from the whiskers to the high level shape of the head to the fuzziness and the deformable aspects of the cat。

 not a cat expert， but I hear these are the features of a cat versus that are essential to discriminate between a cat and a dog。

Learning those features as opposed to having to have experts。

 This is the drawback of systems that Jonathan talked about from the 80s and 90s where you had to bring in experts for any specific domain that you try to solve。

You had to have them encode that information。 Deep learning。 This is This is simply the only。

Big difference between deep learning and other methods is that it learns the representation for you。

 It learns what it means to be a cat。 Nobody has to step in and help it figure out what that cats have whiskers and dogs don't。

What does this mean， The fact that you can learn these features， these whisker features is。

As opposed to having5 or 10 or 100 or 500 features。

 that are encoded by brilliant engineers with PhDs。It can find hundreds of thousands。

 millions of features， automatically。Hundreds of millions of features。 So stuff that。

That can't be put into words it described。 In fact。

 it's one of the limitations in neural networks is they find so many fundamental things about what it means to be a cat that you can't visualize what it really knows。

 It just seems to know stuff。And it finds that stuff automatically。 What， what does this mean。

 It's the critical thing here is because it's able to automatically learn those hundreds of millions of features。

 it's able to utilize data。It doesn't start， the diminishing returns。

Don't hit on until what we don't know when they hit。 The point is。

 with the classical machine learning algorithms， you start hitting a wall when you have tens of thousands of images of cats with deep learning。

 you get better and better with more data。Neural networkss an amazing slide too。Here's a game。

 a simple arcade game， where there's two paddles， balancing a ball back and forth。Okay， great。

 You can figure out an artificial intelligence agent that can play this game。 It can。

 not even that well。 just kind of， it kind of learns to do all right and eventually win。

Here's the fascinating thing。With deep learning。As opposed to encoding。The position of the paddles。

 the position of the ball， having an expert in this game。

There many come in and encode the physics of this game。

The input to the neural network is the raw pixels of the game。So。It's learning in the following way。

You give it an evolution of the game。 You give it a bunch of pixels。 Pels are。

 images that built up of pixels。 They are just numbers from 0 to 2，56。

So there's this array of numbers that represent each image。

 and then you give it several tens of thousands of images that represent a game。

 So you have this stack of pixels and stack of images that represent a game and the only thing you know。

 this giant stack of numbers， the only thing you know is at the end you won or lost。That's it。

So based on that。You have to figure out how to play the game。 You know nothing about games。

 You know nothing about colors or balls or paddles or winning or anything。That's it。 So this is。

 why is this amazing that it even works and works It wins。

 It's amazing because that's exactly what we do as human beings。 This is general intelligence。😊。

So I need you to pause and think about this。We'll， we'll talk about special intelligence and the usefulness。

 And it's okay， there's cool tricks here and there that we can do to get you an edge on your high frequency trading system。

 But this is general intelligence。General intelligence is the same intelligence we use as babies when we're born。

 What we get is an input sensory input of image sensory input right now。 All of us。

 most of us are seeing。Hearing。Feeling with touch。 And that's the only input we get。 We know nothing。

 And with that input， we have to learn something。Nobody is pre teaching us stuff。

 And this is an example of that， a trivial example。

 But one of the first examples where this is truly working。 I sorry to linger on this。

 but it's a fundamental fact。The fact that we have systems that and now outperform human beings in these simple arcade games is incredible。

 This is the research side of things。😊，But let me step back。 These again， the takeaways。

That previous slide is why I think machine learning is limitless。In the future。Currently。

 it's limited。Again， the representation of the data matters。And if you want to have impact。

We currently can only tackle the small problems。What are those problems？Image recognition。

 we can classify given the entire image of a leopard of a boat。Of a might。With pretty good accuracy。

Of what's in that image。 That's image classification。What else？

We can find exactly where in that image each individual object is。 That's called image segmentation。

Again， the process is the same。The learning system in the middle， a neural network。

As long as you give it a set of numbers as input。And the correct set of labels is output。

 it learns to do that for data hasn't seen the best。

Let me pause a second and maybe if you have any questions。

 does anyone have any questions about the techniques of neural networks？Yes。对的。M the two。关。So。

That's a great question。 And in a couple of slides， I'll get to it exactly。

 So the data representation。I'll elaborate in a little bit， but loosely。

 the data representation is for a neural network。Is in the weights。Of。

Each of those arrows that connect the neurons。 That's where the representation is。So Ill。

 I'll show to really clarify that example of。What that means。

The Cartesian versus polar coordinates is just a visual very simple。Viization of the concept。

But you want to be able to represent the data in an arbitrary way。

Where there's no limits to their representation， it can be highly nonlinear， highly complex。

Any other questions。S agree。Generally speaking。It' simply。

Statistical models that are able to recognize training。

Things of that nature where they're not necessarily thinking。But simply recognize。

And so I'm a little confused about。The current。System differs from。And whether you think that。

I is the possibility of transition？So'm actually thinking。

So I have a couple of slides almost asking this question because there's no good answers。

 but one could argue。 And I think somebody in last class broughted up that， you know。

 is machine learning just pattern recognition。呃。It's possible that reasoning， thinking。啊。Is just。

Pattern recognition。And I'll describe。Sort of an intuition behind that。

So we tend to respect thinking a lot。Because we've recently， as human beings learned to do it。

In our evolutionary time。We think that it's somehow special from， for example， perception。

 We've had visual perception for several orders of magnitude longer in our evolution in evolution as。

 as a living species。We've started to learn to reason， I think， about 100000 years ago。

So we think it's somehow special from the same kind of mechanism we use for seeing things。

Perhaps it's exactly the same thing so perception is pattern recognition。

 perhaps reasoning is just a few more layers of that。That's the hope。 but it's an open question。Yes。

コンセントのニューニューアルネットワーク。そは。一 있어야や店に。変かイノベーション。just a？The result。Yes， that's a great question。

There's been very few breakthroughs in neural networks since through the AI winters that we've discussed。

Through a lot of excitement。In spurts， and even recently there's been a very few algorithmic innovations。

The big gains came from compute， some improvements in GPU and better faster computers。

You can't underestimate the power of community。 So the ability to share code and the Internet。

Ability to communicate together through the internet and work on code together and then digitization of data。

 so like ability to have large data sets easily accessible and downloadable。

All of those little things。 But I think it， in terms of the future of deep learning and machine learning。

 it， it all rides on compute， I think， meaning continued， bigger and faster computers。

That doesn't necessarily mean Moore's law in making small and smaller chips。

 It means getting clever in different directions， massive parallelization coming up with ways to do super efficient power efficient implementations in neural networks and so on。

So， let me if。Fly through a few examples of what we can do with machine learning just to give you a flavor I think in future lectures as possible。

 we'll discuss different speakers， different specific applications really dig into those so we can as opposed to working with just images you can work with videos and segment those I mentioned image segmentation。

 we do video segmentation， though through video segment。

 the different parts of a scene that's useful to a particular application here and drive you can segment the road from cars and vegetation。

And lane markings， you can also。This is a subtle but important point yes， go back that one slide。

How do you see the light。 I mean that's such a critical piece of， you know。

 the more I listen to you and read your stuff。 It seems like this critical these。

 these very small piece of information that we just， we know are important like there is a red light。

well the light I have to stop， I have to slow down， how does it filter that out and pick out that？

So this。😀。Oh， hard question， so。呃呃。The question was， how do you detect the traffic light？And lights。

So how do we do as human beings？First of all， let's start there。 The way we do it is by。

The knowledge were bring through the table。So we know what it means to be on the road。

 there's a lot of the huge network of knowledge that you come with。

 and so that makes the perception problem much easier。 This is pure perception。

 You take an image and you separate different parts based purely on tiny patterns of pixels。

So first defines all the edges。And it learns that traffic lights have certain kinds of edges around them and then zoom out a little bit。

 they have a certain collection of edges that make up this black rectangle type shape so it's all about shapes it kind of build up knowing this shape structure of things。

 but it's a purely perception problem and one of the things I argue is that if it's purely a perception approach and you bring no knowledge to the table about the physics of the world。

 the three dimensionmenional physics and the temporal dynamics that youre not going to be able to successfully achieve。

Near 100% accuracy on some of these systems。So that's exactly the right question。

You for all of these things， think about how you as a human being would solve these problems and what is lacking in the machine learning approach。

 what data is lacking in the machine learning approach in order to achieve the same kind of results。

The same kind of reason it required that you were used as a human。So there is also image detection。

Iage detection， which means it's a subtle but important point。

 the stuff I mentioned before image classification is given a image of a cat you find the cat sorry you don't find the cat。

 you say this image is of a cat or not， and then detection or localization is when you actually find where in the image that is。

 That problem is much harder but also doable with machine learning。

With with deep neural networks now， as I said， inputs， outputs can be anything。

 the input can be a video， the output can be video。

 and you can do anything you want with these videos。 you can colorize the video。

 You can add take an old black and white film and produce color images。Again。

 in terms of having an impact in the world using these applications， you have to think。

This is a cool demonstration， but how well does it actually work in the real world。Translation。

 whether that's from text to text or image to image， you can translate here， dark chocolate。

From one language to another。It's class global business of artificial intelligence。

 There's a reference below there。 You can go and generate your own text。

 You can generate the writing of the act of generating handwriting。

 So you can type in some text and given different styles that it learns from other handwriting samples。

 It can generate any kind of text using handwriting again。The input is language。

 the output is a sequence of writing。Of pen movements on the screen。 You can complete sentences。

This is kind of a fun one， where if you start。So you can generate language and you can generate a language where you start。

 you feed the system some input first， so in black there says life is。

And then have the neural network complete those sentences。Life is about kids。

 Life about life is about the weather。 There's a lot of knowledge here。 I think being conveyed。

 And you can start the sense what the meaning of life is。The meaning of life is literary recognition。

True for us， academics or the meaning of life is the tradition of ancient human production。Also true。

But these are all generated by a computer。 You can also caption。

 This has been become very popular recently is caption generation。 given us input is an image。

 The output is a set of text。 that captions the content of the image。

 you find the different objects in the in the image， that's a perception problem。

 And once you find the different objects。 You stitch them together in a sentence that makes sense。

 You generate a bunch of sentences and classify which sentence is the most likely to fit this this image。

And you can so certainly in the。I try to avoid mentioning too driving too much because it is my field。

 That is what I'm excited about， but。Then the moment I started talking about driving。

 it'll all be about driving。 So， but I should mention， of course。

 that deep learning is critical to driving applications for the both the perception and what is really exciting to us now is the end to end。

 the end to end approach。 So whenever you say end to end in any application， what that means。

Is you start from the very raw inputs that the system gets and you produce the very final output that's expected of the system。

 So as opposed to in the cell driving car case， as opposed to breaking a car down into each individual components of perception。

 localization， mapping， control planning and just taking the whole stack and just ignoring all the。

Super complex problems in the middle， just taking the external scene as input and as output produce steering and acceleration and breaking commands。

 And so in this way， taking this input is the image of the external world。 In this case， in a Tesla。

 we can generate steering commands for the car。 Again。

 input a bunch of numbers that that's just images， output。

A single number that gives you the steering of the。Of the car。Okay。

 so let's step back for a second and think about what can't we do with machine learning。 We talked。

 we talked about， you can map numbers to numbers。 Let's think about what we can't do This at the core of artificial intelligence in terms of making an impact on this world is robotics。

So what can't we solve in robotics and artificial intelligence with a machine learning approach。

And let's break down what artificial intelligence means。

 here's a stack starting at the very top is the environment， the world that you operate in。

 there's sensors that sense that world， there is feature extraction and learning from that data and there's some reasoning。

 planning and effectors are the ways you manipulate the world。What can't we learn in this way？

So we've had a lot of success as Jonathan talked about in the history of AI with formal tasks。

 playing games， solving puzzles， recently we're having a lot of breakthroughs with medical diagnosis。

We're still。Sorry。We're still。Struggling， but are very excited about in the robotic space with more mundane tasks of walking。

 of basic perception of natural language， written and spoken。And then there is the human tasks。

Which are perhaps completely out of reach of this pipeline at the moment。Is。Cognition。Imagination。

Sject， subjective experience。 So high level reasoning， not just common sense， but high level。

Human level reasoning。So let's fly through this pipeline。 There's sensors， cameras， Lidar。Audio。

There's communication。That flies the air or wired or wireless or wired。

 I am you measuring the movement of things。So that's the way you think about it。

 That's the way as human beings。 And as any kind of system that you design， you measure the world。

 You don't just get an API to the world。 You need to somehow measure aspects of this world。

So that's how you get the data。 So that's how you convert the world into data you can play with。

 And once you have the data， this is the representation side。

 you have to convert that raw data of raw pixels， a raw audio， raw LiDarR data。

 you have to convert that into data that's useful for the intelligence system for the learning system to use the discriminate between one thing and another。

For vision， that's finding edges， corners， object parts and entire objects。

 And there's the machine learning that I' talk that I've talked about。

There's different kinds of mapping of the representation that you've learned to an actual outputs。

There is once you have this。 So you have this idea of。

 and this goes to maybe a little bit of Simon's question is reasoning。

 This is something that's out of reach and machine learning at the moment。

 This is going to your question。Then we can， we can build a world class machine learning system for taking an image and classifying that it's a duck。

I wonderder if this will work。啊嗯嗯啊。Wake you up。So we could take， this is well studied。

 exceptionally well studied problem。 We could take audio sample。Of a duck and tell that it's a duck。

In fact， what species of bird。It's incredible how much research there is in bird species classification。

 And we can look at video， and we could tell that we can do actual recognition that it's swimming。😊。

But we can't do with learning now is reason。That if it looks like a duck。

 it swims like a duck and quacks like a duck， it's very likely to be a duck。

This is the reasoning problem。 This is the task that I personally am obsessed with and that I hope that machine learning can close。

And then there is the planning action and the effectors。So this is another place where。

Machine learning has not had many strides， there's mechanical issues here that are incredibly difficult。

 the degrees of freedom with all the auators involved with all the just the ability to localize every party yourself in this dynamic space。

Where things are constantly changing when there's degrees of uncertainty， when there's noise。

 just that basic problem is exceptionally difficult。So let me just pose this question。

 We talked about how machine， what machine learning can do with the cats and the duck。

We could do that。 given a representation， it could predict what's in the image。

But one of the open questions is。And deep learning has been able to do the feature extraction。

 the representation learning。 This is the big breakthrough that everybody' is excited about。

But can it also reason？These are the open questions。 Can a reason。Can it do the planning and action。

And as human beings do， can you close the loop entirely。From sensors to effectors。

 So learn not only the brain。But。The way you sense the world。And the way you affect the world。P game。

 so essentially does a real network。So the question was about the pawn game， thank you。

talk to it for a little longer。It it doesn't get punished when it doesn't detect the ball。

 This is the beautiful thing。It gets punished only at the very end of the game for losing the game and gets rewarded for winning the game。

 So it knows nothing about that ball。And it learns about that ball。

That's something you really sit and think about。Because like how do as human beings。

 imagine if you're playing with a physical ball， how do you learn what a ball is。You。

 you get hurt by it。 You like squeeze it， you throw it， You feel the dynamics of it。

 the physics of it， and。Nobody tells you about what a ball is。

 You're just using the raw sensory input。 We take it for granted。 And maybe this is what I can end。

On。Is， this is what something Jonathan brought up。As we take the simplicity of this task for granted。

Because we've been。We've had eyes。We broadly speaking as living species on planet Earth。

 these eyes have been evolved for 540 million years。 So we have 540 million years of data。

 We've been walking for close to that。Bippedo mammals。We have been thinking only very recently。

 So 100000 years versus 100 million years。And。That's why we can't some of these problems that we're trying to solve。

 You can't take for granted how actually difficult they are。 So， for example。

This is the Marvex paradox。 that Jonathan Bart up is that the easy problems are hard。

 The things would think are easy， actually really hard。 This is state of the art robot on the right。

 playing soccer。And that was a state of the art human on the left， playing soccer。And。Yeah。😊。

I'll give it a second。The question was， you know， there's a fundamental difference between the way we' trained neural networks and the way we've trained biological neural networks to revolution by discarding through natural selection。

 a bunch of。The neural networks that didn't work so well。That's， so first of all。

 the process of evolution is。I think， not well understood。meaning sorry， the role I careful here。

 the role of evolution in the evolution of our cognition。Of our intelligence。I don't know if that's。

 So this is an open question。 So maybe clarify this point is neural networks。

Artificial neural networks are fixed， for the most part， In size。 This is exactly right。

 It's like a single human being that gets to learn。 We don't have mechanisms of。

Of modifying or revolving those neural networks yet。

Although you could think of researchers as doing exactly that。

 you have grad students working on different neural networks and the ones that don't do a good job。

 don't get promoted and get a good。 You know， there is a natural selection there。

 But other than that， it's， it's an open question。 It's a fascinating one， y。

So Lea is going to come back， It's not not available next week。

 but he's going to come back the week after。 so we can pick up many of these points here。

 many of points can continue。 Is there any last final。Takeaway you want。

Stay tuned and keep your head up because the future， I believe， is really promising。

And the slides will be。made available for sure。 so we' take five minute break。

I think a lot of the explorations of what it means to build an intelligent machine has been in Scifi movies。

 we're not beginning to actually make it a reality。

 this is space Odyssey to keep with that theme in the previous lecture that we had。This is。

 as opposed to the dreamlike monolith view， when the astronaut is gazing out into the open sky at the stars。

 we're going to look at the practice of AI today。And how we go， if you're familiar with the movie。

 when this new technology appeared before our eyes， and we're full of excitement。

 how we transfer that into actual，Practical impact on our lives。

To quickly review what we talked about last time。I presented the technology and asked the question of whether this technology merely serves a special purpose to answer specific tasks that can be formalized or whether it can be through。

Through the process of transferring the knowledge learned on one domain。

 be generalizable to where an intelligent system that's trained in a small domain can be used to achieve general intelligent tasks like we do as human beings。

 this is kind of the stack of artificial intelligence of going from all the way up into the top of the environment。

 the world， the sensors sets of data， the intelligence system， the way it perceives this world。

 then once you have you convert the world into some numbers。

 you able to extract some representation of that world。

 And this is where machine learning starts to come into play。

 And then there's the part where I will raise it again today is can machine learning be doing the following steps too that we can do very well as human beings is the reasoning step。

 you know you can tell the difference in a cat and a dog。

 but can you now start to reason about what it means to be alive。

 what it means to be a cat with living creature and what it means to be this kind of physical object or this kind of physical object and take。

It's called common sense things we take for granted start to construct models of the world through reasoning。

Descartes， I think therefore I am， we want our neural networks to come up with that on their own。

And once you do that。Action， you'll go right back into the world and you start acting in that world。

 So the question is， can machine learning， can this be learned from data or does do experts need to encode the knowledge of reasoning the knowledge of actions。

 the set of actions， That's kind of the question， the open questions I raise。

 It continues throughout the talk today and。So as we start to think about how artificial intelligence。

 especially machine learning as it re itself through robotics， gets to impact the world。

 we start thinking about what are the easy problems and what are the hard problems。

 and it seems to us that。vision and movement， walking is easy because we've been doing it for millions of years。

 hundreds of millions of years and thinking is hard， reasoning is hard。

 I propose to you that is perhaps because we've only been doing it for a short time and so think we're quite special because we're able to think so we have to kind of question of what is easy and what is hard because when we start to develop some of these systems。

And what you start to realize all of these problems are equally hard。

 So the problem of walking that we take for granted， the actuation and the physical。

The ability to recognize where you are in the physical space to sense the world around you to deal。

Deal with the uncertainty of the perception problem。 And then so all of these robots， by the way。

 this is for the most recent DARPA challenge， which Mi T was also part of。

And so what are these robots doing？They they don't have any。

 they only have sparse communication with human beings on the periphery。

 So most of the stuff they have to do autonomously， like get inside a car。 This is an MIT robot。

 unfortunately。They have to get in the car and the hardest task， they have to get out of the car。

That's walking。 So this kind of raises to you very real。Aspect here。

 you want to build applications that actually work in the real world。

 and that's the first challenge and opportunity here。The many of the technologies as we talked about。

Currently， crumble under the， the reality。Of。Our world。

When we transfer them from a small data set in the lab to the real world。

 for the computer vision is perhaps one of the best illustrations of this。

Computer vision is the task because we talked about of interpreting images。 And so when you。

 there's been a lot of great accomplishments on interpreting images， Cats search as dogs。Now。

 when you try to create a system。Like the Tesla vehicle that I've often that we work with and I always talk about is it's a vision-based robot as radar for basic obstacle avoidance。

 but most of the understanding of the world comes from a single monocular camera now they've expanded a number of cameras。

 but for the most time there's been 100，000 vehicles driving on the roads today with a single。

 essentially a single webcam。So when you start to do that。

 you have to perform all of these extraction of texture， color。

 optical flows so the movement through time， temporal dynamics of the images。

 you have to construct these patterns， construct the understanding of objects and entities and how they interact and from that you have to act in this world。

 and that's all based on this computer vision system so it's no longer cats versus dogs。It's。

It's huge detection of pedestrians， where the wrong。Classification。

 the wrong detection is the difference in life and death。

So let's look at cats where things is a little more comfortable。So computer vision。

 and I would like to illustrate to you why this is such a hard task。 we talked about。

 we've been doing it for 500 million years。 so we think it's easy。

Computer vision is actually incredible。 So all you're getting with your human eyes。

 is you're getting essentially pixels in， there's light coming into your eyes and all you're getting is the reflection from the different surfaces in here of light。

 And there's perception， there's sensors inside your eyes converting that into numbers。

 It's really very similar to this。😊，Numbers in the case of what we use with computers。

 RGB images were the individual pixels are numbers from0 to 255。

 so 256 possible numbers and there're just a bunch of them。 and that's all we get。

 We get a collection of numbers where they're spatially connected ones that are close together are part of the same object。

 So cat pixels are all connected together。 That's the only thing we have to help us。

 but the rest of it is just numbers， intensity numbers。

 and we have to use those numbers to classify what's in the image。And if you really think about it。

 this is a really difficult task。 All you get is these numbers。

 How the hack are you supposed to form a model of the world with which you can。

Detect pedestrians with really 99。99999% accuracy， because these pedestrians or these cars。

 a cyclist in the car context or any kind of applications you're looking at。

Even if your job is in the factory floor to detect the defective gummy bears。

 they are flying past at like 100 miles an hour， your task is you don't want that bad gummy bear to get by that your product and the brand will be damaged。

 however serious or not serious your application is。What you have to be。

 you have to have a computer vision system。That deals with。All of these aspects， viewpoint variation。

 scale variation， no matter the size of the object is still the same object。

 no matter the viewpoint from which the area you look at that object is still the same object。

 The lighting that moves， we have lighting consistently here because we're indoors。

 when you're outdoors or you're moving， the scene is moving， the lighting。

 the complexity of the lighting variations is incredible。😊。

From the illumination to just the movement of the different。Objects in the scene。

Now we've had these conversations， I think about this every time I drive。

 I think about you and this point and how hard it this to see these things and particularly when I'm driving at night。

 and particularly when it's twilight and the light is changing。I think。

 you almost every time I drive is one or two things that I see that are really that I'm drawing on like 200 million years in order to be able to figure out it's it's a guy who's open his car door and I can't see him。

 but I can just see the light doesn't look quite right on that side of the road and I somehow know in my mind it's a person。

But it seems like an almost impossible problem for the machines to get right with sufficient accuracy。

 I will argue that the pure perception task is too hard。

That you come to the table as human beings with all this huge amount of knowledge。

That you're not actually interpreting all the complex lighting variations that you're seeing。

 You actually know enough about the world， enough about your commute home。

 enough about the way the kinds of things you would see in this world about Boston about the way pedestrians move the certain light of day。

 you bring all that to the table that makes the perception task doable。

 And that's one of the big missing pieces in the technology as I'll talk about。

 that's the open problem of machine learning is how to bring all that knowledge， first of all。

 build that knowledge and then bring that knowledge to the table。

 as opposed to starting from scratch every time。And so。Cats， I promise cats。 Okay， so the， to me。

 occlusion。For most of the computer vision community。

 this is one of the biggest challenges and it really highlights。

How far we are from being able to reason about this world， Icclusions are when， what an occlusion is。

 is when the objects， you're trying to detect something about classify the object， De the object。

 The object is。Blocked partially by another object in front of them。

This is something that you think is trivial perhaps you don't even really think about it because we reason in a threediional way。

 but the occlusion aspect is makes perception incredibly difficult so we have to design think about this so this image is converted into numbers and we for the task of detecting is there a cat in this image yes or no。

 you have to be able to reason about this image with that object in the scene most of us are able to very easily detect that there's a cat in this image。

We're able to detect that there is a cat in this image。 Now， think about this。

 there's a single eye and there's an ear。So you have to think about what is it part of our brain that allows us to understand to suppose that with some high degree of accuracy that there's a cat here in this picture。

I mean， the degree of occlusion here is immense。And so I promise。So this is for most of you。呃。

Some of you will think this is， in fact， a monkey eating a banana。

 but I would venture to say that most of us are able to tell it's nevertheless a cat。

You watch this for hours。And so let me give you another。

 this is kind of a paper that's often cited or a set of papers。

To illustrate how difficult computer vision is， how thin the line that we're walking with all of these impressive results that we've been able to show recently in the machine learning community。

In this case。For deep neural networks are easily fooled， paper。

 the seminal paper at this point shows that when you apply。Network trained on imagenets。

 so basically on detecting cats versus dogs or different categories inside images。

If you you can find an arbitrary number of images that look like noise up in the top row。

Where the algorithm。Used to classify those images in an image net of cat versus dog is able to confidently say with 99。

6% accuracy or above that it's seeing a robin or a cheetah or an armadillo or a panda in that noise。

So it's confidently saying given this noise that that's obviously a robin。

So you have to realize that。The kind of。This is patterns。

 the kind of processes it's using to understand what's containing the image is purely a collection of patterns that it has been able to extract from other images that has been human annotated by humans。

And that perhaps is very limiting to trying to create a system that's able to operate in the real world。

 This is a very sort of this is very clean illustration of that concept and the same you can confidently predict in those images below where there's strong patterns。

 it's not even noise， strong patterns that have nothing to do with the entities being detected again。

 confidently that same algorithm is able to see a penguin a starfish or baseball and a guitar in that noise。

And more serious。For people designing robots like myself in on the sensor side， you can flip that。

And say， I can take。A image， and I can distort it with some very little amount of noise。And if that。

If that noise is applied to the image， I can completely change the confident prediction about what's in that image。

 So to explain what's being shown。 So on the left and the column in the left。And again， here。

the same kind of neural network is able to predict accurately confidently that there is a dog in that image。

 but if we apply just a little bit of noise to that image to produce that image imperceptible to our human eyes the difference between those two。

 the same algorithm is saying that there is confidently an ostrich in that image。

So another thing to really think about that noise can have such a significant impact on the prediction of these algorithms。

 This is really。Really， quite honestly， out of all the things I'll say today。

 and I'm aware of one of the biggest challenges of。

Machine learning being applied in the real world is robustness。

 How much noise can you add into the system before everything falls apart？

So how do you validate sensors， so say a car company has to produce a vehicle and it has sensors in that vehicle。

 how do you know that those sensors will not start generating slight noise due to interference of various kinds and because of that noise instead of seeing a pedestrian it will see nothing or the opposite you'll see pedestrians everywhere so of course the most dangerous is when it will not see an object and collide with it in the case of cars。

There's also spoofing， which a lot of people as always with security。

 people are really concerned about。And perhaps people here are really concerned about this issue。

 I think this is a really important issue， but because you can apply noise and convince the system that you're seeing an ostrich when there's in fact no ostrich。

You can do the same thing in a。In an attacking way。

 So you can attack the sensors of a car and make it believe， like with Lidar spoofing。

 So spoof Lidar or radar or ultrasonic sensors to believe that you're seeing pedestrians when they're not there and the opposite to hide pedestrians make pedestrians invisible to the sensor when they're。

 in fact there。So whenever you have intelligent systems operating in this world。

They become susceptible to the fact that everything。

 so much of the work is done in software and based on sensors。 So at any point in the chain。

 if there's a failure。You have to be able to detect that failure。 And right now。

 we have no mechanisms for automatically detecting that failure。 So on the data side。

 So one challenge is that we're constantly dealing with。Is。That we。

Are the algorithms and machine learning algorithms that we're using。Our need labeled data。

And we have very little label data。 Laed data， again， is when you have pairs of。

Iput data and the ground truth， the true label annotation class that that image belongs to or concept。

And it doesn't have to be an image you could be any source of data。

 It's a really costly process to do。So。Because it's so costly。We。

Rely every breakthrough we've had so far relies on that label data。And because of its cost。

 we don't have much of it。So all the problems that come from data can either be solved by having a lot more of this data。

 which I believe is most people believe is too challenging。

 it's too challenging to have human beings annotate huge amounts of data。

Or we have to develop algorithms that are able to do something with the unlabeled data。

It's the unsupervised， semiupervised， sparsely supervised reinforcement learning。

 as we talked about last time I'll mention again here。

So one way you understand something about data when you don't have labels is your reason about it。

 All you're given is a few facts， when you're a baby。

 your parents give you a few facts and you go into this world with those facts。

 and you grow your knowledge graph， your knowledge base。

 your understanding of the world from those few facts。

 We don't have a good method of doing that an automated unrestricted way。

The inefficiency of our learners， the machine learning algorithms haveve talked about the neural networks need a lot of examples of every single concept that they're given in order to learn anything about them。

 thousandshouand， tens of thousands of cats are needed to understand what the spatial patterns at every level。

 the representation of a cat， the visual representation of a cat。We don't。

 We can't do anything with a single example。 There's a few approaches， but nothing quite。Robust， yet。

And。We haven't come up with a way this is also possible。To make annotation， this labeling process。

Somehow， be very cheap。So leveraging， this is something being called human computation。

 that term has fallen out of favor a little bit， one of my big passions is human computation is using something about our behavior。

 something about what we do in this world online or in the real world。To annotate data automatically。

So， for example， as you drive， which is what we do。

 Everybody has to drive and we can collect data about you driving in order to train self driving vehicles to to。

To drive， and that's a free annotation。So here are the annotated data sets we have。

 the supervised learning data sets。There's many， but these are some of the more famous ones from the Toy dataset sets of MNIS to the large broad arbitrary categories of images。

 data sets and which is what imagenet is and there's in healthcare， there's an audio。

 there's in video know there's a huge number of data sets now。

 but each one of them is usually in the scale of hundreds of thousands， millions， tens of millions。

 not billions of trillions， which is what we need to create systems that operate in the real world。

 and again， these are the kinds of machine learning algorithms we have。 There's five listed here。

The teachers on the left is what is。What is the input to the system that requires to train it？

From the supervised learning at the very top is what we have all of our successes and everything else is where the promise lies。

 the semisupervised， the reinforcement or the fully unsupervised learning。

 where the input from the human is very minimal。 and another way to think about this。

So whenever you think about machine learning today， whenever somebody talks about machine learning。

 what they're talking about is systemsmize that memorize patterns。

And so this is one of the big criticisms of the current machine learning approaches。

 where all they're doing is you're providing they're only as good as the human annotated data that they're provided。

We don't have mechanisms for actually understanding。

You can pause and think about this in order to create an intelligent system。

 it shouldn't just memorize。 it should understand the representations inside that data in order to operate in that world。

And that's the open question。One of them。 and one of the challenges and opportunities for machine learning researchers today is。

To extend machine learning from memorization to understanding， this is that duck。The reasoning。

If you get information from the perception systems that it looks like a duck。

 from the audio processing， that it quacks like a duck， and then from video classification。

 that the activity recognition that it swims like a duck。

 the reasoning step is how to connect those facts to then say that it is， in fact a duck。Okay。

 so that's on the algorithm side and the data side。Now。

 this is one of the reasons compute computational power。

 computational hardware that is at the core of the success of machine learning。

So our algorithms have been the same since the '60s since the 80s， '90s。

 depending on how you're counting。The big breakthroughs came in compute。So there's Moore's law。

Most of you know。The way our the CPU side of our computers works for a single CPU is that it's。

For the most part， executing a single action at a time in a sequence。So sequential。

Very different from our brain， which is massively paralleled system。So because it's sequential。

 the clock speed matters because that's how fast essentially。

 those instructions are able to be executed。 And so wherere。We're leveling off physics。

Is stopping us from continuing Moore's law， so Intel AMD are aggressively pushing this Moore's law forward。

But。And there's some promise that it'll actually continue for another 10 or 15 years。

Then there's another form of parallelism， massive parallelism， is the GPU。

 and this is essential for neural networks。This is essential to the success。

 recent success of neural networks is the ability to utilize these。

Inherently parallel architectures of graphics， processing units， GPUs。

 the same thing used for video games。 this is the reason N V stock is doing extremely well。Is。

 is GPs。 So it's parallelism of basic computational processes that make machine learning work on the GPU。

One of the limitations of GPUs， one of the challenges is in bringing them to in scaling and bringing them into real world applications is power usage。

It's power consumption。And so there is a lot of specialized chips。

 specialized just from the neural network architectures coming out from Google with a tensor processing unit from IBM。

 Intel and so on。 It's unclear how far this goes。 So this is sort of the direction of trying to design an electronic brain。

 so it has the efficiency。 Our human brain is exceptionally efficient at running the neural networks in our heads。

Or is of magnitude more efficient than our computers are。

 and this is trying to design systems that are able to go towards that efficiency。

Why do you care about efficiency？For several reasons， one， of course。

 as I'm sure we'll talk about throughout this class， is about the thing in our smartphones。

 battery usage。And this is the big one community， I think。

I think it could be attributed to the big breakthroughs in machine learning recently in the last decade is the。

 you know， compute is important。 Alm development is important。But it's the community of nerds。

 global。 This is global artificial intelligence。 And I will show in several ways why global is essential here is。

 is tens of。😊，Hundreds of thousands， millions of programmers， mechanical engineers。Building robots。

 building intelligence systems， building machine learning algorithms。

 the exciting nature of the growth of the community perhaps is the key for the future to unlocking the power of machine learning。

 So this is just one example。 Github is a repository for code。

 And this is showing on the Y axis at the bottom is 2008 when GiHub first opened this is going up to 2012 quick。

 near exponential growth of the number of users participating and the number of repositories。

 So these are standalone， unique projects that are being hosted on Github。

So this is one example I'll show you about this competition that we're recently running。

 and then I'll challenge people here to participate in this competition if you dare。

 so this is a chance for you to build a neural network in your browser so you can do this on your phone later tonight。

 of course。On your phone， you can specify various parameters of the neural network。

 specify different numbers of layers and the depth， the depth of the network。

 the number of neurons in the network， the type of layers。 and it's， it's pretty self explanatory。

 super easy。In terms of just tweaking little things and remember。

 machine learning to a large part is an art at this point。

 it's more perhaps than even more than a well understood theoretically bounded science。

 which is one of the challenges， but it's also an opportunity。Deep traffic is a chance。

 so we've all been stuck in traffic。 there you go， Americans spend 8 billion hours stuck in traffic every year。

 that's our pitch for this competition。So a deep neural network can help and so you have a neural network that drives that little car with an MIT logo。

 red one on this highway and tries to weave in and out of traffic to get to his destination。

And trying to achieve a speed of 80 miles an hour， which is the speed limit。

 which is this physical speed limit of the car。 Of course。

 the actual speed limit of the road is 65 miles an hour， but we don't care about that。

 We just want to get to work as quickly as possible at home。 So what。

The basic structure this game is。 And I want to explain this game a little bit and then tell you how incredibly popular it's gotten and how incredibly。

Powerful， the networks that people have built from all all over the world。

 the community that's built on this over a single month is incredible。

 And this happens for thousands of projects out there now。Another challenging opportunity。Okay。

 so you may have seen this。 This is kind of ethics。

 most engineers most I personally don't like I love love philosophy。

 but this kind of construction of ethics that's often presented here。

 is one that is not usually concerned to engineering。

 So what is this question know when you have a car and you have a bunch of pedestrians do you hit the larger group of pedestrians or the smaller group of pedestrians do you avoid the group of pedestrians put yourself into danger。

 these kinds of ethical questions of an intelligence system。

 It's a very interesting question it's one that we can debate and there's really no good answer quite honestly。

 but it's a problem that both humans and machines struggle with。

 And so it's not interesting on the engineering side。

 We're interested with problems that we can solve on the engineering side。

 So the kind of problem that I'm obsessed with and very interested in is the real worldorl problem of controlling a vehicle through this space So it happens in a few seconds here。

 So is。A Manhattan， New York intersection， right？This is pedestrians walking perfectly legally。

 I think they have a green light。 Of course there's a lot of jawalking too as well。 Well。

 this car just flat it's not part of the point， but yes， exactly there's an ambulance。

 And so there's another car that starts making a left turn in a little bit。

Me have missed it hopefully not。 So， yeah， And then there's another car after that， too。

 that just illustrates when you design an algorithm that's supposed to move through the space。

 like watch this car， the aggression it shows。 now。

 this isn't a trivial example for those that try to build robots。 This is， this is the real question。

 is how do you design a system。That's able。 So you have。

 you have to think you have to put reward functions， objective functions。

 utility functions under which it performs the planning。 So a car like that has。

Several thousand candidate trajectories you can take through that intersection。

 It can take a trajectory where it speeds up to 60 miles an hour and doesn't stop and just swves and hits everything。

 Okay， that's a bad trajectory， right Then there's a trajectory。

 which most companies take which most the Google sell driving car and every company that's is concerned about PR is whenever there's any kind of obstacle any kind of risk that's at all reasonable that you can maybe even touch an obstacle。

 then you're not going to take that trajectory。 So if that means is you're going to navigate through this intersection at 10 miles an hour and you let people abuse you but youre walking in front of you because they know you're not going to stop And so in the middle there is hundreds。

 thousands of trajectories that are ethically questionable in the sense that you're putting other human beings at risk in order to safely and successfully navigate through intersection and the design of those objective functions is the kind of question you have to ask for intelligence systems for cars。

 there's no grandma。And a few children you have to choose who gets to die， very difficult problems。

 of course， but the problem of what I'm very interested in in streets of Boston。

 streets of New York is how to gently nudge yourself through a crowd of pedestrians in the way we all actually do when we drive in New York。

In order to be able to safely navigate these environments and these questions come up in healthcare。

 these questions come up in factory， in robots in armed and humanoid robots that operate with other human beings。

And that's one of the big challenges。Another sort of fun illustration。

That folks that open the eye use often to illustrate， well， let me just pause for a second。

 the gamified version of this， there's a game called Co runners and you're racing against other boats along this track and your job is there's your score here at the bottom left。

 number of laps， your time and you're trying to get to the destination as quickly as possible while also collecting funky little things like these green。

These green little things along the way。Okay， so what they've done is the build end system， the one。

 the general purpose one that we talked about last time that learns oops。

 that learns how to navigate successfully through the space。So you're trying to maximize the reward。

And what this boat learns to do is instead of finishing the race。It learns to find。

A loop where it can keep going around and around， collecting those green dots。

And it learns the fact that they regenerate with time。So it learns to maximize this score。

By going around and around。Now these are the kinds of things。

 this is the big challenge of reward functions of designing systems。

 of designing what you want your system to achieve is not only is it difficult to the ethical questions are difficult。

 but just avoiding the pitfalls of local Opima of it figuring out something really good that happens in the short term。

 the greedy， whether those psychology experiments of the kid eat the marshmallow and can't wait for you can't delay gratification。

 this kind of the idea of delay gratification in the case of designing intelligence systems is a huge actual serious problem and this is a good illustration of that。

So。We flew through a few concepts here。 is there any is there any questions about some of the compute and the algorithm side we talked about today So the question was。

 yeah you highlighted some of the limitations of machine computer vision algorithm。

 machine learning algorithms， but you haven't highlighted some of the limitations of human beings and if you put those in a column and you compare those is our machines doing better overall。

 or is there any kind of way to compare those I mean there is actually interesting work on imagenet。

 So imagenet is this categorization to ask of what you have to classify images and you can ask the question when I present you images of cats and dogs where are machines better than humans and when are they not so you can compare when machines do better。

 what are the fail points and what are the fail points for humans and there's a lot of interesting visual perception questions there。

 But I think overall， it's certainly true that machines fail differently than human beings but in order to。

Make an artificial intelligence system that's usable and could make you a lot of money。

And people would want to use， it has to be better for that particular task in every single way。

In order， in order for you to want to use the system， it has to be。

 it has to be superior to human performance and usually far superior to human performance。

 So so it's on the philosophical level， It's an interesting thing to compare。 What are we good at。

 what or not。 But if you're using。Amazon Echo， your voice recognition or any kind of natural language chatbots or a car。

 you're not going to be， well， this car is not so good with pedestrians。

 but I appreciate the fact they can stay in the lane。Fortunately。

 you have a very high standard for every single thing that you're good at and it has to be superior to that。

 I think maybe that's unfair to the robots。I'm more of the nerd that makes the technology happen。

 but it's certainly on the self driving car aspect。Policy is probably the biggest challenge。

And I don't think there's good answers there。Some of those ethical questions that come up where it feels like so we work a lot with Tesla so I'm driving a Tesla around every day and we're playing around with it and studying human behavior inside Teslas and it seems like there's so much hunger amongst the media to jump on something and it feels like a very shaky PR terrain。

 a very shaky policy terrain。 we're all walking because we have no idea how。

How we coexist with intelligence systems。 And so， and then， of course。

 government is nervous because how do we regulate。This shaky terrain。

 and everybody's nervous and excited。So I'm not sure there's no。

 that's a perfect transition if that's okay。 same kind of question to Jason in a moment。

 Thanks a lot， Le for another great session。