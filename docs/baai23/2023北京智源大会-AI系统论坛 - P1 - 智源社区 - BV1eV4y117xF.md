# 2023åŒ—äº¬æ™ºæºå¤§ä¼š-AIç³»ç»Ÿè®ºå› - P1 - æ™ºæºç¤¾åŒº - BV1eV4y117xF

OKå•Šï¼Œè°¢è°¢å¤§å®¶ã€‚ğŸ˜Šï¼ŒOkayï¼Œ good morningã€‚ Since we have the some invitedï¼Œ very important speaker hereã€‚

 and we have some foreigners hereã€‚ So allow me to give this opening in both the Chinese and Englishã€‚

 because we don't have the automatic translation system for thisã€‚ğŸ˜Šï¼ŒBshopï¼Œ so let me do it by myselfã€‚

å—¯ï¼Œé‚£ä¸ªå‘ƒå¤§æ¨¡å‹æ—¶ä»£å“ˆå‘ƒã€‚ğŸ˜Šï¼Œå½“æˆ‘çŸ¥é“è¦ç¼ºè¿™ä¸ªworkæ—¶ï¼Œæˆ‘ä¸€ç›´åœ¨æƒ³æ˜¯è¯´whats the important thing for the for the large large model and especially how what is the important topic for the AI system in this new very important new trend for the to support the model scaleå¤§æ¨¡å‹æ—¶ä»£åˆ°åº• AI systemåœ¨ç»„ç»‡è¿™æ ·å­çš„æˆ‘ä»¬åº”è¯¥æ”¾ä»€ä¹ˆæ ·çš„topè¿›æ¥èƒ½å¤Ÿè®© aI systemæ›´å¥½æ”¯å¤§æ¨¡å‹çš„å‘å±•é‚£ of i think the very important thing is the performance is the large scale system performance the training the trainingæœ€é‡è¦çš„å…¶å®æ˜¯è¿™ä¸ªæ€ä¹ˆæ ·å¾ˆé«˜èƒ½çš„å»æ”¯æ’‘è¿™ä¸ªè®­ç»ƒç³»ç»Ÿã€‚

And the other thingï¼Œ I thinkï¼Œ isã€‚The what kinds of chipetã€‚

 including the media chipet and including the the other architecture innovation to support the AI systemã€‚

å¦å¤–ä¸€æ–¹é¢æ˜¯åº•å±‚æˆ‘ä»¬æœ‰ä»€ä¹ˆæ ·å­çš„ AIçš„ chipetï¼ŒåŒ…æ‹¬è‹±ä¼Ÿè¾¾ä¸€ä»£ä¸€ä»£çš„æ–°çš„ chipetï¼Œä¹ŸåŒ…æ‹¬æˆ‘ä»¬å¾ˆå¤šæ–°çš„å‘ƒèŠ¯ç‰‡çš„æ¶æ„ã€‚

And so that's why we have a arranged today's workshopã€‚

 including the high performance parallel optimization for deep learning and different kinds of chipet architectureã€‚

 soæ‰€ä»¥ä»Šæˆ‘ä»¬å¾ˆé«˜å…´é‚€è¯·åˆ°äº†ä½ä¸“ã€‚ğŸ˜Šï¼Œä»£è¡¨ç€æˆ‘ä»¬çš„åŒ…æ‹¬pytoï¼ŒåŒ…æ‹¬rayï¼Œä¹ŸåŒ…åŒ…æ‹¬viè¿™äº›çš„é«˜æ€§èƒ½çš„å»ä¼˜åŒ–æˆ‘ä»¬çš„ï¼Œå°¤å…¶æ˜¯è®­ç»ƒçš„è¿™ä¸ªæ¶æ„çš„æ€§èƒ½ã€‚

åŒæ—¶ä¹Ÿé‚€è¯·åˆ°äº†å‘ƒå‡ ä½ä»£è¡¨èŠ¯ç‰‡å‚å•†çš„teco leaderæ¥ä»‹ç»å‘ƒï¼Œåœ¨è¿™æ–¹é¢ä»–ä»¬çš„insightã€‚But in between the our algorithm and the chipã€‚

 very very important is thecompr AIcomprã€‚ä½†æ˜¯å¤§å®¶è¦æ³¨æ„ï¼Œå…¶å®å¯¹æˆ‘ä»¬è¶Šæ¥è¶Šå¤æ‚çš„å¤§æ¨¡å‹çš„ç®—æ³•å’Œä¸‹é¢è¶Šæ¥è¶Šè®¾è®¡çš„å¾ˆé«˜æ€§èƒ½çš„æˆ‘ä»¬çš„ç¡¬ä»¶çš„å‘ƒè¿™ä¸ªGPUæˆ–è€…çš„çš„å‘ƒæ¶æ„ã€‚

å¾ˆé‡è¦çš„æ˜¯ä¸­é—´çš„AIçš„ç¼–è¯‘å™¨å•Šã€‚So todayï¼Œ I also are very glad that we can invite a veryã€‚ğŸ˜Šï¼ŒVery typicalã€‚

 very very famous compiler in AI including the P A P AI compiler and including the MO IR and also the the other maybe the next next generation aI compiler and also the some research for how to the things for the compilerä»Šå¤©ä¹Ÿå¾ˆé«˜å…´åœ¨ AI compilerè¿™ä¸ªé‡è¦çš„ä¸€ä¸ªé¢†åŸŸæˆ‘ä»¬ä¹Ÿé‚€è¯·åˆ°äº†å¾ˆæœ‰ä»£è¡¨æ€§çš„åŒ…æ‹¬ touchçš„ã€‚

ğŸ˜Šï¼Œç¼–è¯‘å™¨çš„ leaderåŒ…æ‹¬ MLIRå•Šå‘ƒ gogleå•Š from gogle andè¿˜æœ‰å¯èƒ½æ˜¯æœªæ¥æˆ‘ä»¬å†å»è€ƒè™‘ä¸‹ä¸€ä»£çš„ aIç¼–è¯‘å™¨åŒ…æ‹¬æˆ‘ä»¬æ¥è‡ªäºè®¡ç®—æ‰€ä¸­è®¡ç®—æ‰€ä»£è¡¨çš„æ€ä¹ˆç”¨ automaticæ–¹æ³•æ¥ä¼˜åŒ– AIç¼–è¯‘å™¨O so today I don't want to spend a much long any longer time and just want to invite this speaker and because we have very very tight schedule and because so many so many so many things for the AI system todayã€‚

æ‰€ä»¥å—¯é‚£æˆ‘ä»¬å°±ä¸å†èŠ±å…¶ä»–æ—¶é—´äº†ã€‚æ‰€ä»¥OK so let meå•Š introducetrod the firstã€‚ğŸ˜Šï¼ŒBpeã€‚Alber Cohenï¼Œ yeahã€‚

 let me have a introduction for himã€‚ Albert is a research scientist at Googleã€‚

 and he graduated from the University of Veiaã€‚ğŸ˜Šï¼ŒSorryï¼Œ I don't know if I pronounce it correctlyã€‚

 no matterã€‚ And actuallyï¼Œ hes Iï¼Œ I think more thing more he's now work for Google and a lot of things related to the M O I Rã€‚

 Andï¼Œ but he's the very active guideã€‚ a lot of research and work for the G C beforeã€‚ Yeahï¼Œ actuallyã€‚

 he is the very famous person in the compiler areaã€‚ So that's welcomeã€‚ğŸ˜Šï¼Œè°¢è°¢ã€‚ğŸ˜Šã€‚

Thank you for the very introduction and invitation I'm very excited to be hereã€‚

 So I try to speak not too fastï¼Œ but I have natural tendency to speak very fastã€‚

 So we are talking about large scale models language models in particularã€‚

 but I will talk about small things like mostly by making the most I meanã€‚

 more like in the bottom of fashionã€‚ So if you want to get the best possible performance at large scaleã€‚

 you have to start small from like the instructions at low level vectorization and the caches and optimizations for the that are very hardware specificã€‚

 So that's what I'm talking about mostly today about andã€‚ğŸ˜Šã€‚

It's not in my background as you already mentioned Yhu bit I've been working in compilers for more than 20 yearsã€‚

 most likely you are aware of some of the work in the parallelizing and so-called polyheral compiler context if you have worked in the area but I don't want to to talk much about this today we are not into looking for parallel loops into legacy code anymore we have the chance that because of machine learning workloads parallelism is all over the place the question is how do you make good use of it how do you exploit all these degrees of freedom all these domainspec languages for ML and to extract the information you have put it into good use for reaching the best possible performance and in this space I've been working both on applying machine learning to compilers likeã€‚

15 years agoï¼Œ and then I had a breakï¼Œ work on other things and then came back to using that againã€‚

 ML in compilersï¼Œ more like new type of MLï¼Œ like deep learningã€‚

 and also building compilers for ML more recently originally at InRa and Facebookã€‚

 and then I joined Google more than four years agoã€‚So that's a quick introductionã€‚

 So why MLl and compilerï¼Œ So you already say itï¼Œ but if you want to get the best possible performance out of any like domainã€‚

 you need some kind of compiler that will assist the translation of the abstraction of that domain to get the best possible performance of the hardware so let me give some concrete extremes or examplesã€‚

 Some people care mostly aboutã€‚Some people mostly care about peak performanceã€‚

 like most of the people working on language models have to rely on some form of operators optimizedã€‚

P you optimize for given a piece of hardware and other people mostly care about performance portabilityã€‚

 meaning that you want for exampleï¼Œ the model developed on the data center to be easily portable to a mobile device and I took some kind of extremes here like the Waafer scale process of from cebras and edge TpU from Google there are many of course equivalent chips for mobile or edge devices So how do you get both typically how do you get the peak performancements you can for given chip and also how do you make it easier for programmers MLl programmers to port their models across all these different scales So that's what compilers are going to help you do and so this is tough because of this hardware diversity all these different scales but also on the software side things are not very nice although it's improving a little bit so there are many MLl frameworks of course tensorflow jackx by tor many other frameworks and also the model size and complexityã€‚

ğŸ˜Šï¼Œis really increasing very quickly so maybe some of you attended the presentation of my colleague Yan Chi yesterday on a mixture of expertsã€‚

 so the complexity of the models is growing with this typically the size of the data setsã€‚

 the multimodalities and all the optimizations you may want to implementã€‚ğŸ˜Šã€‚

Make the models more and more control intensiveï¼Œ so it's not only matrix productã€‚

 but at the bottom you still have like matrix product anywayã€‚

 but the complexity is increasing and that's really big difficulty for ML compilers as wellã€‚

So let's take a step back and try to understand what ML compilation really means in practiceã€‚

 So the state of the art isã€‚As any compiler would look like so you have a high level languageã€‚

 in this case it's a domain specific language of standard operationsï¼Œ for exampleã€‚

 a Python embedded the domain specific language like Pytch or JX and you have some methods to analyze graphs of some operation of these operations apply high level transformations like automatic differentiationã€‚

 different ways of polylingï¼Œ etcï¼Œ and then there's some kind of magic thing underneath that's called a tensor compiler that will translate these graphs into efficient code that runs on CPUUsã€‚

 GPUsï¼Œ CPUUs all kinds of hardwareã€‚So that's a very rosyï¼Œ very beautiful pictureã€‚

 but in practice it's not quite really that niceï¼Œ it's not that glorious as I prefer to sayã€‚

 on some hardware compilers are really doing a big part of the work like on Google TPUsã€‚

 there is no real like independent libraryã€‚ğŸ˜Šï¼ŒBasically nobody can program it outside using the X compiler for TUs on GPUs from NviDdiaã€‚

 it's quite differentï¼Œ so you have lots of NviDdia engineers writing library code like coDNN that is actually doing most of the efforts if you're running like transformer models most of the time is spent on coDNN operators and heavily optimized attention layer for exampleã€‚

 same thing for CPUUs and most hardware companies are spending quite some time writing hand optimized libraries so there's some level of automation usually but it's eventually very hand optimized code So what's the role of the compiler in this space if you're using libraries eventually so that's a real question and I'm kind of anticipating the next talk I'm sure that Penggu will tell you that compilers are great butã€‚

Typicallyï¼Œ most ML programmers will see it as a bonusã€‚

 so if you can compile and get better performance with legal defaultã€‚

 it's great I'm not going to waste your talkã€‚ğŸ˜Šï¼ŒAnd so why is it a problem actually there are many reasons one reason is that it's difficult to implement fast neural network layers by handã€‚

 of courseï¼Œ as you can imagineï¼Œ it's most practitioners don't know how to do that you need to be a high performance computing expert you need to know the platform very well and the benefits of doing that can be huge so if you write na code it can be10 times smaller than sorry slower10 times slower than like peak performance and so basically anytime you do research on a new model or trying to tweak an attention layer bit or conversion layers a bit doing some kind of custom fusionã€‚

 you have to redo thatï¼Œ you have to implement again the lowle operators on your machine and that's very inefficient maybe some of you heard about the Triton project and language from open AI TVM or a library in that space and language in that space that'sã€‚

Succesfulï¼Œ so there is a lot of need for writing these kind of custom operations and optimizing themã€‚

 but it's still very tedious and you would rather not go back to writing lowleve code if you had a good compiler for it so what about ML compilers today on this picture that was this ML compiler are still there for a reason and the main task that ML compilers are very successful at dealing with today is applying high level transformations so like automatic differentiation I mentioned so I'm mostly looking into JX in this example but also high level distributed computation automation vectorization and of course figuring out at which level you want to apply just in time compilation so if you're able to I'm actually hiding in the slide I remember if you can actually control at what function graularity you want to apply a Giit compiler like XLA in the case of JX you would typically do that with this type of low level sorry very high levelã€‚

Abstractionsã€‚ so that' that's great benefit because you don't have to think as an MLl programmer in terms of low level optimizationsã€‚

 everything is happening behind the scenes in these high levell transforms like Autod and in the Gi compilerã€‚

 The problem is that this is still very high levelã€‚ You're only doing like tracing the computationã€‚

 building syntax trees optimizing like building graphs recognizing graphs automatically and maybe iteratively trace and transform those graphs until you get to some cluster of operations that you can optimize togetherã€‚

 and most of the magic is still happening down thereã€‚ if you want good performanceã€‚

 you need something to actually deliver good performance on the targetã€‚

 And this is not this big cycle of tracing and transformation that will do itã€‚

 you need a library or you need the compiler underneathã€‚ And that's what I'm focusing on hereã€‚ğŸ˜Šã€‚

So the type of benefits you can get from compilation are typically there are numerous but the main ones are getting rid of interpretation overheadã€‚

 that's the most likely purpose of using a compiler way that you don't want to dynamically interpret and dispatch the logic for different operationsã€‚

 you can also use this for fusing operations the memory hierarchy like the cachesã€‚

 local memories have to be leverage in a way and that's typically through fusion that you can do that so avoiding roundtris to main memory or across device and host interfacesã€‚

 you can also do a lot of optimizations at that level by looptyling unrolling vectorization etctera that will be necessary to leverage specific hardware and it's also very important for reducing the exable sizeã€‚

 the code sizeï¼Œ especially on embedded mobile devicesã€‚

 you can specialize the code to one very specific modelï¼Œ unlike libraries which haveã€‚

Provide operations for every possible modelï¼Œ basicallyã€‚ So it's also used embedded space for thatã€‚

 and also a very important thingã€‚ you can do auto tuningã€‚ So with librariesã€‚

 you have to think ahead of all the possible shapesã€‚

 all the possible operations context like layoutsï¼Œ etc ceterã€‚ When you are compilingã€‚

 you can specialize on one particular shape and layout and the context in which an operation executesã€‚

 So that also gives rise to interesting automatic tuning techniquesã€‚ But okayã€‚

 I have to be very humble here because I've been working in the area for 20 years advertising for these great benefitsã€‚

 but we are still thereã€‚ I meanï¼Œ are we making any progress hereã€‚ğŸ˜Šã€‚

And basically the like ML now is everything that what that's what paying the billã€‚

 but the problem still is still thereï¼Œ although we live in a more specialized worldã€‚

 So we have to we have to think twiceã€‚ and I want to briefly advertise these auto tuning efforts from some colleaguesã€‚

ğŸ˜Šï¼ŒPresented at a PA conference a couple of years agoã€‚

 if you want to learn more about auto tuunning because this is a big part of our jobã€‚

 this is one great example of things happening in the space of XLA for Tpusã€‚

 but we are also doing that for other targets just advertising for this piece of work led by my coding Wangpo and more generally if we want to make progress actually I'm coming here not to tell you how to do it I'm mostly coming here to ask for help and collaboration because after 20 years we are still there and so I'm going to ask a few questions like through some proposals in the rest of the presentationã€‚

 So my first claim is I think we need to move from compiling for M into more and more M using completionilã€‚

 So of course this is not new I worked on it like 15 years ago there was work before thatã€‚

 but it's still very much immatureã€‚ So there are not so many production compilers that actually use MLl techniques a little bit of auto tuning offlineã€‚

 but not so much when you are actually actively compiling codeã€‚ğŸ˜Šã€‚

The main reason is that it has been of course a difficult problem to optimize like and be complete problems or like schedulingã€‚

 register allocationï¼Œ et ceteraï¼Œ foreverï¼Œ but now we are dealing with very hardware specific optimizations that are even hard to model correctlyã€‚

 so we don't have a cost model we don't have an analytical means to reason about performanceã€‚

Typicallyï¼Œ the only hope is that the machine itself can learn what is goodï¼Œ what is profitableã€‚

 what has to be optimized for given operational graph operationsã€‚

 And one approach to do that is I would call it controllabilityã€‚

 So you want the compilers to be more open in the sense that there are more gray boxesã€‚

 So you want to intervene as an expert performance engineer into how the compiler generates codeã€‚

 So it's the exact opposite of what I was showing at the beginningã€‚

 where you have a black box very abstractã€‚ You said jet and it's magicalã€‚ In this caseã€‚

 it's not magical at allã€‚ you want to intervene and say I know how to optimize this particular operationã€‚

 Please do this for me automatically because otherwise I have to write triton code or could code or something So scheduling languages that have been popular for reason But on top of thatã€‚

 you still would like to automate those schedules So you want to infer basically you want to search basically what's the best possible schedule for a given machine and given shape etc ceterã€‚

 And you want to also automatically build the schedules for you using machine learning techniquesã€‚ğŸ˜Šã€‚

So that's I think one very important thing and schedulesã€‚

 there are actually many different types ofs or scheduling languages in hyperfa computing actually even started even earlierã€‚

 like in the early 90s there were some ideas or in that space in polyhedral compilers I worked on some of these techniques in like 15 years ago with language that we designed also with Nikcolola Lavaski at Facebook we had these tensor comprehensions work that also had schedules and now Heide and ProTVM you know are very popular in that space basically Heide change the rules of the game completely making these scheduling languages much nicer to use and much more embedded into domain languages so it's not new there are lot of work at NVDdia as well etctera in that space butã€‚

And oh yeahï¼Œ that's the one I was referring to 30 years back we already have this type of firstã€‚

 but I'm not expecting you to read that So why schedules the main reason people want schedulesã€‚

 as I saidï¼Œ is performance engineers want to be able to control what the compilers do because if they cannot they will never get the peak performance you want some kind of way to reuse the existing logic in the compiler without having to completely forgo the compiler and write lowlevel code code on a GPU and one typical example is when you have a clear optimization methodology in mind so in this case it's focusing on matrix product and conversions is the bliss methodology that is essentially sketching how product matrix multiplications are implemented on any acc or even CPUs these days and you have to decompose your computation into layers of tiling and at the bottom you have like some very optimized vectorized unroll code that will be very specific to your machine and actually I figured itã€‚

top down way but the real way these methodologies are implemented is more like bottom up so you first write microchans of heavily optimized unroll loops and vectorized and vector instructions and then you slowly grow more like unclosing loops around these microcans to reflect the memory hierarchy and the partism hiarchy of the machine and so this is this b methodology that is quite popular we did some work in the area as well and realizing that in fact we can build search spaces we can build scheduling languages that have these kind of methodologies captured in a more specific way into the scheduling language and into the search base so that was a paper that I mean the tool was called Tit but it's not in the title that was joint work with In and colleagues from University of Utah in the US and we essentially showed that there are ways to automate the process a bit more than using TVM for example and I'm not going into details but in whichã€‚

TheHere is thatã€‚You can actually have more than one microcannel if you look at like one DNN library of Intelã€‚

 the ML library of Intel on x86ï¼Œ you essentially have three microcannels written in assembly language if you look at coDNN or coubs and NDIS it's about 20 but you can have more than one microcannel that have to be generated automatically if you want to some level of portability or in the worst case you write them by handã€‚

 but you'd rather not do that but you select the best microcannels in this case you have this kind of cr shape of microcannels depending on the unrolling degree on the size basically of the microcannel and on top of that you can grow the rest of the stack completely automatically so I don't have time to go into details but let me skip that the scheduling language can be extremely simple in that case so you don't have to write very complex sequences of transformations you can only say I have like five basic primitives for organizing the computation above the microcannel I can decide on loopã€‚

factors I can decide on what to vectorize like which dimension is it like row column et cetera vectorizationã€‚

 I have to figure out ti sizes like block sizesï¼Œ if you're on a GPUï¼Œ how many threadsã€‚

 how many blocks etc cetera you're going to have also and the rest is so you can yeah you have to generate the rest of the control flow this is this R construct and you have this fancy lambda thing at the bottom that is a way of sequencing channelss or microcanals so you need to accommodate for one very painful thing in hypophone computing which is thatã€‚

ğŸ˜Šï¼ŒAll the dataï¼Œ all the sizesï¼Œ all the shapes are not powers of two in practiceã€‚

 I mean actually in transformers more and more it's like powers of twoã€‚

 but on convols is definitely not powers of twoï¼Œ a very fancy thing like 34 or 17 or something like that not sometimes prime numbers even so you have to be able to compose multiple microcans until you reach the proper size so I'm sorry I'm going fast I'm not explaining those things in detail but but it makes the search space much easier to traverse at much smaller when you have such very specialized scheduling language and in this case in this graph we are comparing the TVM auto scheduler called Ansã€‚

 which is the blue bass so blue graphs so TVM works by chunks of 64 runs like empiricalical evaluations on the machine and every 64 runs so this is log scale at the bottom but this is 64ã€‚

ğŸ˜Šï¼ŒIt will try to select using cost model that is being learnedï¼Œ the scenarioã€‚

 best possible optimization using TVM schedulesï¼Œ and if you use our schedules which are more specialized and compressed you can you can very quickly converge like maybe 10 times less runs or even less than thatã€‚

Just a demonstration of working on the scheduling language helps if you want to reach the automatically the optimal solutionã€‚

So I like to make one proposal here and this is really again calling for help okay we see that there is some room for automation of schedules and writing more domain specific schedulesã€‚

 but there's still one problem that we are facing it's going beyond those static shape like specialization so when you know everything about the layout the shape it's great you can do this completely automatic search offline and you get the best possible implementation for one very specific operation but in practice you still like to bridge the gap with what libraries offer like coulddNN coubla etc they provide you with and one stop shop basically for all possible shapes and it's kind of frustrating that you have this libraries that have very generic on one side and these compilers that do automatic tuning for just one possible shape So I call it TLO tie level operations so Google doesn't have anything there we're just asking questions but we'd like to devise some kind of interface where instead of thinking ofã€‚

ğŸ˜Šï¼ŒThe high level library of operations that are essentially the ones you would write in Numpy or in JX or whatever MLl frameworkã€‚

 you have a library of operations that is automatically synthesized for given model and given hardware at the T levelã€‚

 so I would argue that the compiler is extremely usefulã€‚

As this bli methodology shows to generate optimal code at the tile levelã€‚

 like maybe on a block on the GPU or any code that runs on the L1 on or L2 cache size on the CPUã€‚

 But you don't really need the compiler to orchestrate all the computation at the level of the full node it quite intuitive that you will have some kind of runtime systemã€‚

 some kind of dispatch logic at some point that is going to be interpretedã€‚

 If you're close to the instructionsï¼Œ you need to compileã€‚ Otherwiseï¼Œ it's too expensiveã€‚

 But if you add the level of like full operationsï¼Œ you don't really need a compilerã€‚

 The dynamic dispatch can be fast enoughã€‚ So there is this kind of boundary between tile level operations that are compiled underneathã€‚

 And above that boundaryï¼Œ you have interpret that can quickly dispatchã€‚

Across these different operationsï¼Œ the challenge here is that you don't want those operations to be decided by an expertã€‚

 you want those operations tile level operations to be automatically synthesized for a given domainã€‚

 and I call it like a 2D paros surface so you want to trade basically preference for code size for specialization on a given domain So if you know exactly you're only going to run like large language models with this particular shapes it's no need for optimizing for like LSTMs you're only optimized for that particular shape and model architectureã€‚

But still I think the problem is reasonably well definedfinã€‚ there is no solution to do that yetã€‚

 I mean a couple of papers are coming close but if you're interestedï¼Œ I can mention them offlineã€‚

 so briefly about MLAA since you mentioned MLA earlier so we are building infrastructure to address these issues likes scheduling compiler construction more portable reusable etc using the MLAya framework so I'm not going to give you a course on MLya we don't have time and there are plenty of resources online and maybe of you already know about itã€‚

 but it's MLya has been designed for extensibilityï¼Œ so basically it doesn't do anythingã€‚

 but it's great framework for implementing compiler flows and also bridging frameworks for exampleã€‚

 you may be aware of something called stablehel which is now being used by many frameworks including Pytch to communicate with with XLA and there is this recent open ExcelLA infrastructure initiative that is endorse by many companies which is not governed by Google anymore so it is a separate organizationã€‚

Of course Google was there to start it upï¼Œ but it's a community really like LVM like MLR to build this next generation portable compilers for ML and so I'm skipping MLAR examples sorry just mostly to focus on one aspect of MLR which is extensibility but in extensibility in a very specific way about how do we build schedules in an extensible wayã€‚

 I told you schedules are very important for performanceï¼Œ very important research area as wellã€‚

 but we don't want to start from scratchï¼Œ we don't want to implement a new scheduling infrastructure like TVVM from scratch and using MLR what we can do is that we can start from the baseline optimization flow that takesã€‚

The intermediate representation of the graphs that you are optimizing some like handcoded optimization and uristics and generate code and you can extend it with the same syntax and the same logic and abstractions of MLAI with another IR which represents the transformations this time so for exampleã€‚

 if you want to teach the compiler to do fusion of matrix product and softmax for attention layers you can explain those the logic of this fusion in terms of rewriting rules in terms of patterns in the AI itself and you can drive this this transform AR with either by an expert writing schedule by hand so it's very painful because it's an MLI logic but on syntax but you can of course use the DSL on top of it but so you can write this IR directly saying I want specifically to fuse this matrix product with this softmax or you can also embed a database of transformations that have that can be search automatically so you can add so some AI around itã€‚

Some auto tuning logic that will automatically schedule this transformationã€‚

 So you can make every thingï¼Œ every component I was showing earlier with schedules and automatic schedulingã€‚

As an extenible component of MLya and so this is something we are working on so there are also some extralogã€‚

 maybe a little bit of C++ you have to provide for some transformationsã€‚

 but most of the time you don't have to do itã€‚ and so if you're interested in this approach So MLya can help you do that like building your own scheduling languagesã€‚

 your own custom TVM if you want we call it the transform dialect and there was a great tutorial given by my colleague Alex X code with the main actually contributor of that project at the MLya at the LLVm sorry definitely last month so you can look it up and there is also a paper that was published recently So I don't have time to go into more detail I can conclude by just saying again we have problems to solve some of them we know well some of them have no clue so we are very interested in collaboration if you are in this room you already know that so that ML is not just about data models and compute but it's also about compilers otherwise ways you will not be in this roomã€‚

And it's also just the beginning so we have been working for 20 years and actually many more years on this type of hypo computing techniquesã€‚

 but now in the field of ML we have much more advanced ML techniques to use as well so we should really bring those into the compilers as well thanks for your timeã€‚

ğŸ˜Šã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_1.png)

Okayã€‚Okayï¼Œ okayï¼Œ thank youã€‚Yeahï¼Œ so because the very tight scheduleã€‚

 So I didn't arranged any Q a for this for today's any speakerã€‚

 but welcomed for any of your questions of lineã€‚ Yeahï¼Œ so next I want to introduce Peng Wuã€‚ Yeahã€‚

 actuallyï¼Œ the one special thing I want to mentionedã€‚

 and I don't know how many of you have watch the the launch of pie touch 2 dot0 that event actually I online to watch that and right after the touch2 dot0 and the first invited speaker at the some important tackle announcement the first one is Peng Wu andã€‚

ğŸ˜Šï¼ŒAnd she announced the compiler dynamo for Py 2 0ã€‚ And at that timeï¼Œ I thinkï¼Œ wowã€‚

 Py have the compiler nowã€‚ So that's to be something the special or beginning things for Pã€‚ Yeahã€‚

 So Peng actuallyï¼Œ she have work for IBM research more than 10 yearsã€‚ Actuallyã€‚

 I also work for IBM research for the system for more than 18 years as wellã€‚ğŸ˜Šã€‚

And and she later was for I for Huawei and established Im the programmingã€‚ğŸ˜Šï¼ŒLbï¼Œ yeahã€‚

 and work for Huawei for seven yearsã€‚7 yearsã€‚ And then she joined the matter and let the compiler part for matter for three yearsã€‚

 Okayï¼Œ let's welcome Pg yeahã€‚ğŸ˜Šï¼ŒAlrightã€‚Okayï¼Œ thanks for the introductionã€‚

 You might notice on this agendaã€‚ we have2 P towards 2ã€‚0ã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_3.png)

Talksï¼Œ I didn't realize thats the second oneï¼Œ but I want to qualify this oneã€‚

 So this one is focusing on how we are bringing compiler to the core of Pytorchã€‚ And then laterã€‚

 the next talkï¼Œ Michael is going to focus more on how Pytor 2ã€‚

0 is helping the large language model trainingã€‚So you've already heard one compiler talk and I saw in the agenda there are moreã€‚

 So before I startï¼Œ I want to give you a mental model of like not all compile M compilers are the sameã€‚

 So one basic way to differentiate is in my mental model is to differentiate betweenã€‚

Machine learning framework compiler and machine learning Acccelerator or hardware compiler soã€‚

Albas talkg is more coming from the bottom from like closer to the hardwareã€‚

 And because our compiler is called Pytorch compilerã€‚ So we are more coming from the topã€‚

 and the thinking is very different because as a framework compilerã€‚

 what we are focusing on is how can we enable all these hardware vendors to bring Pytorch models to their hardwareã€‚

 So there are things we do we have to do and there are things that we want the vendors to doã€‚

 So keep that question in mind on the thinking behind machine learning framework compilerã€‚

So today I'm going to tell a storyã€‚ I know there are many people are building their own chips or even their own machine learning frameworksã€‚

 I hope at the end of talkï¼Œ you could get a glimpse of the thinking of how we are burning compiler to Pytor 2ã€‚

0ï¼Œ not for the interests of research ideasï¼Œ really for making real models run fasterã€‚Alrightã€‚

 so this is the release note Earl this yearï¼Œ March 15ï¼Œ 2023ã€‚

 This is where we actually release the Pytorwar 2ã€‚0 binaryã€‚

 and it says that we want to offer the same eager mode development and user experience while fundamentally supercharge how Pytor operates at the compiler level under the hoodã€‚

 So let me just simplify the messageã€‚ I want you to remember two wordsã€‚Graph mode and ease of useã€‚

So it was long believed by the industry that you can either have graph mode or ease of useã€‚

 but you cannot have bothã€‚ And in this Ptor 2ã€‚0 storyï¼Œ I'm gonna challenge that conventional wisdomã€‚

 So essentiallyï¼Œ I'm gonna show you how do we have the cake and eat itï¼Œ tooã€‚But before I get to 2ã€‚0ã€‚

 let'sã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_5.png)

Roow back  five years to 1ã€‚0ã€‚ So Pytorch 1ã€‚0 is was announced almost exactly five years ago 2018 at the timeã€‚

 almost all the major industry backed machine learning frameworks or choosing graph mode because it was believed it's rightly believe so that with graph modeã€‚

 you would have the ability to optimize using compiler and the performance leading would be a lot a higherã€‚

 if you optimize one at the timeã€‚ So Pytorch is actually was actually a late comeer in terms of machine learning frameworksã€‚

 And at the timeï¼Œ we made a criticalï¼Œ also risk a decision toã€‚

Value ease of use above everything elseã€‚ The thinking is that we want to cater to machine learning researchersã€‚

 and for researchersï¼Œ they actually value more time to market than performance initiallyã€‚

But with that work we actually don't know so what we did is actually we are one of the few probably just go all in with non graph mode and then we compensate on the performanceman side by working very closely with vendors to optimize at the library level and optimize whatever we could within the constraints of eager mode executionã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_7.png)

Soã€‚The data showed that this bet paid offã€‚ so since the 1ã€‚0 announcement at the timeã€‚

 we were about 30% of researcher adoptionã€‚ and as you can seeã€‚

 the curve is going up and around two and a half years marked after the 1ã€‚0 releaseã€‚

 we have passed the 50% mark and that essentially meant that were the number one machine learning framework used by machine learning researchersã€‚

And you see a little bit of dip over there so actually I double clicked into that so this is based on same data sourceã€‚

 but rendered in a different wayã€‚ so the middle sessionã€‚

 the red part is a Pytorrch kind of researcher and market share and when you see the dip here is actually more about a few new up and coming machine learning framework and because this is in China so I want to highlight a few homegrown machine learning frameworks there is the mindsbo I believe here which sees a significant increase actually I used to work on the Minsb the accelerator compiler and then there is also the palã€‚

 the purple one let's see and Jas is this green one and then there are like a lot of other things and from this picture you would see that there are a lot of players over there but Pytorrch because of this ease of use is reallyã€‚

Becoming the anchor of a lot of the kind of the framework of choiceã€‚For researchersã€‚Alrightã€‚

 so now let's go to Pytor 2ã€‚0 what this talk would be focusing on one specific featureã€‚

 but a fundamental feature of 2ã€‚0ã€‚ We introduced this very simple API Tor compile as the primary API for introducing graph modeã€‚

Andã€‚What this does is remember ease of use and graph modeã€‚

 So basically what you are when you are writing the modelï¼Œ you're still thinking in terms of eagerã€‚

 So this is very different from the traditional graph mode based MLl framework in those framework you have to think about graph and that's why researchers didn't like thatã€‚

 It's just not intuitive very hard to debug So here you write as if you are in eagerã€‚

 but you do an annotation just one line change to indicate to the compiler this is a part that we want to make it work under graphã€‚

 So it's veryï¼Œ very simple right So does it work and actually in the Pytorrch conference announcement last Decemberã€‚

 This is a data we showed and we're showing over 170 modelsã€‚ Tim is a vision modelã€‚

 torchbech is a bunch of highly popular research models and hugging phase is more like the todayã€‚

Former based models and the results are run on an media A100 and we combine results between AMP and flow 32 with more weights on AMP because that's more performing for training and all these data are for training so these are the geoamine performance speedups that we're reporting at the time and today the number has further increasedã€‚

Alrightï¼Œ soã€‚You might wonder why do I make such a fuss about graph mode Almost all other machine learning frameworks use graph mode right so the problem is that because of Pytorch design for flexibility and expressiveness those good trades actually made it very hard to compile or to introduce graph mode with So if we look back on 1ã€‚

0 and 2ã€‚0 I could say that 1ã€‚0 is about a strategic decision of like who are we catering for and what are we giving up and what are we embracing for So that's 1ã€‚

0 for 2ã€‚0 it's really about actually the emergence of 2ã€‚

0 is really based on tech innovation So the moment we figured out how we can get the graph mode without sacrificing ease of use that's the moment that 2ã€‚

0 becomes realã€‚And that moment is a torch dynamo momentã€‚ Torch dynamo is an out of the boxã€‚

 G capture 4 pi torchã€‚ğŸ˜Šï¼ŒSo if you have followed about Pytorrch compilerï¼Œ I meanã€‚

 people didn't know about Pytorrch compilers maybe twoã€‚

 three years ago because we have too many of those so we have tried many times and our previous graph capture always require significant menu effort when you are trying to work with real modelsã€‚

 So this menu effort ranges from either you have to change your graph or model to make it captureurable or you have to or if you are able to capture a graphã€‚

 you have to make sure it is correct when you replay itã€‚

 So torch scripts scripting effects tracing lazy tensor are all the previous generation graph capture mechanismsã€‚

So how do we solve that problemï¼Œ rightï¼Œ to make the graph capture reliably capturing the correct graphã€‚

Basicallyï¼Œ how do we make the mechanism both sound and alpha boxã€‚So let me give you some intuitionsã€‚

 so the first insight we have is that all the previous graph capture is aiming at capturing one graph model and that's where all the constraints are coming into play where what if there is something that the compiler didn't understand because Python is a very flexible language so what we decided is to allow capturing partial graphs so that whenever we encounter something that we didn't likeã€‚

 will just stop the graph graph break and fall back to eagerã€‚

And for its soundness because part of the graph capture is using tracing mechanismã€‚

 so we introduced guards into the graphsï¼Œ and this would allow us to record what's the valid condition for this graph to be correctã€‚

And in order to use those guards at runtimeï¼Œ we need to check whether the runtime condition satisfy the guard conditionã€‚

 And if it doesn'tï¼Œ then will just in time recaptureã€‚

 So it operates almost like a just in time compilerã€‚

So to give you one example for the previous exampleã€‚

 this one I deliberate introduced one which requires a graph breakã€‚

 so if you print out the graphs to the right hand side of what we captured for this exampleã€‚

 without if you didn't know about all the previous pointsã€‚

 you might be surprised to see actually three graphs that are highlighted by the colored bar over there because there is a data dependent control flow B do sum depending on the value of Bã€‚

 it may or may not go into the if or else branchã€‚And but there are other reasons that we graph breakã€‚

 so for exampleï¼Œ if we have something that's like outside the Python a see extensionã€‚

 we don't know anything about itï¼Œ we will graph break for soundless reasonsã€‚

 and there are some of these conditions actually where over the last couple of months we're gradually reducing the reasons for graph breaksã€‚

 some are implementation limitationsï¼Œ some are just fundamental to the correctness of the programã€‚

I talked about graph breaksï¼Œ but I also want to show like we are supporting this torch dynamicyal graph capture we're really supporting a lot of very complex features of Pythonã€‚

 so here's a list of the complex thingsï¼Œ how do we handle function callã€‚

 how do we handle comprehension and all these container typesï¼Œ loops control flowã€‚

 Lambda and so on and so forthã€‚So I want to give you a glimpse of what the magic under the hood so this picture the left hand side is a default C Python behavior so this is what happens when you use E mode and to the right hand side especially the dotted box here and there those are the new things we added number one it's actually we're very lucky we have a standard python C Python feature that allows us to add these extensions so that means you don't have to download the special version of C Python you just need to use a particular modern version of it and here with this boxes the thing we talked about capturing partial graphs break and then compile the graph into binary and then execute the compile code doing the run-time check going to a cache and recapture all of these things happen transparently behind the scene so that's why users just have to annotate one line of codeã€‚

In order to go into the graph modeã€‚So these things seem to be plausible does it really work we actually run torch dynamo with a very simple backend that just basically fall back to eager to test the robustness of the partial guard graph capture mechanismã€‚

 we run it over 14 kã€‚GitHub Pytorch models with certain star levelã€‚

 So it's not just trivial examplesã€‚ And our pass rate is like 99% aboveã€‚

I didn't talk about integration to Torch Nanoboï¼Œ but actually that's a major design point we really want different backend or accelerate compilers to integrate easily with Torch dmo so I'm going to talk a little bit on the Pytorch native backend were building that's the one here but there are a lot of vendors since the Pytor 2ã€‚

0 announcement and release are engaging with us almost all majorã€‚

Beend compilers are trying to integrate with us and we didn't put the exact number there because it's changing and it also depends on the support levels that will label it as experimental or more more stable onesã€‚

And finallyï¼Œ we already actually show the previous performance memberã€‚

 but what you want to show is that people might question whether partial graph would sacrifice performance for the backend compilerã€‚

 what we observe is that even though we are capturing partial graph the graph is still large enough for a lot of the optimizations to kick in like fusion optimizations and because now we can almost funnel any Pytorch compiler through the graph mode so the widen coverage actually more than compensated on the smaller graph that the graph break is introducingã€‚

 so we do see like quite significant every speedups on a wide side of benchmarksã€‚

Okay so this is torch shineinmo I want to remind people that actually I think of as talk is all about like how do we optimize the actually generate efficient code so keep in mind that Torch dynamo is the breakthrough point for how 2ã€‚

0 exists but by itself capturing graph doesn't improve performance we just enable the backend compiler to connect with Pytorrch models so that magic for improving performance in the P2 stack is torch shineinammo to Kcrs is a Pytorrch native compiler actually I should say a Pytorrch native training compiler because there are a lot of compilers out there many are focusing on inference and there are a lot of compiler out there but fewer are designed for the Pytorrch IR and one of the reasons is because Pytorrch IR is very a Pytorrch offset is very large and complex has mutation all these unpleasant features that are convenient for library developedã€‚

But not convenient for compilerã€‚ So actually aiming for I design specifically for Pythtorch semantics is very importantã€‚

ğŸ˜Šï¼ŒI don't have time to go into the technical details on inductorã€‚

 but I just want to highlight a few things that's unique in Inductor number oneã€‚

 Torch inductor actuallyï¼Œ most of the PD2 stack is written in Python itself and it allows us to move really fast and make the things hackable because nowadays a lot more people know about Python than knowing about COC++ã€‚

And the second part is we started with instead of focusingã€‚

ResNe or a few models demonstrate the valueï¼Œ we actually start with focusing on designing the framework to be general purpose to handle long tail apps firstã€‚

 and this is also very important for handle real modelsã€‚

So that's the breast first approach and the last one is the IR designã€‚

 so I would say that if there's one thing that you want to remember about torch inductor the unique aspect would be the PyTrch IR native design if you're interested in that or extend on torch inductor I would recommend you to look for more information onlineã€‚

So just to give you a mental map of what torteinductor stack looks likeã€‚

 there are many boxes over hereï¼Œ a lot of the compiler researchers might be focusing on the cogen aspect which is the hardest part and what's interesting for us is being practical we actually didn't do the cogen ourselves we actually rely on Triton very early we are a very early adopter of Triton so one message here is like when you are building a practical M compiler stack from top to bottom you actually really want to have the whole thing working like the minimum viable product and there if you can leverage other components if they can do the job you don't have to build it yourself so that's what we adopted it for the GPU cogen we rely on Triton and that turned out to be a veryã€‚

Beenefficial strategic badã€‚ And then we also have a C plus plus backend right nowã€‚

 we're involving the Intel team developing the C plus parts backendï¼Œ especially for the CPUã€‚

So then theyre about already talked about like scheduling fusion so these are what typical machine learning optimizing compilers would doã€‚

 so I wouldn't say anything about itï¼Œ the very unique thing that make everything else work well from PyTtorch is actually our IR design so how do we lower how do we design the IR again don't have time to go into the detailã€‚

 but I would if you were to design your own machine learning compiler to integrate with Pytort I would recommend you to look at how we are designing the Torduct IRã€‚

All rightï¼Œ so I'm almost wrapping up my talk hereï¼Œ but I want you to take away with this picture in mindã€‚

 the left hand side going from eager mode is the 1ã€‚ xã€‚Execution passã€‚ this is eager mode executionã€‚

 basically one operation at a timeã€‚And 2ã€‚0 with the Torchal compile interface is actually completely backward it's an optin interfaceã€‚

 so it's completely backward compatible with 1ã€‚0 and what you will see is that we are trying to funnel the model with the same programming interface into the graph mode which is this part as much as possible and and the key technology in this is actually this part to made it possible because as I mentioned before what made Pytorrch beloved made it really hard to graph captureã€‚

 so once we have this component figured out as an outer box experience is almost like turning on a faucet so the Pytorrch models just flow seamlessly into the graph mode and Torch inductor is the one that's developed in team which is I would say it's the best performing training compiler for Pytorrch todayã€‚

 but I also want you to remember that the role of a machine learningã€‚

Where compiler is not to build everything ourselvesã€‚ And because we do understand like we haveã€‚

Resources to invest in say GPUã€‚ But what about the other acceleratorsã€‚

 So we do want to enable other vendors to hook their own compilers with this partã€‚

 We don't want them to worry about thisã€‚ This is so hard and not very interesting for them to figure outã€‚

 So here there are many other machine learning compilersã€‚

 This figure is a little bit simplified because there are actually different integration point for a machine learning accelerator compiler to interface with Ptorrchã€‚

 You could do it at this level like siblings of torch inductorã€‚

You could actually add backends below toch inductorï¼Œ for exampleã€‚

 the Intel CPU backend is something integration here and increasingly we're seeing people actually integrate at the Triton level so for example we've seen AMDs investing their GPU cogen into Trident so if you are connected with Tridentã€‚

 this thing also flows naturally so I want you to keep in mind the lower in the stack you're integrateã€‚

 the more you can reuse what we build and the less work for you if that integration point is suitable for your hardwareã€‚

All rightï¼Œ soã€‚Let's seeã€‚ Okayï¼Œ so I'm gonna escape hereã€‚

 but what I want to say if I reflect back on 2ã€‚0 I also had research backgroundã€‚

 I think the one thingï¼Œ one theme is we're trying to define machine learning framework compiler that works for real models and we are trying to be veryã€‚

 very flexible that's all we're trying to do and a lot of decisions is to solve the hardest problems first and then try to reuse as much component as possibleã€‚

All rightï¼Œ so what's available now so if today you go to Pytorch Github nightD releasease you would already have a lot of the these are the core technologies that's already built in there and you have Tor compile as the compile mode API or G mode API it's a alpha feature you will have Torch inductor as a default backend for GPU you can use it for both inference and training you have dynamic shape as experimental feature but you have to use it with a flag on there's a lot of progress made on dynamic shapes since March and you can get the weekly updates from the dev development developer forum and we have three performance mode for Tor compile the default reduce overhead is for models where there is a lot of kernel launch so we made it work with Cgraph and other optimizationsã€‚

And Max also tuned this enables auto tuningã€‚And what's coming nextã€‚ So for theã€‚

Maybe before the next Pytororch conferenceï¼Œ which is in October this yearã€‚

 we are still continuing to improve performance because we do believe we are just scratching the surface of what's possible and we are also improving the composably with other Pytororch features for exampleã€‚

 quantization tensor subclassing and so on and so forth in the longer term beyond six months actually we are already doing a lot of work onto PD2 export this is the past where it requires a little of menu effort for you to export a whole model with guards resolved into a sable form and then you can carry this model to whatever runtime you have this is most recommended for if you have like embedded devices or inference in production where Python is not allowedã€‚

And yesï¼Œ we're talking about large models right so so far I have mainly talked about single GPU so now we are also heavily invested in P to distributed and in order to notã€‚

ğŸ˜Šï¼ŒMore than just optimizing for computeï¼Œ We are trying to optimize communication as wellã€‚

And I think Michael probably would talk more about thatã€‚And to concludeï¼Œ I just one messageã€‚

 get involvedï¼Œ actuallyï¼Œ it's a tremendous effort to buildã€‚AMachine learning compiler by yourselfã€‚

 So I would highly recommend if your workload is using Pytorã€‚

 try to integrate at certain points with P2 stackã€‚ It really save you a lot of a lot of the work that's not necessary and there is still a veryã€‚

 very strong momentum for us continue to involve P2 stack so you can And also when you try P2ã€‚

 give us feedbacks if there's anything that's not working or to be the direct compute contributor to P2 stackã€‚

 especially on the compiler sideã€‚All rightï¼Œ thank youã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_9.png)

Okayã€‚Okayï¼Œ thank youï¼Œæœ‹gã€‚Okayï¼Œ so I think we haveï¼Œ yeahï¼Œ we have two topic todayã€‚

 And so that's why I we organize the local deep learning framework topic like aal pal and mind into the session in this afternoon in the other AI open source forum not in this oneã€‚

 Okayï¼Œ then let me introduce Michaelminã€‚ Yeah hes another old friend another old friendã€‚

 He actually work for IBM for a long time and and hes the inventor of the cell processor aelerator sideã€‚

 Yeahï¼Œ and I think some of you my still remember actually is almost the first general purpose the multi processorã€‚

 but with the general purpose call and also the a called to accelerate the workload at that timeã€‚

 including the gaming nameï¼Œ those kind of thingsã€‚ and many yearsã€‚ğŸ˜Šã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_11.png)

After now Michael he is now in matter and leading the acceleration Shen side for the pie 2 do0ã€‚

 I I think so we can see the change of the pie to and it good for the very simple toã€‚

 to use very friendly to programã€‚ And now it more focus on the performanceã€‚

 So that's why we also the very happy to have the Michael here to share this kind of things for pie 2 do0 and to share his things with usã€‚

 Thank youã€‚ğŸ˜Šï¼Œã—ã¦ã“ã‹ã€‚Thanks for the kind introductionã€‚å—¯ã€‚

So I'm excited to be here today to talk about how we optimize by Tor Tudodo toã€‚ğŸ˜Šã€‚

Be the workers for generative AI to be the environment of choice for developers in the generative AI fieldã€‚

 if we look atã€‚Genrative AIï¼Œ we find that generative AI today uses artificial intelligence models that are able to generate new and original content across a range of modalitiesã€‚

 imagesï¼Œ text and musicï¼Œ and these models can produce realistic and creative outputs that have significant potential in a wide range of domains including art designã€‚

 entertainmentï¼Œ etcï¼Œ what we know about generative AI models today is they are the largest neural network models in use and as a result they create a unique set of requirementsã€‚

The most common generative AI model todayï¼Œ the largest models are the largest language models which are advanced AI models that excel in generating humanlike text responses and they have proven incredibly powerful for understanding in generating natural language and enabling models to interact more naturally with humansã€‚

 an example of this is chatCT developed by open AI that's been trained on a very extensive corpusive text data so if we look at the language models sizeã€‚

 we find that over the last five yearsï¼Œ language models have grown in size by five orders of magnitude that's a factor of 10 every year my background this a chip design and their factor of2 x every 18 months was consideredck Bckneã€‚

Speed and it has transformed society and technologyï¼Œ as we all knowã€‚

 that's why we are here compare that with 10 x in a year for AI modelsã€‚

 What comes with that is also an incredible demand for processing power and for new capabilities to take advantage of the hardware and to make these large language models affordableã€‚

 deployable and have the necessaryã€‚Speed to interact naturally with applications and with humansã€‚

The second class of models are diffuser models or latent diffusion models LDMsã€‚

 and they excel in producingã€‚Media output in particular image imagesï¼Œ videoï¼Œ and moreã€‚

 they use latent diffusion processes to create these high quality outputsï¼Œ two common examplesã€‚

 well known examples are Dli2 and stable diffusionã€‚

So when we look at all of these generative AI models todayã€‚

 we find three key requirements to make them performantã€‚

 to make them deployable to make them affordableã€‚ first and foremostã€‚

 this speed and efficiency because these generative AI models are so computationally expensive to both train and query so efficient training and inference areã€‚

Fundamentally importantã€‚For a widespread adoption of these modelsã€‚When we look at these models todayã€‚

 we find that they're all transformer models based on attention mechanismsã€‚ So this is in particularã€‚

 a component that we want to optimize for to improve model performance and efficiencyã€‚And finallyã€‚

 large language modelsï¼Œ large gene AI models are largeã€‚Surprisingly or notã€‚

 And that means that they need distributed computing infrastructure to trainï¼Œ to make trainingã€‚

Make the necessary progress to make training affordable and to fit all the parameters of a model intoã€‚

A device or a plurality of devices that are interconnected using distribution technologiesã€‚

So how do we solve that Well for speed and efficiencyï¼Œ Pang talked about torch compileã€‚

 that's the work for accelerating modelsï¼Œ big and smallã€‚

 and they offer a significant benefit to all of the generative AI modelsã€‚

We optimize transformers with accelerated transformer implementations that we implement in PyTorch2Oã€‚

 and finally for distributed processingï¼Œ we have PyTtororch distributed with data parallel and fullyarded data parallel trainingã€‚

So let me turn to how we accelerate the Pytorch transformformer APIã€‚

 the standard API for transformers in Pytorchã€‚So the PyTtororch API in generalã€‚

 including the Pytorch transformformer API are designed for flexibility and these of use that is what attracts developers to PyTtororch P talked about that before that flexibilityã€‚

 the ego mode execution and a number of options for each of the operators to create to create all types of neural networks with range with a range of choices whether that's doing normalization before the basic attention function or after using different types of activation functions and so forthã€‚

 what all of that brings is a transformer implementation that requires many PyTtororch operators to implement and toã€‚

Control the computational complexityã€‚ These are executed in sequenceã€‚

And allowing for all the different options that might be presentï¼Œ so in a nutshellã€‚

 the flexibility comes at the price of performanceï¼Œ usability comes at the price of performanceã€‚å—¯ã€‚

With torch compile and with the accelerated transformersã€‚

 we can optimize transformer implementationsï¼Œ on the one hand we use fused kernels to combine multiple operations and avoid materializing large arraysã€‚

 large tensors that would introduce bottlenecks around memory accessã€‚

 one of the pans of existence of anybody who has tried to optimize models for accelerators or CPUs for that matterã€‚

 andã€‚They combine many operationsï¼Œ they fold the soft jamm into the matrix multiplyã€‚

 they fold two matrix multiplies into a single larger structureã€‚That wayã€‚

 we can combine many of the operators and implement them with a single fused loop kernelã€‚

The second optimization that we rely on is using a fast path architecture for inferencefrã€‚

 where we optimize inference for where we capture common inference patterns and use kernels that are optimized to exploit variable input sequence lengthã€‚

So if we look at the literature how to accelerate tensionï¼Œ computationã€‚

 there's been steady progress starting with the paper in 2021ï¼Œ that's just two years agoã€‚

That talked about how self-attention can be implemented without needing ON squared memory it introduced a concept of usingsing attention kernels to reduce memory usageã€‚

 and then last year tried Dao and his colleagues from Stanford University published a paper called flash attention fast and memory efficient exact attention the Di A that's based on these insights that Robin Stts first published a year beforeã€‚

The team from Stanford University provided an optimized coUuda implementation for this algorithm and it beat out Nvidious entries for MLPf Transformer training last year in 2022 in Juneã€‚

å•Šã€‚And finallyï¼Œ metata's own FA research labs developed exformasã€‚

 which is similarly based on the observations by Robin Stotts with respect to optimizing for memory usage to speed up attention computationã€‚

They released a package called Xformers that provides fast and memory efficient attention computationã€‚

Functions that are domain agnostic and are widely used in the research communityã€‚

 envision the N LP and moreã€‚So if we look at flash attentionã€‚

 this this figure is taken from the flash attention paperï¼Œ you can see the basic approach ofã€‚

Computing attention while reducing the memory usageï¼Œ rather than materializing the end by Nã€‚

Mattrix from computingã€‚Qï¼Œ K Tã€‚The algorithm has an outer loop that loads sub blocks of the K and the B input into S RAMã€‚

 and then in an inner loop iterates over a Q to computeã€‚

A subset of the final result that is then written to HM to the high bandwidth memory on GPU cardsã€‚

 and then it proceeds to the next blockï¼Œ iterating again over the vector Qã€‚

 the query vector of attentionã€‚The outcome is this hugeã€‚

 huge improvement of performance the authors of the F attentiontention paper report 7ã€‚

6 x speed up over the attention implementation in GT2ã€‚

Owing to folding all of these functions into a single kernel and if you look at these functions they all have in common that they take a large matrix as input and produce another large matrix as output and thereby bottleneck on memory excesses in each of the stepsã€‚

Let me turn briefly to Xformers Xforms is was developed by fair Metas research lab for AI and it consists of customizable blocks that are independent and can be used without the generic boiler plate code that comes with usual withã€‚

Aå•Šã€‚APpiI is provided by Pytorch that office a brought set of options and flexibilityã€‚

 These components are domain agnostic and depend on researchers to integrate them and to combine them to use for vision N LP and many other modalitiesã€‚

 This is a research firstã€‚Effortï¼Œ research first package that contains bleeding edge components that are not yet available in mainstream librariesã€‚

 Or that was the case last yearã€‚And these components are built for efficiency and in particular memory efficiency use to give researchers high speed of iterationã€‚

 it uses its own ka kernels where necessary and otherwise dispatches to libraries like Holas that are available in the open source community todayã€‚

Here is a view of the speedups that can be obtained with Xformers showing a speed up up to 4 x for training and a significant improvementã€‚

 up to 50% in memory efficiencyã€‚Soã€‚Both flash attention and Exers were new based on papers published in 2021 and came out in 2022 and to see the speed at which Pytorch developedã€‚

 they launched in 2023ï¼Œ but Pytororch to thedoï¼Œ one of the advantages of using an open source framework is the hackabilityã€‚

 the pluggability of new ideas and making Pytorch the go to environment for researchers to integrate their own work to have the most impact and make it available to the largest community of usersã€‚

 TriD and his colleagues at Stanford had independently developed flash attention andã€‚

Shortly before submitting theã€‚Paper and the training results to the ML Perf benchmark competitionã€‚

 they approached meta about integrating this capability into Pytorrch and soã€‚Weã€‚

Worked both with the flash attentionã€‚codeodeï¼Œ but also and its developersã€‚

 but also with the Xform developers to integrate these new bleedinguting edge libraries into Pytorch and making them available out of the box for all developers we did that using a new operator scaletop product attention it's not only a new operator it's a new kind of Pytorch operator it's an operator that comes with multiple implementations that support different inputs and hardware models so before in Pytorch you had one implementation of each operator per hardware platform and that would cover the entire operating space this is different here but scale top product attention depending on different hardware typesã€‚

 even different types of inputsã€‚Sizesï¼Œ dimensionsï¼Œ ratiosï¼Œ data typesï¼Œ et ceteraã€‚

 it dispatches to different implementationsã€‚They all have a common interfaceã€‚

 the app of the underlying SDPA operatorï¼Œ and the high levelve SDPA operator implements a selection logic that looks at the inputs that considers the current hardware platform and figures out which implementation to call this allows models to take advantage of optimized implementations depending on what hardware they run on and at the same time makes models portable across different models and hardware types and finally we've integrated SDPA into the Pyth API for multihead attention and transformers both encoders and decoders to accelerate training and inference for new models but also for existing models that were developed and even trained beforeã€‚

I to the0 was released with its support for accelerated transformersã€‚

 It goes hand in hand with forward and backward compatible weight formatsã€‚

 So you can train with the new operator with the acceleratedcc transformers and save the weights to use with the legacy model if that is deployed some place or converselyã€‚

 you can use a previously trained model and fine tune or deploy for production using the accelerated transformer capabilitiesã€‚

ğŸ˜Šï¼ŒSo here isã€‚How the scaled product attention plays outï¼Œ the top level functionã€‚

iss implementation agnostic and then dispatches to one of three implementations for this is for NVDA GPUs in particularã€‚

 there is a fallback kernel SDPA math that that implements mathematical description of the attention logic calling internally a sequence of Pythtorch operatorsã€‚

 it is basically the legacy implementation and the fallback if no better implementation is available we have the SDPA flash attention kernel that provides a coupa implementation of flash attention for a100 and some other devices for 600 bit floating point data types and certain input common input sizes in cases where flashes availableã€‚

It's typically the fastest choice and because it is highly optimized for just these few operating conditions and uses in line PTX and all sorts of other performance enhancements and finally there's the SDPA memory efficient kernel that comes from the Xformer library that provides a coa implementation of memory efficient attention developed by Mes F Paris Research labã€‚

 we have another implementation that is Tritonbased that we can use in the future for hardware that cannot take advantage of the F or the memory efficient implementations that implements but implements a cogenerator for the Triton kernelsã€‚

So this chart shows explores how to optimize large language models with accelerated transferã€‚

Transformers using And Harper's open source nanoGPD implementation as exampleï¼Œ so step number oneã€‚

 if you want good performance and efficiency is always enable to do compile there's really no excuse not to use it as Pg highlighted it has over 99% success rateã€‚

 a cross a broad range of very broad range ofã€‚Models that are out there todayï¼Œ andã€‚

Then the next step is we replaced the Python implementation of attention with the scale do product attention operator that improves overflow handling because the integrated SDPA operator distributes how scaling works to avoid overflows it delivers better runtime but memory efficient kernels being faster than the math implementation and flash offering a further improvement and it delivers slower memory usage which allows you to use a bigger batch sizeã€‚

 finally for large language models patting the vocabulary size helps with Tensor core efficiency because downstream matrix multiplies can take advantage can take better advantage ditch of Tensor core Tensor cores when vocabulary sizes aligned so we see the improvement hereã€‚

We start with a baseline of about 250 milliseconds per batch training time when we enable torch compile it goesã€‚

 it drops to 140 ish milliseconds is we replace the legacy Python attention implementation that was in nanogptT with the different kernels that we have in accelerated transformers we are able to get another significant reduction and then finally patting the vocabulary size to take better advantage of Tensor course offers yet another boost the other thing that you see here is that with the memory efficient attention implementations that's both flash and the Xformer implementation you can use much larger batch sizes because demo they make moreã€‚

fficient use of memory and that in turn is another boost in performance If you look at this here the 2X speed upã€‚

 this plots the validation loss after one hour of training so flash attention by offering a faster iteration time offersã€‚

Much better all lossã€‚Regardless of batch size and you can see the distance between the two curves reflecting about 2x speed up between the python baseline and the flash andã€‚

 so then if we extrapolate that out to the improved validation loss that we get from using larger batchesã€‚

 using larger batches in essence translates to another over 2x improvement in speedupã€‚

 of course that depends on specific convergence behavior of your specific modelã€‚

We worked with hugging face to optimize decota models and we found that we got up to 70% inference speedup for diffuser models and for training LLMs we got 70% training speed improvement and about 20% inference speedup in addition we got over 110% memory savings in some instances plus more importantly the models did not o for for many cases so accelerated transformers enabled new capabilities so in addition we haveã€‚

The fast bath logic and that for inference and the biggest benefit out of fast path is taking advantage of nest tensors these are a subclass of tor the tensor that Pg mentioned earlierã€‚

 and they allow to capture the variable length tensorsã€‚

 inputs that are common in NLP processing and that can deliver up to 4 x speed up on BRT style models and up to 2ã€‚

5x inference speed up on vision transformersã€‚Soã€‚We have parallel training support and Pyth2ã€‚

0 with both data parallel training and fully sharded parallel trainingã€‚

 I'll skip that in the interest of time and so I'll conclude with theã€‚å•Šã€‚

With with a summary that Pytor20 brings significant out of the box acceleration to Pytor transformer API and to models for training and inference that use transformersã€‚

 we've integrated that in torch textï¼Œ torch vision torch multimod far sakeã€‚

 And we've worked the hugging face to integrate as wellã€‚ andã€‚By providing that library integrationã€‚

 we can get performance out of the box for a broad set of models that were developed even before Pythch Studiodoã€‚

 finally Pythtorch Studiodo provides abstraction and portability using both Torch compile and the new SDPA interface that allows models to take advantage of new improvements that will come online both in the compiler and a new high performance kernels for transformers to deliver the best possible performance of models in the futureã€‚

 even devices that are not available today with that thank you for your attention and I'll conclude my talkã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_13.png)

Thank youï¼Œ Michaelã€‚ Thank youã€‚ Yeahï¼Œ Yeahï¼Œ and it's amazing that a lot of good stuff is coming into the P touch2 dot O and not only to accelerating the inference sideã€‚

 but also the important for the training sideï¼Œ soã€‚ğŸ˜Šï¼ŒOkayï¼Œ so let me introduce our next speakerï¼Œ yeahã€‚

Actuallyï¼Œ Jingï¼Œ he is came from any scaleã€‚ any scaleã€‚

 I think people might heard about the Ray that is the another very good framework and especially for optimize the distributed training sideã€‚

 So todayï¼Œ I'm very glad can invite Jingã€‚ he actually is leading the machine learning team at any scale and any scale is the company who is supporting the Ray very fast groupã€‚

 So let's welcome Jingï¼ŒğŸ˜Šã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_15.png)

Yeahã€‚YeahThanks for the kind introduction to yeahï¼Œ very glad to be here I talking Bar Ray with the audience todayã€‚

 My name is Juneã€‚ I let the I lead the machine learning team at any skillã€‚

 which is the company that's behind the open source frameworkï¼Œ Rayã€‚ğŸ˜Šï¼Œå—¯ã€‚

So let me see if I can figure out how to use this thingã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_17.png)

So I don't have the slides actuallyï¼Œ but fortunatelyï¼Œ Michael just show the chartã€‚

 you know during the last talkï¼Œ the scale of the machine learning workflows these days seems to be following some sort of exponential curveã€‚

 and it seems to be actually accelerating even fuel by the large model generative AI stuffã€‚

 So that's actually the reason we believe the folks at any scope believe why the framework of way sort of like catch a lot of attention and popularity gaining a bit of popularity latelyã€‚

 when you're talking about open source frameworksï¼Œ people often use the GiHub star as a metric for measuring how popular the framework is So here's a quick chart to show thatã€‚

The red lines rightï¼Œ and we sort of have caught up to Kafka since the last month in like half of the time and we are marching towards the spark right nowã€‚

å—¯ã€‚So a little bit of history of the frameworkï¼Œ I think sixï¼Œ seven years agoã€‚

 the two founders of any skill they were working on reinforcement learning at Berkeleyã€‚

 the reinforcement learning at those days are a little bit different from today where the computation is actually usually blocked on the simulation side instead of the training side because reinforced learning is essentially just like randomly exploring in some simulated physical worldã€‚

Andã€‚The founders actually just found themselves repeatedly stuck in this situation where they need to write a lot of GRPC and talk to like how these remote simulated environment collect the training sample the actual training of the policyã€‚

 the reinforcement agent is actually pretty fast rightã€‚

 So all the computations are like all blocked on the simulation sideã€‚ğŸ˜Šã€‚

So they thought like let's write some frameworkï¼Œ you knowï¼Œ to make it easier to to do GRPC and to doã€‚

 you knowï¼Œ bandwidth efficientï¼Œ you knowï¼Œ data movement and that kind of stuffã€‚

 So this is how the framework startsã€‚ and this is also how why if you look at Ray RLã€‚

 which is like R reinforcement learning library that's built on top of Ray is one of the first library that's thereã€‚

 things like day1ã€‚And it's a little bitã€‚Unï¼Œ I sayã€‚And then they quickly realized thatã€‚

 you know this framework seems to be useful for not just reinforce learningã€‚

 but other types of distributed computing as wellã€‚ and the abstraction seems to beï¼Œ you knowã€‚

 nice and elegantï¼Œ that's why they started the company any skill to you know try to commercialize Ray And since the companies founded we've added you know other libraries like Serã€‚

 which is to tackle the inference and serving use case of theã€‚ğŸ˜Šï¼ŒYeahã€‚

 for for machine learning and alsoã€‚Last year we launched Ray Airã€‚

 which is sort of like this umbrellaã€‚You knowï¼Œ term toã€‚

 to cover this end to end machine learning use caseã€‚ And also like more recentlyã€‚

 there's actually also Ro data setï¼Œ which I'm gonna talk about laterã€‚

I didn't realize there's animation for this guyã€‚So this is like a picture of the native ecosystem todayã€‚

 So at the very bottomï¼Œ you have physical computationï¼Œ computing cluster and machines and all thatã€‚

 and rate core sort of sits on top of the the physical computing devices to to extract away the infrastructure side of thingsã€‚

 And above rate coreï¼Œ you're pretty much just dealing withï¼Œ you knowï¼Œ native Pythonã€‚

 and you don't have to worry about GPS or CPU or memory and all that kind of stuffã€‚

 And once you have rate coreï¼Œ then we build all these librariesï¼Œ you knowã€‚

 rating for deep deep learning rating tune for hyperpermeter tuning and serve and data set to like cover the entire into an machine learning workflowã€‚

ğŸ˜Šï¼ŒOkayï¼Œ soã€‚Now that we have a high level picture of what you knowã€‚

 the ray is and what the ecosystem might look likeã€‚

 what I wanted to do is to get into some of the more specificã€‚å—¯ã€‚Use cases of the the the theã€‚

 the whole ecosystemã€‚ I want to discuss theï¼Œ the common pain points that weã€‚

We see from our day to day life dealing withï¼Œ you knowã€‚

 customers and users and also explaining a little bit of why how the real libraries are trying to tackle those and try to like make those things easierã€‚

 The first thing is actuallyï¼Œ so this is in likeï¼Œ you knowï¼Œ the workflow orderã€‚

 So the first thing I want to talk about is the data ingeesttion side of things andã€‚And we this isã€‚å‘ƒã€‚

é‚£ä¸ª funnyä½ çš„ we buildã€‚Per the last like this is the most recent addition to the to the ecosystem where we introduced array data setã€‚

 I think I think just yesterdayï¼Œ somebody was asking meï¼Œ likeã€‚

 why do you guys want to build a data setï¼Œ you knowã€‚

 Pytor has a data load they like hacking phase data has a data set the the answer was actuallyã€‚

 you knowï¼Œ while we are dealing withï¼Œ you knowï¼Œ helping users and customers debugging their workload and dealing with these practical mattersã€‚

 you actually found the the distributed data ingestionã€‚

Is actually a little bit trickier than you than you thinkï¼Œ rightã€‚

 Because these thing these things normally run zone CPUã€‚

 like the the pre processing and data loadingã€‚Like there's multiple steps between you load the data and before they get to the GPU for trainingã€‚

So that makes theï¼Œ the cluster usually like a heterogeneousã€‚

 meaning like you will have different shape of the machineã€‚

 You will have different resources available on different nodes of the the machinesã€‚

 And also when youï¼Œ when theï¼Œ theï¼Œ theï¼Œ the scale of your data gets largeï¼Œ oftentimes people willã€‚

Csh if you go like data loader requires you to load all the memoryï¼Œ all the data into into memoriesã€‚

 I was actually always a little bit suspicious if somebody library tells me like I need to load all the data into memory at onceã€‚

 rightï¼Œ usually those things don't workã€‚ğŸ˜Šï¼ŒAnd because of Ray and because of thisã€‚

 like shared memory space on top of multiple machinesã€‚ and within the clusterï¼Œ theã€‚

 the team felt like we can build a library to make these things a little betterã€‚ğŸ˜Šï¼ŒAndã€‚

Oh another important thing is if you actually go the bug people's machine learning workloadsã€‚

 you will realize like a lot to time times when they say the GPU utilization is pretty lowã€‚

 the problem is actually happening on the ingestion sideï¼Œ you knowã€‚

 some sort of like prefeion and stuff will solve those those casesã€‚

So that's why we build a rate data setï¼Œ which provides a distributed and resource of whereã€‚

Data transformation and ingestionï¼Œ meaningï¼Œ you knowï¼Œ if you have a cluster where you have three GPsã€‚

 you knowï¼Œ doing the actual like data parallel training and you have 30 CPU that's doing image reading and transformation before they gets free to GPU data sets are actually aware of these resources and will dispatch your work to the right noteã€‚

 And then it's gonna try to like be smart and take you knowï¼Œ data locality into considerationã€‚

 and you knowã€‚And make the whole thing like pretty efficientã€‚And I think just last releaseã€‚

 we also enable the streaming execution as default strategyï¼Œ meaningã€‚By defaultã€‚

 we'll never try to load your data all into memory once like everything is is streamed from all the way from the the source device and we will apply proper back pressure so thatã€‚

Like your job is efficientï¼Œ but we don'tï¼Œ you knowã€‚

The performance one degree into like barkinggestionï¼Œ basicallyã€‚Coolï¼Œ so the next is batch inferenceã€‚

 Iï¼Œ I mentioned this the the secondï¼Œ because although this is like theï¼Œ theã€‚

 the step after you get the modelï¼Œ I w to discuss this next because it's also handled by the dataset set instead of like just doing ingestionã€‚

ğŸ˜Šï¼ŒA common thing that once you've trained the model is to actually use the model and run it over like a large set of dataã€‚

This we call it like battery infantã€‚ you can think ofï¼Œ you knowï¼Œ for exampleã€‚

 sometimes you have a large car of like tax dataã€‚ you want to run some embedding model to turn them intoã€‚

 you knowï¼Œ embeddingsã€‚That that task actually has a very similar characteristic as the the ingestion partã€‚

 because you're still dealing withï¼Œ you knowï¼Œ a heterogeneous cluster and the data loading will happen somewhereã€‚

 like fromï¼Œ you knowï¼Œ S3 cloud storageã€‚ but the model needs to round GPUï¼Œ rightã€‚

 So that job is also handled we think the best by rate data setã€‚ Andã€‚And I'm gonna show youã€‚Actuallyã€‚

 some benchmark that we didï¼Œ you knowï¼Œ more recentlyã€‚

 we were comparing using ray data set versus usingï¼Œ you knowï¼Œ the sage makerã€‚

Batcher inferencing and also using spark to doï¼Œ to do this like batch inferencing jobã€‚

 Sage maker is actually really about because it's moreã€‚

Like a wrapper around this online serving systemï¼Œ which is totally not fit for offline model inferencingã€‚

 The more interesting part here is is the comparison comparison between radioius and and Sparã€‚

 The team naturally spend a lot of time optimizing the sparkã€‚ğŸ˜Šã€‚

Job to make sure were like doing a fair benchmark comparisonã€‚ And in the endï¼Œ we were beã€‚

 we were able to roughly like outperform maybe 3 x compared with a multi spark clusterã€‚

 The reason behind that is because Sp constantly switches between you know Python and Javaã€‚

 like these two wordsã€‚ So your data gets serialized and decalized like really often between themã€‚

 And alsoï¼Œ I think Sp has less flexibleã€‚ğŸ˜Šï¼ŒProcessing fusingï¼Œ soã€‚They tied to you like overfusingã€‚

 you knowï¼Œ multiple steps togetherã€‚ And youï¼Œ if you have one step in between that like bottleneck on GPUã€‚

 then all your CPU steps are like slower as wellã€‚ That's where ray tasks shines and like give you some extra performanceã€‚

 The funny thing about this is alsoï¼Œ when we publish this benchmarkï¼Œ theã€‚ğŸ˜Šã€‚

The folks at Databricks actually ping us and sayã€‚You knowï¼Œ we can optimize this betterã€‚

 This is not fairã€‚ And they spend some timeã€‚ And theyï¼Œ theyï¼Œ in the endï¼Œ they likeï¼Œ yeahã€‚

 its somethingã€‚We can do this rather than do thisã€‚è¯¶ã€‚Okayï¼Œ so about distributed training and tuningã€‚

This is like a bread butter for any machine learning systemsã€‚

What we know this about distributed training is thatã€‚These daysï¼Œ theã€‚

 the ecosystem for training is like extremely fragmentedï¼Œ meaningã€‚You knowã€‚

 you just get like all kinds of frameworks that you can do thisã€‚ And alsoï¼Œ things move so fastã€‚

It's very hard to say which framework are wing or like right the industry is completely not consolidated onto any particular frameworkã€‚

 especially like these daysï¼Œ you start to useï¼Œ you knowã€‚

 on deep speed or like accelerate to do large model trainingã€‚

 which totally wasn't the case three months agoã€‚ So what we felt like we should do for distributed training is we want to make the you know options openã€‚

 We want to make the system like very extensible So users don't get locked in to some single you know framework thatã€‚

They end up having to switch with a lot of effortã€‚So the the Raing library is ratherã€‚You knowã€‚

 liewayï¼Œ it basically provides a lot of integrations with the existing training frameworks out thereã€‚

 like Ptor lightning or D C orã€‚Hgging face accelerateã€‚

What we take care of is mostly the setting up the clusterã€‚

 setting the wrong time environment part of itã€‚The actual deep learning and the distributedã€‚

 you knowï¼Œ communication is handled by the underlyingã€‚å•Šï¼Œlibraryã€‚

And that would allow that that basically like allowï¼Œ you knowã€‚

 these machine learning engineers and data scientists to do this without worrying about the hardwareã€‚

 as long as you have a very clusterï¼Œ like your job can beï¼Œ you knowï¼Œ up and aroundã€‚å—¯ã€‚And alsoã€‚

 we made it so that it's very easy to turn a training job into a help department training jobã€‚

 becauseã€‚Rune is actually orchestrating the the all the work behind the in anywaysã€‚

 so you just need to like switch a few lines of Python code to do thatã€‚Yeahã€‚

 rating is for some reasonï¼Œ is one of the most popular librariesã€‚å•Šã€‚That that Ray hasã€‚Coolã€‚

 so about servingï¼Œ you knowï¼Œ we talk about the offline batch inferenceã€‚

 so in terms of online servingï¼Œ what we see the users and customer Wang are auto skating because the GPUs are really expensive and the serving traffic is actually naturally like cyclicã€‚

So so they want to scale down to like very few node you know during the night and scale up quicklyã€‚

 So the all of skin speed is actually very crucial for any like practical serving system and they also want thought tolerance we basically like putting a lot of work introduce highly available serving maybe since last year and now you get to enjoy like highly available cluster and the race service deployment with relatively ease one thing I also want to mention is that from day oneã€‚

 actually the selling point of race service because it's built on top of the Raycorps and the ray has this actor concept where you know you can write like a little microserv each thing using simple like Python class and that thing is actually backing all theã€‚

model replicasï¼Œ It made it very easy to do model compositionã€‚And for a lot of the use casesã€‚

 actuallyï¼Œ it's not about a single modelã€‚ You want toï¼Œ you knowã€‚

 score something using a set of models in the like graph fashionã€‚ that actually is a veryã€‚

It's like a pretty sweet spot forï¼Œ for serve and the reason for lot people to use itã€‚ğŸ˜Šï¼ŒAnd of courseã€‚

 more recentlyï¼Œ because of the large models we introduced some features that's not important beforeã€‚

 such as like streaming supportï¼Œ and we had to improve the scalability envelope the law just to be able to scale up the large model deploymentã€‚

Coolï¼Œ so the answer is about generating VIã€‚ I think the the talk these days will not be complete if you don't mention large modelã€‚

ğŸ˜Šï¼Œå—¯ã€‚The raiseã€‚Interaction with the the large model in general AI world is actually pretty earlyã€‚

Even before I joined like maybe roughly around when I joined any skillã€‚

 we were working with this open source teamã€‚They were trying to train this model called GBJã€‚

 which is a replicaï¼Œ tried to open source a replica of G3ï¼Œ I thinkã€‚

And they got a lot of free credit from Googleã€‚ So they were training those things on TPUã€‚

 And we were helping them to celebrate cluster on TP U so they can like have a reasonableï¼Œ you knowã€‚

 developer experienceã€‚ instead of dealing with like a large cluster of TPU instancesã€‚ğŸ˜Šã€‚

So that's the whenï¼Œ this thing startedã€‚ We actually didn'tã€‚

Pay too much attention about this until very recentlyï¼Œ when like data breakricksï¼Œ theyã€‚

They fine tune dling on top of GJã€‚ everyone was surprised likeï¼Œ actuallyï¼Œ the model is pretty goodã€‚

 You knowï¼Œ it's better than you thinkã€‚Andï¼Œ and other people who are usingï¼Œ you knowã€‚

 a ray to do large modelï¼Œ including like cohere AIã€‚ If you guys don't knowï¼Œ like theã€‚

 the founders of cohere are the inventors of theï¼Œ the transformer structureã€‚

And also we recently got permission from Open to say like they actually use the right train record to train G4ã€‚

 So from our perspectiveï¼Œ you know large model is not like a single kind of thing because there are model sizes that you can do with a single instance as long as you have a few GPU cards on the instanceã€‚

 you can do a reasonable sized large model training on thatã€‚ so from our perspectiveã€‚

 we try to support the work for our customers and the open source communities by using by offering a spectrum of thingsã€‚

 for exampleï¼Œ if you're okay with like one you know computer node an a GPU cardsã€‚

 then we have these like off the shelf examples that you can just quickly wrong and your job will be running in no time and if you're trying to run these relatively smallerã€‚

Scallï¼Œ you knowï¼Œ large model trainingã€‚ Then you can use the ecosystemï¼Œ you knowã€‚

 the native ecosystem libraryã€‚And then we take care of the hardware and you can train your jobã€‚

And if you're reallyï¼Œ really hotcoreï¼Œ like a GP ableI and cohereï¼Œ whoï¼Œ you knowã€‚

 who's trying to write your own custom training stack and optimize everything yourselfã€‚

 then they actually useï¼Œ they just they don't use any librariesã€‚

 but they use the rateco to to help with the developer and experience and also the iteration speedã€‚

So that'sï¼Œ that's what I mentionedã€‚ I think it'sã€‚It's useful thing to you knowã€‚

 share at this conferenceã€‚Okayï¼Œ so I was when I was writing these slidesã€‚

 I was looking at the some of ourï¼Œ you knowï¼Œ biggest open source and partners and and customers to seeã€‚

 you knowï¼Œ what are the the most common use cases they have for Rayã€‚ aside fromã€‚

 you know the big model stuffã€‚ surprisinglyï¼Œ actually the most common use cases they will build a internal rate cluster as their machine learning platformã€‚

 So that their data scientistsï¼Œ the machine learning engineer can basically just submit the jobsã€‚

 you knowï¼Œ to this cluster and they get the resultï¼Œ you knowï¼Œ without doing anyã€‚

 know Kubernetes stuff or some because the scientists data scientistsã€‚

 they get to interact with know Pythonï¼Œ which is the thing that they use every day as super familiar that helps with their developer experience at lawã€‚

 we obviouslyï¼Œ so I don't have time to mention orï¼Œ but our is sort of thisã€‚The onlyï¼Œ you knowã€‚

 of the shelf of choiceï¼Œ if you want to do production scale reinforcement learning and we actually have a pretty big users of of the library thereã€‚

 and obviously and actually a very bigï¼Œ you knowï¼Œ adopter of from day oneã€‚ and they are sort of ourã€‚

 you knowã€‚Colleague and theyï¼Œ they submitted a lot peers to theã€‚To the code baseã€‚å•Šã€‚Yeahï¼Œ lastlyã€‚

 I would encourageï¼Œ you knowï¼Œ raise completely open sourceã€‚ You canï¼Œ you knowã€‚

 please take a look at our Github and also theï¼Œ the forum and try to get involvedã€‚

 We welcome all the contributions from itã€‚ğŸ˜Šï¼ŒThe communityï¼Œ the company is actually relatively smallã€‚

 We have very limited resourceã€‚ So a lot of times the the awesome things happen because of the communityã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_19.png)

å•Šï¼Œ that's itã€‚Okayï¼Œ thank you thanks share the Ray the and and those practice with those big nameã€‚

Next I want to introduce our next speaker doctor Tong Xinbaã€‚

 actually the AI compiler is very important and it not only important for the big model the large foundation modelã€‚

 but also to important to important to boost the boost the and adoption of the different architecture of the chipsã€‚

 especially for for for this local market so B A AI our institute start the AI compiler this kinds of research direction from last yearã€‚

 and so Tongin he is the leader for this directionã€‚

 So today I am very honor to introduce Tongin and invite him to deliver his speech in this forumã€‚

 Thank youã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_21.png)

I think to be fairï¼Œ I'm going to use Chineseã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_23.png)

å°±ä»Šå¤©æˆ‘ä»¬ç»™å¤§å®¶é‚£ä¸ªæ±‡æŠ¥ä¸€ä¸‹ï¼Œæˆ‘ä»¬æ™ºæºåœ¨é‚£ä¸ªAIç¼–è¯‘å™¨æ–¹é¢ï¼Œå°±æ˜¯ä¸»è¦æ˜¯è¿‘åŠå¹´å‘ƒï¼Œåˆ°ä¸€å¹´è¿™ä¸ªæ—¶é—´çš„ä¸€äº›é‚£ä¸ªè¿›å±•ã€‚ç°åœ¨åªç°åœ¨ä»ç„¶æ˜¯ä¸€ä¸ªé‚£ä¸ªworking progressçš„çŠ¶æ€ã€‚å¯¹ã€‚å¯¹ã€‚

é‚£ä¸ªä¸»è¦å†…å®¹çš„è¯å°±æ˜¯è¯´æˆ‘ä»¬å…ˆåšä¸€ä¸‹é‚£ä¸ªèƒŒæ™¯ä»‹ç»ï¼Œå°±æ˜¯ä¸»è¦æ˜¯æˆ‘ä»¬çœ‹ä¸€ä¸‹ï¼Œå°±æ˜¯å½“å‰æˆ‘ä»¬è½¯ç¡¬ä»¶é€‚é…è·ŸIç¼–è¯‘å™¨é¢ä¸´çš„ä¸€äº›å‘ƒç—›ç‚¹çš„é—®é¢˜ã€‚ç„¶åå‘¢ï¼Œæˆ‘ä»¬ä¸€èµ·è·Ÿè¿™ä¸ªå‚å•†æ¢è®¨ä¸€ä¸‹ç°åœ¨çš„è¿™ä¸ªè¶‹åŠ¿ï¼ŒæŠ€æœ¯å‘å±•çš„è¶‹åŠ¿ã€‚

è¿˜æœ‰è¿™ä¸ªè¿™ä¸ªéœ€æ±‚çš„è¶‹åŠ¿ã€‚å¦å¤–å°±ä»‹ç»ä¸€ä¸‹æˆ‘ä»¬é‚£ä¸ªæ™ºæºç¼–è¯‘å™¨çš„æ•´ä½“çš„è¿›å±•çš„æƒ…å†µã€‚ç„¶åé‚£ä¸ªæŠ€æœ¯æ–¹é¢çš„è¯ï¼Œå°±æ˜¯è¯´æˆ‘ä»¬åé¢çš„ä¸»è¦çš„æ—¶é—´å¯èƒ½å°±æ˜¯å½“ç„¶ä»Šå¤©æ—¶é—´å¾ˆå°‘äº†ã€‚å¯¹ï¼Œå°±æ˜¯å‘ƒä¸»è¦è®²ä¸€ä¸‹æˆ‘ä»¬ç°åœ¨æ•´ä½“çš„è¿™ä¸ªè®¾è®¡æ€è·¯ã€‚

è¿˜æœ‰è¿˜æœ‰å‡ ä¸ªè¿™ä¸ªå…³é”®æŠ€æœ¯ç‚¹ã€‚å¯¹ï¼Œååé¢çš„è¯æˆ‘ä»¬è®²ä¸€ä¸‹æˆ‘ä»¬çš„é‚£ä¸ªå±•æœ›å’Œæˆ‘ä»¬å¯¹æœªæ¥çš„è¿™ä¸ªæœŸæœ›ã€‚å¯¹ã€‚å‘ƒå‘ƒæ˜¯è¿™æ ·ï¼Œå°±æ˜¯è¯´æˆ‘ä»¬åœ¨åšè¿™ä¸ªå®é™…ä¸Šæˆ‘ä»¬æ˜¯comï¼Œå°±æ˜¯è¯´æˆ‘ä»¬æ™ºæºä¸»è¦æ˜¯åšè¿™ä¸ªæ¨¡å‹è·Ÿç®—æ³•ä¹‹ç±»çš„ç ”ç©¶ã€‚

æ˜¯æˆ‘ä»¬å»å¹´å¼€å§‹ï¼Œæˆ‘ä»¬æƒ³æ·±å…¥å»åšè¿™ä¸ªç³»ç»Ÿçš„è¿™ä¸ªæ”¯æŒã€‚å¯¹ï¼Œå› ä¸ºæˆ‘ä»¬è§‰å¾—å°±æ˜¯è¯´æœ‰å‡ ä¸ªé—®é¢˜æ˜¯è¦è§£å†³çš„ã€‚å°±æ˜¯ç¬¬ä¸€ä¸ªå°±æ˜¯æˆ‘ä»¬å›½äº§å¾ˆå¤šè¿™ä¸ªä¸“è¿™ä¸ªé€šç”¨çš„GPUå•Šï¼Œè¿˜æœ‰è¿™ä¸ªé¢å‘è¿™ä¸ªæ¨¡å‹è®­ç»ƒåŠ é€Ÿå™¨ã€‚ä½†æ˜¯åˆ°ç°åœ¨çš„é—®é¢˜çš„è¯ã€‚

å°±æ˜¯è¯´ä»–å¯èƒ½å°±æ˜¯æ”¯æŒè¿™ä¸ªä¸Šæ¸¸æ¡†æ¶çš„èƒ½åŠ›è¿˜ç›¸å¯¹è¾ƒå¼±ã€‚ç„¶åæˆ‘ä»¬æƒ³å°±æ˜¯æ€ä¹ˆæ ·èƒ½å¤Ÿé€šè¿‡è¿™ä¸ªæŠ€æœ¯ï¼Œå°¤å…¶æ˜¯compilerè¿™ä¸ªæŠ€æœ¯æ¥å»å‘ƒè¿™ä¸ªèƒ½è®©è¿™ä¸ªå‰ç«¯çš„è¿™æˆ‘è¿™è¿™ä¸ªæ¡†æ¶é€‚é…èƒ½æ›´é¡ºæ»‘ä¸€äº›ã€‚å¯¹ã€‚

ç„¶åæˆ‘ä»¬å»å¹´å¹´åº•çš„æ—¶å€™å‘¢ï¼Œå°±æ˜¯è·Ÿè¿™ä¸ªå‚å•†ï¼Œè¿˜æœ‰è¿™ä¸ªå‘ƒå­¦æœ¯ç•Œçš„ä¸€äº›å­¦è€…æˆ‘æˆ‘ä»¬åšäº†ä¸€ä¸ªç®€å•çš„è°ƒè°ƒç ”é—®å·ï¼Œå°±æ˜¯ä¸»è¦æ˜¯æ¢è®¨ä¸€ä¸‹ç°åœ¨çš„è¿™ä¸ªå¯¹ç¼–è¯‘å™¨å•Šï¼Œè¿˜æœ‰è½¯ä»¶é€‚é…çš„éœ€æ±‚çš„é—®é¢˜ã€‚

ç„¶åé‚£ä¸ªå‘ƒæˆ‘ä»¬ç¨å¾®é‚£ä¸ªsummariseè¿™ä¸‹ï¼Œå°±æ˜¯è¯´ä¸€äº›ã€‚ç‚¹é—®é¢˜é›†ä¸­åœ¨è¿™å„¿ï¼Œå°±æ˜¯è¯´å¯¹å‚å•†æ¥è¯´ï¼Œè¿™ä¸ªå¤šæ¡†æ¶çš„ä»‹å…¥æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤´ç–¼çš„é—®é¢˜ã€‚å¦å¤–å°±æ˜¯è¯´AIBNGå¦‚ä½•è¿›è¡ŒåŠŸèƒ½çš„æ³›åŒ–ï¼Œæ€§èƒ½çš„æ³›åŒ–ã€‚

å…¼å®¹æ€§ä¹Ÿæ˜¯ä¸€ä¸ªå®‰å¼„çš„è¿™ä¸ªè¿™ä¸ªé—®é¢˜ã€‚è¿˜æœ‰ä¸€äº›å°±æ˜¯å‘ƒè¿™ä¸ªå‚å•†å¯¹è¿™ä¸ªç¼–å™¨å‘ƒé¢å‘å¤šç§åç«¯çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œè¿˜æ˜¯æŒä¸€äº›è¿™ä¸ªå°±æ˜¯æœ‰æœ‰ä¸€å®šæ€€ç–‘æ€åº¦ã€‚å› ä¸ºå°±æ˜¯å—¯ä¸åŒçš„ç¡¬ä»¶çš„è¯ï¼Œå®ƒçš„è¿™ä¸ªåº•å±‚çš„æ¶æ„å•Šã€‚

è¿˜æœ‰ç»“æ„éƒ½æ˜¯ä¸åŒçš„é‚£ä½ æ€ä¹ˆæ ·å»è®¾è®¡ä¸€ä¸ªè¿™ä¸ªè¿™ä¸ªé€šç”¨çš„è¿™ä¸ªpartæŠ€æœ¯è¿˜æ˜¯æŒºå›°éš¾çš„å¯¹ã€‚å—¯ï¼Œç„¶ç”±æ­¤çš„è¯å¯èƒ½ä¼šå¼•å‡ºä¸€äº›è¿™ä¸ªè·Ÿç¼–è¾‘ç›¸å…³çš„è¿™ä¸ªæŠ€æœ¯æŒ‘æˆ˜ã€‚å¯¹ã€‚å‘ƒã€‚

ç„¶åæˆ‘æˆ‘æˆ‘æˆ‘æˆ‘ä¹Ÿåšäº†ä¸€ä¸ªè¿™ä¸ªå°±æ˜¯æˆ‘ç”»äº†ä¸€ä¸ªè¿™ä¹ˆä¸€ä¸ªå›¾ï¼Œå°±æ˜¯è®©è®©å¤§å®¶é¢„æµ‹ï¼Œæˆ–è€…åƒè¯„ä¼°ä¸€ä¸‹ç°åœ¨è¿™ä¸ªå‘ƒä¸åŒçš„è¿™ä¸ªä¸»æµçš„è¿™ä¸ªAIç¼–å™¨ç°åœ¨çš„è¿™ä¸ªå‘ƒä¸»è¦æ˜¯é¢å‘è¯•è´¹çš„åç«¯è¿ç§»æ€§å•Šï¼Œè¿˜æœ‰æ‰©å±•æ€§å¦‚ä½•ã€‚å¯¹ã€‚

å¦å¤–å°±æ˜¯è¯´å‘ƒæœªæ¥çš„å‘å±•è¶‹åŠ¿æ€ä¹ˆæ ·ã€‚å‡å¦‚è¯´æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå‘ƒæˆ‘ä»¬æŠŠå›¾ä¸Šè¿™ä¸ªè¿™ä¸ªæŠ€æœ¯å‘è¿™ä¸ªå³ä¸Šè§’å»å»æ¨è¿›çš„è¯ï¼Œé‚£å‘ƒå¯¹å°±æ˜¯æœ‰å“ªäº›è¿™ä¸ªéš¾ç‚¹ã€‚å¯¹æˆ‘åšäº†ä¸€ä¸ªè¿™æ ·çš„ä¸€ä¸ªé—®å·ã€‚å¯¹ï¼Œç„¶åæˆ‘ä»¬å¤§å®¶å‘ç°çš„è¯ï¼Œå°±æ˜¯æˆ‘ä»¬ç®€å•çœ‹ä¸€ä¸‹ã€‚

å°±æ˜¯å‘ƒå°±æ˜¯è¿”å›çš„è¿™ä¸ªæ„è§å§ã€‚å°±æ˜¯è¯´æ­£å¥½çœ‹ä¸€ä¸‹è¿™ä¸ªç›¸å¯¹çš„ä½ç½®æ˜¯ä¹ˆæ ·ã€‚ä½†æ˜¯è¿™ä¸ªå¹¶ä¸æ˜¯å®¢è§‚çš„æˆ‘è§‰å¾—ä»–åæ˜ çš„ä¸»è¦æ˜¯é‚£ä¸ªå‘ƒè¿™ä¸ªå‚å•†è¿˜æœ‰å­¦è€…å¯¹ç°åœ¨æˆ‘ä»¬è¿™ä¸ªæ•´ä¸ªAIcompç”Ÿæ€çš„ä¸€äº›çœ‹æ³•ï¼Œæˆ–è€…æˆ–è€…ä¸»è§‚çš„æ„è§å§ã€‚å¯¹ã€‚

æ‰€ä»¥æˆ‘ä»¬çœ‹ä¸€ä¸‹å°±æ˜¯è¯´å‘ƒã€‚æ€»ç»“ä¸‹æ¥æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´å‚å•†å°±æ˜¯è¿™ä¸ªå‘ƒå½“ç„¶æˆ‘ä»¬ä»å‰é¢å¼€å§‹äº†ï¼Œå°±æ˜¯æ•´ä¸ªè°ƒç ”æ¥è¯´ï¼Œå°±æ˜¯å‚å•†å¯¹å‰ç«¯è®¾å¤‡çš„å·¥ä½œé‡å‘€ï¼Œè¿˜æœ‰å¤æ‚æ€§å•Šã€ä¸å¯æ§æ€§æœ‰ä¸€å®šçš„ç„¦è™‘ã€‚å¯¹ï¼Œç„¶åå°±æ˜¯å¯¹è¿™ä¸ªä»æŠ€æœ¯ä¸Šè®²ã€‚

å°±æ˜¯å¯¹ç¼–è¯‘å™¨çš„æŠ½è±¡èƒ½åŠ›ï¼Œè¿˜æœ‰æ³›åŒ–èƒ½åŠ›æœ‰æ¯”è¾ƒé«˜çš„è¯‰æ±‚ã€‚é‚£ä¹ˆæˆ‘ä»¬ç°åœ¨çš„è¿™ä¸ªstate ofçš„compilrçš„æŠ€æœ¯å¯èƒ½è¿˜æ²¡æœ‰å®Œå…¨è¾¾åˆ°è¿™ä¸ªè¿™ä¸ªè¿™ä¸ªå‚å•†çš„è¦æ±‚ã€‚å•Šã€‚

å½“ç„¶æˆ‘æˆ‘æˆ‘æˆ‘ä»¬ä¹Ÿè®¤è¯†åˆ°å°±æ˜¯å¤šå°±æ˜¯å¼€å‘è¿™ç§å¤šåç«¯çš„ç¼–è¯‘å™¨æ˜¯å‘ƒå…·æœ‰æå¤§çš„è¿™ä¸ªæŠ€æœ¯æŒ‘æˆ˜æ€§çš„ã€‚æœ€åå°±æ˜¯æˆ‘ä»¬é€šè¿‡é‚£ä¸ªé‚£ä¸ªé‚£ä¸ªè°ƒç ”çš„ç»“æœï¼Œæˆ‘ä»¬çœ‹åˆ°å°±æ˜¯å‘ƒæˆ‘ä»¬æ™®éè®¤ä¸ºå°±æ˜¯æ¡†æ¶è‡ªå¸¦çš„è¿™ä¸ªç¼–è¯‘å™¨ã€‚

å½“ç„¶è¿™ä¸ªè¿™ä¸ªæ¯”å¦‚è¯´è¿™ä¸ªtochåŠ ä¸Štritonå¯¹å¯¹å¯¹å¯¹ï¼Œè¿™è¿™ä¸ªåˆšæ‰é‚£ä¸ªå½­å·²ç»ä»‹ç»äº†è¿™ä¸ªå·¥ä½œéå¸¸ä¼˜ç§€çš„ã€‚ç„¶åé‚£ä¸ªåº”ç”¨æ€§å•Šæ€§èƒ½éƒ½éå¸¸ä¼˜è¶Šã€‚ä½†æ˜¯å‘¢å‘ƒåœ¨è¿ç§»æ€§å’Œé‚£æ‰©å±•æ€§ã€‚å› ä¸ºå®ƒä¸»è¦é¢å‘è¿™ä¸ªpytochå‘ƒã€‚

è¿™ä¸ªè‡ªèº«çš„æ¡†æ¶ã€‚æ‰€ä»¥åœ¨è¿ç§»æ€§å’Œé‚£æ‰©å±•æ€§ä¸Šå¯èƒ½è¿˜æ˜¯å­˜åœ¨ä¸€å®šçš„è¿™ä¸ªå±€é™æ€§çš„ï¼Œæˆ–è€…è‡³å°‘å¤§å®¶å‘ƒæ„Ÿå®˜æˆ–è€…è¯´å°è±¡æ˜¯è¿™æ ·çš„ã€‚å—¯ï¼Œå°±æ˜¯å‘ƒå¦å¤–å°±æ˜¯è¿™ä¸ªå‘ƒè¿™ä¸ªé¢å‘è¿™ä¸ªå¯è¿ç§»æ€§ï¼Œè¿˜æœ‰æ‰©å±•æ€§çš„è¯ã€‚

å°±æ˜¯å‘ƒæˆ‘ä»¬ä¸»è¦ä¸»è¦çš„è¿™ä¸ªè¿™ä¸ªæŠ€æœ¯æŒ‘æˆ˜æ€§ï¼Œå°±æ˜¯å¦‚ä½•è®¾è®¡æ›´å¥½çš„ç¡¬ä»¶æŠ½è±¡ã€‚å¯¹ã€‚OKå°±è¿™å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬å‘ç°å°±æ˜¯æ•´ä¸ªè¿™ä¸ªå‘ƒè¿™ä¸ªAIç¼–è¯‘å™¨æŠ€æœ¯è¿˜éå¸¸æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ç„¶åé‚£ä¸ªå‘ƒæˆ‘ä»¬ä¹Ÿä¸å¯èƒ½è§£å†³æ‰€æœ‰çš„é—®é¢˜ã€‚

ä½†æ˜¯æˆ‘ä»¬å›è¿‡å¤´æ¥çœ‹å‘¢ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯æœ‰ä¸€ä¸ªé‚£ä¸ªè¿™ä¸ªç§‘æŠ€éƒ¨çš„é¡¹ç›®åšæ”¯æ’‘çš„æœ‰3å¹´çš„è¿™ä¸ªè®¡åˆ’ã€‚æˆ‘ä»¬è¿˜æ˜¯æƒ³ä»å¤´æ¥åšä¸€ä¸ªç¨å¾®å®Œæ•´ä¸€ç‚¹çš„è¿™ä¸ªç¼–è¯‘å™¨ã€‚ç„¶åæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬ä¸»è¦æœ‰ä¸‰ä¸ªç›®æ ‡ã€‚

ç¬¬å°±æ˜¯æˆ‘ä»¬æƒ³å»æ„å»ºä¸€ä¸ªç»Ÿä¸€ä¸­ç«‹çš„è¿™ä¸ªä¸‹ä¸€ä»£AIç¼–è¯‘å™¨çš„åŸºç¡€è®¾æ–½ã€‚ç„¶åå‘ƒæ¯”å¦‚è¯´æˆ‘ä»¬ç”¨é€šç”¨çš„é«˜å±‚è¡¨ç¤ºï¼Œç„¶åç”¨è¿™ä¸ªå¯æ‰©å±•å¤šå±‚çš„æŠ½è±¡ï¼Œè¿™ä¸ªæœ‰ç‚¹åƒMRä¸€æ ·çš„å¯¹å‘ƒæ”¯æŒçµæ´»çš„åŸç¼–ç¼–è¯‘ã€‚

å°±æ˜¯åˆšæ‰é‚£ä¸ªberä»‹ç»çš„è¿™ä¸ªæˆ‘ä¹Ÿæ˜¯æˆ‘éå¸¸å–œæ¬¢MRæœ€è¿‘çš„ä¸€ä¸ªé‚£çš„è¿™éƒ¨åˆ†ã€‚å¯¹è¿™æ ·å°±å¯ä»¥æŠŠè¿™ä¸ªè¿™ä¸ªå‘ƒç¼–è¯‘å™¨çš„ä¼˜åŒ–è·Ÿè¿™ä¸ªningå•Šï¼Œè¿˜æœ‰è¿™ä¸ªoppositionå¯ä»¥è§£è€¦ã€‚å¯¹å‘ƒã€‚

å¦å¤–å°±æ˜¯å‘ƒå¦‚æœä½ æƒ³è¿™ä¸ªèƒ½å¤Ÿå®ç°åº•å±‚ç¼–è¯‘å™¨è·¨å¹³å°çš„è¿™ä¸ªè¿ç§»çš„è¯ï¼Œéœ€è¦ä¸€ä¸ªç²¾å‡†çš„ã€‚ä»£ä»·æ¨¡å‹ã€‚è¿˜æœ‰å°±æ˜¯æˆ‘ä»¬ç°åœ¨MRä¹Ÿåšçš„éå¸¸å¥½ï¼Œå°±æ˜¯structure cogenè¿™ä¸ªæ˜¯æ˜¯ã€‚

ä¹Ÿå°±è¯´æˆ‘ä»¬æˆ‘ä»¬æƒ³å»builä¸€ä¸ªè¿™ç§ä¸­ç«‹çš„ä¸‹ä¸€ä»£çš„AIå˜è¯‘å™¨ï¼Œè€Œä¸æ˜¯é¢å‘å•ä¸€çš„è¿™ä¸ªè¿™ä¸ªå‰ç«¯çš„æ¡†æ¶ã€‚ç„¶åç¬¬äºŒä¸ªç›®æ ‡å°±æ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„è¿™ä¸ªIç¼–è¯‘å™¨çš„è¿™ä¸ªinrastructureã€‚å‘ƒã€‚

èƒ½å¤Ÿæˆä¸ºè¿™ä¸ªç ”ç©¶å’Œäº§å“è½åœ°çš„ä¸€ä¸ªå¼€æºçš„å¹³å°ã€‚ç„¶åç¬¬ä¸‰ä¸ªç›®æ ‡å°±æ˜¯è¯´æˆ‘ä»¬èƒ½å¸Œæœ›ä»–èƒ½çœŸæ­£çš„å»å¸®åŠ©è¿™ä¸ªè¿™ä¸ªå‚å•†å»é€‚é…è¿™ä¸ªæ¯”å¦‚åƒå¤§æ¨¡å‹çš„è¿™æ ·çš„åº”ç”¨ã€‚å¯¹ã€‚å‘ƒï¼Œæ•´ä¸ªè¿›å±•çš„è¯å°±æ˜¯è¯´æˆ‘ä»¬å»å¹´ä¸»è¦æ˜¯åœ¨è¿™ä¸ªè°ƒç ”ã€‚

è¿˜æœ‰è¿™ä¸ªç«‹é¡¹çš„è¿‡ç¨‹å½“ä¸­ã€‚å¯¹å‘ƒï¼ŒåŒ…æ‹¬è¿™ä¸ªå»å‘ƒå‘ƒå¾é›†è¿™ä¸ªæ”¯æ´å­¦è€…ã€‚å¯¹ï¼Œç„¶åæˆ‘ä»¬æ­£å¼çš„é‚£ä¸ªå¼€å‘åº”è¯¥æ˜¯ä»ä»Šå¹´å•Š12æœˆä»½å¼€å§‹çš„ã€‚å¯¹ï¼Œç„¶åç°åœ¨çš„å‘ƒè¿™ä¸ªè¿™ä¸ªçŠ¶æ€å°±æ˜¯è¯´å‘ƒæˆ‘ä»¬è¿™ä¸ªæˆ‘æˆ‘ä»¬ä»è¿™ä¸ªpyto2ã€‚æˆ‘ä»¬ä»pyæ‹–2ã€‚

0å¼€å§‹å»æ”¯æŒã€‚ç„¶åæˆ‘ä»¬èƒ½loweråˆ°è¿™ä¸ªå°±æ˜¯æˆ‘ä»¬æ•´ä¸ªè¿™ä¸ªlowæ¡†æ¶æ˜¯å·²ç»æ‰“å¥½äº†ã€‚ä½†æ˜¯æˆ‘ä»¬ç°åœ¨å°±æ˜¯è¿˜æ²¡æœ‰å®Œå…¨å®ç°è¿™ä¸ªåç«¯ã€‚å¯¹å¯¹å¯¹å¯¹å¯¹ã€‚è¿™æ ·çš„ä¸€ä¸ªçŠ¶æ€å•Šï¼Œä¸‹é¢çš„è¯å°±æ˜¯è¯´ç”±æ—¶é—´çš„é™åˆ¶ã€‚

å°±æˆ‘ä»¬æˆ‘ä»¬ç›´æ¥å°±æ˜¯å‘ƒå°±æ·±å…¥åˆ°æˆ‘ä»¬ç°åœ¨ä¸€äº›æŠ€æœ¯çš„ç»†èŠ‚ã€‚å¯¹ï¼Œå°±æ˜¯æˆ‘ä»¬ç°åœ¨å°±æ˜¯è¯´å› ä¸ºæˆ‘ä»¬æƒ³åšä¸€ä¸ªç¨å¾®å®Œæ•´çš„ä¸€ç‚¹çš„è¿™æ ·çš„ä¸€ä¸ªç¼–è¯‘å™¨çš„æ¶æ„ã€‚å°±æ˜¯æ•´ä½“çš„è¿™ä¸ªæ€è·¯çš„è¯å•Šã€‚

è¿˜æ˜¯å»ºç«‹åœ¨å°±æ˜¯æœ€è¿‘è¿™å‡ å¹´çš„ç¼–è¯‘å™¨æ•´ä¸ªçš„è¿™ä¸ªå‘å±•åŸºç¡€ä¹‹ä¸Šçš„ã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬è¿˜æ˜¯åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ã€‚ç¬¬ä¸€ä¸ªéƒ¨åˆ†å°±æ˜¯é«˜å±‚è·Ÿè¿™ä¸ªbackendè·Ÿåç«¯æ— å…³çš„è¿™ä¸ªé‚£é‚£æˆ‘ä»¬å‰ç«¯çš„è¯ã€‚

å½“ç„¶æˆ‘ä»¬é€šè¿‡åƒè¿™ä¸ªdynamoå•Šå‘ƒè¿™ä¸ªå‘ƒgraphra capturingè¿™æ ·çš„æŠ€æœ¯ã€‚å‘ƒï¼Œæˆ–è€…æ˜¯å¦‚å¦‚æœæ˜¯ä¸€ä¸ªgraphra frameworkå“ˆï¼Œé‚£æˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥importå®ƒçš„è¿™ä¸ªgraphraã€‚

ç„¶åå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥æœ‰å¤šä¸ªå‰ç«¯æŠŠä¸åŒçš„æ¡†æ¶çš„è¿™ä¸ªå‘ƒè¿™ä¸ªæ¨¡å‹æˆ–è€…æ˜¯éƒ¨åˆ†çš„è®¡ç®—å›¾èƒ½lowåˆ°æˆ‘ä»¬çš„graphra IRä¸Šå»ã€‚æ‰€è°“çš„graphra Içš„è¯ï¼Œå°±æ˜¯å‘ƒç”¨æˆ‘ä»¬çš„è¿™ä¸ªè¿™ä¸ªä¸­é—´è¡¨ç¤ºè¡¨ç¤ºçš„ä¸€ä¸ªè®¡ç®—å›¾ã€‚

ç„¶åæˆ‘ä»¬åœ¨ä¸Šé¢å»åšä¸€äº›é‚£ä¸ªç¡¬ä»¶æ— å…³çš„è¿™ä¸ªtranssã€‚å—¯ï¼Œæœ€åçš„è¯å°±æ˜¯è¯´æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ä¸€ä¸ªç›¸å¯¹å®Œæ•´çš„ä¸€ä¸ªä¸€ä¸ªtensorè¿™ä¸ªpenserè®¡ç®—ä¸­å¿ƒçš„è®¡ç®—å›¾ã€‚

ç„¶åä¹‹åçš„è¯æˆ‘ä»¬ä¼šå‘ƒç”¨è¿™ä¸ªå°±æ˜¯è¯´æˆ‘ä»¬åˆšæ‰è¯´åˆ°çš„machine models cost modelså•Šã€‚æˆ‘ä»¬å»æŠŠå®ƒè¿›è¡Œé‡æ–°çš„è¿™ä¸ªç®—å­æ‹†åˆ†å¼èåˆï¼Œç„¶åå‘¢å‘ƒå†æŠŠå®ƒloweråˆ°è¿™ä¸ªtçº§åˆ«è¿™Rä¸Šå»é‚£tç”Ÿçš„è¯ã€‚

æˆ‘ä»¬å¯èƒ½ä¼šä¼šåšä¸€äº›è¿™ä¸ªè¿›ä¸€æ­¥çš„è¿™ä¸ªé¢å‘ä»£ç ç”Ÿæˆçš„ä¼˜åŒ–ã€‚ä½†æ˜¯å¯èƒ½ä¼šå‘ç°è¿™ä¸ªä¼˜åŒ–çš„æ•ˆæœä¸å¥½ã€‚é‚£æˆ‘ä»¬ä¼šé‡æ–°å»è¿›å…¥è¿™ä¸ªä¼˜åŒ–çš„å¾ªä¸€ä¸ªéçš„è¿™ä¸ªå¾ªç¯ã€‚å¯¹è¿™æ•´ä½“çš„ä¸€ä¸ªé‚£ä¸ªæ˜¯ä¸ªæ¶æ„çš„æ€è·¯ã€‚

ç„¶åæˆ‘ä»¬çš„é‚£ä¸ªæ‰€ç”¨çš„ä¸­é—´è¯­è¨€æ˜¯æˆ‘ä»¬ç°åœ¨æˆ‘ä»¬è‡ªå·±é‡é‡æ–°è®¾è®¡çš„ä¸€ä¸ªä¸­é—´å«ba basic step intermediatemedia languageã€‚å®ƒæ˜¯ä¸€ç§æ¯”è¾ƒç®€å•çš„å°±æ˜¯è¯´ä¸€ä¸ªè½»é‡çº§çš„ä¸­é—´è¯­è¨€ã€‚

åé¢æˆ‘ä¼šä»‹ç»ä¸€ä¸‹ã€‚å‘ƒå“¦é‚£æˆ‘ä»¬è¿™å—ï¼Œå› ä¸ºåˆšæ‰é‚£ä¸ªæ˜¯ä¸€ä¸ªæ•´ä½“çš„ä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªé€šç”¨çš„è¿™ä¸ªæ¶æ„ã€‚å¯¹æŠ€æœ¯çš„æ¶æ„ã€‚è¿™ä¸ªæ˜¯æˆ‘ä»¬å‘ƒç°åœ¨è¿™ä¸ª prototypeå°±æ˜¯è¯´åŸå½¢çš„è¿™ä¸ªé¢å‘0çš„ä¸€ä¸ªè¿™ä¸ªæ˜¯å®ç°çš„ä¸€ä¸ªæ¶æ„é‚£æˆ‘ä»¬åˆšæ‰å°±æ˜¯æˆ‘å°±ç”¨è¿™ä¸ªygraphæ¥åšçš„å‰ã€‚

å› ä¸ºåˆšæ‰å½­ç»è¯´åˆ°äº†ï¼Œå°±æ˜¯è¯´å‘ƒå…¶å®æœ‰å¾ˆå¤šè¿™ä¸ªæ¡†æ¶çš„é‚£ä¸ªåŠŸèƒ½æ˜¯è·Ÿé‚£ä¸ªé‡åºå˜ç›¸ç®¡å°±æ˜¯è¯´è¿™ä¸ªè¿™ä¸ªgraphè¿˜æœ‰é‚£ä¸ªè¿˜æœ‰è¿™ä¸ªizationä»€ä¹ˆçš„è¿™ä¸ªæ˜¯åœ¨è¿™ä¸ªæ¡†æ¶çš„è¿™ä¸ªcomprå»å®Œæˆçš„å½“ç„¶è¿™ä¸ªè¿™å¯èƒ½ä¸æ˜¯åœ¨æ¡†æ¶compå°±æ˜¯graphå°±æ˜¯æœ¬èº«æ˜¯ä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªå˜æ¢çš„è¿™ä¸ªå·¥å…·äº†ã€‚

ç„¶åå‡ºæ¥ä¹‹åå‘¢ï¼Œå°±æ˜¯ä¸€ä¸ªé‚£æˆ‘ä»¬çš„è¾“å…¥çš„è¯å°±æ˜¯è¿™ä¸ªå¯¹ç„¶åæˆ‘ä»¬æŠŠå°±æ˜¯è¯´æˆ‘ä»¬ä¸Šå»å¯¹ç”¨loRæ¥è¡¨ç¤ºä¸€ä¸ªéƒ¨åˆ†çš„è®¡ç®—å›¾æˆ‘ä»¬çš„locomprå‘¢å°±æ˜¯åœ¨ã€‚

helloRä¸Šå»åšä¸€äº›è¿™ä¸ªè¿™ä¸ªSCåŸºSCçš„è¿™ç§data flowçš„ä¼˜åŒ–ã€‚å¯¹ï¼Œä½†æ˜¯ç°åœ¨è¿˜è¿˜æ²¡æœ‰æ­£å¼çš„å¼€å§‹ã€‚å› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯æˆ‘ä»¬é˜¶æ®µå°±æ˜¯æˆ‘ä»¬æŠŠè¿™ä¸ªè¿™ä¸ªhelloå·²ç»å®ç°å‡ºæ¥äº†ã€‚

ä½†æ˜¯è¿™ä¸ªä¼˜åŒ–çš„éƒ¨åˆ†è¿˜æ²¡æœ‰å®Œå…¨å¼€å§‹å‘¢ã€‚å¯¹ã€‚é‚£ä¸‹ä¸€ä¸ªé˜¶æ®µå°±æ˜¯è¿™sorryæˆ‘è¿™è¿™ä¸ªåé¢æ˜¯æœ‰ä¸€äº›è¿™ä¸ªå¯¹ï¼Œå°±æ˜¯å‘ƒæˆ‘ä»¬å›å¤´å°±çœ‹ä¸€ä¸‹è¿™ä¸ªè¿™ä¸ªlowerçš„è¿‡ç¨‹ï¼Œå°±æ˜¯lowerçš„è¿‡ç¨‹çš„è¯ã€‚

å…¶å®è·Ÿè¿™ä¸ªå‘ƒå‘ƒæˆ‘ä»¬è¿˜å€ŸåŠ©äº†FXçš„è¿™ä¸ªinrastructureä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬ç”Ÿæˆçš„åŒæ ·æ˜¯ä¸€ä¸ªä¸ªé‡Œé¢å°±çœ‹æˆ‘è¿™æˆ‘ä¸ªçš„å­æ˜¯å§ï¼Ÿ

é‚£æˆ‘ returnä¸ª nodeå®é™…ä¸Šæ˜¯è¿™ä¸ªæŒ‡å‘çš„ä¸€ä¸ªå‡½æ•°æ˜¯æˆ‘ä»¬çš„buer functionã€‚ç„¶åæˆ‘ä»¬å†è·‘ä¸€è¿™ä¸ªå°±ç”Ÿæˆæˆ‘ä»¬çš„çš„ã€‚

å¯¹å› ä¸ºæˆ‘ä»¬çš„æ–¹æˆ‘ä»¬çš„è¿™ä¸ªintermedia languagethon bindingã€‚å¥½ï¼Œé‚£ä¸­é—´è¿™æ®µæˆ‘ä»¬åˆšåˆšä¹Ÿè¯´äº†ã€‚

å°±æ˜¯è¿™ä¸ªæ˜¯æˆ‘ä»¬çš„å‘ƒè¿™ä¸ªä¸­é—´è¡¨ç¤ºé‚£æˆ‘ä»¬çš„compå®é™…ä¸Šæ˜¯ç”¨raæ¥å®ç°çš„å¯¹é‚£æˆ‘æˆ‘è¿™ä¸ªè¿™æ˜¯ä¸€ä¸ªé‚£ä¸ªæˆ‘ä»¬çš„ä¸€ä¸ªå°IRçš„ä¸€ä¸ªå‰ç«¯çš„è¡¨ç¤ºã€‚

å¯¹æˆ‘ä»¬ä¼šä¸Šé¢å»åšä¸€äº›ä¸ªå‰ç«¯çš„è¿™è¿™ä¸ªSçº§åˆ«çš„è¿™è¯åé¢çš„è¯æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªå°±æ˜¯è¯´è¿™ä¸ªæ˜¯æ¯”è¾ƒçš„ä¸€ä¸ªä¸€ä¸ªæŠ€æœ¯é€‰æ‹©ã€‚

å°±æ˜¯æˆ‘ä»¬ç°åœ¨å‘ƒå¸Œæœ›èƒ½å¤Ÿé¢å¯¹è¿™ä¸ªå¤šç§è¿™ä¸ªè¿™ä¸ªå¼‚ç¡¬ä»¶æ¥æ¥åšä¸€ä¸ªè¿™ä¸ªå‘ƒé€šç”¨çš„è™šæ‹Ÿçš„è¿™ä¸ªruntimeè¿™ä¸€å±‚ã€‚åä»–ä»¬åªè¦å»é€‚é…è¿™äº›è¿™ä¸ªçš„è¯å°±èƒ½æ¥å—è¿›æ¥ã€‚å¯¹åé¢è¿™ä¸ªæ˜¯æˆ‘ä»¬ç°åœ¨æˆ‘ä»¬æ‰“ç®—å¼€å±•å‘¢ã€‚

å°±æ˜¯æˆ‘ä»¬ç°åœ¨è¿˜æ²¡åšè¿™ä¸ªçš„IRè¿™ä¸ªä»Šå¹´çš„Q3Qçš„è¯ï¼Œåº”è¯¥æ˜¯ä¸»è¦çš„è¿™ä¸ªç›®æ ‡æ˜¯è¿™ä¸ªè¿™ä¸ªRçš„åº”è¯¥æ˜¯ä»¥ä¸ªä¸ºä¸­æ“ä½œä¸ºä¸­å¿ƒçš„è¿™ä¸ªç¼–çš„è¿‡ç¨‹ã€‚å¯¹å¥½ï¼Œé‚£å°±æ˜¯ä¸»è¦çš„æŠ€æœ¯éš¾ç‚¹æˆ–è€…æŠ€æœ¯ç‚¹çš„è¯ï¼Œæ˜¯è¿™ä¸ªRå¯¹æˆ‘ä»¬å¸Œæœ›ä»–èƒ½å¤Ÿæ˜¯æä¾›ä¸€ä¸ªã€‚

ç¨³å®šçš„è¿™ä¸ªä¸Šå±‚æ¡†æ¶æ¥å…¥çš„æ¥å£ã€‚ç„¶åå®ƒçš„æŠ½è±¡å±‚æ¬¡æ˜¯è·¨è¶Šè¿™ä¸ªä¸Šå±‚è·Ÿåº•å±‚ã€‚è¿™æ ·æˆ‘å¯ä»¥ç”¨å®ƒå»å®ç°è¿™ä¸ªhigh levelçš„opå’Œè¿™ä¸ªlow levelçš„è¿™ä¸ªcodeæ ˆã€‚å¯¹ï¼Œå¥½ã€‚

é‚£æˆ‘ä»¬æ•´ä¸ªè¿™ä¸ªfagIRå°±æ˜¯è¯´æˆ‘ä»¬æ•´ä¸ªè¿™ä¸ªè¿™ä¸ªcompilerå°±ä¸ªç«¯åˆ°ç«¯çš„é¡¹ç›®ï¼Œæˆ‘å«fag IRå°±æ˜¯å®ƒçš„æŠ€æœ¯è·¯çº¿æˆ‘ä»ç„¶æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å±‚æ¬¡åŒ–æ¨¡å—åŒ–çš„è¿™ä¹ˆä¸€ä¸ªå¯¹å‘ƒä¸€ä¸ªè®¾è®¡ã€‚å¯¹ã€‚

ç„¶åbaelæœ¬èº«çš„è¿™ä¸ªä¸­é—´è¡¨ç¤ºæˆ‘ä»¬å€Ÿé‰´MRçš„è¿™ç§å±‚æ¬¡åŒ–çš„è¿™ä¸ªè®¾è®¡ã€‚å¯¹ï¼Œä½†å°±å°±æ˜¯è¯´è¿™ä¸ªåé¢æˆ‘ä¼šå› ä¸ºæ—¶é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬ä¼šå¿«ä¸€ç‚¹ä»‹ç»ã€‚å¯¹ã€‚å‘ƒï¼Œè¿™è¿™ä¸ªæ˜¯ç»™å¤§å®¶çœ‹ä¸€ä¸‹æˆ‘ä»¬è¿™ä¸ªbasil coreIRçš„è¿™ä¸ªæƒ…å†µã€‚å¯¹ã€‚

å› ä¸ºæˆ‘ä»¬è¿™å—çœ‹èµ·æ¥è·Ÿè¿™ä¸ªMRæ˜¯æœ‰ç‚¹ç›¸ä¼¼ä¹‹å¤„çš„å¯¹ã€‚å‘ƒï¼Œä½†æ˜¯æˆ‘ä»¬æˆ‘ä»¬ç›¸å½“æˆ‘ä»¬å¼ºåŒ–äº†è¿™ä¸ªregion payloadã€‚

å°±æ˜¯è¯´æˆ‘ä»¬åœ¨è¿™ä¸ªå‘ƒè¿™ä¸ªå‘ƒcope capturingå°±æ˜¯environment capturingä¸Šé¢ï¼Œæˆ‘ä»¬æˆ‘ä»¬å¼ºè°ƒä¸€ä¸‹ï¼Œå°±æ˜¯regionå®ƒæ˜¯n captureçš„è¿™ä¸ªè¿™ä¸ª payloadã€‚å¯¹çš„ã€‚

è¿™ä¸ªregionçš„è¯æ˜¯ç”¨è¿™ä¸ªåŒèŠ±å—å—è¡¨ç¤ºçš„å‘ƒï¼Œå•å—çš„è¯æ˜¯ä¸€ä¸ªblock blocklockçš„è¯ï¼Œå®ƒä¼šå¯captureå®ƒçš„ environmentvimentsã€‚

æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªblockæ¥å®šä¹‰ä¸€ä¸ª functionï¼Œä¸€ä¸ªclosureéƒ½å¯ä»¥FNçš„è¯å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„ä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªoperationã€‚å•Šï¼Œé‚£è¿™ä¸ªä»–å°±æ˜¯å‘ƒç®€å•çš„å»çœ‹äº†å®ƒè¿™ä¸ªä¸€ä¸ªæ•´ä¸ªçš„è¿™ä¸ªIRçš„ç»“æ„ã€‚

å½“å½“ç„¶æˆ‘ä»¬è¿™ä¸ªä¸Šé¢æ˜¯å®ƒçš„ideçš„è¿™ä¸ªè¯­æ³•ï¼Œè¿˜æœ‰å®ƒçš„è¿™ä¸ªå†…éƒ¨çš„ç»“æ„ã€‚ç„¶åæˆ‘ä»¬çš„è¿™ä¸ªè¡¨ç¤ºçš„è¯ï¼Œæˆ‘ä»¬æ“ä½œå°±æ˜¯å‚æµ‹çº§åˆ«çš„æ“ä½œçš„è¯ã€‚

æˆ‘ä»¬æ˜¯æŠŠå®ƒå°±æ˜¯åˆ†ç±»æˆè¿™ä¸ªä¸åŒç±»å‹çš„è¿™ä¸ªerè¿™ä¸ªç±»å‹çš„è¯å°±æ˜¯è¯´æˆ‘ä»¬å¸Œæœ›å®ƒæ˜¯æ¯”è¿™ä¸ªåƒtoï¼Œè¿˜æœ‰è¿™ä¸ªä»€ä¹ˆå‘ƒå‘ƒsableOç¨å¾®ä½ä¸€ç‚¹çš„å±‚æ¬¡ã€‚ä½†æ˜¯æ¯”è¿™ä¸ªgeneè¦ç¨å¾®é«˜ä¸€ç‚¹çš„å±‚æ¬¡ã€‚å¯¹å‘ƒï¼Œé‚£æˆ‘ä»¬æŠŠå®ƒåˆ†æˆå››ç±»ã€‚

ç¬¬ä¸€ç±»çš„è¯å°±æ˜¯åªæ¶‰åŠåˆ°index mapçš„è¿™ç§viewæ“ä½œï¼Œå†åŠ ä¸ŠåŸºäºè¿™ä¸ªviewçš„ copypyæ“ä½œã€‚ç¬¬äºŒä¸ªå‘¢å°±æ˜¯ä¸€ä¸ªmapç±»å‹çš„æ“ä½œï¼Œç„¶åæ˜¯reduceç±»å‹æ“ä½œä¸“ä¸šç±»å‹æ“ä½œä¸ºä»€ä¹ˆè¿™ä¹ˆåˆ†å‘¢ï¼Ÿ

å°±æ˜¯è¯´ä»–ä»¬çš„å®é™…ä¸Šä»–ä»¬å°±æ˜¯è¯´å‘ƒä»–ä»¬çš„è¿™ä¸ªè¿™ä¸ªä»–ä»¬çš„è®¡ç®—çš„è¿™ä¸ªå¤æ‚æ€§ï¼Œè¿˜æœ‰è¿™ä¸ªå­˜çš„è¿™ä¸ªç‰¹æ€§æ˜¯æœ‰æ˜æ˜¾çš„åŒºåˆ«çš„ï¼Œæˆ‘æˆ‘ä»¬æŠŠå®ƒåˆ†æˆè¿™ä¸ªå››ç±»ã€‚å¦‚æœè¿™å‘ƒå°±æ˜¯ä»¥åå¦‚æœæœ‰å¿…è¦å»è¿™ä¸ªè®¾è®¡æ–°çš„è¿™ä¸ªç±»å‹ã€‚å‘ƒã€‚

çš„è¿™ä¸ªè¿™ä¸ªæ“ä½œçš„è¯ä¹Ÿä¹Ÿæ˜¯æœ‰å¯èƒ½ã€‚ä½†æ˜¯æˆ‘ä»¬è§‰å¾—ç°åœ¨çš„è¿™ä¸ªæ¯”å¦‚ä»¥è½¬é€ä¸ºä¸»çš„è¯ï¼ŒåŸºæœ¬ä¸Šéƒ½è½åˆ°è¿™å‡ ä¸ªé‡Œé¢å»ã€‚å—¯ï¼Œç„¶åä¸¾ä¸€ä¸ªä¾‹å­ï¼Œå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªviewï¼Œå°±æ˜¯viewçš„è¯ã€‚

åŸºæœ¬ä¸Šå°±å¯ä»¥æŠŠè¿™äº›è¿™ä¸ªtensorçš„è¿™ä¸ªindexå‘ƒè¿™ä¸ªã€‚å‘ƒï¼Œè¿™ä¸ªå˜æ¢çš„ä¹‹ç±»çš„æ“ä½œéƒ½ä¸€ç½‘æ‰“å°½äº†ã€‚æ¯”å¦‚è¯´transpose broadcastresha sliceå•Šã€‚

feæˆ‘ä»¬éƒ½æ˜¯è¿™ä¸ªéƒ½å¯ä»¥ç”¨viewviewæ¥è¡¨ç¤ºã€‚ç„¶åå‘¢ï¼Œé‚£ä¸ªåœ¨viewé‡Œé¢è¿™ä¸ªpayloadæ˜¯ç”¨æ¥åšä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¿™ä¸ªpayloadå®šä¹‰äº†ä¸€ä¸ªF find index mapã€‚é‚£æˆ‘ä»¬å¯ä»¥çœ‹æœ€åä¸€ä¸ªä¾‹å­ã€‚

æ¯”å¦‚è¯´æˆ‘ä»¬è¿™view Xæ˜¯ä¸€ä¸ª4ä¹˜5çš„ä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªtensorã€‚ç„¶åæŠŠå®ƒè¿™ä¸ªå‘ƒæ¯”å¦‚åƒæˆ‘ä»¬è¦åšä¸€ä¸ªé‚£ä¸ªconvolutionçš„è¯ï¼Œæ˜ å°„æˆä¸€ä¸ªå››ç»´çš„tenserã€‚

é‚£å®ƒçš„è¿™ä¸ªindexå¯¹åº”å…³ç³»å®é™…ä¸Šå°±æ˜¯HIJKæ˜ å°„æˆHåŠ JåŠ å‘ƒé‚£ä¸ªIåŠ Kå¯¹ã€‚å‘ƒï¼Œmapç±»å‹çš„è¯å¾ˆç®€å•ï¼Œå°±æ˜¯è¯´ä»–è¦ä¹ˆå°±æ˜¯è¯´å®ƒæ˜¯æ²¡æœ‰è¾“å…¥çš„ã€‚

é‚£ä¹ˆå®ƒçš„é‚£ä¸ªå‘ƒé‚£è¡¨ç¤ºå°±æ˜¯è¯´å®ƒçš„è¿™ä¸ªå‘ƒå•ä½çš„å°±æ˜¯è¯´å®ƒçš„è¾“å‡ºä»…ä¾èµ–æœ€å¤šæ¯ä¸ªè¾“å…¥çš„ä¸€ä¸ªå•ä¸ªçš„å•å…ƒçš„æ“ä½œã€‚è¿™ä¸ªå«mapã€‚é‚£å®ƒæœ‰å‡ ä¸ªä¸åŒçš„è¿™ä¸ªæ“ä½œå•Šï¼Œå°±æ˜¯æ¯”å¦‚è¯´è¿™ä¸ªè¿™ä¸ªssper mapã€‚

é‚£å°±æ˜¯è¯´å®ƒä¸éœ€ä¸å»ä¾èµ–ä»»ä½•çš„è¿™ä¸ªè¾“å…¥ã€‚ä½†æ˜¯å‘¢å®ƒçš„è¾“å‡ºå•å…ƒçš„å€¼è·Ÿåªè·Ÿå®ƒçš„é‚£ä¸ªindexç›¸å…³ã€‚ç¬¬äºŒä¸ªå‘¢å°±æ˜¯æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç®€å•çš„è¿™ä¸ªmapç±»å‹çš„è®¡ç®—ã€‚

å¯¹æˆ‘ä»¬è®¡ç®—æ˜¯å…ˆåšäº†ä¸€ä¸ªå‡æ³•æˆ‘åšäº†ä¸€ä¸ªexponentialçš„è®¡ç®—ã€‚é‚£è¿™ä¸ªpayloadå®é™…ä¸Šå®šä¹‰çš„æ˜¯ä¸€ä¸ªsizeä¹‹åçš„å°±map mapapæ˜¯ä¸€ä¸ªtensorè®¡ç®—ã€‚

é‚£é‚£ä¸ªé‚£ä¸ªè¿™ä¸ªmæ¥çš„è¿™ä¸ªå®ƒå®šä¹‰çš„ä¸ªorä¹‹åçš„è¿™ä¸ªè®¡æŠŠä¸ª5ä¹˜3çš„5ä¹˜3çš„ï¼Œç„¶åç”¨ä¸€ä¸ªè¿™ä¸ªè¿™ä¸ªå‡½æ•°æ¥è¡¨ç¤ºreduceçš„è¯å°±æ˜¯æˆ‘ä»¬ç”¨æ¥å»è¡¨ç¤ºé™ç»´çš„è¿™ä¸ªæ“ä½œã€‚

å¯¹ç„¶åè¿™ä¸ªå‘ƒX reduceè¯å°±å¯ä»¥ç”¨æ¥è¡¨ç¤ºä¸€ç³»åˆ—çš„è¿™ç§æˆ‘åªåœ¨ç¬¬ä¸€ç»´åº¦å»reduceåŠ ä¸ŠåŠ ä¸Šè¿™ä¸ªsizeä¹‹æ“ä½œã€‚å°±æˆ‘åªåœ¨ç¬¬ä¸€ç»´åº¦å»æˆ‘å°±ä»¥ç”¨è¿™ä¸ª reduceä¸€ç³»åˆ—çš„è¿™ç§éå¸¸é€šç”¨çš„è¿™ä¸ªç®—èåˆçš„è¿™ä¸ªæ“ä½œå•Šã€‚

é‚£é‚£ä¸ªjoçš„è¯å®é™…ä¸Šå°±æ˜¯ç›¸å½“äºæ˜¯ä¸ªæˆ–é˜µæˆæ“ä½œçš„ä¸€ä¸ªæ‰©å±•ã€‚å®ƒè¡¨ç¤ºä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´æˆ‘ä»¬åœ¨è¿™å½“ç„¶æˆ‘ä»¬æœ‰ç»´åº¦æ˜¯ä¸å˜çš„ã€‚ä½†æˆ‘ä»¬ä¸­é—´çš„è¯æœ‰äº›ç»´åº¦å¯èƒ½æ˜¯éœ€è¦è¿™ä¸ªäº¤å‰ã€‚å¯¹ã€‚

ç„¶åæœ€ä½ç»´åº¦æˆ‘ä»¬å¯ä»¥éœ€è¦contraéœ€è¦éœ€è¦éœ€è¦æ¶ˆå»çš„ã€‚å°±è¿™æ˜¯ä¸€ä¸ªè¿™ä¸ªä¹Ÿæ˜¯ä¸€ä¸ªæ¯”è¾ƒé€šç”¨çš„æ¨¡å¼ã€‚ç„¶åå®ƒçš„è¿™ä¸ªå®é™…ä¸Šå®ƒçš„è®¡ç®—å¤æ‚åº¦æ˜¯æ¯”è¾ƒé«˜ã€‚è€Œä¸”å®ƒæ¶‰åŠåˆ°çš„data movementæ˜¯æ¯”è¾ƒå¤æ‚ã€‚

ä¹Ÿæ˜¯æˆ‘ä»¬éœ€è¦ç‰¹åˆ«è¦æ³¨æ„çš„å¯¹ç„¶åè¿™ä¸ªé’ˆå¯¹è¿™ä¸ªjoçš„æƒ…å†µæ˜¯è¿™æ ·è¿™ä¸ªä¾‹å­é‡Œé¢å¯èƒ½å°±æ˜¯ä¸€ä¸ªçŸ©æˆ˜æˆçš„ä¾‹å­ã€‚ç„¶åä»–æŠŠä¸€ä¸ªå°±æ˜¯100ä¹˜ä»¥5ä¹˜ä»¥1ä¹˜ä»¥3å•Šã€‚

100ä¹˜ä»¥1ä¹˜ä»¥4ä¹˜ä»¥3åˆ°100ä¹˜ä»¥5ä¹˜ä»¥4è¿™æ ·çš„ä¸€ä¸ªjoçš„æ“ä½œçš„ç±»å‹ã€‚é‚£æˆ‘ä»¬å°±ç”¨è¿™ä¸ªpayloadçš„3ä¹˜3åˆ°ä¸ªsræ¥è¡¨ç¤ºå‡ºæ¥çš„ã€‚å¯¹ï¼Œå°±æ˜¯è¿™è¿™ä¸ªåˆšæ‰è·Ÿalberè¯´çš„æœ‰ç‚¹å…³ç³»æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ

å°±æ˜¯è¯´å‘ƒæˆ‘ä»¬æœ€ç»ˆçš„è¿™ä¸ªå°±æ˜¯å¤–å±‚çš„è¿™ä¸ªå¾ªç¯å®é™…ä¸Šæ˜¯å¾ˆå®¹æ˜“è§£é‡Šå‡ºæ¥çš„ã€‚ä½†æ˜¯å†…å±‚çš„å¾ªç¯çš„è¯ï¼Œæ¯”å¦‚åœ¨payloadé‡Œé¢ï¼Œè¿™ä¸ªæˆ‘ä»¬å«sä¹Ÿä¹Ÿå¥½ï¼Œä»–å¯ä»¥ç”¨compræ¥å»åšæ·±å…¥çš„è¿™ä¸ªä¼˜åŒ–ã€‚å¯¹ã€‚

é‚£é‡Œé¢å¦‚æœå†lowerä¸‹å»çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¿™æ‰£æ ˆçš„IRå»æ¥è¡¨ç¤ºå®ƒã€‚Oå‘ƒä¸‹ä¸€æ­¥å°±æ˜¯è¯´æˆ‘å› ä¸ºæˆ‘ä»¬ç°åœ¨è®¾è®¡äº†é«˜å±‚çš„è¿™ä¸ªoperationã€‚é‚£æˆ‘ä»¬æˆ‘ä»¬ä¸‹ä¸€æ­¥å°±æ˜¯è¯´æˆ‘ä»¬è¦å»è®¾è®¡å’Œå®ç°æˆ‘ä»¬codeæ ˆçš„éƒ¨åˆ†ã€‚

å¯¹è¿™éƒ¨æˆ‘ä»¬æ‰“ç®—å°±æ˜¯åˆ†æˆå‡ ä¸ªlevelå»åšå°±å°±æœ€çš„è¯æˆ‘ä»¬å¸Œæœ›æ˜¯ä¸€ä¸ºä¸­å¿ƒè¿™æ ·ä¸€ä¸ªç¤ºã€‚å¯¹ç„¶åexä½œä¸º classå¤„ç†å®ƒåœ¨é¢å¯ä»¥ flowè¿™è¿™ä¸œè¥¿å¯¹å¯¹ç„¶åå†å¾€ä¸‹è¿™ä¸ªé™ä¸€çš„è¯å‘¢ã€‚

æˆ‘ä»¬å°±æŠŠæ˜¾ç¤ºçš„è¿™ä¸ªismè¿˜æœ‰dastructionå¯ä»¥åŠ ä¸Šå»ã€‚å¦å¤–å°±æ˜¯è¯´åœ¨è¿™ä¸€å±‚æˆ‘ä»¬å»åš bufferizationå¯¹ç¬¬ä¸‰å±‚ç¬¬ä¸‰å±‚å°±æ˜¯level threeçš„è¯ã€‚

æˆ‘ä»¬æ˜¯éœ€è¦èƒ½å¤Ÿè·Ÿè¿™ä¸ªåº•çš„ç¡¬ä»¶èƒ½å¤Ÿåšæ›´ç´§å¯†çš„æ˜ å°„å…³ç³»ï¼ŒåŒ…æ‹¬è¿™ä¸ªå°±æ˜¯low levelè¿™æ ·çš„è¿™ç»“æ„çš„æ¯”å¦‚ä¸ªä¹‹çš„æˆ‘éƒ½åŠ åŠ æœ€åå‘¢ã€‚æ¥è¯´åªæ˜¯ä¸€ä¸ªè¿™ä¸ªè¿™ä¸ªåç«¯çš„tartã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_25.png)

å¥½ï¼Œå°±è¿™ä¸ªç°åœ¨æ˜¯æˆ‘ä»¬çš„working progressçŠ¶æ€ã€‚å› ä¸ºæˆ‘ä»¬ç°åœ¨æ²¡æœ‰è¿˜æ²¡æœ‰å®Œå…¨è¿™ä¸ªåšå®Œï¼Œæ‰€ä»¥æ²¡æœ‰è¿™ä¸ªperformanceçš„é‚£ä¸ªdataç»™å¤§å®¶å±•ç¤ºã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_27.png)

å‘ƒï¼Œæˆ‘ä»¬æœªæ¥å‘å±•å®šå‘å‘ƒï¼Œå°±æ˜¯å®šä½å°±æ˜¯è¯´æˆ‘ä»¬å¸Œè¿˜æ˜¯å¸Œæœ›è·Ÿç°åœ¨ç°æœ‰çš„è¿™ä¸ªè¿™ä¸ªå¼€æºçš„ç¼–è¯‘å™¨æœ‰æ‰€åŒºåˆ†ã€‚å¯¹ï¼Œå°±æ˜¯å…¶å®æˆ‘ä»¬è§‚å¯Ÿå°±æ˜¯å¼€æºçš„ç¼–è¯‘å™¨ä»ç„¶å­˜åœ¨å¤§é‡çš„è¿™ä¸ªå­˜å­˜é‡é—®é¢˜ã€‚

æˆ‘ä¸è§‰å¾—å°±æ˜¯è¯´æˆ‘æˆ‘ä»¬ç°åœ¨å—¯å°±æ˜¯è¯´å®Œå…¨æ²¡æœ‰å¿…è¦å»å¼€å‘ä¸€ä¸ªæ–°çš„è¿™ä¸ªç¼–è¯‘å™¨ã€‚æˆ‘è§‰å¾—å®Œå…¨æ˜¯æœ‰è¿™ä¸ªå®¹é‡çš„ã€‚å°±æ˜¯è¯´æˆ‘ä»¬æˆ‘ä»¬æˆ‘ä»¬å‡ ä¸ªç‹¬ç«‹çš„ç¼–è¯‘å™¨ï¼Œç„¶åå»ç›¸äº’å€Ÿé‰´å‘å±•ã€‚è¿™ä¸ªå®Œå…¨æ˜¯æ˜¯å¯ä»¥çš„ï¼Œæ²¡é—®é¢˜ã€‚å¯¹ã€‚å‘ƒã€‚

ç„¶åå°±æ˜¯å°±æ˜¯å½“ç³»ç»Ÿå¤§åˆ°ä¸€å®šè§„æ¨¡ä¹‹åï¼Œå°±æ˜¯å®Œå¤‡åº¦æé«˜ï¼Œå°±æ„å‘³ç€æ˜“ç”¨æ€§çš„è¿™ä¸ªå¯å¯é æ€§çš„ä¸‹é™ã€‚æ‰€ä»¥å°±æ˜¯è¯´å‘ƒæˆ‘æˆ‘ä»¬çš„ç›®æ ‡ä¹Ÿä¸»è¦è§£å†³è¿™æ˜“å®¹æ€§å¯é æ€§çš„è¿™ä¸ªé—®é¢˜ã€‚å°±æ˜“ç”¨æ€§å¯é æ€§æ”¶ç›Šï¼Œå¯èƒ½ä¸äºšäºè¿™ä¸ªæ€§èƒ½çš„è¿™ä¸ªæ”¶ç›Šã€‚å¯¹ã€‚

ç„¶åæˆ‘ä»¬æ•´ä½“ç›®æ ‡å°±æ˜¯è¯´ä»ç„¶è¦ä¿æŒè½»é‡åŒ–ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿç”¨10%çš„å·¥ä½œæŠ•å…¥è§£å†³90%çš„é—®é¢˜å°±å¯ä»¥äº†ã€‚æ¾è€¦åˆå…¶å®æœ‰æœ‰ä¸¤ç‚¹è€ƒè™‘ã€‚

ç¬¬ä¸€ä¸ªå°±æ˜¯è¯´æˆ‘ä»¬ä¸è·Ÿè¿™ä¸ªç‰¹å®šçš„è¿™ä¸ªè¿™ä¸ªå‘ƒç†ç†è®ºä¸Šæˆ‘ä»¬ä¸è·Ÿç‰¹å®šçš„è¿™ä¸ªæ¡†æ¶æ¥è¿›è¡Œç´§è€¦åˆã€‚å¯¹å‘ƒï¼Œç¬¬äºŒç‚¹å°±æ˜¯è¯´å‘ä¸‹çš„è¯ï¼Œæˆ‘æˆ‘ä»¬ä¸å¸Œæœ›èƒ½å¤Ÿçƒ™è˜åˆ°è¿™ä¸ªdeviceè¿™ä¸ªè®¾å¤‡å•†çš„è¿™ä¸ªç”Ÿæ€é‡Œé¢å»ã€‚

ç„¶åç¬¬ä¸‰ä¸ªå°±æ˜¯æˆ‘ä»¬è¦å°±æ˜¯è¯´è¿™ä¸ªIcomperå°±æ˜¯è·Ÿä¼ ç»Ÿcomprä¸å¤ªä¸€æ ·ï¼Œå°±æ˜¯å®é™…ä¸Šè¿™ä¸ªè‡ªåŠ¨ä¼˜åŒ–çš„è¿™ä¸ªéœ€æ±‚è¿˜æ˜¯æ¯”è¾ƒå¼ºçš„å¯¹ï¼Œå› ä¸ºå®ƒé‡Œé¢æ¶‰åŠå¾ˆå¤šè¿™ä¸ªè¯­ç¼–è¯‘çš„è¿™ä¸ªå‚æ•°è°ƒä¼˜çš„è¿™ä¸ªé—®é¢˜ã€‚

æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿç»“åˆè¿™ä¸ªè‡ªåŠ¨è°ƒä¼˜æŠ€æœ¯æ¥å»æ”¹è¿›è¿™ä¸ªè¿™ä¸ªæ€§èƒ½ã€‚æŠ€æœ¯è·¯çº¿çš„è¯å°±æ˜¯è¯´æˆ‘ä»¬ä»ç„¶é‡ç‚¹è¦æ”¾åœ¨è¿™ä¸ªæ¯çš„å¹²é«˜å±‚çš„è¡¨ç¤ºï¼Œé‡ç‚¹è¡¨ç¤ºç®€çº¦å¯ç»„åˆæ€§ã€‚å¯¹ï¼Œè¿™æ ·å°±ä¸ºè¿™ç§å¤šå¤šç¡¬ä»¶çš„è¿™ä¸ªé€‚é…æ¥æä¾›è¿™ä¸ªæ¡ä»¶ã€‚

ç„¶åç¬¬äºŒç‚¹å°±è·Ÿå®è·µç›¸å…³çš„è¿™ä¸ªæˆ‘è§‰å¾—æ˜¯æˆ‘ä»¬ä¸€ä¸ªæˆ‘è§‰å¾—æ˜¯æ¯”è¾ƒæœ‰æ„ä¹‰çš„æ¢ç´¢ã€‚å°±æ˜¯æˆ‘ä»¬è¦é‡‡ç”¨æˆ‘ä»¬é‡‡ç”¨çš„æ˜¯æ¥å¼€å‘è¿™ä¸ªç¼–è¯‘åŸºç¡€è®¾æ–½ã€‚å› ä¸ºä»é•¿è¿œçœ‹ï¼Œæˆ‘ä»¬è§‰å¾—CåŠ åŠ çš„è¿™ä½œä¸ºè¿™ä¸ªstructureçš„åº•å±‚çš„è¯­è¨€ã€‚

å®ƒçš„codebaseçš„å¯ç»´æŠ¤æ€§å®‰å®‰å®‰å…¨æ€§ä¼šå¸¦æ¥å¾ˆå¤§çš„è¿™ä¸ªéšæ‚£ã€‚å°±æ˜¯5å¹´ä¹‹å1å¹´ä¹‹åçš„è¯ï¼Œæˆ‘è§‰å¾—CåŠ åŠ å¯èƒ½ä¼šé€æ¸é€€å‡ºèˆå°å°±æ˜¯è¯´å®ƒæ˜¯åŸºäºè¿™ä¸ªå…¼å…¼å®¹è¿™ä¸ªæ€§èƒ½è·Ÿå¯é åŠ å®‰å…¨æ€§çš„ã€‚æˆ‘è§‰å¾—ç›®å‰æ¥è¯´æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„é€‰æ‹©ã€‚

æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ¢ç´¢ä¸€ä¸‹è¿™ä¸ªè¿™ä¸ªæ–¹å‘ã€‚ç°åœ¨æœ‰è‡ªå·±çš„é‚£ä¸ª compileï¼Œæ¯”å¦‚è¯´ liftçš„å¯¹å§ï¼Ÿä½†æ˜¯å‘¢é¢å‘AIçš„è¿™ä¸ªå°±å°±æ˜¯è¯´æˆ‘ä»¬è¯´çš„è¿™ä¸ªacelerator compileç°åœ¨æ²¡æœ‰å…ˆä¾‹ã€‚

æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿåœ¨è¿™å„¿åšä¸€äº›æ¢ç´¢ã€‚å‘ƒå‘ƒç”Ÿæ€åˆä½œè¿™çš„è¯ï¼Œå°±æ˜¯è¯´æˆ‘ä»¬å¸Œæœ›èƒ½è·Ÿå‚å•†å…±åŒç»´æŠ¤è¿™ä¸ªåŸºç¡€è®¾æ–½ï¼Œè¿˜æœ‰åç«¯ä»£ç ç”Ÿæˆã€‚å‘ƒï¼ŒåŒæ—¶ä¹Ÿå¸Œæœ›è·Ÿè¿™ä¸ªè¿™ä¸ªç§‘ç ”å•ä½æ¥åˆä½œæ¢ç´¢è¿™ä¸ªç¼–è¾‘çš„å‰æ²¿æŠ€æœ¯ã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_29.png)

ä¸ºä»€ä¹ˆã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_31.png)

ã†ã€‚å¥½ï¼Œè°¢è°¢ç™½åšã€‚é‚£ä¸ªåˆšæ‰ç™½åšå‘ƒä»‹ç»çš„è¿™ä¸ªå‘ƒæ™ºæ´åœ¨AIç¼–è¯‘ä¸Šçš„è¿™ä¸ªæˆ‘ä»¬çš„é‚£ä¸ªæˆ‘ä»¬çš„é‚£ä¸ªå‘ƒç›®æ ‡å“ˆçš„ç¡®æ˜¯æŒºammbçš„é‚£è¿™å¯¹æˆ‘ä»¬æ¥è¯´å®é™…ä¸Šæ˜¯ä¸€ä¸ªä¸‰å¹´çš„reseé¡¹ç›®ä¹Ÿæ˜¯ä¸€ä¸ªæ•™ä¸€ä¸ªb batã€‚

é‚£æˆ‘ä»¬æ‰€ä»¥åœ¨ä»ä»Šå¹´å¼€å§‹ä¹Ÿä¼šåˆ©ç”¨åœ¨æ™ºæ´å¤§ä¼šçš„æœºä¼šã€‚æ¯å¹´éƒ½ä¼šç»™å¤§å®¶updateä¸€ä¸‹æˆ‘ä»¬çš„å¾€å‰èµ°çš„tageå‘ƒæˆ‘ä»¬çš„çŠ¶åšåˆ°ä»€ä¹ˆç¨‹åº¦ã€‚æˆ–è®¸æˆ‘ä»¬åˆ°äº†æ˜å¹´ä¹Ÿä¼šæœ‰ä¸åŒçš„æ–¹å‘ä¸Šçš„ï¼Œåœ¨ç¼–è¯‘å™¨æ–¹å‘ï¼Œæˆ‘ä»¬è¯¥åœ¨è¿™æ¡è·¯ä¸Šè¯¥æ€ä¹ˆèµ°ã€‚

ä¼šå¤§å®¶æ›´å¤šçš„ä¸€äº›ä¸åŒçš„æ€è€ƒã€‚å¥½å‘ƒå‘ƒæˆ‘æˆ‘æˆ‘ä»‹ç»å‘ƒå‘ƒä¸‹ä¸€ä½è®²è€…æ˜¯æ¥è‡ªäºå‘ƒæ˜†ä»‘æ–°çš„å‘ƒç½—èˆªã€‚ğŸ˜Šã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_33.png)

è¯¶ï¼Œ sorryã€‚ğŸ˜”ï¼Œæ˜¯æ˜¯ã€‚ğŸ˜”ï¼Œå“ç­‰ä¸€ä¸‹ã€‚ğŸ˜Šï¼ŒWhatjectã€‚Yeahï¼Œ let me introduce our last speakerã€‚ Mrã€‚ Luo Luoã€‚

 he's from the the the company called the Kunun chip chipetã€‚

 And it's one of the major chip to do the AI accelerationã€‚ And so less welcomeã€‚å¥½ï¼Œè°¢è°¢å¤§å®¶ã€‚å—¯ã€‚

é‚£æˆ‘å†ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹æˆ‘è‡ªå·±å’Œè¿™ä¸ªæ˜†ä»‘æ–°å§ã€‚é‚£ä¸ªæˆ‘æ˜¯æ¥è‡ªæ˜†ä»‘æ–°çš„ç ”å‘æ€»ç›‘ç½—èˆªã€‚é‚£æ˜†ä»‘æ–°å‘¢æ˜¯ä¸€å®¶æ¯”è¾ƒå¹´è½»çš„å…¬å¸ï¼Œåœ¨2021å¹´æ‹†åˆ†å‡ºæ¥ã€‚ä½†å®é™…ä¸Šå‘¢ï¼Œä»–å·²ç»æœ‰åå¹´çš„ä¸€ä¸ªå†å²ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ

æ˜¯åœ¨åŸå…ˆæ˜¯åœ¨ç™¾åº¦çš„å†…éƒ¨çš„ä¸€ä¸ªéƒ¨é—¨å«æ™ºèƒ½èŠ¯ç‰‡åŠæ¶æ„éƒ¨ã€‚é‚£ä½ çœ‹å¯ä»¥çœ‹åˆ°è¿™ä¸ªå‘ƒè¿™ä¸ªç…§ç‰‡å•Šå°±æ˜¯è¿™ä¸ªæ˜†ä»‘æ–°äºŒä»£èŠ¯ç‰‡ï¼Œæˆ‘ä»¬ç°åœ¨ä¸»æ¨çš„äºŒä»£èŠ¯ç‰‡çš„å®æ‹å›¾ã€‚æ‰€ä»¥è¿™å¼ å›¾å‘¢å°±æ˜¯å›é¡¾äº†æ•´ä¸ªæ˜†ä»‘æ–°çš„è¿™ä¸ªå†å²å•Šã€‚

è™½ç„¶è¯´æ˜¯æ‰æˆç«‹æ³•æ³•å¾‹æ„ä¹‰ä¸Šæ‰æˆç«‹ä¸¤å¹´çš„è¿™ä¸ªå…¬å¸ï¼Œä½†å®é™…ä¸Šè¿™ä¸ªæŠ€æœ¯å’Œäº§å“å·²ç»å·²ç»ç§¯ç´¯äº†10å¹´ã€‚æˆ‘ä»¬åœ¨2011å¹´çš„æ—¶å€™å°±å¼€å§‹åšè¿™ä¸ªFPGåŠFPGåšè¿™ä¸ªAIåŠ é€Ÿå™¨ã€‚å•Šï¼Œä¸€ç›´åˆ°2018å¹´çš„æ—¶å€™ï¼Œç¬¬ä¸€ä»£èŠ¯ç‰‡é‡äº§ã€‚

å¹¶ä¸”å¤§è§„æ¨¡æ¨å¹¿ã€‚åœ¨2021å¹´çš„æ—¶å€™ï¼Œç¬¬äºŒä»£èŠ¯ç‰‡é‡äº§å•Šï¼Œç°åœ¨ä¸»ä¸»æ¨çš„å°±æ˜¯ç¬¬äºŒä»£èŠ¯ç‰‡ã€‚å—¯ã€‚å¥½ï¼Œé‚£è¿›å…¥æ­£é¢˜å°±æ˜¯å‘ƒè¿™ä¸ªç¬¬ä¸€partå‘¢å°±æ˜¯å¯èƒ½æˆ‘ä¼šå‘ƒæœ‰ä¸€äº›è¿™ä¸ªå¸¦æ¥ä¸€äº›ä¸€äº›è¿™å¯¹è®¡ç®—äº§ä¸šå‘å±•çš„ä¸€ä¸ªåˆ†äº«å•Šã€‚

ä¸€ä¸ªé€»è¾‘çš„åˆ†äº«ã€‚å°±æ˜¯è¯´å‘ƒå¤§å®¶å¯èƒ½è¿‘ä¸¤å¹´å¬è¿™ä¸ªAIèŠ¯ç‰‡è¿™ä¸ªè¯å°±æ˜¯å¬çš„æ¯”è¾ƒå¤šå•Šï¼Œæœ‰æ²¡æœ‰æƒ³è¿‡ä¸ºä»€ä¹ˆè¿‘ä¸¤å¹´AIèŠ¯ç‰‡å•Šï¼Œå°±çªç„¶ç«èµ·æ¥ï¼Œä¸ºä»€ä¹ˆå‰ä¸¤å¹´æ²¡æœ‰å•Šï¼Œå…¶å®è¿™ä¸ªæ˜¯å¸¦æ¥æˆ‘ä¸ªäººçš„ä¸€äº›æ€è€ƒã€‚

å°±æ˜¯æˆ‘ç†è§£è¿™AIèŠ¯ç‰‡æ˜¯æ–°æ—¶ä»£è¿™ä¸ªè®¡ç®—äº§ä¸šå‘å±•çš„ä¸€ä¸ªå¿…ç„¶è¶‹åŠ¿ã€‚è¿™å¼ å›¾å‘¢æ˜¯ä»å·¦åˆ°å³æ˜¯ä¸€ä¸ªæ—¶é—´è½´å•Šï¼Œé‚£ä¸ªé‡çºµåæ ‡æ˜¯ä¸€ä¸ªæŠ½è±¡çš„ä¸€ä¸ªè®¡ç®—äº§ä¸šçš„ä¸€ä¸ªç³»ç»Ÿæ¶æ„å•Šã€‚

é‚£ä¹ˆä»å·¦åˆ°å³æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»æ•´ä¸ªè®¡ç®—äº§ä¸šçš„å‘å±•è¶‹åŠ¿è„‰ç»œæ¥è¯´ï¼Œæˆ‘ä»¬ç†è§£å¯ä»¥å‘å¯ä»¥å¤§è‡´åˆ’åˆ†ä¸º4ä¸ªæ—¶ä»£ï¼Œæœ€å·¦è¾¹å°±æ˜¯ä¸€ä¸ªä¸Šå¤æ—¶ä»£ã€‚è¿™ä¸ªä¸Šå¤æ—¶ä»£é¢å‘çš„è¿™ä¸ªç¾¤ä½“æ˜¯ç§‘å­¦å®¶çš„ç¾¤ä½“ã€‚å®ƒçš„èŠ¯ç‰‡ã€‚æ˜¯å®šåˆ¶åŒ–çš„å¤„ç†å™¨å‘ƒã€‚

æ“ä½œç³»ç»Ÿæ˜¯unuxã€‚å•Šï¼Œè¿™ä¸ªè¿™ä¸ªä¸Šä¸Šé¢è®¡ç®—çš„ä»»åŠ¡å‘¢ï¼Œä¸€èˆ¬æ˜¯ã€‚å‘ƒï¼Œè¿™ä¸ªå®éªŒå®¤é‡Œé¢çš„ä¸€äº›ç§‘å­¦è®¡ç®—å¯¹å§ï¼Ÿç„¶ååˆ°ç¬¬äºŒä¸ªæ—¶ä»£ï¼Œå¯èƒ½å¤§å®¶éƒ½æ¯”è¾ƒç†ŸçŸ¥çš„ï¼Œè¿™ä¸ªå°±æ˜¯PCæ—¶ä»£ã€‚

é‚£æˆ‘ä»¬PCæ—¶ä»£å¯ä»¥çœ‹åˆ°èŠ¯ç‰‡ä¸»æµçš„èŠ¯ç‰‡æ˜¯å‰86çš„èŠ¯ç‰‡ï¼Œæ“ä½œç³»ç»Ÿæ˜¯windowsçš„æ“ä½œç³»ç»Ÿã€‚åœ¨è¿™ä¸ªæ—¶å€™ï¼Œwindowså’Œå‰86è¿™ä¸¤ä¸ªå¼ºå¼ºè”åˆå½¢æˆäº†ä¸€ä¸ªéå¸¸å¼ºå¤§çš„wintailçš„ç”Ÿæ€ã€‚

åœ¨è¿™ä¸ªç”Ÿæ€ä¸Šè¯ç”Ÿäº†éå¸¸å¤šçš„ã€‚å‘ƒï¼Œä¸°å¾ˆä¸°å¯Œçš„æ¡Œé¢çš„åº”ç”¨å•Šã€‚æ¯”å¦‚è¯´æˆ‘ä»¬å¯èƒ½å°±æ˜¯å‘ƒå½“æ—¶èŠå¤©å·¥å…·QQå•Šæˆ–è€…æ˜¯æµè§ˆå™¨å•Šï¼Œæˆ–è€…æ˜¯ä¸€äº›å•æœºè·¯ç”±å™¨ã€‚è¿™ä¸ªæ—¶å€™è¿™ä¸ªæ—¶å€™çš„è¿™ä¸ªè”ç½‘çŠ¶æ€ã€‚

æ˜¯ä¸€ä¸ªå‘ƒå‘ƒè¿˜æ˜¯è§„æ¨¡æ¯”è¾ƒå°çš„ä¸€ä¸ªæœ‰çº¿å±€åŸŸç½‘ï¼Œå¯¹å§ï¼Ÿå—¯å—¯ï¼Œæƒ³æƒ³202000å¹´2000å¹´åˆçš„æ—¶å€™ï¼Œç½‘å§é‚£ä¸ªæ—¶å€™ç‰¹åˆ«ç«å—¯ï¼Œå°±æ˜¯å¯¹å§ï¼Ÿå°±è¿™ä¸ªæ—¶å€™æ˜¯PCæ—¶ä»£çš„ä¸€ä¸ªå…¸å‹çš„ä¸€ä¸ªç°è±¡ï¼Œåˆ°ç¬¬ä¸‰ä¸ªæ—¶ä»£å°±æ˜¯ç§»åŠ¨äº’è”ç½‘æ—¶ä»£ã€‚

ç°åœ¨æˆ‘ä»¬äººæ‰‹ä¸€éƒ¨æ‰‹æœºï¼Œå¯¹å§ï¼Ÿè¿™ä¸ªæ—¶ä»£è¿™ä¸ªè¿æ¥è§„æ¨¡å·²ç»çˆ†ç‚¸å¼çš„å¢é•¿ã€‚å®ƒçš„å…¸å‹çš„è”ç½‘å°±æ˜¯æ— çº¿äº’è”ç½‘ã€‚ç„¶åå¯ä»¥çœ‹åˆ°æˆ‘ä»¬çœ‹ä¹Ÿå¯ä»¥çœ‹åˆ°è¿™ä¸ªæ¶æ„ä¸­é—´å‡ºäº†ä¸€ã€‚å¤šäº†ä¸€æ¡çº¿ï¼Œå¯¹å§ï¼Ÿå•Šï¼Œå¯ä»¥çœ‹åˆ°è¿™é‡Œå¤šäº†ä¸€æ¡çº¿ã€‚

è¿™æ¡çº¿ä»£è¡¨ä»€ä¹ˆï¼Ÿä»£è¡¨é›†å›¢äº§ä¸šè®¡å›¢çš„æ¶æ„ä»£è¡¨å‡ºç°äº†åˆ†åŒ–ï¼Œä¸ºä»€ä¹ˆéƒ½ä¼šå‡ºç°åˆ†åŒ–ï¼Ÿæ˜¯å› ä¸ºæˆ‘ä»¬æ‰‹æŒçš„è¿™ä¸ªç»ˆç«¯è®¾å¤‡ï¼Œå®ƒå¿…é¡»è¦æœ‰ä¸€ä¸ªä½åŠŸè€—çš„ä¸€ä¸ªç¡¬æ€§çš„è¦æ±‚ã€‚è€Œè¿™ä¸ªç¡¬æ€§è¦æ±‚æ˜¯åœ¨ä¸Šä¸€ä¸ªæ—¶ä»£å‰86åŸºäºå‰86èŠ¯ç‰‡åšçš„è¿™ä¸ªã€‚

è¿™ä¸ªä»·æ ¼æˆ‘æ˜¯æ²¡æ³•æ»¡è¶³çš„ï¼Œæ‰€ä»¥å®ƒå‡ºç°äº†ç»“æ„çš„åˆ†åŒ–ã€‚è€Œè¿™ä¸ªåˆ†åŒ–å·¦è¾¹å°±æ˜¯ä»¥arä¸ºä»£è¡¨çš„ä½åŠŸè€—çš„è¿™ä¸ªæ€§å¤„ç†å™¨ä¹‹ä¸Šå­•è‚²å‡ºäº†å®‰roidå’ŒIOSIOSè¿™ä¸ªç”Ÿæ€ã€‚é‚£å³è¾¹è¿˜æ˜¯å‘ƒä¸­å¿ƒä¾§å•Šï¼Œæ˜¯å°±æ˜¯æˆ–è€…æ˜¯äº‘ç«¯ã€‚

æ˜¯å‰86çš„èŠ¯ç‰‡å’Œlinuxuxæ„å»ºçš„è¿™ä¸ªäº‘ç«¯çš„ç”Ÿæ€ã€‚é‚£è¿™ä¸¤è¿™ä¸¤å¥—è¿™ä¸ªæ¶æ„ä¼šå‘ƒä¹Ÿæ˜¯åœ¨å¹¶è¡Œå»å‘å±•ã€‚åƒæ¯”å¦‚è¯´å·¦è¾¹è¿™ä¸ªå‘ƒæ‰‹æŒè®¾æ‰‹æŒè®¾å¤‡ä¸Šæœ‰å„ç§å„æ ·ç¹è£çš„APPå¯¹å§ï¼Ÿå¤§å®¶éƒ½å¯ä»¥ä½“éªŒåˆ°ï¼Œå®é™…ä¸ŠåŸºæœ¬ä¸Šç”Ÿæ´»å‘ƒã€‚

è¿™ä¸ªè¿™ä¸ªå‘ƒçº¿ä¸‹å’Œçº¿ä¸‹åŸºæœ¬æ‰“é€šï¼Œéƒ½æ˜¯é€šè¿‡APPå»å»å»å®ç°çš„ã€‚é‚£åœ¨è¿™ä¸ªæ—¶ä»£å‡ºç°äº†ä¸€ä¸ªå¾ˆå¤§çš„ä¸€ä¸ªå˜åŒ–ï¼Œå°±æ˜¯å› ä¸ºæˆ‘ä»¬æ‰‹æŒç»ˆç«¯çš„è”ç½‘è§„æ¨¡ã€‚æŠ¥ç‚¸æ€§å¢é•¿ä¹‹åï¼Œäº§ç”Ÿäº†å¤§é‡çš„æ•°æ®ã€‚è¿™ä¸ªæ•°æ®æœ‰å¾ˆå¤§ä¸€æ‰¹æ˜¯éç»“æ„åŒ–çš„æ•°æ®ã€‚

è¿™ç§éä¼åŒ–æ•°æ®å®ƒçš„ä¸€ä¸ªé‡è¦çš„ç‰¹å¾æ˜¯ã€‚å‘ƒï¼Œå‘é‡åŒ–ã€‚è¿™ç§å‘é‡åŒ–çš„æ•°æ®ï¼Œå®ƒæ˜¯ä¼ ç»ŸCPUå¤„ç†è¿‡æ¥ï¼Œç”±æ­¤å¯¼è‡´äº†ä»€ä¹ˆå‘¢ï¼Ÿå¯¼è‡´æˆ‘ä»¬ç°åœ¨è¿›å…¥AIç¬¬å››ä¸ªæ—¶ä»£ï¼Œå°±æ˜¯AIæ—¶ä»£çš„æ—¶å€™ï¼Œå®ƒåˆ†åŒ–æ›´å‰§çƒˆäº†ã€‚

å°±æ˜¯CPUå¤„ç†ä¸è¿‡æ¥çš„è¯å‘¢ï¼Œä»–ä¸€å®šéœ€è¦æœ‰ä¸ªç‰¹åˆ«æ“…é•¿å¤„ç†è¿™éç»“æ„åŒ–æ•°æ®çš„èŠ¯ç‰‡å•Šï¼Œåº•å±‚çš„è¿™ä¸ªæ¶æ„è¿è¥è€Œç”Ÿã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼Ÿå³ä¸‹è§’AIèŠ¯ç‰‡åœ¨è¿™ä¸ªæ—¶ä»£åˆ°æ¥çš„çš„åŸå› ã€‚

æ‰€ä»¥æˆ‘ä»¬ç°åœ¨æ­£åœ¨è¿›å…¥çš„æ˜¯AIæ—¶ä»£AIæ—¶ä»£å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ¶æ„åˆ†åŒ–çš„è¶Šæ¥è¶Šå‘ƒå¤æ‚å•Šã€‚é‚£åŸºäºAIèŠ¯ç‰‡ã€‚ä¹‹ä¸Šï¼Œåœ¨ç›¸åŒåŸå¸‚ï¼Œæ“ä½œç³»ç»Ÿç›¸åŒåŸå¸‚ã€‚è¯ç”Ÿäº†å‘ƒï¼Œä¹Ÿä¸æ˜¯è¯´æ˜¯å› ä¸ºAIèŠ¯ç‰‡è¯ç”Ÿäº†è¿™ä¸¤è€…æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

AIèŠ¯ç‰‡å…¶å®æ˜¯å…¶å®å¹¶æƒ…å‘å±•çš„ã€‚å®ƒçš„ä»–ä»¬ä¹‹é—´çš„å…³ç³»å°±å¾ˆåƒå‘ƒè¿™ä¸ªCPUå’Œæ“ä½œç³»ç»Ÿçš„ä¸€ä¸ªå…³ç³»å•Šï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥ä¸ºä»€ä¹ˆè¯´æ·±åº¦å­¦ä¹ æ¡†æ¶å°±åƒæ˜¯AIæ—¶ä»£çš„è¿™ä¸ªæ“ä½œç³»ç»Ÿã€‚åœ¨è¿™ä¸ªä¹‹ä¸Šå‘¢ã€‚å‘ƒã€‚

æœ‰å¾ˆå¤šå¤§æ•°æ®çš„åº”ç”¨ä»¥åŠå„ç§å„æ ·çš„æ¨¡å‹ã€‚ç°åœ¨è®²äº†å¤§æ¨¡å‹å¯¹å§ï¼Ÿå¤§æ¨¡å‹å…¶å®ä¹Ÿæ˜¯åŸºäºå¤§æ•°æ®ç‚¼ç‚¼å•ç‚¼å‡ºæ¥çš„ä¸€ä¸ªæ¨¡å‹å˜›ã€‚é‚£æ‰€ä»¥å¯ä»¥çœ‹åˆ°è¿™ä¸ªç”Ÿæ€åœ¨AIæ—¶ä»£çš„è¿™ä¸ªç”Ÿæ€ä¹Ÿåˆ†åŒ–å‡ºæ¥è¶Šæ¥è¶Šå¤æ‚ã€‚é‚£è¿™é‡Œæˆ‘ç”¨ä¸¤ä¸¤å¥è¯æ¥æ€»ç»“ã€‚

å°±æ˜¯è¯´å‘ƒå‘ƒå‘ƒå€Ÿç”¨è¿™ä¸ªå¯¹ç¤¾ä¼šä¸»è¦çŸ›ç›¾çš„ä¸€ä¸ªè¡¨è¾¾å•Šï¼Œå°±æ˜¯ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™æ ·å„ç§äº§ä¸šçš„ä¸€ä¸ªã€‚æ¶æ„çš„ä¸€ä¸ªè¿­ä»£å‘¢ï¼Œä¸»è¦å°±æ˜¯äººæ°‘ç¾¤ä¼—æ—¥ç›Šå¢é•¿çš„ç¾å¥½ç”Ÿæ´»éœ€æ±‚å’Œè®¡ç®—ç³»ç»Ÿçš„ç®—åŠ›å‘å±•ä¸å¹³è¡¡ä¸å……åˆ†ä¹‹é—´çš„çŸ›ç›¾ã€‚è¿™æ˜¯ã€‚

è®¡ç®—äº§ä¸šæ¶æ„è¿­ä»£çš„ç¬¬ä¸€é©±åŠ¨åŠ›ã€‚é‚£æ¯ä¸€æ¬¡çš„è¿™ä¸ªã€‚æ¶æ„çš„å˜åŒ–æˆ–è€…æ˜¯æ¯ä¸€ä¸ªæ—¶ä»£çš„å˜åŒ–ã€‚å°±ä¼šæœ‰æ–°çš„å•Šè¿™ä¸ªæ—§çš„ç”Ÿæ€ä¼šè¢«çš„é¢ è¦†ï¼Œæ–°çš„ç”Ÿæ€å°±ä¼šæ¶Œç°ã€‚é‚£ç°åœ¨å‡ºç°å°±ä¼šå‡ºç°ä¸€ä¸ªå¾ˆå·¨å¤§çš„ä¸€ä¸ªæœºä¼šã€‚é‚£è¿™ä¸ªå·¨å¤§çš„æœºä¼šã€‚

é‡è¦ä¸€ä¸ªè¡¨å¾æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå¯ä»¥çœ‹åˆ°ã€‚è¿™å¼ å›¾æ˜¯ä¸€ä¸ªå®¢è§‚ç°å®å•Šã€‚è¿™åº”è¯¥æ˜¯ä¸€ä¸ªç ”ç©¶æŠ¥å‘Šä¸Šä¸€å¼ å›¾ï¼Œè¿™å¼ å›¾è¡¨è¾¾çš„æ˜¯ä»€ä¹ˆï¼Ÿçºµåæ ‡æ˜¯æ¨¡å‹çš„å‚æ•°è§„æ¨¡ã€‚æ¨ªåæ ‡æ˜¯æ—¶é—´è½´å•Šï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå…¶å®æ˜¯å¾ˆæ˜æ˜¾çš„è¶‹åŠ¿å°±æ˜¯æ¨¡å‹çš„è§„æ¨¡è¶Šæ¥è¶Šå¤§ã€‚

å‘ˆæŒ‡æ•°å‹çš„çˆ†ç‚¸å¼å¢é•¿ã€‚é‚£å‰å‡ å¹´è¿™ä¸ªã€‚bertåœ¨è¿™å„¿300å¤šå…†çš„è¿™ä¸ªè§„æ¨¡ã€‚botå½“å¹´æ¨å‡ºçš„æ—¶å€™ä¹Ÿæ˜¯éå¸¸çš„ç²¾éªŒå•Šï¼ŒåŒ…æ‰“å¤©ä¸‹å‘ƒï¼Œéœ¸æ¦œå¾ˆé•¿æ—¶é—´ï¼Œä½†æ˜¯æ²¡å‡ å¹´ã€‚è¿™ä¸ªGPT3å‡ºæ¥1751ä¸ªå‚æ•°ã€‚

é‚£é‚£å¤§å®¶ä¹ŸçŸ¥é“è¿™ä¸ªæœ€è¿‘å¾ˆç«äº†ï¼Œæäº†GPTå°±æ˜¯åŸºäºGPT3å¯¹å§ï¼Ÿé‚£è¿™ä¸ªè§„æ¨¡æŒ‡æ•°å‹å¢é•¿ï¼Œé‚£å¯ä»¥é¢„è§åˆ°æœªæ¥è§„æ¨¡æ¨¡å‹çš„è§„æ¨¡ä¹Ÿä¼šä¹Ÿä¼šè¶Šæ¥è¶Šå¤§ã€‚è¿™è¯´æ˜ä»€ä¹ˆå‘¢ï¼Ÿå…¶å®å°±æ˜¯è¯´åœ¨AIæ—¶ä»£ã€‚åˆšåˆšæˆ‘è¯´çš„è¿™ä¸ªæ—§çš„ç§©åºè¢«é¢ è¦†ä¹‹åã€‚

æ¶Œç°å‡ºæ¥çš„å¤§é‡çš„æ–°çš„æœºä¼šå°±æ˜¯å¤§æ¨¡å‹çš„æœºä¼šã€‚é‚£å¤§æ¨¡å‹ä¸€å®šæ˜¯å¯¹åº•å±‚çš„å¤§ç®—åŠ›æ˜¯æœ‰è¦æ±‚ã€‚é‚£ä»æˆ‘æˆ‘çœ‹åˆ°ä¸€ä¸ªç ”ç©¶ï¼Œå°±æ˜¯è¯´æ¨¡å‹çš„è§„æ¨¡æ¯å¢é•¿1å€ï¼Œæ¨¡å‹çš„è§„æ¨¡å¢é•¿ã€‚è·Ÿç®—åŠ›éœ€æ±‚çš„å¹³æ–¹æˆæ­£æ¯”ã€‚ä¹Ÿå°±æ˜¯è¯´æ¨¡å‹å¢é•¿ã€‚

é‚£ç®—åŠ›å¢é•¿æ›´å¤šå•Šï¼Œéœ€æ±‚æ›´å¤šã€‚æ‰€ä»¥å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªå¾ˆæ˜æ˜¾çš„è¶‹åŠ¿å•Šã€‚é‚£ä¹ˆä»è¿™ä¸ªè¶‹åŠ¿æ¥è¯´ï¼Œå¦‚æœæ¨¡å‹å¢é•¿åˆ°ä¸€å®šè§„æ¨¡ï¼Œçªç ´ä¸€ä¸ªä¸´è¿‘ç‚¹çš„æ—¶ä¹‹åä¼šå‡ºç°ä»€ä¹ˆï¼Ÿè¿™å°±æ˜¯æˆ‘ä»¬è¯´çš„æ¶Œç°ã€‚ä¸ºä»€ä¹ˆå¤§å®¶å‘ƒã€‚æ„Ÿå—åˆ°è¿™ä¸ªå‰GPTè¿™ä¹ˆã€‚

å‰å®³ã€‚å¥½åƒèƒ½ç†è§£äººçš„è¿™ä¸ªè‡ªç„¶è¯­è¨€ï¼Œä»¥åŠèƒ½å¤Ÿåšå¾ˆå¤šè¿™ä¸ªæ€ç»´çš„è¿™ä¸ªæ¨ç†ã€‚ä¸ºä»€ä¹ˆï¼Ÿå—¯ï¼Œç°åœ¨å­¦æœ¯ç•Œéƒ½æŠŠè¿™ä¸ªå½’ç»“æœ‰å½’å½’ç»“ä¸ºæ¶Œå¿ã€‚é‚£å®é™…ä¸Šæ¶Œå¿å…¶å®æ˜¯ä¸€ä¸ªå¾ˆæ™®éçš„ä¸€ä¸ªç°è±¡ã€‚åœ¨ç”Ÿç‰©å­¦ä¸­ä¹Ÿæœ‰å¾ˆå¤šè¿™ä¸ªæ¶Œç°çš„ç†è®ºåŸºç¡€ã€‚

ä½ æ¯”å¦‚è¯´ã€‚æœ€ç®€å•çš„å°±æ˜¯æˆ‘ä»¬çš„æˆ‘ä»¬äººè„‘ä¸ºä»€ä¹ˆä¼šäº§ç”Ÿæ„è¯†ï¼Ÿæˆ–è®¸ä¸€ç›´ä¹Ÿæ˜¯ä¸€ä¸ªæ¶Œç°çš„ç»“æœï¼Œå¯¹å§ï¼Ÿè¿™å°±è¿™å°±å¦‚åŒæˆ‘ä»¬ã€‚è¿™æœ‰ä¸ªGPT3ä¸ºä»€ä¹ˆèƒ½å¤Ÿäº§ç”Ÿå¦‚æ­¤æƒŠè‰³çš„æ•ˆæœï¼Ÿå‘ƒï¼Œæˆ‘ä»¬ä¸èƒ½ç†è§£è¿™ä¸ªä¸ºä»€ä¹ˆä¼šäº§ç”Ÿå¦‚æ­¤ç»éªŒæ•ˆæœã€‚

å°±å¦‚åŒæˆ‘ä»¬ä¸èƒ½ç†è§£ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„å¤§è„‘ä¼šäº§ç”Ÿæ„è¯†ä¸€æ ·ã€‚æˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰çš„å½’éŸ³éƒ½å½’ä¸€ä¸ºæ¶Œç°ã€‚é‚£å®é™…ä¸Šè¿™æ˜¯æœ‰ä¸€ä¸ªä»ç”Ÿç‰©å­¦å’ŒAIç ”ç©¶æ¥è¯´æ˜¯æœ‰æ˜¯æœ‰ä¸€ä¸ªä¸€è„‰ç›¸æ‰¿çš„ä¸€ä¸ªç†è®ºåŸºç¡€çš„ã€‚å¯ä»¥çœ‹åˆ°å³è¾¹è¿™äº›å›¾å•Šã€‚

å½“æ¨¡å‹å¢é•¿åˆ°ä¸€å®šè§„æ¨¡ä¹‹åï¼Œå“ï¼Œå®ƒçš„æ•ˆæœã€‚æœ‰ä¸€ä¸ªçˆ†å‘å¼çš„å¢é•¿ï¼Œæœ‰ä¸ªæœ‰ä¸ªæœˆè¿ã€‚è¿™å°±æ˜¯æ¶Œçº¿çš„ä¸€ä¸ªæ•ˆæœã€‚æ‰€ä»¥ä»è¿™ä»ä»è¿™ä»è¿™äº›æ¶Œç°çš„è¿™ä¸ªç°è±¡æ¥è¯´ï¼Œä¹Ÿå¯ä»¥ä¹Ÿå¯ä»¥å‘ƒåœ¨å¯é¢„æœŸçš„æœªæ¥ï¼Œä¹Ÿå¯ä»¥åˆ¤æ–­ã€‚

å°±æ˜¯è¯´æ¨¡å‹è¶Šå¤§å¯èƒ½å®ƒçš„æ•ˆæœä¼šè¶Šå¥½ï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥æ‰€ä»¥è¿™ä¹Ÿè¿™ä¹Ÿå°±æ˜¯ä¸€ä¸ªå¤§æ¨¡å‹çš„ä¸€ä¸ªè¶‹åŠ¿ã€‚æœªæ¥æ¨¡å‹å¯èƒ½ä¼šè¶Šæ¥è¶Šå¤§ã€‚éšç€è·¨æ¨¡æ€å¤šæ¨¡æ€çš„ä¸€ä¸ªæ¨¡å‹çš„ä¸€ä¸ªä¹Ÿå‘ƒä¸€ä¸ª1ä¸€ä¸ªæ²‰æ·€å•Šã€‚

æœ‰æ›´å¤šæ›´å¤šçš„è¿™ä¸ªè·¨æ¨¡æ€çš„çŸ¥è¯†æ²‰æ·€å‹ç¼©åˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œè¿™ä¸ªæ¨¡å‹è§„æ¨¡æ˜¯ä¼šè¶Šæ¥è¶Šå¤§ã€‚é‚£äºæ­¤è¿è¥è€Œç”Ÿçš„ä¼´éšç€å°±æ˜¯å¯¹AIç®—é¢†çš„è¿™ä¸ªéœ€æ±‚ä¼šè¶Šæ¥è¶Šå¤§ã€‚å¥½ã€‚

è¿™ä¸ªç¬¬ä¸€partå°±æ˜¯åšäº†ä¸€äº›è¿™ä¸ªå¯¹è®¡ç®—äº§ä¸šçš„ä¸€ä¸ªé“ºå«å’Œè¶‹åŠ¿çš„ä¸€ä¸ªä¸€ä¸€äº›å±•æœ›ã€‚é‚£ä¹ˆã€‚ç¬¬äºŒæ’æˆ‘å¸¦æ¥çš„è¿™ä¸ªåˆ†äº«å°±æ˜¯é‚£ä½œä¸ºæˆ‘ä»¬æ˜†ä»‘æ˜Ÿï¼Œæˆ–è€…è¯´åœ¨è¿™ä¸ªAIç®—åˆ©è¿™ä¸ªèµ›é“çš„è¿™äº›è¿™äº›ç©å®¶æ¥è¯´ï¼Œæˆ‘ä»¬ç†è§£å¯èƒ½è¦ç»è¿‡äº§ä¸šåŒ–ã€‚

å¯èƒ½è¦å¿…ç»ä¸‰é“çª„é—¨ã€‚ç¬¬ä¸€é“å±•å‘¢å°±æ˜¯é‡äº§ã€‚ä½œä¸ºä¸€ä¸ªèŠ¯ç‰‡å…¬å¸ï¼Œé‚£é‡èŠ¯ç‰‡çš„é‡äº§æ˜¯ã€‚ç¬¬ä¸€é“å’±ä»¬å°±æ˜¯å‰æã€‚å°±æ˜¯è¯´ä»èŠ¯ç‰‡çš„è®¾è®¡æ„æ€ä¸€ç›´åˆ°é‡äº§å…¶å®è¦ç»å†å¾ˆé•¿çš„ä¸€ä¸ªæ—¶é—´ï¼Œå‘¨æœŸå·®ä¸å¤šæ˜¯ä¸¤å¹´çš„ä¸€ä¸ªå‘¨æœŸã€‚

é‚£å¦‚æœæ²¡æœ‰è·¨è¶Šåˆ°è¿™å¥—å±•æœ¬çš„è¯ï¼Œæ²¡æ³•å»è¦†ç›–å‰æœŸçš„æŠ•å…¥çš„å·¨å¤§çš„ä¸€ä¸ªæˆæœ¬ã€‚æ‰€ä»¥é‡äº§å…¶å®ä¹Ÿæ˜¯èŠ¯ç‰‡ä¼ä¸šã€‚è¿™ä¸ªæˆç†Ÿå’ŒèŠ¯ç‰‡ç¨³å®šæ€§ï¼Œè¿˜æœ‰å„ç§æŒ‡æ ‡æˆç†Ÿåº¦å’Œå’Œè¿™ä¸ªè¿™ä¸ªå‘ƒå¾ˆé‡è¦çš„ä¸€ä¸ªæŒ‡æ ‡ä¹‹ä¸€ã€‚ç¬¬äºŒä¸ªå’±ä»¬å°±æ˜¯ç”Ÿæ€ã€‚

å…¶å®æœ¬è´¨ä¸ŠåšèŠ¯ç‰‡çš„å…¬å¸æ˜¯æ²¡æ³•é€ƒé¿ç”Ÿæ€å’Œæˆ–è€…æ˜¯æ²¡æ³•é€ƒé¿è½¯ä»¶çš„ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå¦‚æœåªåšç”Ÿæ€åªåšèŠ¯ç‰‡ï¼Œé‚£ä¸Šå¾ˆå¤šä¸Šå±‚çš„åº”ç”¨ï¼Œå¾ˆå¤šä¸Šå±‚çš„å®¢æˆ·ï¼Œä»–æ ¹æœ¬ä¸šåŠ¡æ ¹æœ¬ç”¨ä¸èµ·æ¥ã€‚

é‚£å®é™…ä¸Šæ¯ä¸€ä¸ªèŠ¯ç‰‡å…¬å¸éƒ½è¦å»è·¨è¿‡ç¬¬äºŒé“å’±ä»¬å»å»ºè®¾ä¸€ä¸ªå¾ˆå®Œæ•´çš„ä¸€ä¸ªè½¯ä»¶ç”Ÿæ€å•Šï¼Œè¿™ä¸ªè½¯ä»¶ç”Ÿæ€å¯èƒ½åŒ…å«å¾ˆå¤šæŠ€æœ¯ç«™å•Šç¼–è¾‘å™¨å•Šï¼Œç”šè‡³è¿˜æœ‰å¼€å‘è€…ç¤¾åŒºå•Šï¼Œæ¥å¸®åŠ©å®¢æˆ·æ›´å¥½çš„ç”¨èµ·è¿™ä¸ªèŠ¯ç‰‡ã€‚

è€Œäº‹å®ä¸Šç°åœ¨å›½å›½å›½å†…å¾ˆå¤šè¿™ä¸ªå‘ƒåŒ…æ‹¬åŒ…æ‹¬æ˜†ä»‘æ˜Ÿè‡ªå·±å•Šï¼Œä¹Ÿéƒ½æ˜¯åœ¨è¿™ä¸ªç”Ÿæ€ä¹Ÿåœ¨ä¸æ–­çš„å®Œå–„ä¸­å•Šï¼Œå¥½åœ¨æ˜†ä»‘æ–°ç°åœ¨ä¾æ‰˜ç™¾åº¦çš„è¿™ä¸ªè¿™ä¸ªæ•´ä¸ªAIçš„æŠ€æœ¯ç”Ÿæ€ã€‚ç™¾åº¦åœ¨åœ¨AIçš„è¿™ä¸ªæŠ€æœ¯ç”Ÿæ€ä¸­ä¹Ÿå¸ƒå±€äº†å0å¹´ã€‚

æ‰€ä»¥è¿™å—ç›¸å¯¹æ¥è®²è¿˜æ˜¯ç›¸å¯¹æ¯”è¾ƒå®Œæ•´ä¸€ç‚¹ã€‚åé¢è¿˜æœ‰ä¸€ä¸ªPPTä¼šä»‹ç»çš„è¿™ä¸€å—ã€‚é‚£ç¬¬ä¸‰ä¸ªäº§å‘¢å°±æ˜¯äº§å“åŒ–ã€‚é‚£å…‰åšå®Œç”Ÿæ€ï¼Œå…‰å…‰åšäº†è½¯ä»¶è¿˜è¿œè¿œä¸å¤Ÿï¼Œè¦æŠŠè¿™ä¸ªè½¯ä»¶å’Œç¡¬ä»¶åšæˆè½¯ç¡¬ä¸€ä½“çš„äº§å“åŒ–çš„æ ‡å“ã€‚è¿™ä¸ªäº§å“æ¢äº†æ ‡å“ã€‚

æœ‰åŠ©äºã€‚è¿™ä¸ªèŠ¯ç‰‡ä½œä¸ºä¸€ä¸ªä¼ä¸šï¼Œèƒ½å¤Ÿå½¢æˆè¿™ä¸ªä¸šåŠ¡çš„é—­ç¯å’Œé£è½®å•Šï¼Œåšåšé¡¹ç›®å®æ–½å’Œè½åœ°çš„æ—¶å€™ï¼Œè¿™ä¸ªè¿™ä¸ªå‘ƒç¼–è¾‘æˆæœ¬æ‰ä¼šæ›´ä½ã€‚æ‰€ä»¥æˆ‘ç†è§£è¿™ä¸ªè¿™ä¸ªåšèŠ¯ç‰‡çš„å…¬å¸å•Šï¼Œå¯èƒ½ä¸èƒ½ä»…ä»…åªè€ƒè™‘ç¡¬ä»¶ç”Ÿæ€ï¼Œè¿˜æœ‰äº§å“åŒ–ã€‚

ç‰¹åˆ«æ˜¯è½¯ç¡¬ä¸€ä½“çš„äº§å“åŒ–å¯èƒ½æ˜¯å¾ˆé‡è¦çš„ä¸€ä¸ªã€‚éœ€è¦è€ƒè™‘çš„è¿™ä¸ªéƒ¨åˆ†å•Šã€‚å¥½ï¼Œç¬¬ä¸‰è¶´å°±æ˜¯é‚£å½“ç„¶å°±æ˜¯æ˜†ä»‘æ˜Ÿåœ¨å¤§æ¨¡å‹è¿™ä¸ªè¡Œä¸šæœ‰å“ªäº›å®è·µï¼Ÿè¿™ä¸ªå°±æ˜¯æˆ‘åˆšæ‰è¯´çš„è¿™ä¸ªå‘ƒæ˜†ä»‘æ–°ç»“åˆç™¾åº¦çš„è¿™ä¸ªæ•´ä¸ªç”Ÿå‘ƒæ•´ä¸ªAIçš„è¿™ä¸ªç”Ÿæ€ã€‚

å°±æ˜¯ç™¾åº¦æå‡ºäº†ã€‚AIå¤§ç†åšè¿™ä¸ªæ¦‚å¿µã€‚å‘ƒï¼ŒåŒ…å«4ä¸ªå››å±‚æ¡†æ¶ï¼Œåº”è¯¥è¯´æ˜¯ç™¾åº¦æ˜¯å›½å†…é¦–ä¸ªå…¨ç«™è‡ªç ”ï¼Œå››å±‚éƒ½è‡ªç ”çš„è¿™ä¸ªå‘ƒå¹¶ä¸”èƒ½å¤Ÿå•ç‹¬å»äº¤ä»˜çš„è¿™ä¸ªå‘ƒAIåŸºç¡€è®¾æ–½ï¼Œå°±æ˜¯å››å±‚æ¶æ„å•Šã€‚

æ˜¯å››å±‚å¤§åº•åº§æœ€åº•å±‚ä¸€å±‚å°±æ˜¯èŠ¯ç‰‡è¿™ä¸ªåœ¨AIæ—¶ä»£çš„ã€‚æˆ‘ä»¬æ‰€è¯´çš„ç®—åŠ›éƒ½æ˜¯AIç®—åŠ›å•Šï¼Œå°±èŠ¯ç‰‡å±‚ç§‘èƒ½å¿ƒï¼Œä¸­é—´å°±æ˜¯éè®²éè®²å°±æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶å•Šï¼Œå°±æ˜¯æ‰€è°“çš„AIæ—¶ä»£çš„æ“ä½œç³»ç»Ÿã€‚ç„¶ååœ¨è¿™å®ä¸Šå­µåŒ–è¿è‚²å‡ºæ¥è¿™ä¸ªæ–‡å¿ƒå¤§æ¨¡å‹ã€‚

é‚£æ–‡å¿ƒå¤§æ¨¡å‹åŒ…å«ä¸¤ç±»ã€‚ä¸€ç±»æ˜¯è¿™ä¸ªæ–‡å¿ƒä¸€è¨€æ˜¯NPçš„å¤§è¯­è¨€æ¨¡å‹ã€‚è¿˜æœ‰æ–‡å¿ƒä¸€ä¸ªå¼è¿™ä¸ªå¤§çš„è¿™ä¸ªå‘ƒæ–‡èº«å›¾çš„è¿™ç§è¿™ç§CVçš„ç±»çš„å¤§æ¨¡å‹å•Šã€‚åœ¨è¿™ä¸ªå¤§æ¨¡å‹ä¹‹ä¸Šå¯ä»¥å­µåŒ–å‡ºæ›´å¤šä¸ªçš„è¿™æ ·äº§ä¸šåº”ç”¨å•Šï¼ŒåŸºäºå¤§æ¨¡å‹çš„äº§ä¸šåº”ç”¨ã€‚

é‚£æ˜†ä»‘æ–°åœ¨è¿™æ ·ä¸€ä¸ªç”Ÿæ€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å¤§æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿå°±æ˜¯åº•å±‚ã€‚å…¶å®æ˜†ä»‘æ˜Ÿä¸»è¦å…³æ³¨çš„ä¸»è¦ä¸»è¦å¯ä»¥äº¤ä»˜å”®å–çš„ï¼Œä¸»è¦è¿˜æ˜¯åº•å±‚çš„è¿™ä¸ªç¡¬ä»¶å’Œè®¾æ–½å˜›ã€‚åœ¨è¿™ä¸ªç¡¬ä»¶è®¾æ–½ä¹‹ä¸Šã€‚

æˆ‘ä»¬æœ‰å„ç§å„æ ·å½¢æ€çš„è¿™ä¸ªä¸¾è¯äº§å“çŸ©è¯å•Šï¼Œå¡çº§çš„ï¼Œè¿˜æœ‰æœåŠ¡å™¨çº§çš„éƒ½æœ‰åœ¨è¿™ä¸ªä¹‹ä¸Šï¼Œæˆ‘ä»¬ä¼šæä¾›æ˜†ä»‘æ˜Ÿåœ¨æ”¯æŒå¤§æ¨¡å‹çš„å‘ƒè¿™ä¸ªæ˜†ä»‘æ–°è½¯ä»¶ç«™ï¼Œç‰¹åˆ«æ˜¯å‰FTè¿™æ ·æˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªå¤§æ¨¡å‹çš„åŠ é€Ÿåº“ã€‚

åœ¨è¿™ä¸ªä¹‹ä¸Šä¼šå‘ƒä¼šæœ‰ä¸€äº›æ•°æ®æ ‡æ³¨å•Šï¼Œæ‰€è®­ç»ƒè¿˜æœ‰è¿™ä¸ªåŸºäºæˆ–è€…æ˜¯å…¶ä»–çš„å…¶ä»–çš„å¼€æºçš„è¿™ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶æ„å»ºå‡ºæ¥çš„è¿™ä¸ªæ¡†æ¶çº§çš„ç”Ÿæ€ä»¥åŠä¹‹ä¸Šçš„å„ç§å¤§æ¨¡å‹å¤§æ¨¡å‹åœ¨ä¸Šé¢åœ¨å­µåŒ–å¾ˆå¤šåº”ç”¨å•Šï¼ŒåŒ…æ‹¬ä¿¡å¯¹è¯æ²Ÿé€šå•Šã€‚

å†…å®¹åˆ›ä½œç­‰ç­‰éƒ½å¯ä»¥éƒ½å¯ä»¥å­µåŒ–å‡ºæ¥ã€‚æ‰€ä»¥è¿™ä¸ªå°±æ˜¯æˆ‘åˆšæ‰è¯´çš„æ˜†ä»‘æ˜Ÿä¸»è¦æ¨ä¸»è¦éœ€è¦ä¸»è¦ç§¯ç´¯çš„è¿™ä¸ªæ ¸å¿ƒçš„è¿™ä¸ªäº§å“çŸ©é˜µå•Šï¼Œå¯ä»¥åˆ†ä¸ºè¿™ä¸‰ä¸ªå±‚æ¬¡ã€‚é‚£æˆ‘ä»¬æ‰€è¯´çš„å¤§æ¨¡å‹å…¶å®æœ‰å¤šå¤§åˆ°åº•æœ‰å¤šå¤§ã€‚

å…¶å®è¿™ä¸ªå…¶å®æ˜¯ä¸€ä¸ªæ˜¯ä¸€ä¸ªæ¯”è¾ƒæ¨¡ç³Šçš„æ¦‚å¿µã€‚æˆ‘ä»¬è¿™é‡Œåˆ’åˆ†äº†ä¸‰ä¸ªå±‚æ¬¡ï¼Œä»10äº¿åˆ°ç™¾äº¿çš„è¿™ä¸ªæ¨¡å‹ï¼Œå¯ä»¥åœ¨å¡Pæ ‡å¡çº§ä¸Šå»åšå»åšå‘ƒæ¨ç†æˆ–è€…è®­ç»ƒã€‚é‚£å¦‚æœæ˜¯ç™¾äº¿åˆ°åƒäº¿çš„è¿™ç§è§„æ¨¡ï¼Œé‚£æˆ‘ä»¬å»ºè®®å°±æ˜¯åœ¨åœ¨è¿™æ ·ä¸€ä¸ªå‘ƒå‘ƒæœåŠ¡å™¨ä¸Šã€‚

è¿™ä¸ªæœåŠ¡å™¨ä¸Šé¢æ­è½½äº†8é¢—èŠ¯ç‰‡8é¢—èŠ¯ç‰‡ä¸¤ä¸¤äº’è”ï¼Œå®ƒæ˜¯OAMçš„è¿™ç§æ¨¡ç»„ï¼Œä¸¤ä¸¤äº’è”æ„å»ºäº†æ˜¯ä¸€ä¸ªå¯ä»¥æ”¯æŒå¤§ä¸è§„æ¨¡è®­ç»ƒæˆ–è€…æ¨ç†çš„è¿™ä¸ªæœåŠ¡å™¨ã€‚å¦‚æœåœ¨åƒäº¿çº§ä»¥ä¸Šçš„è¿™ä¸ªè¿™ä¸ªå‘ƒæ¨¡å‹å•Šã€‚

æ¯”å¦‚è¯´æ°GBTè¿™æ ·1175äº¿ä¸ªå‚æ•°çš„è¿™ç§æ°GBTçš„è¯å‘¢ï¼Œå°±å°±å°±éœ€è¦è¿™ç§å‘ƒæœåŠ¡å™¨ç»„å•Šç»„å‘ƒè¿™ä¸ªç»„åˆæˆçš„è¿™ä¸ªé›†ç¾¤ã€‚é‚£ç»è¿‡æˆ‘ä»¬å®æµ‹å‘¢å‘ƒæ¨ç†çš„è¿™ä¸ªæ€§èƒ½åŸºæœ¬ä¸Šç›¸å¯¹äºä¸»æµçš„å‘ƒè¿™ä¸ª150ç“¦è¿™ä¸ªäº§å“å•Šã€‚

GPUè¿™ç§äº§å“æ˜¯æœ‰ä¸€å®šçš„åŠ é€Ÿçš„å•Šã€‚å•Šï¼Œè¿™æˆ‘è¿™é‡Œè¯´çš„éƒ½æ˜¯150ç“¦è¿™ä¸ªè¿™ä¸ªæ¡£æ¬¡çš„ã€‚é‚£è¿™äº›æŠ€æœ¯æŒ‡æ ‡æˆ‘å°±ä¸ä¸ä¸€å¿µäº†å•Šï¼Œå¤§æ¦‚å°±æ˜¯è¯´æƒ³è¯´çš„æ˜¯è¿™ç§è¿™ç§å¡æ˜¯Pèƒä¸æ ‡å¡ã€‚ç°åœ¨ä¸»å‘ƒä¸»æ¨çš„è¿™ä¸ªPèƒEæ ‡å¡ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»æŠŠæ‰€æœ‰ä¸»æµçš„å‰86çš„æŒ‡ä»¤çº§çš„ã€‚å‘ƒï¼Œæœºå‹å’ŒarMæŒ‡ä»¤æœºçš„æœºå‹å…¨è¿˜æœ‰ä¸€äº›ã€‚å‘ƒï¼Œå¤§è§„æ¨¡çš„é›†ç¾¤å…¨åšäº†é€‚é…å•Šï¼Œè¿™è¿™è¿™æ˜¯è¿™æ˜¯çœ‹æ‰äº§å“å±€ä¸­çš„è¿™ä¸ªè¡¨å¡ã€‚è¿˜æœ‰ç±»å°±æ˜¯åˆšåˆšæˆ‘ä»¬è¯´æˆ‘è¯´çš„è¿™ä¸ªä¸€ä½“æœºä¸€ä½“æœºã€‚

è¿™ä¸ªå¤§æ¦‚æ˜¯6Uçš„è¿™ä¸ªæœºå™¨ï¼Œè¿™é‡Œé¢å°±æ˜¯æœ‰æœ‰å‘ƒæœ‰ä¸€ä¸ªçª„æ¿ï¼Œä¸Šé¢æœ‰8é¢—8é¢—è¿™ä¸ªè¿™ä¸ªäºŒä»£èŠ¯ç‰‡ã€‚å®ƒé€šè¿‡è¿™æ ·ä¸€ä¸ªtopæ‰‘ç»“æ„äº’è”ä¹‹åï¼Œå¯ä»¥è¾¾åˆ°æ¯”è¾ƒå¥½çš„ä¸€ä¸ªå¹¶è¡Œè®­ç»ƒå•Šä¸€ä¸ªæ•ˆæœã€‚æ¨¡å‹å¹¶å½¢çš„æ•ˆæœã€‚

é‚£å¦‚æœæ˜¯å¤šå°æœºå™¨ç»„æˆçš„è¿™ä¸ªé›†ç¾¤å‘¢ï¼Œæˆ‘ä»¬åŒæ—¶å¯ä»¥æä¾›ã€‚å‘ƒï¼Œé›†ç¾¤çº³ç®¡çš„ä¸€ä¸ªæ¡†æ¶å•Šï¼Œä¸ªé«˜æ„å»ºä¸€ä¸ªé«˜å¯æ‰©å±•çš„ä¸€ä¸ªmasã€‚é‚£è¿™é‡Œæƒ³è¯´çš„å°±æ˜¯æ˜†ä»‘æ–°å’Œç™¾åº¦çš„æ‹ddleæ‹leé£è®²ä¹Ÿåšäº†å¾ˆå¤šå¾ˆæ·±åº¦çš„è¿™ä¸ªæŠ€æœ¯ä¼˜åŒ–ã€‚

æ¯”å¦‚è¯´æœ‰è¿™ä¸ªå•Šå‘ƒæ•°æ®å¹¶è¡Œï¼Œè¿˜æœ‰è¿™ä¸ªæ¨¡å‹å¹¶è¡Œï¼Œè¿˜æœ‰åˆ†ç»„åˆ‡ç‰‡ç­‰ç­‰æµæ°´çº¿å¹¶è¡Œï¼Œè¿˜æœ‰è¿™ä¸ªå‚æ•°çš„åˆ†çº§å­˜å‚¨ç­‰ç­‰çš„å„ç§æŠ€æœ¯çš„ä¼˜åŒ–å•Šï¼Œè¿™é‡Œè¿™é‡Œæˆ‘æˆ‘å°±ç®€å•ç®€å•è¯´ä¸€ä¸‹è¿™ä¸ªå‚æ•°åˆ†çº§å­˜å‚¨ä»€ä¹ˆæ¦‚å¿µã€‚å°±æ˜¯è¯´ã€‚

é‚£å…¶å®è¿™ä¸ªå¤§æ¨¡å‹å¾ˆå¤§å‚æ•°å’Œå‚æ•°è§„æ¨¡å¾ˆå¤§ï¼Œå¯èƒ½å¯èƒ½å¡ä¸Šè£…ä¸ä¸‹ã€‚é‚£è£…ä¸ä¸‹çš„è¯æ€ä¹ˆåŠå‘¢ï¼Ÿå°±å¯èƒ½è¦åˆ†çº§å­˜å‚¨ï¼ŒæŠŠçƒ­ç‚¹æ•°æ®è£…åœ¨å¡ä¸Šçš„è¿™ä¸ªæœ‰é™çš„è¿™ä¸ªæ˜¾å­˜é‡Œï¼Œä¸€äº›éçƒ­ç‚¹å‘¢å¯èƒ½è£…åœ¨è£…åœ¨hostçš„å†…å­˜é‡Œã€‚

é‚£æ›´ä¸çƒ­ç‚¹çš„æˆ–è€…å¤§æ¨¡å‹çš„å…¨é‡å‚æ•°è£…åœ¨è¿™ä¸ªç¡¬ç›˜é‡Œï¼Œé€šè¿‡æ•´ä½“çš„ä¸€ä¸ªåè°ƒï¼Œé€ æˆçš„å‘ƒä½¿ä½¿è¿™ä¸ªè¿™ä¸ªæ•´ä½“çš„è¿™ä¸ªè¿™ä¸ªè°ƒåº¦çš„è¿™ä¸ªæˆæœ¬çš„æœ€ä½ã€‚åŒæ—¶æˆ‘ä»¬è·Ÿæ™ºæºè¿™å—ä¹Ÿæœ‰éå¸¸éå¸¸æ·±åº¦çš„è¿™ä¸ªåˆä½œå“ˆã€‚

é‚£æ¯”å¦‚è¯´è¿™ä¸ªè‡´æºè¿™é‡Œæ¨å‡ºçš„è¿™ä¸ªå¤§æ¨¡å‹colå¤§æ¨¡å‹å•Šï¼Œæˆ‘ä»¬ä¹Ÿåšäº†è¿™ä¸ªå‘ƒå¾ˆå¥½çš„ä¸€ä¸ªé€‚é…ï¼Œå¹¶ä¸”å·²ç»éªŒè¯äº†å¤§è§„æ¨¡æ¨æ¨ç†çš„ä¸€ä¸ªä¸€ä¸ªæ€§èƒ½ã€‚ğŸ˜Šï¼Œè¿˜æœ‰nexå‚è¿™ä¸ªåˆ†åˆ†åˆ†å¸ƒå¼çš„ä¸€ä¸ªæ¨ç†ä¹Ÿæ”¯æŒäº†ã€‚

åŒæ—¶è¿™é‡Œè¿˜æœ‰è¿™ä¸ªèµ„æºè¿˜æ¨å‡ºäº†è¿™ä¸ªEvaç³»åˆ—çš„è¿™ä¸ªè§†è§‰å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿéƒ½åšäº†å…¨é‡çš„ä¸€ä¸ªé€‚é…å•Šã€‚è¿™Evaç³»åˆ—å‘¢å®ƒä¼šæˆ‘ä»¬åœ¨è¿™ä¸ªèµ„æºæ¨å‡ºè¿™ä¸ªä¹é¼è®¡ç®—å¹³å°ä¸­ä¹Ÿåšäº†å¾ˆå®Œç¾çš„ä¸€ä¸ªé€‚é…ã€‚æ‰€ä»¥åœ¨åœ¨è¿™ä¸ª9ç‚¹9é¼è¿™ä¸ªå¹³å°ä¸­ã€‚

æˆ‘ä»¬å¯ä»¥æˆ‘ä»¬æ”¯æŒçš„è¿™ä¸ªå‘ƒè¿™ä¸ªä¾æŠŠè¿™ä¸ªç³»åˆ—æ¨¡å‹è¿™ä¸ªè§†è§‰å¤§æ¨¡å‹çš„ä¸€ä¸ªä¸€ä¸ªå¤§æ¨¡å¤§è§„æ¨¡çš„éªŒè¯ã€‚åŒæ—¶èµ„æºä¸æ˜¯åœ¨è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªè¿™ä¸ªflag proofè¿™æ ·çš„ä¸€ä¸ªæµ‹è¯•æ¡†æ¶å•Šï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªæµ‹è¯•æ¡†æ¶ä¸­ã€‚

æˆ‘ä»¬ä¹Ÿæ”¯æŒäº†å¤§éƒ¨åˆ†çš„è¿™ä¸ªæ¨¡å‹å•Šï¼Œä¿æŒ10å¤šä¸ªè¿™ä¸ªæ¨¡å‹çš„è¯„æµ‹ç”Ÿæ€ã€‚è¿™ä¸ªæ¡†æ¶ä¹Ÿæä¾›äº†ä¸€ä¸ªéå¸¸å¥½çš„ä¸€ä¸ªbechmarkçš„ä¸€ä¸ªæµ‹è¯•æ ‡å‡†ã€‚æœªæ¥çš„è¯ä¹Ÿä¼šæˆ‘ä»¬ä¹Ÿä¼šé€æ­¥çš„è¿­ä»£è¿™æ ·ä¸€ä¸ªæ¨¡å‹ã€‚

ä¼šæœ‰è¿™ä¸ªæŠŠè¿™ä¸ªè¦†ç›–ç”Ÿç”Ÿæ€è¦†ç›–åšçš„è¶Šæ¥è¶Šå¥½ã€‚é‚£è¿™é‡Œä¸¾ä¸€äº›ä¾‹å­ï¼Œå°±æ˜¯åœ¨å¤§æ¨¡å‹åº”ç”¨å¯èƒ½ä¼šæœ‰å“ªäº›åœºæ™¯å‘¢ï¼Ÿå°±æ˜¯è¿™é‡Œè¯´æ¯”å¦‚è¯´ã€‚æ”¯æŒå‘ƒæœç´¢å’Œé—®ç­”ï¼Œè¿™æ˜¯éå¸¸è¿™æ˜¯å¾ˆå®¹æ˜“æƒ³åˆ°çš„éå¸¸å…¸å‹çš„ä¸€ä¸ªåœºæ™¯å•Šã€‚åƒç™¾åº¦ç°åœ¨çš„è¿™ä¸ªæœç´¢å‘ƒã€‚

è¿™ä¸ªåå°å°±å¾ˆå¤šéƒ½æ˜¯æ˜†ä»‘æ˜Ÿæ”¯æŒã€‚æ˜†ä»‘æ˜Ÿå†åŠ ä¸Šç™¾åº¦çš„è¿™ä¸ªNOPå¤§æ¨¡å‹æ”¯æŒã€‚åŒæ—¶è¿˜æœ‰ä¸€äº›åœºæ™¯ï¼Œæ¯”å¦‚è¯´èƒ½æºè¡Œä¸šï¼Œå°±ç”µåŠ›è¡Œä¸šçš„è¿™ä¸ªå¤§å¤§æ¨¡å‹å•Šï¼Œä»–ä»–ä»¬ç”¨åœ¨ä»€ä¹ˆä»€ä¹ˆåœ°æ–¹ï¼Œå°±æ˜¯è¯´ä»–ä»¬å¯èƒ½æœ‰å¾ˆå¤šè§„ç« åˆ¶åº¦éå¸¸éå¸¸åºå¤§ã€‚

åŒæ—¶åœ¨è¿™ä¸ªè®¾å¤‡çš„ç»´æŠ¤ä¸Šéå¸¸éå¸¸ç¹çã€‚é‚£é€šè¿‡å¤§æ¨¡å‹æŠŠè¿™äº›çŸ¥è¯†éƒ½å‹ç¼©åˆ°ä¸€ä¸ªå¤§æ¨¡å‹ä¹‹åå‘¢ï¼Œæœ‰åŠ©äºæå‡åˆ°äº†è¿™ä¸ªæ•ˆç‡ã€‚åŒæ—¶è¿˜æœ‰ä¸€äº›CVçš„åœºæ™¯çš„è¿™ä¸ªè¿™ä¸ªCVCVå¤§æ¨¡å‹å•Šï¼Œè¿™é‡Œå°±å‘ƒå°±å°±å°±ä¸ä¸å±•å¼€ä»‹ç»äº†ã€‚

æˆ‘ä»¬åœ¨è¿™ä¸ªæ™ºæ™ºæ…§å·¥ä¸šçš„è¿™ä¸ªæ£€æµ‹åœºæ™¯å‘¢ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰ä½¿ç”¨è¿™ä¸ªCVå¤§æ¨¡å‹ï¼Œèƒ½å¤Ÿå¾ˆæ–¹ä¾¿çš„è®©å·¥å‚çš„è¿™ä¸ªæ£€æµ‹æå‡çš„è¿™ä¸ªæ•ˆç‡å•Šï¼Œè¿™ä¸ªæ˜¯è¿™ä¸ªæ™ºæ…§å·¥ä¸šåœºæ™¯ã€‚æˆ‘ä»¬çš„ä¸€ä¸ªäº¤æ„å›¾ã€‚å·¦è¾¹å°±æ˜¯å®¢æˆ·æµ‹çš„è¿™ä¸ªç³»ç»Ÿå•Šã€‚

æˆ‘ä»¬å¯¹å®ƒæ— ä¾µå…¥çš„æ”¹é€ å•Šï¼Œå³è¾¹æä¾›æ˜¯æ•´å¥—è¿™ä¸ªå¤§æ¨¡å‹çš„å¯è®­ç»ƒä»¥åŠè¿™ä¸ªæ¨ç†è¿™ä¸ªè¿™ä¸ªå¼•æ“ã€‚å®ƒåªéœ€è¦åœ¨åº”ç”¨å±‚æ¢ä¸€ä¸ªè°ƒåº¦å¼•æ“å°±å°±å¯ä»¥äº†ã€‚å°±æ‰€ä»¥å¯¹å®¢æˆ·çš„ä¾µå…¥éå¸¸å°‘ã€‚å‘ƒï¼Œå†ç»™æˆ‘ä¸€ç‚¹æ—¶é—´ï¼Œå¥½å§ï¼Œæœ‰ç‚¹ç‚¹å±•æœ›å•Šã€‚

å°±æ˜¯è¯´è¿™æ˜¯å¸¦æ¥æˆ‘çš„ä¸ªäººé‚£äº›æ€è€ƒå•Šï¼ŒæŠ±æ­‰ã€‚ğŸ˜Šï¼Œé‚£å®é™…ä¸Šæˆ‘ä»¬ç†è§£å¤§æ¨¡å‹å¯¹è¿™ä¸ªäº§ä¸šå‘ç”Ÿçš„è¿™ä¸ªèŒƒå¼å‘ç”Ÿäº†éå¸¸éå¸¸å¤§çš„ä¸€ä¸ªå†²å‡»ï¼Œå‘ç”Ÿå‘ç”Ÿäº†éå¸¸å¤§çš„å˜åŒ–å•Šã€‚

å‘ƒåŸå…ˆæ˜¯ç”¨å„ç§å„æ ·å°æ¨¡å‹å»è§£å†³å„ç§å„æ ·åˆ†æ•£çš„å„ç§åœºæ™¯çš„é—®é¢˜ã€‚ç°åœ¨å‘¢å¥½äº†ï¼Œå¤§å®¶æœ‰ä¸ªå¤§æ¨¡å‹å…¨éƒ¨æ”¶æ•›äº†transformè¿™æ ·ä¸€ä¸ªå¤§æ¨¡å‹çš„èŒƒä¹‹åå‘¢ã€‚

å¯ä»¥é€šè¿‡å¤§æ¨¡å‹å»å»ä½œä¸ºä¸€ä¸ªbaseåšåšä¸ºä¸€ä¸ªbase lineï¼Œç„¶åå»ã€‚ğŸ˜Šï¼Œåšå‘ƒå°‘é‡æ•°æ®çš„ç¿» twoå•Šï¼Œå»å¯ä»¥å¯ä»¥è§£å†³è¡Œä¸šçš„ä¸€ä¸ªå‚ç±»éƒ¨ç½²å’Œé—®é¢˜ã€‚æ‰€ä»¥è¿™ä¸ªå¯¹äº§ä¸šçš„è¿™ä¸ªæ•´ä¸ªèŒƒå¼å‘ç”Ÿäº†å˜åŒ–ã€‚

æ‰€ä»¥æˆ‘ä»¬ç†è§£æœªæ¥å¯èƒ½å¤§å¤§æ¨¡å‹è¿™ä¸ªäº§ä¸šä¼šå¯èƒ½ä¼šåˆ†å±‚æœ€åº•å±‚æ˜¯æœ‰å°‘æ•°çš„å¤§ç§‘æŠ€å…¬å¸ï¼Œä»¥åŠæœ‰éå¸¸é›„åšçš„èµ„æœ¬çš„ï¼Œä¸€ä¸ªéå¸¸é›„åšçš„è¿™ä¸ªç®—åŠ›åŸºç¡€çš„è¿™ä¸ªå…¬å¸å»è®­ç»ƒè¿™æ ·çš„ä¸€ä¸ªä¸€ä¸ªéå¸¸åºå¤§çš„ä¸€ä¸ªå¤§æ¨¡å‹ã€‚

ç„¶ååŒæ—¶ä¼šæœ‰ä¸€äº›é«˜ç§‘æŠ€å…¬å¸å¤„äºä¸­é—´å±‚å»åœ¨åŸºäºè¿™æ ·çš„è¿™ä¸ªå¤§æ¨¡å‹åŸºç¡€ä¸Šå»åšè¡Œä¸šçš„å¤§è¡Œä¸šæ¨¡å‹çš„ç¿» twoæˆ–è€…æ˜¯è½åœ°å•Šï¼Œè¿˜æœ‰è¿˜æœ‰éå¸¸éå¸¸å¤šçš„ç™¾èŠ±é½æ”¾çš„åˆ›ä¸šå…¬å¸ï¼ŒåŸºäºå¾ˆå¤šå¤§æ¨¡å‹å»åšåˆ›ä¸šçš„è¿™ä¸ªåº”ç”¨ã€‚

è¿™å°±æ˜¯æ‰€è°“çš„å¤§æ¨¡å‹åŸç”Ÿçš„è¿™ä¸ªåˆ›ä¸šåˆ›ä¸šåº”ç”¨å•Šã€‚æ‰€ä»¥è¿™ä»¥æœªæ¥å¯èƒ½å¯èƒ½å¾ˆå¤šåˆ›ä¸šå…¬å¸è¦æ€è€ƒå¦‚ä½•å€Ÿç”¨æ–°æ¥çš„å¤§æ¨¡å‹çš„æŠ€æœ¯å»åšä¸€äº›åˆ›ä¸šã€‚é‚£OKè¿˜æœ‰æœ€åä¸€å¼ å›¾ã€‚

å°±æ˜¯æˆ‘ä»¬ç†è§£çš„è¿™ä¸ªæœ€åçš„è¿™ä¸ªåŸºäºè¿™ä¸ªå¤§æ¨¡å‹è¿™æ ·ä¸€ä¸ªè¶‹åŠ¿çš„å‘å±•å•Šï¼Œæˆ‘ä»¬çš„è¿™ä¸ªäº§ä¸šç»ˆå±€ä¼šæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿå¯ä»¥çœ‹åˆ°å·¦è¾¹ã€‚å·¦è¾¹æˆ‘ä»¬å¯ä»¥ç†è§£æ˜¯å¹¿ä¹‰çš„1ä¸ªIOTç‰©è”ç½‘ï¼Œä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ

å°±æ˜¯å‘æŒ¥æ•æ‰ç°å®ä¸–ç•Œä¸­çš„æ‰€æœ‰ä¿¡å·çš„è¿™ä¸ªè®¾å¤‡ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è®¤ä¸ºæ˜¯ç‰©è”ç½‘è®¾å¤‡ã€‚æ¯”å¦‚è¯´æ‰‹æœºæˆ–è€…æ‘„åƒå¤´ï¼Œå¯¹å§ï¼Ÿå¥½ï¼Œå³è¾¹æ˜¯æ˜¯æ‰¿è½½è¿™ä¸ªAIèŠ¯ç‰‡å’ŒAIå¤§æ¨¡å‹æ„å»ºçš„æ˜¯ä¸€ä¸ªæ•°å­—ç”Ÿå‘½ä½“çš„å¤§è„‘ã€‚è¿™é‡Œæå‡ºä¸€ä¸ªæ•°å­—ç”Ÿå‘½ä½“çš„æ¦‚å¿µã€‚

å°±æ˜¯å½“æœªæ¥å¤§æ¨¡å‹ï¼Œè¿˜æœ‰å¤§ç®—åŠ›ï¼Œå†åŠ ä¸Šä¸€ä¸ªåœºæ™¯ï¼Œè¿˜æœ‰è‹¥å¹²çš„åºå¤§çš„å‘ƒIUTè®¾å¤‡ç»„æˆäº†ä¸€ä¸ªä¸œè¥¿æ˜¯ä»€ä¹ˆï¼Ÿå°±å¾ˆåƒå¾ˆåƒè¶Šæ¥è¶Šé€¼è¿‘äººçš„æ™ºèƒ½ã€‚é‚£ä¹Ÿå¯ä»¥çœ‹å¯ä»¥çœ‹åˆ°äººçš„å¤§è„‘ã€‚

å¯¹å§äººå®¶å¤§ä½¬æ•´ä¸ªäººå°è£…çš„å¤§è„‘å°è£…çš„ç»éªŒå°±æ˜¯å°±æ˜¯å¤§æ¨¡å‹å•Šï¼Œç„¶åäººçš„å››è‚¢å°±æ˜¯è¿™ä¸ªè¿™ä¸ªæ„Ÿå—ä¸–ç•Œä¸‡ç‰©çš„è¿™ä¸ªè¿™ä¸ªIOTã€‚æ‰€ä»¥ç»è¿‡è¿™ä¸ªä¸æ–­çš„è¿­ä»£ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªæ•°å­—ç”Ÿå‘½ä½“ã€‚

è¿™ä¸ªæ•°å­—ç”Ÿå‘½ä½“å°±å¯ä»¥åº”ç”¨åœ¨åº”ç”¨åœ¨ä¼ä¸šæˆ–è€…ä¼ç”¨ç”¨åœ¨å›½å®¶å•Šã€‚æ‰€ä»¥æˆ‘ä»¬ç†è§£è¿™ä¸ªæ•°å­—ç”Ÿå‘½ä½“ï¼Œæˆ–è®¸æ˜¯æˆ–è®¸æ˜¯æœªæ¥äº§ä¸šçš„ä¸€ä¸ªç»ˆå±€çš„å½¢æ€ã€‚å¥½ï¼Œè°¢è°¢å¤§å®¶ã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_35.png)

å‘ƒï¼Œä¸å¥½æ„æ€å“ˆï¼Œå‰é¢é‚£ä¸ªæ²¡æœ‰ç‰¹åˆ«æ§åˆ¶è¿™ä¸ªæ—¶é—´å“ˆï¼Œæˆ‘ä»¬æ•´ä¸ªä¼šå·²ç»delayäº†20å¤šåˆ†é’Ÿå•Šï¼Œå¦åˆ™å‘ƒé‚£ä¸ªæ‰€ä»¥åé¢çš„sessionå•Šï¼Œæ¯ä¸€ä¸ªéƒ½è¯·ä¸¥æ ¼çš„éµå®ˆè‡ªå·±çš„æ—¶é—´ï¼Œå¦åˆ™å¤§å®¶æˆ‘æ‹…å¿ƒæˆ‘åœ¨åˆšæ‰è¿˜åœ¨é—®è¯´ã€‚

æˆ‘ä»¬çš„åˆé¥­åˆ°å‡ ç‚¹ï¼Œæˆ‘æ‹…å¿ƒå¤§å®¶æ²¡é‚£ä¸ªæ²¡æœºä¼šåƒåˆé¥­å“ˆã€‚è¿™ä¹ˆå¤šçš„å‘ƒå¬ä¼—ä»Šå¤©åŠ å…¥ã€‚å¥½ï¼Œé‚£å‘ƒæˆ‘ä¸‹é¢è¯·å‡ºä¸‹ä¸€ä½å‘ƒè®²è€…æ˜¯æ¥è‡ªäºä¸­ç§‘é™¢è®¡ç®—æ‰€çš„éƒ­ç¦éƒ­åšå£«ã€‚å¥½ï¼Œè°¢è°¢ã€‚ğŸ˜Šã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_37.png)

å¥½ï¼Œé‚£ä¸ªå‘ƒéå¸¸æ„Ÿè°¢æ—æ€»çš„ä»‹ç»å’Œé‚£ä¸ªé‚€è¯·å•Šå‘ƒï¼Œéå¸¸è£å¹¸ä»Šå¤©èƒ½æ¥å‚åŠ é‚£ä¸ªåŒ—äº¬èµ„æºå¤§ä¼šã€‚ç„¶åå‘ƒæˆ‘æ˜¯æ¥è‡ªä¸­å¤©è®¡ç®—æ‰€çš„éƒ­å¥‡ã€‚ä»Šå¤©æŠ¥å‘Šçš„é¢˜ç›®æ˜¯é¢å‘å¤§æ¨¡å‹çš„ç®—å­è‡ªåŠ¨è°ƒä¼˜æŠ€æœ¯ã€‚å‘ƒã€‚

ä¹Ÿæ˜¯æˆ‘ä»¬åœ¨ç®—å­çš„è‡ªåŠ¨è°ƒä¼˜å’Œä»£ç ä¸‰å±‚æ–¹é¢åšäº†ä¸€äº›æ¢ç´¢ã€‚ä»Šå¤©ä¹Ÿæ˜¯åœ¨è¿™è¾¹è·Ÿå¤§å®¶ä¸€å—åˆ†äº«ä¸€ä¸‹ã€‚å‘ƒï¼Œè¿™æ˜¯ä»Šå¤©æŠ¥å‘Šçš„ä¸€ä¸ªä¸»è¦çš„å†…å®¹å‘ƒï¼Œé¦–å…ˆæ˜¯ç®€å•çš„ä¸€ä¸ªèƒŒæ™¯çš„ä¸€ä¸ªä»‹ç»ã€‚å‘ƒï¼Œç„¶åå‘ƒæ·±åº¦å­¦ä¹ ç®—æ³•æ¨¡å‹çš„è§„æ¨¡ã€‚

å¤§å®¶éƒ½çŸ¥é“ä¹Ÿæ˜¯åœ¨å¿«é€Ÿçš„è¿™æ ·çš„ä¸€ä¸ªå¢é•¿ã€‚ç„¶åä¸ºäº†æ”¯æ”¯æ’‘è¿™æ ·çš„ä¸€ä¸ªå¤§æ¨¡å‹çš„è¿™æ ·é«˜æ•ˆçš„è¿è¿è¡Œï¼Œå…¶å®ç›®å‰ä¹Ÿæ¶Œç°å‡ºäº†å¤§é‡çš„é¢†åŸŸä¸“ç”¨çš„è¿™æ ·çš„ä¸€äº›å‘ƒæ¶æ„ã€‚ä½†æ˜¯å¦‚ä½•å……åˆ†å‘æŒ¥è¿™äº›é¢†åŸŸä¸“ç”¨æ¶æ„è¿™æ ·çš„ä¸€ä¸ªè¿ç®—çš„ä¸€ä¸ªæ½œåŠ›å‘¢ï¼Ÿ

å…¶å®ä¹Ÿæ˜¯å½“å‰AIç³»ç»Ÿé¢ä¸´çš„å…³é”®çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚å‘ƒï¼Œè¿™æ˜¯1ä¸ªAIç³»ç»Ÿè¿™æ ·çš„ä¸€ä¸ªå¤§è‡´çš„ä¸€ä¸ªæƒ…å†µã€‚ç„¶åé«˜æ€§èƒ½ç®—å­åº“å®é™…ä¸Šæ˜¯è¿æ¥ç®—æ³•æ¨¡å‹å’Œåº•å±‚ç¡¬ä»¶ç®—åŠ›çš„ä¸€ä¸ªéå¸¸é‡è¦çš„éå¸¸é‡è¦çš„æ¡¥æ¢ã€‚å‘ƒã€‚

å› ä¸ºå®ƒç›´æ¥å†³å®šäº†ä¸Šå±‚ç®—æ³•çš„æ˜¯å¦èƒ½å¤Ÿé«˜æ•ˆçš„è¿è¡Œã€‚ç„¶ååº•å±‚çš„é‚£ä¸ªç¡¬ä»¶å®ƒçš„ç®—åŠ›ï¼Œå› ä¸ºç®—åŠ›ä¹Ÿåœ¨ä¸æ–­çš„å¢é•¿ï¼Œå®ƒçš„ç®—åŠ›èƒ½å¦å……åˆ†çš„å‘æŒ¥å‡ºæ¥ã€‚å‘ƒï¼Œä¹Ÿæ­£æ˜¯å› ä¸ºé«˜æ€§èƒ½ç®—å­åº“ï¼Œå®ƒæœ‰éå¸¸é‡è¦çš„è¿™æ ·çš„ä¸€ä¸ªä½œç”¨ã€‚

æ‰€ä»¥å®é™…ä¸Šå…¶å®å„å¤§å‚å•†ä¹Ÿéƒ½æ¨å‡ºäº†è‡ªå·±çš„è¿™æ ·çš„é«˜æ€§èƒ½ç®—å­åº“ã€‚åƒè‹±ç‰¹å°”çš„MKLDNè¿˜æœ‰è‹±ä¼Ÿè¾¾çš„é…·Nusç­‰ç­‰ï¼Œè¿™ä¸ªå¤§å®¶åº”è¯¥éƒ½éå¸¸ç†Ÿæ‚‰äº†ã€‚å‘ƒï¼Œä½†æ˜¯ç°åœ¨çš„é«˜æ¸…ç®—å­åº“å®ƒçš„å®ç°å‘¢ã€‚

å®é™…ä¸Šä¸»è¦è¿˜æ˜¯ä¾èµ–äºä¸“å®¶çš„æ‰‹å·¥çš„æ‰‹å·¥çš„ä¼˜åŒ–ã€‚é‚£ä¸€æ–¹é¢å‘¢æ˜¯è¦æ±‚å‘ƒå¼€å‘äººå‘˜ä»–å¯¹ä¸Šå‡ç®—æ³•æœ‰æ·±åˆ»çš„äº†è§£ã€‚ç„¶åå¦å¤–ä¸€æ–¹é¢å‘¢ï¼Œå®é™…ä¸Šä»–å¯¹åº•å±‚çš„ç¡¬ä»¶æ¶æ„ï¼ŒåŒ…æ‹¬ç¼–ç¨‹æ¨¡å‹ï¼Œä»–ä¹Ÿæœ‰æ·±åˆ»çš„ç†è§£ã€‚

ä»–è¿™ä¹Ÿå¯èƒ½å†™å‡ºè¿™æ ·çš„ä¸€ä¸ªé«˜æ€§èƒ½çš„ä»£ç ã€‚æ‰€ä»¥è¿™ä¹Ÿå¯¼è‡´é«˜æ¸…ç®—å­åº“ï¼Œå®ƒçš„å¼€å‘å‘¨æœŸå•Šå’Œå¼€å‘éš¾åº¦éƒ½éå¸¸å¤§ã€‚æ‰€ä»¥éš¾ä»¥æ»¡è¶³ç›®å‰æ·±åº¦å­¦ä¹ åº”ç”¨å¿«é€Ÿéƒ¨ç½²çš„è¿™æ ·ä¸€ä¸ªå®é™…çš„éœ€æ±‚ã€‚å‘ƒå‘ƒï¼Œè¿™æ˜¯å› ä¸ºé«˜æ€§èƒ½ç®—å­åº“å®ƒæœ¬èº«éå¸¸é‡è¦ã€‚

åŒæ—¶å¼€å‘çš„ä»£ä»·åˆéå¸¸å¤§ã€‚æˆ‘ä»¬è®¤ä¸ºç®—å­çš„è‡ªåŠ¨è°ƒä¼˜å’Œè‡ªåŠ¨çš„è¿™æ ·ä¸€ä¸ªä»£ç ç”Ÿæˆå‘ƒï¼Œæ˜¯èƒ½å¤Ÿæœ‰æ•ˆçš„æå‡æ•´ä¸ªé«˜æ€§ç®—å­åº“çš„å¼€å‘æ•ˆç‡å’Œè¿è¡Œæ•ˆç‡çš„ã€‚

ç„¶åè¿™è¾¹ç»™çš„è¿™å¼ å›¾çš„è¯æ˜¯é‚£ä¸ªåŸºäºTVMçš„è¿™æ ·çš„ä¸€ä¸ªå…¸å‹çš„ç®—å­è°ƒä¼˜çš„ä¸€ä¸ªæ•´ä½“çš„ä¸€ä¸ªæ¡†æ¶ã€‚å‘ƒåå…¶ä¸­å‘ƒæœ€æ ¸å¿ƒçš„æ˜¯è¦æœ‰è¿™æ ·çš„ä¸€ä¸ªæ¢ç´¢çš„ä¸€ä¸ªæ¨¡å—ã€‚å‘ƒï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ä»¥è¿™æ ·çš„ç®—å­çš„ä¸€ä¸ªè¡¨è¾¾å¼ä½œä¸ºä½œä¸ºè¾“å…¥ã€‚

ç„¶ååœ¨ç»™å®šçš„ä¸€ä¸ªè°ƒåº¦çš„ç©ºé—´å½“ä¸­å»åšå‘ƒå»åšæ¢ç´¢ã€‚é‚£ä¹ˆä¸ºäº†æå‡è¿™æ ·çš„ä¸€ä¸ªæ¢ç´¢çš„ä¸€ä¸ªæ•ˆç‡çš„è¯ï¼Œä¸€èˆ¬éƒ½ä¼šæ„å»ºèµ·ä¸€ä¸ªå‘ƒæ€§èƒ½é¢„æµ‹çš„è¿™æ ·çš„ä¸€ä¸ªä»£ä»·æ¨¡å‹ã€‚é€šè¿‡è¿™ä¸ªä»£ä»·æ¨¡å‹çš„è¯ï¼Œå®ƒèƒ½å¤Ÿæå‡æ•´ä¸ªæœç´¢çš„ä¸€ä¸ªæ•ˆç‡ã€‚å‘ƒã€‚

é‚£ä¹ˆåœ¨æ‰¾åˆ°äº†ä¸€ä¸ªæœ€ä¼˜çš„è¿™æ ·çš„ä¸€ä¸ªè°ƒåº¦ä¼˜åŒ–çš„ç­–ç•¥ä¹‹åçš„è¯ï¼Œå‘ƒï¼Œä¼šé€šè¿‡ä»£ç çš„è¿™æ ·çš„ä¸€ä¸ªä»£ç ç”Ÿæˆå™¨ï¼Œç„¶åç”Ÿæˆæœ€ç»ˆçš„ä»£ç ï¼Œæ ¸å¿ƒçš„ç›®çš„å‘¢è¿˜æ˜¯å¸Œæœ›èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆï¼Œè¾¾åˆ°æˆ–è€…è¶…è¿‡å‘ƒæ‰‹å·¥ä¼˜åŒ–ä»£ç çš„ä¸€ä¸ªæ€§èƒ½ã€‚ğŸ˜Šï¼Œå‘ƒã€‚

é‚£ä¹ˆå› ä¸ºæ¨¡å‹çš„è§„æ¨¡ä¹Ÿåœ¨ä¸æ–­å¢é•¿ï¼Œåœ¨è¿™æ ·çš„ä¸€ä¸ªå¤§æ¨¡å‹æ—¶ä»£ï¼Œå®é™…ä¸Šç®—å­çš„è‡ªåŠ¨è°ƒä¼˜æŠ€æœ¯çš„è¯ï¼Œä¹Ÿé¢ä¸´äº†ä¸€äº›å‘ƒæ–°çš„ä¸€äº›ä¸€äº›æŒ‘æˆ˜ã€‚é¦–å…ˆç¬¬ä¸€ä¸ªå…¶å®å‰é¢å¾ˆå¤šå˜‰å®¾ä¹Ÿéƒ½æåˆ°äº†ï¼Œå°±æ˜¯ç¡¬ä»¶çš„å¼‚æ„ç¨‹åº¦ï¼Œå®ƒä¼šå˜å¾—è¶Šæ¥è¶Šé«˜ã€‚

å®é™…ä¸Šç°åœ¨å¾ˆå¤šçš„ç®—å­çš„è‡ªåŠ¨è°ƒä¼˜æŠ€æœ¯çš„è¯ï¼Œå…¶å®å‘ƒä¸»è¦è¿˜æ˜¯åœ¨åŒ…æ‹¬åœ¨CPUå•Šè¿˜æœ‰GPUä¸Šå¼€å±•çš„ä¼šæ¯”è¾ƒå¤šå¤šä¸€äº›ã€‚é‚£ä¹ˆåœ¨ä¸€äº›é¢†åŸŸä¸“ç”¨æ¶æ„ä¸Šé¢ï¼Œå®é™…ä¸Šç®—å­è‡ªåŠ¨è°ƒä¼˜å®ƒè¿˜æœ‰å¾ˆå¤§çš„ä¸€ä¸ªæå‡çš„æå‡çš„ç©ºé—´ã€‚

é‚£ä¹ˆç¬¬äºŒä¸ªçš„è¯å°±æ˜¯å› ä¸ºç®—å­çš„è§„æ¨¡ä¹Ÿå˜å¾—éå¸¸çš„å¤§ï¼Œæ‰€ä»¥è°ƒä¼˜å¼€é”€å®é™…ä¸Šä¹Ÿæ˜¯ç¼–è¯‘çš„è¿™æ ·ä¸€ä¸ªå¼€é”€ä¹Ÿä¼šä¹Ÿä¼šå˜å¾—å¾ˆå¤§ã€‚é‚£ä¹ˆæ€ä¹ˆå¯¹è¿™æ ·ä¸€ä¸ªå‘ƒè‡ªåŠ¨è°ƒä¼˜çš„ä¸€ä¸ªè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¹Ÿæ˜¯åœ¨ç¼–è¯‘çš„è¿‡ç¨‹åšä¸€ä¸ªåŠ é€Ÿï¼Œå‘ƒï¼Œä¹Ÿæ˜¯å…³æ³¨çš„ä¸€ä¸ªç‚¹ã€‚

é‚£ä¹ˆç¬¬ä¸‰ä¸ªçš„è¯å°±æ˜¯å‘ƒå¾ˆå¤šå¤§æ¨¡å‹ï¼Œå®ƒéƒ½æœ‰ä¸€äº›ç¨€ç–åŠ¨æ€çš„ç»“æ„ã€‚è¿™å®é™…ä¸Šå¯¹äºæ„å»ºä¸€ä¸ªé«˜æ•ˆï¼Œå¹¶ä¸”å‡†ç¡®çš„ä¸€ä¸ªä»£ä»·æ¨¡å‹ä¹Ÿæå‡ºäº†ä¸€äº›æ–°çš„æŒ‘æˆ˜ã€‚å‘ƒï¼Œé‚£ä¹ˆå‘ƒé‚£ä¹ˆæˆ‘ä»¬çš„è¯åœ¨å‰ä¸¤ä¸ªæ–¹é¢ç›®å‰ä¹Ÿåšäº†ä¸€äº›å·¥ä½œã€‚

é‚£ä¹ˆä»¬åé¢çš„è¯å°±ç®€è¦çš„å‘ƒã€‚ğŸ˜Šï¼Œåˆ†äº«ä¸€ä¸‹æˆ‘ä»¬åœ¨è¿™æ–¹é¢åšçš„ä¸€äº›åˆæ­¥çš„æ¢ç´¢ã€‚é‚£ä¹ˆç¬¬ä¸€ä¸ªæ˜¯é¢å‘ä¸“ç”¨æ¶æ„çš„ä¸€ä¸ªç®—å­ç®—å­è‡ªåŠ¨è°ƒä¼˜çš„æŠ€æœ¯ã€‚å‘ƒå‘ƒå‘ƒå°±åŸºäºæ•´ä¸ªè¿™æ ·çš„ä¸€ä¸ªå‘ƒæœç´¢çš„ä¸€ä¸ªæ¡†æ¶ã€‚

é‚£ä¹ˆæ•´ä¸ªæœç´¢ç©ºé—´å®é™…ä¸Šå†³å®šäº†æˆ‘ä»¬æœ€ç»ˆèƒ½å¤Ÿæ‰¾åˆ°çš„è¿™æ ·çš„ä¸€ä¸ªè°ƒåº¦ç­–ç•¥ã€‚å®ƒçš„æ€§èƒ½å°±æ˜¯æœ€ç»ˆèƒ½å¤Ÿæ‰¾åˆ°çš„ä»£ç çš„æ€§èƒ½ï¼Œåˆ°åº•å®ƒçš„ä¸€ä¸ªä¸Šé™åˆ°åº•æ˜¯ä»€ä¹ˆæ ·çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬è®¤ä¸ºå¯¹äºé¢†åŸŸä¸“ç”¨çš„ç¡¬ä»¶æ¥è¯´ã€‚

æˆ‘ä»¬ä¸ä»…ä»…æ˜¯è¿½æ±‚æ›´å¤§çš„ä¸€ä¸ªæœç´¢ç©ºé—´ã€‚æ›´é‡è¦çš„æ˜¯è¿½æ±‚ä¸€ä¸ªæ›´é«˜è´¨é‡çš„ä¸€ä¸ªæœç´¢ç©ºé—´ã€‚å‘ƒï¼Œè¿™ä¸»è¦è¿˜æ˜¯å› ä¸ºå‘ƒä¸“ç”¨æ¶æ„ï¼Œå®ƒå®é™…ä¸Šè·Ÿé€šç”¨æ¶æ„ç›¸æ¯”ï¼Œå®ƒè¿˜æ˜¯æœ‰å¾ˆå¤šåœ¨ä½“ç³»ç»“æ„å±‚é¢çš„ä¸€äº›çº¦æŸçš„ã€‚æ¯”å¦‚è¯´åƒåœ¨ä¸Šé¢ï¼Œå®ƒåšä¸¾è¯ä¹˜æ³•ã€‚

å®ƒéƒ½æ˜¯åœ¨ä¸€äº›å›ºå®šçš„è§„æ¨¡ä¸Šé¢å»åšã€‚ç„¶ååƒé‡Œé¢çš„ç‰‡ä¸Šçš„å­˜å‚¨éœ€è¦memoryå¯èƒ½æœ‰ä¸€äº›å›ºå®šçš„ä¸€ä¸ªçº¦æŸã€‚é‚£ä¹ˆè¿™æ˜¯å› ä¸ºè¿™äº›çº¦æŸçš„å­˜åœ¨ï¼Œå°±å¯¼è‡´æˆ‘ä»¬å»åˆ»ç”»ä¸€ä¸ªä¸“ç”¨ç¡¬ä»¶ã€‚

å®ƒçš„ä¸€ä¸ªç¨‹åºçš„ä¸€ä¸ªæœç´¢ç©ºé—´å°±å¾ˆéš¾å»åšä¸€ä¸ªå‡†ç¡®çš„æè¿°ï¼Œå°±ä¼šå¯¼è‡´æˆ‘ä»¬åˆ»ç”»çš„ä¸€ä¸ªæœç´¢ç©ºé—´ï¼Œå®ƒè´¨é‡ä¼šå¾ˆå·®ã€‚é‡Œé¢å¾ˆå¤šçš„ç¨‹åºéƒ½å¯èƒ½æ˜¯éæ³•æˆ–è€…æ˜¯æ— æ•ˆçš„ä¸€ä¸ªæ¯”ä¾‹ã€‚å‘ƒæ— æ•ˆçš„ä¸€äº›ç¨‹åºã€‚å‘ƒï¼Œå°±æ¯”å¦‚è¯´åœ¨é‚£ä¸ªå¾®Gå™¨ä¸Šï¼Œé‚£ä¹ˆã€‚

é‚£æˆ‘ä»¬å‘ƒå‰æœŸåšäº†ä¸€äº›å®éªŒï¼Œé‚£ä¹ˆautoTVMçš„æœç´¢ç©ºé—´ï¼Œå®ƒéæ³•ç¨‹åºçš„æ¯”ä¾‹å¯èƒ½ä¼šè¾¾åˆ°å‘ƒ95%ã€‚é‚£ä¹ˆæ­£æ˜¯å› ä¸ºè¿™æœç´¢ç©ºé—´è´¨é‡å¾ˆå·®ã€‚é‚£ä¹ˆä¸€äº›ä¼ ç»Ÿçš„æœç´¢ç®—æ³•åœ¨è¿™ä¸ªé‡Œé¢å»åšæœç´¢çš„è¯ï¼Œå®ƒæ•ˆç‡å°±ä¼šå¾ˆä½ã€‚

å°±å¤§éƒ¨åˆ†æ‰¾åˆ°éƒ½æ˜¯å‘ƒæ— æ•ˆçš„è§£æˆ–è€…æ˜¯ä¸€äº›ä½æ•ˆçš„ä¸€äº›è§£ã€‚ğŸ˜Šï¼Œå‘ƒï¼Œé‚£ä¹ˆå‘ƒè¿™æ ¸å¿ƒçš„åŸå› å‰é¢ä¹Ÿæåˆ°äº†ï¼Œä¸»è¦è¿˜æ˜¯å› ä¸ºä¸“ç”¨æ¶æ„å’Œé€šç”¨æ¶æ„ç›¸æ¯”ï¼Œå®ƒå­˜åœ¨å¤§é‡å‘ƒå¤æ‚å¤šæ ·çš„ä¸€äº›çº¦æŸã€‚å‘ƒï¼Œé‚£ä¹ˆæ­£æ˜¯å› ä¸ºè¿™äº›ä½“ç³»ç»“æ„ã€‚

åŒ…æ‹¬åœ¨ç¼–ç¨‹ç¼–ç¨‹æ¨¡å‹å±‚é¢çš„ä¸€äº›çº¦æŸçš„å­˜åœ¨ã€‚å‘ƒï¼Œå°±å¯¼è‡´æˆ‘ä»¬å»åˆ»åˆ»ç”»å®ƒä¸€ä¸ªç¨‹åºä¼˜åŒ–ç©ºé—´ï¼Œå°±éœ€è¦ä¸€äº›é¢å¤–çš„ä¸€äº›çº¦æŸå˜é‡ã€‚å‘ƒï¼Œä¹Ÿä¹Ÿä¼šæœ‰æ›´å¤šçš„ä¸€äº›çº¦æŸçš„ä¸€äº›æ•°ç›®ã€‚å‘ƒï¼Œæ¯”å¦‚è¯´æˆ‘ä»¬å»åˆ»ç”»ä¸€ä¸ªä¸¾è¯ä¹˜æ³•çš„ä¸€ä¸ªç®—å­ã€‚

è¦å‡†ç¡®çš„æè¿°å®ƒä¸€ä¸ªç¨‹åºä¼˜åŒ–çš„ä¸€ä¸ªç©ºé—´çš„è¯ï¼Œå¯èƒ½éœ€è¦173ä¸ªå˜é‡ã€‚é‚£ä¹ˆå¯¹åº”çš„çº¦æŸçš„æ•°ç›®ä¹Ÿä¼š300å¤šä¸ªã€‚é‚£ä¹ˆéšç€ç®—å­çš„è§„æ¨¡ï¼ŒåŒ…æ‹¬ç®—å­çš„å¤æ‚ç¨‹åº¦çš„å¢åŠ ã€‚

é‚£ä¹ˆå‘ƒéœ€è¦åˆ»ç”»çš„è¿™æ ·çš„ä¸€ä¸ªçº¦æŸå˜é‡å’Œçº¦æŸæ•°ç›®ä¹Ÿä¼šé€æ­¥çš„å¢å¤šã€‚å‘ƒï¼Œé‚£ä¹ˆå¦å¤–ä¸€ä¸ªå±‚é¢çš„è¯ï¼Œå°±æ˜¯å¦‚æœæˆ‘ä»¬æ²¡æœ‰åŠæ³•å¯¹è¿™ä¸ªç¨‹åºç©ºé—´åšä¸€ä¸ªå‡†ç¡®çš„æè¿°çš„è¯ï¼Œå‘ƒï¼Œå°±ä¼šå¯¼è‡´ç°æœ‰çš„ä¸€äº›ä¼˜åŒ–çš„ç®—æ³•å‘€ã€‚

å°±å¾ˆå®¹æ˜“é™·å…¥åˆ°å±€éƒ¨æœ€ä¼˜å‘ƒï¼Œå°±æ¯”å¦‚è¯´åœ¨ä¸€ä¸ªè¿™æ ·ä¸è§„åˆ™çš„ä¼˜åŒ–ç©ºé—´å½“ä¸­å»åšæœç´¢çš„è¯ï¼Œä¼ ç»Ÿçš„åƒä¸€äº›æ¨¡æ‹Ÿé€€ç«çš„æ–¹æ³•å‘ƒï¼ŒåŒ…æ‹¬ã€‚æˆ‘åƒä¸€äº›å‘ƒé—ç®—ç®—æ³•ï¼Œè¿™ä¸ªç”¨çš„æ¯”è¾ƒå¤šçš„å‘ƒï¼Œå®ƒçš„ä¸€ä¸ªè¡¨ç°çš„æ•ˆæœå•Šï¼Œå¯èƒ½è·Ÿéšæœºçš„ç®—æ³•å‘ƒç›¸æ¯”ã€‚

å¹¶æ²¡æœ‰å¤ªå¤šçš„ä¸€äº›ä¼˜åŠ¿ã€‚å‘ƒï¼Œæ‰€ä»¥åœ¨å½“å‰çš„ä¸“ç”¨æ¶æ„ä¸Šé¢ï¼Œç®—çš„æ¡çº¦ï¼Œæˆ‘ä»¬è®¤ä¸ºå­˜åœ¨ä¸¤ä¸ªå¤§çš„é—®é¢˜ã€‚ç¬¬ä¸€ä¸ªå°±æ˜¯è¯´åœ¨æœç´¢ç©ºé—´å½“ä¸­æœ‰æ•ˆè§£çš„æ¯”ä¾‹å‘ƒéå¸¸ä½ã€‚ç¬¬äºŒä¸ªå°±æ˜¯è¯´å‘ƒå¯¹æœç´¢ç©ºé—´è¿›è¡Œæ¢ç´¢ä¼˜åŒ–çš„ä¸€ä¸ªç®—æ³•æ•ˆç‡ä¹Ÿä¹Ÿå¤ªä½äº†ã€‚å‘ƒã€‚

æ‰€ä»¥é’ˆå¯¹è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤ŸåŒæ—¶è§£å†³æœç´¢ç©ºé—´çš„ä¸€ä¸ªç²¾ç¡®æè¿°å’Œå‘ƒé«˜æ•ˆæ¢ç´¢çš„é—®é¢˜ã€‚å‘ƒï¼Œé‚£ä¹ˆé’ˆå¯¹ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå‰é¢ä¹Ÿæåˆ°ï¼Œå› ä¸ºå®ƒçš„é‚£ä¸ªçº¦æŸå‘ƒéå¸¸å¤šï¼Œé‚£ä¹ˆè¦å‡†ç¡®çš„æè¿°çš„è¯ï¼Œå°±æ˜¯äººå·¥å»æ‰‹å†™æ‰‹å†™300å¤šä¸ªçº¦æŸã€‚

ç”šè‡³å†™3800å¤šä¸ªçº¦æŸï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¹Ÿæ˜¯å¸Œæœ›å‘ƒæˆ‘ä»¬å°±é€šè¿‡ä¸€äº›é¢„å®šçš„è§„åˆ™ï¼Œå‘ƒï¼Œå¯¹ç¨‹åºåšåˆ†æï¼Œç„¶åè‡ªåŠ¨çš„ç”Ÿæˆè¿™æ ·ä¸€ä¸ªç²¾ç¡®å‘ƒå¸¦ç²¾ç¡®çº¦æŸçš„ä¸€ä¸ªæœç´¢ç©ºé—´ã€‚é‚£ä¹ˆæœ‰äº†è¿™æ ·ä¸€ä¸ªç²¾ç¡®çš„ä¸€ä¸ªæœç´¢ç©ºé—´ä¹‹åçš„è¯ã€‚

åœ¨è¿™æ ·ä¸€ä¸ªå› ä¸ºå› ä¸ºè¿™ä¸ªç©ºé—´å‘ƒå¯èƒ½ä¼šéå¸¸çš„ä¸è§„åˆ™ã€‚åœ¨è¿™æ ·ä¸€ä¸ªä¸è§„åˆ™çš„ç©ºé—´é‡Œé¢ï¼Œæˆ‘ä»¬å°±ç»§ç»­æå‡ºä¸€ä¸ªä¿æŒçº¦æŸçš„ä¸€ä¸ªæ¼”åŒ–ç®—æ³•æ¥å¯¹è¿™ä¸ªç©ºé—´åšä¸€ä¸ªé«˜æ•ˆçš„æ¢ç´¢ã€‚ğŸ˜Šï¼Œå‘ƒã€‚

é‚£ä¹ˆè¿™ä¸ªçš„è¯å°±æ˜¯å‘ƒæˆ‘ä»¬æ€ä¹ˆå»å¯¹è¿™ä¸ªæœç´¢ç©ºé—´åšä¸€ä¸ªç»§ç»­çš„æè¿°ï¼Œå°±æ˜¯æå‡ºäº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„çº¦æŸç©ºé—´çš„è‡ªåŠ¨ç”Ÿæˆçš„æ–¹æ³•ã€‚ç„¶åè¿™ä¸ªæ¡†å›¾çš„è¯æ˜¯æˆ‘ä»¬æ•´ä¸ªçš„ä¸€ä¸ªå‘ƒå¤§æ¦‚çš„ä¸€ä¸ªæµç¨‹ã€‚å‘ƒï¼Œé‚£ä¹ˆå¯¹äºä¸€ä¸ªç»™å®šçš„ç®—å­æ¥è¯´ï¼Œå‘ƒã€‚

ä¸»è¦æ˜¯ç»™å‡ºå®ƒçš„ä¸€ä¸ªè®¡ç®—çš„ä¸€ä¸ªæè¿°ã€‚åæˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°å®ƒçš„ä¸€ä¸ªè¿ç®—çš„ä¸€ä¸ªå¸¦ä¸ªå›¾ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªå¸¦ä¸ªå›¾çš„ä¸€ä¸ªåŸºç¡€ä¹‹ä¸Šï¼Œå‘ƒï¼Œé€šè¿‡é¢„å®šçš„å‘ƒï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸¤ç±»é¢„å®šçš„è§„åˆ™ï¼Œä¸€ç±»çš„è¯æ˜¯é‚£ä¸ªå‘ƒé’ˆå¯¹è°ƒåº¦ç­–ç•¥çš„ä¸€ä¸ªé¢„å®šçš„è§„åˆ™ã€‚

è¿˜æœ‰ä¸€ç±»æ˜¯é’ˆå¯¹å‘ƒè°ƒåº¦çº¦æŸçš„é¢„å®šè§„åˆ™ã€‚å‘ƒï¼Œé‚£ä¹ˆé¦–å…ˆé€šè¿‡è°ƒåº¦ç­–ç•¥çš„è§„åˆ™ï¼Œæˆ‘ä»¬æ˜¯è‡ªåŠ¨çš„ç”Ÿæˆå®ƒè°ƒåº¦çš„æ¨¡æ¿ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªè°ƒåº¦æ¨¡æ¿çš„åŸºç¡€ä¹‹ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡è°ƒåº¦çº¦æŸçš„è§„åˆ™ï¼Œè‡ªåŠ¨çš„ç”Ÿæˆå®ƒçš„çº¦æŸï¼Œå°±æ˜¯æœ€å³è¾¹é‚£ä¸€æ ã€‚

å°±æœ€å³è¾¹é‚£ä¸€æ æ‰€æ˜¾ç¤ºçš„ã€‚å‘ƒï¼Œè¿™çº¦æŸè¿˜æ˜¯è¿˜æ˜¯æ¯”è¾ƒå¤æ‚çš„ã€‚å‘ƒï¼Œé‚£ä¹ˆæ•´ä¸ªè¿™ä¸ªè¿‡ç¨‹éƒ½æ˜¯å…¨è‡ªåŠ¨çš„ç”Ÿæˆçš„ï¼Œæœ€ç»ˆèƒ½å¤Ÿæ„å»ºèµ·ä¸€ä¸ªå¸¦ç²¾ç¡®çº¦æŸçš„ç¨‹åºæœç´¢ç©ºé—´ã€‚å‘ƒï¼Œé‚£ä¹ˆç¬¬äºŒä¸ªçš„è¯å°±åœ¨æœ‰äº†è¿™æ ·ä¸€ä¸ªæœç´¢ç©ºé—´ä¹‹åçš„è¯ã€‚

å°±æ˜¯å¯¹è¿™ä¸ªæœç´¢ç©ºé—´è¿›è¡Œä¸€ä¸ªé«˜æ•ˆè¦çš„ä¸€ä¸ªæ¢ç´¢ã€‚é‚£ä¹ˆè¿™è¾¹çš„è¯æˆ‘ä»¬æ˜¯æå‡ºä¸€ä¸ªä¿æŒçº¦æŸçš„æ¼”åŒ–çš„è¿™æ ·ä¸€ä¸ªæ–¹æ³•ã€‚å› ä¸ºä¼ ç»Ÿçš„æ¼”åŒ–è®¡ç®—çš„æ–¹æ³•ï¼Œå®ƒå‘ƒæœ¬è´¨ä¸Šæ˜¯å‘ƒæ— çº¦æŸçš„ã€‚å‘ƒï¼Œæ‰€ä»¥å¦‚æœç”¨ä¼ ç»Ÿçš„GAçš„æ–¹æ³•çš„è¯ã€‚

å®ƒå°±å¾ˆå®¹æ˜“åœ¨æœç´¢çš„è¿‡ç¨‹å½“ä¸­å°±è·³å‡ºæˆ‘ä»¬ä¹‹å‰æ‰€å®šä¹‰å¥½çš„ä¸€ä¸ªè¿™æ ·çš„å¸¦çº¦æŸçš„ä¸€ä¸ªç©ºé—´ã€‚å‘ƒï¼Œæ‰€ä»¥æˆ‘ä»¬æå‡ºçš„è¿™ä¸ªæ–¹æ³•èƒ½å¤Ÿä¿è¯åœ¨æ•´ä¸ªæœç´¢çš„è¿‡ç¨‹å½“ä¸­ï¼Œçº¦æŸå§‹ç»ˆå¯ä»¥è¢«æ»¡è¶³ï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜çš„è§£ã€‚å‘ƒã€‚

é‚£ä¹ˆè¿™ä¸ªæ˜¯æˆ‘ä»¬æ•´ä½“çš„ä¸€ä¸ªå®éªŒçš„ä¸€ä¸ªæ•ˆæœã€‚ç„¶åä¹Ÿæ˜¯åœ¨é‚£ä¸ªå‘ƒåœ¨åœ¨åœ¨å‘ƒå¤šä¸ªè‹±ä¼Ÿè¾¾çš„å¹³å°å‘ƒï¼ŒåŒ…æ‹¬V100å•Šï¼ŒT40å’ŒA100ä¸Šæœ‰æ¯”è¾ƒå¥½çš„ä¸€ä¸ªæ•ˆæœã€‚åŒæ—¶åœ¨å…¶ä»–çš„ä¸€äº›å‘ƒé‚£ä¸ªå‘ƒDNAä¸Šã€‚

åŒ…æ‹¬DL boostå’ŒVTAä¸Šä¹Ÿå–å¾—æ¯”è¾ƒå¥½çš„æ•ˆæœã€‚å‘ƒï¼Œç„¶åè¿™ä¸ªå·¥ä½œçš„è¯ï¼Œæˆ‘ä»¬æ˜¯å‘è¡¨åœ¨ä»Šå¹´çš„Sproä¸Šã€‚å‘ƒï¼Œç„¶åç¬¬äºŒä¸ªçš„è¯æ˜¯å‘ƒå¤§æ¨¡å‹ç®—å­çš„ä¸€ä¸ªè‡ªåŠ¨æ¡çº¦çš„ä¸€ä¸ªåŠ é€Ÿã€‚å‘ƒï¼Œé‚£ä¹ˆå‘ƒå¯¹äºå¤§æ¨¡å‹è€Œè¨€ã€‚

ä¸€ä¸ªå‰é¢ä¹Ÿæåˆ°å•Šï¼Œæœç´¢ä¼˜åŒ–çš„æ—¶é—´å¯èƒ½æ˜¯ä¸€ä¸ªæ½œåœ¨çš„ä¸€ä¸ªç“¶é¢ˆã€‚å‘ƒï¼Œæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯å› ä¸ºå®ƒåœ¨æ•´ä¸ªæœç´¢çš„ä¸€ä¸ªè¿‡ç¨‹å½“ä¸­æ˜¯éœ€è¦ç”Ÿæˆå¤§é‡çš„å€™é€‰ç¨‹åºã€‚ç„¶åè¿™äº›å€™é€‰ç¨‹åºå‘¢éƒ½éœ€è¦åœ¨çœŸå®çš„ç¡¬ä»¶ä¸Šé¢å»æ‰§è¡Œã€‚

é‚£ä¹ˆè¿™ä¸ªæ‰§è¡Œçš„ä¸€ä¸€æ–¹é¢æ˜¯å› ä¸ºå®ƒçš„æ•°ç›®å¾ˆå¤šã€‚å¦å¤–ä¸€æ–¹é¢å®ƒæ‰§è¡Œçš„å¼€é”€å¾ˆå¤§ã€‚å‘ƒï¼Œæ‰€ä»¥å®ƒçš„æ•´ä¸ªçš„å‘ƒæœç´¢çš„è¿‡ç¨‹ä¹Ÿä¼šä¹Ÿä¼šæ¯”è¾ƒé•¿ã€‚é‚£ä¹ˆå¤§æ¨¡å‹çš„è§„æ¨¡çš„è¯ï¼Œå‘ƒï¼Œè§„æ¨¡ä¸æ–­çš„å˜å¤§ï¼Œå°±å¯¼è‡´å®ƒçš„æœç´¢ç©ºé—´ä¹Ÿæ›´åŠ çš„å¤§ã€‚

æ‰€ä»¥ä¸€èˆ¬ä¼šè¯„ä¼°æ›´å¤šçš„ç¨‹åºã€‚å‘ƒï¼Œæ¯”å¦‚è¯´åƒåœ¨é‚£ä¸ªauMå’Œ answererä¸Šé¢å¯èƒ½éƒ½éœ€è¦è¯„ä¼°4000ä¸ªç¨‹åºï¼Œæ‰èƒ½ä¿è¯æœç´¢å¾—åˆ°æ¯”è¾ƒå¥½çš„ä¸€ä¸ªå‘ƒæœç´¢åˆ°å¾—åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„ä¸€ä¸ªæ€§èƒ½çš„ç¨‹åºã€‚å‘ƒã€‚

é‚£ä¹ˆä¸ºäº†è§£å†³æœç´¢ä¼˜åŒ–æ—¶é—´è¿‡é•¿çš„é—®é¢˜ï¼Œç„¶åä¹Ÿæœ‰ä¸€äº›ç ”ç©¶å·¥ä½œæ˜¯å‘ƒåŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„è¿™æ ·çš„ä¸€ä¸ªç¼–è¯‘çš„å‘ƒç¼–è¯‘çš„ä»£ä»·æ¨¡å‹ã€‚é‚£ä¹ˆæ ¸å¿ƒçš„æ€è·¯å‘¢å°±æ˜¯è¯´æå‰é€šè¿‡é¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œå‘ƒé¢„è®­ç»ƒçš„æ–¹å¼æå‰æ„å»ºå‡ºä¸€äº›ä»£ä»·æ¨¡å‹ã€‚

è¿™æ ·çš„è¯åœ¨æ•´ä¸ªç¼–è¯‘æœç´¢çš„è¿‡ç¨‹å½“ä¸­ï¼Œå‘ƒï¼Œå°±ç›´æ¥å¯ä»¥åˆ©ç”¨è¿™ä¸ªä»£ä»·æ¨¡å‹æ¥å¯¹å®ƒçš„æ€§èƒ½å‘ƒå»åšé¢„æµ‹ï¼Œå°±ä»£æ›¿å¤§å¤šæ•°çš„ç¡¬ä»¶æµ‹é‡ã€‚å‘ƒï¼ŒåŒæ—¶å‘ƒå¯¹äºå°‘éƒ¨åˆ†æµ‹é‡çš„ç»“æœï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å¯¹è¿™ä¸ªä»£ä»·æ¨¡å‹åšç¿» tuneå‘ƒã€‚

ä½¿å¾—è¿™ä¸ªæ¨¡å‹æ›´åŠ å‡†ç¡®ã€‚ç„¶åè¿™é‡Œé¢çš„ä¸€ä¸ªä»£è¡¨æ€§çš„å·¥ä½œï¼Œå°±æ˜¯å‘ƒå‘ƒå°±æ˜¯tenetå‘ƒtenetå®ƒå’Œä¸ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¿™æ ·ä¸€ä¸ªå‘ƒè¿™æ ·çš„ä¸€ä¸ªæœç´¢ä¼˜åŒ–çš„æ–¹å¼ç›¸æ¯”ï¼Œå®ƒåœ¨ä¼˜åŒ–çš„æ—¶é—´ä¸Šé¢å‡å°‘äº†10å€ã€‚å‘ƒã€‚

ä½†æ˜¯é¢„è®­ç»ƒæ¨¡å‹æ–¹æ–¹æ³•çš„è¿™æ ·çš„ä¸€ä¸ªæ ¸å¿ƒçš„é—®é¢˜å‘¢ï¼Œå°±æ˜¯è¯´å®ƒæ„å»ºå‘ƒæ•´ä¸ªæ¨¡å‹çš„è¿‡ç¨‹å½“ä¸­ï¼Œå®ƒçš„å¼€é”€ä¼šéå¸¸çš„å¤§ã€‚å‘ƒï¼Œæ¯”å¦‚è¯´åƒtensorçš„ï¼Œæˆ‘å¦‚æœè¦é’ˆå¯¹æ•´ä¸ªç¥ç»ç½‘ç»œå»æ„å»ºå…¶å®ƒçš„ä¸€ä¸ªä»£ä»·æ¨¡å‹çš„è¯ã€‚

å®ƒé¦–å…ˆè¦åœ¨æ•´ä¸ªç½‘ç»œå½“ä¸­å»é€‰æ‹©å‘ƒè®¡ç®—çš„ä¸€ä¸ªçº¸å›¾ã€‚ç„¶åå¯¹æ¯ä¸€ä¸ªè®¡ç®—çº¸å›¾å‘ƒï¼Œå»é‡‡æ ·ç­‰ä»·çš„ç¨‹åºæ¥å»åšä¸€ä¸ªæ€§èƒ½çš„ä¸€ä¸ªæµ‹é‡ã€‚å‘ƒï¼Œè¿™é‡Œé¢æ ¸å¿ƒçš„å¼€é”€ï¼Œå°±æ˜¯è¯´åœ¨ç¡¬ä»¶ä¸Šé¢å‘ƒåšæ€§èƒ½æµ‹é‡çš„æ—¶é—´ï¼Œå‘ƒï¼Œå¤§å¤§æ¦‚éœ€è¦4000å°æ—¶ã€‚å‘ƒã€‚

å› ä¸ºåŒ…æ‹¬é‡Œé¢çš„çº¸å›¾çš„æ•°ç›®å•Šï¼Œè¿˜æœ‰æ¯ä¸€ä¸ªçº¸å›¾é‡Œé¢éœ€è¦æµ‹é‡çš„ç¨‹åºæ•°ç›®éƒ½éå¸¸å¤šã€‚ğŸ˜Šï¼Œå‘ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å‘ƒåšçš„ä¸€ä¸ªåˆæ­¥çš„æ¢ç´¢ï¼Œå°±æ˜¯é‡ç‚¹å…³æ³¨æ€ä¹ˆå»é™ä½è¿™é‡Œé¢çš„æ¯ä¸€ä¸ªçº¸å›¾éœ€è¦æµ‹é‡çš„ä¸€ä¸ªç¨‹åºçš„æ•°ç›®ã€‚å‘ƒã€‚

æˆ‘ä»¬ä¹Ÿè§‚å¯Ÿåˆ°åœ¨æ„å»ºé¢„è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹å½“ä¸­ï¼Œå®é™…ä¸Šé‡‡æ ·åˆ°çš„ç¨‹åºçš„æ€§èƒ½ï¼Œå®ƒçš„åˆ†å¸ƒæ˜¯éå¸¸å—¯ä¸å‡è¡¡çš„ã€‚å› ä¸ºæœ¬èº«é‡Œé¢ä¸€äº›é«˜æ€§èƒ½ç¨‹åºï¼Œå®ƒçš„æ¯”ä¾‹å°±éå¸¸çš„ä½ã€‚å‘ƒï¼Œé‚£ä¹ˆè¿‡å¤šçš„æˆ‘ä»¬å»é‡‡æ ·ä¸€äº›ä½æ€§èƒ½çš„ç¨‹åºçš„è¯ã€‚

å°±ä¼šå¯¼è‡´å‘ƒä¼šæœ‰éå¸¸å¤§çš„ä¸€ä¸ªå†—ä½™ã€‚æ‰€ä»¥æˆ‘ä»¬è¦å‡è¡¡ç¨‹åºçš„ä¸€ä¸ªæ€§èƒ½çš„åˆ†å¸ƒæ¥é™ä½å‘ƒé‡‡æ ·å°±è®­ç»ƒé›†å½“ä¸­çš„ä¸€äº›å†—å†—ä½™ã€‚ğŸ˜Šï¼Œå‘ƒï¼Œç„¶åè¿™é‡Œçš„è¯æˆ‘ä»¬ä¹Ÿæ˜¯å‘ƒå‘ƒé’ˆå¯¹å¦‚ä½•å‡å°‘åŸŸè®­ç»ƒæ¨¡å‹çš„ä¸€ä¸ªæ„å»ºå¼€é”€ã€‚

æå‡ºäº†ä¸€ä¸ªåŸºäºåŸºäºä¸»åŠ¨å­¦ä¹ çš„è¿™æ ·çš„ä¸€ä¸ªå‘ƒé¢„è®­ç»ƒçš„æ–¹æ³•ã€‚å‘ƒï¼Œæ ¸å¿ƒæ˜¯å‘ƒæå‡ºäº†åŸºäºå¤šæ ·æ€§é€‰æ‹©ç­–ç•¥çš„ä¸»åŠ¨å­¦ä¹ æ–¹æ³•æ¥é«˜æ•ˆçš„æ„å»ºåŸŸè®­ç»ƒçš„ä»£ä»·æ¨¡å‹ã€‚é‚£ä¹ˆåŸºæœ¬çš„ä¸€ä¸ªè¿‡ç¨‹å°±æ˜¯å·¦è¾¹é‚£ä¸ªæ¡†å›¾ï¼Œå¤§æ¦‚å¤§æ¦‚æœ‰æœ‰å››æ­¥ã€‚

é¦–å…ˆç¬¬ä¸€ä¸ªæ˜¯åšé‚£ä¸ªç¨‹åºçš„ä¸€ä¸ªå‘ƒé‡‡æ ·ã€‚å‘ƒï¼Œè¿™é‡Œé¢çš„è¯ä¼šå‘ƒéšæœºçš„å…ˆé‡‡æ ·å‘ƒï¼Œå¹¶ä¸”æµ‹é‡ä¸€äº›æ€§èƒ½ï¼Œäº§ç”Ÿä¸€äº›æœ‰æ ‡ç­¾çš„æ•°æ®é›†ã€‚ç„¶ååŸºäºè¿™äº›æœ‰æ ‡ç­¾çš„æ•°æ®é›†å»åšæ¨¡å‹çš„è®­ç»ƒã€‚å‘ƒï¼Œç¬¬ä¸‰ä¸ªè¿‡ç¨‹æ˜¯å…³ç¨‹æ˜¯æœ€å…³é”®çš„ã€‚

å°±æ˜¯é€‰æ‹©å…·ä½“é€‰æ‹©å“ªäº›ç¨‹åºåšä¸€ä¸ªæµ‹é‡ã€‚è¿™è¾¹æ˜¯é‡‡ç”¨äº†ä¸€ä¸ªå¤šæ ·æ€§çš„ä¸€ä¸ªé€‰æ‹©ç­–ç•¥ï¼Œå‘ƒï¼Œé€‰æ‹©æœ‰ä»£è¡¨æ€§çš„ä¸€ä¸ªç¨‹åºåšæµ‹é‡ã€‚å‘ƒåå†æŠŠè¿™ä¸ªæµ‹é‡çš„ç¨‹åºåŠ åˆ°æ ‡ç­¾çš„æœ‰æ ‡ç­¾æ•°æ®é›†å½“ä¸­åšæ¨¡å‹çš„ä¸€ä¸ªæ›´æ–°ã€‚å‘ƒã€‚

é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿæ˜¯ä½¿ç”¨tenè¯•çš„5%çš„è¿™æ ·ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå°±è·å¾—äº†æ›´å¥½çš„ä¸€ä¸ªé¢„ç¤ºçš„ç²¾åº¦å’Œä¼˜åŒ–çš„æ•ˆæœã€‚é‚£ä¹ˆé¢„è®­ç»ƒçš„å¼€é”€ä¹Ÿæ˜¯é™ä½äº†20å€ã€‚å‘ƒï¼Œé‚£ä¹ˆè¿™ä¸ªå·¥ä½œçš„è¯ï¼Œæˆ‘ä»¬ä¹Ÿæ˜¯å‘è¡¨åœ¨ä»Šå¹´çš„ICLRä¸Šã€‚å‘ƒã€‚

æœ€ååšä¸€ä¸ªç®€å•çš„ä¸€ä¸ªæ€»ç»“ã€‚å‘ƒï¼Œç„¶åå‘ƒæˆ‘ä»¬è®¤ä¸ºç®—å­è°ƒä¼˜æŠ€æœ¯æ˜¯æå‡é«˜æ€§èƒ½ç®—å­åº“å¼€å‘å’Œè¿è¥æ•ˆç‡çš„éå¸¸æœ‰æ•ˆçš„æ‰‹æ®µã€‚å‘ƒï¼Œé‚£ä¹ˆä¹Ÿæ˜¯éšç€æ·±åœ³å­¦ä¹ å¤§æ¨¡å‹ä¸é¢†åŸŸä¸“ç”¨ç¡¬ä»¶ç¡¬ä»¶çš„ä¸€ä¸ªå¿«é€Ÿçš„å‘å±•ã€‚å‘ƒã€‚

ç®—å­è°ƒä¼˜æŠ€æœ¯ä¹Ÿé¢ä¸´ä¸€äº›æ–°çš„æŒ‘æˆ˜ã€‚å‘ƒï¼ŒåŒ…æ‹¬ç¡¬ä»¶çš„ç¡¬åº¦å‘ƒå¼‚æ„ç¨‹åº¦å˜å¾—è¶Šæ¥è¶Šé«˜ã€‚æ‰€ä»¥å¯èƒ½æ€ä¹ˆå»æå‡åœ¨ä¸“ç”¨æ¶æ„ä¸Šé¢çš„ä¸€äº›ç®—å­è°ƒä¼˜çš„æŠ€æœ¯ï¼Œå¯èƒ½æ˜¯å€¼å¾—å…³æ³¨çš„ä¸€ä¸ªç‚¹ã€‚

ç„¶åå¦å¤–ä¸€ä¸ªå°±æ˜¯è¯´æ€ä¹ˆå»è¿›ä¸€æ­¥å‡å°‘ç®—å­è°ƒä¼˜çš„ä¸€ä¸ªå¼€é”€ã€‚å•Šï¼Œä»¥ä¸Šå°±æ˜¯æˆ‘æŠ¥å‘Šçš„å†…å®¹ï¼Œè°¢è°¢å¤§å®¶ã€‚ğŸ˜Šã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_39.png)

![](img/b8a2226fb956a3703ba6c0ca40227c0d_40.png)

Yeahã€‚å¥½ï¼Œè°¢è°¢è°¢è°¢è°¢è°¢éƒ­ç¦åšå£«ã€‚é‚£ä¸ªéƒ­åšå£«ç»™æˆ‘ä»¬säº†111åˆ†1ã€‚ğŸ˜Šï¼Œ1åˆ†é’Ÿ30ç§’å‘ƒï¼Œå¾ˆå¿«å•Šï¼Œä¸¤ä¸ªå¾ˆå¥½çš„å¾ˆå¥½çš„å·¥ä½œã€‚é‚£ä¸ªå‘ƒæˆ‘è§‰å¾—å¯èƒ½è¿™ä¸ªä»Šå¤©æ—¶é—´å¤ªçŸ­äº†å“ˆã€‚

ä¸‹ä¸€æ¬¡æˆ‘ä»¬å¯èƒ½ä¼šåˆ©ç”¨æˆ‘ä»¬é‚£ä¸ªæ™ºæ´è‡ªå·±æœ¬èº«çš„ä¸€äº›ç¤¾åŒºæ´»åŠ¨ï¼Œå¯ä»¥æœ‰å……è¶³çš„æ—¶é—´ï¼Œæ ·æ ·è¿‡ä¸æ˜¯å†ç»™æˆ‘ä»¬æ›´è¯¦ç»†çš„å»ä»‹ç»è¿™ä¸¤ä¸ªå‘ƒå¾ˆä¸é”™çš„å·¥ä½œã€‚é‚£ä¸‹ä¸€ä¸ªé‚£ä¸ªå‘ƒæ˜¯å‘ƒå¸¦æ¥çš„æ˜¯å‘ƒæ¥è‡ªæˆ‘ä»¬æ™ºæºç ”ç©¶é™¢å•Šã€‚

è´Ÿè´£æ•´ä¸ªæˆ‘ä»¬å‘ƒAIå¹³å°ç ”å‘çš„å‘ƒç”°éªŒå•Šï¼Œç”°è‰³ç»™æˆ‘ä»¬å‘ƒä»‹ç»æ”¯æ´ã€‚æˆ‘ä»¬æ˜¯æ€æ€ä¹ˆæ ·ç”¨æ­å»ºæˆ‘ä»¬çš„æ•´ä¸ªä¹é¼å¹³å°æ¥æ”¯æ’‘å¤§æ¨¡å‹è®­ç»ƒä»¥åŠèŠ¯ç‰‡çš„æ¶æ„çš„å¤šæ ·æ€§ã€‚å¥½ï¼Œè°¢è°¢ã€‚ğŸ˜Šï¼ŒOkayã€‚å—¯ï¼Œå¤§å®¶å¥½ã€‚

æˆ‘æ˜¯æ™ºæºç ”ç©¶é™¢AIç³»ç»Ÿç»„çš„ç”°ç‡•ã€‚å¾ˆè£å¹¸æœ‰è¿™æ ·ä¸€ä¸ªæœºä¼šèƒ½ç»™å¤§å®¶æ±‡æŠ¥ä¸€ä¸‹æˆ‘ä»¬å›¢é˜Ÿçš„å·¥ä½œï¼Œä¹Ÿè¯·å¤§å®¶å¤šå¤šæŒ‡ç‚¹ã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_42.png)

å—¯ï¼Œæ„Ÿä¸€ä¸‹å•Šã€‚è¯¶ã€‚OKæˆ‘ä»Šå¤©æ±‡æŠ¥çš„å†…å®¹å‘¢ä¸»è¦åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ã€‚ç¬¬ä¸€éƒ¨åˆ†å‘¢æ˜¯èƒŒæ™¯å¤§æ¨¡å‹åˆ°åº•éœ€è¦ä»€ä¹ˆæ ·çš„åŸºç¡€è®¾æ–½ã€‚è¿™å—å‰é¢çš„å„ä½è€å¸ˆå·²ç»è®²è¿‡ä¸å°‘äº†ï¼Œæˆ‘ä¼šå¿«ä¸€äº›ã€‚ç¬¬äºŒéƒ¨åˆ†å‘¢æ˜¯å…³äºæˆ‘ä»¬ä¹é¼å¹³å°çš„ä¸€ä¸ªä»‹ç»ã€‚

ä¹é¼å¹³å°å‘¢æ˜¯æˆ‘ä»¬æ™ºæºå†…éƒ¨çš„è¿™ä¸ªäººå·¥æ™ºèƒ½å„ç§ç§‘ç ”å·¥ä½œçš„1ä¸ªAIåŸºç¡€å¹³å°ï¼Œå®ƒæ‰¿è½½äº†å¾ˆå¤šç±»å‹çš„æ¨¡å‹ä»»åŠ¡ã€‚ç”±äºå†…å®¹æ¯”è¾ƒå¤šå‘¢ï¼Œè¿™é‡Œæ—¶é—´åˆæœ‰é™ï¼Œæ‰€ä»¥æˆ‘ä»¬åªé€‰å–äº†ä¸‰ä¸ªå¤§æ¨¡å‹æ¯”è¾ƒç›¸å…³çš„éƒ¨åˆ†æ¥è®²ã€‚

æœ€åå‘¢æ˜¯å¯¹æœªæ¥çš„ä¸€äº›æ€è€ƒç®—æ˜¯æŠ›ä¸ªç –å§ã€‚é‚£ä¹ˆé¦–å…ˆæ˜¯èƒŒæ™¯å¤§å®¶åœ¨å‰é¢è®²çš„ä¹Ÿæ¯”è¾ƒå¤šäº†ã€‚é‚£æˆ‘ä»¬ä»å†å²ä¸Šçœ‹ï¼Œè¿™ä¸ªå¤§æ¨¡å‹çš„ä¸‰æ³¢æµªæ½®ï¼Œå…¶å®æ¯ä¸€æ³¢çš„å‘å±•å‘¢éƒ½æ˜¯ä¸€ä¸ªæ•°æ®ç®—æ³•å’Œç®—åŠ›çš„å…±åŒå‘å±•ã€‚

æ•°æ®å’Œç®—åŠ›èƒ½åŠ›å‘¢æœ‰æ•ˆçš„æ”¯æ’‘äº†æ¨¡å‹ç®—æ³•èƒ½åŠ›çš„ä¸æ–­çš„æå‡ã€‚ä½†åŒæ—¶å‘¢å®ƒä¹Ÿindiateäº†ï¼Œåœ¨å½“æ—¶é‚£ä¸ªæ°´å¹³çº¿ä¸‹ï¼Œæˆ‘ä»¬çš„åŸºç¡€è®¾æ–½ã€‚æ™ºèƒ½è¾¾åˆ°çš„è¿™ä¸ªèƒ½åŠ›å‘¢æ˜¯æ¨¡å‹ç®—æ³•èƒ½åŠ›çš„ä¸€ä¸ªå¤©èŠ±æ¿ã€‚å—¯ï¼Œè¿™å—å‘¢ç®—æ³•è§’åº¦è®²ã€‚

å¤§æ¨¡å‹ä¼šè¶Šæ¥è¶Šå¤§è¶Šæ¥è¶Šå¤§ã€‚è¿™ä¸ªç”¨çš„æ•°æ®å‘¢ä¹Ÿä¼šè¶Šæ¥è¶Šå¤šå»trainè¿™äº›æ¨¡å‹çš„æ•°æ®é›†çš„sizeåœ¨æŒç»­çš„å¢é•¿ã€‚é‚£ä¹ˆæ‰€éœ€è¦çš„ç®—åŠ›å‘¢ï¼Œä¹Ÿæ˜¯æœ‰ç€éå¸¸å¤§çš„éœ€æ±‚ã€‚é‚£ä¹ˆåœ¨è¿™æ ·çš„èƒŒæ™¯ä¸‹å‘¢ï¼Œæˆ‘ä»¬åŸºç¡€è®¾æ–½é¦–å…ˆè¦è§£å†³çš„ã€‚

å°±æ˜¯æ€ä¹ˆå»æ”¯æ’‘ä¸Šå±‚çš„æ¨¡å‹çš„workloadï¼Œå»ä¸æ–­çš„å»æ¨é«˜æ¨¡å‹ç®—æ³•èƒ½åŠ›ï¼Œè§¦åˆ°çš„é‚£ä¸ªå¤©èŠ±æ¿ã€‚é‚£ä¹ˆä»å¦å¤–ä¸€ä¸ªè§’åº¦å»çœ‹å‘¢ã€‚æˆ‘ä»¬å‘ƒæˆ‘ä»¬å¤§æ¨¡å‹éœ€è¦ä»€ä¹ˆæ ·çš„åŸºç¡€è®¾æ–½ã€‚è¿˜æœ‰ä¸€ä¸ªé—®é¢˜è¦è€ƒè™‘çš„æ˜¯è¯´ã€‚

å¤§æ¨¡å‹çš„ä¸€ä¸ªå¼€å‘èŒƒå¼æ˜¯ä»€ä¹ˆæ ·çš„ä¼ ç»Ÿçš„æ¨¡å‹å¼€å‘èŒƒå¼ï¼Œå¤§å®¶å¯èƒ½éƒ½æ˜¯æ¯”è¾ƒéƒ½æ˜¯æ¯”è¾ƒç†Ÿæ‚‰çš„ã€‚ä½†å¤§æ¨¡å‹å‘¢ç›®å‰å¤§å®¶å…¬è®¤çš„æ˜¯ä¸¤ä¸ªé˜¶æ®µã€‚æˆ‘çš„åŸºç¡€æ¨¡å‹çš„è®­ç»ƒï¼Œç”¨æµ·é‡çš„é€šç”¨æ•°æ®æ¥åšåŸºç¡€æ¨¡å‹å‘å¸ƒä¹‹åå‘¢ã€‚

ä¸‹æ¸¸çš„è¡Œä¸šå¸Œæœ›ç”¨å°‘é‡çš„è¿™ä¸ªè¡Œä¸šæ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒè¯•é…åˆæ¨ç†çš„ä¸€ä¸ªéƒ¨ç½²ã€‚è¿™æ ·å‘¢æˆ‘ä»¬å¸Œæœ›èƒ½ä»¥æä½çš„æˆæœ¬å»é™ä½AIå¤§è§„æ¨¡äº§ä¸šåŒ–çš„ä¸€ä¸ªé—¨æ§›ã€‚åœ¨è¿™æ ·çš„èƒŒæ™¯ä¸‹ï¼Œè¦æƒ³åšä¸€ä¸ªæ–°çš„AIå¹³å°å»sportè¿™å—çš„è¯ã€‚

æ˜¾ç„¶æ˜¯è¦æ”¯æŒè¿™ä¸ªå¤§æ¨¡å‹çš„èŒƒå¼çš„ã€‚æ‰€ä»¥è¯´æ€»ç»“èµ·æ¥å‘¢ï¼Œæˆ‘ä»¬çš„AIåŸºç¡€è®¾æ–½å¹²çš„å‘¢æ˜¯ä¸¤å¤§éƒ¨åˆ†çš„å·¥ä½œã€‚ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ€ä¹ˆå»sportä¸Šå±‚çš„è¿™ä¸ªworkloadå»æè‡´çš„ä¼˜åŒ–æˆ‘ä»¬çš„ç®—åŠ›èƒ½åŠ›å»æ¨é«˜æ•´ä¸ªçš„æ¨¡å‹ç®—æ³•è¿™ä¸€å—èƒ½touchåˆ°çš„å¤©èŠ±æ¿ã€‚

ç„¶åç¬¬äºŒéƒ¨åˆ†å‘¢æ˜¯è¯´æˆ‘ä»¬æ€ä¹ˆå»æ”¯æŒæˆ‘ä»¬çš„ã€‚æ¨¡å‹ç ”å‘æµç¨‹å»æé«˜æ¨¡å‹çš„ç”Ÿäº§æ•ˆç‡ã€‚é‚£ä¹ˆé¦–å…ˆæˆ‘å…ˆè®²ä¸€ä¸‹è¿™ä¸ªMLopsçš„éƒ¨åˆ†ï¼Œè¿™ä¸ªæ‰€è°“çš„å„ç§çš„opså‘¢ï¼Œå®ƒå…¶å®ä¸Šåšçš„æ˜¯ã€‚å·¥ä½œè´¨é‡å’Œæ•ˆç‡çš„ä¸€ä¸ªæå‡å’Œä¿éšœã€‚å®ƒçš„æœ¬è´¨æ˜¯è¯´ã€‚

æˆ‘ç¬¬ä¸€è¦è§£å†³å·¥ä½œæµç¨‹çš„æŠ½è±¡å’Œè¿™ä¸ªæŒç»­çš„ä¼˜åŒ–ã€‚ç¬¬äºŒç‚¹å‘¢è¦è§£å†³æˆ‘å¹¿ä¹‰çš„æ•°æ®èµ„äº§çš„ç®¡ç†ã€‚æ¯”å¦‚è¯´æˆ‘çš„ä»£ç æ¨¡å‹ã€æ•°æ®é›†ç‰ˆæœ¬å’Œè¡€å‘˜çš„ç®¡ç†ã€‚ç¬¬ä¸‰ç‚¹å‘¢æ˜¯ä¸€äº›èµ„æºçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚æ¯”å¦‚è¯´æˆ‘åº•å±‚çš„ç®—åŠ›èµ„æºä»»åŠ¡çš„åˆ†é…å’Œè°ƒåº¦ã€‚

é‚£ä¹ˆè½åˆ°è¿™ä¸ªå¤§æ¨¡å‹è¿™ä¸€å—çš„è¯ï¼Œå¤§æ¨¡å‹æœ‰è¿™ä¹ˆå‡ ä¸ªæŒ‘æˆ˜ã€‚ä¸€ä¸ªæ˜¯è¯´å®ƒçš„æ–°èŒƒå¼å‘¢å¯¹å¹³å°çš„è¿™ä¸ªéœ€æ±‚æ˜¯åœ¨æŒç»­æ¼”è¿›å˜åŒ–çš„ã€‚è¿™ç‚¹é¹å‹ä¹Ÿæåˆ°äº†ï¼Œæˆ‘ä»¬è¦ç”¨å‘å±•çš„çœ¼å…‰å»çœ‹é—®é¢˜ï¼Œè½åˆ°å¤§æ¨¡å‹è¿™æ ·ä¸€ä¸ªæ–°å…´é¢†åŸŸæ¥è¯´ä¹Ÿæ˜¯éå¸¸å¿…è¦çš„ã€‚

é‚£ä¹ˆåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å‘¢ï¼Œç”±äºå¤§æ¨¡å‹å®ƒæœ¬èº«çš„è®­ç»ƒæˆæœ¬å¾ˆé«˜ã€‚ç„¶ååé¢çš„å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½éå¸¸ä¾èµ–äºåŸºç¡€æ¨¡å‹èƒ½åŠ›ã€‚æ‰€ä»¥åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å‘¢ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹æœ¬èº«çš„ç®¡ç†è¯„æµ‹ä¼˜åŒ–ï¼Œå‘å¸ƒè¿™ä¸€å—çš„èƒ½åŠ›è¦æ±‚ä¼šæ¯”æ™®é€šå¹³å°è¦æ›´é«˜ã€‚

é‚£ä¹ˆå¤§æ¨¡å‹ç¬¬äºŒç‚¹ç‰¹ç‚¹ã€‚æ˜¯è¯´æˆ‘çš„è®­ç»ƒæ—¶é—´éå¸¸éå¸¸çš„é•¿ï¼Œæˆæœ¬å¾ˆé«˜ï¼Œä¸­é—´ä»»ä½•ä¸€ä¸ªç¯èŠ‚å‡ºé”™ï¼Œå¯¹æ•´ä¸ªçš„è¿™ä¸ªå‘å¸ƒéƒ½æ˜¯æœ‰å½±å“çš„ã€‚æ‰€ä»¥è¿™ä¸€å—çš„è¯ï¼Œæˆ‘ä»¬æ‰€ä»¥è¿™ä¸€å—æˆ‘ä»¬è¦æ³¨æ„æ¯ä¸€ä¸ªç¯èŠ‚è®­ç»ƒçš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚

ç¬¬ä¸‰ç‚¹å‘¢æ˜¯å’±ä»¬åœ¨ç°åœ¨è¿™ä¸ªé˜¶æ®µå»åšå¹³å°çš„è¯ï¼Œå¹³å°ä¸€å®šä¼šé¢ä¸´ç€æ™®é€šæ¨¡å‹å’Œå¤§æ¨¡å‹æ··è·‘çš„è¿™ä¸ªæƒ…å†µã€‚è¿™å¯¹å¹³å°æœ¬èº«çš„è¿™ä¸ªçµæ´»åº¦è¦æ±‚ä¹Ÿä¼šéå¸¸çš„é«˜ã€‚æ‰€ä»¥è¿™é‡Œè§£å†³æ–¹æ³•å‘¢ï¼Œæˆ‘ç®—æŠ›ä¸ªç –å§ã€‚ä¸€ä¸ªæ˜¯åœ¨è®¾è®¡ä¸Šé¢ã€‚

æˆ‘ä»¬è¦æŠŠè¿™ä¸ªåŸºç¡€èƒ½åŠ›è¿›è¡Œæ¨¡å—åŒ–ï¼Œè¿™æ ·èƒ½ä¿è¯æˆ‘ä»¬çš„äº§å“èƒ½æœ‰çµæ´»çš„è¿­ä»£æ€§ï¼Œé«˜å†…èšä½è€¦å’Œï¼Œè¿™ä¸ªæ˜¯éå¸¸å¿…è¦çš„ã€‚ç¬¬äºŒç‚¹æ˜¯ã€‚å¤§å®¶åšè¿™ä¸ªå¹³å°çš„æ—¶å€™ï¼Œä¸€å®šæ˜¯æ ¹æ®è‡ªèº«çš„ä¸šåŠ¡åœºæ™¯éœ€æ±‚ã€‚

ä»¥æ ¸æ¨¡å‹ä¸ºåšåšæ ¸å¿ƒèµ„äº§æ¥å»è®¾è®¡è‡ªå·±çš„è¿™ä¸ªå…¨æµç¨‹çš„ã€‚å½“ç„¶è¿™ä¸ªè¿‡ç¨‹ä¸­è¦å¾ˆæ³¨æ„æ¨¡å‹è¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„å»ºè®¾ã€‚æœ€åä¸€ç‚¹å‘¢æ˜¯ç”±äºå¤§æ¨¡å‹æ•´ä¸ªä»»åŠ¡æµç¨‹çš„å¤æ‚æ€§ã€‚

æˆ‘ä»¬è¦éå¸¸é‡è§†æ¨¡å‹ä»»åŠ¡ç”Ÿäº§ç¯å¢ƒå’Œæ•°æ®é›†ä¹‹é—´è¡€ç¼˜å…³ç³»çš„ä¿å­˜å’Œç»´æŠ¤ã€‚è¿™ä¸ªåœ¨ä¼ ç»Ÿçš„ä¸€äº›è®¡ç®—å¹³å°ä¸Šé¢æ˜¯ç›¸å¯¹æ¯”è¾ƒå¼±çš„ã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥åšåˆ°æ•´ä¸ªé“¾æ¡çš„è¿™ä¸ªè¿½æº¯å’Œå¯¹æ¯”ã€‚ğŸ¤§sorryå‘ƒï¼Œæ”¾åˆ°ä¹é¼å¹³å°ä¸Šå‘¢ã€‚

æˆ‘ä»¬ä¹é¼å¹³å°çš„ç§‘ç ”ä»»åŠ¡æ›´å¤šã€‚æ‰€ä»¥æ—¥å¸¸æƒ…å†µä¸‹å‘¢ã€‚ä¼šå¯¹å‘ƒæ¨¡å‹è®­ç»ƒæœ¬èº«çš„ä»»åŠ¡ç®¡ç†å’Œã€‚å‘ƒï¼Œè®­ç»ƒæ•ˆç‡çš„ä¼˜åŒ–è¿™æ–¹é¢è¦æ±‚æ¯”è¾ƒé«˜ã€‚ä½†æ˜¯åœ¨æ¨ç†æœåŠ¡éƒ¨ç½²æ–¹é¢å‘¢ï¼Œè¯‰æ±‚ä¼šå°‘ä¸€äº›ã€‚

æˆ‘ä»¬æ•´ä¸ªå‘¢æ˜¯ä»¥ä¸€ä¸ªå»ºè®¾å¹³å°ç”Ÿæ€çš„æ€è·¯æ¥åšè¿™ä¸ªäº‹æƒ…çš„ã€‚æ•°æ®æ–¹é¢å‘¢æœ‰æ•°æ®å¹³å°å»å®Œæˆæ•°æ®çš„æ¸…æ´—æ ‡æ³¨å’Œå‹ç¼©ã€‚æ¨¡å‹è¯„æµ‹æ–¹é¢å‘¢æœ‰è¿™ä¸ªã€‚åˆšå‘å¸ƒçš„flagæ¥å®Œæˆæ¨¡å‹çš„å¤šç»´åº¦è¯„æµ‹ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰model hubæ¥å‘å¸ƒæ¨¡å‹ã€‚

åœ¨ä¹é¼å¹³å°å†…éƒ¨è¿™ä¸€å—çš„è¯ï¼Œæˆ‘ä»¬çš„å¤§æ¨¡å‹çš„m opç°åœ¨æ˜¯ä»¥ä¸¤æ¡çº¿è·¯æ¥ç»„ç»‡çš„é¦–å…ˆæ˜¯è¿™ä¸ªåŸºç¡€æ¨¡å‹çš„å¼€å‘è®­ç»ƒå’Œè¯„æµ‹å‘å¸ƒçš„é˜¶æ®µã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­å‘¢ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåœ¨ä¸€ä¸ªå¤§æ¨¡å‹æŒç»­è®­ç»ƒçš„è¿‡ç¨‹ä¸­ã€‚

è¿™ä¸ªæ‹‰ checkpointå‡ºæ¥å»åšè¯„æµ‹ï¼Œä¹Ÿæœ‰å¯èƒ½ä¼šreåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬å»åå¤çš„å»è¿­ä»£ã€‚ç›´åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªæ»¡æ„çš„ç»“æœå‡ºæ¥ã€‚é‚£ä¹ˆä¸‹é¢å‘¢ä¼šåŸºäºè¿™ä¸ªåŸºç¡€æ¨¡å‹çš„å‘å¸ƒå‘ƒåŸºç¡€æ¨¡å‹å‘¢å»åšä¸€äº›è®­ç»ƒå¾®è°ƒã€‚

åŒæ ·ä¹Ÿç¦»ä¸å¼€ checkpointä¿å­˜å’Œè¯„æµ‹çš„è¿™ä¸€ä¸ªè¿‡ç¨‹ã€‚é‚£ä¹ˆä»¥ä¸Šå‘¢å°±æ˜¯æˆ‘ä»¬åœ¨M opæ–¹é¢çš„ä¸€äº›å¯¹å¤§æ¨¡å‹çš„æ¢ç´¢ã€‚ç¬¬äºŒéƒ¨åˆ†å‘¢æ˜¯è¿™ä¸ªè®­ç»ƒæ•ˆç‡ä¼˜åŒ–ï¼Œè¿™æ˜¯ä¸ªéå¸¸å¤§çš„topã€‚å®ƒä¼šæ¶‰åŠåˆ°æˆ‘ä»¬çš„åº•å±‚çš„ç®—å­ä¼˜åŒ–ã€‚

æ•´ä¸ªçš„èµ„æºè°ƒåº¦ä»»åŠ¡ç®¡ç†å’ŒIæ¡†æ¶çš„é…åˆï¼Œæ¶‰åŠåˆ°ä¸€äº›ã€‚å¹¶è¡Œè®­ç»ƒçš„ä¼˜åŒ–æ–¹æ¡ˆè¿™å—å‘ƒæ¶‰åŠéå¸¸å¹¿ã€‚ä½†æ˜¯æˆ‘è¿™é‡Œå‘¢åªæŠ½äº†æˆ‘ä»¬å¹³å°åšæ‰‹åŠ¨çš„æ€§èƒ½åˆ†æå’Œä¸€äº›è‡ªåŠ¨å¹¶è¡Œä¼˜åŒ–æ–¹é¢çš„å·¥ä½œæ¥ã€‚ç»™å¤§å®¶æ±‡æŠ¥ã€‚é¦–å…ˆå‘¢è¿™ä¸ªè®­ç»ƒçš„ä¼˜åŒ–å‘¢ã€‚

å®ƒæ˜¯ä¸ªæŒç»­è¿­ä»£çš„è¿™æ ·ä¸€ä¸ªloçš„è¿‡ç¨‹ï¼Œä¸å¯èƒ½æ˜¯ä¸€ä¸€æŠŠæå®Œçš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¹³å°åœ¨è¿™æ–¹é¢å‘¢ã€‚å—¯ï¼Œæ˜¯åšäº†ä¸€å®šçš„è¿™ä¸ªæ”¯æŒçš„å¹³å°è¿™å—æˆ‘ä»¬è¦æƒ³è·‘ä¸€ä¸ªä»»åŠ¡çš„è¯ï¼Œé¦–å…ˆè¦åˆ›å»ºä¸€ä¸ªå®éªŒã€‚

è¿™ä¸ªå®éªŒé‡Œé¢å‘¢ä¼šç®¡ç†ä¸€ç»„ç›¸å…³çš„ä»»åŠ¡é…ç½®ä»¥åŠè¿™äº›ä»»åŠ¡çš„è¿è¡Œè®°å½•ã€‚æ‰€ä»¥åœ¨åˆ›å»ºè¿™ä¸­é—´çš„æŸäº›ä»»åŠ¡çš„æ—¶å€™å‘¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥å»é‚£ä¸ªçº¢æ¡†æ¡†å‡ºæ¥åœ°æ–¹ï¼Œæˆ‘ä»¬å¯ä»¥ enableæˆ‘ä»¬çš„è¿™ä¸ªæ–°æ€§èƒ½åˆ†æçš„é‡‡æ ·ã€‚

æˆ‘ä»¬æ”¯æŒçš„è¿™ä¸ªæ ‡å‡†çš„æ¥å£ï¼Œä¹Ÿæ”¯æŒä¸€äº›è‡ªå®šä¹‰çš„æ•°æ®é‡‡æ ·åˆ†æã€‚æ¯”å¦‚è¯´æˆ‘å¯ä»¥çœ‹åˆ°ä¸€ä¸ªsstepé‡Œå¤´å„é˜¶æ®µçš„è€—æ—¶æ˜¯å¤šå°‘èµ„æºå ç”¨æ˜¯å¤šå°‘ã€‚é€šè¿‡è¿™äº›åˆ†æä¼˜åŒ–å®Œæˆä¹‹åå‘¢ï¼Œæˆ‘ä»¬ä¼šåœ¨å¹³å°å¯åŠ¨æ–°çš„è¿™ä¸ªè®­ç»ƒä»»åŠ¡ã€‚

èƒ½è·Ÿå†å²ä»»åŠ¡å»åšå›æº¯å’Œæ¯”å¯¹ï¼Œä¹Ÿèƒ½çœ‹åˆ°è¿™ä¸ªä»»åŠ¡çš„èµ„æºä½¿ç”¨æƒ…å†µã€‚é‚£ä¹ˆè¿™ä¸€å—å‘¢åŸºæœ¬ä¸Šéƒ½æ˜¯å¤§å®¶è¦æŠ“æ•°æ®å»åšæ‰‹åŠ¨åˆ†æçš„éƒ¨åˆ†ã€‚ç„¶è€Œå¹¶è¡Œè®­ç»ƒè¿™ä¸€å—çš„ä¼˜åŒ–å‘¢ï¼Œå…¶å®ä¸Šå—¯å¤æ‚åº¦éå¸¸éå¸¸å—¯ sorryorå¤æ‚åº¦éå¸¸ã€‚

è¿™é‡Œæˆ‘å—ä¸€ä¸ªç®€å•çš„ä¾‹å­å…ˆç»™å¤§å®¶æ¥çœ‹ï¼Œè¿™æ˜¯æˆ‘ä»¬é‚£ä¸ªä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„case baselinelineå‘¢æ˜¯ä¸€ä¸ªTPåŠ zero twoçš„å¤§å®¶å¯ä»¥ä»è¿™ä¸ªæˆªå›¾ä¸Šçœ‹åˆ°ï¼Œæˆ‘åœ¨é‚£ä¸ªoptimizerçš„é˜¶æ®µå»åšçš„æ—¶å€™ã€‚

å…¶å®è€—æ—¶æ˜¯éå¸¸é•¿çš„é€šä¿¡ä¹Ÿå¾ˆå¯†é›†ã€‚ç¬¬äºŒä¸ªå‘¢æ˜¯ç»“å½“æ—¶ç»“åˆé‚£ä¸ªä»£ç æˆ‘ä»¬ä¹Ÿå‘ç°äº†ä¸€éƒ¨åˆ†çš„å†—ä½™è®¡ç®—è¿™ä¸ªä¼˜åŒ–å‘¢æ¯”è¾ƒç®€å•ï¼Œæˆ‘ä»¬å¢åŠ äº†çš„ bucket sizeæé«˜äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œç„¶åå‘¢ä¼˜åŒ–ä»£ç å»é™¤äº†å†—ä½™è®¡ç®—ã€‚

è¿™å—æ˜¯è¾¾åˆ°äº†ä¸€ä¸ª1ã€‚17å€çš„æ€§èƒ½æå‡ã€‚åé¢å‘¢æˆ‘ä»¬ä¹Ÿå…¶å®æ¢ç´¢äº†ä¸€äº›å…¶ä»–æ–¹æ¡ˆã€‚è¿™ä¸ªå›¾å‘¢æ˜¯åœ¨ableäº†é‚£ä¸ªzero3çš„æ—¶å€™çš„æƒ…å†µã€‚å¤§å®¶å¯ä»¥å¯ä»¥çœ‹åˆ°è¿™ä¸¤æ®µçš„å†…å®¹ã€‚ğŸ˜Šï¼Œå—¯åå‘è®¡ç®—ã€‚

åè¿™ä¸ªred sorryoræŠ•å½±å¯èƒ½æœ‰ç‚¹ç³Šã€‚ç„¶åå‘¢ï¼Œæ¥ä¸‹æ¥ä¸‹ä¸€å±‚çš„è¿™ä¸ªorgangeå‘ƒ backwardå’Œè¿™ä¸ªredè¿™ä¸ªçº¢è‰²çš„éƒ¨åˆ†æ˜¯å®Œå…¨å¯ä»¥æ”¾åœ¨æå‰æ¥åšçš„å®ç°ä¸€ä¸ªè®¡ç®—å’Œé€šä¿¡çš„ overlapã€‚

æ‰€ä»¥è¿™å—å‘¢æˆ‘ä»¬é‚£ä¸ªä¿®æ”¹ä¼˜åŒ–ä¹‹åå‘¢ï¼Œæ˜¯å¤§å®¶å¯ä»¥çœ‹åˆ°èƒ½åšåˆ°ä¸€éƒ¨åˆ†çš„è¿™ä¸ªoverå¤§çº¦æå‡äº†10%çš„è¿™ä¸ªæ€§èƒ½ã€‚å—¯ï¼Œè¿™ä¸ªå‘¢æ˜¯æ‰‹åŠ¨ä¼˜åŒ–çš„ä¸€ä¸ªæ¡ˆä¾‹ã€‚å…¶å®è¿™ä¸€å—çš„å¹¶è¡Œä¼˜åŒ–å·¥ä½œæ˜¯ä¸€ä¸ªç‰¹åˆ«å¤æ‚çš„äº‹æƒ…ï¼Œä¸€æ—¦èŠ‚ç‚¹è§„æ¨¡ä¸Šå»äº†ã€‚

æˆ‘çš„å¹¶è¡Œç­–ç•¥ï¼Œæ€ä¹ˆå»æ‹†ã€‚è¿™æ˜¯ä¸ªå¾ˆéº»çƒ¦çš„é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä»¬ä¹Ÿå’ŒåŒ—å¤§çš„å´”æ–Œè€å¸ˆå’Œå›¾å›¢é˜Ÿè¿™è¾¹åˆä½œåœ¨å¹³å°ä¸Šé›†æˆäº†ä¸€ä¸ªè‡ªåŠ¨å¹¶è¡Œä¼˜åŒ–æ–¹æ¡ˆå«ã€‚ç„¶åä»–å‘¢èƒ½å¤Ÿå¯¹æˆ‘ä»¬çš„ç¡¬ä»¶ç¯å¢ƒå’Œæ¨¡å‹è¿›è¡Œé‡‡æ ·çš„è¯„ä¼°ã€‚

ç„¶åè¿›è¡Œè‡ªåŠ¨çš„æœç´¢ç”Ÿæˆå¹¶è¡Œç­–ç•¥ã€‚å—¯æ‹¿è¿™ä¸ªç­–ç•¥æ¥å»åšè®­ç»ƒã€‚é‚£ä¹ˆè¿™å—çš„å‘¢å³è¾¹æ”¾äº†ä¸€ä¸ªå‘ƒå‘ƒ2G16å¡çš„ä¸€ä¸ªbotçš„å®éªŒæ•ˆæœã€‚å¯ä»¥çœ‹åˆ°æœ€å³è¾¹çš„è¿™ä¸ªgarçš„è¿™ä¸ªæ€§èƒ½è¿˜æ˜¯æ¯”è¾ƒæœ‰ä¼˜åŠ¿çš„ã€‚

é‚£ä¹ˆä»¥ä¸Šå‘¢å°±æ˜¯æˆ‘ä»¬è¿™ä¸ªåœ¨å¤§æ¨¡å‹è®­ç»ƒæ–¹é¢ä¼˜åŒ–çš„ä¸€å°éƒ¨åˆ†å·¥ä½œã€‚æœ€åå‘¢æˆ‘ä»¬è¿™ä¸ªå¹³å°å…¶å®æ”¯æŒäº†å¾ˆå¤šç±»å‹çš„å¼‚è´­èŠ¯ç‰‡ã€‚å—¯ï¼Œåœ¨è¿™ä¸ªèŠ¯ç‰‡ä¸Šå‘¢ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰ä¸€äº›è¿™ä¸ªå·¥ä½œæ¥ç»™å¤§å®¶reportã€‚åƒæˆ‘ä»¬æ˜¨å¤©å‘å¸ƒçš„çš„ codeæ¨¡å‹ã€‚

å…¶å®ä¸Šæ˜¯åœ¨å¤©æ•°è‡³æ–°çš„é›†ç¾¤ä¸Šè®­ç»ƒå‘¢ï¼Œæˆ‘ä»¬è·‘äº†36çº§çš„æ‰©å±•åŠ é€Ÿï¼Œèƒ½å¤Ÿè¿ç»­ç¨³å®šè¿è¡Œï¼Œå¹¶ä¸”åœ¨ä¸­é—´å‘¢enableableäº†åƒflash attentionè¿™ç±»çš„ä¼˜åŒ–æŠ€æœ¯ã€‚

æˆ‘ä»¬ä¹ŸåŒæ ·åšäº†æ€§èƒ½çš„é‡‡æ ·åˆ†æå’Œä¸€äº›ä¼˜åŒ–å·¥ä½œã€‚å—¯ä¹ˆåœ¨æ˜†ä»‘æ–°ä¸Šé¢å‘¢ï¼Œæˆ‘ä»¬è·‘äº†quiå‘æ˜¨å¤©å‘å¸ƒçš„è¯­è¨€æ¨¡å‹çš„ä¸€äº›æ¨ç†ä»»åŠ¡ã€‚åƒè§†è§‰æ¨¡å‹è¿™ä¸€å—å‘¢ï¼Œæˆ‘ä»¬åœ¨åšæ€§èƒ½çš„è°ƒä¼˜ã€‚

ç„¶åç¡¬ä»¶æ‹“æ™®ç›¸å…³çš„ä¼˜åŒ–éƒ¨åˆ†å·²ç»è¿›å…¥äº†è”è°ƒå’Œæµ‹è¯•çš„è¿™ä¸ªé˜¶æ®µå¾ˆå¿«åº”è¯¥èƒ½ä¸Šçº¿ã€‚é‚£åœ¨è¿™ä¸ªå¯’æ­¦çºªä¸Šé¢å‘¢ï¼Œæˆ‘ä»¬å…¶å®ä¹Ÿè·‘äº†è§†è§‰æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡å’Œä¸€äº›æ£€æµ‹åˆ†å‰²æ¨¡å‹çš„ã€‚å¾®è°ƒã€‚é‚£ä¹ˆåŒ…æ‹¬è¿™ä¸ªåä¸ºçš„å‡è…¾å’Œå¯’æ­¦çºªã€‚

è¿˜æœ‰æ˜†ä»‘æ–°å‘¢ä¸€èµ·æ”¯æŒäº†æ˜¨å¤©å‘å¸ƒçš„å¤§æ¨¡å‹è¯„æµ‹çš„å¹³å°flag eè¿™æ˜¯æˆ‘ä»¬åœ¨å›½äº§èŠ¯ç‰‡ä¸Šçš„ä¸€äº›åº”ç”¨ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å‘¢ï¼Œæˆ‘ä»¬å…¶å®æ˜¯å‘ç°æˆ‘ä»¬çš„å›½äº§èŠ¯ç‰‡æ•´ä¸ªè½¯ç¡¬ä»¶çš„èƒ½åŠ›æ˜¯è¶Šæ¥è¶Šå¥½çš„ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¹Ÿç§¯ç´¯äº†ä¸€äº›ç›¸å…³çš„ç»éªŒã€‚

æ‰€ä»¥æˆ‘ä»¬ä¹Ÿæ¨å‡ºäº†ä¸€æ¬¾è¿™ä¸ªå¼€æºå¼€æ”¾å¼€ç®±å³ç”¨çš„ä¸€ä¸ªä¸€ä¸ªå‘ƒèŠ¯ç‰‡çš„è¯„æµ‹è½¯ä»¶flag proofè¿™ä¸ªæ˜¯å’Œå¤©æ•°ç™¾åº¦æ˜†ä»‘æ–°è¿˜æœ‰åä¸ºåˆšåŠ è¿›æ¥må’Œå‡è…¾ä¸¤ä¸ªå›¢é˜Ÿä¸€èµ·æ¥å…±å»ºçš„ã€‚å®ƒæ”¯æŒå‘ƒä¸åŒã€‚

å®ƒæ”¯æŒæ ‡å‡†çš„chmarkè¿™æ ·ç¡¬ä»¶æœ¬èº«çš„é€‚é…æˆæœ¬ä¼šå¾ˆä½ã€‚ç„¶åæˆ‘ä»¬ä¹Ÿæ”¯æŒå¤šæ¡†æ¶å¤šç§ç±»å‹çš„ä»»åŠ¡æ¨¡å‹ï¼Œå¹¶ä¸”ç›®å‰ä¸ºæ­¢å‘¢æœ‰å¤©æ•°ä¹‹æ˜Ÿå’Œæ˜†ä»‘æ–°ç­‰è¿™ä¸ªç¡¬ä»¶æœ‰å¾ˆå¤šä¸ªæ¨¡å‹caseçš„è¦†ç›–ã€‚æˆ‘ä»¬åœ¨ä¸æ–­çš„å¢å¼ºè¿™ä¸€å—å¯¹äºç”¨æˆ·æ¥è¯´å‘¢ã€‚

æˆ‘ä»¬ä¹Ÿæä¾›éå¸¸å¥½ç”¨çš„å‘½ä»¤èˆªå·¥å…·ã€‚å»æ”¯æŒåƒé“å£ç¯å¢ƒçš„ç®¡ç†ã€‚ç„¶åè¿™ä¸ªæµ‹è¯•ä»»åŠ¡æ•´ä¸ªå…¨ç”Ÿå‘½å‘¨æœŸçš„ä¸€äº›ç®¡ç†ã€‚actlyè¿™ä¸ªé¡¹ç›®å‘¢æ˜¯æˆ‘ä»¬æ•´ä¸ªflag opené£æ™ºå¤§æ¨¡å‹è¿™ä¸ªæŠ€æœ¯å¼€æºä½“ç³»ä¸­çš„ä¸€ä¸ªå°±æ˜¯ä¸€ä¸ªå°çš„é¡¹ç›®å§ã€‚

è¿™ä¸ªå¤§æ¨¡å‹çš„æŠ€æœ¯å¼€æºä½“ç³»å‘¢ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬è‡´æ´å’Œã€‚å›½å†…å¤–å¾ˆå¤šå®¶ä¼ä¸šã€é«˜æ ¡ä»¥åŠç§‘ç ”æœºæ„ä¸€èµ·å…±å»ºçš„å¤§æ¨¡å‹çš„è¿™ä¸ªæŠ€æœ¯ä½“ç³»ï¼ŒåŒ…æ‹¬äº†æˆ‘ä»¬çš„æ¨¡å‹ç®—æ³•çš„ä»£ç ä¸€ç³»åˆ—çš„è¿™ä¸ªæ•°æ®å·¥å…·å’Œæ¨¡å‹è¯„æµ‹å·¥å…·ã€‚

æ‰€ä»¥éå¸¸æ¬¢è¿å¤§å®¶è¿™ä¸ªç°åœ¨æ‹¿èµ·æ‰‹æœºæ‰«ç åŠ å…¥æˆ‘ä»¬çš„è¿™ä¸ªæŠ€æœ¯äº¤æµç¾¤èƒ½å¤Ÿè·å¾—å¾ˆå¤šè¿™ä¸ªå¼€å¼€å‘è€…çš„æ”¯æŒã€‚è¿™ä¸ªå‘ƒè€Œä¸”æˆ‘ä»¬çš„è¿™ä¸ªå¼€æºå¼€æºçš„è¿™ä¸ªlicenseæ˜¯éå¸¸å®½æ¾çš„ã€‚

æ‰€ä»¥å‘ƒå¸Œæœ›å°†æ¥èƒ½æœ‰å…¨çƒçš„è¿™ä¸ªå¼€å‘è€…å’Œæˆ‘ä»¬ä¸€èµ·å»å…±åŒæ¢ç´¢ï¼Œå…±åŒå…±åŒåˆ›æ–°ï¼Œä¹Ÿèƒ½æœ‰ä¼ä¸šæ¥ä¸€èµ·baseåœ¨è¿™ä¸ªä¸Šé¢å»æ„å»ºå®ƒè‡ªå·±çš„AIè½¯ä»¶å’Œå•†ä¸šè½¯ä»¶ã€‚å—¯ã€‚å—¯ï¼Œä»¥ä¸Šå‘¢å°±æ˜¯ä¹é¼å¹³å°çš„ä¸€äº›å·¥ä½œã€‚é‚£æœ€åå‘¢å…³äºæœªæ¥çš„æ€è€ƒã€‚

æˆ‘ç®€å•æŠ›ä¸ªç –å§ï¼Œä¹Ÿå¸Œæœ›å¤§å®¶èƒ½æŒ‡ç‚¹ï¼Œå°±æ˜¯å¼€å¤´è¯´äº†è¿™ä¸ªå—¯åŸºç¡€è®¾æ–½çš„èƒ½åŠ›å’Œä¸Šå±‚ä¸šåŠ¡å‘å±•çš„è¿™ä¸ªèƒ½åŠ›æ°¸è¿œæ˜¯ä¸€ä¸ªç›¸è¾…ç›¸æˆçš„å…³ç³»ã€‚æ‰€ä»¥åšåº•å±‚æœ€å…ˆè¦çœ‹çš„å‘¢ï¼Ÿæ˜¯è¯´æˆ‘ä»¬ä¸Šå±‚æœ‰ä»€ä¹ˆæ ·æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œæ–°çš„æ¨¡å‹ç»“æ„ã€‚

æ–°çš„åº”ç”¨åœºæ™¯å‡ºæ¥ï¼Œå®ƒå¯¹åº•å±‚åˆ°åº•æœ‰ä»€ä¹ˆæ ·çš„è¿™ä¸ªæŒ‘æˆ˜å’Œé—®é¢˜ã€‚æ¯”å¦‚è¯´ä¸–ç•Œæ¨¡å‹è¦æ‹¿å‡ºæ¥æçš„è¯ï¼Œè‚¯å®šä¸æ˜¯ç°åœ¨çš„è¿™ä¸ªè¿™ä¸ªç©æ³•ã€‚é‚£ä¹ˆç«™åœ¨åº•å±‚çš„è§’åº¦çœ‹å‘¢ã€‚

å…¶å®ä¸Šæˆ‘ä»¬è®¤ä¸ºè¿™å››ä¸ªè±¡é™çš„å•æœºçš„åˆ†å¸ƒå¼çš„æ“ä½œç³»ç»Ÿæ¡†æ¶å±‚é¢çš„è¿™äº›ä¸œè¥¿å‘¢ï¼Œéƒ½æ˜¯å€¼å¾—å…³æ³¨çš„ã€‚ä»–ä»¬ä¹‹é—´çš„ååŒçš„å…³ç³»æ˜¯ä»€ä¹ˆæ ·å­ã€‚åœ¨å“ªä¸ªç‚¹å¯èƒ½å–å¾—çªç ´ä¼šå»é¢ è¦†æ•´ä¸ªç°åœ¨infrraçš„è¿™ä¸ªç»“æ„æ˜¯éœ€è¦æŒç»­å…³æ³¨çš„ã€‚

æœ€åéœ€è¦å…³æ³¨çš„è¿˜æœ‰ä¸€ç‚¹ï¼Œå…¶å®ä¸Šäººå·¥æ™ºèƒ½çš„èƒ½åŠ›å‘¢ï¼Œå½±å“åˆ°äº†å„ä¸ªé¢†åŸŸï¼Œä¹ŸåŒ…æ‹¬infrraæœ¬èº«ã€‚AFAI systemè¿™ä¸€å—çš„ä¸€äº›å‘ƒåœ¨å·¥ä¸šç•Œçš„åº”ç”¨è¿›å±•å…¶å®ä¸Šè¿˜æ²¡æœ‰é‚£ä¹ˆå¤šã€‚ä½†æ˜¯åœ¨æœªæ¥çš„è¯ã€‚

æˆ‘ä»¬ä¹Ÿæ˜¯éå¸¸çœ‹å¥½è¿™ä¸€å—çš„å—¯ã€‚å—¯ï¼Œä»¥ä¸Šå°±æ˜¯æˆ‘ä»Šå¤©çš„è¿™ä¸ªå†…å®¹ã€‚å¥½ï¼Œè°¢è°¢å¤§å®¶ã€‚ä¹Ÿè°¢è°¢ã€‚é‚£ä¸ªå—¯å‘ƒå› ä¸ºå› ä¸ºä»Šå¤©é‚£ä¸ªçš„ç¡®æˆ‘ä»¬å®‰æ’äº†å¤ªå¤šå¥½çš„è¿™äº›topå“ˆï¼Œæ‰€ä»¥å‘ƒå¯¼è‡´æˆ‘ä»¬çš„æ—¶é—´ä¹Ÿå¾ˆç´§ï¼Œç›¸å½“ç´§ã€‚

ç»™æ¯ä¸ªspeakerçš„æ—¶é—´ä¹Ÿå¾ˆç´§ã€‚é‚£æ‰€ä»¥å‘¢è¿™å°±æ˜¯é‚£ä¸ªå‘ƒå¯èƒ½ç­‰ä¼šæˆ‘ä»¬ä¼šæœ‰ç¨å¾®æœ‰ä¸€ä¸ªä¸Šçš„è°ƒæ•´ï¼Œå°±æ˜¯è¯´å‘ƒæœ¬æ¥æˆ‘ä»¬æœ€åè¿˜æœ‰ä¸€ä¸ªå°çš„ã€‚é‚£å‘ƒå½“æ—¶å‡†å¤‡äº†ä¸€ä¸ªé—®é¢˜ç»™æˆ‘ä»¬æ¯ä¸€ä½å‚ä¸panelçš„è¿™ä¸ªä¼ä¸šçš„ä»£è¡¨ã€‚

é‚£æˆ‘ä»¬åœ¨è¿™é‡Œå¤´å‘¢å°±æ˜¯è¯´å‘ƒæˆ‘ä»¬æœ€åä¼šæŠŠé‚£ä¸ªcanå°±å…ˆå–æ¶ˆï¼Œå–è€Œä»£ä¹‹çš„æ˜¯è¯´å‘ƒåé¢æ¯ä¸ªä¼ä¸šä¸Šæ¥çš„æ—¶å€™ï¼Œä»–ä»¬å¯ä»¥å‘ƒåˆ©ç”¨ä»–çš„speechçš„æ—¶é—´ï¼ŒæŠŠè¿™ä¸ªå‘ƒå¸Œæœ›æƒ³è¯´çš„ï¼Œå°±åµŒåˆ°ä»–çš„speché‡Œå¤´è¿™æ ·å¯èƒ½å¤§å®¶å…±ä»å®¹ä¸€äº›ã€‚

å‘ƒï¼Œå¥½ï¼Œé‚£å‘ƒæˆ‘æˆ‘è¯·å‡ºä¸‹ä¸€ä½å•Šä¸‹ä¸€ä½æ˜¯æ¥è‡ªäºå¿…è®¤çš„ä¸æ€»å‘ƒä»–ç»™æˆ‘ä»¬å¸¦æ¥è¿™ä¸ªåŸºäºé«˜æ€§èƒ½é€šç”¨GPUæ€ä¹ˆæ‰“é€ å›½äº§å¤§æ¨¡å‹çš„è®­ç»ƒç³»ç»Ÿï¼Œè°¢è°¢ã€‚ğŸ˜Šï¼Œå‘ƒæè€å¸ˆå—¯ã€‚å¥½ã€‚

é‚£ä¸ªä»Šå¤©éå¸¸è£å¹¸æœ‰æœºä¼šåœ¨AIç³»ç»Ÿä¸“é¢˜è®ºå›å’Œå¤§å®¶å‘ƒè®¨è®ºäº¤æµã€‚å‘ƒï¼Œæˆ‘æ˜¯æ¥è‡ªé¿åˆƒçš„ä¸äº‘å¸†ã€‚é¿åˆƒç§‘æŠ€æ˜¯é‚£ä¸ªå›½äº§çš„è¿™ä¸ªé€šç”¨GPUçš„å…¬å¸ã€‚å‘ƒã€‚

é‚£æˆ‘ä¹‹å‰åœ¨ç™¾åº¦ä¸€ç›´åœ¨ä»äº‹è¿™ä¸ªAIç³»ç»Ÿå’Œè¿™ä¸ªå¹¿å‘Šæ¨èé¢†åŸŸçš„å¤§æ¨¡å‹ç›¸å…³çš„ä¸€äº›å·¥ä½œå“ˆã€‚ğŸ˜Šï¼Œå—¯ã€‚å¥½å˜ï¼Œé‚£ä»Šå¤©é‚£ä¸ªæˆ‘ä»¬äº¤æµåˆ†äº«å†…å®¹ä¸»è¦åˆ†è¿™ä¸ªä¸‰éƒ¨åˆ†å‘ƒã€‚

ç¬¬ä¸€ä¸ªæ˜¯æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªGPTè¿™ç§å¤§æ¨¡å‹è®­ç»ƒé‚£ä¸ªåˆ°åº•å‘ƒå­˜åœ¨å“ªäº›æŒ‘æˆ˜ã€‚é‚£ç¬¬äºŒä¸ªæ˜¯åŸºäºé€¼è®¤çš„è¿™ä¸ªå›½äº§çš„GPUï¼Œæˆ‘ä»¬æ€ä¹ˆå»è§£å†³è¿™äº›é—®é¢˜ï¼Œæ„å»ºè¿™ä¸ªå¤§æ¨¡å‹çš„è¿™ä¸ªå‘ƒè®­ç»ƒç³»ç»Ÿå•Šã€‚

æœ€åæˆ‘ä»¬çœ‹ä¸€ä¸‹ç›®å‰è¿™ä¸ªæˆ‘ä»¬å¤§æ¨¡å‹çš„äº§ä¸šç”Ÿæ€çš„ä¸€äº›åˆä½œçš„ä¸€äº›è¿›å±•ã€‚ğŸ˜Šï¼Œå‘ƒï¼ŒPæˆ‘è§‰å¾—å¼•çˆ†äº†è¿™ä¸ªæ–°çš„ä¸€è½®ç§‘æŠ€é©å‘½å“ˆã€‚é‚£ä»æˆ‘ä»¬è¿™ä¸ªè‡´æºçš„è¿™ä¸ªè¿‘æœŸä»Šå¤©çš„è¿™äº›ä¼šè®®ï¼Œæˆ‘ä»¬çœ‹åˆ°å¾ˆå¤šè¿™ä¸ªä¸»é¢˜éƒ½æ˜¯è·Ÿå¤§æ¨¡å‹ç›¸å…³çš„ã€‚å‘ƒã€‚

æ‰€ä»¥æ•´ä¸ªè¡Œä¸šç°åœ¨ä¹Ÿä¹Ÿå¤„åœ¨ä¸€ç§ç™¾æ¨¡é’ˆéªŒçš„è¿™ç§çŠ¶æ€å“ˆã€‚æˆ‘ä»¬çœ‹åˆ°å›½å†…å¤–å®é™…ä¸Šæœ‰å¾ˆå¤šè¿™ç§åŸºç¡€çš„è¿™ç§å¤§æ¨¡å‹ã€‚é‚£ä¹Ÿæœ‰å¾ˆå¤šç›¸å…³çš„ä¸€ç§é‡ŠèŒƒçš„åº”ç”¨ï¼Œå¯¹é‚£GPTå‘¢å°±æ˜¯ä¸ºä»€ä¹ˆèƒ½æˆåŠŸè¿™ä¹ˆç«çˆ†å¯¹å§ï¼Ÿ

æˆ‘è§‰å¾—æ€»ç»“èµ·æ¥æ˜¯å·¥ç¨‹å’Œç®—æ³•è¿™æ ·çš„ä¸€ä¸ªååŒçš„ä¸€ä¸ªåˆ›æ–°ã€‚å‘ƒï¼Œè¿™é‡Œé¢å°±è¯´æœ‰ä¸‰å¤§å› ç´ å•Šå•Šï¼Œä»è¿™ä¸ªæ•°æ®å‘ƒç®—æ³•ç®—åŠ›ä¸‰ä¸ªç»´åº¦ã€‚å‘ƒï¼Œæ•°æ®å…¶å®æ˜¯è¿™ä¸ªç‡ƒæ–™ä¹Ÿæ˜¯è¿™ä¸ªåŸºç¡€å“ˆï¼Œæˆ‘è§‰å¾—ä¹Ÿæ˜¯ç°åœ¨å¾ˆå¤šå‘ƒå¤§æ¨¡å‹å…¬å¸é¢ä¸´çš„ç¬¬ä¸€é“çš„éš¾å…³ã€‚

å¯¹å§ï¼Ÿè¿™ä¸ªæ•°æ®ä»é«˜è´¨é‡çš„æ•°æ®ä»å“ªæ¥å‘ƒï¼Œé‚£ç¬¬äºŒä¸ªæ˜¯è¿™ä¸ªç®—æ³•ï¼Œå®é™…å°±æ˜¯è¡Œä¸šä¹‹å‰ä¹Ÿæœ‰ä¸€äº›è¿™ä¸ªè¿™ä¸ªè®¨è®ºå“ˆï¼Œå°±æ˜¯è¯´GPTä¸€ä¸‹ç«çˆ†äº†ï¼Œéƒ½å®ƒæœ‰ä»€ä¹ˆåˆ›æ–°å—ï¼Ÿå¯¹é‚£å…¶å®å¤§å®¶è¯´ã€‚ğŸ˜Šï¼Œå‘ƒï¼Œä»ç®—æ³•çš„è§’åº¦å‘ƒã€‚

å¥½åƒå¾ˆå¤šä¹Ÿæ˜¯å…¬å¼€çš„ç®—æ³•ï¼Œå¯¹å§ï¼Ÿå¯èƒ½æœ‰çš„ä¹Ÿæ˜¯åˆ«çš„å…¬å¸åšçš„ï¼Œé‚£ä»–ä¸ºä»€ä¹ˆèƒ½æˆåŠŸäº†ï¼Ÿæˆ‘è§‰å¾—é™¤äº†ä¸€äº›å…¬å¼€çš„ä¸œè¥¿ä¹‹å¤–ï¼Œè¿˜æœ‰å¾ˆå¤šå‘ƒå…¶å®å°±æ˜¯ç‚¼å•æ•°ï¼Œå¯¹å§ï¼Ÿé‚£è¿™äº›ç»†èŠ‚å…¶å®æ˜¯è¿™ä¸ªæˆåŠŸæŒæ¡åœ¨ç»†èŠ‚ä¸­ã€‚

é‚£è¿™é‡Œé¢æœ‰å¾ˆå¤šåŒ…æ‹¬è¿™ä¸ªåƒæ¯”å¦‚è¯´è™F16å‘¢ï¼Œå®ƒå…¶å®å°±æ˜¯è·ŸèŠ¯ç‰‡çš„è¿™ç§ç‰¹æ€§å‘ƒéå¸¸ç´§å¯†ç›¸å…³çš„ã€‚ğŸ˜Šï¼Œå—¯ï¼Œé‚£ç¬¬ä¸‰ä¸ªæ˜¯ç®—åŠ›ï¼Œæˆ‘è§‰å¾—å¤§æ¨¡å‹å‘ƒéœ€è¦è¿™ä¸ªåŸºæœ¬ä¸Šéœ€è¦æ•°åƒå¼ GPUå¡ã€‚

è¿™ä¸ªä¹Ÿæ˜¯è¿™ä¸ªå¤§å…¬å¸å’Œè¿™ä¸ªå‘ƒæœ‰é’±äººçš„è¿™ä¸ªæ¸¸æˆå“ˆã€‚å¯¹ï¼Œé‚£æ‰€ä»¥è¿™é‡Œé¢å½“ç„¶å¦å¤–ä¸€ä¸ªå°±æ˜¯è¯´å¤§æ¨¡å‹æœ¬èº«çš„è¿™ä¸ªå°±æ˜¯ç‰¹ç‚¹å°±æ˜¯è¶…å¤§åŠŸæ¨¡å‚æ•°éœ€è¦å­˜å‡ºã€‚é‚£ä¹Ÿéœ€è¦è¿™ç§ä¸“ç”¨çš„è®­ç»ƒæ¡†æ¶å•Šï¼Œå»æå‡è¿™ä¸ªæ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

æ‰€ä»¥å®ƒä¹Ÿä¸æ˜¯ç®€å•çš„è¿™ä¸ªå †ç§¯è¿™ä¸ªç®—åŠ›å‘¢ï¼Œå°±èƒ½å¤Ÿå•Šå»è§£å†³è¿™ä¸ªè¿™ä¸ªç®—åŠ›çš„é—®é¢˜ã€‚é‚£å¿…åˆƒç§‘æŠ€ï¼Œæˆ‘ä»¬ä½œä¸ºä¸€å®¶è¿™ä¸ªGPUçš„å‚å•†ï¼Œæˆ‘æˆ‘ä»¬ä¼šèšç„¦åœ¨ç®—åŠ›è¿™ä¸ªç»´åº¦å•Šï¼Œå»ä¿ƒè¿›è¿™ä¸ªå¤§æ¨¡å‹äº§ä¸šç”Ÿæ€çš„ä¸€ä¸ªå‘å±•ã€‚ğŸ˜Šï¼Œå‘ƒã€‚

æˆ‘ä»¬è¿™ä¸ªè®ºå›å°±å«ç³»ç»Ÿè®ºå›å¯¹å§ï¼Ÿæ‰€ä»¥å…¶å®æ•´ä¸ªAIå°¤å…¶æ˜¯å¤§æ¨¡å‹é‡ŒåŸŸï¼Œæˆ‘è§‰å¾—å®ƒæ­£æ˜¯ä¸€ä¸ªç³»ç»Ÿçš„ä¸€ä¸ªå·¥ç¨‹ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªè½¯ç¡¬ä»¶ååŒçš„è¿™æ ·ä¸€ä¸ªå·¥ç¨‹ã€‚é‚£ä»è¿™ä¸ªå›¾ä¸Šæˆ‘ä»¬ä¹Ÿçœ‹åˆ°è¯ã€‚

æˆ‘ä»¬è¿™ä¸ªç¡¬ä»¶çš„è¿™ä¸ªæ¶æ„å’Œæˆ‘ä»¬è½¯ä»¶çš„è¿™ä¸ªè®­ç»ƒçš„æµç¨‹ï¼Œå®ƒå…¶å®æ˜¯éå¸¸ç´§å¯†ç›¸å…³çš„ç¡¬ä»¶ä¸Šçš„è¿™ä¸ªå­˜å‚¨ï¼ŒåŒ…æ‹¬CPUå†…å­˜è¿™ä¸ªGPUçš„ç®—åŠ›ï¼ŒåŒ…æ‹¬è¿™ä¸ªäº’è”å•Šï¼Œè·Ÿæˆ‘ä»¬è½¯ä»¶ä¸Šä½ çœ‹æ¯ä¸€ä¸ªæµç¨‹å…¶å®éƒ½æœ‰ä¸€ä¸ªéå¸¸ç´§å¯†çš„å¯¹ç­‰å…³ç³»å•Šã€‚

æ‰€ä»¥é‚£æ ¸å¿ƒçš„å°±æ˜¯åœ¨äºè¿™ä¸ªå‰çº¿å’Œåå‘è®¡ç®—é‡Œé¢å¯¹ç®—åŠ›çš„éœ€æ±‚å•Šï¼Œå¦å¤–ä¸€ä¸ªåœ¨è¿™ç§å¤§è§„æ¨¡åˆ†å¸ƒå¼çš„åœºæ™¯çš„è¯ï¼Œå¯¹è¿™ä¸ªè¿™ä¸ªå‚æ•°ååŒæ¢¯åº¦æ›´æ–°çš„è¿™ç§è¦æ±‚å¯¹å•æœºå†…éƒ¨çš„äº’è”å’Œå¤šæœºçš„äº’è”å‘¢éƒ½æäº†å¾ˆé«˜çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚

é‚£å¦å¤–ä¸€ä¸ªæˆ‘ä»¬è¯´è¿™ä¸ªå¤§æ¨¡å‹è®­ç»ƒä¹Ÿæ˜¯ä¸€ä¸ªç®—æ³•å’Œå·¥ç¨‹ååŒä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´å‘¢ï¼Ÿå› ä¸ºåœ¨è¿™ä¸ªç³»ç»Ÿé‡Œé¢æ¯”å¦‚æˆ‘ä»¬ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œä»¥ sizeä¸ºä¾‹ï¼Œæå‡ sizeä¸Šæ˜¯æœ‰åˆ©äºæå‡è¿™ä¸ªè®¡ç®—çš„æ•ˆç‡å‘æŒ¥æ–°ã€‚ğŸ˜Šã€‚

çš„æ€§èƒ½çš„åŒæ—¶å®ƒèƒ½å¤Ÿé™ä½å¤§éƒ¨åˆ†è®­ç»ƒåœºæ™¯è¿™ç§é€šä¿¡å’Œè®¡ç®—çš„è¿™æ ·ä¸€ä¸ªæ¯”ä¾‹ã€‚å¯¹æ‰©å±•æ€§æ˜¯æœ‰å¸®åŠ©çš„ã€‚ä½†æ˜¯å®ƒä¹Ÿå—åˆ°äº†æˆ‘ä»¬è¿™ä¸ªæ¯”å¦‚è¯´å‘ƒGPUè¿™ä¸ªæ˜¾å­˜çš„ä¸€ä¸ªçº¦æŸã€‚é‚£ä¹Ÿå¯èƒ½ä¼šå—åˆ°åœ¨ç®—æ³•å±‚é¢å¯¹è¿™ä¸ªgæ”¶æ•›ä¸Šçš„ä¸€äº›çº¦æŸã€‚

æ‰€ä»¥å®ƒä¹Ÿæ˜¯ä¸€ä¸ªç®—å·¥ç¨‹çš„ä¸€ä¸ªååŒå•Šï¼Œé‚£ä¹ˆæˆ‘è§‰å¾—æ€»ç»“èµ·æ¥ï¼Œåœ¨å‘ƒæ•´ä¸ªåœ¨è®¡ç®—æœºä½“ç»“æ„çš„è§’åº¦å»çœ‹è¿™ä¸ªäº‹æƒ…å‘¢ï¼Œä»è®¡ç®—å•Šé€šä¿¡å•Šï¼ŒåŒ…æ‹¬å­˜å‚¨è¿™ä¸‰ä¸ªç»´åº¦éƒ½æœ‰å¾ˆé«˜çš„è¦æ±‚ã€‚å¦å¤–ä¸€ä¸ªåœ¨è¿™ç§å¤§è§„æ¨¡çš„é›†è®­é‡Œé¢ã€‚

å¯¹äºè¿™ç§å•Šä»1åˆ°1024è¿™ç§æ‰©å±•çš„æ—¶å€™ï¼Œä½ çš„çº¿æ€§åŠ é€Ÿæ¯”æ€ä¹ˆæ ·å•Šï¼ŒåŒ…æ‹¬è¿™ä¸ªå¤§è§„çš„é›†ç¾¤ç¨³å®šæ€§æ€ä¹ˆæ ·ï¼Ÿåˆšæ‰èµ„æºçš„è¿™ä¸ªåŒæ—¶ä¹Ÿåˆ†äº«äº†å¯¹å§ï¼Ÿå°±å¤§è§„æ¨¡é›†è®­ï¼Œå…¶å®è¿™äº›éƒ½æ˜¯å·¥ç¨‹ä¸Šè¦è§£å†³çš„é—®é¢˜ã€‚ğŸ˜Šï¼Œå‘ƒã€‚

é‚£å‰é¢æˆ‘ä»¬æ˜¯ä»è¿™ä¸ªæŠ€æœ¯çš„è§’åº¦å»çœ‹ä¸€ä¸‹è§£å†³çš„é—®é¢˜ã€‚é‚£è¿™é‡Œæˆ‘ä»¬å°è¯•ä»ç”¨æˆ·çš„è§†è§’æ¥çœ‹å•Šï¼Œé‚£ä¹ˆè¦è§£å†³å“ªäº›é—®é¢˜ã€‚æˆ‘æ€»ç»“ä¸‹äº†5ä¸ªå“ˆï¼Œå‘ƒï¼Œè¿™ä¸ªç¬¬ä¸€ä¸ªæ˜¯capacityï¼Œå°±æ˜¯è¿™ä¸ªå¤§æ¨¡å‹ï¼Œä½ é¦–å…ˆè¦å­˜å¾—ä¸‹å¯¹å§ï¼Ÿ

ä½ èƒ½å¤Ÿè·‘å…ˆè§£å†³è¿™ä¸ªåŸºç¡€çš„è¿™ä¸ªæ¸©é¥±éœ€æ±‚å•Šï¼Œé‚£ç¬¬äºŒä¸ªè¯´è¿™ä¸ªcalabilityå°±è¯´å½“ä½ èƒ½è·‘çš„ä¹‹åï¼Œä½ è¦ä¸ºäº†å»æå‡æ€§èƒ½ï¼Œä½ æ‰©å±•æ›´å¤šçš„èŠ‚ç‚¹ï¼Œé‚£ä¹ˆä½ è¿™ä¸ªç³»ç»Ÿçš„ä¸€ä¸ªåŠ é€Ÿæ¯”æ€ä¹ˆæ ·ã€‚

ä½ çš„è¿™ä¸ªæ¡†æ¶æ˜¯ä¸æ˜¯èƒ½å¤Ÿæ¯”è¾ƒå¥½çš„è¿™ç§æä¾›è¿™ç§æ‰©å±•çš„èƒ½åŠ›ã€‚é‚£ç¬¬ä¸‰ä¸ªæ˜¯è¯´usabilityä¸€ä½¿ç”¨ï¼Œå› ä¸ºå…¶å®ç°åœ¨è¿™ä¸ªå¤§æ¨¡å‹çš„ç”Ÿæ€åŒ…æ‹¬æœ‰å¾ˆå¤šæ¨¡å‹ï¼Œå¾ˆå¤šæ¡†æ¶å¯¹å§ï¼Ÿé‚£ä½ ä½ è¿™ä¸ªä»ç”¨æˆ·çš„è§†è§’ï¼Œä½ è¿™ä¸ªç­–ç•¥ï¼Œä½ çš„ç®—æ³•å•Šã€‚

è¿™ä¸ªç”Ÿæ€æ˜¯ä¸æ˜¯è¯´æ˜“äºè¢«ç”¨æˆ·æ¥å—ã€‚é‚£æœ€åä¸¤ä¸ªå…¶å®æ˜¯æœ€å…³é”®çš„å†™åœ¨åé¢ä¸æ˜¯åº”è¯¥ä¸é‡è¦å•Šï¼Œå…¶å®éå¸¸å…³é”®ã€‚é‚£å°±è¯´ç”¨æˆ·éƒ½å¸Œæœ›è·å¾—æˆ‘çš„é€Ÿåº¦å¤Ÿå¿«å•Šï¼Œæˆ‘çš„æˆæœ¬å¤Ÿä½ã€‚

è¿™æ˜¯çœŸæ­£å¤§æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­èƒ½å¤Ÿè½åœ°çš„å¾ˆé‡è¦çš„å› ç´ é‚£æˆ‘ä»¬ä»Šå¤©ã€‚ğŸ˜Šï¼Œå¯èƒ½ä¼šé‡ç‚¹çœ‹å‰é¢ä¸¤ä¸ªå°±å­˜å¾—ä¸‹å’Œè¿™ä¸ªæ˜“æ‰©å±•ã€‚å› ä¸ºå‰é¢ä¸¤ä¸ªé—®é¢˜è§£å†³å®Œä¹‹åå§ï¼Œå…¶å®åé¢çš„é—®é¢˜å°±ç›¸å¯¹çš„è¿åˆƒè€Œè§£äº†ã€‚

é‚£è¿™é‡Œæˆ‘ä»¬çœ‹å¤§æ¨¡å‹çš„è¿™äº›å¸¸è§çš„ä¸€äº›è®­ç»ƒç­–ç•¥å‘¢å‘ƒæ¯”è¾ƒå¸¸è§è¿™ä¸€ä¸ªè¿™ä¸ªtensorå¹¶è¡Œã€‚å®ƒçš„å¥½å¤„å°±æ¥è¯´è¿™ä¸ªåƒtransçš„æ¨¡å‹å°±æ˜¯éå¸¸ç¬¦åˆè¿™ç§ç‰¹ç‚¹ã€‚

ä½†æ˜¯å®ƒå‘¢ä¼šå¼•å…¥ä¸€äº›æ¯”è¾ƒå¤§çš„ä¸€äº›é€šä¿¡çš„å¼€é”€åœ¨é‡Œé¢ç¬¬äºŒä¸ªæ˜¯è¿™ä¸ªæµæ°´çº¿å¹¶è¡Œè¿™ä¸ªä¹Ÿæ˜¯æ¯”è¾ƒå¥½çš„èµ·åˆ°è¿™ä¸ªäººå¤šåŠ›é‡å¤§å˜›ï¼Œå¯¹å§ï¼Ÿä¸€äººå¹²ä¸€éƒ¨åˆ†å•Šï¼Œæ‰€ä»¥å¯¹è¿™ä¸ªå‚æ•°çš„è§„æ¨¡å®é™…ä¸Šæ˜¯æ¯”è¾ƒå¥½çš„æ‰©å±•èƒ½åŠ›ã€‚

ä½†æ˜¯å½“ç„¶æˆ‘ä»¬ä»è¿™ä¸ªå›¾å¯ä»¥çœ‹åˆ°å“ˆï¼Œå°±æ˜¯äº›ç°è‰²çš„éƒ¨åˆ†å®é™…éƒ½æ˜¯ç©ºæ´ã€‚é‚£ä¹ˆè¿™ç©ºæ´å…¶å°±æ„å‘³ç€ä½ çš„è®¡ç®—æ•ˆç‡æ˜¯å˜ä½çš„å•Šï¼Œæ‰€ä»¥æ‰€ä»¥å®ƒæœ‰æœ‰æœ‰å¥½å¤„ä¹Ÿæœ‰å¾ˆå¤§çš„ä¸€äº›å¼Šç«¯å“ˆã€‚

é‚£ä¹ˆå‘ƒæ¯”è¾ƒå¸¸è§çš„æˆ‘ä»¬å°†è¿™ä¸ªtensorå¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œç»“åˆèµ·æ¥ï¼Œæ˜¯å¯ä»¥æ¯”è¾ƒå¥½çš„æå‡è¿™ä¸ªå‚æ•°çš„è§„æ¨¡çš„é‚£é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åƒzeroè¿™ä¸ªå‘ƒè¿™ä¸ªå¾®è½¯çš„è¿™ä¸ªzeroåœ¨è¿™ç§æ•°æ®å¹¶è¡Œçš„ç»´åº¦å•Šã€‚

å»æ‰©å±•äº†è¿™ç§å‚æ•°è§„æ¨¡åˆ‡åˆ†çš„èƒ½åŠ›ã€‚æ‰€ä»¥å‘¢å®ƒéå¸¸å¥½ç”¨ï¼Œå¯¹å§ï¼Ÿå› ä¸ºå®ƒæ­£å¸¸å°±æ˜¯ä¸€ä¸ªæ•°æ®å¹¶è¡Œä¸æŒ‘æ¨¡å‹å•Šä½†ã€‚ğŸ˜Šï¼Œä»–çš„é€šä¿¡å¼€é”€å‘¢å…¶å®ä¹Ÿè¿˜æ˜¯æ¯”è¾ƒå¤§çš„å“ˆã€‚ğŸ˜Šï¼Œé‚£ä¸ºäº†åœ¨è¿™ç§æ›´å°‘çš„èµ„æºï¼Œç”šè‡³åœ¨å•å¡å±‚é¢èƒ½å¤Ÿå»è·‘å¤§æ¨¡å‹ã€‚

è®©å¤§æ¨¡å‹å˜æˆä¸€ä¸ªè¿™ä¸ªè¿™ä¸ªæ™®é€šè€ç™¾å§“ï¼Œå¯¹å§ï¼Ÿä¹Ÿèƒ½å¤Ÿå»å°è¯•çš„è¿™æ ·ä¸€ä¸ªäº‹æƒ…ã€‚é‚£å¾®è½¯åˆæ¨å‡ºäº†è¿™ä¸ªzeroçš„offloadå•Šï¼Œç”šè‡³æ˜¯è¿™æ ·çš„æŠ€æœ¯ã€‚é‚£ä¹ˆè¿™ç§æŠ€æœ¯å®ƒåˆ©ç”¨è¿™ç§è¿™ä¸ªGPUçš„è¿™ä¸ªæ˜¾å­˜ã€‚

åŒ…æ‹¬CPUçš„å†…å­˜ä¸­é—´çš„é‚£æœ€åä»¥åŠè¿™ä¸ªé«˜æ€§èƒ½çš„è¿™ä¸ªå¤§å®¹é‡çš„è¿™ç§å­˜å‚¨ä»‹è´¨ã€‚é€šè¿‡è¿™æ ·ä¸€ç§ä¸‰çº§çš„å¼‚æ„çš„ä¸€ä¸ªæ··åˆå­˜å‚¨æ¶æ„å»è§£å†³è¿™ä¸ªå‚æ•°è§„æ¨¡çš„ä¸€ä¸ªé—®é¢˜å•Šé‚£è¿™ä¸ªå·¥ä½œå®é™…ä¸Šå› ä¸ºæˆ‘ä¹‹å‰åœ¨ç™¾åº¦19å¹´çš„æ—¶å€™ã€‚

æˆ‘ä»¬åœ¨æˆ‘ä»¬å¹¿å‘Šè¿™ä¸ªè¿™ä¸ªæ ¸å¿ƒçš„è¿™ç§ä¸šåŠ¡åœºæ™¯ï¼Œåœ¨è¿™ç§å¸æ”¶å¤§æ¨¡å‹é‡Œé¢å·²ç»æŠŠè¿™ç§æŠ€æœ¯åšèµ·æ¥äº†ã€‚é‚£è¿™ä¸ªå·¥ä½œä¹Ÿå¼•ç”¨äº†æˆ‘ä»¬ç›¸å…³çš„ä¸€ä¸ªè®ºæ–‡ã€‚ğŸ˜Šï¼Œå‘ƒï¼Œé‚£å‰é¢è®²çš„è¿™äº›æŠ€æœ¯å‘¢ï¼Œå®é™…ä¸Šæ˜¯å¯ä»¥ç›¸äº’ç»„åˆçš„å“ˆã€‚

é‚£æ¯”è¾ƒå¸¸è§çš„æ˜¯è¿™ä¸ª3Då¹¶è¡Œã€‚å‘ƒï¼Œå°±å°†æˆ‘ä»¬å‰é¢è®²çš„tensorå¹¶è¡Œæµæ°´çº¿å¹¶å’Œzero dPé‚£ä¸ªä¸‰è€…è¿›è¡Œç»“åˆèµ·æ¥ã€‚å‘ƒï¼Œå¯ä»¥å®ç°ä¸€ä¸ªè§„æ¨¡è¿™ä¸ªå‚æ•°çš„è§„æ¨¡å’Œæ€§èƒ½çš„ä¸€ä¸ªä¸€ä¸ªå¥½çš„æ‰©å±•æ€§ã€‚å‘ƒã€‚

ä½†æ˜¯å‘¢è¿™é‡Œé¢å…¶å®è¿˜æ˜¯æœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´çš„ã€‚æˆ‘ä»¬å¯ä»¥æƒ³è±¡å•Šï¼Œå½“ä½ è¿™ä¸ªå¢åŠ æœºå™¨çš„æ•°é‡çš„æ—¶å€™ï¼Œä½ çš„æ‰€æœ‰çš„æ˜¾å­˜åŠ èµ·æ¥ï¼Œå¦‚æœå·²ç»è¶…è¿‡äº†å•Šä½ æ¨¡å‹çš„é€‰å­˜çš„ä¸€ä¸ªéœ€æ±‚ã€‚è¿™ä¸ªæ—¶å€™å¦‚æœä½ ä¸ºäº†æ›´è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

ä½ ä¼šå¢åŠ æ›´å¤šçš„æœºå™¨ã€‚ä½†æ˜¯è¿˜æ˜¯ä¼šè¿›è¡Œå‚æ•°çš„ä¸€ä¸ªåˆ‡åˆ†ã€‚é‚£ä¹ˆè¿™ç§åˆ‡åˆ†å®é™…ä¸Šæ˜¯ä¸å¿…è¦çš„ã€‚å› ä¸ºå®ƒä¼šå¸¦æ¥é¢å¤–çš„å¼€é”€ï¼Œå®ƒä¼šé™ä½ç³»ç»Ÿçš„ä¸€ä¸ªæ€§èƒ½çš„ä¸€ä¸ªå¯æ‰©å±•æ€§å•Šï¼Œæ‰€ä»¥æ‰€ä»¥è¿™é‡Œé¢ã€‚ğŸ˜Šï¼Œé‚£å‘ƒç™¾åº¦é£è®²å•Šã€‚

é‚£ä¸ªæ˜¯é’ˆå¯¹è¿™ç§åœºæ™¯åˆæå‡ºäº†è¿™ä¸ª4Dæ··åˆå¹¶è¡Œçš„ç­–ç•¥å•Šï¼Œå®ƒå®é™…ä¸Šåœ¨ä¸‹å®šè¿™ä¸ªç»´åº¦å‘¢æ˜¯è·Ÿ0DPæ˜¯æœ‰ç±»ä¼¼çš„åŠŸèƒ½ã€‚ä½†æ˜¯å‘¢å®ƒå’Œå®ƒçš„æ•°æ®å¹¶è¡Œæ˜¯å¯ä»¥åˆ†å¼€é…ç½®çš„ã€‚

æ‰€ä»¥ä»–å¯ä»¥åšåˆ°å°±æ˜¯è¯´å‚æ•°çš„è¿™ç§å•Šè§„æ¨¡çš„æ‰©å±•å’Œæ€§èƒ½çš„æ‰©å±•è¿™ç§è§£è€¦å•Šï¼Œä»è¿™ä¸ªå³è¾¹å³ä¸‹è¿™ä¸ªå›¾ï¼Œæˆ‘ä»¬çœ‹åˆ°å“ˆè¿™ä¸ª512å¼ å¡è¿™æ ·ä¸€ä¸ªä¾‹å­å‘ƒã€‚

4Dæ··åˆå¹¶è¡Œçš„ä¸€ä¸ªç­–ç•¥å‘¢ç›¸æ¯”äºå‰é¢ä¸¤ç§3Dçš„ç­–ç•¥å¤§æ¦‚æœ‰10åˆ°20%çš„ä¸€ä¸ªæ€§èƒ½çš„æå‡ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹è§å•Šï¼Œå³ä½¿åœ¨åŒæ ·çš„èµ„æºçš„æƒ…å†µä¸‹å•Šï¼Œä½ é€šè¿‡æ•´ä¸ªå¤§æ‹‡çš„ç­–ç•¥å…¶å®ä¹Ÿæ˜¯èƒ½å¤Ÿæå‡å¾ˆå¥½çš„æ€§èƒ½çš„ã€‚ğŸ˜Šã€‚

é‚£æˆ‘ä»¬å°†å‰é¢çš„è¿™ä¸ªåˆšæ‰è®²çš„è¿™ä¸ªç­–ç•¥ï¼Œæˆ‘ä»¬æŠŠå®ƒå¯¹æ¯”èµ·æ¥å•Šï¼Œä»è¿™ä¸ªé€šä¿¡çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬å¯¹è¿™ä¸ªç¡¬ä»¶ä¸Šè¿™ä¸ªé€šé€šä¿¡topå•Šå¸¦å®½çš„è¦æ±‚ï¼Œä»¥åŠè¿™ä¸ªç­–ç•¥å¯¹é™ä½æ˜¾å­˜çš„å’Œæ€§èƒ½æå‡çš„ä¸€ä¸ªå½±å“ï¼ŒåŒ…æ‹¬å®ƒè¿™ä¸ªæ–¹æ¡ˆæ˜¯ä¸æ˜¯å¥½ç”¨ã€‚

é‚£æˆ‘ä»¬çœ‹çœ‹å®ƒé€‚ç”¨å“ªäº›åœºæ™¯ã€‚é‚£æˆ‘ä»¬é‡ç‚¹çœ‹ä¸€ä¸‹æµçº¿å¹¶è¡Œå’Œè¿™orå¹¶è¡Œã€‚æµçº¿å‰é¢è®²çš„é€šä¿¡è¦æ±‚æ¯”è¾ƒä½å•Šï¼Œæ‰€ä»¥å‘¢ä½†æ˜¯å®ƒçš„è®¡ç®—æ•ˆç‡ä¸æ˜¯å¾ˆå¥½å•Šï¼Œæ‰€ä»¥è¿™é€šä¿¡é¢è¿™ä¸ªè®¡ç®—ç‚¹ä¸ªæœ‰æœ‰æœ‰åˆ©å•Šã€‚

ä½†è¿™ç§æˆ‘ä¸€èˆ¬æ˜¯åº”ç”¨åœ¨è¿™ä¸ªå‘ƒå°±æ˜¯é“¾è·¯å¸¦å®½ç›¸å¯¹ä½çš„åœºæ™¯ï¼Œä¸€èˆ¬æ˜¯å¤šæœºä¹‹é—´å»åšsorå¹¶è¡Œå®ƒçš„ç‰¹ç‚¹å°±æ˜¯ç›¸å¯¹æ¥è¯´å•Šé€šä¿¡é‡æ˜¯æ¯”è¾ƒå¤§çš„å•Šï¼Œæ‰€ä»¥å®ƒå¯¹ä¸€èˆ¬åœ¨å•æœºå†…éƒ¨è¿™ç§é«˜é€Ÿäº’è”çš„åœºæ™¯å»ç”¨ã€‚é‚£ä¹ˆæŠŠè¿™äº›ç­–ç•¥ç”¨å®Œä¹‹åã€‚

æˆ‘ä»¬å†å»é›†åˆçš„æ—¶å€™ï¼ŒåŸºæœ¬ä¸Šæ˜¯åœ¨å¤šæœºè¡Œåšå•Šï¼Œé‚£é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åƒè¿™ä¸ªoffloadå•Šcompè¿™æŠ€æœ¯å‘¢ï¼Œå…¶å®å¯¹è¿™ç§é™ä½è¿™ä¸ªå°±æ˜¯è¯´è¿™ä¸ªæ˜¾å­˜çš„ä¸€ä¸ªéœ€æ±‚çš„æ•ˆæœæ˜¯éå¸¸éå¸¸å¥½çš„ã€‚ä½†æ˜¯å¯¹æ€§èƒ½å½±å“ä¹Ÿéå¸¸å¤§ã€‚æ‰€ä»¥è¯´åœ¨ä¸€ä¸ªã€‚

ğŸ˜Šï¼Œå®çš„ä¸šåŠ¡åœºæ™¯é‡Œé¢ï¼Œæˆ‘ä»¬ç»™å®šä¸€ä¸ªæ¨¡å‹çš„è§„æ¨¡ï¼Œç„¶åç»™å®šä¸€ä¸ªèµ„æºã€‚é‚£ä¹ˆæ€ä¹ˆå°†è¿™äº›ç­–ç•¥èƒ½å¤Ÿç»„åˆèµ·æ¥ï¼Œè®©ä»–è·‘çš„å‘ƒè¿›é¦–å…ˆèƒ½è·‘èµ·æ¥ã€‚ç¬¬äºŒä¸ªæ€ä¹ˆè·‘å¾—æ›´å¿«ï¼Œè¿™å®é™…ä¸Šè¿˜æ˜¯æœ‰å¾ˆå¤šè¿™ç§ç­–ç•¥åœ¨é‡Œé¢ã€‚

æ‰€ä»¥å…¶å®åˆšæ‰å¿—æ´çš„åŒäº‹ä¹Ÿåˆ†äº«åˆ°ï¼Œä»¥åå†æ€ä¹ˆå»åšè¿™ç§è‡ªåŠ¨çš„å·¡ä¼˜ï¼Œå¯¹å§å•Šï¼ŸğŸ˜Šï¼Œå¥½ï¼Œé‚£æˆ‘ä»¬å‰é¢åˆ†äº«è¿™ä¸ªæ•´ä¸ªGBDå¤§é—¨è®­ç»ƒçš„ä¸€äº›æŒ‘æˆ˜å‘¢ã€‚æˆ‘ä»¬å†çœ‹ä¸‹åŸºäºå•ŠBåˆƒçš„å›½äº§GPUï¼Œæˆ‘ä»¬æ€ä¹ˆå»è§£å†³è¿™äº›é—®é¢˜ã€‚ğŸ˜Šï¼Œå‘ƒã€‚

æ‰€ä»¥æ•´ä½“æˆ‘ä»¬æ„Ÿè§‰æ˜¯ä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç«¯å¤šç«¯çš„å»æ‰“é€ ä¸€ä¸ªæ•´ä½“çš„ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚é‚£ä¹ˆä»æœ€åº•å±‚è¿™ç§é«˜æ€§èƒ½è¿™ç§åŸºç¡€è®¾æ–½å•Šï¼ŒåŸºäºé¿çš„é«˜æ€§èƒ½çš„GPUå»è§£å†³ç®—åŠ›çš„é—®é¢˜ã€‚åŒæ—¶æˆ‘ä»¬åŒ¹é…ç›¸å¯¹åº”çš„é«˜èƒ½çš„å­˜å‚¨äº’è”çš„æŠ€æœ¯ã€‚

é‚£åœ¨æ­¤ä¹‹åŸºç¡€è®¾ç½®æ„å»ºä¸€å¥—è¿™ç§è¿™ç§å¹³å°åˆšæ‰åƒä¹é¼çš„è¿™ä¸ªç®—å¹³å°å°±å±äºè¿™ä¸€ç±»æˆ‘ä»¬æ€ä¹ˆå»ç®¡ç†è¿™ä¹ˆå¤šçš„èµ„æºå‘ç”¨æˆ·æä¸€ä¸ªåº”ç”¨çš„å¹³å°ï¼ŒååŒæ—¶ä¿è¯è¿™ä¸ªè¿‡ç¨‹ä¸­è¿™ä¸ªä»»åŠ¡æ˜¯ç¨³å®šå¯é çš„èƒ½å¤Ÿå¼¹æ€§ä¼¸ç¼©ã€‚

å‘ä¸Šè¿˜æœ‰åŠ é€Ÿåº“å¤§æ¨¡å‹åœºæ™¯å¾ˆå¤§çš„ç‰¹ç‚¹å°±æ˜¯å¯¹é€šä¿¡çš„ä¼šéå¸¸é«˜æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ä¹Ÿæ„å»ºè¿™ç§åƒè¿™ä¸ªæˆ‘å«çš„è¿™ç§ç§ç»„åˆèƒ½çš„é€šè®¯åº“å•Šé‚£å¤§æ¨¡å‹åœºæ™¯è¿˜æœ‰å¾ˆä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯è¯´éœ€è¦è¶…å¤§è§„æ¨¡çš„è®­ç»ƒæ¶è¿™å¯èƒ½æ˜¯åŸºäºæ™®é€šæ›´çš„å†³æ–¹æ¡ˆé‚£è®²é€‚é…äº†æ‰€ä»¥å†ä¸Šå»æ‰¿æ¥è¿™ä¸ªå‹çš„åº”ç”¨ç›®å‰ã€‚

ğŸ˜Šï¼Œå•Šå•Šï¼Œæˆ‘åœ¨ç¼–ç€çš„GPä¸Šï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸€ä¸‹GPTç³»åˆ—çš„è¿™ç§å¸¸è§çš„å¤§éƒ¨åˆ†çš„è®­ç»ƒå‘¢ï¼Œæˆ‘ä»¬éƒ½æ˜¯å¯ä»¥åšéƒ½å¯ä»¥æ”¯æŒçš„ã€‚å‘ƒï¼Œé‚£å‰é¢æˆ‘ä»¬è®²çš„ç¬¬ä¸€å±‚æ˜¯è¿™ä¸ªç®—ä¾‹å•Šï¼Œå‘ƒï¼Œæ‰€ä»¥è¿™é‡Œé¢æˆ‘ä»¬ä»¥Bçš„BR104è¿™ä¸ªPè¿™ä¸ªäº§å“ä¸ºä¾‹å•Šã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°çš„è¯å‘ƒï¼Œä»è¿™ä¸ªLP32è¿™ä¸ªTTF32åŒ…B16å•Šè¿™äº›å¸¸è§çš„è¿™ç§ç®—åŠ›è¿™ä¸ªæ€§èƒ½æ˜¯éå¸¸é«˜çš„å•ŠåŒæ—¶ä¹Ÿæœ‰è¿™ä¸ªç›¸å…³çš„è¿™ç§ä¸“ç”¨çš„è¿™ç§äº’è”çš„èƒ½åŠ›ã€‚é‚£è¿™æ˜¯å•å¡çš„ä¸€ä¸ªæ€§èƒ½ã€‚

å¯¹æˆ‘ä»¬å¤§æ¨¡å‹éœ€è¦ä»1åˆ°100åˆ°1åˆ°1000çš„å§è¿™ä¸ªåå±•ã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹åœ¨è¿™ä¸ªå‘ƒç³»ç»Ÿå±‚é¢æ¯”å¦‚è¯´å•æœº8å¡ä¸ºä¾‹çš„è¯ï¼Œæ€ä¹ˆå»æå‡è¿™ä¸ªå•æœºçš„æ€§èƒ½ã€‚é‚£è¿™é‡Œæˆ‘ä»¬ä¸¾äº†ä¸¤ä¸ªä¾‹å­ã€‚

è¿™ç§æ‹“æ‰‘å“ˆç¬¬ä¸€ä¸ªè¿™ç§PCçš„è¿™ç§æ ‡å‡†å½¢æ€çš„è¿™ç§åŠ é€Ÿå¡ã€‚é‚£è¿™ç§å¡ä¸€èˆ¬æ¥è¯´æ˜¯æ²¡æœ‰è¿™ç§ä¸“ç”¨äº’è”çš„èƒ½åŠ›çš„é‚£åœ¨è¿™ä¹‹ä¸Šæˆ‘ä»¬æ˜¯åšè¿™ä¸ªblinkçš„æŠ€æœ¯ä¸ªæ¡¥èŠ‚å¡æ˜¯å¯ä»¥æä¾›é¢å¤–çš„è¿™ç§é«˜é€Ÿäº’è”èƒ½åŠ›ã€‚

æ‰€ä»¥è¿™å…¶å®å¯¹å¤§ç›®ç§¯åœºæ™¯æ˜¯éå¸¸æœ‰æœ‰åˆ©çš„å•Šé‚£ç¬¬äºŒä¸ªæ˜¯è¿™ç§OMå½¢æ€çš„è¿™ä¸ªåŠ é€Ÿå¡ã€‚å› ä¸ºè¿™ç§å¡å‘¢å®ƒç›¸å¯¹æ¥è¯´å®ƒçš„åŠŸè€—æ›´é«˜ä¸€äº›ï¼Œå¯ä»¥å»å‘ƒæ”¯æŒæ›´å¤šçš„ä¸€äº›äº’è”çš„ç«¯å£ã€‚ğŸ˜Šï¼Œå•Šï¼Œæ‰€ä»¥æ•´ä½“åœ¨ç³»ç»Ÿæ„å»ºä¸Šé¢ã€‚

å®ƒå¯ä»¥æä¾›ä¸€ä¸ªå‘ƒæ›´å¥½çš„ä¸€ä¸ªäº’å«èƒ½åŠ›ã€‚é‚£è¿™é‡Œæ¯”å¦‚è¯´ä¸¾ä¸ªä¾‹å­ï¼Œåƒå¯ä»¥æ„å»ºè¿™ç§ç¯å½¢çš„topæ‰‘å•Šï¼Œç”šè‡³æ˜¯è¿™ä¸ªå…¨äº’è”çš„topæ‰‘å•Šï¼Œæå‡è¿™ä¸ªå•æœºå†…éƒ¨çš„ä¸€ä¸ªé€šä¿¡å¸¦å®½ã€‚ğŸ˜Šï¼Œé‚£æœ‰ä»å•å¡åˆ°å•æœºï¼Œå¯¹å§ï¼Ÿè¿˜ä¸å¤Ÿã€‚

æˆ‘ä»¬è¦æ€ä¹ˆå»æ‰©å±•ä¸€ä¸ªé›†ç¾¤ã€‚é‚£è¿™é‡Œæ€ä¹ˆå»æ‰“é€ è¿™åƒå¼ å¡è¿™æ ·çš„ä¸€ä¸ªä¸€ä¸ªå¤§åŠŸæ¨¡çš„é›†ç¾¤ã€‚æ‰€ä»¥æ•´ä½“è¿™ä¸ªé›†ç¾¤çš„è®¾è®¡ï¼Œæˆ‘è§‰å¾—æœ‰å‡ ä¸ªç†å¿µå˜›ã€‚

ç¬¬ä¸€ä¸ªæ˜¯è¯´å•Šæ˜¯è¿™ç§äºŒå±‚çš„ä¸€ä¸ªcloçš„ç½‘ç»œèƒ½å¤Ÿå»ä¿è¯è¿™ä¸ªé›†ç¾¤æ˜¯è¿™ä¸ªç½‘ç»œæ˜¯æ²¡æœ‰æ”¶æ²¡æœ‰æ²¡æœ‰æ”¶æ•›çš„å“ˆå¯¹ï¼Œç„¶åç¬¬äºŒä¸ªå°±æ˜¯è¯´åœ¨å¯ä»¥æœ‰å¾ˆå¥½çš„å¯æ‰©å±•æ€§ã€‚å•Šç¬¬äºŒä¸ªç‚¹ç¬¬ä¸‰ä¸ªç‚¹å°±æ˜¯è¯´æˆ‘ä»¬è¿™ä¸ªç½‘ç»œè®¾è®¡è¦è¦æœ‰ä¸€å®šçš„è¿™ç§å¯é æ€§ã€‚

æ‰€ä»¥åŸºæœ¬ä¸Šæ¯ä¸ªå‘ƒè¿™ä¸ªè¿™ä¸ªæœºå™¨æœ‰å¤šä¸ªç½‘å¡ï¼Œä»æ•°æ®é¢ä¸Šæ˜¯å¯ä»¥æ›´å¯é ã€‚é‚£åŒæ—¶å‘¢å®ƒä¹Ÿå¯ä»¥æå‡å¤šæœºçš„è¿™æ ·ä¸€ä¸ªå‘ƒé€šä¿¡çš„ä¸€ä¸ªå¸¦å®½ã€‚é‚£è¿™æ˜¯ä»ç¡¬ä»¶å±‚é¢å»çœ‹å“ˆã€‚ä½†æ˜¯å®é™…è¿™ä¸ªç³»ç»Ÿé‡Œé¢ä¸æ˜¯è¯´å…‰æœ‰è¿™æ ·ç¡¬ä»¶å°±å¯ä»¥äº†ã€‚

æˆ‘ä»¬åœ¨å¤§æ¨¡éŸ³åšç­–ç•¥çš„æ—¶å€™ï¼ŒåŒ…æ‹¬æˆ‘ä»¬é€šè®¯åº“åœ¨é€šä¿¡çš„æ—¶å€™ï¼Œä½ æ€ä¹ˆå»å‘ƒåˆ©ç”¨è¿™ç§ä¸åŒçš„ç½‘å¡çš„è¿™ç§å¹¶è¡ŒåŠ é€Ÿèƒ½åŠ›ï¼Œæ€ä¹ˆå»åœ¨é›†ç¾¤é‡Œé¢é¿å…è¿™ç§é“¾è·¯çš„å†²çªå•Šï¼Œå‡å°‘è¿™ä¸ªç½‘ç»œçš„ä¸€ä¸ªå½±å“ã€‚

å…¶å®è·Ÿè°ƒåº¦å•Šè·Ÿè¿™ä¸ªé€šè®¯åº“å…¶å®éƒ½æ˜¯éå¸¸ç´§å¯†ç›¸å…³çš„å•Šã€‚ğŸ˜Šï¼Œå•Šé‚£åœ¨è¿™æ ·ä¸€ä¸ªåŸºå­¦ä¹ å¹³å°çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç®¡ç†æ•°åƒå¼ è¿™æ ·çš„GPUå¡ã€‚å¦‚æœä¸€ä¸ªé›†ç¾¤é‡Œé¢æœ‰ä¸åŒçš„ä»»åŠ¡ã€‚é‚£è¿™ä¸ªå®ä¸Šä¼šæ›´å¤æ‚ã€‚

å…¶å®åˆšæ‰è¿™ä¸ª9é¼çš„è¿™ä¸ªåŒäº‹ä¹Ÿåˆ†äº«è¿‡ï¼Œå¯¹å§ï¼Ÿå½“ä½ å¹³å°çš„ä»»åŠ¡ç§ç±»å¾ˆå¤šçš„æ—¶å€™ï¼Œä½ æ€ä¹ˆä¿è¯èµ„æºçš„ä¸€ä¸ªæ•ˆç‡ï¼ŒåŒæ—¶ä¿è¯å¤§æ¨¡å‹ä»»åŠ¡çš„ä¸€ä¸ªä¸€ä¸ªç¨³å®šæ€§æ‰€ä»¥ä»æ•ˆç‡æˆ‘ä»¬å¸Œæœ›è¯´å¤§æ¨¡å‹çš„ä»»åŠ¡èƒ½å¤Ÿå°½é‡çš„èšç„¦åœ¨è¿™ä¸ªåŒæ ·çš„äº¤æ¢æœºä¸‹é¢å»æå‡è¿™ä¸ªé€šä¿¡çš„æ•ˆç‡ã€‚

ä½†å¦‚æœé›†ç¾¤çš„ä»»åŠ¡å¾ˆå¤šçš„æ—¶å€™ï¼Œå°ä»»åŠ¡æ··æ‚åœ¨é‡Œé¢ï¼Œè¿™ä¸ªç›®æ ‡å°±è¾¾ä¸åˆ°ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ„å»ºè¿™ç§ä»»åŠ¡çš„è¿™ç§è¿ç§»çš„èƒ½åŠ›å•Šï¼Œå°†å°ä¸€äº›è¿™ä¸ªèµ„æºç¢ç‰‡å•Šå»åšä¸€ä¸ªæ•´ç†è®©è¿™ä¸ªç©ºé—²çš„èµ„æºå°½é‡é›†ä¸­åœ¨è¿™ä¸ªäº¤æ¢æœºä¸€ä¸ªäº¤æ¢æœºä¸‹é¢ã€‚

æˆ‘ä»¬å»ä¾¿äºè°ƒåº¦å¤§æ¨¡å‹çš„ä»»åŠ¡ã€‚é‚£åŒæ—¶æˆ‘è§‰å¾—å¤§æ¨¡å‹çš„ä»»åŠ¡å…¶å®OPTä¸ºä¾‹å“ˆï¼Œå°±æ˜¯ä»–ä»¬å†…è®²æ•´ä¸ªä¸€ä¸ªè®­ç»ƒçš„è¿‡ç¨‹ä¸­æœ‰35æ¬¡æ‰‹åŠ¨é‡å¯100å¤šæ¬¡æ•…éšœå¯¹ï¼Ÿè¯´æ•…éšœåŠ›å¾ˆé«˜ã€‚é‚£å¤§æ¨¡å‹ä»»åŠ¡å°±éå¸¸å®¹æ˜“æŒ‚æ‰äº†ã€‚

æ‰€ä»¥è¿™ä¸ªä½ æ€ä¹ˆè¯´åœ¨ä»»åŠ¡å‡ºç°é—®é¢˜çš„æƒ…å†µä¸‹æˆ‘æ€ä¹ˆå»è§£å†³å®¹é”™çš„é—®ã€‚ğŸ˜Šï¼Œé¢˜ã€‚å¦‚æœæˆ‘èƒ½å¤Ÿå»è¿›ä¸€æ­¥çš„è¿™ç§è‡ªåŠ¨ä¼¸ç¼©ï¼Œé‚£ä¹ˆä»èµ„æºæ•ˆç‡å°±èƒ½å¤Ÿæä¾›å¾ˆå¥½çš„ä¸€ä¸ªä¿éšœå•Šï¼Œæ‰€ä»¥è¿™äº›å…¶å®åœ¨çœŸæ­£çš„ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåœ¨å¤§è§„æ¨¡é›†è®­ä¸­ã€‚

å…¶å®è¦è§£å†³å¾ˆéš¾çš„é—®é¢˜çš„ã€‚ğŸ˜Šï¼Œå¥½ï¼Œé‚£æˆ‘ä»¬è¿™é‡Œä¸¾ä¸ªä¾‹å­ï¼Œæ¯”å¦‚è¯´åŸºäºè¿™ä¸ªé¿è®¤è¿™ä¸ªæˆ‘ä»¬104Pè¿™ä¸ªå‘ƒGPUå“ˆï¼Œæˆ‘ä»¬æ€ä¹ˆå»åšè¿™ç§åƒäº¿å‚æ•°çš„è¿™ç§å¤§å¤§æ¨¡å‹çš„ä¸€ä¸ªè®­ç»ƒã€‚é‚£è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªloadé›¶çš„è¿™ä¸ªç‰¹ç‚¹ã€‚

å•æœº8å¼ å¡ã€‚é‚£æˆ‘ä»¬åœ¨å››å¡å±‚é¢å»åštenserå¹¶è¡Œã€‚ä¸ºä»€ä¹ˆè¿™ä¹ˆåšå‘¢ï¼Ÿå› ä¸ºtenserå¹¶ä¸Šå¯¹é€šä¿¡çš„è¦æ±‚æ˜¯æ¯”è¾ƒé«˜çš„ï¼Œé€šä¿¡é‡æ¯”è¾ƒå¤§ã€‚4å¡æˆ‘ä»¬è¿™ä¸ªäº’è”æ˜¯è¿™ä¸ªå¸¦å®½æ˜¯éå¸¸é«˜çš„å•Šã€‚

é‚£åœ¨å•æœºå…«å¡å±‚é¢æˆ‘ä»¬å»åšä¸¤çº§çš„æµæ°´çº¿å¹¶è¡Œï¼Œå› ä¸ºæµæ°´çº¿å¹¶è¡Œå¯¹é€šä¿¡çš„è¦æ±‚ç›¸å¯¹ä¼šæ¯”è¾ƒä½é‚£ï¼Œè¿™ä¸ªå¯èƒ½åœ¨ç³»ç»Ÿå±‚é¢å®ƒå…¶å®æ˜¯ä¸ä¼šå¸¦æ¥ä»€ä¹ˆè¿™ä¸ªæ€§èƒ½è¿™æ ·ä¸€ä¸ªä¸€ä¸ªå‰¯ä½œç”¨ï¼Œå¯¹å§ï¼Ÿ

å°±æœ‰æ•ˆçš„é¿å…è¿™ä¸ªå°±æ˜¯å¯èƒ½çš„ä¸€äº›ç¡¬ä»¶ä¸Šçš„ä¸€äº›ä¸è¶³å§ã€‚ğŸ˜Šï¼Œé‚£å•æœºå¦‚æœæ”¾ä¸ä¸‹è¿™ä¸ªå‚æ•°çš„è§„æ¨¡ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å¤šæœºå•Šå±‚é¢å»é€šè¿‡è¿›ä¸€æ­¥åˆ©å…¥çº¿å»åšè¿™ä¸ªå‚æ•°çš„åˆ‡åˆ†ã€‚èƒ½æŠŠè¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿæ‰¿è½½ä¸‹æ¥ã€‚é‚£å½“æˆ‘ä»¬æŠŠæ¨¡å‹æ‰¿è½½ä¸‹æ¥ä¹‹åã€‚

å¦‚æœæˆ‘ä»¬ä¸ºäº†æ›´è¿›ä¸€æ­¥å»æå‡è¿™ä¸ªè®­ç»ƒçš„æ€§èƒ½ï¼Œæˆ‘ä»¬è‚¯å®šè¦å¢åŠ æ›´å¤šçš„æœºå™¨ã€‚è¿™ä¸ªæ—¶å€™å¯ä»¥é€‰æ‹©DPæˆ–è€…è¯´Pè¿›ä¸€æ­¥çš„å»å»è¿™ä¸ªæå‡æ€§èƒ½ï¼ŒåŒæ—¶å¦‚æœç”¨0çš„è¯ï¼Œå®ƒæœ‰ä¸ªå¥½å¤„å®ƒå¯ä»¥è¿›ä¸€æ­¥çš„å»åšè¿™ä¸ªå‚æ•°çš„åˆ‡åˆ†ã€‚

èƒ½å¤Ÿé™ä½è¿™ä¸ªå•å¡çš„ä¸€ä¸ªæ˜¾å­˜çš„ä¸€ä¸ªéœ€æ±‚ã€‚é‚£å¦‚æœæˆ‘äº†æ˜¾å­˜å‘ƒè¿™ä¸ªå¯ä»¥éœ€æ±‚é™ä¸‹æ¥ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥æå‡ sizeã€‚é‚£ä¹ˆå®ƒåˆè¿›ä¸€æ­¥å¯ä»¥æå‡çš„è®¡ç®—çš„æ•ˆç‡ã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹å¾—å‡ºæ¥ï¼Œå…¶å®æ•´ä¸ªç³»ç»Ÿä¸­å¾ˆå¤šå˜é‡å•Šã€‚

ä½ æ€ä¹ˆæŠŠè¿™ä¸ªå˜é‡åœ¨è¿™ä¸ªå›ºå®šçš„èµ„æºå’Œæ¨¡å‹è§„æ¨¡æƒ…å†µä¸‹æ€ä¹ˆåˆ©ç”¨å¥½ï¼Œå…¶å®æ˜¯å¯¹æ€§èƒ½æ˜¯æœ‰å¾ˆå¥½çš„ä¸€ä¸ªå¸®åŠ©çš„å•Šã€‚ğŸ˜Šï¼Œå‘ƒï¼Œé‚£å¤§æ¨¡å‹çš„ä¸€ä¸ªäº§ä¸šç”Ÿæ€åˆä½œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå…¶å®å‰é¢è®²ä»è¿™ä¸ªç®—ä¾‹å•Šï¼Œä»è¿™ä¸ªç»§ç»­å­¦å¹³å°åŒ…æ‹¬æ¡†æ¶å•Šã€‚

ä»¥åŠè¿™ä¸ªä¸Šé¢çš„è¿™ä¸ªç”šè‡³å¤§æ¨¡å‹çš„æ¡†æ¶è¿˜åŒ…æ‹¬æ¨¡å‹æ•´ä¸ªæŠ€æœ¯ç«™å®é™…ä¸Šæ˜¯æ¯”è¾ƒåšé‡çš„å“ˆã€‚é‚£æ‰€ä»¥è¿™ç§å‘ƒæƒ³è½åœ°å…¶å®ä¹Ÿæ˜¯æœ‰éš¾åº¦çš„ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œä¹Ÿæ˜¯è·Ÿè¿™ä¸ªæ›´å¤šçš„åˆä½œä¼™ä¼´ä¸€èµ·å»æ‰“é€ æ•´ä½“çš„ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚å‘ƒã€‚

é‚£è¿™é‡Œå‘ƒæˆ‘ä»¬è·Ÿæ™ºæºç ”ç©¶é™¢åœ¨åˆšæ‰è®²äº›flag opençš„è¿™ä¸ªå‘ƒå¤§æ¨¡å‹çš„æŠ€æœ¯å¼€æºä½“ç³»é‡Œé¢ï¼Œå¹¶ç«Ÿä¹Ÿæ˜¯æ·±åº¦å‚ä¸è¿›å»çš„ã€‚æˆ‘ä»¬ä¾æ‰˜è¿™ä¸ªå‘ƒè¿™ä¸ªç§‘æŠ€éƒ¨çš„è¿™ä¸ªç›¸å…³çš„é¡¹ç›®ã€‚

ç›®å‰æˆ‘ä»¬æ­£åœ¨å¼€å±•æ„Ÿè°¢å‘ƒå‰é¢è®²è¿™ä¸ªAIç¼–è¾‘ç›¸å…³çš„ä¸€äº›å·¥ä½œå•Šã€‚ğŸ˜Šï¼Œé‚£ç¬¬äºŒä¸ªæˆ‘ä»¬ä¹Ÿæ˜¯åœ¨è¿™ä¸ªæ”¯æ´çš„è¿™ä¸ªAIå¼€æ”¾å®èµ„æºå®éªŒå®¤é‡Œé¢å»åšè¿™ä¸ªå›½äº§è¿™ä¸ªèŠ¯ç‰‡å’Œå¤§æ¨¡å‹çš„è¿™ä¸ªä¸€äº›é€‚é…å•Šå’Œè½åœ°åº”ç”¨çš„ä¸€äº›è¿™ä¸ªå·¥ä½œã€‚å¯¹å‘ƒã€‚

é‚£å¦å¤–ä¸€ä¸ªæˆ‘ä»¬å†è·Ÿå‘ƒç™¾åº¦æ–‡æ˜Ÿè¿™è¾¹ä¹Ÿæ˜¯ä¸ªæ¯”è¾ƒç´§å¯†çš„åˆä½œã€‚å‘ƒï¼Œå› ä¸ºæˆ‘æˆ‘ä¹‹å‰ä¸€ç›´åœ¨ç™¾åº¦å“ˆï¼Œæ‰€ä»¥å‘ƒè¿™ç§ç›¸å¯¹åˆä½œä¼šæ¯”è¾ƒç´§å¯†ä¸€ç‚¹ã€‚åƒç›®å‰è·Ÿç™¾åº¦é£è®²è¿™äº›æ¨¡å‹é€‚é…å•Šï¼ŒåŒ…æ‹¬æ–‡å¿ƒçš„ä¸€äº›åŸºç¡€æ¨¡å‹é€‚é…ï¼Œæˆ‘ä»¬éƒ½å·²ç»åšèµ·æ¥äº†ã€‚

æ‰€ä»¥åé¢ä¹Ÿä¼šä¸€èµ·çš„è¿™ä¸ªå¼€å±•è¿™ä¸ªå‘ƒè¡Œä¸šçš„ä¸€äº›è½åœ°åº”ç”¨ã€‚ğŸ˜Šï¼Œé‚£å‘ƒé™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸€äº›äº§å­¦æ²¿ç”¨çš„ç”Ÿæ€ä»¬ä¹Ÿåšäº†ä¸€äº›å·¥ä½œã€‚å‘ƒã€‚

æˆ‘ä»¬å’Œè¿™ä¸ªä¸Šæµ·äº¤é€šå¤§å­¦å’Œå‘ƒåŒ…æ‹¬è¾¾æ‘©é™¢å‘¢å‘ƒåœ¨è¿™ä¸ªç§‘æŠ€åˆ›æ–°203å¹´çš„è¿™ä¸ªæ–°ä¸€ä»£å‘ƒäººå·¥æ™ºèƒ½çš„é‡å¤§é¡¹ç›®ä¸­ï¼Œå‘ƒï¼Œè¿™ä¸ªé¡¹ç›®æ˜¯è¿™ä¸ªå¤§å°æ¨¡å‹è¿™ä¸ªç«¯äº‘ååŒè¿›åŒ–ä¸ç³»ç»Ÿã€‚

é‚£æˆ‘ä»¬è”åˆåŸºäºé¿åˆƒçš„è¿™ä¸ªå›½äº§GPUå»æ‰“é€ è¿™ä¸ªå¤§æ¨¡å‹çš„è®­ç»ƒå¹³å°å’Œè¿™ä¸ªç›¸å…³çš„è¿™ä¸ªå‘ƒè¿™ä¸ªè½åœ°ç¤ºèŒƒåº”ç”¨å•Šã€‚ğŸ˜Šï¼Œå‘ƒï¼Œå‘ƒå½“ç„¶äº†ã€‚

æˆ‘ä»¬ç°åœ¨ä¹Ÿè¿˜åœ¨è·Ÿä¸€äº›è¿™ä¸ªå•Šæœ€ç»ˆçš„ç”¨æˆ·å•Šåœ¨å‘ƒä¸€äº›æˆ˜ç•¥åˆä½œä¼™ä¼´åœ¨å¼€å±•ä¸€äº›è¿™ä¸ªå¤§æ¨¡å‹çš„ä¸€äº›è¿™ä¸ªè¿™ä¸ªç›¸å…³çš„é€‚é…å’Œè½åœ°åº”ç”¨çš„åˆä½œã€‚å•Šï¼Œå½“ç„¶æˆ‘ä»¬ä¹Ÿå¸Œæœ›è·Ÿæ›´å¤šçš„åˆä½œä¼™ä¼´å•Šèƒ½å¤Ÿä¸€èµ·è¿™ä¸ªå…±å»ºè¿™ä¸ªå¤§æ¨¡å‹çš„ä¸€ä¸ªäº§ä¸šç”Ÿæ€ã€‚å¥½ã€‚

è°¢è°¢ã€‚ğŸ˜Šã€‚

![](img/b8a2226fb956a3703ba6c0ca40227c0d_44.png)

![](img/b8a2226fb956a3703ba6c0ca40227c0d_45.png)

è°¢è°¢è°¢è°¢ä¸æ€»ã€‚é‚£å‘ƒæˆ‘ä»¬å‘ƒä¸‹é¢è¯·æ¥å‘ƒä¸‹ä¸€ä½å‘ƒspeakerï¼Œæ¥è‡ªmediaçš„é«˜çº§æ€»ç›‘å‘ƒï¼Œèµ–ä¿Šæ°ã€èµ–åšå£«ï¼Œè°¢è°¢ã€‚ğŸ˜Šï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ¥è‡ª NVDdiaã€‚

ç„¶åä¹Ÿä¼šç”¨ä¸­æ–‡ä»‹ç» but before my presentation in Chinese I would present in Chinese I want to share something interesting in English first because know some speakers mentioned some keywords like IBM cell and in area because I was doing my internship at Yonghua's team 15 years ago at IBM on sale on sale current optimization and everything and then I got my PhD job from Iã€‚

 In Han on NviDdia GPUs performance modeling and analysis then I got my job at NVDdia soã€‚

This is I feel it's very interestingï¼Œ you know how our lives cross together at today's forumï¼Œ okayã€‚è¡Œã€‚

é‚£æ¥ä¸‹æ¥çš„è¯è¿˜æ˜¯ç”¨ç”¨ç”¨ä¸­æ–‡ä»‹ç»ã€‚ç„¶åæˆ‘è¿™è¾¹PPTæ¯”è¾ƒå°‘ï¼Œä¸€å…±8é¡µå•Šï¼Œç„¶åæ ¸å¿ƒçš„è¯å°±ä¸€é¡µï¼Œæ‰€ä»¥å‘¢å°±æ˜¯æ—¶é—´è‚¯å®šåº”è¯¥æ¥å¾—åŠã€‚å‘ƒï¼Œè¿™ä¸€é¡µçš„è¯å°±éå¸¸ç®€å•çš„ä»‹ç»ä¸€ä¸‹ã€‚

å°±æ˜¯è¯´å‘ƒé‚£ä¸ªå¤§å®¶å°±æ˜¯å‘ƒæ¯”å¦‚è¯´ä»Šå¤©å¯èƒ½ä¸»è¦è®²çš„å°±æ˜¯æˆ‘ä»¬å¦‚ä½•èƒ½å»æ›´å¿«çš„æ›´æ–¹ä¾¿çš„å»è®­ç»ƒå‡ºæ¥è¿™æ ·çš„ä¸€äº›ç‰¹åˆ«æ˜¯ä¸€äº›å¤§æ¨¡å‹ã€‚ä½†æ˜¯å‘¢å°±æ˜¯åˆšæ‰é‚£ä¸ªå‘ƒå°±æ˜¯ä¹Ÿæœ‰å˜‰å®¾ä¹Ÿè®²äº†ï¼Œå°±æ˜¯åœ¨å®é™…çš„è¿™æ ·çš„ä¸€ä¸ªå·¥ä¸šåœºæ™¯é‡Œé¢çš„è¯ã€‚

å…¶å®æœ‰å¾ˆå¤šå¾ˆå¤æ‚çš„ä¸€äº›é—®é¢˜ã€‚æˆ‘ä¸¾ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼Œå°±æ˜¯æˆ‘ä»¬æœ€è¿‘ä¸€æ®µæ—¶é—´è·Ÿæˆ‘ä»¬çš„å¾ˆå¤šå®¢æˆ·åœ¨åˆä½œçš„ä¸€ä¸ªäº‹æƒ…ï¼Œå°±æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯å‘ƒé‚£ä¸ªæ¯”å¦‚è¯´æˆ‘ä»¬å»è·‘ä¸€ä¸ª1000å¡ï¼Œ2000å¡ã€‚

ç”šè‡³æ›´å¤šå¡çš„è¿™æ ·çš„ä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªå¤§æ¨¡å‹è®­ç»ƒã€‚é‚£å¾ˆæœ‰å¯èƒ½æˆ‘è·‘ç€è·‘ç€å°±æŒ‚äº†ï¼Œå¯¹å§ï¼Ÿç„¶åå‘¢ï¼Œå°±æ˜¯å‘ƒå°±æ˜¯ç”šå³ä½¿æ˜¯æˆ‘ä»¬éå¸¸å‘ƒç”¨æˆ‘ä»¬çš„GPUä¹Ÿå¥½ã€‚

ç”¨æˆ‘ä»¬çš„è¿™ç§å‘ƒå°±æ˜¯å¾ˆå¤§çš„æœºå™¨ç¾¤å»åšè¿™ç§æ¨¡å‹è®­ç»ƒçš„ä¸€äº›å…¬å¸å¾ˆå¤§çš„äº’è”ç½‘å…¬å¸ã€‚ä»–ä»¬åœ¨ä¸€å¼€å§‹çš„æ—¶å€™ï¼Œå°±æ˜¯å¯èƒ½åœ¨Hå‡ºç°ä¹‹å‰ï¼Œä¸€èˆ¬å‘¢ä¹Ÿæ²¡æœ‰é‚£ä¹ˆå¤§è§„æ¨¡çš„è¿™æ ·çš„ä¸€äº›ã€‚ğŸ˜Šï¼Œå°±æ˜¯GPUå»åšè¿™ç§åˆ†å¸ƒå¼è®­ç»ƒçš„ä¸€äº›ç»éªŒã€‚

é‚£æ¢å¥è¯è¯´ï¼Œå°±æ˜¯å¯èƒ½ä»ä»Šç‰¹åˆ«æ˜¯ä»Šå¹´å¼€å§‹ï¼Œæˆ‘ä¹Ÿå‘ç°å‘¢ï¼Œæ¯”å¦‚è¯´æˆ‘ç»å¸¸è·‘è¿™æ ·çš„ä¸€ä¸ªå¾ˆå¤šé‡å¡çš„ä»»åŠ¡çš„æ—¶å€™ï¼Œå¯èƒ½æˆ‘è·‘ä¸ªå‡ ä¸ªå°æ—¶å•Šï¼Œæˆ‘è¿™ä¸ªè¿™ä¸ªè¿™ä¸ªè¿™ä¸ªè¿™ä¸ªè¿™ä¸ªä»»åŠ¡å°±è¦é‡å¯è¿™ä¸ªå°±æ˜¯ä¸­é—´é‡åˆ°çš„é—®é¢˜å¯èƒ½æ˜¯å¤šç§å¤šæ ·çš„å•Šã€‚

æœ‰å¯èƒ½æ˜¯æ¯”å¦‚è¯´æˆ‘ä»¬çš„æ‰€è°“çš„GPUçš„å¡æ‰äº†å•Šï¼Œæœ‰å¯èƒ½æ˜¯æ¯”å¦‚è¯´æˆ‘ä»¬çš„ç½‘ç»œè¿™è¾¹çš„è¯ï¼Œå°±æ˜¯æœ‰æœ‰å¯èƒ½ä»–ä¸€ä¸ªlinkå•Šå°±åæ‰äº†å•Šï¼Œæœ‰å¯èƒ½æ˜¯æˆ‘ä»¬çš„è¿™ä¸ªå­˜å‚¨ï¼Œç”šè‡³å­˜å‚¨çš„è¿™æ ·çš„ä¸€ä¸ªç®¡ç†è½¯ä»¶å°±æ˜¯ä¸å“åº”äº†ç­‰ç­‰ã€‚

æ‰€ä»¥å°±æ˜¯è¯´å®é™…çš„è¿™æ ·å·¥ä¸šåœºæ™¯é‡Œé¢çš„è¯ï¼Œå®ƒçš„é—®é¢˜ä¼šéå¸¸éå¸¸çš„å¤æ‚ã€‚ç„¶åå‘¢å°±æ˜¯æˆ‘ä»¬å‘ƒå°±æ˜¯åˆšæ‰è®²äº†ï¼Œå°±æ˜¯å¯èƒ½è·Ÿæˆ‘ä»¬çš„å¾ˆå¤šè¿™ç§å‘ƒé‚£ä¸ªå·¥ä¸šç•Œçš„å®¢æˆ·åˆä½œçš„å…¶å®æŸç§ç¨‹åº¦ä¸Šç¬¬ä¸€ä»¶äº‹æƒ…å°±æ˜¯è¦èŠ±å¾ˆå¤šçš„è¿™æ ·çš„ä¸€äº›ç²¾åŠ›ã€‚

èƒ½è®©è¿™æ ·çš„ä¸€ä¸ªå¤§æ¨¡å‹è®­ç»ƒçš„ä»»åŠ¡çš„è¯ï¼Œä¸€ä¸ªå‘ƒç›¸å¯¹ç¨³å®šã€‚ç”¨çš„æ–¹å¼çš„è¯ï¼Œè¿è¡Œè¶³å¤Ÿé•¿çš„æ—¶é—´ã€‚æ‰“æ¯”æ–¹è¯´ä¸€ä¸ªå‘¨å•Šï¼Œä¸€ä¸ªå‘¨çš„è¯ï¼Œæ¯”å¦‚è¯´æˆ‘æˆ‘é‡åˆ°ä¸€æ¬¡æ•…éšœï¼Œæˆ–è€…è¯´æŒ‚æ‰ä¸€æ¬¡ã€‚

é‚£æˆ‘èŠ±ä¸ªåŠä¸ªå°æ—¶æ—¶é—´æŠŠè¿™ä¸ªjobé‡æ–°launchèµ·æ¥ï¼Œè¿˜æ˜¯æ¯”è¾ƒèƒ½æ¥å—çš„ã€‚ä½†æ˜¯å¦‚æœè¯´ä½ è·‘ä¸¤ä¸ªå°æ—¶å°±è¦æŒ‚äº†ï¼Œç„¶åå†èŠ±åŠä¸ªå°æ—¶ç»™å®ƒå†²å¯è¿™ä¸ªå·¥ç¨‹ä¸Šçš„è¯ï¼Œå°±ä¼šå°±ä¼šéå¸¸çš„è´¹åŠ²å—¯ã€‚é‚£ä¹ˆå°±æ˜¯å‘ƒç”šè‡³å‘¢å°±æ˜¯è¯´å‘ƒã€‚

è¿˜æœ‰éå¸¸éå¸¸ç®€å•çš„å°±æ˜¯å‘ƒæˆ‘æˆ‘æˆ‘æ¯”æ‰“æ¯”æ–¹è¯´æˆ‘è¿™ä¸ªä»»åŠ¡æŒ‚äº†ï¼Œä½ ç”šè‡³è¦çŸ¥é“å“ªä¸ªåœ°æ–¹æŒ‚äº†ï¼Œéƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå¯¹å§ï¼Ÿå°±æ˜¯é‚£ä¹ˆå¤šçš„æœºå™¨ï¼Œé‚£ä¹ˆå¤šcomponentï¼Œé‚£ä¹ˆå¤šçš„è¿™æ ·çš„ä¸€äº›éƒ¨ä»¶ï¼Œåˆ°åº•å“ªä¸ªä¸œè¥¿åäº†ã€‚

æ‰€ä»¥å‘¢å°±æ˜¯ç”šè‡³æ˜¯è¯´åƒå¦‚ä½•èƒ½å¤Ÿé«˜æ•ˆçš„å°±æ˜¯ç”¨å¾ˆå¤šä¸åŒçš„è½¯ä»¶ï¼Œä½ è¦å»ç›‘æ§çš„GPUè¦ç›‘æ§ä½ çš„ç½‘ç»œè®¾å¤‡çš„ï¼Œå°±ç”¨ä»€ä¹ˆæ ·çš„ä¸€äº›è½¯ä»¶èƒ½å¤ŸåŠæ—¶çš„å‘ç°åˆ°åº•å®ƒæ˜¯ä»€ä¹ˆæ ·çš„æ•…éšœã€‚æˆ‘è¿™æ•…éšœçš„è¯ï¼Œæ˜¯è¿™ä¸ªè¿™ä¸ªå¡å°±æ˜¯åæ‰äº†ã€‚

æˆ‘è¦å»èµ°ä¸€äº›è¿™ç§æ¢è´§çš„æµç¨‹ï¼Œè¿˜æ˜¯è¯´æˆ‘æŠŠè¿™ä¸ªä»€ä¹ˆå°±æ˜¯æ¯”è¯´æˆ‘çš„é£æ‰‡é€Ÿåº¦è°ƒå¿«ç‚¹å•Šï¼Œå°±æ˜¯è®©å®ƒæ¸©åº¦å°½é‡çš„æ§åˆ¶åœ¨æ¯”å¦‚è¯´70å¤šåº¦ä¸è¦åˆ°è¾¾80å¤šåº¦è¿™æ ·çš„æƒ…å†µã€‚

å®ƒå°±èƒ½ç»§ç»­ç¨³å®šè¿è¡Œç­‰ç­‰è¿™æ ·éå¸¸éå¸¸å¤šçš„è¿™å¸¸å¤æ‚çš„è¿™å·¥ç¨‹ä¸Šçš„é—®é¢˜ã€‚é‚£ä¹ˆç°åœ¨çš„è¯ä¸€èˆ¬æ¥è¯´å¤§å®¶æ˜¯æ€ä¹ˆåšå‘¢ï¼Ÿæ¯”å¦‚è¯´ä¸¾ä¸ªä¾‹å­ï¼Œå°±æ˜¯æˆ‘ä»¬è¦è·‘1ä¸ª124å¡çš„è¿™æ ·ä¸€ä¸ªéƒ¨é—¨è®­ç»ƒçš„è¿™æ ·ä¸€ä¸ªä»»åŠ¡å•Šã€‚

é‚£ä¹ˆæœ‰å¯èƒ½å‘¢æˆ‘ä¼šå»ä¸€ä¸ªæ¯”å¦‚è¯´1200å—å¡çš„è¿™æ ·ä¸€ä¸ªæ± å­ã€‚ç„¶åå‘¢å°±æ˜¯åœ¨ã€‚ğŸ˜Šï¼Œå¼€å§‹çš„è¯æˆ‘ä¼šå»åšä¸€äº›alal checkï¼Œå°±æ˜¯è®©è¿™äº›GPUå»å•Šè·‘ä¸€äº›ã€‚

æ¯”å¦‚è¯´ä¸€äº›memory intensiveæˆ–è€…è¯´comp intensiveçš„è¿™æ ·çš„ä¸€äº›workloadå»çœ‹ä¸€ä¸‹ã€‚æ¯”å¦‚è¯´è¿™å°±æ˜¯å…ˆé€‰å‡ºæ¥1200å¼ ç›¸å¯¹æ¥è¯´æ¯”è¾ƒå¥åº·çš„è¿™æ ·ä¸€äº›å¡ã€‚

ç„¶åå‘¢æˆ‘å¼€å§‹è¿è¡Œè¿è¡Œåˆ°20å¤©ä¹‹åæœ‰å¯èƒ½æŒ‚æ‰äº†å•Šï¼Œæœ‰å¯èƒ½è¿™ä¸ªä»»åŠ¡æŒ‚æ‰æŒ‚æ‰ä¹‹åï¼Œå¦‚è¯´æˆ‘è¦æ‰¾åˆ°å“ªä¸ªåœ°æ–¹åäº†åäº†ä¹‹åï¼Œæ¯”å¦‚è¯´å‡è¯´æ˜¯ä¸€ä¸ªGPUçš„é—®é¢˜ã€‚

é‚£ä¹ˆæˆ‘æŠŠè¿™å‡ ä¸ªå‘ƒGPUå¯¹åº”çš„è¿™æ ·çš„ä¸€ä¸ªservç»™ä»–ä»è¿™ä¸ªæ± å­é‡Œé¢1024è¿™ä¸ªå­é‡Œé¢æ‹¿å‡ºå»ã€‚ä»åˆšæ‰çš„é‚£ä¸ªå¤§çš„æ± é‡Œé¢è¿˜é—²ç½®é‚£ä¸ªç‚¹æŠ“è¿‡æ¥å‡ ä¸ªå®Œäº†ä¹‹åï¼Œä»æˆ‘çš„å­˜å‚¨é‡Œé¢çš„è¯ã€‚

æŠŠæˆ‘ checkpointè¿›æ¥é‡å¯è¿™æ ·çš„ä¸€ä¸ªä»»åŠ¡å•Šï¼Œæ‰€ä»¥ç°åœ¨çš„è¯ä¸€èˆ¬æ˜¯è¿™ä¹ˆå¹²çš„ã€‚ğŸ˜Šï¼Œå‘ƒï¼Œäº§å“ä»‹ç»æˆ‘å°±ä¸ä»‹ç»äº†ã€‚ç„¶åå‘¢å°±æ˜¯è¿™ä¸ªåœ°æ–¹çš„è¯ç¨å¾®æé‚£ä¹ˆä¸€ç‚¹ç‚¹ï¼Œå°±æ˜¯å¤§å®¶å¯èƒ½å°±æ˜¯å‘ƒå°±æ˜¯å¯èƒ½ç”¨çš„æ¯”è¾ƒå¤šçš„ã€‚

æˆ–è€…è¯´æ¯”è¾ƒç†Ÿæ‚‰çš„æ˜¯ç¬¬ä¸€ä¸ªå°±æ˜¯é‚£ä¸ªMè¿™ä¸ªæ˜¯ä¸€ä¸ªå¼€æºçš„è¿™æ ·ä¸€ä¸ªé¡¹ç›®ã€‚å°±æ˜¯é‚£ä¸ªå°±æ˜¯ä¹Ÿæœ‰å¾ˆå¤šå…¶ä»–çš„ä¸€äº›å·¥ä½œï¼Œå€Ÿé‰´äº†è¿™æ ·çš„ä¸€ä¸ªå¼€æºé¡¹çš„ä¸€äº›æ€è·¯ã€‚

ç„¶åå‘¢å°±æ˜¯å½“ç„¶å°±æ˜¯è¿™ä¸ªè¿™ä¸ªå¼€æºé¡¹ç›®çš„è¯ä¹Ÿå€Ÿé‰´äº†å¾ˆå¤šå…¶ä»–çš„æ¯”å¦‚åƒç­‰ç­‰é‡Œé¢çš„è¿™æ ·çš„ä¸€äº›ä¸€äº›å·¥ä½œã€‚å‘ƒç„¶åå‘¢å°±æ˜¯åé¢ä¸¤ä¸ªçš„è¯å°±åˆšæ‰æˆ‘è¯´æˆ‘ä¸ä»‹ç»äº†ã€‚

å°±æ˜¯moçš„è¯æ˜¯æˆ‘ä»¬çš„ä¸€ä¸ªé‚£ä¸ªå°±æ˜¯åšè‡ªç„¶è¯­è¨€è¯­éŸ³çš„è¿™æ ·çš„ä¸€ä¸ªå‘ƒå¼€æºçš„ä¸€ä¸ªkiï¼Œç„¶åæœ€åé‚£ä¸ªä¸œè¥¿çš„è¯å°±æ˜¯å«mo frameworkå°±æ˜¯è¿™ä¸ªçš„è¯æ˜¯ä¸€ä¸ªå¯ä»¥è¯´æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„è¿™æ ·ä¸€ä¸ªäº§å“ã€‚

å°±æ˜¯è¿™ä¸ªåœ°æ–¹å°±æ˜¯ä¸ºä»€ä¹ˆç¨å¾®æä¸€ä¸‹å‘¢ï¼Œå°±æ˜¯è¯´è¿˜æ˜¯é‚£å¥è¯å°±æ˜¯å·¥ä¸šç•Œçš„è¿™æ ·çš„ä¸€ä¸ªå®é™…çš„è¿™æ ·ä¸€ä¸ªç”¨æˆ·æ¥è®²çš„è¯ï¼Œä»–ä¸æ˜¯è¯´æˆ‘æœ‰ä¸€ä¸ªå¼€æºçš„é¡¹ç›®æ”¾åœ¨é‚£ä¸ªåœ°æ–¹å•Šï¼Œå°±å¯ä»¥äº†ã€‚å°±æ˜¯ä»–ç»å¸¸çš„é—®é¢˜æ˜¯è¯´ã€‚å‘ƒã€‚

é‚£æ¯”å¦‚è¯´æˆ‘ç”šè‡³æ˜¯æˆ‘æœ‰ä¸€å¤§å †çš„æ•°æ®ï¼Œæˆ‘è¿™äº›æ•°æ®çš„è¯ï¼Œè¯¥æ€ä¹ˆå»å‘ƒå¤„ç†ä¸€ä¸‹ï¼Œæ¸…æ´—ä¸€ä¸‹ã€‚ç„¶åå‘¢å˜æˆæˆ‘è¿™ä¸ªåé¢çš„è¿™æ ·çš„ä¸€äº›è®­ç»ƒä»¬å¯ä»¥å‘ƒç”¨çš„è¿™æ ·çš„ä¸€ä¸ªå‘ƒæ ¼å¼ï¼Œç”šè‡³æ˜¯è¿™æ ·çš„ä¸€äº›é—®é¢˜ã€‚

è¿˜æœ‰å‘¢å°±æ˜¯ç”šè‡³é—®é¢˜æ˜¯è¯´å•Šæˆ‘è¦å»é‚£ä¸ªå¤§æ¨¡å‹ï¼Œæ¯”å¦‚è¯´é‚£ä¸ª40ä¸ª billioné‡è¿˜æ˜¯å¤šå°‘çš„ï¼Œæˆ‘è¯¥ä¹°å¤šå°‘å—GPUæˆ–è€…ç”¨å¤šå°‘æœåŠ¡å™¨ï¼Œæˆ‘ä¸­é—´æ˜¯ç”¨rockyè¿˜æ˜¯ç”¨fin bandã€‚ç„¶åå‘¢æˆ‘è¿™äº›è¶…é¤çš„è¯ã€‚

è¯¥æ€ä¹ˆå»è®¾ç­‰ç­‰ã€‚è¿™æ ·çš„ä¸€äº›éå¸¸éå¸¸practicalçš„é—®é¢˜ã€‚æ‰€ä»¥å‘¢å°±æ˜¯è¯´è¿™ä¸ªäº§å“é‡Œå¤´å°±æ˜¯å¯¹è¿™ç§ä¼ä¸šçº§å®¢æˆ·çš„äº§å“é‡Œå¤´è¯ã€‚

ä¼šæä¾›å„ç§å„æ ·çš„è¿™æ ·çš„ä¸€äº›å·¥å…·å»å¸®åŠ©è¿™äº›ä¼ä¸šçº§å®¢æˆ·å»æ›´å¥½çš„å»åšè¿™æ ·çš„ä¸€äº›å¤§å®¶å¯èƒ½è§‰å¾—æ¯”è¾ƒå‚»ç“œçš„è¿™æ ·çš„ä¸€äº›é€‰æ‹©çš„è¿™æ ·çš„ä¸€äº›é—®é¢˜ã€‚ğŸ˜Šï¼Œå‘ƒï¼ŒNV linkN switchitchçš„è¯ï¼Œæˆ‘ä¹Ÿå°±è·³è¿‡å»äº†ã€‚å‘ƒã€‚

å°±æ˜¯è¿™ä¸€é¡µçš„è¯ç¨å¾®å¤šèŠ±ä¸€ç‚¹æ—¶é—´ï¼Œå°±æ˜¯å¯èƒ½æ˜¯å‘ƒé‚£ä¸ªæ¯”è¾ƒæ ¸å¿ƒçš„è¿™æ ·çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚å°±æ˜¯è¯´å‘ƒnemoé‡Œé¢çš„è¯ä¸€äº›å‘ƒå‘ƒå‘ƒnemoå‘ƒå°±æ˜¯megaronLMæˆ–è€…è¯´nemoronå°±æ˜¯ã€‚å‘ƒï¼Œæ€»ä¹‹å°±æ˜¯NVè¿™è¾¹çš„è¯ã€‚

é’ˆå¯¹è¿™ç§å¤§æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è¿™ç§transformer basedçš„è¿™æ ·çš„ä¸€äº›å‘ƒå¤§æ¨¡å‹çš„è¯ï¼Œåœ¨å‘ƒåƒå„ç§å„æ ·çš„è¿™æ ·çš„ä¸€äº›å¹¶è¡Œç­–ç•¥çš„é¢ä¸€äº›æ ¸å¿ƒçš„ä¸€äº›å‘ƒåšæ³•ã€‚å‘ƒï¼Œç„¶åå‘¢ä»¥åŠä»–çš„èƒŒåçš„ä¸€äº›æƒ³æ³•æˆ–è€…è¯´é€»è¾‘ã€‚

ç¬¬ä¸€ä¸ªçš„è¯å°±æ˜¯æœ€ç†Ÿæ‚‰çš„æœ€ç®€å•çš„å°±æ˜¯æ•°æ®å¹¶è¡Œã€‚æ•°æ®å¹¶è¡Œè¿™è¾¹çš„è¯å°±æ˜¯æˆ‘ä¹Ÿæˆ‘ä¹Ÿä¸è®²äº†ï¼Œå°±æ˜¯è¯´è¿™ä¸ªå¤§å®¶åº”è¯¥å‘ƒå°±æ˜¯éå¸¸å®¹æ˜“ç†è§£ã€‚å°±ç‰¹åˆ«æ˜¯å‘¢åœ¨ä¸€å¼€å§‹æˆ‘ä»¬æ‰€è°“çš„å»åšå‘ƒå¼€å§‹çš„ï¼Œåˆ«æ˜¯å¼€å§‹ä¸€äº›CVçš„è¿™ç§æ¨¡å‹è®­ç»ƒçš„æ—¶å€™ã€‚

é‚£æ¯”è¯´ä¸€å—å¡å¾—æ¯”è¾ƒæ…¢ã€‚æˆ‘ç”¨2å—å¡4å—å¡8å—å¡16å—å¡çš„æ—¶å€™ï¼Œé‚£æœ€å¼€å§‹çš„å¤§å®¶è‚¯å®šå…ˆå¥”ç€è¿™ä¸ªæ¥çš„ã€‚ç„¶åå‘¢å°±æ˜¯ç‰¹åˆ«æ˜¯è¿™ç§å‘ƒå¤§æ¨¡å‹çš„è¿™äº›äº‹æƒ…å‡ºæ¥ä¹‹åå‘¢ï¼Œå°±æ˜¯é€æ¸å‘ç°çš„è¯ï¼Œæ•°æ®å¹¶è¡Œçš„è¯ã€‚

å®ƒå®ƒä¼šæœ‰ä¸€äº›å‘ƒå›ºæœ‰çš„è¿™æ ·ä¸èƒ½è¯´æ˜¯ç¼ºé™·å•Šï¼Œä¸€äº›ä¸€äº›æŒ‘æˆ˜æˆ–è€…é—®é¢˜ã€‚å› ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´å‡å¦‚è¯´ä½ çš„æ•°æ®å¹¶è¡Œçš„è¿™æ ·çš„ä¸€ä¸ªä½ GPUä¹Ÿå¥½æˆ–è€…è¯´è®¡ç®—çš„è¿™æ ·çš„ä¸€ä¸ªä¸ªä¸€ä¸ªèŠ‚ç‚¹ä¹Ÿå¥½æ¯”å¤šäº†ä¹‹å®ƒå°±æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿ

ä½ è¦ä¹ˆæ˜¯è¯´ä½ è¿™æ ·åŒä¸€è½®è¿™ä¸ªè¦éå¸¸éå¸¸çš„å¤§ï¼Œè¦ä¹ˆå°±æ˜¯è¯´é‚£ä¸ªæˆ‘è¿™ä¸ªå°±globalè¿™ä¸ªä¸å˜çš„è¯ï¼Œä½ åˆ†åˆ°æ¯ä¸€ä¸ªGPUä¸Šçš„å®ƒè¿™ä¸ªè¿™ä¸ªè¿™ä¸ªçš„è¯å°±åˆ«ã€‚ğŸ˜Šï¼Œå°é‚£ä¹ˆå‘ƒå¤§å®¶ä¹Ÿéƒ½çŸ¥é“ï¼Œç°åœ¨çš„è¿™æ ·çš„ä¸€äº›å¤„ç†å™¨çš„è¯ã€‚

å®ƒå¤„ç†èƒ½åŠ›æ¯”è¾ƒå¼ºï¼Œé‡Œé¢æœ‰å¾ˆå¤šè¿™æ ·ä¸€äº›è®¡ç®—å•å…ƒä¹ˆä¹‹çš„ã€‚å¦‚æœè¯´è¿™ä¸ªå¤ªå°çš„è¯ï¼Œä½ è¿™ä¸ªGPUåˆ©ç”¨ç‡å°±ä¸Šä¸å»ã€‚ä»¥å‘¢å°±æ˜¯ä¸ºä»€ä¹ˆè¯´åƒæ•°æ®å¹¶è¡Œè¿™æ¡è·¯çš„è¯ã€‚

å¼€å§‹çš„ç›¸å°±æ˜¯å°±æ˜¯ç”¨çš„å¡ä¹Ÿå¥½è¿™ç§è¿™ç§æœåŠ¡å™¨å°‘çš„ä»Šçš„è¯å°±æ˜¯å…‰é æ•°æ®ä¸æ‰æœ‰äº†æ¨¡å‹å¹¶è¿™ä»¶äº‹æƒ…æ˜¯åˆšæ¨å‡ºæ¥çš„æ—¶å€™ä¸»è¦æ˜¯åšçš„ç¬¬äºŒä¸ªå›¾è¿™ä¸ªå·¥ä½œã€‚

å°±æ˜¯åˆšæ‰æœ‰å˜‰å®¾ä¹Ÿä»‹ç»äº†å°±æ˜¯åƒè¿™å„ç§å„æ ·æ¨ªå‘çš„çºµå‘çš„è¿™æ ·ä¸€äº›åˆ‡åˆ†è¿™ä¸ªåœ°å€¼å¾—æä¸€ä¸‹çš„å°±æ˜¯åˆšæ‰æœ‰ä¸€ä¸ªå›¾å°±æ˜¯äº‘å¸†è¿™è¾¹é‚£ä¸ªå°±æ˜¯é‚£ä¸ªä¸ªäº‹æƒ…ã€‚å°±æ˜¯æ¨¡å‹çš„çš„è¯é‚£æœ€ç®€å•çš„é€»è¾‘å°±æ˜¯è¿™ä¸ªç½‘ç»œçºµå‘åˆ‡å°±Pæ”¾ä¸ªæ”¾ä¸ªå°±æ˜¯ä¸­é—´ã€‚ğŸ˜Šã€‚

çš„è¯å°±æ˜¯ä»–è‚¯å®šæœ‰ä¸ªæ¶ˆæ¯ä¼ é€’çš„è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹å•Šï¼Œä½†æ˜¯è¿™æ ·è¿‡ç¨‹çš„è¯ï¼Œå°±æ˜¯é‚£æœ‰é‚£ä¸ªé‚£ä¸ªå›¾çš„è¯ï¼Œå¤§å®¶ç¨å¾®å›å¿†ä¸€ä¸‹ï¼Œå°±ä¸­é—´ä¼šæœ‰ä¸€äº›ç°è‰²çš„è¿™æ ·çš„ä¸€äº›åŒºåŸŸï¼Œå°±æ˜¯å°±å«å«bubbleã€‚ä½†å®é™…ä¸Šæ¥è¯´çš„è¯ã€‚

å°±æ˜¯å‘ƒä¸ç®¡æ˜¯é‚£ä¸ªmicroofIMè¿˜æ˜¯å…¶ä»–çš„è¿™æ ·çš„ä¸€äº›å‘ƒåšè¿™ç§å¤§éƒ¨å‹åˆ†å¸ƒå¼çš„è¿™æ ·ä¸€ä¸ªæ¡†æ¶ï¼Œå…¶å®åšäº†å—¯å°±æ˜¯æ¯”è¾ƒå—¯ã€‚ğŸ˜Šï¼Œæ¯”è¾ƒæ¯”è¾ƒæ¯”è¾ƒæ¯”è¾ƒèªæ˜çš„è¿™æ ·ä¸€äº›ç­–ç•¥ã€‚ç„¶åå‘¢å°±æ˜¯æ¥å»å‘ƒå°±æ˜¯å°½é‡çš„è®©å°‘ä¸€äº›ã€‚

æ‰“æ¯”æ–¹è¯´æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯æˆ‘ä»¬æœ€ç›´è§‚å‘ƒå®¹æ˜“ç†è§£çš„å°±æ˜¯è¯´åˆšæ‰è¯´çš„æ¯”å¦‚è¯´æŠŠ0åˆ°3ä¸ªæ”¾åˆ°PUä¸Šï¼Œç„¶åå‘¢å°±æ˜¯4åˆ°7æ”¾åˆ°GPU1å¯¹å§ï¼Ÿä½†æ˜¯å®é™…ä¸Šæ¥è¯´çš„è¯å°±æ˜¯è¯´æˆ‘ä»¬åœ¨å°±æ˜¯è¯´æˆ‘å»çš„å€™ã€‚

ä¸ªPUUçš„æ”¾ç½®çš„è¯å¹¶ä¸æ˜¯è¿™æ ·çš„ä¸€ä¸ªå°±æ˜¯å‘ƒä»ä»æœ€ç®€ä»æœ€ä½åˆ°é«˜è¿™æ ·çš„ä¸€ä¸ªçº¿æ€§è¿™æ ·åˆ‡åˆ†çš„ã€‚å®é™…ä¸Šçš„è¯æŠŠå®ƒç¨å¾®æ‰“ä¹±ä¸€ä¸‹å®Œäº†ä¹‹åçš„è¯å°±æ˜¯ç­‰ç­‰å•Šç”¨è¿™æ ·çš„ä¸€äº›é‚£ä¸ªå°±æ˜¯æ‰€è°“çš„è°ƒåº¦ä¸Šçš„è¿™æ ·çš„ä¸€äº›ç­–ç•¥èƒ½è®©å°½é‡çš„è®©ä»–é‚£ä¸ªç°è‰²çš„åŒºåŸŸä¸€ç‚¹ã€‚

ç„¶åå‘¢å°±ä¸‹é¢ä¸¤ä¸ªçš„è¯æ˜¯NVè¿™è¾¹å°±æ˜¯æ¯”è¾ƒæ–°çš„ä¸€äº›å·¥ä½œã€‚å®é™…ä¸Šå»å¹´çš„è¯å°±æ˜¯æ‰ã€‚é‚£ä¸ªé‚£ä¸ªå‘è¡¨çš„ä¸€äº›å·¥ä½œï¼Œå°±æ˜¯å‘ƒé‚£ä¸ªå‘ƒå°±æ˜¯ä¸€ä¸ªæ˜¯å‘ƒå°±æ˜¯å‘ƒå°±æ˜¯è¿™ä¸¤ä¸ªçš„ä¸»è¦å·¥ä½œï¼Œå®ƒçš„ä¸»è¦ç›®çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´æˆ‘ä»¬å…¶å®å¤§å®¶ä¹Ÿéƒ½å¬åˆ°ã€‚

æ¯”å¦‚è¯´æœ‰å¾ˆå¤šå…¶ä»–çš„å·¥ä½œï¼Œå®ƒçš„ä¸»è¦ç›®çš„æ˜¯è¯´è®©GPUå­˜æˆ–è€…GPUçš„é‡ä¸€å®šçš„è¿™æ ·çš„ä¸€ä¸ªæƒ…å†µä¸‹ï¼Œæˆ‘èƒ½å°½å¯èƒ½å¤šçš„å°½å¯èƒ½å»è®­ç»ƒæ›´å¤§çš„è¿™æ ·çš„ä¸€ä¸ªæ¨¡å‹ã€‚æ¢å¥è¯è¯´å°±æ˜¯å°½é‡çš„å‡å°‘æˆ‘è¿™æ ·çš„ä¸€ä¸ªå‘ƒå¯¹äºæ˜¾å­˜çš„è¿™æ ·çš„ä¸€ä¸ªæ¶ˆè€—ã€‚

æˆ–è€…æ˜¯å¯¹æ˜¾å­˜çš„è¿™æ ·ä¸€ä¸ªpressã€‚è¿™ä¸¤ä¸ªå·¥ä½œçš„è¯ä¸»è¦ä¹Ÿæ˜¯åšç±»ä¼¼çš„å·¥ä½œã€‚ä½†æ˜¯å‘¢å°±æ˜¯NVè¿™è¾¹çš„è¿™æ ·çš„ä¸€ä¸ªspointæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´æˆ‘åœ¨åŸºæœ¬ä¸å½±å“æˆ‘æœ€åæ•´ä½“çš„compute efficiencyçš„å‰æä¸‹ã€‚

æˆ‘èƒ½å¤Ÿå¦‚ä½•å»å‡å°‘å¯¹äºGPU memoryçš„ pressureå•Šï¼Œå°±æ˜¯è¿™ä¸¤å°±æ˜¯è¿™ä¸ªè¿™ä¸ªè¿™ä¸ªNVè¿™è¾¹å·¥ä½œè·Ÿå…¶ä»–çš„å·¥ä½œæœ€å¤§çš„è¿™æ ·çš„ä¸€ä¸ªå‡ºå‘ç‚¹çš„ä¸åŒã€‚å‘ƒï¼Œç„¶åå‘¢å°±æ˜¯å‘ƒsequence parallelã€‚

å®ƒä¸»è¦æŒ‡çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´æˆ‘ä»¬é‚£ä¸ªtransformerçš„ã€‚å‘ƒï¼Œä¸€ç³»åˆ—æ“ä½œå•Šå°±æ˜¯å‰é¢çš„æ¨¡å‹å¹¶å‹çš„è¯ï¼Œä¸»è¦æ˜¯åœ¨å‘ƒattentioné‚£äº›layï¼Œè¿˜æœ‰é‚£ä¸ªMOPæˆ–è€…è¯´é‚£ä¸ªå‰é¡¹çš„é‚£å‡ ä¸ª layerä¸Šé¢åšçš„å·¥ä½œã€‚

ä½†æ˜¯å‘¢å°±æ˜¯åƒ norm outè¿™äº›ä¸œè¥¿çš„è¯ï¼Œå°±æ˜¯ä»–å¯¹äºè®¡ç®—çš„è¿‡ç¨‹ä¸­å‰åæ˜¾å­˜æ¶ˆè€—æ¯”è¾ƒå¤§ã€‚ä½†æ˜¯å‘¢å°±æ˜¯ä¹‹å‰çš„å°±æ˜¯ä½ å°±æ˜¯æ²¡æœ‰åŠæ³•å»åšè¿™æ ·çš„ä¸€ä¸ªæ¨¡å‹å¹¶or parallelæˆ–è¯´ parallelå»åšã€‚

ä½†æ˜¯å‘¢å°±æ˜¯è¯´æˆ‘ä»¬ç°åœ¨æ˜¯åšçš„æ˜¯çš„å¥½å¤„å°±æ˜¯è¯´ä½ åœ¨senceçš„è¿™æ ·çš„ä¸€ä¸ªæ–¹å‘ä¸Šæ¥çœ‹çš„è¯ï¼Œå®é™…ä¸Šå°±æ˜¯å®ƒä¸€ä¸ªsequenceæ¥çœ‹ï¼Œå®ƒçš„è¿™ä¸ªè®¡ç®—æ˜¯independentã€‚

æ‰€ä»¥å‘¢å°±æ˜¯è¯´senceæ˜¯åœ¨senceè¿™ä¸ªè¿™ä¸ªç»´åº¦ä¸Šç„¶å†ç»™ä»–åˆ‡å‡ åˆ€åˆ†æˆåˆ†ç»™ä¸åŒçš„å‡ ä¸ªPUå®ƒå»å¹¶è¡Œäº†å»ç®—ã€‚å‘ƒï¼Œæœ€åé‚£ä¸ªselective activation recompæ˜¯å•¥æ„æ€å‘¢ï¼Ÿ

å°±æ˜¯è¯´å‘ƒå…ˆè¯´é‚£ä¸ªactivation recompå°±æ˜¯å‘ƒã€‚ä¸ºäº†å‡å°‘å¯¹äºå‘ƒå°±æ˜¯activationï¼Œå°±æ˜¯æ¿€æ´»è¿™äº›å‘ƒå¯¹äºæ˜¾å­˜çš„å ç”¨å‘ƒï¼Œå…¶ä¸­æœ‰çš„æ–¹æ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´ã€‚

æˆ‘åœ¨å‘ƒä¸€äº›å‘ƒå°±æ˜¯æˆ‘æˆ‘æˆ‘æˆ‘æˆ‘åœ¨æ˜¾å­˜é‡Œé¢ä¿å­˜å‘ƒä¸€éƒ¨åˆ†æ¿€æ´»å€¼ã€‚ç„¶åå‘¢ï¼Œå°±æ˜¯å‘ƒæ¯”å¦‚è¯´å‡ å±‚ layerï¼Œå°±æ˜¯å®ƒçš„è¾“å…¥çš„é‚£ä¸ªé‚£ä¸ªæ¿€æ´»ã€‚å†ç®—å°±æ˜¯åé¡¹çš„æ—¶å€™ï¼Œå› ä¸ºè¦ç”¨åˆ°æ‰€æœ‰çš„è¿™æ ·çš„æ¿€æ´»å€¼ã€‚é‚£ä¹ˆæˆ‘åœ¨åšåé¡¹ä¼ æ’­çš„æ—¶å€™ã€‚

æˆ‘ä¸´æ—¶çš„å»é‡æ–°å»ç®—é‚£äº›å°±æ˜¯æ¿€æ´»å€¼å‘ƒå°±æ˜¯è¿™æ ·å­çš„è¯å°±æ˜¯è¯´å®ƒçš„æ ¸å¿ƒçš„è¯è¿˜æ˜¯é‚£å¥è¯ï¼Œå°±æ˜¯è¯´åªä¿ç•™ä¸€éƒ¨åˆ†æ¿€æ´»å€¼ã€‚è¿™æˆ‘å¯ä»¥å‡å°‘å¯¹äºæ˜¾å­˜çš„è¿™æ ·çš„ä¸€ä¸ªå‹åŠ›ã€‚ä½†è¿™æ ·çš„é—®é¢˜æ˜¾è€Œæ˜“è§çš„å°±æ˜¯è¯´æˆ‘å¢åŠ äº†å¤§é‡çš„é‡å¤è®¡ç®—ã€‚

æ‰€ä»¥å‘¢å°±æ˜¯è¯´NVè¿™è¾¹çš„selective activationçš„è¯ï¼Œå®ƒä¸»è¦çš„åŒºåˆ«å°±åœ¨äºæ˜¯è¯´ã€‚æˆ‘ä¸æ˜¯æŠŠå°±æ˜¯ä¸æ˜¯è¿™æ ·éå¸¸ç²—æš´çš„ã€‚

å°±æ˜¯æˆ‘æˆ‘æˆ‘å°±å‘ƒä¿ç•™ä¸€éƒ¨åˆ†å°±æ˜¯æŸä¸€äº› layerçš„ä¸€ä¸€ç»„ layerçš„è¿™æ ·çš„ä¸€ä¸ªè¾“å…¥ã€‚è€Œæ˜¯å‘¢çœ‹ä¸€çœ‹ï¼Œå°±æ˜¯æˆ‘æ•´ä¸ªçš„è¿™æ ·çš„ä¸€ä¸ªç‰¹åˆ« transformformerè¿™æ ·çš„ä¸€ä¸ªå‰é¡¹åé¡¹çš„è®¡ç®—è¿‡ç¨‹ä¸­çš„è¯ã€‚

æœ‰å“ªä¸€äº› layeræˆ–è€…æ“ä½œçš„è¯ï¼Œå®ƒå¯¹äºæ˜¾å­˜çš„å ç”¨æ¯”è¾ƒé«˜ã€‚ä½†æ˜¯å®ƒå¯¹äºè®¡ç®—è¿™å—çš„éœ€æ±‚æ¯”è¾ƒå°‘ã€‚å‘ƒï¼Œç„¶åå‘¢å°±æ˜¯å‘ƒæ¢å¥è¯è¯´å°±æ˜¯é€‰æ‹©æŸä¸€äº›ç‰¹å®šçš„è¿™æ ·çš„ä¸€äº›å±‚æˆ–è€…æ“ä½œå»åšè¿™æ ·çš„ä¸€ä¸ªå‘ƒactivationçš„è¿™æ ·çš„ä¸€ä¸ªäº‹æƒ…å•Šã€‚

è¿™æ ·å­çš„è¯çš„å¥½å¤„æ˜¯è¯´æˆ‘è®¡çœäº†ä¸€äº›æ˜¾å­˜ã€‚ä½†æ˜¯å¯¹äºè¿™ç§é¢å¤–çš„è®¡ç®—çš„å¼€é”€å•Šå°±æ¯”è¾ƒå°‘ã€‚é‚£å°±æ˜¯å‘ƒç»†èŠ‚æˆ‘å°±ä¸è®²äº†æœ€åè¾¾åˆ°çš„ä¸€ä¸ªæ•ˆæœæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯å¤§æ¦‚å°±æ˜¯æˆ‘èƒ½çœå°±æ˜¯activationå•Šã€‚

å°±æ˜¯èƒ½çœä¸€å¤šåŠçš„è¿™æ ·çš„ä¸€ä¸ªactivationçš„è¿™æ ·çš„ä¸€ä¸ªæ˜¾å­˜çš„å ç”¨ã€‚ä½†æ˜¯å‘¢å¯¹äºè®¡ç®—çš„å‘ƒé¢å¤–çš„å¼€é”€çš„è¯ï¼Œå¤§æ¦‚åªæœ‰12è¿™æ ·çš„ä¸€ä¸ªé‡çº§ã€‚æ‰€ä»¥å‘¢å°±æ˜¯è¯´åœ¨è¿™ä¸ªåœ°æ–¹ç¨å¾®é‡å¤å¼ºè°ƒä¸€ä¸‹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ

å°±æ˜¯NVè¿™è¾¹çš„è¿™æ ·çš„ä¸€ä¸ªstandæˆ–è€…åšè¿™äº›å·¥ä½œçš„ä¸€ä¸ªæ ¸å¿ƒçš„è¿™æ ·çš„ä¸€ä¸ªç‚¹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯è¯´æˆ‘è¦å»å°½é‡çš„é™ä½æˆ‘æ•´ä½“çš„è¿™æ ·ä¸€ä¸ªæ•ˆç‡ï¼Œå°½é‡å‡å°‘å¯¹æ˜¾å­˜å ç”¨ã€‚

ä½†æ˜¯ä¸€ä¸ªå‰ææ˜¯å°½é‡çš„ä¸å‡å°‘æ•´ä¸ªè®¡ç®—ç³»ç»Ÿçš„ä¸€ä¸ª efficiencyã€‚å—¯ï¼ŒOKé‚£ä¸ªæœ€åçš„è¯å°±æ˜¯ç¨å¾®åšä¸€ä¸ªå°çš„å¹¿å‘Šï¼Œå°±æ˜¯æˆ‘ä»¬è¿™è¾¹åœ¨æ‹›äººã€‚ç„¶åå¤§å®¶çš„è¯å¦‚æœæœ‰æ„Ÿå…´è¶£çš„ï¼Œå°±æ˜¯è¯´å¯ä»¥ç›´æ¥è”ç³»æˆ‘ä»¬çš„é‚£ä¸ªå‘ƒé‚£ä¸ªHRã€‚

ç„¶åå‘¢ï¼Œå°±æ˜¯å‘ƒæŠ¥ä¸€ä¸‹æˆ‘åå­—ï¼Œåº”è¯¥èƒ½æ‰¾åˆ°æˆ‘è¿™è¾¹ã€‚å¥½ï¼Œè°¢è°¢å¤§å®¶ã€‚ğŸ˜Šï¼Œå‘ƒã€‚I will comment the the early his beginning said his internship inI will comment that in the the the last session of thisOå‘ƒå“å‘ƒé‚£æˆ‘ä»¬ä»Šå¤©å‘ƒè¯·æ¥å‘ƒä¸‹ä¸€ä½æ¼”è®²è€…ä¸‹ä¸€ä½æ¼”è®²è€…æ˜¯æ¥è‡ªå‘ƒä¸­å›½ç”µä¿¡çš„æç»ææ€»ã€‚

è°¢è°¢ã€‚ğŸ˜Šï¼Œå—¯ï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯æœ€åä¸€ä½ã€‚å¥½æ¶ˆæ¯å‘¢ï¼Œæ˜¯ç­‰æˆ‘è®²å®Œäº†ä¹‹åï¼Œåˆé¥­å°±æ¥äº†ã€‚ä½†æ˜¯ä¸å¥½çš„æ¶ˆæ¯å‘¢æ˜¯å¤§å®¶å¿…é¡»è¦å¬æˆ‘è®²å®Œï¼Œåˆé¥­æ‰èƒ½æ¥ã€‚ğŸ˜Šï¼Œé‚£ä¸ªæˆ‘å‘¢æ˜¯åœ¨ä¸­å›½ç”µä¿¡å‘¢ä¸€ç›´ä»äº‹äº‘è®¡ç®—å’Œè®¡ç®—å¹³å°ã€‚å°±æ˜¯æ¢å¥è¯è¯´å‘¢ã€‚

å°±æ˜¯å’±ä»¬å¤§æ¨¡å‹çš„åº•å±‚çš„è®¡ç®—å¹³å°çš„æ„å»ºçš„å·¥ä½œäº†ã€‚ä»Šå¤©å‘¢ä¹Ÿæ˜¯æƒ³è·Ÿå¤§å®¶åœ¨è¿™ä¸ªåœºåˆå‘¢è¦èŠä¸€èŠï¼Œå°±æ˜¯åŸºäºå¤§æ¨¡å‹è®­ç»ƒçš„è¿™ç§é«˜å¹¶å‘æ–‡ä»¶çš„ä¸€ä¸ªè®¿é—®çš„æ·±åº¦çš„ä¸€ä¸ªä¼˜åŒ–ã€‚å› ä¸ºä»Šå¤©å’±ä»¬åœ¨ä¼—å¤šçš„æ¼”è®²è€…ä¸­å‘¢ã€‚

å…¶å®æœ‰ä¸å°‘åŒäº‹éƒ½è¦æå‡ºæ¥äº†ï¼Œæ˜¯è¯´åœ¨æˆ‘ä»¬å¦‚ä½•åœ¨ç®—å­æ–¹é¢ï¼Œåœ¨GPUæ–¹é¢ï¼ŒåŒ…æ‹¬åœ¨æ¨¡å‹æ–¹é¢ï¼Œæˆ‘ä»¬ä¸æ–­çš„å»è¿›è¡Œä¼˜åŒ–ã€‚å…¶å®æœ€ç»ˆçš„ä¸€ä¸ªç›®çš„å‘¢æ˜¯è¯´åœ¨æ¨¡å‹è¶Šæ¥è¶Šå¤§çš„æƒ…å†µä¸‹ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•èƒ½å¤Ÿé«˜æ•ˆçš„æ¥æå‡æˆ‘ä»¬çš„æ•´ä¸ªæ¨¡å‹çš„æ•ˆç‡è¾¾åˆ°ä¸€ä¸ªæ›´å¥½çš„æ•ˆæœã€‚å‘ƒï¼Œè¿™ä¸ªå°±ä¸ç»†è¯´äº†ï¼Œå› ä¸ºæ˜¯æ•´ä¸ªå¤§æ¨¡å‹çš„å‘å±•è¶‹åŠ¿å•Šç­‰ç­‰ã€‚å…¶å®å¤§å®¶éƒ½è¯´çš„å¾ˆå¤šäº†ã€‚ä½†æ˜¯æˆ‘åœ¨è¿™é‡Œé¢å…¶å®æƒ³ä»å¦å¤–ä¸€ä¸ªç»´åº¦æ¥è¯´çš„æ˜¯è¯´ã€‚

å°±æ˜¯æ— è®ºæ˜¯è¯´æˆ‘ä»¬ç°åœ¨ä»é€šç”¨å¤§æ¨¡å‹ï¼Œç„¶ååˆ°å¾€åé€šè¿‡æˆ‘ä»¬å¯¹ä¸€äº›ç‰¹å®šåœºæ™¯ï¼ŒåŒ…æ‹¬å¯¹æ•°æ®çš„ä¸€äº›æŠ½è±¡æ¥æ„å»ºã€‚é¢å¯¹æŸä¸€äº›å…·ä½“è¡Œä¸šçš„è¿™ç§è¡Œä¸šæ¨¡å‹ï¼Œä¹ƒè‡³äºåˆ°è¿™ç§ç‰¹å®šçš„ä¸€äº›åœºæ™¯çš„æ¨¡å‹ã€‚

ä¹ƒè‡³äºè¯´æˆ‘ä»¬ä»æœ€æ—©æœŸçš„è¿™ç§å•æ¨¡æ€çš„æ¨¡å‹ï¼Œä¸€ç›´åˆ°ç°åœ¨éšç€å®¢æˆ·çš„éœ€æ±‚è¿›å…¥äº†å¤šåœºæ™¯ï¼Œå¤šæ–‡ä»¶çš„å¼•å…¥ä»¥åï¼Œå½¢æˆè¿™ç§å¤šæ¨¡æ€çš„æ¨¡å‹ä¹‹åï¼Œå…¶å®æˆ‘ä»¬å‘ç°åœ¨æœ€åº•å±‚å‘¢ï¼Œæˆ‘ä»¬èƒ½å¤Ÿçœ‹åˆ°çš„æ˜¯æ¨¡å‹å‘å±•çš„è¶‹åŠ¿å‘¢ã€‚

é‚£å°±æ˜¯å®ƒçš„å‚æ•°çš„é‡çº§ä¼šæˆå‡ ä½•çº§æ•°çš„å¢é•¿ã€‚ä¸¾äº†ä¸€ä¸ªä¾‹å­ï¼Œåœ¨å›¾çš„æœ€æœ€å³ä¸‹è§’å§ï¼Œæœ€å·¦ä¸‹è§’å°±æ˜¯é‚£ç§å¤§æ¨¡å‹ä»å°±æ˜¯æˆ‘ä»¬æ‹¿GPTæ¥ä¸¾ä¸ªä¾‹å­ï¼Œå¯ä»¥çœ‹åˆ°å®ƒçš„å‚æ•°é‡çº§å‘¢ï¼Œä»GPTRä¸€ç›´åˆ°æœªæ¥çš„GPT5ã€‚

çš„æ—¶å€™æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒçš„å‚æ•°é‡çº§æ˜¯ä¹˜å‡ ä½•çº§æ•°åœ¨å¾€ä¸Šå¢é•¿çš„ã€‚é‚£ä¹ˆåœ¨å‡ ä½•çº§æ•°å¾€ä¸Šå¢é•¿çš„å‚æ•°é‡çº§ä¹‹ä¸‹å‘¢ï¼Œæˆ‘ä»¬çœ‹çœ‹ä¼šå‘ç”Ÿä¸€äº›ä»€ä¹ˆæ ·çš„ä¸€äº›å˜åŒ–å’Œå¯¹åº•å±‚çš„å­˜å‚¨ç­‰ç­‰è¿™æ–¹é¢ä¼šäº§ç”Ÿä¸€äº›ä»€ä¹ˆæ ·çš„éœ€æ±‚ã€‚é¦–å…ˆå‘¢å‚æ•°é‡çº§å¢é•¿ä¹‹åã€‚

æˆ‘ä»¬ç¬¬ä¸€ä¸ªèƒ½å¤Ÿçœ‹è§çš„æ˜¯å®ƒå¯¹å‚æ•°é‡çº§çš„å¢é•¿å‘¢ã€‚é‚£é¦–å…ˆå®ƒæ˜¯å¯¹æˆ‘ä»¬è®­ç»ƒæ•°æ®çš„éœ€æ±‚é‡å°±ä¼šè¶Šæ¥è¶Šå¤§ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»å‡ åä¸ªGä¸€ç›´åˆ°å‡ ç™¾ä¸ªå‡ ç™¾ä¸ªTï¼Œä¹ƒè‡³äºå‡ åƒä¸ªPï¼Œç±»ä¼¼äºåƒè¿™æ ·çš„ä¸€ä¸ªä¸€ä¸ªé‡çº§ã€‚

å¹¶ä¸”å‘¢éšç€å¤šæ¨¡æ€çš„éœ€æ±‚å‡ºç°ä¹‹åå‘¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‘¢ï¼Œå°±æ˜¯è¯´åŸæ¥æˆ‘ä»¬éƒ½æ˜¯ä»å•ä¸€çš„æ–‡ä»¶ã€‚åŒ…æ‹¬è¿™ç§å°æ–‡ä»¶ï¼Œä»¥åŠå‘¢æ˜¯ç±»ä¼¼äºåƒè¿™ç§é‚£ç§ç»“æ„åŒ–çš„æ•°æ®ï¼Œç„¶åé€æ¸å‘¢åƒç±»ä¼¼äºè¿™ç§å¤šæ¨¡æ€çš„ã€‚

ç„¶åéç»“æ„åŒ–çš„æ•°æ®çš„éœ€æ±‚é‡ä¼šè¶Šæ¥è¶Šå¤§ã€‚é‚£ä¹ˆå†å¾€ä¸‹å¯¹åº•å±‚çš„å½±å“ï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°äº†ã€‚é¦–å…ˆå‘¢æ˜¯åŸæ¥æˆ‘ä»¬ã€‚å¯¹åº•å±‚å‘¢ç”¨romæˆ–è€…æœ¬åœ°ç£ç›˜è¿›è¡Œå°±èƒ½å¤Ÿè¿›è¡Œå­˜å‚¨ã€‚ä½†æ˜¯éšç€æ¨¡å‹çš„å‚æ•°é‡å‘¢è¶Šæ¥è¶Šå¢å¤§å‘¢ï¼Œé‚£ä¹ˆã€‚æœ¬åœ°çš„ç£ç›˜ã€‚

è¿˜æœ‰romå‘¢ï¼Œå…¶å®æ°¸è¿œæ˜¯å·²ç»ä¸èƒ½å¤Ÿæ”¯æ’‘å®ƒäº†ã€‚é‚£ä¹ˆä»å­˜å‚¨çš„é‡çº§ä¸Šé¢ï¼Œæˆ‘ä»¬ä»Tå¼€å§‹ï¼Œå¯èƒ½åé¢ä¸€ç›´è¦åˆ°ã€‚å‡ åPä¸Šç™¾Pã€‚é‚£ä¹ˆæˆ‘ä»¬å¯¹äºè¿™ç§å­˜å‚¨çš„ä»‹è´¨å¯¹å®ƒçš„è¦æ±‚å°±ä¼šè¶Šæ¥è¶Šé«˜ã€‚å¹¶ä¸”å‘¢å¯¹å®ƒçš„å¸¦å®½ï¼Œä»¥åŠå‘¢å¯¹å®ƒçš„è®¿é—®ã€‚

æˆ‘ä»¬å¸¸è¯´çš„IOè¿™æ–¹é¢çš„è¦æ±‚å‘¢ä¹Ÿä¼šè¶Šæ¥è¶Šé«˜ã€‚é‚£ä¹ˆåœ¨è¿™äº›å˜åŒ–ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°å®ƒçš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¬å¤§æ¦‚æ€»ç»“å½’çº³äº†ä¸€ä¸‹ï¼Œçœ‹çœ‹æœ‰å‡ ç§æŒ‘æˆ˜ã€‚ä¸€ä¸ªå‘¢å°±æ˜¯å¯¹å¤§å®¹é‡å­˜å‚¨çš„æŒ‘æˆ˜ã€‚å¤§å®¹é‡å­˜å‚¨çš„æŒ‘æˆ˜å‘¢ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥çœ‹åˆ°å‘¢å°±æ˜¯ä»å•æ¨¡æ€èµ°å‘å¤šæ¨¡æ€ä¹‹åï¼Œå¯¹å­˜å‚¨å®¹é‡å‘¢ï¼Œå®ƒå¸¦åŠ¨çš„å°±æ˜¯æˆ‘ä»¬çš„å­˜å‚¨å¯èƒ½å·²ç»ä»æœ¬åœ°çš„æœåŠ¡å™¨å¼€å§‹è¦é€æ¸é€æ¸çš„èµ°å‘ä¸“ä¸šçš„è¿™ç§å¤–éƒ¨å…±äº«çš„ä¸€ä¸ªå­˜å‚¨ã€‚å¹¶ä¸”å‘¢è¿™ç§å­˜å‚¨ä¸€å®šæ˜¯è¦èƒ½å¤Ÿçµæ´»çš„å»æ‰©å±•çš„ã€‚

ä¸€å®šæ˜¯èƒ½å¤Ÿçµæ´»çš„å»æ‰©å±•çš„è¿™ç§æ–¹å¼ã€‚ç¬¬äºŒä¸ªå‘¢å°±æ˜¯é«˜å¸¦å®½å’Œè¿™ç§é«˜IOPOSçš„è¿™ç§IOPSè¿™ç§æŒ‘æˆ˜ã€‚å› ä¸ºåœ¨å¤§æ¨¡å‹çš„è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦æ›´åŠ å¿«é€Ÿçš„åŠ è½½å»ç§æµ·é‡çš„å°ã€‚æ–‡ä»¶çš„è®­ç»ƒæ•°æ®ã€‚

åŒæ—¶å‘¢æˆ‘ä»¬è¦å› ä¸ºGPUå¡éƒ½å¾ˆè´µå˜›ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦é™ä½æˆæœ¬ï¼Œé‚£ä¹ˆè¦å°½é‡çš„å»å‡å°‘è¿™ç§GPUå¡çš„è¿™ç§ç©ºè½½çš„ç­‰å¾…çš„æ—¶é—´å»æå‡è¿™ç§ç®—åŠ›çš„åˆ©ç”¨ç‡ã€‚åŒ…æ‹¬æˆ‘ä»¬åœ¨çœ‹å®ƒåœ¨ä¸­é—´æ€çš„æ•°æ®çš„æ—¶å€™ã€‚

ä»–éœ€è¦å¿«é€Ÿçš„å»æŠŠè¿™ç§ä¸­é—´æ€çš„æ•°æ®å»å¾€å­˜å‚¨é‡Œé¢å»å†™ã€‚é‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€æ–¹é¢å‘¢å¯¹å¸¦å®½çš„éœ€æ±‚ï¼Œè¿˜æœ‰å¯¹é‚£ç§é«˜é€Ÿå¹¶è¡Œçš„è¯»å†™çš„éœ€æ±‚çš„æŒ‘æˆ˜ä¼šè¶Šæ¥è¶Šå¤§ã€‚æœ€åå‘¢å°±æ˜¯è¿˜æœ‰è¿™ç§å¯é æ€§ã€‚

å› ä¸ºå‰é¢ä¹Ÿæåˆ°äº†å­˜å‚¨ä»‹è´¨çš„å®¹é‡è¶Šæ¥è¶Šå¤§ã€‚é‚£ä¹ˆåœ¨æ•´ä¸ªæ¨¡å‹é‡Œé¢ã€‚å­˜å‚¨æ¥åˆ¶çš„å®¹é‡å¤§äº†ä»¥åï¼Œåƒè¿™æ ·çš„å­˜å‚¨ï¼Œå®ƒçš„å¯é æ€§ï¼Œå…¶å®ç›´æ¥å†³å®šäº†æˆ‘ä»¬åœ¨æ¨¡å‹åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå®ƒçš„ä¸€ä¸ªæ•ˆç‡å’Œä¸€ä¸ªå®ƒçš„å¯æŒç»­æ€§ã€‚

å› ä¸ºä¸€å—ç›˜å‡ºå­˜å‚¨çš„é‡å¤§äº†ä»¥åï¼Œä¸€å—ç›˜å‡ºäº†é—®é¢˜ä»¥åï¼Œå®ƒå¯èƒ½ä¼šå¯¹æ•´ä¸ªçš„æ•°æ®çš„å½±å“ï¼Œå°±æ¯”ä»¥å‰é‚£ç§å°å­˜å‚¨çš„å½±å“ä¼šå¤§å¾ˆå¤šã€‚é‚£ä¹ˆæˆ‘ä»¬çœ‹åˆ°è¿™å‡ ä¸ªæŒ‘æˆ˜ä¹‹åå‘¢ï¼Œæˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘ä»¬åœ¨å®è·µçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ˜¯å¦‚ä½•å»åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„ã€‚

å‘ƒï¼Œä¸­å›½ç”µä¿¡å‘¢æ˜¯æˆ‘ä»¬ä¸€ç›´åœ¨è‡´åŠ›äºè¿™ç§é¢å‘å¤§æ¨¡å‹è®­ç»ƒçš„åº•å±‚çš„æ”¯ç®—å¹³å°çš„æ­å»ºã€‚åœ¨è¿™ç§æ­å»ºçš„è¿‡ç¨‹ä¸­å‘¢ï¼Œå°±æ˜¯æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªæŠ€æœ¯çš„ä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªä¸€ä¸ªæ¡†æ¶å›¾ã€‚åœ¨è¿™ä¸ªæ¡†æ¶å›¾é‡Œé¢ã€‚

æˆ‘ä»¬çœ‹åˆ°å°±æ˜¯æˆ‘ä»¬ä»å‡ ä¸ªæ–¹é¢å»é‡‡ç”¨äº†ä¸€äº›å®è·µä¸­çš„ä¸€äº›æªæ–½å‘¢æ¥åº”å¯¹è¿™æ–¹é¢çš„æŒ‘æˆ˜ã€‚é¦–å…ˆå‘¢æ˜¯åº”å¯¹è¿™ç§å¤§å®¹é‡çš„å­˜å‚¨çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚é‚£ä¹ˆæ˜¾ç„¶å¤§å®¹é‡å­˜å‚¨ï¼Œæˆ‘ä»¬åº”å¯¹å®ƒçš„æœ€å¥½çš„æ–¹å¼å‘¢ï¼Œé‚£å°±æ˜¯ã€‚è¿™ç§åˆ†å¸ƒå¼çš„æ¶æ„ã€‚

å› ä¸ºæˆ‘ä»¬é€šè¿‡åˆ†å¸ƒå¼çš„æ¶æ„ï¼Œå¯ä»¥æŠŠåŸæ•°æ®å‘¢å¾€åœ¨æˆ‘ä»¬çš„æ¶æ„é‡Œé¢çš„å¤šä¸ªèŠ‚ç‚¹é‡Œé¢å»å­˜å‚¨ï¼Œè€Œä¸”å‘¢å¯ä»¥é€šè¿‡ç±»ä¼¼äºè¿™ç§é‚£ä¸ªã€‚ç§¯æœ¨å¼çš„è¿™ç§æ‰©å®¹çš„æ–¹å¼ã€‚

å°±ing blockè¿™ç§æ–¹å¼å‘¢æ¥è¿›è¡Œçµæ´»çš„ä¸€ä¸ªä¸€ä¸ªç§¯æœ¨å †å å¼çš„ä¸€ä¸ªä¸€ç§æ‰©å®¹ã€‚è¿™æ ·çš„æˆæœ¬å‘¢å°±æœ‰ä¸€ä¸ªæå¤§çš„ä¸€ä¸ªé™ä½ã€‚ç›®å‰å‘¢å¯ä»¥åšåˆ°çš„æ˜¯èƒ½å¤Ÿè¾¾åˆ°äº†å®¹é‡ä¸Šè¾¾åˆ°1ä¸ª10Pè¿™ä¹ˆä¸€ä¸ªä¸€ä¸ªé‡çº§ã€‚ç¬¬äºŒå‘¢é‚£å°±æ˜¯é’ˆå¯¹é«˜æ€§èƒ½ã€‚

è¿˜æœ‰é«˜å¹¶å‘æ€§çš„è¿™æ–¹é¢çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é‡Œé¢æˆ‘ä»¬é€šå¸¸å‘¢æ˜¯é€šè¿‡å‡ ä¸ªæ–¹å¼æ¥å»åšå‡ ç§æªæ–½å§ï¼Œæ¥å»æ¥å»æ¥å»è§£å†³ç›®å‰å­˜åœ¨çš„é—®é¢˜ã€‚ä¸€ä¸ªå‘¢å°±æ˜¯é‚£ä¸ªGDSï¼Œè¿˜æœ‰å‘¢å°±æ˜¯åŒ…æ‹¬æ™ºèƒ½çš„é¢„åŠ é€Ÿï¼Œè¿˜æœ‰é‚£ä¸ªç±»ä¼¼äºå®¢æˆ·ç«¯çš„åŠ é€Ÿã€‚

è¿™ä¸ªå‘¢åœ¨åé¢çš„ç¯‡å¹…é‡Œé¢ï¼Œæˆ‘è¿˜ä¼šå»è¯´ï¼Œåœ¨è¿™é‡Œé¢æ˜¯æƒ³è¯´çš„å‘¢æ˜¯å°±æ˜¯åœ¨ç½‘ç»œå±‚é¢ä¸Šé¢åœ¨æ„å»ºè¿™ç§ç®—åŠ›å¹³å°çš„æ—¶å€™ï¼Œæˆ‘ä»¬åŒæ—¶èƒ½å¤Ÿæ”¯æŒç›®å‰èƒ½å¤Ÿæ”¯æŒçš„æ˜¯IBç½‘ç»œå’Œroyçš„ç½‘ç»œã€‚æˆ‘ä»¬é€šè¿‡è¿™ç§é«˜é€Ÿå¸¦å®½çš„ç½‘ç»œå‘¢ï¼ŒæŠŠè¿™æ–¹é¢ã€‚

å› ä¸ºç½‘ç»œå¸¦å®½å¯¼è‡´çš„ã€‚ç“¶é™çš„å»æå¤§çš„ç»™å®ƒå»é™ä½ã€‚åŒæ—¶å‘¢å°±æ˜¯åœ¨æœ€åº•å±‚çš„æ—¶å€™ï¼Œåœ¨å­˜å‚¨ä»‹è´¨ä¸Šï¼Œæˆ‘ä»¬ç°åœ¨å¹¿æ³›çš„åœ¨ä½¿ç”¨åŸºäºè¿™ç§SSDçš„å…¨é—ªçš„å­˜å‚¨çš„æ–¹å¼ã€‚å½“ç„¶äº†ï¼Œå¤§å®¶ä¹ŸçŸ¥é“å°±æ˜¯è¿™ç§æ–¹å¼çš„æˆæœ¬ä¼šæ¯”è¾ƒé«˜ã€‚

ç›¸ä¿¡åœ¨æœªæ¥çš„ä¸€æ®µæ—¶é—´ï¼Œéšç€æˆæœ¬çš„è¿™æ–¹é¢çš„é™ä½å‘¢ï¼Œæˆ‘ä»¬ä¸ºè¿™ç§å¤§æ¨¡å‹çš„è®­ç»ƒï¼Œå®ƒæ‰€æä¾›çš„è¿™ç§åº•å±‚çš„æ¶æ„çš„æ—¶å€™ï¼Œé‚£æˆæœ¬ä¹Ÿä¼šå¤§å¹…çš„å¾€ä¸‹å»é™ã€‚é‚£ä¹ˆåœ¨åº”å¯¹è¿™ç§é«˜å¯é æ€§çš„æŒ‘æˆ˜æ–¹é¢ã€‚

HAçš„æ–¹å¼å…¶å®æ˜¯ä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒç›®å‰çœ‹å‘¢æ˜¯æ¯”è¾ƒæˆç†Ÿçš„è¿™ç§è¿™ç§æ–¹å¼ã€‚åŒæ—¶å‘¢æˆ‘ä»¬ä¹Ÿé€šè¿‡è¿™ç§ç±»ä¼¼äºåŠ¨æ€çš„çº ç¼ ç å’Œç¼©åˆ—é‡æ„çš„è¿™ç§æ–¹å¼æ¥ä¿è¯å®ƒçš„æ•°æ®çš„ä¸€ä¸ªå¯é æ€§å’Œæ•°æ®åœ¨ä¸¢å¤±äº†ä¹‹åçš„ä¸€ä¸ªä¸€ä¸ªé‡æ„ã€‚

åŒ…æ‹¬åƒç¡¬ç›˜çš„äºšå¥åº·çš„ç®¡ç†ã€‚è¿™ä¸ªåé¢å‘¢ä¹Ÿä¼šæœ‰ä¸€ä¸ªè¯¦ç»†çš„ä»‹ç»ã€‚é‚£é™¤äº†åˆšæ‰è¯´çš„å°±æ˜¯åœ¨ç¡¬ä»¶ä¸Šé¢ï¼Œè¿˜æœ‰å¸¦å®½ä¸Šé¢çš„ä¸€äº›åº”å¯¹æ€§çš„åº”å¯¹æ€§çš„æªæ–½å’Œå®è·µä¹‹å¤–å‘¢ï¼Œé‚£ä¹ˆè¯´ä¸€ä¸‹å‘¢ï¼Œå°±æ˜¯ä»è½¯ä»¶å±‚é¢åˆšæ‰ä¹Ÿè¯´äº†ï¼Œåœ¨è¿™ä¸ªåœ¨è¿™å—å‘¢ã€‚

æˆ‘ä»¬è¦æƒ³è®²çš„ä¸€ä¸‹å‘¢ï¼Œå°±æ˜¯è¯´ä¸­é—´çº¿ç°åœ¨æˆ‘ä»¬ä¹Ÿè¿›è¡Œå†è¿›è¡Œå¼€å‘å‘¢ï¼Œæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå°±æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„é¢„åŠ è½½çš„ä¸€ä¸ªæ–¹å¼ã€‚æˆ‘ä»¬é€šè¿‡æ™ºèƒ½é¢„åŠ è½½å’Œç¼“å­˜åŠ é€Ÿè¿™ç§ç»“åˆçš„æ–¹å¼å‘¢ã€‚

å¸Œæœ›èƒ½å¤ŸåŠ é€Ÿçš„æ˜¯é€šè¿‡åŠ é€Ÿçš„æ˜¯å®ƒçš„æ•°æ®çš„ä¸€ä¸ªæå‰çš„ä¸€ä¸ªå­˜å–ä¸€ä¸ªä¸€ä¸ªè¯»å–ã€‚å› ä¸ºæˆ‘ä»¬ä¹ŸçŸ¥é“å°±æ˜¯ã€‚å°±æ˜¯å°±æ˜¯æˆ‘ä»¬æƒ³é€šè¿‡è¿™ç§é¢„åŠ è½½çš„æ–¹å¼èƒ½å¤Ÿåšåˆ°ä»€ä¹ˆå‘¢ï¼Ÿæ˜¯è¯´ã€‚æˆ‘ä»¬é€šè¿‡å»è§‚å¯Ÿå’Œå»å­¦ä¹ åœ¨å¤§æ¨¡å‹ï¼Œå®ƒå»è¯»å–æ•°æ®è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚

å®ƒçš„è¡Œä¸ºæ–¹å¼ï¼ŒåŒ…æ‹¬ä»–å“ªäº›æ•°æ®çš„ç»å¸¸è¯»å–çš„ä¸€ä¸ªé¢‘ç¹åº¦ã€‚æˆ‘ä»¬é€šè¿‡è¿™ç§æ–¹å¼å‘¢æ¥è¿›è¡Œä¸‹ä¸€æ­¥çš„é¢„æµ‹ã€‚æˆ‘ä»¬å»é¢„æµ‹å“ªäº›æ•°æ®ï¼Œå®ƒèƒ½å¤Ÿä¼šç›¸å¯¹æ¯”è¾ƒçƒ­é—¨ï¼Œåœ¨ä¸‹ä¸€æ­¥çš„è®­ç»ƒä¸­ï¼Œå®ƒèƒ½å¤Ÿä¼šå»è¯»å–ã€‚é‚£ä¹ˆé€šè¿‡è¿™ç§å­¦ä¹ ä¹‹åã€‚

æˆ‘ä»¬æŠŠå¤§é‡çš„è¿™æ–¹é¢é¢„æµ‹çš„æ•°æ®ä¼šæå‰æŠ“åˆ°çš„ç³»ç»Ÿçš„ç¼“å­˜é‡Œé¢å»ã€‚æŠ“åˆ°äº†ç¼“å­˜é‡Œé¢å»ä¹‹åå‘¢ï¼Œå°±æ˜¯é€šè¿‡é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ˜¾ç„¶å‘¢ä½ åœ¨æ¨¡å‹æœ‰éœ€æ±‚çš„æ—¶å€™ï¼Œä»–å‘èµ·äº†å‘èµ·éœ€æ±‚ï¼Œå®ƒç›´æ¥å»å‘å­˜å‚¨ä»‹è´¨å»è¯»çš„æ—¶å€™ï¼Œè¿™ä¸ªé‡Œé¢ä¼šäº§ç”Ÿã€‚

åœ¨è¿™ç§è¿™ç§æ–¹å¼ä¸‹é¢ä¼šäº§ç”Ÿä¸€ä¸ªå»¶è¿Ÿã€‚é‚£ä¹ˆè¿™ç§å»¶è¿Ÿåœ¨æ•°æ®é‡å¤§çš„æ—¶å€™ï¼Œå…¶å®è¿™ç§å»¶è¿Ÿåœ¨æ•´ä¸ªç³»ç»Ÿé‡Œé¢çš„å¼€é”€çš„æ—¶é—´ï¼Œå…¶å®è¿™æ˜¯éœ€è¦æ ¼å¤–å…³æ³¨çš„ã€‚é‚£ä¹ˆé€šè¿‡æå‰è¯»å–åˆ°ç¼“å­˜é‡Œé¢å»ä¹‹åå‘¢ã€‚

å…¶å®æŸç§ç¨‹åº¦ä¸Šèƒ½å¤ŸæŠŠè¿™ç§å»¶è¿Ÿæå¤§çš„å»å‹ç¼©ã€‚é€šè¿‡å‹ç¼©ä¹‹åï¼Œå…¶å®å˜ç›¸çš„ä¹Ÿæ˜¯æå‡äº†æˆ‘ä»¬çš„è®­ç»ƒçš„ä¸€ä¸ªä¸€ä¸ªæ•ˆç‡ã€‚å¦å¤–å‘¢å°±æ˜¯åœ¨GTSæ–¹é¢ï¼Œå°±æ˜¯å¤§å®¶çœ‹è¿™å¼ å›¾ï¼Œå‘ƒï¼Œåœ¨ä¼ ç»Ÿçš„è¯»å–æ–¹é¢å‘¢ã€‚

å°±æ˜¯å½“GPUè¦å»è¯»å–å­˜å‚¨çš„æ•°æ®å¼€å±•è®­ç»ƒçš„æ—¶å€™ï¼Œé€šå¸¸çš„CPUæ˜¯è¦å‚ä¸åˆ°è¿™é‡Œé¢çš„å·¥ä½œçš„ã€‚CPUè¦è´Ÿè´£çš„æ˜¯åœ¨æ•°æ®çš„å¤„ç†å’Œæ•°æ®çš„ä¼ è¾“è¿‡ç¨‹ä¸­ï¼Œå®ƒéƒ½è¦åˆ†é…èµ„æºï¼Œå®ƒè¿›è¡Œæ§åˆ¶ã€‚é‚£ä¹ˆç°åœ¨å‘¢åƒè‹±ä¼Ÿè¾¾å‘¢ã€‚

ç°åœ¨å®ƒæ˜¯æŠŠè¿™æ–¹é¢çš„åŠŸèƒ½å‘¢ï¼Œå®ƒæ˜¯åšåˆ°äº†GPUçš„åº•å±‚çš„èŠ¯ç‰‡é‡Œé¢å»ã€‚æ¢å¥è¯è¯´å‘¢ï¼Œå°±æ˜¯GPUå‘¢ï¼Œå®ƒä¸é€šè¿‡è¦è¯»å–æ•°æ®çš„æ—¶å€™å®ƒä¸é€šè¿‡å®ƒä¸éœ€è¦é€šè¿‡CPUçš„å¹²é¢„æ¥è¿›è¡Œè¯»å–ã€‚

é‚£ä¹ˆæˆ‘ä»¬åœ¨å¼€å‘ä¸­å›½ç”µä¿¡çš„HPFSå°±æ˜¯é«˜æ€§èƒ½çš„æ–‡ä»¶å­˜å‚¨ç³»ç»Ÿçš„æ—¶å€™å‘¢ï¼Œå…¨é¢çš„å»é€‚é…äº†è¿™ç§GTSçš„èƒ½åŠ›ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿæ˜¯åœ¨HPFSçš„å®¢æˆ·ç«¯ã€‚ç„¶åé€šè¿‡GDSçš„é‚£ä¸ªé…·fiçš„åº“å‘¢ï¼Œç„¶åå®ç°äº†å¯¹ã€‚å¯¹ã€‚

è¯·æ±‚ç«¯å‘¢å¯¹GDSçš„ä¸€ä¸ªæ”¯æŒï¼ŒåƒGPUç›´æ¥å»å‘å–å‘å‡ºé‚£ä¸ªè°ƒç”¨çš„æŒ‡ä»¤ã€‚ç„¶ååœ¨æœåŠ¡å™¨ç«¯å‘¢ï¼Œæˆ‘ä»¬é€šè¿‡è¿™ç§RDMAçš„æ–¹å¼å‘¢ï¼Œç„¶åé€šè¿‡å®ƒå‘¢æ˜¯å®ç°äº†å¯¹GDSçš„æ”¯æŒä¹‹åå‘¢ï¼Œå»ç›´æ¥å½¢æˆäº†é‚£ä¸ªæ•°æ®çš„ä¸€ä¸ªè¯»å–ã€‚

é€šè¿‡è¿™ç§æ–¹å¼å‘¢ï¼Œå…¶å®å˜ç›¸çš„ä¹Ÿæ˜¯èƒ½å¤Ÿå¢åŠ äº†ä¸€æ–¹é¢å‘¢æ•°æ®çš„å¸¦å®½ã€‚å¦å¤–ä¸€æ–¹é¢å‘¢é™ä½äº†å®ƒæ•°æ®è¯»å–çš„æ—¶å»¶ï¼ŒåŒæ—¶è¿˜æœ‰ä¸€ç‚¹å¥½å¤„å‘¢ï¼Œå°±æ˜¯å› ä¸ºCPUä¸å†å‚ä¸è¿™ä¸ªè¯»å–çš„å·¥ä½œã€‚å®é™…ä¸Šé€šè¿‡GDSå‘¢ï¼Œæˆ‘ä»¬æ˜¯ç»•è¿‡äº†ä¸»æœºçš„å†…å­˜ã€‚

è¿˜æœ‰å†…æ ¸çš„åè®®ç«™ï¼Œé€šè¿‡è¿™ç§æ–¹å¼å‘¢ï¼Œç›¸å½“äºæ˜¯é‡Šæ”¾äº†ä¸€éƒ¨åˆ†CPUçš„èµ„æºã€‚é‚£ä¹ˆé‡Šæ”¾å‡ºæ¥èµ„æºå¹²å˜›ç”¨ï¼Œåœ¨ç»™å…¶ä»–çš„ç®—åŠ›ä»»åŠ¡é‡Œé¢å»ç»™å…¶ä»–å»ç»™å…¶ä»–ç®—åŠ›ç®—åŠ›å»å»åšèµ„æºçš„ä½¿ç”¨ã€‚

å…¶å®å˜ç›¸çš„æˆ‘ä»¬ä¹Ÿæ˜¯é™ä½äº†è¿™ä¸ªCPUçš„å®ƒçš„ä¸€ä¸ªä¸€ä¸ªèµ„æºçš„ä¸€ä¸ªä½¿ç”¨ã€‚é‚£ä¹ˆåœ¨å®ƒçš„ç¨³å®šæ€§æ–¹é¢ï¼Œæˆ‘ä»¬é€šè¿‡é™¤äº†åˆšæ‰è¯´çš„æªæ–½ä¹‹å¤–å‘¢ï¼Œè¿˜æœ‰ä¸¤ä¸ªæªæ–½ã€‚ä¸€ä¸ªå‘¢å°±æ˜¯é€šè¿‡åŠ¨æ€çº ç¼ ç çš„æ–¹å¼ï¼Œå°±æ˜¯åŠ¨æ€çº åˆ ç ã€‚å¤§å®¶éƒ½çŸ¥é“ã€‚

å°±æ˜¯æˆ‘ä»¬æŠŠæ•°æ®åœ¨ã€‚æµ·é‡çš„å­˜å‚¨é‡Œé¢ï¼Œæˆ‘ä»¬åˆ‡å—ï¼Œåˆ‡å®Œå—ä¹‹åï¼Œé€šè¿‡åŠ å…¥å†—ä½™çš„é‚£ä¸ªå†—ä½™çš„æ•°æ®ä¹‹åå‘¢ï¼Œæˆ‘ä»¬é€šè¿‡åˆé€‚çš„è¿™ç§é‚£ä¸ªçº ç¼ ç çš„æ–¹æ¡ˆï¼Œæˆ‘ä»¬æŠŠå®ƒåˆ†å¸ƒåœ¨äº†å„ä¸ªèŠ‚ç‚¹ä¸Šé¢å»ã€‚

è¿™æ ·å‘¢ä¸€æ—¦æŸä¸€å—ç›˜æˆ–è€…æŸä¸€ä¸ªæ•°æ®ä¸¢å¤±çš„æ—¶å€™å‘¢ï¼Œæˆ‘ä»¬é€šè¿‡çº ç¼ ç çš„æ–¹å¼å¯ä»¥ä¿è¯å®ƒæ•°æ®çš„ä¸€ä¸ªå°±æ˜¯é™ä½å®ƒçš„æ•°æ®çš„ä¸€ä¸ªä¸¢å¤±ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¿è¯ä¸€ä¸ªæ•°æ®çš„å®¹é”™æ€§ã€‚å°±æ˜¯æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‘¢åœ¨3å—ç›˜ä¸¢å¤±çš„æƒ…å†µä¸‹ã€‚

æˆ‘ä»¬é€šè¿‡åŠ¨æ€çº ç¼ ç çš„æ–¹å¼ï¼Œå¯ä»¥æŠŠå®ƒå¯¹ä¸šåŠ¡çš„å½±å“çš„æ¦‚ç‡ã€‚å°±æ˜¯å¦‚æœè¦æ˜¯æ¯”å¦‚è¯´æˆ‘ä»¬ç°åœ¨åˆ—å‡º8ä¸ªTçš„è¯ï¼Œå®ƒå½±å“çš„æ¦‚ç‡èƒ½å¤Ÿé™åˆ°8ã€‚57ä¹˜10çš„-11æ¬¡æ–¹ï¼Œè¿™ä¸ªå°±å·²ç»å¾ˆå°äº†ã€‚

ç„¶åæˆ‘ä»¬å¯ä»¥åšåˆ°æ•´ä¸ªçš„ç³»ç»Ÿçš„ç¨³å®šæ€§èƒ½å¤Ÿä¿æŒåœ¨12ä¸ª9ä»¥ä¸Šè¿™ä¹ˆä¸€ä¸ªå±‚é¢ã€‚å¹¶ä¸”å‘¢åœ¨åº”ç”¨äº†è¿™ç§ç¼©åˆ—é‡æ„çš„æ–¹å¼ä¹‹åå‘¢ï¼Œå°±æ˜¯å½“æˆ‘ä»¬æŸä¸€ä¸ªã€‚æ¯”å¦‚è¯´å­˜å‚¨çš„é›†ç¾¤é‡Œé¢æŸä¸€å—ç›˜å‡ºç°äº†é—®é¢˜ä¹‹åã€‚

é‚£ä¹ˆæˆ‘ä»¬å»è°ƒæ•´æ•´ä¸ªçš„å®ƒçš„æ•°æ®ç‰‡çš„å¤§å°ï¼Œç„¶åå»é‡æ„è¿™ä¸ªä¸²ã€‚é‚£é‡æ„äº†ä¸²ä¹‹åå‘¢ï¼Œç›¸å½“äºæ˜¯è¿™ä¸€å—ç›˜å°±é€€å‡ºäº†ã€‚ç„¶åé€šè¿‡é€šè¿‡é‡æ„ä¸²ä¹‹åå‘¢ï¼Œæ•°æ®å‘¢ï¼Œå®ƒä¸€æ ·å‘¢æ˜¯å¯ä»¥ä¿æŒä¸€ä¸ªå†—ä½™ï¼Œè€Œä¸”ä¸é—´æ–­ï¼Œåšåˆ°äº†ä¸€ä¸ªä¸ä¸¢å¤±ã€‚

è€Œä¸”å°±æ˜¯å³ä¾¿ä¸¢å¤±ä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡å…¶ä»–çš„ç®—æ³•èƒ½å¤Ÿç»™å®ƒè¿›è¡Œæ¢å¤ã€‚å¹¶ä¸”å‘¢å½“è¿™å—ç›˜æ¢å¤äº†ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®ƒè‡ªåŠ¨é‡æ–°è¿›å…¥åˆ°é›†ç¾¤ä¹‹åï¼Œæˆ‘ä»¬å†å»è°ƒæ•´å®ƒçš„ã€‚æ•°æ®åˆ—çš„å¤§å°ï¼Œè®©ä»–åŠ å…¥åˆ°æ•´ä¸ªçš„é›†ç¾¤é‡Œé¢å»å»æ¢å¤å·¥ä½œã€‚

é€šè¿‡è¿™ä¸¤ç§æ–¹å¼å‘¢ï¼Œèƒ½å¤Ÿå»ä¿è¯æˆ‘ä»¬æ•´ä¸ªçš„æ•°æ®çš„ä¸€ä¸ªå®‰å…¨çš„å¯é æ€§ã€‚ç„¶åè¿˜æœ‰å‘¢å°±æ˜¯å› ä¸ºå‰é¢ä¹Ÿè®²åˆ°äº†æ•°æ®ç›˜è¶Šæ¥è¶Šå¤šã€‚æ•°æ®ç›˜å¤šäº†ä¹‹åå‘¢ï¼Œå°±æ˜¯å¤§å®¶åœ¨ä½¿ç”¨è¿™ä¸ªæ•°æ®ç›˜çš„æ—¶å€™éƒ½çŸ¥é“ï¼Œå°¤å…¶ä½¿ç”¨æ—¶é—´é•¿äº†ä¹‹åã€‚

æ•°æ®ç›˜çš„ç¨³å®šæ€§ä¼šé™ä½ï¼Œå®ƒç»å¸¸ä¼šå‡ºç°åç›˜ã€‚ä¸€æ—¦å‡ºç°åç›˜ï¼Œå¹¶ä¸”å°±æ•°æ®çš„ä»‹è´¨è¶Šæ¥è¶Šå¤§çš„æƒ…å†µä¸‹ï¼Œå®ƒå‡ºå»åç›˜ã€‚é‚£ä¹ˆå®ƒä¼šå¯¹æ•´ä¸ªæ•°æ®çš„å­˜å‚¨çš„è´¨é‡ä¼šäº§ç”Ÿè¿™ç§å®è§‚æ€§çš„å½±å“ã€‚é‚£ä¹ˆåœ¨è¿™ç§æ—¶å€™ã€‚

æˆ‘ä»¬ä¹Ÿæ˜¯å¼€å‘äº†ä¸€å¥—æ¨¡å‹æ¥è¿›è¡Œè®­ç»ƒã€‚å½“ç„¶è¿™ä¸ªé‡Œé¢ä¹Ÿå¾ˆæœ‰æ„æ€å˜›ï¼Œå°±æ˜¯æˆ‘ä»¬é€šè¿‡è®­ç»ƒçš„æ¨¡å‹ä¸ºæ¨¡å‹çš„è®­ç»ƒæ¥æä¾›æ¥æä¾›æ”¯æŒã€‚é‚£ä¹ˆå®ƒè¦åšä»€ä¹ˆå‘¢ï¼Ÿå®ƒå°±æ˜¯æˆ‘ä»¬å¯¹æ‰€æœ‰çš„è¿™é‡Œé¢çš„ç›˜è¿›è¡Œç›‘æ§å’Œæ£€æµ‹ï¼Œä½ åŒ…æ‹¬å®ƒçš„æ¸©åº¦ï¼Œå®ƒçš„éœ‡åŠ¨ã€‚

å®ƒçš„è®¿é—®é¢‘ç‡ï¼Œå®ƒçš„IOç­‰ç­‰è¿™äº›æ¥è¿›è¡Œæ¥è¿›è¡Œæ£€æµ‹å’Œè®°å½•ã€‚è¿™æ ·å‘¢ç°åœ¨èƒ½å¤Ÿåšåˆ°å¯¹å®ƒçš„å„ç§å·¥å†µçš„è¯»å–ä¹‹åè¿›è¡Œç»¼åˆåˆ†æã€‚



![](img/b8a2226fb956a3703ba6c0ca40227c0d_47.png)

æˆ‘ä»¬èƒ½å¤Ÿåšåˆ°æå‰14å¤©èƒ½å¤Ÿå¯¹è¿™å—ç›˜æœªæ¥çš„å·¥ä½œçŠ¶å†µè¿›è¡Œé¢„æµ‹ã€‚å½“æˆ‘ä»¬å‘ç°å®ƒçš„IOé™ä¸‹æ¥äº†ä¹‹åï¼Œé€šè¿‡åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™å—ç›˜è¿™å—æ…¢ç›˜ä»æ•´ä¸ªçš„é›†ç¾¤é‡Œé¢ç»™å®ƒéš”ç¦»å‡ºæ¥ã€‚ç›¸å½“äºæ˜¯æˆ‘ä»¬åšäº†ä¸€ä¸ªå®è§‚çš„è°ƒæ§ã€‚

æŠŠå®ƒéš”ç¦»å‡ºæ¥ä¹‹åï¼Œè¿™å—ç›˜å‡ºæ¥ä¹‹åï¼Œé‚£åŒæ—¶å°±æ˜¯åˆšæ‰å‰é¢è®²åˆ°çš„åŠ¨æ€çº ç¼ ç å’Œç¼©åˆ—é‡æ„çš„æ–¹å¼çš„æœºåˆ¶åˆå¼€å§‹èµ·åŠ¨ã€‚è¿™æ ·å½¢æˆäº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„å¾ªç¯ï¼Œæ¥ä¿è¯æ•´ä¸ªæ•°æ®çš„ä¸€ä¸ªé«˜çš„ä¸€ä¸ªå¯é æ€§ã€‚é‚£ä¹ˆè¯´åˆ°æœ€åè¯´åˆ°æœ€åå‘¢ï¼Œå°±æ˜¯ã€‚

åœ¨æ•´ä¸ªçš„å†…å­˜ï¼Œæˆ‘ä»¬é¢å‘å¤§æ¨¡å‹çš„å®è·µè¿‡ç¨‹ä¸­å‘¢ï¼Œå°±æ˜¯æˆ‘ä»¬åœ¨æé«˜å­˜å‚¨çš„ä¸€ä¸ªæ•ˆç‡çš„å·¥ä½œä¸­å‘¢ï¼Œå°±æ˜¯æˆ‘ä»¬å®Œå…¨ä¾æ‰˜çš„æ˜¯ä¸­å›½ç”µä¿¡å¼€å‘çš„è¿™ä¸ªå¤©ç¿¼äº‘çš„äº‘é”€è®¡ç®—å¹³å°æ¥å®ç°è¿™ä¸€ç³»åˆ—çš„æªæ–½ã€‚ä»è¿™ä¸ªæ¡†æ¶å¯ä»¥çœ‹åˆ°å‘¢ã€‚

å°±æ˜¯é€šè¿‡äº‘é”€çš„è®¡ç®—å¹³å°ã€‚æˆ‘ä»¬åœ¨åº•å±‚å°†è®¡ç®—å­˜å‚¨ç½‘ç»œç­‰ä¸€ç³»åˆ—çš„èŠ‚ç‚¹æ¥è¿›è¡Œå°è£…ã€‚ç„¶åé€šè¿‡å¤©äº‘çš„æ“ä½œç³»ç»Ÿå’Œäº‘é”€è®¡ç®—å¹³å°çš„è¿™ç§è°ƒåº¦ç³»ç»Ÿã€‚æˆ‘ä»¬æŠŠå®ƒè¿›è¡Œå°è£…äº†ä¹‹åã€‚

é¢å‘å®¢æˆ·èƒ½å¤Ÿä¸€ä½“åŒ–çš„æ¥æä¾›ä¸€ä½“åŒ–çš„æ¥æä¾›ç±»ä¼¼äºç½‘ç»œè®¡ç®—å¸¦å®½ã€‚ç„¶ååŒ…æ‹¬æ¨¡å‹è¾…åŠ©ï¼Œè¿˜æœ‰ç­‰ç­‰ç›¸åº”çš„ä¸€ç³»åˆ—çš„å·¥å…·ï¼Œå‘å®¢æˆ·æ¥ä¸€ä½“åŒ–çš„æ¥æä¾›æ¥æä¾›è¿™æ–¹é¢çš„å·¥ä½œã€‚ç„¶åä¹Ÿæ˜¯å¸Œæœ›å‘¢å°±æ˜¯åç»­æœ‰æœºä¼šå‘¢ï¼Œå¤§å®¶èƒ½å¤Ÿæ¥è¯•ç”¨å‘ƒã€‚

å¤©ç¿¼äº‘çš„è¿™ä¸ªäº‘é”€çš„è‡³ç®—å¹³å°ï¼Œå› ä¸ºæ—¶é—´é™åˆ¶å‘¢ï¼Œå°±æ˜¯å…·ä½“çš„åŠŸèƒ½å°±ä¸ä»‹ç»äº†ã€‚æ—¶é—´å¾ˆçŸ­ã€‚å…¶å®åœ¨è¿™é‡Œé¢å‘¢æœ‰å¾ˆå¤šæ˜¯å¯ä»¥æ·±å…¥å±•å¼€æ¥æ·±å…¥è®¨è®ºçš„ã€‚ä¹Ÿæ˜¯å¸Œæœ›å‘¢å°±æ˜¯åç»­æœ‰æœºä¼šçº¿ä¸‹çš„æ—¶å€™ã€‚

æˆ‘ä»¬ä¸€èµ·æ¥è¿›è¡Œåœ¨è¿™æ–¹é¢çš„ä¸€äº›æŠ€æœ¯å±‚é¢çš„æ·±å…¥çš„ä¸€ä¸ªä¸€ä¸ªæ²Ÿé€šã€‚å‘ƒï¼Œæˆ‘æƒ³ä»‹ç»çš„åŸºæœ¬ä¸Šå°±è¿™äº›ï¼Œè°¢è°¢ã€‚å¥½å‘ƒï¼Œè¿™ä¸ªä¸æ˜¯æœ€åçš„ä¸€ä¸ªspeerå•Šï¼Œå› ä¸ºæˆ‘ä»¬cancelæ‰äº†é‚£ä¸ªpanelã€‚

æ‰€ä»¥æˆ‘ä»¬æŠŠpanelå‘ƒå‰é¢æ²¡æœ‰è®²çš„å‘ƒï¼Œå¤©æ•°è‡³æ˜Ÿçš„ï¼Œæˆ‘ä»¬è¿˜æ˜¯é‚€è¯·ä»–ä¸Šæ¥åšä¸€ä¸ªå¾ˆç®€çŸ­çš„ä¸€ä¸ª shareã€‚ğŸ˜Šï¼Œå¥½ï¼Œè°¢è°¢å‘ƒã€‚ğŸ˜Šï¼Œå‘ƒï¼Œå¾ˆè£å¹¸å•Šæˆä¸ºæœ€åçš„å‘ƒå‹è½´çš„speakerã€‚

ç„¶åå—¯è€½è¯¯å¤§å®¶æœ€ååœ¨å‘ƒ7åˆ°8åˆ†é’Ÿçš„æ—¶é—´ã€‚ç„¶åå‘ƒç®€å•ä»‹ç»ä¸€ä¸‹æˆ‘ä»¬å’Œæ™ºæºåœ¨å¤§æ¨¡å‹ä¸Šçš„ä¸€äº›åˆä½œï¼Œä»¥åŠå¤©æ•°ä¹‹æ˜Ÿã€‚åœ¨å¤§æ¨¡å‹ä¸Šç›®å‰å·²ç»æ¨å‡ºäº†ä¸€äº›å‘ƒsolutionçš„æ–¹æ¡ˆã€‚ğŸ˜Šã€‚

é‚£å‘ƒé¦–å…ˆå¾ˆæ„Ÿè°¢æ™ºå°±æ˜¯å¤©è§†å¿ƒä½œä¸ºGPGPUçš„è¿™æ ·ä¸€ä¸ªå‚å•†ï¼Œå®é™…ä¸Šå’Œæ™ºæºæœ‰å¾ˆæ·±çš„åˆä½œã€‚å‘ƒï¼Œå°±åƒåˆšæ‰å‰é¢ç™½åšï¼Œç„¶åç”°è€å¸ˆæ‰€ä»‹ç»çš„ã€‚æˆ‘ä»¬åœ¨AIèŠ¯ç‰‡AIçš„å‘ƒç¼–è¯‘å™¨ï¼Œç„¶åè¿˜æœ‰ä¹é¼çš„è¯„æµ‹å¹³å°ä»¥åŠå¤§æ¨¡å‹çš„è®­ç»ƒä¸Šã€‚

è¿™é™…ä¸Šåšäº†å¾ˆå¤šçš„æ·±å…¥çš„å·¥ä½œã€‚é‚£è¿™ä¸€æ¬¡å‘¢æˆ‘ä»¬å’Œæ™ºæºå®é™…ä¸Šä¸€èµ·è¾“å‡ºäº†ä¸€ä¸ªå‘ƒçš„å‘ƒcode7Bçš„æ¨¡å‹ã€‚é‚£è¿™ä¸ªæ¨¡å‹çš„è¯å®é™…ä¸Šæ˜¯æ™ºæºåŸºäºå¤©æ•°çš„å‘ƒBI100çš„å‘ƒé›†ç¾¤ã€‚ç„¶åæ¥è¿›è¡Œè®­ç»ƒçš„ã€‚

ç„¶åè¿™ä¸ªè®­ç»ƒçš„æ•°æ®å‚æ•°å‘¢å¤§æ¦‚æ˜¯åœ¨100Bçš„è¿™ä¸ªå‘ƒä»£ç ä¹‹ä¸Šæ¥è¿›è¡Œå…¨é‡çš„ä¸€ä¸ªè®­ç»ƒã€‚ç„¶ååšä¸€ä¸ªpreçš„å·¥ä½œã€‚è¿™ä¸ªæ•´ä¸ªå·¥ä½œåœ¨19å¤©çš„æ—¶é—´é‡Œå¤´æ¥å®Œæˆã€‚æœ€åæ¥è¾“å‡ºäº†ä¸€ä¸ªå‘ƒè¿™æ ·å­çš„æ¨¡å‹ã€‚ğŸ˜Šã€‚

æ•´ä¸ªæ¨¡å‹çš„æ•ˆæœå‘¢ä¹Ÿèƒ½è¾¾åˆ°å‘ƒhuman evaluation at onepas at oneçš„è¯å¤§æ¦‚èƒ½åˆ°å‘ƒ19è¶…è¿‡19è¿™æ ·ä¸€ä¸ªæ•°ã€‚å®é™…ä¸Šåœ¨åŒçº§åˆ«çš„æ¨¡å‹å°ºå¯¸ä¸Šé¢å‘ƒæ˜¯ä¸€ä¸ªå‘ƒè¾¾åˆ°ä¸€ä¸ªSsoçš„ä¸€ä¸ªæ°´å¹³äº†ã€‚

é‚£åœ¨æ•´ä¸ªçš„è¿™ä¸ªåˆä½œé‡Œå¤´ï¼Œæˆ‘ä»¬ä¼šå‘ç°è¯´åœ¨å¤§æ¨¡å‹çš„è®­ç»ƒé‡Œå¤´ï¼Œå®é™…ä¸Šå‘ƒèŠ¯ç‰‡ä½œä¸ºå›½äº§èŠ¯ç‰‡æ¥è¯´ï¼Œä½œä¸ºå‘ƒè®¡ç®—å¡æ¥è¯´ï¼Œé‚£ä¹ˆé€šç”¨æ€§ï¼Œè¿˜æœ‰ä¸€0ä¸€æ€§æ˜¯ç¬¬è¦ç¬¬ä¸€é‡è¦çš„ç‚¹ã€‚é‚£æˆ‘ä»¬åœ¨æ•´ä¸ªè¿™ä¸ªå·¥ä½œé‡Œå¤´ï¼Œå®é™…ä¸Šä»æ”¯æ´ä»è‹±ä¼Ÿè¾¾çš„é›†ç¾¤ã€‚

ç„¶åæˆ‘ä»¬æ¥æ”¯æŒè¿™ä¸ªBMtrainï¼Œå°±åº•ä¸‹çš„åˆ†å¸ƒå¼ç³»ç»Ÿç”¨çš„æ˜¯BM trainè¿™æ ·ä¸€å¥—æ–¹æ¡ˆï¼Œä»è‹±ä¼Ÿè¾¾çš„é›†ç¾¤ï¼Œç„¶åè¿åˆ°å¤©æ•°çš„é›†ç¾¤é‡Œå¤´ï¼Œå°±è¿ç§»çš„æ—¶é—´å…¶å®å°±æ˜¯ä¸€å¤©çš„æ—¶é—´å°±è·‘èµ·æ¥äº†ã€‚é‚£å½“ç„¶è¿™è¿ç§»ä¹‹åã€‚

å®é™…ä¸Šæˆ‘ä»¬ä¼šå‘ç°é›†ç¾¤çš„æ€§èƒ½å¹¶æ²¡æœ‰è¾¾åˆ°æˆ‘ä»¬æ‰€æœŸæœ›çš„é‚£æ ·å­ä¸€ä¸ªèƒ½åŠ›ã€‚é‚£è¿™ä¸ªæ—¶å€™å°±è¦éœ€è¦åšå¯¹attentionçš„ç®—å­å»åšä¼˜åŒ–ã€‚åœ¨flash attentiontentionä¸Šé¢ã€‚é‚£æˆ‘ä»¬å¯èƒ½ã€‚

åŸºäºæˆ‘ä»¬çš„é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨å‘ƒkuå¤§å±‚é¢å»åšäº†è¿™ä¸ªç®—å­ä¼˜åŒ–ï¼ŒåŒ…æ‹¬äº†ä¸€äº›å‘ƒåº•å±‚çš„OPï¼Œå‡å°‘ä¸€äº›reduceã€‚å°±æ˜¯åœ¨è¿™æ ·å­çš„ä¼˜åŒ–ä¹‹åã€‚å¤§æ¦‚ä¸€å‘¨å·¦å³çš„æ—¶é—´ï¼Œé‚£æ•´ä¸ªé›†ç¾¤å°±å®Œå…¨çš„è·‘èµ·æ¥äº†ã€‚

å®é™…ä¸Šæ•´ä¸ªçš„å·¥ä½œå¤§æ¦‚æ˜¯ä¸€ä¸ªæœˆçš„æ—¶é—´ã€‚å¤§å®¶ä¼šçœ‹åˆ°è¯´é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ä¸€è¿ä¸€æ€§åœ¨è¿™ä¸ªé‡Œé¢æ˜¯å äº†å¾ˆé‡è¦çš„ä½œç”¨ï¼Œæ‰èƒ½ä¿è¯åœ¨ä¸€ä¸ªæœˆä¹‹å†…å®Œæˆè¿™æ ·å­ä¸€ä¸ªå¤§æ¨¡å‹çš„å‘ƒè®­ç»ƒï¼Œå¹¶ä¸”å¾—åˆ°ä¸€ä¸ªç›¸å¯¹æ¯”è¾ƒreasonableçš„ä¸€ä¸ªç»“æœã€‚

ğŸ˜Šï¼Œç„¶åç¬¬äºŒç‚¹ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªå¤§æ¨¡å‹çš„æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹é‡Œé¢ï¼Œæˆ‘ä»¬ä¼šå‘ç°è¯´å‘ƒé›†ç¾¤çš„å¯æ‰©å±•æ€§å®é™…ä¸Šæ˜¯ç¬¬äºŒé‡è¦çš„ç‚¹ã€‚åˆšæ‰å…¶å®å‰é¢æœ‰å¾ˆå¤šå‘ƒspeakkeréƒ½è®²æåˆ°äº†ï¼Œé‚£å¦‚ä½•å»åšè¿™ä¸ªå‘ƒåœ¨ä¸€ä¸ªç®—ä¾‹ï¼Œç„¶åå› é‚£ä¸ªç½‘ç»œæ‹“æ‰‘ã€‚

ç„¶åå¹¶è¡Œæ–‡ä»¶å­˜å‚¨ã€‚è¿™æ ·å­ä¸€ä¸ªç³»ç»Ÿé‡Œå¤´ï¼Œæ€ä¹ˆèƒ½å¤Ÿåœ¨è¿™äº›ç‚¹ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡çš„ä¸€ä¸ªsweetly pointçš„ç‚¹ã€‚è¿™ä¸ªpointçš„ç‚¹èƒ½å¤Ÿè®©é›†ç¾¤çš„å¯æ‰©å±•èƒ½åŠ›æœ€å¤§ã€‚å› ä¸ºæ¯•ç«Ÿå‘ƒå›½äº§èŠ¯ç‰‡ã€‚

æ•´ä¸ªé›†ç¾¤åœ¨å•å¡çš„ç®—åŠ›ä¸Šè·ŸNVçš„ä¸»æµå¡è¿˜æ˜¯æœ‰å·®å¼‚çš„é‚£å¯èƒ½é çš„å°±æ˜¯å¤šæœºçš„è¿™æ ·å­ä¸€ä¸ªå †å æ¥è¿½èµ¶è¿™æ ·å­ä¸€ä¸ªæ•´ä½“çš„æ€§èƒ½ã€‚é‚£åœ¨è¿™ä¸ªé‡Œé¢ï¼Œæˆ‘ä»¬å®é™…ä¸Šæœ‰ä¸€å¥—å¤©æ•°ä¹‹æ˜Ÿæœ‰ä¸€å¥—å®Œæ•´çš„æ–¹æ¡ˆï¼Œæˆ‘ä»¬åšåˆ°äº†ä¸€ä¸ªä»€ä¹ˆäº‹æƒ…å‘¢ã€‚

å°±æ˜¯åœ¨æ•´ä¸ªé›†ç¾¤çš„å¯æ‰©å±•è¿‡ç¨‹é‡Œå¤´ï¼Œéšç€èŠ‚ç‚¹çš„å¢åŠ ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€ç›´ä¿æŒå‘ƒï¼Œè¿™ä¸ªé›†ç¾¤çš„æ•ˆèƒ½åœ¨95%ä»¥ä¸Šï¼Œå°±æ˜¯çº¿æ€§åŠ é€Ÿæ¯”åœ¨95%ä»¥ä¸Šã€‚å‘ƒï¼Œè¿™æ˜¯ç¬¬äºŒä¸ªå¾ˆé‡è¦çš„ç‚¹ã€‚ç¬¬ä¸‰ä¸ªå¾ˆé‡è¦çš„ç‚¹å…¶å®å°±æ˜¯ç¨³å®šæ€§ã€‚

å¤§è´¸å‹çš„è®­ç»ƒçš„å‘¨æœŸå¾ˆé•¿ï¼Œæ•´ä¸ªçš„å‘ƒæƒä¸­é—´çš„å°±æ˜¯è¦ä¿å­˜çš„æƒé‡å¾ˆå¤šã€‚ç„¶åæ•°æ®è¦åŠ è½½çš„é‡ä¹Ÿå¾ˆå¤§ã€‚é‚£åœ¨è¿™ä¸ªè®­ç»ƒçš„è¿‡ç¨‹é‡Œå¤´ï¼Œå®é™…ä¸Šå¤©æ•°ä¹‹æ˜Ÿä¿è¯äº†ä¸€ä»¶äº‹æƒ…ï¼Œå°±æ˜¯åœ¨19å¤©çš„æ—¶é—´é‡Œå¤´æ˜¯æ²¡æœ‰ä»»ä½•ä¸€æ¬¡æ‰å¡ã€‚é‚£ä¿è¯è¿™ä¸ªã€‚

æ•´ä¸ªæ¨¡å‹çš„è®­ç»ƒèƒ½å¤Ÿå¹³ç¨³æœ‰æ•ˆçš„smoothçš„å¾€å‰èµ°ã€‚è¿™ä¸ªæ˜¯æˆ‘ä»¬åœ¨æ•´ä¸ªè¿‡ç¨‹é‡Œé¢æ‰€çœ‹åˆ°çš„ã€‚å‘ƒï¼Œåœ¨å¤§æ¨¡å‹è®­ç»ƒä¸Šï¼Œæˆ‘ä»¬å’Œå±…æ©ä¸€èµ·åˆä½œï¼Œèƒ½å¤Ÿåšåˆ°çš„ä¸€äº›ç‚¹ã€‚ç„¶åå‘ƒå›è¿‡å¤´æ¥å‘¢ï¼Œä»äº§å“çš„è§’åº¦æ¥è¯´ã€‚

å…¶å®å¤©æ•°ä¹‹æ˜Ÿå› ä¸ºå·²ç»å‘å¸ƒäº†ä¸‰æ¬¾å‘ƒæ–°æ–°é‚£ä¸ªå¡ï¼Œæ‰€ä»¥åŸºæœ¬è€Œä¸”æ˜¯é‡äº§äº†ã€‚é‚£åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šå·²ç»è¦†ç›–äº†AIçš„è®­ç»ƒæ¨ç†ï¼Œè¿˜æœ‰ç§‘å­¦è®¡ç®—ï¼Œæ•´ä¸ªè¿™ä¸ªé¢†åŸŸã€‚é‚£åœ¨å¤§æ¨¡å‹çš„è®­ç»ƒçš„è§’åº¦ä¸Šé¢ï¼Œå®é™…ä¸Šæˆ‘ä»¬æœ‰å®Œæ•´çš„æ–¹æ¡ˆã€‚

è¿™ä¸ªæ–¹æ¡ˆå¯ä»¥è¦†ç›–pre trainçš„å·¥ä½œï¼Œå¯ä»¥è¦†ç›–å‘ƒPFTçš„fin tuneä¹Ÿå¯ä»¥è¦†ç›–p tuningçš„è¿™æ ·å­ä¸€äº›ç®€å•çš„å‘ƒä¼˜åŒ–å·¥ä½œã€‚é‚£è¿™ä¸ªæ˜¯ç„¶ååŒæ—¶ç”±äºæˆ‘ä»¬èƒ½å¤Ÿæ”¯æŒã€‚å‘ƒï¼Œä¸åŒçš„åˆ†å¸ƒå¼æ¡†æ¶ã€‚

å°±é™¤äº†deep speedï¼Œç„¶åBM trainclo AIï¼Œç„¶åè¿˜æœ‰micacè¿™æ ·çš„ä¸€äº›æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨åº•å±‚èƒ½å¤Ÿæ”¯æŒè¿™æ ·æ¡†æ¶ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åšåˆ°ä¸€ä¸ªå¾ˆé‡è¦çš„ç‚¹æ˜¯èƒ½å¤Ÿå¾ˆçµæ´»çš„ä»å•èŠ‚ç‚¹æ‰©åˆ°æ•°ç™¾ä¸ªèŠ‚ç‚¹ã€‚

è¿™æ˜¯æˆ‘ä»¬å·²ç»å¯ä»¥æ”¯æŒçš„å‘ƒï¼Œè¿™æ ·çš„ä¸€äº›å¼€æºæ¨¡å‹çš„ä¸€äº›è®­ç»ƒçš„å·¥ä½œã€‚ç„¶ååœ¨æ¨ç†ä¸Šé¢çš„è¯å‘¢ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯å‘ƒèƒ½å¤Ÿæ”¯æŒï¼Œä¹Ÿè¿™äº›æ˜¯æˆ‘ä»¬ç›®å‰å·²ç»æ”¯æŒçš„ä¸€äº›ä¸»æµçš„ä¸€äº›å¤§æ¨¡å‹çš„ä¸€äº›ä¸ªæƒ…å†µã€‚

é‚£ä¼šçœ‹åˆ°æœ€é‡è¦çš„ä¸€ä¸ªç‚¹å°±æ˜¯æˆ‘ä»¬èƒ½å¤Ÿåšåˆ°åœ¨ã€‚å‘ƒã€‚å› ä¸ºæ¨ç†çš„æ—¶å¤´å…¶å®æœ€é‡è¦çš„ä¸€ä¸ªç‚¹ï¼Œå®é™…ä¸Šæ˜¯å¤šè½®ä»»åŠ¡çš„å¤šäººåŒæ—¶å¯¹è¯ã€‚é‚£åœ¨è¿™ä¸ªé‡Œå¤´ã€‚

å®é™…ä¸Šå¤§å®¶çŸ¥é“åœ¨attentioné‡Œé¢å…¶å®å¾ˆé‡è¦çš„ä¸€ä»¶äº‹æƒ…æ˜¯ä½ è¦ä¸å°±æ˜¯æ€ä¹ˆä¿æŒä¸Šä¸‹æ–‡ï¼Œè¦ä¸å°±æ˜¯åœ¨attention statusï¼Œä½ æ€ä¹ˆæŠŠå®ƒä¿å­˜ä¸‹æ¥ã€‚ç„¶åå½“å‘ç°å‘ç”Ÿäº†ç”¨æˆ·åˆ‡æ¢çš„æ—¶å€™ã€‚

æœ€åè¿™ä¸ªè¿™ä¸ªå‰ä¸€ä¸ªç”¨æˆ·çš„å‘ƒsession statusè¿˜èƒ½ä¿æŒä½ã€‚é‚£åœ¨è¿™ä¸Šé¢æˆ‘ä»¬æ˜¯æœ‰å®Œæ•´æ–¹æ¡ˆçš„ã€‚æˆ‘ä»¬å¯ä»¥åšåˆ°åœ¨è¿™ä¸ªå®Œæ•´æ–¹æ¡ˆä¸Šå¤šç”¨æˆ·å‘ƒé«˜å¹¶å‘çš„æƒ…å†µä¸‹ã€‚

æˆ‘ä»¬å¯ä»¥åšåˆ°å‘ƒæˆ‘ä»¬çš„ä¸€ä½“æœºå¯ä»¥åšåˆ°800 tokensæ¯ç§’å•Šï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªæˆ‘ä»¬å‘ƒç‹¬ç‰¹çš„ä¸€ä¸ªåœ°æ–¹ã€‚ğŸ˜Šï¼Œå‘ƒï¼Œæ•´ä¸ªçš„åœ¨åº•å±‚å°±æ˜¯å¤§æ¨¡å‹çš„æ”¯æ’‘ä¸Šé¢ï¼Œå…¶å®å‘ƒå¤©æ•°ä¹‹æ˜Ÿå®é™…ä¸Šæ˜¯åšäº†å¾ˆå¤šçš„å‘ƒä¸åŒçš„å·¥ä½œã€‚

å°±å‘ƒä»ç¡¬ä»¶åˆ°ç¼–è¯‘å™¨åˆ°å‘ƒåˆ†å¸ƒå¼æ–‡åˆ†å¸ƒå¼çš„è¿™ä¸ªè®­ç»ƒç³»ç»Ÿï¼Œç„¶åå†åˆ°ä¸Šå±‚çš„åº”ç”¨çš„ä¼˜åŒ–ï¼Œå®é™…ä¸Šæ˜¯å‘ƒåŒ…å«äº†å¾ˆå¤šçš„æ­¥éª¤ã€‚

ç„¶åç‰¹åˆ«æ˜¯è¦æçš„å°±æ˜¯åƒç°åœ¨çš„flashå°å°ä¸ªå­ex transformè¿™æ ·å­ä¸€äº›ç®—å­æˆ‘ä»¬éƒ½å·²ç»æ”¯æŒäº†ã€‚è€Œä¸”è¿™ä¸ªæ”¯æŒçš„è¯ï¼Œå¯¹äºä¸Šå±‚åº”ç”¨ç”¨æˆ·æ¥è¯´ï¼Œä½ ç”¨æ¥è°ƒç”¨çš„æ—¶å€™ï¼Œä½ æ˜¯æ— æ„ŸçŸ¥çš„ã€‚å‘ƒï¼Œæœ€åè¯´ä¸€ä¸‹ã€‚

æˆ‘ä»¬å¯èƒ½å¯¹æœªæ¥çš„ä¸€ä¸ªå±•æœ›ï¼Œå°±æ˜¯åœ¨å¤§æ¨¡å‹è®­ç»ƒé‡Œå¤´ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå‘ç°ï¼Œå®é™…ä¸Šä¼šç¢°åˆ°å‘ƒæ–°çš„ä¸€äº›é—®é¢˜ã€‚é‚£ç¬¬ä¸€ä¸ªé—®é¢˜å…¶å®å°±æ˜¯ç®—å­çš„ä¼˜åŒ–çš„é—®é¢˜ã€‚

é‚£å…¶å®æ˜¨å¤©æˆ‘å¬åˆ°ä¸€ä¸ªå°±æ¥è‡ªoä¸€ä¸ªç ”ç©¶çš„å…¶å®ä¹Ÿä¹Ÿè®²åˆ°äº†å°±æ²¡æœ‰ä»»ä½•å®˜æ–¹çš„æ•°æ®å»è¯´Gæ˜¯ä¸€ä¸ªè¿˜æ˜¯ä¸ªæ¨¡å‹ä½†é™…ä¸Šåœ¨é‚£ä¸ªä¼šä¸Šä¹Ÿè°ˆåˆ°äº†ï¼Œå…¶å®å®ƒæ˜¯ä¸ªé‚£æ•´ä¸ªçš„åœ¨è¿™ä¸ªè®¡ç®—é‡Œå¤´å®é™…ä¸Šæ€ä¹ˆåœ¨è¿™ä¸ªè¿™ä¸ªç®—å­ä¸Šèƒ½å¤Ÿåšçš„æ›´å¿«ã€‚

ç‰¹åˆ«æ˜¯ä»ç¡¬ä»¶åŠ é€Ÿçš„è§’åº¦ã€‚ç„¶åèƒ½å¤Ÿä»æŒ‡ä»¤çº§çš„è§’åº¦èƒ½å¤Ÿæ€ä¹ˆåšèµ·æ¥ï¼Œè¿™ä¸ªæ˜¯éå¸¸é‡è¦çš„ä¸€ä¸ªç‚¹ã€‚ç„¶åæ¨ç†çš„è¯å‘¢å®é™…ä¸Šå¤§å®¶ä¹Ÿéƒ½çŸ¥é“ç°å¯èƒ½å¦‚æœå»å‹ç¼©å»åšè’¸é¦ï¼Œè¿™æ˜¯å¤§çš„ä¸€ä¸ªç‚¹ã€‚

ç„¶åç¬¬ä¸‰ä¸ªçš„è¯å°±æ˜¯å­˜å‚¨çš„æˆæœ¬å­˜å‚¨æˆæœ¬äº†æœ¬èº«è¦å­˜å‚¨çš„è¿™äº›æ—¥å¿—ã€‚ç„¶åcheckpointä»¥ï¼Œå…¶å®è¿˜æœ‰ä¸€ä¸ªå¾ˆå¤§çš„ç‚¹ï¼Œç°åœ¨æœ‰ä¸€ä¸ªæ–°çš„æ–¹æ¡ˆï¼Œæˆ‘ä»¬ä¹Ÿåœ¨åšçš„ï¼Œå°±æ˜¯è¯´æˆ‘ä»¬æ€ä¹ˆæŠŠVMEç”¨èµ·æ¥ã€‚ä¹ˆæŠŠå°±æ˜¯åœ¨å¤šä¸ªå‘ƒå¤šè½®ã€‚ğŸ˜Šã€‚

å¯¹è¯åˆ‡æ¢ä¸åŒç”¨æˆ·ä¹‹é—´åˆ‡æ¢çš„æ—¶å€™ï¼Œæˆ‘ä»¬æ€ä¹ˆèƒ½å¤Ÿä¸ç”¨æ¯æ¬¡éƒ½å¸¦ä¸Šä¸‹æ–‡ï¼Œè€ŒæŠŠä¸­é—´çš„attentionçš„stateå­˜åˆ°NVMå»ã€‚ç„¶ååœ¨ä¸‹ä¸€è½®åˆ‡æ¢çš„æ—¶å€™ï¼Œå¿«é€ŸåŠ è½½å›æ¥ã€‚

è¿™æ ·å­æ¥ä¿è¯è¿™æ ·å­ä¸€ä¸ªå®æ—¶æ€§ä½å»¶æ—¶å¤§ååçš„è¿™æ ·çš„ä¸€ä¸ªæ•ˆæœã€‚è¿™ä¸ªä¹Ÿæ˜¯ç›®å‰æœªæ¥è¦è§£å†³çš„å°±æ˜¯è¦è¿›ä¸€æ­¥å»ä¼˜åŒ–è§£å†³çš„åœ°æ–¹ï¼Œæœ€åçš„è¯å°±æ˜¯äº’è”ç½‘æˆæœ¬äº†ã€‚ç°åœ¨ä»Šå¤©æ‰€æœ‰çš„å¤§æ¨¡å‹éƒ½å·²ç»æ˜¯æœ‰é’±äººçš„æ¸¸æˆã€‚

é‚£infinçš„äº¤æ¢äº¤åŠ ä¸Šäº¤æ¢æœºæ•´ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„é‚£rockçš„è¯ä¼˜åŠ¿æ˜¯å¯ä»¥åœ¨å¸¦å®½ä¸Šè§£å†³ã€‚ä½†æ˜¯å‘ƒåœ¨å»¶æ—¶ä¸Šå®é™…ä¸Šæ˜¯æœ‰å¾ˆå¤§å·®è·çš„é‚£åœ¨è¿™ä¸ªç‚¹ä¸Šï¼Œé‚£æ€ä¹ˆåˆ©ç”¨å‘ƒã€‚

ç„¶ååˆ©ç”¨è¿™æ ·çš„ä¸€äº›å…¶ä»–çš„æ–¹å¼sçš„æ–¹å¼èƒ½å¤ŸæŠŠå°½é‡çš„æŠŠè¿™äº›å»¶è¿Ÿéƒ½éšè—åˆ°è¿›ä¸€æ­¥éšè—åˆ°è®¡ç®—é‡Œé¢å»ã€‚å°±æ˜¯æˆ‘ä»¬ä¼šå’Œå‘ƒåˆ†å¸ƒå¼çš„è¿™ä¸ªè®­ç»ƒæ¡†æ¶è¿›ä¸€æ­¥å»é…åˆå»ä¼˜åŒ–çš„ç‚¹ï¼Œé‚£ä»¥ä¸Šè¿™äº›å‘¢å°±æ˜¯æˆ‘ä»Šå¤©æƒ³ã€‚ğŸ˜Šï¼Œå‘ƒã€‚

ä»‹ç»å’Œåˆ†äº«çš„å†…å®¹å¾ˆæ„Ÿè°¢å‘ƒï¼Œå¤§å®¶çš„æ—¶é—´ã€‚å¥½ï¼Œè°¢è°¢ã€‚ğŸ˜Šï¼ŒOkayï¼Œ allow me to spend just one or two more minute to closeã€‚

 And just like I saidï¼Œ I want to comment the the internship since I PMmã€‚ And in this conferenceã€‚

 I certainly noticed sevenï¼Œ including myselfï¼Œ7 of our speakerã€‚ğŸ˜Šï¼ŒActuallyã€‚

 have worked for IBM as the researcher or internship or the product team for IBM systemã€‚

 And it's not by designï¼Œ actually because the AIï¼Œ large foundation modelã€‚

 we can have the chance to meet together againã€‚ So that means that the system is so important for AIã€‚

 especially for the large foundation model and large foundation modelã€‚ğŸ˜Šã€‚

Is have the chance to drive system research to NAdaã€‚ So no matter how our tech career changeã€‚

We feel happy that we can meet together again to discuss the new problem together and embrace the new challenge togetherã€‚

 So againï¼Œ in the last sentence is I want to appreciate all our speakers againã€‚

 Thank you for but prepare such a wonderful speeches todayã€‚ And another thing is for our speakerã€‚

 actually they flight a long way from US and French to hearã€‚

 and have the face to face discussion with usã€‚ And againï¼Œ I want to thanks all our audienceã€‚

 stay such a long time until nowã€‚ So let's meet next year in next B AI conferenceã€‚ Thanksã€‚ğŸ˜Šã€‚

