# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ä½¿ç”¨ Scikit-learn è¿›è¡Œæœºå™¨å­¦ä¹ ï¼Œ4å°æ—¶å®æˆ˜è§†è§’åˆ·æ–°çŸ¥è¯†æ¡†æ¶ï¼Œåˆå­¦è€…è¿›é˜¶å¿…å¤‡ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P10ï¼š10ï¼‰æ­£åˆ™åŒ–å’Œæ ‡å‡†åŒ– - ShowMeAI - BV16u41127nr

![](img/ba488996234f1aee5936aea9ea40f4bf_0.png)

Well in this video I'm may be talking about two topicsã€‚ one is regularization and other is standardizationã€‚ standardization is something that we have to use a lot in this course and understandã€‚ and it's really relatively simpleï¼Œ but regularization is a very complicated topic and might require a lot of time and a more advanced machine learning courseã€‚

I'm not trying to get into any math regarding regularizationã€‚ and I'm just trying to try to give the kind of simplest intuition and we aren't going to get deep into itã€‚ but just wanting to know that that's an important and deeper topicã€‚So in terms of things that we've already doneã€‚

![](img/ba488996234f1aee5936aea9ea40f4bf_2.png)

We've been using logistic regression a lot and a problem that it has that I haven't talked about is that it's very sensitive to scalingã€‚And so for exampleï¼Œ you might have a data set and there might be some numbers in it with specific unitsã€‚And you might get one result of due do the classification if you change those unitsï¼Œ so for exampleã€‚ maybe you change miles to feetï¼Œ you might get a different resultã€‚

 which is of course not what we want we just care about the actual kind of informationã€‚ not what units somebody arbitrarily chose to useã€‚Why is that well it's because logistic regression is applying this technique to regularizationï¼Ÿ

Which tries to use smaller coefficientes and in generalã€‚ not have a very large coefficient on just one of our featuresã€‚You can imagine that I have lots and lots of feature columnsã€‚That just by chanceã€‚ maybe one of them does better on the training data than othersã€‚

Even if the other feature columns are kind of still somewhat usefulã€‚ and so what I wouldn't want to do is just by chance choose that best one because then it won't work well later on a test data setã€‚ regularization basically is providing a motivation to use multiple features and not consider one too heavilyã€‚ even if that would do better in the short termã€‚So logistic regression does this linear regression which was the first model we learned in this course does notã€‚

 but there are also things very similar to linear regression that do such as ridge regression and lasal regressionã€‚ we're not trying to talk about those at 320 but they're important and use all the time and so know that this regularization thing is a big dealã€‚

So what would we really likeï¼Œ we don't want our model to be sensitive to unitsã€‚ We would like to standardize it in some way so that we have the same numbers going inã€‚ regardless of what their original units wereã€‚So for this example I just made up kind of a fake scenarioã€‚ we're measuring some sort of quantity in the real world three times and based on we're trying to predict what sort of category it's in the category will either be true or falseã€‚

The underlying rule is that when the true lengthï¼Œ which we don't knowã€‚ is bigger than 5 than the category is trueã€‚ It's less than 5ã€‚ The category is falseã€‚ And so these three kind of noisy measurementsï¼Œ even though they know tell us exactly what the true length isã€‚ they give us some information about that they can help us guess whether it's true or falseã€‚

So here I have that dataï¼Œ that fake data I'm talking aboutï¼Œ the y column here is the categoryã€‚ and then I have my three measurementsï¼Œ x1 x to an x3ã€‚Let me just talk a little bit about how I'm generating thisã€‚So under anumpot randomã€‚ there are a bunch of functions that will generate random dataï¼Œ I'm doing a normal distribution hereã€‚

 you can sample from different distributionsï¼Œ you don't know what that means that's fine for this courseã€‚ but basically I'm generating1000 random numbers with an average of four and putting them in hereã€‚And so this will be an array of numbersã€‚ and then I'm sayingï¼Œ wellï¼Œ whatever that's fiveã€‚ greater than fiveï¼Œ I want to have a trueï¼Œ otherwise I'll have a falseã€‚

When I'm looking at this data frame down hereï¼Œ truefe does not directly go then into any of these columns it's unknownã€‚ but category does and categories what we're trying to predictã€‚So how are we trying to predict if we don't know true feetï¼Œ why have these three other columnsã€‚ which are basically just true feet plus some random noiseã€‚ So if I look at it down hereã€‚

 let me look at this first oneã€‚ All three measurements were less than5ã€‚ So it makes a lot of sense that I'll predict that the y is less than than5ã€‚ Maybe I can even look at some more cases hereã€‚ I wonder ifã€‚I can see where it's trueã€‚Let me do that So I can see some other cases here where it's true rightï¼Œ all in this caseã€‚

 all the measurements were greater than5ã€‚ So I say it's trueã€‚ This is kind of a more interesting exampleã€‚ This number is very largeã€‚ rightã€‚ One measurement was like almost of7ã€‚And even though the other two measurements were less than 5ã€‚ this was enough of a signal that the model decide it's trueã€‚ Wellï¼Œ it's true overallã€‚

 and hopefully the model will decide the sameã€‚ Okayï¼Œ so that's the data we're working withã€‚ And let's see if we canã€‚Train a model to try to predict thisã€‚ so I'm going to create a model and I'm going to use a linear or logistic regression modelã€‚And I'm going to fit itã€‚To my dataï¼Œ and so I may have some x and and Y's for my Y'sã€‚

 I'm just going to pull out y column for my training data and for my Xã€‚I want to pass in a list of all my columns that contain features so x1 and x2 and x3ã€‚ and I'm going be using these againï¼Œ so I'm actually try to put this in a variable called x columnsã€‚And then I don't have to keep typing that whole long thing every timeã€‚

And so I fit it and that's straight and so pretty soon I may look at the coefficients for this modelã€‚ but before that I just want to as an asideï¼Œ see what accuracy it has if I want to see the accuracy of the modelã€‚ I can just say instead of fit I want to score and then to be realistic I shouldn't score it on data that I haven't seen before instead of the thing I trained it on right so this is kind of a better test and I see that it has 89% accuracy is that good 89% seems high that we would be right that often but let me show you why it's not necessarilyã€‚

If I look at this Y columnï¼Œ I see it's almost always false and indeed if I say value countsã€‚ I could see it's only true less than 20% of the timeã€‚ and I can actually just divide this by the length of tests to see thatã€‚ And so what this means is that even if I had a very naive modelï¼Œ it just always says falseã€‚

 I would be getting 81% accuracyï¼Œ 89% is betterï¼Œ but in that context it's not so amazing just given that there's so much skew in that columnã€‚Okayï¼Œ so after I look at the accuracy of a modelï¼Œ the next thing I'll often want to look at are my coefficientsã€‚

 and I like to plot those in some wayï¼Œ ohï¼Œ my model coefficientsã€‚And I see those are right here and and I'd like have some sort of bar plotsã€‚ I know that these things are paired up with these x columns right so this is the coefficient for x1 so on and so forthã€‚ And so the way I'll often make such a bar plot is I'll say PDd do seriesã€‚

And then I will have my Y valuesã€‚And then I'll have index equals x valuesã€‚Dot plotï¼Œ dot barã€‚So on the x valuesï¼Œ I'm going to use those column namesï¼Œ and I'm going to put the coefficientsã€‚To basically have the quantities that are going to go to the Y axisã€‚And it's complaining that the length of one of these things is one when it was supposed to be threeã€‚

 and so x columnsï¼Œ that's pretty simple thereï¼Œ but if I look at thisã€‚This array right here what do I see I see there's it's really a twodiional thing I can flatten it into a one dimensional array with three numbers or if I say negative oneã€‚ it'll make it one dimensional it'll make it you know it doesn't matter how many numbers I have and I'll figure it out so I may put that here now and now I get this plot and this is interesting right I was talking about how maybe sometimes just by chance we focus more in one problem than another and that happens here right x1 x2 and x3 we're all equally noisy but it just so happens that based on the training data the model thinks that x1 is more kind of more useful right that was just by chanceã€‚

And so you could imagine a worse scenario where it picks one column that I really likes and ignores all these other ones that have good information in itã€‚ and so the model will try to avoid doing thatã€‚ regularization means that we'll try not to put too much weight on just one factor we'll try to spread it out a little bit and then if you took it to an extremeã€‚

 you might imagine that I could have a model where I just look at my interceptã€‚My intercept is you can imagine that being like the average and the model could just predict animal all my coefficients could be zero in that caseã€‚ well we all just predict the same thingï¼Œ and we want to have this problem I guess we have another problem that just want to be very accurateã€‚Okayï¼Œ so I have thatã€‚And so let me head back here and I'm going randomly generate this dataã€‚

 but this time I'm just going to change the units on this column and I'm going to change the units to be miles and so there's 5ã€‚280 feet in a mile so I'm just to make a comment here this is feet to miles like that and so I have the same kind of data just different unitsã€‚

 so I might hope that my model won't do anything that differentlyã€‚And so I'm going to run this againã€‚ and I see that not too much has changed hereã€‚ And then I want to think about what's going to happen when I rerun thisã€‚So in this x2 columnã€‚The numbers are all much smaller now because it's in milesã€‚ and so I might expect that to use thisï¼Œ I might have a bigger coefficient on x2ã€‚

If I wanted to be just like beforeã€‚It turns out when I run itï¼Œ I actually see the oppositeã€‚ It's adverse to having such large coefficients on one column because of that regularization thing I talked aboutã€‚ So it actually decidesï¼Œ heyï¼Œ I'm just going to ignore x2 entirelyã€‚ I have to put a bigger or a bigger weight on that that I'm comfortable doing to have it be a factorã€‚

 So I just lost some information thereã€‚ I' using these two columns anymoreã€‚ Nowï¼Œ of courseã€‚ that's silly rightï¼Œ putting a bigger coefficient on it isn't really waiting it moreã€‚ It's just canceling out the fact that I have different units on itã€‚ And so there's different ways to solve thisã€‚ One is that I could just insist that I have the same units for everythingã€‚

 Another way I could do it is I could try to kind of make this a little more uniform in some wayã€‚ And so that's what I'm going to do hereã€‚ğŸ˜Šï¼ŒI'm going head back hereï¼Œ and let meã€‚Let me take my training data and my actuallyï¼Œ where do I want to do thisï¼Ÿ

I take these x columns even soonerï¼Œ Actuallyï¼Œ noï¼Œ that's fineã€‚ I'm going to leave that thereã€‚ so I'm going take my training data and I want to take a slice of it and I want to get all the rows and I want to get columns x1ã€‚Through R x2ã€‚And so this is just my features now well through x3ï¼Œ sorryï¼Œ these are just my featuresã€‚And I want to somehow standardize it so that they all have roughly the same scaleã€‚

 And so I'm to do is I'm just going pull this out into this x variable right hereã€‚ and there's going be two things I'm going to doã€‚ one is that I'm going take the mean of each of each column just like thatã€‚ and I'm going subtract these numbers off of each column I'm going say thatã€‚ And so now all of these columns are centered at  zero right after I subtracted away the mean the average of every column is0ã€‚

 it turns out that that is also helpful for logistic regression to run fasterã€‚ I'm not trying to get any details about why that's useful And then more importantly I want them to be on the same scaleã€‚ and so oopsï¼Œ what happen thereã€‚I jump onto a new column or to a new cell and so if I look at thisã€‚ that's a standard deviation of each column and if I have larger numbers while the standard deviation shall be higher and so standardization the real key part is that I'm dividing all of this by that standard deviation and if I do that I may get a bunch of small numbers that have roughly the same scale so after I've done this all of them will have the same average0 and then the same standard deviation of one and so this would be a better x data and I can actually put this back in to my training data like this so I may say this equals my new X data so I make it out here Im going say manually standardize the dataã€‚

And so after I do that I run all of this stuff again and now I see that great x2 is back in playã€‚ even though I have different unitsï¼Œ it's not getting obsessed with these other columns just based on the unitsã€‚ so this was a good thing to do okay that's what standardization isã€‚Now it turns out that to do this rightï¼Œ I have to calculate this mean and standard deviation on the training dataã€‚

 but then I have to use that same mean and standard deviation on the test data I can't retake the mean on the test data and so the methodology of this gets a little bit complicated and so generally we won't do this generally we'll have ask K learn do that for us and so it turns out that there's a preprocessing step called standardization and we're going to use that as so manually doing this so I'm going to head back hereã€‚

And you can see I've already import my standard scalarã€‚And I'm to run this hereã€‚And this is skipping now for my modelï¼Œ rightï¼Œ I'll just actually leave this for nowã€‚ and that'll be my bad modelã€‚ What I'll do is I'll' create a new modelã€‚ which will be a pipeline modelã€‚And in that pipeline modelã€‚

 I want to have a standard scalar followed by a logistic regressionã€‚Just like that andã€‚This oneã€‚ I may have to actually create them like thatã€‚I have to give them namesï¼Œ rightã€‚ So I'm going pass tus hereã€‚ So I'm going to call that standard scalarã€‚Okayã€‚ I have to put Thomas to separate these thingsã€‚And thenï¼Œ then we go have this logistic regressionã€‚

Like that And this is my new model and then it turns out all this stuff I was doing before of like fittingã€‚ for example it can work the same way I can fit just like I did before because I this new one also fit and so I can do that I could also score it like I did before let me score it now and I get something very similar and then what's might be interesting is that when I actually do this when I actually try to get this bar plot it should show that it's back in play right even though the nonstandardized version is ignoring x2 nowã€‚

 this version should show it so remember this there's gonna be a small error here and the problem is that pipeline doesn't have coefficients this pipeline as a whole doesn't have coefficients but the logistic regression inside of it does have coefficientsã€‚

ğŸ˜Šï¼ŒAnd so how can I get to that it turns out that any pipeline works like a dictionary and I can for example I can copy these names and use that like a key and so that would get me my standard scalar from the beginning or I could pass in this key and that would get me a logistic my logistic regression stage of it and so from that then I could actually see well what are the coefficients involved there and I would I would paste this here instead of what I originally had and so now I can see that when I have the standardization in play as a transformer before my estimator it'll automatically do that and then it'll do the right things as well if I do my fitting here it'll calculate that mean and a standard deviation and I do scoring it's just going to use the same mean in standard deviation from before it would not look at that for the task because that would be kind of a methodological mistake so we're going to be generally doing whenever we have a logistic regression unless we have some very special scenarioã€‚

For exampleï¼Œ the data has already been standardizedã€‚![](img/ba488996234f1aee5936aea9ea40f4bf_4.png)