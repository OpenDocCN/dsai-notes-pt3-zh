- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÈù¢ÂêëÂàùÂ≠¶ËÄÖÁöÑ TensorFlow ÊïôÁ®ãÔºåÁêÜËÆ∫Áü•ËØÜ„ÄÅ‰ª£Á†ÅÊÄùË∑ØÂíåÂ∫îÁî®Ê°à‰æãÔºåÁúüÊ≠£‰ªéÈõ∂ÂºÄÂßãËÆ≤ÊòéÁôΩÔºÅÔºúÂø´ÈÄüÂÖ•Èó®Á≥ªÂàóÔºû - P4ÔºöL4- Á∫øÊÄßÂõûÂΩí  ÂÆåÊï¥È°πÁõÆÊºîÁªÉ
    - ShowMeAI - BV1TT4y1m7Xg
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/938d327f6471f60f92b4ed5bae0092f3_0.png)'
  prefs: []
  type: TYPE_IMG
- en: üéºÔºåHeyÔºå guysÔºå welcome to the next Tensorlow tutorial„ÄÇ In the last video„ÄÇ we learned
    how to create a neural network and then train and evaluate the model and make
    predictions„ÄÇüòäÔºåNow in this tutorialÔºå we implement our first real world project„ÄÇ
    so we deal with a regression problemÔºå and we learn how we load the data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: analyze the data and apply some pre processing„ÄÇAnd yeahÔºå as I said last time„ÄÇ
    we already used a deep neural network„ÄÇ and here we first take a step back and
    apply only a linear regression model„ÄÇ So only one layer„ÄÇ But at the endÔºå we extend
    this to againÔºå a deep neural network„ÄÇ And with this„ÄÇ you should get a better understanding
    of the Keerra stance layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and also of activationctuaation functions„ÄÇ So here I am in a twopyter notebook„ÄÇ
    And you don't need to worry about thisÔºå this just makes life a little bit easier
    for me to explain the code to you and show you the different steps„ÄÇ But you can
    code all of this in a normal python file„ÄÇ So let's start and first„ÄÇ we import
    the things we need„ÄÇ So againÔºå hereÔºå I silence some warnings„ÄÇ Then we import mappllip
    nuy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then this is new„ÄÇ So now we use pandas„ÄÇ and you can install this with Pip
    install pandas„ÄÇüòä„ÄÇSo this makes it very easy to work with data sets and analyze
    them and modify them„ÄÇAnd then here I set some print options to make the outputs
    a little bit nicer„ÄÇAnd then we import the things from Tenofflow„ÄÇ So againÔºå we
    import Tensofflow STf„ÄÇ Then we import Kas„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the last time I told you about the Kaas API and we already use the ks layers„ÄÇ
    And now this is new„ÄÇ So we also import from Tenorflowlow ks layers do experimental„ÄÇ
    we import preprocessing to apply a preprocessing layer to our data„ÄÇ So these are
    the imports we need„ÄÇ And now for the data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we are going to use this autompg data set„ÄÇSo this is a data set from the
    year 1983„ÄÇ and here we have different features and with this we predict thempg
    for a car„ÄÇ So how many miles a car can travel using one gallon of fuel and so
    here this is a very popular website where you find a lot of machine learning data
    sets„ÄÇ And now if you click on the data folder then here you get the link to the
    actual data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now here in the twopiter notebookÔºå this is the exact same URL„ÄÇ and I already
    wrote the different column names up here and then with pandas we can say pandas
    do read CSv So the ending here is not do csv but it's still in the CSv form„ÄÇSo
    we can use this and you can also use this method if you have a Csv file on your
    disk„ÄÇ So this works as well„ÄÇ So now let's load our data and then we can call data
    dot tail So this prints the last five columns So here we see thempg value that
    we want to predict and the different features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So for exampleÔºå we have the number of cylindersÔºå the displacementÔºå the horsepower„ÄÇ
    the weight acceleration model year and or written„ÄÇ So yeah„ÄÇ this is our data sets
    and now we clean our data set„ÄÇ So in here we have some missing numbers and we
    can very simply exclude them by saying data dot drop and a and then we also change
    this last column here because„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is actually a categorical valueÔºå and all of the other ones are numerical
    values„ÄÇ So this might confuse our model„ÄÇ So we want to change this in a one hot
    encoded data„ÄÇ so we can call data set dot pop or written to remove this„ÄÇAnd here
    in the origin„ÄÇ we have the different countriesÔºå USAÔºå Europe and Japan„ÄÇ So we can
    add them like this„ÄÇSo nowÔºå again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if we have a look at our tailÔºå then we see we removed the origin value and instead
    included one hot labels„ÄÇ So here for label 1„ÄÇ our it's the USA for label 2Ôºå it's
    Europe„ÄÇ Then againÔºå label 1„ÄÇ So again„ÄÇ USA has the one and so on„ÄÇ So now we have
    this„ÄÇAnd now we want to split our data into training and test sets„ÄÇ and we can
    do this by calling dot sample and then use a fraction here„ÄÇ So use 80% for training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we againÔºå drop the samples„ÄÇ So all of the samples„ÄÇ all of that we specify
    here are not included in the test set„ÄÇAnd then yeah„ÄÇ let's print print the shape
    and let's also describe our dataset set so we can very easily do this with pandas
    as well by calling the dot describe function„ÄÇ And then here we see we have our
    whole data set has 392 samples and then 10 different columns for now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then our test dataÔºå our training data is 80% so 314 and the rest is for
    testing„ÄÇAnd now if we call the describe functionÔºå then it analyze some statistics
    like the number of samples„ÄÇ the meanÔºå the standard deviationÔºå the min and max„ÄÇSo
    this might be useful to analyze it„ÄÇ And first of allÔºå now what we want to do is
    we want to split the features from the labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we want to this is our label that we predict MP„ÄÇ So first we make a copy„ÄÇ
    So we say these are our training features and our test features„ÄÇ And then we simply
    pop the MP G column from the training and testing data„ÄÇ And when we pop this„ÄÇ
    then this also returns the data„ÄÇ So this is now our training and testing labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now we have this„ÄÇThen here's a simple function to plot the data with mappl
    lip„ÄÇ and now let's simply plot one feature„ÄÇ So let's plot the horsepower feature„ÄÇ
    And we plot this against thempg value„ÄÇ So we see the more horsepower our car has
    also the less is the value of MPg„ÄÇ and this makes sense because the more power
    our gas has the more fuel it needs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the less is the MPg„ÄÇ And so then we also let's plot the weight feature„ÄÇ and
    this should have a similar distribution„ÄÇ So againÔºå the higher the weight of a
    car„ÄÇ the more fuel uses itÔºå and the less is the number of the MPg value„ÄÇSoÔºå yeah„ÄÇSo
    this is how our data looks like„ÄÇ And now if we go back to the describe function
    and have a look at the different mean values„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we see these all have different ranges and this is again„ÄÇ a very important
    issue that we have to consider„ÄÇ So this these different ranges if if we leave
    it like this„ÄÇ then it might confuse our model„ÄÇ So it's recommend to normalize
    the data first and to normalize„ÄÇ So let's print againÔºå let's describe our training
    data set and only the but let's print only the mean and the standard deviation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if we run this again then here we see the different means„ÄÇAnd now to normalize
    it„ÄÇ we used this normalization layer from the pre processingces module„ÄÇ So this
    one here that we imported„ÄÇSo let's code this„ÄÇ So here we create a normalization
    layer„ÄÇ So we say normalizer equals pre processing dot„ÄÇNormalization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this is a kas layer for the sequential API„ÄÇ And to call this„ÄÇ we have to
    adapt it to our to the data„ÄÇ So here we sayÔºå normalizer dot adapt„ÄÇ And then we
    want to adapt it to the training features„ÄÇ And right now„ÄÇ this is a panda data
    set so we can convert this to a nuy array„ÄÇ and then call the train fee„ÄÇCers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So what this is doing for nowÔºå this simply store calculates the mean and the
    variance of this training features and then stores them in the layers„ÄÇ So nowÔºå
    for exampleÔºå we can say print and then normalr dot mean dot nuy„ÄÇ and then if we
    run this„ÄÇThen we see here this„ÄÇ These are our mean valuesÔºå and these are the exact
    same numbers as we see here if we go this first column down„ÄÇSo this simply stores
    it„ÄÇ And now whenever we apply this layer to our data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then it normalizes the features such that it computes this this formula„ÄÇ So
    it subtracts the mean and then divides by the standard deviation„ÄÇ and this means
    that our output has a zero mean and unit variance„ÄÇ So here let's get some example„ÄÇ
    So the first data from the training features„ÄÇThenÔºå let's print this one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the first data unized„ÄÇ And then we also want to print the normalized
    one„ÄÇ So„ÄÇ let's print nor„ÄÇMaized„ÄÇ And then here we have to call this layer normalr
    with our data„ÄÇ So first„ÄÇ and then convert it to numpy„ÄÇ So this is how we convert
    from a tensor flow tensor to a nuy array„ÄÇ So let's run this„ÄÇ And then we see we
    have the„ÄÇThe first layer with different scales and ranges„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and now our normalized data„ÄÇ So they are all somewhere around0 with a unit standard
    deviation„ÄÇ So this is how we apply a normalizer preprocessing layer„ÄÇ And now let's
    tackle our regression problem„ÄÇ So in regression„ÄÇ So we somewhere we have this
    distribution that we know from the training data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and now when we get a new dataÔºå we want to so for example„ÄÇ here we get a new
    weight sample and then we want to predict how muchmpT we have for this car„ÄÇ and
    for thisÔºå we fit a function So a linear function with this formula so we can approximate
    it withmp times x plus B„ÄÇ So this is a„ÄÇBasically a line equation and we use this
    with the layers with the dense layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Sos better understand this„ÄÇ NowÔºå firstÔºå let's not use all of the different training
    features that we have„ÄÇ So all of these so let's just use one so that we can stay
    in the 2D case so in this case„ÄÇ for exampleÔºå its let's use the horsepower So here
    let's define our feature and this should be the horse power label so I will implement
    it like this so then you can just change the feature here and then you can try
    out different features so we can also for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: use weight here so all of these names you can use here as a feature and now
    let's get the single feature or the single data so let's„ÄÇCall this single feature
    equals Ny array„ÄÇ And then from our training features„ÄÇ and then we can access it
    with the feature„ÄÇ This will only return the horsepower feature„ÄÇSo let's print
    this„ÄÇ Let's print single fee„ÄÇFeature dot shape„ÄÇ And let's print train„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: features dot shape„ÄÇSo nowÔºå if we run this„ÄÇÂóØ„ÄÇHere I have a typo array„ÄÇLet's run
    it again„ÄÇ Then we see our single feature only has one feature„ÄÇ And in the whole
    training data„ÄÇ we have 9 different features„ÄÇ So then up here we„ÄÇCreated this preproces
    or normalization layer and then adapted it to all of the features„ÄÇ So basically
    we have to do the same thingÔºå but now adapt it only to the horse power feature„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's call this single feature normalizer then againÔºå our single feature
    normalizer„ÄÇ we call the adapt data and adapt it to the single feature single underscore
    feature„ÄÇ So now we have this and can run this code and here I have one parentheses
    too much„ÄÇ So again„ÄÇ So now we have this single feature„ÄÇ and now we create our
    sequential model as the last time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is very easy with the Kaas API„ÄÇÂóØ„ÄÇSoÔºå we sayÔºå our single„ÄÇFeature„ÄÇModel
    equals„ÄÇ And then we sayÔºå Kas dot„ÄÇModels dot sequential„ÄÇ And then here we use a
    list with all the different layers„ÄÇ So we use this as a first layer„ÄÇ So the single
    feature normalizer„ÄÇ And then we only use one layer„ÄÇ So one dense layer„ÄÇ and the
    output or also called units number of units is only one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is all that we need to build a linear regression model„ÄÇ So this is a
    linear regression or linear„ÄÇModel that applies exactly this formula„ÄÇ So it has
    some weights and multiplies it with our input„ÄÇ And it also has a bias„ÄÇSo this
    is all that we are doing here„ÄÇ So let's run this„ÄÇ and let's print the model summary„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we see we have our normalization layer and our dense layer and only
    five parameterss„ÄÇ5 parameters„ÄÇ So it's very simple„ÄÇ And now as a next step„ÄÇ the
    same as last time we define a loss and a optr„ÄÇ So for the lossÔºå we use ks dot
    losses„ÄÇ And in the case of a linear regressionÔºå we can use the mean absolutearrow„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is one possibilityÔºå we could also use the mean squaredarrow„ÄÇ So you
    can try out both„ÄÇüòä„ÄÇAnd so this is simply this one is doing the so the prediction
    y prediction minus the actual y„ÄÇ and then the absolute value and then sum it up
    over all samples and calculate the mean value and the mean square error is the
    same„ÄÇ except that it's using the square here and not the absolute value„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeah try that out for yourself„ÄÇAnd now the optimizer equals„ÄÇ So the same
    as last time Kaas dot optimizers dot„ÄÇ Let's use the Adamom optimizer„ÄÇ and we have
    to give it a learning rate„ÄÇ Let's try out point1„ÄÇAnd then after we have this„ÄÇ
    then we compile our model„ÄÇ So single feature model dot compileÔºå compile„ÄÇAnd here
    we give it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The optr equals the optimizer and„ÄÇOptim and the loss equals the loss„ÄÇ And yeah„ÄÇ
    last time we also gave it the metrics that we want to track So the accuracy„ÄÇ but
    the accuracy doesn't make sense here for the regression„ÄÇ So we leave it away„ÄÇ
    And then we simply see the loss later„ÄÇ So let's run this„ÄÇAnd now to train the
    model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we simply have to call model dot fit like the last time with our training features
    and here we only want the single feature that we define so the horse power in
    this case„ÄÇ but we still include all the training labels then we define the epochs
    then here I set verbose to one to see some logging and we can also define a validation
    split so this automatically takes 20% of the training data and then uses it for
    the validation data to tweak the hyperparameter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's train our model„ÄÇAnd nowÔºå training is done„ÄÇ And now we see that the
    loss decreased„ÄÇ and we also see the validation loss decreasing„ÄÇAnd at the endÔºå
    we have a training loss of 3„ÄÇ8 and a validation loss of slightly more 4„ÄÇ1„ÄÇ So
    it's not very bad„ÄÇ and by the way„ÄÇ if we call model fitÔºå then this returns the
    historyÔºå where it stores both of these losses„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we can assign it to a variableÔºå which I call history here„ÄÇ and then I simply
    plot the history„ÄÇ So then we can access these two losses by calling history dot
    history and then access the loss and the validation loss„ÄÇSo let's plot this and
    then we see that our losses decreasedÔºå so it'sÔºå it looks very good here„ÄÇAnd then
    to evaluate our modelÔºå we simplyÔºå like last time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: call model dot evaluate with our test features only with horsepower and the
    test labels„ÄÇSo let's run this„ÄÇ And this was very fast„ÄÇ So here we see„ÄÇ we have
    a final loss for the test data of 3„ÄÇ6„ÄÇ So not very bad„ÄÇ And now let's predict
    some samples„ÄÇ So in this caseÔºå I simply create„ÄÇTest data„ÄÇ So all the values between„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The min value and the max value and increase the range a little bit„ÄÇ You could
    also just use a hard coded number hereÔºå like from0 to 250„ÄÇ But I want to be it
    like a little bit more suited for the used feature„ÄÇ So we use this„ÄÇ And then to
    predictÔºå we call model dot predict„ÄÇ And then our new test data that we want to
    predict„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we plot it with our function from the beginning„ÄÇAnd this is the same
    plot as in the beginning„ÄÇ So we plot the horsepower against the MP7 value„ÄÇ And
    then these are our new x values that we predict„ÄÇ and we see that our prediction„ÄÇ
    So here we plot a line„ÄÇ and this is a linear line since we use a linear regression
    model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we see that it's not too bad„ÄÇ So we see the same trend„ÄÇ So the more horsepower
    our car has the lower the MP is„ÄÇ but for exampleÔºå here in this area„ÄÇ it's staying
    the same except that our horsepower is further increasing„ÄÇ So yeah„ÄÇ in this area
    and also maybe in this area areaÔºå it's not perfect„ÄÇ So but in the rest„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it looks it looks okay„ÄÇSoÔºå yeahÔºå so this is how we apply a linear model„ÄÇ and
    again„ÄÇ we only need the one dense layer with one output unit„ÄÇ and we also apply
    this normal normalization layer„ÄÇ So this is all we need to use linear regression„ÄÇ
    And now let's extend this to a deep neural network„ÄÇ So as I saidÔºå with only one
    dense layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can only use a or only get a linear linear function here like thisÔºå which
    is not perfect„ÄÇ So to further improve thisÔºå we can introduce more layers here
    and convert this to a fully feet forward neural net„ÄÇSo we simplyÔºå we still use
    the normalization layer and the dense layer at the end„ÄÇ But in the middle„ÄÇ we
    use some more dense layers„ÄÇ So let's sayÔºå layers dot dense„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then here you can try out different values for the hidden size„ÄÇ And as I
    said in the last time„ÄÇ we also apply activation functions for these layers in
    the middle„ÄÇüòäÔºåSo let's yeah„ÄÇ let's use the relu againÔºå like last time„ÄÇ And then
    let's use the same one again„ÄÇAnd yeah„ÄÇ and then at the endÔºå we use our dense layer
    with one output„ÄÇ And now this is all we need„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we converted our linear regression model to a deep neural network model„ÄÇ
    So let's use this„ÄÇ And then againÔºå we compile it with the loss and again„ÄÇ a atom
    optimizer and call the summary to see the different layers again„ÄÇ And then we
    see there is a lot more parameters that we have to train now„ÄÇ So now again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's call model fit with our horsepower feature and start the training„ÄÇüòäÔºåAnd
    training is done„ÄÇ And we see the final model or final loss is slightly better
    now„ÄÇ so slightly lower than before„ÄÇ So let's evaluate our model„ÄÇ And now it's
    below three„ÄÇ So it's definitely better„ÄÇ And now again„ÄÇ let's make some predictions
    and plot this„ÄÇ And now we see a new plot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we see that our function is no longer only linear„ÄÇ But here we also
    have some nonlinear areas„ÄÇüòäÔºåAnd this is the effects of the activation functions
    that we used„ÄÇ And that's why this is such an important thing in neural networks
    to apply activation functions„ÄÇ So here againÔºå we have this only our linear regression
    model with one dense layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we can only predict a linear function„ÄÇ And now with a deep neural network„ÄÇ
    we can get a non nonlinearar function as prediction„ÄÇ So yeah„ÄÇ I think that's all
    and now so we only used one feature for now so that we can stay in this 2D case„ÄÇ
    but of courseÔºå we can include all of the features and we do it the same„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we don't have to change anything actually„ÄÇ So we use this normalizer that
    we adapt it to all the„ÄÇMaining features„ÄÇ And here we have only our single dense
    layer„ÄÇ Then we compile this„ÄÇAnd then we fit it here to the all of the training
    features and train it and„ÄÇAnd now„ÄÇ training is done„ÄÇ And we see that the loss
    is also below 3„ÄÇ So training on all features makes it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: FurtherÔºå better„ÄÇ And now let's evaluate it as this last thing„ÄÇ And then we see
    we get the final loss„ÄÇSo yeahÔºå so you see you don't the code is actually the same„ÄÇ
    We just use all of the training features here and also for the normalizer„ÄÇ we
    adapted it to all of the training features„ÄÇ So yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think now you learned a lot in this tutorial how we can download and load
    CSv data and how we analyze it and preprocess it with the pandas framework and
    then how we use the normalization layer to normalize our data and then how we
    set up a a single linear regression model with this dense layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then againÔºå training and evaluation is the same like last time„ÄÇ And yeahÔºå
    and we also„ÄÇ you learned about the effect of the activation functions in a deep
    neural network„ÄÇAnd yeah„ÄÇ that's it for today„ÄÇ I hope you enjoyed this tutorial„ÄÇ
    And if you liked it„ÄÇ then please hit the like button and consider subscribing
    to the channel„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I hope to see you in the next videoÔºå bye„ÄÇüòä„ÄÇ![](img/938d327f6471f60f92b4ed5bae0092f3_2.png)
  prefs: []
  type: TYPE_NORMAL
