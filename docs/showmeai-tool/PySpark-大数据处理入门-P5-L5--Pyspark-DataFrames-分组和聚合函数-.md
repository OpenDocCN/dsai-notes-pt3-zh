# PySpark 大数据处理入门，P5：L5- Pyspark DataFrames 分组和聚合函数 

。

![](img/1babe07f0737b77a8ca240e4a3c70599_1.png)

大家好，我的名字是 Krisnaak，欢迎来到我的 YouTube 频道。所以伙计们，我们将继续 Pipar 系列，在这段视频中我们将看到分组和聚合函数。😊，我实际上已经创建了大约四个关于 Pipar 的教程，这基本上是第一个教程。这又是数据框的一部分。

我们为什么要再次使用分组聚合函数进行某种数据预处理。让我们开始处理这个特定的数据，为这个特定的问题我创建了一些具有三个特征的数据，比如名称、部门和工资，你有一些数据，比如 Kris 数据科学的工资。

对，类似这样的内容。因此在这里简而言之。如果我想基本了解这个特定数据，可能会有一些部门，其中 Kris 和其他人授课，基于不同的部门，他们获得不同的工资。所以让我们看看如何执行不同的分组和聚合函数，以及我们如何进行预处理或获取一些从这个特定数据中检索的结果。首先我们要做的就是导入 Pipar。

![](img/1babe07f0737b77a8ca240e4a3c70599_3.png)

![](img/1babe07f0737b77a8ca240e4a3c70599_4.png)

SQL 导入。Spark 会话，和往常一样，我们需要创建一个 Spark 会话。那么在这之后，我们要做什么呢？我将创建一个 Spark 变量。所以我会使用 Spark 会话，点构建器点。应用名称，我想大家一定对此很熟悉。但我再试着给你展示一下。所以让我把它写为 aggregate。点。获取或创建。所以现在我实际上已经创建了一个 Spark 会话。

好吧，如果我去检查我的 Spark 变量，这可能会花一些时间。所以这里是你所有的信息。关于这个特定的 Spark 变量。现在。让我们继续尝试读取数据集。现在，我将写 D F 下划线 pi spark。然后在这里我会写 Sp 点读。点 CSV，CSV 文件名基本上是 test。3 点 CSV。

记住，我会在 Github 上提供这个特定的 CSV 文件。然后我会使用 header 等于 true。逗号推断模式等于2。现在。这是我的 dear 下划线 osscope by spark。接下来我将在下一条语句中写 Df 下划线 cope pipar 点 show。此时你将能够看到我实际上可以看到所有的数据集，这里我有名称。

部门和工资在所有这些特定信息上。如果我真的想查看模式或列，例如哪些列属于什么。就像数据类型，我可以使用 DF 下划线 cope Ipar。点打印模式，现在你可以看到名称是字符串，部门是字符串，工资基本上是一个整数。

现在，让我们执行一些分组操作。首先，我们将从分组操作开始。可能我想按名称分组，并尝试查看平均薪水是多少。你知道吗，假设我们在这里拿一个具体的例子。所以我会写 Pf dot underscorecope bypar dot group by。

假设我想去检查在这个特定数据集中，谁的薪水是最高的。所以我首先按名字分组。如果我执行这个，你可以看到我们将得到一个返回类型为组数据的某个特定内存位置。

你应该始终知道，伙计们，group by 聚合函数是一起工作的。首先，我们需要应用 group by 功能。然后我们需要应用聚合函数。所以在这里，聚合函数我真的想要检查，只需按 dot 然后按 tab。在这里，你将能够看到很多不同的函数示例，比如聚合、平均、计数、最大值、平均值、π 以及很多模式。现在我将做的是使用这个 dot sum，因为我真的需要找出哪个是最高的薪水。

😊，然而在所有这些员工中，谁的薪水是最高的。所以我会说 dot sum。如果我执行它，你将能看到我们得到了一个 sQL dot data frame。里面有姓名和薪水的总和。这非常重要，因为我真的想要薪水的总和。记住，我们不能对字符串应用 sum。这就是为什么它在这里没有执行的原因。

它只是给你名字，因为我们按名字分组。这个 dot sum 将只应用于这个特定的薪水。现在，如果我去写 dot show，你将能够看到。😊Sudanhu 在这里的薪水是最高的 35000。Sunny 有 12000。Krissh 有 19000。

Mahesh 的薪水是 7000。所以如果你去看这里，Sudanhu 基本上在这里。在大数据中。所以如果你计算，总体他的薪水应该是 35000。类似地，你可以计算我在这里的薪水。🤧在这里通过简单计算这一点。你还可以计算 Sunny 的薪水。你也可以看到我的 H。

所以这只是一个例子。在这里我会写一些内容。我们已经进行了分组。做得很好。最高薪水。并且在整个观察中可以确定。我们可以得出结论，Sudanhi 的薪水是最高的。好吧，现在让我们向前迈一步。再向前一步。现在我们将尝试按部门分组，以找出哪个部门提供最高薪水。好的。

我们将进行一个按部门分组的操作。哪个部门提供最高薪水。假设这是我的。这个是我的需求。好的，和不同的需求组合在一起。我只是想给你展示一些例子。所以我将复制这个。南将会做。使用这个部门。好的，然后我基本上将说 dot sum dot show。

如果我执行它，让我看看，部门是一个错误的列名，所以我会写部门，它是部门。所以让我现在写 S。如果我去看看，IOT 在这里给这所有员工的薪资大约是 115000，因为我们在进行总和，大数据给的薪资大约是 15000，数据科学的薪资是 4300。现在假设我去看看大数据这里是 404080008000 和 13000。

130015000，所以我希望我得到的答案是，是的，大数据实际上给我们 15000，所以你可以去计算，假设你想找出平均值，你也可以找到平均值。好吧，让我在这里写，只需复制这个整体内容，粘贴在这里，写我，写而不是总和，我会写平均值，所以默认的平均薪资。

在这里你可以看到，对于某个特定员工，I O T 的薪资大约是 7500。因为这个平均值将基于在该部门工作的人员数量，对吧。所以这样你实际上可以找到。现在，我还可以检查一件事。伙计们，我可以复制这个。我可以尝试找出有多少员工实际上是基于部门工作的。

所以我可以使用 dot count。如果我去执行这个，可能这就是一个方法。好的。现在你会看到在 I O 中有两个人，在大数据中有四个人，在数据科学中也有四个人。所以 4 加 4 加 8，总共有的员工在这里基本上是 10。现在，我还可以直接应用聚合函数来查看。

这些都是一些示例，再次强调，你可以通过让我使用 Df Ppar 进行不同的分组。假设我说 dot aggregate，好吧，在里面我只需提供我的键值对。假设我说，让我找出工资的总和。我想要找出整体的薪资总支出。

所以你将看到总支出大约在 73000 左右。我们也可以直接应用聚合函数，此外，这些都是在应用分组函数后我们基本上应用的聚合函数。现在假设这些可能是薪资，我想要找出假设我举这个例子，我想找出获得最高薪资的人的薪资，抱歉。

所以在这里，我将写 max dot show，而不是 dot sum。现在你可以看到 Sudansha 在这里获得 20000，Krisish 获得 10000，我获得 4000，对吧。所以所有这些特定数据是 Kris 在数据科学中获得的薪资为 10000，所以它基本上挑选了，并没有选择两个记录，但至少当它按名称分组并显示这个特定数据时，你将能够看到。让我们看看我是否也能看到这一点。

所以分组，如果我评分并写下最小值。这里你将能够看到不同记录下的最小值。当我分组时，你将看到Suanhu的最低薪资是5000，而Kris的最低薪资是4000。

我们还可以获取那个特定的信息。现在让我们看看有哪些不同类型的操作。平均值也在其中。所以如果我写AVG，这就和平均数一样。基本上这是可能的平均薪资。你可以查看这些功能，了解这些操作的必要性。

一件重要的事情是，你确实需要进行大量的数据预处理和检索技能。你可以查看这个，并根据自己的需要执行不同的功能。所以我希望你喜欢这个特定的视频。可能在下一个视频中，我将开始介绍Spark Mlib库，我们将解决一些机器学习算法问题。

![](img/1babe07f0737b77a8ca240e4a3c70599_6.png)

所以我希望你喜欢这个特定的视频，下周见。祝你有美好的一天，谢谢。
