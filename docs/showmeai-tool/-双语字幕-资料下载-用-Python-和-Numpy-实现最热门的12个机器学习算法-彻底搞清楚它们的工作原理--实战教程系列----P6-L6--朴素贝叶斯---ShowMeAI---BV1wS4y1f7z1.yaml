- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P6ÔºöL6- Êú¥Á¥†Ë¥ùÂè∂ÊñØ
    - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to a new machine learning from scratch tutorial„ÄÇ Today„ÄÇ
    we are going to implement the naive by classifier using only built and Python
    modules and Ny„ÄÇüòä„ÄÇSo the naive bias classifier is based on the biaseth theoremÔºå
    which says that if we have two events„ÄÇ A and BÔºå then the probability of event
    AÔºå given that B has already happened„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: is equal to the probability of BÔºå given that A has happened„ÄÇ times the probability
    of a divided by the probability of B„ÄÇAnd if we apply this to our case„ÄÇ then our
    formula is the probability of y of our class Y„ÄÇ given the feature vector X is
    equal to the probability of x given y times P of y divided by P of x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And where x is our feature of vectorÔºå which consists of several features„ÄÇAnd
    now it's called naive by years because now we make the assumption that all features
    are mutually independent„ÄÇ which meansÔºå for exampleÔºå if you want to predict the
    probability that a person is going out for a run„ÄÇGiven the feature that the sun
    is shining and alsoÔºå given the feature that the person is healthy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then both of these features might be independentÔºå but both contribute to this
    probability that the person goes out„ÄÇSo in real lifeÔºå a lot of features are not
    mutually independent„ÄÇ but this assumption works fine for a lot of problems„ÄÇAnd
    with this assumptionÔºå we can split„ÄÇThis„ÄÇProbability into the and use the chain
    rule„ÄÇ So we calculate the probability for each„ÄÇFeature„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Given why and multiply each„ÄÇAnd then we multiply it with p of y and divided
    by p of x„ÄÇAnd by the way„ÄÇ this P of y given x is called the posterior probability„ÄÇ
    P of x given y is called the class conditional probability„ÄÇ and P of y is called
    the prior probability of yÔºå and P of x is called the prior probability of x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we want to make a classification„ÄÇ So given this posterior probability„ÄÇ
    we want to select the class with the highest probability„ÄÇ So we choose yÔºå which
    is the arc max of y„ÄÇOf this posterior probability„ÄÇAnd now we can apply our formula„ÄÇ
    And since we are only interested in YÔºå we don't need this P of x so we can cross
    this out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then our formula is thisÔºå so„ÄÇWhy is the arc max of„ÄÇAnd then we multiply
    each class conditional probabilityÔºå and then the prior probability„ÄÇAnd then we
    use a little trick„ÄÇSince all these values are are probabilities between 0 and
    1„ÄÇ So if we multiply a lot of these valuesÔºå then we get very small numbers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we might run into overflow problems„ÄÇ So in order to prevent thisÔºå we apply
    the lock function„ÄÇ So we apply the lock for each of these probabilities„ÄÇ and with
    the lock or the rules for logarithmussÔºå we can change the multiplication sign
    into an a plus sign„ÄÇ So now we have an addition here„ÄÇAnd now we have this formula
    that we need„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and now we need to come up with this„ÄÇAP probabilityÔºå So the prior probability
    is just the frequency„ÄÇWe can see this in a second„ÄÇ And then what is this class
    conditional probabilityÔºå P of x given y„ÄÇAnd here we model this with a gausian
    distribution„ÄÇ So here we can see the formula„ÄÇ So this is one over„ÄÇ and then the
    square root of2 pi times the variance of y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Times the exponential function of minus x minus the mean value squared divided
    by two times the variance„ÄÇAnd here we see a plot of the Gaussian function for
    different means and variations variances„ÄÇSo this is a probability that is always
    between0 and one and„ÄÇYeahÔºå with this formulas„ÄÇ this is all we need to get started„ÄÇ
    So now we can„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Start and implement it„ÄÇ And first of allÔºå of courseÔºå we import Ny S N P„ÄÇAnd
    then we create a class called naive„ÄÇBut is„ÄÇAnd it doesn't need an in it method
    so we can implement the fit method first„ÄÇ so we want to fit training data and
    training labels„ÄÇ And then we also want to implement a predict method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we predict the test labels or test samples„ÄÇAnd now let's start„ÄÇ So let's
    start with the fit method„ÄÇ So what we can do here„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Is so we need the priorsÔºå and we can calculate them in this fit method„ÄÇ and
    we need the class conditional„ÄÇ So here we need the mean for each class and the
    variance for each class„ÄÇ So we can also calculate these„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's do this„ÄÇ So let's get the number of samples and the number of features
    first„ÄÇAnd by the way„ÄÇ our input hereÔºå the X is a nuy N D array where the first
    dimension is the number of samples„ÄÇ and the second dimension or the number of
    rows is the number of samples and the number of columns is the number of features„ÄÇ
    so we can unpack this and say this is x dot shape„ÄÇAnd our y is a 1 D row vector
    also of size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the number of samples„ÄÇ So this is our input„ÄÇ And now let's get the unique classes„ÄÇ
    let's say self classes equals nuy unique of y„ÄÇ So this will find the unique elements
    of an array„ÄÇ So if we have two classesÔºå0 and 1„ÄÇ then this will be an array„ÄÇJust
    with 1Ôºå0 and 1 y in it and1„ÄÇ1 in it„ÄÇThen let's say the number of classes equals„ÄÇü§¢ÔºåThe
    length of this self„ÄÇClasses„ÄÇAnd now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's in it or„ÄÇIn it mean„ÄÇVariarianance and priors„ÄÇ So let's say self dot mean
    equals„ÄÇ And we want to in them with zeros at first„ÄÇ And it gets the size„ÄÇ It has
    number of classes and number of features as tuple here„ÄÇ So it also„ÄÇFor each„ÄÇClass„ÄÇIt
    has„ÄÇThe same number of„ÄÇOr for each class„ÄÇWe need„ÄÇMeans for each feature„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we want to get this or give this a data type of float nuy dot float 64„ÄÇAnd
    we want to do this with the same for the variances„ÄÇ So let's say self do„ÄÇÁé©ÂÑø„ÄÇEquals
    this„ÄÇ And then we want to do self dot„ÄÇPriers equals N dot zeros„ÄÇ And here for
    each class„ÄÇ we want one prior„ÄÇ So this is just a 1 D vector of size number of
    classes with a data type of N dot float 64„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now let's calculate them„ÄÇ So for each class in self dot classesÔºå we„ÄÇNow„ÄÇ
    only we only want the samples that has this class as labels„ÄÇ So let's call this
    XÔºå C equals X„ÄÇ and then where„ÄÇC equals equals„ÄÇWhy„ÄÇAnd now we can calculate the
    mean for each class and fill our self do mean„ÄÇ So we want to fill„ÄÇThis„ÄÇRow and
    all columns here„ÄÇ And we sayÔºå this is x„ÄÇXÔºå C„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: dot and nuly has a or a N D„ÄÇAray has to built in mean functions„ÄÇ So we can say
    mean along the axis0„ÄÇ So please check the mean„ÄÇFunction for yourself„ÄÇAnd we want
    to do the same thing for var„ÄÇ So self var in this row for each column„ÄÇIs X C dot
    var„ÄÇ So Numpy also has a var method„ÄÇAnd then we calculate the priorÔºå So self„ÄÇFriers„ÄÇOf
    this class„ÄÇIs equal to„ÄÇ And now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what information do we already have„ÄÇIf we have the the training samples and
    training labels„ÄÇ so we can say the prior probability that this class will occur
    is equal to the frequency of this class in the training samples„ÄÇSoÔºå we say„ÄÇWe
    get XÔºå C„ÄÇThe shape„ÄÇ0Ôºå so only„ÄÇThis will get the number„ÄÇOf samples with this label„ÄÇAnd
    then we divide it by the number of total samples„ÄÇ So„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we have to convert this to a float because we don't want integers here„ÄÇ
    So let's say float number of samples„ÄÇSo this is the frequency„ÄÇ How often this
    class C is occurring„ÄÇAnd now this is all for our fit method„ÄÇ And now let's implement
    the predict method„ÄÇ So and for this„ÄÇ we create a little helper method„ÄÇ So let's
    call this underscore predict self„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this will only get one samples„ÄÇ So here we have we can have multiple samples„ÄÇ
    So here we say why predict equals and then we use list comprehension„ÄÇAnd called
    this underscore„ÄÇ predict method„ÄÇFor only one sample„ÄÇ And then we do this for each
    sample in our test samples„ÄÇAnd thenÔºå we return them„ÄÇAnd now we have to implement
    our underscore predict method„ÄÇSo here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what we need now is we need„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: To apply this function„ÄÇSo„ÄÇWe have to calculate the posterior probability„ÄÇAnd
    calculate the„ÄÇClass conditional and the prior for each one„ÄÇ And then choose the
    class with the highest probability„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's create an empty list called posteriors„ÄÇEquals an empty list„ÄÇAnd now
    let's go over each class„ÄÇSo let's say for index and C in„ÄÇEnumerate„ÄÇSelf dot classes„ÄÇ
    So here we get also the index„ÄÇAnd the class ladleÔºå with this enumerate function„ÄÇAnd
    now we can say„ÄÇFirstÔºå we get the pia„ÄÇ So the pia equalsÔºå and we already have calculated
    the p„ÄÇ So this is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The prior of self dot„ÄÇPriers„ÄÇWith this in current index„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: And nowÔºå as I saidÔºå then at the endÔºå we apply the lock function„ÄÇ So let's apply
    this right here„ÄÇ So let's say NP P dot lock„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: And then create the posterior„ÄÇSoÔºå the posterior equals„ÄÇonÔºå let's call this„ÄÇClass„ÄÇConditional„ÄÇEquals„ÄÇAnd
    then we apply the gause functions„ÄÇ So for this„ÄÇ let's create this helper function
    and call this probability density function„ÄÇWith self„ÄÇ And then it gets„ÄÇThe class
    index„ÄÇAnd then it gets x„ÄÇAnd here we apply this formula here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_15.png)'
  prefs: []
  type: TYPE_IMG
- en: So we need the mean and the variance„ÄÇAnd then„ÄÇÂóØ„ÄÇApply this function„ÄÇ SoÔºå first
    of all„ÄÇ let's get the mean equals„ÄÇ and we already have the mean„ÄÇ so we can say
    self„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_17.png)
  prefs: []
  type: TYPE_NORMAL
- en: Mean„ÄÇOf this class index and also the variance equal self„ÄÇVre of this class
    index„ÄÇAnd then let's create a de numeratorÔºå which is equals to nuumpy dot x„ÄÇSo„ÄÇ
    the exponential function of minus„ÄÇAnd then we have x minus mean„ÄÇTo the power of
    two„ÄÇDivided by„ÄÇAnd thenÔºå two times„ÄÇThe variances„ÄÇAnd thenÔºå we have the„ÄÇDnominatorÔºå
    sub denominator equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we have N dot„ÄÇS„ÄÇCo are Ts over the square root of„ÄÇTwo times„ÄÇ And we
    have pie„ÄÇ We can also get this from NamiÔºå and Peter pie„ÄÇTimes thevaÔºå and then
    we return„ÄÇNummerator„ÄÇ divided by„ÄÇDo you know me Na„ÄÇSo this is our probability
    density function„ÄÇ And then we can say our class conditional„ÄÇIs„ÄÇÂóØ„ÄÇSelf„ÄÇDot„ÄÇProbability
    density function of our index„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And x„ÄÇAnd then„ÄÇÂóØ„ÄÇWe want to„ÄÇWe want to have the logo rimore„ÄÇ So we have says
    N dot lock„ÄÇAnd then„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: We sum all of them up„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå we can use„ÄÇNampy dot some of this„ÄÇÂóØ„ÄÇAnd then we say our post„ÄÇTior equals„ÄÇPyaÔºå
    plus„ÄÇThe class conditions„ÄÇOur class conditional„ÄÇAnd then we have penned them to
    our posterior„ÄÇ So posteriors dot append posterior„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_23.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now we use or we apply the arc max of this and choose the class with the
    highest probability„ÄÇ So Ny also has a arc max function„ÄÇ So this is very easyÔºå
    so we can now say return self dot„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_25.png)
  prefs: []
  type: TYPE_NORMAL
- en: Classes„ÄÇOffÔºå and the index is now„ÄÇËøô„ÄÇIndex of the„ÄÇPoeriors with the highest„ÄÇProbability„ÄÇSo
    we can say„ÄÇ N dot Arc max„ÄÇOf this poster„ÄÇAnd nowÔºå we are done„ÄÇSo this is the whole
    implementation„ÄÇAnd„ÄÇNow we can run it„ÄÇ So I already have a little„ÄÇTest script where
    I use the p Psych could learn library to load a data set and create a data set
    with 1000 samples and 10 features for each sample„ÄÇ And we have two classes„ÄÇ Then
    we split our data into training and testing samples and the labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we create our naive bias classifierÔºå which I import from this file„ÄÇThat
    I have here„ÄÇAnd then I fit the training data and the training labels„ÄÇ and then
    I predict get the predictions for the test samples„ÄÇAnd then I will calculate the
    accuracy„ÄÇ So let's run this and see if this happenÔºå if this is working„ÄÇSo yetÔºå's
    working„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we have a classification accuracy of 0„ÄÇ96„ÄÇ So pretty good„ÄÇSo yeahÔºå that's
    it„ÄÇ I hope you enjoyed this tutorial and see you next timeÔºå bye„ÄÇ![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_27.png)
  prefs: []
  type: TYPE_NORMAL
