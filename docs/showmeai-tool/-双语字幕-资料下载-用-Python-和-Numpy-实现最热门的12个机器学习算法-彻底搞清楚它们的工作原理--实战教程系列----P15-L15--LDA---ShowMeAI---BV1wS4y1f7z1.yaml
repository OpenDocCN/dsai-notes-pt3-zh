- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P15ÔºöL15-
    LDA - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HeyÔºå guysÔºå welcome to a new machine learning from Sc tutorial„ÄÇ Today„ÄÇ we are
    going to implement the linearar discriminant analysis algorithm or Sha LD using
    only Python and numpy LDA is a dimensionality reduction technique and a popular
    preprocessing step in machine learning pipelines„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: LDA is similar to the PCA technique that I already showed in a previous tutorial„ÄÇ
    The approach and the implementation of PCA and LDA have a lot in common„ÄÇ So I
    highly recommend that you watch this video first„ÄÇ And now let's talk quickly about
    the concept of LDA before we jump to the code„ÄÇ So the goal„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: as I already saidÔºå is feature reduction„ÄÇ So we want to project our data sets
    onto a lower dimensional space and find a good class separation„ÄÇüòäÔºåSo here we have
    the difference between PCA and LEAÔºå so in PCA or principal component analysis„ÄÇ
    we want to find new a onto which we project our data such that we maximize the
    variance on the new axis„ÄÇAnd now in L AÔºå the big difference is that we know the
    feature labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is a supervised technique„ÄÇAnd here we want to find new axis such such
    that the class separation is maximized„ÄÇ So if you have a look at this image hereÔºå
    we have two different classes„ÄÇ and then we could project our data either onto
    the Y axis or onto the X axis„ÄÇ And now in this case„ÄÇ the Y axis would not be a
    good choiceÔºå but the X axis is a good choice because here we still have a good
    class separation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the concept of the LDA„ÄÇAnd here I listed the differences again between
    PCA and LDA„ÄÇ So in PCA againÔºå we want to find the component axis that maximize
    the variance of our data„ÄÇ and in LDAÔºå we want to do this two„ÄÇ So within one class
    within the green field and within the blue field„ÄÇ we still want to have a good
    variance between the single features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but also additionally here we are interested in the axis that maximize the separation
    between multiple classes„ÄÇ So this difference here basically should be maximized
    in the new axis„ÄÇAnd yeah„ÄÇ LDA is supervised learning„ÄÇ So we know our labels and
    PCA is unsupervised„ÄÇ So this is an important thing that we should remember„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now let's jump to the math here we have the socalled scatter matrix„ÄÇ and
    we have two different scatter mattressesÔºå the within class scatter and the between
    class scatter„ÄÇ this basically represents what I was talking about here„ÄÇ So the
    within class scatter makes sure that our features within one class are good separated„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and the between class scatter makes sure that the two or all the class are good
    separated„ÄÇ And if we translate this to the math and we have to deal with the mean
    values and the variances So the formula of the within class scatter is the sum
    over the scatters and each scatter of one class is the sum over and then the feature
    value minus the mean value of all the feature„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Only in this class and then times the same transposed„ÄÇ and then we sum over
    all the features in this class„ÄÇ So this is basically the same as in the PC algorithm
    where we want to compute the covariance matrix„ÄÇ So this is almost the same formula
    is for the covariance matrix„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: except that we don't have the scaling at the beginning„ÄÇ So this is the within
    class getter„ÄÇ and then the between class getter hereÔºå the formula is the sum over
    all the class„ÄÇ and then for each classÔºå we have the number of features in this
    class or sorry„ÄÇ the number of labels in this class„ÄÇ and then times the mean value„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this x bar is the mean the mean value of the features in this class minus
    the mean value in total„ÄÇ So the total„ÄÇOf all features„ÄÇ and then times the same
    transposed„ÄÇ So these are the two matrices that we have to compute„ÄÇ and then we
    calculate the inverse of the within class scatterter and multiply that with the
    between class scatterter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is our eigenvalue and eigenvector problem that we have to solve„ÄÇ So
    this is the same as in the PC„ÄÇ I will not go into detail again„ÄÇ So please make
    sure that„ÄÇ you know what eigenvalues and eigenvectors are„ÄÇ So basically„ÄÇ what
    we have to do then is for this formula„ÄÇ We have to calculate the eigenvalues„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then so let's have a look at the whole approach again here„ÄÇ So here I summarize
    it„ÄÇ So first we want to calculate the between class scatterter and the within
    class scatterter„ÄÇ then here we calculate the inverse of the within class scatter
    and multiply it with the between class scatter„ÄÇ Then of this we calculate the
    eigenvectors and eigenvalues„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we sort the eigenvectors according to their eigenvalues in decreasing order
    and then we choose only the first k eigenvectors that we specified„ÄÇ So only the
    k dimensions that we want to keep and these eigenvectors are called the linear
    discriminants that's why it has this name and then we transform our original data
    points onto this k dimensions and this transformation is basically just a project„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: With the dot products„ÄÇ So this whole approach is the same as in the PC A algorithm„ÄÇ
    except that we have to solve the eigenvalueian eigenvector problem for a different
    formula in the beginning„ÄÇ So that's the approach„ÄÇ And now let's jump to the code„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå of courseÔºå we import nuy S N P„ÄÇ and then we define our class„ÄÇ And let's
    call this LD A„ÄÇ And here we define our in itÔºå which has self„ÄÇ And it also gets
    the number of components that we want to keep„ÄÇ And here we simply store it„ÄÇ So
    we say self and„ÄÇComponents equals n components„ÄÇ And we also create a variable
    that we call self dot linear this„ÄÇPreriminence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this is none in the beginning„ÄÇ And here we want to store the eigenvectors
    that we compute„ÄÇAnd then we define our fit method„ÄÇ So here we have selfÔºå and then
    we have XÔºå and we also have y„ÄÇ because rememberÔºå this is a supervised technique„ÄÇAnd
    then we also implement not the predict method„ÄÇ but we call it transform„ÄÇ So transform„ÄÇThis
    is the same as in the PCA„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we want to get the new features that we want to project„ÄÇSo let's implement
    the fit method„ÄÇ So hereÔºå firstÔºå what we want to get is the number of features„ÄÇ
    and we get this by saying x dot shape and then the index1„ÄÇ So index0 is the number
    of samples„ÄÇ And here we only want to have the number of features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we also want to get all the different class labels„ÄÇ So let's call this
    class labels„ÄÇ and this is equal to nuyÔºå and then we can apply the unique function
    of y„ÄÇ So this will only only return the unique values in our labels as a list„ÄÇ
    And now we want to calculate the two scatter mattresses„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So S underscore W for the within class gather and S underscore B for the between
    class„ÄÇ So let's do this„ÄÇ And first of allÔºå I want to calculate the mean of all
    our samples because we need this„ÄÇfor one of the formulasÔºå we say mean„ÄÇOverallÔºå
    equals numpy dot mean of x„ÄÇ And then along the axis 0„ÄÇ And then let's initialize
    our two mattresses„ÄÇ So we say S W or S underscore W equals nuy zeros„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we want to fill this with zeros„ÄÇ And we want to give this a size of the number
    of features times the number of features„ÄÇAnd the same thing with the between class
    scatter„ÄÇ So we initialized this with zeros„ÄÇ So later„ÄÇ we want to test thisÔºå for
    exampleÔºå with the features of the iris data set„ÄÇ So this has a size of„ÄÇ this hasÔºå
    I think it's 150 samples and four features„ÄÇ So this has size 4 times 4„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is the same  four times 4„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now we have to apply the two formulas„ÄÇ So we have to sum over all the classes
    and then apply these two formulas So we can do this in one for loop„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: So we say for C in class labels that we computed„ÄÇ And then what we want to get
    first is we want to get only the samples of this class„ÄÇ So we say XÔºå C equals
    XÔºå where Y equals equals C„ÄÇ So where we have this label in the current iteration„ÄÇAnd
    then we want to get the mean from these features„ÄÇ mean C equals„ÄÇ And this is Ny
    dot mean of X C along x is 0„ÄÇ So the same as we are doing it here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but only for the features in this class„ÄÇAnd then let's have a look at thereÔºå
    within class„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Forula„ÄÇSo hereÔºå here we have our feature and then subtract the mean value„ÄÇ And
    then this is basically the dot product times the transposed„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: And so let's do this„ÄÇ So here we say our S within plus equals„ÄÇ because here
    we sum over all the classes„ÄÇ So plus equals„ÄÇ And then here we say x C„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6d1044d6beca5a8660b61779287752d_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Minus mean C„ÄÇ And then I transpo this and calculate the dot product times the
    same as we are doing it here„ÄÇSo„ÄÇHere we have to be careful„ÄÇ So if we have a look
    at the formula again and we see that I have to transpo term at the end„ÄÇ and here
    I transpose the first term„ÄÇ And this is because here we are having one more sum„ÄÇ
    So we do this for all the samples in this class„ÄÇ And here we do this sum in one
    operation with the dot product„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So with our numpy operation„ÄÇ And then we have to be careful with the sizes„ÄÇ
    So what we want at the end again is a four times4 matrix like here„ÄÇ because we
    appendice to these mates„ÄÇ And in the beginning„ÄÇ our X C and our mean C has the
    size number of samples in this class times 4„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6d1044d6beca5a8660b61779287752d_14.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/c6d1044d6beca5a8660b61779287752d_15.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/c6d1044d6beca5a8660b61779287752d_16.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/c6d1044d6beca5a8660b61779287752d_17.png)'
  prefs: []
  type: TYPE_IMG
- en: So we have to turn this around„ÄÇ So we have to say this is size 4 times number
    of samples in this class„ÄÇBecause when we multiply this or when we compute the
    dot product with this one here„ÄÇ which is not transpoposedse„ÄÇ So here we have the
    number of samples in this class times 4„ÄÇ And then if we multiply this„ÄÇ then we
    get a matrix of the size 4 times 4„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So these are basic rules of matrix operations„ÄÇ be sure that you understand this„ÄÇ
    So the last dimension of the first matrix must match the first dimension of the
    second matrix„ÄÇ And then the final output size is composed of these two sizes„ÄÇ
    So this is why we have to transpose the first term here„ÄÇ So this might be a little
    bit confusing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: make sure to double check this for yourself„ÄÇAnd then we have the within class
    scatter„ÄÇ And now for the between class scatterÔºå what we want to get is the number
    of samples in this class„ÄÇ We get this N C by saying this is equal to XÔºå C dot
    shape„ÄÇ And here we want to have the index0 because we want to have the number
    of samples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then here againÔºå we have to be careful because we have to reshape our vector„ÄÇ
    So let's say our mean div„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at the formula again„ÄÇ Here„ÄÇ We calculate the mean of this
    class minus to total means„ÄÇ OhÔºå let's do this„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: So this isÔºå let's say we have the mean of this class minus the mean over„ÄÇAnd
    this is only one dimensionalÔºå but we want to so this isÔºå if we have a look at
    the shape„ÄÇ then this would say four comma nothing„ÄÇ but we want to have it to be
    4 by one„ÄÇ So we have to say reshape„ÄÇ And then the number of features times or
    by one„ÄÇAnd this is because„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: againÔºå if we have a look at the final multiplication„ÄÇ So the same way as we
    are doing a tier„ÄÇ we want to have a matrix of size 4 by one and multiply it with
    a matrix of one by 4„ÄÇ So this is basically4 by one transposed„ÄÇAnd then we get
    a  four by four output„ÄÇ So this is why we have to apply the reshape here„ÄÇ And
    then we say S B„ÄÇPlus equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then here we have the number of samples in this class times„ÄÇ And here we
    have the mean the„ÄÇDot„ÄÇ the mean diff„ÄÇTransposedÔºå and these are both of our matrices„ÄÇ
    So we finally have the matrices now„ÄÇ And nowÔºå as I saidÔºå we have to get the inverse
    of the within class get and then multiplied with the between class get„ÄÇ So we
    get the inverse„ÄÇ Also inverse also with numpy by saying nuy L alk„ÄÇDot in of S
    W„ÄÇ And then dot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we multiplied with the between class scatter„ÄÇ And let's call this a and store
    this in this matrix„ÄÇ And then for thisÔºå we have to solve the eigenvalue and eigenvector
    problem„ÄÇ So we have to calculate the eigenvalues and eigenvectors„ÄÇ And now the
    following code is exactly the same as in the PC A algorithm„ÄÇ So please check that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we get the eigenvalues and the eigenvectors by sayingÔºå this is numpy„ÄÇLin
    Ark dot Ig of a„ÄÇAnd then we sort the eigenvectors and the eigenvalues„ÄÇ And for
    this„ÄÇ the same as we are doing it in the PC algorithm„ÄÇ So we transpose the eigenvectors
    by saying eigenvectors equals eigenvectors dot T„ÄÇ So this makes the calculation
    easier And then we sort the eigenvalues„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we say indices equals nuy dot arc sort off„ÄÇ And here we say the eigenvalues„ÄÇ
    and to make it a little bit nice„ÄÇ So we actually want the absolute value of the
    eigenvalues„ÄÇAnd then we want to sort this in decreasing order„ÄÇ So we use this
    slicing and use this little trick from start to end with a step of -1„ÄÇ So this
    will turn the indices around„ÄÇ And then we have it in decreasing order„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now let's get our eigenvalues in decreasing order by saying eigenvalues equals
    eigenvalues of these indices and the same with the eigenvectors or eigenvectors
    equals eigenvectors„ÄÇ of this indices„ÄÇ And then we want to store only the first
    n eigenvectors„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we store this in our linear discriminants that we have here„ÄÇ So we say self
    dot„ÄÇLinear discriminants equals eigenvectors„ÄÇ And then from the start„ÄÇ So the
    biggest eigenvector with the biggest or the highest eigenvalue„ÄÇ And then two self
    dot number of components that we specified„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the number of dimensions that we keep„ÄÇAnd now we are finally done
    with the fit method„ÄÇ So this is the whole fit method„ÄÇ and then under a transform„ÄÇ
    the only thing that we do here is we project our data onto this new components„ÄÇ
    and the transformation is nothing elseÔºå then the dot product„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we can write this in one line and return nuy dot„ÄÇ and then we project our
    data onto the self dot linear discriminants„ÄÇ And since we are transposing it hereÔºå
    we have to transpose it again here„ÄÇ And then we are done„ÄÇSo this isÔºå againÔºå the
    same as in the PCA„ÄÇ Please double check this for yourself„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and and now we are done and now we can run the script„ÄÇ So here I have a little
    test script„ÄÇ and this is basically the same as in the PCA tests„ÄÇThe only thing
    that I exchange here is instead of PC„ÄÇ we create the LEA and want to keep two
    componentsÔºå and then we call the fit and the transform„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we do this for the Iis data setÔºå and then I plot the new labels that are
    project projected onto the new two dimensions„ÄÇSo let's run this„ÄÇ So let's say
    Python LDA„ÄÇUnderscore test up pie„ÄÇ and hope that everything's working„ÄÇ And yeahÔºå
    so here we see our transposed features in only two dimensions now„ÄÇ And we see
    that the classes are very goodÔºå separated„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we have the three different iris classes„ÄÇ and we see that this is working„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_23.png)
  prefs: []
  type: TYPE_NORMAL
- en: So our LDA feature reduction method works„ÄÇ And yeahÔºå pleaseÔºå again„ÄÇ compare
    this with the PCA algorithm„ÄÇ And I hope you enjoyed this tutorial„ÄÇ If you like
    this„ÄÇ then please subscribe to the channel and see you next timeÔºå bye„ÄÇüòä„ÄÇ![](img/c6d1044d6beca5a8660b61779287752d_25.png)
  prefs: []
  type: TYPE_NORMAL
