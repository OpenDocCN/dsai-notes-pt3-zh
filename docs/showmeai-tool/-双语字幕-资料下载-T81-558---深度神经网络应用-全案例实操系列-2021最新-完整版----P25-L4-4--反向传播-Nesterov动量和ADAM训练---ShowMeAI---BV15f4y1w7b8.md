# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P25ÔºöL4.4- ÂèçÂêë‰º†Êí≠„ÄÅNesterovÂä®ÈáèÂíåADAMËÆ≠ÁªÉ - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff Heaton„ÄÇ Welcom to applications of deep neural networks with Washington University„ÄÇ

 In this videoÔºå I am going to show you how„ÄÇüòäÔºåThe back propagation algorithm works internally„ÄÇ

 We'll look at back propagationÔºå classicÔºå also atom„ÄÇ

 Netroov momentum and other techniques that are commonly used to train deep neural networks for the latest on among AI course and projects„ÄÇ

 Click subscribe in the bell next to it to be notified of every new video„ÄÇ

 So classic back propagation„ÄÇ Back propagation has been around for a while„ÄÇ

 It is Jeffrey Henton contributed quite a bit to it„ÄÇ and werebo as well„ÄÇ

 So a variety of people introduced aspects of back propagation„ÄÇ

 and has been continued to be built upon over many years„ÄÇ

 This equation that you see here is sort of your very general training equation„ÄÇ

 This just says that T„ÄÇ Now T is the current epoch or time„ÄÇ This is saying that the weights„ÄÇ

 which are the weights of theta„ÄÇ The weights for the current time are equal to the weights from the previous time minus V the current time„ÄÇ

 V„ÄÇüòä„ÄÇ

![](img/6491afaa3cf5a798f8c7f93f7b8bf376_1.png)

From the current time is just a vector„ÄÇ All of these are vectors that holds the amounts that we're going to change each of the weights by„ÄÇ

 So this doesn't tell us a whole lot by itself„ÄÇ It just says that we are going to change the weights„ÄÇ

 And each time by this vector of change amounts„ÄÇ NowÔºå the vector of change amounts„ÄÇ

 V sub T will see a variety of functions that show us how to calculate v sub T„ÄÇ

 The first is classic back propagation„ÄÇ So if we look at thisÔºå this is gradient descent„ÄÇ

 So you have Eta multiplied by nowÔºå Eta is multiplied by the rest of this„ÄÇ

 This is essentially one unit„ÄÇ This is not„ÄÇThis thing multiplied by the J function„ÄÇ

This is the Nabla or the upside down Delta or the harp shaped operator„ÄÇ

 This says take the gradients of the loss function with respect to the weights from the previous time step or the previous epoch„ÄÇ

 So this is essentially giving you all of the gradients„ÄÇ

Multiplied by the learning rate at a common values for the learning rate are these 0„ÄÇ10„ÄÇ01„ÄÇ

 rarely Would you ever want to make that one so that you're fully adding the gradients to the weights that would that would simply be too chaotic„ÄÇ

 NowÔºå let's see really what the gradient is and how it's actually used„ÄÇ This is a derivative„ÄÇ

 It's a partial derivativeÔºå So you always take the partial derivative of one multivariate calculus„ÄÇ

 you take the derivative of one single variable and a multivariable expression„ÄÇ

 So one single weight with all other weights held constant„ÄÇ

 So this shows you essentially the error function for a particular weight„ÄÇ

 So the error for this weight as you adjust the weight„ÄÇ So if you make the weight0„ÄÇ

 the error is going to be hereÔºå then it goes up and then it swings way down„ÄÇ

 This is potentially true as you vary just one weight in the neural network„ÄÇ

 the error function will go up down and change its value„ÄÇ TypicallyÔºå we're doing gradient„ÄÇ

So we want to get the weight all the way to the lowest point here„ÄÇ NowÔºå we don't want to graph„ÄÇ

 generate this entire graph and and sample the neural network at each of these points„ÄÇ

 that would be computationally expensive because there is a different chart of this for every single weight in the entire neural network„ÄÇ

And as you change one weightÔºå all all the others potentially change2„ÄÇ

 This is where you have to do the partial derivative„ÄÇ

 We're doing the partial derivative for just one weight„ÄÇ sayy the weight was currently at 1„ÄÇ5„ÄÇ

 So all we would know is that the error was here„ÄÇ we don't have the rest of the lead up or continuation of the chart„ÄÇ

 we just have this one pointÔºå that point doesn't tell us too much until we take the derivative of the loss function„ÄÇ

 then that tells us the instantaneous rate of change„ÄÇ So youve got the slope„ÄÇ

 this point right here on the error function curve„ÄÇ Notice this line has a negative slope„ÄÇ

 But if we if we just added that gradient to itÔºå the negative valueÔºå whatever that negative slope is„ÄÇ

 that would decrease the weight„ÄÇ It would go in the wrong direction„ÄÇ

 So if if you truly want to go this directionÔºå because were past just a little bit of the crest of that hill„ÄÇ

 if you want to go in this directionÔºå you need to take the opposite of the slope„ÄÇ

 That is why up here we're„ÄÇTing the sub T because if thisÔºå I meanÔºå say we're right here„ÄÇ

 then the line would be very much this this direction„ÄÇ

 and that would be a very negatively sloped line„ÄÇ you would want a positive number„ÄÇ

 So you continued on your way down„ÄÇ If we were right hereÔºå then the slope would be positive„ÄÇ

 but we would want to subtract one from the weight to continue along this direction„ÄÇ

 So that is classic„ÄÇBack propagation„ÄÇ It's governed by the learning rate„ÄÇ

 If you made your learning rate too big instead set a gradient dissenting down here„ÄÇ

 you'd probably jump completely to the other side of it„ÄÇ

 and you would never find your way down to this lower value„ÄÇ

 So learning rate just describes how quickly we're going to attempt to push the weights to optimal values„ÄÇ

 And this link is pretty handy„ÄÇ It shows a javascript application that I wrote that takes you through all the steps of a classic back propagation„ÄÇ

 So you can see literally how an entire neural network is calculated for X O R„ÄÇ

 Next is momentum propagation„ÄÇ back propagation„ÄÇ So momentum is something that was added to back propagation to prevent from being from settling into a local minima„ÄÇ

 So local minima could be right here„ÄÇ It might be that further over here„ÄÇ

 there would be an even more optimal value„ÄÇ But once the weight gets settled into here„ÄÇ

 It's really hard for it to push its way completely out of that valley and continue on„ÄÇ„ÅÑ„Å´Ôºü

We have the case here„ÄÇ The weightÔºå which is where that ball currently is at„ÄÇ

 is essentially stuck in a local minimum„ÄÇ There might be a global minimum here„ÄÇ

 It's really hard to know where the global minimum is„ÄÇ It's usually almost impossible„ÄÇ

 So this weight would have been continuing downÔºå down downÔºå but it would potentially get stuck here„ÄÇ

 If not for the momentum that pushes it over the hump and allows it to continue„ÄÇ

 Moment just as its name implies„ÄÇ You can think of these weights as moving through high dimension space„ÄÇ

 Moment just gives the gives the weight push and continues that push as it maintains its momentum„ÄÇ

 This is the formula for momentum„ÄÇ Now we have two hyperparameters„ÄÇ We have Eta„ÄÇ

 which is the learning rate„ÄÇ but we also have lambdaÔºå which is the momentum rate„ÄÇ

 The first part of this is completely the same as classic back propagation„ÄÇ

 You are simply taking the gradient„ÄÇMultiply by the learning rate„ÄÇ

 But you have this additional term here„ÄÇ And this is the momentum term„ÄÇ This is lambda times V t -1„ÄÇ

 So whatever our previous delta our previous update wasÔºå we're scaling it by lambda and adding it„ÄÇ

 You're just adding the last update scaled right onto the equation with everything else„ÄÇ

 That's really all that momentum is„ÄÇ So as you were moving down this„ÄÇ

 you would have built up a lot of momentum because you would potentially be moving down fairly fast„ÄÇ

 Then that change is very much a positive weight that would keep getting added on the weight and hopefully push it over the hump and on to possibly out of the local minima and onto a better situation„ÄÇ

 hopefullyÔºå by the wayÔºå very common value for momentum is 0„ÄÇ9„ÄÇ

 They usually favor a fair amount of momentum„ÄÇ learning rate is often much smaller„ÄÇ It's usually 0„ÄÇ

10„ÄÇ01 or some other negative power of 10„ÄÇ NextÔºå we're going to look at„ÄÇ

And online batch propagation so this is an important concept for propagation training„ÄÇ

 We'll see later that we can configure these values in TensorF and determine what the batch sizes will be batchch is simply how many training set elements do you need to go through so each of these each of these gradients that we're calculating is for a single train set element„ÄÇ

So you might have1 thousand elements in your training set„ÄÇ

 How many of those you don't have to literally update the weights every single time you calculate a training row and get the deltas to the weight„ÄÇ

 you can batch those up and you batch them up simply by summing up the gradients So you process the first row of training training data and you get a vector gradients equal to or changes the V equal to the size of the weights and you just you can calculate the next training row and you add those gradients onto whatever read had before„ÄÇ

 you keep vector adding the gradients until you've made it to the batch size„ÄÇ So a batch size„ÄÇ

 if you had a batch size of 10Ôºå that means as it's going through the training set„ÄÇ

 itll make it through 10 elementsÔºå and then at the end of the 10 elements„ÄÇ

 it has the gradients that are basically the sum of that whole run„ÄÇ

 and then it will apply the changes to the weights„ÄÇ

 Online training simply applies the change to the weight as soon„ÄÇ

You calculate the gradient just one at a timeÔºå calculate a gradient for one training row„ÄÇ

 apply it to the weightsÔºå move on to the next training rowÔºå calculate its gradients„ÄÇ

 add it to the weights and continue Having the batch sizes this can provide considerable efficiency to the training of the neural network this is also very big data compliant because if you've got a very„ÄÇ

 very large data set you just need to randomly sample many batches from it so many batch training that's another very common technique for training neural networks many batches are typically between 32 and 64 in size so they're relatively small step and iteration that is just how many training cycles has has the neural network gone through step iteration or even and then all right now we'll look at stochastic gradient descent which is often used in conjunction with many batch training stochastic gradient descent is used to it provides a very stochastic or„ÄÇ

scentWhat's happening is rather than calculating the gradients with the current with the entire data set„ÄÇ

 you just pick small groups and you keep going through these random samples with replacement„ÄÇ

 of the neural network training data and as you go through each of these one by one by one the error will decrease it'll go up sometimes sometimes you'll pick particularly bad sets of training data sometimes you'll pick particularly good sets„ÄÇ

 It just all pretty much depends„ÄÇ So stochastic gradient descent is often used alone or as part of another training„ÄÇ

 It's computationally efficient and it decreases overfitting by focusing only on a small number of relatively good weights and also there's a variety of other techniques like I said„ÄÇ

 back propagation and gradient descent„ÄÇ that's just the main backbone some of these other techniques„ÄÇ

 what they attempt to solve is the learning rate and momentum„ÄÇ These are both hyperparameter„ÄÇ

 These are numbers that you need to tune along with everything else„ÄÇ

 you thought it was bad enough that you had to pick how many hidden letters and how„ÄÇ

neurons you want on each head layer Now you've got a learning rate of momentum„ÄÇ

 and you need to figure out what the best learning rate is„ÄÇ

 what the best momentum is so that you will be able to effectively train this neural network„ÄÇ

 The problem is the learning rateÔºå if you adjust it too small„ÄÇ

 it's never going to accurately train your neural network it's just not taking enough risk if you make it too large your neural network will be very„ÄÇ

 very erratic and momentumÔºå same thing if you make it too largeÔºå things become erratic„ÄÇ

 if you make it too smallÔºå it's not really having effect„ÄÇ Also if you think about it„ÄÇ

 this learning rate is being applied to every every weight in the entire neural network„ÄÇ

 maybe a single learning rate is not enoughÔºå maybe some neurons are learning faster than others„ÄÇ

 So they like a concept of putting on multiple learning rates or you can also sometimes you will see that they will just automatically decrease the learning rate as training progresses So we're trying to move away from having every weight having a global learning rate and momentum and then also„ÄÇ

Move to making those those values very sensitiveÔºå very nonsensitive are very accommodating to values that weren't chosen so well„ÄÇ

 These are some other training techniques that we that I've worked with in the past„ÄÇ

 There's resilient propagation„ÄÇ It works pretty well„ÄÇ

 It basically recognizes that the sign of the gradient is probably the most important thing„ÄÇ

 It tells you which direction the weight should move to better optimize„ÄÇ

 It also does not need a learning rate and momentum„ÄÇ So it was popular back in the day„ÄÇ

 It's not seen as much use with a deep learning N of accelerated a gradient„ÄÇ

 What that does is with stochastic gradient descent„ÄÇ

 It helps to mitigate the risk of just picking a really bad mini batch that then damages the rest of the training that you' that you've done There's addedgrad and Ana delta„ÄÇ

 These are both„ÄÇAt a gradÔºå basically it keeps a per weight decaying learning rate„ÄÇ

But it's monoomically decreasingÔºå it never increases again„ÄÇ

 so that's why added Delta was created to address atgrads issues where that that learning rate could just go in a direction and decrease to pretty much zero„ÄÇ

There's also nongraient methods„ÄÇ If you can't take a derivative of your last function„ÄÇ

 these might be useful„ÄÇ This includes simulated annelingÔºå genetic algorithmsÔºå particle swarm„ÄÇ

 Nedermeed and many more„ÄÇ So the atom update rule talked about before classic back propagation„ÄÇ

 It's just another way of calculating V to put into this weight update algorithm that we have before„ÄÇ

 Now the atom update rule„ÄÇ what is very nice about it is it doesn't have you don't really have to focus too much on a learning rate„ÄÇ

 there is a learning rate present but of the values that you that you have„ÄÇ

 the authors of the original paper King MubaÔºå they give some good recommendations for the hyperparameters„ÄÇ

 and I rarelyÔºå even that learning rate at the end tend to the negative8„ÄÇ

 they don't you often do not have to have to change those„ÄÇ

 This is a relatively new training algorithm„ÄÇ It was introduced in 2014„ÄÇ It's a popular one„ÄÇ

 It deals with training for a sparse for sparse data„ÄÇ

We lots of missing values and then also stochastic error function the error function is stochastic because we're doing stochastic gradient descent so we're constantly randomly subsampling a batch size of many batch and updating the neural network based on that So your the changes that you make from one iteration to the other might not help because you're grabbing a different set of set of training data on each one the paper for this for this is given here at Cornell University the archive if you haven't dealt with archive before that's the Greek letter chi so a archive archive and moment moment momentest method of moments that you see here it's a way of estimating each of the moments of of a distribution of values so that's the mean the variance and reliance and so on so let's look at„ÄÇ

The actual paper for thisÔºå this is very similar to the code that we've seen for the other ones„ÄÇ

 so this is just t equals t plus one that's indicating that we're moving through the time we're initializing the first and second moment the first moment is the mean„ÄÇ

 so the mean of the gradients that you're trying to estimate„ÄÇAnd V is the variance„ÄÇ

 so the second moment there is a third and additional moments„ÄÇ

 but all we're dealing with are these two the first two moments„ÄÇ

 by the way that's where the name add comes from adaptive moment estimation and then we initialize the time set to zero„ÄÇ

 so these two initial estimates are zero„ÄÇWe are going to calculate the GÔºå the gradient„ÄÇ

 so this is the gradientÔºå just like we're doing beforeÔºå very similar to classic back propagation„ÄÇ

 a little bit different terminology instead of the J that I use they use F so this is this is the loss function with the weights that's exactly the notation from the module for the previous one„ÄÇ

And we are going to„ÄÇGet that gradient„ÄÇ These two values deal with a bias on the„ÄÇ

That occurs early on since these are initialized to 0Ôºå that creates a tremendous bias towards  zero„ÄÇ

 So these are just two„ÄÇÂóØ„ÄÇI'm sorry the hats down here are actually that that's where they're calculating the they're dealing with the the initial„ÄÇ

Fact that these are starting out at zero„ÄÇ So M hat and V hat M and VT themselves„ÄÇ

 this is essentially updating each of these„ÄÇ so you'll notice that it's M based on the previous T„ÄÇ

They're updating as they go through itÔºå they are essentially creating more and more of a estimate of the first and second moments„ÄÇ

 and these are vectors„ÄÇObviously they're across the weights„ÄÇ

 so that's essentially creating almost a learning rate for each„ÄÇ

The first one is on the first power and then second powerÔºå so this is squared„ÄÇ

We calculate these adjustment values just to deal with the fact that these started at a 0„ÄÇ

 and then we're going to update the weights„ÄÇ So weight tÔºå this is very simple„ÄÇ

 So this part is exactly like what we had from trying to highlight the bottom row„ÄÇ

 But up to here is exactly what we had for the for classic„ÄÇWe're subtracting the gradient„ÄÇ

 but instead of subtracting the gradientÔºå we're using this formula to update the parameters or weights„ÄÇ

Alpha is the learning rateÔºå so usually learning rates or step sizeÔºå as they call it„ÄÇ

 you're multiplying by the rest of this to scale this„ÄÇAnd you're putting in the two hat values„ÄÇ

 which were based on these just to adjust them so that they're not so biased initially towards zero„ÄÇ

That is essentially AdamÔºå it's a little bit more complicated than your classic back propagation„ÄÇ

 but not a whole lot not I mean this is not terrible to implement and„ÄÇ

In Java or another programming languageÔºå they discuss the algorithm„ÄÇ

And they talk about that initial bias correction where why that is needed and how you are calculating the two hat values that are very necessary for that„ÄÇ

I will admit I have not read through this part and find it somewhat very complex„ÄÇ

 they're doing some analysis of the convergenceÔºå this is essentially proof for why it works„ÄÇ

And then they quote related work and then experiments where they sort of empirically prove through experiment why it works and with E theorems they attempt to talk about why it sort of proofwise why it works„ÄÇ

 related workÔºå R Pro and AgradÔºå those are two other training very similar training techniques that existed prior to Adam and this is just straight sort of from the paper„ÄÇ

 but I like looking the pseudocode a little bit better„ÄÇ

 but I reproduced it there if you're interested in it„ÄÇThis is a really good diagram„ÄÇ

 I did not create itÔºå I have links to where this is where I found it„ÄÇYeah„ÄÇSo anyway„ÄÇ

 and then he cites it to the original author can„ÄÇDefinitely want to give credit where credit is due„ÄÇ

 This is a veryÔºå very good diagram of this„ÄÇ It's an animated g„ÄÇ So these are„ÄÇ

 this is essentially a search space„ÄÇ So you can think of this as a rugged sort of two dimensional plane„ÄÇ

 Star is the lowest point„ÄÇ That's where you want to get to„ÄÇ And it's like„ÄÇüòä„ÄÇ

It's sort of like marbles rolling down a ridge„ÄÇThis is stochastic gradient descent„ÄÇ

 which is the slowest one„ÄÇ So that's your classic back propagation„ÄÇ It's eventually getting there„ÄÇ

 The one that gets there really first is add Delta and„ÄÇIt seems like green„ÄÇMomentum starts out slow„ÄÇ

 but then reallyÔºå as you can seeÔºå builds up the momentum and then blasts right past it„ÄÇ

 So just your different different training methods„ÄÇ

 Adam is out on here because when this was created„ÄÇ

 Adam did not exist or at least was not all that common yet„ÄÇ But you can„ÄÇ

 you can definitely see some of them like momentum„ÄÇ It's very obvious that it's„ÄÇ

It starts to really build that momentumÔºå the little green ball and shoots right past it„ÄÇ

 SGD is just slowly methodically working there and it doesn't even make it„ÄÇ

 they give up and reset it because everybody else has made it„ÄÇ

Kras and Tensorflowlow has a variety of these these techniques available for you„ÄÇ

 Can you the ones that are highlighted they haveÔºå so they have stochastic gradient descent in Adam„ÄÇ

 I believe there's other ways you can get to some of these„ÄÇ I'm sorry„ÄÇ

 these are all of the ones that Tensorflowlow has„ÄÇ I honestly have no idea what FTRL is or I'm familiar with the other ones„ÄÇ

 the ones that I have highlighted are probably the ones you're the most interested in„ÄÇ

 I would be tempted to highlight RMSs Pro as well„ÄÇ you can you can definitely experiment with these and get potentially better results where you specify it is down here with the optimizer„ÄÇ

If you specify one of these that means a learning rate you'll pass in a class here and actually pass in the parameters as well you can go to the Tensorflow documentation and they show you ourki is anyway and they they give you the actual class names for this„ÄÇ

 but you can just use any of them just by putting its name in here you'll need to put an object in there if you want to specify even additional parameters these are some of the results that I got trying these out so you can see that on the miles per gallon data set we got basically different results and some like unit's momentum needed a lot more training than say at„ÄÇ

Showing the last successful or the last number of iterations„ÄÇ So Adam converges pretty quick„ÄÇ

 I I'm pretty fond of that training techniqueÔºå and I use it often At agrad is also pretty good as well„ÄÇ

 So at agradÔºå I like RMS Pro definitelyÔºå definitely I have seen it T's uses„ÄÇ I will„ÄÇüòä„ÄÇ

I will try between these and look at them when I am training neural networks and I often start with the atom„ÄÇ

 but then we'll try at agrad or our MSP„ÄÇThere are probably better„ÄÇ

 more scientific ways to pick thoseÔºå but that tends to work pretty well for me„ÄÇ All right„ÄÇ

 That is the end of this module„ÄÇ

![](img/6491afaa3cf5a798f8c7f93f7b8bf376_3.png)

Thank you for watching this video on how to train a neural network„ÄÇ In the next video„ÄÇ

 we're going to literally calculate a neural network from scratch„ÄÇ

 We're going to see how to export the weights from Kes and use those to actually calculate what the output of the neural network would have been„ÄÇ

 This removes all magic from the process„ÄÇ This content changes often„ÄÇ

 So subscribe to the channel to stay up to date on this course and other topics and artificial intelligence„ÄÇ

üòä„ÄÇ