- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P21ÔºöL21-
    Ë∞ÉÊï¥Â≠¶‰π†Áéá‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÁªìÊûú - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HeyÔºå guysÔºå welcome to another Pytorch tutorial„ÄÇ Today„ÄÇ I'll show you a simple
    but powerful technique that you can use in your Pytorch code to improve the optimization
    process during the training„ÄÇ So what we want to use here is a so called learning
    rate chatular„ÄÇ This means that we adjust our learning rate during the training
    loop„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: either based on the number of epochs or based on validation measurements„ÄÇ So
    let's roll the intro„ÄÇ and then I'll show you how to do this„ÄÇüòä„ÄÇ![](img/4ac8ae1274e2571af0e178a8cbc54a12_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_2.png)'
  prefs: []
  type: TYPE_IMG
- en: üéºÔºåAs you should knowÔºå the learning rate is one of the most important type of
    parameters that you should tweak during the training„ÄÇ and it's highly recommended
    to adjust the learning rate a little bit during the training to get better results„ÄÇ
    Pytorarch provides several methods in their API to do this„ÄÇ And it's really not
    that hard to implement this in your code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let me show you how one thing I want to mention is that by adjusting the
    learning rate„ÄÇ Most of the time we want to decrease the learning rate„ÄÇ So not
    increase it„ÄÇ But of course„ÄÇ it always depends on your specific problem„ÄÇ So Pytorarch
    provides some so-called learning rate scheduleular in the optimization or optim
    module„ÄÇ So let's go through the API documentation and show some examples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we are in the API doc of the optim module„ÄÇ and here in this section
    how to adjust the learning rate it explains how we can„ÄÇdo this„ÄÇ And of courseÔºå
    I will put the link in the description So it tells us that torch Opim Lr schedule
    provides several methods to adjust the learning rate based on the number of epochs„ÄÇ
    or we also have methods that adjust the learning rate based on some validation
    measurements„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we also have this important sentence„ÄÇ Learn rate scheduleuling should be
    applied after Opimrs update„ÄÇ So our coach should look something like this„ÄÇ So
    we create a scheduleer„ÄÇ Then we have our training loop where we have our epochs„ÄÇ
    and then we do the training step„ÄÇ So this might be something like lost dot backward„ÄÇ
    Then we might have a validation step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we call Shaular dot step„ÄÇ So it's that simple„ÄÇ But of course„ÄÇ we have
    to create a scheduler„ÄÇ And for thisÔºå we have different options„ÄÇ So let's go through
    the documentationÔºå and then I show you the different methods we have„ÄÇSo the first
    one is the socalled lambmbda Lr„ÄÇ So this sets the learning rate of each parameter
    group to the initial learning rate times a given function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we might even have different parameter groupsÔºå which we don't care about
    for now„ÄÇ but you should know that then you can use multiple functions here„ÄÇ So
    what we want to do is we create a lambda function„ÄÇ So this is basically a one
    line function„ÄÇ And this can be dependent on the epoch„ÄÇ So here in this example
    we divide the epoch by 30„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we create our lambda Lr with an optr„ÄÇ and then this lambda function
    that we create it„ÄÇ So let's go to the code and let me show you an actual example„ÄÇ
    So what you want to do here is you want to import torch optim Lr scheduler as
    Lr scheduler„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_4.png)'
  prefs: []
  type: TYPE_IMG
- en: Then we have a learning rate„ÄÇ So in this caseÔºå it's 0„ÄÇ1 in the beginning„ÄÇ Then
    we have a model„ÄÇ So in this caseÔºå a simple linear model„ÄÇ Then you also need an
    optimizer„ÄÇ And then here we create this lambda function„ÄÇ So this is a one line
    function and dependent on the epoch„ÄÇ So we divide the epoch by 10„ÄÇAnd then we
    multiply this with the initial learning rate„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So in the first epoch we have one divided by 10 times our learning rate in the
    second epoch„ÄÇ we have two divided by 10 times our initial learning rate and so
    on and then we create our scheduler like this„ÄÇ So we give it the optimizer and
    the lambda function„ÄÇ And in this example„ÄÇ our learning rate is actually increasing„ÄÇ
    But in all the next examplesÔºå it will be decreasing then„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So then here we have our typical training loop„ÄÇ and first of all„ÄÇ I want to
    print optimizer state di to show you how this looks like„ÄÇ Then here we mightÔºå
    for example„ÄÇ do lost dot backwardsÔºå then we call optimizer step then we might
    have some validation step and then we call this schedule step„ÄÇ and then here I
    want to print the actual learning rate and we can„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Access this by calling optimizer state dit„ÄÇ And then here we access the key
    Para groups„ÄÇ and here we only use one„ÄÇ So we access this one with index0„ÄÇ And
    then we use the key learning rate„ÄÇ So this will give us the actual learning rate„ÄÇ
    So let's run this code by saying Python lambda L R„ÄÇ And then here we see we have
    the optimizer state dit with the initial learning rate of 01„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then here the first epoch„ÄÇ we have one over 10 times 0„ÄÇ1„ÄÇ So we have this„ÄÇ
    the second epoch„ÄÇ we have this and on„ÄÇ So I hope that it's clear how this works„ÄÇ
    So let's look at the next example„ÄÇ![](img/4ac8ae1274e2571af0e178a8cbc54a12_6.png)
  prefs: []
  type: TYPE_NORMAL
- en: So the next example is the socalled multiplicative Lr„ÄÇ This basically works
    the same„ÄÇ But here we multiply the learning rate of each parameter group by the
    factor given in the specified function„ÄÇ So againÔºå we create a lambda function
    that may be dependent on the epoch„ÄÇ So here we just return a value„ÄÇ So it actually
    doesn't change with the number of epochs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But now this is multiplicative„ÄÇ So each epoch this will be multiplied to the
    last epoch„ÄÇ So let's go to the code again„ÄÇ and then let's have a look at how this
    looks like„ÄÇ So here we have the same code as before„ÄÇ But now we use the multiplicative
    L R with simply this factor of 095„ÄÇ And then each time it will be multiplied to
    the last learning rate„ÄÇ So let's print„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_8.png)'
  prefs: []
  type: TYPE_IMG
- en: This oneÔºå so let's clear this one and let's run Python multi Lr dot pi„ÄÇ and
    then we see we have our initial learning rate of 0„ÄÇ1„ÄÇ Then we multiply it with
    0„ÄÇ95 which will give us 0„ÄÇ095„ÄÇ and the next one we again multiply it with 0„ÄÇ95
    and this will give us this learning rate and then this and this and so on„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So again the big difference here is that here we multiply it with the learning
    rate„ÄÇ and with the lambda LrÔºå we just use the initial learning rate and then multiply
    it with this function„ÄÇ So then if we go further then we have the step LrÔºå this
    is probably the most easiest to understand„ÄÇ So here it says it decays the learning
    rate of each parameter group by gamma every step size epoch so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: It might look something like this„ÄÇ So we have our step Lr with the optimizer
    then a step size and then this gamma„ÄÇ So as I saidÔºå typically we want to decrease
    it„ÄÇ So we set this to something smaller than one„ÄÇ And then here we can see for
    the first 30 epochs we have our initial learning rate 0„ÄÇ05„ÄÇ then we multiplied
    it with 0„ÄÇ1„ÄÇ So we have 0„ÄÇ005 for the next 30 steps and then again we multiplied
    it with our gamma„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we have this learning rate for the next 30 steps and so on So yeah„ÄÇ this
    is one of the simplest one„ÄÇ but it's actually really powerful„ÄÇ So I used this
    very often myself„ÄÇ Then we have this multi step Lr which decay the learning rate
    of each parameter group by gamma once the number of epoch reaches one of the milestones„ÄÇSo
    basicallyÔºå hereÔºå instead of stepsÔºå we just we can give it milestones„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then it will do the same„ÄÇ So here for epoch 30Ôºå it will apply our gammaÔºå
    then for epoch 80„ÄÇ it will apply our gamma and so on„ÄÇ So this gives us a little
    bit more variation„ÄÇ if we don't want to use the same step size all the time„ÄÇ Then
    we have this exponential Lr here„ÄÇ it decays the learning rate by gammaÔºå every
    epoch„ÄÇ So this is essentially the same as the step Lr„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if we use a step size of one here„ÄÇ So here we simply apply this every epoch„ÄÇ
    and then we don't have to care about these steps„ÄÇ So yeahÔºå that's this one„ÄÇ then
    we also have this cosine uning LrÔºå which I won't go over for nowÔºå but you canÔºå
    of course„ÄÇ have a look at that yourself„ÄÇAnd yeahÔºå but then let's have a look at
    this one again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this one is called reduce L R on plateau„ÄÇ So now hereÔºå this is not dependent
    on the epoch„ÄÇ but here insteadÔºå it's dependent on some measurements or metrics„ÄÇ
    So it reduces the learning rate when a metric has stopped improving„ÄÇ So here we
    want to reduce our learning rate and Py tells us that models often benefit from
    reducing the learning rate by a factor of 2 between 2 and 10 Once learning stagnateates„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This shaular here reads a metric quantity„ÄÇ And if no improvement is seen for
    a patient's number of epochs„ÄÇ The learning rate is reduced„ÄÇ So what we need here
    is we need a optr„ÄÇ we need a mode and a factor and a patients„ÄÇüòäÔºåSo the mode will
    be min or max„ÄÇ So in min mode„ÄÇ learning rate will be reduced when the quantity
    monitored has stopped decreasing in max mode„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will be reduced when the quantity has stopped increasing„ÄÇ Then we need this
    factor„ÄÇ So the factor by which the learning rate will be reduced default is 0„ÄÇ1„ÄÇ
    and we need the patients„ÄÇ So how long we wait for no improvement„ÄÇ So number of
    epochs with no improvement after which learning rate will be reduced„ÄÇ For exampleÔºå
    if patients equals2„ÄÇ Then we will ignore the first two epochs with no improvement
    and will only decrease the learning rate after the third epoch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If the loss still hasn't improved„ÄÇ So our code might look something like this„ÄÇ
    we have our optimizer„ÄÇ We have this reduce L R on pla„ÄÇWith our optimizer and in
    this case„ÄÇ it just uses the default factor and patience„ÄÇ and then we have our
    training loop where we do the training step„ÄÇ We calculate the loss„ÄÇ And after
    the validationÔºå we calculate chatular dot step with this validation loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if this doesn't get better for the number of epochs we want to wait„ÄÇ then
    we reduce the learning rate„ÄÇ and this really can help our model to stop stagnating
    and further improve the optimization„ÄÇ So yeahÔºå then we also have this cyclic Lr
    and one moreÔºå I guess this onecycl Lr„ÄÇ which I won't go over here as wellÔºå but
    you can check that out for yourself„ÄÇ But yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that's what I wanted to show you for now„ÄÇ So again„ÄÇ you can go to this API documentation
    and then check out the different optimizers„ÄÇSo yeah„ÄÇ that's all I wanted to show
    you for now again„ÄÇ this can really help your model during the training process„ÄÇ
    and it's not that hard to implement„ÄÇ So just pick a scheduler and try out different
    ones for yourself„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then call the scheduler That's dot step„ÄÇ And yeahÔºå then you're good to go„ÄÇ
    And I hope you enjoyed this tutorial And if you liked it then please subscribe
    to the channel„ÄÇ and then I hope to see you next time bye„ÄÇ![](img/4ac8ae1274e2571af0e178a8cbc54a12_12.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_13.png)'
  prefs: []
  type: TYPE_IMG
