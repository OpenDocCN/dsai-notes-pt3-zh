- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P21ï¼šL21-
    è°ƒæ•´å­¦ä¹ ç‡ä»¥è·å¾—æ›´å¥½çš„ç»“æœ - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P21ï¼šL21-
    è°ƒæ•´å­¦ä¹ ç‡ä»¥è·å¾—æ›´å¥½çš„ç»“æœ - ShowMeAI - BV12m4y1S7ix
- en: Heyï¼Œ guysï¼Œ welcome to another Pytorch tutorialã€‚ Todayã€‚ I'll show you a simple
    but powerful technique that you can use in your Pytorch code to improve the optimization
    process during the trainingã€‚ So what we want to use here is a so called learning
    rate chatularã€‚ This means that we adjust our learning rate during the training
    loopã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å˜¿ï¼Œå¤§å®¶ï¼Œæ¬¢è¿æ¥åˆ°å¦ä¸€ä¸ªPytorchæ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºä¸€ä¸ªç®€å•ä½†å¼ºå¤§çš„æŠ€æœ¯ï¼Œä½ å¯ä»¥åœ¨ä½ çš„Pytorchä»£ç ä¸­ä½¿ç”¨ï¼Œä»¥æ”¹å–„è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³åœ¨è¿™é‡Œä½¿ç”¨æ‰€è°“çš„å­¦ä¹ ç‡è°ƒåº¦ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨è®­ç»ƒå¾ªç¯ä¸­è°ƒæ•´å­¦ä¹ ç‡ã€‚
- en: either based on the number of epochs or based on validation measurementsã€‚ So
    let's roll the introã€‚ and then I'll show you how to do thisã€‚ğŸ˜Šã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_1.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ˜¯åŸºäºè®­ç»ƒè½®æ•°è¿˜æ˜¯åŸºäºéªŒè¯æµ‹é‡ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹ä»‹ç»ï¼Œç„¶åæˆ‘å°†å‘ä½ å±•ç¤ºå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚ğŸ˜Šï¼[](img/4ac8ae1274e2571af0e178a8cbc54a12_1.png)
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_2.png)'
- en: ğŸ¼ï¼ŒAs you should knowï¼Œ the learning rate is one of the most important type of
    parameters that you should tweak during the trainingã€‚ and it's highly recommended
    to adjust the learning rate a little bit during the training to get better resultsã€‚
    Pytorarch provides several methods in their API to do thisã€‚ And it's really not
    that hard to implement this in your codeã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ï¼Œå¦‚ä½ æ‰€çŸ¥ï¼Œå­¦ä¹ ç‡æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­æœ€é‡è¦çš„å‚æ•°ä¹‹ä¸€ï¼Œä½ åº”è¯¥åœ¨è®­ç»ƒä¸­è¿›è¡Œè°ƒæ•´ã€‚å¼ºçƒˆå»ºè®®åœ¨è®­ç»ƒæœŸé—´ç¨å¾®è°ƒæ•´å­¦ä¹ ç‡ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚Pytorchåœ¨å…¶APIä¸­æä¾›äº†å‡ ç§æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å…¶å®åœ¨ä½ çš„ä»£ç ä¸­å®ç°èµ·æ¥å¹¶ä¸éš¾ã€‚
- en: So let me show you how one thing I want to mention is that by adjusting the
    learning rateã€‚ Most of the time we want to decrease the learning rateã€‚ So not
    increase itã€‚ But of courseã€‚ it always depends on your specific problemã€‚ So Pytorarch
    provides some so-called learning rate scheduleular in the optimization or optim
    moduleã€‚ So let's go through the API documentation and show some examplesã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘å‘ä½ å±•ç¤ºä¸€ä»¶äº‹ï¼Œæˆ‘æƒ³æåˆ°çš„æ˜¯ï¼Œé€šè¿‡è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¤§å¤šæ•°æ—¶å€™æˆ‘ä»¬æƒ³è¦é™ä½å­¦ä¹ ç‡ï¼Œè€Œä¸æ˜¯å¢åŠ ã€‚ä½†å½“ç„¶ï¼Œè¿™æ€»æ˜¯å–å†³äºä½ çš„å…·ä½“é—®é¢˜ã€‚Pytorchåœ¨ä¼˜åŒ–æˆ–optimæ¨¡å—ä¸­æä¾›äº†ä¸€äº›æ‰€è°“çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚è®©æˆ‘ä»¬æµè§ˆä¸€ä¸‹APIæ–‡æ¡£ï¼Œå¹¶å±•ç¤ºä¸€äº›ä¾‹å­ã€‚
- en: So here we are in the API doc of the optim moduleã€‚ and here in this section
    how to adjust the learning rate it explains how we canã€‚do thisã€‚ And of courseï¼Œ
    I will put the link in the description So it tells us that torch Opim Lr schedule
    provides several methods to adjust the learning rate based on the number of epochsã€‚
    or we also have methods that adjust the learning rate based on some validation
    measurementsã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ç°åœ¨åœ¨optimæ¨¡å—çš„APIæ–‡æ¡£ä¸­ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œå¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡è§£é‡Šäº†æˆ‘ä»¬å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œæˆ‘ä¼šåœ¨æè¿°ä¸­æ”¾ä¸Šé“¾æ¥ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬torch Optim
    Lrè°ƒåº¦æä¾›äº†å‡ ç§åŸºäºè®­ç»ƒè½®æ•°è°ƒæ•´å­¦ä¹ ç‡çš„æ–¹æ³•ï¼Œæˆ–è€…æˆ‘ä»¬ä¹Ÿæœ‰åŸºäºæŸäº›éªŒè¯æµ‹é‡æ¥è°ƒæ•´å­¦ä¹ ç‡çš„æ–¹æ³•ã€‚
- en: And we also have this important sentenceã€‚ Learn rate scheduleuling should be
    applied after Opimrs updateã€‚ So our coach should look something like thisã€‚ So
    we create a scheduleerã€‚ Then we have our training loop where we have our epochsã€‚
    and then we do the training stepã€‚ So this might be something like lost dot backwardã€‚
    Then we might have a validation stepã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æœ‰è¿™ä¸ªé‡è¦çš„å¥å­ã€‚å­¦ä¹ ç‡è°ƒåº¦åº”è¯¥åœ¨ä¼˜åŒ–å™¨æ›´æ–°ååº”ç”¨ã€‚æ‰€ä»¥æˆ‘ä»¬çš„ä»£ç åº”è¯¥çœ‹èµ·æ¥åƒè¿™æ ·ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè°ƒåº¦å™¨ã€‚ç„¶åæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æœ‰è®­ç»ƒè½®æ•°ï¼Œç„¶åæˆ‘ä»¬è¿›è¡Œè®­ç»ƒæ­¥éª¤ã€‚è¿™å¯èƒ½æ˜¯åƒloss.backwardã€‚ç„¶åæˆ‘ä»¬å¯èƒ½ä¼šæœ‰ä¸€ä¸ªéªŒè¯æ­¥éª¤ã€‚
- en: and then we call Shaular dot stepã€‚ So it's that simpleã€‚ But of courseã€‚ we have
    to create a schedulerã€‚ And for thisï¼Œ we have different optionsã€‚ So let's go through
    the documentationï¼Œ and then I show you the different methods we haveã€‚So the first
    one is the socalled lambmbda Lrã€‚ So this sets the learning rate of each parameter
    group to the initial learning rate times a given functionã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è°ƒç”¨Shaularçš„stepæ–¹æ³•ã€‚æ‰€ä»¥å°±æ˜¯è¿™ä¹ˆç®€å•ã€‚ä½†å½“ç„¶ï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºä¸€ä¸ªè°ƒåº¦å™¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸åŒçš„é€‰é¡¹ã€‚è®©æˆ‘ä»¬æµè§ˆä¸€ä¸‹æ–‡æ¡£ï¼Œç„¶åæˆ‘ä¼šå‘ä½ å±•ç¤ºæˆ‘ä»¬æœ‰å“ªäº›ä¸åŒçš„æ–¹æ³•ã€‚ç¬¬ä¸€ä¸ªæ˜¯æ‰€è°“çš„lambda
    Lrã€‚å®ƒå°†æ¯ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡è®¾ç½®ä¸ºåˆå§‹å­¦ä¹ ç‡ä¹˜ä»¥ç»™å®šçš„å‡½æ•°ã€‚
- en: So we might even have different parameter groupsï¼Œ which we don't care about
    for nowã€‚ but you should know that then you can use multiple functions hereã€‚ So
    what we want to do is we create a lambda functionã€‚ So this is basically a one
    line functionã€‚ And this can be dependent on the epochã€‚ So here in this example
    we divide the epoch by 30ã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯èƒ½ä¼šæœ‰ä¸åŒçš„å‚æ•°ç»„ï¼Œä½†ç°åœ¨æˆ‘ä»¬ä¸éœ€è¦å…³å¿ƒè¿™ä¸ªã€‚ä½ åº”è¯¥çŸ¥é“ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨å¤šä¸ªå‡½æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬æƒ³è¦åšçš„æ˜¯åˆ›å»ºä¸€ä¸ªlambdaå‡½æ•°ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå•è¡Œå‡½æ•°ã€‚è¿™å¯ä»¥ä¾èµ–äºè®­ç»ƒè½®æ•°ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†è½®æ•°é™¤ä»¥30ã€‚
- en: And then we create our lambda Lr with an optrã€‚ and then this lambda function
    that we create itã€‚ So let's go to the code and let me show you an actual exampleã€‚
    So what you want to do here is you want to import torch optim Lr scheduler as
    Lr schedulerã€‚ğŸ˜Šã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ç”¨optråˆ›å»ºæˆ‘ä»¬çš„lambda Lrï¼Œç„¶åè¿™ä¸ªæˆ‘ä»¬åˆ›å»ºçš„lambdaå‡½æ•°ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç ï¼Œè®©æˆ‘ç»™ä½ å±•ç¤ºä¸€ä¸ªå®é™…çš„ä¾‹å­ã€‚æ‰€ä»¥ä½ è¦åšçš„æ˜¯å¯¼å…¥torch
    optim Lr schedulerä½œä¸ºLr schedulerã€‚ğŸ˜Š
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_4.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_4.png)'
- en: Then we have a learning rateã€‚ So in this caseï¼Œ it's 0ã€‚1 in the beginningã€‚ Then
    we have a modelã€‚ So in this caseï¼Œ a simple linear modelã€‚ Then you also need an
    optimizerã€‚ And then here we create this lambda functionã€‚ So this is a one line
    function and dependent on the epochã€‚ So we divide the epoch by 10ã€‚And then we
    multiply this with the initial learning rateã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªå­¦ä¹ ç‡ã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œèµ·åˆæ˜¯0.1ã€‚æ¥ç€æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ã€‚ç„¶åä½ è¿˜éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æ¥ç€æˆ‘ä»¬åˆ›å»ºè¿™ä¸ªlambdaå‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªå•è¡Œå‡½æ•°ï¼Œä¾èµ–äºepochã€‚å› æ­¤æˆ‘ä»¬å°†epoché™¤ä»¥10ï¼Œç„¶åç”¨åˆå§‹å­¦ä¹ ç‡ä¹˜ä»¥è¿™ä¸ªå€¼ã€‚
- en: So in the first epoch we have one divided by 10 times our learning rate in the
    second epochã€‚ we have two divided by 10 times our initial learning rate and so
    on and then we create our scheduler like thisã€‚ So we give it the optimizer and
    the lambda functionã€‚ And in this exampleã€‚ our learning rate is actually increasingã€‚
    But in all the next examplesï¼Œ it will be decreasing thenã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨ç¬¬ä¸€ä¸ªepochæˆ‘ä»¬æœ‰1é™¤ä»¥10ä¹˜ä»¥æˆ‘ä»¬çš„å­¦ä¹ ç‡ï¼Œåœ¨ç¬¬äºŒä¸ªepochï¼Œæˆ‘ä»¬æœ‰2é™¤ä»¥10ä¹˜ä»¥æˆ‘ä»¬çš„åˆå§‹å­¦ä¹ ç‡ï¼Œä¾æ­¤ç±»æ¨ï¼Œç„¶åæˆ‘ä»¬åƒè¿™æ ·åˆ›å»ºæˆ‘ä»¬çš„è°ƒåº¦å™¨ã€‚å› æ­¤æˆ‘ä»¬ç»™å®ƒä¼˜åŒ–å™¨å’Œlambdaå‡½æ•°ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„å­¦ä¹ ç‡å®é™…ä¸Šåœ¨å¢åŠ ã€‚ä½†åœ¨æ‰€æœ‰æ¥ä¸‹æ¥çš„ä¾‹å­ä¸­ï¼Œå®ƒå°†ä¼šå‡å°‘ã€‚
- en: So then here we have our typical training loopã€‚ and first of allã€‚ I want to
    print optimizer state di to show you how this looks likeã€‚ Then here we mightï¼Œ
    for exampleã€‚ do lost dot backwardsï¼Œ then we call optimizer step then we might
    have some validation step and then we call this schedule stepã€‚ and then here I
    want to print the actual learning rate and we canã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„å…¸å‹è®­ç»ƒå¾ªç¯ã€‚é¦–å…ˆï¼Œæˆ‘æƒ³æ‰“å°ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸ï¼Œä»¥å±•ç¤ºå®ƒçš„æ ·å­ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬å¯èƒ½ï¼Œä¾‹å¦‚ï¼Œè¿›è¡Œlost.dot.backwardï¼Œç„¶åè°ƒç”¨ä¼˜åŒ–å™¨æ­¥éª¤ï¼Œç„¶åæˆ‘ä»¬å¯èƒ½æœ‰ä¸€äº›éªŒè¯æ­¥éª¤ï¼Œæ¥ç€è°ƒç”¨è¿™ä¸ªè°ƒåº¦æ­¥éª¤ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘æƒ³æ‰“å°å®é™…çš„å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚
- en: Access this by calling optimizer state ditã€‚ And then here we access the key
    Para groupsã€‚ and here we only use oneã€‚ So we access this one with index0ã€‚ And
    then we use the key learning rateã€‚ So this will give us the actual learning rateã€‚
    So let's run this code by saying Python lambda L Rã€‚ And then here we see we have
    the optimizer state dit with the initial learning rate of 01ã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸æ¥è®¿é—®å®ƒã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬è®¿é—®å…³é”®å‚æ•°ç»„ï¼Œå¹¶ä¸”è¿™é‡Œæˆ‘ä»¬åªä½¿ç”¨ä¸€ä¸ªã€‚æ‰€ä»¥æˆ‘ä»¬ç”¨ç´¢å¼•0æ¥è®¿é—®è¿™ä¸ªï¼Œå¹¶ä¸”ä½¿ç”¨å…³é”®å­¦ä¹ ç‡ã€‚æ‰€ä»¥è¿™å°†ç»™æˆ‘ä»¬å®é™…çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬é€šè¿‡è¯´Python
    lambda L Ræ¥è¿è¡Œè¿™æ®µä»£ç ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸çš„åˆå§‹å­¦ä¹ ç‡æ˜¯0.1ã€‚
- en: and then here the first epochã€‚ we have one over 10 times 0ã€‚1ã€‚ So we have thisã€‚
    the second epochã€‚ we have this and onã€‚ So I hope that it's clear how this worksã€‚
    So let's look at the next exampleã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_6.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨è¿™é‡Œï¼Œç¬¬ä¸€ä¸ªepochã€‚æˆ‘ä»¬æœ‰1é™¤ä»¥10ä¹˜ä»¥0.1ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰è¿™ä¸ªã€‚ç¬¬äºŒä¸ªepochï¼Œæˆ‘ä»¬æœ‰è¿™ä¸ªï¼Œä¾æ­¤ç±»æ¨ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›è¿™èƒ½æ¸…æ¥šå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹ä¸‹ä¸€ä¸ªä¾‹å­ã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_6.png)
- en: So the next example is the socalled multiplicative Lrã€‚ This basically works
    the sameã€‚ But here we multiply the learning rate of each parameter group by the
    factor given in the specified functionã€‚ So againï¼Œ we create a lambda function
    that may be dependent on the epochã€‚ So here we just return a valueã€‚ So it actually
    doesn't change with the number of epochsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸‹ä¸€ä¸ªä¾‹å­æ˜¯æ‰€è°“çš„ä¹˜æ³•Lrã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ç›¸åŒçš„ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬å°†æ¯ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡ä¹˜ä»¥æŒ‡å®šå‡½æ•°ä¸­ç»™å®šçš„å› å­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å†æ¬¡åˆ›å»ºä¸€ä¸ªå¯èƒ½ä¾èµ–äºepochçš„lambdaå‡½æ•°ã€‚å› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬åªè¿”å›ä¸€ä¸ªå€¼ã€‚æ‰€ä»¥å®ƒå®é™…ä¸Šä¸ä¼šéšç€epochæ•°é‡çš„å˜åŒ–è€Œæ”¹å˜ã€‚
- en: But now this is multiplicativeã€‚ So each epoch this will be multiplied to the
    last epochã€‚ So let's go to the code againã€‚ and then let's have a look at how this
    looks likeã€‚ So here we have the same code as beforeã€‚ But now we use the multiplicative
    L R with simply this factor of 095ã€‚ And then each time it will be multiplied to
    the last learning rateã€‚ So let's printã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç°åœ¨è¿™æ˜¯ä¹˜æ³•çš„ã€‚å› æ­¤æ¯ä¸ªepochè¿™å°†ä¹˜ä»¥ä¸Šä¸€ä¸ªepochã€‚è®©æˆ‘ä»¬å†æ¬¡æŸ¥çœ‹ä»£ç ï¼Œçœ‹çœ‹è¿™çœ‹èµ·æ¥å¦‚ä½•ã€‚å› æ­¤è¿™é‡Œæˆ‘ä»¬æœ‰ä¸ä¹‹å‰ç›¸åŒçš„ä»£ç ã€‚ä½†æ˜¯ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ä¹˜æ³•L
    Rï¼Œç®€å•åœ°ç”¨è¿™ä¸ª0.95çš„å› å­ã€‚ç„¶åæ¯æ¬¡å®ƒå°†ä¹˜ä»¥ä¸Šä¸€ä¸ªå­¦ä¹ ç‡ã€‚æ‰€ä»¥è®©æˆ‘ä»¬æ‰“å°å‡ºæ¥ã€‚
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_8.png)'
- en: This oneï¼Œ so let's clear this one and let's run Python multi Lr dot piã€‚ and
    then we see we have our initial learning rate of 0ã€‚1ã€‚ Then we multiply it with
    0ã€‚95 which will give us 0ã€‚095ã€‚ and the next one we again multiply it with 0ã€‚95
    and this will give us this learning rate and then this and this and so onã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªï¼Œè®©æˆ‘ä»¬æ¸…é™¤è¿™ä¸ªå¹¶è¿è¡ŒPython multi Lr dot piã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°åˆå§‹å­¦ä¹ ç‡ä¸º0.1ã€‚æ¥ç€æˆ‘ä»¬å°†å…¶ä¹˜ä»¥0.95ï¼Œè¿™å°†ç»™æˆ‘ä»¬0.095ã€‚ç„¶åä¸‹ä¸€ä¸ªï¼Œæˆ‘ä»¬å†æ¬¡ä¹˜ä»¥0.95ï¼Œè¿™å°†ç»™æˆ‘ä»¬è¿™ä¸ªå­¦ä¹ ç‡ï¼Œç„¶åæ˜¯è¿™ä¸ªï¼Œä»¥æ­¤ç±»æ¨ã€‚
- en: So again the big difference here is that here we multiply it with the learning
    rateã€‚ and with the lambda Lrï¼Œ we just use the initial learning rate and then multiply
    it with this functionã€‚ So then if we go further then we have the step Lrï¼Œ this
    is probably the most easiest to understandã€‚ So here it says it decays the learning
    rate of each parameter group by gamma every step size epoch soã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å†æ¬¡è¿™é‡Œçš„ä¸»è¦åŒºåˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†å…¶ä¹˜ä»¥å­¦ä¹ ç‡ï¼Œè€Œä½¿ç”¨lambda Lræ—¶ï¼Œæˆ‘ä»¬åªæ˜¯ä½¿ç”¨åˆå§‹å­¦ä¹ ç‡ï¼Œç„¶åå°†å…¶ä¹˜ä»¥è¿™ä¸ªå‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬ç»§ç»­å‰è¿›ï¼Œæˆ‘ä»¬æœ‰æ­¥éª¤Lrï¼Œè¿™å¯èƒ½æ˜¯æœ€ç®€å•ç†è§£çš„ã€‚å› æ­¤ï¼Œè¿™é‡Œè¯´å®ƒæ¯ä¸ªæ­¥é•¿epochå°†æ¯ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡è¡°å‡gammaã€‚
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_10.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_10.png)'
- en: It might look something like thisã€‚ So we have our step Lr with the optimizer
    then a step size and then this gammaã€‚ So as I saidï¼Œ typically we want to decrease
    itã€‚ So we set this to something smaller than oneã€‚ And then here we can see for
    the first 30 epochs we have our initial learning rate 0ã€‚05ã€‚ then we multiplied
    it with 0ã€‚1ã€‚ So we have 0ã€‚005 for the next 30 steps and then again we multiplied
    it with our gammaã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„æ­¥éª¤Lrä¸ä¼˜åŒ–å™¨ï¼Œç„¶åæ˜¯ä¸€ä¸ªæ­¥é•¿ï¼Œå†åŠ ä¸Šè¿™ä¸ªgammaã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œé€šå¸¸æˆ‘ä»¬å¸Œæœ›å‡å°‘å®ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºå°äº1çš„å€¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨å‰30ä¸ªepochä¸­ï¼Œæˆ‘ä»¬çš„åˆå§‹å­¦ä¹ ç‡ä¸º0.05ï¼Œç„¶åæˆ‘ä»¬å°†å…¶ä¹˜ä»¥0.1ã€‚æ‰€ä»¥åœ¨æ¥ä¸‹æ¥çš„30ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°0.005ï¼Œç„¶åå†æ¬¡ä¹˜ä»¥æˆ‘ä»¬çš„gammaã€‚
- en: So we have this learning rate for the next 30 steps and so on So yeahã€‚ this
    is one of the simplest oneã€‚ but it's actually really powerfulã€‚ So I used this
    very often myselfã€‚ Then we have this multi step Lr which decay the learning rate
    of each parameter group by gamma once the number of epoch reaches one of the milestonesã€‚So
    basicallyï¼Œ hereï¼Œ instead of stepsï¼Œ we just we can give it milestonesã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æœ‰æ¥ä¸‹æ¥30ä¸ªæ­¥éª¤çš„å­¦ä¹ ç‡ç­‰ç­‰ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ã€‚è¿™æ˜¯æœ€ç®€å•ä¹‹ä¸€ï¼Œä½†å®é™…ä¸Šéå¸¸å¼ºå¤§ã€‚æˆ‘è‡ªå·±éå¸¸å¸¸ç”¨è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬æœ‰è¿™ä¸ªå¤šæ­¥éª¤Lrï¼Œå®ƒåœ¨è¾¾åˆ°é‡Œç¨‹ç¢‘çš„epochæ•°é‡åï¼Œé€šè¿‡gammaè¡°å‡æ¯ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ç»™å®ƒé‡Œç¨‹ç¢‘ï¼Œè€Œä¸æ˜¯æ­¥éª¤ã€‚
- en: and then it will do the sameã€‚ So here for epoch 30ï¼Œ it will apply our gammaï¼Œ
    then for epoch 80ã€‚ it will apply our gamma and so onã€‚ So this gives us a little
    bit more variationã€‚ if we don't want to use the same step size all the timeã€‚ Then
    we have this exponential Lr hereã€‚ it decays the learning rate by gammaï¼Œ every
    epochã€‚ So this is essentially the same as the step Lrã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒå°†åšåŒæ ·çš„äº‹æƒ…ã€‚å› æ­¤ï¼Œåœ¨ç¬¬30ä¸ªepochæ—¶ï¼Œå®ƒå°†åº”ç”¨æˆ‘ä»¬çš„gammaï¼Œç„¶ååœ¨ç¬¬80ä¸ªepochæ—¶ï¼Œå®ƒå°†åº”ç”¨æˆ‘ä»¬çš„gammaï¼Œä»¥æ­¤ç±»æ¨ã€‚è¿™ç»™äº†æˆ‘ä»¬æ›´å¤šçš„å˜åŒ–ï¼Œå¦‚æœæˆ‘ä»¬ä¸æƒ³ä¸€ç›´ä½¿ç”¨ç›¸åŒçš„æ­¥é•¿ã€‚ç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œæœ‰è¿™ä¸ªæŒ‡æ•°å­¦ä¹ ç‡ï¼ˆexponential
    Lrï¼‰ï¼Œå®ƒæ¯ä¸ªepoché€šè¿‡gammaè¡°å‡å­¦ä¹ ç‡ã€‚å› æ­¤ï¼Œè¿™æœ¬è´¨ä¸Šä¸æ­¥éª¤Lrç›¸åŒã€‚
- en: if we use a step size of one hereã€‚ So here we simply apply this every epochã€‚
    and then we don't have to care about these stepsã€‚ So yeahï¼Œ that's this oneã€‚ then
    we also have this cosine uning Lrï¼Œ which I won't go over for nowï¼Œ but you canï¼Œ
    of courseã€‚ have a look at that yourselfã€‚And yeahï¼Œ but then let's have a look at
    this one againã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨æ­¥é•¿ä¸º1ã€‚é‚£ä¹ˆæˆ‘ä»¬åœ¨æ¯ä¸ªepochç®€å•åœ°åº”ç”¨è¿™ä¸ªï¼Œè€Œä¸å¿…æ‹…å¿ƒè¿™äº›æ­¥éª¤ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬è¿˜æœ‰è¿™ä¸ªä½™å¼¦è°ƒèŠ‚å­¦ä¹ ç‡ï¼ˆcosine
    tuning Lrï¼‰ï¼Œæˆ‘ç°åœ¨ä¸ä¼šè¿‡å¤šè®²è§£ï¼Œä½†ä½ å½“ç„¶å¯ä»¥è‡ªå·±æŸ¥çœ‹ã€‚å¥½å§ï¼Œä½†è®©æˆ‘ä»¬å†çœ‹ä¸€ä¸‹è¿™ä¸ªã€‚
- en: So this one is called reduce L R on plateauã€‚ So now hereï¼Œ this is not dependent
    on the epochã€‚ but here insteadï¼Œ it's dependent on some measurements or metricsã€‚
    So it reduces the learning rate when a metric has stopped improvingã€‚ So here we
    want to reduce our learning rate and Py tells us that models often benefit from
    reducing the learning rate by a factor of 2 between 2 and 10 Once learning stagnateatesã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¢«ç§°ä¸ºåœ¨å¹³ç¨³æœŸå‡å°‘å­¦ä¹ ç‡ï¼ˆreduce L R on plateauï¼‰ã€‚åœ¨è¿™é‡Œï¼Œè¿™å¹¶ä¸ä¾èµ–äºepochï¼Œè€Œæ˜¯ä¾èµ–äºä¸€äº›æµ‹é‡æˆ–æŒ‡æ ‡ã€‚å› æ­¤ï¼Œå½“æŒ‡æ ‡åœæ­¢æ”¹å–„æ—¶ï¼Œå®ƒä¼šå‡å°‘å­¦ä¹ ç‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›é™ä½å­¦ä¹ ç‡ï¼Œè€ŒPyå‘Šè¯‰æˆ‘ä»¬ï¼Œå½“å­¦ä¹ åœæ»æ—¶ï¼Œæ¨¡å‹é€šå¸¸å—ç›Šäºå°†å­¦ä¹ ç‡é™ä½2åˆ°10å€ã€‚
- en: This shaular here reads a metric quantityã€‚ And if no improvement is seen for
    a patient's number of epochsã€‚ The learning rate is reducedã€‚ So what we need here
    is we need a optrã€‚ we need a mode and a factor and a patientsã€‚ğŸ˜Šï¼ŒSo the mode will
    be min or maxã€‚ So in min modeã€‚ learning rate will be reduced when the quantity
    monitored has stopped decreasing in max modeã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè°ƒåº¦å™¨è¯»å–ä¸€ä¸ªåº¦é‡æ•°é‡ã€‚å¦‚æœåœ¨æ‚£è€…çš„ epochs æ•°é‡ä¸Šæ²¡æœ‰çœ‹åˆ°æ”¹è¿›ï¼Œå­¦ä¹ ç‡å°±ä¼šé™ä½ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦çš„æ˜¯ä¸€ä¸ªä¼˜åŒ–å™¨ã€ä¸€ä¸ªæ¨¡å¼ã€ä¸€ä¸ªå› å­å’Œä¸€ä¸ªæ‚£è€…ã€‚ğŸ˜Šï¼Œæ¨¡å¼å¯ä»¥æ˜¯æœ€å°å€¼æˆ–æœ€å¤§å€¼ã€‚åœ¨æœ€å°æ¨¡å¼ä¸‹ï¼Œå½“ç›‘æµ‹çš„æ•°é‡åœæ­¢å‡å°‘æ—¶ï¼Œå­¦ä¹ ç‡ä¼šé™ä½ï¼›åœ¨æœ€å¤§æ¨¡å¼ä¸‹ï¼Œå½“æ•°é‡åœæ­¢å¢åŠ æ—¶ï¼Œå­¦ä¹ ç‡ä¹Ÿä¼šé™ä½ã€‚
- en: it will be reduced when the quantity has stopped increasingã€‚ Then we need this
    factorã€‚ So the factor by which the learning rate will be reduced default is 0ã€‚1ã€‚
    and we need the patientsã€‚ So how long we wait for no improvementã€‚ So number of
    epochs with no improvement after which learning rate will be reducedã€‚ For exampleï¼Œ
    if patients equals2ã€‚ Then we will ignore the first two epochs with no improvement
    and will only decrease the learning rate after the third epochã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬éœ€è¦è¿™ä¸ªå› å­ã€‚å­¦ä¹ ç‡é™ä½çš„å› å­ï¼Œé»˜è®¤æ˜¯ 0.1ã€‚æˆ‘ä»¬è¿˜éœ€è¦æ‚£è€…ã€‚æˆ‘ä»¬éœ€è¦ç­‰å¤šä¹…æ‰ç®—æ²¡æœ‰æ”¹è¿›ã€‚å³åœ¨æ²¡æœ‰æ”¹è¿›çš„æƒ…å†µä¸‹ç»è¿‡çš„ epochs æ•°é‡ï¼Œåªæœ‰åœ¨ç¬¬ä¸‰ä¸ª
    epoch åå­¦ä¹ ç‡æ‰ä¼šé™ä½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚£è€…ç­‰äº 2ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å¿½ç•¥å‰ä¸¤ä¸ªæ²¡æœ‰æ”¹è¿›çš„ epochsï¼Œåªæœ‰åœ¨ç¬¬ä¸‰ä¸ª epoch åæ‰ä¼šé™ä½å­¦ä¹ ç‡ã€‚
- en: If the loss still hasn't improvedã€‚ So our code might look something like thisã€‚
    we have our optimizerã€‚ We have this reduce L R on plaã€‚With our optimizer and in
    this caseã€‚ it just uses the default factor and patienceã€‚ and then we have our
    training loop where we do the training stepã€‚ We calculate the lossã€‚ And after
    the validationï¼Œ we calculate chatular dot step with this validation lossã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæŸå¤±ä»ç„¶æ²¡æœ‰æ”¹å–„ï¼Œæˆ‘ä»¬çš„ä»£ç å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ã€‚æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬æœ‰è¿™ä¸ª reduce L R on plaã€‚ä¸æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ä¸€èµ·ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒåªæ˜¯ä½¿ç”¨é»˜è®¤çš„å› å­å’Œæ‚£è€…ã€‚æ¥ç€æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ï¼Œåœ¨è¿™é‡Œè¿›è¡Œè®­ç»ƒæ­¥éª¤ã€‚æˆ‘ä»¬è®¡ç®—æŸå¤±ã€‚éªŒè¯åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªéªŒè¯æŸå¤±è®¡ç®—
    chatular dot stepã€‚
- en: So if this doesn't get better for the number of epochs we want to waitã€‚ then
    we reduce the learning rateã€‚ and this really can help our model to stop stagnating
    and further improve the optimizationã€‚ So yeahï¼Œ then we also have this cyclic Lr
    and one moreï¼Œ I guess this onecycl Lrã€‚ which I won't go over here as wellï¼Œ but
    you can check that out for yourselfã€‚ But yeahã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœåœ¨æˆ‘ä»¬æƒ³ç­‰å¾…çš„ epochs æ•°é‡å†…æ²¡æœ‰æ”¹å–„ï¼Œæˆ‘ä»¬å°±ä¼šé™ä½å­¦ä¹ ç‡ã€‚è¿™ç¡®å®å¯ä»¥å¸®åŠ©æˆ‘ä»¬çš„æ¨¡å‹åœæ­¢åœæ»å¹¶è¿›ä¸€æ­¥æ”¹å–„ä¼˜åŒ–ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œæˆ‘ä»¬è¿˜æœ‰è¿™ä¸ªå¾ªç¯å­¦ä¹ ç‡ï¼Œè¿˜æœ‰ä¸€ä¸ªæˆ‘æƒ³åº”è¯¥æ˜¯è¿™ä¸ª
    onecycl Lrã€‚æˆ‘åœ¨è¿™é‡Œä¸ä¼šè¯¦ç»†è®²è§£ï¼Œä½†ä½ å¯ä»¥è‡ªå·±æŸ¥çœ‹ã€‚ä¸è¿‡ï¼Œæ˜¯çš„ã€‚
- en: that's what I wanted to show you for nowã€‚ So againã€‚ you can go to this API documentation
    and then check out the different optimizersã€‚So yeahã€‚ that's all I wanted to show
    you for now againã€‚ this can really help your model during the training processã€‚
    and it's not that hard to implementã€‚ So just pick a scheduler and try out different
    ones for yourselfã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ç°åœ¨æƒ³ç»™ä½ å±•ç¤ºçš„å†…å®¹ã€‚æ‰€ä»¥å†æ¬¡å¼ºè°ƒï¼Œä½ å¯ä»¥æŸ¥çœ‹è¿™ä¸ª API æ–‡æ¡£ï¼Œç„¶åäº†è§£ä¸åŒçš„ä¼˜åŒ–å™¨ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘ç›®å‰æƒ³ç»™ä½ å±•ç¤ºçš„ä¸€åˆ‡ï¼Œè¿™çœŸçš„å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸®åŠ©ä½ çš„æ¨¡å‹ï¼Œå¹¶ä¸”å®ç°èµ·æ¥å¹¶ä¸å›°éš¾ã€‚æ‰€ä»¥é€‰æ‹©ä¸€ä¸ªè°ƒåº¦å™¨ï¼Œå°è¯•ä¸åŒçš„é€‰é¡¹å§ã€‚
- en: and then call the scheduler That's dot stepã€‚ And yeahï¼Œ then you're good to goã€‚
    And I hope you enjoyed this tutorial And if you liked it then please subscribe
    to the channelã€‚ and then I hope to see you next time byeã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_12.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè°ƒç”¨è°ƒåº¦å™¨ï¼Œè¿™å°±æ˜¯ dot stepã€‚æ˜¯çš„ï¼Œæ¥ä¸‹æ¥ä½ å°±å¯ä»¥å¼€å§‹äº†ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œå¦‚æœä½ å–œæ¬¢çš„è¯ï¼Œè¯·è®¢é˜…é¢‘é“ã€‚å¸Œæœ›ä¸‹æ¬¡å†è§ï¼Œæ‹œæ‹œï¼![](img/4ac8ae1274e2571af0e178a8cbc54a12_12.png)
- en: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_13.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac8ae1274e2571af0e178a8cbc54a12_13.png)'
