# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘140åˆ†é’Ÿå…¥é—¨ PyTorchï¼Œå®˜æ–¹æ•™ç¨‹æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒç¬¬ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- PyTorch TensorBoard æ”¯æŒ - ShowMeAI - BV19L4y1t7tu

If you haven't alreadyï¼Œ you'll need to set up a Python environment with the latest versions of Pytorrch and tensor boardã€‚The commands on screen show how to do that for Conda and Pip will also be using map plotlib to manipulate imagesã€‚Once you have the dependencies installedï¼Œ you can run the companion notebook for this video in the environment you set upã€‚

![](img/15a6275b42701baef938463f259b9949_1.png)

For this modelï¼Œ we're going to be training a simple neural network to recognize different articles of clothingã€‚ We'll visualize data elements directlyã€‚ track the success of the training processã€‚ We'll use Tensor board to look under the hood at the model itselfã€‚And will do a more advanced visualization of the data set as a whole and its internal relationships for a data setã€‚

 will use fashion emministã€‚This is a set of small image tiles that depict various garments classified by the type of garment depictedã€‚For a modelï¼Œ we'll use a version of Linenette 5 tweet to accommodate the fashion Mnes data setã€‚![](img/15a6275b42701baef938463f259b9949_3.png)

We'll start by importing the libraries we need and the summary writer class from Torchdo Util's do Tenser boardã€‚This is the class wrapping the tensor board support in Pytorrch and will be your primary interface for interacting with Tensor boardã€‚It's good practice to visualize your training data prior to feeding it to your modelã€‚ especially with computer vision tasksã€‚Let's set up our data setã€‚

 We'll use Torch visionion to download training and validation splits of the data setã€‚ We'll talk more about validation shortlyï¼Œ and we'll also set up data loaders for each of the data set splits and define the classes against which we're classifyingã€‚Let's visualize a few instances of the data setã€‚We'll use an iterator to pull out a few instances of the data and create a map plotlib helper function to batch them together in a gridã€‚Let's show them in the notebookã€‚So how do we add this to Tensor boardï¼Ÿ

It's a one liner to write the data to the log directoryã€‚Note that we also called flush on our summary writer objectã€‚ This makes sure that everything we've logged through the writer has been written to diskã€‚![](img/15a6275b42701baef938463f259b9949_5.png)

Nowï¼Œ let's switch to a terminal and start tensor boardã€‚![](img/15a6275b42701baef938463f259b9949_7.png)

We'll copy the URL that the Ten award command line gave us and look at the images tabã€‚Note here that the image we've added has a header containing the label we applied when we save the image to the Tensor board log directoryã€‚Nextï¼Œ we'll use Tensor board to help assess our training processã€‚We'll graph the accumulated training loss for regular time steps and compare it to the loss measured against a validation data setã€‚

 For backgroundï¼Œ here's a brief aside on what we're doing and whyã€‚If you took a math classã€‚ it's likely that you'd be given homework problem sets to solve after a number of homework setsã€‚ you'd be given an examã€‚The exam problems would be similar in natureã€‚ but different in their specifics to the homework problems you've seen alreadyã€‚

 This is intended to make sure you learn the content of the class and not just memorize the homework problemsã€‚Similarlyï¼Œ we can use a validation data setã€‚ that is a portion of the total data set not used for training to see whether our model is learning generally or whether it's overfitted to the training data akin to memorizing the training instances instead of modeling the general function we're trying to optimize the model forã€‚

Let's set up a training loop with validation checks and graph the resultsã€‚Here we have a training loopã€‚ You can see that at the top of the codeã€‚ we declared a variable to accumulate the measured loss of the model's predictionsã€‚ which will report every thousand training stepsã€‚Well also be doing a separate loss check against the validation data setã€‚

For tracking and comparing two different quantitiesï¼Œ we'll use the add Scrs call on summary writerã€‚ which allows us to add a dictionary containing multiple scalar valuesã€‚ each with distinct tags that get their own line on the graphã€‚Let's run the hell and see what that looks likeã€‚Switching over to Tensor board and looking at the Scrs tabã€‚

 we can see that our loss is decreasing monotonically over the training runã€‚ This is a nice reassurance that the training is workingã€‚ğŸ˜Šã€‚But are we overfitting looking at them graphï¼Œ we can see that the validation and training curves are converging nicelyã€‚Nextï¼Œ let's use Tensor board to better understand our model and how data flows through itã€‚To do thisã€‚

 we'll use the add graph method on our summary letterã€‚This method takes as argumentsã€‚ the model and a sample input that will be used to trace data flow through the modelã€‚Will run the cell and switch over to tensor boardã€‚And going to the graphs tabã€‚ we can see a very simple graph showing the model with inputs going in one side and outputs submitted from the otherã€‚

Of courseï¼Œ we'll want more detail in thisï¼Œ and we can get it by double clicking on the model node in the graphã€‚ And here we can see a graph containing all of our layers and arrows indicating how data flows through themã€‚ Note that because the model uses the same max pool object twiceã€‚ the second convolutional layer appears to be embedded in a loopã€‚But as you can see from the codeã€‚

The flow is more linear than thatã€‚We've already used Tensor board to display visualizations of instances of our dataã€‚ But what about the whole data setã€‚An embedding is a mapping of instances from a higher dimensional space to a lower dimension 1ã€‚This is a common technique in N LPã€‚ If you have a 10000 word vocabulary represented by one hot vectorsã€‚ Your words are unit vectors in a 10000 dimensional spaceã€‚

If you train an embedding layer that maps these vectors to a lower dimensional spaceã€‚ relationships can emergeã€‚ For exampleï¼Œ the new vectors for words like goodã€‚ excellent and fabulous will tend to be clustered in that lower dimensional spaceã€‚In our caseã€‚ our 28 by 28 image tiles can be thought of as 784 dimensional vectorsã€‚

We can use the summary writers's add embedding method to project this down to an interactive 3D visualizationã€‚Here's a bit of code to select a random sample of our dataï¼Œ label itï¼Œ and project itã€‚Note thatã€‚ as alwaysï¼Œ we use the flush method to ensure that all our data is written to diskã€‚

![](img/15a6275b42701baef938463f259b9949_9.png)

Switching over to tensorboardã€‚![](img/15a6275b42701baef938463f259b9949_11.png)

We can see on the projector tabï¼Œ a 3D visualization of our new embeddingã€‚![](img/15a6275b42701baef938463f259b9949_13.png)

Zoomed outï¼Œ we can see some large structuresï¼Œ some arcs within the 3D spaceã€‚ zooming in on some of these structuresã€‚ We can see that some of these arcs have clustered similar carbon typesã€‚Zoom in on your own sample of the data and see if you can identify patterns in how different types of garments are clustered in this 3 D projectionã€‚

![](img/15a6275b42701baef938463f259b9949_15.png)

For more information on Py Torch's Tensor boards supportã€‚You can visit the Pytorrch documentation at Pytorchã€‚org for full documentation of torchã€‚utilsã€‚tensorboard summaryumarywriterã€‚The Pitorrch tutorials section at Piytorchã€‚ org has tutorials on using Tensorboardã€‚And the Tensorboard documentationï¼Œ of courseã€‚

 has more detail about Tensor board itselfã€‚ If you want a deeper view of what the summary writer is doing under the hoodã€‚![](img/15a6275b42701baef938463f259b9949_17.png)