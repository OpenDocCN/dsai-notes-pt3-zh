# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P11ï¼šL11- éšæœºæ£®æ— - ShowMeAI - BV1wS4y1f7z1

Hiï¼Œ everybodyã€‚ Welcome to your new machine learning from Sct tutorialã€‚ Todayã€‚ we are going to implement a random forest using only built in Python modules and Nyã€‚ The random forest algorithm is one of the most powerful and most popular algorithmã€‚ So I'm very excited that we can finally implement it in this tutorial in the last tutorialã€‚

 I explained how a single decision tree worksã€‚ So if you haven't watched the previous tutorial yetã€‚ then please do soã€‚ because our random forest model and also the implementation is based on the decision tree model from the last timeã€‚ğŸ˜Šï¼ŒSo if you have understood the decision treesï¼Œ then the approach of the random forest is very easy to understandã€‚If we have a look at this image hereï¼Œ then this shows the whole ideaã€‚

 So the idea is to combine multiple trees into a forestã€‚ so we train multiple trees and each tree gets a random subset of the training dataã€‚ thus the word randomã€‚ We then make a prediction with each of the trees at the endã€‚And we make a maturity vote then to get the final predictionã€‚So this is the whole ideaã€‚

And the random forest has some advantages compared to only one treeï¼Œ for exampleã€‚ by building more treesï¼Œ we have more chances to get the correct prediction and we also reduce the chance of overfitting with a single tree so typically the accuracy of a random forest is higher than with a single tree and that's why it's so powerfulã€‚

Soï¼Œ yeahï¼Œ now we can jump right to the implementationã€‚ Soï¼Œ of courseï¼Œ we importã€‚![](img/657eaccf75713176fcbe8f6ff6eea819_1.png)

Nampyï¼Œ Sï¼Œ and Pã€‚And then we also import the decision tree class from the last timeã€‚ so we say from decision treeï¼Œ import decision treeã€‚And now we can startã€‚ We create our class decision3ã€‚And or sorryï¼Œ nowï¼Œ now we have the random forestã€‚ So now we create our random forest classã€‚And this willã€‚Get an in itã€‚Soã€‚

 and this will get the number of trees we want to have in our forestã€‚ So let's say a number of trees equals 100 by defaultã€‚And then it also gets all the parameters from our decision tree initializezaã€‚ So it gets the minimum samples required for a splitã€‚ It gets the maximum depthã€‚

And it gets an optional number of features for some more randomnessã€‚So let's just copy and paste this hereã€‚ and then we store all of themã€‚ So we say self dot numberã€‚Of trees equals and treesï¼Œ self dot min sample split equals min sample split self dot max depth equals max depth and self dot Nã€‚Fats equalsï¼Œ and featsã€‚And then we implement our predict and our fit methodsï¼Œ soã€‚

The predict method has theã€‚Test dataã€‚ And we start with the fit methodã€‚ So we say fitã€‚Selfã€‚ and this will have the training data and the training labelsã€‚And one more thing that we want to haveï¼Œ and I can put it here first so we want to have an empty array of trees where we want to store each single tree that we now are going to createã€‚ So we say self dot trees equals and this will be an empty listã€‚And then in the fit methodã€‚

 we want to make sure that the list is empty againã€‚And nowï¼Œ we startã€‚Training our treeã€‚ So we say four underscore because we don't need this in range self dot number of treesã€‚ And now we create our treeã€‚ So we say3 equals decision  treeã€‚And this will get all the parametersã€‚ So it gets min sample split equals self dot min sample splitã€‚

Then it will have the max depth equal self dotï¼Œ max depth and the number of features equal self dotã€‚Number of featuresã€‚Andã€‚Nowï¼Œ what we want to do is we want to give our tree a random subsetã€‚ So let's define a global function hereã€‚ Let's call thisã€‚ This is also called bootstpingã€‚ So let's call thisã€‚Boott sampleï¼Œ which will get X and yã€‚

And now we just or first look at how many different number of samples we haveã€‚ So n samples equals x dot shapeã€‚So as alwaysï¼Œ this is a Ny and D where the first dimension is the number of samples and the second dimension and number of featuresã€‚And now we make a random choiceã€‚ So we say indices equals numpy random choiceã€‚And here we put in the number of samples as integerã€‚

 This means that it will make a random choice between 0 and the number of samplesã€‚ So our indices lie in this rate rangeï¼Œ and the size will be of size number of samplesï¼Œ tooã€‚But we also say replace equals trueã€‚ So this means that some of the indices can be there multiple timesã€‚ and others get droppedã€‚ So we randomly drop some of the samples and use only a subsetã€‚And thenã€‚

 we returnã€‚The xã€‚å“¦ã€‚These indicesã€‚And alsoï¼Œ the why of these indicesã€‚So now we only have these selected samplesã€‚ğŸ¼And now we can train our tree with thisã€‚ So firstã€‚ we say x sample and y sample equals bootstrap sample with X and yã€‚ And then we say tree dot fitã€‚X sampleï¼Œ and yã€‚Sampleã€‚And then we simply append this to our tree listã€‚ So we say self dotã€‚

T st a pentã€‚3ã€‚And now we are done with the training phaseã€‚ And now when we predict itã€‚ So we make a prediction with each of our treesã€‚ So we say tree pres or tree predictions equalsã€‚And here I will use a list comprehension and then convert this to a nuier arrayã€‚ So here I say tree dot predictã€‚X 43 in self dot treesã€‚

 So for each trees now we make what we call the tree predict methodã€‚ And now we want to do the maturity voteã€‚ But now we have to be careful because what we get here isã€‚Let's sayï¼Œ for exampleï¼Œ we have three trees and four samplesã€‚And then let's say our first tree for simplicity just does once as predictionsã€‚ So for each sampleã€‚

 it will have a one hereã€‚ and the second tree just makes zerosã€‚ So we have zerosã€‚ and then the third tree also just predicts oneã€‚Soï¼Œ and then againï¼Œ this would beã€‚An arrayã€‚ This would be an arrayï¼Œ and this would be an array in this array thenã€‚ Butã€‚ and now we want to do the maturity voteã€‚ So now what weã€‚

Actually one is we want a arrays that look like thisï¼Œ so we want to have 10ï¼Œ1ï¼Œ10ï¼Œ1ï¼Œ101 and 101ã€‚Soã€‚ this oneã€‚This one from all of the treesã€‚We want to have the corresponding predictionsã€‚ So we convert it to this structureã€‚ And there's a very nice function from Nmpy that is doing exactly thisã€‚ So we say tree pres equals Nmpy swapã€‚Asã€‚å—¯ã€‚With this three predictionsã€‚

 and then we swap axis0 and axis 1ã€‚ So this is doing exactly thisã€‚And now we can do the maturity voteã€‚ So we say why prediction equalsã€‚ And now we predict the most common label for each of these three predictionsï¼Œ soã€‚Nowï¼Œ if we have orã€‚1ï¼Œ0ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ1ã€‚ So now we go over themã€‚And make a maturity vote then over them and then over themã€‚

 So we use less comprehension againã€‚ And we sayï¼Œ most common labelã€‚Of a tree prediction for each tree prediction in tree predictionsã€‚And then we convert this to a nuyã€‚Array and return itã€‚And now the only thing left that we need is the most common label functionã€‚ And we also needed this in the decision tree classã€‚ So here we have the most common label functionã€‚

As a class functionã€‚ So hereï¼Œ as you seeï¼Œ we need this a lot of timeã€‚ So it might be better to do this as a global functionã€‚So we put this here and you might even put it in another file and call this from a helpback class or somethingã€‚ but we just put it hereï¼Œ soã€‚I will not explain this again if you don't know how this worksã€‚

 then please watch the last tutorial so we don't need self anymoreã€‚And we also need to import from collectionsã€‚Import the counter moduleã€‚And now we can do the maturity vote and now we are doneã€‚ so I have a little test script here to test our classã€‚ so I will import the breast cancer data set from the Ecal learn moduleã€‚ Then I will generate some training and test labelsï¼Œ Then I will create our random forest instanceã€‚And here I just use three trees because training might take some timeã€‚ and we didn't optimize our codeï¼Œ soã€‚In our videoï¼Œ I just use three nowã€‚

 Then I will fit the data and I will predict the test data and then calculate the accuracyã€‚ So let's run this and hope that everything is workingã€‚Andã€‚We made a mistakeï¼Œ so we say selfã€‚Ohã€‚ we want to append it to our treesï¼Œ of courseã€‚ And now one more tryã€‚Fingers crossedsã€‚And now we have the accuracyã€‚ So now we see that our model is workingã€‚ And yeahã€‚

 I hope you understood everythingã€‚ And if you liked itã€‚ please subscribe to the channel and see you next timeï¼Œ byeã€‚![](img/657eaccf75713176fcbe8f6ff6eea819_3.png)