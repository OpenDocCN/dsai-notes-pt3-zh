- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‰ΩøÁî® Scikit-learn ËøõË°åÊú∫Âô®Â≠¶‰π†Ôºå4Â∞èÊó∂ÂÆûÊàòËßÜËßíÂà∑Êñ∞Áü•ËØÜÊ°ÜÊû∂ÔºåÂàùÂ≠¶ËÄÖËøõÈò∂ÂøÖÂ§áÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P9Ôºö9ÔºâÂè¨ÂõûÁéáÂíåÁ≤æÂ∫¶
    - ShowMeAI - BV16u41127nr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last video we learned a little bit about confusion matrices and confusion
    matrices give you the whole picture„ÄÇ but often we want to summarize things in
    just one or two numbers and one of those most important numbers which we've already
    seen is accuracy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy tells us what percentage of the time our model is crap„ÄÇBut when it
    gets wrong„ÄÇ it doesn't really tell us what kind of mistakes are being made„ÄÇ and
    so we're going to be learning two metricsÔºå recall and precision„ÄÇ which really
    you can think of as accuracy on a subset of the data right so there's still going
    to be fractions between zero and1 but they'll kind of help us pinpoint where the
    mistakes are actually being made„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so to reviewÔºå here's a confusion matrix„ÄÇAlong the rowsÔºå I have what the
    data actually is„ÄÇ and along the columns I have what the model thinks it is„ÄÇAnd
    so right now I have zeros in all these places„ÄÇBut if I were to see an actual mouse„ÄÇAnd
    the model predicted that it's a mouseÔºå then I would go to the mouse row in the
    mouse column and increment that number by one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And that's any time we're incring numbers on the diagonal or confusion matrix„ÄÇOh„ÄÇ
    that means we made the right decision„ÄÇHere's an example of a wrong decision if
    our model took a look at that picture„ÄÇ which is clearly a dog„ÄÇIt needs in need
    of some grooming and it predicted as a cat„ÄÇ then we go to the dog grow because
    it's actually a dog and the cat column because that's what was predicted and increment
    that and so we might do this over our whole data set and we have a bunch of numbers
    there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now from that we might want to figure out what the accuracy is and the accuracy
    is well what percentage of the times where we correct and so the way I think of
    that is adding up all the numbers on that diagonal„ÄÇ that's how many we got correct
    and then dividing by all the numbers in the matrix so then well look at8 over
    10 or 80% and some observations here is that you know this number is a fraction
    of kind of a subset over a larger amount iss always be going to be between zero
    and1 and the good number is always in the numerator or accuracy so one is the
    best possible number and so precision and recall have those same properties but
    they're going to be a line different subsets of the matrix right we aren't going
    to be taking the whole diagonal divided by the whole whole matrix„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So precision and recallÔºå it turns out we can actually have these metrics„ÄÇFor
    each classÔºå right„ÄÇ So I actually I have„ÄÇSix different metrics hereÔºå I have dog
    recall cat recall mouse recall and then similar dog precision cat precision and
    mouse precision and's interesting I' look at a few of these so when I'm asking
    well what does a cat recall„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what I want to know is when we actually have a cat what percentage of the time
    is the model rightÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And so since I'm asking about what is actually the case„ÄÇ what I'm really doing
    is I'm dividing by the sum of numbers in a row„ÄÇ right because each row represents
    what„ÄÇWhat the data actually is„ÄÇIn this case right so the denominator will be the
    sum of the row and the numerator will just be a single number which is at cat
    how many times do we actually call a cat a cat in this case we'll get2 over4 and
    so this is actually one easier way to remember recall versus precision is because
    recall has an R and row also has an R„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If I'm looking at dog recallÔºå okay so when we actually have a dog„ÄÇ what percentage
    of the time is a model rightÔºå I'm just looking at that top dog row and I'm dividing
    dog dog by the sum of everything else and in this case we always get it right
    when we see a dog right so4 over4 100% dog recall„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The precision questions are asking something a little bit different what we're
    asking here is say for dog precision when the model predicts that it's a dog„ÄÇ
    what percentage of the time is it right and so when we're kind of looking at all
    the predictions now we're talking about columns right because each prediction
    is along a column in this case we're dividing dog dog top left„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: By the dog columnÔºå I have all the different things we predict when we get four
    over sex„ÄÇAnd then similarly for cat precisionÔºå we're dividing cat cat by that
    cat column we see that there's perfect precision here and hopefully what you can
    see is that they are making different kinds of mistakes right for the cat we're
    great on precision but we have a recall problem for the dog it's the opposite
    we have perfect recall but poorer precision and so these kind of two metrics that
    are kind of showing an error right cat recall and dog precision are two ways of
    looking at that same problem sometimes we see a cat and we think it's a dog„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The opposite is not true„ÄÇI'm not trying to talk about it more here but I just
    want to give you some exposure to it„ÄÇ often people try to reduce these numbers
    down to a single score„ÄÇ for example there's something popular machine learning
    called F1 score and a lot of these kind of simple scores are just combinations
    of these other metrics like precision and recall so these are kind of building
    blocks for other metrics„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4322a1ad92201482e285377a84085572_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Let me head over and write some code for this„ÄÇTo Jupyter notebook block„ÄÇü§ß„ÄÇAnd
    and in this case„ÄÇ![](img/4322a1ad92201482e285377a84085572_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: I have my confusion matrix converted to a data frame and I'm showing it down
    here„ÄÇ![](img/4322a1ad92201482e285377a84085572_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Like so I'm kind of similar to one in the slidesÔºå but the numbers are larger
    now and I also have a horse„ÄÇAnd and so the diagonal is good rightÔºå So I can see
    this is actually not doing too badÔºå right„ÄÇ I have a lot of large numbers on the
    diagonal„ÄÇI see that there's a horse problem„ÄÇWhen I see a horse„ÄÇIt actually 90%
    of the time thinks that's a dog„ÄÇ The other problem I have„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: right I big number that's not of that diagonal is right here„ÄÇAbout half of the
    cats it seas get misclassified as dogs„ÄÇ![](img/4322a1ad92201482e285377a84085572_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Okay that's a problem so what I'm going to do is I'm going look at I've already
    produced this confusion matrix„ÄÇ I want to look at things like accuracy scoreÔºå
    recall score precision score„ÄÇ then finally this new metric balanced score that
    I'll introduce„ÄÇüòäÔºåSo first„ÄÇ let's take a look at the accuracy score„ÄÇSo I'm going
    to run accuracy score„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And and I need to feed it in the actual values and then the predicted value„ÄÇ
    So I'll do that actual„ÄÇAnd predictedÔºå these are the two lists that I used„ÄÇTo construct
    my confusion matrix„ÄÇAnd I see that„ÄÇ ohÔºå let me just run this again„ÄÇAnd I see that
    the accuracy is 78„ÄÇ wellÔºå it's about 80%„ÄÇSo that seems pretty good and the key
    thing to note here right is that when we have all these different classes it might
    seem like we're doing good overall„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but there might be cases where we are making a lot of mistakes right so for
    example„ÄÇ when we see a catÔºå we end up being wrong half of the time worse when
    we see a horse„ÄÇ we're wrong 90% of the time and so these other metrics are going
    to help us dig again and actually identify that„ÄÇOkayÔºå so let's say I wanted to
    look at recall for the horse„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which I'm expecting to be 10% when I see a horse„ÄÇWe only know a 10% of the time„ÄÇ
    So one way I could do that is I could„ÄÇAnd my confusion matrix„ÄÇI could get the
    horse„ÄÇWorse value„ÄÇ right from that bottom right„ÄÇAnd and I could divide it by„ÄÇThe
    sum of all the values in the horse row„ÄÇ rightÔºå So I could do that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I get 10% just as I expectedÔºå rightÔºå The shorter way to do that would be
    to use the„ÄÇThis precision score function that's actually built then to ask K learnnÔºå
    right„ÄÇ Im going call this thing„ÄÇAnd so I have the true values and the predicted
    values„ÄÇ so I'll say actual predicted„ÄÇAnd I actually get an error here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And it's complaining about something called multiclass versus binary„ÄÇThese metrics
    are kind of set up for the simple cases where our two classes are just false and
    true„ÄÇ as opposed to four cases like dog cat Mo horse„ÄÇAnd so I have to clean that
    up a little bit„ÄÇ And and the way I may do that is„ÄÇOhÔºå wellÔºå first off„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let me expand this a little bit I have to change this average value„ÄÇ so there's
    different ways to kind of summarize information„ÄÇI understand set average to none„ÄÇAnd
    I eher know it some„ÄÇUnlike that„ÄÇ And then what it's doing is it's actually giving
    me four recalls„ÄÇ one for each of these classifications„ÄÇ Now the order might not
    be the same as up here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so I'm actually going to pass in these labels as well to make sure that
    that I can kind of actually compare these numbers to the different values right„ÄÇ
    So what I see here is that in terms of actually I want to do recall firstÔºå I'm
    sorry„ÄÇI to recall first and so for this recall right going row by row and what
    I see is that recall for the dog it is perfect I see a dog„ÄÇ the model is being
    recognized as a dogÔºå it's also perfect for mice right if it sees a mouse it's
    to recognize it as a mouse„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For catsÔºå right if it sees a cat 50Ôºå 50 on whether it will correctly identify
    itÔºå and then for horse„ÄÇ only a 10% chance said it correctly identifies it„ÄÇOkayÔºå
    those are my four recall numbers„ÄÇ sometimes what I'll want to do„ÄÇIs all want to
    kind of see how I'm doing overall by taking an average of those„ÄÇ And I get 65%
    in that taste„ÄÇ And it turns out there is a special name for this average of recall
    scores„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and that special name is the balanced accuracy score„ÄÇ right„ÄÇ So before this
    accuracy score was sayingÔºå heyÔºå we're doing 80%„ÄÇ But now I actually do this balanced„ÄÇAccuracy
    scoreÔºå it's only 65% much worse and in some ways this is more meaningful„ÄÇ the
    only reason we were very accurate before is we were seeing very few horses even
    though our model is terrible with horses„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we could just go and score is there are not many horses in the model right so
    when we're using these balance metrics it' trying to take into account for that
    is I say even though we have more dogs„ÄÇThen anything else reallyÔºå we're going
    consider these four classes equally important in terms of coming up with our score„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this will be a great one to use if you have a lot of imbalance„ÄÇIn your data
    set right„ÄÇ and accuracy can be a little bit misleading in that case„ÄÇOkayÔºå so that
    was the recall score„ÄÇ let me similarly„ÄÇSo thisÔºå I'm going to actually do the precision„ÄÇ
    which I guess I was already doing oh„ÄÇEarlier and avertently„ÄÇWhat happened there„ÄÇThere
    we go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I'm going to paste that„ÄÇ and so that I'm going to get this precision score„ÄÇ![](img/4322a1ad92201482e285377a84085572_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now I see something differentÔºå rightÔºå I see that actually we do perfect„ÄÇOn
    everything„ÄÇ except the dog„ÄÇAnd why is thatÔºüWhen I'm talking about precision„ÄÇ I'm
    really going column by column and what I actually see here is greatÔºå right„ÄÇ I
    except on the diagonalÔºå I only have zeros in each of these columns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so that means if this model is predicting a catÔºå a mouse or a horse„ÄÇ it's
    probably right only when it's predicting a dog is there a good chance that it's
    making a mistake right in that case„ÄÇ you knowÔºå only two thirds chance that it's
    actually a dog right So this model likes to predict dogs a lot„ÄÇüòäÔºåIf it predicts
    something elseÔºå it's sure it predicts a dog it only trying of two/ thirdÔºå sure„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All rightÔºå so that's the we talk about accuracy„ÄÇRecall balance accuracy„ÄÇ which
    is an average of the recalls and then precision„ÄÇ![](img/4322a1ad92201482e285377a84085572_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: One last thing I want to talk about is binary classification and so for binary
    classifications instead of cat„ÄÇ dogÔºå mouseÔºå we have just false and true„ÄÇ![](img/4322a1ad92201482e285377a84085572_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: And and so I'm computing the confusion matrix here for that„ÄÇ and and if I want
    toÔºå I can„ÄÇ![](img/4322a1ad92201482e285377a84085572_15.png)
  prefs: []
  type: TYPE_NORMAL
- en: I can compute these same metrics like I did beforeÔºå so for example if I do a
    recall score„ÄÇ![](img/4322a1ad92201482e285377a84085572_17.png)
  prefs: []
  type: TYPE_NORMAL
- en: Down hereÔºå I can pass inÔºå you knowÔºå false and true for my labels„ÄÇ![](img/4322a1ad92201482e285377a84085572_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: And„ÄÇWhy is that unhappyÔºå maybe because I didn't run this yetÔºå there we go„ÄÇ I
    can run that and and it's telling meÔºå okayÔºå row by row in that first row„ÄÇ![](img/4322a1ad92201482e285377a84085572_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: one third is track„ÄÇ rightÔºå So I then the second one„ÄÇ70% is correct right those
    are my two recall scores so I can do that just like before„ÄÇ but it turns out when
    we're dealing with binary classification metrics„ÄÇPeople will often just talk about
    predictedÔºå I'm sorry„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they'll just talk about recall and precision without specifying what class they
    mean and when they're doing that„ÄÇWhat they're talking about is the positive classÔºå
    right so if I just talk about„ÄÇRecall in general„ÄÇ oh I don't want that I'm just
    trying to talk about recall in general looks I'm talking about that positive class
    and the same thing for precision and actually this is probably the majority of
    the cases you'll see precision and recall use as kind of this special case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Where I'm having a binary classifierÔºå so just know that when we're doing that„ÄÇ
    we're talking about the positive class„ÄÇ![](img/4322a1ad92201482e285377a84085572_23.png)
  prefs: []
  type: TYPE_NORMAL
