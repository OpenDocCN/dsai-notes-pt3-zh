- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P15ÔºöL15-
    ËøÅÁßªÂ≠¶‰π† - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ welcomele to your new Pytorch tutorial In this tutorial„ÄÇ we will
    talk about transfer learning and how it can be applied in Pytorch„ÄÇ Transfer learning
    is a machine learning method where a model developed for a first task is then
    reused as the starting point for a model on a second task„ÄÇ For exampleÔºå we can
    train a model to classify birds and cats and then use the same model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: modified only a little bit in the last layerÔºå and then use the new model to
    classify bees and dogs„ÄÇ So it's a popular approach in deep learning that allows
    rapid generation of new models„ÄÇ And this is super important because training of
    a completely new model can be very time consuming„ÄÇ It can take multiple days or
    even weeks„ÄÇ So if you use a pretrained model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we typically exchange only the last layer and then do not need to train
    the whole model again„ÄÇ HoweverÔºå transfer learning can achieve pretty good performance
    results„ÄÇ and that's why it's so popular nowadays„ÄÇüòäÔºåSo let's have a look at this
    picture here„ÄÇ We have a typical CNN architecture that I already showed you in
    the last tutorial and this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's say this has been already trained on a lot of data and we have the optimized
    weights and now we only want to take the last fully connected layer„ÄÇ So this one
    here and then modify it and train the last layer on our new data„ÄÇSo then we have
    a new model that has been trained and tweaked in the last layer„ÄÇAnd yeah„ÄÇ this
    is the concept of transfer learning„ÄÇ And now let's have a look at a concrete example
    in Pytorch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So in this exampleÔºå we wantÔºå we are using the pretrained Resnet 18 C N„ÄÇ This
    is a network that is trained on more than a million images from the Inet database„ÄÇ
    And this network is 18 layers deep and can classify images into 1000 object categories„ÄÇ
    And now in our exampleÔºå we have only two classesÔºå So we only want to detect Bs
    and ants„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96844ee58fe06a9d803e6f751f732b19_1.png)'
  prefs: []
  type: TYPE_IMG
- en: And yeahÔºå so let's start„ÄÇ So in this sessionÔºå I already„ÄÇ I also want to show
    you two other new things„ÄÇ So firstÔºå the data sets image folder„ÄÇ how we can use
    this and how use a scheduleula to change the learning rate„ÄÇAnd thenÔºå of course„ÄÇ
    how transfer learning is used„ÄÇSo I already imported the things that we need„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and now we set up the data„ÄÇ and the last time we used the built in data sets
    from the Torch vision data sets„ÄÇ And now here we use the data sets dot image folder
    because we saved our data in a folder and this has to have the structure like
    this„ÄÇ So we have the folder here„ÄÇAnd then we have a training and a validation
    folder„ÄÇ So train and Val„ÄÇ And in each oneÔºå we have folders for each class„ÄÇ So
    here we have ans in ants and Bs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and also in the validation folderÔºå we have ants and Bs„ÄÇAnd now in each folder„ÄÇ
    we have the images here„ÄÇ SoÔºå for exampleÔºå hereÔºå we have some ants„ÄÇ And also„ÄÇ let's
    have a look at some Bs„ÄÇ So here we have a B„ÄÇAnd yeah„ÄÇ so you must structure your
    folder like this„ÄÇ And then you can call the data sets dot image folder and give
    it the path„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we also give it some transforms here„ÄÇAnd thenÔºå we get the„ÄÇClasses„ÄÇ the class
    names by calling image setsÔºå image data sets dot classes„ÄÇAnd yeah„ÄÇ then here I
    defined the training model where I did the loop and did the training and the evaluation„ÄÇ
    I will not go into detail here„ÄÇ you should already know this from the last tutorials
    how at typical training and evaluation loop looks like you can also check the
    whole code on Gitthub„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so I will provide the link in the description„ÄÇ So have a look at this yourself„ÄÇAnd
    now let's use transfer learning„ÄÇ SoÔºå first of allÔºå we want to import the pre trained
    model„ÄÇ So let's set up this model so we can do this by saying model„ÄÇSo model equals„ÄÇ
    And this is available in the Torch visionion dot models module„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I imported torch vision models already„ÄÇ and then I can call models dot Resnet
    16 or sorry„ÄÇ Resnet 18 here„ÄÇ And then I can say pretrained equals true„ÄÇ So this
    is already the optimized weights that are trained on the imagenet data„ÄÇAnd now
    what we want to do is we want to exchange the last fully connected layer„ÄÇ So first
    of all„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's get the number of input features from the last layer„ÄÇ So let's say nu
    features equals model„ÄÇ and we can get this by calling dot F fully connected„ÄÇ And
    then the input features„ÄÇ So this is the number of input features for the last
    layer that we need„ÄÇAnd then let's create a new layer and assign it to the last
    layer„ÄÇ So let's say model„ÄÇDot F C equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we give it a new fully connected layer N N dot linear„ÄÇ And this gets
    the number of input features that we have„ÄÇAnd then as new output features„ÄÇ number
    of outputsÔºå we have two because we have two classes now„ÄÇAnd now we send our model
    to the device„ÄÇ If we have GP support„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we created our device in the beginningÔºå as always„ÄÇ So this is Kuda or simply
    CPU„ÄÇAnd now that we have our new modelÔºå we canÔºå againÔºå as alwaysÔºå define our loss
    and optimize us„ÄÇ So we say criterion„ÄÇEquals NÔºå N dot cross entropis„ÄÇAnd then let's
    say the optr equals„ÄÇ This is from the optimization moduleÔºå Opim dot S GDÔºå stochastic
    gradient descent„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which has to optimize the model parameters„ÄÇ And we have to specify the learning
    rate equals„ÄÇ let's say0001„ÄÇAnd„ÄÇNowÔºå as a new thingÔºå let's use a scheduleula„ÄÇ This
    will update the learning rate„ÄÇ so„ÄÇFor thisÔºå we can say we can create this by saying
    it's called a step L R scheduleular„ÄÇEquals„ÄÇ and the L R scheduletula is available
    also in the torch optimization module„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we already imported this„ÄÇ And then we can say L R scheduletula dot step L
    R„ÄÇ And then here we have to give it the optimizer„ÄÇ So here we say optimizer„ÄÇAnd
    then we say step size„ÄÇ step size equals 7„ÄÇ And then we say gamma equals let's
    sayÔºå01„ÄÇ This means that every 7 epochs„ÄÇ Our learning rate is multiplied by this
    value„ÄÇ So every 7 epochs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Our learning rate has only 10 is now only updated to 10 per cent„ÄÇSoÔºå yeah„ÄÇ this
    is how we use a scheduletula„ÄÇ And then typically what we want to do is in our
    loop„ÄÇ in our loop over the epoch„ÄÇ So for epoch in rangeÔºå let's sayÔºå100„ÄÇ And then
    typically here we use the trainingÔºå where we also do„ÄÇThe„ÄÇThe optimizer dot step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: optimizer dot step„ÄÇ Then we want to evaluate itÔºå evaluate it„ÄÇ And then we also
    have to call scheduler step scheduler step„ÄÇ So this is how we use a scheduler„ÄÇ
    Please have a look at the whole loop here yourself„ÄÇSo yeah„ÄÇ now we set up the
    scheduleular and let's„ÄÇCall the training functions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we say model equals and then train model„ÄÇ So this is the function that
    I created„ÄÇ And then I have to pass the modelÔºå the criterion„ÄÇThe optimizer„ÄÇ the
    scheduler and also the number of epochs„ÄÇ So nu epochsÔºå let's say 20„ÄÇAnd„ÄÇYeah„ÄÇ
    so this is how we useÔºå how we can use transfer learning„ÄÇ So in this case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we use a technique that is called fine tuning„ÄÇBecause hereÔºå we„ÄÇTrain the whole
    model again„ÄÇ but only a little bit„ÄÇ So we fine tune all the weights based on the
    new data„ÄÇAnd with the new last layer„ÄÇ So this is one option„ÄÇ And the second one
    is for this„ÄÇ I copy and paste the same thing„ÄÇAnd„ÄÇLet's see where does it start„ÄÇ
    So here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then as a second optionÔºå what we can do is we can freeze all the„ÄÇAll the
    layers in the beginning and only train the very last layer„ÄÇ So for this„ÄÇ we have
    to loop over all the parameters here after we got our model„ÄÇ So we say 4„ÄÇPm in
    model„ÄÇDot parameters„ÄÇAnd then we can set there requires gra attribute to fall
    so we can say para„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Dot requires gr„ÄÇAnd then sayÔºå weÔºå sorryÔºå Do requires gra„ÄÇRequires gra equals
    false„ÄÇ Now we have it„ÄÇ and this will freeze all the layers in the beginning„ÄÇ And
    now we set up the new last layer„ÄÇ We create a new layer here„ÄÇ and by defaultÔºå
    this has requires gra equals true„ÄÇAnd then again„ÄÇ we set up the loss and optimizer
    and the schedule in this case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we do the training function again„ÄÇ And so yeahÔºå so this is even more
    faster„ÄÇ And let's run this and then have a look at both the evaluations„ÄÇ And I
    also print out the time that it took„ÄÇSo„ÄÇYeahÔºå let's save this„ÄÇ and let's run this
    by saying Python transfer dot pi„ÄÇAnd„ÄÇThis might first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will download all the imagesÔºå and this might take a couple of seconds because
    I don't have GP support here on my MacBook„ÄÇ So I will skip thisÔºå and then I will
    see you in a second„ÄÇAll rightÔºå SoaÔºå I am back„ÄÇ So this took super long on my computerÔºå
    so I reset„ÄÇThe number of epochs to just2 in this example„ÄÇ So let's have a look
    at the results„ÄÇ So after only two epochs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this is the first training where we did the fine tuning of the whole„ÄÇModel„ÄÇ
    so this took three and a half minutes„ÄÇ and the best accuracy now is „ÄÇ92„ÄÇ So 92%„ÄÇAnd
    then this is the second training where we only trained the last layer„ÄÇ So this
    took only one and a half minutes approximately„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and yeah accuracy is also is already over 80%„ÄÇ So of course„ÄÇ it's not as good
    as in when we train the whole trainingÔºå but still pretty good for only two epochs„ÄÇ
    And now let's imagine if we set the number of epochs even higher„ÄÇ So yeah„ÄÇ this
    is why transfer learning is so powerful because we have a pretrained model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we only fine tune it a little bit and do a completely new task and
    then achieve pretty good results too„ÄÇ So yeahÔºå so now I hope you understood how
    transfer learning can be applied in Pythtorch„ÄÇ if you enjoyed the tutorial„ÄÇ Please
    subscribe to the channel and see you next time by„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96844ee58fe06a9d803e6f751f732b19_3.png)'
  prefs: []
  type: TYPE_IMG
