# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P6ÔºöL6- Êú¥Á¥†Ë¥ùÂè∂ÊñØ - ShowMeAI - BV1wS4y1f7z1

HiÔºå everybody„ÄÇ Welcome to a new machine learning from scratch tutorial„ÄÇ Today„ÄÇ

 we are going to implement the naive by classifier using only built and Python modules and Ny„ÄÇüòä„ÄÇ

So the naive bias classifier is based on the biaseth theoremÔºå which says that if we have two events„ÄÇ

 A and BÔºå then the probability of event AÔºå given that B has already happened„ÄÇ

 is equal to the probability of BÔºå given that A has happened„ÄÇ

 times the probability of a divided by the probability of B„ÄÇAnd if we apply this to our case„ÄÇ

 then our formula is the probability of y of our class Y„ÄÇ

 given the feature vector X is equal to the probability of x given y times P of y divided by P of x„ÄÇ

And where x is our feature of vectorÔºå which consists of several features„ÄÇ

And now it's called naive by years because now we make the assumption that all features are mutually independent„ÄÇ

 which meansÔºå for exampleÔºå if you want to predict the probability that a person is going out for a run„ÄÇ

Given the feature that the sun is shining and alsoÔºå given the feature that the person is healthy„ÄÇ

Then both of these features might be independentÔºå but both contribute to this probability that the person goes out„ÄÇ

So in real lifeÔºå a lot of features are not mutually independent„ÄÇ

 but this assumption works fine for a lot of problems„ÄÇAnd with this assumptionÔºå we can split„ÄÇThis„ÄÇ

Probability into the and use the chain rule„ÄÇ So we calculate the probability for each„ÄÇFeature„ÄÇ

Given why and multiply each„ÄÇAnd then we multiply it with p of y and divided by p of x„ÄÇAnd by the way„ÄÇ

 this P of y given x is called the posterior probability„ÄÇ

 P of x given y is called the class conditional probability„ÄÇ

 and P of y is called the prior probability of yÔºå and P of x is called the prior probability of x„ÄÇ

And now we want to make a classification„ÄÇ So given this posterior probability„ÄÇ

 we want to select the class with the highest probability„ÄÇ So we choose yÔºå which is the arc max of y„ÄÇ

Of this posterior probability„ÄÇAnd now we can apply our formula„ÄÇ

 And since we are only interested in YÔºå we don't need this P of x so we can cross this out„ÄÇ

 And then our formula is thisÔºå so„ÄÇWhy is the arc max of„ÄÇ

And then we multiply each class conditional probabilityÔºå and then the prior probability„ÄÇ

And then we use a little trick„ÄÇSince all these values are are probabilities between 0 and 1„ÄÇ

 So if we multiply a lot of these valuesÔºå then we get very small numbers„ÄÇ

 and we might run into overflow problems„ÄÇ So in order to prevent thisÔºå we apply the lock function„ÄÇ

 So we apply the lock for each of these probabilities„ÄÇ

 and with the lock or the rules for logarithmussÔºå we can change the multiplication sign into an a plus sign„ÄÇ

 So now we have an addition here„ÄÇAnd now we have this formula that we need„ÄÇ

 and now we need to come up with this„ÄÇAP probabilityÔºå So the prior probability is just the frequency„ÄÇ

We can see this in a second„ÄÇ And then what is this class conditional probabilityÔºå P of x given y„ÄÇ

And here we model this with a gausian distribution„ÄÇ So here we can see the formula„ÄÇ

 So this is one over„ÄÇ and then the square root of2 pi times the variance of y„ÄÇ

Times the exponential function of minus x minus the mean value squared divided by two times the variance„ÄÇ

And here we see a plot of the Gaussian function for different means and variations variances„ÄÇ

So this is a probability that is always between0 and one and„ÄÇYeahÔºå with this formulas„ÄÇ

 this is all we need to get started„ÄÇ So now we can„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_1.png)

Start and implement it„ÄÇ And first of allÔºå of courseÔºå we import Ny S N P„ÄÇ

And then we create a class called naive„ÄÇBut is„ÄÇAnd it doesn't need an in it method so we can implement the fit method first„ÄÇ

 so we want to fit training data and training labels„ÄÇ

 And then we also want to implement a predict method„ÄÇ

So here we predict the test labels or test samples„ÄÇAnd now let's start„ÄÇ

 So let's start with the fit method„ÄÇ So what we can do here„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_3.png)

Is so we need the priorsÔºå and we can calculate them in this fit method„ÄÇ

 and we need the class conditional„ÄÇ So here we need the mean for each class and the variance for each class„ÄÇ

 So we can also calculate these„ÄÇ

![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_5.png)

So let's do this„ÄÇ So let's get the number of samples and the number of features first„ÄÇAnd by the way„ÄÇ

 our input hereÔºå the X is a nuy N D array where the first dimension is the number of samples„ÄÇ

 and the second dimension or the number of rows is the number of samples and the number of columns is the number of features„ÄÇ

 so we can unpack this and say this is x dot shape„ÄÇAnd our y is a 1 D row vector also of size„ÄÇ

 the number of samples„ÄÇ So this is our input„ÄÇ And now let's get the unique classes„ÄÇ

 let's say self classes equals nuy unique of y„ÄÇ So this will find the unique elements of an array„ÄÇ

 So if we have two classesÔºå0 and 1„ÄÇ then this will be an array„ÄÇJust with 1Ôºå0 and 1 y in it and1„ÄÇ

1 in it„ÄÇThen let's say the number of classes equals„ÄÇü§¢ÔºåThe length of this self„ÄÇClasses„ÄÇAnd now„ÄÇ

 let's in it or„ÄÇIn it mean„ÄÇVariarianance and priors„ÄÇ So let's say self dot mean equals„ÄÇ

 And we want to in them with zeros at first„ÄÇ And it gets the size„ÄÇ

 It has number of classes and number of features as tuple here„ÄÇ So it also„ÄÇFor each„ÄÇClass„ÄÇIt has„ÄÇ

The same number of„ÄÇOr for each class„ÄÇWe need„ÄÇMeans for each feature„ÄÇ

And we want to get this or give this a data type of float nuy dot float 64„ÄÇ

And we want to do this with the same for the variances„ÄÇ So let's say self do„ÄÇÁé©ÂÑø„ÄÇEquals this„ÄÇ

 And then we want to do self dot„ÄÇPriers equals N dot zeros„ÄÇ And here for each class„ÄÇ

 we want one prior„ÄÇ So this is just a 1 D vector of size number of classes with a data type of N dot float 64„ÄÇ

And now let's calculate them„ÄÇ So for each class in self dot classesÔºå we„ÄÇNow„ÄÇ

 only we only want the samples that has this class as labels„ÄÇ So let's call this XÔºå C equals X„ÄÇ

 and then where„ÄÇC equals equals„ÄÇWhy„ÄÇAnd now we can calculate the mean for each class and fill our self do mean„ÄÇ

 So we want to fill„ÄÇThis„ÄÇRow and all columns here„ÄÇ And we sayÔºå this is x„ÄÇXÔºå C„ÄÇ

 dot and nuly has a or a N D„ÄÇAray has to built in mean functions„ÄÇ So we can say mean along the axis0„ÄÇ

 So please check the mean„ÄÇFunction for yourself„ÄÇAnd we want to do the same thing for var„ÄÇ

 So self var in this row for each column„ÄÇIs X C dot var„ÄÇ So Numpy also has a var method„ÄÇ

And then we calculate the priorÔºå So self„ÄÇFriers„ÄÇOf this class„ÄÇIs equal to„ÄÇ And now„ÄÇ

 what information do we already have„ÄÇIf we have the the training samples and training labels„ÄÇ

 so we can say the prior probability that this class will occur is equal to the frequency of this class in the training samples„ÄÇ

SoÔºå we say„ÄÇWe get XÔºå C„ÄÇThe shape„ÄÇ0Ôºå so only„ÄÇThis will get the number„ÄÇOf samples with this label„ÄÇ

And then we divide it by the number of total samples„ÄÇ So„ÄÇ

 and we have to convert this to a float because we don't want integers here„ÄÇ

 So let's say float number of samples„ÄÇSo this is the frequency„ÄÇ How often this class C is occurring„ÄÇ

And now this is all for our fit method„ÄÇ And now let's implement the predict method„ÄÇ So and for this„ÄÇ

 we create a little helper method„ÄÇ So let's call this underscore predict self„ÄÇ

 and this will only get one samples„ÄÇ So here we have we can have multiple samples„ÄÇ

 So here we say why predict equals and then we use list comprehension„ÄÇAnd called this underscore„ÄÇ

 predict method„ÄÇFor only one sample„ÄÇ And then we do this for each sample in our test samples„ÄÇ

And thenÔºå we return them„ÄÇAnd now we have to implement our underscore predict method„ÄÇSo here„ÄÇ

 what we need now is we need„ÄÇ

![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_7.png)

To apply this function„ÄÇSo„ÄÇWe have to calculate the posterior probability„ÄÇAnd calculate the„ÄÇ

Class conditional and the prior for each one„ÄÇ And then choose the class with the highest probability„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_9.png)

So let's create an empty list called posteriors„ÄÇEquals an empty list„ÄÇ

And now let's go over each class„ÄÇSo let's say for index and C in„ÄÇEnumerate„ÄÇSelf dot classes„ÄÇ

 So here we get also the index„ÄÇAnd the class ladleÔºå with this enumerate function„ÄÇAnd now we can say„ÄÇ

FirstÔºå we get the pia„ÄÇ So the pia equalsÔºå and we already have calculated the p„ÄÇ So this is„ÄÇ

The prior of self dot„ÄÇPriers„ÄÇWith this in current index„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_11.png)

And nowÔºå as I saidÔºå then at the endÔºå we apply the lock function„ÄÇ So let's apply this right here„ÄÇ

 So let's say NP P dot lock„ÄÇ

![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_13.png)

And then create the posterior„ÄÇSoÔºå the posterior equals„ÄÇonÔºå let's call this„ÄÇClass„ÄÇConditional„ÄÇEquals„ÄÇ

And then we apply the gause functions„ÄÇ So for this„ÄÇ

 let's create this helper function and call this probability density function„ÄÇWith self„ÄÇ

 And then it gets„ÄÇThe class index„ÄÇAnd then it gets x„ÄÇAnd here we apply this formula here„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_15.png)

So we need the mean and the variance„ÄÇAnd then„ÄÇÂóØ„ÄÇApply this function„ÄÇ SoÔºå first of all„ÄÇ

 let's get the mean equals„ÄÇ and we already have the mean„ÄÇ so we can say self„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_17.png)

Mean„ÄÇOf this class index and also the variance equal self„ÄÇVre of this class index„ÄÇ

And then let's create a de numeratorÔºå which is equals to nuumpy dot x„ÄÇSo„ÄÇ

 the exponential function of minus„ÄÇAnd then we have x minus mean„ÄÇTo the power of two„ÄÇDivided by„ÄÇ

And thenÔºå two times„ÄÇThe variances„ÄÇAnd thenÔºå we have the„ÄÇDnominatorÔºå sub denominator equals„ÄÇ

And here we have N dot„ÄÇS„ÄÇCo are Ts over the square root of„ÄÇTwo times„ÄÇ And we have pie„ÄÇ

 We can also get this from NamiÔºå and Peter pie„ÄÇTimes thevaÔºå and then we return„ÄÇNummerator„ÄÇ

 divided by„ÄÇDo you know me Na„ÄÇSo this is our probability density function„ÄÇ

 And then we can say our class conditional„ÄÇIs„ÄÇÂóØ„ÄÇSelf„ÄÇDot„ÄÇProbability density function of our index„ÄÇ

And x„ÄÇAnd then„ÄÇÂóØ„ÄÇWe want to„ÄÇWe want to have the logo rimore„ÄÇ So we have says N dot lock„ÄÇAnd then„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_19.png)

We sum all of them up„ÄÇ

![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_21.png)

SoÔºå we can use„ÄÇNampy dot some of this„ÄÇÂóØ„ÄÇAnd then we say our post„ÄÇTior equals„ÄÇPyaÔºå plus„ÄÇ

The class conditions„ÄÇOur class conditional„ÄÇAnd then we have penned them to our posterior„ÄÇ

 So posteriors dot append posterior„ÄÇ

![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_23.png)

And now we use or we apply the arc max of this and choose the class with the highest probability„ÄÇ

 So Ny also has a arc max function„ÄÇ So this is very easyÔºå so we can now say return self dot„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_25.png)

Classes„ÄÇOffÔºå and the index is now„ÄÇËøô„ÄÇIndex of the„ÄÇPoeriors with the highest„ÄÇProbability„ÄÇSo we can say„ÄÇ

 N dot Arc max„ÄÇOf this poster„ÄÇAnd nowÔºå we are done„ÄÇSo this is the whole implementation„ÄÇAnd„ÄÇ

Now we can run it„ÄÇ So I already have a little„ÄÇTest script where I use the p Psych could learn library to load a data set and create a data set with 1000 samples and 10 features for each sample„ÄÇ

 And we have two classes„ÄÇ Then we split our data into training and testing samples and the labels„ÄÇ

Then we create our naive bias classifierÔºå which I import from this file„ÄÇThat I have here„ÄÇ

And then I fit the training data and the training labels„ÄÇ

 and then I predict get the predictions for the test samples„ÄÇAnd then I will calculate the accuracy„ÄÇ

 So let's run this and see if this happenÔºå if this is working„ÄÇSo yetÔºå's working„ÄÇ

 And we have a classification accuracy of 0„ÄÇ96„ÄÇ So pretty good„ÄÇSo yeahÔºå that's it„ÄÇ

 I hope you enjoyed this tutorial and see you next timeÔºå bye„ÄÇ



![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_27.png)