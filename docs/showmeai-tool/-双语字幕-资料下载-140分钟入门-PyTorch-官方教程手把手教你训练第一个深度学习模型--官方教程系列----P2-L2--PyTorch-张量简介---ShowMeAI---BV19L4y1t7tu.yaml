- en: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P2：L2- PyTorch 张量简介
    - ShowMeAI - BV19L4y1t7tu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome。In this video， we're going to do a deep dive on pieytorrchtensors。In
    a Pytorch deep learning model。All of your data inputs， outputs， learning weights。
    it is only to be expressed as tensors， multidisional arrays that can contain floating
    point。 integer or boolean data。In particular in this video。
  prefs: []
  type: TYPE_NORMAL
- en: we're going to go over some of the ways to create pie torch tensors。How to use
    tensors in mathematical and logical operations alone or with each other。Methods
    for copying tensors。How to move to GPU for hardware acceleration？
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating tensor shapes and the pie torch nuy bridge。If you haven't already。
    I recommend going to the Pytorch examples Repo and downloading the interactive
    notebook that goes with this video。![](img/629ed54063cce12da521281bede5aad8_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: Okay， so in the first cell here， we'll import PyTtorrch。 we're also going to
    import Python's math module to use some constant status has。First thing we're
    going to go over is creating tensors。 so here we have the very simplest way to
    create a tensor， the torchdot empty call。
  prefs: []
  type: TYPE_NORMAL
- en: The torch module has multiple factory methods that will let you create tensors
    with and without initial values and whatever data type you need。 this is the most
    basic way to allocate a tensorrch do empty here it's going to create a three by
    four tensor。
  prefs: []
  type: TYPE_NORMAL
- en: And we can see。That the object itself。Is of type torch dot tensor。Now when you
    run this cell you may see random looking values in the output that's because Torchta
    empty just allocates memory and does not write any values to it。 so whatever happened
    to be memory at the time you allocated this tensor is what you're going to see
    here。One quick note about tensors and their dimensions and terminology。
  prefs: []
  type: TYPE_NORMAL
- en: sometimes when we have one dimensional tensor， we will call it a vector because
    it's just an ordered tuple of dimensions。A of coordinates。Likewise， a two dimensional
    tensor is often referred to as a matrix。 and anything larger well always call
    a tensor。Now， more often than not。 you'll want to initialize your tensor with
    some value。
  prefs: []
  type: TYPE_NORMAL
- en: Common cases are all zeros are all ones or random values。 And the torch module
    provides factory methods for all of these。 So here if we run the cell。You get
    the things that you might expect in the method names。 you get a two by three tensor
    full of zeros， a  two by three tensor full of ones。
  prefs: []
  type: TYPE_NORMAL
- en: and then tensor full of random values between 0 and 1。 Now speaking of the random
    tensor。 you might have spotted the call to torchdo manual seed rip before instantiating
    that tensor。So what's that about now， initializing tensors such as your model
    learning weights with random values is very common。 but often you will want your
    results to be reproducible。
  prefs: []
  type: TYPE_NORMAL
- en: especially if you're working in a research setting。 So Ptorrch gives you a tool
    for doing it。 the manual seed call。 anytime you call manual seed with a particular
    integer seed you will reinitialize your pseudorandom number generators and get
    the same results again when you call them。
  prefs: []
  type: TYPE_NORMAL
- en: So here in the following cell as an example。We call manual seed， we call Torchdot
    Rand。 we get some values out， we call Torchdot Rand again and get some values
    out。Then when we call a manual seed again and do those two torchdot random calls。
    we'll see both times they yield the same values。 So this is how you make sure
    that identical computations that depend on random numbers will provide identical
    results if you need that reproducibility。
  prefs: []
  type: TYPE_NORMAL
- en: So often， when you are performing operations on two or more tensors。 they will
    need to be of the same shape。 that is having the same number of dimensions and
    the same number of cells in each dimension or the same extent in each dimension。All
    the factory methods I've shown you on the torch module so far have corresponding
    methods。Appended with underscore like。And when you pass in a tensor as an argument
    to empty like or zeros lir。
  prefs: []
  type: TYPE_NORMAL
- en: with these other methods， you will get out a tensor initialized as you specify。
    but of the same shape as a tensor you passed in as an argument。So here we've run
    the cell and we can see that our initial tensor was2 by2 by3 and even though we
    specified no shape for the O tensors。 they all will also come out two by two by3
    and initialized in the way you'd expect。
  prefs: []
  type: TYPE_NORMAL
- en: When we want to find out the shape of the tensor， we can always query its shape
    property。And this will give us back a list of the dimensions and their extents。Now
    the last way to create a tensor that we're going to cover is to specify its data
    directly from a Ptorrch collection。 So here if you look at these examples， we
    have a nested array and we have a tuple and we have a tuple that contains a tuple
    in a list。
  prefs: []
  type: TYPE_NORMAL
- en: And when we call Torchdot tensor with any of these collections as an argument。
    we get back a new tensor that is initialized with the data we specified。 So here
    you can see in all three cases， we've gotten back a tensor that is of the shape
    and containing the data that we'd expect。 So torchdot tensor creates a copy of
    the data。 This is important to know the underlying memory representation of a
    Python list is not the same as the underlying memory representation of a tensor。
  prefs: []
  type: TYPE_NORMAL
- en: So we always copy that data when we're creating a new tensor and initializing
    it with data in this way。Now I mentioned earlier that tensors can have floating
    point or integer Boolean underlying data types。 the simplest way to specify your
    data type is to do it at creation time。So here in this cell I'm creating an int16
    and a float 64 and you'll see A when I print it out is a set of ones represented
    as 16 bit integers。
  prefs: []
  type: TYPE_NORMAL
- en: and you can see none of the ones have that little decimal point after them。
    which is Python's subtle signal that we're dealing with an int rather than the
    float。We also could see that because we overro the default data type。 the default
    is a 32 bit floating point。 When we print the tensor。
  prefs: []
  type: TYPE_NORMAL
- en: Ptorrch helpful reports to us that this is the underlying data type of that
    tensor。Likewise。 when we do a 64 bit float。The other way to change the data type
    of a tensor or to really move it to a new tensor with your required data type。Is
    with the two method。 So here I'm calling B do2 and saying I would rather have
    this data as 32 bit integers。 And if you look closely at the values of B and C
    when they're printed out。
  prefs: []
  type: TYPE_NORMAL
- en: the values of C are just the values of B truncated to make them integers。 So
    it's a float to conversion there。The other thing you may have noticed here is
    that here specified the dimensions of the tensor as a tuple。 canonically Pytorch
    expects a tuple for a tensor's dimensions。 but when the dimensions are the first
    argument of a method。
  prefs: []
  type: TYPE_NORMAL
- en: it lets us cheat a little and just put in a series of integers。 but here to
    make the code a little more readable， I separate up the tensor's shape as a tuple。The
    data types you can use are Boolean five types of eventss and four types of float。Let's
    look at basic arithmetic first and how we can make tensors interact with scalrs
    now if we run this cell。
  prefs: []
  type: TYPE_NORMAL
- en: See， let's look at the first line here， we're going to create a tensor full
    of zeros。 and we're going to add the integer1 to it。 So what does that mean to
    add an integer to a tensor。 Well here。We're going to be doing this operation element
    wise over every element of the tensor。 so every zero in that tensor should have
    a one added to it。 And if we look at our output。
  prefs: []
  type: TYPE_NORMAL
- en: that is in fact， what we see。Likewise， with multiplication， division subtraction，
    exponentiation。 with integer or floating point powers， I'll also note that because
    the binary operation between a tensor and a scalar puts out a tensor of the same
    shape you had originally。
  prefs: []
  type: TYPE_NORMAL
- en: you can chain together these arithmetic operations intuitively。 and you can
    see that in the line where we created the threes。NowDo these same arithmetic operations
    with two tensors behaves sort of into a like you'd expect。 So we take our our
    twos。 Our little two by two tens are full of two floating point2s。
  prefs: []
  type: TYPE_NORMAL
- en: We' going to're going to use the exponiation operator。 Now we're going to specify
    the powers 1，2，3。 and 4。 And so here the。Mathematical operation is going to be
    done。Element wise between corresponding elements of each tensor because they're
    of the same shape。And so if we。Run this cell， you can see that in fact， powers
    of two are in the first tensor。
  prefs: []
  type: TYPE_NORMAL
- en: We've added two tensors of ones and fours to get fives， and if we multiply threes
    and fours。 we get12s。A key thing here is that all the tensors that we've shown
    you in these examples of these tensor binary operations are of identical shape。So
    we can see when we run this cell that when we try to do an operation with two
    tensors of different shape。 we get a runtime error， even though these two tensors
    have the exact same number of cells。
  prefs: []
  type: TYPE_NORMAL
- en: there's no natural way to map between the two。So in the general case。 your tenors
    will have to be of the same shape， there is one important and useful exception
    to that。 and that is what we call broadcasting。Here is an example， so I created
    a random tensor。 two rows by four columns， and I'm multiplying it here you can
    see by a tensor with one row and four columns and we actually get out something
    like what we'd expect so see our random output in the first print statement in
    our second print statement shows all of that doubled。
  prefs: []
  type: TYPE_NORMAL
- en: Well how do we do this， how did we multiply two tensors of different shapes
    and get an intuitive result？
  prefs: []
  type: TYPE_NORMAL
- en: So broadcasting is a way to perform an operation between tensors that have specific
    similarities in their shapes。So here in the cell preus， the one row four column
    tensor was multiplied element wise by each of the two four column rows of the
    random tensor。
  prefs: []
  type: TYPE_NORMAL
- en: So this is an important operation in deep learning。One common example is using
    batches of inputs。 so your piytorrch machine learning model will in the general
    case not expect a single input for either training or inference。 but we will expect
    a batch of inputs so here applying an operation to each instance in the batch
    separately but returning a tensor of the same shape is what you'd expect so here
    we have in our random tensor we had two rows a random values we multiplied by
    one row of twos doing each row individually and that's akin to the batch operation
    that we're performing some operation on each segment of a tensor separately。
  prefs: []
  type: TYPE_NORMAL
- en: There are rules for broadcasting。 The first one is that no empty tensors。 so
    every tensor must have at least one dimension， and then there are some rules for
    the relationship between the dimensions and extents of the two tensors that you
    want to perform an operation on。
  prefs: []
  type: TYPE_NORMAL
- en: So when we compare the dimension sizes of the two tensors going from the last
    to the first。 have to have either each dimension must be equal， or one of the
    dimensions must be of size 1。 or the dimension doesn't exist in one of the tensors。
    here are some examples that show the rules that I just described。
  prefs: []
  type: TYPE_NORMAL
- en: It's probably easier to look at these than to try to reason them out so we start
    with a tensor full of ones。 it's a three dimensional tensor with four layers，
    three rows and two columns。And we will multiply that by a random 3 by two tensor。
    If we look at the output of that。 we can see that we multiplied our random tensor
    by each of the four layers of our original tensor full of ones。
  prefs: []
  type: TYPE_NORMAL
- en: And sos what we say， the operation is broadcast over those layers over that
    first dimension。Likewise。 in the following line here we multiply a times another
    random tensor to get C。 This time we're doing a3 by one tensor， and so what does
    that give us。 this follows the rules because in the last dimension one of the
    dimensions is one and the second dimension they match and then the first dimension
    is absent in one of the tensors。
  prefs: []
  type: TYPE_NORMAL
- en: The output there。Looks like this。 So if we think of our random tensor that went
    into making C as a three element column vector。 which you can see in the output
    here， when we multiply it by a bunch of ones is that every three element column。In
    our output， tensor is the same， so we broadcast this operation over every three
    element column in our tensor。Likewise， in the final example， multiplying a random
    one by two tensor times are a tensor full of ones。
  prefs: []
  type: TYPE_NORMAL
- en: That does something akin to the previous time， except now。 instead of every
    three element column having the operation performed on it。 now every two element
    row has the operation performed on it。Now there's a Pytorch documentation note
    on this topic of broadcasting。
  prefs: []
  type: TYPE_NORMAL
- en: and I urge you to read it if you are interested in more details。Now to give
    you an idea of some operations that will break the rules and not work。 all these
    lines should give you a runtime error so in the first case trying to create B。
    we always compare the dimensions last to first and B's last dimension is three
    or has an extent of three。
  prefs: []
  type: TYPE_NORMAL
- en: A's has an extent of two， those don't match we can't broadcast the multiplication
    here。Like what I would see， it the last two dimensions are two and three instead
    of three and two。 they're different that won't work in the final example。We try
    to create an empty tensor and broadcast an operation over。One with dimensions。
  prefs: []
  type: TYPE_NORMAL
- en: that doesn't work， we can't do it with an empty tensor。Now。 Piytorrch tensors
    have over 300 mathematical operations that you can perform on them and here are
    a few examples from the major categories。In the first section， we just have some
    con functions that you might use for manipulating numbers。 absolute value ceiling
    floor and a clamp， which sets min and max values for your tensor。
  prefs: []
  type: TYPE_NORMAL
- en: And all those will act on every element of the tensor， Likewise， for triometric
    functions。 So here I've created a tensor full of angles， and I want to get the
    sign of those angles and then get the inverse of that sign。 and you can see from
    running the cell。That we get back what we expect。We can do bitwise logical operations
    on either boolean or integer tensors here I've got two integer tensors and I'm
    performing a bitwise exor on them。
  prefs: []
  type: TYPE_NORMAL
- en: And we can see that it does exactly what you'd expect if you' were doing like
    a bit Y as x or in C。 for example。We can do comparisons of tensors as well。So
    we'll get a tensor where you specify some data we'll at tensor fold ones。 we'll
    test their quality。We can see because the tensor D， its first value was one。
  prefs: []
  type: TYPE_NORMAL
- en: But all the rest were different， we can see we had a true and three falses there。
    which was exactly what we'd expect。There are also a number of reduction operations
    that you can perform on a single tensor。So for example， here we can take the maximum
    of a tensor no matter how large the tensor。 this is going to give us back a single
    value。Our tensor with a single value。
  prefs: []
  type: TYPE_NORMAL
- en: if you want to extract that value from that one element output tensor， we use
    the dot item call。And if you look at the output from these reduction knos。First
    we get back a tensor with our value in it， and then after the item call。 we've
    actually extracted the value。You can also do means standard deviations。
  prefs: []
  type: TYPE_NORMAL
- en: there are convenience methods for performing arithmetic operations。 including
    all of the elements of the tensor， so here with the dot Progue call we're taking
    the product of all numbers in the tensor。And we can also， as another example，
    get all the unique elements of a tensor。all these behave more or less as you'd
    expect。Of course。
  prefs: []
  type: TYPE_NORMAL
- en: linear algebra is at the heart of a lot of what we do in deep learning。 so there
    are a lot of vector and matrix and linear algebra operations。 so for example I'll
    create two vectors that correspond to X and Y unit vectors I'll create two matrices。One
    of which is just random and one of which is going to be three times the identity
    matrix。
  prefs: []
  type: TYPE_NORMAL
- en: We can do some things with them。Torch do cross gets a cross product between
    the two vectors。 So if we cross the Y unit vector with the X unit vector in that
    order。 we should expect back the negative Z unit vector， which is， in fact what
    we got。 we can do a matrix multiplication of our two matrices。 So we have our
    random matrix。
  prefs: []
  type: TYPE_NORMAL
- en: And then when we multiply it by three times the identity matrix。 we should expect
    that we get back a matrix that is about three times the value of our input。 and
    in fact we see that。 And you can do more advanced complex operations like singular
    value decomposition as well。And so this is just a very small sampling of the 300
    odd mathematical and logical operations associated with P piyTch tensors。
  prefs: []
  type: TYPE_NORMAL
- en: I urge you look at the documentation to understand the full inventory。Now。 sometimes
    if you're doing a computation with two tensors。 you'll say they're intermediate
    values of some kind。 you may not need those intermediate values when you're done。
    it can be a nice optimization to be able to recycle that memory。 If you need a
    tensor of the same size and data type as the intermediate one you going throw
    away。 So as an example of that here again， I'm going to create a tensor full of
    angles。 I'm going to get signs from them。 And you can see when we run this cell
    and check the output we have a。
  prefs: []
  type: TYPE_NORMAL
- en: that's our angles， we have our signs。Of those angles。And then if we look back
    here。 we can see a has not changed。 So here we see Torch dot sign gave us back
    a new tensor and left the old one in place。But because we're acting on a single
    tensor here， we could。 and if we don't need the input values。 we could just put
    the outputs in that tensor itself。
  prefs: []
  type: TYPE_NORMAL
- en: The underscore on a method like sign means that you are altering the tensor
    in place。 that tensor you're putting in， it's an argument。 So now if we do the
    exact same thing。 B is a tensor containing the same angles that A did。 and we
    take do the same operation。 We do a sign on it。We can see there's our initial
    angles， there's the output of that sign operation。
  prefs: []
  type: TYPE_NORMAL
- en: but this time B has changed， we told it that we wanted to use B's memory for
    this and it was of a compatible data type and size。 and so B was altered in place。Now
    if you want to do this with binary arithmetic operations。There are functions for
    you that that behave similarly to the the binary pitorrch operators。 So here we'll
    create two by two matrices， A and B。 We can look at their values before。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: we'll call the。In place addition method。And you can see here that now a has
    changed。Methods that cover a binary operation。 the calling tensor will be the
    one that is changed in place。And so likewise， here， when we do the same for B，
    if we square the random contents of B。 but do it with the mo underscore， we'll
    get back exactly what we expect。
  prefs: []
  type: TYPE_NORMAL
- en: Note that these in place arithmetic functions on are methods of the torch dot
    tensor objects not attached to torch module like a lot of other functions。 the
    calling tensor， as I said， is the one that gets changed in place。There's another
    option for placing the result of a computation in an existing already allocated
    tensor。Many of the methods and functions we've seen so far， including the creation
    methods for tensors。
  prefs: []
  type: TYPE_NORMAL
- en: have an out argument that lets you specify a tensor to receive the output。 If
    the out tensor is the same shape as the output and the correct data type that
    matches the output data type。 this can happen without a new memory allocation。
    So if we run this cell。We'll create tensors A and B， which are two by two random
    matrices and then C。
  prefs: []
  type: TYPE_NORMAL
- en: which is a 2 by2 matrix full of zeros， and we'll use the Python ID call to get
    the ID of that object。We'll print it out， we will do a matrix multiplication between
    A and B。 and we'll specify C as that optional out argument。And then if we look
    at our next print of C。 we'll see that it's changed just no longer zeros。So C
    was the same size as both A and B with the same data type 32 bit floating point
    pi tors default。
  prefs: []
  type: TYPE_NORMAL
- en: And so when we do that multiplication， specify C to receive the output， we see
    that it does。We also assigned the output as a return value to another label D。
    and if we look we'll see that C and D are actually the same object， this assertion
    didn't fire。 so C and D are the same object， and we can also see by an assertion
    that C's ID did not change。
  prefs: []
  type: TYPE_NORMAL
- en: we're dealing with the same object in memory。So I just wanted to to give you
    a tangible example of how all that works and it works for creation calls too。
    so when we call Torchdot Rand with an optional out argument， again。 as long as
    the shape and data type or what we want， we will get back a tensor in that same
    object。So we've seen how to create and manipulate tensors。 But what about copying
    them， Now。
  prefs: []
  type: TYPE_NORMAL
- en: tensors are like any object in Python， meaning that if you assign it to a variable。
    that variable is just a label for the object， you're not making a copy of the
    object。 create a tensor full of ones in the cell， we'll assign it to A say B equals
    a。 And if look when we change a value of a and print B， the the value within B
    has changed as well。
  prefs: []
  type: TYPE_NORMAL
- en: So these are just two labels for the exact same object。 what if you need a separate
    copy of the data。It may happen if you're building a complex model with multiple
    computation paths。 and so you want to have separate copies of the input to pass
    to different portions of the model。So in this case， you would use the clone method，
    so we're going to do something very similar here。
  prefs: []
  type: TYPE_NORMAL
- en: we're going to create two by two meters full of ones。 we're going to say B is
    in same Josea now。 but now we're going to clone A instead of just doing the assignment。We
    can verify that these are。 in fact different objects in memory with the assertion。
    and we can verify that the contents are the same with a Torrsch EQ call。And when
    we change A， again。
  prefs: []
  type: TYPE_NORMAL
- en: we can verify that B has not changed when we print it out。So there is one important
    thing to be aware of using clone。 which is that if your source tensor has autograd
    enableded， then so will the clone of that tensor。 We're going to cover this more
    deeply in the video on autograd。
  prefs: []
  type: TYPE_NORMAL
- en: But if you want a light version of the details， here it is。嗯。So as an example。
    let's see you you have a complex model with multiple computation paths in its
    forward method。 and the original tensor and its clone or its copy are going to
    contribute to the model's output。 then in order to enable model learning you want
    autograd turned on for both tensors。
  prefs: []
  type: TYPE_NORMAL
- en: If your source tensor has autograd enabled， which it generally will。 if it's
    a set of learning weights or it's derived from a computation involving the weights。Then
    everything else out of redden， it will know already and you'll get the result
    you want。On the other hand， perhaps you are doing a computation where neither
    the original tensor nor its clone need to track gradients。
  prefs: []
  type: TYPE_NORMAL
- en: In that case， as long as the source tensor has autograd turned off， you're good
    to go。There is。 of course， a third case， so imagine you're performing some computation
    in your model's forward function where gradients are turned on for everything
    by default。
  prefs: []
  type: TYPE_NORMAL
- en: but you want to pull out some values midstream to generate metrics。And you want
    those to be separate from the data that's being acted on。 So in this case。 you
    wouldn't want the cloned copy of your source tensor to track gradients。Degrades
    performance and doesn't actually do anything for you in this example use case。So
    for this。
  prefs: []
  type: TYPE_NORMAL
- en: you can use the detaach method on the source tensor。So if we run。This we'll
    see is what we'll create our two by two tensor of random values。 We will set requires
    grad equals true。 So now every computation subsequent to a will have its history
    tracked。 So we know where it came from and can compute backward derivative itus。
    And that happens with clone。
  prefs: []
  type: TYPE_NORMAL
- en: So here we clone B from a in we print B， we see that grad function equals clone
    backwards。 That's telling us B is tracking its history。 Now， instead。 if you look
    at the line where we create C。 we say a dot detached dot clone。And then when we
    print C。 we get the same data except that we don't get the history attached。If
    we print A。
  prefs: []
  type: TYPE_NORMAL
- en: we'll see that that detach call did not actually alter A at all。 it basically
    just says do everything as if autogriide were turned off。One of the core advantages
    of Pytorch is hardware acceleration。 If you have a coa compatible Nvidia GP and
    the drivers installed。
  prefs: []
  type: TYPE_NORMAL
- en: you can radically accelerate your performance for both training and inference。
    Everything we've done so far has been on CPU By default when you create a tensor。
    It's instantiated in CPU and Marine。So how do we move to the faster hardware？First
    things first。 we should check whether that faster hardware is available and we
    do that with Torch。kuudo。
  prefs: []
  type: TYPE_NORMAL
- en: is available。So if I run the cell。You see， it will tell us whether or not we
    have a GPU available on this device。Once you determine that one or more GPUs are
    available。 you need to put the data inplace the GPU can see it。Your CPU works
    on data that is lives in your machine's ramp。
  prefs: []
  type: TYPE_NORMAL
- en: your GPU also has dedicated memory attached to it。whichever device you want
    to perform your computation on。 you must move all the data needed for that operation
    to memory accessible by your target device。 Now we know that we have one or more
    GPUus available， we run。
  prefs: []
  type: TYPE_NORMAL
- en: Now there are multiple ways to get your data onto your target device。 but the
    easiest way is at creation time， you can see here when we have a KUa GPU available。We
    will create a tensor with an optional argument that says device equals kuda。When
    we create a tensor that we want to move to a GPU。
  prefs: []
  type: TYPE_NORMAL
- en: As we'll use the optional device argument on the factory with all the factory
    methods I showed you for creating tensors will take this device argument and here
    we're putting in the string kuta to say。 or we would like to move this tensor
    into memory accessible by the GPU when you print the tensor。
  prefs: []
  type: TYPE_NORMAL
- en: you'll see that it reports that the tensor is living on the GPU device。You can
    also query the number of GPUs and if there's more than one。 you can specify them
    by index with a colon after the Kuta string， so Kuda colon 0， Kuta colon 1。 eta，
    would be the strings you put in as your device argument。As a general engineering
    practice。
  prefs: []
  type: TYPE_NORMAL
- en: it's generally not considered good to specify things with magic constant strings
    all over your code。 so a better practice is to at the beginning of computation。
    choose which device you wanted to do your computation on。And get a handle to that
    device and then pass that handle around here in the next cell。
  prefs: []
  type: TYPE_NORMAL
- en: we have an example of that。Where my device， depending on whether or not we have
    a GPU available or not。 is either Torchdot device Kuta or Torchdot device CPU。
    Once we have the handle to that device。 we can pass that in as the optional argument
    to creating a tensor is shown in the last couple of lines there。Thus creating
    a tensor， what if you have an existing tensor。
  prefs: []
  type: TYPE_NORMAL
- en: it's living in memory for one device and you want to move it to the other device。How
    do you go about that？Well， in this cell， we'll demonstrate you can create if you
    create a tensor in CPU。 for example， and you want to move it to GPU， you can either
    put in the string ka or k colon and 0 cota colon in 1。Or you could pass in a handle
    to a device that you have retrieved earlier。
  prefs: []
  type: TYPE_NORMAL
- en: And you just pass it into the two method like so this is the same two method
    that lets you change the data type of a tensor if you wanted to change both the
    data type and the device。 you would have to specify the names of the arguments
    so D type equals Torch dot flow 16 device equals you know my GPU。
  prefs: []
  type: TYPE_NORMAL
- en: But that's how you move all your tensors， learning weights， everything from
    CPU memory to GPU。Sometimes you'll have a tensor and you'll need it to be in a
    different shape。 so we're going to look at a couple of common cases and the tools
    Pytorrch offers to help you handle that。So one case where you might need to change
    the number of dimensions is when you're passing a single instance of input to
    your model。
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch models expect batches of input rather than single instances。 So， for
    example。 if we had an image classification model that took in a three color image，226
    by 226 pixels。 each input image would be represented as a 3 by 226 by 2，26 tensor。Your
    model is expecting a shape of n times 3 times 2，26 times 2，26。
  prefs: []
  type: TYPE_NORMAL
- en: where n is the number of images in the batch， which might be， for example。8
    or 16 while you're doing training。 But let's see you're doing inference one at
    a time。 How do you make a batch of one。We're going to do that with the unsqueezze
    method。So we start with a random tensor meant to represent our input。
  prefs: []
  type: TYPE_NORMAL
- en: the 3 by 226 by 226 image representation， and then we're going to call unsqueeze  zero
    and get that tensor and check its shape。And we'll see it's changed to  one by
    3 by 226 by 2，26。 So we added a dimension at the beginning。 That's what a 0 on
    unsqueezes， says as we want this new dimension to be the first dimension。 the
    one at index 0。 That's unsqueezing。 What we mean then by squeezing here。
  prefs: []
  type: TYPE_NORMAL
- en: we're taking advantage of the fact that any dimension of extent  one does not
    change the number of elements in the tensor。So for example， here if we。Create
    C， which is a one by one by one by one by one tensor。When we print it， we see
    it only has one element and a lot of square brackets。So continuing the example
    above with our image classifier。
  prefs: []
  type: TYPE_NORMAL
- en: let's say the model's output is a 20 element vector for each input。You then
    expect the output to have a shape of n by 20 where n is the number of instances
    that were in the input batch。 so as many input instances as you put in， you want
    to have that many predictions coming out that means for a single input batch we'll
    get an output of shape 1 by 20。So what if you want to do non batched computation
    with that output。
  prefs: []
  type: TYPE_NORMAL
- en: something as just expecting a 20 element vector for that， we have squeeze。So
    what's happening here here we created a random 1 by 20 vector again。 meant to
    stand in as our output tensor。We can check its shape and verify that it is 1 by
    20。 and then we can call squeeze zero on it。And so what that's saying is we want
    to take that dimension of extent1 and squeeze it away。
  prefs: []
  type: TYPE_NORMAL
- en: When we call that we look at the shape of B， following we can see is' just a
    20 element tensor。Now this can only be done with dimensions of extent 1。So in
    the following stanza with the variable C and D， we create a random pi torch tensor。
    and then we try to squeeze the first dimension of it。
  prefs: []
  type: TYPE_NORMAL
- en: And if you check the shape of the output of squeeze in that case。 you'll find
    it's the same shape you started with。 We didn't lose a dimension as we intended
    because there is no way to do that without destroying data in this case。So squeezing
    unsqueezing will only work with dimensions of extent1。 Another place you might
    use unsqueeze is to help with broadcasting。 if you recall earlier。
  prefs: []
  type: TYPE_NORMAL
- en: we had some code we were demonstratingmin broadcasting or we took4 by3 by two
    tensor multiplied it by a3 by one tensor and the result once we had the dimensions
    aligned was that every three element column in our original tensor had the same
    operation applied to the same multiplication。
  prefs: []
  type: TYPE_NORMAL
- en: Now， what if instead of3 by one， we had just had a three element column vector
    that we wanted to broadcast sum operation over a？
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the next cell， we can see that if we just look at A and B as they're
    created right away。 Brocasting can't happen there。 the trailing dimensions don't
    line up。 So what do we do。 We use unsqueeze。In the cell。To create an extra dimension
    of extent one。And then when we multiply the random three element vector against
    the larger tensor。
  prefs: []
  type: TYPE_NORMAL
- en: we can see every three element column in the tensor has a multiplication operation
    broadcast over it。 so this can be a way to manipulate dimensions to get broadcasting
    to work for you without having to transpose dimensions on either of your tensors。TheSqueeze
    and unsqueeezze methods also have in place versions。 like we saw easier earlier
    with the math methods if I have one input instance。
  prefs: []
  type: TYPE_NORMAL
- en: and I want to make a batch of one instead of calling unsqueeze I can call unsqueezze
    with the underscore and do the operation in place。Now， sometimes you'll want to
    change the shape of a tensor more radically while still preserving the number
    of elements in the tensor in their contents。
  prefs: []
  type: TYPE_NORMAL
- en: In one case where this happens is again， taking the example of an image classifier。
    it's common in such models for the beginning of the computation to involve convolutional
    layers and the end the classification piece to involve fully connected or linear
    layers。
  prefs: []
  type: TYPE_NORMAL
- en: Now the convolutional layers when they're working with images will usually put
    out a three dimensional tensor。 you will have some horizontal and vertical extent
    meant to map the detection of features onto the image spatially。 and then it will
    have a depth as well， and that will be the number of features that that convolution
    kernel has learned to recognize。The fully connected layers that follow， though，
    are expecting just a one dimensional vector。
  prefs: []
  type: TYPE_NORMAL
- en: So how do we translate between these two cases where we have。An output vector
    becomes an input vector， but it needs to change shape。 but keep the same number
    of cells。Well， all we can we can do that is with the reshape method。So here we'll
    create a 6 by 20 by 20 tensor that's a stand in for our convolutional layer output。
  prefs: []
  type: TYPE_NORMAL
- en: And we will reshape that into a one dimensional tensor with6 times 20 times
    20 elements that stand inver the input into our fully connected layer Now when
    it can。 reshape will actually put out a view on the original tensor。So instead
    of creating a。New tensor object with new memory allocation。 It will create a new
    tensor object addressing the same memory underlying the first tensor。 So this
    is important， by the way， if you use reshape and it feeds you back a view of the
    original tensor。
  prefs: []
  type: TYPE_NORMAL
- en: know changes in the source tensor will be reflected in the new tensor unless
    you clone it。There are conditions beyond the scope of this introduction where
    reshape has to return tensor with the data copied for more information。 there's
    a documentation out on the topic， which I urge you to read。The last topic I wrote
    a cover on is introduction to Tensors。
  prefs: []
  type: TYPE_NORMAL
- en: Is the data portability between numpy tensors and pi torch tensors Now， in the
    section above。 we mentioned briefly that pi torches broadcasting semantics are
    just like numps。But the connection between the two libraries goes even deeper
    than that。If you have existing machine learning or scientific code with data stored
    in NI and D arrays。
  prefs: []
  type: TYPE_NORMAL
- en: you may wish to express that same data as Pytorrch tensors。 whether to take
    advantage of PyTtorrch's GPU acceleration or its efficient abstractions for building
    deep learning models。It's easy to switch between numpy and di rays and Pytorrch
    tensors。So the first cell here I' be importing Ny。And we'll create a nuy array。
  prefs: []
  type: TYPE_NORMAL
- en: two by three matrix full of ones。Now to express that as a pi torch tensor we
    call torched up from Numpy with the Numpy array as an argument。 we get back a
    tensor and we print it out， we will see that it's the same shape。 it contains
    the same data and even goes so far as preserving the 64 bit floating point data
    type。 Numpy's default。And the conversion is just as easy the other way。
  prefs: []
  type: TYPE_NORMAL
- en: so here we'll create a random pi torch tensor。And we'll call dot Numpy on it
    and we'll get back a Numpy and D。Now it's important to know these converted objects
    are using the same underlying memory as their source objects。 meaning that changes
    to one are reflected in the other。So when I run the final cell。What you'll see
    is change the value of one element of a numpy array。
  prefs: []
  type: TYPE_NORMAL
- en: and we see that reflected in the pi torch tensor that we made from it。 Likewise。
    when I change a value in the pi torch tensor we made。 it's reflected in the nuy
    tensor we created from that。So again。 if you have code already that's manipulating
    your data in NPI， moving it over to P Torches a breeze。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/629ed54063cce12da521281bede5aad8_3.png)'
  prefs: []
  type: TYPE_IMG
- en: That is our first deep dive on tensors。 The topics we cover today and the documentation
    at Pytororch。 org should be all you need to get going on the videos later in this
    series。 but as well on your own work within Pytorrch。Thank you for listening。
  prefs: []
  type: TYPE_NORMAL
