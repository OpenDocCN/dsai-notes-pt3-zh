- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P29ÔºöL5.3- Âú®Keras‰∏≠‰ΩøÁî®L1ÂíåL2Ê≠£ÂàôÂåñÊù•ÂáèÂ∞ëËøáÊãüÂêà
    - ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå this is Jeff Heaton welcome to applications of Deep neural networks with
    Washington University In this video„ÄÇ we're going to see how to apply L1 and L2
    regularization to Kira's deep neural networks for the latest on my AI course and
    projects click subscribe and the bell next to it to be notified of every new video
    we can use L1 into regularization with Kira just to review L1 and L2 we show their
    graphs here just so that you can see the effect of the absolute value versus the
    squing you can see that L1 more resembles the laplace is a lot sharper and that
    is why it will cause coefficients to go to zero or weights now that we're dealing
    with a neural network to go to zero whereas L2 iser and it will not necessarily
    push them to zero like the L1 does we'll see that we can use both of these either
    independently or together for a neural network I have a link here that shows you
    the actual instructions from Kis on how to do but we'll look at some exam„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7faee9fce20cf27950729f356ade8454_1.png)'
  prefs: []
  type: TYPE_IMG
- en: That show you how to actually put this into your neural network„ÄÇ We're going
    to use the sample data set for this one„ÄÇ We're going to be predicting the product„ÄÇ
    So we are doing classification let's go ahead and run this so that it gets loaded
    into let's go ahead and run this section so that it gets loaded„ÄÇ OkayÔºå the data
    is available so this is where we will do L1 and or L2 regularization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It's done on a per level basis So you can see here we have an active activity„ÄÇRegizer
    and that uses O1„ÄÇ We can also use a kernel regularizer and usually use the activity
    more than the kernel„ÄÇ The activity is dealing with the actual activations of the„ÄÇüòäÔºåOf
    the regularizer„ÄÇ but I have tried back and forth with with both ones to see really
    which gives me the best result I do tend to get better results with L2 particularly
    L2 activity regulars than L1 I'm typically not trying to actually eliminate features
    altogether using using it on the neural network this can be a useful way to do
    that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can read up in the Kira's documentation too a little on the difference between
    activity and kernel it really has to do with at what point in the calculation
    before after the activation function is applied that that regularizer is being
    applied and that's really all there is to it simply placing this as part of the
    first hidden layer causes the regularizer to be applied you can also decide to
    apply a second one here you can do L2 simply by„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Changing that from L1 to L2„ÄÇFeel free to adjust that how however you like again
    this is another hyperparameter so it just requires some some optimization and
    trial and error we'll see when we get to the module that discusses CAle competitions„ÄÇ
    how we can optimize this automatically using using Bayesian optimization or at
    least get it pushed in a good direction as far as„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: What we want to do with all of these hyperparameter„ÄÇ This is the alpha that
    we saw earlier„ÄÇ This is just the degree to which this is being applied„ÄÇ If you
    were to put a zero in here„ÄÇ it would simply cancel the one out and it would not
    be applied at all one would be full power You probably do not want to do that
    that would make the the training very unstable You can go ahead and run this one
    It will go through the crossval and generate the out of sample predictions we
    are simply training this for 500 epochs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're not doing any sort of early stopping because we really want to get a good
    estimation of how well say 500 epochs would do so that we could run this a couple
    of times„ÄÇWith different L1 and L2 values and get an idea of really what what each
    of these is going to do Now as it goes through„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you're going to see the accuracy on each fold„ÄÇ This is classification„ÄÇ so we're
    showing you the percentage„ÄÇThat this was able to predict correctly„ÄÇ so on this
    first one„ÄÇ it was about 66%„ÄÇAnd I'll let this go ahead and fast forward to get
    through the other ones„ÄÇ Okay„ÄÇ and now it's completed„ÄÇ the final accuracy wasÔºå
    as we can see hereÔºå65Ôºå95„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I want to show you something else that's somewhat of an annoying feature
    of„ÄÇOf neural networks„ÄÇ we'll see that when you rerun themÔºå the scores can vary
    quite a bit„ÄÇ So remember 65Ôºå95„ÄÇ and also remember that full2 was the best one„ÄÇ
    We will go ahead and fast forward this so that you don't have to wait for it„ÄÇ
    OkayÔºå we're done„ÄÇ So notice now it is not 65Ôºå95 anymore„ÄÇ It's changed„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it looks like it got a little bit worse„ÄÇ If we had tried adjusting things
    up here„ÄÇ we might falsely think that what we did up there it caused a problem
    and„ÄÇJust was not as good of a adjustment you also can see that full2„ÄÇ though not
    the best this time was still pretty good„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that tends to stay the same because we did the seed value for the cross validation
    so up here where we set the random state to 42„ÄÇThat certain folds will definitely
    be easier than others„ÄÇ and you like that to stay consistent so you can really
    compare apples to apples at least on the folds„ÄÇSo we'll see later in this module
    that there's something called bootstrapping we can do where we run this whole
    bunch of times and we get the results back that are then average together„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we get a better idea of if if our adjustments to the hyperparameters are
    actually making a difference or not Thank you for watching this video and the
    next video we're going to look at dropout which is a type of regularization that
    was specifically created for neural networks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7faee9fce20cf27950729f356ade8454_3.png)'
  prefs: []
  type: TYPE_IMG
- en: This content changes oftenÔºå so subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
