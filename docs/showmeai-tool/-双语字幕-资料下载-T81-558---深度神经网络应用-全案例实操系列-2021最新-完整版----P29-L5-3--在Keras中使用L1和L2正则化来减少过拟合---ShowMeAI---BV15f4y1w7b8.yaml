- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P29ï¼šL5.3- åœ¨Kerasä¸­ä½¿ç”¨L1å’ŒL2æ­£åˆ™åŒ–æ¥å‡å°‘è¿‡æ‹Ÿåˆ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P29ï¼šL5.3- åœ¨Kerasä¸­ä½¿ç”¨L1å’ŒL2æ­£åˆ™åŒ–æ¥å‡å°‘è¿‡æ‹Ÿåˆ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heaton welcome to applications of Deep neural networks with
    Washington University In this videoã€‚ we're going to see how to apply L1 and L2
    regularization to Kira's deep neural networks for the latest on my AI course and
    projects click subscribe and the bell next to it to be notified of every new video
    we can use L1 into regularization with Kira just to review L1 and L2 we show their
    graphs here just so that you can see the effect of the absolute value versus the
    squing you can see that L1 more resembles the laplace is a lot sharper and that
    is why it will cause coefficients to go to zero or weights now that we're dealing
    with a neural network to go to zero whereas L2 iser and it will not necessarily
    push them to zero like the L1 does we'll see that we can use both of these either
    independently or together for a neural network I have a link here that shows you
    the actual instructions from Kis on how to do but we'll look at some examã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†L1å’ŒL2æ­£åˆ™åŒ–åº”ç”¨äºKiraçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚æƒ³äº†è§£æˆ‘çš„AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»è®¢é˜…å’Œæ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿æ”¶åˆ°æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚æˆ‘ä»¬å¯ä»¥å°†L1æ­£åˆ™åŒ–åº”ç”¨äºKiraï¼Œå›é¡¾ä¸€ä¸‹L1å’ŒL2ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå±•ç¤ºäº†å®ƒä»¬çš„å›¾å½¢ï¼Œä»¥ä¾¿ä½ çœ‹åˆ°ç»å¯¹å€¼ä¸å¹³æ–¹çš„æ•ˆæœã€‚ä½ å¯ä»¥çœ‹åˆ°L1æ›´åƒæ˜¯æ‹‰æ™®æ‹‰æ–¯ï¼Œè¾¹ç¼˜æ›´å°–é”ï¼Œè¿™å°±æ˜¯å®ƒä¼šå¯¼è‡´ç³»æ•°æˆ–æƒé‡å½’é›¶çš„åŸå› ï¼Œè€ŒL2åˆ™ä¸ä¼šåƒL1é‚£æ ·å¼ºè¿«å®ƒä»¬å½’é›¶ã€‚æˆ‘ä»¬å¯ä»¥ç‹¬ç«‹æˆ–ä¸€èµ·ä½¿ç”¨è¿™ä¸¤è€…åº”ç”¨äºç¥ç»ç½‘ç»œã€‚æˆ‘è¿™é‡Œæœ‰ä¸€ä¸ªé“¾æ¥ï¼Œå±•ç¤ºäº†Kisçš„å®é™…æ“ä½œè¯´æ˜ï¼Œä½†æˆ‘ä»¬å°†çœ‹ä¸€äº›ä¾‹å­ã€‚
- en: '![](img/7faee9fce20cf27950729f356ade8454_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7faee9fce20cf27950729f356ade8454_1.png)'
- en: That show you how to actually put this into your neural networkã€‚ We're going
    to use the sample data set for this oneã€‚ We're going to be predicting the productã€‚
    So we are doing classification let's go ahead and run this so that it gets loaded
    into let's go ahead and run this section so that it gets loadedã€‚ Okayï¼Œ the data
    is available so this is where we will do L1 and or L2 regularizationã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£å±•ç¤ºäº†å¦‚ä½•å°†å…¶å®é™…åº”ç”¨äºä½ çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªæ ·æœ¬æ•°æ®é›†ã€‚æˆ‘ä»¬å°†é¢„æµ‹äº§å“ã€‚å› æ­¤æˆ‘ä»¬åœ¨è¿›è¡Œåˆ†ç±»ï¼Œè®©æˆ‘ä»¬ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œä»¥ä¾¿å°†å…¶åŠ è½½ã€‚å¥½çš„ï¼Œæ•°æ®å·²ç»å¯ç”¨ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬å°†è¿›è¡ŒL1å’Œ/æˆ–L2æ­£åˆ™åŒ–çš„åœ°æ–¹ã€‚
- en: It's done on a per level basis So you can see here we have an active activityã€‚Regizer
    and that uses O1ã€‚ We can also use a kernel regularizer and usually use the activity
    more than the kernelã€‚ The activity is dealing with the actual activations of theã€‚ğŸ˜Šï¼ŒOf
    the regularizerã€‚ but I have tried back and forth with with both ones to see really
    which gives me the best result I do tend to get better results with L2 particularly
    L2 activity regulars than L1 I'm typically not trying to actually eliminate features
    altogether using using it on the neural network this can be a useful way to do
    thatã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æŒ‰å±‚çº§è¿›è¡Œçš„ï¼Œæ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªæ´»è·ƒçš„æ´»åŠ¨ã€‚Regizerå¹¶ä¸”ä½¿ç”¨O1ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å†…æ ¸æ­£åˆ™åŒ–å™¨ï¼Œé€šå¸¸ä½¿ç”¨æ´»åŠ¨å¤šäºå†…æ ¸ã€‚æ´»åŠ¨å¤„ç†çš„æ˜¯å®é™…çš„æ¿€æ´»ã€‚ğŸ˜Šï¼Œæ­£åˆ™åŒ–å™¨çš„ã€‚ä½†æˆ‘å°è¯•äº†ä¸¤è€…ä¹‹é—´åå¤æ¯”è¾ƒï¼Œçœ‹çœ‹å“ªä¸ªç»™æˆ‘æœ€ä½³ç»“æœï¼Œç‰¹åˆ«æ˜¯L2æ´»åŠ¨æ­£åˆ™åŒ–å™¨çš„ç»“æœå¾€å¾€æ›´å¥½äºL1ã€‚æˆ‘é€šå¸¸ä¸è¯•å›¾å®Œå…¨æ¶ˆé™¤ç‰¹å¾ï¼Œä½¿ç”¨å®ƒåœ¨ç¥ç»ç½‘ç»œä¸Šï¼Œè¿™å¯ä»¥æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ–¹æ³•ã€‚
- en: you can read up in the Kira's documentation too a little on the difference between
    activity and kernel it really has to do with at what point in the calculation
    before after the activation function is applied that that regularizer is being
    applied and that's really all there is to it simply placing this as part of the
    first hidden layer causes the regularizer to be applied you can also decide to
    apply a second one here you can do L2 simply byã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥åœ¨Kiraçš„æ–‡æ¡£ä¸­ç¨å¾®é˜…è¯»ä¸€ä¸‹æ´»åŠ¨å’Œå†…æ ¸ä¹‹é—´çš„åŒºåˆ«ï¼Œå®ƒå®é™…ä¸Šä¸è®¡ç®—çš„å“ªä¸ªç‚¹æœ‰å…³ï¼Œåœ¨æ¿€æ´»å‡½æ•°åº”ç”¨ä¹‹å‰æˆ–ä¹‹åï¼Œæ­£åˆ™åŒ–å™¨è¢«åº”ç”¨ï¼Œè€Œè¿™å°±æ˜¯å…¨éƒ¨ã€‚å°†å…¶æ”¾åœ¨ç¬¬ä¸€ä¸ªéšè—å±‚ä¸­ä¼šå¯¼è‡´æ­£åˆ™åŒ–å™¨è¢«åº”ç”¨ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œåº”ç”¨ç¬¬äºŒä¸ªæ­£åˆ™åŒ–å™¨ï¼Œç®€å•åœ°é€šè¿‡åšL2ã€‚
- en: Changing that from L1 to L2ã€‚Feel free to adjust that how however you like again
    this is another hyperparameter so it just requires some some optimization and
    trial and error we'll see when we get to the module that discusses CAle competitionsã€‚
    how we can optimize this automatically using using Bayesian optimization or at
    least get it pushed in a good direction as far asã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å…¶ä»L1æ›´æ”¹ä¸ºL2ã€‚éšæ„æ ¹æ®ä½ çš„å–œå¥½è¿›è¡Œè°ƒæ•´ï¼Œè¿™åˆæ˜¯å¦ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ‰€ä»¥éœ€è¦ä¸€äº›ä¼˜åŒ–å’Œè¯•é”™ã€‚å½“æˆ‘ä»¬åˆ°è¾¾è®¨è®ºCAleç«èµ›çš„æ¨¡å—æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è‡ªåŠ¨ä¼˜åŒ–ï¼Œæˆ–è€…è‡³å°‘æ¨åŠ¨å®ƒæœç€å¥½çš„æ–¹å‘å‘å±•ã€‚
- en: What we want to do with all of these hyperparameterã€‚ This is the alpha that
    we saw earlierã€‚ This is just the degree to which this is being appliedã€‚ If you
    were to put a zero in hereã€‚ it would simply cancel the one out and it would not
    be applied at all one would be full power You probably do not want to do that
    that would make the the training very unstable You can go ahead and run this one
    It will go through the crossval and generate the out of sample predictions we
    are simply training this for 500 epochsã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³å¯¹æ‰€æœ‰è¿™äº›è¶…å‚æ•°åšçš„äº‹æƒ…ã€‚è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„alphaã€‚è¿™åªæ˜¯åº”ç”¨çš„ç¨‹åº¦ã€‚å¦‚æœä½ åœ¨è¿™é‡Œè¾“å…¥é›¶ï¼Œå®ƒä¼šç®€å•åœ°æŠµæ¶ˆä¸€ï¼Œè€Œæ ¹æœ¬ä¸ä¼šåº”ç”¨ï¼Œ1å°±æ˜¯å®Œå…¨çš„åŠ›é‡ã€‚ä½ å¯èƒ½ä¸æƒ³è¿™æ ·åšï¼Œè¿™ä¼šä½¿è®­ç»ƒéå¸¸ä¸ç¨³å®šã€‚ä½ å¯ä»¥ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œå®ƒä¼šç»è¿‡äº¤å‰éªŒè¯å¹¶ç”Ÿæˆæ ·æœ¬å¤–é¢„æµ‹ï¼Œæˆ‘ä»¬åªæ˜¯è®­ç»ƒäº†500ä¸ªepochã€‚
- en: we're not doing any sort of early stopping because we really want to get a good
    estimation of how well say 500 epochs would do so that we could run this a couple
    of timesã€‚With different L1 and L2 values and get an idea of really what what each
    of these is going to do Now as it goes throughã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ²¡æœ‰è¿›è¡Œä»»ä½•å½¢å¼çš„æ—©åœï¼Œå› ä¸ºæˆ‘ä»¬ç¡®å®æƒ³è¦ä¸€ä¸ªå¥½çš„ä¼°è®¡ï¼Œçœ‹çœ‹500ä¸ªepochä¼šå¦‚ä½•è¡¨ç°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¤šæ¬¡è¿è¡Œè¿™ä¸ªï¼Œä½¿ç”¨ä¸åŒçš„L1å’ŒL2å€¼ï¼ŒçœŸæ­£äº†è§£æ¯ä¸ªå€¼çš„æ•ˆæœã€‚ç°åœ¨éšç€å®ƒçš„è¿›è¡Œã€‚
- en: you're going to see the accuracy on each foldã€‚ This is classificationã€‚ so we're
    showing you the percentageã€‚That this was able to predict correctlyã€‚ so on this
    first oneã€‚ it was about 66%ã€‚And I'll let this go ahead and fast forward to get
    through the other onesã€‚ Okayã€‚ and now it's completedã€‚ the final accuracy wasï¼Œ
    as we can see hereï¼Œ65ï¼Œ95ã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†çœ‹åˆ°æ¯ä¸ªæŠ˜å çš„å‡†ç¡®æ€§ã€‚è¿™æ˜¯åˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬å‘ä½ å±•ç¤ºäº†æ­£ç¡®é¢„æµ‹çš„ç™¾åˆ†æ¯”ã€‚åœ¨ç¬¬ä¸€ä¸ªæŠ˜å ä¸­ï¼Œå¤§çº¦æ˜¯66%ã€‚æˆ‘ä¼šè®©è¿™ä¸ªç»§ç»­å¿«è¿›ï¼Œä»¥å®Œæˆå…¶ä»–æŠ˜å ã€‚å¥½çš„ï¼Œç°åœ¨å®Œæˆäº†ï¼Œæœ€ç»ˆçš„å‡†ç¡®æ€§æ˜¯ï¼Œå¦‚æˆ‘ä»¬æ‰€è§ï¼Œ65ï¼Œ95ã€‚
- en: And I want to show you something else that's somewhat of an annoying feature
    ofã€‚Of neural networksã€‚ we'll see that when you rerun themï¼Œ the scores can vary
    quite a bitã€‚ So remember 65ï¼Œ95ã€‚ and also remember that full2 was the best oneã€‚
    We will go ahead and fast forward this so that you don't have to wait for itã€‚
    Okayï¼Œ we're doneã€‚ So notice now it is not 65ï¼Œ95 anymoreã€‚ It's changedã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³ç»™ä½ å±•ç¤ºä¸€äº›æœ‰ç‚¹çƒ¦äººçš„ç‰¹æ€§ï¼Œè¿™ä¸ç¥ç»ç½‘ç»œæœ‰å…³ã€‚æˆ‘ä»¬ä¼šå‘ç°ï¼Œå½“ä½ é‡æ–°è¿è¡Œå®ƒä»¬æ—¶ï¼Œåˆ†æ•°å¯èƒ½ä¼šæœ‰å¾ˆå¤§å˜åŒ–ã€‚æ‰€ä»¥è®°ä½65ï¼Œ95ã€‚åŒæ—¶ä¹Ÿè¦è®°ä½full2æ˜¯æœ€å¥½çš„ã€‚æˆ‘ä»¬å°†å¿«é€Ÿæ¨è¿›ï¼Œä»¥ä¾¿ä½ ä¸å¿…ç­‰å¾…ã€‚å¥½çš„ï¼Œå®Œæˆäº†ã€‚æ³¨æ„ï¼Œç°åœ¨ä¸å†æ˜¯65ï¼Œ95äº†ã€‚å®ƒå·²ç»æ”¹å˜äº†ã€‚
- en: So it looks like it got a little bit worseã€‚ If we had tried adjusting things
    up hereã€‚ we might falsely think that what we did up there it caused a problem
    andã€‚Just was not as good of a adjustment you also can see that full2ã€‚ though not
    the best this time was still pretty goodã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥æƒ…å†µç¨å¾®å˜ç³Ÿäº†ã€‚å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•è°ƒæ•´ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé”™è¯¯åœ°è®¤ä¸ºä¸Šé¢çš„è°ƒæ•´å¯¼è‡´äº†é—®é¢˜ï¼Œå…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è°ƒæ•´ã€‚ä½ è¿˜å¯ä»¥çœ‹åˆ°full2ï¼Œå°½ç®¡è¿™æ¬¡ä¸æ˜¯æœ€å¥½ï¼Œä½†ä»ç„¶ç›¸å½“ä¸é”™ã€‚
- en: that tends to stay the same because we did the seed value for the cross validation
    so up here where we set the random state to 42ã€‚That certain folds will definitely
    be easier than othersã€‚ and you like that to stay consistent so you can really
    compare apples to apples at least on the foldsã€‚So we'll see later in this module
    that there's something called bootstrapping we can do where we run this whole
    bunch of times and we get the results back that are then average togetherã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é€šå¸¸ä¿æŒä¸å˜ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºäº¤å‰éªŒè¯è®¾ç½®äº†ç§å­å€¼ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬å°†éšæœºçŠ¶æ€è®¾ç½®ä¸º42ã€‚æŸäº›æŠ˜å è‚¯å®šä¼šæ¯”å…¶ä»–æŠ˜å å®¹æ˜“ï¼Œè€Œä½ å¸Œæœ›å®ƒä¿æŒä¸€è‡´ï¼Œè¿™æ ·è‡³å°‘å¯ä»¥è¿›è¡ŒçœŸæ­£çš„æ¯”è¾ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°è¿™ä¸ªæ¨¡å—ä¸­æœ‰ä¸€ä¸ªå«åšè‡ªåŠ©æ³•çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œå¾ˆå¤šæ¬¡ï¼Œç„¶åå°†ç»“æœè¿›è¡Œå¹³å‡ã€‚
- en: so we get a better idea of if if our adjustments to the hyperparameters are
    actually making a difference or not Thank you for watching this video and the
    next video we're going to look at dropout which is a type of regularization that
    was specifically created for neural networksã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·æˆ‘ä»¬å°±èƒ½æ›´å¥½åœ°äº†è§£æˆ‘ä»¬å¯¹è¶…å‚æ•°çš„è°ƒæ•´æ˜¯å¦çœŸçš„æœ‰æ‰€ä¸åŒã€‚æ„Ÿè°¢è§‚çœ‹è¿™æ®µè§†é¢‘ï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºdropoutï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºç¥ç»ç½‘ç»œåˆ›å»ºçš„æ­£åˆ™åŒ–ç±»å‹ã€‚
- en: '![](img/7faee9fce20cf27950729f356ade8454_3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7faee9fce20cf27950729f356ade8454_3.png)'
- en: This content changes oftenï¼Œ so subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligenceã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ï¼Œæ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£è¿™ä¸ªè¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚
