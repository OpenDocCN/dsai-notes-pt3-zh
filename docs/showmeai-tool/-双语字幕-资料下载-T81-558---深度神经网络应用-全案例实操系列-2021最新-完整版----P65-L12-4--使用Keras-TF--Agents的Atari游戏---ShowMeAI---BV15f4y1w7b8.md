# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P65ÔºöL12.4- ‰ΩøÁî®Keras TF- AgentsÁöÑAtariÊ∏∏Êàè - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff Heaton„ÄÇ Wecome to applications of Deep neural networks with Washington University„ÄÇ In this videoÔºå we're going to continue learning about deep reinforcement learning„ÄÇ and we're going to see how to apply a Tf agentsbased program to Atari„ÄÇ We're to look at how to play the pawong game„ÄÇ However„ÄÇ

 the same technique can be used for just about any Atari game„ÄÇ And then in the next part„ÄÇ we're going to look at something that's not a game and see how we can apply deep reinforcement learning to a financial simulation„ÄÇ to see all my videos about Cale neural networks and other AI topics„ÄÇ click the subscribe button and the bell next to it and select al to be notified of every new video„ÄÇ

 So let's look at one of the most common uses for things like deep reinforcement learning„ÄÇ At least the examples that most people get into„ÄÇ even if they don't get into actually applying this to problems of their own„ÄÇ And that's Atari games„ÄÇ NowÔºå in the next moduleÔºå I'm going to show you how to actually adapt this to a problem of your own design„ÄÇüòä„ÄÇ

![](img/8a587b916463673ac1aabf070d00efab_1.png)

![](img/8a587b916463673ac1aabf070d00efab_2.png)

And will create something in reinforcement learning that is not open AIG„ÄÇ But for now„ÄÇ let's do the obligatory Atari example„ÄÇ Now I had to build most of this sort of from scratch there is not a lot of TF agent examples of Atari games out there So I worked on this a fair amount to get it working going open this in Google coab because I would like to use a GPU so be deadly„ÄÇ

 deadly slow if you don't use a GPU Now what I'm going to suggest is go ahead and do a run all because there's a lot of code in here and you'll run into an error„ÄÇ hopefully they fix this by the time you're watching this video but at the time you would get an error basically as you you would run through this part here where you install all of the necessary software and I'll update this as things change because things constantly change with machine learning and then as soon as you went down to here and tried to do all„ÄÇ

These importsÔºå you would get an error if you're getting an error in this import blockÔºå do a runtime„ÄÇ restart and then run all again„ÄÇ Reing the runtime does not erase the installation that happened up here„ÄÇ Basically the reason you get that error is because once you P installed TF agents„ÄÇ it updates the version of protocol buffer that is installed on your computer and Tensorflow itself needs a restart of the Python environment or it's gonna to give you an error at that point„ÄÇ

 Hopefully as time progresses Google coab„ÄÇ I'm really hoping they will install Tf agents just out of the box„ÄÇ I assume they will„ÄÇ it's a Google product„ÄÇ It's just a new product relatively new„ÄÇ So let's talk about how we will do this„ÄÇ We're gonna still use a Dqn a deep Q learning network„ÄÇ This is all based on virtual AtariÔºå which is a completely unrelated project a machine learning„ÄÇ

 It was just a emulator so that you could play old Atari 2600„ÄÇ![](img/8a587b916463673ac1aabf070d00efab_4.png)

Gams now I am somewhat oldÔºå but this is before really even my time„ÄÇ There are 2600s„ÄÇ This is called the Atari 2600 out there when I was pretty young„ÄÇ but this was kind of the older game system out there„ÄÇ I did play PAman on this back in the day„ÄÇ which which was kind of fun„ÄÇ I was more into a system that came out around this time called in television a little bit later than the Atari 2600„ÄÇ

 and then of courseÔºå my absolute true love the Commodore 64„ÄÇ So this is the specs for an Atari 2600„ÄÇ It's not an advanced machine by any stretch„ÄÇ Your play field resolution is 40 by 192 pixels„ÄÇ Now they do some creative coding to sort of stretch that out„ÄÇ but you're able to basically just feed that right into convolution neural network„ÄÇ Also„ÄÇ

 the amount of memory in an Atari is not huge„ÄÇ So it's measured in killbyte„ÄÇ![](img/8a587b916463673ac1aabf070d00efab_6.png)

![](img/8a587b916463673ac1aabf070d00efab_7.png)

![](img/8a587b916463673ac1aabf070d00efab_8.png)

It's not not megabytesÔºå actually I think it's less than a kilobyte so you can push the active memory that the games are running on into the well the video you would push into a convolution neural network the 160 by 192 that's the resolution I think you're normally dealing with but you can actually look at the instate memory of the game and use that for the state space and there's a lot of information in there and that can be used the agent can learn to cheat too because some of the memory might be being utilized for the opponent play so the computer controlled the NPCs in the game So we do all the imports hyperparameters i tuned this a lot to get it to work for pong also did some with breakout and space invadersve got you've got to play with these values and get them working for whatever game you're going to do the number of iterations that„ÄÇ

Is doing nearly a quarter of a million of them here„ÄÇ So that is I typically ran this thing all night„ÄÇ It takes a while to trainÔºå even with a GPU„ÄÇ And I'm also doing 10 collection steps per iteration„ÄÇ replay buffer sizeÔºå that didn't make too much of a differenceÔºå but this learning rate„ÄÇ this 10 value here and the number of iterations„ÄÇ Those are the guys that I was really tuning to try to get better results„ÄÇ

 I did not mess with the shape of the neural network that could also help you as well„ÄÇ The stuff does take a lot of time and not just your time compute time to try out your different hyperparameter tunings„ÄÇ So for an Atari environmentÔºå we're using pongÔºå you can see some of the other games I was playing with here as well„ÄÇ Pong's relatively simple„ÄÇ It is basically two paddles bouncing a ball between themselves„ÄÇ

 Now pong was before my time„ÄÇ I never really got into pong whatsoever„ÄÇ That would probably be I mean„ÄÇ that would be the 1970s„ÄÇA little before I got into into video games mid80s is kind of when I got into a lot of this stuff„ÄÇ So the frame skip rateÔºå you're not going to look at every single frame that is coming through in the Atari This is just things to make the computation time reasonable if you are processing every single frame that's going it's going take a lot of processing time there is a wrappper that TF agents gives you to load the Atari games I definitely recommend you do this because it builds in some of the preprocessing that is necessary or not necessary„ÄÇ

 but it really helps your neural network be able to learn quickly because the idea is I mean look at is the color information really telling you all that much and this is the little panel that's going to go up and down the computer the NPC the computer control player by the way„ÄÇ

 NPC is non-player character I guess I get that from the MMOGs theOOGs this is the opponent score„ÄÇThis is your score and technically the neural network is looking at everything„ÄÇ I don't know that it's getting a whole lot from the scores„ÄÇ but that is potentially something that it uses as well„ÄÇ maybe tries harder when it sees it losing„ÄÇ

 I don't know I'm kidding it probably doesn't it probably learns that those scores are pretty pretty unimportant So we load the Atari environment we are using a separate one for training and evaluation just so that if you run an avval step in there that you're not changing the state of the training which could be confusing that would be„ÄÇ

OhÔºå that would be like having an an alcohol blackout almost„ÄÇ you time would skip forward„ÄÇ but you would not know what happened„ÄÇ Probably a bad example of what's going on there„ÄÇ But it works for me as a visual„ÄÇNowÔºå the agentÔºå this I got from TF agent's example„ÄÇ This is just a wrapper around the normal agent„ÄÇ You can see that we're dividing each of the state values„ÄÇ

 but by 255„ÄÇ What you're doing there„ÄÇ Basically is converting that 0 to 255 RGB values into a floating point value between 0 and 1„ÄÇ and the neural network deals with that a little bit better„ÄÇ Most of the examples have done that„ÄÇ I usually do that when I'm dealing with image data„ÄÇ don't feed 0 to 255 into a convolution neural network„ÄÇ

 It can deal with it okay but it really likes those smaller values better„ÄÇ This information here is important„ÄÇ these are actually hyperparameter„ÄÇ I did not try tuning these too much„ÄÇ I just took example values„ÄÇ but here these are the fully connected layers„ÄÇ So these are the dense layers„ÄÇ

 Most convolution neural networks have dense layers at the veryÔºå very end„ÄÇ So we have just a single layer of 512„ÄÇ You could add multiple„ÄÇOnes thereÔºå if you want„ÄÇ And then the convolution layers„ÄÇ This is the channel count 32„ÄÇ scanning an8 by8 with a stride of4„ÄÇ That's basic convolution information„ÄÇ And then several of these„ÄÇ

 It don't really provide a way to put a max pooling layer in there at least that I've seen but this has worked relatively well„ÄÇ I did not try tuning this to too much„ÄÇ or at allÔºå really„ÄÇ then you build up the Atari Q network that we had up there„ÄÇ So this is the wrapper for the neural network„ÄÇ I have more text that describes kind of what I did because this this example I did literally literally build from scratch except for that Q network wrapper that I found We're gonna use an RMS prop optimizer„ÄÇ

 the learning rate and other things here are very important„ÄÇ Those are hyperparameters that you'll want to tune„ÄÇ we use a global steps so that we have the counter for how far we've gone with training„ÄÇ And here is where we create the DQ N agent„ÄÇ DQ N is good when you have„ÄÇ

actionction space or discrete action space where it has individual values and not some continuous range of numbers In the next part„ÄÇ the next videoÔºå we will see how to deal with a continuous range action space„ÄÇ You use a close a close relative to the DQN neural network I use the same evaluation that I got from TF agents„ÄÇ this is a method that I copied a function that I copied basically what it's doing is 10 evaluation steps and it calculates the average reward over 10 steps„ÄÇ

 Atari games are stochastic„ÄÇ They have random numbers„ÄÇ The same strategy won't necessarily work for two games in a row„ÄÇ but they figure if you played it over 10 games's that's gonna be reasonable replayplay buffer we're using pretty much the same type of replay buffer that I've used in the other Tensorflow T agent examples There's nothing too crazy that you need to do here for Atari I use exactly the same random collection code that I've used in a couple„ÄÇ

Of examples so far„ÄÇ and this is code that I got from the TF agents examples works quite well for that„ÄÇ This is exactly the same training„ÄÇ the agent code that I used in previous examples„ÄÇ Nothing needs to really change here„ÄÇ It's pretty boiler plate„ÄÇ some initial setup„ÄÇ and then we're going to loop through the number of iterations and we're going to see this is where that steps for iteration„ÄÇ

 So setting this to 10„ÄÇ each iteration„ÄÇ we're going to collect 10 steps and use that to train the neural network to build to the data that we randomly sample from that we're training the data„ÄÇ the data collection Here you can see it is it is working very hard training Now negative 20 negative 21„ÄÇ

 I think it actually is is the worst you can possibly do if you're just if you're doing bad at pong everything you're not hitting anything you're shooting like stormtroopers„ÄÇ you're sending you're not hit you're not sending it through the other guy he's sending it right back to you„ÄÇ

So I was very depressed with that when I was learning and working through this no matter what I would try„ÄÇ I would get like negative 21 straight downÔºå down down„ÄÇ It'd be four hours laterÔºå negative 21„ÄÇ but as I started to adjust things„ÄÇ I was able to get this to improve considerably and you can see this is a decent training run„ÄÇ I'm fairly happy with it„ÄÇ It got above zero you can do much better than this„ÄÇ

 This is a pretty average run that I have here„ÄÇ and I'll show you what what mine looks like„ÄÇ it does score pointsÔºå it does much better than the random agent„ÄÇ but you could definitely refine this if you wanted to spend a lot of time optimizing those hyperparameter and throwing compute at it visualization„ÄÇ you can see pretty good progression upward probably more iterations would help me and then you can see the video here I'm let it play It's good but not great Oh I scored scored one„ÄÇ

 score2 I'm ahead even I'm behind So it's playing accept well„ÄÇ I never really got too much into pong to play this on my own so I don't know how„ÄÇHow good I would particularly be doing hereÔºå but it'sÔºå it's definitely losing overall„ÄÇ But if you try the random agentÔºå ohÔºå it'sÔºå it's catching upÔºå this is awesome Am I am my headÔºå Yes„ÄÇ

 so this is„ÄÇIt's it's definitely got some skill„ÄÇ It has definitely learned some things from from this game and is„ÄÇ is now ahead„ÄÇ So I'm not going look at the whole three minutes that I captured„ÄÇ Who knows„ÄÇ Maybe it maybe it reallyÔºå really gets better„ÄÇ but let's look at the random agent And you'll see that the reinforcement learning that I took some time optimizing really does make it different„ÄÇ I meanÔºå it's getting killed„ÄÇ It scored one„ÄÇ I meanÔºå a broke clock is right twice a dayÔºå right„ÄÇ

 So that's that's it playingÔºåsÔºå it doesn't take nearly as many minutes„ÄÇ that's a good example right there„ÄÇ Like howÔºå how long does it stay playing before you hit end of game„ÄÇ I meanÔºå like when IÔºå when I would go to arcades back in the 80s„ÄÇ How long would my $2 worth of quarters last that my mom had given me in in my case„ÄÇ

 usually about5 minutesÔºå I was not that good at video games ever„ÄÇ really like centipede„ÄÇ That was probably my favorite Dragon's layer was probably the worse„ÄÇ You had to spendÔºå I think„ÄÇ an entire dollar and I would get killed in like a minute„ÄÇ![](img/8a587b916463673ac1aabf070d00efab_10.png)

![](img/8a587b916463673ac1aabf070d00efab_11.png)

Thank you for watching my video and in the next one we're going to look at how to do the same sort of thing only with a financial simulation„ÄÇIf you find this kind of thing interestingÔºå please subscribe to my channel„ÄÇ Thank you very much„ÄÇ