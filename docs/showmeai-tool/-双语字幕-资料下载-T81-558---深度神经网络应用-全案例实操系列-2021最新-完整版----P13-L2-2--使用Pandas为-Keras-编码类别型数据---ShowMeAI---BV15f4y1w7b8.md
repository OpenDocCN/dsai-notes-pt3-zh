# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P13ÔºöL2.2- ‰ΩøÁî®Pandas‰∏∫ Keras ÁºñÁ†ÅÁ±ªÂà´ÂûãÊï∞ÊçÆ - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff Eaton„ÄÇWelcome to applications of deep neural networks with Washington University„ÄÇ In this videoÔºå we're going to look at how to deal with categorical values„ÄÇ A categorical value is something that is textual and not numeric„ÄÇüòä„ÄÇThese require special handling to be inputed into a neural network„ÄÇ

There are many classic ways of handling thisÔºå such as dummy variables„ÄÇ however„ÄÇ we're going to look at a number of different ways to actually encode this type of value for a neural network for the latest on my AI course and projects„ÄÇ click subscribe and the bell next to it to be notified of every new video categorical and continuous values are two types of data that you'll run into frequently and tabular data sets„ÄÇ

![](img/8a2d61d5d493f9c40cbab3980011f3a5_1.png)

We can really divide the type of data that you will run into„ÄÇIn four categories in all„ÄÇal nominal ordinal interval and ratio„ÄÇCharacter data„ÄÇ So strings you'll think of as either nominal or ordinal„ÄÇ Nominal are just pure class values like redÔºå greenÔºå blueÔºå orangeÔºå indigo„ÄÇ

 like colors or something like that„ÄÇ NowÔºå colorÔºå you might encode that as„ÄÇAs RGB values or something else rather than dummies but„ÄÇTypically„ÄÇA nominal value is just something that there's no particular order that you can think of for it„ÄÇ You should always try to come up with an orderingÔºå if you can„ÄÇ

 And then that makes the nominal value an ordinal„ÄÇ and ordinals are also textual data in your data sets„ÄÇ but they can beÔºå they can be ordered„ÄÇNow you may have occasionally just unbounded text like a note column or a product name„ÄÇThese are neither nominal nor oralÔºå I guess they would be nominal„ÄÇ but you would have way too many dummy variables„ÄÇ So there's other ways that we'll see when we get it a natural language processing that you deal with free form text on the numeric data side„ÄÇ

 There's interval and ratio„ÄÇ These two types really are rarely handled differently„ÄÇ But interval basically our numeric values that have no defined startÔºå you can think of temperature„ÄÇ In my example down thereÔºå you would never say yesterday was twice as hot as today„ÄÇ because there's really noÔºå I meanÔºå short of Kelvin„ÄÇ

 there's really no defined start for how low a temperature can go„ÄÇ Fahrenheit was originally created„ÄÇ assuming that zero was absolute0 and„ÄÇThat was wrongÔºå but that would have been a„ÄÇThat would have been a good example of making it ratio„ÄÇ So a ratio is a numeric value that has clearly defined start„ÄÇ

 You could say that one car is going twice as fast as the second because miles per hour„ÄÇ the speed of a car„ÄÇ the lowest it can go a 0„ÄÇ So it has a defined starting point„ÄÇ I mean„ÄÇ I guess technically negative 10 could be going backwardsÔºå10 miles an hourÔºå but we're not„ÄÇ we're not considering that for this example„ÄÇ So let's first look at how do we encode continuous values„ÄÇ

 continuous values„ÄÇ sometimes you'll want to normalize them„ÄÇ Now„ÄÇ why you potentially need to normalize values is considerÔºå you met a friend on campus and they said„ÄÇ ohÔºå I didÔºå I took this exam yesterdayÔºå and I got 60 points„ÄÇ WellÔºå is that a good score„ÄÇ is that a bad score„ÄÇ that's completely unnormalize„ÄÇ They told you how many points they got„ÄÇ

 how many points are available on the entire exam„ÄÇ If there's 100„ÄÇ then a score of 60 points is really not so good„ÄÇ if there were only 60 points available„ÄÇOn that examÔºå then that score of 60 is perfect„ÄÇ Z scores let you do this„ÄÇ So Z scores normalize a0 z score means that you're exactly at the mean a negative one z score means your one standard deviation below the mean Similarlyly a plus one means your one standard deviation above So this is kind of like expressing something as a percent but it lets you know rather it has the standard deviation baked into it so that that makes it more applicable for normalization Now let's look at how we could normalize something in the miles per gallon database So we're loading the miles per gallon database and we're going to basically take the MPg column and normalize it to a z score So notice the MPg is right these are all negative numbers and you're not seeing very large one this car here is negative one standard„ÄÇ

viation below the average miles per gallon„ÄÇ This lets you know very quickly which ones are above and below the mean„ÄÇ Another advantage to doing this is sometimes when we want to redact data so make the data hard so it's secret so that you can't really tell what's going on with the data yet you can still build models on it Z score can be a great way to do this like people's ages if you make that a Z score„ÄÇ

 somebody looking at your data may not guess that that is actually the person's age because they'll see values that look just like that miles per gallon„ÄÇ whereas if you see values that range between 18 or 21 and 85 you might guess that that is somebody's age and it's useful to redact private information like people's ages and other thing So thats that's another sometimes used for Z scores But what's good about this is if we were to put every one of these columns into Z scores they'd be a lot less readable in a way because you wouldn't see these actual weights and„ÄÇ

Things like thatÔºå but you would be able to very quickly spot things that are above are below the mean Also these will tend to cause these values to be fairly evenly distributed about zero„ÄÇ which in some cases will help neural networks predict power Also you will have all these values in a similar range so the weights of the car could very much overwhelm the cylinders of the car which is a much much lower number so that's some of the advantages of using Z scores for continuous value andcoding Now for categoricals„ÄÇ

 the tried and true one that you're always going to hear about is dummies and if we load the sample data set that we've seen before„ÄÇ this is the data set that has the job codes areas incomes predicting which of several products the person' is going to buy if we look at this one we can create and we've seen this previously in this course we can basically look at how many areas there are there are four and we can create the dummy„ÄÇ

For you get the AB So for the dummiesÔºå what this is doing is this is showing you essentially the lookup table for these„ÄÇSo for the first one areaÔºå if it's going to be a„ÄÇThe dummyÔºå the first dummy will be true„ÄÇ The rest falseÔºå SimilarlylyÔºå for BÔºå the second andÔºå and so on„ÄÇ Now„ÄÇ if we want to actually encode the data using that sort of lookup tableÔºå we do this„ÄÇ

 And these are the dummies that were created„ÄÇ So the first„ÄÇ the first few rows were probably all value C„ÄÇ Then we had a value D„ÄÇ And we can actually see this then by„ÄÇRunning thisÔºå all of these first ones where C and similarly„ÄÇ C is filled in DÔºå C for the area„ÄÇ And now we have the dummies actually added to it„ÄÇ Now„ÄÇ

 our ultimate goal is to get this completely numeric„ÄÇ So we need to drop the area„ÄÇOne from this„ÄÇ and we do this„ÄÇAnd we can see now that area has been removed„ÄÇ So this is one step closer to to getting this into a form that we can actually present to a neural network„ÄÇ We still have to do a similar encoding on job„ÄÇ Now„ÄÇ

 there's other ways to encode for dummy variables„ÄÇ There's another way to categoricals„ÄÇ other than dummies„ÄÇ You can use something called target encoding„ÄÇ Now„ÄÇ target encoding is a little more advanced of a technique„ÄÇ It's commonly used on kggles„ÄÇ So it's„ÄÇIt's potentially quite powerfulÔºå but it's potentially quite dangerous because you can easily„ÄÇ

 easily overfit with thisÔºå so be very careful with it„ÄÇ You should always be using some sort of a holdoutet or a cross validation technique to make sure that you're not getting artificially high accuracy in results because you might be overfit„ÄÇLet's create a sample data set here real quick„ÄÇ We look at this data set„ÄÇ We have a continuous value hereÔºå which we actually won't use„ÄÇ And then two categoricals„ÄÇ

 categoricals 0 and categorical  one„ÄÇ And then yÔºå which is the target that we're trying to predict„ÄÇ we could create dummy variables for dogÔºå catÔºå Wolf and ti„ÄÇ NowÔºå againÔºå rememberÔºå each of these„ÄÇ each of these has two classes„ÄÇ So„ÄÇIt would be ti and Wolf for cat 1 and dog and cat for cat 0„ÄÇ What we're going to do if we run this line hereÔºå this is going to group by cat 0„ÄÇ

On y to look at the mean„ÄÇ So it's going to tell you what the mean value is of the target of y for dog and cat„ÄÇ So we run that cat is 0„ÄÇ2„ÄÇ Dog is 0„ÄÇ8„ÄÇ This is what you like to see when you're evaluating these for target encoding„ÄÇ FortunatelyÔºå these two are differentiated„ÄÇ This tells you something right there„ÄÇ The output from this„ÄÇCat or dog is very predictive of what the outputs going to be if this was closer to the midpoint between the two targets„ÄÇ

 so the targets are 0 and oneÔºå if this was closer to 0„ÄÇ5„ÄÇTarget encoding is probably not going to help you on that particular category„ÄÇ SimilarlylyÔºå dog is 0„ÄÇ8„ÄÇ So that'sÔºå and since the the target itself is oneÔºå these should sum to the target„ÄÇ So essentially what you can think of target encoding doing is very sum„ÄÇ

 We're going to keep cat at cat 0 as a single column„ÄÇ We're not going to blow it out to two like we would do with dummy variables„ÄÇAnd we're going to essentially„ÄÇPut 0„ÄÇ2 in for all the catsÔºå 0„ÄÇ8 in for all the dogs„ÄÇAnd we have now changed that into a numberÔºå and that number will help to predict the y out„ÄÇ Now„ÄÇ

 this is breaking an absolutely cardinal rule of data science„ÄÇ You are now using the target for„ÄÇYour prediction for before you're actually training„ÄÇThat is potentially dangerous„ÄÇ so be very careful with this techniqueÔºå and I'll show you kind of how we can mitigate that risk a little bit and where this risk comes in is when you have a very small number of one class„ÄÇ we only have one tiger and the tiger is zero„ÄÇ So the average on the tiger is going to be zero„ÄÇ

So any place that we replace the tiger with a zeroÔºå this one row is going to be absolutely correct„ÄÇJust based on that one columnÔºå so it's going to correlate 100% from Cat one to the target„ÄÇThe riskier this is is when you have low cardinality or low numbers of one particular class„ÄÇ the way we do that is by keeping count of how many of each of these we do and specifying a weight that says how much in these low cases will we tend towards the average of y overall„ÄÇ

 so we take the overall average of y and we which can be calculated here„ÄÇ So 0„ÄÇ5 So there's exactly half as many of each„ÄÇAnd for cases like the ti„ÄÇ we would tend very much towards 0„ÄÇ5 rather than 0„ÄÇ So you've got these two factors as part of this„ÄÇ you have the average for Wolffer for tiÔºå but then you also have the overall average for why„ÄÇ

 So we calculate a smooth mean so we try to smooth it out and the smoothing factor is passed in here„ÄÇ You're going to pass in two data frames that you want modified„ÄÇ The reason I have two data frames is because you're probably going to have a training and also a test set„ÄÇYou may want even more than that„ÄÇ You can modify the function„ÄÇ

 That's a change that I made from the original function that I copied from Max Halford„ÄÇ which I've given a link to his original one here I„ÄÇFixed a few things and also made that modification„ÄÇYou pass in the name of the categorical variable that you're going to modify what the name of the target is and then the weighting if this weight is zero„ÄÇ

Then it's going to essentiallyÔºå the risk of overfitting is much higher because it is going more towards what the average for each of those individual categories is„ÄÇAnd if D F2Ôºå if you don't have a second data frameÔºå just pass inÔºå pass in none for that„ÄÇ

 So when setting the weightÔºå it's important to keep in mind that the stronger the weight value passed in„ÄÇ the categories with a smaller number of values will tend towards the overall average of y„ÄÇ which means less chances for overfitting„ÄÇWeaker weight will have a greater potential to overfit Now„ÄÇ encoding categorical values as ordinalsÔºå this is a very strong technique„ÄÇ

 no real chance of overfitting„ÄÇ say you had all of these education levels like kindergarten up through postdoctorate„ÄÇYou could create dummy variables for all of theseÔºå but you're actually losing some information because these are ordered„ÄÇ you don't do fifth grade before postdoctorateÔºå so you could assign a number 0 through 20 for each of these and put in that number in place of the dummy variables so instead of having 21 of these you will have just the one value that you put into there„ÄÇ

And then the other thing you may want to do is you may want to change the weighting because say a graduate student„ÄÇ you might be a graduate student longer than you areÔºå say in sixth grade„ÄÇ one year versus who knows how many years for the graduate Thank you for watching this video on categorical values„ÄÇ

![](img/8a2d61d5d493f9c40cbab3980011f3a5_3.png)

In the next videoÔºå we're going to see how to do other preprocess with pandas such as grouping sorting and shuffling of the dataset set„ÄÇ This content changes oftenÔºå so subscribe to the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇ

