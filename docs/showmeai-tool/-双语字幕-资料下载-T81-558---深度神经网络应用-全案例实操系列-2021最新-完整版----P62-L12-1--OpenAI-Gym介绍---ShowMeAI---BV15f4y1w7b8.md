# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P62ÔºöL12.1- OpenAI Gym‰ªãÁªç - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeffy„ÄÇ Wecome to applications of deep neural networks with Washington University„ÄÇ

 In this moduleÔºå we're going to look at AI gym„ÄÇ So AI gym is a„ÄÇüòä„ÄÇ

Benchmark platform that lets you in Python evaluate your reinforcement learning programs against wellknown games„ÄÇ

 Atari games and other things like that„ÄÇ So we're going to see how you can actually interface into this and use this to even watch the games play on the screen or to train them in an offline way to see all my videos about kgel neural networks and other AI topics„ÄÇ

 click the subscribe button and the bell next to it and select all to be notified of every new video So let's take a look at open AI gym So reinforcement learning is what we're dealing with for this module and the next five parts So reinforcement learning it's not like supervised learning or unsupervised learning it is really sometimes referred to a selfupervised It's going to happen is you're going to give the computer some sort of a game„ÄÇ



![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_1.png)

![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_2.png)

And it doesn't have to be game„ÄÇ It could beÔºå it's really anything that has well defined rules and can be completely simulated in the computer„ÄÇ

 So basically a gameÔºå but„ÄÇTo use this for other types of learning„ÄÇ

 you really need to express your problem in such a way so that the rules are well definedfined so that there's kind of an increment as you go through episode So each play of the game so to speak is an episode you go through a series of steps at each step the computer can take a given action and then at the end„ÄÇ

 its performance is evaluatedÔºå did it succeed or did it notÔºå and at each step„ÄÇ

 it is given a reward based on how well it did And the machine plays it over and over and over again and it's able to learn and get very good at playing this This was used and go and chess to develop extraordinarily powerful computers that can play better than the humans who ever taught it how to do this and it does this basically by playing chess essentially with itself because the rules are well well defined„ÄÇ

It gets better and better and better„ÄÇ And what's amazing about it is it needs no prior knowledge of chess„ÄÇ

 So all those chess books that talk about famous openings and whatnot„ÄÇ

 it's almost like an alien came from another parallel Earth that develop chess on their own„ÄÇ

 but the two planets did not share any techniques on chess„ÄÇ

 So these extremely advanced chess players from the parallel world came to Earth to play and they're playing chess from a whole different rule book„ÄÇ

 So I think that was part of the complexity too for the goplay as well to beat these highly optimized machines„ÄÇ

 So let's see open AI lab„ÄÇ So open AI lab Jim is basically a bunch of games„ÄÇ

 even the Atari games all set up so that you can have your program compete against them„ÄÇ

 So this is open AI gym„ÄÇ you can see that they have Atari games„ÄÇ these are running through an Atari„ÄÇ



![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_4.png)

LaterThat works really well on Mac and on Linux on Windows„ÄÇ you can make it work„ÄÇ

 There is some complexities to getting the Atari emulator working on Windows„ÄÇ

 the classic games like you see here that is the pole cart that's the pendulum„ÄÇ

 Those work just fine and we'll start off using pole cart and using mountain car„ÄÇ

 which are some fairly classic games for exampleÔºå let me show you what those look like„ÄÇ

 if you click on the environmentsÔºå you can see these are the classic control games„ÄÇ

 The ones that we're going to work on in particular are cartpo and mountain car„ÄÇ

 There's two versions of mountain car and this becomes important when we see how to actually interact with these„ÄÇ

 there is this is mountain car that basically has a Boolean throttle„ÄÇ

 Either you've got your foot on the brake all the way down or you've got your foot on the gas all the way down„ÄÇ

 the idea of mountain cart mountain car is to try to get up here to this flag„ÄÇ



![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_6.png)

![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_7.png)

Car's engine is simply not powerful enough to do it„ÄÇ It is going to even if you floor it„ÄÇ

 So we pretty much have to„ÄÇ you've got three controls on this one forward backward and break„ÄÇ

 So pushing it upÔºå it's going get to about here and then just roll back down„ÄÇ

 It just does not have enough momentum So what it needs to do is go as fast as it can roll back back back up here„ÄÇ

 push it hard to go up here and then push it hard to go up here and eventually just by rocking back and forth„ÄÇ

 you get enough momentum that you make it up the hill So it takes a little AI to learn to do that„ÄÇ

 I also show you an algorithm that I write that is just if statements that is able to solve it to basically just whichever direction you're going if you're going up„ÄÇ

 always force the throttle full speed the way up the hill you're going the continuous one„ÄÇ

 you can push the pedal partially down or partially up„ÄÇ I don't think that really helps„ÄÇ

 I think you pretty much need to floor it to make it up up the hill„ÄÇ

 This one's just trying to balance So you've got this pull„ÄÇI mean„ÄÇ

 think of it like if you took a meter rod or a yardstick and you tried to balance it on top of your hand and now walk across the room and have that not fall off„ÄÇ

That's basically what it's trying to do„ÄÇSo let's go back here to the notes for this section for this part in Google Collab in GitHub actually now we're opening it in Collab„ÄÇ

 so it's opened here„ÄÇ Let's look at how to actually make use of AI gym I've got some basic information here but the first thing we're going to do is we're going to import AI gym and each of those games that I showed you is basically called an environment and this function that I just defined here essentially queries and environment you need to open up the environment and then you need to open up the specification for that environment„ÄÇ

 What I'm doing here is you can run this for each of the environments that we're going to be dealing with and I'll show you it for a couple of Atari games as well If you run it for mountain car It tells you the action space The action space of this type is discrete So that means you can be doing three discrete things to run that mountain car„ÄÇ

You can be going backwardsÔºå you can be applying the brakes„ÄÇ I think you're applying the brakes„ÄÇ

 It might be running idle I forget exactly and you can be going forward„ÄÇ

 I'll tell you on dealing with just about any of these environments I usually look at the source code and Github for that environment to really truly understand how they work Now the observation space So you've got the action space„ÄÇ

 these are things that you can do and then the observation space is basically how the world has changed Now there's two box values box basically means continuous so there's two continuous variables that come back and those two are basically telling you the location of that car left and right so how far up the hill essentially is it and in which side so it's essentially the X on that screen that was on and then the other value is how fast is it going and that's a plus or minus depending on which which direction it is moving it and that's the whole„ÄÇ

Unniverse those are the only two values that it needs Max episode steps„ÄÇ

 So each episode is a play of this game and that's how long the episode is gonna last if the car is not figured it out by 200 steps„ÄÇ

 we give up Think of a step like a frame think of you're playing a video game you know a video game your render engine is shooting for maybe 30 frames a second which is what a movie is each of those steps is a frame so it's going to go for a max of of 200 of those nondeterministic I don't really like the naming of this one in particular Most of these episodes or environments are random somewhat So your cart it's not like the physics calculation on that cart is completely predictable now you can seed it and you can put a seed into here and that's what the nondeterministic means if you seed it then that means it will be deterministic but if you don't seed it's it's somewhat random So that is„ÄÇ

Essentially what this means most of the environments that I've worked with are nondeterministic so if you seed them„ÄÇ

 then they're going to be the same„ÄÇ So this is important to understand if you've trained a neural network to play mountain cart and you have it play it with a different seed it might lose because it's facing random situations it's learned some basic ideas about how to how to play the game„ÄÇ

 but if I don't know if you look at the world's best race car driver„ÄÇ

 there's no guarantee that they're going they're going to win every time because there's a lot of random and random are just things outside of their control outside of each game and they may not win It's the same thing now for each of those steps„ÄÇ

 you're going to get a reward So you essentially give it the action„ÄÇ

 it gives you back the observation so what does the world now look like for the next step and then you give it another action and tells you what the world looks like the reward threshold is essentiallys what that's your basic reward so if you get the negative„ÄÇ

110 you didn't do so good„ÄÇ Now mountain car is very stingy on rewards„ÄÇ

 you only get a reward when you win which makes it a little difficult to train for because it's I mean„ÄÇ

 imagine if you had to build this building and you don't get any feedback at each step„ÄÇ

 you build the entire building and they tell you if it was good at the end„ÄÇ

 that's very frustrating but the neural network can actually optimize something that does win mountain car and we'll see an example of that in in the next class Now I can also query the environment for cart pole„ÄÇ

 So that's the one where you're trying to balance the pole and a cart the accent space for this one is discrete„ÄÇ

 So you can't stop you are moving in one direction or another and your observation space is for values I had to go look at the code to figure out what those are„ÄÇ

 but essentially it's the cart positionÔºå you're moving the cart only in two dimensions the cart velocity how fast is it going because as you apply greater force from those two discrete„ÄÇ

Dires you accelerate what is the angle of the pole and what is the pole velocity at tip so that's kind of the physics usually you think of the velocity of a pendulum or a pendulum I means it's it's essentially an inverted pendulum„ÄÇ

And what is the velocity at the tip of that„ÄÇ And you can push the car to the left or the right„ÄÇ

 So the only two of these values that we're dealing with are discrete„ÄÇ

 meaning there's only two So it's one variableÔºå two possible settings or continuousÔºå which is box„ÄÇ

 So there's four continuous values„ÄÇ And if you're dealing with the mountain car continuous variety„ÄÇ

 they're both boxes„ÄÇ So the action space is just the accelerator„ÄÇ

 you can give it positive or negative acceleration and a continuous range or zeroÔºå meaning nothing„ÄÇ

 The maximum episode steps for some reason is 999„ÄÇ and it it's a double negative nondeterministic„ÄÇ

 So it is not deterministic so„ÄÇAnd then the reward range can go from negative to to positive and the reward threshold is 90„ÄÇ

 Don't worry about this warning„ÄÇ I think that's a bug actually in open AI gym Open AI gym is not maintained a lot there's a lot of issues posted on it„ÄÇ

 but it's a great toolkit still to use for this Now breakout This is an Atari game and I want to show you what they do for some of these Atari games which is just kind of fascinating So the observation spaces it's essentially an image 210 by 160 by three So''s essentially a red green blue image and what you're doing here is using the image so what is the user looking at it's looking at the screen just like a user that is cool that it is just looking at that image and learning how to play the game episode steps can go on for quite a while you know 1000 of them Nodeterministic is almost always false on these and the reward range can be anything Re threshold they don't even give you one typically I don't use the range in the threshold„ÄÇ

Kd FYs NowÔºå thisÔºå I thinkÔºå is really fascinating„ÄÇ They also give you breakout Ram„ÄÇ

 You're looking at the Ram of the Atari 2600„ÄÇ The Atari 2600 has 128 bys of Ram bys not K not meg and certainly not gig„ÄÇ

 So this is not that much memory„ÄÇ I meanÔºå heck the image size is bigger than the Ram that it took for that game„ÄÇ

 I meanÔºå one row is 210 times three bys„ÄÇ So that's a lot of that's not a lot of memory„ÄÇ

 but by looking at the memory for this gameÔºå you can kind of tell what's going on„ÄÇ

 And who what are those 128 bys for in the old school„ÄÇ

 they call that a memory map and who knows only the Atari developers know„ÄÇ

 they're probably not even using all 128 of them„ÄÇ And just by looking at the memory manipulate as as you play the game„ÄÇ

 you're able to to play the game„ÄÇ Now both of theseÔºå the action space„ÄÇ So the input is4„ÄÇ

 That's your joystick„ÄÇFour positions up down left right because you're moving that little square around trying to bat the ball and you don't need to give a bat the ball command„ÄÇ

 just the fact that it hits your player causes it to bounce out and what's fascinating and we'll see how to do this we develop a agent that is able to learn to play these Atari games this I also want to show you because we're doing coab if you were running this on your actual local computer what would happen when you try to render and play these games as a window would pop up and you would actually see the Atari game So if you want to actually play the Atari game on your computer in coabab since coab's a virtual environment I'll show you how you can do this So we're gonna to run this part here it does a Pip install we're installing Python virtual display which is great for something like coab I use it also to generate some of the videos that I attach in to show you kind of things playing it gives you a Python virtual„ÄÇ

Environment and it essentially is going to render it to a file that we can then turn into a video„ÄÇ

 let's go ahead and run these So essentially you're going to have it play the guitar game and record a video of it playing and you can potentially download the video or just watch watch it in coab so you don't get to watch it play in real time you kind of see it after the fact but that's just as good and it's all you can really do in collab Now this part here and I do provide a link because I got these functions from a very handy write up that somebody did on how to use this in coab so'm going to run these just define it that shows the video and sets it up for you we're gonna play atlantis Atlantis version zero so I'm going go ahead and run this what's basically going on here is wild true it's not using any AI at all it's essentially doing a it's sampling from the action space so it's basically taking random acts and look there's the game This is not play„ÄÇ



![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_9.png)

In real timeÔºå this is a video that had been recorded briefly while that cell was running„ÄÇ

 but now you can watch it play over and over again it's about a minute and 16 you can see the players getting beat badly you've got a gun here„ÄÇ

 you've got a gun and you've got a gun and it is shooting some of the some of the spaceships but the spaceships start to eventually take it out as as we play further and further the idea here is got to love 1980s graphics this is a underwater city this is water and oops just tuck out the main cannon so it's as you lose your lives but this is just an overview of AIG will be using it more in the next to really every every module in this every part of this module„ÄÇ



![](img/e05a69d9ca1dbcb4403e08d1b85b5a4e_11.png)

Thank you for watching this video and if you're interested in more things about reinforcement learning and training it for games„ÄÇ

 please subscribe to my YouTube channelÔºå thank you very much„ÄÇ

