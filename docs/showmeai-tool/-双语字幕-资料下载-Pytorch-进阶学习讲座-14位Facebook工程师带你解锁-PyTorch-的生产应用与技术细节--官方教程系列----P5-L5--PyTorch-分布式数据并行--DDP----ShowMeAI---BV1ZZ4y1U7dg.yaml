- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5-
    PyTorch åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP) - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5-
    PyTorch åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP) - ShowMeAI - BV1ZZ4y1U7dg
- en: ğŸ¼ã€‚![](img/8bb021252445710b64cf368008f50dc4_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ã€‚![](img/8bb021252445710b64cf368008f50dc4_1.png)
- en: Hey everybodyï¼Œ this is Priam themania and I'm a software engineer at Facebook
    working on Pythtos Disbut I'm going to talk a little bit more about Pyths Dis
    todayã€‚In terms of agendaï¼Œ I'm going to talk a little bit about distributed data
    parallelableã€‚ which is DP and CTNDï¼Œ which is a distributed communication libraryã€‚
    and then I'll talk a little bit about like future work in terms of what's coming
    in the future for PythOs distributedã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯Priam themaniaï¼Œæˆ‘æ˜¯Facebookçš„ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ­£åœ¨ç ”ç©¶Pythtos Disï¼Œä½†ä»Šå¤©æˆ‘å°†è¯¦ç»†è®²è®²Pyths Disã€‚å…³äºè®®ç¨‹ï¼Œæˆ‘ä¼šè°ˆè°ˆåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼Œç®€ç§°DPï¼Œä»¥åŠCTNDï¼Œè¿™æ˜¯ä¸€ç§åˆ†å¸ƒå¼é€šä¿¡åº“ã€‚ç„¶åæˆ‘ä¼šè®¨è®ºä¸€ä¸‹PythOsåˆ†å¸ƒå¼çš„æœªæ¥å·¥ä½œã€‚
- en: '![](img/8bb021252445710b64cf368008f50dc4_3.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb021252445710b64cf368008f50dc4_3.png)'
- en: So let's take a quick refresher of distributed data parallel firstã€‚ so if you
    have a single model that's small enough to fit on a single GPUã€‚ what youd do is
    like you'd use distributed data parallel to train this on a large scale in terms
    of large amount of data and large amount of GPUsã€‚ so you would replicate this
    model on multiple GPUsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªè¶³å¤Ÿå°çš„æ¨¡å‹å¯ä»¥é€‚åº”å•ä¸ªGPUï¼Œä½ ä¼šä½¿ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œåœ¨å¤§è§„æ¨¡çš„æ•°æ®å’Œå¤šä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½ ä¼šåœ¨å¤šä¸ªGPUä¸Šå¤åˆ¶è¿™ä¸ªæ¨¡å‹ã€‚
- en: run the forward and backward Pass and parallel and then once you have the gradientsã€‚
    you'd have like a synchronized gradients operation which all of the ranks will
    enter to kind of aggregate all of the gradientsã€‚ and then you'd kind of continue
    other iterations where you run more forward and backward passes and synchronized
    gradientsã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œå‰å‘å’Œåå‘ä¼ æ’­å¹¶è¡Œå¤„ç†ï¼Œå¾—åˆ°æ¢¯åº¦åï¼Œæ‰€æœ‰çš„rankä¼šè¿›å…¥ä¸€ä¸ªåŒæ­¥æ¢¯åº¦æ“ä½œï¼Œä»¥èšåˆæ‰€æœ‰çš„æ¢¯åº¦ã€‚æ¥ç€ï¼Œä½ ä¼šç»§ç»­å…¶ä»–è¿­ä»£ï¼Œè¿›è¡Œæ›´å¤šçš„å‰å‘å’Œåå‘ä¼ æ’­ä»¥åŠåŒæ­¥æ¢¯åº¦æ“ä½œã€‚
- en: '![](img/8bb021252445710b64cf368008f50dc4_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb021252445710b64cf368008f50dc4_5.png)'
- en: So that was a quick overview now let's talk about what's coming what are kind
    of some of the new improvements in DP so the first one is DP communication hook
    So what this does it allows you to completely override the synchronized gradient
    operation that I just talked about so you can register a Python colable and then
    have some arbitrary logic in terms of how you want to like aggregate the gradients
    So one example here is if youll actually want to do FP16 compression of gradients
    before you communicate that you could have like a callable like this where you
    kind of compress the gradients you convert them to float 16 you all reduce and
    float 16 and then finally you decompress back to float 32 so there's one example
    you could do more fancier things like gossip gridd which is an non full sync SD
    algorithmã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªå¿«é€Ÿæ¦‚è¿°ï¼Œç°åœ¨è®©æˆ‘ä»¬è°ˆè°ˆæœªæ¥çš„æ”¹è¿›ï¼ŒDPçš„ä¸€äº›æ–°è¿›å±•ã€‚ç¬¬ä¸€ä¸ªæ˜¯DPé€šä¿¡é’©å­ã€‚è¿™ä¸ªåŠŸèƒ½å…è®¸ä½ å®Œå…¨è¦†ç›–æˆ‘åˆšæ‰æåˆ°çš„åŒæ­¥æ¢¯åº¦æ“ä½œï¼Œå› æ­¤ä½ å¯ä»¥æ³¨å†Œä¸€ä¸ªPythonå¯è°ƒç”¨å¯¹è±¡ï¼Œç„¶åæ ¹æ®ä½ æƒ³è¦å¦‚ä½•èšåˆæ¢¯åº¦æ¥æ·»åŠ ä»»æ„é€»è¾‘ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœä½ æƒ³åœ¨é€šä¿¡ä¹‹å‰å¯¹æ¢¯åº¦è¿›è¡ŒFP16å‹ç¼©ï¼Œä½ å¯ä»¥æœ‰è¿™æ ·çš„ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œé¦–å…ˆå‹ç¼©æ¢¯åº¦ï¼Œå°†å…¶è½¬æ¢ä¸ºfloat
    16ï¼Œè¿›è¡Œå…¨å±€å½’çº¦ï¼Œæœ€åå†è§£å‹å›float 32ã€‚è¿™æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œä½ è¿˜å¯ä»¥åšæ›´å¤æ‚çš„äº‹æƒ…ï¼Œæ¯”å¦‚gossip griddï¼Œè¿™æ˜¯ä¸€ç§éå…¨åŒæ­¥çš„SDç®—æ³•ã€‚
- en: Okay the next item is support for uneven inputs in DP so if you have like uneven
    number of batches across different ranksã€‚ what would typically happen is some
    ranks which have like finished their data would not enter like this synchronized
    gradient call while other ranks which are still kind of processing data would
    enter this call and as a result this will lead to either a hang or some sort of
    timeout so this has been a longstanding issue in DP that a lot of Pythto users
    have complained about so now we do have a fix for this you can use this modeled
    or joint context wrapper that's shown in the example here so what this ensures
    is that once some ranks are finished with their data theyll kind of do a bunch
    of dummy synchronized operations to kind of match other ranks which are still
    processing data and this guarantees all ranks complete their processing altogether
    so this is a niceã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä¸‹ä¸€ä¸ªé¡¹ç›®æ˜¯DPå¯¹ä¸å‡åŒ€è¾“å…¥çš„æ”¯æŒï¼Œæ‰€ä»¥å¦‚æœä½ åœ¨ä¸åŒçš„rankä¸­æœ‰ä¸å‡åŒ€æ•°é‡çš„æ‰¹æ¬¡ã€‚é€šå¸¸å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œä¸€äº›å·²ç»å®Œæˆå…¶æ•°æ®çš„rankä¸ä¼šè¿›å…¥è¿™ä¸ªåŒæ­¥æ¢¯åº¦è°ƒç”¨ï¼Œè€Œå…¶ä»–ä»åœ¨å¤„ç†æ•°æ®çš„rankåˆ™ä¼šè¿›å…¥è¿™ä¸ªè°ƒç”¨ï¼Œç»“æœä¼šå¯¼è‡´æŒ‚èµ·æˆ–æŸç§è¶…æ—¶ã€‚å› æ­¤ï¼Œè¿™åœ¨DPä¸­ä¸€ç›´æ˜¯ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„é—®é¢˜ï¼Œè®¸å¤šPythtoç”¨æˆ·å¯¹æ­¤è¡¨ç¤ºä¸æ»¡ï¼Œç°åœ¨æˆ‘ä»¬å¯¹æ­¤æœ‰äº†ä¿®å¤ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªåœ¨è¿™é‡Œæ˜¾ç¤ºçš„æ¨¡å‹æˆ–è”åˆä¸Šä¸‹æ–‡åŒ…è£…å™¨ã€‚è¿™ç¡®ä¿äº†ä¸€æ—¦æŸäº›rankå®Œæˆäº†æ•°æ®å¤„ç†ï¼Œå®ƒä»¬ä¼šæ‰§è¡Œä¸€ç³»åˆ—è™šå‡çš„åŒæ­¥æ“ä½œï¼Œä»¥åŒ¹é…ä»åœ¨å¤„ç†æ•°æ®çš„å…¶ä»–rankï¼Œè¿™ç¡®ä¿æ‰€æœ‰rankèƒ½å¤ŸåŒæ—¶å®Œæˆä»–ä»¬çš„å¤„ç†ï¼Œè¿™çœŸæ˜¯å¤ªå¥½äº†ã€‚
- en: wayay to kind of deal with uneven inputs across your trainingã€‚Then we have some
    memory optimizations for DPï¼Œ so DP today creates a bunch of buckets and to kind
    of batch parameters together for an all reduceduce call which is much more efficient
    but what DP does is it creates entire copy of the gradients for these buckets
    so if you have a one gigabyte model you'll have like one gigyte of parametersã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤„ç†è®­ç»ƒä¸­ä¸å‡åŒ€è¾“å…¥çš„ä¸€ç§æ–¹æ³•ã€‚ç„¶åæˆ‘ä»¬æœ‰ä¸€äº›é’ˆå¯¹DPçš„å†…å­˜ä¼˜åŒ–ï¼Œå› æ­¤ä»Šå¤©çš„DPä¼šåˆ›å»ºä¸€å †æ¡¶ï¼Œå°†å‚æ•°æ‰¹å¤„ç†åœ¨ä¸€èµ·ä»¥è¿›è¡Œä¸€æ¬¡æ›´é«˜æ•ˆçš„å…¨å½’çº¦è°ƒç”¨ï¼Œä½†DPæ‰€åšçš„æ˜¯ä¸ºè¿™äº›æ¡¶åˆ›å»ºæ•´ä¸ªæ¢¯åº¦çš„å‰¯æœ¬ï¼Œå› æ­¤å¦‚æœä½ æœ‰ä¸€ä¸ªä¸€GBçš„æ¨¡å‹ï¼Œä½ å°†æ‹¥æœ‰å¤§çº¦ä¸€GBçš„å‚æ•°ã€‚
- en: one gigabyte of gradients and the DP would take another gigabyte because it
    creates a copy of these gradients so to get around this we have a new parameter
    and D called gradient as bucket view so what this does is it makes the dot grad
    field of your parameter a view of the bucket so that way we kind of have only
    one copy of the gradientsã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€GBçš„æ¢¯åº¦ï¼ŒDPä¼šå†å ç”¨ä¸€GBï¼Œå› ä¸ºå®ƒä¼šåˆ›å»ºè¿™äº›æ¢¯åº¦çš„å‰¯æœ¬ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å‚æ•°Dï¼Œç§°ä¸ºæ¢¯åº¦æ¡¶è§†å›¾ï¼Œå› æ­¤è¿™ä½¿å¾—ä½ çš„å‚æ•°çš„dot gradå­—æ®µæˆä¸ºæ¡¶çš„ä¸€ä¸ªè§†å›¾ï¼Œè¿™æ ·æˆ‘ä»¬å°±åªæ‹¥æœ‰ä¸€ä»½æ¢¯åº¦å‰¯æœ¬ã€‚
- en: Then I'd like to talk about combining DP and RPpC so Shan in his talk kind of
    described the RPpC framework and how it can be used for distributed model parallelism
    and DDP is used for distributed data parallelism so now we can kind of combine
    both of these frameworks together to kind of have more complicated training paradigms
    so as you can see in this example we have a DP model where it's a model wrapped
    in DP here so that model is replicated and then we have some remote parameters
    here on worker1 so now if you'd like to train this model you set up your distributed
    optimizer in your forward pass you retrieve the RF which is typically retrieve
    via RPpC and then you feed that into DP you compute the loss and then you run
    your backward and optimizer step so this will kind of run the backward it will
    kind of aggregate the gradients across all of the replicas and then it'll also
    update the remote parametersã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘æƒ³è°ˆè°ˆå°†DPä¸RPpCç»“åˆä½¿ç”¨ï¼Œå› æ­¤åœ¨ä»–çš„æ¼”è®²ä¸­ï¼ŒShanæè¿°äº†RPpCæ¡†æ¶ä»¥åŠå®ƒå¦‚ä½•ç”¨äºåˆ†å¸ƒå¼æ¨¡å‹å¹¶è¡Œï¼Œè€ŒDDPç”¨äºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸¤ä¸ªæ¡†æ¶ç»“åˆèµ·æ¥ï¼Œå½¢æˆæ›´å¤æ‚çš„è®­ç»ƒèŒƒå¼ã€‚å¦‚ä½ æ‰€è§ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªDPæ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹è¢«åŒ…è£…åœ¨DPä¸­ï¼Œå› æ­¤è¿™ä¸ªæ¨¡å‹è¢«å¤åˆ¶ï¼Œç„¶åæˆ‘ä»¬åœ¨worker1ä¸Šæœ‰ä¸€äº›è¿œç¨‹å‚æ•°ï¼Œæ‰€ä»¥ç°åœ¨å¦‚æœä½ æƒ³è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œä½ éœ€è¦åœ¨å‰å‘ä¼ æ’­ä¸­è®¾ç½®åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ï¼Œä½ æ£€ç´¢RFï¼Œé€šå¸¸æ˜¯é€šè¿‡RPpCæ¥æ£€ç´¢çš„ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°DPä¸­ï¼Œè®¡ç®—æŸå¤±ï¼Œç„¶åè¿è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–æ­¥éª¤ï¼Œè¿™å°†æ‰§è¡Œåå‘ä¼ æ’­ï¼Œå®ƒå°†è·¨æ‰€æœ‰å‰¯æœ¬èšåˆæ¢¯åº¦ï¼Œç„¶åè¿˜ä¼šæ›´æ–°è¿œç¨‹å‚æ•°ã€‚
- en: And it'll kind of run the optimizer remotery as wellã€‚ So as you can seeã€‚ you
    can like combine both of these frameworks pretty seamlesslyã€‚![](img/8bb021252445710b64cf368008f50dc4_7.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¹Ÿä¼šè¿œç¨‹è¿è¡Œä¼˜åŒ–å™¨ã€‚å¦‚ä½ æ‰€è§ï¼Œä½ å¯ä»¥éå¸¸é¡ºåˆ©åœ°ç»“åˆè¿™ä¸¤ä¸ªæ¡†æ¶ã€‚![](img/8bb021252445710b64cf368008f50dc4_7.png)
- en: Okay now I' like to talk about dynamic bucketing in DP so DP would kind of split
    the parameters into multiple buckets as I just mentioned and it kind of assumes
    that the order of the backward pass is the reverse of model do parameters when
    it kind of builds these buckets so if this if this order is not true and in many
    models this is the case what would happen is like the buckets are not built in
    the optimal order and as a result maybe bucket2 gets ready before bucket one and
    then bucket one needs to wait before it can kind of schedule it's all reduce so
    this kind of results in suboptimal performance to kind of get around this DP now
    records the order of the parameters in the first backward pass and then rebuilds
    the buckets in the optimal order so that you can now schedule the all reduces
    optimally so this kind of showed about like3 to7% speed up in models like B and
    Robertaã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘æƒ³è°ˆè°ˆ DP ä¸­çš„åŠ¨æ€åˆ†æ¡¶ã€‚DP ä¼šå°†å‚æ•°åˆ†æˆå¤šä¸ªæ¡¶ï¼Œå¦‚æˆ‘åˆšæ‰æåˆ°çš„ï¼Œå®ƒå‡è®¾åå‘ä¼ æ’­çš„é¡ºåºæ˜¯æ¨¡å‹å‚æ•°çš„åå‘é¡ºåºã€‚å½“å®ƒæ„å»ºè¿™äº›æ¡¶æ—¶ï¼Œå¦‚æœè¿™ä¸ªé¡ºåºä¸æˆç«‹ï¼Œè€Œåœ¨è®¸å¤šæ¨¡å‹ä¸­ç¡®å®æ˜¯è¿™æ ·ï¼Œé‚£ä¹ˆæ¡¶å°±ä¸ä¼šä»¥æœ€ä½³é¡ºåºæ„å»ºï¼Œç»“æœå¯èƒ½æ˜¯æ¡¶
    2 åœ¨æ¡¶ 1 ä¹‹å‰å‡†å¤‡å¥½ï¼Œéšåæ¡¶ 1 éœ€è¦ç­‰å¾…æ‰èƒ½è°ƒåº¦å®ƒçš„å…¨å½’çº¦ã€‚è¿™ä¼šå¯¼è‡´æ€§èƒ½ suboptimalã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒDP ç°åœ¨è®°å½•äº†ç¬¬ä¸€æ¬¡åå‘ä¼ æ’­ä¸­çš„å‚æ•°é¡ºåºï¼Œå¹¶ä»¥æœ€ä½³é¡ºåºé‡å»ºæ¡¶ï¼Œè¿™æ ·ä½ å°±å¯ä»¥æ›´ä¼˜åœ°è°ƒåº¦å…¨å½’çº¦ã€‚å› æ­¤ï¼Œåœ¨åƒ
    B å’Œ Roberta è¿™æ ·çš„æ¨¡å‹ä¸­ï¼Œæ€§èƒ½æé«˜äº†å¤§çº¦ 3% åˆ° 7%ã€‚
- en: Then finally a few miscellaneous improvements we've added better error handling
    in niel by a couple of environment variables so you can look at the documentation
    for these for more details we had like a distributed keyvalue store in CD this
    is mostly used for Rndezvous and coordination so we've mostly just formalized
    this API added some good documentation around it for users we've added Windows
    support for CTD and I'd like to thank Microsoft for this contributionã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæœ€åæ˜¯ä¸€äº›æ‚é¡¹æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨ niel ä¸­æ·»åŠ äº†æ›´å¥½çš„é”™è¯¯å¤„ç†ï¼Œé€šè¿‡å‡ ä¸ªç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ã€‚ä½ å¯ä»¥æŸ¥çœ‹æ–‡æ¡£ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚æˆ‘ä»¬åœ¨ C&D ä¸­æœ‰ä¸€ä¸ªåˆ†å¸ƒå¼é”®å€¼å­˜å‚¨ï¼Œä¸»è¦ç”¨äºä¼šé¢å’Œåè°ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸»è¦åªæ˜¯è§„èŒƒåŒ–äº†è¿™ä¸ª
    APIï¼Œå¹¶ä¸ºç”¨æˆ·æ·»åŠ äº†ä¸€äº›å¥½çš„æ–‡æ¡£ã€‚æˆ‘ä»¬ä¸º CTD æ·»åŠ äº† Windows æ”¯æŒï¼Œæˆ‘æƒ³æ„Ÿè°¢ Microsoft çš„è´¡çŒ®ã€‚
- en: So now what's coming soon in Py distributed so this is probably in the short
    term maybe like Pytch 1ã€‚8 or Pytch 1ã€‚9 we're adding point to point communication
    support and process group and C1D so this is built upon nichels point to point
    send and receive support we're adding native GPU support for the RPpC framework
    so you can send and receive GPU tensors over RPC seamlessly we're going to add
    a remote module remote device kind of API for distributed model parallelism so
    you don't have to use the RC framework directly if you want to like just place
    a module or part of a model on a different host different GPU you can just use
    remote module to kind of do that and it will be a nice highlevel APIã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œç°åœ¨åœ¨ Py åˆ†å¸ƒå¼ä¸­å³å°†æ¨å‡ºçš„å†…å®¹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿè¿™å¯èƒ½æ˜¯åœ¨çŸ­æœŸå†…ï¼Œæ¯”å¦‚ Pytch 1.8 æˆ– Pytch 1.9ï¼Œæˆ‘ä»¬å°†æ·»åŠ ç‚¹å¯¹ç‚¹é€šä¿¡æ”¯æŒå’Œè¿›ç¨‹ç»„ä»¥åŠ
    C1Dã€‚è¿™æ˜¯åŸºäº nichels çš„ç‚¹å¯¹ç‚¹å‘é€å’Œæ¥æ”¶æ”¯æŒã€‚æˆ‘ä»¬è¿˜å°†ä¸º RPpC æ¡†æ¶æ·»åŠ åŸç”Ÿ GPU æ”¯æŒï¼Œè¿™æ ·ä½ å°±å¯ä»¥æ— ç¼åœ°é€šè¿‡ RPC å‘é€å’Œæ¥æ”¶ GPU
    å¼ é‡ã€‚æˆ‘ä»¬å°†ä¸ºåˆ†å¸ƒå¼æ¨¡å‹å¹¶è¡Œæ€§æ·»åŠ ä¸€ä¸ªè¿œç¨‹æ¨¡å—å’Œè¿œç¨‹è®¾å¤‡ç±»å‹çš„ APIï¼Œå› æ­¤å¦‚æœä½ æƒ³å°†ä¸€ä¸ªæ¨¡å—æˆ–æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ”¾åœ¨ä¸åŒçš„ä¸»æœºæˆ– GPU ä¸Šï¼Œä½ åªéœ€ä½¿ç”¨è¿œç¨‹æ¨¡å—æ¥å®ç°ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªä¸é”™çš„é«˜çº§
    APIã€‚
- en: We're going to add pipeline parallelism to Pytchï¼Œ so this is a very popular
    way of training models which don't fit on a single GPUã€‚ so it' will be very useful
    for training much larger modelsã€‚And then finallyã€‚ we're going to add a C&D extension
    to support third party collective communication libraries and I like to sorryã€‚
    thank Intel for this contributionã€‚Okayï¼Œ so that was short term nowã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¸º Pytch æ·»åŠ ç®¡é“å¹¶è¡Œæ€§ï¼Œè¿™æ˜¯ä¸€ç§éå¸¸æµè¡Œçš„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œé€‚ç”¨äºä¸èƒ½åœ¨å•ä¸ª GPU ä¸Šè¿è¡Œçš„æ¨¡å‹ã€‚å› æ­¤ï¼Œè¿™å¯¹äºè®­ç»ƒæ›´å¤§æ¨¡å‹å°†éå¸¸æœ‰ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ·»åŠ 
    C&D æ‰©å±•ä»¥æ”¯æŒç¬¬ä¸‰æ–¹é›†ä½“é€šä¿¡åº“ï¼Œæˆ‘æƒ³å¯¹æ­¤è¡¨ç¤ºæ„Ÿè°¢ã€‚æ„Ÿè°¢ Intel çš„è´¡çŒ®ã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯çŸ­æœŸçš„è®¡åˆ’ã€‚
- en: if we think about more longer termï¼Œ maybe a year from now or even longer what
    we are thinking about in Pyth distributedã€‚ so we think about adding zero style
    training framework for really large model so this was like a very interesting
    paradigm that wasã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è€ƒè™‘æ›´é•¿æœŸçš„è§„åˆ’ï¼Œä¹Ÿè®¸æ˜¯ä¸€å¹´ä¹‹åæˆ–æ›´é•¿æ—¶é—´ï¼Œæˆ‘ä»¬æ­£åœ¨è€ƒè™‘åœ¨ Pyth åˆ†å¸ƒå¼ä¸­æ·»åŠ é›¶æ ·å¼è®­ç»ƒæ¡†æ¶ï¼Œä»¥æ”¯æŒçœŸæ­£çš„å¤§å‹æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„èŒƒå¼ã€‚
- en: That was proposed by Microsoft and we are trying to incorporate that into Py
    distributed then were planning to add intralaopalism so this is a very interesting
    technique that was used by megatron from NviDdia to kind of train large transformer
    models then we're also planning to add like tocr support for C1D APIsã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”±å¾®è½¯æè®®çš„ï¼Œæˆ‘ä»¬æ­£åœ¨å°è¯•å°†å…¶çº³å…¥Pyåˆ†å¸ƒå¼ï¼Œç„¶åè®¡åˆ’æ·»åŠ å†…éƒ¨å¹¶è¡Œæ€§ï¼Œå› æ­¤è¿™æ˜¯ä¸€ç§éå¸¸æœ‰è¶£çš„æŠ€æœ¯ï¼Œæ›¾è¢«NviDdiaçš„Megatronç”¨äºè®­ç»ƒå¤§å‹å˜æ¢å™¨æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜è®¡åˆ’ä¸ºC1D
    APIæ·»åŠ ç±»ä¼¼çš„æ”¯æŒã€‚
- en: So today if you have like a very complex model with some collective communication
    within the model you can't really tocr that model because this is not possible
    for C1D API so we're planning to add support for that then we have like auto tuunning
    for DDP so DDP has many parameters that need to be tuned manually today for example
    like the bucket size for the buckets that I talked about so we're planning to
    add some auto tuning where users don't have to tune DP for the particular environmentsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»Šå¤©å¦‚æœä½ æœ‰ä¸€ä¸ªéå¸¸å¤æ‚çš„æ¨¡å‹ï¼Œæ¨¡å‹å†…éƒ¨æœ‰ä¸€äº›é›†ä½“é€šä¿¡ï¼Œä½ æ— æ³•çœŸæ­£å¯¹è¯¥æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œå› ä¸ºè¿™å¯¹äºC1D APIæ¥è¯´æ˜¯ä¸å¯èƒ½çš„ï¼Œå› æ­¤æˆ‘ä»¬è®¡åˆ’æ·»åŠ å¯¹è¿™ä¸€ç‚¹çš„æ”¯æŒï¼Œæ­¤å¤–æˆ‘ä»¬è¿˜è®¡åˆ’ä¸ºDDPæ·»åŠ è‡ªåŠ¨è°ƒä¼˜ï¼Œå› ä¸ºDDPæœ‰è®¸å¤šå‚æ•°éœ€è¦æ‰‹åŠ¨è°ƒä¼˜ï¼Œä¾‹å¦‚æˆ‘æåˆ°çš„æ¡¶çš„å¤§å°ï¼Œå› æ­¤æˆ‘ä»¬è®¡åˆ’æ·»åŠ ä¸€äº›è‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½ï¼Œç”¨æˆ·ä¸å¿…ä¸ºç‰¹å®šç¯å¢ƒè°ƒä¼˜DPã€‚
- en: Then we have an idea called hybrid parallelism where we're planning to make
    sure that things like pipeline parallelismã€‚ modismï¼Œ data parallelismï¼Œ and things
    like even intralaopalism work together seamlessly so users can kind of mix and
    match and figure out what's the best training paradigm for themã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªå«åšæ··åˆå¹¶è¡Œæ€§çš„æƒ³æ³•ï¼Œæˆ‘ä»¬è®¡åˆ’ç¡®ä¿åƒç®¡é“å¹¶è¡Œæ€§ã€æ¨¡å‹å¹¶è¡Œæ€§ã€æ•°æ®å¹¶è¡Œæ€§ä»¥åŠå†…éƒ¨å¹¶è¡Œæ€§èƒ½å¤Ÿæ— ç¼åä½œï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥çµæ´»ç»„åˆï¼Œæ‰¾å‡ºæœ€é€‚åˆä»–ä»¬çš„è®­ç»ƒèŒƒå¼ã€‚
- en: And then the next step to that is like once you have hybrid parallelismã€‚ can
    we kind of automate this completely in terms of like the user just gives us a
    model and their training resources and then we figure out what combination of
    hybrid parallelism kind of works best for this model and the user doesn't have
    to worry about thisã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä¸€æ—¦ä½ æœ‰äº†æ··åˆå¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬æ˜¯å¦å¯ä»¥å®Œå…¨è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œç”¨æˆ·åªéœ€æä¾›æ¨¡å‹å’Œè®­ç»ƒèµ„æºï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥æ‰¾å‡ºæœ€é€‚åˆè¯¥æ¨¡å‹çš„æ··åˆå¹¶è¡Œæ€§ç»„åˆï¼Œç”¨æˆ·æ— éœ€æ‹…å¿ƒã€‚
- en: '![](img/8bb021252445710b64cf368008f50dc4_9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb021252445710b64cf368008f50dc4_9.png)'
- en: Finallyï¼Œ I would like to share this page of like distributed overview on Pythã€‚
    so it's a place where you have all the information for Pyths distributedã€‚ I'll
    definitely recommend that you check this outã€‚![](img/8bb021252445710b64cf368008f50dc4_11.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘æƒ³åˆ†äº«è¿™é¡µå…³äºPythåˆ†å¸ƒå¼æ¦‚è¿°çš„é¡µé¢ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«Pythåˆ†å¸ƒå¼æ‰€æœ‰ä¿¡æ¯çš„åœ°æ–¹ã€‚æˆ‘ä¸€å®šæ¨èä½ å»æŸ¥çœ‹ä¸€ä¸‹ã€‚![](img/8bb021252445710b64cf368008f50dc4_11.png)
- en: ğŸ¼That's all I hadã€‚ Thank you very muchï¼Œ and thank you for watchingã€‚![](img/8bb021252445710b64cf368008f50dc4_13.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼è¿™å°±æ˜¯æˆ‘æ‰€è¦è¯´çš„ã€‚éå¸¸æ„Ÿè°¢ä½ ä»¬çš„è§‚çœ‹ï¼![](img/8bb021252445710b64cf368008f50dc4_13.png)
