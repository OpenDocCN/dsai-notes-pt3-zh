- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P11ï¼šL11-
    PyTorch æ€§èƒ½ - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½ Facebook å·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ -
    P11ï¼šL11- PyTorch æ€§èƒ½ - ShowMeAI - BV1ZZ4y1U7dg
- en: ğŸ¼ã€‚![](img/323adbff4029fa414d7811534950c65f_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ã€‚![](img/323adbff4029fa414d7811534950c65f_1.png)
- en: Helloï¼Œ I'm Naal Gahan and I'm an applied research scientist at Facebook and
    I work on Pytor performanceã€‚Today I will talk about low precision support and
    new obstructions for performance that we've implemented in Pyrch and operator
    benchmarkingã€‚So low precision supportã€‚ğŸ˜Šã€‚![](img/323adbff4029fa414d7811534950c65f_3.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¥½ï¼Œæˆ‘æ˜¯ Naal Gahanï¼Œæ˜¯ Facebook çš„åº”ç”¨ç ”ç©¶ç§‘å­¦å®¶ï¼Œæˆ‘åœ¨ Pytorch æ€§èƒ½æ–¹é¢å·¥ä½œã€‚ä»Šå¤©æˆ‘å°†è®¨è®ºä½ç²¾åº¦æ”¯æŒå’Œæˆ‘ä»¬åœ¨ Pytorch
    ä¸­å®æ–½çš„æ–°æ€§èƒ½æ”¹è¿›åŠæ“ä½œåŸºå‡†æµ‹è¯•ã€‚å› æ­¤ï¼Œä½ç²¾åº¦æ”¯æŒã€‚ğŸ˜Šã€‚![](img/323adbff4029fa414d7811534950c65f_3.png)
- en: On modern hardware you can sacrifice a little bit of precision to achieve a
    substantially better performance Pytorch makes working with low precision data
    types easy and convenient with support quantization but that's covered elsewhere
    I will mostly be talking about T32 and physics6 data types and before 16 is undirected
    development for CPU and GPU and it is already supported on PytoroticsLã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°ä»£ç¡¬ä»¶ä¸Šï¼Œä½ å¯ä»¥ç‰ºç‰²ä¸€äº›ç²¾åº¦ï¼Œä»¥è·å¾—æ˜¾è‘—æ›´å¥½çš„æ€§èƒ½ã€‚Pytorch ä½¿å¾—å¤„ç†ä½ç²¾åº¦æ•°æ®ç±»å‹å˜å¾—ç®€å•æ–¹ä¾¿ï¼Œå¹¶æ”¯æŒé‡åŒ–ï¼Œä½†è¿™åœ¨å…¶ä»–åœ°æ–¹æœ‰è¯¦ç»†ä»‹ç»ã€‚æˆ‘å°†ä¸»è¦è®¨è®º
    T32 å’Œ FP16 æ•°æ®ç±»å‹ï¼Œè€Œ FP16 åœ¨ CPU å’Œ GPU çš„æ— æ–¹å‘å¼€å‘ä¸Šå·²ç»å¾—åˆ°äº†æ”¯æŒã€‚
- en: The diagrams on the right show the memory representation of the correspondingent
    data types green boxes are exponent bitsã€‚ red boxes are monicip bitsã€‚ğŸ˜Šï¼ŒYou can
    see that before 16 can represent a wide dynamic rangee with eight exponent bitsã€‚
    it's the same dynamic rangee as for standard FP32 data typeã€‚ however it only has
    a limited precision with 7 minusisab bits FP16 is making an opposite tradeoff
    and it has a limited dynamic rangee with a better precision T32 is the best of
    both worldsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä¾§çš„å›¾è¡¨æ˜¾ç¤ºäº†ç›¸åº”æ•°æ®ç±»å‹çš„å†…å­˜è¡¨ç¤ºï¼Œç»¿è‰²æ¡†æ˜¯æŒ‡æ•°ä½ï¼Œçº¢è‰²æ¡†æ˜¯æœ‰æ•ˆä½ã€‚ğŸ˜Šä½ å¯ä»¥çœ‹åˆ° FP16 å¯ä»¥ç”¨å…«ä¸ªæŒ‡æ•°ä½è¡¨ç¤ºå¹¿æ³›çš„åŠ¨æ€èŒƒå›´ï¼Œå…¶åŠ¨æ€èŒƒå›´ä¸æ ‡å‡† FP32
    æ•°æ®ç±»å‹ç›¸åŒï¼Œä½†åªæœ‰ 7 ä¸ªæœ‰æ•ˆä½çš„æœ‰é™ç²¾åº¦ã€‚FP16 åˆ™ä½œå‡ºäº†ç›¸åçš„æƒè¡¡ï¼Œå…·æœ‰æœ‰é™çš„åŠ¨æ€èŒƒå›´å’Œæ›´å¥½çš„ç²¾åº¦ï¼Œè€Œ T32 åˆ™å…¼å…·ä¸¤è€…çš„ä¼˜ç‚¹ã€‚
- en: its dynamic rangee is the same as the standard FP32 data typeã€‚ but it has as
    many monisab bits as a P16 doesã€‚T32 is enabled in a byytor on new MPR GPUs It
    is backed by 32 bit storage so standard networks can transparently benefit from
    it when computationally intensive operations read input data they read only 10
    bits of mons so there is some acurcisã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçš„åŠ¨æ€èŒƒå›´ä¸æ ‡å‡† FP32 æ•°æ®ç±»å‹ç›¸åŒï¼Œä½†ä¸ FP16 ä¸€æ ·æœ‰è®¸å¤šæœ‰æ•ˆä½ã€‚T32 åœ¨æ–°çš„ MPR GPU ä¸Šå¯ç”¨ï¼Œæ”¯æŒ 32 ä½å­˜å‚¨ï¼Œå› æ­¤æ ‡å‡†ç½‘ç»œåœ¨è®¡ç®—å¯†é›†å‹æ“ä½œè¯»å–è¾“å…¥æ•°æ®æ—¶å¯ä»¥é€æ˜åœ°å—ç›Šï¼Œåªè¯»å–
    10 ä½æœ‰æ•ˆä½ï¼Œå› æ­¤å­˜åœ¨ä¸€äº›ç²¾åº¦é—®é¢˜ã€‚
- en: however internal accumulation happens in a P32 so we don't expect any effect
    on conversionsã€‚You can turn T32 on and off to see its effects on performance and
    conversionvergence using the commentss below with benchmark in approximately 3x
    speed up on Hi face Roberta model and approximately2 x speed up on ster conves
    such as Resn that's compared to FP32 performance on the same hardwareã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå†…éƒ¨ç´¯ç§¯å‘ç”Ÿåœ¨ FP32 ä¸­ï¼Œå› æ­¤æˆ‘ä»¬ä¸æœŸæœ›è½¬æ¢ä¼šå—åˆ°å½±å“ã€‚ä½ å¯ä»¥å¼€å¯å’Œå…³é—­ T32ï¼Œä»¥è§‚å¯Ÿå…¶å¯¹æ€§èƒ½å’Œè½¬æ¢çš„å½±å“ï¼Œä½¿ç”¨ä¸‹é¢çš„è¯„è®ºä¸åŸºå‡†æµ‹è¯•ï¼ŒHi
    face Roberta æ¨¡å‹å¤§çº¦æé«˜äº† 3 å€çš„é€Ÿåº¦ï¼Œè€Œåœ¨ ResNet ç­‰è¾ƒä½å¤æ‚åº¦çš„æ¨¡å‹ä¸­åˆ™æé«˜äº†å¤§çº¦ 2 å€ï¼Œç›¸æ¯”äºåŒä¸€ç¡¬ä»¶ä¸Šçš„ FP32 æ€§èƒ½ã€‚
- en: '![](img/323adbff4029fa414d7811534950c65f_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/323adbff4029fa414d7811534950c65f_5.png)'
- en: TF32 works out of the box and doesn't require changes to existing scripts and
    that's great however if you are willing to change your existing script a little
    bit to achieve even a better performanceã€‚ FP16 can be usefulã€‚Automatic mixed precision
    is now supported in Pythr and this feature has evolved from popular ABIX package
    maintained by NviDã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TF32 å¼€ç®±å³ç”¨ï¼Œæ— éœ€å¯¹ç°æœ‰è„šæœ¬è¿›è¡Œä¿®æ”¹ï¼Œè¿™éå¸¸å¥½ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ„¿æ„ç¨å¾®è°ƒæ•´ç°æœ‰è„šæœ¬ï¼Œä»¥å®ç°æ›´å¥½çš„æ€§èƒ½ï¼ŒFP16 å°†ä¼šå¾ˆæœ‰ç”¨ã€‚è‡ªåŠ¨æ··åˆç²¾åº¦ç°åœ¨åœ¨ Pytorch
    ä¸­å¾—åˆ°æ”¯æŒï¼Œè¯¥åŠŸèƒ½æºè‡ªç”± NVIDIA ç»´æŠ¤çš„æµè¡Œ ABIX åŒ…ã€‚
- en: ğŸ¤¢ï¼ŒIt automates training of networks in FP16 and takes care of numerical issues
    that arise in a P16 training due to its limited dynamic rangeã€‚InPyrche was designed
    to cover a wide range of use casesã€‚It supports operations on unskilled gradientsï¼Œ
    it supports operations on Sprse gradientsã€‚ higher auto gradients can be computed
    via torsche autogradï¼Œ custom autograd functionsã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤¢å®ƒè‡ªåŠ¨åŒ–è®­ç»ƒ FP16 ç½‘ç»œï¼Œå¹¶å¤„ç†ç”±äº FP16 è®­ç»ƒä¸­çš„æœ‰é™åŠ¨æ€èŒƒå›´è€Œäº§ç”Ÿçš„æ•°å€¼é—®é¢˜ã€‚Pytorch æ—¨åœ¨è¦†ç›–å¹¿æ³›çš„ç”¨ä¾‹ï¼Œæ”¯æŒå¯¹æ— åæ¢¯åº¦çš„æ“ä½œï¼Œæ”¯æŒå¯¹ç¨€ç–æ¢¯åº¦çš„æ“ä½œã€‚æ›´é«˜é˜¶çš„è‡ªåŠ¨æ¢¯åº¦å¯ä»¥é€šè¿‡
    Torch çš„ autograd è®¡ç®—ï¼Œæ”¯æŒè‡ªå®šä¹‰ autograd å‡½æ•°ã€‚
- en: both Python and C+ path are supported if you have complicated scripts involving
    multiple models and multiple losses that is supported toã€‚Andter example and documentation
    are available on Pytor's websiteã€‚![](img/323adbff4029fa414d7811534950c65f_7.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple example of using ampã€‚ There are two important partsã€‚ The first
    one is scalar object that controls all scaling and the second one is M Autotocast
    context manager all scaling is required to ensure conversions and numerical stability
    and Autocast context manager make sure that mods and convols are run in FP16 thus
    achieving the best performance while operations that require full precision are
    run in FP32ã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Finallyï¼Œ there is a slightly different syntax for calling an optimizer step
    and you also have to update a low scale for the next iteration as shown in this
    snippetã€‚ but overall the code changes required are pretty minimal and you should
    be able to achieve an even better performance than you can with TF32 Also if P16
    is supported on all the regens of GPUs such as Volta enteringã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/323adbff4029fa414d7811534950c65f_9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Now let's move on to the next topic and that's channels lost in many backends
    and for many data types such as for exampleã€‚ T32 and FP16 that we've just discussedï¼Œ
    convolutions perform best when data is in channels lost format Pyr supports channels
    lost physical memory format while preserving traditional semantic meaning of the
    dimensions so for example for four details our first dimension is still number
    of batch elements second dimension is channels and the rest are spatial dimensionsã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: To fully take advantage of channel SA supportï¼Œ all or most operations in the
    model have to support channel SA and most operations in Pytorrch doã€‚ for exampleï¼Œ
    a popular model in TorchViion are coveredã€‚![](img/323adbff4029fa414d7811534950c65f_11.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Here we have a small example that shows how to convert model to channels Los
    format by calling a helper functionã€‚ the input has to be in channels Los format
    alsoï¼Œ so you have to either modify your data load to directly provide input in
    channels lost or call a conversion function on input manually December in the
    scriptã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Input layout is propagated across most operationsï¼Œ so intermediate variables
    in the network will remain channel fastã€‚Copy and Tensor factory operations also
    preserve layout as shown here on the example ones like operationã€‚ similar like
    tensor factories will also preserve the layout of their inputs pointwise operations
    always preserve the layout of their inputs and copy operations also produce the
    outputs in the same format as the input was for Convnets you can expect about
    20% gain from switching to channel costã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/323adbff4029fa414d7811534950c65f_13.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Another interesting feature that we implemented in Pythr forage APIs that provide
    efficient pointwise operations on batches of tensorsã€‚Instead of launching a small
    kernel for each tensor in the batchã€‚ for each launches a few larger kernelsï¼Œ each
    processing many tensorsã€‚This pattern is especially common and useful for optimizersã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨Pythr forage APIsä¸­å®ç°çš„å¦ä¸€ä¸ªæœ‰è¶£ç‰¹æ€§æ˜¯æä¾›äº†å¯¹æ‰¹é‡å¼ é‡çš„é«˜æ•ˆé€ç‚¹æ“ä½œã€‚ä¸å…¶ä¸ºæ‰¹é‡ä¸­çš„æ¯ä¸ªå¼ é‡å¯åŠ¨ä¸€ä¸ªå°å†…æ ¸ï¼Œä¸å¦‚ä¸ºæ¯ä¸ªå¼ é‡å¯åŠ¨å‡ ä¸ªè¾ƒå¤§çš„å†…æ ¸ï¼Œæ¯ä¸ªå†…æ ¸å¤„ç†å¤šä¸ªå¼ é‡ã€‚è¿™ç§æ¨¡å¼åœ¨ä¼˜åŒ–å™¨ä¸­å°¤å…¶å¸¸è§ä¸”æœ‰ç”¨ã€‚
- en: Even now you can avoid launching multiple kernels if you copy the tensors to
    a contiguous memory region and then operate directly on this contiguous memory
    regionã€‚ but this requires extra memory and can be brittle if different subsets
    of tensors participate in operations for each operates on tensous in disjoint
    memory regions directly and lists can be cheaply assembled before each applicationã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ç°åœ¨ï¼Œå¦‚æœæ‚¨å°†å¼ é‡å¤åˆ¶åˆ°è¿ç»­çš„å†…å­˜åŒºåŸŸï¼Œç„¶åç›´æ¥åœ¨è¯¥è¿ç»­å†…å­˜åŒºåŸŸä¸Šè¿›è¡Œæ“ä½œï¼Œæ‚¨ä¹Ÿå¯ä»¥é¿å…å¯åŠ¨å¤šä¸ªå†…æ ¸ã€‚ä½†è¿™éœ€è¦é¢å¤–çš„å†…å­˜ï¼Œå¹¶ä¸”å¦‚æœæ¯æ¬¡æ“ä½œçš„å¼ é‡å­é›†ä¸åŒï¼Œå¯èƒ½ä¼šå˜å¾—è„†å¼±ï¼Œå› ä¸ºæ¯ä¸ªæ“ä½œç›´æ¥åœ¨ä¸ç›¸äº¤çš„å†…å­˜åŒºåŸŸä¸Šè¿›è¡Œï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ¯æ¬¡åº”ç”¨å‰å»‰ä»·åœ°ç»„è£…åˆ—è¡¨ã€‚
- en: '![](img/323adbff4029fa414d7811534950c65f_15.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/323adbff4029fa414d7811534950c65f_15.png)'
- en: Here are the timelines that compare operating on batches of tensors in a loop
    and forageã€‚The GPU timeline for a loop is shown in the bottom rowã€‚The you can
    see that GPU here is idle most of the timeï¼Œ they are very short kernelã€‚ but mostly
    it's space between kernels CPUU is busy constantly launching those small kernelsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æ¯”è¾ƒåœ¨å¾ªç¯ä¸­å¯¹å¼ é‡æ‰¹æ¬¡å’Œforageæ“ä½œçš„æ—¶é—´çº¿ã€‚å¾ªç¯çš„GPUæ—¶é—´çº¿æ˜¾ç¤ºåœ¨åº•éƒ¨è¡Œã€‚æ‚¨å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„GPUå¤§éƒ¨åˆ†æ—¶é—´å¤„äºé—²ç½®çŠ¶æ€ï¼Œå†…æ ¸éå¸¸çŸ­ï¼Œä½†å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯åœ¨å†…æ ¸ä¹‹é—´çš„ç©ºé—²æ—¶é—´ã€‚CPUåˆ™ä¸æ–­å¿™äºå¯åŠ¨è¿™äº›å°å†…æ ¸ã€‚
- en: In contrast when we are doing the same operation using4H APIsã€‚ GPU is always
    busy and it's running a relatively larger kernels CPUU is busy in the beginning
    submitting those kernels but then it idle for the rest of the time while the upper
    timeline shows just a handful of tensors processed in the lower timeline a few
    hundred tenors were processed in the same amount of timeã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨4H APIsè¿›è¡Œç›¸åŒçš„æ“ä½œæ—¶ï¼ŒGPUæ€»æ˜¯å¤„äºå¿™ç¢ŒçŠ¶æ€ï¼Œå¹¶è¿è¡Œç›¸å¯¹è¾ƒå¤§çš„å†…æ ¸ã€‚CPUä¸€å¼€å§‹åœ¨æäº¤è¿™äº›å†…æ ¸æ—¶å¿™ç¢Œï¼Œä½†éšååœ¨å…¶ä½™æ—¶é—´å†…å¤„äºé—²ç½®çŠ¶æ€ï¼Œè€Œä¸Šæ–¹çš„æ—¶é—´çº¿ä»…æ˜¾ç¤ºå¤„ç†çš„å°‘é‡å¼ é‡ï¼Œåœ¨ä¸‹æ–¹çš„æ—¶é—´çº¿ä¸­ï¼Œåœ¨ç›¸åŒçš„æ—¶é—´å†…å¤„ç†äº†å‡ ç™¾ä¸ªå¼ é‡ã€‚
- en: '![](img/323adbff4029fa414d7811534950c65f_17.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/323adbff4029fa414d7811534950c65f_17.png)'
- en: Poche 107 has common optimizers implemented using forage APIs for and they achieve
    approximately from3 to 6x or even larger speedup depending on the number of parameters
    in your networkã€‚ forage is also easy to use to implement your own optimizers or
    if you have the patterns in your network that require operating on the irregular
    batches of tensorsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Poche 107ä½¿ç”¨forage APIså®ç°äº†é€šç”¨ä¼˜åŒ–å™¨ï¼Œèƒ½å¤Ÿå®ç°çº¦ä»3åˆ°6å€ç”šè‡³æ›´å¤§çš„åŠ é€Ÿï¼Œå…·ä½“å–å†³äºç½‘ç»œä¸­çš„å‚æ•°æ•°é‡ã€‚forageä¹Ÿæ˜“äºä½¿ç”¨ï¼Œå¯ä»¥å®ç°æ‚¨è‡ªå·±çš„ä¼˜åŒ–å™¨ï¼Œæˆ–è€…å¦‚æœæ‚¨ç½‘ç»œä¸­çš„æ¨¡å¼éœ€è¦å¯¹ä¸è§„åˆ™æ‰¹æ¬¡çš„å¼ é‡è¿›è¡Œæ“ä½œã€‚
- en: So give it a tryï¼Œ replace your optimizer with forage basedï¼Œ and see if it improves
    the performanceã€‚Finallyï¼Œ let's talk about benchmarking utilitiesã€‚Pytorch benchmarking
    utilities are aimed at Pytorarch users and developersã€‚ Of courseï¼Œ you can roll
    your own benchmarking utilitiesã€‚ but it requires taking care of a few problemsã€‚
    You want benchmarks to run long enough to get reliable time and measurementsã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¯•è¯•çœ‹ï¼Œå°†æ‚¨çš„ä¼˜åŒ–å™¨æ›¿æ¢ä¸ºåŸºäºforageçš„ï¼Œå¹¶æŸ¥çœ‹å®ƒæ˜¯å¦èƒ½æ”¹å–„æ€§èƒ½ã€‚æœ€åï¼Œè®©æˆ‘ä»¬è°ˆè°ˆåŸºå‡†æµ‹è¯•å·¥å…·ã€‚PytorchåŸºå‡†æµ‹è¯•å·¥å…·æ˜¯é’ˆå¯¹Pytorarchç”¨æˆ·å’Œå¼€å‘è€…çš„ã€‚å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥è‡ªå·±å¼€å‘åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œä½†è¿™éœ€è¦å¤„ç†ä¸€äº›é—®é¢˜ã€‚æ‚¨å¸Œæœ›åŸºå‡†æµ‹è¯•è¿è¡Œè¶³å¤Ÿé•¿çš„æ—¶é—´ï¼Œä»¥è·å¾—å¯é çš„æ—¶é—´å’Œæµ‹é‡ç»“æœã€‚
- en: but you don't want them to run forever you need to collect statistics to estimate
    noise in the measurementsã€‚ you need to make sure that you are comparing apples
    to apples that all synchronizations are called and multith on the CPU is properly
    controlledã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ‚¨ä¸å¸Œæœ›å®ƒä»¬æ°¸è¿œè¿è¡Œï¼Œæ‚¨éœ€è¦æ”¶é›†ç»Ÿè®¡æ•°æ®ä»¥ä¼°è®¡æµ‹é‡ä¸­çš„å™ªå£°ã€‚æ‚¨éœ€è¦ç¡®ä¿æ‚¨æ¯”è¾ƒçš„æ˜¯åŒç±»é¡¹ï¼Œæ‰€æœ‰åŒæ­¥è°ƒç”¨éƒ½å·²æ‰§è¡Œï¼Œä¸”CPUä¸Šçš„å¤šçº¿ç¨‹å¾—åˆ°äº†é€‚å½“æ§åˆ¶ã€‚
- en: If you are a developer working on a new operation for Pytorarch or if you are
    working on an existing operationã€‚ you want to make sure that the performance is
    good for a variety of input sizes and not just for a particular input size resulting
    in hyper optimization After all your benchmarking is done you are likely left
    with a wall of numbers that is hard to analyze and represent so you need some
    way of post processing the data Our benchmarking utilities make all of these things
    easy Thisnippet shows how to use timer and compare Apis to compare performanceã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ˜¯ä¸€åå¼€å‘è€…ï¼Œæ­£åœ¨ä¸º Pytorach å¼€å‘æ–°æ“ä½œï¼Œæˆ–è€…åœ¨å¤„ç†ç°æœ‰æ“ä½œï¼Œä½ éœ€è¦ç¡®ä¿åœ¨å„ç§è¾“å…¥å¤§å°ä¸‹æ€§èƒ½è‰¯å¥½ï¼Œè€Œä¸ä»…ä»…æ˜¯é’ˆå¯¹æŸä¸€ç‰¹å®šè¾“å…¥å¤§å°è¿›è¡Œè¶…ä¼˜åŒ–ã€‚å®Œæˆæ‰€æœ‰åŸºå‡†æµ‹è¯•åï¼Œä½ å¯èƒ½ä¼šé¢å¯¹ä¸€å¤§å †éš¾ä»¥åˆ†æå’Œè¡¨ç¤ºçš„æ•°å­—ï¼Œå› æ­¤ä½ éœ€è¦ä¸€äº›åå¤„ç†æ•°æ®çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å·¥å…·ä½¿æ‰€æœ‰è¿™äº›äº‹æƒ…å˜å¾—ç®€å•ã€‚è¿™ä¸ªä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è®¡æ—¶å™¨å’Œæ¯”è¾ƒ
    API æ¥æ¯”è¾ƒæ€§èƒ½ã€‚
- en: '![](img/323adbff4029fa414d7811534950c65f_19.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/323adbff4029fa414d7811534950c65f_19.png)'
- en: Of two similar pytharch operations take and gather on the different data types
    timer APIs are modeled on Python timesã€‚ so they should feel familiarã€‚ They also
    have some additional options to include metadata to make subsequent analysis a
    little bit easier and the left part of this slide shows the output of this script
    where the times are shown in tabular formã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸¤ç§ç›¸ä¼¼çš„ pytharch æ“ä½œï¼Œå¦‚ take å’Œ gatherï¼Œé’ˆå¯¹ä¸åŒæ•°æ®ç±»å‹çš„è®¡æ—¶å™¨ API æ˜¯åŸºäº Python çš„æ—¶é—´æ¨¡å‹ï¼Œå› æ­¤å®ƒä»¬åº”è¯¥æ˜¾å¾—å¾ˆç†Ÿæ‚‰ã€‚å®ƒä»¬è¿˜æä¾›äº†ä¸€äº›é™„åŠ é€‰é¡¹ï¼Œå¯ä»¥åŒ…å«å…ƒæ•°æ®ï¼Œä»¥ä¾¿åç»­åˆ†ææ›´ä¸ºç®€ä¾¿ï¼Œå¹»ç¯ç‰‡å·¦ä¾§æ˜¾ç¤ºäº†è¯¥è„šæœ¬çš„è¾“å‡ºï¼Œæ—¶é—´ä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºã€‚
- en: We hope that you'll find our benchmarking utilities convenientã€‚![](img/323adbff4029fa414d7811534950c65f_21.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä½ ä¼šè§‰å¾—æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å·¥å…·å¾ˆæ–¹ä¾¿ã€‚![](img/323adbff4029fa414d7811534950c65f_21.png)
- en: Here are resources that you can use to get more information about the topics
    I've talked aboutã€‚![](img/323adbff4029fa414d7811534950c65f_23.png)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€äº›èµ„æºï¼Œä½ å¯ä»¥ç”¨æ¥è·å–æ›´å¤šå…³äºæˆ‘æåˆ°çš„è¯é¢˜çš„ä¿¡æ¯ã€‚![](img/323adbff4029fa414d7811534950c65f_23.png)
- en: Thank you for listening and hope to see you at Pyr's performance discussionã€‚![](img/323adbff4029fa414d7811534950c65f_25.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ çš„è†å¬ï¼Œå¸Œæœ›èƒ½åœ¨ Pyr çš„æ€§èƒ½è®¨è®ºä¸­è§åˆ°ä½ ã€‚![](img/323adbff4029fa414d7811534950c65f_25.png)
