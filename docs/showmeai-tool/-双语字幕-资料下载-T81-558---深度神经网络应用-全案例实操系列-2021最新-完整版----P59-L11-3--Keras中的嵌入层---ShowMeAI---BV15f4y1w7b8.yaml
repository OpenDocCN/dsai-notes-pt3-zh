- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P59ÔºöL11.3- Keras‰∏≠ÁöÑÂµåÂÖ•Â±Ç -
    ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/6c6a4e57e3618c11a532abc5118438e7_0.png)'
  prefs: []
  type: TYPE_IMG
- en: HiÔºå this is Jeff Heaton„ÄÇ Wel to applications of deep neural networks with Washington
    University„ÄÇ So what are Kira's embedding layers„ÄÇ This is another layer type that
    you can use in Kias„ÄÇ But what do they doÔºå They're used with natural language processing
    for the latest on my AI course and projects„ÄÇ click subscribe in the bell next
    to it to be notified of every new video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Kis provides something called an embedding layer„ÄÇ These are very often used
    with natural language processing in Kiras„ÄÇ HoweverÔºå they don't really have to
    be used just with NLP„ÄÇ really„ÄÇ how I think of an embedding layer is almost an
    alternative to one hot encoding with one hot encoding you or dummy variables or
    whatever you want to call that where you take a categorical value So say you have
    100 different possibilities for that categorical value Now you need a way to encode
    that into„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: say dummy variables„ÄÇ You're going to have 100 dummy variables„ÄÇ That gets impractical
    if you deal with extremely large cardinalities for„ÄÇüòä„ÄÇ![](img/6c6a4e57e3618c11a532abc5118438e7_2.png)
  prefs: []
  type: TYPE_NORMAL
- en: Those categoricalsÔºå particularlyicular if you're dealing with words in the English
    language„ÄÇ Think about how you would dummy and code just English words„ÄÇ You would
    need one dummy variable for every English word that you had„ÄÇ Now„ÄÇ some of the
    options in spacey could be useful for that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You could turn words into their stem words like having could be transformed
    into have that way you just have one of those to deal with a lot of the verbs
    you could transform like bra„ÄÇ you could always have a run„ÄÇ So that way you don't
    have to brought could always be bring or bringing also going to the reword of
    brought„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But those are those are some of the things you can do to get that dimension
    down„ÄÇ But the embedding layerÔºå you can actually learn an embedding layer for your
    words or whatever vocabulary or categorical„ÄÇ you want to send it towards„ÄÇ NowÔºå
    this is most often used on sequences„ÄÇ The type that we would send into an LsTM
    or a temporal convolution neural networkÔºå but„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That also does not have to be the case„ÄÇ So let's look at a simple embedding
    layer„ÄÇ Now„ÄÇ we're going to see that when we create an embedding layer like here„ÄÇ
    And by the way„ÄÇ you can see another example of this back in our image captioning
    example„ÄÇ we made use of embedding and we loaded the glove embedding layer directly
    into it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but we didn't talk a lot about what the embedding layer actually did„ÄÇ It's all
    exactly like what you're going to see here„ÄÇ So now we're learning how to actually
    even train an embedding layer„ÄÇ So here we've defined this embedding layer„ÄÇ and
    our input dimension count is going to be 10„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So input dimension„ÄÇ that's essentially how many categories or how many words„ÄÇ
    what's your vocabulary size„ÄÇ If you were using one hot encoding„ÄÇ you would have
    ended up with 10 dummy variables here„ÄÇ However„ÄÇ we're going to sort of dimension
    reduce this a little bitÔºå not really a dimension reduction„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but we're going to encode these into four number of vectors rather than the
    10 number vector that a dummy„ÄÇNormally have and it's not zeros and ones„ÄÇ All of
    these four elements are going to be used in that vector„ÄÇ NowÔºå the input link„ÄÇ
    This is kind of interesting„ÄÇ This is essentially your sequence link„ÄÇ If you're
    dealing with natural language processing„ÄÇ So in this case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're just going to have two of these because this is a reallyÔºå really simple
    example„ÄÇ and you'll notice this neural network„ÄÇ and I use the term loosely only
    has one embedding layer„ÄÇ So this neural network is going to essentially just kick
    out the embedding directly to the output layer and you'll see it„ÄÇ I'm saying atom
    and mean square errorÔºå but that really doesn't matter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're not going to train this very simple neural network that we're creating„ÄÇ
    I'll go ahead and run this„ÄÇ and it doesn't really do anything other than define
    this model„ÄÇ Now you should really think of the embedding layer as a lookup table„ÄÇ
    So we've got these 10 input dimensions„ÄÇ And each of those 10 categorical values
    that you're going to pass in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: each one of those will return a different unique„ÄÇSet of four numbers from the
    output dimensions„ÄÇ So this lookup tableÔºå you can really think of it as 10 rows
    and four columns„ÄÇ It's a lookup table„ÄÇ That is all an embedding layer is is a
    lookup table„ÄÇ We're going to go ahead and now run this„ÄÇ We're going to give it
    some input data„ÄÇ The input data is just going to be a little sequence here of
    one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 2Ôºå1 and two are both well within that input rangeÔºå and it is going to change
    these two input categoricals„ÄÇ these two integers„ÄÇ The input into these as always
    integer„ÄÇ So you're transforming your characters or your words„ÄÇ and this is most
    often used for words Le often for characters„ÄÇ So that doesn't have to be the case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You transform you always provide integers„ÄÇ because they're basically lookups„ÄÇ
    These are essentially the rows in that weight matrix That is the embedding layer„ÄÇ
    And then we're going to request to predict this„ÄÇ and we're going to print out
    the shape of the input data and also the prediction that came back„ÄÇ The lookup
    table„ÄÇ You might have expected that to be all zero„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Because we never defined a lookup table„ÄÇ We never trained this neural network„ÄÇ
    So where are these numbers coming fromÔºå They're random initializationsÔºå essentially„ÄÇ
    So they're like the random weights that all layers of a neural network have„ÄÇ This
    doesn't really make a lot of sense until you actually look at the embedding weights„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if we look at the embedding weights„ÄÇ notice there's 10 rows and four columns
    So these are the 10 vocabulary elements And then we we just requested there be
    four of these that four is arbitrary we could have made that six or8 or 102 wouldn't
    really matter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It's sort of a dimension reduction conceptÔºå though though not exactly„ÄÇ now what
    we're going to do But againÔºå along the lines of why I call it a dimension reduction
    is because instead of having the 10 dummy variables you would have now you have
    these four values„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see what these weights actually mean„ÄÇ So this first one that one that
    corresponds to this very„ÄÇ very first column and this whole thing can be„ÄÇis a column
    or dimension„ÄÇ vectorÔºå notice the 0„ÄÇ4763„ÄÇ Not it is exactly the same vector as
    this one right here„ÄÇ the second one„ÄÇ This is row„ÄÇ assuming you count with 0 as
    you're starting numberÔºå0Ôºå1Ôºå This is1Ôºå2 is the next one negative 2Ôºå70Ôºå2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 7„ÄÇ Look at that„ÄÇ It's just a lookup table„ÄÇ That's all the embedding layer really
    is„ÄÇ So the glove embeddings that we used in earlier in this class„ÄÇ that is essentially
    just a table for a large number of English wordsÔºå I forget how many„ÄÇ And I think
    it had vector sizes of 200„ÄÇ if my memory serves„ÄÇ Don't quote me on that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it had some arbitrary vector length for each of those glove embeddings„ÄÇ
    We just took that matrix and loaded it right into the weights„ÄÇ We called set weights
    on it„ÄÇ It's all we did„ÄÇ And we defined this embedding layer„ÄÇ NowÔºå when you train
    that neural network„ÄÇ You want to mark those embedding weight„ÄÇüòäÔºåAs nontrainableÔºå
    otherwise„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they'll start to get pulled away from the values that they were originally set
    at by whoever trained it„ÄÇ And if you're doing transfer learningÔºå you probably
    don't want those weights modified„ÄÇ We'll see more about how to train these in
    a moment„ÄÇ NowÔºå I compared this to dummy variables„ÄÇ So usually what you want to
    do to prove that something is equivalent to something else is see if you can emulate
    that thing in something else„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to use an embedding layer to basically provide dummy variables
    for us„ÄÇ So what I am doing here is I am creating an input dimension 3„ÄÇ So that
    would be a categorical variable that had three possible values„ÄÇ The dummies for
    this would look like this„ÄÇ essentially the diagonal that you see with dummy variables
    because dummies just a briefly review essentially one of the values is is one
    or hot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That's why it's called one hot„ÄÇ And the rest are 0„ÄÇ And this is a simple way
    that you can encode categorical„ÄÇValues the output dimension is also going to be
    three because there's three columns in what we're encoding it to„ÄÇ If you're doing
    dummy variablesÔºå these will always be the same„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is why dummy variables are so inefficientÔºå because say your input dimensions
    was 100„ÄÇ You had 100 categories„ÄÇ You could still make this very small„ÄÇ You want
    to want to make it too small„ÄÇ but you can make it say 4 or 8 and train for it„ÄÇ
    We'll see how we can do that in just a moment„ÄÇ Then input lengthÔºå that's your
    sequence length„ÄÇ So that's how many of these you want to encode at a time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we're going compile it with atom and MSC againÔºå we're going to never train
    this neural network„ÄÇ So these two really don't matter„ÄÇ but we're going to do set
    weights on the embedding layer„ÄÇ Now we do have to transform this look up up here
    into a list because you can potentially not going to really get into that„ÄÇ but
    you can you can have multiple lookup matrices for this if it's going sort of in
    multiple directions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but that would be a more advanced set„ÄÇ you can refer to the Kira's documentation
    if you're interested in„ÄÇExactlyÔºå that is a list„ÄÇ Let's go ahead and run it„ÄÇ But
    for now„ÄÇ just always embed your matrix list and you'll be good to go gonna go
    ahead and run that„ÄÇ Now we have created essentially our dummy emulator as an embedding
    layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm going to go ahead and run it down hereÔºå we're going to encode these two
    categoricals and run it and essentially look what it's doing„ÄÇ There's the dummy
    variables„ÄÇ so you could put one of these on the front of your neural network and
    not even have to encode your dummy variables„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there's better ways of going about it„ÄÇ but this is one way that you you could
    do that„ÄÇ if you wanted to make your neural network truly so that you could pass
    in these enter your values and have it automatically transform these into dummies„ÄÇ
    This is cool„ÄÇ you'll do this kind of thing a lotÔºå this is when you want to use
    transfer learning to bring your dummy variables in„ÄÇ HoweverÔºå the real fun gets
    in„ÄÇ maybe it's not the real fun„ÄÇüòäÔºåYou can train these yourself„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this is a great way to deal with if your neural network needs to take in
    a high dimensionmen categorical that does not have an easy way that you can transform
    it into dummies„ÄÇ say you have I don't know a 20000 Carnality categorical„ÄÇ you
    could literally just define it as a embedding pick some arbitrary number of dimensions
    like I don't know„ÄÇ2040„ÄÇ It's a hyperparameter you'd have to play with it and literally
    the atom update rule or back propagation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All of them„ÄÇ it'll do gradient descent and it will train your embeddings for
    you„ÄÇ Let's go ahead and see how we can do this„ÄÇ This uses some of the curs functions
    for natural language processing„ÄÇ This shows really how easily you can build these
    NLP neural networks now So here are 10 restaurant reviews„ÄÇ The first ones are
    all bad„ÄÇ Never coming back„ÄÇ horriblerrible service„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: rude waitress Col food horrible food these other guys„ÄÇRe liked it„ÄÇ AwesomeÔºå
    awesomewesome service„ÄÇ Rocks„ÄÇ Poor work couldn't have done better„ÄÇ So these are
    all just different„ÄÇ different values that you can choose for this„ÄÇ And notice
    I put in random exclamation points„ÄÇ and then even just a sort of random one that
    was more applying to sayÔºå evaluating contractors„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but noise is good„ÄÇ One means negative 0 is positive„ÄÇ So these are the labels„ÄÇ
    This is the y„ÄÇ we're going train a neural network on it„ÄÇ So we're gonna say our
    vocabulary size is 50„ÄÇ We can just pick that to be whatever the heck we want„ÄÇ
    We don't have to really count the number of words in there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we're going to use the car is one hot„ÄÇ The car is one hot is kind of cool„ÄÇüòä„ÄÇIt
    sort of scares me a little bitÔºå but it's good for examples„ÄÇ It is basically going
    in there and tokenizing for you„ÄÇ So breaking these words into breaking these sentences
    into words„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then assigning each to an index„ÄÇ So it's doing a lot in the background„ÄÇNormally„ÄÇ
    I like to have a little more control over thatÔºå I want to know how it's being
    tokenized and I would like to know how it is assigning these indexes to it„ÄÇ But
    this is good for for an example„ÄÇ when we get into showing how to connect one of
    these to an API that others will use sort of in a production environment„ÄÇ we'll
    see that we really care about locking down what these index values are„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I wouldn't want if'm if I'm deploying this in a real worldor corporate situation„ÄÇ
    I wouldn't want coming to become an index ofÔºå sayÔºå5 one time retrain an now for„ÄÇ
    and potentially the data coming in is now encoded wrong„ÄÇSo you have to be careful
    with all of that„ÄÇ Then I am going to go ahead and go ahead and run this part„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We're going to go ahead and run it and encode these„ÄÇ These are all of your sequences„ÄÇ
    We would like these to all be of consistent length because that's how sequences
    work„ÄÇ We're going to do the max length of4„ÄÇ Look at that„ÄÇ They're all nice and
    zero padded„ÄÇ Thank you„ÄÇ KirasÔºå We're going to create a very simple sequential
    neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We are going to have one dense layer at the end„ÄÇ So there is learning going
    on in here„ÄÇ There is one weight matrix„ÄÇ But then we're also learning in the embedding
    layer„ÄÇ The neural network is going to actually learn how to create these embeddings
    so that they are a way that separate those words and map them into Euclidean space
    in a meaningful way„ÄÇ This saves you having to deal with tons and tons of dummy
    variables and creating a very complex neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: embeddings are great for NLP„ÄÇ go ahead and run it We'll print a summary„ÄÇ There
    you see it„ÄÇ go ahead and fit it„ÄÇ I'm just running at„ÄÇüòäÔºå100 epochsÔºå veryÔºå very
    fast to train„ÄÇ Let's look at essentially the embeddings„ÄÇ Each line is an embedding
    for a different word„ÄÇ I'm not even going to try to explain the rhyme or reason
    for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There' essentially like weights that were calculated in the same way that the
    weights were for the actual layers„ÄÇ and there is one layer on here that's learning
    as well„ÄÇ but the embeddings learn right or long with the other weights in the
    neural network„ÄÇ and then we can evaluate this neural network at the end„ÄÇ accuracy
    is perfect„ÄÇ actually„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The reason accuracy is perfect is I really didn't put any overlap here„ÄÇ All
    the words for negative reviews werere not in the positive reviews„ÄÇ This is just
    a toy example„ÄÇ Thank you for watching this video and the next video we're going
    to look at end to end natural language processing This content changes often„ÄÇ
    So subscribe to the channel to stay up to date on this course and other topics
    in artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c6a4e57e3618c11a532abc5118438e7_4.png)'
  prefs: []
  type: TYPE_IMG
