- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P53ï¼šL10.2- ä½¿ç”¨Keraså’ŒTensorFlowç¼–ç¨‹LSTM
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P53ï¼šL10.2- ä½¿ç”¨Keraså’ŒTensorFlowç¼–ç¨‹LSTM
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Hea andã€‚ Wecome to applications of deep neural networkss with
    Washington Universityã€‚ In this videoï¼Œ we're going to look at LSTM and GU neural
    networks for time series for the latest on my AI course and projectsã€‚ click subscribe
    in the bell next to it to be notified of every new videoã€‚ we are looking at recurrent
    neural networksã€‚ Recurrent neural networksã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œäºšï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ç”¨äºæ—¶é—´åºåˆ—çš„LSTMå’ŒGRUç¥ç»ç½‘ç»œã€‚æƒ³è¦è·å–æˆ‘AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»æ—è¾¹çš„è®¢é˜…é“ƒé“›ï¼Œä»¥ä¾¿æ¥æ”¶åˆ°æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚æˆ‘ä»¬æ­£åœ¨ç ”ç©¶é€’å½’ç¥ç»ç½‘ç»œã€‚é€’å½’ç¥ç»ç½‘ç»œã€‚
- en: there's a lot of different types of theseã€‚ and we'll start by looking at one
    of the most classic typesã€‚ mainly because it's relatively easy to understandã€‚
    And then we'll look at the more modern onesã€‚ the LSTms and the Gsã€‚ there are definitely
    some similarities between all of these different types of recurrent neural networksã€‚
    So the recurrent neural networksã€‚ They usually have some concept of a context
    neuronã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰å¾ˆå¤šä¸åŒç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å°†é¦–å…ˆçœ‹å…¶ä¸­ä¸€ç§ç»å…¸ç±»å‹ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒç›¸å¯¹å®¹æ˜“ç†è§£ã€‚ç„¶åæˆ‘ä»¬å°†çœ‹æ›´ç°ä»£çš„ç±»å‹ï¼Œå¦‚LSTMå’ŒGRUã€‚è¿™äº›ä¸åŒç±»å‹çš„é€’å½’ç¥ç»ç½‘ç»œä¹‹é—´ç¡®å®å­˜åœ¨ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ã€‚å› æ­¤ï¼Œé€’å½’ç¥ç»ç½‘ç»œé€šå¸¸æœ‰ä¸€äº›ä¸Šä¸‹æ–‡ç¥ç»å…ƒçš„æ¦‚å¿µã€‚
- en: The context neuron represents a short of short term memoryã€‚ It holds a value
    between calls to the neural networkã€‚ So when you're trying to predict with the
    neural networkã€‚ That is one call to the neural networkã€‚ğŸ˜Šã€‚![](img/d578a667a9760200144c8f5677c9fef3_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡ç¥ç»å…ƒä»£è¡¨ä¸€ç§çŸ­æœŸè®°å¿†ã€‚å®ƒåœ¨å¯¹ç¥ç»ç½‘ç»œçš„è°ƒç”¨ä¹‹é—´ä¿æŒä¸€ä¸ªå€¼ã€‚æ‰€ä»¥å½“ä½ è¯•å›¾ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹æ—¶ï¼Œé‚£å°±æ˜¯ä¸€æ¬¡å¯¹ç¥ç»ç½‘ç»œçš„è°ƒç”¨ã€‚ğŸ˜Šã€‚![](img/d578a667a9760200144c8f5677c9fef3_1.png)
- en: Then you call it again and again and againï¼Œ as you predict row row rowã€‚ the
    context neurons start out as zeroï¼Œ but they hold a value as subsequent inputs
    come to the neural network as a sequence is being processedã€‚ So before we saw
    that we have these sequences that had values for each of the input neurons coming
    inã€‚ these context neurons have a zero at the beginning of each sequence and they
    have the gain of value and they keep updating their value as these sequences are
    processedã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ ä¼šä¸€æ¬¡åˆä¸€æ¬¡åœ°è°ƒç”¨å®ƒï¼Œå½“ä½ é€è¡Œé¢„æµ‹æ—¶ï¼Œä¸Šä¸‹æ–‡ç¥ç»å…ƒå¼€å§‹æ—¶ä¸ºé›¶ï¼Œä½†å®ƒä»¬ä¼šéšç€åç»­è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­è€ŒæŒæœ‰ä¸€ä¸ªå€¼ï¼Œéšç€åºåˆ—çš„å¤„ç†è€Œå˜åŒ–ã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çœ‹åˆ°è¿™äº›è¾“å…¥ç¥ç»å…ƒæ¯ä¸ªéƒ½æœ‰å€¼çš„åºåˆ—ä¹‹å‰ï¼Œè¿™äº›ä¸Šä¸‹æ–‡ç¥ç»å…ƒåœ¨æ¯ä¸ªåºåˆ—å¼€å§‹æ—¶ä¸ºé›¶ï¼Œå¹¶ä¸”éšç€è¿™äº›åºåˆ—çš„å¤„ç†ï¼Œå®ƒä»¬ä¿æŒæ›´æ–°å…¶å€¼ã€‚
- en: When you move to the next sequenceï¼Œ this value always goes back to zeroã€‚ the
    context always go to zero between sequencesï¼Œ That's a very important characteristic
    of themã€‚ So this is an example of two hidden neurons down here that have a that
    have contextã€‚ So instead of simply feeding forward like we have beforeã€‚We're now
    going to receive some refluxã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªåºåˆ—æ—¶ï¼Œè¿™ä¸ªå€¼æ€»æ˜¯ä¼šå›åˆ°é›¶ã€‚ä¸Šä¸‹æ–‡åœ¨åºåˆ—ä¹‹é—´æ€»æ˜¯å›åˆ°é›¶ï¼Œè¿™æ˜¯ä¸€ç§éå¸¸é‡è¦çš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸‹é¢ä¸¤ä¸ªå…·æœ‰ä¸Šä¸‹æ–‡çš„éšè—ç¥ç»å…ƒçš„ä¾‹å­ã€‚æ‰€ä»¥æˆ‘ä»¬ç°åœ¨ä¸ä¼šåƒä¹‹å‰é‚£æ ·ç®€å•åœ°å‘å‰ä¼ é€’ã€‚æˆ‘ä»¬å°†ä¼šæ¥æ”¶ä¸€äº›åé¦ˆã€‚
- en: some data back from what we're outputtingã€‚ So you have the inputã€‚ This is input
    1ï¼Œ input 2ã€‚ going into Hi 1 and Hi 2ã€‚ These neurons are going to output something
    based on the weightedã€‚ sum values that come into themã€‚ and then the activation
    functionã€‚ They're going to produce some outputã€‚And in classic neural network processingã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æˆ‘ä»¬è¾“å‡ºçš„æ•°æ®ä¸­è·å–ä¸€äº›åé¦ˆã€‚æ‰€ä»¥ä½ æœ‰è¾“å…¥ã€‚è¿™æ˜¯è¾“å…¥1ï¼Œè¾“å…¥2ã€‚è¿›å…¥Hi 1å’ŒHi 2ã€‚è¿™äº›ç¥ç»å…ƒå°†åŸºäºä¼ å…¥çš„åŠ æƒå’Œå€¼è¾“å‡ºä¸€äº›ç»“æœï¼Œç„¶åç»è¿‡æ¿€æ´»å‡½æ•°ã€‚å®ƒä»¬å°†äº§ç”Ÿä¸€äº›è¾“å‡ºã€‚åœ¨ç»å…¸ç¥ç»ç½‘ç»œå¤„ç†ä¸­ã€‚
- en: that output would just go on next to the next layer or out of the neural network
    if this was the finalã€‚ but in this caseã€‚We're going to take the same value that
    was output from the hidden neuron onwardã€‚ and that dotted line means we're going
    to copy it to the contextã€‚ So typically you'll have one context neuron per neuron
    on the previous layerã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ªè¾“å‡ºä¼šç›´æ¥ä¼ åˆ°ä¸‹ä¸€å±‚æˆ–ç¥ç»ç½‘ç»œçš„è¾“å‡ºç«¯ï¼Œå¦‚æœè¿™æ˜¯æœ€åä¸€å±‚ã€‚ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æŠŠä»éšè—ç¥ç»å…ƒè¾“å‡ºçš„ç›¸åŒå€¼ç»§ç»­ä¼ é€’ã€‚é‚£æ¡è™šçº¿æ„å‘³ç€æˆ‘ä»¬å°†æŠŠå®ƒå¤åˆ¶åˆ°ä¸Šä¸‹æ–‡ä¸­ã€‚å› æ­¤ï¼Œé€šå¸¸æ¯ä¸ªä¸Šä¸€å±‚çš„ç¥ç»å…ƒä¼šæœ‰ä¸€ä¸ªä¸Šä¸‹æ–‡ç¥ç»å…ƒã€‚
- en: and you're simply going to copy the outputã€‚ There is no weightã€‚ there is nothingã€‚
    It is simply a copyã€‚ The solid lines have weightsã€‚ So what's going to happen is
    the the input to to this first hidden hidden1 is not just going to be what's coming
    from the previous layerã€‚ but it's going to get the output from context1ã€‚ and it's
    also going to get the output from context2ã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªæ˜¯ç®€å•åœ°å¤åˆ¶è¾“å‡ºã€‚æ²¡æœ‰æƒé‡ï¼Œä»€ä¹ˆéƒ½æ²¡æœ‰ã€‚å®ƒåªæ˜¯ä¸€ä¸ªå¤åˆ¶ã€‚å®çº¿æœ‰æƒé‡ã€‚æ‰€ä»¥æ¥ä¸‹æ¥å‘ç”Ÿçš„äº‹æƒ…æ˜¯ï¼Œè¾“å…¥åˆ°è¿™ä¸ªç¬¬ä¸€ä¸ªéšè—å±‚hidden1ä¸ä»…ä»…æ˜¯æ¥è‡ªå‰ä¸€å±‚çš„å†…å®¹ï¼Œè¿˜ä¼šå¾—åˆ°æ¥è‡ªcontext1çš„è¾“å‡ºï¼ŒåŒæ—¶ä¹Ÿä¼šå¾—åˆ°æ¥è‡ªcontext2çš„è¾“å‡ºã€‚
- en: at least in this particular caseã€‚ The way that this neuron is is set upã€‚ this
    network is set upã€‚ These are both weightedã€‚ So it will learn how to process that
    valueï¼Œ there is no weighting hereã€‚ simply whatever was outputã€‚To the next layer
    is also copied to hereã€‚ So you're constantly keeping the value from the previous
    output from the hidden neuronã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘åœ¨è¿™ä¸ªç‰¹å®šçš„æƒ…å†µä¸‹ã€‚è¿™ä¸ªç¥ç»å…ƒçš„è®¾ç½®ï¼Œè¿™ä¸ªç½‘ç»œçš„è®¾ç½®ã€‚è¿™ä¸¤è€…éƒ½æ˜¯åŠ æƒçš„ã€‚æ‰€ä»¥å®ƒä¼šå­¦ä¹ å¦‚ä½•å¤„ç†è¿™ä¸ªå€¼ï¼Œè¿™é‡Œæ²¡æœ‰æƒé‡ã€‚ç®€å•åœ°è¯´ï¼Œæ— è®ºè¾“å‡ºä»€ä¹ˆï¼Œéƒ½å°†å…¶å¤åˆ¶åˆ°ä¸‹ä¸€ä¸ªå±‚ã€‚å› æ­¤ï¼Œä½ å§‹ç»ˆä¿ç•™æ¥è‡ªéšè—ç¥ç»å…ƒçš„å…ˆå‰è¾“å‡ºå€¼ã€‚
- en: And allowing that to stay until the next callï¼Œ and then you copy it backã€‚ you
    let it form the input back to the hidden one on on the next callã€‚ and it becomes
    just part of the inputã€‚ So reallyï¼Œ it's like there's three input neurons to hidden
    oneã€‚ but two of those are the context layers that are just the outputs from the
    previous oneã€‚ Nowã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”è®©å®ƒä¿æŒç›´åˆ°ä¸‹ä¸€ä¸ªè°ƒç”¨ï¼Œç„¶åä½ æŠŠå®ƒå¤åˆ¶å›å»ã€‚ä½ è®©å®ƒå½¢æˆä¸‹ä¸€ä¸ªè°ƒç”¨æ—¶å¯¹éšè—å±‚çš„è¾“å…¥ï¼Œå¹¶ä¸”å®ƒå°±æˆä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ã€‚å› æ­¤ï¼Œå®é™…ä¸Šï¼Œå®ƒå°±åƒæœ‰ä¸‰ä¸ªè¾“å…¥ç¥ç»å…ƒè¿æ¥åˆ°hidden1ï¼Œä½†å…¶ä¸­ä¸¤ä¸ªæ˜¯ä¸Šä¸‹æ–‡å±‚ï¼Œæ­£å¥½æ˜¯æ¥è‡ªä¸Šä¸€ä¸ªçš„è¾“å‡ºã€‚
- en: this output really lives onã€‚ It's not like it just affects the nextï¼Œ the next
    one and so onã€‚ Since the value that was copied into this contextã€‚ it's fed back
    into hereã€‚ it refluxesã€‚ It's almost like butterfly effectã€‚ It has an effect on
    what this is going this neuron' is going to outputã€‚ which then gets copied into
    here and here and it keeps going for each subsequent call of of this neural networkã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¾“å‡ºç¡®å®æ˜¯æŒç»­å­˜åœ¨çš„ã€‚å®ƒå¹¶ä¸æ˜¯ä»…ä»…å½±å“ä¸‹ä¸€ä¸ªã€ä¸‹ä¸‹ä¸€ä¸ªï¼Œç­‰ç­‰ã€‚ç”±äºå¤åˆ¶åˆ°è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å€¼ï¼Œå®ƒä¼šè¢«åé¦ˆåˆ°è¿™é‡Œï¼Œåæµã€‚å‡ ä¹å°±åƒæ˜¯è´è¶æ•ˆåº”ã€‚å®ƒå¯¹è¿™ä¸ªç¥ç»å…ƒçš„è¾“å‡ºäº§ç”Ÿå½±å“ï¼Œè¿™ä¸ªè¾“å‡ºç„¶ååˆè¢«å¤åˆ¶åˆ°è¿™é‡Œï¼Œå¹¶ä¸”åœ¨æ¯æ¬¡è°ƒç”¨è¿™ä¸ªç¥ç»ç½‘ç»œæ—¶æŒç»­è¿›è¡Œã€‚
- en: That is how the context affects the output over timeã€‚ and that is what allows
    the neural network to be time seriesã€‚ and the fact that these solid lines that
    are causing the copy backwards through the neural networkã€‚ The fact that they're
    weighted allows the neural network to learn what to do with this data over time
    as it comes into the neural networkã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸Šä¸‹æ–‡å¦‚ä½•éšç€æ—¶é—´çš„æ¨ç§»å½±å“è¾“å‡ºçš„æ–¹å¼ã€‚è¿™ä¹Ÿæ˜¯è®©ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤„ç†æ—¶é—´åºåˆ—çš„åŸå› ã€‚è€Œè¿™äº›å®çº¿å¯¼è‡´åœ¨ç¥ç»ç½‘ç»œä¸­å‘åå¤åˆ¶çš„äº‹å®ï¼Œå®ƒä»¬æœ‰æƒé‡ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿéšç€æ•°æ®çš„è¾“å…¥ï¼Œé€æ¸å­¦ä¹ å¦‚ä½•å¤„ç†è¿™äº›æ•°æ®ã€‚
- en: That is a time series neural networkã€‚ So LSTM neural networks are a way of having
    that sort of memoryã€‚ Nowï¼Œ the really almost layperson a way that I can describe
    this before we go into it really in too much detail would be if we had a scientific
    sort of calculatorã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æ—¶é—´åºåˆ—ç¥ç»ç½‘ç»œã€‚æ‰€ä»¥LSTMç¥ç»ç½‘ç»œæ˜¯å®ç°è¿™ç§è®°å¿†çš„ä¸€ç§æ–¹å¼ã€‚ç°åœ¨ï¼Œæˆ‘å¯ä»¥ç”¨ä¸€ç§å‡ ä¹æ˜¯å¤–è¡Œçš„æ–¹å¼æ¥æè¿°è¿™ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªç§‘å­¦è®¡ç®—å™¨ã€‚
- en: we won't even use any of the fancy features of this calculatorã€‚ we care mainly
    about memory plus remember and memory clearã€‚ LSTM neural networks and gated recurrent
    GRU neural networks have gates and theseã€‚are the real innovation of this type
    of neural networkã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”šè‡³ä¸ä¼šä½¿ç”¨è¿™ä¸ªè®¡ç®—å™¨çš„ä»»ä½•é«˜çº§åŠŸèƒ½ã€‚æˆ‘ä»¬ä¸»è¦å…³å¿ƒçš„æ˜¯è®°å¿†åŠ å’Œè®°å¿†æ¸…é™¤ã€‚LSTMç¥ç»ç½‘ç»œå’Œé—¨æ§é€’å½’GRUç¥ç»ç½‘ç»œæœ‰é—¨ï¼Œè¿™äº›æ˜¯è¿™ç§ç¥ç»ç½‘ç»œçš„çœŸæ­£åˆ›æ–°ã€‚
- en: They let it remember things for a long time or a short timeã€‚ There are three
    gates that you're dealing withã€‚The input gateã€‚ the forget gate and the recall
    or sometimes the output gateã€‚ you can think of these gates as dealing with the
    memory of the neural network or the contextã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å¯ä»¥è®©å®ƒè®°ä½ä¸œè¥¿ï¼Œæ— è®ºæ˜¯é•¿æ—¶é—´è¿˜æ˜¯çŸ­æ—¶é—´ã€‚ä½ è¦å¤„ç†ä¸‰ä¸ªé—¨ï¼šè¾“å…¥é—¨ã€é—å¿˜é—¨å’Œå›å¿†é—¨ï¼Œæœ‰æ—¶å«è¾“å‡ºé—¨ã€‚ä½ å¯ä»¥æŠŠè¿™äº›é—¨è§†ä¸ºä¸ç¥ç»ç½‘ç»œæˆ–ä¸Šä¸‹æ–‡çš„è®°å¿†æœ‰å…³ã€‚
- en: You can think of as the memory buttons on a calculatorã€‚ So say we have the output
    from the neuron is5ã€‚ We have these three gatesã€‚ And the three gatesã€‚ let you know
    what you should doã€‚ Should you remember itã€‚ That would be like memory plus So
    I put it into thereã€‚ we have that five now in the contextã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æŠŠå®ƒä»¬æƒ³è±¡æˆè®¡ç®—å™¨ä¸Šçš„è®°å¿†æŒ‰é’®ã€‚å‡è®¾æˆ‘ä»¬ç¥ç»å…ƒçš„è¾“å‡ºæ˜¯5ã€‚æˆ‘ä»¬æœ‰è¿™ä¸‰ä¸ªé—¨ã€‚è¿™ä¸‰ä¸ªé—¨ä¼šå‘Šè¯‰ä½ è¯¥æ€ä¹ˆåšã€‚ä½ åº”è¯¥è®°ä½å®ƒå—ï¼Ÿè¿™å°±åƒè®°å¿†åŠ ï¼Œå› æ­¤æˆ‘æŠŠå®ƒæ”¾è¿›å»ã€‚ç°åœ¨æˆ‘ä»¬åœ¨ä¸Šä¸‹æ–‡ä¸­æœ‰äº†è¿™ä¸ªäº”ã€‚
- en: when it next worksï¼Œ we might need to decide if we should recall itã€‚ That's the
    MR buttonã€‚ that's that's the output or the recall gateã€‚ If you do thatï¼Œ your5
    comes backã€‚ And then there's also the forget gateã€‚ The forget gate means something
    happens with the neural network inputã€‚ it decides it needs to drop the memory
    and you can now clear itã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å®ƒä¸‹æ¬¡å·¥ä½œæ—¶ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å†³å®šæ˜¯å¦åº”è¯¥å›å¿†èµ·æ¥ã€‚è¿™å°±æ˜¯MRæŒ‰é’®ã€‚è¿™æ˜¯è¾“å‡ºæˆ–å›å¿†é—¨ã€‚å¦‚æœä½ è¿™æ ·åšï¼Œä½ çš„5å°±ä¼šå›æ¥ã€‚è¿˜æœ‰ä¸€ä¸ªé—å¿˜é—¨ã€‚é—å¿˜é—¨æ„å‘³ç€ç¥ç»ç½‘ç»œè¾“å…¥å‘ç”Ÿäº†ä¸€äº›äº‹æƒ…ã€‚å®ƒå†³å®šéœ€è¦ä¸¢å¼ƒè®°å¿†ï¼Œä½ ç°åœ¨å¯ä»¥æ¸…é™¤å®ƒã€‚
- en: And if you do you can't even do an MR because there is no There is nothing rememberedã€‚
    It's important for these recurrent neural networks to be able to forgetã€‚ And that
    was a limitation of theã€‚Of the element neural networkã€‚ you would just have to
    slowly forget over time The other limitation was you couldn't hold something for
    a long timeã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿™æ ·åšï¼Œä½ ç”šè‡³æ— æ³•è¿›è¡ŒMRï¼Œå› ä¸ºæ²¡æœ‰è®°ä½ä»»ä½•ä¸œè¥¿ã€‚å¯¹äºè¿™äº›é€’å½’ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œèƒ½å¤Ÿå¿˜è®°æ˜¯å¾ˆé‡è¦çš„ã€‚è¿™æ˜¯å…ƒç´ ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªé™åˆ¶ã€‚ä½ å¿…é¡»æ…¢æ…¢åœ°éšç€æ—¶é—´çš„æ¨ç§»è€Œå¿˜è®°ã€‚å¦ä¸€ä¸ªé™åˆ¶æ˜¯ä½ ä¸èƒ½é•¿æ—¶é—´ä¿ç•™æŸäº›ä¸œè¥¿ã€‚
- en: so the fact that these new neural networks have the gatesã€‚ you can remember
    something and put it into your memory and then forget it later over time It's
    Otoman said as far as human beingsã€‚ one of our greatest strengths and weaknessesã€‚Is
    to be able to forget if we remembered everythingã€‚ our minds would be simply too
    cluttered and we would not be able to functionã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™äº›æ–°ç¥ç»ç½‘ç»œæœ‰é—¨çš„äº‹å®ï¼Œä½ å¯ä»¥è®°ä½æŸäº›ä¸œè¥¿å¹¶å°†å…¶æ”¾å…¥è®°å¿†ä¸­ï¼Œç„¶åéšç€æ—¶é—´çš„æ¨ç§»å†å¿˜è®°ã€‚æ­£å¦‚å¥¥ç‰¹æ›¼æ‰€è¯´ï¼Œå¯¹äºäººç±»è€Œè¨€ï¼Œæˆ‘ä»¬æœ€å¤§çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ä¹‹ä¸€æ˜¯èƒ½å¤Ÿå¿˜è®°ã€‚å¦‚æœæˆ‘ä»¬è®°ä½äº†ä¸€åˆ‡ï¼Œæˆ‘ä»¬çš„å¤´è„‘å°†å˜å¾—è¿‡äºæ··ä¹±ï¼Œæ— æ³•æ­£å¸¸è¿ä½œã€‚
- en: The important thing is knowing what to forget and forgetting only things that
    are unimportantã€‚ So let's look at the LSTM neural networkã€‚ This is a diagram of
    the LSTMã€‚ you have your output which is essentially Y hat for the current timeï¼Œ
    previous timeï¼Œ next timeã€‚ Now this is not three different LSTMsã€‚ This is the same
    LSTM and the LSTM is like a neuron in in the networkã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦çŸ¥é“è¯¥å¿˜è®°ä»€ä¹ˆï¼Œå¹¶ä¸”åªå¿˜è®°ä¸é‡è¦çš„ä¸œè¥¿ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹LSTMç¥ç»ç½‘ç»œã€‚è¿™æ˜¯LSTMçš„å›¾ç¤ºã€‚ä½ æœ‰è¾“å‡ºï¼Œæœ¬è´¨ä¸Šæ˜¯å½“å‰æ—¶é—´ã€å‰ä¸€æ—¶é—´å’Œä¸‹ä¸€æ—¶é—´çš„Yå¸½ã€‚ç°åœ¨ï¼Œè¿™ä¸æ˜¯ä¸‰ä¸ªä¸åŒçš„LSTMã€‚è¿™æ˜¯åŒä¸€ä¸ªLSTMï¼Œè€ŒLSTMå°±åƒç½‘ç»œä¸­çš„ä¸€ä¸ªç¥ç»å…ƒã€‚
- en: you'll have multiple LSTMsï¼Œ just like you have multiple hidden neurons And you
    have inputã€‚ this is the input over timeã€‚ So you just have a single input value
    for the current previous and nextã€‚ and over timeã€‚ each each the same LSTM over
    time is passing the Y hat to the next oneã€‚ So the predictionï¼Œ but it's also keeping
    a context over timeã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†æ‹¥æœ‰å¤šä¸ªLSTMï¼Œå°±åƒä½ æœ‰å¤šä¸ªéšè—ç¥ç»å…ƒä¸€æ ·ã€‚ä½ æœ‰è¾“å…¥ã€‚è¿™æ˜¯éšæ—¶é—´å˜åŒ–çš„è¾“å…¥ã€‚å› æ­¤ï¼Œä½ åªéœ€è¦ä¸€ä¸ªå½“å‰ã€å‰ä¸€å’Œä¸‹ä¸€çš„å•ä¸€è¾“å…¥å€¼ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ¯ä¸ªç›¸åŒçš„LSTMéƒ½ä¼šå°†Yå¸½ä¼ é€’ç»™ä¸‹ä¸€ä¸ªã€‚æ‰€ä»¥è¿™æ˜¯é¢„æµ‹ï¼Œä½†å®ƒä¹Ÿåœ¨éšç€æ—¶é—´ä¿æŒä¸Šä¸‹æ–‡ã€‚
- en: all these networks have some sort of context beã€‚neural networksã€‚ classic Elman
    and Jordan neural networks or be they more modern LSTM and GRU units internallyã€‚
    this is what's going on with a LSTMã€‚ you've got these gates forge gate input gate
    output gate and you've got a variety of clamping of thresholdholding functions
    sigmoids and a tan H'll see those in a moment you can look through this diagram
    if you prefer it basically represents the same thing as these equationsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›ç½‘ç»œéƒ½æœ‰æŸç§ä¸Šä¸‹æ–‡ã€‚ç¥ç»ç½‘ç»œã€‚ç»å…¸çš„Elmanå’ŒJordanç¥ç»ç½‘ç»œï¼Œæˆ–æ›´ç°ä»£çš„LSTMå’ŒGRUå•å…ƒåœ¨å†…éƒ¨ã€‚è¿™å°±æ˜¯LSTMçš„è¿ä½œã€‚ä½ æœ‰è¿™äº›é—¨ï¼šé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ï¼Œè¿˜æœ‰å„ç§é˜ˆå€¼å¤¹æŒå‡½æ•°ï¼Œæ¯”å¦‚sigmoidå’Œtanhã€‚ç¨åä½ ä¼šçœ‹åˆ°è¿™äº›ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥æŸ¥çœ‹è¿™ä¸ªå›¾ç¤ºï¼Œå®ƒåŸºæœ¬ä¸Šä»£è¡¨äº†ä¸è¿™äº›æ–¹ç¨‹ç›¸åŒçš„å†…å®¹ã€‚
- en: I almost like to think think of these in terms of the equations and I'll take
    you through the equations pretty quickly This is essentially looking at a this
    is essentially looking at sort of a linear algebra computation So you've got the
    weights that's the weight F weight F is the forget gate weight I the input gate
    the bias for each of these layers so you have bias neurons and then this vectorã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‡ ä¹å–œæ¬¢ç”¨æ–¹ç¨‹æ¥æ€è€ƒè¿™äº›ï¼Œæˆ‘ä¼šå¿«é€Ÿå¸¦ä½ æµè§ˆè¿™äº›æ–¹ç¨‹ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯åœ¨çœ‹ä¸€ç§çº¿æ€§ä»£æ•°è®¡ç®—ã€‚å› æ­¤ä½ æœ‰æƒé‡ï¼Œé‚£æ˜¯é—å¿˜é—¨çš„æƒé‡ã€è¾“å…¥é—¨çš„æƒé‡ï¼Œæ¯ä¸€å±‚çš„åå·®ï¼Œæ‰€ä»¥ä½ æœ‰åå·®ç¥ç»å…ƒå’Œè¿™ä¸ªå‘é‡ã€‚
- en: Of the output from the previous one and the input for the currentã€‚ This is a
    vector that you're multiplying that weight overã€‚ So the first thing that you want
    to do with thisï¼Œ and by the wayï¼Œ S means a sigmoid functionã€‚10 H means a hyperbolic
    tangent before we talk about sigmoid and 10 Hã€‚ let me show you thatã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸€ä¸ªè¾“å‡ºå’Œå½“å‰è¾“å…¥ã€‚è¿™æ˜¯ä½ è¦ä¹˜ä»¥çš„æƒé‡å‘é‡ã€‚å› æ­¤ï¼Œä½ è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ï¼Œé¡ºä¾¿è¯´ä¸€ä¸‹ï¼ŒSè¡¨ç¤ºsigmoidå‡½æ•°ï¼Œ10 Hè¡¨ç¤ºåŒæ›²æ­£åˆ‡ã€‚åœ¨æˆ‘ä»¬è°ˆè®ºsigmoidå’Œ10
    Hä¹‹å‰ï¼Œè®©æˆ‘å…ˆç»™ä½ å±•ç¤ºä¸€ä¸‹ã€‚
- en: notice the shapes of theseã€‚ This is a sigmoid functionã€‚ it goes from0 to1ã€‚ and
    it clamps these valuesã€‚ So if if it's very negativeã€‚ it clamps it to0ã€‚ It's a
    step functionã€‚ It's very positiveï¼Œ it clamps it to oneã€‚ hyperbolic tangentã€‚ and
    by the wayã€‚ this has nothing to do with triometryã€‚ machine learning research just
    like the shape of these moreã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„è¿™äº›å½¢çŠ¶ã€‚è¿™æ˜¯ä¸€ä¸ªsigmoidå‡½æ•°ã€‚å®ƒçš„å€¼ä»0åˆ°1ï¼Œå¹¶ä¸”ä¼šå¤¹ä½è¿™äº›å€¼ã€‚æ‰€ä»¥å¦‚æœéå¸¸è´Ÿï¼Œå®ƒä¼šå¤¹ä½åˆ°0ã€‚å¦‚æœéå¸¸æ­£ï¼Œå®ƒä¼šå¤¹ä½åˆ°1ã€‚åŒæ›²æ­£åˆ‡ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œè¿™ä¸ä¸‰è§’å­¦æ— å…³ã€‚æœºå™¨å­¦ä¹ ç ”ç©¶ä»…ä»…æ˜¯å› ä¸ºè¿™äº›å½¢çŠ¶ã€‚
- en: so it's a step functionã€‚ just another type of step functionã€‚ But notice that
    the that the range is now negative1 to1ã€‚ So it'sã€‚It's got a biggerï¼Œ bigger rangeã€‚
    and sometimes we need these negative values and we'll see when we go through these
    equations that we do need those negative valuesã€‚ So let's go through the equationsã€‚
    The first thing we have to do is calculate the coefficients for the forget gate
    and the input gate and ultimately the output gateã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªé˜¶è·ƒå‡½æ•°ã€‚å¦ä¸€ç§ç±»å‹çš„é˜¶è·ƒå‡½æ•°ã€‚ä½†æ³¨æ„ï¼Œç°åœ¨çš„èŒƒå›´æ˜¯è´Ÿ1åˆ°1ã€‚æ‰€ä»¥ï¼Œå®ƒæœ‰ä¸€ä¸ªæ›´å¤§çš„èŒƒå›´ã€‚æœ‰æ—¶æˆ‘ä»¬éœ€è¦è¿™äº›è´Ÿå€¼ï¼Œæˆ‘ä»¬ä¼šåœ¨é€šè¿‡è¿™äº›æ–¹ç¨‹æ—¶çœ‹åˆ°ï¼Œæˆ‘ä»¬ç¡®å®éœ€è¦è¿™äº›è´Ÿå€¼ã€‚ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™äº›æ–¹ç¨‹ã€‚æˆ‘ä»¬é¦–å…ˆè¦åšçš„æ˜¯è®¡ç®—é—å¿˜é—¨ã€è¾“å…¥é—¨å’Œæœ€ç»ˆçš„è¾“å‡ºé—¨çš„ç³»æ•°ã€‚
- en: because these three valuesï¼Œ they're going to be largely zero or one because
    see these are where the sigmoids are usedã€‚ you want those to be0 or1 because those
    valuesï¼Œ you want them on or offã€‚ It's a coefficient So forget if the forget gate
    is calculated to be0ã€‚ it's kind of backwards a little bitï¼Œ but zero means we should
    forget one means we should rememberã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™ä¸‰ä¸ªå€¼ä¸»è¦ä¼šæ˜¯é›¶æˆ–ä¸€ï¼Œå› ä¸ºè¿™äº›æ˜¯sigmoidä½¿ç”¨çš„åœ°æ–¹ã€‚ä½ å¸Œæœ›å®ƒä»¬æ˜¯0æˆ–1ï¼Œå› ä¸ºè¿™äº›å€¼ï¼Œä½ å¸Œæœ›å®ƒä»¬å¼€å¯æˆ–å…³é—­ã€‚è¿™æ˜¯ä¸€ä¸ªç³»æ•°ã€‚å› æ­¤ï¼Œå¦‚æœé—å¿˜é—¨è®¡ç®—ä¸º0ï¼Œè¿™æœ‰ç‚¹åå‘ï¼Œä½†é›¶æ„å‘³ç€æˆ‘ä»¬åº”è¯¥é—å¿˜ï¼Œä¸€æ„å‘³ç€æˆ‘ä»¬åº”è¯¥è®°ä½ã€‚
- en: So we calculate the forget gate based on the sigmoid functionã€‚ which flips it
    sort of into that 01 range by looking at the weight for forgettingã€‚ So this weight
    for forgettingï¼Œ that's the training capabilityã€‚ This value will change as the
    neural network learnsã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æ ¹æ®sigmoidå‡½æ•°è®¡ç®—é—å¿˜é—¨ã€‚å®ƒé€šè¿‡æŸ¥çœ‹é—å¿˜æƒé‡ï¼Œå°†å…¶è½¬æ¢ä¸º0åˆ°1èŒƒå›´ã€‚å› æ­¤ï¼Œé—å¿˜çš„æƒé‡å°±æ˜¯è®­ç»ƒèƒ½åŠ›ã€‚éšç€ç¥ç»ç½‘ç»œçš„å­¦ä¹ ï¼Œè¿™ä¸ªå€¼ä¼šæ”¹å˜ã€‚
- en: and by multiplying by the previous output and the current input that that vector
    and adding the biasã€‚ which is also a learning parameterã€‚ So by adjusting these
    twoï¼Œ we learn when to forgetã€‚ if F becomes a0ï¼Œ we're going to forgetã€‚ If I becomes
    oneï¼Œ we're going to rememberã€‚ So you have the weightsï¼Œ these are exactly the same
    functions as before these are your two learning parametersã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¹˜ä»¥å‰ä¸€ä¸ªè¾“å‡ºå’Œå½“å‰è¾“å…¥çš„å‘é‡ï¼Œå¹¶æ·»åŠ åç½®ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå­¦ä¹ å‚æ•°ã€‚é€šè¿‡è°ƒæ•´è¿™ä¸¤ä¸ªï¼Œæˆ‘ä»¬å­¦ä¹ ä½•æ—¶é—å¿˜ã€‚å¦‚æœFå˜æˆ0ï¼Œæˆ‘ä»¬å°±ä¼šé—å¿˜ã€‚å¦‚æœIå˜æˆ1ï¼Œæˆ‘ä»¬å°±ä¼šè®°ä½ã€‚æ‰€ä»¥ä½ æœ‰æƒé‡ï¼Œè¿™äº›ä¸ä¹‹å‰çš„åŠŸèƒ½å®Œå…¨ç›¸åŒï¼Œè¿™äº›æ˜¯ä½ çš„ä¸¤ä¸ªå­¦ä¹ å‚æ•°ã€‚
- en: those will be adjustedï¼Œ that's how it learns when to when to remember or when
    to feed the input inã€‚ C is your contextã€‚ C with the little tilde above it just
    means your candidate context Now contextã€‚Is the value that it's rememberingï¼Œ The
    value that it's remembering is the output from the neuronã€‚ The output from the
    neuroura can be negative one to oneã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¼šè¢«è°ƒæ•´ï¼Œè¿™å°±æ˜¯å®ƒå­¦ä¹ ä½•æ—¶è®°ä½æˆ–ä½•æ—¶è¾“å…¥çš„æ–¹å¼ã€‚Cæ˜¯ä½ çš„ä¸Šä¸‹æ–‡ã€‚Cä¸Šæ–¹çš„å°æ³¢æµªå·ä»…è¡¨ç¤ºä½ çš„å€™é€‰ä¸Šä¸‹æ–‡ã€‚ç°åœ¨çš„ä¸Šä¸‹æ–‡æ˜¯å®ƒæ­£åœ¨è®°ä½çš„å€¼ï¼Œè®°ä½çš„å€¼æ˜¯æ¥è‡ªç¥ç»å…ƒçš„è¾“å‡ºã€‚ç¥ç»å…ƒçš„è¾“å‡ºå¯ä»¥æ˜¯ä»è´Ÿä¸€åˆ°ä¸€ã€‚
- en: You can't use a sigmoid for that because negative1 to oneï¼Œ it's going to clip
    off half the valuesã€‚ anything below 0ã€‚ So that's why we use a hyperbolic tangent
    is the hyperbolic tangent can flow between negative one and 1ã€‚ So we we've got
    the fullï¼Œ the fuller range of values that we can deal withã€‚But againã€‚ the training
    works just like just like beforeã€‚ we have a weight for the contextã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¸èƒ½ç”¨sigmoidæ¥åšï¼Œå› ä¸ºè´Ÿ1åˆ°1ä¼šå‰ªåˆ‡æ‰ä¸€åŠçš„å€¼ï¼Œä»»ä½•ä½äº0çš„å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒæ›²æ­£åˆ‡æ˜¯å› ä¸ºåŒæ›²æ­£åˆ‡å¯ä»¥åœ¨è´Ÿä¸€å’Œä¸€ä¹‹é—´æµåŠ¨ã€‚è¿™æ ·æˆ‘ä»¬å°±æ‹¥æœ‰äº†æ›´å®Œæ•´çš„å€¼èŒƒå›´ã€‚ä½†è®­ç»ƒçš„å·¥ä½œæ–¹å¼ä¸ä¹‹å‰å®Œå…¨ç›¸åŒã€‚æˆ‘ä»¬æœ‰ä¸Šä¸‹æ–‡çš„æƒé‡ã€‚
- en: So that's how it's learning what to specifically do with the contextã€‚ And we
    have a biasã€‚ This is just the candidate contextã€‚ What really becomes the context
    is thisã€‚ And now this is a different format because we're not we're not learningï¼Œ
    these are for learningã€‚ This is just a switch gateã€‚ Notice you have the forget
    and the inputã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å®ƒå­¦ä¹ å¦‚ä½•å…·ä½“å¤„ç†ä¸Šä¸‹æ–‡çš„æ–¹å¼ã€‚æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªåç½®ã€‚è¿™åªæ˜¯å€™é€‰ä¸Šä¸‹æ–‡ã€‚çœŸæ­£æˆä¸ºä¸Šä¸‹æ–‡çš„æ˜¯è¿™ä¸ªã€‚ç°åœ¨è¿™æ˜¯ä¸€ä¸ªä¸åŒçš„æ ¼å¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¸åœ¨å­¦ä¹ ï¼Œè¿™äº›æ˜¯ç”¨äºå­¦ä¹ çš„ã€‚è¿™åªæ˜¯ä¸€ä¸ªå¼€å…³é—¨ã€‚æ³¨æ„ä½ æœ‰é—å¿˜é—¨å’Œè¾“å…¥é—¨ã€‚
- en: input says what should go into the contextï¼Œ forget means should we remember
    the previous contextã€‚ Plus just pipes both of those two togetherã€‚ So forgetting
    the contextã€‚ if this is0 multiplied it's a coefficientï¼Œ So if it's a0ã€‚ if's going
    to wipe out the previous context because0 times anything is 0 input means so this
    kills off the previous context if we're forgetting inputã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥è¡¨ç¤ºåº”è¯¥è¿›å…¥ä¸Šä¸‹æ–‡çš„å†…å®¹ï¼Œé—å¿˜æ„å‘³ç€æˆ‘ä»¬æ˜¯å¦åº”è¯¥è®°ä½ä¹‹å‰çš„ä¸Šä¸‹æ–‡ã€‚åŠ å·å°†è¿™ä¸¤ä¸ªç»“åˆåœ¨ä¸€èµ·ã€‚å› æ­¤ï¼Œé—å¿˜ä¸Šä¸‹æ–‡ã€‚å¦‚æœè¿™æ˜¯0ä¹˜ä»¥ä¸€ä¸ªç³»æ•°ï¼Œå¦‚æœæ˜¯0çš„è¯ï¼Œé‚£ä¹ˆå®ƒå°†æŠ¹å»ä¹‹å‰çš„ä¸Šä¸‹æ–‡ï¼Œå› ä¸º0ä¹˜ä»¥ä»»ä½•ä¸œè¥¿éƒ½æ˜¯0ï¼Œè¾“å…¥æ„å‘³ç€å¦‚æœæˆ‘ä»¬åœ¨é—å¿˜è¾“å…¥ï¼Œè¿™å°†æ¶ˆç­ä¹‹å‰çš„ä¸Šä¸‹æ–‡ã€‚
- en: this is why it's like the memory plus button because this is yourã€‚Previous contextã€‚
    I'm sorryã€‚ this one's your previous contextã€‚ This is the candidate contextã€‚ So
    this is what we calculated up here that might be going into the contextã€‚ if we
    should put something into So then the final now that you've got the output gate
    createdã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒåƒæ˜¯å†…å­˜åŠ æŒ‰é’®ï¼Œå› ä¸ºè¿™æ˜¯ä½ çš„å‰ä¸€ä¸ªä¸Šä¸‹æ–‡ã€‚æŠ±æ­‰ï¼Œè¿™ä¸ªæ˜¯ä½ çš„å‰ä¸€ä¸ªä¸Šä¸‹æ–‡ã€‚è¿™æ˜¯å€™é€‰ä¸Šä¸‹æ–‡ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œè®¡ç®—çš„å¯èƒ½è¿›å…¥ä¸Šä¸‹æ–‡çš„å†…å®¹ã€‚å¦‚æœæˆ‘ä»¬åº”è¯¥æ”¾å…¥æŸäº›ä¸œè¥¿ã€‚æ‰€ä»¥ç°åœ¨ä½ å·²ç»åˆ›å»ºäº†è¾“å‡ºé—¨ã€‚
- en: your final output is essentially going to is going to come from the previous
    context multiplied by your your outputã€‚ that is essentially how the LSTM is calculatedã€‚
    There's another type of neural of recurrent neural network that is very similar
    to thisã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„æœ€ç»ˆè¾“å‡ºæœ¬è´¨ä¸Šå°†æ¥è‡ªäºä¹‹å‰çš„ä¸Šä¸‹æ–‡ä¹˜ä»¥ä½ çš„è¾“å‡ºã€‚è¿™å°±æ˜¯LSTMçš„è®¡ç®—æ–¹å¼ã€‚è¿˜æœ‰å¦ä¸€ç§ç±»å‹çš„é€’å½’ç¥ç»ç½‘ç»œï¼Œä¸æ­¤éå¸¸ç›¸ä¼¼ã€‚
- en: that is called the GRUã€‚ the gated recurrent unitã€‚ It works really extremely
    similar to thisã€‚ me zoom this in a little bitã€‚ This is the academic paperã€‚ This
    is not the academic paper that introduced the GRUã€‚ but it's an empirical evaluation
    gated recurrentã€‚networksworks empirical just means experimentalã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§°ä¸ºGRUï¼Œé—¨æ§é€’å½’å•å…ƒã€‚å®ƒçš„å·¥ä½œåŸç†ä¸æ­¤éå¸¸ç›¸ä¼¼ã€‚è®©æˆ‘æ”¾å¤§ä¸€ç‚¹ã€‚è¿™æ˜¯å­¦æœ¯è®ºæ–‡ã€‚è¿™ä¸æ˜¯ä»‹ç»GRUçš„å­¦æœ¯è®ºæ–‡ï¼Œä½†å®ƒæ˜¯å¯¹é—¨æ§é€’å½’ç½‘ç»œçš„å®è¯è¯„ä¼°ã€‚å®è¯æ„å‘³ç€å®éªŒæ€§çš„ã€‚
- en: So they're not doing any sort of mathematical proof for handwavingã€‚ They're
    just literally trying to unt testest data and showing you what a GRU can actually
    accomplishã€‚ in a lot of casesï¼Œ GRs and LSTMs function really pretty similar and
    do sort of have a similar accuracy none This paper deals with evaluating more
    the processing time is a GRU does not have as many gatesã€‚ and that makes it more
    simpleã€‚ So you'll notice in the abstractï¼Œ they're saying that they evaluatedã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»–ä»¬å¹¶æ²¡æœ‰åšä»»ä½•æ•°å­¦è¯æ˜æ¥å«ç³Šå…¶è¾ã€‚ä»–ä»¬åªæ˜¯å­—é¢ä¸Šå°è¯•æµ‹è¯•æ•°æ®ï¼Œå¹¶å±•ç¤ºGRUå®é™…ä¸Šèƒ½å®ç°ä»€ä¹ˆã€‚åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼ŒGRUå’ŒLSTMçš„åŠŸèƒ½éå¸¸ç›¸ä¼¼ï¼Œå¹¶ä¸”æœ‰ç€ç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚è¿™ç¯‡è®ºæ–‡æ›´ä¾§é‡äºè¯„ä¼°å¤„ç†æ—¶é—´ï¼ŒGRUæ²¡æœ‰é‚£ä¹ˆå¤šé—¨ï¼Œå› æ­¤æ›´ç®€å•ã€‚å› æ­¤ä½ ä¼šæ³¨æ„åˆ°åœ¨æ‘˜è¦ä¸­ï¼Œä»–ä»¬è¯´ä»–ä»¬è¿›è¡Œäº†è¯„ä¼°ã€‚
- en: Alsoï¼Œ we found GRU to be comparable to LSTN and they also evaluate on previous
    nonrecurrent neural networksã€‚ So if we look through the paper the part that I
    want to show you that is particularly interestingã€‚Is you have here the LSTM that
    we just looked at with its gatesã€‚ you can see the input gate up at the topï¼Œ the
    F gate there and the output gate thereã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°GRUä¸LSTMç›¸å½“ï¼Œå¹¶ä¸”ä»–ä»¬ä¹Ÿè¯„ä¼°äº†ä¹‹å‰çš„éé€’å½’ç¥ç»ç½‘ç»œã€‚å› æ­¤å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è®ºæ–‡ï¼Œæˆ‘æƒ³å±•ç¤ºçš„ç‰¹åˆ«æœ‰è¶£çš„éƒ¨åˆ†æ˜¯ä½ åœ¨è¿™é‡Œæœ‰æˆ‘ä»¬åˆšæ‰çœ‹åˆ°çš„LSTMåŠå…¶é—¨ã€‚ä½ å¯ä»¥çœ‹åˆ°é¡¶éƒ¨çš„è¾“å…¥é—¨ï¼ŒFé—¨å’Œè¾“å‡ºé—¨ã€‚
- en: so you have three gates in the recurrent or the GRU you just have the you just
    have two gatesã€‚ R and Z so' it's a simpler sort of algorithm to that and it does
    not require nearly since there's another gateã€‚ it requires much less computation
    time and they show you how to calculate these gatesã€‚But at the veryï¼Œ very endï¼Œ
    the paper gets to showing the learning and showing really that the LSTM and GRU
    are somewhat equivalent and they show wall clock timeã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨é€’å½’ç¥ç»ç½‘ç»œæˆ–GRUä¸­ï¼Œä½ åªæœ‰ä¸¤ä¸ªé—¨ï¼šRå’ŒZï¼Œè¿™ä½¿å¾—å®ƒçš„ç®—æ³•æ›´ç®€å•ï¼Œå¹¶ä¸”å‡ ä¹ä¸éœ€è¦å¦ä¸€ä¸ªé—¨ã€‚å› æ­¤ï¼Œå®ƒæ‰€éœ€çš„è®¡ç®—æ—¶é—´æ›´å°‘ï¼Œå¹¶ä¸”ä»–ä»¬å±•ç¤ºäº†å¦‚ä½•è®¡ç®—è¿™äº›é—¨ã€‚ä½†æ˜¯åœ¨æœ€åï¼Œè®ºæ–‡å±•ç¤ºäº†å­¦ä¹ è¿‡ç¨‹ï¼Œå®é™…ä¸ŠLSTMå’ŒGRUåœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ç­‰ä»·çš„ï¼Œå¹¶å±•ç¤ºäº†å®é™…çš„æ—¶é—´ã€‚
- en: Notice hereï¼Œ the GRU is taking much less time to process than the G than the
    LSTMã€‚ So that's that's their advantageã€‚ You can process much more your your training
    impactpos will take epos will take much lessã€‚ much less timeã€‚ Now we're going
    to look at an actual example of LSTMã€‚ We're going to see two examplesã€‚ The first
    example is just reallyï¼Œ really simpleã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„è¿™é‡Œï¼ŒGRUçš„å¤„ç†æ—¶é—´æ¯”LSTMå°‘å¾—å¤šã€‚è¿™å°±æ˜¯ä»–ä»¬çš„ä¼˜åŠ¿ã€‚ä½ å¯ä»¥å¤„ç†å¾—æ›´å¤šï¼Œä½ çš„è®­ç»ƒå½±å“ä¼šæ¶ˆè€—æ›´å°‘çš„æ—¶é—´ã€‚ç°åœ¨æˆ‘ä»¬å°†çœ‹ä¸€ä¸ªå®é™…çš„LSTMä¾‹å­ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ä¸¤ä¸ªä¾‹å­ã€‚ç¬¬ä¸€ä¸ªä¾‹å­éå¸¸ç®€å•ã€‚
- en: It's meant to show youã€‚What this recurrent type of neural network can do just
    in its most simple state and yet do something that a normal neural network would
    not be able to doã€‚ And then the next example will look like we'll look at an actual
    applications where we look at time series data for sunspots and we'll see how
    this type of recurrent neural network can predict something that is time series
    and be able to at least in a very basic wayã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ—¨åœ¨å‘ä½ å±•ç¤ºè¿™ç§é€’å½’ç±»å‹çš„ç¥ç»ç½‘ç»œåœ¨æœ€ç®€å•çŠ¶æ€ä¸‹å¯ä»¥åšä»€ä¹ˆï¼Œè€Œè¿™ä¸€ç‚¹æ˜¯æ™®é€šç¥ç»ç½‘ç»œæ— æ³•åšåˆ°çš„ã€‚æ¥ä¸‹æ¥çš„ä¾‹å­å°†ä¼šçœ‹ä¸€ä¸ªå®é™…çš„åº”ç”¨ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹å¤ªé˜³é»‘å­çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶è§‚å¯Ÿè¿™ç§é€’å½’ç¥ç»ç½‘ç»œå¦‚ä½•é¢„æµ‹æ—¶é—´åºåˆ—ä¸­çš„æŸäº›ä¸œè¥¿ï¼Œè‡³å°‘åœ¨ä¸€ä¸ªéå¸¸åŸºæœ¬çš„å±‚é¢ä¸Šã€‚
- en: predict predict sunspot activityã€‚ So this is showing you how to build a very
    simple tensorflow through Kiras LSTM neural networkã€‚ I'm going to go ahead and
    run thisã€‚ It takes it a moment to trainï¼Œ but it's not too badã€‚ So it's running
    there you can see that from the asteriskã€‚ This is showing the neural network how
    to predict something from time seriesã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å¤ªé˜³é»‘å­æ´»åŠ¨ã€‚è¿™å±•ç¤ºäº†å¦‚ä½•é€šè¿‡Kerasçš„LSTMç¥ç»ç½‘ç»œæ„å»ºä¸€ä¸ªéå¸¸ç®€å•çš„tensorflowã€‚æˆ‘å°†ç»§ç»­è¿è¡Œè¿™ä¸ªç¨‹åºã€‚å®ƒéœ€è¦ä¸€äº›æ—¶é—´æ¥è®­ç»ƒï¼Œä½†è¿˜ç®—ä¸é”™ã€‚æ‰€ä»¥å®ƒæ­£åœ¨è¿è¡Œï¼Œä½ å¯ä»¥ä»æ˜Ÿå·ä¸­çœ‹åˆ°ã€‚è¿™å‘ç¥ç»ç½‘ç»œå±•ç¤ºäº†å¦‚ä½•ä»æ—¶é—´åºåˆ—ä¸­é¢„æµ‹æŸäº›ä¸œè¥¿ã€‚
- en: This time series is essentially almost think of it like a camera in front of
    a house like just a pinholeã€‚Cameraï¼Œ it sees a car driving by and it might see
    just a little bit of the paint color as the car is going byã€‚ So the same position
    we're looking straight throughã€‚ we're seeing zeroã€‚ That means we're seeing nothingã€‚
    but then at the next time sliceï¼Œ we see a carã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ—¶é—´åºåˆ—åŸºæœ¬ä¸Šå¯ä»¥æƒ³è±¡æˆåƒæ˜¯ä¸€ä¸ªåœ¨æˆ¿å­å‰é¢çš„ç›¸æœºï¼Œå°±åƒä¸€ä¸ªé’ˆå­”ç›¸æœºï¼Œå®ƒçœ‹åˆ°ä¸€è¾†è½¦é©¶è¿‡ï¼Œå¯èƒ½åªçœ‹åˆ°äº†ä¸€ç‚¹ç‚¹è½¦æ¼†çš„é¢œè‰²ã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬ç›´è§†çš„åŒä¸€ä½ç½®ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯é›¶ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä»€ä¹ˆéƒ½æ²¡çœ‹åˆ°ã€‚ä½†æ˜¯åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´åˆ‡ç‰‡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€è¾†è½¦ã€‚
- en: then at the next time slice againã€‚ we still see the car because it's it's going
    in front of our pinhole that we're seeingã€‚And now we see zero again for the rest
    of the sequence because basically the car has cleared our pinhole and is moved
    on its wayã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨ä¸‹ä¸€ä¸ªæ—¶é—´åˆ‡ç‰‡ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥çœ‹åˆ°è¿™è¾†è½¦ï¼Œå› ä¸ºå®ƒæ­£å¥½åœ¨æˆ‘ä»¬çœ‹åˆ°çš„é’ˆå­”å‰é¢ã€‚ç°åœ¨åœ¨æ¥ä¸‹æ¥çš„åºåˆ—ä¸­æˆ‘ä»¬å†æ¬¡çœ‹åˆ°é›¶ï¼Œå› ä¸ºåŸºæœ¬ä¸Šè¿™è¾†è½¦å·²ç»ä»æˆ‘ä»¬çš„é’ˆå­”ä¸­ç»è¿‡å¹¶ç»§ç»­å‰è¡Œã€‚
- en: So these are sequences hereï¼Œ a different colored car goes by a two colored carã€‚
    whatever two happens to beï¼Œ maybe that's maybe one's redï¼Œ two's blueï¼Œ something
    such as thatã€‚ ideallyï¼Œ if we wanted to make this more advancedï¼Œ we would probably
    have three inputsã€‚ and we would make this some sort of RGB for each and every
    single one of theseã€‚ butã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€äº›åºåˆ—ï¼Œä¸€è¾†ä¸åŒé¢œè‰²çš„è½¦ç»è¿‡ï¼Œå¯èƒ½æ˜¯ä¸€è¾†åŒè‰²è½¦ã€‚ä¸ç®¡è¿™ä¸¤ä¸ªé¢œè‰²æ˜¯ä»€ä¹ˆï¼Œæˆ–è®¸ä¸€ä¸ªæ˜¯çº¢è‰²ï¼Œå¦ä¸€ä¸ªæ˜¯è“è‰²ï¼Œç±»ä¼¼è¿™æ ·çš„ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®©è¿™ä¸ªæ›´å¤æ‚ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæœ‰ä¸‰ä¸ªè¾“å…¥ã€‚æˆ‘ä»¬ä¼šä¸ºæ¯ä¸€ä¸ªé¢œè‰²åˆ¶ä½œæŸç§RGBè¡¨ç¤ºã€‚ä½†æ˜¯ã€‚
- en: We're dealing with a veryï¼Œ very simple exampleã€‚ I have these training set elementsã€‚
    and I' am setting the Y up so that the Y in this caseï¼Œ it's saying heyã€‚ one colored
    car went by the second one is a two colored carã€‚ then a three colored carã€‚ Then
    we have an example of a two colored car againã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¤„ç†çš„æ˜¯ä¸€ä¸ªéå¸¸éå¸¸ç®€å•çš„ä¾‹å­ã€‚æˆ‘æœ‰è¿™äº›è®­ç»ƒé›†å…ƒç´ ã€‚æˆ‘æ­£åœ¨è®¾ç½®Yï¼Œåœ¨è¿™ä¸ªæƒ…å†µä¸‹ï¼Œå®ƒåœ¨è¯´ï¼Œå˜¿ã€‚ä¸€è¾†å•è‰²çš„è½¦ç»è¿‡ï¼Œç¬¬äºŒè¾†æ˜¯åŒè‰²è½¦ï¼Œç„¶åæ˜¯ä¸€è¾†ä¸‰è‰²è½¦ã€‚ç„¶åæˆ‘ä»¬åˆæœ‰ä¸€è¾†åŒè‰²è½¦çš„ä¾‹å­ã€‚
- en: and it's two and another example of three and oneã€‚ it's teaching the neural
    network that no matter where at in that sequence it is at you it's still that
    colored car just this one car it barely made it into our time slice because it
    showed up later than these other onesã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸¤ä¸ªï¼Œå¦å¤–ä¸€ä¸ªä¾‹å­æ˜¯ä¸‰ä¸ªå’Œä¸€ä¸ªã€‚å®ƒåœ¨æ•™ç¥ç»ç½‘ç»œï¼Œæ— è®ºåœ¨åºåˆ—ä¸­çš„å“ªä¸ªä½ç½®ï¼Œå®ƒä»ç„¶æ˜¯é‚£è¾†æœ‰é¢œè‰²çš„è½¦ï¼Œåªæ˜¯è¿™è¾†è½¦åˆšå¥½è¿›å…¥äº†æˆ‘ä»¬çš„æ—¶é—´åˆ‡ç‰‡ï¼Œå› ä¸ºå®ƒæ¯”å…¶ä»–è½¦æ™šå‡ºç°ã€‚
- en: but it's still a one colored car it the same as this car up here that occurred
    very earlyã€‚ So what we do next is we take all of these elementsã€‚ and we're going
    to train itã€‚ Now by the wayã€‚ we could represent and there's just one inputã€‚ we
    could represent this asã€‚An oldcho neural network we would have oneï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ã€‚
    We would have six inputsã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ä»ç„¶æ˜¯åŒä¸€è¾†é¢œè‰²çš„è½¦ï¼Œå’Œä¸Šé¢å¾ˆæ—©å‡ºç°çš„è¿™è¾†è½¦æ˜¯ä¸€æ ·çš„ã€‚é‚£ä¹ˆæ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„æ˜¯æŠŠæ‰€æœ‰è¿™äº›å…ƒç´ æ‹¿å‡ºæ¥ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºå¹¶ä¸”åªæœ‰ä¸€ä¸ªè¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå®ƒè¡¨ç¤ºä¸ºä¸€ä¸ªæ—§çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ä¼šæœ‰1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ã€‚æˆ‘ä»¬å°†ä¼šæœ‰å…­ä¸ªè¾“å…¥ã€‚
- en: but the problem is the one and we would have to get rid of all these we would
    if we wanted to do that we would basically get rid of all these whoops not get
    rid of the numberã€‚ but we would get rid of all these square square brackets and
    we would make them all look like that and it would just be classic inputs we don't
    want to do that we're treating these as sequences but even if they were classic
    inputsã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é—®é¢˜æ˜¯è¿™ä¸ªä¸€ï¼Œæˆ‘ä»¬éœ€è¦å»æ‰æ‰€æœ‰è¿™äº›ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¿™ä¹ˆåšï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šä¼šå»æ‰æ‰€æœ‰è¿™äº›æ–¹æ‹¬å·ï¼Œä½¿å®ƒä»¬çœ‹èµ·æ¥åƒé‚£æ ·ï¼Œå®ƒå°†åªæ˜¯ç»å…¸çš„è¾“å…¥ã€‚æˆ‘ä»¬ä¸æƒ³è¿™æ ·åšï¼Œæˆ‘ä»¬æŠŠè¿™äº›å½“ä½œåºåˆ—å¤„ç†ï¼Œå³ä½¿å®ƒä»¬æ˜¯ç»å…¸çš„è¾“å…¥ã€‚
- en: these were all these were all this would be input1 input2 input3ã€‚ if you move
    something that was on input1 to input3ã€‚ that's a whole different pattern recognition
    to the neural network you can't just flexibly move these ones to way over here
    and have it still recognize it in classic neural networks and LSTms you absolutely
    can't So this neural network has trained and we have the output here and you see
    the results from training we trained it for 200 epos that ran for a little whileã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›éƒ½æ˜¯è¾“å…¥1ã€è¾“å…¥2ã€è¾“å…¥3ã€‚å¦‚æœä½ æŠŠåœ¨è¾“å…¥1ä¸Šçš„æŸä¸ªä¸œè¥¿ç§»åŠ¨åˆ°è¾“å…¥3ï¼Œé‚£å°†æ˜¯å®Œå…¨ä¸åŒçš„æ¨¡å¼è¯†åˆ«ï¼Œå¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œä½ ä¸èƒ½éšæ„åœ°æŠŠè¿™äº›ç§»åŠ¨åˆ°è¿™é‡Œå¹¶è®©å®ƒä»ç„¶è¯†åˆ«å‡ºæ¥ã€‚åœ¨ç»å…¸ç¥ç»ç½‘ç»œå’ŒLSTMä¸­ï¼Œä½ ç»å¯¹ä¸èƒ½ã€‚æ‰€ä»¥è¿™ä¸ªç¥ç»ç½‘ç»œå·²ç»è®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¾—åˆ°äº†è¾“å‡ºï¼Œä½ å¯ä»¥çœ‹åˆ°è®­ç»ƒçš„ç»“æœï¼Œæˆ‘ä»¬è®­ç»ƒäº†200ä¸ªå‘¨æœŸï¼Œè¿è¡Œäº†ä¸€æ®µæ—¶é—´ã€‚
- en: Not too badã€‚ It was running in the background while I was explaining thingsã€‚
    And there we we have itã€‚ So now for this neural networkï¼Œ I have this code down
    here and I can try examples on it now let me this code is meant that I can just
    modify it in any way than I want to So this is a live demo I don't know exactly
    what it's going to produce I hope it's going to produce something that makes it
    look smart that's the idea So this is two so this is a twocolored car that happened
    to be going through here I can run it and I hope it will say which it does if
    that twocoled car occurred here got rid of an extra price that I didn't want to
    it should still say two or not if I forgot a comm my computer was not in insert
    mode anymore So let's go ahead and run that there it does happen to confuse it
    if you get near the beginning so that's two as So if we moved it more over there
    it should still see it as a two if we make itã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜ä¸é”™ã€‚åœ¨æˆ‘è§£é‡Šçš„è¿‡ç¨‹ä¸­ï¼Œå®ƒåœ¨åå°è¿è¡Œã€‚ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†ç»“æœã€‚æ‰€ä»¥ç°åœ¨å¯¹äºè¿™ä¸ªç¥ç»ç½‘ç»œï¼Œæˆ‘åœ¨è¿™é‡Œæœ‰è¿™ä¸ªä»£ç ï¼Œæˆ‘å¯ä»¥åœ¨ä¸Šé¢å°è¯•ä¾‹å­ã€‚è®©æˆ‘è¯´ï¼Œè¿™æ®µä»£ç çš„è®¾è®¡æ˜¯æˆ‘å¯ä»¥éšæ„ä¿®æ”¹ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå®æ—¶æ¼”ç¤ºï¼Œæˆ‘ä¸çŸ¥é“å®ƒå°†äº§ç”Ÿä»€ä¹ˆï¼Œå¸Œæœ›å®ƒèƒ½äº§ç”Ÿä¸€äº›çœ‹èµ·æ¥å¾ˆèªæ˜çš„ä¸œè¥¿ï¼Œè¿™å°±æ˜¯æƒ³æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªä¸¤è‰²çš„è½¦ï¼Œæ°å¥½ç»è¿‡è¿™é‡Œï¼Œæˆ‘å¯ä»¥è¿è¡Œå®ƒï¼Œå¸Œæœ›å®ƒä¼šè¯†åˆ«å‡ºæ¥ï¼Œç»“æœç¡®å®å¦‚æ­¤ã€‚å¦‚æœé‚£è¾†åŒè‰²è½¦åœ¨è¿™é‡Œå‡ºç°ï¼Œå»æ‰äº†æˆ‘ä¸æƒ³è¦çš„å¤šä½™ä»·æ ¼ï¼Œå®ƒä»ç„¶åº”è¯¥è¯´æ˜¯äºŒï¼Œæˆ–è€…å¦‚æœæˆ‘å¿˜è®°äº†ä¸€ä¸ªé€—å·ï¼Œæˆ‘çš„ç”µè„‘ä¸å†å¤„äºæ’å…¥æ¨¡å¼ã€‚è®©æˆ‘ä»¬ç»§ç»­è¿è¡Œï¼Œè¿™ç¡®å®ä¼šè®©å®ƒæ··æ·†ï¼Œå¦‚æœä½ æ¥è¿‘å¼€å¤´ã€‚æ‰€ä»¥è¿™æ˜¯äºŒã€‚å¦‚æœæˆ‘ä»¬æŠŠå®ƒç§»å¾—æ›´è¿œï¼Œå®ƒä»ç„¶åº”è¯¥è§†ä¸ºäºŒã€‚
- en: LThat could mean it's a longer carï¼Œ or it could mean that it was a slower moving
    carã€‚ It still should say twoï¼Œ if we switch this all over to onesï¼Œ it should recognizeã€‚That
    this is a one colored carï¼Œ which it doesã€‚ If I don't knowã€‚ this is a one colored
    car with a little bit of red two color paint in it may be maybe one is blueã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æ„å‘³ç€å®ƒæ˜¯ä¸€è¾†æ›´é•¿çš„è½¦ï¼Œæˆ–è€…å¯èƒ½æ„å‘³ç€å®ƒæ˜¯ä¸€è¾†è¡Œé©¶è¾ƒæ…¢çš„è½¦ã€‚å¦‚æœæˆ‘ä»¬æŠŠæ‰€æœ‰çš„æ”¹æˆä¸€ï¼Œå®ƒåº”è¯¥èƒ½è¯†åˆ«å‡ºè¿™æ˜¯ä¸€è¾†å•è‰²è½¦ï¼Œå®ƒç¡®å®å¦‚æ­¤ã€‚å¦‚æœæˆ‘ä¸çŸ¥é“ï¼Œè¿™æ˜¯ä¸€è¾†å•è‰²è½¦ï¼Œå¯èƒ½å¸¦æœ‰ä¸€ç‚¹çº¢è‰²çš„æ¶‚æ–™ï¼Œä¹Ÿè®¸ä¸€éƒ¨åˆ†æ˜¯è“è‰²ã€‚
- en: 2 is redã€‚ See what it does thereã€‚ I have no idea what it'll do thereã€‚ I recognize
    it as it as a two carï¼Œ but at least it doesn't recognize it as say a three or
    something such as thatã€‚So this is learning sequenceã€‚ You can see in just a veryï¼Œ
    very simpleã€‚ simple example that it learns toã€‚Recognize these patternsã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 2æ˜¯çº¢è‰²ã€‚çœ‹çœ‹å®ƒåœ¨é‚£é‡Œçš„è¡¨ç°ã€‚æˆ‘ä¸çŸ¥é“å®ƒä¼šæ€ä¹ˆåšã€‚æˆ‘å°†å®ƒè¯†åˆ«ä¸ºä¸€è¾†ä¸¤å¢è½¦ï¼Œä½†è‡³å°‘å®ƒä¸æŠŠå®ƒè¯†åˆ«ä¸ºä¸‰å¢è½¦æˆ–å…¶ä»–ä¸œè¥¿ã€‚æ‰€ä»¥è¿™æ˜¯å­¦ä¹ åºåˆ—ã€‚ä½ å¯ä»¥åœ¨ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­ä¸­çœ‹åˆ°å®ƒæ˜¯å¦‚ä½•å­¦ä¹ è¯†åˆ«è¿™äº›æ¨¡å¼çš„ã€‚
- en: and it can even be very short and it recognizes itã€‚So that's the power of an
    LSTMã€‚ It recognizes these these patterns really sort of over timeã€‚ Next we're
    going to look at sunspots exampleã€‚ Now you can get daily sunspot data files from
    this website I have them loaded onto my instance but you would have to download
    these if you want to run this particular example So let's go ahead and read this
    inã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç”šè‡³å¯ä»¥éå¸¸çŸ­ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¯†åˆ«å®ƒã€‚è¿™å°±æ˜¯LSTMçš„å¼ºå¤§ä¹‹å¤„ã€‚å®ƒèƒ½å¤Ÿè¯†åˆ«è¿™äº›æ¨¡å¼ï¼Œç¡®å®æ˜¯éšç€æ—¶é—´çš„æ¨ç§»ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†çœ‹çœ‹å¤ªé˜³é»‘å­ç¤ºä¾‹ã€‚ç°åœ¨ä½ å¯ä»¥ä»è¿™ä¸ªç½‘ç«™è·å–æ¯æ—¥å¤ªé˜³é»‘å­æ•°æ®æ–‡ä»¶ï¼Œæˆ‘å·²ç»å°†å®ƒä»¬åŠ è½½åˆ°æˆ‘çš„å®ä¾‹ä¸­ï¼Œä½†å¦‚æœä½ æƒ³è¿è¡Œè¿™ä¸ªç‰¹å®šçš„ç¤ºä¾‹ï¼Œä½ éœ€è¦ä¸‹è½½è¿™äº›æ–‡ä»¶ã€‚è®©æˆ‘ä»¬ç»§ç»­è¯»å–è¿™ä¸ªæ•°æ®ã€‚
- en: This shows you basically the year and it goes back pretty far so 1818 first
    month first day it gives you and by the way this value is just a way of encoding
    the date in the year it gives you the sunspot value negative one means that we
    don't have it and the observation number number of observation So there's quite
    a bit of missing data near the beginning of the file if we run it and we trim
    the rows that have missing observations we have 11000 and something and we can
    also just takeã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ˜¾ç¤ºäº†å¹´ä»½ï¼Œè€Œä¸”å›æº¯å¾—ç›¸å½“è¿œï¼Œæ‰€ä»¥1818å¹´ç¬¬ä¸€æœˆç¬¬ä¸€å¤©ï¼Œå®ƒç»™ä½ æä¾›äº†è¿™ä¸ªå€¼ï¼Œé¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè¿™ä¸ªå€¼åªæ˜¯ç¼–ç å¹´ä»½æ—¥æœŸçš„ä¸€ç§æ–¹å¼ï¼Œå®ƒç»™å‡ºäº†å¤ªé˜³é»‘å­å€¼ï¼Œè´Ÿä¸€æ„å‘³ç€æˆ‘ä»¬æ²¡æœ‰æ•°æ®ï¼Œè¿˜æœ‰è§‚å¯Ÿæ•°é‡ã€‚å¦‚æœæˆ‘ä»¬è¿è¡Œå®ƒå¹¶ä¸”å»æ‰ç¼ºå°‘è§‚å¯Ÿçš„æ•°æ®è¡Œï¼Œæˆ‘ä»¬ä¼šæœ‰11000å¤šæ¡è®°å½•ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥å–ã€‚
- en: The what we're going to do is we're going to take the training set of everything
    before the year 2000ã€‚ We're going to take the test set that we're going to evaluate
    it on is everything after 2000ã€‚ And we create a training set for each of theseã€‚
    So we create a set of pandas for the training one for the test for the sunspot
    valueã€‚ That's what we're trying to predict the actual sunspot countã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åšçš„æ˜¯å–2000å¹´å‰çš„è®­ç»ƒé›†ã€‚æˆ‘ä»¬è¦è¯„ä¼°çš„æµ‹è¯•é›†æ˜¯2000å¹´åçš„æ‰€æœ‰æ•°æ®ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºä¸€ä¸ªè®­ç»ƒé›†ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«åˆ›å»ºä¸€ä¸ªpandasé›†åˆï¼Œç”¨äºå¤ªé˜³é»‘å­çš„å€¼ã€‚è¿™å°±æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„å®é™…å¤ªé˜³é»‘å­æ•°é‡ã€‚
- en: And we print out the number of observations of each that we haveã€‚ So the training
    data are definitely biggerã€‚ You have nearly 55ã€‚ you have over 55000 test set has
    a little over 6000 valuesã€‚ Now what we've got to do is convert this into sequencesã€‚
    And this is the this is perhaps the somewhat tricky part of thisã€‚ So this takes
    that sunspot data like we had and converts it into a cube like we're going to
    use to train the LSTM with to do this what weã€‚is we have to use this two sequences
    functionã€‚ So this gives you the sequence size and then the observationsã€‚ So the
    sequence size is going to be what we define the sequence size to beã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰“å°å‡ºæ¯ä¸ªè§‚å¯Ÿçš„æ•°é‡ã€‚æ‰€ä»¥è®­ç»ƒæ•°æ®ç»å¯¹æ›´å¤§ï¼Œä½ å‡ ä¹æœ‰55000ä¸ªï¼Œæµ‹è¯•é›†ç•¥å¤šäº6000ä¸ªå€¼ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦åšçš„æ˜¯å°†å…¶è½¬æ¢ä¸ºåºåˆ—ã€‚è¿™å¯èƒ½æ˜¯ç¨å¾®æ£˜æ‰‹çš„éƒ¨åˆ†ã€‚æ‰€ä»¥è¿™ä¼šå°†å¤ªé˜³é»‘å­æ•°æ®è½¬æ¢æˆä¸€ä¸ªç«‹æ–¹ä½“ï¼Œç”¨äºè®­ç»ƒLSTMï¼Œä¸ºæ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨è¿™ä¸ªä¸¤ä¸ªåºåˆ—çš„å‡½æ•°ã€‚è¿™ç»™ä½ åºåˆ—å¤§å°å’Œè§‚å¯Ÿå€¼ã€‚æ‰€ä»¥åºåˆ—å¤§å°å°†æ˜¯æˆ‘ä»¬å®šä¹‰çš„ã€‚
- en: so it's going to be 10 you take the data and you chop out 10 observations then
    you move forward a little bit chop out the next 10ã€‚ the next 10 is a sliding window
    across and it builds all those rows of of the cubeã€‚ but with the observations
    on backã€‚ So here we have the we have it converted into that sequenceã€‚ So we take
    the window by getting the observations from one up to1 plus up to I plus the sequence
    sizeã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åºåˆ—å¤§å°å°†æ˜¯10ï¼Œä½ å–æ•°æ®å¹¶å‰ªæ‰10ä¸ªè§‚å¯Ÿå€¼ï¼Œç„¶åå‘å‰ç§»åŠ¨ä¸€ç‚¹ï¼Œå‰ªæ‰ä¸‹ä¸€ä¸ª10ã€‚ä¸‹ä¸€ä¸ª10æ˜¯ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼Œæ„å»ºå‡ºæ‰€æœ‰çš„ç«‹æ–¹ä½“è¡Œï¼Œä½†è§‚å¯Ÿå€¼æ˜¯åœ¨åé¢çš„ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºè¿™ä¸ªåºåˆ—ã€‚æˆ‘ä»¬é€šè¿‡è·å–ä»1åˆ°1åŠ ä¸ŠIåŠ ä¸Šåºåˆ—å¤§å°çš„è§‚å¯Ÿå€¼æ¥å–çª—å£ã€‚
- en: we're looping over the entire range of observationsã€‚ So whatever the length
    of observations is up to minus the sequence size so that we stop while we still
    have enough to build out an entire sequence and we essentially build build this
    up if we looked at whatã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨å¾ªç¯æ•´ä¸ªè§‚å¯ŸèŒƒå›´ã€‚æ‰€ä»¥ä¸è®ºè§‚å¯Ÿçš„é•¿åº¦æ˜¯å¤šå°‘ï¼Œç›´åˆ°å‡å»åºåˆ—å¤§å°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨ä»æœ‰è¶³å¤Ÿæ•°æ®æ„å»ºå‡ºå®Œæ•´åºåˆ—æ—¶åœæ­¢ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šä¼šæ„å»ºè¿™ä¸ªã€‚
- en: This really looked like X trainï¼Œ I print out the shape of them up hereï¼Œ the
    shape of themã€‚ so it's got 55ï¼Œ000 rowsï¼Œ it's gotã€‚The 10ï¼Œ which is the sequenceã€‚
    but it's only got the one column because it's just the value that we're trying
    to predict over timeã€‚ The number of sunpotsã€‚ we just print out X trainã€‚ This is
    kind of what it looks likeã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥çœŸçš„åƒæ˜¯Xè®­ç»ƒï¼Œæˆ‘åœ¨è¿™é‡Œæ‰“å°å‡ºå®ƒä»¬çš„å½¢çŠ¶ï¼Œå½¢çŠ¶æœ‰55000è¡Œï¼Œæœ‰10ï¼Œè¿™æ˜¯åºåˆ—ã€‚ä½†å®ƒåªæœ‰ä¸€åˆ—ï¼Œå› ä¸ºå®ƒåªæ˜¯æˆ‘ä»¬è¯•å›¾é¢„æµ‹çš„æ—¶é—´å€¼ï¼Œå³å¤ªé˜³é»‘å­çš„æ•°é‡ã€‚æˆ‘ä»¬åªæ‰“å°å‡ºXè®­ç»ƒã€‚è¿™å°±æ˜¯å®ƒçš„æ ·å­ã€‚
- en: You're seeing the individual sunpot values going acrossã€‚ It's a three dimensional
    data structureã€‚ So you have the 255ï¼Œ2ï¼Œ55 up to hereã€‚ that's one rowã€‚ And you have
    all of the sunpot valuesã€‚ the 10 of them acrossã€‚ There's not enough to display
    10ã€‚ So that's why you've got the the three dotsã€‚ Now we are going to try to build
    the model and fit itã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°ä¸ªåˆ«çš„å¤ªé˜³é»‘å­å€¼åœ¨è¿™é‡Œç§»åŠ¨ã€‚è¿™æ˜¯ä¸€ä¸ªä¸‰ç»´æ•°æ®ç»“æ„ã€‚æ‰€ä»¥ä½ æœ‰255ï¼Œ2ï¼Œ55åˆ°è¿™é‡Œã€‚è¿™æ˜¯ä¸€è¡Œã€‚ä½ æœ‰æ‰€æœ‰çš„å¤ªé˜³é»‘å­å€¼ï¼Œ10ä¸ªåœ¨ä¸€èµ·ã€‚æ²¡æœ‰è¶³å¤Ÿçš„å±•ç¤º10ä¸ªã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ æœ‰ä¸‰ä¸ªç‚¹ã€‚ç°åœ¨æˆ‘ä»¬å°†å°è¯•æ„å»ºæ¨¡å‹å¹¶æ‹Ÿåˆå®ƒã€‚
- en: When we run thisã€‚ it's going to train it for 1000 epochsã€‚ And that takes a little
    bit of timeã€‚ We do have an early stopping going onã€‚ So it's not going to take
    thatã€‚It's not really going to take that full amount of timeã€‚ this is potentially
    something that a GPU instance would train a lot fasterã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è¿è¡Œè¿™ä¸ªæ—¶ï¼Œå®ƒå°†è®­ç»ƒ1000ä¸ªå‘¨æœŸã€‚è¿™éœ€è¦ä¸€ç‚¹æ—¶é—´ã€‚æˆ‘ä»¬ç¡®å®å¼€å¯äº†æ—©åœæœºåˆ¶ï¼Œå› æ­¤ä¸ä¼šçœŸæ­£èŠ±è´¹é‚£ä¹ˆå¤šæ—¶é—´ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªGPUå®ä¾‹è®­ç»ƒä¼šå¿«å¾—å¤šçš„æƒ…å†µã€‚
- en: but that we'll see an example of that on the last class use when we do high
    performance computing and we make use of a GPU instance among other thingsã€‚So
    we've got 6131 samples that we're training onã€‚ we report the validation lossã€‚
    notice it's quickly dropping after we let it go a whileï¼Œ it won't hit the entire
    1000 it will stopã€‚ but I'll let this run on time lapse here real quickã€‚ All rightï¼Œ
    early stopping has kicked inã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬å°†åœ¨æœ€åä¸€èŠ‚è¯¾ä¸­çœ‹åˆ°ä¸€ä¸ªä¾‹å­ï¼Œå½“æ—¶æˆ‘ä»¬è¿›è¡Œé«˜æ€§èƒ½è®¡ç®—ï¼Œå¹¶åˆ©ç”¨GPUå®ä¾‹ç­‰å…¶ä»–å·¥å…·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰6131ä¸ªæ ·æœ¬æ­£åœ¨è®­ç»ƒã€‚æˆ‘ä»¬æŠ¥å‘ŠéªŒè¯æŸå¤±ã€‚æ³¨æ„ï¼Œåœ¨è®©å®ƒè¿è¡Œä¸€æ®µæ—¶é—´åï¼ŒæŸå¤±è¿…é€Ÿä¸‹é™ï¼Œå®ƒä¸ä¼šè¾¾åˆ°1000ä¸ªå‘¨æœŸçš„å…¨éƒ¨æ—¶é—´ï¼Œä¼šæå‰åœæ­¢ã€‚ä¸è¿‡æˆ‘ä¼šè®©è¿™ä¸ªè¿‡ç¨‹åœ¨æ—¶é—´å¿«è¿›ä¸­è¿è¡Œä¸€ä¸‹ã€‚å¥½çš„ï¼Œæ—©åœæœºåˆ¶å·²ç»ç”Ÿæ•ˆã€‚
- en: you can see that that kicked in after 16 Ex and we now have the neural network
    trained and ready to go and we can see what the RMSsCã€‚ So it's predicting all
    plus or minus 22 sunspots and can that gives you an idea of the overall accuracyã€‚
    It's not particularly advanced networkï¼Œ but it does show how you represent the
    data going in breaking up into those sequences with with the two sequences function
    that I gave you is definitely the key part of doing thatã€‚ Thank you for watching
    this videoã€‚ And in next videoã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°åœ¨16ä¸ªExä¹‹åè¿™ä¸€ç‚¹å¼€å§‹ç”Ÿæ•ˆï¼Œæˆ‘ä»¬ç°åœ¨å·²ç»è®­ç»ƒå¥½äº†ç¥ç»ç½‘ç»œå¹¶å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥çœ‹åˆ°RMSsCã€‚æ‰€ä»¥å®ƒé¢„æµ‹äº†å¤§çº¦22ä¸ªå¤ªé˜³é»‘å­ï¼Œè¿™ç»™äº†ä½ ä¸€ä¸ªæ•´ä½“å‡†ç¡®æ€§çš„æ¦‚å¿µã€‚è¿™ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«å…ˆè¿›çš„ç½‘ç»œï¼Œä½†å®ƒç¡®å®å±•ç¤ºäº†å¦‚ä½•è¡¨ç¤ºè¾“å…¥æ•°æ®ï¼Œå°†å…¶åˆ†è§£æˆé‚£ä¸¤ç»„åºåˆ—ï¼Œæ­£æ˜¯æˆ‘ç»™ä½ çš„é‚£ä¸¤ä¸ªåºåˆ—å‡½æ•°æ˜¯å®ç°è¿™ä¸€ç‚¹çš„å…³é”®éƒ¨åˆ†ã€‚æ„Ÿè°¢ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚ä¸‹ä¸€æœŸè§†é¢‘è§ã€‚
- en: we're going to look at how we can use LSTMs and CNNNs togetherã€‚![](img/d578a667a9760200144c8f5677c9fef3_3.png)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•å°†LSTMså’ŒCNNNsç»“åˆä½¿ç”¨ã€‚![](img/d578a667a9760200144c8f5677c9fef3_3.png)
- en: To caption images this content changes oftenï¼Œ so subscribe to the channel to
    stay up to date on this course and other topics in artificial intelligenceã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸ºå›¾åƒåŠ ä¸Šè¯´æ˜ï¼Œè¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ï¼Œæ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£è¿™é—¨è¯¾ç¨‹å’Œäººå·¥æ™ºèƒ½çš„å…¶ä»–ä¸»é¢˜ã€‚
