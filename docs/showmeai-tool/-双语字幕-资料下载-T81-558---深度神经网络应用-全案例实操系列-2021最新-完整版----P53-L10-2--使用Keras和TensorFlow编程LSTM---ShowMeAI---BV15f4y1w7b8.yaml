- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P53ÔºöL10.2- ‰ΩøÁî®KerasÂíåTensorFlowÁºñÁ®ãLSTM
    - ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå this is Jeff Hea and„ÄÇ Wecome to applications of deep neural networkss with
    Washington University„ÄÇ In this videoÔºå we're going to look at LSTM and GU neural
    networks for time series for the latest on my AI course and projects„ÄÇ click subscribe
    in the bell next to it to be notified of every new video„ÄÇ we are looking at recurrent
    neural networks„ÄÇ Recurrent neural networks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there's a lot of different types of these„ÄÇ and we'll start by looking at one
    of the most classic types„ÄÇ mainly because it's relatively easy to understand„ÄÇ
    And then we'll look at the more modern ones„ÄÇ the LSTms and the Gs„ÄÇ there are definitely
    some similarities between all of these different types of recurrent neural networks„ÄÇ
    So the recurrent neural networks„ÄÇ They usually have some concept of a context
    neuron„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The context neuron represents a short of short term memory„ÄÇ It holds a value
    between calls to the neural network„ÄÇ So when you're trying to predict with the
    neural network„ÄÇ That is one call to the neural network„ÄÇüòä„ÄÇ![](img/d578a667a9760200144c8f5677c9fef3_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: Then you call it again and again and againÔºå as you predict row row row„ÄÇ the
    context neurons start out as zeroÔºå but they hold a value as subsequent inputs
    come to the neural network as a sequence is being processed„ÄÇ So before we saw
    that we have these sequences that had values for each of the input neurons coming
    in„ÄÇ these context neurons have a zero at the beginning of each sequence and they
    have the gain of value and they keep updating their value as these sequences are
    processed„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: When you move to the next sequenceÔºå this value always goes back to zero„ÄÇ the
    context always go to zero between sequencesÔºå That's a very important characteristic
    of them„ÄÇ So this is an example of two hidden neurons down here that have a that
    have context„ÄÇ So instead of simply feeding forward like we have before„ÄÇWe're now
    going to receive some reflux„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: some data back from what we're outputting„ÄÇ So you have the input„ÄÇ This is input
    1Ôºå input 2„ÄÇ going into Hi 1 and Hi 2„ÄÇ These neurons are going to output something
    based on the weighted„ÄÇ sum values that come into them„ÄÇ and then the activation
    function„ÄÇ They're going to produce some output„ÄÇAnd in classic neural network processing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that output would just go on next to the next layer or out of the neural network
    if this was the final„ÄÇ but in this case„ÄÇWe're going to take the same value that
    was output from the hidden neuron onward„ÄÇ and that dotted line means we're going
    to copy it to the context„ÄÇ So typically you'll have one context neuron per neuron
    on the previous layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and you're simply going to copy the output„ÄÇ There is no weight„ÄÇ there is nothing„ÄÇ
    It is simply a copy„ÄÇ The solid lines have weights„ÄÇ So what's going to happen is
    the the input to to this first hidden hidden1 is not just going to be what's coming
    from the previous layer„ÄÇ but it's going to get the output from context1„ÄÇ and it's
    also going to get the output from context2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: at least in this particular case„ÄÇ The way that this neuron is is set up„ÄÇ this
    network is set up„ÄÇ These are both weighted„ÄÇ So it will learn how to process that
    valueÔºå there is no weighting here„ÄÇ simply whatever was output„ÄÇTo the next layer
    is also copied to here„ÄÇ So you're constantly keeping the value from the previous
    output from the hidden neuron„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And allowing that to stay until the next callÔºå and then you copy it back„ÄÇ you
    let it form the input back to the hidden one on on the next call„ÄÇ and it becomes
    just part of the input„ÄÇ So reallyÔºå it's like there's three input neurons to hidden
    one„ÄÇ but two of those are the context layers that are just the outputs from the
    previous one„ÄÇ Now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this output really lives on„ÄÇ It's not like it just affects the nextÔºå the next
    one and so on„ÄÇ Since the value that was copied into this context„ÄÇ it's fed back
    into here„ÄÇ it refluxes„ÄÇ It's almost like butterfly effect„ÄÇ It has an effect on
    what this is going this neuron' is going to output„ÄÇ which then gets copied into
    here and here and it keeps going for each subsequent call of of this neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That is how the context affects the output over time„ÄÇ and that is what allows
    the neural network to be time series„ÄÇ and the fact that these solid lines that
    are causing the copy backwards through the neural network„ÄÇ The fact that they're
    weighted allows the neural network to learn what to do with this data over time
    as it comes into the neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That is a time series neural network„ÄÇ So LSTM neural networks are a way of having
    that sort of memory„ÄÇ NowÔºå the really almost layperson a way that I can describe
    this before we go into it really in too much detail would be if we had a scientific
    sort of calculator„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we won't even use any of the fancy features of this calculator„ÄÇ we care mainly
    about memory plus remember and memory clear„ÄÇ LSTM neural networks and gated recurrent
    GRU neural networks have gates and these„ÄÇare the real innovation of this type
    of neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: They let it remember things for a long time or a short time„ÄÇ There are three
    gates that you're dealing with„ÄÇThe input gate„ÄÇ the forget gate and the recall
    or sometimes the output gate„ÄÇ you can think of these gates as dealing with the
    memory of the neural network or the context„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can think of as the memory buttons on a calculator„ÄÇ So say we have the output
    from the neuron is5„ÄÇ We have these three gates„ÄÇ And the three gates„ÄÇ let you know
    what you should do„ÄÇ Should you remember it„ÄÇ That would be like memory plus So
    I put it into there„ÄÇ we have that five now in the context„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: when it next worksÔºå we might need to decide if we should recall it„ÄÇ That's the
    MR button„ÄÇ that's that's the output or the recall gate„ÄÇ If you do thatÔºå your5
    comes back„ÄÇ And then there's also the forget gate„ÄÇ The forget gate means something
    happens with the neural network input„ÄÇ it decides it needs to drop the memory
    and you can now clear it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And if you do you can't even do an MR because there is no There is nothing remembered„ÄÇ
    It's important for these recurrent neural networks to be able to forget„ÄÇ And that
    was a limitation of the„ÄÇOf the element neural network„ÄÇ you would just have to
    slowly forget over time The other limitation was you couldn't hold something for
    a long time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so the fact that these new neural networks have the gates„ÄÇ you can remember
    something and put it into your memory and then forget it later over time It's
    Otoman said as far as human beings„ÄÇ one of our greatest strengths and weaknesses„ÄÇIs
    to be able to forget if we remembered everything„ÄÇ our minds would be simply too
    cluttered and we would not be able to function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The important thing is knowing what to forget and forgetting only things that
    are unimportant„ÄÇ So let's look at the LSTM neural network„ÄÇ This is a diagram of
    the LSTM„ÄÇ you have your output which is essentially Y hat for the current timeÔºå
    previous timeÔºå next time„ÄÇ Now this is not three different LSTMs„ÄÇ This is the same
    LSTM and the LSTM is like a neuron in in the network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you'll have multiple LSTMsÔºå just like you have multiple hidden neurons And you
    have input„ÄÇ this is the input over time„ÄÇ So you just have a single input value
    for the current previous and next„ÄÇ and over time„ÄÇ each each the same LSTM over
    time is passing the Y hat to the next one„ÄÇ So the predictionÔºå but it's also keeping
    a context over time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: all these networks have some sort of context be„ÄÇneural networks„ÄÇ classic Elman
    and Jordan neural networks or be they more modern LSTM and GRU units internally„ÄÇ
    this is what's going on with a LSTM„ÄÇ you've got these gates forge gate input gate
    output gate and you've got a variety of clamping of thresholdholding functions
    sigmoids and a tan H'll see those in a moment you can look through this diagram
    if you prefer it basically represents the same thing as these equations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I almost like to think think of these in terms of the equations and I'll take
    you through the equations pretty quickly This is essentially looking at a this
    is essentially looking at sort of a linear algebra computation So you've got the
    weights that's the weight F weight F is the forget gate weight I the input gate
    the bias for each of these layers so you have bias neurons and then this vector„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Of the output from the previous one and the input for the current„ÄÇ This is a
    vector that you're multiplying that weight over„ÄÇ So the first thing that you want
    to do with thisÔºå and by the wayÔºå S means a sigmoid function„ÄÇ10 H means a hyperbolic
    tangent before we talk about sigmoid and 10 H„ÄÇ let me show you that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: notice the shapes of these„ÄÇ This is a sigmoid function„ÄÇ it goes from0 to1„ÄÇ and
    it clamps these values„ÄÇ So if if it's very negative„ÄÇ it clamps it to0„ÄÇ It's a
    step function„ÄÇ It's very positiveÔºå it clamps it to one„ÄÇ hyperbolic tangent„ÄÇ and
    by the way„ÄÇ this has nothing to do with triometry„ÄÇ machine learning research just
    like the shape of these more„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's a step function„ÄÇ just another type of step function„ÄÇ But notice that
    the that the range is now negative1 to1„ÄÇ So it's„ÄÇIt's got a biggerÔºå bigger range„ÄÇ
    and sometimes we need these negative values and we'll see when we go through these
    equations that we do need those negative values„ÄÇ So let's go through the equations„ÄÇ
    The first thing we have to do is calculate the coefficients for the forget gate
    and the input gate and ultimately the output gate„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because these three valuesÔºå they're going to be largely zero or one because
    see these are where the sigmoids are used„ÄÇ you want those to be0 or1 because those
    valuesÔºå you want them on or off„ÄÇ It's a coefficient So forget if the forget gate
    is calculated to be0„ÄÇ it's kind of backwards a little bitÔºå but zero means we should
    forget one means we should remember„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we calculate the forget gate based on the sigmoid function„ÄÇ which flips it
    sort of into that 01 range by looking at the weight for forgetting„ÄÇ So this weight
    for forgettingÔºå that's the training capability„ÄÇ This value will change as the
    neural network learns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and by multiplying by the previous output and the current input that that vector
    and adding the bias„ÄÇ which is also a learning parameter„ÄÇ So by adjusting these
    twoÔºå we learn when to forget„ÄÇ if F becomes a0Ôºå we're going to forget„ÄÇ If I becomes
    oneÔºå we're going to remember„ÄÇ So you have the weightsÔºå these are exactly the same
    functions as before these are your two learning parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: those will be adjustedÔºå that's how it learns when to when to remember or when
    to feed the input in„ÄÇ C is your context„ÄÇ C with the little tilde above it just
    means your candidate context Now context„ÄÇIs the value that it's rememberingÔºå The
    value that it's remembering is the output from the neuron„ÄÇ The output from the
    neuroura can be negative one to one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can't use a sigmoid for that because negative1 to oneÔºå it's going to clip
    off half the values„ÄÇ anything below 0„ÄÇ So that's why we use a hyperbolic tangent
    is the hyperbolic tangent can flow between negative one and 1„ÄÇ So we we've got
    the fullÔºå the fuller range of values that we can deal with„ÄÇBut again„ÄÇ the training
    works just like just like before„ÄÇ we have a weight for the context„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that's how it's learning what to specifically do with the context„ÄÇ And we
    have a bias„ÄÇ This is just the candidate context„ÄÇ What really becomes the context
    is this„ÄÇ And now this is a different format because we're not we're not learningÔºå
    these are for learning„ÄÇ This is just a switch gate„ÄÇ Notice you have the forget
    and the input„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: input says what should go into the contextÔºå forget means should we remember
    the previous context„ÄÇ Plus just pipes both of those two together„ÄÇ So forgetting
    the context„ÄÇ if this is0 multiplied it's a coefficientÔºå So if it's a0„ÄÇ if's going
    to wipe out the previous context because0 times anything is 0 input means so this
    kills off the previous context if we're forgetting input„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is why it's like the memory plus button because this is your„ÄÇPrevious context„ÄÇ
    I'm sorry„ÄÇ this one's your previous context„ÄÇ This is the candidate context„ÄÇ So
    this is what we calculated up here that might be going into the context„ÄÇ if we
    should put something into So then the final now that you've got the output gate
    created„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: your final output is essentially going to is going to come from the previous
    context multiplied by your your output„ÄÇ that is essentially how the LSTM is calculated„ÄÇ
    There's another type of neural of recurrent neural network that is very similar
    to this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that is called the GRU„ÄÇ the gated recurrent unit„ÄÇ It works really extremely
    similar to this„ÄÇ me zoom this in a little bit„ÄÇ This is the academic paper„ÄÇ This
    is not the academic paper that introduced the GRU„ÄÇ but it's an empirical evaluation
    gated recurrent„ÄÇnetworksworks empirical just means experimental„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So they're not doing any sort of mathematical proof for handwaving„ÄÇ They're
    just literally trying to unt testest data and showing you what a GRU can actually
    accomplish„ÄÇ in a lot of casesÔºå GRs and LSTMs function really pretty similar and
    do sort of have a similar accuracy none This paper deals with evaluating more
    the processing time is a GRU does not have as many gates„ÄÇ and that makes it more
    simple„ÄÇ So you'll notice in the abstractÔºå they're saying that they evaluated„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: AlsoÔºå we found GRU to be comparable to LSTN and they also evaluate on previous
    nonrecurrent neural networks„ÄÇ So if we look through the paper the part that I
    want to show you that is particularly interesting„ÄÇIs you have here the LSTM that
    we just looked at with its gates„ÄÇ you can see the input gate up at the topÔºå the
    F gate there and the output gate there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so you have three gates in the recurrent or the GRU you just have the you just
    have two gates„ÄÇ R and Z so' it's a simpler sort of algorithm to that and it does
    not require nearly since there's another gate„ÄÇ it requires much less computation
    time and they show you how to calculate these gates„ÄÇBut at the veryÔºå very endÔºå
    the paper gets to showing the learning and showing really that the LSTM and GRU
    are somewhat equivalent and they show wall clock time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Notice hereÔºå the GRU is taking much less time to process than the G than the
    LSTM„ÄÇ So that's that's their advantage„ÄÇ You can process much more your your training
    impactpos will take epos will take much less„ÄÇ much less time„ÄÇ Now we're going
    to look at an actual example of LSTM„ÄÇ We're going to see two examples„ÄÇ The first
    example is just reallyÔºå really simple„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It's meant to show you„ÄÇWhat this recurrent type of neural network can do just
    in its most simple state and yet do something that a normal neural network would
    not be able to do„ÄÇ And then the next example will look like we'll look at an actual
    applications where we look at time series data for sunspots and we'll see how
    this type of recurrent neural network can predict something that is time series
    and be able to at least in a very basic way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: predict predict sunspot activity„ÄÇ So this is showing you how to build a very
    simple tensorflow through Kiras LSTM neural network„ÄÇ I'm going to go ahead and
    run this„ÄÇ It takes it a moment to trainÔºå but it's not too bad„ÄÇ So it's running
    there you can see that from the asterisk„ÄÇ This is showing the neural network how
    to predict something from time series„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This time series is essentially almost think of it like a camera in front of
    a house like just a pinhole„ÄÇCameraÔºå it sees a car driving by and it might see
    just a little bit of the paint color as the car is going by„ÄÇ So the same position
    we're looking straight through„ÄÇ we're seeing zero„ÄÇ That means we're seeing nothing„ÄÇ
    but then at the next time sliceÔºå we see a car„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then at the next time slice again„ÄÇ we still see the car because it's it's going
    in front of our pinhole that we're seeing„ÄÇAnd now we see zero again for the rest
    of the sequence because basically the car has cleared our pinhole and is moved
    on its way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So these are sequences hereÔºå a different colored car goes by a two colored car„ÄÇ
    whatever two happens to beÔºå maybe that's maybe one's redÔºå two's blueÔºå something
    such as that„ÄÇ ideallyÔºå if we wanted to make this more advancedÔºå we would probably
    have three inputs„ÄÇ and we would make this some sort of RGB for each and every
    single one of these„ÄÇ but„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We're dealing with a veryÔºå very simple example„ÄÇ I have these training set elements„ÄÇ
    and I' am setting the Y up so that the Y in this caseÔºå it's saying hey„ÄÇ one colored
    car went by the second one is a two colored car„ÄÇ then a three colored car„ÄÇ Then
    we have an example of a two colored car again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and it's two and another example of three and one„ÄÇ it's teaching the neural
    network that no matter where at in that sequence it is at you it's still that
    colored car just this one car it barely made it into our time slice because it
    showed up later than these other ones„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it's still a one colored car it the same as this car up here that occurred
    very early„ÄÇ So what we do next is we take all of these elements„ÄÇ and we're going
    to train it„ÄÇ Now by the way„ÄÇ we could represent and there's just one input„ÄÇ we
    could represent this as„ÄÇAn oldcho neural network we would have oneÔºå2Ôºå3Ôºå4Ôºå5Ôºå6„ÄÇ
    We would have six inputs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but the problem is the one and we would have to get rid of all these we would
    if we wanted to do that we would basically get rid of all these whoops not get
    rid of the number„ÄÇ but we would get rid of all these square square brackets and
    we would make them all look like that and it would just be classic inputs we don't
    want to do that we're treating these as sequences but even if they were classic
    inputs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: these were all these were all this would be input1 input2 input3„ÄÇ if you move
    something that was on input1 to input3„ÄÇ that's a whole different pattern recognition
    to the neural network you can't just flexibly move these ones to way over here
    and have it still recognize it in classic neural networks and LSTms you absolutely
    can't So this neural network has trained and we have the output here and you see
    the results from training we trained it for 200 epos that ran for a little while„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Not too bad„ÄÇ It was running in the background while I was explaining things„ÄÇ
    And there we we have it„ÄÇ So now for this neural networkÔºå I have this code down
    here and I can try examples on it now let me this code is meant that I can just
    modify it in any way than I want to So this is a live demo I don't know exactly
    what it's going to produce I hope it's going to produce something that makes it
    look smart that's the idea So this is two so this is a twocolored car that happened
    to be going through here I can run it and I hope it will say which it does if
    that twocoled car occurred here got rid of an extra price that I didn't want to
    it should still say two or not if I forgot a comm my computer was not in insert
    mode anymore So let's go ahead and run that there it does happen to confuse it
    if you get near the beginning so that's two as So if we moved it more over there
    it should still see it as a two if we make it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: LThat could mean it's a longer carÔºå or it could mean that it was a slower moving
    car„ÄÇ It still should say twoÔºå if we switch this all over to onesÔºå it should recognize„ÄÇThat
    this is a one colored carÔºå which it does„ÄÇ If I don't know„ÄÇ this is a one colored
    car with a little bit of red two color paint in it may be maybe one is blue„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 2 is red„ÄÇ See what it does there„ÄÇ I have no idea what it'll do there„ÄÇ I recognize
    it as it as a two carÔºå but at least it doesn't recognize it as say a three or
    something such as that„ÄÇSo this is learning sequence„ÄÇ You can see in just a veryÔºå
    very simple„ÄÇ simple example that it learns to„ÄÇRecognize these patterns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and it can even be very short and it recognizes it„ÄÇSo that's the power of an
    LSTM„ÄÇ It recognizes these these patterns really sort of over time„ÄÇ Next we're
    going to look at sunspots example„ÄÇ Now you can get daily sunspot data files from
    this website I have them loaded onto my instance but you would have to download
    these if you want to run this particular example So let's go ahead and read this
    in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This shows you basically the year and it goes back pretty far so 1818 first
    month first day it gives you and by the way this value is just a way of encoding
    the date in the year it gives you the sunspot value negative one means that we
    don't have it and the observation number number of observation So there's quite
    a bit of missing data near the beginning of the file if we run it and we trim
    the rows that have missing observations we have 11000 and something and we can
    also just take„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The what we're going to do is we're going to take the training set of everything
    before the year 2000„ÄÇ We're going to take the test set that we're going to evaluate
    it on is everything after 2000„ÄÇ And we create a training set for each of these„ÄÇ
    So we create a set of pandas for the training one for the test for the sunspot
    value„ÄÇ That's what we're trying to predict the actual sunspot count„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we print out the number of observations of each that we have„ÄÇ So the training
    data are definitely bigger„ÄÇ You have nearly 55„ÄÇ you have over 55000 test set has
    a little over 6000 values„ÄÇ Now what we've got to do is convert this into sequences„ÄÇ
    And this is the this is perhaps the somewhat tricky part of this„ÄÇ So this takes
    that sunspot data like we had and converts it into a cube like we're going to
    use to train the LSTM with to do this what we„ÄÇis we have to use this two sequences
    function„ÄÇ So this gives you the sequence size and then the observations„ÄÇ So the
    sequence size is going to be what we define the sequence size to be„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's going to be 10 you take the data and you chop out 10 observations then
    you move forward a little bit chop out the next 10„ÄÇ the next 10 is a sliding window
    across and it builds all those rows of of the cube„ÄÇ but with the observations
    on back„ÄÇ So here we have the we have it converted into that sequence„ÄÇ So we take
    the window by getting the observations from one up to1 plus up to I plus the sequence
    size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're looping over the entire range of observations„ÄÇ So whatever the length
    of observations is up to minus the sequence size so that we stop while we still
    have enough to build out an entire sequence and we essentially build build this
    up if we looked at what„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This really looked like X trainÔºå I print out the shape of them up hereÔºå the
    shape of them„ÄÇ so it's got 55Ôºå000 rowsÔºå it's got„ÄÇThe 10Ôºå which is the sequence„ÄÇ
    but it's only got the one column because it's just the value that we're trying
    to predict over time„ÄÇ The number of sunpots„ÄÇ we just print out X train„ÄÇ This is
    kind of what it looks like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You're seeing the individual sunpot values going across„ÄÇ It's a three dimensional
    data structure„ÄÇ So you have the 255Ôºå2Ôºå55 up to here„ÄÇ that's one row„ÄÇ And you have
    all of the sunpot values„ÄÇ the 10 of them across„ÄÇ There's not enough to display
    10„ÄÇ So that's why you've got the the three dots„ÄÇ Now we are going to try to build
    the model and fit it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: When we run this„ÄÇ it's going to train it for 1000 epochs„ÄÇ And that takes a little
    bit of time„ÄÇ We do have an early stopping going on„ÄÇ So it's not going to take
    that„ÄÇIt's not really going to take that full amount of time„ÄÇ this is potentially
    something that a GPU instance would train a lot faster„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but that we'll see an example of that on the last class use when we do high
    performance computing and we make use of a GPU instance among other things„ÄÇSo
    we've got 6131 samples that we're training on„ÄÇ we report the validation loss„ÄÇ
    notice it's quickly dropping after we let it go a whileÔºå it won't hit the entire
    1000 it will stop„ÄÇ but I'll let this run on time lapse here real quick„ÄÇ All rightÔºå
    early stopping has kicked in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can see that that kicked in after 16 Ex and we now have the neural network
    trained and ready to go and we can see what the RMSsC„ÄÇ So it's predicting all
    plus or minus 22 sunspots and can that gives you an idea of the overall accuracy„ÄÇ
    It's not particularly advanced networkÔºå but it does show how you represent the
    data going in breaking up into those sequences with with the two sequences function
    that I gave you is definitely the key part of doing that„ÄÇ Thank you for watching
    this video„ÄÇ And in next video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're going to look at how we can use LSTMs and CNNNs together„ÄÇ![](img/d578a667a9760200144c8f5677c9fef3_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: To caption images this content changes oftenÔºå so subscribe to the channel to
    stay up to date on this course and other topics in artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
