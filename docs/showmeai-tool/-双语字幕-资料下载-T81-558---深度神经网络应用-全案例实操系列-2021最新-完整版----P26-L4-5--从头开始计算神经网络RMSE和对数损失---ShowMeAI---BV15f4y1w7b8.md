# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P26ÔºöL4.5- ‰ªéÂ§¥ÂºÄÂßãËÆ°ÁÆóÁ•ûÁªèÁΩëÁªúRMSEÂíåÂØπÊï∞ÊçüÂ§± - ShowMeAI - BV15f4y1w7b8

Hi this is Jeff HeatonÔºå welcome to App of Deep neural Networks with Washington University„ÄÇIn this videoÔºå we're going to see how to calculate some of the error metrics by hand so that you can literally see how these numeric values actually come about„ÄÇ We'll look in particularÔºå at log loss and root mean square error for the latest on my AI course and projects„ÄÇ Click subscribe in the bell next to it to be notified of every new video„ÄÇ Now„ÄÇ

 we'll see how to calculate R S E and log loss„ÄÇ completely from scratch„ÄÇ We'll start with regression„ÄÇüòä„ÄÇ![](img/17f6495026b126d073de254c1e55eb13_1.png)

You can see the code hereÔºå the code near the top is demonstrating how you would calculate mean square error and then root mean square error using the built in functions normally this is how you want to do and this is what we saw earlier however„ÄÇ if you want to see how to actually calculate thisÔºåYou can see that we get the exact same results„ÄÇ

 we're basically taking a sum of squares„ÄÇHere you can see the predicted minus the expected„ÄÇ and we sum those the squaresÔºå and then finally we divide that by the length of the predicted„ÄÇ so it's kind of like an average where the squares are being used to negate the signs for one thing„ÄÇ and it's also it can be easier to take the derivative of those square terms than the absolute values for training purposes„ÄÇ

For classificationÔºå we're doing a similar sort of thing„ÄÇExcept we're using the log loss„ÄÇ You can see at the top how you have the expected and the predicted and how you can literally calculate each„ÄÇ each piece of the of the log loss and sum of them together„ÄÇ getting the exact same value as you would have gotten from the built in function„ÄÇ Now„ÄÇ

 we'll see how to work through all of these„ÄÇ NowÔºå let's go through and calculate both of those by hand„ÄÇWe're going to start with log loss„ÄÇHere's the equation for it„ÄÇ Okay„ÄÇ this equation might look somewhat complicatedÔºå but there's actually several parts to it„ÄÇ and let me kind of take this piece by piece and show you really what is going on there„ÄÇFirst of all„ÄÇ

 let's look at the graph of what a logarith actually looks like because that helps to explain some of this„ÄÇSo for the log rhythmÔºå there's a couple of very important points log of zero is„ÄÇEssentially undefined or asymptote to negative infinity„ÄÇ log of1 is 0 of is as you plot the logarithm„ÄÇIt crosses here at 0„ÄÇ Now„ÄÇ

 I am really bad at drawing equationsÔºå but you'll get the general idea„ÄÇ bit of infinity kind of asymptotically there„ÄÇAnd thenÔºå it grows really relatively„ÄÇSmall as it„ÄÇ as it continues in that direction„ÄÇLogarÔºå I in my studies„ÄÇ it's come up really twice where we care about what it actually looks like„ÄÇ

 It comes up a lot more than that„ÄÇ but just in the areas that I particularly dealt with„ÄÇ if you're dealing with computer science„ÄÇIt is this region that is particularly interesting„ÄÇ because when you're doing algorithm analysisÔºå orderÔºå order N type stuffÔºå a logarithmic„ÄÇScale is actually pretty good„ÄÇ It's not really increasing all that fast compared to something like exponential„ÄÇ

 which is just going to go up veryÔºå veryÔºå very fast„ÄÇBut that's that part of the of the logar graph is not what we're interested here in the realm of data science or machine learning„ÄÇ Data science and machine learning is more interested in this segment of it„ÄÇThat is where we use to analyze error„ÄÇSo this is the data science region„ÄÇ

It's probably not strictly mathematically correctiveÔºå computer science and data science„ÄÇ I'm sure the two cross over into these other two realms„ÄÇ but this is just a good way to think of this in a very simplified view„ÄÇSo in data science„ÄÇOr in this machine learning log loss function that we're calculating this region„ÄÇ

 if you think of one is being„ÄÇIs being completely correctÔºå or you've guessed it correct„ÄÇ So you're trying to classify something„ÄÇ It was true„ÄÇ You've chosen true„ÄÇ That means that the error that is contributed toÔºå to your to your error equation is going to be 0„ÄÇ which is good„ÄÇYou guessed it completely correct„ÄÇWhat you don't want to be is confidently incorrect„ÄÇ

 So in these machine learning algorithmsÔºå if you want to say true„ÄÇ you don't just usually say true or false„ÄÇ you will sayÔºå I think with 0„ÄÇ9% probabilityÔºå it is true„ÄÇDon't you wish you could have done that back on true false questions in in school if youre„ÄÇ if you're just really not sure you could put 0„ÄÇ5 probability„ÄÇ

 meaning I don't know it could be true it could be false„ÄÇ You'd mostly lose half a point„ÄÇWell„ÄÇ or you could do 0„ÄÇ75 probability„ÄÇ You would get you'd get three quarters of your point if you're correct„ÄÇ but only lose „ÄÇ25 if you were not„ÄÇ So that'sÔºå that's how these are evaluated„ÄÇ What you don't want to beÔºå is confidently incorrect„ÄÇ So if you say it's true with 100% probability„ÄÇ

 And then it's false„ÄÇ then you're down here at a infinitely bad score„ÄÇ I'm glad school isn't like that„ÄÇ You can't get an infinitely bad score„ÄÇ You can just get a 0„ÄÇ But„ÄÇHere you can be so bad that it's infinitely badÔºå that's why usually when you look at these sort of predictions„ÄÇ you'll very rarely see predictions at zero or1Ôºå they're going to be very close to it because they want the optimizer algorithms will usually clamp it at that„ÄÇ

So notice twoÔºå for that regionÔºå for the data science part of itÔºå everything is negative„ÄÇYou could talk about negative errors„ÄÇ You could say I have a negative 025 log loss or something like that„ÄÇ But just to get itÔºå we're used to errors being reported as positive„ÄÇ So that's why you have this negative up front„ÄÇ That essentially just shifts it entirely to the„ÄÇ

To the positive rangeÔºå that's all that negative is accomplishing there„ÄÇThe one over n„ÄÇThat part of it is essentiallyÔºå that's the average„ÄÇSo n is the number of elements in your training set„ÄÇ You don't want to have extremely large error for extremely large training sets„ÄÇ

 you need you want to normalize that just like the percent on your exam paper„ÄÇThat's why you divide it by the numberÔºå the number of questions„ÄÇ If you just deal in points„ÄÇ if you sayÔºå heyÔºå I scored 30 points on my exam„ÄÇ WellÔºå if it's out of 30Ôºå that's pretty good„ÄÇ If it's out of 3000Ôºå that's really bad„ÄÇ So that is that is what the negative and the one over n is„ÄÇ

 is accomplishing„ÄÇ Then we need to sumit sum all of thoseÔºå all of those log loss errors„ÄÇ And that is going toÔºå that is basically going to„ÄÇGive you the summation of all your log losses„ÄÇAnd then you divide them by n„ÄÇAll of your log lossesÔºå zero„ÄÇ ones near zero are not particularly bad you are close and either true or false„ÄÇ

Or the higher ones you are„ÄÇYou were largely incorrect„ÄÇ You try to prevent„ÄÇYou usually clamp this so that you don't have Y hat valuesÔºå Y hatÔºå by the wayÔºå are your predictions„ÄÇSo the two y hats that you see here and hereÔºå those are your predictions„ÄÇWhy is the truth„ÄÇ That is the value you're comparing it against„ÄÇ And we have the subscript eye on each of these just so that we can calculate„ÄÇ

Or just because that's the individual number or the individual training set element and your prediction„ÄÇThe first part of itÔºå which is right here„ÄÇThat's dealing with true cases„ÄÇAnd then the other part of it is dealing with false cases„ÄÇSo you're classifyingÔºå is it true or falseÔºü

The way that these are controlled is sort of a mathematical if statement„ÄÇ Mathematicians like to use coefficientsÔºå often for if statements„ÄÇAnd the two F statements that you are dealing with are basically here„ÄÇAnd„ÄÇAnd there„ÄÇIf y sub I„ÄÇ that is the absolute truthÔºå that is the value from the training set that you're comparing against„ÄÇ

 If it's trueÔºå then this is going to be one„ÄÇ The green y sub I is going to be one„ÄÇ If it's false„ÄÇ then it's going to be 0„ÄÇ So thisÔºå the first coefficientÔºå the green arrow is going to be one„ÄÇ In all cases where we are dealing with true values„ÄÇ and the red arrow is going to be false in all cases where we're dealing with false values„ÄÇ

Because of the1 minus„ÄÇ So that essentially turns off these two sides„ÄÇ So one of those two sides is going to cancel out each time„ÄÇ depending on if it's true or if it's false„ÄÇSo if this really is true„ÄÇ then sub1 y sub i is going to be one„ÄÇSo it won't cancel out„ÄÇ and then the log ideally„ÄÇ

 if the real answer is trueÔºå you would like why hat to also be true or one„ÄÇ if that is the case and you nail this right on the noseÔºå then log of one0„ÄÇ so that term will then cancel out„ÄÇBut that's good because you had„ÄÇ you had a perfectly correct answer„ÄÇ And it nowÔºå you don't want any error to be contributed„ÄÇ

 If you had guess „ÄÇ9Ôºå then the log „ÄÇ9 will be added to yourÔºå to your error„ÄÇ But that's not so bad because you're still„ÄÇBasicallyÔºå the closer you get to zero„ÄÇ the worse your score is going to be„ÄÇLook at the at the„ÄÇAt the curve on the Cartesian plane that I have drawn„ÄÇ NowÔºå on the other side of the coin„ÄÇ

 if it's falseÔºå So if the correct answer y sub I is is is false or 0Ôºå then that first„ÄÇ the true term is going to cancel out„ÄÇAnd then the false is going to be one„ÄÇ So it's going to have a coefficient of 1Ôºå1-0 is 1 multiplied by the log of 1„ÄÇ1 minus y sub I„ÄÇ We're basically now doing that in reverse„ÄÇSo doing that in reverse„ÄÇ

 we are now basically going to deal with„ÄÇIf you had a one now in this case„ÄÇ So it was false„ÄÇ but you guessed trueÔºå you're now infinitely wrong because 1-1 is 0„ÄÇ and that takes you to„ÄÇ to infinity on the on the log„ÄÇ NowÔºå if you had guessed close like 0„ÄÇ1Ôºå WellÔºå1-0„ÄÇ1 is „ÄÇ9„ÄÇ and that's not quite as bad„ÄÇ you're kind of in the same in the same location that I just described„ÄÇ

 So you're closer up that closer to 0 on that log calculation„ÄÇNow when you actually implement this„ÄÇThis approach has a couple of errors when I mean issues when you're dealing with this in computer science or implementing this as a program for one„ÄÇYour depending on how smart yourÔºå your programming language is to cancel out things that are 0„ÄÇ You're potentially calculating the log rather twice„ÄÇ You really don't need to„ÄÇ

 because it's going to cancel out„ÄÇ The other issue that I have been burned by at least a couple of times is that if either of these sides go to negative infinity„ÄÇYou're now effectively multiplyingÔºå even though„ÄÇYou even though you would like that to cancel out in most programming languages„ÄÇ

 zero times infinity is in factÔºå zero or is in factÔºå infinity„ÄÇMathematically„ÄÇ you can make the case that it is0 if you useÔºå I believe it's Haital's rule„ÄÇ looking at basically the rates at which get of infinity on one side or the other„ÄÇ you multiply it by the good half„ÄÇ And even though you may have answered this completely correctly„ÄÇ

 and it shouldn't cause the equation to blow upÔºå it does cause the equation to blow up because you've got infinity on one side or the other of a multiplication„ÄÇ and the whole thing then goes to infinity„ÄÇOr NAÔºå depending on the implementation„ÄÇSo now let's look at how we will actually calculate this„ÄÇ So the way that I'll show you to calculate itÔºå we're not going to actually„ÄÇ

 we're going to do it more like the computer would do it„ÄÇ We're not going to literally take the log two logs for every single line on this„ÄÇ So let's go ahead and look at a couple of cases„ÄÇ We're going to look at the cases just like we had„ÄÇIn the code that I showed you earlierÔºå so that's how I know my math is correct„ÄÇ

 I'm largely regurgitating what I already did in Python„ÄÇSo we're going to have the data set where we're going to have why„ÄÇAnd then why hatÔºü

So this is the why is the correct answerÔºå why hat is what we predictedÔºü

We're not really calculating the differences„ÄÇUp in the top partÔºå there„ÄÇ the only difference really is the the one minuses„ÄÇ but largelyÔºå that's what we're getting to„ÄÇ EssentiallyÔºå we areÔºå we are calculating the„ÄÇWe're calculating how far off„ÄÇYou are from and and adding the log of that„ÄÇ So the difference is that I am calculating here„ÄÇ

By the wayÔºå this is the absolute value of the differenceÔºå not a square like we would do in R R C„ÄÇ but just the absolute differences„ÄÇ Now we need to take the logs of those mentioned„ÄÇ let's go ahead and sun theseÔºå and then we will divide those by n„ÄÇ the absolute value of that„ÄÇ That is„ÄÇ That is the value that we had for in the Python„ÄÇ

 So this is how you calculate the log loss just completely from hand„ÄÇL R on a sea„ÄÇPencil battery is low„ÄÇ I need to hurry„ÄÇEverything needs to be recharged these daysÔºå even pencils on„ÄÇ you have the one over n„ÄÇ just like before„ÄÇ You don't need a negative because these these are inherently going to stay positive„ÄÇ And it's a sum of squares„ÄÇ You're essentially taking y hat minus Y for each of these doesn't really matter the order that those are even taken in„ÄÇ

 because you're going to square those differences„ÄÇ But numbers„ÄÇ But these are the ones that I used in Python„ÄÇ And that lets me check my math to know that I'm actually doing these correctly„ÄÇNow we take the difference„ÄÇGo'ming to go ahead and square each of those„ÄÇAnd sum that going to divide them by n„ÄÇThis value is actually called the sum of squared errors„ÄÇ

This numberÔºå if you'reÔºå if you're just writing an algorithm and you're trying to optimize„ÄÇ meaning you're trying to push a number to zeroÔºå stop hereÔºå use this on squared errors because„ÄÇDoing the square root on top of it is just to get it into the same units as the training data„ÄÇ If you're just optimizingÔºå just use some of squareÔºå that'sÔºå that's plenty good„ÄÇ

 But if you're trying to report this to„ÄÇTo somebody to actually see how off you are„ÄÇ then you need to take the square root to get RNSC„ÄÇ![](img/17f6495026b126d073de254c1e55eb13_3.png)

So that is the Romen square error„ÄÇ Thank you for watching this video„ÄÇ In the next video„ÄÇ we're going to begin to look at regularizationÔºå which is yet another way that you can combat overfitting for a neural network„ÄÇ This content changes often„ÄÇ So subscribe to the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇüòä„ÄÇ