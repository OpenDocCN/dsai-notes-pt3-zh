- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- ä½¿ç”¨
    Autograd è®¡ç®—æ¢¯åº¦ - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- ä½¿ç”¨
    Autograd è®¡ç®—æ¢¯åº¦ - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome to a new pytorrch tutorialã€‚ Todayã€‚ we learn about the
    autograd package in pytorrch and how we can calculate gradients with itã€‚ Grds
    are essential for our model optimizationã€‚ So this is a very important concept
    that we should understandã€‚Luckilyï¼Œ Pytorch provides the autograd packageï¼Œ which
    can do all the computations for usã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°æ–°çš„ pytorrch æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†å­¦ä¹  pytorrch ä¸­çš„ autograd åŒ…ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•ç”¨å®ƒè®¡ç®—æ¢¯åº¦ã€‚æ¢¯åº¦å¯¹äºæˆ‘ä»¬çš„æ¨¡å‹ä¼˜åŒ–è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬åº”è¯¥ç†è§£çš„é‡è¦æ¦‚å¿µã€‚å¹¸è¿çš„æ˜¯ï¼ŒPytorch
    æä¾›äº† autograd åŒ…ï¼Œå®ƒå¯ä»¥ä¸ºæˆ‘ä»¬æ‰§è¡Œæ‰€æœ‰è®¡ç®—ã€‚
- en: We just have to know how to use itã€‚ So let's start to see how we can calculate
    gradients in pieytorrchã€‚ So first of allï¼Œ we import torchã€‚ Of courseã€‚ğŸ˜Šï¼ŒAnd now
    let's create a tenor x equals torch dot R n of size 3ã€‚ And now let's print our
    xã€‚ So this is a tenzor with three valuesã€‚ So three random valuesã€‚And nowã€‚ let's
    say laterï¼Œ we want to calculate the gradients of some function with respect to
    xã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªéœ€è¦çŸ¥é“å¦‚ä½•ä½¿ç”¨å®ƒã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬å¼€å§‹çœ‹çœ‹å¦‚ä½•åœ¨ pytorrch ä¸­è®¡ç®—æ¢¯åº¦ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥ torchã€‚å½“ç„¶ã€‚ğŸ˜Š ç„¶åè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¼ é‡ x ç­‰äº
    torch dot R nï¼Œå¤§å°ä¸º 3ã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°æˆ‘ä»¬çš„ xã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå€¼çš„å¼ é‡ã€‚ä¸‰ä¸ªéšæœºå€¼ã€‚ç°åœ¨ã€‚å‡è®¾æˆ‘ä»¬ç¨åæƒ³è®¡ç®—æŸä¸ªå‡½æ•°ç›¸å¯¹äº x çš„æ¢¯åº¦ã€‚
- en: Then what we have to do is we must specify the argument requires Gr equals trueã€‚
    So by defaultã€‚ this is falseã€‚And now if we run this againï¼Œ then we see that also
    Pyto tracks that it requires the gradientã€‚And nowã€‚Whenever we do operations with
    this tenorï¼Œ Py toch will create a so called computational graph for usã€‚ So now
    let's say we do the operation x plus 2ï¼Œ and we start this in an outputã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬è¦åšçš„å°±æ˜¯æŒ‡å®šå‚æ•° requires Gr ç­‰äº trueã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ falseã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬å†æ¬¡è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ° Pyto ä¹Ÿè·Ÿè¸ªå®ƒéœ€è¦æ¢¯åº¦ã€‚ç°åœ¨ï¼Œæ¯å½“æˆ‘ä»¬å¯¹è¿™ä¸ªå¼ é‡è¿›è¡Œæ“ä½œæ—¶ï¼ŒPytoch
    ä¼šä¸ºæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ‰€è°“çš„è®¡ç®—å›¾ã€‚å› æ­¤ï¼Œç°åœ¨å‡è®¾æˆ‘ä»¬è¿›è¡Œæ“ä½œ x åŠ  2ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨è¾“å‡ºä¸­ã€‚
- en: So we say y equals x plus 2ã€‚ Then this will create the computational graphã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_1.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´ y ç­‰äº x åŠ  2ã€‚è¿™å°†åˆ›å»ºè®¡ç®—å›¾ã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_1.png)
- en: And this looks like thisã€‚So for each nodeï¼Œ we have a for each operationã€‚ we
    have a node with inputs and an outputã€‚ So here the operation is the plusã€‚ So in
    additionã€‚And our inputs are x and 2ï¼Œ and the output is yã€‚And now with this graph
    and the technique that is called back propagationã€‚ we can then calculate the gradientsã€‚I
    will explain the concept of Beck propagation in detail in the next videoã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥åƒè¿™æ ·ã€‚å¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªéå†æ“ä½œã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå¸¦è¾“å…¥å’Œè¾“å‡ºçš„èŠ‚ç‚¹ã€‚æ‰€ä»¥è¿™é‡Œçš„æ“ä½œæ˜¯åŠ æ³•ã€‚å› æ­¤ï¼Œè¾“å…¥æ˜¯ x å’Œ 2ï¼Œè¾“å‡ºæ˜¯ yã€‚ç°åœ¨é€šè¿‡è¿™ä¸ªå›¾å’Œç§°ä¸ºåå‘ä¼ æ’­çš„æŠ€æœ¯ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¢¯åº¦ã€‚æˆ‘å°†åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­è¯¦ç»†è§£é‡Šåå‘ä¼ æ’­çš„æ¦‚å¿µã€‚
- en: But for nowï¼Œ it's fine to just know that we or how we can use itã€‚So firstï¼Œ we
    do a forward passã€‚ So here we apply this operationã€‚ And in the forward passï¼Œ we
    calculate the output yã€‚And since we specified that it requires the gradientã€‚ Pytoch
    will then automatically create and store a function for usã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç°åœ¨ï¼Œåªéœ€çŸ¥é“æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨å®ƒå°±å¯ä»¥äº†ã€‚å› æ­¤ï¼Œé¦–å…ˆæˆ‘ä»¬è¿›è¡Œå‰å‘ä¼ é€’ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åº”ç”¨è¿™ä¸ªæ“ä½œã€‚åœ¨å‰å‘ä¼ é€’ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—è¾“å‡º yã€‚ç”±äºæˆ‘ä»¬æŒ‡å®šå®ƒéœ€è¦æ¢¯åº¦ï¼ŒPytoch
    ä¼šè‡ªåŠ¨ä¸ºæˆ‘ä»¬åˆ›å»ºå¹¶å­˜å‚¨ä¸€ä¸ªå‡½æ•°ã€‚
- en: And this function is then used in the back proagation and to get the gradientsã€‚
    So here y has an attribute Gr underscore F Nã€‚ So this will point to a gradient
    functionã€‚ And in this caseï¼Œ it's called at add backwardã€‚And with this functionã€‚
    we can then calculate the gradients in the so called backward passã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°éšåç”¨äºåå‘ä¼ æ’­ä»¥è·å–æ¢¯åº¦ã€‚å› æ­¤ï¼Œy æ‹¥æœ‰ä¸€ä¸ªå±æ€§ Gr underscore F Nã€‚è¿™å°†æŒ‡å‘ä¸€ä¸ªæ¢¯åº¦å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç§°ä¸º at add
    backwardã€‚é€šè¿‡è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ‰€è°“çš„åå‘ä¼ é€’ä¸­è®¡ç®—æ¢¯åº¦ã€‚
- en: So this will calculate the gradient of y with respect to x in this caseã€‚ So
    now if we print yã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°†è®¡ç®— y å¯¹ x çš„æ¢¯åº¦ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ã€‚å¦‚æœæˆ‘ä»¬æ‰“å° yã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_3.png)
- en: Then we will see exactly this grat Fn attributeã€‚ And hereï¼Œ this is an at backward
    functionã€‚ So because hereï¼Œ our operation was a plusã€‚And thenï¼Œ ourã€‚Then we do the
    back propagation laterã€‚ So that's why it's called at backwardã€‚And let's do some
    more operation with our tensrsã€‚So let's say we have C equals y times y times 2ï¼Œ
    for exampleã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸ª grat Fn å±æ€§ã€‚è¿™é‡Œï¼Œè¿™æ˜¯ä¸€ä¸ªåå‘å‡½æ•°ã€‚å› ä¸ºåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„æ“ä½œæ˜¯åŠ æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬å†è¿›è¡Œåå‘ä¼ æ’­ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆç§°ä¹‹ä¸ºåå‘ã€‚è®©æˆ‘ä»¬ç”¨æˆ‘ä»¬çš„å¼ é‡åšä¸€äº›æ›´å¤šçš„æ“ä½œã€‚å‡è®¾æˆ‘ä»¬æœ‰
    C ç­‰äº y ä¹˜ä»¥ y ä¹˜ä»¥ 2ã€‚
- en: So this tensor then also has this gra function attribute So here Grt Fn equals
    mile backward because here our operation is a multiplicationã€‚ And for exampleï¼Œ
    we can say C equals C dot meanã€‚ So we can apply a mean operationã€‚ And then our
    gradient function is the mean backwardã€‚And nowã€‚ when we want to calculate the
    gradientsï¼Œ the only thing that we must do is to call C dot backwardã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªå¼ é‡ä¹Ÿæœ‰è¿™ä¸ª gra å‡½æ•°å±æ€§ã€‚å› æ­¤è¿™é‡Œ Grt Fn ç­‰äº mile backwardï¼Œå› ä¸ºè¿™é‡Œæˆ‘ä»¬çš„æ“ä½œæ˜¯ä¹˜æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ C ç­‰äº
    C.meanã€‚è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åº”ç”¨å‡å€¼æ“ä½œã€‚ç„¶åæˆ‘ä»¬çš„æ¢¯åº¦å‡½æ•°å°±æ˜¯ mean backwardã€‚ç°åœ¨ï¼Œå½“æˆ‘ä»¬æƒ³è¦è®¡ç®—æ¢¯åº¦æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åšçš„å”¯ä¸€äº‹æƒ…å°±æ˜¯è°ƒç”¨ C.backwardã€‚
- en: So this will then calculate the gradient of C with respect to xã€‚ So x then has
    a gradientã€‚ a dot Gr attribute where the gradients are storedã€‚ so we can print
    thisã€‚And now if you run thisã€‚ then we see that we have the gradients here in this
    tenorã€‚ So this is all we have to doã€‚And now let's have a look what happens when
    we don't specify this argumentã€‚ So first of allã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™å°†è®¡ç®— C å…³äº x çš„æ¢¯åº¦ã€‚æ‰€ä»¥ x å…·æœ‰ä¸€ä¸ªæ¢¯åº¦ï¼Œä¸€ä¸ª dot Gr å±æ€§ï¼Œå…¶ä¸­å­˜å‚¨äº†æ¢¯åº¦ã€‚æˆ‘ä»¬å¯ä»¥æ‰“å°å®ƒã€‚ç°åœ¨å¦‚æœä½ è¿è¡Œè¿™ä¸ªã€‚é‚£ä¹ˆæˆ‘ä»¬çœ‹åˆ°åœ¨è¿™ä¸ªå¼ é‡ä¸­æœ‰æ¢¯åº¦ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¿…é¡»åšçš„æ‰€æœ‰äº‹æƒ…ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬ä¸æŒ‡å®šè¿™ä¸ªå‚æ•°æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ‰€ä»¥é¦–å…ˆã€‚
- en: if we print our tenzosã€‚Then we see that they don't have this gra function attributeã€‚
    And if we try to call the backward functionï¼Œ then this will produce an  errorã€‚
    So it says tens does not require gr and does not have the gr functionã€‚So remember
    that we must specify this argument and then it will workã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ‰“å°æˆ‘ä»¬çš„å¼ é‡ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°å®ƒä»¬æ²¡æœ‰è¿™ä¸ª gra å‡½æ•°å±æ€§ã€‚å¦‚æœæˆ‘ä»¬å°è¯•è°ƒç”¨åå‘å‡½æ•°ï¼Œé‚£ä¹ˆè¿™å°†äº§ç”Ÿä¸€ä¸ªé”™è¯¯ã€‚å› æ­¤ï¼Œå®ƒæ˜¾ç¤ºå¼ é‡ä¸éœ€è¦ gr å¹¶ä¸”æ²¡æœ‰
    gr å‡½æ•°ã€‚æ‰€ä»¥è®°ä½ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šè¿™ä¸ªå‚æ•°ï¼Œç„¶åå®ƒå°†æ­£å¸¸å·¥ä½œã€‚
- en: And one thing that we should also know is so in the background what this basically
    doesã€‚ this will create a socal vector Jacobcobian product to get the gradientsã€‚
    So this will look like thisã€‚ I will not go into the mathematical detailsã€‚ but
    we should know that we have the Jacobcobian matrixã€‚ with the partial derivativesã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜åº”è¯¥çŸ¥é“çš„ä¸€ä»¶äº‹æ˜¯ï¼Œåœ¨åå°è¿™åŸºæœ¬ä¸Šåšäº†ä»€ä¹ˆã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªæ ‡é‡å‘é‡é›…å¯æ¯”ä¹˜ç§¯ä»¥è·å¾—æ¢¯åº¦ã€‚æ‰€ä»¥å®ƒå°†çœ‹èµ·æ¥åƒè¿™æ ·ã€‚æˆ‘ä¸ä¼šæ·±å…¥æ•°å­¦ç»†èŠ‚ã€‚ä½†æˆ‘ä»¬åº”è¯¥çŸ¥é“ï¼Œæˆ‘ä»¬æœ‰é›…å¯æ¯”çŸ©é˜µä¸åå¯¼æ•°ã€‚
- en: And then we multiply this with a gradient vectorã€‚ and then we will get the final
    the final gradients that we are interested inã€‚ So this is also called the chain
    ruleã€‚ And I will also explain this more in detail in the next video But yeahã€‚
    we should know that actually we must multiply it with a vectorã€‚ So in this caseã€‚
    since our C is a scala valueï¼Œ We don't have to put the don't have to use an argumentã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å…¶ä¸æ¢¯åº¦å‘é‡ç›¸ä¹˜ã€‚ç„¶åæˆ‘ä»¬å°†å¾—åˆ°æˆ‘ä»¬æ„Ÿå…´è¶£çš„æœ€ç»ˆæ¢¯åº¦ã€‚è¿™ä¹Ÿç§°ä¸ºé“¾å¼æ³•åˆ™ã€‚æˆ‘å°†åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­æ›´è¯¦ç»†åœ°è§£é‡Šè¿™ä¸€ç‚¹ã€‚ä½†æˆ‘ä»¬åº”è¯¥çŸ¥é“ï¼Œå®é™…ä¸Šæˆ‘ä»¬å¿…é¡»å°†å…¶ä¸ä¸€ä¸ªå‘é‡ç›¸ä¹˜ã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºæˆ‘ä»¬çš„
    C æ˜¯ä¸€ä¸ªæ ‡é‡å€¼ï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¼ é€’å‚æ•°ã€‚
- en: '![](img/cecd5af4135b2f12f1bfa0bb78e5d673_5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cecd5af4135b2f12f1bfa0bb78e5d673_5.png)'
- en: And here for our backward functionã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_7.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œå¯¹äºæˆ‘ä»¬çš„åå‘å‡½æ•°ã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_7.png)
- en: So our C here has only v valueã€‚ So this is fineã€‚ But let's say we didn't apply
    the mean operationã€‚ So now our C has more than one value in itã€‚ So it's also size
    1 by 3ã€‚ And now when we try to call the backward function like thisã€‚ And this
    will produce an errorã€‚ So Gr can be implicitly created only for scala outputsã€‚
    So in this caseï¼Œ we have toã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çš„ C è¿™é‡Œåªæœ‰ä¸€ä¸ªæ ‡é‡å€¼ã€‚è¿™æ˜¯å¯ä»¥çš„ã€‚ä½†å‡è®¾æˆ‘ä»¬æ²¡æœ‰åº”ç”¨å‡å€¼æ“ä½œã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬çš„ C ä¸­æœ‰å¤šä¸ªå€¼ã€‚æ‰€ä»¥å®ƒçš„å¤§å°ä¹Ÿæ˜¯ 1x3ã€‚ç°åœ¨ï¼Œå½“æˆ‘ä»¬å°è¯•åƒè¿™æ ·è°ƒç”¨åå‘å‡½æ•°æ—¶ã€‚è¿™å°†äº§ç”Ÿä¸€ä¸ªé”™è¯¯ã€‚å› æ­¤ï¼ŒGr
    åªèƒ½ä¸ºæ ‡é‡è¾“å‡ºéšå¼åˆ›å»ºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»ã€‚
- en: Give it the gradient argumentã€‚ So we have to create a vector of the same sizeï¼Œ
    soã€‚Let's say V equals torch dot1orã€‚ And here we putï¼Œ for exampleï¼Œ011ã€‚0 and0ã€‚0ï¼Œ0ï¼Œ1ã€‚
    and we give it a data type of torch dot float 32ã€‚ and then we must pass this vector
    to our backward functionã€‚ and now it will work againã€‚So nowï¼Œ if we run thisã€‚Then
    this is okayã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®ƒæ¢¯åº¦å‚æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬å¿…é¡»åˆ›å»ºä¸€ä¸ªç›¸åŒå¤§å°çš„å‘é‡ã€‚æ‰€ä»¥ï¼Œå‡è®¾ V ç­‰äº torch.dot(1)ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æ”¾ç½®ï¼Œä¾‹å¦‚ï¼Œ011.0 å’Œ 0.0ï¼Œ0ï¼Œ1ã€‚å¹¶ä¸”æˆ‘ä»¬å°†æ•°æ®ç±»å‹è®¾ç½®ä¸º
    torch.float32ã€‚ç„¶åæˆ‘ä»¬å¿…é¡»å°†è¿™ä¸ªå‘é‡ä¼ é€’ç»™æˆ‘ä»¬çš„åå‘å‡½æ•°ã€‚ç°åœ¨å®ƒå°†å†æ¬¡æ­£å¸¸å·¥ä½œã€‚æ‰€ä»¥ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚é‚£ä¹ˆè¿™æ˜¯å¯ä»¥çš„ã€‚
- en: So we should know that in the backgroundï¼Œ this is a a vector Jacobbian productã€‚
    And a lot of timesã€‚ the last operation is some operation that will create a scala
    valueã€‚ So this isã€‚ it's okay to call it like this without an argumentã€‚ But if
    this is not an a scalaã€‚ Then we must give it the the vectorã€‚And yeahã€‚Then some
    other thing that we should know is how we can prevent Pyth from tracking the history
    and calculating this gra f n attributeã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥çŸ¥é“ï¼Œåœ¨åå°ï¼Œè¿™æ˜¯ä¸€ä¸ªå‘é‡é›…å¯æ¯”ä¹˜ç§¯ã€‚å¾ˆå¤šæ—¶å€™ï¼Œæœ€åä¸€ä¸ªæ“ä½œæ˜¯æŸä¸ªæ“ä½œï¼Œå°†åˆ›å»ºä¸€ä¸ªæ ‡é‡å€¼ã€‚å› æ­¤ï¼Œå¯ä»¥åœ¨æ²¡æœ‰å‚æ•°çš„æƒ…å†µä¸‹åƒè¿™æ ·è°ƒç”¨ã€‚ä½†å¦‚æœè¿™ä¸æ˜¯æ ‡é‡ï¼Œæˆ‘ä»¬å¿…é¡»æä¾›å‘é‡ã€‚è¿˜æœ‰ä¸€äº›å…¶ä»–äº‹æƒ…æˆ‘ä»¬åº”è¯¥çŸ¥é“ï¼Œå¦‚ä½•é˜²æ­¢Pythè·Ÿè¸ªå†å²å¹¶è®¡ç®—è¿™ä¸ªgrad_fnå±æ€§ã€‚
- en: So for exampleï¼Œ sometimes during our training loop when we want to update our
    weightsã€‚ Then this operation should not be part of the gradient computationã€‚ So
    in one of the next tutorialsã€‚ I will give a concrete example of how we apply this
    autocrad packageã€‚ And then it will become clearerï¼Œ maybeã€‚ But yeahï¼Œ for nowã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæœ‰æ—¶åœ¨æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¸­ï¼Œå½“æˆ‘ä»¬æƒ³æ›´æ–°æƒé‡æ—¶ï¼Œè¿™ä¸ªæ“ä½œä¸åº”è¯¥æ˜¯æ¢¯åº¦è®¡ç®—çš„ä¸€éƒ¨åˆ†ã€‚åœ¨æ¥ä¸‹æ¥çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä¼šç»™å‡ºä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹ï¼Œè¯´æ˜æˆ‘ä»¬å¦‚ä½•åº”ç”¨è¿™ä¸ªautogradåŒ…ã€‚ç„¶åå¯èƒ½ä¼šå˜å¾—æ›´æ¸…æ™°ã€‚ä½†ç°åœ¨æ˜¯è¿™æ ·ã€‚
- en: we should know how we can prevent this from from tracking the gradientsã€‚ and
    we have three option for thisã€‚ So the first one is to call the requiresã€‚Gratã€‚
    underscore functionï¼Œ and set this to falseã€‚The second option is to call X dot
    detã€‚ So this will create a new Tenzoor that doesn't require the gradientã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åº”è¯¥çŸ¥é“å¦‚ä½•é˜²æ­¢è·Ÿè¸ªæ¢¯åº¦ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰ä¸‰ä¸ªé€‰é¡¹ã€‚ç¬¬ä¸€ä¸ªæ˜¯è°ƒç”¨requires_gradä¸‹åˆ’çº¿å‡½æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºfalseã€‚ç¬¬äºŒä¸ªé€‰é¡¹æ˜¯è°ƒç”¨x dot
    detachã€‚å› æ­¤è¿™å°†åˆ›å»ºä¸€ä¸ªä¸éœ€è¦æ¢¯åº¦çš„æ–°å¼ é‡ã€‚
- en: And the second option would be to wrap this in a width statementã€‚ So with torchï¼Œ
    dotï¼Œ no grã€‚ And then we can do our operationsã€‚Soï¼Œ yeahï¼Œ let's try each of theseã€‚
    So firstã€‚We can sayã€‚ x dot requiresã€‚Gratï¼Œ underscore and set this to falseã€‚ So
    whenever a function has a trailing underscore in Pytorchã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªé€‰é¡¹æ˜¯å°†å…¶åŒ…è£…åœ¨ä¸€ä¸ªå®½åº¦è¯­å¥ä¸­ã€‚å› æ­¤ä½¿ç”¨torchï¼Œdotï¼Œä¸éœ€è¦æ¢¯åº¦ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¿›è¡Œæ“ä½œã€‚æ˜¯çš„ï¼Œè®©æˆ‘ä»¬å°è¯•æ¯ä¸€ä¸ªã€‚è¿™æ˜¯ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬å¯ä»¥è¯´x dotéœ€è¦æ¢¯åº¦ï¼Œå¸¦ä¸‹åˆ’çº¿ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºfalseã€‚å› æ­¤ï¼Œæ¯å½“ä¸€ä¸ªå‡½æ•°åœ¨Pytorchä¸­æœ‰ä¸€ä¸ªå°¾éšçš„ä¸‹åˆ’çº¿æ—¶ã€‚
- en: then this means that it will modify our variable in placeã€‚ So nowï¼Œ if we print
    Xã€‚Then we will see that it doesn't have this require Gr attribute anymoreã€‚ So
    now this is falseã€‚So this is the first optionï¼Œ and the second option would be
    to call x detaachã€‚ So we say y equals x do detaachã€‚ So this will create a new
    vector with the same or a newtenzo with the same valuesã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€å®ƒå°†åœ¨åŸåœ°ä¿®æ”¹æˆ‘ä»¬çš„å˜é‡ã€‚å› æ­¤ï¼Œç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æ‰“å°xã€‚ç„¶åæˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒä¸å†å…·æœ‰è¿™ä¸ªrequires_gradå±æ€§ã€‚æ‰€ä»¥ç°åœ¨è¿™æ˜¯falseã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªé€‰é¡¹ï¼Œç¬¬äºŒä¸ªé€‰é¡¹æ˜¯è°ƒç”¨x.detachã€‚å› æ­¤æˆ‘ä»¬è¯´yç­‰äºx.detachã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰ç›¸åŒå€¼çš„æ–°å‘é‡æˆ–æ–°å¼ é‡ã€‚
- en: but it doesn't require the gradientã€‚ So here we see that our y has the same
    valuesã€‚ but doesn't require the gradientsã€‚And the last option is to wrap it in
    a torch in a width with statement with torch dot no Grã€‚ And then we can do some
    operationsï¼Œ for exampleï¼Œ y equalsã€‚X plus 2ã€‚And nowï¼Œ if we print our yã€‚ then we
    see that it doesn't have the gradient function attribute hereã€‚ Soï¼Œ yeahã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒä¸éœ€è¦æ¢¯åº¦ã€‚å› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„yå…·æœ‰ç›¸åŒçš„å€¼ï¼Œä½†ä¸éœ€è¦æ¢¯åº¦ã€‚æœ€åä¸€ä¸ªé€‰é¡¹æ˜¯å°†å…¶åŒ…è£…åœ¨ä¸€ä¸ªtorchçš„withè¯­å¥ä¸­ï¼Œä½¿ç”¨torch.no_gradã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€äº›æ“ä½œï¼Œä¾‹å¦‚yç­‰äºxåŠ 2ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æ‰“å°æˆ‘ä»¬çš„yï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒåœ¨è¿™é‡Œæ²¡æœ‰æ¢¯åº¦å‡½æ•°å±æ€§ã€‚å› æ­¤ï¼Œæ˜¯çš„ã€‚
- en: if we don't use this and would run it like thisã€‚Then our y has the gradient
    functionã€‚ So these are the three ways how we can stop Pythot from creating this
    gradient functions and tracking the history in our computational graphã€‚And now
    one more very important thing that we should also know is that whenever we call
    the backward functionã€‚ then the gradient for this tenzoor will be accumulated
    into the dot gra attributeã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨è¿™ä¸ªå¹¶ä¸”åƒè¿™æ ·è¿è¡Œå®ƒã€‚é‚£ä¹ˆæˆ‘ä»¬çš„yå°±ä¼šæœ‰æ¢¯åº¦å‡½æ•°ã€‚å› æ­¤ï¼Œè¿™ä¸‰ç§æ–¹æ³•å¯ä»¥é˜»æ­¢Pytorchåˆ›å»ºæ¢¯åº¦å‡½æ•°å¹¶è·Ÿè¸ªæˆ‘ä»¬è®¡ç®—å›¾ä¸­çš„å†å²ã€‚è€Œç°åœ¨è¿˜æœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„äº‹æƒ…ï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥çŸ¥é“çš„æ˜¯ï¼Œæ¯å½“æˆ‘ä»¬è°ƒç”¨backwardå‡½æ•°æ—¶ï¼Œæ¢¯åº¦å°†è¢«ç´¯ç§¯åˆ°dot_gradå±æ€§ä¸­ã€‚
- en: So the values will be summed upã€‚ So here we we must be very carefulã€‚ So let's
    create some dummy training exampleã€‚ where we have some have some weightsã€‚ So this
    is a a tenzoor with ones in it of sizeï¼Œ let's say4ï¼Œ and they require the gradientã€‚
    So requires gra equals trueã€‚ And now let's say we have a training loop where we
    say four epo in range and firstã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·å€¼å°±ä¼šè¢«æ±‚å’Œã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¿…é¡»éå¸¸å°å¿ƒã€‚å› æ­¤è®©æˆ‘ä»¬åˆ›å»ºä¸€äº›è™šæ‹Ÿçš„è®­ç»ƒç¤ºä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬æœ‰ä¸€äº›æƒé‡ã€‚è¿™æ˜¯ä¸€ä¸ªå…¨ä¸º1çš„å¼ é‡ï¼Œå¤§å°ä¸º4ï¼Œå¹¶ä¸”éœ€è¦æ¢¯åº¦ã€‚å› æ­¤requires_gradç­‰äºtrueã€‚ç°åœ¨å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬è¯´å¯¹äºæ¯ä¸ªepochåœ¨èŒƒå›´å†…ã€‚
- en: let's only do one iterationã€‚ And here we doï¼Œ let's say model outputã€‚Equalsï¼Œ
    let's sayã€‚Weights times 3 dot sumã€‚ So this is just a dummy operationï¼Œ which will
    simulate some model outputã€‚ And then we want to calculate the gradientsã€‚ So we
    say model output dot backwardã€‚And now we have the gradientï¼Œ so we can call weight
    dot Grã€‚And print thisã€‚å—¯ã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…ˆè¿›è¡Œä¸€æ¬¡è¿­ä»£ã€‚è¿™é‡Œæˆ‘ä»¬å‡è®¾æ¨¡å‹è¾“å‡ºç­‰äºæƒé‡ä¹˜ä»¥3çš„æ€»å’Œã€‚è¿™åªæ˜¯ä¸€ä¸ªè™šæ‹Ÿæ“ä½œï¼Œç”¨æ¥æ¨¡æ‹Ÿä¸€äº›æ¨¡å‹è¾“å‡ºã€‚ç„¶åæˆ‘ä»¬æƒ³è®¡ç®—æ¢¯åº¦ã€‚æ‰€ä»¥æˆ‘ä»¬è°ƒç”¨æ¨¡å‹è¾“å‡ºçš„`.backward()`ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¢¯åº¦ï¼Œå¯ä»¥è°ƒç”¨`weights.Gr`å¹¶æ‰“å°å‡ºæ¥ã€‚
- en: So I what gradients here are 3ã€‚So the tensor is filled with threesã€‚ And now
    if we do another iterationã€‚ So if we say we have two iterationsã€‚ then the second
    backward call will again accumulate the values and write them into the Gr attributeã€‚
    So now our grs has sixes in itã€‚ And now if we do a third iterationã€‚ Then it has
    nines in itã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æ¢¯åº¦æ˜¯3ï¼Œæ‰€ä»¥å¼ é‡å¡«å……äº†ä¸‰çš„å€¼ã€‚å¦‚æœæˆ‘ä»¬è¿›è¡Œç¬¬äºŒæ¬¡è¿­ä»£ï¼Œé‚£ä¹ˆç¬¬äºŒæ¬¡çš„`backward`è°ƒç”¨å°†å†æ¬¡ç´¯åŠ å€¼å¹¶å†™å…¥`Gr`å±æ€§ã€‚ç°åœ¨æˆ‘ä»¬çš„`grs`ä¸­æœ‰å…­ã€‚å†è¿›è¡Œç¬¬ä¸‰æ¬¡è¿­ä»£åï¼Œå®ƒå°†å˜ä¸ºä¹ã€‚
- en: So all the values are summed upã€‚ And now our weights or our gradients are clearly
    incorrectã€‚ So before we do the next iteration and optimization stepã€‚ we must empty
    the gradientsã€‚ So we must call weights dot Gr dot0 underscoreã€‚ And now if we run
    this then our gradients are correct againã€‚So this is one very importantã€‚Th that
    we must know during our training stepsã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å€¼éƒ½å·²è¢«æ±‚å’Œã€‚ç°åœ¨æˆ‘ä»¬çš„æƒé‡æˆ–æ¢¯åº¦æ˜¾ç„¶ä¸æ­£ç¡®ã€‚åœ¨è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£å’Œä¼˜åŒ–æ­¥éª¤ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»æ¸…ç©ºæ¢¯åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è°ƒç”¨`weights.Gr.0_`ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œæ¢¯åº¦å°†å†æ¬¡æ­£ç¡®ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒæ­¥éª¤ä¸­å¿…é¡»äº†è§£çš„ä¸€ä¸ªéå¸¸é‡è¦çš„äº‹é¡¹ã€‚
- en: and later we will work with the Pytorch built in Oprã€‚ So let's say we have a
    optr from the torch optimization packageã€‚ So torch do optim do SGD for stochastic
    gradient descentã€‚ which has our weights as parameters and some learning rate and
    now with this optimizerã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä½¿ç”¨Pytorchå†…ç½®çš„ä¼˜åŒ–å™¨ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¥è‡ªtorchä¼˜åŒ–åŒ…çš„ä¼˜åŒ–å™¨ã€‚torchçš„optimæ¨¡å—æä¾›äº†éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œå®ƒçš„å‚æ•°åŒ…æ‹¬æˆ‘ä»¬çš„æƒé‡å’Œä¸€äº›å­¦ä¹ ç‡ï¼Œç°åœ¨æœ‰äº†è¿™ä¸ªä¼˜åŒ–å™¨ã€‚
- en: we can call or do a optimization step and then before we do the next iterationã€‚
    we must call the optimizer do0 gra functionï¼Œ which will do exactly the sameã€‚Soï¼Œ
    yeahã€‚ we will talk about the optimizes in some later tutorialsã€‚ But yeahï¼Œ for
    nowã€‚ the things you should remember is that whenever we want to calculate the
    gradientsã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è°ƒç”¨æˆ–è¿›è¡Œä¸€ä¸ªä¼˜åŒ–æ­¥éª¤ï¼Œç„¶ååœ¨è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹å‰ï¼Œå¿…é¡»è°ƒç”¨ä¼˜åŒ–å™¨çš„`do0_gra`å‡½æ•°ï¼Œå®ƒä¼šæ‰§è¡Œå®Œå…¨ç›¸åŒçš„æ“ä½œã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ã€‚æˆ‘ä»¬å°†åœ¨åé¢çš„æ•™ç¨‹ä¸­è®¨è®ºä¼˜åŒ–å™¨ã€‚ä½†ç°åœ¨ï¼Œä½ åº”è¯¥è®°ä½çš„æ˜¯ï¼Œæ¯å½“æˆ‘ä»¬æƒ³è®¡ç®—æ¢¯åº¦æ—¶ã€‚
- en: We must specify the require gr parameter and set this to trueã€‚ Then we can simply
    calculate the gradients with calling the backward function and before we want
    to do the next operation or the next iteration in our optimization stepsã€‚ we must
    empty our gradientsã€‚ So we must call the0 function againã€‚And we also should know
    how we can prevent some operations from being tracked in the computational graphã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»æŒ‡å®š`require_gr`å‚æ•°å¹¶è®¾ç½®ä¸ºtrueã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è°ƒç”¨`backward`å‡½æ•°æ¥è®¡ç®—æ¢¯åº¦ã€‚åœ¨è¿›è¡Œä¸‹ä¸€ä¸ªæ“ä½œæˆ–ä¼˜åŒ–æ­¥éª¤çš„ä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»æ¸…ç©ºæˆ‘ä»¬çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å†æ¬¡è°ƒç”¨`the0`å‡½æ•°ã€‚æˆ‘ä»¬è¿˜åº”è¯¥çŸ¥é“å¦‚ä½•é˜²æ­¢æŸäº›æ“ä½œåœ¨è®¡ç®—å›¾ä¸­è¢«è·Ÿè¸ªã€‚
- en: And that's all I wanted to show you for now with the autograd packageã€‚ And I
    hope you liked itã€‚ Please subscribe to the channel and see you next timeï¼Œ byeã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_9.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘æƒ³å‘ä½ å±•ç¤ºçš„æœ‰å…³autogradåŒ…çš„å†…å®¹ã€‚å¸Œæœ›ä½ å–œæ¬¢ã€‚è¯·è®¢é˜…é¢‘é“ï¼Œä¸‹æ¬¡å†è§ï¼Œæ‹œã€‚![](img/cecd5af4135b2f12f1bfa0bb78e5d673_9.png)
