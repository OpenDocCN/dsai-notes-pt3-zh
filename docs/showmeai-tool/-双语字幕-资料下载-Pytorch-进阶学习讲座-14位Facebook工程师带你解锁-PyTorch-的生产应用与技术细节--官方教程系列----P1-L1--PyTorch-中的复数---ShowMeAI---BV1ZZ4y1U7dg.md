# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P1ï¼šL1- PyTorch ä¸­çš„å¤æ•° - ShowMeAI - BV1ZZ4y1U7dg

ğŸ¼ã€‚

![](img/30cfbf4aab59021d051c0076fb315c26_1.png)

Hiï¼Œ everyoneã€‚ Thanks for having me here todayã€‚ My name is Anjuliã€‚

 and I've spent the last year working on an exciting project in the Pytorch teamã€‚

 That project is complex numbersï¼Œ and that's all we're going to be talking about todayã€‚

 So jumping right inã€‚ let's talk a little about what complex numbers areã€‚

 Some of you may already be familiar with complex numbersã€‚

 but I'll just go over them briefly to make sure we're on the same pageã€‚ğŸ˜Šã€‚



![](img/30cfbf4aab59021d051c0076fb315c26_3.png)

Conft numbers are numbers that can be expressed as a+ Bã€‚

 where A and B are real numbers and I is a unit imaginary numberã€‚

 which equals to square root of minus1ã€‚So now that we have established what complex numbers areã€‚

 let's talk a little about where they are usedã€‚Complex numbers of applications in a variety of fieldsã€‚

 especially ones in mathï¼Œ physics and engineering quantum mechanics and signal processing are some of the examples you might be familiar withã€‚

 and thanks to Euler who came up with this amazing formula as shown on the screen that relates in imaginary number to sine and cosine terms so for exampleã€‚

 this relationship can be used to rewrite a standard cosine wave equation as a product of two complex exponentials and this often simplifies a math involved since complex exponentials are easier to manipulate than their sinusoidal counterpartsã€‚

ğŸ˜Šï¼ŒWe can also get benefit from complex numbers in the field of deep learningã€‚

Recent work on neural nets and older fundamental theoretical analysis suggests that complex numbers could have a rich representational capacity before Pytorch's lack of native complex support made it harder than it had to be to unlock that potential and at Pytororch we're about empowering research in our users our philosophy is to make things easy to use and to get the hard things out of users way So as we've been receiving a request for complex number supportã€‚

 we decided to do something about itã€‚ Here are some of the request from the Pytorge discussion forum that we have received over the yearsã€‚

 we're going to talk about some of these things listed here laterã€‚

 but what I would really like to emphasize on is that this is a community drivenn project and we prioritize the features weve built based on the ongoing feedback from the community as well with the community to do the actual developmentã€‚



![](img/30cfbf4aab59021d051c0076fb315c26_5.png)

So what we heard from the community boils down to three main pointsã€‚

 which were the motivation behind adding native complex supportã€‚ First is natural representationã€‚

 Historicallyï¼Œ we represented complex numbers as a tuple of two real numbersã€‚

 but we heard from many usersã€‚ that was just plain exhausting to write code with that conventionã€‚

 And so we wanted to introduce an API that makes working with complex numbers in Pytorrch easierã€‚

 as well as makes the code more maintainableã€‚ Secondï¼Œ is complex functionalityã€‚

 If would like to provide nuy like support for complex operations and leverage Pytorch's ability to run with acceleratorsã€‚

ğŸ˜Šï¼ŒSo farï¼Œ we've also added many specialized kernels on both CPUU and GPU to optimize complex operationsã€‚

 in fact on CPUU we also support recization for complex operationsã€‚And third is autogradã€‚

 as I mentioned beforeï¼Œ we want to support neuralNe researchã€‚

 and so we're working to add complex autograd supportï¼Œ which is helpful in optimization problemsã€‚



![](img/30cfbf4aab59021d051c0076fb315c26_7.png)

![](img/30cfbf4aab59021d051c0076fb315c26_8.png)

So let's take a look at how will complex tensor simplify our life in Pitorrchï¼Ÿ



![](img/30cfbf4aab59021d051c0076fb315c26_10.png)

Those of you who view spectrallos in Pytorch before might be familiar with the code on the rightã€‚

 the display tensor shows how we have historically represented complex numbers in torchã€‚

 As you can see it looks bulkyï¼Œ ugly and hard to readã€‚

 and there's really no good reason to not have complex data types in torchã€‚ so in Pytorch 1ã€‚6ã€‚

 we introduced two native complex D typesï¼Œ complex 64 and complex 128ã€‚

 which correspond to float and double data typesã€‚I would also like to mention here that the functions in the newly added towards 15 moduleã€‚

 which Michael will talk about later support complex numbersã€‚

Now another good thing about having native complex support is that you no longer have to write all the annoying time consuming and error prone workarounds which used to be necessaryã€‚

 and with our latest releaseï¼Œ many common operations like Mamallï¼Œ SVDã€‚

 etc are available for our complex number usersã€‚We've also added support for core properties as well as Tensa constructors to have natural complex number support that you may be familiar with from Ny and Pythonã€‚

 and on their right is an example of a Facebook code of function written using the native complex supportã€‚

The details of the code are not really importantï¼Œ it's really to tell that we're constantly adding functionality to prioritize use cases that our community finds valuableã€‚

ğŸ˜Šã€‚

![](img/30cfbf4aab59021d051c0076fb315c26_12.png)

Here are some examples of the operators we have added so farã€‚

 as you can see we have added support for many linear algebra opsã€‚

 trignometric ops and algebraic opsï¼Œ and we're constantly working to add moreã€‚



![](img/30cfbf4aab59021d051c0076fb315c26_14.png)

And it doesn't stop thereï¼Œ with the latest releaseï¼Œ you can also differentiate complex functionsã€‚



![](img/30cfbf4aab59021d051c0076fb315c26_16.png)

For those of you who are familiar with complex differentiationã€‚

 we compute the conjugate wording derivatives For those of you who are not but just want to be able to use optimizers with complex parametersã€‚

 we got you covered with a gradientd convention the existing optimizers work out of the box with a common case of optimizing real valued objective and in case you are just curious to write custom gradient functions you can check out our complex autograd documentation on our websiteã€‚

And that's where we stand todayï¼Œ going forwardï¼Œ we're actively working to a and distribute computing supportã€‚

 which would help us deliver performance wins bigger than beforeã€‚ğŸ˜Šã€‚

We're also expanding complex operator coverage and working to add native complex support to torch audioã€‚

 which uses a lot of complex numbersã€‚

![](img/30cfbf4aab59021d051c0076fb315c26_18.png)

So I hope we got you excited about using conflict numbers in Pytorchã€‚

 We had the documentation up on our website to get you startedï¼Œ So try itï¼Œ give us feedbackã€‚

 let us know what you think and really every bit of feedback we get like every bug report and every you know this was weird and we didn't really understand why this happened is really valuable to us we also have a lot of interesting ongoing discussions at Github like conflict support for To N N modules that you can get involved in Finallyã€‚

 if you would like to request new functionality or join the amazing community of contributors check us out on Githubã€‚

 we're constantly monitoring the complex table on Githubã€‚

 So filing new issues and engaging in discussions is a great way to let us know what youd like to see in the upcoming releasesã€‚

 I would like to bring this presentation to a close by giving a shout out to our amazing open source contributors and thanking them for adding a lot of conflict support to Pytorchã€‚

ğŸ˜Šã€‚

![](img/30cfbf4aab59021d051c0076fb315c26_20.png)

![](img/30cfbf4aab59021d051c0076fb315c26_21.png)

And thank you for tuning in today and showing interestã€‚

