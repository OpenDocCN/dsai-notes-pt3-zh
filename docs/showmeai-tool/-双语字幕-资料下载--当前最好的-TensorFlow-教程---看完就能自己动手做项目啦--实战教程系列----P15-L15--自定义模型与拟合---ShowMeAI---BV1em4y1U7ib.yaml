- en: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P15：L15- 自定义模型与拟合
    - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[双语字幕+资料下载]“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P15：L15- 自定义模型与拟合
    - ShowMeAI - BV1em4y1U7ib'
- en: Alrighty， what is going on guys。 Welcome back for another video In this video。
    we're gonna to explore how to build more flexible training loops。 So far we've
    been using model that fit and if you can use model that fit that's great。 but
    sometimes you need more flexibility。 So in this video we will look at customizing
    model that fit。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，伙计们发生了什么。 欢迎回来看另一个视频在这个视频中。 我们将探索如何构建更灵活的训练循环。 到目前为止，我们一直在使用模型适合，如果您可以使用模型适合那是很棒的。
    但有时候你需要更多的灵活性。 所以在这个视频中我们将看看如何自定义模型适合。
- en: And then in the next video how we will look at how to build custom training
    loops from scratch。![](img/a08532ac4d7c10fff24b74debc5b0588_1.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在下一个视频中，我们将看看如何从头开始构建自定义训练循环。 ![](img/a08532ac4d7c10fff24b74debc5b0588_1.png)
- en: '![](img/a08532ac4d7c10fff24b74debc5b0588_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a08532ac4d7c10fff24b74debc5b0588_2.png)'
- en: All right， so first of all， here are just some basic imports， those are you've
    all seen those before。 and then we're just going to load the the Ms data set。So
    we're not going to do anything complicated。 I'm just going to show you the general
    structure and then that can be applied to many different problems。Alright， so
    we're going to xtrain y train， x test， y test， and we're just going to M load
    data。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，首先，这里只是一些基本的导入，你们以前都见过这些。 然后我们只需加载Ms数据集。 所以我们不会做任何复杂的事情。 我只是想向你展示一般结构，然后可以应用于许多不同的问题。
    好吧，我们要xtrain y train，x 测试，y 测试，我们只是要M加载数据。
- en: then we're going to do xtrain is xtrain dot reshape then we're just going to
    have。 I guess minus1 for all the examples and then 28281 and we're doing reshape
    here just to add this channel right here。And then as type converted to flow 32
    and then normalize with dividing by 255。 So let let's copy this and let's go new
    line。And do X test。test， and then。Let's create our model。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要做 xtrain 是 xtrain 点重塑然后我们只会有。 我猜所有的例子减去1，然后28281，我们在这里做重塑只是为了添加这个频道。 然后作为类型转换为流32，然后通过除以255来规范化。
    所以让我们复制这个，让我们换一行。 干 X 测试。 测试，并且。 让我们创建我们的模型。
- en: first of all， so as the model is equal to ks that sequential。Then we're going
    to do layer thatt input。And then the shape of the input is 28，281。Layers come
    to the 64，3 kernel size and I'm padding same so I'm just going through this quickly。
    this is not really the most important part of the video。😔，So now that we have
    a model。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，所以模型等于ks顺序。 然后我们要做的层输入。 然后输入的形状是28,281。 层来到64,3内核大小，我填充相同，所以我只是快速地通过这个。 这实际上不是视频的最重要部分。
    😔，所以现在我们有了一个模型。
- en: we're going to create a class and we're going to call it custom fit。And we're
    going inherit from Kaosta model。 and then the first thing we're going to do is
    we're going to create an init function and all we're going to send in here is
    the model。 So we're going to first call super to inherit from Kaosta model。 So
    we're going to self and then in it。Then we're going to do set that model equal
    to model。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要创建一个类，我们要称它为自定义适合。 然后我们要从Kaosta模型继承。 然后我们要做的第一件事是我们要创建一个init函数，我们要发送到这里的所有东西都是模型。
    所以我们要首先调用超级来继承Kaosta模型。 所以我们要做的是自己然后在这里。 然后我们要做的是将模型设置为模型。
- en: Then what we're going to do is we're going to define one training step and that's
    going to be used in a model dot fit right。 So our goal is basically。We want to
    do something like training is a custom custom， wait custom。 what the hell custom
    fit of that model。 we're gonna to send in that model。 Then we're going to do training
    dot fit and we're gonna to send in x train， Y train。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要做的是定义一个训练步骤，这将在模型点拟合中使用。 所以我们的目标基本上是。 我们想做像训练一样的自定义，等待自定义。 那模型的什么自定义适合。
    我们要送进那个模型。 然后我们将做训练点适合，我们将送入 x 训练，Y 训练。
- en: And then batch size and then number of vpos sort of as normal。 although this
    dot fit is going to be done in a custom way。 we're going to sort of define how
    we want that to be done。So I mean there are many use cases of this you where you
    need to do custom training loops and sort of you use model Efi when you can and
    when you can't you try to customize your model thatfi which is what we're doing
    in this video and then for that most flexibility you do the training loop from
    scratch but an example of when you actually need to do this is generative adversarial
    networks I'm not assuming you're familiar with that I'm just sort of saying there
    are many examples where。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后批量大小和vpos数量通常是这样的。尽管这个适配将以自定义方式进行，我们将定义我们想要如何执行。因此，我的意思是，有很多用例需要自定义训练循环，尽可能使用模型Efi，当不能时尝试自定义你的模型，这就是我们在这个视频中所做的。为了获得最大的灵活性，你从头开始训练循环，但实际上需要这样做的一个例子是生成对抗网络。我并不假设你熟悉这个，只是说有很多这样的例子。
- en: This is useful。 Alright， so let's then do a train step。 we're going to send
    in data and then we're going that's going to be a twople of x and y。 so we're
    just going to do x and y is equal to data。Then what we're going to do is we're
    going to do with。TF gradient tape as tape。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有用。好的，那么我们进行一次训练步骤。我们将发送数据，然后我们将得到一个x和y的元组。所以我们只需让x和y等于数据。接着我们将使用TF梯度记录作为记录。
- en: And why we're doing this is because now we're going to do the for propagation
    and then the loss function。 and when we're doing it under that tape， it's going
    to record all of the operations that was done and then that will then be useful
    for calculating the gradients for back propagation。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做的原因是因为现在我们将进行前向传播和损失函数。当我们在那个上下文中进行操作时，它将记录所有执行的操作，这将有助于计算反向传播的梯度。
- en: So basically we're going to do y prediction is a self dot model。 We're going
    to send in x。 we're going to specify training is true， and then for this loss
    function。 we're going to do loss equals self dot compiled loss。 and then we're
    going to send in y and then y prediction。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们要做的y预测是一个自我点模型。我们将发送x。我们会指定训练为真，然后对于这个损失函数，我们将做损失等于自我编译的损失。然后我们将发送y和y预测。
- en: and this is going to be done in the compile So right here we're going to do
    training dot compile we're going to send in。And here we're going to send in optimizer
    is Kara's Oprs， Adamom。 then we're going to send in loss is Kara's losses， sparse，
    categorical cross entropy。 and from logic equals true。Then also we're going to
    do metrics is accuracy。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在编译中完成。所以在这里，我们将进行训练编译，我们将发送。在这里我们将发送优化器为Kara的Oprs，Adamom。然后我们将发送损失为Kara的损失，稀疏的分类交叉熵。逻辑等于真。然后我们还将做指标为准确性。
- en: And so this is for the first one where we're doing the compile。 I'm also going
    to show you how to do a custom compile。But let's take that as we go。 so we're
    going to first now continue doing the train step when we have this compile。 and
    so this self dot compiled loss is using this sparse category across entropy from
    the training dot compile。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们第一次进行编译的情况。我还将向你展示如何进行自定义编译。但让我们逐步进行。因此我们现在将继续在这个编译下进行训练步骤。自我编译的损失使用这个稀疏分类交叉熵来自训练编译。
- en: After that we basically want to get the gradients right， we've now done the
    for propagation。 this part is the for propagation， which we're doing with this
    gradient under this tape to record all of the operations。Then we're going to do
    training variables is a self dot trainable variables and these are all stored
    from this parent class。 this cars dot model， so we don't have to bother with that，
    then we want to get the gradient。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在那之后，我们基本上想要获得梯度，对吧，我们现在完成了前向传播。这部分是前向传播，我们在这个记录下进行所有操作。然后我们将训练变量设置为自我可训练变量，这些变量都存储在这个父类中。这辆车的模型，因此我们不必担心这个，然后我们想要获得梯度。
- en: we're going to do tape dot the gradient。And we're going to do loss and then
    training variables right。 so we're getting the gradient of the loss with respect
    to the training variables。 which is ultimately what we want to change。Then we're
    going to do a step， an optimizer step。 a gradient in descent step。 and we're going
    to do self dot optimizer dot apply。Gradients。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行带有梯度的记录。然后我们将做损失和训练变量，对吧。所以我们正在获得相对于训练变量的损失梯度，这正是我们想要改变的。接着我们将进行一步，优化器步骤，一个梯度下降步骤，我们将执行自我优化器的应用。梯度。
- en: And then here we're going do zip gradients and then training variables。 And
    then we're going to do self that compiled metrics that update state y and then
    y prediction。 And this is this is going to be for the accuracy。 And then in the
    end， we're going to return。M dot name。And you'll see what it means so M dot name
    M dot result for M in self dot metrics allright。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这里我们将执行zip梯度和训练变量。接下来我们将自我编译的度量更新状态Y和Y预测。这将用于准确率。最后，我们将返回M.name。你会看到它的含义，M.name
    M.result对于自我度量。
- en: so we we're getting the MD dot name which is going to be the loss for example。
    and then we're getting the result which is the current loss and then we're doing
    that for all of the metrics and that's going to be the loss and the accuracy in
    this case。And yeah， so I think that's it for just this first step。 and I think
    we should now be able to run this。And as you can see here， it does seem to work。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们获取MD.name，这将是损失，比如说。然后我们获取结果，即当前损失，然后我们为所有度量执行这一步，这将是损失和准确率。在这种情况下。我想这就是第一步的全部内容。我们现在应该能够运行这个。如你所见，它似乎有效。
- en: and yeah so basically。So basically the next step now is that we want to do our
    own compile。 so what we're going to do right here is we're going to define compile。
    we're going to send in the optimizer and we're going to send in the loss。And we're
    going to do super custom itself that compile。So yeah。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以基本上。接下来的步骤是我们想要进行自己的编译。所以我们要在这里定义编译。我们将传入优化器和损失。然后我们将执行自定义的编译。所以，是的。
- en: and then we're going to do self that optimizer is equal to optimizer， Se that
    loss is equal to loss。And all we have to do then is we have basically the same
    thing right， training。tcomp。 except we're not going to send in a metric right
    here。 so we're just going to use the optimizer and the loss。And。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将自我优化器等于优化器，自我损失等于损失。然后我们只需做基本上相同的事情，训练.tcomp。除了我们不在这里传入度量，所以我们只使用优化器和损失。
- en: And that should also basically be it now we just have to change this right here
    to this compiled loss。 we're just going to do self that loss， it's which we've
    stored right here。So self dot loss and then let's see， yeah， and then we can still
    use self dot optimizer and let's just rerun it。And now， as you can see， we're
    not getting an accuracy。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基本上就是这样，我们只需将这个地方改为编译后的损失。我们将使用存储在这里的自我损失。所以自我损失，然后让我们看看，是的，我们仍然可以使用自我优化器，然后让我们重新运行它。现在，如你所见，我们没有得到准确率。
- en: So we're going to have to keep track of a that metric by ourselvesse。 So what
    we can do is for example， we could。Created right here。 we could do。Accuracy metric
    is。Ca us that matrix that spae categorical accuracy。And let's just call it name。's
    see name equals。Accuracy。And then， in the。And then right here， inside of the compiled
    metric。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要自己跟踪这个度量。我们可以，比如说，在这里创建。我们可以做。准确率度量是。Ca us那个矩阵spae类别准确率。我们就叫它名字。's看名字等于。准确率。然后，在这里，在编译的度量内。
- en: what we're going to do is is accuracy metric that update state。 we're going
    to send in Y and then y prediction， and then we can remove this compiled metric。嗯。And
    yeah， so that should hopefully be it。 Let's see if we can run this。All right。
    so now since we're keeping track of the accuracy by ourselves。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的是准确率度量的更新状态。我们将传入Y和Y预测，然后我们可以去掉这个编译的度量。嗯。所以希望这就是全部。让我们看看能否运行这个。好的。既然我们自己在跟踪准确率。
- en: what we're going to do here is we're going to write it explicitly。 so we're
    going to do loss is in this case， just loss。And then we're going to do accuracy。Is。Accury
    metric dot result。And hopefully now we should get the loss and the accuracy Yeah。
    so this looks pretty familiar to what we did previously。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将明确写出。所以我们将做损失在这种情况下，就是损失。然后我们将做准确率。是。准确率度量.result。希望现在我们可以得到损失和准确率。是的。这看起来和我们之前做的很相似。
- en: except now we're doing the compile completely by ourselves。And then allright。
    so now we got the compile， we got a train step what we normally do as well is
    in the end after training。 we're doing training that evaluate and then x test，
    Y test， and then we're specifying batch size。 let's say 32。One thing here is that
    this dot fit works on the train step and then evaluate works on a test step。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 只是现在我们完全由自己来进行 compile。好的，所以现在我们得到了 compile，得到了训练步骤，我们通常会在训练结束后执行训练的评估，然后是 x
    test 和 y test，接着我们指定批量大小。我们说 32。这里有一件事是，这个 dot fit 在训练步骤上有效，而 evaluate 则在测试步骤上有效。
- en: so to make this work we actually need to define another function and we need
    to do test step although this one is going to be a little bit easier since well
    first of all we're going to unpack the data and then we're going to compute prediction
    so we're going to do by prediction is self。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个工作，我们实际上需要定义另一个函数，并且我们需要执行测试步骤，尽管这个步骤会稍微简单一点，因为首先我们将解包数据，然后计算预测，因此我们的预测将是
    self。
- en: t model X and then we're specifying the training is false。And what we're doing
    is this is if we're using batch norm or dropout。That has different behaviors during
    testing and training we're just telling the model this is now in testing。 so make
    sure that those modules that have different behaviors are set to test mode or
    evaluation mode。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在指定训练为 false 的同时使用 t model X。我们所做的是，如果我们使用批量归一化或 dropout，那么在测试和训练期间它们的行为是不同的，我们只是告诉模型现在是在测试模式。因此，请确保这些在不同情况下有不同行为的模块设置为测试模式或评估模式。
- en: Then we're going to compute the loss， which is sub loss of y prediction。And
    then we're going do accuracysymmetric do update state y y prediction。And in the
    end。 we're going to return a dictionary of loss， which is just going to be loss。
    and then accuracy。We're doing acrosymmetric that result all right， so this is
    a very like it's very similar to the training step。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将计算损失，即 y 预测的子损失。接着我们将进行准确性对称更新状态 y y 预测。最后，我们将返回一个包含损失的字典，损失就是 loss，准确性则是
    accuracy。我们正在执行对称的结果，一切都很好，因此这与训练步骤非常相似。
- en: although it's much more simplified and it's simplified because we're not doing
    a gradient descent update so we don't need to keep track of this tape of making
    sure that we have all the gradients and all of that stuff。And yeah， so let's run
    this for yeah two epochs and then let's do the evaluation。All right。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它被大大简化了，因为我们没有进行梯度下降更新，所以我们不需要跟踪这个 tape 来确保我们拥有所有的梯度等等。因此，让我们运行这个，大约两个 epochs，然后进行评估。好的。
- en: so after this， we see that we get 93 up the  first0poC， 97 and then almost 98
    on the test set。But yeah， so I mean what we want to establish here that this does
    seem to train and it's working so yeah that's how you create your own you know
    specifying the training step and a test step which overwrites how the training
    do fit and then training that evaluate is done so in this way you can build more
    complicated and complex models in the training steps but still have the flexibility
    of doing training dot fit and that means that you can still use the training do
    compile although in this last one we overwrote the。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这之后，我们看到在 test set 上得到了 93，第一次是 0poC，97，然后几乎达到 98。不过，我想我们想在这里建立的是，这似乎确实在训练并且运行良好。所以这就是如何创建你自己的，指定训练步骤和测试步骤，从而覆盖训练的
    fit，然后进行评估。这样，你可以在训练步骤中构建更复杂的模型，但仍然拥有执行训练 dot fit 的灵活性，这意味着你仍然可以使用训练 dot compile，尽管在最后一次我们进行了覆盖。
- en: The compile， but but you get the point in you can still use the their compile
    and the metrics and all of that stuff Yeah。 if you have any questions leave them
    in the comment section below thank you so much for watching the video and I hope
    to see you in the next one。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: compile，但你明白我的意思，你仍然可以使用他们的 compile 和所有那些指标。如果你有任何问题，请在下面的评论区留言，非常感谢你观看这个视频，希望下次再见到你。
- en: '![](img/a08532ac4d7c10fff24b74debc5b0588_4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a08532ac4d7c10fff24b74debc5b0588_4.png)'
