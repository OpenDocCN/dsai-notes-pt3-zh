- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P3：L1.3- 什么是迁移学习？ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来了！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P3：L1.3- 什么是迁移学习？ - ShowMeAI - BV1Jm4y1X7UL
- en: What is transfer learning？The idea of transfer learning is to leverage the knowledge
    acquired by a model train with lotss of data on another task。The Model A will
    be trained specifically for task A。Now let's say you want to train it all B for
    a different task。One option would be to train the model from scratch。This could
    take lots of computation。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是迁移学习？迁移学习的理念是利用在另一个任务上用大量数据训练的模型所获得的知识。模型A将专门为任务A进行训练。现在假设你想为不同的任务B训练它。一种选择是从头开始训练模型，这可能需要大量计算。
- en: time and data。Instead， we could initialize Model B with the same weights as
    Model A。 transferring the knowledge of Model A on T P。When training from scratch。
    all the middle' weights are initialized randomly。In this example。 we are training
    a belt model on the task of recognizing if two sentences are similar on it。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 时间和数据。相反，我们可以用与模型A相同的权重初始化模型B，从而将模型A在TP上的知识转移过来。从头开始训练时，所有的中间权重都是随机初始化的。在这个例子中，我们在识别两个句子是否相似的任务上训练一个模型。
- en: On the left， it's trained from scratch。And on the right， it's venting a protrained
    model。As we can see， using transfer learning on the preed model yields better
    results。And it doesn't matter if we train longer， so training from scratch is
    keptped around 70% accuracy。 or the wetrain would all beat the 86% easily。This
    is because portraytrained models are usually trained on large amounts of data。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 左边是从头开始训练，右边是微调的预训练模型。可以看到，在预训练模型上使用迁移学习能取得更好的结果。而且无论我们训练多久，从头开始的训练保持在70%的准确率，而微调的模型则能轻松达到86%。这是因为预训练模型通常是在大量数据上训练的。
- en: but provides a model with a statistical understanding of the language used during
    per traininging。In computer visions， transfer learning has been applied successfully
    for almost 10 years。Models are frequently retained on ImageNet by a asset set
    containing 1。2 million of photo images。Each image is classified by one of 1000
    levels。Training like this。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但提供一个模型对在预训练期间使用的语言有统计理解。在计算机视觉中，迁移学习已经成功应用了近10年。模型通常在一个包含120万张照片的ImageNet数据集上进行微调，每张图像被分类为1000个类别之一。这样的训练方式。
- en: unlabeled data is called supervised learning。In natural language processing。
    transfer learning is a bit more recent。A key difference with ImageNet is that
    the training is usually self supervised。Which means it doesn't require humanation
    for the labels。A very common portraying objective is to guess the next world in
    a sentence。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 无标签数据称为监督学习。在自然语言处理领域，迁移学习相对较新。与ImageNet的一个关键区别在于，训练通常是自监督的，这意味着不需要人工标注。一个很常见的预训练目标是猜测句子中的下一个单词。
- en: Which only requires lots and lots of text。GPT2， for instance。 was portrayed
    this way using the content of 45 million links posted by users on read。Another
    example of selfupvised per training objective is to predict the value of randomly
    massed squls。Which is similar to fit in blood B tests you may have done in school。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这只需要大量的文本。例如，GPT2就是这样使用用户在read上发布的4500万个链接的内容来表现的。另一个自监督预训练目标的例子是预测随机掩蔽的词汇，这类似于你在学校做的血液B测试。
- en: Et was between this way using the English Wikipedia and had1，000 and published
    books。In practice。 transfer learning is applied on a given model by throwing away
    its head。 that is its last layers focused on the per training objective。And we
    sitting it with have a new randomly initialized a suitable for the desk attempt。For
    instance。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用英语维基百科和1,000本已出版的书籍之间的方式。在实践中，迁移学习是通过抛弃一个给定模型的头部来应用的，即其最后几层专注于预训练目标。然后我们用一个新随机初始化的模型进行适配。例如。
- en: when you invented in the build model earlier， we remove the a that classified
    Mque and replaced it with a classifier with two outputposts since our task at
    two levels。To be as efficient as possible， the protrained model used should be
    as similar as possible to the task it's fine tune on。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你早些时候在构建模型时，我们移除了分类Mque的部分，并用具有两个输出的分类器替换它，因为我们的任务是两个层次的。为了尽可能高效，所用的预训练模型应该与其微调的任务尽可能相似。
- en: For instance， if the problem is to classify German sentences。It's best to use
    a German portraying model。But with the good comes to bad。 the betweened model
    does not only transfer its knowledge， but also any bias it may contain。ImageNe
    mostly contains images coming from the United States and Western Europe。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果问题是对德语句子进行分类，最好使用德语模型。但好的东西也会带来坏处。这个模型不仅传递其知识，还传递它可能包含的任何偏见。ImageNe主要包含来自美国和西欧的图像。
- en: so models fine tuned with it usually will perform better on images from these
    countries。But Beni also studied the bias in the prediction of its Gpyy3。 which
    was between using the guess and X world objectives。Changing the gender of the
    prone from E Westbury to she Westbury changed the predictions from mostly neutral
    objectives。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用它微调的模型通常在这些国家的图像上表现更好。但Beni也研究了其Gpyy3预测中的偏见，这涉及到使用猜测和X世界目标之间的关系。将性别从E Westbury改为she
    Westbury改变了大部分中立目标的预测。
- en: To almost only physical ones。In the model code of the GT2 Mor。 Open AI also
    acknowledges its bias and discourages its use in systems that interact with humans。![](img/d4c722df1d794b9f140ba25884ae87c9_1.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎仅限于物理模型。在GT2 Mor的模型代码中，Open AI也承认其偏见，并不鼓励在与人类互动的系统中使用它。![](img/d4c722df1d794b9f140ba25884ae87c9_1.png)
- en: Yeah。![](img/d4c722df1d794b9f140ba25884ae87c9_3.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。![](img/d4c722df1d794b9f140ba25884ae87c9_3.png)
