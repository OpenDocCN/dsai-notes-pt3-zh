- en: 【双语字幕+资料下载】Pytorch 进阶学习讲座！14位Facebook工程师带你解锁 PyTorch 的生产应用与技术细节 ＜官方教程系列＞ - P13：L13-
    PyTorch 移动版 - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】Pytorch进阶学习讲座！14位Facebook工程师带你解锁PyTorch的生产应用与技术细节 ＜官方教程系列＞ - P13：L13-
    PyTorch移动版 - ShowMeAI - BV1ZZ4y1U7dg
- en: 🎼。![](img/26aca7c4871d57c8c593b5a764582be0_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 🎼。![](img/26aca7c4871d57c8c593b5a764582be0_1.png)
- en: Hi， I'm David。I'm an engineer on Pytorch， and today I'm going to be talking
    about some of the improvements we've made to Pytorch Mobi over the past year。Specifically，
    improvements to CPU performance， our prototype support for GPUs。 our expanded
    documentation and tutorials， and some news about mobile inference accelerators。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是大卫。我是Pytorch的一名工程师，今天我要谈谈我们在过去一年中对Pytorch Mobi所做的一些改进。具体来说，CPU性能的提升，我们对GPU的原型支持，扩展的文档和教程，以及关于移动推理加速器的一些消息。
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_3.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_3.png)'
- en: So first， CPU performance。CPUs are the most ubiquitous computational units on
    mobile phones。They are also the most versatile and in some cases also the most
    powerful。 and so CPU performance will always be a high priority for us on PyTtorch
    Mo。![](img/26aca7c4871d57c8c593b5a764582be0_5.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，CPU性能。CPU是手机上最普遍的计算单元。它们也是最灵活的，在某些情况下也是最强大的。因此，CPU性能始终是我们在PyTtorch Mobi上的高优先级！![](img/26aca7c4871d57c8c593b5a764582be0_5.png)
- en: And we've made a lot of improvements over the past year。Here is a comparison
    between PyTtororch 1。3。 the first release of PyTtorch Mobi， and a latest release
    Pythtororch 1。7。As a benchmark。 we're using a floating point version of the mobileNet
    V2 model。And initially。 we were running this model in about 250 milliseconds，
    but now in the latest release。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一年里，我们做了很多改进。这里是PyTtorch 1.3、PyTtorch Mobi的首次发布和最新发布的Pytororch 1.7之间的比较。作为基准，我们使用了mobileNet
    V2模型的浮点版本。最初，我们大约在250毫秒内运行此模型，但现在在最新版本中。
- en: we're over 10 times faster， running this model in under 15 milliseconds。And
    so if you compare us to some of the other mobile inference frameworks out there。
    we're about in the middle of the pack， which is something we're pretty happy about
    considering the high quality of the other frameworks we're comparing against。Now，
    mobile phones are more limited than servers， and so we can't run all the fancy
    compiler machinery that Pytorch on the server side uses to get the best performance
    out of your model。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的速度超过了10倍，在15毫秒内运行此模型。因此，如果你将我们与其他一些移动推理框架进行比较，我们大约处于中间位置，考虑到我们与之比较的其他框架的高质量，我们对此感到相当满意。现在，手机的限制比服务器要多，因此我们不能运行所有花哨的编译器机制，以从模型中获得最佳性能。
- en: you have to do a little bit of upfront preparation to make sure your model runs
    as fast as possible。![](img/26aca7c4871d57c8c593b5a764582be0_7.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要进行一些前期准备，以确保你的模型运行得尽可能快。![](img/26aca7c4871d57c8c593b5a764582be0_7.png)
- en: But fortunately， we've been able to bundle all that up into a single function
    that you can call to get the best performance It's called optimizeize for mobile。
    it's very simple to use as you can see here， you just import it from Pytororch。
    you run it on your model and you save the resulting model to disk。And this will
    ensure that you're running optimizations like folding batch norm operations into
    a prior convolution。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们已经能够将所有这些内容打包成一个单一的函数，你可以调用它来获得最佳性能。它被称为**为移动优化**，使用起来非常简单，如你所见，只需从Pytororch导入它。你在模型上运行它，然后将生成的模型保存到磁盘。这将确保你执行诸如将批量归一化操作折叠到先前卷积中的优化。
- en: Prepacking model weights to get maximum throughput and running model freezing
    to eliminate unnecessary overhead。 and we hope to add more optimizations to this
    entry point in the future。So that covers the preparation of your model， but then
    when you run the model。 there are some other things you can do to improve performance
    as well。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 预打包模型权重以获得最大吞吐量，并运行模型冻结以消除不必要的开销。我们希望在未来将更多优化添加到这个入口点。因此，这涵盖了模型的准备，但在运行模型时，你还可以做一些其他事情来提升性能。
- en: We've recently released a caching allocator for memory。Now， the default behavior
    on PyTtorch。 both on mobile and on the server side。Is that as soon as you're done
    with a tensor。 its memory buffer gets immediately released back to the system
    allocator and in some cases。 back to the operating system， and this is great for
    memory efficiency。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近发布了一个用于内存的缓存分配器。现在，PyTtorch的默认行为，无论是在移动端还是服务器端，都是一旦你完成了张量，它的内存缓冲区就会立即释放回系统分配器，在某些情况下，释放回操作系统，这对于内存效率非常有利。
- en: but if you're running the same model over and over again。 you can waste a lot
    of time freeing and reallocating these buffers very quickly and the caching allocator
    gives you more explicit control over that memory policy。😊，So the way you use it
    is fairly simple， you create this caching allocator object。 normally right around
    the same time as when you're loading your model。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你不断重复运行相同的模型，你可能会浪费大量时间快速释放和重新分配这些缓冲区，而缓存分配器则可以让你更明确地控制内存策略。😊使用方式相当简单，你创建这个缓存分配器对象，通常在加载模型时就会创建。
- en: and then every time you're going to run the model。 you first create this caching
    allocator guard object。 which ensures that your caching allocator is being actively
    used during that inference。And that's all you need to do， we've seen in performance
    improvements on the order of 5 to 20% from using the caching allocator。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你要运行模型时，首先创建这个缓存分配器保护对象，确保你的缓存分配器在推理过程中被积极使用。你只需要这样做，我们观察到使用缓存分配器后性能提升在5%到20%之间。
- en: of course， at the cost of some increased memory usage while the inference is
    running now moving on from CPU to GPU。![](img/26aca7c4871d57c8c593b5a764582be0_9.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这会导致在推理运行时内存使用量增加，现在从CPU移动到GPU。![](img/26aca7c4871d57c8c593b5a764582be0_9.png)
- en: Many of you probably know that GPUs have been popular for machine learning on
    the server side for quite a while now。 but almost all mobile devices have a GPU
    as well， and they can be used for accelerating inference on device。Now， in high
    end phones with powerful GPUs， you can see significant performance wins from using
    the GPU。 but even on devices with more modest GPUs， you can get other benefits
    like reducing power consumption and freeing up the CPU to do other intensive operations
    like running a video chat call。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人可能知道，GPU在服务器端的机器学习中已经流行了一段时间，但几乎所有的移动设备也都有GPU，它们可以用于加速设备上的推理。在高端手机上，使用强大的GPU可以显著提升性能，但即使在配置较低的设备上，你也能获得其他好处，比如降低功耗和释放CPU以进行其他密集型操作，例如进行视频通话。
- en: for example。And so today we're releasing prototype support for GPU inference
    using two different APIs on iOS we have metal。 which is Apple's high performance
    low level API for GPU access， and on Android we have Vulcan。 which is the next
    generation open standard for crossplatform GPU access。The way you make use of
    these backends is with optimized for mobile。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，今天我们发布了对iOS上使用两种不同API进行GPU推理的原型支持，我们有Metal，这是苹果的高性能低级GPU访问API，而在Android上我们有Vulkan，这是下一代跨平台GPU访问的开放标准。使用这些后端的方式是针对移动设备进行优化。
- en: the utility function we introduced earlier for getting the best performance。
    you just have to pass this one other argument to it to tell it which backend you
    want to use and it will perform the appropriate preparations for your model。Now，
    when you run the model， there's a few extra small steps you need to do。This first
    example is for iOS。When you take your input tensor。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前介绍的实用函数用于获取最佳性能，你只需传递另一个参数，告诉它你想使用哪个后端，它将为你的模型进行适当的准备。现在，当你运行模型时，还有几个额外的小步骤需要做。这个第一个示例是针对iOS的。当你获取输入张量时。
- en: you need to call this dot metal method it to move it to the GPU so that metal
    can access it。 and then after you're finished running the model you need to call
    dot CPUU and your result to bring it back to the CPU for further processing。If
    you are using Vulcan on Android using the C++ API， you'll do something very similar。
    but if you're using our Java API， there's a simpler method。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要调用这个`.metal`方法，将其移动到GPU，以便Metal可以访问它。然后在运行完模型后，你需要调用`.CPUU`将结果带回CPU进行进一步处理。如果你在Android上使用Vulkan的C++
    API，你会做一些非常相似的事情。但如果你使用我们的Java API，会有更简单的方法。
- en: you can just pass this one extra argument when you're loading the model， device。Vulcan
    to tell it which device the model is going to run on and it can automatically
    handle moving your input and output to the GPU and back whenever you run your
    model。Now， the benefits that you get from running on GPU will vary from device
    to device and model to model。 but just to give one example， we've seen a 33% improvement
    in performance from switching to metal from our best CPU implementation with a
    ResNe 18 model on an iPhone 11。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载模型时，你只需传递一个额外的参数，即设备（device），告诉Vulcan模型将在哪个设备上运行，它可以在运行模型时自动处理输入和输出的移动到GPU和回退。现在，从GPU运行所获得的好处会因设备和模型而异，但举个例子，我们发现将ResNet
    18模型从我们最佳的CPU实现切换到Metal后，性能提高了33%，在iPhone 11上表现尤为明显。
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_11.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_11.png)'
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_12.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_12.png)'
- en: I think the release I'm most excited about today is not a code release at all。
    but rather our expanded set of documentation and tutorials one of our highest
    priorities on Pytororch mobile is to make it easy to use and accessible and we
    think that documentation is an important way of enabling that so I'll talk about
    some of the doc tutorials that we have available one of the most interesting ones
    is the Pytororch mobile performance recipes this is a onestop shop for performance
    tips and tricks including how to do operator fusion quantization。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我今天最期待的发布并不是代码发布，而是我们扩展的文档和教程集。Pytorch移动的首要任务之一是让其易于使用和可访问，我们认为文档是实现这一目标的重要方式。因此，我将介绍一些可用的文档教程，其中一个最有趣的是Pytorch移动性能食谱，这是一个关于性能技巧和窍门的一站式资源，包括如何进行操作符融合和量化。
- en: making sure that you're using the best memory format。![](img/26aca7c4871d57c8c593b5a764582be0_14.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你使用最佳的内存格式。![](img/26aca7c4871d57c8c593b5a764582be0_14.png)
- en: Making sure you're reusing memory appropriately。And also how to set up your
    benchmarks to make sure that you're able to measure your models and verify you're
    getting the gains that you expect from these optimizations。We are also releasing
    today tutorials for Vulcan and metal which go into a little bit more depth on
    how to use these APIs appropriately to get access to GPUs。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你适当地重用内存。同时，如何设置基准测试，以确保你能够测量你的模型，并验证你从这些优化中获得的收益。我们今天还发布了针对Vulcan和Metal的教程，深入介绍如何正确使用这些API以获取GPU的访问权限。
- en: And we have a number of demo apps that are being released to give you a live
    example of how you integrate Pythorch mobile into a app。 and these are available
    for both Android and iOS and they cover a set of features like image segmentation
    and machine translation。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发布了一些演示应用，展示如何将Pytorch移动集成到应用程序中，这些应用可用于Android和iOS，并涵盖了一系列功能，如图像分割和机器翻译。
- en: We also have a tutorial out that shows how to use a custom operator in an Android
    application this has been a little tricky so far because you have to configure
    your Android NK to use an external dependency。 but the tutorial should walk you
    through it and simplify the process and we have an example here of using the ROI
    align operator to run the sorry faster RCNN object detection model in this demo
    app。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发布了一个教程，展示如何在Android应用中使用自定义操作符，这到目前为止有点棘手，因为你必须配置Android NDK以使用外部依赖项。但这个教程将引导你完成这一过程并简化操作，这里有一个使用ROI对齐操作符在这个演示应用中运行更快的RCNN目标检测模型的示例。
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_16.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_16.png)'
- en: The last release we're making today is something to give you access to mobile
    inference accelerators these take the benefits of running your model on GPU and
    just bring them to the next level but I'm going to let the next presenter talk
    about the details of this release and I will just leave you with some links to
    further resources from the Pytororch。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天发布的最后一项是让你可以访问移动推理加速器，这些加速器利用在GPU上运行模型的好处，将其提升到一个新的水平，但我将让下一位演讲者来详细介绍这一发布的细节，并留下相关链接以获取更多Pytorch资源。
- en: org website you can click on mobile to see our homepage tutorials then mobile
    to see our in-depth tutorials and tutorials recipes mobile that will take you
    to the recipes which are little quick tips and tricks for how to use specific
    features I hope you have a great time using Pytororch mobile Thank you very much。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在官网上，您可以通过手机点击查看我们的首页教程，然后查看我们的深入教程和教程食谱，移动设备将带您到食谱，这些是关于如何使用特定功能的小技巧和窍门。希望您在使用Pytorch移动版时玩得愉快，非常感谢。
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_18.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_18.png)'
