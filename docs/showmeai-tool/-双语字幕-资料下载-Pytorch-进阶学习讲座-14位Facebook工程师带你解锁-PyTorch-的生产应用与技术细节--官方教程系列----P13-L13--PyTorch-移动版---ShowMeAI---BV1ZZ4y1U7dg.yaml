- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPytorch ËøõÈò∂Â≠¶‰π†ËÆ≤Â∫ßÔºÅ14‰ΩçFacebookÂ∑•Á®ãÂ∏àÂ∏¶‰Ω†Ëß£ÈîÅ PyTorch ÁöÑÁîü‰∫ßÂ∫îÁî®‰∏éÊäÄÊúØÁªÜËäÇ ÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P13ÔºöL13-
    PyTorch ÁßªÂä®Áâà - ShowMeAI - BV1ZZ4y1U7dg
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üéº„ÄÇ![](img/26aca7c4871d57c8c593b5a764582be0_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: HiÔºå I'm David„ÄÇI'm an engineer on PytorchÔºå and today I'm going to be talking
    about some of the improvements we've made to Pytorch Mobi over the past year„ÄÇSpecificallyÔºå
    improvements to CPU performanceÔºå our prototype support for GPUs„ÄÇ our expanded
    documentation and tutorialsÔºå and some news about mobile inference accelerators„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_3.png)'
  prefs: []
  type: TYPE_IMG
- en: So firstÔºå CPU performance„ÄÇCPUs are the most ubiquitous computational units on
    mobile phones„ÄÇThey are also the most versatile and in some cases also the most
    powerful„ÄÇ and so CPU performance will always be a high priority for us on PyTtorch
    Mo„ÄÇ![](img/26aca7c4871d57c8c593b5a764582be0_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: And we've made a lot of improvements over the past year„ÄÇHere is a comparison
    between PyTtororch 1„ÄÇ3„ÄÇ the first release of PyTtorch MobiÔºå and a latest release
    Pythtororch 1„ÄÇ7„ÄÇAs a benchmark„ÄÇ we're using a floating point version of the mobileNet
    V2 model„ÄÇAnd initially„ÄÇ we were running this model in about 250 millisecondsÔºå
    but now in the latest release„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're over 10 times fasterÔºå running this model in under 15 milliseconds„ÄÇAnd
    so if you compare us to some of the other mobile inference frameworks out there„ÄÇ
    we're about in the middle of the packÔºå which is something we're pretty happy about
    considering the high quality of the other frameworks we're comparing against„ÄÇNowÔºå
    mobile phones are more limited than serversÔºå and so we can't run all the fancy
    compiler machinery that Pytorch on the server side uses to get the best performance
    out of your model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you have to do a little bit of upfront preparation to make sure your model runs
    as fast as possible„ÄÇ![](img/26aca7c4871d57c8c593b5a764582be0_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: But fortunatelyÔºå we've been able to bundle all that up into a single function
    that you can call to get the best performance It's called optimizeize for mobile„ÄÇ
    it's very simple to use as you can see hereÔºå you just import it from Pytororch„ÄÇ
    you run it on your model and you save the resulting model to disk„ÄÇAnd this will
    ensure that you're running optimizations like folding batch norm operations into
    a prior convolution„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Prepacking model weights to get maximum throughput and running model freezing
    to eliminate unnecessary overhead„ÄÇ and we hope to add more optimizations to this
    entry point in the future„ÄÇSo that covers the preparation of your modelÔºå but then
    when you run the model„ÄÇ there are some other things you can do to improve performance
    as well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We've recently released a caching allocator for memory„ÄÇNowÔºå the default behavior
    on PyTtorch„ÄÇ both on mobile and on the server side„ÄÇIs that as soon as you're done
    with a tensor„ÄÇ its memory buffer gets immediately released back to the system
    allocator and in some cases„ÄÇ back to the operating systemÔºå and this is great for
    memory efficiency„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but if you're running the same model over and over again„ÄÇ you can waste a lot
    of time freeing and reallocating these buffers very quickly and the caching allocator
    gives you more explicit control over that memory policy„ÄÇüòäÔºåSo the way you use it
    is fairly simpleÔºå you create this caching allocator object„ÄÇ normally right around
    the same time as when you're loading your model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then every time you're going to run the model„ÄÇ you first create this caching
    allocator guard object„ÄÇ which ensures that your caching allocator is being actively
    used during that inference„ÄÇAnd that's all you need to doÔºå we've seen in performance
    improvements on the order of 5 to 20% from using the caching allocator„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: of courseÔºå at the cost of some increased memory usage while the inference is
    running now moving on from CPU to GPU„ÄÇ![](img/26aca7c4871d57c8c593b5a764582be0_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: Many of you probably know that GPUs have been popular for machine learning on
    the server side for quite a while now„ÄÇ but almost all mobile devices have a GPU
    as wellÔºå and they can be used for accelerating inference on device„ÄÇNowÔºå in high
    end phones with powerful GPUsÔºå you can see significant performance wins from using
    the GPU„ÄÇ but even on devices with more modest GPUsÔºå you can get other benefits
    like reducing power consumption and freeing up the CPU to do other intensive operations
    like running a video chat call„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: for example„ÄÇAnd so today we're releasing prototype support for GPU inference
    using two different APIs on iOS we have metal„ÄÇ which is Apple's high performance
    low level API for GPU accessÔºå and on Android we have Vulcan„ÄÇ which is the next
    generation open standard for crossplatform GPU access„ÄÇThe way you make use of
    these backends is with optimized for mobile„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the utility function we introduced earlier for getting the best performance„ÄÇ
    you just have to pass this one other argument to it to tell it which backend you
    want to use and it will perform the appropriate preparations for your model„ÄÇNowÔºå
    when you run the modelÔºå there's a few extra small steps you need to do„ÄÇThis first
    example is for iOS„ÄÇWhen you take your input tensor„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you need to call this dot metal method it to move it to the GPU so that metal
    can access it„ÄÇ and then after you're finished running the model you need to call
    dot CPUU and your result to bring it back to the CPU for further processing„ÄÇIf
    you are using Vulcan on Android using the C++ APIÔºå you'll do something very similar„ÄÇ
    but if you're using our Java APIÔºå there's a simpler method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can just pass this one extra argument when you're loading the modelÔºå device„ÄÇVulcan
    to tell it which device the model is going to run on and it can automatically
    handle moving your input and output to the GPU and back whenever you run your
    model„ÄÇNowÔºå the benefits that you get from running on GPU will vary from device
    to device and model to model„ÄÇ but just to give one exampleÔºå we've seen a 33% improvement
    in performance from switching to metal from our best CPU implementation with a
    ResNe 18 model on an iPhone 11„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_11.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_12.png)'
  prefs: []
  type: TYPE_IMG
- en: I think the release I'm most excited about today is not a code release at all„ÄÇ
    but rather our expanded set of documentation and tutorials one of our highest
    priorities on Pytororch mobile is to make it easy to use and accessible and we
    think that documentation is an important way of enabling that so I'll talk about
    some of the doc tutorials that we have available one of the most interesting ones
    is the Pytororch mobile performance recipes this is a onestop shop for performance
    tips and tricks including how to do operator fusion quantization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: making sure that you're using the best memory format„ÄÇ![](img/26aca7c4871d57c8c593b5a764582be0_14.png)
  prefs: []
  type: TYPE_NORMAL
- en: Making sure you're reusing memory appropriately„ÄÇAnd also how to set up your
    benchmarks to make sure that you're able to measure your models and verify you're
    getting the gains that you expect from these optimizations„ÄÇWe are also releasing
    today tutorials for Vulcan and metal which go into a little bit more depth on
    how to use these APIs appropriately to get access to GPUs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we have a number of demo apps that are being released to give you a live
    example of how you integrate Pythorch mobile into a app„ÄÇ and these are available
    for both Android and iOS and they cover a set of features like image segmentation
    and machine translation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We also have a tutorial out that shows how to use a custom operator in an Android
    application this has been a little tricky so far because you have to configure
    your Android NK to use an external dependency„ÄÇ but the tutorial should walk you
    through it and simplify the process and we have an example here of using the ROI
    align operator to run the sorry faster RCNN object detection model in this demo
    app„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_16.png)'
  prefs: []
  type: TYPE_IMG
- en: The last release we're making today is something to give you access to mobile
    inference accelerators these take the benefits of running your model on GPU and
    just bring them to the next level but I'm going to let the next presenter talk
    about the details of this release and I will just leave you with some links to
    further resources from the Pytororch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: org website you can click on mobile to see our homepage tutorials then mobile
    to see our in-depth tutorials and tutorials recipes mobile that will take you
    to the recipes which are little quick tips and tricks for how to use specific
    features I hope you have a great time using Pytororch mobile Thank you very much„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_18.png)'
  prefs: []
  type: TYPE_IMG
