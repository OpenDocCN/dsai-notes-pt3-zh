# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P2ï¼šL2- ç¥ç»ç½‘ç»œ - ShowMeAI - BV1wS4y1f7z1

Hiï¼Œ everybodyã€‚ Welcome to a new tutorialã€‚ This is the first video of a new series called machine learning from scratchã€‚ In this seriesï¼Œ we are going to implement popular machine learning algorithms using only built in Python modules and Nyã€‚ So todayï¼Œ we will start with the key nearest neighbor algorithm or short K and N algorithmã€‚ğŸ˜Šã€‚The concept of K And N is fairly easyã€‚ A sample is classified by a popularity vote of its nearest neighborsã€‚

 So let's consider an example with two classes and a two dimensional feature vectorã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_1.png)

So let's have a look at this figureã€‚ So here we have two classesã€‚ the blue class and the orange classï¼Œ and we have feature vectors with two dimensionsã€‚ So we have x1 on this axis and x 2 on this axisã€‚And what we do here is we have some training samplesã€‚And then for each new sample that we want to classifyã€‚

 we calculate the distance of this sample to each of the training samplesã€‚ And then we have a look at the nearest neighborsã€‚ So in this caseã€‚ we have a look at the three nearest neighborsã€‚ So these onesã€‚ And then we choose or predict the label based on the most commonã€‚Class labels hereã€‚

 So we have two blue glasses and one orange glassã€‚ So this then will be a blue classã€‚And this is the whole concept of the K and Nã€‚And what we also have to know is in order to calculate the distancesã€‚We used the Euclidean distanceã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_3.png)

So in a 2D exampleï¼Œ the Euclidean distance of two points is defined as the square root overã€‚ and then we have for eachã€‚ğŸ¤¢ï¼ŒFeature vector componentã€‚ we have the squaredã€‚Differenceã€‚ so we have x 2 minus x1 squared plus y2 minus y1 squaredã€‚ So this is the Euclidean distance in a 2D caseã€‚ and in a more general caseï¼Œ the formulaã€‚



![](img/46ae3dd4f47308ef10799ea81647c68c_5.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_6.png)

Is defined asã€‚Thisï¼Œ so it's the square root over the sum from I equals 0 to nã€‚ where n is the number of dimensionsã€‚ And then we have the sum overã€‚Eachã€‚Componentã€‚ and for each componentï¼Œ we calculate the squared distance or the square differenceã€‚So this is the Euclidean distanceï¼Œ and this is all we have to know in order to implement the K and Nã€‚

 So let's startã€‚ And firstï¼Œ let's define a class called K and Nã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_8.png)

And this hasï¼Œ of courseï¼Œ an innate methodã€‚So in itselfï¼Œ and this will get a Kã€‚ So this is the number of nearest neighbors we want to considerã€‚ And this will also get a default valueã€‚ So the default is 3ã€‚And in the in itã€‚Ohï¼Œ sorryã€‚We define the in itã€‚ So in the inï¼Œ we simply want to store the Kã€‚ So we say self dot K equals kã€‚

And then what we want to implement hereã€‚ And here we want to follow the conventions of other machine learning librariesã€‚ For exampleï¼Œ the psychic Learn libraryã€‚ So we have aã€‚å—¯ã€‚Fit methodã€‚So thisã€‚ this will fit the traininging samples and someã€‚Training labelsã€‚ And this will usually involve a the training stepã€‚ So we want to implement this oneã€‚

 And then we also want to implement a predict methodã€‚Sorryï¼Œ this also has selfã€‚And the predict methodã€‚ So here we want to predict new samplesã€‚So these are the methods we want to implementã€‚And nowï¼Œ before we go onï¼Œ first of allã€‚ let's have a look at how our data looksã€‚ So what is this X and this Yï¼Œ And for thisã€‚

 I wrote some test scriptã€‚ So here I used the famous irris data setsã€‚ So you might have heard about this alreadyã€‚ So I can get this from the ps could learn moduleã€‚ And then I will generate some training samples and some test samplesã€‚ and the associated training labels and test labelsã€‚



![](img/46ae3dd4f47308ef10799ea81647c68c_10.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_11.png)

So let's first have a look at how thisã€‚Training samplesã€‚ lookã€‚ so we want to print the shape of this oneã€‚So this is an an nuy N D array of shapeï¼Œ120 by 4ã€‚ So 120 is the number of samplesã€‚ and four is the number of features for each sampleã€‚ So for exampleã€‚ let's print the first sampleã€‚So this is four features in itã€‚

So this is how our training samples lookã€‚ And now let's have a look at our training labelsã€‚Soã€‚This is a 1 D row vectorï¼Œ also of size 120ã€‚ So for each of our training samplesï¼Œ we have theã€‚The label for itã€‚So if we print thisï¼Œ then we see this will be a 1 D vector with only one rowã€‚

![](img/46ae3dd4f47308ef10799ea81647c68c_13.png)

And now let's what we see here is we have label 0ï¼Œ1 and 2ã€‚ So this is a three class problemã€‚ So let's also plot thisã€‚å—¯ã€‚And nowï¼Œ for exampleï¼Œ what I plot here is I only plot the first twoã€‚Feature us so that we have a 2D caseã€‚So this is how our data looksã€‚ So we have three classesï¼Œ redã€‚ green and blue andã€‚

![](img/46ae3dd4f47308ef10799ea81647c68c_15.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_16.png)

Yeahã€‚So this is how our data looksã€‚ And now we can continue by implementing thisã€‚ So in our fit methodã€‚So in the K and N algorithmï¼Œ this doesn't involve a training stepã€‚ So what we do here is we simply want to store our training samples and then we can use them laterã€‚ so we can sayï¼Œ let's store themã€‚ So let's say selfã€‚

 and then we call this X train equals X and self Y trainã€‚Equals yã€‚So this is all for our fit methodã€‚And nowï¼Œ for our predict methodã€‚Soï¼Œ this will getã€‚Can get multiple samples hereï¼Œ So weã€‚å—¯ã€‚Can see thisï¼Œ because weï¼Œ we use a capital x for thisã€‚So this can have multiple samplesã€‚ so we can write a little helper methodã€‚ So we want to do this for each of the samplesã€‚

 We want to say we want to get the predictedã€‚Labelsã€‚Equalsï¼Œ and thenã€‚We useã€‚Or we write a helper method that we call underscore predictã€‚ And this will only get one sampleã€‚So here we use list comprehensionã€‚ So what we want to do then is we want to call this selfã€‚Predictã€‚With one sample Xã€‚ And then we want to do this for all of our samplesã€‚In ourã€‚Tests samples hereã€‚

 So for small x in capital xã€‚And then this will be a listã€‚ So let's convert this to a nuy arrayï¼Œ andã€‚Thenã€‚This is our predict methodã€‚ Andï¼Œ of courseï¼Œ we have to import Nmpyã€‚ So we say import Ny S and Pã€‚And nowï¼Œ how does our underscore predict method look likeï¼Œ So now againã€‚Let's have a lookã€‚

![](img/46ae3dd4f47308ef10799ea81647c68c_18.png)

At the figure hereã€‚ So what we do isã€‚We want to calculate all the distancesã€‚And then we have a look at the nearest neighbours and the labels of the nearest neighborsã€‚ And then we do a maturity vote and choose the most common class labelã€‚ So let's write some comments hereã€‚ Soï¼Œ first of allï¼Œ we want to compute the distancesã€‚



![](img/46ae3dd4f47308ef10799ea81647c68c_20.png)

Thenï¼Œ we want toã€‚Get the can nearest neighborsï¼Œ So get Caã€‚Nearest samplesï¼Œ andã€‚We want to get theã€‚Alsoï¼Œ want to get the labelsã€‚ And then we do a maturity modeã€‚So we want to get the mostã€‚Come on class labelã€‚Soã€‚Let's do thisã€‚ So let's sayï¼Œ distances equalsã€‚And nowï¼Œ as I saidã€‚ we use the Euclidean distance hereã€‚ So let's define thisã€‚

 and we want to define this as a global functionã€‚ So you might want to write this in a separate file or call this in some utility classã€‚ So here I will simply do it in the same fileã€‚ So I sayã€‚è¦ã€‚Kiyianã€‚This tenseã€‚Of two feature vectorsã€‚ So let's say x1 and x 2ã€‚ And now againï¼Œ let's have a look at the formulaã€‚ So this is the square rootã€‚ and then the sum over each square distanceã€‚ So we have the square rootã€‚

 So we can sayã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_22.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_23.png)

Nampai dot Sï¼Œ Qï¼Œ R Tã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_25.png)

And then we have the sumã€‚ so we can use numpy dot sumã€‚ So this will calculate the sum for over eachã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_27.png)

Feature vector componentã€‚ And here we have theã€‚Squared differenceã€‚ So we can say x 1 minus x2 to the power of 2ã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_29.png)

So this is all we needã€‚ And we want to return thisï¼Œ of courseã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_31.png)

And now in our predict methodsã€‚What we want to do isã€‚We want to calculate the distances of this one new sample to all the training samplesã€‚ So we also use list comprehensions hereï¼Œ and we call this Euclidean distance with our new test sampleã€‚Soï¼Œ and then to each of the training samplesã€‚ So we say small x trainã€‚

 And then we want to calculate this forã€‚Extrain in capitalï¼Œ or in self dotã€‚X trainã€‚So now we have all the distancesã€‚And now we want to get the key nearest samples and the labelsã€‚So what we do here is we sort our distances andã€‚We can do thisã€‚ So let's call this decayã€‚Indicesã€‚And thisã€‚ And here we use nuumpy dotã€‚Ark sortã€‚So this will sort the distances andã€‚

Will return the indicesã€‚Of how this is sortedã€‚ So here we callã€‚Distancesã€‚And this will be an arrayã€‚ And we also wantï¼Œ we only want to have the K closest samplesã€‚ So let's use slicing hereã€‚ And let's start at the beginningã€‚ So 0ã€‚ or we can omit thisã€‚ And this is goes only until self dot Kã€‚ So this will be theã€‚Indices of the K nearest samplesã€‚ And now let's get the labelsã€‚ So we get theã€‚

Kayã€‚Nearestã€‚Labels equalsã€‚ And here we canã€‚ we also use list comprehensionsã€‚ and then we get the label of each training and labelsã€‚With this in the indexï¼Œ So the index Iã€‚ and then for I in K indcsã€‚So now we have the labels of our nearest neighborsã€‚ and then we use a maturity vote and get the most common class labelã€‚

 So let's call this most common equalsã€‚ And for thisï¼Œ we use another Python moduleã€‚ So we use the counter moduleã€‚ So we say from collections importã€‚Counterã€‚And thenã€‚Weiiã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_33.png)

Get thereã€‚Or we get a counter of theï¼Œ nearest labelsã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_35.png)

And then this is a method called most commonã€‚ And we only want to have the first or the very most commonã€‚å—¯ã€‚And now let's have a look how this looksï¼Œ soã€‚If I comment this out and let's write a short example what the collections or the counter module will doã€‚

 So let's say we have a list A equalsï¼Œ and this is some values in itã€‚ So 1ï¼Œ1ï¼Œ1ï¼Œ2ï¼Œ2ã€‚ and then some errorã€‚Labelsï¼Œ as though also from collections importã€‚Counterã€‚And now we get the most common equalsï¼Œ soã€‚We create a counter of this listï¼Œ and then one most commonã€‚

![](img/46ae3dd4f47308ef10799ea81647c68c_37.png)

Soã€‚Let's print thisã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_39.png)

Soã€‚I have toã€‚Close this firstã€‚ So let's run this againã€‚å—¯ã€‚So this will be a listã€‚ and then we have a tuple of the most common itemã€‚ Soï¼Œ for exampleï¼Œ if weï¼Œ in this caseã€‚ we only haveï¼Œ we want to have oneï¼Œ the one most common itemã€‚ and this is a tupleã€‚ and the first item here is the most common itemã€‚ So this is oneã€‚ and the second item is theã€‚

Number of timesï¼Œ this is in our listã€‚ So it three times  oneã€‚ Soï¼Œ for exampleï¼Œ if you use twoã€‚ then it will also put in the second most common itemsã€‚ In this caseï¼Œ it is2ã€‚ and we have two times 2ã€‚ So we only wantã€‚One most commonã€‚So as I saidï¼Œ this is a listã€‚ So in order to get the first itemï¼Œ we use the index 0ã€‚

 So now we have the tuple and now to get the actual items againï¼Œ we use the first indexã€‚ and then we have oneã€‚So here we have to do the same thingï¼Œ and we want to return thisã€‚ So let's return most common of index 0 of index 0ã€‚And this is all our implementation of the K and N algorithmã€‚Soï¼Œ let's try this outï¼Œ soã€‚Let'sã€‚

 in our test exampleï¼Œ we already have our test fileã€‚ We already have our training and test samplesã€‚ So let's use ourã€‚Ourã€‚K and N classï¼Œ So we can say from K And N importã€‚K and Nã€‚ and then create a classifierã€‚ So Cï¼Œ F equalsã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_41.png)

K and Nã€‚And soï¼Œ let's useã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_43.png)

K equals 3ã€‚And then we want to call the fit method firstã€‚ So we want to fit our X trainã€‚And theã€‚Why trainã€‚And then we get the predictionsã€‚ And this isã€‚By we do this by calling CF dot predictã€‚ And then we want to predictã€‚Ourã€‚Test sample samplesã€‚ So x testã€‚And now let's calculate the accuracyã€‚ So accuracyã€‚ So this is defined by how many of our predictionsã€‚Are correctly classifiedã€‚

So this is theã€‚ So here we use the sumã€‚å—¯ã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_45.png)

The sumã€‚And hereï¼Œ we writeã€‚Predictions equalsï¼Œ equalsã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_47.png)

Why testã€‚So for each predictionï¼Œ that isã€‚The rightã€‚Or the same as the correct labelï¼Œ itã€‚Es1ã€‚And then we divide this by the number of test samplesã€‚ So we divide this by Lngï¼Œ yã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_49.png)

Testã€‚And let's print our accuracyã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_51.png)

And see if this worksã€‚Soï¼Œ yeahï¼Œ in this caseï¼Œ it's 1ã€‚0ã€‚ So all of our predictions areã€‚Correctã€‚ so let's use anotherã€‚Number of neighborsã€‚ So K equals 5ã€‚ So usually you want you want to use an odd number hereã€‚ So let's run thisã€‚Ohï¼Œ sorryã€‚ So in this caseï¼Œ it's 096ï¼Œ soã€‚Not as good as with three neighboursï¼Œ but also very goodã€‚ And yeahã€‚

 we see that this is workingã€‚ And this is our whole implementation of the K And Mã€‚ And yeahã€‚ I hope you enjoyed this tutorial and see you in the next tutorialï¼Œ byeã€‚ğŸ˜Šã€‚![](img/46ae3dd4f47308ef10799ea81647c68c_53.png)