- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P43ï¼šL8.2- ä½¿ç”¨Scikit-Learnå’ŒKerasæ„å»ºé›†æˆæ¨¡å‹
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P43ï¼šL8.2- ä½¿ç”¨Scikit-Learnå’ŒKerasæ„å»ºé›†æˆæ¨¡å‹
    - ShowMeAI - BV15f4y1w7b8
- en: '![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_0.png)'
- en: Hiï¼Œ this is Jeff Heatonï¼Œ welcome to applications of Deep neural Networks with
    Washington University in this video we're going to look at ensemblesã€‚We're going
    to do in particular heterogeneous ensembles this lets you takeã€‚Models that are
    not of the same type and combine them together for even stronger results for the
    latest on my AI course and projectsã€‚ click subscribe and the bell next to it to
    be notified of every new videoã€‚ Nowã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶é›†æˆæ–¹æ³•ã€‚æˆ‘ä»¬å°†ç‰¹åˆ«å…³æ³¨å¼‚æ„é›†æˆï¼Œè¿™è®©ä½ å¯ä»¥ç»“åˆä¸åŒç±»å‹çš„æ¨¡å‹ä»¥è·å¾—æ›´å¼ºçš„ç»“æœã€‚æƒ³äº†è§£æˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®ï¼Œè¯·ç‚¹å‡»è®¢é˜…åŠæ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚
- en: we're going look at a couple of techniques that might be very helpful to you
    for this semester's kle competitionã€‚ This allows you to use neural network with
    ensemblesã€‚ ensemmbling is a very important aspect of kggleã€‚ This is where you
    create heterogeneous ensemblesã€‚ Nowï¼Œ ensemmbling is something that's built into
    many machine learning algorithms such as random forestã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çœ‹çœ‹ä¸€äº›å¯¹ä½ åœ¨æœ¬å­¦æœŸKaggleæ¯”èµ›ä¸­éå¸¸æœ‰å¸®åŠ©çš„æŠ€å·§ã€‚è¿™å…è®¸ä½ ä½¿ç”¨ç¥ç»ç½‘ç»œä¸é›†æˆã€‚é›†æˆæ˜¯Kaggleä¸­çš„ä¸€ä¸ªéå¸¸é‡è¦çš„æ–¹é¢ã€‚è¿™å°±æ˜¯ä½ åˆ›å»ºå¼‚æ„é›†æˆçš„åœ°æ–¹ã€‚ç°åœ¨ï¼Œé›†æˆæ˜¯è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰å†…ç½®çš„åŠŸèƒ½ã€‚
- en: Random forests do ensemmbling just part of itselfã€‚ It includes trees and these
    trees are ensemled with each otherã€‚ because there's not multiple different types
    of model in thisã€‚ that's referred to as a homogeneous ensembleã€‚ Nowã€‚ we're going
    to look at evaluating feature importance Firstã€‚ This is a good paper that talks
    to you about how to do feature perturbation rankingã€‚ This is a veryã€‚ğŸ˜Šã€‚![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_2.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—åœ¨è‡ªèº«çš„åŸºç¡€ä¸Šå°±è¿›è¡Œé›†æˆã€‚å®ƒåŒ…æ‹¬æ ‘ï¼Œè€Œè¿™äº›æ ‘æ˜¯å½¼æ­¤é›†æˆçš„ã€‚å› ä¸ºè¿™é‡Œæ²¡æœ‰å¤šç§ä¸åŒç±»å‹çš„æ¨¡å‹ï¼Œæ‰€ä»¥è¿™è¢«ç§°ä¸ºåŒè´¨é›†æˆã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†é¦–å…ˆè¯„ä¼°ç‰¹å¾é‡è¦æ€§ã€‚è¿™æ˜¯ä¸€ç¯‡å¾ˆå¥½çš„è®ºæ–‡ï¼Œå‘Šè¯‰ä½ å¦‚ä½•è¿›è¡Œç‰¹å¾æ‰°åŠ¨æ’åã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ã€‚ğŸ˜Šã€‚![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_2.png)
- en: Popular technique that can be used across any type of regression or classification
    algorithmã€‚ It does not use any internals to the actual modelã€‚ This is also an
    interesting paper because it has both Drã€‚ Joy and Dr Death in it interesting part
    about the names of the authors of the paper These are some of the other methods
    that you can use to evaluate feature importanceã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§å¯ä»¥ç”¨äºä»»ä½•ç±»å‹å›å½’æˆ–åˆ†ç±»ç®—æ³•çš„æµè¡ŒæŠ€æœ¯ã€‚å®ƒä¸ä½¿ç”¨å®é™…æ¨¡å‹çš„ä»»ä½•å†…éƒ¨ã€‚è¿™ä¹Ÿæ˜¯ä¸€ç¯‡æœ‰è¶£çš„è®ºæ–‡ï¼Œå› ä¸ºé‡Œé¢æœ‰ä¹”ä¼Šåšå£«å’Œå¾·æ–¯åšå£«ï¼Œä½œè€…åå­—çš„æœ‰è¶£ä¹‹å¤„ã€‚è¿™äº›æ˜¯ä½ å¯ä»¥ç”¨æ¥è¯„ä¼°ç‰¹å¾é‡è¦æ€§çš„å…¶ä»–ä¸€äº›æ–¹æ³•ã€‚
- en: which input is the most importantã€‚ Now this is dealing mainly with tabular data
    where you have columns like you would see in Excelã€‚ if you're dealing with image
    data feature importance is a lot more difficult to really determine it's not like
    one pixel is more importantã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å“ªä¸ªè¾“å…¥æ˜¯æœ€é‡è¦çš„ã€‚ç°åœ¨ä¸»è¦æ˜¯å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼äºExcelä¸­çš„åˆ—ã€‚å¦‚æœä½ å¤„ç†çš„æ˜¯å›¾åƒæ•°æ®ï¼Œç‰¹å¾é‡è¦æ€§å°±æ›´éš¾ä»¥ç¡®å®šï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªåƒç´ æ›´é‡è¦ã€‚
- en: say than another pixelã€‚ This is also a paper up here that I was involved in
    where we publish code that could be used with Tensorflow to implement some of
    these algorithms up hereã€‚ So this is a function that just introduces and gives
    you a basic perturbation ranking algorithmã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´å¾—æ¯”å¦ä¸€ä¸ªåƒç´ æ›´é‡è¦ã€‚è¿™ä¹Ÿæ˜¯ä¸€ç¯‡æˆ‘å‚ä¸çš„è®ºæ–‡ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å¯ä»¥ä¸TensorFlowä¸€èµ·ä½¿ç”¨çš„ä»£ç æ¥å®ç°ä¸€äº›è¿™äº›ç®—æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªä»…ä»‹ç»å¹¶æä¾›åŸºæœ¬æ‰°åŠ¨æ’åç®—æ³•çš„å‡½æ•°ã€‚
- en: ''' go ahead and run it so that it''s loaded the way that thisã€‚Is actually pretty
    simpleã€‚ I have a separate video that''s not part of this courseã€‚ but it''s a video
    that I put together on how perturbation ranking worksã€‚ I''ll put a link to that
    so that you can access that if you would like to read up more on the internals
    of itã€‚'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '''ç»§ç»­è¿è¡Œå®ƒï¼Œè¿™æ ·å®ƒå°±ä¼šä»¥è¿™ç§æ–¹å¼åŠ è½½ã€‚å…¶å®è¿™éå¸¸ç®€å•ã€‚æˆ‘æœ‰ä¸€ä¸ªå•ç‹¬çš„è§†é¢‘ï¼Œä¸æ˜¯è¿™ä¸ªè¯¾ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œä½†æˆ‘åˆ¶ä½œäº†ä¸€ä¸ªå…³äºæ‰°åŠ¨æ’åå¦‚ä½•å·¥ä½œçš„å½±ç‰‡ã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£å…¶å†…éƒ¨å·¥ä½œï¼Œæˆ‘ä¼šæä¾›ä¸€ä¸ªé“¾æ¥ä¾›ä½ è®¿é—®ã€‚'
- en: But essentially what this is going to do is it's going to go through each column
    of the data set and shuffle them1 by one by oneã€‚ we're going to use the same neural
    network to evaluate all the columnsã€‚ but we're going to score at once for each
    columnã€‚ So say there are 10 columns in your x10 predictorsã€‚ You want to know which
    of those 10 is the most importantã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æœ¬è´¨ä¸Šï¼Œè¿™ä¸ªè¿‡ç¨‹å°†é€åˆ—éå†æ•°æ®é›†å¹¶é€ä¸€æ‰“ä¹±ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯„ä¼°æ‰€æœ‰åˆ—ï¼Œä½†æˆ‘ä»¬å°†ä¸ºæ¯ä¸€åˆ—ä¸€æ¬¡æ€§è¯„åˆ†ã€‚æ‰€ä»¥å‡è®¾ä½ çš„æ•°æ®é›†ä¸­æœ‰10åˆ—ï¼Œå³10ä¸ªé¢„æµ‹å˜é‡ã€‚ä½ æƒ³çŸ¥é“è¿™10ä¸ªä¸­å“ªä¸ªæ˜¯æœ€é‡è¦çš„ã€‚
- en: You start with the first one and you shuffle those you perturb them so that
    column1 is randomized that effectively destroys column1ã€‚ but the max and the menï¼Œ
    the standard deviation the mean the median and all that is still exactly the sameã€‚
    So you're not introducing any sort of biasï¼Œ but yet you're destroying one of the
    columnsã€‚ Now you evaluate the errorã€‚On that neural network producing predictions
    with that column randomized like thatã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä»ç¬¬ä¸€ä¸ªå¼€å§‹ï¼Œæ‰“ä¹±è¿™äº›åˆ—ï¼Œä½ å¯¹åˆ—1è¿›è¡Œæ‰°åŠ¨ï¼Œè¿™æ ·åˆ—1å°±è¢«éšæœºåŒ–ï¼Œè¿™æœ‰æ•ˆåœ°æ‘§æ¯äº†åˆ—1ã€‚ä½†æœ€å¤§å€¼ã€æœ€å°å€¼ã€æ ‡å‡†å·®ã€å‡å€¼ã€ä¸­ä½æ•°ç­‰ä»ç„¶å®Œå…¨ç›¸åŒã€‚æ‰€ä»¥ä½ æ²¡æœ‰å¼•å…¥ä»»ä½•åå·®ï¼Œä½†å´æ‘§æ¯äº†ä¸€åˆ—ã€‚ç°åœ¨ä½ è¯„ä¼°è¯¯å·®ã€‚åœ¨é‚£ä¸ªéšæœºåŒ–åˆ—çš„ç¥ç»ç½‘ç»œä¸Šç”Ÿæˆé¢„æµ‹ã€‚
- en: If column1 was not very importantï¼Œ then the score is not going to drop muchã€‚
    Your accuracy or your log loss or whichever one you're using if column1 was very
    importantã€‚ then shuffle in it is going to reallyï¼Œ really hurt your scoreã€‚ So that's
    why this perturbation rankã€‚ it's essentially looping for I in the range of how
    many columns we haveã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåˆ—1å¹¶ä¸æ˜¯éå¸¸é‡è¦ï¼Œé‚£ä¹ˆåˆ†æ•°ä¸ä¼šå¤§å¹…ä¸‹é™ã€‚ä½ çš„å‡†ç¡®ç‡æˆ–æ—¥å¿—æŸå¤±ï¼Œæˆ–è€…ä½ ä½¿ç”¨çš„å…¶ä»–æŒ‡æ ‡ï¼Œå¦‚æœåˆ—1éå¸¸é‡è¦ï¼Œé‚£ä¹ˆæ‰“ä¹±å®ƒå°†çœŸæ­£ã€çœŸæ­£ä¼¤å®³ä½ çš„åˆ†æ•°ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ‰°åŠ¨æ’åã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯å¯¹æˆ‘ä»¬æ‹¥æœ‰çš„åˆ—æ•°è¿›è¡Œå¾ªç¯ã€‚
- en: we make a copy of the column because we're about to shuffle it then we shuffle
    it effectively destroying itã€‚ but we have a copy so that we can we don't want
    to be like a tornado it'll lightnt a path of destruction across the countrysideã€‚
    we want to restore it and not be a tornadoã€‚ So then we look at if it's regressionã€‚
    if it's regressionï¼Œ we do a prediction on this and we look at the mean squared
    errorã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¤åˆ¶è¿™ä¸€åˆ—ï¼Œå› ä¸ºæˆ‘ä»¬å³å°†æ‰“ä¹±å®ƒï¼Œç„¶åæœ‰æ•ˆåœ°æ‘§æ¯å®ƒã€‚ä½†æˆ‘ä»¬æœ‰ä¸€ä¸ªå‰¯æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æƒ³åƒé¾™å·é£ä¸€æ ·ï¼Œå®ƒä¼šåœ¨ä¹¡é—´ç•™ä¸‹æ¯ç­çš„ç—•è¿¹ã€‚æˆ‘ä»¬æƒ³è¦æ¢å¤å®ƒï¼Œè€Œä¸æ˜¯æˆä¸ºé¾™å·é£ã€‚æ¥ç€æˆ‘ä»¬æŸ¥çœ‹æ˜¯å¦æ˜¯å›å½’é—®é¢˜ã€‚å¦‚æœæ˜¯å›å½’ï¼Œæˆ‘ä»¬ä¼šå¯¹æ­¤è¿›è¡Œé¢„æµ‹ï¼Œå¹¶æŸ¥çœ‹å‡æ–¹è¯¯å·®ã€‚
- en: if it is a classificationï¼Œ then we predict the probabilities and we do log lossã€‚
    Both of those give us an error that we want to minimizeã€‚And we keep track of our
    errors and then we restore the column that we previously destroyedã€‚ We determine
    what the max error was and we basically calculate the importance of each of these
    relative to how close it was to the maximum errorã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬é¢„æµ‹æ¦‚ç‡å¹¶è®¡ç®—æ—¥å¿—æŸå¤±ã€‚è¿™ä¸¤è€…éƒ½ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªå¸Œæœ›æœ€å°åŒ–çš„è¯¯å·®ã€‚æˆ‘ä»¬è·Ÿè¸ªæˆ‘ä»¬çš„è¯¯å·®ï¼Œç„¶åæ¢å¤æˆ‘ä»¬ä¹‹å‰æ‘§æ¯çš„åˆ—ã€‚æˆ‘ä»¬ç¡®å®šæœ€å¤§è¯¯å·®æ˜¯ä»€ä¹ˆï¼Œç„¶ååŸºæœ¬ä¸Šè®¡ç®—è¿™äº›åˆ—ç›¸å¯¹äºå…¶æ¥è¿‘æœ€å¤§è¯¯å·®çš„ç›¸å¯¹é‡è¦æ€§ã€‚
- en: the maximum errorï¼Œ the column that resulted in the maximum error is the most
    important column so the most important column is going to have a 1ã€‚0 for importance
    all the others will be some proportion of thatã€‚ we're going to run the iris data
    set through this the decent sizeized neural network well that just fit it so we
    have a model now fit for that then I'm going to get the accuracy which perfect
    accuracyã€‚ not hard to do with the iris data set and I'm going to run and you can
    see basically the importance so the pal length is the most important column to
    predicting what ir you're dealing with and then it drops considerably you will
    only know which column is the most important you won't know how important the
    most important column is you just have a ranking of these soã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§è¯¯å·®ï¼Œå¯¼è‡´æœ€å¤§è¯¯å·®çš„åˆ—æ˜¯æœ€é‡è¦çš„åˆ—ï¼Œå› æ­¤æœ€é‡è¦çš„åˆ—çš„é‡è¦æ€§å°†ä¸º1.0ï¼Œå…¶ä»–åˆ—ä¼šæ˜¯å…¶æŸä¸ªæ¯”ä¾‹ã€‚æˆ‘ä»¬å°†ç”¨é¸¢å°¾èŠ±æ•°æ®é›†æ¥è¿è¡Œè¿™ä¸ªé€‚åˆçš„ç¥ç»ç½‘ç»œï¼Œå› æ­¤æˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ªé€‚åˆè¯¥æ•°æ®é›†çš„æ¨¡å‹ï¼Œç„¶åæˆ‘å°†è·å¾—å®Œç¾çš„å‡†ç¡®ç‡ã€‚è¿™åœ¨é¸¢å°¾èŠ±æ•°æ®é›†ä¸­å¹¶ä¸éš¾åšåˆ°ï¼Œç„¶åæˆ‘å°†è¿è¡Œï¼Œä½ å¯ä»¥åŸºæœ¬ä¸Šçœ‹åˆ°é‡è¦æ€§ï¼Œæ‰€ä»¥èŠ±è¼é•¿åº¦æ˜¯é¢„æµ‹ä½ æ­£åœ¨å¤„ç†çš„é¸¢å°¾èŠ±ç§ç±»çš„æœ€é‡è¦åˆ—ï¼Œç„¶åå®ƒçš„å½±å“æ€¥å‰§ä¸‹é™ã€‚ä½ åªä¼šçŸ¥é“å“ªä¸ªåˆ—æ˜¯æœ€é‡è¦çš„ï¼Œè€Œä¸çŸ¥é“æœ€é‡è¦åˆ—çš„é‡è¦æ€§åˆ°åº•æœ‰å¤šå¤§ï¼Œä½ åªæ˜¯å¾—åˆ°è¿™äº›åˆ—çš„æ’åã€‚
- en: You'll never see anything higher or lower than 1ã€‚0 for the most important column
    We can also do this on regressionã€‚ We'll use the miles per gallon database because
    that is a pretty simple oneã€‚ you can apply this to muchï¼Œ much more complicated
    neural networks it will take a little bit longer to run if you're doing this on
    your kggle because you might have 2030ã€‚40100 columns and it needs to run across
    each of those Now after running this we can see which of the various fields are
    most important to the miles per gallon neural network we can see that the displacementã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ°¸è¿œä¸ä¼šçœ‹åˆ°æ¯”1.0æ›´é«˜æˆ–æ›´ä½çš„æœ€é‡è¦åˆ—ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨å›å½’ä¸Šåšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¯åŠ ä»‘å¤šå°‘è‹±é‡Œçš„æ•°æ®åº“ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç›¸å½“ç®€å•çš„ä¾‹å­ã€‚ä½ å¯ä»¥å°†å…¶åº”ç”¨äºæ›´å¤æ‚çš„ç¥ç»ç½‘ç»œï¼Œå¦‚æœä½ åœ¨Kaggleä¸Šè¿›è¡Œæ­¤æ“ä½œï¼Œå¯èƒ½ä¼šèŠ±è´¹æ›´é•¿çš„æ—¶é—´ï¼Œå› ä¸ºä½ å¯èƒ½æœ‰20ã€30ã€40ã€100åˆ—ï¼Œå¹¶ä¸”éœ€è¦åœ¨æ¯ä¸€åˆ—ä¸Šè¿è¡Œã€‚ç°åœ¨ï¼Œåœ¨è¿è¡Œæ­¤åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å“ªäº›å­—æ®µå¯¹æ¯åŠ ä»‘å¤šå°‘è‹±é‡Œçš„ç¥ç»ç½‘ç»œæœ€é‡è¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä½ç§»ã€‚
- en: which is just the cubic inches or some other volume of measure for the engine
    is the most important next is horsepower weight year so on and so forth cylinders
    would be already represented by the displacement so it's not too surprising to
    me that that is a less important one but you'll notice most of these are period
    close in terms of their overall importance to that Now we're going to take aã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç«‹æ–¹è‹±å¯¸æˆ–å…¶ä»–å‘åŠ¨æœºä½“ç§¯æµ‹é‡çš„æœ€é‡è¦ç‰¹å¾ï¼Œæ¥ä¸‹æ¥æ˜¯é©¬åŠ›ã€é‡é‡ã€å¹´ä»½ç­‰ç­‰ã€‚æ°”ç¼¸å°†é€šè¿‡ä½ç§»è¡¨ç¤ºï¼Œæ‰€ä»¥æˆ‘ä¸å¤ªæƒŠè®¶è¿™æ˜¯ä¸€ä¸ªè¾ƒä¸é‡è¦çš„ç‰¹å¾ï¼Œä½†ä½ ä¼šæ³¨æ„åˆ°å¤§å¤šæ•°ç‰¹å¾åœ¨æ•´ä½“é‡è¦æ€§æ–¹é¢ç›¸å¯¹æ¥è¿‘ã€‚ç°åœ¨æˆ‘ä»¬å°†é‡‡å–ä¸€ä¸ªè¡ŒåŠ¨ã€‚
- en: At the biological response data set that is provided by Caleã€‚ because I'm going
    to use this as an example of how to build an ensembleã€‚ If we open this one upã€‚
    I'm not going to do itï¼Œ but I have a link there that you can look atã€‚ It's basically
    got nearly 1700 columns and maybe 3000 or so rowsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯Caleæä¾›çš„ç”Ÿç‰©å“åº”æ•°æ®é›†ã€‚å› ä¸ºæˆ‘å°†ä»¥æ­¤ä½œä¸ºæ„å»ºé›†æˆçš„ç¤ºä¾‹ã€‚å¦‚æœæˆ‘ä»¬æ‰“å¼€è¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä¸ä¼šå®é™…æ“ä½œï¼Œä½†æˆ‘æœ‰ä¸€ä¸ªé“¾æ¥å¯ä»¥ä¾›ä½ æŸ¥çœ‹ã€‚å®ƒåŸºæœ¬ä¸Šæœ‰è¿‘1700åˆ—å’Œå¤§çº¦3000è¡Œã€‚
- en: So it's got a tremendous amount of columns Feature importance could be useful
    to maybe remove some of thoseã€‚ Unfortunatelyï¼Œ most of them are pretty importantã€‚
    What we're going to look at here is how we can combine theseã€‚Into a ensembleã€‚
    And by theseï¼Œ I mean several different models like neural networkï¼Œ random forestã€‚
    gradient boosting and so onã€‚ So I'm going to go ahead and run thisã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒæœ‰å¤§é‡çš„åˆ—ã€‚ç‰¹å¾é‡è¦æ€§å¯èƒ½æœ‰åŠ©äºç§»é™¤å…¶ä¸­ä¸€äº›ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¤§å¤šæ•°éƒ½æ˜¯ç›¸å½“é‡è¦çš„ã€‚æˆ‘ä»¬è¦å…³æ³¨çš„æ˜¯å¦‚ä½•å°†è¿™äº›ç»„åˆæˆä¸€ä¸ªé›†æˆã€‚è¿™é‡Œçš„â€œè¿™äº›â€æ˜¯æŒ‡å‡ ç§ä¸åŒçš„æ¨¡å‹ï¼Œæ¯”å¦‚ç¥ç»ç½‘ç»œã€éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ç­‰ã€‚æ‰€ä»¥æˆ‘ä¼šç»§ç»­è¿è¡Œè¿™ä¸ªã€‚
- en: which will essentially open theseï¼Œ these filesã€‚ I have them resident here on
    a local driveã€‚ These are kagle filesã€‚ So I can't actually put them in a place
    that would let you access themã€‚ You need to download them yourself from Kagalã€‚
    I just put them in a data directoryã€‚ You can really put that any location you
    wantã€‚ And I run thatã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åŸºæœ¬ä¸Šæ‰“å¼€è¿™äº›æ–‡ä»¶ã€‚æˆ‘å°†å®ƒä»¬ä¿å­˜åœ¨æœ¬åœ°é©±åŠ¨å™¨ä¸Šã€‚è¿™äº›æ˜¯Kaggleæ–‡ä»¶ã€‚å› æ­¤æˆ‘å®é™…ä¸Šä¸èƒ½å°†å®ƒä»¬æ”¾åœ¨ä¸€ä¸ªå¯ä»¥è®©ä½ è®¿é—®çš„åœ°æ–¹ã€‚ä½ éœ€è¦è‡ªå·±ä»Kaggleä¸‹è½½å®ƒä»¬ã€‚æˆ‘åªæ˜¯æŠŠå®ƒä»¬æ”¾åœ¨æ•°æ®ç›®å½•ä¸­ã€‚ä½ å¯ä»¥æŠŠå®ƒæ”¾åœ¨ä»»ä½•ä½ æƒ³è¦çš„ä½ç½®ã€‚æˆ‘ä¼šè¿è¡Œè¿™ä¸ªã€‚
- en: You can see here when I print out the shape of thisã€‚ It's really a fairly square
    data setï¼Œ which isã€‚ which is difficult where you've got nearly as many columns
    as you do row 3700 rows 17ï¼Œ7ï¼Œ7 columnsã€‚ So let's go ahead and we're going to run
    thisï¼Œ fit a neural network on this and get some predictionsã€‚ This is a classificationã€‚Neural
    networkï¼Œ because it is basically telling us if a biological response happened
    or notã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ï¼Œå½“æˆ‘æ‰“å°å‡ºè¿™ä¸ªæ•°æ®é›†çš„å½¢çŠ¶æ—¶ï¼Œå®ƒå®é™…ä¸Šæ˜¯ä¸€ä¸ªç›¸å½“æ–¹å½¢çš„æ•°æ®é›†ï¼Œè¿™å¾ˆå›°éš¾ï¼Œå› ä¸ºåˆ—å‡ ä¹å’Œè¡Œä¸€æ ·å¤šï¼Œ3700è¡Œå’Œ1777åˆ—ã€‚æˆ‘ä»¬ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œæ‹Ÿåˆä¸€ä¸ªç¥ç»ç½‘ç»œå¹¶è·å–ä¸€äº›é¢„æµ‹ã€‚è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ç¥ç»ç½‘ç»œï¼Œå› ä¸ºå®ƒåŸºæœ¬ä¸Šå‘Šè¯‰æˆ‘ä»¬æ˜¯å¦å‘ç”Ÿäº†ç”Ÿç‰©å“åº”ã€‚
- en: You can see that the validation log loss is around 0ã€‚55ã€‚ log losses is what
    Cagel is actually using for this particular one to rank itã€‚ The validation accuracy
    is around 76%ã€‚ So not good not horribleã€‚ We'll look at the featureã€‚ importance
    for this oneã€‚ It essentiallyï¼Œ most of these are in the 90sã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°éªŒè¯çš„æ—¥å¿—æŸå¤±å¤§çº¦æ˜¯0.55ã€‚æ—¥å¿—æŸå¤±æ˜¯Kaggleå®é™…ç”¨äºæ’åçš„ã€‚éªŒè¯å‡†ç¡®ç‡å¤§çº¦ä¸º76%ã€‚æ‰€ä»¥ï¼Œè¿™å¹¶ä¸å¥½ï¼Œä¹Ÿä¸ç®—å¤ªç³Ÿã€‚æˆ‘ä»¬ä¼šçœ‹çœ‹è¿™ä¸ªçš„ç‰¹å¾é‡è¦æ€§ã€‚åŸºæœ¬ä¸Šï¼Œå¤§å¤šæ•°ç‰¹å¾éƒ½åœ¨90ä»¥ä¸Šã€‚
- en: And even past into the 1700s is also 90sã€‚ So they're all importantã€‚ So that
    is very difficult with this particular oneã€‚ ensembles were very critical to getting
    a good score for this one for the actual kagle competitors who worked on itã€‚ So
    I am going to start by just introducing some code that I have hereã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”šè‡³è¿½æº¯åˆ°1700å¹´ä»£ä¹Ÿæœ‰90å¹´ä»£çš„æ•°æ®ã€‚å› æ­¤å®ƒä»¬éƒ½æ˜¯é‡è¦çš„ã€‚è¿™å¯¹äºè¿™ä¸ªç‰¹å®šçš„ä»»åŠ¡æ¥è¯´éå¸¸å›°éš¾ã€‚é›†æˆå¯¹è·å¾—è‰¯å¥½çš„å¾—åˆ†è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹å‚ä¸è¿™ä¸ªç«èµ›çš„Kaggleç«äº‰è€…æ¥è¯´ã€‚å› æ­¤ï¼Œæˆ‘å°†å¼€å§‹ä»‹ç»ä¸€äº›æˆ‘è¿™é‡Œçš„ä»£ç ã€‚
- en: and you can use this to build up an ensembleã€‚ You see here I have code that
    builds a artificial neural networkã€‚ I'm going go ahead and run this because it
    takes it a little while to run and explain what's going on while it is actuallyã€‚This
    builds the artificial neural networkã€‚ I am giving it a number of classes hereã€‚
    Typically you'll want this is really just placeholder codeã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¥æ„å»ºä¸€ä¸ªé›†æˆã€‚ä½ çœ‹åˆ°è¿™é‡Œæˆ‘æœ‰æ„å»ºäººå·¥ç¥ç»ç½‘ç»œçš„ä»£ç ã€‚æˆ‘å°†ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€äº›æ—¶é—´ï¼Œå¹¶åœ¨å®é™…è¿è¡Œæ—¶è§£é‡Šå‘ç”Ÿäº†ä»€ä¹ˆã€‚è¿™æ„å»ºäº†äººå·¥ç¥ç»ç½‘ç»œã€‚æˆ‘åœ¨è¿™é‡Œç»™å‡ºäº†ä¸€ä¸ªç±»åˆ«çš„æ•°é‡ã€‚é€šå¸¸ä½ ä¼šå¸Œæœ›è¿™ä»…ä»…æ˜¯å ä½ç¬¦ä»£ç ã€‚
- en: you'll want to put in more dense layers than I have hereã€‚ I also calculate the
    log loss multi log lossã€‚ that's a type of error calculation that we saw earlier
    in the modulesã€‚ and the stretch code here is basically used to normalize the y
    ranges that are predictedã€‚ So it's it's type of averaging or normalization to
    stretch it outã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†éœ€è¦æ¯”æˆ‘åœ¨è¿™é‡Œæ›´å¤šçš„ç¨ å¯†å±‚ã€‚æˆ‘è¿˜è®¡ç®—äº†å¯¹æ•°æŸå¤±å¤šé‡å¯¹æ•°æŸå¤±ã€‚è¿™æ˜¯ä¸€ç§æˆ‘ä»¬åœ¨æ—©æœŸæ¨¡å—ä¸­çœ‹åˆ°çš„é”™è¯¯è®¡ç®—ç±»å‹ã€‚è¿™é‡Œçš„æ‹‰ä¼¸ä»£ç åŸºæœ¬ä¸Šç”¨äºè§„èŒƒåŒ–é¢„æµ‹çš„yèŒƒå›´ã€‚å› æ­¤ï¼Œå®ƒæ˜¯ä¸€ç§å¹³å‡æˆ–è§„èŒƒåŒ–çš„æ–¹å¼æ¥æ‹‰ä¼¸å®ƒã€‚
- en: This is a technique that I've seen in a couple of kggsã€‚ I copied it from one
    of the winning solutions hereã€‚ you'll want to look if you're doing a regression
    or single classification like this it might be useful to youã€‚ I am going to use
    the stratified kfold Basically that is making sure that each of our folds are
    balanced in the same way that the training set is Otherwise you might introduceã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§æˆ‘åœ¨å‡ ä¸ªKaggleæ¯”èµ›ä¸­çœ‹åˆ°çš„æŠ€æœ¯ã€‚æˆ‘ä»è¿™é‡Œä¸€ä¸ªè·èƒœæ–¹æ¡ˆä¸­å¤åˆ¶äº†å®ƒã€‚å¦‚æœä½ åœ¨åšå›å½’æˆ–åƒè¿™æ ·çš„å•ä¸€åˆ†ç±»ï¼Œè¿™å¯èƒ½å¯¹ä½ æœ‰ç”¨ã€‚æˆ‘å°†ä½¿ç”¨åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ï¼ŒåŸºæœ¬ä¸Šæ˜¯ç¡®ä¿æˆ‘ä»¬çš„æ¯ä¸€æŠ˜åœ¨è®­ç»ƒé›†ä¸­çš„åˆ†å¸ƒæ˜¯å¹³è¡¡çš„ï¼Œå¦åˆ™ä½ å¯èƒ½ä¼šå¼•å…¥ã€‚
- en: Inconsistenciesï¼Œ if you haveï¼Œ say 20% positive in the overall training setã€‚
    you want 20% positive in each of those kfoldsã€‚ Otherwise your ground truth is
    going to be offã€‚ I have information on the stratified kfold in the previous module
    that talks aboutã€‚How to do cross validation Here we have a list of models and
    these modelsã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸€è‡´æ€§ï¼Œæ¯”å¦‚è¯´åœ¨æ•´ä½“è®­ç»ƒé›†ä¸­æœ‰20%çš„æ­£æ ·æœ¬ã€‚ä½ å¸Œæœ›åœ¨æ¯ä¸ªKæŠ˜ä¸­ä¹Ÿæœ‰20%çš„æ­£æ ·æœ¬ã€‚å¦åˆ™ä½ çš„çœŸå®æ ‡ç­¾ä¼šåç¦»ã€‚æˆ‘åœ¨ä¹‹å‰çš„æ¨¡å—ä¸­æœ‰å…³äºåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯çš„ä¿¡æ¯ï¼Œè®¨è®ºäº†å¦‚ä½•è¿›è¡Œäº¤å‰éªŒè¯ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ¨¡å‹åˆ—è¡¨ï¼Œè¿™äº›æ¨¡å‹ã€‚
- en: these are all the ones that you want to ensemble togetherã€‚ So I am building
    an ensemble of the kras classifier to we basically build that artificial neural
    network that we have up thereã€‚ random forest classifier a couple of times and
    also extra treesã€‚ which is a type of random forest and then also gradient boostingã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›éƒ½æ˜¯ä½ æƒ³è¦ç»„åˆåœ¨ä¸€èµ·çš„ã€‚å› æ­¤ï¼Œæˆ‘æ­£åœ¨æ„å»ºä¸€ä¸ªå…‹æ‹‰æ–¯åˆ†ç±»å™¨çš„é›†æˆï¼ŒåŸºæœ¬ä¸Šæ˜¯æ„å»ºæˆ‘ä»¬ä¸Šé¢æåˆ°çš„äººå·¥ç¥ç»ç½‘ç»œã€‚éšæœºæ£®æ—åˆ†ç±»å™¨è¿è¡Œäº†å‡ æ¬¡ï¼Œè¿˜æœ‰é¢å¤–çš„æ ‘ï¼Œè¿™æ˜¯ä¸€ç§éšæœºæ£®æ—ï¼Œç„¶åè¿˜æœ‰æ¢¯åº¦æå‡ã€‚
- en: I load my data sets and I run across all of these and build up the ensembleã€‚
    I have other videos that I'll link to that get into really the mechanics of what
    this is all doing overall what is happening here is it's building up a data set
    where each of these model predictions is one column So since we have  oneã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åŠ è½½æˆ‘çš„æ•°æ®é›†ï¼Œç„¶ååœ¨è¿™äº›æ¨¡å‹ä¸Šè¿è¡Œï¼Œå¹¶æ„å»ºé›†æˆã€‚æˆ‘è¿˜æœ‰å…¶ä»–è§†é¢‘ä¼šé“¾æ¥åˆ°è¿™äº›å†…å®¹ï¼Œæ·±å…¥è®²è§£è¿™èƒŒåçš„æœºåˆ¶ã€‚æ€»ä½“ä¸Šï¼Œå‘ç”Ÿçš„äº‹æƒ…æ˜¯æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹éƒ½æ˜¯ä¸€åˆ—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªã€‚
- en: 2ï¼Œ3ï¼Œ4ï¼Œ56ï¼Œ7 we have 7 of thoseã€‚ you're going to essentially have seven columns
    the Y is going to be the realã€‚ğŸ˜Šï¼ŒFrom the data setï¼Œ whether the biological response
    happened or notã€‚ and you're essentially training a linear regression across all
    of theseã€‚ So using the outputsã€‚ the predictions from all of these classifiers
    to predict what the actual output would beã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¼Œ3ï¼Œ4ï¼Œ56ï¼Œ7ï¼Œæˆ‘ä»¬æœ‰7ä¸ªè¿™æ ·çš„ã€‚ä½ å°†åŸºæœ¬ä¸Šæœ‰ä¸ƒåˆ—ï¼ŒYå°†æ˜¯çœŸå®çš„ğŸ˜Šï¼Œæ¥è‡ªæ•°æ®é›†ï¼Œç”Ÿç‰©ååº”æ˜¯å¦å‘ç”Ÿã€‚ä½ å®é™…ä¸Šæ˜¯åœ¨æ‰€æœ‰è¿™äº›ä¸Šè®­ç»ƒçº¿æ€§å›å½’ã€‚å› æ­¤ï¼Œåˆ©ç”¨æ‰€æœ‰è¿™äº›åˆ†ç±»å™¨çš„è¾“å‡ºå’Œé¢„æµ‹æ¥é¢„æµ‹å®é™…çš„è¾“å‡ºã€‚
- en: You're using these models as inputs to another modelã€‚ which is the ensembling
    model to form that predictionï¼Œ Then we blend it togetherã€‚ we're using logistic
    regression to do thatã€‚It's a type of linear regressionã€‚ And we build that fit
    based on thatã€‚ And then we finally build our prediction file based on the output
    from that linear regressionã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ­£åœ¨å°†è¿™äº›æ¨¡å‹ç”¨ä½œå¦ä¸€ä¸ªæ¨¡å‹çš„è¾“å…¥ï¼Œå³é›†æˆæ¨¡å‹æ¥å½¢æˆé¢„æµ‹ï¼Œç„¶åæˆ‘ä»¬å°†å…¶æ··åˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬ä½¿ç”¨é€»è¾‘å›å½’æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™æ˜¯ä¸€ç§çº¿æ€§å›å½’ã€‚æˆ‘ä»¬æ ¹æ®æ­¤å»ºç«‹é€‚åˆåº¦ï¼Œæœ€ååŸºäºçº¿æ€§å›å½’çš„è¾“å‡ºæ„å»ºæˆ‘ä»¬çš„é¢„æµ‹æ–‡ä»¶ã€‚
- en: Here you can see we're basically going through all of the folds on each of these
    various model types and it continues At the endã€‚ it will give you the final submission
    file that you will actually send to Cagelã€‚ Thank you for watching this video in
    the next videoã€‚ we're going to take a survey of all of the hyperparameters that
    make up neural networks and see how you can better optimize thoseã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åŸºæœ¬ä¸Šæ­£åœ¨æŸ¥çœ‹è¿™äº›ä¸åŒæ¨¡å‹ç±»å‹çš„æ‰€æœ‰æŠ˜å ï¼Œæœ€åå®ƒå°†ç»™å‡ºä½ å®é™…ä¸Šä¼šå‘é€ç»™Cagelçš„æœ€ç»ˆæäº¤æ–‡ä»¶ã€‚æ„Ÿè°¢ä½ è§‚çœ‹æœ¬è§†é¢‘ï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æ„æˆç¥ç»ç½‘ç»œçš„æ‰€æœ‰è¶…å‚æ•°è¿›è¡Œè°ƒæŸ¥ï¼Œçœ‹çœ‹ä½ å¦‚ä½•æ›´å¥½åœ°ä¼˜åŒ–è¿™äº›å‚æ•°ã€‚
- en: This content changes oftenã€‚ So subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligenceã€‚ğŸ˜Šã€‚![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_4.png)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å†…å®¹ç»å¸¸å˜åŒ–ã€‚å› æ­¤ï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿éšæ—¶äº†è§£æœ¬è¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜çš„æœ€æ–°åŠ¨æ€ã€‚ğŸ˜Šï¼![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_4.png)
