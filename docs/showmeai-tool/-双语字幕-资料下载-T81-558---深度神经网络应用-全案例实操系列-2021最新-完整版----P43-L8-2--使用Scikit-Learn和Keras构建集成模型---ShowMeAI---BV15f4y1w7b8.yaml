- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P43ÔºöL8.2- ‰ΩøÁî®Scikit-LearnÂíåKerasÊûÑÂª∫ÈõÜÊàêÊ®°Âûã
    - ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_0.png)'
  prefs: []
  type: TYPE_IMG
- en: HiÔºå this is Jeff HeatonÔºå welcome to applications of Deep neural Networks with
    Washington University in this video we're going to look at ensembles„ÄÇWe're going
    to do in particular heterogeneous ensembles this lets you take„ÄÇModels that are
    not of the same type and combine them together for even stronger results for the
    latest on my AI course and projects„ÄÇ click subscribe and the bell next to it to
    be notified of every new video„ÄÇ Now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're going look at a couple of techniques that might be very helpful to you
    for this semester's kle competition„ÄÇ This allows you to use neural network with
    ensembles„ÄÇ ensemmbling is a very important aspect of kggle„ÄÇ This is where you
    create heterogeneous ensembles„ÄÇ NowÔºå ensemmbling is something that's built into
    many machine learning algorithms such as random forest„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Random forests do ensemmbling just part of itself„ÄÇ It includes trees and these
    trees are ensemled with each other„ÄÇ because there's not multiple different types
    of model in this„ÄÇ that's referred to as a homogeneous ensemble„ÄÇ Now„ÄÇ we're going
    to look at evaluating feature importance First„ÄÇ This is a good paper that talks
    to you about how to do feature perturbation ranking„ÄÇ This is a very„ÄÇüòä„ÄÇ![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_2.png)
  prefs: []
  type: TYPE_NORMAL
- en: Popular technique that can be used across any type of regression or classification
    algorithm„ÄÇ It does not use any internals to the actual model„ÄÇ This is also an
    interesting paper because it has both Dr„ÄÇ Joy and Dr Death in it interesting part
    about the names of the authors of the paper These are some of the other methods
    that you can use to evaluate feature importance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which input is the most important„ÄÇ Now this is dealing mainly with tabular data
    where you have columns like you would see in Excel„ÄÇ if you're dealing with image
    data feature importance is a lot more difficult to really determine it's not like
    one pixel is more important„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: say than another pixel„ÄÇ This is also a paper up here that I was involved in
    where we publish code that could be used with Tensorflow to implement some of
    these algorithms up here„ÄÇ So this is a function that just introduces and gives
    you a basic perturbation ranking algorithm„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ''' go ahead and run it so that it''s loaded the way that this„ÄÇIs actually pretty
    simple„ÄÇ I have a separate video that''s not part of this course„ÄÇ but it''s a video
    that I put together on how perturbation ranking works„ÄÇ I''ll put a link to that
    so that you can access that if you would like to read up more on the internals
    of it„ÄÇ'
  prefs: []
  type: TYPE_NORMAL
- en: But essentially what this is going to do is it's going to go through each column
    of the data set and shuffle them1 by one by one„ÄÇ we're going to use the same neural
    network to evaluate all the columns„ÄÇ but we're going to score at once for each
    column„ÄÇ So say there are 10 columns in your x10 predictors„ÄÇ You want to know which
    of those 10 is the most important„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You start with the first one and you shuffle those you perturb them so that
    column1 is randomized that effectively destroys column1„ÄÇ but the max and the menÔºå
    the standard deviation the mean the median and all that is still exactly the same„ÄÇ
    So you're not introducing any sort of biasÔºå but yet you're destroying one of the
    columns„ÄÇ Now you evaluate the error„ÄÇOn that neural network producing predictions
    with that column randomized like that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If column1 was not very importantÔºå then the score is not going to drop much„ÄÇ
    Your accuracy or your log loss or whichever one you're using if column1 was very
    important„ÄÇ then shuffle in it is going to reallyÔºå really hurt your score„ÄÇ So that's
    why this perturbation rank„ÄÇ it's essentially looping for I in the range of how
    many columns we have„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we make a copy of the column because we're about to shuffle it then we shuffle
    it effectively destroying it„ÄÇ but we have a copy so that we can we don't want
    to be like a tornado it'll lightnt a path of destruction across the countryside„ÄÇ
    we want to restore it and not be a tornado„ÄÇ So then we look at if it's regression„ÄÇ
    if it's regressionÔºå we do a prediction on this and we look at the mean squared
    error„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if it is a classificationÔºå then we predict the probabilities and we do log loss„ÄÇ
    Both of those give us an error that we want to minimize„ÄÇAnd we keep track of our
    errors and then we restore the column that we previously destroyed„ÄÇ We determine
    what the max error was and we basically calculate the importance of each of these
    relative to how close it was to the maximum error„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the maximum errorÔºå the column that resulted in the maximum error is the most
    important column so the most important column is going to have a 1„ÄÇ0 for importance
    all the others will be some proportion of that„ÄÇ we're going to run the iris data
    set through this the decent sizeized neural network well that just fit it so we
    have a model now fit for that then I'm going to get the accuracy which perfect
    accuracy„ÄÇ not hard to do with the iris data set and I'm going to run and you can
    see basically the importance so the pal length is the most important column to
    predicting what ir you're dealing with and then it drops considerably you will
    only know which column is the most important you won't know how important the
    most important column is you just have a ranking of these so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You'll never see anything higher or lower than 1„ÄÇ0 for the most important column
    We can also do this on regression„ÄÇ We'll use the miles per gallon database because
    that is a pretty simple one„ÄÇ you can apply this to muchÔºå much more complicated
    neural networks it will take a little bit longer to run if you're doing this on
    your kggle because you might have 2030„ÄÇ40100 columns and it needs to run across
    each of those Now after running this we can see which of the various fields are
    most important to the miles per gallon neural network we can see that the displacement„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is just the cubic inches or some other volume of measure for the engine
    is the most important next is horsepower weight year so on and so forth cylinders
    would be already represented by the displacement so it's not too surprising to
    me that that is a less important one but you'll notice most of these are period
    close in terms of their overall importance to that Now we're going to take a„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: At the biological response data set that is provided by Cale„ÄÇ because I'm going
    to use this as an example of how to build an ensemble„ÄÇ If we open this one up„ÄÇ
    I'm not going to do itÔºå but I have a link there that you can look at„ÄÇ It's basically
    got nearly 1700 columns and maybe 3000 or so rows„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it's got a tremendous amount of columns Feature importance could be useful
    to maybe remove some of those„ÄÇ UnfortunatelyÔºå most of them are pretty important„ÄÇ
    What we're going to look at here is how we can combine these„ÄÇInto a ensemble„ÄÇ
    And by theseÔºå I mean several different models like neural networkÔºå random forest„ÄÇ
    gradient boosting and so on„ÄÇ So I'm going to go ahead and run this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which will essentially open theseÔºå these files„ÄÇ I have them resident here on
    a local drive„ÄÇ These are kagle files„ÄÇ So I can't actually put them in a place
    that would let you access them„ÄÇ You need to download them yourself from Kagal„ÄÇ
    I just put them in a data directory„ÄÇ You can really put that any location you
    want„ÄÇ And I run that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can see here when I print out the shape of this„ÄÇ It's really a fairly square
    data setÔºå which is„ÄÇ which is difficult where you've got nearly as many columns
    as you do row 3700 rows 17Ôºå7Ôºå7 columns„ÄÇ So let's go ahead and we're going to run
    thisÔºå fit a neural network on this and get some predictions„ÄÇ This is a classification„ÄÇNeural
    networkÔºå because it is basically telling us if a biological response happened
    or not„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the validation log loss is around 0„ÄÇ55„ÄÇ log losses is what
    Cagel is actually using for this particular one to rank it„ÄÇ The validation accuracy
    is around 76%„ÄÇ So not good not horrible„ÄÇ We'll look at the feature„ÄÇ importance
    for this one„ÄÇ It essentiallyÔºå most of these are in the 90s„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And even past into the 1700s is also 90s„ÄÇ So they're all important„ÄÇ So that
    is very difficult with this particular one„ÄÇ ensembles were very critical to getting
    a good score for this one for the actual kagle competitors who worked on it„ÄÇ So
    I am going to start by just introducing some code that I have here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and you can use this to build up an ensemble„ÄÇ You see here I have code that
    builds a artificial neural network„ÄÇ I'm going go ahead and run this because it
    takes it a little while to run and explain what's going on while it is actually„ÄÇThis
    builds the artificial neural network„ÄÇ I am giving it a number of classes here„ÄÇ
    Typically you'll want this is really just placeholder code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you'll want to put in more dense layers than I have here„ÄÇ I also calculate the
    log loss multi log loss„ÄÇ that's a type of error calculation that we saw earlier
    in the modules„ÄÇ and the stretch code here is basically used to normalize the y
    ranges that are predicted„ÄÇ So it's it's type of averaging or normalization to
    stretch it out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is a technique that I've seen in a couple of kggs„ÄÇ I copied it from one
    of the winning solutions here„ÄÇ you'll want to look if you're doing a regression
    or single classification like this it might be useful to you„ÄÇ I am going to use
    the stratified kfold Basically that is making sure that each of our folds are
    balanced in the same way that the training set is Otherwise you might introduce„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: InconsistenciesÔºå if you haveÔºå say 20% positive in the overall training set„ÄÇ
    you want 20% positive in each of those kfolds„ÄÇ Otherwise your ground truth is
    going to be off„ÄÇ I have information on the stratified kfold in the previous module
    that talks about„ÄÇHow to do cross validation Here we have a list of models and
    these models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: these are all the ones that you want to ensemble together„ÄÇ So I am building
    an ensemble of the kras classifier to we basically build that artificial neural
    network that we have up there„ÄÇ random forest classifier a couple of times and
    also extra trees„ÄÇ which is a type of random forest and then also gradient boosting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I load my data sets and I run across all of these and build up the ensemble„ÄÇ
    I have other videos that I'll link to that get into really the mechanics of what
    this is all doing overall what is happening here is it's building up a data set
    where each of these model predictions is one column So since we have  one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 2Ôºå3Ôºå4Ôºå56Ôºå7 we have 7 of those„ÄÇ you're going to essentially have seven columns
    the Y is going to be the real„ÄÇüòäÔºåFrom the data setÔºå whether the biological response
    happened or not„ÄÇ and you're essentially training a linear regression across all
    of these„ÄÇ So using the outputs„ÄÇ the predictions from all of these classifiers
    to predict what the actual output would be„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You're using these models as inputs to another model„ÄÇ which is the ensembling
    model to form that predictionÔºå Then we blend it together„ÄÇ we're using logistic
    regression to do that„ÄÇIt's a type of linear regression„ÄÇ And we build that fit
    based on that„ÄÇ And then we finally build our prediction file based on the output
    from that linear regression„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here you can see we're basically going through all of the folds on each of these
    various model types and it continues At the end„ÄÇ it will give you the final submission
    file that you will actually send to Cagel„ÄÇ Thank you for watching this video in
    the next video„ÄÇ we're going to take a survey of all of the hyperparameters that
    make up neural networks and see how you can better optimize those„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This content changes often„ÄÇ So subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligence„ÄÇüòä„ÄÇ![](img/a5e310b66ad5b4d0e89ea1a7cc4dbf83_4.png)
  prefs: []
  type: TYPE_NORMAL
