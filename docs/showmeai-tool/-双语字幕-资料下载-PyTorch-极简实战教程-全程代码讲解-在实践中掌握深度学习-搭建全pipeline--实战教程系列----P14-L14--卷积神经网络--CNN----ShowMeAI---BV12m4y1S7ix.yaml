- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P14ï¼šL14-
    å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P14ï¼šL14-
    å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome to a new Pytorch tutorialã€‚ Todayã€‚ we are implementing
    a convolutional neural network and do image classification based on the Cypher
    10 dataset setã€‚ The Cypher 10 is a very popular image data set with 10 different
    classesã€‚ Like we have airplanesã€‚ carsï¼Œ birdsï¼Œ cats and other classesã€‚ and this
    data set is available directly in Pytorsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°æ–°çš„ PyTorch æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå¹¶åŸºäº CIFAR-10 æ•°æ®é›†è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚CIFAR-10 æ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«
    10 ç§ä¸åŒçš„ç±»åˆ«ï¼Œæ¯”å¦‚é£æœºã€æ±½è½¦ã€é¸Ÿç±»ã€çŒ«ä»¥åŠå…¶ä»–ç±»åˆ«ã€‚è¿™ä¸ªæ•°æ®é›†åœ¨ PyTorch ä¸­ç›´æ¥å¯ç”¨ã€‚
- en: So we will create a convolutional neural net that can classify these imagesã€‚
    So now let's talk about convolutional neural networks very brieflyã€‚ I will not
    go into too much detail nowï¼Œ because this tutorial should be focused on the Pytorch
    implementationã€‚ but I will provide further links in the description if you want
    to learn more in detailã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå¯ä»¥å¯¹è¿™äº›å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚ç°åœ¨è®©æˆ‘ä»¬ç®€è¦è°ˆè°ˆå·ç§¯ç¥ç»ç½‘ç»œã€‚æˆ‘ä¸ä¼šè¿‡å¤šç»†èŠ‚ï¼Œå› ä¸ºæœ¬æ•™ç¨‹åº”è¯¥ä¸“æ³¨äº PyTorch çš„å®ç°ï¼Œä½†å¦‚æœä½ æƒ³æ›´è¯¦ç»†åœ°äº†è§£ï¼Œæˆ‘ä¼šåœ¨æè¿°ä¸­æä¾›è¿›ä¸€æ­¥çš„é“¾æ¥ã€‚ğŸ˜Š
- en: So convolutional neural nets or confnets are similar to ordinary neural networksã€‚
    They are made up of neurons that have learnable weights and biasesã€‚ And the main
    difference now is that convolutional nets mainly work on image data and apply
    the so called convolutional filtersã€‚ So a typical conf architecture looks like
    thisã€‚So we have our imageã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œæˆ–ç§°ä¸º confnets ä¸æ™®é€šç¥ç»ç½‘ç»œç±»ä¼¼ã€‚å®ƒä»¬ç”±å…·æœ‰å¯å­¦ä¹ æƒé‡å’Œåå·®çš„ç¥ç»å…ƒç»„æˆã€‚ä¸»è¦çš„åŒºåˆ«åœ¨äºå·ç§¯ç½‘ç»œä¸»è¦å¤„ç†å›¾åƒæ•°æ®ï¼Œå¹¶åº”ç”¨æ‰€è°“çš„å·ç§¯æ»¤æ³¢å™¨ã€‚å› æ­¤ï¼Œå…¸å‹çš„
    conf æ¶æ„çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„å›¾åƒã€‚
- en: and then we have different convolutional layers and optional activationation
    functions followed by socalled pooling layersã€‚ And these layers are used to automatically
    learn some features from the imagesã€‚ and then at the endã€‚ we have a one or more
    fully connected layers for the actual classification taskã€‚ So yeahã€‚ this is a
    typical architecture of a CNNã€‚And these convolutional filtersã€‚Nowã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰ä¸åŒçš„å·ç§¯å±‚å’Œå¯é€‰çš„æ¿€æ´»å‡½æ•°ï¼Œæ¥ç€æ˜¯æ‰€è°“çš„æ± åŒ–å±‚ã€‚è¿™äº›å±‚ç”¨äºè‡ªåŠ¨å­¦ä¹ ä¸€äº›å›¾åƒç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªå…¨è¿æ¥å±‚ç”¨äºå®é™…åˆ†ç±»ä»»åŠ¡ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ˜¯
    CNN çš„å…¸å‹æ¶æ„ã€‚è€Œè¿™äº›å·ç§¯æ»¤æ³¢å™¨ï¼Œç°åœ¨ã€‚
- en: they work by applying a filter kernel to our imageã€‚ So we put the filter at
    the first position position in our imageã€‚ So this is the filter hereã€‚ And this
    is the input imageã€‚ So we put it at the first positionï¼Œ the positionã€‚ and then
    we compute the output value by multiplying and summing up all the valuesã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬é€šè¿‡å°†æ»¤æ³¢å™¨å†…æ ¸åº”ç”¨äºæˆ‘ä»¬çš„å›¾åƒæ¥å·¥ä½œã€‚å› æ­¤æˆ‘ä»¬æŠŠæ»¤æ³¢å™¨æ”¾åœ¨å›¾åƒçš„ç¬¬ä¸€ä¸ªä½ç½®ã€‚è¿™å°±æ˜¯æ»¤æ³¢å™¨ã€‚è¿™æ˜¯è¾“å…¥å›¾åƒã€‚å› æ­¤æˆ‘ä»¬å°†å…¶æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®ï¼Œç„¶åé€šè¿‡ç›¸ä¹˜å¹¶æ±‚å’Œæ‰€æœ‰å€¼æ¥è®¡ç®—è¾“å‡ºå€¼ã€‚
- en: and then we write the value into the output imageã€‚ So here at the red positionã€‚
    And then we slide our filter to the next positionã€‚ So the green positionã€‚ then
    if you can see this hereã€‚And then we do the same thing and the same filter operationã€‚
    And then we slide our filter over the whole image until we are doneã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å€¼å†™å…¥è¾“å‡ºå›¾åƒã€‚å› æ­¤è¿™é‡Œæ˜¯çº¢è‰²ä½ç½®ã€‚æ¥ç€æˆ‘ä»¬å°†æ»¤æ³¢å™¨æ»‘åŠ¨åˆ°ä¸‹ä¸€ä¸ªä½ç½®ï¼Œå³ç»¿è‰²ä½ç½®ã€‚å¦‚æœä½ èƒ½çœ‹åˆ°è¿™é‡Œã€‚ç„¶åæˆ‘ä»¬åšåŒæ ·çš„äº‹æƒ…å’Œç›¸åŒçš„æ»¤æ³¢æ“ä½œã€‚æ¥ç€æˆ‘ä»¬åœ¨æ•´ä¸ªå›¾åƒä¸Šæ»‘åŠ¨æˆ‘ä»¬çš„æ»¤æ³¢å™¨ï¼Œç›´åˆ°å®Œæˆã€‚
- en: So this is how convolutional filters workã€‚And now with this transformã€‚ our resulting
    image may have a smaller size because our filter does not fit in the corners hereã€‚
    except if we use a technique that is called paddingã€‚ but we will not cover this
    here in this lectureã€‚So getting the correct size is an important step that we
    will see later in practiceã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å·ç§¯æ»¤æ³¢å™¨çš„å·¥ä½œåŸç†ã€‚ç°åœ¨é€šè¿‡è¿™ä¸ªå˜æ¢ï¼Œæˆ‘ä»¬çš„ç»“æœå›¾åƒå¯èƒ½ä¼šæœ‰è¾ƒå°çš„å°ºå¯¸ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ»¤æ³¢å™¨åœ¨è¿™é‡Œçš„è§’è½ä¸é€‚ç”¨ï¼Œé™¤éæˆ‘ä»¬ä½¿ç”¨ä¸€ç§å«åšå¡«å……çš„æŠ€æœ¯ã€‚ä½†åœ¨æœ¬è®²åº§ä¸­æˆ‘ä»¬ä¸ä¼šæ¶µç›–è¿™ä¸ªå†…å®¹ã€‚å› æ­¤ï¼Œè·å–æ­£ç¡®çš„å°ºå¯¸æ˜¯ä¸€ä¸ªé‡è¦çš„æ­¥éª¤ï¼Œæˆ‘ä»¬ç¨åä¼šåœ¨å®è·µä¸­çœ‹åˆ°ã€‚
- en: And now let's also talk about pooling layers brieflyã€‚ So pooling layers or more
    specificã€‚ In this caseï¼Œ the max pooling max pooling is used to downs sampleampble
    an image by applying a maximum filter to subreã€‚ So here we have a filter of size
    2 by2ã€‚ And then we look at the two by two subre in our original imageã€‚ And we
    write the maximum value of this region into the output imageã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¹Ÿç®€è¦è°ˆè°ˆæ± åŒ–å±‚ã€‚å› æ­¤æ± åŒ–å±‚ï¼Œæˆ–æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€å¤§æ± åŒ–ç”¨äºé€šè¿‡å¯¹å­åŒºåŸŸåº”ç”¨æœ€å¤§æ»¤æ³¢å™¨æ¥ä¸‹é‡‡æ ·å›¾åƒã€‚è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ª 2x2 çš„æ»¤æ³¢å™¨ã€‚ç„¶åæˆ‘ä»¬æŸ¥çœ‹åŸå§‹å›¾åƒä¸­çš„
    2x2 å­åŒºåŸŸï¼Œå¹¶å°†è¯¥åŒºåŸŸçš„æœ€å¤§å€¼å†™å…¥è¾“å‡ºå›¾åƒã€‚
- en: So max pooling is used to reduce the computational cost by reducing the size
    of the imageã€‚ So this reduces the number of parameters that our model has to learnã€‚And
    it also helps to avoid overfitting by providing an abstracted form of the inputã€‚So
    yeahã€‚ these are all the concepts we must know and againã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§æ± åŒ–ç”¨äºé€šè¿‡å‡å°å›¾åƒçš„å¤§å°æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™å‡å°‘äº†æ¨¡å‹éœ€è¦å­¦ä¹ çš„å‚æ•°æ•°é‡ï¼Œå¹¶é€šè¿‡æä¾›è¾“å…¥çš„æŠ½è±¡å½¢å¼å¸®åŠ©é¿å…è¿‡æ‹Ÿåˆã€‚æ‰€ä»¥è¿™äº›éƒ½æ˜¯æˆ‘ä»¬å¿…é¡»äº†è§£çš„æ¦‚å¿µã€‚
- en: please check out the provided links if you want to learn more and now enough
    of the theory and let's get to the codeã€‚ So here I already wrote the most things
    that we need so we import the things that we need then we make sure that we also
    have the GPU support then we define the hyperpar and if you don't know how I structure
    my Pythtorch files and please also watch the previous tutorials because there
    I already explained all of these stepsã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹æä¾›çš„é“¾æ¥ã€‚ç°åœ¨ç†è®ºéƒ¨åˆ†å°±åˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å·²ç»å†™å¥½äº†å¤§éƒ¨åˆ†éœ€è¦çš„å†…å®¹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„ä¸œè¥¿ï¼Œç¡®ä¿æˆ‘ä»¬æœ‰ GPU
    æ”¯æŒï¼Œç„¶åå®šä¹‰è¶…å‚æ•°ã€‚å¦‚æœä½ ä¸çŸ¥é“æˆ‘æ˜¯å¦‚ä½•æ„å»ºæˆ‘çš„ Pytorch æ–‡ä»¶çš„ï¼Œè¯·æŸ¥çœ‹ä¹‹å‰çš„æ•™ç¨‹ï¼Œå› ä¸ºé‚£é‡Œæˆ‘å·²ç»è§£é‡Šäº†æ‰€æœ‰è¿™äº›æ­¥éª¤ã€‚
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_1.png)'
- en: So then first of allï¼Œ we load the dataset set and here as I saidã€‚ the Spher
    10 dataset is already available in Pytorarch so we can use it for from the Pytorch
    dot datas moduleã€‚ Then we define our Pytorch data sets and the Pytorch data loaders
    so then we can do automatically batch optimization and batch trainingã€‚Then I defined
    the classes and hard coded them hereã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®é›†ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼ŒSpher 10 æ•°æ®é›†å·²ç»åœ¨ Pytorach ä¸­å¯ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è‡ª Pytorch çš„æ•°æ®æ¨¡å—ã€‚ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„
    Pytorch æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è‡ªåŠ¨è¿›è¡Œæ‰¹é‡ä¼˜åŒ–å’Œæ‰¹é‡è®­ç»ƒã€‚æ¥ç€æˆ‘åœ¨è¿™é‡Œå®šä¹‰äº†ç±»ï¼Œå¹¶è¿›è¡Œäº†ç¡¬ç¼–ç ã€‚
- en: And then here now we have to implement the convolutional netã€‚And then as alwaysã€‚
    we typically we create our model and we create our loss and the optimizerã€‚ So
    in this caseã€‚ as this is a multiclass classification problemï¼Œ we use the cross
    entropy lossã€‚ and then as optimizer we use the stochastic gradient descentã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬éœ€è¦å®ç°å·ç§¯ç½‘ç»œã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬é€šå¸¸åˆ›å»ºæˆ‘ä»¬çš„æ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè¿™æ˜¯ä¸€ä¸ªå¤šç±»åˆ«åˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œç„¶åä½œä¸ºä¼˜åŒ–å™¨ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ã€‚
- en: which has to optimize the model parameters and it gets the defined learning
    rate and then we have the typical training loopã€‚ which does the batch optimization
    so we loop over the number of epochs and then we loop over the training loader
    so we get all the different batchesã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œå®ƒè·å–å®šä¹‰çš„å­¦ä¹ ç‡ï¼Œç„¶åæˆ‘ä»¬æœ‰å…¸å‹çš„è®­ç»ƒå¾ªç¯ï¼Œè¿›è¡Œæ‰¹é‡ä¼˜åŒ–ï¼Œæˆ‘ä»¬å¾ªç¯éå† epoch æ•°ï¼Œå¹¶å¾ªç¯éå†è®­ç»ƒåŠ è½½å™¨ï¼Œä»¥è·å–æ‰€æœ‰ä¸åŒçš„æ‰¹æ¬¡ã€‚
- en: And then here againï¼Œ we have to push the images and the labels to the device
    to get the GPU supportã€‚ Then we to do our typical forward pass and create the
    lossã€‚ And then we do the backward pass where we must not forget to call to empty
    the gradients first you with the zero Grã€‚Then we call the backward function and
    optimize a stepã€‚And then print some informationã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å†æ¬¡éœ€è¦å°†å›¾åƒå’Œæ ‡ç­¾æ¨é€åˆ°è®¾å¤‡ä¸Šä»¥è·å¾— GPU æ”¯æŒã€‚æ¥ç€æˆ‘ä»¬è¿›è¡Œå…¸å‹çš„å‰å‘ä¼ æ’­å¹¶åˆ›å»ºæŸå¤±ã€‚ç„¶åè¿›è¡Œåå‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¿…é¡»è®°å¾—å…ˆè°ƒç”¨ zero_grad
    æ¥æ¸…ç©ºæ¢¯åº¦ã€‚ç„¶åæˆ‘ä»¬è°ƒç”¨ backward å‡½æ•°å¹¶ä¼˜åŒ–ä¸€æ­¥ï¼Œæœ€åæ‰“å°ä¸€äº›ä¿¡æ¯ã€‚
- en: Then when we are doneï¼Œ we evaluate the modelã€‚ And as alwaysã€‚ we wrap this in
    a width torch do no gr argument or statementã€‚ So because we don't need theã€‚ the
    backward propagation hereã€‚And the gradient calculationsã€‚ And then we calculate
    the accuracyã€‚ So we calculate the accuracy of the total networkï¼Œ and we calculate
    the a for each single classã€‚ Soã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬è¯„ä¼°æ¨¡å‹ã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å°†å…¶åŒ…è£¹åœ¨ä¸€ä¸ª with torch.no_grad çš„è¯­å¥ä¸­ï¼Œå› ä¸ºè¿™é‡Œä¸éœ€è¦åå‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—å‡†ç¡®ç‡ï¼Œæ‰€ä»¥æˆ‘ä»¬è®¡ç®—æ•´ä¸ªç½‘ç»œçš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸ºæ¯ä¸ªå•ç‹¬çš„ç±»åˆ«è®¡ç®—å‡†ç¡®ç‡ã€‚
- en: yeahï¼Œ so this is the scriptã€‚ You can also find this on my Githubã€‚ So please
    check that out thereã€‚And now the only thing that is missing now is to implement
    the convolutional netã€‚So for thisã€‚ we define a class confnetï¼Œ which must inherit
    an end dot moduleã€‚ And as alwaysã€‚ we have to define or implement the init function
    and the forward function for the forward passã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™å°±æ˜¯è„šæœ¬ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„ GitHub ä¸Šæ‰¾åˆ°è¿™ä¸ªå†…å®¹ï¼Œæ‰€ä»¥è¯·å»é‚£é‡ŒæŸ¥çœ‹ã€‚ç°åœ¨å”¯ä¸€ç¼ºå°‘çš„å°±æ˜¯å®ç°å·ç§¯ç½‘ç»œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç±» confnetï¼Œå®ƒå¿…é¡»ç»§æ‰¿è‡ª
    nn.Moduleã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æˆ–å®ç° init å‡½æ•°å’Œå‰å‘ä¼ æ’­çš„ forward å‡½æ•°ã€‚
- en: So now let's write some code hereã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_3.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åœ¨è¿™é‡Œå†™ä¸€äº›ä»£ç ã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_3.png)
- en: So for thisï¼Œ we have a look at the architecture againã€‚ So hereï¼Œ firstã€‚ we have
    a convolutional layer and then followed by a reo activation functionã€‚ Then we
    apply a max poolingã€‚ Then we have a second convolutional layer with a relu function
    and a max poolingã€‚ And then we have three different fully connected layersã€‚ And
    then at the very endã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å†æ¬¡æŸ¥çœ‹æ¶æ„ã€‚åœ¨è¿™é‡Œï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå·ç§¯å±‚ï¼Œç„¶åæ˜¯ä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°ã€‚æ¥ç€åº”ç”¨æœ€å¤§æ± åŒ–ã€‚ç„¶åæˆ‘ä»¬æœ‰ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼Œå¸¦æœ‰ReLUå‡½æ•°å’Œæœ€å¤§æ± åŒ–ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æœ‰ä¸‰ä¸ªä¸åŒçš„å…¨è¿æ¥å±‚ã€‚åœ¨æœ€åã€‚
- en: we have the softms and the cross entropyã€‚ So the softm is already included in
    the cross entropy loss hereã€‚ So we don't need to care about thisã€‚ So yeahï¼Œ so
    let's set up or create all these layersã€‚ So let's say self dot con1 equals And
    here we get the first convolutional layer by we get this by saying n and dot conf
    to theã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰softmså’Œäº¤å‰ç†µã€‚å› æ­¤softmå·²ç»åŒ…å«åœ¨è¿™é‡Œçš„äº¤å‰ç†µæŸå¤±ä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å…³æ³¨è¿™ä¸ªã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®æˆ–åˆ›å»ºæ‰€æœ‰è¿™äº›å±‚ã€‚å‡è®¾self.dot
    con1ç­‰äºï¼Œé€šè¿‡è¯´nå’Œdot confè·å–ç¬¬ä¸€ä¸ªå·ç§¯å±‚ã€‚
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_5.png)'
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_6.png)'
- en: And now we have to specify the sizesã€‚ So the input channel size now is 3 because
    our images have three color channelsã€‚So that's why the input channel size is 3ã€‚
    And then let's say the output channel size is 6 and the kernel size is 5ã€‚ so  five
    times 5ã€‚And now let's define a pooling layerã€‚ self pool equals N N dot max pool
    2 D with a kernel size of 2 and a stride of2ã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦æŒ‡å®šå¤§å°ã€‚è¾“å…¥é€šé“å¤§å°ç°åœ¨æ˜¯3ï¼Œå› ä¸ºæˆ‘ä»¬çš„å›¾åƒæœ‰ä¸‰ä¸ªé¢œè‰²é€šé“ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¾“å…¥é€šé“å¤§å°æ˜¯3ã€‚ç„¶åå‡è®¾è¾“å‡ºé€šé“å¤§å°æ˜¯6ï¼Œå·ç§¯æ ¸å¤§å°æ˜¯5ï¼Œå³5ä¹˜5ã€‚ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæ± åŒ–å±‚ï¼Œself
    poolç­‰äºN N.dot max pool 2 Dï¼Œå·ç§¯æ ¸å¤§å°ä¸º2ï¼Œæ­¥å¹…ä¸º2ã€‚
- en: So this is exactly in the as in the image that we have seenã€‚ So our kernel size
    is size 2 by 2ã€‚ And after each operationï¼Œ we shifted to pixels to the rightã€‚ So
    that's why the stride is 2ã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_8.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„å›¾åƒä¸­é‚£æ ·ã€‚æˆ‘ä»¬çš„å·ç§¯æ ¸å¤§å°æ˜¯2ä¹˜2ã€‚æ¯æ¬¡æ“ä½œåï¼Œæˆ‘ä»¬å°†åƒç´ å‘å³ç§»åŠ¨ä¸¤ä¸ªå•ä½ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ­¥å¹…æ˜¯2ã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_8.png)
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_9.png)'
- en: And then let's define the second convolutional layerã€‚ So self con2 equalsã€‚ And
    now the input channel size must be equal to the last output channel sizeã€‚ So here
    we say 6ã€‚ And as outputï¼Œ let's say 16 and kernel size is still 5ã€‚ And so now we
    have our convolutional layersã€‚ And now let's set up the fully connected layer
    by saying self dot F1 equals and and dot linearã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®©æˆ‘ä»¬å®šä¹‰ç¬¬äºŒä¸ªå·ç§¯å±‚ã€‚self con2ç­‰äºã€‚ç°åœ¨è¾“å…¥é€šé“å¤§å°å¿…é¡»ç­‰äºæœ€åçš„è¾“å‡ºé€šé“å¤§å°ã€‚å› æ­¤è¿™é‡Œæˆ‘ä»¬è¯´6ã€‚ä½œä¸ºè¾“å‡ºï¼Œå‡è®¾æ˜¯16ï¼Œå·ç§¯æ ¸å¤§å°ä»ç„¶æ˜¯5ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†å·ç§¯å±‚ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬é€šè¿‡è¯´self.dot
    F1ç­‰äºå’ŒN N.dot linearæ¥è®¾ç½®å…¨è¿æ¥å±‚ã€‚
- en: And now here as an input sizeã€‚ So firstï¼Œ I will write this for youã€‚ So this
    is 16 times 5 times 5ã€‚ and as output sizeï¼Œ I will simply say I will say 100ã€‚ So
    you can try out a different one hereã€‚ And I will explain in a secondï¼Œ why this
    is 16 times 5 times 5ã€‚Then let's set up the next fully collected layerã€‚ So this
    has 120 input featuresã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½œä¸ºè¾“å…¥å¤§å°ã€‚æˆ‘å…ˆä¸ºä½ å†™è¿™ä¸ªã€‚è¿™æ˜¯16ä¹˜5ä¹˜5ã€‚ä½œä¸ºè¾“å‡ºå¤§å°ï¼Œæˆ‘ä¼šç®€å•åœ°è¯´100ã€‚æ‰€ä»¥ä½ å¯ä»¥å°è¯•ä¸åŒçš„å€¼ã€‚æˆ‘ä¼šè§£é‡Šä¸ºä»€ä¹ˆè¿™æ˜¯16ä¹˜5ä¹˜5ã€‚ç„¶åè®©æˆ‘ä»¬è®¾ç½®ä¸‹ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œè¿™é‡Œæœ‰120ä¸ªè¾“å…¥ç‰¹å¾ã€‚
- en: and let's say84 output featuresã€‚ And then let's use a next of final fully connected
    layerã€‚ So we have F 1ï¼Œ F2 and F3ã€‚ And this is an input size of 84ã€‚ and the output
    size must be 10 because we have 10 different classesã€‚So you can change the 120
    here and also the 84ï¼Œ but this must be fixed and also the 10 must be fixedã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æœ‰84ä¸ªè¾“å‡ºç‰¹å¾ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨æœ€ç»ˆçš„å…¨è¿æ¥å±‚ã€‚æˆ‘ä»¬æœ‰F1ã€F2å’ŒF3ã€‚è¿™æ˜¯è¾“å…¥å¤§å°ä¸º84ï¼Œè¾“å‡ºå¤§å°å¿…é¡»ä¸º10ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰10ä¸ªä¸åŒçš„ç±»åˆ«ã€‚å› æ­¤ä½ å¯ä»¥æ”¹å˜è¿™é‡Œçš„120å’Œ84ï¼Œä½†è¿™å¿…é¡»å›ºå®šï¼Œ10ä¹Ÿå¿…é¡»å›ºå®šã€‚
- en: So now let's have a look at why this isï¼Œ this must be this numberã€‚ So here I
    have a little script that does exactly the same thingã€‚ So ohã€‚ let me change the
    number of epochã€‚ Oh yeahï¼Œ this is4ã€‚ So hereã€‚I have the same thing in the beginningã€‚
    I load the data setï¼Œ and let's alsoã€‚å—¯ã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä¸ºä»€ä¹ˆè¿™ä¸ªæ•°å­—å¿…é¡»æ˜¯è¿™ä¸ªæ•°å­—ã€‚è¿™é‡Œæˆ‘æœ‰ä¸€ä¸ªå°è„šæœ¬ï¼Œå®ƒåšçš„æ­£æ˜¯ç›¸åŒçš„äº‹æƒ…ã€‚å“¦ï¼Œè®©æˆ‘æ”¹å˜epochçš„æ•°é‡ã€‚å“¦ï¼Œæ˜¯çš„ï¼Œè¿™æ˜¯4ã€‚æ‰€ä»¥åœ¨å¼€å§‹æ—¶ï¼Œæˆ‘æœ‰ç›¸åŒçš„å†…å®¹ã€‚æˆ‘åŠ è½½æ•°æ®é›†ï¼Œå—¯ã€‚
- en: Print or plot some imagesã€‚And then here I have the same layersã€‚ So here I have
    the first convolutional layer and the pooling layer and the second convolutional
    layerã€‚ And first of allï¼Œ let's run this and plot the imagesï¼Œ soã€‚Let's say Pythonï¼Œ
    CNN test dot piã€‚Andã€‚I've already downloaded itã€‚ So it printsã€‚ also yeahï¼Œ it's
    very blurtã€‚ but I think you can see thisã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰“å°æˆ–ç»˜åˆ¶ä¸€äº›å›¾åƒã€‚è¿™é‡Œæˆ‘æœ‰ç›¸åŒçš„å±‚ã€‚æˆ‘æœ‰ç¬¬ä¸€ä¸ªå·ç§¯å±‚å’Œæ± åŒ–å±‚ï¼Œè¿˜æœ‰ç¬¬äºŒä¸ªå·ç§¯å±‚ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªå¹¶ç»˜åˆ¶å›¾åƒã€‚æˆ‘ä»¬å¯ä»¥è¯´Pythonï¼ŒCNN test
    dot piã€‚ç„¶åï¼Œæˆ‘å·²ç»ä¸‹è½½å¥½äº†ã€‚æ‰€ä»¥å®ƒä¼šæ‰“å°ã€‚æ˜¯çš„ï¼Œè™½ç„¶éå¸¸æ¨¡ç³Šï¼Œä½†æˆ‘æƒ³ä½ å¯ä»¥çœ‹åˆ°è¿™ä¸ªã€‚
- en: This is a horse and maybe a bird and another horseã€‚ And yeahï¼Œ Iï¼Œ I don't recognize
    this actuallyã€‚ So let's runï¼Œ run this againã€‚See some better picturesï¼Œ maybeã€‚Soï¼Œ
    yeahï¼Œ it's still very blurredã€‚ Then I think this is a deerï¼Œ a carï¼Œ a f and a shipã€‚Soã€‚Yeahï¼Œ
    soã€‚Let's seeã€‚How the sizes lookã€‚ So firstï¼Œ we just print images that shapeã€‚ So
    this is 4 by 3 by 32 by 32ã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€åŒ¹é©¬ï¼Œä¹Ÿè®¸æ˜¯ä¸€åªé¸Ÿï¼Œè¿˜æœ‰å¦ä¸€åŒ¹é©¬ã€‚æ˜¯çš„ï¼Œæˆ‘å…¶å®ä¸è®¤è¯†è¿™ä¸ªã€‚è®©æˆ‘ä»¬å†è¿è¡Œä¸€ä¸‹ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½çœ‹åˆ°æ›´å¥½çš„å›¾ç‰‡ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œä»ç„¶éå¸¸æ¨¡ç³Šã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€åªé¹¿ã€ä¸€è¾†è½¦ã€ä¸€è‰˜èˆ¹ã€‚æ‰€ä»¥ï¼Œå¥½çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å°ºå¯¸çš„æ ·å­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åªæ‰“å°å½¢çŠ¶çš„å›¾åƒã€‚æ‰€ä»¥è¿™æ˜¯4ä¹˜3ä¹˜32ä¹˜32ã€‚
- en: And this is because our batch size is 4ã€‚ And then we have three different colour
    channelsã€‚ And then ourã€‚Images have subized 32 by 32ã€‚So now let's apply the first
    convolutional layerã€‚ So we say x equals cont1ã€‚ and this will get the imagesã€‚ And
    now let's print the next size after this operationã€‚So let's don'tã€‚ sorryï¼Œ I don't
    want toã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„æ‰¹å¤„ç†å¤§å°æ˜¯4ã€‚ç„¶åæˆ‘ä»¬æœ‰ä¸‰ç§ä¸åŒçš„é¢œè‰²é€šé“ã€‚æˆ‘ä»¬çš„å›¾åƒå°ºå¯¸ä¸º32ä¹˜32ã€‚ç°åœ¨è®©æˆ‘ä»¬åº”ç”¨ç¬¬ä¸€ä¸ªå·ç§¯å±‚ã€‚æˆ‘ä»¬è¯´xç­‰äºcont1ï¼Œè¿™å°†è·å–å›¾åƒã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°è¿™ä¸ªæ“ä½œåçš„ä¸‹ä¸€ä¸ªå¤§å°ã€‚æŠ±æ­‰ï¼Œæˆ‘ä¸æƒ³è¦ã€‚
- en: Pot this anymoreã€‚So now we have the next sizeï¼Œ so this is 4 by 6 by 28 by 28
    and so there's6ã€‚ now we have six output channels as we specified here and then
    the image size is 28 by 28 because as I saidã€‚ the resulting image may be smaller
    because our filter doesn't fit in the corners here and the formula to calculate
    the output size is thisã€‚ So this is the input width minus the filter size plus2
    times paddingã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å†ä¹Ÿä¸éœ€è¦äº†ã€‚ç°åœ¨æˆ‘ä»¬æœ‰ä¸‹ä¸€ä¸ªå¤§å°ï¼Œæ‰€ä»¥è¿™æ˜¯4ä¹˜6ä¹˜28ä¹˜28ï¼Œå› æ­¤æ˜¯6ã€‚ç°åœ¨æˆ‘ä»¬æœ‰å…­ä¸ªè¾“å‡ºé€šé“ï¼Œå¦‚æ­¤å¤„æŒ‡å®šï¼Œå›¾åƒå¤§å°æ˜¯28ä¹˜28ï¼Œå› ä¸ºæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œç”Ÿæˆçš„å›¾åƒå¯èƒ½ä¼šæ›´å°ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ»¤æ³¢å™¨æ— æ³•é€‚åº”è§’è½ï¼Œè®¡ç®—è¾“å‡ºå¤§å°çš„å…¬å¼æ˜¯è¾“å…¥å®½åº¦å‡å»æ»¤æ³¢å™¨å¤§å°åŠ ä¸Š2å€çš„å¡«å……ã€‚
- en: So in this case we don't have padding and then divided by the strip and then
    plus1 So in this example we have an input size 5 by5 a filter size 3 by3 padding
    is 0 and stride is 1ã€‚ So then we have the output size is 5 minus-3 plus 1ã€‚ So
    this is2 then divided byã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰å¡«å……ï¼Œç„¶åé™¤ä»¥æ­¥å¹…å†åŠ 1ã€‚æ‰€ä»¥åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„è¾“å…¥å¤§å°æ˜¯5ä¹˜5ï¼Œæ»¤æ³¢å™¨å¤§å°æ˜¯3ä¹˜3ï¼Œå¡«å……æ˜¯0ï¼Œæ­¥å¹…æ˜¯1ã€‚å› æ­¤è¾“å‡ºå¤§å°æ˜¯5å‡å»3åŠ 1ã€‚æ‰€ä»¥æ˜¯2ï¼Œç„¶åé™¤ä»¥ã€‚
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_11.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_11.png)'
- en: 1 is still 2ï¼Œ and then plus oneã€‚So that's why here our output image is 3 by
    3ã€‚ And now we have to apply the same formula in our caseã€‚ So we have 32 minus
    the filter sizeã€‚ So-5ã€‚ So this is 27ã€‚Plus 0ï¼Œ still 27 divided by  oneï¼Œ still 27ï¼Œ
    and then plus 1ã€‚ So that's why it's 28ã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1ä»ç„¶æ˜¯2ï¼Œç„¶ååŠ ä¸Š1ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„è¾“å‡ºå›¾åƒæ˜¯3ä¹˜3ã€‚ç°åœ¨æˆ‘ä»¬å¿…é¡»åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹åº”ç”¨ç›¸åŒçš„å…¬å¼ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰32å‡å»æ»¤æ³¢å™¨å¤§å°ï¼Œå‡å»5ã€‚å› æ­¤æ˜¯27ã€‚åŠ 0ï¼Œä»ç„¶æ˜¯27ï¼Œé™¤ä»¥1ï¼Œä»ç„¶æ˜¯27ï¼Œç„¶ååŠ 1ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ˜¯28ã€‚
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_13.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_13.png)'
- en: So here we have 28 by 28ã€‚ Then let's apply the next layerã€‚ So the next operation
    is the pooling layerã€‚ So let's save this and run thisã€‚So now our size is 4 by
    6 by 14 by 14ã€‚ So this is because as in the exampleã€‚ our pooling layer with a
    kernel size 2 by2 and a strip of2 will reduce the images by a factor of2ã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰28ä¹˜28ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬åº”ç”¨ä¸‹ä¸€å±‚ã€‚ä¸‹ä¸€ä¸ªæ“ä½œæ˜¯æ± åŒ–å±‚ã€‚æˆ‘ä»¬ä¿å­˜è¿™ä¸ªå¹¶è¿è¡Œå®ƒã€‚ç°åœ¨æˆ‘ä»¬çš„å°ºå¯¸æ˜¯4ä¹˜6ä¹˜14ä¹˜14ã€‚è¿™æ˜¯å› ä¸ºï¼Œå¦‚ç¤ºä¾‹æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ± åŒ–å±‚çš„æ ¸å¤§å°ä¸º2ä¹˜2ï¼Œæ­¥å¹…ä¸º2ï¼Œå°†å›¾åƒå‡å°‘äº†2å€ã€‚
- en: So yeahï¼Œ and now let's apply the second convolutional layerã€‚ So let's print
    the size after this operationã€‚ So clear this firstã€‚And run thisã€‚ And then againã€‚
    we would have to apply the formulaï¼Œ as I just showed you to reduce the sizeã€‚ So
    here Pythtorarch can figure this out for usã€‚ So the size is 4 by 16 and this is
    because the next channel output sizeã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ˜¯çš„ï¼Œç°åœ¨è®©æˆ‘ä»¬åº”ç”¨ç¬¬äºŒä¸ªå·ç§¯å±‚ã€‚è®©æˆ‘ä»¬æ‰“å°è¿™ä¸ªæ“ä½œåçš„å¤§å°ã€‚é¦–å…ˆæ¸…é™¤è¿™ä¸ªã€‚ç„¶åè¿è¡Œè¿™ä¸ªã€‚æ¥ç€ï¼Œæˆ‘ä»¬éœ€è¦åº”ç”¨æˆ‘åˆšæ‰å±•ç¤ºçš„å…¬å¼æ¥å‡å°å°ºå¯¸ã€‚å› æ­¤è¿™é‡ŒPyTorchå¯ä»¥ä¸ºæˆ‘ä»¬è®¡ç®—å‡ºæ¥ã€‚å°ºå¯¸æ˜¯4ä¹˜16ï¼Œè¿™å°±æ˜¯ä¸‹ä¸€ä¸ªé€šé“çš„è¾“å‡ºå¤§å°ã€‚
- en: and that we specified is 16ï¼Œ and then the resulting image is 10 by 10ã€‚ and then
    we apply another pooling operation that will again reduce the size by a factor
    of 2ã€‚ So this is why now we see that the final size after both convolutional layers
    and the pooling layers is 4 by 16 by 5 by 5ã€‚ Soã€‚And nowï¼Œ if we have a look againã€‚
    So now afterã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒ‡å®šçš„å¤§å°æ˜¯16ï¼Œç»“æœå›¾åƒæ˜¯10ä¹˜10ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨å¦ä¸€ä¸ªæ± åŒ–æ“ä½œï¼Œè¿™å°†å†æ¬¡å°†å¤§å°å‡å°‘ä¸€åŠã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ç°åœ¨çœ‹åˆ°çš„ï¼Œåœ¨ä¸¤ä¸ªå·ç§¯å±‚å’Œæ± åŒ–å±‚ä¹‹åçš„æœ€ç»ˆå¤§å°æ˜¯4ä¹˜16ä¹˜5ä¹˜5ã€‚æ‰€ä»¥ã€‚å¦‚æœæˆ‘ä»¬å†çœ‹çœ‹ã€‚é‚£ä¹ˆåœ¨æ­¤ä¹‹åã€‚
- en: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_15.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdf3154fd8a3cad04eb6fed3d8610645_15.png)'
- en: These convolutional layersã€‚ Nowï¼Œ when we put them into our classification layersã€‚
    we want to flatten the sizeï¼Œ So we want to flatten our 3D tenor to a 1 D tenorã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_17.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å·ç§¯å±‚ã€‚ç°åœ¨ï¼Œå½“æˆ‘ä»¬å°†å®ƒä»¬æ”¾å…¥åˆ†ç±»å±‚æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦å±•å¹³å¤§å°ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›å°†æˆ‘ä»¬çš„3Då¼ é‡å±•å¹³ä¸º1Då¼ é‡ã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_17.png)
- en: And now this is why nowï¼Œ if we have a look at the size nowã€‚ the input size of
    the first linear layer is exactly this that we have hereã€‚ So 16 times 5 times
    5ã€‚ So this is very important to get the correct size hereã€‚ But now we know why
    this is this must be 16 times 5 times 5ã€‚ And now we have the correct sizesã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹ç°åœ¨çš„å¤§å°ï¼Œç¬¬ä¸€å±‚çº¿æ€§å±‚çš„è¾“å…¥å¤§å°æ­£æ˜¯æˆ‘ä»¬è¿™é‡Œçš„16ä¹˜5ä¹˜5ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œä»¥ç¡®ä¿è¿™é‡Œçš„å¤§å°æ­£ç¡®ã€‚ä½†ç°åœ¨æˆ‘ä»¬çŸ¥é“ä¸ºä»€ä¹ˆè¿™å¿…é¡»æ˜¯16ä¹˜5ä¹˜5ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ­£ç¡®çš„å¤§å°ã€‚
- en: So now we have all the layers definedã€‚ and now we have to apply them in the
    forward passã€‚ So we say x equalsã€‚ And now let's apply the first convolutional
    layerï¼Œ which gets xã€‚ And then after thatï¼Œ we apply an activationation functionã€‚
    So we can do this by calling Fã€‚ So I importedã€‚Torch and and functional S Fã€‚ And
    then I can call F dot relu and then put in this as the argumentã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†æ‰€æœ‰å±‚ï¼Œç°åœ¨å¿…é¡»åœ¨å‰å‘ä¼ æ’­ä¸­åº”ç”¨å®ƒä»¬ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´xç­‰äºã€‚ç°åœ¨è®©æˆ‘ä»¬åº”ç”¨ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œè·å–xã€‚ç„¶ååœ¨æ­¤ä¹‹åï¼Œæˆ‘ä»¬åº”ç”¨ä¸€ä¸ªæ¿€æ´»å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨Fæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥æˆ‘å¯¼å…¥äº†Torchå’ŒåŠŸèƒ½SFã€‚ç„¶åæˆ‘å¯ä»¥è°ƒç”¨F.dot.reluï¼Œå¹¶å°†å…¶ä½œä¸ºå‚æ•°ä¼ å…¥ã€‚
- en: And then after the activationation functionã€‚ So by the wayã€‚ the activationation
    function does not change the sizeã€‚So now we apply the first pooling layer So self
    dot pool and wrap this hereã€‚ And so this is the first convolutional and pooling
    layerã€‚ And then we do the same thing with the second convolutional layerã€‚ And
    now we have to pass it to the first fully connected layerã€‚ And for thisï¼Œ we have
    to flatten itã€‚ So we can do this by saying x equals x dot viewã€‚ And the first
    sizeï¼Œ we can simply say1ã€‚ So Pytorch then can automatically define the correct
    size for usã€‚ So this is the number of batchesã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æ¿€æ´»å‡½æ•°ä¹‹åã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæ¿€æ´»å‡½æ•°ä¸ä¼šæ”¹å˜å¤§å°ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬åº”ç”¨ç¬¬ä¸€ä¸ªæ± åŒ–å±‚ï¼Œæ‰€ä»¥self.dot poolå¹¶åŒ…è£¹åœ¨è¿™é‡Œã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯å’Œæ± åŒ–å±‚ã€‚ç„¶åæˆ‘ä»¬å¯¹ç¬¬äºŒä¸ªå·ç§¯å±‚åšåŒæ ·çš„äº‹æƒ…ã€‚ç°åœ¨æˆ‘ä»¬å¿…é¡»å°†å…¶ä¼ é€’ç»™ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶å±•å¹³ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯´xç­‰äºx.dot.viewæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ç¬¬ä¸€ä¸ªå¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´1ã€‚è¿™æ ·Pytorchå°±å¯ä»¥è‡ªåŠ¨ä¸ºæˆ‘ä»¬å®šä¹‰æ­£ç¡®çš„å¤§å°ã€‚è¿™æ˜¯æ‰¹æ¬¡çš„æ•°é‡ã€‚
- en: the number of samples we have in our batch hereã€‚ So four in this caseã€‚ And then
    here we must say 16 times 5 times 5ã€‚And now we have our tens of Latinsã€‚ And now
    let's call the first fully connected layer by saying x equals self dot F1ã€‚ And
    this will get Xã€‚ And then we apply an activationation function Againï¼Œ we simply
    use the reloã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿™é‡Œæ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°é‡ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯å››ä¸ªã€‚ç„¶åè¿™é‡Œæˆ‘ä»¬å¿…é¡»è¯´16ä¹˜5ä¹˜5ã€‚ç°åœ¨æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„å¼ é‡ã€‚ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡è¯´xç­‰äºself.dot F1æ¥è°ƒç”¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚è¿™å°†è·å–Xã€‚ç„¶åæˆ‘ä»¬å†æ¬¡åº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨reluã€‚
- en: I also have a whole tutorial about activationation functionsã€‚ So please check
    that out if you haven't alreadyã€‚So now after thisï¼Œ we apply the second oneã€‚ So
    x equals thisï¼Œ the second fully connected layer with a re activationation functionã€‚
    And at the very endï¼Œ we simply have x equals self dot the last fully connected
    layer F C 3 with Xã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜æœ‰ä¸€ä¸ªå…³äºæ¿€æ´»å‡½æ•°çš„å®Œæ•´æ•™ç¨‹ã€‚å¦‚æœä½ è¿˜æ²¡çœ‹è¿‡ï¼Œè¯·æŸ¥çœ‹ã€‚ç°åœ¨åœ¨æ­¤ä¹‹åï¼Œæˆ‘ä»¬åº”ç”¨ç¬¬äºŒä¸ªã€‚æ‰€ä»¥xç­‰äºè¿™ä¸ªï¼Œç¬¬äºŒä¸ªå…¨è¿æ¥å±‚å¸¦æœ‰æ¿€æ´»å‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬ç®€å•åœ°æœ‰xç­‰äºself.dotæœ€åä¸€ä¸ªå…¨è¿æ¥å±‚F
    C 3å’ŒXã€‚
- en: And no activation function at the endã€‚ and also no softmax activation function
    hereã€‚ because this is already included in our loss that we set up hereã€‚ So then
    we can simply return Xã€‚ And this is the whole convolutional net modelã€‚ Now you
    should know how we can set up thisã€‚And yeahã€‚ so then we create our model hereï¼Œ
    and then we continue with theã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œä¹Ÿæ²¡æœ‰è¿™é‡Œçš„softmaxæ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºè¿™å·²ç»åŒ…å«åœ¨æˆ‘ä»¬è®¾ç½®çš„æŸå¤±ä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿”å›Xã€‚è¿™å°±æ˜¯æ•´ä¸ªå·ç§¯ç½‘ç»œæ¨¡å‹ã€‚ç°åœ¨ä½ åº”è¯¥çŸ¥é“æˆ‘ä»¬å¦‚ä½•è®¾ç½®è¿™ä¸ªã€‚å—¯ï¼Œç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºæ¨¡å‹ï¼Œç„¶åç»§ç»­è¿›è¡Œã€‚
- en: A training loop that I already showed youã€‚ So now let's save thisã€‚ and let's
    run thisã€‚ So clear this and say Pythonï¼Œ C andN dot P and hope that this will start
    the trainingã€‚Soã€‚Oh yeahã€‚ One thing I forgotï¼Œ of courseï¼Œ is to call the super in
    itã€‚ So never forget to call superã€‚ And this has to get the con and selfï¼Œ and thenã€‚Dot
    underscore in itã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å·²ç»å‘ä½ å±•ç¤ºè¿‡çš„è®­ç»ƒå¾ªç¯ã€‚æ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬ä¿å­˜è¿™ä¸ªï¼Œè¿è¡Œå®ƒã€‚æ¸…é™¤è¿™ä¸ªï¼Œè¾“å…¥Pythonï¼ŒCå’ŒNç‚¹Pï¼Œå¸Œæœ›è¿™èƒ½å¯åŠ¨è®­ç»ƒã€‚æ‰€ä»¥ã€‚å“¦ï¼Œå¯¹äº†ï¼Œæˆ‘å¿˜äº†ä¸€ä»¶äº‹ï¼Œå½“ç„¶ï¼Œè¦è°ƒç”¨superçš„åˆå§‹åŒ–ã€‚æ‰€ä»¥æ°¸è¿œä¸è¦å¿˜è®°è°ƒç”¨superã€‚è¿™éœ€è¦è·å–conå’Œselfï¼Œç„¶åã€‚ç‚¹ä¸‹åˆ’çº¿åˆå§‹åŒ–ã€‚
- en: So let's clear this again and try this one more timeã€‚Andã€‚Nowï¼Œ this should start
    the trainingã€‚ So I don't have GP U support on my MacBookã€‚ so this can take a few
    minutesã€‚ So I think I will skip this and continue when the training is doneã€‚ So
    we'll see you in a secondã€‚Alrightï¼Œ so now we are backã€‚ Our training has finishedã€‚
    and if we have a lookã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬å†æ¸…é™¤ä¸€æ¬¡ï¼Œå†è¯•ä¸€æ¬¡ã€‚ç°åœ¨ï¼Œè¿™åº”è¯¥ä¼šå¯åŠ¨è®­ç»ƒã€‚æˆ‘åœ¨æˆ‘çš„MacBookä¸Šæ²¡æœ‰GPUæ”¯æŒï¼Œæ‰€ä»¥è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚æ‰€ä»¥æˆ‘æƒ³æˆ‘ä¼šè·³è¿‡è¿™ä¸ªï¼Œç­‰è®­ç»ƒå®Œæˆåç»§ç»­ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸€ä¼šå„¿è§ã€‚å¥½ï¼Œç°åœ¨æˆ‘ä»¬å›æ¥äº†ã€‚æˆ‘ä»¬çš„è®­ç»ƒå·²ç»å®Œæˆï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹ã€‚
- en: we can see that the loss slowly decreasedï¼Œ and then we have the final evaluationã€‚
    So the accuracy of the total network is 46ã€‚6% and the accuracy of each class is
    listed hereã€‚ So it's not very goodã€‚ and this is because we only specified for
    epochs hereã€‚ So you might want to try out and more epochs but yeahã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æŸå¤±å€¼ç¼“æ…¢ä¸‹é™ï¼Œç„¶åæˆ‘ä»¬æœ‰äº†æœ€ç»ˆè¯„ä¼°ã€‚å› æ­¤ï¼Œæ€»ç½‘ç»œçš„å‡†ç¡®ç‡ä¸º46.6%ï¼Œæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡åœ¨è¿™é‡Œåˆ—å‡ºã€‚æ‰€ä»¥æ•ˆæœä¸æ˜¯å¾ˆå¥½ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¿™é‡ŒåªæŒ‡å®šäº†è®­ç»ƒçš„è½®æ•°ã€‚å› æ­¤ï¼Œä½ å¯èƒ½æƒ³å°è¯•æ›´å¤šçš„è½®æ•°ï¼Œä¸è¿‡æ˜¯è¿™æ ·çš„ã€‚
- en: now you should know how a convolutional neural net can be implementedã€‚ And I
    hope you enjoyed this tutorialã€‚ If you enjoyed thisã€‚ please leave a like and subscribe
    to the channel and see a next time byã€‚ğŸ˜Šã€‚![](img/bdf3154fd8a3cad04eb6fed3d8610645_19.png)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ åº”è¯¥çŸ¥é“å¦‚ä½•å®ç°å·ç§¯ç¥ç»ç½‘ç»œã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªï¼Œè¯·ç‚¹èµå¹¶è®¢é˜…é¢‘é“ï¼Œæˆ‘ä»¬ä¸‹æ¬¡å†è§ã€‚ğŸ˜Šï¼[](img/bdf3154fd8a3cad04eb6fed3d8610645_19.png)
