# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é¢å‘åˆå­¦è€…çš„ TensorFlow æ•™ç¨‹ï¼Œç†è®ºçŸ¥è¯†ã€ä»£ç æ€è·¯å’Œåº”ç”¨æ¡ˆä¾‹ï¼ŒçœŸæ­£ä»é›¶å¼€å§‹è®²æ˜ç™½ï¼ï¼œå¿«é€Ÿå…¥é—¨ç³»åˆ—ï¼ - P4ï¼šL4- çº¿æ€§å›å½’  å®Œæ•´é¡¹ç›®æ¼”ç»ƒ - ShowMeAI - BV1TT4y1m7Xg

![](img/938d327f6471f60f92b4ed5bae0092f3_0.png)

ğŸ¼ï¼ŒHeyï¼Œ guysï¼Œ welcome to the next Tensorlow tutorialã€‚ In the last videoã€‚ we learned how to create a neural network and then train and evaluate the model and make predictionsã€‚ğŸ˜Šï¼ŒNow in this tutorialï¼Œ we implement our first real world projectã€‚ so we deal with a regression problemï¼Œ and we learn how we load the dataã€‚

 analyze the data and apply some pre processingã€‚And yeahï¼Œ as I said last timeã€‚ we already used a deep neural networkã€‚ and here we first take a step back and apply only a linear regression modelã€‚ So only one layerã€‚ But at the endï¼Œ we extend this to againï¼Œ a deep neural networkã€‚ And with thisã€‚ you should get a better understanding of the Keerra stance layerã€‚

 and also of activationctuaation functionsã€‚ So here I am in a twopyter notebookã€‚ And you don't need to worry about thisï¼Œ this just makes life a little bit easier for me to explain the code to you and show you the different stepsã€‚ But you can code all of this in a normal python fileã€‚ So let's start and firstã€‚ we import the things we needã€‚ So againï¼Œ hereï¼Œ I silence some warningsã€‚ Then we import mappllip nuyã€‚

 and then this is newã€‚ So now we use pandasã€‚ and you can install this with Pip install pandasã€‚ğŸ˜Šã€‚So this makes it very easy to work with data sets and analyze them and modify themã€‚And then here I set some print options to make the outputs a little bit nicerã€‚And then we import the things from Tenofflowã€‚ So againï¼Œ we import Tensofflow STfã€‚ Then we import Kasã€‚

 So the last time I told you about the Kaas API and we already use the ks layersã€‚ And now this is newã€‚ So we also import from Tenorflowlow ks layers do experimentalã€‚ we import preprocessing to apply a preprocessing layer to our dataã€‚ So these are the imports we needã€‚ And now for the data setã€‚

 So we are going to use this autompg data setã€‚So this is a data set from the year 1983ã€‚ and here we have different features and with this we predict thempg for a carã€‚ So how many miles a car can travel using one gallon of fuel and so here this is a very popular website where you find a lot of machine learning data setsã€‚ And now if you click on the data folder then here you get the link to the actual dataã€‚

 So now here in the twopiter notebookï¼Œ this is the exact same URLã€‚ and I already wrote the different column names up here and then with pandas we can say pandas do read CSv So the ending here is not do csv but it's still in the CSv formã€‚So we can use this and you can also use this method if you have a Csv file on your diskã€‚ So this works as wellã€‚ So now let's load our data and then we can call data dot tail So this prints the last five columns So here we see thempg value that we want to predict and the different featuresã€‚

 So for exampleï¼Œ we have the number of cylindersï¼Œ the displacementï¼Œ the horsepowerã€‚ the weight acceleration model year and or writtenã€‚ So yeahã€‚ this is our data sets and now we clean our data setã€‚ So in here we have some missing numbers and we can very simply exclude them by saying data dot drop and a and then we also change this last column here becauseã€‚

This is actually a categorical valueï¼Œ and all of the other ones are numerical valuesã€‚ So this might confuse our modelã€‚ So we want to change this in a one hot encoded dataã€‚ so we can call data set dot pop or written to remove thisã€‚And here in the originã€‚ we have the different countriesï¼Œ USAï¼Œ Europe and Japanã€‚ So we can add them like thisã€‚So nowï¼Œ againã€‚

 if we have a look at our tailï¼Œ then we see we removed the origin value and instead included one hot labelsã€‚ So here for label 1ã€‚ our it's the USA for label 2ï¼Œ it's Europeã€‚ Then againï¼Œ label 1ã€‚ So againã€‚ USA has the one and so onã€‚ So now we have thisã€‚And now we want to split our data into training and test setsã€‚ and we can do this by calling dot sample and then use a fraction hereã€‚ So use 80% for trainingã€‚

And we againï¼Œ drop the samplesã€‚ So all of the samplesã€‚ all of that we specify here are not included in the test setã€‚And then yeahã€‚ let's print print the shape and let's also describe our dataset set so we can very easily do this with pandas as well by calling the dot describe functionã€‚ And then here we see we have our whole data set has 392 samples and then 10 different columns for nowã€‚

 And then our test dataï¼Œ our training data is 80% so 314 and the rest is for testingã€‚And now if we call the describe functionï¼Œ then it analyze some statistics like the number of samplesã€‚ the meanï¼Œ the standard deviationï¼Œ the min and maxã€‚So this might be useful to analyze itã€‚ And first of allï¼Œ now what we want to do is we want to split the features from the labelsã€‚

 So we want to this is our label that we predict MPã€‚ So first we make a copyã€‚ So we say these are our training features and our test featuresã€‚ And then we simply pop the MP G column from the training and testing dataã€‚ And when we pop thisã€‚ then this also returns the dataã€‚ So this is now our training and testing labelsã€‚

 So now we have thisã€‚Then here's a simple function to plot the data with mappl lipã€‚ and now let's simply plot one featureã€‚ So let's plot the horsepower featureã€‚ And we plot this against thempg valueã€‚ So we see the more horsepower our car has also the less is the value of MPgã€‚ and this makes sense because the more power our gas has the more fuel it needsã€‚

 So the less is the MPgã€‚ And so then we also let's plot the weight featureã€‚ and this should have a similar distributionã€‚ So againï¼Œ the higher the weight of a carã€‚ the more fuel uses itï¼Œ and the less is the number of the MPg valueã€‚Soï¼Œ yeahã€‚So this is how our data looks likeã€‚ And now if we go back to the describe function and have a look at the different mean valuesã€‚

 then we see these all have different ranges and this is againã€‚ a very important issue that we have to considerã€‚ So this these different ranges if if we leave it like thisã€‚ then it might confuse our modelã€‚ So it's recommend to normalize the data first and to normalizeã€‚ So let's print againï¼Œ let's describe our training data set and only the but let's print only the mean and the standard deviationã€‚

 So if we run this again then here we see the different meansã€‚And now to normalize itã€‚ we used this normalization layer from the pre processingces moduleã€‚ So this one here that we importedã€‚So let's code thisã€‚ So here we create a normalization layerã€‚ So we say normalizer equals pre processing dotã€‚Normalizationã€‚

 so this is a kas layer for the sequential APIã€‚ And to call thisã€‚ we have to adapt it to our to the dataã€‚ So here we sayï¼Œ normalizer dot adaptã€‚ And then we want to adapt it to the training featuresã€‚ And right nowã€‚ this is a panda data set so we can convert this to a nuy arrayã€‚ and then call the train feeã€‚Cersã€‚

So what this is doing for nowï¼Œ this simply store calculates the mean and the variance of this training features and then stores them in the layersã€‚ So nowï¼Œ for exampleï¼Œ we can say print and then normalr dot mean dot nuyã€‚ and then if we run thisã€‚Then we see here thisã€‚ These are our mean valuesï¼Œ and these are the exact same numbers as we see here if we go this first column downã€‚So this simply stores itã€‚ And now whenever we apply this layer to our dataã€‚

 then it normalizes the features such that it computes this this formulaã€‚ So it subtracts the mean and then divides by the standard deviationã€‚ and this means that our output has a zero mean and unit varianceã€‚ So here let's get some exampleã€‚ So the first data from the training featuresã€‚Thenï¼Œ let's print this oneã€‚

So this is the first data unizedã€‚ And then we also want to print the normalized oneã€‚ Soã€‚ let's print norã€‚Maizedã€‚ And then here we have to call this layer normalr with our dataã€‚ So firstã€‚ and then convert it to numpyã€‚ So this is how we convert from a tensor flow tensor to a nuy arrayã€‚ So let's run thisã€‚ And then we see we have theã€‚The first layer with different scales and rangesã€‚

 and now our normalized dataã€‚ So they are all somewhere around0 with a unit standard deviationã€‚ So this is how we apply a normalizer preprocessing layerã€‚ And now let's tackle our regression problemã€‚ So in regressionã€‚ So we somewhere we have this distribution that we know from the training dataã€‚

 and now when we get a new dataï¼Œ we want to so for exampleã€‚ here we get a new weight sample and then we want to predict how muchmpT we have for this carã€‚ and for thisï¼Œ we fit a function So a linear function with this formula so we can approximate it withmp times x plus Bã€‚ So this is aã€‚Basically a line equation and we use this with the layers with the dense layerã€‚

 Sos better understand thisã€‚ Nowï¼Œ firstï¼Œ let's not use all of the different training features that we haveã€‚ So all of these so let's just use one so that we can stay in the 2D case so in this caseã€‚ for exampleï¼Œ its let's use the horsepower So here let's define our feature and this should be the horse power label so I will implement it like this so then you can just change the feature here and then you can try out different features so we can also for exampleã€‚

 use weight here so all of these names you can use here as a feature and now let's get the single feature or the single data so let'sã€‚Call this single feature equals Ny arrayã€‚ And then from our training featuresã€‚ and then we can access it with the featureã€‚ This will only return the horsepower featureã€‚So let's print thisã€‚ Let's print single feeã€‚Feature dot shapeã€‚ And let's print trainã€‚

 features dot shapeã€‚So nowï¼Œ if we run thisã€‚å—¯ã€‚Here I have a typo arrayã€‚Let's run it againã€‚ Then we see our single feature only has one featureã€‚ And in the whole training dataã€‚ we have 9 different featuresã€‚ So then up here weã€‚Created this preproces or normalization layer and then adapted it to all of the featuresã€‚ So basically we have to do the same thingï¼Œ but now adapt it only to the horse power featureã€‚

 So let's call this single feature normalizer then againï¼Œ our single feature normalizerã€‚ we call the adapt data and adapt it to the single feature single underscore featureã€‚ So now we have this and can run this code and here I have one parentheses too muchã€‚ So againã€‚ So now we have this single featureã€‚ and now we create our sequential model as the last timeã€‚

 So this is very easy with the Kaas APIã€‚å—¯ã€‚Soï¼Œ we sayï¼Œ our singleã€‚Featureã€‚Model equalsã€‚ And then we sayï¼Œ Kas dotã€‚Models dot sequentialã€‚ And then here we use a list with all the different layersã€‚ So we use this as a first layerã€‚ So the single feature normalizerã€‚ And then we only use one layerã€‚ So one dense layerã€‚ and the output or also called units number of units is only oneã€‚

 So this is all that we need to build a linear regression modelã€‚ So this is a linear regression or linearã€‚Model that applies exactly this formulaã€‚ So it has some weights and multiplies it with our inputã€‚ And it also has a biasã€‚So this is all that we are doing hereã€‚ So let's run thisã€‚ and let's print the model summaryã€‚

 And then we see we have our normalization layer and our dense layer and only five parameterssã€‚5 parametersã€‚ So it's very simpleã€‚ And now as a next stepã€‚ the same as last time we define a loss and a optrã€‚ So for the lossï¼Œ we use ks dot lossesã€‚ And in the case of a linear regressionï¼Œ we can use the mean absolutearrowã€‚

 So this is one possibilityï¼Œ we could also use the mean squaredarrowã€‚ So you can try out bothã€‚ğŸ˜Šã€‚And so this is simply this one is doing the so the prediction y prediction minus the actual yã€‚ and then the absolute value and then sum it up over all samples and calculate the mean value and the mean square error is the sameã€‚ except that it's using the square here and not the absolute valueã€‚

 So yeah try that out for yourselfã€‚And now the optimizer equalsã€‚ So the same as last time Kaas dot optimizers dotã€‚ Let's use the Adamom optimizerã€‚ and we have to give it a learning rateã€‚ Let's try out point1ã€‚And then after we have thisã€‚ then we compile our modelã€‚ So single feature model dot compileï¼Œ compileã€‚And here we give itã€‚

 The optr equals the optimizer andã€‚Optim and the loss equals the lossã€‚ And yeahã€‚ last time we also gave it the metrics that we want to track So the accuracyã€‚ but the accuracy doesn't make sense here for the regressionã€‚ So we leave it awayã€‚ And then we simply see the loss laterã€‚ So let's run thisã€‚And now to train the modelã€‚

 we simply have to call model dot fit like the last time with our training features and here we only want the single feature that we define so the horse power in this caseã€‚ but we still include all the training labels then we define the epochs then here I set verbose to one to see some logging and we can also define a validation split so this automatically takes 20% of the training data and then uses it for the validation data to tweak the hyperparameterã€‚

So let's train our modelã€‚And nowï¼Œ training is doneã€‚ And now we see that the loss decreasedã€‚ and we also see the validation loss decreasingã€‚And at the endï¼Œ we have a training loss of 3ã€‚8 and a validation loss of slightly more 4ã€‚1ã€‚ So it's not very badã€‚ and by the wayã€‚ if we call model fitï¼Œ then this returns the historyï¼Œ where it stores both of these lossesã€‚

 So we can assign it to a variableï¼Œ which I call history hereã€‚ and then I simply plot the historyã€‚ So then we can access these two losses by calling history dot history and then access the loss and the validation lossã€‚So let's plot this and then we see that our losses decreasedï¼Œ so it'sï¼Œ it looks very good hereã€‚And then to evaluate our modelï¼Œ we simplyï¼Œ like last timeã€‚

 call model dot evaluate with our test features only with horsepower and the test labelsã€‚So let's run thisã€‚ And this was very fastã€‚ So here we seeã€‚ we have a final loss for the test data of 3ã€‚6ã€‚ So not very badã€‚ And now let's predict some samplesã€‚ So in this caseï¼Œ I simply createã€‚Test dataã€‚ So all the values betweenã€‚

The min value and the max value and increase the range a little bitã€‚ You could also just use a hard coded number hereï¼Œ like from0 to 250ã€‚ But I want to be it like a little bit more suited for the used featureã€‚ So we use thisã€‚ And then to predictï¼Œ we call model dot predictã€‚ And then our new test data that we want to predictã€‚

 And then we plot it with our function from the beginningã€‚And this is the same plot as in the beginningã€‚ So we plot the horsepower against the MP7 valueã€‚ And then these are our new x values that we predictã€‚ and we see that our predictionã€‚ So here we plot a lineã€‚ and this is a linear line since we use a linear regression modelã€‚

 And we see that it's not too badã€‚ So we see the same trendã€‚ So the more horsepower our car has the lower the MP isã€‚ but for exampleï¼Œ here in this areaã€‚ it's staying the same except that our horsepower is further increasingã€‚ So yeahã€‚ in this area and also maybe in this area areaï¼Œ it's not perfectã€‚ So but in the restã€‚

 it looks it looks okayã€‚Soï¼Œ yeahï¼Œ so this is how we apply a linear modelã€‚ and againã€‚ we only need the one dense layer with one output unitã€‚ and we also apply this normal normalization layerã€‚ So this is all we need to use linear regressionã€‚ And now let's extend this to a deep neural networkã€‚ So as I saidï¼Œ with only one dense layerã€‚

 we can only use a or only get a linear linear function here like thisï¼Œ which is not perfectã€‚ So to further improve thisï¼Œ we can introduce more layers here and convert this to a fully feet forward neural netã€‚So we simplyï¼Œ we still use the normalization layer and the dense layer at the endã€‚ But in the middleã€‚ we use some more dense layersã€‚ So let's sayï¼Œ layers dot denseã€‚

 And then here you can try out different values for the hidden sizeã€‚ And as I said in the last timeã€‚ we also apply activation functions for these layers in the middleã€‚ğŸ˜Šï¼ŒSo let's yeahã€‚ let's use the relu againï¼Œ like last timeã€‚ And then let's use the same one againã€‚And yeahã€‚ and then at the endï¼Œ we use our dense layer with one outputã€‚ And now this is all we needã€‚

 And now we converted our linear regression model to a deep neural network modelã€‚ So let's use thisã€‚ And then againï¼Œ we compile it with the loss and againã€‚ a atom optimizer and call the summary to see the different layers againã€‚ And then we see there is a lot more parameters that we have to train nowã€‚ So now againã€‚

 let's call model fit with our horsepower feature and start the trainingã€‚ğŸ˜Šï¼ŒAnd training is doneã€‚ And we see the final model or final loss is slightly better nowã€‚ so slightly lower than beforeã€‚ So let's evaluate our modelã€‚ And now it's below threeã€‚ So it's definitely betterã€‚ And now againã€‚ let's make some predictions and plot thisã€‚ And now we see a new plotã€‚

 And now we see that our function is no longer only linearã€‚ But here we also have some nonlinear areasã€‚ğŸ˜Šï¼ŒAnd this is the effects of the activation functions that we usedã€‚ And that's why this is such an important thing in neural networks to apply activation functionsã€‚ So here againï¼Œ we have this only our linear regression model with one dense layerã€‚

 So here we can only predict a linear functionã€‚ And now with a deep neural networkã€‚ we can get a non nonlinearar function as predictionã€‚ So yeahã€‚ I think that's all and now so we only used one feature for now so that we can stay in this 2D caseã€‚ but of courseï¼Œ we can include all of the features and we do it the sameã€‚

 So we don't have to change anything actuallyã€‚ So we use this normalizer that we adapt it to all theã€‚Maining featuresã€‚ And here we have only our single dense layerã€‚ Then we compile thisã€‚And then we fit it here to the all of the training features and train it andã€‚And nowã€‚ training is doneã€‚ And we see that the loss is also below 3ã€‚ So training on all features makes itã€‚

Furtherï¼Œ betterã€‚ And now let's evaluate it as this last thingã€‚ And then we see we get the final lossã€‚So yeahï¼Œ so you see you don't the code is actually the sameã€‚ We just use all of the training features here and also for the normalizerã€‚ we adapted it to all of the training featuresã€‚ So yeahã€‚

 I think now you learned a lot in this tutorial how we can download and load CSv data and how we analyze it and preprocess it with the pandas framework and then how we use the normalization layer to normalize our data and then how we set up a a single linear regression model with this dense layerã€‚

And then againï¼Œ training and evaluation is the same like last timeã€‚ And yeahï¼Œ and we alsoã€‚ you learned about the effect of the activation functions in a deep neural networkã€‚And yeahã€‚ that's it for todayã€‚ I hope you enjoyed this tutorialã€‚ And if you liked itã€‚ then please hit the like button and consider subscribing to the channelã€‚

 And I hope to see you in the next videoï¼Œ byeã€‚ğŸ˜Šã€‚![](img/938d327f6471f60f92b4ed5bae0092f3_2.png)