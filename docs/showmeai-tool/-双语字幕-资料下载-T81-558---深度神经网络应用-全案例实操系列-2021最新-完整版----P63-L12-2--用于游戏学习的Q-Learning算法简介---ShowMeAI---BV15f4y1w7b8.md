# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P63ÔºöL12.2- Áî®‰∫éÊ∏∏ÊàèÂ≠¶‰π†ÁöÑQ-LearningÁÆóÊ≥ïÁÆÄ‰ªã - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff Hayton„ÄÇ welcome to applications of deep neural networks with Washington University„ÄÇ In this videoÔºå we're going to build on using G„ÄÇ We saw open AI gym in the previous video„ÄÇ Now we're going to apply Q learning to that„ÄÇ And we're going to see how we can build up using machine learning a basic lookup table that shows us what action to take for any state that the environment might work on„ÄÇ Then we're going to continue this and show how deep neural network can become a very complex representation of a table so that we don't have to store literally every combination in there„ÄÇ

 And that begins the deep Q neural network to see all my videos about Cale neural networks and other AI topics„ÄÇ Click the subscribe button and the bell next to it and select all to be notified of every new video„ÄÇ So let's have a look at Q learning„ÄÇ I'm going to run this in Google coabab„ÄÇ This really does not require GPU„ÄÇ The code here actually is not even designed to take advantage of a GP„ÄÇ

üòä„ÄÇ![](img/30d4f6c3c88418548a35fcff3628ad27_1.png)

I'm going to give you just a Python implementation of doing a tablebased Q learning lookup„ÄÇ which is really kind of how Q learning works In the next part„ÄÇ we will extend this and we'll actually use Kra's TF agents to actually implement this in deep learning and then go of completely beyond this and use something that has a much more complex environment like an Atari game So here I have the setup for Google Coab we definitely want to use Tensorflow 2„ÄÇ

0 and I have some additional code here that if you're running coabab will execute because coab at least as it was as of the recording of this video does not include TF agents and some other things that I want so that I'm able to display videos literally write in the browser here TF agents provides a very good way to take a snapshot of just a single image from an environment running as it's rendering and show it to you there's other ways„ÄÇ

Get around itÔºå but TF agents makes that real easy other than that we're not using TF agents for this video„ÄÇ We'll use TF agents in the next one„ÄÇ I will go ahead and run this part„ÄÇ it will say note using Google coab once this actually runs that I run this part to install all of the libraries that I would like to have this can take a bit so we'll fast forward through it Now Q learning there's a couple of terms that I'll refer to as we're going through this So it's good to go ahead and go through those now agent think of this in terms of a video game although in the very last part of this chapter we will do something that's not a video game but the agent that's your player that your computer player that is going through the environment playing the video game now the environment is the universe that the agent exists in the environment has state and the state of the environment changes as the agent interacts with it and the agent interacts with the environment by„ÄÇ



![](img/30d4f6c3c88418548a35fcff3628ad27_3.png)

![](img/30d4f6c3c88418548a35fcff3628ad27_4.png)

First of allÔºå just seeing the state of the environment in this part this video„ÄÇ the environment state is just going to be a vector of some numbers that tell us the position of a cart or how fast it's going things like that but when we get to Atari the environment is actually a grid that's the video screen as the game is being played a step is one cycle through the environment so you take an action that's like move up„ÄÇ

 move downÔºå move leftÔºå move right and then see what that does to the environment and one complete cycle of performing some action and seeing the results that's a step all of the steps until you accomplish your goal or get killed or the simulation ends that's called an episode so that's like one play of the video game an Epoch is a training construct it can be some number of episodes depending on how you configure figure things and then the terminal state is the step that you get to that causes the game to end„ÄÇ

you winÔºå you run out of livesÔºå something like that„ÄÇ So Q learning basically builds up table so that every state in this environment„ÄÇ it gives us a indication of which action is going to be the most rewarding because as you're going through this you get a reward at the end of each step and that tells you how how effective that step was some gains are mean they don't give you any reward until the very„ÄÇ

 very end and only if you want it„ÄÇ those can be a little harder to optimize for„ÄÇ but Q learning still does that„ÄÇ that would be almost like going through your foury degree and at the very end„ÄÇ either you get a diploma or you don't And if you don't who knows why you did something wrong„ÄÇ But that's the torture that we put computers through„ÄÇ

 So we're going to work with something called the mountain car„ÄÇ This is a classic reinforcement learning hello world kind of thing it is essentially a mountain car Now if you run this code here„ÄÇ I'm not going to„ÄÇOn it because it's already rendered„ÄÇ But this will show you one frame of whatever environment you're specifying„ÄÇ

 I'm specifying the mountain car„ÄÇ If I put in space invadersÔºå it would draw the space invaders„ÄÇ all the little aliens coming down to be blasted„ÄÇ So this is how mountain car works„ÄÇ The idea of this game is your mountain car essentiallyÔºå you can provide forward thrust„ÄÇ So you really only have three actions you can take One action is no action„ÄÇ

 And then the other two actions are either thrust this way up the hill or thrust this way up this side of the hill„ÄÇ The trick is your little car is just not strong enough to make it up the hill„ÄÇ try to just blast up the hill„ÄÇ I'll show you that in a moment„ÄÇ You're not that successful„ÄÇ So what you've got to do is blast up the hill„ÄÇ get as far as you can go and then you'll roll back but you'll roll back and build up some potential energy up the hill on this side„ÄÇ

 and then you chargeÔºå charge straight through and go„ÄÇüòäÔºåUp again„ÄÇ maybe usually you'll make it in two shots„ÄÇ but the idea is to get all the way up here using potential energy from that other hill and conserving your momentum there„ÄÇ so you can apply a left forceÔºå you can apply no force or you can apply a right force So those are the numbers that specify those discrete actions the way the Q table works you can only apply a discrete action So these are the three things there's no like apply maximum force left apply half force like your car like your accelerator when you press that down flooring the car gives you a completely different result than tapping it this is possible with Q learning but you need to use some special tricks and other things„ÄÇ

 we'll get into that when we get into deep Q learning using the deep neural network but typically it's assumed that these are discrete actions but continuous actions are fine you just have to use different algorithms for those and in our very last part of this module will„ÄÇ

Look at an application of deep neural networksÔºå deep queue networks that is not a game„ÄÇ and it does take continuous actions„ÄÇ NowÔºå the mountain car environment„ÄÇ essentially all you have is the position of this car„ÄÇ I believe zero is right here and then negative and positive and then the velocity„ÄÇ

 How fast is it going and that's positive or negative too and those are continuous values„ÄÇ So the velocity that you're going this way or the velocity that you're going that way„ÄÇ negative versus positive„ÄÇ Now this is using some code that allows you to actually visualize it and see it go up I don't have the code the video from last time„ÄÇ So let me go ahead and run this„ÄÇ So I'm going to run this and what this does is it gives you a couple of functions to show the video and actually see the video in a web browser which is handy because a lot of these you'll require a GPU and you may want to actually run those in Google coab to get advantage of the free GPU but you can also display it in a browser if you're running it locally or a little screen„ÄÇ

üòäÔºå'll typically pop up and show it to you„ÄÇ So now let me go ahead and run it„ÄÇ So what this does„ÄÇ This is a mountain car that very stubbornly just tries to apply maximum acceleration to the right„ÄÇ So it just it just floors it well there is no maximum it just pushes with full power to the right„ÄÇ tries to scale the mountain and ultimately is not strong enough„ÄÇ Now this output you see here„ÄÇ

 you can see the state„ÄÇ So the state is showing essentially where the car so it's velocity how fast it's going„ÄÇ it kind of reaches full velocity and varies a little bit„ÄÇ this is a little bit random„ÄÇ And then this is the position„ÄÇ you can see it's basically going further and further and further up the hill and at some point yeah right around there„ÄÇ it starts to slide back down„ÄÇ pictureture is worth a thousand words„ÄÇ So if you look at this„ÄÇ

 this is what it does it tries it tries who's the Greek guy that pushes the boulder up the hill sisyphus if I remember my history classes is right„ÄÇThat's basically an impossible task„ÄÇ NowÔºå what I do is I use old school programming„ÄÇ I program a car to solve this„ÄÇ And for a simple case like this„ÄÇ yeah„ÄÇ I can I can write a completely deterministic programÔºå just give it some rules„ÄÇ

 and the rule essentially that I'm going to look at„ÄÇ I'm going to look at the state„ÄÇ and I'm going to only pay attention to the state„ÄÇ I am not going to pay attention to the velocity„ÄÇ I don't care„ÄÇ I want to know just the position of the state„ÄÇ and this position if the position is greater than0 zero„ÄÇ So this way„ÄÇ

 then I'm going to push this way„ÄÇ if the car is going this way„ÄÇ I am going to push this way and try to getÔºå try to get it as far up this side as possible to build up as much potential energy as it possibly can„ÄÇ and the program to do this is really pretty simple„ÄÇ If the position So the first element of the state is greater than zeroÔºå I apply to„ÄÇ

 which is push it to the right„ÄÇ OtherwiseÔºå I apply zeroÔºå which is push it to the left„ÄÇ One is no power at all„ÄÇ I don't believe any bra is apply but„ÄÇWe really don't need to break in order to achieve our goal or ever let up on the engine„ÄÇ So here we run it„ÄÇ We see the same steps„ÄÇ Notice there is no reward„ÄÇ

 This is like trying to get through your four years of college and the only feedback you get as you graduate or you don't„ÄÇ but we get all the away to the end„ÄÇ eventuallyÔºå ohÔºå and I don't display the last steps„ÄÇ So you don't even see your reward that you that you ultimately get when this thing wins„ÄÇ But let's take a look at how my programmed car works„ÄÇ

 You can see it's a bit more successful and it wins„ÄÇ All rightÔºå that is a hardcod rule„ÄÇ Imagine if the Tesla engineer is set there and try to think of every possible case that would ever come up when you're driving an actual car„ÄÇ it would be quite miserable„ÄÇ and it just wouldn't work„ÄÇ You would never think of everything„ÄÇ I mean„ÄÇ this is almost a1 B world like flatwor because I can't go really up or down„ÄÇ

 I'm only going left and right„ÄÇ So reinforcement learning„ÄÇ how this works„ÄÇ is we have our agent and we have the environment„ÄÇAgent takes some action on the environment„ÄÇ And there's two outputs from that„ÄÇ There is the state for the next frame„ÄÇ and then there's the reward that you receive from that action for the mountain car„ÄÇ

 you only receive a reward when you actually succeed and make it all the way up the hill„ÄÇ Now„ÄÇ I'll mention this briefly because this is sometimes done in reinforcement learning is if the environment doesn't necessarily give you a reward„ÄÇ you can sometimes get greater effects by engineering your own reward„ÄÇ And that might be doing something like sayingÔºå okay„ÄÇ

 I'm going to give it a reward for maybe getting up the hill further than it's ever gotten before or something„ÄÇ So use your your mindÔºå your heuristics as a human to actually come up with some intermediate reward just maybe to give it better„ÄÇ better results„ÄÇ Now this line here means that training occurs and it adjusts it„ÄÇ And then the next state goes into the agent„ÄÇ and it continues„ÄÇ

 It continues and continues and continues„ÄÇNowÔºå this equation is how Q learning is typically represented„ÄÇ It looks kind of big and crazy if you've not seen a lot of these kind of equations before„ÄÇ but it's not that bad„ÄÇ And I want to take you through the parts of it because this is actually quite important„ÄÇ So the way Q learning works is it's keeping that table„ÄÇ

 So a table for every possible state that the environment can be in„ÄÇ Now in this case„ÄÇ this is a grid of all the possible positions and all the possible velocities because those are your two states that you have and state T„ÄÇ that just means all of those states„ÄÇ So S is essentially a array holding those two values in the case of this problem„ÄÇ and for each action that you might take„ÄÇ So each of those different states in that grid„ÄÇ

 there's three actions you can take„ÄÇ So this is really a cube every value on the grid is essentially the three actions and anticipated reward that you would get for for being there„ÄÇ So what this is doing what this is saying is the„ÄÇValue is going to be changed„ÄÇ

 It is essentially going to be added to„ÄÇ So this this old valueÔºå this is almost like in programming„ÄÇ a equals a plus plus something else plus sub delta a equals a plus1„ÄÇ if this whole thing was one„ÄÇ So this whole part over here is the delta We're calculating how much we want to change that Q value of I„ÄÇ This is very typical of machine learning„ÄÇ This is how I don't even know how many algorithms I studied have exactly this format„ÄÇ

Like the weight in a neural networkÔºå the weight equals the weight plus essentially the gradient times a learning rate„ÄÇ The learning rate is there to scale it back„ÄÇ Learn to walk before you learn to run„ÄÇ If you didn't have this learning rate hereÔºå then this whole big value would just be flipping those Q values all over the place„ÄÇ It would be too aggressive of a change„ÄÇ And it would be like the learning algorithm got super excited about every little new thing that it learned that it completely overrode most of what it had learned previously„ÄÇ

 So your alpha is your learning rate„ÄÇ I use a learning rate in this example of01„ÄÇ So one10th of whatever we're going to calculate for this delta is what we're going to apply„ÄÇ And they call it a temporal difference„ÄÇ I took this right from Wikipedia„ÄÇ The equation„ÄÇ Now you get back a reward„ÄÇ So let's look at how that reward is everything hinges on the reward actually happens„ÄÇ

 I like to look at these equations„ÄÇüòäÔºåin blocks and then rip the block apart„ÄÇ So inside of this delta that is being multiplied by the learning rateÔºå there's really two parts„ÄÇ Notice this old value„ÄÇ That's the same old value that you add here„ÄÇ You could do some algebraic simplification and do some canceling and not have that„ÄÇ

 but that's just the way this is written„ÄÇ basically we want this whole thing to be a delta to be added to this„ÄÇ so we have to subtract off the old value so that basically this is our new value„ÄÇ This is the old value so the difference between these two„ÄÇ We desire to get to hear this new value„ÄÇ So we have to subtract the old value from it so that we need to see the magnitude change„ÄÇ

 and then we scale it by the learning rate and at it„ÄÇ So let's look at the new value because that's the interesting part„ÄÇ that is just setting up the delta„ÄÇ We need to know how much we want to change it by„ÄÇ So this is the theoretical new value that we would like to eventually take it to„ÄÇ So the reward„ÄÇ

 Re is important„ÄÇThe reward is what we were given for applying this state and action that we currently did„ÄÇ So we're updating the Q value for whatever action we took on the current state„ÄÇ So the reward is I mean fundamentally the reward would be the estimate of the future value that we're going to get So those values in the table„ÄÇ they tell you the reward for each of those actions at a particular state„ÄÇ

 usually you just take the highest of those actions„ÄÇ but we want to evaluate the other two actions that we're not taking now we want to get to the eventual reward that we're going to get So what we do is after we've applied so we've done our action we've gotten a reward or not then we take this discount factor this is set to 0„ÄÇ

95 so we're taking 95% of whatever the maximum Q value was for this next state that we're moving into based on that action So whatever that action„ÄÇultted in this new state„ÄÇ We look at the Q values for this new state and we take the maximum of those action Q values„ÄÇ

 So that is the maximum that is an estimateÔºå as it says of the optimal future reward for the next the next step in all of this„ÄÇ You don't want to necessarily apply this directly to it because then its mind is too much to the future„ÄÇ and is the here and now is quite important The reward that we just got it's a balance between these two„ÄÇ but a very common setting for this one„ÄÇ and the one I'm using is 95%„ÄÇ

 So we're taking both of these into consideration considerably„ÄÇ So that's it„ÄÇ That is how this equation works„ÄÇ and I wanted to take you through each step of it because this is a very common format equation for machine learning and I give you a description of what these values are„ÄÇ

 we just went through all of those„ÄÇ So let's look at the Q learning car„ÄÇ the Q learning car is going to basically learn to perform similar to the preprogrammed car that„ÄÇI gaveve you„ÄÇ the trick is it's learned by a machine that has no concept of gravity or hills or anything like that„ÄÇ It just plays with the car until it finally gets its reward and distributes that across the actions and eventually gets a pretty good program„ÄÇ

 So this is the Q learning car„ÄÇ This is calculating to the discrete state„ÄÇ this is just putting the position into one of those 10 buckets„ÄÇ This is running the game„ÄÇ So while we're not doneÔºå we are going to basically this is a very important part to understand here„ÄÇ This is how epsilon is used„ÄÇ Epsilon is a value„ÄÇ One means make the car move completely erratically random0 means make the car move completely according to the Q table„ÄÇ

5 means do it kind of half in half„ÄÇ This epsilon value starts out very random at first„ÄÇ and then we decay it as we go so that it goes closer and closer to zero„ÄÇ and then eventually for the last partÔºå it is performing completely from the Q table„ÄÇAnd learning completely on the Q table„ÄÇ The randomness forces it to explore and try out radical new ideas„ÄÇ

 And then once you've locked in on a pretty good overall technique„ÄÇ then the machine learning can take over and refine that simulated andneing is a very similar process to this and other Monte Carlo techniques as well„ÄÇ We get our reward by calling the environment„ÄÇ Con the state into a discrete value„ÄÇ We check to see if we've reached the goal position if we have we're done„ÄÇ

 and then we update the Q value„ÄÇ This is that big hair equation from before written in Python„ÄÇ and then we switch the state to to the new state that we have moved to„ÄÇ So this is basically running the game„ÄÇ This runs one step of the game„ÄÇ and these are all these hyperparameters„ÄÇ Learn rate is the learning rate for the algorithm„ÄÇ

 how quickly we update those Q values Disc I told you before that's the 95% How many episodes we want to run how often do we want an update I set that to every thousand discrete grid size„ÄÇUsing just 10 so that whole left or right between the two mountains is effectively broken into into 10 values just to keep it a discrete input into Q learning„ÄÇ

 which is how key learning typically works We can get Q learning to work on continuous values but that that gets into other algorithms that we'll see later in this chapter This is the decay for the epsilon like I told you about„ÄÇSo we load up the mountain car environment„ÄÇ We set up the epsilon change so that it can decay eventually to0 over the episodes that we wanted to„ÄÇ

 We're gonna loop until we're done with the episodes„ÄÇ When we hit the show every so show every 1000 in the case that I gave you„ÄÇ it will show you what's going on and run a game„ÄÇ you'll only see the game if you're running this locally„ÄÇ we count the successes and we run it„ÄÇ So you can see from the first thousand episodes„ÄÇ

 there were no successes at all„ÄÇ We don't get any successes until 4000„ÄÇ And through a lot of this since there's no feedback„ÄÇ The only really capability it has here is trying random approaches„ÄÇ And finally„ÄÇ it gets some success„ÄÇ And that is why the epsilon is decaying because we want that randomness to start to go away„ÄÇ

 And once it starts to get some learning to refine what it's learned„ÄÇ You can see here of the thousand episodesÔºå it finally is completely successful„ÄÇ However„ÄÇ there's a lot of random„ÄÇto the environment„ÄÇ So it'sÔºå it's not learned completely yet„ÄÇ I like to let it go until it gets to fairly consistent success like here„ÄÇ

 you could modify this so that the code actually looks at this and stops based on the the thousands getting consistent„ÄÇ HoweverÔºå the problem you have then is„ÄÇYou increase the total number of episodes and then your decay of the epsilon doesn't work as well„ÄÇ

 because you don't know how deep you're going at the very beginning„ÄÇ Now to run it„ÄÇ we can see it run here„ÄÇ and it rocksÔºå and it's not quite as good as the preprogram card„ÄÇ but it does it„ÄÇ Now let's look inside a deep let's look inside a Q learning's brain and see what's going on here I can dump the table„ÄÇ So the velocity is discreetized as is the position„ÄÇ We have to discreteet these two„ÄÇ Otherwise„ÄÇ

 how would we have a tableÔºå we would have infinite values here„ÄÇ Now„ÄÇ when we get into deep Q learningÔºå we can deal quite easily out of the box with continuous values for both of these two„ÄÇ HoweverÔºå deep Q learning does require some further algorithms added to it if we want to continuous actions so that basically we can apply the accelerator maybe halfway or three quarterss away or something like that„ÄÇ Now just looking at the gridÔºå this doesn't tell me a lot„ÄÇ first of all„ÄÇ

I don't feel that velocity is even all that useful of an input for this problem When I did my preprogrammed car I used only position„ÄÇ So what I did was I took the mean of these so that I can see kind of row and column at a time because like I said for this problem I'm not seeing really the value of the velocity and you can see when I take the means look these line up you can see that it's learned almost a gradient and it learns particular this is the one that I'm more interested in„ÄÇ

 it's learning to apply more of it and less well one direction versus the other direction up here versus down here„ÄÇ Now up here these last two those are a bit anomalies but that could also because it's already hit the goal stated it's already crashing to the top of the mountain it may not really matter what it's doing at that point and that's Q learning Q learning is tablebased will'll look more at this with deep learning where we can get deal better with continuous values first in„ÄÇ



![](img/30d4f6c3c88418548a35fcff3628ad27_6.png)

St and then also in the actions„ÄÇ Thank you for watching this video„ÄÇ In the next video„ÄÇ we're going to look at how to extend this table into deep Q learning that way we don't have to represent literally every combination we can let the neural network learn to generalize„ÄÇ

