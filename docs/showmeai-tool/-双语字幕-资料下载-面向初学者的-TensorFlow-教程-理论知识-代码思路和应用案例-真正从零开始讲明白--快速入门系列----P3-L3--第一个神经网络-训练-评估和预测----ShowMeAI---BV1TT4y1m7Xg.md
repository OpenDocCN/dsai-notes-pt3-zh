# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÈù¢ÂêëÂàùÂ≠¶ËÄÖÁöÑ TensorFlow ÊïôÁ®ãÔºåÁêÜËÆ∫Áü•ËØÜ„ÄÅ‰ª£Á†ÅÊÄùË∑ØÂíåÂ∫îÁî®Ê°à‰æãÔºåÁúüÊ≠£‰ªéÈõ∂ÂºÄÂßãËÆ≤ÊòéÁôΩÔºÅÔºúÂø´ÈÄüÂÖ•Èó®Á≥ªÂàóÔºû - P3ÔºöL3- Á¨¨‰∏Ä‰∏™Á•ûÁªèÁΩëÁªú(ËÆ≠ÁªÉ„ÄÅËØÑ‰º∞ÂíåÈ¢ÑÊµã) - ShowMeAI - BV1TT4y1m7Xg

![](img/3f9d346b7c507e285fe99bf4d69f54ed_0.png)

üéºÔºåHeyÔºå guysÔºå welcomee to the third Tens offlow tutorial„ÄÇ Today„ÄÇ

 we're going to build our first neural network„ÄÇ I will not explain the theory here and instead focus on the implementation„ÄÇ

 but I will provide some links in the description„ÄÇ If you want to learn more about this„ÄÇ

 And I will also provide the slidesÔºå as well as the code in my Github reportpo„ÄÇ

 So we implement a neural network that will look like this„ÄÇ Our neural network has multiple layers„ÄÇ

 and it gets an input imageÔºå and then it processes this imageÔºå and at the end„ÄÇ

 it produces a probability for each class„ÄÇüòäÔºåSo we have a multi class classification problem here„ÄÇ

 And this means that at the endÔºå we use a soft max layer to get the probabilities„ÄÇ

So this is what we are going to code„ÄÇ And I promise that the final code will look relatively easy because Tensorflow takes care of a lot of things for us„ÄÇ

 So even beginners should be able to build a neural network like this„ÄÇ

 and Tensorflow provides two different kind of APIsÔºå the Kaas sequential API and the subclassing API„ÄÇ

 The Kaas API abstracts away a lot of things and makes implementing the algorithms much easier„ÄÇ

 So they say here that this one is for beginners and the other one is for experts„ÄÇ

 So I don't think that this is a good descriptionÔºå because I think that Kaas is good not just for beginners„ÄÇ

 but it's also a good fit for experience programmers„ÄÇ

 So this is a great API here and you can implement a lot of things„ÄÇ

 And then only when you need more flexibility you can or should switch over to the subclassing API„ÄÇüòä„ÄÇ



![](img/3f9d346b7c507e285fe99bf4d69f54ed_2.png)

So in this beginner courseÔºå we concentrate on the Kaa sequential API„ÄÇ

 but I will also cover the subclassing API in later episodes„ÄÇ

 So for now let's use the sequential API and let's jump to the code„ÄÇ

 So I already imported Tenofflow and I silenced some warnings like in the last episode so you don't need to worry about this„ÄÇ



![](img/3f9d346b7c507e285fe99bf4d69f54ed_4.png)

And now let's start implementing our network with the Kaas API„ÄÇ So we also say from Tensflow„ÄÇ

 we want to import Kaas„ÄÇ And since Tensorflowlow 2Ôºå this is included„ÄÇ

 So before it was a separate API„ÄÇ But now it is fully integrated„ÄÇ

So then we also import nuy S N P and we import matplot Li do pi plot S PLT because I want to show you a plot„ÄÇ

And now let's get our data set first„ÄÇ And in this tutorialÔºå we use the Mnis data set„ÄÇ

 So the famous data set for handwritten diit classification„ÄÇ

 And this is included in Kas do data sets do Mni„ÄÇ And then we get training and testing sets by saying x train and Y train„ÄÇ

 and then comma and then another tuple and then x test and y test„ÄÇ And this is Mist dot load data„ÄÇ

 So we have to use two tuples here because this is what the load data returns„ÄÇAnd thenÔºå for example„ÄÇ

 let's print X train dot shape„ÄÇAnd let's also print Y train dot shape„ÄÇ

 So now let's open our terminal„ÄÇ And here I' am already inside the virtual environment with Tensorflow installed„ÄÇ

 So let's run this file„ÄÇSo this is this file here„ÄÇ And let's have a look at the shape„ÄÇ

So we see our xtrain has this shapeÔºå so we have 60000 training images„ÄÇ

 and each image has the size 28 by 28Ôºå and then we have 60000 labels corresponding to the data here and by the way„ÄÇ

 this is a nuy and D right nowÔºå so this is not a tens of load tensor„ÄÇ

 but we can still use it for our model then„ÄÇ So the first thing I want to do is I want to normalize the data because right now„ÄÇ

 the images have values between0 and 20055„ÄÇ and we want to normalize this so that the values are between 0 and1„ÄÇ

 So we say x strain and x test equals and then we say x strain divided by 255 as float„ÄÇ

And the second one X tests divided by 255„ÄÇ So we can do this in one line for both data sets„ÄÇ

And now let me copy and paste some code in here„ÄÇ So I want to plot the data„ÄÇ

 So we just say P T I to plot6 different images„ÄÇ So the first six digits„ÄÇ

 So let's run this and have a look at the plot„ÄÇ AllrightÔºå so this is what the plot looks like„ÄÇ

 So here we see the handwritten diits„ÄÇ So very simple„ÄÇ And this is what we are going to classify„ÄÇ

 So let's remove this„ÄÇ and let's build our model„ÄÇ So let's build our model„ÄÇ And as I said„ÄÇ

 we are using the sequential API„ÄÇ So we set model equals„ÄÇ And this is a ks dot model dot sequential„ÄÇ

 And here we pass in a list with all the different layers„ÄÇ So similar like here„ÄÇ

 we want to have different fully connected layers„ÄÇ

![](img/3f9d346b7c507e285fe99bf4d69f54ed_6.png)

![](img/3f9d346b7c507e285fe99bf4d69f54ed_7.png)

So let's start with ks dot layers dot flatten„ÄÇ So this just flattens our image to be to reduce one dimension of this 28 times 28„ÄÇ

And then we are going to use our first real fully connected layer„ÄÇ So we say ks dot layers dot dense„ÄÇ

 So the dense layer is the fully connected layer in the Kaas API„ÄÇ

 And here we have to specify the output„ÄÇ So this is a hidden size that we can specify„ÄÇ

 So I am going to use 128Ôºå but you can use a different one here„ÄÇüòä„ÄÇ

And then we also say activation equals reus„ÄÇ So we're going to use the reou activation function„ÄÇ

 So if you have a look againÔºå at this plotÔºå then usually all these layers are followed by activation functions„ÄÇ

 So I have a full tutorial about activation functions in the pytorch beginner course„ÄÇ

 but the same concept applies here too„ÄÇ So I will put the link in the description„ÄÇ

 and you can check it out„ÄÇ But basically what you should know is that it introduces nonlineity„ÄÇ

 and it improves the training„ÄÇ So it makes our model better„ÄÇ

 So we should use a activation function after each of these layers„ÄÇ

 and then so let's use just one in the middle„ÄÇ and then let's use our final layers„ÄÇ So this is again„ÄÇ

 a dense layer„ÄÇüòä„ÄÇ

![](img/3f9d346b7c507e285fe99bf4d69f54ed_9.png)

![](img/3f9d346b7c507e285fe99bf4d69f54ed_10.png)

And now we need to specify 10 outputs„ÄÇ So we want to have„ÄÇ so we do have 10 different classes„ÄÇ

 And for each classÔºå we need one output„ÄÇ So So in this simple example„ÄÇ

 we only have two different classes doc and cat„ÄÇ And that's why we only need two output layers„ÄÇ

 but in our exampleÔºå we have 10 classes„ÄÇ So that's why this must be 10„ÄÇ So this has to be 10„ÄÇ

 but you can play around with this size„ÄÇ And then as I saidÔºå we want to get the probabilities„ÄÇ

 So that's why we need a soft marks layer„ÄÇ So we could„ÄÇ

 we could include this here by saying ks dot layers dot soft marks„ÄÇ

 But actually in the tens of load docsÔºå they say that this is not recommend it„ÄÇ

 but instead you should„ÄÇ

![](img/3f9d346b7c507e285fe99bf4d69f54ed_12.png)

![](img/3f9d346b7c507e285fe99bf4d69f54ed_13.png)

![](img/3f9d346b7c507e285fe99bf4d69f54ed_14.png)

![](img/3f9d346b7c507e285fe99bf4d69f54ed_15.png)

Included in your loss function later„ÄÇ So we leave this out for now„ÄÇ

 So now this is all that we need for our model and our model can automatically determine the correct input sizes„ÄÇ

 we only need to specify the output sizes„ÄÇ but let's say after we create this„ÄÇ

 we want to call model dot summary and we want to print this„ÄÇ So right now this doesn't work„ÄÇ

 But if we specify the first input size„ÄÇ So here we can say input shape equals and then 28 by 28„ÄÇ

Then this works„ÄÇ But you don't have to do this„ÄÇ You can also„ÄÇ

 it can also figure this out when you fit it to the training data later„ÄÇ

 but then you can only print this after you compile your model„ÄÇ

 So it's a good practice to include this here„ÄÇ So now let's save this and run this again„ÄÇ

 and then we will print the model summary„ÄÇ And here I forgot to include an S„ÄÇ

 So it's called Kara dot model dot sequential„ÄÇAlrightÔºå so here this is the printed summaries„ÄÇ

 So it prints all the different layers„ÄÇ So here we see our first flattening layer„ÄÇ

 and this is the shape 784„ÄÇ So this is 28 times 28„ÄÇ It reduces it to one dimension„ÄÇ

 And this is not an actual layer that we have to train„ÄÇ So it doesn't have parameters„ÄÇ

 And we have our first dense layer with 128 output shape„ÄÇ And this is the number of parameters„ÄÇ

 and then our second dense layer with 10 outputs„ÄÇ And hereÔºå the first one is just none„ÄÇ

 So this is the number of samples that we don't know yet„ÄÇüòä„ÄÇ

And then we also see the total parameterss and the trainable parameterss„ÄÇSoÔºå yeah„ÄÇ

 so now we already have our model„ÄÇ And by the wayÔºå so I think you can already see how simple it is with this sequential API„ÄÇ

 And there's also a second way to do it with the sequential API so you can set up your model like this„ÄÇ

 And then at each layer separately„ÄÇ So you can call model dot at„ÄÇ And then you use this layer„ÄÇ

 and then you seeÔºå sayÔºå model dot at„ÄÇ And then you use this layer„ÄÇüòä„ÄÇ

Model dot at and the second layer and then model dot at„ÄÇ and the last layer„ÄÇ

 So this is doing the same thing„ÄÇ But this is the advantage that now you could print model summary after each operation and figure out how your training looks„ÄÇ

 SoÔºå yeahÔºå keep that in mind that you can do it like this as well„ÄÇ

 So we're just going to use the first one„ÄÇAnd comment this out again„ÄÇ

 So now we have our model and now the next thing we need is we need the loss and optimizer„ÄÇ

 So in a multiclass classification problem„ÄÇ typicallyy we use the categorical cross entropy„ÄÇ

 So we say loss equals and then we say ks dot losses„ÄÇ

 And then here this is actually called sparse categorical cross entropy„ÄÇ

 And this is because our y is an integer class label„ÄÇ So it's 0Ôºå1Ôºå2Ôºå3 or something like that„ÄÇ

 And that's why we use this sometimes the label is also encoded as one hot„ÄÇ

 So a one for the class 0 and then90s for the„ÄÇOther classes„ÄÇ So in this case„ÄÇ

 we just use the categorical cross entropyÔºå but in our caseÔºå we use this one„ÄÇ

 And we also say from logics equals true„ÄÇ And this is because here at the end„ÄÇ

 we didn't include the soft marks„ÄÇ So we still have the raw numbersÔºå the raw las„ÄÇ

 So we have to use thisÔºå Otherwise it won't train very well„ÄÇAnd then we need to create an optimizer„ÄÇ

 So we say Kas dot optimizers„ÄÇ And then hereÔºå let's use the Adamom optimizer„ÄÇ

 a very popular optimizer„ÄÇ and we need to give it a learning rate„ÄÇ So we say this is 0001„ÄÇ

 So this is the one of the most important socalled hyperpar that you should tweak in the beginning to get a good result„ÄÇ

 So play around with this„ÄÇ And then we also define some metrics that we want to track„ÄÇ

 So in this caseÔºå and this should be a list„ÄÇ And here we only want to track the accuracy metric„ÄÇ

So when we have thisÔºå then we call model dot compile„ÄÇ

 So this is what we do with the Tensorflow framework„ÄÇ And then we say our loss equals the loss„ÄÇ

 our optimizer equals the optimizer and the metrics equals the metrics„ÄÇ

 So this will configure the model for training and after that we can start the training„ÄÇ

 So let's define the batch size and set this to 64 and set the number of epochs for our training„ÄÇ

 So here I only use5 epochs and then we can simply call model dot fit and we want to fit the x train and the Y train„ÄÇ

 So our training data and we specify the batch size equal„ÄÇThe batch size„ÄÇ

And the epoch equals the epoch„ÄÇAnd then we said shuffle equals true„ÄÇ So this is the default„ÄÇ

 but I want to stress this that you should always do this during training„ÄÇ

 And then let's set ver equals 2„ÄÇ So this is just for logging0 means no output 1 means a progress bar and two means normal logging„ÄÇ

 So this is all that we have to do to train our model„ÄÇ So again„ÄÇ

 we build our model with the sequential API„ÄÇ Then we define the loss and the optr and optional sum metrics„ÄÇ

 So we don't have to use this here„ÄÇ but we can„ÄÇ And then start the training with model dot fit„ÄÇ

 And now we can already start training this„ÄÇSoÔºå let's run this„ÄÇ

So now we see that it's starting the training and after each epochÔºå it prints the somesymmetric„ÄÇ

 so it prints the loss and the accuracy because we specified this„ÄÇAnd we see that after five epochs„ÄÇ

 we already have a very low loss„ÄÇ So this decreased and our accuracy is 98 per cent„ÄÇ

 So our simple neural network is already very good for this task„ÄÇSoÔºå now„ÄÇThat we trained it„ÄÇ

 What we want to do then is want to evaluate our model„ÄÇ

 and we can also very simply do this with by saying model equal model dot evaluate and then we use our test data„ÄÇ

 So we use X test and y test„ÄÇ And againÔºå here we use the or can use the batch size and set this to our batch size„ÄÇ

 and I also set ver both equals true equals2 for some logging„ÄÇ So this is how we evaluate it„ÄÇ

 and so the batch size means that it takes some batches and performs the computations on the batches„ÄÇ

 So this can make the training and evaluation faster and even and also improve the training„ÄÇ

 So let's„ÄÇLet's run this again„ÄÇ So for nowÔºå I didn't save the model„ÄÇ

 So now I have to train it again and then evaluate it„ÄÇ So let's run it„ÄÇ And in later episodes„ÄÇ

 we learn how we save our model after the training„ÄÇ AlrightÔºå so training a done So again„ÄÇ

 we see the five different epochs„ÄÇ and then we have one print statement for the evaluation„ÄÇ

 So we see that it's„ÄÇSlightly lowered the accuracy for the test setÔºå but it's still very good„ÄÇ

 So now we already have a very good training to classify the amnes digits„ÄÇ

And now let's see how we can do some predictions with our model„ÄÇ

 So there we have several different options„ÄÇ and again„ÄÇ



![](img/3f9d346b7c507e285fe99bf4d69f54ed_17.png)

For the predictionsÔºå we need the soft max layer to call the probabilities„ÄÇ So we didn't„ÄÇ



![](img/3f9d346b7c507e285fe99bf4d69f54ed_19.png)

Put this in here as the last layerÔºå but it was automatically included here during the training„ÄÇ

 but now we need it„ÄÇ So our first option is to create a new new model„ÄÇ

 So let's call this probability model„ÄÇ And this is also a Kaas dot model dot C sequential model„ÄÇ

 And here this is one new thing that I want to show you„ÄÇ we can pass a whole model into this„ÄÇ

 So we can put in our original model„ÄÇ So now it has all of the layers of this model„ÄÇ

 And now we use the kra dot layers dot softms layer„ÄÇ

 And now we that we have this we can call this probability model„ÄÇ

 So we say predictions equals probability model and then pass„ÄÇIn the X test dataÔºå so„ÄÇ

Now we have this„ÄÇ we canÔºå for exampleÔºå get the first prediction by saying this is the predictions index0„ÄÇ

 And let's print this here„ÄÇ So print prediction 0„ÄÇ And then this is the probability„ÄÇ

 So now we want to choose the class with the highest probability„ÄÇ

 So we can do this by calling the arc max function„ÄÇ So we say label 0 equals nuy dot arc max„ÄÇ

 and then of this prediction pre 0 and then print the label 0„ÄÇ So now let's save this„ÄÇ

 And now I run the training and evaluation again and then print this for you„ÄÇ AllrightÔºå so it's done„ÄÇ

 And here it it prints the prediction 0„ÄÇ So for„ÄÇ

![](img/3f9d346b7c507e285fe99bf4d69f54ed_21.png)

![](img/3f9d346b7c507e285fe99bf4d69f54ed_22.png)

Each classÔºå it has a probability„ÄÇ and then we take the label or the index with the highest probabilities„ÄÇ

 This is index 7 in this case„ÄÇ So I think it's„ÄÇThis one„ÄÇ So 9„ÄÇ49„ÄÇ This is the highest one„ÄÇ So yeah„ÄÇ

 this works„ÄÇ So this is the first way to do it„ÄÇ The second way is to use our original model plus the soft marks separately„ÄÇ

 So we can say the same as we are doing here predictions equals„ÄÇ

 And then we call the original model and pass an x test„ÄÇ

 And then we apply the soft marks for ourselves„ÄÇ So we say predictions equals Tens offlow dot N N dot soft maxs„ÄÇ

 And then here we put in the predictions„ÄÇ And now we have the same as we have here„ÄÇ

 So let's print this one„ÄÇüòäÔºåAnd then when we run this and print this„ÄÇ

 then we should see the exact exact same numbers„ÄÇOhÔºå sorryÔºå I stopped this for now„ÄÇ

 So here I forgot to get the prediction 0 so that we see this is the same here„ÄÇ

So let's clear this and run it again„ÄÇ AlrightÔºå so againÔºå our training is done„ÄÇ

 And here you can see that it prints the exact same numbers and the same class label„ÄÇ

 So you can do it like this„ÄÇ And then as a third option instead of calling model and model like this„ÄÇ

 you can also call model dot predict„ÄÇ and then the data„ÄÇ

 And here you can also specify the batch size equals the batch size„ÄÇ

 then it computes it for each each batch„ÄÇ So this is also what you can do„ÄÇ and again„ÄÇ

 the numbers should be the same„ÄÇAnd now let's not only get one single prediction„ÄÇ

 So let's say we have two or five different predictions„ÄÇ So let's call the 0Ôºå5s„ÄÇ

 And then we get this by saying predictions and use slicing from index 0 to index 5„ÄÇ

And now let's print the pre0Ôºå5 dot shape first for you„ÄÇ And now if we call the arc max for this„ÄÇ

 So now let's say our label 0Ôºå5s equalsÔºå and then if we get the predictions„ÄÇ

 if we call the nuy dot arc maxÔºå with this5 predictions„ÄÇ then we also have to specify the x„ÄÇ

 and in this caseÔºå we want to compute it along xs 1„ÄÇAnd now we should get five different labels„ÄÇ

 So print label 50s„ÄÇAnd now let's clear this and run it one more time and hope that this works„ÄÇ

 AlrightÔºå so againÔºå this is done„ÄÇ we see that againÔºå with this model predict„ÄÇ

 We get the same numbers„ÄÇ And then this is our shape„ÄÇ

 So 5 by 10 So  five different samples and or predictions„ÄÇ

 and for each prediction10 different probabilities like here„ÄÇ And if we call the arcms along X 1„ÄÇ

 then we get five different labels„ÄÇ So againÔºå the first label is label 7 like here„ÄÇ

 And these are the next predictions„ÄÇ YeahÔºå so this is how you can predict it„ÄÇ

 And I think this is all you should know to build your first neural network„ÄÇ So again„ÄÇ

 you build your sequential modelÔºå then you set up loss and optimizerÔºå Then you compile the model„ÄÇ

Then you call model fit and model evaluate„ÄÇ And then when you want to predict„ÄÇ

 you can call model predict„ÄÇ But yeahÔºå don't forget to call the soft marks if you want the actual probabilities„ÄÇ

 And yeahÔºå I think that's all for now„ÄÇ And I hope you enjoyed this tutorial„ÄÇ

 Please hit the like button and consider subscribing to the channel„ÄÇ

 And I hope to see you in the next video by„ÄÇüòä„ÄÇ

![](img/3f9d346b7c507e285fe99bf4d69f54ed_24.png)