- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë140ÂàÜÈíüÂÖ•Èó® PyTorchÔºåÂÆòÊñπÊïôÁ®ãÊâãÊääÊâãÊïô‰Ω†ËÆ≠ÁªÉÁ¨¨‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P5ÔºöL5- PyTorch TensorBoard
    ÊîØÊåÅ - ShowMeAI - BV19L4y1t7tu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you haven't alreadyÔºå you'll need to set up a Python environment with the
    latest versions of Pytorrch and tensor board„ÄÇThe commands on screen show how to
    do that for Conda and Pip will also be using map plotlib to manipulate images„ÄÇOnce
    you have the dependencies installedÔºå you can run the companion notebook for this
    video in the environment you set up„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15a6275b42701baef938463f259b9949_1.png)'
  prefs: []
  type: TYPE_IMG
- en: For this modelÔºå we're going to be training a simple neural network to recognize
    different articles of clothing„ÄÇ We'll visualize data elements directly„ÄÇ track
    the success of the training process„ÄÇ We'll use Tensor board to look under the
    hood at the model itself„ÄÇAnd will do a more advanced visualization of the data
    set as a whole and its internal relationships for a data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: will use fashion emminist„ÄÇThis is a set of small image tiles that depict various
    garments classified by the type of garment depicted„ÄÇFor a modelÔºå we'll use a version
    of Linenette 5 tweet to accommodate the fashion Mnes data set„ÄÇ![](img/15a6275b42701baef938463f259b9949_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by importing the libraries we need and the summary writer class
    from Torchdo Util's do Tenser board„ÄÇThis is the class wrapping the tensor board
    support in Pytorrch and will be your primary interface for interacting with Tensor
    board„ÄÇIt's good practice to visualize your training data prior to feeding it to
    your model„ÄÇ especially with computer vision tasks„ÄÇLet's set up our data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We'll use Torch visionion to download training and validation splits of the
    data set„ÄÇ We'll talk more about validation shortlyÔºå and we'll also set up data
    loaders for each of the data set splits and define the classes against which we're
    classifying„ÄÇLet's visualize a few instances of the data set„ÄÇWe'll use an iterator
    to pull out a few instances of the data and create a map plotlib helper function
    to batch them together in a grid„ÄÇLet's show them in the notebook„ÄÇSo how do we
    add this to Tensor boardÔºü
  prefs: []
  type: TYPE_NORMAL
- en: It's a one liner to write the data to the log directory„ÄÇNote that we also called
    flush on our summary writer object„ÄÇ This makes sure that everything we've logged
    through the writer has been written to disk„ÄÇ![](img/15a6275b42701baef938463f259b9949_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: NowÔºå let's switch to a terminal and start tensor board„ÄÇ![](img/15a6275b42701baef938463f259b9949_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: We'll copy the URL that the Ten award command line gave us and look at the images
    tab„ÄÇNote here that the image we've added has a header containing the label we
    applied when we save the image to the Tensor board log directory„ÄÇNextÔºå we'll use
    Tensor board to help assess our training process„ÄÇWe'll graph the accumulated training
    loss for regular time steps and compare it to the loss measured against a validation
    data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For backgroundÔºå here's a brief aside on what we're doing and why„ÄÇIf you took
    a math class„ÄÇ it's likely that you'd be given homework problem sets to solve after
    a number of homework sets„ÄÇ you'd be given an exam„ÄÇThe exam problems would be similar
    in nature„ÄÇ but different in their specifics to the homework problems you've seen
    already„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is intended to make sure you learn the content of the class and not just
    memorize the homework problems„ÄÇSimilarlyÔºå we can use a validation data set„ÄÇ that
    is a portion of the total data set not used for training to see whether our model
    is learning generally or whether it's overfitted to the training data akin to
    memorizing the training instances instead of modeling the general function we're
    trying to optimize the model for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let's set up a training loop with validation checks and graph the results„ÄÇHere
    we have a training loop„ÄÇ You can see that at the top of the code„ÄÇ we declared
    a variable to accumulate the measured loss of the model's predictions„ÄÇ which will
    report every thousand training steps„ÄÇWell also be doing a separate loss check
    against the validation data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For tracking and comparing two different quantitiesÔºå we'll use the add Scrs
    call on summary writer„ÄÇ which allows us to add a dictionary containing multiple
    scalar values„ÄÇ each with distinct tags that get their own line on the graph„ÄÇLet's
    run the hell and see what that looks like„ÄÇSwitching over to Tensor board and looking
    at the Scrs tab„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can see that our loss is decreasing monotonically over the training run„ÄÇ
    This is a nice reassurance that the training is working„ÄÇüòä„ÄÇBut are we overfitting
    looking at them graphÔºå we can see that the validation and training curves are
    converging nicely„ÄÇNextÔºå let's use Tensor board to better understand our model
    and how data flows through it„ÄÇTo do this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we'll use the add graph method on our summary letter„ÄÇThis method takes as arguments„ÄÇ
    the model and a sample input that will be used to trace data flow through the
    model„ÄÇWill run the cell and switch over to tensor board„ÄÇAnd going to the graphs
    tab„ÄÇ we can see a very simple graph showing the model with inputs going in one
    side and outputs submitted from the other„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Of courseÔºå we'll want more detail in thisÔºå and we can get it by double clicking
    on the model node in the graph„ÄÇ And here we can see a graph containing all of
    our layers and arrows indicating how data flows through them„ÄÇ Note that because
    the model uses the same max pool object twice„ÄÇ the second convolutional layer
    appears to be embedded in a loop„ÄÇBut as you can see from the code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The flow is more linear than that„ÄÇWe've already used Tensor board to display
    visualizations of instances of our data„ÄÇ But what about the whole data set„ÄÇAn
    embedding is a mapping of instances from a higher dimensional space to a lower
    dimension 1„ÄÇThis is a common technique in N LP„ÄÇ If you have a 10000 word vocabulary
    represented by one hot vectors„ÄÇ Your words are unit vectors in a 10000 dimensional
    space„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If you train an embedding layer that maps these vectors to a lower dimensional
    space„ÄÇ relationships can emerge„ÄÇ For exampleÔºå the new vectors for words like good„ÄÇ
    excellent and fabulous will tend to be clustered in that lower dimensional space„ÄÇIn
    our case„ÄÇ our 28 by 28 image tiles can be thought of as 784 dimensional vectors„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We can use the summary writers's add embedding method to project this down to
    an interactive 3D visualization„ÄÇHere's a bit of code to select a random sample
    of our dataÔºå label itÔºå and project it„ÄÇNote that„ÄÇ as alwaysÔºå we use the flush method
    to ensure that all our data is written to disk„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15a6275b42701baef938463f259b9949_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Switching over to tensorboard„ÄÇ![](img/15a6275b42701baef938463f259b9949_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: We can see on the projector tabÔºå a 3D visualization of our new embedding„ÄÇ![](img/15a6275b42701baef938463f259b9949_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: Zoomed outÔºå we can see some large structuresÔºå some arcs within the 3D space„ÄÇ
    zooming in on some of these structures„ÄÇ We can see that some of these arcs have
    clustered similar carbon types„ÄÇZoom in on your own sample of the data and see
    if you can identify patterns in how different types of garments are clustered
    in this 3 D projection„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15a6275b42701baef938463f259b9949_15.png)'
  prefs: []
  type: TYPE_IMG
- en: For more information on Py Torch's Tensor boards support„ÄÇYou can visit the Pytorrch
    documentation at Pytorch„ÄÇorg for full documentation of torch„ÄÇutils„ÄÇtensorboard
    summaryumarywriter„ÄÇThe Pitorrch tutorials section at Piytorch„ÄÇ org has tutorials
    on using Tensorboard„ÄÇAnd the Tensorboard documentationÔºå of course„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: has more detail about Tensor board itself„ÄÇ If you want a deeper view of what
    the summary writer is doing under the hood„ÄÇ![](img/15a6275b42701baef938463f259b9949_17.png)
  prefs: []
  type: TYPE_NORMAL
