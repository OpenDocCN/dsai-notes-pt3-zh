- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Hugging Faceé€ŸæˆæŒ‡å—ï¼ä¸€éæå®šNLPä»»åŠ¡ä¸­æœ€å¸¸ç”¨çš„åŠŸèƒ½æ¿å—ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- æ¨¡å‹å’Œåˆ†è¯å™¨ - ShowMeAI
    - BV1cF411v7kC
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can use this model and tokenizer directly and do some of the
    steps manuallyã€‚ this will give you a little bit more flexibilityã€‚ So down hereã€‚
    let's first use the tokenizer and see what this doesã€‚ So firstã€‚ let's call the
    tokenizer dot tokenize functionã€‚ So we sayã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: let's call the tokens and then equals tokenizer dot tokenize and then the string
    or the sentence we want to tokenizeã€‚ So let's copy and paste this in hereã€‚ And
    then once we get the tokensã€‚ we can use them and get the token Is out of itã€‚ So
    we can say token Is equalsã€‚ And then we again use the tokenizer and the function
    convert tokenizer toã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_2.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_8.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Called Isã€‚ And then it meet the tokensã€‚ So this is one way how to do thisã€‚ Or
    we can do this directly by saying token I equalsã€‚ And then we call this tokenizer
    like a functionã€‚ And then againï¼Œ we give it the same string hereã€‚ So now let's
    print all these three variables to see where is the differenceã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: So first we print the tokensï¼Œ then we print the token Isã€‚ And then here let's
    actually give this a different nameã€‚ So let's call this input Iã€‚ So now let's
    run this and see how this looks likeã€‚ Alrightï¼Œ so here is the resultã€‚ So as you
    can see when we call tokenizer dot tokenizeï¼Œ then we get a list of stringsã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_10.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_11.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_12.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_13.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_14.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_15.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_16.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_17.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: The list of the words backs so now each word is a sorry each word is a separate
    token and for exampleã€‚ this one is our smiley face or our emoji So yeah this is
    what the tokenized function does and then once we call this convert tokens to
    IDs we get this one back so now it converted each token to an ID so each word
    has a very unique ID and this is basically the mathematical representation or
    the numerical representation that our model then can understand So this is what
    we get after this function and if we call this tokenr directly then we get a dictionary
    back and here we have the key input Is and we also have the attention mask soã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_19.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: For now you don't really have to worry about this but let's have a look at the
    input IDs so if we compare the token IDs with the input IDs then we see we have
    the exact same sequence of token IDs but we also have this 101 and 102 token and
    this is just the beginning of string and the end of string token but basically
    it's the same so yeah this is the difference between these three functions and
    then these input IDs this is what we can pass to our model later to do the predictions
    manually so now like before we can also use multiple sentences of course for our
    tokenr so for example usually in your code you have your training data so let's
    say xtrain and in this example let's justã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ä½ ä¸å¿…å¤ªæ‹…å¿ƒè¿™ä¸ªï¼Œä½†æˆ‘ä»¬æ¥çœ‹çœ‹è¾“å…¥ IDsã€‚å¦‚æœæˆ‘ä»¬æ¯”è¾ƒ token IDs å’Œè¾“å…¥ IDsï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒä»¬çš„ token ID é¡ºåºå®Œå…¨ç›¸åŒï¼Œä½†æˆ‘ä»¬è¿˜æœ‰
    101 å’Œ 102 çš„ tokenï¼Œè¿™åªæ˜¯å­—ç¬¦ä¸²çš„å¼€å§‹å’Œç»“æŸ tokenï¼Œä½†åŸºæœ¬ä¸Šæ˜¯ç›¸åŒçš„ã€‚æ‰€ä»¥è¿™æ˜¯è¿™ä¸‰ä¸ªå‡½æ•°ä¹‹é—´çš„åŒºåˆ«ï¼Œè¿™äº›è¾“å…¥ IDs æ˜¯æˆ‘ä»¬ç¨åå¯ä»¥ä¼ é€’ç»™æ¨¡å‹ä»¥è¿›è¡Œæ‰‹åŠ¨é¢„æµ‹çš„å†…å®¹ã€‚å› æ­¤ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å½“ç„¶ä¹Ÿå¯ä»¥ä¸ºæˆ‘ä»¬çš„
    tokenizer ä½¿ç”¨å¤šä¸ªå¥å­ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸åœ¨ä½ çš„ä»£ç ä¸­ï¼Œä½ ä¼šæœ‰è®­ç»ƒæ•°æ®ï¼Œæ¯”å¦‚ xtrainï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­å°±è®©æˆ‘ä»¬ã€‚
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_21.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_21.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_22.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_22.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_23.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_23.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_24.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_24.png)'
- en: Use these two sentencesã€‚ So this is our X train and then we and then we can
    pass this to our tokenizer and let's call this batchã€‚ So this is our batch that
    we put into our model laterã€‚ So we say batch equals tokenizer and then we call
    this tokenizer directly with our training dataã€‚ and then I also want to show you
    some useful argumentã€‚ So we say padding equals trueã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸¤ä¸ªå¥å­ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çš„ X trainï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å°†å…¶ä¼ é€’ç»™æˆ‘ä»¬çš„ tokenizerï¼Œç§°ä¹‹ä¸ºæ‰¹æ¬¡ã€‚è¿™æ˜¯æˆ‘ä»¬ç¨åè¾“å…¥æ¨¡å‹çš„æ‰¹æ¬¡ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ batch
    ç­‰äº tokenizerï¼Œç„¶åæˆ‘ä»¬ç›´æ¥ç”¨è®­ç»ƒæ•°æ®è°ƒç”¨è¿™ä¸ª tokenizerã€‚æˆ‘è¿˜æƒ³å‘ä½ å±•ç¤ºä¸€äº›æœ‰ç”¨çš„å‚æ•°ã€‚å› æ­¤æˆ‘ä»¬è¯´ padding ç­‰äº trueã€‚
- en: and we also say truncation equals trueã€‚ and then we say max length equals 412ã€‚
    and we say return tenss equals and then as a string P for pi torchã€‚ So this will
    ensure that all of our samples in our batch have the same lengthã€‚ So it will apply
    padding and truncationã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è®¾ç½®äº† truncation ä¸º trueï¼Œå¹¶ä¸”è®¾ç½®äº† max length ä¸º 412ï¼Œå¹¶ä¸”æˆ‘ä»¬è®¾ç½®äº† return tensors ä¸ºå­—ç¬¦ä¸²
    P ä»£è¡¨ pytorchã€‚è¿™å°†ç¡®ä¿æˆ‘ä»¬æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚å› æ­¤å®ƒå°†åº”ç”¨å¡«å……å’Œæˆªæ–­ã€‚
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_26.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_26.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_27.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_27.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_28.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_28.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_29.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_29.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_30.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_30.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_31.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_31.png)'
- en: Necessary and this is also importantã€‚ So in this caseã€‚ we want to have a pytorch
    tenor returned directlyã€‚ So I will show you later what's the difference if you
    don't use thisã€‚ But for now let's just use this and then first of allã€‚ let's print
    this batch and see how this looks likeã€‚ And then we see we get a dictionary and
    again it has the key input Is and the key attention mask and then here it has
    two tenzosã€‚ So the first one for the first sentence and the second one for the
    second sentence and the same for the attention mask So two tensã€‚ So yeah as I
    saidï¼Œ these input id are these unique Is that our model can understandã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¿…è¦çš„ï¼Œè¿™ä¸€ç‚¹ä¹Ÿå¾ˆé‡è¦ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›ç›´æ¥è¿”å›ä¸€ä¸ª pytorch tensorã€‚æˆ‘ç¨åä¼šå‘ä½ å±•ç¤ºå¦‚æœä¸ä½¿ç”¨è¿™ä¸ªä¼šæœ‰ä»€ä¹ˆä¸åŒã€‚ä½†ç°åœ¨è®©æˆ‘ä»¬å…ˆä½¿ç”¨è¿™ä¸ªï¼Œé¦–å…ˆæ‰“å°è¿™ä¸ªæ‰¹æ¬¡ï¼Œçœ‹çœ‹å®ƒçš„æ ·å­ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå­—å…¸ï¼Œå†æ¬¡åŒ…å«äº†é”®
    input å’Œé”® attention maskï¼Œè¿™é‡Œæœ‰ä¸¤ä¸ª tensorã€‚ç¬¬ä¸€ä¸ªæ˜¯ç¬¬ä¸€å¥è¯ï¼Œç¬¬äºŒä¸ªæ˜¯ç¬¬äºŒå¥è¯ï¼Œattention mask ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæ€»å…±ä¸¤ä¸ª
    tensorã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œè¿™äº›è¾“å…¥ ID æ˜¯æˆ‘ä»¬æ¨¡å‹å¯ä»¥ç†è§£çš„å”¯ä¸€ IDã€‚
- en: So yeah now we have this batch and now we can pass this to our modelã€‚![](img/4e0ddcb38791825272bb7f961349fc05_33.png)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªæ‰¹æ¬¡ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å°†å…¶ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚![](img/4e0ddcb38791825272bb7f961349fc05_33.png)
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_34.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_34.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_35.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_35.png)'
- en: '![](img/4e0ddcb38791825272bb7f961349fc05_36.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e0ddcb38791825272bb7f961349fc05_36.png)'
- en: Andã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”ã€‚
