- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P7ÔºöL7- ÊÑüÁü•Âô®
    - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to a new machine learning from scratch tutorial„ÄÇ Today„ÄÇ
    we are going to implement a perceptron using only built and Python modules and
    Ny„ÄÇ The perceptron can be seen as one single unit of an artificial neural network„ÄÇ
    So the perceptron is a simplified model of a biological neuron„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and it simulates the behavior of only one cell„ÄÇ So let's have a look at this
    image here„ÄÇ we have one cell„ÄÇ and our cell gets an inputÔºå so it gets input signals„ÄÇ
    and they are weighted and summed up„ÄÇ And if the whole input signal then reaches
    a certain threshold„ÄÇ Our cell fires a signal and delivers an output„ÄÇ So in our
    caseÔºå it either fires a1 or a0„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now if we model this mathematicallyÔºå then it looks like this„ÄÇ so we have
    our input features and they are multiplied with some weights and then summed up„ÄÇ
    and then we apply an activation function and get our output class„ÄÇSo this is the
    model and now the linear partÔºå the linear model simply looks like this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this is just a linear function„ÄÇW transpose times x plus B„ÄÇ So here we multiply
    and sum up our weights and the bias„ÄÇ So the bias is the W 0 here in this picture„ÄÇAnd
    after this linear model„ÄÇ we apply the activation function„ÄÇAnd in the simplest
    case„ÄÇ we simply use the so called unit step function„ÄÇ and this is defined as it's
    either one if our input reaches a certain„ÄÇ a certain threshold or serial otherwise„ÄÇSo
    in this pictureÔºå the threshold is 0„ÄÇ So if the input is larger than 0Ôºå then the
    output is one„ÄÇ and otherwiseÔºå it's 0„ÄÇAnd now this is all we need to model the
    output„ÄÇ And now the whole output is looks like this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So firstÔºå we apply the linear functionÔºå and then we apply the activationation
    function„ÄÇAnd now we have to come up with the weights and the bias„ÄÇ and for this„ÄÇ
    we use a simple update rule that is called the Perceptron rule„ÄÇSo we look at each
    training sample X IÔºå and for each training sampleÔºå we then apply the update step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this is defined as the new weight is the old weight plus the delelta weight
    and the deelta weight or delta W is defined as alpha times the actual„ÄÇLabel minus
    the predicted label times the training sample X„ÄÇ and here alpha is a learning
    rate between 0 and1„ÄÇ So this is just a scaling factor„ÄÇAnd now let's have a look
    at what this update will mean„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look at the four possible cases in a two class problem„ÄÇ So our
    output can be one„ÄÇ the actual label can be one and the predicted„ÄÇLabel is also
    one„ÄÇ Then the difference is 0„ÄÇ So we have no change for our weights here„ÄÇAnd the
    same is if the actual class is 0 and the predicted class is also 0„ÄÇ so correctly
    classified and the difference is 0„ÄÇ And againÔºå no change for our weights„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But now what happens if we have a misclassification„ÄÇ So if the actual class
    is one and the predicted class is 0„ÄÇ this means that our weights are too low„ÄÇ
    and then we see that the difference is one„ÄÇ So our weights are increased here„ÄÇ
    And if the actual class is 0Ôºå and the predicted class is 1„ÄÇ then our weights are
    too high„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we see that the difference is-1„ÄÇ So then our weights are decreased„ÄÇ So the
    weights are pushed towards the positive or negative class in case of a misclassification„ÄÇAnd
    this is a simple and intuitive ruleÔºå but it works„ÄÇAnd this is all we need„ÄÇ So
    we look at each training sample and then apply the update rule„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we do this a couple of times„ÄÇ So we iterate for a certain number of
    iterations„ÄÇ and then we have the final weights and are done„ÄÇ So this is all we
    need to know„ÄÇ and now we can get started and implement it„ÄÇ![](img/47427fb8775e23c1bec39e64630a9c33_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå first of allÔºå of courseÔºå we use numpy„ÄÇ So we import numpy S and P„ÄÇ And then
    we create a class and call it perceptron„ÄÇPerceptron„ÄÇAnd it gets an innate method„ÄÇ
    of course„ÄÇAnd here it has self„ÄÇ and it gets the learning rate„ÄÇ And I will give
    this a default of 0„ÄÇ01„ÄÇ Then it gets a number of iterations„ÄÇ So n its„ÄÇ And I will
    also give this a default„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let's say 1000„ÄÇ Then I will simply store them„ÄÇ So I will say self dot L R equals
    learning rate and self dot n„ÄÇIts equalsÔºå and its„ÄÇThen we create a the activationation
    function„ÄÇ So let's say self dot activationation fun„ÄÇEqualsÔºå and now let's create
    this hereÔºå and„ÄÇAs I said„ÄÇ the activation function is simply the unit step function„ÄÇ
    So let's call this„ÄÇUnit step„ÄÇFk„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: With self and an input XÔºå and„ÄÇWe could simply say return one„ÄÇ if x is larger
    or equal than 0 and else return 0„ÄÇBut„ÄÇWe see later that we„ÄÇ this would only work
    for one single„ÄÇSampleÔºå but we see later that we want to apply the activation function
    in the predict method for all the test sample„ÄÇ So we want to apply this for a
    N D array as well„ÄÇ And for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can use a simple function that is called numpy dot where„ÄÇ So we return nuy
    dot where„ÄÇ And this will get a condition„ÄÇ So x is larger or equal than 0„ÄÇ And
    if the condition is true„ÄÇ then we return one and otherwise 0„ÄÇSo this will work
    for one single sample„ÄÇ but also for multiple samples in one vector„ÄÇSo now this
    is our activation function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now we can say self dot activationation fun equals self dot unit step fun„ÄÇAnd
    now let's also create the weights„ÄÇ say self dot weights equals none and self dot„ÄÇBs
    equals none„ÄÇ So now that we know that we have to implement them„ÄÇOr get them„ÄÇ And
    now we implement two functions„ÄÇ As alwaysÔºå we implement the fit and the predict
    method„ÄÇ So first„ÄÇDefine the fit method with X and y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this gets the training samples and the training labels„ÄÇAnd thenÔºå of courseÔºå
    oh„ÄÇ I forgot this self„ÄÇAnd thenÔºå we„ÄÇAlsoÔºå define the„ÄÇPredict methodÔºå which gets
    self„ÄÇ and then the test samples„ÄÇ And now we start„ÄÇWith this predict methodÔºå because
    this is very simple„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_3.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's have a look at the approximation of our output again„ÄÇ So here first„ÄÇ we
    apply this linear function and then the activation function„ÄÇSo let's do this„ÄÇ
    So first„ÄÇ the linear functions„ÄÇ So let's say linear„ÄÇ![](img/47427fb8775e23c1bec39e64630a9c33_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Output equals„ÄÇ and this is w transpose times x plus the bias„ÄÇ and W transpose
    times x is nothing else but the dot product„ÄÇ so we can usempie dot„ÄÇoff„ÄÇX and self
    dot„ÄÇSelf dot weights plus self dot bias„ÄÇ So now we have the linear function„ÄÇ and
    now we apply the activation function„ÄÇ So we say why predict equals self dot activation
    function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And as in inputÔºå it gets the linear output here„ÄÇ And then we simply return the
    y predicted„ÄÇ So this is the whole predict method„ÄÇ And now let's jump to the fit
    method„ÄÇ So first of all„ÄÇ let's get the dimensions of the x vector„ÄÇ So this is
    an N D array of size M times N where M or the number of rows is the number of
    samples and N or the number of columns is the number of features„ÄÇSo we say N samples
    and N„ÄÇFeatures equals x dot shape„ÄÇAnd now we in it our weights„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have to give them an initial valueÔºå and we can simply set them to 0 in the
    beginning„ÄÇ So we say self dot weights equals nuy dot0s„ÄÇOf sizeÔºå number of features„ÄÇ
    So for each feature„ÄÇ we put a0 here for our weight„ÄÇAnd alsoÔºå ourÔºå the biasÔºå this
    is simply 0„ÄÇAnd now we can start or one more thing we have to do is we want to
    make sure that our y only consists of classes 0 and1„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so let's say„ÄÇY underscore equals„ÄÇ And now let's convert all the values to 0
    or one„ÄÇ If this is not already the case„ÄÇ So we use list comprehension for this„ÄÇ
    So we say one if I is larger than 0 elseÔºå it's 0Ôºå4 I in y„ÄÇAnd now let's convert
    this to Ny array„ÄÇSo„ÄÇ nowÔºå we have our„ÄÇWhyÔºå and now we can start the training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's again have a look at this update rule„ÄÇ So we want to look at each training
    sample and we also want to do this a couple of iterations„ÄÇ So we need twoÔºå four
    loops here„ÄÇ So let's say the first one for underscore because we don't need this
    in range self dot and its„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ü§¢„ÄÇ![](img/47427fb8775e23c1bec39e64630a9c33_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_8.png)'
  prefs: []
  type: TYPE_IMG
- en: So this is the number of iterations we defined„ÄÇ and then our second loop„ÄÇ And
    for this„ÄÇ I used the enumererate methodÔºå so I can say four„ÄÇIndex and also XÔºå Y
    in„ÄÇEn nuerate X„ÄÇ So I want to iterate over the draining„ÄÇLmb training samples and
    the enumererate function will give me the index and then also the current sample„ÄÇSo
    these are our two loopsÔºå and now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Sorry„ÄÇLet's apply„ÄÇThis update rule„ÄÇ So let'sÔºå again„ÄÇ we have to calculate the
    predicted value and then apply the update„ÄÇ So let's say the linear output equals
    nuy dot of„ÄÇ![](img/47427fb8775e23c1bec39e64630a9c33_12.png)
  prefs: []
  type: TYPE_NORMAL
- en: The current sample„ÄÇAnd ourselvesÔºå that awaits„ÄÇPlusÔºå the self of bias„ÄÇThen we
    apply the activation function and get the predicted value„ÄÇ So why predict equals
    self dot activation function of the linear output„ÄÇ So here we can see that in
    this caseÔºå we use it for only one sample„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here down in the predict methodÔºå we use the activation function for multiple
    samples„ÄÇ And that's why we need this nuy where function here in our activation
    function„ÄÇSo yeah„ÄÇ and now let's continue„ÄÇ So now we have our predicted y„ÄÇ and
    now let's have a look at the formula again„ÄÇ So we have the learning rate times
    the difference„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_14.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_15.png)'
  prefs: []
  type: TYPE_IMG
- en: And thenÔºå times xÔºå so„ÄÇLet's call this update equals self dot learning rate times„ÄÇ
    And here we have the actual„ÄÇLabel So y underscore of this current index minus
    y predict that„ÄÇAnd then„ÄÇWe say self„ÄÇDot weights plus equals update times XÔºå I„ÄÇAnd
    self dot bias plus equals update times 1„ÄÇ So we don't need this times 1„ÄÇAnd now
    we are done„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the whole implementation of the perceptron„ÄÇAnd now let's test this„ÄÇ
    So here I've written„ÄÇ I've already written a little test script hereÔºå so I importpart
    the perceptron here„ÄÇAnd then I will create two„ÄÇTwo blops„ÄÇ so I can use this from
    the SK learn module„ÄÇ I can use a function that is called make blops„ÄÇ So this will
    create two classes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then I will split our data into training samples and test samples and training„ÄÇTraining
    labels and test labels„ÄÇ Then I will create a perceptron„ÄÇ I will fit the training
    data„ÄÇ and then I will predict the test labels„ÄÇAnd then I will calculate the accuracy„ÄÇ
    and I will also plot this„ÄÇ So let's run this„ÄÇAnd I hope I didn't forget anything„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now here is the plot„ÄÇ So here we see our two plotlops„ÄÇ And here we have our
    linear decision boundary„ÄÇ So this is the„ÄÇDecision function at our perceptron generated„ÄÇAnd
    we see that it separates our two classes perfect here„ÄÇ SoÔºå and we also see that
    our accuracy is one„ÄÇ So it's perfect in this case„ÄÇAnd yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we see that the perceptron works„ÄÇ and one thing that we should be aware of is
    that the perceptron only works for linearly separable classes„ÄÇ So if classes can
    be separated with a linear functionÔºå like here in our case„ÄÇ let's have a look
    at this again„ÄÇThen it works very wellÔºå but otherwise not so much„ÄÇAnd for further
    improvements means we can try out different activationation functions„ÄÇ For example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the sigmoid function and then apply a gradientÔºå decent method rather than the
    perception rule in order to update our weights„ÄÇBut yeahÔºå for nowÔºå that should
    be all I wanted to show you„ÄÇ And I hope you enjoyed this tutorial and see you
    next timeÔºå bye„ÄÇ![](img/47427fb8775e23c1bec39e64630a9c33_17.png)
  prefs: []
  type: TYPE_NORMAL
