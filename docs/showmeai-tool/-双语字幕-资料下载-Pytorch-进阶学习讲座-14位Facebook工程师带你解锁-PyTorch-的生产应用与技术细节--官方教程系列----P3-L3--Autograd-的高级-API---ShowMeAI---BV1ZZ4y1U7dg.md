# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPytorch ËøõÈò∂Â≠¶‰π†ËÆ≤Â∫ßÔºÅ14‰ΩçFacebookÂ∑•Á®ãÂ∏àÂ∏¶‰Ω†Ëß£ÈîÅ PyTorch ÁöÑÁîü‰∫ßÂ∫îÁî®‰∏éÊäÄÊúØÁªÜËäÇ ÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P3ÔºöL3- Autograd ÁöÑÈ´òÁ∫ß API - ShowMeAI - BV1ZZ4y1U7dg

üéº„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_1.png)

Hi everyoneÔºå my name is AlbanÔºå I'm a research engineer here at Facebook and I'm working on the frontend team for Pytorch in New York„ÄÇMore particularly on the Autograd sub system„ÄÇ and so today I'm going to talk to you about the brand new high level API that we added for the Autograd„ÄÇ

In particularÔºå I'm going to give you a quick overview of like the motivations for this new API„ÄÇ what it actually looks like and how to use it and finally some future works for what's going to come for it„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_3.png)

So to begin with the motivation for it is that the current autograd API we have is not for everyone„ÄÇIn particularÔºå the dot backward function is really geared toward Torch„ÄÇt andN users as it populates the dot grad fields of all the tensors and it's really associated with the states and the optimizes that we use with StN„ÄÇSimilarlyÔºå autogradgrad is geared toward neural network user in the sense that it is built to do back propagation and all the namings and all the API design has been done„ÄÇ

Thinking about neural networks„ÄÇAnd so we think that now nowadays that we have more general users that use Autograd to do more optimization than just neural network„ÄÇWe needed a good generic API that these people can use to actually do everything they need„ÄÇMoreover„ÄÇ

 for higher level functions such as computing a Jacobian that I'm going to use here is like an example for many things„ÄÇIt's very nice to have a reference implementation within Pythtorch„ÄÇThe reason for that is up to fairly recently if you wanted to compute the Jacobn with Spytorch„ÄÇ you had to copy paste some code from a guest that was made few years ago„ÄÇ

And so people have basically a stale version of that gÔºå potentially in their code„ÄÇ And we can't really make any improvement to the way„ÄÇJacoian are computed for everyone to basically get automatically this new upgraded version„ÄÇAnd so this new API is going to help us bring more performance improvements to the users via this generic API„ÄÇ

And„ÄÇFollowing this ideaÔºå we also added recently a whole benchmark system around this API to make sure that existing models and models that are used very often by our users„ÄÇHave good performanceÔºå and we can also measure improvements we're making by different change„ÄÇ

 but also we can measure regressions and make sure they are not significant and we can catch them and fix them before they hit main releases„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_5.png)

So where does this new API liveÔºüSo it lives under the torch dot autogra dot functional symbolial„ÄÇYou can find it in the autograd documentation under like the functional higher level API name„ÄÇAnd it has a slightly different spirit comparing to the existing APIs„ÄÇ mainly that it takes function as inputs„ÄÇAnd not the result of the forwardd pass„ÄÇ

The main reason for that is our twofoldÔºå one is because it's closer to the mathematical formulation and people are getting used to differentiate functions directly„ÄÇ not do the forward paths and then as for the gradient„ÄÇSecond„ÄÇ it also allows us to get more freedom respect to what happens during the forward pass„ÄÇAnd in particularÔºå for some of the optimization we have planned„ÄÇ

 we will need to do some special things during the forward pass that we don't want the user to have to worry about„ÄÇ and so this new pay is going to allow us to do that very efficiently„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_7.png)

![](img/ae640e7b46aaf17bba8e02172f49117e_8.png)

So what does this new API contain The first big part is first order gradients„ÄÇSo as I mentioned beforeÔºå we have a function that directly computed Jacobian for you„ÄÇ given a function and some input points„ÄÇWe also have a vector Jacobian product which corresponds to backward mode automatic differentiation„ÄÇ and in the neural network world it corresponds to the backward propagation algorithm„ÄÇ

So this is actually very close to the existing autograd dot grad function„ÄÇAnd finally„ÄÇ we also provide a Jacobin vector product„ÄÇ So the other way around„ÄÇ and this one corresponds more to forward mode automatic differentiation„ÄÇAnd can be used to compute directional derivatives„ÄÇ

And we also have the second product of this API corresponds to second order gradient and very similar functions so„ÄÇA hassian function that computes all the second order derivatives„ÄÇA vector Hessian product which allows us to efficiently compute with backward mode automatic differentiation„ÄÇ a product between the Hesian and a given vector„ÄÇ

![](img/ae640e7b46aaf17bba8e02172f49117e_10.png)

And the similarlyÔºå the other way around the hesian vector product to that corresponds more to foreign mode automatic differentiation„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_12.png)

So now an example on how to use DC APII„ÄÇSo againÔºå as an example for the Jacobian function„ÄÇ now you don't have to copy past code from that old guestÔºå you can just import it from torch„ÄÇAnd as you can see in the first exampleÔºå where you have some input„ÄÇ you just call jackhoion on it and you get the value of the jackbion and that's easy„ÄÇ

 that's everything you need to do„ÄÇWhat is very nice as well is that you can compose this new API with the existing autoquad API and here„ÄÇ for exampleÔºå if you input requires gradientsÔºå you can ask the Jacobian computation to create graph and so to be able to back through it and as you can see you can then compute norm of the Jacob you just got and backward through that„ÄÇ



![](img/ae640e7b46aaf17bba8e02172f49117e_14.png)

And then you can compose that with the rest of your training to get all the quantities that you need„ÄÇThere are many more examples and„ÄÇA few examples here are like gradient penalties based on Jacobn computations„ÄÇJacoian vector product computations that corresponds to the forward mode automatic differentiation„ÄÇThis is especially interesting because they actually compute directional derivatives in higher dimension„ÄÇ

These derivatives are very useful for quite a few optimization algorithms„ÄÇAnd similarly„ÄÇ the second order methods that compute Hessian or Hesian vector products allow you to do Newton step methods or approximate Newton step methods much more efficiently and much in a much easier way than with the current Auto RE API„ÄÇ

So to finish with some future work„ÄÇThe first part is actually things that we are already currently working on„ÄÇSo the first one is for node 8Ôºå so the idea would be for both Jacob vector and Hesian vector products„ÄÇTo replace that with an actual for node automatic differentiation„ÄÇ So this is work in progress„ÄÇ hopefully we'll be able to release that very soon„ÄÇ

And get very good performance improvements based on that„ÄÇYou so one interesting thing about this new API is that you will get the benefits of this new forward mode ID for free if you already use that API when it is released„ÄÇSimilarlyÔºå you can get badge gradients„ÄÇSo this is a collaboration with the VMap features that you heard about„ÄÇ and the idea here will be to speed up computation for Jacobian and Hesian computations„ÄÇ

And finally torch„ÄÇnN composibility is another topic we're working on„ÄÇ The reason for that is currently NN modules hold a lot of states and they are not functional so they don't work very out of the box with the new API that we design„ÄÇ So the idea here is going to be to work on Torch end to try and provide a functional version of the NN modules so that we can work with them more efficiently„ÄÇAnd for longer time workÔºå we are looking for ideas and please do share your feedback on how this works„ÄÇ

 how it helps you do what you want to do„ÄÇDo open issues on GiHub or on the Pythtorch forum if you have questions or concerns about what is happening here„ÄÇ and please help us make DC API exactly what you're looking for„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_16.png)

üéºSo thank you very much for listening and have a good day„ÄÇ![](img/ae640e7b46aaf17bba8e02172f49117e_18.png)