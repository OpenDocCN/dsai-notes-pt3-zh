- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P28ï¼šL4.5- ä½¿ç”¨ TensorFlow è¿›è¡Œå­¦ä¹ ç‡è°ƒåº¦ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠ NLP ä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P28ï¼šL4.5- ä½¿ç”¨ TensorFlow è¿›è¡Œå­¦ä¹ ç‡è°ƒåº¦ - ShowMeAI - BV1Jm4y1X7UL
- en: In our other videosï¼Œ we talked about the basics of fine tuning a language model
    with Tensorflowã€‚ And as alwaysï¼Œ when I refer to videosï¼Œ I'll link them belowã€‚
    But stillï¼Œ can we do betterã€‚ So here's the code from our model fine tuning videoã€‚
    And whileï¼Œ while it worksã€‚ we could definitely tweak a couple of thingsã€‚ by farã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å…¶ä»–çš„è§†é¢‘ä¸­ï¼Œæˆ‘ä»¬è°ˆåˆ°äº†å¦‚ä½•ä½¿ç”¨ Tensorflow å¾®è°ƒè¯­è¨€æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ã€‚æ¯å½“æˆ‘æåˆ°è§†é¢‘æ—¶ï¼Œæˆ‘ä¼šåœ¨ä¸‹é¢é“¾æ¥å®ƒä»¬ã€‚ä½†ä»ç„¶ï¼Œæˆ‘ä»¬èƒ½åšå¾—æ›´å¥½å—ï¼Ÿæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬æ¨¡å‹å¾®è°ƒè§†é¢‘ä¸­çš„ä»£ç ã€‚è™½ç„¶å®ƒèƒ½å·¥ä½œï¼Œä½†æˆ‘ä»¬ç»å¯¹å¯ä»¥è°ƒæ•´å‡ ä¸ªåœ°æ–¹ã€‚
- en: the most important thing is the learning w In this videoï¼Œ we'll talk about how
    to change itã€‚ which will make your training much more consistently successfulã€‚ğŸ˜Šï¼ŒIn
    factã€‚ there are two things we want to change about the default learning rate for
    atã€‚ The first is that it's way too high for our modelsã€‚ So by defaultã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€é‡è¦çš„æ˜¯å­¦ä¹ ç‡ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•æ”¹å˜å®ƒï¼Œè¿™å°†ä½¿ä½ çš„è®­ç»ƒæ›´åŠ ä¸€è‡´æˆåŠŸã€‚ğŸ˜Šå®é™…ä¸Šï¼Œæˆ‘ä»¬æƒ³æ”¹å˜é»˜è®¤å­¦ä¹ ç‡çš„ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œå¯¹äºæˆ‘ä»¬çš„æ¨¡å‹æ¥è¯´ï¼Œå®ƒçš„å€¼å¤ªé«˜äº†ã€‚å› æ­¤é»˜è®¤æƒ…å†µä¸‹ã€‚
- en: Adam uses a learning rate of 10 to the -3ï¼Œ which is very high for training transformersã€‚
    We're going to start at 5 by 10 to the-5ï¼Œ which is 20 times lower than the defaultã€‚
    And secondlyã€‚ we don't just want a constant learning rateã€‚ We can get even better
    performance if we decay the learning rate down to a tiny valueã€‚ or even to 0 over
    the course of trainingã€‚ So that's what this polynomial decay schedule thing is
    doingã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Adam ä½¿ç”¨çš„å­¦ä¹ ç‡æ˜¯ 10 çš„ -3 æ¬¡æ–¹ï¼Œå¯¹äºè®­ç»ƒå˜æ¢å™¨æ¥è¯´éå¸¸é«˜ã€‚æˆ‘ä»¬å°†ä» 5 ä¹˜ä»¥ 10 çš„ -5 æ¬¡æ–¹å¼€å§‹ï¼Œæ¯”é»˜è®¤å€¼ä½ 20 å€ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ä»…ä»…æƒ³è¦ä¸€ä¸ªæ’å®šçš„å­¦ä¹ ç‡ã€‚å¦‚æœæˆ‘ä»¬å°†å­¦ä¹ ç‡é€æ¸é™ä½åˆ°ä¸€ä¸ªå¾ˆå°çš„å€¼ï¼Œç”šè‡³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é™åˆ°
    0ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æ‰€ä»¥è¿™å°±æ˜¯è¿™ä¸ªå¤šé¡¹å¼è¡°å‡è°ƒåº¦çš„ä½œç”¨ã€‚
- en: So that name might be intimidatingï¼Œ especially if you only vaguely remember
    what a polynomial is from Atslasã€‚ So I'll show you what that decay looks like
    in a secondã€‚ But first we need to tell the scheduler how long training is going
    to beã€‚ So that it decays of the right speedï¼Œ And that's what this code here is
    doingã€‚ğŸ˜Šã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªåç§°å¯èƒ½è®©äººæœ›è€Œç”Ÿç•ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ åªæ¨¡ç³Šåœ°è®°å¾—æ¥è‡ª Atslas çš„å¤šé¡¹å¼æ˜¯ä»€ä¹ˆæ ·å­ã€‚é‚£ä¹ˆæˆ‘ä¼šåœ¨ç¨åç»™ä½ å±•ç¤ºè¿™ä¸ªè¡°å‡çš„æ ·å­ã€‚ä½†é¦–å…ˆæˆ‘ä»¬éœ€è¦å‘Šè¯‰è°ƒåº¦å™¨è®­ç»ƒå°†æŒç»­å¤šé•¿æ—¶é—´ï¼Œä»¥ä¾¿å®ƒä»¥æ­£ç¡®çš„é€Ÿåº¦è¡°å‡ï¼Œè¿™å°±æ˜¯è¿™æ®µä»£ç çš„ä½œç”¨ã€‚ğŸ˜Š
- en: So we're computing how many mini batches the model is going to see over the
    entire training run and to compute thatã€‚ we're taking the size of the training
    setï¼Œ dividing it by the batch size which gives us the number of batches per epochã€‚
    and then we're multiplying that by the number of epochs to get a total number
    of batchesã€‚ it's going to see over the whole training runã€‚ğŸ˜Šï¼ŒSo once we know how
    many batchesã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å°†çœ‹åˆ°å¤šå°‘ä¸ªå°æ‰¹æ¬¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å–è®­ç»ƒé›†çš„å¤§å°ï¼Œé™¤ä»¥æ‰¹æ¬¡å¤§å°ï¼Œå¾—åˆ°æ¯ä¸ª epoch çš„æ‰¹æ¬¡æ•°ï¼Œç„¶åå°†å…¶ä¹˜ä»¥ epoch
    æ•°ï¼Œä»¥è·å¾—æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€»æ‰¹æ¬¡æ•°ã€‚ğŸ˜Šæ‰€ä»¥ä¸€æ—¦æˆ‘ä»¬çŸ¥é“äº†å¤šå°‘æ‰¹æ¬¡ã€‚
- en: how many training steps we're takingï¼Œ we just pass all of that information to
    the scheduler and we're ready to goã€‚So what does the polynomial decay schedule
    look likeï¼ŸWith default optionsã€‚ it's actually just a linear scheduleï¼Œ so it looks
    like thisã€‚ It starts at our initial valueã€‚ which is 5 by 10 to the minus5 or 5
    e minus5 and then it decays down at a constant rate until it hits zero right at
    the very end of training So why do they call it polynomial and not linear Well
    if you tweak the options you can get a higher order at a truly polynomial decay
    schedule but there's no need to do that right now by defaultã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦è¿›è¡Œçš„è®­ç»ƒæ­¥éª¤æ•°é‡ï¼Œæˆ‘ä»¬åªéœ€å°†æ‰€æœ‰è¿™äº›ä¿¡æ¯ä¼ é€’ç»™è°ƒåº¦å™¨ï¼Œå°±å‡†å¤‡å¥½äº†ã€‚é‚£ä¹ˆå¤šé¡¹å¼è¡°å‡è°ƒåº¦çœ‹èµ·æ¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿä½¿ç”¨é»˜è®¤é€‰é¡¹æ—¶ï¼Œå®ƒå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªçº¿æ€§è°ƒåº¦ï¼Œå› æ­¤çœ‹èµ·æ¥åƒè¿™æ ·ã€‚å®ƒä»æˆ‘ä»¬çš„åˆå§‹å€¼å¼€å§‹ï¼Œå³
    5 ä¹˜ä»¥ 10 çš„ -5 æ¬¡æ–¹ï¼Œæˆ–è€… 5 e -5ï¼Œç„¶åä»¥æ’å®šé€Ÿç‡è¡°å‡ï¼Œç›´åˆ°åœ¨è®­ç»ƒç»“æŸæ—¶è¾¾åˆ° 0ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆå«å®ƒå¤šé¡¹å¼è€Œä¸æ˜¯çº¿æ€§å‘¢ï¼Ÿå¦‚æœä½ è°ƒæ•´é€‰é¡¹ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªæ›´é«˜é˜¶çš„çœŸæ­£å¤šé¡¹å¼è¡°å‡è°ƒåº¦ï¼Œä½†ç°åœ¨æ²¡æœ‰å¿…è¦è¿™æ ·åšï¼Œé»˜è®¤æƒ…å†µä¸‹ã€‚
- en: you get a linear schedule and if you were aware that a linear function is a
    special case of a polynomial you can feel proudã€‚ğŸ˜Šï¼ŒSo that asideï¼Œ how do we actually
    use the schedulerã€‚ So easilyï¼Œ we just pass it to Adamã€‚ You'll notice the first
    time when we compiled the modelï¼Œ we just pass the string atã€‚ Curris recognizes
    the names of common optimizers and loss functions if you pass them with strings
    So it saves time and it avoids imports to do it that wayã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šå¾—åˆ°ä¸€ä¸ªçº¿æ€§è°ƒåº¦ï¼Œå¦‚æœä½ çŸ¥é“çº¿æ€§å‡½æ•°æ˜¯å¤šé¡¹å¼çš„ç‰¹æ®Šæƒ…å†µï¼Œä½ å¯ä»¥æ„Ÿåˆ°éª„å‚²ã€‚ğŸ˜Šé‚£ä¹ˆï¼Œæ’‡å¼€è¿™äº›ï¼Œæˆ‘ä»¬å¦‚ä½•å®é™…ä½¿ç”¨è°ƒåº¦å™¨å‘¢ï¼Ÿæˆ‘ä»¬åªéœ€å°†å…¶ä¼ é€’ç»™ Adamã€‚ä½ ä¼šæ³¨æ„åˆ°ç¬¬ä¸€æ¬¡æˆ‘ä»¬ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä»…ä»…ä¼ é€’äº†å­—ç¬¦ä¸²ã€‚Curris
    ä¼šè¯†åˆ«å¸¸è§ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°çš„åç§°ï¼Œæ‰€ä»¥ç”¨å­—ç¬¦ä¸²ä¼ é€’å¯ä»¥èŠ‚çœæ—¶é—´ï¼Œé¿å…å¯¼å…¥ã€‚
- en: if you only want the default settingsã€‚ but we're professional machine learners
    now with our very own learning rate schedule So we have to do things properlyã€‚
    So the first thing we do is we import the optimizerã€‚ Then we initialize it with
    our scheduler in the learning rate argumentã€‚ and then we compile the model using
    our new optimizer and whatever loss function you wantã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åªæƒ³è¦é»˜è®¤è®¾ç½®ã€‚ä½†æˆ‘ä»¬ç°åœ¨æ˜¯ä¸“ä¸šçš„æœºå™¨å­¦ä¹ è€…ï¼Œæ‹¥æœ‰è‡ªå·±çš„å­¦ä¹ ç‡è°ƒåº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»åšå¥½äº‹æƒ…ã€‚æˆ‘ä»¬åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å¯¼å…¥ä¼˜åŒ–å™¨ã€‚ç„¶åç”¨å­¦ä¹ ç‡å‚æ•°åˆå§‹åŒ–å®ƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨å’Œä½ æƒ³è¦çš„ä»»ä½•æŸå¤±å‡½æ•°ç¼–è¯‘æ¨¡å‹ã€‚
- en: we'll leave that unchangedã€‚ This will be sparse categorical cross entropyã€‚ if
    you're following long from the fine tuning videoã€‚ but it can be anything else
    that you're using yourselfã€‚ğŸ˜Šã€‚So now we have a high performance model ready to
    goã€‚ all that remains is to fit the model just like we did beforeã€‚And remember
    because we've compiled the model with the new optimizer at the new learning rateã€‚
    we actually don't need to change anything about the fit I call at allã€‚ we just
    call fit here exactly the same command we used before if you've seen in other
    videosã€‚ğŸ˜Šã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¿æŒä¸å˜ã€‚è¿™å°†æ˜¯ç¨€ç–åˆ†ç±»äº¤å‰ç†µã€‚å¦‚æœä½ åœ¨è§‚çœ‹å¾®è°ƒè§†é¢‘æ—¶é•¿æ—¶é—´è·Ÿéšã€‚ä½†å®ƒå¯ä»¥æ˜¯ä½ è‡ªå·±ä½¿ç”¨çš„ä»»ä½•ä¸œè¥¿ã€‚ğŸ˜Šã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡†å¤‡å¥½çš„é«˜æ€§èƒ½æ¨¡å‹ã€‚å‰©ä¸‹çš„å°±æ˜¯åƒä¹‹å‰ä¸€æ ·æ‹Ÿåˆæ¨¡å‹ã€‚è¯·è®°ä½ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ç”¨æ–°çš„ä¼˜åŒ–å™¨å’Œæ–°çš„å­¦ä¹ ç‡ç¼–è¯‘äº†æ¨¡å‹ã€‚æˆ‘ä»¬å®é™…ä¸Šä¸éœ€è¦å¯¹è°ƒç”¨çš„æ‹Ÿåˆåšä»»ä½•æ›´æ”¹ã€‚æˆ‘ä»¬åªéœ€åœ¨è¿™é‡Œè°ƒç”¨ä¸ä¹‹å‰è§†é¢‘ä¸­ä½¿ç”¨çš„å®Œå…¨ç›¸åŒçš„å‘½ä»¤ã€‚ğŸ˜Šã€‚
- en: But now we get a beautiful training with a niceï¼Œ smoothã€‚ a good initial learning
    rate and a solid learning rate decayingã€‚And you will get much better performance
    as a resultã€‚![](img/23f9dd7967986f28f1dcc38c80344a1b_1.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªç¾ä¸½çš„è®­ç»ƒï¼Œé¡ºç•…è€Œè‰¯å¥½ã€‚ä¸€ä¸ªå¥½çš„åˆå§‹å­¦ä¹ ç‡å’Œç¨³å®šçš„å­¦ä¹ ç‡è¡°å‡ã€‚å› æ­¤ä½ å°†è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚![](img/23f9dd7967986f28f1dcc38c80344a1b_1.png)
- en: ã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/23f9dd7967986f28f1dcc38c80344a1b_3.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23f9dd7967986f28f1dcc38c80344a1b_3.png)'
