# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P13ï¼šL2.6- åŸºäºè¯çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL

Yesã€‚Let's take a look at word based tokenizationã€‚World based organization is the idea of splitting the raw text into words by splitting on spaces or other specific rulesã€‚Like punctuationã€‚In this algorithmï¼Œ each word has a specific number or ID attributed to itã€‚

Here lets has VI 250ï¼Œ Du has 861 and tokenization followed by an exclamation mark has a 345ã€‚ğŸ˜Šã€‚This approach is interestingï¼Œ as the model has representations that are based on entire wordsã€‚The information held in a single number is highï¼Œ as a word contains a lot of contextual and semantic informationã€‚Howeverï¼Œ this approach does have its limitsã€‚ğŸ˜Šï¼ŒFor exampleã€‚

 the word dog and the word dogs are very similarï¼Œ and their meaning is closeã€‚The word based tokenization hallï¼Œ howeverï¼Œ will attribute entirely different ideas to these two wordsã€‚ and the model will therefore learn two different embeddings for these two wordsã€‚This is unfortunateã€‚ as we would like the model to understand that these words are indeed relatedã€‚

And that dogs is simply the plural form of the word dogã€‚Another issue with this approach is that there are a lot of different words in an languageã€‚If we want our model to understand all possible sentences in that languageã€‚ then we will need to have an ID for each different wordã€‚And the total number of wordsã€‚

 which is also known as the vocabulary sizeï¼Œ can quickly become very largeã€‚This is an issue because each ID is mapped to a large vector that represents the words meanã€‚And keeping track of these mappings requires an enormous number of weightsã€‚When the vocabulary size is very largeã€‚ğŸ˜Šï¼ŒIf we want our models to stay leanã€‚

 we can opt for our tokenizer to ignore certain words that we don't necessarily needã€‚For exampleã€‚ hereï¼Œ when training our tokenizer in textï¼Œ we might want to take only the 10ã€‚000 most frequent words in that textï¼Œ rather than taking all words from in that text or all language's wordsã€‚To create our basic vocabularyã€‚The tokenizer will know how to convert those 10ã€‚

000 words into numbersï¼Œ but any other word will be converted to the out of vocabulary wordã€‚ or like shown hereï¼Œ the unknown wordã€‚Unfortunatelyï¼Œ this is a compromiseã€‚ the model will have the exact same representation for all words that it doesn't knowã€‚ğŸ˜Šã€‚Which can result in a lot of lost information if many unknown words are presentã€‚



![](img/2d2fd0eb1726791600f98ab3dc5465fa_1.png)

![](img/2d2fd0eb1726791600f98ab3dc5465fa_2.png)

Yeahã€‚