# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂÆòÊñπÊïôÁ®ãÊù•Âï¶ÔºÅ5‰Ωç Hugging Face Â∑•Á®ãÂ∏àÂ∏¶‰Ω†‰∫ÜËß£ Transformers ÂéüÁêÜÁªÜËäÇÂèäNLP‰ªªÂä°Â∫îÁî®ÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P33ÔºöL6.1- SylvainÁöÑÂú®Á∫øÁõ¥Êí≠ËÆ≤Ëß£ - ShowMeAI - BV1Jm4y1X7UL

But yeah I think it's time to begin so welcome to thefi course and we're going to let you the chapter 1 together and this is really the place to ask any questions you might have so please do so in the chat I'm going to make some booth every five to 10 minutes and go through the chats read the question and answer them to the best of my knowledge„ÄÇ

So the urging for his course„ÄÇAnd this session will go over chapter1 as I said„ÄÇ and this chapter is meant as a very general introduction to what transformer models can do so you don't need to worry about setting up be just yet a few good samples will to see how you can run them in coab directly„ÄÇ

Or the Greenface website„ÄÇI'm not going to watch review the introduction video because you can do that on your own time and there's not really anything informative in it„ÄÇ and so the goal of the first section of the courseÔºå which is probably right now„ÄÇ which is this section in pink is to„ÄÇIntroduce you to transform models the web Strer went today and then teach you how to download the pretrain model from the Eb„ÄÇ find unit on your own data for text classification tasks„ÄÇ

And thenluute the result back to the model heb„ÄÇAnd then see section 2 will dive a little bit deeper and consider all NLP tag can adjust text classification and when the last part of the course is going to dive even a little bit deeper„ÄÇ so part two should be released in the fall and part three should be released at the beginning of next year„ÄÇ

And so once you've finished this part of the course„ÄÇ you should be able to download the between model Once you it on your own problem„ÄÇ and then„ÄÇCreate either a smaller uploadlet the result back to the hub„ÄÇ And so you should„ÄÇLook at the forum„ÄÇWhich I'm gonna„ÄÇShow you dressed here at discuss the phase„ÄÇ

go which is where you will be able to ask any question you have after this live stream in the course category there is one to big per chapter for all your questions and then there is also one to pick I'm going to follow very closely which is„ÄÇSo chair your projects topic on which you should definitely share anything you built after following the first section of the course„ÄÇ

ÂóØ„ÄÇAlrighty„ÄÇSo let's dive in case I don't see any general questions for now with„ÄÇThe beginning of chapter1„ÄÇSo„ÄÇTransformer models are all about doing NLP task andLP stands for natural language processing„ÄÇ and it's a field that's related to everything language„ÄÇSo the the goal of NLP task is either to classify some text„ÄÇ

 so for instance getting the sentiment of a reviewÔºå detecting if an email is spam„ÄÇ detecting if a comment someone posted online is rather nice or rather not nice if a sentence is grammatically correct„ÄÇ etcteraÔºå etcter„ÄÇOur task could be classifying edge word in the textÔºå so for instance„ÄÇ being able to pass the gramatic component to be able to see if that word is a person or a location or an organization„ÄÇ

Another type of NLP task is generated text content„ÄÇ so completing a prompt which is what Jo smartF does when you're trying to compose a message„ÄÇ usually it suggests using X and even Gmail does that now on email„ÄÇÂóØ„ÄÇFillling the B syn text„ÄÇ which is another kind of text generation„ÄÇAnother kind of task is to extract an answer from a text„ÄÇ

 so given a very long text and then a question being able a model„ÄÇ being able to extract the answer to that model„ÄÇThe answer sort of that question„ÄÇ sorry from the context or generating these new sentences from an input text„ÄÇ So this is for instanceÔºå generating translation of a text summarizing a text I saw not over recently on making a new text in and of style„ÄÇ

 for instanceÔºå a casual style or more„ÄÇÂóØ„ÄÇForal style„ÄÇSo those are all the kind of enLP testque tackle in the courseÔºå especially in section2„ÄÇAnd so it's rather challenging because a computer doesn't process information the same way as we do„ÄÇ And so a new transformer model let you deep learning to„ÄÇ

Are able to from like a few from a few samples that you have label for are able to generalize properly without you having to to create„ÄÇRs for instanceÔºå so before deep learning was all the rage„ÄÇ one thing would be to pass the text and have some special rules if I see this world maybe it means its's positive if I see this world maybe it means it's negative etctera right„ÄÇ so that's not what's done now this is mostly using transformable models with a again face library„ÄÇ

And those modelsÔºå you shouldÔºå if you have not yet you should follow an introduction to a deep learning course„ÄÇ those model are usually trained and they don't follow a set of given role that has been written by your human„ÄÇ they have weightsÔºå which follow an algorithm called gradient descent and following the training data that are fed they make valueless function or ametric a little bit better at each step and so we get at the end a model that is kind of a black box„ÄÇBut we can use and generalize fairly well on dataÔºå but looks like the data on the training set„ÄÇ

So let's look at what this model can do in practice„ÄÇ so there are two ways we'll be able to do that the first way is by clicking on this open in color button„ÄÇSo Collab is a platform maintained by Google which provides free access to resources like GPUs or TUs in a Jupy down the book environment„ÄÇ

![](img/1b5e5fce582c60e42117fe3a72e59e6a_1.png)

SoÔºå you can execute„ÄÇGood directly„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_3.png)

Inside it„ÄÇAnd so the first cell needs to be executed to install all the library I'm going to let this run where we go through a little bit of the course and then we'll be able to run the other good samples I'm going to come back to that window a little bit later So transformers models are everywhere they are used by a lot of companies now those are examples of company that are using the Gface model app to have to share as models we have pretrained and probably to use them internally as well„ÄÇ

And the Transformless library is the main interface library that provide access to those model„ÄÇAnd the bottle up„ÄÇWhich we're going to have a quick look„ÄÇAnd a little bit later„ÄÇ the model app is where all post Sptrain model as taught is part of the evening phase ecosystem and we will be able to run the same code as wellll try in the Coab notebook on the that model app a little bit later„ÄÇThe thing you will need to do to access the forum and to be able to play around with all the models is create give a second following the link here on the 6 of the chapter 1„ÄÇ

And so we'll have a look first at the higher higher level API object„ÄÇ which is called the pipeline plan function„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_5.png)

I'm going just give a littleelps few minutes„ÄÇJust going to give a few minutes for the video to both„ÄÇFor the video to load itself a little bit and take the time to answer any questions that you have let miss horse through the chat„ÄÇI soÔºå there were a few„ÄÇSo the first question I see is there still a place for LSTM models approaches like OMF„ÄÇ if so where do they outperform transformer models or at least should be considered that's a very very good questions„ÄÇ

 very good question right and there is definitely still a place for LSTM model„ÄÇThe main reason transformer models are heavily used right now is that the computation is more efficient especially on hardwarePUs and TUs„ÄÇ because LSTMs rely on a recurrent mechanism and that recurt mechanism is a little bit harder to optimize but for instance when in fit as a very good state of the art results on the EMDB data set„ÄÇ which is classifying movie reviews and it's been only very recently that it was performedform that transformers and that task in which I think is linked to the fact that the EMDB reviews very long and transformer models are good with input that go up to the sequence like we can manage„ÄÇ

 usually it's 512 but for 12 to sorryÔºå but yeah if you have things that are longer than that it looks like LSTM models can see„ÄÇIt'll be a very good fit„ÄÇAnother questionÔºå I'm going to answer it just after this video because it's about the pipeline„ÄÇSo yeahÔºå let's watch the videoÔºå I'm going to disappear from the screen so you can watch with me being in the way and I'll come back after the video„ÄÇ

![](img/1b5e5fce582c60e42117fe3a72e59e6a_7.png)

„ÄÇIt's pipe end function„ÄÇThe pipeline function is the most I level API of the Transformers library„ÄÇIt groupgroup together all the steps to go from moretex to usable predictions„ÄÇThe model used is at the core a pipelineÔºå but the pipeline also include all the necessary prepoing since the model does not expect texts but number„ÄÇAs well as some post processing to make the output of the model human readable„ÄÇ

Let's look at the first phase on Paul with the sentiment analysis pipeline„ÄÇThis pipeline performs text classification on a given input and determines if it's positive or negative„ÄÇHere it attributed the positive label on the given textÔºå with a confidence of 95%„ÄÇYou can pass multiple texts to the same pipelineÔºå which will be processed and passed through the model together as a batch„ÄÇ

The output is a list of individual results in the same order as the input text„ÄÇHere we find the same label on score for the first text„ÄÇ and the second text is church negative with a confidence of 99„ÄÇ9%„ÄÇThe zero shot classification pipeline is a more general text classification pipeline„ÄÇ

It allows you to provide the labels you want„ÄÇHere we want to classify our input text along the labels education„ÄÇ politicsÔºå and business„ÄÇThe pipeline successfully recognizes it's more about education than the yoga labels„ÄÇWith a confidence of 84%„ÄÇMoving on to our tasksÔºå the text generation pipeline will to complete a given prompt„ÄÇThe output is generated with a bit of randomnessÔºå so it changes each time you call the generator object on a given prompt„ÄÇ

Up until nowÔºå we've used the Biman APIpi with a default model associated to each task„ÄÇBut you can use it with any model that has been freetrained or fine tune on this task„ÄÇOh„ÄÇGoing on the model HÔºå again F dogo slash models„ÄÇYou can filter the available models by task„ÄÇThe defaultform adults used in our previous example was GPT2„ÄÇ

 but were many more models available and not just in English„ÄÇLet's go back to the next generation pipeline and load it with another modelÔºå distill JPT2„ÄÇThis is a lighter version of GT2 created by the Eingface team„ÄÇWhen applying the pipeline to a given promptÔºå we can specify several arguments„ÄÇ

Such as the maximum length of the generated textÔºå all the number of sentences we want to return since there is some randomness in the generation„ÄÇGenerating text by guess the X12 in is intenseÔºå why the pertraining objective of GPT2„ÄÇThe field mass pipeline is a pering objective of BroÔºå which is to guess the value of mass qua„ÄÇIn this caseÔºå we ask the two most likely values for the missing words according to the model„ÄÇ

 and get mathematical or computational as possible answers„ÄÇAnd a task transform most model than form is to classify each word in a sentence instead of the sentence as a wall„ÄÇOne example of this is named entity recognition„ÄÇWhich is the task of identifying entities such as persons„ÄÇ organizations or locations in a sentence„ÄÇHereÔºå the model correctly finds the personÔºå sva„ÄÇ

 the organizationÔºå GfaceÔºå as well as the location Brooklyn inside the input text„ÄÇThe group entities equal2 argument use is to make the pipeline group together the different walls linked to the same entity„ÄÇ such as eggging and face here„ÄÇnovel task available with the byg API is extractive question and sorry„ÄÇProviding a context and a questionÔºå the model will identify the span of text in the context containing the answer to the question„ÄÇ

Getting short summaries of very longats is also something the Transers library can help with with the summarization bike„ÄÇFinallyÔºå the last task supported by the B API is translation„ÄÇHere we use a French English model found on the Mo hub to get the English version of our quick text„ÄÇHere is a brief summary of all the tasks we've looked into in this video try them out through the influence switchts in the modern hub„ÄÇ



![](img/1b5e5fce582c60e42117fe3a72e59e6a_9.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_10.png)

Yeah„ÄÇSorry about the song at the beginning so I'm trying to get better wrong A I can't have my head on at the same time I'm playing the video so that part seem to have been dealt with and for the fact that it's leggy and be angle a lot trying to find a solution but I don't know what it could be it so it could be a little bit better the next video I tried minimizing OS and footage could be better„ÄÇ



![](img/1b5e5fce582c60e42117fe3a72e59e6a_12.png)

So the question I skipped before the video was when you do pipeline sentiment analysis„ÄÇ how can you determine which model it is using the dog string doesn't help much„ÄÇ Pat true its not in the documentation and we should probably make an effort to have it better document it the best way to do that is to inspect a source code for now„ÄÇWhich I'm going to show you in one second„ÄÇSoÔºå you need to go„ÄÇ

Into pipelines in it and that's a great question we actually make it more accessible and easier to see„ÄÇ but you need to go in the source code in the pipelines module and in the in file and where you will see for each pipeline the default model that is used so here we are using the text classification and sentimentalizes have the same default as the model that was used is this still well based in case find you in SS2 English„ÄÇ

Don't know why the person didn't pick a short nameÔºå but„ÄÇThat's the name of it„ÄÇLet me see if there are any other questions„ÄÇYesÔºå I'll send the links of the next video on the chat„ÄÇ or you can follow on the on the course chapter at the same time and look at them directly in and of our brother„ÄÇOkayÔºå it looks like there are number of our questionsÔºå so let's continue„ÄÇSo this chapter„ÄÇ

 So the rest of this section is just a good samples but shown in the video„ÄÇ So I'm not going to go through that InsteadÔºå I'm going to look at my call upÔºå which hopefully„ÄÇShould as„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_14.png)

I need to restart own timeÔºå all lovely„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_16.png)

So hopefully it's not going to take a long time to reinstall everything„ÄÇÂ•ΩÂ•ΩÂ•Ω„ÄÇWhy are you being with me„ÄÇOkay„ÄÇSo for could be a little bit smoother for you„ÄÇSo once we have installed the Transformers libraryÔºå we can run the first cellÔºå for instance„ÄÇ which add the same good as in the video and yeah„ÄÇV is a book with the last version of To show which was just released now„ÄÇ

 and so we can't run anything on Collab„ÄÇSo that's also lovely„ÄÇNot sure if we can get this„ÄÇyou know your version somehow„ÄÇ„ÅÇ„ÄÇOkayÔºå so hopefully the Pytoch works folks will fix the bug very soon and you will all be able to run all the collapse pretty easily in the meantime„ÄÇ

 I'm going to try to show you the same thing in a tripyter notebook and'm going where I'm not going do something that can run it„ÄÇGe„ÄÇSo let me just go„ÄÇ I'm not gonna be here„ÄÇSo if you prefer winning notebooks locally„ÄÇ everything is in the report called Notes inside the Eing F Or and with this notebook you have a course sub folder and when you have chapter by chapter or directly for the videos„ÄÇSo let's look„ÄÇAt the same notebook I was trying to run in Collab„ÄÇZoom in a little bit„ÄÇ

Let is done lessÔºå okay„ÄÇSo after running the install„ÄÇ which I don't need to do because it's my have everything installed„ÄÇ you can run the code that we were seeing on the video you can play around and put another sentence„ÄÇ try with several sentences at the same time„ÄÇTwice as there wrote classificationification pipeline and all the pipeline were saw in the video„ÄÇ

üòäÔºåAs the next„ÄÇThe other way you could try all of that is model ever„ÄÇÂóØ„ÄÇspec ed„ÄÇ‰∏çÂæóÊúâ‰Ω†ÊÄé‰πàÊ†∑„ÄÇËØ∂„ÄÇWhere you can go so the model appears do co s models and where you can click on any model„ÄÇ So for instanceÔºå if we go and the distill be based on case fine turn SS to English„ÄÇ which we saw as a default model for the sentiment analysis pipeline„ÄÇ

 we can go there and whereas a small wet where we can„ÄÇTry it on any sentence„ÄÇAnd gets the same results„ÄÇSo„ÄÇYou've got oh meta table„ÄÇ so you've got at least three different ways of trying all the good samples„ÄÇ So on the with the inference API and the websiteÔºå sometimes when you were when you're trying a model you have a small progress by here while it's loading and then you can try it on b or sentences in a„ÄÇ

So„ÄÇLet's go„ÄÇBack to the course„ÄÇOn the next section„ÄÇAbout how transformer models work so transformer models are pretty recent„ÄÇ So the architecture in itself was released in a paper in 2017 and then the first pretrain model was G„ÄÇAnd was released by Open AI in June 2018Ôºå a second betweentrain model Westbro released by Google in October 2018 and then it kind of accelerated like here are just a few samples of model„ÄÇ

 but there are manyÔºå many more„ÄÇThat were released and even though so we would need at least like to to have this image be twice or three times as big to be able to put every model that was released between 2019 and right now„ÄÇIt's really hard to follow the base at which everything is released„ÄÇ

 but the transformformers library as I think 60 different architectures now and is trying to add as soon as the papers are released„ÄÇAnd so what we're going to look at here is not what's inside a transformable model in details„ÄÇ in detailsÔºå but a little bit of an overview and there are three types of transformer models which we're going to explore in this section„ÄÇ so we are GT model GT like model that are called ons or autoregressive models„ÄÇ

Wwhichch are transformable models that are basically to generate text„ÄÇ look so what we were talking about beforeÔºå like when your phone is trying to suggest you ideas of words that you could put in the sentence afterward„ÄÇThe other type of transformer model is the be like model a GT like sorry it's not the noncodeders model it's a decoders model„ÄÇ sorry autoregressive or decoders model for GT bet like its encodecoder model or autoencoing models„ÄÇ

 so those models are best suited to classify things so generate a9 representation of the sentence which allows you to either classify the whole sentence„ÄÇ classify every words in the sentence are they also very good for the extractive question and throwinging task we talked about earlier„ÄÇ

And the last type of model are the sequence to sequence transformer models are encode decos model„ÄÇ which has the ones that are better suited for the sequence to sequence text such as translation„ÄÇ sumorizationÔºå basically writing a new text from an input text„ÄÇSo all those transformer models are language models and the main difference between the first two types„ÄÇ

 the encodeders and the decoders is that the decoders are portrayed by kissing the next world„ÄÇ so that's why we are very good at generating text because that's how they are betrayed„ÄÇWhereas the en those models are usually betweened with feeling some random masks in the sentence„ÄÇ so we have two examples here„ÄÇAnd those transformformers models are super big so we started with like open AGTD with tests rather than tell millions of parameters and now we have models with like billions or tens of billions of parameters„ÄÇ

Which is why it's very important to reuse this modelÔºå which is the whole point of transfer learning„ÄÇ so transfer learning is when you take your betweentrain model„ÄÇ which is huge and between on lots and lots of amount of data which took a lot of compute so a lot of later bra animated a lot of CO2 and you reuse that model to find unit on a new tasks„ÄÇ the task that you want to work with and„ÄÇYeahÔºå so by reducing the model instead of training a new one from scratch„ÄÇ

 you're saving computeÔºå you're saving money and you need less data than what the model always be trained on„ÄÇLet you see our new questions„ÄÇVery one question is GT3 the only model that allows you to do zero short learningÔºü

So that is how this model was advertisedÔºå it's not the only one because T5 for instance„ÄÇ has the same kind of prompt where you tell itÔºå for instance„ÄÇ summarize this text or translate this text from this language to this language and so it can also do some sort of zero learning„ÄÇAnd the link to the notebook but it was answered already So let's go and watch the transfer learning video I hope this one is going to be a little bit better than the previous one and or in the chat like will share the link if you want to watch it directly if it's too leg„ÄÇ



![](img/1b5e5fce582c60e42117fe3a72e59e6a_18.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_19.png)

What is transfer learningÔºüThe idea of transfer learning is to leverage signal knowledge acquired by a model train with lotss of data on another task„ÄÇThe Model A will be trained specifically for task A„ÄÇNow let's say you want to train Ad all B for a different task„ÄÇOne option would be to train the model from scratch„ÄÇThis could take lots of computation„ÄÇ

 time and data„ÄÇInsteadÔºå we could initialize Model B with the same weights as Model A„ÄÇ transferring the knowledge of Model A on T P„ÄÇWhen trend from scratch„ÄÇ all the middle's weights are initialized randomly„ÄÇIn this example„ÄÇ we are training a belt model on the task of recognizing if two sentences are similar on it„ÄÇ

On the leftÔºå it's trained from scratch„ÄÇAnd on the rightÔºå it's venting a proed model„ÄÇAs we can see„ÄÇ using transfer learning on the preed model yields better results„ÄÇAnd it doesn't matter if we train longerÔºå so training from scratch is kept around 70% accuracy„ÄÇ or the betweentrain would all beat the 86% easily„ÄÇ

This is because portraytrained models are usually trained on large amounts of data but provides a model with a statistical understanding of the language used during portraytraining„ÄÇIn computer visionsÔºå transfer learning has been applied successfully for almost 10 years„ÄÇ

Models are frequently proed on ImageNet by a asset containing 1„ÄÇ2 million of photo images„ÄÇEach image is classified by one of 1000 levels„ÄÇTraining like this„ÄÇ unlabeled data is called supervised learning„ÄÇIn natural language processing„ÄÇ transfer learning is a bit more recent„ÄÇA key difference with ImageNet is that the training is usually self supervised„ÄÇ

Which means it doesn't require humanation for the labels„ÄÇA very common portraying objective is to guess the next word in a sentence„ÄÇWhich only requires lots and lots of text„ÄÇGPT2Ôºå for instance„ÄÇ was retrieveed this way using the content of 45 million links posted by users on web„ÄÇ

Another example of selfupvised per training objective is to predict the value of randomly massed quals„ÄÇWhich is similar to fit in blood B tests you may have done in school„ÄÇBrt was between this way using the English Wikiped and had100 and published books„ÄÇIn practice„ÄÇ transfer learning is applied on a given model by throwing away its head„ÄÇ

 that is its last layers focused on the per training objective„ÄÇAnd' with a new randomly initialized a suitable for the desk attempt„ÄÇFor instance„ÄÇ when invented in the build model earlierÔºå we remove the a that classified Mque and replaced it with a classifier with two outputposts since our task at two levels„ÄÇTo be as efficient as possibleÔºå the protrained model used should be as similar as possible to the task it's fine tune on„ÄÇ

For instanceÔºå if the problem is to classify German sentences„ÄÇIt's best to use a German portraying model„ÄÇBut with the good comes to bad„ÄÇ the proed model does not only transfer its knowledgeÔºå but also any bias it may contain„ÄÇImageNe mostly contains images coming from the United States and Western Europe„ÄÇ

 so models fine tuned with it usually will perform better on images from these countries„ÄÇBut Beni also studied the bias in the prediction of its Gpyy3„ÄÇ which was between using the guess and X world objectives„ÄÇChanging the gender of the prone from E Westbury to she Westbury changed the predictions from mostly neutral objectives„ÄÇ

To almost only physical ones„ÄÇIn the model code of the GT2 Mor„ÄÇ Open AI also acknowledges its bias and discourages its use in systems that interact with humans„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_21.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_22.png)

ÂóØ„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_24.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_25.png)

OkayÔºå so let me see if there are any questions on the video„ÄÇüòîÔºåI don't think so„ÄÇOkay„ÄÇ so this was what transfer learning is very high level introductionions and this is what's usually done with transformerss model„ÄÇAnd for instanceÔºå the model we used before distill B fine tune necess to English„ÄÇ I'm forgetting alphat nameÔºå as the name indicates it was fine tuned on the62 dataset set which is a dataset set containing sentences and you have to classify them between positive and negative„ÄÇ

And„ÄÇUnlesss are are any new questionsÔºå I don't think so we're already to dive into the transformer architecture a little bit more so Omar will post the link of that video in the chat„ÄÇ I hope it's not too leggy againÔºå so this video is going to introduce as the difference between Odo decos and sequence to sequence transformer model we briefly talked about before„ÄÇ

Let's study the transformer architecture„ÄÇThis video is the introductory video to the Ens„ÄÇ decoders and Ender decoder series of videos„ÄÇIn the series„ÄÇ we'll try to understand what makes a transformer network and we'll try to explain it in simple high level terms„ÄÇNo advanced understanding of neural networks is necessary„ÄÇ

 but an understanding of basic vectors and tensors may help„ÄÇTo get started„ÄÇ we'll take up this diagram from the original transformer paper entitledAttention is All you need by Vawaing it up„ÄÇAs we'll see hereÔºå we can leverage only some parts of it according to what we're trying to do„ÄÇWe want to dive into these specific layersÔºå building up that architecture„ÄÇ

 but we' try to understand the different ways this architecture can be used„ÄÇLet's first start by splitting that architecture into two parts on the left you have the encoder and on the right„ÄÇ the decoder„ÄÇThese two can be used togetherÔºå but they can also be used independently„ÄÇLet's understand how these work„ÄÇThe encoder accepts inputs that represent text„ÄÇ

 it converts these textÔºå these words into numerical representations„ÄÇThese numerical representations can also be called embeddings or features„ÄÇWe'll see that it uses the self attention mechanism as its main component„ÄÇAnd we recommend you check out the video on encoders specifically to understand what is this numerical presentation as well as how it works„ÄÇ

üòäÔºåWe'll study the self attention mechanism in more detailÔºå as well as its bidirectional properties„ÄÇüòä„ÄÇThe decoder is similar to the encoderÔºå it can also accept text inputs„ÄÇIt uses a similar mechanism as the encoderÔºå which is the masked self attention as well„ÄÇIt differs from the encoder due to its unititedirectional feature and is traditionally used in an autoregressive manner„ÄÇ

Here tooÔºå we recommend you check out the video on decoders„ÄÇ especially to understand how all of this works„ÄÇCombining the two parts results in what is known as an anchor decoder or a sequence to sequence transform„ÄÇThe encoder accepts inputs and computes a high level representation of those inputs„ÄÇThese outputs are then passed to the decoder„ÄÇ The decoder uses the encoder's outputs and alongside other inputs to generate a prediction„ÄÇ

And then predict an outputÔºå which is will reuse in future iterationsÔºå hence the term autoregressive„ÄÇFinallyÔºå to get an understanding of encoder decoders as a whole„ÄÇ we recommend you check out the video on encoder decoder„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_27.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_28.png)

„ÄÇ

![](img/1b5e5fce582c60e42117fe3a72e59e6a_30.png)

Okay so that was the general introduction and so basically this is the diagram you have to remember about the general transformformers architecture and so what is an encode decoder or an encode decoder model we dive into that a little bit more just before going there we briefly talked about attention which was in the„ÄÇ

diagramia for the original paperÔºå so the core of the architecture the transformer model is this layer called multied attention„ÄÇ which is a layer that basically as indicates base attention„ÄÇ so it's going to look at your wall sentence„ÄÇsequence of input and for each word it's going to compute some score li to pay attention to this or this word of is a world so why is that it's because the transformer architecture was originally designed for translation and when you're translating if you're translating a given word you need that word but you need to understand the context around it so for instance you might need the gender of if the word are none you may need its gender which would be the word before you may need some words after„ÄÇ

So that attention layer is where on top of computing some contextual representation of a given world„ÄÇ it's there to tell them all all for this specific word also in to this one in particular this world not so useful but this one you should really pay attention to it will have videos coming in section 2 and section 3 of the calls that go into more details of what attention layer is but that's the I level1 introduction„ÄÇ

And the key difference between encode codeos and decoders is thattting on codes as the attention mechanism is allowed to look at every word in the sentence„ÄÇ So the word before and the word afterÔºå that's because like the bird model when you need to„ÄÇDetect„ÄÇ

 sorry to guess what is the value of a mask wordÔºå it's useful to look at what was before and also what we after„ÄÇThe decoders models like GT have to predict the next world„ÄÇ so if they were allowed to look at the word after it would be cheating„ÄÇ so in those models the attention layer is allowed to look at what was before in the sentence so for instance when trying to guess silver as the attention layer can only look at my name and is they can't look at what is after in the sentence„ÄÇ

Yeah„Åù„ÄÇWe are going to switch to the next„ÄÇSection of this chapter on Ocuoss modelsÔºå but before that„ÄÇ let me see if there are any questions„ÄÇOn the decoder diagram„ÄÇ what does the output shifted right refer toÔºå please„ÄÇLet me check that diagram„ÄÇThis„ÄÇ so that's because„ÄÇHere„ÄÇWe are training a translation model remember this is the original architecture of the transformer model so when you translate a phrase sentence in English and the output is a sentence in French so here the labels your this part of the transformer model is the decoder and it will try to guess the next word so it's going to begin with nothing and then when to try to begin the first word of the sentence then it will have the first word of the sentence to try begin to get the second word of the sentence then it will have the first word in the sentence to get the third word in the sentence„ÄÇ

 etctera et cetera so that's where the shift right means it means that here we have the output so the text in the desired language shifted one token to the right„ÄÇAnd another question wasÔºå can you provide a quick overview intuition of oh a digital version of a model like distber is able to maintain accuracy while being significantly sorry„ÄÇ

 more lightweight so quick overview intuition„ÄÇSure„ÄÇIn general„ÄÇ transformer models or deep learningruning models have lots of parametersÔºå several millions„ÄÇ but a lot of those parameters are either redundant or not necessarily really useful and so for instance varies a lot of research into pruning transformers networks so pruning means removing some of those weights to be able to go faster especially at eachfer„ÄÇ

 distillation is another way of reducing the size so the process is to have a smaller model try to get some output as a bigger model and it works reasonably well to still have some good performance„ÄÇSo that's because the intriition behind it is that a very big model as lots of parameters„ÄÇ

 but not all of those parameters are really useful„ÄÇAnd so let's dive into the encoders model so we have three more videosÔºå encoders„ÄÇ decos and then sequence to sequence model„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_32.png)

We put it in full screenÔºå and design„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_34.png)

In this videoÔºå we'll study the encoder architecture„ÄÇAn example of a popular encoder only architecture is BtÔºå which is the most popular model of its kind„ÄÇLet's first start by understanding how it works„ÄÇWe'll use a small example using three words„ÄÇ We use these as inputs and pass them through the encoder„ÄÇ

We retrieve a numerical representation of each word„ÄÇHereÔºå for example„ÄÇ the encode converts those three words welcomee to NYC in these three sequences of numbers„ÄÇThe encode outputs exactly one sequence of numbers per input word„ÄÇThis numerical representation can also be called a feature vector or a feature tensor„ÄÇ

Let's dive in this representation„ÄÇ It contains one vector per word that was passed through the encoder„ÄÇEach of these vector is a numerical representation of the word in question„ÄÇThe dimension of that vector is defined by the architecture of the model for the base bird model„ÄÇ it is 768„ÄÇThese representations contain the value of a wordÔºå but contextualized„ÄÇFor example„ÄÇ

 the vector attributed to the word2 isn't the representation of only the two word„ÄÇIt also takes into account the words around itÔºå which we call the context„ÄÇAs in it looks to the left contextÔºå the words on the left of the one we studying„ÄÇ hear the word welcomeÔºå and the context on the rightÔºå here the word NYC„ÄÇ

 and it outputs a value for the word given its context„ÄÇIt is therefore a contextualized value„ÄÇüòä„ÄÇOne could say that the vector of 768 values holds the meaning of the word within the text„ÄÇIt does this thanks to the self attention mechanism„ÄÇThe self attention mechanism relates to different positions or different words in a single sequence in order to compute a representation of that sequence„ÄÇ

As we've seen beforeÔºå this means that the originalot representation of a word has been affected by other words in the sequence„ÄÇWe won't dive into these specifics hereÔºå but we' offer some further readings if you want to get a better understanding at what happens under the hood„ÄÇ

So when should one use an encodeÔºüEncodeders can be used as tenderalone models in a wide variety of tasks„ÄÇFor exampleÔºå BertÔºå arguably the most famous transformer modelÔºå is a standalone anchor model„ÄÇ and at the time of release it be the state of the art in many sequence classification tasks„ÄÇ question answering tasksÔºå and masked language modeling to only cite a few„ÄÇüòä„ÄÇ

The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence„ÄÇThis vector can then be handled down the road by additional neurons to make sense of it„ÄÇLet's take a look at some examples where encodes really shine„ÄÇFirst of all„ÄÇ mask language modeling or MLM„ÄÇIt's the task of predicting a hidden word and a sequence of word„ÄÇHere„ÄÇ

 for exampleÔºå we have hidden the word between my and is„ÄÇThis is one of the objectives with which Bert was trained„ÄÇ It was trained to predict hidden words in a sequence„ÄÇEncodes shine in this scenario in particular as bidirectional information is crucial here„ÄÇ

If we didn't have the words on the rightÔºå is it Silva and the dot„ÄÇ then there is very little chance that Bt would have been able to identify name as the correct word„ÄÇThe encoder needs to have a good understanding of the sequence in order to predict a masked word„ÄÇ as even if the text is grammatically correct„ÄÇIt does not necessarily make sense in the context of the sequence„ÄÇ

As mentioned earlierÔºå encodes are good at doing sequence classification„ÄÇüòä„ÄÇSentiment analysis is an example of sequence classification„ÄÇThe model's aim is to identify the sentiment of a sequence„ÄÇIt can range from giving a sequence a rating from one to five stars if doing review analysis to giving a positive or negative rating to a sequence„ÄÇ

 which is what is shown here„ÄÇFor exampleÔºå hereÔºå given the two sequences„ÄÇ we use the model to compute a prediction and to classify the sequences among these two classes„ÄÇ positive and negative„ÄÇWhile the two sequences are very similar containing the same words„ÄÇ the meaning is entirely differentÔºå and the encoder model is able to grasp that difference„ÄÇ



![](img/1b5e5fce582c60e42117fe3a72e59e6a_36.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_37.png)

ÂóØ„ÄÇSo that was it forcodo's modelÔºå some examples are AlberttÔºå BtÔºå distill BtÔºå ElectÔºå Roberta„ÄÇLet's have a look at the next sectionÔºå which is going to be about decoderos and just before that let me check if there are any questions„ÄÇDon't see any new questions in the chatÔºå don't forget that you can ask any questions in the chat that's kind of the point of having the live session„ÄÇ

![](img/1b5e5fce582c60e42117fe3a72e59e6a_39.png)

Yeah„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_41.png)

In this videoÔºå we'll study the decoder architecture„ÄÇAn example of a popular decoder only architecture is GPT2„ÄÇIn order to understand how decoders work„ÄÇ we recommend taking a look at the video regarding encodersÔºå they're extremely similar to decoders„ÄÇOne can use a decoder for most of the same tasks as an encoder„ÄÇ

 albeit with generally a little loss of performance„ÄÇLet's take the same approach we have taken with the encoder to try and understand the architectural differences between an encoder and ID coder„ÄÇWe'll use a small example using three words„ÄÇ We pass them through their decoder„ÄÇWe retrieve a numerical representation for each word„ÄÇHereÔºå for example„ÄÇ

 the decoder converts the three words welcomee to NYC and these are three sequences of numbers„ÄÇThe decoder outputs exactly one sequence of numbers per input word„ÄÇThis numerical representation can also be called a feature vector or a feature tensor„ÄÇLet's dive in this representation„ÄÇIt contains one vector per word that was passed through the decoder„ÄÇ

Each of these vectors is a numerical representation of the word in question„ÄÇüòä„ÄÇThe dimension of that vector is defined by the architecture of the model„ÄÇWhether a decoder differs from the encoder is principally with its self attention mechanism„ÄÇ it's using what is called masked self attention„ÄÇHereÔºå for exampleÔºå if we focus on the word 2„ÄÇ

 we'll see that this vector is absolutely unmodified by the NYC word„ÄÇThat's because all the words on the rightÔºå also known as the right context of the word is masked„ÄÇRather than benefiting from all the words on the left and rightÔºå so the bidirectional context„ÄÇ decoders only have access to a single context„ÄÇWhich can be the left context or the right context„ÄÇ

The mask self attention mechanism differs from the self attention mechanism by using an additional mask to hide the context on either side of the word„ÄÇThe words numerical representation will not be affected by the words in the hidden context„ÄÇSo when should one use a decoder decoders like encoders can be used as standalone models„ÄÇ as they generate a numerical representationÔºå they can also be used in a wide variety of tasks„ÄÇ

 howeverÔºå the strength of a decoder lies in the way a word can only have access to its left context„ÄÇHaving only access to their left contextÔºå they are inherently good at text generationration„ÄÇ the ability to generate a word or a sequence of wordsÔºå given a known sequence of words„ÄÇüòä„ÄÇThis is known as causal language modeling or natural language generation„ÄÇ

Here's an example of how causal language modeling works„ÄÇ We start with an initial wordÔºå which is my„ÄÇWe use this as input for the decoder„ÄÇüòäÔºåThe model outputs a vector of numbers„ÄÇ and this vector contains information about the sequenceÔºå which is here a single word„ÄÇWe apply a small transformation to that vector so that it maps to all the words known by the model„ÄÇ

 which is a mapping that we'll see later called a language modeling head„ÄÇWe identify that the model believes that the most probable following word is name„ÄÇWe then take that new word and add it to the initial sequence„ÄÇFrom myÔºå we are now at my name„ÄÇThis is where the autoregressive aspect comes in„ÄÇüòä„ÄÇ

Outtoregressive models reuse their past outputs as inputs and the following steps„ÄÇOnce again„ÄÇ we do the exact same operation„ÄÇWe cast that sequence through the decoder and retrieve the most probable following word„ÄÇIn this caseÔºå it is the word is„ÄÇWe repeat the operation until we're satisfied„ÄÇStarting from a single wordÔºå we've now generated a full sentence„ÄÇWe decided to stop there„ÄÇ

 but we could continue for a while„ÄÇ GP T 2Ôºå for exampleÔºå has a maximum context size of 1024„ÄÇ We could eventually generate up to 1024 wordsÔºå and the decoder would still have some memory of their first words and that sequence„ÄÇüòäÔºåYeah„ÄÇÍ≤åÏûÑ„ÄÇLet me look at some questions on the decoders model Oh one question is it possible to use larger sentences more than 512 words as inputs for encodes„ÄÇ great question so it depends but most of the time no for instance spt as a maximum length of 512 so you can't use larger sentences than that„ÄÇ

Some newer models like LongformerÔºå for instanceÔºå can accept a longer context„ÄÇ so you should look at the documentationÔºå but first would be very specific on codes„ÄÇ like not all ons can do that„ÄÇAnother thing that you can do is split your sentence into several parts of 512 worlds and then if you get a representation„ÄÇ you pass each of those chunk to the model so you get a representation for each of those checks and you can„ÄÇ

 for instanceÔºå average where you get at the end to try to train a classifier for larger sentences„ÄÇ but that's kind of the only wayÔºå either a specific model that has been trained to have longer inputs such as longform or splitting your inputs input bits„ÄÇ

So another questionÔºå how does the mask language modeling objective deal with long gray worlds that end up being a multi doken world„ÄÇ example sea planee with Pipe becomes„ÄÇCi„ÄÇüòäÔºåAshAsh plane„ÄÇ so you will see about that for those who are more beginners„ÄÇ you will see about that separation in the next chapter with the Tokenals video„ÄÇ

If not using wall wall masking then the model can cheat and see parts of the longer world„ÄÇ so that is absolutely correct and that's why you will see that you have several versions of PE there is one that has been pretrained with wall wall masking and another one that has been trained without that it would be cheating for the ash ash plane because the model„ÄÇ

I mean it would be cheating where we're kissing CÔºå sorry when you've got the ash ash plane in the context„ÄÇ when you're trying to guess ash ash plane and you've got the shifts because in the context it's less cheating because the model has to guess that the model did not hand there„ÄÇ

 but it's true that using world wall masking would remove that kind of specific bias and cheating we've seen„ÄÇWeve the upper ris„ÄÇOkayÔºå so let's look at the last video and then I'll answer to more question after the last video on sequence to sequence transformable laws which combine both theionders and the decoders„ÄÇ

In this videoÔºå we'll study the encoder decoder architecture„ÄÇAn example of a popular encoder decoder model is T5„ÄÇIn order to understand how the encoder decoder works„ÄÇ we recommend you check out the videos on encoders and decoders as a standalone models„ÄÇ

Understanding how they work individually will help understanding how an encoder decoder works„ÄÇLet's start from what we've seen about the encoder„ÄÇThe encoder takes words as inputs„ÄÇ casts them through the encoderÔºå and retrieves a numerical representation for each word casts through it„ÄÇWe now know that this numerical representation holds information about the meaning of the sequence„ÄÇ

Let's put this aside and add the decoder to the diagram„ÄÇIn this scenario„ÄÇ we're using the decoder in a manner that we haven't seen before„ÄÇWe're passing the outputs of the encoder directly to it„ÄÇAdditionally to the encoder outputs„ÄÇ we also give the decoder a sequence„ÄÇWhen prompting the decoder for an output with no initial sequence„ÄÇ

 we can give it the value that indicates the start of a sequence„ÄÇüòä„ÄÇAnd that's where the anchor to decor magic happens„ÄÇüòäÔºåThe encoder accepts a sequence as input„ÄÇIt computes a prediction and outputs a numerical representation„ÄÇüòä„ÄÇThen it sends that over to the decoder„ÄÇIt hasÔºå in a senseÔºå encoded that sequence„ÄÇAnd the decoder„ÄÇ

 in turnÔºå using this input alongside its usual sequence input will take a stab at decoding the sequence„ÄÇThe decoder dedes a sequenceÔºå and outputs a word„ÄÇAs of now„ÄÇ we don't really need to make sense of that wordÔºå but we can understand that the decoder is essentially decoding what the encoder has output„ÄÇThe startup sequence hereÔºå the startup sequence word here indicates that it should start decoding the sequence„ÄÇ

Now that we have both the encoder numerical representation and an initial generated word„ÄÇ we don't need the encoder anymore„ÄÇAs we have seen before with the decoder„ÄÇ it can act in an autoregressive manner„ÄÇThe word it has just output can now be used as an input„ÄÇThis„ÄÇ in combination with the numerical representation output by the encoder„ÄÇ

 can now be used to generate a second word„ÄÇPlease note that the first word is still here as the model still outputs it„ÄÇ howeverÔºå we have grade it out as we have no need for it anymore„ÄÇüòäÔºåWe can continue on and on„ÄÇ for exampleÔºå on the decoder outputs a value that we consider a stopping value„ÄÇ like a dot meaning the end of a sequence„ÄÇHere we've seen the full mechanism of the encoder decoder transformer„ÄÇ

 let's go over one more time„ÄÇ we have an initial sequence that is sent to the encoder„ÄÇüòä„ÄÇThat encoder output is then sent to the decoder for it to be decoded„ÄÇWhile it can now discard the encoder after a single use„ÄÇ the decoder will be used several times until we have generated every word that we need„ÄÇ

So let's see a concrete example with translation language modelingÔºå also called transduction„ÄÇ which is the act of translating a sequence„ÄÇHere we would like to translate this English sequence We to NOIC in French„ÄÇWe're using a transformer model that is trained for that task explicitly„ÄÇ we use the encoder to create a representation of the English sentence„ÄÇ

 we cast this to the decoder with the use of the start sequence word„ÄÇ we ask it to output the first word„ÄÇIt outputs the avenueÔºå which means' welcome„ÄÇAnd we then use B avenue as the input sequence for the decoder„ÄÇThis alongside the encoder numerical representationÔºå allows the decoder to predict the second word a„ÄÇ

 which is two in English„ÄÇüòäÔºåFinallyÔºå we ask the decoder to predict a third wordÔºå it predicts NYC„ÄÇ which is correctÔºå we've translated the sentence„ÄÇWhere the encoder decoder really shines is that we have an encoder and a decoder„ÄÇ which often do not share weights„ÄÇThereforeÔºå we have an entire block„ÄÇ the encoder that can be trained to understand the sequence and extract the relevant information„ÄÇ

For the translation scenario we've seen earlierÔºå for example„ÄÇ this would mean parsing and understanding what was said in the English language„ÄÇIt would mean extracting information from that language and putting all of that in a vector dense in information„ÄÇüòäÔºåOn the other handÔºå we have the decoder whose sole purpose is to decode the numerical representation output by the encoder„ÄÇ

This decoder can be specialized in a completely different language or even modality like images or speech„ÄÇüòäÔºåEncosÔºå decoders are special for several reasons„ÄÇFirstly„ÄÇ theyre able to manage sequence to sequence tasks like translation that we have just seen„ÄÇSecondly„ÄÇ the weights between the encoder and the decoder parts are not necessarily shared„ÄÇ

Let's take another example of translation„ÄÇHere where translating transformers are powerful in French„ÄÇFirstlyÔºå this means that from a sequence of three words„ÄÇ we're able to generate a sequence of four words„ÄÇOne could argue that this could be handled with a decoder that would generate the translation in an autoregressive manner„ÄÇAnd they would be right„ÄÇAnother example of where sequence to sequence transformers shine is in summarization„ÄÇ

üòäÔºåHere we have very very long sequenceÔºå generally a full textÔºå and we want to summarize it„ÄÇSince the encoder and decoders are separatedÔºå we can have different context lengthsÔºå for example„ÄÇ a very long context for the encoder which handles the text and a smaller context for the decoder„ÄÇ which handles the summarized sequence„ÄÇThere are a lot of sequence to sequence models„ÄÇ

 This contains a few examples of popular encoder decoder models available in the Transformers library„ÄÇAdditionallyÔºå you can load an encoder and a decoder inside an encoder decoder model„ÄÇüòäÔºåTherefore„ÄÇ according to the specific task you're targetingÔºå you may choose to use specific encoders and decoders which have proven their worth on these specific tasks„ÄÇ

![](img/1b5e5fce582c60e42117fe3a72e59e6a_43.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_44.png)

Yeah„ÄÇOkayÔºå so let's see if we have any questions„ÄÇOhÔºå okay„ÄÇ is pararaing kind of decodder problem or both encodeder decodder problemÔºü

And is there any pipeline available for sameÔºüSo paraphrasing so generating a similar task to the input will be more like an encode or decoder pronym„ÄÇ howeverÔºå it seems that it's going to be very hard to train a model on that task specifically because it's going to have a tendency to want to include the same thing it received„ÄÇ

Unless you want to parase in another styleÔºå for instance„ÄÇ going from formal style to very casual style„ÄÇThere is no pipeline available provide that desk out of the books in the transformformers library„ÄÇSo let's go to the next section of Cha1Ôºå which is the last section of the chapter and talk a little bit about bias and limitations of the transformer models„ÄÇSo like any deep learning model in general transformers are just one specific case„ÄÇ

 but it's really a problem for all kind of deeping model so there are powerful tools and as you know probably if you take a deep learning introduction course via powerful tools but you don't really control or a get from the input to the output this was more controlled by the training they cut and the training data they receive but if you don't take any precautions you can have those models make predictions that you don't necessarily want in the app deploy so for instance„ÄÇ

 let's just have a very quick look at what B which has been retrained on the objective of thin maskQu and just have a similar example to the one we saw in the transfer learning video with GPT where we just change the gender of the sentence so if you put this man works as a„ÄÇ

Cass the word or this woman man works as a guess the word for man„ÄÇ we get like some kind of neutral jobs„ÄÇProbably moree stere stetypeÔºå stereotypically maleÔºå for women„ÄÇ we get very stereotypically female jobs and prostitute„ÄÇ which is necessarily something we would want our model to output as one of the top five possibilities„ÄÇ

So and that model was not trained on data but was particularly flagged as problematic„ÄÇ more like usually considered neutralÔºå it's only Wikipedia and a corus of un publishedlish book„ÄÇSo you have to be like GBTÔºå for instanceÔºå is more known because it's been trained of kind of the wall of the internet according to editit users„ÄÇ which is considered a little bit more sexist or„ÄÇOxyenophobic or thing like that so you have to be very careful because that bias is in the pretrained model so this is done with the bird paste and case checkpoint specifically and completely reproducible because there is no randomness in the prediction so if you're go a notebook on tris you're going to get those results„ÄÇ

So this is present in any model in some way or novel and it's also going to persist after your fine tu„ÄÇ so you have always have to be super careful that when you fine your model in your training data there are going to be enough samples of the outputs you'd like to see and you should always roll out your model into production by taking some care to analyze the results that you get and potentially if you see that you have some predictions you would like to avoid try to correct your training data to add more samples„ÄÇ

To add more samples that would correct the bias of your model„ÄÇAnd that's it for chapter one„ÄÇ let me see if there are more questions„ÄÇIn the chat otherwise we're going to be ready to tune out so once you have finished this video don't hesitate to take a quiz I'm not going to do that on the video to be sure that you understood all the terms that were all the content sorry but we saw in this chapter there are two left sessions„ÄÇ

That you can find again on the forum„ÄÇ This is not the forum anymore„ÄÇThere are two live session next week for Cha 2Ôºå be sure to try and wide on the other so there's going to be one with me and next Wednesday hopefully all the technical issues will be resolved because I'm going to take a little bit of care to make sure it works a little bit better especially for the videos and so„ÄÇ

The first one is going to be with Lewis on morning European time on Wednesday and the second one is going to be on next first day at the same time as this one„ÄÇTake the time to create an account there if hub to play with the hopefully the bug in codeab is going to be sold very soon and you can play with also code on that platform„ÄÇ

 otherwise you can always use„ÄÇThe inference API on the website and also think about a project that you'd like to run„ÄÇAnd to try your skills on as you learn more using the course„ÄÇ so of anything related to text classificationÔºå because that's what will focus in this part of the course„ÄÇ but try to think of yeah what kind of model you would like to build doing something that you could then share with the community and then in the next fews will help you with the course to do that in practice„ÄÇ

Let me check when last time we got questions„ÄÇAnd„ÄÇYeahÔºå I think that's kind of whole„ÄÇOkay„ÄÇ thanks a lot for following and see you next week„ÄÇ![](img/1b5e5fce582c60e42117fe3a72e59e6a_46.png)

![](img/1b5e5fce582c60e42117fe3a72e59e6a_47.png)

ÂóØ„ÄÇ