# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P30ï¼šL5.4- Kerasä½¿ç”¨Dropoutä»¥å‡å°‘è¿‡æ‹Ÿåˆ - ShowMeAI - BV15f4y1w7b8

Hiï¼Œ this is Jeff Hetonï¼Œ welcome to applications of deep neural networks with Washington University In this videoã€‚ I'm going to show you how to make use of dropoutï¼Œ which is another type of regularization technique that you can use in Carra's neural networks along with L1 and L2 for the latest on my AI course and projects click subscribe in the bell next to it to be notified of every new video let's look at what dropout is actually doing for neural network Now dropout is added layer by layer as you can see hereã€‚

 dropoutï¼Œ you specify a percentage of the neurons for that layer and in this caseï¼Œ this neuronã€‚ this neuron and this neuron would be dropped out Now this changes each time through the steps in the epochs that way you're constantly changing which which of these neurons is actually available and not available when they're dropped outã€‚

 they keep their weights but they no longer contribute any values by these dashed lines to the next layer So this is a comparison that I've heard of this oftenã€‚![](img/1a9b299d3e905cb95a8e5cd8656ae1bc_1.png)

Would be like if you went to work and every day the CEO of the company told half the people to just go home at random that'd be pretty cool actuallyã€‚ but what that would do for you is it would make sure that none of the workers become particularly specialized at their own tasksã€‚

 They're very agile that way andã€‚Not overfi to the particular tasksã€‚ So this kind of keeps the neural network on its toesã€‚ This also attempts toã€‚Simulate having a bunch of neural networksï¼Œ each with different configurations of neurons put into there to help mitigate the randomness that we saw in the last part where the neural networkã€‚ you can train them multiple timesã€‚ and you get completely different resultsã€‚

 So this can lower the variability of the output of the neural network to some degreeã€‚ it's almost like built in andsemblingã€‚ you have a lot of virtual neural networks that are created for each each layer as the dropout is randomly appliedã€‚ Nowï¼Œ dropout this is important when it's done all of those neurons that were missingã€‚ which change back and forthï¼Œ all the neurons go backã€‚ So each of those neuronsã€‚

 those subnet in their return and you have the full neural network then for for your use after you're done with fittingã€‚ So dropout is only really affecting the neural network during training andã€‚out is yet another hyperparameter that we need to optimizeã€‚ which can influence the effectiveness of our neural networkã€‚

 This is a pretty good animation that I rather like that shows you essentially how how dropout works to some degreeã€‚ Basically as the training iterations are going throughã€‚ It's randomly selecting dropout neurons to to go awayã€‚ The white neurons are the ones that are still there in the black ones are dropped outã€‚

 You can see the input and the output neurons remain in the in the neural networkã€‚ Also biasesã€‚ You don't you don't drop out biasesã€‚ So let's see how we would do thisã€‚ We're going to do classificationã€‚ I'm going to go ahead and run this so that we get the classification data loadedã€‚ We're going to predict product from the sampleã€‚ğŸ˜Šï¼ŒThe simple data set that I that I gave you earlierã€‚

 this is essentially how you do dropoutã€‚ It's very simpleã€‚ You add almost a layerã€‚ a dropout sort of layerï¼Œ it's affecting the layer before itã€‚ So this is causing this previous layer 50 to drop out 50% of the neuronsã€‚ you can also add one to additional layer usually most literature that I've seen suggest not dropping out from your final hidden layerã€‚

 the layer that is just before the outputã€‚ so we'll follow thatã€‚ but if you wanted to add it to that layer too that's that's exactly how you would do itã€‚ go ahead and run thisã€‚It is basically setting up to do the Kfold cross validationã€‚And we can see now that it's completedï¼Œ our final accuracy was 70%ï¼Œ which is actually pretty goodã€‚

If we were to rerun thisï¼Œ we might get a different a different accuracyã€‚ but this does help stabilize some of the effects of the of the random weightsã€‚ So we'll try thatã€‚ Let's go ahead and run it a second timeã€‚And we can see the resultã€‚ It'sï¼Œ it's 71%ã€‚ So it actuallyã€‚ we can see that overallã€‚On our sample size of two anywayã€‚

 this dropout does seem to be helping compared to the previous one that we were looking at where we were using L1 and L2 in the previous the previous part in a different videoã€‚Now it didn't remove the variance completelyï¼Œ we were at 70% and removed moved it up to 71 just from rerunning itã€‚

In the next videoï¼Œ we're going to look at how to useã€‚Boottrapping so that we can get better benchmarks by running this through a number of times and averaging things togetherã€‚ Thank you for watching this video and the next video we're going to look at L1 L2 and drop out all together and get an idea of which you should be using and why this content changes often so subscribe to the channel to stay up to date on this course and other topics and artificial intelligenceã€‚



![](img/1a9b299d3e905cb95a8e5cd8656ae1bc_3.png)