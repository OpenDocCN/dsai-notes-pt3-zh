- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P26ï¼šL4.3- Kerasä»‹ç» - ShowMeAI - BV1Jm4y1X7UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this videoï¼Œ I'm going to give you a very quick introduction to how our transformer
    models work together with TensorFlow and Kasã€‚The very short explanation is that
    all of our TensorF models are also Keis model objectsã€‚ and so they have the standard
    KeIS model APIã€‚ğŸ˜Šï¼ŒIf you're an experienced machine learning engineer who's used
    Kais a lotã€‚ that's probably all you need to know to start working with themï¼Œ but
    for everyone elseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: including the prodigal Pytorrch engineers out there who are returning to the
    foldã€‚ I'm going to quickly introduce Keis models and how we work with themã€‚In
    other videosã€‚ which I'll link belowï¼Œ I'll run through training with Keis models
    in more detailï¼Œ but firstã€‚ at a high levelï¼Œ what is a Keis modelï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_2.png)'
  prefs: []
  type: TYPE_IMG
- en: So your model basically contains your entire networkã€‚ it contains the layers
    and the weights for those layers and also tells the model what to do with themã€‚
    so it defines the whole path all the way from your inputs to your outputsã€‚If you've
    used Keis beforeï¼Œ you probably started using model objects by building them out
    by handã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you added one layer after anotherï¼Œ maybe using the model dot ad or the functional
    approachã€‚And there's nothing wrong with thatã€‚ you can lots of great models are
    built that wayã€‚ but you can also preload an entire model weights and allã€‚ And
    this is really helpful because if you as you can see hereã€‚
  prefs: []
  type: TYPE_NORMAL
- en: if you try reading the paper or if you try looking at the codeã€‚ you'll see the
    inside of a transformer is pretty complex and writing it all out from scratch
    and getting it right would be hired even for an experienced machine learning engineerã€‚
    but because it's all packed inside a modelï¼Œ you don't need to worry about that
    complexity on that if you don't want toã€‚ if you're a researcher if you want to
    really dig in thereï¼Œ you canã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but you can also just load a pre-trained preconfigured transformer model in
    just one line of codeã€‚ğŸ˜Šã€‚![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_4.png)
  prefs: []
  type: TYPE_NORMAL
- en: And when I mentioned earlier about the KaIS APIï¼Œ the advantage of it is that
    whether you write your own model from scratch or load a pre-trained oneã€‚ you interact
    with the model through that same API so you use exactly the same few methods and
    you're going to see them again and again these methods like fitã€‚
  prefs: []
  type: TYPE_NORMAL
- en: compile and predict and like I mentioned will see cover concrete examples of
    how to use those methods in the videos I'll link below For nowã€‚ the key thing
    to take away from this video if you've never seen Keis before is that this neat
    encapsulation means that all the complexity of a huge neuralnet becomes manageable
    because you interact with it in exactly the same way using exactly the same methodsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_6.png)'
  prefs: []
  type: TYPE_IMG
- en: Whether it's a huge pretrained language model or a simple model you wrote out
    by handã€‚![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_8.png)
  prefs: []
  type: TYPE_NORMAL
- en: ã€‚
  prefs: []
  type: TYPE_NORMAL
