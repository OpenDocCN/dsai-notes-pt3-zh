# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- è®­ç»ƒç®¡é“ï¼šæ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨ - ShowMeAI - BV12m4y1S7ix

Hiï¼Œ everybodyã€‚ Welcome back to your new Pytorch tutorialã€‚ In the last tutorialã€‚

 we implemented logistic regression from scratch and then learned how we can use Pyt to calculate the gradients for us with Beck propagationã€‚

 Nowï¼Œ we will continue where we left offã€‚ And now we are going to replace the manually computed loss and parameter updates by using the loss and optimizer classes in Pytorchã€‚

ğŸ˜Šï¼ŒAnd then we also replaced the manually computed model prediction by implementing a pytorch modelã€‚

 Then Pytorch can do the complete pipeline for usã€‚ So this video covers steps 3 and 4ã€‚

And please watch the previous tutorial first to see the steps 1 and 2ã€‚ So now let's startã€‚



![](img/2b520fac31b9d1c4d2989f76ec98bd5f_1.png)

And firstï¼Œ I want to talk about the general training pipeline in Pytorchã€‚ So typicallyã€‚

 we have three stepsã€‚ So the first step is to design our modelã€‚

So we design the number of inputs and outputsã€‚ So input size and output sizeã€‚And then alsoã€‚

 we designed the forward pass with all the different operations or all the different layersã€‚

Then as a second stepï¼Œ we design or we come up with the so we construct the loss and the optimizerã€‚

 and then as a last stepï¼Œ we do our training loopã€‚So this is the training loopã€‚

 So we start by doing our forward passã€‚ So here we computeã€‚Or let's write this downã€‚

 Compute the predictionã€‚Then we do the backward passï¼Œ backward passã€‚ So we get the gradientsã€‚

 and Pytorch can do everything for usã€‚ We only have to define or to design our modelã€‚ Soã€‚

 and after we have the gradientsã€‚ We can then update our weightsã€‚So now we update our weightsã€‚

 and then we iterate this a couple of time until we are doneã€‚ And that's the whole pipelineã€‚

So now let's continueã€‚ and now let's replace the loss and the optimizationã€‚So for thisã€‚

 we import the neural network moduleï¼Œ so we import torch dot N N S and Nã€‚

 so we can use some functions from thisã€‚And now we don't want to define the loss manually anymore so we can simply delete thisã€‚

And nowã€‚Down here before our trainingï¼Œ we still need to define our lossã€‚ so we can say loss equalsã€‚

 And here we can use a loss which is provided from pytorchã€‚ So we can say N N dot M E lossã€‚

 which is exactly what we implement beforeã€‚ So this is the mean squared errorã€‚

 And this is a callable functionã€‚ And then we also want a optr from p chargechã€‚

 So we say optr equals torch dot optim from the optimization moduleï¼Œ And then here we use S GDã€‚

 which stands for stochastic radiant descentï¼Œ which will need some parameterssã€‚

 some parameters that it should optimizeã€‚ and it will need this as a listã€‚ So we put our W hereã€‚

 And then it also needs the Lrã€‚ So the learning rateï¼Œ which isã€‚



![](img/2b520fac31b9d1c4d2989f76ec98bd5f_3.png)

Our previously defined learning rateã€‚

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_5.png)

And then in our training loopã€‚ So the loss computation is now still the same because this is a callable functionã€‚

 which gets the actual y and the predicted yã€‚And then we don't need to manually update our weights any more so we can simply say optr dot stepã€‚

 which will do an optimization stepã€‚ And then we also still have to empty our gradients after the optimization stepã€‚

 So we can say optimizationr dot0 graã€‚And now we are done with step 3ã€‚

 So let's run this to see if this is workingã€‚Andã€‚Soï¼Œ yeahï¼Œ still workingã€‚

 Our prediction is good after the trainingã€‚And let's continue with step 4 and replace our manually implemented forward method with the with a pytorch modelã€‚

 soã€‚For thisï¼Œ let's we also don't need our weights anymore because then our pie touch model knows the parametersã€‚

So here we say model equals N N dot linearã€‚ So usually we had have to design this for ourselvesã€‚

But since this is very trivial for linear regressionã€‚ So this is only one layerã€‚

 this is already provided in pytorchã€‚ So this is N N dot linearã€‚

 and this needs an input size and an output size of our featuresã€‚And for thisã€‚

 we need to do some modificationã€‚ So now ourã€‚X and y need to have a different shapeã€‚

 So this must be a 2 D array Nowï¼Œ where the number of rows is the number of samplesã€‚

 And for each rowï¼Œ we have the number of or the the featuresã€‚ So this has a new shapeã€‚Sorryã€‚

In new shapeã€‚That looks like thisã€‚And the same for our yã€‚ So our y is the same shape nowï¼Œ So2ã€‚4ï¼Œ6ã€‚

And 8ã€‚ So now let's get the shapeã€‚ So this is yã€‚ I have to be careful nowã€‚

 So we can say number of samples and number of featuresã€‚Equals x dot shapeã€‚ And now let's print thisã€‚

 So print the number of samples and the number of featuresã€‚ And now let's run thisã€‚

 So this will run into an errorã€‚ But I think we get until hereã€‚ So the shape is now 4 by oneã€‚

 So we have four samples and one feature for each sampleã€‚ And now we define our modelã€‚

 So this needs an input and an output sizeã€‚ So the inputã€‚Input size equals the number of featuresã€‚

 and the output sizeï¼Œ output size is still the sameã€‚ So this is also the number of featuresã€‚

 So this is one as an input size and one as an output sizeã€‚ Now we need to give this to our modelã€‚

 So we say here input sizeã€‚

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_7.png)

And output sizeã€‚

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_9.png)

And then one moreï¼Œ then when weã€‚Want to get the predictionã€‚ We can simply say we can call the modelã€‚

 but now this cannot have a float valueã€‚ So this must be a tenzorã€‚ So let's create a test tenzorã€‚

 Let's say Xã€‚Testã€‚Equals torch dot Tenorï¼Œ which gets only one sample with5ã€‚

 And then it gets a data type ofï¼Œ sayï¼Œ torch dot float 32ã€‚And then here we pass the test sampleã€‚

 And since this is only one has only one valueï¼Œ we can call the dot item to get the actual float value thenã€‚

 So now let's copy and paste this down hereã€‚å—¯ã€‚And now we also have to modify ourã€‚Optimizer hereã€‚

 So we don't have our weights nowã€‚ So this lists with the parameters hereã€‚

 we can simply say model dotã€‚

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_11.png)

Terrameterï¼Œ and call this functionã€‚And now hereï¼Œ forï¼Œ for the predictionï¼Œ we alsoã€‚

 we simply call the modelã€‚

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_13.png)

And now we are doneã€‚ So now we are usingã€‚The pieytor model to get thisã€‚ And also down hereã€‚ Nowã€‚

 if we want to print them againï¼Œ we have to unpack themã€‚ So let's sayã€‚

W and an optional bias equals model parametersã€‚This will unpack themã€‚

 And then if we want to print the actualï¼Œ this will be a list of listsã€‚

 So let's get the first or the actual first weight with this and we can also call the item because we don't want to see the tenorã€‚

And now I think we are doneã€‚ So let's run this to see if this is workingã€‚ And yeahï¼Œ soã€‚

The final output is not perfectã€‚ So this might be because the initialization now is randomlyã€‚

 And alsoï¼Œ this optimizer technique might be a little differentã€‚ So you might want to play aroundã€‚

 play around with the learning rate and the number of iterationsã€‚ but basicallyï¼Œ it worksã€‚

 and it gets better and betterã€‚With every stepã€‚And yeahã€‚

 so this is how we can construct the whole training pipelineã€‚ And one more thingã€‚ So in this caseã€‚

 we didn't have to upï¼Œ have to come up with the model for ourselvesã€‚ So here we only had one layerã€‚

 And this was already provided in Pytorchã€‚ But let's say we need a custom modelã€‚

 So let's write a custom linear regression modelã€‚Then we have to derive this from Nï¼Œ N dot moduleã€‚

 and this will get a in itã€‚Methtã€‚Which has selfï¼Œ and which gets theã€‚Inputã€‚Dimmensionsã€‚

And the output dimensionsã€‚

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_15.png)

And then here we call superï¼Œ the super classï¼Œ So super of linear regression with selfã€‚ and then dotã€‚



![](img/2b520fac31b9d1c4d2989f76ec98bd5f_17.png)

In itï¼Œ this is how we call the superconorã€‚ And here we would define our layersã€‚So in this caseã€‚

 we say our self dot line or linear layer equals Nn dot linearã€‚

 and this will get the input dimension and the output dimension and then we store them hereã€‚

 and then we also have to implement the forward pass in our model class so self and xã€‚

 and here we can simply return self dot linear of xã€‚



![](img/2b520fac31b9d1c4d2989f76ec98bd5f_19.png)

![](img/2b520fac31b9d1c4d2989f76ec98bd5f_20.png)

And this is the whole thingã€‚ And now we can say our model equals linear regression with the input size and the output sizeã€‚

 And now this will do the same thingã€‚ So now this is just a dummy exampleã€‚

 because this is a simple wrapper that will do exactly the sameã€‚ But basicallyã€‚

 this is how we design our pieyto modelã€‚ So now let's comment this out and use this class to see if this is workingã€‚

Andã€‚Yeahï¼Œ so it's still workingã€‚ So that's all for nowã€‚ And nowï¼Œ Pyto can do most of the work for usã€‚

 Of courseï¼Œ we still have to design our model and have to know which loss and optimr we want to useã€‚

 but we don't have to worry about the underlying algorithms anymoreï¼Œ soã€‚Yeahã€‚

 you can find all the code on Giubã€‚ And if you like thisã€‚

 please subscribe to the channel and see you next timeï¼Œ byeã€‚



![](img/2b520fac31b9d1c4d2989f76ec98bd5f_22.png)