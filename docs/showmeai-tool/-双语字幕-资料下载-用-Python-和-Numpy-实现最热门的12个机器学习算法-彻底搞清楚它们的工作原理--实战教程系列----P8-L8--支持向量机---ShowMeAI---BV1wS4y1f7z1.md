# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼šL8- æ”¯æŒå‘é‡æœº - ShowMeAI - BV1wS4y1f7z1

Hiï¼Œ everybodyã€‚ Welcome to your new machine learning from scratcht tutorialã€‚ Todayã€‚ we are going to implement the SVM algorithm using only built in Python modules and Nyã€‚ğŸ˜Šã€‚The SVM or support vector machine is a very popular algorithmã€‚ It follows the idea to use a linear model and to find a linear decision boundary also called a hyperplane that best separates our dataã€‚

And hereï¼Œ the choice as the best hyperplane is the one that represents the largest separation or the largest march in between the two classesã€‚ So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximizedã€‚

 So if we have a look at this imageï¼Œ then we want to find a hyperplane and the hyperplane has to satisfy this equation W times x minus B equals 0ã€‚And we want toã€‚Find the hyperplane so that the distance to both theã€‚both classes is maximizedã€‚ So we used the class plus one here and-1 hereã€‚So thisï¼Œ this tense or the margin should be maximizedã€‚And firstï¼Œ let's have a look at the math behind itã€‚

 so it's a little bit more complex than in my previous tutorialsã€‚ but I promise that once you have understood itï¼Œ the final implementation is fairly simpleã€‚Soã€‚We use the linear modelï¼Œ W times x minus Bã€‚ That should be 0ã€‚ And then ourã€‚å—¯ã€‚Our function should also satisfy the condition that W times x minus B should be greater or equal than oneã€‚

For our class plus oneã€‚ So all the samples here must lie on the left side of this equation or this line hereã€‚And allã€‚The samples of the classï¼Œ-1 must lieã€‚On the right side from this equationã€‚ So if we put this mathematicallyï¼Œ then we should it must satisfy W times x minus B should be greater or equal than1 for class1ã€‚ or it should be less or equal than-1 for class -1ã€‚So if you put this in only one equationã€‚

 then we multiply ourã€‚Linear function with the class labelã€‚ And this should be greater or equal than oneã€‚So this is the condition that we want to satisfyã€‚And now we want to come up with the W and the Bã€‚ So our weights and the biasã€‚And for thisã€‚ we use the cost function and then apply gradient descentã€‚

 So if you're not familiar with gradient descent alreadyã€‚ then please watch one of my previous tutorialsã€‚ For exampleã€‚ the one with linear regression there I explain this a little bit more in detailã€‚So now let'sã€‚Conueã€‚ So we use theã€‚Use a cost function hereã€‚ And in this caseï¼Œ we use the hinge lossã€‚

 and this is defined as the maximum of 0 andã€‚1 minusã€‚ And here we have our conditionã€‚ Y I times our linear modelã€‚ So what this means is ifã€‚ if we plot the hinge loss and here the blue line is the hinge lossã€‚ So this is either 0ï¼Œ ifã€‚Y timesã€‚F is greater or equal than oneã€‚ So if they have the the same signã€‚Then it's 0ã€‚ And theï¼Œ if theyã€‚Yeahã€‚

 if they are correctly classified and are larger than oneï¼Œ then our loss is 0ã€‚ So this means if we have a look at this image againï¼Œ ifã€‚For the green glassï¼Œ if itsã€‚ if it lies on this sideã€‚Thenï¼Œ it's 0ã€‚And for the blue classï¼Œ if it lies on this sideã€‚ then it's also 0ã€‚And otherwiseï¼Œ then we have a linear functionã€‚

 So the further we are away from our decision boundary lineï¼Œ the higher is our lossã€‚And so this is one part of our cost functionã€‚ and the other part isã€‚As I already saidã€‚ we want to maximize the margin hereã€‚So between these two classes and the margin isã€‚Defined as2 over the magnitude of wã€‚ So this is dependent from our weightã€‚

 dependent on our weight vectorã€‚ So we want to maximize thisï¼Œ and thereforeã€‚ we want to minimize the magnitudeã€‚ So we put this or add this to ourã€‚Cost functionsã€‚ So we also put this termã€‚The magnitude of W to the power of two times a lambda parameterã€‚And then here we have our hinge lossã€‚So the lambda parameter tries to find a trade off between these two termsã€‚

 So we it says basically says which is more importantã€‚ So we want toï¼Œ of courseã€‚ we want to have the right classificationã€‚ We want to lie on the correct side of our linesã€‚ but we also want to have the the line such that the margin isã€‚Is maximizedã€‚å—¯ã€‚So yeahã€‚ so if we look at the two casesï¼Œ if ourï¼Œ if we are on the correct side of the lineã€‚

 So if Y I times F on xï¼Œ F of x is greater or equal than oneã€‚Then we simplyã€‚ we only have this term because this is the hinge loss is 0ã€‚ And otherwiseã€‚ then our cost function is this yearã€‚And now we want to minimize thatã€‚ So we want to get the derivatives or the gradients of our cost functionã€‚So in the first caseã€‚

 if we are greater or equal than oneã€‚Our derivativeã€‚Is only is two times lambda timesã€‚Wï¼Œ soã€‚ and here we only look at one component of our Wã€‚ So we get rid of the magnitudeã€‚And the derivative with respect to the B is 0ã€‚ So please double check that for yourself hereã€‚ I will not explain the derivatives and detailsã€‚And in the other caseã€‚

 so if if Y I times F on x is not greater or equal than oneã€‚Then our derivative with respect to the W is this equation hereã€‚ and the derivative with respect to our bias is only Y Iã€‚So againã€‚ please double check it for yourselfã€‚And then when we have our gradientsï¼Œ we can use the update ruleã€‚

 So the new weight is the old weight minus because we use gradient deã€‚ So we go into negative direction minus the learning rate or the step size times the derivativeã€‚ So these are our update rules andã€‚Nowï¼Œ I hope youve understood the concept and the math behind thisã€‚ And now we can start implementing itã€‚ So this is now straightforwardã€‚ Soï¼Œ first of allã€‚

 we import Ny S and Pï¼Œ of courseã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_1.png)

And then we create our class S we amï¼Œ which will get an in itã€‚Methodã€‚And here I will put in a learning rateï¼Œ which will get a default value of 0001ã€‚ and it will get a lambda parameterï¼Œ which will also get a defaultã€‚ And I will say this is 001ã€‚ So this is usually also a small valueã€‚And then it will get the number of iterations for our optimizationã€‚

 which will get the default of 1000ã€‚So then I will simply store themã€‚ So I will say self dot L R equals learning rateã€‚Self doã€‚Lambda Para equals Lambda Paraã€‚ So note that I cannot use Lambda here because Lada is a keyword and Python for the Lada functionã€‚Soã€‚ yeahï¼Œ them self doã€‚Andã€‚Its equals and itsã€‚ Then I will say self dot W equals none and self dot B equals noneã€‚

 So I have to come up with them laterã€‚ and then we define our two functionsã€‚ So as alwaysã€‚ one is the predict functionï¼Œ where we fit the training samples and the training labelsã€‚And theã€‚Sorryï¼Œ this is the fit methodã€‚And the other one is the predict methodã€‚Where we predict the labels of the test samplesã€‚And now let's start with the predict methodã€‚

 because this is very shortï¼Œ soã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_3.png)

We want toï¼Œ as I saidï¼Œ if we look at the mathï¼Œ we apply this linear modelã€‚ and then we look at the sign of thisã€‚ So if it's positiveï¼Œ then we say it's class 1ã€‚ and if it's negativeï¼Œ then we say it's class-1ã€‚ So we say linearã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_5.png)

Output equals Ny dotã€‚Dotï¼Œ So the dot productã€‚Of X and self dot Wï¼Œ minus self dotã€‚Bã€‚ and then we choose the sizeã€‚ so we can simply sayï¼Œ returnã€‚Nampy dot sign of this linear outputã€‚So this is the whole predict implementationã€‚ And now let's continue with the fit methodï¼Œ soã€‚First of allã€‚As I saidï¼Œ we used the classes plus 1 and-1 hereã€‚

 So we want to make sure that our y has only -1 and plus1ã€‚ So oftentimes it has 0 and1ã€‚ So let's convert thisã€‚ So let's say y underscore equalsã€‚ And here we can use nuy dot where this will get a conditionã€‚ So we say yã€‚ if this is less or equal than 0ï¼Œ then we put in -1ï¼Œ and otherwise we put in plus1ã€‚Soã€‚

This will convert all the zeros or smaller numbers to -1 and the other numbers to plus 1ã€‚And nowã€‚ let's get the number of samples and the number ofã€‚Featuresã€‚And this is simply X dotã€‚Shapeã€‚ because our input vector X is a nuy and DRAã€‚Where the number of rows is the number of samples and the number of columns is the number of featuresã€‚Then we want to initialize our W and our Bã€‚ And we simply put in zeros in the beginningã€‚

 So we say selfã€‚Tt W equals Ny zeros of size and featuresã€‚ So for eachã€‚Feature component we put in a0 for our weight componentã€‚And then we say self dot B equals 0ã€‚ And now we canã€‚Start with our gradient descentsã€‚ So we sayï¼Œ for underscoreã€‚ because we don't need this in range self dot and it iterã€‚

 So the number of iterations we want to do thisã€‚And thenï¼Œ we iterate over ourã€‚Train samplesã€‚ So I say4 index and Xï¼Œ I in enumerate Xã€‚So this will give me the current index and also the current sampleã€‚And nowã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_7.png)

What I want to do now is let'sã€‚Have a look at theã€‚math againã€‚ So I want toã€‚I want to calculate the weight or the derivative of our cost function with respect to the W and with respect to the biasã€‚And here I firstï¼Œ but at firstï¼Œ I look if this condition is satisfiedï¼Œ so I willã€‚Sayã€‚ and the condition is Y I times our linear functionã€‚ So I say condition equalsã€‚



![](img/0c2f47f3d7d980a73cb14308e5d73802_9.png)

Why underscore of the current indexã€‚Timesï¼Œ and then the linear functionï¼Œ soã€‚Nampai dotã€‚Of the current sample and our self dot W minus self dotã€‚Bã€‚å—¯ã€‚This should be greater or equal than oneã€‚So if this is satisfied and the condition is true and otherwise it's falseã€‚ So now I say ifã€‚

![](img/0c2f47f3d7d980a73cb14308e5d73802_11.png)

Conditionï¼Œ so if this is trueã€‚Thenï¼Œ ourã€‚Divatives look like thisã€‚ So the derivative with respect to the B is just 0ã€‚ And so we only need thisï¼Œ soã€‚I sayã€‚å—¯ã€‚So it's two times lambda times Wã€‚ And then in our updateï¼Œ we go in So we sayã€‚The new way is the old weight minus the learning rate times thisã€‚ So I write this in one stepã€‚

 So I say self dot W minus equal self dot learning rate timesã€‚ And now hereã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_13.png)

We haveã€‚Two times self dot Lambda parameter times self dot Wã€‚So this is the first updateã€‚Or if our condition is satisfied and we only need this updateã€‚ And otherwiseã€‚ we say self dots W minus equal self times L R the learning rate timesã€‚ and let's againã€‚ have a look at the equationã€‚ So it'sã€‚

![](img/0c2f47f3d7d980a73cb14308e5d73802_15.png)

Two times lambda times w minus Yï¼Œ I times Xï¼Œ I soã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_17.png)

Two timesã€‚Our lambda timesã€‚W minus nuy dotã€‚ So I want to multiply our vectorsï¼Œ Xï¼Œ I andã€‚Why I so the y underscore of the current indexã€‚So this is our update for the W and our self dotã€‚Bã€‚Is minus equal self times learning rate times the derivative and the derivative is onlyã€‚

![](img/0c2f47f3d7d980a73cb14308e5d73802_19.png)

Or just Y Iï¼Œ So onlyã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_21.png)

Why underscore of the indexã€‚And now we are doneã€‚ So this is the whole implementationã€‚And now let's test thisã€‚ So I've written a little test script that willã€‚Import this SVM classã€‚ and then it will generate aã€‚Some test samplesã€‚ So it will generate two glassesã€‚And then I will create my SVM classifier and fit the dataã€‚

And then I wrote a little function to visualize this so you can find the code on Gitthubï¼Œ by the wayã€‚ so please check it out for yourself and now if we run thisã€‚ so let's say Python as we am underscore Te Pã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_23.png)

And nowï¼Œ this shouldã€‚Calculate the weights and the biasï¼Œ and it should also plot the decisionã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_25.png)

Fun the the yellow line and the two lines on both sides hereã€‚And we see that it's workingã€‚ Soï¼Œ yeahã€‚That's all about the S VMã€‚ I hope you enjoyed thisã€‚ And if you like thisã€‚ please subscribe to my channel and see you next timeï¼Œ byeã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_27.png)