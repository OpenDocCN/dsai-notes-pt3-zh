- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘â€œå½“å‰æœ€å¥½çš„ TensorFlow æ•™ç¨‹ï¼â€ï¼Œçœ‹å®Œå°±èƒ½è‡ªå·±åŠ¨æ‰‹åšé¡¹ç›®å•¦ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼šL8- ä½¿ç”¨ Keras è¿›è¡Œæ¨¡å‹å­ç±»åŒ–
    - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘â€œå½“å‰æœ€å¥½çš„ TensorFlow æ•™ç¨‹ï¼â€ï¼Œçœ‹å®Œå°±èƒ½è‡ªå·±åŠ¨æ‰‹åšé¡¹ç›®å•¦ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼šL8- ä½¿ç”¨ Keras è¿›è¡Œæ¨¡å‹å­ç±»åŒ–
    - ShowMeAI - BV1em4y1U7ib
- en: '![](img/2b6b959a397871940a0ba2149e837d11_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b6b959a397871940a0ba2149e837d11_0.png)'
- en: What is going on guysã€‚ Hope you're doing awesome and welcome back in this videoã€‚
    We're going to take the next step in building our models and we're going to learn
    about model subclassingã€‚ which is a incredibly flexible way to build models and
    so so far we've touched on the sequential API that has very little flexibility
    but it's very convenient and then we also saw examples on the functional API for
    more flexibility in building our models and in this video we're going to use the
    subclassing which has the most amount of flexibility So I'm just going copy in
    all of the imports and stuff like that so we have seen all of this before so that
    we don't waste too much time on this So OS and for ignoring information messages
    from Tensorflow Keas layers to build our model Ms the data and these two lines
    will help you out if you running into an issues on running on the GPU and then
    we have these three lines right here that are just going to load the datasetã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼å¸Œæœ›ä½ ä»¬ä¸€åˆ‡éƒ½å¥½ï¼Œæ¬¢è¿å›æ¥è§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚æˆ‘ä»¬å°†è¿ˆå‡ºæ„å»ºæ¨¡å‹çš„ä¸‹ä¸€æ­¥ï¼Œå­¦ä¹ æ¨¡å‹å­ç±»åŒ–ã€‚è¿™æ˜¯ä¸€ç§æå…¶çµæ´»çš„æ„å»ºæ¨¡å‹çš„æ–¹æ³•ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ¥è§¦åˆ°äº†é¡ºåº
    APIï¼Œå®ƒçš„çµæ´»æ€§å¾ˆå°ï¼Œä½†ä½¿ç”¨èµ·æ¥éå¸¸æ–¹ä¾¿ï¼›æˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†åŠŸèƒ½æ€§ API çš„ä¾‹å­ï¼Œå®ƒåœ¨æ„å»ºæ¨¡å‹æ—¶æä¾›äº†æ›´å¤šçš„çµæ´»æ€§ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å­ç±»åŒ–ï¼Œè¿™æ˜¯æœ€çµæ´»çš„æ–¹æ³•ã€‚æ‰€ä»¥æˆ‘ä¼šå¤åˆ¶æ‰€æœ‰çš„å¯¼å…¥å’Œå…¶ä»–å†…å®¹ï¼Œæˆ‘ä»¬ä¹‹å‰éƒ½è§è¿‡è¿™äº›ï¼Œç¡®ä¿ä¸æµªè´¹å¤ªå¤šæ—¶é—´ã€‚å¯¼å…¥
    OS å’Œå¿½ç•¥ TensorFlow çš„ä¿¡æ¯æ¶ˆæ¯ï¼Œä½¿ç”¨ Keras çš„å±‚æ¥æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚è¿™ä¸¤è¡Œä»£ç å°†å¸®åŠ©ä½ è§£å†³åœ¨ GPU ä¸Šè¿è¡Œæ—¶é‡åˆ°çš„é—®é¢˜ï¼Œç„¶åè¿™ä¸‰è¡Œä»£ç å°†åŠ è½½æ•°æ®é›†ã€‚
- en: We're loading MNist and then we're doing a reshape right hereã€‚ we're only doing
    the reshape to add this dimension here for the number of channelsã€‚ and then we're
    converting it to F 32 and we're dividing by 255 to normalize the dataã€‚Alright
    so all of this is something that you've seen before what we want to do now is
    is well first to sort of motivate what we're going to do is we've seen the example
    that we have a convolutional network and then mapping it to or running it through
    batch norm and then running it to rather right this is a common structure right
    that we we've used multiple times and let's say you would do this and you would
    write that code times 10 now that would be a lot of code to write so what we can
    do is we can actually create a class for this kind of block so let's do that and
    let's do class CNN blockã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨åŠ è½½ MNIST æ•°æ®é›†ï¼Œç„¶ååœ¨è¿™é‡Œè¿›è¡Œå½¢çŠ¶è°ƒæ•´ã€‚æˆ‘ä»¬åªæ˜¯åœ¨è¿™é‡Œè°ƒæ•´å½¢çŠ¶ï¼Œä»¥æ·»åŠ é€šé“æ•°é‡çš„ç»´åº¦ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸º F32ï¼Œå¹¶é€šè¿‡ 255 è¿›è¡Œå½’ä¸€åŒ–ã€‚å¥½å§ï¼Œè¿™äº›éƒ½æ˜¯ä½ ä¹‹å‰è§è¿‡çš„å†…å®¹ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦åšçš„æ˜¯ï¼Œé¦–å…ˆæ¿€åŠ±ä¸€ä¸‹æˆ‘ä»¬è¦åšçš„äº‹æƒ…ï¼šæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ä¸€ä¸ªå·ç§¯ç½‘ç»œçš„ä¾‹å­ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°æ‰¹é‡å½’ä¸€åŒ–ï¼Œç„¶åå†è¿è¡Œã€‚è¿™ä¸ªç»“æ„æˆ‘ä»¬å·²ç»ä½¿ç”¨äº†å¤šæ¬¡ï¼Œå‡è®¾ä½ éœ€è¦å†™è¿™æ®µä»£ç 
    10 æ¬¡ï¼Œé‚£å°†ä¼šå†™å‡ºå¾ˆå¤šä»£ç ã€‚æ‰€ä»¥æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ä¸ºè¿™ç§å—åˆ›å»ºä¸€ä¸ªç±»ã€‚è®©æˆ‘ä»¬æ¥åšè¿™ä¸ªï¼Œå®šä¹‰ç±» CNN å—ã€‚
- en: And we're going to inherit from layers thatt layerã€‚And what this is going to
    do is it's going to keep track of everything under hood for doing back propagation
    and all of that stuffã€‚And thenã€‚We're just going to create an init function and
    we're going to specify the number of out channels and then let's say the kernel
    sizeã€‚So if you're familiar with Pytorch subclassing is using Kra subclassing is
    essentially exactly the same as creating Pytorch modelsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»å±‚ç»§æ‰¿ï¼Œè¿™å°†è·Ÿè¸ªæ‰€æœ‰è¿›è¡Œåå‘ä¼ æ’­ç­‰æ“ä½œçš„å†…å®¹ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆå§‹åŒ–å‡½æ•°ï¼ŒæŒ‡å®šè¾“å‡ºé€šé“çš„æ•°é‡ï¼Œä»¥åŠå†…æ ¸å¤§å°ã€‚å¦‚æœä½ ç†Ÿæ‚‰ PyTorchï¼Œå­ç±»åŒ–çš„ä½¿ç”¨ä¸åˆ›å»º
    PyTorch æ¨¡å‹æ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚
- en: so we create our in function first and we have information about how we want
    to build this block and we're going to do super to run the parentã€‚Parent class
    layer that layerï¼Œ and then we're going to do CNNM block selfã€‚In itã€‚And we're going
    to create our comb layerã€‚ So Sephã€‚ co is layers Com2 Dã€‚And we're and we're going
    to specify the out channels and then the kernel sidesã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„åˆå§‹åŒ–å‡½æ•°ï¼Œäº†è§£å¦‚ä½•æ„å»ºè¿™ä¸ªæ¨¡å—ï¼Œå¹¶ä½¿ç”¨ super æ¥è¿è¡Œçˆ¶ç±»å±‚ã€‚ç„¶åæˆ‘ä»¬å°†åš CNNM å—çš„åˆå§‹åŒ–ï¼Œå¹¶åˆ›å»ºæˆ‘ä»¬çš„ç»„åˆå±‚ã€‚å› æ­¤ï¼ŒSeph.co
    æ˜¯ layers.Com2Dï¼Œæˆ‘ä»¬å°†æŒ‡å®šè¾“å‡ºé€šé“å’Œå†…æ ¸å¤§å°ã€‚
- en: And then let's do padding equals the sameã€‚And then for the batch roomã€‚ we're
    just going to do layers dot batch normalizationã€‚And then we're just going to do
    a call methodã€‚ so we're going to do call input tensorï¼Œ and then we're going to
    specify trainingã€‚ and let's just set it to default as training to default falseã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®©å¡«å……ä¿æŒä¸å˜ã€‚ç„¶åå¯¹äºæ‰¹å½’ä¸€åŒ–ï¼Œæˆ‘ä»¬å°†åªåš layers.batch.normalizationã€‚ç„¶åæˆ‘ä»¬å°†åªåšè°ƒç”¨æ–¹æ³•ã€‚æ‰€ä»¥æˆ‘ä»¬å°†åšè°ƒç”¨è¾“å…¥å¼ é‡ï¼Œç„¶åæˆ‘ä»¬å°†æŒ‡å®šè®­ç»ƒã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤è®¾ç½®ä¸ºè®­ç»ƒï¼Œé»˜è®¤å€¼ä¸º
    falseã€‚
- en: So this is we're specifying training here because we're using batch normã€‚ how
    batch norm works is different when it's in training mode or evaluation mode but
    anywaysã€‚ we have a call method right here andã€‚The call methodï¼Œ if you're used
    Pytorchã€‚ is essentially the forward methodã€‚But anyway hereã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡ŒæŒ‡å®šè®­ç»ƒï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†æ‰¹å½’ä¸€åŒ–ã€‚æ‰¹å½’ä¸€åŒ–çš„å·¥ä½œæ–¹å¼åœ¨è®­ç»ƒæ¨¡å¼å’Œè¯„ä¼°æ¨¡å¼ä¸‹æ˜¯ä¸åŒçš„ï¼Œä½†æ— è®ºå¦‚ä½•ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªè°ƒç”¨æ–¹æ³•åœ¨è¿™é‡Œã€‚è°ƒç”¨æ–¹æ³•ï¼Œå¦‚æœä½ ç†Ÿæ‚‰
    Pytorchï¼Œæœ¬è´¨ä¸Šå°±æ˜¯å‰å‘æ–¹æ³•ã€‚ä½†æ— è®ºå¦‚ä½•ã€‚
- en: so we're initializing the modules we're going to use in this case comm 2 D and
    batchor and then in the call method we're taking some input tensor and we're going
    to run it through those layers so we're going to do x equals selfã€‚com of input
    tensï¼Œ then we're going to do selfã€‚baor of xã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æ­£åœ¨åˆå§‹åŒ–è¦ä½¿ç”¨çš„æ¨¡å—ï¼Œè¿™é‡Œæ˜¯ comm 2 D å’Œæ‰¹å½’ä¸€åŒ–ã€‚ç„¶ååœ¨è°ƒç”¨æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æ¥å—ä¸€äº›è¾“å…¥å¼ é‡ï¼Œå¹¶å°†å…¶é€šè¿‡è¿™äº›å±‚è¿è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åš x
    ç­‰äº self.com çš„è¾“å…¥å¼ é‡ï¼Œç„¶åæˆ‘ä»¬å°†åš self.baor çš„ xã€‚
- en: and then we're going to specify training equals training and then we're going
    to do x equals Tfã€‚nnã€‚ Reluã€‚Of xã€‚So we're doing comm batch numberrluï¼Œ and then
    we're going to return Xã€‚That's the structure that we saw right hereã€‚Although now
    we can reuse it multiple times from this classã€‚ so for exampleï¼Œ we could do something
    likeã€‚Our model is Kra dot sequentialã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†æŒ‡å®šè®­ç»ƒç­‰äºè®­ç»ƒï¼Œç„¶åæˆ‘ä»¬å°†åš x ç­‰äº Tf.nn.Relu.Of xã€‚æ‰€ä»¥æˆ‘ä»¬æ­£åœ¨è¿›è¡Œæ‰¹å½’ä¸€åŒ–çš„ ReLUï¼Œç„¶åæˆ‘ä»¬å°†è¿”å› Xã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„ç»“æ„ã€‚è™½ç„¶ç°åœ¨æˆ‘ä»¬å¯ä»¥å¤šæ¬¡ä»è¿™ä¸ªç±»ä¸­é‡ç”¨å®ƒã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€äº›ç±»ä¼¼çš„äº‹æƒ…ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯
    Kra.dot.sequentialã€‚
- en: And we're going to do CNN block and let's say we have 32 out channelsã€‚CNN blocklock
    64 out channelsã€‚CNM block 128 and then layers flattenã€‚And then layers1s 10 nodesã€‚
    Allrightï¼Œ so I meanã€‚ if you would write this code right hereï¼Œ just for this blockï¼Œ
    it would be multiple linesã€‚ three linesï¼Œ rightï¼Œ it would be for theumbmberï¼Œ the
    batch norm and the the Relu which would be quite annoying would just take up unnecessary
    space and not be as concise So this is one way we can do it and we could we could
    also specify now so we could do model dot compileã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¦åš CNN å—ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ 32 ä¸ªè¾“å‡ºé€šé“ã€‚CNN å— 64 ä¸ªè¾“å‡ºé€šé“ã€‚CNN å— 128ï¼Œç„¶åå±‚å±•å¹³ã€‚ç„¶åå±‚1s 10 ä¸ªèŠ‚ç‚¹ã€‚å¥½çš„ï¼Œå¦‚æœä½ åœ¨è¿™é‡Œå†™è¿™æ®µä»£ç ï¼Œä»…ä»…ä¸ºäº†è¿™ä¸ªå—ï¼Œå®ƒå°†æ˜¯å¤šè¡Œã€‚ä¸‰è¡Œï¼Œå¯¹å§ï¼Œå®ƒå°†æ˜¯æ‰¹å½’ä¸€åŒ–ã€ReLU
    å’Œå…¶ä»–çš„ï¼Œè¿™ä¼šéå¸¸éº»çƒ¦ï¼Œä¼šå ç”¨ä¸å¿…è¦çš„ç©ºé—´ï¼Œå¹¶ä¸”ä¸é‚£ä¹ˆç®€æ´ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬å¯ä»¥åšåˆ°çš„ä¸€ç§æ–¹å¼ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç°åœ¨æŒ‡å®šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åš model.compileã€‚
- en: we can do optimizer equals ks do optimizes do atom and we could do loss equals
    ks losses sparsã€‚Pateorical cross entropyï¼Œ then from logicits equals trueã€‚Metricsï¼Œ
    we're gonna use accuracy as metricã€‚ and then we could do model that fitã€‚On our
    x trainï¼Œ y trainã€‚Bch sizeï¼Œ64 epochsã€‚I don't knowï¼Œ3ã€‚ and then verbose equalsã€‚2ï¼Œ
    and then let's also do model dot evaluate x testï¼Œ y testã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åšä¼˜åŒ–å™¨ç­‰äº ks.do.optimize.do.atomï¼Œæˆ‘ä»¬å¯ä»¥åšæŸå¤±ç­‰äº ks.losses.spars.Pateorical.cross.entropyï¼Œç„¶åä»é€»è¾‘ä¸Šè®²ç­‰äº
    trueã€‚æŒ‡æ ‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºæŒ‡æ ‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åš model.fitã€‚åœ¨æˆ‘ä»¬çš„ x trainï¼Œy trainã€‚æ‰¹å¤§å°ï¼Œ64 è½®æ¬¡ã€‚æˆ‘ä¸çŸ¥é“ï¼Œ3ã€‚ç„¶åè¯¦ç»†ç¨‹åº¦ç­‰äº
    2ï¼Œç„¶åè®©æˆ‘ä»¬ä¹Ÿåš model.evaluate x testï¼Œy testã€‚
- en: batch size or 64 where both equals 2ã€‚ and then let's run thatã€‚So after 3 bucksã€‚
    we get 99% training and then 98ã€‚75 on the test setã€‚So what's pretty amazing as
    well in creating these blocks and doing these call methods is that it kind of
    feels like first of all it's pretty much like Pytors if you're used to that but
    if you're used to Nmpy it kind of feels like you're you're using Nmpy but I've
    also heard that it kind of feels like objectoriented Nmpy and I would agree with
    that because it feels very intuitive we can do things like print X dot shape and
    we can run that and then it would each time this is called it would then print
    the shape of x at that point and so this CNN block is run multiple times depending
    on the channels right so first we had 32 then 64 and then 128 and then it's rerun
    again so 32 6428 which makes debugging incredibly simpleã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹å¤§å°ä¸º64ï¼Œä¸¤ä¸ªéƒ½ç­‰äº2ã€‚ç„¶åæˆ‘ä»¬è¿è¡Œå®ƒã€‚æ‰€ä»¥ç»è¿‡3ä¸ªå—åï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šè·å¾—99%çš„å‡†ç¡®ç‡ï¼Œåœ¨æµ‹è¯•é›†ä¸Šä¸º98.75%ã€‚åˆ›é€ è¿™äº›å—å¹¶è°ƒç”¨è¿™äº›æ–¹æ³•çš„æƒŠäººä¹‹å¤„åœ¨äºï¼Œå®ƒæ„Ÿè§‰éå¸¸åƒPytorsï¼Œå¦‚æœä½ ä¹ æƒ¯äº†ï¼Œä½†å¦‚æœä½ ä¹ æƒ¯äºNmpyï¼Œå®ƒåˆåƒæ˜¯åœ¨ä½¿ç”¨Nmpyã€‚æˆ‘å¬è¯´è¿™æœ‰ç‚¹åƒé¢å‘å¯¹è±¡çš„Nmpyï¼Œæˆ‘åŒæ„ï¼Œå› ä¸ºè¿™æ„Ÿè§‰éå¸¸ç›´è§‚ï¼Œæˆ‘ä»¬å¯ä»¥åšè¯¸å¦‚æ‰“å°X.dot
    shapeçš„äº‹æƒ…ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ï¼Œå®ƒå°†æ‰“å°å‡ºæ­¤æ—¶xçš„å½¢çŠ¶ï¼Œè€Œè¿™ä¸ªCNNå—æ ¹æ®é€šé“è¿è¡Œå¤šæ¬¡ï¼Œç¬¬ä¸€æ¬¡æ˜¯32ï¼Œç„¶åæ˜¯64ï¼Œå†åˆ°128ï¼Œç„¶åå†æ¬¡è¿è¡Œï¼Œå› æ­¤æ˜¯32ï¼Œ6428ï¼Œè¿™ä½¿å¾—è°ƒè¯•å˜å¾—éå¸¸ç®€å•ã€‚
- en: it makes it so that you can just print the shapes wherever you feel like printing
    itã€‚But anywaysã€‚ let let's now make something more complicated or I guess more
    a bigger modelã€‚ So we're gonna to do class something called Resblock so this is
    be we're going to build something similar to Resnet and hopefully at the end of
    this video you can sort of view it from the perspective that what if we use the
    functional or just sequential API to reconstruct something similar as we're doing
    in this video and hopefully you'll feel that it would be veryã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšå¯ä»¥è®©ä½ åœ¨ä»»ä½•ä½ æƒ³æ‰“å°çš„åœ°æ–¹æ‰“å°å½¢çŠ¶ã€‚ä¸è¿‡ï¼Œè®©æˆ‘ä»¬ç°åœ¨åšä¸€äº›æ›´å¤æ‚çš„ä¸œè¥¿ï¼Œæˆ–è€…è¯´ä¸€ä¸ªæ›´å¤§çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªåä¸ºResblockçš„ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ„å»ºä¸€äº›ç±»ä¼¼äºResnetçš„ä¸œè¥¿ï¼Œå¹¶å¸Œæœ›åœ¨è¿™ä¸ªè§†é¢‘ç»“æŸæ—¶ï¼Œä½ èƒ½å¤Ÿä»è¿™æ ·çš„è§’åº¦çœ‹å¾…ï¼šå¦‚æœæˆ‘ä»¬ä½¿ç”¨åŠŸèƒ½æ€§æˆ–é¡ºåºAPIé‡å»ºä¸€äº›ç±»ä¼¼çš„å†…å®¹ï¼Œæˆ–è®¸ä½ ä¼šè§‰å¾—è¿™éå¸¸å¥½ã€‚
- en: very difficult or take a lot of space and not to be as compact as we're doing
    it in this video So if you're not familiar with Resnetã€‚ there's a video in the
    description that you can watch so you're more familiar we're not going to reconstruct
    Resnet but we're going to do something similar to Resnet So anyways we're going
    to do layer dot layer againã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸å›°éš¾æˆ–å ç”¨å¾ˆå¤šç©ºé—´ï¼Œå¹¶ä¸åƒæˆ‘ä»¬åœ¨è¿™ä¸ªè§†é¢‘ä¸­é‚£æ ·ç´§å‡‘ã€‚å› æ­¤ï¼Œå¦‚æœä½ ä¸ç†Ÿæ‚‰Resnetï¼Œå¯ä»¥è§‚çœ‹æè¿°ä¸­çš„è§†é¢‘ï¼Œä»¥ä¾¿æ›´ç†Ÿæ‚‰ã€‚æˆ‘ä»¬ä¸ä¼šé‡å»ºResnetï¼Œè€Œæ˜¯åšä¸€äº›ç±»ä¼¼äºResnetçš„äº‹æƒ…ã€‚æ‰€ä»¥æˆ‘ä»¬å†æ¬¡è¿›è¡Œå±‚å å±‚ã€‚
- en: And then we're going to do in itã€‚And then let's doï¼Œ we're gonna sending channelsã€‚
    Alrightã€‚ so let's do that one res blockã€‚ And first of allï¼Œ we got it called theã€‚Super
    in itã€‚ And then let's say that the res block is gonna have three of theseã€‚ these
    CNN blocks that we createdã€‚ Alrightï¼Œ so we could do something like we could send
    in channels right here And let's say it's defaultã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†è¿›è¡Œåˆå§‹åŒ–ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è®¾ç½®é€šé“ã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥åšä¸€ä¸ªres blockã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è°ƒç”¨super initã€‚ç„¶åå‡è®¾è¿™ä¸ªres blockå°†åŒ…å«ä¸‰ä¸ªæˆ‘ä»¬åˆ›å»ºçš„CNNå—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œä¼ é€’é€šé“ï¼Œå‡è®¾å®ƒçš„é»˜è®¤å€¼ã€‚
- en: I du't knowï¼Œ32ã€‚6428ï¼Œ something like thatã€‚ And then we're gonna doã€‚Or actuallyã€‚
    let's not initialize themã€‚ Let's just say we have channelsã€‚ It's going to be a
    list of three valuesã€‚ and we're going to create three CNN blocksã€‚ So let's do
    self CNNN 1 is CNN blockã€‚ and we're going to do channelsã€‚Channels0 and then kernel
    size 3ã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“ï¼Œ32ï¼Œ6428ï¼Œç±»ä¼¼è¿™æ ·çš„ã€‚ç„¶åæˆ‘ä»¬å°†åšä¸€äº›ã€‚å…¶å®ï¼Œè®©æˆ‘ä»¬ä¸åˆå§‹åŒ–å®ƒä»¬ã€‚å‡è®¾æˆ‘ä»¬æœ‰é€šé“ã€‚å®ƒå°†æ˜¯ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå€¼çš„åˆ—è¡¨ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸‰ä¸ªCNNå—ã€‚æ‰€ä»¥è®©æˆ‘ä»¬åšself
    CNNN 1æ˜¯CNNå—ï¼Œç„¶åæˆ‘ä»¬å°†ä½¿ç”¨é€šé“ï¼Œé€šé“0å’Œå·ç§¯æ ¸å¤§å°3ã€‚
- en: and we don't actually have to specify thatã€‚ So rememberï¼Œ we use the keyword
    argument hereã€‚ and we set it to3 so we can just keep it like that and then self
    dot CNN N 2ã€‚ We're going do another blockã€‚ and let's use channels 1ã€‚And then self
    CNN3ã€‚ we're going to do CNN blockã€‚Channels2ã€‚And then we could do something like
    poolingã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”æˆ‘ä»¬å®é™…ä¸Šä¸éœ€è¦æŒ‡å®šè¿™ä¸€ç‚¹ã€‚æ‰€ä»¥è®°ä½ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å…³é”®å­—å‚æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸º3ï¼Œå› æ­¤å¯ä»¥ä¿æŒè¿™æ ·ï¼Œç„¶åæ˜¯self.dot CNN N 2ã€‚æˆ‘ä»¬å°†åšå¦ä¸€ä¸ªå—ï¼Œä½¿ç”¨é€šé“1ã€‚ç„¶åself
    CNN3ï¼Œæˆ‘ä»¬å°†è¿›è¡ŒCNNå—ï¼Œé€šé“2ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥åšä¸€äº›ç±»ä¼¼æ± åŒ–çš„æ“ä½œã€‚
- en: Layers dot max oing 2 dã€‚ğŸ˜”ï¼ŒAnd then soã€‚We're going to run three of these blocks
    where each block is a CNNã€‚ a batchor and then Reluï¼Œ and then we're going to run
    a max pool to sort of have the input size in the height and the widthã€‚And then
    we're going to use the an identity mapping similar to Resnets with these skip
    connectionsã€‚ so what we're going to do thereã€‚Is that we got to do an identity
    mapping so that there it has the same number of channelsã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Layers.dot max oing 2 dã€‚ğŸ˜”ï¼Œç„¶åæ‰€ä»¥ã€‚æˆ‘ä»¬å°†è¿è¡Œä¸‰ä¸ªè¿™æ ·çš„å—ï¼Œæ¯ä¸ªå—éƒ½æ˜¯ä¸€ä¸ªCNNï¼Œä¸€ä¸ªbatchorï¼Œç„¶åæ˜¯Reluï¼Œæ¥ç€æˆ‘ä»¬å°†è¿è¡Œä¸€ä¸ªæœ€å¤§æ± åŒ–ï¼Œä»¥ä¿æŒè¾“å…¥å°ºå¯¸çš„é«˜åº¦å’Œå®½åº¦ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªèº«ä»½æ˜ å°„ï¼Œç±»ä¼¼äºResnetsï¼Œé…åˆè¿™äº›è·³è·ƒè¿æ¥ã€‚å› æ­¤æˆ‘ä»¬è¦åšçš„æ˜¯è¿›è¡Œèº«ä»½æ˜ å°„ï¼Œä»¥ç¡®ä¿å®ƒå…·æœ‰ç›¸åŒçš„é€šé“æ•°é‡ã€‚
- en: So remember we're not doing so we're using same convolution so the height and
    the width wont won't changeã€‚ but the channels mightã€‚ So we need to do a identity
    mapping what we're going to do layers come to Dã€‚ We're just going specify channels
    1ã€‚And then we're going to do kernel size 3ã€‚ adding equals the sameã€‚And then for
    the callï¼Œ so if this is a little bitã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®°ä½ï¼Œæˆ‘ä»¬ä¸æ”¹å˜ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„å·ç§¯ï¼Œæ‰€ä»¥é«˜åº¦å’Œå®½åº¦ä¸ä¼šæ”¹å˜ã€‚ä½†é€šé“å¯èƒ½ä¼šæ”¹å˜ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦è¿›è¡Œèº«ä»½æ˜ å°„ï¼Œæˆ‘ä»¬å°†è®¾ç½®å±‚æ¥Dã€‚æˆ‘ä»¬åªæ˜¯æŒ‡å®šé€šé“ä¸º1ã€‚ç„¶åæˆ‘ä»¬å°†åšå·ç§¯æ ¸å¤§å°ä¸º3ã€‚æ·»åŠ ä¿æŒç›¸åŒã€‚ç„¶åå¯¹äºè°ƒç”¨ï¼Œå¦‚æœè¿™ä¸€ç‚¹æœ‰äº›ã€‚
- en: this doesn't feel super clear right nowï¼Œ don't worryï¼Œ I'm going to explain it
    againã€‚ but we're going to do the calm methodï¼Œ we're going to do an inputtensor
    and we're going to specify training to be false as defaultã€‚And then we're going
    to do x is selfã€‚ CNNnn1ï¼Œ the input we're going to send the input tensor through
    the first CNNn blockã€‚And then we're going to do training equals trainingã€‚ We're
    going to do cells that CNN 2 of xã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™æ„Ÿè§‰ä¸æ˜¯å¾ˆæ¸…æ¥šï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä¼šå†è§£é‡Šä¸€éã€‚ä½†æˆ‘ä»¬å°†ä½¿ç”¨å†·é™çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªè¾“å…¥å¼ é‡ï¼Œå¹¶å°†è®­ç»ƒé»˜è®¤è®¾ç½®ä¸ºfalseã€‚ç„¶åæˆ‘ä»¬å°†åšxç­‰äºselfã€‚CNNnn1ï¼Œæˆ‘ä»¬å°†æŠŠè¾“å…¥å¼ é‡ä¼ é€’é€šè¿‡ç¬¬ä¸€ä¸ªCNNnå—ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†è®­ç»ƒç­‰äºè®­ç»ƒã€‚æˆ‘ä»¬å°†å¯¹xçš„CNN
    2è¿›è¡Œå¤„ç†ã€‚
- en: then againï¼Œ specify trainingã€‚And then we're going to run it through the last
    oneï¼Œ all rightã€‚ and for this oneï¼Œ we're going to do xï¼Œ but we're also going to
    add this identity mappingã€‚We're gonna do inputã€‚tenensor and then we're gonna specify
    training equals trainingã€‚ So what we're doing here is that we're using these skip
    connections right hereã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå†æ¬¡æŒ‡å®šè®­ç»ƒã€‚ç„¶åæˆ‘ä»¬å°†å…¶é€šè¿‡æœ€åä¸€ä¸ªï¼Œå¥½çš„ã€‚å¯¹äºè¿™ä¸ªï¼Œæˆ‘ä»¬å°†åšxï¼Œä½†æˆ‘ä»¬ä¹Ÿä¼šæ·»åŠ è¿™ä¸ªèº«ä»½æ˜ å°„ã€‚æˆ‘ä»¬å°†åšinput.tensotï¼Œç„¶åæˆ‘ä»¬å°†æŒ‡å®šè®­ç»ƒç­‰äºè®­ç»ƒã€‚å› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„æ˜¯ä½¿ç”¨è¿™äº›è·³è·ƒè¿æ¥ã€‚
- en: but for this one it has passed through this CNNnn2 and changing the number of
    channels that it was originally to the number of channels that we send in through
    this list So this integer channels of index1 so that's all we're doing here running
    it through the CNN block first second block and then to match number of channels
    so that we can actually do this additionã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹äºè¿™ä¸ªï¼Œå®ƒå·²ç»é€šè¿‡CNNnn2å¹¶å°†åŸå§‹çš„é€šé“æ•°é‡æ›´æ”¹ä¸ºé€šè¿‡è¿™ä¸ªåˆ—è¡¨ä¼ å…¥çš„é€šé“æ•°é‡ã€‚å› æ­¤ï¼Œè¿™ä¸ªæ•´æ•°é€šé“çš„index1ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬è¦åšçš„ï¼Œå°†å…¶é€šè¿‡CNNå—è¿è¡Œï¼Œç„¶ååŒ¹é…é€šé“æ•°é‡ï¼Œä»¥ä¾¿æˆ‘ä»¬å®é™…å¯ä»¥è¿›è¡Œè¿™ä¸ªåŠ æ³•ã€‚
- en: we're first running the input tensor through an identity mapping and then we're
    adding that to xã€‚ All right so when editing this I actually see that there's a
    mistake here So for the identity mapping we use the kernel size of three which
    shouldn't be the case that then we're actually doing a comp layer but we just
    want to do in identity mappingã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå°†è¾“å…¥å¼ é‡é€šè¿‡èº«ä»½æ˜ å°„ï¼Œç„¶åå°†å…¶æ·»åŠ åˆ°xã€‚å¥½çš„ï¼Œæ‰€ä»¥åœ¨ç¼–è¾‘è¿™ä¸ªæ—¶ï¼Œæˆ‘å®é™…ä¸Šå‘ç°è¿™é‡Œæœ‰ä¸ªé”™è¯¯ã€‚å› æ­¤ï¼Œå¯¹äºèº«ä»½æ˜ å°„ï¼Œæˆ‘ä»¬ä½¿ç”¨å·ç§¯æ ¸å¤§å°ä¸º3ï¼Œè¿™ä¸åº”è¯¥æ˜¯è¿™æ ·ï¼Œç„¶åæˆ‘ä»¬å®é™…ä¸Šåœ¨åšä¸€ä¸ªcompå±‚ï¼Œä½†æˆ‘ä»¬åªæ˜¯æƒ³åšä¸€ä¸ªèº«ä»½æ˜ å°„ã€‚
- en: changing the number of channels as output So what you would do here is you would
    actually change this kernel size to one and in this way the only thing that it
    would actually do isã€‚Is it sort of doing identity mapping and changing the number
    of channelsã€‚ And then at the endã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¹å˜è¾“å‡ºçš„é€šé“æ•°é‡ã€‚æ‰€ä»¥åœ¨è¿™é‡Œä½ éœ€è¦åšçš„å°±æ˜¯å°†è¿™ä¸ªå·ç§¯æ ¸å¤§å°æ”¹ä¸º1ï¼Œè¿™æ ·å”¯ä¸€çš„ä½œç”¨å°±æ˜¯è¿›è¡Œèº«ä»½æ˜ å°„å¹¶æ”¹å˜é€šé“çš„æ•°é‡ã€‚ç„¶ååœ¨æœ€åã€‚
- en: let's just do this pooling of X and let's return that So return self pooling
    of xã€‚Alrightã€‚ so so far we have hadï¼Œ let's remove thatã€‚ We're not going to use
    that againã€‚ So so far we've done the CNNM blockã€‚ we've done this res blockã€‚ and
    then let's do theã€‚ let's do the final sort of the modelï¼Œ rightï¼Ÿ So we're gonna
    do class resnetã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…ˆè¿›è¡ŒXçš„æ± åŒ–ï¼Œç„¶åè¿”å›ç»“æœã€‚å› æ­¤è¿”å›selfçš„Xæ± åŒ–ã€‚å¥½çš„ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»è¿›è¡Œäº†ï¼Œå»æ‰é‚£ä¸ªã€‚æˆ‘ä»¬ä¸ä¼šå†ä½¿ç”¨å®ƒã€‚å› æ­¤åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†CNNMå—ã€‚æˆ‘ä»¬å®Œæˆäº†è¿™ä¸ªreså—ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬åšæœ€åçš„æ¨¡å‹ï¼Œå¯¹å—ï¼Ÿæ‰€ä»¥æˆ‘ä»¬è¦åšçš„æ˜¯class
    resnetã€‚
- en: Let's called resonant like and then now for the parent method we're going to
    use ks do model All right so this is for the layersã€‚ we're going to use layersã€‚
    layer meaning we're not going to use that as our final model so when we're inheriting
    from ksã€‚ model it has the functionality that layers that layer hasã€‚ but it also
    has some additional functionality so for example we have builtin training evaluation
    so that we can use what we're familiar with in a model do fit modelevaluate modelã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°ä¹‹ä¸ºå…±æŒ¯ï¼Œç°åœ¨å¯¹äºçˆ¶æ–¹æ³•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ksåšæ¨¡å‹ã€‚å¥½çš„ï¼Œè¿™é€‚ç”¨äºå±‚ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å±‚ï¼Œå±‚çš„æ„æ€æ˜¯æˆ‘ä»¬ä¸ä¼šæŠŠå®ƒä½œä¸ºæœ€ç»ˆæ¨¡å‹ï¼Œå› æ­¤å½“æˆ‘ä»¬ä»ksæ¨¡å‹ç»§æ‰¿æ—¶ï¼Œå®ƒå…·æœ‰å±‚çš„åŠŸèƒ½ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›é¢å¤–çš„åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰å†…ç½®çš„è®­ç»ƒè¯„ä¼°ï¼Œä»¥ä¾¿å¯ä»¥åœ¨æ¨¡å‹ä¸­ä½¿ç”¨æˆ‘ä»¬ç†Ÿæ‚‰çš„`do
    fit`å’Œ`model evaluate`ã€‚
- en: pre so those are available if you inherit from ks do model but not if you do
    layers layer and you also have other properties so that you can do for example
    model layers checking all of the layers in the model model summary and you can
    also do things like serialization and then saving your model we're going to cover
    that in another video though butã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›åœ¨ä»ksåšæ¨¡å‹ç»§æ‰¿æ—¶æ˜¯å¯ç”¨çš„ï¼Œä½†å¦‚æœä½ åªåšå±‚ï¼Œåˆ™ä¸å¯ç”¨ã€‚æ­¤å¤–ï¼Œä½ è¿˜æœ‰å…¶ä»–å±æ€§ï¼Œå¯ä»¥ä¾‹å¦‚æ£€æŸ¥æ¨¡å‹ä¸­çš„æ‰€æœ‰å±‚ã€æ¨¡å‹æ‘˜è¦ï¼Œå¹¶ä¸”ä½ ä¹Ÿå¯ä»¥åšåºåˆ—åŒ–ï¼Œç„¶åä¿å­˜ä½ çš„æ¨¡å‹ï¼Œè™½ç„¶æˆ‘ä»¬å°†åœ¨å¦ä¸€ä¸ªè§†é¢‘ä¸­è®¨è®ºè¿™ä¸ªã€‚
- en: Just know that CAs that model has additional functionality that layer that layer
    doesn't haveã€‚ and you should use CAs that model on your final model and hopefully
    that makes senseã€‚So what we're going to do hereï¼Œ we're going to define in itã€‚
    and here we're going to specify the number of classesï¼Œ so let's say 10ã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€çŸ¥é“CAsæ¨¡å‹å…·æœ‰å±‚æ‰€æ²¡æœ‰çš„é¢å¤–åŠŸèƒ½ã€‚ä½ åº”è¯¥åœ¨æœ€ç»ˆæ¨¡å‹ä¸Šä½¿ç”¨CAsæ¨¡å‹ï¼Œå¸Œæœ›è¿™èƒ½è®©ä½ æ˜ç™½ã€‚é‚£ä¹ˆæˆ‘ä»¬è¦åšçš„å°±æ˜¯åœ¨è¿™é‡Œå®šä¹‰å®ƒï¼Œå¹¶åœ¨è¿™é‡ŒæŒ‡å®šç±»åˆ«çš„æ•°é‡ï¼Œæ¯”å¦‚è¯´10ã€‚
- en: and then we're going to firstï¼Œ let's see we're going to call super of Resnet
    likeã€‚And thenï¼Œ selfã€‚ and thenã€‚In itã€‚ And then we're going to do self the block1
    is going to be res blockã€‚OfLet's seeã€‚ So this is something you can play around
    withï¼Œ let's just say 32ï¼Œ 32ï¼Œ 64ã€‚And then block twoã€‚ let's do rest blockã€‚128ï¼Œ128
    to 56ã€‚And thenï¼Œ block 3ã€‚ğŸ˜”ï¼ŒRes blocklock128256512ã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬é¦–å…ˆï¼Œçœ‹çœ‹æˆ‘ä»¬å°†è°ƒç”¨Resnetçš„superã€‚ç„¶åï¼Œselfã€‚ç„¶åï¼Œåˆå§‹åŒ–ã€‚æ¥ç€self.block1å°†æ˜¯reså—ã€‚æˆ‘ä»¬çœ‹çœ‹ã€‚è¿™æ˜¯ä½ å¯ä»¥å°è¯•çš„å†…å®¹ï¼Œæ¯”å¦‚è¯´32ã€32ã€64ã€‚ç„¶åblock
    2ï¼Œè®¾ä¸ºreså—128ã€128åˆ°56ã€‚ç„¶åï¼Œblock 3ï¼ŒReså—128ã€256ã€512ã€‚
- en: So we're just specifying the channels for each of the the CNN CNN CNN blocks
    in this res block and so you can sort of see that we're scaling this and making
    this bigger and bigger and it might become difficult to sort of understand what
    we're doing but if you just step through it step by step we first created the
    CNN block just using combat re because we want to reuse it for different numbers
    of channels then we built this res block that it uses these these blocks multiple
    times and also a pooling layer together with this identity mapping and then we're
    just using that block in the resnet like model and then at the end we're going
    to do self dot pool we're going to do layers global average pooling 2D and you
    can read about this but this is essentially gonna averageã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæ˜¯ä¸ºè¿™ä¸ªreså—ä¸­çš„æ¯ä¸ªCNNå—æŒ‡å®šé€šé“ï¼Œå› æ­¤ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åœ¨æ‰©å±•è¿™ä¸ªå—ï¼Œå¯èƒ½ä¼šå˜å¾—éš¾ä»¥ç†è§£ï¼Œä½†å¦‚æœä½ ä¸€æ­¥ä¸€æ­¥æ¥ï¼Œé¦–å…ˆæˆ‘ä»¬åˆ›å»ºCNNå—ï¼Œä»…ä½¿ç”¨combat
    reï¼Œå› ä¸ºæˆ‘ä»¬æƒ³ä¸ºä¸åŒçš„é€šé“æ•°é‡é‡ç”¨å®ƒã€‚ç„¶åæˆ‘ä»¬æ„å»ºè¿™ä¸ªreså—ï¼Œå®ƒå¤šæ¬¡ä½¿ç”¨è¿™äº›å—ï¼Œå¹¶ä¸è¿™ä¸ªèº«ä»½æ˜ å°„ä¸€èµ·ä½¿ç”¨ä¸€ä¸ªæ± åŒ–å±‚ï¼Œæœ€åæˆ‘ä»¬åœ¨resnetæ ·å¼çš„æ¨¡å‹ä¸­ä½¿ç”¨è¿™ä¸ªå—ï¼Œæœ€åæˆ‘ä»¬å°†æ‰§è¡Œself.dot
    poolï¼Œå°†æ‰§è¡Œlayers.global average pooling 2Dï¼Œä½ å¯ä»¥é˜…è¯»æœ‰å…³æ­¤å†…å®¹çš„èµ„æ–™ï¼Œä½†æœ¬è´¨ä¸Šè¿™ä¼šè¿›è¡Œå¹³å‡ã€‚
- en: Pull the height and the widthã€‚ Then we're just going to do aï¼Œ for exampleï¼Œ you
    could replace a flatã€‚ So instead of doing thisï¼Œ you could doã€‚Layersã€‚ flatten if
    you feel more comfortable with that if you so we're using it sort of in the same
    functionality that we want to flatten it or to downscale the heighten the width
    and then send it through a last classification layerã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹‰åŠ¨é«˜åº¦å’Œå®½åº¦ã€‚ç„¶åæˆ‘ä»¬å°†åšä¸€ä¸ªï¼Œä¾‹å¦‚ï¼Œä½ å¯ä»¥æ›¿æ¢ä¸€ä¸ªå¹³å¦å±‚ã€‚æ‰€ä»¥ä¸ä½¿ç”¨è¿™ä¸ªï¼Œä½ å¯ä»¥åšlayers.flattenï¼Œå¦‚æœä½ å¯¹æ­¤æ›´ä¸ºèˆ’é€‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»¥ç›¸åŒçš„åŠŸèƒ½ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å¸Œæœ›å°†å…¶å¹³å¦åŒ–æˆ–ç¼©å°é«˜åº¦å’Œå®½åº¦ï¼Œç„¶åé€šè¿‡æœ€åçš„åˆ†ç±»å±‚å‘é€å®ƒã€‚
- en: so that's what we're going to do now we're going to do self dot classifier layers
    dense and then of number of classesã€‚And so now what's all that's left is to do
    the callï¼Œ so we're going to do input tensorã€‚ we're going to do trainingï¼Œ set it
    test default falseã€‚And thenã€‚We're going to do Se to block one of input tensorã€‚
    We're going to specify trainingã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨æˆ‘ä»¬è¦åšçš„æ˜¯æ‰§è¡Œ`self.dot classifier layers dense`å’Œç±»çš„æ•°é‡ã€‚ç°åœ¨å‰©ä¸‹çš„å°±æ˜¯è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦è¿›è¡Œè¾“å…¥å¼ é‡ï¼ˆinput
    tensorï¼‰ã€‚æˆ‘ä»¬å°†è®¾ç½®è®­ç»ƒï¼Œé»˜è®¤ä¸º`false`ã€‚ç„¶åï¼Œæˆ‘ä»¬è¦æ‰§è¡Œè¾“å…¥å¼ é‡çš„ç¬¬ä¸€ä¸ªå—ï¼ˆblock oneï¼‰ã€‚æˆ‘ä»¬å°†æŒ‡å®šè®­ç»ƒã€‚
- en: And from my understanding specifying training here that's going to be done inside
    this model that fit or model evaluateval we're also going to show in future videos
    how to do custom training loops and so onã€‚ but I think this is done internally
    in model that fit if we just specify training right here that's going to send
    the argument depending on for example here we're training so that would set training
    to true and then when for the evaluation it would then set model to false or rather
    training to falseã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘çš„ç†è§£ï¼Œè¿™é‡ŒæŒ‡å®šè®­ç»ƒå°†åœ¨æ¨¡å‹æ‹Ÿåˆï¼ˆmodel fitï¼‰æˆ–æ¨¡å‹è¯„ä¼°ï¼ˆmodel evaluateï¼‰æ—¶å®Œæˆï¼Œæˆ‘ä»¬å°†åœ¨æœªæ¥çš„è§†é¢‘ä¸­å±•ç¤ºå¦‚ä½•è¿›è¡Œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ç­‰ç­‰ã€‚ä½†æ˜¯æˆ‘è®¤ä¸ºå¦‚æœæˆ‘ä»¬åœ¨è¿™é‡ŒæŒ‡å®šè®­ç»ƒï¼Œè¿™å°†åœ¨æ¨¡å‹æ‹Ÿåˆå†…éƒ¨å®Œæˆï¼Œè¿™ä¼šæ ¹æ®ä¾‹å¦‚è¿™é‡Œçš„è®­ç»ƒå°†å‚æ•°è®¾ç½®ä¸º`true`ï¼Œåœ¨è¯„ä¼°æ—¶åˆ™è®¾ç½®ä¸º`false`ã€‚
- en: And thenã€‚We're going to do block 2 of xï¼Œ again specify trainingã€‚We're going
    to do Se to block 3 of xã€‚ againï¼Œ specify trainingã€‚And then at the endï¼Œ we're going
    to do self dot pool of xã€‚ And then we're going to do x isã€‚Or actually just return
    self dot classifier of xã€‚And what would probably help now is doingã€‚Model that
    summaryã€‚So after the model that fit hereã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è¦è¿›è¡Œè¾“å…¥`x`çš„ç¬¬äºŒä¸ªå—ï¼ˆblock 2ï¼‰ï¼Œå†æ¬¡æŒ‡å®šè®­ç»ƒã€‚æ¥ç€æˆ‘ä»¬è¦è¿›è¡Œè¾“å…¥`x`çš„ç¬¬ä¸‰ä¸ªå—ï¼ˆblock 3ï¼‰ï¼ŒåŒæ ·ï¼ŒæŒ‡å®šè®­ç»ƒã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰§è¡Œ`self.dot
    pool of x`ï¼Œç„¶åå°†`x`è¿”å›ï¼Œå®é™…ä¸Šå°±æ˜¯è¿”å›`self.dot classifier of x`ã€‚æ­¤æ—¶è¿›è¡Œæ¨¡å‹æ‘˜è¦ï¼ˆmodel summaryï¼‰ä¼šæœ‰æ‰€å¸®åŠ©ã€‚åœ¨æ¨¡å‹æ‹Ÿåˆï¼ˆfitï¼‰ä¹‹åã€‚
- en: let's do print model dot summaryã€‚Right thereã€‚ And let's just run it for single
    epochã€‚ First of allã€‚ Alrightï¼Œ yeahï¼Œ we need to also specify our modelã€‚ So we gotta
    do model equals resã€‚Neã€‚Likeã€‚ and then we can do classes equals 10ï¼Œ and now we
    can hopefully run thatã€‚Alrightã€‚ so we can see after one Epoï¼Œ we have 97% on the
    trainingã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‰“å°æ¨¡å‹æ‘˜è¦ï¼ˆmodel dot summaryï¼‰ã€‚å°±åœ¨é‚£é‡Œã€‚æˆ‘ä»¬è®©å®ƒè¿è¡Œä¸€ä¸ªå‘¨æœŸï¼ˆepochï¼‰ã€‚é¦–å…ˆï¼Œå¥½çš„ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æŒ‡å®šæˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‰€ä»¥æˆ‘ä»¬è¦åš`model
    equals res.Ne`ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è®¾ç½®ç±»ä¸º`10`ï¼Œå¸Œæœ›ç°åœ¨å¯ä»¥è¿è¡Œã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç»è¿‡ä¸€ä¸ªå‘¨æœŸåï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡ä¸º`97%`ã€‚
- en: And then here we can see the the sort of theã€‚The layers of our model and we
    can see we have a re blockã€‚ res blockï¼Œ Res blockï¼Œ and then average pullingï¼Œ and
    then thens what's kind of annoying here is that we have multiple output shapes
    and this is usually the case when you're doing subclassingã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªæ®‹å·®å—ï¼ˆres blockï¼‰ï¼Œå†æ˜¯å¹³å‡æ± åŒ–ï¼ˆaverage poolingï¼‰ï¼Œç„¶åæœ‰ç‚¹çƒ¦äººçš„æ˜¯æˆ‘ä»¬æœ‰å¤šä¸ªè¾“å‡ºå½¢çŠ¶ï¼Œè¿™é€šå¸¸å‘ç”Ÿåœ¨ä½ è¿›è¡Œå­ç±»åŒ–æ—¶ã€‚
- en: But I found one workaroundã€‚ I'm not sure if this is the best way to do itã€‚ but
    just for now you can use this so we can do model selfã€‚We're going to do x carriess
    that input shape specifyã€‚ And then let's do 28ï¼Œ28ï¼Œ28ï¼Œ1ã€‚AndThen returnã€‚ ks dot
    model inputs equals xï¼Œ and then outputs equals self dot call of xã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘æ‰¾åˆ°äº†ä¸€ç§å˜é€šæ–¹æ³•ã€‚æˆ‘ä¸ç¡®å®šè¿™æ˜¯å¦æ˜¯æœ€ä½³æ–¹æ³•ï¼Œä½†ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ‰§è¡Œæ¨¡å‹`self`ã€‚æˆ‘ä»¬è¦åšçš„æ˜¯æŒ‡å®šè¾“å…¥å½¢çŠ¶çš„`x`ã€‚ç„¶åè®©æˆ‘ä»¬åš`28,
    28, 28, 1`ã€‚æ¥ç€è¿”å›`ks.dot model inputs equals x`ï¼Œç„¶åè¾“å‡ºä¸º`self.dot call of x`ã€‚
- en: And so what this is going to do is it's going to overwrite the model call and
    then we could do something like model modelã€‚ modelï¼Œ call that and then do dot
    summary and in this way we're actually going to get the output shapes So I'm just
    going to let this run and then we'll see what it looks likeã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¦†ç›–æ¨¡å‹çš„è°ƒç”¨ï¼ˆcallï¼‰ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥åƒè¿™æ ·æ‰§è¡Œæ¨¡å‹è°ƒç”¨ï¼ˆmodel model callï¼‰ï¼Œæ¥ç€åš`.summary`ï¼Œé€šè¿‡è¿™ç§æ–¹å¼æˆ‘ä»¬å®é™…ä¸Šå°†è·å¾—è¾“å‡ºå½¢çŠ¶ã€‚æ‰€ä»¥æˆ‘å°±è®©è¿™ä¸ªè¿è¡Œï¼Œç„¶åæˆ‘ä»¬çœ‹çœ‹ç»“æœæ˜¯ä»€ä¹ˆæ ·çš„ã€‚
- en: Alright so what we can see now is that the output shapes are actually included
    we have none for the number of batchesã€‚ 28281 for the input image after the first
    re block it's 64 number of channels and then it's 14 by 14 so it's included  one
    max pooling and then we're max pulling again where max pulling one more time and
    then that's run through an average pooling so as you can see here as you can see
    here we have in this case 512 channels and then we have heightened width of3 by
    3 we're essentially averaging all of those3 by3512 into a single 512 and then
    we're running that through a single dense layer and total number parameters are
    3 million which is I guess much larger than what we've done but not relatively
    to other models this is still pretty small but so hopefully you can see that if
    you would have built this model right this is a hugeã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¾“å‡ºå½¢çŠ¶å®é™…ä¸Šæ˜¯åŒ…å«çš„ï¼Œæˆ‘ä»¬å¯¹æ‰¹æ¬¡æ•°æ²¡æœ‰ä»»ä½•é™åˆ¶ã€‚ç»è¿‡ç¬¬ä¸€æ¬¡é‡æ„å—åï¼Œè¾“å…¥å›¾åƒçš„å½¢çŠ¶æ˜¯28281ï¼Œé€šé“æ•°ä¸º64ï¼Œæ¥ä¸‹æ¥æ˜¯14ä¹˜14ï¼Œæ‰€ä»¥å®ƒåŒ…æ‹¬äº†ä¸€ä¸ªæœ€å¤§æ± åŒ–ï¼Œç„¶åæˆ‘ä»¬å†æ¬¡è¿›è¡Œäº†æœ€å¤§æ± åŒ–ï¼Œå†è¿›è¡Œäº†ä¸€ä¸ªæœ€å¤§æ± åŒ–ï¼Œç„¶åé€šè¿‡å¹³å‡æ± åŒ–å¤„ç†ã€‚å¦‚ä½ æ‰€è§ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æœ‰512ä¸ªé€šé“ï¼Œå®½åº¦å’Œé«˜åº¦ä¸º3ä¹˜3ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯å°†æ‰€æœ‰çš„3ä¹˜3512é€šé“å¹³å‡æˆä¸€ä¸ªå•ä¸€çš„512ï¼Œç„¶åæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¯†é›†å±‚è¿›è¡Œå¤„ç†ï¼Œæ€»å‚æ•°é‡ä¸º300ä¸‡ï¼Œæˆ‘æƒ³è¿™æ¯”æˆ‘ä»¬ä¹‹å‰åšçš„è¦å¤§å¾—å¤šï¼Œä½†ç›¸å¯¹å…¶ä»–æ¨¡å‹æ¥è¯´ï¼Œè¿™ä»ç„¶ç®—æ˜¯æ¯”è¾ƒå°çš„ã€‚å› æ­¤ï¼Œå¸Œæœ›ä½ èƒ½çœ‹åˆ°ï¼Œå¦‚æœä½ æ„å»ºäº†è¿™ä¸ªæ¨¡å‹ï¼Œé‚£å°†æ˜¯å·¨å¤§çš„ã€‚
- en: And we can't even see so from the model summary we can't actually see all of
    the layers right we're just seeing these blocks but these blocksã€‚ so each block
    has multiple of these CNN blocks right all of these resonant blocks as multiple
    so this model is pretty large if you would plot it and if you would use this I
    mean use the sequential API I don't you can't even use the sequential API I think
    but it would beã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”šè‡³æ— æ³•ä»æ¨¡å‹æ‘˜è¦ä¸­çœ‹åˆ°æ‰€æœ‰å±‚ï¼Œå¯¹å§ï¼Ÿæˆ‘ä»¬åªçœ‹åˆ°è¿™äº›å—ï¼Œä½†è¿™äº›å—ï¼Œæ¯ä¸ªå—éƒ½æœ‰å¤šä¸ªCNNå—ï¼Œæ‰€æœ‰è¿™äº›æ®‹å·®å—éƒ½æ˜¯å¤šä¸ªã€‚å¦‚æœä½ ç»˜åˆ¶è¿™ä¸ªæ¨¡å‹ï¼Œä¼šå‘ç°å®ƒéå¸¸åºå¤§ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯é¡ºåºAPIï¼Œæˆ‘è®¤ä¸ºä½ ç”šè‡³æ— æ³•ä½¿ç”¨é¡ºåºAPIï¼Œä½†å®ƒä¼šæ˜¯ã€‚
- en: It would be a lot more annoying to build thisã€‚ And so hopefully this is a something
    that illustratesã€‚Why doing subclassing can be very good and you also have a lot
    of flexibility that you can pretty much build your models as you want and you
    can also you can print everything during the call and that's very intuitiveã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºè¿™ä¸ªæ¨¡å‹ä¼šæ›´åŠ éº»çƒ¦ã€‚å› æ­¤ï¼Œå¸Œæœ›è¿™èƒ½å¤Ÿè¯´æ˜ä¸ºä»€ä¹ˆè¿›è¡Œå­ç±»åŒ–æ˜¯éå¸¸å¥½çš„ï¼Œä½ ä¹Ÿæœ‰å¾ˆå¤šçµæ´»æ€§ï¼Œå¯ä»¥å‡ ä¹æŒ‰ç…§è‡ªå·±çš„æƒ³æ³•æ„å»ºæ¨¡å‹ï¼Œå¹¶ä¸”ä½ å¯ä»¥åœ¨è°ƒç”¨è¿‡ç¨‹ä¸­æ‰“å°ä¸€åˆ‡ï¼Œè¿™éå¸¸ç›´è§‚ã€‚
- en: I really feel the subclassing is a great way to build your models and in the
    next video we're also going to show how to do these custom layers using this subclassing
    so that for exampleã€‚ let's say we're now using these this dense layer right how
    would you actually go about building a dense layer by yourself and stuff like
    that So that's what we're going to do in next video I thought just for funã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœŸçš„è§‰å¾—å­ç±»åŒ–æ˜¯æ„å»ºæ¨¡å‹çš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬è¿˜å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨å­ç±»åŒ–æ¥åˆ›å»ºè‡ªå®šä¹‰å±‚ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬ç°åœ¨ä½¿ç”¨è¿™ä¸ªå¯†é›†å±‚ï¼Œå®é™…ä¸Šä½ è¯¥å¦‚ä½•è‡ªå·±æ„å»ºä¸€ä¸ªå¯†é›†å±‚ç­‰ç­‰ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬ä¸‹ä¸ªè§†é¢‘è¦åšçš„ï¼Œæˆ‘æƒ³åªæ˜¯ä¸ºäº†å¥½ç©ã€‚
- en: we could run this for a little bit longer and we could see what kind of accuracy
    we can actually get with this the largest model we've built in these tutorials
    so far So let's run this and this is gonna to take a while but it's going to go
    pretty fast for you So yeah I'm going let it run and I'm gonna to wait until it's
    doneã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å†è¿è¡Œä¸€æ®µæ—¶é—´ï¼Œçœ‹çœ‹æˆ‘ä»¬å®é™…ä¸Šèƒ½ä»åˆ°ç›®å‰ä¸ºæ­¢æ„å»ºçš„æœ€å¤§æ¨¡å‹ä¸­è·å¾—ä»€ä¹ˆæ ·çš„å‡†ç¡®åº¦ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹è¿è¡Œï¼Œè¿™ä¼šèŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œä½†å¯¹ä½ æ¥è¯´ä¼šå¾ˆå¿«ã€‚æ‰€ä»¥æˆ‘ä¼šè®©å®ƒè¿è¡Œï¼Œç›´åˆ°å®Œæˆã€‚
- en: Al right so after about 20 epochs we get 99ã€‚84% on the training then 99ã€‚14 on
    the test set I think if you train this for just a little bit longer so that you
    get this up to 99 when I trained it previously I actually got 99ã€‚4% on a test
    set but then I trained for a couple of more epos and then what I think you could
    also do is add regularization and the model would improve but nonetheless I think
    this video really demonstrates the power of subclassing and hopefully you think
    subclassing is awesome after this video thank you so much for watching this video
    and I hope to see you in the next oneã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç»è¿‡å¤§çº¦20ä¸ªå‘¨æœŸï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šå¾—åˆ°äº†99.84%çš„å‡†ç¡®ç‡ï¼Œåœ¨æµ‹è¯•é›†ä¸Šå¾—åˆ°äº†99.14%çš„å‡†ç¡®ç‡ã€‚æˆ‘è®¤ä¸ºå¦‚æœä½ å†è®­ç»ƒä¸€æ®µæ—¶é—´ï¼Œå¯ä»¥å°†å…¶æå‡åˆ°99ã€‚å½“æˆ‘ä¹‹å‰è®­ç»ƒæ—¶ï¼Œæˆ‘åœ¨æµ‹è¯•é›†ä¸Šå®é™…ä¸Šå¾—åˆ°äº†99.4%çš„å‡†ç¡®ç‡ï¼Œä½†ç„¶åæˆ‘åˆè®­ç»ƒäº†å‡ ä¸ªå‘¨æœŸã€‚æˆ‘è®¤ä¸ºä½ è¿˜å¯ä»¥æ·»åŠ æ­£åˆ™åŒ–ï¼Œæ¨¡å‹ä¼šæ”¹å–„ã€‚ä¸è¿‡ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ªè§†é¢‘ç¡®å®å±•ç¤ºäº†å­ç±»åŒ–çš„å¼ºå¤§åŠŸèƒ½ï¼Œå¸Œæœ›ä½ åœ¨è§‚çœ‹å®Œè¿™ä¸ªè§†é¢‘åä¹Ÿè§‰å¾—å­ç±»åŒ–å¾ˆæ£’ã€‚éå¸¸æ„Ÿè°¢ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼Œå¸Œæœ›åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­è§åˆ°ä½ ã€‚
- en: '![](img/2b6b959a397871940a0ba2149e837d11_2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b6b959a397871940a0ba2149e837d11_2.png)'
