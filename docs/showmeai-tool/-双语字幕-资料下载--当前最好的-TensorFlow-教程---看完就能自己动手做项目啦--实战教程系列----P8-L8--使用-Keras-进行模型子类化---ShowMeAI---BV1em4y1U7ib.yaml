- en: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P8：L8- 使用 Keras 进行模型子类化
    - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P8：L8- 使用 Keras 进行模型子类化
    - ShowMeAI - BV1em4y1U7ib
- en: '![](img/2b6b959a397871940a0ba2149e837d11_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b6b959a397871940a0ba2149e837d11_0.png)'
- en: What is going on guys。 Hope you're doing awesome and welcome back in this video。
    We're going to take the next step in building our models and we're going to learn
    about model subclassing。 which is a incredibly flexible way to build models and
    so so far we've touched on the sequential API that has very little flexibility
    but it's very convenient and then we also saw examples on the functional API for
    more flexibility in building our models and in this video we're going to use the
    subclassing which has the most amount of flexibility So I'm just going copy in
    all of the imports and stuff like that so we have seen all of this before so that
    we don't waste too much time on this So OS and for ignoring information messages
    from Tensorflow Keas layers to build our model Ms the data and these two lines
    will help you out if you running into an issues on running on the GPU and then
    we have these three lines right here that are just going to load the dataset。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好！希望你们一切都好，欢迎回来观看这个视频。我们将迈出构建模型的下一步，学习模型子类化。这是一种极其灵活的构建模型的方法。到目前为止，我们接触到了顺序
    API，它的灵活性很小，但使用起来非常方便；我们也看到了功能性 API 的例子，它在构建模型时提供了更多的灵活性。在这个视频中，我们将使用子类化，这是最灵活的方法。所以我会复制所有的导入和其他内容，我们之前都见过这些，确保不浪费太多时间。导入
    OS 和忽略 TensorFlow 的信息消息，使用 Keras 的层来构建我们的模型。这两行代码将帮助你解决在 GPU 上运行时遇到的问题，然后这三行代码将加载数据集。
- en: We're loading MNist and then we're doing a reshape right here。 we're only doing
    the reshape to add this dimension here for the number of channels。 and then we're
    converting it to F 32 and we're dividing by 255 to normalize the data。Alright
    so all of this is something that you've seen before what we want to do now is
    is well first to sort of motivate what we're going to do is we've seen the example
    that we have a convolutional network and then mapping it to or running it through
    batch norm and then running it to rather right this is a common structure right
    that we we've used multiple times and let's say you would do this and you would
    write that code times 10 now that would be a lot of code to write so what we can
    do is we can actually create a class for this kind of block so let's do that and
    let's do class CNN block。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在加载 MNIST 数据集，然后在这里进行形状调整。我们只是在这里调整形状，以添加通道数量的维度，然后将其转换为 F32，并通过 255 进行归一化。好吧，这些都是你之前见过的内容。现在我们想要做的是，首先激励一下我们要做的事情：我们已经看到了一个卷积网络的例子，并将其映射到批量归一化，然后再运行。这个结构我们已经使用了多次，假设你需要写这段代码
    10 次，那将会写出很多代码。所以我们实际上可以为这种块创建一个类。让我们来做这个，定义类 CNN 块。
- en: And we're going to inherit from layers thatt layer。And what this is going to
    do is it's going to keep track of everything under hood for doing back propagation
    and all of that stuff。And then。We're just going to create an init function and
    we're going to specify the number of out channels and then let's say the kernel
    size。So if you're familiar with Pytorch subclassing is using Kra subclassing is
    essentially exactly the same as creating Pytorch models。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从层继承，这将跟踪所有进行反向传播等操作的内容。然后，我们创建一个初始化函数，指定输出通道的数量，以及内核大小。如果你熟悉 PyTorch，子类化的使用与创建
    PyTorch 模型是完全一样的。
- en: so we create our in function first and we have information about how we want
    to build this block and we're going to do super to run the parent。Parent class
    layer that layer， and then we're going to do CNNM block self。In it。And we're going
    to create our comb layer。 So Seph。 co is layers Com2 D。And we're and we're going
    to specify the out channels and then the kernel sides。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们创建我们的初始化函数，了解如何构建这个模块，并使用 super 来运行父类层。然后我们将做 CNNM 块的初始化，并创建我们的组合层。因此，Seph.co
    是 layers.Com2D，我们将指定输出通道和内核大小。
- en: And then let's do padding equals the same。And then for the batch room。 we're
    just going to do layers dot batch normalization。And then we're just going to do
    a call method。 so we're going to do call input tensor， and then we're going to
    specify training。 and let's just set it to default as training to default false。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们让填充保持不变。然后对于批归一化，我们将只做 layers.batch.normalization。然后我们将只做调用方法。所以我们将做调用输入张量，然后我们将指定训练。我们将其默认设置为训练，默认值为
    false。
- en: So this is we're specifying training here because we're using batch norm。 how
    batch norm works is different when it's in training mode or evaluation mode but
    anyways。 we have a call method right here and。The call method， if you're used
    Pytorch。 is essentially the forward method。But anyway here。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这里指定训练，因为我们使用了批归一化。批归一化的工作方式在训练模式和评估模式下是不同的，但无论如何。我们有一个调用方法在这里。调用方法，如果你熟悉
    Pytorch，本质上就是前向方法。但无论如何。
- en: so we're initializing the modules we're going to use in this case comm 2 D and
    batchor and then in the call method we're taking some input tensor and we're going
    to run it through those layers so we're going to do x equals self。com of input
    tens， then we're going to do self。baor of x。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们正在初始化要使用的模块，这里是 comm 2 D 和批归一化。然后在调用方法中，我们接受一些输入张量，并将其通过这些层运行。因此，我们将做 x
    等于 self.com 的输入张量，然后我们将做 self.baor 的 x。
- en: and then we're going to specify training equals training and then we're going
    to do x equals Tf。nn。 Relu。Of x。So we're doing comm batch numberrlu， and then
    we're going to return X。That's the structure that we saw right here。Although now
    we can reuse it multiple times from this class。 so for example， we could do something
    like。Our model is Kra dot sequential。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将指定训练等于训练，然后我们将做 x 等于 Tf.nn.Relu.Of x。所以我们正在进行批归一化的 ReLU，然后我们将返回 X。这就是我们在这里看到的结构。虽然现在我们可以多次从这个类中重用它。例如，我们可以做一些类似的事情。我们的模型是
    Kra.dot.sequential。
- en: And we're going to do CNN block and let's say we have 32 out channels。CNN blocklock
    64 out channels。CNM block 128 and then layers flatten。And then layers1s 10 nodes。
    Allright， so I mean。 if you would write this code right here， just for this block，
    it would be multiple lines。 three lines， right， it would be for theumbmber， the
    batch norm and the the Relu which would be quite annoying would just take up unnecessary
    space and not be as concise So this is one way we can do it and we could we could
    also specify now so we could do model dot compile。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要做 CNN 块，假设我们有 32 个输出通道。CNN 块 64 个输出通道。CNN 块 128，然后层展平。然后层1s 10 个节点。好的，如果你在这里写这段代码，仅仅为了这个块，它将是多行。三行，对吧，它将是批归一化、ReLU
    和其他的，这会非常麻烦，会占用不必要的空间，并且不那么简洁。所以这是我们可以做到的一种方式，我们也可以现在指定，所以我们可以做 model.compile。
- en: we can do optimizer equals ks do optimizes do atom and we could do loss equals
    ks losses spars。Pateorical cross entropy， then from logicits equals true。Metrics，
    we're gonna use accuracy as metric。 and then we could do model that fit。On our
    x train， y train。Bch size，64 epochs。I don't know，3。 and then verbose equals。2，
    and then let's also do model dot evaluate x test， y test。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做优化器等于 ks.do.optimize.do.atom，我们可以做损失等于 ks.losses.spars.Pateorical.cross.entropy，然后从逻辑上讲等于
    true。指标，我们将使用准确率作为指标。然后我们可以做 model.fit。在我们的 x train，y train。批大小，64 轮次。我不知道，3。然后详细程度等于
    2，然后让我们也做 model.evaluate x test，y test。
- en: batch size or 64 where both equals 2。 and then let's run that。So after 3 bucks。
    we get 99% training and then 98。75 on the test set。So what's pretty amazing as
    well in creating these blocks and doing these call methods is that it kind of
    feels like first of all it's pretty much like Pytors if you're used to that but
    if you're used to Nmpy it kind of feels like you're you're using Nmpy but I've
    also heard that it kind of feels like objectoriented Nmpy and I would agree with
    that because it feels very intuitive we can do things like print X dot shape and
    we can run that and then it would each time this is called it would then print
    the shape of x at that point and so this CNN block is run multiple times depending
    on the channels right so first we had 32 then 64 and then 128 and then it's rerun
    again so 32 6428 which makes debugging incredibly simple。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 批大小为64，两个都等于2。然后我们运行它。所以经过3个块后，我们在训练集上获得99%的准确率，在测试集上为98.75%。创造这些块并调用这些方法的惊人之处在于，它感觉非常像Pytors，如果你习惯了，但如果你习惯于Nmpy，它又像是在使用Nmpy。我听说这有点像面向对象的Nmpy，我同意，因为这感觉非常直观，我们可以做诸如打印X.dot
    shape的事情，每次调用时，它将打印出此时x的形状，而这个CNN块根据通道运行多次，第一次是32，然后是64，再到128，然后再次运行，因此是32，6428，这使得调试变得非常简单。
- en: it makes it so that you can just print the shapes wherever you feel like printing
    it。But anyways。 let let's now make something more complicated or I guess more
    a bigger model。 So we're gonna to do class something called Resblock so this is
    be we're going to build something similar to Resnet and hopefully at the end of
    this video you can sort of view it from the perspective that what if we use the
    functional or just sequential API to reconstruct something similar as we're doing
    in this video and hopefully you'll feel that it would be very。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做可以让你在任何你想打印的地方打印形状。不过，让我们现在做一些更复杂的东西，或者说一个更大的模型。我们要创建一个名为Resblock的类，所以我们将构建一些类似于Resnet的东西，并希望在这个视频结束时，你能够从这样的角度看待：如果我们使用功能性或顺序API重建一些类似的内容，或许你会觉得这非常好。
- en: very difficult or take a lot of space and not to be as compact as we're doing
    it in this video So if you're not familiar with Resnet。 there's a video in the
    description that you can watch so you're more familiar we're not going to reconstruct
    Resnet but we're going to do something similar to Resnet So anyways we're going
    to do layer dot layer again。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 非常困难或占用很多空间，并不像我们在这个视频中那样紧凑。因此，如果你不熟悉Resnet，可以观看描述中的视频，以便更熟悉。我们不会重建Resnet，而是做一些类似于Resnet的事情。所以我们再次进行层叠层。
- en: And then we're going to do in it。And then let's do， we're gonna sending channels。
    Alright。 so let's do that one res block。 And first of all， we got it called the。Super
    in it。 And then let's say that the res block is gonna have three of these。 these
    CNN blocks that we created。 Alright， so we could do something like we could send
    in channels right here And let's say it's default。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行初始化。接下来，让我们设置通道。好的，所以我们来做一个res block。首先，我们需要调用super init。然后假设这个res block将包含三个我们创建的CNN块。因此，我们可以在这里传递通道，假设它的默认值。
- en: I du't know，32。6428， something like that。 And then we're gonna do。Or actually。
    let's not initialize them。 Let's just say we have channels。 It's going to be a
    list of three values。 and we're going to create three CNN blocks。 So let's do
    self CNNN 1 is CNN block。 and we're going to do channels。Channels0 and then kernel
    size 3。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道，32，6428，类似这样的。然后我们将做一些。其实，让我们不初始化它们。假设我们有通道。它将是一个包含三个值的列表。我们将创建三个CNN块。所以让我们做self
    CNNN 1是CNN块，然后我们将使用通道，通道0和卷积核大小3。
- en: and we don't actually have to specify that。 So remember， we use the keyword
    argument here。 and we set it to3 so we can just keep it like that and then self
    dot CNN N 2。 We're going do another block。 and let's use channels 1。And then self
    CNN3。 we're going to do CNN block。Channels2。And then we could do something like
    pooling。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 而且我们实际上不需要指定这一点。所以记住，我们在这里使用关键字参数，并将其设置为3，因此可以保持这样，然后是self.dot CNN N 2。我们将做另一个块，使用通道1。然后self
    CNN3，我们将进行CNN块，通道2。接下来我们可以做一些类似池化的操作。
- en: Layers dot max oing 2 d。😔，And then so。We're going to run three of these blocks
    where each block is a CNN。 a batchor and then Relu， and then we're going to run
    a max pool to sort of have the input size in the height and the width。And then
    we're going to use the an identity mapping similar to Resnets with these skip
    connections。 so what we're going to do there。Is that we got to do an identity
    mapping so that there it has the same number of channels。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Layers.dot max oing 2 d。😔，然后所以。我们将运行三个这样的块，每个块都是一个CNN，一个batchor，然后是Relu，接着我们将运行一个最大池化，以保持输入尺寸的高度和宽度。然后我们将使用一个身份映射，类似于Resnets，配合这些跳跃连接。因此我们要做的是进行身份映射，以确保它具有相同的通道数量。
- en: So remember we're not doing so we're using same convolution so the height and
    the width wont won't change。 but the channels might。 So we need to do a identity
    mapping what we're going to do layers come to D。 We're just going specify channels
    1。And then we're going to do kernel size 3。 adding equals the same。And then for
    the call， so if this is a little bit。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以记住，我们不改变，因此我们使用相同的卷积，所以高度和宽度不会改变。但通道可能会改变。因此我们需要进行身份映射，我们将设置层来D。我们只是指定通道为1。然后我们将做卷积核大小为3。添加保持相同。然后对于调用，如果这一点有些。
- en: this doesn't feel super clear right now， don't worry， I'm going to explain it
    again。 but we're going to do the calm method， we're going to do an inputtensor
    and we're going to specify training to be false as default。And then we're going
    to do x is self。 CNNnn1， the input we're going to send the input tensor through
    the first CNNn block。And then we're going to do training equals training。 We're
    going to do cells that CNN 2 of x。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这感觉不是很清楚，不用担心，我会再解释一遍。但我们将使用冷静的方法，我们将使用一个输入张量，并将训练默认设置为false。然后我们将做x等于self。CNNnn1，我们将把输入张量传递通过第一个CNNn块。接着，我们将训练等于训练。我们将对x的CNN
    2进行处理。
- en: then again， specify training。And then we're going to run it through the last
    one， all right。 and for this one， we're going to do x， but we're also going to
    add this identity mapping。We're gonna do input。tenensor and then we're gonna specify
    training equals training。 So what we're doing here is that we're using these skip
    connections right here。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次指定训练。然后我们将其通过最后一个，好的。对于这个，我们将做x，但我们也会添加这个身份映射。我们将做input.tensot，然后我们将指定训练等于训练。因此我们在这里所做的是使用这些跳跃连接。
- en: but for this one it has passed through this CNNnn2 and changing the number of
    channels that it was originally to the number of channels that we send in through
    this list So this integer channels of index1 so that's all we're doing here running
    it through the CNN block first second block and then to match number of channels
    so that we can actually do this addition。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于这个，它已经通过CNNnn2并将原始的通道数量更改为通过这个列表传入的通道数量。因此，这个整数通道的index1，所以这就是我们要做的，将其通过CNN块运行，然后匹配通道数量，以便我们实际可以进行这个加法。
- en: we're first running the input tensor through an identity mapping and then we're
    adding that to x。 All right so when editing this I actually see that there's a
    mistake here So for the identity mapping we use the kernel size of three which
    shouldn't be the case that then we're actually doing a comp layer but we just
    want to do in identity mapping。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将输入张量通过身份映射，然后将其添加到x。好的，所以在编辑这个时，我实际上发现这里有个错误。因此，对于身份映射，我们使用卷积核大小为3，这不应该是这样，然后我们实际上在做一个comp层，但我们只是想做一个身份映射。
- en: changing the number of channels as output So what you would do here is you would
    actually change this kernel size to one and in this way the only thing that it
    would actually do is。Is it sort of doing identity mapping and changing the number
    of channels。 And then at the end。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 改变输出的通道数量。所以在这里你需要做的就是将这个卷积核大小改为1，这样唯一的作用就是进行身份映射并改变通道的数量。然后在最后。
- en: let's just do this pooling of X and let's return that So return self pooling
    of x。Alright。 so so far we have had， let's remove that。 We're not going to use
    that again。 So so far we've done the CNNM block。 we've done this res block。 and
    then let's do the。 let's do the final sort of the model， right？ So we're gonna
    do class resnet。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先进行X的池化，然后返回结果。因此返回self的X池化。好的。到目前为止，我们已经进行了，去掉那个。我们不会再使用它。因此到目前为止，我们已经完成了CNNM块。我们完成了这个res块。接下来让我们做最后的模型，对吗？所以我们要做的是class
    resnet。
- en: Let's called resonant like and then now for the parent method we're going to
    use ks do model All right so this is for the layers。 we're going to use layers。
    layer meaning we're not going to use that as our final model so when we're inheriting
    from ks。 model it has the functionality that layers that layer has。 but it also
    has some additional functionality so for example we have builtin training evaluation
    so that we can use what we're familiar with in a model do fit modelevaluate model。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为共振，现在对于父方法，我们将使用ks做模型。好的，这适用于层。我们将使用层，层的意思是我们不会把它作为最终模型，因此当我们从ks模型继承时，它具有层的功能，但也有一些额外的功能。例如，我们有内置的训练评估，以便可以在模型中使用我们熟悉的`do
    fit`和`model evaluate`。
- en: pre so those are available if you inherit from ks do model but not if you do
    layers layer and you also have other properties so that you can do for example
    model layers checking all of the layers in the model model summary and you can
    also do things like serialization and then saving your model we're going to cover
    that in another video though but。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些在从ks做模型继承时是可用的，但如果你只做层，则不可用。此外，你还有其他属性，可以例如检查模型中的所有层、模型摘要，并且你也可以做序列化，然后保存你的模型，虽然我们将在另一个视频中讨论这个。
- en: Just know that CAs that model has additional functionality that layer that layer
    doesn't have。 and you should use CAs that model on your final model and hopefully
    that makes sense。So what we're going to do here， we're going to define in it。
    and here we're going to specify the number of classes， so let's say 10。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 只需知道CAs模型具有层所没有的额外功能。你应该在最终模型上使用CAs模型，希望这能让你明白。那么我们要做的就是在这里定义它，并在这里指定类别的数量，比如说10。
- en: and then we're going to first， let's see we're going to call super of Resnet
    like。And then， self。 and then。In it。 And then we're going to do self the block1
    is going to be res block。OfLet's see。 So this is something you can play around
    with， let's just say 32， 32， 64。And then block two。 let's do rest block。128，128
    to 56。And then， block 3。😔，Res blocklock128256512。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们首先，看看我们将调用Resnet的super。然后，self。然后，初始化。接着self.block1将是res块。我们看看。这是你可以尝试的内容，比如说32、32、64。然后block
    2，设为res块128、128到56。然后，block 3，Res块128、256、512。
- en: So we're just specifying the channels for each of the the CNN CNN CNN blocks
    in this res block and so you can sort of see that we're scaling this and making
    this bigger and bigger and it might become difficult to sort of understand what
    we're doing but if you just step through it step by step we first created the
    CNN block just using combat re because we want to reuse it for different numbers
    of channels then we built this res block that it uses these these blocks multiple
    times and also a pooling layer together with this identity mapping and then we're
    just using that block in the resnet like model and then at the end we're going
    to do self dot pool we're going to do layers global average pooling 2D and you
    can read about this but this is essentially gonna average。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是为这个res块中的每个CNN块指定通道，因此你可以看到我们在扩展这个块，可能会变得难以理解，但如果你一步一步来，首先我们创建CNN块，仅使用combat
    re，因为我们想为不同的通道数量重用它。然后我们构建这个res块，它多次使用这些块，并与这个身份映射一起使用一个池化层，最后我们在resnet样式的模型中使用这个块，最后我们将执行self.dot
    pool，将执行layers.global average pooling 2D，你可以阅读有关此内容的资料，但本质上这会进行平均。
- en: Pull the height and the width。 Then we're just going to do a， for example， you
    could replace a flat。 So instead of doing this， you could do。Layers。 flatten if
    you feel more comfortable with that if you so we're using it sort of in the same
    functionality that we want to flatten it or to downscale the heighten the width
    and then send it through a last classification layer。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 拉动高度和宽度。然后我们将做一个，例如，你可以替换一个平坦层。所以不使用这个，你可以做layers.flatten，如果你对此更为舒适。因此，我们以相同的功能使用它，我们希望将其平坦化或缩小高度和宽度，然后通过最后的分类层发送它。
- en: so that's what we're going to do now we're going to do self dot classifier layers
    dense and then of number of classes。And so now what's all that's left is to do
    the call， so we're going to do input tensor。 we're going to do training， set it
    test default false。And then。We're going to do Se to block one of input tensor。
    We're going to specify training。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们要做的是执行`self.dot classifier layers dense`和类的数量。现在剩下的就是调用，所以我们要进行输入张量（input
    tensor）。我们将设置训练，默认为`false`。然后，我们要执行输入张量的第一个块（block one）。我们将指定训练。
- en: And from my understanding specifying training here that's going to be done inside
    this model that fit or model evaluateval we're also going to show in future videos
    how to do custom training loops and so on。 but I think this is done internally
    in model that fit if we just specify training right here that's going to send
    the argument depending on for example here we're training so that would set training
    to true and then when for the evaluation it would then set model to false or rather
    training to false。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的理解，这里指定训练将在模型拟合（model fit）或模型评估（model evaluate）时完成，我们将在未来的视频中展示如何进行自定义训练循环等等。但是我认为如果我们在这里指定训练，这将在模型拟合内部完成，这会根据例如这里的训练将参数设置为`true`，在评估时则设置为`false`。
- en: And then。We're going to do block 2 of x， again specify training。We're going
    to do Se to block 3 of x。 again， specify training。And then at the end， we're going
    to do self dot pool of x。 And then we're going to do x is。Or actually just return
    self dot classifier of x。And what would probably help now is doing。Model that
    summary。So after the model that fit here。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们要进行输入`x`的第二个块（block 2），再次指定训练。接着我们要进行输入`x`的第三个块（block 3），同样，指定训练。最后，我们将执行`self.dot
    pool of x`，然后将`x`返回，实际上就是返回`self.dot classifier of x`。此时进行模型摘要（model summary）会有所帮助。在模型拟合（fit）之后。
- en: let's do print model dot summary。Right there。 And let's just run it for single
    epoch。 First of all。 Alright， yeah， we need to also specify our model。 So we gotta
    do model equals res。Ne。Like。 and then we can do classes equals 10， and now we
    can hopefully run that。Alright。 so we can see after one Epo， we have 97% on the
    training。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印模型摘要（model dot summary）。就在那里。我们让它运行一个周期（epoch）。首先，好的，是的，我们还需要指定我们的模型。所以我们要做`model
    equals res.Ne`。然后我们可以设置类为`10`，希望现在可以运行。好的，所以我们可以看到经过一个周期后，我们在训练集上的准确率为`97%`。
- en: And then here we can see the the sort of the。The layers of our model and we
    can see we have a re block。 res block， Res block， and then average pulling， and
    then thens what's kind of annoying here is that we have multiple output shapes
    and this is usually the case when you're doing subclassing。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这里我们可以看到模型的层次结构。我们有一个残差块（res block），再是平均池化（average pooling），然后有点烦人的是我们有多个输出形状，这通常发生在你进行子类化时。
- en: But I found one workaround。 I'm not sure if this is the best way to do it。 but
    just for now you can use this so we can do model self。We're going to do x carriess
    that input shape specify。 And then let's do 28，28，28，1。AndThen return。 ks dot
    model inputs equals x， and then outputs equals self dot call of x。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但我找到了一种变通方法。我不确定这是否是最佳方法，但现在你可以使用这个方法，所以我们可以执行模型`self`。我们要做的是指定输入形状的`x`。然后让我们做`28,
    28, 28, 1`。接着返回`ks.dot model inputs equals x`，然后输出为`self.dot call of x`。
- en: And so what this is going to do is it's going to overwrite the model call and
    then we could do something like model model。 model， call that and then do dot
    summary and in this way we're actually going to get the output shapes So I'm just
    going to let this run and then we'll see what it looks like。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这将覆盖模型的调用（call），然后我们可以像这样执行模型调用（model model call），接着做`.summary`，通过这种方式我们实际上将获得输出形状。所以我就让这个运行，然后我们看看结果是什么样的。
- en: Alright so what we can see now is that the output shapes are actually included
    we have none for the number of batches。 28281 for the input image after the first
    re block it's 64 number of channels and then it's 14 by 14 so it's included  one
    max pooling and then we're max pulling again where max pulling one more time and
    then that's run through an average pooling so as you can see here as you can see
    here we have in this case 512 channels and then we have heightened width of3 by
    3 we're essentially averaging all of those3 by3512 into a single 512 and then
    we're running that through a single dense layer and total number parameters are
    3 million which is I guess much larger than what we've done but not relatively
    to other models this is still pretty small but so hopefully you can see that if
    you would have built this model right this is a huge。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们可以看到输出形状实际上是包含的，我们对批次数没有任何限制。经过第一次重构块后，输入图像的形状是28281，通道数为64，接下来是14乘14，所以它包括了一个最大池化，然后我们再次进行了最大池化，再进行了一个最大池化，然后通过平均池化处理。如你所见，在这种情况下我们有512个通道，宽度和高度为3乘3，我们基本上是将所有的3乘3512通道平均成一个单一的512，然后我们通过一个密集层进行处理，总参数量为300万，我想这比我们之前做的要大得多，但相对其他模型来说，这仍然算是比较小的。因此，希望你能看到，如果你构建了这个模型，那将是巨大的。
- en: And we can't even see so from the model summary we can't actually see all of
    the layers right we're just seeing these blocks but these blocks。 so each block
    has multiple of these CNN blocks right all of these resonant blocks as multiple
    so this model is pretty large if you would plot it and if you would use this I
    mean use the sequential API I don't you can't even use the sequential API I think
    but it would be。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至无法从模型摘要中看到所有层，对吧？我们只看到这些块，但这些块，每个块都有多个CNN块，所有这些残差块都是多个。如果你绘制这个模型，会发现它非常庞大。如果你使用的是顺序API，我认为你甚至无法使用顺序API，但它会是。
- en: It would be a lot more annoying to build this。 And so hopefully this is a something
    that illustrates。Why doing subclassing can be very good and you also have a lot
    of flexibility that you can pretty much build your models as you want and you
    can also you can print everything during the call and that's very intuitive。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个模型会更加麻烦。因此，希望这能够说明为什么进行子类化是非常好的，你也有很多灵活性，可以几乎按照自己的想法构建模型，并且你可以在调用过程中打印一切，这非常直观。
- en: I really feel the subclassing is a great way to build your models and in the
    next video we're also going to show how to do these custom layers using this subclassing
    so that for example。 let's say we're now using these this dense layer right how
    would you actually go about building a dense layer by yourself and stuff like
    that So that's what we're going to do in next video I thought just for fun。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的觉得子类化是构建模型的一个很好的方法，在下一个视频中，我们还将展示如何使用子类化来创建自定义层。例如，假设我们现在使用这个密集层，实际上你该如何自己构建一个密集层等等。所以这就是我们下个视频要做的，我想只是为了好玩。
- en: we could run this for a little bit longer and we could see what kind of accuracy
    we can actually get with this the largest model we've built in these tutorials
    so far So let's run this and this is gonna to take a while but it's going to go
    pretty fast for you So yeah I'm going let it run and I'm gonna to wait until it's
    done。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再运行一段时间，看看我们实际上能从到目前为止构建的最大模型中获得什么样的准确度。所以让我们开始运行，这会花费一些时间，但对你来说会很快。所以我会让它运行，直到完成。
- en: Al right so after about 20 epochs we get 99。84% on the training then 99。14 on
    the test set I think if you train this for just a little bit longer so that you
    get this up to 99 when I trained it previously I actually got 99。4% on a test
    set but then I trained for a couple of more epos and then what I think you could
    also do is add regularization and the model would improve but nonetheless I think
    this video really demonstrates the power of subclassing and hopefully you think
    subclassing is awesome after this video thank you so much for watching this video
    and I hope to see you in the next one。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，经过大约20个周期，我们在训练集上得到了99.84%的准确率，在测试集上得到了99.14%的准确率。我认为如果你再训练一段时间，可以将其提升到99。当我之前训练时，我在测试集上实际上得到了99.4%的准确率，但然后我又训练了几个周期。我认为你还可以添加正则化，模型会改善。不过，我认为这个视频确实展示了子类化的强大功能，希望你在观看完这个视频后也觉得子类化很棒。非常感谢你观看这个视频，希望在下一个视频中见到你。
- en: '![](img/2b6b959a397871940a0ba2149e837d11_2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b6b959a397871940a0ba2149e837d11_2.png)'
