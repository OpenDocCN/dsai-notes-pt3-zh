# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P66ÔºöL12.5- ÈùûÊ∏∏ÊàèTF-AgentÁöÑÂº∫ÂåñÂ≠¶‰π† - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeffine and welcome to App of Deep neural Networks with Washington University„ÄÇ

So reinforcement learningÔºå most demos that you've seen of this probably lets you play some sort of Atari game or run a mountain car up a hill or balance a pole or something fun like that„ÄÇ

What I haven't seen a lot of examples of is how to actually apply this to something that is not an Atari game or is not some sort of physics simulation„ÄÇ

What I'm going to show you in this video is how to actually apply reinforcement learning using TF agents to a problem of your own design„ÄÇ

 NowÔºå reinforcement learning is outside of the area of normal classification and regression„ÄÇ

 really it's something where you're constantly performing actions on an environment of some sort and receiving feedback for this„ÄÇ

The example that I'm going to show in this section is how to create sort of a financial simulation„ÄÇ

 We're going to look at net wealth building and sort of personal finance„ÄÇ

 This is really a pretty simple financial simulation„ÄÇ

 This is not meant to guide you and real life or how to do any sort of real financial investing„ÄÇ

 but it shows you how to actually put something that is not a video game into the context that it can be solved and worked with with„ÄÇ

Reinforcement learningÔºå and you can design an agent that can actually learn how to do this to see all my videos about Cale„ÄÇ

 neural networks and other AI topicsÔºå click the subscribe button and the bell next to it and select all to be notified of every new video So now let's try to apply reinforcement learning to something that is not just in the open AI gym or an Atari game or something like that„ÄÇ

 I'm going to create this problem literally from scratch„ÄÇ

 Now it's not going to be the most profound problem that I'm going to apply this to„ÄÇ

 It's really the idea of this demonstration is to show you how you can create your own environment and apply deep reinforcement learning to it„ÄÇ

 We're going to also look at the continuous action space and see how you can deal with having one or more actions that is a number„ÄÇ

 like how how far do you want to press down the accelerator pedal„ÄÇ

 say in a car or how hard to apply the brakesÔºå because you can do both of those at the same time„ÄÇ

 For this example that I'm going to put together„ÄÇ It's going to be a finance kind of personal finance example where you are„ÄÇ

üòä„ÄÇ

![](img/bd52a9eda0cfe550681151d31dca6c38_1.png)

![](img/bd52a9eda0cfe550681151d31dca6c38_2.png)

![](img/bd52a9eda0cfe550681151d31dca6c38_3.png)

Letttting the agent be a person that is trying to invest and save for retirement it'll be a pretty simplified example where they can choose between putting their money in a tax deferred account like a United States IRA or they could choose a taxable account and there's limits you can only put so much in that tax deferred account you can also choose do you want to pay for your house you want to pay the mortgage off early„ÄÇ

 all these kind of things So these are kind of step by step by step actions that are really conducive to reinforcement learning if I just wanted to the computer to look at a list of stocks and give me the best allocation for investing that would be more like a traditional neural network so we're gonna create an environment entirely of our own This is a class in TF agents an objectoriented class here are the rules you've got to make sure that you accomplish these things or you won't be able to use your new class It has to be a child of Jim environment then you've got to„ÄÇ

Iplement a seed function that allows your environment to be seeed„ÄÇ if you give it a constant seed„ÄÇ

 then you should get constant results„ÄÇ You need to implement a reset function that basically starts your environment over again is this is done often by the reinforcement learning algorithms„ÄÇ

 It's always turning back the clock and hitting reset and starting from the beginning This is not as critical„ÄÇ

 but you need to implement a render function„ÄÇ This draws one frame of your simulation„ÄÇ

 Now we're doing a finance simulation here„ÄÇ and I kept it pretty simple„ÄÇ

 We're just outputting text to a window„ÄÇ same sort of thing step is a function that will take you through each step in the you apply an action to that step and then you see how the environment changes and we'll get to what the environment is in a moment„ÄÇ

 you also need to register your environment and we see the code to do this as well That's so that Tf agents can actually find your environment when when you want to spin it up„ÄÇ

 So here's the simulation that„ÄÇGoing to produce Now this is a very simple simulation„ÄÇ

 This is just to show you how to get your own environment up and running„ÄÇ

 I was tempted to just create my own mini video game or something like that„ÄÇ

 but I might save that for a future example„ÄÇ but there's already enough examples of doing this I want something where we're playing a financial simulation doing personal finances is like the old Milton Bradley game of life a bit not to be confused with Conway's game of life which which is something else„ÄÇ

 So you have a random starting salary between 40k and 60k dollars you have a home loan that starts out so you have to buy a home's I'm keeping it simple that is 1„ÄÇ

5 times and four times you're starting salary so maybe you are very conservative or maybe kind of went nuts so this is looking at how much how much house this person can potentially afford and they'll need to adjust their strategy depending on how crazy they went on real estate purchases The home loan is always an amortized 30 year fixed loan with a fixed monthly payment„ÄÇ

If you don't make that paymentÔºå bad things will start to happen to you Pay higher than the homes monthly payment pays down the loan quicker Pay below the monthly price results in missed payments When you reach a certain number of payments„ÄÇ

 I think I set it around 15„ÄÇ they take your house away You don't lose the value but they sell it at auction and you lose„ÄÇ

 I think half of your equity„ÄÇ you can allocate income between luxury purchases„ÄÇ

 which in this game does absolutely nothing for you home payments either above or below the payment amount as well as a taxable and a tax advantage savings account„ÄÇ

 It might have been really interesting that if you spent more on luxury or are happier and you lived longer therefore you got longer to invest I don't know„ÄÇ

 might have been an interesting twist on this I'll probably take this game of life thing a bit further and do a separate video on it outside of the course where we look at a fairly complicated financial simulation So the state the state is composed of the following values„ÄÇ

 This is what the program is looking at to determine what it should do„ÄÇ

The age of the person in monthsÔºå the salary Now the salary goes up slowly relative to inflation„ÄÇ

 no chances for promotions or anything like that„ÄÇ So it's pretty simple simulation„ÄÇ the home value„ÄÇ

 This will increase relative to inflation„ÄÇ This is the required home payment„ÄÇ

 This will stay the same mostly the only time this will change is when you pay off your home then it goes to zero„ÄÇ

 This is the account balance for the tax advantage„ÄÇ This is similar to a United States IRA account„ÄÇ

 individual retirement account„ÄÇ and then the taxable account„ÄÇ

 This is similar to just a standard money market savings account„ÄÇ

 It does pay you some actually pays you pretty good interest compared to what a money market would do„ÄÇ

 So this assumes you probably have this invested in a independent brokerage account„ÄÇ

 selflf-directed brokerage account„ÄÇ least that's what they call them in in the US„ÄÇ then the action„ÄÇ

 So the action is floating point because we're doing a continuous action space„ÄÇ Unlike previously„ÄÇ

 The agent puts a number into each of these four„ÄÇ And then I normalize them so that they some„ÄÇ

To to one to get percents„ÄÇ So if you put 11Ôºå111 on all of these„ÄÇ

 that would normalize out to 25% into each„ÄÇ So this is just letting you pick your percents that you want to devote of your salary to the home loan to the savings tax advantage account to the normal taxable savings account into luxuries„ÄÇ

 Now this doesn't guarantee that you'll get all of these percents in there because there's various things going on here„ÄÇ

 you do have to pay tax on your salary„ÄÇ So thatll decrease the salary sum you might still be allocating money towards your home loan„ÄÇ

 but you might have paid your house off„ÄÇ So any value that you put in here is basically ignored„ÄÇ

 It's just going to go to0„ÄÇ You might have run out of of salary before you can even put it into the taxable account or the savings account„ÄÇ

 AlsoÔºå you can only put so much in the tax advantage account„ÄÇ

 at least in the United States that's sort of on the trick with IRAs„ÄÇ

 There's a limit to how much you can put in there per year„ÄÇ

 There's a limit to how much you can put into your 401 any yearÔºå all these all these„ÄÇ

Of ramifications and I try to put some of those in there as well„ÄÇ

 The limit that I actually put in for the tax advantage account is similar to4 and K„ÄÇ

 So here is the program„ÄÇ or at least the class„ÄÇ This is a object oriented class„ÄÇ

 It inherits from gym environment metadata you must just present that for display I went with pretty much what most of the classes in gym go with one video frame per second Def not a highend video game there by any stretch and it's going to display in RGb form„ÄÇ

 The number of state elements that we have are 7 and it's the first seven of these a few of these are calculated values that I do add to the output just for debugging purposes„ÄÇ

 you'll find that when you create a complex environment you'll spend a lot of time just debugging the environment before you even get around to hooking it up to Tf agents So here are the positions„ÄÇ

 So the state is essentially a vector„ÄÇ The first element in the vector is age than salary and so on and so forth„ÄÇ

 This is also another constant I put in there for a million' see that„ÄÇThat for normalization„ÄÇ

 I normalize all the dollar amounts to millionsÔºå even though salary is much„ÄÇ

 much smaller than that it's still a fractional piece of a million„ÄÇ

 that helps to keep these numbers from bigger than what the neural network would necessarily want to deal with„ÄÇ

 These are the values that I put in for inflation interest tax rate„ÄÇ

 Some of these are normalized to're not normalized but divided by 12 because they're monthly values„ÄÇ

 this is expenses„ÄÇ you have no choice you have to pay the expenses that's your food clothing„ÄÇ

 water internet connection netÔºå all the criticals„ÄÇ Then your salary range low and high starting age and retiring age or essentially age that you die at„ÄÇ

 This is the constructor I set verbo mode default fall setting ver most mode to true just causes my program to print out some logging information so that you can help debug it I define the action space„ÄÇ

 The action spaces between  zero and1„ÄÇ It's important that you define this so that the reinforcement learning algorithm no„ÄÇ

What those ranges are and can assign random numbers and other things as well„ÄÇ

 This is the observation space„ÄÇ I set the low and high values there„ÄÇ

 It can potentially go higher than2 million it might be clipping it that might be a good idea to put that to maybe4 or10 or something like that but anyway that's the ranges there„ÄÇ

 we go ahead and seed and reset I set the log empty the log is my own thing that is basically just giving me information that I can convert into a pandas data frame later so that I can debug and make sure that the simulation is doing what I actually wanted to do This is a handy function that I wrote the calculates the person's net worth looking at the home value„ÄÇ

 the principal amount still on the still owed on the home so you subtract the amount that you owe on the home from the value of the home and that that gives you that component of the net worth and then we add in the worth also as far as the tax advantage account and taxable again I'm making a very simple here you mainly save the taxes going into the account not so much coming out„ÄÇ

Not like in the USA RothRA„ÄÇ This is where we evaluate the action„ÄÇ

 So remember I said that these four action values that you get back„ÄÇ They're just numbers„ÄÇ

 I can't force that they necessarily all sum up to 1„ÄÇ

0 for 100% So what I'm going to do here is apply some of this I'm going to look at the home payment if it's approach 0 I am going to calculate so this is the total this is the total amount so they requested a number on all of these„ÄÇ

 I need to convert those numbers to percent„ÄÇ so I sum it up and then I'm going to divide each of these by this total account Now I don't want to cause a division by zero„ÄÇ

 So if the total account has fallen below or is approaching0 then I just return these all to 0„ÄÇ

 this is kind of an error situationÔºå but it does happen when bad strategies are evaluated„ÄÇ

 so we have to just quickly set those all to  zero and we're done otherwise we divide them all this is how we normalize it into something that sums up to 1„ÄÇ

0 and then we return these four values and these are„ÄÇ

Real percents After we've converted these larger numbersÔºå the TF agents gives you into percents„ÄÇ

 This is the step function„ÄÇ What this is doing is basically performing the simulation so we've got the action„ÄÇ

 We're going to use the action values that were passed in and pass it to that evaluate action function that we just saw„ÄÇ

 we're going also break apart the values that are in the state vector so that we have then I don't want to constantly be looking into a hash map that would be annoying so we deal first with expenses„ÄÇ

 that's that certain percent that you have to pay that's necessities like internet connection„ÄÇ

 at least it is to me„ÄÇ But yeahÔºå food waterÔºå those kind of things„ÄÇ

 This deals with first the tax advantage deposit account„ÄÇ

 We deal with that one first because you are limited in how much you can put into that in a given year„ÄÇ

 if you start to exceedcc to that then it just decreases you so that you cannot„ÄÇ This is like a 401„ÄÇ

 there is a company match„ÄÇ We calculate how much tax the person owes deduct that the home payment„ÄÇ

Kind of complicated because we are using amortization„ÄÇ

 so I put in the amortization formulas so that you basically got that continuously decreasing interest payment and continuously increasing principal payment amount„ÄÇ

If you had a late payment you're in trouble and we count that up„ÄÇ

 if it gets more than 15 late paymentsÔºå they will foreclose on you and forecloing„ÄÇ

 basically they just sell your house off„ÄÇ you lose 20000 in fees that they just charge for the trouble of selling your house and then so you lose a considerable amount„ÄÇ

't you don't really want to go there„ÄÇ OtherwiseÔºå if it's just a normal late payment„ÄÇ

 you're charged a $10% late fee based on the amount of your payment„ÄÇ

 not a 10% late fee based on the principal value or house that'd be horrendous„ÄÇ

 And then we update the house value to reflect the payment„ÄÇ

 we make the deposits into the taxable account and we make sure to credit investment returns„ÄÇ

 the investment returns are pretty similarÔºå we do randomly vary those a bit„ÄÇ

 just like we randomly vary the inflation amountÔºå this just put some randomness into the game„ÄÇ

 which makes it harder but this is also what reinforcement learning is quite good at doing dealing with the random numbers„ÄÇ

 we deal with yearly events So every time we're at month 12 monthly events„ÄÇNot too much there„ÄÇ

 We're just aging the agent by one„ÄÇ It's a time to retire„ÄÇ I meanÔºå they retire at 80„ÄÇ

 I guess I probably should have said that this is more like when they died or something I'm making everybody quit at the same amount„ÄÇ

 but also the person works right up to 80 so again this is a simplified simulator and then we normalize and finish up So for the state I'm dividing each of these state values your accounts by multiples of a million that gets it into much smaller values and it makes it easier for the neural network to deal with usually you don't want to be throwing in large multimillion dollar numbers into a neural network that just doesn't work that well when we perform the yearly we are basically counting the investment rate or the inflation rate we're adding some randomness into that we give them the salary increase we increase the home value by the amount that inflation dictates and so on this is reset it starts it over're reborn so to speak we set the late count to0 all all these values to zero so that we are starting over from„ÄÇ

Geting we generate a random salary based on between those ranges„ÄÇ

 We calculate that random house multiple between 1„ÄÇ5 and 4„ÄÇ

 We calculate the mortgage amount so that we know the amount of the principal and there's that famous amortization formula and then we set this all into the state„ÄÇ

 Render is pretty simple„ÄÇ Rler is using pill so it's returning an image basically just display all of these values in a green font with a black background„ÄÇ

'll see what that looks like in a moment and print those all out„ÄÇ

 And that's it at that point you have an environment Now you need to register your environment so that you can see it and I call this just simple game of life again not conway's game of life I'm kind of thinking of the old board game life„ÄÇ

 All right so let's test the environmentÔºå make sure that it works„ÄÇ

 I instantiate it and create it just like this I do a reset and all I'm doing here is just playing the game of life so to speak with fixed values So one in one I am basically paying off the home loan and investing into the tax advantage account which is actually a very good strategy I didn't quite make this„ÄÇ

Simulator complicated enough that there is a tremendous amount for the deep reinforcement learning to learn„ÄÇ

 but this alone performs quite well„ÄÇ You can see that it generates a net worth of about $8 million at the end„ÄÇ

 so thats that's good„ÄÇ Also remember that's not inflation adjusted So over 80 years that might not be quite quite as good„ÄÇ

 hyperpara„ÄÇ I adjusted these a little bit to make it so that it it learns relatively well„ÄÇ

 You could definitely do some more work here„ÄÇ The main thing that I change is we do 50000 iterations„ÄÇ

 and we do 50 collection steps on each of those iterations„ÄÇ

 So that's how many episodes are actually run just so that we have enough data each time to work this on„ÄÇ

 batch size is 64„ÄÇ We log every 2500 and we evaluate every 5000 Now„ÄÇ

 this is what our environment looks like if we want to just render it so that we can see it„ÄÇ

 this is what it looks like„ÄÇ And later we'll look at the video and we can watch this person age to 80 and see what they're doing„ÄÇ

 We create the two environments so that we can both evaluate this and train it and not„ÄÇ



![](img/bd52a9eda0cfe550681151d31dca6c38_5.png)

![](img/bd52a9eda0cfe550681151d31dca6c38_6.png)

Step on each other„ÄÇ Now we are using not a DQN„ÄÇ We had a deep Q learning network before the problem with a deep QN„ÄÇ

 It's not a major problemÔºå but in this case it isÔºå is that the DQN only supports discrete action space So think of a joy stick forcing it up down left right„ÄÇ

 not how hard you're pressing it we have these four accounts and I would like to be able to give a variable amount into each of these four retirement accounts„ÄÇ

 So to do this we need to use a new type of neural networkbased reinforcement learning called a DtPg algorithm„ÄÇ

 deep deterministic polyc gradients I won't go too deep into how that algorithm actually works if you want to read the original paper on it„ÄÇ

 It's here it's I have not gone through and implemented one of these myself but they do work quite well for continuous action space reinforcement learning This is what the structure of it sort of looks like your environment is„ÄÇ

Giving you back your stateÔºå which is X„ÄÇ that same state goes into the critic and the actor„ÄÇ

 So there's two neural networks at play here„ÄÇ This is kind of like agan where you have basically one neural network that's generating images a second one that is discriminating telling it if those images are real or not This is similar„ÄÇ

 you've got two adversarial neural networks you've got the actor the really adversarial of each other„ÄÇ

 What it's really doing is the actor is returning in this case for the game of life it would return for floating point numbers and those are the actions that you're going to get„ÄÇ

 The reason a deep Q network like that like we had before could not do continuous values is you need this nice table where you have where you're potentially getting a value for every single one of those actions„ÄÇ

 What we have now is we feed in the current state and the actor return the step that wants it to take the action and the critic„ÄÇ

Evaluates how well the actor did on picking continuous values for the action„ÄÇ

 So you're picking those four continuous values for the accounts„ÄÇ You're not picking which account„ÄÇ

 you're picking the four continuous values„ÄÇ How well those continuous values maximized the eventual reward and that's basically how actor critic reinforcement neural network works„ÄÇ

 The TD error is important to„ÄÇ That's the temporal difference error So that's just looking at how different was the actor's estimation of future reward versus what the environment actually gave it back as the future reward„ÄÇ

 That is what the objective function is trying to minimize These are the hyperparameters for these two neural networks we create those two neural networks and then we create the so we create the actor first„ÄÇ

 and then we create the critic and then we put both of those together into the DDPG agent initialize it and we're basically ready to go„ÄÇ

 We're going to use the same metric that we've used in previous„ÄÇles I've given you„ÄÇ

 This is where we take a certain number of episodes„ÄÇ

 and we look at the average return over those episodes„ÄÇ We do our initial data collection„ÄÇ

 just like previous examples„ÄÇ We've got to run through the simulator a couple of times„ÄÇ and actually„ÄÇ

 in this case a thousand times and get some sample environment changes based on these actions and actual rewards„ÄÇ

 That's what we will begin to train the neural network on„ÄÇ And again„ÄÇ

 this is the same code that I showed you in previous examples that we've done a reinforcement learning„ÄÇ

 The main point of this example is how to create your own environment„ÄÇ

 Now we're going train the agent againÔºå this is the same code„ÄÇ And by the way„ÄÇ

 this code changes really so little that you could actually switch out So up here„ÄÇ

 you could switch out my simple game of life and put in the mountain car continuous which is a continuous environment that they have in the gym„ÄÇ

 and it would work just fine„ÄÇ It would train the continuous mountain car„ÄÇ

 So back down to here where we're training the agent„ÄÇ It goes through all the iterations requested„ÄÇüòä„ÄÇ

Coects the requested number of data and this is the output„ÄÇ

 If you want to go through line by line on these my previous examples on Tensorflowlow agents learning is shows you exactly how this works„ÄÇ

 but it's just a loop going through and printing out the results as as it trains and this is going up to 50000 steps the visualization looks like this So it learns it really quite quickly and then reaches a plateau and this is not that hard of an optimization problem for it to learn the videos for this if we play one of these you'll see basically it's counting up their aging 2122 and you can see it learned to apply„ÄÇ

 this is a pretty boring agent actually it just gives up on the house„ÄÇ

s it be confiscated and then it just goes into investing in the tax advantage savings it doesn't quite figure out that there's a limit to that it probably should have put the rest in the tax textable and as you retrain it„ÄÇ

 it learns„ÄÇ

![](img/bd52a9eda0cfe550681151d31dca6c38_8.png)

![](img/bd52a9eda0cfe550681151d31dca6c38_9.png)

![](img/bd52a9eda0cfe550681151d31dca6c38_10.png)

Different approaches so I've run this a couple of times„ÄÇ

 there are smarter agents that trains this one this one's okay„ÄÇ

 I mean it's building up the net worth and you can went to zero because it sold it I do hit them with rent or something like that but anyway is creating your„ÄÇ



![](img/bd52a9eda0cfe550681151d31dca6c38_12.png)