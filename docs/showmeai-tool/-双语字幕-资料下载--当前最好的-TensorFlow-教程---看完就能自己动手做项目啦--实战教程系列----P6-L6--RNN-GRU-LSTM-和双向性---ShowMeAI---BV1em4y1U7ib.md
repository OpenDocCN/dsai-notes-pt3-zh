# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P6ÔºöL6- RNN„ÄÅGRU„ÄÅLSTM ÂíåÂèåÂêëÊÄß - ShowMeAI - BV1em4y1U7ib

What is going on guys hope you're doing awesomeÔºå roll that intro and then let's do some RNN and shit„ÄÇ



![](img/7a25be1c8cdf0d1403f6e221e59c1458_1.png)

![](img/7a25be1c8cdf0d1403f6e221e59c1458_2.png)

![](img/7a25be1c8cdf0d1403f6e221e59c1458_3.png)

AlrightÔºå so I got the usual imports that we normally have„ÄÇ

 This is for ignoring the Tensorflow messages that can be quite annoying„ÄÇ

 although we'll still get error messages and then Tensorflowlow Kas layers to construct our layers and then the emminis data set and then this is just so that the if you have any trouble running on a GPU these two lines will most likely help you„ÄÇ

 AllrightÔºå so let's let's start with what we actually want to do„ÄÇ

 we're going to start with loading our data so we're going to do Xtrain„ÄÇY train and then X test„ÄÇ

 Y test„ÄÇIs equal to Ms dot load dataÔºå then we're going to do xtrain equals xtrain as type float 32„ÄÇ

 Currently it's float 64 just to save on own computationÔºå we're going to convert it„ÄÇ

 and then we're going to normalize by dividing by 255Ôºå So it's in between 0 and 1„ÄÇ

And let's do the same for the the test set„ÄÇ So flow 32Ôºå divided by 255„ÄÇ

 So what we're doing here is that we have„ÄÇWe have an image of 28 by 28 pixels and how this is going to work when we're going to send it in to an RN or a GRU or an LSTM„ÄÇ

 we're going to do all three of thoseÔºå but we're essentially going to view for each time step you're going to sort of unroll one row of the image at a time So for a particular time step let's say the first time step It's going to take the first row of the image and send that in and then for the second time step it's going to take the second row and send that in and just to be clear you wouldn't use sequence models to handle images„ÄÇ

 It's not the best model for itÔºå you would use a commnet that we covered two videos ago but„ÄÇ

It works to use RNN„ÄÇ and as we'll seeÔºå will'll get reasonable performance„ÄÇ

 although this is more to illustrate how you would actually„ÄÇ

Implement an R N and a GR U and LSTM in Tensorflow„ÄÇ and the data set is not the optimal one„ÄÇ

 but we're just picking in a simple one to illustrate this example„ÄÇ Al rightÔºå so with that said„ÄÇ

 let's actually do our modelÔºå we're going to do kos dot sequential„ÄÇ

We're going to do model that add and then specify the input„ÄÇ in this case„ÄÇ

 we're going to specify none and then 28„ÄÇSo what we're specifying none here is because we dont have to have a specific number of time step right so we have 28 pixels in each time step and then in this case we actually have 28 time step but we don't have to specify that dimension„ÄÇ

And then we're going to do model„ÄÇ add and then layers simple Rn„ÄÇ So that's just for a basic Rn„ÄÇ

 and then let's say 512 nodes„ÄÇAnd then as an additional argument we can do return sequences equals true so that it's returning the output from each time step and in that way we can stack multiple RnN layers on top of each other so the output from this RnN is going to be 512 nodes and then return sequences it's going to output 512 for each time step in this case we're going to have 28 time steps and then we can also do activation we can set it to Relu„ÄÇ

And then we can add another oneÔºå we can demand let add layers simple R nÔºå and let's do 512 again„ÄÇ

 and we're going to set activation equals Relu„ÄÇAnd then for the output layer we're going to do model add layers„ÄÇ

 dense and we're going to have 10 output nodesÔºå so you would notice here that we're not doing return sequences on this second simple Rnn so here for the output we don't have return sequences equals true„ÄÇ

 meaning that it's going to pass every time step and then at the last the last output of this simple Rnn here„ÄÇ

 we're going to take a layer dense on top of that one and we're going to have 10 output nodes„ÄÇ

Let's do print model summary first so as we can see here on the model summary for this first Rnn we're gonna to have none none and then 512 so we're going have 512 output node and then we're going have for each time step here and the reason that we have none and none is that we have the first one for the batches or one of these are for the batches and one of them are for the hidden states I think this one is for the batches this one is for the hidden states and then at the second one we're not having return sequences equals true so we only have none for the batches and then we have 512 nodes from sort of the last hidden state when it's past all of the input and then at the end we just have a layer on top of that one So all that's left now is for us to do model do compile and we're going to specify a loss function and ks do losses sparse category„ÄÇ

croros entropy then we're going to set from logics equals true because we do not have a softmax on our dense layer at the end and then optimizer„ÄÇ

 we're going to Kas do optimizers do atom„ÄÇAnd let's set the learning rate to 0„ÄÇ001„ÄÇAnd then metrics„ÄÇ

 we're just going keep track of the accuracy„ÄÇ All that's left now is for us to do model that's fit on the training set„ÄÇ

And then specifying the batch sizeÔºå let's say 64„ÄÇ and then let's run for 10pos„ÄÇ

And verbo equals2 just for printing every epoch„ÄÇ And then at the end„ÄÇ

 we want to do an evaluation on our test set„ÄÇ So we're going to send an X test and then the labels Y test„ÄÇ

We're also going to specify the batch size 64Ôºå and then again verboos equals„ÄÇEquals 2„ÄÇAlright„ÄÇ

 so let's run that and see if this works„ÄÇAlrightÔºå so after 10 epos we got 98% on on the training set and almost 98% on the test set as well„ÄÇ

 I just want to say here that we used an activation REL„ÄÇ

 the default when training recurrent networks is that you use 10h„ÄÇ

 so I don't know if that would work better in this caseÔºå but anyways„ÄÇJust wanted to mention that„ÄÇ

 So if you wouldn't specify an activation function„ÄÇ

 it would default B 10h when when building these recurrent nets„ÄÇ

 So also one thing here is that it took a little bit longer than I thought to run this so let's just use 256 for our next models what we want to do now is pretty much the same thing but we want to build a GRU instead and all we got to do to do that is we just got us change this this simple Rn to a GRU and that's pretty much that's all you have to do so if we now rerun that we can see what it gets and I guess this is not really a fair comparison but GRU should perform better than simple Rnns although now we're using half of the units and then we're using 10h instead of RElu but the point is not really to compare the to just see that it works and and show you how to use simple Rnn GRU and then„ÄÇ

LSTMÔºå which is quite simple as wellÔºå we're just going to change this to an LSTMÔºå but anyways„ÄÇ

 then I also want to show you how to do a bidirectional layer„ÄÇAlrightÔºå so after 10pos we had 99„ÄÇ

5% on the training and we get close to 99% on the test set„ÄÇ

 which is quite good actually it's two layered GRus with 256 units I mean to get that is pretty decent actually so let's now change this to an LSTM and see if it's an improvement so„ÄÇ

LSTMs and GRUs are equivalent in terms of performance„ÄÇ

 I think LSTMs are a little bit better than GRUsÔºå but let's see if that's the case on this data set„ÄÇ

AlrightÔºå so it seems that we get pretty much the identical performance„ÄÇ

 LSTMs were a little bit betterÔºå perhaps on the test headÔºå but pretty much the same„ÄÇ

 So what we want to do now is we want to add instead of using just a one directionional LSTM„ÄÇ

 We want to use a bidirectional and it's pretty easy to add that as well„ÄÇ

 we're just going do layers do bidirectional and let's do it like this„ÄÇ and then like this„ÄÇ

 So we're just gonna add layers bidirectional and then we're gonna send in that LSTM layer„ÄÇ

 And let's do that for the second one„ÄÇOrRatherÔºå let's do„ÄÇAlright„ÄÇ

 so let's let's first do Mo print a model that summary and just see how it looks like„ÄÇAlright„ÄÇ

 so I'm not going to let this train„ÄÇ So what we get here is as you can see since we added this layer bidirectional„ÄÇ

 we're going to get 512 nodes instead of this 256„ÄÇ So what it's doing here is we're specifying the number of nodes for each hidden for each of its computation for each hidden state in the LSTM to be 256 nodes„ÄÇ

 but since we add this bidirectional we're going have one going forward and one going backwards So this is going get doubled in the amount of nodes as we see here„ÄÇ

 So what we can do then is for the second one we can do also a layers that bidirectional and then„ÄÇ

We can add that„ÄÇüòîÔºåRight there and that's also going to have 512 nodes„ÄÇ

 So let's run that and see if the bidirectional is any better than the one direction just having one direction on the LSTM„ÄÇ

After 10 eposÔºå we see that the performance is about the same as just using one direction so the bidirectional didn't really help and„ÄÇ

And I'm not really sure why that is maybe it just needs more training or it just doesn't help that much„ÄÇ

For this particular datasetÔºå but in general using bidirectional as more of a default is a good option„ÄÇ

 but anyways that was the basics of how to do a simple RN a GRU LTM and then also how to add bidirectality and in this scenario we use the MNIS dataset so we made it very easy for ourselves when using the Ms data and when training on more complex data you need to think about more things such as padding the data and masking the data for each batch and we're going to cover that in future videos when we're going to load more complex data and loading more custom data„ÄÇ

 So with that said thank you for watching and I hope to see you in the next video„ÄÇ



![](img/7a25be1c8cdf0d1403f6e221e59c1458_5.png)