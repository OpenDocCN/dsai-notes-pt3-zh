# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P34ï¼šL6.3- åœ¨Kerasä¸­å®ç°ResNet - ShowMeAI - BV15f4y1w7b8

Hiï¼Œ this is Jeff Heatonã€‚ Welcome to applications of Deep neural networks with Washington Universityã€‚ In this videoï¼Œ we're going to look at Resnetï¼Œ which is a type of neural network that can be implemented in Carras that makes use of residual or what really makes this neural network unique is that it has skip connections that don't simply go to the next layer for the latest on my AI course and projectsã€‚

 Click subscribe and the bell next to it to be notified of every new videoã€‚ Resnet demonstrates really how flexible Keras isã€‚ Carras does not have a Resnet layerã€‚ at least at this pointã€‚ So we're going to literally construct our own Resnet layerã€‚ and it's not terribly difficultã€‚ There's a function that I found on the Carras website that implements thatã€‚

 We'll see how Kas can doã€‚ This This made a bigï¼Œ big splash at the I SVC classification challengeã€‚ Thisã€‚ğŸ˜Šã€‚![](img/c990de25bca059d59d22e195401e0376_1.png)

Use the C far 10 data set and dot just took first placeï¼Œ beat everything else that that was thereã€‚ This is the paper describing the processã€‚ And I took some of these images from the paperã€‚ First of allï¼Œ let's just talk about what is a residualã€‚ So what is a residual neural networkã€‚ or resnetï¼Œ residualã€‚ If you look at Miriam Websterã€‚

 It's a internal artifact of experience or activity that influences later behaviorã€‚ And that's exactly what it is in these these neural networksã€‚ It's a skip layerã€‚ That's anotherã€‚ I like skip layer better than residualï¼Œ but nonethelessï¼Œ either works equally well to describe itã€‚ Nowï¼Œ it's also important to note that it's skipping two layersã€‚ too normal weightedã€‚ğŸ˜Šï¼ŒRellU layersã€‚

 Nowï¼Œ the re comes in like activation functions are typically applied after the weighted layerã€‚ So this is pretty normalã€‚ It's almost like we take note of what the input into this layer wasã€‚We fire this other weighted layerã€‚But we don't apply the value yet or whatever the nonlinearity isã€‚But we add that X input right into hereã€‚ So whatever was output here gets added to thereã€‚

 and we have theã€‚The value in some waysï¼Œ this is a little bit like aã€‚ this is almost the reverse of a recurrent neural networkã€‚ And we'll see recurrent neural networks very soonã€‚ instead of going backwardsã€‚ like the recurrent neural networks doesï¼Œ this goesï¼Œ this goes forwardã€‚

 Why would you want to do such a thingï¼Ÿ Wellï¼Œ the short answer isã€‚ it gives a greater predictive powerã€‚ and it lets you go muchï¼Œ muchï¼Œ much deeperã€‚ the paper gets into this moreã€‚ But it shows that as you train deeper and deeper neural networks before thisã€‚You would get better and better predictive resultsï¼Œ Then you'd just hit a wallã€‚

 and your results would start to get worse and worse and worse as you added deeper and deeper and deeper layersã€‚ This has been experimented with for 100 layerï¼Œ100 hidden layersã€‚ and beyondã€‚ I think there's even been a few of 1 thousand00ã€‚ that has not necessarily shownã€‚Completely promising resultsï¼Œ but this research changes quite rapidlyã€‚

 This is what one of these looks like don't worry about the VGGã€‚Neural networkï¼Œ that isï¼Œ I meanã€‚ that's more the traditional convolutionï¼Œ neural network and some otherã€‚Kind of tweaks addedã€‚ added to thatã€‚ That's basically the competition that they were trying to beat for thisã€‚ this competitionã€‚ And this is a 34ã€‚Plainï¼Œ so like we just learned about convolution neural networkã€‚

 this is all based on convolutionsã€‚And this is the 34 layer residualã€‚ so this is really what this looks likeã€‚You can see all these skip layersï¼Œ skip layerï¼Œ skip layerã€‚ and so onï¼Œ and so forthã€‚The different colorsï¼Œ 64 filtersï¼Œ 128 filtersï¼Œ 256 filtersã€‚ so on and so forthï¼Œ and then they finally get into a averaging pool SC is fully connectedã€‚Thisã€‚

 the VGG is using several layers of fully connected and the convolutions similar toã€‚With poolingã€‚ this uses an average pool rather than a max pool kind of towards the endã€‚ Let's go ahead and look at the code to run thisã€‚ This is using the Cf data setã€‚ It needs to download it Now in Googleã€‚ when you're using Google Coabã€‚

 often you're going to find yourself having to re downloadload thisã€‚ You want to be careful because Google only keeps the stuff that you keep in Google Driveã€‚ My introductory video on how to use Google Coab explains all of that done with thatã€‚ Nowã€‚ we're going to grab the dataã€‚ğŸ˜Šï¼ŒThat data was stored as a pickleã€‚ so we're going toã€‚

Extract that data that we just downloaded and prepare it to be trained so now that we've downloaded itã€‚ let's just go ahead and display it so that we can see this is what this code does hereã€‚ it displays it so that you can see some of the samples from this from this data setã€‚Carsï¼Œ dogsã€‚ againï¼Œ predominantly animalsï¼Œ but this isï¼Œ this is that data setã€‚

 These are some parameters that describeã€‚What we're doing as far as the implementation of thisã€‚ we're going to use 200 epochs that takes a while to trainï¼Œ even with a GPUã€‚ this runs for a couple of hoursã€‚Btch size of 32 number of classes we pull right from the dataã€‚ it's 10 colors we pull right from the data setï¼Œ it's three channels redï¼Œ green and blueã€‚

Subtract pixel meaning is that's getting us basically to centering this about 0ã€‚ and that helps for predictive accuracyã€‚ There are two versions of this of resnet Ressonnet version 1 is the original and Resnet version 2 had some improvementsã€‚'ll talk about that later in this videoã€‚ We are choosingã€‚The depthã€‚Calculated on on the size of the image and the number of colorsã€‚

 We're choosing the depth based on the version and colorsã€‚ This is a useful functionã€‚ This is following along with the paperã€‚ This is the learning rate schedulerã€‚This is essentially going to bring the learning rate down as we cross a number of epochsã€‚ so whenever we change the learning rateï¼Œ it's reported hereã€‚This can be very effectiveã€‚

 We're using this with the atom training andã€‚Typicallyã€‚ you want to decrease the learning rate as you go forwardã€‚ so this demonstrates this techniqueã€‚ you might want to make use of this in the Cagle competition or other thingsã€‚ you have to experiment with it to see how well it particularly worksã€‚By the wayã€‚

 all of this data or this code I got from the Kiras websiteã€‚So this is just examplesã€‚ I've updated it to the latestã€‚Versions of Tensorflow and Cars and also reworked it a little bit toã€‚ I thinkï¼Œ make it a little more readable and segment it into a Jupiter notebookã€‚ This essentially creates a resonant layerã€‚ So that includes the two normal layers and the skip connectionã€‚

 It's essentially creating the convolutionï¼Œ2D layerã€‚ It does put a batch normalization in thereã€‚ the batch normalizationã€‚ğŸ˜Šï¼ŒIs a good layer that you that you might want to use that basically helps to really helps to keep the vanisheding gradient problem from being too muchã€‚

 too much of an issueã€‚ It's normalizing on each batchï¼Œ each of your mini batchesã€‚ We are also using a special weight initializer called the he normalã€‚This isï¼Œ againã€‚ following what the original paper didã€‚ We are also using a kernel regularizer of L2ã€‚ also following the paperã€‚ This implements a resnet version 1ã€‚

 which is primarily what we're dealing with hereï¼Œ but I included both of thoseã€‚Both the resnet version 1 and 2ã€‚ this specifies the the input parameterï¼Œ specify the shapeã€‚ and it is essentially stacking up those layers and culminating with the final dense outputã€‚ It's building those individual resnet blocksã€‚ A resnet blocks is those two convolution layers stacked betweenã€‚

 So it createsã€‚It creates the two resonnet layers Nowã€‚ the resonant layers that it's talking about above are essentially the two parts of the resonnet blockã€‚ we're keeping track ofã€‚So we have Y hereã€‚ X feeds inã€‚ So X is the inputã€‚ Then the second one is feeding to hereã€‚ This portion here is caught is implementing the skip layerã€‚

 Finallyï¼Œ we do an averaging poolã€‚And the final dense layerï¼Œ we do alsoã€‚ I'm going to go ahead and run this so that we have it in memoryã€‚We do also have a second version of Resnetï¼Œ the primary difference of the full the V2 variant compared to V1 is the use of the batch normalization before every layer thatã€‚Cause some improvements to it overallã€‚ We're dealing mainly with the version oneã€‚

 but both of them thereï¼Œ you canï¼Œ you can try out the twoã€‚ see the differences between the the normalizationã€‚At each point or just at the at the end of those resnet blocksã€‚ Here is where we actually run thisã€‚ So I get the input shapeï¼Œ I get the train and testã€‚If we want to subtract the pixel meanã€‚

 this is doing thatã€‚ that centers it about 0 more effectivelyï¼Œ Typ a good ideaã€‚ We run the ressonnet V1 or V2 depending on what we haveï¼Œ and we compile it and reproduce a summaryã€‚So this just builds the neural networkã€‚ doesn't actually runã€‚ And it's ready to goã€‚ It shows you the entire structure of the network againï¼Œ trying to match the original paperã€‚

 This is where we actually execute and train itã€‚ I'm going to run it just so that it can be going because this is going to go for a long timeã€‚ We get the learning rate scheduler so this causes it to reduce the learning rate until we plateauã€‚ So this causes it to reduce the learning rate when we do plateauã€‚ If we're not using image augmentationï¼Œ which by the way we areï¼Œ we do just a normal fitã€‚

Image augmentation can be very useful to letting it learn to not overfit the data that it hasã€‚ This allows it to shift and move about the image somewhatã€‚ do each of these is just various random sort of transformations that you can do to the image toã€‚Basicallyã€‚Distord it in various ways as it trainsã€‚ So it's like it gets brand new images each timeã€‚

 but they're all based on the train set imagesã€‚ It's like you can randomly horizontally and vertically flip itã€‚ We're not we are flipping it horizontallyï¼Œ but not vertically againã€‚ I'm following basically what the Kira's example had set upã€‚ So you could try some of these other valuesã€‚ It might give you better resultsã€‚

 And you might want to also use the image data generatorã€‚ which is doing these transformations on your own neural networks when you are dealing with image dataã€‚ Then we basically fit it run itã€‚ And we re report the lapse timeã€‚ takes a couple of hoursã€‚ typicallyyï¼Œ you probably don't even want to try this with a CPU other than a GPã€‚

 I have not tried running it could be a dayã€‚ I really have not tried it on CPUã€‚ Thank you for watching this video in the next videoã€‚ We're going to look at openCã€‚ This content changes oftenã€‚ So subscribe to the channel to stay up to date on this course and other topicsã€‚

![](img/c990de25bca059d59d22e195401e0376_3.png)

And artificial intelligenceã€‚