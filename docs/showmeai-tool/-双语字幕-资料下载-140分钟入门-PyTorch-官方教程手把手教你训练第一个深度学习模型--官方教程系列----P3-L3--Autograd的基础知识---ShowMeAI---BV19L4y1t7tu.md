# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë140ÂàÜÈíüÂÖ•Èó® PyTorchÔºåÂÆòÊñπÊïôÁ®ãÊâãÊääÊâãÊïô‰Ω†ËÆ≠ÁªÉÁ¨¨‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P3ÔºöL3- AutogradÁöÑÂü∫Á°ÄÁü•ËØÜ - ShowMeAI - BV19L4y1t7tu

HiÔºå and welcome back to the Pytorrch trainingin video series„ÄÇ In this video„ÄÇ we're going to cover autograph„ÄÇ just Pytorrch's toolning for rapidly and dynamically computing the gradients that drive back propagation based learning in your machine learning model„ÄÇ

üòäÔºåIn particularÔºå we're going to go over what Autograd does for you„ÄÇAnd why it makes Piytorrch so flexible and powerful for machine learning„ÄÇWe'll walk through a basic code example to you feel for what autograta is doing under the hood„ÄÇThen we'll see autograd's role in a training loop„ÄÇAfter that„ÄÇ

 we'll talk about why and how to turn the autograd feature off and on for a particular tensor or a particular context„ÄÇWe'll see the autograd profiler in actionÔºå and we'll look at the autograrad high level API that was released with Pytor 1„ÄÇ5„ÄÇ![](img/25eee23f7825074bf201a2a5a23d28cb_1.png)

The autograd feature of Pytorrch is a large part of what makes Piytorrch a fast and flexible framework for building deep learning projects„ÄÇIt does this by easing the computation of the partial derivatives„ÄÇ also called gradients that drive back propagation based learning„ÄÇI'm not going to belabor the math hereÔºå although if you'd like a refresher„ÄÇ

 go ahead and download the notebook and follow along in detail„ÄÇThe important concept here is that when we're training our modelÔºå we compute a loss function„ÄÇ which tells us how far our model's prediction is from the ideal„ÄÇWe then need to find the partial derivatives of the loss function with respect to the model's learning weights„ÄÇ

 These derivatives tell us in what direction we have to adjust the weights in order to minimize the loss„ÄÇ This involves the iterative application of the chain rule of differential calculus over every path through the computation„ÄÇAutograd makes this computation faster by tracing your computation at runtime„ÄÇEvery output tensor from your model's computation carries with it a history of the operations that LED to it„ÄÇ

 This history allows the rapid computation of derivatives over the graph„ÄÇAll the way back to your model's learning weights„ÄÇIn addition„ÄÇ because this history is gathered at runtimeÔºå you'll get the correct derivatives„ÄÇ even if your model has a dynamic structure with decision branches and loops„ÄÇ

 This offers a lot of flexibility over tools that depend on analysis of a static computation rack„ÄÇLet's have a look at a simple example of autograd in action„ÄÇFirst„ÄÇ we'll import pie torch and matte plot lib so we can graph some stuff„ÄÇNext„ÄÇ we'll make a one dimensional tensor holding a bunch of values between0 and 2 pi„ÄÇ

 and we'll add the requires grad equals true flat„ÄÇNote that when we print A„ÄÇ pitorrch lets us know that A wants gradients computed on any calculation it's involved in„ÄÇNow we'll perform a computation here we'll just take the sign of all the values in A and we'll graph that and that looks right„ÄÇIf you notice the calls that attach hereÔºå I'll be covering those later in the section on turning autograd on and off„ÄÇ

So printing a tensor be„ÄÇWe see that Pytorrch tells us it has a grad function„ÄÇ This means that B came from a computation where at least one of the inputs required the calculation of gradients„ÄÇ The grad function tells us that B came from the s operation„ÄÇLet's perform a couple of more steps„ÄÇ we'll double be and add one to it and we do thisÔºå we see that the output tensors again contain information about the operations that generated them into the grad function property„ÄÇ

By defaultÔºå Autograd expects the final function in a gradient computation to be single value„ÄÇThis is the case when we're computing the derivatives over learning weights„ÄÇ The loss function has a single scalar value for its output„ÄÇIt doesn't strictly have to be a single valueÔºå but we'll go over that in a minute„ÄÇ

Here we'll just sum the elements of the tensor and call that the final output for this computation„ÄÇWe can actually use the grad function property of any output or intermediate tensor to walk back to the beginning of the computational history using the grad functions's next functions property„ÄÇ

Here you can see that D knows it came from an addition operation„ÄÇ which knows it came from a multiplication operationÔºå so I'm back to A„ÄÇA does not have a gridd functionÔºå it is an input or leaf node of this computation graph„ÄÇ and so represents the target variables for which we want to compute the gradients„ÄÇ

So we've looked a little at the history trackingÔºå but how do we actually compute gradientsÔºü

That's easyÔºå just call the backward method on the output Tensor like so„ÄÇLooking back over the computationÔºå we have a sineÔºå the derivative of which is cosine„ÄÇWe have the multiplication by twoÔºå which should add a factor of two to our gradient„ÄÇIn the addition„ÄÇ which should not change the derivative at all„ÄÇGraphing the grad property on AÔºå we see thatÔºå in fact„ÄÇ

 the compute gradients are twice the cosine„ÄÇBe aware that gradients are only computed for inputs or leaf nodes of the computation„ÄÇ intermediate tensors will not have gradients attached after the back pass„ÄÇSo we peeked into the hood at how autograd„ÄÇComputes gradients in a simple case„ÄÇNext„ÄÇ we'll examine this role in the training loop of a Ptorrch model„ÄÇ

To see how autograd works in your trainingÔºå let's build a small model and watch how it changes over a single training batch„ÄÇWe'll define and instantiate a model and create some standard in tensors for the training„ÄÇ input and ideal output„ÄÇYou may have seen that we did not specify requires bright equals true for the model's layers within a subclass of Torch„ÄÇ NN dot moduleÔºå the gradient tracking is managed for you„ÄÇIf we look at the layers of the model„ÄÇ

 you can see the randomly initialized weights and that they have no gradients computed yet„ÄÇYou might have noticed there's a grad function on the weights I sampled„ÄÇ There's no grad function on the weights themselves because they're a leaf node of the computation graph„ÄÇBut the slice operation counts as a differentiable operation„ÄÇ

 So my little slice of the weights is a grad function indicating that it came from the slice„ÄÇSo let's see how this changes after one training batch„ÄÇFor a loss function„ÄÇ we'll use the square of the Euclidean distance between our prediction and our ideal output„ÄÇWe'll also set up a basic optimizer using stochastic gradient descent„ÄÇ

Note that we initialize the optimizer with the model's learning weights of parameters„ÄÇ The optimizer is responsible for adjusting the weights„ÄÇSo what happens when we call lost do backwardÔºüWe can see that the weights have not changed„ÄÇ but that we do have gradients computed„ÄÇ AgainÔºå these gradients guide the optimizer in determining how to adjust the weights to minimize the loss score„ÄÇ

In order to actually update the weightsÔºå we have to call optimizer„ÄÇt step„ÄÇAnd we can see that the rates have changed„ÄÇThis is how learning happens in your pie torch models„ÄÇThere's one more important step in the process„ÄÇAfter you call optimizer„ÄÇstep„ÄÇ you need to call optimizer„ÄÇ0rograd„ÄÇIf you don'tÔºå the gradients will accumulate over every training batch„ÄÇ

For exampleÔºå if we run a training batch five times without calling a zero grad„ÄÇYou can see the gradients turn up mostly with much larger magnitudes because they were accumulated over each batch„ÄÇAnd you can see the calling the zero grad resets the gradients„ÄÇIf your model' is not learning or training is giving you strange results„ÄÇ

 the very first thing you should check is whether you're calling zero grad after each training step„ÄÇSometimes you'll want to control whether gradients get tracked for a calculation„ÄÇ There are multiple ways to do thisÔºå depending on the situation„ÄÇ The easiest is just to set the acquire grad flag directlyÔºå like so„ÄÇ

And we can see that B1 has a grad functionÔºå but B2 does not because we turned off history tracking in A prior to computing B2„ÄÇIf you only need autograd turned off temporarilyÔºå you can use the Torch„ÄÇ Norad Con Manager„ÄÇWhen we run this cell„ÄÇWe can see that history is tracked for all computations„ÄÇ except the one in the nograd context„ÄÇ Nograd can also be used as a function or method decorator„ÄÇ

 causing history tracking to be turned off for computations inside the decorated function„ÄÇIt's a corresponding context managerÔºå Torchdot and Agrad„ÄÇFor turning autograd on in a local context„ÄÇ it may also be used as a decorator„ÄÇFinallyÔºå you may have a tensor tracking history„ÄÇ but you need a copy that doesn't„ÄÇIn this caseÔºå the Tensor object has a detached method which creates a copy of the tensor that is detached from the computation history„ÄÇ

We did this above when we grabbed some of our tensors„ÄÇ Ma Totlib expects a nuy array„ÄÇ but the implicit tensor to nuy array conversion is not enabled for tensorors tracking history„ÄÇ Once we make our attached copyÔºå we're good to go„ÄÇThere's one more important note about autogra mechanics„ÄÇ You have to be careful about using in place operations on Tensor's track gradients„ÄÇ

 Doing so may destroy information you need to correctly do your backward pass later„ÄÇ In fact„ÄÇ Pytorch will even give you a runtime error„ÄÇIf you try to perform an in place operation on an input tensor that requires gradients„ÄÇAutograd tracks every step of your tensor computation„ÄÇ combiningbining that information with time measurements would be useful for profiling gradient tract computations„ÄÇ

 and in factÔºå this feature is part of autograd„ÄÇThe next code cell„ÄÇShows basic usage of the profiler„ÄÇThe autograd profiler can also group results by code blocks or input shape and can export results for other tracing tools the late documentation has full details„ÄÇ

Pytororch 1„ÄÇ5 saw the introduction of the Autograd High level API„ÄÇWhich exposes some of the core operations underlying autograd„ÄÇIn order to explain this best„ÄÇ I'm going to go into some more mathematical depth on what autograrad is doing under the hood„ÄÇSo say you have a function with n inputs and M outputsÔºå what' say y equals a function of x„ÄÇ

The complete set of partial derivatives of the outputs with respect to the inputs is a matrix called the Jacobian„ÄÇNowÔºå if you have a second functionÔºå which we'll call L„ÄÇ which equals G of y that takes an n dimensional input„ÄÇThat is the same dimensionality as the output of our first function and return to scalar output„ÄÇ

You can express its gradients with respect to Y is a column vector„ÄÇIt's really just a one column Jacob„ÄÇTo tie this back to what we've been talking about„ÄÇ imagine the first function as your pitorrch model with potentially many inputs and many learning weights and many outputs„ÄÇ and the second function as a loss function with the model's output as input and the loss value as the scalar output„ÄÇ

If we multiply the first functions to Cobian by the gradient of the second function and apply the chain rule„ÄÇ we get another column vector„ÄÇThis column vector represents the partial derivatives of the second function with respect to the inputs of the first function„ÄÇ

Or in the case of our machine learning model„ÄÇThe partial derivative of loss with respect to learning weights„ÄÇTchto autograd is an engine for computing these vector Jacobian products„ÄÇ This is how we accumulate the gradients over the learning weights during the backward pass„ÄÇFor this reasonÔºå the backward call can also take an optional vector input„ÄÇ

The vector represents a set of gradients over the output tensor„ÄÇ which are multiplied by the Jacobian of the autograd trace tensor that precedes it„ÄÇLet's try a specific example with a small vector„ÄÇIf we tried to call Y dot backward now„ÄÇ we'd get a runtime error and a message that gradientians can only be implicitly computed for scalar outputs„ÄÇ

For a multidimensional outputÔºå Autora expectses to provide the gradients for those three outputs that it can multiply into the Jacobia„ÄÇMost of the output gradients here are all related to the powers of the two„ÄÇ which we'd expect from the repeated doubling operation in the previous cell„ÄÇThere's an API on Autograd that gives you direct access to important differential matrix and vector operations„ÄÇ

 in particularÔºå it allows you to calculate the Jacobian and Hesher matrices of a particular function for particular inputs„ÄÇThe Hesians like the JacobanÔºå but expresses all partial secondary„ÄÇLet's take the Jacobian of a single function and evaluate it for two single element inputs„ÄÇAnd if you look closelyÔºå the first output should equal2 times e to the x since the derivative of e to the x is the exponential itself„ÄÇ

And the second value should be three„ÄÇNow you canÔºå of courseÔºå do this with higher order tensors„ÄÇHere we've computed Jacobbian have that same adding function with a different set of inputs„ÄÇThere's also a function to directly compute the vector Jacobcoium product if you provide the vector„ÄÇAutograd's JVP method does the same matrix multiplication as VJP with the operarans reversed„ÄÇ

 the VHP and HVVP methods do the same for the vector Hesian project„ÄÇFor more information„ÄÇ including important performance notesÔºå see the documentation for the new Autograd Funal API„ÄÇ![](img/25eee23f7825074bf201a2a5a23d28cb_3.png)

![](img/25eee23f7825074bf201a2a5a23d28cb_4.png)