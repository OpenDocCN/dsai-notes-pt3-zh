- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P61：L11.5- 使用Keras和TensorFlow从头开始学习英语
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P61：L11.5- 使用Keras和TensorFlow从头开始学习英语
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeffine。 welcomelcom to applications of deep neural networks with
    Washington University。 You know what， Maybe we humans are overrated。After all。
    Al 0 was able to master chess in hours without even the benefit of human knowledge。
    We try to do natural language processing。 we use all these fancy addins like an
    L T K and Space E and other things。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是Jeffine。欢迎来到华盛顿大学的深度神经网络应用。你知道吗，也许我们人类被高估了。毕竟，AI在几小时内就能掌握国际象棋，甚至不需要人类知识的帮助。我们尝试进行自然语言处理，使用了各种高级插件，比如L
    T K和Space E等。
- en: Maybe we should just let the neural networks， try to learn English from scratch
    or other languages as well。 This is called end to end neural networks。 For the
    latest on my AI course and projects。 Click subscribe and the bell next to it to
    be notified of every new video。 Connuing onward。 We're going to look at another
    type of。😊。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们应该让神经网络从零开始学习英语或其他语言。这被称为端到端神经网络。有关我最新的AI课程和项目，请点击订阅，并点击旁边的铃铛以接收每个新视频的通知。继续前进。我们将看另一种类型的。😊
- en: '![](img/511897e72e03ef43ebc5740c467bd96e_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511897e72e03ef43ebc5740c467bd96e_1.png)'
- en: Welcome to applications of deep learning with Washington University and see
    how we can teach a neural network to learn to read So these are the two papers
    that I am pulling this from based on and then I'm also using the code from the
    Kira's examples where the creator of Kiuras implemented intoto end neural networks
    as an example of cururas this will probably eventually be built intokira's layer
    specific type so that you can use this directly but for now you see some of the
    plumbing used to actually implement this network type and there's also an online
    JavaScript demo that you can use to play with this if you would like to as well
    we are going to go ahead and actually create a end to end neural network here
    and make use of it so these are just some necessary imports and utility functions
    that were created by in the Ki's example I made some changes to these to make
    it work a little bit better and eliminate some of the warnings that Hatheng was
    generated because it was generated possibly on an older version of Kis I also
    added a Q&A part at the end。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到华盛顿大学的深度学习应用，看看我们如何教神经网络学习阅读。这是我从这两篇论文中提取的内容，同时我也使用了Kira示例中的代码，Kira的创建者实现了端到端神经网络作为cururas的一个示例，这最终可能会构建成Kira的特定层类型，以便你可以直接使用，但现在你会看到一些用于实际实现这种网络类型的底层代码，还有一个在线JavaScript演示，你也可以用来玩一玩。如果你愿意，我们将实际创建一个端到端神经网络并加以利用，这些只是一些必要的导入和工具函数，它们是根据Kira的示例创建的，我对这些进行了些许更改，使其工作得更好，并消除了一些Hatheng生成的警告，因为这些警告可能是在Kis的较旧版本上生成的。我还在最后添加了一个问答部分。
- en: The original code it trained the network and it gave you the accuracy but it
    didn't give you any way to use it。 So I created that part at the end。 Let's go
    ahead and run this。 It tells you it's using Tensorflows the back end and then
    we see that it is still running。 it takes it a little while to little end but
    while it's doing that let me show you some of the parts that we're doing here。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码训练了网络并给出了准确率，但没有提供任何使用它的方法。所以我在最后创建了那部分。让我们继续运行它。它告诉你正在使用Tensorflow作为后端，然后我们看到它仍在运行。它需要一点时间才能完成，但在此期间，让我向你展示一下我们正在做的一些部分。
- en: This is the tokenized function it uses a regular expression to split apart the
    words of a sentence so this would turn something like Bob D the apple period where
    is the apple question mark so this is very much in the tasks form but this gives
    you an array of words rather than just one big sentence So this is called tokenization
    it tokenizes the sentence and breaks it apart for you This par is the stories
    So this takes stories that are in Babbby task format and breaks them into the
    three parts So you will get and sort of separated by tab delineation I won't take
    you through all of the steps of this but basically what it is doing is taking
    the three parts the parts are you will have a story where it'll say John walk
    to the bathroom Mary went to the bedroom that's essentially。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标记化的函数，它使用正则表达式分离句子中的单词，因此这会将“Bob D the apple. Where is the apple?”转换为一个数组，而不是一个大句子。这被称为标记化，它将句子标记化并为你分解。这一部分是故事，因此它将以Babbby任务格式存在的故事分解为三部分。你将会看到，它们是通过制表符分隔的，我不会详细讲解所有步骤，但基本上，它的作用是提取三个部分，其中包括“约翰走进浴室，玛丽走进卧室”的故事。
- en: First part of it the first of three the second part is in the question where
    is John or some other some other thing and then the third is the answer the answer
    to this type of neural network is always a single word so you are asking a questions
    about the story and then you then give it a question about it and it gives you
    a word back the important thing to realize about the training of the neural network
    is the weights and the training of the neural network are learning to read not
    learning the specific sentences because after the neural network is trained you'll
    give it new sentences and it should be able to answer questions about them because
    the training data has all sorts of instances of Mary's in the bathroom Marys in
    the bedroom all different locations those are specific to that particular training
    example it's not the locations that it's trying to learn it's trying to learn
    how to read new stories and be able to answer about them and then this function
    here that we call will get the stories by calling the other functions that that
    we just had the parse stories and it returns the stories in a vector format that
    can be passed to the neural network find。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是三部分中的第一部分，第二部分是关于“约翰在哪里”或其他问题，第三部分是答案，这种类型的神经网络的答案总是一个单词。因此，你在问一个故事的问题，然后给出一个相关的问题，它会返回一个单词。重要的是要意识到神经网络的训练涉及权重，而不是学习特定的句子，因为在神经网络训练后，你会给它新的句子，它应该能够回答关于这些句子的问题，因为训练数据中包含了各种情况，比如“玛丽在浴室”和“玛丽在卧室”，这些都是特定于某个训练示例的位置，它并不是在学习这些位置，而是在学习如何阅读新故事并能对其进行回答。然后，这里我们称之为的函数会通过调用其他函数获取故事，这些函数刚才提到的解析故事，并将故事以向量格式返回，这可以传递给神经网络进行处理。
- en: vectorize the stories I will show what this specifically looks like。 but essentially
    what it's doing is it's taking all of the sentences， the stories。 figuring out
    how many unique words there are and then vectorizing them and changing them into
    pure numeric form where each word has a numeric index that it gets replaced by
    and we'll see how that works in just a little bit this block of code here basically
    downloads the data so we're getting the data from AWS and downloading it to this
    GZ file then we extract out the parts that we want we're doing the 10k challenge。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对故事进行向量化，我将展示这具体是怎样的。但本质上，它的作用是提取所有的句子、故事，计算有多少个独特的单词，然后进行向量化，并转换成纯数字形式，每个单词都有一个数字索引来替换，我们稍后会看看它是如何工作的。这段代码基本上下载数据，所以我们从AWS获取数据并下载到这个GZ文件中，然后提取出我们想要的部分，我们正在进行10K挑战。
- en: This is a smaller data， it's still fairly big that keeps us able to run it on
    our computers I should point out the computer that I normally run these examples
    from we're using a AWS instance like I discussed before I basically hit my website
    you won't be able to use that URL because it's my AWS instance and it it costs
    me money not a lot but I normally use a8 gigab。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个较小的数据集，虽然仍然相当大，但它让我们能够在电脑上运行。我要指出的是，我通常运行这些示例的电脑使用的是AWS实例，正如我之前讨论的那样，我基本上访问我的网站，你无法使用那个URL，因为它是我的AWS实例，这会让我产生一些费用，虽然不多，但我通常使用的是8GB的内存。
- en: instancest to do this class session since there's considerably more I use a
    I actually use a 16 gigte。 if I was using the full the bigger data sets forms
    of these。 I would need a much much more ra might work on a computer with8TP so
    we run this I am going to go ahead and run this portion since I ran the previous
    portion。 it is extracting the stories for the challenge and it gets them Now I
    had already run this it had already downloaded them if you were rent this for
    the first time it will probably have to download those。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化这个课堂环节，因为有相当多的内容，我实际上使用了一个16GB的内存。如果我使用的是更大的数据集，我会需要更高的内存，可能在一台8TB的计算机上运行。因此我们来运行这个部分，我会继续运行这一部分，因为我已经运行了之前的部分。它正在提取挑战的故事，并且现在我已经运行过了，它已经下载了它们。如果你是第一次运行这个，它可能需要下载这些。
- en: Let me run this so you can see what the data actually looks like So here's a
    story Mary moved to the bathroom John went to the hallway where is Mary answer
    is bathroom Here's another story and he gonna be multiple sentences。 Mary moved
    to the bathroom John went to the hallway Daniel went back to the hallway Sand
    move to the garden Where is Daniel So it needs to read these and find the last
    location that he said that he moved to and they describe it different ways。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我运行这个，让你看看数据实际上是什么样子的。所以这是一个故事，玛丽搬到了浴室，约翰去了走廊，玛丽在哪里，答案是浴室。这里还有另一个故事，而且将会有多个句子。玛丽搬到了浴室，约翰去了走廊，丹尼尔回到了走廊，桑德搬到了花园，丹尼尔在哪里？所以它需要读取这些并找到他最后说他搬到的位置，并用不同的方式描述它。
- en: back to move to went to So it really does need to learn to read this are。Training
    examples I need to emphasize again the neural network is not remembering that
    Mary is in the bathroom。 That's just unique to this particular sentence。 and there's
    a whole bunch of these。 there's like 10000 of these that it trains over and the
    answer is always a single word so it learns to read a sentence like this and we'll
    see that in the end we can make up our own ad hoc story for it it will read it
    and it will answer something about it but we'll see that it has some very important
    limitations So first we need to build the vocabulary and this is a very important
    limitation of this program we will run that and let me show you what that looks
    like It builds the vocabulary very quickly What it did is it went over all 10000
    of these training stories and the 1000 test stories so we're going to train it
    on these and we're going to test it on these and we're going to see how well it
    can learn to read it pulls out some very important maximums here that are going
    to be implied are imposed on the trained neural network We have 22 unique words
    that's going to be the vocabulary if you look over all of those sentences and
    you。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回到“搬到”和“去”的问题。所以它确实需要学习读取这些训练示例。我需要再次强调，神经网络并不记得玛丽在浴室。这仅仅是这个特定句子的独特性。还有一大堆这样的句子，大约有10000个供其训练，答案总是一个单词，因此它学习读取这样的句子。最终我们会看到，我们可以为它编造自己的临时故事，它会读取并回答一些内容，但我们会看到它有一些非常重要的局限性。所以首先我们需要构建词汇，这是这个程序一个非常重要的限制。我们将运行它，让我给你看看那是什么样子。它非常快速地构建词汇。它对所有这10000个训练故事和1000个测试故事进行了处理，所以我们将在这些上进行训练，并在这些上进行测试，看看它能学会多好地读取。它提取了一些非常重要的极值，这些将会被隐含或施加在训练的神经网络上。我们有22个独特的单词，这将是词汇，如果你查看所有那些句子。
- en: Mary move to bathroom all those words and you find out how many unique。 There's
    only 22 unique words。 This neural network gets very， very memory intense when
    you add more words because those are basically becoming dummy variables so that
    is a big limitation of this but if you throw a memory at it and you throw a computer
    at it there are examples of this that can learn much bigger vocabularies the maximum
    story length is 68 so we're going to essentially create an input that is 68 words
    by 22 dummy variables that's the embedding and another input of the question so
    the query or the question the maximum length that any query was in there is four
    words。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 玛丽搬到浴室，所有那些单词，你会发现有多少个独特的。只有22个独特的单词。当你添加更多单词时，这个神经网络变得非常非常占用内存，因为这些基本上变成了虚拟变量，所以这是一个很大的限制。但是如果你给它更多的内存和计算机，确实有示例可以学习更大的词汇。最大故事长度是68，所以我们将基本上创建一个输入，由68个单词和22个虚拟变量组成，那就是嵌入，另一个输入是问题，因此查询或问题的最大长度是4个单词。
- en: So the input to this neural network is going to become the story which is a
    maximum of 68 words by dummy by 22 dummies and the query length is four words
    by 22 so it effectively becomes 7268 plus4 times the 22 dummies is the input you
    can see this gets big quickly if you start to add additional words for that and
    this is what the vocabulary looks like。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个神经网络的输入将成为故事，最大为68个单词，旁边是22个虚拟值，查询长度是四个单词，旁边也是22个虚拟值。因此，它实际上变成7268加上4乘以22个虚拟值的输入。你可以看到，如果你开始添加额外的单词，这个会迅速变得很大，这就是词汇的样子。
- en: Now this neural network will take a while to train it will take up to on the
    faster instance that I'm running this on。 it will take about four minutes to train
    but on slower instances with with fewer compute cores this could take longer so
    we want to save the neural network We also want to save this vocabulary because
    we need the lookup This is based on maps and Python can change the ordering for
    maps you're not guaranteed that putting things into the map will always be the
    same size so this is the lookup table this is telling us that zero is period two
    is Daniel and so on and so forth so when we vectorize these what it's going to
    do is it'll change these words into lookups so Mary is4 Mary will become a4 moved
    15 so that'll become 415 etc so to build the training set let me go ahead and
    execute this part of it it tells you what the input is going to look like so you
    have a 10000 times 68 that's the input vector 10000 times4 and then the answers
    which is going to be the vocab size let me show you some individual train。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个神经网络需要一些时间来训练，在我运行的这个较快实例上，它大约需要四分钟进行训练，但在计算核心较少的较慢实例上，这可能会花费更长时间，因此我们希望保存这个神经网络。我们还希望保存这个词汇表，因为我们需要查找。这是基于映射的，而Python中的映射顺序是可以改变的，因此你不能保证将东西放入映射后总是会有相同的大小。这是查找表，它告诉我们零对应的是句号，二对应的是丹尼尔，依此类推。当我们对这些进行向量化时，它会将这些词转变为查找值，例如玛丽是4，玛丽会变成4，移动15，所以它会变成415等等。因此，为了构建训练集，让我继续执行这部分，它告诉你输入将会是什么样子，所以你有一个10000乘以68的输入向量，一个10000乘以4的输出，然后答案将是词汇大小，让我给你展示一些单独的训练元素。
- en: ElementsSo this shows you the story so you'll notice we're always making this
    be the same length。 so the 68， which was the maximum word size of any of the stories
    and it's padded。 it's padded to the left So all zeros at the beginning and then
    the actual the actual words that you get here Now one important thing to show
    you these zeros the way the padding works and we have to adjust for this at the
    end。 it's a very important part and I will show you this at the end but notice
    how it says notice how we have 21 words if you count these two things if you were
    paying attention you probably saw 22 unique words so the math doesn't quite add
    up the additional value that we have is really the empty So null 0 is actually
    null0 is the first the first value in that map but really the value is going in
    get one added to it so0 would be one and so on we see that we have to adjust for
    that at the end but that is why we have zeros here So those are actually nulls
    and then we get into the actual values of of the lookup same thing here for the
    question Now this question was the maximum of four。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了故事，所以你会注意到我们始终保持这个长度相同。68是所有故事中单词的最大长度，并且进行了填充。填充是在左边，所以开头是全零，然后是你在这里得到的实际单词。现在一个重要的事情是要展示这些零，填充的工作方式，我们必须在最后进行调整。这是一个非常重要的部分，我会在最后给你展示，但请注意我们有21个单词，如果你数一数这两个东西，如果你在关注，你可能会看到22个独特的单词，所以数学不太对劲。我们有的额外值实际上是空值。所以null
    0实际上是映射中的第一个值，但实际上这个值加1，所以0会变成1，依此类推。我们看到我们必须在最后进行调整，但这就是我们这里有零的原因。因此那些实际上是空值，然后我们进入查找的实际值，问题也是如此。现在这个问题的最大长度是四。
- en: So we have no zero padding now we need to compile the neural network because
    Karas does not natively support the end to end neural network there is basically
    the compute graph has to be created here at least to some degree I will show you
    a little bit of what this looks like we won't go really through all the details
    I would have to go I would have to take you through the entire paper really to
    get through that but we're basically creating three encoders two for the sentences
    and then two for the you sort of extract facts out of the sentence and then you
    encode those facts into with the question to try to build up the answer so that's
    why you need two encoders for for the input which are there and then you have
    another encoder for the question and the input dimension becomes the vocabulary
    size so they can build out those dummy variables to hold that output dimension
    then becomes essentially the number of values in in the next layer then we feed
    these all together and eventually all funnels into an LSTM we're using dropouts
    regularization。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在没有零填充，我们需要编译神经网络，因为Keras本身不支持端到端的神经网络，基本上必须在这里创建计算图，至少在某种程度上。我将向你展示这看起来有点像什么，我们不会真正深入所有细节，我需要带你看整篇论文才能了解，但我们基本上创建了三个编码器，其中两个用于句子，然后两个用于从句子中提取事实，然后将这些事实与问题编码在一起，试图构建答案。所以你需要两个用于输入的编码器，而你还有一个用于问题的编码器，输入维度变成词汇大小，以便构建那些占位变量，输出维度基本上是下一层中的值的数量。然后我们将这些全部结合在一起，最终汇聚到一个LSTM中，我们使用了dropout正则化。
- en: Throughout this and then it finally goes into a softmax on the answer。 which
    is essentially the classification of which of the vocabulary does it feel is the
    answer to this。 And then finally we create the model and train it or in compile
    it。 we don't train it This part runs pretty quickly It creates basically the neural
    network and the neural network is now built and compiled This is where we're going
    to actually train it we're going to use a batch size of 32 and 120 epos Now this
    is very important。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，它最终进入了答案的softmax，这基本上是对词汇中哪个是答案的分类。最后，我们创建模型并训练或编译它，我们并不训练。这个部分运行得非常快，基本上创建了神经网络，神经网络现在已经构建并编译。这是我们将实际训练它的地方，我们将使用32的批量大小和120的epochs。现在这一点非常重要。
- en: We're going to save the neural network so that since it takes a while to build
    this。 we don't want to just throw it away so we save the neural network。 I added
    this code from the original example。 they did not save the neural network but
    we save it to a H5 file which is a binary file。 I also save the vocabulary to
    a pickle file Pickle is your standard serialization in Python and I report on
    the elapsed time So I am going to go ahead and run this and this will I'll speed
    this up。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保存神经网络，因为构建这个网络需要一段时间，我们不想就这样丢弃它，所以我们保存神经网络。我从原始示例中添加了这段代码，他们没有保存神经网络，但我们将其保存为H5文件，这是一种二进制文件。我还将词汇保存为pickle文件，pickle是Python中的标准序列化格式，我报告了经过的时间。所以我将继续运行这个，并加速处理。
- en: but itll could take4 to 8 minutes to actually do this you see the epoch speeding
    along here and it's going through the training set。 which is100 elements， so it
    just continues to go through this。Using any sort of early stopping it is just
    going to stop at 120 Okay not too bad。 about four minutes with this faster AWS
    instance。 This code I'm not going to execute the neural network was saved the
    chatbot was saved H5 and the vocabulary to a pickle file if you were running this
    and you already had the chat bot and the pickle file saved you could basically
    just run this part instead of the training we're going to evaluate the accuracy
    noticeice that we we do this very similar to the code that we've done before except
    look at the input the input due to the way that that compute graph was structured
    earlier。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上完成这个过程可能需要4到8分钟，你可以看到时代正在快速推进，并且它正在处理训练集。训练集包含100个元素，所以它只是继续进行。使用任何形式的提前停止，它将在120时停止，好的，还不错。使用这个更快的AWS实例大约需要四分钟。这个代码我不会执行，神经网络已保存，聊天机器人已保存为H5格式，词汇也保存为pickle文件。如果你正在运行这个并且已经保存了聊天机器人和pickle文件，你基本上可以只运行这部分，而不是训练，我们将评估准确性。注意，我们的做法与之前的代码非常相似，只是看输入，由于计算图早期的结构方式，输入有所不同。
- en: the input is actually two vectors it is the inputs and the queries so the inputs
    are the stories Mary went to the hallway etc queries are the questions are where
    is Mary and we'll run it and we'll print out the predictions so this is the typical
    probability that that we've had before you'll see now we're not seeing all of
    the values but each row is basically showing you the 22 vocab words that you had
    is showing you what the probability of。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 输入实际上是两个向量，一个是输入，另一个是查询，所以输入是故事，玛丽去了走廊，等等，查询是问题，比如玛丽在哪里，我们将运行它并打印出预测结果，这就是我们之前看到的典型概率。你现在会看到我们并没有看到所有的值，但每一行基本上显示的是你拥有的22个词汇的概率。
- en: of those words being the answer。 Now you can see if you use a decentsized vocabulary
    where you have maybe 20000 words。 you can the inputs to this type of network become
    very， very large。 So just like before we're gonna take the arg maxes of these
    the arg max now is basically the index of each of these so that's showing vocabular
    word 12 was the expected answer9 was the expected answer there are 151215 not
    all of the words and the vocab list tend to have tend to be the answer very often
    you can definitely see that certain numbers are chosen more often than others
    we can also print out our final accuracy95%'s actually very good Now I want to
    show you how to do an ad hoc query the original example from Cars that I copy
    these from didn't do didn't display the accuracy or show you how to break these
    vectors apart so that's all added by this class if you happen to look at the original
    the original example that I have a link to just above they also don't show you
    how to do an ad hoc query which I think is probably the coolest part of this you've
    trained this great neural network that learns how to read so you want to try to
    test。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词是答案。现在你可以看到，如果你使用一个合适大小的词汇量，比如20000个词，这种类型的网络的输入会变得非常非常大。因此，就像之前一样，我们将取这些的arg
    max，现在的arg max基本上是每个的索引，所以这显示词汇单词12是期望的答案，9是期望的答案，151215，并不是所有的词汇列表中的词都经常是答案。你肯定能看到某些数字比其他数字更常被选中。我们也可以打印出我们的最终准确率，95%实际上非常好。现在我想告诉你如何进行临时查询。我从汽车中复制的原始示例没有显示准确率或告诉你如何分解这些向量，因此这些都是这个类添加的。如果你碰巧查看我上面链接的原始示例，他们也没有告诉你如何进行临时查询，我认为这是最酷的部分。你训练了这个伟大的神经网络，它学会了如何阅读，所以你想尝试测试。
- en: So I am giving it literally raw text right here。 It prints out its vocabulary。
    It's reminding you。 remember I only know these words and I tell it Mary moved
    to the bathroom。 John went to the garden。 etc。 Where is Mary， and I run it the
    ad hoc query This is an area that I added to the program so you'll see Mary moved
    to the garden。 John went to the garden That's an easy one I just kind of made
    that up and I say where is Mary。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我在这里给出的是字面上的原始文本。它会输出其词汇量。它在提醒你。记住，我只知道这些词，我告诉它玛丽去洗手间了。约翰去了花园。等等。玛丽在哪里，我运行临时查询。这是我添加到程序中的一个区域，所以你会看到玛丽去了花园。约翰去了花园。这是一个简单的例子，我只是随便编造的，然后我问，玛丽在哪里。
- en: they're both in the garden So if if I run that it'll say garden。 I can change
    this to Mary went to the bathroom and it better change its example or it's answer
    there。 a live demo bathroom good Mary traveled and there's several ways you can
    spell travel depending on the English system。 they're using the two L so I have
    to used the2 Ls but it doesn't matter it's still understand that John went to
    the garden。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他们都在花园里。所以如果我运行这个，它会说花园。我可以把它改成玛丽去了洗手间，它会更好地改变示例或答案。一个实时演示，洗手间好，玛丽旅行了，根据英语系统，"旅行"可以有几种拼写方式。他们使用两个L，所以我必须使用两个L，但这没关系，它仍然理解约翰去了花园。
- en: there's additional people that you can add into here I can add Daniel and we're
    moving Mary to the hallway and we can run it and it's not retraining the neural
    network It's using the neural network that is learned to read these type of sentences
    and answering the questions。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里添加其他人，我可以添加丹尼尔，我们把玛丽移到走廊，然后我们可以运行它，而它并没有重新训练神经网络，而是使用已经学会阅读这些句子的神经网络来回答问题。
- en: I can also change the query。Say where is Daniel and it adjust it for that。 I'll
    try Now let me show you a few things that don't work so well。 If I say Daniel
    went to the game。 that's outside the vocabulary。 it will blow up on that because
    that's not in the dictionary to look up so we can't say Daniel went to the game
    I have not honestly tried this one but I'm going to try here。 I'm just curious
    what it will do does it have a concept of time So if I say Daniel went to the
    hallway。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我也可以改变查询，比如问丹尼尔在哪里，它会为此进行调整。让我试试。现在让我给你展示一些效果不太好的事情。如果我说丹尼尔去了比赛，那就超出了词汇范围。因为在字典中找不到，所以会崩溃。我老实说还没有尝试这个，但我想试试。我只是好奇它会怎么做，它是否有时间的概念。如果我说丹尼尔去了走廊。
- en: Mary went to the bathroom。 Daniel went to the bedroom Daniel has changed his
    location he was in the hallway。 then he went to the bedroom。 what happens if I
    ask it about that it's smart。 it figures that out because if I change this Daniel
    was in the hallway。 he went to the bathroom and then it updates it for the other
    one。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 玛丽去了洗手间。丹尼尔去了卧室。丹尼尔改变了他的地点，他曾在走廊，然后去了卧室。如果我问它这个问题会发生什么？它很聪明。它能理解，因为如果我改变这个，丹尼尔在走廊，他去了洗手间，然后它会为另一个更新。
- en: if you think about writing a traditional program to do this。 you would be parsing
    this apart and keeping some sort of state of where everybody is at The neural
    network is just figuring out how to do that and it figures out how to override
    it with subsequent answers and I bet there's examples in the training set that
    teach it this teach it that the last answer is correct you could adjust the training
    set so that the first answer is correct like。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑编写一个传统程序来做到这一点，你需要解析这些内容并保持某种状态，以了解每个人的位置。神经网络只是弄清楚如何做到这一点，并且它会随着后续答案的出现而覆盖这些状态。我敢打赌，训练集中有一些示例教它这一点，教它最后一个答案是正确的。你可以调整训练集，以便第一个答案是正确的。
- en: groundund information it would learn that you could also put the word originally
    in there and train it for Daniel went to the hallway。 Mary went to the bathroom。
    Daniel went to the bedroom。 Where was Daniel originally and it would very likely
    learn to know that that was the first location that the given person was in。 I'm
    going to also try something else mean to it。 I'm going to ask it where is Sandra
    who does not even appear in this in this story。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基础信息，它会学习到你也可以在这里加入“最初”这个词，并为“丹尼尔去了走廊。玛丽去了洗手间。丹尼尔去了卧室。”进行训练。问丹尼尔最初在哪里，它很可能会学会知道那是该人最初所在的位置。我还想尝试一些其他的事情，问它桑德拉在哪里，她在这个故事中根本没有出现。
- en: So it can't say I don't know。 it's saying that she's in the garden。 if I had
    to wager and let me actually go ahead and print it's prediction。 So that's the
    probability except I had switched it to a Max So I'm printing out just the probability
    it's confidence almost So it is showing that yeah it's pretty sure it's 90% sure
    that's the highest probability that I see here yeah if I had to venture a guess
    it is saying that Sandra is in the garden because in the train set that's probably
    where she most commonly was。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它不能说我不知道。它说她在花园里。如果我必须打赌，让我实际上去打印它的预测。那么这就是概率，除了我将其切换到最大值，所以我只打印出概率，几乎是它的置信度。所以它显示出是的，它相当确定，90%的置信度是我在这里看到的最高概率。如果我必须猜测，它在说桑德拉在花园里，因为在训练集中，她最常待的地方可能就是那里。
- en: I don't know I would have to I would have to verify that that hypothesis but
    this is really a pretty cool technology This is。You could play with this pretty
    easily and train it to learn your own types of sentences。 You could do things
    like whereas Daniel originally like I had told you there。 you just need to generate
    the appropriate training data for this and it will literally learn to read those
    sentences that you have the more complicated those sentences become the more difficult
    the bigger the neural network needs needs to be so the neural network back up
    here I didn't really show you a lot of its architecture without having to go into
    into the paper but you would need to increase that size of 32 and then some of
    these dense layers here right now it's just doing a dense layer the vocab size
    so that's what's fed into the softmax but you could create some additional dense
    and regularization layers In fact the cares author is even noting that you would
    probably need to add those So this is showing you how you can basically teach
    the neural network to read completely from scratch it's limited it's got a small
    vocabulary but for four minutes on a 16 gigab computer this is actually pretty
    Thank you for watching the video and。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道，我需要验证那个假设，但这真的是一个相当酷的技术。你可以轻松地玩这个，并训练它学习你自己的句型。你可以做一些事情，比如丹尼尔最初提到的那样。你只需生成适当的训练数据，它就会真的学会阅读你提供的那些句子。句子越复杂，神经网络需要的规模就越大，所以神经网络的架构我没有详细展示，而是直接说到你需要将大小从32增加，然后在这些密集层中，现在只是在做一个密集层，词汇大小就是输入到softmax的内容，但你可以创建一些额外的密集层和正则化层。事实上，作者甚至指出你可能需要添加这些。因此，这显示了你如何基本上从零开始教神经网络阅读，它的能力有限，词汇量也小，但在16GB的计算机上运行四分钟，这实际上还不错。感谢观看视频。
- en: '![](img/511897e72e03ef43ebc5740c467bd96e_3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511897e72e03ef43ebc5740c467bd96e_3.png)'
- en: Next module we're going to look at reinforcement learning this content changes
    often so subscribe to the channel to stay up to date on this course and other
    topics in artificial intelligence。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的模块我们将探讨强化学习，这个内容变化频繁，因此请订阅频道以便及时了解本课程和其他人工智能主题。
