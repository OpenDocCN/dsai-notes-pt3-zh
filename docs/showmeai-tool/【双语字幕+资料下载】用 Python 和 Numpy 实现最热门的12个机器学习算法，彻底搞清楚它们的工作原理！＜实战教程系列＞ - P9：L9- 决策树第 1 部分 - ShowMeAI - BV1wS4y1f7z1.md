# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P9ÔºöL9- ÂÜ≥Á≠ñÊ†ëÁ¨¨ 1 ÈÉ®ÂàÜ - ShowMeAI - BV1wS4y1f7z1

HiÔºå everybody„ÄÇ Welcome to a new machine learning from Sc tutorial„ÄÇ Today„ÄÇ

 we are going to implement a decision tree using only built in Python modules and nuy„ÄÇ

 A decision tree is a very simple but powerful concept„ÄÇ

 The idea is to build a tree that splits our data such that we have the best separation of our classes„ÄÇ

 A decision tree is also the basis for the very popular random forest model„ÄÇ

 which we will implement in the next tutorial„ÄÇ So I hope you will stick with me„ÄÇüòäÔºåAnd as always„ÄÇ

 we will start with the theory first„ÄÇ So let's look at an example to understand the concept and the math behind the decision tree„ÄÇ

 Let's say we want to predict if a person is walking or taking the bus to go to work„ÄÇ

 and we have 10 observations or 10 samples with two different features„ÄÇ

So we have the feature if it is raining and then we have the featureÔºå how much time do they haveÔºü

And then we have the prediction or the class labelÔºå yes or no„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_1.png)

And now we want to build our tree that splits our data„ÄÇ

 so we put all the samples in the root node and then we apply a question so we ask if it is raining and if the answer is yes„ÄÇ

 then we go to the right and we put all the samples where the answer is yes„ÄÇ

 so these three into the right note and if the answer is no„ÄÇ

 then we go to the left and put all the other samples in the left node„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_3.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_4.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_5.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_6.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_7.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_8.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_9.png)

And here on the rightÔºå we can immediately say that the answer is noÔºå They don't walk„ÄÇ So we put the„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_11.png)

Or return the class label 0„ÄÇ And here on the leftÔºå we can further split the data and grow our tree„ÄÇ

 So we ask the next question„ÄÇ So do they have more than 10 minutes and again„ÄÇ

 if the answer is yes and we go to the right and put all of the„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_13.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_14.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_15.png)

Samples where where they have more than 10 minutes in the right note and all the other ones in the left note and then again here we can say that the answer is yes or the class label is1 and here the class label is 0„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_17.png)

And we could also stop with the growing here and return the most common class label„ÄÇ

 So here we can say that it's more likely that they will walk„ÄÇ

m but we could also further split our data and but we don't want to grow our data too much because we don't want to overfit our data„ÄÇ

 but we also want to have a good prediction„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_19.png)

So this is the concept of how the decision tree works and now the only thing that we have to find out is at what at which node we want to apply which question so why do we ask for rain at first and not for the time and why do we ask for the time is greater than 10 and why don't we ask for if it's greater than 5 or greater than 20„ÄÇ

 so this is the best the so-called best„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_21.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_22.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_23.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_24.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_25.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_26.png)

Split feature and best split value or split threshold that we want to find out„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_28.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_29.png)

SoÔºå the concept is to„ÄÇAt each noteÔºå we want to find the best split value and the best split threshold and store them„ÄÇ

And this is our training phase and later when we want to predict a new test sample„ÄÇ

 then we start at the top and then we traverse our tree and apply the questions or the features that we start and go to the left or to the right until we reach a leaf node so a leaf node the node at the bottom„ÄÇ

 and then we apply the most common label that we stored based on our test samples„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_31.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_32.png)

So this is the concept„ÄÇ And now how do we find out the best split„ÄÇ So for thisÔºå we need some math„ÄÇ

 So we calculate the entropy and the entropy is also is the a measure for uncertainty„ÄÇ

 So the formula is minusÔºå and then we have the sum of p of x times lock of p of x and p of x is the number of occurrences over the total number of samples„ÄÇ

 So in our example we have 110 samples and five times the answer is or the label is 0 and five times the answer is„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_34.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_35.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_36.png)

1„ÄÇAnd then we have itsÔºå our entropy is minus5 over 10 times lock of 5 over 10 minus„ÄÇ

 And then we look at the next„ÄÇLabelÔºå so one againÔºå we have five labels„ÄÇ

 so minus5 over 10 times lock of 5 over 10„ÄÇ And if you calculate thisÔºå then this is one„ÄÇ

 So this is the worst possible case here in our in the first note we cannot make a prediction which are if it is one or0„ÄÇ

 because we have an equal amount of both„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_38.png)

Samples or both classes here„ÄÇSo one is the worst possible case and like here the entropy here would be0„ÄÇ

 so this is the best case here we are very certain that we have the class0„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_40.png)

And now so we calculate the entropy and then we split our data and calculate the entropy for our childs„ÄÇ

And then we calculate how much information we gain through this split and this measure is actually really called information gain„ÄÇ

 And this is calculated by the entropy of the parent minus weighted average of all the child entroies„ÄÇ

 So like in our exampleÔºå we have the root note with our 10 observations„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_42.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_43.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_44.png)

And then on the left sideÔºå we have 7 observations with hereÔºå2 zeros„ÄÇAnd5 one„ÄÇ And on the right side„ÄÇ

 we have three zeros„ÄÇ So our entropy is the entropy of the parent-7 over 10 times the entropy on the left side plus 3 over 10 times the entropy of the right side„ÄÇ

 So this is the information gain„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_46.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_47.png)

And now we do a greedy searchÔºå so we go over all the possible features and over all the possible feature values or thresholds„ÄÇ

 so for rain we check if it's yes or no or it means one or zero and if the answer and for the time feature„ÄÇ

 we check for5Ôºå 10Ôºå 15Ôºå 20Ôºå 25 and 30 so we do a greedy search„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_49.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_50.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_51.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_52.png)

And thenÔºå we store„ÄÇOr select and store the best feature and the best threshold„ÄÇ So this is the„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_54.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_55.png)

ConceptÔºå so let's look at the approach a little bit more in detail„ÄÇ So first„ÄÇ

 we have the training phase where we want to build our tree„ÄÇ So we start at a top node„ÄÇ

 and at each nodeÔºå we select the best split based on the best information game„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_57.png)

And we do a greedy searchÔºå so we loop over all features and over all thresholds„ÄÇ

Then we saved the best split feature and split threshold at each node and we built the tree recursively„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_59.png)

And we also have to apply some stopping criteria to stop growing„ÄÇ And in our example„ÄÇ

 we will use a maximum depth„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_61.png)

So for exampleÔºå we say if the depth three is5Ôºå then we would stop„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_63.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_64.png)

And we use a minimum samples at a node„ÄÇ SoÔºå for example„ÄÇ

 we could say if we have less than five samples in a node then we don't split any further„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_66.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_67.png)

And if we have no more class distribution in a node like here„ÄÇ

 then we can no longer split because we only have class 0 labels here„ÄÇ

 So these are our stopping criteria that we need„ÄÇ And then when we have a leaf node„ÄÇ

 then we store the most common class label of this node„ÄÇ So here we store 0Ôºå1 and 0„ÄÇ



![](img/e1b43f93535591fc8a4dc158e9ae735d_69.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_70.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_71.png)

![](img/e1b43f93535591fc8a4dc158e9ae735d_72.png)

So this is our training phase and then later when we do the testing with new samples„ÄÇ

 then we traverse the tree and we also do this recursively„ÄÇ

 so we implement a function that calls itself„ÄÇAnd at each note„ÄÇ

 we look at the best split feature of the test feature vector and go left or rightÔºå depending on if„ÄÇ

The value of this feature„ÄÇIs smaller or equal than our threshold that we storedÔºü

And then when we reach the leaf node or the bottomÔºå we return the stored most common class label„ÄÇ

 So this is the concept of the„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_74.png)

Decis 3„ÄÇ And now I think I will stop this video here„ÄÇ

 and we will continue in the next part or in the second part with the implementation„ÄÇ

 So see you then„ÄÇ

![](img/e1b43f93535591fc8a4dc158e9ae735d_76.png)