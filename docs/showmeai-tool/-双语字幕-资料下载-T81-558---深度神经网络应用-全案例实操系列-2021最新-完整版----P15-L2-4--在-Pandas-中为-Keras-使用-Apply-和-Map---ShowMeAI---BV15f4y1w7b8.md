# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P15ÔºöL2.4- Âú® Pandas ‰∏≠‰∏∫ Keras ‰ΩøÁî® Apply Âíå Map - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeffine and welcome to applications of Deep neural networks with Washington University In this video„ÄÇ we're going to look at how to use apply and map functions together with pandas„ÄÇ This allows you to write Lambda functions or other functions that will be executed across your entire data frame„ÄÇ This allows you to do relatively complicated transformations to your data frame that might be very useful for feature engineering„ÄÇ

 which allows you to represent the data in ways that will allow your neural network to be more predictive for the latest on my AI course and projects„ÄÇ click subscribescribe and the bell next to it to be notified of every new video„ÄÇ applyly and map or two functions that are provided by a pandas data frame that you can use directly on these„ÄÇ Now we've seen map already for just general Python data structures lists in particular„ÄÇüòä„ÄÇ



![](img/5e8029765ef65f0d9c303a190e424729_1.png)

But map can be used with data frames to do similar types of transformations for more advanced feature engineering that you'll probably have to do with some of the projects in this class„ÄÇApply and map can be veryÔºå very useful and we'll look at a specific case„ÄÇ

In the later part about how to do just that„ÄÇSo let's go ahead and load in the autompG data set„ÄÇWe are going to do a very simple mapping on it„ÄÇ This is one of the most common things that you will do with for maps„ÄÇ This is similar to a decode„ÄÇIn SQL„ÄÇWhat we're going to do is this origin value that you have here specifies the region that the car was from„ÄÇ One is North America„ÄÇ2 is EuropeÔºå and3 is Asia„ÄÇSo what we're going to do for this is we're going to take the origin„ÄÇ

We're going to create a column called originig name and we're going to fill it in all cases where this is what the map will do„ÄÇ it's going to look up the origin value and it's going to replace it with North America for one„ÄÇ Europe for two and Asia for threeÔºå so let's run that„ÄÇAnd if I scroll that over a little bit„ÄÇ you can see North AmericaÔºå North AmericaÔºå Asia for that three„ÄÇAsiaÔºå Europe is two„ÄÇAnd so on„ÄÇ

 So this can be a very good way to summarize values if you could actually„ÄÇI mean„ÄÇ say you had numbers in here that were all the 50 states„ÄÇOf the United States of America„ÄÇ you could put a bunch of values in here that go to the same thing„ÄÇ You could do one to North AmericaÔºå2 to North America or even state namesÔºå Missouri to North America„ÄÇ

 California to North AmericaÔºå New YorkÔºå to North America and so on and so forth„ÄÇ Whereas if you saw FranceÔºå it would go to Europe„ÄÇ So you can also use this to to summarize„ÄÇ apply is another„ÄÇThing that you can use„ÄÇWith these data sets„ÄÇApply basically takes a function„ÄÇ typically a lambda and runs it against every single„ÄÇRow in the data„ÄÇSo here„ÄÇ

 let's go ahead and run thisÔºå this is calculating something that I call an efficiency„ÄÇIt's basically the displacement divided by the horsepower„ÄÇSo how big is your engine versus how much horsepower do you actually getÔºü

 And this gives you a general indication of the efficiency of the car„ÄÇ So we could add that in to the data set„ÄÇ This shows how you're„ÄÇ you're sort of engineering a feature based on„ÄÇBased on this ratio„ÄÇHere I'll show you a more complex example of feature engineering„ÄÇ

 This was actually an assignment in previous semesters„ÄÇYou'll probably see an assignment about like this one in this semester„ÄÇThis basically is using a data set that we have here from the IRS„ÄÇ which is a United States government„ÄÇTaxaing authority„ÄÇAnd we're going to„ÄÇ

 there's also more data documentation on here if you care to read about it„ÄÇ but this is basically a data set that gives you the estimated adjust gross income„ÄÇPlus„ÄÇ other things for zip codes in the United StatesÔºå so the fields that you care about are state„ÄÇ which is the state exampleÔºå Missouri„ÄÇZip code„ÄÇIs the zip code of within that stateÔºü

AGI Stub are six different income brackets„ÄÇSo one is the lowest incomeÔºå six is the highest income„ÄÇEach zip code is going to have six rows for it then they give you a countÔºå which is in one„ÄÇOf the number of people in each of those income brackets„ÄÇ so that gives you sort of a distribution of wealth in that particular zip code„ÄÇ

What we're going to try to do is put those bands back together so that you have a„ÄÇEstimate of what what the overall AGI is for that that particular zip codeÔºå so this is a reduction„ÄÇ you've got zip codeÔºå you've got six rows for each zip code„ÄÇ you want to crunch that down to just one row per zip code and just give an overall estimate of the adjust roasted income„ÄÇ

And this is kind of what the file looks like„ÄÇ See you've got 63017„ÄÇ this is a zip code that I work in„ÄÇAGI StubÔºå so that's your six values they have for each and then the count„ÄÇSo the second to the highest is the largest income„ÄÇAnd this shows this particular income if you do probably the most„ÄÇ

Famous zip code in in the United States of AmericaÔºå90210Ôºå it'd probably be pretty high„ÄÇ because that's a very affluent zip code„ÄÇ So you see a lot of numbers down here„ÄÇ What we're going to do to put it back together„ÄÇ These are this is the bands for each of these1 is 1 to 25 k 6 is 200000 or more„ÄÇ NowÔºå one of the issues with putting this back together is 200000 or more„ÄÇ That's the R more„ÄÇ

 that's a pretty big category that could be up toÔºå I don't knowÔºå probably many millions of dollars„ÄÇ HoweverÔºå much those guys are making„ÄÇ So what we're going to do is we're going to establish a median for each of these ranges„ÄÇ125„ÄÇ Now this last one since we don't know what the ceiling is 2Ôºå12500„ÄÇ That's probably the biggest flaw in this estimate that I'm that I'm putting together„ÄÇ But you„ÄÇ

 you can see about where those are at„ÄÇ So the way that we can estimate6Ôºå3Ôºå0Ôºå17s„ÄÇ average Agi„ÄÇIs we're going to basically sum all those in ones„ÄÇ So that's our total count„ÄÇ We're going to add those all up„ÄÇ We're going to total up the AgisÔºå but waiting them by the medians„ÄÇ So we're going to take the count of people that„ÄÇSo the 4710 times 12500 because that's how many people are in bandand one„ÄÇ

 that's the median for band one„ÄÇBand twoÔºå there were two 2780 and 37500 was the median for that one„ÄÇ so we're essentially this is sort of a weighted weighted sum you have here„ÄÇAnd then we're going to divide thoseÔºå and that gives us the average income for this zip code is around 88„ÄÇ000„ÄÇWhich seems reasonably correct„ÄÇThis is good too because this gives us a value that we can check by hand because we calculated this by hand„ÄÇ

 so our automated process that's going to calculate for everything single zip code should come up with the same value„ÄÇ So we're going to load that data set into Ram it it's big and gigantic you'd probably want to stream it„ÄÇ but this is this is loading the entire data frame It'll be loaded in a second„ÄÇThat might have took a while„ÄÇ That took probably 20 seconds for me„ÄÇ Now what we're going to do„ÄÇ

 So this is going to show you a good example of using multiple of these techniques together„ÄÇ Now this data set does have some junk data in it„ÄÇ So we have zip codes that are0 and 9999„ÄÇ There's no such zip codes„ÄÇ they do have a purpose in the data fileÔºå but we're simply not using them„ÄÇSo we are going to get all the rowsÔºå that's what Lo does for us„ÄÇ

 where the data frame is not equal to zero zip code and the zip code is not equal to 9999„ÄÇAnd we're also going to only pull back these fields„ÄÇ so we're chopping this thing down horizontally and vertically„ÄÇ we're getting rid of a bunch of columns and all the rows that have these invalid zip codes„ÄÇ

You run that„ÄÇThis is actually very quick and now the data frame has been modified if you were to display the data frame„ÄÇYou'll see that it has literally just these four„ÄÇAnd the invalid ones are also gone„ÄÇ but we're not going to go hunting for them„ÄÇWhat we're now going to do is we're going to take all of these AGI stubs and replace those with the medians„ÄÇThose medians that I gave you oneÔºå36Ôºå so this is a brilliant application of the map„ÄÇWe run that„ÄÇ

AgainÔºå it's veryÔºå very quick„ÄÇAnd if we look at this„ÄÇ we now see that those should have transformed and they have the AGI stubs are now„ÄÇThose actual income medians„ÄÇ NextÔºå we group it by zip code„ÄÇ and for each of those zip codes now„ÄÇ we can basically perform a lambda„ÄÇ So this is a great use for apply„ÄÇ

 We're going to do apply across it and we're going to basically sum because we want to sum up all of the values„ÄÇüòäÔºåIn each of those zip codesÔºå but we're going to sum them weighted„ÄÇ so we're going to basically wait the n1„ÄÇ So how many of them we have„ÄÇ say the 30 there multiplied by the AGI StubÔºå and then we're dividing the whole thing by in one„ÄÇ

 which„ÄÇWhich is the sum of the N1sÔºå actuallyÔºå you can see the sum there for that entire zip code„ÄÇThen we reset the index so that the row numbers align like we've seen before„ÄÇWe run that„ÄÇThat does take just a few seconds„ÄÇ Let's see what that actually did„ÄÇ We now pretty much have the data set where we want it to be„ÄÇZip code„ÄÇ

 and this is the average for each of those„ÄÇThe column headers are a little bit sloppyÔºå zip code„ÄÇ that's goodÔºå but zeroÔºå what's zeroÔºå zero should be our average AGI„ÄÇOur AGI estimate„ÄÇWe'll just rename those two columns and then we display the top„ÄÇAnd we can see now we've got this nice output„ÄÇ You'd probably want a two CSsv then„ÄÇ

 and that that would be a completely valid submission for that previous assignment that I ran in„ÄÇLast semester„ÄÇWe'll look up our zip code that we calculated by handÔºå and that matches our 88„ÄÇ689 that we had before„ÄÇ Thank you for watching this video In the next video„ÄÇ we're going to continue with using pandas and talk about feature engineeringÔºå which is how you„ÄÇüòä„ÄÇ



![](img/5e8029765ef65f0d9c303a190e424729_3.png)

Represent your data set in a way that helps the neural network to be more predictive„ÄÇ This content changes oftenÔºå so subscribe to the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇ