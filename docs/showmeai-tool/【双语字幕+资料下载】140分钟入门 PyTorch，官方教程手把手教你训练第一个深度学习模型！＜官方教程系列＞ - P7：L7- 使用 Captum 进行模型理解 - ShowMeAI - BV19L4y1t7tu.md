# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë140ÂàÜÈíüÂÖ•Èó® PyTorchÔºåÂÆòÊñπÊïôÁ®ãÊâãÊääÊâãÊïô‰Ω†ËÆ≠ÁªÉÁ¨¨‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P7ÔºöL7- ‰ΩøÁî® Captum ËøõË°åÊ®°ÂûãÁêÜËß£ - ShowMeAI - BV19L4y1t7tu

![](img/2fb9bd9df95d3221c8b5a298fc267de0_0.png)

Welcome to the next video in the Pytorrch Tra series„ÄÇ

 This video gives an overview of Cap Pytorrch's tool set for model interpretability„ÄÇüòäÔºåIn this video„ÄÇ

 we'll discuss„ÄÇThe basic concepts of captain that we'll be covering attributions„ÄÇ

 attribution algorithms and visualizations„ÄÇWe'll demonstrate how to perform and visualize feature attributions for a computer vision classifier„ÄÇ

Will apply layer attribution to the same classifier to examine the activity of a model's hidden layers„ÄÇ

And finallyÔºå we'll look at captive insights and API for creating visualization widgets for images„ÄÇ

 text and other features„ÄÇ

![](img/2fb9bd9df95d3221c8b5a298fc267de0_2.png)

Captain provides a deep set of tools for explaining the behavior of your pietorrch models„ÄÇ

 This video and the accompanying interactive notebook provide only an overview of core features„ÄÇ

 The website at Captain AI contains more in depth tutorialsÔºå documentation and an API reference„ÄÇ



![](img/2fb9bd9df95d3221c8b5a298fc267de0_4.png)

![](img/2fb9bd9df95d3221c8b5a298fc267de0_5.png)

To run the interactive notebook associated with this videoÔºå you'll want to install Python version 3„ÄÇ

6 or higher„ÄÇFlaask 1„ÄÇ1 or higherÔºå and the latest versions of piey torchÔºå torch visionion and captain„ÄÇ

Captain can be easily installed with Pip or with Ananaconda by specifying the Pytorr channel„ÄÇ

To start withÔºå we're going to take a pre trained image classifier„ÄÇ

 Resnet trained against the imagenet data setÔºå and we're going to use the tools within cap to gain insight into how the model responds to a particular input image to give its prediction„ÄÇ

This first sells a bunch of importsÔºå including attribution methods and visualization tools from captain„ÄÇ

 which we'll examine shortly„ÄÇNextÔºå we'll get our pretrain model„ÄÇ

Then we'll pull up an image to work with whereverver you got this video in the interactive notebook should also include a folder of images for use in this tutorial„ÄÇ

 In our caseÔºå it's going to be a cat„ÄÇNext we'll define some image transforms to prepare the image for consumption by the model„ÄÇ

And bring in the human readable labels of the thousand imagenet classes„ÄÇNow„ÄÇ

 let's see what the model thinks this is„ÄÇAnd thinks our cat is a cat„ÄÇ

But why does the model think this is a picture of a cat„ÄÇ



![](img/2fb9bd9df95d3221c8b5a298fc267de0_7.png)

For the answer to thatÔºå we can look under the hood of the model of cap„ÄÇ

The core abstraction in cap is the attribution„ÄÇ and that is a quantitative method of attributing a particular output or activity of a model with its input„ÄÇ

The first kind of attribution is feature attribution„ÄÇ

This lets us ask which parts of the input were most important in determining a model's prediction„ÄÇ

It lets us find answers to questions like„ÄÇWhich words in this input question were most significant in deciding the answer„ÄÇ

Which pixels in this input image drove the model's classification of the image„ÄÇ

Which features of the input data were most significant to my regression model's prediction„ÄÇ

Feature attribution just covers inputs and outputsÔºå though„ÄÇ

What if we want to see what's happening inside the model„ÄÇFor thatÔºå we have layer attribution„ÄÇ

This attributes the activity of a hidden layer of a model to the model's input„ÄÇ

It lets us answer questionsÔºå like„ÄÇWhich neurons in this layer were most activeÔºå given this input„ÄÇ

Which neurons in this layer were most important to how the input influenced a particular output neuro„ÄÇ

How is the activation map output by this convolutional layer correlated to my input image„ÄÇFinally„ÄÇ

 there's neuron attribution„ÄÇ This is similar to layer attribution„ÄÇ

 but goes down to the level of individual neurons in the model„ÄÇIn this tutorial„ÄÇ

 we're going to look at feature attribution and layer attribution„ÄÇ



![](img/2fb9bd9df95d3221c8b5a298fc267de0_9.png)

FirstÔºå feature attribution„ÄÇAttributions are realized by an attribution algorithm„ÄÇ



![](img/2fb9bd9df95d3221c8b5a298fc267de0_11.png)

A particular method of mapping model activity to inputs„ÄÇ

The first feature attribution algorithm we'll look at is called integrated gradients„ÄÇ

This algorithm numerically approximates the integral of the gradients of the model's output with respect to its inputs„ÄÇ

 essentially finding the most important paths through the model for a given input output pair„ÄÇ

We'll go ahead and create an integrated gradient objectÔºå initializing it with our model„ÄÇ

Then we'll call the attribute method on it„ÄÇWe'll feed it our input„ÄÇ

 our output label and an optional number of steps to run„ÄÇ

Note that running the cell can take a couple of minutes„ÄÇ

 The process of integrating the gradients is computationally intensive„ÄÇ

Once that cell finishes runningÔºå we have a sort of numerical importance map of the cat image with respect to the cat label generated by the model„ÄÇ

For a simple regression model with few output categoriesÔºå we might just print that out as a table„ÄÇ

 but for a more complicated C model with a larger input like an image„ÄÇ

 it would help to be able to relate the importance map to the image visually„ÄÇ

Captain's got you covered„ÄÇVisualization module gives you tools for exactly that„ÄÇHere„ÄÇ

 we're going to make two calls to visualize image at„ÄÇThe first displays the original image„ÄÇFirst„ÄÇ

 we need to make some adjustments to the image„ÄÇ We call a squeeze to remove the batch dimension on the image„ÄÇ

 We make sure we're running on CPU„ÄÇ We detach the image tensor from computation history„ÄÇ Otherwise„ÄÇ

 the image tensor will keep tracking its computation history unnecessarily„ÄÇAnd finally„ÄÇ

 we make it a numpy array and switch the dimensions around and put the color channels last„ÄÇ

The first argument of this method would normally be the attributions„ÄÇ But for this call„ÄÇ

 we're going to make that none„ÄÇ We're just displaying the original image„ÄÇ

The second argument is our transformed image„ÄÇThird argument is a visualization method„ÄÇ

 a string that indicates how you want the visualization to work„ÄÇHereÔºå we told Captain„ÄÇ

 we just want to display the aboginal image„ÄÇFinallyÔºå we give our visualization an instructive title„ÄÇ

The second call will make a visual mapping of the important regions of our image„ÄÇ

The first argument is the attributions we got from integrated gradients„ÄÇ

 And the second is our transformative image„ÄÇFor a method will specify heat nap„ÄÇ

Where color intensity maps to the importance of an image region„ÄÇ

Capize you to use custom color maps from map plotlib„ÄÇ

 And we've made one here that will slightly enhance the contrast of our heat map„ÄÇ

We specify s as positive„ÄÇ We're only looking at positive attributions„ÄÇRunning a cell„ÄÇ

 we can see that the model is paying attention to the outline of the cat„ÄÇ

 as well as the region around the centre of the cat's face„ÄÇ

Let's try another feature attribution algorithm„ÄÇ NextÔºå we'll try occlusion„ÄÇ

Integrated gradients was a gradient based attribution algorithm„ÄÇ Occlusion is different„ÄÇ

 It's a perturbation based method that involves screening out portions of the image and seeing how that affects the output„ÄÇ

As beforeÔºå we're going to specify our input image and our output labeled to the attribution algorithm„ÄÇ

For occlusionÔºå we're going to specify a few more items„ÄÇ

 The first are the sliding window and the stride length„ÄÇ

 And these are analogous to similar configuration options in a convolutional neural network„ÄÇ

We're also going to set our baseline„ÄÇThat isÔºå our representation of an occluded image cellÔºå 0„ÄÇ

Depending on how your data are normalizedÔºå you may wish to specify a different baseline„ÄÇ

 but for zero centered dataÔºå it makes sense to use 0„ÄÇ

We'll run the attribute call and give it a minute„ÄÇAnd in the next cellÔºå we're doing something new„ÄÇ

We're calling visualized image Adder multiple„ÄÇTo show multiple visualizations of the occlusion attribution„ÄÇ

Besides the original imageÔºå we'll show three visualizations„ÄÇ

 The first two are heat maps of both positive and negative attributions„ÄÇ

 You can see that we're providing a list of methods with heat map being the second and third„ÄÇ

 We're also specifying a sign for each visualization„ÄÇ

 And here you can see that we've askedtra positive attributions on one heat map and negative on the other„ÄÇ

 These indicate which„ÄÇFor our final visualizationÔºå we'll use the mask method„ÄÇ

 This uses positive attributions to selectively screen the original image„ÄÇ

 giving a striking visual representation of the areas of the image the model paid most attention to for this input output pair„ÄÇ

Running the cellÔºå you can see that this maps well to what we learn from integrated gradients„ÄÇ

 Most of the activities are on the cat's outlined in the center of its face„ÄÇ

What about what the model is doing under the hood„ÄÇLet's use a layer attribution algorithm to check the activity of one of the hidden layers„ÄÇ

Gradcam is another gradient based attribution algorithm designed for conves„ÄÇ

It computes the gradients of the output with respect to the specified model layer„ÄÇ

 averagevers the gradients for each channel and multiplies this average by the layer activations„ÄÇ

And uses this as a measure of the importance of a layer's output„ÄÇ

To get started with layer attributionÔºå we'll create a layer gradcam object and initialize it with our model and the layer we wish to examine„ÄÇ

Then we'll give it the input output pair and ask it to do attribution„ÄÇ

We can visualize this with a heat mapÔºå as we did before„ÄÇ in this way„ÄÇ

 you can visually examine which areas of a confidencein activation map were like to your output„ÄÇ

We can do better than thisÔºå though„ÄÇSince the output of a convolutional layer is usually spatially correlated to the input„ÄÇ

 we can take advantage of that by up sampling that activation map and comparing it directly with the input„ÄÇ

The layer attribution parent class has a convenience method for up samplingling the lower resolution convenant activation map up to the input size„ÄÇ

We'll do that with the interpolate method here„ÄÇAnd asked the visualizer for a blended heat map„ÄÇ

 showing the original image with a heat map superimposed and a masked image„ÄÇ

Visualizations like this can give you insight into how hidden layers contribute to a particular output from your model„ÄÇ

Captain comes with an advanced visualization tool called Cap Insights„ÄÇ

 which lets you put together multiple visualizations in an in browserrowsed widget that lets you configure the attribution algorithm and its parameters„ÄÇ

Captain Inights lets you visualize textÔºå image and arbitrary data„ÄÇ

We're going to try three images nowÔºå the catÔºå a teapot and a trilaite fossil„ÄÇAgain„ÄÇ

 these images should be available wherever you got the interactive notebook that goes with its video„ÄÇ

FirstÔºå we'll query the model to see what it thinks each of these are„ÄÇAnd it seems to be really okay„ÄÇ

NowÔºå let's set up cap Ins„ÄÇWe're going to use the attribution visualizer object„ÄÇ

 and we'll configure it with our model„ÄÇA scoring function for the model's outputs hereÔºå softm„ÄÇ

A list of the classes the model recognizes here I am stripping out an ordered list of the image net class names„ÄÇ

We'll tell it that we're looking at image features„ÄÇ

 Captive Insights also handles text and arbitrary dataÔºå as well„ÄÇAnd it'll give it a data set„ÄÇ

 which is just an iterable that returns a batch of images and labels„ÄÇ

Note that we haven't specified an algorithm or a visualization method„ÄÇ

These are things that you set up in the in browserrower widget„ÄÇNowÔºå we ask the visualizer to render„ÄÇ

It starts off emptyÔºå but we can set up configuration parameters and ask it to fetch our visualized attributions with the fetch button„ÄÇ

I'm going to leave things at the default setting for integrated gradients„ÄÇ

 Captain needs a few minutes to generate the attributions„ÄÇ

But now we can see that it ranks the first few predictions for each image with their probabilities and provides heat map attribution for the important regions of the image„ÄÇ

 In this wayÔºå Captain insightights lets you experiment with attribution methods and understand the activity that LED to your model's predictions„ÄÇ

 both correct and incorrect and lets you do it visually with minimal code„ÄÇFinally„ÄÇ

 don't forget to look at Captain AI for documentation„ÄÇ

 tutorials and API reference and access to the source on Gitthub„ÄÇ



![](img/2fb9bd9df95d3221c8b5a298fc267de0_13.png)