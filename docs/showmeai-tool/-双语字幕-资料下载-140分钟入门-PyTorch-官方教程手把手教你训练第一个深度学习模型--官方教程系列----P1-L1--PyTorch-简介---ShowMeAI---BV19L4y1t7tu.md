# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë140ÂàÜÈíüÂÖ•Èó® PyTorchÔºåÂÆòÊñπÊïôÁ®ãÊâãÊääÊâãÊïô‰Ω†ËÆ≠ÁªÉÁ¨¨‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P1ÔºöL1- PyTorch ÁÆÄ‰ªã - ShowMeAI - BV19L4y1t7tu

HelloÔºå my name is Brad Heinz„ÄÇ I'm a partner engineer working with the Piytorrch team at Facebook„ÄÇIn this videoÔºå I'll be giving you an introduction to PytorrchÔºå its features„ÄÇ key concepts and associated tools and libraries„ÄÇ This overview assumes that you are new to doing machine learning with Pytorrch„ÄÇ

![](img/1181a3784e7b926ea0c178203ada7ae1_1.png)

In this videoÔºå we're going to cover an overview of Pytorr and related projects„ÄÇTenssors„ÄÇ which are the core data abstraction of Ptorrch„ÄÇAutograd„ÄÇ which tries the eager mode computation that makes rapid iteration in your model possible„ÄÇWe'll talk about building a model with Ptorrch modules„ÄÇ

We'll talk about how to load your data efficiently to train your model„ÄÇWe'll demonstrate a basic training loop„ÄÇAnd finallyÔºå we'll talk about deployment with Torchscript„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_3.png)

Before we get startedÔºå you'll want to install Pytorrch and torch visions so you can follow along with the demos and exercises„ÄÇ If you haven't installed the latest version of Pytorrch yetÔºå visit Pytorrch dot org„ÄÇThe front page has an installed wizard shown here„ÄÇThere are two important things to note here„ÄÇ First„ÄÇ coupa drivers are not available for the Mac„ÄÇ Therefore„ÄÇ

 GP PU acceleration is not going to be available by a pietorch on the Mac„ÄÇSecond„ÄÇ if you're working on a Linux or Windows machine with one or more NviIdia couda compatible GPUs attached„ÄÇ make sure the version of couta tool kit you installed matches the couda drivers on your machine„ÄÇ

![](img/1181a3784e7b926ea0c178203ada7ae1_5.png)

So what is PytorchÔºü

![](img/1181a3784e7b926ea0c178203ada7ae1_7.png)

Pytorch„ÄÇorg„ÄÇTells us that Pytorch is an open source machine learning framework that accelerates the path from research prototyping to production deployment„ÄÇLet's unpack that„ÄÇFirst„ÄÇPytorchches software for machine learning„ÄÇ It contains a full toolkit for building and deploying M L applications„ÄÇ including deep learning primitivesÔºå such as neural network layer types„ÄÇ

 activation functions and gradient based optimizers„ÄÇ It has hardware acceleration on Nvidia GPus„ÄÇ and it has associated libraries for computer vision„ÄÇ text and natural language and audio applications„ÄÇTorchssion„ÄÇ the Pytorrch Library for Computer vision applications„ÄÇ

 also includes pre trained models and packaged data sets that you can use to train your own models„ÄÇPytorch is built to enable fast iteration on your M L models and applications„ÄÇ You can work in regular idiomatic Python„ÄÇ There is no new domain specific language to learn to build your computation graph with autograd„ÄÇ Pytorch's automatic differentiation engineÔºå the backward passover your model is done with a single function call and done correctly„ÄÇ

 no matter which path through the code a computation took„ÄÇ offering you unparalleled flexibility in model design„ÄÇPytorch has the tooling to work at enterprise scale with tools like Torchcr„ÄÇ which is a way to create serializable and optimizable models from your Pytorrch codeÔºå Torchserv„ÄÇ

 Pytorrch's model serving solution and multiple options for quantizing your model for performance„ÄÇAnd finallyÔºå Pytorrch is free in open source software„ÄÇ free to use and open to contributions from the community„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_9.png)

Its open source nature fosters a rich ecosystem of community projects as well„ÄÇ supporting use cases from satochastic processes to graph based neural networks„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_11.png)

The Pyetorrch community is large and growingÔºå with over 1200 contributors to the project from around the world and over 50% cent year on year growth in research paper citations„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_13.png)

Petorrch is in use at top tier companies like these and provides the foundations for projects like Alan N LP„ÄÇ the Open source Research Library for deep learninging with Natural LanguageÔºå Fast AI„ÄÇ which simplifies training fast and accurate neural nets using best modern practices„ÄÇClassy vision and end to end framework for image and video classification and cap„ÄÇ

 an open source extensible library that helps you understand and interpret your model's behavior„ÄÇNow that you've been introduced to PytorrchÔºå let's look under the hood„ÄÇ Tensors will be at the center of everything you do in P torch„ÄÇ Your modelsÔºå inputs„ÄÇ outputs and learning weights are all in the form of tensors„ÄÇ



![](img/1181a3784e7b926ea0c178203ada7ae1_15.png)

NowÔºå if Tensor is not a part of your normal mathematical vocabulary„ÄÇ just know that in this context„ÄÇ we're talking about a multidisional arrayÔºå but with a lot of extra bells and whistles„ÄÇPietorrch tensors come bundled with over 300 mathematical and logical operations that can be performed on them„ÄÇThough you access tensors through a Python APIÔºå the computation actually happens in compiled C++ code optimized for CPUU and GPU„ÄÇ

Let's look at some typical tensor manipulations in Pytorrch„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_17.png)

The first thing we'll need to do is import pi torch with the import torch call„ÄÇThen we'll go ahead and create our first tensor here„ÄÇ I'm going to take create a two dimensional tensor with five rows and three columns and fill it with zeros„ÄÇ I'm going to query it for the data type of those zeros„ÄÇ

And here you can see I got my requested matrix of 15 zerosÔºå and the data is 32 B floating point„ÄÇ By defaultÔºå pi torch creates all tensors as 32 bit floating point„ÄÇWhat if you wanted integers insteadÔºå you can always override the default„ÄÇHere in the next cell„ÄÇ I create a tensor full of ones„ÄÇ I request that they be 16 B integers and note that when I print it without being asked„ÄÇ

 Pytorrch tells me that these are 16 B integers because it's not the default that might not be what I expect„ÄÇIt's common to initialize learning weights randomly„ÄÇ often with a specific seed for the random number generators that you can reproduce your results on subsequent runs here„ÄÇ we demonstrate„ÄÇSeeding the pytorch random mirror generator with a specific number„ÄÇ

Generating a random tensor„ÄÇGenerating a second random tensor„ÄÇ which we expect to be different from the first reseing the random number generator with the same input„ÄÇAnd then finallyÔºå creating another random tensorÔºå which we expect to match the first since it was the first thing created after seeding the R G„ÄÇAnd sure enoughÔºå those are the results we get„ÄÇFirst tensor and the third tensor do match„ÄÇ

 and the second one does not„ÄÇArithmetic with Pytorch tensors is intuitive„ÄÇ Tensors of similar shapes may be addedÔºå multipliedÔºå et cetera„ÄÇ and operations between a scalar and a tensor will distribute over all the cells of the tensor„ÄÇ So let's look at a couple of examples„ÄÇFirstÔºå I'm just going to create a tensor full of ones„ÄÇ

Then I'm going to create another tensor full of ones„ÄÇ but I'm going to multiply it by a scalar 2„ÄÇ And what's going to happen is all of those ones are going to become twos„ÄÇ The multiplication is distributed over every element of the tensor„ÄÇThen I'll add the two tensors„ÄÇ I can do this because they're of the same shape„ÄÇThe operation happens element wise between the two of them„ÄÇ

 And we get out now a tensor full of threes„ÄÇ When I query that tensor for its shape„ÄÇ it's the same shape as the two input tensors from the addition operation„ÄÇFinally„ÄÇ I create two random tensors of different shapes and attempt to add them„ÄÇ I get a runtime error because there is no clean way to do element wise arithmetic operations between two tensors of different shapes„ÄÇ

Here is a small sample of the mathematical operations available on Pytorch tensors„ÄÇ I'm going to create a random tensor and adjust it so its values are between -1 and 1„ÄÇI can take the absolute value of it and see all the values turn positive„ÄÇI can take the inverse sign of it because values be are between minus1 and 1 and get an angle back„ÄÇ

I can do linear algebra operations like taking the determinant or doing singular value decomposition„ÄÇAnd there are statistical and aggregate operations as well„ÄÇMeans and standard deviations and minimums and maximumimsÔºå et cetera„ÄÇThere's a good deal more to know about the power of Pytorch tensors„ÄÇ

 including how to set them up for parallel computation on GPU„ÄÇ We'll be going into more depth in another video„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_19.png)

![](img/1181a3784e7b926ea0c178203ada7ae1_20.png)

As an introduction to autoradÔºå Pytorrchs automated differentiation engine„ÄÇ Let's consider the basic mechanics of a single training pass„ÄÇFor this example„ÄÇ we'll use a simple recurrent neural network or RN„ÄÇWe start with four tensorsÔºå XÔºå the input H„ÄÇ the hidden state of the R N N that gives it its memory and two sets of learning weights„ÄÇ

1 each for the input and the hidden state„ÄÇNextÔºå we multiply the weights by their respective tensors„ÄÇIn M here stands for Nature's multiplication„ÄÇAfter that„ÄÇ we add the outputs of the two matrix multiplications„ÄÇAnd pass the result through an activation function hereÔºå Hybolic tangent„ÄÇAnd finally„ÄÇ

 we compute the loss for this output„ÄÇ The loss is the difference between the correct output and the actual prediction of our model„ÄÇSo we've taken a training inputÔºå run it through a modelÔºå gotten an output and determined the loss„ÄÇThis is the point in the training loopÔºå where we have to compute the derivatives of that loss with respect to every parameter of the model and use the gradients over learning weights to decide how to adjust those weights in a way that reduces the loss„ÄÇ

Even for a small model like thisÔºå that's a bunch of parameters and a lot of derivatives to compute„ÄÇBut here's the good news„ÄÇ You can do it in one line of code„ÄÇEach tensor generated by this computation knows how it came to be„ÄÇ For example„ÄÇ I to H carries metadata indicating that it came from the matrix multiplication of W X and X„ÄÇ

 And so it continues down the rest of the graph„ÄÇThis history tracking enables the backward method to rapidly calculate the gradients your model needs for learning„ÄÇThis history tracking is one of the things that enables flexibility and rapid iteration in your models„ÄÇ

 Even in a complex model with decision branches and loops„ÄÇ the computation history will track the particular path through the model that a particular input took and compute the backward derivatives correctly„ÄÇIn a later videoÔºå we'll show you how to do more tricks with autograd„ÄÇ like using the autograd profiler and taking second derivatives and how to turn off autograd when you don't need it„ÄÇ

We've talked so far about tensors and automatic differentiation and some of the ways they interact with your Pytorrch model„ÄÇ But what does that model look like in code„ÄÇLet's build and run a simple one to get a feel for it„ÄÇFirstÔºå we're going to import pie torch„ÄÇWe're also going to import torchsha Nn„ÄÇ which contains the neural network layers that we're going to compose into our model„ÄÇ

 as well as the parent class of the model itself„ÄÇ And we're going to import torchta Nn do functional to give us activation functions and max pullinging functions that we'll use to connect the layers„ÄÇSo here we have a diagram of Linenette 5„ÄÇ It's one of the earliest convolutional neural networks and one of the drivers of the explosion and deep learning„ÄÇ

 It was built to read small images of handwritten numbers„ÄÇ The eddenist data set and correctly classify which digit was represented in the image„ÄÇHere's the abridged version of how it works„ÄÇ Layer C 1 is a convolutional layer„ÄÇ meaning that it scans the input image for features it learn during training„ÄÇ

It outputs a map of where it saw each each of its learned features in this image„ÄÇ This activation map is down sampled in layer S 2„ÄÇLayer C 3 is another convolutional layer„ÄÇ This time scanning C1's activation map for combinations of features„ÄÇ It also puts out an activation map describing the spatial locations of these feature combinations„ÄÇ

 which is down sampled in layer S 4„ÄÇFinallyÔºå the fully connected layers of the end at 5 F 6 and output are a classifier that takes the final activation map and classifies it into one of 10 bins representing the 10 digits„ÄÇSo how do we express this simple neural network in codeÔºüLooking over this code„ÄÇ

 you should be able to spot some structural similarities with a diagram above„ÄÇThis demonstrates the structure of a typical pi torch model„ÄÇ It inherits from torrssha and N dot module„ÄÇAnd modules may be nested„ÄÇ In fact„ÄÇ even the co2 D and linear layers here are subclasss of Torta and end dot module„ÄÇ

Every model will have an in it where it constructs the layers that it will compose into its computation graph„ÄÇAnd loads any data artifacts it might need„ÄÇ For exampleÔºå an NLP model might load a vocabulary„ÄÇA model will have a forward function„ÄÇ This is where the actual computation happens„ÄÇ and input is passed through the network layers in various functions to generate an output„ÄÇ

 a prediction„ÄÇOther than thatÔºå you can build out your model class like any other Python class„ÄÇ adding whatever properties and methods you need to support your model's computation„ÄÇSo let's instanttantiate this„ÄÇAnd run an input through it„ÄÇSo there are a few important things happening here„ÄÇ We're creating an instance of limit„ÄÇ

We are printing the object„ÄÇ now a sub of Tortra and in a module„ÄÇWill report the layers it has created and their shapes and parameters„ÄÇ This can provide a handy overview of a model if you want to get the gist of its processing„ÄÇBelow thatÔºå we created dummy input representing a 32 by 32 image with one color channel„ÄÇNormally„ÄÇ

 you would load an image tile and convert it to a tensor of this shape„ÄÇYou may have noticed an extra dimension to our tensor„ÄÇ This is the batch dimension„ÄÇPytorrch models assume they are working on batches of data„ÄÇ For example„ÄÇ a batch of 16 of our image tiles would have the shape 16 by 1 by 32 by 32„ÄÇ

Since we're only using one imageÔºå we create a batch of one with shape 1 by one by 32 by 32„ÄÇWe ask the model for an inference by calling it like a functionÔºå net input„ÄÇThe output of this call represents the model's confidence that the input represents a particular digit„ÄÇSince this instance of the model hasn't been trained„ÄÇ

 we shouldn't expect to see any signal in the output„ÄÇLooking at the shape of the output„ÄÇ we can see that it also has a batch dimensionÔºå the size of which should always match the input batch dimension„ÄÇ Had we passed in an input batch of 16 instancesÔºå output would have a shape of 16 by 10„ÄÇYou've seen how a model is built and how to give it a batch of input and examine the output„ÄÇ

 The model didn't do muchÔºå thoughÔºå because it hasn't been trained yet for that„ÄÇ We'll need to feed it a bunch of data„ÄÇIn order to train our model„ÄÇ we're going to need a way to feed it data in bulk„ÄÇ This is where the Ptorrch data set and data load classes come into play„ÄÇLet's see them in action„ÄÇ

So here I'm declaring Matpllib in line because we'll be rendering some images in the notebook book„ÄÇ I'm importing pitorrch„ÄÇ I'm also importing torch vision and torch vision transforms„ÄÇ These are going to give us our data sets and some transforms that we need to apply to the images„ÄÇTo make them digestible by our pieytorrch model„ÄÇSo the first thing we need to do is transform our incoming images into a pi torch tensor„ÄÇ

Here we specify two transformations for our input Transs to Tensor takes images loaded by the pillow library„ÄÇAnd converts them into Py toch tensors„ÄÇ transformers dot normalize„ÄÇ adjust the values of the tensor so that their average is 0Ôºå and their standard deviation is 0„ÄÇ5„ÄÇ Most activation functions had their strongest radiance around the0 point„ÄÇ

 So centering our data there can speed learning„ÄÇThere are many more transforms available„ÄÇ including croppingÔºå centeringÔºå rotationÔºå reflection„ÄÇ and most of the other things you might do to an image„ÄÇNext„ÄÇ we're going to create an instance of the Car 10 data set„ÄÇ

 This is a set of 32 by 32 color image tiles representing 10 classes of objects„ÄÇ6 of animals and four vehicles„ÄÇWhen you're on this all above„ÄÇ it may take a minute or two for this the data set to finish downloading for you„ÄÇ so be aware of that„ÄÇSo this is an example of creating a data set in Pytorrch„ÄÇ

 downloadable data sets like Siffer 10 above are subclasses of TorchÔºå Us data data set„ÄÇData set classes in pitorrch include the downloadable dataset sets in torchvision„ÄÇ Torch text and torch audioÔºå as well as utility data set classes such as Torchvision datasets do image folder„ÄÇ which will read a folder of labeled images„ÄÇ You can also create your own subclasses of data set„ÄÇ

When we instantiate our data setÔºå we need to tell it a few things„ÄÇThe file system path where we want the data to goÔºå whether or not we're using this set for training„ÄÇ because most data sets will be split between training and test subsets„ÄÇWhether we would like to download the data set if we haven't already and the transformations that we want to apply to the images„ÄÇ

Once you have your data set readyÔºå you can give it to the data loader„ÄÇNow a data set subclass wraps access to the data and is specialized the type of the data is serving„ÄÇThe data loader knows nothing about the dataÔºå but organizes the input tensors served by the data set into batches with the parameters you specify In the example above„ÄÇ we've asked a data loader to give us batches of four images from train set„ÄÇRandomizing their order„ÄÇ

With shuffle equals true„ÄÇ And we told to spin up two workers to load data from disk„ÄÇ It's good practice to visualize the batches your data loader serves„ÄÇRning the cells should show you a strip of four images and you should see a correct label for each one„ÄÇAnd so here are our four images which do in fact look like a catÔºå a deer in two trucks„ÄÇ

We've looked under the hood at tensors and autograd„ÄÇ and we've seen how pi torch models are constructed and how to efficiently feed them datatum„ÄÇIt's time to put all the pieces together and see how a model gets trained„ÄÇSo here we are back in our notebookÔºå you'll see the imports here„ÄÇ

 all of these should look familiar from earlier in the video except for Torchdot Opum„ÄÇ which I'll be talking about soon„ÄÇThe first thing we'll need is training and test data sets„ÄÇ So if you haven't already run the cell below and make sure the data set is downloaded„ÄÇ you may take a minute if you haven't done so already„ÄÇ

We'll run our check on the output from the data loader„ÄÇAnd again„ÄÇ we should see a strip of four imagesÔºå a plainÔºå plainÔºå plain ship„ÄÇ That looks correct„ÄÇ Sorry„ÄÇ data letter is good„ÄÇThis is the model we'll train„ÄÇ Now„ÄÇ this model looks familiar It's because it's a variant of Lyette„ÄÇ

 which we discussed earlier in this videoÔºå but it's adapted to take three color images„ÄÇThe final ingredients we need are a loss functionÔºå and an optimizer„ÄÇThe loss function„ÄÇ as discussed earlier in this videoÔºå is a measure of how far from our ideal output the model's prediction was„ÄÇ Cross entropy loss is a typical loss function for classification models like ours„ÄÇ

The optimizer is what drives the learning„ÄÇHereÔºå we've created an optimizer that implements stochastic gradient descent„ÄÇ one of the more straightforward optimization algorithms„ÄÇ Besides parameters of the algorithm„ÄÇ like the learning rate and momentum„ÄÇ we also pass in net dot parameters„ÄÇ which is a collection of all the learning weights in the modelÔºå which is what the optimizer adjusts„ÄÇ

FinallyÔºå all this is assembled into the training loop„ÄÇGo ahead and run this cell„ÄÇ as it'll take a couple of minutes to execute„ÄÇSo here we're only doing two training epics„ÄÇ as you can see from line 1Ôºå that is two complete passes over the training data set„ÄÇEach pass has an inner loop that iterates over the training data„ÄÇ

 serving batches of transformed images in their correct labels„ÄÇZing the gradients in line 9 is a very important step„ÄÇ When you run a batch„ÄÇ gradients are accumulated over that batch„ÄÇ And if we don't reset the gradients for every batch„ÄÇ they will keep accumulating and provide incorrect valuesÔºå and learning will stop„ÄÇIn line 12„ÄÇ

 we ask the model for its actual prediction on the batch„ÄÇIn the following lineÔºå line 13„ÄÇ we compute the loss„ÄÇ The difference between the outputs and the labels„ÄÇIn line 14„ÄÇ we do our backward pass and calculate the gradients that will direct the learning„ÄÇIn line 15„ÄÇ the optimizer performs one learning step„ÄÇIt uses the gradients from the backward call to nudge the learning weights in the direction it thinks will reduce the loss„ÄÇ

So the remainder of the loop just does some light reporting on the epic number and how many training instances have been completed„ÄÇWhat the collective losses is over the training epic„ÄÇSo note that the loss is monotonomically descended„ÄÇ indicating that our model is continuing to improve its performance in the training data set„ÄÇ

As a final stepÔºå we should check that the model is actually doing general learning and not simply memorizing a data set„ÄÇ This is called overfitting and will often indicate that either your data set is too small and doesn't have enough examples or that your model is too large that it's overspecified„ÄÇ

For modeling the data„ÄÇYou're treatinging it„ÄÇSo our training was done„ÄÇÂóØ„ÄÇSo anywaysÔºå the way we„ÄÇCheck for overfitting and guard against it is to test the model on data it hasn't trained on„ÄÇ that's where we have a test data set„ÄÇSo here I'm just going to run the test data through„ÄÇ we'll get in accuracy measure out„ÄÇ55% OkayÔºå so that's not exactly the state of the art„ÄÇ

 but it's much better than the 10% we'd expect to see from a random output„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_22.png)

This demonstrates that some general learning did happen in the model„ÄÇNow„ÄÇ when you go to the trouble of building and training a non trivial model is usually because you want to use it for something„ÄÇ You need to connect it to a system that feeds its inputs and processes the model's predictions„ÄÇIf you're keen on optimizing performanceÔºå you may want to do this without a dependency on the Python interpreter„ÄÇ

 The good news is that Pytorrch accommodates you with torch script„ÄÇüòäÔºåTorchscript is a static„ÄÇ high performance subset of Python„ÄÇ When you convert a model to torchscript„ÄÇ the dynamic and pythonic nature of your model is fully preserved„ÄÇ Control flow is preserved when convert to torchscript„ÄÇ

 and you can still use convenient Python data structuresÔºå like lists and dictionaries„ÄÇLooking at the code on the rightÔºå you'll see a py torch model defined in Python„ÄÇBelow that„ÄÇ in instance of the model is createdÔºå and then we'll call torch dot dot script my module„ÄÇThat one line of code is all it takes to convert your Python model to torchscript„ÄÇ

 The serialized version of this gets saved in the final line„ÄÇ and it contains all the information about your model's computation graph and its learning weights„ÄÇThe torch grip rendering of the model is shown at the right„ÄÇTorch script is meant to be consumed by the piytorrch just in time compiler or Jit„ÄÇ

 The Jit seeks runtime optimizations such as operation reordering and layer fusion to maximize your model's performance on CPU or GP hardware„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_24.png)

So how do you load and execute a torch script model„ÄÇ You start by loading the serialized package with a torch shott Jet dot load„ÄÇ and then you can call it just like any other model„ÄÇWhat's moreÔºå you can do this in Python or„ÄÇYou can load it into the Pytorrch C plus plus runtime to remove the interpreted language dependency„ÄÇ

In subsequent videos we'll go into more detail about Torch scriptÔºå best practices for deployment„ÄÇ and will cover Torch Ser P Torch's model serving solution„ÄÇ![](img/1181a3784e7b926ea0c178203ada7ae1_26.png)

So that's our lightningÔºå fast overview of Pyetorrch„ÄÇ The models and data sets we used here were quite simple„ÄÇ But Pytorrchches used in production at large enterprises for powerful real world use cases„ÄÇ like translating between human languagesÔºå describing the content of video scenes or generating realistic human voices in the videos to follow will' give you access to that power will go deeper on all the topics covered here with more complex use cases like the ones you'll see in the real world„ÄÇ

üòäÔºåThank you for your time and attention„ÄÇ And I hope to see you around the Pytorrch Fors„ÄÇ