# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPytorch ËøõÈò∂Â≠¶‰π†ËÆ≤Â∫ßÔºÅ14‰ΩçFacebookÂ∑•Á®ãÂ∏àÂ∏¶‰Ω†Ëß£ÈîÅ PyTorch ÁöÑÁîü‰∫ßÂ∫îÁî®‰∏éÊäÄÊúØÁªÜËäÇ ÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P4ÔºöL4- PyTorch ÂàÜÂ∏ÉÂºè RPC - ShowMeAI - BV1ZZ4y1U7dg

üéº„ÄÇ

![](img/5747d0e2c4e80d16a5a2b785aa519633_1.png)

HelloÔºå everyone„ÄÇ My name is Shen Li„ÄÇI am a research scientist in the P team„ÄÇToday„ÄÇ

 I will talk about Py distributed package and will'll focus on RPC„ÄÇ

Which is a new feature that we introduced this year„ÄÇ

I will first briefly explain what is PC and then go through some basic RC APIs„ÄÇAfter that„ÄÇ

 I will present high level ideas of how to implement different training applications using RPRPC„ÄÇ



![](img/5747d0e2c4e80d16a5a2b785aa519633_3.png)

AllrightÔºå so what is PythalRPCÔºå PythalRPC is a low level framework for general distributed training„ÄÇ

Before RPC distributeded data parallelel or DDP has been the main feature that Py coach offered for distributed training„ÄÇ

HoweverÔºå GDPDP is one specific training paradigm where every process has a full replica of the model„ÄÇ

 and you split the input across multiple processes„ÄÇIt works for many applications„ÄÇ

 but there are also many other applications that do not fit into GDPP„ÄÇFor example„ÄÇ

 if your model is too large for one machineÔºå you will need multi machine model parallelism„ÄÇ

Which does not work with DP„ÄÇ You will have to manually send intermediate outputs and ingredients across processes and also resume all required in application code„ÄÇ

Which can become very complicated„ÄÇAnother example is that sometimes we need to mix parameter server with DP„ÄÇ

 using parameter servers for sparse sensors and DP for dense sensors„ÄÇ

This cannot be easily done with just DDP„ÄÇOne option to solve this problem is by adding a different feature to support each of these use cases„ÄÇ

NeverthelessÔºå if we did thatÔºå it will create a large API surface„ÄÇ

 which can become a hurdle for users to pick up„ÄÇAnd it will also create a high engineering overhead to write and maintain„ÄÇ

MoreoverÔºå we cannot fully anticipate what new training paradigms might emerge in the future„ÄÇ

Another option is that we can provide a set of flexible low level tools„ÄÇ

 which would allow users to improvise„ÄÇSo here comes Pwach RPRPC„ÄÇ

We hope it can fill the gaps for the distributed training applications that cannot use our GDPDP„ÄÇ

And if some new distributed training paradigm becomes widely adopted„ÄÇ

 we always have the option to introduce a dedicated high level API for it„ÄÇ



![](img/5747d0e2c4e80d16a5a2b785aa519633_5.png)

So now we know why we introduce RPRPC„ÄÇThen what features do RC offer„ÄÇS up 1„ÄÇ7„ÄÇ

 theres already a long list of features in IP PC package„ÄÇ

But I will only focus on the most basic parts in today's talk„ÄÇThe first one is remote execution„ÄÇ

This allows you to run a user function on a specific remote process„ÄÇ

Which is what you would expect from any RPC system„ÄÇThe second one is remote reference„ÄÇ

Which allows you to reference the remote object and pass it around„ÄÇ

Using remote execution without transmitting the real data„ÄÇIt serves as a discreted ShaPoer„ÄÇ

The third one is a distributed daughter grad„ÄÇIt will stitch together local autograd graphs of remote executions into one distributed autogra graph„ÄÇ

 so that when you call backward on the loss tensor in one process„ÄÇ

 it will reach out to all participating processes and machines and compute gradients accordingly„ÄÇ



![](img/5747d0e2c4e80d16a5a2b785aa519633_7.png)

That's a very high level explanation„ÄÇ Let's now dive into more details„ÄÇRemote execution„ÄÇ

It allows running a carable remotely„ÄÇIn Part sharp CÔºå all processes are„ÄÇ

Any process can be both color and colly„ÄÇAfter initialization by calling in RRPC„ÄÇ

 any process will have a message listener running in the background and use a thread pool to process requests and responses„ÄÇ

After thatÔºå there are three ways to run a callable remotely„ÄÇRRPC syncncÔºå RC A sync and a remote„ÄÇ

RRPC S rolls R PCC synchronously„ÄÇIt will block until the return value is available on the color„ÄÇ

RBC ASyncÔºå on the other handÔºå immediately returns a future object of the result„ÄÇ

The third API is remoteÔºå which also returns right away„ÄÇ but instead of returning a future object„ÄÇ

 it returns a remote reference„ÄÇWhich can be treated as a discreted sharePoer of the result„ÄÇ

The difference between R PCC async and remote is that the former will always fetch the result back to the color„ÄÇ

But the remote API does not do that„ÄÇ The remote reference will keep the result alive on the call Li„ÄÇ

The code in this slide shows how to pass different types of callables to A R PCC API„ÄÇ

It can be Pythtch building operatorsÔºå it can be user Python functions can also be script functions„ÄÇ

If performance is a concernÔºå please use script functions„ÄÇ

 as there won't be a contentions on the global interpreter log„ÄÇ

 meaning a different IP PCCs can run concurrently on the call„ÄÇRemote reference„ÄÇ

A remote reference is like a distributed sharepointer„ÄÇ

 It points to an object on the local or remote processes and will manage the lifetime of the data object using the reference count„ÄÇ

This is useful when a call would like to directly forward the output from the col to another process and avoid fetching the real data back to the color„ÄÇ

In the exampleÔºå worker Ze is acting as a coordinator process„ÄÇ

It sets up data dependencies across four other processes„ÄÇ

 and none of the real data object goes through W0„ÄÇThe white dashed arrows are lightweight R PCC remote cause„ÄÇ

 and the bold red arrows are heavyweight data patterns„ÄÇLet's look at one of the remote cause„ÄÇ

 the one from W 0 to W 3„ÄÇThe arguments R and RB are remote references of outputs from W1W2 when using R and RB as arguments in RPRPC API„ÄÇ

 the RRPC system will automatically fork R and RB on W3 and increment the reference count accordingly„ÄÇ

Then on W 3Ôºå it costs2 here to fetch the real data from W1 and W2„ÄÇ

 which will block until the data is received in W 3„ÄÇIn this way„ÄÇ

 the remote reference allows WDro to asynchronously and efficiently set up function executions and data dependencies in a distributed environment„ÄÇ

Remote execution and remote reference help applications to piece together forward path across processes and machines„ÄÇ

Another important component in Pytorch is the autograd systemÔºå which powers the backward path„ÄÇ

The RPC framework extends local autograd engine to work in a distributed environment„ÄÇ

And it also provides distributed o to update all parameters involved in the application„ÄÇ

One difference between local autograd and distributed autograd is that instead of storing the gradient in the peri dotgrad field„ÄÇ

 the distributed autograd engine stores the gradients in a dedicated context„ÄÇ

 and there can be multiple autograd contexts coexist for the same set of parameters„ÄÇ

The reason for this design is because there can be multiple concurrent backward passes showing same parameters and when this happens„ÄÇ

 we need to make sure that these backward passes do not step into each other's toes„ÄÇHence„ÄÇ

 in order to use distributed autograÔºå you need to first create„ÄÇIt contacts„ÄÇ

All R PCs made within that context will carry the context I information„ÄÇ

 which will help callers and colleagues to find each other in the back of the path„ÄÇ

In this tool example on the rightÔºå it first uses two remote calls to create two kilometers and initializes the distributed optimizeim using the list of kilometer remote references„ÄÇ

After thatÔºå it runs the forward pass by simply fetch the parameter from the owners and then sum them together„ÄÇ

Then it feeds the loss tensor to the discreted autogra backward function„ÄÇ

Which were compute gradients for all parameters in the distributed auto grid graph and store the gradient in the context„ÄÇ

FinallyÔºå we can pass the contact I D to the distributed optimizeimr step function„ÄÇ

Which will reach out to owners of all parametersÔºå retrieve corresponding ingredients from context and update parameters using the provided local optim„ÄÇ

In this caseÔºå it's STGD„ÄÇAs you can seeÔºå the API for distributed training is very similar to local training„ÄÇ

Except that you will need to create a context for it„ÄÇ



![](img/5747d0e2c4e80d16a5a2b785aa519633_9.png)

Given all these new tools„ÄÇWhat can you do with themÔºü

It unlocks many discrete training scenarios on PythtorchÔºå and I will briefly describe three of them„ÄÇ

The first one is a parameter server where you can have one parametermeter server„ÄÇ

Or several sharded peri service holding the premiers„ÄÇ

 And then there can be multiple trainers running training integrations„ÄÇ

The R PC framework can help link them together„ÄÇAnother example is a distributed model parallel„ÄÇ

Where the model might not fit in one machine„ÄÇIn this case„ÄÇ

 you can divide the model into multiple sub modules and use RPC and RF to piece them together„ÄÇ

The third example is pipeline parallelism„ÄÇ You can use the asynchronous APIs in R PCC to process one batch and then run interventions on multiple batchges concurrently„ÄÇ

Tinorials are available for all these use cases„ÄÇAll right„ÄÇ

 that's a very short introduction of Python Sha PCC„ÄÇWith this talk„ÄÇ

 I want to make sure that at least I deliver one message to youÔºå which is„ÄÇ

 if DDP does not sufficient for your use caseÔºå please try P tryC„ÄÇWe have quite a few tutorials„ÄÇ

 and we also have an extensive API page„ÄÇ We are actively monitoring data issues and forum questions for RPC on daily basis„ÄÇ

 so let us know if you encounter any problem and also let us know if there's any way that Pyth RRPC can be improved„ÄÇ



![](img/5747d0e2c4e80d16a5a2b785aa519633_11.png)

![](img/5747d0e2c4e80d16a5a2b785aa519633_12.png)