# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P17ÔºöL17- ÂÆåÊï¥ÁöÑ TensorBoard ÊåáÂçó - ShowMeAI - BV1em4y1U7ib

The goal of this video is for you to get a thorough understanding of how to use Tensorboard to easier understand the bug and modify our models„ÄÇThis is a long video because there is so much to cover when it comes to Tensor board„ÄÇ So let me first of all show you an overview of the things we're going to learn how to do in this tutorial„ÄÇ We're going to start off learning about what is perhaps the most basic but also probably the most useful thing which is obtaining these accuracy and these loss plots„ÄÇ

 and in this example right hereÔºå you see the accuracy plot when training our model with varying learning rates„ÄÇ and then we're going to move on and learn about how to visualize images So in this example here we're performed some minor data augmentation in turning a very few percentage of the images to grayscale and so visualizing these changes can be very useful to make sure that what we intend to happen is actually what is going on„ÄÇ

 So for exampleÔºå here we have a horse that has been converted to a grayscale image Also we're going to see how to both create and„ÄÇthese confusion matrices to tensor board where we have the predicted label on the y axis and then we have the true label on the X axis in this way we can see what and where the model is misclassifying images„ÄÇ

 So for exampleÔºå here we have that it correctly predicts airplane 65% of the time but in 7% of the time it's misclassifying a bird as an airplane Also we can see that it predicts cat correctly 40% of the time where the largest mistake happens when it misclassifies it as a dog in about 21% of the cases other useful things is that you can visualize the graph of our model so here we have a very„ÄÇ

 very simple model which takes in two inputs X and Y performs a matrix multiply and then send that through a re and that will be the output of the model„ÄÇAnd so this can be useful if you're building more complicated or complex models like Resnets where you want to make sure that all of the skip connections are at the correct places„ÄÇ

 or if you just want to get a better understanding and see more visually how the model looks like other cool things that you can do with Tensor board is then you can visualize and see the distribution of the various parameters of your model so this can be useful to debug if you get error messages and you want to see exactly where in the model the error is occurring then these distribution plots can be useful to know which layer the default lies in but we also have a lot more things to look at so were for example going to do hyperparameter search using the Hpars API of Tensor board with this you can see what parameters to use for your model„ÄÇ

By seeing the different correlations between in this case the dropout„ÄÇ the learning rate and then the number of units and then how that finally corresponds to the accuracy after a single epoC run so to easier understand this right here we can restrict our attention to only looking at what corresponds to the best acuracies and what we can see here is that the model preferably wants quite low dropout and then it wants a very high learning rate and also high number of units so in this case we're checking the accuracy on the training set so that's probably white wants very low levels of dropout and we are also going to learn how to use the projector tab of Tensor board which is pretty awesome visually and you can use different algorithms like the TSneE and then the PCA to get an understanding of how your model is learning to represent these images„ÄÇ

Projected down from a very high dimensional space to 3D where we can easily visualize them„ÄÇ For exampleÔºå here we are running it on the MNIS data set and we canÔºå for example„ÄÇ use the TS& algorithm and if we for example now search for I don't know zero then we can see that there are clusters of that zero right here and similarly if we would search for two„ÄÇ we would see that there's a cluster over here of the number two and so this can be useful to understand how the model works and also how the model create its representation the Tensorflow profiler is also a very useful tool that allows you to see what is taking up the most amount of time and sort of what aspects you should aim to improve in your training so for example here we're given a summary of the different parts and how much time it takes out of our training So for example in this graph right here we can see that the input is taking„ÄÇ

Very big part of the time and so what they also do is give you some recommendations and they say that the program is highly input bound because about 84% of the total step time is waiting for input and so what we should try to focus on in this particular program is improving the efficiency of our input pipeline and there are also other parts of the profiler where you can see many different other parts as well„ÄÇ

 but more on that later now you have an idea of the different parts and areas of Tensor board that we're going to cover so let's get started„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_1.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_2.png)

AlrightÔºå so to make this video at least relatively well structured„ÄÇ I've divided it into different files where if each file we're going to focus on one specific part of Tensor board„ÄÇAnd for each of them I have a starter code and my thinking is that I'm going to go through this starter code which is based on code for many of the previous tutorials I've done„ÄÇ so I'm going to go through it relatively quickly and then you can check out the previous videos and I'm going to refer to those to go into more depth in each of the different parts„ÄÇ

 but so we're going to base everything from this starter code and then we're just going to modify it or add some parts to write to Tensor board„ÄÇSo to start off with these are the basic imports that we're going to need and then this is what we've seen previously„ÄÇ

 we're going to add those two lines just to avoid any GPp errors„ÄÇWe're loading the Cypher 10 data set from Tensorflowlow datas„ÄÇ we're normalizing those images by dividing by 255„ÄÇ and then we have some data augmentation on those images we also then we do the mapping and the cache and then the batching and prefeting on those training and test set„ÄÇ

Although we're not shuffling on the test set„ÄÇAnd then we have the class names right here„ÄÇ and these are used later on in the projector tab to see the correct label for each respective image„ÄÇThen we have the a model right hereÔºå very simple model using comb layersÔºå two comb layers„ÄÇ a max pullingingÔºå a flatten and then two dense layers which they drop out in between„ÄÇ

And then right here we have some setup for custom training loops so we're getting the model first of all„ÄÇ then we have the loss functionÔºå we have the optimizer at at in this case and then we have a accuracyysymmetric to keep track of the current accuracy„ÄÇ

And then we have these two train ridersÔºå which is what we're going to use to write to attentionure board and these are going to be train and for the test so for each of them we're going to keep track of a step for the train step and the test step„ÄÇAll rightÔºå and then we have custom loop„ÄÇWe we're going through for epoch in range of nu Epos and then for batching that in DS train and then so this is pretty basic„ÄÇ

 we're doing for propagationÔºå backward propagation„ÄÇ and then we're doing gradient into or an optimizer step right here„ÄÇThen we're doing the same thing for the training setÔºå although we're not doing back propagation„ÄÇ we're just checking what the accuracy is„ÄÇAl rightÔºå so for this first file„ÄÇ

 I'm actually going to do something that may look a little bit weird„ÄÇ but we're going to remove a lot of this code„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_4.png)

So we're going to go just to hereÔºå I think and„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_6.png)

The idea here is that that starter code is what we're going to base pretty much every of those files you can see right here„ÄÇ but this specific one we want to use callbacks and this is a more simple way where we're not using custom training loops so I want to show that first and so after we get the model we're just going to do model dot compile we're going to specify our optimizer in this case we're just going to use Adam„ÄÇ

And then we're going to specify the loss„ÄÇ In this caseÔºå we're going to use sparse categorical„ÄÇCross entropyÔºå and then from logÔºå it equals true„ÄÇWe're also going to specify the metric„ÄÇ which in this case is going to be the accuracy„ÄÇAlrightÔºå and then we're going to do„ÄÇTensor board callback„ÄÇSo this is perhaps the most simple way to use Tensor board and I'm going to show you exactly what it does„ÄÇ

 but it's very convenient so all you have to do is you need to do cars„ÄÇ callllbacks do Tensor board you got to specify a log directory where it's going to keep track of all the logs and so let's just do that let's call it TBb callback directory„ÄÇAnd then we specify some histogram frequency and this is justÔºå we're going to set that to one„ÄÇ and that is for the distribution plots of the gradients and all of that stuff that I showed you previously„ÄÇ

AlrightÔºå so late lastly we're going to do the model that fitÔºå so we're going to send in the DS train„ÄÇ we're going to send the number of epochsÔºå let's say I don't know a5„ÄÇ then we're going to do validation data we're going to send in the test set actually I mean normally you would let's just for purposes let's just pretend the test set is the validation data I know this is very incorrect to do but so normally you would split the training data into a validation data and then send that in but that doesn't really matter for this purpose„ÄÇ

And then we're going to specify the callbackÔºå we're going to do Tensor board callback„ÄÇAnd then we're going to specify verbos equals 2„ÄÇAlrightÔºå so that's it„ÄÇ let's run this and I'm going to show you then how it looks like in Tensor board„ÄÇ But actually„ÄÇ before we do thatÔºå I have to show you how you would actually run it„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_8.png)

So how you would run this is you would go to the folder where you have„ÄÇThats specific where you're running this specific script„ÄÇ and then you're going to also have activated the environment that you have Tensorflow on„ÄÇAnd then you would do rather tensor boardÔºå you would do log De„ÄÇ

 and then you would specify the one that you called right hereÔºå so TB callback directory„ÄÇAnd if you run that„ÄÇWe're going to get back a URLÔºå which is local hosts 6006 in this case„ÄÇ and so if we go to that„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_10.png)

And we update it we'll get something that looks like this where we have the training and then the validation accuracy for each epoC and then we also have it for the loss function so as you can see which is perhaps a little bit strange have that the validation accuracy is actually higher but just keep in mind that we're using dropout and then we're also using quite I mean and we're also using some data augmentation which might make it more difficult for the model to overfit on the training data so just for example we're converting some of them to grayscale and of course that that could be quite more challenging so that's probably why normally you would see this in reverse that the training is higher than the validation„ÄÇ

Allright but as we can see right up on this tab right here we have four different ones„ÄÇ so that's just for the scrsÔºå that's just for the loss and the accuracy„ÄÇ you also have the graph that sort of tells us how it looks like right we have our input„ÄÇ two comb layers max pull flat and dense drop out and then another denses„ÄÇ

So you can inspect it right hereÔºå then we have the distributions that I showed you previously where you can see the gradients of all of the parameters„ÄÇOr rather the distribution of the parameters„ÄÇAlright„ÄÇ so that's the most basic way to set up Tensor board using callbacks and what I'm going to move on to is show you how to do it from custom training loops and so that's what we'll be working with from now on„ÄÇ but just know that most of the things that I'm going to show you you can also do using callbacks and using model that fit but using custom training loops allows us to have more flexibility makes a lot of things actually more simple to do„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_12.png)

So let's do full screen on this„ÄÇAnd againÔºå this is just the starter code that you saw previously and all that we're going to do is we're going to move down to the part where we have the custom training loops„ÄÇAnd here we want to now add so that we're printing to Tensor board„ÄÇ And so one idea could be that„ÄÇ

Either you're writing it to Tensor board for every batch and so you could do it here„ÄÇ or you could do it right here after thisÔºå that would work„ÄÇ but perhaps it would be cleaner a cleaner graph if you do it every epoC so what we could do is right here we're going to do with the train rider as default„ÄÇThen we want to send in the loss and we also want to send in the the accuracy and just one thing right here is that we're going to send in the loss„ÄÇ

Just like this and this is actually just going to be the last loss of that batch so just a note you would probably want to send in the mean loss over that batch and so you would have a list of the losses and then you would take the mean of that and send that in but this works just to show how you can write a tensor board so we're going to send in the loss right there and then we're going to send in step and we can use the epoC as our step„ÄÇ

Then we're going to do the same thingÔºå dot scalar„ÄÇWe're going to do it with let' see with accuracy„ÄÇ so let me scroll downÔºå we're going to do it with sending the accuracy and the accuracy are going to be stored in this accuracy metric dot result„ÄÇAnd keep in mind here that that's reset in between the epochs„ÄÇ and it's also reset in between the training and the testing„ÄÇ

 So this should be itated through the test set„ÄÇAll right„ÄÇ and then we're going to add chsmetryduct result„ÄÇWe're going to specify the step again as epoch„ÄÇ and then that's it„ÄÇNow we've done it from the training we also want to do it for the test so what we could do here since we're going to do pretty much the same thing„ÄÇ we could just copy that and paste that in and we've got to make sure that we're changing all of this stuff test writer step is still epoch step is still epoC and yeah so that should be it hopefully there are no mistakes in this right now„ÄÇ

So one thing to keep in mind here is that we're now using another„ÄÇA log directory so here we're using logs and in logs we're storing two subfolders for the train and the test So let's run this first of all and again we're going to open up this tensor this Ananaconda prompt right again„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_14.png)

And we're going to use thisÔºå but we're going to specify the log gear to instead be just logs and then it's automatically going to find the training and the testing„ÄÇ So if we run that and we then go to fire Firefox or your browser and go to Tboard to on that URL„ÄÇ

 local hosts 6006„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_16.png)

And I guess we just have to wait a little bit here because it's going write every epoch so allright„ÄÇ so here we can now see the accuracy plot for example and also the last plot and that these are going to be very you know clear discrete steps because we're just recording it every epoch„ÄÇ

 but of course you could as I saidÔºå do it every batch and that would give you a much smoother graph„ÄÇBut what is perhaps soÔºüWhat is perhaps a little bit odd here is that the test set has higher accuracy than the training and I would imagine that has to do with the augmentation I chose to do for example I made some of the RGB on the training to grayscale and that is of course going to make it a lot harder for the model to train on those so that's probably the reason why so that's what I pretty much wanted to show you on that you could also do more complicated or more complex things and you could for example use it for hyperparameter search you could do something like for learning rate in and then you could specify at learning rates you want something like this„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_18.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_19.png)

And„ÄÇAnd let's just indent„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_21.png)

At one step„ÄÇ And so what you could do then is„ÄÇYou could„ÄÇsortrt of train a model for specific number of vpoOs with each respective learning rate and here I'm using a grid search just on those five different learning rate so let's do that we can do train step equals test step and we're just going to set that to zero every time„ÄÇ

And then we're going to do the train rid is T F dot summary dot create file writer„ÄÇ And then we're going to do logs train„ÄÇ But then we're also going to add„ÄÇLet's see„ÄÇ I're also going to add string of learning rate just to make sort of we can distinguish between the different runs„ÄÇ So let's do that again and let's do the test writer„ÄÇAnd here we're going to do test„ÄÇ

And then we need to get the model because we want to sort of refresh the we want to reset the weight for each time„ÄÇ so we're going to do get modelÔºå then we're going to do optimizer is ks„ÄÇ optimizeims„ÄÇ Adam„ÄÇSo we're alsoÔºå you knowÔºå we need to re initialitialize the optimizer with this specific learning rate„ÄÇAnd then you could do it this way and you can write the test writer like this from some epochs„ÄÇ

 but do let's just do it for every batchÔºå as I said„ÄÇThat might make for a smoother graph so what we're going to do here we're going to specify the train step rather than the epoch and then after here we're going to iterate up the train step by one and that's going to be sort of one discrete step in the plot and then we're going to do that for the test writer as well„ÄÇ

 so we got to change this to the test let's see„ÄÇTest step„ÄÇ and we've got to do it for all of these to make sure that we don't do anything wrong here„ÄÇWe're going to iterate up that by one as well„ÄÇAl right„ÄÇ so let's think is there anything else now we need to change„ÄÇ

 we could decrease the number of epochs just to make it faster„ÄÇSo let me run this and I'll show you what it looks like in Tensor board„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_23.png)

All rightÔºå so looking at the tensor board right here we and I've just separated out for the train so you could just write train right here and you can get all of the different ones and these correspond to different learning rates and you can also do some smoothing right here so you can get you know the loss just varies a lot from e for batch from batch to batch and so you could use some smoothing to make some better looking plots and then so for example„ÄÇ

 just in this caseÔºå if we compare we would have that the learning rates the initial learning rate should probably be between 0„ÄÇ001 and then 0„ÄÇ0001 between this„ÄÇThis orange and this red one right here„ÄÇ And so that's it for sort of getting lost plot and accuracy plots and you can play around with that„ÄÇ and you can that is veryÔºå very useful„ÄÇ So let's move on to getting images So what we're going to start with right here is using just visualizing images and probably the first thing we're going to do here is we're going to add some data augmentation we're going to use matplot Lib and to plot it„ÄÇ

 they want values between0 and1 So we can matpl1„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_25.png)

ValuesÔºå so what we can do is we can do image is T F clip„ÄÇBy value„ÄÇImage and then clip value min is0 clip value max is 1 and so by the way„ÄÇ if you're wondering why it would be under or over this0 and one range since we divide by 255 that should be the natural range but because of this data augmentation some pixel values can become lower or greater than1 so we just want to make sure at the end that they are actually between0 and 1 and that's going to be used later on but I'm going to show you y in just a moment„ÄÇ

So what we could do here is we could actually remove the training part so we could just do for batch like this and what we could do is we could iterate through for batch XY in enumerate DS train„ÄÇAnd so what we're going to do first of all is we're going to create a figure and we're going to create an image grid„ÄÇ

 so we're going to send in x and Y to that and then class names so this right here is a function we're going to create in this u function right here„ÄÇBut we're going to create an image gridÔºå and then we're going to do width ridr as default„ÄÇ We're going to do Tfsmary„ÄÇ image„ÄÇAnd we're going to send in visualized images„ÄÇAnd we're going to do plot to imageÔºå and we're going to send in the figure and then the step„ÄÇ

It's just equals to step„ÄÇAnd so we could„ÄÇIn this case„ÄÇ we're just going to just have a single writer and do it like this„ÄÇAnd instead of using train step„ÄÇ we're just going to call it step„ÄÇLike this„ÄÇ And then that's pretty much all we're going to do„ÄÇ we're going iterate up step by one All rightÔºå so this is how it's going to look like in this file„ÄÇ

 but of course we need to create this image grid and we need to create this plot to image I just want to say first of all that there are easy ways to do this and what I'm doing here by we're going to create this image grid in this plot to image„ÄÇI unnecessarily complicatedÔºå but it helps or rather„ÄÇ

 if you want it in a clean way so that you get it in a grid and you get it with the labels above it and so on„ÄÇThen this is how you're going to want to do it there are easier ways to just plot the images„ÄÇ but they don't look very nice and of course we want it to look nice„ÄÇ so let's go to u and what we're going to first of all is I'm just going to copy in some imports that are going to be used in this file„ÄÇ

And the thing we're going to do firstÔºå so we hadÔºå let's seeÔºå we had the defined plot to image„ÄÇAnd we're going to send in some figure„ÄÇSo let's to pass on that first of all„ÄÇ and then we had another image rather another functions we had image gridÔºå and we send in some data„ÄÇ we send in some labels and we send in some class names„ÄÇ

And what I'm going to do is I'm going to copy in from TensorFlow official tutorials this plot to image„ÄÇ so this is stolen from TensorFlow official guideÔºå you can see the URL right here but essentially so it converts the mapplot lib plot specified by figure to a PNG image and returns it this applied figure is closed and inaccessible after this call so basically it's converting the mapplot lib plot into a PNG in memory and then it gets decoded to a tensor which can then be sent to the a tensor board so here we're using IO to save the plot in memory„ÄÇ

Then we're closing the figure preventing from being displayed directly inside the notebook„ÄÇAnd then we're converting PNG buffer to TF image by image„ÄÇ decocode PNG„ÄÇAnd then lastly„ÄÇ we're adding the batch dimension„ÄÇSo I mean I don't want to focus too much on this„ÄÇ this is not really relevant I would say this is just some boiler play code just to make sure that it's in the right format„ÄÇ

 which is a tensor and then this image grid is what we're going to do first so we're going to create image grid which is going to create this matpllib figure then we're going to send it to this plot to image and that's going to then be sent to Tensor board„ÄÇ

So first of allÔºå the data data should be in this formatÔºå so we should have batch size„ÄÇ HW and then channels„ÄÇ So what we could do first of all„ÄÇ you see we could just assert data dot number of dimensions is4 what we're going to do is we're going to create a figure So figure is PLT dot figure of fig size and this is just Matpllib right that's the import„ÄÇRight here importpl pipeline as PLT„ÄÇSo we're creating that with a fig size of 10 and 10 and this is in I think this is in inches so it can be a bit confusing„ÄÇ

 but I've tried out different values and this is just what looks nice so but anyways then number of images are going to be data shape0 I guess this is specific to sort of a specific batch size the fig size that we're using here and what I've tried for is batch 32 you can also use larger batch size and it's still going to look relatively okay what I'm doing here is that I'm creating the number of images to be data shape of0 which is you know a general format that's not specific to 32 then we're going to do size is„ÄÇ

Integer of N do seal of MP square root of nu images and why we're doing this is because we want a nice looking grid„ÄÇ a square gridÔºå so the size of the grid is going to be the square root of the number of images just sealed to the up1 integer value„ÄÇAnd then what we're going to do is we're going to iterate for I in range of data„ÄÇShape of 0„ÄÇSo for each image in our dataÔºå we're going to create a subplot for that one„ÄÇ

 and we're going the size of the subplot is just going to be size„ÄÇSize and size just for the number of images in total„ÄÇ and the specific image right now is going to be specified by this indexÔºå which is i+1„ÄÇAnd the title of this plot is going to be class names„ÄÇ

 labels of so the labels are the values of the index for that one„ÄÇ and then we're doing class names of that index right there to get the name of it then we're going to do xt and we're just going to send in an empty list and we're going to do yt and we're going to do similar and then we're going to do plot grid equals or plot grid false„ÄÇ

And„ÄÇThen we're going to do if data shape of three is one„ÄÇ we're going to do plotted and we're going to do C map is PLT dot Cm dot binary just to get gray scale right„ÄÇ So if gray scale„ÄÇThis is what we're going to do and if it's RGB„ÄÇ we're just going to do plot I show of data of I„ÄÇAnd then in the end„ÄÇRightÔºå let's see„ÄÇ right there„ÄÇ

 we're gonna return the figure„ÄÇAnd here we don't have to specify any CMap because RGB is by default„ÄÇSo this is a lot of Matlolib and I don't want to go into the specifics of that but you get the general picture right here„ÄÇ even if you're not too familiar with Matplthlib I would think we're creating a figure and then we're creating a subplot which are going to be these specific training images and the size of it is size so row the number of rows is size and that is dependent on the number of images that we send in„ÄÇ

 so let's say we send in 16 images then the grid is going to be 4 by4 and each of those cells in that4 by4 grid is going to be an image from our training set„ÄÇSo now we have the image grid and then we have plot to imageÔºå so if we go back„ÄÇThis should most likely work now so we're doing first image grid then we're doing plot to image and here we are writing every single image in our batch„ÄÇSo that wouldn't really be necessaryÔºå perhaps you want to do it every epoch or something„ÄÇ

 but this could be useful just to check all of the the imagesÔºå so let's run it„ÄÇAnd let's see what it looks like„ÄÇ AlrightÔºå and then right here we need to import it right„ÄÇ so we need to do right here we need to do from us import plot to image and then image grid„ÄÇAnd rerun it and hopefully it works„ÄÇAl rightÔºå writer as the is not defined so„ÄÇI think this should be„ÄÇ

üòîÔºåWrier dot as default„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_27.png)

Alright so if we open up the tensor board now we get it in a nice format„ÄÇ we get the labels printed above it and as you can see here this is a square grid so for example here it's six by6 and then these last ones aren't covered by any image and that's because we did that seal and round it upwards but I would say this looks pretty good this is how this is a nice way to visualize the images and then you can see the step right here for each of the batches one thing here is that if you scroll back just one you're going to see that it's actually six steps„ÄÇ

And„ÄÇIf you call when you in AnanacondaÔºå you call Tensor board„ÄÇ you have another argument you can send inÔºå which is samples per plugin and that would specify how many images you want in this step slider right here so for example if you just specify it so if you specify that for example to 94„ÄÇ

 then that would mean that one slide step right here is for one batch rather than jumping you know in this case about 25 batches„ÄÇSo that's just one thing to keep in mind if you are wondering about that„ÄÇBut anyways„ÄÇ that's how we can visualize images„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_29.png)

Now the next thing is we want to create this confusion matrix and this is also a little bit complicated„ÄÇ but as you saw previously it's going to turn out pretty good so let's go through it step by step and again this is the starter code as usual how this is going to work is that normally you would create the confusion matrix by sending in all of the images at the same time with rather the outputs and the y labels at the same time and then that would create a confusion matrix but in this case we're creating it or rather we want to update it as we do it for every batch right let's say we have an enormous data set we can't send it in at the same time so what we're going do„ÄÇ

As we're going to create a confusion matrix„ÄÇThat is initially empty„ÄÇ so we're going to do MP zeros and then length of class names„ÄÇAnd then length of class namesÔºå right„ÄÇ that's going to be the the the number of rows and the number of columns„ÄÇAnd what we're going to do is we're going to update this throughout going through our batch„ÄÇ

 So we're going to go through here and we're going to do confusion is plus equals„ÄÇA function get confusion matrixÔºå and we're going to send in whyÔºå Y prediction„ÄÇ and then the class names„ÄÇAnd that's going to then add up to that confusion matrix„ÄÇ and we might get some numerical roundup errors and so on„ÄÇ

 but at least this approximation is very good and it's going to be enough to get a good representation of what the model is learning„ÄÇThen after that batchÔºå we're going to do width train rider do as default„ÄÇWe're going to do Tf„ÄÇsummary„ÄÇim„ÄÇAnd we in we're going to specify the nameÔºå which is confusion matrix„ÄÇThen we're going to do„ÄÇApp plot confusion matrixÔºå and again this is going to be another function„ÄÇ

And we're going to send in confusion and we're going to divide by the batch index because we're going to get a confusion matrix here at every update that is going to be have its own probabilities between zero and 1„ÄÇ but because this is a linear operatorÔºå then we can divide by the batch index„ÄÇ

And we will get an average of the probabilities for the entire epoC Of course„ÄÇ you know you could get overflow by this since you you're essentially let's say you're adding one every single batch„ÄÇ but to get overflow you would need to have an enormously large data set„ÄÇYou know even if you have 100Ôºå000 samplesÔºå having 100Ôºå000 is very far away from overflowing„ÄÇ

 so this is okayÔºå just keep that in mind though and then we have class names„ÄÇAnd then we're going to specify this step as the epoch„ÄÇ so we're just plotting the confusion matrix every epoch because this can also be quite you know quite expensive all right„ÄÇ and then we don't have to do it for the test setÔºå but of course you can do it for the test set if you want„ÄÇ

 but I'm just going to remove this actually„ÄÇLike this because we don'tÔºå that's not really needed„ÄÇAnd we can save on some computeÔºå So what we want to do now is we want to go to the u function and we want to create those two functions„ÄÇAlrightÔºå so we're going to first to get confusion matrix„ÄÇ We're going to send in y y labels„ÄÇ we're going to send in LosÔºå we're going to send in class names„ÄÇ

 So this right here are the correct labels„ÄÇ This is just what the output from the model is„ÄÇAnd these are the class names„ÄÇlet's pass on that just quickly and then we also want to do define plot confusion matrix and here we want to send it as input„ÄÇ some confusion matrix and I'm just going to write CM we're also going to send in some class names so maybe first of all we want to actually create the confusion matrix and you know to not do this from scratch we're going to use SK learn to produce the confusion matrix„ÄÇ

 creating confusion matrix would be a separate video I guess„ÄÇBut we would do the predictions are the nuy arg max of logicits and then x is1„ÄÇ So we're converting it to Numpy here as well„ÄÇ So of going from nuy to Tensorflow is pretty seamless„ÄÇ And so we just have to do nu argm directly and then„ÄÇThat will be the predictions„ÄÇ

 Then we're going to do the confusion matrix is SK learnarn dot matrix dot confusion matrix„ÄÇAnd we're going to send in the y labelsÔºå we're going to send in the predictions„ÄÇAnd then we're going to send in labels as NP a range of length of class names„ÄÇ and this is essentially to make sure that we're getting a confusion matrix which is length of class names in both the rows and the columns because let's say we have a batch size of 32 we would not perhaps get one example from each class every and that would destroy our confusion matrix„ÄÇ

 so we're sending this in just to make sure that it's actually the size we want„ÄÇAnd then we're just going to return the confusion matrix„ÄÇAlright„ÄÇ and then we want to do the plot confusion matrix and what we're going to do here is we're going to do size is the length of number of class names„ÄÇWe're going to create a figureÔºå which is PLT dot figureÔºå and then fig size is sizeÔºå comma size„ÄÇ

We're going to do plot„ÄÇ in show and we're going to do sending the confusion matrix„ÄÇ we're going to do interpolation interpolation is nearest„ÄÇAnd you can read documentation for what this is exactly doing„ÄÇ I'm not entirely sure what the different interpolation algorithms are„ÄÇ

 but this seems to be a default one and then we're going to use CM is PLt„ÄÇcm„ÄÇbls„ÄÇSo this is just gonna make the„ÄÇSort of the color of it blue„ÄÇ we're going to see exactly how it looks like later onÔºå but then we're going to PL T back title„ÄÇ We're going to specify the confusion matrix„ÄÇAnd then the indices are going to be NPR range of length of class names„ÄÇ

All right„ÄÇAnd then we're going to do plot do x ticks„ÄÇ We're gonna to send in the indices„ÄÇ That's how many„ÄÇSort of x values„ÄÇ How many„ÄÇHow many ones we're going to have and then we can specify the names of them and we're going to do class names and then to have them look in a nice way and so that the texts don't overlap„ÄÇ

 we're going to specify a rotation to be 45 degrees„ÄÇAnd then we're going to do ytics„ÄÇ and we're going to do indices and then class names„ÄÇAlright„ÄÇ and then we want to make sure that we normalizeÔºå so we want to normalize confusion matrix„ÄÇAnd how we do that is we do CM is NP dot around„ÄÇWe're going to do CMm type„ÄÇFloat„ÄÇ

And we're going to divide by seeing that sum x is 1„ÄÇAnd we're going to domp donuax„ÄÇ and we're going to specify the number of decimals„ÄÇU„ÄÇSo this just to make sure that everything adds up to one„ÄÇFor the sort of the predictions for a specific one can only add up to one over all of the different labels„ÄÇ

So that's what we're doing there then we want to print or yeah„ÄÇ so we want to have the text of the probability for that specific one„ÄÇSo we're going to first do threshold is CM dot max„ÄÇ and we're going to divide it by  two and we're going to do something pretty cool with this„ÄÇ

 but you're going to see it in just a second„ÄÇSo we're going to go through for I in range of size„ÄÇThen we're going to do 4 J in range of sizeÔºå right„ÄÇ we need to go through every row and then we need to go through every column and we're going to do first color is white if the CMI J is greater than the threshold Ls black„ÄÇSo if it's over some thresholdÔºå then we're going to make it white because then the background color of that cell is going to be relatively dark„ÄÇ

 so then we're going to make it whiteÔºå otherwise if it's lower then it means it's relatively light cell or a light colored cell„ÄÇ sort of a more white cellÔºå then we're going to make the text black„ÄÇAnd then we're going to do plot do textÔºå We're going to specify the position as IJ„ÄÇ and then we're going to send in the actual valueÔºå which is CMIj„ÄÇ

And we're going to do horizontal alignment„ÄÇ We're going to do center just to center the text„ÄÇAnd then we're going to do color equals colorÔºå and that is the thing we specified over here„ÄÇAlright„ÄÇ and then after thatÔºå we're going to do plot tight„ÄÇLayoutÔºå and this is just to get some nice format„ÄÇ And then we're going to do plot X labelÔºå and we're going do true label as the„ÄÇX values„ÄÇ

 and then the plot that y label is the predicted„ÄÇPredicted label„ÄÇAnd similarlyÔºå as we did before„ÄÇThis is now a matlolib figure to get it nice to tensor board„ÄÇ It needs to be converted to a tensor„ÄÇ so we can reuse the plot to image so we can deploy plot to image of that figure„ÄÇAnd then we can return CMm image„ÄÇAlrightÔºå so now we can go back and hopefully this should now work„ÄÇ

 So if we run this nowÔºå this is not going to work actually right we need to import this stuff as well from Us import what was it called get confusion matrix and then plot confusion matrix and hopefully that will run now So let's see„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_31.png)

All right so as you can see it seems to be working„ÄÇ of course we have only run it for a single epoOP now so our model is quite bad and this is represented of course in the values of the diagonal„ÄÇ which is how much well the probability of it accurately classifying each specific class so in this case airplane about 50% and so on so I mean let's increase the number of epochs„ÄÇ let's say to5 and rerun it and then you'll see as this improves over the epochs„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_33.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_34.png)

AlrightÔºå so as you can seeÔºå it's become betterÔºå the model is still pretty bad„ÄÇ we're using a very small model„ÄÇAnd so you can still see that it's not perfectÔºå but anyways„ÄÇ the visualization seems to work„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_36.png)

Let's now move on to doing a graph„ÄÇThis is going to be a guess„ÄÇA little bit simpler„ÄÇ So let's see here„ÄÇ I'm actually not going to need anything of this„ÄÇ so we're going to remove all of the data set stuff right here„ÄÇWe're going to remove all of it„ÄÇSo the thing we're actually going to do is we're going to do writer„ÄÇ we're going do T F summary„ÄÇ

 create„ÄÇFile writer„ÄÇWe're going to specify logsÔºå and then we're going to specify graph„ÄÇViz„ÄÇ let's just call it that„ÄÇAnd then we're going to just define some function„ÄÇ my function is's going to take x and y's inputÔºå then she're going to turn Tf„ÄÇnn„ÄÇ Relu and then Tf„ÄÇt mat Mole and then x and y„ÄÇSo it's just doing matrix multiply and then taking the re of that„ÄÇ

And we're going to do x is Tf random uniform shape 3 by 3Ôºå and y is just similarly Tf random uniform„ÄÇThree by three as well„ÄÇAnd then we'll have to createÔºå we need to do add Tf dot function„ÄÇSo since Tenflow 2„ÄÇ0Ôºå eager mode is executed by default„ÄÇBut creating these graphs is actually not possible to do in eager mode so we need to make sure that it's run in a static graph„ÄÇ

 and so this at Tia functionÔºå makes sure that the function is converted and run in a static graph„ÄÇAnd when that is done„ÄÇNow we're going to do Tf that summary that trace on„ÄÇ and we're going to do graph true profiler true„ÄÇThen we're going to do out is my function of that x and y that we just send in„ÄÇThen we're going do with writer as default„ÄÇDefault„ÄÇWe're going to do TF summary dot trace export„ÄÇ

 and we're going to do nameÔºå we're just going to call it function trace„ÄÇüòî„ÄÇWe're going to do step equals0 because we're just doing one and then profiler„ÄÇOur directory„ÄÇIs going to be„ÄÇLogsÔºå and then we can't do it in this wayÔºå so we need to do it like this„ÄÇ just a weird thing„ÄÇThat we need to do and then graph visualization and let's see„ÄÇ

We also need to do it like thisÔºå if I recall correctly„ÄÇAnd that should work now if you of course if you want to visualize a model this is still going to work so I mean if you have a model just to return a model of x here so just create your model here using Kaara sequential and so on and then just to return model of X and that should still work what Im want to do here is just show you a very minimal simple example but of course this this can be used in general so if we run this and then go to Tensor board or let's see that it's no error first of all yeah so we get some warnings here and I'm not really sure what to do about him I mean„ÄÇ

Sometimes in TensorflowÔºå you just get errorsÔºå but if stuff works„ÄÇLike it should then„ÄÇI don't know„ÄÇI don't know what to do about this AnywaysÔºå we're just going to look at the graph and it's going to look no data set name train„ÄÇ al right„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_38.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_39.png)

And then we have this x and Y matrix multiply ReÔºå and then that's the output„ÄÇSometimes these graphs can look a little bit weird„ÄÇ like I'm not sure why we have two identity right here„ÄÇÂóØ„ÄÇBut yeah„ÄÇSo anyways„ÄÇ you can see sort of the most relevant parts here that we're doing this and then re and personally I haven't used this too much but„ÄÇ

It can be usefulÔºå I'd imagine if you want to debug some very complex model„ÄÇBut anyways„ÄÇ let's move back to thecode so we're going to want to do hyperparameters now and this is something that's very useful„ÄÇ I would say if you want to do a hyperparameter search and you get it visualized very nicely so it looks very good„ÄÇ

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_41.png)

And yeahÔºå so we're going to againÔºå just work from the start code and we're going to modify it„ÄÇ And the thing we're going to modify now is„ÄÇWe're going to create sort of„ÄÇA function„ÄÇ and we're going to call it train model let„ÄÇOne epoch„ÄÇAnd then we're going to send in something called H PRs and actually we need to import that„ÄÇ

 first of allÔºå so what we need to import is from Tensorboard dot plugins„ÄÇPlugins that H PR import API as H„ÄÇ That's just for the H parameters API„ÄÇ and we're going to import it as H„ÄÇ And so let's move back down to that function„ÄÇ and what we're going to do here„ÄÇAs„ÄÇLet's say in our model right here we're gonna copy in that model and let's say we want to sort of play around with when it comes to the model„ÄÇ

 let's say we want to play around with this right here let's just call a number of units in the dense layer right here and of course you could imagine doing this for different values and then so on but I'm just going to use it that for this example and what I'm going to show you is how to get the Hpm from that and let's just say we want to play around with the dropout as well„ÄÇ

So waitÔºå let's remove this and„ÄÇDrop out of some drop rate„ÄÇAnd„ÄÇThen let's also say we want to play around with the learning rate„ÄÇ So from this h parameterss„ÄÇ we're going to get unitsÔºå we're going to do H parameterss„ÄÇ and then we're going to call it H nu units„ÄÇWe're going to get a drop rate and of course„ÄÇ

 we haven't defined theseÔºå but we're going to do it later on„ÄÇ so we're going to get H dropout and then we're also going to get learning rate„ÄÇAnd that is Kaos optimizeimrs„ÄÇadom„ÄÇ<|OTHER|>The learning rate will be set to learning rate„ÄÇ and we need to get the learning rate as well„ÄÇAnd that is from H's H learning rate„ÄÇAlright„ÄÇ

 so now that we have thoseÔºå we want we have the the oh waitÔºå that's on the learning rate optimizer„ÄÇ that's the optimizerÔºå and we're using the learning rate that we got from H primes„ÄÇAnd then we're creating the model using this units in this drop rate„ÄÇ Then we're going to actually create„ÄÇOne training epochÔºå and we're going to do it with this„ÄÇ

So we're going to copy that and put it over here„ÄÇSays this functionalityity ohÔºå not this one„ÄÇThis oneÔºå train model 1 epoch„ÄÇ This is going to train it for one epoch so we can paste that and„ÄÇThat should be it„ÄÇThen we're going to want to write to Tensor boardÔºå so write to TBb„ÄÇWe're going to do that by specifying their run directory„ÄÇ

 And since we're doing many different runs with different values of the learning rate and drop rate and so on„ÄÇ we're gonna to first specify logs and then train and then we're going do„ÄÇPlus string of units„ÄÇThen we're going to do unitsÔºå so this is just going to be an integer„ÄÇ then we're going to be units and we're going to underscore underscore just to make it readable„ÄÇ

 then we're going to do string of drop rate„ÄÇAnd then we're going to do drop out„ÄÇThen we're going to do string of learning rate„ÄÇüòîÔºåAnd then we're going to do„ÄÇLearning rate„ÄÇAllright so this might look a little bit weirdÔºå this is just not to get one line too long„ÄÇ so this looks all right„ÄÇWe can easily read it„ÄÇAnd then when we have the run directory„ÄÇ

 which is going to be different depending on the unit drop rate and the learning rate„ÄÇ we're going to do w„ÄÇT F dot summaryÔºå dot create file„ÄÇRer of that run directory„ÄÇ we're going to do as default„ÄÇSo notice here that we're not using a train writer„ÄÇ we're creating the file writer as we sort of write it and so that's going to be different for everyone„ÄÇ

And then we're going to do h parameters of h parameterss„ÄÇAnd„ÄÇWe're going to do accuracy is accuracy metric dot result„ÄÇ So this right here is just going to record the values that was used for this run„ÄÇ and then we have the accuracy„ÄÇ and then we have Tf summary do scalar„ÄÇAccuracy„ÄÇ

 and then we're going to send in that accuracyÔºå and we're just going to do step one„ÄÇAnd then after thatÔºå we've got to do reset the states of thecsymmetric„ÄÇAll rightÔºå now we can„ÄÇ I guess remove this get model because we created the model for everyone„ÄÇThen we we„ÄÇDon't have to have that„ÄÇüòîÔºåWe don't have to have the number of Vpo or yeahÔºå we could„ÄÇ

We could have the number of epochsÔºå but I'm actually going to just run it for a single epoch anyways„ÄÇ so I'm going to remove that„ÄÇThen we want to have the loss„ÄÇ we want to have the optimizer we want to have the acrosymmetric„ÄÇ and then we don't need those because we're writing it already using a new bright every time„ÄÇ

But then we're going to specify H nu units„ÄÇWe're going to do H„ÄÇt H per Ram„ÄÇAndWe'm going to do nu units„ÄÇThat's what we're going to call it„ÄÇ then we're going to do Hp dot discrete„ÄÇAnd we're going to send in 32Ôºå 64Ôºå 128„ÄÇSo what this means is that we're choosing the number of units in in that particular dense layer to be 32 64 or 128„ÄÇ

 So we're kind of using yeahÔºå so we're kind of using a kind of a grid search here and if you are familiar with hyperparameter search you probably know that using a random search is much more is better„ÄÇAnd I'm not going to explain whyÔºå but essentially using random search is better„ÄÇOf course„ÄÇ

 what you would do here is that you would perhaps random sample the values first using from some maybe using random search and and then that's what you would put into these discrete values but anyways„ÄÇ just for the illustration here I'm just using it in a very simple way„ÄÇIn discrete steps„ÄÇ

In a grid search mannerÔºå and then we're going to do H dropoutÔºå and we're going to do H dot H PRm„ÄÇ we're going to send in dropout„ÄÇWe're going to do HB„ÄÇt discrete and we're going to specify 0„ÄÇ1„ÄÇO„ÄÇ2Ôºå0„ÄÇ3„ÄÇAnd up fiveÔºå so let's say we have those four values and then we have the HP learning rate„ÄÇAnd let's do HB do H parameter„ÄÇ<|OTHER|>And it's called learning rate„ÄÇAnd again„ÄÇ

 we're going to HP out discrete„ÄÇÂóØ„ÄÇAnd then1 e minus3Ôºå1 e minus5 and 1 e minus5Ôºå 4 and5„ÄÇ So anyways„ÄÇ what you can also do here is that there is something called H dot int interval and then I think it's real interval„ÄÇAnd then you can sample from those„ÄÇBut„ÄÇYeahÔºå I don't really see the point of those really„ÄÇ I would just sample from them myself and then find the values and then put that in as discrete values„ÄÇ

So I think this this would work well in general as well„ÄÇ but there might be some use case that I'm not really familiar with„ÄÇSo anywaysÔºå when we have those„ÄÇWe can change this right hereÔºå we're not going to use the normal training loop that we've used previously„ÄÇ what we're rather going to do is we're going to do full learning rate in HP learning rate„ÄÇ

 domainin values„ÄÇAnd we're going to do four units in H nu units that domain values„ÄÇAnd also for rate in HP dropout„ÄÇThe domain values„ÄÇIn this way we're just iterating through the loan rate units and rate in these discrete values„ÄÇThen„ÄÇWe're going to run it just for a single epoch„ÄÇ but otherwise you would do you know for epoC in range of the number of epos right here„ÄÇ

But we're just going to do it for one„ÄÇ so then we're going to do h parameters is equal to a dictionary„ÄÇ and then we're going to specify H learning rate is equal to a learning rate„ÄÇAnd what is wrong hereÔºü

üòîÔºåYeahÔºå it needs to be not equal to like this and then Hum units„ÄÇAnd we're going to specify that as units„ÄÇAn HP dropout is just rate„ÄÇAlright„ÄÇ and then we're gonna call that functionÔºå which is train model 1 epoch of H per„ÄÇ So actually„ÄÇ one thing here is that you couldn't„ÄÇ you wouldn't do for epoch here because that would„ÄÇ

Sort of reset this„ÄÇ YeahÔºå what you have to do then is modify this„ÄÇ So you would have to add one loop in here„ÄÇ I would imagineÔºå but„ÄÇAnyways„ÄÇ this works just to illustrate how you would do it for a single epoch with different hyperparameters„ÄÇThis might take a while to runÔºå so I'm just going to run it and hopefully we don't get any errors as I say that„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_43.png)

AlrightÔºå so we get the H parameters right here and I'm going to just make this a little bit larger and you're going to see how it updates as it's getting more data points„ÄÇAll rightÔºå now we get all of these hyperparameter and that this might look a little bit confusing„ÄÇ

 So what you could doÔºå see if I can do itÔºå you can just specify sort of„ÄÇSome the top ones„ÄÇ the top accuracies and„ÄÇIn this case we can see that the top ones correspond to having high number of units right a larger model is always better basically with a learning rate of 0„ÄÇ

001 so the highest of the ones we chose and then preferably with the lowest level of dropout and again this is because we're doing it on the training set you would actually want to do it on the validation set to make sure that this is correct„ÄÇBut againÔºå we just want to illustrate how this works in Tensor board„ÄÇ

So that's it for using the H parameters„ÄÇ Now we want to move on to doing the projector and the projector is very cool and can be very useful so let's move to that file and is we actually don't need these things So I'm going to remove the model and I'm going to remove let's see pretty much everything here„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_45.png)

And we're not going need„ÄÇAnythingÔºå reallyÔºå we're going to need one batchÔºå one X batchÔºå one y batch„ÄÇAnd we can do that by using next iter of the S strain„ÄÇThen we're going to send that to some function„ÄÇ which we're going to create plot to projector„ÄÇWe're going to send in X batch and we're going to send in X batch again„ÄÇ I'm going to explain this in a secondÔºå and then we're going to send in Y batch„ÄÇ

 We're going to send in class names„ÄÇWe're also going to send in some log directory„ÄÇ we're just going to call it„ÄÇProge like that projector„ÄÇ And so why we're sending in exppat to times here is because„ÄÇThis function we're going to create is going to obviously take in some some data pointsÔºå right„ÄÇ

 the images„ÄÇAnd it's going to take in some labels but it's also going to take in some feature vector„ÄÇ so normally you would have sent in your your imagesÔºå your data„ÄÇTo your model and then out from that modelÔºå you would take some feature vector„ÄÇ which might be in some arbitrary number of dimensions„ÄÇ

So then this projector is going to take in this feature vector and try to obtain a projection down to the images and see sort of what of the features and use one of these algorithms like TS&E or PCA to project it down and see what does our model sort of think are in similar groups depending on what the feature vector says„ÄÇ

And what we're going to do in this case is we're not going to send it into some model„ÄÇ So what that means is we're going to use the images itself„ÄÇ We're going to reshape the images„ÄÇTo just be one long feature vector with the pixel values for that image„ÄÇ and that's going to be our feature vector„ÄÇAgainÔºå as most of this stuff in this tutorial it's been for illustrating how to use Tensorboard so this is how to make it simpler for us„ÄÇ

 but normally this would be a feature vector so just to be U clearÔºå you would feature vector„ÄÇ you would do model of XpatÔºå you would get some feature vector and here you would call in with that feature vector„ÄÇNow we're not going to do thatÔºå as I saidÔºå so all we need to do is create this plot to projector„ÄÇAnd before I forgetÔºå let's do from U's import plot to projector„ÄÇ

And then we're going to go to these u functions again„ÄÇAnd we're going to need two things„ÄÇFirst of allÔºå we're going to need something called a create sprite image and a sprite image is essentially that we're going to take the image„ÄÇ the input or the X patchÔºå the input X patch„ÄÇAnd we're going to create a sprite image and a sprite image is that it's going to have one image with all of the images in that batch that we send in„ÄÇ So let me see if I can bring up one example of that„ÄÇ

So a sprite image of emnist is going or this is fashion emnist„ÄÇ So this right here„ÄÇ this is a sprite image„ÄÇ as you can see it's a sort of a collection of of manyÔºå many„ÄÇ many different images inside that single one and„ÄÇThat's going to be useful to send to Tensor board so that we just have to send one image rather than many„ÄÇ and so that's what we need to do first of all„ÄÇAnd„ÄÇI'm not going to do this„ÄÇ

 I'm just going to steal it and I'm going to copy paste it„ÄÇ so this is stolen from this guy„ÄÇ Andrew B„ÄÇ Martin and„ÄÇYeahÔºå you can read through thisÔºå we're just going to use it„ÄÇ I'm going not going to focus on what this does„ÄÇSoBut essentiallyÔºå it creates this sprite image and„ÄÇSo basically it does some reshaping and some stuff and adds padding to get this image„ÄÇ

We're going to create this define plot to projector function and what we're going to send in is X„ÄÇ that feature vector I was talking aboutÔºå YÔºå class names„ÄÇ and we're going to specify some log directory„ÄÇAs default„ÄÇ let's just say default log trajectoryy and then let' see we're going to also get Me file„ÄÇ

And we're going to specify metadata„ÄÇ TSv„ÄÇAll rightÔºå so this is going to make a lot more sense„ÄÇFor what we're going to use them for„ÄÇSo first of all„ÄÇ we need to sort of make sure that the number of x dimensions is four„ÄÇBecause what we're going to do is we're going to assume it's of this form where we have the batch size„ÄÇ

 we have the heightÔºå the widthÔºå and then the channels„ÄÇAnd one thing that can be super frustrating is that„ÄÇYou can't run this if youve already if you have a directory with this log directory already and so what we're going to do is we're going to do if OSpa is directory„ÄÇ so if there is a directory in log direction that means that there's some old projector files there„ÄÇ

 so what we're going to do then is we're going to do shuttle„ÄÇ remove3 and remove tree of that log directory„ÄÇAnd thenÔºå we're gonna make their„ÄÇMake that directory of that log directory„ÄÇ So essentiallyÔºå we're making it clean from scratch„ÄÇ So we're gonna„ÄÇCreate a newÔºå cleanÔºå fresh folder„ÄÇAlrightÔºå then we're going to do„ÄÇ

I'm going to specify the Sprites fileÔºå and that's going to be OS path join of log directory„ÄÇ and then we're just going to call it sprite sub PNG„ÄÇAnd we're going to do sprite is create sprite of x„ÄÇOf input the input data„ÄÇ then we're going to do CM M V2Ôºå that inrite spites file„ÄÇWhat the sprites file and then the sprite„ÄÇ

All rightÔºå then we want to generate the label names„ÄÇSo we're going to do that by doing labels is class names of y of I„ÄÇ so each respective integer target value for each training exampleÔºå so I is a training example„ÄÇ then we're going to do for I in range of int of y shape of0„ÄÇLike that„ÄÇ

Then we're going to write to that metadata TSV file and that's used for plotting in the labels in the projector„ÄÇ so we're going to do open OS Pat joinÔºå log directory and that metadata met file„ÄÇ that's what we called itÔºå and then we're going to write to that one„ÄÇAnd we're going to open it as F„ÄÇSo for label in labelsÔºå we're going to do Ft right„ÄÇ

We're going to do new line and we're going to do dot format labelÔºå allÔºü

So we're just going to send in the label and then we're going to make a new line and that's just the structure tensor word wanted in„ÄÇAnd now what I'm going to do is we're going to do the feature vector„ÄÇ So we're going to do if feature vector dot n dimension is not equal to 2„ÄÇ that would mean that it's not a feature vector so that would mean thatÔºå for example„ÄÇ

 like we are doing itÔºå we're sending in exppat two times„ÄÇ So if that is the case„ÄÇ then we're going to do print„ÄÇ<|OTHER|>And we're going to do see note„ÄÇFeature vector is not a form batch comma featured„ÄÇAnd then we can do reshaping to try and get it to this form„ÄÇAnd then what we're going to do is feature vector„ÄÇIs equal to Tf do reshape„ÄÇAnd then feature vector„ÄÇ

 and then„ÄÇFeature vector dot shape0Ôºå and then minus-1„ÄÇAllrightÔºå and then we're going to do„ÄÇFeature vector is T F variable„ÄÇ So we're just gonna convert it to a T F variable„ÄÇAnd then we're going to do checkpoint is Tf train dot checkpoint of embedding equals feature vector„ÄÇAnd then checkpoint dot save and then OS path showing log directory„ÄÇ

And then we're going to specify embeddings„ÄÇckKP„ÄÇÂóØ„ÄÇSo we're basically saving that feature vector into the log directory and then embedding that CKPT thats what„ÄÇTenssor board wants to be called and then„ÄÇWe're going to set up a config that's going to be right that's going to write to to the projector„ÄÇ

 So we're going to config is projector dot projector config„ÄÇEmbbedding is config do embeddings add„ÄÇAnd then we're going to do embedding dot tensor nameÔºå we're going to specify embedding„ÄÇ and then we need to do this right hereÔºå we need to do dot attribute„ÄÇ and then variable name variable value„ÄÇAnd then we need to do embedding that met data path„ÄÇ

We need to do meta file„ÄÇAnd„ÄÇYeahÔºå if you feel this is a little bit clumsyÔºå I mean„ÄÇ the code is kind of„ÄÇThere's a lot of code to do this„ÄÇI wouldn't focus too much on the specific code in PythtorÔºå for example this is done in one line„ÄÇ literally one lineÔºå but in TensorFlow this is a little bit more difficult the embedding projector is usually done to project embeddings makes sense right„ÄÇ

When we're doing it for imagesÔºå we need to do it in this„ÄÇVery convoluted way„ÄÇ but anyways this is how we do itÔºå but if you find a better way than do comment„ÄÇ but this is the best way that I've found so far„ÄÇThen we're going to do embedding„ÄÇ sprite„ÄÇ image pathÔºå and then we're going to do sprite„ÄÇp andng„ÄÇ So we're just specifying the image path„ÄÇ

And then we're doing embedding thatt sp single image dimension extend„ÄÇ And then we need to specify x shape of1 and then x shape of 2„ÄÇ So we're specifying the„ÄÇAt the height and the width„ÄÇAnd lastlyÔºå we got to do projectorÔºå visualize embedding„ÄÇ and we're going to send in log directoryÔºå and then the config and thats„ÄÇThat's it„ÄÇ

 so now hopefully if there are no errors„ÄÇThis should run„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_47.png)

And this is not right„ÄÇüòî„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_49.png)

So the error is that we when we're normalizing the imagesÔºå when we're creating these sprite images„ÄÇ we need to make sure that we don't divide by 255 and„ÄÇIf we rerun it„ÄÇWe should hopefully obtain the correct pixel values now„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_51.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_52.png)

And as you can see hereÔºå we have the images and we can see right there that's a dog and then that's a boat or a ship„ÄÇThat's a horseÔºå I thinkÔºå and let's see what else„ÄÇYeahÔºå anywaysÔºå and you can play around with this„ÄÇ We have very few images hereÔºå rightÔºå So what you would probably want to do is you would change the batch size to something like 500„ÄÇ

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_54.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_55.png)

And if we rerun itÔºå this is quite quickÔºå actually„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_57.png)

AlrightÔºå so what you need to do hereÔºå I think if you rerun itÔºå you need to do„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_59.png)

Sort of a new one„ÄÇ So if we rerun it now„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_61.png)

And then we open it again in Tensor board„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_63.png)

Sometimes this worksÔºå sometimes it doesn't„ÄÇBut now it works„ÄÇ allright„ÄÇ So now we get 500 images and then you could doÔºå you knowÔºå use the T S and E PC A and this also„ÄÇ I've tried to make it„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_65.png)

GeneralÔºå in the way that we could just replace this by Mist and we could just run it directly and then if we now check it„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_67.png)

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_68.png)

All right„ÄÇYeah„ÄÇThen of course we can't do this data augmentation„ÄÇ so let's remove the data augmentation completelyÔºå but we can't convert something to gray scale that's already grayscale„ÄÇ but other than thatÔºå I think this should be general in the way that we can just change that to Mist„ÄÇ and then you would be able to project„ÄÇ

![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_70.png)

Any type that you like„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_72.png)

Al rightÔºå white waitÔºå there's one more thing„ÄÇ actually„ÄÇ YeahÔºå so this is the last file of this one„ÄÇ but„ÄÇI want to show you the other thing too in a Tensorflowlow Pror„ÄÇ so Tensorflow Pror is available since Tensorflowlow 2„ÄÇ2 and if you follow these tutorials you know that I'm using TensorFlow 2„ÄÇ

1 on the GPU and so I can't use it and it's actually necessary that you have a GPU to run„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_74.png)

So what I did is I did it in„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_76.png)

Google CollabÔºå and this is following very closely to one of their official tutorials„ÄÇBut so essentiallyÔºå I'm going to just step through this and perhaps you're watching this video„ÄÇIn the future when you can easily install TensorFlow 2„ÄÇ2 and above with Ananaconda„ÄÇ and so you can probably do this and create just a new file„ÄÇ

 so you wouldn't have to do this on Google CollabÔºå but this works just to illustrate„ÄÇAll right„ÄÇ so first of allÔºå we just have some import right here and then we got to do this Pip install Tensor board plugin profile„ÄÇWe're importing Tensorflow and here we're making sure that we're running on the GPUÔºå as I said„ÄÇ that's a requirement„ÄÇAnd then TensorFlowÔºå I think it works on the CPUÔºå but for some functionality„ÄÇ

 there's a requirement that you do it on the GPU„ÄÇSo then we're doing tensor data sets„ÄÇ we're importing MNISÔºå very similar to what you've„ÄÇTo what you have seen throughout this tutorial„ÄÇThen we're normalizing the imageÔºå we're doing some map and then batch„ÄÇ then we create some very simple model with a model compile„ÄÇ

And then we create some logs and they create these logs„ÄÇ this is from Tensflowlow official tutorial using this daytime now„ÄÇ which is kind of clever so that you would get a new file„ÄÇ new log file every time you run it because the time has changed„ÄÇSo that's a pretty good way to avoid those errors instead of doing in the projector way of removing the folder if it exists„ÄÇ then doing something like this could be easierÔºå I think„ÄÇAll right„ÄÇ so then we havesor tensor board callback and we're just creating that and all they're doing is doing this profile batch and they're doing it 500 to 520 and„ÄÇSo this is using it with Tensor board callbacksÔºå but there are multiple ways you can do it if you have custom loops as well„ÄÇ

 I'm going to reference to that official tutorial in the description of this video if you want to check out more how you would do it„ÄÇAnd then we're just doing model that fit and again using this tensor board to call back„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_78.png)

And then if we then do load extension of Tensor boards„ÄÇ so we open up Tensor board and we run it with log de equal logs„ÄÇThen we can get all of these different thingsÔºå so we get an overview page„ÄÇ and maybe I need to rerun this„ÄÇI think soÔºå so we're gonna„ÄÇFactory reset runtime„ÄÇ

 and then we're going to run all„ÄÇSo then we can open up the profiler and what we'll get is we have some different tools here we have an overview page and to be honest I haven't looked into these in detail„ÄÇ but I'll show you just an overview of what they do so this is the one that's perhaps the easiest to look at and the easiest to sort of understand here they give a summary of all the different parts so you can see the timing for all of the different components of the training„ÄÇ

 you can also see different thingsÔºå the placement let's number of TF ops executed on the host and the device„ÄÇ not really sure what the difference between host and devices here„ÄÇSo again„ÄÇ I'm no expert in terms profilerÔºå but maybe someone can comment what this difference is„ÄÇBut other things you can see here is like the device compute precisionÔºå if you're using  16 bit„ÄÇ

 how many percentage of the computation is done on 16 bitÔºüAnd then„ÄÇYou can see some recommendations here and this graph is quite useful as well„ÄÇ so you can see for exampleÔºå the input is highly input boundÔºå 85% is waiting for input„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_80.png)

So what we could doÔºå for exampleÔºå is now that we know that we could do dot dot cache„ÄÇ what could we do else we could do non parallel calls is autotune„ÄÇAnd we could do that for both„ÄÇ so none„ÄÇüòîÔºåParallel calls is auto tuneÔºå and then we can do„ÄÇAdo cache„ÄÇAnd then we could also do dot pre fetch and we can do auto tuneune dot pre fetchchÔºå pre fetchch„ÄÇ

And we could again send in auto tune„ÄÇ and yeahÔºå I guess I'll just„ÄÇRerun this again„ÄÇSo„ÄÇRun time„ÄÇ run allÔºå and we'll see if there's any difference in the„ÄÇIn the output from the Tensorflowlow Pror„ÄÇOh by the wayÔºå this profile batch is between which batch and this says that you want to profile so right here we want to profile between the index batch of 500 two index batch of 520 so we're just doing the profile for 20 batches so if you do it for too many batches you could get out of memory errors so that's why they recommend doing just 20 or 10 batches and also not to put it in the initial batches rather just to do it in the middle of training because it could take some time to initialize things„ÄÇ



![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_82.png)

AlrightÔºå so let's again look at the profiler and see if there's any difference so right now we can see that your program is moderately input bound because 160% is waiting for input„ÄÇ but that's a major improvement over the 85% that we just had„ÄÇSo let's see what more you have„ÄÇ

ActuallyÔºå you know whatÔºå you can play around with this yourself and you can see„ÄÇ and I'm also going to reference to a Tensor flow„ÄÇOfficial tutorial worthy going into more depth on this Tensorflow profiler„ÄÇBut yeahÔºå so for example here you can see sort of the matrix multiply that is taking up 25%„ÄÇ the biggest chunk of the operation timeÔºå and then you have different things like trace viewer and that's you can get a very„ÄÇ

 very low level view of all of the operations done on the CPU and GP and you can see exactly what is taking up time„ÄÇSo this is a veryÔºå very cool toolÔºå this is more of an introductory to that and just show you how to get it up and running and yeah so„ÄÇThat's it„ÄÇ This is going to be a very long video„ÄÇ I can see that„ÄÇ and hopefully„ÄÇThis video is at least useful in understanding Tensor board„ÄÇYeah„ÄÇ

 thank you so much for watching the video and I hope to see you in the next one„ÄÇ![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_84.png)