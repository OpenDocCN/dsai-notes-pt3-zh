- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P47ÔºöL9.1- KerasËøÅÁßªÂ≠¶‰π†ÁÆÄ‰ªã -
    ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/57e575643f505aee4c9c36ac4901f183_0.png)'
  prefs: []
  type: TYPE_IMG
- en: HiÔºå this is Jeff Heaton„ÄÇ Welcom to applications of deep neural networks with
    Washington University„ÄÇ You know whatÔºå It's a lot of work to train a neural networkÔºå
    and it can take a lot of compute time„ÄÇ It's mostly your computer doing the workÔºå
    but still it can take a longÔºå long time„ÄÇ especially if you don't have a multi
    thousand dollar GPU„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is where transferfer learning comes in handy„ÄÇ Transfer learningÔºå lets you
    take„ÄÇüòä„ÄÇNeurural networks that were developed by larger companies or startups or
    other people who have a lot of money to blow on GPUs„ÄÇ and you can use this training
    in your own projects for the latest on my AI course and projects„ÄÇ click subscribe
    in the bell next to it to be notified of every new video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57e575643f505aee4c9c36ac4901f183_2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/57e575643f505aee4c9c36ac4901f183_3.png)'
  prefs: []
  type: TYPE_IMG
- en: Transfer learning is a very important concept in deep learning„ÄÇ This is because
    it can take a tremendous amount of time„ÄÇ Compute resources and money to train
    advanced neural networks„ÄÇ particularly in the area of computer vision and natural
    language processing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You don't see transfer learning as much with tabular data„ÄÇ because there's just
    not a lot of actual neural networks out there that are already trained for that
    sort of thing„ÄÇ It depends on your data set for tabular and the tabular dataset
    sets tend to be very different„ÄÇ Comp imaging and computer vision there tends to
    be a lot of commonalities„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Some of the elements that you will see like edges and wheels and eyesÔºå noses„ÄÇ
    mouths are common across manyÔºå many computer vision tasks„ÄÇ So you can take some
    of the learning that large companies such as Google and Microsoft and others have
    put into training„ÄÇ these advanced neural networks and transfer„ÄÇüòäÔºåData into your
    neural network to begin as a basis„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if you're going to teach a neural network to recognize a couple of very specific
    image types„ÄÇ you probably don't want to train the neural network entirely from
    scratch„ÄÇ if you can transfer in a neural network weight system that has already
    been trained on imagenet or some of the other common computer vision data sets„ÄÇ
    The way that this works is you will typically find a pretrained neural network
    that will have a variety of layers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it doesn't even really matter exactly what these layers are„ÄÇ you can definitely
    display them and we'll see that when we run the programs here for this example„ÄÇ
    and you will typically shear off the top part„ÄÇ So imagenet is very common one
    and some of the others you'll typically see them trained for 10 different image
    categories„ÄÇ So we might want to train it for something much smaller than that„ÄÇ
    We will„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Move that top layer and transfer just these layers„ÄÇ These layers„ÄÇ All of the
    weights are going to move across into our neural network„ÄÇ and we will train just
    the new layers that we add for our new neural network„ÄÇ So here we add a dense
    layer that is going to learn from these lower layers and only these top two layers
    are going to actually be trained„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: These bottom layers essentially become feature engineeringÔºå especially for computer
    vision tasks„ÄÇ These are learning all of the building blocks that went into imagenet
    and the other dataset sets that the neural network was trained on„ÄÇ FirstÔºå we're
    going to look at a very simple transfer learning example„ÄÇ and this will be tabular
    data„ÄÇ This lets you look at a very simple will use the iris data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: just so that we can see the mechanics of how this all comes together„ÄÇ Typically„ÄÇ
    you probably won't be creating transfer learning neural networks that others will
    transfer into theirs unless you're going to really„ÄÇüòäÔºåCommit to getting a lot of
    data and building an advanced training set up with GPUs and other things like
    that and spending some serious money For this one„ÄÇ thoughÔºå we will create that
    initial neural network that we are going to transfer into something else„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we're going to we're going to basically try to learn to classify some additional
    flowers beyond those three irriss that the data was originally set up for„ÄÇ We'll
    hope that some of the learning for the original iris data set will transfer to
    this new one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here the first thing we're going to do is actually get our training data
    loaded in and build our neural network„ÄÇ So this neural network that we're building
    here this simulates what the tech companies or researchers are doing using advanced
    computer rays to really seriously train these„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now we have a iris data set„ÄÇ We would probably save it at this point so that
    we could transfer it„ÄÇ We'll check the accuracy real quick„ÄÇ which runs about 98%„ÄÇ
    So that's pretty accurate and„ÄÇPrint out the summary„ÄÇ We use summary a lot to analyze
    the neural networks that we're transferring in Here we can see that we essentially
    have some dense layers„ÄÇ the 50Ôºå25 and3Ôºå just like we set up up here for the neural
    network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So 5025 and 3 summary can be very good for looking at these things and seeing
    what's going on„ÄÇ total parameters So that's how many weights we have in the neural
    network trainable parameters is the same in this case because everything is trainable
    Now when we transfer this simple neural network into something else„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're going to mark some of these untrainable and we'll see that„ÄÇ So now I am
    going to show you that basically we can rebuild this neural network„ÄÇ we're creating
    model2 sequential„ÄÇ and I'm taking all the layers in the original model and adding
    them when we run this„ÄÇ we see an exact duplicate up here 1603 just like we had
    up here„ÄÇ And just as a sanity check„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're going to evaluate model 2„ÄÇSeeHere and see what the accuracy is„ÄÇ the accuracy
    should exactly match the original one that we transferred„ÄÇ So we have basically
    done transfer learning here„ÄÇ We've transferred everything„ÄÇ which is typically
    not what you'll want to do„ÄÇ We would have a duplicate of this neural network that
    can classify the same three flowers that the original neural network could not
    too useful„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: where it becomes useful is if we have these fake flowers that we want to add
    to it„ÄÇ Now I'm just calling them fake flowersÔºå because are hypothetical„ÄÇ This
    shows again„ÄÇ the mechanics you would go through if you wanted to truly create
    your own neural network that others could transfer„ÄÇ What we're going to do is
    create now model 3„ÄÇ and I'm just going to move those first two layers in and drop
    off the output layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So these will be feature engineering for the new model„ÄÇ You'll see we have fewer
    parameters because we've dropped off the top layer that was going to do the top
    are the final layer„ÄÇ depending on your perspectiveÔºå but the output layerÔºå what
    we dropped off„ÄÇ Now we're going„ÄÇüòä„ÄÇTo add on a new softm layer with four output
    neurons to classify those fourth leg fake flowers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We run that And we see that basically now we have the 5025Ôºå but we have a four
    attached to it„ÄÇ Now„ÄÇ we could have also added some additional dense layers to
    maybe give some additional processing„ÄÇ But for this simple of a networkÔºå not reallyÔºå
    not really necessary„ÄÇ NowÔºå we also„ÄÇ when we added these mark the new ones trainable
    false„ÄÇ Now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: for the original layers that we transferred inÔºå we march trainable is false
    up here„ÄÇ That's very important for transfer learningÔºå because we don't want to
    train the weights that we already had„ÄÇ that's what we're transferring in„ÄÇ That's
    the benefit„ÄÇ So here you'll see that the total parameters is 1629„ÄÇ But only 1525
    of themÔºå the first two layers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the input and the 5025Ôºå those are the only trainable layers„ÄÇ So those numbers
    are different here„ÄÇ I create just some contrived values„ÄÇüòäÔºåThat we will use for
    these four flowers„ÄÇ I'm only giving two training samples on each„ÄÇ But again„ÄÇ this
    is just showing the mechanics that you would go through„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here's the one hot encoding for each of those four flowers that correspond to
    the X inputs here„ÄÇ We're going to fit the neural network with just training that
    final output layer„ÄÇ and we're going to try to see how well it can classify those
    four new flower types„ÄÇ We run it„ÄÇ we see the accuracy the last time I run it was
    about 88„ÄÇ We'll see what this is1„ÄÇ0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So actually perfect accuracy in this case„ÄÇ So it's the stochastic nature of
    neural networks„ÄÇ you'll get different results„ÄÇ So here we've gone through the
    entire process of this„ÄÇ So this is how you would truly create a neural network
    that you would want to transfer„ÄÇ The rest of this course„ÄÇ we will be transferring
    in other people's neural networks to us rather than being the producer of those„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we'll simply be the consumerÔºå which is usually how it goes„ÄÇ Thank you for
    watching this video in the next video„ÄÇ we're going to take a look at how„ÄÇüòä„ÄÇ![](img/57e575643f505aee4c9c36ac4901f183_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: To actually„ÄÇFind some of these„ÄÇOpen neural networks that you can put into your
    own learning for transfer learning„ÄÇ This content changes often„ÄÇ so subscribe to
    the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
