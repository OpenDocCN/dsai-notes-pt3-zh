# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‰ΩøÁî® Scikit-learn ËøõË°åÊú∫Âô®Â≠¶‰π†Ôºå4Â∞èÊó∂ÂÆûÊàòËßÜËßíÂà∑Êñ∞Áü•ËØÜÊ°ÜÊû∂ÔºåÂàùÂ≠¶ËÄÖËøõÈò∂ÂøÖÂ§áÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P12Ôºö12Ôºâ‰ΩøÁî® sklearn ËøõË°å K ÂùáÂÄºËÅöÁ±ª - ShowMeAI - BV16u41127nr

![](img/77644a3896823f6a90d56917ef525e6a_0.png)

P„ÄÇIn the last video we built our own Chas class and this one we're going to be learning about how the one that comes with SKL works„ÄÇAnd usually you'll want to do that because it works and it kind of gets all these tricky details right„ÄÇ for example„ÄÇYou can easily have it automatically generate varying numbers of starting clusters„ÄÇIt will often have strategies that are smarter than pure randomness for choosing the positions of those before it runs an algorithm„ÄÇ

It generally has some logic around when it has been updating the centers enough and it's not going to get any need better and so you can set an upper cap on that but it's not going to do it more than necessary in general„ÄÇ so overall youret want to use K means that terms of scalar rather than rolling your own So the K means actually has these three methods with it that we need to know we have fit which is not surprising right we've seen fit for both transformers and estimators what's a little bit strange about it is that it has both transform like you might expect for a transformer and predict like you might expect for an estimator so so K means has some similarities with both transformers and estimators even though estimators and prediction is a little bit it's kind of a strange use of it right because we aren't predicting some label that was given to us we're both coming up with the labels and predicting them at the same„ÄÇ

so it's not really a classic prediction„ÄÇSo I'm going to just show some rough code or kind of data to demonstrate what these three are doing in the context of the code we wrote before„ÄÇ before we actually dive into using a means„ÄÇNow before in our class we saw that we had these assigned points„ÄÇ

 an updates centerÔºå and that was the real core of what we needed to doÔºå and a fit method„ÄÇ what it'll probably be doing is some sort of loop like this for I and range of something„ÄÇYesterday I' was just going to be calling both of those it'll call the sign points„ÄÇAnd it will call update centers and it'll do that a bunch of times to try to find the right answer and how many times is that„ÄÇ

 well when I call that number of times it does that„ÄÇ the number of EpoCs and so I might say Epos here and like I was saying in the actual K means that comes with SK learn this will be an upper bound right if it sees that this is not improving anything further„ÄÇ

 it might have some break that happens if it's already done„ÄÇIf not getting any better„ÄÇ I'm not going to do that Im just trying to write kind of rough code to give you an idea of what's happening„ÄÇ And so when we do this right on our versionÔºå I plot this at the end„ÄÇHopefully„ÄÇ hopefully it's solving this and kind of updating those points and indeed it is right it kind of figuring out where each of those centroids should go so that's the fit method now in the process of doing this we created a lot of supplemental information for original data frame right so this was original data frame that just has some points„ÄÇ

And in contrast to that„ÄÇWe have this data frame that the K means class was using with some extra information first we have the distance„ÄÇ each of the clusters and second can be useful information in and of itself by looking at those three distances we can figure out which number is smallest in this case„ÄÇ

 number in the x column is smallest this is going to be this row is going to be an x cluster right in this next one the O number is smallest„ÄÇSo we're going to be in the old cluster„ÄÇAnd so when we're looking at„ÄÇ let me ask just shorten this up a„ÄÇWhen we're looking at using k meanss for either transformation or prediction„ÄÇ only difference is whether we're using these distances„ÄÇOr the labels„ÄÇ

 And so let me do the transformation first to show you what we'll get there„ÄÇ we'll be effectively getting this data„ÄÇü§ß„ÄÇThat's what we'll get out when we do a transformation in tas and I'll eventually talk about how that's useful as a pre processingcess step before we do something like logistic regression„ÄÇ

And then for predictionÔºå all we're really getting is well what group does it fit nicely into and again„ÄÇ right this is not really like classic prediction because we're both deciding what the labels are and deciding which points go with each label„ÄÇOkayÔºå I'm going to use K means on this same data from K means from SK learn this time instead of our own„ÄÇAnd so I'm going say K M equals K means„ÄÇAnd there's a bunch of configuration options here„ÄÇ

 for exampleÔºå how many clusters we want to start with Im going say„ÄÇWe would like to start with three of them and then what we can say KM dot„ÄÇBut you always have to do a fit regardless of whether we're going to do a transformation or prediction next„ÄÇAnd I want to fit that data frame„ÄÇ Let me just look at that one more time„ÄÇ data frame that had„ÄÇ

So I want to fit that data„ÄÇAnd we do„ÄÇ And And once we do thatÔºå then we can do either of those things„ÄÇ we could say transform„ÄÇAnd there might be some cases where this is a kind of a training data and then we're trying to apply our clusters or force our clusters on some you know second data frame maybe some test data and it'll be very common though we want to do it to the same original data and so when I'm doing this transformation here I saw well I have three clusters that's why I'm doing three columns of numbers here the three distances„ÄÇ

Or each row in my original data set„ÄÇAnd it is very common right rather than doing fit and then transform„ÄÇ you know why not do both at onceÔºüJust like sell„ÄÇThat would be a fine thing to do the same way I can also do a fit predict„ÄÇ And then instead of sayingÔºå wellÔºå which group is in itÔºå it's trying to tell me„ÄÇSpecificallyÔºå oh„ÄÇ you're in group zeroÔºå you're in group twoÔºå so onÔºå and so forth„ÄÇ

And so something we might want to do is that I might want to write a copy of my original data frame„ÄÇAnd thenÔºå add this prediction in„ÄÇAnd I'm going to say„ÄÇA lot I IÔºå I going call that clusterÔºå right„ÄÇ I could call it a classificationÔºå something like that„ÄÇAnd„ÄÇAnd let me actually look at this now„ÄÇI can see while these are the clusters that is predicted to be in„ÄÇ

 they I look at the tail to see some others„ÄÇRight and and so I could plot it and I could assign different colors to the different clusters if I wanted to right I could say that„ÄÇdot scatter x equals x0 y equals x1Ôºå rightÔºå we're having x in both dimensions„ÄÇ

I get those three thingsÔºå but I'd really like to see what color they are„ÄÇ So I'm going to pass in„ÄÇcolor equals data frame 2 of what cluster you are„ÄÇ And and you notice one of these vanishes„ÄÇ So 0 ends up being white„ÄÇAnd so what I should do is I should pass in a different color math„ÄÇAnd so let me head over here and look at the different color maps in matpl Lib„ÄÇÂóØ„ÄÇ

Cluster 0 is not more similar to cluster1 than it is to cluster2 right so I don't really care about getting what is called a sequential color map„ÄÇ something like thatÔºå where's kind of on the spectrum really01 and two or different categories for me„ÄÇ so I going be looking into the qualitative color maps„ÄÇAnd I'm just try to go with this one here„ÄÇ use it in my set of colorsÔºå and I'm not trying to have more than 10 clustersters„ÄÇWhen me do that„ÄÇ

 andm going say I want the tab 10 color map„ÄÇAnd now I can see it's actually giving those different colors„ÄÇ those different groups of points„ÄÇIf I wanted toÔºå I could also look at the centroids and draw that on top of here„ÄÇ and I get the centroids like so I can say dataÔºå I'm sorry„ÄÇCame in start„ÄÇUusster centers„ÄÇAnd what am I getting here allÔºüBut the ordinance of each centroid are like a row here„ÄÇ

 and I have three centroidsÔºå and that's why I have three rows„ÄÇAnd so I could absolutely„ÄÇWrapped that up in a data frame„ÄÇAnd I could plot itÔºå rightÔºå I could say do plot dot scatter„ÄÇAnd I can say all the x is x0Ôºå ohÔºå x is 0„ÄÇY is 1„ÄÇAnd I could plot those three points„ÄÇAnd let me just make them larger and redÔºå rightÔºå so I'm going say color equals red„ÄÇA size equals 100„ÄÇ

Actually„ÄÇUse S hereÔºå and I should really combine this with what I had before„ÄÇI can actually see the centroids„ÄÇAnd to really make it workÔºå I have to say„ÄÇThat it should use the same regionÔºå right„ÄÇYouÔºå let me just split this up here„ÄÇ I it' getting too long„ÄÇCentral on and stop hot top ScÔºå same Ax„ÄÇAnd so I can do all that same stuff„ÄÇ

 just like I did with our own version before„ÄÇOkay„ÄÇLet me address an issue which is how did I know that we should use three clusters and well the answer here is that I just kind of eyeballed it„ÄÇ what if there's like 20 clusters right that might not be so easy to do or what if instead of having nice two dimensional data„ÄÇ

 I have x0Ôºå x1Ôºå x2 x3Ôºå x4 x5„ÄÇIt won't be obvious before and how many clusters there are„ÄÇ And so the strategy that you'll do is you'll try a her number of clusters and see how well it does„ÄÇ and this measure of how well it does is called inertia„ÄÇ And so I can look at inertia„ÄÇAnd our data like this„ÄÇ And wellÔºå what is this measuringÔºå It's measuring the average„ÄÇWhered„ÄÇ

Distance from points to nearest„ÄÇCentroid that's what that means right so for example„ÄÇ this one over here is actually kind of far from that centroid right so I'll contribute a lot to this score whereas this one right here is really close to a centroid so hopefully everything's kind of neatly around a centroid and of course the more the more centroids I have this number will go down this inertia go down right lower is better it means no the number means everything's nearcentroid and so what we'll do is we'll actually try different numbers of clusters and see how quickly inertia drops off so let's do this„ÄÇ

So I'm going to go back and grab all of this stuff I had before„ÄÇAnd„ÄÇAnd somebody grab this and„ÄÇSo that let meÔºå let me do this„ÄÇ This is what I really need„ÄÇI am going to have a little loop like thisÔºå and I don't even care about making the predictions anymore„ÄÇ I just want to know that inertia score„ÄÇA„ÄÇEa„ÄÇOh„ÄÇE squ„ÄÇOkay„ÄÇ

 and so you can see what I'm going to do here„ÄÇ right I'm going to try different amounts„ÄÇ of course„ÄÇ as I add„ÄÇMore of these thingsÔºå the nursetry goes down until the ball if I have the same number of clusters and points„ÄÇThen luckilyÔºå each point hit its own cluster„ÄÇ So I may have a loop here„ÄÇ I'm going say 4 k in range„ÄÇ And I want to have one to 10 clustersÔºå rightÔºå So KÔºå that's why it's K means„ÄÇ

K is the number of centroids and mean versus the fact that a centroid is kind of the average of other Xy values„ÄÇI run this thing„ÄÇAnd and I want to put all of these in a dictionary or better than a dictionary„ÄÇ even a seriesÔºå rightÔºå it's Im going to say„ÄÇS„ÄÇIt was a series„ÄÇAnd so I'm going to do it like this„ÄÇ I'm going to say„ÄÇWars of K equals this thing„ÄÇThisÔºå this inertia„ÄÇI'm going to try running this„ÄÇ

And when I'm alld onÔºå or I think there's maybe some issues here„ÄÇ still„ÄÇ one of the issues that's actually relatively new in pandas is that they don't like you„ÄÇTo leave it ambiguous what the type is going to be„ÄÇ So I may be very explicit up front„ÄÇ this series is going to have floats„ÄÇAnd then the other thing it's complaining about isÔºå well„ÄÇ

 I have a key error of one„ÄÇAnd the reason why I have that is because when I just put brackets after a series„ÄÇ it's guessing whether this is an index„ÄÇOr an integer position„ÄÇ And I was guessing incorrectly that an integer position„ÄÇSo it's guessing this„ÄÇ which of course doesn't work if I change it to thatvoila I can get my scores„ÄÇ

And once I have my scoresÔºå of courseÔºå I can plot my scores„ÄÇLike so„ÄÇ And and I should also do this„ÄÇ I should say„ÄÇI should say that some labels here„ÄÇThe X label is kÔºå which is the number of clusters„ÄÇAnd my y label is what my Y label is„ÄÇaverage„ÄÇBd distance„ÄÇYour nearest your nearest centroid„ÄÇRight and so when I'm looking at this hereÔºå I see that having two centroids„ÄÇ

Is much better than having oneÔºå so there's two very clear clusters„ÄÇD from two to three„ÄÇ another big improvement after that doesn't make sense to have four or centroids right that's not going to give me much improvement„ÄÇLet's try running this again right Im going run run it from the top because sometimes these clusters will tend to overlap each other so just because„ÄÇAnd way back here I created three clusters doesn't mean there's going to be three clear clusters at the end„ÄÇ

 so let me just run this again„ÄÇü§ß„ÄÇAnd hereÔºå rightÔºå it's not as clear is that two clusters or three clusters and not surprisingly„ÄÇWhile there's less benefit going from2 to„ÄÇGreatÔºå that let me just try running it a couple more times„ÄÇGet some more intuition here„ÄÇOkayÔºå so that that it' very clear that we want to go to three„ÄÇÂëÉ„ÄÇI want to see one where it's really overlappingÔºå which is kind of a matter of luck here„ÄÇ

 What about that oneÔºå not so much benefit going from two to three„ÄÇ right because these two are overlapping each other so muchÔºå rightÔºüOkay„ÄÇ so that's what you'll often want to do so one of the use cases for these things is that you will create a plot like this just so you can say something about your data„ÄÇ you can say how many kind of distinct clusters there are and that of course is a matter of prediction next time I'm may be talking about how we can do where it was my notes„ÄÇ

What are the uses for these transformationsÔºå why might we want to get data about the distance to each of the clustersÔºü



![](img/77644a3896823f6a90d56917ef525e6a_2.png)

![](img/77644a3896823f6a90d56917ef525e6a_3.png)

In the last couple of videosÔºå we've been looking at how we can use K means to identify how many clusters there are in the data and that can be useful in and of itself„ÄÇAnother common strategy is that we'll use K means as preprocess for another stage in our pipeline„ÄÇ

 and more generally you might apply some unsupervised learning technique like K means or principal component analysis to create better inputs for a supervised technique„ÄÇ for exampleÔºå logistic regression„ÄÇSo I'm going do that here and I've really tried to create data that will really make this work well and so here you can see in the left both my training data and my test data on the right„ÄÇ

And what you can see in the training data is that I've created five clusters here„ÄÇAnd these clusters are kind of right on top of each other and out of the five clusters„ÄÇ four of them have black dots„ÄÇAnd one of them has gray dotsÔºå and other than that„ÄÇ they're just gray dots kind of randomly distributed throughout the space„ÄÇ

And then on the right we want to predict that and so clearly there are some patterns here„ÄÇ for exampleÔºå as a human I might predict that these are in a similar area as this cluster of black dots over here so I probably guess that these are black and then other dots right like if I'm saying in this space right here„ÄÇ

 those are probably gray is in the training data those were gray„ÄÇüòä„ÄÇAnd so certainly it would be hard to draw a single line that separates the black dots from the gray dots„ÄÇFor the purposes of a logistic regressionÔºå so I have to do some sort of pre processing„ÄÇ Okay„ÄÇ so I'm going to create my pipeline down hereÔºå A P is a pipeline„ÄÇ

And pipeline is just a list of steps„ÄÇ and the last stepÔºå the most important one„ÄÇMaybe a logistic regression„ÄÇLike so„ÄÇ And in the first one is going to be standard scaling„ÄÇBandard scalar like soÔºå And then eventually I'm going to add in K means as a pre processing step to help logistic regression work better„ÄÇAnd so I'm going to do thisÔºå I'm going to fit my modelÔºå so P dot fit„ÄÇ

And what I want to fit to I've already taken my data frame up here and split it into training and test data„ÄÇAnd I can see that my two input column are x0 and x1„ÄÇSo I'm going to just put those in a variable here and going say x columns„ÄÇIs x0ÔºüAnd x1„ÄÇ then my y column I'm trying to predict is just yÔºå as I'm going to predict on the training data or fit on the training data those x columns„ÄÇ

 And I'm going to compare that to my„ÄÇWhyÔºå ColinÔºü and then after I do thatÔºå I want to score„ÄÇ How well does this classifier work„ÄÇ So I'm going to score it„ÄÇMy testing data„ÄÇAnd so I run that and I see it's not doing very well it's only getting about 63% correct because it's hard to separate those black dots„ÄÇFrom the gray dots„ÄÇAnd and that's because when I just have a regular logistic regression has to only put a straight line there„ÄÇ

 So if I introduce K meansan as a pre processing step„ÄÇLet's try that„ÄÇAnd great ca meansians here„ÄÇAnd„ÄÇ andÔºå and let me specify the number of clusters„ÄÇ I'll sayÔºå end clusters„ÄÇEquals let's say„ÄÇ three first„ÄÇI'm going to try running that„ÄÇAnd what happened thereÔºüI have an extra parentheesis„ÄÇ just kind of randomly„ÄÇThere we go„ÄÇ That's where it's supposed to be„ÄÇExcuse me„ÄÇHe„ÄÇ

 so still not very good because there's not enough clusters hereÔºå right„ÄÇ I guess the original there are five clusters„ÄÇ If I tried jumping up to 5„ÄÇYou can see that I'm doing significantly better but if I go up something like 10 better still right I can kind of capture the different areas and realize how close they are but having more clusters when I'm doing this preprocessing is generally not going to be as problematic is having to a few clusters so what's happening here when I'm taking the input variable to this well I'm using this K means step on the data right so what this is outputting is the distance to each of those1 clusters that identifying right so I might know is one of my variables well what is the distance to this cluster here and of course if that distance is small then it's probably a black point I might also have another problem that says well what is the distance to this cluster here if that's small well then it's actually probably a gray point„ÄÇ

So this is one way to do the preprocessingÔºå an alternativeÔºå which probably works just as well„ÄÇ would be polynomial features could also kind of figure out that more complex boundary between the black and the green„ÄÇ![](img/77644a3896823f6a90d56917ef525e6a_5.png)