- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P27ï¼šL4.4- ä½¿ç”¨ TensorFlow è¿›è¡Œå¾®è°ƒ(è¿ç§»å­¦ä¹ ) - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½Hugging Faceå·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£TransformersåŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P27ï¼šL4.4-
    ä½¿ç”¨TensorFlowè¿›è¡Œå¾®è°ƒï¼ˆè¿ç§»å­¦ä¹ ï¼‰ - ShowMeAI - BV1Jm4y1X7UL
- en: So in this videoï¼Œ we're going to see how to load and fine tune a pre trained
    modelã€‚ It's very quickã€‚ And if you've watched our pipeline videosï¼Œ which I'll
    link belowï¼Œ the process is very similarã€‚ğŸ˜Šã€‚This timeï¼Œ thoughï¼Œ we're going to be
    using transfer learning and doing some training ourselvesã€‚ rather than just loading
    a model and using it as isï¼Œ like we did in the pipeline videosã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åŠ è½½å’Œå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚è¿‡ç¨‹éå¸¸å¿«ã€‚å¦‚æœä½ çœ‹è¿‡æˆ‘ä»¬çš„ç®¡é“è§†é¢‘ï¼Œæˆ‘ä¼šåœ¨ä¸‹é¢é“¾æ¥ï¼Œè¿‡ç¨‹éå¸¸ç›¸ä¼¼ã€‚ğŸ˜Šä¸è¿‡è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿ç§»å­¦ä¹ å¹¶è¿›è¡Œä¸€äº›è®­ç»ƒï¼Œè€Œä¸ä»…ä»…æ˜¯åŠ è½½æ¨¡å‹å¹¶ç›´æ¥ä½¿ç”¨ï¼Œå°±åƒæˆ‘ä»¬åœ¨ç®¡é“è§†é¢‘ä¸­æ‰€åšçš„é‚£æ ·ã€‚
- en: So if you to learn more about transfer learningï¼Œ if you don't know much about
    itã€‚ you can head to the what is transfer learning videoã€‚ And I'll link that below
    as wellã€‚ğŸ˜Šï¼ŒBut for nowã€‚ let's look at this codeã€‚ So to startï¼Œ we pick which model
    we want to useã€‚ In this caseã€‚ we're going to use the famous plastic Btã€‚But what
    does thisï¼Œ this line hereï¼Œ this monstrosityã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºè¿ç§»å­¦ä¹ çš„å†…å®¹ï¼Œè€Œä½ å¯¹æ­¤äº†è§£ä¸å¤šï¼Œä½ å¯ä»¥å»çœ‹ä¸€ä¸‹â€œä»€ä¹ˆæ˜¯è¿ç§»å­¦ä¹ â€çš„è§†é¢‘ã€‚æˆ‘ä¼šåœ¨ä¸‹é¢é“¾æ¥ã€‚ğŸ˜Šä½†ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™æ®µä»£ç ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‘—åçš„å¡‘æ–™Btã€‚ä½†è¿™è¡Œä»£ç åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ
- en: this TF auto model for sequence classificationï¼Œ What does that meanã€‚Wellã€‚ the
    TF stands for Tensorflowï¼Œ and the rest means take a language model and stick a
    sequence classification head onto it if it doesn't have one alreadyã€‚So what we're
    going to do here is load Btï¼Œ which is a general general purpose language model
    that doesn't have a sequence classification headã€‚ We're going to use the from
    pre traineded methodï¼Œ and that method ensures that all our weights come from the
    pre trained modelã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªTFè‡ªåŠ¨æ¨¡å‹ç”¨äºåºåˆ—åˆ†ç±»ï¼Œè¿™æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿå¥½å§ï¼ŒTFä»£è¡¨Tensorflowï¼Œå…¶ä½™éƒ¨åˆ†æ˜¯æŒ‡å¦‚æœè¯­è¨€æ¨¡å‹è¿˜æ²¡æœ‰åºåˆ—åˆ†ç±»å¤´ï¼Œå°±å°†å…¶æ·»åŠ ä¸Šå»ã€‚æ‰€ä»¥æˆ‘ä»¬è¦åšçš„æ˜¯åŠ è½½Btï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œæ²¡æœ‰åºåˆ—åˆ†ç±»å¤´ã€‚æˆ‘ä»¬å°†ä½¿ç”¨from
    pre trainededæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ç¡®ä¿æˆ‘ä»¬æ‰€æœ‰çš„æƒé‡éƒ½æ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: So they're not randomly initialized with the exception of the new sequence classification
    head we're going to addã€‚ğŸ˜Šï¼ŒSo this method needs to know two thingsã€‚Firstlyï¼Œ it
    needs to know the name of the model you wanted to loadã€‚ And secondlyï¼Œ it needs
    to know how many classes your problem hasã€‚So if you want to follow along with
    the data from our data sets videosï¼Œ which I'll link belowã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æˆ‘ä»¬å°†è¦æ·»åŠ çš„æ–°åºåˆ—åˆ†ç±»å¤´å¤–ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚ğŸ˜Šæ‰€ä»¥è¿™ä¸ªæ–¹æ³•éœ€è¦çŸ¥é“ä¸¤ä»¶äº‹ã€‚é¦–å…ˆï¼Œå®ƒéœ€è¦çŸ¥é“ä½ æƒ³åŠ è½½çš„æ¨¡å‹çš„åç§°ã€‚å…¶æ¬¡ï¼Œå®ƒéœ€è¦çŸ¥é“ä½ çš„é—®é¢˜æœ‰å¤šå°‘ä¸ªç±»åˆ«ã€‚å¦‚æœä½ æƒ³è·Ÿéšæˆ‘ä»¬æ•°æ®é›†è§†é¢‘ä¸­çš„æ•°æ®ï¼Œæˆ‘ä¼šåœ¨ä¸‹é¢é“¾æ¥ã€‚
- en: then you'll have two classesï¼Œ positive and negativeï¼Œ and thus nu labels equals
    2ã€‚But what about this compile thingã€‚So if you're familiar with Carisã€‚ you've probably
    seen this alreadyã€‚ But if notï¼Œ this is one of the core methods in Carisã€‚ you always
    need to compile your model before you train itã€‚ compileile needs to know two thingsã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä½ å°†æœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œæ­£ç±»å’Œè´Ÿç±»ï¼Œå› æ­¤nuæ ‡ç­¾ç­‰äº2ã€‚ä½†æ˜¯è¿™ä¸ªç¼–è¯‘çš„ä¸œè¥¿å‘¢ï¼Ÿå¦‚æœä½ ç†Ÿæ‚‰Carisï¼Œä½ å¯èƒ½å·²ç»çœ‹è¿‡è¿™ä¸ªã€‚å¦‚æœæ²¡æœ‰ï¼Œè¿™æ˜¯Carisä¸­çš„æ ¸å¿ƒæ–¹æ³•ä¹‹ä¸€ï¼Œä½ æ€»æ˜¯éœ€è¦åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ç¼–è¯‘å®ƒã€‚ç¼–è¯‘éœ€è¦çŸ¥é“ä¸¤ä»¶äº‹ã€‚
- en: Firstlyï¼Œ the loss functionï¼Œ which is basicallyï¼Œ what are we trying to optimizeã€‚
    And here we import the sparse categorical cross entropy loss functionã€‚ So that's
    a mouthfulã€‚ If you've never encountered it beforeã€‚ but it's the standard loss
    function for any neural network that's doing a classification taskã€‚ It basically
    encourages the network to output large valuesã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒæŸå¤±å‡½æ•°ï¼ŒåŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬æƒ³è¦ä¼˜åŒ–çš„å†…å®¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¼å…¥ç¨€ç–åˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚å¬èµ·æ¥å¤æ‚ï¼Œå¦‚æœä½ ä»¥å‰ä»æœªé‡åˆ°è¿‡ã€‚ä½†è¿™æ˜¯ä»»ä½•è¿›è¡Œåˆ†ç±»ä»»åŠ¡çš„ç¥ç»ç½‘ç»œçš„æ ‡å‡†æŸå¤±å‡½æ•°ã€‚å®ƒåŸºæœ¬ä¸Šé¼“åŠ±ç½‘ç»œè¾“å‡ºå¤§çš„å€¼ã€‚
- en: So large probabilities for the right class and low values of low probabilities
    for the wrong classesã€‚Notice that youï¼Œ you can specify the last function as a
    string like we do with the optr hereã€‚ But there's a very common pitfall hereã€‚
    By defaultã€‚ the last assumes the output is probabilities from a softm layerã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·å¯¹äºæ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡ä¼šå¾ˆå¤§ï¼Œè€Œå¯¹äºé”™è¯¯ç±»åˆ«çš„æ¦‚ç‡åˆ™å¾ˆå°ã€‚æ³¨æ„ï¼Œä½ å¯ä»¥åƒæˆ‘ä»¬åœ¨è¿™é‡Œå¯¹optræ‰€åšçš„é‚£æ ·ï¼Œä»¥å­—ç¬¦ä¸²çš„å½¢å¼æŒ‡å®šæŸå¤±å‡½æ•°ã€‚ä½†è¿™é‡Œæœ‰ä¸€ä¸ªå¸¸è§çš„é™·é˜±ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒæŸå¤±å‡½æ•°å‡è®¾è¾“å‡ºæ˜¯æ¥è‡ªsoftmaxå±‚çš„æ¦‚ç‡ã€‚
- en: But what our model has actually output is the values before the softmã€‚ These
    are often called the logicits or logicitsã€‚ You saw these before in the video about
    pipelinesã€‚ğŸ˜Šï¼ŒIf you get that this wrongï¼Œ your model won't trainã€‚ And it'll be very
    annoying to figure out whyã€‚ In factï¼Œ I'm going to go so far as to say that if
    you remember absolutely nothing else from this videoã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬æ¨¡å‹å®é™…è¾“å‡ºçš„æ˜¯softmä¹‹å‰çš„å€¼ã€‚è¿™äº›é€šå¸¸è¢«ç§°ä¸ºé€»è¾‘å€¼æˆ–é€»è¾‘é‡ã€‚ä½ åœ¨å…³äºç®¡é“çš„è§†é¢‘ä¸­è§è¿‡è¿™äº›ã€‚ğŸ˜Šå¦‚æœä½ æé”™äº†è¿™ä¸€ç‚¹ï¼Œä½ çš„æ¨¡å‹å°†æ— æ³•è®­ç»ƒã€‚è€Œä¸”ææ¸…æ¥šä¸ºä»€ä¹ˆä¼šéå¸¸éº»çƒ¦ã€‚å®é™…ä¸Šï¼Œæˆ‘ç”šè‡³æ•¢è¯´ï¼Œå¦‚æœä½ ä»è¿™ä¸ªè§†é¢‘ä¸­ä»€ä¹ˆéƒ½ä¸è®°å¾—ã€‚
- en: remember to always check whether your model is outputting logicits or probabilitiesã€‚
    and make sure your loss is set up to match thatã€‚ So this is going save you a lot
    of debugging headaches in your careerã€‚ That would otherwise be very difficult
    to track down and very annoyingã€‚But leaving that asideã€‚ the second thing compile
    needs to know is the optr you wantã€‚ In our caseï¼Œ we're going to use atomã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œå§‹ç»ˆæ£€æŸ¥ä½ çš„æ¨¡å‹æ˜¯å¦è¾“å‡ºé€»è¾‘å€¼æˆ–æ¦‚ç‡ï¼Œå¹¶ç¡®ä¿ä½ çš„æŸå¤±è®¾ç½®ä¸ä¹‹åŒ¹é…ã€‚è¿™å°†ä¸ºä½ åœ¨èŒä¸šç”Ÿæ¶¯ä¸­èŠ‚çœå¾ˆå¤šè°ƒè¯•çš„éº»çƒ¦ã€‚å¦åˆ™è¿™äº›é—®é¢˜å°†å¾ˆéš¾è¿½è¸ªä¸”éå¸¸çƒ¦äººã€‚ä½†æ’‡å¼€è¿™ä¸€ç‚¹ï¼Œç¼–è¯‘æ‰€éœ€çŸ¥é“çš„ç¬¬äºŒä»¶äº‹æ˜¯ä½ æƒ³è¦çš„optrã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨atomã€‚
- en: which is sort of the standard optimizer for deep learning these daysã€‚ The one
    thing you might want to change is the learning rateã€‚ And to do that we'll need
    to import the actual optrï¼Œ rather than just calling it by stringã€‚ So much like
    we did with the lossã€‚ But we can talk about that in another videoã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç°åœ¨æ·±åº¦å­¦ä¹ çš„æ ‡å‡†ä¼˜åŒ–å™¨ä¹‹ä¸€ã€‚ä½ å¯èƒ½æƒ³è¦æ”¹å˜çš„å”¯ä¸€ä¸€ç‚¹æ˜¯å­¦ä¹ ç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥å®é™…çš„optrï¼Œè€Œä¸æ˜¯ä»…ä»…é€šè¿‡å­—ç¬¦ä¸²è°ƒç”¨å®ƒã€‚å°±åƒæˆ‘ä»¬ä¹‹å‰å¤„ç†æŸå¤±æ—¶åšçš„é‚£æ ·ã€‚ä¸è¿‡æˆ‘ä»¬å¯ä»¥åœ¨å¦ä¸€ä¸ªè§†é¢‘ä¸­è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚
- en: and I'll link that below for nowï¼Œ let's just try training the modelã€‚ğŸ˜Šï¼ŒSo how
    do you train the modelï¼Ÿ
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä¼šåœ¨ä¸‹é¢é“¾æ¥è¿™ä¸ªï¼Œå…ˆè¯•ç€è®­ç»ƒæ¨¡å‹ã€‚ğŸ˜Šé‚£ä¹ˆï¼Œå¦‚ä½•è®­ç»ƒæ¨¡å‹å‘¢ï¼Ÿ
- en: Wellï¼Œ if you've used Kais beforeï¼Œ this will all be very familiar to youã€‚ But
    if notã€‚ let's look at what we're doing hereã€‚ Fit is pretty much the central method
    for Kais modelsã€‚ It tells the model to break the input into batches and then train
    on itã€‚ So the first input is tokenized textã€‚ you'll almost always be getting this
    from a tokenizerã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œå¦‚æœä½ ä»¥å‰ç”¨è¿‡Kaisï¼Œè¿™ä¸€åˆ‡å¯¹ä½ æ¥è¯´éƒ½ä¼šå¾ˆç†Ÿæ‚‰ã€‚ä½†å¦‚æœæ²¡æœ‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åœ¨åšä»€ä¹ˆã€‚Fitå‡ ä¹æ˜¯Kaisæ¨¡å‹çš„æ ¸å¿ƒæ–¹æ³•ã€‚å®ƒå‘Šè¯‰æ¨¡å‹å°†è¾“å…¥åˆ†æˆæ‰¹æ¬¡ï¼Œç„¶åè¿›è¡Œè®­ç»ƒã€‚æ‰€ä»¥ç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ä»¤ç‰ŒåŒ–æ–‡æœ¬ã€‚ä½ å‡ ä¹æ€»æ˜¯ä¼šä»ä¸€ä¸ªä»¤ç‰Œå™¨ä¸­è·å¾—è¿™ä¸ªã€‚
- en: And if you want to learn more about that processã€‚ what exactly these inputs
    look likeã€‚ please check out our videos on tokensã€‚ And againï¼Œ there'll be links
    for those belowã€‚So those are our inputsã€‚But then the second argument is our labelsã€‚
    and this is really straightforwardã€‚ This is just a one dimensional nuy or tensorflow
    array of integersã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºè¿™ä¸ªè¿‡ç¨‹çš„ä¿¡æ¯ï¼Œè¿™äº›è¾“å…¥åˆ°åº•æ˜¯ä»€ä¹ˆæ ·å­ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬å…³äºä»¤ç‰Œçš„è§†é¢‘ã€‚å†æ¬¡å¼ºè°ƒï¼Œä¸‹é¢ä¼šæœ‰ç›¸å…³é“¾æ¥ã€‚æ‰€ä»¥è¿™äº›æ˜¯æˆ‘ä»¬çš„è¾“å…¥ã€‚ä½†ç¬¬äºŒä¸ªå‚æ•°æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾ï¼Œè¿™çœŸçš„å¾ˆç®€å•ã€‚è¿™åªæ˜¯ä¸€ä¸ªä¸€ç»´çš„nuyæˆ–tensorflowæ•´æ•°æ•°ç»„ã€‚
- en: and they correspond to the classes for examplesã€‚ that's itã€‚ğŸ˜Šã€‚So if you're following
    along with our data from our data sets videoï¼Œ there'll only be two classesã€‚ so
    this will just be a vector of zeros and onesï¼Œ but you can have many more classes
    than that for your own problemsã€‚So once we have our inputs and our labelsï¼Œ we
    do the same thing with the validation dataã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å¯¹åº”äºæ ·æœ¬çš„ç±»åˆ«ã€‚å°±è¿™æ ·ã€‚ğŸ˜Šæ‰€ä»¥å¦‚æœä½ åœ¨è·Ÿéšæˆ‘ä»¬æ•°æ®é›†è§†é¢‘ä¸­çš„æ•°æ®ï¼Œåªæœ‰ä¸¤ä¸ªç±»åˆ«ã€‚å› æ­¤è¿™å°†åªæ˜¯ä¸€ä¸ªé›¶å’Œä¸€çš„å‘é‡ï¼Œä½†ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„é—®é¢˜æ‹¥æœ‰æ›´å¤šçš„ç±»åˆ«ã€‚ä¸€æ—¦æˆ‘ä»¬æœ‰äº†è¾“å…¥å’Œæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯¹éªŒè¯æ•°æ®åšåŒæ ·çš„äº‹æƒ…ã€‚
- en: We pass the validation inputs and the validation labels in a tupleã€‚ and then
    we canã€‚ if we want to specify date details like the batch size for trainingã€‚ And
    then you just pass the whole thing to model dot fit and you let it ripã€‚ So if
    everything works outï¼Œ you should see a little training progress bar as your last
    goes downã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»¥å…ƒç»„çš„å½¢å¼ä¼ é€’éªŒè¯è¾“å…¥å’ŒéªŒè¯æ ‡ç­¾ã€‚å¦‚æœæˆ‘ä»¬æƒ³æŒ‡å®šè®­ç»ƒçš„æ‰¹é‡å¤§å°ç­‰ç»†èŠ‚ï¼Œå°±å¯ä»¥è¿™æ ·åšã€‚ç„¶åä½ åªéœ€å°†æ•´ä¸ªå†…å®¹ä¼ é€’ç»™model.dot fitï¼Œæ”¾æ‰‹å»åšã€‚å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ªå°çš„è®­ç»ƒè¿›åº¦æ¡ï¼Œéšç€æŸå¤±çš„é™ä½è€Œä¸‹é™ã€‚
- en: And while that's runningï¼Œ you knowï¼Œ you sit backï¼Œ you call your boss and you
    tell them you're a senior NLP machine learning engineer nowã€‚ and you're going
    to want to salary review next quarterã€‚ So this is reallyï¼Œ I'm kidding a bitã€‚ But
    this is really all it takes to apply the power of a massive pretrained language
    model to your N LP problemã€‚ğŸ˜Šï¼ŒBut could we do better than thisï¼Œ Likeï¼Œ is there
    any changes we could makeï¼Œ So there certainly areã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªè¿è¡Œæ—¶ï¼Œä½ çŸ¥é“ï¼Œä½ å¯ä»¥åä¸‹æ¥ï¼Œç»™ä½ çš„è€æ¿æ‰“ç”µè¯ï¼Œå‘Šè¯‰ä»–ä»¬ä½ ç°åœ¨æ˜¯ä¸€åé«˜çº§NLPæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå¹¶ä¸”ä¸‹ä¸ªå­£åº¦ä½ ä¼šæƒ³è¦è–ªèµ„å®¡æŸ¥ã€‚æ‰€ä»¥è¿™æ˜¯ï¼Œå¼€ä¸ªç©ç¬‘ï¼Œä½†è¿™çœŸçš„å°±æ˜¯å°†ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åŠ›é‡åº”ç”¨äºä½ çš„NLPé—®é¢˜æ‰€éœ€çš„ä¸€åˆ‡ã€‚ğŸ˜Šä½†æˆ‘ä»¬èƒ½åšåˆ°æ›´å¥½å—ï¼Ÿåƒï¼Œæ˜¯å¦æœ‰ä»»ä½•æˆ‘ä»¬å¯ä»¥åšçš„æ”¹å˜ï¼Ÿå½“ç„¶æœ‰ã€‚
- en: There's a few more advanced careis features like a tune scheduled learning rateã€‚
    We could get an even lower lossã€‚ And even therefore for an even more accurate
    modelã€‚ And alsoã€‚ when we when fit finishesï¼Œ what do we do with our model once
    it's trainedã€‚ So these are all topicsã€‚ I'm going to cover these and more in the
    videosã€‚ And againã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€äº›æ›´é«˜çº§çš„careisåŠŸèƒ½ï¼Œæ¯”å¦‚è°ƒåº¦çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´ä½çš„æŸå¤±ï¼Œå› æ­¤æ¨¡å‹ä¼šæ›´å‡†ç¡®ã€‚æ­¤å¤–ï¼Œå½“æ‹Ÿåˆå®Œæˆåï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•å¤„ç†è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚è¿™äº›éƒ½æ˜¯æˆ‘å°†åœ¨è§†é¢‘ä¸­æ¢è®¨çš„ä¸»é¢˜ï¼Œè¿˜æœ‰æ›´å¤šå†…å®¹ã€‚
- en: I'm going to link those subsequent videos belowã€‚ğŸ˜Šã€‚![](img/c842c6db02d2a7c1478de0863bd1f759_1.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æŠŠåç»­è§†é¢‘é“¾æ¥åœ¨ä¸‹é¢ã€‚ğŸ˜Šã€‚![](img/c842c6db02d2a7c1478de0863bd1f759_1.png)
- en: '![](img/c842c6db02d2a7c1478de0863bd1f759_2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c842c6db02d2a7c1478de0863bd1f759_2.png)'
- en: ã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
