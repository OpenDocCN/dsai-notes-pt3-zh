- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P27：L4.4- 使用 TensorFlow 进行微调(迁移学习) - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来啦！5位Hugging Face工程师带你了解Transformers原理细节及NLP任务应用！＜官方教程系列＞ - P27：L4.4-
    使用TensorFlow进行微调（迁移学习） - ShowMeAI - BV1Jm4y1X7UL
- en: So in this video， we're going to see how to load and fine tune a pre trained
    model。 It's very quick。 And if you've watched our pipeline videos， which I'll
    link below， the process is very similar。😊。This time， though， we're going to be
    using transfer learning and doing some training ourselves。 rather than just loading
    a model and using it as is， like we did in the pipeline videos。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将看到如何加载和微调一个预训练模型。过程非常快。如果你看过我们的管道视频，我会在下面链接，过程非常相似。😊不过这次，我们将使用迁移学习并进行一些训练，而不仅仅是加载模型并直接使用，就像我们在管道视频中所做的那样。
- en: So if you to learn more about transfer learning， if you don't know much about
    it。 you can head to the what is transfer learning video。 And I'll link that below
    as well。😊，But for now。 let's look at this code。 So to start， we pick which model
    we want to use。 In this case。 we're going to use the famous plastic Bt。But what
    does this， this line here， this monstrosity。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于迁移学习的内容，而你对此了解不多，你可以去看一下“什么是迁移学习”的视频。我会在下面链接。😊但现在，让我们看看这段代码。首先，我们选择要使用的模型。在这种情况下，我们将使用著名的塑料Bt。但这行代码到底是什么意思呢？
- en: this TF auto model for sequence classification， What does that mean。Well。 the
    TF stands for Tensorflow， and the rest means take a language model and stick a
    sequence classification head onto it if it doesn't have one already。So what we're
    going to do here is load Bt， which is a general general purpose language model
    that doesn't have a sequence classification head。 We're going to use the from
    pre traineded method， and that method ensures that all our weights come from the
    pre trained model。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个TF自动模型用于序列分类，这意味着什么呢？好吧，TF代表Tensorflow，其余部分是指如果语言模型还没有序列分类头，就将其添加上去。所以我们要做的是加载Bt，这是一种通用的语言模型，没有序列分类头。我们将使用from
    pre traineded方法，这个方法确保我们所有的权重都来自预训练模型。
- en: So they're not randomly initialized with the exception of the new sequence classification
    head we're going to add。😊，So this method needs to know two things。Firstly， it
    needs to know the name of the model you wanted to load。 And secondly， it needs
    to know how many classes your problem has。So if you want to follow along with
    the data from our data sets videos， which I'll link below。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们将要添加的新序列分类头外，它们并不是随机初始化的。😊所以这个方法需要知道两件事。首先，它需要知道你想加载的模型的名称。其次，它需要知道你的问题有多少个类别。如果你想跟随我们数据集视频中的数据，我会在下面链接。
- en: then you'll have two classes， positive and negative， and thus nu labels equals
    2。But what about this compile thing。So if you're familiar with Caris。 you've probably
    seen this already。 But if not， this is one of the core methods in Caris。 you always
    need to compile your model before you train it。 compileile needs to know two things。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你将有两个类别，正类和负类，因此nu标签等于2。但是这个编译的东西呢？如果你熟悉Caris，你可能已经看过这个。如果没有，这是Caris中的核心方法之一，你总是需要在训练模型之前编译它。编译需要知道两件事。
- en: Firstly， the loss function， which is basically， what are we trying to optimize。
    And here we import the sparse categorical cross entropy loss function。 So that's
    a mouthful。 If you've never encountered it before。 but it's the standard loss
    function for any neural network that's doing a classification task。 It basically
    encourages the network to output large values。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，损失函数，基本上是我们想要优化的内容。在这里，我们导入稀疏分类交叉熵损失函数。听起来复杂，如果你以前从未遇到过。但这是任何进行分类任务的神经网络的标准损失函数。它基本上鼓励网络输出大的值。
- en: So large probabilities for the right class and low values of low probabilities
    for the wrong classes。Notice that you， you can specify the last function as a
    string like we do with the optr here。 But there's a very common pitfall here。
    By default。 the last assumes the output is probabilities from a softm layer。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这样对于正确类别的概率会很大，而对于错误类别的概率则很小。注意，你可以像我们在这里对optr所做的那样，以字符串的形式指定损失函数。但这里有一个常见的陷阱。默认情况下，损失函数假设输出是来自softmax层的概率。
- en: But what our model has actually output is the values before the softm。 These
    are often called the logicits or logicits。 You saw these before in the video about
    pipelines。😊，If you get that this wrong， your model won't train。 And it'll be very
    annoying to figure out why。 In fact， I'm going to go so far as to say that if
    you remember absolutely nothing else from this video。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们模型实际输出的是softm之前的值。这些通常被称为逻辑值或逻辑量。你在关于管道的视频中见过这些。😊如果你搞错了这一点，你的模型将无法训练。而且搞清楚为什么会非常麻烦。实际上，我甚至敢说，如果你从这个视频中什么都不记得。
- en: remember to always check whether your model is outputting logicits or probabilities。
    and make sure your loss is set up to match that。 So this is going save you a lot
    of debugging headaches in your career。 That would otherwise be very difficult
    to track down and very annoying。But leaving that aside。 the second thing compile
    needs to know is the optr you want。 In our case， we're going to use atom。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，始终检查你的模型是否输出逻辑值或概率，并确保你的损失设置与之匹配。这将为你在职业生涯中节省很多调试的麻烦。否则这些问题将很难追踪且非常烦人。但撇开这一点，编译所需知道的第二件事是你想要的optr。在我们的案例中，我们将使用atom。
- en: which is sort of the standard optimizer for deep learning these days。 The one
    thing you might want to change is the learning rate。 And to do that we'll need
    to import the actual optr， rather than just calling it by string。 So much like
    we did with the loss。 But we can talk about that in another video。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是现在深度学习的标准优化器之一。你可能想要改变的唯一一点是学习率。为此，我们需要导入实际的optr，而不是仅仅通过字符串调用它。就像我们之前处理损失时做的那样。不过我们可以在另一个视频中讨论这个问题。
- en: and I'll link that below for now， let's just try training the model。😊，So how
    do you train the model？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我会在下面链接这个，先试着训练模型。😊那么，如何训练模型呢？
- en: Well， if you've used Kais before， this will all be very familiar to you。 But
    if not。 let's look at what we're doing here。 Fit is pretty much the central method
    for Kais models。 It tells the model to break the input into batches and then train
    on it。 So the first input is tokenized text。 you'll almost always be getting this
    from a tokenizer。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果你以前用过Kais，这一切对你来说都会很熟悉。但如果没有，让我们看看我们在做什么。Fit几乎是Kais模型的核心方法。它告诉模型将输入分成批次，然后进行训练。所以第一个输入是令牌化文本。你几乎总是会从一个令牌器中获得这个。
- en: And if you want to learn more about that process。 what exactly these inputs
    look like。 please check out our videos on tokens。 And again， there'll be links
    for those below。So those are our inputs。But then the second argument is our labels。
    and this is really straightforward。 This is just a one dimensional nuy or tensorflow
    array of integers。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于这个过程的信息，这些输入到底是什么样子，请查看我们关于令牌的视频。再次强调，下面会有相关链接。所以这些是我们的输入。但第二个参数是我们的标签，这真的很简单。这只是一个一维的nuy或tensorflow整数数组。
- en: and they correspond to the classes for examples。 that's it。😊。So if you're following
    along with our data from our data sets video， there'll only be two classes。 so
    this will just be a vector of zeros and ones， but you can have many more classes
    than that for your own problems。So once we have our inputs and our labels， we
    do the same thing with the validation data。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它们对应于样本的类别。就这样。😊所以如果你在跟随我们数据集视频中的数据，只有两个类别。因此这将只是一个零和一的向量，但你可以根据自己的问题拥有更多的类别。一旦我们有了输入和标签，我们对验证数据做同样的事情。
- en: We pass the validation inputs and the validation labels in a tuple。 and then
    we can。 if we want to specify date details like the batch size for training。 And
    then you just pass the whole thing to model dot fit and you let it rip。 So if
    everything works out， you should see a little training progress bar as your last
    goes down。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以元组的形式传递验证输入和验证标签。如果我们想指定训练的批量大小等细节，就可以这样做。然后你只需将整个内容传递给model.dot fit，放手去做。如果一切顺利，你应该会看到一个小的训练进度条，随着损失的降低而下降。
- en: And while that's running， you know， you sit back， you call your boss and you
    tell them you're a senior NLP machine learning engineer now。 and you're going
    to want to salary review next quarter。 So this is really， I'm kidding a bit。 But
    this is really all it takes to apply the power of a massive pretrained language
    model to your N LP problem。😊，But could we do better than this， Like， is there
    any changes we could make， So there certainly are。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个运行时，你知道，你可以坐下来，给你的老板打电话，告诉他们你现在是一名高级NLP机器学习工程师，并且下个季度你会想要薪资审查。所以这是，开个玩笑，但这真的就是将一个大规模预训练语言模型的力量应用于你的NLP问题所需的一切。😊但我们能做到更好吗？像，是否有任何我们可以做的改变？当然有。
- en: There's a few more advanced careis features like a tune scheduled learning rate。
    We could get an even lower loss。 And even therefore for an even more accurate
    model。 And also。 when we when fit finishes， what do we do with our model once
    it's trained。 So these are all topics。 I'm going to cover these and more in the
    videos。 And again。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些更高级的careis功能，比如调度的学习率。我们可以得到更低的损失，因此模型会更准确。此外，当拟合完成后，我们该如何处理训练好的模型。这些都是我将在视频中探讨的主题，还有更多内容。
- en: I'm going to link those subsequent videos below。😊。![](img/c842c6db02d2a7c1478de0863bd1f759_1.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把后续视频链接在下面。😊。![](img/c842c6db02d2a7c1478de0863bd1f759_1.png)
- en: '![](img/c842c6db02d2a7c1478de0863bd1f759_2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c842c6db02d2a7c1478de0863bd1f759_2.png)'
- en: 。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 。
