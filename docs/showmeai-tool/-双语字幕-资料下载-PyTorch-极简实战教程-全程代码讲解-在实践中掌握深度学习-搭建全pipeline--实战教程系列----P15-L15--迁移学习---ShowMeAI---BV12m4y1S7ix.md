# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL15- è¿ç§»å­¦ä¹  - ShowMeAI - BV12m4y1S7ix

Hiï¼Œ everybodyã€‚ welcomele to your new Pytorch tutorial In this tutorialã€‚ we will talk about transfer learning and how it can be applied in Pytorchã€‚ Transfer learning is a machine learning method where a model developed for a first task is then reused as the starting point for a model on a second taskã€‚ For exampleï¼Œ we can train a model to classify birds and cats and then use the same modelã€‚

 modified only a little bit in the last layerï¼Œ and then use the new model to classify bees and dogsã€‚ So it's a popular approach in deep learning that allows rapid generation of new modelsã€‚ And this is super important because training of a completely new model can be very time consumingã€‚ It can take multiple days or even weeksã€‚ So if you use a pretrained modelã€‚

 then we typically exchange only the last layer and then do not need to train the whole model againã€‚ Howeverï¼Œ transfer learning can achieve pretty good performance resultsã€‚ and that's why it's so popular nowadaysã€‚ğŸ˜Šï¼ŒSo let's have a look at this picture hereã€‚ We have a typical CNN architecture that I already showed you in the last tutorial and thisã€‚

 let's say this has been already trained on a lot of data and we have the optimized weights and now we only want to take the last fully connected layerã€‚ So this one here and then modify it and train the last layer on our new dataã€‚So then we have a new model that has been trained and tweaked in the last layerã€‚And yeahã€‚ this is the concept of transfer learningã€‚ And now let's have a look at a concrete example in Pytorchã€‚

 So in this exampleï¼Œ we wantï¼Œ we are using the pretrained Resnet 18 C Nã€‚ This is a network that is trained on more than a million images from the Inet databaseã€‚ And this network is 18 layers deep and can classify images into 1000 object categoriesã€‚ And now in our exampleï¼Œ we have only two classesï¼Œ So we only want to detect Bs and antsã€‚ğŸ˜Šã€‚



![](img/96844ee58fe06a9d803e6f751f732b19_1.png)

And yeahï¼Œ so let's startã€‚ So in this sessionï¼Œ I alreadyã€‚ I also want to show you two other new thingsã€‚ So firstï¼Œ the data sets image folderã€‚ how we can use this and how use a scheduleula to change the learning rateã€‚And thenï¼Œ of courseã€‚ how transfer learning is usedã€‚So I already imported the things that we needã€‚

 and now we set up the dataã€‚ and the last time we used the built in data sets from the Torch vision data setsã€‚ And now here we use the data sets dot image folder because we saved our data in a folder and this has to have the structure like thisã€‚ So we have the folder hereã€‚And then we have a training and a validation folderã€‚ So train and Valã€‚ And in each oneï¼Œ we have folders for each classã€‚ So here we have ans in ants and Bsã€‚

 and also in the validation folderï¼Œ we have ants and Bsã€‚And now in each folderã€‚ we have the images hereã€‚ Soï¼Œ for exampleï¼Œ hereï¼Œ we have some antsã€‚ And alsoã€‚ let's have a look at some Bsã€‚ So here we have a Bã€‚And yeahã€‚ so you must structure your folder like thisã€‚ And then you can call the data sets dot image folder and give it the pathã€‚

And we also give it some transforms hereã€‚And thenï¼Œ we get theã€‚Classesã€‚ the class names by calling image setsï¼Œ image data sets dot classesã€‚And yeahã€‚ then here I defined the training model where I did the loop and did the training and the evaluationã€‚ I will not go into detail hereã€‚ you should already know this from the last tutorials how at typical training and evaluation loop looks like you can also check the whole code on Gitthubã€‚

 so I will provide the link in the descriptionã€‚ So have a look at this yourselfã€‚And now let's use transfer learningã€‚ Soï¼Œ first of allï¼Œ we want to import the pre trained modelã€‚ So let's set up this model so we can do this by saying modelã€‚So model equalsã€‚ And this is available in the Torch visionion dot models moduleã€‚

 So I imported torch vision models alreadyã€‚ and then I can call models dot Resnet 16 or sorryã€‚ Resnet 18 hereã€‚ And then I can say pretrained equals trueã€‚ So this is already the optimized weights that are trained on the imagenet dataã€‚And now what we want to do is we want to exchange the last fully connected layerã€‚ So first of allã€‚

 let's get the number of input features from the last layerã€‚ So let's say nu features equals modelã€‚ and we can get this by calling dot F fully connectedã€‚ And then the input featuresã€‚ So this is the number of input features for the last layer that we needã€‚And then let's create a new layer and assign it to the last layerã€‚ So let's say modelã€‚Dot F C equalsã€‚

 And now we give it a new fully connected layer N N dot linearã€‚ And this gets the number of input features that we haveã€‚And then as new output featuresã€‚ number of outputsï¼Œ we have two because we have two classes nowã€‚And now we send our model to the deviceã€‚ If we have GP supportã€‚

 So we created our device in the beginningï¼Œ as alwaysã€‚ So this is Kuda or simply CPUã€‚And now that we have our new modelï¼Œ we canï¼Œ againï¼Œ as alwaysï¼Œ define our loss and optimize usã€‚ So we say criterionã€‚Equals Nï¼Œ N dot cross entropisã€‚And then let's say the optr equalsã€‚ This is from the optimization moduleï¼Œ Opim dot S GDï¼Œ stochastic gradient descentã€‚

 which has to optimize the model parametersã€‚ And we have to specify the learning rate equalsã€‚ let's say0001ã€‚Andã€‚Nowï¼Œ as a new thingï¼Œ let's use a scheduleulaã€‚ This will update the learning rateã€‚ soã€‚For thisï¼Œ we can say we can create this by saying it's called a step L R scheduleularã€‚Equalsã€‚ and the L R scheduletula is available also in the torch optimization moduleã€‚

 So we already imported thisã€‚ And then we can say L R scheduletula dot step L Rã€‚ And then here we have to give it the optimizerã€‚ So here we say optimizerã€‚And then we say step sizeã€‚ step size equals 7ã€‚ And then we say gamma equals let's sayï¼Œ01ã€‚ This means that every 7 epochsã€‚ Our learning rate is multiplied by this valueã€‚ So every 7 epochsã€‚

 Our learning rate has only 10 is now only updated to 10 per centã€‚Soï¼Œ yeahã€‚ this is how we use a scheduletulaã€‚ And then typically what we want to do is in our loopã€‚ in our loop over the epochã€‚ So for epoch in rangeï¼Œ let's sayï¼Œ100ã€‚ And then typically here we use the trainingï¼Œ where we also doã€‚Theã€‚The optimizer dot stepã€‚

 optimizer dot stepã€‚ Then we want to evaluate itï¼Œ evaluate itã€‚ And then we also have to call scheduler step scheduler stepã€‚ So this is how we use a schedulerã€‚ Please have a look at the whole loop here yourselfã€‚So yeahã€‚ now we set up the scheduleular and let'sã€‚Call the training functionsã€‚

 So here we say model equals and then train modelã€‚ So this is the function that I createdã€‚ And then I have to pass the modelï¼Œ the criterionã€‚The optimizerã€‚ the scheduler and also the number of epochsã€‚ So nu epochsï¼Œ let's say 20ã€‚Andã€‚Yeahã€‚ so this is how we useï¼Œ how we can use transfer learningã€‚ So in this caseã€‚

 we use a technique that is called fine tuningã€‚Because hereï¼Œ weã€‚Train the whole model againã€‚ but only a little bitã€‚ So we fine tune all the weights based on the new dataã€‚And with the new last layerã€‚ So this is one optionã€‚ And the second one is for thisã€‚ I copy and paste the same thingã€‚Andã€‚Let's see where does it startã€‚ So hereã€‚

 and then as a second optionï¼Œ what we can do is we can freeze all theã€‚All the layers in the beginning and only train the very last layerã€‚ So for thisã€‚ we have to loop over all the parameters here after we got our modelã€‚ So we say 4ã€‚Pm in modelã€‚Dot parametersã€‚And then we can set there requires gra attribute to fall so we can say paraã€‚

Dot requires grã€‚And then sayï¼Œ weï¼Œ sorryï¼Œ Do requires graã€‚Requires gra equals falseã€‚ Now we have itã€‚ and this will freeze all the layers in the beginningã€‚ And now we set up the new last layerã€‚ We create a new layer hereã€‚ and by defaultï¼Œ this has requires gra equals trueã€‚And then againã€‚ we set up the loss and optimizer and the schedule in this caseã€‚

 And then we do the training function againã€‚ And so yeahï¼Œ so this is even more fasterã€‚ And let's run this and then have a look at both the evaluationsã€‚ And I also print out the time that it tookã€‚Soã€‚Yeahï¼Œ let's save thisã€‚ and let's run this by saying Python transfer dot piã€‚Andã€‚This might firstã€‚

 it will download all the imagesï¼Œ and this might take a couple of seconds because I don't have GP support here on my MacBookã€‚ So I will skip thisï¼Œ and then I will see you in a secondã€‚All rightï¼Œ Soaï¼Œ I am backã€‚ So this took super long on my computerï¼Œ so I resetã€‚The number of epochs to just2 in this exampleã€‚ So let's have a look at the resultsã€‚ So after only two epochsã€‚

 so this is the first training where we did the fine tuning of the wholeã€‚Modelã€‚ so this took three and a half minutesã€‚ and the best accuracy now is ã€‚92ã€‚ So 92%ã€‚And then this is the second training where we only trained the last layerã€‚ So this took only one and a half minutes approximatelyã€‚

 and yeah accuracy is also is already over 80%ã€‚ So of courseã€‚ it's not as good as in when we train the whole trainingï¼Œ but still pretty good for only two epochsã€‚ And now let's imagine if we set the number of epochs even higherã€‚ So yeahã€‚ this is why transfer learning is so powerful because we have a pretrained modelã€‚

 and then we only fine tune it a little bit and do a completely new task and then achieve pretty good results tooã€‚ So yeahï¼Œ so now I hope you understood how transfer learning can be applied in Pythtorchã€‚ if you enjoyed the tutorialã€‚ Please subscribe to the channel and see you next time byã€‚

![](img/96844ee58fe06a9d803e6f751f732b19_3.png)