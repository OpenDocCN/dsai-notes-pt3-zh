# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂÆòÊñπÊïôÁ®ãÊù•Âï¶ÔºÅ5‰Ωç Hugging Face Â∑•Á®ãÂ∏àÂ∏¶‰Ω†‰∫ÜËß£ Transformers ÂéüÁêÜÁªÜËäÇÂèäNLP‰ªªÂä°Â∫îÁî®ÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P35ÔºöL6.3- SylvainÁöÑÂú®Á∫øÁõ¥Êí≠ËÆ≤Ëß£ - ShowMeAI - BV1Jm4y1X7UL

Yeah„ÄÇWelcome to the live session where we'll go over chapter2 of the Iing this course„ÄÇ I'm joined by Lewis who on the chat and we' is going to answer all your question quicker than I will and don't hesitate to ask all your questions because I'm going to read themlo and answer them on the live stream that's like the main advantage of following this live stream instead of just watching the course by yourself„ÄÇ

So over inside this chapter2Ôºå we all look at the pipeline object that we used at length during chapter 1 on all NLP tasks„ÄÇAnd we'll see exactly how it worksÔºå we'll see how it loads a model„ÄÇ how it preprocesses the inputs with a tokenizerÔºå and then how it processes with output to get the predictions and probabilities that we got during chapterpt1„ÄÇSo we'll watch a few videos that answer all the questions that you have and we'll do more life coding than in chapter 1 because„ÄÇ

Because chapter 1 was just a general introductionction and varies a lot of more code in chapter 2„ÄÇSo as an introductionÔºå as you may knowÔºå again face is mainly known for its Transformers library„ÄÇ which is a library containing a lot of transformers model„ÄÇ and it provides an easy API to download pre trade model and to use the different architectures„ÄÇ

 I think there are more than 60 architectures now available into the library„ÄÇAnd„ÄÇIt all exposes unified API but inside and provides you with either a torch module or a tons of flu care model that you can use by yourself or that you can„ÄÇA train with the API that the library also provides„ÄÇAnd the goal is the views flexibility and simplicity and the library doesn't contain any abstraction at all„ÄÇ

 it's not a library composed of building blocksÔºå every model„ÄÇ every one of the 60 architectures I was talking about is completely defined in its own modeling files so we can have a quick look for instance at the modeling B file which contains also the code of the bird model inside the library as if you look at the input you see like there are just torch imports and then some internal classes that this uses but we share between all models which are mainly the output types that we use will see exactly what the outputs are a little bit later when we could but there is no other inputs where is not like an attention block that we reuse the Que model the attention block for the B model is defined inside this modeling file so we have the B buildingsddings you have the B self attention etca„ÄÇ

 etc„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_1.png)

The idea is that if you want to play around with the model and change a line of code inside the model you won't have to learn 50 different files„ÄÇ it's not subclassing somethingÔºå that subclassing somethingÔºå that' subclass somethingÔºå everything„ÄÇ ohÔºå I'm guessing I'm just realizing I'm hiding a bits okayÔºå I guess I'm just going to scroll faster„ÄÇYou have everything in that modeling belt file and you can play around and modify everything you want if you want to experiment with it„ÄÇ

 and that's really one of the strengths of the transformformer library„ÄÇ one of the features that our users have said very accurate„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_3.png)

Which is why I wanted to show it to you briefly„ÄÇAnd„ÄÇSo yeah„ÄÇ we'll see how the pipeline API loads that B model„ÄÇ it was actually a digitaltill B model that we use in chapter 1 and the corresponding dokenizer„ÄÇAnd how to do everything that this pipeline function was doing by hand so that you can tweak any of those steps if you need to on your own tasks„ÄÇ

So let's begin with the first section and first video we're going to watch this introductory video that' is going to present what's happening behind the pipeline I'm just not going to stream them from YouTube because that's making this the live stream like a lot I'm just I have all the videos look at it So if you give me just one minute„ÄÇ

I'm going to extract it„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_5.png)

And played from my computer„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_7.png)

Yeah„ÄÇWhat happens inside the pipeline functionÔºüIn this video„ÄÇ we'll look at what actually happens when we use the pipeline function of the transformforms library„ÄÇNow specificallyÔºå well look at the sentiment analysis pipeline and then we went from the two following sentences to the positive and negative labels with respective scores„ÄÇAs we've seen in the pipeline presentationÔºå there are three stages in the pipeline„ÄÇFirst„ÄÇ

 we convert the redex to numbers the model can make signs of using a tokenizer„ÄÇThen those numbers goes through the modelÔºå which outputs loads„ÄÇFinally„ÄÇ the best processing steps transform those delegates into labels and skull„ÄÇLet's look in details at those three steps and how to replicate them using the Transform library„ÄÇ

 beginning with the first stage tokenization„ÄÇSo to process has several steps first„ÄÇ the text is split into small chunks called tokens„ÄÇThey can be words„ÄÇ part of words or punctuation symbolsÔºå then the tokensizer will add some special tokens in the model expect„ÄÇHereÔºå the middle used expect a CLS token at the beginning and a S token at the end of the sentence to classify„ÄÇ

LastlyÔºå the token isone patches each token to its unique ID in the vocabulary of the pro model„ÄÇTo load the tokenizerÔºå the transformformers library provides the autotokenizer API„ÄÇThe most important method of this class is from Pretrained„ÄÇ which will download and cache the configuration and the vocabulary associated to a given checkpoint„ÄÇ

HereÔºå the checkpoint used by default for the supplement analysis pipeline is distillbel baseline case5 tune SS2 English„ÄÇ which is a bit of a mouthful„ÄÇWe instance to tookken as with a checkpoint„ÄÇ and feed it to the two sentences„ÄÇSince the two sentences are not the same size„ÄÇ well need to pad the shed oneand to be able to build an array„ÄÇ

This is done by the tokenizer with the option padding equal true„ÄÇWith truation equal true„ÄÇ we ensure that any sentence longer and the maximum the model can handle is truncated„ÄÇLastly„ÄÇ the return tensil option gal the tokenizer to return the bytch tensil„ÄÇLooking as a result„ÄÇ we see we have a dictionary with two keysÔºå input ID contains the ideas of both sentences with zero where the padding is applied„ÄÇ

The second key attention mask indicates where petting has been applied„ÄÇ so the model does not pay attention to it„ÄÇThis is always is inside the took step„ÄÇNow let's have a look at the second step„ÄÇËøô‰∏çÊáÇ„ÄÇAs for tokenizer„ÄÇ or is a nottomod API over from pretrained methodÔºå it will download lu and cache the configuration of the model as well as the pretrain weight„ÄÇ

HoweverÔºå the autotomodl API will only instantiate the body of the model„ÄÇ that is the part of the model that is left once the pro traininging head is removed„ÄÇIt will output a high dimensional tensorÔºå that is a representation of the sentence's past„ÄÇ but which is not directly useful for a classification problem„ÄÇHereÔºå the tensor has two sentences„ÄÇ

 each of 16 tokensÔºå and the last dimension is the Indian size of our modelÔºå 768„ÄÇTo get an output link to our classification problem„ÄÇ we need to use the Automodal for sequence classificationification class„ÄÇIt works exactly as zero to model classÔºå except by 12 built a model with a classification head„ÄÇüòä„ÄÇ

Praise one auto class for each common NLP task in the transformformers library„ÄÇHere„ÄÇ after giving our models the two sentencesÔºå we get a tensor of size 2 by 2„ÄÇ one result for each sentence and for each possible level„ÄÇThose outputs are not probabilities yet„ÄÇ we can see they don't sum to one„ÄÇThis is because each model of the transformformer's library returns look it„ÄÇ

To make sense of those looksÔºå we need to dig into the third and last step of the pipeline plus processing„ÄÇTo conduct Lo into probabilitiesÔºå we need to apply a softmax layers to them„ÄÇAs we can see„ÄÇ this transforms them into positive number that's a up to1„ÄÇThe last step is to know which of those corresponds to the positive or the negative label„ÄÇ

 this is given by the ID2lipal field of the model conflict„ÄÇThe first proba is index0„ÄÇ correspond to the negative level and the second index1 correspond to the positive level„ÄÇThis is how our classifier built with the pipeline function peaked with labels and compute those scores„ÄÇüòäÔºåNow that you know how each step worksÔºå you can easily tweak them to your needs„ÄÇ



![](img/0e882caa799deede0a88d67b58737bfe_9.png)

ÂóØ„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_11.png)

Yeah„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_13.png)

ÂóØÂØπ„ÄÇÂìéÂëÄ„ÄÇBack„ÄÇÂóØ„ÄÇI think that something„ÄÇWronong with webcam„ÄÇ Let me just double double check„ÄÇOkay„ÄÇ somehow my head disappeared„ÄÇ I don't know why exactly„ÄÇ if you can see it„ÄÇ feel free to say it in the chat„ÄÇIn the meantimeÔºå there is a question„ÄÇ what does the triple that signify in from that activations import act to function in the PY files shownoneÔºü

Very good questionÔºå so this is standard Python if you're building the package„ÄÇIn Python„ÄÇ and you're trying to import things„ÄÇYou have several levels„ÄÇ so it's because of the structure of the transformer repo„ÄÇ let me try to pull it back and quit my VS code„ÄÇ

![](img/0e882caa799deede0a88d67b58737bfe_15.png)

But„ÄÇRise either Here we are„ÄÇ So you as a transformers folder and then inside that transformer models„ÄÇ you have a model subfold and then a bird subfold and when the modeling file is here And since we've organized the good that way to avoid everything all the files directly in the transformers folder because as I said we have like 60 different architecture„ÄÇ

 So that would be a little of files there are organized like this And when you're trying to import when you say from dot it's to go back inside the structure„ÄÇ So from import blah blah blah would be in the bird foldert is in the model folder and then thet gets back to the transformers folder„ÄÇ

 So it's just a way to come back to the root of the post directory„ÄÇÂóØ„ÄÇI I didn't see any of questions„ÄÇ but don't hesitate to ask at any time in the chat your questions and I'll answer as best as I can and as I expected my webca is not showing anymore and I have no idea why because„ÄÇ



![](img/0e882caa799deede0a88d67b58737bfe_17.png)

I just„ÄÇS the video and it' is not working„ÄÇLet me just try something„ÄÇHere we are„ÄÇ sorry about that hair too just shut it down and restart started it„ÄÇSo we're good to continue our expression behind the pipeline function and so we'll just take a little bit at the code that we just doing in the video I'm not going to show it from the section side but remember that in most sections in all the sections that have could you have an open coll button at the top like this which I opened a little bit earlier just to execute the first cell which is installing everything and can take a bit of time and then the second cell which is download the model I executed it already so that we have the result instantaneously here and we are ready to look at the rest of the notebook so like in the video let me move myself on the other side of the screen because the code„ÄÇ

Is mostly on the left„ÄÇAnd„ÄÇSo like in the videoÔºå we all look exactly at the code that is executed when we try to use the sentiment analysis pipeline on two sentences like that„ÄÇAnd see how we get to the results on those labels„ÄÇ So as we've seen in the video„ÄÇ

 the first step the prepossessing is done by a tokenizer so we'll look into the tokens in detail a little bit further ahead but for not just you just need to know that the tokenizer texts the input text so those two sentences I've been waiting for the U face calls my own life and I edit it so much it's going to take those two sentences and convert that them into numbers because the model doesn't understand text it understand numbers So basically behind the scenes it's going to split that text into small chunks that we call token and so will tokenizer and then associate each of us token to a unique ID which is the numbers that were gonna to see„ÄÇ

To load the tokenizerÔºå we need to know the identifier of the tokenizer„ÄÇSo here this is the identifier of the model that is used by sentiment analysis pipeline by default„ÄÇ and then we just call autotokenize order from pretrained and„ÄÇIt's gonna download if you I've the files already downloaded because I executed this first„ÄÇ

 but if it's the first time you're executing thisÔºå it's going to download the files of the tokenizer and in particular the vocabulary which contains the mapping token to unique ID and instantiate it and once you have that objective available„ÄÇ you can fit it your input directly like this so raw input inside the tokenizer and we all exactly explain what for spaing and tru mean„ÄÇ

A little bit for and we tell it to return tensilors and since we are using Pytoch here„ÄÇ we tell it to returns Pytoch tensor with viPT„ÄÇYou can also say TF of tons of litensils„ÄÇ N for N arrays or Fls for flexÔºå which isÔºå I guess for FlÔºå it's also an n arrays„ÄÇAnd if we execute itÔºå we can see that we get an output which is a dictionary with input ID and attention mask we'll explain what the attention mask means a little bit for in the course„ÄÇ

In this last sessionÔºå sorryÔºå and the input I are the unique numbers I was talking about„ÄÇ So it's converted that text into small chunks of petaccle tokens and each of us token have been associated to unique number And once we are that we can„ÄÇUse as the model„ÄÇOn this input„ÄÇJust if we want to have a look„ÄÇBack„ÄÇWhy are you annoying me if we want to have a look back at how those Is correspond to the text that we had at the beginning„ÄÇ

 we can use the decocode method of autokenizerÔºå So I type tokenizer the Decode„ÄÇAnd then„ÄÇ I'm gonna to„ÄÇTake my inputsÔºå grab the key input I„ÄÇOhello„ÄÇD di„ÄÇLike thisÔºå so this is a tensor„ÄÇ so I'm going to convert it to a list„ÄÇAnd take this is a list of lists because I have two the two sentences in my sentence in my tensor sorry„ÄÇ so I'm just taking the first oneÔºå for instanceÔºå and if I that execute that sorry I can see my original text which has been a bit prepossessed„ÄÇ

 there is no capital anymore for the I for instance„ÄÇAnd we can also see that the tokenizer added something at the beginning and something at the end„ÄÇ This is perfectly logicalÔºå as the tokenizer is adding those„ÄÇThe token is always adding for tokens because the model expects them„ÄÇ

So I'm just going to pause here for questions before I go into the model part of the code„ÄÇWhat does in Ututuanizer meanÔºüAnd what type of tokenazizer is used„ÄÇ So the auto for auto tokenizer mean in the auto tokenazizer class means that you can„ÄÇLoad any tokenizer corresponding to any architecture using that API„ÄÇ So for instance„ÄÇ

 here our model is a distilled bird model„ÄÇ So the tokenizer is going to be a distilled bird tokenizer can double check that by just adding a console So if I type„ÄÇDkenizer„ÄÇAnd print the output„ÄÇIt's going to tell me it's a pretend tokener fast„ÄÇ which is not super useful„ÄÇM„ÄÇBut the representation is not purposeable„ÄÇ but the type should be a distill belt tokenizer fast„ÄÇÂóØ„ÄÇAnd if I had used a bird checkpoint„ÄÇ

 I would have a bird tokenizer fastÔºå if I had usedÔºå I don't know about checkpoint„ÄÇ I would have a badt tokenizer fastÔºå etc cea etcettera„ÄÇ so the U in autotokenazizer means that that class is going to pick the right so class of tokenazizer so when corresponding to the model used by your checkpoint automatically so your code here tokenazizer equals auto tokenizer that from between checkpoint is going to work from for any checkpoint on the model whatever the class of your model as long as it's a class of model that has been implemented in transformers it's going to work on it„ÄÇ

Whereas if you were using here Distill B tokenizer„ÄÇ you would have to change the class used if you change the type of checkpointÔºå for instance„ÄÇ if you would use the belt modelÔºå you would have to change it to bird tokenizer if you were using a GT2 checkpoint„ÄÇ you would need to change it to GT2 tokenizerÔºå etcaÔºå etca„ÄÇÂóØ„ÄÇThe second question was„ÄÇ

 will I be broken into I and M and the answer to that is yesÔºå we'll see a little bit„ÄÇ I'm just going to push an answer more completely a little bit later when we see exactly the different type of tokenizer„ÄÇAnd what is the difference between a fast tokenizer and a standard tokenizer Very good question So we usually have for each model two tokens one that is called slow or standout and the ver that is called fast the fast tokenizer is backed by the cookingface tokenizer library which is not written in Python but in rust and which in turn because you may have known that Python is a slow language and so if you do the wall tokenization in pure Python it can be a bit slow when you have lots of lots of text whereas the tokenizer fast backed by rust is going to be extremely fast„ÄÇ

So the main differenceÔºå that's the main difference between the two of them if you are just processing one text„ÄÇ you won't see any differenceÔºå but if you're processing 10000 of texts at the same time as the tokenator fast is going to be much faster than the python tokenizer„ÄÇ

And that's also the question we have for the moment„ÄÇ I'm going to continue and look at the model„ÄÇSo the same way we have nout to kal APIÔºå we have a newto modelal API and again the auto in automod API means that this class is going to pick the right subclass belt model„ÄÇ

 GT2 modelÔºå distill belt modelÔºå etc depending on the checkpoint that it receives so here since it's a dist belt checkpoint it's gonna„ÄÇI would put distill button modelÔºå I can just show you here„ÄÇMa that modelÔºå and it should„ÄÇI have a nice wrap we can see hereÔºå this steel bird bottle„ÄÇÂóØ„ÄÇI'm gonna to remove that cell because it's a bit annoying„ÄÇThank you for that„ÄÇÂóØ„ÄÇSoÔºå that model„ÄÇIs„ÄÇ

Coming with againÔºå we didn't download any file here because when we executed the pipeline instruction at the very beginning„ÄÇ we downloaded everything we needed„ÄÇ so the model is already cached„ÄÇ that's why we don't see any download here and we get the warning which I'm going explain just after this„ÄÇ which is because„ÄÇThis automod class is going to give us the base pretrained model„ÄÇ

 and this space pretrained model doesn't output classification of a sentence between positive and negative„ÄÇ it outputs the hidden state of the pretrained modelÔºå which is a dimension of 168„ÄÇSo that's why here we have a warning because that model auto model is missing a classification head and as the warning was saying„ÄÇ some weights of the checkpoint were not used when initializing the model„ÄÇ

 specifically classifier biasÔºå classifier weightsÔºå etca„ÄÇ which are all the weights of the classifier head„ÄÇSo Automod is something that can be useful if you just want the tensor or hidden features outputted by your pre model„ÄÇ but here we want to classify our sentences between positive and negative so we need a model with a classification head and that's given to us by the automod for sequence classification class„ÄÇAnd this oneÔºå when I execute the cell is not going to output any warninging because all the weights are going to be used and there is not going to be anything problematic when you dig the checkpoint„ÄÇ

And if we„ÄÇG our input to thatmo and look at the shape and we can see that it's going to be a tons of size 2 by 2„ÄÇOne little comment about the output is that the output of transformer models„ÄÇ so that was the thing that were imported at the beginning of our modeling file„ÄÇ if you remember like we had a lot of import from the the dot model outputs„ÄÇMoule„ÄÇ

 So those outputs are a bit of an hybrid between the name to and the dictionary so you can access everything either by doing dot like this So output dot logits„ÄÇ you can also access things„ÄÇBy„ÄÇÂóØ„ÄÇAsking„ÄÇWith the key„ÄÇSoÔºå outputs„ÄÇAnd when we ask for the logicit key„ÄÇ

 like it would if like a dictionaryÔºå which works as well„ÄÇÂóØ„ÄÇAnd if we just ask for the base representationÔºå we can see it's a sequence classifier output which contain Lu sheet and thistensor„ÄÇ so here it contains only one thing but most of the transformer model can return lots of things as output„ÄÇ for instance if I added labels here it would return a loss„ÄÇ

 we could also ask the model to return all the hiddenden states or all the attentions results and in which case our output is becoming a bit quoted„ÄÇ which is why it's organized as this spec class that behaves likeedt and an imable„ÄÇSo those legits„ÄÇ because those names of what we get are numbers which appear a bit random„ÄÇ they don't really look like probabilities and we will need to do one last step of post processing as we saw in the video to convert them into probabilities„ÄÇ

 that is apply the surfmax„ÄÇAnd if we just import from torturer soft max function plate„ÄÇ we can see that now we get the exact same score that we had at the beginning„ÄÇ so for instance for the second sentence like we have that 0„ÄÇ99 so 99„ÄÇ95% and if we look back at the result of the pipeline we can see that we had 99„ÄÇ

945 yeah so the exact same scores„ÄÇAnd to know which which one was the negative and which one was positively label„ÄÇThe pipeline is using that field from the model configurationÔºå so for each model„ÄÇ the configuration file associated to it is accessible via the configure attributebute and the ID2 level field contains a correspondence between integers and levels„ÄÇSo let's see if we have any questions„ÄÇÂìéÂëÄ„ÄÇFirst question is„ÄÇ

 is there any reason to use the standard Python to canÔºü

I work at a game phase so I'm a little bit biased and I'm going to say no you have to using the fast tokenser is always going be better„ÄÇ so it's going to be even if you don't have many many text„ÄÇ it's going to be at the same speed at the very minimum maybe faster than the Python tokenazer standardt Python tokenneer„ÄÇ but also it has many more featuresÔºå so we'll look at them in the second part of the course mainly but it has features that have been designed specifically for tasks like like token classification or question answering that allow you to know for instance„ÄÇ

 if from which word the token comes from or to exactly which span of text both tokens represent in the original text which fit that are a little bit not a little bit way out together to get with the slow tokener„ÄÇAnother question is„ÄÇAre there any similar tutorials or resources for sentiment analysis for multilabels that or regression tasks for sequences not right now which is that's a very good question so not right now and we should definitely work on that so the main thing you would have to change is the processing at the end instead of applying softmax you would apply for a regression you wouldn't apply anything I guess and then for multiplelabel so multiple label multiple possible labels for each of your sentence you would apply probably a stingoid to your result„ÄÇ

And then last question is the first two input IDÔºå the two lines are the same„ÄÇ but the words are not why is that so we have to look back at the decoding because the two words are indeed the same so the sentence here„ÄÇBegins with 101 and 101 which is the idea that CSO„ÄÇ so that's why you have the first two input id that the same and then the 1045 correspond to the I because the two sentences begin with I and then it starts being different because you have the ver or8 for the two sentences„ÄÇ

And another question would it be possible to do a video explaining the code structure of the library and the ID behind it so that's to maybe make it easier to contribute very„ÄÇ very good question and it's actually scheduled for the last part of the course in the part of the course we have a chapter that's going to be dedicated to how to contribute to againface libraries and particular the transformformers library and then well have video explaining the good structure of all the libraries of the again phase ecosystem„ÄÇ

AgainÔºå don't hesitate to ask any questions I'm going to pose regularly to answer it„ÄÇSo that's pretty much everything that was behind the pipeline and we've seen it in detail in the code„ÄÇ So now let's have a look at the main object inside the pipelineÔºå which is the model„ÄÇSo again„ÄÇ we have a short video that I'm going to show from my computer and then we'll look at the code in detail and if you have questions„ÄÇ

 I can answer them and live code with you„ÄÇLet me just grab the video„ÄÇÂëÉ„ÄÇWhich of course„ÄÇ I can't find easily otherwiseÔºå but would be too easy„ÄÇ Why did it disappear„ÄÇAnd one was ver poormiss„ÄÇ I just moved it„ÄÇRecently„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_19.png)

And„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_21.png)

How to instantiate a transforms model„ÄÇIn this video„ÄÇ we'll look at how we can create and use the model from the Transformers library„ÄÇAs we' seen before„ÄÇ the Automodal class allows you to instantiate a portrayed model from any checkpoint on the I face app„ÄÇIt will pick the right model class from the library to instant shade the proper architecture and load the weights of the preed model inside„ÄÇ

As we can seeÔºå when given a bird checkpointÔºå we end up with a bird model and similarly for GPT2 or part„ÄÇBeyond the scenesÔºå this APII can take the name of a checkpoint on the U„ÄÇ in which case it will download and cache the configuration file as well as the model weights file„ÄÇYou can also specify the path to a local folder that contains a valid configuration file and a model of waste file„ÄÇ

To instant shade the between modelÔºå the Automodal API will first open the configuration file to look at the configuration class that should be used„ÄÇThe configuration class depends on the type of the modelÔºå BÔºå GPPT2Ôºå or BtÔºå for instance„ÄÇOnce it has a proper configuration classÔºå it can instantiate that configuration„ÄÇ which is a blueprint to know how to create the model„ÄÇ

It also uses this configuration class to find the proper model class„ÄÇ which is when combined with the root configuration to load the model„ÄÇThis model is not yet a portraytrain modelÔºå as it has just been initialized with random weights„ÄÇThe last step is to load the weight from the model file inside this model„ÄÇ

To easily load the configuration of a model from any checkpoint or a folder containing the configuration file„ÄÇ we can use the autoconfigug class„ÄÇLike the Automod class„ÄÇ it will pick the right configuration class from the library„ÄÇWe can also use a specific class corresponding to a checkpoint„ÄÇ

 but well need to change your code each time we want to try a different model architecture„ÄÇAs we said beforeÔºå the configuration of a model is a blueprint that contains all the information necessary to create the model architecture„ÄÇFor instanceÔºå the bird model associated with a birth based case checkpoint as 12 layers„ÄÇ the hidden side of 768„ÄÇAnd the vocabulary size of 28996„ÄÇOnce we add a configuration„ÄÇ

 we can create a model that has the same architecture as a checkpointÔºå but is randomly initialized„ÄÇWe can vet training it from scratch like any by doch model„ÄÇWe can also change any part of the configuration by using keyword arguments„ÄÇSo sequence one sniet of codeÔºå instant sheets a randomly initialized B model with 10 layers instead of 12„ÄÇ

Saving a model once its trend off fine is very easy„ÄÇWe just have to use the safe between method„ÄÇHere„ÄÇ the model will be saved in a folder named My belt model inside the current working directory„ÄÇSuch a model can then be re using the from between method„ÄÇTo learn how to easily approach this model to the webÔºå check out the push to video„ÄÇ



![](img/0e882caa799deede0a88d67b58737bfe_23.png)

ÂóØ„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_25.png)

OkayÔºå so let's see if we have any questionsÔºå not just yet„ÄÇ Don't hesitate to ask any question in the chat and I'll answer them„ÄÇRegularlyÔºå and for this„ÄÇ let's open the code app„ÄÇAnd look a little bit at the code behind the Automod API„ÄÇAnd in particular„ÄÇ we'll seeÔºå for instanceÔºå I told you a little bit earlier that our model could return more venture surligit and it could return„ÄÇ

 for instanceÔºå all the hidden states or things like that and we'll see how to do that just here„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_27.png)

Yeses„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_29.png)

So„ÄÇIf you had any questionsÔºå that would be an need all time because I didn't execute this notebook in advance„ÄÇ so we need to wait for it to install everything„ÄÇOkayÔºå that didn't think so„ÄÇÂóØ„ÄÇSo„ÄÇTo create a random model that looks exactly like the per model„ÄÇ we can just instantiate the default configuration and use that configuration inside the model„ÄÇ

Like we saw in the videoÔºå the config contains lots of fields that are related to what's happening inside the model„ÄÇ so for instance we have the hidden size configured„ÄÇ we have the number of words that our model can takenÔºå we have the vocabulary size„ÄÇThe mobile type„ÄÇ which is builtÔºå the activation it' usedÔºå which is KiuÔºå etcteraÔºå etca„ÄÇÂóØ„ÄÇSoÔºå that model„ÄÇ

Using just random usage the config is going to be randomly initialized and there is nothing to download there if we„ÄÇWant to use a pretrained model we have to use the from pretrained method„ÄÇ which is going to download the exact config and then the model weights and as we saw in the video„ÄÇ it's going to use the config to first instantiate randomly initialized model and then load the weights from that checkpoint inside the model we have„ÄÇ

And if we„ÄÇWant to change anything„ÄÇ„ÅÑ„ÅÆ„ÇÇ„ÅÆ„ÄÇMore specifically in its configuration„ÄÇ we can say it in several places„ÄÇ SoÔºå for instanceÔºå we can start with„ÄÇÂëÉ„ÄÇA config that is exactly like birds„ÄÇSo con to convicted from pre traineded„ÄÇB pesquiist„ÄÇWhich is gonna downloadÔºå and it's already downloaded from here„ÄÇ

Where you mean the two configurefig is not defined„ÄÇ Oh yeahÔºå I have only use B config„ÄÇ So let's continue with that„ÄÇSo„ÄÇB con from between„ÄÇ which is gonna reuse the con that was download here„ÄÇ So this is the configuration of the per model„ÄÇ And if we want to change anything inside itÔºå we saw the videoÔºå for instance„ÄÇ

 to change certain number of hidden layers„ÄÇ but let's say that„ÄÇI want to change the fact that I want my model to return all the hidden states„ÄÇWhich I would say with outputs in states equal to„ÄÇSo I can do this in the config and then instant shape my model with model equal bad model config„ÄÇ I can also directly change this when I do„ÄÇThat model that from betweentrained here„ÄÇ

So since if I were to change here the number of hidden layers it wouldn't work anymore„ÄÇ the command would fail because I would then try to load a checkpoints that has been defined with 12 layers inside the model with 10 layers so bytoch would complete I mean it would probably work but I would have a warning with like the widths not being used and the model would probably not get super useful results but for something like outputed in states which doesn't really change the way the model was pretrained this is going to work super nicely„ÄÇ

And if I try to take inputs„ÄÇSo let's define some random input and then pass it to a tokenizer„ÄÇ So I'll have to„ÄÇUse the part toagonizer and then instantiate it with a form betweentrain method„ÄÇShould ohÔºå yesÔºå it's finding it here„ÄÇShould have executeded the elder or or„ÄÇSo if I'm creating a do like this„ÄÇAnd thenÔºå applying it„ÄÇÂëÉ„ÄÇTo my inputs„ÄÇüéºReturn a tensor„ÄÇ

 I don't need to put the padding and location that we saw beforeÔºå because there is only one sentence„ÄÇ I'll see why a little bit earlier„ÄÇ So once I have done thatÔºå I can look at my„ÄÇOutputs„ÄÇAnd it should have„ÄÇNowÔºå two keys„ÄÇStill one key„ÄÇYeahWith a little bit more because the bird model has a pull output you on top of the luggit but oh and it's not luets anymore sorry it's less hidden states because this is not the classification model it's a base model I used a be model which is the same as using automod not a be model for second classification so I get a last hidden states instead of look key the puller output is specific to B so it's always as that and then I can say have a last key with hidden states and a list of all the turnsult which correspond to all the hidden states of my model so this is how you change the configuration of your model on the fly either insides when we create a config if you are trying to initialize a randomly initialized model or to the form pretrain letter if you are trying to use„ÄÇ

A pre model in particularÔºå if you're using a classification modelÔºå for instance„ÄÇ a sequence classification modelÔºå you can specify the most important argument is going to be nu levels„ÄÇBecause when you add your classification headÔºå you want to control how many outputs that classification head hass„ÄÇ so you would do that with a new labels argument„ÄÇSo that model„ÄÇ

 And then once you finish training or a tuning your model„ÄÇ you can use safe pretrain to save it on the floor on your„ÄÇOn your hard drive and you can use Pushtb„ÄÇ which we just released today actually„ÄÇSo on your model to directly upload your model on the Higing face hub so that anyone in the world can use it„ÄÇDon't see any questions againÔºå don't hesitate to ask any questions„ÄÇ

 I'm going to answer them regularly„ÄÇAnd so this is all we have to„ÄÇ this is all we various intersection for models and then let's look at tokenizer„ÄÇ which is responsible for prepoing the input I'm going to move myselfÔºå oh not to screen„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_31.png)

![](img/0e882caa799deede0a88d67b58737bfe_32.png)

Come back„ÄÇGoing to move myself„ÄÇBack on the left„ÄÇBecause„ÄÇAnd we„ÄÇ we look at this section here and look that the code inside the code app„ÄÇÂóØ„ÄÇSo tokenizer„ÄÇOh yeah let's look at the video with tokens of first and then I'll comment everything that's happening in this section„ÄÇT can as introduction video„ÄÇ

![](img/0e882caa799deede0a88d67b58737bfe_34.png)

![](img/0e882caa799deede0a88d67b58737bfe_35.png)

Yeah„ÄÇIn the next few minutesÔºå we'll take a look at the tokens„ÄÇüòäÔºåIn natural language processing„ÄÇ most of the data that we handle consists of raw textÔºõ however„ÄÇ machine learning models cannot read or understand text in its raw form„ÄÇThey can only work with numbers„ÄÇSo the tokenizers objective will be to translate the text into numbers„ÄÇ

There are several possible approaches to this conversion„ÄÇ and the objective is to find the most meaningful representation„ÄÇüòä„ÄÇWe'll take a look at three distinct organization algorithmsÔºå we compare them one to one„ÄÇ so we recommend you take a look at the videos in the following orderÔºå first word based„ÄÇ

 followed by character based and finally sub word based„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_37.png)

![](img/0e882caa799deede0a88d67b58737bfe_38.png)

Yeah„ÄÇSo we won't look at the video„ÄÇ ActuallyÔºå we're gonna look directly at the text inside inside the the„ÄÇThe course and I'll comment because we won't have time to watch all those videos in the slide we„ÄÇSo world based tokens so you can look at the video in your free time but we're gonna explain it a little bit more in depth with what I' to do„ÄÇ but the world based tokenizer is just going to split your sentence by word so the easiest way to do that is to take all the spaces and then split your text onto those spaces more advanced would be to include some walls to split and punctuation so for instance the exclam mark separated it from tokenization or here let's split it between let and aworth as„ÄÇ

So we can see this on this example with Chi and Sun weather Preter„ÄÇ which is separated into five words here„ÄÇSo the world organizers are„ÄÇWere used a lot before transformersÔºå mostly the advantage is that„ÄÇYou split naturally your text onto the spaces and punctuation see disadvantage is that you end up with pretty large vocabularies because there there are lots of different worlds in English and every time someone makes a typo in some world you end up with in a new world in your vocabulary„ÄÇ

So each word gets assigned in ID starting from0 going up to the size of the vocabulary and then since we can't guarantee that the user is never going to make a tape or anything„ÄÇ there is a special ruleÔºå if we encounter a token that doesn't exist in the vocabulary„ÄÇ

 it's usually replaced by something called the unknown token which is usually something that looks like that and between brackets„ÄÇSo this is one of the other drawbacks of the world based tokennea„ÄÇ so the first one is that we have very largeocabularies„ÄÇ the second one is that we need to learn that the word Do and the word Dos are very similar„ÄÇ

 they won't know that from scratch because when the model is initialized randomly„ÄÇ it's going to have a set of abidding for that word dos and another one for that word dogss and it's going to need to learn by seeing lots and lots of data that those two world look a little a bitlike„ÄÇ

And the last disadvantage is that Ung token so every word that the depo is going to end up like this and the more can learn in your representation of that„ÄÇ it it says if you had just deleted the world in the sentence„ÄÇSo another way is to just split your text on all characters which is the what character best organizes to in this case your vocabulary is not going to be very large because 256 SI characters„ÄÇ for instanceÔºå a little bit more if you take the wall any good thing„ÄÇ

 but you're not going to end up with models but have a vocabulary size of I don't know 300„ÄÇ000 or something like that„ÄÇSo this is better for the vocabulary size„ÄÇ you probably won't get a known token because you all see all the different character possible„ÄÇBut the drawback is that now the representation is based on characterÔºå so the model as to on that„ÄÇ

 for instanceÔºå the letter E is not does not mean the same thing when it's between an a T compared to the letter E here with the key and the n beside the word organization„ÄÇSo is a representation of each letter is less meaningfulÔºå that's what I't trying to say„ÄÇ

And compared to what we had we've worked„ÄÇThe overall drawback is that we end up with very long sentences„ÄÇ for instanceÔºå for leads to tokenizationÔºå if we look back with two world based tokenization it was split into five words with the tokenization with the character based tokenization it splits in much more concor here but it's between 15 and 20„ÄÇ

 let's say so we end up with longer sentences and our transformer models are usually constrained by a maximum lengths„ÄÇ So for instanceÔºå the built model can only do5 can only treats 512 tokens at a time„ÄÇ So using a character based tokenization algorithm would„ÄÇMake sure the maximum sentence„ÄÇ you can feed them all pretty short„ÄÇSo that's why transformmonology usually use a compromise between word and character based tocanization„ÄÇ

 which is the world tokenization„ÄÇSo sub organization„ÄÇAs the name indicates„ÄÇ it's going to split your text into subworsÔºå so it's still split between words„ÄÇ but some wordss are got intoÔºå for instance hereÔºå you've got lets do and then token and Iization are separated into„ÄÇNotice that you get the small and„ÄÇWith like the animals know to say that in English„ÄÇ

 but that special thing between an inferior superior sign with slash W„ÄÇ which means that it's the end of a world„ÄÇ So Tuken doesn't have it because that's for the„ÄÇThat's because we want the model to be able to differentiate token as a single world and token followed by something else like tokenization„ÄÇ so the Iization as the specificx that says here its in the other world that token does not„ÄÇ

And so that depends on the convention used by the tokenizer„ÄÇ some tokens have a thing at the beginning of world„ÄÇ some tokens have a thing at the end of the world„ÄÇAnd so this approach„ÄÇ this approach allows you to have a vocabulary that's not going to be too huge and the two can still have some meaningfulmantic some semantic meaning that's more meaningful than just characters„ÄÇ

And the last thing is that for worlds based tokensÔºå for instanceÔºå Doug and dogs„ÄÇ well two separate wordss hereÔºå Do as dogs would probably be split into Do and nest„ÄÇ the same way tokenization is split between token and Iization so it can learn that they have the same prefix and then the sization is going to be used in another world like modernization„ÄÇAnd the twoken can the model sorry can then make sense of the Su fixes and learn that they are always kind of the same„ÄÇ

And so in the next part of the courseÔºå we'll look at into detail as a different because there are three different„ÄÇSuborgan algorithm that are by level word piece and sentence piece will explain exactly the difference between them in the next part of the course„ÄÇ

So let's see if we have any questions before we look at oh the tokenazer work practice„ÄÇYes„ÄÇ I'm just going to put myself here properly„ÄÇTuckenneer breaks a sentence into tokens„ÄÇ but no limatizations or stming is performed before learning„ÄÇÂóØ„ÄÇYou know„ÄÇIt's„ÄÇLets meet the bullsh„ÄÇI would say noÔºå but you should ask the questions from where people that are more competent than me can answer you because I'm not completely sure„ÄÇ

Could you provide intuition into word piece usingbed and sentence piece type tosÔºüI could„ÄÇ but it's going to take a bit of time so again I'm going to readdirect you on the form where I can take the time to properly answer you and there is also maybe Louisis can share it here phrase his the tokenazal summary in the transformal documentation that explains the difference between what piece and the sentence piece which is using Uniigram behind the scene and the key difference between the two„ÄÇ

Does the W slashW tag add any information in the subwe tokenizationÔºå so yes„ÄÇ as I said it's what allows them all all to know the difference between a single world like I mean between token used as a single world or token inside a world like tokenization or tokenizer„ÄÇ

And then let's see how the tokener work in practice„ÄÇÂëÉ„ÄÇSo can we have seen to load the tokenizer using the form pretrain method„ÄÇAnd what it returns„ÄÇ and we'll now quickly look at the video on the tokenization pipeline„ÄÇ which is going to explain what's happened when we feed the tokener sequence like that and how it returns those numbers„ÄÇ

Let me just grab it from my computerÔºå and then I'll continue answering questions„ÄÇÂóØ„ÄÇSo took a nice pipeline„ÄÇIn this videoÔºå while look at how tokenizer converts ver text to numbers that a transformer model can make sense of„ÄÇ like when we execute this good„ÄÇHere is a quick overview of what happens inside the tokenizer object„ÄÇFirstÔºå the text is split into tuetsÔºå which are wordsÔºå parts of wordsÔºå or punctuation symbols„ÄÇ

Then the tokenizer adds potential special tos and converts each token to our unique respective ID„ÄÇ as defined by the touckenizer's vocabulary„ÄÇAs we'll see it doesn't quite happen in this order„ÄÇ but doing it like this is better for her understandings„ÄÇThe first step is to split our input text into tokensÔºå we use the tokenized method for this„ÄÇ

To do thatÔºå the tokenizer may first perform some operations like lower casing or words„ÄÇ then follow a set of rules to split the result in small chunks of text„ÄÇMost of the transformable models use a word organization algorithm„ÄÇWhich means that one given word can be split in several tokensÔºå like tokens here„ÄÇ

Look at the Tokenization algorithms video linked below for more information„ÄÇThe ash ash prefix we see in front of I is a convention used by bird to indicate thistoken is not the beginning of the world„ÄÇOther organrs may use different convention however„ÄÇFor instance„ÄÇ Albert tokenizers will add a long end score in front of all the tokens that had its space before them„ÄÇ

Which is a convention shared by all sentence based tors„ÄÇThe second step of the tokenization pipeline is to map those tokens to respective IDs„ÄÇ as defined by the vocabulary of the tokenizer„ÄÇThis is why we need to download the file when we instant hit a tokenizer with the form pre method„ÄÇWe have to make sure we use the same mapping as when the model was portrayed„ÄÇTo do this„ÄÇ

 we use the converttugans to IDs method„ÄÇYou may have noticed that we don't have the exact same results as in our first slide„ÄÇOr note as this look like a list of random numbers anyway„ÄÇ in which case allow me to refresh your memory„ÄÇWe the number at the beginning and the number at the end that are missing„ÄÇThose are the special tickets„ÄÇSo special tokens are added by the proper formalal method„ÄÇ

 which knows the indices of a token in the vocabulary and just adds the proper numbers in the input IDs list„ÄÇYou can look at the special tokens and more generally at how the tokenizer has changed your text by using the decocode method and the outputs of the tokenizer object„ÄÇ

As for the prefix for beginning of worlds part of worlds„ÄÇ both special token vary depending on which tor you are using„ÄÇSo belt tokener uses CLS on„ÄÇ but the Robertta tokener uses HTMLl like on calls S and/lash S„ÄÇNow that you know how the tocanazer worksÔºå you can forget all was intermediately admitted and don remember that you have to call it on your input texts„ÄÇ

The output of the decokenizer don't just contain the input IDÔºå however„ÄÇTo learn where the attention mask isÔºå check out the batch input Together video„ÄÇTo learn about targettype ideasÔºå you get the process spells of start video„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_40.png)

![](img/0e882caa799deede0a88d67b58737bfe_41.png)

Yeah„ÄÇSo we have one questions that's linked to what we were seeing just before the video are token slash W and token going to have separate representation IDs and yes„ÄÇ we are going to have separate representation IDs because we are not the same token„ÄÇAs which is the„ÄÇ

 the the whole meaning of that slashable you2an„ÄÇNe and sorry specific„ÄÇSo tokenizer or the tokenization pipelineÔºå I'm not going to livecode was intermediately admitted because you shouldn't really learn them„ÄÇ we're just showing them to show you the steps inside the pipeline the main thing to remember is that you just have to call your tokenizer on your input like this because this is the main E that's the most useful and now we'll look at what the attention mask is and what padding and Fun means the arguments that we had at the very beginning so that we fully explain what's happening inside all the tokenizer„ÄÇ

OhÔºå another questionÔºå is there a reason you would save a proed organizer or then better to just have a local copy„ÄÇVery good question„ÄÇ SoÔºå yeahÔºå there is no real reason to save your patron organizer if you don't need to„ÄÇIf you didn't make any change inside it and you always you would always have a local copy because auto tokenizer that from pretrained is going to cache the files to avoid you download them again so there is no reason to save it the one exception is when youre creating folder that you want to push to the model hub in which case you should save your tokenizer inside that folder so that when you push user you push your model the configuration and the tokenizer that's used with it and we have all those threethink„ÄÇ

The the the A face website is going to be able to apply in front APII in your model and you will be able to play with the Wichat online„ÄÇother than that you won't really need to use the safe pretrain method on the tokener„ÄÇ it's mostly for the model that's going to be super useful or and we will see part two closer to do that if you're training a tokener from scratch because you're pretraining a model„ÄÇ for instanceÔºå in a new languageÔºå then youll need to use the safe pretrain method to save the result of your tokener„ÄÇ

So we're going to watch the last video for today live session about batching inputs to cover and then we'll look more crisly at the good to„ÄÇ let me just„ÄÇLaunch the collab first so that we don't have to wait after the video and then we'll watch the video together„ÄÇ

ÂóØ„ÄÇCome on„ÄÇSth„ÄÇYesÔºå I want to run it„ÄÇAnd let me grab the videoÔºå patching and put together„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_43.png)

![](img/0e882caa799deede0a88d67b58737bfe_44.png)

ÂóØ„ÄÇYeah„ÄÇHow to batch inputs together in this videoÔºå well see how2 batch input sequences together„ÄÇIn general allÔºå the sentences we want to pass through our model won't all have the same length„ÄÇHere we are using the model we saw in the sentiment analysis pipeline and want to classify two sentences„ÄÇWhen tokenizing them and mapping each token to its corresponding input I„ÄÇ

 we get two lists of different length„ÄÇTrying to create a densor or an newbi array from the two will result in an error because all arrays and densilrs should be a recangro„ÄÇOne way to overcome this limit is to make the second sentence the same length at the first by adding a special token as many times as necessary„ÄÇ

Another way would be to truk the first sequence to the length of the second„ÄÇBut we would then lose a lot of information that may be necessary to properly classify the sentence„ÄÇIn generalÔºå we only truncate sentences when we are longer than the maximum length the model can handle„ÄÇThe value used to pad the circums should not be picked randomly„ÄÇ

The model has been portrayed with a certain padding IDÔºå which you can find in tokenizer„ÄÇpa tokenite„ÄÇNow that we have better sentencesÔºå we can make a batch with them„ÄÇIf we pass the two sentences to the model separately and patched together however„ÄÇ we notice that we don't get the same results for the sentence that is pad here the second one is that the bug in the transformerers library now if you remember that transformers will all make easy use of attention layers„ÄÇ

 this should not come as a total surprise„ÄÇWhen computing is the contextual representation of each token„ÄÇThe attention layers look at all the other words in the sentence„ÄÇIf you have just a sentence or the sentence with soball padic tos added„ÄÇ each logicalal don't get the same values„ÄÇTo get the same results with or without padding„ÄÇ

 we need to indicate to the attention layers that we should ignore those padding tickets„ÄÇThis is done by creating an attention maskÔºå a tonsil with the same shape as the input IDs with series and ones„ÄÇOnce indicates the tokens the attention layers should consider in the context„ÄÇ and the the tokens which should ignore„ÄÇNowÔºå passing this attention mask along with the input ID will give us the same results as when we send the two sentences individually to the model„ÄÇ

This is all done behind the scenes by the tokenizer when you apply to several sentences with a flag bedding equal true„ÄÇIt will apply his bedding with a proper value to the smaller sentences and create the appropriate attention mask„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_46.png)

![](img/0e882caa799deede0a88d67b58737bfe_47.png)

„ÄÇSoÔºå let's„ÄÇLook at the same thing in collab„ÄÇLet for any questions„ÄÇ noÔºå I don't see any questions„ÄÇ don't hesitate to ask your questions in the chat again„ÄÇ and let's look at the same code that we had to look again at what the padding and attention mask are exactly„ÄÇSo as we saw in the videoÔºå if we try to„ÄÇApply our model„ÄÇDirectly„ÄÇ

Oh no it it not take the exact same thing as in the video„ÄÇ If we try to apply our model directly on just one sentence that we recognize and converted to to ideas like that using the the same code as in the previous video„ÄÇ it's gonna fail because the model wants batches of input so it wants„ÄÇIt's actually the dokenizer even if you have just one sentence is adding one dimension„ÄÇ

 you can see like there are two pairs of brackets surrounding that so here this is a tonsor of shape1 by a guess 60„ÄÇAnd so if you want to pass just one sentence that you processed manually to a model„ÄÇ you have two had one dimensionÔºå for instanceÔºå by adding a pair of brackets here„ÄÇThis is for a separate exampleÔºå so now looking at two sentences togetherÔºå so if we have two lists„ÄÇ

 so let's say we have both id and both ID and we want to make a pair of sentences„ÄÇWe can't we can create a list of lists like thatÔºå but we can't create an array with the two of them because then don't have the same shape„ÄÇSo we need to add a padding index so we could ever hide it at the handle at the beginning„ÄÇ most of the Transal model expects the padding to be applied on the right with the exception of ExcelNe which expects it at the beginning„ÄÇ

 but the tokenizer should be responsible to apply the padding because the tokenizer knows what the model wants and is going to apply it on the right side„ÄÇÂóØ„ÄÇSo once you have thosepat IÔºå you can create„ÄÇYou can sorry„ÄÇ create done source from them and then pass them through the model„ÄÇAnd if we look like we we've just done in the video at the outputs„ÄÇ

 if we pass the sentences separatelyÔºå So we still have to add power brackets if we want the model to be applied on them because the model expects a batch„ÄÇ So the batch can have just one thing inside itÔºå but it needs to be something that has two dimension„ÄÇ

So if we pass sequence1 and sequence 2Ôºå we get those two results and if we pass the patch with the two sentences„ÄÇ we get the same result for the first sentenceÔºå but different result for the second sentence we can see that Ph2 are difference here which is because the circumst with a paddingken if we don't do anything special the model is not going to compute the same results it's not going to properly ignore the paddingken so that's so in the video is because of the attention layers the attention layers are call the transformers model and are we explain that at length during chapter1 there are layers that not computer presentation not just of one word but one word within its context because the transformer models were designed for translation at the very beginning and if we want to translate the word we don't just need to pay attention to that word but all the words around it for instance to know which genders the word hard„ÄÇ

It's a singular or parole knownÔºå things like that„ÄÇAnd so if we don't say to if we don't tell the attention layer that this is not a real token it just affect token we needed to to have a rectangle and make a batch the attention layer is going to pay attention to the token and computer contextual representation that adds that token into account So to tell the attention layer now that's not a real token don't pay attention to it we have to create what is called an attention mask so here we put ones where we want the attention layer to pay attention and zero where we want the attention layer to ignore so is the same shape as the batch adss that we are using and the zero is put where we have the token I token ID„ÄÇ

And if we do that and pass those through the model now we see that we get the same output„ÄÇ so here the first sentence is still the same because there was not bedding„ÄÇ but this output here is the same as this output here„ÄÇ which was the output of the model with the second sentence„ÄÇSo this is what's done by the tokenizer„ÄÇ

 if you remember when we were passinging all two sentencesÔºå it was returning something with two keys„ÄÇ one was the input Is which correspond to this thing here„ÄÇ and therefore one was the attention mask which corresponds to this thing here„ÄÇThe truncation argumentÔºå so when we passed the two sentences to auto at the very beginning„ÄÇ

 we said padding equal true and truncation equal2Ôºå so truncation equal true is going to truncate very very long sentences because for be model„ÄÇ for instanceÔºå can only handle sequences that have a length of 512 maximum„ÄÇSo if we have a sequence that's longer than thatÔºå the model is going to fail and we need to truncate it„ÄÇ you shouldn't truncate your input for any other reason when making them shorter than the maximum length the model can handle otherwise because when you trun sorry you remove information so for padding we are adding something and we can tell the attention layer to ignore it so at the end we get the exact same results with or without padding with truncation you're ignoring information but you just can't recover so you will never be able to get the same results without truncation„ÄÇ

But sadly it's needed because transformers have a maximum length„ÄÇ so if you have very long inputs that are greater than than at maximum length you need to locate them„ÄÇAnd if we go back looking at the course in the putting all„ÄÇPutting it all section„ÄÇWe have the various patting on from strategy„ÄÇ So here„ÄÇ

So if we go back with the two sentences we are using at the very beginning of this left session and pass them we can pass them to the skener„ÄÇ which is going to then output list of listÔºå but we won't be able to patch them together without applying padding so we have various strategies to apply padding when we do padding equal through it's the same as doing padding equal longest which is going to pad the sequences up to the maximum length inside the samples that you're passing so here„ÄÇ

We have two sequences with patting equal longestÔºå it's going to add the second sentences„ÄÇ the second sentence here to the length of the longer sentence„ÄÇ If we go back to the notebook behind pipelineÔºå we can see that here„ÄÇWith all those zeros being added so that the circum sentence inside the batch is the exact same length at the first„ÄÇ

You can also say patting equal max length max length refer to the model max lengthÔºå so for instance„ÄÇ it's 512 for be or distberÔºå so this is going to add every sentence to 512 the maximum length of the model of adoption should not be considered when„ÄÇWhen you have short sentencesÔºå for instanceÔºå its in general it's better to add to the longest thing you have unless you need to use fixed shaped for some reason„ÄÇ for instanceÔºå TPUus like fixed shapes better in which case you would use the max length option and which is a bit inefficient but for instance with the TPU accelerator„ÄÇ

 it's the only way to get some real speed because they need all the inputs towards the exact same shape„ÄÇAnd you can also specify a max length that you wantÔºå so for instance„ÄÇ if you know that in your dataset setÔºå all your sentences are shorter than 1 around28„ÄÇ you could say patting equal max length max length equal 1028„ÄÇAnd so you should also use truncation„ÄÇ

 as we saidÔºå because you need to truncate when your inputs when you there are longer than the maximum length a model can handle„ÄÇ you can also specify the maximum length to which you want to truncate your inputs„ÄÇSo that's it for padding on„ÄÇ Do we have any questions„ÄÇÂóØ„ÄÇNo questionsÔºå Small links shared by Lewiss„ÄÇSo„ÄÇThe last thing isÔºå as we saw beforeÔºå you can tell you token to return bytoch tensor and soft flu tensor on entire array„ÄÇ

And as we saw in one of the videosÔºå the tokenizerÔºå the tokener pipeline video specifically„ÄÇ the tokenazer adds the special token at the beginning and at the end of our sentence„ÄÇ so it depends exactly on which model youre usingÔºå but the tokener will all know what token's special token the model expects and will put them at the beginning and at the end of your sentences„ÄÇAnd so to conclude this chapterÔºå here is the world codeÔºå it's just missing the post processing„ÄÇ

 but here is the world code that was executed behind the scene by the pipeline„ÄÇÂóØ„ÄÇSo you import auto tokenizer on automod for sequence classification„ÄÇ the auto beam meaning that you can use any checkpoint from the hub„ÄÇ it's going to pick the right architecture for you„ÄÇ

The checkpoint that was used by the pipeline is this1„ÄÇ you can use any of our checkpoint on the model hub that correspond to a sentiment analysis task„ÄÇWe can load the tokenazizer with the from betweentrain method„ÄÇ we can load our model with the from betweenttrain method and if we have two sentences„ÄÇ

 we can tokenize them together with padding equal2 con equal2 which on tensor source equal piy toch if we're using py toch tensor flu which using tensor flu and then we pass the tokens to the model and to get the plus process results it's just missing the soft max here to have the exact same result as pipeline„ÄÇ

One last question I'm confused between padding you call longest and dynamic padding„ÄÇ Is it similar„ÄÇ YesÔºå it's veryÔºå very similar„ÄÇ Well look at dynamic padding in the next chapter when doing training„ÄÇ but for those who don't know what it is dynamic padding is when you„ÄÇWhen you go through your training data and beL batches„ÄÇ

 dynamic padding means each time you have to build a batch„ÄÇ you pad your sentences to the maximum length inside the batch„ÄÇ and so that's what padding equal longest will do if you pass your sentences a small batch of sentences is going to create a batch with for instance eight sentences and the second dimension is going to be the maximum length inside that batch„ÄÇWhereas patting equal max length is going to add everything to a fixed max length„ÄÇ

 either the maximum length of model canon all or the maximum length you passed along to the dekenizer„ÄÇAnd so that's it for the basic use of models and tokenizers„ÄÇThank you for following the live stream so now that weve seen that chapter together you should be able to complete the questionnaire at the hand and then before going to chapter 3„ÄÇ it's useful if you try to run again the pipeline collab that we saw together on the batching input togetherab that we looked at together and try to understand what every cell is doing and maybe even try to redo it yourself„ÄÇ

And then try to think of a problem you want to work on on a classification with a text classification problem you want to work on and try to use a base model and an organizer on that problem to be able to get outputs from some inputs and we'll see in the next chapter of to actually find tuneni model on your given problem and in chapter 4 we'll see how to upload as a result to the bundle hub so that you can venture that model to with the rest of the community and use the widgets of the inference API online to have demos of„ÄÇ

Of your model„ÄÇ let me look at the questions before we end this live stream„ÄÇOh„ÄÇ following up from the previous question with patting equal whole longest„ÄÇ we don't need to use that decorator with padding„ÄÇ That's not entirely true„ÄÇ And we' all see exactly why in the next chapterÔºå we„ÄÇ

 we need to be in the dynamic of training a modelÔºå but„ÄÇIf you are playing padding all longest while you' doing tokenization on your wall data set„ÄÇ it's going to add to the longest element in your data set„ÄÇ so that's not really the same thing as doing dynamic padding„ÄÇ

But yeah well we'll dive into that in the next chapter„ÄÇOtherwise„ÄÇ thank you all for following this live stream and next week on Cha 3 there are four different live streams because Cha 3 is very different for Pytch and Tensorflow since it's about training and functioning until now the code add some little differences and you can see that you have a switch here for the course if you want to switch between Pytch and Tensofflow if you want to look at exactly what' difference but for next chapter is going to be very different so you have live session specifically for Pythtch and live section specifically for Tensorflow be sure to check on the formss which one you want to attend to and market into your calendar„ÄÇ

And yeahÔºå let's first this up„ÄÇ Thanks a lot for comingÔºå bye bye„ÄÇ![](img/0e882caa799deede0a88d67b58737bfe_49.png)

„ÄÇ

![](img/0e882caa799deede0a88d67b58737bfe_51.png)