# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P19ÔºöL19- ‰ΩøÁî®Âæ™ÁéØÁ•ûÁªèÁΩëÁªúËøõË°åÂêçÁß∞ÂàÜÁ±ª - ShowMeAI - BV12m4y1S7ix

HeyÔºå guysÔºå welcomee to your new Pytorch tutorial„ÄÇ Today„ÄÇ we will be talking about recurrent neural nets or short Rn Ns„ÄÇ I will briefly explain the theory behind Rn Ns and the different kind of applications„ÄÇ and then we will implement a Rn N from scratch in Pytorarch to do name classification„ÄÇ

 This should give you a good understanding of how Rn Ns work internally„ÄÇ So let's start„ÄÇ Rn Ns are a class of neural networks that allow previous outputs to be used as inputs while having hidden states„ÄÇ Here's an image that shows the architecture of Rn Ns in the simplest way„ÄÇ So we have an input and then internally do some operations and get hidden states„ÄÇ

 And then we take those hidden states and put them back into the next step„ÄÇüòä„ÄÇSo we can use the previous knowledge to update our new state„ÄÇ And then at the end„ÄÇ we also get an output„ÄÇ So we can also unfold this graph to get a better understanding and basically we are working with a sequence here„ÄÇ SoÔºå for exampleÔºå if we have a whole sentenceÔºå then we might use every single word as one input„ÄÇ

 So we have the first input and some initial hidden„ÄÇ and then we do our operations and get the output and a new hidden state and then we use this hidden state and then put it into the next input„ÄÇ So we take the next input and use the previous hidden states and again„ÄÇ do our operations and get a new output and the new updated hidden state„ÄÇ

 And then we take the next input and so on„ÄÇ So this is basically the architecture of„ÄÇRNAnd now why are R&N so important And for this there is a nice article by And Carpathy„ÄÇ The article is called the unreasonable effectiveness of recurrent neural networks and I highly recommend to give this a read„ÄÇ so I will put the link in the description„ÄÇ

![](img/67851926e9649c02f2608968bb25321d_1.png)

But the core takeaway is that R and ends are so exciting because they allow us to operate over sequences of vectors„ÄÇ So with traditional neural netsÔºå we have just a one to one relationshipÔºå for example„ÄÇ when we do image classification„ÄÇüòäÔºåThen our input„ÄÇ So our image is of fixed length„ÄÇ and our also our output is of fixed length„ÄÇAnd now with R and endsÔºå we can work with sequences„ÄÇ

 and there are a lot of different types„ÄÇ So basicallyÔºå we can have a sequence in our input„ÄÇ and we can also have a sequence in our output or also in both input and output„ÄÇ SoÔºå for example„ÄÇ we can have a one to many relationship where we have only one inputÔºå for example„ÄÇ this is used in image captioning when we have one in image and then we want to describe what we see in the image and get multiple outputs„ÄÇ

Then we can also have a many to one relationship„ÄÇ So we have this isÔºå for example„ÄÇ the case in sentiment classificationÔºå or what we are doing later with our name classification„ÄÇ So we have a sequence as inputsÔºå and then apply our R and N„ÄÇ and then we use the last output and do some classification with this„ÄÇ

 Then we can also have a many to many relationship„ÄÇ This isÔºå for example„ÄÇ used in machine translation where we have a whole sentenceÔºå for exampleÔºå in English as an input„ÄÇ And then we put out a whole sentence in FrenchÔºå for example„ÄÇThen we can also have a many to many relationship with a synct wayÔºå for example„ÄÇ

 in video classificationÔºå where we want to classify each single frame„ÄÇ So yeah„ÄÇ these are our possible applications of R andNs„ÄÇ So they are mostly used in the fields of natural language processing and speech recognition„ÄÇ but they couldÔºå for exampleÔºå also used for image classification„ÄÇ So yeah„ÄÇ that's what makes RnNs so powerful„ÄÇ And now let's have a brief look at some pros and cons„ÄÇ

 So the advantages are that we have the possibility of processing inputs of any length„ÄÇThen our model size is not increasing with the size of the input„ÄÇ Then the computation takes into account historical information„ÄÇ So the previous data„ÄÇAnd our weights are shared across time„ÄÇAnd some drawbacks are that the computation might be slowered and with normal neural nets„ÄÇ

 and it can be difficult to access information from a long time ago„ÄÇ and we are also not able to consider any future input for the current state„ÄÇ So yeah„ÄÇ that's basically the theory behind R andNs„ÄÇ And now we can directly jump to the code„ÄÇ So in our exampleÔºå we want to do name classifications„ÄÇ So I downloaded the data„ÄÇ

 and I will also put the link in the description„ÄÇ So basically we have different files with different names„ÄÇ So these are all last names from different countriesÔºå for example„ÄÇ We have Arabic Chinese C Dutch English and so on„ÄÇ So I think these are 18 different countries„ÄÇ And now we what„ÄÇWe want to do is we want to classify this and detect from which country the name is and what we are going to do here is we take the whole name as a sequence and then we use each single letter and put it in our R&N as one input„ÄÇ

And for thisÔºå we need some help of functions„ÄÇ So I already implemented them here„ÄÇ and I will only go briefly through this code„ÄÇ So what we want to do here first„ÄÇ we want to have a help of function to convert our data to ASI„ÄÇ for example„ÄÇ if we have this name with some special characters„ÄÇ and then we process this„ÄÇ So let's run this file„ÄÇ

 Then we see it remove the special characters and we only have ASI characters left„ÄÇ and basically„ÄÇ we also print all the possible letters„ÄÇ So this is from a to C and also in capital letters„ÄÇ and we also allow these signs„ÄÇüòäÔºåAnd then we have a helper function to load the data„ÄÇ So this basically loads all those filesÔºå and then it loads all the names„ÄÇ

 and it gets the country from the file name„ÄÇ So this is what the load data function will do„ÄÇAnd then we have some functions to turn our data to Tenzo„ÄÇ So we have letter to index and letter to Tzo and also a line to atenzo„ÄÇ And what we are doing here is we are using a technique that is called one hot encoding„ÄÇ

 So we need a way to display our data that can be used for training„ÄÇ And for this„ÄÇ we use one hot encoding„ÄÇ So if you've watched my tutorial about the chatbot in Pyr„ÄÇ then you might already know this„ÄÇSo a one hot back vector is filled with zeros„ÄÇ except for a one at the index of the current letter„ÄÇ For example„ÄÇ

 if we have only five possible charactersÔºå ABC D and E„ÄÇ So our a would be a vector of length 5„ÄÇ and we have a one at the position where a is„ÄÇ and the same for B„ÄÇ a vector of length 5 and the second index is a 1 and the rest zeros„ÄÇ So this is one hot encoding„ÄÇ So if we go back to our file for exampleÔºå I can show you how the load data function looks„ÄÇ

 So this will return a dictionary with the country is key„ÄÇ and then the„ÄÇCorresponding names as the„ÄÇValueÔºå SoÔºå for exampleÔºå if we run this and have a look at the key Italian and only take the first five entries„ÄÇ then we see we have five different Italian names here„ÄÇThenÔºå as I saidÔºå we do this one hot encoding„ÄÇ So for thisÔºå we can use the letter to tenor function„ÄÇ So now if we run this„ÄÇ

 So let's save this and run this and then print the tenzo for ch„ÄÇ Then we see it has this size„ÄÇ So this is of shape 1 by 57Ôºå because we have 57„ÄÇPossible characters„ÄÇ This is what I printed here„ÄÇ These are all the letters„ÄÇ and then we have a cha at the position where the capital J is„ÄÇ So this is how our input will look like like later„ÄÇ andÔºå of course„ÄÇ

 we do this not only for a single characterÔºå but for the whole name„ÄÇ So for this„ÄÇ we have the line to tensor function„ÄÇ And now if we here we print the size„ÄÇ So this will be of size5 by  one by 57„ÄÇ And the one is because our model expects it to be in this shape„ÄÇAnd the5 is because of the number of characters and the 57 is because of the number of all different characters„ÄÇ

 AllrightÔºå so these are all the help or functions that we need„ÄÇ AndÔºå of course„ÄÇ I will put the code on Github„ÄÇ and I also provide the link to the data so you can download these files„ÄÇ So now we can start writing our R and N„ÄÇ So for thisÔºå of courseÔºå we import the things we need„ÄÇ So we import torch„ÄÇ We import torch dot N N as N N„ÄÇ

 Then I also want to import map plot Li dot pi plot S P L T because I want to show you a plot later map„ÄÇPot„ÄÇLipÔºå then we import our utility functions„ÄÇ So I say from us„ÄÇ import all the different letters and also the number of different letters„ÄÇ So this is 57„ÄÇ Then we also say from uÔºå we want to import these helper functions„ÄÇ So load data„ÄÇ

 then letter to tenor and line to tensor and random trainingx example„ÄÇ So this is basically a function„ÄÇThat will do a random choice from those names and return the corresponding country„ÄÇSo now that we have thatÔºå we can start implementing our R and N„ÄÇ So we need to have a class and call this R and N„ÄÇ And this should inherit from N„ÄÇ

 N dot module as all of our Pytor models„ÄÇAnd by the way„ÄÇ there is already a R andN module available in Pytch so you can directly use this„ÄÇ but this is what we are doing in the next tutorial„ÄÇ So for now we want to implement this from scratch to get a better understanding So let's have a look at our model architecture again So this is what our R andN for name classification will look like„ÄÇ

 So we have an input and a hidden stateÔºå and then internally So this is what we are doing here internally„ÄÇ so we combine them and then we process our combined Tensor and we apply two different hidden layers„ÄÇ so we have our input to a output and our input to a hidden layer„ÄÇ

![](img/67851926e9649c02f2608968bb25321d_3.png)

![](img/67851926e9649c02f2608968bb25321d_4.png)

These are just two normal linear layers and then we get one hidden output and then we use this for the next input and we also get a output and since we are doing classification„ÄÇ so a multiclass classification task we apply the softmax layer and then get the output„ÄÇ

So now this is what we want to implement„ÄÇSo nowÔºå first of allÔºå of course„ÄÇ we define our init function„ÄÇ This will get self„ÄÇ Then it gets the input size„ÄÇ It also gets the hidden size„ÄÇ this is going to be a hyperparmeter that we can specify„ÄÇ And we also get the output size„ÄÇ and in our in function„ÄÇ First„ÄÇ

 we want to call the super R and N and self„ÄÇ And then the in it„ÄÇüòäÔºåSorry„ÄÇIn it„ÄÇThen here we want to store our hidden size„ÄÇ So we say self dot hidden size equals hidden size„ÄÇ Then we define our two different linear layers„ÄÇ So we call this input to hidden I to H equals N N dot linear and the size is the input size plus the hidden size because we combine them„ÄÇ And for thisÔºå the output size is still the hidden size„ÄÇ Then we do the same with our output„ÄÇ

 So we say input to output„ÄÇ and this is going to be a linear layer as well„ÄÇ So the input size is the same„ÄÇ And here we use the output size„ÄÇ And then we also need a soft max layer„ÄÇ So we say self dot soft max equals N N dot„ÄÇSoft max„ÄÇ And then we say that dimension equals long dimension 1Ôºå cause our inputÔºå for example„ÄÇ

 just test the shape 1 by 57„ÄÇ We need the second dimension„ÄÇSo this is our init function„ÄÇ Then we„ÄÇ of courseÔºå also have to define the forward pass„ÄÇ So we say define the forward function„ÄÇ and this gets self„ÄÇ and then it gets an input tensor„ÄÇ And as you should know now„ÄÇ this also gets the hidden tensor„ÄÇ So we use the hidden tensor for the forward pass„ÄÇ

 And then here we process this„ÄÇ So first thingÔºå let's have a look at this again„ÄÇ we combine our input and our hidden tensor„ÄÇ And then we apply those linear layers and the soft marks„ÄÇ And then we return two different tensor„ÄÇ So the output tensor and the new hidden tensor„ÄÇSo let's do this„ÄÇ So let's call this combined equals„ÄÇ And for thisÔºå we can use torch dot cat„ÄÇ

 And then as a tupleÔºå we want to combine the input tensor and the hidden tensor„ÄÇ And here again„ÄÇ along dimension 1„ÄÇ Then we apply our linear layer„ÄÇ So we say hidden equal self dot I to H„ÄÇ And then here we put in the combined„ÄÇTensarÔºå then we say our output equals self dot I to O„ÄÇO with our combined tensor as well„ÄÇ And for the outputÔºå we also apply the soft max„ÄÇ

 So we say output equals self dot soft max with the output„ÄÇ And at the end„ÄÇ we return the output first and then the new hidden state„ÄÇAnd so this is basically all we need for our R and N implementation„ÄÇ And I'm also going to add a new„ÄÇ a little helper function„ÄÇ And I call this in it„ÄÇHidden„ÄÇ

 so I need some initial hidden state in the beginning„ÄÇ And what I want to do here simply is I want to return an empty tensor„ÄÇ So I say torch dot zeros„ÄÇ And this has the shape 1 by self dot hidden size„ÄÇ And yeahÔºå so now we can start applying this„ÄÇ So now let's load the data„ÄÇ So let's say our categories lines and our all possible categories equals load data„ÄÇ

 So this is a dictionary with the country as key and the names as values„ÄÇ And this is just a list of all the different countries„ÄÇ And then the number of category„ÄÇ![](img/67851926e9649c02f2608968bb25321d_6.png)

Gorries equals the length of all categories„ÄÇ For exampleÔºå we can print the number of categories„ÄÇ and let's save this for now and see if everything is working„ÄÇ So let's say Python are an end on pi„ÄÇAnd now this is working„ÄÇ So we see we have8 different categories„ÄÇ so this is because we have8 different files here„ÄÇAnd now we need to define or set up our R and N„ÄÇ

 So we say R and N„ÄÇEqualsÔºå and then we use our R and N classÔºå and this gets the input size„ÄÇ which is the number of possible letters„ÄÇ Then it needs the hidden size„ÄÇ So N hidden„ÄÇ and it needs the output sizeÔºå and this is the number of categories„ÄÇ And now our hidden size„ÄÇ So N hidden is an hyperparmeter that we can define„ÄÇ So hereÔºå let's try 128„ÄÇAnd nowÔºå as an example„ÄÇ

 let's do one single step„ÄÇSo we canÔºå for exampleÔºå say our input tensor equals„ÄÇ and then we use the letter to tensor function forÔºå let's sayÔºå for a and our„ÄÇ then we need a hidden tensor„ÄÇ So hidden tensor equals R and N dot in it hidden and then we process this„ÄÇ So we say output and next hidden equals R and N with the input tensor and the initial hidden tensor„ÄÇ

And nowÔºå for exampleÔºå we could print the output size or shape„ÄÇ And let's also print the next hidden shape or size„ÄÇ And let's run this and see if this is working„ÄÇSo yeahÔºå we see our N R and N applied the forward pass and we get a new output with this shape and a new hidden state with this shape„ÄÇ So it's still the size of the defined hidden size„ÄÇ So this is how it works for only one character„ÄÇ

 and now if we go back„ÄÇ So now basically we want to treat our name as one sequence and then each single character is one input„ÄÇ So we repeatedly apply the R andNs for all the characters in the name„ÄÇ And then at the very end„ÄÇ we take the last output and apply the soft mark and then take the one with the highest probability„ÄÇ So this is what we want to do for one name„ÄÇ So now we say we have the whole sequence„ÄÇ

The whole name and here we say our input tensor equals and then we use line to tensor and here„ÄÇ for exampleÔºå we use the name Albert„ÄÇAnd then our hidden tensor is the same„ÄÇ And also„ÄÇ this call is the same„ÄÇSo we grab this and copy this„ÄÇ And then here we use slicing„ÄÇ So we only use the very first letter now for this simple example„ÄÇ

 So let's try this and run it and see if this is working„ÄÇSoÔºå yeahÔºå So this is working„ÄÇ And now here„ÄÇ what we have to do is we have to repeatedly apply these characters„ÄÇ So for thisÔºå let's„ÄÇWrite some help of functions first„ÄÇ So let me„ÄÇComment our the print statements out again„ÄÇ So now let's define a function„ÄÇ and let's call this category to category from output„ÄÇ

 And this gets the output„ÄÇ And as I saidÔºå we applied the soft marks at the very end„ÄÇ So this is basically a likelihood of each character of each category„ÄÇ So we want to return the index of the greatest value„ÄÇ So we can get this„ÄÇ So category index equals„ÄÇ And here we can use torch dot arc„ÄÇüòäÔºåMaxÔºå and then here we put in the output„ÄÇ

 and then we can call the dot item because this is only one value„ÄÇ and then we can return all categoriesÔºå the„ÄÇList with this index category index„ÄÇ So this is one„ÄÇHel a functionÔºå for exampleÔºå NowÔºå if we print the category from output and then hear our output from this name„ÄÇ then if we run it„ÄÇThen we getÔºå nowÔºå this is Irish„ÄÇ andÔºå of courseÔºå this is not trained yet„ÄÇ

 So this doesn't look like an Irish name to me„ÄÇSo now what we want to do is we want to train our R and N„ÄÇ of course„ÄÇ So hereÔºå as alwaysÔºå we want to set up a criterion and a optr„ÄÇ So we say criterion equals„ÄÇ And here we use N N dot N L L loss„ÄÇ This is the negative lock likelihood loss„ÄÇ Then we need to specify a learning rate„ÄÇ And here we have to be careful So In this case„ÄÇ

 I try point005„ÄÇ and the learning rate is very important here„ÄÇ So you might want to play around with this a little bit„ÄÇ And then we say our optr equals torch dot optim dot S G DÔºå so stochastic gradient„ÄÇDescent„ÄÇ and then we want to optimize R N N dot parameters„ÄÇ And as a learning rate„ÄÇ

 we use the defined learning rate„ÄÇSo now we have our loss and criterion and let's define another help a function and call this training„ÄÇ So this is going to be one training stepÔºå and this gets a line tenor„ÄÇ So the whole name as a tenor„ÄÇ and it also gets the category„ÄÇCategory tenor„ÄÇ So this is the actual class label or the index of the class label„ÄÇ And now here we want to get a initial hidden state„ÄÇ

 So we say hidden equals R and N dot in it hidden„ÄÇ And thenÔºå as I said„ÄÇWe want to do this repeatedly„ÄÇ so we need a four loops„ÄÇ we say4 I in range and then the length of the line tensor„ÄÇ So we say line tensor dot size and then index0„ÄÇ So this is the length of the name basically„ÄÇ and then we apply this So we say output and hidden equals R and N with the line tensor of the current index or the current character and the previous hidden state So note that we put in the hidden state and then assign it also to the same variable„ÄÇ

 So the new hidden state will be the output from the R andn„ÄÇ and then we do this for the whole name and then forÔºåThe very last character„ÄÇ we get the final outputÔºå and then we use this to calculate our loss„ÄÇ So we say loss equals Here we apply our criterion with the output and the category tenor„ÄÇ

 and then as alwaysÔºå we do our optimizer step„ÄÇ So firstÔºå we say optimizer0 gradients„ÄÇ Then we say loss dot backward and then we say optimizer dot step„ÄÇ And then at the end of each training step„ÄÇ let's return the output„ÄÇ and let's also return the loss dot itemÔºå So not as tenorÔºå but as float value„ÄÇ

And now we have this helper function for the training step„ÄÇ And now we can do our typical typical training loop„ÄÇ So for thisÔºå let's track something„ÄÇ So let's say the current loss equals 0 in the beginning„ÄÇ Then all losses equals an empty list„ÄÇ So here we want to put in all the losses„ÄÇ So we can plot them later„ÄÇ

 Then let's say our plot steps and also our print steps equals„ÄÇ Let's say1000 and5000„ÄÇ and the number of iterations equalsÔºå let's say100000„ÄÇ And then we say 4 I in range and„ÄÇEers„ÄÇ and now what we want to do is we want to get a random training sample„ÄÇ So we have this as a helper function„ÄÇAnd this returns the category„ÄÇ

 Then it returns the actual line or the name„ÄÇ Then the category is tenorÔºå and also the line as tenor„ÄÇ and we get this by calling the random training example function from the utility class„ÄÇ and this needs the category lines as input and all categories„ÄÇ and then we call the training function„ÄÇ So we say output and loss equals training and this gets the line tenor„ÄÇ

 and it gets the category tenorÔºå then we add the loss to our current loss„ÄÇ So we say current loss plus equals loss„ÄÇ and then we want to print some information„ÄÇ So we say if I plus„ÄÇOne modo„ÄÇPloot steps„ÄÇ So every thousand step„ÄÇ If this equals 0„ÄÇ So here we want to calculate the current running loss and append it to all losses„ÄÇ

 we say all losses dot append„ÄÇ And here we say current loss divided by the number of plot steps„ÄÇ and then we set our current loss back to 0„ÄÇ because here we add it up for every iteration„ÄÇ and then only every thousand stepÔºå we append it„ÄÇ So we have to divide it by the number„ÄÇ and then we get the average„ÄÇAnd now we do the same with the print step„ÄÇ

 So we say if I plus one modular„ÄÇPrint steps equals equals 0„ÄÇ Then we want some print some information„ÄÇ So firstÔºå we want to get the guess„ÄÇ So we say guess equals category from output and we put in the outputÔºå of course„ÄÇ Then we check if this is correct„ÄÇ So we say correct equals„ÄÇ and then we say„ÄÇCorrect„ÄÇ

 and this is correct„ÄÇ If the guess equals equals the actual category that we get from the random training example„ÄÇAnd if this is not correctÔºå then we print wrong„ÄÇ And we also want to print the actual category„ÄÇ So let's use this as an F string here„ÄÇ And here let's print the actual category„ÄÇ And then we print as an F string againÔºå we want to print the current iteration step„ÄÇThen we also„ÄÇ

 let's print I divided by the number of iters times 100„ÄÇ Then let's also print the current loss„ÄÇ So we say loss„ÄÇ and let's print only four decimal values„ÄÇAnd let's also print the current line„ÄÇ So basicallyÔºå this is the name„ÄÇ And then let's print the guess„ÄÇAnd let's also print the„ÄÇ if it's correct or not„ÄÇ And now we are done„ÄÇ So this is basically all that we need„ÄÇ

And now when we are doneÔºå let's plot our losses„ÄÇ So let's trade a figure with mapplotlip„ÄÇ So P L D L T dot figure and P L T dot plot and here we want to plot all the losses and then say plot show and now we could already start our training„ÄÇ And now what weÔºå for exampleÔºå what we can do is we can save our model here and then use it later for for whatever we want„ÄÇ But in this caseÔºå I simply want to try it myself„ÄÇ So I say while true„ÄÇ

 And then I say I get an sentence as input and„ÄÇÂóØ„ÄÇLet's say„ÄÇInput„ÄÇAnd then I put this to a function„ÄÇOr I firstÔºå let's say if sentence equals equals quitÔºå then I breakke„ÄÇ and otherwise I want to predict the sentence„ÄÇ So for this„ÄÇ let's create another little helper functionÔºå let's call this„ÄÇPredict„ÄÇ

 and this gets the input lines of a raw text„ÄÇ So first of allÔºå I print„ÄÇLet's print a new line„ÄÇ and then let's print the input lineÔºå as well„ÄÇSo for thisÔºå of courseÔºå we need an F string„ÄÇAnd now„ÄÇ here in our predictionÔºå we should use torch dot„ÄÇ No gras„ÄÇ We can turn off the gradients now„ÄÇ And then what we want to do is we want to say our line tensor equals line to tenor from the raw input line„ÄÇ

And now we want to do the same as we are doing in our training step„ÄÇ So we have the initial hidden states and then repeatedly apply our R and N„ÄÇ And let's do this„ÄÇ So let me copy this and put it here„ÄÇ So we have the initial hidden state„ÄÇ Then we say4 I in line tens or size 0„ÄÇ and then we get the new output and the new hidden state by applying the R and N„ÄÇ

And then at the very endÔºå we want to get the guess„ÄÇ So we say guess equals„ÄÇ and then we say category from output„ÄÇ And we use the last output from the last step„ÄÇ and then we simply want to print the guess here„ÄÇ So in this example„ÄÇ I don't calculate the accuracy or anything„ÄÇ I just print the„ÄÇGuess and see if it's correct or not„ÄÇ

So yeahÔºå so let's„ÄÇSave this„ÄÇ And let's run this and hope that everything is working„ÄÇ So this might take a few seconds or minutes„ÄÇ So this is from the first example„ÄÇThat I showed youÔºü

And„ÄÇNow we see we have step„ÄÇ This should be 5000Ôºå actually„ÄÇ and this should be 5% of the training is done„ÄÇ and then we have the loss„ÄÇ and we have the name and we see that the guess is wrong because this is actually Po„ÄÇ![](img/67851926e9649c02f2608968bb25321d_8.png)

All rightÔºå so now it's doneÔºå and it's plotting the losses„ÄÇ And you see that it's decreasing very quickly„ÄÇ And then it's trying or jumping around a little bit„ÄÇ but still decreasing„ÄÇ I think this is a pretty good result„ÄÇ So let's have a look at some„ÄÇ![](img/67851926e9649c02f2608968bb25321d_10.png)

Random guesses during the training„ÄÇ So we see that in the beginningÔºå almost every guess is wrong„ÄÇ And then it's starting to learn something things„ÄÇ So now it's starting to do correct predictions„ÄÇ ButÔºå of courseÔºå it's still not perfect„ÄÇ So they are still wrong corrections„ÄÇ But yeah„ÄÇ at the very endÔºå we have this loss„ÄÇ So this is pretty good„ÄÇ And now we can try it ourselves„ÄÇ So now„ÄÇ

 for exampleÔºå we could go and try some names from those files„ÄÇ So for example„ÄÇ let's start with some German names„ÄÇ So let's try acker„ÄÇ And we see it says German„ÄÇ So let's try OlerÔºå for exampleÔºå and this is also correct„ÄÇ So let's try some Italian names„ÄÇ I think these are pretty clear to detect„ÄÇ So let's try Abba Dhi„ÄÇ and it says„ÄÇItalian„ÄÇ

 so this is correct„ÄÇ So let's try some RussianÔºå for exampleÔºå Let's say a ba off„ÄÇ and it says Russian„ÄÇ YeahÔºå great„ÄÇ So let's try something more difficult„ÄÇ For exampleÔºå Chinese„ÄÇ So let's try Bao„ÄÇ and it says correct Bao„ÄÇ Let's try bai„ÄÇStill Chinese„ÄÇ So it looks like it's„ÄÇ it's working pretty nice in this example„ÄÇ Of courseÔºå it's still not perfect„ÄÇ But for now„ÄÇ

 all the guesses are correct„ÄÇ So now you see how we can train a R and N to do name classification„ÄÇ And yeahÔºå I hope you enjoyed this tutorial„ÄÇ And now know how R and ends can be implemented in Pytorch„ÄÇ If you like this tutorialÔºå and please consider subscribing to the channel and leave me alike and see you next time bye„ÄÇüòä„ÄÇ

![](img/67851926e9649c02f2608968bb25321d_12.png)