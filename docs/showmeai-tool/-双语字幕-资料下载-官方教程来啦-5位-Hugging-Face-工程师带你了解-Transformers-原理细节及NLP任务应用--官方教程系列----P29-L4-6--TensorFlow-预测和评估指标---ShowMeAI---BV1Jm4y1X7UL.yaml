- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P29：L4.6- TensorFlow 预测和评估指标 - ShowMeAI - BV1Jm4y1X7UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our other videos。 and as always， there'll be links below if you want to check
    those out。 We showed you how to initialize and finet a transformer model in Tensorflow。
    So the question now is。 what can we do with a model after we train it。 The obvious
    thing to try is to use it to get predictions for new data。 So let's see how to
    do that， Again， if you're familiar with Kais。
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that because these are just standard caris models。 we can use
    the standard Keis predict method as shown here。😊。You simply pass in tokenized
    text to this method like you'd get from a tokenizer and you get your results。Our
    models can output several different things， depending on the options you said。
  prefs: []
  type: TYPE_NORMAL
- en: but most of the time， the thing you want is the output logts。 If you haven't
    come across them before， Los sometimes pronounced logits。 because no one's sure
    are the outputs of the last layer of the network because before a softm has been
    applied。 So if you want to turn the logics into the model's probability outputs。
  prefs: []
  type: TYPE_NORMAL
- en: you just apply a softm like so。What if we want， But what if we want to turn
    those probabilities into class predictions。 Again， it's very straightforward。
    We just pick the biggest probability for each output。 and you can get that immediately
    with the Argm function。 Argmax will return the index of the largest probability
    in each row。
  prefs: []
  type: TYPE_NORMAL
- en: which means that it will get a vector of integers， So 0。 if the largest probability
    was in the zeroth position，1 in the first position and so on。 So these are our
    class predictions indicating class 0， class 1 and so on。 In fact。 if class predictions
    are all you want。 you can skip the softm step entirely because the largest logic
    will always be the largest probability as well。
  prefs: []
  type: TYPE_NORMAL
- en: 😊，So if probabilities and class predictions are all you want。 then you've seen
    everything you need at this point。 but if you're interested in benchmarking your
    model or using it for research。 you might want to delve deeper into the results
    you get and one way to do that is to compute some metrics for the model's predictions。
  prefs: []
  type: TYPE_NORMAL
- en: If you're following along with our dataset sets and fine tuning videos。 we got
    our data from the MRRPC data， which is part of the blue benchmark。Each of the
    blue dataset sets， as well as many other datasets in our datasets light hub has
    some predefined metrics。 and we read we can load them easily with the dataset's
    load metric function。
  prefs: []
  type: TYPE_NORMAL
- en: For the M or PC dataset set， the built in metrics are accuracy。 which just measures
    the percentage of time the model's prediction was correct， and the F1 score。 which
    is a slightly more complex measure that measures how well the model trades off
    precision and recall。To compute those metrics to benchmark our model， we just
    pass them the model's predictions under the ground truth labels and we get our
    results in a straightforward dip Python dit。
  prefs: []
  type: TYPE_NORMAL
- en: 😊，If you're familiar with Keis， though， you might notice that this is a slightly
    weird way to compute metrics because we're only computing metrics at the very
    end of training。 but in Keis， you have this built in ability to compute a wide
    range of metrics on the fly while you're training。
  prefs: []
  type: TYPE_NORMAL
- en: which gives you a very useful insight into how training is going。So if you want
    to use built in metrics， it's very straightforward and you use the standard careis
    approach again。 you just pass a metric argument to the compile method。 As with
    things like loss and optimizer。 you can specify the metrics you want by string
    or you can import the actual metric objects and pass specific arguments to them。
  prefs: []
  type: TYPE_NORMAL
- en: but note that unlike loss and accuracy， you have to supply metrics as a list。
    even if there's only one metric you want。Once a model has been compiled with a
    metric。 it will report that metric for training， validation and predictions。Assuming
    there are labels passed to the predictions， you can even write your own metric
    classes。
  prefs: []
  type: TYPE_NORMAL
- en: although this is a bit beyond the scope of this course。 I'll link to the relevant
    T F docs below because it can be very handy if you want a metric that isn't supported
    by default in Caris。 such as the F1 score。![](img/beadff4afadc3bf0228241989afe4e3f_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: 。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/beadff4afadc3bf0228241989afe4e3f_3.png)'
  prefs: []
  type: TYPE_IMG
