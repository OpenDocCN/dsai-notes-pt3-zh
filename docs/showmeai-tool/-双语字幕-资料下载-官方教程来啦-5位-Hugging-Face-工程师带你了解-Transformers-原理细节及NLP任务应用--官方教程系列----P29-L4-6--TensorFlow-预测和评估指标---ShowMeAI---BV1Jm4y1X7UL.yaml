- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P29ï¼šL4.6- TensorFlow é¢„æµ‹å’Œè¯„ä¼°æŒ‡æ ‡ - ShowMeAI - BV1Jm4y1X7UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our other videosã€‚ and as alwaysï¼Œ there'll be links below if you want to check
    those outã€‚ We showed you how to initialize and finet a transformer model in Tensorflowã€‚
    So the question now isã€‚ what can we do with a model after we train itã€‚ The obvious
    thing to try is to use it to get predictions for new dataã€‚ So let's see how to
    do thatï¼Œ Againï¼Œ if you're familiar with Kaisã€‚
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that because these are just standard caris modelsã€‚ we can use
    the standard Keis predict method as shown hereã€‚ğŸ˜Šã€‚You simply pass in tokenized
    text to this method like you'd get from a tokenizer and you get your resultsã€‚Our
    models can output several different thingsï¼Œ depending on the options you saidã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but most of the timeï¼Œ the thing you want is the output logtsã€‚ If you haven't
    come across them beforeï¼Œ Los sometimes pronounced logitsã€‚ because no one's sure
    are the outputs of the last layer of the network because before a softm has been
    appliedã€‚ So if you want to turn the logics into the model's probability outputsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you just apply a softm like soã€‚What if we wantï¼Œ But what if we want to turn
    those probabilities into class predictionsã€‚ Againï¼Œ it's very straightforwardã€‚
    We just pick the biggest probability for each outputã€‚ and you can get that immediately
    with the Argm functionã€‚ Argmax will return the index of the largest probability
    in each rowã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which means that it will get a vector of integersï¼Œ So 0ã€‚ if the largest probability
    was in the zeroth positionï¼Œ1 in the first position and so onã€‚ So these are our
    class predictions indicating class 0ï¼Œ class 1 and so onã€‚ In factã€‚ if class predictions
    are all you wantã€‚ you can skip the softm step entirely because the largest logic
    will always be the largest probability as wellã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒSo if probabilities and class predictions are all you wantã€‚ then you've seen
    everything you need at this pointã€‚ but if you're interested in benchmarking your
    model or using it for researchã€‚ you might want to delve deeper into the results
    you get and one way to do that is to compute some metrics for the model's predictionsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: If you're following along with our dataset sets and fine tuning videosã€‚ we got
    our data from the MRRPC dataï¼Œ which is part of the blue benchmarkã€‚Each of the
    blue dataset setsï¼Œ as well as many other datasets in our datasets light hub has
    some predefined metricsã€‚ and we read we can load them easily with the dataset's
    load metric functionã€‚
  prefs: []
  type: TYPE_NORMAL
- en: For the M or PC dataset setï¼Œ the built in metrics are accuracyã€‚ which just measures
    the percentage of time the model's prediction was correctï¼Œ and the F1 scoreã€‚ which
    is a slightly more complex measure that measures how well the model trades off
    precision and recallã€‚To compute those metrics to benchmark our modelï¼Œ we just
    pass them the model's predictions under the ground truth labels and we get our
    results in a straightforward dip Python ditã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒIf you're familiar with Keisï¼Œ thoughï¼Œ you might notice that this is a slightly
    weird way to compute metrics because we're only computing metrics at the very
    end of trainingã€‚ but in Keisï¼Œ you have this built in ability to compute a wide
    range of metrics on the fly while you're trainingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which gives you a very useful insight into how training is goingã€‚So if you want
    to use built in metricsï¼Œ it's very straightforward and you use the standard careis
    approach againã€‚ you just pass a metric argument to the compile methodã€‚ As with
    things like loss and optimizerã€‚ you can specify the metrics you want by string
    or you can import the actual metric objects and pass specific arguments to themã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but note that unlike loss and accuracyï¼Œ you have to supply metrics as a listã€‚
    even if there's only one metric you wantã€‚Once a model has been compiled with a
    metricã€‚ it will report that metric for trainingï¼Œ validation and predictionsã€‚Assuming
    there are labels passed to the predictionsï¼Œ you can even write your own metric
    classesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: although this is a bit beyond the scope of this courseã€‚ I'll link to the relevant
    T F docs below because it can be very handy if you want a metric that isn't supported
    by default in Carisã€‚ such as the F1 scoreã€‚![](img/beadff4afadc3bf0228241989afe4e3f_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/beadff4afadc3bf0228241989afe4e3f_3.png)'
  prefs: []
  type: TYPE_IMG
