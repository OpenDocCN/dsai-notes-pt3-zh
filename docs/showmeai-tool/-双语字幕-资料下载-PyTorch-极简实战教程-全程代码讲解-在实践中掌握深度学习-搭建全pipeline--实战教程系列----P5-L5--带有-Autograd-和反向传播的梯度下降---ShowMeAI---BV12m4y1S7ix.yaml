- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P5ÔºöL5- Â∏¶Êúâ
    Autograd ÂíåÂèçÂêë‰º†Êí≠ÁöÑÊ¢ØÂ∫¶‰∏ãÈôç - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to a new Pytorch tutorial„ÄÇ In this tutorial„ÄÇ I show you
    a concrete example of how we optimize our model with automatic gradient computation
    using the pytorch autogra package„ÄÇ So we start by implementing the linear regression
    algorithm from scratch„ÄÇ where we do every step manually„ÄÇ So we implement the equations
    to calculate the model prediction and the loss function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we do a numerical computation of the gradients and implement the formula„ÄÇ
    And then we implement the gradient decent algorithm to optimize our parameters„ÄÇ
    When this is workingÔºå we see how we can replace the manually computed gradients
    with the automatic back propagation algorithm from Pytorch„ÄÇüòäÔºåSo this is step number
    2„ÄÇ and in the third stepsÔºå we in the third step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we replace the manually computed loss and parameter updates by using the loss
    and optimize the classes in Pytorch„ÄÇ and in the final stepÔºå we replaced the manually
    computed model prediction by implementing a pytorch model„ÄÇSo when we understood
    each of these stepsÔºå Pytorch can do most of the work for us„ÄÇ Of course„ÄÇ we still
    have to to design our model and have to know which loss and optimizer we should
    use„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But we don't have to worry about the underlying algorithms any more„ÄÇSo now this
    video will cover steps 1 and 2„ÄÇAnd in the next videoÔºå we will see the steps 3
    and 4„ÄÇ So let's start„ÄÇ And I assume that you already know how linear regression
    and gradient decent works„ÄÇ And if notÔºå then please watch my machine learning from
    scratch tutorial about this algorithm„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because now I will not explain all the steps in detail„ÄÇ![](img/49d809dfcdcd868a09e2ad42202f36fc_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: But I put the link in the description„ÄÇSo now we do everything from scratch„ÄÇ
    So we use only nuy„ÄÇ So we import nuy S and PÔºå and then we use linear regression„ÄÇ
    So we use a function which just just does a linear combination of some weights
    and our inputs„ÄÇ and we don't care about the bias here„ÄÇSo in our exampleÔºå let's
    say F equals„ÄÇTwo times x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So our weight must be 2„ÄÇ And then let's do some training samples„ÄÇ So let's say
    x„ÄÇEquals numpy dot array„ÄÇ And then we put some tests or training samples„ÄÇ So let's
    say 1Ôºå2Ôºå3Ôºå and 4„ÄÇ And this will be of numpy„ÄÇOrÔºå let's get„ÄÇGive this a data type
    onÔºå say this is Nmpy dot floatat 32„ÄÇAnd thenÔºå we also want a y„ÄÇAnd„ÄÇSince our formula
    isÔºå this is 2 x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have to multiply each of the values by 2„ÄÇ So 2Ôºå4Ôºå6 and 8„ÄÇAnd now we initialize
    our weights„ÄÇ So we simply say w equals 0 in the beginning„ÄÇAnd now we have to calculate
    our model prediction„ÄÇ And we also have to calculate the loss„ÄÇAnd then we have
    to calculate the gradients„ÄÇ So now we do each of these steps manually„ÄÇ So let's
    define a functionÔºå and we call this forward„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is a forward pass to follow the conventions of pytorrchÔºå which will
    get x„ÄÇ and then our model output is simply w times x„ÄÇSo this is the forward pass„ÄÇNowÔºå
    the loss„ÄÇSo here we define the function lossÔºå which depends on why and why predicted„ÄÇSo„ÄÇThe
    this is the model output„ÄÇAnd now here in this case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is the or the loss equals the mean squared error in the case of linear
    regression„ÄÇ And we can calculate this by sayingÔºå this is„ÄÇÂóØ„ÄÇLet's say y predict
    it minus y„ÄÇ And then to the power of 2„ÄÇ And then we do the mean operation„ÄÇSo this
    is the loss„ÄÇ and now we manually have have to calculate the gradient of the loss
    with respect to our parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look at the mean squared error„ÄÇ So the formula is1 over n because
    it's the mean„ÄÇ And then we have our w times x our prediction minus the actual
    value to the power of2„ÄÇ And now if you want to have the derivative„ÄÇ So the derivative
    of this„ÄÇ let's call this j or objective function with respect to w equals1 over
    n And then we have two times x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then times W times x minus y„ÄÇ So this is the numerical computationd„ÄÇComputed
    derivative„ÄÇ Also please double check the math for yourself„ÄÇAnd now we implement
    this„ÄÇ So we say define gradient„ÄÇ which is dependent„ÄÇOn x and y and also Y predicted„ÄÇ
    And now we can do this in one line„ÄÇ So we return numpy dot„ÄÇ We need a dot product
    of two times x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49d809dfcdcd868a09e2ad42202f36fc_3.png)'
  prefs: []
  type: TYPE_IMG
- en: And then here we have y predicted minus y„ÄÇ![](img/49d809dfcdcd868a09e2ad42202f36fc_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Said this is this formula here„ÄÇAnd thenÔºå of courseÔºå we also need the mean„ÄÇ So
    let's say this is dot mean„ÄÇ We can simply do it like this in Ny„ÄÇAnd now„ÄÇYeahÔºå
    these are the„ÄÇThings we need NowÔºå let's print our„ÄÇPrediction before the training„ÄÇ
    So let's print„ÄÇ and we use an F string„ÄÇ So prediction before training„ÄÇAnd let's
    say we want to predict the value5„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which should be 10„ÄÇAnd hereÔºå we can do„ÄÇIn the F string„ÄÇ we can actually use
    an expression so we can call this forward method and with five„ÄÇAnd let's say we
    only want three decimal values„ÄÇAnd nowÔºå let's start our training„ÄÇSo let's define
    some parameters„ÄÇ So we need a learning rateÔºå which isÔºå let's sayÔºå001„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we need a number of iterations„ÄÇ So we say an its equals 10„ÄÇAnd now
    let's do our training loop„ÄÇ So we say for epoch in range and iters„ÄÇAnd then firstÔºå
    we do the„ÄÇPredictionÔºå which is the forward pass„ÄÇ So this is the forward pass„ÄÇAnd
    we can simply do this with our functions„ÄÇ So we say y prediction or y pre equals
    forward„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we put in our capital x„ÄÇAnd now we want to have the loss„ÄÇ So our loss
    L equals the loss of„ÄÇThe actual y„ÄÇAnd our Y predict it„ÄÇNowÔºå we need to get the„ÄÇGrasÔºå
    so our gradientsÔºå with respect to W„ÄÇ So D W equals the gradient function that
    we just implemented„ÄÇ which is dependent on x and y and the y predicted„ÄÇsrry„ÄÇWhy
    pret„ÄÇAnd„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to update our weights„ÄÇ and yesÔºå so the update formula in the gradient
    decentcent algorithm is just we go into the negative direction of the training
    of the gradient„ÄÇ So minus x„ÄÇ And then hereÔºå the step width or the so called learning
    rate times our our gradient„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the update formula„ÄÇAnd then let's say we also want to print some
    information here„ÄÇ So we say if epoch modularÔºå just say one hereÔºå because now we
    want to print every step„ÄÇIf this is 0„ÄÇ we want to printÔºå let's say„ÄÇWe want to
    print the epoch„ÄÇ And here we print epoch plus 1„ÄÇAnd then we want to get the weightÔºå
    which is„ÄÇThe weight„ÄÇWÔºå alsoÔºå just 3„ÄÇDeimal numbers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we also want to have the loss„ÄÇ So the loss equals„ÄÇThe loss„ÄÇ And here
    we sayÔºå point„ÄÇ8„ÄÇ let's say„ÄÇAnd yeahÔºå and then at the end„ÄÇWe want to print the
    prediction after the training„ÄÇSoÔºå now„ÄÇ let's predict„ÄÇPrediict a print prediction„ÄÇAfter
    training„ÄÇAnd yeahÔºå so„ÄÇ Now„ÄÇ let's run this and see what happens„ÄÇSo everything
    should be working„ÄÇ And nowÔºå so yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: before our trainingÔºå the prediction is0„ÄÇAnd then for each step„ÄÇ remember that
    our formula should be  two times x„ÄÇ So our w should be 2 in the beginning„ÄÇ And
    we see that with each training stepÔºå it it increases our weightÔºå and it decreases
    our loss„ÄÇ So it gets better with every step„ÄÇ And after the trainingÔºå our model
    prediction is 9„ÄÇ999„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it's almost there„ÄÇ So let's sayÔºå for exampleÔºå now we want to have more iterations
    here„ÄÇ say we only did we only did 10 iterationsÔºå which is not much„ÄÇ Now„ÄÇ if we
    run this and let's print every second step only„ÄÇThen we seeÔºå in the endÔºå our loss
    is 0„ÄÇ and the prediction is correct„ÄÇNowÔºå this is the implementation where we did
    everything manually„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now let's replace the gradient calculation„ÄÇ So let's select all of this
    and copy this into a separate file„ÄÇAnd now we don't use numpy anymore„ÄÇ So now
    let's only import torch and do everything with pie torch„ÄÇAndÔºå of courseÔºå what
    we now want to get rid of is this gradientÔºå the the manually computed gradient„ÄÇ
    So we simply delete this„ÄÇ We don't need this anymore„ÄÇAnd now we don't have nuy
    arrays„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is now a„ÄÇTorch dot Tzar„ÄÇAnd our data type is now a torch dot float 32
    and the same with our y„ÄÇ which is now a„ÄÇTorch dot„ÄÇTensor and also the data types
    from the torch module„ÄÇBut everything else is the same here„ÄÇ So the same syntax„ÄÇAnd
    now our W also has to be a tenor„ÄÇ So let's say this is a torch dot tenor with0„ÄÇ0
    in the beginning„ÄÇ and it also gets a data type of„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: say torch dot float 32„ÄÇ And since we are interested in the gradient of our loss
    with respect to this parameter„ÄÇ we need to specify that this requires the gradient
    computation„ÄÇ So requires gra equals true„ÄÇ![](img/49d809dfcdcd868a09e2ad42202f36fc_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: NowÔºå the forward function and the loss function is still the same because we
    can use the same syntax in pytorrch„ÄÇAnd now„ÄÇIn our training loopÔºå the forward
    pass is still the same„ÄÇ The loss is the same„ÄÇ And now the gradientÔºå this is the
    equal to the backward pass„ÄÇ So rememberÔºå in backward propagation„ÄÇ we first do
    a forward pass„ÄÇ That's why we used the syntax„ÄÇ And then later for the gradients„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we use the backward pass„ÄÇ So here we simply call L dot„ÄÇBackward„ÄÇAnd this will
    calculate the gradient of our loss with respect to W„ÄÇ So Pyto does all the computations
    for us„ÄÇ And now we update our weights„ÄÇ But here we want to be careful„ÄÇ and I explained
    this in the tutorial about the auto gridd package„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Because we don't want to be„ÄÇ this operation should not be part of our„ÄÇGraadient
    trackingrek graph„ÄÇ So this should not be part of the computational graph„ÄÇ So we
    need to wrap this in a width torch dot„ÄÇ no gr statement„ÄÇAnd then one more thing
    that we should also know„ÄÇ And I also talked about this alreadyÔºå we must empty
    or  zero the gradients again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because whenever we call backwardÔºå it will write our„ÄÇGrads and accumulate them
    in the W dot Gr attribute„ÄÇ So before the next iteration„ÄÇ we want to make sure
    that our gradients are 0 again„ÄÇ So we can say w times gra times0 underscore„ÄÇ So
    this will modify it in place„ÄÇAnd nowÔºå we are done„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now let's run this and see if this is working„ÄÇAnd W is not defined„ÄÇ OhÔºå
    yeahÔºå CoÔºå Of course„ÄÇ this is now W dot gra„ÄÇAnd let's try this again„ÄÇ and now it's
    working„ÄÇAnd now we also see that it will increase our WÔºå and it will decrease
    our loss„ÄÇ And here we said we had 20 iterations„ÄÇBut it's not correctÔºå Not completely
    correct„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is because the„ÄÇBackward or the back propagation is not as exact as
    the numerical gradient computation„ÄÇ So let's try some more iterations here„ÄÇ Let's
    say we want 100 iterations and print our update every 10th step„ÄÇ So let's try
    this again„ÄÇAnd now we see after the training is done„ÄÇ we have the correct correct
    prediction„ÄÇSoÔºå yeahÔºå that's it for this video„ÄÇ And in the next video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we will continue here and replace the manually computed loss and weight updates
    with Pytorch loss and optimizer classes„ÄÇSo if you like this videoÔºå please subscribe
    to the channel and see you next timeÔºå bye„ÄÇ![](img/49d809dfcdcd868a09e2ad42202f36fc_9.png)
  prefs: []
  type: TYPE_NORMAL
