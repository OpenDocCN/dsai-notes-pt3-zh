- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12-
    Windows ä¸Šçš„ PyTorch - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorchè¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é”PyTorchçš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12-
    Windowsä¸Šçš„PyTorch - ShowMeAI - BV1ZZ4y1U7dg
- en: ğŸ¼ã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_1.png)
- en: Helloï¼Œ let's talk about recent improvements we made in Windows Pla supportã€‚I'm
    Max Lucano Product manager working at Microsoftï¼Œ in Azure AI platform Teamã€‚Some
    of you might wonder why Microsoft is talking about Pytorch and Windowsï¼Œ let me
    clear this upã€‚Recentlyï¼Œ Microsoft has become a main of Pythtune Windowsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæˆ‘ä»¬æ¥è°ˆè°ˆæˆ‘ä»¬åœ¨Windowså¹³å°æ”¯æŒæ–¹é¢æœ€è¿‘çš„æ”¹è¿›ã€‚æˆ‘æ˜¯Max Lucanoï¼Œå¾®è½¯Azure AIå¹³å°å›¢é˜Ÿçš„äº§å“ç»ç†ã€‚ä½ ä»¬å¯èƒ½ä¼šæƒ³ï¼Œå¾®è½¯ä¸ºä»€ä¹ˆè¦è°ˆè®ºPytorchå’ŒWindowsï¼Œä¸‹é¢æˆ‘æ¥æ¾„æ¸…ä¸€ä¸‹ã€‚æœ€è¿‘ï¼Œå¾®è½¯æˆä¸ºäº†Pytorotch
    Windowsçš„ä¸»è¦æ”¯æŒè€…ã€‚
- en: Our goal is to improve quality of the Windows platform support so Windows users
    have great experience building AI applications on the platform of their choiceã€‚In
    this segmentï¼Œ we' will talk about recent progress we have made in enabling various
    features of Pytorch on Windows platformã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ”¹å–„Windowså¹³å°æ”¯æŒçš„è´¨é‡ï¼Œä»¥ä¾¿Windowsç”¨æˆ·åœ¨è‡ªå·±é€‰æ‹©çš„å¹³å°ä¸Šæ„å»ºAIåº”ç”¨æ—¶è·å¾—è‰¯å¥½çš„ä½“éªŒã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è°ˆè®ºæˆ‘ä»¬åœ¨Windowså¹³å°ä¸Šå¯ç”¨Pytorchå„é¡¹åŠŸèƒ½æ–¹é¢å–å¾—çš„æœ€æ–°è¿›å±•ã€‚
- en: The recent one of the major features that we have enabled in the current release
    is distributed trainingã€‚ so we'll talk a little bit about thatã€‚And we'll also
    demo how distribute the training workss on Windowsã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_3.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œæˆ‘ä»¬åœ¨å½“å‰ç‰ˆæœ¬ä¸­å¯ç”¨çš„ä¸»è¦åŠŸèƒ½ä¹‹ä¸€æ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼Œå› æ­¤æˆ‘ä»¬å°†ç¨å¾®è°ˆä¸€ä¸‹è¿™ä¸ªå†…å®¹ï¼Œå¹¶æ¼”ç¤ºå¦‚ä½•åœ¨Windowsä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_3.png)
- en: But firstï¼Œ let's talk about YV as a Python community should care about Windows
    platformã€‚According to latest SeflowDevelop surveyï¼Œ Windows remains to be most
    popular operating system among the broadest audience of developersã€‚And this might
    not be the case for today's Pythers communityã€‚Because today's Pyths community
    is biased more towards professional researchers who are comfortable working with
    Linux and Maccosã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é¦–å…ˆï¼Œè®©æˆ‘ä»¬è°ˆè°ˆä½œä¸ºPythonç¤¾åŒºçš„YVï¼Œåº”è¯¥å…³å¿ƒWindowså¹³å°ã€‚æ ¹æ®æœ€æ–°çš„SeflowDevelopè°ƒæŸ¥ï¼ŒWindowsä»ç„¶æ˜¯æœ€å—å¹¿æ³›å¼€å‘è€…æ¬¢è¿çš„æ“ä½œç³»ç»Ÿã€‚è€Œè¿™å¯¹äºä»Šå¤©çš„Pythorsç¤¾åŒºæ¥è¯´å¯èƒ½å¹¶éå¦‚æ­¤ï¼Œå› ä¸ºå¦‚ä»Šçš„Pythorsç¤¾åŒºæ›´å€¾å‘äºé‚£äº›ä¹ æƒ¯ä½¿ç”¨Linuxå’ŒMacçš„ä¸“ä¸šç ”ç©¶äººå‘˜ã€‚
- en: But if you look into the futureï¼Œ if you look into the next set of users that
    we would like to bring into Pytororchã€‚ then Windows users would be the greatest
    of the largest setã€‚And this is further confirmed by our own user research if you
    look at the LinkedIn profilesã€‚ then the number of machine learning professionals
    people who claim ML proficiency on the LinkedIn profiles and but who don't have
    PhD degrees is seven times larger than the number of PhDs and even also in the
    last year this number of non PhD ML professionals have grown almost doubled in
    the last yearã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ å±•æœ›æœªæ¥ï¼Œçœ‹çœ‹æˆ‘ä»¬å¸Œæœ›å¼•å…¥Pytorchçš„ä¸‹ä¸€æ‰¹ç”¨æˆ·ï¼Œé‚£ä¹ˆWindowsç”¨æˆ·å°†æ˜¯æœ€å¤§çš„ç”¨æˆ·ç¾¤ä½“ã€‚è¿™ä¸€ç‚¹é€šè¿‡æˆ‘ä»¬è‡ªå·±çš„ç”¨æˆ·ç ”ç©¶å¾—åˆ°äº†è¿›ä¸€æ­¥ç¡®è®¤ï¼Œå¦‚æœæŸ¥çœ‹LinkedInçš„ä¸ªäººèµ„æ–™ï¼Œé‚£ä¹ˆå£°ç§°å…·å¤‡æœºå™¨å­¦ä¹ èƒ½åŠ›ä½†æ²¡æœ‰åšå£«å­¦ä½çš„æœºå™¨å­¦ä¹ ä¸“ä¸šäººå£«äººæ•°æ˜¯åšå£«ç”Ÿäººæ•°çš„ä¸ƒå€ï¼Œè€Œåœ¨è¿‡å»çš„ä¸€å¹´ä¸­ï¼Œè¿™äº›éåšå£«çš„æœºå™¨å­¦ä¹ ä¸“ä¸šäººå£«æ•°é‡å‡ ä¹ç¿»äº†ä¸€ç•ªã€‚
- en: well the number of PhDs obviously haven't grown that muchã€‚So this means that
    potential future users of Pythtorch are software engineers and analysts and those
    users who have great care for Windows platformã€‚Let's talk about the state of Pytorchã€‚First
    of allã€‚ Pytorch and Windows is a community effort and through this community effortã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œåšå£«äººæ•°æ˜¾ç„¶æ²¡æœ‰å¢é•¿å¤ªå¤šã€‚è¿™æ„å‘³ç€æœªæ¥ä½¿ç”¨Pytorchçš„æ½œåœ¨ç”¨æˆ·æ˜¯è½¯ä»¶å·¥ç¨‹å¸ˆå’Œåˆ†æå¸ˆï¼Œä»¥åŠé‚£äº›éå¸¸å…³æ³¨Windowså¹³å°çš„ç”¨æˆ·ã€‚æˆ‘ä»¬æ¥è°ˆè°ˆPytorchçš„ç°çŠ¶ã€‚é¦–å…ˆï¼ŒPytorchå’ŒWindowsæ˜¯ä¸€ä¸ªç¤¾åŒºåŠªåŠ›çš„ç»“æœï¼Œé€šè¿‡è¿™ä¸ªç¤¾åŒºçš„åŠªåŠ›ã€‚
- en: we have achieved fairly good state actually in terms of breadth of coverage
    of various features of Pytororchã€‚ we have as you can see from a table we have
    a good coverage as wellã€‚There are some gaps when it comes to depth of coverage
    of the featuresã€‚ and so we'll need to do a bit more work thereã€‚The original acknowledgement
    should go to Je and Poo was the original author of who brought the Windows platform
    support to Pythtorchã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬åœ¨Pytorchçš„å„é¡¹åŠŸèƒ½è¦†ç›–èŒƒå›´æ–¹é¢å–å¾—äº†ç›¸å½“ä¸é”™çš„çŠ¶æ€ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„è¡¨æ ¼ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰å¾ˆå¥½çš„è¦†ç›–ã€‚ä¸è¿‡åœ¨åŠŸèƒ½æ·±åº¦çš„è¦†ç›–ä¸Šè¿˜æœ‰ä¸€äº›ç©ºç™½ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åœ¨è¿™æ–¹é¢åšæ›´å¤šçš„å·¥ä½œã€‚æœ€åˆçš„æ„Ÿè°¢åº”è¯¥å½’åŠŸäºJeå’ŒPooï¼Œä»–ä»¬æ˜¯å°†Windowså¹³å°æ”¯æŒå¼•å…¥Pytorchçš„åŸä½œè€…ã€‚
- en: he basically single handedly implemented original Windows support and he recently
    also added Torch audio support as wellã€‚On our side at Microsoftï¼Œ we have started
    actually working on Windows and now we have four contributors working in the space
    and two of them have become long term maintainersã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–åŸºæœ¬ä¸Šç‹¬è‡ªå®ç°äº†åŸå§‹çš„Windowsæ”¯æŒï¼Œæœ€è¿‘è¿˜æ·»åŠ äº†TorchéŸ³é¢‘æ”¯æŒã€‚åœ¨å¾®è½¯æ–¹é¢ï¼Œæˆ‘ä»¬å®é™…ä¸Šå·²ç»å¼€å§‹åœ¨Windowsä¸Šå·¥ä½œï¼Œç°åœ¨æˆ‘ä»¬æœ‰å››ä½è´¡çŒ®è€…åœ¨è¿™ä¸ªé¢†åŸŸå·¥ä½œï¼Œå…¶ä¸­ä¸¤ä½å·²ç»æˆä¸ºé•¿æœŸç»´æŠ¤è€…ã€‚
- en: We started with improving quality of the Pythtors builtã€‚ so we added test automation
    to bring it on par with Linuxï¼Œ the improved tutorialsã€‚ so new users of Pyths have
    great experience learning Pythtorch and Windowsã€‚And this releaseã€‚ our major releaseï¼Œ
    the features that we have enabled is distributed trainingã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼€å§‹æ”¹è¿›Pythtorsçš„è´¨é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¢åŠ äº†æµ‹è¯•è‡ªåŠ¨åŒ–ï¼Œä½¿å…¶ä¸Linuxå¹³èµ·å¹³åï¼Œå¹¶æ”¹è¿›äº†æ•™ç¨‹ã€‚æ–°çš„Pythç”¨æˆ·åœ¨å­¦ä¹ Pythtorchå’ŒWindowsæ—¶æœ‰å¾ˆå¥½çš„ä½“éªŒã€‚åœ¨æ­¤æ¬¡å‘å¸ƒä¸­ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç‰¹æ€§æ˜¯åˆ†å¸ƒå¼è®­ç»ƒã€‚
- en: and we'll talk a little bit more about that on the next slideã€‚We also partnered
    with Vi Amazon on bringingian Windows support to Torservve and Elijah Repert's
    worked on bringing Windows support to job Bi as wellã€‚ great thank you here as
    wellã€‚So this will be the trainingã€‚First of allã€‚ it's available now in the latest
    1ã€‚7 version of Pythï¼Œ so if you install it on Windowsã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€å¼ å¹»ç¯ç‰‡ä¸Šè¯¦ç»†è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¿˜ä¸Vi Amazonåˆä½œï¼Œä¸ºTorservveå¸¦æ¥äº†Windowsæ”¯æŒï¼ŒElijah Repertä¹Ÿåœ¨ä¸ºjob
    Biæä¾›Windowsæ”¯æŒã€‚éå¸¸æ„Ÿè°¢å¤§å®¶çš„åŠªåŠ›ã€‚è¿™å°†æ˜¯åŸ¹è®­çš„å†…å®¹ã€‚é¦–å…ˆï¼Œæœ€æ–°çš„1.7ç‰ˆæœ¬çš„Pythç°åœ¨å¯ä»¥ä½¿ç”¨ï¼Œå¦‚æœä½ åœ¨Windowsä¸Šå®‰è£…å®ƒã€‚
- en: you' will have the binaries that support distributed training alreadyã€‚Both multi
    GPU and multinode configurations are supported one cave here though is we haven't
    spent much time testing multin yetã€‚ so please expect it to be unstable at this
    pointï¼Œ but we plan to do additional testing in the upcoming releasesã€‚Pythtorarch
    has two types of distributed training distributed data parallel and a new RPC
    framework in this release we enable an distributed training distribute the data
    parallel mode and we look forward to enabling RPC in the upcoming release as well
    RPC is exciting new features that is going to be used in a variety of the new
    features of Pythtorch so it's important to bring that to Windows as wellã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†æ‹¥æœ‰æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒçš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚æ”¯æŒå¤šGPUå’Œå¤šèŠ‚ç‚¹é…ç½®ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰èŠ±å¾ˆå¤šæ—¶é—´æµ‹è¯•multinã€‚å› æ­¤ï¼Œè¯·æœŸå¾…ç›®å‰å¯èƒ½ä¸ç¨³å®šï¼Œä½†æˆ‘ä»¬è®¡åˆ’åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­è¿›è¡Œé¢å¤–æµ‹è¯•ã€‚Pythtorarchæœ‰ä¸¤ç§ç±»å‹çš„åˆ†å¸ƒå¼è®­ç»ƒï¼šåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œå’Œæ–°çš„RPCæ¡†æ¶ã€‚åœ¨æ­¤æ¬¡å‘å¸ƒä¸­ï¼Œæˆ‘ä»¬å¯ç”¨äº†åˆ†å¸ƒå¼è®­ç»ƒçš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¨¡å¼ï¼Œå¹¶æœŸå¾…åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­å¯ç”¨RPCã€‚RPCæ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„æ–°ç‰¹æ€§ï¼Œå°†ç”¨äºPythtorchçš„å„ç§æ–°ç‰¹æ€§ï¼Œå› æ­¤å°†å…¶å¼•å…¥Windowsä¹Ÿå¾ˆé‡è¦ã€‚
- en: The situation is a little bit more complicated on the distributed drain backend
    front in the current release we have ambulance glue backend but the niel and MPPI
    backends are not currently supported the situation with NikIel is even more complicated
    because NikIel is supported by Nvidia and Nvidia doesn't provide support for Windows
    platform so we will have to work with Nvidia and figuring out what the plan here
    for NikIel backendã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰ç‰ˆæœ¬åœ¨åˆ†å¸ƒå¼è®­ç»ƒåç«¯çš„æƒ…å†µç¨å¾®å¤æ‚ä¸€äº›ï¼Œæˆ‘ä»¬æœ‰æ•‘æŠ¤è½¦èƒ¶æ°´åç«¯ï¼Œä½†nielå’ŒMPPIåç«¯å½“å‰ä¸å—æ”¯æŒã€‚ä¸NikIelçš„æƒ…å†µæ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºNikIelç”±Nvidiaæ”¯æŒï¼Œè€ŒNvidiaä¸ä¸ºWindowså¹³å°æä¾›æ”¯æŒï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸Nvidiaåˆä½œï¼Œç¡®å®šNikIelåç«¯çš„è®¡åˆ’ã€‚
- en: '![](img/3eee7b9d5034fa1a327cea58b77217c6_5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3eee7b9d5034fa1a327cea58b77217c6_5.png)'
- en: With thatï¼Œ we are ready to dive in and to actually see in action how byytor
    Windows worksã€‚So what I have here is an Azure virtual machine equipped with a
    powerful DPã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_7.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬å‡†å¤‡æ·±å…¥äº†è§£byytor Windowsçš„å®é™…æ“ä½œã€‚æˆ‘è¿™é‡Œæœ‰ä¸€å°é…å¤‡å¼ºå¤§DPçš„Azureè™šæ‹Ÿæœºã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_7.png)
- en: So let's see in VDdia SMMIï¼Œ you can see that we have two NviDAV100 GPUs on this
    machineã€‚ so that should be plenty of horsepower to do some speed up improvement
    for our model trainingã€‚ so let's see what we can do hereã€‚I also have Pytorchs
    latest version of Pytorrch installed hereã€‚ of courseï¼Œ 1ã€‚7 with Kudo supportã€‚And
    now we can go to Pythtorch tutorials and see what kind ofã€‚å—¯ã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨VDdia SMMIä¸­æŸ¥çœ‹ä¸€ä¸‹ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™å°æœºå™¨ä¸Šæœ‰ä¸¤å—NviDAV100 GPUï¼Œå› æ­¤è¿™åº”è¯¥æœ‰è¶³å¤Ÿçš„æ€§èƒ½æ¥åŠ é€Ÿæˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬èƒ½åœ¨è¿™é‡Œåšäº›ä»€ä¹ˆã€‚æˆ‘è¿™é‡Œä¹Ÿå®‰è£…äº†Pytorchçš„æœ€æ–°ç‰ˆæœ¬ï¼Œå½“ç„¶æ˜¯1.7ï¼Œå¹¶ä¸”æ”¯æŒKudoã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å»Pythtorchæ•™ç¨‹ï¼Œçœ‹çœ‹æœ‰ä»€ä¹ˆæ ·çš„å†…å®¹ã€‚å—¯ã€‚
- en: Sample we can use here so Python tutorialsï¼Œ as I mentionedã€‚ we have improved
    some of them and actually all of them and enabled their smooth separation on Windows
    platform so let's choose this getting started with distributed data parallel so
    this tutorial guides users through three examples of various types of distributed
    data parallel training and it provides sample code to perform that on their own
    machineã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„ç¤ºä¾‹æ˜¯Pythonæ•™ç¨‹ï¼Œæ­£å¦‚æˆ‘æåˆ°çš„ã€‚æˆ‘ä»¬æ”¹è¿›äº†å…¶ä¸­ä¸€äº›ï¼Œå®é™…ä¸Šæ˜¯æ‰€æœ‰çš„ï¼Œå¹¶ä¸”åœ¨Windowså¹³å°ä¸Šå®ç°äº†å®ƒä»¬çš„å¹³æ»‘åˆ†ç¦»ï¼Œå› æ­¤æˆ‘ä»¬é€‰æ‹©è¿™ä¸ªå…³äºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œçš„å…¥é—¨æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹é€šè¿‡ä¸‰ä¸ªä¸åŒç±»å‹çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒç¤ºä¾‹å¼•å¯¼ç”¨æˆ·ï¼Œå¹¶æä¾›ç¤ºä¾‹ä»£ç ä¾›ä»–ä»¬åœ¨è‡ªå·±çš„æœºå™¨ä¸Šæ‰§è¡Œã€‚
- en: you can see that we added some Windows specific instructions here to make sure
    that users succeed on the Windows platformã€‚I have copied this source code into
    my virtual machineã€‚ so now we have this tutorial represented in my visual studio
    code environmentã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åœ¨è¿™é‡Œæ·»åŠ äº†ä¸€äº›ç‰¹å®šäºWindowsçš„è¯´æ˜ï¼Œä»¥ç¡®ä¿ç”¨æˆ·åœ¨Windowså¹³å°ä¸ŠæˆåŠŸã€‚æˆ‘å·²ç»å°†è¿™ä¸ªæºä»£ç å¤åˆ¶åˆ°æˆ‘çš„è™šæ‹Ÿæœºä¸­ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬åœ¨æˆ‘çš„Visual
    Studio Codeç¯å¢ƒä¸­å±•ç¤ºäº†è¿™ä¸ªæ•™ç¨‹ã€‚
- en: '![](img/3eee7b9d5034fa1a327cea58b77217c6_9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3eee7b9d5034fa1a327cea58b77217c6_9.png)'
- en: And we can spend a little bit of time exploringï¼Œ but models we are actually
    going to try and then run this exampleã€‚So first comment that you might have you
    might see here or first point that you see here is initialization method PyTtorch
    supports three initialization methodsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥èŠ±ä¸€ç‚¹æ—¶é—´è¿›è¡Œæ¢ç´¢ï¼Œä½†æˆ‘ä»¬å®é™…ä¸Šè¦å°è¯•å¹¶è¿è¡Œè¿™ä¸ªç¤ºä¾‹ã€‚å› æ­¤ï¼Œä½ å¯èƒ½åœ¨è¿™é‡Œçœ‹åˆ°çš„ç¬¬ä¸€ä¸ªè¯„è®ºæˆ–ç¬¬ä¸€ä¸ªè¦ç‚¹æ˜¯åˆå§‹åŒ–æ–¹æ³•ï¼ŒPyTorchæ”¯æŒä¸‰ç§åˆå§‹åŒ–æ–¹æ³•ã€‚
- en: TCP IP communication environmental variables and remote file store so currently
    that's one of the limitations of the releases we only support remote file storeã€‚
    so this is where we specify the path to the file which is used by workers to communicate
    their configuration settings to each otherã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TCP/IPé€šä¿¡ç¯å¢ƒå˜é‡å’Œè¿œç¨‹æ–‡ä»¶å­˜å‚¨ï¼Œç›®å‰è¿™æ˜¯æˆ‘ä»¬å‘å¸ƒç‰ˆæœ¬çš„ä¸€ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬ä»…æ”¯æŒè¿œç¨‹æ–‡ä»¶å­˜å‚¨ã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æŒ‡å®šæ–‡ä»¶çš„è·¯å¾„ï¼Œè¯¥è·¯å¾„è¢«å·¥ä½œèŠ‚ç‚¹ç”¨æ¥ç›¸äº’ä¼ è¾¾å…¶é…ç½®è®¾ç½®ã€‚
- en: In this caseï¼Œ we use single machine with multiple GPUsã€‚ so we are just using
    local file for communication between twoã€‚ two workr processes running on the same
    machineï¼Œ for the case of real distributed training across multiple machinesã€‚ you'd
    want to use the remote file systemï¼Œ place the file on the remote file systemã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å•å°æœºå™¨å’Œå¤šä¸ªGPUã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªæ˜¯ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åœ¨ä¸¤ä¸ªè¿›ç¨‹ä¹‹é—´è¿›è¡Œé€šä¿¡ã€‚å¯¹äºè·¨å¤šå°æœºå™¨çš„çœŸå®åˆ†å¸ƒå¼è®­ç»ƒï¼Œä½ éœ€è¦ä½¿ç”¨è¿œç¨‹æ–‡ä»¶ç³»ç»Ÿï¼Œå°†æ–‡ä»¶æ”¾åœ¨è¿œç¨‹æ–‡ä»¶ç³»ç»Ÿä¸Šã€‚
- en: Let's explore what models we are going to try here so in this tutorial there
    are three models that have been triedã€‚ the first one is a simple two layer network
    and what tutorial does it basically does a distributed training using standard
    distributed data parallel model to train this model across two GPUs and aggregate
    the gradients from both of them so standard mode of training the model on multiple
    GPUsã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹æˆ‘ä»¬è¦å°è¯•çš„æ¨¡å‹ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæœ‰ä¸‰ç§æ¨¡å‹è¢«å°è¯•ã€‚ç¬¬ä¸€ä¸ªæ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤å±‚ç½‘ç»œï¼Œè¿™ä¸ªæ•™ç¨‹åŸºæœ¬ä¸Šæ˜¯ä½¿ç”¨æ ‡å‡†çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¨¡å‹åœ¨ä¸¤ä¸ªGPUä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶æ±‡æ€»æ¥è‡ªè¿™ä¸¤ä¸ªGPUçš„æ¢¯åº¦ï¼Œå› æ­¤è¿™æ˜¯åœ¨å¤šä¸ªGPUä¸Šè®­ç»ƒæ¨¡å‹çš„æ ‡å‡†æ¨¡å¼ã€‚
- en: The second example is illustrating how checkpoints and synchronization can be
    doneã€‚ so again it trains the model but then one of the workers saves the checkpointã€‚
    so around zero saves the checkpoint and then another workers loads the checkpoint
    and between those two we use distributed training barrier synchronization to make
    sure that the actions happen in the correct sequenceã€‚That's another example of
    distributed data parallel concept now available on Windowsã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªç¤ºä¾‹è¯´æ˜äº†å¦‚ä½•è¿›è¡Œæ£€æŸ¥ç‚¹å’ŒåŒæ­¥ã€‚å› æ­¤ï¼Œå®ƒå†æ¬¡è®­ç»ƒæ¨¡å‹ï¼Œä½†å…¶ä¸­ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ä¿å­˜æ£€æŸ¥ç‚¹ã€‚å¤§çº¦åœ¨é›¶æ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œç„¶åå¦ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹åŠ è½½æ£€æŸ¥ç‚¹ã€‚åœ¨è¿™ä¸¤è€…ä¹‹é—´ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒå±éšœåŒæ­¥ï¼Œä»¥ç¡®ä¿æ“ä½œæŒ‰æ­£ç¡®é¡ºåºè¿›è¡Œã€‚è¿™æ˜¯åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¦‚å¿µçš„å¦ä¸€ä¸ªç¤ºä¾‹ï¼Œç°åœ¨å¯åœ¨Windowsä¸Šä½¿ç”¨ã€‚
- en: And final example here actually illustrates how distributed model par training
    can be doneã€‚ it trains one layer on one GPU and then passes data to the second
    GPU to train the second layerã€‚So this model parallel training is becoming more
    and more important with the introduction of the large NLP modelsã€‚And one last
    point on this script before we run it is how it actually runsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªç¤ºä¾‹å®é™…ä¸Šè¯´æ˜äº†å¦‚ä½•è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹å¹¶è¡Œè®­ç»ƒã€‚å®ƒåœ¨ä¸€ä¸ªGPUä¸Šè®­ç»ƒä¸€å±‚ï¼Œç„¶åå°†æ•°æ®ä¼ é€’ç»™ç¬¬äºŒä¸ªGPUä»¥è®­ç»ƒç¬¬äºŒå±‚ã€‚å› æ­¤ï¼Œéšç€å¤§å‹NLPæ¨¡å‹çš„å¼•å…¥ï¼Œè¿™ç§æ¨¡å‹å¹¶è¡Œè®­ç»ƒå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨æˆ‘ä»¬è¿è¡Œè¿™ä¸ªè„šæœ¬ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªæœ€åè¦æåˆ°çš„ç‚¹æ˜¯å®ƒæ˜¯å¦‚ä½•å®é™…è¿è¡Œçš„ã€‚
- en: So you can see that this script automatically spans number of workers that is
    necessary depending on the number of GPUusã€‚ The voltage size will in our case
    will be a number of GPUus So now we can all we need to do is basically run this
    scriptã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_11.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨æ ¹æ® GPU æ•°é‡æ‰©å±•æ‰€éœ€çš„å·¥ä½œçº¿ç¨‹ã€‚æˆ‘ä»¬è¿™å„¿çš„ç”µå‹å¤§å°å°†æ˜¯ GPU æ•°é‡ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬æ‰€éœ€è¦åšçš„å°±æ˜¯è¿è¡Œè¿™ä¸ªè„šæœ¬ã€‚![](img/3eee7b9d5034fa1a327cea58b77217c6_11.png)
- en: Let's seeã€‚And we can monitor its progress using an N SMIã€‚You can see that now
    both GPUs are utilized by our scriptã€‚ so now it's currently started running the
    second exampleï¼Œ the two point in exampleã€‚ and you can see that the load is a little
    bit greater and it goes asynchronously between GPUs but then it levelss up and
    finally the second model parallel training is also kind of a morechronous operates
    in a more asynchronous mannerã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ N SMI ç›‘æ§å®ƒçš„è¿›å±•ã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨ä¸¤ä¸ª GPU éƒ½è¢«æˆ‘ä»¬çš„è„šæœ¬åˆ©ç”¨äº†ã€‚æ‰€ä»¥ç°åœ¨å®ƒå¼€å§‹è¿è¡Œç¬¬äºŒä¸ªç¤ºä¾‹ï¼Œä¸¤ä¸ªç‚¹çš„ç¤ºä¾‹ã€‚ä½ å¯ä»¥çœ‹åˆ°è´Ÿè½½ç¨å¾®å¤§ä¸€äº›ï¼Œå®ƒåœ¨
    GPU ä¹‹é—´å¼‚æ­¥è¿›è¡Œï¼Œä½†éšåå®ƒä¼šå¹³è¡¡ï¼Œå¹¶ä¸”æœ€ç»ˆç¬¬äºŒä¸ªæ¨¡å‹å¹¶è¡Œè®­ç»ƒä¹Ÿæ˜¯ä»¥æ›´å¼‚æ­¥çš„æ–¹å¼è¿è¡Œã€‚
- en: You can also see here in the NDSM that it actually spans multiple Python processes
    to utilize various two GPU cards at the same timeã€‚So with thatï¼Œ you have seen
    how simple it actually is of running distributed data parallel on Windowsã€‚ it's
    very similar to Linux and just as easy to run Windows these days as it is on Linuxã€‚With
    thatã€‚ we conclude our demoã€‚Thank you very much for watching this segmentã€‚I'm Maxim
    Lucianovã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ NDSM ä¸­çœ‹åˆ°ï¼Œå®ƒå®é™…ä¸Šè·¨è¶Šå¤šä¸ª Python è¿›ç¨‹ï¼Œä»¥åŒæ—¶åˆ©ç”¨ä¸¤ä¸ª GPU å¡ã€‚å› æ­¤ï¼Œä½ å·²ç»çœ‹åˆ°åœ¨ Windows ä¸Šè¿è¡Œåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ˜¯å¤šä¹ˆç®€å•ã€‚è¿™ä¸
    Linux éå¸¸ç›¸ä¼¼ï¼Œå¦‚ä»Šåœ¨ Windows ä¸Šè¿è¡Œä¸åœ¨ Linux ä¸Šä¸€æ ·å®¹æ˜“ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬çš„æ¼”ç¤ºç»“æŸã€‚éå¸¸æ„Ÿè°¢ä½ è§‚çœ‹è¿™ä¸€éƒ¨åˆ†ã€‚æˆ‘æ˜¯ Maxim Lucianovã€‚
- en: please send me message if you have any questions or ideas about Pytorch and
    Windows and have a great rest of your Pythtorch developer dayã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹ Pytorch å’Œ Windows æœ‰ä»»ä½•é—®é¢˜æˆ–æƒ³æ³•ï¼Œè¯·ç»™æˆ‘å‘æ¶ˆæ¯ï¼Œç¥ä½ åº¦è¿‡æ„‰å¿«çš„ Pythtorch å¼€å‘è€…æ—¥ã€‚
