# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12- æ¿€æ´»å‡½æ•° - ShowMeAI - BV12m4y1S7ix

Hiï¼Œ everybodyã€‚ Welcome to your new Pytorch tutorialã€‚ This timeã€‚ I want to talk about activationation functionsã€‚ Actation functions are an extremely important feature of neural networksã€‚ So let's have a look at what activationation functions areã€‚ why they are used what different types of functions there are and how we incorporate them into our pyt modelã€‚

 So activationation functions apply a linear transformation to the layer output and basically decide whether a neuron should be activated or notã€‚ğŸ˜Šï¼ŒSo why do we use themï¼Œ Why is only a linear transformation not good enoughã€‚So typicallyã€‚ we would have a linear layer in our network that applies a linear transformationã€‚ So here it multiplies the input input with some weights and maybe add sub buyers and then delivers the outputã€‚

And let's suppose we don't have activationctuation functions in betweenã€‚Then we would have only linear transformations after each otherã€‚ So our whole network from input to output is essentially just a linear regression modelã€‚ And this linear model is not suited for more complex tasksã€‚

 So the conclusion is that with non nonlinear transformations in between our network can learn better and perform more complex tasksã€‚ So after each layerï¼Œ we typically want to apply this activation functionsã€‚ So hereã€‚Firstã€‚ we have our normal linear layerï¼Œ and then we also apply this activationctuaation functionã€‚And with thisï¼Œ our network can learn betterã€‚And now let's talk about the most popular activation functionsã€‚

 So the ones I want to show you is the binary step functionï¼Œ the smoid functionã€‚ the hyperbolic tangent functionï¼Œ the realloï¼Œ the leaky reulu and the softmã€‚So let's start with the simple step functionã€‚ So this will just output one if our input is greater than a thresholdã€‚ So here the threshold is 0 and0 otherwiseï¼Œ this is not used in practice actuallyã€‚

 but this should demonstrate the example of if the neuron should be activatedted or notã€‚And yeahã€‚ so a more popular choice is the sigoid functionã€‚ And you should already know this if you've watched my tutorial about logistic regressionã€‚ So the formula is  one over one plus E to the minus xã€‚ and this will output a probability between 0 and1ã€‚

And this is typically used in the last layer of a binary classification problemã€‚Soï¼Œ yeahã€‚ then we have the hyperbolic tangent function or ton Hã€‚ This is basically a scaled sigmoid function and also a little bit shiftedã€‚ So this will output a value between-1 and plus oneã€‚

 And this is actually a good choice in hidden layersã€‚ So you should know about the ton H functionã€‚Then we have the relo functionï¼Œ and this is the most popular choice in in most of the networksã€‚ So the relu function will output 0 for negative valuesã€‚ and it will simply output the input as output for positive valuesã€‚

 So it is actually a linear function for values greater than 0ï¼Œ and it is just0 for negative valuesã€‚ So it doesn't look that much different from just a linear transformationï¼Œ but in factã€‚ it is nonlineararï¼Œ and it is actually the most popular choice in the networksã€‚ and its typically a very good choice for an activationation functionã€‚

 So the rule of thumb is if you don't know which function you should useã€‚ then just use a relo for hidden layersã€‚Yeahï¼Œ so this is the reluï¼Œ very popular choiceã€‚ Then we also have the leaky relu functionã€‚ So this is a slightly modified and slightly improved version of the reluã€‚ So this will still just output the input for x greater than 0ã€‚

 but this will multiply our input with a very small value for negative numbersã€‚ So here I've written a times x for negative numbersã€‚ and this a is typically very smallã€‚ So it'sã€‚ for exampleï¼Œ0001ã€‚And this is an improved version of the relo that tries to solve the so called vanish ingredient problemã€‚Because with a normal reluï¼Œ our values here are 0ã€‚ And this means that also the gradient later in the back propagation is 0ã€‚

 And when the gradient is 0ï¼Œ then this means that these weights will never be updatedã€‚ So these neurons won't learn anythingã€‚ And we also say that these neurons are deadã€‚And this is why sometimes you want to use the leaky relu functionã€‚ So whenever you notice that your weights won't update during trainingã€‚

 then try to use the leaky relu instead of the normal reloã€‚And yeahï¼Œ then as a last functionã€‚ I want to show you the softmax functionã€‚ and you also should already know this because I have a whole tutorial about the soft softmax functionã€‚ So this will just this will basically squash the inputs to be outputs between 0 and one so that we have a probability as an outputã€‚ And this is typically a good choice in the last layer of a multi class classification problemã€‚Soã€‚

 yeahï¼Œ that's the different activationation functions I wanted to show youã€‚ And now let's jump to the code and see how we can use them in Pytorchã€‚ So we have two optionsã€‚![](img/e9ab40438781f445672555aa3780e63b_1.png)

And the first one is to create our functions as N N modulesã€‚ So in our network in the init functionã€‚ firstï¼Œ we define all the layers we want to haveã€‚ So hereï¼Œ for exampleï¼Œ firstã€‚ we have a linear layerã€‚And then after thatï¼Œ we want to have a relu activationctuaation functionã€‚ so we create our relu module hereã€‚And we can get that from the torch dot and N moduleã€‚

 So this contains all the different functions I just showed youã€‚And then we have the next layer for hereï¼Œ exampleï¼Œ it's the next linear layerã€‚ and then the next activationation functionã€‚ So here we have a sigoidid at the endã€‚And then in the forward passï¼Œ we simply call all these functions after each otherã€‚ So firstã€‚

 we have the linearï¼Œ the first linear layerï¼Œ which gets an outputã€‚ And then we use this output and put it into our reloã€‚ And then againã€‚ we use this output and put it in the next linear layer and so onã€‚So this is the first way how we can use itã€‚ and the second way is to use these functions directlyã€‚

 So in the init functionï¼Œ we only define our linear layerã€‚ So linear 1 and linear 2 and then in the forward pass we apply this linear layer and then also call this torch dot relu function here and then the torch dot seeoidoid function directlyã€‚ So this is just from the torch APIã€‚And yeahï¼Œ this is a different way how we can use itã€‚ Both ways will achieve the same thingã€‚ It's justã€‚How you prefer your codeã€‚ And yeahã€‚

 so all the functions that I just showed youï¼Œ you can get from the N N moduleã€‚ So here we have an N reluï¼Œ but we canï¼Œ for exampleã€‚ also have an N dot sigmoid and we have an N dot soft marksï¼Œ and we have an N dot ton Hã€‚å—¯ã€‚And alsoã€‚ N and dot leaky reluã€‚ So all these functions are available hereã€‚

And they are also available in the torch API like thisã€‚ So here we have torch dot reloã€‚ and we have torch dot sigmoidã€‚ We also have torch dot soft maxã€‚And torch dot1n Hã€‚And but sometimes they are not used in the the functions are not available in the torch API directlyã€‚ but they are available in torch do n n dot functionalã€‚

 So here I import a torch and n functional as Fã€‚ and then I can call hereï¼Œ for exampleï¼Œ F dot Reluã€‚ So this is the same as torch dot reluã€‚ but hereï¼Œ for exampleã€‚ is the torch is F dot leaky relu is only available in this APIã€‚Soï¼Œ yeahã€‚ but that's how we can use the activation functions and pytorrchã€‚ And it's actually very easyã€‚

And I hope you understood everything and now feel comfortable with activationation functionsã€‚ If you like thisï¼Œ please subscribe to the channel and see you next time byã€‚![](img/e9ab40438781f445672555aa3780e63b_3.png)