- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P20ï¼šL3.4- åœ¨Kerasä¸­æå‰åœæ­¢ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P20ï¼šL3.4- åœ¨Kerasä¸­æå‰åœæ­¢ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Haytonï¼Œ welcome to App of Deep neuralural Networks with Washington
    Universityã€‚In this videoï¼Œ we're going to see our first technique for fighting
    against the constant enemy of the neural network programmerã€‚ And that's overfittingã€‚
    We're going to see how to use early stoppingã€‚ rather than trying to decide how
    many epochsã€‚ you want to train your neural network 4ã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·æµ·é¡¿ï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¯¹æŠ—ç¥ç»ç½‘ç»œç¨‹åºå‘˜çš„å¸¸è§æ•Œäººâ€”â€”è¿‡æ‹Ÿåˆçš„ç¬¬ä¸€ä¸ªæŠ€å·§ã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨æ—©æœŸåœæ­¢ï¼Œè€Œä¸æ˜¯å†³å®šè®­ç»ƒç¥ç»ç½‘ç»œçš„è½®æ•°ã€‚
- en: you can set aside a validation set and train the neural network until that validation
    set No longer the error on it no longer improves for the latest or among AI course
    and projectsã€‚ Click subscribe in the bell next to it to be notified of every new
    videoã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é¢„ç•™ä¸€ä¸ªéªŒè¯é›†ï¼Œå¹¶è®­ç»ƒç¥ç»ç½‘ç»œï¼Œç›´åˆ°éªŒè¯é›†ä¸Šçš„é”™è¯¯ä¸å†æ”¹å–„ã€‚ç‚¹å‡»æ—è¾¹çš„é“ƒé“›è®¢é˜…ï¼Œä»¥ä¾¿æ”¶åˆ°æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚
- en: Earl stopping is a method that helps to prevent overfittingã€‚ We'll see other
    methods to help prevent overfitting as well as we progressed this classã€‚ğŸ˜Šã€‚![](img/4c43fcf66b6cd3d24858fb5574a8cd57_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ—©æœŸåœæ­¢æ˜¯ä¸€ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚åœ¨æˆ‘ä»¬è¯¾ç¨‹çš„è¿›å±•ä¸­ï¼Œæˆ‘ä»¬è¿˜å°†çœ‹åˆ°å…¶ä»–é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚ğŸ˜Šï¼[](img/4c43fcf66b6cd3d24858fb5574a8cd57_1.png)
- en: But what early stopping really tries to do is keep you from training the neural
    network too farã€‚ This is a common visualization to show what ill effects overfitting
    can have as you train more and moreã€‚ X axis shows you the errorã€‚ The Y axis shows
    you the training timeã€‚ The dashed line is the validation set and the black line
    is the training setã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ—©æœŸåœæ­¢å®é™…ä¸Šè¯•å›¾é¿å…ä½ è¿‡åº¦è®­ç»ƒç¥ç»ç½‘ç»œã€‚è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„å¯è§†åŒ–ï¼Œå±•ç¤ºäº†éšç€è®­ç»ƒè¶Šæ¥è¶Šå¤šï¼Œè¿‡æ‹Ÿåˆä¼šäº§ç”Ÿçš„è´Ÿé¢å½±å“ã€‚Xè½´æ˜¾ç¤ºé”™è¯¯ï¼ŒYè½´æ˜¾ç¤ºè®­ç»ƒæ—¶é—´ã€‚è™šçº¿æ˜¯éªŒè¯é›†ï¼Œé»‘çº¿æ˜¯è®­ç»ƒé›†ã€‚
- en: So what is happening to the errorã€‚ of your training values versus your validation
    values as you train longer and longer and longerã€‚ Generalï¼Œ the validation and
    training will be pretty similarã€‚ The validation might be a little bit worse as
    you begin to train becauseã€‚The training set is going to be somewhat overfit because
    the neural network has those values availableã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œéšç€ä½ ä¸æ–­è®­ç»ƒï¼Œè®­ç»ƒå€¼ä¸éªŒè¯å€¼çš„é”™è¯¯å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿé€šå¸¸ï¼ŒéªŒè¯å’Œè®­ç»ƒä¼šç›¸ä¼¼ã€‚éšç€è®­ç»ƒå¼€å§‹ï¼ŒéªŒè¯å¯èƒ½ä¼šç¨å¾®å·®ä¸€äº›ï¼Œå› ä¸ºè®­ç»ƒé›†ä¼šæœ‰äº›è¿‡æ‹Ÿåˆï¼Œå› ä¸ºç¥ç»ç½‘ç»œæœ‰è¿™äº›å€¼å¯ç”¨ã€‚
- en: It's memorizing some of themã€‚ It's fitting very well to the training setã€‚ So
    the validation set will usually be a little bit worseã€‚ as you're progressingã€‚
    Then as the training set gets better and better and better at some pointã€‚ the
    validation set will start to almost converge away from this to a higher error
    valueã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ­£åœ¨è®°ä½å…¶ä¸­çš„ä¸€äº›ã€‚å®ƒå¯¹è®­ç»ƒé›†çš„æ‹Ÿåˆéå¸¸å¥½ã€‚å› æ­¤ï¼Œéšç€ä½ çš„è¿›å±•ï¼ŒéªŒè¯é›†é€šå¸¸ä¼šç¨å¾®å·®ä¸€ç‚¹ã€‚ç„¶åï¼Œå½“è®­ç»ƒé›†è¶Šæ¥è¶Šå¥½æ—¶ï¼ŒéªŒè¯é›†åœ¨æŸä¸ªæ—¶åˆ»ä¼šå¼€å§‹å‡ ä¹ä»è¿™ä¸ªç‚¹è¶‹å‘ä¸€ä¸ªæ›´é«˜çš„é”™è¯¯å€¼ã€‚
- en: And you'll get worse and worse values on your validationã€‚ you'll get better
    and better values on your trainingã€‚ This is result of overfittingã€‚ at this point
    hereï¼Œ overfitting really started to occurã€‚ Nowã€‚ you may not recognize it till
    somewhere out hereã€‚ because you have to thinkï¼Œ okayï¼Œ it'sã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„éªŒè¯å€¼ä¼šè¶Šæ¥è¶Šå·®ï¼Œè€Œè®­ç»ƒå€¼ä¼šè¶Šæ¥è¶Šå¥½ã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆçš„ç»“æœã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œè¿‡æ‹ŸåˆçœŸçš„å¼€å§‹å‘ç”Ÿã€‚ç°åœ¨ï¼Œä½ å¯èƒ½ä¸ä¼šåœ¨å¤–é¢æŸä¸ªåœ°æ–¹æ„è¯†åˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºä½ å¿…é¡»æƒ³ï¼Œå¥½çš„ï¼Œæƒ…å†µåœ¨å˜å·®ï¼Œæˆ–è®¸ä¼šå˜å¥½ï¼Œæˆ–è®¸ä¼šå˜å¥½ã€‚ç°åœ¨å¯èƒ½ä¸å†å˜å¥½äº†ã€‚
- en: it's getting worseã€‚ Maybe it'll get betterã€‚ Maybe it'll get betterã€‚ Now it's
    probably not getting not getting betterã€‚ Nowï¼Œ there are cases where you'll go
    out this farã€‚ And then itll it'll come back down hereã€‚ This is a value you will
    see in this class called patienceã€‚ How patient are youã€‚To see these values start
    to move towards convergence againã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæœ‰äº›æƒ…å†µä¸‹ä½ ä¼šåˆ°è¾¾è¿™ä¸ªç¨‹åº¦ï¼Œç„¶ååˆä¼šå›è½åˆ°è¿™é‡Œã€‚åœ¨è¿™é—¨è¯¾ä¸­ä½ ä¼šçœ‹åˆ°ä¸€ä¸ªå€¼å«è€å¿ƒã€‚ä½ æœ‰å¤šè€å¿ƒï¼Œæ¥è§‚å¯Ÿè¿™äº›å€¼å†æ¬¡è¶‹å‘æ”¶æ•›ã€‚
- en: rather than diverge away into validationï¼Œ just overfitting andï¼Œ and not getting
    a better valueã€‚ This is what early stopping isã€‚ You stop earlyã€‚ You stop at this
    pointã€‚ and we'll see that it's important that we capture the weights here where
    we hadã€‚Essentiallyã€‚ the best or the lowest validation errorï¼Œ which is probably
    right in here where we had the lowest validation errorã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸æ˜¯åœ¨éªŒè¯ä¸Šå‘æ•£ï¼Œåªæ˜¯è¿‡æ‹Ÿåˆï¼Œè€Œæ²¡æœ‰å¾—åˆ°æ›´å¥½çš„å€¼ã€‚è¿™å°±æ˜¯æ—©æœŸåœæ­¢çš„æ„ä¹‰ã€‚ä½ æå‰åœæ­¢ï¼Œåœ¨è¿™ä¸ªç‚¹ä¸Šåœæ­¢ã€‚æˆ‘ä»¬å°†çœ‹åˆ°æ•æ‰åˆ°æœ€ä½³æˆ–æœ€ä½éªŒè¯é”™è¯¯çš„æƒé‡æ˜¯å¤šä¹ˆé‡è¦ï¼Œè¿™å¯èƒ½æ­£æ˜¯åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰æœ€ä½çš„éªŒè¯é”™è¯¯ã€‚
- en: And then we save those weights so that we have them so that even when we decided
    to stopã€‚ maybe here or hereï¼Œ depending how patient we wereï¼Œ we go back to hereã€‚So
    that's an important thing to rememberï¼Œ tooï¼Œ about patientsã€‚ patients will really
    not hurt you on the overall best score that you'll get at the endã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä¿å­˜è¿™äº›æƒé‡ï¼Œè¿™æ ·å³ä½¿æˆ‘ä»¬å†³å®šåœ¨è¿™é‡Œæˆ–è¿™é‡Œåœæ­¢ï¼Œå–å†³äºæˆ‘ä»¬çš„è€å¿ƒï¼Œæˆ‘ä»¬ä¹Ÿèƒ½è¿”å›åˆ°è¿™é‡Œã€‚å› æ­¤ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªé‡è¦çš„äº‹æƒ…ï¼Œå…³äºè€å¿ƒã€‚è€å¿ƒå®é™…ä¸Šä¸ä¼šå½±å“ä½ æœ€ç»ˆè·å¾—çš„æ•´ä½“æœ€ä½³å¾—åˆ†ã€‚
- en: you can be as patient as you wantã€‚ It's just going to take a lot longerã€‚To train
    because you're going to have to go much further out if you set a gigantic patience
    number of epochs that you're willing to waitã€‚ Nowï¼Œ to do thisï¼Œ we need to segment
    the original data set into several data setsã€‚ Nowã€‚ Often we'll just have the first
    twoï¼Œ but sometimes we'll have a holdout set as wellã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„æ„æ„¿ä¿æŒè€å¿ƒã€‚è¿™åªä¼šè®©è®­ç»ƒæ—¶é—´å˜å¾—æ›´é•¿ï¼Œå› ä¸ºå¦‚æœä½ è®¾ç½®ä¸€ä¸ªå·¨å¤§çš„è€å¿ƒå‘¨æœŸï¼Œä½ å°±éœ€è¦èµ°å¾—æ›´è¿œã€‚ç°åœ¨ï¼Œè¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦å°†åŸå§‹æ•°æ®é›†åˆ†æˆå‡ ä¸ªæ•°æ®é›†ã€‚ç°åœ¨ï¼Œé€šå¸¸æˆ‘ä»¬åªä¼šæœ‰å‰ä¸¤ä¸ªï¼Œä½†æœ‰æ—¶ä¹Ÿä¼šæœ‰ä¸€ä¸ªä¿ç•™é›†ã€‚
- en: You may even split it up into more than just these threeï¼Œ depending really on
    what you're doingã€‚ It also depends on how much how much data you haveã€‚ If you
    have a lot of dataã€‚ then you may split this into many training sets and really
    ensure that you're not overfitting that you're not ever evaluating based on anything
    that you are trained onã€‚You can make this very pureï¼Œ and you may even split out
    a separate validation set just to be used for nothing other than early stopã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç”šè‡³å¯ä»¥å°†å…¶æ‹†åˆ†ä¸ºä¸æ­¢è¿™ä¸‰ç»„ï¼Œå…·ä½“å–å†³äºä½ åœ¨åšä»€ä¹ˆã€‚å®ƒè¿˜å–å†³äºä½ æœ‰å¤šå°‘æ•°æ®ã€‚å¦‚æœä½ æœ‰å¾ˆå¤šæ•°æ®ï¼Œé‚£ä¹ˆä½ å¯ä»¥å°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªè®­ç»ƒé›†ï¼ŒçœŸæ­£ç¡®ä¿ä½ æ²¡æœ‰è¿‡æ‹Ÿåˆï¼Œç¡®ä¿ä½ ä¸ä¼šåŸºäºä½ è®­ç»ƒè¿‡çš„ä»»ä½•ä¸œè¥¿è¿›è¡Œè¯„ä¼°ã€‚ä½ å¯ä»¥è®©è¿™ä¸ªè¿‡ç¨‹éå¸¸çº¯å‡€ï¼Œç”šè‡³å¯ä»¥æ‹†åˆ†å‡ºä¸€ä¸ªå•ç‹¬çš„éªŒè¯é›†ï¼Œä»…ç”¨äºæ—©æœŸåœæ­¢ã€‚
- en: but every time you split theseï¼Œ you lose some of your training dataã€‚ So it's
    it's a trade off between how purest do you want to be in terms of yourã€‚Never crossing
    training and validationï¼Œ never validating on anything that the neural network
    is trained onã€‚ how much of your training set you want to train on because your
    neural network will typically perform betterã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ¯æ¬¡ä½ è¿›è¡Œæ‹†åˆ†æ—¶ï¼Œä½ éƒ½ä¼šå¤±å»ä¸€äº›è®­ç»ƒæ•°æ®ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªæƒè¡¡ï¼Œæ¶‰åŠåˆ°ä½ å¸Œæœ›åœ¨è®­ç»ƒå’ŒéªŒè¯ä¹‹é—´ä¿æŒå¤šçº¯å‡€ï¼Œä»ä¸åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡çš„ä»»ä½•ä¸œè¥¿ä¸Šè¿›è¡ŒéªŒè¯ã€‚ä½ å¸Œæœ›åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå¤šå°‘ï¼Œå› ä¸ºç¥ç»ç½‘ç»œé€šå¸¸ä¼šè¡¨ç°å¾—æ›´å¥½ã€‚
- en: at least up to a pointï¼Œ the bigger the training set that you give itã€‚In kle
    competitionsã€‚ for exampleã€‚I will usually accept some small cross contamination
    of training and validationã€‚ meaning I might use a validation set for early stoppingã€‚Just
    because I know that I have that holdout set that Kaggle will eventually give meã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘åˆ°æŸç§ç¨‹åº¦ï¼Œä½ æä¾›çš„è®­ç»ƒé›†è¶Šå¤§ã€‚åœ¨Kaggleæ¯”èµ›ä¸­ï¼Œä¾‹å¦‚ï¼Œæˆ‘é€šå¸¸ä¼šæ¥å—ä¸€äº›å°çš„è®­ç»ƒå’ŒéªŒè¯çš„äº¤å‰æ±¡æŸ“ã€‚è¿™æ„å‘³ç€æˆ‘å¯èƒ½ä¼šä½¿ç”¨ä¸€ä¸ªéªŒè¯é›†è¿›è¡Œæ—©æœŸåœæ­¢ã€‚åªæ˜¯å› ä¸ºæˆ‘çŸ¥é“æˆ‘æœ‰é‚£ä¸ªKaggleæœ€ç»ˆä¼šç»™æˆ‘çš„ä¿ç•™é›†ã€‚
- en: I can't do that too much or I overfit to even Kaggles's holdout set to some
    degreeã€‚So it's really just a trade off and we'll talk more about this as we get
    into hyperparameter tuning and other things where you do need to create additional
    setã€‚ The ultimate insurance policyï¼Œ thoughï¼Œ is you set that final holdout set
    that you keep until the very end and you don't you don't use this for anything
    during the training process until you sayã€‚ okayï¼Œ I am I'm very happy with my modelï¼Œ
    let me see what happens when Iã€‚Try the finalã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸èƒ½åšå¤ªå¤šï¼Œå¦åˆ™æˆ‘ä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¿‡æ‹Ÿåˆåˆ°Kaggleçš„ä¿ç•™é›†ã€‚å› æ­¤ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªæƒè¡¡ï¼Œæˆ‘ä»¬å°†åœ¨è¿›å…¥è¶…å‚æ•°è°ƒä¼˜å’Œå…¶ä»–éœ€è¦åˆ›å»ºé™„åŠ é›†çš„å†…å®¹æ—¶è¿›ä¸€æ­¥è®¨è®ºã€‚ç„¶è€Œï¼Œ*ç»ˆæ*ä¿é™©ç­–ç•¥æ˜¯ï¼Œä½ è®¾å®šä¸€ä¸ªæœ€ç»ˆçš„ä¿ç•™é›†ï¼Œç›´åˆ°æœ€åæ‰ä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä½¿ç”¨å®ƒï¼Œç›´åˆ°ä½ è¯´ï¼Œå¥½çš„ï¼Œæˆ‘å¯¹æˆ‘çš„æ¨¡å‹å¾ˆæ»¡æ„ï¼Œè®©æˆ‘çœ‹çœ‹å½“æˆ‘å°è¯•æœ€ç»ˆæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚
- en: final holdout setã€‚ And that'sï¼Œ that's the error rate that you usually decide
    to commit to as far as howã€‚How well trained your model actually isã€‚So this is
    what it really looks likeã€‚In terms of something like an early stoppingã€‚What you're
    going to do is take your data setã€‚Break it into maybe 80% and 20% this 20% up
    here becomes the validationã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆä¿ç•™é›†ã€‚è¿™å°±æ˜¯ä½ é€šå¸¸å†³å®šæ‰¿è¯ºçš„é”™è¯¯ç‡ï¼Œå…³äºä½ çš„æ¨¡å‹å®é™…è®­ç»ƒå¾—å¤šå¥½ã€‚è¿™å®é™…ä¸Šæ˜¯ä»€ä¹ˆæ ·å­ã€‚åœ¨æ—©æœŸåœæ­¢æ–¹é¢ï¼Œä½ è¦åšçš„æ˜¯å°†æ•°æ®é›†åˆ†ä¸º80%å’Œ20%ï¼Œè¿™ä¸ª20%æˆä¸ºéªŒè¯é›†ã€‚
- en: the 80% becomes the trainingã€‚And you're going to fit or train your model on
    this 80%ã€‚ and then you're going to validate it on this 20%ã€‚So you'll get a training
    error hereã€‚ That is how well it was fit to your training setï¼Œ but then you take
    what you trained on that 80% and validate it with that remaining 20%ã€‚ and that
    gives you greater confidence that this is truly the error that your neural network
    is going to experience when it sees new dataã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 80%æˆä¸ºè®­ç»ƒé›†ã€‚ä½ å°†æ ¹æ®è¿™80%æ¥æ‹Ÿåˆæˆ–è®­ç»ƒä½ çš„æ¨¡å‹ã€‚ç„¶åä½ å°†åœ¨è¿™20%ä¸Šè¿›è¡ŒéªŒè¯ã€‚å› æ­¤ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªè®­ç»ƒè¯¯å·®ã€‚è¿™æ˜¯å®ƒä¸è®­ç»ƒé›†æ‹Ÿåˆçš„ç¨‹åº¦ï¼Œä½†ä¹‹åä½ å°†ç”¨é‚£80%è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ä¸å‰©ä½™çš„20%è¿›è¡ŒéªŒè¯ã€‚è¿™è®©ä½ æ›´æœ‰ä¿¡å¿ƒï¼Œè¿™æ˜¯ä½ çš„ç¥ç»ç½‘ç»œåœ¨çœ‹åˆ°æ–°æ•°æ®æ—¶çœŸæ­£ä¼šç»å†çš„è¯¯å·®ã€‚
- en: Now we're going to look at early stopping and early stopping does require a
    sort of validation setã€‚Now that causes your validation set to now be used for
    early stopping so now it's part of the training setã€‚ so this is where it's good
    to have that final holdout set that you might want to ultimately evaluate on just
    be aware of that if you're truly using your validation set as your early stoppingã€‚
    then you probably need another validation set to actually commit to an error on
    or you can accept that maybe you've got some inflation in your validation set
    and you can truly use whatever score you stopped on as an estimate of what the
    final neural network might doã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¦è®¨è®ºæ—©åœæ³•ï¼Œæ—©åœæ³•ç¡®å®éœ€è¦ä¸€ç§éªŒè¯é›†ã€‚è¿™ä¼šå¯¼è‡´ä½ çš„éªŒè¯é›†ç°åœ¨è¢«ç”¨ä½œæ—©åœæ³•ï¼Œæ‰€ä»¥å®ƒç°åœ¨æ˜¯è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ã€‚å› æ­¤ï¼Œè¿™æ—¶å€™æœ‰ä¸€ä¸ªæœ€ç»ˆçš„ä¿ç•™é›†æ˜¯å¾ˆå¥½çš„ï¼Œä½ å¯èƒ½å¸Œæœ›æœ€ç»ˆåœ¨è¿™ä¸ªä¿ç•™é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¦‚æœä½ çœŸçš„å°†éªŒè¯é›†ç”¨ä½œæ—©åœæ³•ï¼Œè¯·æ³¨æ„è¿™ä¸€ç‚¹ã€‚é‚£ä¹ˆï¼Œä½ å¯èƒ½éœ€è¦å¦ä¸€ä¸ªéªŒè¯é›†æ¥çœŸæ­£æ‰¿è®¤ä¸€ä¸ªé”™è¯¯ï¼Œæˆ–è€…ä½ å¯ä»¥æ¥å—ä½ åœ¨éªŒè¯é›†ä¸­å¯èƒ½æœ‰ä¸€äº›è†¨èƒ€ï¼Œå¹¶ä¸”ä½ å¯ä»¥çœŸå®åœ°ä½¿ç”¨ä½ åœæ­¢æ—¶çš„å¾—åˆ†ä½œä¸ºæœ€ç»ˆç¥ç»ç½‘ç»œå¯èƒ½è¡¨ç°çš„ä¼°è®¡ã€‚
- en: but be aware that you have crossed at that pointï¼Œ your early stopping validation
    set to your trainingã€‚Is what we're going to do is just monitor thisã€‚ And as soon
    as that validation setï¼Œ no longerã€‚Is decreasing and our patience expiresã€‚ We're
    going to stopã€‚ We're going to early stopã€‚ I'm going to show you two examples of
    how you will do thisã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¯·æ³¨æ„ï¼Œä½ åœ¨æ­¤æ—¶å·²ç»å°†æ—©åœæ³•çš„éªŒè¯é›†è½¬æ¢ä¸ºè®­ç»ƒé›†ã€‚æˆ‘ä»¬è¦åšçš„æ˜¯ç›‘æ§è¿™ä¸€ç‚¹ã€‚å½“éªŒè¯é›†ä¸å†å‡å°‘å¹¶ä¸”æˆ‘ä»¬çš„è€å¿ƒç”¨å°½æ—¶ï¼Œæˆ‘ä»¬å°†åœæ­¢ã€‚æˆ‘ä»¬å°†æå‰åœæ­¢ã€‚æˆ‘å°†ç»™ä½ å±•ç¤ºä¸¤ä¸ªç¤ºä¾‹ï¼Œå‘Šè¯‰ä½ å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: We'll do early stopping with classification and early stopping with regressionã€‚
    The code's mostly the sameï¼Œ but this gives you some good starting point code to
    look at for thisã€‚ So for this oneï¼Œ we're going to use the iris data setã€‚ So we
    the supple linked widthã€‚And pal length and widthã€‚Let's go ahead and run this so
    that we load the data inã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨åˆ†ç±»å’Œå›å½’è¿›è¡Œæ—©åœæ³•ã€‚ä»£ç å¤§ä½“ä¸Šæ˜¯ç›¸åŒçš„ï¼Œä½†è¿™ä¸ºä½ æä¾›äº†ä¸€äº›è‰¯å¥½çš„èµ·å§‹ä»£ç æ¥å‚è€ƒã€‚å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†ã€‚æˆ‘ä»¬æœ‰èŠ±è¼çš„é•¿åº¦å’Œå®½åº¦ï¼Œä»¥åŠèŠ±ç“£çš„é•¿åº¦å’Œå®½åº¦ã€‚ç°åœ¨æˆ‘ä»¬æ¥è¿è¡Œè¿™ä¸ªï¼Œä»¥ä¾¿åŠ è½½æ•°æ®ã€‚
- en: Since as the star goes awayï¼Œ it's loadedï¼Œ okayï¼Œ the data is loadedã€‚And we we
    ran itã€‚ And you can see the results hereã€‚ We'll look at the results firstã€‚ And
    then I'll show you kind of what happenedã€‚ We trained and trained and trainedã€‚
    You can see the training of validation lossã€‚ And then it says that right around
    101ã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ˜Ÿå·æ¶ˆå¤±æ—¶ï¼Œæ•°æ®å·²ç»åŠ è½½å¥½äº†ã€‚æˆ‘ä»¬è¿è¡Œäº†å®ƒï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ç»“æœã€‚æˆ‘ä»¬å…ˆæŸ¥çœ‹ç»“æœï¼Œç„¶åæˆ‘ä¼šå‘Šè¯‰ä½ å‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘ä»¬ä¸æ–­è®­ç»ƒã€‚ä½ å¯ä»¥çœ‹åˆ°è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±ï¼Œç„¶ååœ¨å¤§çº¦101çš„æ—¶å€™ã€‚
- en: we decided to early stopï¼Œ and we restore the model weights of that last best
    epochã€‚ And we set up our neural networkï¼Œ largely just like we did beforeã€‚It is
    going to have its classificationï¼Œ so we're using the Y shapeã€‚ we're using the
    number of classes and soft Mac and categorical cross entropyã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†³å®šæå‰åœæ­¢ï¼Œå¹¶æ¢å¤äº†æœ€åä¸€ä¸ªæœ€ä½³å‘¨æœŸçš„æ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬è®¾ç½®ç¥ç»ç½‘ç»œçš„æ–¹å¼å¤§ä½“ä¸Šå’Œä¹‹å‰ä¸€æ ·ã€‚å®ƒå°†è¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨Yå½¢çŠ¶ã€‚æˆ‘ä»¬ä½¿ç”¨ç±»åˆ«çš„æ•°é‡ä»¥åŠsoftmaxå’Œåˆ†ç±»äº¤å‰ç†µã€‚
- en: those three together tell you that this is a classification neural networkã€‚
    make sure you set it up like thatã€‚We're creating a monitor now this is the part
    that does the early stoppingã€‚We are telling it that we have a patience of fiveã€‚
    so we're going to only wait  five epochs for to quit improvingã€‚ verbose is oneã€‚
    So that'll give us that little notice that we saw down thereã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸‰è€…åˆåœ¨ä¸€èµ·å‘Šè¯‰ä½ è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ç¥ç»ç½‘ç»œã€‚ç¡®ä¿ä½ åƒè¿™æ ·è®¾ç½®å®ƒã€‚æˆ‘ä»¬ç°åœ¨æ­£åœ¨åˆ›å»ºä¸€ä¸ªç›‘æ§å™¨ï¼Œè¿™æ˜¯æ‰§è¡Œæ—©åœæ³•çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬å‘Šè¯‰å®ƒæˆ‘ä»¬æœ‰äº”ä¸ªå‘¨æœŸçš„è€å¿ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªç­‰å¾…äº”ä¸ªå‘¨æœŸä»¥åœæ­¢æ”¹è¿›ã€‚è¯¦ç»†ç¨‹åº¦è®¾ç½®ä¸º1ï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½çœ‹åˆ°ä¸‹é¢é‚£ä¸ªå°é€šçŸ¥ã€‚
- en: and we're telling it trulyï¼Œ restore the best weightsã€‚ And now we're going to
    fit itã€‚ But notice we're giving it X train Y trainã€‚ and we're passing in validation
    dataã€‚ We haven't done this beforeã€‚ So the validation dataã€‚ The X test Y test versus
    the X train Y trainã€‚ that came from up hereã€‚ We're splitting into validation and
    training setsã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœŸæ­£å‘Šè¯‰å®ƒï¼Œæ¢å¤æœ€ä½³æƒé‡ã€‚ç°åœ¨æˆ‘ä»¬å°†è¿›è¡Œæ‹Ÿåˆã€‚ä½†æ³¨æ„æˆ‘ä»¬ç»™äº†å®ƒXè®­ç»ƒYè®­ç»ƒï¼Œå¹¶ä¸”ä¼ å…¥äº†éªŒè¯æ•°æ®ã€‚æˆ‘ä»¬ä»¥å‰æ²¡æœ‰è¿™æ ·åšã€‚æ‰€ä»¥éªŒè¯æ•°æ®ï¼ŒXæµ‹è¯•Yæµ‹è¯•ä¸Xè®­ç»ƒYè®­ç»ƒæ¥è‡ªä¸Šé¢çš„æ‹†åˆ†ã€‚æˆ‘ä»¬å°†å…¶æ‹†åˆ†ä¸ºéªŒè¯å’Œè®­ç»ƒé›†ã€‚
- en: So we're splitting 25% is going to the test setã€‚ The remaining 75% is going
    to validationã€‚ So you have X trainï¼Œ X test Y trainã€‚ Those are the expected valuesï¼Œ
    the Y'sã€‚And why testã€‚ So now you have the four measurements split into a train
    and a testã€‚ and you have the expected classï¼Œ the type of ir that it actually was
    split intoã€‚Train and testã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†25%åˆ†é…ç»™æµ‹è¯•é›†ã€‚å‰©ä¸‹çš„75%åˆ†é…ç»™éªŒè¯é›†ã€‚æ‰€ä»¥ä½ æœ‰Xè®­ç»ƒï¼ŒXæµ‹è¯•Yè®­ç»ƒã€‚è¿™äº›æ˜¯æœŸæœ›å€¼ï¼ŒYçš„å€¼ã€‚ç°åœ¨ä½ æœ‰å››ä¸ªæµ‹é‡æ‹†åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•ã€‚å¹¶ä¸”ä½ æœ‰æœŸæœ›ç±»åˆ«ï¼Œå®é™…ä¸Šè¢«æ‹†åˆ†çš„IRç±»å‹ã€‚è®­ç»ƒå’Œæµ‹è¯•ã€‚
- en: The random state 42ï¼Œ that just gives us a way to consistently split itã€‚ and
    then the only other thing we do un fit down here is we pass a call backï¼Œ monitorã€‚
    so that's how this early stopping object that we created actually plugs into the
    trainingã€‚Or the fit process when we call fit as we trainã€‚ So this is how you implement
    early stopping for classificationã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºçŠ¶æ€42ï¼Œè¿™åªæ˜¯ç»™æˆ‘ä»¬ä¸€ä¸ªä¸€è‡´æ‹†åˆ†çš„æ–¹æ³•ã€‚ç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„å”¯ä¸€å…¶ä»–äº‹æƒ…æ˜¯ä¼ é€’ä¸€ä¸ªå›è°ƒï¼Œç›‘æ§ã€‚å› æ­¤è¿™å°±æ˜¯æˆ‘ä»¬åˆ›å»ºçš„æ—©æœŸåœæ­¢å¯¹è±¡å¦‚ä½•å®é™…æ’å…¥åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ–¹å¼ã€‚å½“æˆ‘ä»¬è°ƒç”¨fitæ—¶ï¼Œå®ƒåœ¨è®­ç»ƒæ—¶å·¥ä½œã€‚æ‰€ä»¥è¿™å°±æ˜¯å¦‚ä½•ä¸ºåˆ†ç±»å®ç°æ—©æœŸåœæ­¢ã€‚
- en: It's pretty much the same exact process for regressionã€‚ You create a monitorã€‚
    you pass it into the callbacksã€‚ and you need to provide validation dataã€‚ I just
    give you the exact code for this so that you can quickly make use of itã€‚ By the
    wayã€‚ if we do want to see the accuracy with that we got for early stoppingã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›å½’æ¥è¯´ï¼Œè¿‡ç¨‹å‡ ä¹å®Œå…¨ç›¸åŒã€‚ä½ åˆ›å»ºä¸€ä¸ªç›‘æ§ã€‚ä½ å°†å…¶ä¼ é€’åˆ°å›è°ƒä¸­ã€‚ä½ éœ€è¦æä¾›éªŒè¯æ•°æ®ã€‚æˆ‘ç»™ä½ ç¡®åˆ‡çš„ä»£ç ï¼Œä»¥ä¾¿ä½ å¯ä»¥å¿«é€Ÿä½¿ç”¨ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬æƒ³æŸ¥çœ‹æ—©æœŸåœæ­¢çš„å‡†ç¡®æ€§ã€‚
- en: notice the accuracy is very goodã€‚ğŸ˜Šï¼ŒSo we didn't have to guess at the number
    of epochsã€‚ It trained until it saw thatã€‚We had trained enoughã€‚ so this is a way
    that you can estimate and not have to guess at how many epochs you want to give
    your neural networkã€‚This is a way to have one fewer hyperparameterï¼Œ hyperparametersã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå‡†ç¡®ç‡éå¸¸å¥½ã€‚ğŸ˜Šå› æ­¤æˆ‘ä»¬ä¸å¿…çŒœæµ‹è®­ç»ƒçš„è½®æ•°ã€‚å®ƒè®­ç»ƒåˆ°çœ‹åˆ°è¿™ä¸€ç‚¹ä¸ºæ­¢ã€‚æˆ‘ä»¬è®­ç»ƒå¾—è¶³å¤Ÿå¤šã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ç§ä½ å¯ä»¥ä¼°ç®—è€Œä¸å¿…çŒœæµ‹ç»™ä½ çš„ç¥ç»ç½‘ç»œå¤šå°‘è½®æ¬¡çš„æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§å‡å°‘è¶…å‚æ•°çš„æ–¹æ³•ã€‚
- en: all these things you have to pick about your neural network that will influence
    behavior like the number of epochsã€‚ how many layers you haveï¼Œ we'll see how to
    pick layers and other things automatically later in this classã€‚ but this is one
    way that you can at leastã€‚Use mathematical accuracy to some degree to pick your
    number of epochs to train and a simple example of early stopping with regressionã€‚
    we use the auto miles per gallonã€‚ This is the same setup that we've done beforeã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºä½ çš„ç¥ç»ç½‘ç»œï¼Œä½ å¿…é¡»é€‰æ‹©æ‰€æœ‰è¿™äº›ä¼šå½±å“è¡Œä¸ºçš„å› ç´ ï¼Œæ¯”å¦‚è½®æ•°ã€å±‚æ•°ã€‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°å¦‚ä½•è‡ªåŠ¨é€‰æ‹©å±‚å’Œå…¶ä»–å†…å®¹ã€‚ä½†è¿™æ˜¯ä¸€ç§ä½ è‡³å°‘å¯ä»¥ä½¿ç”¨æ•°å­¦å‡†ç¡®åº¦åœ¨æŸç§ç¨‹åº¦ä¸Šé€‰æ‹©è®­ç»ƒè½®æ•°çš„æ–¹æ³•ï¼Œä»¥åŠå›å½’çš„æ—©æœŸåœæ­¢çš„ç®€å•ç¤ºä¾‹ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨æ¯åŠ ä»‘è‹±é‡Œæ•°ã€‚è¿™ä¸æˆ‘ä»¬ä¹‹å‰åšçš„è®¾ç½®ç›¸åŒã€‚
- en: We're splitting it into x and Y so that y is the miles per gallonã€‚ That's what
    we're trying to predict X is all these other values that we're using to predict
    miles per gallonã€‚And then we use exactly the same code that we used in the previous
    one for classificationã€‚ We split it into train and testï¼Œ 25% splitã€‚Now we're doing
    regression thoughã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…¶æ‹†åˆ†ä¸ºxå’ŒYï¼Œä½¿å¾—yä¸ºæ¯åŠ ä»‘çš„è‹±é‡Œæ•°ã€‚è¿™å°±æ˜¯æˆ‘ä»¬è¯•å›¾é¢„æµ‹çš„ï¼ŒXæ˜¯æˆ‘ä»¬ç”¨æ¥é¢„æµ‹æ¯åŠ ä»‘è‹±é‡Œæ•°çš„æ‰€æœ‰å…¶ä»–å€¼ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ä¸ä¹‹å‰åˆ†ç±»æ—¶å®Œå…¨ç›¸åŒçš„ä»£ç ã€‚æˆ‘ä»¬å°†å…¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œ25%çš„æ‹†åˆ†ã€‚ç°åœ¨æˆ‘ä»¬è¿›è¡Œå›å½’ã€‚
- en: so notice we have one output in neuronã€‚We have mean squared error that tells
    you that this is a regression neural networkã€‚ and this is all the same we're giving
    we're passing the monitor inã€‚And we're giving it X and Yã€‚Both for validation and
    for trainingã€‚ So you're giving it that two sidesï¼Œ the trainingã€‚And the validationã€‚
    And now we can run itã€‚And it trainsï¼Œ trains trainsã€‚ You can see the testingã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„æˆ‘ä»¬åœ¨ç¥ç»å…ƒä¸­æœ‰ä¸€ä¸ªè¾“å‡ºã€‚æˆ‘ä»¬æœ‰å‡æ–¹è¯¯å·®ï¼Œè¿™è¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå›å½’ç¥ç»ç½‘ç»œã€‚å…¶ä»–éƒ¨åˆ†éƒ½æ˜¯ç›¸åŒçš„ï¼Œæˆ‘ä»¬ä¼ é€’äº†ç›‘æ§ã€‚æˆ‘ä»¬æä¾›äº†Xå’ŒYï¼Œæ—¢ç”¨äºéªŒè¯ä¹Ÿç”¨äºè®­ç»ƒã€‚æ‰€ä»¥ä½ æä¾›äº†è®­ç»ƒå’ŒéªŒè¯çš„ä¸¤ä¸ªæ–¹é¢ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿è¡Œå®ƒã€‚å®ƒè®­ç»ƒã€è®­ç»ƒã€è®­ç»ƒã€‚ä½ å¯ä»¥çœ‹åˆ°æµ‹è¯•ç»“æœã€‚
- en: The the trainã€‚Loss and the validation loss both are decreasingã€‚And at some pointï¼Œ
    it decides to stopã€‚It restores the final weightsï¼Œ so it only made it to 31 epochs
    before it lost patients hereã€‚Then you can measure the final root mean squareã€‚Not
    a particularly good root mean squareã€‚12 nowã€‚ rememberï¼Œ neural networks start with
    random weightsã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±éƒ½åœ¨å‡å°‘ã€‚åœ¨æŸä¸ªæ—¶åˆ»ï¼Œå®ƒå†³å®šåœæ­¢ã€‚å®ƒæ¢å¤äº†æœ€ç»ˆæƒé‡ï¼Œæ‰€ä»¥å®ƒåœ¨è¿™é‡Œåªè®­ç»ƒåˆ°äº† 31 ä¸ªå‘¨æœŸï¼Œå°±å¤±å»äº†è€å¿ƒã€‚ç„¶åä½ å¯ä»¥æµ‹é‡æœ€ç»ˆçš„å‡æ–¹æ ¹ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«å¥½çš„å‡æ–¹æ ¹ï¼Œç°åœ¨æ˜¯
    12ã€‚è¯·è®°ä½ï¼Œç¥ç»ç½‘ç»œæ˜¯ä»éšæœºæƒé‡å¼€å§‹çš„ã€‚
- en: So this gives me a chance to quickly illustrate something interestingã€‚ Remember
    Ro mean square was 12ã€‚ Let's retrain the whole thingã€‚ Nowï¼Œ let's recalculate thisã€‚4ourã€‚Notice
    that huge varianceï¼Œ this is really annoying about neural networksã€‚ but since they
    start with random weightsï¼Œ they will get sometimes different error valuesã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©æˆ‘æœ‰æœºä¼šå¿«é€Ÿè¯´æ˜ä¸€äº›æœ‰è¶£çš„äº‹æƒ…ã€‚è®°å¾—å‡æ–¹æ ¹æ˜¯ 12ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚ç°åœ¨ï¼Œé‡æ–°è®¡ç®—è¿™ä¸ªå€¼ã€‚æ³¨æ„åˆ°å·¨å¤§çš„æ–¹å·®ï¼Œè¿™çœŸè®©äººçƒ¦æ¼ï¼Œä½†ç”±äºå®ƒä»¬ä»éšæœºæƒé‡å¼€å§‹ï¼Œæœ‰æ—¶ä¼šå¾—åˆ°ä¸åŒçš„è¯¯å·®å€¼ã€‚
- en: This makes it very hard to try to tune somethingã€‚ Imagine if you had thrown
    in an extra hidden layerã€‚When you did have 12ï¼Œ but then you throw in a hidden
    layer oh my gosh I went down to fourã€‚ that's greatï¼Œ no it was just the normal
    variance you have in neural networksã€‚We'll see in the next module that something
    called bootstrapping will useã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—è°ƒä¼˜å˜å¾—éå¸¸å›°éš¾ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœä½ å†æ·»åŠ ä¸€ä¸ªéšè—å±‚ã€‚å½“ä½ æœ‰ 12 ä¸ªæ—¶ï¼Œä½†ä½ å†åŠ ä¸€ä¸ªéšè—å±‚ï¼Œå¤©å“ªï¼Œæˆ‘çš„ç»“æœä¸‹é™åˆ°äº† 4ã€‚è¿™å¾ˆä¸é”™ï¼Œä¸ï¼Œå…¶å®åªæ˜¯ç¥ç»ç½‘ç»œä¸­æ­£å¸¸çš„æ–¹å·®ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªæ¨¡å—ä¸­çœ‹åˆ°ä¸€ç§å«åšè‡ªä¸¾çš„æ–¹æ³•å°†ä¼šä½¿ç”¨ã€‚
- en: you're just literally going to have to run this a few times an average togetherã€‚Is
    how you handle that and things likeã€‚Drop outï¼Œ help mitigate this at least a little
    bitã€‚This is just one of the facts of life dealing with neural networksã€‚ They give
    a great deal of variance in in the score that you get from the outputã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å®é™…ä¸Šåªéœ€è¦å¤šè¿è¡Œå‡ æ¬¡ï¼Œå–ä¸ªå¹³å‡ã€‚ä½ å¦‚ä½•å¤„ç†è¿™äº›äº‹æƒ…ï¼Œæ¯”å¦‚è¯´ï¼ŒDrop outï¼Œæœ‰åŠ©äºè‡³å°‘å‡è½»ä¸€ç‚¹ã€‚è¿™åªæ˜¯å¤„ç†ç¥ç»ç½‘ç»œæ—¶çš„ç”Ÿæ´»äº‹å®ä¹‹ä¸€ã€‚å®ƒä»¬åœ¨è¾“å‡ºç»“æœä¸­æä¾›äº†å¾ˆå¤§çš„æ–¹å·®ã€‚
- en: '![](img/4c43fcf66b6cd3d24858fb5574a8cd57_3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c43fcf66b6cd3d24858fb5574a8cd57_3.png)'
- en: Thank you for watching this video On early stoppingã€‚ In the next partã€‚ we're
    going to see how to actually extract the weights from a saved Kara's neural network
    and put those weights into a mathematical equation and actually calculate the
    valuesã€‚ This removes all the magic from the processï¼Œ And you can see that it's
    simply mathematical calculationsã€‚ This content changes oftenã€‚ So subscribe to
    the channel to stay up to date on this course and other topics and artificial
    intelligenceã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ è§‚çœ‹è¿™æ®µå…³äºæ—©åœçš„è§†é¢‘ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä»ä¿å­˜çš„Kerasç¥ç»ç½‘ç»œä¸­æå–æƒé‡ï¼Œå¹¶å°†è¿™äº›æƒé‡æ”¾å…¥æ•°å­¦æ–¹ç¨‹ä¸­ï¼Œå®é™…è®¡ç®—å‡ºæ•°å€¼ã€‚è¿™æ¶ˆé™¤äº†è¿‡ç¨‹ä¸­çš„æ‰€æœ‰é­”æ³•ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™ä»…ä»…æ˜¯æ•°å­¦è®¡ç®—ã€‚è¿™äº›å†…å®¹ç»å¸¸å˜åŒ–ã€‚æ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æœ¬è¯¾ç¨‹åŠå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚
- en: ğŸ˜Šã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Š
