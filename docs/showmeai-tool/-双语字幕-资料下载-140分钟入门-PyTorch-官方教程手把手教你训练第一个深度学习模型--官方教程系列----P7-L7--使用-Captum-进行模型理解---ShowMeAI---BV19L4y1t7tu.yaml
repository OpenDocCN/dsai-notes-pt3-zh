- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë140ÂàÜÈíüÂÖ•Èó® PyTorchÔºåÂÆòÊñπÊïôÁ®ãÊâãÊääÊâãÊïô‰Ω†ËÆ≠ÁªÉÁ¨¨‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P7ÔºöL7- ‰ΩøÁî® Captum
    ËøõË°åÊ®°ÂûãÁêÜËß£ - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
- en: Welcome to the next video in the Pytorrch Tra series„ÄÇ This video gives an overview
    of Cap Pytorrch's tool set for model interpretability„ÄÇüòäÔºåIn this video„ÄÇ we'll discuss„ÄÇThe
    basic concepts of captain that we'll be covering attributions„ÄÇ attribution algorithms
    and visualizations„ÄÇWe'll demonstrate how to perform and visualize feature attributions
    for a computer vision classifier„ÄÇ
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Will apply layer attribution to the same classifier to examine the activity
    of a model's hidden layers„ÄÇAnd finallyÔºå we'll look at captive insights and API
    for creating visualization widgets for images„ÄÇ text and other features„ÄÇ![](img/2fb9bd9df95d3221c8b5a298fc267de0_2.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Captain provides a deep set of tools for explaining the behavior of your pietorrch
    models„ÄÇ This video and the accompanying interactive notebook provide only an overview
    of core features„ÄÇ The website at Captain AI contains more in depth tutorialsÔºå
    documentation and an API reference„ÄÇ
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_4.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: To run the interactive notebook associated with this videoÔºå you'll want to install
    Python version 3„ÄÇ6 or higher„ÄÇFlaask 1„ÄÇ1 or higherÔºå and the latest versions of
    piey torchÔºå torch visionion and captain„ÄÇCaptain can be easily installed with Pip
    or with Ananaconda by specifying the Pytorr channel„ÄÇTo start withÔºå we're going
    to take a pre trained image classifier„ÄÇ
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Resnet trained against the imagenet data setÔºå and we're going to use the tools
    within cap to gain insight into how the model responds to a particular input image
    to give its prediction„ÄÇThis first sells a bunch of importsÔºå including attribution
    methods and visualization tools from captain„ÄÇ
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: which we'll examine shortly„ÄÇNextÔºå we'll get our pretrain model„ÄÇThen we'll pull
    up an image to work with whereverver you got this video in the interactive notebook
    should also include a folder of images for use in this tutorial„ÄÇ In our caseÔºå
    it's going to be a cat„ÄÇNext we'll define some image transforms to prepare the
    image for consumption by the model„ÄÇAnd bring in the human readable labels of the
    thousand imagenet classes„ÄÇNow„ÄÇ
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: let's see what the model thinks this is„ÄÇAnd thinks our cat is a cat„ÄÇBut why
    does the model think this is a picture of a cat„ÄÇ![](img/2fb9bd9df95d3221c8b5a298fc267de0_7.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: For the answer to thatÔºå we can look under the hood of the model of cap„ÄÇThe core
    abstraction in cap is the attribution„ÄÇ and that is a quantitative method of attributing
    a particular output or activity of a model with its input„ÄÇThe first kind of attribution
    is feature attribution„ÄÇThis lets us ask which parts of the input were most important
    in determining a model's prediction„ÄÇ
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: It lets us find answers to questions like„ÄÇWhich words in this input question
    were most significant in deciding the answer„ÄÇWhich pixels in this input image
    drove the model's classification of the image„ÄÇWhich features of the input data
    were most significant to my regression model's prediction„ÄÇFeature attribution
    just covers inputs and outputsÔºå though„ÄÇ
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What if we want to see what's happening inside the model„ÄÇFor thatÔºå we have layer
    attribution„ÄÇThis attributes the activity of a hidden layer of a model to the model's
    input„ÄÇIt lets us answer questionsÔºå like„ÄÇWhich neurons in this layer were most
    activeÔºå given this input„ÄÇWhich neurons in this layer were most important to how
    the input influenced a particular output neuro„ÄÇ
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How is the activation map output by this convolutional layer correlated to my
    input image„ÄÇFinally„ÄÇ there's neuron attribution„ÄÇ This is similar to layer attribution„ÄÇ
    but goes down to the level of individual neurons in the model„ÄÇIn this tutorial„ÄÇ
    we're going to look at feature attribution and layer attribution„ÄÇ
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: FirstÔºå feature attribution„ÄÇAttributions are realized by an attribution algorithm„ÄÇ![](img/2fb9bd9df95d3221c8b5a298fc267de0_11.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: A particular method of mapping model activity to inputs„ÄÇThe first feature attribution
    algorithm we'll look at is called integrated gradients„ÄÇThis algorithm numerically
    approximates the integral of the gradients of the model's output with respect
    to its inputs„ÄÇ essentially finding the most important paths through the model
    for a given input output pair„ÄÇ
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We'll go ahead and create an integrated gradient objectÔºå initializing it with
    our model„ÄÇThen we'll call the attribute method on it„ÄÇWe'll feed it our input„ÄÇ
    our output label and an optional number of steps to run„ÄÇNote that running the
    cell can take a couple of minutes„ÄÇ
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The process of integrating the gradients is computationally intensive„ÄÇOnce that
    cell finishes runningÔºå we have a sort of numerical importance map of the cat image
    with respect to the cat label generated by the model„ÄÇFor a simple regression model
    with few output categoriesÔºå we might just print that out as a table„ÄÇ but for a
    more complicated C model with a larger input like an image„ÄÇ
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: it would help to be able to relate the importance map to the image visually„ÄÇCaptain's
    got you covered„ÄÇVisualization module gives you tools for exactly that„ÄÇHere„ÄÇ we're
    going to make two calls to visualize image at„ÄÇThe first displays the original
    image„ÄÇFirst„ÄÇ we need to make some adjustments to the image„ÄÇ We call a squeeze
    to remove the batch dimension on the image„ÄÇ
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: We make sure we're running on CPU„ÄÇ We detach the image tensor from computation
    history„ÄÇ Otherwise„ÄÇ the image tensor will keep tracking its computation history
    unnecessarily„ÄÇAnd finally„ÄÇ we make it a numpy array and switch the dimensions
    around and put the color channels last„ÄÇThe first argument of this method would
    normally be the attributions„ÄÇ But for this call„ÄÇ
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: we're going to make that none„ÄÇ We're just displaying the original image„ÄÇThe
    second argument is our transformed image„ÄÇThird argument is a visualization method„ÄÇ
    a string that indicates how you want the visualization to work„ÄÇHereÔºå we told Captain„ÄÇ
    we just want to display the aboginal image„ÄÇFinallyÔºå we give our visualization
    an instructive title„ÄÇ
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The second call will make a visual mapping of the important regions of our image„ÄÇThe
    first argument is the attributions we got from integrated gradients„ÄÇ And the second
    is our transformative image„ÄÇFor a method will specify heat nap„ÄÇWhere color intensity
    maps to the importance of an image region„ÄÇ
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Capize you to use custom color maps from map plotlib„ÄÇ And we've made one here
    that will slightly enhance the contrast of our heat map„ÄÇWe specify s as positive„ÄÇ
    We're only looking at positive attributions„ÄÇRunning a cell„ÄÇ we can see that the
    model is paying attention to the outline of the cat„ÄÇ
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: as well as the region around the centre of the cat's face„ÄÇLet's try another
    feature attribution algorithm„ÄÇ NextÔºå we'll try occlusion„ÄÇIntegrated gradients
    was a gradient based attribution algorithm„ÄÇ Occlusion is different„ÄÇ It's a perturbation
    based method that involves screening out portions of the image and seeing how
    that affects the output„ÄÇ
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: As beforeÔºå we're going to specify our input image and our output labeled to
    the attribution algorithm„ÄÇFor occlusionÔºå we're going to specify a few more items„ÄÇ
    The first are the sliding window and the stride length„ÄÇ And these are analogous
    to similar configuration options in a convolutional neural network„ÄÇ
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We're also going to set our baseline„ÄÇThat isÔºå our representation of an occluded
    image cellÔºå 0„ÄÇDepending on how your data are normalizedÔºå you may wish to specify
    a different baseline„ÄÇ but for zero centered dataÔºå it makes sense to use 0„ÄÇWe'll
    run the attribute call and give it a minute„ÄÇAnd in the next cellÔºå we're doing
    something new„ÄÇ
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: We're calling visualized image Adder multiple„ÄÇTo show multiple visualizations
    of the occlusion attribution„ÄÇBesides the original imageÔºå we'll show three visualizations„ÄÇ
    The first two are heat maps of both positive and negative attributions„ÄÇ You can
    see that we're providing a list of methods with heat map being the second and
    third„ÄÇ
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: We're also specifying a sign for each visualization„ÄÇ And here you can see that
    we've askedtra positive attributions on one heat map and negative on the other„ÄÇ
    These indicate which„ÄÇFor our final visualizationÔºå we'll use the mask method„ÄÇ This
    uses positive attributions to selectively screen the original image„ÄÇ
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: giving a striking visual representation of the areas of the image the model
    paid most attention to for this input output pair„ÄÇRunning the cellÔºå you can see
    that this maps well to what we learn from integrated gradients„ÄÇ Most of the activities
    are on the cat's outlined in the center of its face„ÄÇWhat about what the model
    is doing under the hood„ÄÇLet's use a layer attribution algorithm to check the activity
    of one of the hidden layers„ÄÇ
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Gradcam is another gradient based attribution algorithm designed for conves„ÄÇIt
    computes the gradients of the output with respect to the specified model layer„ÄÇ
    averagevers the gradients for each channel and multiplies this average by the
    layer activations„ÄÇAnd uses this as a measure of the importance of a layer's output„ÄÇ
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To get started with layer attributionÔºå we'll create a layer gradcam object and
    initialize it with our model and the layer we wish to examine„ÄÇThen we'll give
    it the input output pair and ask it to do attribution„ÄÇWe can visualize this with
    a heat mapÔºå as we did before„ÄÇ in this way„ÄÇ you can visually examine which areas
    of a confidencein activation map were like to your output„ÄÇ
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: We can do better than thisÔºå though„ÄÇSince the output of a convolutional layer
    is usually spatially correlated to the input„ÄÇ we can take advantage of that by
    up sampling that activation map and comparing it directly with the input„ÄÇThe layer
    attribution parent class has a convenience method for up samplingling the lower
    resolution convenant activation map up to the input size„ÄÇWe'll do that with the
    interpolate method here„ÄÇAnd asked the visualizer for a blended heat map„ÄÇ
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: showing the original image with a heat map superimposed and a masked image„ÄÇVisualizations
    like this can give you insight into how hidden layers contribute to a particular
    output from your model„ÄÇCaptain comes with an advanced visualization tool called
    Cap Insights„ÄÇ which lets you put together multiple visualizations in an in browserrowsed
    widget that lets you configure the attribution algorithm and its parameters„ÄÇ
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Captain Inights lets you visualize textÔºå image and arbitrary data„ÄÇWe're going
    to try three images nowÔºå the catÔºå a teapot and a trilaite fossil„ÄÇAgain„ÄÇ these
    images should be available wherever you got the interactive notebook that goes
    with its video„ÄÇFirstÔºå we'll query the model to see what it thinks each of these
    are„ÄÇAnd it seems to be really okay„ÄÇ
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: NowÔºå let's set up cap Ins„ÄÇWe're going to use the attribution visualizer object„ÄÇ
    and we'll configure it with our model„ÄÇA scoring function for the model's outputs
    hereÔºå softm„ÄÇA list of the classes the model recognizes here I am stripping out
    an ordered list of the image net class names„ÄÇWe'll tell it that we're looking
    at image features„ÄÇ
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Captive Insights also handles text and arbitrary dataÔºå as well„ÄÇAnd it'll give
    it a data set„ÄÇ which is just an iterable that returns a batch of images and labels„ÄÇNote
    that we haven't specified an algorithm or a visualization method„ÄÇThese are things
    that you set up in the in browserrower widget„ÄÇNowÔºå we ask the visualizer to render„ÄÇ
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: It starts off emptyÔºå but we can set up configuration parameters and ask it to
    fetch our visualized attributions with the fetch button„ÄÇI'm going to leave things
    at the default setting for integrated gradients„ÄÇ Captain needs a few minutes to
    generate the attributions„ÄÇBut now we can see that it ranks the first few predictions
    for each image with their probabilities and provides heat map attribution for
    the important regions of the image„ÄÇ
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ÂÆÉÊúÄÂàùÊòØÁ©∫ÁöÑÔºå‰ΩÜÊàë‰ª¨ÂèØ‰ª•ËÆæÁΩÆÈÖçÁΩÆÂèÇÊï∞ÔºåÂπ∂ÈÄöËøáÊèêÂèñÊåâÈíÆËØ∑Ê±ÇÂÆÉËé∑ÂèñÊàë‰ª¨ÂèØËßÜÂåñÁöÑÂΩíÂõ†„ÄÇÊàëÂ∞Ü‰øùÊåÅÈõÜÊàêÊ¢ØÂ∫¶ÁöÑÈªòËÆ§ËÆæÁΩÆ„ÄÇCaptainÈúÄË¶ÅÂá†ÂàÜÈíüÁîüÊàêÂΩíÂõ†„ÄÇ‰ΩÜÁé∞Âú®Êàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÂÆÉ‰∏∫ÊØè‰∏™ÂõæÂÉèÊéíÂêçÂâçÂá†ÁöÑÈ¢ÑÊµãÂèäÂÖ∂Ê¶ÇÁéáÔºåÂπ∂Êèê‰æõÈáçË¶ÅÂå∫ÂüüÁöÑÁÉ≠ÂõæÂΩíÂõ†„ÄÇ
- en: In this wayÔºå Captain insightights lets you experiment with attribution methods
    and understand the activity that LED to your model's predictions„ÄÇ both correct
    and incorrect and lets you do it visually with minimal code„ÄÇFinally„ÄÇ don't forget
    to look at Captain AI for documentation„ÄÇ tutorials and API reference and access
    to the source on Gitthub„ÄÇ
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ÈÄöËøáËøôÁßçÊñπÂºèÔºåCaptain insightightsËÆ©‰Ω†ÂÆûÈ™åÂΩíÂõ†ÊñπÊ≥ïÔºåÁêÜËß£ÂØºËá¥Ê®°ÂûãÈ¢ÑÊµãÁöÑÊ¥ªÂä®ÔºåÊó†ËÆ∫ÊòØÊ≠£Á°ÆÁöÑËøòÊòØÈîôËØØÁöÑÔºåÂπ∂‰∏î‰ª•ÊúÄÂ∞ëÁöÑ‰ª£Á†ÅËøõË°åÂèØËßÜÂåñ„ÄÇÊúÄÂêéÔºåÂà´Âøò‰∫ÜÊü•ÁúãCaptain
    AI‰ª•Ëé∑ÂèñÊñáÊ°£„ÄÅÊïôÁ®ãÂíåAPIÂèÇËÄÉÔºå‰ª•ÂèäÂú®GitHub‰∏äÁöÑÊ∫ê‰ª£Á†ÅËÆøÈóÆ„ÄÇ
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_13.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_13.png)'
