- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘140åˆ†é’Ÿå…¥é—¨ PyTorchï¼Œå®˜æ–¹æ•™ç¨‹æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒç¬¬ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- ä½¿ç”¨ Captum
    è¿›è¡Œæ¨¡å‹ç†è§£ - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
- en: Welcome to the next video in the Pytorrch Tra seriesã€‚ This video gives an overview
    of Cap Pytorrch's tool set for model interpretabilityã€‚ğŸ˜Šï¼ŒIn this videoã€‚ we'll discussã€‚The
    basic concepts of captain that we'll be covering attributionsã€‚ attribution algorithms
    and visualizationsã€‚We'll demonstrate how to perform and visualize feature attributions
    for a computer vision classifierã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Will apply layer attribution to the same classifier to examine the activity
    of a model's hidden layersã€‚And finallyï¼Œ we'll look at captive insights and API
    for creating visualization widgets for imagesã€‚ text and other featuresã€‚![](img/2fb9bd9df95d3221c8b5a298fc267de0_2.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Captain provides a deep set of tools for explaining the behavior of your pietorrch
    modelsã€‚ This video and the accompanying interactive notebook provide only an overview
    of core featuresã€‚ The website at Captain AI contains more in depth tutorialsï¼Œ
    documentation and an API referenceã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_4.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: To run the interactive notebook associated with this videoï¼Œ you'll want to install
    Python version 3ã€‚6 or higherã€‚Flaask 1ã€‚1 or higherï¼Œ and the latest versions of
    piey torchï¼Œ torch visionion and captainã€‚Captain can be easily installed with Pip
    or with Ananaconda by specifying the Pytorr channelã€‚To start withï¼Œ we're going
    to take a pre trained image classifierã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Resnet trained against the imagenet data setï¼Œ and we're going to use the tools
    within cap to gain insight into how the model responds to a particular input image
    to give its predictionã€‚This first sells a bunch of importsï¼Œ including attribution
    methods and visualization tools from captainã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: which we'll examine shortlyã€‚Nextï¼Œ we'll get our pretrain modelã€‚Then we'll pull
    up an image to work with whereverver you got this video in the interactive notebook
    should also include a folder of images for use in this tutorialã€‚ In our caseï¼Œ
    it's going to be a catã€‚Next we'll define some image transforms to prepare the
    image for consumption by the modelã€‚And bring in the human readable labels of the
    thousand imagenet classesã€‚Nowã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: let's see what the model thinks this isã€‚And thinks our cat is a catã€‚But why
    does the model think this is a picture of a catã€‚![](img/2fb9bd9df95d3221c8b5a298fc267de0_7.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: For the answer to thatï¼Œ we can look under the hood of the model of capã€‚The core
    abstraction in cap is the attributionã€‚ and that is a quantitative method of attributing
    a particular output or activity of a model with its inputã€‚The first kind of attribution
    is feature attributionã€‚This lets us ask which parts of the input were most important
    in determining a model's predictionã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: It lets us find answers to questions likeã€‚Which words in this input question
    were most significant in deciding the answerã€‚Which pixels in this input image
    drove the model's classification of the imageã€‚Which features of the input data
    were most significant to my regression model's predictionã€‚Feature attribution
    just covers inputs and outputsï¼Œ thoughã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What if we want to see what's happening inside the modelã€‚For thatï¼Œ we have layer
    attributionã€‚This attributes the activity of a hidden layer of a model to the model's
    inputã€‚It lets us answer questionsï¼Œ likeã€‚Which neurons in this layer were most
    activeï¼Œ given this inputã€‚Which neurons in this layer were most important to how
    the input influenced a particular output neuroã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: How is the activation map output by this convolutional layer correlated to my
    input imageã€‚Finallyã€‚ there's neuron attributionã€‚ This is similar to layer attributionã€‚
    but goes down to the level of individual neurons in the modelã€‚In this tutorialã€‚
    we're going to look at feature attribution and layer attributionã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Firstï¼Œ feature attributionã€‚Attributions are realized by an attribution algorithmã€‚![](img/2fb9bd9df95d3221c8b5a298fc267de0_11.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: A particular method of mapping model activity to inputsã€‚The first feature attribution
    algorithm we'll look at is called integrated gradientsã€‚This algorithm numerically
    approximates the integral of the gradients of the model's output with respect
    to its inputsã€‚ essentially finding the most important paths through the model
    for a given input output pairã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We'll go ahead and create an integrated gradient objectï¼Œ initializing it with
    our modelã€‚Then we'll call the attribute method on itã€‚We'll feed it our inputã€‚
    our output label and an optional number of steps to runã€‚Note that running the
    cell can take a couple of minutesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The process of integrating the gradients is computationally intensiveã€‚Once that
    cell finishes runningï¼Œ we have a sort of numerical importance map of the cat image
    with respect to the cat label generated by the modelã€‚For a simple regression model
    with few output categoriesï¼Œ we might just print that out as a tableã€‚ but for a
    more complicated C model with a larger input like an imageã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: it would help to be able to relate the importance map to the image visuallyã€‚Captain's
    got you coveredã€‚Visualization module gives you tools for exactly thatã€‚Hereã€‚ we're
    going to make two calls to visualize image atã€‚The first displays the original
    imageã€‚Firstã€‚ we need to make some adjustments to the imageã€‚ We call a squeeze
    to remove the batch dimension on the imageã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: We make sure we're running on CPUã€‚ We detach the image tensor from computation
    historyã€‚ Otherwiseã€‚ the image tensor will keep tracking its computation history
    unnecessarilyã€‚And finallyã€‚ we make it a numpy array and switch the dimensions
    around and put the color channels lastã€‚The first argument of this method would
    normally be the attributionsã€‚ But for this callã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: we're going to make that noneã€‚ We're just displaying the original imageã€‚The
    second argument is our transformed imageã€‚Third argument is a visualization methodã€‚
    a string that indicates how you want the visualization to workã€‚Hereï¼Œ we told Captainã€‚
    we just want to display the aboginal imageã€‚Finallyï¼Œ we give our visualization
    an instructive titleã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The second call will make a visual mapping of the important regions of our imageã€‚The
    first argument is the attributions we got from integrated gradientsã€‚ And the second
    is our transformative imageã€‚For a method will specify heat napã€‚Where color intensity
    maps to the importance of an image regionã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Capize you to use custom color maps from map plotlibã€‚ And we've made one here
    that will slightly enhance the contrast of our heat mapã€‚We specify s as positiveã€‚
    We're only looking at positive attributionsã€‚Running a cellã€‚ we can see that the
    model is paying attention to the outline of the catã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: as well as the region around the centre of the cat's faceã€‚Let's try another
    feature attribution algorithmã€‚ Nextï¼Œ we'll try occlusionã€‚Integrated gradients
    was a gradient based attribution algorithmã€‚ Occlusion is differentã€‚ It's a perturbation
    based method that involves screening out portions of the image and seeing how
    that affects the outputã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: As beforeï¼Œ we're going to specify our input image and our output labeled to
    the attribution algorithmã€‚For occlusionï¼Œ we're going to specify a few more itemsã€‚
    The first are the sliding window and the stride lengthã€‚ And these are analogous
    to similar configuration options in a convolutional neural networkã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We're also going to set our baselineã€‚That isï¼Œ our representation of an occluded
    image cellï¼Œ 0ã€‚Depending on how your data are normalizedï¼Œ you may wish to specify
    a different baselineã€‚ but for zero centered dataï¼Œ it makes sense to use 0ã€‚We'll
    run the attribute call and give it a minuteã€‚And in the next cellï¼Œ we're doing
    something newã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: We're calling visualized image Adder multipleã€‚To show multiple visualizations
    of the occlusion attributionã€‚Besides the original imageï¼Œ we'll show three visualizationsã€‚
    The first two are heat maps of both positive and negative attributionsã€‚ You can
    see that we're providing a list of methods with heat map being the second and
    thirdã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: We're also specifying a sign for each visualizationã€‚ And here you can see that
    we've askedtra positive attributions on one heat map and negative on the otherã€‚
    These indicate whichã€‚For our final visualizationï¼Œ we'll use the mask methodã€‚ This
    uses positive attributions to selectively screen the original imageã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: giving a striking visual representation of the areas of the image the model
    paid most attention to for this input output pairã€‚Running the cellï¼Œ you can see
    that this maps well to what we learn from integrated gradientsã€‚ Most of the activities
    are on the cat's outlined in the center of its faceã€‚What about what the model
    is doing under the hoodã€‚Let's use a layer attribution algorithm to check the activity
    of one of the hidden layersã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Gradcam is another gradient based attribution algorithm designed for convesã€‚It
    computes the gradients of the output with respect to the specified model layerã€‚
    averagevers the gradients for each channel and multiplies this average by the
    layer activationsã€‚And uses this as a measure of the importance of a layer's outputã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To get started with layer attributionï¼Œ we'll create a layer gradcam object and
    initialize it with our model and the layer we wish to examineã€‚Then we'll give
    it the input output pair and ask it to do attributionã€‚We can visualize this with
    a heat mapï¼Œ as we did beforeã€‚ in this wayã€‚ you can visually examine which areas
    of a confidencein activation map were like to your outputã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: We can do better than thisï¼Œ thoughã€‚Since the output of a convolutional layer
    is usually spatially correlated to the inputã€‚ we can take advantage of that by
    up sampling that activation map and comparing it directly with the inputã€‚The layer
    attribution parent class has a convenience method for up samplingling the lower
    resolution convenant activation map up to the input sizeã€‚We'll do that with the
    interpolate method hereã€‚And asked the visualizer for a blended heat mapã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: showing the original image with a heat map superimposed and a masked imageã€‚Visualizations
    like this can give you insight into how hidden layers contribute to a particular
    output from your modelã€‚Captain comes with an advanced visualization tool called
    Cap Insightsã€‚ which lets you put together multiple visualizations in an in browserrowsed
    widget that lets you configure the attribution algorithm and its parametersã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Captain Inights lets you visualize textï¼Œ image and arbitrary dataã€‚We're going
    to try three images nowï¼Œ the catï¼Œ a teapot and a trilaite fossilã€‚Againã€‚ these
    images should be available wherever you got the interactive notebook that goes
    with its videoã€‚Firstï¼Œ we'll query the model to see what it thinks each of these
    areã€‚And it seems to be really okayã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Nowï¼Œ let's set up cap Insã€‚We're going to use the attribution visualizer objectã€‚
    and we'll configure it with our modelã€‚A scoring function for the model's outputs
    hereï¼Œ softmã€‚A list of the classes the model recognizes here I am stripping out
    an ordered list of the image net class namesã€‚We'll tell it that we're looking
    at image featuresã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Captive Insights also handles text and arbitrary dataï¼Œ as wellã€‚And it'll give
    it a data setã€‚ which is just an iterable that returns a batch of images and labelsã€‚Note
    that we haven't specified an algorithm or a visualization methodã€‚These are things
    that you set up in the in browserrower widgetã€‚Nowï¼Œ we ask the visualizer to renderã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: It starts off emptyï¼Œ but we can set up configuration parameters and ask it to
    fetch our visualized attributions with the fetch buttonã€‚I'm going to leave things
    at the default setting for integrated gradientsã€‚ Captain needs a few minutes to
    generate the attributionsã€‚But now we can see that it ranks the first few predictions
    for each image with their probabilities and provides heat map attribution for
    the important regions of the imageã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœ€åˆæ˜¯ç©ºçš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥è®¾ç½®é…ç½®å‚æ•°ï¼Œå¹¶é€šè¿‡æå–æŒ‰é’®è¯·æ±‚å®ƒè·å–æˆ‘ä»¬å¯è§†åŒ–çš„å½’å› ã€‚æˆ‘å°†ä¿æŒé›†æˆæ¢¯åº¦çš„é»˜è®¤è®¾ç½®ã€‚Captainéœ€è¦å‡ åˆ†é’Ÿç”Ÿæˆå½’å› ã€‚ä½†ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä¸ºæ¯ä¸ªå›¾åƒæ’åå‰å‡ çš„é¢„æµ‹åŠå…¶æ¦‚ç‡ï¼Œå¹¶æä¾›é‡è¦åŒºåŸŸçš„çƒ­å›¾å½’å› ã€‚
- en: In this wayï¼Œ Captain insightights lets you experiment with attribution methods
    and understand the activity that LED to your model's predictionsã€‚ both correct
    and incorrect and lets you do it visually with minimal codeã€‚Finallyã€‚ don't forget
    to look at Captain AI for documentationã€‚ tutorials and API reference and access
    to the source on Gitthubã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCaptain insightightsè®©ä½ å®éªŒå½’å› æ–¹æ³•ï¼Œç†è§£å¯¼è‡´æ¨¡å‹é¢„æµ‹çš„æ´»åŠ¨ï¼Œæ— è®ºæ˜¯æ­£ç¡®çš„è¿˜æ˜¯é”™è¯¯çš„ï¼Œå¹¶ä¸”ä»¥æœ€å°‘çš„ä»£ç è¿›è¡Œå¯è§†åŒ–ã€‚æœ€åï¼Œåˆ«å¿˜äº†æŸ¥çœ‹Captain
    AIä»¥è·å–æ–‡æ¡£ã€æ•™ç¨‹å’ŒAPIå‚è€ƒï¼Œä»¥åŠåœ¨GitHubä¸Šçš„æºä»£ç è®¿é—®ã€‚
- en: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_13.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fb9bd9df95d3221c8b5a298fc267de0_13.png)'
