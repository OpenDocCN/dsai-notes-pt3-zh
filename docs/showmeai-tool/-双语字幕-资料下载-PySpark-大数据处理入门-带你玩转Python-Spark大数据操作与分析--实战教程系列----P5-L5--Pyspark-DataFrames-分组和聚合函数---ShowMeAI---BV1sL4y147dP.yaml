- en: 【双语字幕+资料下载】PySpark 大数据处理入门，带你玩转Python+Spark大数据操作与分析！＜实战教程系列＞ - P5：L5- Pyspark
    DataFrames 分组和聚合函数 - ShowMeAI - BV1sL4y147dP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】PySpark 大数据处理入门，带你玩转 Python + Spark 大数据操作与分析！＜实战教程系列＞ - P5：L5- Pyspark
    DataFrames 分组和聚合函数 - ShowMeAI - BV1sL4y147dP
- en: 。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![](img/1babe07f0737b77a8ca240e4a3c70599_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1babe07f0737b77a8ca240e4a3c70599_1.png)'
- en: Hello all， my name is Krisnaak and welcome to my YouTube channel。 So guys we
    will be continuing the Pipar series and in this particular video we are going
    to see group by an aggregate function。😊，Already I have actually created somewhere
    around four tutorials on Pipar。 this is basically the fit tutorial。 And again，
    this is a part of a data frame。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，我的名字是 Krisnaak，欢迎来到我的 YouTube 频道。所以伙计们，我们将继续 Pipar 系列，在这段视频中我们将看到分组和聚合函数。😊，我实际上已经创建了大约四个关于
    Pipar 的教程，这基本上是第一个教程。这又是数据框的一部分。
- en: why we should actually use group by aggregate functions again for doing some
    kind of data preprosing。 So let's begin for this particular data for this particular
    problem I created a data which has three features like name departments and salary
    and you have some of the data like Kris data science salary。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要再次使用分组聚合函数进行某种数据预处理。让我们开始处理这个特定的数据，为这个特定的问题我创建了一些具有三个特征的数据，比如名称、部门和工资，你有一些数据，比如
    Kris 数据科学的工资。
- en: right， something like this。 So over here in short。 if I want to basically understand
    about this particular data they are some departments probably where Krisish and
    other people teach and based on different different departments。 they get a different
    different salary， So let's see how we can perform different different group by
    an aggregate functions and see how we can preproces or how we can get some or
    retrieve some kind of results from this particular data。 So to begin with what
    we are going to do we are a first of all going to import Pipar。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对，类似这样的内容。因此在这里简而言之。如果我想基本了解这个特定数据，可能会有一些部门，其中 Kris 和其他人授课，基于不同的部门，他们获得不同的工资。所以让我们看看如何执行不同的分组和聚合函数，以及我们如何进行预处理或获取一些从这个特定数据中检索的结果。首先我们要做的就是导入
    Pipar。
- en: '![](img/1babe07f0737b77a8ca240e4a3c70599_3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1babe07f0737b77a8ca240e4a3c70599_3.png)'
- en: '![](img/1babe07f0737b77a8ca240e4a3c70599_4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1babe07f0737b77a8ca240e4a3c70599_4.png)'
- en: SQL import。Spark session， as usual， we have to create a spark session。 So after
    this。 what we have to do， I'll create a spark variable。 So I'll use spark session，
    dot builder dot。Ap name， I think everybody must be familiar with this。 but again，
    I'm trying to show you this one。 So let me write it as aggregate。Dot。Get or create。
    So now I've actually created a spark session。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 导入。Spark 会话，和往常一样，我们需要创建一个 Spark 会话。那么在这之后，我们要做什么呢？我将创建一个 Spark 变量。所以我会使用
    Spark 会话，点构建器点。应用名称，我想大家一定对此很熟悉。但我再试着给你展示一下。所以让我把它写为 aggregate。点。获取或创建。所以现在我实际上已经创建了一个
    Spark 会话。
- en: Okay， probably this will take some time now if I go and check out my spark variable。
    so here is your entire information okay。With respect to this particular spark
    very well。 Now。 let's go ahead and try to read the data set。 Now， I will just
    write D F underscore pi spark。And then here I'll write Sp dot read。Dot CSv， the
    CV file name is basically test。3 dot Csv。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果我去检查我的 Spark 变量，这可能会花一些时间。所以这里是你所有的信息。关于这个特定的 Spark 变量。现在。让我们继续尝试读取数据集。现在，我将写
    D F 下划线 pi spark。然后在这里我会写 Sp 点读。点 CSV，CSV 文件名基本上是 test。3 点 CSV。
- en: And remember， I'll be giving this particular CV file in the Github also。 and
    then I'll be using header is equal to true。Coma infer schema is equal to2。Now。
    this is my dear underscoreosscope by spark。Now， what I'll do in the next statement。
    I will write Df underscorecope pipar dot show。Right now here you'll be able to
    see that I am actually being able to see all the data sets here I have name。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我会在 Github 上提供这个特定的 CSV 文件。然后我会使用 header 等于 true。逗号推断模式等于2。现在。这是我的 dear 下划线
    osscope by spark。接下来我将在下一条语句中写 Df 下划线 cope pipar 点 show。此时你将能够看到我实际上可以看到所有的数据集，这里我有名称。
- en: departments and salary on all this particular information。If I really want to
    see the schema or the columns like which all columns where it belongs to。 just
    like a data types， I can definitely use DF underscorecope Ipar。Dot print schema
    right and now here you can see name is a string department is string and salary
    is basically an in teacher okay。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 部门和工资在所有这些特定信息上。如果我真的想查看模式或列，例如哪些列属于什么。就像数据类型，我可以使用 DF 下划线 cope Ipar。点打印模式，现在你可以看到名称是字符串，部门是字符串，工资基本上是一个整数。
- en: Now， let's perform some group by operation。 First， we'll start by group by operation。Probably
    I want to group by name and probably try to see what will be the mean average
    salary。You know what suppose let's let's take a specific example over here。 So
    I'll write Pf dot underscorecope bypar dot group by。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行一些分组操作。首先，我们将从分组操作开始。可能我想按名称分组，并尝试查看平均薪水是多少。你知道吗，假设我们在这里拿一个具体的例子。所以我会写
    Pf dot underscorecope bypar dot group by。
- en: supposeupp I want to go and check who is having the maximum salary out of all
    these people that are present in this particular data set。 So I'll first of all
    group by name if I execute this you can see that we will be getting a return type
    of group data at some specific memory location。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我想去检查在这个特定数据集中，谁的薪水是最高的。所以我首先按名字分组。如果我执行这个，你可以看到我们将得到一个返回类型为组数据的某个特定内存位置。
- en: And you should always know that guys group by aggregate functions works together。
    That basically limits is first of all， we we need to apply a group by functionality。
    and then we need to apply an aggregate function。 So aggregate function here really
    want to check just press dot and press tab So here you'll be able to see a lot
    of different different function examples like aggregate average count max mean
    pi and many mode right now what I'm going to do I'm just going to use this dot
    sum because I really need to find which is the maximum salary。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该始终知道，伙计们，group by 聚合函数是一起工作的。首先，我们需要应用 group by 功能。然后我们需要应用聚合函数。所以在这里，聚合函数我真的想要检查，只需按
    dot 然后按 tab。在这里，你将能够看到很多不同的函数示例，比如聚合、平均、计数、最大值、平均值、π 以及很多模式。现在我将做的是使用这个 dot sum，因为我真的需要找出哪个是最高的薪水。
- en: 😊，HoweverFrom out of all these particular employees， who is having the maximum
    salary。 So here I'll say dot sum。 And if I execute it， you'll be able to see that
    we are getting a sQL dot data frame。 which has name and sum of salary。 This is
    very much important as sum of salary because I really want to have the sum of
    the salary。 remember， we cannot apply sum on the string。 So that is the reason
    it has not done over here。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，然而在所有这些员工中，谁的薪水是最高的。所以我会说 dot sum。如果我执行它，你将能看到我们得到了一个 sQL dot data frame。里面有姓名和薪水的总和。这非常重要，因为我真的想要薪水的总和。记住，我们不能对字符串应用
    sum。这就是为什么它在这里没有执行的原因。
- en: it is just giving you the name because we have group by name。 And this dot sum
    will just get applied on this particular salary。 Now。 if I go and write dot show
    here you will be able to see。😊。Sudanhu over here is having the highest salary
    of 35000。 Sunny has 12000。 Krissh has 19000。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是给你名字，因为我们按名字分组。这个 dot sum 将只应用于这个特定的薪水。现在，如果我去写 dot show，你将能够看到。😊Sudanhu
    在这里的薪水是最高的 35000。Sunny 有 12000。Krissh 有 19000。
- en: Mahesh has 7000。 So if you go and see over here， Sudanhu is basically present
    here。 here and in big data。 So overall， his salary should be 35000 if you compute
    it Similarlyly。 you can go and compute my salary over here。🤧Over here by just
    calculating this。 and then you can also compute sunny salary。 and you can also
    see my H。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Mahesh 的薪水是 7000。所以如果你去看这里，Sudanhu 基本上在这里。在大数据中。所以如果你计算，总体他的薪水应该是 35000。类似地，你可以计算我在这里的薪水。🤧在这里通过简单计算这一点。你还可以计算
    Sunny 的薪水。你也可以看到我的 H。
- en: So this is one just an example。 So here I'll just write。 we have grouped。Do
    fine。The maximum salary。AndDefinite over here from this entire observation。 we
    can retrieve that Sudanhi is having the highest salary。 Okay， now let's go to
    one step ahead。 one more step ahead。 now we will try to group by departments to
    find out which department gives maximum salary Okay。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是一个例子。在这里我会写一些内容。我们已经进行了分组。做得很好。最高薪水。并且在整个观察中可以确定。我们可以得出结论，Sudanhi 的薪水是最高的。好吧，现在让我们向前迈一步。再向前一步。现在我们将尝试按部门分组，以找出哪个部门提供最高薪水。好的。
- en: we are going to do a group by。Departments。Which gives maximum salary。 Suppose
    this is my。This is my requirement。 Okay， and different different types of requirement
    makeup up。 I'm just trying to show you some examples。 So I'm just going to copy
    this。南 going do。Use this department。Okay， and then I'm basically going to say
    dot sum dot show。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行一个按部门分组的操作。哪个部门提供最高薪水。假设这是我的。这个是我的需求。好的，和不同的需求组合在一起。我只是想给你展示一些例子。所以我将复制这个。南将会做。使用这个部门。好的，然后我基本上将说
    dot sum dot show。
- en: If I execute it let me see department is a wrong column name so I'll write department
    it is department So let me write S now if I go and see Iot over here gives some
    salary around115000 to this employees to all the employees right combined because
    we are doing the sum big data gives somewhere around 15000 data science gifts
    Im around 4300 Now suppose if I go and see big data over here 404080008000 and
    13000。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我执行它，让我看看，部门是一个错误的列名，所以我会写部门，它是部门。所以让我现在写 S。如果我去看看，IOT 在这里给这所有员工的薪资大约是 115000，因为我们在进行总和，大数据给的薪资大约是
    15000，数据科学的薪资是 4300。现在假设我去看看大数据这里是 404080008000 和 13000。
- en: 130015000 so I hope I'm getting yes， big data is actually giving us 15000 so
    you can go and calculate it suppose if you want to find out the mean you can also
    find out the mean okay。 so let me just write it over here， just copy this entire
    thing paste it over here and write me write instead of instead of sum I'll write
    to write mean so by default the mean salary。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 130015000，所以我希望我得到的答案是，是的，大数据实际上给我们 15000，所以你可以去计算，假设你想找出平均值，你也可以找到平均值。好吧，让我在这里写，只需复制这个整体内容，粘贴在这里，写我，写而不是总和，我会写平均值，所以默认的平均薪资。
- en: Here you can see that for a particular employee somewhere for I O T to 7500。
    because this mean will be based on how many number of people are working in the
    department， right。 So like this， you can actually find out Now， I can also check
    one more thing。 guys， I can copy this。 I can try to find out how many number of。Employees
    are actually working based on the department。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到，对于某个特定员工，I O T 的薪资大约是 7500。因为这个平均值将基于在该部门工作的人员数量，对吧。所以这样你实际上可以找到。现在，我还可以检查一件事。伙计们，我可以复制这个。我可以尝试找出有多少员工实际上是基于部门工作的。
- en: so I can use dot count。 And then if I go and execute this， probably this is
    a method。Okay。Now here you will be seeing that in Io there are two people in big
    data there are four people in data science they are four people。 so4 plus 4 plus8
    total employees that are present over here is basically 10。Now。 one more way that
    I can basically apply a directly aggregate function also to see。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我可以使用 dot count。如果我去执行这个，可能这就是一个方法。好的。现在你会看到在 I O 中有两个人，在大数据中有四个人，在数据科学中也有四个人。所以
    4 加 4 加 8，总共有的员工在这里基本上是 10。现在，我还可以直接应用聚合函数来查看。
- en: these are all some of the examples and again， you can do different different
    group by let me use Df Ppar。 suppose I say dot aggregate okay， and inside this
    I will just give my key value pairs like this。 suppose I say let me say that salary，
    I want to find out the sum of the salaries。The overall salary that is basically
    given to the entire total expenditure insert。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是一些示例，再次强调，你可以通过让我使用 Df Ppar 进行不同的分组。假设我说 dot aggregate，好吧，在里面我只需提供我的键值对。假设我说，让我找出工资的总和。我想要找出整体的薪资总支出。
- en: So the total expenditure that you will be able to see somewhere around 73000
    alright。 so we can also apply direct aggregate function， otherwise this all are
    also aggregate function which we basically apply after after you know applying
    a group by function now suppose this are probably the salary I want to find out
    suppose I'll take this example I want to find out the maximum salary that the
    person is basically getting who is getting the maximum salary sorry。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你将看到总支出大约在 73000 左右。我们也可以直接应用聚合函数，此外，这些都是在应用分组函数后我们基本上应用的聚合函数。现在假设这些可能是薪资，我想要找出假设我举这个例子，我想找出获得最高薪资的人的薪资，抱歉。
- en: So here instead of writing dot sum now I'll write max dot show now here you
    can see Sudansha is basically getting 20000 over here10000 Krisish is getting
    10000 my is getting for 4000 right so all this particular data is theyy Kris is
    basically getting with respect to data science over here 10000 so it has basically
    picked up it is not picking up both the records but at least when it is grouping
    by name and then it is showing this particular data that time you will be able
    to see it let's see whether Ill be also able to see this or not。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里，我将写 max dot show，而不是 dot sum。现在你可以看到 Sudansha 在这里获得 20000，Krisish 获得 10000，我获得
    4000，对吧。所以所有这些特定数据是 Kris 在数据科学中获得的薪资为 10000，所以它基本上挑选了，并没有选择两个记录，但至少当它按名称分组并显示这个特定数据时，你将能够看到。让我们看看我是否也能看到这一点。
- en: So group by， if I score and write min。So here you will be able to see minimum
    value with respect to different different records when Im grouping by here you
    will be able to see that Suanhu sorry。 Sudanhu is getting a minimum salary of
    50002000 Kris is getting a minimum salary of 4000 right。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以分组，如果我评分并写下最小值。这里你将能够看到不同记录下的最小值。当我分组时，你将看到Suanhu的最低薪资是5000，而Kris的最低薪资是4000。
- en: we can also get that particular information。 Now let's see what all different
    different types of operation are there。Average is also there。 So if I write A
    V G， it's just like mean only guys。 So this is basically the mean salary that
    probably again。Again。 you can check out different different functionalities why
    these all things are basically required。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以获取那个特定的信息。现在让我们看看有哪些不同类型的操作。平均值也在其中。所以如果我写AVG，这就和平均数一样。基本上这是可能的平均薪资。你可以查看这些功能，了解这些操作的必要性。
- en: Under one thing is that you really need to do a lot of data preprosing a lot
    of retrieving skills that you basically do。 You can check it out this one and
    you can do different different functionalities as you like。 So I hope you like
    this particular video Probably in the next video I'm going to start with spark
    Mlib libraries where we'll be solving some machine learning algorithm problems。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一件重要的事情是，你确实需要进行大量的数据预处理和检索技能。你可以查看这个，并根据自己的需要执行不同的功能。所以我希望你喜欢这个特定的视频。可能在下一个视频中，我将开始介绍Spark
    Mlib库，我们将解决一些机器学习算法问题。
- en: '![](img/1babe07f0737b77a8ca240e4a3c70599_6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1babe07f0737b77a8ca240e4a3c70599_6.png)'
- en: So I hope you like this particular video I'll see you all in the next week。
    Have a great day thank you baba。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我希望你喜欢这个特定的视频，下周见。祝你有美好的一天，谢谢。
