# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P15ÔºöL15- Ëá™ÂÆö‰πâÊ®°Âûã‰∏éÊãüÂêà - ShowMeAI - BV1em4y1U7ib

AlrightyÔºå what is going on guys„ÄÇ Welcome back for another video In this video„ÄÇ we're gonna to explore how to build more flexible training loops„ÄÇ So far we've been using model that fit and if you can use model that fit that's great„ÄÇ but sometimes you need more flexibility„ÄÇ So in this video we will look at customizing model that fit„ÄÇ

 And then in the next video how we will look at how to build custom training loops from scratch„ÄÇ![](img/a08532ac4d7c10fff24b74debc5b0588_1.png)

![](img/a08532ac4d7c10fff24b74debc5b0588_2.png)

All rightÔºå so first of allÔºå here are just some basic importsÔºå those are you've all seen those before„ÄÇ and then we're just going to load the the Ms data set„ÄÇSo we're not going to do anything complicated„ÄÇ I'm just going to show you the general structure and then that can be applied to many different problems„ÄÇAlrightÔºå so we're going to xtrain y trainÔºå x testÔºå y testÔºå and we're just going to M load data„ÄÇ

 then we're going to do xtrain is xtrain dot reshape then we're just going to have„ÄÇ I guess minus1 for all the examples and then 28281 and we're doing reshape here just to add this channel right here„ÄÇAnd then as type converted to flow 32 and then normalize with dividing by 255„ÄÇ So let let's copy this and let's go new line„ÄÇAnd do X test„ÄÇtestÔºå and then„ÄÇLet's create our model„ÄÇ

 first of allÔºå so as the model is equal to ks that sequential„ÄÇThen we're going to do layer thatt input„ÄÇAnd then the shape of the input is 28Ôºå281„ÄÇLayers come to the 64Ôºå3 kernel size and I'm padding same so I'm just going through this quickly„ÄÇ this is not really the most important part of the video„ÄÇüòîÔºåSo now that we have a model„ÄÇ

 we're going to create a class and we're going to call it custom fit„ÄÇAnd we're going inherit from Kaosta model„ÄÇ and then the first thing we're going to do is we're going to create an init function and all we're going to send in here is the model„ÄÇ So we're going to first call super to inherit from Kaosta model„ÄÇ So we're going to self and then in it„ÄÇThen we're going to do set that model equal to model„ÄÇ

Then what we're going to do is we're going to define one training step and that's going to be used in a model dot fit right„ÄÇ So our goal is basically„ÄÇWe want to do something like training is a custom customÔºå wait custom„ÄÇ what the hell custom fit of that model„ÄÇ we're gonna to send in that model„ÄÇ Then we're going to do training dot fit and we're gonna to send in x trainÔºå Y train„ÄÇ

And then batch size and then number of vpos sort of as normal„ÄÇ although this dot fit is going to be done in a custom way„ÄÇ we're going to sort of define how we want that to be done„ÄÇSo I mean there are many use cases of this you where you need to do custom training loops and sort of you use model Efi when you can and when you can't you try to customize your model thatfi which is what we're doing in this video and then for that most flexibility you do the training loop from scratch but an example of when you actually need to do this is generative adversarial networks I'm not assuming you're familiar with that I'm just sort of saying there are many examples where„ÄÇ

This is useful„ÄÇ AlrightÔºå so let's then do a train step„ÄÇ we're going to send in data and then we're going that's going to be a twople of x and y„ÄÇ so we're just going to do x and y is equal to data„ÄÇThen what we're going to do is we're going to do with„ÄÇTF gradient tape as tape„ÄÇ

And why we're doing this is because now we're going to do the for propagation and then the loss function„ÄÇ and when we're doing it under that tapeÔºå it's going to record all of the operations that was done and then that will then be useful for calculating the gradients for back propagation„ÄÇ

So basically we're going to do y prediction is a self dot model„ÄÇ We're going to send in x„ÄÇ we're going to specify training is trueÔºå and then for this loss function„ÄÇ we're going to do loss equals self dot compiled loss„ÄÇ and then we're going to send in y and then y prediction„ÄÇ

 and this is going to be done in the compile So right here we're going to do training dot compile we're going to send in„ÄÇAnd here we're going to send in optimizer is Kara's OprsÔºå Adamom„ÄÇ then we're going to send in loss is Kara's lossesÔºå sparseÔºå categorical cross entropy„ÄÇ and from logic equals true„ÄÇThen also we're going to do metrics is accuracy„ÄÇ

And so this is for the first one where we're doing the compile„ÄÇ I'm also going to show you how to do a custom compile„ÄÇBut let's take that as we go„ÄÇ so we're going to first now continue doing the train step when we have this compile„ÄÇ and so this self dot compiled loss is using this sparse category across entropy from the training dot compile„ÄÇ

After that we basically want to get the gradients rightÔºå we've now done the for propagation„ÄÇ this part is the for propagationÔºå which we're doing with this gradient under this tape to record all of the operations„ÄÇThen we're going to do training variables is a self dot trainable variables and these are all stored from this parent class„ÄÇ this cars dot modelÔºå so we don't have to bother with thatÔºå then we want to get the gradient„ÄÇ

 we're going to do tape dot the gradient„ÄÇAnd we're going to do loss and then training variables right„ÄÇ so we're getting the gradient of the loss with respect to the training variables„ÄÇ which is ultimately what we want to change„ÄÇThen we're going to do a stepÔºå an optimizer step„ÄÇ a gradient in descent step„ÄÇ and we're going to do self dot optimizer dot apply„ÄÇGradients„ÄÇ

And then here we're going do zip gradients and then training variables„ÄÇ And then we're going to do self that compiled metrics that update state y and then y prediction„ÄÇ And this is this is going to be for the accuracy„ÄÇ And then in the endÔºå we're going to return„ÄÇM dot name„ÄÇAnd you'll see what it means so M dot name M dot result for M in self dot metrics allright„ÄÇ

 so we we're getting the MD dot name which is going to be the loss for example„ÄÇ and then we're getting the result which is the current loss and then we're doing that for all of the metrics and that's going to be the loss and the accuracy in this case„ÄÇAnd yeahÔºå so I think that's it for just this first step„ÄÇ and I think we should now be able to run this„ÄÇAnd as you can see hereÔºå it does seem to work„ÄÇ

 and yeah so basically„ÄÇSo basically the next step now is that we want to do our own compile„ÄÇ so what we're going to do right here is we're going to define compile„ÄÇ we're going to send in the optimizer and we're going to send in the loss„ÄÇAnd we're going to do super custom itself that compile„ÄÇSo yeah„ÄÇ

 and then we're going to do self that optimizer is equal to optimizerÔºå Se that loss is equal to loss„ÄÇAnd all we have to do then is we have basically the same thing rightÔºå training„ÄÇtcomp„ÄÇ except we're not going to send in a metric right here„ÄÇ so we're just going to use the optimizer and the loss„ÄÇAnd„ÄÇ

And that should also basically be it now we just have to change this right here to this compiled loss„ÄÇ we're just going to do self that lossÔºå it's which we've stored right here„ÄÇSo self dot loss and then let's seeÔºå yeahÔºå and then we can still use self dot optimizer and let's just rerun it„ÄÇAnd nowÔºå as you can seeÔºå we're not getting an accuracy„ÄÇ

 So we're going to have to keep track of a that metric by ourselvesse„ÄÇ So what we can do is for exampleÔºå we could„ÄÇCreated right here„ÄÇ we could do„ÄÇAccuracy metric is„ÄÇCa us that matrix that spae categorical accuracy„ÄÇAnd let's just call it name„ÄÇ's see name equals„ÄÇAccuracy„ÄÇAnd thenÔºå in the„ÄÇAnd then right hereÔºå inside of the compiled metric„ÄÇ

 what we're going to do is is accuracy metric that update state„ÄÇ we're going to send in Y and then y predictionÔºå and then we can remove this compiled metric„ÄÇÂóØ„ÄÇAnd yeahÔºå so that should hopefully be it„ÄÇ Let's see if we can run this„ÄÇAll right„ÄÇ so now since we're keeping track of the accuracy by ourselves„ÄÇ

 what we're going to do here is we're going to write it explicitly„ÄÇ so we're going to do loss is in this caseÔºå just loss„ÄÇAnd then we're going to do accuracy„ÄÇIs„ÄÇAccury metric dot result„ÄÇAnd hopefully now we should get the loss and the accuracy Yeah„ÄÇ so this looks pretty familiar to what we did previously„ÄÇ

 except now we're doing the compile completely by ourselves„ÄÇAnd then allright„ÄÇ so now we got the compileÔºå we got a train step what we normally do as well is in the end after training„ÄÇ we're doing training that evaluate and then x testÔºå Y testÔºå and then we're specifying batch size„ÄÇ let's say 32„ÄÇOne thing here is that this dot fit works on the train step and then evaluate works on a test step„ÄÇ

 so to make this work we actually need to define another function and we need to do test step although this one is going to be a little bit easier since well first of all we're going to unpack the data and then we're going to compute prediction so we're going to do by prediction is self„ÄÇ

t model X and then we're specifying the training is false„ÄÇAnd what we're doing is this is if we're using batch norm or dropout„ÄÇThat has different behaviors during testing and training we're just telling the model this is now in testing„ÄÇ so make sure that those modules that have different behaviors are set to test mode or evaluation mode„ÄÇ

Then we're going to compute the lossÔºå which is sub loss of y prediction„ÄÇAnd then we're going do accuracysymmetric do update state y y prediction„ÄÇAnd in the end„ÄÇ we're going to return a dictionary of lossÔºå which is just going to be loss„ÄÇ and then accuracy„ÄÇWe're doing acrosymmetric that result all rightÔºå so this is a very like it's very similar to the training step„ÄÇ

 although it's much more simplified and it's simplified because we're not doing a gradient descent update so we don't need to keep track of this tape of making sure that we have all the gradients and all of that stuff„ÄÇAnd yeahÔºå so let's run this for yeah two epochs and then let's do the evaluation„ÄÇAll right„ÄÇ

 so after thisÔºå we see that we get 93 up the  first0poCÔºå 97 and then almost 98 on the test set„ÄÇBut yeahÔºå so I mean what we want to establish here that this does seem to train and it's working so yeah that's how you create your own you know specifying the training step and a test step which overwrites how the training do fit and then training that evaluate is done so in this way you can build more complicated and complex models in the training steps but still have the flexibility of doing training dot fit and that means that you can still use the training do compile although in this last one we overwrote the„ÄÇ

The compileÔºå but but you get the point in you can still use the their compile and the metrics and all of that stuff Yeah„ÄÇ if you have any questions leave them in the comment section below thank you so much for watching the video and I hope to see you in the next one„ÄÇ



![](img/a08532ac4d7c10fff24b74debc5b0588_4.png)