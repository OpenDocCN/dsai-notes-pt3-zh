# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P19ÔºöL19- ‰ΩøÁî® TextLineDataset Ëá™ÂÆö‰πâÊñáÊú¨Êï∞ÊçÆÈõÜ - ShowMeAI - BV1em4y1U7ib

What is going onÔºå guysÔºå Welcome back for another video„ÄÇ So in this video„ÄÇ we're gonna take a look at how we can use Tensorflow textline data to be able to load our own custom text dataset that we might be working with for our project„ÄÇ So let me show you the dataset that we're mainly going work with in this video„ÄÇ So this dataset by the way if you're familiar is a sentiment analysis data where we get a movie review and we're supposed to tell if the review is positive or negative„ÄÇ

 All rightÔºå so for exampleÔºå in this caseÔºå we have once againÔºå Mr„ÄÇ Cosner has dragged out a movie for far longer than necessaryÔºå and then it goes on„ÄÇ then we also have a column for the label if it's negative or a positive review And then we have„ÄÇ if it belongs to the test set or the training set„ÄÇ

 one thing to keep in mind here is that this dataset actually has unsupervised reviews as well„ÄÇ So that is one thing that we're gonna keep in mind when we're cleaning and filtering this dataset to be able to„ÄÇ![](img/fd1d96600c664ffac3a740a72c447896_1.png)

for So one thing I do want to say is that every data is going to be differently structured„ÄÇ but I think that showing you how to deal with this data in particular„ÄÇ using textline data is going to give you an understanding of dealing with text data that extends to many„ÄÇ many different kinds of data I'm also at the end of the video going to show you a couple of ideas of what to do when dealing with dataset sets that aren't structured in a similar fashion as this one So let's get started„ÄÇ



![](img/fd1d96600c664ffac3a740a72c447896_3.png)

AlrightÔºå so let's start with creating a text line data set„ÄÇ So we're going to do Ds train is TF dot data do text line data set„ÄÇAnd we're going to send in that emdb„ÄÇcsv file that we just took a look at„ÄÇ and then as you saw this file right here is going to contain the example both from the training and the test set„ÄÇ

 so what we can do is we can create another one T test to be Tf data text line data set of imMDb„ÄÇ cv„ÄÇOh IMDb do CSV and then end quotes So what we're going do here is for exampleÔºå for the Ds train„ÄÇ we're going to try to filter out all of the examples that that belongs to the test set and then the opposite for for the test that we're going to try to filter out all that belongs to the training set So before doing anything it might be good to just know that this works So we're going to do for line in DSs train and we're going to print line All right and if we do this it's going to print every line sequentially so what we might we might not want to do that„ÄÇ

So what we can do is we can do first of all dot skip1 this is going to skip the one that sort of the first row which describes the all of the columns and then we can do dot take and we're just going do dot take5 so that means that we're gonna just take five examples So let's look at and so let's run that and see how it looks like So as you can see we get the first example or index01 and then we get the the review right here„ÄÇ

 we get the test set and then we get that negative review So what we want here is this right here we want this and we can get it we can get that by doing printf dot string dot split that line and then we can do is separated by by a comma and's essentially going split the line by a comma and in this case it's actually going to split the review as well So what we could do„ÄÇ

To avoid that is if we could send in an argument right here to be max split equals 4„ÄÇ So it's gonna split this oneÔºå this oneÔºå this oneÔºå this one„ÄÇ and then it's going to split all the restÔºå which is the review just once„ÄÇ So it's not gonna continue splitting the the review„ÄÇ So we can rerun that and see how it looks like„ÄÇ

 And as we can see here now it's doing that on a single line„ÄÇAlrightÔºå so what we want here„ÄÇ first of allÔºå is the test setÔºå rightÔºå and that would be the the first index here„ÄÇSo the thing we're going to do is we're going to create a function and we're going to call it filter train„ÄÇAnd we're going to send in one line„ÄÇAnd the first thing we're going to do is we're going to split line and we're going to do TF string„ÄÇ

 start split the lineÔºå and we're going to split it by a comma„ÄÇ And then we can send that send in that max split equals 4„ÄÇSo then we can first of all„ÄÇ see what is the belonging of that particular example so we could call it data belonging„ÄÇ so this is either going to be training or test set„ÄÇSo we're going to do that by split line index1„ÄÇ

But then we can also get the sentiment categoryÔºå which is split line of index 2„ÄÇ so this is either going to be positive negative or it's going to be unsupervised and we don't want the unsupervised ones to be in our data set and so we're going to filter based on these two right here and the data belonging can be train or test as we saw in the as we saw previously All right„ÄÇ

 so what we're going to do here is we're just going to return true„ÄÇLet's see if we can do it like this„ÄÇ return true„ÄÇTrueÔºå if„ÄÇData set belonging is equal to train„ÄÇAnd if the sentiment category„ÄÇIs not unsupervised All right„ÄÇ then that's an example that we want to have in our Ds train„ÄÇSo then if it's not„ÄÇ

 we're going to return false„ÄÇSo what we can do here then is we can do dot filter and we can send it through this filter function„ÄÇ which we you just called filter train„ÄÇSo if we would now do the same thing we're calling DSstrain again„ÄÇ we can see right here that we're just obtaining the ones that belong to the training set and that's exactly what we want for the DSs train So what we want to do and now is we want to have an identical one for the the test set so we're just going to call it filter test and then the rest should be exactly the same except this should be test right there and then we're going to do the same thing on a test set filter and then we're gonna send it through filter test All right so one thing here is that you might actually want to do this beforehand so you might want to just split them into two different CSsv files so that you would have for example„ÄÇ

 trained CSsv and then test csv and in this way you're going to save on some compute by not having to filter the examples sort of when you're actually training the model but you might need to use this filter function for something else so it's good to„ÄÇ

Kknowow that it exists and how to use it„ÄÇ So now that we've actually done that„ÄÇ what we want to do is we want to let's see if we can make some kind of to do list„ÄÇSo we want to„ÄÇ first of allÔºå we want to create some kind of vocabulary right If we're gonna send this into our model„ÄÇ we want to create a vocabulary„ÄÇ first of allÔºå and I've shown how to do this in a previous video in Tensorflow covering Tensorflow data„ÄÇ

 but I'm going to show it again because I think it's very valuable and could help you out„ÄÇ So then when we've created the vocabulary„ÄÇ we want to be able to to be able to numericalize your your text string„ÄÇ sort of the text string to indices right and we're going to use token see if it's token text encoder„ÄÇ I believe it's called let me check that„ÄÇ YeahÔºå so it's called token text encoder that's going do that for us„ÄÇ

 And then lastlyÔºå we're going to use we want to pad de batches so we can send in„ÄÇS in„ÄÇTo and R and N„ÄÇ for example„ÄÇRightÔºå so that's sort of what we want to do now„ÄÇ And to be able to create the vocabularyÔºå we first want to have a tokenizer„ÄÇ and there are a couple of different tokens„ÄÇ The one I'm going to use is tokenizer at Tensorflow dataet„ÄÇ

 TFTS do features do text dot tokenizer„ÄÇAnd we're going to create a function„ÄÇ So we're going to call build vocabulary„ÄÇ And so the tokenizer is going to split a sentence„ÄÇ So let's say I have a sentence„ÄÇ I love banana„ÄÇ Then that's going split it into a list with I love„ÄÇBananaÔºå alrightÔºå And then that's gonna then be able to get tokenized„ÄÇ

Or rather sorry that's going to be able to be numericalized that's our step2 to something like the index„ÄÇ the index for this particular word in our vocabulary so that's let's say we just have those three words maybe that would be 0„ÄÇ1 and 2 and then this is something that can be sent in to our R andN later on so for the building of the vocabulary we're going to send in some data and maybe DSs train we're going to send in DSs train we're going send in some threshold and we're going send in some threshold because we want if if this word occurs this many times then we want to include it in our data„ÄÇ

And in this caseÔºå we're going to chooseÔºå I don't knowÔºå let's say 200„ÄÇ this is going to be dependent on dataÔºå but let's just choose 200 because it's a relatively large data set„ÄÇAnd also there are different ways to build a vocabulary„ÄÇ and you can imagine doing this in multiple waysÔºå and I'm perhaps showing you a more simple one„ÄÇ

So first of allÔºå what we're going to need is frequenciesÔºå which is going to be just a dictionary„ÄÇThen we're going to do vocabularyÔºå and we're going to do a set„ÄÇAnd first of all„ÄÇ we're going to do vocabulary dot update„ÄÇAnd we're going to send in some start token all right start of sentence token and that's just what we call it in this example„ÄÇ And then we have vocabulary dot update and we have end of sentence right so EOS for end of sentence and then token So we want to have those in our vocabulary and then we're going to do four line in DS train dot skip of one So we're going to skip the first row and we're going to do split line Tf string dot split line„ÄÇ

Coma and then max split equals 4„ÄÇ Then we're going to obtain the text or the review„ÄÇ and we're going to do that by split line of index 4„ÄÇThen we're going to get the tokenized text„ÄÇAnd to get itÔºå we're going to do tokenizer dot tokenize of text„ÄÇ and we need to convert it to numpy as well„ÄÇ And then we're also going to do dot lower because if the letters or if the word is capitalized„ÄÇ

 then it doesn't really mean anything„ÄÇ We're just going to make everything lowercase to reduce the number of words in our vocabulary„ÄÇ then we're going to do four word„ÄÇIn tokenized text„ÄÇWe're going to do if we're not in this frequencies dictionary„ÄÇWe're going to add it„ÄÇ so we're going to do frequencies of word„ÄÇ we're going to set that to one„ÄÇ

But if it is in our vocabularyÔºå so else we're going to do frequencies of word„ÄÇ we're going to add one to that„ÄÇThen we're gonna check if we've reached the thresholdÔºå right„ÄÇ That's the one we sent to the top here„ÄÇSoÔºå if frequencies„ÄÇOf word„ÄÇIs equal to some threshold„ÄÇThen we're going to do vocabulary dot update and then tokenize text„ÄÇAlright„ÄÇ

And then at the end of thisÔºå we're just going to return the vocabulary„ÄÇIt's quite a simple function„ÄÇAnd we're just going to do that by sending in our DSs train„ÄÇ So what we can do then is we can do vocabulary„ÄÇEquals build vocabulary„ÄÇSo calling that function„ÄÇ and we're just going to send in DSs train„ÄÇNowÔºå we might not want to do this every time because it might take a lot of„ÄÇ

 might take some time to compute this„ÄÇ So what we can do then is we can do vocab file„ÄÇ We can open vocabulary dot„ÄÇOÔºå PGÔºå and then we can do„ÄÇRight by„ÄÇWe're going to do pickle„ÄÇ so we're going to use pickle„ÄÇ that's why we imported it„ÄÇ We're going to do pickle do dump vocabularyÔºå and then vocab file„ÄÇAlrightÔºå so this would„ÄÇ

Build vocabulary and save it to vocabulary dot O B G„ÄÇ rightÔºå So then if we've created it„ÄÇ maybe we want to have something for loading it so we could have„ÄÇWe can do by vocab file as„ÄÇOpen„ÄÇVocabababularyÔºå that object„ÄÇ and then our„ÄÇRB for reading it„ÄÇ and then vocabulary is pickle dot load that vocab file„ÄÇAlrightÔºå in this case„ÄÇ

 let's just uncommon that„ÄÇ first of allÔºå So we're just gonna build a vocabulary and then we can save it if we want to run it at some other time„ÄÇAll rightÔºå so then we're going to create some encoder that can do the numericalizing„ÄÇ which is converting the tokenized string to indices„ÄÇSo we do that by encoder is Tfds„ÄÇfeatures„ÄÇtext„ÄÇ token text encoder„ÄÇAnd first of allÔºå we're going to send in the list of the vocabulary„ÄÇ

Then we're gonna specify some end of vocabulary token„ÄÇ and we're gonna specify that to be the„ÄÇUnknown„ÄÇAnd those are going to be all the words that didn't„ÄÇThat wasn't inside of our data set of a frequency of at least 200„ÄÇ Then we're going to do lowercase equals true„ÄÇ and we're also going to set the tokenizer„ÄÇ

 which is just tokenizer in this case„ÄÇ AllrightÔºå then we're going to define my encoder„ÄÇAnd what we get here is we're gonna get some text tensorÔºå and we're gonna get some label„ÄÇ And the first thing we're gonna to do is encoded text is justÔºå we're gonna use our encoderÔºå right„ÄÇ this token text encoder„ÄÇcanSo to numericalize this text tensor that we send in„ÄÇ

So we do that by encoder dot encodeÔºå and we do texttensor„ÄÇ nuy„ÄÇAnd then we're just going to return the encoded text comma label„ÄÇAll right„ÄÇ now one thing we need to do in TensorflowÔºå which is a little bit unnecessarily complexÔºå perhaps„ÄÇ but we need to make sure that this Python function is a part of our computational graph„ÄÇ

So we need to define another function„ÄÇ We're gonna call it encode„ÄÇMap function„ÄÇ And we're gonna send in a line right here and that's gonna be just the long line of of the sort of the label„ÄÇ the review and all of that that we saw and printed before„ÄÇ So the first thing we're gonna do a split line„ÄÇ We're going do TF strings that split line comma separator and then max split equals 4 again„ÄÇ

 then we're gonna get the label stringÔºå that's going be the split line of index 2„ÄÇ So I'm not going print the line again„ÄÇ and just believe me this is index 2 of of that„ÄÇ If we did the split that's going to be index 2„ÄÇ So this is going to be negative or positive„ÄÇAnd that is going to be that because we also removed all new ones that are unsupervised„ÄÇ

Then the review is going to beÔºå and we're going to add a start token right here in the beginning„ÄÇ we're going to add it with split line of of index 4„ÄÇ So that's sort of the last one„ÄÇ which is going to be a long sort of text string„ÄÇAnd then we're going toÔºå in the end„ÄÇ also add end of sentence token„ÄÇ AllrightÔºå so this is our review„ÄÇ

 and this is going to then be sent to this token text encoder„ÄÇ which is going to be the do the tokenization„ÄÇ It's going to do lowercase„ÄÇ It's going to then map it to some numerical values„ÄÇ and we can later send that into our model„ÄÇSo the label is going to be one„ÄÇThe label string is equal to positive„ÄÇ

 otherwise it's going to be zero for negative encoded textÔºå comma label„ÄÇIs going to be equals to Tf that pi function„ÄÇAnd we're going to do my encoder the one be defined above„ÄÇ the input is going to be the review and also the the label„ÄÇThen we're going need to specify the output and the output in this case is going to be a T F in 64 and then T f into 32„ÄÇ

 So this is for the„ÄÇThe the sentenceÔºå the review and this is going to be for the label which is just going to be0 or1 then the encoded text dot set shape„ÄÇ this is something we need to do for the computational graph we're going to set that to none in a list and we're going to set it to none because the review length can be varied a review could be just you know 500 characters or it could be 100 characters or then we're going do label dot set shape and we're just going to send in a list because it's only going to be one value then we're going to return the encoded text and we're going to return the label as well„ÄÇ

AlrightÔºå so now we're sort of to a point where we have defined everything that we need to process our data„ÄÇ and we're just going to perform these mappings on the on the DS train and the DS test data set„ÄÇSo first of allÔºå we're going to do autotune and you you're hopefully familiar with all of these steps right here from from a Tensorflow data set tutorial„ÄÇ So we're going to do Tf data do experimental dot autotune„ÄÇ

We're going to do DSstrain equals DSstrain dot map„ÄÇ and we're going to map it through that in map function„ÄÇAnd we're gonna specify the number of parallel calls„ÄÇ And this is just to for if we're„ÄÇThis is just going to do them in parallel and make for faster data loading„ÄÇAnd then„ÄÇ

We can call dot cache as wellÔºå and this is going to cache some in memory to make it faster for the next one„ÄÇSo we need to do also a shuffle so that as you saw„ÄÇ all of the examples are sort of negative and then positiveÔºå and we want to have randomized„ÄÇ So the data set is 25000 in length for the training„ÄÇ So we're gonna do shuffle 25000„ÄÇ

 Now perhaps 25000 is a bit much„ÄÇ you could have done something like 5000„ÄÇ but let's just„ÄÇDo shuffle 2 about 5000 to make sure it's completely random„ÄÇThen we need to batch this and remember all of the reviews can be different in length„ÄÇ so we're going to need to do padding and we do that by doing padded batch„ÄÇ

Specifying the batch size 32„ÄÇAnd then we specify padded shapes and on newer tenflow versions„ÄÇ you don't need to do this„ÄÇBut for older onesÔºå I think before 2„ÄÇ2 perhaps„ÄÇ then you need to specify this„ÄÇAnd we needÔºå we're gonna specify none here because we don't know the review length„ÄÇ and then we're gonna specify just an empty tuple„ÄÇ And that's going to be for the the label because it's only going to be an integer„ÄÇ

 OkayÔºå so now that we have thatÔºå we can do the same thing for the test set„ÄÇ but we're not gonna need to map it and shuffle it„ÄÇ so we can just do„ÄÇThis right here„ÄÇ mapping it through the encode map function and then padded batch„ÄÇ32„ÄÇ and againÔºå setting the shapes„ÄÇ which shouldn't beenÔºå you shouldn't have to do this„ÄÇ but if you're using older versions„ÄÇ

 you might have to„ÄÇ So I'm just including it here„ÄÇ I'm gonna do this quickly„ÄÇ and I'm gonna create a model„ÄÇ We've actually done this exactly in the previous video„ÄÇAlright„ÄÇ so pretty quicklyÔºå we're just creating a very simple model„ÄÇ and we're actually just doing the embeddingÔºå and then we're doing a global average pulling across those embeddings„ÄÇ

 the outputs from those embeddings„ÄÇ and then we're just mapping that through a dense layer and then output node for one single one„ÄÇ And then we're specifying a binary cross entropy because we just have two classes with the autumn atom optimizer„ÄÇ AlrightÔºå so hopefully there are no errors And this should run„ÄÇAlrightÔºå and it doesn't„ÄÇ So let's see„ÄÇ text„ÄÇAll rightÔºå so we actually got there pretty early in this build vocabulary function„ÄÇ

So let's see what we got from„ÄÇ YeahÔºå so here wrote text do N„ÄÇ we should have the review„ÄÇSo we want to convert that toumpÔºå we want to make a lowercase„ÄÇ and then we want to tokenize the actual review„ÄÇSo hopefully let's see if that runs„ÄÇAlright„ÄÇ so pretty amazingly this actually ran so what we can see is that we got„ÄÇ

 let's see 97% after 15 e box in the training and then on the test set we got 88% but I mean it doesn't really matter what performance we had the important thing is that it actually works so one thing is that when building the vocabulary„ÄÇ perhaps you could change it so that it just takes the I don't know„ÄÇ

 top 1000 words and then you wouldn't have to set this threshold and I think that should be pretty easy to implement as well and as I said in the beginning how this works is going to be different for every for every data set because this structure is going to be different so I want to give an example of also if you would have some kind of different structure so let's see I have I have a test example with three CSV files and it's actually let's see it's actually the exact same„ÄÇ



![](img/fd1d96600c664ffac3a740a72c447896_5.png)

![](img/fd1d96600c664ffac3a740a72c447896_6.png)

And bring itÔºå it's also the IMDB data it's just that we have a few examples on one file„ÄÇ we have a few examples on another fileÔºå and then we have another few examples on a third file So what you can do in that scenario if you have it split up into you know several CSV files„ÄÇ

 which might be very common in practice if you're working with large datasets„ÄÇ![](img/fd1d96600c664ffac3a740a72c447896_8.png)

![](img/fd1d96600c664ffac3a740a72c447896_9.png)

Then let's do justÔºå I import system and see that exit„ÄÇ so it doesn't run the code below„ÄÇ Then what you could do is you could specify the file names„ÄÇ So in this caseÔºå that's„ÄÇTest example1 that CSV test example2 that CSvÔºå and then test example 3 do CSv„ÄÇSo what we could do is we could simply do data set is TF data text line„ÄÇData set of those file names„ÄÇ

 and it's going to work out all of it„ÄÇ So it's gonna going to be like you had them all in one CSV file„ÄÇ which is pretty amazing„ÄÇ So that is one thing you can do„ÄÇ We can justÔºå to make sure that it works„ÄÇ We can just print line„ÄÇüòäÔºåAnd as you can seeÔºå it printed all of themÔºå including the„ÄÇThe the first one right here„ÄÇ and it's actually printing them for all of them„ÄÇ

 So maybe what you would want to do„ÄÇIs remove this right here for those CSV files„ÄÇBut so that is one way that is quite convenient for the if you have multiple files„ÄÇNow one thing is that you might want to do different preprocessing on all of those CSV files„ÄÇ they might not be equally equally in structured like they were here and then what you could do is you could specify just data1 is TF data text line data set and then we could do test example one that CSV and then let's do skip one just for the first row but perhaps you would do something you know in general you would do dot map and then preprocess one if if you would have sort of different processing so let's just copy that„ÄÇ

For all of them„ÄÇ So we're gonna do 2Ôºå3Ôºå2Ôºå3„ÄÇAnd then to combine them when you've now processed them differently„ÄÇ depending on the structure of those individual CSV fileÔºå you would do something like data is„ÄÇData set1 dot concatenate and then data set2 and then concatenate data set 3„ÄÇ and in this way we now obtain the entire data set again„ÄÇ

 so if we do four line in data and then print that lineÔºåWe get the entire data set„ÄÇWhich is including all of those three CSV file„ÄÇ AlrightÔºå so that is if you would have„ÄÇ So this would be example if you have multiple files„ÄÇAnd againÔºå let's do import this„ÄÇThis this start exit„ÄÇ

![](img/fd1d96600c664ffac3a740a72c447896_11.png)

So let's say we had a kind of translation data set„ÄÇSo I'm actually going to create this our„ÄÇ we're just going to do English and then we're going to do Swedish al right so here we have them we have English CSV and then we have Swedish CSV„ÄÇ AlrightÔºå so let's do a language1 and let's just do I love tuna I love potato„ÄÇ

![](img/fd1d96600c664ffac3a740a72c447896_13.png)

I love bacon„ÄÇ Then we can do languageÔºå too„ÄÇIn Swedish„ÄÇ So you wouldnt people both to understand its probably yeahÔºå got what did we write tuna„ÄÇToun fik„ÄÇ yog and scar PotisÔºå yog and scar„ÄÇBaconÔºå so now we have those threeÔºå rightÔºå so we have„ÄÇ![](img/fd1d96600c664ffac3a740a72c447896_15.png)

Just three examples in this caseÔºå but how we would load them„ÄÇIs we canÔºå first of all„ÄÇWe can specify some tokenizerÔºå you knowÔºå as we did previously„ÄÇWhich would be TFDS do features do text tokenizer„ÄÇThen we would do English is TF do data doc text line data set„ÄÇAnd we would have that English do CSv„ÄÇ and then for the Swedish„ÄÇWem would have TF data„ÄÇ

 text line data setÔºå Swedish dot TSV„ÄÇAnd then what you could do is you can do DÔºå D S„ÄÇOur data set is T of that dataÔºå that data set that zip„ÄÇAnd then you can do English and then Swedish„ÄÇSo now we could do four English and Swedish in data set„ÄÇ and then we can just skip the first row again„ÄÇThen we could do something like print„ÄÇ

 tokenizer do tokenize„ÄÇEnglish dot Nai„ÄÇAnd then we could do tokenizer do tokenize Swedish dot numpy„ÄÇAnd then we need to also do decode to UTF 8 because we use special characters„ÄÇ So special characters would be something like„ÄÇThis right here„ÄÇSo that's just we're why we're doing Decode UTF 8„ÄÇBut let's run that and just see how it looks like„ÄÇ

And then we can see that it's splitting those examples that we had„ÄÇ which this is one pairÔºå right„ÄÇ This is the translation of this first one„ÄÇAnd what you would do need to do then is you would need to do sort of„ÄÇ first of allÔºå you need a vocabulary„ÄÇAnd then you would need for each language„ÄÇThen you would need to do tokenize and miracleize words„ÄÇ

And so this is hopefully what I've shown you how to do so you would be relatively confident that you can be able to use those two„ÄÇ and then we need to do padded batch to get batches„ÄÇU and so on„ÄÇ and then we also need toÔºå you know„ÄÇ create a model so create model„ÄÇAnd„ÄÇThe model in this case is probably something like a sequence sequence model that might be a a bit too advanced for you to implement„ÄÇ or it could even be a transformer network„ÄÇAnd I haven't covered any of those because I think they're a little bit too advanced„ÄÇ

And is beyond this video„ÄÇ So what we wanted to focus on in this video which hopefully has been as clear as I can make it is how you would load custom data sets for text„ÄÇ using this textline dataset and hopefully I've also shown you some different ways depending on how your dataset is structured So I've said this before no data set is exactly the same„ÄÇ

 All the data is going to be differently structured„ÄÇ So how you load it is going to depend on your dataset„ÄÇ but hopefully you've learned some principles of sort of the general pattern of how you should load the data that you can apply to your own project„ÄÇ So that's it for this videoÔºå hopefully you found the video useful Thank you so much for watching and I hope to see you in the next video„ÄÇ



![](img/fd1d96600c664ffac3a740a72c447896_17.png)