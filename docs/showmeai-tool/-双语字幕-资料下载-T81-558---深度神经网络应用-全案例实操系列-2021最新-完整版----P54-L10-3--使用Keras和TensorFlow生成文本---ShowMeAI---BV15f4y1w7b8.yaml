- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P54ï¼šL10.3- ä½¿ç”¨Keraså’ŒTensorFlowç”Ÿæˆæ–‡æœ¬
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P54ï¼šL10.3- ä½¿ç”¨Keraså’ŒTensorFlowç”Ÿæˆæ–‡æœ¬
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ Welcome to applications at Deep neural Networks with
    Washington University In this moduleã€‚ we're going to take a look at text generation
    using LSTMsã€‚ We're going to see how we can create whimsicalã€‚ğŸ˜Šï¼ŒRandomized generations
    created from text documentsã€‚ so you could give itï¼Œ for exampleï¼Œ William Shakespeare's
    worksã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨æ¨¡å—ã€‚åœ¨è¿™ä¸€æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨LSTMè¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä»æ–‡æœ¬æ–‡ä»¶åˆ›å»ºå¥‡æ€å¦™æƒ³çš„ğŸ˜Šéšæœºç”Ÿæˆã€‚
- en: and it would generate Shakespeare Ask text for youã€‚More interestinglyï¼Œ tooã€‚
    you could give it source code and it would generateã€‚Reasonably validï¼Œ though not
    compilableã€‚ C source code based on the C code that it was trained onã€‚Nowã€‚ this
    somewhat whimsical use for LSTM has gotten definitely some publicity in the last
    couple of yearsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šä¸ºä½ ç”Ÿæˆèå£«æ¯”äºšé£æ ¼çš„æ–‡æœ¬ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œä½ å¯ä»¥ç»™å®ƒæºä»£ç ï¼Œå®ƒä¼šç”Ÿæˆåˆç†æœ‰æ•ˆçš„ï¼Œè™½ç„¶ä¸èƒ½ç¼–è¯‘çš„Cæºä»£ç ï¼Œè¿™äº›æ˜¯åŸºäºå®ƒæ‰€è®­ç»ƒçš„Cä»£ç ç”Ÿæˆçš„ã€‚ç°åœ¨ï¼Œè¿™ç§LSTMçš„å¥‡æ€å¦™æƒ³ç”¨æ³•åœ¨è¿‡å»å‡ å¹´ä¸­ç¡®å®å¾—åˆ°äº†ä¸å°‘å…³æ³¨ã€‚
- en: it does lead into the next part where we get into using the same sort of technology
    to generate captions for pictures for the latest on my AI course and projects
    click subscribe and the bell next to it to be notified of every new videoã€‚ we're
    going to make use of this technology both in this part and also in the next one
    where we do captioning where're basically a neural network or actually several
    neural networks can look at an image and give you a caption for it like you might
    see a dog running across the grass in the picture in the neural network we'll
    literally write a dog running across the grass So in this part what we're going
    to do we're going take a baby steps so to speak and we're going to look at how
    a LSTM can be trained on any sort of text and it will output random nonsensical
    textã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼•å‡ºäº†ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŒæ ·çš„æŠ€æœ¯ä¸ºå›¾ç‰‡ç”Ÿæˆå­—å¹•ã€‚è¦è·å–æˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®ï¼Œç‚¹å‡»è®¢é˜…å¹¶ç‚¹å‡»æ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†å’Œä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬éƒ½å°†åˆ©ç”¨è¿™é¡¹æŠ€æœ¯ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬ä¼šåšå­—å¹•ç”Ÿæˆï¼ŒåŸºæœ¬ä¸Šæ˜¯ç¥ç»ç½‘ç»œï¼Œæˆ–è€…å®é™…ä¸Šæ˜¯å‡ ä¸ªç¥ç»ç½‘ç»œå¯ä»¥æŸ¥çœ‹å›¾åƒå¹¶ä¸ºå…¶ç”Ÿæˆå­—å¹•ï¼Œæ¯”å¦‚ä½ å¯èƒ½ä¼šçœ‹åˆ°ä¸€åªç‹—åœ¨è‰åœ°ä¸Šå¥”è·‘ï¼Œç¥ç»ç½‘ç»œä¼šå­—é¢ä¸Šå†™å‡ºâ€œç‹—åœ¨è‰åœ°ä¸Šå¥”è·‘â€ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡å–å©´å„¿æ­¥ä¼ï¼Œçœ‹çœ‹å¦‚ä½•è®­ç»ƒLSTMåœ¨ä»»ä½•ç±»å‹çš„æ–‡æœ¬ä¸Šï¼Œå¹¶è¾“å‡ºéšæœºçš„æ— æ„ä¹‰æ–‡æœ¬ã€‚
- en: '![](img/12a40ad6cb8e26a4cb77c05d5b96a105_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12a40ad6cb8e26a4cb77c05d5b96a105_1.png)'
- en: Very much in that formatã€‚ Now this uses all the same technology and builds us
    up to doing something more useful where we're going to actually generate the captionsã€‚
    but this is sort of the first step for itã€‚ This was really popular a couple of
    years backã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸å¤šä»¥è¿™ç§æ ¼å¼ã€‚ç°åœ¨è¿™ä½¿ç”¨äº†æ‰€æœ‰ç›¸åŒçš„æŠ€æœ¯ï¼Œå¹¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåšä¸€äº›æ›´æœ‰ç”¨çš„äº‹æƒ…ï¼Œå³å®é™…ç”Ÿæˆå­—å¹•ï¼Œä½†è¿™ç®—æ˜¯ç¬¬ä¸€æ­¥ã€‚è¿™åœ¨å‡ å¹´å‰éå¸¸å—æ¬¢è¿ã€‚
- en: there is a classic sort of blog post I guess you would call it called the unreasonable
    effectiveness of recurrent neural networks I am going to show you this one really
    quickly because some of the examples hereã€‚ just the different types of text that
    can be generatedã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ç¯‡ç»å…¸çš„åšå®¢æ–‡ç« ï¼Œæˆ‘æƒ³ä½ ä¼šç§°ä¹‹ä¸ºâ€œé€’å½’ç¥ç»ç½‘ç»œçš„éç†æ€§æœ‰æ•ˆæ€§â€ã€‚æˆ‘å°†å¾ˆå¿«ç»™ä½ å±•ç¤ºè¿™ä¸€ç‚¹ï¼Œå› ä¸ºè¿™é‡Œçš„ä¸€äº›ä¾‹å­å±•ç¤ºäº†ä¸åŒç±»å‹çš„æ–‡æœ¬å¯ä»¥è¢«ç”Ÿæˆã€‚
- en: This is Andre Carpathy's blog when he was a student at Stanford University working
    on his PhD incidentcidentally the captioning technology that we'll be looking
    at in the next part was also very much pioneered by him So one of the first things
    that he tried to generate was there's essays by Paul Graham you can read right
    here basically he created the LSTM and it was able to just randomly generate this
    sortã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å®‰å¾·çƒˆÂ·å¡å¸•è¥¿çš„åšå®¢ï¼Œå½“ä»–è¿˜æ˜¯æ–¯å¦ç¦å¤§å­¦çš„å­¦ç”Ÿæ—¶æ­£åœ¨æ”»è¯»åšå£«å­¦ä½ï¼Œå¶ç„¶é—´ï¼Œä»–æ‰€ç ”å‘çš„å­—å¹•æŠ€æœ¯ä¹Ÿåœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­æœ‰ç€é‡è¦çš„åœ°ä½ã€‚æ‰€ä»¥ä»–å°è¯•ç”Ÿæˆçš„ç¬¬ä¸€ä»¶äº‹æƒ…å°±æ˜¯ä¿ç½—Â·æ ¼é›·å„å§†çš„æ–‡ç« ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»ï¼ŒåŸºæœ¬ä¸Šä»–åˆ›é€ äº†LSTMï¼Œå®ƒèƒ½å¤Ÿéšæœºç”Ÿæˆè¿™æ ·çš„æ–‡æœ¬ã€‚
- en: Of textã€‚ now it's nonsensicalã€‚ this the surprised in investors weren't going
    to raise moneyã€‚ I'm not the company with the time they're allã€‚If you're not paying
    attentionã€‚ it looks like legitimate text and it even puts in captionsã€‚ and if
    you think about what's really going on hereã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨çš„æ–‡æœ¬æ˜¯æ— æ„ä¹‰çš„ï¼Œè¿™è®©æŠ•èµ„è€…æ„Ÿåˆ°æƒŠè®¶ï¼Œä»–ä»¬ä¸ä¼šç­¹é›†èµ„é‡‘ã€‚æˆ‘å¹¶ä¸æ˜¯åœ¨ä¸å…¬å¸çš„æ—¶é—´ç›¸å¤„ï¼Œä»–ä»¬éƒ½æ˜¯ã€‚å¦‚æœä½ æ²¡æœ‰æ³¨æ„ï¼Œè¿™çœ‹èµ·æ¥åƒæ˜¯åˆæ³•çš„æ–‡æœ¬ï¼Œå®ƒç”šè‡³æ’å…¥äº†å­—å¹•ã€‚å¦‚æœä½ æƒ³æƒ³è¿™é‡Œåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆã€‚
- en: what's really fascinating is this neural network had to learn this from scratchã€‚
    it doesn't know Englishã€‚ it learned where to put apostropphesï¼Œ it learned how
    to use commasã€‚ it learned how to use periods at the end of sentencesã€‚ It learned
    how to put capital letters at the beginnings of sentencesã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: çœŸæ­£ä»¤äººç€è¿·çš„æ˜¯ï¼Œè¿™ä¸ªç¥ç»ç½‘ç»œå¿…é¡»ä»é›¶å¼€å§‹å­¦ä¹ ã€‚å®ƒå¹¶ä¸çŸ¥é“è‹±è¯­ã€‚å®ƒå­¦ä¼šäº†å¦‚ä½•æ”¾ç½®æ’‡å·ï¼Œå­¦ä¼šäº†å¦‚ä½•ä½¿ç”¨é€—å·ï¼Œå­¦ä¼šäº†åœ¨å¥å­æœ«å°¾ä½¿ç”¨å¥å·ã€‚å®ƒå­¦ä¼šäº†åœ¨å¥å­å¼€å¤´ä½¿ç”¨å¤§å†™å­—æ¯ã€‚
- en: So it picked up a lot of grammar on its ownã€‚ and it learns this grammar by reading
    textã€‚ not by having grammatical rulesï¼Œ hardcoded into the programã€‚ hardcoding
    of grammatical rules and parse trees were very common in natural language processing
    probably a decade agoã€‚ but with the advent of neural networks and deep neural
    networks in recurrent neural networks like we're dealing with hereã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå®ƒè‡ªå­¦äº†å¾ˆå¤šè¯­æ³•ã€‚å®ƒé€šè¿‡é˜…è¯»æ–‡æœ¬å­¦ä¹ è¿™äº›è¯­æ³•ï¼Œè€Œä¸æ˜¯é€šè¿‡å°†è¯­æ³•è§„åˆ™ç¡¬ç¼–ç åˆ°ç¨‹åºä¸­ã€‚åå¹´å‰ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¯­æ³•è§„åˆ™å’Œè§£ææ ‘çš„ç¡¬ç¼–ç éå¸¸å¸¸è§ã€‚ä½†éšç€ç¥ç»ç½‘ç»œå’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼Œä»¥åŠæˆ‘ä»¬æ­£åœ¨å¤„ç†çš„å¾ªç¯ç¥ç»ç½‘ç»œçš„å‡ºç°ã€‚
- en: there's much better ways to do thisã€‚ We'll also see you also had the neural
    networks generate Shakespeareã€‚ Now what's very interestingã€‚This is if you've read
    Shakespeare beforeï¼Œ it's it's playsï¼Œ it's dramaã€‚ So it's actors speaking different
    partsã€‚ So this is very much what Shakespeare would look likeã€‚ You have a nameã€‚
    you have what that person is going to say the next nameã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ›´å¥½çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°ç¥ç»ç½‘ç»œç”Ÿæˆèå£«æ¯”äºšçš„ä½œå“ã€‚ç°åœ¨éå¸¸æœ‰è¶£çš„æ˜¯ï¼Œå¦‚æœä½ ä»¥å‰è¯»è¿‡èå£«æ¯”äºšï¼Œè¿™æ˜¯æˆå‰§ï¼Œè¿™æ˜¯æ¼”å‘˜ä»¬è¯´ä¸åŒçš„è§’è‰²ã€‚è¿™éå¸¸åƒèå£«æ¯”äºšçš„æ ·å­ã€‚ä½ æœ‰ä¸€ä¸ªåå­—ï¼Œä½ æœ‰é‚£ä¸ªäººæ¥ä¸‹æ¥è¦è¯´çš„è¯çš„ä¸‹ä¸€ä¸ªåå­—ã€‚
- en: what they're going to say and so onã€‚ And the recurrent neural network was able
    to figure out the general format of a Shakespearean play and wrote this outã€‚ This
    is not real Shakespeareï¼Œ but it's meant to look kind of like Shakespeareã€‚ aasã€‚
    I think I shall be become now there it didn't figure out something on English
    that would be all one word become approached in the day when littleã€‚ I meanï¼Œ it's
    it's nonsenseï¼Œ but it's string together these words correctlyã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šé¢„æµ‹æ¥ä¸‹æ¥ä¼šè¯´ä»€ä¹ˆã€‚å¾ªç¯ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¼„æ¸…æ¥šèå£«æ¯”äºšæˆå‰§çš„æ€»ä½“æ ¼å¼ï¼Œå¹¶å°†å…¶å†™å‡ºæ¥ã€‚è¿™å¹¶ä¸æ˜¯çœŸæ­£çš„èå£«æ¯”äºšï¼Œä½†å®ƒçš„æ ·å­æœ‰ç‚¹åƒèå£«æ¯”äºšã€‚æˆ‘æƒ³æˆ‘ç°åœ¨åº”è¯¥å˜æˆè¿™é‡Œé¢ï¼Œå®ƒæ²¡æœ‰å¼„æ¸…æ¥šè‹±è¯­çš„æŸäº›ä¸œè¥¿ï¼Œæ‰€æœ‰çš„ä¸€è¯å˜æˆäº†â€œæˆä¸ºâ€ï¼Œæ¥è¿‘ä¸€å¤©æ—¶çš„â€œå°â€ã€‚æˆ‘æ˜¯è¯´ï¼Œè¿™å°±æ˜¯æ— æ„ä¹‰ï¼Œä½†å®ƒæ­£ç¡®åœ°å°†è¿™äº›å•è¯ä¸²åœ¨ä¸€èµ·ã€‚
- en: It's putting it's using the articles correctlyã€‚ It often figures out verb agreement
    and other aspects of thisã€‚ It usually punctuates the sentences correctlyã€‚ So this
    was generating Shakespeareã€‚ The Wikipedia generation wasã€‚imIf you've ever looked
    at view source on a Wikipedia pageã€‚ it shows you the media Wiki markupã€‚ So if
    you ever wanted to write a page on Wikipediaã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ­£ç¡®ä½¿ç”¨å† è¯ã€‚å®ƒé€šå¸¸èƒ½å¤Ÿå¼„æ¸…æ¥šåŠ¨è¯çš„ä¸€è‡´æ€§å’Œå…¶ä»–æ–¹é¢ã€‚å®ƒé€šå¸¸èƒ½æ­£ç¡®æ ‡ç‚¹å¥å­ã€‚æ‰€ä»¥è¿™æ˜¯ç”Ÿæˆèå£«æ¯”äºšçš„å†…å®¹ã€‚ç»´åŸºç™¾ç§‘ç”Ÿæˆçš„å†…å®¹æ˜¯ã€‚å¦‚æœä½ æ›¾ç»æŸ¥çœ‹è¿‡ç»´åŸºç™¾ç§‘é¡µé¢çš„æºä»£ç ï¼Œå®ƒæ˜¾ç¤ºä½ åª’ä½“ç»´åŸºæ ‡è®°ã€‚æ‰€ä»¥å¦‚æœä½ æ›¾æƒ³åœ¨ç»´åŸºç™¾ç§‘ä¸Šå†™ä¸€ç¯‡æ–‡ç« ã€‚
- en: you'd write in this formã€‚ you put the double bracket that just means linked
    so this is linked to the John Claire article off in Wikipedia somewhere elseã€‚
    So it's emulating the Wikipedia formï¼Œ which is which is very cool and it also
    starts to figure out what URLs look like it has no idea what a URL was before
    reading this and this is not even a real page on Yahoo if you tried to go to it
    even back when this was generatedã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šç”¨è¿™ç§å½¢å¼ä¹¦å†™ã€‚ä½ æ”¾å…¥åŒæ‹¬å·ï¼Œè¿™æ„å‘³ç€é“¾æ¥ï¼Œå› æ­¤è¿™é“¾æ¥åˆ°ç»´åŸºç™¾ç§‘æŸå¤„çš„çº¦ç¿°Â·å…‹è±å°”æ–‡ç« ã€‚æ‰€ä»¥å®ƒæ¨¡æ‹Ÿäº†ç»´åŸºç™¾ç§‘çš„å½¢å¼ï¼Œè¿™éå¸¸é…·ï¼Œå¹¶ä¸”å®ƒä¹Ÿå¼€å§‹å¼„æ¸…æ¥šURLçš„æ ·å­ã€‚åœ¨é˜…è¯»è¿™äº›å†…å®¹ä¹‹å‰ï¼Œå®ƒå¯¹URLä¸€æ— æ‰€çŸ¥ï¼Œè€Œè¿™ç”šè‡³ä¸æ˜¯Yahooä¸Šçš„çœŸå®é¡µé¢ï¼Œå¦‚æœä½ å°è¯•è®¿é—®ï¼Œå³ä½¿åœ¨ç”Ÿæˆæ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: it would give you an errorã€‚ it's essentially a hallucinization of of the Record
    neural networkã€‚ I believe is what Andre called it in hereã€‚ It figures out other
    interesting parts of Wikipedia notation I found this one very interestingã€‚ If
    youve worked with academic papersï¼Œ particularly for publication and academic journalã€‚And
    conferencesï¼Œ I'm sure you've dealt with latex latex is very good for mathematically
    rich textã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šç»™ä½ ä¸€ä¸ªé”™è¯¯ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯è®°å½•ç¥ç»ç½‘ç»œçš„å¹»è§‰ã€‚æˆ‘ç›¸ä¿¡è¿™æ˜¯å®‰å¾·çƒˆåœ¨è¿™é‡Œç§°ä¹‹ä¸ºçš„ã€‚å®ƒå¼„æ¸…æ¥šäº†ç»´åŸºç™¾ç§‘ç¬¦å·çš„å…¶ä»–æœ‰è¶£éƒ¨åˆ†ï¼Œæˆ‘è§‰å¾—è¿™ä¸ªéå¸¸æœ‰è¶£ã€‚å¦‚æœä½ æ›¾ç»å¤„ç†è¿‡å­¦æœ¯è®ºæ–‡ï¼Œç‰¹åˆ«æ˜¯ä¸ºäº†å‘è¡¨å’Œå­¦æœ¯æœŸåˆŠï¼Œä»¥åŠä¼šè®®ï¼Œæˆ‘ç›¸ä¿¡ä½ å·²ç»å¤„ç†è¿‡LaTeXï¼ŒLaTeXå¯¹äºå¯Œå«æ•°å­¦çš„æ–‡æœ¬éå¸¸æœ‰æ•ˆã€‚
- en: And here the recurrent neural network was trained on a bunch of latex and was
    then asked to generate its own latexã€‚ And as if you read the blog post for more
    detail on itã€‚ it was mostly compileilableã€‚ There were some errors that the authors
    had to fixã€‚ but it was able to generate very mathematically looking latexï¼Œ even
    diagramsã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„é€’å½’ç¥ç»ç½‘ç»œåœ¨ä¸€å † LaTeX æ–‡æ¡£ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œç„¶åè¢«è¦æ±‚ç”Ÿæˆè‡ªå·±çš„ LaTeXã€‚å°±åƒå¦‚æœä½ é˜…è¯»åšå®¢æ–‡ç« å¯ä»¥è·å¾—æ›´å¤šç»†èŠ‚ä¸€æ ·ï¼Œå®ƒå¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯å¯ä»¥ç¼–è¯‘çš„ã€‚è™½ç„¶æœ‰ä¸€äº›é”™è¯¯éœ€è¦ä½œè€…ä¿®å¤ï¼Œä½†å®ƒèƒ½å¤Ÿç”Ÿæˆéå¸¸åƒæ•°å­¦çš„
    LaTeXï¼Œç”šè‡³è¿˜æœ‰å›¾è¡¨ã€‚
- en: So this is all just hallucinating from the text that it's readã€‚ So it's almost
    like a child stringing together words and starting to figure out Englishã€‚ This
    is this is essentially what's what's going onã€‚ And the next partã€‚ we'll see how
    we use the same technology to really conjure up actual real Englishã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸€åˆ‡éƒ½æ˜¯ä»å®ƒæ‰€é˜…è¯»çš„æ–‡æœ¬ä¸­äº§ç”Ÿçš„å¹»è§‰ã€‚è¿™å‡ ä¹å°±åƒä¸€ä¸ªå­©å­æ‹¼å‡‘å•è¯å¹¶å¼€å§‹ç†è§£è‹±è¯­ã€‚è¿™å°±æ˜¯æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ç›¸åŒçš„æŠ€æœ¯çœŸæ­£ç”ŸæˆçœŸå®çš„è‹±è¯­ã€‚
- en: And this is what late source code looks like if you've never seen it beforeã€‚
    This essentially generates much of this up hereã€‚ This is probably a shortened
    form of itã€‚ I found this one absolutely fascinatingã€‚ They tookã€‚Source code written
    in C and trained the recurrent neural network on lots and lots and lots of C source
    codeã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»¥å‰æ²¡æœ‰è§è¿‡ï¼Œè¿™å°±æ˜¯ LaTeX æºä»£ç çš„æ ·å­ã€‚è¿™åŸºæœ¬ä¸Šç”Ÿæˆäº†ä¸Šé¢çš„å¤§éƒ¨åˆ†å†…å®¹ã€‚è¿™å¯èƒ½æ˜¯å®ƒçš„ç®€åŒ–å½¢å¼ã€‚æˆ‘è§‰å¾—è¿™ä¸ªéå¸¸è¿·äººã€‚ä»–ä»¬ç”¨ C è¯­è¨€ç¼–å†™äº†æºä»£ç ï¼Œå¹¶åœ¨å¤§é‡
    C æºä»£ç ä¸Šè®­ç»ƒäº†é€’å½’ç¥ç»ç½‘ç»œã€‚
- en: And then they told it to just dream up C source code of its ownã€‚ This looks
    if you've not worked with C beforeï¼Œ I mean it's very similar Java ja in its overall
    syntax and it's amazing how much the recurrent neural network picked up on look
    it's all properly tabbedã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä»–ä»¬å‘Šè¯‰å®ƒè‡ªå·±æ¢¦è§ C æºä»£ç ã€‚å¦‚æœä½ ä»¥å‰æ²¡æœ‰æ¥è§¦è¿‡ Cï¼Œå®é™…ä¸Šå®ƒçš„æ•´ä½“è¯­æ³•ä¸ Java éå¸¸ç›¸ä¼¼ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯é€’å½’ç¥ç»ç½‘ç»œå¸æ”¶äº†å¤šå°‘ï¼Œçœ‹ï¼Œå®ƒçš„ç¼©è¿›éƒ½æ˜¯æ­£ç¡®çš„ã€‚
- en: I mean that is that is pretty cool it's declaring variables it's doing checks
    now it does tend to use local variables that it's never defined So that is an
    issue there but it's learning how to do the pointer syntax correctlyã€‚ it's putting
    semicolons in all the right places it's using brackets when it needs so that's
    multiple lines that needs a bracket that's a single line it does notã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯è¯´ï¼Œè¿™çœŸçš„å¾ˆé…·ï¼Œå®ƒåœ¨å£°æ˜å˜é‡ï¼Œå®ƒåœ¨è¿›è¡Œæ£€æŸ¥ã€‚ç°åœ¨å®ƒç¡®å®å€¾å‘äºä½¿ç”¨ä¸€äº›ä»æœªå®šä¹‰çš„å±€éƒ¨å˜é‡ã€‚æ‰€ä»¥è¿™æ˜¯ä¸ªé—®é¢˜ï¼Œä½†å®ƒæ­£åœ¨å­¦ä¹ å¦‚ä½•æ­£ç¡®ä½¿ç”¨æŒ‡é’ˆè¯­æ³•ã€‚å®ƒåœ¨æ‰€æœ‰æ­£ç¡®çš„ä½ç½®æ”¾ç½®äº†åˆ†å·ï¼Œå½“éœ€è¦æ—¶ä½¿ç”¨æ‹¬å·ï¼Œå¯¹äºéœ€è¦å¤šè¡Œçš„å†…å®¹ä½¿ç”¨æ‹¬å·ï¼Œå¯¹äºå•è¡Œåˆ™ä¸ä½¿ç”¨ã€‚
- en: So it's almost spooky how much it's really figuring out about source code it
    puts in comments that have random text in it it's sort of doing what what the
    predecessor won just higher up on the screen that was generating Shakespeare It's
    learning to generate randomã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‡ ä¹æ˜¯ä»¤äººæ¯›éª¨æ‚šç„¶çš„ï¼Œå®ƒå¯¹æºä»£ç çš„ç†è§£è¾¾åˆ°äº†ä»€ä¹ˆç¨‹åº¦ï¼Œå®ƒåœ¨æ³¨é‡Šä¸­åŠ å…¥äº†éšæœºæ–‡æœ¬ï¼Œè¿™ç§æ–¹å¼ç±»ä¼¼äºå…ˆå‰ç”Ÿæˆèå£«æ¯”äºšçš„ç¨‹åºã€‚å®ƒæ­£åœ¨å­¦ä¹ ç”Ÿæˆéšæœºå†…å®¹ã€‚
- en: English text for commentsã€‚ So now let's see how we can generate text just like
    thisã€‚ These are just my importsã€‚ I'm gonna go ahead and run thatã€‚ Nothing too
    unusual going on thereã€‚ I am going to read in a text documentã€‚ So an entire bookã€‚
    We're gonna use just one book to train it onã€‚ So it won't be perfect that it'll
    come up with some interesting stuffã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è‹±è¯­æ–‡æœ¬ç”¨äºè¯„è®ºã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ç”Ÿæˆè¿™æ ·çš„æ–‡æœ¬ã€‚è¿™äº›åªæ˜¯æˆ‘çš„å¯¼å…¥ã€‚æˆ‘å°†ç»§ç»­æ‰§è¡Œè¿™ä¸ªã€‚æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„äº‹æƒ…å‘ç”Ÿã€‚æˆ‘å°†è¯»å–ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚ä¹Ÿå°±æ˜¯ä¸€æœ¬å®Œæ•´çš„ä¹¦ã€‚æˆ‘ä»¬å°†åªç”¨ä¸€æœ¬ä¹¦æ¥è®­ç»ƒå®ƒã€‚æ‰€ä»¥å®ƒä¸ä¼šå®Œç¾ï¼Œä½†ä¼šäº§ç”Ÿä¸€äº›æœ‰è¶£çš„ä¸œè¥¿ã€‚
- en: We're going to use the book Treasure Islandã€‚ So Treasure Island was written
    some time agoã€‚ and what's neat about this bookã€‚ It's a children's bookã€‚ but it
    pretty much set the stage from pirate stories thereafterã€‚ I meanã€‚ one leggged
    pirates Pirates with parrots on their shouldersã€‚ Xmarks the spotã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ã€Šé‡‘é“¶å²›ã€‹è¿™æœ¬ä¹¦ã€‚æ‰€ä»¥ã€Šé‡‘é“¶å²›ã€‹æ˜¯åœ¨å¾ˆä¹…ä»¥å‰å†™çš„ã€‚è¿™æœ¬ä¹¦çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºï¼Œå®ƒæ˜¯ä¸€æœ¬å„¿ç«¥ä¹¦ï¼Œä½†å‡ ä¹ä¸ºæ­¤åçš„æµ·ç›—æ•…äº‹å¥ å®šäº†åŸºç¡€ã€‚æˆ‘çš„æ„æ€æ˜¯ï¼Œç‹¬è…¿æµ·ç›—ã€è‚©è†€ä¸Šæœ‰é¹¦é¹‰çš„æµ·ç›—ï¼Œ"X"
    æ ‡è®°ç€åœ°ç‚¹ã€‚
- en: All this came from Treasure Islandã€‚ I read it as a childã€‚ It is a very interesting
    bookã€‚ and I was not a big fiction readerï¼Œ highly recommend if you're interested
    in pirateã€‚ but let's see if we can generate Pirate stories using neural networksã€‚
    I'm going to read in the text of itã€‚ and you can see it's coming hereã€‚ It's from
    Project Gutenbergã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›éƒ½æ¥è‡ªã€Šé‡‘é“¶å²›ã€‹ã€‚æˆ‘å°æ—¶å€™è¯»è¿‡è¿™æœ¬ä¹¦ã€‚è¿™æ˜¯ä¸€æœ¬éå¸¸æœ‰è¶£çš„ä¹¦ï¼Œè€Œæˆ‘å¹¶ä¸æ˜¯ä¸€ä¸ªå–œæ¬¢è¯»å°è¯´çš„äººï¼Œå¼ºçƒˆæ¨èç»™å¯¹æµ·ç›—æ„Ÿå…´è¶£çš„äººã€‚ä½†æˆ‘ä»¬æ¥çœ‹çœ‹èƒ½å¦ç”¨ç¥ç»ç½‘ç»œç”Ÿæˆæµ·ç›—æ•…äº‹ã€‚æˆ‘å°†è¯»å–å®ƒçš„æ–‡æœ¬ã€‚ä½ å¯ä»¥çœ‹åˆ°å®ƒæ¥è‡ªå¤è…¾å ¡è®¡åˆ’ã€‚
- en: which is a lot of Englishã€‚ maybe other languages I haven't kept up with Gutenbergã€‚
    They have the bookã€‚Text onlineã€‚ So it's it's great for natural language processing
    projectsã€‚ I'm going to take this raw text that you saw loaded in up here and do
    some conversions on itã€‚ I'm going to remove anything that is not ASciIã€‚ So0 to
    127ã€‚ By the wayã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­åŒ…å«å¤§é‡çš„è‹±è¯­ã€‚ä¹Ÿè®¸è¿˜æœ‰å…¶ä»–è¯­è¨€ï¼Œæˆ‘æ²¡æœ‰è·Ÿä¸Šå¤è…¾å ¡é¡¹ç›®ã€‚ä»–ä»¬æœ‰è¿™æœ¬ä¹¦ã€‚åœ¨çº¿æ–‡æœ¬ã€‚æ‰€ä»¥è¿™å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†é¡¹ç›®éå¸¸æ£’ã€‚æˆ‘å°†ä½¿ç”¨æ‚¨åœ¨ä¸Šé¢çœ‹åˆ°çš„åŸå§‹æ–‡æœ¬ï¼Œå¹¶å¯¹å…¶è¿›è¡Œä¸€äº›è½¬æ¢ã€‚æˆ‘ä¼šåˆ é™¤ä»»ä½•éASCIIå­—ç¬¦ã€‚èŒƒå›´æ˜¯0åˆ°127ã€‚é¡ºä¾¿æä¸€ä¸‹ã€‚
- en: everything I'm doing on this is very much English basedã€‚ but I have seen LSTms
    applied to other languages such as Chinese and Japanese and Asian languages as
    wellã€‚ So if you're interested in doing this sort of text generationã€‚ you can generate
    this on really nearly anythingã€‚ You saw in a previous partã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ‰€åšçš„ä¸€åˆ‡éƒ½éå¸¸åŸºäºè‹±è¯­ã€‚ä½†æˆ‘è§è¿‡LSTMè¢«åº”ç”¨äºå…¶ä»–è¯­è¨€ï¼Œæ¯”å¦‚ä¸­æ–‡ã€æ—¥æ–‡å’Œå…¶ä»–äºšæ´²è¯­è¨€ã€‚æ‰€ä»¥å¦‚æœæ‚¨å¯¹è¿™ç§æ–‡æœ¬ç”Ÿæˆæ„Ÿå…´è¶£ï¼Œå‡ ä¹å¯ä»¥åœ¨ä»»ä½•å†…å®¹ä¸Šç”Ÿæˆã€‚æ‚¨åœ¨å‰é¢çš„éƒ¨åˆ†ä¸­çœ‹åˆ°ã€‚
- en: I showed just an exampleï¼Œ not the actual codeï¼Œ but link to somebody's example
    that was generating Chinese fonts using Gsã€‚ So I'm going to pull that inï¼Œ get
    the process textï¼Œ run this partï¼Œ it tells me the corpus lengthã€‚ So this is the
    length of that document of Tresure Islandã€‚ There's a total of 60 different charactersï¼Œ
    especially since I stripped a lot of the non ASi onesã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯å±•ç¤ºäº†ä¸€ä¸ªä¾‹å­ï¼Œè€Œä¸æ˜¯å®é™…çš„ä»£ç ï¼Œè€Œæ˜¯é“¾æ¥åˆ°æŸäººçš„ä¾‹å­ï¼Œè¯¥ä¾‹å­ä½¿ç”¨Gsç”Ÿæˆä¸­æ–‡å­—ä½“ã€‚æ‰€ä»¥æˆ‘ä¼šå¼•å…¥è¿™ä¸ªï¼Œè·å–å¤„ç†åçš„æ–‡æœ¬ï¼Œè¿è¡Œè¿™ä¸€éƒ¨åˆ†ï¼Œå®ƒå‘Šè¯‰æˆ‘è¯­æ–™åº“çš„é•¿åº¦ã€‚è¿™æ˜¯ã€Šå­¤å²›æƒŠé­‚ã€‹æ–‡æ¡£çš„é•¿åº¦ã€‚æ€»å…±æœ‰60ä¸ªä¸åŒçš„å­—ç¬¦ï¼Œå°¤å…¶æ˜¯å› ä¸ºæˆ‘å‰”é™¤äº†å¾ˆå¤šéASCIIå­—ç¬¦ã€‚
- en: and by the wayï¼Œ this is all very muchã€‚ğŸ˜Šï¼Œode here is based on one of the Kira's
    examplesã€‚ made a few changes to itï¼Œ but this is I have the link to the to the
    original example up higher if you wantã€‚ you can basically run this code on just
    about any text I ran it on Treasure Islandã€‚ The original code was actually looking
    at Nietzsches a philosopher and generate additional textã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè¿™ä¸€åˆ‡éƒ½éå¸¸ä¸é”™ã€‚ğŸ˜Š è¿™æ®µä»£ç åŸºäºKiraçš„ä¸€ä¸ªç¤ºä¾‹ã€‚æˆ‘å¯¹å…¶è¿›è¡Œäº†å°‘è®¸æ›´æ”¹ï¼Œä½†å¦‚æœæ‚¨éœ€è¦ï¼Œæˆ‘åœ¨ä¸Šé¢æœ‰åŸå§‹ç¤ºä¾‹çš„é“¾æ¥ã€‚åŸºæœ¬ä¸Šï¼Œæ‚¨å¯ä»¥åœ¨å‡ ä¹ä»»ä½•æ–‡æœ¬ä¸Šè¿è¡Œè¿™æ®µä»£ç ï¼Œæˆ‘åœ¨ã€Šå­¤å²›æƒŠé­‚ã€‹ä¸Šè¿è¡Œäº†å®ƒã€‚åŸå§‹ä»£ç å®é™…ä¸Šæ˜¯æŸ¥çœ‹å“²å­¦å®¶å°¼é‡‡å¹¶ç”Ÿæˆé¢å¤–çš„æ–‡æœ¬ã€‚
- en: it looks like it's fromnietzscheã€‚ So when you run this part hereã€‚ it is using
    the max length of 40 that is your sequence sizeã€‚ We're going to grab 40 character
    blocks of this text as we go through and we are going to then give it the next
    characterã€‚ So the 40 first character after that and it is going to use that to
    essentially train itã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥æ˜¯æ¥è‡ªå°¼é‡‡çš„ã€‚æ‰€ä»¥å½“æ‚¨è¿è¡Œè¿™ä¸€éƒ¨åˆ†æ—¶ï¼Œå®ƒä½¿ç”¨çš„æœ€å¤§é•¿åº¦æ˜¯40ï¼Œè¿™æ˜¯æ‚¨çš„åºåˆ—å¤§å°ã€‚æˆ‘ä»¬å°†æŠ“å–è¿™ä¸ªæ–‡æœ¬çš„40ä¸ªå­—ç¬¦å—ï¼Œç„¶åç»™å®ƒä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚æ‰€ä»¥åœ¨é‚£ä¹‹åçš„ç¬¬40ä¸ªå­—ç¬¦ï¼Œå®ƒå°†ç”¨æ¥è®­ç»ƒæ¨¡å‹ã€‚
- en: So given these 40 characters try to generate the next character and that's how
    we're going to actually generate it then we're going to pick 40 characters of
    real text as the seed to startã€‚Then it'll generate the 41st character then we
    give it 39 characters of the seed and the 41stã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šè¿™40ä¸ªå­—ç¬¦ï¼Œå°è¯•ç”Ÿæˆä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å®é™…ç”Ÿæˆæ–‡æœ¬çš„æ–¹å¼ã€‚ç„¶åæˆ‘ä»¬å°†é€‰æ‹©40ä¸ªçœŸå®æ–‡æœ¬å­—ç¬¦ä½œä¸ºç§å­å¼€å§‹ã€‚æ¥ç€ï¼Œå®ƒä¼šç”Ÿæˆç¬¬41ä¸ªå­—ç¬¦ï¼Œç„¶åæˆ‘ä»¬ç»™å®ƒ39ä¸ªç§å­å­—ç¬¦å’Œç¬¬41ä¸ªå­—ç¬¦ã€‚
- en: the 41st that it had generatedï¼Œ then it generates to the next one so it keeps
    generating more and more and eventually we're out of the seed and it's all generated
    text because it generated the first character then secondã€‚ third fourth and so
    on we'll see the actual code to do the but this is basically going through the
    entire file breaking it into 40 character chunks with the 41st as the y step size
    that just means move forward three characters for each of those 40 character chunks
    that that you grab if you put a one in hereã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆçš„ç¬¬41ä¸ªå­—ç¬¦ï¼Œç„¶åå®ƒç”Ÿæˆä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå› æ­¤å®ƒä¼šä¸æ–­ç”Ÿæˆæ›´å¤šï¼Œæœ€ç»ˆæˆ‘ä»¬ä¼šè¶…å‡ºç§å­ï¼Œå¾—åˆ°çš„éƒ½æ˜¯ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå› ä¸ºå®ƒç”Ÿæˆäº†ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼Œç„¶åæ˜¯ç¬¬äºŒä¸ªã€ç¬¬ä¸‰ä¸ªã€ç¬¬å››ä¸ªï¼Œä¾æ­¤ç±»æ¨ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å®é™…çš„ä»£ç ï¼Œä½†åŸºæœ¬ä¸Šè¿™æ˜¯éå†æ•´ä¸ªæ–‡ä»¶ï¼Œå°†å…¶åˆ†æˆ40ä¸ªå­—ç¬¦çš„å—ï¼Œç¬¬41ä¸ªä½œä¸ºyæ­¥é•¿ï¼Œè¿™æ„å‘³ç€å¯¹æ¯ä¸ªæŠ“å–çš„40ä¸ªå­—ç¬¦å—å‘å‰ç§»åŠ¨ä¸‰ä¸ªå­—ç¬¦ï¼Œå¦‚æœæ‚¨åœ¨è¿™é‡Œè¾“å…¥1ã€‚
- en: it would grab as many sequences it could because it would keep shifting it by
    just one character and you would get even more redundant sequences because youd
    get those same when I mean if you were scooting over by one you would get 39 of
    the same characters when you shiftedã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šæŠ“å–å°½å¯èƒ½å¤šçš„åºåˆ—ï¼Œå› ä¸ºå®ƒåªä¼šå°†å…¶ç§»åŠ¨ä¸€ä¸ªå­—ç¬¦ï¼Œæ‚¨å°†å¾—åˆ°æ›´å¤šå†—ä½™åºåˆ—ï¼Œå› ä¸ºå¦‚æœæ‚¨å‘å‰ç§»åŠ¨ä¸€ä¸ªå­—ç¬¦ï¼Œæ‚¨å°†è·å¾—39ä¸ªç›¸åŒçš„å­—ç¬¦ã€‚
- en: Fward by oneã€‚ So we'll run this and that reduces Treasure Island into 132000
    of these these sequencesã€‚ And if you print out one of these see that's showing
    you real good example of it the project Gutenberg of ebook treasure and then see
    a shifts over threeã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å‘å‰ç§»åŠ¨ä¸€ä¸ªã€‚å› æ­¤æˆ‘ä»¬å°†è¿è¡Œè¿™ä¸ªï¼Œå®ƒå°†ã€Šé‡‘é“¶å²›ã€‹å‡å°‘ä¸º 132000 ä¸ªè¿™æ ·çš„åºåˆ—ã€‚å¦‚æœä½ æ‰“å°å‡ºå…¶ä¸­ä¸€ä¸ªï¼Œå¯ä»¥çœ‹åˆ°è¿™å¾ˆå¥½åœ°å±•ç¤ºäº†é¡¹ç›® Gutenberg
    çš„ç”µå­ä¹¦ã€Šé‡‘é“¶å²›ã€‹ï¼Œç„¶åçœ‹åˆ°å®ƒå‘å³ç§»åŠ¨äº†ä¸‰ä¸ªã€‚
- en: So these are all of the inputs and then it's going to be trained to predict
    what that next character would beã€‚ Now we're going to vectorize itã€‚ this changes
    it into the actual x and y that is going to go into there that takes a moment
    to runã€‚ it's not too badã€‚ we're using essentially indicesã€‚ So if we did if we
    look at the shapes of these Those are all those sequences that you had40 is the
    input vector and 60 or the dummy variables for the outputã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›éƒ½æ˜¯è¾“å…¥ï¼Œç„¶åå®ƒå°†è¢«è®­ç»ƒä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯ä»€ä¹ˆã€‚ç°åœ¨æˆ‘ä»¬å°†è¿›è¡Œå‘é‡åŒ–ã€‚è¿™å°†å…¶è½¬æ¢ä¸ºå®é™…çš„ x å’Œ yï¼Œæ¥ä¸‹æ¥ä¼šç”¨åˆ°ï¼Œè¿è¡Œéœ€è¦ä¸€ç‚¹æ—¶é—´ï¼Œä½†è¿˜ä¸é”™ã€‚æˆ‘ä»¬å®é™…ä¸Šä½¿ç”¨çš„æ˜¯ç´¢å¼•ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™äº›å½¢çŠ¶ï¼Œæ‰€æœ‰è¿™äº›åºåˆ—ï¼Œ40
    æ˜¯è¾“å…¥å‘é‡ï¼Œ60 æ˜¯è¾“å‡ºçš„è™šæ‹Ÿå˜é‡ã€‚
- en: So the output of this is basically going to be a dummy variable of those 60
    charactersã€‚ remember I said there were 60 different possible characters that could
    pop up in treasuresure Islandã€‚ this is basically coming up with thoseã€‚Then the
    Y shape that's very simpleã€‚ that's 132000ã€‚ but it's the same dummy variablesã€‚
    So those are the expected characters So all those 60 characters that come throughout
    the training data as well as the output the characters are all represented as
    dummy variables So this is where these get a little bit wasteful I mean imagine
    if I had the full 255 ASI set it would be 255 dummy variables this would be insane
    with Uniicode and just to see the dummy variablesã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªè¾“å‡ºåŸºæœ¬ä¸Šå°†æ˜¯è¿™ 60 ä¸ªå­—ç¬¦çš„è™šæ‹Ÿå˜é‡ã€‚è®°å¾—æˆ‘è¯´è¿‡å¯èƒ½ä¼šåœ¨ã€Šé‡‘é“¶å²›ã€‹ä¸­å‡ºç° 60 ä¸ªä¸åŒçš„å­—ç¬¦ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ç”Ÿæˆè¿™äº›å­—ç¬¦çš„ã€‚ç„¶å Y å½¢çŠ¶éå¸¸ç®€å•ã€‚å®ƒæ˜¯
    132000ï¼Œä½†è¿™äº›éƒ½æ˜¯ç›¸åŒçš„è™šæ‹Ÿå˜é‡ã€‚è¿™äº›éƒ½æ˜¯é¢„æœŸå­—ç¬¦ã€‚æ‰€ä»¥æ‰€æœ‰åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„ 60 ä¸ªå­—ç¬¦ä»¥åŠè¾“å‡ºå­—ç¬¦éƒ½è¡¨ç¤ºä¸ºè™šæ‹Ÿå˜é‡ã€‚è¿™å°±æ˜¯è¿™äº›å˜å¾—æœ‰ç‚¹æµªè´¹çš„åœ°æ–¹ã€‚æˆ‘æ˜¯è¯´ï¼Œå¦‚æœæˆ‘æœ‰å®Œæ•´çš„
    255 ASI é›†ï¼Œé‚£å°†æ˜¯ 255 ä¸ªè™šæ‹Ÿå˜é‡ï¼Œè¿™ç®€ç›´ç–¯ç‹‚ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ Unicodeï¼Œçœ‹çœ‹è¿™äº›è™šæ‹Ÿå˜é‡ã€‚
- en: these are basically the output So one is true the rest of false that is saying
    thats this particular character is the next one that is expected the LSTM model
    itself is relatively simpleã€‚ I grab the hyperpara right from the example in Car
    we have one LSTM layer that has 128 I'm really not even using dropout or anything
    like that I've seen some examples that do make use of that and the example trained
    this with RMS you could probablyã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸºæœ¬ä¸Šæ˜¯è¾“å‡ºã€‚æ‰€ä»¥ä¸€ä¸ªæ˜¯çœŸçš„ï¼Œå…¶ä½™éƒ½æ˜¯å‡çš„ï¼Œè¿™è¡¨ç¤ºè¿™ä¸ªç‰¹å®šçš„å­—ç¬¦æ˜¯ä¸‹ä¸€ä¸ªé¢„æœŸçš„å­—ç¬¦ï¼ŒLSTM æ¨¡å‹æœ¬èº«ç›¸å¯¹ç®€å•ã€‚æˆ‘ç›´æ¥ä»ç¤ºä¾‹ä¸­è·å–è¶…å‚æ•°ï¼Œåœ¨ Car
    ä¸­æˆ‘ä»¬æœ‰ä¸€ä¸ª LSTM å±‚ï¼ŒåŒ…å« 128 ä¸ªç¥ç»å…ƒï¼Œæˆ‘å®é™…ä¸Šæ²¡æœ‰ä½¿ç”¨ dropout æˆ–å…¶ä»–ç±»ä¼¼çš„ä¸œè¥¿ï¼Œæˆ‘è§è¿‡ä¸€äº›ä¾‹å­ç¡®å®ä½¿ç”¨äº†è¿™äº›ï¼Œç¤ºä¾‹ä½¿ç”¨ RMS è¿›è¡Œäº†è®­ç»ƒï¼Œä½ å¯èƒ½å¯ä»¥ã€‚
- en: This with Adam just as wellã€‚ I'm going go ahead run this so that it starts training
    the neural network while I'm explaining some of it because it does take it a little
    while to runã€‚ you'd probably want to run this on your on a GPU on Google coab
    or something like thatã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Adam ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘å°†ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œè®©å®ƒå¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œï¼ŒåŒæ—¶æˆ‘è§£é‡Šä¸€äº›å†…å®¹ï¼Œå› ä¸ºè¿è¡Œéœ€è¦ä¸€ç‚¹æ—¶é—´ã€‚ä½ å¯èƒ½æƒ³åœ¨ Google Colab æˆ–ç±»ä¼¼çš„åœ°æ–¹åœ¨
    GPU ä¸Šè¿è¡Œè¿™ä¸ªã€‚
- en: Okay so here's the summary of it that largely echoes just what we just what
    we had thereã€‚ despite those stars it has actually already made it through these
    This is the sample functionã€‚ This is what actually generates the textã€‚ So you
    pass it pres Pres are the output neuronsã€‚ So those are going to be the 60 values
    in the case of Treasure Islandã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™é‡Œæ˜¯æ€»ç»“ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå‘¼åº”äº†æˆ‘ä»¬åˆšæ‰æåˆ°çš„å†…å®¹ã€‚å°½ç®¡æœ‰è¿™äº›æ˜Ÿå·ï¼Œå®ƒå®é™…ä¸Šå·²ç»é€šè¿‡äº†è¿™äº›ã€‚è¿™æ˜¯æ ·æœ¬å‡½æ•°ã€‚è¿™å®é™…ä¸Šæ˜¯ç”Ÿæˆæ–‡æœ¬çš„ã€‚å› æ­¤ï¼Œä½ ä¼ é€’çš„ pres
    æ˜¯è¾“å‡ºç¥ç»å…ƒã€‚è¿™å°†åœ¨ã€Šé‡‘é“¶å²›ã€‹ä¸­æ˜¯ 60 ä¸ªå€¼ã€‚
- en: this is going to be 60 probabilitiesã€‚ So whichever of those 60 output neurons
    has the highest valueã€‚ that is the character that has the highest likelihoodã€‚
    Howeverã€‚ we don't just do it thats simply we do normalize those output neurons
    into a softm so that their probability so they sum to 10 and then weã€‚Have something
    else called temperature Now temperature is what Andrea called it in his blog post
    that I was showing you earlier essentially 1ã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ˜¯ 60 ä¸ªæ¦‚ç‡ã€‚æ‰€ä»¥è¿™ 60 ä¸ªè¾“å‡ºç¥ç»å…ƒä¸­å€¼æœ€é«˜çš„å°±æ˜¯å…·æœ‰æœ€é«˜å¯èƒ½æ€§çš„å­—ç¬¦ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¹¶ä¸æ˜¯ç®€å•åœ°è¿™æ ·åšï¼Œæˆ‘ä»¬å°†è¿™äº›è¾“å‡ºç¥ç»å…ƒè§„èŒƒåŒ–ä¸º softmï¼Œä½¿å®ƒä»¬çš„æ¦‚ç‡ç›¸åŠ ä¸º
    10ï¼Œç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªå«æ¸©åº¦çš„ä¸œè¥¿ã€‚ç°åœ¨æ¸©åº¦å°±æ˜¯å®‰å¾·çƒˆåœ¨ä»–ä¹‹å‰å±•ç¤ºç»™ä½ çœ‹çš„åšå®¢æ–‡ç« ä¸­æåˆ°çš„ä¸œè¥¿ï¼Œæœ¬è´¨ä¸Šæ˜¯ 1ã€‚
- en: 0 is going to be the most conservativeã€‚ it'll pretty much take the character
    with the highest probability zero will be somewhat more random we'll see the examples
    of whats what that's producing the zero will have more grammatical errors but
    we'll produce more interesting text and we we'll go we'll look at some examples
    in between as well you can actually set that to even higher than than 1ã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ˜¯æœ€ä¿å®ˆçš„ã€‚å®ƒå‡ ä¹ä¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å­—ç¬¦ï¼Œé›¶åˆ™ä¼šç¨å¾®æ›´éšæœºï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å®ƒäº§ç”Ÿçš„ç¤ºä¾‹ï¼Œé›¶ä¼šæœ‰æ›´å¤šçš„è¯­æ³•é”™è¯¯ï¼Œä½†ä¼šç”Ÿæˆæ›´æœ‰è¶£çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å…¶ä¸­çš„ä¸€äº›ä¾‹å­ï¼Œä½ å®é™…ä¸Šå¯ä»¥å°†å…¶è®¾ç½®å¾—æ›´é«˜ï¼Œç”šè‡³é«˜äº1ã€‚
- en: 0 to make it even more conservative This is essentially a softm function it's
    essentially doing a summation and then and ensuring that all of those probabilities
    actually add up to 1ã€‚0 This is the text generator So we hook this up as a callback
    at the end of each epoch So this is kind of useful since this takes a while for
    this neural network to train I think we train it for sixã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 0ä¸ºä½¿å…¶æ›´ä¿å®ˆï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªsoftmaxå‡½æ•°ï¼Œå®ƒåŸºæœ¬ä¸Šæ˜¯åœ¨åšä¸€ä¸ªæ±‚å’Œï¼Œå¹¶ç¡®ä¿æ‰€æœ‰è¿™äº›æ¦‚ç‡åŠ èµ·æ¥ç¡®å®ç­‰äº1ã€‚0è¿™æ˜¯æ–‡æœ¬ç”Ÿæˆå™¨ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨æ¯ä¸ªepochç»“æŸæ—¶å°†å…¶ä½œä¸ºå›è°ƒæŒ‚æ¥ä¸Šã€‚è¿™æ˜¯éå¸¸æœ‰ç”¨çš„ï¼Œå› ä¸ºè¿™ä¸ªç¥ç»ç½‘ç»œè®­ç»ƒéœ€è¦ä¸€äº›æ—¶é—´ï¼Œæˆ‘æƒ³æˆ‘ä»¬è®­ç»ƒäº†å…­ä¸ªepochã€‚
- en: Upo we can see how well it is generating at the endã€‚ So essentially what we
    are doing is we display it for these temperaturesã€‚ This is 0ã€‚2 pretty liberal
    up to 1ã€‚2 pretty conservative very conservative and we essentially generate the
    seed So the seed comes from that text that we processed treasure island that seed
    is going to be used to prime the pump so to speak those 40 characters are the
    first characters that will be used to generate the next character and then it
    keeps going from thereã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒåœ¨æœ€åç”Ÿæˆçš„æ•ˆæœã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬ä¸ºè¿™äº›æ¸©åº¦å±•ç¤ºå®ƒã€‚è¿™æ˜¯0.2ï¼Œæ¯”è¾ƒå®½æ¾ï¼Œåˆ°1.2åˆ™ç›¸å¯¹ä¿å®ˆï¼Œéå¸¸ä¿å®ˆï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šç”Ÿæˆç§å­ã€‚å› æ­¤ï¼Œç§å­æ¥è‡ªæˆ‘ä»¬å¤„ç†è¿‡çš„æ–‡æœ¬ã€Šé‡‘é“¶å²›ã€‹ï¼Œè¿™ä¸ªç§å­å°†ç”¨äºå¯åŠ¨ç”Ÿæˆï¼Œæ¢å¥è¯è¯´ï¼Œè¿™40ä¸ªå­—ç¬¦æ˜¯ç”¨äºç”Ÿæˆä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼Œç„¶åå®ƒä¼šç»§ç»­ä¸‹å»ã€‚
- en: then we're going to generate it for the next 400 characters and we build that
    up as the input sequence to the neural network and we predict as we predict we
    add whatever we generate as we predict we add the next character onto what we
    generate it so we keep adding more and more characters onto it and eventually
    the seed rolls off the edge because the first character is removed because as
    soon as we add a generate a character onto the endã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†ä¸ºæ¥ä¸‹æ¥çš„400ä¸ªå­—ç¬¦ç”Ÿæˆå†…å®¹ï¼Œå¹¶å°†å…¶æ„å»ºä¸ºè¾“å…¥åºåˆ—ä¾›ç¥ç»ç½‘ç»œä½¿ç”¨ï¼Œåœ¨æˆ‘ä»¬è¿›è¡Œé¢„æµ‹æ—¶ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆçš„å†…å®¹åŠ åˆ°æˆ‘ä»¬é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ä¸Šï¼Œå› æ­¤æˆ‘ä»¬ä¸æ–­æ·»åŠ æ›´å¤šçš„å­—ç¬¦ï¼Œæœ€ç»ˆç§å­ä¼šä»è¾¹ç¼˜æ»‘è½ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªå­—ç¬¦è¢«ç§»é™¤ï¼Œå› ä¸ºä¸€æ—¦æˆ‘ä»¬åœ¨æœ«å°¾æ·»åŠ ä¸€ä¸ªç”Ÿæˆçš„å­—ç¬¦ã€‚
- en: We have to pop something off the beginning so that it stays in the 400 range
    and then we print out the text as as we didã€‚ So this will literally output the
    text as it is training the neural network we make that function we just looked
    at as a lambmbda callback so that we it can be called by Kiras as it's running
    and then we essentially fit it kind of like we've seen before because we have
    that callbackã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»ä»å¼€å§‹çš„éƒ¨åˆ†å¼¹å‡ºä¸€äº›å†…å®¹ï¼Œä»¥ä¿æŒåœ¨400èŒƒå›´å†…ï¼Œç„¶åæˆ‘ä»¬åƒä»¥å‰ä¸€æ ·æ‰“å°æ–‡æœ¬ã€‚æ‰€ä»¥è¿™å®é™…ä¸Šä¼šè¾“å‡ºè®­ç»ƒç¥ç»ç½‘ç»œæ—¶çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬æŠŠåˆšæ‰çœ‹åˆ°çš„å‡½æ•°ä½œä¸ºä¸€ä¸ªlambdaå›è°ƒï¼Œè¿™æ ·åœ¨Kerasè¿è¡Œæ—¶å°±å¯ä»¥è°ƒç”¨å®ƒï¼Œç„¶åæˆ‘ä»¬åŸºæœ¬ä¸Šåƒä¹‹å‰çœ‹åˆ°çš„é‚£æ ·æ‹Ÿåˆå®ƒï¼Œå› ä¸ºæˆ‘ä»¬æœ‰é‚£ä¸ªå›è°ƒã€‚
- en: this is what the output's going to look like This is the very first epoch you
    can see it was it was training here and here is the generated text this probably
    got a hold of some of the media wiki but information can be found at found captain
    of the sound Now remember the neural network is on its first epoch it has not
    trained very well but look it's already figured out and the the goes before nouns
    usually that sort of thing This was with a very low temperature so this will be
    the lower quality or most least conservative if we go to the most conservative
    the 1ã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯è¾“å‡ºçš„æ ·å­ã€‚è¿™æ˜¯ç¬¬ä¸€æ¬¡epochï¼Œä½ å¯ä»¥çœ‹åˆ°å®ƒåœ¨è¿™é‡Œè¿›è¡Œè®­ç»ƒï¼Œè¿™é‡Œæ˜¯ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè¿™å¯èƒ½è·å–äº†ä¸€äº›åª’ä½“ç»´åŸºçš„ä¿¡æ¯ï¼Œä½†å¯ä»¥åœ¨â€œå£°éŸ³çš„èˆ¹é•¿â€æ‰¾åˆ°ã€‚ç°åœ¨è¯·è®°ä½ï¼Œç¥ç»ç½‘ç»œæ˜¯åœ¨ç¬¬ä¸€æ¬¡epochä¸Šï¼Œå®ƒçš„è®­ç»ƒæ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œä½†çœ‹çœ‹ï¼Œå®ƒå·²ç»ææ¸…æ¥šäº†åè¯å‰é€šå¸¸ä¼šæœ‰çš„â€œtheâ€è¿™ç±»äº‹ç‰©ã€‚è¿™æ˜¯ä½¿ç”¨éå¸¸ä½çš„æ¸©åº¦ï¼Œæ‰€ä»¥è¿™å°†æ˜¯è´¨é‡è¾ƒä½æˆ–æœ€ä¸ä¿å®ˆçš„ã€‚å¦‚æœæˆ‘ä»¬å»æœ€ä¿å®ˆçš„1.0ã€‚
- en: 2ã€‚Information can be found at the following still not very good because it's
    really just just startingã€‚ We've done a second epoch third epoch it continues
    to train and it keeps showing you as it goesã€‚ll it'll end at 60 so five of 60
    we're not going to let this thing run into its entirety but we can see it is generating
    its own pirate stories of you this here house that's probably how the pirates
    would talk actually that's not gramatic I don't believe that's grammatically of
    you and this house and what I say of you sounds like a pirate story I'm not really
    good at speaking in a pirate accent IM and here you can see it's literally outputting
    it as it as it queries the neural network So this is text generation this is a
    good example of it This is not particularly useful other than showing that it
    can really pick up English and things in it in the next part we'll see how we
    can generate next part we' see how we can generate captions from images Thank
    you for watching this video and the next videoã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2ã€‚ä¿¡æ¯å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°ï¼Œå°½ç®¡ç›®å‰çš„æ•ˆæœä»ç„¶ä¸ä½³ï¼Œå› ä¸ºå®ƒå®é™…ä¸Šåˆšåˆšå¼€å§‹ã€‚æˆ‘ä»¬å·²ç»è¿›è¡Œäº†ç¬¬äºŒä¸ªå‘¨æœŸå’Œç¬¬ä¸‰ä¸ªå‘¨æœŸï¼Œå®ƒç»§ç»­è®­ç»ƒï¼Œå¹¶éšç€è¿›ç¨‹ä¸æ–­å±•ç¤ºç»“æœã€‚å®ƒå°†åœ¨60æ—¶ç»“æŸï¼Œå› æ­¤äº”ä¸ªä¸­çš„60æˆ‘ä»¬ä¸ä¼šè®©è¿™ä¸ªä¸œè¥¿å®Œå…¨è¿è¡Œï¼Œä½†æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ­£åœ¨ç”Ÿæˆè‡ªå·±çš„æµ·ç›—æ•…äº‹ã€‚è¿™å¬èµ·æ¥åƒæ˜¯æµ·ç›—çš„è°ˆè¯ï¼Œå®é™…ä¸Šæˆ‘ä¸è®¤ä¸ºè¿™åœ¨è¯­æ³•ä¸Šæ˜¯æ­£ç¡®çš„ã€‚è¿™å¬èµ·æ¥åƒæ˜¯å…³äºä½ çš„æ•…äº‹ï¼Œæˆ‘å¹¶ä¸æ“…é•¿ç”¨æµ·ç›—å£éŸ³è¯´è¯ã€‚åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥çœ‹åˆ°å®ƒå®é™…ä¸Šæ˜¯ä½œä¸ºæŸ¥è¯¢ç¥ç»ç½‘ç»œæ—¶è¾“å‡ºçš„ã€‚è¿™æ˜¯æ–‡æœ¬ç”Ÿæˆçš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚é™¤äº†æ˜¾ç¤ºå®ƒç¡®å®èƒ½å¤ŸæŒæ¡è‹±è¯­åŠå…¶ç›¸å…³å†…å®¹ä¹‹å¤–ï¼Œè¿™å¹¶æ²¡æœ‰ç‰¹åˆ«çš„ç”¨å¤„ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä»å›¾åƒç”Ÿæˆæ ‡é¢˜ã€‚æ„Ÿè°¢è§‚çœ‹æœ¬è§†é¢‘ä»¥åŠä¸‹ä¸€ä¸ªè§†é¢‘ã€‚
- en: '![](img/12a40ad6cb8e26a4cb77c05d5b96a105_3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12a40ad6cb8e26a4cb77c05d5b96a105_3.png)'
- en: We're going to look at how we can use the same technology to label pictures
    and to generate captions to describe what is going on in that pictureã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨ç›¸åŒçš„æŠ€æœ¯æ¥æ ‡è®°å›¾ç‰‡å¹¶ç”Ÿæˆæè¿°è¯¥å›¾ç‰‡å†…å®¹çš„æ ‡é¢˜ã€‚
