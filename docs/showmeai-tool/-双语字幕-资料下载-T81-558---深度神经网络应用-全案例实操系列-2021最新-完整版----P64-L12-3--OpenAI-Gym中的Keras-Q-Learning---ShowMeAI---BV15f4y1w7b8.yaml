- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P64ï¼šL12.3- OpenAI Gymä¸­çš„Keras
    Q-Learning - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hiï¼Œ this is Jeff Heatonï¼Œ Wee to applications of Deep neural networks with Washington
    University In this partã€‚ we're going to look at how to use a deep Q neural networkã€‚
    So a deep reinforcement neural network that allows the computer to learn to play
    a gameã€‚ We're going to start with a relatively simple exampleï¼Œ we're going to
    see Polcart using TF agentsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: And well then in the next partï¼Œ expand this to be more complexã€‚ We'll use an
    Atari gameã€‚ And then following thatï¼Œ stillï¼Œ we will actually create sort of a
    financial simulationã€‚ So do something that's not even a video game to see all
    my videos about Cale neural networks and other AI topicsã€‚ click the subscribe
    button and the bell next to it and select al to be notified of every new videoã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: now we're going to look at Q learning using the open AI gymã€‚ we're going to
    use Cars to do at this timeã€‚ So we're going to use Tf agentsã€‚ So we're actually
    using deep neural networks now to build up that Q table that you saw beforeã€‚ So
    essentially in Q learning you build up a table that has a list of every possible
    state of the world can be inã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: And then the anticipated rewards of your next stepsã€‚ Now I am going to run this
    in coabab because a GPU is useful for thisã€‚ I am going to go ahead and chooseã€‚ğŸ˜Šã€‚![](img/1a1a7c7d49eba0604d4b8360d28b107c_1.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a1a7c7d49eba0604d4b8360d28b107c_2.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: '![](img/1a1a7c7d49eba0604d4b8360d28b107c_3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: rununtime and run all because at least with the current version of thisã€‚ hopefully
    this improves by the time that you run thisã€‚ you'll get an error actually going
    through this with the current version2ã€‚2 of Tensorflow and some of the other things
    that they have installed in Google Coabã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: So see it's running this part of the program here that just gets me the right
    version of Tensorflow that I wantã€‚ This part here is doing some installs to put
    various software in that we need like TF agents and this other software lets me
    just capture the game being played as video so that I can play it in coabab you
    can use this on your own if you're running locally on your computer or you can
    leave this part out and it'll just pop up a window and show you how the game is
    being played so this is where you normally get this error and I want to explain
    this to you because this only started happening a few days ago when Google upgraded
    some stuff inã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a1a7c7d49eba0604d4b8360d28b107c_5.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Lb and TF agents as wellã€‚ And if you get this errorã€‚ basically and let me explain
    one other thing tooï¼Œ if you get an error like thisã€‚ this error is not telling
    me a great dealã€‚ if you Google thisã€‚ they're gonna basically just say to reinstall
    proto buff which is actually not what it takes So this took me about an hour And
    the way I figured this out was basically by going up here and running each of
    these in part and then seeing which of them was actually causing the error and
    when I ran it separatelyã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: I actually got to the error that was really happening and it was basically saying
    that since I installed all those packages above thereã€‚ I need to restart my runtime
    which I'm doing now And now I can basically do a runtime and run all and it will
    it'll make it through there just fineã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: s a little little trick there you might not need to do this if you don't get
    the error Don't worry about itã€‚ they fixed it since I recorded this video Now
    while this is running is this takes a little whileã€‚And notice tooï¼Œ even though
    I restarted the environmentã€‚ it's saying already newest version that was for my
    last install of thisã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: So if I go away from this for a few hours in coabab times out and takes my environment
    awayã€‚ then I've got to rerun it from scratchã€‚ But even though I restarted the
    environmentã€‚ my changes to the hard drive by installing these was still the sameã€‚
    So that that is kind of how that worksã€‚ Now this should have made it through the
    part that was airing beforeã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: certainly hope so between Google Coab and Tensorflowã€‚ they like to send me back
    to the drawing board a lotã€‚ but that's part of part of what I do as far as making
    these videos is showing you how all this stuff works together and believe me machine
    learning changes at the speed of light code that I had working a week ago may
    or may not still be workingã€‚ So that's why always make sure that you check my
    Github repository to get the latest version of these thingsã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: So as we go through all of this code that I'll be explained in a moment It's
    trainingã€‚ So it has made it completely throughã€‚ So I'm going to letã€‚Go ahead and
    train and run while I explain sort of what's going on hereã€‚ These are all of the
    imports that you needã€‚ Just go ahead and use thoseã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: These are the hyperparameterã€‚ some of these are importantã€‚ Some of them not
    so much number of iterations is very importantã€‚ This is how long you're going
    to trainã€‚ You're not really using early stopping in this capacityã€‚ you're you're
    simply picking the number of iterationsã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: I did not set up this so that it's restartable that might be a very good modification
    to make for this because if you're doing something really complexã€‚ you may train
    it for 20000 apo and then realize I want to train it for 20000 moreã€‚ This is very
    importantã€‚ This will define the success or failure of your project you need enoughã€‚
    You need enough iterationsã€‚ I collect stepsã€‚ This is not that importantã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: At least I've not found it to beã€‚ This is how many times you run through episodes
    to get some initial dataã€‚To train your neural network with collection steps per
    iterationã€‚ This is actually very importantã€‚ For a simple one like thisã€‚ I set
    it to oneï¼Œ but you might want to set this on the Atariã€‚ I think I set this to
    about 10ã€‚ It takes it a lot longer to trainï¼Œ but you get muchã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: much better resultsï¼Œ typically Collect steps per iteration and iterations itself
    really is the only downside of increasing those is going to make it's just going
    to take a long time to run into trainã€‚ I wouldn't set this too far above 10ã€‚ I
    set it usually somewhere between  one and 10ã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This is the size of your bufferã€‚ So essentially it's running through episodes
    and it's storing information from each game that it playsã€‚ initially it fills
    10 of them before it startsã€‚ This is just how many how many steps you're really
    willing to store in thereã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This is the batch sizeï¼Œ which is pretty typical for machine learningã€‚ You might
    get better results with a little smallerã€‚ your learning rateã€‚ This is absolutely
    criticalã€‚I typically spend most of my tuning time tuning the learning rateã€‚ the
    collections per iteration and the number of iterationsã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Basically you want this number as big as possibleã€‚ if you set it too big then
    it's going to it's going to become unstable very quickly 10 to the negative third
    works really pretty well for this but if you find you' nuts not stabilizingã€‚ change
    this to negative4 negative5 so on so forthã€‚ but the bigger you make it like a
    negative3 or negative2 you probably won't get by with but in that range it's going
    to train faster it'll need fewer iterationsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: This is how often we want a log report and this won't affect your training muchã€‚
    but it lets you know what's going onã€‚ the number of evaluation episodes So how
    many episodes how many complete runs of the game do you want your evaluation based
    on 1s a good number and then how often do you want to evaluate don't set this
    too low because it does take timeã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Do these evaluations we'll see in the Atari game I let it go muchã€‚ much further
    before doing evaluations just to keep things efficient The environment that we're
    going to useã€‚ we're gonna to use cartpo that is where it's basically trying to
    move a cart that has a pole balanced on it and you don't want the pole to fall
    over so basically figuring out gravity and angular momentum of sort of thing I
    like this code here this lets you render what the environment looks like so that's
    a great way to visualize it there's the cart there's the pole the pole is connected
    by a hinge to the cart and I print out some basic data on it so you can see that
    the observation there's basically four values being observed which is basically
    the velocity of the cart the angle of the pole and the angular velocity of the
    pole and also the position of the pole that makes of the whole cart that makes
    the fourth one the reward is pretty simple is just just a number value based on
    howã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Long you've kept the pole upright and this describes sort of the values that
    are being keptã€‚ let's seeã€‚I'll go ahead and mention this tooã€‚ You'll see that
    we deal with T F pi environment and T F Tensorflow environmentã€‚ There's two environment
    types that you can deal withã€‚ Usually you're dealing with Python environmentsï¼Œ
    but Tensorflow environments are written completely in Tensorflowã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: So that means you can literally compile that along with the entire execution
    tree of the neural network and they become veryã€‚ very performantã€‚ Howeverï¼Œ usuallyï¼Œ
    at least in this caseï¼Œ the Atari gamesã€‚ All these things are written in Pythonã€‚
    So or at least written in languages other than Tensorflowã€‚ So we almost have to
    use the Python environmentã€‚ I found myself usually using the Python environmentã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: I've not used the Tensorflow environments a lotã€‚ I bet you could get considerably
    fasterã€‚ğŸ˜Šã€‚Access thereã€‚ although sometimes I just prefer to throw compute at it
    because the more stuff that I program that is Tensorflow specificã€‚ if I ever want
    to try something else like a pytorrch and swap it outã€‚ There's more legacy sort
    of stuffed tied to one particular platformã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: do it in Python lets me stay pretty agnosticã€‚ So just a few design considerations
    hereï¼Œ the agentã€‚ we are going to use a DqN for this oneã€‚ DQN is great when your
    action space is boolean or is discreteã€‚ So you either in this caseï¼Œ you're either
    applying force in one direction or the other left or rightã€‚ Tru or falseã€‚ It's
    not how much forceã€‚ If you do need to have a continuous or a numeric action spaceã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: then I recommend DDPQã€‚ I'm sorryï¼Œ DDPGã€‚ That is the one that we will be seeing
    not next partã€‚ but in the fifth part for this moduleã€‚Because that lets us have
    continuous action space and for the financial simulation that I'm runningã€‚ I want
    continuousï¼Œ I wanted to be able to tell me what percent of income should be invested
    in a particular direction so the agent we create the Q networkã€‚ the Q network
    is the underlying deep neural network and then we place the Q network into the
    DQN network Most of this code for your own useã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘æ¨èDDPGã€‚æŠ±æ­‰ï¼ŒDDPGã€‚è¿™æ˜¯æˆ‘ä»¬å°†åœ¨æ¨¡å—çš„ç¬¬äº”éƒ¨åˆ†çœ‹åˆ°çš„å†…å®¹ï¼Œè€Œä¸æ˜¯ä¸‹ä¸€éƒ¨åˆ†ã€‚å› ä¸ºè¿™è®©æˆ‘ä»¬æ‹¥æœ‰è¿ç»­çš„åŠ¨ä½œç©ºé—´ï¼Œè€Œå¯¹äºæˆ‘æ­£åœ¨è¿è¡Œçš„é‡‘èæ¨¡æ‹Ÿï¼Œæˆ‘å¸Œæœ›æ˜¯è¿ç»­çš„ï¼Œèƒ½å¤Ÿå‘Šè¯‰æˆ‘åº”è¯¥å°†æ”¶å…¥çš„å¤šå°‘ç™¾åˆ†æ¯”æŠ•èµ„äºæŸä¸ªç‰¹å®šæ–¹å‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºçš„ä»£ç†çš„Qç½‘ç»œæ˜¯åŸºç¡€çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç„¶åæˆ‘ä»¬å°†Qç½‘ç»œæ”¾å…¥DQNç½‘ç»œã€‚å¤§éƒ¨åˆ†ä»£ç ä¾›ä½ è‡ªè¡Œä½¿ç”¨ã€‚
- en: you can just take it in this form is it's adjusting to what the environment
    says the action and observation space is the policies this is used inside of the
    agent to determine what your what you're going to do next we're going to use a
    random policy here so that we are able to fill that initial buffer with with value
    so that the neural network can start training metrics and evaluation I like this
    compute average return I got it from the TF agent examples but thisã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç›´æ¥ä»¥è¿™ç§å½¢å¼ä½¿ç”¨å®ƒï¼Œå› ä¸ºå®ƒä¼šæ ¹æ®ç¯å¢ƒæ‰€è¯´çš„åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´è¿›è¡Œè°ƒæ•´ã€‚è¿™äº›ç­–ç•¥åœ¨ä»£ç†å†…éƒ¨ä½¿ç”¨ï¼Œä»¥ç¡®å®šä½ æ¥ä¸‹æ¥è¦åšä»€ä¹ˆã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨éšæœºç­–ç•¥ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¡«å……é‚£ä¸ªåˆå§‹ç¼“å†²åŒºï¼Œä»¥ä¾¿ç¥ç»ç½‘ç»œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚åº¦é‡å’Œè¯„ä¼°æ–¹é¢ï¼Œæˆ‘å–œæ¬¢è®¡ç®—å¹³å‡å›æŠ¥ï¼Œè¿™æ¥è‡ªTFä»£ç†ç¤ºä¾‹ï¼Œä½†è¿™ã€‚
- en: Basically is looking at what was the reward return coming from the episode as
    it played out and summing that upã€‚ So this is a function really this entire carpool
    example comes from the TF agents examples and then I expand it to play an Atari
    game in the next part but this is basically just looking at what was their average
    return over a number of episodes in this case 10 I find this to be a good way
    to measure this because the world that the agent is operating is stochastic so
    there is some randomness going on there so each of these episode runs are not
    going to be the same and the average that it gets with those randomness gives
    you an idea of how good it's performing and you can see with this comment here
    that was in the T agents example TF agents does have a number of builtin metrics
    that you can use and I definitely encourage you to take a look at those we compute
    the average return for the random policy just showing that it works replayã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šæ˜¯æŸ¥çœ‹ä»æ¯ä¸ªepisodeä¸­è·å¾—çš„å¥–åŠ±å›æŠ¥å¹¶è¿›è¡Œæ±‚å’Œã€‚å› æ­¤ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ•´ä¸ªæ‹¼è½¦ç¤ºä¾‹æ¥è‡ªTFä»£ç†ç¤ºä¾‹ï¼Œç„¶åæˆ‘å°†å…¶æ‰©å±•ä»¥åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ç©Atariæ¸¸æˆï¼Œä½†åŸºæœ¬ä¸Šåªæ˜¯æŸ¥çœ‹åœ¨10ä¸ªepisodeä¸­ä»–ä»¬çš„å¹³å‡å›æŠ¥ã€‚æˆ‘å‘ç°è¿™æ˜¯ä¸€ç§å¾ˆå¥½çš„è¡¡é‡æ–¹å¼ï¼Œå› ä¸ºä»£ç†æ“ä½œçš„ä¸–ç•Œæ˜¯éšæœºçš„ï¼Œæ‰€ä»¥æ¯ä¸ªepisodeçš„è¿è¡Œä¸ä¼šç›¸åŒï¼Œå¹¶ä¸”éšç€è¿™äº›éšæœºæ€§å¾—åˆ°çš„å¹³å‡å€¼è®©ä½ äº†è§£å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„æ³¨é‡Šï¼Œå®ƒåœ¨TFä»£ç†ç¤ºä¾‹ä¸­ï¼ŒTFä»£ç†ç¡®å®æœ‰ä¸€äº›å†…ç½®çš„åº¦é‡ä¾›ä½ ä½¿ç”¨ï¼Œæˆ‘ç»å¯¹é¼“åŠ±ä½ æŸ¥çœ‹è¿™äº›ã€‚æˆ‘ä»¬è®¡ç®—éšæœºç­–ç•¥çš„å¹³å‡å›æŠ¥ï¼Œä»…ä»…æ˜¯ä¸ºäº†æ˜¾ç¤ºå®ƒæœ‰æ•ˆçš„é‡æ”¾ã€‚
- en: Bufffferï¼Œ this is very important is fundamentally you're training a neural network
    through all of this and this lets you store the data that you're training the
    neural network on because the way the neural network is actually working is you
    have a state and that state in old school cu learning every possibility of the
    state can result in some action and we simply train the neural network for each
    of these actions that you might have predict what the Q value is what the possible
    reward is so you feed the neural network in the state and it returns to you all
    the rewards of the action Now this is using a table so the problem with the table
    is you have to have a row for every single possible state and you just look it
    up and you find it Now with a neural network the state is going into the input
    neurons and then the neural network through neural network magic predicts for
    you which action you should take so it gives you the Q values of all your possibleã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Bufferï¼Œè¿™éå¸¸é‡è¦ï¼ŒåŸºæœ¬ä¸Šä½ æ˜¯åœ¨é€šè¿‡è¿™ä¸€åˆ‡è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè¿™è®©ä½ èƒ½å¤Ÿå­˜å‚¨ä½ æ­£åœ¨è®­ç»ƒç¥ç»ç½‘ç»œçš„æ•°æ®ã€‚å› ä¸ºç¥ç»ç½‘ç»œçš„å®é™…å·¥ä½œæ–¹å¼æ˜¯ä½ æœ‰ä¸€ä¸ªçŠ¶æ€ï¼Œè€Œåœ¨ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¯ç§çŠ¶æ€çš„æ‰€æœ‰å¯èƒ½æ€§éƒ½å¯ä»¥å¯¼è‡´æŸä¸ªåŠ¨ä½œï¼Œæˆ‘ä»¬åªæ˜¯ä¸ºæ¯ä¸ªè¿™äº›åŠ¨ä½œè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥é¢„æµ‹Qå€¼å’Œå¯èƒ½çš„å¥–åŠ±ã€‚å› æ­¤ï¼Œä½ å°†çŠ¶æ€è¾“å…¥ç¥ç»ç½‘ç»œï¼Œå®ƒä¼šè¿”å›æ‰€æœ‰åŠ¨ä½œçš„å¥–åŠ±ã€‚ç°åœ¨ï¼Œè¿™é‡Œæ˜¯ä½¿ç”¨ä¸€ä¸ªè¡¨æ ¼ï¼Œæ‰€ä»¥è¡¨æ ¼çš„é—®é¢˜æ˜¯ä½ å¿…é¡»ä¸ºæ¯ä¸€ä¸ªå¯èƒ½çš„çŠ¶æ€éƒ½æœ‰ä¸€è¡Œï¼Œç„¶åæŸ¥æ‰¾æ‰¾åˆ°å®ƒã€‚ç°åœ¨ä½¿ç”¨ç¥ç»ç½‘ç»œæ—¶ï¼ŒçŠ¶æ€è¾“å…¥åˆ°è¾“å…¥ç¥ç»å…ƒï¼Œç„¶åç¥ç»ç½‘ç»œé€šè¿‡ç¥ç»ç½‘ç»œçš„â€œé­”æ³•â€ä¸ºä½ é¢„æµ‹ä½ åº”è¯¥é‡‡å–çš„åŠ¨ä½œï¼Œå› æ­¤å®ƒç»™å‡ºæ‰€æœ‰å¯èƒ½çš„Qå€¼ã€‚
- en: And whichever action has the highest estimated Q valueï¼Œ you're going to use
    that oneã€‚ typically as your action that you're going to chooseã€‚ Now neural networks
    need to be trainedã€‚ so that's what that replay buffer is for as you run through
    episodesã€‚ you take all of these states and you put it through here and you see
    what your reward actually was and that becomes that becomes the replay bufferã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¼°è®¡Qå€¼æœ€é«˜çš„åŠ¨ä½œï¼Œä½ å°†é€‰æ‹©é‚£ä¸ªåŠ¨ä½œã€‚é€šå¸¸ä½œä¸ºä½ è¦é€‰æ‹©çš„è¡ŒåŠ¨ã€‚ç°åœ¨ç¥ç»ç½‘ç»œéœ€è¦è¢«è®­ç»ƒã€‚è¿™å°±æ˜¯é‡æ”¾ç¼“å†²åŒºçš„ä½œç”¨ï¼Œå½“ä½ ç»å†å¤šä¸ªå›åˆæ—¶ã€‚ä½ å°†æ‰€æœ‰è¿™äº›çŠ¶æ€æ”¾å…¥è¿™é‡Œï¼Œçœ‹çœ‹ä½ çš„å¥–åŠ±å®é™…ä¸Šæ˜¯ä»€ä¹ˆï¼Œè¿™å°±æˆä¸ºé‡æ”¾ç¼“å†²åŒºã€‚
- en: the training dataï¼Œ because you're going to actually see what the rewards were
    from what the neural network did and that adds additional rows inã€‚ So the neural
    network helps you get up to a certain stateã€‚ But then that next state you let
    the neural network tell you what to doã€‚ but then the reward you train the neural
    network based on that reward so that it can predict what next step it should have
    taken So it's constantly getting that reinforcement the neural network is helping
    you get to these states further and further into the game but then you feed the
    reward actually from the video game did youã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®ï¼Œå› ä¸ºä½ å®é™…ä¸Šä¼šçœ‹åˆ°ç¥ç»ç½‘ç»œæ‰€åšçš„å¥–åŠ±ï¼Œè¿™ä¼šæ·»åŠ é¢å¤–çš„è¡Œã€‚å› æ­¤ï¼Œç¥ç»ç½‘ç»œå¸®åŠ©ä½ è¾¾åˆ°æŸä¸ªçŠ¶æ€ã€‚ä½†æ¥ä¸‹æ¥é‚£ä¸ªçŠ¶æ€ï¼Œä½ è®©ç¥ç»ç½‘ç»œå‘Šè¯‰ä½ è¯¥æ€ä¹ˆåšã€‚ä½†å¥–åŠ±æ˜¯ä½ æ ¹æ®é‚£ä¸ªå¥–åŠ±è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥ä¾¿å®ƒå¯ä»¥é¢„æµ‹ä¸‹ä¸€æ­¥åº”è¯¥é‡‡å–ä»€ä¹ˆã€‚æ‰€ä»¥å®ƒä¸æ–­è·å¾—è¿™ç§å¼ºåŒ–ï¼Œç¥ç»ç½‘ç»œå¸®åŠ©ä½ åœ¨æ¸¸æˆä¸­é€æ¸è¾¾åˆ°è¿™äº›çŠ¶æ€ï¼Œä½†ä½ å®é™…ä¸Šæ˜¯ä»è§†é¢‘æ¸¸æˆä¸­é¦ˆé€å¥–åŠ±ã€‚
- en: Score a point or did you get killed that all has to factor into those Q values
    And that's what you're teaching the neural network to do because this works better
    than having a tableã€‚ You can't have a table for every possible combinationã€‚ The
    neural network learns to generalize this theoretical gigantic high dimensionmenal
    table and basically predict for you those Q valuesã€‚ data collectionã€‚ this is where
    you're generating those 1000 steps so that you get a random set of data So what
    we're basically doingã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—åˆ†è¿˜æ˜¯è¢«å‡»æ€ï¼Œè¿™äº›éƒ½è¦è€ƒè™‘åˆ°é‚£äº›Qå€¼ä¸­ã€‚è¿™å°±æ˜¯ä½ æ•™ç¥ç»ç½‘ç»œåšçš„äº‹æƒ…ï¼Œå› ä¸ºè¿™æ¯”ä½¿ç”¨è¡¨æ ¼æ›´æœ‰æ•ˆã€‚ä½ æ— æ³•ä¸ºæ¯ä¸€ç§å¯èƒ½çš„ç»„åˆåˆ›å»ºä¸€ä¸ªè¡¨æ ¼ã€‚ç¥ç»ç½‘ç»œå­¦ä¹ æ¦‚æ‹¬è¿™ä¸ªç†è®ºä¸Šçš„å·¨å¤§é«˜ç»´è¡¨æ ¼ï¼Œå¹¶åŸºæœ¬ä¸Šä¸ºä½ é¢„æµ‹é‚£äº›Qå€¼ã€‚æ•°æ®æ”¶é›†ã€‚è¿™å°±æ˜¯ä½ ç”Ÿæˆé‚£1000æ­¥çš„åœ°æ–¹ï¼Œä»¥ä¾¿è·å–ä¸€ç»„éšæœºæ•°æ®ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åŸºæœ¬ä¸Šæ‰€åšçš„ã€‚
- en: we we have a collect step that is being called by the main function collect
    data and this is right from the example and Tf agents What we're doing is we're
    getting we're calculating the time step So how far we want to project we're taking
    the action over that time step and then we are we are getting all the data from
    that step in the trajectory that it use to get to the next step of the action
    and adding that into the training batchã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ªæ”¶é›†æ­¥éª¤ï¼Œè¿™æ˜¯ç”±ä¸»å‡½æ•°è°ƒç”¨çš„ï¼Œæ”¶é›†æ•°æ®ã€‚è¿™æ˜¯ç›´æ¥æ¥è‡ªç¤ºä¾‹å’ŒTFä»£ç†çš„ã€‚æˆ‘ä»¬æ‰€åšçš„å°±æ˜¯è®¡ç®—æ—¶é—´æ­¥ã€‚æˆ‘ä»¬å¸Œæœ›é¢„æµ‹å¤šè¿œï¼Œæˆ‘ä»¬åœ¨é‚£ä¸ªæ—¶é—´æ­¥é‡‡å–è¡ŒåŠ¨ï¼Œç„¶åæˆ‘ä»¬è·å–æ‰€æœ‰æ•°æ®ï¼Œè¿™äº›æ•°æ®ç”¨äºä»é‚£ä¸ªæ­¥éª¤åˆ°è¾¾ä¸‹ä¸€ä¸ªåŠ¨ä½œçš„è½¨è¿¹ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°è®­ç»ƒæ‰¹æ¬¡ä¸­ã€‚
- en: That is how we're slowly building one by one these rows that the neural network
    is actually learning to train on then we go through the number of requested steps
    This is actually a bug I found in the TF agents code I left it exactly like this
    but they do have a hyperparameter up higher where they let you specify the number
    of steps and down here they ignore it and just do100 I fix that actually in the
    Atari example in the next one because that becomes important and then we convert
    this into a data set and we're basically ready to go Now we're training the agent
    I kick this off earlier and fortunate it finished so that is good we are basically
    going through a loop here listen set up the number of iterations that we want
    to step it through so that's the 20000 from up higher it's one by one building
    the continuing to build the data set that we have so that we continue to train
    the neural network on it based on the rewards that we're actually seeing if we
    reach the log interval that we wanted then we go aheadã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•é€æ­¥æ„å»ºè¿™äº›è¡Œï¼Œç¥ç»ç½‘ç»œå®é™…ä¸Šæ˜¯åœ¨å­¦ä¹ è®­ç»ƒï¼Œç„¶åæˆ‘ä»¬æŒ‰ç…§è¯·æ±‚çš„æ­¥æ•°è¿›è¡Œå¤„ç†ã€‚è¿™å®é™…ä¸Šæ˜¯æˆ‘åœ¨TFä»£ç†ä»£ç ä¸­å‘ç°çš„ä¸€ä¸ªbugï¼Œæˆ‘å°±è¿™æ ·ç•™ç€å®ƒï¼Œä½†ä»–ä»¬ç¡®å®åœ¨ä¸Šé¢æœ‰ä¸€ä¸ªè¶…å‚æ•°ï¼Œå…è®¸ä½ æŒ‡å®šæ­¥æ•°ï¼Œè€Œåœ¨è¿™é‡Œå´å¿½ç•¥å®ƒï¼Œä»…åš100ã€‚æˆ‘å®é™…ä¸Šåœ¨ä¸‹ä¸€ä¸ªAtariç¤ºä¾‹ä¸­ä¿®å¤äº†è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºè¿™å¾ˆé‡è¦ã€‚ç„¶åæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæ•°æ®é›†ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šå‡†å¤‡å¥½äº†ã€‚ç°åœ¨æˆ‘ä»¬åœ¨è®­ç»ƒä»£ç†ï¼Œæˆ‘æ—©äº›æ—¶å€™å¯åŠ¨äº†è¿™ä¸ªï¼Œå¹¶ä¸”å¹¸è¿çš„æ˜¯å®ƒå®Œæˆäº†ï¼Œè¿™å¾ˆå¥½ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šåœ¨è¿™é‡Œç»å†ä¸€ä¸ªå¾ªç¯ï¼Œè®¾ç½®æˆ‘ä»¬å¸Œæœ›ç»è¿‡çš„è¿­ä»£æ¬¡æ•°ï¼Œæ‰€ä»¥ä»ä¸Šé¢æ˜¯20000ï¼Œè¿™æ˜¯é€æ­¥æ„å»ºæˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®é›†ï¼Œä»¥ä¾¿æˆ‘ä»¬ç»§ç»­æ ¹æ®å®é™…çœ‹åˆ°çš„å¥–åŠ±è®­ç»ƒç¥ç»ç½‘ç»œã€‚å¦‚æœæˆ‘ä»¬è¾¾åˆ°æƒ³è¦çš„æ—¥å¿—é—´éš”ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç»§ç»­ã€‚
- en: And print out some progress informationã€‚ If we've hit the evaluation intervalã€‚
    then we evaluate it using that average return function that I showed you earlierã€‚
    And here you can see as it's trainingï¼Œ the average return tends to increase increase
    increase as it goesã€‚ I think 200 is the maximum it can getã€‚ Nowï¼Œ to see it actually
    runï¼Œ we've got the video codeã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶æ‰“å°å‡ºä¸€äº›è¿›åº¦ä¿¡æ¯ã€‚å¦‚æœæˆ‘ä»¬è¾¾åˆ°äº†è¯„ä¼°é—´éš”ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä½¿ç”¨æˆ‘ä¹‹å‰ç»™ä½ å±•ç¤ºçš„å¹³å‡å›æŠ¥å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥çœ‹åˆ°éšç€è®­ç»ƒï¼Œå¹³å‡å›æŠ¥è¶‹å‘äºä¸æ–­å¢åŠ ã€‚æˆ‘è®¤ä¸ºæœ€å¤§å¯ä»¥è¾¾åˆ°200ã€‚ç°åœ¨ï¼Œä¸ºäº†çœ‹åˆ°å®ƒå®é™…è¿è¡Œï¼Œæˆ‘ä»¬æœ‰è§†é¢‘ä»£ç ã€‚
- en: we've seen this beforeã€‚ So it increases increasesï¼Œ gets to just about its maximum
    and stopsã€‚ If you want to see it actually workï¼Œ you can see it is balancing that
    poll really quite wellã€‚ and yet moving it across the field of viewã€‚ it's harder
    than it looksã€‚ If you try to run this with just the random oneï¼Œ it dies quite
    rapidlyã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰è§è¿‡è¿™ä¸ªã€‚æ‰€ä»¥å®ƒé€æ¸å¢åŠ ï¼Œå‡ ä¹è¾¾åˆ°æœ€å¤§å€¼ååœæ­¢ã€‚å¦‚æœä½ æƒ³çœ‹åˆ°å®ƒå®é™…è¿ä½œï¼Œå¯ä»¥å‘ç°å®ƒåœ¨å¹³è¡¡é‚£ä¸ªæ†å­ï¼Œåšå¾—ç›¸å½“ä¸é”™ï¼ŒåŒæ—¶åˆåœ¨è§†é‡ä¸­ç§»åŠ¨ã€‚è¿™æ¯”çœ‹èµ·æ¥è¦éš¾å¾—å¤šã€‚å¦‚æœä½ å°è¯•ç”¨éšæœºæ–¹æ³•è¿è¡Œï¼Œå®ƒä¼šå¾ˆå¿«å¤±æ•ˆã€‚
- en: Once that center gravity is goneï¼Œ it stops because it's gonna fallã€‚ There's
    no reason to continueã€‚ All rightï¼Œ that is a basic introduction into Q learningã€‚ğŸ˜Šã€‚![](img/1a1a7c7d49eba0604d4b8360d28b107c_7.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦é‡å¿ƒæ¶ˆå¤±ï¼Œå®ƒå°±ä¼šåœæ­¢ï¼Œå› ä¸ºå®ƒä¼šä¸‹è½ã€‚æ²¡æœ‰ç†ç”±ç»§ç»­ã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯Qå­¦ä¹ çš„åŸºæœ¬ä»‹ç»ã€‚ğŸ˜Šã€‚![](img/1a1a7c7d49eba0604d4b8360d28b107c_7.png)
- en: Thank you for watching my video and the next part we're going to see how to
    apply the same technique to an Atari gameã€‚If you're interested in this sort of
    thingï¼Œ please subscribe to my channelï¼Œ Thank you very muchã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢è§‚çœ‹æˆ‘çš„è§†é¢‘ï¼Œæ¥ä¸‹æ¥çš„éƒ¨åˆ†æˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•å°†ç›¸åŒçš„æŠ€æœ¯åº”ç”¨äºä¸€æ¬¾Atariæ¸¸æˆã€‚å¦‚æœä½ å¯¹è¿™ç±»å†…å®¹æ„Ÿå…´è¶£ï¼Œè¯·è®¢é˜…æˆ‘çš„é¢‘é“ï¼Œéå¸¸æ„Ÿè°¢ã€‚
