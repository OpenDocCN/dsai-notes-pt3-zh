- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P64ÔºöL12.3- OpenAI Gym‰∏≠ÁöÑKeras
    Q-Learning - ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå this is Jeff HeatonÔºå Wee to applications of Deep neural networks with Washington
    University In this part„ÄÇ we're going to look at how to use a deep Q neural network„ÄÇ
    So a deep reinforcement neural network that allows the computer to learn to play
    a game„ÄÇ We're going to start with a relatively simple exampleÔºå we're going to
    see Polcart using TF agents„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And well then in the next partÔºå expand this to be more complex„ÄÇ We'll use an
    Atari game„ÄÇ And then following thatÔºå stillÔºå we will actually create sort of a
    financial simulation„ÄÇ So do something that's not even a video game to see all
    my videos about Cale neural networks and other AI topics„ÄÇ click the subscribe
    button and the bell next to it and select al to be notified of every new video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: now we're going to look at Q learning using the open AI gym„ÄÇ we're going to
    use Cars to do at this time„ÄÇ So we're going to use Tf agents„ÄÇ So we're actually
    using deep neural networks now to build up that Q table that you saw before„ÄÇ So
    essentially in Q learning you build up a table that has a list of every possible
    state of the world can be in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then the anticipated rewards of your next steps„ÄÇ Now I am going to run this
    in coabab because a GPU is useful for this„ÄÇ I am going to go ahead and choose„ÄÇüòä„ÄÇ![](img/1a1a7c7d49eba0604d4b8360d28b107c_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a1a7c7d49eba0604d4b8360d28b107c_2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/1a1a7c7d49eba0604d4b8360d28b107c_3.png)'
  prefs: []
  type: TYPE_IMG
- en: rununtime and run all because at least with the current version of this„ÄÇ hopefully
    this improves by the time that you run this„ÄÇ you'll get an error actually going
    through this with the current version2„ÄÇ2 of Tensorflow and some of the other things
    that they have installed in Google Coab„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So see it's running this part of the program here that just gets me the right
    version of Tensorflow that I want„ÄÇ This part here is doing some installs to put
    various software in that we need like TF agents and this other software lets me
    just capture the game being played as video so that I can play it in coabab you
    can use this on your own if you're running locally on your computer or you can
    leave this part out and it'll just pop up a window and show you how the game is
    being played so this is where you normally get this error and I want to explain
    this to you because this only started happening a few days ago when Google upgraded
    some stuff in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a1a7c7d49eba0604d4b8360d28b107c_5.png)'
  prefs: []
  type: TYPE_IMG
- en: Lb and TF agents as well„ÄÇ And if you get this error„ÄÇ basically and let me explain
    one other thing tooÔºå if you get an error like this„ÄÇ this error is not telling
    me a great deal„ÄÇ if you Google this„ÄÇ they're gonna basically just say to reinstall
    proto buff which is actually not what it takes So this took me about an hour And
    the way I figured this out was basically by going up here and running each of
    these in part and then seeing which of them was actually causing the error and
    when I ran it separately„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I actually got to the error that was really happening and it was basically saying
    that since I installed all those packages above there„ÄÇ I need to restart my runtime
    which I'm doing now And now I can basically do a runtime and run all and it will
    it'll make it through there just fine„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: s a little little trick there you might not need to do this if you don't get
    the error Don't worry about it„ÄÇ they fixed it since I recorded this video Now
    while this is running is this takes a little while„ÄÇAnd notice tooÔºå even though
    I restarted the environment„ÄÇ it's saying already newest version that was for my
    last install of this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if I go away from this for a few hours in coabab times out and takes my environment
    away„ÄÇ then I've got to rerun it from scratch„ÄÇ But even though I restarted the
    environment„ÄÇ my changes to the hard drive by installing these was still the same„ÄÇ
    So that that is kind of how that works„ÄÇ Now this should have made it through the
    part that was airing before„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: certainly hope so between Google Coab and Tensorflow„ÄÇ they like to send me back
    to the drawing board a lot„ÄÇ but that's part of part of what I do as far as making
    these videos is showing you how all this stuff works together and believe me machine
    learning changes at the speed of light code that I had working a week ago may
    or may not still be working„ÄÇ So that's why always make sure that you check my
    Github repository to get the latest version of these things„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So as we go through all of this code that I'll be explained in a moment It's
    training„ÄÇ So it has made it completely through„ÄÇ So I'm going to let„ÄÇGo ahead and
    train and run while I explain sort of what's going on here„ÄÇ These are all of the
    imports that you need„ÄÇ Just go ahead and use those„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: These are the hyperparameter„ÄÇ some of these are important„ÄÇ Some of them not
    so much number of iterations is very important„ÄÇ This is how long you're going
    to train„ÄÇ You're not really using early stopping in this capacity„ÄÇ you're you're
    simply picking the number of iterations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I did not set up this so that it's restartable that might be a very good modification
    to make for this because if you're doing something really complex„ÄÇ you may train
    it for 20000 apo and then realize I want to train it for 20000 more„ÄÇ This is very
    important„ÄÇ This will define the success or failure of your project you need enough„ÄÇ
    You need enough iterations„ÄÇ I collect steps„ÄÇ This is not that important„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: At least I've not found it to be„ÄÇ This is how many times you run through episodes
    to get some initial data„ÄÇTo train your neural network with collection steps per
    iteration„ÄÇ This is actually very important„ÄÇ For a simple one like this„ÄÇ I set
    it to oneÔºå but you might want to set this on the Atari„ÄÇ I think I set this to
    about 10„ÄÇ It takes it a lot longer to trainÔºå but you get much„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: much better resultsÔºå typically Collect steps per iteration and iterations itself
    really is the only downside of increasing those is going to make it's just going
    to take a long time to run into train„ÄÇ I wouldn't set this too far above 10„ÄÇ I
    set it usually somewhere between  one and 10„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is the size of your buffer„ÄÇ So essentially it's running through episodes
    and it's storing information from each game that it plays„ÄÇ initially it fills
    10 of them before it starts„ÄÇ This is just how many how many steps you're really
    willing to store in there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is the batch sizeÔºå which is pretty typical for machine learning„ÄÇ You might
    get better results with a little smaller„ÄÇ your learning rate„ÄÇ This is absolutely
    critical„ÄÇI typically spend most of my tuning time tuning the learning rate„ÄÇ the
    collections per iteration and the number of iterations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Basically you want this number as big as possible„ÄÇ if you set it too big then
    it's going to it's going to become unstable very quickly 10 to the negative third
    works really pretty well for this but if you find you' nuts not stabilizing„ÄÇ change
    this to negative4 negative5 so on so forth„ÄÇ but the bigger you make it like a
    negative3 or negative2 you probably won't get by with but in that range it's going
    to train faster it'll need fewer iterations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is how often we want a log report and this won't affect your training much„ÄÇ
    but it lets you know what's going on„ÄÇ the number of evaluation episodes So how
    many episodes how many complete runs of the game do you want your evaluation based
    on 1s a good number and then how often do you want to evaluate don't set this
    too low because it does take time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Do these evaluations we'll see in the Atari game I let it go much„ÄÇ much further
    before doing evaluations just to keep things efficient The environment that we're
    going to use„ÄÇ we're gonna to use cartpo that is where it's basically trying to
    move a cart that has a pole balanced on it and you don't want the pole to fall
    over so basically figuring out gravity and angular momentum of sort of thing I
    like this code here this lets you render what the environment looks like so that's
    a great way to visualize it there's the cart there's the pole the pole is connected
    by a hinge to the cart and I print out some basic data on it so you can see that
    the observation there's basically four values being observed which is basically
    the velocity of the cart the angle of the pole and the angular velocity of the
    pole and also the position of the pole that makes of the whole cart that makes
    the fourth one the reward is pretty simple is just just a number value based on
    how„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Long you've kept the pole upright and this describes sort of the values that
    are being kept„ÄÇ let's see„ÄÇI'll go ahead and mention this too„ÄÇ You'll see that
    we deal with T F pi environment and T F Tensorflow environment„ÄÇ There's two environment
    types that you can deal with„ÄÇ Usually you're dealing with Python environmentsÔºå
    but Tensorflow environments are written completely in Tensorflow„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that means you can literally compile that along with the entire execution
    tree of the neural network and they become very„ÄÇ very performant„ÄÇ HoweverÔºå usuallyÔºå
    at least in this caseÔºå the Atari games„ÄÇ All these things are written in Python„ÄÇ
    So or at least written in languages other than Tensorflow„ÄÇ So we almost have to
    use the Python environment„ÄÇ I found myself usually using the Python environment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I've not used the Tensorflow environments a lot„ÄÇ I bet you could get considerably
    faster„ÄÇüòä„ÄÇAccess there„ÄÇ although sometimes I just prefer to throw compute at it
    because the more stuff that I program that is Tensorflow specific„ÄÇ if I ever want
    to try something else like a pytorrch and swap it out„ÄÇ There's more legacy sort
    of stuffed tied to one particular platform„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: do it in Python lets me stay pretty agnostic„ÄÇ So just a few design considerations
    hereÔºå the agent„ÄÇ we are going to use a DqN for this one„ÄÇ DQN is great when your
    action space is boolean or is discrete„ÄÇ So you either in this caseÔºå you're either
    applying force in one direction or the other left or right„ÄÇ Tru or false„ÄÇ It's
    not how much force„ÄÇ If you do need to have a continuous or a numeric action space„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then I recommend DDPQ„ÄÇ I'm sorryÔºå DDPG„ÄÇ That is the one that we will be seeing
    not next part„ÄÇ but in the fifth part for this module„ÄÇBecause that lets us have
    continuous action space and for the financial simulation that I'm running„ÄÇ I want
    continuousÔºå I wanted to be able to tell me what percent of income should be invested
    in a particular direction so the agent we create the Q network„ÄÇ the Q network
    is the underlying deep neural network and then we place the Q network into the
    DQN network Most of this code for your own use„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can just take it in this form is it's adjusting to what the environment
    says the action and observation space is the policies this is used inside of the
    agent to determine what your what you're going to do next we're going to use a
    random policy here so that we are able to fill that initial buffer with with value
    so that the neural network can start training metrics and evaluation I like this
    compute average return I got it from the TF agent examples but this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Basically is looking at what was the reward return coming from the episode as
    it played out and summing that up„ÄÇ So this is a function really this entire carpool
    example comes from the TF agents examples and then I expand it to play an Atari
    game in the next part but this is basically just looking at what was their average
    return over a number of episodes in this case 10 I find this to be a good way
    to measure this because the world that the agent is operating is stochastic so
    there is some randomness going on there so each of these episode runs are not
    going to be the same and the average that it gets with those randomness gives
    you an idea of how good it's performing and you can see with this comment here
    that was in the T agents example TF agents does have a number of builtin metrics
    that you can use and I definitely encourage you to take a look at those we compute
    the average return for the random policy just showing that it works replay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: BufffferÔºå this is very important is fundamentally you're training a neural network
    through all of this and this lets you store the data that you're training the
    neural network on because the way the neural network is actually working is you
    have a state and that state in old school cu learning every possibility of the
    state can result in some action and we simply train the neural network for each
    of these actions that you might have predict what the Q value is what the possible
    reward is so you feed the neural network in the state and it returns to you all
    the rewards of the action Now this is using a table so the problem with the table
    is you have to have a row for every single possible state and you just look it
    up and you find it Now with a neural network the state is going into the input
    neurons and then the neural network through neural network magic predicts for
    you which action you should take so it gives you the Q values of all your possible„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And whichever action has the highest estimated Q valueÔºå you're going to use
    that one„ÄÇ typically as your action that you're going to choose„ÄÇ Now neural networks
    need to be trained„ÄÇ so that's what that replay buffer is for as you run through
    episodes„ÄÇ you take all of these states and you put it through here and you see
    what your reward actually was and that becomes that becomes the replay buffer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the training dataÔºå because you're going to actually see what the rewards were
    from what the neural network did and that adds additional rows in„ÄÇ So the neural
    network helps you get up to a certain state„ÄÇ But then that next state you let
    the neural network tell you what to do„ÄÇ but then the reward you train the neural
    network based on that reward so that it can predict what next step it should have
    taken So it's constantly getting that reinforcement the neural network is helping
    you get to these states further and further into the game but then you feed the
    reward actually from the video game did you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Score a point or did you get killed that all has to factor into those Q values
    And that's what you're teaching the neural network to do because this works better
    than having a table„ÄÇ You can't have a table for every possible combination„ÄÇ The
    neural network learns to generalize this theoretical gigantic high dimensionmenal
    table and basically predict for you those Q values„ÄÇ data collection„ÄÇ this is where
    you're generating those 1000 steps so that you get a random set of data So what
    we're basically doing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we we have a collect step that is being called by the main function collect
    data and this is right from the example and Tf agents What we're doing is we're
    getting we're calculating the time step So how far we want to project we're taking
    the action over that time step and then we are we are getting all the data from
    that step in the trajectory that it use to get to the next step of the action
    and adding that into the training batch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That is how we're slowly building one by one these rows that the neural network
    is actually learning to train on then we go through the number of requested steps
    This is actually a bug I found in the TF agents code I left it exactly like this
    but they do have a hyperparameter up higher where they let you specify the number
    of steps and down here they ignore it and just do100 I fix that actually in the
    Atari example in the next one because that becomes important and then we convert
    this into a data set and we're basically ready to go Now we're training the agent
    I kick this off earlier and fortunate it finished so that is good we are basically
    going through a loop here listen set up the number of iterations that we want
    to step it through so that's the 20000 from up higher it's one by one building
    the continuing to build the data set that we have so that we continue to train
    the neural network on it based on the rewards that we're actually seeing if we
    reach the log interval that we wanted then we go ahead„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And print out some progress information„ÄÇ If we've hit the evaluation interval„ÄÇ
    then we evaluate it using that average return function that I showed you earlier„ÄÇ
    And here you can see as it's trainingÔºå the average return tends to increase increase
    increase as it goes„ÄÇ I think 200 is the maximum it can get„ÄÇ NowÔºå to see it actually
    runÔºå we've got the video code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we've seen this before„ÄÇ So it increases increasesÔºå gets to just about its maximum
    and stops„ÄÇ If you want to see it actually workÔºå you can see it is balancing that
    poll really quite well„ÄÇ and yet moving it across the field of view„ÄÇ it's harder
    than it looks„ÄÇ If you try to run this with just the random oneÔºå it dies quite
    rapidly„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Once that center gravity is goneÔºå it stops because it's gonna fall„ÄÇ There's
    no reason to continue„ÄÇ All rightÔºå that is a basic introduction into Q learning„ÄÇüòä„ÄÇ![](img/1a1a7c7d49eba0604d4b8360d28b107c_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for watching my video and the next part we're going to see how to
    apply the same technique to an Atari game„ÄÇIf you're interested in this sort of
    thingÔºå please subscribe to my channelÔºå Thank you very much„ÄÇ
  prefs: []
  type: TYPE_NORMAL
