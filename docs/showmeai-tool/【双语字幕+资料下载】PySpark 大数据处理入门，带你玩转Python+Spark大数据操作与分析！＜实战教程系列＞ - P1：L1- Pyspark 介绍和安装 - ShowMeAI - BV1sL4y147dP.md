# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PySpark å¤§æ•°æ®å¤„ç†å…¥é—¨ï¼Œå¸¦ä½ ç©è½¬Python+Sparkå¤§æ•°æ®æ“ä½œä¸åˆ†æï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P1ï¼šL1- Pyspark ä»‹ç»å’Œå®‰è£… - ShowMeAI - BV1sL4y147dP

![](img/ee846c25d771f30e25168217f8991826_0.png)

Hello allã€‚ My name is Krisna and welcome to my YouTube channelã€‚ So guysã€‚

 we are going to start Apache Spark seriesï¼Œ and specifically if I talk about Sparkã€‚

 we will be focusing on how we can use Spk with Pythonã€‚

 So we are going to discuss about the library call Pi Spkã€‚ğŸ˜Šã€‚

We will try to understand everything why Spk is actually required and probably we also try to cover a lot of thingsã€‚

 There is something called as Mlib Spk Mlibï¼Œ which will basically say that how you can apply machine learningã€‚

 you know in Apache Spk itself with the help of this Spk API called as Pipark libraries and apart from that we'll also try to see in the future once we understand the basics of the Pipark libraryã€‚

 how we can actually preprocess our data setï¼Œ how we can use the Pipar data frames we'll also try to seeã€‚

How we can implement or how we can use Pipark in cloud platforms like data breaksï¼Œ Amazonï¼Œ Awsã€‚

 you knowï¼Œ so all these kind of clouds will try to cover and remember Apache spark is quite handyã€‚

 Let me tell you just let me just give you some of the reasons So why Aache spark is pretty much good because understandã€‚

 suppose if you have a huge amount of data Okay suppose if I say that I'm having 64 gb dataï¼Œ1ã€‚

8 gb dataï¼Œ you knowï¼Œ we may have some kind of systemsï¼Œ a standalone systemsï¼Œ you knowã€‚

 where we can have 32 gb of Ram probably 64 gb of Ram right now in the workstation that I'm working in it has 64 gb Ramã€‚

 So max to Macï¼Œ it can directly upload a data set of 32 gbã€‚

48 gb But what if if we have a data set of 1ã€‚8 g you know that is the time guysã€‚

 we don't just depend on a local system will try to preprocess that particular data perform any kind of operation in distributeã€‚

uted systems right distributeded system basically means that all there will be multiple systemsã€‚

 you knowï¼Œ where we can actually run this kind of jobs or process or try to do any kind of activities that we really want and definitely Apache Sp will actually help us to do that and this has been pretty much amazing And yesã€‚

 people wanted this kind of videos a lot So how we are going to go through this specific playlistes that will try to first of allã€‚

 start with the installationã€‚ We'll try to use Pipark because that is also Apache spark it is a spark API with Python when you are actually working with Python we basically use Pipark library And yesã€‚

 we can also use spark with other programming languages like Java scale and all right and will try to understand from basics you know from basic how do we read a dataã€‚

 how do we connect to a data source probably how do we play with the data frames know in this Apache Sp that is your Piã€‚

ğŸ˜Šï¼ŒAlso they provide you data structures like data framesã€‚

 which is pretty much similar to the pans data frameï¼Œ but yesã€‚

 different kind of operations are supported over there which we'll be looking one by one as we go ahead and then we'll try to enter into Mlib Apache Sp Mlib so basically it is called as spark Mlib which will actually help us to perform machine learning which where we'll be able to perform some machine learning algorithm task where we'll be able to do regression classification clustering and finally well try to see how we can actually do the same operation in cloud where I'll try to show you some examples where we will be having a huge data set we will try to do the operation in the clusters of system you know in a distributed system and we'll try to see how we can use spark in that right so all those things will basically get covered now some of the advantages of Apache Sp and white is very much famous because it runs workloads hundred multiplied byã€‚

times fasterï¼Œ you knowï¼Œ that basically means And if you know about big data guysã€‚

 when we talk about big dataï¼Œ we are basically talking about huge data setï¼Œ rightã€‚

 and there if you have heard of this terminology called as map produceï¼Œ rightã€‚

Trusush mean Apache Spk is much more faster 00 times faster than map produce alsoã€‚ okayã€‚

 and it is some of the more advantage that it is ease of useã€‚

 you can write application quickly in Java scalear Python or Rï¼Œ as I saidã€‚

 well be focusing on Python where we'll be using a library called Piparkã€‚

 Then you can also combine SQL streaming and complex analyticsã€‚ When I talk about complex analyticsã€‚

 I'm basically talking about this Mlib machine learning libraries that will work definitely well with the Apache Spkã€‚

And Apache sparks can run on Hadoop Apache meso Knet standalone in or in the clouds cloudã€‚

 different types of cloud guys when I talk about Aws data breaksï¼Œ all these thingsã€‚

 we can definitely workã€‚ and it actually runs in a cluster cluster basically means in a distributed So these are some of the examplesã€‚

 Now if I go with respect to which version ofpar spark will be usingpar 3ã€‚1ã€‚

1 will be using will try to work and if you just go and search for here you can see SQL and data frames and all here you can see spark streaming machineli that is called as machine learning in all And apart from that if I go and see the overview here you can see that Apache spark is a fast and general purpose cluster computing systemã€‚

 it provides highle APIs in scale or Java and Python that makes parallel job easy to write an optimized engine that supports general competition graphã€‚

 So it is basically to work with huge amount of dataã€‚ğŸ˜Šï¼ŒAnd shortï¼Œ you knowã€‚

 and that is pretty much handyã€‚ will'll try to workã€‚ Nowï¼Œ if I go and search for this park in Pythonã€‚

 you knowï¼Œ this page will get basically go open and this things will try to discuss how to install itã€‚

 And in this video will try to install the Pipar libraryã€‚

 if I talk about Pipar library you'll be able to see that Pipar library is pretty much amazingã€‚

 this library is if you really want to work if you want to work this spark functionality with Pythonã€‚

 you basically use the specific library and let's proceed and let's try to see that how we can quicklyã€‚

ğŸ˜Šï¼ŒYeahï¼Œ how we can quickly you know install the specific libraries and check out like what all things we can actually doã€‚

 Okayï¼Œ so all these things we'll try to seeã€‚ So let's beginã€‚

 please make sure that you create a new environment when you are working with Pi So I have created a new environment called my and hereã€‚

 first of allï¼Œ I'll try to install the Pi librariesã€‚

 So I'll just install Pipar and let's see in this we'll focus on installationã€‚

 we'll focus on reading some data sets and try to see that what all things we can actually doã€‚

 Okay and after doing this what we can actually do is that you can see that our Pi has been installed in order to check whether the installation is perfect or not I'll just write import Pi So this this looks perfectly find it is working know we are able to see that the Pi is basically installed properly Now you may be facing some kind of problemsã€‚

 is with respect to Pipar So that is the reason why I'm telling you create a new environment If you' are facing some kindã€‚

ğŸ˜Šï¼ŒIsueï¼Œ just let me know what is the error that you are getting probably writing in the comment sectionã€‚

 Okayï¼Œ now let's do one thingã€‚ I'll just open Excel sheet Okayã€‚

 and probably I'll just try to create aã€‚

![](img/ee846c25d771f30e25168217f8991826_2.png)

Some data setsï¼Œ I'll say nameï¼Œ probablyï¼Œ I'll just say nameã€‚And Hï¼Œ rightã€‚

 And suppose my name over here that I'm going to write is squ and also 31ã€‚ I'm going to say Suan Sã€‚

Rightï¼Œ Shoan Shuã€‚ I will just sayï¼Œ okayï¼Œ 30ã€‚ and probablyã€‚

 I'll just write some more names like Sunnyã€‚ Probably I'll also give the data as 29ã€‚

 So this three data will just try to see how we can read this specific fileã€‚ Okayã€‚

 I'm just going to save itã€‚Let's seeã€‚ I'll save it in the same location where my Jupiter notebook is guysã€‚

 E I created a folderï¼Œ I guessã€‚You can save it in any location where your notebook file is openã€‚

 rightï¼Œ So it is not necessary and just making sure that you don't see any of my filesã€‚Okayã€‚

 and I'm just saving itã€‚ Okayï¼Œ I'm saving it as test 1ã€‚ Here you can seeã€‚

 I'm saving it as test 1 dot C SVã€‚ So I'll save itã€‚ Let's keep this particular file savedã€‚ Okayï¼Œ nowã€‚

 if I probably want toï¼Œ you knowã€‚ğŸ˜Šã€‚

![](img/ee846c25d771f30e25168217f8991826_4.png)

Read with the pandaã€‚ So what we writeï¼Œ we write P D dot read underscore CSVï¼Œ rightã€‚

 And I basically use this particular data sets called asã€‚testest1ã€‚Dot CS Svã€‚

 right So when I am executing this hereï¼Œ you will be able to see this specific informationã€‚ Nowã€‚

 when I really want to work with Piparï¼Œ alwaysï¼Œ first of allï¼Œ rememberã€‚

 we need to start a spark sessionã€‚ And in order to start a spark sessionã€‚ first of allã€‚

 let me create some more fieldsã€‚ Just see thisã€‚ Just follow this particular steps with respect to creating a pass sessionã€‚

 So Ill write from Piparã€‚ğŸ˜Šï¼ŒDot sqLã€‚Importã€‚Spark sessionã€‚ Okayï¼Œ and then I'll execute thisã€‚

 You can see that it is exhibiting fineã€‚ Then I'll writeï¼Œ sorryã€‚I don't know what has openedã€‚

 so Ill Ill create a variable called a spark and probably Ill use the spark session dot builderã€‚

And I'll say app name and hereï¼Œ I'll just give my session nameã€‚ Okayï¼Œ so it'll be like practiceã€‚

 suppose I'm practicing these thingsã€‚ And then I can say get or createã€‚

 So when I actually execute thisï¼Œ you'll be able to see a spark session will get createdã€‚

 And if you' are executing for the first timeï¼Œ itll probably take some amount of time other than that if I executed multiple timesã€‚

 then you'll be able to work it now here you can definitely see that in this when you're executing in a localã€‚

 therell always be only one clusterã€‚ But when you are actually working in a cloudã€‚

 you can create multiple clusters and instancesã€‚ Okayã€‚

 so the spark version that you'll be using is v 3ã€‚1ã€‚

1 here you can see that this is basically present in the master when probably you'll be working in multiple instancesã€‚

 there you'll be seeing masters and cluster one cluster to all those kind of information Okay so this is with respect to Sp Now let's I'll just write Df of pipar where I will try to read a data setã€‚

ğŸ˜Šï¼ŒWith respect to sparkã€‚ Okayï¼Œ now in order to read a reader set what I can writeã€‚

 I can write like the spark dotã€‚Readï¼Œ dotã€‚There is a lot of options like CSV formatã€‚

 JDBC parque scheme or table textï¼Œ lot of options thereã€‚

 So here we are going to take CSV and here Im just going to write tips1ã€‚Teps 1ï¼Œ do Cï¼Œ rightã€‚

 And if I just try to execute it hereï¼Œ I'm getting some error saying that this particular file does not existã€‚

 Let me seeã€‚I think this file is presentã€‚å—¯å—¯å—¯ã€‚Just let me see guysã€‚ why this is not getting executedã€‚

 Ti 1ã€‚D F file openã€‚ here I can see test 1 dot C Vã€‚ Okayï¼Œ sorryï¼Œ I did not write that C SV fileã€‚

 I guess test 1 dot C SVã€‚Okayï¼Œ this has now workã€‚ Nowï¼Œ if I go and see D dots Piparã€‚

 it is showing these two stringsï¼Œ right this two column C0 and C1ã€‚ Now here you can see that guysã€‚

 I've created this particular Cv file right and it is just taking this A B as a default column probably So it is saying C0 and C1 So what we can actually do is that and probably if you really want to see your entire data setã€‚

 you can basically see like this Df underscore Pipar dot show here we'll be able to see name and age this this information I really want to make my column name or age as my main column right But when I'm directly reading this Cv fileã€‚

 probably we are getting underscore c0 underscore C1ã€‚

 So in order to solve this what I will do is that we have a different technique So I'll write spark dot read dot option there is something called as option and inside this option what you can basically give is that there'll be an option with respect toã€‚

ğŸ˜Šï¼ŒHeaderï¼Œ like I seeï¼Œ there will be something like key value that you will be providing in optionã€‚

 So what you can doï¼Œ you can just write headerã€‚Commer trueã€‚ So whatever valueã€‚

 the first column first row value will be thereï¼Œ that will be considered as your headerã€‚

 And if I write Csv with respect to test1ã€‚ nowï¼Œ I'm just going to read this test1 dataset setã€‚

Test 1 dot CS Svã€‚ Nowï¼Œ once I execute this hereï¼Œ you'll be able to see that I am able to get now name string H stringã€‚

 Okayï¼Œ but let's see our complete data setã€‚ So here if I execute this nowã€‚

 I'll be able to see the entire data set with this particular columnsã€‚ Okayã€‚

 so let me just quickly save this in my Df underscore Pi sparkã€‚ğŸ˜Šï¼ŒAnd nowã€‚

 let's go and see the type ofã€‚D F underscore pi spaï¼Œ okayã€‚Nowï¼Œ when I execute this hereã€‚

 you'll be able to see guys when I was reading this Df right when I wasã€‚

 if I go and see the type of this with the help of pandas here you'll be able to see that there is pandas dot code do frame do data frameã€‚

 But here you'll be seeing that when you are reading this particular data setã€‚

 it is of type Pipar do sQl do data frame do data frameã€‚ Yesã€‚

 so that is pandas data frame is sQL dot data frame data frameã€‚ Yesã€‚

 most of the Apis are almost sameã€‚ The functionalities are same There a lot of things that we will be learning as we go aheadã€‚

 butã€‚ğŸ˜Šï¼ŒIf I quickly want to see myã€‚Probablyï¼Œ I don't know whether head will workã€‚ Let's seeã€‚ Yesã€‚

 head is also workingã€‚ So if I use dot headï¼Œ probably you'll be able to see the rose information I basically shown over hereã€‚

 Nowï¼Œ if I really want to see the more information regarding my columns I will be able to use something called as print schema now in this this print schema is just like Df dot infoã€‚

 which will actually tell about your columns like name is string and age string Okay so all these are some basic operations that you have actually done after installer againã€‚

 the main thing why I'm focusing on this is that just try to install this spice spark and keep it ready for my next session I will be trying to show you how we can change the data type how we can work with data framesã€‚

 how we can actually do data precrossingï¼Œ how we can handle null valuesï¼Œ missing valuesã€‚

 how we can delete the columnsï¼Œ how we can do various thingsã€‚

 all those things will basically really discussing over thereï¼Œ how to drop columns and So I hopeã€‚ğŸ˜Šã€‚



![](img/ee846c25d771f30e25168217f8991826_6.png)

Like this particular videoã€‚ So this is just my pie spark introductionï¼Œ okayã€‚

And we will continue the next sessionã€‚ Probablyã€‚ I'll also give you this information in my Github where we will probablyã€‚

 okayï¼Œ Pi Sp is already thereã€‚

![](img/ee846c25d771f30e25168217f8991826_8.png)

æˆ‘ã€‚Basic introductionï¼Œ fineï¼Œ So we will try to do this and we'll try to cover this a entire thing as we go ahead in the next sessionã€‚

 Rememberï¼Œ guysï¼Œ againï¼Œ our main aim is basically to make you understand how probably well be working in cloudsã€‚

 And before thatï¼Œ we really need to know all the basic stuffs that we need to understand regarding Piparã€‚

 But yesï¼Œ it is an amazing library itll actually help us to implement all the spark APIpis functionalityities that are that basically supports with Pythonã€‚

 So everything all the things that are related to machine learning we can also do with the help of Piã€‚

 So I hope you like this particular videoll see all in the next video have a great dayã€‚

 Thank you on dollã€‚ğŸ˜Šã€‚

![](img/ee846c25d771f30e25168217f8991826_10.png)