- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P7ÔºöL7- ÂáΩÊï∞Âºè API ÁöÑÊõ¥Ê∑±ÂÖ•Á§∫‰æã
    - ShowMeAI - BV1em4y1U7ib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/2b04a13feb7a7ead68512f94b99b8884_0.png)'
  prefs: []
  type: TYPE_IMG
- en: What's going on guysÔºå hope you're doing awesome and welcome back for another
    Tensorflowlow tutorial so I feel a little bit bad because so far we've been using
    both the sequential and the functional API but really for the examples that I've
    given that really wouldn't be a point to use the functional API so I want to give
    you a more real example where you can't actually use just a sequential API and
    so we're taking another look at MNist but with a little twist we now have two
    MIS digits per example for example here we have the digit zero and one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I'm going have a link in the description for you to download this so that
    you can also follow along with the video„ÄÇ but anyways we're not gonna to actually
    focus on on the custom data„ÄÇ so loading the actual data that's going to be for
    a future separate video for now I just want to give you a little bit more an actual
    example for when the functional API becomes useful so let's get to the code and
    what we have right here is just the basic imports that we've been using so we
    also have one more which is pandas and this is going to be used to load the data
    set so you can just do Conda install pandas I believe and then you'll have this
    and so as I said we're not gonna to focus on the actual data loading part meaning
    I'm going to copy paste and stuff here and I don't like copy pasting stuff but
    we're not going to focus on on that part in this video so you can also there's
    going to be a link in the description where you can copy paste this code as well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b04a13feb7a7ead68512f94b99b8884_2.png)'
  prefs: []
  type: TYPE_IMG
- en: I'm just going to paste that and basically right here this is for loading the
    actual data all right so we're using pandas to read from a CSV file and then we're
    using Tf do data and again I'm going to cover this in a separate video to actually
    load the data So what I do want to focus on is that now we have two target values
    for each example meaning we can't use the sequential because remember sequential
    can only map one input to one output but now we actually have two outputs so we're
    going to build a model and remember we have to use the functional now so we're
    going to do start with Ks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: input we're going to specify the input shape and in this case they are 64 by
    64 pixels and then they are one channel just because they are grayscale and then
    we're going to do a comm layer comm 2D and we're going to specify filters let's
    say 32 we're going specify the current„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Size let's say3 just for keeping it simple„ÄÇ and then yeah„ÄÇ I guess we can do
    padding equals same and then we're going to also do regularization„ÄÇ So let's actually
    go to the top here and I also want to specify just some hyperparameter„ÄÇ so let's
    do hyperparameters„ÄÇAnd let's specify the batch size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let's do 64 and let's specify weight decay to be 0„ÄÇ001„ÄÇ so for L2 normalization
    that we did two videos agoÔºå I believe„ÄÇ and then let's specify the learning rate
    0„ÄÇ001„ÄÇAlrightÔºå so let's go back now to our model„ÄÇ So we're going to do kernel
    regularizer„ÄÇ I'm going to T cart„ÄÇActuallyÔºå we can„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can also import that„ÄÇ So let's do layers„ÄÇInput regulars„ÄÇAnd thenÔºå go back„ÄÇüòî„ÄÇAnd
    then regularizers dot L2Ôºå and then of weight decay„ÄÇAll right„ÄÇAnd then we got a
    of inputs right there„ÄÇNow let's do a batch layer„ÄÇ So layers batch normalization
    of X„ÄÇ So so far nothing is newÔºå rightÔºå we've done all of this before„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And there's not that much differentÔºå but I just wanted to give you sort of a
    more in depth example where you would actually use this„ÄÇ So the difference is
    going to be when when we get to the output„ÄÇBut anyways„ÄÇ then let's you do kas
    that activationÔºå thatt re„ÄÇOf xÔºå then let's do another comm to D„ÄÇto the„ÄÇOf„ÄÇ I don't
    knowÔºå let's say„ÄÇ64Ôºå3Ôºå and then kernel regularizer equal regulars dot L2„ÄÇOf weight
    decay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then let's also send in the input X on that„ÄÇ and again„ÄÇ we're going to do
    just a batch normalizationÔºå send in xÔºå and then Kaos activations that relu„ÄÇOf
    x„ÄÇAnd let's do a max pooling right here„ÄÇSo„ÄÇI don't know„ÄÇ Let's do another comlay„ÄÇCom
    to the„ÄÇLet's just specify 64Ôºå3„ÄÇThen let's just do relu„ÄÇ So no bathroom here and
    then kernel regularizer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Caras just regulars that L2 of weight„ÄÇOkay„ÄÇAnd then„ÄÇYeahÔºå we can do one more
    so come to„ÄÇTo D 128 filters nowÔºå so let's double itÔºå3 activation equals ReluÔºå
    then just send in x„ÄÇAnd then we can do one max max pooling and then„ÄÇLet's flatten
    it„ÄÇ and then let's now get to the output„ÄÇSo what we're going to do is we're going
    to do one dense layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so layer denÔºå it's do 128 nodes„ÄÇAnd let's set it activation equals value on
    that and sending next„ÄÇAnd„ÄÇLet's also add some regularization so let's do layers
    drop out 0„ÄÇ5 and then send in X„ÄÇAnd then„ÄÇYeahÔºå we can actually do one more layer
    then so let's do layer down 64 activation equals rather of x„ÄÇ And now we get to
    the actual„ÄÇ So now we get to the actual output„ÄÇ So what we want to do now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's do output one and let's do a dense layer„ÄÇYou going do 10 nodesÔºå and then„ÄÇLet's
    call it„ÄÇ let's give it a name„ÄÇ So it's our first number„ÄÇAnd then we're going to
    send in xÔºå and then output 2„ÄÇ we're going to do layer den of 10 name„ÄÇSecond number„ÄÇ
    and then we're gonna send in x„ÄÇ So as you can see here we we're using the input
    X to map to two different outputs right so this would essentially give two different
    branches of this thing right here and this is when things like the functional
    API becomes useful right just these two lines right here because we so the sequential
    is nice„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's very simpleÔºå it's very compact but it can't do things like this and that's„ÄÇThat's
    this narrow when we actually have to use the functional and you could also combine
    them„ÄÇ So for example you couldÔºå you could do this right here in in a sequential
    and then you could just do these two on the functional„ÄÇ So you could you can also
    combine the two„ÄÇ but anyways„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm just gonna so this is just for two outputs and then I'm gonna do model„ÄÇ![](img/2b04a13feb7a7ead68512f94b99b8884_4.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b04a13feb7a7ead68512f94b99b8884_5.png)'
  prefs: []
  type: TYPE_IMG
- en: modelel was cares that model input equals input outputs is now a list and we're
    going to do output 1„ÄÇ outputs 2„ÄÇAnd then when we do model that„ÄÇüòîÔºåCompile„ÄÇWe're
    going to set the optimizer„ÄÇ let's do optimizers do Adam„ÄÇAnd let's set learning
    rate here„ÄÇAnd then we're going to specify the loss„ÄÇ and then the loss is actually
    going to be two losses„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to do lossesÔºå sparse„ÄÇ We're going to use sparse categorical„ÄÇPross
    entropy„ÄÇ and then from largeÔºå it equals true„ÄÇOr actuallyÔºå let's remove that this
    time„ÄÇ So let's do„ÄÇWe can specify here activation equals softftmax„ÄÇLike that„ÄÇ So
    that then we don't need to do from Loit„ÄÇSo activation softmax„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we're going to do one more loss function„ÄÇAnd I think there's a way
    to make this more compact„ÄÇ I haven't actually tried itÔºå but I think that if you
    would just specify this„ÄÇ that would be for both„ÄÇ but you could try that out„ÄÇ So
    the safe way is just to write them all up So we're going to use spars„ÄÇCategorical
    cross entropy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For both„ÄÇAnd then we want to keep track of metrics equals„ÄÇAccuracy„ÄÇAnd now as
    usual we're going to do model do fit in this case„ÄÇ we're just going to send in
    train data and batch size and everything is taking care inside this data loading
    part which I haven't covered again„ÄÇ but we don't care too much about that then
    let's do epoch 5 both equals2 and then model dot evaluate„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate on the on the test data set„ÄÇAnd then verbose equals2„ÄÇAnd yeah„ÄÇ so let's
    run this and let's see what kind of accuracy we get when we're training on this
    multi digigit Mist if we do not get the errors„ÄÇ so we„ÄÇCars has no attribute„ÄÇüòîÔºåActivationÔºå
    so we' got to do activationsÔºå I believe„ÄÇ so let's find that errorÔºå let's see„ÄÇActivations„ÄÇGo
    back„ÄÇüòîÔºåThere we go activations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So activations right there„ÄÇ And let's hope there's nowhere probably„ÄÇThere's
    going be more„ÄÇAll right„ÄÇ so I've probably forgot to send it in right here„ÄÇüòîÔºåAre
    there any more errorsÔºå let's see„ÄÇI cannot find anymoreÔºå so let's run it again„ÄÇAlrightÔºå
    outputs  one is outputs once„ÄÇ Yeah„ÄÇ we can do outputs„ÄÇ So I call it output„ÄÇLike
    that„ÄÇNowÔºå please work„ÄÇ got them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: No data provided for first„ÄÇNumber„ÄÇAlrightÔºå so after a very long timeÔºå I think
    I found the error„ÄÇ So the error was no data provided for first number„ÄÇ knee data
    for each key in first number and second number„ÄÇ AlrightÔºå and surprisingly„ÄÇ I have
    no idea whyÔºå but if I just remove„ÄÇThis so that it says first nu and second nu„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then if I rerun it„ÄÇIt actually starts training without an error and„ÄÇI have no
    ideaÔºå actually„ÄÇ why that is„ÄÇ If you knowÔºå thenÔºå please do comment becauseuse I'm
    very surprised by that„ÄÇ Probably one of the weirdest errors I've had„ÄÇAfter five
    entire epochs we have 96„ÄÇ3% on the training on both pretty much both of the digits
    and that's a little bit interesting to see that when it start to recognize one
    of the digits it also starts to recognize the other one so they improve pretty
    much on equal level and then so if you would train this for longer I would suspect
    that you would improve this quite a bit but then on the test set anyways„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we get about 90% but then on the second value would get 83 so I guess on the
    test set might be more difficult to recognize the second number but anyways one
    thing I also wanted to check is that what if we remove and we just have a single
    loss function it would be nice if this would then extend to both of them so let's
    try and see if that works and that does seem to work all right so that's it for
    this video and taking a deep„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: look at the Fun API with a more real exampleÔºå thank you so much for watching
    the video and I hope to see you in the next one„ÄÇ![](img/2b04a13feb7a7ead68512f94b99b8884_7.png)
  prefs: []
  type: TYPE_NORMAL
