- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P4ï¼šL4-
    PyTorch åˆ†å¸ƒå¼ RPC - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P4ï¼šL4-
    PyTorch åˆ†å¸ƒå¼ RPC - ShowMeAI - BV1ZZ4y1U7dg
- en: ğŸ¼ã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_1.png)
- en: Helloï¼Œ everyoneã€‚ My name is Shen Liã€‚I am a research scientist in the P teamã€‚Todayã€‚
    I will talk about Py distributed package and will'll focus on RPCã€‚Which is a new
    feature that we introduced this yearã€‚I will first briefly explain what is PC and
    then go through some basic RC APIsã€‚After thatã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæˆ‘å«æ²ˆä¸½ã€‚æˆ‘æ˜¯På›¢é˜Ÿçš„ç ”ç©¶ç§‘å­¦å®¶ã€‚ä»Šå¤©ï¼Œæˆ‘å°†è®¨è®ºPyåˆ†å¸ƒå¼åŒ…ï¼Œå¹¶é‡ç‚¹è®²è§£RPCã€‚è¿™æ˜¯æˆ‘ä»¬ä»Šå¹´å¼•å…¥çš„æ–°ç‰¹æ€§ã€‚æˆ‘å°†é¦–å…ˆç®€è¦è§£é‡Šä»€ä¹ˆæ˜¯RPCï¼Œç„¶åä»‹ç»ä¸€äº›åŸºæœ¬çš„RPC
    APIã€‚ä¹‹åã€‚
- en: I will present high level ideas of how to implement different training applications
    using RPRPCã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_3.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨RPCå®ç°ä¸åŒè®­ç»ƒåº”ç”¨çš„é«˜çº§æ€è·¯ã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_3.png)
- en: Allrightï¼Œ so what is PythalRPCï¼Œ PythalRPC is a low level framework for general
    distributed trainingã€‚Before RPC distributeded data parallelel or DDP has been
    the main feature that Py coach offered for distributed trainingã€‚Howeverï¼Œ GDPDP
    is one specific training paradigm where every process has a full replica of the
    modelã€‚ and you split the input across multiple processesã€‚It works for many applicationsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œé‚£ä¹ˆä»€ä¹ˆæ˜¯PyRPCï¼ŸPyRPCæ˜¯ä¸€ä¸ªé€šç”¨åˆ†å¸ƒå¼è®­ç»ƒçš„ä½çº§æ¡†æ¶ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ä¸€ç›´æ˜¯PyTorchä¸ºåˆ†å¸ƒå¼è®­ç»ƒæä¾›çš„ä¸»è¦ç‰¹æ€§ã€‚ç„¶è€Œï¼ŒDDPæ˜¯ä¸€ç§ç‰¹å®šçš„è®­ç»ƒèŒƒå¼ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰æ¨¡å‹çš„å®Œæ•´å‰¯æœ¬ï¼Œå¹¶å°†è¾“å…¥åˆ†é…åˆ°å¤šä¸ªè¿›ç¨‹ã€‚è¿™ç§æ–¹å¼é€‚ç”¨äºè®¸å¤šåº”ç”¨ã€‚
- en: but there are also many other applications that do not fit into GDPPã€‚For exampleã€‚
    if your model is too large for one machineï¼Œ you will need multi machine model
    parallelismã€‚Which does not work with DPã€‚ You will have to manually send intermediate
    outputs and ingredients across processes and also resume all required in application
    codeã€‚Which can become very complicatedã€‚Another example is that sometimes we need
    to mix parameter server with DPã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–åº”ç”¨ä¸é€‚ç”¨äºDDPã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„æ¨¡å‹å¤ªå¤§ï¼Œæ— æ³•æ”¾å…¥ä¸€å°æœºå™¨ä¸­ï¼Œä½ å°†éœ€è¦å¤šæœºæ¨¡å‹å¹¶è¡Œã€‚è¿™ä¸æ•°æ®å¹¶è¡Œä¸å…¼å®¹ã€‚ä½ éœ€è¦æ‰‹åŠ¨åœ¨è¿›ç¨‹é—´å‘é€ä¸­é—´è¾“å‡ºå’Œå‚æ•°ï¼Œå¹¶åœ¨åº”ç”¨ä»£ç ä¸­æ¢å¤æ‰€æœ‰å¿…éœ€çš„å†…å®¹ï¼Œè¿™å¯èƒ½å˜å¾—éå¸¸å¤æ‚ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦å°†å‚æ•°æœåŠ¡å™¨ä¸æ•°æ®å¹¶è¡Œç»“åˆä½¿ç”¨ã€‚
- en: using parameter servers for sparse sensors and DP for dense sensorsã€‚This cannot
    be easily done with just DDPã€‚One option to solve this problem is by adding a different
    feature to support each of these use casesã€‚Neverthelessï¼Œ if we did thatï¼Œ it will
    create a large API surfaceã€‚ which can become a hurdle for users to pick upã€‚And
    it will also create a high engineering overhead to write and maintainã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¨€ç–ä¼ æ„Ÿå™¨ä½¿ç”¨å‚æ•°æœåŠ¡å™¨ï¼Œå¯¹äºå¯†é›†ä¼ æ„Ÿå™¨ä½¿ç”¨æ•°æ®å¹¶è¡Œã€‚ä»…ç”¨DDPæ— æ³•è½»æ¾å®ç°è¿™ä¸€ç‚¹ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§é€‰æ‹©æ˜¯ä¸ºæ¯ç§ç”¨ä¾‹æ·»åŠ ä¸åŒçš„ç‰¹æ€§ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è¿™æ ·åšï¼Œå°†ä¼šåˆ›å»ºä¸€ä¸ªåºå¤§çš„APIè¡¨é¢ï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºç”¨æˆ·ä¸Šæ‰‹çš„éšœç¢ã€‚åŒæ—¶ï¼Œä¹Ÿä¼šå¢åŠ ç¼–å†™å’Œç»´æŠ¤çš„å·¥ç¨‹å¼€é”€ã€‚
- en: Moreoverï¼Œ we cannot fully anticipate what new training paradigms might emerge
    in the futureã€‚Another option is that we can provide a set of flexible low level
    toolsã€‚ which would allow users to improviseã€‚So here comes Pwach RPRPCã€‚We hope
    it can fill the gaps for the distributed training applications that cannot use
    our GDPDPã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬æ— æ³•å®Œå…¨é¢„æµ‹æœªæ¥å¯èƒ½ä¼šå‡ºç°å“ªäº›æ–°çš„è®­ç»ƒèŒƒå¼ã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯æä¾›ä¸€ç»„çµæ´»çš„ä½çº§å·¥å…·ï¼Œå…è®¸ç”¨æˆ·è¿›è¡Œå³å…´åˆ›ä½œã€‚æ‰€ä»¥è¿™é‡Œæ¨å‡ºäº†PyRPCã€‚æˆ‘ä»¬å¸Œæœ›å®ƒèƒ½å¤Ÿå¡«è¡¥é‚£äº›æ— æ³•ä½¿ç”¨DDPçš„åˆ†å¸ƒå¼è®­ç»ƒåº”ç”¨çš„ç©ºç™½ã€‚
- en: And if some new distributed training paradigm becomes widely adoptedã€‚ we always
    have the option to introduce a dedicated high level API for itã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_5.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰æ–°çš„åˆ†å¸ƒå¼è®­ç»ƒèŒƒå¼è¢«å¹¿æ³›é‡‡ç”¨ï¼Œæˆ‘ä»¬æ€»æ˜¯å¯ä»¥é€‰æ‹©ä¸ºå…¶å¼•å…¥ä¸€ä¸ªä¸“ç”¨çš„é«˜çº§APIã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_5.png)
- en: So now we know why we introduce RPRPCã€‚Then what features do RC offerã€‚S up 1ã€‚7ã€‚
    theres already a long list of features in IP PC packageã€‚But I will only focus
    on the most basic parts in today's talkã€‚The first one is remote executionã€‚This
    allows you to run a user function on a specific remote processã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“ä¸ºä»€ä¹ˆè¦å¼•å…¥RPCã€‚é‚£ä¹ˆï¼ŒRPCæä¾›äº†ä»€ä¹ˆç‰¹æ€§å‘¢ï¼Ÿåœ¨1.7ç‰ˆæœ¬ä¸­ï¼ŒIP RPCåŒ…ä¸­å·²ç»æœ‰ä¸€é•¿ä¸²ç‰¹æ€§ã€‚ä½†æˆ‘ä»Šå¤©çš„è®²åº§åªä¼šå…³æ³¨æœ€åŸºæœ¬çš„éƒ¨åˆ†ã€‚ç¬¬ä¸€ä¸ªæ˜¯è¿œç¨‹æ‰§è¡Œã€‚è¿™å…è®¸ä½ åœ¨ç‰¹å®šçš„è¿œç¨‹è¿›ç¨‹ä¸Šè¿è¡Œç”¨æˆ·å‡½æ•°ã€‚
- en: Which is what you would expect from any RPC systemã€‚The second one is remote
    referenceã€‚Which allows you to reference the remote object and pass it aroundã€‚Using
    remote execution without transmitting the real dataã€‚It serves as a discreted ShaPoerã€‚The
    third one is a distributed daughter gradã€‚It will stitch together local autograd
    graphs of remote executions into one distributed autogra graphã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯ä½ æœŸæœ›ä»ä»»ä½• RPC ç³»ç»Ÿä¸­è·å¾—çš„ã€‚ç¬¬äºŒä¸ªæ˜¯è¿œç¨‹å¼•ç”¨ï¼Œå…è®¸ä½ å¼•ç”¨è¿œç¨‹å¯¹è±¡å¹¶ä¼ é€’å®ƒã€‚ä½¿ç”¨è¿œç¨‹æ‰§è¡Œè€Œä¸ä¼ è¾“å®é™…æ•°æ®ã€‚å®ƒå……å½“ç¦»æ•£çš„å…±äº«æŒ‡é’ˆã€‚ç¬¬ä¸‰ä¸ªæ˜¯åˆ†å¸ƒå¼è‡ªåŠ¨æ¢¯åº¦ã€‚å®ƒå°†è¿œç¨‹æ‰§è¡Œçš„æœ¬åœ°è‡ªåŠ¨æ¢¯åº¦å›¾æ‹¼æ¥æˆä¸€ä¸ªåˆ†å¸ƒå¼è‡ªåŠ¨å›¾ã€‚
- en: so that when you call backward on the loss tensor in one processã€‚ it will reach
    out to all participating processes and machines and compute gradients accordinglyã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_7.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œå½“ä½ åœ¨ä¸€ä¸ªè¿›ç¨‹ä¸­å¯¹æŸå¤±å¼ é‡è°ƒç”¨ backward æ—¶ï¼Œå®ƒä¼šè”ç³»æ‰€æœ‰å‚ä¸çš„è¿›ç¨‹å’Œæœºå™¨ï¼Œå¹¶ç›¸åº”åœ°è®¡ç®—æ¢¯åº¦ã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_7.png)
- en: That's a very high level explanationã€‚ Let's now dive into more detailsã€‚Remote
    executionã€‚It allows running a carable remotelyã€‚In Part sharp Cï¼Œ all processes
    areã€‚Any process can be both color and collyã€‚After initialization by calling in
    RRPCã€‚ any process will have a message listener running in the background and use
    a thread pool to process requests and responsesã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªéå¸¸é«˜å±‚æ¬¡çš„è§£é‡Šã€‚ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥äº†è§£æ›´å¤šç»†èŠ‚ã€‚è¿œç¨‹æ‰§è¡Œã€‚å®ƒå…è®¸åœ¨è¿œç¨‹è¿è¡Œå¯è°ƒç”¨å¯¹è±¡ã€‚åœ¨ Part sharp C ä¸­ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½æ˜¯çš„ã€‚ä»»ä½•è¿›ç¨‹æ—¢å¯ä»¥æ˜¯é¢œè‰²ä¹Ÿå¯ä»¥æ˜¯åè°ƒè€…ã€‚åœ¨é€šè¿‡è°ƒç”¨
    RRPC åˆå§‹åŒ–åï¼Œä»»ä½•è¿›ç¨‹éƒ½ä¼šåœ¨åå°è¿è¡Œæ¶ˆæ¯ç›‘å¬å™¨ï¼Œå¹¶ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†è¯·æ±‚å’Œå“åº”ã€‚
- en: After thatï¼Œ there are three ways to run a callable remotelyã€‚RRPC syncncï¼Œ RC
    A sync and a remoteã€‚RRPC S rolls R PCC synchronouslyã€‚It will block until the return
    value is available on the colorã€‚RBC ASyncï¼Œ on the other handï¼Œ immediately returns
    a future object of the resultã€‚The third API is remoteï¼Œ which also returns right
    awayã€‚ but instead of returning a future objectã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥è¿œç¨‹è¿è¡Œå¯è°ƒç”¨å¯¹è±¡ã€‚RRPC åŒæ­¥ã€RCA å¼‚æ­¥å’Œè¿œç¨‹ã€‚RRPC S ä»¥åŒæ­¥æ–¹å¼æ»šåŠ¨ R PCCã€‚å®ƒå°†é˜»å¡ï¼Œç›´åˆ°è¿”å›å€¼åœ¨é¢œè‰²ä¸Šå¯ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼ŒRBC
    ASync ç«‹å³è¿”å›ç»“æœçš„æœªæ¥å¯¹è±¡ã€‚ç¬¬ä¸‰ä¸ª API æ˜¯è¿œç¨‹ï¼Œä¹Ÿç«‹å³è¿”å›ï¼Œä½†ä¸æ˜¯è¿”å›æœªæ¥å¯¹è±¡ã€‚
- en: it returns a remote referenceã€‚Which can be treated as a discreted sharePoer
    of the resultã€‚The difference between R PCC async and remote is that the former
    will always fetch the result back to the colorã€‚But the remote API does not do
    thatã€‚ The remote reference will keep the result alive on the call Liã€‚The code
    in this slide shows how to pass different types of callables to A R PCC APIã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿”å›ä¸€ä¸ªè¿œç¨‹å¼•ç”¨ï¼Œå¯ä»¥è§†ä¸ºç»“æœçš„ç¦»æ•£å…±äº«æŒ‡é’ˆã€‚R PCC å¼‚æ­¥å’Œè¿œç¨‹ä¹‹é—´çš„åŒºåˆ«åœ¨äºï¼Œå‰è€…æ€»æ˜¯å°†ç»“æœå–å›åˆ°é¢œè‰²ã€‚ä½†æ˜¯è¿œç¨‹ API å¹¶ä¸è¿™æ ·åšã€‚è¿œç¨‹å¼•ç”¨å°†ä½¿ç»“æœåœ¨è°ƒç”¨æœŸé—´ä¿æŒæ´»è·ƒã€‚æœ¬é¡µä¸­çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•å°†ä¸åŒç±»å‹çš„å¯è°ƒç”¨å¯¹è±¡ä¼ é€’ç»™
    A R PCC APIã€‚
- en: It can be Pythtch building operatorsï¼Œ it can be user Python functions can also
    be script functionsã€‚If performance is a concernï¼Œ please use script functionsã€‚
    as there won't be a contentions on the global interpreter logã€‚ meaning a different
    IP PCCs can run concurrently on the callã€‚Remote referenceã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥æ˜¯ Pythtch æ„å»ºçš„æ“ä½œç¬¦ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”¨æˆ·å®šä¹‰çš„ Python å‡½æ•°æˆ–è„šæœ¬å‡½æ•°ã€‚å¦‚æœæ€§èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œè¯·ä½¿ç”¨è„šæœ¬å‡½æ•°ï¼Œå› ä¸ºè¿™ä¸ä¼šå¯¹å…¨å±€è§£é‡Šå™¨é”äº§ç”Ÿç«äº‰ï¼Œè¿™æ„å‘³ç€ä¸åŒçš„
    IP PCC å¯ä»¥åœ¨è°ƒç”¨ä¸Šå¹¶å‘è¿è¡Œã€‚è¿œç¨‹å¼•ç”¨ã€‚
- en: A remote reference is like a distributed sharepointerã€‚ It points to an object
    on the local or remote processes and will manage the lifetime of the data object
    using the reference countã€‚This is useful when a call would like to directly forward
    the output from the col to another process and avoid fetching the real data back
    to the colorã€‚In the exampleï¼Œ worker Ze is acting as a coordinator processã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿œç¨‹å¼•ç”¨å°±åƒä¸€ä¸ªåˆ†å¸ƒå¼å…±äº«æŒ‡é’ˆã€‚å®ƒæŒ‡å‘æœ¬åœ°æˆ–è¿œç¨‹è¿›ç¨‹ä¸­çš„ä¸€ä¸ªå¯¹è±¡ï¼Œå¹¶å°†é€šè¿‡å¼•ç”¨è®¡æ•°ç®¡ç†æ•°æ®å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸã€‚å½“ä¸€ä¸ªè°ƒç”¨å¸Œæœ›å°†è¾“å‡ºç›´æ¥è½¬å‘åˆ°å¦ä¸€ä¸ªè¿›ç¨‹å¹¶é¿å…å°†å®é™…æ•°æ®å–å›æ—¶ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œworker
    Ze å……å½“åè°ƒè€…è¿›ç¨‹ã€‚
- en: It sets up data dependencies across four other processesã€‚ and none of the real
    data object goes through W0ã€‚The white dashed arrows are lightweight R PCC remote
    causeã€‚ and the bold red arrows are heavyweight data patternsã€‚Let's look at one
    of the remote causeã€‚ the one from W 0 to W 3ã€‚The arguments R and RB are remote
    references of outputs from W1W2 when using R and RB as arguments in RPRPC APIã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåœ¨å››ä¸ªå…¶ä»–è¿›ç¨‹ä¹‹é—´å»ºç«‹æ•°æ®ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”æ²¡æœ‰å®é™…æ•°æ®å¯¹è±¡ç»è¿‡ W0ã€‚ç™½è‰²è™šçº¿ç®­å¤´æ˜¯è½»é‡çº§çš„ R PCC è¿œç¨‹è°ƒç”¨ï¼Œè€Œç²—çº¢è‰²ç®­å¤´æ˜¯é‡é‡çº§çš„æ•°æ®æ¨¡å¼ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªè¿œç¨‹è°ƒç”¨ï¼Œä»
    W0 åˆ° W3ã€‚å‚æ•° R å’Œ RB æ˜¯æ¥è‡ª W1W2 çš„è¾“å‡ºçš„è¿œç¨‹å¼•ç”¨ï¼Œå½“åœ¨ RPRPC API ä¸­ä½¿ç”¨ R å’Œ RB ä½œä¸ºå‚æ•°æ—¶ã€‚
- en: the RRPC system will automatically fork R and RB on W3 and increment the reference
    count accordinglyã€‚Then on W 3ï¼Œ it costs2 here to fetch the real data from W1 and
    W2ã€‚ which will block until the data is received in W 3ã€‚In this wayã€‚ the remote
    reference allows WDro to asynchronously and efficiently set up function executions
    and data dependencies in a distributed environmentã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RRPCç³»ç»Ÿå°†è‡ªåŠ¨åœ¨W3ä¸Šåˆ†å‰Rå’ŒRBï¼Œå¹¶ç›¸åº”å¢åŠ å¼•ç”¨è®¡æ•°ã€‚åœ¨W3ä¸Šï¼Œä»W1å’ŒW2è·å–çœŸå®æ•°æ®çš„æˆæœ¬ä¸º2ï¼Œè¿™å°†é˜»å¡ç›´åˆ°åœ¨W3ä¸­æ”¶åˆ°æ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè¿œç¨‹å¼•ç”¨å…è®¸WDroåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å¼‚æ­¥é«˜æ•ˆåœ°è®¾ç½®åŠŸèƒ½æ‰§è¡Œå’Œæ•°æ®ä¾èµ–ã€‚
- en: Remote execution and remote reference help applications to piece together forward
    path across processes and machinesã€‚Another important component in Pytorch is the
    autograd systemï¼Œ which powers the backward pathã€‚The RPC framework extends local
    autograd engine to work in a distributed environmentã€‚And it also provides distributed
    o to update all parameters involved in the applicationã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿œç¨‹æ‰§è¡Œå’Œè¿œç¨‹å¼•ç”¨å¸®åŠ©åº”ç”¨ç¨‹åºåœ¨è¿›ç¨‹å’Œæœºå™¨ä¹‹é—´æ‹¼æ¥å‰å‘è·¯å¾„ã€‚Pytorchä¸­çš„å¦ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†æ˜¯è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿï¼Œå®ƒæ”¯æŒåå‘è·¯å¾„ã€‚RPCæ¡†æ¶æ‰©å±•äº†æœ¬åœ°è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å·¥ä½œï¼Œå¹¶æä¾›åˆ†å¸ƒå¼ä¼˜åŒ–ä»¥æ›´æ–°åº”ç”¨ç¨‹åºä¸­æ¶‰åŠçš„æ‰€æœ‰å‚æ•°ã€‚
- en: One difference between local autograd and distributed autograd is that instead
    of storing the gradient in the peri dotgrad fieldã€‚ the distributed autograd engine
    stores the gradients in a dedicated contextã€‚ and there can be multiple autograd
    contexts coexist for the same set of parametersã€‚The reason for this design is
    because there can be multiple concurrent backward passes showing same parameters
    and when this happensã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬åœ°è‡ªåŠ¨å¾®åˆ†å’Œåˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†ä¹‹é—´çš„ä¸€ä¸ªåŒºåˆ«åœ¨äºï¼Œåˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†å¼•æ“å°†æ¢¯åº¦å­˜å‚¨åœ¨ä¸“ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè€Œä¸æ˜¯å­˜å‚¨åœ¨peri dotgradå­—æ®µä¸­ã€‚å¯¹äºåŒä¸€ç»„å‚æ•°ï¼Œå¯ä»¥åŒæ—¶å­˜åœ¨å¤šä¸ªè‡ªåŠ¨å¾®åˆ†ä¸Šä¸‹æ–‡ã€‚è¿™æ ·è®¾è®¡çš„åŸå› æ˜¯ï¼Œå¯èƒ½ä¼šæœ‰å¤šä¸ªå¹¶å‘åå‘ä¼ é€’æ˜¾ç¤ºç›¸åŒçš„å‚æ•°ï¼Œè€Œåœ¨è¿™ç§æƒ…å†µä¸‹ã€‚
- en: we need to make sure that these backward passes do not step into each other's
    toesã€‚Henceã€‚ in order to use distributed autograï¼Œ you need to first createã€‚It contactsã€‚All
    R PCs made within that context will carry the context I informationã€‚ which will
    help callers and colleagues to find each other in the back of the pathã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ç¡®ä¿è¿™äº›åå‘ä¼ é€’ä¸ä¼šç›¸äº’å¹²æ‰°ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿ç”¨åˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†ï¼Œä½ éœ€è¦é¦–å…ˆåˆ›å»ºä¸Šä¸‹æ–‡ã€‚åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­åˆ›å»ºçš„æ‰€æœ‰RPCå°†æºå¸¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™å°†å¸®åŠ©è°ƒç”¨è€…å’ŒåŒäº‹åœ¨åå‘è·¯å¾„ä¸­æ‰¾åˆ°å½¼æ­¤ã€‚
- en: In this tool example on the rightï¼Œ it first uses two remote calls to create
    two kilometers and initializes the distributed optimizeim using the list of kilometer
    remote referencesã€‚After thatï¼Œ it runs the forward pass by simply fetch the parameter
    from the owners and then sum them togetherã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å³ä¾§çš„è¿™ä¸ªå·¥å…·ç¤ºä¾‹ä¸­ï¼Œå®ƒé¦–å…ˆä½¿ç”¨ä¸¤ä¸ªè¿œç¨‹è°ƒç”¨æ¥åˆ›å»ºä¸¤ä¸ªåƒç±³ï¼Œå¹¶ä½¿ç”¨åƒç±³è¿œç¨‹å¼•ç”¨åˆ—è¡¨åˆå§‹åŒ–åˆ†å¸ƒå¼ä¼˜åŒ–ã€‚ä¹‹åï¼Œå®ƒé€šè¿‡ç®€å•åœ°ä»æ‰€æœ‰è€…è·å–å‚æ•°å¹¶å°†å®ƒä»¬ç›¸åŠ æ¥è¿è¡Œå‰å‘ä¼ é€’ã€‚
- en: Then it feeds the loss tensor to the discreted autogra backward functionã€‚Which
    were compute gradients for all parameters in the distributed auto grid graph and
    store the gradient in the contextã€‚Finallyï¼Œ we can pass the contact I D to the
    distributed optimizeimr step functionã€‚Which will reach out to owners of all parametersï¼Œ
    retrieve corresponding ingredients from context and update parameters using the
    provided local optimã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒå°†æŸå¤±å¼ é‡è¾“å…¥åˆ°åˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†åå‘å‡½æ•°ä¸­ã€‚è¯¥å‡½æ•°è®¡ç®—åˆ†å¸ƒå¼è‡ªåŠ¨å¾®åˆ†å›¾ä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶å°†æ¢¯åº¦å­˜å‚¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šä¸‹æ–‡IDä¼ é€’ç»™åˆ†å¸ƒå¼ä¼˜åŒ–æ­¥éª¤å‡½æ•°ã€‚è¯¥å‡½æ•°å°†è”ç³»æ‰€æœ‰å‚æ•°çš„æ‰€æœ‰è€…ï¼Œä»ä¸Šä¸‹æ–‡ä¸­æ£€ç´¢ç›¸åº”çš„æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨æä¾›çš„æœ¬åœ°ä¼˜åŒ–æ›´æ–°å‚æ•°ã€‚
- en: In this caseï¼Œ it's STGDã€‚As you can seeï¼Œ the API for distributed training is
    very similar to local trainingã€‚Except that you will need to create a context for
    itã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_9.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯STGDã€‚å¦‚ä½ æ‰€è§ï¼Œåˆ†å¸ƒå¼è®­ç»ƒçš„APIä¸æœ¬åœ°è®­ç»ƒéå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ä½ éœ€è¦ä¸ºå…¶åˆ›å»ºä¸€ä¸ªä¸Šä¸‹æ–‡ã€‚
- en: Given all these new toolsã€‚What can you do with themï¼Ÿ
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºæ‰€æœ‰è¿™äº›æ–°å·¥å…·ï¼Œä½ èƒ½ç”¨å®ƒä»¬åšäº›ä»€ä¹ˆï¼Ÿ
- en: It unlocks many discrete training scenarios on Pythtorchï¼Œ and I will briefly
    describe three of themã€‚The first one is a parameter server where you can have
    one parametermeter serverã€‚Or several sharded peri service holding the premiersã€‚
    And then there can be multiple trainers running training integrationsã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è§£é”äº†è®¸å¤šåœ¨Pythtorchä¸Šç¦»æ•£çš„è®­ç»ƒåœºæ™¯ï¼Œæˆ‘å°†ç®€è¦æè¿°å…¶ä¸­ä¸‰ä¸ªã€‚ç¬¬ä¸€ä¸ªæ˜¯å‚æ•°æœåŠ¡å™¨ï¼Œä½ å¯ä»¥æœ‰ä¸€ä¸ªå‚æ•°æœåŠ¡å™¨ï¼Œæˆ–è€…å¤šä¸ªåˆ†ç‰‡çš„periæœåŠ¡æŒæœ‰å‚æ•°ï¼Œç„¶åå¯ä»¥æœ‰å¤šä¸ªè®­ç»ƒè€…è¿è¡Œè®­ç»ƒé›†æˆã€‚
- en: The R PC framework can help link them togetherã€‚Another example is a distributed
    model parallelã€‚Where the model might not fit in one machineã€‚In this caseã€‚ you
    can divide the model into multiple sub modules and use RPC and RF to piece them
    togetherã€‚The third example is pipeline parallelismã€‚ You can use the asynchronous
    APIs in R PCC to process one batch and then run interventions on multiple batchges
    concurrentlyã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: R PCæ¡†æ¶å¯ä»¥å¸®åŠ©å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯åˆ†å¸ƒå¼æ¨¡å‹å¹¶è¡Œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•é€‚åº”ä¸€å°æœºå™¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å°†æ¨¡å‹åˆ†å‰²æˆå¤šä¸ªå­æ¨¡å—ï¼Œå¹¶ä½¿ç”¨RPCå’ŒRFå°†å®ƒä»¬æ‹¼æ¥åœ¨ä¸€èµ·ã€‚ç¬¬ä¸‰ä¸ªä¾‹å­æ˜¯ç®¡é“å¹¶è¡Œã€‚ä½ å¯ä»¥ä½¿ç”¨R
    PCCä¸­çš„å¼‚æ­¥APIæ¥å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡ï¼Œç„¶ååœ¨å¤šä¸ªæ‰¹æ¬¡ä¸ŠåŒæ—¶è¿è¡Œå¹²é¢„ã€‚
- en: Tinorials are available for all these use casesã€‚All rightã€‚ that's a very short
    introduction of Python Sha PCCã€‚With this talkã€‚ I want to make sure that at least
    I deliver one message to youï¼Œ which isã€‚ if DDP does not sufficient for your use
    caseï¼Œ please try P tryCã€‚We have quite a few tutorialsã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç”¨ä¾‹éƒ½æœ‰æ•™ç¨‹å¯ç”¨ã€‚å¥½å§ï¼Œè¿™å°±æ˜¯å…³äºPython Sha PCCçš„éå¸¸ç®€çŸ­çš„ä»‹ç»ã€‚åœ¨è¿™ä¸ªè®²åº§ä¸­ï¼Œæˆ‘å¸Œæœ›ç¡®ä¿è‡³å°‘å‘ä½ ä¼ è¾¾ä¸€ä¸ªä¿¡æ¯ï¼Œé‚£å°±æ˜¯ï¼Œå¦‚æœDDPä¸è¶³ä»¥æ»¡è¶³ä½ çš„ç”¨ä¾‹ï¼Œè¯·å°è¯•P
    tryCã€‚æˆ‘ä»¬æœ‰å¾ˆå¤šæ•™ç¨‹ã€‚
- en: and we also have an extensive API pageã€‚ We are actively monitoring data issues
    and forum questions for RPC on daily basisã€‚ so let us know if you encounter any
    problem and also let us know if there's any way that Pyth RRPC can be improvedã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_11.png)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªå…¨é¢çš„APIé¡µé¢ã€‚æˆ‘ä»¬æ¯å¤©éƒ½åœ¨ç§¯æç›‘æ§æ•°æ®é—®é¢˜å’Œè®ºå›å…³äºRPCçš„æé—®ã€‚æ‰€ä»¥å¦‚æœä½ é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ï¼Œå¦‚æœPyth RRPCæœ‰ä»»ä½•æ”¹è¿›çš„ç©ºé—´ï¼Œä¹Ÿè¯·å‘ŠçŸ¥æˆ‘ä»¬ã€‚![](img/5747d0e2c4e80d16a5a2b785aa519633_11.png)
- en: '![](img/5747d0e2c4e80d16a5a2b785aa519633_12.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5747d0e2c4e80d16a5a2b785aa519633_12.png)'
