- en: 【双语字幕+资料下载】Pytorch 进阶学习讲座！14位Facebook工程师带你解锁 PyTorch 的生产应用与技术细节 ＜官方教程系列＞ - P3：L3-
    Autograd 的高级 API - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】Pytorch进阶学习讲座！14位Facebook工程师带你解锁PyTorch的生产应用与技术细节＜官方教程系列＞ - P3：L3-
    Autograd的高级API - ShowMeAI - BV1ZZ4y1U7dg
- en: 🎼。![](img/ae640e7b46aaf17bba8e02172f49117e_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 🎼。![](img/ae640e7b46aaf17bba8e02172f49117e_1.png)
- en: Hi everyone， my name is Alban， I'm a research engineer here at Facebook and
    I'm working on the frontend team for Pytorch in New York。More particularly on
    the Autograd sub system。 and so today I'm going to talk to you about the brand
    new high level API that we added for the Autograd。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，我的名字是Alban，我是Facebook的一名研究工程师，正在纽约的Pytorch前端团队工作。更具体地说，我在Autograd子系统上工作。所以今天我将与大家讨论我们为Autograd添加的全新高级API。
- en: In particular， I'm going to give you a quick overview of like the motivations
    for this new API。 what it actually looks like and how to use it and finally some
    future works for what's going to come for it。![](img/ae640e7b46aaf17bba8e02172f49117e_3.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我将简要概述这个新API的动机、实际样子以及如何使用，最后介绍一些未来的工作计划！[](img/ae640e7b46aaf17bba8e02172f49117e_3.png)
- en: So to begin with the motivation for it is that the current autograd API we have
    is not for everyone。In particular， the dot backward function is really geared
    toward Torch。t andN users as it populates the dot grad fields of all the tensors
    and it's really associated with the states and the optimizes that we use with
    StN。Similarly， autogradgrad is geared toward neural network user in the sense
    that it is built to do back propagation and all the namings and all the API design
    has been done。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这个新API的动机是当前的autograd API并不适合所有人。特别是，dot backward函数非常针对Torch和N用户，因为它填充了所有张量的dot
    grad字段，并且与我们在StN中使用的状态和优化器紧密相关。类似地，autogradgrad则更倾向于神经网络用户，因为它是为反向传播构建的，所有命名和API设计都是如此。
- en: Thinking about neural networks。And so we think that now nowadays that we have
    more general users that use Autograd to do more optimization than just neural
    network。We needed a good generic API that these people can use to actually do
    everything they need。Moreover。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 想到神经网络。因此，我们认为，现在有更多的普通用户使用Autograd进行比单纯的神经网络更多的优化。我们需要一个好的通用API，供这些人使用，以满足他们的所有需求。此外。
- en: for higher level functions such as computing a Jacobian that I'm going to use
    here is like an example for many things。It's very nice to have a reference implementation
    within Pythtorch。The reason for that is up to fairly recently if you wanted to
    compute the Jacobn with Spytorch。 you had to copy paste some code from a guest
    that was made few years ago。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更高级的功能，比如计算雅可比，我将在这里用作许多事情的示例。在Pythtorch中拥有一个参考实现是非常好的。原因是直到最近，如果你想用Spytorch计算雅可比，你需要复制粘贴几年前做的代码。
- en: And so people have basically a stale version of that g， potentially in their
    code。 And we can't really make any improvement to the way。Jacoian are computed
    for everyone to basically get automatically this new upgraded version。And so this
    new API is going to help us bring more performance improvements to the users via
    this generic API。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，人们在他们的代码中基本上拥有一个过时的版本g。我们不能真的对计算雅可比的方式进行改进，让每个人自动获得这个新的升级版本。因此，这个新API将帮助我们通过这个通用API为用户带来更多性能改进。
- en: And。Following this idea， we also added recently a whole benchmark system around
    this API to make sure that existing models and models that are used very often
    by our users。Have good performance， and we can also measure improvements we're
    making by different change。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，基于这个想法，我们最近还围绕这个API添加了一个完整的基准系统，以确保现有模型和我们用户经常使用的模型具有良好的性能，并且我们也可以通过不同的变化来衡量我们正在做的改进。
- en: but also we can measure regressions and make sure they are not significant and
    we can catch them and fix them before they hit main releases。![](img/ae640e7b46aaf17bba8e02172f49117e_5.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也可以测量回归并确保它们不显著，并在它们影响主要发布之前捕捉并修复它们！[](img/ae640e7b46aaf17bba8e02172f49117e_5.png)
- en: So where does this new API live？So it lives under the torch dot autogra dot
    functional symbolial。You can find it in the autograd documentation under like
    the functional higher level API name。And it has a slightly different spirit comparing
    to the existing APIs。 mainly that it takes function as inputs。And not the result
    of the forwardd pass。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个新API在哪里呢？它位于torch.dot.autogra.functional符号下。你可以在autograd文档中找到它，名为功能性高级API。与现有API相比，它具有稍微不同的理念，主要是将函数作为输入，而不是前向传递的结果。
- en: The main reason for that is our twofold， one is because it's closer to the mathematical
    formulation and people are getting used to differentiate functions directly。 not
    do the forward paths and then as for the gradient。Second。 it also allows us to
    get more freedom respect to what happens during the forward pass。And in particular，
    for some of the optimization we have planned。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的主要原因有两个，一个是因为它更接近数学公式，且人们习惯于直接对函数进行求导，而不是先进行前向路径再计算梯度。第二，它还使我们在前向传播中对发生的事情有了更多的自由，特别是对于我们计划的一些优化。
- en: we will need to do some special things during the forward pass that we don't
    want the user to have to worry about。 and so this new pay is going to allow us
    to do that very efficiently。![](img/ae640e7b46aaf17bba8e02172f49117e_7.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播期间，我们需要做一些特殊的事情，不希望用户担心。因此，这个新API将允许我们非常高效地做到这一点。![](img/ae640e7b46aaf17bba8e02172f49117e_7.png)
- en: '![](img/ae640e7b46aaf17bba8e02172f49117e_8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae640e7b46aaf17bba8e02172f49117e_8.png)'
- en: So what does this new API contain The first big part is first order gradients。So
    as I mentioned before， we have a function that directly computed Jacobian for
    you。 given a function and some input points。We also have a vector Jacobian product
    which corresponds to backward mode automatic differentiation。 and in the neural
    network world it corresponds to the backward propagation algorithm。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个新API包含什么呢？第一大部分是一阶梯度。正如我之前提到的，我们有一个直接为你计算雅可比矩阵的函数，给定一个函数和一些输入点。我们还有一个向量雅可比乘积，它对应于反向模式自动微分，在神经网络领域对应于反向传播算法。
- en: So this is actually very close to the existing autograd dot grad function。And
    finally。 we also provide a Jacobin vector product。 So the other way around。 and
    this one corresponds more to forward mode automatic differentiation。And can be
    used to compute directional derivatives。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上非常接近现有的autograd dot grad函数。最后，我们还提供了一个雅可比向量乘积。另一种方式，这个对应于前向模式自动微分，可以用于计算方向导数。
- en: And we also have the second product of this API corresponds to second order
    gradient and very similar functions so。A hassian function that computes all the
    second order derivatives。A vector Hessian product which allows us to efficiently
    compute with backward mode automatic differentiation。 a product between the Hesian
    and a given vector。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个API的第二个部分对应于二阶梯度以及非常相似的函数。一个计算所有二阶导数的海森矩阵函数。一个向量海森乘积，它使我们能够有效地进行反向模式自动微分，计算海森矩阵与给定向量之间的乘积。
- en: '![](img/ae640e7b46aaf17bba8e02172f49117e_10.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae640e7b46aaf17bba8e02172f49117e_10.png)'
- en: And the similarly， the other way around the hesian vector product to that corresponds
    more to foreign mode automatic differentiation。![](img/ae640e7b46aaf17bba8e02172f49117e_12.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，海森向量乘积则更对应于前向模式自动微分。![](img/ae640e7b46aaf17bba8e02172f49117e_12.png)
- en: So now an example on how to use DC APII。So again， as an example for the Jacobian
    function。 now you don't have to copy past code from that old guest， you can just
    import it from torch。And as you can see in the first example， where you have some
    input。 you just call jackhoion on it and you get the value of the jackbion and
    that's easy。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来举一个如何使用DC API的例子。再次作为雅可比函数的例子，现在你不需要从旧的代码中复制粘贴代码，你可以直接从torch导入。正如你在第一个例子中看到的，当你有一些输入时，只需调用雅可比函数，你就能得到雅可比值，这很简单。
- en: that's everything you need to do。What is very nice as well is that you can compose
    this new API with the existing autoquad API and here。 for example， if you input
    requires gradients， you can ask the Jacobian computation to create graph and so
    to be able to back through it and as you can see you can then compute norm of
    the Jacob you just got and backward through that。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要做的所有事情。很不错的是，你可以将这个新API与现有的autoquad API组合起来。例如，如果你输入需要梯度，你可以请求雅可比矩阵的计算来创建图形，从而能够反向传播，正如你所看到的，你可以计算刚得到的雅可比矩阵的范数并进行反向传播。
- en: '![](img/ae640e7b46aaf17bba8e02172f49117e_14.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae640e7b46aaf17bba8e02172f49117e_14.png)'
- en: And then you can compose that with the rest of your training to get all the
    quantities that you need。There are many more examples and。A few examples here
    are like gradient penalties based on Jacobn computations。Jacoian vector product
    computations that corresponds to the forward mode automatic differentiation。This
    is especially interesting because they actually compute directional derivatives
    in higher dimension。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以将其与训练的其余部分组合，以获得你所需的所有量。还有许多更多的例子，这里的一些例子包括基于雅可比计算的梯度惩罚，以及与前向模式自动微分相对应的雅可比向量积计算。这尤其有趣，因为它们实际上计算高维的方向导数。
- en: These derivatives are very useful for quite a few optimization algorithms。And
    similarly。 the second order methods that compute Hessian or Hesian vector products
    allow you to do Newton step methods or approximate Newton step methods much more
    efficiently and much in a much easier way than with the current Auto RE API。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导数对许多优化算法非常有用。同样，计算海森矩阵或海森向量积的二阶方法使你能够更高效、更简单地执行牛顿步法或近似牛顿步法，而不是使用当前的 Auto
    RE API。
- en: So to finish with some future work。The first part is actually things that we
    are already currently working on。So the first one is for node 8， so the idea would
    be for both Jacob vector and Hesian vector products。To replace that with an actual
    for node automatic differentiation。 So this is work in progress。 hopefully we'll
    be able to release that very soon。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以最后谈谈一些未来的工作。第一部分实际上是我们目前正在进行的工作。第一个是针对节点 8，目标是对于雅可比向量和海森向量积，将其替换为实际的前向自动微分。这项工作正在进行中，希望我们能很快发布。
- en: And get very good performance improvements based on that。You so one interesting
    thing about this new API is that you will get the benefits of this new forward
    mode ID for free if you already use that API when it is released。Similarly， you
    can get badge gradients。So this is a collaboration with the VMap features that
    you heard about。 and the idea here will be to speed up computation for Jacobian
    and Hesian computations。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，你可以获得非常好的性能提升。关于这个新 API，有一件有趣的事情是，如果你在其发布时已经使用了这个 API，你将免费享受到这个新前向模式 ID
    的好处。同样，你可以获得徽章渐变。因此，这与你听说过的 VMap 功能是一个合作，目的是加速雅可比和海森计算的计算速度。
- en: And finally torch。nN composibility is another topic we're working on。 The reason
    for that is currently NN modules hold a lot of states and they are not functional
    so they don't work very out of the box with the new API that we design。 So the
    idea here is going to be to work on Torch end to try and provide a functional
    version of the NN modules so that we can work with them more efficiently。And for
    longer time work， we are looking for ideas and please do share your feedback on
    how this works。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，torch.nN 的可组合性是我们正在研究的另一个主题。原因在于，当前的 NN 模块持有大量状态，并且它们不是功能性的，因此与我们设计的新 API
    并不完全兼容。因此，这里的想法是尝试在 Torch 端提供 NN 模块的功能性版本，以便我们能更高效地使用它们。对于长期的工作，我们在寻找想法，请分享你对这些工作的反馈。
- en: how it helps you do what you want to do。Do open issues on GiHub or on the Pythtorch
    forum if you have questions or concerns about what is happening here。 and please
    help us make DC API exactly what you're looking for。![](img/ae640e7b46aaf17bba8e02172f49117e_16.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它如何帮助你实现你想做的事情。如果你对这里发生的事情有问题或担忧，请在 GitHub 或 Pythtorch 论坛上公开问题。并请帮助我们让 DC API
    完全符合你的需求。![](img/ae640e7b46aaf17bba8e02172f49117e_16.png)
- en: 🎼So thank you very much for listening and have a good day。![](img/ae640e7b46aaf17bba8e02172f49117e_18.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 🎼非常感谢你们的倾听，祝你们有美好的一天。![](img/ae640e7b46aaf17bba8e02172f49117e_18.png)
