- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3-
    Autograd çš„é«˜çº§ API - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ğŸ¼ã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Hi everyoneï¼Œ my name is Albanï¼Œ I'm a research engineer here at Facebook and
    I'm working on the frontend team for Pytorch in New Yorkã€‚More particularly on
    the Autograd sub systemã€‚ and so today I'm going to talk to you about the brand
    new high level API that we added for the Autogradã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In particularï¼Œ I'm going to give you a quick overview of like the motivations
    for this new APIã€‚ what it actually looks like and how to use it and finally some
    future works for what's going to come for itã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_3.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: So to begin with the motivation for it is that the current autograd API we have
    is not for everyoneã€‚In particularï¼Œ the dot backward function is really geared
    toward Torchã€‚t andN users as it populates the dot grad fields of all the tensors
    and it's really associated with the states and the optimizes that we use with
    StNã€‚Similarlyï¼Œ autogradgrad is geared toward neural network user in the sense
    that it is built to do back propagation and all the namings and all the API design
    has been doneã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about neural networksã€‚And so we think that now nowadays that we have
    more general users that use Autograd to do more optimization than just neural
    networkã€‚We needed a good generic API that these people can use to actually do
    everything they needã€‚Moreoverã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: for higher level functions such as computing a Jacobian that I'm going to use
    here is like an example for many thingsã€‚It's very nice to have a reference implementation
    within Pythtorchã€‚The reason for that is up to fairly recently if you wanted to
    compute the Jacobn with Spytorchã€‚ you had to copy paste some code from a guest
    that was made few years agoã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: And so people have basically a stale version of that gï¼Œ potentially in their
    codeã€‚ And we can't really make any improvement to the wayã€‚Jacoian are computed
    for everyone to basically get automatically this new upgraded versionã€‚And so this
    new API is going to help us bring more performance improvements to the users via
    this generic APIã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Andã€‚Following this ideaï¼Œ we also added recently a whole benchmark system around
    this API to make sure that existing models and models that are used very often
    by our usersã€‚Have good performanceï¼Œ and we can also measure improvements we're
    making by different changeã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: but also we can measure regressions and make sure they are not significant and
    we can catch them and fix them before they hit main releasesã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_5.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: So where does this new API liveï¼ŸSo it lives under the torch dot autogra dot
    functional symbolialã€‚You can find it in the autograd documentation under like
    the functional higher level API nameã€‚And it has a slightly different spirit comparing
    to the existing APIsã€‚ mainly that it takes function as inputsã€‚And not the result
    of the forwardd passã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The main reason for that is our twofoldï¼Œ one is because it's closer to the mathematical
    formulation and people are getting used to differentiate functions directlyã€‚ not
    do the forward paths and then as for the gradientã€‚Secondã€‚ it also allows us to
    get more freedom respect to what happens during the forward passã€‚And in particularï¼Œ
    for some of the optimization we have plannedã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: we will need to do some special things during the forward pass that we don't
    want the user to have to worry aboutã€‚ and so this new pay is going to allow us
    to do that very efficientlyã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_7.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae640e7b46aaf17bba8e02172f49117e_8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: So what does this new API contain The first big part is first order gradientsã€‚So
    as I mentioned beforeï¼Œ we have a function that directly computed Jacobian for
    youã€‚ given a function and some input pointsã€‚We also have a vector Jacobian product
    which corresponds to backward mode automatic differentiationã€‚ and in the neural
    network world it corresponds to the backward propagation algorithmã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: So this is actually very close to the existing autograd dot grad functionã€‚And
    finallyã€‚ we also provide a Jacobin vector productã€‚ So the other way aroundã€‚ and
    this one corresponds more to forward mode automatic differentiationã€‚And can be
    used to compute directional derivativesã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: And we also have the second product of this API corresponds to second order
    gradient and very similar functions soã€‚A hassian function that computes all the
    second order derivativesã€‚A vector Hessian product which allows us to efficiently
    compute with backward mode automatic differentiationã€‚ a product between the Hesian
    and a given vectorã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae640e7b46aaf17bba8e02172f49117e_10.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: And the similarlyï¼Œ the other way around the hesian vector product to that corresponds
    more to foreign mode automatic differentiationã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_12.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: So now an example on how to use DC APIIã€‚So againï¼Œ as an example for the Jacobian
    functionã€‚ now you don't have to copy past code from that old guestï¼Œ you can just
    import it from torchã€‚And as you can see in the first exampleï¼Œ where you have some
    inputã€‚ you just call jackhoion on it and you get the value of the jackbion and
    that's easyã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: that's everything you need to doã€‚What is very nice as well is that you can compose
    this new API with the existing autoquad API and hereã€‚ for exampleï¼Œ if you input
    requires gradientsï¼Œ you can ask the Jacobian computation to create graph and so
    to be able to back through it and as you can see you can then compute norm of
    the Jacob you just got and backward through thatã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae640e7b46aaf17bba8e02172f49117e_14.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: And then you can compose that with the rest of your training to get all the
    quantities that you needã€‚There are many more examples andã€‚A few examples here
    are like gradient penalties based on Jacobn computationsã€‚Jacoian vector product
    computations that corresponds to the forward mode automatic differentiationã€‚This
    is especially interesting because they actually compute directional derivatives
    in higher dimensionã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å¯ä»¥å°†å…¶ä¸è®­ç»ƒçš„å…¶ä½™éƒ¨åˆ†ç»„åˆï¼Œä»¥è·å¾—ä½ æ‰€éœ€çš„æ‰€æœ‰é‡ã€‚è¿˜æœ‰è®¸å¤šæ›´å¤šçš„ä¾‹å­ï¼Œè¿™é‡Œçš„ä¸€äº›ä¾‹å­åŒ…æ‹¬åŸºäºé›…å¯æ¯”è®¡ç®—çš„æ¢¯åº¦æƒ©ç½šï¼Œä»¥åŠä¸å‰å‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†ç›¸å¯¹åº”çš„é›…å¯æ¯”å‘é‡ç§¯è®¡ç®—ã€‚è¿™å°¤å…¶æœ‰è¶£ï¼Œå› ä¸ºå®ƒä»¬å®é™…ä¸Šè®¡ç®—é«˜ç»´çš„æ–¹å‘å¯¼æ•°ã€‚
- en: These derivatives are very useful for quite a few optimization algorithmsã€‚And
    similarlyã€‚ the second order methods that compute Hessian or Hesian vector products
    allow you to do Newton step methods or approximate Newton step methods much more
    efficiently and much in a much easier way than with the current Auto RE APIã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯¼æ•°å¯¹è®¸å¤šä¼˜åŒ–ç®—æ³•éå¸¸æœ‰ç”¨ã€‚åŒæ ·ï¼Œè®¡ç®—æµ·æ£®çŸ©é˜µæˆ–æµ·æ£®å‘é‡ç§¯çš„äºŒé˜¶æ–¹æ³•ä½¿ä½ èƒ½å¤Ÿæ›´é«˜æ•ˆã€æ›´ç®€å•åœ°æ‰§è¡Œç‰›é¡¿æ­¥æ³•æˆ–è¿‘ä¼¼ç‰›é¡¿æ­¥æ³•ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å½“å‰çš„ Auto
    RE APIã€‚
- en: So to finish with some future workã€‚The first part is actually things that we
    are already currently working onã€‚So the first one is for node 8ï¼Œ so the idea would
    be for both Jacob vector and Hesian vector productsã€‚To replace that with an actual
    for node automatic differentiationã€‚ So this is work in progressã€‚ hopefully we'll
    be able to release that very soonã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æœ€åè°ˆè°ˆä¸€äº›æœªæ¥çš„å·¥ä½œã€‚ç¬¬ä¸€éƒ¨åˆ†å®é™…ä¸Šæ˜¯æˆ‘ä»¬ç›®å‰æ­£åœ¨è¿›è¡Œçš„å·¥ä½œã€‚ç¬¬ä¸€ä¸ªæ˜¯é’ˆå¯¹èŠ‚ç‚¹ 8ï¼Œç›®æ ‡æ˜¯å¯¹äºé›…å¯æ¯”å‘é‡å’Œæµ·æ£®å‘é‡ç§¯ï¼Œå°†å…¶æ›¿æ¢ä¸ºå®é™…çš„å‰å‘è‡ªåŠ¨å¾®åˆ†ã€‚è¿™é¡¹å·¥ä½œæ­£åœ¨è¿›è¡Œä¸­ï¼Œå¸Œæœ›æˆ‘ä»¬èƒ½å¾ˆå¿«å‘å¸ƒã€‚
- en: And get very good performance improvements based on thatã€‚You so one interesting
    thing about this new API is that you will get the benefits of this new forward
    mode ID for free if you already use that API when it is releasedã€‚Similarlyï¼Œ you
    can get badge gradientsã€‚So this is a collaboration with the VMap features that
    you heard aboutã€‚ and the idea here will be to speed up computation for Jacobian
    and Hesian computationsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ­¤ï¼Œä½ å¯ä»¥è·å¾—éå¸¸å¥½çš„æ€§èƒ½æå‡ã€‚å…³äºè¿™ä¸ªæ–° APIï¼Œæœ‰ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…æ˜¯ï¼Œå¦‚æœä½ åœ¨å…¶å‘å¸ƒæ—¶å·²ç»ä½¿ç”¨äº†è¿™ä¸ª APIï¼Œä½ å°†å…è´¹äº«å—åˆ°è¿™ä¸ªæ–°å‰å‘æ¨¡å¼ ID
    çš„å¥½å¤„ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è·å¾—å¾½ç« æ¸å˜ã€‚å› æ­¤ï¼Œè¿™ä¸ä½ å¬è¯´è¿‡çš„ VMap åŠŸèƒ½æ˜¯ä¸€ä¸ªåˆä½œï¼Œç›®çš„æ˜¯åŠ é€Ÿé›…å¯æ¯”å’Œæµ·æ£®è®¡ç®—çš„è®¡ç®—é€Ÿåº¦ã€‚
- en: And finally torchã€‚nN composibility is another topic we're working onã€‚ The reason
    for that is currently NN modules hold a lot of states and they are not functional
    so they don't work very out of the box with the new API that we designã€‚ So the
    idea here is going to be to work on Torch end to try and provide a functional
    version of the NN modules so that we can work with them more efficientlyã€‚And for
    longer time workï¼Œ we are looking for ideas and please do share your feedback on
    how this worksã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œtorch.nN çš„å¯ç»„åˆæ€§æ˜¯æˆ‘ä»¬æ­£åœ¨ç ”ç©¶çš„å¦ä¸€ä¸ªä¸»é¢˜ã€‚åŸå› åœ¨äºï¼Œå½“å‰çš„ NN æ¨¡å—æŒæœ‰å¤§é‡çŠ¶æ€ï¼Œå¹¶ä¸”å®ƒä»¬ä¸æ˜¯åŠŸèƒ½æ€§çš„ï¼Œå› æ­¤ä¸æˆ‘ä»¬è®¾è®¡çš„æ–° API
    å¹¶ä¸å®Œå…¨å…¼å®¹ã€‚å› æ­¤ï¼Œè¿™é‡Œçš„æƒ³æ³•æ˜¯å°è¯•åœ¨ Torch ç«¯æä¾› NN æ¨¡å—çš„åŠŸèƒ½æ€§ç‰ˆæœ¬ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½æ›´é«˜æ•ˆåœ°ä½¿ç”¨å®ƒä»¬ã€‚å¯¹äºé•¿æœŸçš„å·¥ä½œï¼Œæˆ‘ä»¬åœ¨å¯»æ‰¾æƒ³æ³•ï¼Œè¯·åˆ†äº«ä½ å¯¹è¿™äº›å·¥ä½œçš„åé¦ˆã€‚
- en: how it helps you do what you want to doã€‚Do open issues on GiHub or on the Pythtorch
    forum if you have questions or concerns about what is happening hereã€‚ and please
    help us make DC API exactly what you're looking forã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_16.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¦‚ä½•å¸®åŠ©ä½ å®ç°ä½ æƒ³åšçš„äº‹æƒ…ã€‚å¦‚æœä½ å¯¹è¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…æœ‰é—®é¢˜æˆ–æ‹…å¿§ï¼Œè¯·åœ¨ GitHub æˆ– Pythtorch è®ºå›ä¸Šå…¬å¼€é—®é¢˜ã€‚å¹¶è¯·å¸®åŠ©æˆ‘ä»¬è®© DC API
    å®Œå…¨ç¬¦åˆä½ çš„éœ€æ±‚ã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_16.png)
- en: ğŸ¼So thank you very much for listening and have a good dayã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_18.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼éå¸¸æ„Ÿè°¢ä½ ä»¬çš„å€¾å¬ï¼Œç¥ä½ ä»¬æœ‰ç¾å¥½çš„ä¸€å¤©ã€‚![](img/ae640e7b46aaf17bba8e02172f49117e_18.png)
