- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P8ÔºöL8- ÊîØÊåÅÂêëÈáèÊú∫
    - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to your new machine learning from scratcht tutorial„ÄÇ
    Today„ÄÇ we are going to implement the SVM algorithm using only built in Python
    modules and Ny„ÄÇüòä„ÄÇThe SVM or support vector machine is a very popular algorithm„ÄÇ
    It follows the idea to use a linear model and to find a linear decision boundary
    also called a hyperplane that best separates our data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And hereÔºå the choice as the best hyperplane is the one that represents the largest
    separation or the largest march in between the two classes„ÄÇ So we choose the hyperplane
    so that the distance from it to the nearest data point on each side is maximized„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if we have a look at this imageÔºå then we want to find a hyperplane and the
    hyperplane has to satisfy this equation W times x minus B equals 0„ÄÇAnd we want
    to„ÄÇFind the hyperplane so that the distance to both the„ÄÇboth classes is maximized„ÄÇ
    So we used the class plus one here and-1 here„ÄÇSo thisÔºå this tense or the margin
    should be maximized„ÄÇAnd firstÔºå let's have a look at the math behind it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's a little bit more complex than in my previous tutorials„ÄÇ but I promise
    that once you have understood itÔºå the final implementation is fairly simple„ÄÇSo„ÄÇWe
    use the linear modelÔºå W times x minus B„ÄÇ That should be 0„ÄÇ And then our„ÄÇÂóØ„ÄÇOur
    function should also satisfy the condition that W times x minus B should be greater
    or equal than one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For our class plus one„ÄÇ So all the samples here must lie on the left side of
    this equation or this line here„ÄÇAnd all„ÄÇThe samples of the classÔºå-1 must lie„ÄÇOn
    the right side from this equation„ÄÇ So if we put this mathematicallyÔºå then we should
    it must satisfy W times x minus B should be greater or equal than1 for class1„ÄÇ
    or it should be less or equal than-1 for class -1„ÄÇSo if you put this in only one
    equation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we multiply our„ÄÇLinear function with the class label„ÄÇ And this should be
    greater or equal than one„ÄÇSo this is the condition that we want to satisfy„ÄÇAnd
    now we want to come up with the W and the B„ÄÇ So our weights and the bias„ÄÇAnd for
    this„ÄÇ we use the cost function and then apply gradient descent„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if you're not familiar with gradient descent already„ÄÇ then please watch one
    of my previous tutorials„ÄÇ For example„ÄÇ the one with linear regression there I
    explain this a little bit more in detail„ÄÇSo now let's„ÄÇConue„ÄÇ So we use the„ÄÇUse
    a cost function here„ÄÇ And in this caseÔºå we use the hinge loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this is defined as the maximum of 0 and„ÄÇ1 minus„ÄÇ And here we have our condition„ÄÇ
    Y I times our linear model„ÄÇ So what this means is if„ÄÇ if we plot the hinge loss
    and here the blue line is the hinge loss„ÄÇ So this is either 0Ôºå if„ÄÇY times„ÄÇF is
    greater or equal than one„ÄÇ So if they have the the same sign„ÄÇThen it's 0„ÄÇ And
    theÔºå if they„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if they are correctly classified and are larger than oneÔºå then our loss is 0„ÄÇ
    So this means if we have a look at this image againÔºå if„ÄÇFor the green glassÔºå if
    its„ÄÇ if it lies on this side„ÄÇThenÔºå it's 0„ÄÇAnd for the blue classÔºå if it lies on
    this side„ÄÇ then it's also 0„ÄÇAnd otherwiseÔºå then we have a linear function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the further we are away from our decision boundary lineÔºå the higher is our
    loss„ÄÇAnd so this is one part of our cost function„ÄÇ and the other part is„ÄÇAs I
    already said„ÄÇ we want to maximize the margin here„ÄÇSo between these two classes
    and the margin is„ÄÇDefined as2 over the magnitude of w„ÄÇ So this is dependent from
    our weight„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: dependent on our weight vector„ÄÇ So we want to maximize thisÔºå and therefore„ÄÇ
    we want to minimize the magnitude„ÄÇ So we put this or add this to our„ÄÇCost functions„ÄÇ
    So we also put this term„ÄÇThe magnitude of W to the power of two times a lambda
    parameter„ÄÇAnd then here we have our hinge loss„ÄÇSo the lambda parameter tries to
    find a trade off between these two terms„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we it says basically says which is more important„ÄÇ So we want toÔºå of course„ÄÇ
    we want to have the right classification„ÄÇ We want to lie on the correct side of
    our lines„ÄÇ but we also want to have the the line such that the margin is„ÄÇIs maximized„ÄÇÂóØ„ÄÇSo
    yeah„ÄÇ so if we look at the two casesÔºå if ourÔºå if we are on the correct side of
    the line„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if Y I times F on xÔºå F of x is greater or equal than one„ÄÇThen we simply„ÄÇ
    we only have this term because this is the hinge loss is 0„ÄÇ And otherwise„ÄÇ then
    our cost function is this year„ÄÇAnd now we want to minimize that„ÄÇ So we want to
    get the derivatives or the gradients of our cost function„ÄÇSo in the first case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if we are greater or equal than one„ÄÇOur derivative„ÄÇIs only is two times lambda
    times„ÄÇWÔºå so„ÄÇ and here we only look at one component of our W„ÄÇ So we get rid of
    the magnitude„ÄÇAnd the derivative with respect to the B is 0„ÄÇ So please double
    check that for yourself here„ÄÇ I will not explain the derivatives and details„ÄÇAnd
    in the other case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so if if Y I times F on x is not greater or equal than one„ÄÇThen our derivative
    with respect to the W is this equation here„ÄÇ and the derivative with respect to
    our bias is only Y I„ÄÇSo again„ÄÇ please double check it for yourself„ÄÇAnd then when
    we have our gradientsÔºå we can use the update rule„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the new weight is the old weight minus because we use gradient de„ÄÇ So we
    go into negative direction minus the learning rate or the step size times the
    derivative„ÄÇ So these are our update rules and„ÄÇNowÔºå I hope youve understood the
    concept and the math behind this„ÄÇ And now we can start implementing it„ÄÇ So this
    is now straightforward„ÄÇ SoÔºå first of all„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we import Ny S and PÔºå of course„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: And then we create our class S we amÔºå which will get an in it„ÄÇMethod„ÄÇAnd here
    I will put in a learning rateÔºå which will get a default value of 0001„ÄÇ and it
    will get a lambda parameterÔºå which will also get a default„ÄÇ And I will say this
    is 001„ÄÇ So this is usually also a small value„ÄÇAnd then it will get the number
    of iterations for our optimization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which will get the default of 1000„ÄÇSo then I will simply store them„ÄÇ So I will
    say self dot L R equals learning rate„ÄÇSelf do„ÄÇLambda Para equals Lambda Para„ÄÇ
    So note that I cannot use Lambda here because Lada is a keyword and Python for
    the Lada function„ÄÇSo„ÄÇ yeahÔºå them self do„ÄÇAnd„ÄÇIts equals and its„ÄÇ Then I will say
    self dot W equals none and self dot B equals none„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I have to come up with them later„ÄÇ and then we define our two functions„ÄÇ
    So as always„ÄÇ one is the predict functionÔºå where we fit the training samples and
    the training labels„ÄÇAnd the„ÄÇSorryÔºå this is the fit method„ÄÇAnd the other one is
    the predict method„ÄÇWhere we predict the labels of the test samples„ÄÇAnd now let's
    start with the predict method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because this is very shortÔºå so„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: We want toÔºå as I saidÔºå if we look at the mathÔºå we apply this linear model„ÄÇ and
    then we look at the sign of this„ÄÇ So if it's positiveÔºå then we say it's class
    1„ÄÇ and if it's negativeÔºå then we say it's class-1„ÄÇ So we say linear„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Output equals Ny dot„ÄÇDotÔºå So the dot product„ÄÇOf X and self dot WÔºå minus self
    dot„ÄÇB„ÄÇ and then we choose the size„ÄÇ so we can simply sayÔºå return„ÄÇNampy dot sign
    of this linear output„ÄÇSo this is the whole predict implementation„ÄÇ And now let's
    continue with the fit methodÔºå so„ÄÇFirst of all„ÄÇAs I saidÔºå we used the classes plus
    1 and-1 here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we want to make sure that our y has only -1 and plus1„ÄÇ So oftentimes it has
    0 and1„ÄÇ So let's convert this„ÄÇ So let's say y underscore equals„ÄÇ And here we can
    use nuy dot where this will get a condition„ÄÇ So we say y„ÄÇ if this is less or equal
    than 0Ôºå then we put in -1Ôºå and otherwise we put in plus1„ÄÇSo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This will convert all the zeros or smaller numbers to -1 and the other numbers
    to plus 1„ÄÇAnd now„ÄÇ let's get the number of samples and the number of„ÄÇFeatures„ÄÇAnd
    this is simply X dot„ÄÇShape„ÄÇ because our input vector X is a nuy and DRA„ÄÇWhere
    the number of rows is the number of samples and the number of columns is the number
    of features„ÄÇThen we want to initialize our W and our B„ÄÇ And we simply put in zeros
    in the beginning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we say self„ÄÇTt W equals Ny zeros of size and features„ÄÇ So for each„ÄÇFeature
    component we put in a0 for our weight component„ÄÇAnd then we say self dot B equals
    0„ÄÇ And now we can„ÄÇStart with our gradient descents„ÄÇ So we sayÔºå for underscore„ÄÇ
    because we don't need this in range self dot and it iter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the number of iterations we want to do this„ÄÇAnd thenÔºå we iterate over our„ÄÇTrain
    samples„ÄÇ So I say4 index and XÔºå I in enumerate X„ÄÇSo this will give me the current
    index and also the current sample„ÄÇAnd now„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: What I want to do now is let's„ÄÇHave a look at the„ÄÇmath again„ÄÇ So I want to„ÄÇI
    want to calculate the weight or the derivative of our cost function with respect
    to the W and with respect to the bias„ÄÇAnd here I firstÔºå but at firstÔºå I look if
    this condition is satisfiedÔºå so I will„ÄÇSay„ÄÇ and the condition is Y I times our
    linear function„ÄÇ So I say condition equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_9.png)'
  prefs: []
  type: TYPE_IMG
- en: Why underscore of the current index„ÄÇTimesÔºå and then the linear functionÔºå so„ÄÇNampai
    dot„ÄÇOf the current sample and our self dot W minus self dot„ÄÇB„ÄÇÂóØ„ÄÇThis should be
    greater or equal than one„ÄÇSo if this is satisfied and the condition is true and
    otherwise it's false„ÄÇ So now I say if„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_11.png)'
  prefs: []
  type: TYPE_IMG
- en: ConditionÔºå so if this is true„ÄÇThenÔºå our„ÄÇDivatives look like this„ÄÇ So the derivative
    with respect to the B is just 0„ÄÇ And so we only need thisÔºå so„ÄÇI say„ÄÇÂóØ„ÄÇSo it's
    two times lambda times W„ÄÇ And then in our updateÔºå we go in So we say„ÄÇThe new way
    is the old weight minus the learning rate times this„ÄÇ So I write this in one step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I say self dot W minus equal self dot learning rate times„ÄÇ And now here„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: We have„ÄÇTwo times self dot Lambda parameter times self dot W„ÄÇSo this is the
    first update„ÄÇOr if our condition is satisfied and we only need this update„ÄÇ And
    otherwise„ÄÇ we say self dots W minus equal self times L R the learning rate times„ÄÇ
    and let's again„ÄÇ have a look at the equation„ÄÇ So it's„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Two times lambda times w minus YÔºå I times XÔºå I so„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_17.png)
  prefs: []
  type: TYPE_NORMAL
- en: Two times„ÄÇOur lambda times„ÄÇW minus nuy dot„ÄÇ So I want to multiply our vectorsÔºå
    XÔºå I and„ÄÇWhy I so the y underscore of the current index„ÄÇSo this is our update
    for the W and our self dot„ÄÇB„ÄÇIs minus equal self times learning rate times the
    derivative and the derivative is only„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_19.png)'
  prefs: []
  type: TYPE_IMG
- en: Or just Y IÔºå So only„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: Why underscore of the index„ÄÇAnd now we are done„ÄÇ So this is the whole implementation„ÄÇAnd
    now let's test this„ÄÇ So I've written a little test script that will„ÄÇImport this
    SVM class„ÄÇ and then it will generate a„ÄÇSome test samples„ÄÇ So it will generate
    two glasses„ÄÇAnd then I will create my SVM classifier and fit the data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then I wrote a little function to visualize this so you can find the code
    on GitthubÔºå by the way„ÄÇ so please check it out for yourself and now if we run
    this„ÄÇ so let's say Python as we am underscore Te P„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_23.png)
  prefs: []
  type: TYPE_NORMAL
- en: And nowÔºå this should„ÄÇCalculate the weights and the biasÔºå and it should also
    plot the decision„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_25.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fun the the yellow line and the two lines on both sides here„ÄÇAnd we see that
    it's working„ÄÇ SoÔºå yeah„ÄÇThat's all about the S VM„ÄÇ I hope you enjoyed this„ÄÇ And
    if you like this„ÄÇ please subscribe to my channel and see you next timeÔºå bye„ÄÇ![](img/0c2f47f3d7d980a73cb14308e5d73802_27.png)
  prefs: []
  type: TYPE_NORMAL
