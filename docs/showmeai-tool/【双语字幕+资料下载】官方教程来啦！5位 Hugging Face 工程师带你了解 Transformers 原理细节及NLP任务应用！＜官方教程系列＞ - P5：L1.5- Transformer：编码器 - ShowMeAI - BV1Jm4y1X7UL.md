# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL1.5- Transformerï¼šç¼–ç å™¨ - ShowMeAI - BV1Jm4y1X7UL

In this videoï¼Œ we'll study the encoder architectureã€‚

An example of a popular anchor only architecture is Btï¼Œ which is the most popular model of its kindã€‚

Let's first start by understanding how it worksã€‚We'll use a small example using three wordsã€‚

 We use these as inputs and pass them through the encoderã€‚

We retrieve a numerical representation of each wordã€‚Hereï¼Œ for exampleã€‚

 the encode converts those three words welcome to NYC in these three sequences of numbersã€‚

The encoder outputs exactly one sequence of numbers per input wordã€‚

This numerical representation can also be called a feature vector or a feature tensorã€‚

Let's dive in this representationã€‚ It contains one vector per word that was passed through the encoderã€‚

Each of these vector is a numerical representation of the word in questionã€‚

The dimension of that vector is defined by the architecture of the model for the base bird modelã€‚

 it is 768ã€‚These representations contain the value of a wordï¼Œ but contextualizedã€‚For exampleã€‚

 the vector attributed to the word2 isn't the representation of only the two wordã€‚

It also takes into account the words around itï¼Œ which we call the contextã€‚

As in it looks to the left contextï¼Œ the words on the left of the one we studyingã€‚

 hear the word welcomeï¼Œ and the context on the rightï¼Œ here the wordnyCã€‚

 and it outputs a value for the word given its contextã€‚It is therefore a contextualized valueã€‚ğŸ˜Šã€‚

One could say that the vector of 768 values holds the meaning of the word within the textã€‚

It does this thanks to the self attention mechanismã€‚

The self attention mechanism relates to different positions or different words in a single sequence in order to compute a representation of that sequenceã€‚

As we've seen beforeï¼Œ this means that the resulting representation of a word has been affected by other words in the sequenceã€‚

ğŸ˜Šï¼ŒWe won't dive into these specifics hereï¼Œ but we' offer some further readings if you want to get a better understanding at what happens under the hoodã€‚

So when should one use an encodeï¼ŸEncodeders can be used as tenderalone models in a wide variety of tasksã€‚

ğŸ˜Šï¼ŒFor exampleï¼Œ Bertï¼Œ arguably the most famous transformer modelï¼Œ is a standalone anchor modelã€‚

 and at the time of release it be the state of the art in many sequence classification tasksã€‚

 question answering tasksï¼Œ and masked language modelling to only cite a fewã€‚ğŸ˜Šã€‚

The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequenceã€‚

This vector can then be handled down the road by additional neurons to make sense of themã€‚

Let's take a look at some examples where encodes really shineã€‚First of allã€‚

 mask language modeling or MLMã€‚It's the task of predicting a hidden word and a sequence of wordã€‚Hereã€‚

 for exampleï¼Œ we have hidden the word between my and isã€‚

This is one of the objectives with which Bert was trainedã€‚

 It was trained to predict hidden words in a sequenceã€‚Encodes shine in this scenario in particularã€‚

 as bidirectional information is crucial hereã€‚If we didn't have the words on the right is Silva and the dotã€‚

 then there is very little chance that Bt would have been able to identify name as the correct wordã€‚

The encoder needs to have a good understanding of the sequence in order to predict a masked wordã€‚

 as even if the text is grammatically correctã€‚It does not necessarily make sense in the context of the sequenceã€‚

As mentioned earlierï¼Œ encodes are good at doing sequence classificationã€‚ğŸ˜Šã€‚

Sentiment analysis is an example of sequence classificationã€‚

The model's aim is to identify the sentiment of a sequenceã€‚

It can range from giving a sequence a rating from one to five stars if doing review analysis to giving a positive or negative rating to a sequenceã€‚

 which is what is shown hereã€‚For exampleï¼Œ hereï¼Œ given the two sequencesã€‚

 we use the model to compute a prediction and to classify the sequences among these two classesã€‚

 positive and negativeã€‚While the two sequences are very similar containing the same wordsã€‚

 the meaning is entirely differentï¼Œ and the encoder model is able to grasp that differenceã€‚



![](img/9effa565b433264b98944ee23b41a7f0_1.png)

ã€‚

![](img/9effa565b433264b98944ee23b41a7f0_3.png)