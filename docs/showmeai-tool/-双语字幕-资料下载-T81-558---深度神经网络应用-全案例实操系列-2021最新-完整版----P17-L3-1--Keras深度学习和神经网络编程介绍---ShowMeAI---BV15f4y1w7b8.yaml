- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P17ï¼šL3.1- Kerasæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œç¼–ç¨‹ä»‹ç»
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P17ï¼šL3.1- Kerasæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œç¼–ç¨‹ä»‹ç»
    - ShowMeAI - BV15f4y1w7b8
- en: Hi this is Jeffy and welcome to applications of Deep neural networkss with Washington
    University in this video we're going to have a general introduction to deep neural
    networks and conceptually how they workã€‚This will allow us to build upon the Python
    that we learn in previous parts of this course and to actually construct neural
    networks for the latest on my AI course and projectsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯Jeffyï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨çš„è§†é¢‘ï¼Œåœ¨è¿™æ®µè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œä¸€èˆ¬ä»‹ç»ï¼Œä»¥åŠå®ƒä»¬çš„æ¦‚å¿µå¦‚ä½•è¿ä½œã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æœ¬è¯¾ç¨‹ä¹‹å‰éƒ¨åˆ†å­¦ä¹ çš„PythonåŸºç¡€ä¸Šæ„å»ºï¼Œå®é™…ä¸Šæ„å»ºå‡ºç¥ç»ç½‘ç»œï¼Œä»¥ä¾¿äºæˆ‘AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°è¿›å±•ã€‚
- en: click subscribe in the bell next to it to be notified of every new videoã€‚ neuralural
    networks have been around for a whileã€‚ Deep learning is just the ability to train
    neural networks that are very deepã€‚ğŸ˜Šã€‚![](img/8444b070a2a80e4ecb951689e6339e17_1.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»æ—è¾¹çš„å°é“ƒé“›è®¢é˜…ï¼Œä»¥ä¾¿æ”¶åˆ°æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚ç¥ç»ç½‘ç»œå·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´äº†ã€‚æ·±åº¦å­¦ä¹ å°±æ˜¯è®­ç»ƒéå¸¸æ·±çš„ç¥ç»ç½‘ç»œçš„èƒ½åŠ›ã€‚ğŸ˜Šï¼[](img/8444b070a2a80e4ecb951689e6339e17_1.png)
- en: You might have seen other machine learning models and other classes like support
    vector machines and gradient boostting and XG Bot and light GBBM and all these
    various ways of training a model based on dataã€‚Neurural networks can be drop in
    replacements for other models like thisã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è§è¿‡å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹å’Œå…¶ä»–ç±»åˆ«ï¼Œæ¯”å¦‚æ”¯æŒå‘é‡æœºã€æ¢¯åº¦æå‡ã€XGBoostå’ŒLightGBMï¼Œä»¥åŠæ‰€æœ‰è¿™äº›åŸºäºæ•°æ®è®­ç»ƒæ¨¡å‹çš„ä¸åŒæ–¹å¼ã€‚ç¥ç»ç½‘ç»œå¯ä»¥ä½œä¸ºè¿™äº›æ¨¡å‹çš„æ›¿ä»£å“ã€‚
- en: The neural network simply takes in the data that you would have normally sent
    to a support vector machine or other modelã€‚And outputs either a classification
    result where it attempts to classify or determine the type of what the input was
    sent to itã€‚Or it can be regression and it outputs a number based on the data that
    you're trying to have the neural network predict fromã€‚Howeverï¼Œ neural networksï¼Œ
    their real power lies in their ability to take end data and produce data in ways
    that don't fit into your typical classification and regressionã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œç®€å•åœ°æ¥å—ä½ é€šå¸¸ä¼šå‘é€ç»™æ”¯æŒå‘é‡æœºæˆ–å…¶ä»–æ¨¡å‹çš„æ•°æ®ï¼Œå¹¶è¾“å‡ºåˆ†ç±»ç»“æœï¼Œè¯•å›¾å¯¹è¾“å…¥çš„å†…å®¹è¿›è¡Œåˆ†ç±»æˆ–ç¡®å®šç±»å‹ã€‚æˆ–è€…å®ƒå¯ä»¥è¿›è¡Œå›å½’ï¼Œå¹¶æ ¹æ®ä½ æƒ³è®©ç¥ç»ç½‘ç»œé¢„æµ‹çš„æ•°æ®è¾“å‡ºä¸€ä¸ªæ•°å­—ã€‚ç„¶è€Œï¼Œç¥ç»ç½‘ç»œçš„çœŸæ­£å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒä»¬èƒ½å¤Ÿå¤„ç†æœ€ç»ˆæ•°æ®å¹¶ä»¥ä¸é€‚åˆå…¸å‹åˆ†ç±»å’Œå›å½’çš„æ–¹å¼ç”Ÿæˆæ•°æ®ã€‚
- en: Type modelsï¼Œ for exampleï¼Œ you can input an image into a neural networkã€‚ You
    can even have a neural networkï¼Œ accept an image and produce another imageã€‚So it
    gets veryã€‚Expressive what you can actually do with the neural network is the input
    can be just about anything and the output can be just about anythingã€‚ and the
    input and output by no means have to be of the same typeã€‚In other modelsï¼Œ you
    wouldã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å‹æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œä½ å¯ä»¥å°†å›¾åƒè¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚ä½ ç”šè‡³å¯ä»¥è®©ç¥ç»ç½‘ç»œæ¥æ”¶ä¸€å¼ å›¾åƒå¹¶ç”Ÿæˆå¦ä¸€å¼ å›¾åƒã€‚å› æ­¤ï¼Œç¥ç»ç½‘ç»œæ‰€èƒ½åšçš„éå¸¸å¯Œæœ‰è¡¨ç°åŠ›ï¼Œè¾“å…¥å‡ ä¹å¯ä»¥æ˜¯ä»»ä½•ä¸œè¥¿ï¼Œè¾“å‡ºä¹Ÿå¯ä»¥æ˜¯ä»»ä½•ä¸œè¥¿ï¼Œè¾“å…¥å’Œè¾“å‡ºå¹¶ä¸ä¸€å®šè¦æ˜¯åŒä¸€ç±»å‹ã€‚åœ¨å…¶ä»–æ¨¡å‹ä¸­ï¼Œä½ ä¼šã€‚
- en: Simply send in a one dimensional vector so a list of predictorsã€‚You can alsoï¼Œ
    thoughã€‚ with a neural networkï¼Œ pass in a 2D matrixã€‚ Nowï¼Œ this is where you start
    to pass inã€‚ say an image with a grid of pixelsã€‚ The power of the neural network
    is that it realizes the pixels that are near each other are more important to
    each otherã€‚ whereas the other model typesã€‚ change in the order of the input factor
    really has no effect on anythingã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€å‘é€ä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œå³ä¸€ç»„é¢„æµ‹å˜é‡ã€‚ç„¶è€Œï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œï¼Œä½ ä¹Ÿå¯ä»¥ä¼ å…¥ä¸€ä¸ªäºŒç»´çŸ©é˜µã€‚ç°åœ¨ï¼Œè¿™å°±æ˜¯ä½ å¼€å§‹ä¼ å…¥ï¼Œæ¯”å¦‚ä¸€ä¸ªå¸¦æœ‰åƒç´ ç½‘æ ¼çš„å›¾åƒçš„åœ°æ–¹ã€‚ç¥ç»ç½‘ç»œçš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒæ„è¯†åˆ°ç›¸é‚»çš„åƒç´ å½¼æ­¤ä¹‹é—´æ›´é‡è¦ï¼Œè€Œå…¶ä»–æ¨¡å‹ç±»å‹ä¸­ï¼Œè¾“å…¥å› ç´ çš„é¡ºåºå˜åŒ–å¯¹ä»»ä½•äº‹æƒ…éƒ½æ²¡æœ‰å½±å“ã€‚
- en: It can also pass in a 3D matrix or 3D tensorã€‚This is would essentially be a
    color image where that third dimension is specifying the color of the individual
    pixels that you're passing inã€‚And dimensionsï¼Œ it can be used in several different
    ways with neural networks when you talk about the dimensionality of the neural
    networkã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¹Ÿå¯ä»¥ä¼ å…¥ä¸€ä¸ªä¸‰ç»´çŸ©é˜µæˆ–ä¸‰ç»´å¼ é‡ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€å¹…å½©è‰²å›¾åƒï¼Œå…¶ä¸­ç¬¬ä¸‰ç»´æŒ‡å®šä½ ä¼ å…¥çš„å•ä¸ªåƒç´ çš„é¢œè‰²ã€‚è€Œåœ¨è°ˆè®ºç¥ç»ç½‘ç»œçš„ç»´åº¦æ—¶ï¼Œè¿™ç§ç»´åº¦å¯ä»¥ä»¥å‡ ç§ä¸åŒçš„æ–¹å¼ä½¿ç”¨ã€‚
- en: Usually you're talking about what the input vector or input matrix looks likeã€‚
    how many input neurons do you have and how are they arrangedï¼Œ are they a grid
    or they a boxï¼Ÿ
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ä½ ä¼šè°ˆè®ºè¾“å…¥å‘é‡æˆ–è¾“å…¥çŸ©é˜µçš„æ ·å­ã€‚ä½ æœ‰å¤šå°‘ä¸ªè¾“å…¥ç¥ç»å…ƒï¼Œå®ƒä»¬æ˜¯å¦‚ä½•æ’åˆ—çš„ï¼Œæ˜¯ç½‘æ ¼è¿˜æ˜¯ç›’å­ï¼Ÿ
- en: Dimenssions can also refer to the number of weights that are in a neural networkã€‚Nowã€‚
    traditional modelsã€‚You would talk about regression and classification and neural
    networks do this tooã€‚ the output neurons of the neural network become either the
    single regression output or the classificationã€‚So a regression neural network
    like you see hereï¼Œ now these are two example neural networks that I put togetherã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç»´åº¦ä¹Ÿå¯ä»¥æŒ‡ç¥ç»ç½‘ç»œä¸­çš„æƒé‡æ•°é‡ã€‚ç°åœ¨ï¼Œä¼ ç»Ÿæ¨¡å‹ä¸­ä¼šè°ˆåˆ°å›å½’å’Œåˆ†ç±»ï¼Œç¥ç»ç½‘ç»œä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç¥ç»ç½‘ç»œçš„è¾“å‡ºç¥ç»å…ƒå˜æˆå•ä¸€çš„å›å½’è¾“å‡ºæˆ–åˆ†ç±»è¾“å‡ºã€‚æ‰€ä»¥åƒä½ è¿™é‡Œçœ‹åˆ°çš„å›å½’ç¥ç»ç½‘ç»œï¼Œè¿™é‡Œæœ‰ä¸¤ä¸ªæˆ‘ç»„åˆåœ¨ä¸€èµ·çš„ç¤ºä¾‹ç¥ç»ç½‘ç»œã€‚
- en: I work in the life insurance industry so you'll see a number of examples from
    me sort of in an insure tech sort of way where you've gotã€‚Inputs that are related
    to medical records and other things that you'd be interested in for life insuranceã€‚Here
    you're asking the neural network to predict the maximum face amountã€‚ So how much
    should we insure somebody forï¼Œ what's the maximum amount we would go on the risk
    for with an individualã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨å¯¿é™©è¡Œä¸šå·¥ä½œï¼Œå› æ­¤ä½ å°†çœ‹åˆ°æˆ‘æä¾›çš„è®¸å¤šä¾‹å­ï¼Œç±»ä¼¼äºä¿é™©ç§‘æŠ€çš„æ–¹å¼ï¼Œå…¶ä¸­åŒ…å«ä¸åŒ»ç–—è®°å½•ç­‰ç›¸å…³çš„è¾“å…¥ï¼Œè¿™äº›æ˜¯ä½ åœ¨å¯¿é™©ä¸­æ„Ÿå…´è¶£çš„å†…å®¹ã€‚åœ¨è¿™é‡Œï¼Œä½ è¯·æ±‚ç¥ç»ç½‘ç»œé¢„æµ‹æœ€å¤§ä¿é¢ã€‚é‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä¸ºæŸäººæŠ•ä¿å¤šå°‘ï¼Œé’ˆå¯¹ä¸ªäººçš„æœ€å¤§é£é™©é‡‘é¢æ˜¯å¤šå°‘ã€‚
- en: Classification neural networksï¼Œ those produce classesã€‚ so we would maybe have
    a preferred standard substandard or a declineã€‚That just puts the the potential
    insurance app into the correct bucketã€‚ So regression classificationã€‚ These are
    your traditional types of modelï¼Œ and we'll see later on in this class that neural
    networks can have much more complicated outputs than justã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç¥ç»ç½‘ç»œä¼šäº§ç”Ÿç±»åˆ«ã€‚å› æ­¤æˆ‘ä»¬å¯èƒ½ä¼šæœ‰ä¸€ä¸ªä¼˜é€‰æ ‡å‡†ã€æ¬¡æ ‡å‡†æˆ–ä¸‹é™ã€‚è¿™åªæ˜¯å°†æ½œåœ¨çš„ä¿é™©ç”³è¯·æ”¾å…¥æ­£ç¡®çš„æ¡¶ä¸­ã€‚å› æ­¤ï¼Œå›å½’åˆ†ç±»ã€‚è¿™äº›æ˜¯ä½ ä¼ ç»Ÿçš„æ¨¡å‹ç±»å‹ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬è¯¾ç¨‹åé¢çœ‹åˆ°ï¼Œç¥ç»ç½‘ç»œå¯ä»¥äº§ç”Ÿæ¯”è¿™æ›´å¤æ‚çš„è¾“å‡ºã€‚
- en: Classification or regressionï¼Œ a neural network can even be both classification
    and regression at the same timeã€‚The output neuronsï¼Œ so the ones on the far rightï¼Œ
    here there's just oneã€‚ if it's a regression neural networkï¼Œ it's always going
    to be just one output neuronã€‚Here we have additional output neuronsï¼Œ one output
    neuron for each classã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»æˆ–å›å½’ï¼Œç¥ç»ç½‘ç»œç”šè‡³å¯ä»¥åŒæ—¶è¿›è¡Œåˆ†ç±»å’Œå›å½’ã€‚è¾“å‡ºç¥ç»å…ƒï¼Œä¹Ÿå°±æ˜¯æœ€å³ä¾§çš„é‚£äº›ï¼Œè¿™é‡Œåªæœ‰ä¸€ä¸ªã€‚å¦‚æœæ˜¯å›å½’ç¥ç»ç½‘ç»œï¼Œå®ƒæ€»æ˜¯åªæœ‰ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚è¿™é‡Œæˆ‘ä»¬æœ‰é¢å¤–çš„è¾“å‡ºç¥ç»å…ƒï¼Œæ¯ä¸ªç±»åˆ«å¯¹åº”ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚
- en: That's how you can tell a classification neural network if it's a binary classification
    neural networkã€‚ meaning it's only classifying between two thingsï¼Œ usually it'll
    just have one output neuron and that specifies the probability of it being one
    of those classesã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ç¥ç»ç½‘ç»œçš„åˆ†ç±»æ–¹å¼æ¥åˆ¤æ–­å®ƒæ˜¯å¦æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ç¥ç»ç½‘ç»œï¼Œè¿™æ„å‘³ç€å®ƒåªæ˜¯åœ¨åˆ†ç±»ä¸¤ä¸ªäº‹ç‰©ï¼Œé€šå¸¸å®ƒåªæœ‰ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒï¼ŒæŒ‡å®šäº†å®ƒå±äºå…¶ä¸­ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚
- en: Howeverï¼Œ if you're dealing with a multi class classification withã€‚With three
    or more classesã€‚ usually you're going toã€‚You're going to have one output neuron
    per classã€‚ You would never have a single class classification neural network because
    there's just one classã€‚ it would always be that classï¼Œ so you can have to have
    at least two classes so that there's at least something to differentiate between
    for the neural network to classifyã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœä½ åœ¨å¤„ç†å¤šç±»åˆ«åˆ†ç±»ï¼Œç±»åˆ«æ•°ä¸ºä¸‰æˆ–æ›´å¤šï¼Œé€šå¸¸ä½ ä¼šä¸ºæ¯ä¸ªç±»åˆ«æœ‰ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚ä½ ç»ä¸ä¼šæœ‰å•ä¸€ç±»åˆ«çš„åˆ†ç±»ç¥ç»ç½‘ç»œï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªç±»åˆ«ã€‚å®ƒæ€»æ˜¯é‚£ä¸ªç±»åˆ«ï¼Œå› æ­¤ä½ è‡³å°‘éœ€è¦æœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œä»¥ä¾¿ç¥ç»ç½‘ç»œæœ‰åŒºåˆ«çš„å¯¹è±¡è¿›è¡Œåˆ†ç±»ã€‚
- en: This is the structure of a neural networkã€‚They have multiple layersã€‚ Now we'll
    see that there are additional layer types and other things that will make this
    more complicatedã€‚ but for tabular neural networksï¼Œ this is where the input to
    it looks sort of like rows and columns from Excelã€‚ This is what it'll look likeã€‚You're
    going to have your input layerã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚å®ƒä»¬æœ‰å¤šä¸ªå±‚ã€‚ç°åœ¨æˆ‘ä»¬å°†çœ‹åˆ°è¿˜æœ‰å…¶ä»–ç±»å‹çš„å±‚å’Œå…¶ä»–ä¼šä½¿å…¶æ›´å¤æ‚çš„å†…å®¹ã€‚ä½†å¯¹äºè¡¨æ ¼ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œå®ƒçš„è¾“å…¥çœ‹èµ·æ¥æœ‰ç‚¹åƒExcelä¸­çš„è¡Œå’Œåˆ—ã€‚è¿™å°±æ˜¯å®ƒçš„æ ·å­ã€‚ä½ ä¼šæœ‰è¾“å…¥å±‚ã€‚
- en: These input neurons are the values that come into the neural networkã€‚ Then you're
    going to have several hidden layersï¼Œ finally going to the output layerã€‚These are
    bias neuronsï¼Œ you don't send input into those directlyï¼Œ they are simply thereã€‚2ã€‚To
    give the neural network additional predictive powerã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¾“å…¥ç¥ç»å…ƒæ˜¯è¿›å…¥ç¥ç»ç½‘ç»œçš„å€¼ã€‚æ¥ä¸‹æ¥ï¼Œä½ ä¼šæœ‰å‡ ä¸ªéšè—å±‚ï¼Œæœ€ååˆ°è¾¾è¾“å‡ºå±‚ã€‚è¿™äº›æ˜¯åç½®ç¥ç»å…ƒï¼Œä½ ä¸ä¼šç›´æ¥å‘å®ƒä»¬å‘é€è¾“å…¥ï¼Œå®ƒä»¬ä»…ä»…æ˜¯å­˜åœ¨äºè¿™é‡Œï¼Œä»¥å¢å¼ºç¥ç»ç½‘ç»œçš„é¢„æµ‹èƒ½åŠ›ã€‚
- en: we'll see exactly what bias neurons are for in a momentï¼Œ they handleã€‚They handle
    a situation where the inputs are both zeroã€‚ but you don't necessarily want the
    output to also be zeroã€‚The arrows are the weights between the various entities
    in theã€‚And the neural networkã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é©¬ä¸Šä¼šçœ‹åˆ°åç½®ç¥ç»å…ƒçš„å…·ä½“ä½œç”¨ï¼Œå®ƒä»¬å¤„ç†è¾“å…¥éƒ½ä¸ºé›¶çš„æƒ…å†µã€‚ä½†ä½ å¹¶ä¸ä¸€å®šå¸Œæœ›è¾“å‡ºä¹Ÿä¸ºé›¶ã€‚ç®­å¤´æ˜¯ç¥ç»ç½‘ç»œä¸­å„ä¸ªå®ä½“ä¹‹é—´çš„æƒé‡ã€‚
- en: In hidden layersï¼Œ you can have lots of hidden layers in deep learningï¼Œ hundreds
    of themã€‚It's usually four types of neurons and a neural network input neurons
    take in the input to the rest of the neural networkã€‚Hidden neuronsï¼Œ neither they're
    hidden because they're between the input and output neuronsã€‚ input neurons receive
    the input for the neural networkã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éšè—å±‚ä¸­ï¼Œä½ å¯ä»¥åœ¨æ·±åº¦å­¦ä¹ ä¸­æœ‰å¾ˆå¤šéšè—å±‚ï¼Œæ•°ä»¥ç™¾è®¡ã€‚é€šå¸¸æœ‰å››ç§ç±»å‹çš„ç¥ç»å…ƒï¼Œç¥ç»ç½‘ç»œä¸­çš„è¾“å…¥ç¥ç»å…ƒæ¥æ”¶å…¶ä½™ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€‚éšè—ç¥ç»å…ƒï¼Œå› ä¸ºå®ƒä»¬ä½äºè¾“å…¥å’Œè¾“å‡ºç¥ç»å…ƒä¹‹é—´ã€‚è¾“å…¥ç¥ç»å…ƒæ¥æ”¶ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€‚
- en: outputput neurons receive the output that are sent out of the neural network
    hidden or between thatã€‚Context neurons will see more about those when we get into
    time series and recurrent neural networksã€‚ they maintain state between calls to
    the neural networkã€‚And then bias neuronsã€‚ they are essentially like the y intercept
    andã€‚Traditional mathematicsï¼Œ linearã€‚Linear equationsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç¥ç»å…ƒæ¥æ”¶ä»ç¥ç»ç½‘ç»œå‘é€å‡ºçš„è¾“å‡ºï¼Œæˆ–è€…åœ¨è¿™ä¹‹é—´ã€‚ä¸Šä¸‹æ–‡ç¥ç»å…ƒåœ¨æˆ‘ä»¬è¿›å…¥æ—¶é—´åºåˆ—å’Œé€’å½’ç¥ç»ç½‘ç»œæ—¶ä¼šæ›´å¤šäº†è§£å®ƒä»¬ã€‚å®ƒä»¬åœ¨å¯¹ç¥ç»ç½‘ç»œçš„è°ƒç”¨ä¹‹é—´ä¿æŒçŠ¶æ€ã€‚ç„¶åæ˜¯åç½®ç¥ç»å…ƒï¼Œå®ƒä»¬æœ¬è´¨ä¸Šå°±åƒyæˆªè·å’Œä¼ ç»Ÿæ•°å­¦ä¸­çš„çº¿æ€§æ–¹ç¨‹ã€‚
- en: You also have several layer types that these neurons go intoã€‚ there's the input
    layer that receives the inputï¼Œ the output layer that sends the output from the
    neural network and then the hidden layers between thatã€‚In a later partï¼Œ we're
    going to manually calculate the output from a neural networkã€‚But for nowã€‚ we'll
    see that the calculation that a neural network actually goes through is not that
    complexã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜æœ‰å‡ ç§å±‚ç±»å‹ï¼Œç¥ç»å…ƒä¼šè¿›å…¥è¿™äº›å±‚ã€‚è¾“å…¥å±‚æ¥æ”¶è¾“å…¥ï¼Œè¾“å‡ºå±‚å‘é€ç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼Œç„¶åæ˜¯éšè—å±‚ã€‚åœ¨åé¢çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨è®¡ç®—ç¥ç»ç½‘ç»œçš„è¾“å‡ºã€‚ä½†ç°åœ¨ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ç¥ç»ç½‘ç»œå®é™…ç»å†çš„è®¡ç®—å¹¶ä¸å¤æ‚ã€‚
- en: It's essentially a weighted sum passed into a activation functionã€‚So the input
    to calculating one hidden neuronã€‚Or an output neuron in the neural networkã€‚Essentially
    takes in the feature vector or the vector coming into itã€‚ So if we were calculating
    for this neuron down hereã€‚The input vector would be 1ï¼Œ2ï¼Œ3ã€‚ these valuesã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠ æƒå’Œï¼Œä¼ é€’åˆ°æ¿€æ´»å‡½æ•°ã€‚æ‰€ä»¥è®¡ç®—ä¸€ä¸ªéšè—ç¥ç»å…ƒæˆ–ç¥ç»ç½‘ç»œä¸­çš„è¾“å‡ºç¥ç»å…ƒçš„è¾“å…¥ï¼Œæœ¬è´¨ä¸Šæ˜¯æ¥æ”¶ç‰¹å¾å‘é‡æˆ–è¿›å…¥å®ƒçš„å‘é‡ã€‚å¦‚æœæˆ‘ä»¬ä¸ºä¸‹é¢çš„è¿™ä¸ªç¥ç»å…ƒè®¡ç®—ï¼Œè¾“å…¥å‘é‡å°±æ˜¯1ï¼Œ2ï¼Œ3ã€‚è¿™äº›å€¼ã€‚
- en: and essentiallyï¼Œ you would multiply input 1 times weight 1ï¼Œ input 2 times weight
    2ã€‚And put three times weight threeã€‚Some those altogetherã€‚That value then gets
    passed into the activation functionï¼Œ and that becomes the output of that neuronã€‚And
    that's what this equation is basically showing you hereã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œä½ ä¼šå°†è¾“å…¥1ä¹˜ä»¥æƒé‡1ï¼Œè¾“å…¥2ä¹˜ä»¥æƒé‡2ï¼Œè¾“å…¥3ä¹˜ä»¥æƒé‡3ã€‚å°†è¿™äº›ç›¸åŠ ã€‚ç„¶åè¿™ä¸ªå€¼ä¼ é€’åˆ°æ¿€æ´»å‡½æ•°ï¼Œè¿™å°±æ˜¯è¯¥ç¥ç»å…ƒçš„è¾“å‡ºã€‚è¿™ä¸ªæ–¹ç¨‹åŸºæœ¬ä¸Šå°±æ˜¯åœ¨å‘ä½ å±•ç¤ºè¿™ä¸ªã€‚
- en: It' essentially the summation of all of thetasï¼Œ thetas or weightsï¼Œ timesï¼Œ all
    of the x'sã€‚ the x's are inputsï¼Œ and then phi is the the activation functionã€‚ And
    this is basically done over and over and over again to calculate every hidden
    and output neuronã€‚In the neural networkã€‚This gives you an example of thisï¼Œ the
    input is one and 2ã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœ¬è´¨ä¸Šæ˜¯æ‰€æœ‰Î¸çš„æ€»å’Œï¼ŒÎ¸æˆ–æƒé‡ï¼Œä¹˜ä»¥æ‰€æœ‰çš„xã€‚xæ˜¯è¾“å…¥ï¼Œç„¶åÏ†æ˜¯æ¿€æ´»å‡½æ•°ã€‚è¿™åŸºæœ¬ä¸Šåœ¨ç¥ç»ç½‘ç»œä¸­åå¤è¿›è¡Œï¼Œä»¥è®¡ç®—æ¯ä¸ªéšè—å’Œè¾“å‡ºç¥ç»å…ƒã€‚è¿™ç»™ä½ ä¸€ä¸ªç¤ºä¾‹ï¼Œè¾“å…¥æ˜¯1å’Œ2ã€‚
- en: Now this third neuron hereï¼Œ that's actually your bias neuronã€‚And the way that
    it gets represented as the bias neuroon is we cancateate a one onto the end of
    thisã€‚ So one is going into one first neuron 2 goes into the second and this oneã€‚Goes
    into the thirdã€‚Neuron that becomes basically the biasã€‚ So whatever this weight
    is just gets added to it kind of like an interceptã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™ä¸ªç¬¬ä¸‰ä¸ªç¥ç»å…ƒï¼Œå®é™…ä¸Šæ˜¯ä½ çš„åç½®ç¥ç»å…ƒã€‚å®ƒä½œä¸ºåç½®ç¥ç»å…ƒçš„è¡¨ç¤ºæ–¹å¼æ˜¯å°†ä¸€ä¸ªä¸€è¿æ¥åˆ°æœ«å°¾ã€‚æ‰€ä»¥ä¸€ä¸ªè¾“å…¥åˆ°ç¬¬ä¸€ä¸ªç¥ç»å…ƒï¼ŒäºŒè¾“å…¥åˆ°ç¬¬äºŒä¸ªï¼Œè¿™ä¸ªåˆ™è¾“å…¥åˆ°ç¬¬ä¸‰ä¸ªã€‚è¿™ä¸ªç¥ç»å…ƒåŸºæœ¬ä¸Šå°±æ˜¯åç½®ã€‚å› æ­¤ï¼Œè¿™ä¸ªæƒé‡ä¼šè¢«åŠ ä¸Šï¼Œå°±åƒä¸€ä¸ªæˆªè·ã€‚
- en: Since this is oneï¼Œ we're multiplying one times the third weightã€‚ That's basically
    how bias neurons neurons workã€‚We might have this as our weightsã€‚ So those might
    be the three weight valuesã€‚ The third one would be called the bias valueã€‚ We multiply
    each of these inputs by each of these weight values in summitã€‚ and this 0ã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™æ˜¯1ï¼Œæˆ‘ä»¬å°†1ä¹˜ä»¥ç¬¬ä¸‰ä¸ªæƒé‡ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯åç½®ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬çš„æƒé‡å¯èƒ½æ˜¯è¿™æ ·ã€‚å› æ­¤ï¼Œè¿™å¯èƒ½æ˜¯ä¸‰ä¸ªæƒé‡å€¼ã€‚ç¬¬ä¸‰ä¸ªç§°ä¸ºåç½®å€¼ã€‚æˆ‘ä»¬å°†è¿™äº›è¾“å…¥ä¹˜ä»¥æ¯ä¸ªæƒé‡å€¼å¹¶æ±‚å’Œã€‚è¿™æ˜¯0ã€‚
- en: 8 becomes the summation that is then passed to the activation functionã€‚Activation
    functions are just functions that introduce non linearity into the neural networkã€‚
    so that everything's not linear rectified linear unitã€‚ we will see is the most
    popular of the activation functions in modern deepã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 8å˜æˆäº†è¢«ä¼ é€’åˆ°æ¿€æ´»å‡½æ•°çš„æ€»å’Œã€‚æ¿€æ´»å‡½æ•°åªæ˜¯å¼•å…¥éçº¿æ€§çš„å‡½æ•°åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œä½¿å¾—ä¸€åˆ‡éƒ½ä¸æ˜¯çº¿æ€§çš„ä¿®æ­£çº¿æ€§å•å…ƒã€‚æˆ‘ä»¬å°†çœ‹åˆ°è¿™æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ ä¸­æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ã€‚
- en: Deep learning and other family members related to rectified linear units like
    the leakyã€‚Ralueã€‚Soft Mac is always used in classification neural networks for
    the final outputã€‚Of the neural networkã€‚ this ensures that all of the output neurons
    sum to oneã€‚ because you'd like those output neurons be the probability of each
    of the classes that the neural network is trying to classify in softftmax just
    ensures that all those probabilities are truly probabilities and they add up to
    one as probabilities usually doã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ åŠå…¶ä»–ä¸ä¿®æ­£çº¿æ€§å•å…ƒç›¸å…³çš„å®¶æ—æˆå‘˜ï¼Œå¦‚leaky ReLUã€‚softmaxé€šå¸¸ç”¨äºåˆ†ç±»ç¥ç»ç½‘ç»œçš„æœ€ç»ˆè¾“å‡ºã€‚è¿™ç¡®ä¿äº†æ‰€æœ‰è¾“å‡ºç¥ç»å…ƒçš„æ€»å’Œä¸º1ï¼Œå› ä¸ºä½ å¸Œæœ›è¿™äº›è¾“å‡ºç¥ç»å…ƒä»£è¡¨ç¥ç»ç½‘ç»œè¯•å›¾åˆ†ç±»çš„æ¯ä¸ªç±»çš„æ¦‚ç‡ï¼Œsoftmaxç¡®ä¿è¿™äº›æ¦‚ç‡æ˜¯çœŸæ­£çš„æ¦‚ç‡ï¼Œå¹¶ä¸”åŠ èµ·æ¥ä¸º1ï¼Œæ­£å¦‚æ¦‚ç‡é€šå¸¸åº”æœ‰çš„é‚£æ ·ã€‚
- en: The rectified linear unit is a pretty simple activation functionï¼Œ but extremely
    effectiveã€‚You're essentially taking the max of0 and xï¼Œ so x is the value that
    was passed inã€‚Softmax looks like thisï¼Œ againï¼Œ it's essentially summing them togetherã€‚
    dividing each one by the summationï¼Œ that normalization is what ensures that they
    add up all to 1ã€‚0ã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®æ­£çº¿æ€§å•å…ƒæ˜¯ä¸€ä¸ªç›¸å½“ç®€å•çš„æ¿€æ´»å‡½æ•°ï¼Œä½†æå…¶æœ‰æ•ˆã€‚ä½ å®é™…ä¸Šæ˜¯åœ¨å–0å’Œxçš„æœ€å¤§å€¼ï¼Œæ‰€ä»¥xæ˜¯ä¼ å…¥çš„å€¼ã€‚softmaxçœ‹èµ·æ¥åƒè¿™æ ·ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯å°†å®ƒä»¬ç›¸åŠ ï¼Œç„¶åå°†æ¯ä¸ªå€¼é™¤ä»¥æ€»å’Œï¼Œè¿™ç§å½’ä¸€åŒ–ç¡®ä¿å®ƒä»¬åŠ èµ·æ¥éƒ½ä¸º1ã€‚
- en: By the wayï¼Œ if you'd like to experiment with the softftmax and see how these
    individual values are createdã€‚ I have a JavaScript example that I have a link
    to right hereã€‚And why is the rectified linear unitï¼Ÿ
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œå¦‚æœä½ æƒ³å®éªŒä¸€ä¸‹softmaxå¹¶çœ‹çœ‹è¿™äº›å•ç‹¬çš„å€¼æ˜¯å¦‚ä½•ç”Ÿæˆçš„ã€‚æˆ‘æœ‰ä¸€ä¸ªJavaScriptç¤ºä¾‹ï¼Œé“¾æ¥å°±åœ¨è¿™é‡Œã€‚é‚£ä¸ºä»€ä¹ˆæ˜¯ä¿®æ­£çº¿æ€§å•å…ƒå‘¢ï¼Ÿ
- en: So popularï¼Œ why is this such a popularï¼ŸActation functionã€‚Used to be before the
    deep learning daysã€‚ the most common activation functions were the hyperbolic tangent
    and sigmoidã€‚Sigmoid is shown hereã€‚So as you would input values into itï¼Œ it would
    have the squashing effectã€‚As you went to negative infinity or positive infinityï¼Œ
    And that was desired behavior that workedã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆå—æ¬¢è¿çš„åŸå› æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿä¸ºä»€ä¹ˆè¿™æ˜¯å¦‚æ­¤å—æ¬¢è¿çš„æ¿€æ´»å‡½æ•°ï¼Ÿåœ¨æ·±åº¦å­¦ä¹ ä¹‹å‰ï¼Œæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯åŒæ›²æ­£åˆ‡å’Œsigmoidã€‚è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯sigmoidã€‚å½“ä½ å°†å€¼è¾“å…¥åˆ°å®ƒæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿå‹ç¼©æ•ˆæœã€‚æ— è®ºæ˜¯è´Ÿæ— ç©·å¤§è¿˜æ˜¯æ­£æ— ç©·å¤§ï¼Œè¿™ç§è¡Œä¸ºéƒ½æ˜¯æ‰€æœŸæœ›çš„ã€‚
- en: that worked quite wellã€‚Wellï¼Œ you're typically optimizing these neural networks
    throughã€‚ through gradient descentã€‚ So you're taking the derivative of the error
    functionã€‚ So this is the error functionã€‚As we change a weightã€‚ So as we change
    one weightï¼Œ the error goes upã€‚ The error goes downã€‚You'd like to get the error
    at the minimum locationã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å·¥ä½œå¾—å¾ˆå¥½ã€‚é€šå¸¸ï¼Œä½ æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿™äº›ç¥ç»ç½‘ç»œã€‚å› æ­¤ï¼Œä½ æ˜¯åœ¨è®¡ç®—è¯¯å·®å‡½æ•°çš„å¯¼æ•°ã€‚è¿™æ˜¯è¯¯å·®å‡½æ•°ã€‚å½“æˆ‘ä»¬æ”¹å˜ä¸€ä¸ªæƒé‡æ—¶ï¼Œè¯¯å·®ä¼šä¸Šå‡ï¼Œè¯¯å·®ä¼šä¸‹é™ã€‚ä½ å¸Œæœ›å°†è¯¯å·®é™åˆ°æœ€å°ã€‚
- en: except you can't see this entire graph at onceã€‚ You'd have to literally calculate
    the neural network for every number betweenã€‚A reasonable rangeã€‚ You only really
    see the dot that you're actually oddã€‚ but that's what the derivative is for in
    calculusã€‚ You take the derivativeã€‚ and it gives you the slope or the instantaneous
    rate of change of wherever the weight is atã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åªä¸è¿‡ä½ æ— æ³•ä¸€æ¬¡çœ‹åˆ°æ•´ä¸ªå›¾å½¢ã€‚ä½ å®é™…ä¸Šå¿…é¡»ä¸ºåˆç†èŒƒå›´å†…çš„æ¯ä¸ªæ•°å­—è®¡ç®—ç¥ç»ç½‘ç»œã€‚ä½ åªèƒ½çœ‹åˆ°ä½ å®é™…ä¸Šæ‰€å¤„çš„ç‚¹ï¼Œä½†è¿™å°±æ˜¯å¾®ç§¯åˆ†ä¸­çš„å¯¼æ•°çš„ä½œç”¨ã€‚ä½ å–å¯¼æ•°ï¼Œå®ƒä¼šç»™ä½ æƒé‡æ‰€åœ¨çš„ç¬æ—¶å˜åŒ–ç‡æˆ–æ–œç‡ã€‚
- en: And you can tell by the slope of this valueã€‚ This has a negative slopeã€‚ So that
    means we need to increase the weight to move towards the minimumã€‚ You're always
    changing the weight by the inverse of the sign of the of the slope or the gradientã€‚
    This is typically called gradient in machine learningã€‚So if we look at the derivativeã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡è¿™ä¸ªå€¼çš„æ–œç‡æ¥åˆ¤æ–­ã€‚è¿™æœ‰ä¸€ä¸ªè´Ÿæ–œç‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦å¢åŠ æƒé‡ä»¥æœå‘æœ€å°å€¼ç§»åŠ¨ã€‚ä½ æ€»æ˜¯é€šè¿‡æ–œç‡æˆ–æ¢¯åº¦çš„ç¬¦å·çš„åå‘æ¥æ”¹å˜æƒé‡ã€‚è¿™é€šå¸¸åœ¨æœºå™¨å­¦ä¹ ä¸­è¢«ç§°ä¸ºæ¢¯åº¦ã€‚å› æ­¤å¦‚æœæˆ‘ä»¬æŸ¥çœ‹å¯¼æ•°ã€‚
- en: Of the sigmoid functionï¼Œ you can see it by the dashed lineã€‚Notice how it quickly
    converges to zero this since that derivative is called a gradientã€‚The fact that
    it would go to0 or vanish means we would have a vanishing gradientã€‚ This is the
    vanishing gradient problemã€‚In the deep learning solvedã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨sigmoidå‡½æ•°ä¸­ï¼Œä½ å¯ä»¥é€šè¿‡è™šçº¿çœ‹åˆ°å®ƒã€‚æ³¨æ„å®ƒæ˜¯å¦‚ä½•è¿…é€Ÿæ”¶æ•›åˆ°é›¶çš„ï¼Œå› ä¸ºé‚£ä¸ªå¯¼æ•°è¢«ç§°ä¸ºæ¢¯åº¦ã€‚å®ƒä¼šåˆ°0æˆ–æ¶ˆå¤±çš„äº‹å®æ„å‘³ç€æˆ‘ä»¬ä¼šæœ‰æ¶ˆå¤±æ¢¯åº¦ã€‚è¿™æ˜¯æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­è§£å†³äº†ã€‚
- en: one of the many problems that it mostly solvedã€‚Because this saturates to 0ã€‚
    it's not as desirable as theã€‚As the re that the rectified linear unit doesn't
    saturate to 0ã€‚ like the sigmoid function does in both directionsã€‚ We'll look at
    why bias neurons are neededã€‚Essentiallyï¼Œ they are the interceptã€‚Like when you
    previously worked with Y equals Mx plus K linear equationsã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å®ƒä¸»è¦è§£å†³çš„ä¼—å¤šé—®é¢˜ä¹‹ä¸€ã€‚å› ä¸ºå®ƒé¥±å’Œåˆ°0ï¼Œæ‰€ä»¥ä¸å¦‚ã€‚å› ä¸ºä¿®æ­£çº¿æ€§å•å…ƒåœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šéƒ½ä¸ä¼šé¥±å’Œåˆ°0ï¼Œå°±åƒsigmoidå‡½æ•°é‚£æ ·ã€‚æˆ‘ä»¬å°†çœ‹çœ‹ä¸ºä»€ä¹ˆéœ€è¦åç½®ç¥ç»å…ƒã€‚æœ¬è´¨ä¸Šï¼Œå®ƒä»¬æ˜¯æˆªè·ã€‚å°±åƒä½ ä¹‹å‰å¤„ç†çš„Yç­‰äºMxåŠ Kçº¿æ€§æ–¹ç¨‹ã€‚
- en: If you look at this oneï¼Œ this is an example of when we change the weightã€‚ If
    you notice we change the weightï¼Œ it's effectively changing the slopeã€‚And you reallyã€‚
    the line always has to pass through zeroã€‚When the input isã€‚So all of these pass
    through zeroã€‚ there's no way to really shift thatã€‚When you change the bias neuronï¼Œ
    now you're shiftingã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æŸ¥çœ‹è¿™ä¸ªï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ”¹å˜æƒé‡çš„ä¸€ä¸ªä¾‹å­ã€‚å¦‚æœä½ æ³¨æ„åˆ°æˆ‘ä»¬æ”¹å˜äº†æƒé‡ï¼Œå®ƒå®é™…ä¸Šæ”¹å˜äº†æ–œç‡ã€‚å¹¶ä¸”ä½ çœŸçš„ã€‚çº¿æ€»æ˜¯å¿…é¡»é€šè¿‡é›¶ã€‚å½“è¾“å…¥æ˜¯ã€‚å› æ­¤æ‰€æœ‰è¿™äº›éƒ½é€šè¿‡é›¶ã€‚æ²¡æœ‰åŠæ³•çœŸæ­£æ”¹å˜è¿™ä¸€ç‚¹ã€‚å½“ä½ æ”¹å˜åç½®ç¥ç»å…ƒæ—¶ï¼Œç°åœ¨ä½ åœ¨ç§»åŠ¨ã€‚
- en: you're not affecting the slopeã€‚ So using those two togetherï¼Œ you can reallyã€‚
    you can affect the slope and you can shift itã€‚ You can move itã€‚And then all of
    those neurons together can contribute in sort of the additive effect of all of
    the neuronsã€‚Let's this line then break and basically approximate any functionã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¹¶æ²¡æœ‰å½±å“æ–œç‡ã€‚å› æ­¤å°†è¿™ä¸¤è€…ç»“åˆä½¿ç”¨ï¼Œä½ å®é™…ä¸Šå¯ä»¥ã€‚ä½ å¯ä»¥å½±å“æ–œç‡å¹¶è¿›è¡Œç§»åŠ¨ã€‚ä½ å¯ä»¥ç§»åŠ¨å®ƒã€‚æ‰€æœ‰è¿™äº›ç¥ç»å…ƒä¸€èµ·å¯ä»¥ä»¥æŸç§åŠ æ³•æ•ˆæœè´¡çŒ®æ‰€æœ‰ç¥ç»å…ƒã€‚è®©æˆ‘ä»¬è®©è¿™æ¡çº¿æ–­å¼€å¹¶åŸºæœ¬ä¸Šè¿‘ä¼¼ä»»ä½•å‡½æ•°ã€‚
- en: '![](img/8444b070a2a80e4ecb951689e6339e17_3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8444b070a2a80e4ecb951689e6339e17_3.png)'
- en: Thank you for watching this videoã€‚ Now that we've seen a general introduction
    to deep neural networksã€‚ We're ready to start to look at Tensorflow and Cars and
    see how these are actually implemented in Python so that you can make use of themã€‚
    This content changes oftenã€‚ So subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligenceã€‚ğŸ˜Šã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢è§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°æ·±åº¦ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬ä»‹ç»ã€‚æˆ‘ä»¬å‡†å¤‡å¼€å§‹æŸ¥çœ‹Tensorflowå’Œæ±½è½¦ï¼Œçœ‹çœ‹è¿™äº›æ˜¯å¦‚ä½•åœ¨Pythonä¸­å®ç°çš„ï¼Œä»¥ä¾¿ä½ å¯ä»¥åˆ©ç”¨å®ƒä»¬ã€‚è¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ã€‚æ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æœ¬è¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚ğŸ˜Š
