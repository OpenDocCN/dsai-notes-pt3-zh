- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PySpark å¤§æ•°æ®å¤„ç†å…¥é—¨ï¼Œå¸¦ä½ ç©è½¬Python+Sparkå¤§æ•°æ®æ“ä½œä¸åˆ†æï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- Databricks
    ç®€ä»‹ - ShowMeAI - BV1sL4y147dP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PySpark å¤§æ•°æ®å¤„ç†å…¥é—¨ï¼Œå¸¦ä½ ç©è½¬Python+Sparkå¤§æ•°æ®æ“ä½œä¸åˆ†æï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- **Databricks**
    ç®€ä»‹ - ShowMeAI - BV1sL4y147dP
- en: ã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_1.png)'
- en: Hello allã€‚ My name is Krishna and welcome to my usual channelã€‚ So guysã€‚ we will
    be continuing the Pipark seriesã€‚ And in this particular videoã€‚ we are going to
    understand what is this databs platformã€‚ Now rememberã€‚ guys in my previous videos
    I've already uploaded all these particular videos with respect to Pipark with
    Python till the Pipar Mã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ã€‚æˆ‘æ˜¯**Krishna**ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘çš„é¢‘é“ã€‚æ‰€ä»¥å¤§å®¶ï¼Œæˆ‘ä»¬å°†ç»§ç»­**Pipark**ç³»åˆ—ã€‚åœ¨è¿™ä¸ªç‰¹å®šçš„è§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£è¿™ä¸ª**Databricks**å¹³å°ã€‚è®°ä½ï¼Œå¤§å®¶ï¼Œåœ¨æˆ‘ä¹‹å‰çš„è§†é¢‘ä¸­ï¼Œæˆ‘å·²ç»ä¸Šä¼ äº†æ‰€æœ‰å…³äº**Pipark**ä¸Pythonçš„å†…å®¹ï¼Œç›´åˆ°**Pipar
    M**ã€‚
- en: And I told you that in this particular video we were to discuss about linear
    regressionã€‚ how we can implement linear regression with the help of Piparkã€‚ But
    before thatã€‚ I really want to help you know what exactly is a databricks platformã€‚
    And this is an amazing platform where you can actually use Piparkï¼Œ or you can
    work with Apacheparkã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å‘Šè¯‰è¿‡ä½ ï¼Œåœ¨è¿™ä¸ªç‰¹å®šçš„è§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºçº¿æ€§å›å½’ï¼Œä»¥åŠå¦‚ä½•å€ŸåŠ©**Pipark**å®ç°çº¿æ€§å›å½’ã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘çœŸçš„æƒ³å¸®åŠ©ä½ äº†è§£ä»€ä¹ˆæ˜¯**Databricks**å¹³å°ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„å¹³å°ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨**Pipark**ï¼Œæˆ–è€…ä¸**Apachepark**åˆä½œã€‚
- en: And one more amazing thing about this particular platform is that they also
    provide you cluster instancesã€‚ So suppose if you have a huge amount of data probably
    want to distribute the parallel processing or probably want to distribute it in
    multiple clusters you can definitely do with the help of databsã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä¸ªå…³äºè¿™ä¸ªç‰¹å®šå¹³å°çš„æƒŠäººä¹‹å¤„åœ¨äºï¼Œä»–ä»¬è¿˜ä¸ºä½ æä¾›äº†é›†ç¾¤å®ä¾‹ã€‚æ‰€ä»¥å¦‚æœä½ æœ‰å¤§é‡æ•°æ®ï¼Œæƒ³è¦è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œæˆ–è€…æƒ³è¦åœ¨å¤šä¸ªé›†ç¾¤ä¸­åˆ†é…æ•°æ®ï¼Œä½ ç»å¯¹å¯ä»¥å€ŸåŠ©**Databricks**åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: ğŸ˜Šã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_3.png)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_3.png)
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_4.png)'
- en: Nowï¼Œ if I really want to use this particular platformã€‚ there are two ways one
    is for community version and one is for the paid versionã€‚ which is like Azure
    or AWS cloud you can actually use in the back endã€‚Dataricricks also helps you
    to implement ML flow okay and this ML flow is with respect to the CICD pipeline
    so you can also perform those kind of experiments alsoã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘çœŸçš„æƒ³ä½¿ç”¨è¿™ä¸ªå¹³å°ï¼Œæœ‰ä¸¤ç§æ–¹å¼ï¼šä¸€ç§æ˜¯ç¤¾åŒºç‰ˆæœ¬ï¼Œå¦ä¸€ç§æ˜¯ä»˜è´¹ç‰ˆæœ¬ï¼Œä¾‹å¦‚**Azure**æˆ–**AWS**äº‘ï¼Œä½ å®é™…ä¸Šå¯ä»¥åœ¨åç«¯ä½¿ç”¨å®ƒã€‚**Dataricricks**è¿˜å¸®åŠ©ä½ å®ç°**ML
    flow**ï¼Œè€Œè¿™ä¸ª**ML flow**ä¸**CICD**ç®¡é“æœ‰å…³ï¼Œæ‰€ä»¥ä½ ä¹Ÿå¯ä»¥è¿›è¡Œè¿™äº›å®éªŒã€‚
- en: Altogetherï¼Œ an amazing platformã€‚ What I will be focusing in my YouTube channel
    is that I will try to show you both with the community version also and in the
    upcoming videos will try to execute try to execute with both AWS and Azure when
    we are using AWS and Azureã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„å¹³å°ã€‚æˆ‘åœ¨æˆ‘çš„YouTubeé¢‘é“ä¸Šä¼šä¸“æ³¨äºå±•ç¤ºç¤¾åŒºç‰ˆæœ¬ï¼Œå¹¶ä¸”åœ¨æ¥ä¸‹æ¥çš„è§†é¢‘ä¸­ä¼šå°è¯•ä½¿ç”¨**AWS**å’Œ**Azure**è¿›è¡Œæ‰§è¡Œã€‚
- en: what we will try to do is that whenever we create the instancesã€‚ multiple instances
    know that will try to create in this particular cloud platform we'll also try
    to pull the data from S3 bucketã€‚ which is the storage unit in AWs and try to show
    you that how we can work with hugeã€‚ huge data sets all those things be actually
    showed as we go ahead Now let's understand what this databs isã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å°è¯•åšçš„æ˜¯ï¼Œæ¯å½“æˆ‘ä»¬åˆ›å»ºå¤šä¸ªå®ä¾‹æ—¶ï¼ŒçŸ¥é“æˆ‘ä»¬ä¼šå°è¯•åœ¨è¿™ä¸ªç‰¹å®šçš„äº‘å¹³å°ä¸Šåˆ›å»ºå¤šä¸ªå®ä¾‹ï¼Œæˆ‘ä»¬è¿˜ä¼šå°è¯•ä»**S3 bucket**ä¸­æ‹‰å–æ•°æ®ï¼Œ**S3
    bucket**æ˜¯**AWS**ä¸­çš„å­˜å‚¨å•å…ƒï¼Œå¹¶å°è¯•å‘ä½ å±•ç¤ºå¦‚ä½•å¤„ç†åºå¤§çš„æ•°æ®é›†ï¼Œæ‰€æœ‰è¿™äº›å†…å®¹ä¼šéšç€æˆ‘ä»¬çš„è¿›å±•è€Œå±•ç¤ºã€‚ç°åœ¨è®©æˆ‘ä»¬äº†è§£ä¸€ä¸‹è¿™ä¸ª**Databricks**æ˜¯ä»€ä¹ˆã€‚
- en: it is an open and unified data analyticss platform for data engineering data
    science and machine learning analyticss remember databs actually helps us to perform
    data engineering When I say data engineering probably working with big data it
    also helps us to execute some machine learning algorithms probably any kind of
    data science problem statementã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯ä¸€ä¸ªå¼€æ”¾å’Œç»Ÿä¸€çš„æ•°æ®åˆ†æå¹³å°ï¼Œé€‚ç”¨äºæ•°æ®å·¥ç¨‹ã€æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ åˆ†æã€‚è®°ä½ï¼Œ**Databricks**å®é™…ä¸Šå¸®åŠ©æˆ‘ä»¬æ‰§è¡Œæ•°æ®å·¥ç¨‹ï¼Œå½“æˆ‘è¯´æ•°æ®å·¥ç¨‹æ—¶ï¼Œå¯èƒ½æ˜¯åœ¨å¤„ç†å¤§æ•°æ®ï¼Œå®ƒè¿˜å¸®åŠ©æˆ‘ä»¬æ‰§è¡Œä¸€äº›æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¯èƒ½æ˜¯ä»»ä½•ç±»å‹çš„æ•°æ®ç§‘å­¦é—®é¢˜é™ˆè¿°ã€‚
- en: ğŸ˜Šï¼ŒWillll be able to do it and probably it suppose three kind of platform cloud
    platformsã€‚ one is AWS Microsoft Azure and Google Cloud Now if you really want
    to start start with this well start with the community version and you just have
    to go into this particular URL and just type try databs and then you just enter
    all your details to get registered for freeã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä½ å°†èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹ï¼Œå¯èƒ½ä¼šæ¶‰åŠä¸‰ç§äº‘å¹³å°ï¼šä¸€ä¸ªæ˜¯**AWS**ï¼Œä¸€ä¸ªæ˜¯**Microsoft Azure**ï¼Œè¿˜æœ‰ä¸€ä¸ªæ˜¯**Google Cloud**ã€‚å¦‚æœä½ çœŸçš„æƒ³å¼€å§‹ï¼Œå¯ä»¥å…ˆä»ç¤¾åŒºç‰ˆæœ¬å¼€å§‹ï¼Œä½ åªéœ€è®¿é—®è¿™ä¸ªç‰¹å®šçš„URLï¼Œè¾“å…¥â€œtry
    databsâ€ï¼Œç„¶åè¾“å…¥æ‰€æœ‰ä½ çš„è¯¦ç»†ä¿¡æ¯ä»¥å…è´¹æ³¨å†Œã€‚
- en: Now once you are registered you once you get started for freeã€‚ youll get two
    options over there on the right hand side you will be seeing the community version
    which you really want to use it for free and in the left hand side you will be
    having an option where they will tell you that you need to work with this three
    cloud platforms and you can select that alsoã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ æ³¨å†Œå¹¶å¼€å§‹å…è´¹ä½¿ç”¨ï¼Œä½ å°†åœ¨å³ä¾§çœ‹åˆ°ä¸¤ä¸ªé€‰é¡¹ï¼Œä½ å¯ä»¥çœ‹åˆ°ä½ æƒ³å…è´¹ä½¿ç”¨çš„ç¤¾åŒºç‰ˆæœ¬ï¼Œè€Œåœ¨å·¦ä¾§åˆ™ä¼šæœ‰ä¸€ä¸ªé€‰é¡¹ï¼Œå‘Šè¯‰ä½ éœ€è¦ä¸è¿™ä¸‰å¤§äº‘å¹³å°åˆä½œï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©é‚£ä¸ªã€‚
- en: So for right now I will try to show you a community versionï¼Œ which will be very
    simpleã€‚ very very easyã€‚ So let's go to the community versionã€‚ So this is how the
    community version actually looks like if you really want to go into the cloud
    version you can just click on upgrade okay so just click on upgrade and this is
    the URL of the community version and this version this URL you'll be able to get
    when you register for the community version tomorrows you think that you probably
    want to work with the cloudã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘å°†å°è¯•å‘ä½ å±•ç¤ºä¸€ä¸ªç¤¾åŒºç‰ˆæœ¬ï¼Œå®ƒä¼šéå¸¸ç®€å•ï¼Œéå¸¸å®¹æ˜“ã€‚è®©æˆ‘ä»¬è¿›å…¥ç¤¾åŒºç‰ˆæœ¬ã€‚å®é™…ä¸Šï¼Œç¤¾åŒºç‰ˆæœ¬æ˜¯è¿™æ ·çš„ï¼Œå¦‚æœä½ çœŸçš„æƒ³ä½¿ç”¨äº‘ç‰ˆæœ¬ï¼Œå¯ä»¥ç‚¹å‡»å‡çº§ï¼Œå¥½çš„ï¼Œç‚¹å‡»å‡çº§ï¼Œè¿™å°±æ˜¯ç¤¾åŒºç‰ˆæœ¬çš„URLï¼Œæ³¨å†Œç¤¾åŒºç‰ˆæœ¬æ—¶ï¼Œä½ å°†èƒ½å¤Ÿè·å¾—è¿™ä¸ªURLï¼Œæ˜å¤©ä½ å¯èƒ½ä¼šæƒ³ä½¿ç”¨äº‘æœåŠ¡ã€‚
- en: you just have to click on this upgrade now now in this you'll be able to see
    three things one is explore to the explore the quickstar tutorialã€‚ import and
    explore dataï¼Œ create a blank notebook and many more things over here what kind
    of task you'll be able to do in the community version one is you can create a
    new notebook you can create a tableã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªéœ€ç‚¹å‡»è¿™ä¸ªå‡çº§ï¼Œç°åœ¨ä½ å°†çœ‹åˆ°ä¸‰ä¸ªé€‰é¡¹ï¼Œä¸€ä¸ªæ˜¯æ¢ç´¢å¿«é€Ÿå…¥é—¨æ•™ç¨‹ï¼Œå¯¼å…¥å’Œæ¢ç´¢æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªç©ºç™½ç¬”è®°æœ¬ï¼Œè¿˜æœ‰æ›´å¤šä½ å¯ä»¥åœ¨ç¤¾åŒºç‰ˆæœ¬ä¸­å®Œæˆçš„ä»»åŠ¡ï¼Œæ¯”å¦‚åˆ›å»ºæ–°ç¬”è®°æœ¬æˆ–åˆ›å»ºè¡¨æ ¼ã€‚
- en: create a cluster create new Mflow experimentã€‚ I hope I have actually showed
    you Mflow experiment we can also create this MLflow experiment by combining to
    a database in the backend then we can import libraries read documentã€‚ğŸ˜Šã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_6.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªé›†ç¾¤ï¼Œåˆ›å»ºæ–°çš„Mflowå®éªŒã€‚æˆ‘å¸Œæœ›æˆ‘å·²ç»å‘ä½ å±•ç¤ºäº†Mflowå®éªŒï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡å°†å…¶ä¸åç«¯æ•°æ®åº“ç»“åˆæ¥åˆ›å»ºè¿™ä¸ªMLflowå®éªŒï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¯¼å…¥åº“ï¼Œè¯»å–æ–‡æ¡£ã€‚ğŸ˜Šï¼[](img/4bf769d9eaf5cd192f62275495d3c7b7_6.png)
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_7.png)'
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_8.png)'
- en: Can do a lot of taskã€‚ Nowï¼Œ first of allï¼Œ what we need to do is that probably
    I'll create a clusterã€‚Nowï¼Œ in order to create a clusterï¼Œ I will click on thisï¼Œ
    create a cluster hereã€‚ you can basically just write down any cluster nameã€‚ Supp
    I'll say Apache or I'll just sayã€‚Py spark clusterã€‚ Suppose this is myã€‚Cluster
    that I want to basically createã€‚ Okayã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥å®Œæˆå¾ˆå¤šä»»åŠ¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åšçš„å¯èƒ½æ˜¯åˆ›å»ºä¸€ä¸ªé›†ç¾¤ã€‚ç°åœ¨ï¼Œä¸ºäº†åˆ›å»ºä¸€ä¸ªé›†ç¾¤ï¼Œæˆ‘ä¼šç‚¹å‡»è¿™é‡Œçš„åˆ›å»ºé›†ç¾¤ã€‚ä½ å¯ä»¥åŸºæœ¬ä¸Šå†™ä¸‹ä»»ä½•é›†ç¾¤åç§°ã€‚æˆ‘ä¼šè¯´Apacheï¼Œæˆ–è€…æˆ‘ä¼šè¯´Py
    sparké›†ç¾¤ã€‚å‡è®¾è¿™æ˜¯æˆ‘æƒ³è¦åˆ›å»ºçš„é›†ç¾¤ã€‚å¥½çš„ã€‚
- en: and then here by default over hereï¼Œ you can see 8ã€‚2 scalarã€‚ This one spark 3ã€‚1ã€‚1
    is selectedã€‚ So we will be working with spark 3ã€‚1ã€‚1 If you remember in my local
    also I actually installed this particular version only okay by default you will
    be able to see that they will be providing you one instance with 15 gb memory
    and some more configuration if you really want to upgrade your configurationã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™é‡Œé»˜è®¤æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥çœ‹åˆ°8.2 scalarã€‚é€‰å®šçš„æ˜¯spark 3.1.1ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨spark 3.1.1ã€‚å¦‚æœä½ è®°å¾—ï¼Œæˆ‘åœ¨æœ¬åœ°ä¹Ÿå®‰è£…äº†è¿™ä¸ªç‰¹å®šç‰ˆæœ¬ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œä½ ä¼šçœ‹åˆ°ä»–ä»¬æä¾›ä¸€ä¸ª15
    GBå†…å­˜çš„å®ä¾‹å’Œæ›´å¤šé…ç½®ï¼Œå¦‚æœä½ çœŸçš„æƒ³å‡çº§ä½ çš„é…ç½®ã€‚
- en: you can basically go and click over here Okay and remember in the free version
    you will be able to work in an instance unless and until it is not idle for two
    hours otherwise it will get disconnectedã€‚So over here you can see one driverï¼Œ15ã€‚3
    Gb memory2 course and one Dã€‚ Okayã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åŸºæœ¬ä¸Šå¯ä»¥ç‚¹å‡»è¿™é‡Œã€‚å¥½çš„ï¼Œè®°ä½åœ¨å…è´¹ç‰ˆæœ¬ä¸­ï¼Œä½ å°†èƒ½å¤Ÿåœ¨ä¸€ä¸ªå®ä¾‹ä¸­å·¥ä½œï¼Œé™¤éå®ƒç©ºé—²è¶…è¿‡ä¸¤ä¸ªå°æ—¶ï¼Œå¦åˆ™å®ƒå°†æ–­å¼€è¿æ¥ã€‚æ‰€ä»¥åœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°ä¸€ä¸ªé©±åŠ¨ç¨‹åºï¼Œ15.3
    GBå†…å­˜ï¼Œ2æ ¸å’Œä¸€ä¸ªDã€‚
- en: all these things are thereã€‚ You can also understand what debut is De is nothing
    but a data bricks unitã€‚ If you want to click over hereï¼Œ youll be able to understand
    what exactly debut is okay and youll be able to select a cloud and basically work
    with that perfect till here everything is fineã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½åœ¨è¿™é‡Œã€‚ä½ è¿˜å¯ä»¥ç†è§£ä»€ä¹ˆæ˜¯debutï¼Œdeæ˜¯æ•°æ®ç –å•ä½ã€‚å¦‚æœä½ æƒ³ç‚¹å‡»è¿™é‡Œï¼Œä½ å°†èƒ½å¤Ÿç†è§£debutç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Œå¥½çš„ï¼Œä½ å°†èƒ½å¤Ÿé€‰æ‹©ä¸€ä¸ªäº‘ï¼Œå¹¶åŸºæœ¬ä¸Šä¸ä¹‹åˆä½œï¼Œå®Œç¾ï¼Œåˆ°è¿™é‡Œä¸€åˆ‡éƒ½å¾ˆå¥½ã€‚
- en: let's startï¼Œ let's create the clusterã€‚ Nowï¼Œ once you you will be seeing that
    the cluster is basically getting createdã€‚ you also have lot of options over hereï¼Œ
    like notebookï¼Œ librariesï¼Œ event logsã€‚ spark Ui driver logs and all It's not like
    you just have you' will be able to work with Python over here here you have lot
    of optionsã€‚ so suppose if I go and click on librariesã€‚ And if I click on install
    new here you will be having an option to upload the librariesã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹ï¼Œåˆ›å»ºé›†ç¾¤ã€‚ç°åœ¨ï¼Œä¸€æ—¦ä½ çœ‹åˆ°é›†ç¾¤æ­£åœ¨åˆ›å»ºã€‚ä½ è¿™é‡Œè¿˜æœ‰å¾ˆå¤šé€‰é¡¹ï¼Œæ¯”å¦‚ç¬”è®°æœ¬ã€åº“ã€äº‹ä»¶æ—¥å¿—ã€‚Spark UIé©±åŠ¨ç¨‹åºæ—¥å¿—ç­‰ç­‰ã€‚è¿™å¹¶ä¸æ˜¯è¯´ä½ åªèƒ½åœ¨è¿™é‡Œä½¿ç”¨Pythonï¼Œä½ è¿™é‡Œæœ‰å¾ˆå¤šé€‰é¡¹ã€‚æ‰€ä»¥å‡è®¾æˆ‘å»ç‚¹å‡»åº“ã€‚å¦‚æœæˆ‘ç‚¹å‡»å®‰è£…æ–°åº“ï¼Œä½ å°†æœ‰ä¸€ä¸ªé€‰é¡¹æ¥ä¸Šä¼ åº“ã€‚
- en: you can also install the libraries from Pi from Maynã€‚ which we basically use
    along a Java then you have different different workspaceã€‚ So here what I'm going
    to do is that suppose if you select pipe and suppose you want to install some
    of the libraryã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥ä»Maynçš„Piå®‰è£…åº“ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šåœ¨Javaä¸­ä½¿ç”¨è¿™äº›åº“ï¼Œç„¶åä½ ä¼šæœ‰ä¸åŒçš„å·¥ä½œç©ºé—´ã€‚æ‰€ä»¥æˆ‘å°†åšçš„æ˜¯ï¼Œå‡è®¾ä½ é€‰æ‹©ç®¡é“ï¼Œå¦‚æœä½ æƒ³å®‰è£…ä¸€äº›åº“ã€‚
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_10.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_10.png)'
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_11.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_11.png)'
- en: I like Tensorflow or probably you want to go with ks you can basically write
    like this probably I want a scale learn you knowã€‚ so I can just give comma separated
    and start installing them Okay but by default I know I'm going to work with Pi
    Sp so I'm not going to install any libraries so let's see how much time this will
    probably take this is just getting executed over hereã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å–œæ¬¢Tensorflowï¼Œæˆ–è€…ä½ å¯èƒ½æƒ³é€‰æ‹©ksï¼Œä½ å¯ä»¥è¿™æ ·å†™ï¼Œæˆ‘å¯èƒ½æƒ³è¦ä¸€ä¸ªscale learnã€‚ä½ çŸ¥é“ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥ç”¨é€—å·åˆ†éš”å¹¶å¼€å§‹å®‰è£…å®ƒä»¬ã€‚å¥½çš„ï¼Œä½†é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘çŸ¥é“æˆ‘å°†ä½¿ç”¨Pi
    Spï¼Œæ‰€ä»¥æˆ‘ä¸æ‰“ç®—å®‰è£…ä»»ä½•åº“ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¯èƒ½éœ€è¦å¤šé•¿æ—¶é—´ï¼Œè¿™é‡Œåªæ˜¯æ‰§è¡Œã€‚
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_13.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_13.png)'
- en: And let's go back to my homeã€‚ So apart from this yearï¼Œ you'll be also able to
    upload the data setã€‚ and that particular data will give you an environment like
    how you're storing the data in the doã€‚ Okayï¼Œ so before the cluster is getting
    createdã€‚ now the cluster has got created here you can see Pi pocket is in running
    state now and remember this cluster only has one instanceã€‚ you want to create
    multiple clustersã€‚ We have to use the cloud platform1ï¼Œ which will be chargeableã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°æˆ‘çš„ä¸»é¡µã€‚æ‰€ä»¥é™¤äº†è¿™ä¸€å¹´ï¼Œä½ è¿˜èƒ½å¤Ÿä¸Šä¼ æ•°æ®é›†ã€‚é‚£ä¸ªç‰¹å®šçš„æ•°æ®å°†ç»™ä½ ä¸€ä¸ªç¯å¢ƒï¼Œåƒä½ å¦‚ä½•åœ¨doä¸­å­˜å‚¨æ•°æ®ã€‚å¥½çš„ï¼Œæ‰€ä»¥åœ¨é›†ç¾¤åˆ›å»ºä¹‹å‰ï¼Œç°åœ¨é›†ç¾¤å·²ç»åˆ›å»ºäº†ï¼Œä½ å¯ä»¥çœ‹åˆ°Pi
    pocketå¤„äºè¿è¡ŒçŠ¶æ€ï¼Œè®°ä½è¿™ä¸ªé›†ç¾¤åªæœ‰ä¸€ä¸ªå®ä¾‹ã€‚å¦‚æœä½ æƒ³åˆ›å»ºå¤šä¸ªé›†ç¾¤ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨äº‘å¹³å°1ï¼Œè¿™å°†æ˜¯æ”¶è´¹çš„ã€‚
- en: Okay so in here I'm going to click on export the dataã€‚ Now see guysï¼Œ you can
    upload the dataã€‚ you can also bring from S 3 bucketã€‚ you can also then bring from
    S3 bucketã€‚ These all things I'll try to show youã€‚ then you also have Dfs you knowã€‚ğŸ˜Šï¼ŒAnd
    D V F Fã€‚ you will basically be storing inside this particular formatã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå°†ç‚¹å‡»å¯¼å‡ºæ•°æ®ã€‚ç°åœ¨å¤§å®¶çœ‹ï¼Œä½ å¯ä»¥ä¸Šä¼ æ•°æ®ã€‚ä½ ä¹Ÿå¯ä»¥ä»S3æ¡¶ä¸­è·å–ã€‚ç„¶åä½ ä¹Ÿå¯ä»¥ä»S3æ¡¶ä¸­è·å–ã€‚è¿™äº›æˆ‘ä¼šè¯•ç€ç»™ä½ å±•ç¤ºã€‚ç„¶åä½ ä¹Ÿæœ‰Dfsï¼Œä½ çŸ¥é“ã€‚ğŸ˜Šï¼Œè¿˜æœ‰D
    V F Fã€‚ä½ åŸºæœ¬ä¸Šä¼šå­˜å‚¨åœ¨è¿™ä¸ªç‰¹å®šçš„æ ¼å¼ä¸­ã€‚
- en: Then you have other data sources like Amazon Re script Amazon kindnessesã€‚ Amazon
    Kinnesses is basically used for live streaming dataã€‚ Okayï¼Œ then you have cassandraã€‚Cassra
    is also a no SQL database and JDBC last searchã€‚ So different different data data
    sources also there we'll also try to see with respect to partners integrationã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ æœ‰å…¶ä»–æ•°æ®æºï¼Œæ¯”å¦‚Amazon Re script Amazon kindnessesã€‚Amazon Kinnessesä¸»è¦ç”¨äºå®æ—¶æµæ•°æ®ã€‚å¥½çš„ï¼Œç„¶åä½ æœ‰Cassandraã€‚Cassandraä¹Ÿæ˜¯ä¸€ä¸ªNo
    SQLæ•°æ®åº“ï¼Œè¿˜æœ‰JDBCæœ€åæœç´¢ã€‚å› æ­¤ï¼Œä¹Ÿæœ‰ä¸åŒçš„æ•°æ®æºï¼Œæˆ‘ä»¬è¿˜ä¼šå°è¯•ä¸åˆä½œä¼™ä¼´çš„é›†æˆã€‚
- en: So they are also like real time capture in data lakeã€‚ and many more things are
    thereã€‚ So you can definitely have a look onto thisã€‚ Now what I'm going to do is
    that I'm just going to click over here and try to upload a data let me just seeã€‚Let
    me just upload the data setsã€‚ I'll just go to my Pipar folderã€‚So here is my pass
    well I'm just going to upload this test data set probablyã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒä»¬ä¹Ÿåƒå®æ—¶æ•è·åœ¨æ•°æ®æ¹–ä¸­ï¼Œè¿˜æœ‰æ›´å¤šçš„ä¸œè¥¿åœ¨è¿™é‡Œã€‚å› æ­¤ï¼Œä½ ç»å¯¹å¯ä»¥çœ‹çœ‹è¿™ä¸ªã€‚ç°åœ¨æˆ‘å°†åšçš„æ˜¯ï¼Œç‚¹å‡»è¿™é‡Œå°è¯•ä¸Šä¼ æ•°æ®ï¼Œè®©æˆ‘çœ‹çœ‹ã€‚æˆ‘å°†ä¸Šä¼ æ•°æ®é›†ã€‚æˆ‘ä¼šå»æˆ‘çš„Piparæ–‡ä»¶å¤¹ã€‚æ‰€ä»¥è¿™é‡Œæ˜¯æˆ‘çš„è·¯å¾„ï¼Œæˆ‘å°†ä¸Šä¼ è¿™ä¸ªæµ‹è¯•æ•°æ®é›†ã€‚
- en: or I'll try to upload this test oneã€‚Nowï¼Œ here you can see that the data set
    has been uploadedã€‚ Now it is saying that create table with UI create table in
    note notebookã€‚ supposeupp if I go and click thisï¼Œ you knowã€‚So here you will be
    able to see this is the codeã€‚ this is the entire code to basically create a table
    in the Uiã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…æˆ‘ä¼šå°è¯•ä¸Šä¼ è¿™ä¸ªæµ‹è¯•æ–‡ä»¶ã€‚ç°åœ¨ï¼Œä½ å¯ä»¥çœ‹åˆ°æ•°æ®é›†å·²ç»ä¸Šä¼ ã€‚ç°åœ¨å®ƒè¯´è¦é€šè¿‡ UI åˆ›å»ºè¡¨æ ¼ï¼Œåœ¨ç¬”è®°æœ¬ä¸­åˆ›å»ºè¡¨æ ¼ã€‚å¦‚æœæˆ‘å»ç‚¹å‡»è¿™ä¸ªï¼Œä½ çŸ¥é“çš„ã€‚æ‰€ä»¥è¿™é‡Œä½ å°†èƒ½å¤Ÿçœ‹åˆ°è¿™æ˜¯ä»£ç ã€‚è¿™æ˜¯åˆ›å»ºè¡¨æ ¼çš„å®Œæ•´ä»£ç ã€‚
- en: but what I really want to do is that I don't want to create a table instead
    I'll just try to execute some of the Pipar code which we have already learned
    till now Okayã€‚ so what I' am going to doã€‚I'll just remove thisã€‚ I don't want itã€‚
    I'll remove thisï¼Œ okayã€‚Okayã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘çœŸæ­£æƒ³åšçš„æ˜¯æˆ‘ä¸æƒ³åˆ›å»ºè¡¨æ ¼ï¼Œè€Œæ˜¯å°è¯•æ‰§è¡Œä¸€äº›æˆ‘ä»¬åˆ°ç°åœ¨ä¸ºæ­¢å·²ç»å­¦è¿‡çš„ Pipar ä»£ç ã€‚å¥½çš„ã€‚æ‰€ä»¥æˆ‘å°†åšçš„å°±æ˜¯åˆ é™¤è¿™ä¸ªã€‚æˆ‘ä¸æƒ³è¦å®ƒã€‚æˆ‘å°†åˆ é™¤è¿™ä¸ªï¼Œå¥½çš„ã€‚å¥½çš„ã€‚
- en: let me read the data set now for reading the data setã€‚Over hereã€‚ you'll be able
    to see that my dataset path is basically thisã€‚ It is a CSV file info schema header
    schemaï¼Œ all these things are thereã€‚ So let me remove this alsoã€‚ So let me start
    reading the dataã€‚ So by defaultï¼Œ Sp is already uploadedã€‚ So I write Sp dotã€‚Sk
    dotã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ç°åœ¨è¯»å–æ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œä½ ä¼šçœ‹åˆ°æˆ‘çš„æ•°æ®é›†è·¯å¾„åŸºæœ¬ä¸Šæ˜¯è¿™ä¸ªã€‚å®ƒæ˜¯ä¸€ä¸ª CSV æ–‡ä»¶ï¼ŒåŒ…å« schema å¤´éƒ¨ schemaï¼Œæ‰€æœ‰è¿™äº›å†…å®¹éƒ½åœ¨è¿™é‡Œã€‚æ‰€ä»¥è®©æˆ‘ä¹Ÿåˆ é™¤è¿™ä¸ªã€‚è®©æˆ‘å¼€å§‹è¯»å–æ•°æ®ã€‚å› æ­¤é»˜è®¤æƒ…å†µä¸‹ï¼ŒSp
    å·²ç»ä¸Šä¼ ã€‚æ‰€ä»¥æˆ‘å†™ Sp dotã€‚Sk dotã€‚
- en: Read dot cvã€‚I hope so it will workã€‚ And for the first timeï¼Œ rememberï¼Œ this is
    my file locationã€‚File locationï¼Œ okayï¼Œ file underscore locationã€‚ And then I will
    also be using two more optionã€‚ One is header physicalical to trueã€‚And then I have
    infer schema is once I execute this now you will be seeing that automatically
    the first time menu executing it will say that launch and run so we are going
    to launch the cluster and run it So I'm just going to click it fail to create
    reject requestquies since the total number of nodes would exit the limit one Why
    this is there let's see our clusters we just have one clusterã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯» dot cvã€‚æˆ‘å¸Œæœ›å®ƒèƒ½æ­£å¸¸å·¥ä½œã€‚é¦–å…ˆï¼Œè¯·è®°ä½ï¼Œè¿™æ˜¯æˆ‘çš„æ–‡ä»¶ä½ç½®ã€‚æ–‡ä»¶ä½ç½®ï¼Œå¥½çš„ï¼Œæ–‡ä»¶ä¸‹åˆ’çº¿ä½ç½®ã€‚ç„¶åæˆ‘è¿˜ä¼šä½¿ç”¨ä¸¤ä¸ªé€‰é¡¹ã€‚ä¸€ä¸ªæ˜¯å°† header
    physicalical è®¾ç½®ä¸º trueã€‚ç„¶åæˆ‘æœ‰ infer schema ä¸€æ—¦æˆ‘æ‰§è¡Œè¿™ä¸ªï¼Œç°åœ¨ä½ ä¼šçœ‹åˆ°ç¬¬ä¸€æ¬¡èœå•æ‰§è¡Œæ—¶ä¼šæ˜¾ç¤ºå¯åŠ¨å¹¶è¿è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å¯åŠ¨é›†ç¾¤å¹¶è¿è¡Œå®ƒã€‚æˆ‘å°†ç‚¹å‡»å®ƒï¼Œåˆ›å»ºå¤±è´¥ï¼Œæ‹’ç»è¯·æ±‚ï¼Œå› ä¸ºèŠ‚ç‚¹æ€»æ•°ä¼šè¶…è¿‡é™åˆ¶ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„é›†ç¾¤ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé›†ç¾¤ã€‚
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_15.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_15.png)'
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_16.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_16.png)'
- en: Okayï¼Œ there were some examples that has been taken over hereã€‚ So let me remove
    one of themã€‚ Okayã€‚ let me just execute thisã€‚Okayï¼Œ I I'll go over hereã€‚Space let
    me delete itï¼Œ okayã€‚Perfectã€‚ now I'll try to read thisã€‚ Let's seeã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_18.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™é‡Œæœ‰ä¸€äº›ç¤ºä¾‹å·²ç»è¢«æå–å‡ºæ¥ã€‚æ‰€ä»¥è®©æˆ‘åˆ é™¤å…¶ä¸­ä¸€ä¸ªã€‚å¥½çš„ã€‚è®©æˆ‘æ‰§è¡Œè¿™ä¸ªã€‚å¥½çš„ï¼Œæˆ‘ä¼šå»è¿™é‡Œã€‚ç©ºæ ¼è®©æˆ‘åˆ é™¤å®ƒï¼Œå¥½çš„ã€‚å®Œç¾ã€‚ç°åœ¨æˆ‘å°†å°è¯•è¯»å–è¿™ä¸ªã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_18.png)
- en: Againï¼Œ it says fail to create the cluster reject request rejected since the
    total number of nodes would exceed the limit of oneã€‚ and it is not allowing us
    to execute more than one fileï¼Œ I guessã€‚ So because of thatã€‚ I'm just reloading
    itã€‚ Let's see nowã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_20.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œå®ƒæ˜¾ç¤ºåˆ›å»ºé›†ç¾¤å¤±è´¥ï¼Œæ‹’ç»è¯·æ±‚è¢«æ‹’ç»ï¼Œå› ä¸ºèŠ‚ç‚¹æ€»æ•°ä¼šè¶…è¿‡é™åˆ¶ï¼Œå¹¶ä¸”å®ƒä¸å…è®¸æˆ‘ä»¬æ‰§è¡Œè¶…è¿‡ä¸€ä¸ªæ–‡ä»¶ï¼Œæˆ‘æƒ³ã€‚æ­£å› ä¸ºå¦‚æ­¤ã€‚æˆ‘åªæ˜¯é‡æ–°åŠ è½½å®ƒã€‚ç°åœ¨çœ‹çœ‹ã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_20.png)
- en: Now it has got executed C guys before there were two filesã€‚ So because of that
    it was not allowing me to runã€‚ Now I just re I deleted one file and I I reloaded
    one fileã€‚ Okayï¼Œ so now you can see that it is getting run now okay you can also
    press shift tab to basically see some hints and all same like how we do it in
    Jupyter notebook Now here you will be able to see that my file will be running
    absolutely fine and it shows it shows this Df it shows that okay is a pipar dot
    sql do data frame do data frame now let me just execute the other thingsã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ƒå·²ç»æ‰§è¡Œäº†ï¼ŒC ä¼™è®¡ä»¬ï¼Œä¹‹å‰æœ‰ä¸¤ä¸ªæ–‡ä»¶ã€‚æ‰€ä»¥å› ä¸ºè¿™ä¸ªåŸå› ï¼Œå®ƒä¸å…è®¸æˆ‘è¿è¡Œã€‚ç°åœ¨æˆ‘åˆ é™¤äº†ä¸€ä¸ªæ–‡ä»¶å¹¶é‡æ–°åŠ è½½äº†ä¸€ä¸ªæ–‡ä»¶ã€‚å¥½çš„ï¼Œç°åœ¨ä½ å¯ä»¥çœ‹åˆ°å®ƒæ­£åœ¨è¿è¡Œï¼Œä½ ä¹Ÿå¯ä»¥æŒ‰
    shift tab åŸºæœ¬ä¸ŠæŸ¥çœ‹ä¸€äº›æç¤ºï¼Œå°±åƒæˆ‘ä»¬åœ¨ Jupyter notebook ä¸­æ‰€åšçš„é‚£æ ·ã€‚ç°åœ¨åœ¨è¿™é‡Œä½ ä¼šçœ‹åˆ°æˆ‘çš„æ–‡ä»¶è¿è¡Œå¾—éå¸¸é¡ºåˆ©ï¼Œæ˜¾ç¤ºäº† Dfï¼Œæ˜¾ç¤ºå®ƒæ˜¯
    pipar dot sql do data frame do data frame ç°åœ¨è®©æˆ‘æ‰§è¡Œå…¶ä»–å†…å®¹ã€‚
- en: Now suppose if I want Df dot printã€‚Seeï¼Œ I'm just using that tab feature print
    schemaã€‚ If I go and see this hereï¼Œ you'll be able to see find out all the valuesï¼Œ
    rightï¼Œ So in shortã€‚ this is basically now running in my instance of the clusterï¼Œ
    rightã€‚ I will be able to upload any huge data probably a 50 gb data set also from
    S3 bucket and right that I'll try to show you how we can do it from S3 bucket
    in the upcoming videosã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å‡è®¾æˆ‘æƒ³Dç‚¹æ‰“å°ã€‚çœ‹ï¼Œæˆ‘åªæ˜¯ä½¿ç”¨é‚£ä¸ªé€‰é¡¹å¡åŠŸèƒ½æ‰“å°æ¶æ„ã€‚å¦‚æœæˆ‘å»çœ‹è¿™é‡Œï¼Œä½ å°†èƒ½å¤Ÿæ‰¾åˆ°æ‰€æœ‰çš„å€¼ï¼Œå¯¹å§ï¼Œç®€è€Œè¨€ä¹‹ã€‚è¿™åŸºæœ¬ä¸Šç°åœ¨åœ¨æˆ‘çš„é›†ç¾¤å®ä¾‹ä¸­è¿è¡Œï¼Œå¯¹å§ã€‚æˆ‘å°†èƒ½å¤Ÿä¸Šä¼ ä»»ä½•å¤§å‹æ•°æ®ï¼Œå¯èƒ½æ˜¯50GBçš„æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥ä»S3æ¡¶ä¸Šä¼ ï¼Œæ¥ä¸‹æ¥çš„è§†é¢‘æˆ‘ä¼šå‘ä½ å±•ç¤ºå¦‚ä½•ä»S3æ¡¶åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: But what I'm going to show you guys in the upcoming future will try to run all
    this kind of problem statements through the data so that you'll be able to learn
    itã€‚ Okayï¼Œ now let me just go and do one more thingã€‚ So this is my D dot showã€‚
    Okay so this is my entire dataã€‚ probably I will just want to select some columnã€‚
    I can actually write D dot select and hereã€‚I just want to say salary dot showã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘æ¥ä¸‹æ¥è¦å‘å¤§å®¶å±•ç¤ºçš„ï¼Œå°†ä¼šå°è¯•é€šè¿‡æ•°æ®è¿è¡Œæ‰€æœ‰è¿™ç±»é—®é¢˜è¯­å¥ï¼Œä»¥ä¾¿ä½ ä»¬èƒ½å¤Ÿå­¦ä¹ ã€‚å¥½å§ï¼Œç°åœ¨è®©æˆ‘å†åšä¸€ä»¶äº‹ã€‚è¿™æ˜¯æˆ‘çš„Dç‚¹æ˜¾ç¤ºã€‚å¥½å§ï¼Œè¿™æ˜¯æˆ‘æ‰€æœ‰çš„æ•°æ®ã€‚å¯èƒ½æˆ‘åªæƒ³é€‰æ‹©æŸäº›åˆ—ã€‚æˆ‘å®é™…ä¸Šå¯ä»¥å†™Dç‚¹é€‰æ‹©ï¼Œåœ¨è¿™é‡Œã€‚æˆ‘åªæƒ³è¯´è–ªèµ„ç‚¹æ˜¾ç¤ºã€‚
- en: I'm just selecting salary dot show hereï¼Œ you will be able to seeã€‚ So everything
    that you want to doã€‚ you will be able to do itã€‚ And remember over hereï¼Œ you'll
    be able to find out around 15 G Bã€‚ And you can definitely perform any kind of
    thingsã€‚ Okayï¼Œ here alsoã€‚ you have same options like how we have it inã€‚ğŸ˜Šï¼ŒYou knowã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯é€‰æ‹©è–ªèµ„ç‚¹æ˜¾ç¤ºåœ¨è¿™é‡Œï¼Œä½ å°†èƒ½å¤Ÿçœ‹åˆ°ã€‚æ‰€ä»¥ä½ æƒ³åšçš„ä¸€åˆ‡ï¼Œä½ éƒ½èƒ½åšåˆ°ã€‚è¯·è®°ä½ï¼Œåœ¨è¿™é‡Œï¼Œä½ å°†èƒ½å¤Ÿæ‰¾åˆ°å¤§çº¦15GBã€‚è€Œä¸”ä½ ç»å¯¹å¯ä»¥æ‰§è¡Œä»»ä½•ç±»å‹çš„äº‹æƒ…ã€‚å¥½å§ï¼Œè¿™é‡Œä¹Ÿæœ‰ç›¸åŒçš„é€‰é¡¹ï¼Œå°±åƒæˆ‘ä»¬åœ¨ğŸ˜Šï¼Œä½ çŸ¥é“çš„ã€‚
- en: in Jupiter notebook every option is there you will be able to find out all these
    particular options in Jupiter notebook also right so this is basically running
    in 15ã€‚25 gb2 course okay in that particular cluster you have two courseï¼Œ then
    you have spark 3ã€‚1ã€‚1 spark 2ã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Jupyter Notebookä¸­ï¼Œæ¯ä¸ªé€‰é¡¹éƒ½æœ‰ï¼Œä½ å°†èƒ½å¤Ÿåœ¨Jupyter Notebookä¸­æ‰¾åˆ°æ‰€æœ‰è¿™äº›ç‰¹å®šé€‰é¡¹ï¼Œå¯¹å§ï¼Œæ‰€ä»¥è¿™åŸºæœ¬ä¸Šåœ¨15.25GBçš„é›†ç¾¤ä¸­è¿è¡Œã€‚å¥½å§ï¼Œåœ¨é‚£ä¸ªç‰¹å®šé›†ç¾¤ä¸­ï¼Œä½ æœ‰ä¸¤ä¸ªæ ¸å¿ƒï¼Œç„¶åä½ æœ‰Spark
    3.1.1å’ŒSpark 2ã€‚
- en: 12ã€‚And you' will be able to see all this particular information So what I would
    like to want guysã€‚ please try to make a specific environment for you and then
    try to start it try to keep everything ready and from the upcoming videos we will
    try to see how we can execute how we can implement problem statement how we can
    implement different algorithms and probably I'll also show you how we can upload
    a data set from the cloud like AWS and all will start with AWS because it has
    a lot of functionalities altogetherã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 12ã€‚ä½ å°†èƒ½å¤Ÿçœ‹åˆ°æ‰€æœ‰è¿™äº›ç‰¹å®šä¿¡æ¯ã€‚æ‰€ä»¥æˆ‘æƒ³è¦çš„æ˜¯ï¼Œè¯·å°è¯•ä¸ºè‡ªå·±åˆ›å»ºä¸€ä¸ªç‰¹å®šçš„ç¯å¢ƒï¼Œç„¶åå°è¯•å¯åŠ¨å®ƒï¼Œå°½é‡æŠŠä¸€åˆ‡å‡†å¤‡å¥½ï¼Œä»æ¥ä¸‹æ¥çš„è§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•çœ‹çœ‹å¦‚ä½•æ‰§è¡Œï¼Œå¦‚ä½•å®ç°é—®é¢˜è¯­å¥ï¼Œå¦‚ä½•å®ç°ä¸åŒçš„ç®—æ³•ï¼Œå¯èƒ½æˆ‘è¿˜ä¼šå‘ä½ å±•ç¤ºå¦‚ä½•ä»äº‘ç«¯ä¸Šä¼ æ•°æ®é›†ï¼Œæ¯”å¦‚AWSï¼Œæˆ‘ä»¬å°†ä»AWSå¼€å§‹ï¼Œå› ä¸ºå®ƒæœ‰å¾ˆå¤šåŠŸèƒ½ã€‚
- en: And probably well be learning more things as we go itã€‚ So I hope you like this
    particular videoã€‚ please just subscribe the channel if you are not as I see next
    week to have a great dayã€‚ Thank youã€‚ Manalï¼Œ bye byeã€‚ğŸ˜Šã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_22.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”éšç€æˆ‘ä»¬ç»§ç»­å­¦ä¹ ï¼Œå¯èƒ½ä¼šå­¦åˆ°æ›´å¤šçš„ä¸œè¥¿ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªè§†é¢‘ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œä¸‹å‘¨è§ï¼Œç¥ä½ æœ‰ç¾å¥½çš„ä¸€å¤©ã€‚è°¢è°¢ä½ ã€‚Manalï¼Œå†è§ã€‚ğŸ˜Šã€‚![](img/4bf769d9eaf5cd192f62275495d3c7b7_22.png)
