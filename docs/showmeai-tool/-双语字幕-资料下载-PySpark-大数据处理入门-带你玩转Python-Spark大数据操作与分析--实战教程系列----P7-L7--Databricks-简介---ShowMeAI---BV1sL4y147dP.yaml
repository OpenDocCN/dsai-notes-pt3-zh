- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPySpark Â§ßÊï∞ÊçÆÂ§ÑÁêÜÂÖ•Èó®ÔºåÂ∏¶‰Ω†Áé©ËΩ¨Python+SparkÂ§ßÊï∞ÊçÆÊìç‰Ωú‰∏éÂàÜÊûêÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P7ÔºöL7- Databricks
    ÁÆÄ‰ªã - ShowMeAI - BV1sL4y147dP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: „ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Hello all„ÄÇ My name is Krishna and welcome to my usual channel„ÄÇ So guys„ÄÇ we will
    be continuing the Pipark series„ÄÇ And in this particular video„ÄÇ we are going to
    understand what is this databs platform„ÄÇ Now remember„ÄÇ guys in my previous videos
    I've already uploaded all these particular videos with respect to Pipark with
    Python till the Pipar M„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I told you that in this particular video we were to discuss about linear
    regression„ÄÇ how we can implement linear regression with the help of Pipark„ÄÇ But
    before that„ÄÇ I really want to help you know what exactly is a databricks platform„ÄÇ
    And this is an amazing platform where you can actually use PiparkÔºå or you can
    work with Apachepark„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And one more amazing thing about this particular platform is that they also
    provide you cluster instances„ÄÇ So suppose if you have a huge amount of data probably
    want to distribute the parallel processing or probably want to distribute it in
    multiple clusters you can definitely do with the help of databs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòä„ÄÇ![](img/4bf769d9eaf5cd192f62275495d3c7b7_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_4.png)'
  prefs: []
  type: TYPE_IMG
- en: NowÔºå if I really want to use this particular platform„ÄÇ there are two ways one
    is for community version and one is for the paid version„ÄÇ which is like Azure
    or AWS cloud you can actually use in the back end„ÄÇDataricricks also helps you
    to implement ML flow okay and this ML flow is with respect to the CICD pipeline
    so you can also perform those kind of experiments also„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: AltogetherÔºå an amazing platform„ÄÇ What I will be focusing in my YouTube channel
    is that I will try to show you both with the community version also and in the
    upcoming videos will try to execute try to execute with both AWS and Azure when
    we are using AWS and Azure„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what we will try to do is that whenever we create the instances„ÄÇ multiple instances
    know that will try to create in this particular cloud platform we'll also try
    to pull the data from S3 bucket„ÄÇ which is the storage unit in AWs and try to show
    you that how we can work with huge„ÄÇ huge data sets all those things be actually
    showed as we go ahead Now let's understand what this databs is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it is an open and unified data analyticss platform for data engineering data
    science and machine learning analyticss remember databs actually helps us to perform
    data engineering When I say data engineering probably working with big data it
    also helps us to execute some machine learning algorithms probably any kind of
    data science problem statement„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåWillll be able to do it and probably it suppose three kind of platform cloud
    platforms„ÄÇ one is AWS Microsoft Azure and Google Cloud Now if you really want
    to start start with this well start with the community version and you just have
    to go into this particular URL and just type try databs and then you just enter
    all your details to get registered for free„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now once you are registered you once you get started for free„ÄÇ youll get two
    options over there on the right hand side you will be seeing the community version
    which you really want to use it for free and in the left hand side you will be
    having an option where they will tell you that you need to work with this three
    cloud platforms and you can select that also„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So for right now I will try to show you a community versionÔºå which will be very
    simple„ÄÇ very very easy„ÄÇ So let's go to the community version„ÄÇ So this is how the
    community version actually looks like if you really want to go into the cloud
    version you can just click on upgrade okay so just click on upgrade and this is
    the URL of the community version and this version this URL you'll be able to get
    when you register for the community version tomorrows you think that you probably
    want to work with the cloud„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you just have to click on this upgrade now now in this you'll be able to see
    three things one is explore to the explore the quickstar tutorial„ÄÇ import and
    explore dataÔºå create a blank notebook and many more things over here what kind
    of task you'll be able to do in the community version one is you can create a
    new notebook you can create a table„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: create a cluster create new Mflow experiment„ÄÇ I hope I have actually showed
    you Mflow experiment we can also create this MLflow experiment by combining to
    a database in the backend then we can import libraries read document„ÄÇüòä„ÄÇ![](img/4bf769d9eaf5cd192f62275495d3c7b7_6.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_7.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_8.png)'
  prefs: []
  type: TYPE_IMG
- en: Can do a lot of task„ÄÇ NowÔºå first of allÔºå what we need to do is that probably
    I'll create a cluster„ÄÇNowÔºå in order to create a clusterÔºå I will click on thisÔºå
    create a cluster here„ÄÇ you can basically just write down any cluster name„ÄÇ Supp
    I'll say Apache or I'll just say„ÄÇPy spark cluster„ÄÇ Suppose this is my„ÄÇCluster
    that I want to basically create„ÄÇ Okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then here by default over hereÔºå you can see 8„ÄÇ2 scalar„ÄÇ This one spark 3„ÄÇ1„ÄÇ1
    is selected„ÄÇ So we will be working with spark 3„ÄÇ1„ÄÇ1 If you remember in my local
    also I actually installed this particular version only okay by default you will
    be able to see that they will be providing you one instance with 15 gb memory
    and some more configuration if you really want to upgrade your configuration„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can basically go and click over here Okay and remember in the free version
    you will be able to work in an instance unless and until it is not idle for two
    hours otherwise it will get disconnected„ÄÇSo over here you can see one driverÔºå15„ÄÇ3
    Gb memory2 course and one D„ÄÇ Okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: all these things are there„ÄÇ You can also understand what debut is De is nothing
    but a data bricks unit„ÄÇ If you want to click over hereÔºå youll be able to understand
    what exactly debut is okay and youll be able to select a cloud and basically work
    with that perfect till here everything is fine„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's startÔºå let's create the cluster„ÄÇ NowÔºå once you you will be seeing that
    the cluster is basically getting created„ÄÇ you also have lot of options over hereÔºå
    like notebookÔºå librariesÔºå event logs„ÄÇ spark Ui driver logs and all It's not like
    you just have you' will be able to work with Python over here here you have lot
    of options„ÄÇ so suppose if I go and click on libraries„ÄÇ And if I click on install
    new here you will be having an option to upload the libraries„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can also install the libraries from Pi from Mayn„ÄÇ which we basically use
    along a Java then you have different different workspace„ÄÇ So here what I'm going
    to do is that suppose if you select pipe and suppose you want to install some
    of the library„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_10.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_11.png)'
  prefs: []
  type: TYPE_IMG
- en: I like Tensorflow or probably you want to go with ks you can basically write
    like this probably I want a scale learn you know„ÄÇ so I can just give comma separated
    and start installing them Okay but by default I know I'm going to work with Pi
    Sp so I'm not going to install any libraries so let's see how much time this will
    probably take this is just getting executed over here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_13.png)'
  prefs: []
  type: TYPE_IMG
- en: And let's go back to my home„ÄÇ So apart from this yearÔºå you'll be also able to
    upload the data set„ÄÇ and that particular data will give you an environment like
    how you're storing the data in the do„ÄÇ OkayÔºå so before the cluster is getting
    created„ÄÇ now the cluster has got created here you can see Pi pocket is in running
    state now and remember this cluster only has one instance„ÄÇ you want to create
    multiple clusters„ÄÇ We have to use the cloud platform1Ôºå which will be chargeable„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Okay so in here I'm going to click on export the data„ÄÇ Now see guysÔºå you can
    upload the data„ÄÇ you can also bring from S 3 bucket„ÄÇ you can also then bring from
    S3 bucket„ÄÇ These all things I'll try to show you„ÄÇ then you also have Dfs you know„ÄÇüòäÔºåAnd
    D V F F„ÄÇ you will basically be storing inside this particular format„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then you have other data sources like Amazon Re script Amazon kindnesses„ÄÇ Amazon
    Kinnesses is basically used for live streaming data„ÄÇ OkayÔºå then you have cassandra„ÄÇCassra
    is also a no SQL database and JDBC last search„ÄÇ So different different data data
    sources also there we'll also try to see with respect to partners integration„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So they are also like real time capture in data lake„ÄÇ and many more things are
    there„ÄÇ So you can definitely have a look onto this„ÄÇ Now what I'm going to do is
    that I'm just going to click over here and try to upload a data let me just see„ÄÇLet
    me just upload the data sets„ÄÇ I'll just go to my Pipar folder„ÄÇSo here is my pass
    well I'm just going to upload this test data set probably„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: or I'll try to upload this test one„ÄÇNowÔºå here you can see that the data set
    has been uploaded„ÄÇ Now it is saying that create table with UI create table in
    note notebook„ÄÇ supposeupp if I go and click thisÔºå you know„ÄÇSo here you will be
    able to see this is the code„ÄÇ this is the entire code to basically create a table
    in the Ui„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but what I really want to do is that I don't want to create a table instead
    I'll just try to execute some of the Pipar code which we have already learned
    till now Okay„ÄÇ so what I' am going to do„ÄÇI'll just remove this„ÄÇ I don't want it„ÄÇ
    I'll remove thisÔºå okay„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let me read the data set now for reading the data set„ÄÇOver here„ÄÇ you'll be able
    to see that my dataset path is basically this„ÄÇ It is a CSV file info schema header
    schemaÔºå all these things are there„ÄÇ So let me remove this also„ÄÇ So let me start
    reading the data„ÄÇ So by defaultÔºå Sp is already uploaded„ÄÇ So I write Sp dot„ÄÇSk
    dot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Read dot cv„ÄÇI hope so it will work„ÄÇ And for the first timeÔºå rememberÔºå this is
    my file location„ÄÇFile locationÔºå okayÔºå file underscore location„ÄÇ And then I will
    also be using two more option„ÄÇ One is header physicalical to true„ÄÇAnd then I have
    infer schema is once I execute this now you will be seeing that automatically
    the first time menu executing it will say that launch and run so we are going
    to launch the cluster and run it So I'm just going to click it fail to create
    reject requestquies since the total number of nodes would exit the limit one Why
    this is there let's see our clusters we just have one cluster„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_15.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_16.png)'
  prefs: []
  type: TYPE_IMG
- en: OkayÔºå there were some examples that has been taken over here„ÄÇ So let me remove
    one of them„ÄÇ Okay„ÄÇ let me just execute this„ÄÇOkayÔºå I I'll go over here„ÄÇSpace let
    me delete itÔºå okay„ÄÇPerfect„ÄÇ now I'll try to read this„ÄÇ Let's see„ÄÇ![](img/4bf769d9eaf5cd192f62275495d3c7b7_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: AgainÔºå it says fail to create the cluster reject request rejected since the
    total number of nodes would exceed the limit of one„ÄÇ and it is not allowing us
    to execute more than one fileÔºå I guess„ÄÇ So because of that„ÄÇ I'm just reloading
    it„ÄÇ Let's see now„ÄÇ![](img/4bf769d9eaf5cd192f62275495d3c7b7_20.png)
  prefs: []
  type: TYPE_NORMAL
- en: Now it has got executed C guys before there were two files„ÄÇ So because of that
    it was not allowing me to run„ÄÇ Now I just re I deleted one file and I I reloaded
    one file„ÄÇ OkayÔºå so now you can see that it is getting run now okay you can also
    press shift tab to basically see some hints and all same like how we do it in
    Jupyter notebook Now here you will be able to see that my file will be running
    absolutely fine and it shows it shows this Df it shows that okay is a pipar dot
    sql do data frame do data frame now let me just execute the other things„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose if I want Df dot print„ÄÇSeeÔºå I'm just using that tab feature print
    schema„ÄÇ If I go and see this hereÔºå you'll be able to see find out all the valuesÔºå
    rightÔºå So in short„ÄÇ this is basically now running in my instance of the clusterÔºå
    right„ÄÇ I will be able to upload any huge data probably a 50 gb data set also from
    S3 bucket and right that I'll try to show you how we can do it from S3 bucket
    in the upcoming videos„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But what I'm going to show you guys in the upcoming future will try to run all
    this kind of problem statements through the data so that you'll be able to learn
    it„ÄÇ OkayÔºå now let me just go and do one more thing„ÄÇ So this is my D dot show„ÄÇ
    Okay so this is my entire data„ÄÇ probably I will just want to select some column„ÄÇ
    I can actually write D dot select and here„ÄÇI just want to say salary dot show„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm just selecting salary dot show hereÔºå you will be able to see„ÄÇ So everything
    that you want to do„ÄÇ you will be able to do it„ÄÇ And remember over hereÔºå you'll
    be able to find out around 15 G B„ÄÇ And you can definitely perform any kind of
    things„ÄÇ OkayÔºå here also„ÄÇ you have same options like how we have it in„ÄÇüòäÔºåYou know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: in Jupiter notebook every option is there you will be able to find out all these
    particular options in Jupiter notebook also right so this is basically running
    in 15„ÄÇ25 gb2 course okay in that particular cluster you have two courseÔºå then
    you have spark 3„ÄÇ1„ÄÇ1 spark 2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 12„ÄÇAnd you' will be able to see all this particular information So what I would
    like to want guys„ÄÇ please try to make a specific environment for you and then
    try to start it try to keep everything ready and from the upcoming videos we will
    try to see how we can execute how we can implement problem statement how we can
    implement different algorithms and probably I'll also show you how we can upload
    a data set from the cloud like AWS and all will start with AWS because it has
    a lot of functionalities altogether„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And probably well be learning more things as we go it„ÄÇ So I hope you like this
    particular video„ÄÇ please just subscribe the channel if you are not as I see next
    week to have a great day„ÄÇ Thank you„ÄÇ ManalÔºå bye bye„ÄÇüòä„ÄÇ![](img/4bf769d9eaf5cd192f62275495d3c7b7_22.png)
  prefs: []
  type: TYPE_NORMAL
