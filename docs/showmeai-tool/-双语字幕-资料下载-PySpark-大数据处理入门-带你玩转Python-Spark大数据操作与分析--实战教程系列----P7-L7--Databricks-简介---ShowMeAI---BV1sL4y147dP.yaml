- en: 【双语字幕+资料下载】PySpark 大数据处理入门，带你玩转Python+Spark大数据操作与分析！＜实战教程系列＞ - P7：L7- Databricks
    简介 - ShowMeAI - BV1sL4y147dP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】PySpark 大数据处理入门，带你玩转Python+Spark大数据操作与分析！＜实战教程系列＞ - P7：L7- **Databricks**
    简介 - ShowMeAI - BV1sL4y147dP
- en: 。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_1.png)'
- en: Hello all。 My name is Krishna and welcome to my usual channel。 So guys。 we will
    be continuing the Pipark series。 And in this particular video。 we are going to
    understand what is this databs platform。 Now remember。 guys in my previous videos
    I've already uploaded all these particular videos with respect to Pipark with
    Python till the Pipar M。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。我是**Krishna**，欢迎来到我的频道。所以大家，我们将继续**Pipark**系列。在这个特定的视频中，我们将了解这个**Databricks**平台。记住，大家，在我之前的视频中，我已经上传了所有关于**Pipark**与Python的内容，直到**Pipar
    M**。
- en: And I told you that in this particular video we were to discuss about linear
    regression。 how we can implement linear regression with the help of Pipark。 But
    before that。 I really want to help you know what exactly is a databricks platform。
    And this is an amazing platform where you can actually use Pipark， or you can
    work with Apachepark。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉过你，在这个特定的视频中，我们将讨论线性回归，以及如何借助**Pipark**实现线性回归。但在此之前，我真的想帮助你了解什么是**Databricks**平台。这是一个很棒的平台，你可以在这里使用**Pipark**，或者与**Apachepark**合作。
- en: And one more amazing thing about this particular platform is that they also
    provide you cluster instances。 So suppose if you have a huge amount of data probably
    want to distribute the parallel processing or probably want to distribute it in
    multiple clusters you can definitely do with the help of databs。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个关于这个特定平台的惊人之处在于，他们还为你提供了集群实例。所以如果你有大量数据，想要进行并行处理，或者想要在多个集群中分配数据，你绝对可以借助**Databricks**做到这一点。
- en: 😊。![](img/4bf769d9eaf5cd192f62275495d3c7b7_3.png)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/4bf769d9eaf5cd192f62275495d3c7b7_3.png)
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_4.png)'
- en: Now， if I really want to use this particular platform。 there are two ways one
    is for community version and one is for the paid version。 which is like Azure
    or AWS cloud you can actually use in the back end。Dataricricks also helps you
    to implement ML flow okay and this ML flow is with respect to the CICD pipeline
    so you can also perform those kind of experiments also。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我真的想使用这个平台，有两种方式：一种是社区版本，另一种是付费版本，例如**Azure**或**AWS**云，你实际上可以在后端使用它。**Dataricricks**还帮助你实现**ML
    flow**，而这个**ML flow**与**CICD**管道有关，所以你也可以进行这些实验。
- en: Altogether， an amazing platform。 What I will be focusing in my YouTube channel
    is that I will try to show you both with the community version also and in the
    upcoming videos will try to execute try to execute with both AWS and Azure when
    we are using AWS and Azure。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这是一个很棒的平台。我在我的YouTube频道上会专注于展示社区版本，并且在接下来的视频中会尝试使用**AWS**和**Azure**进行执行。
- en: what we will try to do is that whenever we create the instances。 multiple instances
    know that will try to create in this particular cloud platform we'll also try
    to pull the data from S3 bucket。 which is the storage unit in AWs and try to show
    you that how we can work with huge。 huge data sets all those things be actually
    showed as we go ahead Now let's understand what this databs is。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试做的是，每当我们创建多个实例时，知道我们会尝试在这个特定的云平台上创建多个实例，我们还会尝试从**S3 bucket**中拉取数据，**S3
    bucket**是**AWS**中的存储单元，并尝试向你展示如何处理庞大的数据集，所有这些内容会随着我们的进展而展示。现在让我们了解一下这个**Databricks**是什么。
- en: it is an open and unified data analyticss platform for data engineering data
    science and machine learning analyticss remember databs actually helps us to perform
    data engineering When I say data engineering probably working with big data it
    also helps us to execute some machine learning algorithms probably any kind of
    data science problem statement。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个开放和统一的数据分析平台，适用于数据工程、数据科学和机器学习分析。记住，**Databricks**实际上帮助我们执行数据工程，当我说数据工程时，可能是在处理大数据，它还帮助我们执行一些机器学习算法，可能是任何类型的数据科学问题陈述。
- en: 😊，Willll be able to do it and probably it suppose three kind of platform cloud
    platforms。 one is AWS Microsoft Azure and Google Cloud Now if you really want
    to start start with this well start with the community version and you just have
    to go into this particular URL and just type try databs and then you just enter
    all your details to get registered for free。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，你将能够做到这一点，可能会涉及三种云平台：一个是**AWS**，一个是**Microsoft Azure**，还有一个是**Google Cloud**。如果你真的想开始，可以先从社区版本开始，你只需访问这个特定的URL，输入“try
    databs”，然后输入所有你的详细信息以免费注册。
- en: Now once you are registered you once you get started for free。 youll get two
    options over there on the right hand side you will be seeing the community version
    which you really want to use it for free and in the left hand side you will be
    having an option where they will tell you that you need to work with this three
    cloud platforms and you can select that also。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你注册并开始免费使用，你将在右侧看到两个选项，你可以看到你想免费使用的社区版本，而在左侧则会有一个选项，告诉你需要与这三大云平台合作，你也可以选择那个。
- en: So for right now I will try to show you a community version， which will be very
    simple。 very very easy。 So let's go to the community version。 So this is how the
    community version actually looks like if you really want to go into the cloud
    version you can just click on upgrade okay so just click on upgrade and this is
    the URL of the community version and this version this URL you'll be able to get
    when you register for the community version tomorrows you think that you probably
    want to work with the cloud。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将尝试向你展示一个社区版本，它会非常简单，非常容易。让我们进入社区版本。实际上，社区版本是这样的，如果你真的想使用云版本，可以点击升级，好的，点击升级，这就是社区版本的URL，注册社区版本时，你将能够获得这个URL，明天你可能会想使用云服务。
- en: you just have to click on this upgrade now now in this you'll be able to see
    three things one is explore to the explore the quickstar tutorial。 import and
    explore data， create a blank notebook and many more things over here what kind
    of task you'll be able to do in the community version one is you can create a
    new notebook you can create a table。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需点击这个升级，现在你将看到三个选项，一个是探索快速入门教程，导入和探索数据，创建一个空白笔记本，还有更多你可以在社区版本中完成的任务，比如创建新笔记本或创建表格。
- en: create a cluster create new Mflow experiment。 I hope I have actually showed
    you Mflow experiment we can also create this MLflow experiment by combining to
    a database in the backend then we can import libraries read document。😊。![](img/4bf769d9eaf5cd192f62275495d3c7b7_6.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个集群，创建新的Mflow实验。我希望我已经向你展示了Mflow实验，我们还可以通过将其与后端数据库结合来创建这个MLflow实验，然后我们可以导入库，读取文档。😊！[](img/4bf769d9eaf5cd192f62275495d3c7b7_6.png)
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_7.png)'
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_8.png)'
- en: Can do a lot of task。 Now， first of all， what we need to do is that probably
    I'll create a cluster。Now， in order to create a cluster， I will click on this，
    create a cluster here。 you can basically just write down any cluster name。 Supp
    I'll say Apache or I'll just say。Py spark cluster。 Suppose this is my。Cluster
    that I want to basically create。 Okay。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以完成很多任务。首先，我们需要做的可能是创建一个集群。现在，为了创建一个集群，我会点击这里的创建集群。你可以基本上写下任何集群名称。我会说Apache，或者我会说Py
    spark集群。假设这是我想要创建的集群。好的。
- en: and then here by default over here， you can see 8。2 scalar。 This one spark 3。1。1
    is selected。 So we will be working with spark 3。1。1 If you remember in my local
    also I actually installed this particular version only okay by default you will
    be able to see that they will be providing you one instance with 15 gb memory
    and some more configuration if you really want to upgrade your configuration。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里默认情况下，你可以看到8.2 scalar。选定的是spark 3.1.1。因此，我们将使用spark 3.1.1。如果你记得，我在本地也安装了这个特定版本，默认情况下，你会看到他们提供一个15
    GB内存的实例和更多配置，如果你真的想升级你的配置。
- en: you can basically go and click over here Okay and remember in the free version
    you will be able to work in an instance unless and until it is not idle for two
    hours otherwise it will get disconnected。So over here you can see one driver，15。3
    Gb memory2 course and one D。 Okay。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你基本上可以点击这里。好的，记住在免费版本中，你将能够在一个实例中工作，除非它空闲超过两个小时，否则它将断开连接。所以在这里你可以看到一个驱动程序，15.3
    GB内存，2核和一个D。
- en: all these things are there。 You can also understand what debut is De is nothing
    but a data bricks unit。 If you want to click over here， youll be able to understand
    what exactly debut is okay and youll be able to select a cloud and basically work
    with that perfect till here everything is fine。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能都在这里。你还可以理解什么是debut，de是数据砖单位。如果你想点击这里，你将能够理解debut究竟是什么，好的，你将能够选择一个云，并基本上与之合作，完美，到这里一切都很好。
- en: let's start， let's create the cluster。 Now， once you you will be seeing that
    the cluster is basically getting created。 you also have lot of options over here，
    like notebook， libraries， event logs。 spark Ui driver logs and all It's not like
    you just have you' will be able to work with Python over here here you have lot
    of options。 so suppose if I go and click on libraries。 And if I click on install
    new here you will be having an option to upload the libraries。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始，创建集群。现在，一旦你看到集群正在创建。你这里还有很多选项，比如笔记本、库、事件日志。Spark UI驱动程序日志等等。这并不是说你只能在这里使用Python，你这里有很多选项。所以假设我去点击库。如果我点击安装新库，你将有一个选项来上传库。
- en: you can also install the libraries from Pi from Mayn。 which we basically use
    along a Java then you have different different workspace。 So here what I'm going
    to do is that suppose if you select pipe and suppose you want to install some
    of the library。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从Mayn的Pi安装库。我们基本上在Java中使用这些库，然后你会有不同的工作空间。所以我将做的是，假设你选择管道，如果你想安装一些库。
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_10.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_10.png)'
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_11.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_11.png)'
- en: I like Tensorflow or probably you want to go with ks you can basically write
    like this probably I want a scale learn you know。 so I can just give comma separated
    and start installing them Okay but by default I know I'm going to work with Pi
    Sp so I'm not going to install any libraries so let's see how much time this will
    probably take this is just getting executed over here。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢Tensorflow，或者你可能想选择ks，你可以这样写，我可能想要一个scale learn。你知道，所以我可以用逗号分隔并开始安装它们。好的，但默认情况下，我知道我将使用Pi
    Sp，所以我不打算安装任何库。让我们看看这可能需要多长时间，这里只是执行。
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_13.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_13.png)'
- en: And let's go back to my home。 So apart from this year， you'll be also able to
    upload the data set。 and that particular data will give you an environment like
    how you're storing the data in the do。 Okay， so before the cluster is getting
    created。 now the cluster has got created here you can see Pi pocket is in running
    state now and remember this cluster only has one instance。 you want to create
    multiple clusters。 We have to use the cloud platform1， which will be chargeable。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我的主页。所以除了这一年，你还能够上传数据集。那个特定的数据将给你一个环境，像你如何在do中存储数据。好的，所以在集群创建之前，现在集群已经创建了，你可以看到Pi
    pocket处于运行状态，记住这个集群只有一个实例。如果你想创建多个集群，我们必须使用云平台1，这将是收费的。
- en: Okay so in here I'm going to click on export the data。 Now see guys， you can
    upload the data。 you can also bring from S 3 bucket。 you can also then bring from
    S3 bucket。 These all things I'll try to show you。 then you also have Dfs you know。😊，And
    D V F F。 you will basically be storing inside this particular format。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，所以我在这里将点击导出数据。现在大家看，你可以上传数据。你也可以从S3桶中获取。然后你也可以从S3桶中获取。这些我会试着给你展示。然后你也有Dfs，你知道。😊，还有D
    V F F。你基本上会存储在这个特定的格式中。
- en: Then you have other data sources like Amazon Re script Amazon kindnesses。 Amazon
    Kinnesses is basically used for live streaming data。 Okay， then you have cassandra。Cassra
    is also a no SQL database and JDBC last search。 So different different data data
    sources also there we'll also try to see with respect to partners integration。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你有其他数据源，比如Amazon Re script Amazon kindnesses。Amazon Kinnesses主要用于实时流数据。好的，然后你有Cassandra。Cassandra也是一个No
    SQL数据库，还有JDBC最后搜索。因此，也有不同的数据源，我们还会尝试与合作伙伴的集成。
- en: So they are also like real time capture in data lake。 and many more things are
    there。 So you can definitely have a look onto this。 Now what I'm going to do is
    that I'm just going to click over here and try to upload a data let me just see。Let
    me just upload the data sets。 I'll just go to my Pipar folder。So here is my pass
    well I'm just going to upload this test data set probably。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们也像实时捕获在数据湖中，还有更多的东西在这里。因此，你绝对可以看看这个。现在我将做的是，点击这里尝试上传数据，让我看看。我将上传数据集。我会去我的Pipar文件夹。所以这里是我的路径，我将上传这个测试数据集。
- en: or I'll try to upload this test one。Now， here you can see that the data set
    has been uploaded。 Now it is saying that create table with UI create table in
    note notebook。 supposeupp if I go and click this， you know。So here you will be
    able to see this is the code。 this is the entire code to basically create a table
    in the Ui。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我会尝试上传这个测试文件。现在，你可以看到数据集已经上传。现在它说要通过 UI 创建表格，在笔记本中创建表格。如果我去点击这个，你知道的。所以这里你将能够看到这是代码。这是创建表格的完整代码。
- en: but what I really want to do is that I don't want to create a table instead
    I'll just try to execute some of the Pipar code which we have already learned
    till now Okay。 so what I' am going to do。I'll just remove this。 I don't want it。
    I'll remove this， okay。Okay。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但我真正想做的是我不想创建表格，而是尝试执行一些我们到现在为止已经学过的 Pipar 代码。好的。所以我将做的就是删除这个。我不想要它。我将删除这个，好的。好的。
- en: let me read the data set now for reading the data set。Over here。 you'll be able
    to see that my dataset path is basically this。 It is a CSV file info schema header
    schema， all these things are there。 So let me remove this also。 So let me start
    reading the data。 So by default， Sp is already uploaded。 So I write Sp dot。Sk
    dot。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我现在读取数据集。在这里，你会看到我的数据集路径基本上是这个。它是一个 CSV 文件，包含 schema 头部 schema，所有这些内容都在这里。所以让我也删除这个。让我开始读取数据。因此默认情况下，Sp
    已经上传。所以我写 Sp dot。Sk dot。
- en: Read dot cv。I hope so it will work。 And for the first time， remember， this is
    my file location。File location， okay， file underscore location。 And then I will
    also be using two more option。 One is header physicalical to true。And then I have
    infer schema is once I execute this now you will be seeing that automatically
    the first time menu executing it will say that launch and run so we are going
    to launch the cluster and run it So I'm just going to click it fail to create
    reject requestquies since the total number of nodes would exit the limit one Why
    this is there let's see our clusters we just have one cluster。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读 dot cv。我希望它能正常工作。首先，请记住，这是我的文件位置。文件位置，好的，文件下划线位置。然后我还会使用两个选项。一个是将 header
    physicalical 设置为 true。然后我有 infer schema 一旦我执行这个，现在你会看到第一次菜单执行时会显示启动并运行，所以我们将启动集群并运行它。我将点击它，创建失败，拒绝请求，因为节点总数会超过限制。这是为什么呢？让我们看看我们的集群，我们只有一个集群。
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_15.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_15.png)'
- en: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_16.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf769d9eaf5cd192f62275495d3c7b7_16.png)'
- en: Okay， there were some examples that has been taken over here。 So let me remove
    one of them。 Okay。 let me just execute this。Okay， I I'll go over here。Space let
    me delete it， okay。Perfect。 now I'll try to read this。 Let's see。![](img/4bf769d9eaf5cd192f62275495d3c7b7_18.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这里有一些示例已经被提取出来。所以让我删除其中一个。好的。让我执行这个。好的，我会去这里。空格让我删除它，好的。完美。现在我将尝试读取这个。让我们看看。![](img/4bf769d9eaf5cd192f62275495d3c7b7_18.png)
- en: Again， it says fail to create the cluster reject request rejected since the
    total number of nodes would exceed the limit of one。 and it is not allowing us
    to execute more than one file， I guess。 So because of that。 I'm just reloading
    it。 Let's see now。![](img/4bf769d9eaf5cd192f62275495d3c7b7_20.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，它显示创建集群失败，拒绝请求被拒绝，因为节点总数会超过限制，并且它不允许我们执行超过一个文件，我想。正因为如此。我只是重新加载它。现在看看。![](img/4bf769d9eaf5cd192f62275495d3c7b7_20.png)
- en: Now it has got executed C guys before there were two files。 So because of that
    it was not allowing me to run。 Now I just re I deleted one file and I I reloaded
    one file。 Okay， so now you can see that it is getting run now okay you can also
    press shift tab to basically see some hints and all same like how we do it in
    Jupyter notebook Now here you will be able to see that my file will be running
    absolutely fine and it shows it shows this Df it shows that okay is a pipar dot
    sql do data frame do data frame now let me just execute the other things。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它已经执行了，C 伙计们，之前有两个文件。所以因为这个原因，它不允许我运行。现在我删除了一个文件并重新加载了一个文件。好的，现在你可以看到它正在运行，你也可以按
    shift tab 基本上查看一些提示，就像我们在 Jupyter notebook 中所做的那样。现在在这里你会看到我的文件运行得非常顺利，显示了 Df，显示它是
    pipar dot sql do data frame do data frame 现在让我执行其他内容。
- en: Now suppose if I want Df dot print。See， I'm just using that tab feature print
    schema。 If I go and see this here， you'll be able to see find out all the values，
    right， So in short。 this is basically now running in my instance of the cluster，
    right。 I will be able to upload any huge data probably a 50 gb data set also from
    S3 bucket and right that I'll try to show you how we can do it from S3 bucket
    in the upcoming videos。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我想D点打印。看，我只是使用那个选项卡功能打印架构。如果我去看这里，你将能够找到所有的值，对吧，简而言之。这基本上现在在我的集群实例中运行，对吧。我将能够上传任何大型数据，可能是50GB的数据集，也可以从S3桶上传，接下来的视频我会向你展示如何从S3桶做到这一点。
- en: But what I'm going to show you guys in the upcoming future will try to run all
    this kind of problem statements through the data so that you'll be able to learn
    it。 Okay， now let me just go and do one more thing。 So this is my D dot show。
    Okay so this is my entire data。 probably I will just want to select some column。
    I can actually write D dot select and here。I just want to say salary dot show。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但我接下来要向大家展示的，将会尝试通过数据运行所有这类问题语句，以便你们能够学习。好吧，现在让我再做一件事。这是我的D点显示。好吧，这是我所有的数据。可能我只想选择某些列。我实际上可以写D点选择，在这里。我只想说薪资点显示。
- en: I'm just selecting salary dot show here， you will be able to see。 So everything
    that you want to do。 you will be able to do it。 And remember over here， you'll
    be able to find out around 15 G B。 And you can definitely perform any kind of
    things。 Okay， here also。 you have same options like how we have it in。😊，You know。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是选择薪资点显示在这里，你将能够看到。所以你想做的一切，你都能做到。请记住，在这里，你将能够找到大约15GB。而且你绝对可以执行任何类型的事情。好吧，这里也有相同的选项，就像我们在😊，你知道的。
- en: in Jupiter notebook every option is there you will be able to find out all these
    particular options in Jupiter notebook also right so this is basically running
    in 15。25 gb2 course okay in that particular cluster you have two course， then
    you have spark 3。1。1 spark 2。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，每个选项都有，你将能够在Jupyter Notebook中找到所有这些特定选项，对吧，所以这基本上在15.25GB的集群中运行。好吧，在那个特定集群中，你有两个核心，然后你有Spark
    3.1.1和Spark 2。
- en: 12。And you' will be able to see all this particular information So what I would
    like to want guys。 please try to make a specific environment for you and then
    try to start it try to keep everything ready and from the upcoming videos we will
    try to see how we can execute how we can implement problem statement how we can
    implement different algorithms and probably I'll also show you how we can upload
    a data set from the cloud like AWS and all will start with AWS because it has
    a lot of functionalities altogether。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 12。你将能够看到所有这些特定信息。所以我想要的是，请尝试为自己创建一个特定的环境，然后尝试启动它，尽量把一切准备好，从接下来的视频中，我们将尝试看看如何执行，如何实现问题语句，如何实现不同的算法，可能我还会向你展示如何从云端上传数据集，比如AWS，我们将从AWS开始，因为它有很多功能。
- en: And probably well be learning more things as we go it。 So I hope you like this
    particular video。 please just subscribe the channel if you are not as I see next
    week to have a great day。 Thank you。 Manal， bye bye。😊。![](img/4bf769d9eaf5cd192f62275495d3c7b7_22.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 而且随着我们继续学习，可能会学到更多的东西。所以我希望你喜欢这个视频。如果你还没有，请订阅频道，下周见，祝你有美好的一天。谢谢你。Manal，再见。😊。![](img/4bf769d9eaf5cd192f62275495d3c7b7_22.png)
