# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Hugging Faceé€ŸæˆæŒ‡å—ï¼ä¸€éæå®šNLPä»»åŠ¡ä¸­æœ€å¸¸ç”¨çš„åŠŸèƒ½æ¿å—ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- å¾®è°ƒ(Fine Tuning) - ShowMeAI - BV1cF411v7kC

Another a look at how we can finet tune our own modelsã€‚ So this is very importantã€‚

 and I already prepared some code hereï¼Œ and I will go over this very roughlyã€‚

 but there's also a very great documentation about thisã€‚

 So we can go to this documentation page hereã€‚ and you can also open this in coloã€‚

 So either with Pytorch or tens of flow codeã€‚ So this is really helpfulã€‚

 So I encourage you to check this outã€‚ But now let's go over this brieflyã€‚

 So basically there are five steps you have to doã€‚ So in this exampleï¼Œ it's for Pytorchã€‚

 So we have to prepare our data setï¼Œ for exampleï¼Œ load it from a Cv file or whateverã€‚

 Then we have to load a pretrained tokenizer and then call it with our dataset setã€‚

 So then we get the encodings or the token Isã€‚ğŸ˜Šã€‚

![](img/4ee3097c7f433d913bfc9e33552e43f7_1.png)

![](img/4ee3097c7f433d913bfc9e33552e43f7_2.png)

Then we have to build a pytorch dataset set out of this with these encodingã€‚

 So if you don't know what a pytorch dataset isï¼Œ then I will have a link for you here where I explain thisã€‚

 Then we also load a pretrain model and then we can either load a hugging face trainer and train itã€‚

 So this abstracts away a lot of thingsï¼Œ or we can just use a native or normal pytorch training pipeline like in our other pytorch codeã€‚

 So yeahï¼Œ this is what we have to doã€‚ So let's go over this very quicklyã€‚ So in this caseã€‚

 we define our base model nameã€‚ So we want to start with a thistilbert face uncased versionã€‚

 So in this caseï¼Œ for exampleï¼Œ not the fine tuned oneã€‚ So just this oneã€‚ then step 1ã€‚

 we prepare the dataã€‚ So we write a helpful function to create text andã€‚

Labels out of the actual text and here we downloaded some data set and put it in our folderã€‚

 so I already did this here and yeah this is available at this website and this contains movie reviews so we want to find you in our models on movie reviews for sentiment classification So here we create training text and the training labels with our helper function and we also do a train test split to get validation text and labels and yeah then as a next step we create or we define a Pytorrch data setã€‚

 So this inherits from pitorrch data set so torch U data we import data and then we define this here So again I have a tutorial where I explain how this works but basically it needs the encodecodings andã€‚



![](img/4ee3097c7f433d913bfc9e33552e43f7_4.png)

![](img/4ee3097c7f433d913bfc9e33552e43f7_5.png)

The labels and it stores them in hereã€‚ So yeahï¼Œ this needs the encodingsã€‚ So for the encodingsã€‚

 we need a tokenizerã€‚ So againï¼Œ we use this from pretrain function with the model name And in this caseã€‚

 since we know we use the distill bird oneï¼Œ we can use this classã€‚

 So remember before we used a generic tokenizerï¼Œ this auto tokenizer classã€‚

 And here we used a more concrete one So we use the distal bird tokenizer fast then we apply it to training validation and test set and get the encodingsã€‚

 then we put them in our data set and create the Pytorch data set and then we import a trainer and the training argumentã€‚

 So this is an available in Trans us library and then we can set this up so we canã€‚

Create the argumentsã€‚ So hereï¼Œ for exampleï¼Œ we specify the number of training epochsã€‚

 the output directoryï¼Œ the learning rate and other parameters we want and then we create our model again from a concrete model class and then with this dot from pretrained function and then we set up this trainer and give it the model and the training arguments and then the training set and the validation set and then we simply have to call trainer the train and this will do all the training for us and afterwards you can test it on your test data set and then you have a fine tune modelã€‚

 So yeahï¼Œ this is basically all you know it and then I also want to show you that instead of using this trainer if you want to do it manually and have even more flexibility you can just use a normal Pytht training loopã€‚

For thisï¼Œ we use a data loader and we need an optimizationã€‚ So in this caseã€‚

 we use an optimizer from the Transformers libraryã€‚

 and then here we specify our device then again we create this modelã€‚

 We push it to the device and set it to training mode then we create a data loader and the optimizer and then we do the typical training loop So we say for epoch in nu epos and for batch in our training loader and then we do the stuff we always doã€‚

 we say optimizer zero gradï¼Œ we also push it to the device if necessaryã€‚

 then we call the model and we calculate the loss with this and in this case this is already contained in the output so we can just access the loss like this then we call loss the backward and optimizer step and iterate and afterwards we can set our model toã€‚

Evaluation mode againã€‚ and yeahï¼Œ this is how we do it in native Pythtorch codeã€‚

 and yeah so this is basically how we do a fine tuning and then can fine tune our own models And then afterwards you can also upload them to the hugging face model hub if you want So yeah I think that's pretty cool and yeah that's all that I wanted to show you for now I think that's enough to get started with hugging face and I hope you enjoyed this tutorial and then I hope to see in the next videoã€‚

ğŸ˜Šã€‚

![](img/4ee3097c7f433d913bfc9e33552e43f7_7.png)