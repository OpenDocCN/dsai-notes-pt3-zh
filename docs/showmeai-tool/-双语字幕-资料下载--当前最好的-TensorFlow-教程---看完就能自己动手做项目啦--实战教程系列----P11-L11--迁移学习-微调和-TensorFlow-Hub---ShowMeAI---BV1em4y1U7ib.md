# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P11ÔºöL11- ËøÅÁßªÂ≠¶‰π†„ÄÅÂæÆË∞ÉÂíå TensorFlow Hub - ShowMeAI - BV1em4y1U7ib

In this video I will show you how to use pretrain models„ÄÇ including how to freeze layers and do fine tuning„ÄÇ so let's get to it right after this beautiful intro„ÄÇ![](img/e49a5edbd4e401e53208da1309187c97_1.png)

![](img/e49a5edbd4e401e53208da1309187c97_2.png)

AlrightÔºå so the code in front of us right now should hopefully feel very familiar„ÄÇ We've been using these for pretty much all of the videos except this command right here„ÄÇ Tensorflowlow hub„ÄÇ So I'm going go into what that is a little bit later in the tutorial but first of all„ÄÇ if you if you don't have it just Google Conda Tensorflowlow hubub and you'll get to this page it's also gonna to be in the description and you can download it with this command if you're using Conda if you're using Pip you're just going to do Pip install Tensorflowlow hub that's it„ÄÇ



![](img/e49a5edbd4e401e53208da1309187c97_4.png)

![](img/e49a5edbd4e401e53208da1309187c97_5.png)

![](img/e49a5edbd4e401e53208da1309187c97_6.png)

And so so what I want to do in this video is essentially three things„ÄÇ I'm going to first show you if you have a pretrain model that you've previously trained„ÄÇ so check out my previous video on saving and loading models if you're unfamiliar with that„ÄÇAnd then„ÄÇI'm going to show you how to use a Pret Kas modelÔºå so KRS has a bunch of pretrain models that you can just easily import„ÄÇ

And then lastlyÔºå I'm going to show you„ÄÇHow to load pretrain models from Tensorflowlow hub„ÄÇ So Tensorflowlow hub has a bunch of pretrain models„ÄÇ And I'm going toÔºå as I said„ÄÇ I'm going to go into that a little bit later„ÄÇ So first of all„ÄÇ for this pretrain for our own pretrain modelÔºå I'm just going copy in sort of the data dataset loading„ÄÇ

 So the Ms data set right here„ÄÇAnd so we've done this multiple videos„ÄÇ And then I'm just gonna load a pretrain model„ÄÇ So model is care thatmod do load model„ÄÇ and this could beÔºå I meanÔºå if you've trained itÔºå So it's in the pretrained folder„ÄÇ So So this could be sort of a model that you've trained or I guess it could be a model that you've found on Github or something like that„ÄÇ

 So you would just load that model and then you could do print model dot summary„ÄÇAnd then„ÄÇ and then what you would do is you would check which of the parts you wantÔºå rightÔºå if you„ÄÇ if this isÔºå if you want entire modelÔºå then that's just loading the model you can continue training„ÄÇ But normally when we're doing transfer learningÔºå we're gonna pick out a couple of layers„ÄÇ

 So let's say we want„ÄÇSo let's say we want everything except the last one right let's say we have we have let's say that this number of classes would be 1000 for imagenet or something like that„ÄÇ But for Ms we would have 10 classes so then we would have to replace this last dense layer with our own but we could use sort of the previous layers from that particular model So how we would do that is we would do sort of base inputs we would do model dot layers we would take the layer that we want for it to start in and that's the first one so we're going to do model layers 0 and then dot input then we're gonna to do base outputs and why I'm calling it base is because we're going use this pre chain model as our base model and then we're gonna sort of make a layer on top of that one So what we do here is that we sort of check which either by just checking sort zero1 to„ÄÇ

3Ôºå4Ôºå5Ôºå6Ôºå7„ÄÇ So you want the output from the„ÄÇSeventh one„ÄÇ or you could count on from backwards so you could doÔºå this is minus1 and then minus2„ÄÇSo what we're going to do here is we're going to do model that layers and then minus2„ÄÇAnd then dot output„ÄÇ So we want to output from this flatten layer here„ÄÇ In other wordsÔºå we're„ÄÇ

 we're removing this layer„ÄÇThe dense last layer„ÄÇ So there are other ways to do this as well„ÄÇ You could also do it with get layer and then sort of take the name„ÄÇ But„ÄÇ we're just gonna stick with the index„ÄÇ That's fine„ÄÇ And then we're gonna do„ÄÇOutput we're going to build our own„ÄÇ so this could be a model„ÄÇ

 a sequential model or something like that„ÄÇ We're just going to add a single layer„ÄÇ So all we're going to do is layers then 10 10 output nodes and then of base outputs right so we're gonna first run it through here and then the output of base outputs is going be sent into this this final output„ÄÇ

And then so I guess we could call it that final outputs„ÄÇAnd then we're gonna do model equals ks that„ÄÇ noÔºå waitÔºå we're gonna doÔºå yeahÔºå Ks that model base input„ÄÇInputs is equal to base input„ÄÇ outputs is equal to final output„ÄÇOutputsÔºå I guessÔºå'cause it'sÔºå I have multiple„ÄÇSo then„ÄÇWe could do print„ÄÇmodel that summary„ÄÇAnd perhaps this should also be called something else then model so we could„ÄÇ

 you know we could call this new model since we've changed the the other one interesting interestingly„ÄÇ we didn't actually change anything„ÄÇ We just replaced it since the previous model that we had pretrained it had this exact layer„ÄÇ but you sort of get the point that you could replace this with whatever you want with different number of classes and so on„ÄÇSo this is just a sort of a simple example to illustrate how you would actually do it„ÄÇ

 And so if we now print the new model that summary we'll yeah so as I said„ÄÇ we'll get the exact say modelÔºå but this one here is is now a different one so if we for example„ÄÇ would change this to 15 then the sort of the final layer would now have 15 output nodes right here but of course we want 10 in this case„ÄÇAnd then you would do as normal so you would I'm gonna copy in the compile and the fit„ÄÇ

 I don't think it's very relevant„ÄÇ we've seen that in previous videos„ÄÇ so you would just do in this caseÔºå new model thatt compile and the new model thatt fit so we could now run this„ÄÇAlrightÔºå so after just a single epochÔºå we can see that it has over 97% accuracy and this is only so this sort of suggests that the pre tradinging had some effect now also„ÄÇ so we can see here that yeahÔºå after three epochsÔºå it has almost 99%„ÄÇ

And what you could also do is let's say that this pretrain model„ÄÇ you don't want to actually train the entire thingÔºå whichÔºå you know„ÄÇ would be the case if you want to do fine tuning„ÄÇ So what we would have to do is we would have to freeze the layer from this pretrain model and how you could do that very simply is do model that trainable„ÄÇTrainable equals false„ÄÇ So that's gonna freeze all of the layers and„ÄÇ

One other thing you could also do is you could iterate through the layers of the model„ÄÇ so for layer in model dot layers„ÄÇAnd then you could doÔºå yeahÔºå in this case„ÄÇ we've already set every layer to not be trainable so we could do something like assert La trainable is false„ÄÇ but what you could also do is if we hadn't done this one liner right hereÔºå for example„ÄÇ

 if you wanted to just change specific layersÔºå you could also iterate through specific layers like doing one to5 or something like that and then you could do layers do trainable is false„ÄÇYeahÔºå so just two different ways of doing the same thing here„ÄÇ Yeah„ÄÇ

 so I noticed one error and when editing the videoÔºå I wrote layers that trainable is false and„ÄÇYeah„ÄÇ I'm not sure really sure what that that does exactly„ÄÇ but we want to do a layer dot trainable is false so that was just a typo right there some for some reason it still ran and I'm not sure what difference it made but this is what we you know what we want to do and and that is already done in this one liner so it doesn't really matter„ÄÇ

 but if you would iterate your layers this is how you would do it„ÄÇThe the benefit of doing this is if we would now run this„ÄÇ rerun it and now I don't know if you if you saw it„ÄÇ but it took about 15 seconds or 16 seconds or something like that to run one epoch and so if we now run it without we'll see that it's almost half the actual time so the benefit of doing this fine tuning and freezing layers is that it's going to run much faster and sort of this is the I guess the common use case of pretrain models is that you would take this gigantic model freeze the layers up to some certain point and then just add a couple of linear layers at the end for your specific for your specific use case and„ÄÇ

YeahÔºå so actually we can see here also is that its it actually got better performance in this case„ÄÇ sort of the same„ÄÇ And that's because these these layers that have imported had already been trained on Ms previously„ÄÇ So I guess that's one scenario when you're just you have your own model or you've loaded your own model I'm gonna remove the code for this right now and we're gonna move on to the next one„ÄÇ So that's if you want to use a pretrained ks model„ÄÇ

 So so the ks library has a lot of models that you can import very easily and I'm going show you just just a use case of that„ÄÇ and it's gonna to be very similar to what we did but using the the CAs API for those models„ÄÇ so let's just create some random data just for demonstration to run the model So we're gonna do Tf random do normal and then shape we're gonna do let's say three example five examples299„ÄÇ

![](img/e49a5edbd4e401e53208da1309187c97_8.png)

![](img/e49a5edbd4e401e53208da1309187c97_9.png)

and9 by three„ÄÇ And this is just that it's gonna fit the model that we're gonna import„ÄÇ So I'll show you in just a second„ÄÇ but those are for the the X label or sort of the the features„ÄÇSo they are images of 299 and then three channels for RGB„ÄÇ and then we have TF constant and let's say 0Ôºå1Ôºå2Ôºå3Ôºå4„ÄÇ

 so five classes and they are all of a different unique class„ÄÇAnd then model is Kas that applications„ÄÇ And then here there are a bunch of different models you can use„ÄÇ And so I'm gonna pick inception V3„ÄÇ And then there are a bunch of arguments that you can send in here„ÄÇ and you can read the more of the official documentations„ÄÇ

 But one of the most important ones are that„ÄÇYou can do this include top equals true or false„ÄÇ And essentially what you can do here is that for the for the last final fully connected layers„ÄÇ you could remove those and just obtain sort of feature vectors that you can then send into your own sequential model or something like that„ÄÇ So this is probably one of the most important arguments„ÄÇ And we could just do that first of all„ÄÇ

 And then let's do modeled a summary just to„ÄÇSee what it looks like„ÄÇYeah„ÄÇ so what we can see here is that in this caseÔºå actually for the inception V3„ÄÇ there's only a fully connected at the absolute end„ÄÇ So I'm not assuming you're familiar with the inception module„ÄÇ

 but essentially it has these concatetnations of different convolutionaryary networks at this layer and then it's doing a global average pooling and then at the end„ÄÇ it's doing a fully connected„ÄÇ So if you would do include top falses„ÄÇ it would just remove this fully connected layer at the end„ÄÇBut let's say we just want to start with this one„ÄÇ So what we're gonna do now is we could do very similar to what we did previously„ÄÇ

 You could do base input is a model that layers 0 dot input„ÄÇ And then you could do base output is„ÄÇThese outputs is modeled that layers„ÄÇ And then let's sayÔºå again„ÄÇ we just want to remove the last fully connectedÔºå but of course„ÄÇ if it would be several fully connected at the end„ÄÇ

 you could do-3-4 sort of removing the exact amount that you want„ÄÇ In this case„ÄÇ we just want to remove the last one„ÄÇAnd then dot output„ÄÇ And then we're gonna do sort of our last So we're do final outputs„ÄÇ Again„ÄÇ we're just doing a single layer So let layer stands then five nodescause we have five classes„ÄÇ

And then base outputs„ÄÇ So very similar to what we did previously„ÄÇ And then new model is Kas that model inputs equals base inputs„ÄÇAndThen outputs equals final outputs„ÄÇ Then againÔºå you would just sort of get the compile„ÄÇ So I'm just gonna copy that in just to save some time„ÄÇSo the compileÔºå So we' just using atom„ÄÇ

 Spish categoricalÔºå nothing„ÄÇNothing new„ÄÇ And then we're gonna do new model dot fit„ÄÇ So let's do new model dot fit„ÄÇX and YÔºå and then epox are say 15„ÄÇ and then verbose equals 2„ÄÇ And this should be very quickÔºå rightÔºå We just have five examplesÔºå5 random data points„ÄÇSo let's see if it can overfit 5 data points using this gigantic inception V3 network„ÄÇ

Base input is not definedÔºå allright so„ÄÇüòîÔºåA base inputs right there„ÄÇAlright„ÄÇ so training on these 15 epos went pretty quickly„ÄÇ and as we can see„ÄÇ it's got 100% accuracy and the loss is very low so„ÄÇBut of course„ÄÇ this was just a demonstration of how you would import stuff using this ks that applications models and so what I want to show you now is how to use the Tensorflow hub And so Tensorflowlow hub essentially and it just Tf hubub dev It's essentially where you can get a lot of different models pretrain models for different scenarios So let's say we just want images that you can sort of so there's a lot of models here that you can just you can go through and just check in this case„ÄÇ

 let's say we want the inception v3 again so we can just go to this one right here„ÄÇ and then it has this is for the feature vector So this is similarly to the kis one where you could include top equals false that would give you a feature vector and that's exactly what this does„ÄÇ

 So they've separated the inception V3 for a model that has the top of the fully connected ones and then one that just returns the feature vector„ÄÇ![](img/e49a5edbd4e401e53208da1309187c97_11.png)

And then what you do is you just copy the URL„ÄÇAnd when you have the URL„ÄÇ![](img/e49a5edbd4e401e53208da1309187c97_13.png)

And then you can go back to the code„ÄÇ So againÔºå let's do some random data„ÄÇ So let's do TF random normal shape is 5Ôºå2Ôºå99Ôºå2Ôºå99Ôºå3„ÄÇ sort of exactly what we just did for the„ÄÇLoading from the ks„ÄÇ And againÔºå we're loading the exact same model„ÄÇ So this is not going to be anything new just showing you how to do it with Tensorflow hub„ÄÇ

 So then you would do URL equalsÔºå and then you would paste the URL right here„ÄÇAnd then„ÄÇYou would do base modelÔºå and then„ÄÇUub equals ks„ÄÇLayer„ÄÇU„ÄÇAnd thenÔºå input shape„ÄÇ299Ôºå2Ôºå99Ôºå3„ÄÇ And then what you would do is you would do model equals kas sequential„ÄÇAnd you wouldÔºå then„ÄÇDo the base modelÔºå rightÔºå which is not including the fully connected layers„ÄÇ

 And then you can add whatever layers you want„ÄÇ So layer standss1 hundred28 nodes„ÄÇActation equals„ÄÇNot re„ÄÇOur layer standsÔºå64 activation equals relu„ÄÇ And then yeah„ÄÇ let's do one final layer stands of 10 upÔºå10 up 5 output nodes„ÄÇ We just have five classes„ÄÇAnd then againÔºå you will do model that compile and model that fit„ÄÇ So I'm just gonna copy those in„ÄÇ

And then I guess sort of one thing I missed to do for the KRS model is thatÔºå of course„ÄÇ you can do as the exact first example that we did so you could do base model that trainable equals false so that you're you're doing fine tuning„ÄÇI should have probably showed that also for the other one because this is something you can do for all models„ÄÇAnd yeahÔºå just for the exampleÔºå let's try and run this and see what it looks like„ÄÇAlright„ÄÇ

 so in this scenario it didn't actually overfit too much„ÄÇ so we would have to probably run it for longerÔºå but as we can see at least it obtained a 100% accuracy„ÄÇAnd yeahÔºå that's pretty much it„ÄÇ Those are the examples I wanted to show you sort of different ways you can do pretraining and fine tuning freezing layers and so on„ÄÇ Hopefully this video was useful for you if you have any questions then leave them in the comment and hope to see you in the next video„ÄÇ



![](img/e49a5edbd4e401e53208da1309187c97_15.png)