# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P14ï¼šL2.7- åŸºäºå­—ç¬¦çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL

Before diving in character based tokenizationï¼Œ understanding why this kind of tokenization is interesting requires understanding the flaws of word based tokenizationã€‚

If you haven't seen the first video on the word based organizationã€‚

 we recommend you check it out before looking at this videoã€‚ğŸ˜Šï¼ŒOkayã€‚

 let's take a look at character based tokenizationã€‚ğŸ˜Šã€‚

We now split our text into individual characters rather than wordsã€‚ğŸ˜Šã€‚

There are generally a lot of different words and languagesï¼Œ while the number of characters stays lowã€‚

To beginï¼Œ let's take a look at the English languageã€‚It has an estimated 170ï¼Œ000 different wordsã€‚

 so we would need a very large vocabulary to encompass all wordsã€‚ğŸ˜Šã€‚

Where they character based vocabularyï¼Œ we can get by with only 256 charactersã€‚

 which includes lettersï¼Œ numbers and special charactersã€‚

Even languages with a lot of different characters like the Chinese languages can have dictionaries with up to 20ã€‚

000 different charactersï¼Œ but more than 375ï¼Œ000 different wordsã€‚

So character based vocabularies let us use fewer different tokens than the word based tokenization dictionaries we would otherwise useã€‚

ğŸ˜Šï¼ŒThese vocabularies are also more complete than their word based vocabulary counterpartsã€‚

As our vocabulary contains all characters used in a languageã€‚

 even the words unseen during the tokenizer training can still be tokenizedã€‚

 so out vocabulary tokens will be less frequentã€‚ğŸ˜Šï¼ŒThis includes the ability to correctly tokenize misspelled words rather than discarding them as unknown straight awayã€‚

ğŸ˜Šï¼ŒHoweverï¼Œ this algorithm isn't perfect eitherã€‚ğŸ˜Šï¼ŒIntuitivelyã€‚

 characters do not hold as much information individually as a word will holdã€‚ğŸ˜Šï¼ŒFor exampleã€‚

 let's hold more information than its first data Lã€‚Of courseï¼Œ this is not trueful for all languagesã€‚

 as some languages likeideogram based languagesï¼Œ have a lot of information held in a single characterã€‚

ğŸ˜Šï¼ŒBut for others like Roman based languagesï¼Œ the model will have to make sense of multiple tokens at a time to get the information otherwise held in a single wordã€‚

This leads to another issue with character based tokenizersã€‚

 Their sequences are translated into very large amounts of tokens to be processed by the modelã€‚

And this can have an impact on the size of the context the model will carry aroundã€‚

 and it will reduce the size of the text we can use as input for a modelï¼Œ which is often limitedã€‚ğŸ˜Šã€‚

This organizationï¼Œ while it has some issuesï¼Œ has seen some very good results in the pastã€‚

 and so it should be considered when approaching a new problem as it solves issues encountered in the word based algorithmã€‚



![](img/6a739a8b766233902cc0ce2bd62c0279_1.png)

å—¯ã€‚

![](img/6a739a8b766233902cc0ce2bd62c0279_3.png)