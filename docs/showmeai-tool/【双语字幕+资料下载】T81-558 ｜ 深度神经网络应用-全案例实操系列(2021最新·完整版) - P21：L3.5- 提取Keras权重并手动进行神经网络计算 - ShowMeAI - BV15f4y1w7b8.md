# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P21ï¼šL3.5- æå–Kerasæƒé‡å¹¶æ‰‹åŠ¨è¿›è¡Œç¥ç»ç½‘ç»œè®¡ç®— - ShowMeAI - BV15f4y1w7b8

Hiï¼Œ this is Jeffineã€‚ Welcome to applications of deep neural networks with Washington Universityã€‚

 In this videoï¼Œ we're going to see how to actually extract the weights from a neural network created by Kes and put those weights into an equation so that we can actually calculate the output from the neural network and see that this there's really no magic to this processã€‚

ğŸ˜Šï¼ŒThat it is simply weights multiplied against the inputs that produces a final output for the latest on my AI course and projectsã€‚

 click subscribe in the bell next to it to be notified of every new videoã€‚

 This is code from Ks and Tensorflowlowï¼Œ But this applies to really just about anythingã€‚

 I am going to show you how to actually extract the weights from the neural networkã€‚

 so that we can then put them onto a diagram and actually actually calculate it and come up with the same number that Tensorflowlow would haveã€‚

ğŸ˜Šã€‚

![](img/99c863804a8e2678a48ffb4d5482d3aa_1.png)

For thisï¼Œ we're going to use an exclusive or neural networkï¼Œ that's the XOR functionã€‚

These are the inputs to itï¼Œ0ï¼Œ0ï¼Œ the usual truth table for any sort of an and or an orã€‚

 This is the expected outputã€‚ The thing to remember with X O R is if the two inputs are the sameã€‚

 it's going to be 0ã€‚If the two outputs are differentï¼Œ zero and 1 versus 10ï¼Œ it's going to be oneã€‚

Here we set up a neural networkã€‚ It has two inputs and a hidden layer of twoã€‚

 We're going for the absolute smallest neural network just to show that you don't really need that much to calculate an excluvoã€‚

Alsoï¼Œ since we are going to calculate this by hand or just being lazyã€‚

 we don't I don't want to give you a truly deepï¼Œ deep neural network and then calculate it by handã€‚

 that would not be fineã€‚So we're going to optimize it with mean square errorã€‚

 and the final output is going to be one neural networkï¼Œ it's a regression neural networkã€‚

 you can do XOR as classification or regression in this caseï¼Œ I'm doing it as regressionã€‚

I basically train it hereã€‚ I am training it for100ï¼Œ000 epochsã€‚

 It may take longer because there's so few weights in this that your initial random values or your weights are really going to have a lot of determination on the success of your trainingã€‚

 We could do this in way fewer than this many epochsã€‚

 This is basically grilling a cheese sandwich with a gen engineï¼Œ butã€‚

I am really not trying to show you how to tune these things right nowã€‚

 I just want to get the weights set up so that they're good for a exclusive or networkã€‚

 we're going to put those on a diagram and show you that basically you can then calculate the same output so you can see the weights behind what Kis gives youã€‚

I train itï¼Œ I predict itï¼Œ this is good for the neural network becauseã€‚

Each of these four correspond to these four up here in the inputã€‚ This is scientific notationã€‚

 So to the negative 4ï¼Œ that's 0ã€‚000ã€‚ That numberã€‚ So these two are effectively0ã€‚

 and these two in the middle are effectively oneã€‚ So this may look weird if you're not used to seeing numbers like thisã€‚

 but this is a great outputã€‚ the neural network has trained quite wellã€‚

So then what we're able to do is I writeï¼Œ I wrote this little program hereï¼Œ this dumps the weightsã€‚

 so you're seeing that the layer0 counting starting at zeroã€‚

Weight from the bias to the next layer layer 1ã€‚ againï¼Œ we're counting with 0ã€‚ neuron 1ã€‚

 neuron neuron 0ï¼Œ neuron 1ã€‚ These are the biasesã€‚And these are the actual weightsã€‚

So there's not a lot of weights and biases in this neural networkï¼Œ it's a fairly small oneã€‚

So I'm going to show draw a diagram here in just a minuteã€‚

 and we're going to copy all of these values up onto the diagramã€‚

 Then we're going to calculate it by handã€‚ I give you the code down here to calculate it by hand as wellã€‚

 if you want to do itï¼Œ you set the input to 1 and 0ï¼Œ So1 in 0 and an X O Rã€‚ Those are differentã€‚

 So it should be 1ã€‚ So it outputs a 0ã€‚96ã€‚ Nowï¼Œ since we're calculating this by handã€‚

 I truncated quite a few of these numbersã€‚ So 1ã€‚29 becomes 1ã€‚3ï¼Œ and we'll do that when we diagram itã€‚

 So we lose a little bit of accuracyï¼Œ butã€‚These are the two input neuronsã€‚

We are going to feed this values 0 and  oneã€‚ So that's what we're calculating forã€‚

 You could calculate itï¼Œ reallyï¼Œ forã€‚Just about any other value that you wanted toã€‚

 But for the other four that you would have in the exclusive  fourã€‚

 we're also going to have the bias neuronã€‚ So like we saw in earlier class videosã€‚

 you're essentially calculatingã€‚A weighted sum over and over and over again for a neural networkã€‚

 So we're going to calculate the value for hidden neuron0ã€‚H0ã€‚

Each zero has inputs from those previous threeã€‚These all have weightsã€‚

 as we saw from the output from the program beforeï¼Œ so I'm just going to copy those inã€‚

So this is going to become essentially a weighted sum that we're going to use to calculate what hidden 0 is actually what its value actually isã€‚

 So to do thatï¼Œ we're going to multiply the input 0 times its weightï¼Œ which is 1ã€‚3ã€‚æˆ‘æ˜¯ã€‚1 times 1ã€‚3ã€‚

That one is from the firstï¼Œ which is actually the second input neuronã€‚

 And then we have to add the biasã€‚ and we basically perform thatã€‚Calculationï¼Œ which ends up being 0ã€‚

 So the value for hidden in hidden neuron 0 is 0ã€‚ Nextï¼Œ we're going to calculate the value forã€‚

Hid neuroron 1ï¼Œ which is the second oneã€‚ It has a similar sort of thing going onã€‚

 It has three weights coming into itã€‚Is going to copy those values from the output that we had from the Kiras programã€‚

So that is simplyã€‚1ã€‚2ï¼Œ we are calculating basically the weighted sum againã€‚

The first input I0 drops outï¼Œ second one contributes that one pointã€‚1ã€‚2 to itã€‚

 and then the intercept or the bias forã€‚For thisï¼Œ for the second hidden neuron is zeroã€‚

This whole thingï¼Œ togetherã€‚Is basically equal to 1ã€‚2ã€‚ Then this valueã€‚

 we have to pass it through the through the activation functionã€‚

 Both of these needed to go through the activation functionã€‚ That is the rectified linear unitã€‚

 So that is basically going to be the maxã€‚Of 0 and 1ã€‚2ã€‚Which is also 1ã€‚2ã€‚

So the output from this hidden neuron here is 1ã€‚2ã€‚Same sort of deal up here to apply its activation functionã€‚

 It's going to be maxed because it's also the valueã€‚ the max 0ï¼Œ0 is 0ã€‚

activation functions that you can useï¼Œ I may give you a different one for for the midterm examination on this oneã€‚

The only other ones that you will ever seeï¼Œ at least in this classï¼Œ are sigmoidã€‚Pollic tangentã€‚

There are othersï¼Œ but these are for regressionã€‚If you're doing a classificationã€‚

 you might also see the softm butã€‚Focus primarily on theseã€‚Now the outputã€‚

 the final layer so that we actually get what this neural network is providing for usã€‚

This is going to have similar to beforeã€‚ We do have another bias connectionã€‚

 So you're going to have essentially three and bound connections againã€‚

And we're essentially going to just copy the weights that we had from the output in Carasã€‚There is 0ã€‚

And this results in the final equation that ties this whole thing togetherã€‚

And that is going to basically beã€‚Is the output from from hidden hidden 0ã€‚Times 1ã€‚6ã€‚Plusï¼Œ 1ã€‚2ã€‚

Times 0ã€‚8ã€‚You can see the 1ã€‚2 times the 0ã€‚8 and then plus the plus theã€‚The bias or the interceptã€‚

 which is zeroã€‚And then this whole thing equals 0ã€‚96ã€‚

That's your final output that is approximately equal to 1ã€‚0ã€‚

 which is what you would have expected from a exclusive orã€‚Ofã€‚Zero and whatã€‚There you have itã€‚

 That is the calculation for this neural network done using the weights that Kiro is actually trained for usã€‚

 So we can see that there is really no magic thereã€‚ğŸ˜Šï¼ŒThis simple of a neural networkã€‚

 you could have actually just hand coded these weights that would have not been difficultã€‚

 I'll probably do a video on that as well so that you can see how to actually hand code one of theseã€‚

 You do need about this size of neural networkã€‚ You definitely need the hidden layerã€‚



![](img/99c863804a8e2678a48ffb4d5482d3aa_3.png)

Thank you for watching this videoã€‚ In the next videoã€‚

 we're going to see more advanced training techniques that we can use with a neural network toã€‚ğŸ˜Šã€‚

To get better results and to measure or error in a variety of different waysã€‚

 this content changes oftenã€‚ so subscribe to the channel to stay up to date on this course and other topics in artificial intelligenceã€‚

