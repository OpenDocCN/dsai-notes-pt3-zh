- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P33ï¼šL6.1- Sylvainçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P33ï¼šL6.1- Sylvainçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
- en: But yeah I think it's time to begin so welcome to thefi course and we're going
    to let you the chapter 1 together and this is really the place to ask any questions
    you might have so please do so in the chat I'm going to make some booth every
    five to 10 minutes and go through the chats read the question and answer them
    to the best of my knowledgeã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡æˆ‘è§‰å¾—æ˜¯æ—¶å€™å¼€å§‹äº†ï¼Œæ¬¢è¿æ¥åˆ°è¿™ä¸ªè¯¾ç¨‹ï¼Œæˆ‘ä»¬å°†ä¸€èµ·å­¦ä¹ ç¬¬ä¸€ç« ï¼Œè¿™é‡Œæ˜¯æé—®çš„æœ€ä½³åœºæ‰€ï¼Œè¯·åœ¨èŠå¤©ä¸­æé—®ï¼Œæˆ‘ä¼šæ¯äº”åˆ°ååˆ†é’ŸæŸ¥çœ‹ä¸€æ¬¡èŠå¤©ï¼Œé˜…è¯»é—®é¢˜å¹¶å°½é‡å›ç­”ã€‚
- en: So the urging for his courseã€‚And this session will go over chapter1 as I saidã€‚
    and this chapter is meant as a very general introduction to what transformer models
    can do so you don't need to worry about setting up be just yet a few good samples
    will to see how you can run them in coab directlyã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¯¾ç¨‹çš„ç›®çš„å¦‚æˆ‘æ‰€è¯´ï¼Œè¿™ä¸€èŠ‚å°†æ¶µç›–ç¬¬ä¸€ç« ï¼Œè¿™ä¸€ç« æ—¨åœ¨éå¸¸ä¸€èˆ¬æ€§åœ°ä»‹ç»å˜æ¢æ¨¡å‹çš„åŠŸèƒ½ï¼Œæ‰€ä»¥ä½ ä¸éœ€è¦æ‹…å¿ƒç›®å‰çš„è®¾ç½®ï¼Œå‡ ä¸ªå¥½çš„æ ·æœ¬å°†å±•ç¤ºå¦‚ä½•åœ¨coabä¸­ç›´æ¥è¿è¡Œå®ƒä»¬ã€‚
- en: Or the Greenface websiteã€‚I'm not going to watch review the introduction video
    because you can do that on your own time and there's not really anything informative
    in itã€‚ and so the goal of the first section of the courseï¼Œ which is probably right
    nowã€‚ which is this section in pink is toã€‚Introduce you to transform models the
    web Strer went today and then teach you how to download the pretrain model from
    the Ebã€‚ find unit on your own data for text classification tasksã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ˜¯Greenfaceç½‘ç«™ã€‚æˆ‘ä¸ä¼šå†çœ‹ä»‹ç»è§†é¢‘ï¼Œå› ä¸ºä½ å¯ä»¥åœ¨è‡ªå·±çš„æ—¶é—´é‡Œè§‚çœ‹ï¼Œè§†é¢‘å†…å®¹æ²¡æœ‰ä»€ä¹ˆä¿¡æ¯ã€‚ç¬¬ä¸€éƒ¨åˆ†çš„ç›®æ ‡ï¼Œå¯èƒ½å°±æ˜¯ç°åœ¨è¿™ä¸ªç²‰è‰²éƒ¨åˆ†ï¼Œæ˜¯ä¸ºäº†å‘ä½ ä»‹ç»å˜æ¢æ¨¡å‹ï¼Œä»Šå¤©çš„ç½‘ç»œç»“æ„ï¼Œå¹¶æ•™ä½ å¦‚ä½•ä»Ebä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‰¾å‡ºè‡ªå·±çš„æ•°æ®ä»¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚
- en: And thenluute the result back to the model hebã€‚And then see section 2 will dive
    a little bit deeper and consider all NLP tag can adjust text classification and
    when the last part of the course is going to dive even a little bit deeperã€‚ so
    part two should be released in the fall and part three should be released at the
    beginning of next yearã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†ç»“æœä¼ å›æ¨¡å‹ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥æ¢è®¨æ‰€æœ‰NLPä»»åŠ¡å¦‚ä½•è°ƒæ•´æ–‡æœ¬åˆ†ç±»ï¼Œè€Œè¯¾ç¨‹çš„æœ€åä¸€éƒ¨åˆ†å°†è¿›ä¸€æ­¥æ·±å…¥ã€‚å› æ­¤ç¬¬äºŒéƒ¨åˆ†å°†åœ¨ç§‹å­£å‘å¸ƒï¼Œç¬¬ä¸‰éƒ¨åˆ†å°†åœ¨æ˜å¹´åˆå‘å¸ƒã€‚
- en: And so once you've finished this part of the courseã€‚ you should be able to download
    the between model Once you it on your own problemã€‚ and thenã€‚Create either a smaller
    uploadlet the result back to the hubã€‚ And so you shouldã€‚Look at the forumã€‚Which
    I'm gonnaã€‚Show you dressed here at discuss the phaseã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ å®Œæˆäº†è¿™éƒ¨åˆ†è¯¾ç¨‹ï¼Œä½ åº”è¯¥èƒ½å¤Ÿä¸‹è½½è¯¥æ¨¡å‹ï¼Œå¹¶åœ¨è‡ªå·±çš„é—®é¢˜ä¸Šè¿›è¡Œä½¿ç”¨ã€‚ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªè¾ƒå°çš„ä¸Šä¼ ï¼Œç»“æœå°†ä¼ å›ä¸­å¿ƒã€‚å› æ­¤ä½ åº”è¯¥æŸ¥çœ‹è®ºå›ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•è®¨è®ºè¿™ä¸ªé˜¶æ®µã€‚
- en: go which is where you will be able to ask any question you have after this live
    stream in the course category there is one to big per chapter for all your questions
    and then there is also one to pick I'm going to follow very closely which isã€‚So
    chair your projects topic on which you should definitely share anything you built
    after following the first section of the courseã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä½ å°†åœ¨è¿™ä¸ªç›´æ’­ç»“æŸåï¼Œå¯ä»¥åœ¨è¯¾ç¨‹ç±»åˆ«ä¸­æå‡ºä»»ä½•é—®é¢˜ï¼Œæ¯ç« éƒ½æœ‰ä¸€ä¸ªè®¨è®ºåŒºä¾›ä½ æé—®ï¼Œå¹¶ä¸”æˆ‘ä¼šå¯†åˆ‡å…³æ³¨å¦ä¸€ä¸ªè®¨è®ºåŒºï¼Œæ‰€ä»¥è¯·åˆ†äº«ä½ çš„é¡¹ç›®ä¸»é¢˜ï¼Œç¡®ä¿åˆ†äº«åœ¨å­¦ä¹ è¯¾ç¨‹ç¬¬ä¸€éƒ¨åˆ†åæ‰€åšçš„ä»»ä½•æ„å»ºã€‚
- en: å—¯ã€‚Alrightyã€‚So let's dive in case I don't see any general questions for now withã€‚The
    beginning of chapter1ã€‚Soã€‚Transformer models are all about doing NLP task andLP
    stands for natural language processingã€‚ and it's a field that's related to everything
    languageã€‚So the the goal of NLP task is either to classify some textã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œå¥½çš„ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨å§ã€‚å¦‚æœç°åœ¨æ²¡æœ‰ä»»ä½•ä¸€èˆ¬æ€§é—®é¢˜ï¼Œæˆ‘ä»¬å°±å¼€å§‹ç¬¬ä¸€ç« ã€‚å˜æ¢æ¨¡å‹ä¸»è¦ç”¨äºå¤„ç†NLPä»»åŠ¡ï¼Œè€ŒNLPä»£è¡¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸è¯­è¨€ç›¸å…³çš„é¢†åŸŸã€‚å› æ­¤ï¼ŒNLPä»»åŠ¡çš„ç›®æ ‡æ˜¯å¯¹ä¸€äº›æ–‡æœ¬è¿›è¡Œåˆ†ç±»ã€‚
- en: so for instance getting the sentiment of a reviewï¼Œ detecting if an email is
    spamã€‚ detecting if a comment someone posted online is rather nice or rather not
    nice if a sentence is grammatically correctã€‚ etcteraï¼Œ etcterã€‚Our task could be
    classifying edge word in the textï¼Œ so for instanceã€‚ being able to pass the gramatic
    component to be able to see if that word is a person or a location or an organizationã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè·å–è¯„è®ºçš„æƒ…æ„Ÿï¼Œæ£€æµ‹é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶ï¼Œåˆ¤æ–­æŸä¸ªåœ¨çº¿è¯„è®ºæ˜¯å‹å¥½è¿˜æ˜¯ä¸å‹å¥½ï¼Œåˆ¤æ–­å¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®ï¼Œç­‰ç­‰ã€‚æˆ‘ä»¬çš„ä»»åŠ¡å¯ä»¥æ˜¯å¯¹æ–‡æœ¬ä¸­çš„è¯æ±‡è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚ï¼Œèƒ½å¤Ÿè§£æè¯­æ³•æˆåˆ†ï¼Œä»¥åˆ¤æ–­è¿™ä¸ªè¯æ˜¯äººåã€åœ°ç‚¹è¿˜æ˜¯ç»„ç»‡ã€‚
- en: Another type of NLP task is generated text contentã€‚ so completing a prompt which
    is what Jo smartF does when you're trying to compose a messageã€‚ usually it suggests
    using X and even Gmail does that now on emailã€‚å—¯ã€‚Fillling the B syn textã€‚ which
    is another kind of text generationã€‚Another kind of task is to extract an answer
    from a textã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: so given a very long text and then a question being able a modelã€‚ being able
    to extract the answer to that modelã€‚The answer sort of that questionã€‚ sorry from
    the context or generating these new sentences from an input textã€‚ So this is for
    instanceï¼Œ generating translation of a text summarizing a text I saw not over recently
    on making a new text in and of styleã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: for instanceï¼Œ a casual style or moreã€‚å—¯ã€‚Foral styleã€‚So those are all the kind
    of enLP testque tackle in the courseï¼Œ especially in section2ã€‚And so it's rather
    challenging because a computer doesn't process information the same way as we
    doã€‚ And so a new transformer model let you deep learning toã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Are able to from like a few from a few samples that you have label for are able
    to generalize properly without you having to to createã€‚Rs for instanceï¼Œ so before
    deep learning was all the rageã€‚ one thing would be to pass the text and have some
    special rules if I see this world maybe it means its's positive if I see this
    world maybe it means it's negative etctera rightã€‚ so that's not what's done now
    this is mostly using transformable models with a again face libraryã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: And those modelsï¼Œ you shouldï¼Œ if you have not yet you should follow an introduction
    to a deep learning courseã€‚ those model are usually trained and they don't follow
    a set of given role that has been written by your humanã€‚ they have weightsï¼Œ which
    follow an algorithm called gradient descent and following the training data that
    are fed they make valueless function or ametric a little bit better at each step
    and so we get at the end a model that is kind of a black boxã€‚But we can use and
    generalize fairly well on dataï¼Œ but looks like the data on the training setã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: So let's look at what this model can do in practiceã€‚ so there are two ways we'll
    be able to do that the first way is by clicking on this open in color buttonã€‚So
    Collab is a platform maintained by Google which provides free access to resources
    like GPUs or TUs in a Jupy down the book environmentã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Soï¼Œ you can executeã€‚Good directlyã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_3.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Inside itã€‚And so the first cell needs to be executed to install all the library
    I'm going to let this run where we go through a little bit of the course and then
    we'll be able to run the other good samples I'm going to come back to that window
    a little bit later So transformers models are everywhere they are used by a lot
    of companies now those are examples of company that are using the Gface model
    app to have to share as models we have pretrained and probably to use them internally
    as wellã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: And the Transformless library is the main interface library that provide access
    to those modelã€‚And the bottle upã€‚Which we're going to have a quick lookã€‚And a
    little bit laterã€‚ the model app is where all post Sptrain model as taught is part
    of the evening phase ecosystem and we will be able to run the same code as wellll
    try in the Coab notebook on the that model app a little bit laterã€‚The thing you
    will need to do to access the forum and to be able to play around with all the
    models is create give a second following the link here on the 6 of the chapter
    1ã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers åº“æ˜¯æä¾›è®¿é—®è¿™äº›æ¨¡å‹çš„ä¸»è¦æ¥å£åº“ã€‚æˆ‘ä»¬å°†å¿«é€Ÿæµè§ˆä¸€ä¸‹ã€‚ç¨åæ¨¡å‹åº”ç”¨æ˜¯æ‰€æœ‰åè®­ç»ƒæ¨¡å‹çš„æ‰€åœ¨ï¼Œå®ƒæ˜¯æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿåœ¨ç¨åçš„æ¨¡å‹åº”ç”¨ä¸­è¿è¡Œä¸åœ¨
    Coab ç¬”è®°æœ¬ä¸Šå°è¯•çš„ç›¸åŒä»£ç ã€‚æ‚¨éœ€è¦åšçš„äº‹æƒ…æ˜¯è®¿é—®è®ºå›å¹¶èƒ½å¤Ÿç©å¼„æ‰€æœ‰æ¨¡å‹ï¼Œç‚¹å‡»ç« èŠ‚ 1 ä¸­çš„é“¾æ¥ã€‚
- en: And so we'll have a look first at the higher higher level API objectã€‚ which
    is called the pipeline plan functionã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_5.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬é¦–å…ˆæ¥çœ‹æ›´é«˜å±‚æ¬¡çš„ API å¯¹è±¡ï¼Œç§°ä¸ºç®¡é“è®¡åˆ’å‡½æ•°ã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_5.png)
- en: I'm going just give a littleelps few minutesã€‚Just going to give a few minutes
    for the video to bothã€‚For the video to load itself a little bit and take the time
    to answer any questions that you have let miss horse through the chatã€‚I soï¼Œ there
    were a fewã€‚So the first question I see is there still a place for LSTM models
    approaches like OMFã€‚ if so where do they outperform transformer models or at least
    should be considered that's a very very good questionsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šç»™å¤§å®¶å‡ åˆ†é’Ÿæ—¶é—´ã€‚åªæ˜¯æƒ³ç»™è§†é¢‘ä¸€äº›åŠ è½½æ—¶é—´ï¼Œå¹¶ä¸”å›ç­”ä½ ä»¬åœ¨èŠå¤©ä¸­æå‡ºçš„ä»»ä½•é—®é¢˜ã€‚æ‰€ä»¥ï¼Œæˆ‘çœ‹åˆ°çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼ŒLSTM æ¨¡å‹ï¼ˆå¦‚ OMFï¼‰æ˜¯å¦ä»ç„¶æœ‰ç”¨ï¼Ÿå¦‚æœæœ‰ï¼Œå®ƒä»¬åœ¨å“ªäº›æ–¹é¢ä¼˜äºå˜å‹å™¨æ¨¡å‹ï¼Œæˆ–è€…è‡³å°‘åº”è¯¥è¢«è€ƒè™‘ï¼Ÿè¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ã€‚
- en: very good question right and there is definitely still a place for LSTM modelã€‚The
    main reason transformer models are heavily used right now is that the computation
    is more efficient especially on hardwarePUs and TUsã€‚ because LSTMs rely on a recurrent
    mechanism and that recurt mechanism is a little bit harder to optimize but for
    instance when in fit as a very good state of the art results on the EMDB data
    setã€‚ which is classifying movie reviews and it's been only very recently that
    it was performedform that transformers and that task in which I think is linked
    to the fact that the EMDB reviews very long and transformer models are good with
    input that go up to the sequence like we can manageã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸å¥½çš„é—®é¢˜ï¼Œç¡®å®ä»ç„¶æœ‰ LSTM æ¨¡å‹çš„åº”ç”¨ã€‚å½“å‰å˜å‹å™¨æ¨¡å‹è¢«å¹¿æ³›ä½¿ç”¨çš„ä¸»è¦åŸå› æ˜¯è®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¡¬ä»¶ TPU å’Œ GPU ä¸Šï¼Œå› ä¸º LSTM
    ä¾èµ–äºé€’å½’æœºåˆ¶ï¼Œè€Œè¿™ç§æœºåˆ¶çš„ä¼˜åŒ–ç›¸å¯¹è¾ƒéš¾ã€‚ä¸è¿‡ï¼Œä¾‹å¦‚ï¼ŒLSTM åœ¨ EMDB æ•°æ®é›†ä¸Šå–å¾—äº†éå¸¸å¥½çš„å…ˆè¿›ç»“æœï¼Œè¯¥æ•°æ®é›†ç”¨äºç”µå½±è¯„è®ºåˆ†ç±»ã€‚æœ€è¿‘æ‰å‘ç°å˜å‹å™¨æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸
    EMDB è¯„è®ºéå¸¸é•¿æœ‰å…³ï¼Œè€Œå˜å‹å™¨æ¨¡å‹åœ¨å¤„ç†è¾ƒé•¿åºåˆ—æ—¶è¡¨ç°è‰¯å¥½ã€‚
- en: usually it's 512 but for 12 to sorryï¼Œ but yeah if you have things that are longer
    than that it looks like LSTM models can seeã€‚It'll be a very good fitã€‚Another questionï¼Œ
    I'm going to answer it just after this video because it's about the pipelineã€‚So
    yeahï¼Œ let's watch the videoï¼Œ I'm going to disappear from the screen so you can
    watch with me being in the way and I'll come back after the videoã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æ¥è¯´ï¼Œå®ƒæ˜¯ 512ï¼Œä½†å¯¹äº 12 æ¥è¯´ï¼Œå¦‚æœæœ‰è¶…è¿‡è¿™ä¸ªé•¿åº¦çš„å†…å®¹ï¼Œä¼¼ä¹ LSTM æ¨¡å‹å¯ä»¥çœ‹åˆ°ã€‚è¿™å°†æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é€‰æ‹©ã€‚å¦ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä¼šåœ¨è¿™ä¸ªè§†é¢‘ä¹‹åå›ç­”ï¼Œå› ä¸ºå®ƒæ˜¯å…³äºç®¡é“çš„ã€‚æ‰€ä»¥ï¼Œå—¯ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹è§†é¢‘ï¼Œæˆ‘ä¼šä»å±å¹•ä¸Šæ¶ˆå¤±ï¼Œè¿™æ ·ä½ ä»¬å¯ä»¥ä¸å—å¹²æ‰°åœ°è§‚çœ‹ï¼Œè§†é¢‘ç»“æŸåæˆ‘ä¼šå›æ¥ã€‚
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_7.png)'
- en: ã€‚It's pipe end functionã€‚The pipeline function is the most I level API of the
    Transformers libraryã€‚It groupgroup together all the steps to go from moretex to
    usable predictionsã€‚The model used is at the core a pipelineï¼Œ but the pipeline
    also include all the necessary prepoing since the model does not expect texts
    but numberã€‚As well as some post processing to make the output of the model human
    readableã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç®¡é“ç»“æŸå‡½æ•°ã€‚ç®¡é“å‡½æ•°æ˜¯ Transformers åº“ä¸­æœ€é«˜å±‚æ¬¡çš„ APIã€‚å®ƒå°†æ‰€æœ‰æ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ï¼Œä»æ–‡æœ¬åˆ°å¯ç”¨çš„é¢„æµ‹ã€‚ä½¿ç”¨çš„æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç®¡é“ï¼Œä½†ç®¡é“è¿˜åŒ…å«æ‰€æœ‰å¿…è¦çš„é¢„å¤„ç†ï¼Œå› ä¸ºæ¨¡å‹å¹¶ä¸æœŸæœ›æ–‡æœ¬ï¼Œè€Œæ˜¯æ•°å­—ã€‚åŒæ—¶è¿˜åŒ…æ‹¬ä¸€äº›åå¤„ç†ï¼Œä½¿æ¨¡å‹çš„è¾“å‡ºæ˜“äºäººç±»ç†è§£ã€‚
- en: Let's look at the first phase on Paul with the sentiment analysis pipelineã€‚This
    pipeline performs text classification on a given input and determines if it's
    positive or negativeã€‚Here it attributed the positive label on the given textï¼Œ
    with a confidence of 95%ã€‚You can pass multiple texts to the same pipelineï¼Œ which
    will be processed and passed through the model together as a batchã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The output is a list of individual results in the same order as the input textã€‚Here
    we find the same label on score for the first textã€‚ and the second text is church
    negative with a confidence of 99ã€‚9%ã€‚The zero shot classification pipeline is a
    more general text classification pipelineã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: It allows you to provide the labels you wantã€‚Here we want to classify our input
    text along the labels educationã€‚ politicsï¼Œ and businessã€‚The pipeline successfully
    recognizes it's more about education than the yoga labelsã€‚With a confidence of
    84%ã€‚Moving on to our tasksï¼Œ the text generation pipeline will to complete a given
    promptã€‚The output is generated with a bit of randomnessï¼Œ so it changes each time
    you call the generator object on a given promptã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Up until nowï¼Œ we've used the Biman APIpi with a default model associated to
    each taskã€‚But you can use it with any model that has been freetrained or fine
    tune on this taskã€‚Ohã€‚Going on the model Hï¼Œ again F dogo slash modelsã€‚You can filter
    the available models by taskã€‚The defaultform adults used in our previous example
    was GPT2ã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: but were many more models available and not just in Englishã€‚Let's go back to
    the next generation pipeline and load it with another modelï¼Œ distill JPT2ã€‚This
    is a lighter version of GT2 created by the Eingface teamã€‚When applying the pipeline
    to a given promptï¼Œ we can specify several argumentsã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Such as the maximum length of the generated textï¼Œ all the number of sentences
    we want to return since there is some randomness in the generationã€‚Generating
    text by guess the X12 in is intenseï¼Œ why the pertraining objective of GPT2ã€‚The
    field mass pipeline is a pering objective of Broï¼Œ which is to guess the value
    of mass quaã€‚In this caseï¼Œ we ask the two most likely values for the missing words
    according to the modelã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: and get mathematical or computational as possible answersã€‚And a task transform
    most model than form is to classify each word in a sentence instead of the sentence
    as a wallã€‚One example of this is named entity recognitionã€‚Which is the task of
    identifying entities such as personsã€‚ organizations or locations in a sentenceã€‚Hereï¼Œ
    the model correctly finds the personï¼Œ svaã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: the organizationï¼Œ Gfaceï¼Œ as well as the location Brooklyn inside the input textã€‚The
    group entities equal2 argument use is to make the pipeline group together the
    different walls linked to the same entityã€‚ such as eggging and face hereã€‚novel
    task available with the byg API is extractive question and sorryã€‚Providing a context
    and a questionï¼Œ the model will identify the span of text in the context containing
    the answer to the questionã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Getting short summaries of very longats is also something the Transers library
    can help with with the summarization bikeã€‚Finallyï¼Œ the last task supported by
    the B API is translationã€‚Here we use a French English model found on the Mo hub
    to get the English version of our quick textã€‚Here is a brief summary of all the
    tasks we've looked into in this video try them out through the influence switchts
    in the modern hubã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_10.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Yeahã€‚Sorry about the song at the beginning so I'm trying to get better wrong
    A I can't have my head on at the same time I'm playing the video so that part
    seem to have been dealt with and for the fact that it's leggy and be angle a lot
    trying to find a solution but I don't know what it could be it so it could be
    a little bit better the next video I tried minimizing OS and footage could be
    betterã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_12.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: So the question I skipped before the video was when you do pipeline sentiment
    analysisã€‚ how can you determine which model it is using the dog string doesn't
    help muchã€‚ Pat true its not in the documentation and we should probably make an
    effort to have it better document it the best way to do that is to inspect a source
    code for nowã€‚Which I'm going to show you in one secondã€‚Soï¼Œ you need to goã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Into pipelines in it and that's a great question we actually make it more accessible
    and easier to seeã€‚ but you need to go in the source code in the pipelines module
    and in the in file and where you will see for each pipeline the default model
    that is used so here we are using the text classification and sentimentalizes
    have the same default as the model that was used is this still well based in case
    find you in SS2 Englishã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Don't know why the person didn't pick a short nameï¼Œ butã€‚That's the name of itã€‚Let
    me see if there are any other questionsã€‚Yesï¼Œ I'll send the links of the next video
    on the chatã€‚ or you can follow on the on the course chapter at the same time and
    look at them directly in and of our brotherã€‚Okayï¼Œ it looks like there are number
    of our questionsï¼Œ so let's continueã€‚So this chapterã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: So the rest of this section is just a good samples but shown in the videoã€‚ So
    I'm not going to go through that Insteadï¼Œ I'm going to look at my call upï¼Œ which
    hopefullyã€‚Should asã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_14.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: I need to restart own timeï¼Œ all lovelyã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_16.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: So hopefully it's not going to take a long time to reinstall everythingã€‚å¥½å¥½å¥½ã€‚Why
    are you being with meã€‚Okayã€‚So for could be a little bit smoother for youã€‚So once
    we have installed the Transformers libraryï¼Œ we can run the first cellï¼Œ for instanceã€‚
    which add the same good as in the video and yeahã€‚V is a book with the last version
    of To show which was just released nowã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: and so we can't run anything on Collabã€‚So that's also lovelyã€‚Not sure if we
    can get thisã€‚you know your version somehowã€‚ã‚ã€‚Okayï¼Œ so hopefully the Pytoch works
    folks will fix the bug very soon and you will all be able to run all the collapse
    pretty easily in the meantimeã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æ— æ³•åœ¨Collabä¸Šè¿è¡Œä»»ä½•ä¸œè¥¿ã€‚è¿™ä¹Ÿå¾ˆæ£’ã€‚ä¸ç¡®å®šæˆ‘ä»¬èƒ½å¦ä»¥æŸç§æ–¹å¼è·å–ä½ çš„ç‰ˆæœ¬ã€‚ã‚ã€‚å¥½çš„ï¼Œæ‰€ä»¥å¸Œæœ›Pytochçš„å·¥ä½œäººå‘˜èƒ½å°½å¿«ä¿®å¤è¿™ä¸ªbugï¼Œä½ ä»¬éƒ½èƒ½åœ¨æ­¤æœŸé—´è½»æ¾è¿è¡Œæ‰€æœ‰çš„collapseã€‚
- en: I'm going to try to show you the same thing in a tripyter notebook and'm going
    where I'm not going do something that can run itã€‚Geã€‚So let me just goã€‚ I'm not
    gonna be hereã€‚So if you prefer winning notebooks locallyã€‚ everything is in the
    report called Notes inside the Eing F Or and with this notebook you have a course
    sub folder and when you have chapter by chapter or directly for the videosã€‚So
    let's lookã€‚At the same notebook I was trying to run in Collabã€‚Zoom in a little
    bitã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å°è¯•åœ¨tripyterç¬”è®°æœ¬ä¸­å‘ä½ å±•ç¤ºåŒæ ·çš„äº‹æƒ…ï¼Œæˆ‘ä¸æ‰“ç®—åšå¯ä»¥è¿è¡Œçš„äº‹æƒ…ã€‚Geã€‚æ‰€ä»¥è®©æˆ‘å»ã€‚æˆ‘ä¸ä¼šåœ¨è¿™é‡Œã€‚æ‰€ä»¥å¦‚æœä½ æ›´å–œæ¬¢åœ¨æœ¬åœ°èµ¢å¾—ç¬”è®°æœ¬ã€‚æ‰€æœ‰å†…å®¹éƒ½åœ¨åä¸ºNotesçš„æŠ¥å‘Šä¸­ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªEing
    F Orï¼Œä½¿ç”¨è¿™ä¸ªç¬”è®°æœ¬ä½ æœ‰ä¸€ä¸ªè¯¾ç¨‹å­æ–‡ä»¶å¤¹ï¼Œå½“ä½ æŒ‰ç« èŠ‚æˆ–ç›´æ¥ç”¨äºè§†é¢‘æ—¶ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘ä»¬å°è¯•åœ¨Collabä¸­è¿è¡Œçš„åŒä¸€ä¸ªç¬”è®°æœ¬ã€‚æ”¾å¤§ä¸€ç‚¹ã€‚
- en: Let is done lessï¼Œ okayã€‚So after running the installã€‚ which I don't need to do
    because it's my have everything installedã€‚ you can run the code that we were seeing
    on the video you can play around and put another sentenceã€‚ try with several sentences
    at the same timeã€‚Twice as there wrote classificationification pipeline and all
    the pipeline were saw in the videoã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å‡å°‘ä¸€ç‚¹ï¼Œå¥½å—ã€‚æ‰€ä»¥åœ¨è¿è¡Œå®‰è£…åã€‚æˆ‘ä¸éœ€è¦è¿™æ ·åšï¼Œå› ä¸ºæˆ‘å·²ç»å®‰è£…å¥½ä¸€åˆ‡ã€‚ä½ å¯ä»¥è¿è¡Œæˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ä»£ç ï¼Œå¯ä»¥éšæ„æ›´æ”¹ï¼Œè¯•è¯•å…¶ä»–å¥å­ï¼ŒåŒæ—¶å°è¯•å¤šä¸ªå¥å­ã€‚ä¸¤æ¬¡é‚£é‡Œå†™çš„åˆ†ç±»ç®¡é“ï¼Œå’Œæˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„æ‰€æœ‰ç®¡é“ã€‚
- en: ğŸ˜Šï¼ŒAs the nextã€‚The other way you could try all of that is model everã€‚å—¯ã€‚spec edã€‚ä¸å¾—æœ‰ä½ æ€ä¹ˆæ ·ã€‚è¯¶ã€‚Where
    you can go so the model appears do co s models and where you can click on any
    modelã€‚ So for instanceï¼Œ if we go and the distill be based on case fine turn SS
    to Englishã€‚ which we saw as a default model for the sentiment analysis pipelineã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ¥ä¸‹æ¥ã€‚ä½ å¯ä»¥å°è¯•æ‰€æœ‰è¿™äº›çš„å¦ä¸€ç§æ–¹å¼æ˜¯æ¨¡å‹ã€‚å—¯ã€‚è§„èŒƒæ•™è‚²ã€‚ä¸å¾—æœ‰ä½ æ€ä¹ˆæ ·ã€‚è¯¶ã€‚ä½ å¯ä»¥å»çš„åœ°æ–¹ï¼Œæ‰€ä»¥æ¨¡å‹ä¼šå‡ºç°ï¼Œä½ å¯ä»¥ç‚¹å‡»ä»»ä½•æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å»ï¼Œå¹¶å°†è’¸é¦åŸºäºæ¡ˆä¾‹å¥½çš„è½¬æˆè‹±è¯­ã€‚è¿™æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„æƒ…æ„Ÿåˆ†æç®¡é“çš„é»˜è®¤æ¨¡å‹ã€‚
- en: we can go there and whereas a small wet where we canã€‚Try it on any sentenceã€‚And
    gets the same resultsã€‚Soã€‚You've got oh meta tableã€‚ so you've got at least three
    different ways of trying all the good samplesã€‚ So on the with the inference API
    and the websiteï¼Œ sometimes when you were when you're trying a model you have a
    small progress by here while it's loading and then you can try it on b or sentences
    in aã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å»é‚£é‡Œï¼Œåœ¨ä¸€ä¸ªå°æ¹¿åœ°æ–¹ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚åœ¨ä»»ä½•å¥å­ä¸Šè¯•è¯•ã€‚å¹¶å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚æ‰€ä»¥ã€‚ä½ æœ‰å“¦å…ƒè¡¨ã€‚æ‰€ä»¥ä½ è‡³å°‘æœ‰ä¸‰ç§ä¸åŒçš„æ–¹æ³•æ¥å°è¯•æ‰€æœ‰çš„å¥½æ ·æœ¬ã€‚åœ¨æ¨ç†APIå’Œç½‘ç«™ä¸Šï¼Œæœ‰æ—¶å½“ä½ å°è¯•ä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œä½ ä¼šçœ‹åˆ°å°è¿›åº¦æ¡åœ¨åŠ è½½ï¼Œç„¶åä½ å¯ä»¥åœ¨bæˆ–å¥å­ä¸­è¯•è¯•ã€‚
- en: Soã€‚Let's goã€‚Back to the courseã€‚On the next sectionã€‚About how transformer models
    work so transformer models are pretty recentã€‚ So the architecture in itself was
    released in a paper in 2017 and then the first pretrain model was Gã€‚And was released
    by Open AI in June 2018ï¼Œ a second betweentrain model Westbro released by Google
    in October 2018 and then it kind of accelerated like here are just a few samples
    of modelã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ã€‚è®©æˆ‘ä»¬ã€‚å›åˆ°è¯¾ç¨‹ã€‚ä¸‹ä¸€éƒ¨åˆ†ã€‚å…³äºå˜æ¢å™¨æ¨¡å‹å¦‚ä½•å·¥ä½œçš„ï¼Œå˜æ¢å™¨æ¨¡å‹æ˜¯ç›¸å½“æ–°çš„ã€‚æ‰€ä»¥æ¶æ„æœ¬èº«æ˜¯åœ¨2017å¹´çš„ä¸€ç¯‡è®ºæ–‡ä¸­å‘å¸ƒçš„ï¼Œç¬¬ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ˜¯Gã€‚ç”±Open
    AIåœ¨2018å¹´6æœˆå‘å¸ƒï¼Œç¬¬äºŒä¸ªé¢„è®­ç»ƒæ¨¡å‹Westbroåœ¨2018å¹´10æœˆç”±è°·æ­Œå‘å¸ƒï¼Œç„¶åå®ƒæœ‰ç‚¹åŠ é€Ÿäº†ï¼Œè¿™é‡Œåªæ˜¯å‡ ä¸ªæ¨¡å‹çš„ç¤ºä¾‹ã€‚
- en: but there are manyï¼Œ many moreã€‚That were released and even though so we would
    need at least like to to have this image be twice or three times as big to be
    able to put every model that was released between 2019 and right nowã€‚It's really
    hard to follow the base at which everything is releasedã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿˜æœ‰å¾ˆå¤šï¼Œå¾ˆå¤šæ›´å¤šã€‚é‚£äº›å·²ç»å‘å¸ƒï¼Œå°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦è¿™ä¸ªå›¾åƒå˜æˆä¸¤å€æˆ–ä¸‰å€å¤§ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ”¾ä¸‹è‡ª2019å¹´è‡³ä»Šå‘å¸ƒçš„æ¯ä¸€ä¸ªæ¨¡å‹ã€‚çœŸçš„å¾ˆéš¾è·Ÿä¸Šå‘å¸ƒçš„åŸºç¡€ã€‚
- en: but the transformformers library as I think 60 different architectures now and
    is trying to add as soon as the papers are releasedã€‚And so what we're going to
    look at here is not what's inside a transformable model in detailsã€‚ in detailsï¼Œ
    but a little bit of an overview and there are three types of transformer models
    which we're going to explore in this sectionã€‚ so we are GT model GT like model
    that are called ons or autoregressive modelsã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Wwhichch are transformable models that are basically to generate textã€‚ look
    so what we were talking about beforeï¼Œ like when your phone is trying to suggest
    you ideas of words that you could put in the sentence afterwardã€‚The other type
    of transformer model is the be like model a GT like sorry it's not the noncodeders
    model it's a decoders modelã€‚ sorry autoregressive or decoders model for GT bet
    like its encodecoder model or autoencoing modelsã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: so those models are best suited to classify things so generate a9 representation
    of the sentence which allows you to either classify the whole sentenceã€‚ classify
    every words in the sentence are they also very good for the extractive question
    and throwinging task we talked about earlierã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: And the last type of model are the sequence to sequence transformer models are
    encode decos modelã€‚ which has the ones that are better suited for the sequence
    to sequence text such as translationã€‚ sumorizationï¼Œ basically writing a new text
    from an input textã€‚So all those transformer models are language models and the
    main difference between the first two typesã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: the encodeders and the decoders is that the decoders are portrayed by kissing
    the next worldã€‚ so that's why we are very good at generating text because that's
    how they are betrayedã€‚Whereas the en those models are usually betweened with feeling
    some random masks in the sentenceã€‚ so we have two examples hereã€‚And those transformformers
    models are super big so we started with like open AGTD with tests rather than
    tell millions of parameters and now we have models with like billions or tens
    of billions of parametersã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Which is why it's very important to reuse this modelï¼Œ which is the whole point
    of transfer learningã€‚ so transfer learning is when you take your betweentrain
    modelã€‚ which is huge and between on lots and lots of amount of data which took
    a lot of compute so a lot of later bra animated a lot of CO2 and you reuse that
    model to find unit on a new tasksã€‚ the task that you want to work with andã€‚Yeahï¼Œ
    so by reducing the model instead of training a new one from scratchã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: you're saving computeï¼Œ you're saving money and you need less data than what
    the model always be trained onã€‚Let you see our new questionsã€‚Very one question
    is GT3 the only model that allows you to do zero short learningï¼Ÿ
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: So that is how this model was advertisedï¼Œ it's not the only one because T5 for
    instanceã€‚ has the same kind of prompt where you tell itï¼Œ for instanceã€‚ summarize
    this text or translate this text from this language to this language and so it
    can also do some sort of zero learningã€‚And the link to the notebook but it was
    answered already So let's go and watch the transfer learning video I hope this
    one is going to be a little bit better than the previous one and or in the chat
    like will share the link if you want to watch it directly if it's too legã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_18.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_19.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: What is transfer learningï¼ŸThe idea of transfer learning is to leverage signal
    knowledge acquired by a model train with lotss of data on another taskã€‚The Model
    A will be trained specifically for task Aã€‚Now let's say you want to train Ad all
    B for a different taskã€‚One option would be to train the model from scratchã€‚This
    could take lots of computationã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: time and dataã€‚Insteadï¼Œ we could initialize Model B with the same weights as
    Model Aã€‚ transferring the knowledge of Model A on T Pã€‚When trend from scratchã€‚
    all the middle's weights are initialized randomlyã€‚In this exampleã€‚ we are training
    a belt model on the task of recognizing if two sentences are similar on itã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: On the leftï¼Œ it's trained from scratchã€‚And on the rightï¼Œ it's venting a proed
    modelã€‚As we can seeã€‚ using transfer learning on the preed model yields better
    resultsã€‚And it doesn't matter if we train longerï¼Œ so training from scratch is
    kept around 70% accuracyã€‚ or the betweentrain would all beat the 86% easilyã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: This is because portraytrained models are usually trained on large amounts of
    data but provides a model with a statistical understanding of the language used
    during portraytrainingã€‚In computer visionsï¼Œ transfer learning has been applied
    successfully for almost 10 yearsã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Models are frequently proed on ImageNet by a asset containing 1ã€‚2 million of
    photo imagesã€‚Each image is classified by one of 1000 levelsã€‚Training like thisã€‚
    unlabeled data is called supervised learningã€‚In natural language processingã€‚ transfer
    learning is a bit more recentã€‚A key difference with ImageNet is that the training
    is usually self supervisedã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Which means it doesn't require humanation for the labelsã€‚A very common portraying
    objective is to guess the next word in a sentenceã€‚Which only requires lots and
    lots of textã€‚GPT2ï¼Œ for instanceã€‚ was retrieveed this way using the content of
    45 million links posted by users on webã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Another example of selfupvised per training objective is to predict the value
    of randomly massed qualsã€‚Which is similar to fit in blood B tests you may have
    done in schoolã€‚Brt was between this way using the English Wikiped and had100 and
    published booksã€‚In practiceã€‚ transfer learning is applied on a given model by
    throwing away its headã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: that is its last layers focused on the per training objectiveã€‚And' with a new
    randomly initialized a suitable for the desk attemptã€‚For instanceã€‚ when invented
    in the build model earlierï¼Œ we remove the a that classified Mque and replaced
    it with a classifier with two outputposts since our task at two levelsã€‚To be as
    efficient as possibleï¼Œ the protrained model used should be as similar as possible
    to the task it's fine tune onã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: For instanceï¼Œ if the problem is to classify German sentencesã€‚It's best to use
    a German portraying modelã€‚But with the good comes to badã€‚ the proed model does
    not only transfer its knowledgeï¼Œ but also any bias it may containã€‚ImageNe mostly
    contains images coming from the United States and Western Europeã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: so models fine tuned with it usually will perform better on images from these
    countriesã€‚But Beni also studied the bias in the prediction of its Gpyy3ã€‚ which
    was between using the guess and X world objectivesã€‚Changing the gender of the
    prone from E Westbury to she Westbury changed the predictions from mostly neutral
    objectivesã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: To almost only physical onesã€‚In the model code of the GT2 Morã€‚ Open AI also
    acknowledges its bias and discourages its use in systems that interact with humansã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_21.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_22.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: å—¯ã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_24.png)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_25.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Okayï¼Œ so let me see if there are any questions on the videoã€‚ğŸ˜”ï¼ŒI don't think
    soã€‚Okayã€‚ so this was what transfer learning is very high level introductionions
    and this is what's usually done with transformerss modelã€‚And for instanceï¼Œ the
    model we used before distill B fine tune necess to Englishã€‚ I'm forgetting alphat
    nameï¼Œ as the name indicates it was fine tuned on the62 dataset set which is a
    dataset set containing sentences and you have to classify them between positive
    and negativeã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Andã€‚Unlesss are are any new questionsï¼Œ I don't think so we're already to dive
    into the transformer architecture a little bit more so Omar will post the link
    of that video in the chatã€‚ I hope it's not too leggy againï¼Œ so this video is going
    to introduce as the difference between Odo decos and sequence to sequence transformer
    model we briefly talked about beforeã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Let's study the transformer architectureã€‚This video is the introductory video
    to the Ensã€‚ decoders and Ender decoder series of videosã€‚In the seriesã€‚ we'll try
    to understand what makes a transformer network and we'll try to explain it in
    simple high level termsã€‚No advanced understanding of neural networks is necessaryã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: but an understanding of basic vectors and tensors may helpã€‚To get startedã€‚ we'll
    take up this diagram from the original transformer paper entitledAttention is
    All you need by Vawaing it upã€‚As we'll see hereï¼Œ we can leverage only some parts
    of it according to what we're trying to doã€‚We want to dive into these specific
    layersï¼Œ building up that architectureã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: but we' try to understand the different ways this architecture can be usedã€‚Let's
    first start by splitting that architecture into two parts on the left you have
    the encoder and on the rightã€‚ the decoderã€‚These two can be used togetherï¼Œ but
    they can also be used independentlyã€‚Let's understand how these workã€‚The encoder
    accepts inputs that represent textã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: it converts these textï¼Œ these words into numerical representationsã€‚These numerical
    representations can also be called embeddings or featuresã€‚We'll see that it uses
    the self attention mechanism as its main componentã€‚And we recommend you check
    out the video on encoders specifically to understand what is this numerical presentation
    as well as how it worksã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒWe'll study the self attention mechanism in more detailï¼Œ as well as its bidirectional
    propertiesã€‚ğŸ˜Šã€‚The decoder is similar to the encoderï¼Œ it can also accept text inputsã€‚It
    uses a similar mechanism as the encoderï¼Œ which is the masked self attention as
    wellã€‚It differs from the encoder due to its unititedirectional feature and is
    traditionally used in an autoregressive mannerã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Here tooï¼Œ we recommend you check out the video on decodersã€‚ especially to understand
    how all of this worksã€‚Combining the two parts results in what is known as an anchor
    decoder or a sequence to sequence transformã€‚The encoder accepts inputs and computes
    a high level representation of those inputsã€‚These outputs are then passed to the
    decoderã€‚ The decoder uses the encoder's outputs and alongside other inputs to
    generate a predictionã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: And then predict an outputï¼Œ which is will reuse in future iterationsï¼Œ hence
    the term autoregressiveã€‚Finallyï¼Œ to get an understanding of encoder decoders as
    a wholeã€‚ we recommend you check out the video on encoder decoderã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_27.png)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_28.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: ã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_30.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Okay so that was the general introduction and so basically this is the diagram
    you have to remember about the general transformformers architecture and so what
    is an encode decoder or an encode decoder model we dive into that a little bit
    more just before going there we briefly talked about attention which was in theã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: diagramia for the original paperï¼Œ so the core of the architecture the transformer
    model is this layer called multied attentionã€‚ which is a layer that basically
    as indicates base attentionã€‚ so it's going to look at your wall sentenceã€‚sequence
    of input and for each word it's going to compute some score li to pay attention
    to this or this word of is a world so why is that it's because the transformer
    architecture was originally designed for translation and when you're translating
    if you're translating a given word you need that word but you need to understand
    the context around it so for instance you might need the gender of if the word
    are none you may need its gender which would be the word before you may need some
    words afterã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: So that attention layer is where on top of computing some contextual representation
    of a given worldã€‚ it's there to tell them all all for this specific word also
    in to this one in particular this world not so useful but this one you should
    really pay attention to it will have videos coming in section 2 and section 3
    of the calls that go into more details of what attention layer is but that's the
    I level1 introductionã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ³¨æ„åŠ›å±‚æ˜¯è®¡ç®—ç»™å®šä¸–ç•Œçš„ä¸€äº›ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„åœ°æ–¹ã€‚å®ƒç”¨äºå‘Šè¯‰æ‰€æœ‰ä¸è¿™ä¸ªç‰¹å®šå•è¯ç›¸å…³çš„å†…å®¹ï¼Œå°¤å…¶æ˜¯è¿™ä¸ªå•è¯ï¼Œè™½ç„¶è¿™ä¸ªä¸–ç•Œä¸å¤ªæœ‰ç”¨ï¼Œä½†è¿™ä¸ªä½ çœŸçš„åº”è¯¥å…³æ³¨ï¼Œå®ƒå°†åœ¨ç¬¬2èŠ‚å’Œç¬¬3èŠ‚ä¸­æœ‰è§†é¢‘ï¼Œè¯¦ç»†è®²è§£æ³¨æ„åŠ›å±‚æ˜¯ä»€ä¹ˆï¼Œä½†è¿™å°±æ˜¯æˆ‘å¯¹ç¬¬ä¸€å±‚çš„ä»‹ç»ã€‚
- en: And the key difference between encode codeos and decoders is thattting on codes
    as the attention mechanism is allowed to look at every word in the sentenceã€‚ So
    the word before and the word afterï¼Œ that's because like the bird model when you
    need toã€‚Detectã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºï¼Œç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æŸ¥çœ‹å¥å­ä¸­çš„æ¯ä¸ªå•è¯ã€‚å› æ­¤ï¼Œå‰é¢çš„å•è¯å’Œåé¢çš„å•è¯ï¼Œå› ä¸ºåƒé¸Ÿæ¨¡å‹é‚£æ ·ï¼Œå½“ä½ éœ€è¦æ£€æµ‹æ—¶ã€‚
- en: sorry to guess what is the value of a mask wordï¼Œ it's useful to look at what
    was before and also what we afterã€‚The decoders models like GT have to predict
    the next worldã€‚ so if they were allowed to look at the word after it would be
    cheatingã€‚ so in those models the attention layer is allowed to look at what was
    before in the sentence so for instance when trying to guess silver as the attention
    layer can only look at my name and is they can't look at what is after in the
    sentenceã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ±æ­‰ï¼Œè¦çŒœæµ‹ä¸€ä¸ªæ©ç è¯çš„å€¼ï¼ŒæŸ¥çœ‹å‰é¢çš„å†…å®¹ä»¥åŠåé¢çš„å†…å®¹æ˜¯æœ‰ç”¨çš„ã€‚è§£ç å™¨æ¨¡å‹å¦‚GTå¿…é¡»é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚æ‰€ä»¥å¦‚æœå®ƒä»¬è¢«å…è®¸æŸ¥çœ‹åé¢çš„å•è¯ï¼Œé‚£å°†æ˜¯ä½œå¼Šã€‚å› æ­¤åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œæ³¨æ„åŠ›å±‚åªå…è®¸æŸ¥çœ‹å¥å­ä¸­å‰é¢çš„å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå½“è¯•å›¾çŒœæµ‹â€œé“¶â€æ—¶ï¼Œæ³¨æ„åŠ›å±‚åªèƒ½æŸ¥çœ‹â€œæˆ‘çš„åå­—â€ï¼Œè€Œä¸èƒ½æŸ¥çœ‹å¥å­ä¸­çš„åé¢å†…å®¹ã€‚
- en: Yeahãã€‚We are going to switch to the nextã€‚Section of this chapter on Ocuoss modelsï¼Œ
    but before thatã€‚ let me see if there are any questionsã€‚On the decoder diagramã€‚
    what does the output shifted right refer toï¼Œ pleaseã€‚Let me check that diagramã€‚Thisã€‚
    so that's becauseã€‚Hereã€‚We are training a translation model remember this is the
    original architecture of the transformer model so when you translate a phrase
    sentence in English and the output is a sentence in French so here the labels
    your this part of the transformer model is the decoder and it will try to guess
    the next word so it's going to begin with nothing and then when to try to begin
    the first word of the sentence then it will have the first word of the sentence
    to try begin to get the second word of the sentence then it will have the first
    word in the sentence to get the third word in the sentenceã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘ä»¬å°†åˆ‡æ¢åˆ°æœ¬ç« å…³äºOcuossæ¨¡å‹çš„ä¸‹ä¸€èŠ‚ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘çœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜ã€‚åœ¨è§£ç å™¨å›¾ä¸­ï¼Œè¾“å‡ºå³ç§»æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Œè¯·é—®ã€‚è®©æˆ‘æ£€æŸ¥ä¸€ä¸‹é‚£ä¸ªå›¾ã€‚è¿™é‡Œã€‚å› ä¸ºæˆ‘ä»¬æ­£åœ¨è®­ç»ƒä¸€ä¸ªç¿»è¯‘æ¨¡å‹ï¼Œè¯·è®°ä½è¿™æ˜¯å˜æ¢å™¨æ¨¡å‹çš„åŸå§‹æ¶æ„ï¼Œæ‰€ä»¥å½“ä½ ç¿»è¯‘ä¸€ä¸ªè‹±æ–‡çŸ­è¯­å¥å­æ—¶ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªæ³•è¯­å¥å­ã€‚å› æ­¤ï¼Œè¿™é‡Œçš„æ ‡ç­¾ä½ è¿™ä¸ªå˜æ¢å™¨æ¨¡å‹çš„éƒ¨åˆ†æ˜¯è§£ç å™¨ï¼Œå®ƒä¼šå°è¯•çŒœæµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œæ‰€ä»¥å®ƒä¼šä»æ— å¼€å§‹ï¼Œç„¶åå°è¯•å¼€å§‹å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯ï¼Œç„¶åå®ƒå°†æœ‰å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯æ¥å°è¯•è·å–å¥å­çš„ç¬¬äºŒä¸ªå•è¯ï¼Œç„¶åå®ƒå°†æœ‰å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯æ¥è·å–å¥å­çš„ç¬¬ä¸‰ä¸ªå•è¯ã€‚
- en: etctera et cetera so that's where the shift right means it means that here we
    have the output so the text in the desired language shifted one token to the rightã€‚And
    another question wasï¼Œ can you provide a quick overview intuition of oh a digital
    version of a model like distber is able to maintain accuracy while being significantly
    sorryã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰ç­‰ï¼Œæ‰€ä»¥å³ç§»çš„æ„æ€æ˜¯æˆ‘ä»¬æœ‰è¾“å‡ºï¼Œå³æ‰€éœ€è¯­è¨€çš„æ–‡æœ¬å³ç§»ä¸€ä¸ªæ ‡è®°ã€‚è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œä½ èƒ½å¦æä¾›ä¸€ä¸ªå¿«é€Ÿçš„ç›´è§‚æ¦‚è¿°ï¼Œè¯´æ˜æ•°å­—ç‰ˆæœ¬çš„æ¨¡å‹å¦‚distberå¦‚ä½•åœ¨æ˜¾è‘—å‡è½»çš„æƒ…å†µä¸‹ä¿æŒå‡†ç¡®æ€§ã€‚
- en: more lightweight so quick overview intuitionã€‚Sureã€‚In generalã€‚ transformer models
    or deep learningruning models have lots of parametersï¼Œ several millionsã€‚ but a
    lot of those parameters are either redundant or not necessarily really useful
    and so for instance varies a lot of research into pruning transformers networks
    so pruning means removing some of those weights to be able to go faster especially
    at eachferã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«é€Ÿæ¦‚è¿°ç›´è§‚ã€‚å¥½çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå˜æ¢å™¨æ¨¡å‹æˆ–æ·±åº¦å­¦ä¹ æ¨¡å‹æœ‰å¾ˆå¤šå‚æ•°ï¼Œæ•°ç™¾ä¸‡ä¸ªã€‚ä½†å…¶ä¸­å¾ˆå¤šå‚æ•°è¦ä¹ˆæ˜¯å†—ä½™çš„ï¼Œè¦ä¹ˆå¹¶ä¸ä¸€å®šçœŸæ­£æœ‰ç”¨ï¼Œå› æ­¤ä¾‹å¦‚æœ‰å¾ˆå¤šç ”ç©¶è‡´åŠ›äºä¿®å‰ªå˜æ¢å™¨ç½‘ç»œï¼Œä¿®å‰ªæ„å‘³ç€ç§»é™¤ä¸€äº›æƒé‡ä»¥ä¾¿èƒ½å¤Ÿæ›´å¿«ï¼Œå°¤å…¶æ˜¯åœ¨æ¯ä¸ªæ—¶åˆ»ã€‚
- en: distillation is another way of reducing the size so the process is to have a
    smaller model try to get some output as a bigger model and it works reasonably
    well to still have some good performanceã€‚So that's because the intriition behind
    it is that a very big model as lots of parametersã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: but not all of those parameters are really usefulã€‚And so let's dive into the
    encoders model so we have three more videosï¼Œ encodersã€‚ decos and then sequence
    to sequence modelã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_32.png)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: We put it in full screenï¼Œ and designã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_34.png)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In this videoï¼Œ we'll study the encoder architectureã€‚An example of a popular
    encoder only architecture is Btï¼Œ which is the most popular model of its kindã€‚Let's
    first start by understanding how it worksã€‚We'll use a small example using three
    wordsã€‚ We use these as inputs and pass them through the encoderã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: We retrieve a numerical representation of each wordã€‚Hereï¼Œ for exampleã€‚ the encode
    converts those three words welcomee to NYC in these three sequences of numbersã€‚The
    encode outputs exactly one sequence of numbers per input wordã€‚This numerical representation
    can also be called a feature vector or a feature tensorã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive in this representationã€‚ It contains one vector per word that was
    passed through the encoderã€‚Each of these vector is a numerical representation
    of the word in questionã€‚The dimension of that vector is defined by the architecture
    of the model for the base bird modelã€‚ it is 768ã€‚These representations contain
    the value of a wordï¼Œ but contextualizedã€‚For exampleã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: the vector attributed to the word2 isn't the representation of only the two
    wordã€‚It also takes into account the words around itï¼Œ which we call the contextã€‚As
    in it looks to the left contextï¼Œ the words on the left of the one we studyingã€‚
    hear the word welcomeï¼Œ and the context on the rightï¼Œ here the word NYCã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: and it outputs a value for the word given its contextã€‚It is therefore a contextualized
    valueã€‚ğŸ˜Šã€‚One could say that the vector of 768 values holds the meaning of the word
    within the textã€‚It does this thanks to the self attention mechanismã€‚The self attention
    mechanism relates to different positions or different words in a single sequence
    in order to compute a representation of that sequenceã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: As we've seen beforeï¼Œ this means that the originalot representation of a word
    has been affected by other words in the sequenceã€‚We won't dive into these specifics
    hereï¼Œ but we' offer some further readings if you want to get a better understanding
    at what happens under the hoodã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: So when should one use an encodeï¼ŸEncodeders can be used as tenderalone models
    in a wide variety of tasksã€‚For exampleï¼Œ Bertï¼Œ arguably the most famous transformer
    modelï¼Œ is a standalone anchor modelã€‚ and at the time of release it be the state
    of the art in many sequence classification tasksã€‚ question answering tasksï¼Œ and
    masked language modeling to only cite a fewã€‚ğŸ˜Šã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that encoders are very powerful at extracting vectors that carry
    meaningful information about a sequenceã€‚This vector can then be handled down the
    road by additional neurons to make sense of itã€‚Let's take a look at some examples
    where encodes really shineã€‚First of allã€‚ mask language modeling or MLMã€‚It's the
    task of predicting a hidden word and a sequence of wordã€‚Hereã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: for exampleï¼Œ we have hidden the word between my and isã€‚This is one of the objectives
    with which Bert was trainedã€‚ It was trained to predict hidden words in a sequenceã€‚Encodes
    shine in this scenario in particular as bidirectional information is crucial hereã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: If we didn't have the words on the rightï¼Œ is it Silva and the dotã€‚ then there
    is very little chance that Bt would have been able to identify name as the correct
    wordã€‚The encoder needs to have a good understanding of the sequence in order to
    predict a masked wordã€‚ as even if the text is grammatically correctã€‚It does not
    necessarily make sense in the context of the sequenceã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlierï¼Œ encodes are good at doing sequence classificationã€‚ğŸ˜Šã€‚Sentiment
    analysis is an example of sequence classificationã€‚The model's aim is to identify
    the sentiment of a sequenceã€‚It can range from giving a sequence a rating from
    one to five stars if doing review analysis to giving a positive or negative rating
    to a sequenceã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: which is what is shown hereã€‚For exampleï¼Œ hereï¼Œ given the two sequencesã€‚ we use
    the model to compute a prediction and to classify the sequences among these two
    classesã€‚ positive and negativeã€‚While the two sequences are very similar containing
    the same wordsã€‚ the meaning is entirely differentï¼Œ and the encoder model is able
    to grasp that differenceã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_36.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_37.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: å—¯ã€‚So that was it forcodo's modelï¼Œ some examples are Alberttï¼Œ Btï¼Œ distill Btï¼Œ
    Electï¼Œ Robertaã€‚Let's have a look at the next sectionï¼Œ which is going to be about
    decoderos and just before that let me check if there are any questionsã€‚Don't see
    any new questions in the chatï¼Œ don't forget that you can ask any questions in
    the chat that's kind of the point of having the live sessionã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_39.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Yeahã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_41.png)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In this videoï¼Œ we'll study the decoder architectureã€‚An example of a popular
    decoder only architecture is GPT2ã€‚In order to understand how decoders workã€‚ we
    recommend taking a look at the video regarding encodersï¼Œ they're extremely similar
    to decodersã€‚One can use a decoder for most of the same tasks as an encoderã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: albeit with generally a little loss of performanceã€‚Let's take the same approach
    we have taken with the encoder to try and understand the architectural differences
    between an encoder and ID coderã€‚We'll use a small example using three wordsã€‚ We
    pass them through their decoderã€‚We retrieve a numerical representation for each
    wordã€‚Hereï¼Œ for exampleã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: the decoder converts the three words welcomee to NYC and these are three sequences
    of numbersã€‚The decoder outputs exactly one sequence of numbers per input wordã€‚This
    numerical representation can also be called a feature vector or a feature tensorã€‚Let's
    dive in this representationã€‚It contains one vector per word that was passed through
    the decoderã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Each of these vectors is a numerical representation of the word in questionã€‚ğŸ˜Šã€‚The
    dimension of that vector is defined by the architecture of the modelã€‚Whether a
    decoder differs from the encoder is principally with its self attention mechanismã€‚
    it's using what is called masked self attentionã€‚Hereï¼Œ for exampleï¼Œ if we focus
    on the word 2ã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: we'll see that this vector is absolutely unmodified by the NYC wordã€‚That's because
    all the words on the rightï¼Œ also known as the right context of the word is maskedã€‚Rather
    than benefiting from all the words on the left and rightï¼Œ so the bidirectional
    contextã€‚ decoders only have access to a single contextã€‚Which can be the left context
    or the right contextã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The mask self attention mechanism differs from the self attention mechanism
    by using an additional mask to hide the context on either side of the wordã€‚The
    words numerical representation will not be affected by the words in the hidden
    contextã€‚So when should one use a decoder decoders like encoders can be used as
    standalone modelsã€‚ as they generate a numerical representationï¼Œ they can also
    be used in a wide variety of tasksã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: howeverï¼Œ the strength of a decoder lies in the way a word can only have access
    to its left contextã€‚Having only access to their left contextï¼Œ they are inherently
    good at text generationrationã€‚ the ability to generate a word or a sequence of
    wordsï¼Œ given a known sequence of wordsã€‚ğŸ˜Šã€‚This is known as causal language modeling
    or natural language generationã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Here's an example of how causal language modeling worksã€‚ We start with an initial
    wordï¼Œ which is myã€‚We use this as input for the decoderã€‚ğŸ˜Šï¼ŒThe model outputs a vector
    of numbersã€‚ and this vector contains information about the sequenceï¼Œ which is
    here a single wordã€‚We apply a small transformation to that vector so that it maps
    to all the words known by the modelã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: which is a mapping that we'll see later called a language modeling headã€‚We identify
    that the model believes that the most probable following word is nameã€‚We then
    take that new word and add it to the initial sequenceã€‚From myï¼Œ we are now at my
    nameã€‚This is where the autoregressive aspect comes inã€‚ğŸ˜Šã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Outtoregressive models reuse their past outputs as inputs and the following
    stepsã€‚Once againã€‚ we do the exact same operationã€‚We cast that sequence through
    the decoder and retrieve the most probable following wordã€‚In this caseï¼Œ it is
    the word isã€‚We repeat the operation until we're satisfiedã€‚Starting from a single
    wordï¼Œ we've now generated a full sentenceã€‚We decided to stop thereã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: but we could continue for a whileã€‚ GP T 2ï¼Œ for exampleï¼Œ has a maximum context
    size of 1024ã€‚ We could eventually generate up to 1024 wordsï¼Œ and the decoder would
    still have some memory of their first words and that sequenceã€‚ğŸ˜Šï¼ŒYeahã€‚ê²Œì„ã€‚Let me
    look at some questions on the decoders model Oh one question is it possible to
    use larger sentences more than 512 words as inputs for encodesã€‚ great question
    so it depends but most of the time no for instance spt as a maximum length of
    512 so you can't use larger sentences than thatã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Some newer models like Longformerï¼Œ for instanceï¼Œ can accept a longer contextã€‚
    so you should look at the documentationï¼Œ but first would be very specific on codesã€‚
    like not all ons can do thatã€‚Another thing that you can do is split your sentence
    into several parts of 512 worlds and then if you get a representationã€‚ you pass
    each of those chunk to the model so you get a representation for each of those
    checks and you canã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: for instanceï¼Œ average where you get at the end to try to train a classifier
    for larger sentencesã€‚ but that's kind of the only wayï¼Œ either a specific model
    that has been trained to have longer inputs such as longform or splitting your
    inputs input bitsã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: So another questionï¼Œ how does the mask language modeling objective deal with
    long gray worlds that end up being a multi doken worldã€‚ example sea planee with
    Pipe becomesã€‚Ciã€‚ğŸ˜Šï¼ŒAshAsh planeã€‚ so you will see about that for those who are more
    beginnersã€‚ you will see about that separation in the next chapter with the Tokenals
    videoã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: If not using wall wall masking then the model can cheat and see parts of the
    longer worldã€‚ so that is absolutely correct and that's why you will see that you
    have several versions of PE there is one that has been pretrained with wall wall
    masking and another one that has been trained without that it would be cheating
    for the ash ash plane because the modelã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: I mean it would be cheating where we're kissing Cï¼Œ sorry when you've got the
    ash ash plane in the contextã€‚ when you're trying to guess ash ash plane and you've
    got the shifts because in the context it's less cheating because the model has
    to guess that the model did not hand thereã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: but it's true that using world wall masking would remove that kind of specific
    bias and cheating we've seenã€‚Weve the upper risã€‚Okayï¼Œ so let's look at the last
    video and then I'll answer to more question after the last video on sequence to
    sequence transformable laws which combine both theionders and the decodersã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: In this videoï¼Œ we'll study the encoder decoder architectureã€‚An example of a
    popular encoder decoder model is T5ã€‚In order to understand how the encoder decoder
    worksã€‚ we recommend you check out the videos on encoders and decoders as a standalone
    modelsã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how they work individually will help understanding how an encoder
    decoder worksã€‚Let's start from what we've seen about the encoderã€‚The encoder takes
    words as inputsã€‚ casts them through the encoderï¼Œ and retrieves a numerical representation
    for each word casts through itã€‚We now know that this numerical representation
    holds information about the meaning of the sequenceã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Let's put this aside and add the decoder to the diagramã€‚In this scenarioã€‚ we're
    using the decoder in a manner that we haven't seen beforeã€‚We're passing the outputs
    of the encoder directly to itã€‚Additionally to the encoder outputsã€‚ we also give
    the decoder a sequenceã€‚When prompting the decoder for an output with no initial
    sequenceã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: we can give it the value that indicates the start of a sequenceã€‚ğŸ˜Šã€‚And that's
    where the anchor to decor magic happensã€‚ğŸ˜Šï¼ŒThe encoder accepts a sequence as inputã€‚It
    computes a prediction and outputs a numerical representationã€‚ğŸ˜Šã€‚Then it sends that
    over to the decoderã€‚It hasï¼Œ in a senseï¼Œ encoded that sequenceã€‚And the decoderã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: in turnï¼Œ using this input alongside its usual sequence input will take a stab
    at decoding the sequenceã€‚The decoder dedes a sequenceï¼Œ and outputs a wordã€‚As of
    nowã€‚ we don't really need to make sense of that wordï¼Œ but we can understand that
    the decoder is essentially decoding what the encoder has outputã€‚The startup sequence
    hereï¼Œ the startup sequence word here indicates that it should start decoding the
    sequenceã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have both the encoder numerical representation and an initial generated
    wordã€‚ we don't need the encoder anymoreã€‚As we have seen before with the decoderã€‚
    it can act in an autoregressive mannerã€‚The word it has just output can now be
    used as an inputã€‚Thisã€‚ in combination with the numerical representation output
    by the encoderã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: can now be used to generate a second wordã€‚Please note that the first word is
    still here as the model still outputs itã€‚ howeverï¼Œ we have grade it out as we
    have no need for it anymoreã€‚ğŸ˜Šï¼ŒWe can continue on and onã€‚ for exampleï¼Œ on the decoder
    outputs a value that we consider a stopping valueã€‚ like a dot meaning the end
    of a sequenceã€‚Here we've seen the full mechanism of the encoder decoder transformerã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: let's go over one more timeã€‚ we have an initial sequence that is sent to the
    encoderã€‚ğŸ˜Šã€‚That encoder output is then sent to the decoder for it to be decodedã€‚While
    it can now discard the encoder after a single useã€‚ the decoder will be used several
    times until we have generated every word that we needã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: So let's see a concrete example with translation language modelingï¼Œ also called
    transductionã€‚ which is the act of translating a sequenceã€‚Here we would like to
    translate this English sequence We to NOIC in Frenchã€‚We're using a transformer
    model that is trained for that task explicitlyã€‚ we use the encoder to create a
    representation of the English sentenceã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: we cast this to the decoder with the use of the start sequence wordã€‚ we ask
    it to output the first wordã€‚It outputs the avenueï¼Œ which means' welcomeã€‚And we
    then use B avenue as the input sequence for the decoderã€‚This alongside the encoder
    numerical representationï¼Œ allows the decoder to predict the second word aã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: which is two in Englishã€‚ğŸ˜Šï¼ŒFinallyï¼Œ we ask the decoder to predict a third wordï¼Œ
    it predicts NYCã€‚ which is correctï¼Œ we've translated the sentenceã€‚Where the encoder
    decoder really shines is that we have an encoder and a decoderã€‚ which often do
    not share weightsã€‚Thereforeï¼Œ we have an entire blockã€‚ the encoder that can be
    trained to understand the sequence and extract the relevant informationã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: For the translation scenario we've seen earlierï¼Œ for exampleã€‚ this would mean
    parsing and understanding what was said in the English languageã€‚It would mean
    extracting information from that language and putting all of that in a vector
    dense in informationã€‚ğŸ˜Šï¼ŒOn the other handï¼Œ we have the decoder whose sole purpose
    is to decode the numerical representation output by the encoderã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: This decoder can be specialized in a completely different language or even modality
    like images or speechã€‚ğŸ˜Šï¼ŒEncosï¼Œ decoders are special for several reasonsã€‚Firstlyã€‚
    theyre able to manage sequence to sequence tasks like translation that we have
    just seenã€‚Secondlyã€‚ the weights between the encoder and the decoder parts are
    not necessarily sharedã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Let's take another example of translationã€‚Here where translating transformers
    are powerful in Frenchã€‚Firstlyï¼Œ this means that from a sequence of three wordsã€‚
    we're able to generate a sequence of four wordsã€‚One could argue that this could
    be handled with a decoder that would generate the translation in an autoregressive
    mannerã€‚And they would be rightã€‚Another example of where sequence to sequence transformers
    shine is in summarizationã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒHere we have very very long sequenceï¼Œ generally a full textï¼Œ and we want to
    summarize itã€‚Since the encoder and decoders are separatedï¼Œ we can have different
    context lengthsï¼Œ for exampleã€‚ a very long context for the encoder which handles
    the text and a smaller context for the decoderã€‚ which handles the summarized sequenceã€‚There
    are a lot of sequence to sequence modelsã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: This contains a few examples of popular encoder decoder models available in
    the Transformers libraryã€‚Additionallyï¼Œ you can load an encoder and a decoder inside
    an encoder decoder modelã€‚ğŸ˜Šï¼ŒThereforeã€‚ according to the specific task you're targetingï¼Œ
    you may choose to use specific encoders and decoders which have proven their worth
    on these specific tasksã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_43.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_44.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Yeahã€‚Okayï¼Œ so let's see if we have any questionsã€‚Ohï¼Œ okayã€‚ is pararaing kind
    of decodder problem or both encodeder decodder problemï¼Ÿ
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: And is there any pipeline available for sameï¼ŸSo paraphrasing so generating a
    similar task to the input will be more like an encode or decoder pronymã€‚ howeverï¼Œ
    it seems that it's going to be very hard to train a model on that task specifically
    because it's going to have a tendency to want to include the same thing it receivedã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Unless you want to parase in another styleï¼Œ for instanceã€‚ going from formal
    style to very casual styleã€‚There is no pipeline available provide that desk out
    of the books in the transformformers libraryã€‚So let's go to the next section of
    Cha1ï¼Œ which is the last section of the chapter and talk a little bit about bias
    and limitations of the transformer modelsã€‚So like any deep learning model in general
    transformers are just one specific caseã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: but it's really a problem for all kind of deeping model so there are powerful
    tools and as you know probably if you take a deep learning introduction course
    via powerful tools but you don't really control or a get from the input to the
    output this was more controlled by the training they cut and the training data
    they receive but if you don't take any precautions you can have those models make
    predictions that you don't necessarily want in the app deploy so for instanceã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: let's just have a very quick look at what B which has been retrained on the
    objective of thin maskQu and just have a similar example to the one we saw in
    the transfer learning video with GPT where we just change the gender of the sentence
    so if you put this man works as aã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Cass the word or this woman man works as a guess the word for manã€‚ we get like
    some kind of neutral jobsã€‚Probably moree stere stetypeï¼Œ stereotypically maleï¼Œ
    for womenã€‚ we get very stereotypically female jobs and prostituteã€‚ which is necessarily
    something we would want our model to output as one of the top five possibilitiesã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: So and that model was not trained on data but was particularly flagged as problematicã€‚
    more like usually considered neutralï¼Œ it's only Wikipedia and a corus of un publishedlish
    bookã€‚So you have to be like GBTï¼Œ for instanceï¼Œ is more known because it's been
    trained of kind of the wall of the internet according to editit usersã€‚ which is
    considered a little bit more sexist orã€‚Oxyenophobic or thing like that so you
    have to be very careful because that bias is in the pretrained model so this is
    done with the bird paste and case checkpoint specifically and completely reproducible
    because there is no randomness in the prediction so if you're go a notebook on
    tris you're going to get those resultsã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: So this is present in any model in some way or novel and it's also going to
    persist after your fine tuã€‚ so you have always have to be super careful that when
    you fine your model in your training data there are going to be enough samples
    of the outputs you'd like to see and you should always roll out your model into
    production by taking some care to analyze the results that you get and potentially
    if you see that you have some predictions you would like to avoid try to correct
    your training data to add more samplesã€‚
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œè¿™åœ¨ä»»ä½•æ¨¡å‹ä¸­éƒ½æ˜¯å­˜åœ¨çš„ï¼Œæˆ–è€…è¯´æ˜¯æ–°çš„ï¼Œå¹¶ä¸”åœ¨ä½ çš„å¾®è°ƒåä¹Ÿä¼šæŒç»­å­˜åœ¨ã€‚å› æ­¤ä½ æ€»æ˜¯å¿…é¡»éå¸¸å°å¿ƒï¼Œå½“ä½ åœ¨è®­ç»ƒæ•°æ®ä¸­å¾®è°ƒæ¨¡å‹æ—¶ï¼Œå¿…é¡»ç¡®ä¿ä½ å¸Œæœ›çœ‹åˆ°çš„è¾“å‡ºæœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œå¹¶ä¸”ä½ åº”è¯¥å§‹ç»ˆåœ¨å°†æ¨¡å‹æŠ•å…¥ç”Ÿäº§æ—¶è®¤çœŸåˆ†æä½ å¾—åˆ°çš„ç»“æœï¼Œå¦‚æœä½ çœ‹åˆ°ä¸€äº›é¢„æµ‹ä½ å¸Œæœ›é¿å…çš„ï¼Œå°è¯•çº æ­£ä½ çš„è®­ç»ƒæ•°æ®ä»¥æ·»åŠ æ›´å¤šæ ·æœ¬ã€‚
- en: To add more samples that would correct the bias of your modelã€‚And that's it
    for chapter oneã€‚ let me see if there are more questionsã€‚In the chat otherwise
    we're going to be ready to tune out so once you have finished this video don't
    hesitate to take a quiz I'm not going to do that on the video to be sure that
    you understood all the terms that were all the content sorry but we saw in this
    chapter there are two left sessionsã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ æ›´å¤šæ ·æœ¬ä»¥çº æ­£ä½ æ¨¡å‹çš„åå·®ã€‚è¿™å°±æ˜¯ç¬¬ä¸€ç« çš„å†…å®¹ã€‚è®©æˆ‘çœ‹çœ‹è¿˜æœ‰æ²¡æœ‰æ›´å¤šé—®é¢˜ã€‚åœ¨èŠå¤©ä¸­ï¼Œå¦åˆ™æˆ‘ä»¬å°±å‡†å¤‡ç»“æŸäº†ï¼Œæ‰€ä»¥ä¸€æ—¦ä½ å®Œæˆäº†è¿™ä¸ªè§†é¢‘ï¼Œä¸è¦çŠ¹è±«ï¼Œå»åšä¸ªæµ‹éªŒï¼Œæˆ‘ä¸ä¼šåœ¨è§†é¢‘ä¸­è¿›è¡Œï¼Œä»¥ç¡®ä¿ä½ ç†è§£äº†æ‰€æœ‰æœ¯è¯­å’Œå†…å®¹ï¼Œä½†æˆ‘ä»¬åœ¨è¿™ä¸€ç« ä¸­çœ‹åˆ°çš„è¿˜æœ‰ä¸¤ä¸ªå·¦ä¾§ä¼šè®®ã€‚
- en: That you can find again on the forumã€‚ This is not the forum anymoreã€‚There are
    two live session next week for Cha 2ï¼Œ be sure to try and wide on the other so
    there's going to be one with me and next Wednesday hopefully all the technical
    issues will be resolved because I'm going to take a little bit of care to make
    sure it works a little bit better especially for the videos and soã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨è®ºå›ä¸Šå†æ¬¡æ‰¾åˆ°ã€‚è¿™å·²ç»ä¸æ˜¯è®ºå›äº†ã€‚ä¸‹å‘¨æœ‰ä¸¤ä¸ªå…³äºç¬¬2ç« çš„ç›´æ’­ä¼šè®®ï¼Œè¯·åŠ¡å¿…å°è¯•å‚ä¸ï¼Œå¦å¤–ä¸€ä¸ªä¼šæ˜¯æˆ‘ï¼ŒæœŸå¾…ä¸‹å‘¨ä¸‰å¸Œæœ›æ‰€æœ‰æŠ€æœ¯é—®é¢˜éƒ½èƒ½å¾—åˆ°è§£å†³ï¼Œå› ä¸ºæˆ‘ä¼šç¨å¾®å…³æ³¨ä¸€ä¸‹ï¼Œç¡®ä¿å®ƒè¿ä½œå¾—æ›´å¥½ï¼Œå°¤å…¶æ˜¯è§†é¢‘æ–¹é¢ã€‚
- en: The first one is going to be with Lewis on morning European time on Wednesday
    and the second one is going to be on next first day at the same time as this oneã€‚Take
    the time to create an account there if hub to play with the hopefully the bug
    in codeab is going to be sold very soon and you can play with also code on that
    platformã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªä¼šè®®å°†æ˜¯ä¸Lewisåœ¨æ¬§æ´²æ—¶é—´å‘¨ä¸‰çš„æ—©æ™¨ï¼Œç¬¬äºŒä¸ªä¼šè®®å°†åœ¨ä¸‹å‘¨å››çš„åŒä¸€æ—¶é—´è¿›è¡Œã€‚è¯·èŠ±æ—¶é—´åœ¨é‚£é‡Œåˆ›å»ºä¸€ä¸ªè´¦æˆ·ï¼Œå¦‚æœhubä¸Šæœ‰é—®é¢˜ï¼Œå¸Œæœ›codeabçš„bugèƒ½å¾ˆå¿«è§£å†³ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨é‚£ä¸ªå¹³å°ä¸Šè¿›è¡Œä»£ç å®éªŒã€‚
- en: otherwise you can always useã€‚The inference API on the website and also think
    about a project that you'd like to runã€‚And to try your skills on as you learn
    more using the courseã€‚ so of anything related to text classificationï¼Œ because
    that's what will focus in this part of the courseã€‚ but try to think of yeah what
    kind of model you would like to build doing something that you could then share
    with the community and then in the next fews will help you with the course to
    do that in practiceã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ä½ æ€»æ˜¯å¯ä»¥ä½¿ç”¨ç½‘ç«™ä¸Šçš„æ¨ç†APIï¼Œå¹¶è€ƒè™‘ä¸€ä¸ªä½ æƒ³è¦è¿è¡Œçš„é¡¹ç›®ã€‚åœ¨ä½ å­¦ä¹ è¯¾ç¨‹çš„è¿‡ç¨‹ä¸­ï¼Œå°è¯•ä½ çš„æŠ€èƒ½ï¼Œå› æ­¤ä¸æ–‡æœ¬åˆ†ç±»ç›¸å…³çš„ä»»ä½•å†…å®¹ï¼Œå› ä¸ºè¿™éƒ¨åˆ†è¯¾ç¨‹å°†é›†ä¸­äºæ­¤ã€‚ä½†è¯•ç€æƒ³æƒ³ä½ æƒ³æ„å»ºä»€ä¹ˆæ ·çš„æ¨¡å‹ï¼Œåšä¸€äº›ä½ å¯ä»¥åˆ†äº«ç»™ç¤¾åŒºçš„äº‹æƒ…ï¼Œç„¶ååœ¨æ¥ä¸‹æ¥çš„å‡ å‘¨å†…ï¼Œè¿™å°†å¸®åŠ©ä½ åœ¨å®è·µä¸­å®Œæˆè¯¾ç¨‹ã€‚
- en: Let me check when last time we got questionsã€‚Andã€‚Yeahï¼Œ I think that's kind of
    wholeã€‚Okayã€‚ thanks a lot for following and see you next weekã€‚![](img/1b5e5fce582c60e42117fe3a72e59e6a_46.png)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘æ£€æŸ¥ä¸€ä¸‹ä¸Šæ¬¡æˆ‘ä»¬æœ‰ä»€ä¹ˆé—®é¢˜ã€‚å—¯ï¼Œæˆ‘æƒ³è¿™å¤§æ¦‚å°±æ˜¯å…¨éƒ¨äº†ã€‚å¥½çš„ï¼Œéå¸¸æ„Ÿè°¢ä½ çš„å…³æ³¨ï¼Œä¸‹å‘¨è§ï¼![](img/1b5e5fce582c60e42117fe3a72e59e6a_46.png)
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_47.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_47.png)'
- en: å—¯ã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚
