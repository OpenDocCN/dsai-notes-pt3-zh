# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P14ÔºöL14- AdaBoost - ShowMeAI - BV1wS4y1f7z1

HeyÔºå guysÔºå welcome to another machine learning from Sc tutorial„ÄÇ Today„ÄÇ

 we are going to implement the addda boost algorithm using only Nmpy and built in Python modules„ÄÇüòä„ÄÇ

A a boost uses the boosting approachÔºå which follows the simple idea to combine multiple weak classifier into one strong classifier„ÄÇ

 And this approach works really well in practice„ÄÇ So let's start with a theory before we jump to the code„ÄÇ

 So let's have a look at this 2 D example here to understand the concept„ÄÇ

 So here we have our samples with only two different features on the X axis and on the Y axis„ÄÇ

 And now the first classifier makes a split based on the Y axis in this example„ÄÇ

 So it draws a horizontal decision line at some threshold„ÄÇ So the dashed line that we can see here„ÄÇüòä„ÄÇ

And we can see that some predictions are correctÔºå but we also have misclassifications„ÄÇ

 And now with these misclassificationsÔºå we can then calculate a performance measure„ÄÇ

 So the accuracy for this classifier„ÄÇ And with this measure„ÄÇ

 we calculate and update weights for all training samples„ÄÇ And now the second classifier comes in„ÄÇ

 and it uses these weights and finds a different and possibly better decision boundary„ÄÇ

 So the second classifier in this example here chooses a feature on the x axis and draws a vertical line„ÄÇ

 And then againÔºå we calculate the performance and update the weights„ÄÇ

 And then we repeat the step for as many classifiers as we want„ÄÇüòäÔºåAnd then here at the very end„ÄÇ

 we have all the different decision linesÔºå and we also have all the different classifier performances„ÄÇ

 and then we combine all our classifiers so we can make a weighted sum with the calculated performances„ÄÇ

 And this allows us to draw the perfect decision line that we can see here„ÄÇ

 which can be more complex than a simple linear decision line„ÄÇüòä„ÄÇ

And the idea with the way that some here he at the end means that the better the classifier is the more impact it has for the final outcome„ÄÇ

 So this is basically the concept„ÄÇ And now let's look at all the different steps and also the math behind this in detail„ÄÇ

So the first thing that we need is a weak classifier„ÄÇ and this is also called weak learner„ÄÇ

So a weak learner is always a very simple classifier„ÄÇ And in the case of the addda boost„ÄÇ

 we use a so called decision stamp for this„ÄÇ So a decision stamp is basically a decision tree with only one split„ÄÇ

 So what we can see here„ÄÇ So we look at only one feature of our samples and only at one threshold„ÄÇ

 And then based on if our feature value is greater or smaller than the threshold„ÄÇ

 We say that it is class -1 or class plus one„ÄÇüòäÔºåSo this is the decision stump„ÄÇ

 And then we need the formula for the error„ÄÇ So the first time„ÄÇ

 the very first time during our iterationÔºå the error is calculated as the number of misclassifications divided by the total number of samples„ÄÇ

 And this is the natural approach for the error„ÄÇ So if we have a look at our example again„ÄÇ

 Then we can see that we have 10 samples in this case„ÄÇ and in the first in the first classifier„ÄÇ

 we have three misclassifications„ÄÇ So this means that our error rate is 0„ÄÇ3 or 30%„ÄÇ

So this is the first time„ÄÇ But the next time we also want to take into account the weights„ÄÇ

 So if a sample was mis misclassifiedÔºå we give it a higher weight for the next iteration„ÄÇ

 And this means that our formula is then calculated„ÄÇ

As the sum over the weights for all misclassifications„ÄÇ And if our error is greater than 05„ÄÇ

 we simply flip the error„ÄÇ So we flip all decisionÔºå all decisionsÔºå and we also flip our error„ÄÇ

 So it is then1 minus the error„ÄÇSo this is the error„ÄÇ and now we need the weights„ÄÇ

 So the weights are initially set to one over n for each sample„ÄÇ

 And this also matches the error calculation in the first step„ÄÇ

 So if we say we calculate the error as the sum over all misclassified weights„ÄÇ

 and we also say that each weight is one over n in the beginning„ÄÇ

 then it is equal to the number of misclassifications divided by the number of samples like here„ÄÇSo„ÄÇ

 yeahÔºå that's why the initial weights are one over n for each sample„ÄÇ

And then we also need the update rule that is defined here„ÄÇ

 So we have the old weight times the exponential function of minus alpha times the actual y times h of x„ÄÇ

 where h of x is our prediction„ÄÇAnd alpha is the accuracy of the classifier„ÄÇ So if this is-1„ÄÇ

 we have a misclassification„ÄÇ and if this is plus one hereÔºå then we have a correct classification„ÄÇ

 and this whole formula basically makes sure that misclassified samples have a higher impact for the next classifier„ÄÇ

So yeahÔºå this is what you should remember from the weights„ÄÇAnd now the performance„ÄÇ

 So we need to calculate the performance or alpha for each classifierÔºå and we can do this„ÄÇ

 and we need this for the final prediction thenÔºå and the formula for the performance is calculated as this„ÄÇ

 So it's 0„ÄÇ5 times the lock of one minus the error divided by the error„ÄÇ

So let me make this a little bit larger for you„ÄÇSo this is the performance„ÄÇ

And our error is always between 0 and1„ÄÇ So I plotted alpha for different errors in this range here„ÄÇ

 And we can see that it is equally distributed somewhere between a positive value here and a negative value here„ÄÇ

So with a low errorÔºå we have a high positive valueÔºå and with a high error here close to one„ÄÇ

 we have a high negative value„ÄÇ So since we are flipping the decision then this will then be correct classifications again with a high contribution to the negative side„ÄÇ

 So the side hereÔºå where the class is  -1„ÄÇ So this is the concept of the alpha„ÄÇ

 And now we need the prediction So now if if we have understood all of this„ÄÇ

 then the final prediction is very easy to understand„ÄÇ So we just choose the sign here„ÄÇ

 the sign of the sum over all predictions where we weigh each prediction with the performance of the classifier„ÄÇ

 So alpha times the prediction here„ÄÇSo the better our classifier„ÄÇ

 the more impact it has for the final prediction„ÄÇAnd the better the classifier„ÄÇ

 the more it points into the negative or positive side„ÄÇ

 And then we take the better side as prediction for our class„ÄÇ SoÔºå yeah„ÄÇ

 that's the concept of the prediction„ÄÇAnd it can be a bit confusing with the different formulas and the side flipping„ÄÇ

 but the basic concept is not so difficult„ÄÇ And let's summarize all the different training steps that we must do in the code„ÄÇ

 SoÔºå first of allÔºå we initializeÔºå initialize over weights for each sample and set the value to one over N„ÄÇ

 Then we choose the number of weak learners we want„ÄÇ And then we iterate over this„ÄÇ

 And then we train each decision stampump„ÄÇ So we do a greedy search to find the best split feature and the best split threshold„ÄÇ

Then we calculate the error for this decision stump„ÄÇ So this is with the formula„ÄÇ

 the sum over the misclassified weights„ÄÇ Then we also flip the error and the decision if it is greater than 05„ÄÇ

Then we calculate the alpha with the formula„ÄÇAnd then we need the predictions„ÄÇ

 And then with the predictions and the alphaÔºå we can then calculate„ÄÇ We can then update the weights„ÄÇ

So this is what we must do in the code now„ÄÇ And yeah„ÄÇ

 I promise you that since now that we have all the formulas and all the training steps here„ÄÇ

 the implementation is pretty straightforward and should not be so hard„ÄÇ So let's jump to the code„ÄÇ



![](img/736cbfb71e784efea36f6f12e912a093_1.png)

So the first thing we do is import Ny„ÄÇ So import Ny S and P„ÄÇ

 And this is the only module that we're gonna to need„ÄÇ

And now we create a class for the decision stampump„ÄÇ So class decision stampump„ÄÇ

And this gets an in it„ÄÇ So define an in it„ÄÇ And this only has self„ÄÇ

 And here we want to store a couple of things„ÄÇ So the first thing that we want to store is the so called polarity„ÄÇ

 So self dot polarity equals one„ÄÇ And this tells us if the sample should be classified or as-1 or plus one for the given threshold„ÄÇ

 So if we want to look at the right or the left side„ÄÇ

 And this is needed because if we want to flip the errorÔºå then we also must flip the polarity„ÄÇ

 So this gets clearer in a second„ÄÇAnd now the second thing that we want to store here is the feature index„ÄÇ

 So self dot feature index equals none in the beginning„ÄÇ And we also want to store the threshold„ÄÇ

 So the split threshold self dot threshold equals none in the beginning„ÄÇ

 And we also want to store the variable for the performance„ÄÇ So the alpha„ÄÇ

So we say self dot alpha equals none„ÄÇ So this is the things that we want to store„ÄÇ

 And then we also define a predict method for the decision stampump„ÄÇ

 So we say define predict and it gets self and it gets x„ÄÇ So the samples that it should predict„ÄÇ

 And now what we want to do here isÔºå simply look at only one feature of this sample and then compare it with the threshold and say if it's smaller than it's  minus-1 and otherwise it's plus one„ÄÇ

 So that's the whole concept of the decision stampump„ÄÇ So let's do this„ÄÇ

 So let's say the number of samples equals X dot shape0„ÄÇ and then let's get only this feature„ÄÇ

 So let's say x column equals x„ÄÇ

![](img/736cbfb71e784efea36f6f12e912a093_3.png)

![](img/736cbfb71e784efea36f6f12e912a093_4.png)

And then we can use a colon„ÄÇ so we still want all the samples„ÄÇ

 but only this feature index that we calculate later during the training„ÄÇ So self dot feature index„ÄÇ

 and now we make our predictions„ÄÇ So we say predictions equals„ÄÇ and by defaultÔºå we say this is one„ÄÇ

 So let's say nuy ones with the size of the number of samples„ÄÇ And then we must check the polarity„ÄÇ

 So we say if self dot polarity equals equals one„ÄÇ So this is the default case„ÄÇ

 then we say that all the predictions that are smaller where the feature vector is smaller than the threshold than it's minus-1„ÄÇ

SoÔºå let's sayÔºå predictions„ÄÇAnd then at these indexes„ÄÇ

 where x column is smaller than self dot threshold Then these predictions are -1„ÄÇ

 And in the other caseÔºå else„ÄÇ So if our polarity is -1„ÄÇ

 then we want to do it exactly the other way around„ÄÇ So let me copy this„ÄÇ

 But we want to say if the x value is greater than our threshold„ÄÇ then these are the -1 predictions„ÄÇ

SoÔºå yeahÔºå this is the all that our the decision stump is doing„ÄÇ

 and then we can return the predictions„ÄÇSo this is the class for the decision decision stamp„ÄÇ

 And now we need a class for the actual add boost algorithm„ÄÇ So let's say class add boost„ÄÇ

And let's make this a small letter„ÄÇ And now we need a in it first„ÄÇ So define a in it„ÄÇ

 And this gets self„ÄÇ and the only parameter it gets is the number of classifiers that we want„ÄÇ

 So let's say N CÔºå L F equals5 by default„ÄÇ And then in the in itÔºå we store this number„ÄÇ

 So we say self dot N CÔºå L F equals the number of classifier„ÄÇ And then as always„ÄÇ

 we want to implement the fit and the predict method„ÄÇ So let's start with the fit method„ÄÇ

 So let's say define fit„ÄÇ and it has self and it has X and y„ÄÇ So the training samples and the labels„ÄÇ

And nowÔºå the first thing we do is to get the shape of this vector„ÄÇ

 So the number of samples and also the number of features„ÄÇFeatures equals x dot shape„ÄÇ

And then we want to inÔºå initialize our weights„ÄÇ So in it the weights„ÄÇ And as I said„ÄÇ

 all the weights for each sample is set to one over n in the beginning„ÄÇ So let's say w equals nuy„ÄÇ

 And then we can use a a method from numpy that is called full„ÄÇ So nuumpy full„ÄÇ

 and it gets the size number of samples„ÄÇ And then it gets an initial value„ÄÇ And here we say one„ÄÇ

Over the number of samples„ÄÇ So this sets each value to this calculated value„ÄÇAnd then„ÄÇ

 this is our initialization„ÄÇ And now let's iterate through all the classifiers and do the training„ÄÇ

 So firstÔºå we create a list where we want to store all the classifiers„ÄÇ So let's say self dot C„ÄÇ

 L F's„ÄÇüòäÔºåËØ∂„ÄÇAnd this is an empty list in the beginning„ÄÇ And now let's do the iteration„ÄÇ

 So let's say4 underscore in range„ÄÇ And here we have the number of classifiers that we specify„ÄÇ

 So self dot NÔºå CÔºå LÔºå F„ÄÇAnd now what we want to do here is we want to do the greedy search„ÄÇ

 So we want to iterate over all the features and all the threshold„ÄÇ

 So this is similar to the decision tree implementation that I did in another tutorial„ÄÇ

 And I recommend that you check that outÔºå too„ÄÇ So we want to do a similar thing here„ÄÇ So„ÄÇFirst„ÄÇ

 we create our classifier„ÄÇ So let's say C LF equals decision stump„ÄÇ

 And now let's define a min error in the beginning„ÄÇ So we want to find the best feature value„ÄÇ

 the split feature and the split threshold where this error then is minimum„ÄÇ So in the beginning„ÄÇ

 we just say this is float„ÄÇInÔºå so this is a very high number„ÄÇ

 And now let's iterate over all the features„ÄÇ So let's say for feature„ÄÇFor feature„ÄÇ

 I in the range of„ÄÇAnd here we have the number of features that we got in the beginning„ÄÇ

And then we want to get only this feature„ÄÇ So let's say X column„ÄÇ

 So this is similar to what we did here„ÄÇSo we can do the same thing and say x column„ÄÇEquals this„ÄÇ

 So all the samplesÔºå but only this feature index„ÄÇ So I call it feature IÔºå in this case„ÄÇ

And then we get one to get only the unique values„ÄÇ And these are our thresholds„ÄÇ

 So let's say thresholds equals Nai dot unique„ÄÇAnd hereÔºå the unique values of our column„ÄÇ

 So x column„ÄÇ And now we iterate over all the thresholds„ÄÇ So let's say for threshold in„ÄÇThresholds„ÄÇ

And now what we want to do is we want to predict with the polarity 1 first and then calculate the error with the formulas that I showed you in the beginning„ÄÇ

 So let's say our polarity equals  one„ÄÇ And then let's do the predictions„ÄÇ So predictions equals„ÄÇ

 And this is similar to what we did here„ÄÇ So in the beginningÔºå justÔºå it's just one„ÄÇ

And then we use this formula here„ÄÇ So since our polarity is  one„ÄÇ

 we have to compare it by saying if it's smaller than our threshold„ÄÇ

 So predictions where our column valueÔºå our feature value is smaller than our threshold than there our predictions are -1„ÄÇ

So now we predicted all the samples and now we want to calculate the error and as I said„ÄÇ

 the error is the sum over the weights of the misclassified samples„ÄÇ

 So let's get the misclassified weight„ÄÇ So let's say misclassified equals W and the w where our y our training labels is not equal to the predictions that we just did„ÄÇ

So these are the misclassified weights„ÄÇ And now we want to simply calculate this sum over these weights„ÄÇ

 So arrow„ÄÇEror equals the sum over this misclassified weight„ÄÇ So this is the error„ÄÇ

 And now we also want to flip our error If it is greater than 05„ÄÇ

 So we say if error is greater than 05„ÄÇ We simply say that our new error equals 1 the error and then we also flip the polarity„ÄÇ

 So we say P equals-1„ÄÇSo now we have our error and now we check if our error is smaller than the min error„ÄÇ

 So let's say if error is smaller than the min error„ÄÇThen this is our new min era„ÄÇ

 So we say min error equals error„ÄÇ And now this is the best current fit for our decision stamp„ÄÇ

 So we want to store this„ÄÇ So we say CF dot polarity equals P and sorryÔºå only P„ÄÇ

 and we also want to store the threshold and the feature„ÄÇ

 So CF dot threshold equals the current threshold and C F dot feature index equals feature I„ÄÇ

And yeahÔºå so this is the whole training loop for a classifier„ÄÇ

And now when we are done with post four loopsÔºå what we want to do here is have to check if I'm on the right indent„ÄÇ

So now what we have to do is to calculate the performance„ÄÇ so calculate alpha„ÄÇ

 So we say and CF dot alpha equals„ÄÇ and then we need this formula here„ÄÇ



![](img/736cbfb71e784efea36f6f12e912a093_6.png)

So 05 times the lock of1 minus the error divided by the error„ÄÇ



![](img/736cbfb71e784efea36f6f12e912a093_8.png)

And we also use a little epsilon so that we don't divide by 0„ÄÇ

 So let's say Es equals this small value„ÄÇÂóØ„ÄÇAnd this is our epsilon„ÄÇ And now let's use the formula„ÄÇ

 So 05 times and nuy dot the lock„ÄÇAnd here we have one minus the error„ÄÇAnd then divide by here„ÄÇ

 let's say error plus our epsilon„ÄÇ And let's wrap this in another parenthses„ÄÇ So this one„ÄÇ

And let me check if this is correct„ÄÇ So let's do another one around this one here„ÄÇAnd then„ÄÇ

 this should be fine„ÄÇSo this is our alpha„ÄÇ And now we want to update the weights„ÄÇ And for this„ÄÇ

 we also need the prediction„ÄÇ So let's check the formula formula again„ÄÇ

 So this is the old weight times the exponential function of minus the alpha that we just calculated times the actual predictions or the actual labels times the predictions„ÄÇ

 And then we normalize it„ÄÇ So this is the formula that we need„ÄÇ So let's write this here„ÄÇ

 And let's first get the predictions„ÄÇ So we can say predictions equals„ÄÇ

 and we already implemented this so we can simply say CF dot prediction„ÄÇ



![](img/736cbfb71e784efea36f6f12e912a093_10.png)

![](img/736cbfb71e784efea36f6f12e912a093_11.png)

X„ÄÇXÔºå yeah„ÄÇ So we get the column up here„ÄÇ so we can put the whole x here„ÄÇ

 And now we have the predictionsÔºå and now we can use them and update the weights„ÄÇ

 So we say our weights is multiplied equals„ÄÇ And then we say nuy X„ÄÇ So the exponential function„ÄÇ

 And then minus C F dot alpha„ÄÇTimesÔºå and here the actual labels„ÄÇ and then times the predictions„ÄÇ

The timesÔºå predictions„ÄÇSoÔºå and then we want to normalize it„ÄÇ

 So we divide it by the sum over this weight„ÄÇ So we say w divided equals„ÄÇ

 And then here we say nuy dot sum W„ÄÇAnd now we are done„ÄÇ So we updated our weights„ÄÇ

 and then we want to store this classifier„ÄÇ So we want to save it„ÄÇ So we say self dot CÔºå L„ÄÇ

 F dot append the current classifier„ÄÇ So we append CÔºå L F„ÄÇAnd now we are done with the fit method„ÄÇ

 So this is the whole training of our add boost classifier„ÄÇAnd„ÄÇNowÔºå what we also need isÔºå of course„ÄÇ

 we want to have the predict method„ÄÇ So let's implement this down here„ÄÇ So let's say defineÔºå predict„ÄÇ

 and here it gets self and it also gets X„ÄÇ And now this is the formula that I showed you here„ÄÇ

 So we look at the s of the sum„ÄÇ And here we multiply each alpha with the with the prediction„ÄÇ Yeah„ÄÇ

 So let's do this„ÄÇ

![](img/736cbfb71e784efea36f6f12e912a093_13.png)

![](img/736cbfb71e784efea36f6f12e912a093_14.png)

So let's say CÔºå L F dot Prats equals„ÄÇ And here we use list comprehension„ÄÇ

 And then we do this for each of the classifiers„ÄÇ So we say CÔºå L F dot alpha times„ÄÇ

 And here we use the predictions CÔºå L F dot predict„ÄÇ And here we want to predict X„ÄÇ

 And we want to do this for each of the start classifiers„ÄÇ So we say for CÔºå L F in self dot CÔºå L Fs„ÄÇ

So these are all the predictions in the sum„ÄÇ And now we need to calculate the sum„ÄÇ

 So we say y pre equals and then nuy dot sum„ÄÇ And here we say CÔºå L F pres and along the axis 0„ÄÇ

 So now we have the sum„ÄÇ And now the very last thing that we need to do is to look at the s„ÄÇ

 So we say y pret equals nuy dot s sign of thisÔºå Y pret„ÄÇAnd this is our final prediction„ÄÇ

 And then we can return this„ÄÇ So let's return why pret„ÄÇAnd now we should be done„ÄÇ

 So now we have the fit method and the predict method„ÄÇ

And now here I've already written a little test script„ÄÇ

 So here I import this class that we just created„ÄÇ So from addda boost to import adddabu„ÄÇ

Then I also have a accuracy measure here„ÄÇ And then in this example„ÄÇ

 we load the breast concert data set from the SQ learnide assets„ÄÇ

 and then the important thing that we must do here is to set all the labels that are 0 at the moment to-1 because add boost needs the labels as-1 and plus1„ÄÇ

 and then we do a train test split as alwaysÔºå And then here we create a add boost classifier„ÄÇ

 and in this caseÔºå I put in five classifier„ÄÇ Then we call the fit method„ÄÇ

 and then we call the predict method and then we calculate the accuracy„ÄÇSo this is the test script„ÄÇ

 So let's run this and hope that everything's working„ÄÇ So let's say Python a boost test„ÄÇ

 it's called and hit enterÔºå and it's runningÔºå and it's calculating„ÄÇAnd I hope that it's working„ÄÇ

 And now we are done„ÄÇ SoÔºå yeahÔºå so here we have a accuracy accuracy„ÄÇ

 and it's pretty good in this example„ÄÇ So we have 094„ÄÇ So we see that it's working„ÄÇAnd yeah„ÄÇ

 I hope you enjoyed the tutorial and see you next timeÔºå bye„ÄÇ



![](img/736cbfb71e784efea36f6f12e912a093_16.png)