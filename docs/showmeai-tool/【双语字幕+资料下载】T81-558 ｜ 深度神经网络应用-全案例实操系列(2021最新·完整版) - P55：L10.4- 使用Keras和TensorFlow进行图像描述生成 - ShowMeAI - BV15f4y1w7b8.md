# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P55ÔºöL10.4- ‰ΩøÁî®KerasÂíåTensorFlowËøõË°åÂõæÂÉèÊèèËø∞ÁîüÊàê - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff Heaton„ÄÇ Welcome to applications of Deep neural networks with Washington University„ÄÇ

 In this videoÔºå we're going to look at an interesting combination of convolution neural networks in LSTM that allows us to caption images„ÄÇ

 to see several things going on in the image and describe it in a sentence for the latest on my AI course and projects„ÄÇ

 Click subscribe in the bell next to it to be notified of every new video„ÄÇ imageage captioning„ÄÇ

 at least where I first saw it was with Andre Carpathy's dissertation„ÄÇ Now„ÄÇ

 we talked about this guy before in the last sectionÔºå some of the LSTM text generation came from him„ÄÇ

 and he did some veryÔºå very interesting work with that as well„ÄÇ

 So a very interesting guy researcher now works for Tesla„ÄÇ

 who created a lot of the code for some of this computer vision software that we're dealing with right now„ÄÇ

 This is an image actually from his dissertationÔºå or at least from his website showing what the captioning does„ÄÇ

üòä„ÄÇ

![](img/2287d406430b06732c757e98a8dcb007_1.png)

Typically before captioningÔºå we would try to just classify something as say a cat„ÄÇ Well„ÄÇ

 there's also a skateboard in here„ÄÇ and there's a few other things as well door and they're just partial pieces„ÄÇ

 later then we got into multi-imaging classification So we'd say cat skateboard kind of like yellow Now we're wanting to actually combine that text generation that we have with the image classification„ÄÇ

 be able to actually write a caption for these„ÄÇ Now we're going to use heavy transfer learning because this would take forever to train this thing from the ground up even with the transfer learning„ÄÇ

 we're going to use a relatively small image set and it's not going to be perfect„ÄÇ

 but it will generate captions that have some that have pretty good meaning for what they're looking at they won't be perfect„ÄÇ

 We'll see that when we use images like I have a set of images„ÄÇ

 the photos directory that's in the Github„ÄÇRepository for this course that are just personal family photos that I use in machine learning„ÄÇ

 you've seen a number of them in this classÔºå some of them are by dog Hickory and these are good tests for that because they're completely outside of imagenet and they're just things that I have that are pictures that I chose because I thought they are interesting from a machine learning aspect that some of them might not be as easy for these neural networks to classify Now the two things that we're going to transfer from are inception V3 and glove Inception v3 that's another one of the many imagenet trained neural networks mobilenet we've used a couple of times in this class up till now I do have it set so that you can use mobilenet instead of inception but for this one inception works better I'll get into what the actual differences when we get down to that part and then glove that is a natural language processing embedding„ÄÇ

We'll be learning more about those in the next module when we get into natural language processing„ÄÇ

 This is essentially how this is going to work„ÄÇ It's actually pretty similar to the text generation that we did in the previous chapter„ÄÇ

 But instead of just generating random sort of nonsense sentences„ÄÇ

 now we're going to actually generate us sentences„ÄÇ

 you can see why we started with the previous part and learned how to just teach the neural networks„ÄÇ

 The basics of grammar and how to string things together„ÄÇ And really„ÄÇ

 we're just guiding them to the knowledge of grammar„ÄÇ We're not actually teaching them grammar„ÄÇ

 believe meÔºå the early days of natural language process was obsessed with teaching the neural networks all about or preprocessing based on grammar rules and codifying the grammar rules„ÄÇ

 and believe meÔºå thats that's very difficult„ÄÇ NowÔºå the idea is to use big data„ÄÇ

 large corpes of text and to be able to let the neural network„ÄÇLearn the language for themselves„ÄÇ

 This is how it works„ÄÇ NowÔºå previouslyÔºå what we would do when we were generating the nonsense sentences is we would put in maybe 20 characters„ÄÇ

 Well now we're not doing this character based„ÄÇ We're doing it word based„ÄÇ

 We could do captions character based„ÄÇ it would require potentially more training„ÄÇ

 and this is just simply the technique I am using at this point„ÄÇ

 if you'd like to try it in character basedÔºå Id definitely encourage you„ÄÇ

 and I'd be curious to see what results you got„ÄÇ you could definitely use this code as a starting point for this„ÄÇ

 but since it is now word basedÔºå we are going to present a vector„ÄÇ

 So this this is going into the neural network„ÄÇ and the individual elements are not characters„ÄÇ

 they're words and we have a special token word called startÔºå this is what kicks the whole thing off„ÄÇ

 So instead of giving it a seed like we did before„ÄÇ since we needed to generate the entire caption„ÄÇ

 we really can't seed it because we don't we don't know how the captions„ÄÇ

start so all captions start with the word startÔºå which is and not even the real word start„ÄÇ

 This is just token that we're using„ÄÇ And then we initially send it just start but here's the trick„ÄÇ

 neural networks can accept many different inputs and multiple things„ÄÇ

 So we're input to this is really very similar to the previous part„ÄÇ

 but we're putting an entire image into it too So there's two inputs that are coming in and we'll see that the Kira's functional API as opposed to sequence is absolutely necessary for this„ÄÇ

 So we use the model and Kira's functional API for Resnet and for a couple of other things so far in this course„ÄÇ

 but we will definitely be using model and functional API for this because this is a fairly complicated neural network are giving it a picture and then we gradually build this up just like we did in the previous part„ÄÇ

 So we pass its start„ÄÇA picture of a dog running in the grass„ÄÇ that's actually my dog„ÄÇ

 And then the neural network takes both of those„ÄÇ and it returns probabilities„ÄÇ And it'll sayÔºå okay„ÄÇ

 I think maybe the nextÔºå the next one is runs„ÄÇ Now this is if we had put in a and dog here„ÄÇ

 So start a dog runs would be the highest probability„ÄÇ Now„ÄÇ

 this is really long because this list of dummy variables is every single word that is in this thing vocabulary„ÄÇ

 we'll see the size of the vocabulary in a moment„ÄÇ but a lot of dummy variables„ÄÇ

 So this is the overall structure of what we're going to build„ÄÇ

 and we're going to train this neural network„ÄÇ obviously using the transferred learning so that we don't have to train this literally from scratch„ÄÇ

 that would be that would be Hughes always use transferred learning if you can„ÄÇ

 I am not going to actually run this code„ÄÇ It takes this probably I would seem to remember about two hours„ÄÇ

3 hours„ÄÇ maybe for to actually train this neural network„ÄÇ So I've got it all„ÄÇ

Ran and the version that is up on Github should be pre-ran with this„ÄÇ

 Now you might see some different captions„ÄÇ if you compare what you're seeing in this video to the actual website„ÄÇ

 because if I rerun thisÔºå this is all stochastic„ÄÇ So you can get you can definitely get different results„ÄÇ

 And I tend to rerun these as the semester progresses as different versions of Tensorflow come out So you'll see different results„ÄÇ

 these are all the imports that you need„ÄÇ the only thing that's somewhat interesting here is these are the start in in tokens„ÄÇ

 So we start it up here with the start token and it keeps adding additional words„ÄÇ

 So we put in just startÔºå it would hopefully give us a then we would give a start and a it would hopefully give us dog then we give a start a and dog and it would hopefully give us runs and it continues„ÄÇ

 eventually it'll give us an in token„ÄÇ So either we hit the wall here and run out of space„ÄÇ

 that'll stop us or we get a in token„ÄÇ So this is how the output of the neural network can be very not fixed length„ÄÇ

If you need a neural network to give you a to generate a sentence or to give you something that's not fixed length„ÄÇ

 This is what you're typically needing to do„ÄÇ You build it piece by piece by piece and let the neural network keep adding another element on it for you Epochs that's simply how many epos we're going train it for we're not using early stopping or anything like that use inception is true if you want to try mobilenet just put false in there doesn't work as well„ÄÇ

 I have my hours minutes string because we we time how long these things take definitely use coab for this„ÄÇ

 the GPU is your friend or if you have your own faster GPU definitely use that„ÄÇ

 You're going to need to download some data sets for this So here I have the path content my drive Now you might need to change this if you're putting your stuff in different locations„ÄÇ

 but you'll need to create the directories for for each of those in that folder„ÄÇ

 you can see from the source code they're all just named this and then you need to create a data directory that's where it's going to create the„ÄÇ

Output files those should all be directly off of captions„ÄÇ

 So unzip these these two and put them there„ÄÇ By the way„ÄÇ

 getting a hold of the flicker8k data set for this can be a little tricky„ÄÇ read the article here„ÄÇ

 there's some copyright questions around that one„ÄÇ So its it's difficult„ÄÇ

 It's not difficult to get a hold of„ÄÇ I can't put it into my Github repository because it it's not mine„ÄÇ

 So if you click that linkÔºå you can find out exactly how to get a hold of it„ÄÇ

 So we're going to clean this data set and begin to process it the data set by the way„ÄÇ

 what this data isÔºå is it's from Flickr„ÄÇ it has 8000 images and captions for them„ÄÇ

 So it's exactly what we need and we need to basically break this up and what we're doing here is we're cleaning up the descriptions„ÄÇ

 We're essentially converting them to lower„ÄÇre for this null punctuation„ÄÇ

 These are punctuations we're removing certain punctuation„ÄÇ We are essentially removing„ÄÇ

Very short words here„ÄÇ We're removing words that do not have alpha alphabetic components to them„ÄÇ

 and we're gradually figuring out the length„ÄÇ So we need to know what that maximum caption size is because that's going to be our sequence length„ÄÇ

 and we gradually build up our dictionary of these„ÄÇ Then we can print out what we collect„ÄÇ

 So look up is essentially the number of unique words and then the number of words in our dictionary and the max length„ÄÇ

 the max caption length„ÄÇ Then we load the glove embeddings„ÄÇ

 Now what the glove embeddings are is essentially vectors for each of the words in the vocabulary„ÄÇ

And each of those vocabulary words is going to have a corresponding vector„ÄÇ

 and those are the features that well put into the neural network to actually do the predictions rather than doing„ÄÇ

 say the index numbers or dummies„ÄÇ This is much better than using a dummy variable for each vocabulary word because there's a lot of vocabulary words and that would be a lot of additional data there„ÄÇ

 We do use dummies for the vocabulary words on the output from the neural network„ÄÇ

 but we do not on the input„ÄÇ The other advantage is those vectors of the wordsÔºå similar words„ÄÇ

 the vectors will be closer to each other in Euclidean space„ÄÇ

 So using linear algebra where you're basically calculating distances between two vectors„ÄÇ

 two similar words will be fairly close in space„ÄÇ we read in all of the image names„ÄÇ

 We're basically getting ready to load everything„ÄÇ We have 6000 images in the training set 1000 in the test„ÄÇ

 and then we build up all of these descriptions So there„ÄÇAre going to start with the start token„ÄÇ

 Then they're going to have the actual words of the caption and they're going to end with stop„ÄÇ

 So this is how we basically start and stop the captioning process as we build the sequence like we saw up in the diagram earlier„ÄÇ

 NowÔºå I have the code here to use inception or to use mobilenet„ÄÇ

 this number in the output dimensionsÔºå that is really the reason why„ÄÇ

 So this turns each of the images„ÄÇ So using inceptionÔºå if you use inception just straight up„ÄÇ

 it would return 1000 probabilities because there's 1000 images in imagenet and each of those images„ÄÇ

 it would give you the probability that the image is one of those„ÄÇ

 We strip that layer off and below that is a 2048 densely connected layer„ÄÇ

 And we use basically those outputs like we saw earlier in feature engineering from transfer„ÄÇ

Learning we use that as feature engineering„ÄÇ so this 2048 vector that comes out out of inception with the top top layers sheared off„ÄÇ

 And that's what we're doing here„ÄÇ we're removing two layers„ÄÇ

 Those become essentially engineered features for those images„ÄÇ And again„ÄÇ

 it it's like with glove that 2048 vector similar images should be closer together in vector space„ÄÇ

 notice how many of the output dimension is here„ÄÇ There's not densely connected layers there because the mobile net is trying to be very compatible with mobile devices and power consumption„ÄÇ

 So your number of dimensions when you shear off those top layers explodes and you've got 50 That's not ideal for feature engineering because that's going to be a very sparse vector„ÄÇ

 at least when I've inspected on many of those will be0 and you won't get quite you just will not get as good a results as using the 2048„ÄÇ

 you're welcome to experiment with it though and the code should all work„ÄÇ

 These are the key constant„ÄÇThat you need to change the height and the width because different transferred neural networks are trained for different image sizes and it prints out a summary„ÄÇ

 This is quite long„ÄÇ It's a long neural network that we transfer in„ÄÇ

 We're going to create the training sets„ÄÇ So for each imageÔºå we need to encode it„ÄÇ

 and we're basically encoding the image to whatever that output dimension size is„ÄÇ

 Now this is what it is„ÄÇ you can't change that constant and change the output size„ÄÇ

 So we're essentially taking the image„ÄÇ we're resizing it to a standard size„ÄÇ

 We're not worried about keeping the aspect ratio„ÄÇ The transferred neural network tends to figure that out„ÄÇ

 We do any preprocessing that the transferred neural network needs„ÄÇ We expand the dimensions here„ÄÇ

 We're essentially taking the long string that these images are loaded in„ÄÇ

 and putting it back into the grid that a image really should be„ÄÇ

 Here is where we perform any preprocessing actually not up here„ÄÇ that's essentially converting„ÄÇ

To an array and then we call the either mobilenet inception to predict„ÄÇ

 that's where it turns into that 2048 vector„ÄÇ and then we reshape it so it's in the right size„ÄÇ

 essentially for prediction or for input into the neural network This is where we generate the training set This is where we call this over and over this can take some time So we actually pickle the training set after we load so we're loading all those JpeEgs or PGs or whatever that image data is and turning them into the 2048 vectors that this thing crunches them down into„ÄÇ

 So this is a lot of image process doesn't take necessarily a tremendous amount of time but it can take a while this is where we process the captions and we get them into a similar structure Now we have 30000 captions because there's up to five different captions provided for each image that's just the way the data is So each image is actually multiple caption which which is kind of nice We're going to get rid of„ÄÇ

Wds that don't occur that often„ÄÇ So our vocabulary drops down to just 1651„ÄÇ That helps a lot„ÄÇ

 And we build up two indexes„ÄÇ each of those 1651 words that we're dealing with„ÄÇ

 which we also add to the tokens„ÄÇ We have one that takes an index number and gives you a word back and a similar one that takes a word and gives you an index back„ÄÇ

 So you've got a double directional sort of dictionary to look these words up in„ÄÇ

 We do add two to the max length that accounts for this start and end token„ÄÇ

 And then this is what it looks like really using the data generator„ÄÇ

 We're going to start would just start„ÄÇ It should so we're calling that neural network multiple times„ÄÇ

 call it would just start it adds a So this is what the training set actually looks like„ÄÇ

 So just one picture„ÄÇ and one caption„ÄÇ The idea here is this training sets going to be gigantic„ÄÇ

 So for each image„ÄÇ This is just one image„ÄÇ This is hickory„ÄÇ my dog running on the grass„ÄÇ

 He's not in the data setÔºå but I'm using him as an example„ÄÇ for this one„ÄÇ

 we would have five different captions of him„ÄÇ We would have maybe like„ÄÇBudog runs on the grass„ÄÇ

 dog running all these different variants of what the caption could be But for each of these„ÄÇ

 this is showing just one caption„ÄÇ We need to generate all the phases of it„ÄÇ

 We need to generate with this image and just start return A with this image start an a return dog with this image„ÄÇ

 start a dog return runs„ÄÇ there are a lot of data in this training set„ÄÇ

 So we're going to use something called a generator to make it not so insane in terms of the RA requirement„ÄÇ

 and then you'd get your second image because the data set has 5Ôºå6000 of theseÔºå So each one of those„ÄÇ

 youd be you'd have to literally duplicate the image in the training set five times for each of the five captions„ÄÇ

 and then each of the captions gets a number of additional entry„ÄÇ

 So you would need to duplicate that picture get again for each of these intermediate places„ÄÇ

 It's a lot of data„ÄÇ So we use a data generator„ÄÇ The data generator is what we're going to„ÄÇ

not generate this big matrix like we did before to pass in„ÄÇ

 And essentially what's going on here is it's looping through all of the keys„ÄÇ

 Those are essentially all of the images that we have„ÄÇ

 And then for each one we're looping through all the description„ÄÇ So there's five of them„ÄÇ

 And then we generate„ÄÇ So these are percent of the dimensions we need to generate one for each picture„ÄÇ

 one for each description„ÄÇ And then one for each combination of those words in there so that we catch all the intermediate form„ÄÇ

 And we're also mindful of how many photos we want per batch that's a training hyperparameter„ÄÇ

 The way that this works is this is this big loop is not being ran just straight out and generate„ÄÇ

 it would just dump a ton of data if it did„ÄÇ That's what the yield command in Python does„ÄÇ

 This is essentially a dynamic collection that you're building„ÄÇ and every time you hit yield„ÄÇ

 It essentially keeps this loop sort of in freeze dry eye mode„ÄÇ So it just freezes it„ÄÇ

 And lets the program go on with what it's doing„ÄÇ And then the next time this gets„ÄÇ

CalledIt goes right back to here and restarts the loops exactly where they were and returns„ÄÇ

 returns it„ÄÇ These are the glove embedding so that we have those available so that we can turn those words into the 200 per word„ÄÇ

 You have to set that constant„ÄÇ You can't change that that is fixed by glove„ÄÇ

 And then we essentially just build the inputs for all of those caption words that we have and look them all up„ÄÇ

 And you can see then essentially the shape of this„ÄÇ So we're using a embedding layer„ÄÇ

 We have 1652 words„ÄÇ and each of those 1652 words has 200 elements„ÄÇ

 Those 200 elements are the vectors that glove turns each of the words into so that similar words will be closer in vector space„ÄÇ

 This is using something called akira's embedding layer„ÄÇ

 We will learn more about this in the next module when you get an NLP„ÄÇ that these are great„ÄÇ

 This lets you do this whole lookup inside of ks and let ks do it internally„ÄÇ

 This is what the neural network looks like„ÄÇüòäÔºåSo input1 is going to be your image input2 is that gradually increasing caption that you're going to send in each time„ÄÇ

 This is where the embedding layer comes in„ÄÇ It uses the vocabulary size and it uses this embedding matrix that we had created that essentially becomes the weights of that layer we'll see that we transfer this in in a moment When we create it here we don't transfer it in„ÄÇ

 it's initialized with random weightsÔºå but it's essentially a matrix of this size so that it can do those lookups for you„ÄÇ

 it'll look up each of those words and put in the correct 200 values„ÄÇ

 we've got some dropout layers going on and we've got a 256 LSTM really very similar to the type of LSTM layer that we used for O text generation in the previous part we set up we basically add these to the neural network we had a final dense 256 layer and then the final output layer is going to be the vocab size because you've got dummy variables coming out of„ÄÇ

EssentiallyÔºå then we create a model so this is using the kas functional API so that we can have the inputs„ÄÇ

 we can have multiple inputsÔºå inputs one and inputs multiple inputs here when I'm talking about it is like maybe three pictures coming in or one picture in a caption or three pictures in a caption who knows however you want to set it up„ÄÇ

 This is the summary of our network that I just described„ÄÇ This is very important„ÄÇ

 This is where we're basically taking that embedding matrix from glove and putting that we are just putting that right into the weights of the neural network„ÄÇ

 So we're overr the weights of the neural network„ÄÇ and it becomes a lookup for us„ÄÇ

 whenever it sees word 5„ÄÇ for exampleÔºå itll go to the column row„ÄÇ

 depending on on the orientation that matrix and place that into the feature vector„ÄÇ

 we can pile the entire neural network for categorical cross entropy because it is a classification neural network„ÄÇ

 We're now going to train it the batch size of three„ÄÇDo I got this from some of the original papers„ÄÇ

 We're basically doing it„ÄÇ So we have those 10 epochs„ÄÇ

 We are going to do 20 epochs at this learning rate and then a final epoch„ÄÇ

 we're sort of decreasing the learning rate„ÄÇ we could also use a scheduler for this„ÄÇ

 but this is pretty pretty straightforward„ÄÇ Now we do save the neural network„ÄÇ

 If we see it already existsÔºå we just load it„ÄÇ we don't rebuild it because this part's going to take a bit of time„ÄÇ

 NowÔºå when we need to actually generate the captionÔºå this is the function that's going to do it„ÄÇ

 This is actually somewhat similar to the function that we had in the previous part where we were just generate random text„ÄÇ

 it's going to go in a range up to the max length that is going to build a sequence essentially just with the just with the start tag starting„ÄÇ

 it is going to pad the sequence because it has to go to the to the end„ÄÇ essentially with with zeros„ÄÇ

 We're going to request a prediction„ÄÇ Our max gets us of those„ÄÇThose predictions„ÄÇ

 which of them is has the highest probabilityÔºå because that's the word that we're going to add„ÄÇ

 Then we add a space to it„ÄÇ If we've gotten the stop tokenÔºå then we stop and we continue„ÄÇ

 and then finallyÔºå we split this out and return a textual string that tells us essentially what the caption was„ÄÇ

 Now if we call this and evaluate it„ÄÇ These are some actual results„ÄÇ

 So you see these two people riding on a bike together„ÄÇ

 It says man and white shirt is standing by a woman in a blue hat„ÄÇ Okay close„ÄÇ

 it had a decent idea what's going on„ÄÇ If we look at some more of theseÔºå another inside of here„ÄÇ

 There is a dog being barraged by tennis balls„ÄÇ dog is chasing a ball„ÄÇ OkayÔºå yeah„ÄÇ

 he's kind of jumping at it„ÄÇ I'll buy that„ÄÇ There' is a dog on concrete or maybe snow„ÄÇ

 Two dogs are running through grass„ÄÇ OkayÔºå new is a dog„ÄÇ

 black and white dogs running through the snow„ÄÇ This is the most spot on one that I've seen yet of the three„ÄÇ

 So that's very good„ÄÇ AgainÔºå these are not perfect„ÄÇ

Would have to expend considerably more training and probably get a bigger data set to really get these a lot a lot better„ÄÇ

 Look like two guys walking with some strange graffiti on the ground„ÄÇ

 man in black coat is standing next to women in black jacket fairly close„ÄÇ

 It wasn't figuring out the genders„ÄÇ but it got the gender right on one guy„ÄÇ

 two kids playing on a trampoline„ÄÇ little boy in red shirt is jumping off of a swing„ÄÇ I can get that„ÄÇ

 maybe he's maybe there's a swing back there„ÄÇ but I did rerun this a few times and it was picking up on the trampoline„ÄÇ

 So that's kind of neat that it can see that sometimes two women in a bikini near a shoreline„ÄÇ

 So group of young people„ÄÇ Okay I'll buy that are climbing up rock into the water„ÄÇ

 So you can see it's figuring out sees rocks it sees water„ÄÇ

 It's neat that it's getting the grammar right into the water„ÄÇ

 so I mean it's putting articles in front of things like it should„ÄÇ

 It's a dog dog is running through grass„ÄÇ So okay on these pictures that it was„ÄÇNow„ÄÇ

 it wasn't trained with these„ÄÇ These are from the same set„ÄÇ This is from the test set„ÄÇ

 If you evaluate them on some of my photosÔºå which are here from from Github„ÄÇ

 it doesn't do quite as well„ÄÇ NowÔºå this is kind of mean„ÄÇ

 but this is what you do to test these things out and to show the limitations„ÄÇ

 I am standing next to this is at a university in Florida„ÄÇ

 this is actually the university that I graduated with my doctorate that„ÄÇ And this that's a tardis„ÄÇ

 if you've ever watched Doctor„ÄÇ HoÔºå that is that's a whole thing in Dr„ÄÇ HoÔºå But it's a phone booth„ÄÇ

 probably didn't have any tardiss let maybe phone booths and it' training set„ÄÇ So man in black shirt„ÄÇ

 it's actually a blue shirtÔºå but I blame the camera for that„ÄÇ and jeansÔºå it's seen my jeans„ÄÇ

 So that's cool on the street„ÄÇ OkayÔºå it probably thought these things usually occur on a street„ÄÇ

 but it was actually inside of a buildingÔºå but that's actually pretty good„ÄÇ

 This is me sitting there man in black shirt„ÄÇNot even close„ÄÇ and tie„ÄÇ

 I only wear a tie if I am forced to cast drink„ÄÇ I amÔºå I'm not drinking„ÄÇ This is my mother's dog„ÄÇ

 Two dogs are fighting in the grass„ÄÇ I don't know the dog maybe a split personality going on Now I noticed in the flickr data set„ÄÇ

 a lot of them had people doing actions„ÄÇ There's no people in here„ÄÇ

 This is a bed and breakfast that my wife and I visitedÔºå and we just took a picture of it„ÄÇ

 So it's completely just a landscape shot„ÄÇ There's no workers„ÄÇ there are steps here„ÄÇ

 So maybe this is my wifeÔºå I and my dog man in a red shirt„ÄÇ

 So it's talking about my wife who is not a man„ÄÇ She is a woman is sitting on a stool with his shoes„ÄÇ

 not even close„ÄÇ This is me sitting on„ÄÇ I like this rock„ÄÇ It's at Washington University„ÄÇ

 I call it the github rock because it looks pretty similar to Github with all those green squares„ÄÇ

 man in swim trunks„ÄÇ I am not in swim trunks in the middle War shoe„ÄÇ

 I would might lose my job is holding drink in his hand„ÄÇ Yeah it really wants me to get„ÄÇ

I am not in the middle of the university with swim trunks and a drink so anyway„ÄÇ

 these are just trying it on some of the images so up to date on this course and other topics and artificial intelligence„ÄÇ



![](img/2287d406430b06732c757e98a8dcb007_3.png)