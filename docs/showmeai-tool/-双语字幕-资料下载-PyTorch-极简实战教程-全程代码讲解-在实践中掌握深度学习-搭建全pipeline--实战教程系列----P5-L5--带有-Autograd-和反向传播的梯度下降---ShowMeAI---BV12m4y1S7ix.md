# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- å¸¦æœ‰ Autograd å’Œåå‘ä¼ æ’­çš„æ¢¯åº¦ä¸‹é™ - ShowMeAI - BV12m4y1S7ix

Hiï¼Œ everybodyã€‚ Welcome to a new Pytorch tutorialã€‚ In this tutorialã€‚

 I show you a concrete example of how we optimize our model with automatic gradient computation using the pytorch autogra packageã€‚

 So we start by implementing the linear regression algorithm from scratchã€‚

 where we do every step manuallyã€‚ So we implement the equations to calculate the model prediction and the loss functionã€‚

 Then we do a numerical computation of the gradients and implement the formulaã€‚

 And then we implement the gradient decent algorithm to optimize our parametersã€‚

 When this is workingï¼Œ we see how we can replace the manually computed gradients with the automatic back propagation algorithm from Pytorchã€‚

ğŸ˜Šï¼ŒSo this is step number 2ã€‚ and in the third stepsï¼Œ we in the third stepã€‚

 we replace the manually computed loss and parameter updates by using the loss and optimize the classes in Pytorchã€‚

 and in the final stepï¼Œ we replaced the manually computed model prediction by implementing a pytorch modelã€‚

So when we understood each of these stepsï¼Œ Pytorch can do most of the work for usã€‚ Of courseã€‚

 we still have to to design our model and have to know which loss and optimizer we should useã€‚

 But we don't have to worry about the underlying algorithms any moreã€‚

So now this video will cover steps 1 and 2ã€‚And in the next videoï¼Œ we will see the steps 3 and 4ã€‚

 So let's startã€‚ And I assume that you already know how linear regression and gradient decent worksã€‚

 And if notï¼Œ then please watch my machine learning from scratch tutorial about this algorithmã€‚

 because now I will not explain all the steps in detailã€‚



![](img/49d809dfcdcd868a09e2ad42202f36fc_1.png)

But I put the link in the descriptionã€‚So now we do everything from scratchã€‚ So we use only nuyã€‚

 So we import nuy S and Pï¼Œ and then we use linear regressionã€‚

 So we use a function which just just does a linear combination of some weights and our inputsã€‚

 and we don't care about the bias hereã€‚So in our exampleï¼Œ let's say F equalsã€‚Two times xã€‚

 So our weight must be 2ã€‚ And then let's do some training samplesã€‚ So let's say xã€‚

Equals numpy dot arrayã€‚ And then we put some tests or training samplesã€‚ So let's say 1ï¼Œ2ï¼Œ3ï¼Œ and 4ã€‚

 And this will be of numpyã€‚Orï¼Œ let's getã€‚Give this a data type onï¼Œ say this is Nmpy dot floatat 32ã€‚

And thenï¼Œ we also want a yã€‚Andã€‚Since our formula isï¼Œ this is 2 xã€‚

 we have to multiply each of the values by 2ã€‚ So 2ï¼Œ4ï¼Œ6 and 8ã€‚And now we initialize our weightsã€‚

 So we simply say w equals 0 in the beginningã€‚And now we have to calculate our model predictionã€‚

 And we also have to calculate the lossã€‚And then we have to calculate the gradientsã€‚

 So now we do each of these steps manuallyã€‚ So let's define a functionï¼Œ and we call this forwardã€‚

 So this is a forward pass to follow the conventions of pytorrchï¼Œ which will get xã€‚

 and then our model output is simply w times xã€‚So this is the forward passã€‚Nowï¼Œ the lossã€‚

So here we define the function lossï¼Œ which depends on why and why predictedã€‚Soã€‚

The this is the model outputã€‚And now here in this caseã€‚

 this is the or the loss equals the mean squared error in the case of linear regressionã€‚

 And we can calculate this by sayingï¼Œ this isã€‚å—¯ã€‚Let's say y predict it minus yã€‚

 And then to the power of 2ã€‚ And then we do the mean operationã€‚So this is the lossã€‚

 and now we manually have have to calculate the gradient of the loss with respect to our parametersã€‚

 So let's have a look at the mean squared errorã€‚ So the formula is1 over n because it's the meanã€‚

 And then we have our w times x our prediction minus the actual value to the power of2ã€‚

 And now if you want to have the derivativeã€‚ So the derivative of thisã€‚

 let's call this j or objective function with respect to w equals1 over n And then we have two times xã€‚

 and then times W times x minus yã€‚ So this is the numerical computationdã€‚Computed derivativeã€‚

 Also please double check the math for yourselfã€‚And now we implement thisã€‚ So we say define gradientã€‚

 which is dependentã€‚On x and y and also Y predictedã€‚ And now we can do this in one lineã€‚

 So we return numpy dotã€‚ We need a dot product of two times xã€‚



![](img/49d809dfcdcd868a09e2ad42202f36fc_3.png)

And then here we have y predicted minus yã€‚

![](img/49d809dfcdcd868a09e2ad42202f36fc_5.png)

Said this is this formula hereã€‚And thenï¼Œ of courseï¼Œ we also need the meanã€‚

 So let's say this is dot meanã€‚ We can simply do it like this in Nyã€‚And nowã€‚Yeahï¼Œ these are theã€‚

Things we need Nowï¼Œ let's print ourã€‚Prediction before the trainingã€‚ So let's printã€‚

 and we use an F stringã€‚ So prediction before trainingã€‚And let's say we want to predict the value5ã€‚

 which should be 10ã€‚And hereï¼Œ we can doã€‚In the F stringã€‚

 we can actually use an expression so we can call this forward method and with fiveã€‚

And let's say we only want three decimal valuesã€‚And nowï¼Œ let's start our trainingã€‚

So let's define some parametersã€‚ So we need a learning rateï¼Œ which isï¼Œ let's sayï¼Œ001ã€‚

 And then we need a number of iterationsã€‚ So we say an its equals 10ã€‚

And now let's do our training loopã€‚ So we say for epoch in range and itersã€‚And then firstï¼Œ we do theã€‚

Predictionï¼Œ which is the forward passã€‚ So this is the forward passã€‚

And we can simply do this with our functionsã€‚ So we say y prediction or y pre equals forwardã€‚

And then we put in our capital xã€‚And now we want to have the lossã€‚ So our loss L equals the loss ofã€‚

The actual yã€‚And our Y predict itã€‚Nowï¼Œ we need to get theã€‚Grasï¼Œ so our gradientsï¼Œ with respect to Wã€‚

 So D W equals the gradient function that we just implementedã€‚

 which is dependent on x and y and the y predictedã€‚srryã€‚Why pretã€‚Andã€‚

Now we have to update our weightsã€‚ and yesï¼Œ so the update formula in the gradient decentcent algorithm is just we go into the negative direction of the training of the gradientã€‚

 So minus xã€‚ And then hereï¼Œ the step width or the so called learning rate times our our gradientã€‚

 So this is the update formulaã€‚And then let's say we also want to print some information hereã€‚

 So we say if epoch modularï¼Œ just say one hereï¼Œ because now we want to print every stepã€‚If this is 0ã€‚

 we want to printï¼Œ let's sayã€‚We want to print the epochã€‚ And here we print epoch plus 1ã€‚

And then we want to get the weightï¼Œ which isã€‚The weightã€‚Wï¼Œ alsoï¼Œ just 3ã€‚Deimal numbersã€‚

 And then we also want to have the lossã€‚ So the loss equalsã€‚The lossã€‚ And here we sayï¼Œ pointã€‚8ã€‚

 let's sayã€‚And yeahï¼Œ and then at the endã€‚We want to print the prediction after the trainingã€‚Soï¼Œ nowã€‚

 let's predictã€‚Prediict a print predictionã€‚After trainingã€‚And yeahï¼Œ soã€‚ Nowã€‚

 let's run this and see what happensã€‚So everything should be workingã€‚ And nowï¼Œ so yeahã€‚

 before our trainingï¼Œ the prediction is0ã€‚And then for each stepã€‚

 remember that our formula should be  two times xã€‚ So our w should be 2 in the beginningã€‚

 And we see that with each training stepï¼Œ it it increases our weightï¼Œ and it decreases our lossã€‚

 So it gets better with every stepã€‚ And after the trainingï¼Œ our model prediction is 9ã€‚999ã€‚

 So it's almost thereã€‚ So let's sayï¼Œ for exampleï¼Œ now we want to have more iterations hereã€‚

 say we only did we only did 10 iterationsï¼Œ which is not muchã€‚ Nowã€‚

 if we run this and let's print every second step onlyã€‚Then we seeï¼Œ in the endï¼Œ our loss is 0ã€‚

 and the prediction is correctã€‚Nowï¼Œ this is the implementation where we did everything manuallyã€‚

 And now let's replace the gradient calculationã€‚ So let's select all of this and copy this into a separate fileã€‚

And now we don't use numpy anymoreã€‚ So now let's only import torch and do everything with pie torchã€‚

Andï¼Œ of courseï¼Œ what we now want to get rid of is this gradientï¼Œ the the manually computed gradientã€‚

 So we simply delete thisã€‚ We don't need this anymoreã€‚And now we don't have nuy arraysã€‚

 So this is now aã€‚Torch dot Tzarã€‚And our data type is now a torch dot float 32 and the same with our yã€‚

 which is now aã€‚Torch dotã€‚Tensor and also the data types from the torch moduleã€‚

But everything else is the same hereã€‚ So the same syntaxã€‚And now our W also has to be a tenorã€‚

 So let's say this is a torch dot tenor with0ã€‚0 in the beginningã€‚ and it also gets a data type ofã€‚

 say torch dot float 32ã€‚ And since we are interested in the gradient of our loss with respect to this parameterã€‚

 we need to specify that this requires the gradient computationã€‚ So requires gra equals trueã€‚



![](img/49d809dfcdcd868a09e2ad42202f36fc_7.png)

Nowï¼Œ the forward function and the loss function is still the same because we can use the same syntax in pytorrchã€‚

And nowã€‚In our training loopï¼Œ the forward pass is still the sameã€‚ The loss is the sameã€‚

 And now the gradientï¼Œ this is the equal to the backward passã€‚ So rememberï¼Œ in backward propagationã€‚

 we first do a forward passã€‚ That's why we used the syntaxã€‚ And then later for the gradientsã€‚

 we use the backward passã€‚ So here we simply call L dotã€‚Backwardã€‚

And this will calculate the gradient of our loss with respect to Wã€‚

 So Pyto does all the computations for usã€‚ And now we update our weightsã€‚

 But here we want to be carefulã€‚ and I explained this in the tutorial about the auto gridd packageã€‚

Because we don't want to beã€‚ this operation should not be part of ourã€‚Graadient trackingrek graphã€‚

 So this should not be part of the computational graphã€‚ So we need to wrap this in a width torch dotã€‚

 no gr statementã€‚And then one more thing that we should also knowã€‚

 And I also talked about this alreadyï¼Œ we must empty or  zero the gradients againã€‚

 because whenever we call backwardï¼Œ it will write ourã€‚

Grads and accumulate them in the W dot Gr attributeã€‚ So before the next iterationã€‚

 we want to make sure that our gradients are 0 againã€‚ So we can say w times gra times0 underscoreã€‚

 So this will modify it in placeã€‚And nowï¼Œ we are doneã€‚

And now let's run this and see if this is workingã€‚And W is not definedã€‚ Ohï¼Œ yeahï¼Œ Coï¼Œ Of courseã€‚

 this is now W dot graã€‚And let's try this againã€‚ and now it's workingã€‚

And now we also see that it will increase our Wï¼Œ and it will decrease our lossã€‚

 And here we said we had 20 iterationsã€‚But it's not correctï¼Œ Not completely correctã€‚

 And this is because theã€‚Backward or the back propagation is not as exact as the numerical gradient computationã€‚

 So let's try some more iterations hereã€‚ Let's say we want 100 iterations and print our update every 10th stepã€‚

 So let's try this againã€‚And now we see after the training is doneã€‚

 we have the correct correct predictionã€‚Soï¼Œ yeahï¼Œ that's it for this videoã€‚ And in the next videoã€‚

 we will continue here and replace the manually computed loss and weight updates with Pytorch loss and optimizer classesã€‚

So if you like this videoï¼Œ please subscribe to the channel and see you next timeï¼Œ byeã€‚



![](img/49d809dfcdcd868a09e2ad42202f36fc_9.png)