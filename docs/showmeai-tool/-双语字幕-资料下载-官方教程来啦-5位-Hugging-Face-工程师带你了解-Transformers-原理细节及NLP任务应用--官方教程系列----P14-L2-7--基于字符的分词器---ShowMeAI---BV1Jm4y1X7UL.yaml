- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P14ï¼šL2.7- åŸºäºå­—ç¬¦çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P14ï¼šL2.7- åŸºäºå­—ç¬¦çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL
- en: Before diving in character based tokenizationï¼Œ understanding why this kind of
    tokenization is interesting requires understanding the flaws of word based tokenizationã€‚If
    you haven't seen the first video on the word based organizationã€‚ we recommend
    you check it out before looking at this videoã€‚ğŸ˜Šï¼ŒOkayã€‚ let's take a look at character
    based tokenizationã€‚ğŸ˜Šã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥äº†è§£åŸºäºå­—ç¬¦çš„åˆ†è¯ä¹‹å‰ï¼Œç†è§£è¿™ç§åˆ†è¯çš„æœ‰è¶£ä¹‹å¤„éœ€è¦äº†è§£åŸºäºè¯çš„åˆ†è¯çš„ç¼ºé™·ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰çœ‹åˆ°å…³äºåŸºäºè¯çš„ç»„ç»‡çš„ç¬¬ä¸€éƒ¨è§†é¢‘ï¼Œå»ºè®®ä½ åœ¨è§‚çœ‹è¿™ä¸ªè§†é¢‘ä¹‹å‰å…ˆå»çœ‹çœ‹ã€‚ğŸ˜Šå¥½çš„ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åŸºäºå­—ç¬¦çš„åˆ†è¯ã€‚ğŸ˜Š
- en: We now split our text into individual characters rather than wordsã€‚ğŸ˜Šã€‚There are
    generally a lot of different words and languagesï¼Œ while the number of characters
    stays lowã€‚To beginï¼Œ let's take a look at the English languageã€‚It has an estimated
    170ï¼Œ000 different wordsã€‚ so we would need a very large vocabulary to encompass
    all wordsã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦è€Œä¸æ˜¯å•è¯ã€‚ğŸ˜Šä¸€èˆ¬æ¥è¯´ï¼Œæœ‰å¾ˆå¤šä¸åŒçš„å•è¯å’Œè¯­è¨€ï¼Œè€Œå­—ç¬¦çš„æ•°é‡ç›¸å¯¹è¾ƒå°‘ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è‹±è¯­ã€‚å®ƒä¼°è®¡æœ‰170,000ä¸ªä¸åŒçš„å•è¯ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªéå¸¸å¤§çš„è¯æ±‡æ¥æ¶µç›–æ‰€æœ‰å•è¯ã€‚ğŸ˜Š
- en: Where they character based vocabularyï¼Œ we can get by with only 256 charactersã€‚
    which includes lettersï¼Œ numbers and special charactersã€‚Even languages with a lot
    of different characters like the Chinese languages can have dictionaries with
    up to 20ã€‚000 different charactersï¼Œ but more than 375ï¼Œ000 different wordsã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºå­—ç¬¦çš„è¯æ±‡ä¸­ï¼Œæˆ‘ä»¬åªéœ€256ä¸ªå­—ç¬¦å³å¯ï¼Œè¿™åŒ…æ‹¬å­—æ¯ã€æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ã€‚å³ä½¿æ˜¯å­—ç¬¦ä¼—å¤šçš„è¯­è¨€ï¼Œå¦‚æ±‰è¯­ï¼Œä¹Ÿå¯ä»¥æ‹¥æœ‰å¤šè¾¾20,000ä¸ªä¸åŒå­—ç¬¦çš„å­—å…¸ï¼Œä½†ä¸åŒçš„å•è¯è¶…è¿‡375,000ä¸ªã€‚
- en: So character based vocabularies let us use fewer different tokens than the word
    based tokenization dictionaries we would otherwise useã€‚ğŸ˜Šï¼ŒThese vocabularies are
    also more complete than their word based vocabulary counterpartsã€‚As our vocabulary
    contains all characters used in a languageã€‚ even the words unseen during the tokenizer
    training can still be tokenizedã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åŸºäºå­—ç¬¦çš„è¯æ±‡è®©æˆ‘ä»¬ä½¿ç”¨æ¯”åŸºäºè¯çš„åˆ†è¯å­—å…¸æ›´å°‘çš„ä¸åŒä»¤ç‰Œã€‚ğŸ˜Šè¿™äº›è¯æ±‡ä¹Ÿæ¯”å®ƒä»¬çš„åŸºäºè¯çš„è¯æ±‡æ›´å®Œæ•´ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¯æ±‡åŒ…å«äº†è¯­è¨€ä¸­ä½¿ç”¨çš„æ‰€æœ‰å­—ç¬¦ï¼Œç”šè‡³åœ¨åˆ†è¯å™¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„å•è¯ä»ç„¶å¯ä»¥è¢«åˆ†è¯ã€‚
- en: so out vocabulary tokens will be less frequentã€‚ğŸ˜Šï¼ŒThis includes the ability to
    correctly tokenize misspelled words rather than discarding them as unknown straight
    awayã€‚ğŸ˜Šï¼ŒHoweverï¼Œ this algorithm isn't perfect eitherã€‚ğŸ˜Šï¼ŒIntuitivelyã€‚ characters
    do not hold as much information individually as a word will holdã€‚ğŸ˜Šï¼ŒFor exampleã€‚
    let's hold more information than its first data Lã€‚Of courseï¼Œ this is not trueful
    for all languagesã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çš„è¯æ±‡ä»¤ç‰Œä¼šæ›´å°‘å‡ºç°ã€‚ğŸ˜Šè¿™åŒ…æ‹¬æ­£ç¡®æ ‡è®°æ‹¼å†™é”™è¯¯çš„å•è¯çš„èƒ½åŠ›ï¼Œè€Œä¸æ˜¯ç«‹å³å°†å…¶è§†ä¸ºæœªçŸ¥ã€‚ğŸ˜Šç„¶è€Œï¼Œè¿™ä¸ªç®—æ³•ä¹Ÿå¹¶ä¸å®Œç¾ã€‚ğŸ˜Šç›´è§‚åœ°è¯´ï¼Œå­—ç¬¦å•ç‹¬æ‰€æ‰¿è½½çš„ä¿¡æ¯ä¸å¦‚ä¸€ä¸ªå•è¯æ‰€æ‰¿è½½çš„ä¿¡æ¯å¤šã€‚ğŸ˜Šä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬æŒæœ‰æ¯”å…¶é¦–ä¸ªæ•°æ®Læ›´å¤šçš„ä¿¡æ¯ã€‚å½“ç„¶ï¼Œè¿™å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰è¯­è¨€ã€‚
- en: as some languages likeideogram based languagesï¼Œ have a lot of information held
    in a single characterã€‚ğŸ˜Šï¼ŒBut for others like Roman based languagesï¼Œ the model will
    have to make sense of multiple tokens at a time to get the information otherwise
    held in a single wordã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºä¸€äº›è¯­è¨€ï¼ˆå¦‚è¡¨æ„æ–‡å­—è¯­è¨€ï¼‰åœ¨å•ä¸ªå­—ç¬¦ä¸­æ‰¿è½½äº†å¾ˆå¤šä¿¡æ¯ã€‚ğŸ˜Šä½†å¯¹äºåƒç½—é©¬å­—æ¯è¿™æ ·çš„è¯­è¨€ï¼Œæ¨¡å‹å¿…é¡»åŒæ—¶ç†è§£å¤šä¸ªä»¤ç‰Œæ‰èƒ½è·å–åŸæœ¬åœ¨å•è¯ä¸­æ‰¿è½½çš„ä¿¡æ¯ã€‚
- en: This leads to another issue with character based tokenizersã€‚ Their sequences
    are translated into very large amounts of tokens to be processed by the modelã€‚And
    this can have an impact on the size of the context the model will carry aroundã€‚
    and it will reduce the size of the text we can use as input for a modelï¼Œ which
    is often limitedã€‚ğŸ˜Šã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¼è‡´äº†åŸºäºå­—ç¬¦çš„åˆ†è¯å™¨çš„å¦ä¸€ä¸ªé—®é¢˜ã€‚å®ƒä»¬çš„åºåˆ—è¢«è½¬æ¢ä¸ºå¤§é‡çš„ä»¤ç‰Œä¾›æ¨¡å‹å¤„ç†ã€‚è¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹æºå¸¦çš„ä¸Šä¸‹æ–‡å¤§å°ï¼Œå¹¶å‡å°‘æˆ‘ä»¬å¯ä»¥ç”¨ä½œæ¨¡å‹è¾“å…¥çš„æ–‡æœ¬å¤§å°ï¼Œè¿™é€šå¸¸æ˜¯æœ‰é™çš„ã€‚ğŸ˜Š
- en: This organizationï¼Œ while it has some issuesï¼Œ has seen some very good results
    in the pastã€‚ and so it should be considered when approaching a new problem as
    it solves issues encountered in the word based algorithmã€‚![](img/6a739a8b766233902cc0ce2bd62c0279_1.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»„ç»‡è™½ç„¶æœ‰ä¸€äº›é—®é¢˜ï¼Œä½†åœ¨è¿‡å»å–å¾—äº†ä¸€äº›éå¸¸å¥½çš„ç»“æœã€‚å› æ­¤ï¼Œåœ¨é¢å¯¹æ–°é—®é¢˜æ—¶ï¼Œåº”è€ƒè™‘å®ƒï¼Œå› ä¸ºå®ƒè§£å†³äº†åŸºäºè¯çš„ç®—æ³•ä¸­é‡åˆ°çš„é—®é¢˜ã€‚![](img/6a739a8b766233902cc0ce2bd62c0279_1.png)
- en: å—¯ã€‚![](img/6a739a8b766233902cc0ce2bd62c0279_3.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚![](img/6a739a8b766233902cc0ce2bd62c0279_3.png)
