# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P27ï¼šL4.4- ä½¿ç”¨ TensorFlow è¿›è¡Œå¾®è°ƒ(è¿ç§»å­¦ä¹ ) - ShowMeAI - BV1Jm4y1X7UL

So in this videoï¼Œ we're going to see how to load and fine tune a pre trained modelã€‚ It's very quickã€‚

 And if you've watched our pipeline videosï¼Œ which I'll link belowï¼Œ the process is very similarã€‚ğŸ˜Šã€‚

This timeï¼Œ thoughï¼Œ we're going to be using transfer learning and doing some training ourselvesã€‚

 rather than just loading a model and using it as isï¼Œ like we did in the pipeline videosã€‚

 So if you to learn more about transfer learningï¼Œ if you don't know much about itã€‚

 you can head to the what is transfer learning videoã€‚ And I'll link that below as wellã€‚ğŸ˜Šï¼ŒBut for nowã€‚

 let's look at this codeã€‚ So to startï¼Œ we pick which model we want to useã€‚ In this caseã€‚

 we're going to use the famous plastic Btã€‚But what does thisï¼Œ this line hereï¼Œ this monstrosityã€‚

 this TF auto model for sequence classificationï¼Œ What does that meanã€‚Wellã€‚

 the TF stands for Tensorflowï¼Œ and the rest means take a language model and stick a sequence classification head onto it if it doesn't have one alreadyã€‚

So what we're going to do here is load Btï¼Œ which is a general general purpose language model that doesn't have a sequence classification headã€‚

 We're going to use the from pre traineded methodï¼Œ and that method ensures that all our weights come from the pre trained modelã€‚

 So they're not randomly initialized with the exception of the new sequence classification head we're going to addã€‚

ğŸ˜Šï¼ŒSo this method needs to know two thingsã€‚Firstlyï¼Œ it needs to know the name of the model you wanted to loadã€‚

 And secondlyï¼Œ it needs to know how many classes your problem hasã€‚

So if you want to follow along with the data from our data sets videosï¼Œ which I'll link belowã€‚

 then you'll have two classesï¼Œ positive and negativeï¼Œ and thus nu labels equals 2ã€‚

But what about this compile thingã€‚So if you're familiar with Carisã€‚

 you've probably seen this alreadyã€‚ But if notï¼Œ this is one of the core methods in Carisã€‚

 you always need to compile your model before you train itã€‚ compileile needs to know two thingsã€‚

 Firstlyï¼Œ the loss functionï¼Œ which is basicallyï¼Œ what are we trying to optimizeã€‚

 And here we import the sparse categorical cross entropy loss functionã€‚ So that's a mouthfulã€‚

 If you've never encountered it beforeã€‚ but it's the standard loss function for any neural network that's doing a classification taskã€‚

 It basically encourages the network to output large valuesã€‚

 So large probabilities for the right class and low values of low probabilities for the wrong classesã€‚

Notice that youï¼Œ you can specify the last function as a string like we do with the optr hereã€‚

 But there's a very common pitfall hereã€‚ By defaultã€‚

 the last assumes the output is probabilities from a softm layerã€‚

 But what our model has actually output is the values before the softmã€‚

 These are often called the logicits or logicitsã€‚ You saw these before in the video about pipelinesã€‚

ğŸ˜Šï¼ŒIf you get that this wrongï¼Œ your model won't trainã€‚ And it'll be very annoying to figure out whyã€‚

 In factï¼Œ I'm going to go so far as to say that if you remember absolutely nothing else from this videoã€‚

 remember to always check whether your model is outputting logicits or probabilitiesã€‚

 and make sure your loss is set up to match thatã€‚ So this is going save you a lot of debugging headaches in your careerã€‚

 That would otherwise be very difficult to track down and very annoyingã€‚But leaving that asideã€‚

 the second thing compile needs to know is the optr you wantã€‚ In our caseï¼Œ we're going to use atomã€‚

 which is sort of the standard optimizer for deep learning these daysã€‚

 The one thing you might want to change is the learning rateã€‚

 And to do that we'll need to import the actual optrï¼Œ rather than just calling it by stringã€‚

 So much like we did with the lossã€‚ But we can talk about that in another videoã€‚

 and I'll link that below for nowï¼Œ let's just try training the modelã€‚ğŸ˜Šï¼ŒSo how do you train the modelï¼Ÿ

 Wellï¼Œ if you've used Kais beforeï¼Œ this will all be very familiar to youã€‚ But if notã€‚

 let's look at what we're doing hereã€‚ Fit is pretty much the central method for Kais modelsã€‚

 It tells the model to break the input into batches and then train on itã€‚

 So the first input is tokenized textã€‚ you'll almost always be getting this from a tokenizerã€‚

 And if you want to learn more about that processã€‚ what exactly these inputs look likeã€‚

 please check out our videos on tokensã€‚ And againï¼Œ there'll be links for those belowã€‚

So those are our inputsã€‚But then the second argument is our labelsã€‚

 and this is really straightforwardã€‚ This is just a one dimensional nuy or tensorflow array of integersã€‚

 and they correspond to the classes for examplesã€‚ that's itã€‚ğŸ˜Šã€‚

So if you're following along with our data from our data sets videoï¼Œ there'll only be two classesã€‚

 so this will just be a vector of zeros and onesï¼Œ but you can have many more classes than that for your own problemsã€‚

So once we have our inputs and our labelsï¼Œ we do the same thing with the validation dataã€‚

 We pass the validation inputs and the validation labels in a tupleã€‚ and then we canã€‚

 if we want to specify date details like the batch size for trainingã€‚

 And then you just pass the whole thing to model dot fit and you let it ripã€‚

 So if everything works outï¼Œ you should see a little training progress bar as your last goes downã€‚

 And while that's runningï¼Œ you knowï¼Œ you sit backï¼Œ you call your boss and you tell them you're a senior NLP machine learning engineer nowã€‚

 and you're going to want to salary review next quarterã€‚ So this is reallyï¼Œ I'm kidding a bitã€‚

 But this is really all it takes to apply the power of a massive pretrained language model to your N LP problemã€‚

ğŸ˜Šï¼ŒBut could we do better than thisï¼Œ Likeï¼Œ is there any changes we could makeï¼Œ So there certainly areã€‚

 There's a few more advanced careis features like a tune scheduled learning rateã€‚

 We could get an even lower lossã€‚ And even therefore for an even more accurate modelã€‚ And alsoã€‚

 when we when fit finishesï¼Œ what do we do with our model once it's trainedã€‚ So these are all topicsã€‚

 I'm going to cover these and more in the videosã€‚ And againã€‚

 I'm going to link those subsequent videos belowã€‚ğŸ˜Šã€‚



![](img/c842c6db02d2a7c1478de0863bd1f759_1.png)

![](img/c842c6db02d2a7c1478de0863bd1f759_2.png)

ã€‚