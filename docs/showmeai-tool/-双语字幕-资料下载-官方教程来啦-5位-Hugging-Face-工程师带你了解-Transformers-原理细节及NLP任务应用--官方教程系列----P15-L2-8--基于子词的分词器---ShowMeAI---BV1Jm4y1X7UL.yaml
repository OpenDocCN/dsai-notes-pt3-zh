- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P15ï¼šL2.8- åŸºäºå­è¯çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½Hugging Faceå·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£TransformersåŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL2.8-
    åŸºäºå­è¯çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL
- en: Let's take a look at subward based tokenizationã€‚Understanding why sub word based
    tokenization is interesting requires understanding the flaws of word based and
    character based tokenizationã€‚If you haven't seen the first videos on word based
    and character based tokenizationã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹åŸºäºå­è¯çš„åˆ†è¯ã€‚ç†è§£ä¸ºä»€ä¹ˆåŸºäºå­è¯çš„åˆ†è¯æœ‰è¶£ï¼Œéœ€è¦ç†è§£åŸºäºå•è¯å’ŒåŸºäºå­—ç¬¦çš„åˆ†è¯çš„ç¼ºé™·ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰çœ‹è¿‡å…³äºåŸºäºå•è¯å’ŒåŸºäºå­—ç¬¦çš„åˆ†è¯çš„ç¬¬ä¸€éƒ¨è§†é¢‘ã€‚
- en: we recommend you check them out before looking at this videoã€‚Subward based tokenization
    lies in between characteror based and word based tokenization algorithmsã€‚The idea
    is to find a middle ground between very large vocabulariesã€‚ a large quantity of
    out of vocabulary tokensï¼Œ and a loss of meaning across very similar wordsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®æ‚¨åœ¨æŸ¥çœ‹æ­¤è§†é¢‘ä¹‹å‰æŸ¥çœ‹å®ƒä»¬ã€‚åŸºäºå­è¯çš„åˆ†è¯å¤„äºåŸºäºå­—ç¬¦å’ŒåŸºäºå•è¯çš„åˆ†è¯ç®—æ³•ä¹‹é—´ã€‚å…¶æƒ³æ³•æ˜¯åœ¨éå¸¸å¤§çš„è¯æ±‡é‡ã€è¾ƒå¤§é‡çš„è¶…å‡ºè¯æ±‡çš„æ ‡è®°ä»¥åŠåœ¨éå¸¸ç›¸ä¼¼çš„å•è¯ä¹‹é—´ä¸§å¤±æ„ä¹‰ä¹‹é—´æ‰¾åˆ°ä¸­é—´åœ°å¸¦ã€‚
- en: for word based tokens and very long sequences as well as less meaningful individual
    tokens for character based tokenizersã€‚These algorithms rely on the following principleã€‚Frequently
    used words should not be splin into smaller subwordsã€‚ while rare words should
    be decomposed into meaningful subsã€‚An example is the word dogã€‚ we would like to
    have our tokenizer to have a single ID for the word dogã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåŸºäºå•è¯çš„æ ‡è®°å’Œéå¸¸é•¿çš„åºåˆ—ï¼Œä»¥åŠåŸºäºå­—ç¬¦çš„åˆ†è¯å™¨çš„å•ä¸ªæ ‡è®°ï¼Œè¿™äº›ç®—æ³•ä¾èµ–äºä»¥ä¸‹åŸåˆ™ã€‚å¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆè¾ƒå°çš„å­è¯ï¼Œè€Œç¨€æœ‰è¯åº”è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚ä¸€ä¸ªä¾‹å­æ˜¯å•è¯dogã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åˆ†è¯å™¨ä¸ºå•è¯dogä¿ç•™ä¸€ä¸ªå”¯ä¸€çš„IDã€‚
- en: rather than splitting it into characters DONGã€‚Howeverï¼Œ when encountering the
    word dogsã€‚ we would like our tokenizer to understand that at the rootã€‚ this is
    still the word dog with an added S that slightly changes the meaning while keeping
    the original ideaã€‚Another example is a complex word like tokenizationï¼Œ which can
    be split into meaningful subwordsã€‚ğŸ˜Šã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸æ˜¯å°†å…¶æ‹†åˆ†æˆå­—ç¬¦DONGã€‚ç„¶è€Œï¼Œå½“é‡åˆ°å•è¯dogsæ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åˆ†è¯å™¨ç†è§£ï¼Œæ ¹æœ¬ä¸Šï¼Œè¿™ä»ç„¶æ˜¯å•è¯dogï¼ŒåŠ ä¸Šä¸€ä¸ªSï¼Œè¿™ç¨å¾®æ”¹å˜äº†æ„ä¹‰ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æƒ³æ³•ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯åƒtokenizationè¿™æ ·å¤æ‚çš„è¯ï¼Œå¯ä»¥æ‹†åˆ†æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚ğŸ˜Šã€‚
- en: The root of the word is tokenï¼Œ and Iization completes the root to give it a
    slightly different meaningã€‚ğŸ˜Šï¼ŒIt makes sense to split the word into two token as
    the root of the word labeled as the start of the word andization as additional
    information labeled as a completion of the wordã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å•è¯çš„æ ¹æ˜¯tokenï¼Œiizationå®Œæˆæ ¹éƒ¨ï¼Œèµ‹äºˆå…¶ç¨å¾®ä¸åŒçš„å«ä¹‰ã€‚ğŸ˜Šï¼Œå°†å•è¯æ‹†åˆ†æˆä¸¤ä¸ªtokenæ˜¯åˆç†çš„ï¼Œå°†æ ‡è®°ä¸ºå•è¯å¼€å¤´çš„è¯æ ¹å’Œæ ‡è®°ä¸ºå•è¯å®Œæˆçš„é™„åŠ ä¿¡æ¯çš„izationã€‚
- en: In turnï¼Œ the model will now be able to make sense of token in different situationsã€‚It
    will understand that the words tokenï¼Œ tokensï¼Œ tokenizing and tokenizations have
    a similar meaning and are linkedã€‚It will also understand that tokenizationï¼Œ modernizationï¼Œ
    and immunizationã€‚ which all have the same suffixesï¼Œ are probably used in these
    same syntactic situationsã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åè¿‡æ¥ï¼Œæ¨¡å‹ç°åœ¨èƒ½å¤Ÿåœ¨ä¸åŒæƒ…å†µä¸‹ç†è§£tokenã€‚å®ƒå°†ç†è§£å•è¯tokenã€tokensã€tokenizingå’Œtokenizationså…·æœ‰ç›¸ä¼¼çš„å«ä¹‰å¹¶ä¸”æ˜¯å…³è”çš„ã€‚å®ƒè¿˜å°†ç†è§£tokenizationã€modernizationå’Œimmunizationï¼Œè¿™äº›éƒ½æœ‰ç›¸åŒçš„åç¼€ï¼Œå¯èƒ½åœ¨ç›¸åŒçš„å¥æ³•æƒ…å†µä¸‹ä½¿ç”¨ã€‚
- en: Subword based tokenrs generally have a way to identify which tokens are a start
    of word and which tokens complete start of wordssã€‚So here token as the start of
    a word and hash hashization as completion of the wordã€‚Here the hash hash prefix
    indicates that Iization is part of award word rather than the beginning of itã€‚ğŸ˜Šï¼ŒThe
    hash hash comes from the bird tokenizer based on the word peace algorithmã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå­è¯çš„åˆ†è¯å™¨é€šå¸¸æœ‰åŠæ³•è¯†åˆ«å“ªäº›æ ‡è®°æ˜¯å•è¯çš„å¼€å§‹ï¼Œå“ªäº›æ ‡è®°æ˜¯å®Œæˆå•è¯çš„ã€‚è¿™é‡Œçš„tokenæ˜¯å•è¯çš„å¼€å§‹ï¼Œhash hashizationæ˜¯å•è¯çš„å®Œæˆã€‚è¿™é‡Œçš„hash
    hashå‰ç¼€è¡¨ç¤ºiizationæ˜¯ä¸€ä¸ªå•è¯çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å®ƒçš„å¼€å¤´ã€‚ğŸ˜Šï¼Œhash hashæ¥è‡ªäºåŸºäºå•è¯peaceç®—æ³•çš„é¸Ÿç±»åˆ†è¯å™¨ã€‚
- en: Other tokens use other prefixes which can be placed to indicate part of words
    like in here or start of words insteadã€‚There are a lot of different algorithms
    that can be used for subway tokenizationã€‚ and most models obtaining state of the
    art results in English today use some kind of subward tokenization algorithmsã€‚These
    approaches help in reducing the vocabulary sizes by sharing information across
    different wordsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æ ‡è®°ä½¿ç”¨å…¶ä»–å‰ç¼€ï¼Œå¯ä»¥æ”¾ç½®åœ¨è¿™é‡Œè¡¨ç¤ºéƒ¨åˆ†å•è¯æˆ–å•è¯çš„å¼€å§‹ã€‚æœ‰å¾ˆå¤šä¸åŒçš„ç®—æ³•å¯ä»¥ç”¨äºåŸºäºå­è¯çš„åˆ†è¯ï¼Œè€Œå¦‚ä»Šå¤§å¤šæ•°æ¨¡å‹åœ¨è‹±è¯­ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œéƒ½ä½¿ç”¨æŸç§åŸºäºå­è¯çš„åˆ†è¯ç®—æ³•ã€‚è¿™äº›æ–¹æ³•é€šè¿‡åœ¨ä¸åŒå•è¯ä¹‹é—´å…±äº«ä¿¡æ¯ï¼Œå¸®åŠ©å‡å°‘è¯æ±‡é‡ã€‚
- en: having the ability to have prefixes and suffixes understood as suchã€‚They keep
    meaning across very similar words by recognizing similar tokensï¼Œ making them upã€‚![](img/682dd3ed84761ab915a2a8c8a20c6be7_1.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å…·å¤‡å°†å‰ç¼€å’Œåç¼€ç†è§£ä¸ºè¿™æ ·çš„èƒ½åŠ›ã€‚é€šè¿‡è¯†åˆ«ç›¸ä¼¼çš„æ ‡è®°ï¼Œå®ƒä»¬åœ¨éå¸¸ç›¸ä¼¼çš„è¯æ±‡ä¸­ä¿æŒæ„ä¹‰ï¼Œæ„é€ å‡ºæ–°è¯ã€‚![](img/682dd3ed84761ab915a2a8c8a20c6be7_1.png)
- en: '![](img/682dd3ed84761ab915a2a8c8a20c6be7_2.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/682dd3ed84761ab915a2a8c8a20c6be7_2.png)'
- en: Yeahã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚
