# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P22ï¼šL3.5- ä½¿ç”¨PyTorchç¼–å†™è®­ç»ƒæµç¨‹ - ShowMeAI - BV1Jm4y1X7UL

Write your own training loop bit by dochã€‚In this videoã€‚

 along Kaar we can do the same functioning as in the train video but without relying on web classã€‚

 this wayï¼Œ you'll be able to easily customize each type to of the training loop to your needsã€‚

This is also very useful to manually debug something things that went along with the train APIã€‚

Before we dive into the codeï¼Œ here is a sketch of a training loopã€‚

We take a batch of training data and feed it to the modelã€‚With the labelsï¼Œ we can get computer lessã€‚

That number is not useful in turnï¼Œ but is used to compute the gradient of our model weightsï¼›

 that is the derivative of thes with respect to each model weightã€‚

Those gradients are then used by the optimizer to update the model ways and make them a little bit betterã€‚

We then repeated the process with a new batch of training dataã€‚If any of this isn'tã€‚

 don't hesitate to take a refresher on your fabric deep learning courseã€‚

We'll use the G MRRPC dataset set here againï¼Œ and we're seen to propose head the data using the data sets library with dynamicy Pddingã€‚

Check out the videos link below if you haven't seen them alreadyã€‚With this doneã€‚

 we only have to define Pytorrch data loadorsï¼Œ which will be responsible to convert the elements of a dataset set into patchesã€‚

We use our data coulator for padding as a correct function and sh for the training set to make sure we don't go through the samples in the same order as a cheaperã€‚

To check that everything works as intendedï¼Œ we try to grab a batch of data and inspect itã€‚

Like how that asset set elementsï¼Œ it's a dictionaryã€‚

 but this time the values are not a single list of integersã€‚

 but the tonso of shapepa sizes by sequence lengthã€‚

The next step is to send the training data in our modelã€‚

Now that will need to actually create a modelã€‚As seen in the model API videoã€‚

 we use the front pretrained method and adjust the number of labels to the number of classes we have on this dataset set here tooã€‚

Againï¼Œ to be sure everything is going wellï¼Œ we pass the batch we to our model and check there is no errorã€‚

I the labels are providedï¼Œ the models of the Transence library always returns the list directlyã€‚

We'll be able to do less step backward to compute all the gradientã€‚

 and we'll then need the Optr to do the training stepã€‚We use the Adam W optimizer hereã€‚

 which is the variant of Adam with proper weight decayã€‚

 but you can pick any by doch optimizer you likeã€‚Using the previous loss and computing the gradients we've listeded backwardã€‚

 we check that we can do the optimizer step without an errorã€‚

Don't forget to zero your gradient afterwardï¼Œ or the next step from get added to the gradient you computedã€‚

We got already white or training loopï¼Œ but we add two more things to make it as good as it can beã€‚

The first one is a learning rate scheduler to progressively decay a learning rate to0ã€‚

The GtSched function from the Transformer library is just a convenience function to easily build searcher Schã€‚

You can again use any Pythto learning weight scheduler insteadã€‚Finallyã€‚

 if we want our training to take a couple of minutes instead of a few hours we'll need to use a GPU the first step is to get one for instance by using a collaborative bookã€‚

Then you need to actually send your model and training that on it by using a dashch deviceã€‚

Do will check the following lines point your good databases for you or be prepared for your training to last more than an hourã€‚

We cannot put everything togetherã€‚Firstï¼Œ we put our model in training modeã€‚

 which will activate the training behavior for some layers like dropoutã€‚

Then we go through the number of reports books we picked and all the data in our training dataã€‚

Then we go through all the steps we've seen alreadyï¼Œ send the data to the GPUã€‚

 compute the model outputsï¼Œ and in particular the lessã€‚Use the list to compute gradientsã€‚

 then make a training step with the optimizerã€‚Updates are earn rate and our schedule error for the next detereration and the gradient of the optimizerã€‚

Once this is finishedï¼Œ we can evaluate our model very easily with a metric from the dataset sets libraryã€‚

ğŸ˜Šï¼ŒFirstï¼Œ we put our model in the evaluation mode to deactivate layers like dropoutã€‚

Then goes through all the des the evaluation theã€‚As we' seen in the trainer videoã€‚

 the model outputs logggets and we need to apply the Agm function to convert them into predictionsã€‚

The metric object then has a net batch method we can use to send it for intermediate predictionsã€‚

Once the evaluation loop is finishedï¼Œ you just have to call the compute method to get the final resultsã€‚

Congratulationsï¼Œ you have no find model all by yourselfã€‚



![](img/e8f7bf95fa7a3fb25fc811f317b88cfb_1.png)

ã€‚

![](img/e8f7bf95fa7a3fb25fc811f317b88cfb_3.png)