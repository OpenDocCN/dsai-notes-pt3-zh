- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P10ÔºöL10-
    ÂÜ≥Á≠ñÊ†ëÁ¨¨ 2 ÈÉ®ÂàÜ - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to the second part of the decision tree tutorial„ÄÇ If
    you haven't watched the first partÔºå then please do soÔºå because thereÔºå I will explain
    the theory„ÄÇ So here we continue with the implementationÔºå and we can start right
    away„ÄÇ So we say import numpy S and P„ÄÇ and then„ÄÇBefore we implement the decision
    tree classÔºå we will first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Create our entropy method to calculate the entropy„ÄÇ and we implement this as
    global functions„ÄÇ So we say define entropy„ÄÇ and this will get a vector y of all
    our class labels„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: And let's have a look at the formula„ÄÇ So we have to calculate the number of
    occurrencesÔºå and we can„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Do this with a function„ÄÇ And we call this hist or histogram„ÄÇ And we can use
    nuy bin count„ÄÇOf y„ÄÇ So this will calculate the number of occurrences of all class
    labels„ÄÇ and then we divide them by the number of total samples„ÄÇ So we say P equals
    hist divided by length of y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_5.png)'
  prefs: []
  type: TYPE_IMG
- en: And then we apply the actual formula„ÄÇ So we say minus the sum of p of x times
    the log of p of x„ÄÇ So we can do this in one line and say return minus nu pi sum„ÄÇ
    And here we use less comprehension„ÄÇ So we can say P times nupy log2 of P for all
    P in P P and we also have to use a condition if P is greater than 0 because the
    lock is not defined for negative numbers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_7.png)'
  prefs: []
  type: TYPE_IMG
- en: So this is the entropy„ÄÇAnd now we also implement a helper class and call this
    note„ÄÇ So here we will store the information for our note„ÄÇ So this will get an
    in it„ÄÇWhich gets self„ÄÇ And then let's have a look at thisÔºå so„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: If we are in the middleÔºå then we want to store the best split feature and the
    best split threshold„ÄÇAnd we also want to store the left and the right child treesÔºå
    because we need them later and„ÄÇIf we and nowÔºå if we are at a leaf noteÔºå then we
    also want to store the actual value here„ÄÇ So the most common class label„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_11.png)'
  prefs: []
  type: TYPE_IMG
- en: SoÔºå we sayÔºå feature„ÄÇEquals none„ÄÇThreshold equals none left equals none right
    equals none„ÄÇ And then we use a little trickier„ÄÇ So we use an asterisk and a comma„ÄÇ
    and then we say value equals none„ÄÇ So now if we want to use this value parameter„ÄÇ
    we have to use it as a„ÄÇKey word only parameter„ÄÇ So later when we create our leaf
    node„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which only gets the valueÔºå then we also have to write value equals something„ÄÇ
    So then it's clearer that this is a leaf node„ÄÇ And here we simply store them„ÄÇ
    So we say self feature equals feature self threshold equals threshold„ÄÇSelf left
    equals left„ÄÇSelf„ÄÇ right equals right„ÄÇAnd self value equals value„ÄÇ And now we also
    create a little help a function to determine if we are at a leaf node„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So define fine is leaf node„ÄÇWhich gets self„ÄÇ And here we simply say ifÔºå if we
    are„ÄÇ if we have a valueÔºå then we are at a leaf node and otherwise not„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_14.png)'
  prefs: []
  type: TYPE_IMG
- en: So we return self that value is not none„ÄÇSo if we have a valueÔºå then we return
    true„ÄÇAnd this is our helper class for the notes„ÄÇ And now we can start with the
    actual decision3 class„ÄÇ So this also gets an in it„ÄÇWhich gets self„ÄÇ And then it
    will get some stopping criteria„ÄÇ So we call this min samples split„ÄÇ And by defaultÔºå
    let's say this is2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the minimum samples required to further split our  treeÔºå Then the max depth„ÄÇ
    And by default„ÄÇ this is 100„ÄÇAnd then alsoÔºå it gets a parameter that we call the
    number of features or n Fes„ÄÇ And this is none„ÄÇ So we don't need thisÔºå but we can
    specify it„ÄÇ So as I said„ÄÇ we do a greedy search over all the featuresÔºå but we
    can also only loop over a subset of number of features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we randomly select this subsetÔºå So this is one of the random factors„ÄÇAnd
    that's also one of the reasons why it's called random forest„ÄÇ If we extend our
    decision trees to a random forest„ÄÇ So this is one random factor„ÄÇAnd now we simply
    store them„ÄÇ So we say self dot min samples split equals min samples„ÄÇSorry„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: min sample split self dot max step equals max„ÄÇDeep„ÄÇSelf dot n features equals
    and features„ÄÇ And we also create a root„ÄÇ And by in the beginningÔºå this is none„ÄÇ
    So we later need to know our root so that we know where we should start traversing
    our tree„ÄÇAnd now we implement the fit method„ÄÇWhich gets the training data and
    training labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we want to grow our tree„ÄÇAnd then it gets the predict method„ÄÇWith the
    test labels„ÄÇ So here we want to traverse our tree„ÄÇSo let's start with growing
    our tree„ÄÇ So we say self dot root equals„ÄÇ And now we call and create a help a
    function self dot grow3„ÄÇ which gets X and y„ÄÇAnd we also apply a safety check„ÄÇ
    So we say self dot and features fits equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: X dot shape„ÄÇoff„ÄÇ1Ôºå so this is a nuy and DRA„ÄÇ And the second dimension is the
    number of features„ÄÇIf not self dot and feed„ÄÇ So if this is not specifiedÔºå if this
    is none„ÄÇ then we simply take the maximum number of features„ÄÇAnd otherwiseÔºå we
    take the minimum of self„ÄÇAnd feets and X dot shape 1„ÄÇSo this just makes sure that
    it can never be greater than the actual number of features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we implement the„ÄÇGrow tree„ÄÇMethodÔºå which gets self„ÄÇ and then it gets
    X and y„ÄÇ And also also a depth„ÄÇ And this is 0 in the beginning„ÄÇ So we need to
    keep track of the depth„ÄÇAnd now let's do this„ÄÇ So firstÔºå let's get the number
    of samples and the number of features„ÄÇ This is x dot shape„ÄÇAnd then we also want
    to get the number of different labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the length of nuy„ÄÇUnique„ÄÇOf y„ÄÇ So all the different labels„ÄÇAnd nowÔºå
    first„ÄÇWhat we do here is firstÔºå we apply our stopping criteria„ÄÇTia„ÄÇSo we say if„ÄÇ
    and now let's again have a look what we said„ÄÇ So we want to check for the maximum
    depth„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_16.png)
  prefs: []
  type: TYPE_NORMAL
- en: Then the minimum samples required and if we have no more class distributionÔºå
    so we say if„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: And we say depth is greater or equal than self max depth„ÄÇOr„ÄÇIf„ÄÇThe number of
    different labels„ÄÇEquals one„ÄÇ So if we have only one class at this nodeÔºå or if
    we have number of samples„ÄÇIs smaller than the minimum samples required„ÄÇSo„ÄÇIf this
    is trueÔºå then we are at the leaf node„ÄÇ So we says let leaf value equals self dot
    most„ÄÇCommon label„ÄÇOf this y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we create and return our leaf node„ÄÇ So we say return node„ÄÇ And then
    we have to say value equals leaf value„ÄÇ And now it gets clear why I use just ask
    the risk„ÄÇ So here I have to use the value as keyword„ÄÇ And now it's clear that
    this is a leaf node„ÄÇAnd now we also need this help a function to say to get the
    most common label„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's write this down here„ÄÇ So define most common labelÔºå which gets self„ÄÇAnd
    then it gets a vector of the glass labels„ÄÇ And for thisÔºå we use a Python module„ÄÇ
    the counter module„ÄÇ So we say from collections import counter„ÄÇ So I talked about
    this in previous videos already„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: please check that out if you're not familiar with the counter module„ÄÇSo here
    we can create a counter object„ÄÇ Count equals counter of y„ÄÇ So this will„ÄÇÂóØ„ÄÇCalculate
    all the the number of occurrences for all the ysÔºå similar to the nu pin count„ÄÇ
    and then we have a most common function„ÄÇ So we say most common equals counter
    dot most common of one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we only need the very most common label„ÄÇ and this will return a lists of
    tus„ÄÇ So we want to have the first element of the list„ÄÇ So this is the very most
    common tuple„ÄÇ and in the tupleÔºå there is the value stored and also the number
    of occurrences„ÄÇ So we only need the value„ÄÇ So we again say index 0„ÄÇ So please
    double check it for yourself„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we return this„ÄÇThis will determine the most common label„ÄÇ So now we
    have the tier and„ÄÇNow„ÄÇ if we didn't meet the stopping criteriaÔºå then we continue„ÄÇ
    So firstÔºå we select the feature indices„ÄÇ So feature indices equals nuy random
    choice„ÄÇAnd this will get the number of features„ÄÇ So it will select random numbers
    from between 0 and the number of features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the array should be of sizeÔºå self dot and fes that we specified„ÄÇ And we
    also say replace equals false because we don't have„ÄÇ don't want to have the same
    indices multiple times„ÄÇAnd nowÔºå we do our greedy search„ÄÇSoÔºå we say„ÄÇ best thresh„ÄÇAnd
    bestÔºå all let's do it the other way around„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: best feature and best thrash equals self dot„ÄÇBest criteria„ÄÇ And this will get
    X and y and all the feature indices„ÄÇ So this is another helper method„ÄÇ So let's
    define it here„ÄÇ define itself best„ÄÇCriiteterria„ÄÇ which gets self and X and y and
    the feature„ÄÇInice sees„ÄÇSoÔºå here„ÄÇWe do our greedy search„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we say best gain equals -1„ÄÇ So we want to go over all the features and all
    the feature values and calculate the information gain„ÄÇSo„ÄÇNowÔºå let's say split
    index and„ÄÇüéºSplit threshold equals none„ÄÇ So both of them are none„ÄÇ And then we
    have our first loop„ÄÇ So4 feature„ÄÇIndex in„ÄÇFeature indices„ÄÇ And now we only want
    to select„ÄÇThe column vector of this x„ÄÇ So we say x„ÄÇX column equals„ÄÇX of„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we want to have all samples„ÄÇ but only at this index„ÄÇAnd then we want to
    go over all the possible thresholdsÔºå So we say„ÄÇThresholds equals NyÔºå unique„ÄÇOf„ÄÇÂóØ„ÄÇX
    column„ÄÇ So we don't want to check the same value twice„ÄÇ So like in this example
    here„ÄÇ we would check for 30Ôºå15Ôºå5Ôºå10„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_20.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 20 and 25„ÄÇAnd now we go over all the possible thresholds„ÄÇ So we say for th„ÄÇOld
    in thresholds„ÄÇAnd now we calculate the information gainÔºå we say gain equals self
    dot„ÄÇIn formation gain„ÄÇWhich gets y and the column vectorÔºå and also the current
    thresholds„ÄÇAnd then we say if gain is greater than our best gainÔºå then our new
    best gain is the current gain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And our best split index is the current feature index„ÄÇAnd our best split threshold
    is the current threshold„ÄÇAnd at the endÔºå we want to return them„ÄÇ So we return
    the split„ÄÇIndexÔºå first„ÄÇAnd then the split threshold„ÄÇSo this is the greedy search„ÄÇ
    And now we need another helper function to calculate the information gain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's say define information gain with cell„ÄÇAnd then it gets y and an X column
    and the threshold„ÄÇ So let's call a split thrash„ÄÇAnd now„ÄÇLet's say„ÄÇLet's have a
    look at the formulaÔºå again„ÄÇSo„ÄÇSorry„ÄÇ so we want to calculate the entropy of the
    parent and then a weighted average of the children„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_23.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/84741cb53cd03bf1f8d55692e7a39444_24.png)'
  prefs: []
  type: TYPE_IMG
- en: So„ÄÇWe calculate the parent entropy„ÄÇThenÔºå we generate„ÄÇA split„ÄÇThen we calculate
    the weighted average of the child entropiece„ÄÇ and then we return the information
    gain„ÄÇSo the parent entropyÔºå we can simply say parent„ÄÇEntropy equals entropy because
    we already have this function„ÄÇ So parent entropy of what entropy of y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we generate our split„ÄÇSo we say left indices and right„ÄÇIndices equals self
    dot split„ÄÇAnd we split based on this X columnÔºå and a split threshold„ÄÇSo„ÄÇWe need
    another help of functionÔºå so„ÄÇI hope you can still follow me„ÄÇ I hope I try to explain
    everything as clear as possible„ÄÇ but the code is a little bit more complex than
    usual„ÄÇ But yeahÔºå let's continue„ÄÇ So let's say„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: define split self„ÄÇAnd then it gets x column and our split threshold„ÄÇAnd here
    we can say left indices„ÄÇ So here we apply our question and we can use a function
    that is called nuy arc where and here we ask if the value of x column is smaller
    or equal than our split threshold„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this will return an array where with all where all the these conditions are
    true for all the values in our x column and we want to Latin this array because
    we only want a 1 d vector please check it for yourself„ÄÇ and we do the same for
    the right indicesÔºå so we say nuy„ÄÇArcÔºå where„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we check if X column is greater than our split threshold„ÄÇ And then
    we flatten this„ÄÇAnd then we return the left indices and the right indices„ÄÇSo now
    we have the split function„ÄÇAnd now„ÄÇ if we have the„ÄÇÂóØ„ÄÇWith this functionÔºå we generate
    the split here„ÄÇ And firstÔºå we check if„ÄÇLeng„ÄÇOf the left indices„ÄÇIs 0 or the length
    of the„ÄÇRight in the C is 0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we can immediately return 0 as information gain„ÄÇ And otherwiseÔºå we continue„ÄÇ
    So we calculate the weighted average„ÄÇ So we need the number of total occurrences
    of or the the the number of„ÄÇÂóØ„ÄÇOf our samples„ÄÇSoÔºå this is length of„ÄÇYÔºå and then
    the number of our left samples and the number of our right samples equals Lng
    left indices and Lng right indices„ÄÇ Then we calculate the entropy„ÄÇ So we say EÔºå
    L and E R equals entropy„ÄÇOf y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: of all the left indices and entropy of y of all the right indices„ÄÇAnd nowÔºå our
    child„ÄÇEntropy equals the weighted average„ÄÇ So we say N L divided by N times„ÄÇThe
    left entropy plus„ÄÇAnd now N R divided by n times the right entropy„ÄÇ So this is
    the child entropy„ÄÇ And now let's calculate the information gain„ÄÇ This is just
    the parent entropy minus the child entropy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and now we return it„ÄÇ So return the information gain„ÄÇSo now we have this so
    now this function is complete„ÄÇAnd now we can have to continue with the growing„ÄÇSo„ÄÇNowÔºå
    after we have selected the best criteria„ÄÇ we want to split our tree with this
    best feature and best threshold„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we say left indices and right indices equals self dot split„ÄÇ So again„ÄÇ here
    we can use our split function„ÄÇ So we say X„ÄÇOff„ÄÇAnd now we have to be careful„ÄÇ
    So now we want to have all samples„ÄÇÂóØ„ÄÇBut only the best„ÄÇFeature index„ÄÇSoÔºå this
    is a column„ÄÇAnd here we put in the best threshold„ÄÇAnd„ÄÇNowÔºå with our left and right
    indices„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can continue growing„ÄÇ So we go to the left and say left„ÄÇEquals„ÄÇAnd we call
    this function here in itself recursively„ÄÇ So we say self dot„ÄÇGrow tree„ÄÇAnd here
    we say X„ÄÇAnd then we only want the left indicesÔºå but we want all„ÄÇThe features„ÄÇSo
    and S Y„ÄÇ we want only want the left indices„ÄÇAnd then we also say depth plus one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now our depth is increased by one„ÄÇAnd now we do the same thing for the right
    side„ÄÇ So we say right„ÄÇEquals scroll3 with the right indices„ÄÇAnd now we return
    a new note in the middle„ÄÇ So this will get the best„ÄÇFeature„ÄÇThe best threshold„ÄÇAnd
    the leftÔºå and the right„ÄÇChilds„ÄÇ but no value here„ÄÇSo this is the growing method„ÄÇ
    And now the only thing left is to implement the predict method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I promise this will be fairly easy„ÄÇ So we only need one helper function„ÄÇ
    So we say return„ÄÇÂóØ„ÄÇNumpy array„ÄÇ And here againÔºå we use list comprehension„ÄÇ So
    we traverse our tree„ÄÇ So self dot traverse3Ôºå where we put in one sample for all
    the samples in capital X„ÄÇSo this is our predict method„ÄÇ And now let's implement
    the„ÄÇTraverse3 method as last thing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this will get self and one sample„ÄÇ And it also gets a note where we start„ÄÇ
    So we have to put in self dot root in the beginning because we start at the top
    and here„ÄÇWe also do this recursively„ÄÇ so firstÔºå we can check for the stopping
    criteria so we check if we have reached a leaf node so we can say if node is leaf
    node„ÄÇ So that's why we implemented this helper function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we return node dot value because if we are at a leaf„ÄÇ we also have a value
    and otherwise we apply our question so we go to the left or the right„ÄÇSo„ÄÇ we say
    if„ÄÇX of this note„ÄÇThe feature index that we stored„ÄÇIs smaller or equal than the
    threshold that we start at this note„ÄÇ Then we return„ÄÇSelf dot Traverse3„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we go to the left side ofÔºå and we put in X and note left„ÄÇ So that's why we
    start the left and the right notes at the notes„ÄÇAnd otherwiseÔºå we return„ÄÇSelf
    dot traverses3 of x and no dot right„ÄÇSoÔºå yeahÔºå this is the whole Traverse implementation„ÄÇ
    And now we are finally done„ÄÇ This is the whole implementation that we need for
    a decision tree„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And let's check this„ÄÇ So I've little little I've written a little help a script„ÄÇA
    test script„ÄÇWhere I implement this decision tree class and I use the data set„ÄÇ
    the breast cancer data set from the psychic learn module„ÄÇAnd I will split our
    data in training labels and training samples and test labels and test samples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then I will create our decision tree and fit the data„ÄÇ then predict the test
    data and calculate the accuracy„ÄÇ So let's check„ÄÇ let's run the script and check
    if everything's working„ÄÇNeeds a little time to„ÄÇÂóØ„ÄÇSorry„ÄÇ as traverse3 missing one
    positional argument note„ÄÇÂóØ„ÄÇSelf do traverse 3 X„ÄÇ Oh„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: self do root must be hereÔºå not here„ÄÇSo let's clear this and give this another
    try„ÄÇAnd fingers crosseds„ÄÇNote objectÔºå test no attribute feature index„ÄÇSo this
    isÔºå how did we call itÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Just featureÔºå sorry„ÄÇIf note that„ÄÇFeature„ÄÇThrehold„ÄÇThe thresholdÔºå threshold yo„ÄÇSoÔºå
    one more time„ÄÇI hope I have no more typos„ÄÇSo yeahÔºå now we have the accuracy and
    everything's working„ÄÇ And yeah„ÄÇ I hope you enjoyed this tutorial„ÄÇ If you liked
    itÔºå please subscribe to the channel„ÄÇ And in the next tutorialÔºå we will continue
    by extending this decision tree to the random forest model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå see you then bye„ÄÇüòä„ÄÇ![](img/84741cb53cd03bf1f8d55692e7a39444_26.png)
  prefs: []
  type: TYPE_NORMAL
