- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂÆòÊñπÊïôÁ®ãÊù•Âï¶ÔºÅ5‰Ωç Hugging Face Â∑•Á®ãÂ∏àÂ∏¶‰Ω†‰∫ÜËß£ Transformers ÂéüÁêÜÁªÜËäÇÂèäNLP‰ªªÂä°Â∫îÁî®ÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû
    - P36ÔºöL6.4- LewisÁöÑÂú®Á∫øÁõ¥Êí≠ËÆ≤Ëß£ - ShowMeAI - BV1Jm4y1X7UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OkayÔºå so maybe we can get started All rightÔºå so today we're looking at chapter3
    of the Huging F course„ÄÇAnd this chapter is kind of like bringing together all
    of the components that we've been looking at in the first two chapters„ÄÇ so just
    as a recap in the first chapter we looked at sort of the concepts around the transformer
    and some concepts around tokenization„ÄÇAnd pre training„ÄÇAnd in chapterpt 2Ôºå we
    started taking a deep dive into what really happens inside this pipeline API that
    we were using a lot in the first chapter and trying to really unpack sort of how
    to train a model or at least how to understand the inputs and outputs of a model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And also the inputs and outputs for the tokenizer„ÄÇSo today we're actually going
    to bring this all together and we're going to do a couple of things„ÄÇ we're going
    to look at the datas libraryÔºå which is this very nifty library developed by hugging
    face for processing data sets of more or less any size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we're going to train our very first model using something called the
    trainer and the trainer is an API which basically wraps a lot of the complexity
    around writing your own training loop in Pytorch or TensorF„ÄÇAnd then we're going
    to sort of take a like we're going to kind of unpack what's going on in some sense
    in that trainer by writing our own training loop„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In Pytorage and this will be a great excuse to introduce a very cool library
    that Sylvanna has developed called Huging Face Acccceerate„ÄÇAnd this library is
    really designed for doing what's called distributed training„ÄÇ so how you can kind
    of speed up your training when you've got multiple GPUs or potentially even TUs„ÄÇ
    which is what we'll see today in Google Coab„ÄÇAnd of courseÔºå if there's any questions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: just ask them in the chat at any time and I'll be pausing at various points
    to take„ÄÇIt's time to answer them„ÄÇAnd so maybe just to start„ÄÇThe kind of ecosystem
    that we kind of have a hugging piece at the moment roughly revolves around these
    sort of components„ÄÇ So we've got the hubÔºå which we've seen several times now already„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we use the hub for both loading models in transformersÔºå loading tokenizers„ÄÇ
    and also loading data sets„ÄÇAnd then transformers interacts with the data sets
    library„ÄÇ as we'll see todayÔºå as a way of training our models„ÄÇAnd then once the
    models are trained„ÄÇ we can then push them back to the hub so that then we can
    share them with our colleagues and we can also interact with them using some of
    the widgets that are developed by the Huging F team„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå with thatÔºå let's„ÄÇTake a quick look at the dataset sets library so„ÄÇThe data
    sets library is a kind of general purpose library for processing data sets of
    more or less any type„ÄÇ so originally it was specifically for text data sets for
    NLP„ÄÇBut since then„ÄÇ it's kind of grown into something that can handle imagesÔºå
    audioÔºå and over time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we'll have things like time series and so on„ÄÇAnd the main power of this library
    is that it provides a kind of uniform API for processing data and at least for
    me personally„ÄÇ this has been one of the biggest productivity gains in my sort
    of career as a data scientist because previously like I was always having to manipulate
    different types of data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: CSVÔºå Jason textÔºå whateverÔºå and each time it was always a little bit idiosyncratic
    and you had to write your own sort of custom like functions for dealing with all
    these data types and the data sets library provides„ÄÇA very simple API where you
    can basically load a data setÔºå more or less in one line of code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then you can process it using something called the map function„ÄÇ which we'll
    discuss in more detail today„ÄÇAnd more or less with this map function„ÄÇ you can
    kind of do extremely like fast and crazy processing„ÄÇ even on monster data sets
    that are like you know a terabyte in size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can actually process this on your laptop because this does a very clever
    kind of lazy loading of memory using something called Apache arrow„ÄÇOkayÔºå so let
    me just have a quick look„ÄÇGreat„ÄÇüòäÔºåCool„ÄÇ so maybe what we can do is we can start
    by looking at this introductory video on the DiocS Library„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_1.png)'
  prefs: []
  type: TYPE_IMG
- en: So I'm going to play this and please let me know if you can't hear it„ÄÇThe Egen
    F dataset sets libraryÔºå a quick overview„ÄÇThe Eing F Datas Library is a library
    that provides an API to quickly download many public dataset sets and pre them„ÄÇThis
    video well explore how to do that„ÄÇSt dig part is easy with the load that asset
    function„ÄÇYou can directly download and cache a dataset from its identifier on
    the dataset app„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here we fetch the M PC dataset set from the G benchmark„ÄÇ which is a dataset
    set containing pairs of sentences where the task is to determine the power phase„ÄÇThe
    object returned by the Lo dataset function is the dataset diict„ÄÇ which is sort
    of dictionary containing each split of a dataset„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Can access each split by indexing with its name„ÄÇThis split is within an instance
    of the dataset class with columns here„ÄÇ sometimes1Ôºå sometimes two label on IDX
    and root„ÄÇWe can access a given element by its index„ÄÇThe amazing thing about the
    Ugen phase Data library is that everything is saved to disk using apppasharo„ÄÇWhich
    means that even if your dataset is hugeÔºå you won't get out of R„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: only these elements you request are in memory„ÄÇAccessing a slice of your dataset
    is as easy as one element„ÄÇ so result is when dictionary with list of values for
    each case here the list of labels„ÄÇ the list of first sentences and the list of
    circumstances sentences„ÄÇThe features attribute of a data gives us more information
    about its columns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: particularicular we can see hereÔºå it gives us a correspondence between the integers
    and names for the labels„ÄÇ0 stands for not equivalent and one for equivalent„ÄÇTo
    process all the elements of our data set„ÄÇ we need to tokenize them„ÄÇHave a look
    at the video prepoed sentence pairs for a refresher„ÄÇ but you just have to send
    the two sentences to the decokenizer with some additional keyword arguments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here we indicate a maximum length of 128 and pet inputs shorter than in length
    trunc get inputs for longer„ÄÇWe put all of this in the tokenized function but we
    can directly apply to all the splits in our data set with the map method„ÄÇAs long
    as the function returns a dictionary like object„ÄÇ the map pattern will add new
    columns as needed or update existing ones„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: To speed up prepoing and take advantage of the fact all tokener is back by rust
    thanks to the Eging phase tokenos library„ÄÇWe can process several elements at the
    same time in autokenized function using the batch equal2 argument„ÄÇSince the tokenizer
    can adult list of first sentencesÔºå list of second sentences„ÄÇ the tokenized function
    does not need to change for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can also use printer processing with the MA methodÔºå check out its documentationation
    link below„ÄÇOnce this is doneÔºå we're almost ready for training„ÄÇ we just remove
    the columns we don't need anymore with the remove columns method„ÄÇ we name label
    to labels since the models from the Uing phase transformforms library expect that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And set the output format with a desired packetantÔºå dorchÔºå turns of flu on imp„ÄÇIf
    needed„ÄÇ we can also generate a short sample of a data set using the select method„ÄÇ![](img/a1136557282a500704797139dc43d7b8_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: GreatÔºå so that was a whirlwind tour of the Do Library„ÄÇ Are there any questions
    at this stage before we dive into some code„ÄÇOkay so feel free to post them in
    the chat as they come so one thing I just want to mention that might be useful
    so on the Hi phase hub„ÄÇWe have looked at the models„ÄÇHub in the last lesson„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and today we're going to kind of be looking a lot more closely at the data sets„ÄÇAnd
    there was a question last week about sort of how do you figure out like a good
    choice of model for„ÄÇ sayÔºå multiclass versus multilaÔºüAnd at the time we sort of
    realized that there's no sort of easy way of like filtering these types of models„ÄÇ
    but it turns out that for data sets there is a little bit better search for thisÔºå
    so for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if I'm working on a new projectÔºå one of the fastest ways to get results early
    is to basically build a sort of baseline or prototype using some public data„ÄÇ
    this is often much cheaper and faster than waiting for you know to get permission
    to access data in your company or waiting for some domain experts to help you
    with the labeling„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so for exampleÔºå if we were looking at like maybe like a multiclass thing„ÄÇ
    maybe I would pick a text classification„ÄÇAnd then there are other tags here that
    I can sort of filter by„ÄÇ so just in case I was interested in question answeringÔºå
    I can select that„ÄÇAnd then inside text classificationÔºå there are a set of subtasks
    that we can look at„ÄÇ So for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: here I can maybe select multi label classification„ÄÇAnd then this will return
    a range of data sets which are candidates for the multilabel case„ÄÇ so perhaps
    I'm doing something about fake newsÔºå then I can look at data set called fake news
    English and vice versa so the datasets hub is a great way of like bootstrapping
    your machine learning projects at work„ÄÇ and I've found this is a great way also
    of finding some interesting data sets that maybe go beyond the standard ones that
    you often see in NLP„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so almost everyone uses IMDV movie reviews to show their training but there's
    a lot of extra things here that are quite interesting and challenging because
    many of these are community provided„ÄÇAnd just a side noteÔºå if you have an interesting
    data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That you think would be valuable to the communityÔºå you can either contribute
    it directly to the data sets repository„ÄÇAnd so inside the datasets repositoryÔºå
    we have a datasets folder which has all of these hundreds of data sets that are
    basically reviewed and curated by the Huging piece team so you can open a pull
    request and there's instructions in the documentation on how to do this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Or if you wish to provide a community data setÔºå which is something that more
    or less you don't have to integrate into the library directly„ÄÇ you can also do
    that and there are some links in the documentation on how to basically submit
    data sets to the hub„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look if there's any questions done so far„ÄÇGreat„ÄÇ so maybe Oma
    can put some links to that community„ÄÇËØ∂„ÄÇVersus canonical des„ÄÇOkay so there's a
    question from SRM sumUma datas library looks very specific to NLP I couldn't find
    many data sets related to vision„ÄÇ is there a limitation to host vision related
    dataÔºüSo the short answer is no„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there isn't a limitation in the libraryÔºå it's just that we haven't devoted a
    lot of effort to integrating vision dataset sets and if you have some ideas of
    vision dataset sets you would like to add by all means feel free to either open
    a PR in the dataset sets library„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Or just reach out in the forums basically„ÄÇAt the momentÔºå we haveÔºå if I look
    hereÔºå let's see„ÄÇ Can I find vision„ÄÇDo we have tag revisionÔºüLet's have a look„ÄÇSo
    I don't think we have a tag for vision yet„ÄÇBut I know that we haveÔºå let's see„ÄÇ
    I think we have CFAR„ÄÇWe don't have CFfaÔºå maybe imt„ÄÇInteresting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I was fairly sure we had these data sets„ÄÇ OkayÔºå so„ÄÇ so maybe some of these vision
    data sets are not yet's because I this okay„ÄÇCAÔºå yeahÔºå good Okay„ÄÇ so we have CF
    10Ôºå CF 100 and these data sets you can use using the data library„ÄÇAnd there's
    also othersÔºå I think I'm pretty sure mnes is here„ÄÇ So there's fashion mnesist„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: mist and so on„ÄÇSo great question„ÄÇSo I think I can ch the link in the chat here„ÄÇ
    So this will„ÄÇ this is the link to Mist„ÄÇLet's put this here„ÄÇüòîÔºåCool„ÄÇ so I hope that
    answers your question S our consumer„ÄÇÂóØ„ÄÇüòäÔºåOkay„ÄÇ so now maybe what we can do to
    start with is let's have a sort of look at the coabab for the data sets library„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we kind of get our hands dirty with thisÔºå so„ÄÇAs usual„ÄÇ the first thing I
    want to do is install data sets and transformers„ÄÇ and I'm going to use the capture„ÄÇ![](img/a1136557282a500704797139dc43d7b8_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_6.png)'
  prefs: []
  type: TYPE_IMG
- en: Magic command to capture all the Pip junk that comes out„ÄÇÂóØ„ÄÇüòäÔºåSo once this will
    take a few minutes„ÄÇAnd remember that the basic process in the sort of transformers's
    workflow is we first we need a tokenizer to convert our text raw text into input
    IDs„ÄÇAnd then we need a model to process those input ID and convert them into numerical
    outputs that we can build predictions on„ÄÇSo basically hereÔºå I'm just instantiating
    a tokenizer and a model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is going to be Bt based uncasedÔºå and this model is an auto model for
    sequence classification„ÄÇ so it's a text classification head that we're adding
    on top of the pretrain model„ÄÇAnd this is just a little code snippet that shows
    an example of how to process basically the raw sequences„ÄÇ convert them into input
    IDs„ÄÇAnd then pass this and the labels through theÔºå through the model„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but we saw that last weekÔºå so should be clear the thing that is more interesting
    for us today is to start looking at the data sets they read so„ÄÇThe first thing
    or the most common thing you're going to encounter„ÄÇIs the load dataset set functionÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And if we just have a look at this load„ÄÇOops„ÄÇLoad data set function„ÄÇYou can
    see that it has a lot of different arguments that you can provide the most common
    ones that I use are the path„ÄÇ which is essentially the name of the data set this
    is what you will see for exampleÔºå on the hub„ÄÇ so in this case we have glueÔºå but
    it could be MNIST it could be whatever„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And many data sets on the hubÔºå they often have configurations which are like
    sortized subsets„ÄÇ so for example glue is a benchmark which has many different
    tasks and each task has a name so here we have the Microsoft paraphrase comprehension
    task„ÄÇAnd so if you want to access a subtask or a subset in a given data setÔºå you
    use this name argument„ÄÇAnd then the other one that we'll see that is really common
    is to specify the split„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So a lot of the time by well by defaultÔºå it will return all the splits that
    are defined in the„ÄÇIn the library so here we're getting a train validation and
    test split„ÄÇ but sometimes you just want the train split and so you can specify
    those explicitly and I'll show you how„ÄÇOkayÔºå so now when we do low data setÔºå we
    get a data sets object and there are two sort of types of objects you'll typically
    see so the most common one you'll get at the start is something called a data
    dit„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this isÔºå you can think of it as basically a dictionary where the keys are
    just a string that correspond to the split„ÄÇAnd the values are something called
    a data set object„ÄÇSo let's just have a look at one of these examples here„ÄÇ so
    if I index into the train key or the train setÔºå I'm going to get now a data set
    object„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And if I look at the first element in that data objectÔºå I've got an IDÔºå I've
    got a label„ÄÇ and I've got the two sentences that I need to figure out if one is
    a paraphrase of the other or not„ÄÇSo in this caseÔºå we can see that the first sentence
    says Amrosy accused his brother whom he called the witness of deliberately distorting
    his evidence„ÄÇAnd the second sentence saysÔºå referring to him as only the witness
    and Rosy accused his brother of deliberately destroy the evidence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that's a pretty good example of the second sentence being a or maybe the
    first one being a paraphrase of the second one„ÄÇ so it's a bit shorter and it captures
    the same information„ÄÇAnd we can see that the label here is oneÔºå which as a guessÔºå
    is probably indicating that that's true„ÄÇIt is a paraphrasible„ÄÇOkayÔºå so just to
    like unpack a little bit this data set dit versus data set thing so because this
    was a bit confusing to me the first time I saw this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we know that the raw data sets are given by this data set diict object„ÄÇAnd
    this data set D object has these keysÔºå rightÔºå so we have train validation and
    Teset„ÄÇAnd if we look at just like a normal sort of Python dictionaryÔºå we look
    at the values„ÄÇ then we get now a dictionary or we get a list of the different
    values„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So just think of dataset as a dictionary which maps keys or splits to dataset
    sets and the thing that you do nearly all of your work or your heavy work on are
    actually the data set objects so if we look at a data set object„ÄÇSoÔºå this raw„ÄÇTrain
    data„ÄÇ This data object has a large number of operations that we can do„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we're going to see some of them today„ÄÇ so we can add columns„ÄÇ We can do
    filtering„ÄÇ we can load from different formats„ÄÇ We can extract some information
    about the data„ÄÇ So maybe let's have a look at that„ÄÇ If we look at the the info
    attribute of a data set„ÄÇ This will often tell us maybe we can print it„ÄÇThis will
    okayÔºå that like that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this will tell us a bit of a description about the data set itself„ÄÇ And there's
    a bunch of information about the data types and so on„ÄÇUSo yeah„ÄÇ basically just
    remember that data set is the thing that we're going to be doing most of our work
    on and data D is just a way of collecting all the splits together„ÄÇSo let's just
    see if there's any questions none so farÔºå OkayÔºå good„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So one of the important aspects of a data set is that it has essentially types
    so one of the things that you may have experienced in your work is that most of
    the data you deal with is very like messy or maybe it has like like a CSV file
    and you've got a mix of strings and numbers and so on so forth„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what the datasets library does is it defines very explicitly what the types
    should be for every column in the dataset set„ÄÇAnd so this is very useful because
    it allows you to catch errors earlier and also to do some very fast processing„ÄÇSo
    if you look at the features attribute of a data set„ÄÇWhat you'll get is a dictionary
    which shows you essentially the column name as a key and then the data type for
    that column and so we can see here that indeed sentence one is a string„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we get something called a value type and here it specifies a string„ÄÇBut the
    other one that's kind of interesting is maybe the label so here the label column
    it's not just an integer I mean it could be an integer„ÄÇ but data sets provides
    something called a class label type and this class label type contains information
    about the number of classes that we have or the number of unique labels that we
    have„ÄÇ what the names of those labels are„ÄÇAnd those are the more basically the
    two things that you need to know is just the names and the number of classes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we can see thatÔºå for exampleÔºå the ID has just got a data type of integer
    32„ÄÇAnd so one of the things you can do with these featuresÔºå so one thing that's
    like kind of nice„ÄÇ if we so features are just a dictionary so we can access the
    values by key„ÄÇSo if we get the label„ÄÇ this is going to give us this class label
    type„ÄÇAnd this class label type has a couple of handy functions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so one function that I often use a lot is how do I convert my numerical features
    so label one into something that I is like human readable so what I can do is
    there's an int to string function„ÄÇAnd if I put label one here„ÄÇIt should tell meÔºå
    indeedÔºå that„ÄÇThat corresponds to equivalent„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which means that one sentence is a paraphrase or the other„ÄÇAnd just as a sandy
    checkÔºå if we do zero„ÄÇ then it should be the same for not to equivalent„ÄÇAnd so
    this class label feature„ÄÇHas some documentation on the hub with other functions
    you can look at„ÄÇ but I find this is quite a powerful way of quickly switching
    between labels that are numbers and labels that are strings so that you can understand
    what's in your data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a lookÔºå are there any questions„ÄÇOkayÔºå so there's a question from
    IM homess„ÄÇ is there a way to benefit from the convenience of the data set library
    but use a data set that might be private„ÄÇ for exampleÔºå customer dataÔºüYesÔºå that's
    a great question and the answer is yesÔºå you can so there are„ÄÇüòäÔºåSo previously before
    I joined Hugingface I was working for a telecom company and everything was like
    completely closed off in you know on premise we couldn't use Huging face hubub
    so I actually had to solve this problem so the way you could do this or the way
    I used to do this most frequently is I would actually use pandas„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I would say input pandas as PD„ÄÇAnd then I would create my data frame„ÄÇ So
    let's suppose I'm going to just make a dummy data frame„ÄÇ So let's say I have„ÄÇOops„ÄÇMy
    data frame as followsÔºå and I'm going to sayÔºå okay„ÄÇLet's call this a text„ÄÇThen
    I'm going to put„ÄÇHello world„ÄÇAnd then maybe another element today„ÄÇOkayÔºå and then
    I'll add another column„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is maybe the say the label„ÄÇAnd then maybe this is going to be positiveÔºå
    positive„ÄÇSo this is a super simple data frameÔºå but this is something that I would
    load locally„ÄÇAnd of course„ÄÇ pandas is amazing for data processing„ÄÇ But if you
    want to use the the data sets functionality„ÄÇ what you can do is create your own
    data setÔºå your own custom data set„ÄÇ So from data sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can import„ÄÇThe data set objective itself„ÄÇAnd then what we can do is we can
    create„ÄÇData set„ÄÇFrom the„ÄÇLet's seeÔºå so I think it's fromÔºå is it from pandasÔºå I
    thinkÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And then this should fingers crossed create now a data set object„ÄÇWhich has
    the features of text and labelÔºå has two rows„ÄÇ And if we nowÔºå for example„ÄÇ look
    at all the elementsÔºå we can see that indeedÔºå we've now got our own custom data
    set„ÄÇSo this is more or less how I used to work 90% of the time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I hope that answers your question I'm homes there are other ways of„ÄÇLoading
    data„ÄÇ you can load it from CSVÔºå you can load it from Jason„ÄÇAnd I think that more
    or less covers most of the use cases„ÄÇThe only time things get a little bit painful
    is if you're dealing with very large data sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you knowÔºå things that maybe don't fit into a pandas data frame„ÄÇ but there's
    a streaming functionality that has just been implemented into the data sets library„ÄÇAnd
    so yeahÔºå I think basically you can cover almost all use cases this way„ÄÇOkayÔºå coolÔºå
    so„ÄÇÂóØ„ÄÇWhat we've just done so far is we've just loaded our raw data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the thing that we we would like to do right is we would like to tokenise
    this so what we've been doing in all of the lessons so far is we've basically
    been tokenizing kind of like string by string or like maybe a list of strings
    so if we look at this example here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then when we do the tokenization„ÄÇYou can see that„ÄÇTokenizing the first sentence
    column is giving us now a list of input IDs„ÄÇ so we have now basically a set of
    input IDsÔºå well let's just have a look at these guys„ÄÇÂóØ„ÄÇSo yeah„ÄÇ we've got a big
    list of input IDs corresponding to the first sentence and also to the second sentence„ÄÇAnd
    remember that these Is is what are we used to feed into the transformer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: These are the things that go into the embedding layer and then into the transformer
    stack„ÄÇ And at the endÔºå we get something like budgets we can make predictions with„ÄÇOkayÔºå
    so„ÄÇThat's the stuff we've been doing all the time and you can alsoÔºå as I think
    we may have seen„ÄÇ you can convert your IDs back to tokens using the convert IDs
    to tokens„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But the thing that is like maybe most interesting or most common is how do you
    basically tokenise the whole data setÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And the most common way to do this is to define a function„ÄÇYou can call it whatever
    you want„ÄÇ here it's called tokenized function„ÄÇAnd what this function will do is
    it will operate on every row of the data set and apply whatever you define the
    operation to be in that function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So once you've defined your functionÔºå you can then„ÄÇApply a map„ÄÇOnto your dataset
    and then this will automatically tokenize every single row in your data„ÄÇ so we've
    got all the raw stringsÔºå sentence oneÔºå sentence two„ÄÇ and it's converted them automatically
    into input IDs and also this attention mask that we saw last week where we needed
    to figure out how to sort of disable the padding tokens from the attention mechanism„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So„ÄÇüòäÔºåThis is a very powerful way of kind of in just more or less one or two
    lines of code„ÄÇ automatically tokenizing every single example in your data set
    and it's very fast it can be basically multi processcesed and it can also be run
    in a batched way on a GPU to be even faster„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But maybe let's just sort of look at a sort of very simple example just to break
    down what's going on here„ÄÇLet's suppose I wanted to do something which is quite
    common„ÄÇ maybe adding a column now that there are faster ways of doing this in
    data sets„ÄÇ but I'm going to show you how we could do this with a function„ÄÇSo let's
    add a column„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what this column should expect is an exampleÔºå or let's just maybe even makeiaÔºå
    it's a row„ÄÇ it's a row in our data set„ÄÇAnd the main thing that this function is
    to return is it has to return a dictionary„ÄÇAnd the reason for that isÔºå if you
    look at„ÄÇOne of our examples from the training set„ÄÇ you can see that it's kind
    of like a dictionary right we've got keys for the column name and value for the
    actual element or you know the element in that say cell if it was a table„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So what we need to do is we need to return„ÄÇIt's strange„ÄÇSeems a coab does two„ÄÇSo
    we need to return a dictionary and let's justÔºå I'm going to make something up„ÄÇ
    I'm going to say this is a new column„ÄÇAnd I'm going to just make all the value
    of menu new column be„ÄÇJust a wordÔºå hello okay„ÄÇAnd if we do this„ÄÇThen if I take
    my raw data sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Well let's just take maybe the raw training dataset set„ÄÇJust to keep it simple„ÄÇThen
    when we apply mapÔºå we just need to feed at this function„ÄÇAnd it will then automatically
    create a new column„ÄÇAnd just add it to our raw data set„ÄÇNow one thing you should
    be aware of is that this operation is not in place„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so if you're familiar with pandasÔºå there's often operations that are in place„ÄÇ
    which means you just run this line and then it kind of changes the state of the
    object in data sets most„ÄÇ if not all operations out of place and so what this
    would mean is that if you wanted to actually have that column stored in your memory„ÄÇ
    you would create you know data set with say extra column one you column„ÄÇAnd you
    would then do„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you knowÔºå that equals the map„ÄÇAnd thenÔºå when you„ÄÇLook at„ÄÇThis„ÄÇYou now see that
    we have a new column with Ho„ÄÇSo that's been now sort of stored in the memory of
    this new data set object„ÄÇOkayÔºå so just to recapÔºå you a function and the function
    has to always return a dictionary where the key is the name of the column and
    the value is the value you want for that row„ÄÇCoolÔºå so let's take a small thing„ÄÇ
    Let's see„ÄÇI am Homemes just asking„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: please could you explain what the attention mask does again okayÔºå greatÔºå very
    good question„ÄÇSo let's have a look at this example here„ÄÇSo in this exampleÔºå we've
    got two sentences„ÄÇAnd we can see thatÔºå this is maybe not a good exampleÔºå maybe
    let's add some padding„ÄÇÂóØ„ÄÇOkayÔºå actually„ÄÇ what I need to do to show you an example„ÄÇIs
    I'm going to„ÄÇYeahÔºå I'm going to do this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I'm going to„ÄÇI'm going to apply truncation in my functionÔºå and I'm going
    to add some padding„ÄÇAnd then this is just to show you„ÄÇWhat we're talking about
    here„ÄÇOkay„ÄÇ so here I've just tokenized the raw data sets and I've got this tokenized
    data sets object„ÄÇAnd list„ÄÇGet the first elementÔºå which hopefully will show us„ÄÇÂìàÂìà„ÄÇÂóØ„ÄÇOkayÔºå
    this is a bit of a messy one„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but let me just try to summarizerise so remember in the last chapter„ÄÇ we looked
    at this concept of padding and the reason we had to do padding is that all the
    operations that we do inside the transformer are basically matrix multiplication„ÄÇAnd
    when you do matrix multiificationÔºå you want to make sure that the matrices you're
    operating on are more or less square„ÄÇ So if I haveÔºå for exampleÔºå one sentence
    and I represent this as a vector„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then in order to compare one sentence to another sentence„ÄÇ it helps if those
    arrays or those vectors have the same size„ÄÇAnd so padding is a technique where
    you can basically in the simplest case„ÄÇ look at the longest sentence in your batch
    and then just put a zero at the end of every other sentence which basically pads
    out to the length of the longest one and then this will guarantee that your sentence
    or your batch has all vectors of the same size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then when you stack them togetherÔºå youve notice something that's rectangular„ÄÇNow
    that's padding and more or less we do it for just computation reasons„ÄÇBut the
    problem that it introduces is that the attention mechanism you may remember from
    one of the earlier chapters„ÄÇ it basically takes an embeddingÔºå so these numerical
    representations that we have of the sequence and it then updates them to create
    something called a contextualized embedding and these contextualized embeddings
    essentially contain for every token in that sequence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they contain information that relates the sort of meaning of that token relative
    to the whole sequence„ÄÇSo an example would be if I have a sentence likeÔºå I don't
    knowÔºå time flies like an arrow„ÄÇ then flies is a verb in that case„ÄÇBut if I have
    another sentence„ÄÇ which is like fruit flies like a bananaÔºå then flies is an insect„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so the attention mechanism allows us to distinguish these two cases because
    the contextualized embedding that represents flies is different in those two cases
    and it uses the whole sequence to develop that representation„ÄÇNowÔºå because attention
    operates on every single token in the sequence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it also operates on padding tokens and this would be like a bit of a problem
    because these padding tokens are like these artificial things we injected just
    to make sure all the matrices are square„ÄÇAnd so the attention mask is a way of
    saying to the attention mechanism„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: pay no attention to the padding tokens„ÄÇAnd what an attention mask will look
    likeÔºüHere„ÄÇIt will have a bunch of onesÔºå so these will be all the tokens at the
    start of the sequence that are just the things we want to develop contextualized
    embeddings for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then at some point you will hit the start of the padding sequence„ÄÇ so you've
    got all your words and now you're just adding zeros to your token embeddings„ÄÇAnd
    here the attention mask will then switch to a zero„ÄÇ and that will when it goes
    through the attention layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will just disable attention computations on that„ÄÇSo I hope that explains
    your question I Homemes„ÄÇ it was a bit of a long winded one„ÄÇAnd so ones for all
    the input Is correct exactly right Yep„ÄÇ so all the input Is which are not zero
    or generally not zero„ÄÇ it will be a one and then it will be zero for all the input
    Is that are zero so we can have a look here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All the input IDs are on zero and now we've got the padding„ÄÇÂóØ„ÄÇIds„ÄÇOkayÔºå great„ÄÇ
    so there's another question which is from Resh MecheikÔºå can we do the reverse
    of a map functionÔºüÂóØ„ÄÇLet me think if I understand what you mean„ÄÇüòîÔºåSo what kind
    of example would you have in mindÔºå RashÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I'mNot sure I understand the questionÔºå but maybe you can write it in the chat
    and then I'll come back to it in a bit„ÄÇOkayÔºå so„ÄÇWe've seen how to„ÄÇDo tokenization
    across the whole dataset set using the map function„ÄÇAnd the„ÄÇI think that's more
    or less the main thing in this co„ÄÇ So I'm just going to delete this„ÄÇAnd the other
    thing„ÄÇüòäÔºåThat is worth pointing out is there's kind of two ways you can do padding„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So one way is to explicitly define the padding in the tokenization step„ÄÇ So
    what you do is you say padding true or however you wish to implement it„ÄÇAnd then
    when you do tokenization with the mat function„ÄÇ it will automatically pad all
    the sequences according to how you defined it„ÄÇNow„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the drawback with this is that when you do training„ÄÇMaybe your paddingÔºå for
    exampleÔºå here„ÄÇ picked the longest sentence in the whole data set and then paded
    everything out to the thing in the whole data set„ÄÇBut when we do trainingÔºå what
    we're really doing is we're doing training on batches and so one thing that is
    quite common is to do something called dynamic padding„ÄÇ which we'll see shortlyÔºå
    which is a way of basically sort of adding the padding tokens on the fly and this
    lets us do computations much more efficiently„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think let's take a look at that video now„ÄÇ![](img/a1136557282a500704797139dc43d7b8_8.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look at how we can do padding on the flow„ÄÇ![](img/a1136557282a500704797139dc43d7b8_10.png)
  prefs: []
  type: TYPE_NORMAL
- en: What is dynamic beddingÔºüIn the batchching inputs Together video„ÄÇ we have seen
    that to be able to group inputs of different lengths in the same batch„ÄÇWe need
    to add adding togans to all the shot inputs until by all of the sims„ÄÇHereÔºå for
    instance„ÄÇ the longest sentence is the third oneÔºå and we need to add five„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: two or seven petans to the other sentences to have four sentences of the same
    length„ÄÇWhen dealing with a word data setÔºå there are value being strategies we
    can apply„ÄÇSo most of one is to add all the elements of the data set to the same
    length„ÄÇ the length of the longest sampleÔºå this will then give us patches that
    all have the same shape determined by the maximum sequence length„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that patches composed from short sentences we have a lot of
    patting tokens„ÄÇ which will introduce more computations in the model we ultimately
    don't need„ÄÇTo avoid this„ÄÇ another strategy is to patch the elements when we batch
    them together to the longerest sentence inside the batch„ÄÇThis wayÔºå batches compose
    of short input voltage gets smaller and the batch containing the longest sentence
    in the dataset set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This will lead some nice speed on CPU and GPU„ÄÇSo the downside is that all batches
    will then have different shapes„ÄÇ which slow down things on accelerators like TUs„ÄÇLet's
    do have to apply both strategies in practice„ÄÇWe have actually seen to applied
    fixed padding in the dataset sets of a view video when we proposepo the R PCC
    dataset„ÄÇ aftering the dataset EnkenizerÔºå we applied the tokenization to all the
    data set with padding and procation to make all samples of lengths 128„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: As a resultÔºå if we pass this data set to a byy toch dataÔºå we get patches of
    shape patch size here„ÄÇ 16 by 128„ÄÇTo apply a dynamic paddingÔºå we must defer the
    padding to the batch preparation„ÄÇSo we remove that part from a tokenized function
    we still leave the transition part so that inputs that are bigger than the maximum
    lengths accepted by the model„ÄÇ usually 512 get trunccateated to that length„ÄÇThen
    we paddle samples poll dynamically by using a data curator„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Those classes in the Tos library are responsible for applyinging all the final
    preproing needed before forming a batch„ÄÇHereÔºå the decorulator with padding will
    pass the samples whose maximum length inside a patch of sentences„ÄÇWe pass it to
    the Pythto staalloor as a collate function and observe that the batch sheets generated
    at various lengths all way below the 128 from before„ÄÇDynamic pitching will almost
    always be faster on CPUs and GPUsÔºå so you should apply it if you can„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Remember to switch back to fixed bidding howeverÔºå if you run your training script
    on TU or need batches of fixed chips„ÄÇOkayÔºå so that was a nice explanation of dynamic
    paddingÔºå so just to summarize„ÄÇ if I have a data set we say a thousand examples„ÄÇAnd
    let's suppose that one example is just way longer than all the others„ÄÇ maybe there's
    an error or something„ÄÇSo if I just did my tokenization„ÄÇÂóØ„ÄÇOn this data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: as we just did before in the colaab„ÄÇ And we just paded to the maximum length
    of the longest example„ÄÇ Then all the samples will get pushed out to this very
    long one„ÄÇ And so we'll have a lot of zeros everywhere that we have to then do
    computations on„ÄÇ which are slower„ÄÇSo the alternative is that since we do most
    of our training in terms of batches that we just pad at the batch level„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so instead of padding for the whole data setÔºå we just look at the elements in
    a batch and we pad to the longest for example element in that batch or the longest
    example in that batch„ÄÇAnd what this will do is it will then require less computationÔºå
    so it will be faster„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But the downside as Svan explained is that each batch will have different sizes„ÄÇ
    so we're going to have you know maybe in one batch the sentence is only 10 tokens
    long for the longest one„ÄÇ so everything is 10 tokens longÔºå maybe the second batch
    is 30 and so on„ÄÇIn the Transformers library„ÄÇ we have what are called data colllators„ÄÇAnd
    these data collators are basically functions that allow us to kind of cleverly
    package together all these different size batches in a way that we can then do
    training efficiently„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's maybe look at some questions and then just revisit this col datac stuff„ÄÇSo
    there's a question fromÔºå let's see„ÄÇOkayÔºå so we've got a question from which says
    hi there's a limited labeled data sets that aren't English„ÄÇ would it make sense
    to machine translate a data set to a certain language or domain specific genre
    to fine tune on Yes„ÄÇ that's a very good question and generally highly recommended
    so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_12.png)'
  prefs: []
  type: TYPE_IMG
- en: A good example of this is there's a data set called MLQA„ÄÇ![](img/a1136557282a500704797139dc43d7b8_14.png)
  prefs: []
  type: TYPE_NORMAL
- en: And this is a multilingual question answering dataset and the authors of this
    dataset basically translated the S dataset„ÄÇ which is in English into several languages
    so I think they have GermanÔºå SpanishÔºå HindiÔºå Vietnamese„ÄÇ simplified Chinese„ÄÇSo
    this is one way of creating data in your domain or your language„ÄÇ which you can
    then build models on„ÄÇThe only drawback is that it's often being known that if
    you're training like very large transform models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they can learn somehow that„ÄÇThere's a translation that's been made and they
    will take shortcuts to get good performance so in the end the model that you get
    on your translated text it may be good just in the translated context„ÄÇ but if
    you then deploy this with real human interactions then this could be sort of out
    of domain and maybe the model has only really learned how to detect that it was
    translated and not how to actually solve the task you care about„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you have to be a bit careful with thisÔºå but it's something that is worth
    trying and it's something I've used as well in Switzerland where I live„ÄÇ there
    are four national languages plus English and so most of the time you don't have
    data in English it comes in Italian or French and so you need to do some of this
    translation tricks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇüòäÔºåAnd one thing that came out recentlyÔºå which isÔºå I think„ÄÇ quite cool is Facebook
    released a model called M2 M 100„ÄÇAnd this model is a translation model„ÄÇ so it's
    a sequence or it's an encoded decoder transformer that we saw in chapter1 and
    it can do basically translations across 100 different languages„ÄÇ so you imagine
    you've got basically 100 by 100 matrix of all languages and for every single pair
    you can do a translation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so this gives you 9900 pairs you can translate across and it's a very good
    model„ÄÇ so if you're looking to do translationÔºå I would recommend using this if
    your language is in the1 hundred0 that they cover„ÄÇOkayÔºå and then maybe to revisit
    Russia's questionÔºå how could you revert the column that was created before in
    the mapÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Okay now understand what you're askingÔºå so the question is let's suppose we've
    added this column„ÄÇAnd we've now got a data set with a new columnÔºå which says hello„ÄÇI'll
    show you the specific one for this example is like if I want to delete the column„ÄÇ
    then what I can do is I can remove columns„ÄÇAnd then here I just need to provide
    a list of column names so I could provide new column„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then this will delete„ÄÇThe new column and recover back the original data
    set that we had„ÄÇThat more or lessÔºå I think would cover most of the cases that
    you want to undo the map operation„ÄÇThe only time it won't really work is if you
    then do other things to your data set„ÄÇ maybe you change the content of these original
    columns and then you know undoing that isn't easily done„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå and then we have another question by SRM ZumaÔºå which is„ÄÇWhy is dynamic
    padding slow on TUs compared to GPUs is there any intuition behind this Okay„ÄÇ
    so this is testing my very limited TPU knowledgeÔºå but my understanding very limited
    understanding is that it's a sort of fundamentally different architecture for
    basically doing numerical or numerical linear algebra or algebraic equations or
    multiplications of matrices„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I think it's simply just due to the way that the chips are designed„ÄÇ that
    they are much more efficient if everything is a fixed sized matrix and you're
    not trying to do this like shuffling of data to do the collation„ÄÇBut OmarÔºå who's
    in the chat as Haka LamaÔºå used to work at Google so he can maybe provide a much
    better answer in the chat„ÄÇüòäÔºåÂóØ„ÄÇSo yeahÔºå this is it's a very good question„ÄÇ I will
    also really like to know a detailed answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that'll be couple one note„ÄÇüòäÔºåOkayÔºå so what I wanted to do before we start
    training the model is just have a quick look at this what this data col is doing„ÄÇSo
    in transformersÔºå there are different data collators to basically handle this dynamic
    padding„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the one that most of the time we use is data col with padding„ÄÇAnd what this
    will do is„ÄÇ let's see„ÄÇ can I see itÔºå YeahÔºå if I get a few samples from my data
    set„ÄÇWhat I've got here in my samples„ÄÇIs just a list ofÔºå you knowÔºå the tokenized„ÄÇInputs
    and they've got the„ÄÇThe same length for their inputes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So they've been that's probably because I„ÄÇAddded paddingÔºå so let me get rid
    of padding here„ÄÇSorry„ÄÇSo I'm just going to retokenize my data without the padding
    in the tokenizer„ÄÇBecause we want to do dynamic padding„ÄÇAnd one thing that's very
    cool„ÄÇ I didn't really mention is that in datas all the operations that you do
    are cached so they're basically stored as arrow tables in your hard drive and
    so then when you want to reprocess it basically checks have I done this computation
    before and if it does it will just load the cached version and that's extremely
    fast so you can see that took like a second and this is very cool if you've done
    something that you know you process like a million examples and then you restart
    the notebook and you don't have to wait again to reprocess them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the cache will just do that instantaneously„ÄÇOkayÔºå good„ÄÇ so so now I've got I've
    tokenized my data„ÄÇ I have no paddingÔºå so all the examples of different lengths„ÄÇAnd
    so„ÄÇWhat the data creator will do when I pass it through these samples is it will
    automatically resize in that batch„ÄÇAll the samples to the longest length in that
    batch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you can see here that the longest example has 67 tokens„ÄÇ And so what it's
    done now is it's created tensesorsÔºå each of which have essentially 67„ÄÇColumns
    you knowÔºå for each of the longest tokens and some of those will have padding„ÄÇ
    and that's what the data colator has done for us„ÄÇ and And then when we do training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will do all of that for us on the fly„ÄÇÂóØ„ÄÇYeah exactly so the question that
    SRum is asking about the TPUs is that square matrices are what GPUs like as well
    yes that's true so if I'm not mistaken the TPU context is to do with this distinction
    of whether you do dynamic padding or not and with dynamic padding we are kind
    of creating square matrices„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: On a batch levelÔºå but we're also moving data around„ÄÇ so we're having to sort
    of kind of dynamically create data on the fly with different shapes and my suspicion
    is that's the thing that maybe slows it down but it's a great question and I should
    look at the answer at some point„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: CoolÔºå so it's you've asked a very good question that stumped the two guys in
    the course„ÄÇ They great„ÄÇ thanks„ÄÇüòäÔºåOkayÔºå coolÔºå so that's more or less data sets
    tokenization„ÄÇüòä„ÄÇLet's now take a look at training„ÄÇSo„ÄÇWhat we're going to do is
    look at the trainer API„ÄÇSo'll launch the video„ÄÇSotrino API„ÄÇThe Transforms library
    provides a Traer API that allows you to easily function transformformals models
    and your data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The trainer class takes sure that assets your model„ÄÇ as well as the training
    IP parameters and can perform the training on any kind of setupÔºå CPUUÔºå GPU„ÄÇ multiple
    GPusÔºå TUus„ÄÇCan also compute the predictions on any dataset set„ÄÇ and if you provide
    a matrixÔºå evaluate your model on any dataset set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can also involve final data processing such as dynamic padding as long as
    you provide a tokenizer or given data coulator World5 CP in the MRRPC data set
    since it's relatively small and easy to preprocess„ÄÇAs we saw in the dataset sets
    of a view videoÔºå and we can proposepo it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You do not apply padding during the preproingÔºå as we will use dynamic padding
    before dataator with padding„ÄÇNote that we don't do the final steps of renaming
    removing ins or set the format to torch tensils„ÄÇThe trainer will do all of this
    automatically for us by analyzing the model signature„ÄÇThe last step before creating
    the trainer are to define a model and some training of epi parameterss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We saw to do the first in the model API video„ÄÇFor the secondÔºå we use the training
    argument class„ÄÇIt only takes a path to a folder where results and checkpoint will
    be saved„ÄÇ but you can also customize all the app parameters your trainer will
    use learning grade„ÄÇ number of training asÔºå etc„ÄÇIt's been very easy to create a
    trainer and launch a training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This should display your properties bar and after a few minutes if you're running
    on a GPU„ÄÇ you should have the training finished„ÄÇThe result will be however antiticclmatic
    however„ÄÇ as you will only get a training class which doesn't really tell you anything
    about how well your model is performing„ÄÇThis is because we didn't specify any
    metric for the evaluation to get those metrics„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we'll first gave the predictions on the wall evaluation set using the predict
    method„ÄÇIt returns a namem to poll with three fields predictionÔºå which contains
    the model predictions„ÄÇ level IDsÔºå which contains the levels if you let as a add
    web and matrixÔºå which is empty here„ÄÇ we're trying to do that„ÄÇThe predictions are
    the lus of the model for all the sentences in the dataset set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so an by array of shape 408 by2„ÄÇTo match them with our labels„ÄÇ we need to take
    the maximum look for each prediction to know which of the two classes was predicted
    we do this with the a max function„ÄÇThen we can use the matrix from the dataset
    library„ÄÇ it can be loaded as easily as a dataset set with a load metric function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and it returns the evaluation metric used for the dataset„ÄÇWe can see our model
    did learn something as it is 85„ÄÇ7% accurate„ÄÇTo monitor the evaluation matrix during
    trainingeeÔºå we need to define a compute matrix function„ÄÇ but does the same step
    as beforeÔºå it takes a name to hold with predictions on labels and must return
    a dictionary with the metrics we want to keep track of„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: By passing the epoC evaluation strategy to our training arguments„ÄÇ we tell the
    trainer to evaluate at the end of every epoch„ÄÇLunching a training inside your
    notebook will then display a progress bar and complete the table you see here
    as you pass every apo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_16.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a1136557282a500704797139dc43d7b8_17.png)'
  prefs: []
  type: TYPE_IMG
- en: OkayÔºå so that was a lot of information if you've never seen the trainer„ÄÇ it
    might be too many things at once so what we're going to do is kind of walk through
    this together and then hopefully at the end of this you'll then have all the tools
    you need to start training your own models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And solving your own problems„ÄÇSo in Google CoLab there are different runtime
    available„ÄÇ so by defaultÔºå most of the time it's like a CPUÔºå so there's no acceleration
    and so what you need to do is hit runtime„ÄÇ select the runtime type and for this
    notebook we're going to use a GPU later we'll see how to use a GPU„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_19.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a1136557282a500704797139dc43d7b8_20.png)'
  prefs: []
  type: TYPE_IMG
- en: So then we save this„ÄÇAnd now this is going to launch in Google Coab sort of
    a machine that has a GPU in the back end„ÄÇAnd one thing you can do„ÄÇ![](img/a1136557282a500704797139dc43d7b8_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: To check what kind of GP you're running is to do NviDdia SMI„ÄÇ![](img/a1136557282a500704797139dc43d7b8_24.png)
  prefs: []
  type: TYPE_NORMAL
- en: And let's see what we get„ÄÇIf it wakes up„ÄÇSo here we've got a Tesla Kaie„ÄÇ so
    these are most of the time„ÄÇsort of default GPU you getÔºå they're not super great„ÄÇ
    but if you look online you can often find there's people who have saved the state
    of things like P100 collabs and then you can just use them and then they're very
    fast„ÄÇAnd of courseÔºå if you reset the the run timeÔºå so this is sometimes a hack
    if I'm„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_26.png)'
  prefs: []
  type: TYPE_IMG
- en: Trying to„ÄÇGet a better GP„ÄÇ You can do factory reset„ÄÇ![](img/a1136557282a500704797139dc43d7b8_28.png)
  prefs: []
  type: TYPE_NORMAL
- en: And then this will just wipe the whole back end and hopefully should now still
    be a GPU yep„ÄÇAnd maybe we'll get something a little bit different to a KÔºå let's
    see what we get„ÄÇOkay„ÄÇ I'm still stuck with a KatieÔºå but sometimes if you're lucky„ÄÇ
    it will give you a P100 or even a Tesla„ÄÇOkay„ÄÇüòäÔºåSo we've got our GPU back to coab
    now„ÄÇ So again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we do the same thing we„ÄÇInstall our dependencies„ÄÇAnd what we have here in this
    code is just the same things we did before„ÄÇ so we load a data set„ÄÇAnd then we
    define a tokenizer from a checkpointÔºå so we're using B basincased„ÄÇ We have our
    tokenize function„ÄÇAnd what this tokenized function is doing is it's taking the
    two sentences„ÄÇ remember that this data set„ÄÇIs about trying to predict whether
    one sentence is a paraphrase of another„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we pass both sentences to the tokenizer„ÄÇWe use truncation is true so that
    if one of the sentences is longer than the maximum„ÄÇSequence length of BtÔºå which
    is 512 tokensÔºå it will just trunccate it to 512„ÄÇAnd then we tokenize everything
    and we define this data color so we can do dynamic padding„ÄÇSo I'm just going to
    run that„ÄÇAnd it should be pretty fast„ÄÇüòîÔºåOkayÔºå so so while that's running„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's start by looking at„ÄÇThis training arguments„ÄÇ this is the first thing that
    you encounter with the trainerÔºå so training arguments„ÄÇ you can think of it as
    just like a configÔºå it's basically a class where you can define various hyperpara
    for training„ÄÇAnd the only thing you need to specify is an output directory where
    all the information from the training will be stored„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Or if you save the modelÔºå this is where it will go so if we look at training
    arguments„ÄÇWe can see that let's seeÔºå can we see through here„ÄÇ you can see that
    we have this output directory and then there's a huge range of things like really
    a lot„ÄÇ you can specify the learning rateÔºå you can specify the parameters of the
    optimizer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can define the optimizerÔºå you can define callbacks kind of like fast AI„ÄÇ
    which is no surprise because Sil develop the trainer„ÄÇAnd these callbacks lead
    to control training in clever ways like early stopping and things like this„ÄÇSo
    I won't go through all of thisÔºå but there's basically a large number of parameters
    you can set and you can find that in the documentation for the trainer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And it comes with some pretty good defaultsÔºå so most of the time it will just
    work out of the box„ÄÇSo now we need to load a modelÔºå and we're specifying two labels
    because we've got just two classes„ÄÇ Is it a paraphrase or not„ÄÇAnd againÔºå it's
    a sequence classification model as we're doing text classification„ÄÇAnd then comes
    the next periodÔºå which isÔºå you know maybe the more complicated part„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we need to instantiate a trainerÔºå so a trainer more or less at minimum needs
    a few things„ÄÇ it needs a model„ÄÇSo this is the model that we're going to trainÔºå
    it needs training arguments„ÄÇ these are the things that define how the training
    will operate„ÄÇAnd it needs one of a few data sets„ÄÇ so it needs either a training
    data set or a evaluation data set or a test set and you can have one and none
    of the others„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it needs at least one thing basically needs at least one data set generally
    that you want to train on you can instantiate it without it„ÄÇ but generally you
    want to specify one„ÄÇSo here we're passing the tokenized data and that's important
    you don't want to pass the raw data to your trainer because it will feed it to
    the model and then the model will go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I don't know what you're doing what I should do with these strings„ÄÇAnd the other
    two things that are kind of interesting is passing a data coator„ÄÇAnd a tokenizer
    so providing these two arguments will do dynamic padding„ÄÇ which will be faster
    for training„ÄÇAnd the way it works is you say give me a colalleator„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and there are different types of data colalatorÔºå but the one we're using is
    the most common one„ÄÇAnd it also needs tokenizerÔºå so it's basically the way the
    data cl works is it kind of combines the tokenizer with the batching to work out
    how to arrange the inputs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so by providing these two argumentsÔºå we will then get some speed up in training„ÄÇÂóØm„ÄÇAnd
    so this will instantiate the trainer„ÄÇAnd„ÄÇLet's see if it works„ÄÇIt's crossed okay
    good so one thing that I usually do before I launch training is I just do like
    a sanity check that I can run the evaluate function because a lot of the time
    what happens is when you run train and or train you will train for some number
    of steps and then you will evaluate you know maybe at the end of an epoOC and
    that's kind of like one of the default strategies„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But then you know if your evaluationÔºå maybe your metrics are not implemented
    correctly or whatever„ÄÇ if your evaluation failsÔºå then you're going to fail after
    waiting for a whole training of one epoch and that's like really annoying so a
    sort of sanity check that I do is just to make sure that I can run evaluate„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And if that worksÔºå then I'm relatively confident that the training run will
    work and what you can see that this evaluation has done is it's given us some
    information about the number of examples in the validation set and it's provided
    us with a value of the loss and then some kind of runtime metrics„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: sort of how performance is this processing batches„ÄÇAnd so obviously this is
    a random model at the moment„ÄÇ we've just initialized Bert as a backbone and just
    stacked on like a linear layer with random weights so this is kind of a garbage
    loss so the goal will be after training for this to be going down„ÄÇCoolÔºå so I'm
    gonna now run the training„ÄÇ And this takes a few minutes„ÄÇ So while it's running„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we'll then watch the accelerate videoÔºå which will show us how to„ÄÇWellÔºå maybe
    not„ÄÇ maybe I'll just run this and then see if there's any questions„ÄÇOkay„ÄÇSo„ÄÇDK
    crazy diviv asks„ÄÇWe're using Bt base uncased and the article mentions word piece
    for Bt„ÄÇ but we Pip installed transformer sentence pieceÔºå are we're using it here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If I were to reverse engineerÔºå how would I programmatically get from checkpoint
    and figure out which tokenize„ÄÇ that's a great question„ÄÇSo„ÄÇThe question we have
    is„ÄÇ why are we doing Pip install transformformers sentence pieceÔºüSo this is an
    optional dependency„ÄÇ which is a different tokenization algorithm that is used
    by models like XLM Robbaa„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And it's listed as an optional dependency because it's a bit heavy„ÄÇ so if you
    don't want to use any models that don't require sentence piece for the tokenization„ÄÇ
    you can just leave it and it will work„ÄÇSo for example„ÄÇ if we just did PIip install
    with transformersÔºå we will get word peace for free„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will come part of itÔºå but if we want to use these other models we have to
    define explicitly the sentence piece dependency„ÄÇSo that's like maybe clarifies
    the first part of the question„ÄÇWe're always using wordpiece with Bt and we only
    install sentence piece for the models when we need it„ÄÇAnd the reason we have it
    in the course is because there are some cases later in the future parts where
    we're going to be using the sentence piece tokenizer and so it's just useful if
    we just always have access to it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so„ÄÇIf I were to reverse engineerÔºå how would I programmatically get from
    checkpoint to figuring out which tokenizer to useÔºü
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so maybe we can„ÄÇI'll answer this in two ways„ÄÇ so the simplest way is the
    following in transformers„ÄÇWe have an auto model„ÄÇAClas for different types of modelsÔºå
    so this auto model will just do the the encoding and we also have auto tokenizer„ÄÇAnd„ÄÇOhÔºå
    oops„ÄÇInput„ÄÇAnd these classesÔºå they do this I pairing„ÄÇ So if I take auto tokenizer
    from„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Pre trained„ÄÇ And I put B base„ÄÇUncaseed„ÄÇIt will automatically assign the tokenizer
    that was associated with that checkpoint and then load it into my tokenization
    of my tokenizer object„ÄÇAnd similarlyÔºå if I do auto model„ÄÇFrom pre trained„ÄÇ And
    I do the same„ÄÇÂóØ„ÄÇCheckpoint„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will automatically figure out that for that checkpoint I need these set of
    weights„ÄÇSo the most important thing that you need to know is just to make sure
    that you use the same checkpoint for both the tokenizer and the model when you
    do the from pretrained and so what I often do in my code is I'll have an explicit
    variable„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is Bt base uncaseops„ÄÇLike thisÔºå and then I'll just load this variable
    into all of my pre trained„ÄÇCalls so that then I know that they're matched„ÄÇü§ßÂïä„ÄÇOkayÔºå
    so that's sort ofÔºå let's say„ÄÇ standard way of how we link these things together„ÄÇÂóØ„ÄÇüòäÔºåNow
    you're asking„ÄÇ how could we programmatically get from checkpoint to tokenizerÔºüLet's
    have a look at what's inside„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: One of these model files„ÄÇËøôËøôËøô„ÄÇLet's just check that my training is working„ÄÇüòîÔºåYeahÔºåGood„ÄÇOkay„ÄÇ
    so in the model we have a config„ÄÇAnd„ÄÇThis config„ÄÇLet's see„ÄÇOkay„ÄÇ so the config
    tells us the name of the checkpoint„ÄÇÂóØ„ÄÇSo„ÄÇIf you wanted to programmatically make
    sure thatÔºå I mean„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's suppose you start with like the config„ÄÇI guess what you could do is you
    could then use this attribute as the thing that you feed to the tokenizer„ÄÇ SoÔºå
    for exampleÔºå I could do tokenizer equals auto tokenizer from pretrain„ÄÇAnd then
    I would take my modelÔºå I would take my configÔºå and then I would access what is
    itÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Is it nameÔºüÂì¶„ÄÇCan I exit this as an attribute„ÄÇYeahÔºå so I could do this„ÄÇSo I could
    load my tokenizer this way and then this would let's say guarantee that the checkpoints
    match„ÄÇ but to be honest this is like a bit complicatedÔºå so I would more often
    than not just recommend defining a variable with your checkpoint and then just
    feeding that variable into your tokenizer model„ÄÇSo I hope that answers that questionÔºå
    DÔºå crazyative„ÄÇüòîÔºåBut feel free to ask if it's not clear„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so our model is pretty close to being done with the training and you can see
    that by default in the trainer„ÄÇ every 500 stepsÔºå the model will run an evaluation
    on the validation set and log the loss„ÄÇBy default„ÄÇ So this is just the training
    loss„ÄÇBut the thing that Sylvan explained in the video is that what we really want
    to do is compute metrics like accuracy or F1 score or whatever„ÄÇAnd so the way
    you do this in practice is you define a function called compute metrics„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this metrics function has to basically return a dictionary where you have
    a key corresponding to the name of the metric„ÄÇAnd then the values that you would
    compute from your predictions„ÄÇAnd so in this example here„ÄÇ Sylvan is leveraging
    the Datas libraryÔºå which has its own metrics as well„ÄÇAnd then once you have loaded
    the metric for this task in glue„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you just have to extract the predictions and the ground truth labels„ÄÇAnd then
    you can just do metric„ÄÇcompute and this will automatically create this dictionary
    for you which you can then feed into the trainer as follows so you can do compute
    metrics like this and then what this will do is that every 500 steps by default
    it will then compute the metrics in addition to the training loss and this is
    quite handy because this is how you can track the performance of the model on
    the validation set as you train and you can make decisions about okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: is it getting better or„ÄÇ„ÅÇ„Åì„ÅÆ„ÄÇOkayÔºå so let's just have a look at this„ÄÇ So OkayÔºå
    so goodÔºå the model is„ÄÇTrained„ÄÇÂóØ„ÄÇAnd you can see that the training loss here is
    around 0„ÄÇ35„ÄÇ We saw when we did the evaluation lossÔºå it was roughly doubleÔºå and
    that was with the random weights„ÄÇ So nowÔºå if I run evaluate again„ÄÇI should see
    hopefully the loss has gone down„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's gone it's gone upÔºå that's very interesting„ÄÇüòäÔºåI'm not sure where that's
    the case„ÄÇSo I'm going to just say that's the demo gods being meant to me„ÄÇGenerally„ÄÇ
    this should have gone downÔºå unusualÔºå might just be a random fluctuation in what
    we've done„ÄÇOkay„ÄÇ and so just to sort of dig down a little bit into how we build
    this metrics function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The trainer has a predict method and you can just feed into that predict method
    data set from data sets„ÄÇAnd this predicts method will return an object called
    predictions„ÄÇAnd this's how look„ÄÇSo„ÄÇ this predictions„ÄÇIs a prediction output objectÔºå
    it's just like a data class which has attributes and the attributes it has are
    predictions„ÄÇSo if we look at predictions„ÄÇOops„ÄÇThat's not going to work„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It's just an array of basically all of the logicits from the model and we also
    have the ground truth„ÄÇLabels listed like as IDsÔºå so this is you know paraphraseÔºå
    not paraphraseÔºå not paraphrase„ÄÇ on and so forth„ÄÇSo the main work you often have
    to do when you build your compute metrics function is you need to convert your
    logicits into let's say„ÄÇ integers or label IDs because that's the thing that you
    want to compare against„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so one way to do that is to just compute an AGmax„ÄÇ which will basically
    say for every prediction in terms of logics„ÄÇ find the index with the largest or
    the highest logic„ÄÇAnd so then once we apply that„ÄÇ our pres now is a tensor of
    integers„ÄÇAnd then I can feed those predictions into my my metric from data sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then I can run compute on that and it should„ÄÇHopefully give us these values„ÄÇSo
    any questions about computing metricsÔºüOkay„ÄÇüòäÔºåSo I'm not going to run this last
    cell„ÄÇ it's just the same thing we did beforeÔºå it will run the training but instead
    of showing the training loss it will show the validation metrics that we use„ÄÇ
    but I encourage you to play with this yourselfÔºå maybe with a new data set just
    to understand how this is really working„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Okay so that's the trainer APIÔºå this is what I use let's say 90% of the time
    it's just very convenient„ÄÇ it works and it means I don't have to think too much
    about writing my own lowlevel training script or my training code which can often
    be errorprone right and so a good reason to use like highle APIs like the trainer
    or you know fast AI or Ptorch lightning or whatever is that they abstract away
    a lot of the boilerplate code which if you do it yourself will probably have mistakes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this has been kind of battle tested throughÔºå you know thousands of users„ÄÇOkay„ÄÇ
    so to wrap up the sessionÔºå I want to now look at something that's like really
    exciting for me„ÄÇ at least is Silvan developed a library could accelerateÔºå which
    saysÔºå you know„ÄÇ sometimes I really need to have control of the training loop„ÄÇAnd
    you can do this onÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: CPU and GPU pretty easily„ÄÇ There's lots of tutorials on doing that„ÄÇBut there's
    been a kind of say trend in the last few years towards having access to like multi
    GPU or TU machines and these offer in principle a lot of speed up because you
    can now distribute your training so you you can distribute your batches to these
    devices„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: do your computations on these devices and then do back propagation through the
    whole say cluster of devices„ÄÇBut in practiceÔºå it's been very difficult to do this
    as a beginner and that's because you have to understand all this distributed setup
    in Pytorrch or TensorFlow and again many ways to make errors and you know keeping
    track of how data is basically parallellyed across nodes is a bit of a pain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this accelerate library is designed to make this simple for us and so let's
    take a look at the final video„ÄÇIs it this one„ÄÇTer„ÄÇSupercharge your by doch training
    loop with eggging face accelerate„ÄÇThere are multiple setups on which you can run
    your trainingÔºå it could be on CPUUÔºå GPUusÔºå GPUs„ÄÇDistributed on one machine with
    several devices or even several machines„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: often called nodes with multiplepo devices„ÄÇOn top of that„ÄÇ there are new tweaks
    to make your training faster or more efficient„ÄÇ like mixed precision and dip speed„ÄÇEach
    of a setup or training trick requires you to change the code of your training
    loop in one way or another and to learn a new API„ÄÇAll were setups sound all by
    the trainer API and also have all field party libraries that can help„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The problem with those is that it can feel like a black box and that it might
    not be easy to implement the tweak to the training loop you need„ÄÇAccelerate has
    been designed specifically to let you retain full control over your training loop
    and be as nontrusive as possible with just four lines of code to add to your training
    loop you are shown of the example of the training loop video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate will install all the seteps and training tricks monsoons on the first
    slide„ÄÇIt's only one API to learn on master instead of 10 different ones„ÄÇMore specifically„ÄÇ
    you have to import and instant sheet an accelerator object that will handle all
    the necessary code for your specific set„ÄÇThen you have to send it to modelÔºå optimizerÔºå
    and data you're using in the prepare method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is the main method to remember„ÄÇAccelerate on those device placement„ÄÇ so
    you don't need to put your batchge on the specific device you're using„ÄÇFinally„ÄÇ
    you have to replace the lost dot backward line by Ac tall dot backward loss„ÄÇAnd
    that's all it„ÄÇAccelator also involves distributed evaluation„ÄÇYou can still use
    the classic evaluation loop„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: such as what we saw in the training group videoÔºå in which case all processes
    will perform the full evaluation„ÄÇTo use a distributed evaluationÔºå you just have
    to adapt your evaluation look like this„ÄÇ that along the evaluation that error
    to the acceleratedccelerator or per like training„ÄÇThen you can dismiss the line
    that places the batch on the proper device„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And just before passing your predictions and labels to your metric„ÄÇ use accelerator
    to gather to give the predictions and labels from each process„ÄÇA distributed training
    script has to be launched several times on different processesÔºå for instance„ÄÇ
    one per GPU you're usingÔºå you can use the Pytoch tools to do that if you're familiar
    with them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The Acrate also provides an easy API to configure your setup and launch your
    training script„ÄÇIn a terminalÔºå run accelerate Config and answer a small questionnaireer
    to generate a configuration file with all the relevant information„ÄÇThen you can
    just run accelerate launch followed by the past or your training script„ÄÇIn a notebook„ÄÇ
    you can use a notebook launcher function to launch your training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_30.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a1136557282a500704797139dc43d7b8_31.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay„ÄÇSo before we have a look at low level training loops in Pytorch and the
    Accelator„ÄÇ there's a question from EBtan who saysÔºåCan I find out how many labeled
    samples do we need to get good results with transfer learning and fine tuningÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Is there a rule of thumb hereÔºüThat's a reallyÔºå really good question„ÄÇüòäÔºåAt least
    in my experience„ÄÇ the answer really depends on the task that youre trying to tackle
    so generally speaking and you know the language that you're dealing with and the
    domain so these are all factors to consider so a rough hierarchy at least from
    my perspective is that text classification is generally one of the simpler tasks
    to tackle with this and in a sort of standard text classification context you
    may only need maybe 100 samples with transfer learning to get quite good results„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And of courseÔºå the thing that you should always do is build a baselineÔºå so build
    a really„ÄÇ really simple modelÔºå not a transformerÔºå do something like naive phase
    or logistic regression or something like this„ÄÇJust as a sanity check that on your
    small data setÔºå yourÔºå yourÔºå your results when they„ÄÇ do they transfer from the
    training set to the„ÄÇValidation set because there's a really„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: really good chance of overfitting here„ÄÇSo roughly a few like a hundred to a
    few hundred samples for text classification„ÄÇ at least for me has worked wellÔºå
    but then it gets a bit harder as you do different tasks so if you move to say
    question answering„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here there's like sayÔºå two strategies you can take„ÄÇSo one strategy„ÄÇWhich is
    maybe the first one to start with„ÄÇIs if you're doing question answering or extractive
    question answering like S„ÄÇIs to look for a model that has been trained already
    on squad in your domain ideally„ÄÇ So let's suppose I was doing„ÄÇSquad in GermanÔºå
    which is a bit different to the standard case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: maybe I look for squad here„ÄÇAndÔºå let's see„ÄÇI believe there's a German squad
    and maybe I can specify the language to German„ÄÇOkayÔºå we don't have that„ÄÇSo let's
    have a lookÔºå I believe„ÄÇThere's a company called Deepset who have done this It's
    called QuadÔºå okay„ÄÇSo German here is basically a language model that has been fine
    tuned on a German version of S„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and now this lets you answer questions in German„ÄÇUmÔºå so„ÄÇüòä„ÄÇThen I would take
    this model and if my corpus is in GermanÔºå but it's likeÔºå you knowÔºå custom„ÄÇ maybe
    it's my business dataÔºå I would then just try to see how this model works on that
    data set„ÄÇNow„ÄÇ generallyÔºå you'll find that it doesn't perform as good as the original
    squad model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then you will do some say domain adaptation from the S model to your domain„ÄÇ
    so you basically just do a little bit of fine tuning on your domain and typically
    you'll then see the model adapts to your corpus and you'll get much better performance„ÄÇAnd
    in that context forÔºå sayÔºå question answeringÔºå typically you need light on the
    scale of a few thousand„ÄÇ youÔºå a thousand to a few thousand examples to get at
    least good results„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But you have to be very careful because sometimes when you do this domain adaptation
    from one model to another„ÄÇ if you sort of overfit your domainÔºå you'll end up forgetting
    all of the good features that the fine tune model had to start with and so it's
    generally tricky in that sense„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇüòäÔºåAnd then for other tasks like named entity recognitionÔºå this is I think
    very problem specific„ÄÇ it really depends on the entitiesÔºå what are the frequencies
    of the entities that you have and here again I think you're dealing with on the
    scale of like a few thousand examples to get okay results„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So to summarize„ÄÇüòäÔºåÂóØ„ÄÇüòäÔºåOkayÔºå well just to summarize I'll answer to the question„ÄÇ
    so to summarize we're talking about maybe a few hundred examples„ÄÇ labeled examples
    for simple problems like text classification to a few thousand„ÄÇFor the labeled
    case and maybe even more if you're doing something that's very niche„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I mean if you're doing maybe I'm doing like maybe like legal analysis of legal
    contracts and maybe the domain is so different from any existing pret model that
    I actually need to do some sort of finetning on a legal corpus that looks like
    what I have and so then you don't need labeled data per say you just need a lot
    of unlabeled legal contracts that you can then finet a language model and then
    transfer to your domain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so there's a follow up question which saysÔºå thank you for the answer„ÄÇ
    What if I have more than a few hundred„ÄÇOf classes for a task„ÄÇ OkayÔºå yes„ÄÇ so then
    that thing gets very hard„ÄÇI would put that in so maybe for text classification
    there's like a hierarchy of complexity so the simplest case is like binary classification
    and then maybe multiclass is after that and then maybe multiclass with like you
    know extreme number of labels like1 hundred or1 thousand different labels or different
    categories and then maybe multilabel is like somewhere around that so I think
    in general it will be harder and what you will find if you have a data set which
    has like1 hundred0 classes or more is that the model will be very confident about
    the majority class so there's always a distribution generally in these classes
    and most of the time it's like a power law so you'll have a few classes that are
    very common and then there'll be some that are quite rare just because you know
    they tags or something that people don't really use very often„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the model will struggle a lot in those rare tasksÔºå in those rare tags or
    those rare labels„ÄÇAnd so then what I would recommend you do is you try to focus
    on the hard examples„ÄÇ so you try to collect more data for those rare examples
    to boost the signal for the model„ÄÇSo you don't need maybe like 10 more examples
    of the common class„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you need100 more examples of the rare ones„ÄÇm but in general„ÄÇ as long as you
    can kind of get like a good coverage in the labels„ÄÇ it should work except it it's
    just going to be tough becauseÔºå you knowÔºå with 100 options„ÄÇ the model has more
    chances to pay mistakes„ÄÇAnd in factÔºå there's a very good trick I should mention„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: is that when you train a model„ÄÇYou can extract or compute the loss that the
    model has on every single sample„ÄÇAnd if you do thisÔºå this will tell you roughly
    which examples the model is most confused about„ÄÇ and this is actually a technique
    from FastAo„ÄÇ![](img/a1136557282a500704797139dc43d7b8_33.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_34.png)'
  prefs: []
  type: TYPE_IMG
- en: WellÔºå I don't know if they've entered itÔºå but„ÄÇAt least this is where I first
    saw it„ÄÇ![](img/a1136557282a500704797139dc43d7b8_36.png)
  prefs: []
  type: TYPE_NORMAL
- en: And„ÄÇLet's see if we look here at fast AI„ÄÇÂóØ„ÄÇSoÔºå I think„ÄÇIt's called most confused„ÄÇüòîÔºåBut
    let's see„ÄÇOkayÔºå so things have changed a bit in V2„ÄÇüòîÔºåOkayÔºå I can't quite find
    it„ÄÇ but basically in the FAA library they have a function which lets you plot
    the most confused examples that the model is having trouble with and then with
    those you can then work out where you need to collect more data but roughly speaking
    what's happening is it just confuse the loss for every example in your validation
    set and then just sorts them and so if you do that yourself manually you'll be
    able to see where you need to improve the model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so let's wrap up by diving into the Acelerate library„ÄÇSo the difference
    here is that„ÄÇ instead of using„ÄÇ![](img/a1136557282a500704797139dc43d7b8_38.png)
  prefs: []
  type: TYPE_NORMAL
- en: „ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_40.png)'
  prefs: []
  type: TYPE_IMG
- en: GPPUÔºå we're going to use the TPU hardware in the back end„ÄÇ So we activate it
    this way„ÄÇ![](img/a1136557282a500704797139dc43d7b8_42.png)
  prefs: []
  type: TYPE_NORMAL
- en: And we install transformers and data setsÔºå as we always have„ÄÇ![](img/a1136557282a500704797139dc43d7b8_44.png)
  prefs: []
  type: TYPE_NORMAL
- en: And the the main thing that you need to do now is you need to install some accelerate
    and some TPU specific libraries„ÄÇ So in order to run pie torch on a TPUÔºå we need
    some special wheels they called more basically a binary file that we can install
    here and„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This will then allow us to run Pitorrch on TPU„ÄÇAnd the first thing we do is
    always the same„ÄÇ it's like we just tokenize the dataÔºå So this is pretty familiar
    by now„ÄÇAnd one thing that Sylvan is doing here is he's removing all of the text
    columns or the columns that we don't really want for training„ÄÇExplicitly„ÄÇThis
    is to make it easier so that the trainer doesn't get confused when it receives
    raw text„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And he's also renaming the label column to labels and this just helps the trainer
    auto detect which column it should compute the metrics on„ÄÇAnd the other thing
    that he's doing is setting the format of the elements in the data set to pytorage
    tensors„ÄÇSo I'm just going to show you that just quickly„ÄÇIf we look at our tokenized„ÄÇTokenized
    data sets„ÄÇ train oops„ÄÇüòîÔºåWe look at one element„ÄÇÂóØm„ÄÇNowÔºå what we've seen so far
    in today's session is that everything was just a Python list„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but you can change the format of your data set from lists to„ÄÇTsors„ÄÇ but you
    can also set them to tensorflow tensors if you're running Tensorflow or even I
    think you can do it for nu arrays„ÄÇ So it really depends on what you're operating
    on that you want to manipulate„ÄÇ but this is a very handy way of switching the
    formats„ÄÇOops„ÄÇSo yeahÔºå tortureÔºå by torture doesn'tÔºü
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so because we're not using the trainer and we're going to use our own
    training loop„ÄÇ we need to have what are called data loaders so in Pytorch„ÄÇThere's
    kind of two concepts that are important one is like the concept of a data set„ÄÇ
    So these are the things that we've been doing most of the time so far„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But then when you want to feed batches to your modelÔºå there's an API called
    data loader„ÄÇ which will automatically sample from the data set and then provide
    those samples to the model so you basically feed in the data set you care about
    and then you can feed in whether you want to randomly shuffle the elements and
    that's important for training just in case you have some you ordering in your
    data that is artificial you can specify the batchche and also how the collation
    is done for the dynamic padding so speak„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so you can create a data loader and then if you look at an element of
    a data loader„ÄÇ it's just going to be basically a set of tensesors associated with
    each batch„ÄÇAnd then we stand here our model just like we always have„ÄÇAnd„ÄÇThen„ÄÇOnce
    this is loaded„ÄÇWe can just do a sanity check that if we feed a single batch which
    just has input IDs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: attention to mask labelÔºå and these other ideas I want to talk about„ÄÇ it returns
    a tensor which is the lossÔºå so this is like a sanity check that our model is working„ÄÇAnd
    because we're going low level into the training loop„ÄÇ now„ÄÇ we also have to specify
    the optimization algorithm for basically minimizing the loss function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I would say the defaultÔºå a very good default is to use this modified at called
    Adam W„ÄÇ and you can just use the default learning rate„ÄÇAnd you just pass the parameters
    of your model that you want to optimize„ÄÇSo just to show you quicklyÔºå if we have
    a look at this„ÄÇThis is a generator„ÄÇ so I need to create a list„ÄÇAnd then if I just
    look at one of these elements„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the parameters of the model consists of tenss„ÄÇ So these are
    like our weights and biases„ÄÇAlong with a name that tells us you know which layer
    we're looking at and so by feeding these to Adam we're basically saying here is
    my instructions of weights and biases I want you to optimize and then it will
    proceed when we do the training to do the optimization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And there's also some extra things we need to specifyÔºå like the number of epochs„ÄÇ
    the number of training steps and the scheduler for how we want to sort of control
    the learning rate during training„ÄÇSo if these concepts are kind of the first time
    you've seen them„ÄÇ we have information in the course notes to help you understand
    that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but basically speaking using a constant learning rate throughout training is
    suboptimal„ÄÇ both in terms of speed in terms of just getting to like a good local
    minimum and so we have schedulers which will essentially control how the learning
    rate increases and decreases during training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so we can define the scheduler„ÄÇDid I not define the optimizerÔºüOkayÔºå so„ÄÇI'm
    just going to load thisÔºå this is just an example when you're doing things on a
    GPU„ÄÇ you specify the device„ÄÇAnd this is then more or less what a training loop
    looks like in Pytage„ÄÇ So you say we're going to do training nowÔºå so we're going
    to activate dropout in the model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we loop over for every epoC and every single epoC we're going to loop
    over every single batch in the training data loader and then we're going to set
    those tensors to the device we care about we're going to compute the loss and
    then we're going to just do the backward pass in Pywch and then we just basically
    take a step now with the optimizer and the scheduler and then we update and this
    will then just do training in a normal way on a GPU„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Or a CPU„ÄÇBut the thing that's kind of cool is that we're using a TPU so we have
    in a TPU we have eight cores on coLab and so what we want to do is we want to
    basically be able to do distributed training this way„ÄÇAnd there's a nice kind
    of picture here of what's going on in the distributed training„ÄÇWhere„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'll put this in the chatÔºå you can have a bunch of different machines and each
    machine has its own process so you can think of each process as being like its
    own little like a controller that is processing data„ÄÇAnd so what we can do with
    with a TPU is we can think of it as having eight different processes and we need
    to communicate the sharing of weights„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: essentially how we do back propagation across these processes„ÄÇ and the accelerateerate
    library allows us to do that very simply„ÄÇÂóØ„ÄÇByÔºå basically„ÄÇWe define something called
    an accelerator„ÄÇAnd then we have to prepare the data loader„ÄÇ the evaluation data
    loader and the model using this accelerator API„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this will automatically work out what hardware I'm running„ÄÇ how many cores
    I have how should I distribute the data„ÄÇ how I should you know basically copy
    the model across these nodes or these devices and then everything else after that
    is exactly the same as we saw before„ÄÇ so you don't really have to change„ÄÇAnything
    in your training script is basically untouched it's just the way you prepare the„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The the training loop„ÄÇSo if you run thisÔºå it won't work as it stands„ÄÇ what we
    need to do is we need to wrap all of the accelerator code inside a training function„ÄÇAnd
    so if we do that„ÄÇWe just need to„ÄÇPut all this„ÄÇInside a training function„ÄÇAnd one
    thing I discovered yesterday is we need to make sure that the data loaders we
    use here„ÄÇËØ∂ËØ∂„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ice's have a quick look„ÄÇÂóØ„ÄÇAnd you are not global variables„ÄÇThat's how I think
    I just do this„ÄÇOkay„ÄÇüòä„ÄÇSo here I'm just initializing a model I'm initializing the
    optimizes we did before and here the magic of accelerate is basically going to
    do this distributed placement for us and so it's going to give us data loaders
    a model and an optr and then everything else runs as normal and this is where
    this question about tokenizing the data is important so what we've been doing
    previously is using dynamic padding so we don't use padding in the tokenizer directly„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And in factÔºå hereÔºå if you look at when I tokenized the data„ÄÇWhere is it„ÄÇHere
    but in my tokenizing functionÔºå I just use truncation„ÄÇBut because we're running
    on a TPU„ÄÇ I want to activate paddingÔºå so I need to put padding equals true„ÄÇAnd
    I'm going to specify the max length that I want to pad not to the max length of
    the full model because it's too large„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm just going to specify kind of arbitrarily that it's 128 tokens„ÄÇAnd so once
    you've paded all your inputsÔºå we can then wrap„ÄÇThere„ÄÇ we need to then also re
    instantsantiate the data loaders here„ÄÇAnd nowÔºå we can„ÄÇCreate a training function„ÄÇAnd
    fingers crossedÔºå this will then allow us to launch„ÄÇA TPU training„ÄÇSo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so while that's workingÔºå let's have a quick look at the questions„ÄÇÂóØ„ÄÇOkay„ÄÇ
    so there's a question from I am Home„ÄÇ There is a hugging past Titch session today
    and tomorrow all labels chapter 3„ÄÇ The main difference is the time zones„ÄÇ So we
    try to capture people who live on the sort of east and west of globe„ÄÇAnd there's
    also a TensorFlow session tonight with Matt Carrigan„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and so that's for all the TensorFficionados„ÄÇÂóØ„ÄÇThere's a question from Uns about
    regarding the domainspec cases for text classification before I train my model
    for my downstream task would it make sense to pre- fine tune with language modeling
    to better understand the context yes in general that's a very good thing to do
    the amount of boost you'll get in performance is very dependent on the domain
    shift so if I have Bt which was pre-train on Wikipedia and my text is kind of
    like say factual or kind of like Wikipedia then the boost I get from fine tuning
    but on my corpus won't be so much but if my corpus is like very different in domain
    like maybe it's like source code or something like this then then you will see
    typically a bigger boost in performance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it's always worth doing and in general it's pretty easy to do„ÄÇThere's a question
    about if I want to classify articles instead of sentences does Bert and friends
    still work or should I look at something like a long form that's a great question
    so Bert is basically limited or most of these sort of vanilla models are limited
    to 512 tokens so if your documents are longer than that you will have to do one
    or two things use a model which can process longer sequences like longform or
    big bird from memory I think they can process 4096 tokens so they're much much
    bigger like eight times bigger than BerRT„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But if that's not like sufficientÔºå you'll then have to basically chunk or kind
    of split your document into passages and then you'll have to feed those passages
    into the model and then do some aggregation to work out how you build up a kind
    of representation for the full document that you can then feed into your classification
    head„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There's a question about to improve the baseline modelÔºå take a Roberta modelÔºå
    train it„ÄÇYes„ÄÇ that's rightÔºå exactly„ÄÇI am Holmes asksÔºå what is the difference between
    using P Toch's default Adam W and the transformformers oneÔºü
  prefs: []
  type: TYPE_NORMAL
- en: That's a good question off the top of my head I don't know my guess is that
    there are some I mean there's a long history of Adam and not being implemented
    correctly so„ÄÇüòäÔºåYeahÔºå off the top of my headÔºå I have a suspicion that perhaps the
    transformers one is is correct and the point which one maybe isn't but„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Maybe do some experimentsÔºå find outÔºå sorryÔºå I don't know that off top of my
    head„ÄÇOkayÔºå cool„ÄÇ So those are more or less the questions„ÄÇ Let's see Something
    went wrong„ÄÇ And this is now where I get to do some debugging„ÄÇ So let's have a
    look„ÄÇ I got an error and it says you should probably activate truncation or padding
    with padding equals true„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: To have batched tenses with the same length„ÄÇOkayÔºå so I clearly did something
    wrong when I„ÄÇD find my data loader„ÄÇ So let's have a look„ÄÇAt what went wrong„ÄÇSo„ÄÇI„ÄÇFirstÔºå
    tokenized„ÄÇMy daughter„ÄÇWhich is good„ÄÇI then need to set the format„ÄÇÂïä„ÄÇI want to
    get rid of this clay function„ÄÇThis co function is going to be the thing that gave
    us problems„ÄÇOkayÔºå so now„ÄÇShould beÊàë to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Wunch the TPU training„ÄÇOkayÔºå so this is now launching on these eight TPU cores„ÄÇAnd
    fingers crossed„ÄÇ I didn't make any mistakes„ÄÇGreatÔºå thanks for the feedback„ÄÇ I
    am Homeme and DK creativezative„ÄÇ I'm hoping you're enjoying it and thanks for
    the questions„ÄÇüòäÔºåSo this looks like it is training„ÄÇAnd„ÄÇYesÔºå great„ÄÇ Oh noÔºå no problem„ÄÇOkayÔºå
    so„ÄÇWhat is itÔºå Stack expects tensor to be equal size but 96„ÄÇüòî„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so„ÄÇÂóØ„ÄÇOkayÔºå so here I'm going to do someÔºå I know we're running over time„ÄÇ
    so I apologize„ÄÇ I'm going to seeÔºå let's see if I can debug this„ÄÇFast and if not„ÄÇ
    then I'll just fix the notebook later„ÄÇ OkayÔºå so so basically I'm running into
    an error and this is a good way of seeing how TPs are a little bit more fun than
    all It's saying the stack expects each tensor to be equal size butve got 96 at
    entry0 and 100 entry 1 So this is suggesting to me that somehow I've screwed up
    the tokenization So instead of having everything with the same shape there there's
    some„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Like something's gone wrong„ÄÇ So let's have a look at„ÄÇMy data loader„ÄÇOkayÔºå so„ÄÇSo
    let's have a look at my tokenization first„ÄÇWhen I did tokenization„ÄÇI set padding
    equals true and max length„ÄÇAnd so now„ÄÇWhen I look at my tokenized data sets„ÄÇLet's
    have a look at„ÄÇËøôËøôËøô„ÄÇSo maybe let's have a look at the input Is„ÄÇSo the input IDs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we want these to all be the same shape„ÄÇI'm going to just take„ÄÇÂóØ„ÄÇLet's just takeÔºå
    say 10 of them„ÄÇAnd then what I'm going to do is I'm just going to say tensort
    size„ÄÇFor tensor in this„ÄÇSo let's see what we get„ÄÇOkayÔºå so this is telling me that
    all the tensors have the same size„ÄÇ which is good„ÄÇÂóØ„ÄÇLet's just do a sanitity checkÔºå
    if I„ÄÇGo from the end„ÄÇYeahÔºå thats good„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so my tensors are all the same size„ÄÇAnd should just do this again„ÄÇAnd let's
    see„ÄÇNow„ÄÇWhat is complaining on about thisÔºüüòîÔºåInvalid type„ÄÇÂêì„ÄÇBecause I need to„ÄÇReturn
    tenses equals„ÄÇQuite toch„ÄÇThis is so annoyingÔºå okayÔºå let's not do that„ÄÇThe too
    petting„ÄÇAnd then let's remove the column„ÄÇ Let's set the formatÔºå good„ÄÇOkay„ÄÇüòäÔºåNow
    let's see what we get here„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we can see there's already a problem„ÄÇInterestingÔºå so„ÄÇÂóØ„ÄÇüòäÔºåÂëµÂëµ„ÄÇüòäÔºåCoolÔºå all right„ÄÇ
    so we're going to try and deg this and in factÔºå if someone sees what I'm doing
    wrong„ÄÇ this will be great so so here's a problem The problem is I've created a
    data loader„ÄÇüòäÔºåHere„ÄÇ train data loaderÔºå and it's complaining that when I try to
    put the tensesors together„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they're somehow not of the same shape„ÄÇSo„ÄÇLet's see what have I done wrongÔºüOkayÔºå
    so„ÄÇLet's do that„ÄÇüòî„ÄÇSoÔºå let's„ÄÇLet's look at the first batch„ÄÇAnd let's look at„ÄÇOkayÔºå
    let's look at the input I„ÄÇAnd then let's look at the size of each tensor„ÄÇIn here„ÄÇÂïäÂìà„ÄÇüòäÔºåSo„ÄÇIt
    seems that my tokenized data sets are not„ÄÇThe ones I was looking for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look what's going on if I look at„ÄÇSo we saw before that these
    were meant to be 89 in size„ÄÇ so let's have a look at this„ÄÇAnd then we go T dot
    size or T in here„ÄÇÂóØ„ÄÇOkay„ÄÇSo„ÄÇYou can see that in my tokenized data setsÔºå all the
    inputs have size 89„ÄÇBut„ÄÇFor some reason„ÄÇ in the batch of the training loaderÔºå
    everything is 71„ÄÇÂóØ„ÄÇüòäÔºåSo let's see„ÄÇWch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Why would that be differentÔºå Why would the batch be different„ÄÇÂóØ„ÄÇÂØπÂØπÈÉΩ„ÄÇÂóØ„ÄÇüòä„ÄÇDon't
    tell me I need this clay function„ÄÇüòîÔºåNoÔºå that's not what I want because now„ÄÇüòî„ÄÇThey're
    going to be dynamically powdered„ÄÇÂóØ„ÄÇüòäÔºåThat's an interesting thing I'm going to
    have a look„ÄÇ„ÅÜ„ÄÇSoÔºå let's see if we have„ÄÇSo just to just to tell you what I've done„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so I've reintroduced a collate function into the„ÄÇInto the data load„ÄÇHere„ÄÇI was
    pretty sure we don't want to have that because we want to have fixed sizes„ÄÇButÔºå
    let's see„ÄÇIf this has„ÄÇSort of magically debug the problem Okay„ÄÇ so for reasons
    I don't understand adding a collate function to the data loader seems to have
    been important„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For letting us do the training„ÄÇ And you can see here I'm doing on one TPU coreÔºå
    I'm running„ÄÇThe training„ÄÇAnd what should all these„ÄÇAll these messages right„ÄÇ you
    can see here these are the warning messages you often get when you instantiate
    a model and so what accelator has done is it's copied the model into the eight
    TPU core but done so in a way that the initialization is the same so we've got
    the same model at the start„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then it should instantiate„ÄÇAnother seven parallel trainings„ÄÇ so now we're
    training the model copied in eight pieces across eight cores and each core will
    get its own batch of data and then back propagation will be synchronized so that
    then when we update the model we update based on the losses computed for every
    device„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇüòäÔºåSo this is yeahÔºå obviously a little bit of a hack I'm a bit unhappy that
    it's everything I said about needing to use padding seems to have been mixed up
    with the colade function but yeah that's the joy of life coding maybe you can
    dive into this yourself and try to understand exactly you know how one is to set
    this up there's something I'm still kind of missing in my head at the moment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåBut neverthelessusÔºå that's some„ÄÇLet's say training on a TPU and one thing
    that I would recommend you do as like a homework problem is try to work through
    these training notebooks on a new data set„ÄÇAnd so you can find many options on
    the hubÔºå so if you go to data sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can look at text classification and for most of the time this will cover
    all the things you have everything you need now to study this„ÄÇAnd the other thing
    that you can„ÄÇDo isÔºå for exampleÔºå we're doing paraphrase detection right„ÄÇ we're
    checking if two sentences of the paraphrases of another another example of this„ÄÇWould
    be„ÄÇThe corera data set„ÄÇAnd this is a very common problem on things like stack
    overflow or Qra where you've got people asking questions and sometimes the question
    are duplicate and you often rely on the community to flag question as duplicate
    or not and so this would be a nice data set to use which would be very similar
    to what we've done but give you a kind of different flavor of dealing with different
    columns and different inputs so I'm going to put that in the chat as a suggestion„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And yeahÔºå I would also recommend see if you can get the TPU training working„ÄÇ
    I think TPUs are a very exciting development because they offer in principle eight
    times as much speed up in principle„ÄÇ but reality is around three times„ÄÇOver conventional
    say GPU training and they're free on co labs„ÄÇ so now you can use accelerate to
    train faster than normal and do really cool distributed stuff„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All right so that was a bit longer and sorry for that that's my fault for not
    being able to deog my code so I hope you enjoyed it and tonight there's Matt doing
    TensorFlow if you're into that and I think tomorrow is then Sil doing the next
    round of this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåAnd next weekÔºå we're going to be„ÄÇDiving into the sort of more advanced parts
    of the library„ÄÇ So what we're going to be doing next week is how do we share models
    in the hub„ÄÇ How do we push everything to the hubÔºå How do we share metrics„ÄÇ And
    so this is going to be the kind of icing on the cake for everything we don't„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I'm not back for chapter fourÔºå but Omar is and Omar is awesome„ÄÇ he's in the
    chat right now and so you should definitely come„ÄÇ I'll actually be in the chat
    so it'll be like a body swap„ÄÇAnd so I hope you guys see you then„ÄÇüòä„ÄÇCoolÔºå so with
    that I'm going to stop recording and stop the stream so if you have any questions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: just put them in the forum and see you next time„ÄÇ![](img/a1136557282a500704797139dc43d7b8_46.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1136557282a500704797139dc43d7b8_47.png)'
  prefs: []
  type: TYPE_IMG
- en: „ÄÇ
  prefs: []
  type: TYPE_NORMAL
