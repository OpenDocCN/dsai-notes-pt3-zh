- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P36ï¼šL6.4- Lewisçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P36ï¼šL6.4- Lewisçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
- en: Okayï¼Œ so maybe we can get started All rightï¼Œ so today we're looking at chapter3
    of the Huging F courseã€‚And this chapter is kind of like bringing together all
    of the components that we've been looking at in the first two chaptersã€‚ so just
    as a recap in the first chapter we looked at sort of the concepts around the transformer
    and some concepts around tokenizationã€‚And pre trainingã€‚And in chapterpt 2ï¼Œ we
    started taking a deep dive into what really happens inside this pipeline API that
    we were using a lot in the first chapter and trying to really unpack sort of how
    to train a model or at least how to understand the inputs and outputs of a modelã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ã€‚é‚£ä¹ˆï¼Œä»Šå¤©æˆ‘ä»¬è¦çœ‹çš„æ˜¯ Hugging Face è¯¾ç¨‹çš„ç¬¬ä¸‰ç« ã€‚è¿™ä¸€ç« å°†æŠŠæˆ‘ä»¬åœ¨å‰ä¸¤ç« ä¸­çœ‹åˆ°çš„æ‰€æœ‰ç»„ä»¶ç»“åˆèµ·æ¥ã€‚ä½œä¸ºå›é¡¾ï¼Œåœ¨ç¬¬ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸
    transformer ç›¸å…³çš„æ¦‚å¿µï¼Œä»¥åŠä¸åˆ†è¯å’Œé¢„è®­ç»ƒç›¸å…³çš„ä¸€äº›æ¦‚å¿µã€‚åœ¨ç¬¬äºŒç« ä¸­ï¼Œæˆ‘ä»¬å¼€å§‹æ·±å…¥äº†è§£æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­å¤§é‡ä½¿ç”¨çš„è¿™ä¸ªç®¡é“ API å®é™…ä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¹¶è¯•å›¾çœŸæ­£è§£æå¦‚ä½•è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæˆ–è€…è‡³å°‘å¦‚ä½•ç†è§£æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºã€‚
- en: And also the inputs and outputs for the tokenizerã€‚So today we're actually going
    to bring this all together and we're going to do a couple of thingsã€‚ we're going
    to look at the datas libraryï¼Œ which is this very nifty library developed by hugging
    face for processing data sets of more or less any sizeã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰åˆ†è¯å™¨çš„è¾“å…¥å’Œè¾“å‡ºã€‚æ‰€ä»¥ä»Šå¤©æˆ‘ä»¬å°†æŠŠè¿™ä¸€åˆ‡ç»“åˆèµ·æ¥ï¼Œåšå‡ ä»¶äº‹æƒ…ã€‚æˆ‘ä»¬å°†æŸ¥çœ‹æ•°æ®é›†åº“ï¼Œè¿™æ˜¯ Hugging Face å¼€å‘çš„ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åº“ï¼Œç”¨äºå¤„ç†å„ç§è§„æ¨¡çš„æ•°æ®é›†ã€‚
- en: And then we're going to train our very first model using something called the
    trainer and the trainer is an API which basically wraps a lot of the complexity
    around writing your own training loop in Pytorch or TensorFã€‚And then we're going
    to sort of take a like we're going to kind of unpack what's going on in some sense
    in that trainer by writing our own training loopã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå«åšè®­ç»ƒå™¨çš„ä¸œè¥¿æ¥è®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ã€‚è®­ç»ƒå™¨æ˜¯ä¸€ä¸ª APIï¼ŒåŸºæœ¬ä¸Šå°è£…äº†åœ¨ Pytorch æˆ– TensorFlow ä¸­ç¼–å†™è‡ªå·±è®­ç»ƒå¾ªç¯çš„è®¸å¤šå¤æ‚æ€§ã€‚ç„¶åæˆ‘ä»¬å°†å°è¯•è§£æè¿™ä¸ªè®­ç»ƒå™¨ä¸­å‘ç”Ÿçš„äº‹æƒ…ï¼Œé€šè¿‡ç¼–å†™æˆ‘ä»¬è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ¥å®ç°ã€‚
- en: In Pytorage and this will be a great excuse to introduce a very cool library
    that Sylvanna has developed called Huging Face Acccceerateã€‚And this library is
    really designed for doing what's called distributed trainingã€‚ so how you can kind
    of speed up your training when you've got multiple GPUs or potentially even TUsã€‚
    which is what we'll see today in Google Coabã€‚And of courseï¼Œ if there's any questionsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Pytorch ä¸­ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æœºä¼šæ¥ä»‹ç»ä¸€ä¸ªéå¸¸é…·çš„åº“ï¼ŒSylvanna å¼€å‘çš„ Hugging Face Accelerateã€‚è¿™ä¸ªåº“æ—¨åœ¨è¿›è¡Œæ‰€è°“çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ä½ æœ‰å¤šä¸ª
    GPU æˆ–è€…ç”šè‡³ TPU æ—¶ï¼Œå¦‚ä½•åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä»Šå¤©æˆ‘ä»¬å°†åœ¨ Google Colab ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚å½“ç„¶ï¼Œå¦‚æœæœ‰ä»»ä½•é—®é¢˜ã€‚
- en: just ask them in the chat at any time and I'll be pausing at various points
    to takeã€‚It's time to answer themã€‚And so maybe just to startã€‚The kind of ecosystem
    that we kind of have a hugging piece at the moment roughly revolves around these
    sort of componentsã€‚ So we've got the hubï¼Œ which we've seen several times now alreadyã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ—¶åœ¨èŠå¤©ä¸­é—®æˆ‘é—®é¢˜ï¼Œæˆ‘ä¼šåœ¨ä¸åŒçš„æ—¶åˆ»æš‚åœæ¥å›ç­”ã€‚ç°åœ¨æ˜¯å›ç­”é—®é¢˜çš„æ—¶å€™ã€‚æ‰€ä»¥ï¼Œä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥å…ˆå¼€å§‹ã€‚æˆ‘ä»¬ç›®å‰çš„ç”Ÿæ€ç³»ç»Ÿå¤§è‡´å›´ç»•è¿™äº›ç»„ä»¶å±•å¼€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸­å¿ƒï¼Œè¿™ä¸€ç‚¹æˆ‘ä»¬å·²ç»çœ‹è¿‡å¥½å‡ æ¬¡äº†ã€‚
- en: And we use the hub for both loading models in transformersï¼Œ loading tokenizersã€‚
    and also loading data setsã€‚And then transformers interacts with the data sets
    libraryã€‚ as we'll see todayï¼Œ as a way of training our modelsã€‚And then once the
    models are trainedã€‚ we can then push them back to the hub so that then we can
    share them with our colleagues and we can also interact with them using some of
    the widgets that are developed by the Huging F teamã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸­å¿ƒæ¥åŠ è½½ transformers ä¸­çš„æ¨¡å‹ï¼ŒåŠ è½½åˆ†è¯å™¨ï¼Œä»¥åŠåŠ è½½æ•°æ®é›†ã€‚ç„¶åï¼Œtransformers ä¸æ•°æ®é›†åº“è¿›è¡Œäº¤äº’ã€‚æ­£å¦‚æˆ‘ä»¬ä»Šå¤©å°†çœ‹åˆ°çš„ï¼Œè¿™æ˜¯è®­ç»ƒæˆ‘ä»¬æ¨¡å‹çš„ä¸€ç§æ–¹å¼ã€‚æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ¨é€å›ä¸­å¿ƒï¼Œä»¥ä¾¿ä¸åŒäº‹åˆ†äº«ï¼Œå¹¶ä½¿ç”¨
    Hugging Face å›¢é˜Ÿå¼€å‘çš„ä¸€äº›å°éƒ¨ä»¶ä¸ä¹‹äº’åŠ¨ã€‚
- en: Soï¼Œ with thatï¼Œ let'sã€‚Take a quick look at the dataset sets library soã€‚The data
    sets library is a kind of general purpose library for processing data sets of
    more or less any typeã€‚ so originally it was specifically for text data sets for
    NLPã€‚But since thenã€‚ it's kind of grown into something that can handle imagesï¼Œ
    audioï¼Œ and over timeã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¯ä¸å¤šè¯´ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹æ•°æ®é›†åº“ã€‚æ•°æ®é›†åº“æ˜¯ä¸€ç§é€šç”¨åº“ï¼Œç”¨äºå¤„ç†å‡ ä¹ä»»ä½•ç±»å‹çš„æ•°æ®é›†ã€‚æœ€åˆï¼Œå®ƒæ˜¯ä¸“é—¨é’ˆå¯¹ NLP çš„æ–‡æœ¬æ•°æ®é›†ã€‚ä½†æ­¤åï¼Œå®ƒé€æ¸å‘å±•æˆå¯ä»¥å¤„ç†å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§ç±»å‹çš„æ•°æ®é›†ã€‚
- en: we'll have things like time series and so onã€‚And the main power of this library
    is that it provides a kind of uniform API for processing data and at least for
    me personallyã€‚ this has been one of the biggest productivity gains in my sort
    of career as a data scientist because previously like I was always having to manipulate
    different types of dataã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¼šæœ‰æ—¶é—´åºåˆ—ç­‰åŠŸèƒ½ã€‚è¿™ä¸ªåº“çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒæä¾›äº†ä¸€ç§ç»Ÿä¸€çš„APIç”¨äºå¤„ç†æ•°æ®ï¼Œå¯¹æˆ‘ä¸ªäººè€Œè¨€ï¼Œè¿™åœ¨æˆ‘ä½œä¸ºæ•°æ®ç§‘å­¦å®¶çš„èŒä¸šç”Ÿæ¶¯ä¸­æ˜¯æœ€å¤§çš„ç”Ÿäº§åŠ›æå‡ä¹‹ä¸€ï¼Œå› ä¸ºä»¥å‰æˆ‘æ€»æ˜¯éœ€è¦å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ã€‚
- en: CSVï¼Œ Jason textï¼Œ whateverï¼Œ and each time it was always a little bit idiosyncratic
    and you had to write your own sort of custom like functions for dealing with all
    these data types and the data sets library providesã€‚A very simple API where you
    can basically load a data setï¼Œ more or less in one line of codeã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CSVã€JSONæ–‡æœ¬ç­‰ï¼Œæ¯æ¬¡å¤„ç†è¿™äº›æ•°æ®ç±»å‹æ—¶æ€»æ˜¯æœ‰ç‚¹ç‹¬ç‰¹ï¼Œä½ å¿…é¡»ä¸ºå¤„ç†æ‰€æœ‰è¿™äº›æ•°æ®ç±»å‹ç¼–å†™è‡ªå·±çš„è‡ªå®šä¹‰å‡½æ•°ï¼Œè€Œæ•°æ®é›†åº“æä¾›äº†ä¸€ä¸ªéå¸¸ç®€å•çš„APIï¼Œä½ åŸºæœ¬ä¸Šå¯ä»¥åœ¨ä¸€è¡Œä»£ç ä¸­åŠ è½½æ•°æ®é›†ã€‚
- en: and then you can process it using something called the map functionã€‚ which we'll
    discuss in more detail todayã€‚And more or less with this map functionã€‚ you can
    kind of do extremely like fast and crazy processingã€‚ even on monster data sets
    that are like you know a terabyte in sizeã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå«åšmapå‡½æ•°æ¥å¤„ç†å®ƒã€‚æˆ‘ä»¬ä»Šå¤©ä¼šæ›´è¯¦ç»†åœ°è®¨è®ºè¿™ä¸ªå‡½æ•°ã€‚å¤§è‡´ä¸Šï¼Œä½¿ç”¨è¿™ä¸ªmapå‡½æ•°ï¼Œä½ å¯ä»¥å¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œæå…¶å¿«é€Ÿå’Œç–¯ç‹‚çš„å¤„ç†ï¼Œå³ä½¿æ•°æ®é›†çš„å¤§å°è¾¾åˆ°ä¸€TBã€‚
- en: you can actually process this on your laptop because this does a very clever
    kind of lazy loading of memory using something called Apache arrowã€‚Okayï¼Œ so let
    me just have a quick lookã€‚Greatã€‚ğŸ˜Šï¼ŒCoolã€‚ so maybe what we can do is we can start
    by looking at this introductory video on the DiocS Libraryã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å®é™…ä¸Šå¯ä»¥åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šå¤„ç†è¿™ä¸ªï¼Œå› ä¸ºå®ƒä½¿ç”¨ä¸€ç§å«åšApache Arrowçš„èªæ˜æ‡’åŠ è½½å†…å­˜çš„æ–¹å¼ã€‚å¥½çš„ï¼Œè®©æˆ‘å¿«é€Ÿçœ‹ä¸€ä¸‹ã€‚å¤ªå¥½äº†ã€‚ğŸ˜Šï¼Œé…·ã€‚ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥ä»çœ‹è¿™ä¸ªDiocSåº“çš„ä»‹ç»è§†é¢‘å¼€å§‹ã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_1.png)'
- en: So I'm going to play this and please let me know if you can't hear itã€‚The Egen
    F dataset sets libraryï¼Œ a quick overviewã€‚The Eing F Datas Library is a library
    that provides an API to quickly download many public dataset sets and pre themã€‚This
    video well explore how to do thatã€‚St dig part is easy with the load that asset
    functionã€‚You can directly download and cache a dataset from its identifier on
    the dataset appã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¦æ’­æ”¾è¿™ä¸ªè§†é¢‘ï¼Œå¦‚æœä½ å¬ä¸è§ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚Egen Fæ•°æ®é›†åº“çš„å¿«é€Ÿæ¦‚è¿°ã€‚Eing Fæ•°æ®é›†åº“æ˜¯ä¸€ä¸ªæä¾›APIçš„åº“ï¼Œå¯ä»¥å¿«é€Ÿä¸‹è½½è®¸å¤šå…¬å…±æ•°æ®é›†å¹¶è¿›è¡Œé¢„å¤„ç†ã€‚è¿™ä¸ªè§†é¢‘å°†æ¢è®¨å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚åŠ è½½æ•°æ®é›†çš„èµ„äº§åŠŸèƒ½ä½¿è¿™éƒ¨åˆ†å˜å¾—ç®€å•ã€‚ä½ å¯ä»¥ç›´æ¥ä»æ•°æ®é›†åº”ç”¨ä¸­æ ¹æ®å…¶æ ‡è¯†ç¬¦ä¸‹è½½å’Œç¼“å­˜æ•°æ®é›†ã€‚
- en: Here we fetch the M PC dataset set from the G benchmarkã€‚ which is a dataset
    set containing pairs of sentences where the task is to determine the power phaseã€‚The
    object returned by the Lo dataset function is the dataset diictã€‚ which is sort
    of dictionary containing each split of a datasetã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬ä»GåŸºå‡†æµ‹è¯•ä¸­è·å–M PCæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†åŒ…å«å¥å­å¯¹ï¼Œä»»åŠ¡æ˜¯ç¡®å®šç›¸ä¼¼åº¦ã€‚Loæ•°æ®é›†å‡½æ•°è¿”å›çš„å¯¹è±¡æ˜¯æ•°æ®é›†å­—å…¸ï¼Œå®ƒæ˜¯ä¸€ç§åŒ…å«æ•°æ®é›†æ¯ä¸ªåˆ†ç‰‡çš„å­—å…¸ã€‚
- en: Can access each split by indexing with its nameã€‚This split is within an instance
    of the dataset class with columns hereã€‚ sometimes1ï¼Œ sometimes two label on IDX
    and rootã€‚We can access a given element by its indexã€‚The amazing thing about the
    Ugen phase Data library is that everything is saved to disk using apppasharoã€‚Which
    means that even if your dataset is hugeï¼Œ you won't get out of Rã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡åç§°ç´¢å¼•è®¿é—®æ¯ä¸ªåˆ†ç‰‡ã€‚è¿™ä¸ªåˆ†ç‰‡æ˜¯åœ¨æ•°æ®é›†ç±»çš„ä¸€ä¸ªå®ä¾‹ä¸­ï¼Œåˆ—æœ‰æ—¶ä¸º1ï¼Œæœ‰æ—¶ä¸º2ï¼Œæ ‡ç­¾ä¸ºIDXå’Œrootã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•è®¿é—®ç»™å®šå…ƒç´ ã€‚Ugen Phaseæ•°æ®åº“çš„æƒŠäººä¹‹å¤„åœ¨äºï¼Œæ‰€æœ‰å†…å®¹éƒ½ä½¿ç”¨Apache
    Arrowä¿å­˜åˆ°ç£ç›˜ã€‚è¿™æ„å‘³ç€å³ä½¿ä½ çš„æ•°æ®é›†å¾ˆå¤§ï¼Œä¹Ÿä¸ä¼šè€—å°½Rçš„å†…å­˜ã€‚
- en: only these elements you request are in memoryã€‚Accessing a slice of your dataset
    is as easy as one elementã€‚ so result is when dictionary with list of values for
    each case here the list of labelsã€‚ the list of first sentences and the list of
    circumstances sentencesã€‚The features attribute of a data gives us more information
    about its columnsã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰ä½ è¯·æ±‚çš„è¿™äº›å…ƒç´ ä¼šåœ¨å†…å­˜ä¸­ã€‚è®¿é—®æ•°æ®é›†çš„ä¸€éƒ¨åˆ†å°±åƒè®¿é—®ä¸€ä¸ªå…ƒç´ ä¸€æ ·ç®€å•ã€‚å› æ­¤ç»“æœæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªæ¡ˆä¾‹çš„å€¼åˆ—è¡¨ï¼Œè¿™é‡Œæ˜¯æ ‡ç­¾åˆ—è¡¨ã€ç¬¬ä¸€å¥è¯åˆ—è¡¨å’Œä¸Šä¸‹æ–‡å¥å­åˆ—è¡¨ã€‚æ•°æ®çš„ç‰¹å¾å±æ€§ä¸ºæˆ‘ä»¬æä¾›äº†å…³äºå…¶åˆ—çš„æ›´å¤šä¿¡æ¯ã€‚
- en: particularicular we can see hereï¼Œ it gives us a correspondence between the integers
    and names for the labelsã€‚0 stands for not equivalent and one for equivalentã€‚To
    process all the elements of our data setã€‚ we need to tokenize themã€‚Have a look
    at the video prepoed sentence pairs for a refresherã€‚ but you just have to send
    the two sentences to the decokenizer with some additional keyword argumentsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œç»™å‡ºäº†æ•´æ•°ä¸æ ‡ç­¾åç§°ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚0ä»£è¡¨ä¸ç­‰ä»·ï¼Œ1ä»£è¡¨ç­‰ä»·ã€‚ä¸ºäº†å¤„ç†æˆ‘ä»¬æ•°æ®é›†ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œæ ‡è®°ã€‚æŸ¥çœ‹è§†é¢‘ä¸­é¢„å¤„ç†çš„å¥å­å¯¹ä»¥è·å–æ›´å¤šå›é¡¾ï¼Œä½†ä½ åªéœ€å°†è¿™ä¸¤ä¸ªå¥å­å‘é€åˆ°è§£ç å™¨ï¼Œå¹¶é™„ä¸Šå…¶ä»–ä¸€äº›å…³é”®å­—å‚æ•°ã€‚
- en: Here we indicate a maximum length of 128 and pet inputs shorter than in length
    trunc get inputs for longerã€‚We put all of this in the tokenized function but we
    can directly apply to all the splits in our data set with the map methodã€‚As long
    as the function returns a dictionary like objectã€‚ the map pattern will add new
    columns as needed or update existing onesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®šæœ€å¤§é•¿åº¦ä¸º128ï¼Œå¹¶ä¸”å¯¹æ¯”è¾“å…¥é•¿åº¦è¾ƒçŸ­çš„è¿›è¡Œæˆªæ–­å¤„ç†ã€‚æˆ‘ä»¬å°†è¿™ä¸€åˆ‡æ”¾åœ¨tokenizedå‡½æ•°ä¸­ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç›´æ¥åº”ç”¨åˆ°æ•°æ®é›†çš„æ‰€æœ‰æ‹†åˆ†ä¸Šï¼Œä½¿ç”¨mapæ–¹æ³•ã€‚åªè¦å‡½æ•°è¿”å›ä¸€ä¸ªå­—å…¸ç±»å‹çš„å¯¹è±¡ï¼Œmapæ¨¡å¼å°±ä¼šæ ¹æ®éœ€è¦æ·»åŠ æ–°åˆ—æˆ–æ›´æ–°ç°æœ‰åˆ—ã€‚
- en: To speed up prepoing and take advantage of the fact all tokener is back by rust
    thanks to the Eging phase tokenos libraryã€‚We can process several elements at the
    same time in autokenized function using the batch equal2 argumentã€‚Since the tokenizer
    can adult list of first sentencesï¼Œ list of second sentencesã€‚ the tokenized function
    does not need to change for thisã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŠ é€Ÿé¢„å¤„ç†ï¼Œå¹¶åˆ©ç”¨æ‰€æœ‰tokenizeræ˜¯ç”±rustæ”¯æŒçš„è¿™ä¸€äº‹å®ï¼Œå¾—ç›ŠäºEging phase tokenosåº“ã€‚æˆ‘ä»¬å¯ä»¥åœ¨autokenizedå‡½æ•°ä¸­ä½¿ç”¨batch
    equal2å‚æ•°åŒæ—¶å¤„ç†å¤šä¸ªå…ƒç´ ã€‚ç”±äºtokenizerå¯ä»¥å¤„ç†ç¬¬ä¸€å¥çš„åˆ—è¡¨å’Œç¬¬äºŒå¥çš„åˆ—è¡¨ï¼Œå› æ­¤tokenizedå‡½æ•°ä¸éœ€è¦å¯¹æ­¤è¿›è¡Œæ›´æ”¹ã€‚
- en: You can also use printer processing with the MA methodï¼Œ check out its documentationation
    link belowã€‚Once this is doneï¼Œ we're almost ready for trainingã€‚ we just remove
    the columns we don't need anymore with the remove columns methodã€‚ we name label
    to labels since the models from the Uing phase transformforms library expect thatã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥ä½¿ç”¨MAæ–¹æ³•è¿›è¡Œæ‰“å°å¤„ç†ï¼ŒæŸ¥çœ‹ä¸‹é¢çš„æ–‡æ¡£é“¾æ¥ã€‚ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å‡ ä¹å¯ä»¥å‡†å¤‡è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åªéœ€ä½¿ç”¨remove columnsæ–¹æ³•åˆ é™¤ä¸å†éœ€è¦çš„åˆ—ã€‚æˆ‘ä»¬å°†labelå‘½åä¸ºlabelsï¼Œå› ä¸ºUing
    phase transformformsåº“ä¸­çš„æ¨¡å‹æœŸæœ›è¿™æ ·ã€‚
- en: And set the output format with a desired packetantï¼Œ dorchï¼Œ turns of flu on impã€‚If
    neededã€‚ we can also generate a short sample of a data set using the select methodã€‚![](img/a1136557282a500704797139dc43d7b8_3.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶è®¾ç½®æ‰€éœ€çš„è¾“å‡ºæ ¼å¼ï¼Œdorchï¼ŒæµåŠ¨æ€§è½¬æ¢ä¸ºimpã€‚å¦‚æœéœ€è¦ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨selectæ–¹æ³•ç”Ÿæˆæ•°æ®é›†çš„çŸ­æ ·æœ¬ã€‚![](img/a1136557282a500704797139dc43d7b8_3.png)
- en: Greatï¼Œ so that was a whirlwind tour of the Do Libraryã€‚ Are there any questions
    at this stage before we dive into some codeã€‚Okay so feel free to post them in
    the chat as they come so one thing I just want to mention that might be useful
    so on the Hi phase hubã€‚We have looked at the modelsã€‚Hub in the last lessonã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œè¿™å°±æ˜¯Doåº“çš„å¿«é€Ÿä»‹ç»ã€‚åœ¨æˆ‘ä»¬æ·±å…¥ä»£ç ä¹‹å‰ï¼Œè¿™ä¸ªé˜¶æ®µæœ‰æ²¡æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå¥½çš„ï¼Œè¯·éšæ—¶åœ¨èŠå¤©ä¸­æå‡ºé—®é¢˜ï¼Œæˆ‘æƒ³æåˆ°çš„ä¸€ç‚¹æ˜¯ï¼Œè¿™å¯èƒ½ä¼šæœ‰ç”¨ï¼Œåœ¨Hi phase
    hubä¸­ã€‚æˆ‘ä»¬åœ¨ä¸Šä¸€è¯¾ä¸­çœ‹è¿‡æ¨¡å‹ã€‚
- en: and today we're going to kind of be looking a lot more closely at the data setsã€‚And
    there was a question last week about sort of how do you figure out like a good
    choice of model forã€‚ sayï¼Œ multiclass versus multilaï¼ŸAnd at the time we sort of
    realized that there's no sort of easy way of like filtering these types of modelsã€‚
    but it turns out that for data sets there is a little bit better search for thisï¼Œ
    so for exampleã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†æ›´ä»”ç»†åœ°æŸ¥çœ‹æ•°æ®é›†ã€‚ä¸Šå‘¨æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå…³äºå¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Œæ¯”å¦‚å¤šç±»ä¸å¤šæ ‡ç­¾ï¼Ÿå½“æ—¶æˆ‘ä»¬æ„è¯†åˆ°æ²¡æœ‰ç®€å•çš„æ–¹æ³•æ¥ç­›é€‰è¿™äº›ç±»å‹çš„æ¨¡å‹ï¼Œä½†äº‹å®è¯æ˜ï¼Œå¯¹äºæ•°æ®é›†æ¥è¯´ï¼Œæœ‰ä¸€äº›æ›´å¥½çš„æœç´¢æ–¹æ³•ï¼Œæ¯”å¦‚è¯´ã€‚
- en: if I'm working on a new projectï¼Œ one of the fastest ways to get results early
    is to basically build a sort of baseline or prototype using some public dataã€‚
    this is often much cheaper and faster than waiting for you know to get permission
    to access data in your company or waiting for some domain experts to help you
    with the labelingã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘åœ¨è¿›è¡Œä¸€ä¸ªæ–°é¡¹ç›®ï¼Œè·å¾—æ—©æœŸç»“æœçš„æœ€å¿«æ–¹æ³•ä¹‹ä¸€æ˜¯åŸºæœ¬ä¸Šä½¿ç”¨ä¸€äº›å…¬å…±æ•°æ®æ„å»ºä¸€ä¸ªåŸºçº¿æˆ–åŸå‹ã€‚è¿™é€šå¸¸æ¯”ç­‰å¾…è·å¾—å…¬å¸æ•°æ®è®¿é—®æƒé™æˆ–ç­‰å¾…ä¸€äº›é¢†åŸŸä¸“å®¶å¸®åŠ©ä½ è¿›è¡Œæ ‡æ³¨ä¾¿å®œå¾—å¤šä¸”å¿«å¾—å¤šã€‚
- en: And so for exampleï¼Œ if we were looking at like maybe like a multiclass thingã€‚
    maybe I would pick a text classificationã€‚And then there are other tags here that
    I can sort of filter byã€‚ so just in case I was interested in question answeringï¼Œ
    I can select thatã€‚And then inside text classificationï¼Œ there are a set of subtasks
    that we can look atã€‚ So for exampleã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹å¤šç±»çš„å†…å®¹ã€‚ä¹Ÿè®¸æˆ‘ä¼šé€‰æ‹©æ–‡æœ¬åˆ†ç±»ã€‚ç„¶åè¿™é‡Œæœ‰å…¶ä»–æ ‡ç­¾å¯ä»¥è¿›è¡Œè¿‡æ»¤ã€‚æ‰€ä»¥å¦‚æœæˆ‘å¯¹é—®ç­”æ„Ÿå…´è¶£ï¼Œæˆ‘å¯ä»¥é€‰æ‹©å®ƒã€‚åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä¸€ç»„å­ä»»åŠ¡ã€‚ä¾‹å¦‚ã€‚
- en: here I can maybe select multi label classificationã€‚And then this will return
    a range of data sets which are candidates for the multilabel caseã€‚ so perhaps
    I'm doing something about fake newsï¼Œ then I can look at data set called fake news
    English and vice versa so the datasets hub is a great way of like bootstrapping
    your machine learning projects at workã€‚ and I've found this is a great way also
    of finding some interesting data sets that maybe go beyond the standard ones that
    you often see in NLPã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘å¯ä»¥é€‰æ‹©å¤šæ ‡ç­¾åˆ†ç±»ã€‚è¿™å°†è¿”å›ä¸€ç³»åˆ—æ•°æ®é›†ï¼Œä½œä¸ºå¤šæ ‡ç­¾æ¡ˆä¾‹çš„å€™é€‰é¡¹ã€‚å› æ­¤ï¼Œä¹Ÿè®¸æˆ‘æ­£åœ¨å¤„ç†å‡æ–°é—»ï¼Œé‚£ä¹ˆæˆ‘å¯ä»¥æŸ¥çœ‹ä¸€ä¸ªåä¸ºå‡æ–°é—»è‹±è¯­çš„æ•°æ®é›†ï¼Œåä¹‹äº¦ç„¶ï¼Œå› æ­¤æ•°æ®é›†ä¸­å¿ƒæ˜¯å¯åŠ¨å·¥ä½œä¸­æœºå™¨å­¦ä¹ é¡¹ç›®çš„å¥½æ–¹æ³•ã€‚æˆ‘å‘ç°è¿™ä¹Ÿæ˜¯å¯»æ‰¾ä¸€äº›æœ‰è¶£çš„æ•°æ®é›†çš„å¥½æ–¹æ³•ï¼Œå¯èƒ½è¶…å‡ºä½ åœ¨NLPä¸­å¸¸è§çš„æ ‡å‡†æ•°æ®é›†ã€‚
- en: so almost everyone uses IMDV movie reviews to show their training but there's
    a lot of extra things here that are quite interesting and challenging because
    many of these are community providedã€‚And just a side noteï¼Œ if you have an interesting
    data setã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹æ¯ä¸ªäººéƒ½ä½¿ç”¨IMDBç”µå½±è¯„è®ºæ¥å±•ç¤ºä»–ä»¬çš„è®­ç»ƒï¼Œä½†è¿™é‡Œæœ‰å¾ˆå¤šé¢å¤–çš„ä¸œè¥¿éå¸¸æœ‰è¶£å’Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè®¸å¤šæ•°æ®é›†æ˜¯ç¤¾åŒºæä¾›çš„ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªæœ‰è¶£çš„æ•°æ®é›†ã€‚
- en: That you think would be valuable to the communityï¼Œ you can either contribute
    it directly to the data sets repositoryã€‚And so inside the datasets repositoryï¼Œ
    we have a datasets folder which has all of these hundreds of data sets that are
    basically reviewed and curated by the Huging piece team so you can open a pull
    request and there's instructions in the documentation on how to do thisã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è®¤ä¸ºå¯¹ç¤¾åŒºæœ‰ä»·å€¼ï¼Œä½ å¯ä»¥ç›´æ¥å°†å…¶è´¡çŒ®ç»™æ•°æ®é›†åº“ã€‚å› æ­¤ï¼Œåœ¨æ•°æ®é›†åº“å†…éƒ¨ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†æ–‡ä»¶å¤¹ï¼Œé‡Œé¢æœ‰æ•°ç™¾ä¸ªåŸºæœ¬ä¸Šç”±Hugging Faceå›¢é˜Ÿå®¡æ ¸å’Œç­–åˆ’çš„æ•°æ®é›†ï¼Œä½ å¯ä»¥æ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚ï¼Œæ–‡æ¡£ä¸­æœ‰å…³äºå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„è¯´æ˜ã€‚
- en: Or if you wish to provide a community data setï¼Œ which is something that more
    or less you don't have to integrate into the library directlyã€‚ you can also do
    that and there are some links in the documentation on how to basically submit
    data sets to the hubã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…å¦‚æœä½ æƒ³æä¾›ä¸€ä¸ªç¤¾åŒºæ•°æ®é›†ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä½ ä¸éœ€è¦ç›´æ¥å°†å…¶é›†æˆåˆ°åº“ä¸­ã€‚ä½ ä¹Ÿå¯ä»¥è¿™æ ·åšï¼Œæ–‡æ¡£ä¸­æœ‰ä¸€äº›é“¾æ¥å…³äºå¦‚ä½•æäº¤æ•°æ®é›†åˆ°ä¸­å¿ƒã€‚
- en: So let's have a look if there's any questions done so farã€‚Greatã€‚ so maybe Oma
    can put some links to that communityã€‚è¯¶ã€‚Versus canonical desã€‚Okay so there's a
    question from SRM sumUma datas library looks very specific to NLP I couldn't find
    many data sets related to visionã€‚ is there a limitation to host vision related
    dataï¼ŸSo the short answer is noã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹åˆ°ç›®å‰ä¸ºæ­¢æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜ã€‚å¤ªå¥½äº†ã€‚ä¹Ÿè®¸Omaå¯ä»¥å°†ä¸€äº›é“¾æ¥æ”¾åˆ°é‚£ä¸ªç¤¾åŒºã€‚è¯¶ã€‚ä¸è§„èŒƒçš„desç›¸æ¯”ã€‚å¥½å§ï¼ŒSRM sumUmaæœ‰ä¸ªé—®é¢˜ï¼Œæ•°æ®é›†åº“çœ‹èµ·æ¥éå¸¸å…·ä½“äºNLPï¼Œæˆ‘æ‰¾ä¸åˆ°å¾ˆå¤šä¸è§†è§‰ç›¸å…³çš„æ•°æ®é›†ã€‚æ˜¯å¦æœ‰æ‰˜ç®¡è§†è§‰ç›¸å…³æ•°æ®çš„é™åˆ¶ï¼Ÿç®€çŸ­çš„å›ç­”æ˜¯æ²¡æœ‰ã€‚
- en: there isn't a limitation in the libraryï¼Œ it's just that we haven't devoted a
    lot of effort to integrating vision dataset sets and if you have some ideas of
    vision dataset sets you would like to add by all means feel free to either open
    a PR in the dataset sets libraryã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åº“ä¸­æ²¡æœ‰é™åˆ¶ï¼Œåªæ˜¯æˆ‘ä»¬å°šæœªæŠ•å…¥å¤§é‡ç²¾åŠ›æ•´åˆè§†è§‰æ•°æ®é›†ï¼Œå¦‚æœä½ æœ‰ä¸€äº›æƒ³æ·»åŠ çš„è§†è§‰æ•°æ®é›†çš„æƒ³æ³•ï¼Œè¯·éšæ„åœ¨æ•°æ®é›†åº“ä¸­æ‰“å¼€ä¸€ä¸ªæ‹‰å–è¯·æ±‚ã€‚
- en: Or just reach out in the forums basicallyã€‚At the momentï¼Œ we haveï¼Œ if I look
    hereï¼Œ let's seeã€‚ Can I find visionã€‚Do we have tag revisionï¼ŸLet's have a lookã€‚So
    I don't think we have a tag for vision yetã€‚But I know that we haveï¼Œ let's seeã€‚
    I think we have CFARã€‚We don't have CFfaï¼Œ maybe imtã€‚Interestingã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…åŸºæœ¬ä¸Šåœ¨è®ºå›ä¸Šè”ç³»ã€‚åœ¨æ­¤æ—¶ï¼Œå¦‚æœæˆ‘æŸ¥çœ‹è¿™é‡Œï¼Œçœ‹çœ‹ã€‚æˆ‘èƒ½æ‰¾åˆ°è§†è§‰å—ï¼Ÿæˆ‘ä»¬æœ‰æ ‡ç­¾ä¿®è®¢å—ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹ã€‚æ‰€ä»¥æˆ‘ä¸è®¤ä¸ºæˆ‘ä»¬è¿˜æœ‰è§†è§‰æ ‡ç­¾ã€‚ä½†æˆ‘çŸ¥é“æˆ‘ä»¬æœ‰ï¼Œè®©æˆ‘çœ‹çœ‹ã€‚æˆ‘æƒ³æˆ‘ä»¬æœ‰CFARã€‚æˆ‘ä»¬æ²¡æœ‰CFfaï¼Œå¯èƒ½æ˜¯imtã€‚å¾ˆæœ‰è¶£ã€‚
- en: I was fairly sure we had these data setsã€‚ Okayï¼Œ soã€‚ so maybe some of these vision
    data sets are not yet's because I this okayã€‚CAï¼Œ yeahï¼Œ good Okayã€‚ so we have CF
    10ï¼Œ CF 100 and these data sets you can use using the data libraryã€‚And there's
    also othersï¼Œ I think I'm pretty sure mnes is hereã€‚ So there's fashion mnesistã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸å½“ç¡®å®šæˆ‘ä»¬æœ‰è¿™äº›æ•°æ®é›†ã€‚å¥½çš„ï¼Œé‚£ä¹ˆã€‚ä¹Ÿè®¸ä¸€äº›è§†è§‰æ•°æ®é›†è¿˜æœªä¸Šçº¿ï¼Œå› ä¸ºæˆ‘è¿™å¥½å§ã€‚CAï¼Œæ˜¯çš„ï¼Œå¥½çš„ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰CF 10ï¼ŒCF 100ï¼Œè¿™äº›æ•°æ®é›†å¯ä»¥ä½¿ç”¨æ•°æ®åº“ã€‚è¿˜æœ‰å…¶ä»–çš„ï¼Œæˆ‘æƒ³æˆ‘å¾ˆç¡®å®šmnesä¹Ÿåœ¨è¿™é‡Œã€‚æ‰€ä»¥æœ‰æ—¶å°šmnesistã€‚
- en: mist and so onã€‚So great questionã€‚So I think I can ch the link in the chat hereã€‚
    So this willã€‚ this is the link to Mistã€‚Let's put this hereã€‚ğŸ˜”ï¼ŒCoolã€‚ so I hope that
    answers your question S our consumerã€‚å—¯ã€‚ğŸ˜Šï¼ŒOkayã€‚ so now maybe what we can do to
    start with is let's have a sort of look at the coabab for the data sets libraryã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: mistç­‰ç­‰ã€‚æ‰€ä»¥è¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚æˆ‘æƒ³æˆ‘å¯ä»¥åœ¨èŠå¤©ä¸­åˆ†äº«é“¾æ¥ã€‚è¿™æ˜¯ã€‚è¿™ä¸ªæ˜¯Mistçš„é“¾æ¥ã€‚è®©æˆ‘ä»¬æ”¾åœ¨è¿™é‡Œã€‚ğŸ˜”ï¼Œé…·ã€‚å¸Œæœ›è¿™èƒ½å›ç­”ä½ çš„é—®é¢˜ï¼Œæ¶ˆè´¹è€…ã€‚å—¯ã€‚ğŸ˜Šï¼Œå¥½çš„ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åšçš„äº‹æƒ…æ˜¯å…ˆçœ‹çœ‹æ•°æ®é›†åº“çš„coababã€‚
- en: So we kind of get our hands dirty with thisï¼Œ soã€‚As usualã€‚ the first thing I
    want to do is install data sets and transformersã€‚ and I'm going to use the captureã€‚![](img/a1136557282a500704797139dc43d7b8_5.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¼€å§‹åŠ¨æ‰‹æ“ä½œã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘æƒ³åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å®‰è£…æ•°æ®é›†å’Œå˜æ¢å™¨ã€‚æˆ‘å°†ä½¿ç”¨æ•è·ã€‚![](img/a1136557282a500704797139dc43d7b8_5.png)
- en: '![](img/a1136557282a500704797139dc43d7b8_6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_6.png)'
- en: Magic command to capture all the Pip junk that comes outã€‚å—¯ã€‚ğŸ˜Šï¼ŒSo once this will
    take a few minutesã€‚And remember that the basic process in the sort of transformers's
    workflow is we first we need a tokenizer to convert our text raw text into input
    IDsã€‚And then we need a model to process those input ID and convert them into numerical
    outputs that we can build predictions onã€‚So basically hereï¼Œ I'm just instantiating
    a tokenizer and a modelã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é­”æ³•å‘½ä»¤æ¥æ•è·æ‰€æœ‰è¾“å‡ºçš„Pipåƒåœ¾ã€‚å—¯ã€‚ğŸ˜Šï¼Œæ‰€ä»¥è¿™å°†èŠ±è´¹å‡ åˆ†é’Ÿã€‚è¯·è®°ä½ï¼Œå˜æ¢å™¨å·¥ä½œæµä¸­çš„åŸºæœ¬è¿‡ç¨‹æ˜¯ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ä¸ªåˆ†è¯å™¨æ¥å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºè¾“å…¥IDã€‚ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥å¤„ç†è¿™äº›è¾“å…¥IDå¹¶å°†å…¶è½¬æ¢ä¸ºæˆ‘ä»¬å¯ä»¥æ„å»ºé¢„æµ‹çš„æ•°å€¼è¾“å‡ºã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘åªæ˜¯å®ä¾‹åŒ–äº†ä¸€ä¸ªåˆ†è¯å™¨å’Œä¸€ä¸ªæ¨¡å‹ã€‚
- en: So this is going to be Bt based uncasedï¼Œ and this model is an auto model for
    sequence classificationã€‚ so it's a text classification head that we're adding
    on top of the pretrain modelã€‚And this is just a little code snippet that shows
    an example of how to process basically the raw sequencesã€‚ convert them into input
    IDsã€‚And then pass this and the labels through theï¼Œ through the modelã€‚Okayã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°†æ˜¯åŸºäºBtçš„æ— å¤§å°å†™æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåºåˆ—åˆ†ç±»çš„è‡ªåŠ¨æ¨¡å‹ã€‚å› æ­¤ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ·»åŠ çš„æ–‡æœ¬åˆ†ç±»å¤´ã€‚è¿™åªæ˜¯ä¸€ä¸ªå°ä»£ç ç‰‡æ®µï¼Œå±•ç¤ºäº†å¦‚ä½•å¤„ç†åŸºæœ¬çš„åŸå§‹åºåˆ—ï¼Œå°†å…¶è½¬æ¢ä¸ºè¾“å…¥IDã€‚ç„¶åé€šè¿‡æ¨¡å‹ä¼ é€’è¿™äº›å’Œæ ‡ç­¾ã€‚å¥½çš„ã€‚
- en: but we saw that last weekï¼Œ so should be clear the thing that is more interesting
    for us today is to start looking at the data sets they read soã€‚The first thing
    or the most common thing you're going to encounterã€‚Is the load dataset set functionï¼Ÿ
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¸Šå‘¨å·²ç»çœ‹è¿‡äº†ï¼Œæ‰€ä»¥ä»Šå¤©å¯¹æˆ‘ä»¬æ›´æœ‰è¶£çš„äº‹æƒ…æ˜¯å¼€å§‹æŸ¥çœ‹å®ƒä»¬è¯»å–çš„æ•°æ®é›†ã€‚ä½ å°†é‡åˆ°çš„ç¬¬ä¸€ä»¶äº‹æˆ–æœ€å¸¸è§çš„äº‹æƒ…æ˜¯åŠ è½½æ•°æ®é›†å‡½æ•°ï¼Ÿ
- en: And if we just have a look at this loadã€‚Oopsã€‚Load data set functionã€‚You can
    see that it has a lot of different arguments that you can provide the most common
    ones that I use are the pathã€‚ which is essentially the name of the data set this
    is what you will see for exampleï¼Œ on the hubã€‚ so in this case we have glueï¼Œ but
    it could be MNIST it could be whateverã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªåŠ è½½ã€‚å“å‘€ã€‚åŠ è½½æ•°æ®é›†å‡½æ•°ã€‚ä½ å¯ä»¥çœ‹åˆ°å®ƒæœ‰å¾ˆå¤šä¸åŒçš„å‚æ•°ï¼Œä½ å¯ä»¥æä¾›ï¼Œæˆ‘ä½¿ç”¨çš„æœ€å¸¸è§çš„å‚æ•°æ˜¯è·¯å¾„ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯æ•°æ®é›†çš„åç§°ï¼Œè¿™å°±æ˜¯ä½ åœ¨ä¸­å¿ƒçœ‹åˆ°çš„å†…å®¹ï¼Œæ‰€ä»¥åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­æˆ‘ä»¬æœ‰glueï¼Œä½†å®ƒå¯ä»¥æ˜¯MNISTï¼Œæˆ–è€…å…¶ä»–ä»»ä½•ä¸œè¥¿ã€‚
- en: And many data sets on the hubï¼Œ they often have configurations which are like
    sortized subsetsã€‚ so for example glue is a benchmark which has many different
    tasks and each task has a name so here we have the Microsoft paraphrase comprehension
    taskã€‚And so if you want to access a subtask or a subset in a given data setï¼Œ you
    use this name argumentã€‚And then the other one that we'll see that is really common
    is to specify the splitã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸­å¿ƒä¸Šï¼Œè®¸å¤šæ•°æ®é›†é€šå¸¸æœ‰é…ç½®ï¼Œå°±åƒç»†åˆ†å­é›†ã€‚ä¾‹å¦‚ï¼Œglueæ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œå®ƒæœ‰è®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªåç§°ï¼Œè¿™é‡Œæˆ‘ä»¬æœ‰å¾®è½¯çš„åŒä¹‰å¥ç†è§£ä»»åŠ¡ã€‚å¦‚æœä½ æƒ³è®¿é—®ç»™å®šæ•°æ®é›†ä¸­çš„å­ä»»åŠ¡æˆ–å­é›†ï¼Œä½ ä½¿ç”¨è¿™ä¸ªåç§°å‚æ•°ã€‚ç„¶åæˆ‘ä»¬å°†çœ‹åˆ°çš„å¦ä¸€ä¸ªéå¸¸å¸¸è§çš„å‚æ•°æ˜¯æŒ‡å®šæ‹†åˆ†ã€‚
- en: So a lot of the time by well by defaultï¼Œ it will return all the splits that
    are defined in theã€‚In the library so here we're getting a train validation and
    test splitã€‚ but sometimes you just want the train split and so you can specify
    those explicitly and I'll show you howã€‚Okayï¼Œ so now when we do low data setï¼Œ we
    get a data sets object and there are two sort of types of objects you'll typically
    see so the most common one you'll get at the start is something called a data
    ditã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¾ˆå¤šæ—¶å€™é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå°†è¿”å›åº“ä¸­å®šä¹‰çš„æ‰€æœ‰æ‹†åˆ†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¾—åˆ°äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ‹†åˆ†ã€‚ä½†æœ‰æ—¶ä½ åªæƒ³è¦è®­ç»ƒæ‹†åˆ†ï¼Œå› æ­¤å¯ä»¥æ˜ç¡®æŒ‡å®šè¿™äº›ï¼Œæˆ‘ä¼šå‘Šè¯‰ä½ æ€ä¹ˆåšã€‚å¥½çš„ï¼Œç°åœ¨å½“æˆ‘ä»¬åŠ è½½æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ•°æ®é›†å¯¹è±¡ï¼Œé€šå¸¸ä½ ä¼šçœ‹åˆ°ä¸¤ç§ç±»å‹çš„å¯¹è±¡ï¼Œæœ€å¸¸è§çš„æ˜¯ç§°ä¸ºæ•°æ®é›†çš„ä¸œè¥¿ã€‚
- en: And this isï¼Œ you can think of it as basically a dictionary where the keys are
    just a string that correspond to the splitã€‚And the values are something called
    a data set objectã€‚So let's just have a look at one of these examples hereã€‚ so
    if I index into the train key or the train setï¼Œ I'm going to get now a data set
    objectã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å¯¹åº”äºåˆ’åˆ†çš„å­—ç¬¦ä¸²ï¼Œå€¼æ˜¯ç§°ä¸ºæ•°æ®é›†å¯¹è±¡çš„ä¸œè¥¿ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™é‡Œçš„ä¸€ä¸ªç¤ºä¾‹ã€‚æ‰€ä»¥å¦‚æœæˆ‘ç´¢å¼•è®­ç»ƒé”®æˆ–è®­ç»ƒé›†ï¼Œæˆ‘ç°åœ¨å°†å¾—åˆ°ä¸€ä¸ªæ•°æ®é›†å¯¹è±¡ã€‚
- en: And if I look at the first element in that data objectï¼Œ I've got an IDï¼Œ I've
    got a labelã€‚ and I've got the two sentences that I need to figure out if one is
    a paraphrase of the other or notã€‚So in this caseï¼Œ we can see that the first sentence
    says Amrosy accused his brother whom he called the witness of deliberately distorting
    his evidenceã€‚And the second sentence saysï¼Œ referring to him as only the witness
    and Rosy accused his brother of deliberately destroy the evidenceã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘æŸ¥çœ‹è¯¥æ•°æ®å¯¹è±¡ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘æœ‰ä¸€ä¸ªIDï¼Œä¸€ä¸ªæ ‡ç­¾ï¼Œä»¥åŠæˆ‘éœ€è¦åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦æ˜¯å¦ä¸€ä¸ªå¥å­çš„åŒä¹‰æ”¹å†™çš„ä¸¤ä¸ªå¥å­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ªå¥å­è¯´â€œé˜¿å§†ç½—è¥¿æŒ‡æ§ä»–ç§°ä¸ºè¯äººçš„å…„å¼Ÿæ•…æ„æ­ªæ›²è¯æ®â€ã€‚è€Œç¬¬äºŒä¸ªå¥å­åˆ™è¯´ï¼Œç§°ä»–ä¸ºä»…ä»…æ˜¯è¯äººï¼Œé˜¿å§†ç½—è¥¿æŒ‡æ§ä»–çš„å…„å¼Ÿæ•…æ„é”€æ¯è¯æ®ã€‚
- en: So that's a pretty good example of the second sentence being a or maybe the
    first one being a paraphrase of the second oneã€‚ so it's a bit shorter and it captures
    the same informationã€‚And we can see that the label here is oneï¼Œ which as a guessï¼Œ
    is probably indicating that that's trueã€‚It is a paraphrasibleã€‚Okayï¼Œ so just to
    like unpack a little bit this data set dit versus data set thing so because this
    was a bit confusing to me the first time I saw thisã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªç›¸å½“å¥½çš„ä¾‹å­ï¼Œç¬¬äºŒä¸ªå¥å­æ˜¯ç¬¬ä¸€ä¸ªå¥å­çš„åŒä¹‰æ”¹å†™ï¼Œæˆ–è€…ä¹Ÿè®¸ç¬¬ä¸€ä¸ªæ˜¯ç¬¬äºŒä¸ªçš„åŒä¹‰æ”¹å†™ã€‚æ‰€ä»¥å®ƒç¨å¾®çŸ­ä¸€ç‚¹ï¼Œæ•æ‰åˆ°äº†ç›¸åŒçš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„æ ‡ç­¾æ˜¯1ï¼Œä½œä¸ºä¸€ä¸ªçŒœæµ‹ï¼Œå¯èƒ½è¡¨ç¤ºè¿™æ˜¯çœŸçš„ã€‚å®ƒæ˜¯å¯ä»¥æ”¹å†™çš„ã€‚å¥½ï¼Œæ‰€ä»¥ä¸ºäº†ç¨å¾®æ‹†è§£ä¸€ä¸‹è¿™ä¸ªæ•°æ®é›†å­—å…¸å’Œæ•°æ®é›†å¯¹è±¡çš„åŒºåˆ«ï¼Œå› ä¸ºç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªæ—¶æˆ‘è§‰å¾—æœ‰ç‚¹å›°æƒ‘ã€‚
- en: so we know that the raw data sets are given by this data set diict objectã€‚And
    this data set D object has these keysï¼Œ rightï¼Œ so we have train validation and
    Tesetã€‚And if we look at just like a normal sort of Python dictionaryï¼Œ we look
    at the valuesã€‚ then we get now a dictionary or we get a list of the different
    valuesã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çŸ¥é“åŸå§‹æ•°æ®é›†æ˜¯ç”±è¿™ä¸ªæ•°æ®é›†å­—å…¸å¯¹è±¡ç»™å‡ºçš„ã€‚è¿™ä¸ªæ•°æ®é›†å¯¹è±¡æœ‰è¿™äº›é”®ï¼Œå¯¹å§ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ã€‚å¦‚æœæˆ‘ä»¬åƒæŸ¥çœ‹æ™®é€šçš„Pythonå­—å…¸é‚£æ ·æŸ¥çœ‹å®ƒçš„å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç°åœ¨å¾—åˆ°çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæˆ–è€…æˆ‘ä»¬å¾—åˆ°ä¸åŒå€¼çš„åˆ—è¡¨ã€‚
- en: So just think of dataset as a dictionary which maps keys or splits to dataset
    sets and the thing that you do nearly all of your work or your heavy work on are
    actually the data set objects so if we look at a data set objectã€‚Soï¼Œ this rawã€‚Train
    dataã€‚ This data object has a large number of operations that we can doã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æŠŠæ•°æ®é›†æƒ³è±¡æˆä¸€ä¸ªå­—å…¸ï¼Œå°†é”®æˆ–åˆ’åˆ†æ˜ å°„åˆ°æ•°æ®é›†ï¼Œè€Œä½ å‡ ä¹æ‰€æœ‰çš„å·¥ä½œæˆ–é‡å·¥ä½œéƒ½æ˜¯åœ¨æ•°æ®é›†å¯¹è±¡ä¸Šè¿›è¡Œçš„ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸ªæ•°æ®é›†å¯¹è±¡ã€‚è¿™äº›åŸå§‹è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸ªæ•°æ®å¯¹è±¡æœ‰å¾ˆå¤šæ“ä½œå¯ä»¥æ‰§è¡Œã€‚
- en: and we're going to see some of them todayã€‚ so we can add columnsã€‚ We can do
    filteringã€‚ we can load from different formatsã€‚ We can extract some information
    about the dataã€‚ So maybe let's have a look at thatã€‚ If we look at the the info
    attribute of a data setã€‚ This will often tell us maybe we can print itã€‚This will
    okayï¼Œ that like thatã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘ä»¬å°†çœ‹åˆ°å…¶ä¸­çš„ä¸€äº›å†…å®¹ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ·»åŠ åˆ—ã€‚æˆ‘ä»¬å¯ä»¥è¿›è¡Œè¿‡æ»¤ã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸åŒæ ¼å¼åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥æå–ä¸€äº›å…³äºæ•°æ®çš„ä¿¡æ¯ã€‚æ‰€ä»¥ä¹Ÿè®¸æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ•°æ®é›†çš„ä¿¡æ¯å±æ€§ï¼Œè¿™é€šå¸¸ä¼šå‘Šè¯‰æˆ‘ä»¬ï¼Œä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºæ¥ã€‚è¿™å°±å¯ä»¥äº†ï¼Œå°±è¿™æ ·ã€‚
- en: So this will tell us a bit of a description about the data set itselfã€‚ And there's
    a bunch of information about the data types and so onã€‚USo yeahã€‚ basically just
    remember that data set is the thing that we're going to be doing most of our work
    on and data D is just a way of collecting all the splits togetherã€‚So let's just
    see if there's any questions none so farï¼Œ Okayï¼Œ goodã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°†å‘Šè¯‰æˆ‘ä»¬ä¸€äº›å…³äºæ•°æ®é›†æœ¬èº«çš„æè¿°ã€‚è¿˜æœ‰å…³äºæ•°æ®ç±»å‹ç­‰çš„ä¿¡æ¯ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼ŒåŸºæœ¬ä¸Šåªéœ€è®°ä½æ•°æ®é›†æ˜¯æˆ‘ä»¬å°†è¦åšå¤§éƒ¨åˆ†å·¥ä½œçš„åœ°æ–¹ï¼Œè€Œæ•°æ®Dåªæ˜¯æ”¶é›†æ‰€æœ‰åˆ’åˆ†åœ¨ä¸€èµ·çš„ä¸€ç§æ–¹å¼ã€‚æ‰€ä»¥æˆ‘ä»¬æ¥çœ‹çœ‹æœ‰æ²¡æœ‰é—®é¢˜ï¼Œæš‚æ—¶æ²¡æœ‰ï¼Œå¥½ã€‚
- en: So one of the important aspects of a data set is that it has essentially types
    so one of the things that you may have experienced in your work is that most of
    the data you deal with is very like messy or maybe it has like like a CSV file
    and you've got a mix of strings and numbers and so on so forthã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ•°æ®é›†çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯å®ƒæœ¬è´¨ä¸Šæœ‰ç±»å‹ï¼Œæ‰€ä»¥ä½ åœ¨å·¥ä½œä¸­å¯èƒ½ç»å†è¿‡çš„å¤§éƒ¨åˆ†æ•°æ®æ˜¯éå¸¸æ··ä¹±çš„ï¼Œæˆ–è€…ä¹Ÿè®¸å®ƒåƒä¸€ä¸ªCSVæ–‡ä»¶ï¼Œé‡Œé¢æ··åˆäº†å­—ç¬¦ä¸²ã€æ•°å­—ç­‰ç­‰ã€‚
- en: And what the datasets library does is it defines very explicitly what the types
    should be for every column in the dataset setã€‚And so this is very useful because
    it allows you to catch errors earlier and also to do some very fast processingã€‚So
    if you look at the features attribute of a data setã€‚What you'll get is a dictionary
    which shows you essentially the column name as a key and then the data type for
    that column and so we can see here that indeed sentence one is a stringã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†åº“æ‰€åšçš„å°±æ˜¯æ˜ç¡®åœ°å®šä¹‰æ¯ä¸€åˆ—åœ¨æ•°æ®é›†ä¸­çš„ç±»å‹ã€‚è¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ä½ æ›´æ—©åœ°æ•æ‰é”™è¯¯ï¼ŒåŒæ—¶ä¹Ÿèƒ½è¿›è¡Œä¸€äº›éå¸¸å¿«é€Ÿçš„å¤„ç†ã€‚å› æ­¤ï¼Œå¦‚æœä½ æŸ¥çœ‹æ•°æ®é›†çš„ç‰¹å¾å±æ€§ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªå­—å…¸ï¼ŒåŸºæœ¬ä¸Šæ˜¾ç¤ºåˆ—åä½œä¸ºé”®ï¼Œç„¶åæ˜¯è¯¥åˆ—çš„æ•°æ®ç±»å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç¡®å®ï¼Œå¥å­ä¸€æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
- en: so we get something called a value type and here it specifies a stringã€‚But the
    other one that's kind of interesting is maybe the label so here the label column
    it's not just an integer I mean it could be an integerã€‚ but data sets provides
    something called a class label type and this class label type contains information
    about the number of classes that we have or the number of unique labels that we
    haveã€‚ what the names of those labels areã€‚And those are the more basically the
    two things that you need to know is just the names and the number of classesã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå«åšå€¼ç±»å‹çš„ä¸œè¥¿ï¼Œè¿™é‡Œå®ƒæŒ‡å®šä¸ºå­—ç¬¦ä¸²ã€‚ä½†å¦ä¸€ä¸ªæœ‰è¶£çš„å¯èƒ½æ˜¯æ ‡ç­¾ï¼Œå› æ­¤è¿™é‡Œçš„æ ‡ç­¾åˆ—ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè™½ç„¶å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œä½†æ•°æ®é›†æä¾›äº†ä¸€ç§å«åšç±»æ ‡ç­¾ç±»å‹çš„ä¸œè¥¿ï¼Œè¿™ä¸ªç±»æ ‡ç­¾ç±»å‹åŒ…å«æˆ‘ä»¬æ‹¥æœ‰çš„ç±»åˆ«æ•°é‡æˆ–å”¯ä¸€æ ‡ç­¾çš„æ•°é‡çš„ä¿¡æ¯ï¼Œä»¥åŠè¿™äº›æ ‡ç­¾çš„åç§°ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯ä½ éœ€è¦äº†è§£çš„ä¸¤ä¸ªè¦ç‚¹ï¼šæ ‡ç­¾çš„åç§°å’Œç±»åˆ«çš„æ•°é‡ã€‚
- en: And then we can see thatï¼Œ for exampleï¼Œ the ID has just got a data type of integer
    32ã€‚And so one of the things you can do with these featuresï¼Œ so one thing that's
    like kind of niceã€‚ if we so features are just a dictionary so we can access the
    values by keyã€‚So if we get the labelã€‚ this is going to give us this class label
    typeã€‚And this class label type has a couple of handy functionsã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¾‹å¦‚ï¼ŒIDçš„æ•°æ®æ˜¾ç¤ºç±»å‹ä¸ºæ•´æ•°32ã€‚å› æ­¤ï¼Œä½ å¯ä»¥å¯¹è¿™äº›ç‰¹å¾åšçš„ä¸€ä»¶äº‹æƒ…æ˜¯ï¼Œç‰¹å¾åªæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡é”®è®¿é—®å€¼ã€‚å¦‚æœæˆ‘ä»¬è·å–æ ‡ç­¾ï¼Œè¿™å°†ç»™æˆ‘ä»¬è¿™ä¸ªç±»æ ‡ç­¾ç±»å‹ã€‚è¿™ä¸ªç±»æ ‡ç­¾ç±»å‹æœ‰å‡ ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½ã€‚
- en: so one function that I often use a lot is how do I convert my numerical features
    so label one into something that I is like human readable so what I can do is
    there's an int to string functionã€‚And if I put label one hereã€‚It should tell meï¼Œ
    indeedï¼Œ thatã€‚That corresponds to equivalentã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ç»å¸¸ä½¿ç”¨çš„ä¸€ä¸ªåŠŸèƒ½æ˜¯å¦‚ä½•å°†æˆ‘çš„æ•°å€¼ç‰¹å¾è½¬æ¢ä¸ºç±»ä¼¼äºäººç±»å¯è¯»çš„å½¢å¼ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ•´æ•°è½¬å­—ç¬¦ä¸²çš„å‡½æ•°ã€‚å¦‚æœæˆ‘åœ¨è¿™é‡Œæ”¾å…¥æ ‡ç­¾ä¸€ï¼Œå®ƒåº”è¯¥å‘Šè¯‰æˆ‘ï¼Œç¡®å®ï¼Œå®ƒå¯¹åº”äºç­‰æ•ˆå€¼ã€‚
- en: which means that one sentence is a paraphrase or the otherã€‚And just as a sandy
    checkï¼Œ if we do zeroã€‚ then it should be the same for not to equivalentã€‚And so
    this class label featureã€‚Has some documentation on the hub with other functions
    you can look atã€‚ but I find this is quite a powerful way of quickly switching
    between labels that are numbers and labels that are strings so that you can understand
    what's in your data setã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ä¸€å¥è¯æ˜¯å¦ä¸€ä¸ªçš„åŒä¹‰å¥ã€‚ä½œä¸ºä¸€ä¸ªç®€å•çš„æ£€æŸ¥ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨é›¶ï¼Œé‚£ä¹ˆå¯¹äºéç­‰æ•ˆçš„æƒ…å†µï¼Œå®ƒåº”è¯¥æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œè¿™ä¸ªç±»æ ‡ç­¾ç‰¹å¾åœ¨ä¸­å¿ƒæœ‰ä¸€äº›æ–‡æ¡£ä»¥åŠå…¶ä»–ä½ å¯ä»¥æŸ¥çœ‹çš„åŠŸèƒ½ï¼Œä½†æˆ‘å‘ç°è¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿåœ¨æ•°å­—æ ‡ç­¾å’Œå­—ç¬¦ä¸²æ ‡ç­¾ä¹‹é—´åˆ‡æ¢ï¼Œä»¥ä¾¿ä½ èƒ½ç†è§£ä½ çš„æ•°æ®é›†ä¸­çš„å†…å®¹ã€‚
- en: So let's have a lookï¼Œ are there any questionsã€‚Okayï¼Œ so there's a question from
    IM homessã€‚ is there a way to benefit from the convenience of the data set library
    but use a data set that might be privateã€‚ for exampleï¼Œ customer dataï¼ŸYesï¼Œ that's
    a great question and the answer is yesï¼Œ you can so there areã€‚ğŸ˜Šï¼ŒSo previously before
    I joined Hugingface I was working for a telecom company and everything was like
    completely closed off in you know on premise we couldn't use Huging face hubub
    so I actually had to solve this problem so the way you could do this or the way
    I used to do this most frequently is I would actually use pandasã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œæœ‰æ²¡æœ‰é—®é¢˜ã€‚å¥½çš„ï¼ŒIM homessæäº†ä¸€ä¸ªé—®é¢˜ï¼Œæ˜¯å¦æœ‰åŠæ³•åˆ©ç”¨æ•°æ®é›†åº“çš„ä¾¿åˆ©ï¼Œä½†ä½¿ç”¨å¯èƒ½æ˜¯ç§æœ‰çš„æ•°æ®é›†ï¼Œä¾‹å¦‚å®¢æˆ·æ•°æ®ï¼Ÿæ˜¯çš„ï¼Œè¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œç­”æ¡ˆæ˜¯å¯ä»¥çš„ï¼Œæ‰€ä»¥ä¹‹å‰åœ¨æˆ‘åŠ å…¥Hugging
    Faceä¹‹å‰ï¼Œæˆ‘ä¸ºä¸€å®¶ç”µä¿¡å…¬å¸å·¥ä½œï¼Œä¸€åˆ‡éƒ½æ˜¯å®Œå…¨å°é—­çš„ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨Hugging Face hubï¼Œå› æ­¤æˆ‘å®é™…ä¸Šå¿…é¡»è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘é€šå¸¸ä½¿ç”¨çš„æ–¹å¼æ˜¯åˆ©ç”¨pandasã€‚
- en: So I would say input pandas as PDã€‚And then I would create my data frameã€‚ So
    let's suppose I'm going to just make a dummy data frameã€‚ So let's say I haveã€‚Oopsã€‚My
    data frame as followsï¼Œ and I'm going to sayï¼Œ okayã€‚Let's call this a textã€‚Then
    I'm going to putã€‚Hello worldã€‚And then maybe another element todayã€‚Okayï¼Œ and then
    I'll add another columnã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¼šå¯¼å…¥ pandas ä¸º PDã€‚ç„¶åæˆ‘ä¼šåˆ›å»ºæˆ‘çš„æ•°æ®æ¡†ã€‚æ‰€ä»¥å‡è®¾æˆ‘æ‰“ç®—åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„æ•°æ®æ¡†ã€‚æ¯”å¦‚è¯´æˆ‘æœ‰ã€‚å“å‘€ã€‚æˆ‘çš„æ•°æ®æ¡†å¦‚ä¸‹ï¼Œæˆ‘è¦è¯´ï¼Œå¥½çš„ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºæ–‡æœ¬ã€‚ç„¶åæˆ‘ä¼šå†™ã€‚ä½ å¥½ï¼Œä¸–ç•Œã€‚ç„¶åå¯èƒ½å†æ·»åŠ å¦ä¸€ä¸ªå…ƒç´ ï¼Œä»Šå¤©ã€‚å¥½çš„ï¼Œç„¶åæˆ‘ä¼šæ·»åŠ å¦ä¸€åˆ—ã€‚
- en: which is maybe the say the labelã€‚And then maybe this is going to be positiveï¼Œ
    positiveã€‚So this is a super simple data frameï¼Œ but this is something that I would
    load locallyã€‚And of courseã€‚ pandas is amazing for data processingã€‚ But if you
    want to use the the data sets functionalityã€‚ what you can do is create your own
    data setï¼Œ your own custom data setã€‚ So from data setsã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æ˜¯æ ‡ç­¾ã€‚ç„¶åè¿™å¯èƒ½æ˜¯æ­£é¢çš„ï¼Œæ­£é¢çš„ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªè¶…çº§ç®€å•çš„æ•°æ®æ¡†ï¼Œä½†è¿™æ˜¯æˆ‘ä¼šæœ¬åœ°åŠ è½½çš„ä¸œè¥¿ã€‚å½“ç„¶ï¼Œpandas å¯¹æ•°æ®å¤„ç†éå¸¸æ£’ã€‚ä½†å¦‚æœä½ æƒ³ä½¿ç”¨æ•°æ®é›†åŠŸèƒ½ï¼Œä½ å¯ä»¥åˆ›å»ºè‡ªå·±çš„æ•°æ®é›†ï¼Œè‡ªå·±çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚æ‰€ä»¥ä»æ•°æ®é›†å¼€å§‹ã€‚
- en: you can importã€‚The data set objective itselfã€‚And then what we can do is we can
    createã€‚Data setã€‚From theã€‚Let's seeï¼Œ so I think it's fromï¼Œ is it from pandasï¼Œ I
    thinkï¼Ÿ
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å¯¼å…¥ã€‚æ•°æ®é›†çš„ç›®æ ‡æœ¬èº«ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åˆ›å»ºã€‚æ•°æ®é›†ã€‚æˆ‘ä»¬çœ‹çœ‹ï¼Œæˆ‘æƒ³è¿™æ˜¯æ¥è‡ªäº pandasï¼Œæ˜¯å—ï¼Ÿ
- en: And then this should fingers crossed create now a data set objectã€‚Which has
    the features of text and labelï¼Œ has two rowsã€‚ And if we nowï¼Œ for exampleã€‚ look
    at all the elementsï¼Œ we can see that indeedï¼Œ we've now got our own custom data
    setã€‚So this is more or less how I used to work 90% of the timeã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™åº”è¯¥ï¼Œç¥å¥½è¿ï¼Œç°åœ¨åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å¯¹è±¡ã€‚å®ƒå…·æœ‰æ–‡æœ¬å’Œæ ‡ç­¾çš„ç‰¹å¾ï¼ŒåŒ…å«ä¸¤è¡Œã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨ï¼Œä¾‹å¦‚ï¼ŒæŸ¥çœ‹æ‰€æœ‰å…ƒç´ ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¡®å®å¾—åˆ°äº†æˆ‘ä»¬è‡ªå·±çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚æ‰€ä»¥è¿™å¤§è‡´å°±æ˜¯æˆ‘å·¥ä½œ
    90% çš„æ–¹å¼ã€‚
- en: I hope that answers your question I'm homes there are other ways ofã€‚Loading
    dataã€‚ you can load it from CSVï¼Œ you can load it from Jasonã€‚And I think that more
    or less covers most of the use casesã€‚The only time things get a little bit painful
    is if you're dealing with very large data setsã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™èƒ½å›ç­”ä½ çš„é—®é¢˜ï¼Œæˆ‘æƒ³è¿˜æœ‰å…¶ä»–åŠ è½½æ•°æ®çš„æ–¹æ³•ã€‚ä½ å¯ä»¥ä» CSV åŠ è½½ï¼Œå¯ä»¥ä» JSON åŠ è½½ã€‚æˆ‘è®¤ä¸ºè¿™å¤§è‡´æ¶µç›–äº†å¤§éƒ¨åˆ†ç”¨ä¾‹ã€‚å”¯ä¸€è®©äº‹æƒ…æœ‰ç‚¹ç—›è‹¦çš„æƒ…å†µæ˜¯å½“ä½ å¤„ç†éå¸¸å¤§çš„æ•°æ®é›†æ—¶ã€‚
- en: you knowï¼Œ things that maybe don't fit into a pandas data frameã€‚ but there's
    a streaming functionality that has just been implemented into the data sets libraryã€‚And
    so yeahï¼Œ I think basically you can cover almost all use cases this wayã€‚Okayï¼Œ coolï¼Œ
    soã€‚å—¯ã€‚What we've just done so far is we've just loaded our raw data setã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œæœ‰äº›ä¸œè¥¿å¯èƒ½ä¸é€‚åˆæ”¾å…¥ pandas æ•°æ®æ¡†ä¸­ï¼Œä½†æ•°æ®é›†åº“ä¸­åˆšåˆšå®ç°äº†æµå¼åŠŸèƒ½ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºè¿™æ ·å¯ä»¥åŸºæœ¬è¦†ç›–å‡ ä¹æ‰€æœ‰çš„ç”¨ä¾‹ã€‚å¥½çš„ï¼Œé…·ï¼Œæ‰€ä»¥ã€‚å—¯ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªåŠ è½½äº†æˆ‘ä»¬çš„åŸå§‹æ•°æ®é›†ã€‚
- en: And the thing that we we would like to do right is we would like to tokenise
    this so what we've been doing in all of the lessons so far is we've basically
    been tokenizing kind of like string by string or like maybe a list of strings
    so if we look at this example hereã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›åšçš„äº‹æƒ…æ˜¯å¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ï¼Œæ‰€ä»¥åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬åœ¨æ‰€æœ‰è¯¾ç¨‹ä¸­åŸºæœ¬ä¸Šéƒ½æ˜¯é€ä¸ªå­—ç¬¦ä¸²æˆ–è€…å­—ç¬¦ä¸²åˆ—è¡¨è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™é‡Œçš„è¿™ä¸ªä¾‹å­ã€‚
- en: Then when we do the tokenizationã€‚You can see thatã€‚Tokenizing the first sentence
    column is giving us now a list of input IDsã€‚ so we have now basically a set of
    input IDsï¼Œ well let's just have a look at these guysã€‚å—¯ã€‚So yeahã€‚ we've got a big
    list of input IDs corresponding to the first sentence and also to the second sentenceã€‚And
    remember that these Is is what are we used to feed into the transformerã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå½“æˆ‘ä»¬è¿›è¡Œæ ‡è®°åŒ–æ—¶ã€‚ä½ å¯ä»¥çœ‹åˆ°ã€‚æ ‡è®°åŒ–ç¬¬ä¸€å¥åˆ—ç°åœ¨ç»™æˆ‘ä»¬å¸¦æ¥äº†ä¸€ä¸ªè¾“å…¥ ID çš„åˆ—è¡¨ã€‚æ‰€ä»¥æˆ‘ä»¬ç°åœ¨åŸºæœ¬ä¸Šæœ‰ä¸€ç»„è¾“å…¥ IDï¼Œå—¯ã€‚æ˜¯çš„ã€‚æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªä¸ç¬¬ä¸€å¥å’Œç¬¬äºŒå¥å¯¹åº”çš„å¤§è¾“å…¥
    ID åˆ—è¡¨ã€‚è¯·è®°ä½ï¼Œè¿™äº› ID æ˜¯æˆ‘ä»¬ç”¨äºè¾“å…¥åˆ°å˜æ¢å™¨çš„ã€‚
- en: These are the things that go into the embedding layer and then into the transformer
    stackã€‚ And at the endï¼Œ we get something like budgets we can make predictions withã€‚Okayï¼Œ
    soã€‚That's the stuff we've been doing all the time and you can alsoï¼Œ as I think
    we may have seenã€‚ you can convert your IDs back to tokens using the convert IDs
    to tokensã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯è¿›å…¥åµŒå…¥å±‚ç„¶åè¿›å…¥å˜æ¢å™¨å †æ ˆçš„ä¸œè¥¿ã€‚æœ€åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€äº›é¢„ç®—ï¼Œå¯ä»¥ç”¨æ¥è¿›è¡Œé¢„æµ‹ã€‚å¥½çš„ï¼Œæ‰€ä»¥ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä¸€ç›´åœ¨åšçš„äº‹æƒ…ï¼Œä½ ä¹Ÿå¯ä»¥ï¼Œæ­£å¦‚æˆ‘æƒ³æˆ‘ä»¬å¯èƒ½çœ‹åˆ°çš„é‚£æ ·ï¼Œå¯ä»¥ä½¿ç”¨å°†
    ID è½¬æ¢ä¸ºæ ‡è®°çš„åŠŸèƒ½ã€‚
- en: But the thing that is like maybe most interesting or most common is how do you
    basically tokenise the whole data setï¼Ÿ
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯èƒ½æœ€æœ‰è¶£æˆ–æœ€å¸¸è§çš„äº‹æƒ…æ˜¯ä½ å¦‚ä½•åŸºæœ¬ä¸Šå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡è®°åŒ–ï¼Ÿ
- en: And the most common way to do this is to define a functionã€‚You can call it whatever
    you wantã€‚ here it's called tokenized functionã€‚And what this function will do is
    it will operate on every row of the data set and apply whatever you define the
    operation to be in that functionã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸è§çš„åšæ³•æ˜¯å®šä¹‰ä¸€ä¸ªå‡½æ•°ã€‚ä½ å¯ä»¥éšæ„å‘½åã€‚åœ¨è¿™é‡Œå®ƒè¢«ç§°ä¸ºæ ‡è®°åŒ–å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°ä¼šåœ¨æ•°æ®é›†çš„æ¯ä¸€è¡Œä¸Šè¿›è¡Œæ“ä½œï¼Œå¹¶åº”ç”¨ä½ åœ¨å‡½æ•°ä¸­å®šä¹‰çš„æ“ä½œã€‚
- en: So once you've defined your functionï¼Œ you can thenã€‚Apply a mapã€‚Onto your dataset
    and then this will automatically tokenize every single row in your dataã€‚ so we've
    got all the raw stringsï¼Œ sentence oneï¼Œ sentence twoã€‚ and it's converted them automatically
    into input IDs and also this attention mask that we saw last week where we needed
    to figure out how to sort of disable the padding tokens from the attention mechanismã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ å®šä¹‰äº†ä½ çš„å‡½æ•°ï¼Œä½ å°±å¯ä»¥ã€‚å¯¹ä½ çš„æ•°æ®é›†åº”ç”¨æ˜ å°„ã€‚è¿™å°†è‡ªåŠ¨å¯¹ä½ æ•°æ®ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œæ ‡è®°ã€‚å› æ­¤æˆ‘ä»¬å¾—åˆ°äº†æ‰€æœ‰çš„åŸå§‹å­—ç¬¦ä¸²ï¼Œå¥å­ä¸€ï¼Œå¥å­äºŒã€‚å®ƒä»¬è¢«è‡ªåŠ¨è½¬æ¢ä¸ºè¾“å…¥IDï¼Œä»¥åŠæˆ‘ä»¬ä¸Šå‘¨çœ‹åˆ°çš„è¿™ä¸ªæ³¨æ„åŠ›æ©ç ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºå¦‚ä½•ç¦ç”¨å¡«å……æ ‡è®°ä»¥é€‚åº”æ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: Soã€‚ğŸ˜Šï¼ŒThis is a very powerful way of kind of in just more or less one or two
    lines of codeã€‚ automatically tokenizing every single example in your data set
    and it's very fast it can be basically multi processcesed and it can also be run
    in a batched way on a GPU to be even fasterã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ã€‚ğŸ˜Šè¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨å¤§çº¦ä¸€åˆ°ä¸¤è¡Œä»£ç ä¸­ï¼Œè‡ªåŠ¨å¯¹æ•°æ®é›†ä¸­æ¯ä¸€ä¸ªç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼Œé€Ÿåº¦éå¸¸å¿«ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥è¿›è¡Œå¤šè¿›ç¨‹å¤„ç†ï¼Œå¹¶ä¸”ä¹Ÿå¯ä»¥åœ¨GPUä¸Šæ‰¹é‡è¿è¡Œï¼Œä»¥æ›´å¿«çš„é€Ÿåº¦å¤„ç†ã€‚
- en: But maybe let's just sort of look at a sort of very simple example just to break
    down what's going on hereã€‚Let's suppose I wanted to do something which is quite
    commonã€‚ maybe adding a column now that there are faster ways of doing this in
    data setsã€‚ but I'm going to show you how we could do this with a functionã€‚So let's
    add a columnã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¹Ÿè®¸æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€ä¸ªéå¸¸ç®€å•çš„ç¤ºä¾‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£è¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…ã€‚å‡è®¾æˆ‘æƒ³åšä¸€äº›æ¯”è¾ƒå¸¸è§çš„äº‹æƒ…ã€‚ä¹Ÿè®¸æ·»åŠ ä¸€ä¸ªåˆ—ï¼Œç°åœ¨åœ¨æ•°æ®é›†ä¸­æœ‰æ›´å¿«çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ä½†æˆ‘å°†å‘ä½ å±•ç¤ºå¦‚ä½•ç”¨ä¸€ä¸ªå‡½æ•°æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬æ¥æ·»åŠ ä¸€ä¸ªåˆ—ã€‚
- en: And what this column should expect is an exampleï¼Œ or let's just maybe even makeiaï¼Œ
    it's a rowã€‚ it's a row in our data setã€‚And the main thing that this function is
    to return is it has to return a dictionaryã€‚And the reason for that isï¼Œ if you
    look atã€‚One of our examples from the training setã€‚ you can see that it's kind
    of like a dictionary right we've got keys for the column name and value for the
    actual element or you know the element in that say cell if it was a tableã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œè¿™ä¸ªåˆ—åº”è¯¥æœŸæœ›çš„æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ–è€…è¯´è®©æˆ‘ä»¬å‡è®¾è¿™æ˜¯ä¸€è¡Œã€‚å®ƒæ˜¯æˆ‘ä»¬æ•°æ®é›†ä¸­çš„ä¸€è¡Œã€‚è€Œè¿™ä¸ªå‡½æ•°ä¸»è¦éœ€è¦è¿”å›çš„æ˜¯ï¼Œå®ƒå¿…é¡»è¿”å›ä¸€ä¸ªå­—å…¸ã€‚è¿™æ ·åšçš„åŸå› æ˜¯ï¼Œå¦‚æœä½ æŸ¥çœ‹ã€‚æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ã€‚ä½ ä¼šçœ‹åˆ°å®ƒæœ‰ç‚¹åƒä¸€ä¸ªå­—å…¸ï¼Œå¯¹å§ï¼Œæˆ‘ä»¬æœ‰åˆ—åçš„é”®å’Œå®é™…å…ƒç´ çš„å€¼ï¼Œæˆ–è€…è¯´å¦‚æœè¿™æ˜¯ä¸€ä¸ªè¡¨æ ¼ä¸­çš„å•å…ƒæ ¼ã€‚
- en: So what we need to do is we need to returnã€‚It's strangeã€‚Seems a coab does twoã€‚So
    we need to return a dictionary and let's justï¼Œ I'm going to make something upã€‚
    I'm going to say this is a new columnã€‚And I'm going to just make all the value
    of menu new column beã€‚Just a wordï¼Œ hello okayã€‚And if we do thisã€‚Then if I take
    my raw data setsã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦åšçš„æ˜¯è¿”å›ã€‚å¥‡æ€ªã€‚ä¼¼ä¹ååŒåº”ç”¨äº†ä¸¤æ¬¡ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå‡è®¾æˆ‘éšä¾¿ç¼–é€ ä¸€ä¸ªã€‚æˆ‘å°†è¯´è¿™æ˜¯ä¸€ä¸ªæ–°åˆ—ã€‚å¹¶ä¸”æˆ‘ä¼šæŠŠæ–°åˆ—çš„æ‰€æœ‰å€¼éƒ½è®¾ç½®ä¸ºã€‚åªæ˜¯ä¸€ä¸ªå•è¯ï¼Œhelloï¼Œå¥½å—ã€‚å¦‚æœæˆ‘ä»¬è¿™æ ·åšã€‚é‚£ä¹ˆå¦‚æœæˆ‘æ‹¿æˆ‘çš„åŸå§‹æ•°æ®é›†ã€‚
- en: Well let's just take maybe the raw training dataset setã€‚Just to keep it simpleã€‚Then
    when we apply mapï¼Œ we just need to feed at this functionã€‚And it will then automatically
    create a new columnã€‚And just add it to our raw data setã€‚Now one thing you should
    be aware of is that this operation is not in placeã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæˆ‘ä»¬å°±ç”¨åŸå§‹è®­ç»ƒæ•°æ®é›†æ¥è¿›è¡Œç¤ºèŒƒã€‚ä¸ºäº†ç®€åŒ–èµ·è§ã€‚å½“æˆ‘ä»¬åº”ç”¨æ˜ å°„æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­æä¾›è¾“å…¥ã€‚ç„¶åå®ƒä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªæ–°åˆ—ã€‚å¹¶å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„åŸå§‹æ•°æ®é›†ä¸­ã€‚ç°åœ¨ä½ éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œè¿™ä¸ªæ“ä½œä¸æ˜¯å°±åœ°è¿›è¡Œçš„ã€‚
- en: so if you're familiar with pandasï¼Œ there's often operations that are in placeã€‚
    which means you just run this line and then it kind of changes the state of the
    object in data sets mostã€‚ if not all operations out of place and so what this
    would mean is that if you wanted to actually have that column stored in your memoryã€‚
    you would create you know data set with say extra column one you columnã€‚And you
    would then doã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ç†Ÿæ‚‰pandasï¼Œé€šå¸¸ä¼šæœ‰å°±åœ°æ“ä½œã€‚è¿™æ„å‘³ç€ä½ åªéœ€è¿è¡Œè¿™ä¸€è¡Œï¼Œç„¶åå®ƒä¼šæ”¹å˜æ•°æ®é›†å¯¹è±¡çš„çŠ¶æ€ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ“ä½œéƒ½ä¸æ˜¯å°±åœ°è¿›è¡Œçš„ï¼Œè¿™æ„å‘³ç€å¦‚æœä½ æƒ³å°†è¯¥åˆ—å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚ä½ éœ€è¦åˆ›å»ºä¸€ä¸ªåŒ…å«é¢å¤–åˆ—çš„æ•°æ®é›†ï¼Œæ¯”å¦‚è¯´â€œé¢å¤–åˆ—ä¸€â€ã€‚
- en: you knowï¼Œ that equals the mapã€‚And thenï¼Œ when youã€‚Look atã€‚Thisã€‚You now see that
    we have a new column with Hoã€‚So that's been now sort of stored in the memory of
    this new data set objectã€‚Okayï¼Œ so just to recapï¼Œ you a function and the function
    has to always return a dictionary where the key is the name of the column and
    the value is the value you want for that rowã€‚Coolï¼Œ so let's take a small thingã€‚
    Let's seeã€‚I am Homemes just askingã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œè¿™ç­‰äºæ˜ å°„ã€‚ç„¶åï¼Œå½“ä½ æŸ¥çœ‹è¿™ä¸ªæ—¶ã€‚ä½ ç°åœ¨ä¼šçœ‹åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªæ–°çš„åˆ—Hoã€‚å› æ­¤ï¼Œè¿™ç°åœ¨åœ¨è¿™ä¸ªæ–°æ•°æ®é›†å¯¹è±¡çš„å†…å­˜ä¸­å­˜å‚¨äº†ã€‚å¥½çš„ï¼Œæ€»ç»“ä¸€ä¸‹ï¼Œä½ æœ‰ä¸€ä¸ªå‡½æ•°ï¼Œè€Œè¿™ä¸ªå‡½æ•°å¿…é¡»å§‹ç»ˆè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®æ˜¯åˆ—çš„åç§°ï¼Œå€¼æ˜¯ä½ æƒ³è¦çš„é‚£ä¸€è¡Œçš„å€¼ã€‚å¥½å§ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªå°çš„ä¾‹å­ã€‚æˆ‘æ˜¯Homemesï¼Œåªæ˜¯åœ¨è¯¢é—®ã€‚
- en: please could you explain what the attention mask does again okayï¼Œ greatï¼Œ very
    good questionã€‚So let's have a look at this example hereã€‚So in this exampleï¼Œ we've
    got two sentencesã€‚And we can see thatï¼Œ this is maybe not a good exampleï¼Œ maybe
    let's add some paddingã€‚å—¯ã€‚Okayï¼Œ actuallyã€‚ what I need to do to show you an exampleã€‚Is
    I'm going toã€‚Yeahï¼Œ I'm going to do thisã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ä½ å†è§£é‡Šä¸€ä¸‹æ³¨æ„åŠ›æ©ç çš„ä½œç”¨ï¼Œå¥½å—ï¼Ÿå¾ˆå¥½ï¼Œéå¸¸å¥½çš„é—®é¢˜ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªä¾‹å­ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªå¥å­ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¥½çš„ä¾‹å­ï¼Œä¹Ÿè®¸æˆ‘ä»¬æ¥æ·»åŠ ä¸€äº›å¡«å……ã€‚å—¯ã€‚å¥½çš„ï¼Œå®é™…ä¸Šã€‚æˆ‘éœ€è¦åšçš„æ˜¯ç»™ä½ å±•ç¤ºä¸€ä¸ªä¾‹å­ã€‚æˆ‘è¦ã€‚æ˜¯çš„ï¼Œæˆ‘è¦è¿™æ ·åšã€‚
- en: So I'm going toã€‚I'm going to apply truncation in my functionï¼Œ and I'm going
    to add some paddingã€‚And then this is just to show youã€‚What we're talking about
    hereã€‚Okayã€‚ so here I've just tokenized the raw data sets and I've got this tokenized
    data sets objectã€‚And listã€‚Get the first elementï¼Œ which hopefully will show usã€‚å“ˆå“ˆã€‚å—¯ã€‚Okayï¼Œ
    this is a bit of a messy oneã€‚Okayã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è¦åœ¨æˆ‘çš„å‡½æ•°ä¸­åº”ç”¨æˆªæ–­ï¼Œå¹¶ä¸”æˆ‘è¦æ·»åŠ ä¸€äº›å¡«å……ã€‚ç„¶åè¿™åªæ˜¯ä¸ºäº†å‘ä½ å±•ç¤ºæˆ‘ä»¬åœ¨è¿™é‡Œè®¨è®ºçš„å†…å®¹ã€‚å¥½çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘åªæ˜¯å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œäº†åˆ†è¯ï¼Œå¾—åˆ°äº†è¿™ä¸ªåˆ†è¯æ•°æ®é›†å¯¹è±¡ã€‚ç„¶ååˆ—è¡¨ã€‚è·å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå¸Œæœ›èƒ½æ˜¾ç¤ºç»™æˆ‘ä»¬ã€‚å“ˆå“ˆã€‚å—¯ã€‚å¥½çš„ï¼Œè¿™ä¸ªæœ‰ç‚¹æ··ä¹±ã€‚å¥½çš„ã€‚
- en: but let me just try to summarizerise so remember in the last chapterã€‚ we looked
    at this concept of padding and the reason we had to do padding is that all the
    operations that we do inside the transformer are basically matrix multiplicationã€‚And
    when you do matrix multiificationï¼Œ you want to make sure that the matrices you're
    operating on are more or less squareã€‚ So if I haveï¼Œ for exampleï¼Œ one sentence
    and I represent this as a vectorã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è®©æˆ‘å°è¯•æ€»ç»“ä¸€ä¸‹ï¼Œæ‰€ä»¥è®°ä½åœ¨ä¸Šä¸€ç« ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¡«å……çš„æ¦‚å¿µï¼Œè€Œæˆ‘ä»¬éœ€è¦å¡«å……çš„åŸå› æ˜¯æˆ‘ä»¬åœ¨å˜æ¢å™¨å†…éƒ¨è¿›è¡Œçš„æ‰€æœ‰æ“ä½œåŸºæœ¬ä¸Šéƒ½æ˜¯çŸ©é˜µä¹˜æ³•ã€‚å½“ä½ è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ï¼Œä½ æƒ³ç¡®ä¿ä½ æ“ä½œçš„çŸ©é˜µå¤§è‡´æ˜¯æ–¹å½¢çš„ã€‚æ‰€ä»¥å¦‚æœæˆ‘æœ‰ä¸€ä¸ªå¥å­ï¼Œå¹¶ä¸”æˆ‘å°†å…¶è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡ã€‚
- en: Then in order to compare one sentence to another sentenceã€‚ it helps if those
    arrays or those vectors have the same sizeã€‚And so padding is a technique where
    you can basically in the simplest caseã€‚ look at the longest sentence in your batch
    and then just put a zero at the end of every other sentence which basically pads
    out to the length of the longest one and then this will guarantee that your sentence
    or your batch has all vectors of the same sizeã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä¸ºäº†æ¯”è¾ƒä¸€ä¸ªå¥å­å’Œå¦ä¸€ä¸ªå¥å­ï¼Œå¦‚æœè¿™äº›æ•°ç»„æˆ–å‘é‡çš„å¤§å°ç›¸åŒä¼šæ›´æœ‰å¸®åŠ©ã€‚å› æ­¤ï¼Œå¡«å……æ˜¯ä¸€ç§æŠ€æœ¯ï¼ŒåŸºæœ¬ä¸Šåœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥æŸ¥çœ‹æ‰¹æ¬¡ä¸­æœ€é•¿çš„å¥å­ï¼Œç„¶ååœ¨æ¯ä¸ªå…¶ä»–å¥å­çš„æœ«å°¾åŠ ä¸€ä¸ªé›¶ï¼Œè¿™æ ·å°±å¡«å……åˆ°æœ€é•¿å¥å­çš„é•¿åº¦ï¼Œä»è€Œä¿è¯ä½ çš„å¥å­æˆ–æ‰¹æ¬¡ä¸­æ‰€æœ‰å‘é‡çš„å¤§å°ç›¸åŒã€‚
- en: and then when you stack them togetherï¼Œ youve notice something that's rectangularã€‚Now
    that's padding and more or less we do it for just computation reasonsã€‚But the
    problem that it introduces is that the attention mechanism you may remember from
    one of the earlier chaptersã€‚ it basically takes an embeddingï¼Œ so these numerical
    representations that we have of the sequence and it then updates them to create
    something called a contextualized embedding and these contextualized embeddings
    essentially contain for every token in that sequenceã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå½“ä½ å°†å®ƒä»¬å †å åœ¨ä¸€èµ·æ—¶ï¼Œä½ ä¼šæ³¨æ„åˆ°å®ƒå‘ˆçŸ©å½¢ã€‚ç°åœ¨è¿™å°±æ˜¯å¡«å……ï¼Œæˆ‘ä»¬è¿™æ ·åšå¤šå¤šå°‘å°‘æ˜¯å‡ºäºè®¡ç®—çš„åŸå› ã€‚ä½†å®ƒå¸¦æ¥çš„é—®é¢˜æ˜¯ä½ å¯èƒ½è¿˜è®°å¾—ä¹‹å‰ç« èŠ‚æåˆ°çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒåŸºæœ¬ä¸Šå–ä¸€ä¸ªåµŒå…¥ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¯¹åºåˆ—çš„æ•°å€¼è¡¨ç¤ºï¼Œç„¶åæ›´æ–°å®ƒä»¬ä»¥åˆ›å»ºç§°ä¸ºä¸Šä¸‹æ–‡åŒ–åµŒå…¥çš„ä¸œè¥¿ï¼Œè€Œè¿™äº›ä¸Šä¸‹æ–‡åŒ–åµŒå…¥æœ¬è´¨ä¸Šä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªæ ‡è®°åŒ…å«ä¿¡æ¯ã€‚
- en: they contain information that relates the sort of meaning of that token relative
    to the whole sequenceã€‚So an example would be if I have a sentence likeï¼Œ I don't
    knowï¼Œ time flies like an arrowã€‚ then flies is a verb in that caseã€‚But if I have
    another sentenceã€‚ which is like fruit flies like a bananaï¼Œ then flies is an insectã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬åŒ…å«ä¸è¯¥æ ‡è®°ç›¸å¯¹äºæ•´ä¸ªåºåˆ—çš„æ„ä¹‰ç›¸å…³çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘æœ‰ä¸€ä¸ªå¥å­ï¼Œæ¯”å¦‚ï¼Œæˆ‘ä¸çŸ¥é“ï¼Œæ—¶é—´åƒç®­ä¸€æ ·é£é€ã€‚ç„¶ååœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œâ€œé£â€æ˜¯ä¸€ä¸ªåŠ¨è¯ã€‚ä½†å¦‚æœæˆ‘æœ‰å¦ä¸€ä¸ªå¥å­ï¼Œæ¯”å¦‚â€œæœè‡åƒé¦™è•‰ä¸€æ ·é£â€ï¼Œé‚£ä¹ˆâ€œé£â€å°±æ˜¯ä¸€ç§æ˜†è™«ã€‚
- en: And so the attention mechanism allows us to distinguish these two cases because
    the contextualized embedding that represents flies is different in those two cases
    and it uses the whole sequence to develop that representationã€‚Nowï¼Œ because attention
    operates on every single token in the sequenceã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä½¿æˆ‘ä»¬èƒ½å¤ŸåŒºåˆ†è¿™ä¸¤ç§æƒ…å†µï¼Œå› ä¸ºä»£è¡¨é£è¡Œçš„ä¸Šä¸‹æ–‡åŒ–åµŒå…¥åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹æ˜¯ä¸åŒçš„ï¼Œå¹¶ä¸”å®ƒä½¿ç”¨æ•´ä¸ªåºåˆ—æ¥å‘å±•è¯¥è¡¨ç¤ºã€‚ç°åœ¨ï¼Œç”±äºæ³¨æ„åŠ›åœ¨åºåˆ—ä¸­çš„æ¯ä¸ªå•ä¸ªæ ‡è®°ä¸Šè¿›è¡Œæ“ä½œã€‚
- en: it also operates on padding tokens and this would be like a bit of a problem
    because these padding tokens are like these artificial things we injected just
    to make sure all the matrices are squareã€‚And so the attention mask is a way of
    saying to the attention mechanismã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¹Ÿä¼šä½œç”¨äºå¡«å……æ ‡è®°ï¼Œè¿™å¯èƒ½ä¼šæœ‰äº›é—®é¢˜ï¼Œå› ä¸ºè¿™äº›å¡«å……æ ‡è®°å°±åƒæ˜¯æˆ‘ä»¬æ³¨å…¥çš„äººå·¥ä¸œè¥¿ï¼Œä»¥ç¡®ä¿æ‰€æœ‰çŸ©é˜µæ˜¯æ–¹å½¢çš„ã€‚å› æ­¤ï¼Œæ³¨æ„åŠ›æ©ç æ˜¯ä¸€ç§å‘Šè¯‰æ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•ã€‚
- en: pay no attention to the padding tokensã€‚And what an attention mask will look
    likeï¼ŸHereã€‚It will have a bunch of onesï¼Œ so these will be all the tokens at the
    start of the sequence that are just the things we want to develop contextualized
    embeddings forã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦å…³æ³¨å¡«å……æ ‡è®°ã€‚é‚£ä¹ˆæ³¨æ„åŠ›æ©ç çœ‹èµ·æ¥ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿåœ¨è¿™é‡Œã€‚å®ƒä¼šæœ‰ä¸€å †ä¸€ï¼Œæ‰€ä»¥è¿™äº›å°†æ˜¯åºåˆ—å¼€å¤´çš„æ‰€æœ‰æ ‡è®°ï¼Œè¿™äº›æ­£æ˜¯æˆ‘ä»¬å¸Œæœ›å¼€å‘ä¸Šä¸‹æ–‡åŒ–åµŒå…¥çš„å†…å®¹ã€‚
- en: And then at some point you will hit the start of the padding sequenceã€‚ so you've
    got all your words and now you're just adding zeros to your token embeddingsã€‚And
    here the attention mask will then switch to a zeroã€‚ and that will when it goes
    through the attention layerã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æŸä¸ªæ—¶åˆ»ï¼Œä½ ä¼šè¾¾åˆ°å¡«å……åºåˆ—çš„å¼€å§‹ã€‚æ‰€ä»¥ä½ å·²ç»æœ‰äº†æ‰€æœ‰çš„å•è¯ï¼Œç°åœ¨åªæ˜¯åœ¨ä½ çš„æ ‡è®°åµŒå…¥ä¸­æ·»åŠ é›¶ã€‚æ­¤æ—¶ï¼Œæ³¨æ„åŠ›æ©ç å°†åˆ‡æ¢ä¸ºé›¶ï¼Œè¿™æ ·å®ƒé€šè¿‡æ³¨æ„åŠ›å±‚æ—¶ã€‚
- en: it will just disable attention computations on thatã€‚So I hope that explains
    your question I Homemesã€‚ it was a bit of a long winded oneã€‚And so ones for all
    the input Is correct exactly right Yepã€‚ so all the input Is which are not zero
    or generally not zeroã€‚ it will be a one and then it will be zero for all the input
    Is that are zero so we can have a look hereã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ç¦ç”¨è¯¥éƒ¨åˆ†çš„æ³¨æ„åŠ›è®¡ç®—ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›è¿™è§£é‡Šäº†ä½ çš„é—®é¢˜ï¼Œæˆ‘çš„æœ‹å‹ã€‚è¿™ä¸ªé—®é¢˜æœ‰ç‚¹é•¿ã€‚è€Œä¸”æ‰€æœ‰è¾“å…¥IDæ˜¯å®Œå…¨æ­£ç¡®çš„ã€‚æ²¡é”™ï¼Œæ‰€æœ‰ä¸ä¸ºé›¶çš„è¾“å…¥IDä¼šæ˜¯1ï¼Œè€Œæ‰€æœ‰ä¸ºé›¶çš„è¾“å…¥IDä¼šæ˜¯0ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹ã€‚
- en: All the input IDs are on zero and now we've got the paddingã€‚å—¯ã€‚Idsã€‚Okayï¼Œ greatã€‚
    so there's another question which is from Resh Mecheikï¼Œ can we do the reverse
    of a map functionï¼Ÿå—¯ã€‚Let me think if I understand what you meanã€‚ğŸ˜”ï¼ŒSo what kind
    of example would you have in mindï¼Œ Rashï¼Ÿ
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¾“å…¥IDä¸ºé›¶ï¼Œç°åœ¨æˆ‘ä»¬æœ‰äº†å¡«å……ã€‚å—¯ã€‚å¥½çš„ï¼Œæ¥ä¸‹æ¥æœ‰ä¸ªé—®é¢˜æ¥è‡ªResh Mecheikï¼Œæˆ‘ä»¬èƒ½å¦åšmapå‡½æ•°çš„åå‘æ“ä½œï¼Ÿå—¯ã€‚è®©æˆ‘æƒ³æƒ³æˆ‘æ˜¯å¦ç†è§£ä½ çš„æ„æ€ã€‚ğŸ˜”ï¼Œé‚£ä¹ˆä½ å¿ƒä¸­æœ‰ä»€ä¹ˆæ ·çš„ä¾‹å­ï¼ŒRashï¼Ÿ
- en: I'mNot sure I understand the questionï¼Œ but maybe you can write it in the chat
    and then I'll come back to it in a bitã€‚Okayï¼Œ soã€‚We've seen how toã€‚Do tokenization
    across the whole dataset set using the map functionã€‚And theã€‚I think that's more
    or less the main thing in this coã€‚ So I'm just going to delete thisã€‚And the other
    thingã€‚ğŸ˜Šï¼ŒThat is worth pointing out is there's kind of two ways you can do paddingã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ç¡®å®šæˆ‘ç†è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†ä¹Ÿè®¸ä½ å¯ä»¥æŠŠå®ƒå†™åœ¨èŠå¤©ä¸­ï¼Œæˆ‘ç¨åå†å›åˆ°è¿™ä¸ªé—®é¢˜ã€‚å¥½çš„ï¼Œæ‰€ä»¥ã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨mapå‡½æ•°å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡è®°åŒ–ã€‚æˆ‘è®¤ä¸ºè¿™å¤§è‡´æ˜¯è¿™ä¸ªè¯¾ç¨‹çš„ä¸»è¦å†…å®¹ã€‚æ‰€ä»¥æˆ‘å°†åˆ é™¤è¿™ä¸€éƒ¨åˆ†ã€‚è¿˜æœ‰å¦ä¸€ä»¶å€¼å¾—æŒ‡å‡ºçš„äº‹æƒ…æ˜¯ï¼Œæœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥è¿›è¡Œå¡«å……ã€‚
- en: So one way is to explicitly define the padding in the tokenization stepã€‚ So
    what you do is you say padding true or however you wish to implement itã€‚And then
    when you do tokenization with the mat functionã€‚ it will automatically pad all
    the sequences according to how you defined itã€‚Nowã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ ‡è®°åŒ–æ­¥éª¤ä¸­æ˜ç¡®å®šä¹‰å¡«å……ã€‚æ‰€ä»¥ä½ å¯ä»¥è®¾ç½®å¡«å……ä¸ºçœŸï¼Œæˆ–è€…æŒ‰ä½ æƒ³è¦çš„æ–¹å¼å®ç°å®ƒã€‚å½“ä½ ç”¨matå‡½æ•°è¿›è¡Œæ ‡è®°åŒ–æ—¶ï¼Œå®ƒä¼šæ ¹æ®ä½ çš„å®šä¹‰è‡ªåŠ¨å¡«å……æ‰€æœ‰åºåˆ—ã€‚ç°åœ¨ã€‚
- en: the drawback with this is that when you do trainingã€‚Maybe your paddingï¼Œ for
    exampleï¼Œ hereã€‚ picked the longest sentence in the whole data set and then paded
    everything out to the thing in the whole data setã€‚But when we do trainingï¼Œ what
    we're really doing is we're doing training on batches and so one thing that is
    quite common is to do something called dynamic paddingã€‚ which we'll see shortlyï¼Œ
    which is a way of basically sort of adding the padding tokens on the fly and this
    lets us do computations much more efficientlyã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšçš„ç¼ºç‚¹æ˜¯ï¼Œå½“ä½ è¿›è¡Œè®­ç»ƒæ—¶ã€‚ä¹Ÿè®¸ä½ çš„å¡«å……ï¼Œä¾‹å¦‚è¿™é‡Œï¼Œé€‰æ‹©äº†æ•´ä¸ªæ•°æ®é›†ä¸­æœ€é•¿çš„å¥å­ï¼Œç„¶åå¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„å†…å®¹ã€‚ä½†æ˜¯å½“æˆ‘ä»¬è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨å¯¹æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å¸¸è§çš„ä¸€ç§åšæ³•æ˜¯è¿›è¡ŒåŠ¨æ€å¡«å……ã€‚æˆ‘ä»¬å¾ˆå¿«å°±ä¼šçœ‹åˆ°è¿™æ˜¯ä¸€ç§åœ¨è¿è¡Œæ—¶æ·»åŠ å¡«å……æ ‡è®°çš„æ–¹æ³•ï¼Œè¿™è®©æˆ‘ä»¬å¯ä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œè®¡ç®—ã€‚
- en: So I think let's take a look at that video nowã€‚![](img/a1136557282a500704797139dc43d7b8_8.png)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³ç°åœ¨æ¥çœ‹ä¸€ä¸‹é‚£ä¸ªè§†é¢‘ã€‚![](img/a1136557282a500704797139dc43d7b8_8.png)
- en: So let's have a look at how we can do padding on the flowã€‚![](img/a1136557282a500704797139dc43d7b8_10.png)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨æµä¸Šè¿›è¡Œå¡«å……ã€‚![](img/a1136557282a500704797139dc43d7b8_10.png)
- en: What is dynamic beddingï¼ŸIn the batchching inputs Together videoã€‚ we have seen
    that to be able to group inputs of different lengths in the same batchã€‚We need
    to add adding togans to all the shot inputs until by all of the simsã€‚Hereï¼Œ for
    instanceã€‚ the longest sentence is the third oneï¼Œ and we need to add fiveã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯åŠ¨æ€å¡«å……ï¼Ÿåœ¨æ‰¹å¤„ç†è¾“å…¥çš„åŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°èƒ½å¤Ÿå°†ä¸åŒé•¿åº¦çš„è¾“å…¥åˆ†ç»„åˆ°åŒä¸€ä¸ªæ‰¹æ¬¡ä¸­ã€‚æˆ‘ä»¬éœ€è¦ä¸ºæ‰€æœ‰çŸ­è¾“å…¥æ·»åŠ å¡«å……ï¼Œç›´åˆ°æ‰€æœ‰æ ·æœ¬éƒ½è¾¾åˆ°æœ€é•¿çš„é•¿åº¦ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæœ€é•¿çš„å¥å­æ˜¯ç¬¬ä¸‰ä¸ªå¥å­ï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ äº”ä¸ªå¡«å……ã€‚
- en: two or seven petans to the other sentences to have four sentences of the same
    lengthã€‚When dealing with a word data setï¼Œ there are value being strategies we
    can applyã€‚So most of one is to add all the elements of the data set to the same
    lengthã€‚ the length of the longest sampleï¼Œ this will then give us patches that
    all have the same shape determined by the maximum sequence lengthã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºå…¶ä»–å¥å­æ·»åŠ ä¸¤ä¸ªæˆ–ä¸ƒä¸ªå¡«å……ï¼Œä»¥ä½¿å››ä¸ªå¥å­å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚åœ¨å¤„ç†è¯æ•°æ®é›†æ—¶ï¼Œæœ‰ä¸€äº›æˆ‘ä»¬å¯ä»¥åº”ç”¨çš„ç­–ç•¥ã€‚å› æ­¤ï¼Œæœ€å¸¸è§çš„ä¸€ç§æ˜¯å°†æ•°æ®é›†çš„æ‰€æœ‰å…ƒç´ è°ƒæ•´ä¸ºç›¸åŒçš„é•¿åº¦ï¼Œå³æœ€é•¿æ ·æœ¬çš„é•¿åº¦ï¼Œè¿™æ ·å°±ä¼šå¾—åˆ°æ‰€æœ‰å½¢çŠ¶ç›¸åŒçš„å—ï¼Œå½¢çŠ¶ç”±æœ€å¤§åºåˆ—é•¿åº¦å†³å®šã€‚
- en: The downside is that patches composed from short sentences we have a lot of
    patting tokensã€‚ which will introduce more computations in the model we ultimately
    don't needã€‚To avoid thisã€‚ another strategy is to patch the elements when we batch
    them together to the longerest sentence inside the batchã€‚This wayï¼Œ batches compose
    of short input voltage gets smaller and the batch containing the longest sentence
    in the dataset setã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼ºç‚¹æ˜¯ç”±çŸ­å¥å­ç»„æˆçš„å—ä¸­æœ‰å¾ˆå¤šå¡«å……æ ‡è®°ï¼Œè¿™ä¼šåœ¨æ¨¡å‹ä¸­å¼•å…¥æ›´å¤šä¸å¿…è¦çš„è®¡ç®—ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œå¦ä¸€ç§ç­–ç•¥æ˜¯åœ¨å°†å…ƒç´ æ‰¹å¤„ç†åœ¨ä¸€èµ·æ—¶ï¼Œä½¿ç”¨æ‰¹æ¬¡ä¸­çš„æœ€é•¿å¥å­è¿›è¡Œå¡«å……ã€‚è¿™æ ·ï¼Œç”±çŸ­è¾“å…¥ç»„æˆçš„æ‰¹æ¬¡ä½“ç§¯å˜å°ï¼Œè€ŒåŒ…å«æ•°æ®é›†ä¸­æœ€é•¿å¥å­çš„æ‰¹æ¬¡åˆ™ä¿æŒè¾ƒå¤§ã€‚
- en: This will lead some nice speed on CPU and GPUã€‚So the downside is that all batches
    will then have different shapesã€‚ which slow down things on accelerators like TUsã€‚Let's
    do have to apply both strategies in practiceã€‚We have actually seen to applied
    fixed padding in the dataset sets of a view video when we proposepo the R PCC
    datasetã€‚ aftering the dataset Enkenizerï¼Œ we applied the tokenization to all the
    data set with padding and procation to make all samples of lengths 128ã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æé«˜ CPU å’Œ GPU çš„é€Ÿåº¦ã€‚å› æ­¤ï¼Œç¼ºç‚¹æ˜¯æ‰€æœ‰æ‰¹æ¬¡å°†å…·æœ‰ä¸åŒçš„å½¢çŠ¶ï¼Œè¿™ä¼šåœ¨åƒ TUs è¿™æ ·çš„åŠ é€Ÿå™¨ä¸Šå¯¼è‡´é€Ÿåº¦å˜æ…¢ã€‚æˆ‘ä»¬ç¡®å®éœ€è¦åœ¨å®è·µä¸­åŒæ—¶åº”ç”¨è¿™ä¸¤ç§ç­–ç•¥ã€‚å®é™…ä¸Šï¼Œåœ¨æˆ‘ä»¬æå‡º
    R PCC æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬åœ¨ä¸€æ®µè§†é¢‘çš„æ•°æ®é›†ä¸­åº”ç”¨äº†å›ºå®šå¡«å……ï¼Œåœ¨å¤„ç†æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰æ•°æ®é›†è¿›è¡Œäº†å¸¦å¡«å……çš„æ ‡è®°åŒ–ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰æ ·æœ¬çš„é•¿åº¦ä¸º 128ã€‚
- en: As a resultï¼Œ if we pass this data set to a byy toch dataï¼Œ we get patches of
    shape patch size hereã€‚ 16 by 128ã€‚To apply a dynamic paddingï¼Œ we must defer the
    padding to the batch preparationã€‚So we remove that part from a tokenized function
    we still leave the transition part so that inputs that are bigger than the maximum
    lengths accepted by the modelã€‚ usually 512 get trunccateated to that lengthã€‚Then
    we paddle samples poll dynamically by using a data curatorã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†è¿™ä¸ªæ•°æ®é›†ä¼ é€’ç»™ä¸€ä¸ª byy toch æ•°æ®ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°å½¢çŠ¶ä¸º patch size çš„å—ï¼Œè¿™é‡Œæ˜¯ 16 x 128ã€‚ä¸ºäº†åº”ç”¨åŠ¨æ€å¡«å……ï¼Œæˆ‘ä»¬å¿…é¡»å°†å¡«å……æ¨è¿Ÿåˆ°æ‰¹æ¬¡å‡†å¤‡ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¿™ä¸€éƒ¨åˆ†ä»æ ‡è®°åŒ–å‡½æ•°ä¸­ç§»é™¤ï¼Œä½†ä»ç„¶ä¿ç•™è½¬æ¢éƒ¨åˆ†ï¼Œä»¥ç¡®ä¿å¤§äºæ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼ˆé€šå¸¸ä¸º
    512ï¼‰çš„è¾“å…¥è¢«æˆªæ–­åˆ°è¯¥é•¿åº¦ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ•°æ®ç­–åˆ’è€…åŠ¨æ€å¡«å……æ ·æœ¬ã€‚
- en: Those classes in the Tos library are responsible for applyinging all the final
    preproing needed before forming a batchã€‚Hereï¼Œ the decorulator with padding will
    pass the samples whose maximum length inside a patch of sentencesã€‚We pass it to
    the Pythto staalloor as a collate function and observe that the batch sheets generated
    at various lengths all way below the 128 from beforeã€‚Dynamic pitching will almost
    always be faster on CPUs and GPUsï¼Œ so you should apply it if you canã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Tos åº“ä¸­çš„è¿™äº›ç±»è´Ÿè´£åœ¨å½¢æˆæ‰¹æ¬¡ä¹‹å‰è¿›è¡Œæ‰€æœ‰æœ€ç»ˆçš„é¢„å¤„ç†ã€‚è¿™é‡Œï¼Œå¸¦å¡«å……çš„è£…é¥°å™¨å°†é€šè¿‡å¥å­çš„æœ€å¤§é•¿åº¦ä¼ é€’æ ·æœ¬ã€‚æˆ‘ä»¬å°†å…¶ä½œä¸ºèšåˆå‡½æ•°ä¼ é€’ç»™ Pythtoï¼Œå¹¶è§‚å¯Ÿåˆ°ç”Ÿæˆçš„å„ç§é•¿åº¦çš„æ‰¹æ¬¡éƒ½ä½äºä¹‹å‰çš„
    128ã€‚åŠ¨æ€å¡«å……å‡ ä¹æ€»æ˜¯åœ¨ CPU å’Œ GPU ä¸Šæ›´å¿«ï¼Œå› æ­¤å¦‚æœå¯ä»¥çš„è¯ï¼Œä½ åº”è¯¥åº”ç”¨å®ƒã€‚
- en: Remember to switch back to fixed bidding howeverï¼Œ if you run your training script
    on TU or need batches of fixed chipsã€‚Okayï¼Œ so that was a nice explanation of dynamic
    paddingï¼Œ so just to summarizeã€‚ if I have a data set we say a thousand examplesã€‚And
    let's suppose that one example is just way longer than all the othersã€‚ maybe there's
    an error or somethingã€‚So if I just did my tokenizationã€‚å—¯ã€‚On this data setã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—å¦‚æœä½ åœ¨TUä¸Šè¿è¡Œè®­ç»ƒè„šæœ¬æˆ–éœ€è¦å›ºå®šèŠ¯ç‰‡æ‰¹æ¬¡ï¼Œè¿˜æ˜¯è¦åˆ‡æ¢å›å›ºå®šç«æ ‡æ¨¡å¼ã€‚å¥½çš„ï¼Œåˆšæ‰å¯¹åŠ¨æ€å¡«å……çš„è§£é‡Šå¾ˆå¥½ï¼Œæ‰€ä»¥æ€»ç»“ä¸€ä¸‹ã€‚å¦‚æœæˆ‘æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œæ¯”å¦‚è¯´ä¸€åƒä¸ªä¾‹å­ã€‚å‡è®¾å…¶ä¸­ä¸€ä¸ªä¾‹å­çš„é•¿åº¦è¿œè¿œè¶…è¿‡å…¶ä»–ä¾‹å­ã€‚ä¹Ÿè®¸æ˜¯æœ‰é”™è¯¯æˆ–è€…å…¶ä»–åŸå› ã€‚é‚£ä¹ˆå¦‚æœæˆ‘åªæ˜¯å¯¹è¿™ä¸ªæ•°æ®é›†è¿›è¡Œäº†æ ‡è®°åŒ–ã€‚å—¯ã€‚
- en: as we just did before in the colaabã€‚ And we just paded to the maximum length
    of the longest exampleã€‚ Then all the samples will get pushed out to this very
    long oneã€‚ And so we'll have a lot of zeros everywhere that we have to then do
    computations onã€‚ which are slowerã€‚So the alternative is that since we do most
    of our training in terms of batches that we just pad at the batch levelã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬ä¹‹å‰åœ¨colaabä¸­åšçš„é‚£æ ·ã€‚æˆ‘ä»¬åªå¯¹æœ€é•¿ä¾‹å­çš„æœ€å¤§é•¿åº¦è¿›è¡Œäº†å¡«å……ã€‚ç„¶åæ‰€æœ‰æ ·æœ¬éƒ½ä¼šè¢«æ¨é€åˆ°è¿™ä¸ªå¾ˆé•¿çš„ä¾‹å­ä¸Šã€‚å› æ­¤æˆ‘ä»¬ä¼šåœ¨éœ€è¦è®¡ç®—çš„åœ°æ–¹æœ‰å¾ˆå¤šé›¶ï¼Œè¿™æ ·ä¼šå˜å¾—æ›´æ…¢ã€‚å› æ­¤ï¼Œæ›¿ä»£æ–¹æ¡ˆæ˜¯å› ä¸ºæˆ‘ä»¬å¤§éƒ¨åˆ†è®­ç»ƒéƒ½æ˜¯ä»¥æ‰¹æ¬¡è¿›è¡Œçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨æ‰¹æ¬¡çº§åˆ«è¿›è¡Œå¡«å……ã€‚
- en: so instead of padding for the whole data setï¼Œ we just look at the elements in
    a batch and we pad to the longest for example element in that batch or the longest
    example in that batchã€‚And what this will do is it will then require less computationï¼Œ
    so it will be fasterã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬ä¸æ˜¯å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå¡«å……ï¼Œè€Œæ˜¯åªæŸ¥çœ‹ä¸€ä¸ªæ‰¹æ¬¡ä¸­çš„å…ƒç´ ï¼Œå¹¶å¯¹è¯¥æ‰¹æ¬¡ä¸­æœ€é•¿çš„ä¾‹å­è¿›è¡Œå¡«å……ã€‚è¿™æ ·åšçš„ç»“æœæ˜¯å°†éœ€è¦æ›´å°‘çš„è®¡ç®—ï¼Œå› æ­¤ä¼šæ›´å¿«ã€‚
- en: But the downside as Svan explained is that each batch will have different sizesã€‚
    so we're going to have you know maybe in one batch the sentence is only 10 tokens
    long for the longest oneã€‚ so everything is 10 tokens longï¼Œ maybe the second batch
    is 30 and so onã€‚In the Transformers libraryã€‚ we have what are called data colllatorsã€‚And
    these data collators are basically functions that allow us to kind of cleverly
    package together all these different size batches in a way that we can then do
    training efficientlyã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ­£å¦‚Svanæ‰€è§£é‡Šçš„é‚£æ ·ï¼Œæ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ä¼šæœ‰æ‰€ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬å¯èƒ½åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­ï¼Œæœ€é•¿çš„å¥å­åªæœ‰10ä¸ªæ ‡è®°ã€‚å› æ­¤æ‰€æœ‰çš„å¥å­éƒ½æ˜¯10ä¸ªæ ‡è®°ï¼Œä¹Ÿè®¸ç¬¬äºŒä¸ªæ‰¹æ¬¡æ˜¯30ä¸ªï¼Œä»¥æ­¤ç±»æ¨ã€‚åœ¨Transformersåº“ä¸­ï¼Œæˆ‘ä»¬æœ‰æ‰€è°“çš„æ•°æ®æ•´ç†å™¨ã€‚è¿™äº›æ•°æ®æ•´ç†å™¨åŸºæœ¬ä¸Šæ˜¯å…è®¸æˆ‘ä»¬èªæ˜åœ°å°†è¿™äº›ä¸åŒå¤§å°çš„æ‰¹æ¬¡æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°è¿›è¡Œè®­ç»ƒçš„å‡½æ•°ã€‚
- en: So let's maybe look at some questions and then just revisit this col datac stuffã€‚So
    there's a question fromï¼Œ let's seeã€‚Okayï¼Œ so we've got a question from which says
    hi there's a limited labeled data sets that aren't Englishã€‚ would it make sense
    to machine translate a data set to a certain language or domain specific genre
    to fine tune on Yesã€‚ that's a very good question and generally highly recommended
    soã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›é—®é¢˜ï¼Œç„¶åå†å›åˆ°è¿™ä¸ªcol datacçš„å†…å®¹ã€‚æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè®©æˆ‘çœ‹çœ‹ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æ”¶åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼Œå†…å®¹æ˜¯â€œå—¨ï¼Œæœ‰ä¸€äº›æœ‰é™çš„æ ‡æ³¨æ•°æ®é›†ä¸æ˜¯è‹±æ–‡çš„ã€‚å°†æ•°æ®é›†æœºå™¨ç¿»è¯‘æˆæŸç§è¯­è¨€æˆ–ç‰¹å®šé¢†åŸŸçš„ç±»å‹è¿›è¡Œå¾®è°ƒæ˜¯å¦åˆç†ï¼Ÿâ€æ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼Œé€šå¸¸æ˜¯å¼ºçƒˆæ¨èçš„ã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_12.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_12.png)'
- en: A good example of this is there's a data set called MLQAã€‚![](img/a1136557282a500704797139dc43d7b8_14.png)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯æœ‰ä¸€ä¸ªæ•°æ®é›†å«åšMLQAã€‚![](img/a1136557282a500704797139dc43d7b8_14.png)
- en: And this is a multilingual question answering dataset and the authors of this
    dataset basically translated the S datasetã€‚ which is in English into several languages
    so I think they have Germanï¼Œ Spanishï¼Œ Hindiï¼Œ Vietnameseã€‚ simplified Chineseã€‚So
    this is one way of creating data in your domain or your languageã€‚ which you can
    then build models onã€‚The only drawback is that it's often being known that if
    you're training like very large transform modelsã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€é—®ç­”æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†çš„ä½œè€…åŸºæœ¬ä¸Šå°†è‹±è¯­çš„Sæ•°æ®é›†ç¿»è¯‘æˆäº†å‡ ç§è¯­è¨€ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºä»–ä»¬æœ‰å¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€å°åœ°è¯­ã€è¶Šå—è¯­å’Œç®€ä½“ä¸­æ–‡ã€‚è¿™æ˜¯åˆ›å»ºä½ æ‰€åœ¨é¢†åŸŸæˆ–è¯­è¨€æ•°æ®çš„ä¸€ç§æ–¹å¼ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸Šæ„å»ºæ¨¡å‹ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯ï¼Œå¦‚æœä½ åœ¨è®­ç»ƒéå¸¸å¤§çš„å˜æ¢æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šè¢«è®¤ä¸ºæœ‰é—®é¢˜ã€‚
- en: they can learn somehow thatã€‚There's a translation that's been made and they
    will take shortcuts to get good performance so in the end the model that you get
    on your translated text it may be good just in the translated contextã€‚ but if
    you then deploy this with real human interactions then this could be sort of out
    of domain and maybe the model has only really learned how to detect that it was
    translated and not how to actually solve the task you care aboutã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æ€»èƒ½ä»¥æŸç§æ–¹å¼å­¦ä¹ åˆ°è¿™ä¸€ç‚¹ã€‚å·²ç»æœ‰ä¸€ä¸ªç¿»è¯‘è¢«åšè¿‡ï¼Œä»–ä»¬ä¼šé‡‡å–æ·å¾„æ¥è·å¾—è‰¯å¥½çš„è¡¨ç°ï¼Œå› æ­¤æœ€ç»ˆä½ åœ¨ç¿»è¯‘æ–‡æœ¬ä¸­å¾—åˆ°çš„æ¨¡å‹å¯èƒ½åœ¨ç¿»è¯‘çš„ä¸Šä¸‹æ–‡ä¸­è¡¨ç°è‰¯å¥½ã€‚ä½†å¦‚æœä½ å°†å…¶ç”¨äºçœŸå®çš„äººé™…äº¤äº’ï¼Œè¿™å¯èƒ½å°±ä¼šè¶…å‡ºé¢†åŸŸï¼Œæˆ–è®¸æ¨¡å‹åªæ˜¯å­¦ä¼šäº†æ£€æµ‹å‡ºè¿™æ˜¯ç¿»è¯‘è¿‡çš„ï¼Œè€Œä¸æ˜¯å¦‚ä½•çœŸæ­£è§£å†³ä½ å…³å¿ƒçš„ä»»åŠ¡ã€‚
- en: So you have to be a bit careful with thisï¼Œ but it's something that is worth
    trying and it's something I've used as well in Switzerland where I liveã€‚ there
    are four national languages plus English and so most of the time you don't have
    data in English it comes in Italian or French and so you need to do some of this
    translation tricksã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ è¦å¯¹æ­¤ç¨å¾®å°å¿ƒä¸€äº›ï¼Œä½†è¿™å€¼å¾—å°è¯•ï¼Œæˆ‘åœ¨ç‘å£«ä½çš„æ—¶å€™ä¹Ÿç”¨è¿‡è¿™ä¸€ç‚¹ã€‚é‚£é‡Œæœ‰å››ç§å›½å®¶è¯­è¨€åŠ ä¸Šè‹±è¯­ï¼Œå› æ­¤å¤§éƒ¨åˆ†æƒ…å†µä¸‹ä½ æ²¡æœ‰è‹±è¯­çš„æ•°æ®ï¼Œéƒ½æ˜¯æ„å¤§åˆ©è¯­æˆ–æ³•è¯­ï¼Œå› æ­¤ä½ éœ€è¦è¿›è¡Œä¸€äº›ç¿»è¯‘æŠ€å·§ã€‚
- en: å—¯ã€‚ğŸ˜Šï¼ŒAnd one thing that came out recentlyï¼Œ which isï¼Œ I thinkã€‚ quite cool is Facebook
    released a model called M2 M 100ã€‚And this model is a translation modelã€‚ so it's
    a sequence or it's an encoded decoder transformer that we saw in chapter1 and
    it can do basically translations across 100 different languagesã€‚ so you imagine
    you've got basically 100 by 100 matrix of all languages and for every single pair
    you can do a translationã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚ğŸ˜Š æœ€è¿‘å‡ºç°çš„ä¸€ä»¶äº‹æƒ…ï¼Œæˆ‘è®¤ä¸ºæŒºé…·çš„æ˜¯ï¼ŒFacebookå‘å¸ƒäº†ä¸€ä¸ªå«M2M 100çš„æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªç¿»è¯‘æ¨¡å‹ã€‚æ‰€ä»¥å®ƒæ˜¯ä¸€ä¸ªåºåˆ—æˆ–è€…æ˜¯æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« çœ‹åˆ°çš„ç¼–ç è§£ç å˜æ¢å™¨ï¼Œå®ƒå¯ä»¥åŸºæœ¬ä¸Šè¿›è¡Œ100ç§ä¸åŒè¯­è¨€ä¹‹é—´çš„ç¿»è¯‘ã€‚æ‰€ä»¥ä½ å¯ä»¥æƒ³è±¡ï¼Œä½ æœ‰ä¸€ä¸ª100ä¹˜100çš„æ‰€æœ‰è¯­è¨€çš„çŸ©é˜µï¼Œå¯¹äºæ¯ä¸€ä¸ªè¯­è¨€å¯¹ä½ éƒ½å¯ä»¥è¿›è¡Œç¿»è¯‘ã€‚
- en: And so this gives you 9900 pairs you can translate across and it's a very good
    modelã€‚ so if you're looking to do translationï¼Œ I would recommend using this if
    your language is in the1 hundred0 that they coverã€‚Okayï¼Œ and then maybe to revisit
    Russia's questionï¼Œ how could you revert the column that was created before in
    the mapï¼Ÿ
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ç»™ä½ æä¾›äº†9900å¯¹å¯ä»¥ç¿»è¯‘çš„å†…å®¹ï¼Œè€Œä¸”è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¨¡å‹ã€‚å¦‚æœä½ æ­£åœ¨å¯»æ‰¾ç¿»è¯‘ï¼Œæˆ‘å»ºè®®ä½ ä½¿ç”¨è¿™ä¸ªï¼Œå¦‚æœä½ çš„è¯­è¨€åœ¨ä»–ä»¬æ¶µç›–çš„1000ç§è¯­è¨€ä¸­ã€‚å¥½çš„ï¼Œæ¥ä¸‹æ¥ä¹Ÿè®¸å¯ä»¥å›åˆ°ä¿„ç½—æ–¯çš„é—®é¢˜ï¼Œå¦‚ä½•æ¢å¤åœ¨åœ°å›¾ä¸­ä¹‹å‰åˆ›å»ºçš„åˆ—ï¼Ÿ
- en: Okay now understand what you're askingï¼Œ so the question is let's suppose we've
    added this columnã€‚And we've now got a data set with a new columnï¼Œ which says helloã€‚I'll
    show you the specific one for this example is like if I want to delete the columnã€‚
    then what I can do is I can remove columnsã€‚And then here I just need to provide
    a list of column names so I could provide new columnã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘æ˜ç™½ä½ åœ¨é—®ä»€ä¹ˆï¼Œæ‰€ä»¥é—®é¢˜æ˜¯ï¼Œå‡è®¾æˆ‘ä»¬æ·»åŠ äº†è¿™ä¸€åˆ—ã€‚ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œæ–°å¢äº†ä¸€åˆ—ï¼Œæ˜¾ç¤ºâ€œhelloâ€ã€‚æˆ‘ä¼šç»™ä½ å±•ç¤ºä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œæ¯”å¦‚å¦‚æœæˆ‘æƒ³åˆ é™¤è¿™ä¸€åˆ—ã€‚é‚£ä¹ˆæˆ‘å¯ä»¥ç§»é™¤åˆ—ã€‚æ¥ç€åœ¨è¿™é‡Œæˆ‘åªéœ€æä¾›ä¸€ä¸ªåˆ—åçš„åˆ—è¡¨ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥æä¾›â€œnew
    columnâ€ã€‚
- en: And then this will deleteã€‚The new column and recover back the original data
    set that we hadã€‚That more or lessï¼Œ I think would cover most of the cases that
    you want to undo the map operationã€‚The only time it won't really work is if you
    then do other things to your data setã€‚ maybe you change the content of these original
    columns and then you know undoing that isn't easily doneã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™å°†åˆ é™¤ã€‚æ–°åˆ—ï¼Œå¹¶æ¢å¤åˆ°æˆ‘ä»¬åŸæ¥çš„æ•°æ®é›†ã€‚å·®ä¸å¤šï¼Œæˆ‘è®¤ä¸ºè¿™æ¶µç›–äº†ä½ æƒ³è¦æ’¤é”€æ˜ å°„æ“ä½œçš„å¤§å¤šæ•°æƒ…å†µã€‚å”¯ä¸€çš„æƒ…å†µæ˜¯å¦‚æœä½ å¯¹æ•°æ®é›†è¿›è¡Œäº†å…¶ä»–æ“ä½œã€‚ä¹Ÿè®¸ä½ æ”¹å˜äº†è¿™äº›åŸå§‹åˆ—çš„å†…å®¹ï¼Œé‚£ä¹ˆä½ çŸ¥é“ï¼Œæ’¤é”€è¿™ä¸€æ­¥å°±ä¸é‚£ä¹ˆå®¹æ˜“äº†ã€‚
- en: Okayï¼Œ and then we have another question by SRM Zumaï¼Œ which isã€‚Why is dynamic
    padding slow on TUs compared to GPUs is there any intuition behind this Okayã€‚
    so this is testing my very limited TPU knowledgeï¼Œ but my understanding very limited
    understanding is that it's a sort of fundamentally different architecture for
    basically doing numerical or numerical linear algebra or algebraic equations or
    multiplications of matricesã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç„¶åæˆ‘ä»¬æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œæ¥è‡ªSRM Zumaï¼Œé—®é¢˜æ˜¯ã€‚ä¸ºä»€ä¹ˆåœ¨TUsä¸ŠåŠ¨æ€å¡«å……æ¯”åœ¨GPUä¸Šæ…¢ï¼Ÿè¿™èƒŒåæœ‰ä»€ä¹ˆç›´è§‰å—ï¼Ÿå¥½çš„ï¼Œæ‰€ä»¥è¿™è€ƒéªŒæˆ‘çš„æœ‰é™TPUçŸ¥è¯†ï¼Œä½†æˆ‘çš„ç†è§£æ˜¯ï¼Œè¿™æ˜¯ä¸€ç§ä»æ ¹æœ¬ä¸Šä¸åŒçš„æ¶æ„ï¼Œä¸»è¦ç”¨äºè¿›è¡Œæ•°å€¼ã€æ•°å€¼çº¿æ€§ä»£æ•°æˆ–ä»£æ•°æ–¹ç¨‹ï¼Œæˆ–è€…çŸ©é˜µçš„ä¹˜æ³•ã€‚
- en: And I think it's simply just due to the way that the chips are designedã€‚ that
    they are much more efficient if everything is a fixed sized matrix and you're
    not trying to do this like shuffling of data to do the collationã€‚But Omarï¼Œ who's
    in the chat as Haka Lamaï¼Œ used to work at Google so he can maybe provide a much
    better answer in the chatã€‚ğŸ˜Šï¼Œå—¯ã€‚So yeahï¼Œ this is it's a very good questionã€‚ I will
    also really like to know a detailed answerã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™ä»…ä»…æ˜¯ç”±äºèŠ¯ç‰‡è®¾è®¡çš„æ–¹å¼ã€‚å¦‚æœæ‰€æœ‰å†…å®¹éƒ½æ˜¯å›ºå®šå¤§å°çš„çŸ©é˜µï¼Œå®ƒä»¬ä¼šæ›´æœ‰æ•ˆï¼Œè€Œä¸æ˜¯è¯•å›¾å¯¹æ•°æ®è¿›è¡Œæ´—ç‰Œä»¥è¿›è¡Œèšåˆã€‚ä½†æ˜¯åœ¨èŠå¤©ä¸­ä»¥Haka Lamaèº«ä»½å‡ºç°çš„Omaræ›¾åœ¨è°·æ­Œå·¥ä½œï¼Œæ‰€ä»¥ä»–æˆ–è®¸å¯ä»¥åœ¨èŠå¤©ä¸­æä¾›ä¸€ä¸ªæ›´å¥½çš„ç­”æ¡ˆã€‚ğŸ˜Šï¼Œå—¯ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚æˆ‘ä¹Ÿæƒ³çŸ¥é“è¯¦ç»†çš„ç­”æ¡ˆã€‚
- en: So that'll be couple one noteã€‚ğŸ˜Šï¼ŒOkayï¼Œ so what I wanted to do before we start
    training the model is just have a quick look at this what this data col is doingã€‚So
    in transformersï¼Œ there are different data collators to basically handle this dynamic
    paddingã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œæœ‰å‡ ç‚¹éœ€è¦æ³¨æ„ã€‚ğŸ˜Šï¼Œå¥½çš„ï¼Œæˆ‘æƒ³åœ¨å¼€å§‹è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œå¿«é€Ÿçœ‹ä¸€ä¸‹è¿™ä¸ªæ•°æ®èšåˆå™¨åœ¨åšä»€ä¹ˆã€‚åœ¨transformersä¸­ï¼Œæœ‰ä¸åŒçš„æ•°æ®èšåˆå™¨æ¥å¤„ç†è¿™ç§åŠ¨æ€å¡«å……ã€‚
- en: And the one that most of the time we use is data col with paddingã€‚And what this
    will do isã€‚ let's seeã€‚ can I see itï¼Œ Yeahï¼Œ if I get a few samples from my data
    setã€‚What I've got here in my samplesã€‚Is just a list ofï¼Œ you knowï¼Œ the tokenizedã€‚Inputs
    and they've got theã€‚The same length for their inputesã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œæˆ‘ä»¬å¤§å¤šæ•°æ—¶é—´ä½¿ç”¨çš„æ˜¯å¸¦å¡«å……çš„æ•°æ®èšåˆå™¨ã€‚è¿™å°†ä¼šâ€¦â€¦è®©æˆ‘çœ‹çœ‹ã€‚æˆ‘å¯ä»¥çœ‹åˆ°å®ƒå—ï¼Ÿæ˜¯çš„ï¼Œå¦‚æœæˆ‘ä»æ•°æ®é›†ä¸­è·å–å‡ ä¸ªæ ·æœ¬ï¼Œæˆ‘è¿™é‡Œçš„æ ·æœ¬åªæ˜¯ä¸€ä¸ªæ ‡è®°åŒ–è¾“å…¥çš„åˆ—è¡¨ï¼Œå®ƒä»¬çš„è¾“å…¥é•¿åº¦æ˜¯ç›¸åŒçš„ã€‚
- en: So they've been that's probably because Iã€‚Addded paddingï¼Œ so let me get rid
    of padding hereã€‚Sorryã€‚So I'm just going to retokenize my data without the padding
    in the tokenizerã€‚Because we want to do dynamic paddingã€‚And one thing that's very
    coolã€‚ I didn't really mention is that in datas all the operations that you do
    are cached so they're basically stored as arrow tables in your hard drive and
    so then when you want to reprocess it basically checks have I done this computation
    before and if it does it will just load the cached version and that's extremely
    fast so you can see that took like a second and this is very cool if you've done
    something that you know you process like a million examples and then you restart
    the notebook and you don't have to wait again to reprocess themã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»–ä»¬å¯èƒ½æ˜¯å› ä¸ºæˆ‘åŠ äº†å¡«å……ï¼ŒæŠ±æ­‰ï¼Œè®©æˆ‘æŠŠè¿™é‡Œçš„å¡«å……å»æ‰ã€‚æ‰€ä»¥æˆ‘åªæ˜¯æƒ³åœ¨åˆ†è¯å™¨ä¸­å¯¹æ•°æ®è¿›è¡Œé‡æ–°åˆ†è¯ï¼Œè€Œä¸åŠ å¡«å……ã€‚å› ä¸ºæˆ‘ä»¬æƒ³è¦åšåŠ¨æ€å¡«å……ã€‚æœ‰ä¸€ä»¶éå¸¸é…·çš„äº‹æƒ…æˆ‘æ²¡æœ‰æåˆ°ï¼Œå°±æ˜¯åœ¨æ•°æ®ä¸­ä½ æ‰€åšçš„æ‰€æœ‰æ“ä½œéƒ½æ˜¯ç¼“å­˜çš„ï¼Œå› æ­¤å®ƒä»¬åŸºæœ¬ä¸Šæ˜¯ä½œä¸ºç®­å¤´è¡¨å­˜å‚¨åœ¨ä½ çš„ç¡¬ç›˜ä¸Šï¼Œæ‰€ä»¥å½“ä½ æƒ³è¦é‡æ–°å¤„ç†æ—¶ï¼Œå®ƒåŸºæœ¬ä¸Šä¼šæ£€æŸ¥æˆ‘æ˜¯å¦ä¹‹å‰åšè¿‡è¿™ä¸ªè®¡ç®—ï¼Œå¦‚æœåšè¿‡ï¼Œå®ƒä¼šç›´æ¥åŠ è½½ç¼“å­˜ç‰ˆæœ¬ï¼Œè¿™æ ·éå¸¸å¿«ï¼Œä½ ä¼šçœ‹åˆ°è¿™åªç”¨äº†å¤§çº¦ä¸€ç§’é’Ÿï¼Œå¦‚æœä½ å¤„ç†è¿‡åƒä¸€ç™¾ä¸‡ä¸ªç¤ºä¾‹ï¼Œç„¶åé‡æ–°å¯åŠ¨ç¬”è®°æœ¬ï¼Œä½ å°±ä¸å¿…å†ç­‰å¾…é‡æ–°å¤„ç†å®ƒä»¬ã€‚
- en: the cache will just do that instantaneouslyã€‚Okayï¼Œ goodã€‚ so so now I've got I've
    tokenized my dataã€‚ I have no paddingï¼Œ so all the examples of different lengthsã€‚And
    soã€‚What the data creator will do when I pass it through these samples is it will
    automatically resize in that batchã€‚All the samples to the longest length in that
    batchã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å­˜ä¼šç¬é—´å®Œæˆè¿™ä¸€åˆ‡ã€‚å¥½çš„ï¼Œæ‰€ä»¥ç°åœ¨æˆ‘å·²ç»å¯¹æ•°æ®è¿›è¡Œäº†åˆ†è¯ï¼Œæ²¡æœ‰å¡«å……ï¼Œå› æ­¤æ‰€æœ‰ç¤ºä¾‹çš„é•¿åº¦å„ä¸ç›¸åŒã€‚é‚£ä¹ˆï¼Œå½“æˆ‘å°†æ•°æ®ä¼ é€’ç»™è¿™äº›æ ·æœ¬æ—¶ï¼Œæ•°æ®åˆ›å»ºå™¨ä¼šè‡ªåŠ¨å°†è¯¥æ‰¹æ¬¡ä¸­æ‰€æœ‰æ ·æœ¬è°ƒæ•´ä¸ºè¯¥æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ã€‚
- en: So you can see here that the longest example has 67 tokensã€‚ And so what it's
    done now is it's created tensesorsï¼Œ each of which have essentially 67ã€‚Columns
    you knowï¼Œ for each of the longest tokens and some of those will have paddingã€‚
    and that's what the data colator has done for usã€‚ and And then when we do trainingã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°ï¼Œæœ€é•¿çš„ç¤ºä¾‹æœ‰67ä¸ªæ ‡è®°ã€‚å› æ­¤ç°åœ¨å®ƒåˆ›å»ºäº†å¼ é‡ï¼Œæ¯ä¸ªå¼ é‡æœ¬è´¨ä¸Šæœ‰67åˆ—ï¼Œä½ çŸ¥é“çš„ï¼Œé’ˆå¯¹æ¯ä¸ªæœ€é•¿çš„æ ‡è®°ï¼Œå…¶ä¸­ä¸€äº›ä¼šæœ‰å¡«å……ã€‚è¿™å°±æ˜¯æ•°æ®èšåˆå™¨ä¸ºæˆ‘ä»¬åšçš„ã€‚å½“æˆ‘ä»¬è¿›è¡Œè®­ç»ƒæ—¶ã€‚
- en: it will do all of that for us on the flyã€‚å—¯ã€‚Yeah exactly so the question that
    SRum is asking about the TPUs is that square matrices are what GPUs like as well
    yes that's true so if I'm not mistaken the TPU context is to do with this distinction
    of whether you do dynamic padding or not and with dynamic padding we are kind
    of creating square matricesã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šä¸ºæˆ‘ä»¬å³æ—¶å¤„ç†æ‰€æœ‰è¿™äº›ã€‚å—¯ï¼Œæ²¡é”™ï¼ŒSRumé—®TPUçš„é—®é¢˜æ˜¯ï¼Œå¹³æ–¹çŸ©é˜µä¹Ÿæ˜¯GPUå–œæ¬¢çš„ï¼Œæ˜¯çš„ï¼Œæ²¡é”™ã€‚æ‰€ä»¥å¦‚æœæˆ‘æ²¡è®°é”™çš„è¯ï¼ŒTPUçš„ä¸Šä¸‹æ–‡æ˜¯ä¸åŠ¨æ€å¡«å……çš„åŒºåˆ«æœ‰å…³ï¼Œè€Œä½¿ç”¨åŠ¨æ€å¡«å……æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨åˆ›å»ºå¹³æ–¹çŸ©é˜µã€‚
- en: On a batch levelï¼Œ but we're also moving data aroundã€‚ so we're having to sort
    of kind of dynamically create data on the fly with different shapes and my suspicion
    is that's the thing that maybe slows it down but it's a great question and I should
    look at the answer at some pointã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰¹å¤„ç†çº§åˆ«ä¸Šï¼Œæˆ‘ä»¬è¿˜åœ¨ç§»åŠ¨æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»åŠ¨æ€åˆ›å»ºä¸åŒå½¢çŠ¶çš„æ•°æ®ã€‚æˆ‘æ€€ç–‘è¿™å¯èƒ½æ˜¯å‡æ…¢é€Ÿåº¦çš„åŸå› ï¼Œä½†è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œæˆ‘åº”è¯¥åœ¨æŸä¸ªæ—¶å€™æŸ¥çœ‹ç­”æ¡ˆã€‚
- en: Coolï¼Œ so it's you've asked a very good question that stumped the two guys in
    the courseã€‚ They greatã€‚ thanksã€‚ğŸ˜Šï¼ŒOkayï¼Œ coolï¼Œ so that's more or less data sets
    tokenizationã€‚ğŸ˜Šã€‚Let's now take a look at trainingã€‚Soã€‚What we're going to do is
    look at the trainer APIã€‚So'll launch the videoã€‚Sotrino APIã€‚The Transforms library
    provides a Traer API that allows you to easily function transformformals models
    and your data setã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œä½ æå‡ºäº†ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼Œè¿™è®©è¯¾ç¨‹ä¸­çš„ä¸¤ä½è€å¸ˆæ„Ÿåˆ°å›°æƒ‘ã€‚ä»–ä»¬å¾ˆæ£’ã€‚è°¢è°¢ã€‚ğŸ˜Š å¥½å§ï¼Œå¾ˆå¥½ï¼Œè¿™å·®ä¸å¤šå°±æ˜¯æ•°æ®é›†çš„åˆ†è¯å¤„ç†ã€‚ğŸ˜Š ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒã€‚æˆ‘ä»¬è¦åšçš„æ˜¯æŸ¥çœ‹è®­ç»ƒå™¨
    APIã€‚æˆ‘ä»¬å°†å¯åŠ¨è§†é¢‘ã€‚Sotrino APIã€‚Transforms åº“æä¾›äº†ä¸€ä¸ªè®­ç»ƒå™¨ APIï¼Œå…è®¸ä½ è½»æ¾åœ°è½¬æ¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚
- en: The trainer class takes sure that assets your modelã€‚ as well as the training
    IP parameters and can perform the training on any kind of setupï¼Œ CPUUï¼Œ GPUã€‚ multiple
    GPusï¼Œ TUusã€‚Can also compute the predictions on any dataset setã€‚ and if you provide
    a matrixï¼Œ evaluate your model on any dataset setã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨ç±»ç¡®ä¿ä½ çš„æ¨¡å‹ä»¥åŠè®­ç»ƒ IP å‚æ•°ï¼Œå¹¶å¯ä»¥åœ¨ä»»ä½•è®¾ç½®ä¸Šæ‰§è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬ CPUã€GPUã€å¤š GPUã€TPUã€‚ä¹Ÿå¯ä»¥åœ¨ä»»ä½•æ•°æ®é›†ä¸Šè®¡ç®—é¢„æµ‹ï¼Œå¦‚æœä½ æä¾›çŸ©é˜µï¼Œå¯ä»¥åœ¨ä»»ä½•æ•°æ®é›†ä¸Šè¯„ä¼°ä½ çš„æ¨¡å‹ã€‚
- en: You can also involve final data processing such as dynamic padding as long as
    you provide a tokenizer or given data coulator World5 CP in the MRRPC data set
    since it's relatively small and easy to preprocessã€‚As we saw in the dataset sets
    of a view videoï¼Œ and we can proposepo itã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥è¿›è¡Œæœ€ç»ˆçš„æ•°æ®å¤„ç†ï¼Œä¾‹å¦‚åŠ¨æ€å¡«å……ï¼Œåªè¦ä½ æä¾›ä¸€ä¸ªåˆ†è¯å™¨æˆ–åœ¨ MRRPC æ•°æ®é›†ä¸­ç»™å®šçš„æ•°æ®è®¡ç®—å™¨ World5 CPï¼Œå› ä¸ºå®ƒç›¸å¯¹è¾ƒå°ä¸”æ˜“äºé¢„å¤„ç†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­çš„è§†é¢‘é›†ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æå‡ºè¿™äº›ã€‚
- en: You do not apply padding during the preproingï¼Œ as we will use dynamic padding
    before dataator with paddingã€‚Note that we don't do the final steps of renaming
    removing ins or set the format to torch tensilsã€‚The trainer will do all of this
    automatically for us by analyzing the model signatureã€‚The last step before creating
    the trainer are to define a model and some training of epi parameterssã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åœ¨é¢„å¤„ç†è¿‡ç¨‹ä¸­ä¸åº”ç”¨å¡«å……ï¼Œå› ä¸ºæˆ‘ä»¬å°†åœ¨å¡«å……æ•°æ®å™¨ä¹‹å‰ä½¿ç”¨åŠ¨æ€å¡«å……ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ä¼šè¿›è¡Œæœ€ç»ˆæ­¥éª¤çš„é‡å‘½åã€åˆ é™¤æˆ–è®¾ç½®æ ¼å¼ä¸º torch å¼ é‡ã€‚è®­ç»ƒå™¨å°†é€šè¿‡åˆ†ææ¨¡å‹ç­¾åè‡ªåŠ¨å®Œæˆæ‰€æœ‰è¿™äº›å·¥ä½œã€‚åœ¨åˆ›å»ºè®­ç»ƒå™¨ä¹‹å‰çš„æœ€åä¸€æ­¥æ˜¯å®šä¹‰æ¨¡å‹å’Œä¸€äº›è®­ç»ƒçš„è¶…å‚æ•°ã€‚
- en: We saw to do the first in the model API videoã€‚For the secondï¼Œ we use the training
    argument classã€‚It only takes a path to a folder where results and checkpoint will
    be savedã€‚ but you can also customize all the app parameters your trainer will
    use learning gradeã€‚ number of training asï¼Œ etcã€‚It's been very easy to create a
    trainer and launch a trainingã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆåœ¨æ¨¡å‹ API è§†é¢‘ä¸­è¿›è¡Œäº†ç¬¬ä¸€æ­¥ã€‚å¯¹äºç¬¬äºŒæ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è®­ç»ƒå‚æ•°ç±»ã€‚å®ƒåªéœ€è¦ä¸€ä¸ªæ–‡ä»¶å¤¹è·¯å¾„æ¥ä¿å­˜ç»“æœå’Œæ£€æŸ¥ç‚¹ï¼Œä½†ä½ ä¹Ÿå¯ä»¥è‡ªå®šä¹‰è®­ç»ƒå™¨å°†ä½¿ç”¨çš„æ‰€æœ‰å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€è®­ç»ƒæ¬¡æ•°ç­‰ã€‚åˆ›å»ºä¸€ä¸ªè®­ç»ƒå™¨å¹¶å¯åŠ¨è®­ç»ƒéå¸¸ç®€å•ã€‚
- en: This should display your properties bar and after a few minutes if you're running
    on a GPUã€‚ you should have the training finishedã€‚The result will be however antiticclmatic
    howeverã€‚ as you will only get a training class which doesn't really tell you anything
    about how well your model is performingã€‚This is because we didn't specify any
    metric for the evaluation to get those metricsã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥ä¼šæ˜¾ç¤ºä½ çš„å±æ€§æ ï¼Œå¦‚æœä½ åœ¨ä½¿ç”¨ GPUï¼Œè¿‡å‡ åˆ†é’Ÿä½ åº”è¯¥å°±èƒ½å®Œæˆè®­ç»ƒã€‚ç„¶è€Œï¼Œç»“æœå¯èƒ½ä¼šè®©äººæ„Ÿåˆ°æ„å¤–ï¼Œå› ä¸ºä½ åªä¼šå¾—åˆ°ä¸€ä¸ªè®­ç»ƒç±»åˆ«ï¼Œè¿™å¹¶ä¸èƒ½çœŸæ­£å‘Šè¯‰ä½ æ¨¡å‹è¡¨ç°å¾—å¦‚ä½•ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰æŒ‡å®šä»»ä½•è¯„ä¼°æŒ‡æ ‡æ¥è·å–è¿™äº›æŒ‡æ ‡ã€‚
- en: we'll first gave the predictions on the wall evaluation set using the predict
    methodã€‚It returns a namem to poll with three fields predictionï¼Œ which contains
    the model predictionsã€‚ level IDsï¼Œ which contains the levels if you let as a add
    web and matrixï¼Œ which is empty hereã€‚ we're trying to do thatã€‚The predictions are
    the lus of the model for all the sentences in the dataset setã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆä½¿ç”¨é¢„æµ‹æ–¹æ³•å¯¹å¢™è¯„ä¼°é›†è¿›è¡Œäº†é¢„æµ‹ã€‚å®ƒè¿”å›ä¸€ä¸ªåç§°ï¼ŒåŒ…å«ä¸‰ä¸ªå­—æ®µï¼šé¢„æµ‹ï¼ŒåŒ…å«æ¨¡å‹é¢„æµ‹ï¼›çº§åˆ« IDï¼ŒåŒ…å«çº§åˆ«ï¼›å¦‚æœä½ è®©æˆ‘ä»¬æ·»åŠ ç½‘ç»œå’ŒçŸ©é˜µï¼Œè¿™é‡Œæ˜¯ç©ºçš„ã€‚æˆ‘ä»¬æ­£åœ¨å°è¯•åšåˆ°è¿™ä¸€ç‚¹ã€‚é¢„æµ‹æ˜¯æ¨¡å‹å¯¹æ•°æ®é›†ä¸­æ‰€æœ‰å¥å­çš„è¾“å‡ºã€‚
- en: so an by array of shape 408 by2ã€‚To match them with our labelsã€‚ we need to take
    the maximum look for each prediction to know which of the two classes was predicted
    we do this with the a max functionã€‚Then we can use the matrix from the dataset
    libraryã€‚ it can be loaded as easily as a dataset set with a load metric functionã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæœ‰ä¸€ä¸ªå½¢çŠ¶ä¸º408x2çš„æ•°ç»„ã€‚ä¸ºäº†å°†å®ƒä»¬ä¸æˆ‘ä»¬çš„æ ‡ç­¾åŒ¹é…ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªé¢„æµ‹å–æœ€å¤§å€¼ï¼Œä»¥äº†è§£é¢„æµ‹çš„æ˜¯ä¸¤ä¸ªç±»ä¸­çš„å“ªä¸€ä¸ªï¼Œæˆ‘ä»¬ç”¨maxå‡½æ•°å®Œæˆè¿™ä¸€ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•°æ®é›†åº“ä¸­çš„çŸ©é˜µï¼Œå®ƒå¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åŠ è½½ã€‚
- en: and it returns the evaluation metric used for the datasetã€‚We can see our model
    did learn something as it is 85ã€‚7% accurateã€‚To monitor the evaluation matrix during
    trainingeeï¼Œ we need to define a compute matrix functionã€‚ but does the same step
    as beforeï¼Œ it takes a name to hold with predictions on labels and must return
    a dictionary with the metrics we want to keep track ofã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿”å›ç”¨äºæ•°æ®é›†çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹ç¡®å®å­¦ä¹ äº†ä¸€äº›ä¸œè¥¿ï¼Œå‡†ç¡®ç‡ä¸º85.7%ã€‚ä¸ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§è¯„ä¼°çŸ©é˜µï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªè®¡ç®—çŸ©é˜µå‡½æ•°ã€‚å®ƒçš„æ­¥éª¤ä¸ä¹‹å‰ç›¸åŒï¼Œæ¥å—ä¸€ä¸ªåç§°æ¥ä¿å­˜å¯¹æ ‡ç­¾çš„é¢„æµ‹ï¼Œå¹¶ä¸”å¿…é¡»è¿”å›ä¸€ä¸ªåŒ…å«æˆ‘ä»¬æƒ³è¦è·Ÿè¸ªçš„æŒ‡æ ‡çš„å­—å…¸ã€‚
- en: By passing the epoC evaluation strategy to our training argumentsã€‚ we tell the
    trainer to evaluate at the end of every epochã€‚Lunching a training inside your
    notebook will then display a progress bar and complete the table you see here
    as you pass every apoã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†epoCè¯„ä¼°ç­–ç•¥ä¼ é€’ç»™æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ï¼Œæˆ‘ä»¬å‘Šè¯‰è®­ç»ƒå™¨åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚åœ¨ä½ çš„ç¬”è®°æœ¬ä¸­å¯åŠ¨è®­ç»ƒå°†æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå¹¶åœ¨æ¯ä¸ªå‘¨æœŸç»“æŸæ—¶å®Œæˆä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„è¡¨æ ¼ã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_16.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_16.png)'
- en: '![](img/a1136557282a500704797139dc43d7b8_17.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_17.png)'
- en: Okayï¼Œ so that was a lot of information if you've never seen the trainerã€‚ it
    might be too many things at once so what we're going to do is kind of walk through
    this together and then hopefully at the end of this you'll then have all the tools
    you need to start training your own modelsã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œè¿™ä¿¡æ¯é‡å¾ˆå¤§ï¼Œå¦‚æœä½ ä»æœªè§è¿‡è®­ç»ƒå™¨ï¼Œè¿™å¯èƒ½ä¸€ä¸‹å­æœ‰å¤ªå¤šå†…å®¹ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä¸€èµ·èµ°è¿‡è¿™ä¸ªè¿‡ç¨‹ï¼Œå¸Œæœ›åˆ°æœ€åä½ å°†æ‹¥æœ‰å¼€å§‹è®­ç»ƒè‡ªå·±æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰å·¥å…·ã€‚
- en: And solving your own problemsã€‚So in Google CoLab there are different runtime
    availableã€‚ so by defaultï¼Œ most of the time it's like a CPUï¼Œ so there's no acceleration
    and so what you need to do is hit runtimeã€‚ select the runtime type and for this
    notebook we're going to use a GPU later we'll see how to use a GPUã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³ä½ è‡ªå·±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œåœ¨Google CoLabä¸­æœ‰ä¸åŒçš„è¿è¡Œæ—¶å¯ç”¨ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œé»˜è®¤æ˜¯CPUï¼Œæ²¡æœ‰åŠ é€Ÿï¼Œå› æ­¤ä½ éœ€è¦åšçš„æ˜¯ç‚¹å‡»è¿è¡Œæ—¶ï¼Œé€‰æ‹©è¿è¡Œæ—¶ç±»å‹ï¼Œå¯¹äºè¿™ä¸ªç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å°†ç¨åä½¿ç”¨GPUï¼Œçœ‹çœ‹å¦‚ä½•ä½¿ç”¨GPUã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_19.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_19.png)'
- en: '![](img/a1136557282a500704797139dc43d7b8_20.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_20.png)'
- en: So then we save thisã€‚And now this is going to launch in Google Coab sort of
    a machine that has a GPU in the back endã€‚And one thing you can doã€‚![](img/a1136557282a500704797139dc43d7b8_22.png)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä¿å­˜è¿™ä¸ªã€‚ç°åœ¨è¿™å°†åœ¨Google Coabä¸­å¯åŠ¨ä¸€ç§å…·æœ‰GPUçš„æœºå™¨ã€‚ä½ å¯ä»¥åšçš„ä¸€ä»¶äº‹ã€‚![](img/a1136557282a500704797139dc43d7b8_22.png)
- en: To check what kind of GP you're running is to do NviDdia SMIã€‚![](img/a1136557282a500704797139dc43d7b8_24.png)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ä½ æ­£åœ¨è¿è¡Œå“ªç§GPçš„æ–¹æ³•æ˜¯æ‰§è¡ŒNviDdia SMIã€‚![](img/a1136557282a500704797139dc43d7b8_24.png)
- en: And let's see what we getã€‚If it wakes upã€‚So here we've got a Tesla Kaieã€‚ so
    these are most of the timeã€‚sort of default GPU you getï¼Œ they're not super greatã€‚
    but if you look online you can often find there's people who have saved the state
    of things like P100 collabs and then you can just use them and then they're very
    fastã€‚And of courseï¼Œ if you reset the the run timeï¼Œ so this is sometimes a hack
    if I'mã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¾—åˆ°ä»€ä¹ˆã€‚å¦‚æœå®ƒé†’æ¥ã€‚è¿™é‡Œæˆ‘ä»¬å¾—åˆ°äº†Tesla Kaieï¼Œè¿™äº›é€šå¸¸æ˜¯ä½ å¾—åˆ°çš„é»˜è®¤GPUï¼Œè™½ç„¶ä¸æ˜¯ç‰¹åˆ«å¥½ï¼Œä½†å¦‚æœä½ åœ¨ç½‘ä¸ŠæŸ¥æ‰¾ï¼Œé€šå¸¸å¯ä»¥æ‰¾åˆ°æœ‰äººä¿å­˜äº†åƒP100è¿™æ ·çš„åä½œçŠ¶æ€ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒä»¬ï¼Œéå¸¸å¿«ã€‚å½“ç„¶ï¼Œå¦‚æœä½ é‡ç½®è¿è¡Œæ—¶ï¼Œæœ‰æ—¶è¿™æ˜¯ä¸€ä¸ªå°æŠ€å·§ã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_26.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_26.png)'
- en: Trying toã€‚Get a better GPã€‚ You can do factory resetã€‚![](img/a1136557282a500704797139dc43d7b8_28.png)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•è·å¾—æ›´å¥½çš„GPã€‚ä½ å¯ä»¥è¿›è¡Œå‡ºå‚é‡ç½®ã€‚![](img/a1136557282a500704797139dc43d7b8_28.png)
- en: And then this will just wipe the whole back end and hopefully should now still
    be a GPU yepã€‚And maybe we'll get something a little bit different to a Kï¼Œ let's
    see what we getã€‚Okayã€‚ I'm still stuck with a Katieï¼Œ but sometimes if you're luckyã€‚
    it will give you a P100 or even a Teslaã€‚Okayã€‚ğŸ˜Šï¼ŒSo we've got our GPU back to coab
    nowã€‚ So againã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™å°†æ¸…é™¤æ•´ä¸ªåç«¯ï¼Œå¸Œæœ›ç°åœ¨ä»ç„¶æ˜¯GPUã€‚ä¹Ÿè®¸æˆ‘ä»¬ä¼šå¾—åˆ°ä¸€äº›ä¸Kæœ‰ç‚¹ä¸åŒçš„ä¸œè¥¿ï¼Œçœ‹çœ‹æˆ‘ä»¬å¾—åˆ°ä»€ä¹ˆã€‚å¥½çš„ï¼Œæˆ‘ä»ç„¶å¡åœ¨Katieï¼Œä½†æœ‰æ—¶å¦‚æœä½ è¿æ°”å¥½ï¼Œå®ƒä¼šç»™ä½ ä¸€ä¸ªP100æˆ–ç”šè‡³æ˜¯Teslaã€‚å¥½çš„ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠæˆ‘ä»¬çš„GPUæ¢å¤åˆ°coabäº†ã€‚å†ä¸€æ¬¡ã€‚
- en: we do the same thing weã€‚Install our dependenciesã€‚And what we have here in this
    code is just the same things we did beforeã€‚ so we load a data setã€‚And then we
    define a tokenizer from a checkpointï¼Œ so we're using B basincasedã€‚ We have our
    tokenize functionã€‚And what this tokenized function is doing is it's taking the
    two sentencesã€‚ remember that this data setã€‚Is about trying to predict whether
    one sentence is a paraphrase of anotherã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åšåŒæ ·çš„äº‹æƒ…ï¼Œå®‰è£…æˆ‘ä»¬çš„ä¾èµ–é¡¹ã€‚æˆ‘ä»¬åœ¨è¿™æ®µä»£ç ä¸­æ‰€åšçš„å°±æ˜¯ä¹‹å‰åšè¿‡çš„äº‹æƒ…ã€‚æ‰€ä»¥æˆ‘ä»¬åŠ è½½æ•°æ®é›†ã€‚ç„¶åæˆ‘ä»¬ä»æ£€æŸ¥ç‚¹å®šä¹‰ä¸€ä¸ªåˆ†è¯å™¨ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨B basincasedã€‚æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„åˆ†è¯å‡½æ•°ã€‚è¿™ä¸ªåˆ†è¯å‡½æ•°çš„ä½œç”¨æ˜¯è·å–è¿™ä¸¤ä¸ªå¥å­ã€‚è®°ä½ï¼Œè¿™ä¸ªæ•°æ®é›†æ˜¯å…³äºå°è¯•é¢„æµ‹ä¸€ä¸ªå¥å­æ˜¯å¦æ˜¯å¦ä¸€ä¸ªå¥å­çš„é‡Šä¹‰ã€‚
- en: so we pass both sentences to the tokenizerã€‚We use truncation is true so that
    if one of the sentences is longer than the maximumã€‚Sequence length of Btï¼Œ which
    is 512 tokensï¼Œ it will just trunccate it to 512ã€‚And then we tokenize everything
    and we define this data color so we can do dynamic paddingã€‚So I'm just going to
    run thatã€‚And it should be pretty fastã€‚ğŸ˜”ï¼ŒOkayï¼Œ so so while that's runningã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†è¿™ä¸¤ä¸ªå¥å­ä¼ é€’ç»™åˆ†è¯å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨æˆªæ–­ä¸ºçœŸï¼Œè¿™æ ·å¦‚æœå…¶ä¸­ä¸€ä¸ªå¥å­è¶…è¿‡äº†æœ€å¤§åºåˆ—é•¿åº¦Btï¼ˆ512ä¸ªæ ‡è®°ï¼‰ï¼Œå®ƒä¼šå°†å…¶æˆªæ–­ä¸º512ã€‚ç„¶åæˆ‘ä»¬å¯¹æ‰€æœ‰å†…å®¹è¿›è¡Œåˆ†è¯ï¼Œå¹¶å®šä¹‰è¿™ä¸ªæ•°æ®é›†ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¿›è¡ŒåŠ¨æ€å¡«å……ã€‚æ‰€ä»¥æˆ‘å°±è¦è¿è¡Œè¿™ä¸ªäº†ï¼Œåº”è¯¥ä¼šéå¸¸å¿«ã€‚ğŸ˜”ï¼Œå¥½çš„ï¼Œæ‰€ä»¥åœ¨å®ƒè¿è¡Œçš„æ—¶å€™ã€‚
- en: let's start by looking atã€‚This training argumentsã€‚ this is the first thing that
    you encounter with the trainerï¼Œ so training argumentsã€‚ you can think of it as
    just like a configï¼Œ it's basically a class where you can define various hyperpara
    for trainingã€‚And the only thing you need to specify is an output directory where
    all the information from the training will be storedã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹è®­ç»ƒå‚æ•°ã€‚è¿™æ˜¯ä½ ä¸è®­ç»ƒå¸ˆæ¥è§¦çš„ç¬¬ä¸€ä»¶äº‹ï¼Œæ‰€ä»¥è®­ç»ƒå‚æ•°ã€‚ä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªé…ç½®ï¼Œå®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç±»ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­å®šä¹‰å„ç§è¶…å‚æ•°ç”¨äºè®­ç»ƒã€‚ä½ éœ€è¦æŒ‡å®šçš„å”¯ä¸€å†…å®¹æ˜¯ä¸€ä¸ªè¾“å‡ºç›®å½•ï¼Œæ‰€æœ‰è®­ç»ƒçš„ä¿¡æ¯éƒ½ä¼šå­˜å‚¨åœ¨è¿™é‡Œã€‚
- en: Or if you save the modelï¼Œ this is where it will go so if we look at training
    argumentsã€‚We can see that let's seeï¼Œ can we see through hereã€‚ you can see that
    we have this output directory and then there's a huge range of things like really
    a lotã€‚ you can specify the learning rateï¼Œ you can specify the parameters of the
    optimizerã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¿å­˜æ¨¡å‹ï¼Œå®ƒå°†å­˜æ”¾åœ¨è¿™é‡Œï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è®­ç»ƒå‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ï¼Œæˆ‘ä»¬èƒ½å¦é€è¿‡è¿™é‡Œçœ‹åˆ°ã€‚ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æœ‰è¿™ä¸ªè¾“å‡ºç›®å½•ï¼Œè¿˜æœ‰è®¸å¤šå†…å®¹ï¼Œç¡®å®å¾ˆå¤šã€‚ä½ å¯ä»¥æŒ‡å®šå­¦ä¹ ç‡ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¼˜åŒ–å™¨çš„å‚æ•°ã€‚
- en: you can define the optimizerï¼Œ you can define callbacks kind of like fast AIã€‚
    which is no surprise because Sil develop the trainerã€‚And these callbacks lead
    to control training in clever ways like early stopping and things like thisã€‚So
    I won't go through all of thisï¼Œ but there's basically a large number of parameters
    you can set and you can find that in the documentation for the trainerã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å®šä¹‰ä¼˜åŒ–å™¨ï¼Œä½ å¯ä»¥å®šä¹‰ç±»ä¼¼äºfast AIçš„å›è°ƒã€‚è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºSilå¼€å‘äº†è¿™ä¸ªè®­ç»ƒå™¨ã€‚è¿™äº›å›è°ƒå¯ä»¥ä»¥èªæ˜çš„æ–¹å¼æ§åˆ¶è®­ç»ƒï¼Œä¾‹å¦‚æå‰åœæ­¢ç­‰ã€‚æ‰€ä»¥æˆ‘ä¸ä¼šè¯¦ç»†ä»‹ç»æ‰€æœ‰è¿™äº›ï¼Œä½†åŸºæœ¬ä¸Šä½ å¯ä»¥è®¾ç½®å¤§é‡å‚æ•°ï¼Œä½ å¯ä»¥åœ¨è®­ç»ƒå™¨çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚
- en: And it comes with some pretty good defaultsï¼Œ so most of the time it will just
    work out of the boxã€‚So now we need to load a modelï¼Œ and we're specifying two labels
    because we've got just two classesã€‚ Is it a paraphrase or notã€‚And againï¼Œ it's
    a sequence classification model as we're doing text classificationã€‚And then comes
    the next periodï¼Œ which isï¼Œ you know maybe the more complicated partã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å®ƒæä¾›äº†ä¸€äº›ç›¸å½“ä¸é”™çš„é»˜è®¤å€¼ï¼Œæ‰€ä»¥å¤§å¤šæ•°æ—¶å€™å®ƒä¼šç›´æ¥å¼€ç®±å³ç”¨ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬æŒ‡å®šä¸¤ä¸ªæ ‡ç­¾ï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªç±»åˆ«ã€‚å®ƒæ˜¯ä¸€ä¸ªé‡Šä¹‰è¿˜æ˜¯ä¸æ˜¯ã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—åˆ†ç±»æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚æ¥ä¸‹æ¥æ˜¯ä¸‹ä¸€ä¸ªé˜¶æ®µï¼Œè¿™å¯èƒ½æ˜¯æ›´å¤æ‚çš„éƒ¨åˆ†ã€‚
- en: so we need to instantiate a trainerï¼Œ so a trainer more or less at minimum needs
    a few thingsã€‚ it needs a modelã€‚So this is the model that we're going to trainï¼Œ
    it needs training argumentsã€‚ these are the things that define how the training
    will operateã€‚And it needs one of a few data setsã€‚ so it needs either a training
    data set or a evaluation data set or a test set and you can have one and none
    of the othersã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦å®ä¾‹åŒ–ä¸€ä¸ªè®­ç»ƒå™¨ï¼Œä¸€ä¸ªè®­ç»ƒå™¨è‡³å°‘éœ€è¦ä¸€äº›ä¸œè¥¿ã€‚å®ƒéœ€è¦ä¸€ä¸ªæ¨¡å‹ã€‚è¿™æ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œå®ƒéœ€è¦è®­ç»ƒå‚æ•°ã€‚è¿™äº›æ˜¯å®šä¹‰è®­ç»ƒå¦‚ä½•è¿è¡Œçš„å†…å®¹ã€‚å®ƒéœ€è¦ä¸€äº›æ•°æ®é›†ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†æˆ–è¯„ä¼°æ•°æ®é›†æˆ–æµ‹è¯•é›†ï¼Œä½ å¯ä»¥æœ‰ä¸€ä¸ªï¼Œè€Œæ²¡æœ‰å…¶ä»–çš„ã€‚
- en: but it needs at least one thing basically needs at least one data set generally
    that you want to train on you can instantiate it without itã€‚ but generally you
    want to specify oneã€‚So here we're passing the tokenized data and that's important
    you don't want to pass the raw data to your trainer because it will feed it to
    the model and then the model will goã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒè‡³å°‘éœ€è¦ä¸€ä»¶äº‹ï¼ŒåŸºæœ¬ä¸Šè‡³å°‘éœ€è¦ä¸€ä¸ªä½ æƒ³è¦è®­ç»ƒçš„æ•°æ®é›†ã€‚ä½ å¯ä»¥åœ¨æ²¡æœ‰å®ƒçš„æƒ…å†µä¸‹å®ä¾‹åŒ–ï¼Œä½†ä¸€èˆ¬æ¥è¯´ä½ æƒ³è¦æŒ‡å®šä¸€ä¸ªã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬ä¼ é€’äº†åˆ†è¯åçš„æ•°æ®ï¼Œè¿™å¾ˆé‡è¦ï¼Œå› ä¸ºä½ ä¸æƒ³å°†åŸå§‹æ•°æ®ä¼ é€’ç»™ä½ çš„è®­ç»ƒå™¨ï¼Œå› ä¸ºå®ƒä¼šå°†å…¶å–‚ç»™æ¨¡å‹ï¼Œç„¶åæ¨¡å‹å°±ä¼šå¼€å§‹å¤„ç†ã€‚
- en: I don't know what you're doing what I should do with these stringsã€‚And the other
    two things that are kind of interesting is passing a data coatorã€‚And a tokenizer
    so providing these two arguments will do dynamic paddingã€‚ which will be faster
    for trainingã€‚And the way it works is you say give me a colalleatorã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“ä½ åœ¨åšä»€ä¹ˆï¼Œæˆ‘è¯¥å¦‚ä½•å¤„ç†è¿™äº›å­—ç¬¦ä¸²ã€‚è¿˜æœ‰å¦å¤–ä¸¤ä»¶æœ‰è¶£çš„äº‹æ˜¯ä¼ é€’æ•°æ®åè°ƒå™¨å’Œåˆ†è¯å™¨ï¼Œå› æ­¤æä¾›è¿™ä¸¤ä¸ªå‚æ•°å°†å®ç°åŠ¨æ€å¡«å……ï¼Œè¿™æ ·è®­ç»ƒä¼šæ›´å¿«ã€‚å®ƒçš„å·¥ä½œæ–¹å¼æ˜¯ä½ è¦æ±‚ç»™æˆ‘ä¸€ä¸ªåè°ƒå™¨ã€‚
- en: and there are different types of data colalatorï¼Œ but the one we're using is
    the most common oneã€‚And it also needs tokenizerï¼Œ so it's basically the way the
    data cl works is it kind of combines the tokenizer with the batching to work out
    how to arrange the inputsã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®åè°ƒå™¨æœ‰ä¸åŒç±»å‹ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æœ€å¸¸è§çš„ä¸€ç§ã€‚åŒæ—¶å®ƒä¹Ÿéœ€è¦åˆ†è¯å™¨ï¼Œå› æ­¤æ•°æ®åè°ƒå™¨çš„å·¥ä½œæ–¹å¼æ˜¯å°†åˆ†è¯å™¨ä¸æ‰¹å¤„ç†ç»“åˆèµ·æ¥ï¼Œä»¥ç¡®å®šå¦‚ä½•å®‰æ’è¾“å…¥ã€‚
- en: And so by providing these two argumentsï¼Œ we will then get some speed up in trainingã€‚å—¯mã€‚And
    so this will instantiate the trainerã€‚Andã€‚Let's see if it worksã€‚It's crossed okay
    good so one thing that I usually do before I launch training is I just do like
    a sanity check that I can run the evaluate function because a lot of the time
    what happens is when you run train and or train you will train for some number
    of steps and then you will evaluate you know maybe at the end of an epoOC and
    that's kind of like one of the default strategiesã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œé€šè¿‡æä¾›è¿™ä¸¤ä¸ªå‚æ•°ï¼Œæˆ‘ä»¬å°†ä¼šåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚å—¯ã€‚è¿™æ ·å°±ä¼šå®ä¾‹åŒ–è®­ç»ƒå™¨ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦æœ‰æ•ˆã€‚å¥½çš„ï¼Œä¸€åˆ‡æ­£å¸¸ï¼Œæ‰€ä»¥åœ¨æˆ‘å¯åŠ¨è®­ç»ƒä¹‹å‰ï¼Œé€šå¸¸æˆ‘ä¼šè¿›è¡Œä¸€ä¸ªç†æ™ºæ£€æŸ¥ï¼Œç¡®ä¿æˆ‘å¯ä»¥è¿è¡Œè¯„ä¼°å‡½æ•°ï¼Œå› ä¸ºå¾ˆå¤šæ—¶å€™å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œå½“ä½ è¿è¡Œè®­ç»ƒæ—¶ï¼Œä½ ä¼šè®­ç»ƒè‹¥å¹²æ­¥ï¼Œç„¶ååœ¨ä¸€ä¸ªå‘¨æœŸç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯é»˜è®¤ç­–ç•¥ä¹‹ä¸€ã€‚
- en: But then you know if your evaluationï¼Œ maybe your metrics are not implemented
    correctly or whateverã€‚ if your evaluation failsï¼Œ then you're going to fail after
    waiting for a whole training of one epoch and that's like really annoying so a
    sort of sanity check that I do is just to make sure that I can run evaluateã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ çš„è¯„ä¼°ï¼Œå¯èƒ½ä½ çš„æŒ‡æ ‡å®ç°å¾—ä¸æ­£ç¡®æˆ–è€…å…¶ä»–åŸå› ã€‚å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œé‚£ä¹ˆä½ åœ¨ç­‰å¾…æ•´ä¸ªä¸€ä¸ªå‘¨æœŸçš„è®­ç»ƒåä¹Ÿä¼šå¤±è´¥ï¼Œè¿™çœŸçš„å¾ˆè®©äººçƒ¦æ¼ã€‚å› æ­¤ï¼Œæˆ‘åšçš„ä¸€ä¸ªç†æ™ºæ£€æŸ¥å°±æ˜¯ç¡®ä¿æˆ‘å¯ä»¥è¿è¡Œè¯„ä¼°ã€‚
- en: And if that worksï¼Œ then I'm relatively confident that the training run will
    work and what you can see that this evaluation has done is it's given us some
    information about the number of examples in the validation set and it's provided
    us with a value of the loss and then some kind of runtime metricsã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™æ ·æœ‰æ•ˆï¼Œæˆ‘ç›¸å¯¹æœ‰ä¿¡å¿ƒè®­ç»ƒè¿‡ç¨‹ä¼šæˆåŠŸï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™ä¸ªè¯„ä¼°æä¾›äº†å…³äºéªŒè¯é›†ä¸­ç¤ºä¾‹æ•°é‡çš„ä¿¡æ¯ï¼Œå¹¶ä¸”ç»™å‡ºäº†æŸå¤±å€¼å’Œä¸€äº›è¿è¡Œæ—¶æŒ‡æ ‡ã€‚
- en: sort of how performance is this processing batchesã€‚And so obviously this is
    a random model at the momentã€‚ we've just initialized Bert as a backbone and just
    stacked on like a linear layer with random weights so this is kind of a garbage
    loss so the goal will be after training for this to be going downã€‚Coolï¼Œ so I'm
    gonna now run the trainingã€‚ And this takes a few minutesã€‚ So while it's runningã€‚
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¤„ç†æ‰¹æ¬¡çš„æ€§èƒ½å¦‚ä½•ï¼Ÿæ˜¾ç„¶ï¼Œç›®å‰è¿™æ˜¯ä¸€ä¸ªéšæœºæ¨¡å‹ã€‚æˆ‘ä»¬åªæ˜¯åˆå§‹åŒ–äº†Bertä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œç„¶åå †å äº†ä¸€ä¸ªå¸¦æœ‰éšæœºæƒé‡çš„çº¿æ€§å±‚ï¼Œæ‰€ä»¥è¿™ç®—æ˜¯ä¸€ä¸ªæ— ç”¨çš„æŸå¤±ï¼Œç›®æ ‡æ˜¯åœ¨è®­ç»ƒåä½¿å…¶ä¸‹é™ã€‚é…·ï¼Œæ‰€ä»¥æˆ‘ç°åœ¨è¦è¿è¡Œè®­ç»ƒã€‚è¿™éœ€è¦å‡ åˆ†é’Ÿã€‚å› æ­¤åœ¨å®ƒè¿è¡Œæ—¶ã€‚
- en: we'll then watch the accelerate videoï¼Œ which will show us how toã€‚Wellï¼Œ maybe
    notã€‚ maybe I'll just run this and then see if there's any questionsã€‚Okayã€‚Soã€‚DK
    crazy diviv asksã€‚We're using Bt base uncased and the article mentions word piece
    for Btã€‚ but we Pip installed transformer sentence pieceï¼Œ are we're using it hereã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä¼šè§‚çœ‹åŠ é€Ÿè§†é¢‘ï¼Œå®ƒä¼šå‘æˆ‘ä»¬å±•ç¤ºå¦‚ä½•ã€‚å—¯ï¼Œä¹Ÿè®¸ä¸æ˜¯ã€‚ä¹Ÿè®¸æˆ‘ä¼šå…ˆè¿è¡Œè¿™ä¸ªï¼Œç„¶åçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜ã€‚å¥½çš„ã€‚DK crazy divivé—®ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨Bt
    base uncasedï¼Œæ–‡ç« æåˆ°Btä½¿ç”¨çš„æ˜¯WordPieceï¼Œä½†æˆ‘ä»¬é€šè¿‡pipå®‰è£…äº†transformersçš„sentence pieceï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å®ƒå—ï¼Ÿ
- en: If I were to reverse engineerï¼Œ how would I programmatically get from checkpoint
    and figure out which tokenizeã€‚ that's a great questionã€‚Soã€‚The question we have
    isã€‚ why are we doing Pip install transformformers sentence pieceï¼ŸSo this is an
    optional dependencyã€‚ which is a different tokenization algorithm that is used
    by models like XLM Robbaaã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘æƒ³åå‘å·¥ç¨‹ï¼Œæˆ‘è¯¥å¦‚ä½•ä»¥ç¼–ç¨‹æ–¹å¼ä»æ£€æŸ¥ç‚¹è·å–å¹¶å¼„æ¸…æ¥šä½¿ç”¨å“ªä¸ªåˆ†è¯å™¨ã€‚è¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚æ‰€ä»¥ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è¿›è¡ŒPipå®‰è£…transformers
    sentence pieceï¼Ÿè¿™æ˜¯ä¸€ä¸ªå¯é€‰ä¾èµ–é¡¹ï¼Œå®ƒæ˜¯XLM Robbaaç­‰æ¨¡å‹ä½¿ç”¨çš„ä¸åŒåˆ†è¯ç®—æ³•ã€‚
- en: And it's listed as an optional dependency because it's a bit heavyã€‚ so if you
    don't want to use any models that don't require sentence piece for the tokenizationã€‚
    you can just leave it and it will workã€‚So for exampleã€‚ if we just did PIip install
    with transformersï¼Œ we will get word peace for freeã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¢«åˆ—ä¸ºå¯é€‰ä¾èµ–é¡¹ï¼Œå› ä¸ºå®ƒæœ‰ç‚¹é‡ã€‚å› æ­¤ï¼Œå¦‚æœä½ ä¸æƒ³ä½¿ç”¨ä»»ä½•ä¸éœ€è¦å¥å­åˆ†ç‰‡è¿›è¡Œåˆ†è¯çš„æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä¸å®‰è£…ï¼Œå®ƒä»ç„¶ä¼šæ­£å¸¸å·¥ä½œã€‚æ‰€ä»¥ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä»…é€šè¿‡transformersè¿›è¡ŒPipå®‰è£…ï¼Œæˆ‘ä»¬å°†å…è´¹è·å¾—wordpieceã€‚
- en: it will come part of itï¼Œ but if we want to use these other models we have to
    define explicitly the sentence piece dependencyã€‚So that's like maybe clarifies
    the first part of the questionã€‚We're always using wordpiece with Bt and we only
    install sentence piece for the models when we need itã€‚And the reason we have it
    in the course is because there are some cases later in the future parts where
    we're going to be using the sentence piece tokenizer and so it's just useful if
    we just always have access to itã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šä½œä¸ºä¸€éƒ¨åˆ†ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œå°±å¿…é¡»æ˜ç¡®åœ°å®šä¹‰å¥å­åˆ†ç‰‡ä¾èµ–é¡¹ã€‚æ‰€ä»¥è¿™å¯èƒ½æ¾„æ¸…äº†é—®é¢˜çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬å§‹ç»ˆä¸Btä¸€èµ·ä½¿ç”¨wordpieceï¼Œä»…åœ¨éœ€è¦æ—¶æ‰ä¸ºæ¨¡å‹å®‰è£…å¥å­åˆ†ç‰‡ã€‚æˆ‘ä»¬åœ¨è¯¾ç¨‹ä¸­åŒ…å«å®ƒçš„åŸå› æ˜¯ï¼Œå› ä¸ºåœ¨æœªæ¥çš„æŸäº›éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨å¥å­åˆ†ç‰‡åˆ†è¯å™¨ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å§‹ç»ˆèƒ½å¤Ÿè®¿é—®å®ƒä¼šå¾ˆæœ‰ç”¨ã€‚
- en: Okayï¼Œ soã€‚If I were to reverse engineerï¼Œ how would I programmatically get from
    checkpoint to figuring out which tokenizer to useï¼Ÿ
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œå¦‚æœæˆ‘æƒ³åå‘å·¥ç¨‹ï¼Œæˆ‘è¯¥å¦‚ä½•ä»¥ç¼–ç¨‹æ–¹å¼ä»æ£€æŸ¥ç‚¹å¼„æ¸…æ¥šä½¿ç”¨å“ªä¸ªåˆ†è¯å™¨ï¼Ÿ
- en: Okayï¼Œ so maybe we canã€‚I'll answer this in two waysã€‚ so the simplest way is the
    following in transformersã€‚We have an auto modelã€‚AClas for different types of modelsï¼Œ
    so this auto model will just do the the encoding and we also have auto tokenizerã€‚Andã€‚Ohï¼Œ
    oopsã€‚Inputã€‚And these classesï¼Œ they do this I pairingã€‚ So if I take auto tokenizer
    fromã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥ã€‚æˆ‘å°†ç”¨ä¸¤ç§æ–¹å¼æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æ‰€ä»¥æœ€ç®€å•çš„æ–¹æ³•å¦‚ä¸‹ï¼Œåœ¨transformersä¸­ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªè‡ªåŠ¨æ¨¡å‹ã€‚é€‚ç”¨äºä¸åŒç±»å‹æ¨¡å‹çš„AClasï¼Œå› æ­¤è¿™ä¸ªè‡ªåŠ¨æ¨¡å‹å°†è¿›è¡Œç¼–ç ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰è‡ªåŠ¨åˆ†è¯å™¨ã€‚å“¦ï¼Œå“å‘€ã€‚è¾“å…¥ã€‚è¿™äº›ç±»ä¼šè¿›è¡Œé…å¯¹ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»è‡ªåŠ¨åˆ†è¯å™¨è·å–ã€‚
- en: Pre trainedã€‚ And I put B baseã€‚Uncaseedã€‚It will automatically assign the tokenizer
    that was associated with that checkpoint and then load it into my tokenization
    of my tokenizer objectã€‚And similarlyï¼Œ if I do auto modelã€‚From pre trainedã€‚ And
    I do the sameã€‚å—¯ã€‚Checkpointã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒã€‚æˆ‘æ”¾å…¥BåŸºç¡€ã€‚æ— å¤§å°å†™ã€‚å®ƒä¼šè‡ªåŠ¨åˆ†é…ä¸è¯¥æ£€æŸ¥ç‚¹å…³è”çš„åˆ†è¯å™¨ï¼Œç„¶ååŠ è½½åˆ°æˆ‘çš„åˆ†è¯å™¨å¯¹è±¡çš„åˆ†è¯ä¸­ã€‚åŒæ ·ï¼Œå¦‚æœæˆ‘åšè‡ªåŠ¨æ¨¡å‹ã€‚ä»é¢„è®­ç»ƒã€‚ç„¶åæˆ‘åšåŒæ ·çš„ã€‚å—¯ã€‚æ£€æŸ¥ç‚¹ã€‚
- en: it will automatically figure out that for that checkpoint I need these set of
    weightsã€‚So the most important thing that you need to know is just to make sure
    that you use the same checkpoint for both the tokenizer and the model when you
    do the from pretrained and so what I often do in my code is I'll have an explicit
    variableã€‚
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šè‡ªåŠ¨ç¡®å®šå¯¹äºè¯¥æ£€æŸ¥ç‚¹æˆ‘éœ€è¦è¿™ç»„æƒé‡ã€‚æ‰€ä»¥ä½ éœ€è¦çŸ¥é“çš„æœ€é‡è¦çš„äº‹æƒ…å°±æ˜¯ç¡®ä¿åœ¨ä½¿ç”¨from pretrainedæ—¶ï¼Œåˆ†è¯å™¨å’Œæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ£€æŸ¥ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘åœ¨ä»£ç ä¸­ç»å¸¸ä¼šæœ‰ä¸€ä¸ªæ˜¾å¼å˜é‡ã€‚
- en: which is Bt base uncaseopsã€‚Like thisï¼Œ and then I'll just load this variable
    into all of my pre trainedã€‚Calls so that then I know that they're matchedã€‚ğŸ¤§å•Šã€‚Okayï¼Œ
    so that's sort ofï¼Œ let's sayã€‚ standard way of how we link these things togetherã€‚å—¯ã€‚ğŸ˜Šï¼ŒNow
    you're askingã€‚ how could we programmatically get from checkpoint to tokenizerï¼ŸLet's
    have a look at what's insideã€‚
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯BtåŸºç¡€æ— å¤§å°å†™çš„å†…å®¹ã€‚åƒè¿™æ ·ï¼Œç„¶åæˆ‘ä¼šå°†è¿™ä¸ªå˜é‡åŠ è½½åˆ°æˆ‘çš„æ‰€æœ‰é¢„è®­ç»ƒè°ƒç”¨ä¸­ï¼Œè¿™æ ·æˆ‘å°±çŸ¥é“å®ƒä»¬æ˜¯åŒ¹é…çš„ã€‚ğŸ¤§å•Šã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å°†è¿™äº›ä¸œè¥¿è¿æ¥åœ¨ä¸€èµ·çš„æ ‡å‡†æ–¹æ³•ã€‚å—¯ã€‚ğŸ˜Šï¼Œç°åœ¨ä½ åœ¨é—®ã€‚æˆ‘ä»¬å¦‚ä½•ä»¥ç¼–ç¨‹æ–¹å¼ä»æ£€æŸ¥ç‚¹è·å–åˆ°åˆ†è¯å™¨ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹é‡Œé¢æœ‰ä»€ä¹ˆã€‚
- en: One of these model filesã€‚è¿™è¿™è¿™ã€‚Let's just check that my training is workingã€‚ğŸ˜”ï¼ŒYeahï¼ŒGoodã€‚Okayã€‚
    so in the model we have a configã€‚Andã€‚This configã€‚Let's seeã€‚Okayã€‚ so the config
    tells us the name of the checkpointã€‚å—¯ã€‚Soã€‚If you wanted to programmatically make
    sure thatï¼Œ I meanã€‚
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å‹æ–‡ä»¶ä¹‹ä¸€ã€‚è¿™è¿™è¿™ã€‚è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æˆ‘çš„è®­ç»ƒæ˜¯å¦æ­£å¸¸ã€‚ğŸ˜”ï¼Œå¥½çš„ï¼Œå¾ˆå¥½ã€‚å¥½çš„ã€‚åœ¨æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé…ç½®ã€‚è¿™ä¸ªé…ç½®ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚å¥½çš„ã€‚è¿™ä¸ªé…ç½®å‘Šè¯‰æˆ‘ä»¬æ£€æŸ¥ç‚¹çš„åç§°ã€‚å—¯ã€‚æ‰€ä»¥ã€‚å¦‚æœä½ æƒ³ä»¥ç¼–ç¨‹æ–¹å¼ç¡®ä¿ï¼Œæˆ‘çš„æ„æ€æ˜¯ã€‚
- en: let's suppose you start with like the configã€‚I guess what you could do is you
    could then use this attribute as the thing that you feed to the tokenizerã€‚ Soï¼Œ
    for exampleï¼Œ I could do tokenizer equals auto tokenizer from pretrainã€‚And then
    I would take my modelï¼Œ I would take my configï¼Œ and then I would access what is
    itï¼Ÿ
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ ä»é…ç½®å¼€å§‹ã€‚æˆ‘æƒ³ä½ å¯ä»¥æŠŠè¿™ä¸ªå±æ€§ç”¨ä½œä¼ é€’ç»™åˆ†è¯å™¨çš„å†…å®¹ã€‚å› æ­¤ï¼Œä¾‹å¦‚ï¼Œæˆ‘å¯ä»¥è¿™æ ·åšï¼štokenizer ç­‰äº auto tokenizer from
    pretrainã€‚ç„¶åæˆ‘ä¼šæ‹¿æˆ‘çš„æ¨¡å‹ï¼Œæˆ‘ä¼šæ‹¿æˆ‘çš„é…ç½®ï¼Œç„¶åæˆ‘ä¼šè®¿é—®å®ƒæ˜¯ä»€ä¹ˆï¼Ÿ
- en: Is it nameï¼Ÿå“¦ã€‚Can I exit this as an attributeã€‚Yeahï¼Œ so I could do thisã€‚So I could
    load my tokenizer this way and then this would let's say guarantee that the checkpoints
    matchã€‚ but to be honest this is like a bit complicatedï¼Œ so I would more often
    than not just recommend defining a variable with your checkpoint and then just
    feeding that variable into your tokenizer modelã€‚So I hope that answers that questionï¼Œ
    Dï¼Œ crazyativeã€‚ğŸ˜”ï¼ŒBut feel free to ask if it's not clearã€‚Okayã€‚
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯åå­—å—ï¼Ÿå“¦ã€‚æˆ‘å¯ä»¥æŠŠè¿™ä¸ªå½“ä½œä¸€ä¸ªå±æ€§å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘å¯ä»¥è¿™æ ·åšã€‚æ‰€ä»¥æˆ‘å¯ä»¥è¿™æ ·åŠ è½½æˆ‘çš„åˆ†è¯å™¨ï¼Œè¿™æ ·å¯ä»¥ä¿è¯æ£€æŸ¥ç‚¹åŒ¹é…ã€‚ä½†è€å®è¯´ï¼Œè¿™æœ‰ç‚¹å¤æ‚ï¼Œæ‰€ä»¥æˆ‘æ›´å€¾å‘äºå®šä¹‰ä¸€ä¸ªå˜é‡æ¥ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œç„¶åæŠŠè¿™ä¸ªå˜é‡ä¼ é€’ç»™åˆ†è¯å™¨æ¨¡å‹ã€‚æˆ‘å¸Œæœ›è¿™èƒ½å›ç­”ä½ çš„é—®é¢˜ï¼ŒDï¼Œcrazyativeã€‚ğŸ˜”ï¼Œä½†å¦‚æœä¸æ¸…æ¥šï¼Œè¯·éšæ—¶é—®ã€‚
- en: so our model is pretty close to being done with the training and you can see
    that by default in the trainerã€‚ every 500 stepsï¼Œ the model will run an evaluation
    on the validation set and log the lossã€‚By defaultã€‚ So this is just the training
    lossã€‚But the thing that Sylvan explained in the video is that what we really want
    to do is compute metrics like accuracy or F1 score or whateverã€‚And so the way
    you do this in practice is you define a function called compute metricsã€‚
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¨¡å‹æ¥è¿‘å®Œæˆè®­ç»ƒï¼Œä½ å¯ä»¥çœ‹åˆ°åœ¨è®­ç»ƒå™¨ä¸­é»˜è®¤æ¯ 500 æ­¥ï¼Œæ¨¡å‹å°†åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°å¹¶è®°å½•æŸå¤±ã€‚é»˜è®¤æƒ…å†µä¸‹ã€‚è¿™åªæ˜¯è®­ç»ƒæŸå¤±ã€‚ä½† Sylvan åœ¨è§†é¢‘ä¸­è§£é‡Šçš„äº‹æƒ…æ˜¯æˆ‘ä»¬çœŸæ­£æƒ³åšçš„æ˜¯è®¡ç®—åƒå‡†ç¡®ç‡æˆ–
    F1 åˆ†æ•°è¿™æ ·çš„æŒ‡æ ‡ã€‚å› æ­¤ï¼Œå®é™…ä¸Šä½ è¿™æ ·åšçš„æ–¹å¼æ˜¯å®šä¹‰ä¸€ä¸ªåä¸º compute metrics çš„å‡½æ•°ã€‚
- en: And this metrics function has to basically return a dictionary where you have
    a key corresponding to the name of the metricã€‚And then the values that you would
    compute from your predictionsã€‚And so in this example hereã€‚ Sylvan is leveraging
    the Datas libraryï¼Œ which has its own metrics as wellã€‚And then once you have loaded
    the metric for this task in glueã€‚
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŒ‡æ ‡å‡½æ•°åŸºæœ¬ä¸Šå¿…é¡»è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸æŒ‡æ ‡åç§°å¯¹åº”çš„é”®ï¼Œä»¥åŠä½ ä»é¢„æµ‹ä¸­è®¡ç®—å‡ºçš„å€¼ã€‚å› æ­¤åœ¨è¿™é‡Œçš„è¿™ä¸ªä¾‹å­ä¸­ï¼ŒSylvan åˆ©ç”¨ Datas åº“ï¼Œè¯¥åº“ä¹Ÿæœ‰è‡ªå·±çš„æŒ‡æ ‡ã€‚ä¸€æ—¦ä½ åŠ è½½äº†è¯¥ä»»åŠ¡åœ¨
    glue ä¸­çš„æŒ‡æ ‡ã€‚
- en: you just have to extract the predictions and the ground truth labelsã€‚And then
    you can just do metricã€‚compute and this will automatically create this dictionary
    for you which you can then feed into the trainer as follows so you can do compute
    metrics like this and then what this will do is that every 500 steps by default
    it will then compute the metrics in addition to the training loss and this is
    quite handy because this is how you can track the performance of the model on
    the validation set as you train and you can make decisions about okayã€‚
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªéœ€è¦æå–é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ã€‚ç„¶åä½ åªéœ€è°ƒç”¨ metric.computeï¼Œè¿™å°†è‡ªåŠ¨ä¸ºä½ åˆ›å»ºè¿™ä¸ªå­—å…¸ï¼Œç„¶åä½ å¯ä»¥å°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå› æ­¤ä½ å¯ä»¥åƒè¿™æ ·è®¡ç®—æŒ‡æ ‡ï¼Œè¿™æ ·é»˜è®¤æƒ…å†µä¸‹æ¯
    500 æ­¥å°±ä¼šè®¡ç®—ä¸€æ¬¡æŒ‡æ ‡ï¼Œé™¤äº†è®­ç»ƒæŸå¤±ä¹‹å¤–ï¼Œè¿™éå¸¸æ–¹ä¾¿ï¼Œå› ä¸ºè¿™å°±æ˜¯ä½ åœ¨è®­ç»ƒæ—¶è·Ÿè¸ªæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¡¨ç°çš„æ–¹æ³•ï¼Œä»è€Œåšå‡ºå†³ç­–ã€‚
- en: is it getting better orã€‚ã‚ã“ã®ã€‚Okayï¼Œ so let's just have a look at thisã€‚ So Okayï¼Œ
    so goodï¼Œ the model isã€‚Trainedã€‚å—¯ã€‚And you can see that the training loss here is
    around 0ã€‚35ã€‚ We saw when we did the evaluation lossï¼Œ it was roughly doubleï¼Œ and
    that was with the random weightsã€‚ So nowï¼Œ if I run evaluate againã€‚I should see
    hopefully the loss has gone downã€‚Okayã€‚
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯åœ¨å˜å¥½è¿˜æ˜¯ï¼Ÿå•Šï¼Œè¿™ä¸ªã€‚å¥½çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªã€‚æ‰€ä»¥ï¼Œå¥½å§ï¼Œæ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ã€‚å—¯ã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„è®­ç»ƒæŸå¤±å¤§çº¦æ˜¯ 0.35ã€‚å½“æˆ‘ä»¬è¿›è¡Œè¯„ä¼°æ—¶ï¼ŒæŸå¤±å¤§çº¦æ˜¯åŒå€ï¼Œå¹¶ä¸”é‚£æ˜¯éšæœºæƒé‡ã€‚æ‰€ä»¥ç°åœ¨ï¼Œå¦‚æœæˆ‘å†è¿è¡Œè¯„ä¼°ï¼Œå¸Œæœ›æŸå¤±èƒ½é™ä½ã€‚å¥½çš„ã€‚
- en: it's gone it's gone upï¼Œ that's very interestingã€‚ğŸ˜Šï¼ŒI'm not sure where that's
    the caseã€‚So I'm going to just say that's the demo gods being meant to meã€‚Generallyã€‚
    this should have gone downï¼Œ unusualï¼Œ might just be a random fluctuation in what
    we've doneã€‚Okayã€‚ and so just to sort of dig down a little bit into how we build
    this metrics functionã€‚
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå‡é«˜äº†ï¼Œè¿™å¾ˆæœ‰è¶£ã€‚ğŸ˜Šï¼Œæˆ‘ä¸ç¡®å®šä¸ºä»€ä¹ˆä¼šè¿™æ ·ã€‚æ‰€ä»¥æˆ‘åªèƒ½è¯´è¿™æ˜¯æ¼”ç¤ºä¹‹ç¥åœ¨æ‰å¼„æˆ‘ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™åº”è¯¥æ˜¯é™ä½çš„ï¼Œåå¸¸ï¼Œå¯èƒ½åªæ˜¯æˆ‘ä»¬æ‰€åšçš„éšæœºæ³¢åŠ¨ã€‚å¥½çš„ã€‚ä¸ºäº†æ·±å…¥æ¢è®¨æˆ‘ä»¬æ˜¯å¦‚ä½•æ„å»ºè¿™ä¸ªæŒ‡æ ‡å‡½æ•°çš„ã€‚
- en: The trainer has a predict method and you can just feed into that predict method
    data set from data setsã€‚And this predicts method will return an object called
    predictionsã€‚And this's how lookã€‚Soã€‚ this predictionsã€‚Is a prediction output objectï¼Œ
    it's just like a data class which has attributes and the attributes it has are
    predictionsã€‚So if we look at predictionsã€‚Oopsã€‚That's not going to workã€‚
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå™¨æœ‰ä¸€ä¸ªé¢„æµ‹æ–¹æ³•ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®é›†è¾“å…¥åˆ°è¯¥é¢„æµ‹æ–¹æ³•ä¸­ã€‚è¿™ä¸ªé¢„æµ‹æ–¹æ³•å°†è¿”å›ä¸€ä¸ªåä¸ºpredictionsçš„å¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚è¿™ä¸ªpredictionsæ˜¯ä¸€ä¸ªé¢„æµ‹è¾“å‡ºå¯¹è±¡ï¼Œå°±åƒä¸€ä¸ªå…·æœ‰å±æ€§çš„æ•°æ®ç±»ï¼Œå…·æœ‰çš„å±æ€§æ˜¯é¢„æµ‹ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹é¢„æµ‹ã€‚å“å‘€ï¼Œè¿™ä¸è¡Œã€‚
- en: It's just an array of basically all of the logicits from the model and we also
    have the ground truthã€‚Labels listed like as IDsï¼Œ so this is you know paraphraseï¼Œ
    not paraphraseï¼Œ not paraphraseã€‚ on and so forthã€‚So the main work you often have
    to do when you build your compute metrics function is you need to convert your
    logicits into let's sayã€‚ integers or label IDs because that's the thing that you
    want to compare againstã€‚
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ˜¯æ¨¡å‹ä¸­æ‰€æœ‰é€»è¾‘å•ä½çš„æ•°ç»„ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰çœŸå®æ ‡ç­¾ã€‚æ ‡ç­¾ä»¥IDçš„å½¢å¼åˆ—å‡ºï¼Œå› æ­¤è¿™æ˜¯ä½ çŸ¥é“çš„åŒä¹‰å¥ã€éåŒä¹‰å¥ã€éåŒä¹‰å¥ï¼Œç­‰ç­‰ã€‚å› æ­¤ï¼Œæ„å»ºè®¡ç®—æŒ‡æ ‡å‡½æ•°æ—¶ï¼Œæ‚¨é€šå¸¸éœ€è¦åšçš„ä¸»è¦å·¥ä½œæ˜¯å°†é€»è¾‘å•ä½è½¬æ¢ä¸ºæ•´æ•°æˆ–æ ‡ç­¾IDï¼Œå› ä¸ºè¿™æ˜¯æ‚¨å¸Œæœ›è¿›è¡Œæ¯”è¾ƒçš„å†…å®¹ã€‚
- en: And so one way to do that is to just compute an AGmaxã€‚ which will basically
    say for every prediction in terms of logicsã€‚ find the index with the largest or
    the highest logicã€‚And so then once we apply thatã€‚ our pres now is a tensor of
    integersã€‚And then I can feed those predictions into my my metric from data setsã€‚
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæœ‰ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—AGmaxï¼Œè¿™åŸºæœ¬ä¸Šä¼šè¯´ï¼Œå¯¹äºæ¯ä¸ªé¢„æµ‹çš„é€»è¾‘ï¼Œæ‰¾åˆ°æœ€å¤§çš„æˆ–æœ€é«˜çš„é€»è¾‘ç´¢å¼•ã€‚å› æ­¤ï¼Œä¸€æ—¦æˆ‘ä»¬åº”ç”¨è¿™ä¸ªï¼Œæˆ‘ä»¬çš„é¢„æµ‹ç°åœ¨æ˜¯ä¸€ä¸ªæ•´æ•°å¼ é‡ã€‚ç„¶åæˆ‘å¯ä»¥å°†è¿™äº›é¢„æµ‹è¾“å…¥åˆ°æˆ‘çš„æ•°æ®é›†æŒ‡æ ‡ä¸­ã€‚
- en: and then I can run compute on that and it shouldã€‚Hopefully give us these valuesã€‚So
    any questions about computing metricsï¼ŸOkayã€‚ğŸ˜Šï¼ŒSo I'm not going to run this last
    cellã€‚ it's just the same thing we did beforeï¼Œ it will run the training but instead
    of showing the training loss it will show the validation metrics that we useã€‚
    but I encourage you to play with this yourselfï¼Œ maybe with a new data set just
    to understand how this is really workingã€‚
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿è¡Œè®¡ç®—ï¼Œå®ƒåº”è¯¥å¸Œæœ›ç»™æˆ‘ä»¬è¿™äº›å€¼ã€‚é‚£ä¹ˆå…³äºè®¡ç®—æŒ‡æ ‡æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿå¥½çš„ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘ä¸æ‰“ç®—è¿è¡Œæœ€åä¸€ä¸ªå•å…ƒï¼Œå®ƒåªæ˜¯æˆ‘ä»¬ä¹‹å‰åšçš„åŒæ ·çš„äº‹æƒ…ï¼Œå®ƒå°†è¿è¡Œè®­ç»ƒï¼Œä½†ä¸æ˜¯æ˜¾ç¤ºè®­ç»ƒæŸå¤±ï¼Œè€Œæ˜¯æ˜¾ç¤ºæˆ‘ä»¬ä½¿ç”¨çš„éªŒè¯æŒ‡æ ‡ã€‚ä¸è¿‡ï¼Œæˆ‘é¼“åŠ±æ‚¨è‡ªå·±å°è¯•ä¸€ä¸‹ï¼Œå¯èƒ½ä½¿ç”¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œä»¥ä¾¿ç†è§£å®ƒæ˜¯å¦‚ä½•çœŸæ­£å·¥ä½œçš„ã€‚
- en: Okay so that's the trainer APIï¼Œ this is what I use let's say 90% of the time
    it's just very convenientã€‚ it works and it means I don't have to think too much
    about writing my own lowlevel training script or my training code which can often
    be errorprone right and so a good reason to use like highle APIs like the trainer
    or you know fast AI or Ptorch lightning or whatever is that they abstract away
    a lot of the boilerplate code which if you do it yourself will probably have mistakesã€‚
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™å°±æ˜¯è®­ç»ƒå™¨APIï¼Œè¿™å¤§çº¦æ˜¯æˆ‘90%çš„æ—¶é—´æ‰€ä½¿ç”¨çš„ï¼Œå®ƒéå¸¸æ–¹ä¾¿ï¼Œèƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼Œè¿™æ„å‘³ç€æˆ‘ä¸å¿…è¿‡å¤šè€ƒè™‘ç¼–å†™è‡ªå·±çš„ä½çº§è®­ç»ƒè„šæœ¬æˆ–è®­ç»ƒä»£ç ï¼Œè¿™é€šå¸¸å®¹æ˜“å‡ºé”™ã€‚å› æ­¤ï¼Œä½¿ç”¨åƒè®­ç»ƒå™¨ã€fast
    AIæˆ–Pytorch lightningè¿™æ ·çš„é«˜çº§APIçš„ä¸€ä¸ªå¥½ç†ç”±æ˜¯ï¼Œå®ƒä»¬æŠ½è±¡æ‰äº†å¾ˆå¤šæ¨¡æ¿ä»£ç ï¼Œå¦‚æœè‡ªå·±åšå¯èƒ½ä¼šæœ‰é”™è¯¯ã€‚
- en: And this has been kind of battle tested throughï¼Œ you know thousands of usersã€‚Okayã€‚
    so to wrap up the sessionï¼Œ I want to now look at something that's like really
    exciting for meã€‚ at least is Silvan developed a library could accelerateï¼Œ which
    saysï¼Œ you knowã€‚ sometimes I really need to have control of the training loopã€‚And
    you can do this onï¼Œ you knowã€‚
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”è¿™ç»è¿‡æ•°åƒåç”¨æˆ·çš„å®é™…æ£€éªŒã€‚å¥½çš„ï¼Œç»“æŸæœ¬æ¬¡ä¼šè®®ï¼Œæˆ‘æƒ³ç°åœ¨çœ‹çœ‹ä¸€äº›å¯¹æˆ‘æ¥è¯´éå¸¸ä»¤äººå…´å¥‹çš„äº‹æƒ…ã€‚è‡³å°‘æ˜¯Silvanå¼€å‘çš„ä¸€ä¸ªåº“å¯ä»¥åŠ é€Ÿï¼Œè¯´æ˜ï¼Œæœ‰æ—¶æˆ‘çœŸçš„éœ€è¦æ§åˆ¶è®­ç»ƒå¾ªç¯ã€‚æ‚¨å¯ä»¥åœ¨è¿™äº›æ–¹é¢è¿›è¡Œæ“ä½œã€‚
- en: CPU and GPU pretty easilyã€‚ There's lots of tutorials on doing thatã€‚But there's
    been a kind of say trend in the last few years towards having access to like multi
    GPU or TU machines and these offer in principle a lot of speed up because you
    can now distribute your training so you you can distribute your batches to these
    devicesã€‚
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: CPUå’ŒGPUçš„ä½¿ç”¨ç›¸å¯¹ç®€å•ã€‚æœ‰å¾ˆå¤šæ•™ç¨‹å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ä½†åœ¨è¿‡å»å‡ å¹´ä¸­ï¼Œæœ‰ä¸€ç§è¶‹åŠ¿æ˜¯è®¿é—®å¤šGPUæˆ–TUæœºå™¨ï¼Œè¿™äº›æœºå™¨åŸåˆ™ä¸Šå¯ä»¥æä¾›å¾ˆå¤šåŠ é€Ÿï¼Œå› ä¸ºæ‚¨å¯ä»¥åˆ†é…è®­ç»ƒï¼Œä»è€Œå°†æ‰¹æ¬¡åˆ†é…åˆ°è¿™äº›è®¾å¤‡ä¸Šã€‚
- en: do your computations on these devices and then do back propagation through the
    whole say cluster of devicesã€‚But in practiceï¼Œ it's been very difficult to do this
    as a beginner and that's because you have to understand all this distributed setup
    in Pytorrch or TensorFlow and again many ways to make errors and you know keeping
    track of how data is basically parallellyed across nodes is a bit of a painã€‚
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›è®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—ï¼Œç„¶ååœ¨æ•´ä¸ªè®¾å¤‡é›†ç¾¤ä¸­è¿›è¡Œåå‘ä¼ æ’­ã€‚ä½†å®é™…ä¸Šï¼Œä½œä¸ºåˆå­¦è€…åšåˆ°è¿™ä¸€ç‚¹éå¸¸å›°éš¾ï¼Œå› ä¸ºä½ éœ€è¦ç†è§£PyTorchæˆ–TensorFlowä¸­çš„æ‰€æœ‰åˆ†å¸ƒå¼è®¾ç½®ï¼Œå†åŠ ä¸Šå®¹æ˜“å‡ºé”™çš„å¤šç§æ–¹å¼ï¼Œè·Ÿè¸ªæ•°æ®å¦‚ä½•åœ¨èŠ‚ç‚¹ä¹‹é—´å¹¶è¡Œåˆ†é…ä¹Ÿæœ‰ç‚¹éº»çƒ¦ã€‚
- en: So this accelerate library is designed to make this simple for us and so let's
    take a look at the final videoã€‚Is it this oneã€‚Terã€‚Supercharge your by doch training
    loop with eggging face accelerateã€‚There are multiple setups on which you can run
    your trainingï¼Œ it could be on CPUUï¼Œ GPUusï¼Œ GPUsã€‚Distributed on one machine with
    several devices or even several machinesã€‚
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ä¸ªåŠ é€Ÿåº“æ—¨åœ¨è®©æˆ‘ä»¬ç®€å•åŒ–ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æœ€åçš„è§†é¢‘ã€‚æ˜¯è¿™ä¸ªå—ï¼ŸTerã€‚é€šè¿‡Egging FaceåŠ é€Ÿä½ çš„PyTorchè®­ç»ƒå¾ªç¯ã€‚æœ‰å¤šä¸ªè®¾ç½®å¯ä»¥è¿è¡Œä½ çš„è®­ç»ƒï¼Œå¯èƒ½æ˜¯åœ¨CPUã€GPUæˆ–å¤šä¸ªGPUä¸Šã€‚åˆ†å¸ƒåœ¨ä¸€å°æœºå™¨ä¸Šæˆ–ç”šè‡³å¤šå°æœºå™¨ä¸Šã€‚
- en: often called nodes with multiplepo devicesã€‚On top of thatã€‚ there are new tweaks
    to make your training faster or more efficientã€‚ like mixed precision and dip speedã€‚Each
    of a setup or training trick requires you to change the code of your training
    loop in one way or another and to learn a new APIã€‚All were setups sound all by
    the trainer API and also have all field party libraries that can helpã€‚
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸è¢«ç§°ä¸ºå…·æœ‰å¤šä¸ªè®¾å¤‡çš„èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰æ–°çš„è°ƒæ•´å¯ä»¥ä½¿ä½ çš„è®­ç»ƒæ›´å¿«æˆ–æ›´é«˜æ•ˆï¼Œæ¯”å¦‚æ··åˆç²¾åº¦å’Œdipé€Ÿåº¦ã€‚æ¯ä¸ªè®¾ç½®æˆ–è®­ç»ƒæŠ€å·§éƒ½éœ€è¦ä½ ä»¥æŸç§æ–¹å¼æ›´æ”¹è®­ç»ƒå¾ªç¯çš„ä»£ç ï¼Œå¹¶å­¦ä¹ æ–°çš„APIã€‚æ‰€æœ‰çš„è®¾ç½®éƒ½ç”±è®­ç»ƒå™¨APIç®¡ç†ï¼Œè¿˜æœ‰å„ç§ç¬¬ä¸‰æ–¹åº“å¯ä»¥æä¾›å¸®åŠ©ã€‚
- en: The problem with those is that it can feel like a black box and that it might
    not be easy to implement the tweak to the training loop you needã€‚Accelerate has
    been designed specifically to let you retain full control over your training loop
    and be as nontrusive as possible with just four lines of code to add to your training
    loop you are shown of the example of the training loop videoã€‚
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå®ƒå¯èƒ½ä¼šè®©äººæ„Ÿè§‰åƒä¸€ä¸ªé»‘ç®±ï¼Œä¸”å®ç°ä½ æ‰€éœ€çš„è®­ç»ƒå¾ªç¯è°ƒæ•´å¯èƒ½ä¸å®¹æ˜“ã€‚Accelerateè¢«ç‰¹åˆ«è®¾è®¡æˆè®©ä½ ä¿æŒå¯¹è®­ç»ƒå¾ªç¯çš„å®Œå…¨æ§åˆ¶ï¼Œå¹¶å°½å¯èƒ½ä¸å¹²æ‰°ï¼Œåªéœ€å‘ä½ çš„è®­ç»ƒå¾ªç¯æ·»åŠ å››è¡Œä»£ç ï¼Œä½ å¯ä»¥åœ¨è®­ç»ƒå¾ªç¯è§†é¢‘çš„ç¤ºä¾‹ä¸­çœ‹åˆ°ã€‚
- en: Accelerate will install all the seteps and training tricks monsoons on the first
    slideã€‚It's only one API to learn on master instead of 10 different onesã€‚More specificallyã€‚
    you have to import and instant sheet an accelerator object that will handle all
    the necessary code for your specific setã€‚Then you have to send it to modelï¼Œ optimizerï¼Œ
    and data you're using in the prepare methodã€‚
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Accelerateå°†ä¼šåœ¨ç¬¬ä¸€å¼ å¹»ç¯ç‰‡ä¸Šå®‰è£…æ‰€æœ‰çš„æ­¥éª¤å’Œè®­ç»ƒæŠ€å·§ã€‚åªéœ€å­¦ä¹ ä¸€ä¸ªAPIï¼Œè€Œä¸æ˜¯10ä¸ªä¸åŒçš„ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä½ å¿…é¡»å¯¼å…¥å¹¶å³æ—¶åˆ›å»ºä¸€ä¸ªåŠ é€Ÿå™¨å¯¹è±¡ï¼Œå®ƒå°†å¤„ç†ä½ ç‰¹å®šè®¾ç½®æ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚ç„¶åï¼Œä½ éœ€è¦å°†å…¶å‘é€åˆ°æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œä½ åœ¨prepareæ–¹æ³•ä¸­ä½¿ç”¨çš„æ•°æ®ã€‚
- en: This is the main method to rememberã€‚Accelerate on those device placementã€‚ so
    you don't need to put your batchge on the specific device you're usingã€‚Finallyã€‚
    you have to replace the lost dot backward line by Ac tall dot backward lossã€‚And
    that's all itã€‚Accelator also involves distributed evaluationã€‚You can still use
    the classic evaluation loopã€‚
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½è¿™æ˜¯ä¸»è¦çš„æ–¹æ³•ã€‚Accelerateå¤„ç†è®¾å¤‡æ”¾ç½®ï¼Œå› æ­¤ä½ ä¸éœ€è¦å°†ä½ çš„æ‰¹é‡æ”¾ç½®åœ¨ä½ æ­£åœ¨ä½¿ç”¨çš„ç‰¹å®šè®¾å¤‡ä¸Šã€‚æœ€åï¼Œä½ éœ€è¦ç”¨`Ac tall dot backward
    loss`æ›¿æ¢ä¸¢å¤±çš„`dot backward`è¡Œã€‚å°±è¿™æ ·ã€‚åŠ é€Ÿå™¨è¿˜æ¶‰åŠåˆ†å¸ƒå¼è¯„ä¼°ã€‚ä½ ä»ç„¶å¯ä»¥ä½¿ç”¨ç»å…¸çš„è¯„ä¼°å¾ªç¯ã€‚
- en: such as what we saw in the training group videoï¼Œ in which case all processes
    will perform the full evaluationã€‚To use a distributed evaluationï¼Œ you just have
    to adapt your evaluation look like thisã€‚ that along the evaluation that error
    to the acceleratedccelerator or per like trainingã€‚Then you can dismiss the line
    that places the batch on the proper deviceã€‚
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬åœ¨è®­ç»ƒç»„è§†é¢‘ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰€æœ‰è¿‡ç¨‹å°†æ‰§è¡Œå®Œæ•´è¯„ä¼°ã€‚è¦ä½¿ç”¨åˆ†å¸ƒå¼è¯„ä¼°ï¼Œä½ åªéœ€å°†è¯„ä¼°å¾ªç¯è°ƒæ•´å¦‚ä¸‹ï¼Œæ²¿ç€è¯„ä¼°å°†é”™è¯¯ä¼ é€’ç»™åŠ é€Ÿå™¨ï¼Œæˆ–åƒè®­ç»ƒä¸€æ ·ã€‚ç„¶åä½ å¯ä»¥çœç•¥å°†æ‰¹é‡æ”¾ç½®åœ¨é€‚å½“è®¾å¤‡ä¸Šçš„é‚£ä¸€è¡Œã€‚
- en: And just before passing your predictions and labels to your metricã€‚ use accelerator
    to gather to give the predictions and labels from each processã€‚A distributed training
    script has to be launched several times on different processesï¼Œ for instanceã€‚
    one per GPU you're usingï¼Œ you can use the Pytoch tools to do that if you're familiar
    with themã€‚
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°†ä½ çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ä½ çš„æŒ‡æ ‡ä¹‹å‰ï¼Œä½¿ç”¨åŠ é€Ÿå™¨æ¥æ”¶é›†æ¯ä¸ªè¿‡ç¨‹çš„é¢„æµ‹å’Œæ ‡ç­¾ã€‚ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬å¿…é¡»åœ¨ä¸åŒçš„è¿›ç¨‹ä¸Šå¤šæ¬¡å¯åŠ¨ï¼Œä¾‹å¦‚ï¼Œæ¯ä¸ªGPUä¸€ä¸ªï¼Œå¦‚æœä½ ç†Ÿæ‚‰å®ƒä»¬ï¼Œå¯ä»¥ä½¿ç”¨PyTorchå·¥å…·æ¥å®ç°ã€‚
- en: The Acrate also provides an easy API to configure your setup and launch your
    training scriptã€‚In a terminalï¼Œ run accelerate Config and answer a small questionnaireer
    to generate a configuration file with all the relevant informationã€‚Then you can
    just run accelerate launch followed by the past or your training scriptã€‚In a notebookã€‚
    you can use a notebook launcher function to launch your trainingã€‚
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Acrateè¿˜æä¾›äº†ä¸€ä¸ªç®€å•çš„APIæ¥é…ç½®ä½ çš„è®¾ç½®å¹¶å¯åŠ¨ä½ çš„è®­ç»ƒè„šæœ¬ã€‚åœ¨ç»ˆç«¯ä¸­ï¼Œè¿è¡Œaccelerate configå¹¶å›ç­”ä¸€ä¸ªå°é—®å·ï¼Œä»¥ç”ŸæˆåŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯çš„é…ç½®æ–‡ä»¶ã€‚ç„¶åä½ å¯ä»¥è¿è¡Œaccelerate
    launchï¼Œåé¢è·Ÿä¸Šä½ çš„è®­ç»ƒè„šæœ¬ã€‚åœ¨ç¬”è®°æœ¬ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ç¬”è®°æœ¬å¯åŠ¨å‡½æ•°æ¥å¯åŠ¨è®­ç»ƒã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_30.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_30.png)'
- en: '![](img/a1136557282a500704797139dc43d7b8_31.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_31.png)'
- en: Okayã€‚So before we have a look at low level training loops in Pytorch and the
    Accelatorã€‚ there's a question from EBtan who saysï¼ŒCan I find out how many labeled
    samples do we need to get good results with transfer learning and fine tuningï¼Ÿ
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚åœ¨æˆ‘ä»¬æŸ¥çœ‹Pytorchå’ŒåŠ é€Ÿå™¨ä¸­çš„ä½çº§è®­ç»ƒå¾ªç¯ä¹‹å‰ï¼Œæœ‰ä¸ªé—®é¢˜æ¥è‡ªEBtanï¼Œä»–é—®ï¼Œæˆ‘å¯ä»¥æ‰¾å‡ºéœ€è¦å¤šå°‘æ ‡è®°æ ·æœ¬æ‰èƒ½åœ¨è¿ç§»å­¦ä¹ å’Œå¾®è°ƒä¸­è·å¾—å¥½ç»“æœå—ï¼Ÿ
- en: Is there a rule of thumb hereï¼ŸThat's a reallyï¼Œ really good questionã€‚ğŸ˜Šï¼ŒAt least
    in my experienceã€‚ the answer really depends on the task that youre trying to tackle
    so generally speaking and you know the language that you're dealing with and the
    domain so these are all factors to consider so a rough hierarchy at least from
    my perspective is that text classification is generally one of the simpler tasks
    to tackle with this and in a sort of standard text classification context you
    may only need maybe 100 samples with transfer learning to get quite good resultsã€‚
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰æ²¡æœ‰ç»éªŒæ³•åˆ™ï¼Ÿè¿™æ˜¯ä¸ªéå¸¸ï¼Œéå¸¸å¥½çš„é—®é¢˜ã€‚ğŸ˜Šï¼Œè‡³å°‘åœ¨æˆ‘çœ‹æ¥ï¼Œç­”æ¡ˆçœŸçš„å–å†³äºä½ æƒ³è¦è§£å†³çš„ä»»åŠ¡ï¼Œæ‰€ä»¥ä¸€èˆ¬æ¥è¯´ï¼Œä½ éœ€è¦è€ƒè™‘ä½ æ‰€å¤„ç†çš„è¯­è¨€å’Œé¢†åŸŸï¼Œè¿™äº›éƒ½æ˜¯éœ€è¦è€ƒè™‘çš„å› ç´ ã€‚ä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼Œç²—ç•¥çš„å±‚æ¬¡ç»“æ„æ˜¯æ–‡æœ¬åˆ†ç±»é€šå¸¸æ˜¯æœ€ç®€å•çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œåœ¨æ ‡å‡†æ–‡æœ¬åˆ†ç±»çš„èƒŒæ™¯ä¸‹ï¼Œä½ å¯èƒ½åªéœ€è¦å¤§çº¦100ä¸ªæ ·æœ¬ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ å¯ä»¥è·å¾—ç›¸å½“ä¸é”™çš„ç»“æœã€‚
- en: And of courseï¼Œ the thing that you should always do is build a baselineï¼Œ so build
    a reallyã€‚ really simple modelï¼Œ not a transformerï¼Œ do something like naive phase
    or logistic regression or something like thisã€‚Just as a sanity check that on your
    small data setï¼Œ yourï¼Œ yourï¼Œ your results when theyã€‚ do they transfer from the
    training set to theã€‚Validation set because there's a reallyã€‚
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ åº”è¯¥å§‹ç»ˆåšçš„äº‹æƒ…æ˜¯å»ºç«‹åŸºå‡†ï¼Œå› æ­¤æ„å»ºä¸€ä¸ªéå¸¸ï¼Œéå¸¸ç®€å•çš„æ¨¡å‹ï¼Œä¸è¦ä½¿ç”¨å˜æ¢å™¨ï¼Œåšä¸€äº›åƒæœ´ç´ è´å¶æ–¯æˆ–é€»è¾‘å›å½’ä¹‹ç±»çš„äº‹æƒ…ã€‚ä»…ä»…ä½œä¸ºä¸€ä¸ªç†æ™ºæ£€æŸ¥ï¼Œçœ‹çœ‹åœ¨ä½ çš„å°æ•°æ®é›†ä¸Šï¼Œå½“ä½ çš„ç»“æœæ˜¯å¦èƒ½ä»è®­ç»ƒé›†è½¬ç§»åˆ°éªŒè¯é›†ï¼Œå› ä¸ºç¡®å®å­˜åœ¨ã€‚
- en: really good chance of overfitting hereã€‚So roughly a few like a hundred to a
    few hundred samples for text classificationã€‚ at least for me has worked wellï¼Œ
    but then it gets a bit harder as you do different tasks so if you move to say
    question answeringã€‚
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰å¾ˆå¤§çš„è¿‡æ‹Ÿåˆé£é™©ã€‚æ‰€ä»¥å¤§è‡´ä¸Šï¼Œå‡ ç™¾ä¸ªæ ·æœ¬å¯¹äºæ–‡æœ¬åˆ†ç±»æ¥è¯´è‡³å°‘å¯¹æˆ‘æ¥è¯´æ•ˆæœå¾ˆå¥½ï¼Œä½†éšç€ä½ å¤„ç†ä¸åŒçš„ä»»åŠ¡ï¼Œè¿™ä¼šå˜å¾—æœ‰ç‚¹å›°éš¾ï¼Œæ¯”å¦‚å¦‚æœä½ è½¬å‘é—®ç­”ã€‚
- en: Here there's like sayï¼Œ two strategies you can takeã€‚So one strategyã€‚Which is
    maybe the first one to start withã€‚Is if you're doing question answering or extractive
    question answering like Sã€‚Is to look for a model that has been trained already
    on squad in your domain ideallyã€‚ So let's suppose I was doingã€‚Squad in Germanï¼Œ
    which is a bit different to the standard caseã€‚
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸¤ç§ç­–ç•¥å¯ä»¥é€‰æ‹©ã€‚å…¶ä¸­ä¸€ç§ç­–ç•¥ï¼Œå¯èƒ½æ˜¯å¼€å§‹æ—¶çš„ç¬¬ä¸€ç§ï¼Œå°±æ˜¯å¦‚æœä½ åœ¨è¿›è¡Œé—®ç­”æˆ–æŠ½å–å¼é—®ç­”ï¼Œæ¯”å¦‚Sï¼Œé‚£ä¹ˆè¦å¯»æ‰¾ä¸€ä¸ªåœ¨ä½ æ‰€åœ¨é¢†åŸŸçš„SQUADä¸Šå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚å‡è®¾æˆ‘åœ¨åšå¾·è¯­çš„SQUADï¼Œè¿™ä¸æ ‡å‡†æƒ…å†µæœ‰äº›ä¸åŒã€‚
- en: maybe I look for squad hereã€‚Andï¼Œ let's seeã€‚I believe there's a German squad
    and maybe I can specify the language to Germanã€‚Okayï¼Œ we don't have thatã€‚So let's
    have a lookï¼Œ I believeã€‚There's a company called Deepset who have done this It's
    called Quadï¼Œ okayã€‚So German here is basically a language model that has been fine
    tuned on a German version of Sã€‚
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘åœ¨è¿™é‡Œå¯»æ‰¾SQUADã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘ç›¸ä¿¡æœ‰ä¸€ä¸ªå¾·è¯­SQUADï¼Œä¹Ÿè®¸æˆ‘å¯ä»¥å°†è¯­è¨€æŒ‡å®šä¸ºå¾·è¯­ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æ²¡æœ‰é‚£ä¸ªã€‚è®©æˆ‘ä»¬çœ‹çœ‹ï¼Œæˆ‘ç›¸ä¿¡ï¼Œæœ‰ä¸€å®¶å…¬å¸å«Deepsetåšäº†è¿™ä¸ªï¼Œå®ƒå«Quadï¼Œå¥½çš„ã€‚æ‰€ä»¥å¾·è¯­åœ¨è¿™é‡ŒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªåœ¨å¾·è¯­ç‰ˆæœ¬çš„Sä¸Šå¾®è°ƒçš„è¯­è¨€æ¨¡å‹ã€‚
- en: and now this lets you answer questions in Germanã€‚Umï¼Œ soã€‚ğŸ˜Šã€‚Then I would take
    this model and if my corpus is in Germanï¼Œ but it's likeï¼Œ you knowï¼Œ customã€‚ maybe
    it's my business dataï¼Œ I would then just try to see how this model works on that
    data setã€‚Nowã€‚ generallyï¼Œ you'll find that it doesn't perform as good as the original
    squad modelã€‚
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿ä½ èƒ½å¤Ÿç”¨å¾·è¯­å›ç­”é—®é¢˜ã€‚å—¯ï¼Œæ‰€ä»¥ã€‚ğŸ˜Šã€‚ç„¶åæˆ‘ä¼šæ‹¿è¿™ä¸ªæ¨¡å‹ï¼Œå¦‚æœæˆ‘çš„è¯­æ–™åº“æ˜¯å¾·è¯­ï¼Œä½†å®ƒæ˜¯å®šåˆ¶çš„ï¼Œå¯èƒ½æ˜¯æˆ‘çš„å•†ä¸šæ•°æ®ï¼Œæˆ‘ä¼šå°è¯•çœ‹çœ‹è¿™ä¸ªæ¨¡å‹åœ¨é‚£ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚é€šå¸¸ï¼Œä½ ä¼šå‘ç°å®ƒçš„è¡¨ç°ä¸å¦‚åŸå§‹çš„SQUADæ¨¡å‹ã€‚
- en: And then you will do some say domain adaptation from the S model to your domainã€‚
    so you basically just do a little bit of fine tuning on your domain and typically
    you'll then see the model adapts to your corpus and you'll get much better performanceã€‚And
    in that context forï¼Œ sayï¼Œ question answeringï¼Œ typically you need light on the
    scale of a few thousandã€‚ youï¼Œ a thousand to a few thousand examples to get at
    least good resultsã€‚
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å°†ä¼šä»Sæ¨¡å‹åˆ°ä½ çš„é¢†åŸŸè¿›è¡Œä¸€äº›é¢†åŸŸé€‚é…ã€‚æ‰€ä»¥ä½ åŸºæœ¬ä¸Šåªæ˜¯å¯¹ä½ çš„é¢†åŸŸè¿›è¡Œä¸€äº›å¾®è°ƒï¼Œé€šå¸¸ä½ ä¼šçœ‹åˆ°æ¨¡å‹é€‚åº”ä½ çš„è¯­æ–™åº“ï¼Œå¹¶ä¸”ä½ ä¼šè·å¾—æ›´å¥½çš„è¡¨ç°ã€‚åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼Œæ¯”å¦‚é—®ç­”ä»»åŠ¡ï¼Œé€šå¸¸ä½ éœ€è¦å¤§çº¦å‡ åƒä¸ªç¤ºä¾‹ï¼Œå¤§çº¦ä¸€åƒåˆ°å‡ åƒä¸ªç¤ºä¾‹æ‰èƒ½è·å¾—è‡³å°‘ä¸é”™çš„ç»“æœã€‚
- en: But you have to be very careful because sometimes when you do this domain adaptation
    from one model to anotherã€‚ if you sort of overfit your domainï¼Œ you'll end up forgetting
    all of the good features that the fine tune model had to start with and so it's
    generally tricky in that senseã€‚
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ å¿…é¡»éå¸¸å°å¿ƒï¼Œå› ä¸ºæœ‰æ—¶å€™åœ¨å°†ä¸€ä¸ªæ¨¡å‹çš„é¢†åŸŸé€‚é…åˆ°å¦ä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œå¦‚æœä½ è¿‡åº¦æ‹Ÿåˆä½ çš„é¢†åŸŸï¼Œä½ å°†ä¼šå¿˜è®°å¾®è°ƒæ¨¡å‹èµ·åˆå…·å¤‡çš„æ‰€æœ‰ä¼˜è‰¯ç‰¹å¾ï¼Œå› æ­¤åœ¨è¿™ä¸ªæ„ä¹‰ä¸Šé€šå¸¸æ˜¯å¾ˆæ£˜æ‰‹çš„ã€‚
- en: å—¯ã€‚ğŸ˜Šï¼ŒAnd then for other tasks like named entity recognitionï¼Œ this is I think
    very problem specificã€‚ it really depends on the entitiesï¼Œ what are the frequencies
    of the entities that you have and here again I think you're dealing with on the
    scale of like a few thousand examples to get okay resultsã€‚
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚ğŸ˜Š ç„¶åå¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œæ¯”å¦‚å‘½åå®ä½“è¯†åˆ«ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯éå¸¸é—®é¢˜ç‰¹å®šçš„ï¼ŒçœŸçš„å–å†³äºä½ æ‹¥æœ‰çš„å®ä½“ï¼Œä»¥åŠè¿™äº›å®ä½“çš„é¢‘ç‡ï¼Œæˆ‘è§‰å¾—ä½ å¯èƒ½éœ€è¦å‡ åƒä¸ªç¤ºä¾‹æ‰èƒ½è·å¾—è¿˜ä¸é”™çš„ç»“æœã€‚
- en: So to summarizeã€‚ğŸ˜Šï¼Œå—¯ã€‚ğŸ˜Šï¼ŒOkayï¼Œ well just to summarize I'll answer to the questionã€‚
    so to summarize we're talking about maybe a few hundred examplesã€‚ labeled examples
    for simple problems like text classification to a few thousandã€‚For the labeled
    case and maybe even more if you're doing something that's very nicheã€‚
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ€»ç»“ä¸€ä¸‹ã€‚ğŸ˜Š å—¯ã€‚ğŸ˜Š å¥½å§ï¼Œç®€å•æ€»ç»“ä¸€ä¸‹æˆ‘çš„å›ç­”ã€‚æˆ‘ä»¬åœ¨è°ˆè®ºçš„å¯èƒ½æ˜¯å‡ ç™¾ä¸ªæ ‡è®°ç¤ºä¾‹ç”¨äºåƒæ–‡æœ¬åˆ†ç±»è¿™æ ·ç®€å•çš„é—®é¢˜ï¼Œæˆ–è€…å‡ åƒä¸ªã€‚åœ¨æ ‡è®°çš„æƒ…å†µä¸‹ï¼Œå¦‚æœä½ åšçš„äº‹æƒ…éå¸¸å°ä¼—ï¼Œç”šè‡³å¯èƒ½éœ€è¦æ›´å¤šã€‚
- en: I mean if you're doing maybe I'm doing like maybe like legal analysis of legal
    contracts and maybe the domain is so different from any existing pret model that
    I actually need to do some sort of finetning on a legal corpus that looks like
    what I have and so then you don't need labeled data per say you just need a lot
    of unlabeled legal contracts that you can then finet a language model and then
    transfer to your domainã€‚
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ„æ€æ˜¯ï¼Œå¦‚æœæˆ‘åœ¨åšæ³•å¾‹åˆåŒçš„æ³•å¾‹åˆ†æï¼Œå¯èƒ½è¿™ä¸ªé¢†åŸŸä¸ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æœ‰å¾ˆå¤§ä¸åŒï¼Œæˆ‘å®é™…ä¸Šéœ€è¦å¯¹ä¸€ä¸ªçœ‹èµ·æ¥åƒæˆ‘æ‹¥æœ‰çš„æ³•å¾‹è¯­æ–™åº“è¿›è¡ŒæŸç§å¾®è°ƒï¼Œå› æ­¤ä½ ä¸éœ€è¦æ ‡è®°æ•°æ®ï¼Œä½ åªéœ€è¦å¤§é‡çš„æœªæ ‡è®°æ³•å¾‹åˆåŒï¼Œç„¶åå¯ä»¥å¾®è°ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæ¥ç€è½¬ç§»åˆ°ä½ çš„é¢†åŸŸã€‚
- en: Okayï¼Œ so there's a follow up question which saysï¼Œ thank you for the answerã€‚
    What if I have more than a few hundredã€‚Of classes for a taskã€‚ Okayï¼Œ yesã€‚ so then
    that thing gets very hardã€‚I would put that in so maybe for text classification
    there's like a hierarchy of complexity so the simplest case is like binary classification
    and then maybe multiclass is after that and then maybe multiclass with like you
    know extreme number of labels like1 hundred or1 thousand different labels or different
    categories and then maybe multilabel is like somewhere around that so I think
    in general it will be harder and what you will find if you have a data set which
    has like1 hundred0 classes or more is that the model will be very confident about
    the majority class so there's always a distribution generally in these classes
    and most of the time it's like a power law so you'll have a few classes that are
    very common and then there'll be some that are quite rare just because you know
    they tags or something that people don't really use very oftenã€‚
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ªåç»­é—®é¢˜ï¼Œæ„Ÿè°¢ä½ çš„å›ç­”ã€‚å¦‚æœæˆ‘æœ‰å‡ ç™¾ä¸ªä»¥ä¸Šçš„ä»»åŠ¡ç±»åˆ«ï¼Œå¥½çš„ï¼Œæ˜¯çš„ã€‚é‚£ä¹ˆè¿™å°±å˜å¾—éå¸¸å›°éš¾ã€‚æˆ‘è®¤ä¸ºè¿™å¯ä»¥å½’å…¥æ–‡æœ¬åˆ†ç±»çš„å¤æ‚æ€§å±‚æ¬¡ä¸­ï¼Œæœ€ç®€å•çš„æƒ…å†µæ˜¯äºŒåˆ†ç±»ï¼Œæ¥ç€å¯èƒ½æ˜¯å¤šåˆ†ç±»ï¼Œç„¶åæ˜¯æœ‰å¾ˆå¤šæ ‡ç­¾çš„å¤šåˆ†ç±»ï¼Œæ¯”å¦‚ä¸€ç™¾æˆ–ä¸€åƒä¸ªä¸åŒçš„æ ‡ç­¾æˆ–ç±»åˆ«ï¼Œç„¶åå¤šæ ‡ç­¾å¤§è‡´åœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºæ€»ä½“ä¸Šä¼šæ›´éš¾ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œæœ‰ä¸€åƒä¸ªæˆ–æ›´å¤šç±»åˆ«ï¼Œä½ ä¼šå‘ç°æ¨¡å‹ä¼šå¯¹ä¸»è¦ç±»åˆ«éå¸¸è‡ªä¿¡ï¼Œé€šå¸¸åœ¨è¿™äº›ç±»åˆ«ä¸­æ€»ä¼šæœ‰ä¸€ç§åˆ†å¸ƒï¼Œè€Œå¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯å¹‚å¾‹åˆ†å¸ƒï¼Œæ‰€ä»¥ä½ ä¼šæœ‰ä¸€äº›éå¸¸å¸¸è§çš„ç±»åˆ«ï¼Œè¿˜æœ‰ä¸€äº›éå¸¸ç¨€æœ‰çš„ç±»åˆ«ï¼Œåªå› ä¸ºè¿™äº›æ ‡ç­¾æˆ–è€…å…¶ä»–äººä¸å¸¸ç”¨ã€‚
- en: So the model will struggle a lot in those rare tasksï¼Œ in those rare tags or
    those rare labelsã€‚And so then what I would recommend you do is you try to focus
    on the hard examplesã€‚ so you try to collect more data for those rare examples
    to boost the signal for the modelã€‚So you don't need maybe like 10 more examples
    of the common classã€‚
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ¨¡å‹åœ¨é‚£äº›ç¨€æœ‰ä»»åŠ¡ã€ç¨€æœ‰æ ‡ç­¾æˆ–ç¨€æœ‰æ ‡ç­¾ä¸Šä¼šé‡åˆ°å¾ˆå¤šå›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘å»ºè®®ä½ é›†ä¸­ç²¾åŠ›å¤„ç†å›°éš¾ç¤ºä¾‹ï¼Œå°½é‡æ”¶é›†æ›´å¤šç¨€æœ‰ç¤ºä¾‹çš„æ•°æ®ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„ä¿¡å·ã€‚å› æ­¤ï¼Œä½ å¯èƒ½ä¸éœ€è¦å¤šæ”¶é›†10ä¸ªå¸¸è§ç±»åˆ«çš„ç¤ºä¾‹ã€‚
- en: you need100 more examples of the rare onesã€‚m but in generalã€‚ as long as you
    can kind of get like a good coverage in the labelsã€‚ it should work except it it's
    just going to be tough becauseï¼Œ you knowï¼Œ with 100 optionsã€‚ the model has more
    chances to pay mistakesã€‚And in factï¼Œ there's a very good trick I should mentionã€‚
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦å¤š100ä¸ªç¨€æœ‰ç¤ºä¾‹ã€‚ä½†ä¸€èˆ¬æ¥è¯´ï¼Œåªè¦åœ¨æ ‡ç­¾ä¸Šæœ‰è‰¯å¥½çš„è¦†ç›–ï¼Œå®ƒåº”è¯¥å°±èƒ½å·¥ä½œï¼Œåªæ˜¯ä¼šæ¯”è¾ƒå›°éš¾ï¼Œå› ä¸ºä½ çŸ¥é“ï¼Œæœ‰100ä¸ªé€‰é¡¹ï¼Œæ¨¡å‹çŠ¯é”™è¯¯çš„å‡ ç‡æ›´é«˜ã€‚äº‹å®ä¸Šï¼Œæœ‰ä¸€ä¸ªå¾ˆå¥½çš„æŠ€å·§æˆ‘åº”è¯¥æåˆ°ã€‚
- en: is that when you train a modelã€‚You can extract or compute the loss that the
    model has on every single sampleã€‚And if you do thisï¼Œ this will tell you roughly
    which examples the model is most confused aboutã€‚ and this is actually a technique
    from FastAoã€‚![](img/a1136557282a500704797139dc43d7b8_33.png)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä½ å¯ä»¥æå–æˆ–è®¡ç®—æ¨¡å‹åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šçš„æŸå¤±ã€‚å¦‚æœä½ è¿™æ ·åšï¼Œè¿™å°†å¤§è‡´å‘Šè¯‰ä½ æ¨¡å‹åœ¨å“ªäº›ç¤ºä¾‹ä¸Šæœ€å›°æƒ‘ï¼Œè€Œè¿™å®é™…ä¸Šæ˜¯FastAoä¸­çš„ä¸€ç§æŠ€æœ¯ã€‚![](img/a1136557282a500704797139dc43d7b8_33.png)
- en: '![](img/a1136557282a500704797139dc43d7b8_34.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_34.png)'
- en: Wellï¼Œ I don't know if they've entered itï¼Œ butã€‚At least this is where I first
    saw itã€‚![](img/a1136557282a500704797139dc43d7b8_36.png)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œæˆ‘ä¸çŸ¥é“ä»–ä»¬æ˜¯å¦å·²ç»è¾“å…¥äº†ï¼Œä½†è‡³å°‘è¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°å®ƒçš„åœ°æ–¹ã€‚![](img/a1136557282a500704797139dc43d7b8_36.png)
- en: Andã€‚Let's see if we look here at fast AIã€‚å—¯ã€‚Soï¼Œ I thinkã€‚It's called most confusedã€‚ğŸ˜”ï¼ŒBut
    let's seeã€‚Okayï¼Œ so things have changed a bit in V2ã€‚ğŸ˜”ï¼ŒOkayï¼Œ I can't quite find
    itã€‚ but basically in the FAA library they have a function which lets you plot
    the most confused examples that the model is having trouble with and then with
    those you can then work out where you need to collect more data but roughly speaking
    what's happening is it just confuse the loss for every example in your validation
    set and then just sorts them and so if you do that yourself manually you'll be
    able to see where you need to improve the modelã€‚
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹fast AIã€‚å¦‚æœæˆ‘ä»¬çœ‹çœ‹ã€‚å—¯ã€‚æˆ‘æƒ³ï¼Œå«åšâ€œæœ€å›°æƒ‘çš„â€ã€‚ğŸ˜”ï¼Œä½†è®©æˆ‘ä»¬çœ‹çœ‹ã€‚å¥½çš„ï¼ŒV2ä¸­æƒ…å†µæœ‰æ‰€å˜åŒ–ã€‚ğŸ˜”ï¼Œå¥½çš„ï¼Œæˆ‘æ‰¾ä¸åˆ°å®ƒã€‚ä½†åŸºæœ¬ä¸Šåœ¨FAAåº“ä¸­ï¼Œä»–ä»¬æœ‰ä¸€ä¸ªåŠŸèƒ½ï¼Œå¯ä»¥ç»˜åˆ¶æ¨¡å‹é‡åˆ°å›°éš¾çš„æœ€å›°æƒ‘ç¤ºä¾‹ï¼Œç„¶åé€šè¿‡è¿™äº›ä½ å¯ä»¥æ‰¾å‡ºéœ€è¦æ”¶é›†æ›´å¤šæ•°æ®çš„åœ°æ–¹ã€‚ä½†å¤§è‡´æ¥è¯´ï¼Œå‘ç”Ÿçš„æƒ…å†µæ˜¯å®ƒä¼šæ··æ·†éªŒè¯é›†ä¸­æ¯ä¸ªç¤ºä¾‹çš„æŸå¤±ï¼Œç„¶åå¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œæ‰€ä»¥å¦‚æœä½ æ‰‹åŠ¨è¿™æ ·åšï¼Œä½ å°†èƒ½çœ‹åˆ°éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚
- en: Okayï¼Œ so let's wrap up by diving into the Acelerate libraryã€‚So the difference
    here is thatã€‚ instead of usingã€‚![](img/a1136557282a500704797139dc43d7b8_38.png)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæˆ‘ä»¬æ¥æ€»ç»“ä¸€ä¸‹ï¼Œæ·±å…¥äº†è§£Acelerateåº“ã€‚æ‰€ä»¥è¿™é‡Œçš„åŒºåˆ«æ˜¯ï¼Œä¸æ˜¯ä½¿ç”¨ã€‚![](img/a1136557282a500704797139dc43d7b8_38.png)
- en: ã€‚
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/a1136557282a500704797139dc43d7b8_40.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_40.png)'
- en: GPPUï¼Œ we're going to use the TPU hardware in the back endã€‚ So we activate it
    this wayã€‚![](img/a1136557282a500704797139dc43d7b8_42.png)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: GPPUï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åç«¯çš„TPUç¡¬ä»¶ã€‚æ‰€ä»¥æˆ‘ä»¬ä»¥è¿™ç§æ–¹å¼æ¿€æ´»å®ƒã€‚![](img/a1136557282a500704797139dc43d7b8_42.png)
- en: And we install transformers and data setsï¼Œ as we always haveã€‚![](img/a1136557282a500704797139dc43d7b8_44.png)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·å®‰è£…transformerså’Œæ•°æ®é›†ã€‚![](img/a1136557282a500704797139dc43d7b8_44.png)
- en: And the the main thing that you need to do now is you need to install some accelerate
    and some TPU specific librariesã€‚ So in order to run pie torch on a TPUï¼Œ we need
    some special wheels they called more basically a binary file that we can install
    here andã€‚
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç°åœ¨éœ€è¦åšçš„ä¸»è¦äº‹æƒ…æ˜¯å®‰è£…ä¸€äº›åŠ é€Ÿå’ŒTPUç‰¹å®šçš„åº“ã€‚å› æ­¤ï¼Œä¸ºäº†åœ¨TPUä¸Šè¿è¡ŒPyTorchï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›ç‰¹æ®Šçš„è½®å­ï¼Œä»–ä»¬åŸºæœ¬ä¸Šç§°ä¹‹ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå®‰è£…ã€‚
- en: This will then allow us to run Pitorrch on TPUã€‚And the first thing we do is
    always the sameã€‚ it's like we just tokenize the dataï¼Œ So this is pretty familiar
    by nowã€‚And one thing that Sylvan is doing here is he's removing all of the text
    columns or the columns that we don't really want for trainingã€‚Explicitlyã€‚This
    is to make it easier so that the trainer doesn't get confused when it receives
    raw textã€‚
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å…è®¸æˆ‘ä»¬åœ¨TPUä¸Šè¿è¡ŒPyTorchã€‚æˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ€»æ˜¯ä¸€æ ·çš„ï¼Œå°±åƒæˆ‘ä»¬åªæ˜¯å¯¹æ•°æ®è¿›è¡Œåˆ†è¯ã€‚è¿™ç°åœ¨å·²ç»å¾ˆç†Ÿæ‚‰äº†ã€‚Sylvanåœ¨è¿™é‡Œåšçš„ä¸€ä»¶äº‹æ˜¯ç§»é™¤æ‰€æœ‰æ–‡æœ¬åˆ—æˆ–æˆ‘ä»¬åœ¨è®­ç»ƒæ—¶ä¸éœ€è¦çš„åˆ—ã€‚æ˜ç¡®æ¥è¯´ï¼Œè¿™æ˜¯ä¸ºäº†è®©è®­ç»ƒè€…åœ¨æ¥æ”¶åŸå§‹æ–‡æœ¬æ—¶ä¸è‡³äºæ··æ·†ã€‚
- en: And he's also renaming the label column to labels and this just helps the trainer
    auto detect which column it should compute the metrics onã€‚And the other thing
    that he's doing is setting the format of the elements in the data set to pytorage
    tensorsã€‚So I'm just going to show you that just quicklyã€‚If we look at our tokenizedã€‚Tokenized
    data setsã€‚ train oopsã€‚ğŸ˜”ï¼ŒWe look at one elementã€‚å—¯mã€‚Nowï¼Œ what we've seen so far
    in today's session is that everything was just a Python listã€‚
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–è¿˜å°†æ ‡ç­¾åˆ—é‡å‘½åä¸º labelsï¼Œè¿™æœ‰åŠ©äºè®­ç»ƒå™¨è‡ªåŠ¨æ£€æµ‹åº”è¯¥åœ¨å“ªä¸€åˆ—è®¡ç®—æŒ‡æ ‡ã€‚å¦ä¸€ä»¶ä»–æ­£åœ¨åšçš„äº‹æƒ…æ˜¯å°†æ•°æ®é›†ä¸­å…ƒç´ çš„æ ¼å¼è®¾ç½®ä¸º Pytorch å¼ é‡ã€‚å› æ­¤ï¼Œæˆ‘åªæ˜¯æƒ³å¿«é€Ÿç»™ä½ å±•ç¤ºä¸€ä¸‹ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æˆ‘ä»¬çš„æ ‡è®°åŒ–æ•°æ®é›†ï¼Œè®­ç»ƒ
    oopsã€‚ğŸ˜”ï¼Œæˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸ªå…ƒç´ ã€‚å—¯mã€‚ç°åœ¨ï¼Œæˆ‘ä»¬åœ¨ä»Šå¤©çš„ä¼šè®®ä¸­çœ‹åˆ°çš„ä¸€åˆ‡éƒ½åªæ˜¯ä¸€ä¸ª Python åˆ—è¡¨ã€‚
- en: but you can change the format of your data set from lists toã€‚Tsorsã€‚ but you
    can also set them to tensorflow tensors if you're running Tensorflow or even I
    think you can do it for nu arraysã€‚ So it really depends on what you're operating
    on that you want to manipulateã€‚ but this is a very handy way of switching the
    formatsã€‚Oopsã€‚So yeahï¼Œ tortureï¼Œ by torture doesn'tï¼Ÿ
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä½ å¯ä»¥å°†æ•°æ®é›†çš„æ ¼å¼ä»åˆ—è¡¨æ›´æ”¹ä¸ºå¼ é‡ã€‚ä½†ä½ ä¹Ÿå¯ä»¥å°†å®ƒä»¬è®¾ç½®ä¸º TensorFlow å¼ é‡ï¼Œå¦‚æœä½ è¿è¡Œ TensorFlowï¼Œç”šè‡³æˆ‘è®¤ä¸ºä½ å¯ä»¥ä¸º nu
    æ•°ç»„æ‰§è¡Œæ­¤æ“ä½œã€‚å› æ­¤ï¼Œè¿™å®é™…ä¸Šå–å†³äºä½ æƒ³è¦æ“ä½œå’Œä¿®æ”¹çš„å†…å®¹ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„åˆ‡æ¢æ ¼å¼çš„æ–¹æ³•ã€‚å“å‘€ï¼Œæ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œé€¼è¿«ï¼ŒçœŸçš„ä¸ï¼Ÿ
- en: Okayï¼Œ so because we're not using the trainer and we're going to use our own
    training loopã€‚ we need to have what are called data loaders so in Pytorchã€‚There's
    kind of two concepts that are important one is like the concept of a data setã€‚
    So these are the things that we've been doing most of the time so farã€‚
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ä½¿ç”¨è®­ç»ƒå™¨ï¼Œè€Œæ˜¯ä½¿ç”¨è‡ªå·±çš„è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬éœ€è¦è¢«ç§°ä¸ºæ•°æ®åŠ è½½å™¨çš„ä¸œè¥¿ï¼Œæ‰€ä»¥åœ¨ Pytorch ä¸­ï¼Œæœ‰ä¸¤ä¸ªé‡è¦çš„æ¦‚å¿µï¼Œä¸€ä¸ªæ˜¯æ•°æ®é›†çš„æ¦‚å¿µã€‚è¿™äº›éƒ½æ˜¯æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢å¤§éƒ¨åˆ†æ—¶é—´æ‰€åšçš„äº‹æƒ…ã€‚
- en: But then when you want to feed batches to your modelï¼Œ there's an API called
    data loaderã€‚ which will automatically sample from the data set and then provide
    those samples to the model so you basically feed in the data set you care about
    and then you can feed in whether you want to randomly shuffle the elements and
    that's important for training just in case you have some you ordering in your
    data that is artificial you can specify the batchche and also how the collation
    is done for the dynamic padding so speakã€‚
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œå½“ä½ æƒ³è¦å‘æ¨¡å‹è¾“å…¥æ‰¹æ¬¡æ—¶ï¼Œæœ‰ä¸€ä¸ªå«åšæ•°æ®åŠ è½½å™¨çš„ APIï¼Œå®ƒä¼šè‡ªåŠ¨ä»æ•°æ®é›†ä¸­æŠ½æ ·ï¼Œç„¶åå°†è¿™äº›æ ·æœ¬æä¾›ç»™æ¨¡å‹ï¼Œå› æ­¤ä½ åŸºæœ¬ä¸Šåªéœ€è¾“å…¥ä½ å…³å¿ƒçš„æ•°æ®é›†ï¼Œç„¶åå¯ä»¥æŒ‡å®šæ˜¯å¦æƒ³è¦éšæœºæ‰“ä¹±å…ƒç´ ï¼Œè¿™å¯¹è®­ç»ƒå¾ˆé‡è¦ï¼Œä»¥é˜²æ•°æ®ä¸­æœ‰ä¸€äº›äººä¸ºçš„é¡ºåºï¼Œä½ è¿˜å¯ä»¥æŒ‡å®šæ‰¹æ¬¡ä»¥åŠåŠ¨æ€å¡«å……çš„æ±‡æ€»æ–¹å¼ã€‚
- en: Okayï¼Œ so you can create a data loader and then if you look at an element of
    a data loaderã€‚ it's just going to be basically a set of tensesors associated with
    each batchã€‚And then we stand here our model just like we always haveã€‚Andã€‚Thenã€‚Once
    this is loadedã€‚We can just do a sanity check that if we feed a single batch which
    just has input IDsã€‚
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œç„¶åå¦‚æœä½ æŸ¥çœ‹æ•°æ®åŠ è½½å™¨çš„ä¸€ä¸ªå…ƒç´ ï¼Œå®ƒåŸºæœ¬ä¸Šå°†æ˜¯ä¸æ¯ä¸ªæ‰¹æ¬¡å…³è”çš„ä¸€ç»„å¼ é‡ã€‚ç„¶åæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·åœ¨è¿™é‡Œç«™ç€æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸€æ—¦åŠ è½½å®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€ä¸ª
    sanity checkï¼Œå¦‚æœæˆ‘ä»¬è¾“å…¥ä¸€ä¸ªåªæœ‰è¾“å…¥ ID çš„å•ä¸€æ‰¹æ¬¡ã€‚
- en: attention to mask labelï¼Œ and these other ideas I want to talk aboutã€‚ it returns
    a tensor which is the lossï¼Œ so this is like a sanity check that our model is workingã€‚And
    because we're going low level into the training loopã€‚ nowã€‚ we also have to specify
    the optimization algorithm for basically minimizing the loss functionã€‚
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„æ©ç æ ‡ç­¾ï¼Œä»¥åŠæˆ‘æƒ³è°ˆè®ºçš„å…¶ä»–æƒ³æ³•ã€‚å®ƒè¿”å›ä¸€ä¸ªå¼ é‡ï¼Œè¿™æ˜¯æŸå¤±ï¼Œå› æ­¤è¿™å°±åƒæ˜¯ä¸€ä¸ª sanity checkï¼Œç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹åœ¨å·¥ä½œã€‚è€Œä¸”ï¼Œå› ä¸ºæˆ‘ä»¬è¿›å…¥è®­ç»ƒå¾ªç¯çš„ä½å±‚æ¬¡ï¼Œç°åœ¨æˆ‘ä»¬è¿˜å¿…é¡»æŒ‡å®šä¼˜åŒ–ç®—æ³•ï¼Œä»¥åŸºæœ¬ä¸Šæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚
- en: So I would say the defaultï¼Œ a very good default is to use this modified at called
    Adam Wã€‚ and you can just use the default learning rateã€‚And you just pass the parameters
    of your model that you want to optimizeã€‚So just to show you quicklyï¼Œ if we have
    a look at thisã€‚This is a generatorã€‚ so I need to create a listã€‚And then if I just
    look at one of these elementsã€‚
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¼šè¯´é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸€ä¸ªéå¸¸å¥½çš„é»˜è®¤å€¼æ˜¯ä½¿ç”¨è¿™ä¸ªä¿®æ”¹è¿‡çš„ Adam Wï¼Œå¹¶ä¸”ä½ å¯ä»¥ä½¿ç”¨é»˜è®¤çš„å­¦ä¹ ç‡ã€‚ä½ åªéœ€ä¼ é€’ä½ æƒ³è¦ä¼˜åŒ–çš„æ¨¡å‹å‚æ•°ã€‚å› æ­¤ï¼Œåªæ˜¯ä¸ºäº†å¿«é€Ÿå±•ç¤ºï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ã€‚æ‰€ä»¥æˆ‘éœ€è¦åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ã€‚å¦‚æœæˆ‘æŸ¥çœ‹å…¶ä¸­ä¸€ä¸ªå…ƒç´ ã€‚
- en: You can see that the parameters of the model consists of tenssã€‚ So these are
    like our weights and biasesã€‚Along with a name that tells us you know which layer
    we're looking at and so by feeding these to Adam we're basically saying here is
    my instructions of weights and biases I want you to optimize and then it will
    proceed when we do the training to do the optimizationã€‚
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„å‚æ•°ç”±å¼ é‡ç»„æˆã€‚å› æ­¤ï¼Œè¿™äº›å°±åƒæˆ‘ä»¬çš„æƒé‡å’Œåç½®ã€‚è¿˜æœ‰ä¸€ä¸ªåç§°å‘Šè¯‰æˆ‘ä»¬ä½ çŸ¥é“æˆ‘ä»¬åœ¨çœ‹å“ªä¸ªå±‚ï¼Œå› æ­¤é€šè¿‡å°†è¿™äº›ä¼ é€’ç»™Adamï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯åœ¨è¯´è¿™æ˜¯æˆ‘æƒ³è¦ä½ ä¼˜åŒ–çš„æƒé‡å’Œåç½®çš„æŒ‡ä»¤ï¼Œç„¶ååœ¨æˆ‘ä»¬è¿›è¡Œè®­ç»ƒæ—¶å®ƒå°†ç»§ç»­è¿›è¡Œä¼˜åŒ–ã€‚
- en: And there's also some extra things we need to specifyï¼Œ like the number of epochsã€‚
    the number of training steps and the scheduler for how we want to sort of control
    the learning rate during trainingã€‚So if these concepts are kind of the first time
    you've seen themã€‚ we have information in the course notes to help you understand
    thatã€‚
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€äº›é¢å¤–çš„ä¸œè¥¿éœ€è¦æŒ‡å®šï¼Œæ¯”å¦‚epochsçš„æ•°é‡ã€è®­ç»ƒæ­¥éª¤çš„æ•°é‡ï¼Œä»¥åŠæˆ‘ä»¬æƒ³è¦æ§åˆ¶è®­ç»ƒæœŸé—´å­¦ä¹ ç‡çš„è°ƒåº¦å™¨ã€‚å› æ­¤ï¼Œå¦‚æœè¿™äº›æ¦‚å¿µæ˜¯ä½ ç¬¬ä¸€æ¬¡è§åˆ°ï¼Œæˆ‘ä»¬åœ¨è¯¾ç¨‹ç¬”è®°ä¸­æä¾›äº†ä¿¡æ¯æ¥å¸®åŠ©ä½ ç†è§£ã€‚
- en: but basically speaking using a constant learning rate throughout training is
    suboptimalã€‚ both in terms of speed in terms of just getting to like a good local
    minimum and so we have schedulers which will essentially control how the learning
    rate increases and decreases during trainingã€‚
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åŸºæœ¬ä¸Šè¯´ï¼Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ’å®šå­¦ä¹ ç‡æ˜¯æ¬¡ä¼˜çš„ï¼Œæ— è®ºæ˜¯åœ¨é€Ÿåº¦ä¸Šè¿˜æ˜¯åœ¨è·å¾—è‰¯å¥½å±€éƒ¨æœ€å°å€¼æ–¹é¢ï¼Œå› æ­¤æˆ‘ä»¬æœ‰è°ƒåº¦å™¨ï¼ŒåŸºæœ¬ä¸Šæ§åˆ¶å­¦ä¹ ç‡åœ¨è®­ç»ƒæœŸé—´å¦‚ä½•å¢åŠ å’Œå‡å°‘ã€‚
- en: And so we can define the schedulerã€‚Did I not define the optimizerï¼ŸOkayï¼Œ soã€‚I'm
    just going to load thisï¼Œ this is just an example when you're doing things on a
    GPUã€‚ you specify the deviceã€‚And this is then more or less what a training loop
    looks like in Pytageã€‚ So you say we're going to do training nowï¼Œ so we're going
    to activate dropout in the modelã€‚
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬å¯ä»¥å®šä¹‰è°ƒåº¦å™¨ã€‚æˆ‘æ²¡æœ‰å®šä¹‰ä¼˜åŒ–å™¨å—ï¼Ÿå¥½çš„ã€‚é‚£ä¹ˆï¼Œæˆ‘åªæ˜¯è¦åŠ è½½è¿™ä¸ªï¼Œè¿™åªæ˜¯ä½ åœ¨GPUä¸Šè¿›è¡Œæ“ä½œæ—¶çš„ä¸€ä¸ªç¤ºä¾‹ã€‚ä½ éœ€è¦æŒ‡å®šè®¾å¤‡ã€‚è¿™å¤§è‡´å°±æ˜¯åœ¨Pytageä¸­è®­ç»ƒå¾ªç¯çš„æ ·å­ã€‚å› æ­¤ä½ è¯´æˆ‘ä»¬ç°åœ¨è¦è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬å°†åœ¨æ¨¡å‹ä¸­æ¿€æ´»dropoutã€‚
- en: And then we loop over for every epoC and every single epoC we're going to loop
    over every single batch in the training data loader and then we're going to set
    those tensors to the device we care about we're going to compute the loss and
    then we're going to just do the backward pass in Pywch and then we just basically
    take a step now with the optimizer and the scheduler and then we update and this
    will then just do training in a normal way on a GPUã€‚
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªepochå¾ªç¯ï¼Œå¯¹äºæ¯ä¸ªepochï¼Œæˆ‘ä»¬å°†å¾ªç¯éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸€ä¸ªbatchï¼Œç„¶åæˆ‘ä»¬å°†è¿™äº›å¼ é‡è®¾ç½®åˆ°æˆ‘ä»¬å…³å¿ƒçš„è®¾å¤‡ä¸Šï¼Œæˆ‘ä»¬å°†è®¡ç®—æŸå¤±ï¼Œç„¶åæˆ‘ä»¬å°†åœ¨Pywchä¸­æ‰§è¡Œåå‘ä¼ æ’­ï¼Œç„¶åæˆ‘ä»¬åŸºæœ¬ä¸Šä¼šä¸ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ä¸€èµ·é‡‡å–ä¸€æ­¥ï¼Œç°åœ¨æˆ‘ä»¬æ›´æ–°ï¼Œè¿™å°†ä»¥æ­£å¸¸æ–¹å¼åœ¨GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: Or a CPUã€‚But the thing that's kind of cool is that we're using a TPU so we have
    in a TPU we have eight cores on coLab and so what we want to do is we want to
    basically be able to do distributed training this wayã€‚And there's a nice kind
    of picture here of what's going on in the distributed trainingã€‚Whereã€‚
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…æ˜¯CPUã€‚ä½†æœ‰è¶£çš„æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨TPUï¼Œå› æ­¤åœ¨TPUä¸­ï¼Œæˆ‘ä»¬åœ¨coLabä¸Šæœ‰å…«ä¸ªæ ¸å¿ƒï¼Œå› æ­¤æˆ‘ä»¬æƒ³è¦åŸºæœ¬ä¸Šä»¥è¿™ç§æ–¹å¼è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚è¿™æœ‰ä¸€ä¸ªå…³äºåˆ†å¸ƒå¼è®­ç»ƒä¸­å‘ç”Ÿçš„äº‹æƒ…çš„ä¸é”™çš„ç¤ºæ„å›¾ã€‚
- en: I'll put this in the chatï¼Œ you can have a bunch of different machines and each
    machine has its own process so you can think of each process as being like its
    own little like a controller that is processing dataã€‚And so what we can do with
    with a TPU is we can think of it as having eight different processes and we need
    to communicate the sharing of weightsã€‚
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šæŠŠè¿™ä¸ªæ”¾åœ¨èŠå¤©ä¸­ï¼Œä½ å¯ä»¥æ‹¥æœ‰ä¸€å †ä¸åŒçš„æœºå™¨ï¼Œæ¯å°æœºå™¨éƒ½æœ‰è‡ªå·±çš„è¿›ç¨‹ï¼Œå› æ­¤ä½ å¯ä»¥æŠŠæ¯ä¸ªè¿›ç¨‹æƒ³è±¡æˆä¸€ä¸ªå¤„ç†æ•°æ®çš„å°æ§åˆ¶å™¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†TPUè§†ä¸ºæ‹¥æœ‰å…«ä¸ªä¸åŒè¿›ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œæƒé‡å…±äº«çš„é€šä¿¡ã€‚
- en: essentially how we do back propagation across these processesã€‚ and the accelerateerate
    library allows us to do that very simplyã€‚å—¯ã€‚Byï¼Œ basicallyã€‚We define something called
    an acceleratorã€‚And then we have to prepare the data loaderã€‚ the evaluation data
    loader and the model using this accelerator APIã€‚
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬å¦‚ä½•åœ¨è¿™äº›è¿›ç¨‹ä¸­è¿›è¡Œåå‘ä¼ æ’­ã€‚è€Œaccelerateåº“ä½¿æˆ‘ä»¬èƒ½å¤Ÿéå¸¸ç®€å•åœ°åšåˆ°è¿™ä¸€ç‚¹ã€‚å—¯ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ç§å«åšåŠ é€Ÿå™¨çš„ä¸œè¥¿ã€‚ç„¶åæˆ‘ä»¬å¿…é¡»ä½¿ç”¨è¿™ä¸ªåŠ é€Ÿå™¨APIå‡†å¤‡æ•°æ®åŠ è½½å™¨ã€è¯„ä¼°æ•°æ®åŠ è½½å™¨å’Œæ¨¡å‹ã€‚
- en: and this will automatically work out what hardware I'm runningã€‚ how many cores
    I have how should I distribute the dataã€‚ how I should you know basically copy
    the model across these nodes or these devices and then everything else after that
    is exactly the same as we saw beforeã€‚ so you don't really have to changeã€‚Anything
    in your training script is basically untouched it's just the way you prepare theã€‚
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è‡ªåŠ¨è®¡ç®—æˆ‘æ­£åœ¨è¿è¡Œçš„ç¡¬ä»¶ã€‚æˆ‘æœ‰å¤šå°‘æ ¸å¿ƒï¼Œåº”è¯¥å¦‚ä½•åˆ†é…æ•°æ®ã€‚æˆ‘åŸºæœ¬ä¸Šåº”è¯¥å¦‚ä½•åœ¨è¿™äº›èŠ‚ç‚¹æˆ–è®¾å¤‡ä¹‹é—´å¤åˆ¶æ¨¡å‹ï¼Œä¹‹åçš„å…¶ä»–ä¸€åˆ‡ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„å®Œå…¨ç›¸åŒã€‚æ‰€ä»¥ä½ å®é™…ä¸Šä¸éœ€è¦æ›´æ”¹ã€‚ä½ çš„è®­ç»ƒè„šæœ¬åŸºæœ¬ä¸Šä¿æŒä¸å˜ï¼Œåªæ˜¯å‡†å¤‡æ•°æ®çš„æ–¹å¼æœ‰æ‰€ä¸åŒã€‚
- en: The the training loopã€‚So if you run thisï¼Œ it won't work as it standsã€‚ what we
    need to do is we need to wrap all of the accelerator code inside a training functionã€‚And
    so if we do thatã€‚We just need toã€‚Put all thisã€‚Inside a training functionã€‚And one
    thing I discovered yesterday is we need to make sure that the data loaders we
    use hereã€‚è¯¶è¯¶ã€‚
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯ã€‚æ‰€ä»¥å¦‚æœä½ è¿™æ ·è¿è¡Œï¼Œè¿™ä¸ä¼šç”Ÿæ•ˆã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯å°†æ‰€æœ‰çš„åŠ é€Ÿå™¨ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªè®­ç»ƒå‡½æ•°å†…ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬è¿™æ ·åšã€‚æˆ‘ä»¬åªéœ€è¦ã€‚æŠŠè¿™ä¸€åˆ‡ã€‚æ”¾åœ¨ä¸€ä¸ªè®­ç»ƒå‡½æ•°é‡Œã€‚è€Œæˆ‘æ˜¨å¤©å‘ç°çš„ä¸€ä»¶äº‹æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨çš„æ•°æ®åŠ è½½å™¨ã€‚è¯¶è¯¶ã€‚
- en: ice's have a quick lookã€‚å—¯ã€‚And you are not global variablesã€‚That's how I think
    I just do thisã€‚Okayã€‚ğŸ˜Šã€‚So here I'm just initializing a model I'm initializing the
    optimizes we did before and here the magic of accelerate is basically going to
    do this distributed placement for us and so it's going to give us data loaders
    a model and an optr and then everything else runs as normal and this is where
    this question about tokenizing the data is important so what we've been doing
    previously is using dynamic padding so we don't use padding in the tokenizer directlyã€‚
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆå¿«é€Ÿçœ‹ä¸€ä¸‹ã€‚å—¯ã€‚ä½ æ²¡æœ‰å…¨å±€å˜é‡ã€‚è¿™å°±æ˜¯æˆ‘æƒ³æˆ‘è¯¥æ€ä¹ˆåšã€‚å¥½çš„ã€‚ğŸ˜Šã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘åªæ˜¯åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œåˆå§‹åŒ–æˆ‘ä»¬ä¹‹å‰åšçš„ä¼˜åŒ–å™¨ï¼Œè€Œè¿™é‡Œâ€œåŠ é€Ÿâ€çš„é­”åŠ›åŸºæœ¬ä¸Šä¼šä¸ºæˆ‘ä»¬åšè¿™ä¸ªåˆ†å¸ƒå¼æ”¾ç½®ï¼Œæ‰€ä»¥å®ƒå°†ç»™æˆ‘ä»¬æ•°æ®åŠ è½½å™¨ã€æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼Œç„¶åå…¶ä»–ä¸€åˆ‡æ­£å¸¸è¿è¡Œï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå…³äºæ ‡è®°åŒ–æ•°æ®çš„é—®é¢˜å¾ˆé‡è¦ï¼Œæˆ‘ä»¬ä¹‹å‰æ‰€åšçš„æ˜¯ä½¿ç”¨åŠ¨æ€å¡«å……ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ ‡è®°åŒ–å™¨ä¸­å¹¶ä¸ç›´æ¥ä½¿ç”¨å¡«å……ã€‚
- en: And in factï¼Œ hereï¼Œ if you look at when I tokenized the dataã€‚Where is itã€‚Here
    but in my tokenizing functionï¼Œ I just use truncationã€‚But because we're running
    on a TPUã€‚ I want to activate paddingï¼Œ so I need to put padding equals trueã€‚And
    I'm going to specify the max length that I want to pad not to the max length of
    the full model because it's too largeã€‚
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®ä¸Šï¼Œåœ¨è¿™é‡Œï¼Œå¦‚æœä½ æŸ¥çœ‹æˆ‘æ ‡è®°åŒ–æ•°æ®çš„æ—¶åˆ»ã€‚åœ¨å“ªé‡Œã€‚è¿™é‡Œåœ¨æˆ‘çš„æ ‡è®°åŒ–å‡½æ•°ä¸­ï¼Œæˆ‘åªä½¿ç”¨äº†æˆªæ–­ã€‚ä½†æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨TPUä¸Šè¿è¡Œã€‚æˆ‘æƒ³è¦æ¿€æ´»å¡«å……ï¼Œæ‰€ä»¥æˆ‘éœ€è¦è®¾ç½®padding
    equals trueã€‚æˆ‘å°†æŒ‡å®šæˆ‘æƒ³è¦å¡«å……çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œå› ä¸ºé‚£å¤ªå¤§äº†ã€‚
- en: I'm just going to specify kind of arbitrarily that it's 128 tokensã€‚And so once
    you've paded all your inputsï¼Œ we can then wrapã€‚Thereã€‚ we need to then also re
    instantsantiate the data loaders hereã€‚And nowï¼Œ we canã€‚Create a training functionã€‚And
    fingers crossedï¼Œ this will then allow us to launchã€‚A TPU trainingã€‚Soã€‚
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä»»æ„æŒ‡å®šä¸º128ä¸ªæ ‡è®°ã€‚æ‰€ä»¥ä¸€æ—¦ä½ å¡«å……äº†æ‰€æœ‰è¾“å…¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡ŒåŒ…è£…ã€‚æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œé‡æ–°å®ä¾‹åŒ–æ•°æ®åŠ è½½å™¨ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚åˆ›å»ºä¸€ä¸ªè®­ç»ƒå‡½æ•°ã€‚å¸Œæœ›è¿™æ ·èƒ½è®©æˆ‘ä»¬å¯åŠ¨ã€‚TPUè®­ç»ƒã€‚
- en: Okayï¼Œ so while that's workingï¼Œ let's have a quick look at the questionsã€‚å—¯ã€‚Okayã€‚
    so there's a question from I am Homeã€‚ There is a hugging past Titch session today
    and tomorrow all labels chapter 3ã€‚ The main difference is the time zonesã€‚ So we
    try to capture people who live on the sort of east and west of globeã€‚And there's
    also a TensorFlow session tonight with Matt Carriganã€‚
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œåœ¨è¿™è¿›è¡Œçš„åŒæ—¶ï¼Œæˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹é—®é¢˜ã€‚å—¯ã€‚å¥½çš„ã€‚é‚£ä¹ˆæœ‰ä¸€ä¸ªæ¥è‡ªâ€œI am Homeâ€çš„é—®é¢˜ã€‚ä»Šå¤©å’Œæ˜å¤©éƒ½æœ‰ä¸€ä¸ªå…³äºTitchçš„æ‹¥æŠ±ä¼šï¼Œæ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯ç¬¬ä¸‰ç« ã€‚ä¸»è¦çš„åŒºåˆ«åœ¨äºæ—¶åŒºã€‚æ‰€ä»¥æˆ‘ä»¬å°è¯•æ•æ‰å±…ä½åœ¨åœ°çƒä¸œè¥¿ä¸¤ä¾§çš„äººä»¬ã€‚ä»Šæ™šè¿˜æœ‰ä¸€ä¸ªä¸Matt
    Carrigançš„TensorFlowä¼šè®®ã€‚
- en: and so that's for all the TensorFficionadosã€‚å—¯ã€‚There's a question from Uns about
    regarding the domainspec cases for text classification before I train my model
    for my downstream task would it make sense to pre- fine tune with language modeling
    to better understand the context yes in general that's a very good thing to do
    the amount of boost you'll get in performance is very dependent on the domain
    shift so if I have Bt which was pre-train on Wikipedia and my text is kind of
    like say factual or kind of like Wikipedia then the boost I get from fine tuning
    but on my corpus won't be so much but if my corpus is like very different in domain
    like maybe it's like source code or something like this then then you will see
    typically a bigger boost in performanceã€‚
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸ºæ‰€æœ‰TensorFficionadoså‡†å¤‡çš„ã€‚å—¯ã€‚æœ‰ä¸€ä¸ªæ¥è‡ªUnsçš„é—®é¢˜ï¼Œå…³äºæ–‡æœ¬åˆ†ç±»çš„é¢†åŸŸç‰¹å®šæ¡ˆä¾‹ï¼Œåœ¨æˆ‘ä¸ºä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œé¢„å…ˆå¾®è°ƒè¯­è¨€å»ºæ¨¡ä»¥æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡æ˜¯å¦æœ‰æ„ä¹‰ï¼Ÿæ˜¯çš„ï¼Œé€šå¸¸æ¥è¯´ï¼Œè¿™æ˜¯éå¸¸å¥½çš„åšæ³•ï¼Œä½ åœ¨æ€§èƒ½ä¸Šè·å¾—çš„æå‡ä¸é¢†åŸŸè½¬ç§»å¯†åˆ‡ç›¸å…³ï¼Œå› æ­¤å¦‚æœæˆ‘æœ‰ä¸€ä¸ªåœ¨ç»´åŸºç™¾ç§‘ä¸Šé¢„è®­ç»ƒçš„Btï¼Œè€Œæˆ‘çš„æ–‡æœ¬åˆæœ‰ç‚¹åƒè¯´äº‹å®æ€§æˆ–ç±»ä¼¼äºç»´åŸºç™¾ç§‘çš„å†…å®¹ï¼Œé‚£ä¹ˆæˆ‘ä»å¾®è°ƒæˆ‘çš„è¯­æ–™åº“ä¸­è·å¾—çš„æå‡å°±ä¸ä¼šé‚£ä¹ˆå¤§ï¼Œä½†å¦‚æœæˆ‘çš„è¯­æ–™åº“åœ¨é¢†åŸŸä¸Šéå¸¸ä¸åŒï¼Œæ¯”å¦‚å¯èƒ½æ˜¯æºä»£ç ä¹‹ç±»çš„ï¼Œé‚£ä¹ˆé€šå¸¸ä¼šçœ‹åˆ°æ€§èƒ½çš„æ›´å¤§æå‡ã€‚
- en: So it's always worth doing and in general it's pretty easy to doã€‚There's a question
    about if I want to classify articles instead of sentences does Bert and friends
    still work or should I look at something like a long form that's a great question
    so Bert is basically limited or most of these sort of vanilla models are limited
    to 512 tokens so if your documents are longer than that you will have to do one
    or two things use a model which can process longer sequences like longform or
    big bird from memory I think they can process 4096 tokens so they're much much
    bigger like eight times bigger than BerRTã€‚
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ€»æ˜¯å€¼å¾—å»åšï¼Œå¹¶ä¸”é€šå¸¸æ¯”è¾ƒå®¹æ˜“åšåˆ°ã€‚æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¦‚æœæˆ‘æƒ³å¯¹æ–‡ç« è€Œä¸æ˜¯å¥å­è¿›è¡Œåˆ†ç±»ï¼ŒBertå’Œç›¸å…³æ¨¡å‹æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼Œæˆ–è€…æˆ‘æ˜¯å¦åº”è¯¥è€ƒè™‘åƒé•¿æ–‡æœ¬è¿™æ ·çš„æ¨¡å‹ï¼Œè¿™ä¸ªé—®é¢˜éå¸¸å¥½ã€‚å› æ­¤ï¼ŒBertåŸºæœ¬ä¸Šæ˜¯æœ‰é™çš„ï¼Œæˆ–è€…è¯´å¤§å¤šæ•°è¿™ç§æ™®é€šæ¨¡å‹çš„é™åˆ¶æ˜¯512ä¸ªæ ‡è®°ï¼Œå¦‚æœä½ çš„æ–‡æ¡£é•¿åº¦è¶…è¿‡è¿™ä¸ªï¼Œä½ å°†ä¸å¾—ä¸åšä¸€ä¸¤ä»¶äº‹ï¼Œä½¿ç”¨ä¸€ä¸ªå¯ä»¥å¤„ç†æ›´é•¿åºåˆ—çš„æ¨¡å‹ï¼Œæ¯”å¦‚longformæˆ–big
    birdï¼Œè®°å¾—å®ƒä»¬å¯ä»¥å¤„ç†4096ä¸ªæ ‡è®°ï¼Œæ‰€ä»¥å®ƒä»¬è¦å¤§å¾—å¤šï¼Œå¤§çº¦æ˜¯Bertçš„å…«å€ã€‚
- en: But if that's not like sufficientï¼Œ you'll then have to basically chunk or kind
    of split your document into passages and then you'll have to feed those passages
    into the model and then do some aggregation to work out how you build up a kind
    of representation for the full document that you can then feed into your classification
    headã€‚
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœè¿™è¿˜ä¸å¤Ÿï¼Œä½ å°±éœ€è¦å°†æ–‡æ¡£åŸºæœ¬ä¸Šåˆ†å—æˆ–æ‹†åˆ†æˆå¤šä¸ªæ®µè½ï¼Œç„¶åå°†è¿™äº›æ®µè½è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œå†è¿›è¡Œä¸€äº›èšåˆï¼Œä»¥ä¾¿æ„å»ºå‡ºä¸€ä¸ªå¯ä»¥è¾“å…¥åˆ°åˆ†ç±»å¤´çš„å®Œæ•´æ–‡æ¡£çš„è¡¨ç¤ºã€‚
- en: There's a question about to improve the baseline modelï¼Œ take a Roberta modelï¼Œ
    train itã€‚Yesã€‚ that's rightï¼Œ exactlyã€‚I am Holmes asksï¼Œ what is the difference between
    using P Toch's default Adam W and the transformformers oneï¼Ÿ
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯å…³äºå¦‚ä½•æ”¹è¿›åŸºçº¿æ¨¡å‹ï¼Œä½¿ç”¨Robertaæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚æ˜¯çš„ï¼Œæ²¡é”™ï¼Œæ­£æ˜¯è¿™æ ·ã€‚æˆ‘æ˜¯Holmesï¼Œé—®é“ä½¿ç”¨P Tochçš„é»˜è®¤Adam Wå’Œtransformersçš„ç‰ˆæœ¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- en: That's a good question off the top of my head I don't know my guess is that
    there are some I mean there's a long history of Adam and not being implemented
    correctly soã€‚ğŸ˜Šï¼ŒYeahï¼Œ off the top of my headï¼Œ I have a suspicion that perhaps the
    transformers one is is correct and the point which one maybe isn't butã€‚
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼ŒæŒ‰ç†è¯´æˆ‘ä¸å¤ªç¡®å®šï¼Œæˆ‘çŒœæ˜¯æœ‰ä¸€äº›ï¼Œæˆ‘çš„æ„æ€æ˜¯Adamæœ‰å¾ˆé•¿çš„å†å²ï¼Œå¯èƒ½æ²¡æœ‰è¢«æ­£ç¡®å®ç°ã€‚æ‰€ä»¥ï¼ŒğŸ˜Šï¼Œå¯¹ï¼Œæˆ‘çš„ç›´è§‰æ˜¯transformersçš„ç‰ˆæœ¬å¯èƒ½æ˜¯æ­£ç¡®çš„ï¼Œè€ŒP
    Tochçš„é‚£ä¸ªå¯èƒ½ä¸å¤ªå‡†ç¡®ï¼Œä½†ã€‚
- en: Maybe do some experimentsï¼Œ find outï¼Œ sorryï¼Œ I don't know that off top of my
    headã€‚Okayï¼Œ coolã€‚ So those are more or less the questionsã€‚ Let's see Something
    went wrongã€‚ And this is now where I get to do some debuggingã€‚ So let's have a
    lookã€‚ I got an error and it says you should probably activate truncation or padding
    with padding equals trueã€‚
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸åšä¸€äº›å®éªŒï¼Œæ‰¾å‡ºç­”æ¡ˆï¼ŒæŠ±æ­‰ï¼Œæˆ‘å½“ä¸‹ä¸å¤ªæ¸…æ¥šã€‚å¥½çš„ï¼Œé…·ã€‚è¿™äº›é—®é¢˜å¤§è‡´ä¸Šå°±æ˜¯è¿™äº›ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å‡ºäº†ä»€ä¹ˆé—®é¢˜ã€‚è¿™æ—¶æˆ‘å¯ä»¥è¿›è¡Œä¸€äº›è°ƒè¯•ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘æ”¶åˆ°äº†ä¸€ä¸ªé”™è¯¯ï¼Œæç¤ºä½ å¯èƒ½åº”è¯¥æ¿€æ´»æˆªæ–­æˆ–å¡«å……ï¼Œå¹¶å°†å¡«å……è®¾ç½®ä¸ºtrueã€‚
- en: To have batched tenses with the same lengthã€‚Okayï¼Œ so I clearly did something
    wrong when Iã€‚D find my data loaderã€‚ So let's have a lookã€‚At what went wrongã€‚Soã€‚Iã€‚Firstï¼Œ
    tokenizedã€‚My daughterã€‚Which is goodã€‚I then need to set the formatã€‚å•Šã€‚I want to
    get rid of this clay functionã€‚This co function is going to be the thing that gave
    us problemsã€‚Okayï¼Œ so nowã€‚Should beæˆ‘ toã€‚
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¹å¤„ç†å…·æœ‰ç›¸åŒé•¿åº¦çš„å¼ é‡ã€‚å¥½å§ï¼Œæ‰€ä»¥æˆ‘æ˜¾ç„¶åœ¨æ•°æ®åŠ è½½å™¨æ—¶åšé”™äº†ä»€ä¹ˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹å‡ºäº†ä»€ä¹ˆé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘å¯¹æˆ‘çš„æ•°æ®è¿›è¡Œäº†æ ‡è®°ï¼Œè¿™æ˜¯å¥½çš„ã€‚ç„¶åæˆ‘éœ€è¦è®¾ç½®æ ¼å¼ã€‚å•Šã€‚æˆ‘æƒ³æ‘†è„±è¿™ä¸ªclayå‡½æ•°ã€‚è¿™ä¸ªcoå‡½æ•°å°†æ˜¯å¯¼è‡´æˆ‘ä»¬é—®é¢˜çš„æ‰€åœ¨ã€‚å¥½å§ï¼Œç°åœ¨åº”è¯¥æ˜¯æˆ‘ã€‚
- en: Wunch the TPU trainingã€‚Okayï¼Œ so this is now launching on these eight TPU coresã€‚And
    fingers crossedã€‚ I didn't make any mistakesã€‚Greatï¼Œ thanks for the feedbackã€‚ I
    am Homeme and DK creativezativeã€‚ I'm hoping you're enjoying it and thanks for
    the questionsã€‚ğŸ˜Šï¼ŒSo this looks like it is trainingã€‚Andã€‚Yesï¼Œ greatã€‚ Oh noï¼Œ no problemã€‚Okayï¼Œ
    soã€‚What is itï¼Œ Stack expects tensor to be equal size but 96ã€‚ğŸ˜”ã€‚
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: å¯åŠ¨TPUè®­ç»ƒã€‚å¥½çš„ï¼Œç°åœ¨åœ¨è¿™å…«ä¸ªTPUæ ¸å¿ƒä¸Šå¯åŠ¨ã€‚å¸Œæœ›ä¸ä¼šå‡ºé”™ã€‚è°¢è°¢ä½ çš„åé¦ˆã€‚æˆ‘æ˜¯Homemeå’ŒDK creativezativeã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªï¼Œè°¢è°¢ä½ çš„æé—®ã€‚ğŸ˜Šï¼Œæ‰€ä»¥è¿™çœ‹èµ·æ¥åœ¨è®­ç»ƒä¸­ã€‚æ˜¯çš„ï¼Œå¤ªå¥½äº†ã€‚å“¦ä¸ï¼Œæ²¡é—®é¢˜ã€‚å¥½çš„ï¼Œé‚£ä¹ˆã€‚StackæœŸæœ›å¼ é‡å¤§å°ç›¸ç­‰ï¼Œä½†æœ‰96ã€‚ğŸ˜”ã€‚
- en: Okayï¼Œ soã€‚å—¯ã€‚Okayï¼Œ so here I'm going to do someï¼Œ I know we're running over timeã€‚
    so I apologizeã€‚ I'm going to seeï¼Œ let's see if I can debug thisã€‚Fast and if notã€‚
    then I'll just fix the notebook laterã€‚ Okayï¼Œ so so basically I'm running into
    an error and this is a good way of seeing how TPs are a little bit more fun than
    all It's saying the stack expects each tensor to be equal size butve got 96 at
    entry0 and 100 entry 1 So this is suggesting to me that somehow I've screwed up
    the tokenization So instead of having everything with the same shape there there's
    someã€‚
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥ã€‚å—¯ã€‚å¥½çš„ï¼Œåœ¨è¿™é‡Œæˆ‘å°†è¿›è¡Œä¸€äº›ï¼Œæˆ‘çŸ¥é“æˆ‘ä»¬è¶…æ—¶äº†ã€‚å¯¹æ­¤æˆ‘æ„Ÿåˆ°æŠ±æ­‰ã€‚æˆ‘ä¼šçœ‹çœ‹ï¼Œçœ‹çœ‹èƒ½å¦å¿«é€Ÿè°ƒè¯•ã€‚å¦‚æœä¸èƒ½ã€‚é‚£æˆ‘ç¨åä¼šä¿®å¤ç¬”è®°æœ¬ã€‚å¥½çš„ï¼ŒåŸºæœ¬ä¸Šæˆ‘é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹å¼æ¥çœ‹åˆ°TPUæ¯”å…¶ä»–çš„æ›´æœ‰è¶£ã€‚å®ƒè¯´å †æ ˆæœŸæœ›æ¯ä¸ªå¼ é‡å¤§å°ç›¸ç­‰ï¼Œä½†åœ¨entry0æœ‰96ï¼Œåœ¨entry1æœ‰100ã€‚æ‰€ä»¥è¿™è®©æˆ‘è§‰å¾—æˆ‘åœ¨åˆ†è¯æ—¶æé”™äº†ã€‚å› æ­¤ï¼ŒåŸæœ¬åº”è¯¥æ˜¯ç›¸åŒå½¢çŠ¶çš„ï¼Œç°åœ¨å´å‡ºç°äº†ä¸€äº›é—®é¢˜ã€‚
- en: Like something's gone wrongã€‚ So let's have a look atã€‚My data loaderã€‚Okayï¼Œ soã€‚So
    let's have a look at my tokenization firstã€‚When I did tokenizationã€‚I set padding
    equals true and max lengthã€‚And so nowã€‚When I look at my tokenized data setsã€‚Let's
    have a look atã€‚è¿™è¿™è¿™ã€‚So maybe let's have a look at the input Isã€‚So the input IDsã€‚
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½åƒå‡ºé”™äº†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘çš„æ•°æ®åŠ è½½å™¨ã€‚å¥½çš„ï¼Œæ‰€ä»¥ã€‚æˆ‘ä»¬å…ˆçœ‹çœ‹æˆ‘çš„åˆ†è¯ã€‚å½“æˆ‘è¿›è¡Œåˆ†è¯æ—¶ã€‚æˆ‘è®¾ç½®äº†paddingä¸ºtrueå’Œæœ€å¤§é•¿åº¦ã€‚æ‰€ä»¥ç°åœ¨ã€‚å½“æˆ‘æŸ¥çœ‹æˆ‘çš„åˆ†è¯æ•°æ®é›†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚è¿™è¿™è¿™ã€‚æ‰€ä»¥ä¹Ÿè®¸è®©æˆ‘ä»¬çœ‹çœ‹è¾“å…¥æ˜¯ã€‚è¾“å…¥IDã€‚
- en: we want these to all be the same shapeã€‚I'm going to just takeã€‚å—¯ã€‚Let's just takeï¼Œ
    say 10 of themã€‚And then what I'm going to do is I'm just going to say tensort
    sizeã€‚For tensor in thisã€‚So let's see what we getã€‚Okayï¼Œ so this is telling me that
    all the tensors have the same sizeã€‚ which is goodã€‚å—¯ã€‚Let's just do a sanitity checkï¼Œ
    if Iã€‚Go from the endã€‚Yeahï¼Œ thats goodã€‚Okayã€‚
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›å®ƒä»¬éƒ½æ˜¯ç›¸åŒå½¢çŠ¶ã€‚æˆ‘ä¼šç›´æ¥å–ã€‚å—¯ã€‚æˆ‘ä»¬å°±å–ï¼Œæ¯”å¦‚è¯´10ä¸ªã€‚ç„¶åæˆ‘å°†è¯´å¼ é‡å¤§å°ã€‚å¯¹äºè¿™ä¸ªå¼ é‡ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹å¾—åˆ°çš„ç»“æœã€‚å¥½çš„ï¼Œè¿™å‘Šè¯‰æˆ‘æ‰€æœ‰çš„å¼ é‡éƒ½æœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™å¾ˆå¥½ã€‚å—¯ã€‚æˆ‘ä»¬å†åšä¸ªå®Œæ•´æ€§æ£€æŸ¥ï¼Œå¦‚æœæˆ‘ã€‚ä»æœ«å°¾å¼€å§‹ã€‚æ˜¯çš„ï¼Œè¿™å¾ˆå¥½ã€‚å¥½çš„ã€‚
- en: so my tensors are all the same sizeã€‚And should just do this againã€‚And let's
    seeã€‚Nowã€‚What is complaining on about thisï¼ŸğŸ˜”ï¼ŒInvalid typeã€‚å“ã€‚Because I need toã€‚Return
    tenses equalsã€‚Quite tochã€‚This is so annoyingï¼Œ okayï¼Œ let's not do thatã€‚The too
    pettingã€‚And then let's remove the columnã€‚ Let's set the formatï¼Œ goodã€‚Okayã€‚ğŸ˜Šï¼ŒNow
    let's see what we get hereã€‚Okayã€‚
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘çš„å¼ é‡éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚åº”è¯¥å†åšä¸€æ¬¡ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚ç°åœ¨ã€‚å…³äºè¿™ç‚¹æœ‰ä»€ä¹ˆæŠ±æ€¨ï¼ŸğŸ˜”ï¼Œæ— æ•ˆç±»å‹ã€‚å“ã€‚å› ä¸ºæˆ‘éœ€è¦ã€‚è¿”å›å¼ é‡ç­‰äºã€‚ç›¸å½“æ¼äººã€‚å¥½çš„ï¼Œåˆ«è¿™æ ·åšã€‚å¤ªè¿‡éº»çƒ¦ã€‚ç„¶åæˆ‘ä»¬å»æ‰è¿™ä¸€åˆ—ã€‚è®¾ç½®æ ¼å¼ï¼Œå¥½çš„ã€‚ğŸ˜Šï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™é‡Œå¾—åˆ°çš„æ˜¯ä»€ä¹ˆã€‚å¥½çš„ã€‚
- en: so we can see there's already a problemã€‚Interestingï¼Œ soã€‚å—¯ã€‚ğŸ˜Šï¼Œå‘µå‘µã€‚ğŸ˜Šï¼ŒCoolï¼Œ all rightã€‚
    so we're going to try and deg this and in factï¼Œ if someone sees what I'm doing
    wrongã€‚ this will be great so so here's a problem The problem is I've created a
    data loaderã€‚ğŸ˜Šï¼ŒHereã€‚ train data loaderï¼Œ and it's complaining that when I try to
    put the tensesors togetherã€‚
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å·²ç»çœ‹åˆ°æœ‰ä¸ªé—®é¢˜ã€‚æœ‰è¶£ï¼Œæ‰€ä»¥ã€‚å—¯ã€‚ğŸ˜Šï¼Œå‘µå‘µã€‚ğŸ˜Šï¼Œå¾ˆå¥½ï¼Œå¥½çš„ã€‚æ‰€ä»¥æˆ‘ä»¬è¦å°è¯•è°ƒè¯•è¿™ä¸ªï¼Œå®é™…ä¸Šï¼Œå¦‚æœæœ‰äººçœ‹åˆ°æˆ‘åšé”™äº†ä»€ä¹ˆã€‚è¿™å°†å¾ˆæ£’ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œé—®é¢˜æ˜¯æˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ã€‚ğŸ˜Šï¼Œåœ¨è¿™é‡Œã€‚è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œå®ƒæŠ±æ€¨å½“æˆ‘å°è¯•æŠŠå¼ é‡æ”¾åœ¨ä¸€èµ·æ—¶ã€‚
- en: they're somehow not of the same shapeã€‚Soã€‚Let's see what have I done wrongï¼ŸOkayï¼Œ
    soã€‚Let's do thatã€‚ğŸ˜”ã€‚Soï¼Œ let'sã€‚Let's look at the first batchã€‚And let's look atã€‚Okayï¼Œ
    let's look at the input Iã€‚And then let's look at the size of each tensorã€‚In hereã€‚å•Šå“ˆã€‚ğŸ˜Šï¼ŒSoã€‚It
    seems that my tokenized data sets are notã€‚The ones I was looking forã€‚
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬çš„å½¢çŠ¶ä¼¼ä¹ä¸ä¸€æ ·ã€‚æ‰€ä»¥ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘åšé”™äº†ä»€ä¹ˆï¼Ÿå¥½çš„ï¼Œæ‰€ä»¥ã€‚æˆ‘ä»¬æ¥è¯•è¯•ã€‚ğŸ˜”ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬ã€‚çœ‹çœ‹ç¬¬ä¸€æ‰¹ã€‚ç„¶åæˆ‘ä»¬æ¥çœ‹ã€‚å¥½çš„ï¼Œçœ‹çœ‹æˆ‘è¾“å…¥çš„å†…å®¹ã€‚æˆ‘ï¼Œç„¶åçœ‹çœ‹æ¯ä¸ªå¼ é‡çš„å¤§å°ã€‚åœ¨è¿™é‡Œã€‚å•Šå“ˆã€‚ğŸ˜Šï¼Œæ‰€ä»¥ã€‚çœ‹æ¥æˆ‘çš„åˆ†è¯æ•°æ®é›†ä¸æ˜¯ã€‚æˆ‘æƒ³è¦çš„é‚£äº›ã€‚
- en: So let's have a look what's going on if I look atã€‚So we saw before that these
    were meant to be 89 in sizeã€‚ so let's have a look at thisã€‚And then we go T dot
    size or T in hereã€‚å—¯ã€‚Okayã€‚Soã€‚You can see that in my tokenized data setsï¼Œ all the
    inputs have size 89ã€‚Butã€‚For some reasonã€‚ in the batch of the training loaderï¼Œ
    everything is 71ã€‚å—¯ã€‚ğŸ˜Šï¼ŒSo let's seeã€‚Wchã€‚
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹å‘ç”Ÿäº†ä»€ä¹ˆã€‚å¦‚æœæˆ‘æŸ¥çœ‹çš„è¯ã€‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°è¿™äº›åº”è¯¥æ˜¯89çš„å¤§å°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬è¿™é‡Œä½¿ç”¨T.dot.sizeæˆ–Tã€‚å—¯ã€‚å¥½çš„ã€‚æ‰€ä»¥ã€‚ä½ å¯ä»¥çœ‹åˆ°åœ¨æˆ‘çš„åˆ†è¯æ•°æ®é›†ä¸­ï¼Œæ‰€æœ‰è¾“å…¥çš„å¤§å°éƒ½æ˜¯89ã€‚ä½†æ˜¯ã€‚å‡ºäºæŸç§åŸå› ã€‚åœ¨è®­ç»ƒåŠ è½½å™¨çš„æ‰¹æ¬¡ä¸­ï¼Œä¸€åˆ‡éƒ½æ˜¯71ã€‚å—¯ã€‚ğŸ˜Šï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹ã€‚Wchã€‚
- en: Why would that be differentï¼Œ Why would the batch be differentã€‚å—¯ã€‚å¯¹å¯¹éƒ½ã€‚å—¯ã€‚ğŸ˜Šã€‚Don't
    tell me I need this clay functionã€‚ğŸ˜”ï¼ŒNoï¼Œ that's not what I want because nowã€‚ğŸ˜”ã€‚They're
    going to be dynamically powderedã€‚å—¯ã€‚ğŸ˜Šï¼ŒThat's an interesting thing I'm going to
    have a lookã€‚ã†ã€‚Soï¼Œ let's see if we haveã€‚So just to just to tell you what I've doneã€‚
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä¼šä¸åŒï¼Œä¸ºä»€ä¹ˆæ‰¹æ¬¡ä¼šä¸åŒã€‚å—¯ã€‚å¯¹å¯¹éƒ½ã€‚å—¯ã€‚ğŸ˜Šã€‚åˆ«å‘Šè¯‰æˆ‘æˆ‘éœ€è¦è¿™ä¸ªåˆå¹¶å‡½æ•°ã€‚ğŸ˜”ï¼Œä¸ï¼Œè¿™ä¸æ˜¯æˆ‘æƒ³è¦çš„ï¼Œå› ä¸ºç°åœ¨ã€‚ğŸ˜”ã€‚å®ƒä»¬å°†åŠ¨æ€å¡«å……ã€‚å—¯ã€‚ğŸ˜Šï¼Œè¿™å¾ˆæœ‰è¶£ï¼Œæˆ‘è¦çœ‹çœ‹ã€‚ã†ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æœ‰æ²¡æœ‰ã€‚å°±åªæ˜¯å‘Šè¯‰ä½ æˆ‘åšäº†ä»€ä¹ˆã€‚
- en: so I've reintroduced a collate function into theã€‚Into the data loadã€‚Hereã€‚I was
    pretty sure we don't want to have that because we want to have fixed sizesã€‚Butï¼Œ
    let's seeã€‚If this hasã€‚Sort of magically debug the problem Okayã€‚ so for reasons
    I don't understand adding a collate function to the data loader seems to have
    been importantã€‚
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘é‡æ–°å¼•å…¥äº†ä¸€ä¸ªåˆå¹¶å‡½æ•°åˆ°æ•°æ®åŠ è½½ä¸­ã€‚åœ¨è¿™é‡Œã€‚æˆ‘ç›¸å½“ç¡®å®šæˆ‘ä»¬ä¸æƒ³è¿™æ ·åšï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦å›ºå®šçš„å¤§å°ã€‚ä½†æ˜¯ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ã€‚è¿™æ˜¯å¦æœ‰æŸç§é­”åŠ›æ¥è°ƒè¯•è¿™ä¸ªé—®é¢˜ã€‚å¥½çš„ï¼Œæ‰€ä»¥å‡ºäºæˆ‘ä¸æ˜ç™½çš„åŸå› ï¼Œå‘æ•°æ®åŠ è½½å™¨æ·»åŠ åˆå¹¶å‡½æ•°ä¼¼ä¹æ˜¯é‡è¦çš„ã€‚
- en: For letting us do the trainingã€‚ And you can see here I'm doing on one TPU coreï¼Œ
    I'm runningã€‚The trainingã€‚And what should all theseã€‚All these messages rightã€‚ you
    can see here these are the warning messages you often get when you instantiate
    a model and so what accelator has done is it's copied the model into the eight
    TPU core but done so in a way that the initialization is the same so we've got
    the same model at the startã€‚
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿›è¡Œè®­ç»ƒã€‚ä½ å¯ä»¥çœ‹åˆ°åœ¨ä¸€ä¸ªTPUæ ¸å¿ƒä¸Šï¼Œæˆ‘æ­£åœ¨è¿è¡Œã€‚è®­ç»ƒã€‚æ‰€æœ‰è¿™äº›æ¶ˆæ¯å¯¹å§ã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™äº›æ˜¯ä½ åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶ç»å¸¸æ”¶åˆ°çš„è­¦å‘Šæ¶ˆæ¯ï¼Œæ‰€ä»¥åŠ é€Ÿå™¨æ‰€åšçš„å°±æ˜¯å°†æ¨¡å‹å¤åˆ¶åˆ°å…«ä¸ªTPUæ ¸å¿ƒï¼Œä½†ä»¥ç›¸åŒçš„æ–¹å¼è¿›è¡Œåˆå§‹åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸€å¼€å§‹å¾—åˆ°äº†ç›¸åŒçš„æ¨¡å‹ã€‚
- en: And then it should instantiateã€‚Another seven parallel trainingsã€‚ so now we're
    training the model copied in eight pieces across eight cores and each core will
    get its own batch of data and then back propagation will be synchronized so that
    then when we update the model we update based on the losses computed for every
    deviceã€‚
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒåº”è¯¥å®ä¾‹åŒ–å¦å¤–ä¸ƒä¸ªå¹¶è¡Œè®­ç»ƒã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬åœ¨å…«ä¸ªæ ¸å¿ƒä¸Šè®­ç»ƒå¤åˆ¶çš„æ¨¡å‹ï¼Œæ¯ä¸ªæ ¸å¿ƒéƒ½ä¼šè·å¾—è‡ªå·±çš„æ•°æ®æ‰¹æ¬¡ï¼Œç„¶ååå‘ä¼ æ’­å°†è¢«åŒæ­¥ï¼Œè¿™æ ·å½“æˆ‘ä»¬æ›´æ–°æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬æ˜¯åŸºäºæ¯ä¸ªè®¾å¤‡è®¡ç®—çš„æŸå¤±è¿›è¡Œæ›´æ–°ã€‚
- en: å—¯ã€‚ğŸ˜Šï¼ŒSo this is yeahï¼Œ obviously a little bit of a hack I'm a bit unhappy that
    it's everything I said about needing to use padding seems to have been mixed up
    with the colade function but yeah that's the joy of life coding maybe you can
    dive into this yourself and try to understand exactly you know how one is to set
    this up there's something I'm still kind of missing in my head at the momentã€‚
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚ğŸ˜Šï¼Œæ‰€ä»¥è¿™æ˜¾ç„¶æœ‰ç‚¹å°æŠ€å·§ï¼Œæˆ‘æœ‰ç‚¹ä¸é«˜å…´çš„æ˜¯ï¼Œæˆ‘æ‰€è¯´çš„éœ€è¦ä½¿ç”¨å¡«å……çš„å†…å®¹ä¼¼ä¹å’Œåˆå¹¶å‡½æ•°æ··æ·†åœ¨ä¸€èµ·ï¼Œä½†è¿™å°±æ˜¯ç”Ÿæ´»ç¼–ç çš„ä¹è¶£ï¼Œä¹Ÿè®¸ä½ å¯ä»¥è‡ªå·±æ·±å…¥äº†è§£å¹¶å°è¯•ç†è§£ç©¶ç«Ÿå¦‚ä½•è®¾ç½®è¿™ä¸ªï¼Œæˆ‘è„‘ä¸­ä»ç„¶æœ‰ä¸€äº›ä¸œè¥¿ç¼ºå¤±ã€‚
- en: ğŸ˜Šï¼ŒBut neverthelessusï¼Œ that's someã€‚Let's say training on a TPU and one thing
    that I would recommend you do as like a homework problem is try to work through
    these training notebooks on a new data setã€‚And so you can find many options on
    the hubï¼Œ so if you go to data setsã€‚
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä½†æ˜¯æ— è®ºå¦‚ä½•ï¼Œé‚£æ˜¯ä¸€äº›ã€‚æˆ‘ä»¬å¯ä»¥è¯´åœ¨TPUä¸Šè®­ç»ƒï¼Œæˆ‘æ¨èä½ åšçš„ä¸€ä¸ªå®¶åº­ä½œä¸šæ˜¯å°è¯•åœ¨æ–°çš„æ•°æ®é›†ä¸Šå¤„ç†è¿™äº›è®­ç»ƒç¬”è®°æœ¬ã€‚æ‰€ä»¥ä½ å¯ä»¥åœ¨ä¸­å¿ƒæ‰¾åˆ°è®¸å¤šé€‰é¡¹ï¼Œå¦‚æœä½ å»æ•°æ®é›†ã€‚
- en: you can look at text classification and for most of the time this will cover
    all the things you have everything you need now to study thisã€‚And the other thing
    that you canã€‚Do isï¼Œ for exampleï¼Œ we're doing paraphrase detection rightã€‚ we're
    checking if two sentences of the paraphrases of another another example of thisã€‚Would
    beã€‚The corera data setã€‚And this is a very common problem on things like stack
    overflow or Qra where you've got people asking questions and sometimes the question
    are duplicate and you often rely on the community to flag question as duplicate
    or not and so this would be a nice data set to use which would be very similar
    to what we've done but give you a kind of different flavor of dealing with different
    columns and different inputs so I'm going to put that in the chat as a suggestionã€‚
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹çœ‹æ–‡æœ¬åˆ†ç±»ï¼Œå¤§éƒ¨åˆ†æ—¶é—´è¿™å°†æ¶µç›–ä½ ç°åœ¨å­¦ä¹ è¿™ä¸€åˆ‡æ‰€éœ€çš„æ‰€æœ‰å†…å®¹ã€‚è€Œä½ å¯ä»¥åšçš„å¦ä¸€ä»¶äº‹æ˜¯ï¼Œæ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬æ­£åœ¨è¿›è¡ŒåŒä¹‰å¥æ£€æµ‹ã€‚æˆ‘ä»¬æ­£åœ¨æ£€æŸ¥ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯å½¼æ­¤çš„åŒä¹‰å¥ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯Coreraæ•°æ®é›†ã€‚è¿™æ˜¯åƒStack
    Overflowæˆ–Q&Aç½‘ç«™ä¸Šéå¸¸å¸¸è§çš„é—®é¢˜ï¼Œå› ä¸ºæœ‰äººåœ¨æé—®ï¼Œæœ‰æ—¶é—®é¢˜æ˜¯é‡å¤çš„ï¼Œä½ é€šå¸¸ä¾èµ–ç¤¾åŒºæ ‡è®°é—®é¢˜æ˜¯å¦é‡å¤ã€‚å› æ­¤ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªä¸é”™çš„æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ï¼Œä¸æˆ‘ä»¬åšçš„éå¸¸ç›¸ä¼¼ï¼Œä½†ä¼šè®©ä½ ä½“éªŒåˆ°å¤„ç†ä¸åŒåˆ—å’Œä¸åŒè¾“å…¥çš„ä¸åŒé£æ ¼ï¼Œæ‰€ä»¥æˆ‘ä¼šæŠŠè¿™ä¸ªå»ºè®®æ”¾åœ¨èŠå¤©ä¸­ã€‚
- en: And yeahï¼Œ I would also recommend see if you can get the TPU training workingã€‚
    I think TPUs are a very exciting development because they offer in principle eight
    times as much speed up in principleã€‚ but reality is around three timesã€‚Over conventional
    say GPU training and they're free on co labsã€‚ so now you can use accelerate to
    train faster than normal and do really cool distributed stuffã€‚
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”æˆ‘ä¹Ÿå»ºè®®ä½ çœ‹çœ‹èƒ½å¦è®©TPUè®­ç»ƒè¿è¡Œèµ·æ¥ã€‚æˆ‘è®¤ä¸ºTPUæ˜¯ä¸€ä¸ªéå¸¸ä»¤äººå…´å¥‹çš„å‘å±•ï¼Œå› ä¸ºå®ƒä»¬åŸåˆ™ä¸Šæä¾›å…«å€çš„åŠ é€Ÿï¼Œä½†å®é™…ä¸Šå¤§çº¦æ˜¯ä¸‰å€ã€‚ä¸ä¼ ç»Ÿçš„GPUè®­ç»ƒç›¸æ¯”ï¼Œå®ƒä»¬åœ¨Colabä¸Šæ˜¯å…è´¹çš„ã€‚å› æ­¤ï¼Œç°åœ¨ä½ å¯ä»¥ä½¿ç”¨TPUæ¥æ¯”æ­£å¸¸æƒ…å†µä¸‹æ›´å¿«åœ°è®­ç»ƒï¼Œå¹¶è¿›è¡Œéå¸¸é…·çš„åˆ†å¸ƒå¼æ“ä½œã€‚
- en: All right so that was a bit longer and sorry for that that's my fault for not
    being able to deog my code so I hope you enjoyed it and tonight there's Matt doing
    TensorFlow if you're into that and I think tomorrow is then Sil doing the next
    round of thisã€‚
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™æœ‰ç‚¹é•¿ï¼Œå¯¹æ­¤æˆ‘æ„Ÿåˆ°æŠ±æ­‰ï¼Œè¿™éƒ½æ˜¯æˆ‘æ²¡æœ‰è°ƒè¯•ä»£ç é€ æˆçš„ï¼Œå¸Œæœ›ä½ å–œæ¬¢ä»Šæ™šçš„å†…å®¹ï¼ŒMattå°†åšTensorFlowï¼Œå¦‚æœä½ å¯¹æ­¤æ„Ÿå…´è¶£ï¼Œè€Œæˆ‘è®¤ä¸ºæ˜å¤©Silå°†è¿›è¡Œä¸‹ä¸€è½®çš„è®²è§£ã€‚
- en: ğŸ˜Šï¼ŒAnd next weekï¼Œ we're going to beã€‚Diving into the sort of more advanced parts
    of the libraryã€‚ So what we're going to be doing next week is how do we share models
    in the hubã€‚ How do we push everything to the hubï¼Œ How do we share metricsã€‚ And
    so this is going to be the kind of icing on the cake for everything we don'tã€‚
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä¸‹å‘¨æˆ‘ä»¬å°†æ·±å…¥åº“çš„æ›´é«˜çº§éƒ¨åˆ†ã€‚æˆ‘ä»¬ä¸‹å‘¨è¦åšçš„äº‹æƒ…æ˜¯å¦‚ä½•åœ¨ä¸­å¿ƒå…±äº«æ¨¡å‹ï¼Œå¦‚ä½•å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ°ä¸­å¿ƒï¼Œä»¥åŠå¦‚ä½•å…±äº«æŒ‡æ ‡ã€‚å› æ­¤ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬æ‰€åšçš„ä¸€åˆ‡çš„ç‚¹ç›ä¹‹ç¬”ã€‚
- en: So I'm not back for chapter fourï¼Œ but Omar is and Omar is awesomeã€‚ he's in the
    chat right now and so you should definitely comeã€‚ I'll actually be in the chat
    so it'll be like a body swapã€‚And so I hope you guys see you thenã€‚ğŸ˜Šã€‚Coolï¼Œ so with
    that I'm going to stop recording and stop the stream so if you have any questionsã€‚
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è¿˜æ²¡å›åˆ°ç¬¬å››ç« ï¼Œä½†Omarå›æ¥äº†ï¼ŒOmarå¾ˆæ£’ã€‚ä»–ç°åœ¨åœ¨èŠå¤©ä¸­ï¼Œæ‰€ä»¥ä½ ç»å¯¹åº”è¯¥æ¥ã€‚æˆ‘å®é™…ä¸Šä¹Ÿä¼šåœ¨èŠå¤©ä¸­ï¼Œæ‰€ä»¥è¿™å°±åƒæ¢èº«ä½“ä¸€æ ·ã€‚å¸Œæœ›ä½ ä»¬åˆ°æ—¶å€™èƒ½è§åˆ°æˆ‘ã€‚ğŸ˜Šã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ç°åœ¨è¦åœæ­¢å½•åˆ¶å’Œç›´æ’­ï¼Œå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ã€‚
- en: just put them in the forum and see you next timeã€‚![](img/a1136557282a500704797139dc43d7b8_46.png)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: æŠŠå®ƒä»¬æ”¾åœ¨è®ºå›ä¸Šï¼Œæˆ‘ä»¬ä¸‹æ¬¡è§ã€‚![](img/a1136557282a500704797139dc43d7b8_46.png)
- en: '![](img/a1136557282a500704797139dc43d7b8_47.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1136557282a500704797139dc43d7b8_47.png)'
- en: ã€‚
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
