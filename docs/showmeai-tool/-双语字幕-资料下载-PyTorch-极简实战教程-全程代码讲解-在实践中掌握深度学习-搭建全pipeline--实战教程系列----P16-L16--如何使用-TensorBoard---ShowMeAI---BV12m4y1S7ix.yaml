- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P16ÔºöL16-
    Â¶Ç‰Ωï‰ΩøÁî® TensorBoard - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HeyÔºå guysÔºå welcome to a new Pytorch tutorial„ÄÇ In this video„ÄÇ we will learn how
    to use the Tenzo board to visualize and analyze our model and training pipeline„ÄÇ
    Tenszoboard is a visualization toolkit in order to experiment with our models„ÄÇ
    It is actually developed by the Tenorflow guysÔºå but it can be used with Pytorch
    as well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here on the official website„ÄÇ We can do a few things„ÄÇ we see a few things
    that we can do with Tenszobar„ÄÇ SoÔºå for example„ÄÇ we can track and visualize metrics
    such as the loss and the accuracy„ÄÇ We can visualize our model graph„ÄÇ We can view
    histograms„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We can project embeddings to a lower dimensional spaceÔºå and we can display images„ÄÇ
    text and audio dataÔºå and we can profile our programs and much more„ÄÇüòä„ÄÇSo now I
    want to show you how we can use this in our code„ÄÇ So I'm going to use the code
    from tutorial number 13 here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3dacee23dcf8b95d479be89b099b38_1.png)'
  prefs: []
  type: TYPE_IMG
- en: All right„ÄÇ So here is the code„ÄÇ So this is the exact code from tutorial number
    13„ÄÇ And if you haven't watched this oneÔºå and I recommend that you watch this one
    first„ÄÇ So I will briefly explain the code again now„ÄÇSo in this tutorialÔºå we used
    the Mnes data set„ÄÇ So we did diit classification here„ÄÇ So here we are loading
    the Mnis data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we are plotting some of the images„ÄÇ and then we create a simple feet forward
    neural net„ÄÇ So this is a fully connected neural network with one hidden layer„ÄÇ
    So we see we have one linear layer firstÔºå Then we have a relu activationctuaation
    function„ÄÇ and then another linear layer„ÄÇ and that's our whole forward pass„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we set up our training pipeline„ÄÇ So we have our loss and optimizer„ÄÇ Then
    we do the training„ÄÇSo hereÔºå as alwaysÔºå we do a forward passÔºå a backward parÔºå and
    then update our weights„ÄÇ And then at the endÔºå we evaluate our model and plot the
    accuracy„ÄÇ So now let's use the tennor board for this code to analyze our model
    a little bit more„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the firstÔºå first thing we want to do is to install tensor board„ÄÇ So for
    thisÔºå we can do Pip„ÄÇInstall„ÄÇTenensil board„ÄÇAnd this will install all the things
    that we need„ÄÇ So in my case„ÄÇ I've already installed this„ÄÇ So this was fast„ÄÇ And
    we don't have to install the whole Tens of flow library„ÄÇ So Tenzzo board is enough
    here„ÄÇAnd now we can start the tensor board by saying tenor board„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we have to specify the path where we save the lock files„ÄÇ and we do
    this by giving it the argument minus minus loer equals„ÄÇ and by default„ÄÇ this is
    called in the runs directory„ÄÇ So let's hit enter„ÄÇAnd then it will start up the
    Tensor board at local horse 60Ôºå06„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we have a warning that it doesn't find Tensorflow and it will run it
    with a reduced feature set„ÄÇ but that is fine„ÄÇ So let's open up the tensor board„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now here we have the tensor part„ÄÇ And right nowÔºå we see that no dash ports
    are active„ÄÇ And this is because we haven't written any data„ÄÇ So let's do this„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's jump to the code again„ÄÇAnd now the first thing we want to do is to
    import the Tenzo board„ÄÇ SoÔºå and for thisÔºå we say from torch dot us dot Tenzobar„ÄÇWe
    importÔºå and this is called summary rid„ÄÇ So we import a summary ridÔºå and„ÄÇSo here
    I have a typo„ÄÇAnd now let's create a rider„ÄÇ So let's say rider equals summary
    rider„ÄÇ And then let's give it a„ÄÇDiectctory„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: where it should save the log files and the default directory isÔºå as I saidÔºå
    the runs folder„ÄÇ But let's be more specific here„ÄÇ So let's call this runs and
    then M N„ÄÇAnd now we have our writer set up„ÄÇ And now the first thing we want to
    do is here„ÄÇ So here in the codeÔºå we plotted some images„ÄÇ And nowÔºå instead of plotting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's add the image to our tensor board„ÄÇ And for this„ÄÇ the only thing we have
    to do is we want to create a grit„ÄÇ And then call the writer at image method„ÄÇ So
    let's do this„ÄÇ So let's say our image grit equals„ÄÇ And we also get this from torch
    vision dot us„ÄÇDot makeÔºå make grit„ÄÇ And then let's give it the data„ÄÇ So here we
    have one batch of our example data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's put this in here„ÄÇ And then let's call„ÄÇRrierÔºå dot at„ÄÇImage„ÄÇAnd then
    here we give the image grid„ÄÇAnd we also have to provide a label for this image
    in the beginning„ÄÇ So let's call thisÔºå for exampleÔºå M Nist„ÄÇImages„ÄÇAnd now„ÄÇÂóØ„ÄÇI want
    to exit hereÔºå so I use„ÄÇ I import cis or system„ÄÇ And then here I use an early exit
    because I don't want to run the whole training pipeline right now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here I call cis dot exit„ÄÇ and I want to make sure that all the events are
    written here„ÄÇ So that's why I also call Rder„ÄÇDot close„ÄÇ So this makes sure that
    all the outputs are being flushed here„ÄÇAnd now let's save this„ÄÇ and let's go to
    the terminalÔºå and let's run this„ÄÇSo let's say Python„ÄÇ and then our file was feet
    forward dot P and hit enter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now let's go to our Tensor board again and let's reload this„ÄÇ And then we
    see we have our images here„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: And here we have our grid that we just created„ÄÇAnd this is 60„ÄÇ this is 8 by
    8 because we specified our batch size to be 64„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3dacee23dcf8b95d479be89b099b38_10.png)'
  prefs: []
  type: TYPE_IMG
- en: And yeahÔºå so now we can analyze our data„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_12.png)
  prefs: []
  type: TYPE_NORMAL
- en: And let's go ahead and do something more with our Tzor board„ÄÇSo the next thing
    we want to do is to add a graph„ÄÇ So to analyze our model„ÄÇ So if we scroll down
    furtherÔºå then we see that here we create our„ÄÇNeural nets„ÄÇ So let's comment this
    thisis exit out again„ÄÇAnd then here we create our model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then here our loss and optimizer„ÄÇ And now down here„ÄÇ let's add our model
    graph So we can do this by saying rider at graph„ÄÇ And then here we give it the
    model„ÄÇAnd then we also can give it an input so we can sayÔºå again„ÄÇ we have our
    example data„ÄÇ So this is one batch„ÄÇ And then we have to reshape the same way that
    we are doing it here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's reshape our batch data„ÄÇAnd„ÄÇThen againÔºå let's call Rer dot close and
    writerer exit„ÄÇinsist system exits„ÄÇ And againÔºå let's run our„ÄÇFile„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_14.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now let's head over to our tensor board again„ÄÇ and let's reload this„ÄÇ And
    then we see here up here„ÄÇ we also have the graphs tab„ÄÇ So let's go to the graph„ÄÇ
    And here we see our model„ÄÇSo we have the input and then the neural net„ÄÇ And now
    if we do a double clickÔºå then we see more details„ÄÇ So here we see our whole model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so now we see we have the first linear layer„ÄÇ Then we have the relu activationctuaation
    function„ÄÇ And then we have the second linear layer„ÄÇAnd we also see the weights
    and the biases for each linear layer„ÄÇ SoÔºå yeahÔºå so now we can inspect this furtherÔºå
    if we want„ÄÇAnd yeah„ÄÇ this is really helpful to analyze the structure of our model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3dacee23dcf8b95d479be89b099b38_16.png)'
  prefs: []
  type: TYPE_IMG
- en: SoÔºå yeahÔºå now we have our model„ÄÇ and now let's analyze some metricsÔºå so„ÄÇWhat
    we did in the original script is we simplyÔºå during the trainingÔºå we printed every
    100th step„ÄÇ We print the current loss„ÄÇSo nowÔºå instead of just printing did this„ÄÇ
    let's add this to our teno board„ÄÇ So let's add the training loss and also the
    accuracy for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And for thisÔºå we want to have the mean loss during this batch training„ÄÇ So let's
    add a two values up here before we start our loop„ÄÇ So the first one is our running
    loss„ÄÇAnd this is 0 in the beginning„ÄÇ And then let's also sayÔºå the running„ÄÇCorrect„ÄÇPredictions
    equals 0 in the beginning„ÄÇ And nowÔºå every hundredth step„ÄÇWe„ÄÇNo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: sorry before for in each iteration„ÄÇ NowÔºå we add the„ÄÇLoss to the running loss„ÄÇ
    So we say running loss plus equals loss dot item„ÄÇ And we also add the number of
    correct predictions to the running correct„ÄÇ So for thisÔºå we want to get the predictions„ÄÇ
    And we can do this the same way as we are doing it down here by calling torch
    dot max„ÄÇSo„ÄÇLet's do this up hereÔºå as well„ÄÇSo we get the predicted values„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we say running correct plus equals„ÄÇ And here we say predicted equals
    equals the actual labels„ÄÇ and then the sum„ÄÇ And this is a tensor with only one
    item„ÄÇ So we can call dot item„ÄÇAnd nowÔºå yeah„ÄÇ here we add this to the running loss„ÄÇ
    And now every hundredth step„ÄÇ we want to calculate the mean value and add this
    to the tenor board„ÄÇ So we call rider dot at Scala„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we have to add have to give it a label„ÄÇ So hereÔºå let's give it the label
    draining loss„ÄÇAnd now the actual loss is the running loss divided by 100Ôºå because
    we sum this up for 100 steps„ÄÇ And then we also have to give it the current global
    step„ÄÇAnd this is the by saying epoch and times the number of total steps that
    we extracted up here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the length of the training loader„ÄÇ and then plus IÔºå and I is the
    current batch iteration„ÄÇSo this is the current global step„ÄÇ So here we add the
    training loss„ÄÇ And now let's do the same thing again and add the accuracy„ÄÇ So
    let's say accuracy„ÄÇAnd then here we have to sayÔºå running„ÄÇCorrect„ÄÇDivided by 100„ÄÇAnd
    after that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have to set the running loss and the running predicted to the running correct
    to 0 again„ÄÇ So let's say running loss equals0„ÄÇ0 and running loss„ÄÇAnd noÔºå sorry„ÄÇ
    Run correct equals 0 again„ÄÇAnd then„ÄÇYeahÔºå now we have to save this„ÄÇ and now we
    have to run the whole training pipeline„ÄÇ So let's comment this system exit out
    again„ÄÇAnd nowÔºå let's run our script„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we should still see theÔºå the printing outputs here„ÄÇ So for every 100th step„ÄÇ
    we see that how the loss is decreasing„ÄÇAnd now we should be done„ÄÇ And now we also
    see the whole accuracy of our network„ÄÇ And now let's go to our Tensor board again
    and againÔºå hit reload„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we have one more entry up here„ÄÇ And this is the scholars entry„ÄÇ And
    here we have our two plots„ÄÇ So yeahÔºå we see that it worked„ÄÇ So we see the accuracy„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: For each of the steps„ÄÇ And we also see how the training loss is decreasing„ÄÇAnd
    yeah„ÄÇ so here by defaultÔºå Tenzo flowÔºå Tenzo board is smoothing this line so we
    can modify the smoothing parameter here„ÄÇBut yeahÔºå and now we can analyze how the
    loss is decreasing„ÄÇ And soÔºå for example„ÄÇ if we see that at some pointÔºå it is not
    decreasing furtherÔºå and we can see that at this point„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have to improve something„ÄÇSoÔºå for exampleÔºå what we can do then is we can
    try out a different learning rate„ÄÇ of courseÔºå So this is usually one of the first
    things that we want to optimize„ÄÇ So let's modify the learning rate„ÄÇ And now let's
    call the folder mist„ÄÇ Let's say simply2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3dacee23dcf8b95d479be89b099b38_20.png)'
  prefs: []
  type: TYPE_IMG
- en: And then againÔºå let's clear this and run our script again„ÄÇAnd„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: Then this should already update our tenor board during the„ÄÇDuring the file runnings„ÄÇ
    So now we see a second graph„ÄÇAnd also here we see a second graph in the loss graph„ÄÇ
    Now„ÄÇ let's reload this again„ÄÇ And now yeahÔºå it should be done„ÄÇ And nowÔºå for example„ÄÇ
    here we see then another graph with a different learning rate„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is how we can interactively optimize and analyze our model„ÄÇAnd nowÔºå
    as a last thing„ÄÇ what I want to show you is how we can add a precision recall
    curve„ÄÇ So precision recall curve lets you understand your model performance under
    different threshold settings„ÄÇ And this makes more sense in a binary classification
    problem„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But if we analyze each class separately here„ÄÇ Then we do have a binary classification
    problem„ÄÇ So let's add a precision recall curve for each class here„ÄÇAndFor those
    of you who do not know what a precision and recall mean„ÄÇ then I have a link for
    you in the description„ÄÇ So please check that out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now what we want to do here is„ÄÇ let's have a look at the official documentation
    here„ÄÇ So I also I recommend that you check out this link„ÄÇ So let's search for
    at PR„ÄÇ And then we see here we have the method at precision at PRr curve„ÄÇ So this
    adds a precision recall curve„ÄÇ And this needs the attack first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then it needs the labels„ÄÇ and here we see the labels is the ground truth
    data„ÄÇ So a binary label for each element„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_24.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3dacee23dcf8b95d479be89b099b38_25.png)'
  prefs: []
  type: TYPE_IMG
- en: And then it needs the predictions and the predictions are the probability that
    an element be classified is true„ÄÇ and the value should be between 0 and1„ÄÇ So this
    is important here„ÄÇ So we need to have the actual labels and also the predictions
    here„ÄÇ All right„ÄÇ So now let's go to the code and at a precision recall curve for
    each class„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here in our evaluation„ÄÇ we want to create a list where we store our labels„ÄÇ
    So let's say labels equals an empty list„ÄÇ And also a list for the predictions„ÄÇ
    So pres equals an empty list„ÄÇ And then during the batch evaluation„ÄÇ So what we
    do here„ÄÇ So for the labelsÔºå we can say labels stopped app penÔºå the actual labels
    is the predicted labels„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now for the„ÄÇüòä„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_27.png)
  prefs: []
  type: TYPE_NORMAL
- en: PredictionsÔºå we have to be careful„ÄÇ So for hereÔºå we need probabilities between
    0 and 1„ÄÇAnd now here we get the outputs from our model„ÄÇ And if we have a look
    at the neural net again„ÄÇ and we see that we have a linear layer at the end„ÄÇ So
    these are raw values„ÄÇ And here we even have a comment„ÄÇ So no activationctuaation
    and no softms at the end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because in this caseÔºå this is applied in our loss function in the cross entropy
    loss„ÄÇ But now„ÄÇ againÔºå in the evaluationÔºå we want to have actual probabilities„ÄÇAnd„ÄÇIf
    you've watched my tutorial about activationation functions„ÄÇ then you know which
    activationation function we must use here to get the probabilities„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is the soft max function„ÄÇ So this  squeezes our values to be probabilities
    between 0 and 1„ÄÇ So let's call the softms here explicitly for our outputs„ÄÇ And
    for thisÔºå let's import F„ÄÇ So functional„ÄÇ So let's say here„ÄÇüòäÔºåLet's import torch
    dot NÔºå N dot functionalÔºå S F„ÄÇCapital F„ÄÇAnd thenÔºå down here„ÄÇWe want to calculate
    the soft max for each output in our output„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's use list comprehension for this„ÄÇAnd let's call this class„ÄÇü§¢ÔºåÂóØ„ÄÇPredictions
    equals„ÄÇ And now here we use list comprehension and call F dot soft max„ÄÇ And then
    here we say of the output„ÄÇ And then we do this„ÄÇ We have also have to give it the
    dimension„ÄÇ So let's say dimension equals along dimension 0„ÄÇ And then we want to
    do this for output in outputs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then let's add this to ourÔºå what did we call it prettz„ÄÇ So prets dotÔºå a
    pen„ÄÇ And then here class predictions„ÄÇ And then when we are done with the for loop„ÄÇ
    we want to convert this to a1zo„ÄÇ So here we say labels equals„ÄÇ and then torch
    dot cut the labels„ÄÇ So right nowÔºå this is a list„ÄÇ And when we want to concatenate
    all the elements in our list along one dimension„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Into a one dimensional tenor„ÄÇ And for the predictionsÔºå we want to have a two
    dimensional tensor„ÄÇ So for each„ÄÇFor each classÔºå we want to stack the predictions„ÄÇ
    and then we want to concatenate this„ÄÇ So we say„ÄÇPrats equals„ÄÇ And then here we
    say torch dot cat„ÄÇ And then here we use list comprehension again and say torch
    dot stack„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we stack each batch and say four batch in our predictions„ÄÇSo you should
    check the the shape of these tensors for yourself„ÄÇ So this has shape„ÄÇ I think
    how many„ÄÇ we haveÔºå I think 10000 samples„ÄÇ So this is 10000 by one and this should
    be 10000 by 10„ÄÇ So for each classÔºå we stacked it here„ÄÇ and now when we are done„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now the last thing we have to do is to have the actual PR curve„ÄÇ So for thisÔºå
    we say classes„ÄÇ So our class labels in this caseÔºå it's just the range 10 because
    we have the„ÄÇDitchitts from 0 to 9„ÄÇ And now let's iterate over this„ÄÇ So4 I in classes„ÄÇ
    And then we say we get the labels I equals„ÄÇ So this is where labels equals equals
    I„ÄÇ And then the same thing with the predictions I equals the predictions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we want to have all the samples„ÄÇ But only for the class I„ÄÇ And then
    we call writer dot at„ÄÇ And this is called at PR curve„ÄÇAnd this needs a tag„ÄÇ So
    for the tag„ÄÇ we just use the class label as string„ÄÇ And then here we have the„ÄÇLabels
    first„ÄÇ and then the predictions„ÄÇ So predictions IÔºå and then as global global stepÔºå
    we just use0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then let's call writer dot close„ÄÇ And then we are done„ÄÇ So now let's save
    this and run our script one more time„ÄÇAnd now when this is done„ÄÇ and we should
    see precision recall curve for each of the class labels„ÄÇSo„ÄÇAlmost done„ÄÇ And a
    pen„ÄÇ So I have a typo here„ÄÇSo I have two different labels variables„ÄÇ So let's
    call this„ÄÇÂóØ„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let's just call this labels 1 here„ÄÇ So labels1 and labels 1 and labels1„ÄÇAnd
    now„ÄÇLet's run this one more time„ÄÇSorry about that„ÄÇSo let's clear this and run
    this one more time„ÄÇAnd now againÔºå we have to go through the training pipeline„ÄÇOhÔºå
    I didn't save it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3dacee23dcf8b95d479be89b099b38_29.png)'
  prefs: []
  type: TYPE_IMG
- en: AlrightÔºå so now we are doneÔºå So let's reload our tensor board one more time„ÄÇAnd
    now we have one more entry up here„ÄÇ and this is the PR curve„ÄÇ And now we should
    see the precision recall curves for each of our class label„ÄÇ So here we have label
    0Ôºå label 1 and so on„ÄÇAnd then we can inspect the precision and the recall for
    the different thresholds„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here on the y axissÔºå we have the precision and on the x axisÔºå we have the
    recall„ÄÇ And then„ÄÇ for exampleÔºå for each for different thresholds we can analyze
    it and see how many true positives„ÄÇ how many false positivesÔºå how many true negatives
    and false negatives we have„ÄÇSo this is also really helpful to analyze the model„ÄÇAnd
    yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so that's all I wanted to show you for the Tenser board„ÄÇ I hope you enjoyed
    this tutorial„ÄÇ and please consider subscribing to the channel and see you next
    timeÔºå bye„ÄÇüòä„ÄÇ![](img/ee3dacee23dcf8b95d479be89b099b38_31.png)
  prefs: []
  type: TYPE_NORMAL
