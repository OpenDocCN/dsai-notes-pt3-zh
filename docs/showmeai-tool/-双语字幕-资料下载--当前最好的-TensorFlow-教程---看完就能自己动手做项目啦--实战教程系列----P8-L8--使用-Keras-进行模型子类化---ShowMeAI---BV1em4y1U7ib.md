# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‚ÄúÂΩìÂâçÊúÄÂ•ΩÁöÑ TensorFlow ÊïôÁ®ãÔºÅ‚ÄùÔºåÁúãÂÆåÂ∞±ËÉΩËá™Â∑±Âä®ÊâãÂÅöÈ°πÁõÆÂï¶ÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P8ÔºöL8- ‰ΩøÁî® Keras ËøõË°åÊ®°ÂûãÂ≠êÁ±ªÂåñ - ShowMeAI - BV1em4y1U7ib

![](img/2b6b959a397871940a0ba2149e837d11_0.png)

What is going on guys„ÄÇ Hope you're doing awesome and welcome back in this video„ÄÇ We're going to take the next step in building our models and we're going to learn about model subclassing„ÄÇ which is a incredibly flexible way to build models and so so far we've touched on the sequential API that has very little flexibility but it's very convenient and then we also saw examples on the functional API for more flexibility in building our models and in this video we're going to use the subclassing which has the most amount of flexibility So I'm just going copy in all of the imports and stuff like that so we have seen all of this before so that we don't waste too much time on this So OS and for ignoring information messages from Tensorflow Keas layers to build our model Ms the data and these two lines will help you out if you running into an issues on running on the GPU and then we have these three lines right here that are just going to load the dataset„ÄÇ

We're loading MNist and then we're doing a reshape right here„ÄÇ we're only doing the reshape to add this dimension here for the number of channels„ÄÇ and then we're converting it to F 32 and we're dividing by 255 to normalize the data„ÄÇAlright so all of this is something that you've seen before what we want to do now is is well first to sort of motivate what we're going to do is we've seen the example that we have a convolutional network and then mapping it to or running it through batch norm and then running it to rather right this is a common structure right that we we've used multiple times and let's say you would do this and you would write that code times 10 now that would be a lot of code to write so what we can do is we can actually create a class for this kind of block so let's do that and let's do class CNN block„ÄÇ

And we're going to inherit from layers thatt layer„ÄÇAnd what this is going to do is it's going to keep track of everything under hood for doing back propagation and all of that stuff„ÄÇAnd then„ÄÇWe're just going to create an init function and we're going to specify the number of out channels and then let's say the kernel size„ÄÇSo if you're familiar with Pytorch subclassing is using Kra subclassing is essentially exactly the same as creating Pytorch models„ÄÇ

 so we create our in function first and we have information about how we want to build this block and we're going to do super to run the parent„ÄÇParent class layer that layerÔºå and then we're going to do CNNM block self„ÄÇIn it„ÄÇAnd we're going to create our comb layer„ÄÇ So Seph„ÄÇ co is layers Com2 D„ÄÇAnd we're and we're going to specify the out channels and then the kernel sides„ÄÇ

And then let's do padding equals the same„ÄÇAnd then for the batch room„ÄÇ we're just going to do layers dot batch normalization„ÄÇAnd then we're just going to do a call method„ÄÇ so we're going to do call input tensorÔºå and then we're going to specify training„ÄÇ and let's just set it to default as training to default false„ÄÇ

So this is we're specifying training here because we're using batch norm„ÄÇ how batch norm works is different when it's in training mode or evaluation mode but anyways„ÄÇ we have a call method right here and„ÄÇThe call methodÔºå if you're used Pytorch„ÄÇ is essentially the forward method„ÄÇBut anyway here„ÄÇ

 so we're initializing the modules we're going to use in this case comm 2 D and batchor and then in the call method we're taking some input tensor and we're going to run it through those layers so we're going to do x equals self„ÄÇcom of input tensÔºå then we're going to do self„ÄÇbaor of x„ÄÇ

 and then we're going to specify training equals training and then we're going to do x equals Tf„ÄÇnn„ÄÇ Relu„ÄÇOf x„ÄÇSo we're doing comm batch numberrluÔºå and then we're going to return X„ÄÇThat's the structure that we saw right here„ÄÇAlthough now we can reuse it multiple times from this class„ÄÇ so for exampleÔºå we could do something like„ÄÇOur model is Kra dot sequential„ÄÇ

And we're going to do CNN block and let's say we have 32 out channels„ÄÇCNN blocklock 64 out channels„ÄÇCNM block 128 and then layers flatten„ÄÇAnd then layers1s 10 nodes„ÄÇ AllrightÔºå so I mean„ÄÇ if you would write this code right hereÔºå just for this blockÔºå it would be multiple lines„ÄÇ three linesÔºå rightÔºå it would be for theumbmberÔºå the batch norm and the the Relu which would be quite annoying would just take up unnecessary space and not be as concise So this is one way we can do it and we could we could also specify now so we could do model dot compile„ÄÇ

 we can do optimizer equals ks do optimizes do atom and we could do loss equals ks losses spars„ÄÇPateorical cross entropyÔºå then from logicits equals true„ÄÇMetricsÔºå we're gonna use accuracy as metric„ÄÇ and then we could do model that fit„ÄÇOn our x trainÔºå y train„ÄÇBch sizeÔºå64 epochs„ÄÇI don't knowÔºå3„ÄÇ and then verbose equals„ÄÇ2Ôºå and then let's also do model dot evaluate x testÔºå y test„ÄÇ

 batch size or 64 where both equals 2„ÄÇ and then let's run that„ÄÇSo after 3 bucks„ÄÇ we get 99% training and then 98„ÄÇ75 on the test set„ÄÇSo what's pretty amazing as well in creating these blocks and doing these call methods is that it kind of feels like first of all it's pretty much like Pytors if you're used to that but if you're used to Nmpy it kind of feels like you're you're using Nmpy but I've also heard that it kind of feels like objectoriented Nmpy and I would agree with that because it feels very intuitive we can do things like print X dot shape and we can run that and then it would each time this is called it would then print the shape of x at that point and so this CNN block is run multiple times depending on the channels right so first we had 32 then 64 and then 128 and then it's rerun again so 32 6428 which makes debugging incredibly simple„ÄÇ

 it makes it so that you can just print the shapes wherever you feel like printing it„ÄÇBut anyways„ÄÇ let let's now make something more complicated or I guess more a bigger model„ÄÇ So we're gonna to do class something called Resblock so this is be we're going to build something similar to Resnet and hopefully at the end of this video you can sort of view it from the perspective that what if we use the functional or just sequential API to reconstruct something similar as we're doing in this video and hopefully you'll feel that it would be very„ÄÇ

 very difficult or take a lot of space and not to be as compact as we're doing it in this video So if you're not familiar with Resnet„ÄÇ there's a video in the description that you can watch so you're more familiar we're not going to reconstruct Resnet but we're going to do something similar to Resnet So anyways we're going to do layer dot layer again„ÄÇ

And then we're going to do in it„ÄÇAnd then let's doÔºå we're gonna sending channels„ÄÇ Alright„ÄÇ so let's do that one res block„ÄÇ And first of allÔºå we got it called the„ÄÇSuper in it„ÄÇ And then let's say that the res block is gonna have three of these„ÄÇ these CNN blocks that we created„ÄÇ AlrightÔºå so we could do something like we could send in channels right here And let's say it's default„ÄÇ

 I du't knowÔºå32„ÄÇ6428Ôºå something like that„ÄÇ And then we're gonna do„ÄÇOr actually„ÄÇ let's not initialize them„ÄÇ Let's just say we have channels„ÄÇ It's going to be a list of three values„ÄÇ and we're going to create three CNN blocks„ÄÇ So let's do self CNNN 1 is CNN block„ÄÇ and we're going to do channels„ÄÇChannels0 and then kernel size 3„ÄÇ

 and we don't actually have to specify that„ÄÇ So rememberÔºå we use the keyword argument here„ÄÇ and we set it to3 so we can just keep it like that and then self dot CNN N 2„ÄÇ We're going do another block„ÄÇ and let's use channels 1„ÄÇAnd then self CNN3„ÄÇ we're going to do CNN block„ÄÇChannels2„ÄÇAnd then we could do something like pooling„ÄÇ

Layers dot max oing 2 d„ÄÇüòîÔºåAnd then so„ÄÇWe're going to run three of these blocks where each block is a CNN„ÄÇ a batchor and then ReluÔºå and then we're going to run a max pool to sort of have the input size in the height and the width„ÄÇAnd then we're going to use the an identity mapping similar to Resnets with these skip connections„ÄÇ so what we're going to do there„ÄÇIs that we got to do an identity mapping so that there it has the same number of channels„ÄÇ

 So remember we're not doing so we're using same convolution so the height and the width wont won't change„ÄÇ but the channels might„ÄÇ So we need to do a identity mapping what we're going to do layers come to D„ÄÇ We're just going specify channels 1„ÄÇAnd then we're going to do kernel size 3„ÄÇ adding equals the same„ÄÇAnd then for the callÔºå so if this is a little bit„ÄÇ

 this doesn't feel super clear right nowÔºå don't worryÔºå I'm going to explain it again„ÄÇ but we're going to do the calm methodÔºå we're going to do an inputtensor and we're going to specify training to be false as default„ÄÇAnd then we're going to do x is self„ÄÇ CNNnn1Ôºå the input we're going to send the input tensor through the first CNNn block„ÄÇAnd then we're going to do training equals training„ÄÇ We're going to do cells that CNN 2 of x„ÄÇ

 then againÔºå specify training„ÄÇAnd then we're going to run it through the last oneÔºå all right„ÄÇ and for this oneÔºå we're going to do xÔºå but we're also going to add this identity mapping„ÄÇWe're gonna do input„ÄÇtenensor and then we're gonna specify training equals training„ÄÇ So what we're doing here is that we're using these skip connections right here„ÄÇ

 but for this one it has passed through this CNNnn2 and changing the number of channels that it was originally to the number of channels that we send in through this list So this integer channels of index1 so that's all we're doing here running it through the CNN block first second block and then to match number of channels so that we can actually do this addition„ÄÇ

 we're first running the input tensor through an identity mapping and then we're adding that to x„ÄÇ All right so when editing this I actually see that there's a mistake here So for the identity mapping we use the kernel size of three which shouldn't be the case that then we're actually doing a comp layer but we just want to do in identity mapping„ÄÇ

 changing the number of channels as output So what you would do here is you would actually change this kernel size to one and in this way the only thing that it would actually do is„ÄÇIs it sort of doing identity mapping and changing the number of channels„ÄÇ And then at the end„ÄÇ

 let's just do this pooling of X and let's return that So return self pooling of x„ÄÇAlright„ÄÇ so so far we have hadÔºå let's remove that„ÄÇ We're not going to use that again„ÄÇ So so far we've done the CNNM block„ÄÇ we've done this res block„ÄÇ and then let's do the„ÄÇ let's do the final sort of the modelÔºå rightÔºü So we're gonna do class resnet„ÄÇ

Let's called resonant like and then now for the parent method we're going to use ks do model All right so this is for the layers„ÄÇ we're going to use layers„ÄÇ layer meaning we're not going to use that as our final model so when we're inheriting from ks„ÄÇ model it has the functionality that layers that layer has„ÄÇ but it also has some additional functionality so for example we have builtin training evaluation so that we can use what we're familiar with in a model do fit modelevaluate model„ÄÇ

pre so those are available if you inherit from ks do model but not if you do layers layer and you also have other properties so that you can do for example model layers checking all of the layers in the model model summary and you can also do things like serialization and then saving your model we're going to cover that in another video though but„ÄÇ

Just know that CAs that model has additional functionality that layer that layer doesn't have„ÄÇ and you should use CAs that model on your final model and hopefully that makes sense„ÄÇSo what we're going to do hereÔºå we're going to define in it„ÄÇ and here we're going to specify the number of classesÔºå so let's say 10„ÄÇ

 and then we're going to firstÔºå let's see we're going to call super of Resnet like„ÄÇAnd thenÔºå self„ÄÇ and then„ÄÇIn it„ÄÇ And then we're going to do self the block1 is going to be res block„ÄÇOfLet's see„ÄÇ So this is something you can play around withÔºå let's just say 32Ôºå 32Ôºå 64„ÄÇAnd then block two„ÄÇ let's do rest block„ÄÇ128Ôºå128 to 56„ÄÇAnd thenÔºå block 3„ÄÇüòîÔºåRes blocklock128256512„ÄÇ

 So we're just specifying the channels for each of the the CNN CNN CNN blocks in this res block and so you can sort of see that we're scaling this and making this bigger and bigger and it might become difficult to sort of understand what we're doing but if you just step through it step by step we first created the CNN block just using combat re because we want to reuse it for different numbers of channels then we built this res block that it uses these these blocks multiple times and also a pooling layer together with this identity mapping and then we're just using that block in the resnet like model and then at the end we're going to do self dot pool we're going to do layers global average pooling 2D and you can read about this but this is essentially gonna average„ÄÇ

Pull the height and the width„ÄÇ Then we're just going to do aÔºå for exampleÔºå you could replace a flat„ÄÇ So instead of doing thisÔºå you could do„ÄÇLayers„ÄÇ flatten if you feel more comfortable with that if you so we're using it sort of in the same functionality that we want to flatten it or to downscale the heighten the width and then send it through a last classification layer„ÄÇ

 so that's what we're going to do now we're going to do self dot classifier layers dense and then of number of classes„ÄÇAnd so now what's all that's left is to do the callÔºå so we're going to do input tensor„ÄÇ we're going to do trainingÔºå set it test default false„ÄÇAnd then„ÄÇWe're going to do Se to block one of input tensor„ÄÇ We're going to specify training„ÄÇ

And from my understanding specifying training here that's going to be done inside this model that fit or model evaluateval we're also going to show in future videos how to do custom training loops and so on„ÄÇ but I think this is done internally in model that fit if we just specify training right here that's going to send the argument depending on for example here we're training so that would set training to true and then when for the evaluation it would then set model to false or rather training to false„ÄÇ

And then„ÄÇWe're going to do block 2 of xÔºå again specify training„ÄÇWe're going to do Se to block 3 of x„ÄÇ againÔºå specify training„ÄÇAnd then at the endÔºå we're going to do self dot pool of x„ÄÇ And then we're going to do x is„ÄÇOr actually just return self dot classifier of x„ÄÇAnd what would probably help now is doing„ÄÇModel that summary„ÄÇSo after the model that fit here„ÄÇ

 let's do print model dot summary„ÄÇRight there„ÄÇ And let's just run it for single epoch„ÄÇ First of all„ÄÇ AlrightÔºå yeahÔºå we need to also specify our model„ÄÇ So we gotta do model equals res„ÄÇNe„ÄÇLike„ÄÇ and then we can do classes equals 10Ôºå and now we can hopefully run that„ÄÇAlright„ÄÇ so we can see after one EpoÔºå we have 97% on the training„ÄÇ

 And then here we can see the the sort of the„ÄÇThe layers of our model and we can see we have a re block„ÄÇ res blockÔºå Res blockÔºå and then average pullingÔºå and then thens what's kind of annoying here is that we have multiple output shapes and this is usually the case when you're doing subclassing„ÄÇ

But I found one workaround„ÄÇ I'm not sure if this is the best way to do it„ÄÇ but just for now you can use this so we can do model self„ÄÇWe're going to do x carriess that input shape specify„ÄÇ And then let's do 28Ôºå28Ôºå28Ôºå1„ÄÇAndThen return„ÄÇ ks dot model inputs equals xÔºå and then outputs equals self dot call of x„ÄÇ

And so what this is going to do is it's going to overwrite the model call and then we could do something like model model„ÄÇ modelÔºå call that and then do dot summary and in this way we're actually going to get the output shapes So I'm just going to let this run and then we'll see what it looks like„ÄÇ

Alright so what we can see now is that the output shapes are actually included we have none for the number of batches„ÄÇ 28281 for the input image after the first re block it's 64 number of channels and then it's 14 by 14 so it's included  one max pooling and then we're max pulling again where max pulling one more time and then that's run through an average pooling so as you can see here as you can see here we have in this case 512 channels and then we have heightened width of3 by 3 we're essentially averaging all of those3 by3512 into a single 512 and then we're running that through a single dense layer and total number parameters are 3 million which is I guess much larger than what we've done but not relatively to other models this is still pretty small but so hopefully you can see that if you would have built this model right this is a huge„ÄÇ

And we can't even see so from the model summary we can't actually see all of the layers right we're just seeing these blocks but these blocks„ÄÇ so each block has multiple of these CNN blocks right all of these resonant blocks as multiple so this model is pretty large if you would plot it and if you would use this I mean use the sequential API I don't you can't even use the sequential API I think but it would be„ÄÇ

It would be a lot more annoying to build this„ÄÇ And so hopefully this is a something that illustrates„ÄÇWhy doing subclassing can be very good and you also have a lot of flexibility that you can pretty much build your models as you want and you can also you can print everything during the call and that's very intuitive„ÄÇ

 I really feel the subclassing is a great way to build your models and in the next video we're also going to show how to do these custom layers using this subclassing so that for example„ÄÇ let's say we're now using these this dense layer right how would you actually go about building a dense layer by yourself and stuff like that So that's what we're going to do in next video I thought just for fun„ÄÇ

 we could run this for a little bit longer and we could see what kind of accuracy we can actually get with this the largest model we've built in these tutorials so far So let's run this and this is gonna to take a while but it's going to go pretty fast for you So yeah I'm going let it run and I'm gonna to wait until it's done„ÄÇ

Al right so after about 20 epochs we get 99„ÄÇ84% on the training then 99„ÄÇ14 on the test set I think if you train this for just a little bit longer so that you get this up to 99 when I trained it previously I actually got 99„ÄÇ4% on a test set but then I trained for a couple of more epos and then what I think you could also do is add regularization and the model would improve but nonetheless I think this video really demonstrates the power of subclassing and hopefully you think subclassing is awesome after this video thank you so much for watching this video and I hope to see you in the next one„ÄÇ



![](img/2b6b959a397871940a0ba2149e837d11_2.png)