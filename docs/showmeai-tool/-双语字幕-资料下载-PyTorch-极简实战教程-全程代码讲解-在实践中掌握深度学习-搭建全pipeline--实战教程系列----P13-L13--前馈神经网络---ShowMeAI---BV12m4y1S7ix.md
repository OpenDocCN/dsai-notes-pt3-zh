# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P13ÔºöL13- ÂâçÈ¶àÁ•ûÁªèÁΩëÁªú - ShowMeAI - BV12m4y1S7ix

HiÔºå everybody„ÄÇ Welcome to a new Pytorch tutorial„ÄÇ Today„ÄÇ we will implement our first multilayer neural network that can do diit classification based on the famous endlessnes data set„ÄÇ In this tutorialÔºå we put all the things from the last tutorials together„ÄÇ So we use the data loader to load our data„ÄÇ We apply a transform to the data„ÄÇ

 Then we will implement our neural net with input layerÔºå hidden layer and output layer„ÄÇ and we will also apply activation functions„ÄÇ Then we set up the loss and the optimizer and implement the training loop that can use batch training„ÄÇ and finallyÔºå we evaluate our model and calculate the accuracy„ÄÇ And additionally„ÄÇ we will make sure that our whole code can also run on the GPu if we have GPU support„ÄÇ

 So let's start„ÄÇ and first of allÔºå we import the things we need„ÄÇ So we import torch„ÄÇ Then we import torch dot and N as an N„ÄÇ Then we import torch„ÄÇüòäÔºåVision for the data sets„ÄÇ and we import torch vision dot transforms as transforms„ÄÇAnd we also import Matpllip dot pipl S P L T to show you some data later„ÄÇ And then first of all„ÄÇ

 we do the device configurationÔºå So device config„ÄÇ And for this„ÄÇ we create a device by saying device equals torch dot deviceÔºå and this has the name Kuda„ÄÇ if we have GP support„ÄÇ So if„ÄÇTorch dot kuda dot is available„ÄÇ And if it is not available„ÄÇ So else„ÄÇ we call our device simply CPU„ÄÇ And then laterÔºå we have to push our tenzos to the device„ÄÇ

 And this will guarantee that it will run on the GP if this is supported„ÄÇSoÔºå yeah„ÄÇ so let's define some hyper parameters„ÄÇAnd here let's define the input size„ÄÇ And this is 784„ÄÇ And this is because later we see that our images have the size 28 by 28„ÄÇ And then we will flatten this array to be a 1 d Tenzoa and 28 times 28 is 784„ÄÇ

 So that's why our input size has to be 784„ÄÇ Then let's define a hidden size„ÄÇ And here I will say this is 100„ÄÇ You can also try our different sizes here„ÄÇAnd the number of classes„ÄÇ And this has to be 10 because we have 10 different classes„ÄÇ We have the digtits from 0 to 9„ÄÇThen let's define the number of epochs„ÄÇ

 And here I will simply say two so that a training doesn't take too long„ÄÇ but you can set this to a higher value„ÄÇThen we define the batch size hereÔºå and this isÔºå let's say„ÄÇ 100„ÄÇ And let's also define the learning rate here by saying learning rate equals 0„ÄÇ001„ÄÇAnd now let's import the famous Amnes data„ÄÇ So you can have that from the Pytorrch library by saying training data set equals„ÄÇ

 And here we use torchvision dot data sets dot Amist„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_1.png)

And this will have to have the root where it has to be storedÔºå So root„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_3.png)

EqualsÔºå and hereÔºå this should be in the same folder„ÄÇ So dot„ÄÇ And then it should create a folder called data„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_5.png)

And then we say train equals true„ÄÇ So this is our training data set„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_7.png)

And then we say we apply a transform right away„ÄÇ So we say transform equals transforms dot to tenzo„ÄÇ So we convert this to a tenzzo here„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_9.png)

And then we also say download equals true„ÄÇ So it should be downloaded if it is not available already„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_11.png)

Then let's copy this and do the same thing with our test data set„ÄÇ And here we have to say train equals false„ÄÇ And we also don't have to download this any more„ÄÇSo now let's continue and create the data loaders by saying train loader equals„ÄÇ And here we get this from torch dot u dot data dot data loader„ÄÇ

 and then it will have to have the data set by saying data set equals„ÄÇ And here it gets the training„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_13.png)

![](img/ee06be2433f88c7e880533bc41c954a0_14.png)

![](img/ee06be2433f88c7e880533bc41c954a0_15.png)

Data setÔºå a train data set„ÄÇ Then we have to specify the batch size„ÄÇ So this is equal to the batch size„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_17.png)

And then we also have to sayÔºå or we can say shuffle equals true„ÄÇ So this is pretty good for training„ÄÇAnd then we copy this again and do the same thing for our test loader„ÄÇ So test loader equals gets the test data set„ÄÇ And we can say shuffle equals false because it doesn't matter for the evaluation„ÄÇAnd now let's have a look at one batch of this data by saying examples equals„ÄÇ

 And then we convert it to a inner objectÔºå Iter of the train loader„ÄÇ And then we can call the next method and unpack this into samples and into labels by saying this equals examples dot next„ÄÇ And now let's print the size of these„ÄÇ So let's print samples dot shape„ÄÇ

![](img/ee06be2433f88c7e880533bc41c954a0_19.png)

And alsoÔºå printÔºå print the label dot shape„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_21.png)

And now let's save this and run this„ÄÇ So let's call Python feet forward dot p to see if this is working so far„ÄÇ And yesÔºå here we have the size of the samples„ÄÇ So this is 100 by 1 by 28 by 28„ÄÇ And this is because our batch size is 100„ÄÇ So we have 100 samples in our batch„ÄÇ Then the one is because we only have one channel„ÄÇ So we don't have any colored channels here„ÄÇ

 So only one channel„ÄÇ And this is our actual image array„ÄÇ So 28 by 28Ôºå as I said in the beginning„ÄÇAnd our labels is only a tenor of size 100„ÄÇ So for each class labelÔºå we have one value here„ÄÇSo yeah„ÄÇ this is our some example data„ÄÇ And now let's also plot this here to see how this is looking„ÄÇ So for I in range 6 and here we use matplotlipÔºå So I call PLT dot subplot of the with two rows and three columns and the index I plus1 and then I can say PLt do I show„ÄÇ

 and here I want to show the actual dataÔºå So samples of I and then of0 because we want to access the first channel and then I will also give this a column map So C map equals gray„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_23.png)

![](img/ee06be2433f88c7e880533bc41c954a0_24.png)

![](img/ee06be2433f88c7e880533bc41c954a0_25.png)

![](img/ee06be2433f88c7e880533bc41c954a0_26.png)

And then I say PÔºå LT dot show„ÄÇAnd let's save thisÔºå and run this again„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_28.png)

And here we have a look at the data„ÄÇ So these are some example handwritten digtits„ÄÇ and now we want to classify these digtits„ÄÇ So for this„ÄÇ we want to set up a fully connected neural network with one hidden layer„ÄÇ So let's do this„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_30.png)

![](img/ee06be2433f88c7e880533bc41c954a0_31.png)

So let's comment this out again„ÄÇAnd now let's create a class neural net„ÄÇ And this has to be derived from an and dot module„ÄÇ And now we have to define the in it and the forward method„ÄÇ So the in it method„ÄÇ So this will self„ÄÇ And then it will has to have the input sizeÔºå then the hidden sizeÔºå and then the output size„ÄÇ

 So the output size is the number of classes„ÄÇ And here firstÔºå we want to call the super in it„ÄÇ So super of neural net and self and dot in it„ÄÇSelf„ÄÇDot„ÄÇIn it„ÄÇAnd then we create our layers„ÄÇ So firstÔºå we want to have a linear layer by saying self dot L 1 equals Nn dot linear„ÄÇ And this will have has the input size as input and the output size is the hidden size„ÄÇ

 Then after the first layerÔºå we want to apply a activation function„ÄÇ And here I simply use the famous relu activation„ÄÇ So self dot relu equals N N dot reallu„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_33.png)

And then at the endÔºå we have another linear layer„ÄÇ So self dot L 2 equals„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_35.png)

NÔºå N dot linear„ÄÇ Now we have to be careful„ÄÇ So the input size here is the hidden size„ÄÇ and the output size is the number of classes„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_37.png)

And now let's define the forward method„ÄÇ So this will have self and one sample X„ÄÇ and now we apply all these layers„ÄÇ So we say out equals„ÄÇ and now we use the first layer L1„ÄÇ which gets the sample X and then the next out is self dot relu now use the activationation function„ÄÇ which will get the previous output hereÔºå and the last out equals self dot L2 out„ÄÇ

 So this will apply the second linear function and now we have to be careful again because here at the end„ÄÇ we don't want an activation function„ÄÇ So we don't apply the softm here as usual in multiclass classification problems because in a second„ÄÇ we will see that we will use the cross entropy lossÔºå and this will apply„ÄÇ

![](img/ee06be2433f88c7e880533bc41c954a0_39.png)

![](img/ee06be2433f88c7e880533bc41c954a0_40.png)

Ply the soft max for us„ÄÇ So no soft max here„ÄÇ So we simply say return out„ÄÇ So this is our whole model„ÄÇ And then we can create it here by saying model equals neural net„ÄÇAnd this will get the input sizeÔºå then the hidden size and the number of classes„ÄÇSoÔºå yeah„ÄÇ now we have the models„ÄÇ So now let's come create the loss and the optr„ÄÇ

 So here we say criterion equals N N dot cross entropy loss„ÄÇ And this will apply the soft marks for us„ÄÇ So that's why we don't want this here„ÄÇ So be very careful about this„ÄÇAnd now let's create our optimizer as well by saying tor optimizer equals torch dot optim dot„ÄÇÂóØ„ÄÇNowÔºå let's use the atom optimizer here„ÄÇ And this has to get the parameters„ÄÇ

 And here we can use model dot parameters„ÄÇ And it also has to get the learning rate„ÄÇ L R equals learning rate„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_42.png)

![](img/ee06be2433f88c7e880533bc41c954a0_43.png)

Now we have the loss and the optimizer„ÄÇ and now we can do our training loop„ÄÇ So training loop now„ÄÇAnd for thisÔºå let's first define the number of total steps„ÄÇ So n total steps equals„ÄÇ And this is the length of the training loader„ÄÇSo now we can do the typical loop„ÄÇ So we say four epoch in range„ÄÇ

![](img/ee06be2433f88c7e880533bc41c954a0_45.png)

Noum epochs„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_47.png)

And so this will loop over the epochs„ÄÇ And now we loop over all the batches„ÄÇ So here we say for I„ÄÇ And then againÔºå we unpack this„ÄÇ So we say imagesÔºå images and labels„ÄÇAnd then we iterate over„ÄÇ enumerate over our train loader„ÄÇ So the enumerate function will give us the actual index and then the data and the data here is the tuple of the images and the labels and now we have to reshape our images first„ÄÇ because if we have a look at the shapeÔºå then we see that this is 100 by1 by 28 by 28„ÄÇ

 as I showed you in the beginning„ÄÇ And now we set our input size is 784„ÄÇ So our image tensor needs the size100 by and 784 is second dimension„ÄÇ![](img/ee06be2433f88c7e880533bc41c954a0_49.png)

So the number of batches first„ÄÇSoÔºå let's reshape our„ÄÇOur tenor first„ÄÇ So we can do this by saying images equals images dot reshape„ÄÇ And here we put in -1 as the first dimension„ÄÇ So then Tsor can find out this automatically for us„ÄÇAnd here as second dimensionÔºå we want to have 28 by 28„ÄÇ And then we also call2 device„ÄÇ

 We will push this to the GP UÔºå if it is available„ÄÇAnd we have also have to push it to the„ÄÇ push the labels to the device„ÄÇ So labels equals labels to„ÄÇDeevvice„ÄÇAnd now let's do the forward pass„ÄÇ So firstÔºå we do the forward pass„ÄÇ and afterwards„ÄÇ the backward pass„ÄÇSo the forward passÔºå we simply say outputs equals model„ÄÇ

 And this will get the images„ÄÇ and then we calculate the loss by saying loss„ÄÇEquals„ÄÇ and then here we call our criterion„ÄÇ And this will get the predicted outputs and the actual labels„ÄÇ So this is the forward pass„ÄÇ And then in the backward pass„ÄÇ the first thing we want to do is call optr dot0 gra to empty the values in the gradient attribute„ÄÇ



![](img/ee06be2433f88c7e880533bc41c954a0_51.png)

![](img/ee06be2433f88c7e880533bc41c954a0_52.png)

And then we can do the next step by saying lost dot backward„ÄÇ So this will do the back propagation„ÄÇ And now we can call optr dot step„ÄÇ So this will do an update step and update the parameters for us„ÄÇAnd nowÔºå let's also print some„ÄÇPrint the loss„ÄÇ So let's sayÔºå if I plus1„ÄÇMoulular 100 equals equals 0„ÄÇ So every 100th stepÔºå we want to print some information„ÄÇ So let's print the current epoch„ÄÇ

 So by saying this is epoch epoch plus  one„ÄÇ And then we want to print all the„ÄÇEpoch„ÄÇ So number of epochs„ÄÇ Then let's also print the current step by saying step„ÄÇ And this is I plus1„ÄÇ And then the total number of steps by saying n total steps„ÄÇAnd we also want to print the loss by saying loss equals loss dot item„ÄÇ

 And let's also say we only want to print four decimal values„ÄÇSoÔºå yeah„ÄÇ now we are done with the training„ÄÇ So this is the whole training loop„ÄÇ And now let's do the testing and the evaluation„ÄÇAnd for this„ÄÇ we don't want to compute the gradients for all the steps we do„ÄÇ

 So we want to wrap this in a with torch dot no gra statement„ÄÇAnd then first„ÄÇ we say the number of correct predictions equals 0 and the number of samples equals 0 in the beginning„ÄÇ And then we loop over all the batches in the test samples„ÄÇ So we say four images and labels in„ÄÇAnd here we can simply say in test loader„ÄÇ and then againÔºå we have to reshape this„ÄÇ

 So like we did here„ÄÇ So images and labelsÔºå we want to reshape this and put it and push it to the device„ÄÇAnd„ÄÇThen let's call„ÄÇ let's calculate the predictions by saying outputs equals models„ÄÇ So this is our trained model nowÔºå and this will get the test images here„ÄÇAnd then let's get the actual predictions by saying underscore and then predictions equals torch dot max„ÄÇ

Of the outputs and along the dimensionÔºå along the number one„ÄÇSo the torch up max function will return the value and the index„ÄÇ So we are interested in the actual index„ÄÇ So this is the class label„ÄÇ So that's why we don't need the first actual value„ÄÇSo these are our predictions„ÄÇ And now„ÄÇ

 let's say the number of samples plus equals„ÄÇ And here we say labels dot shape 0„ÄÇ So this will give us the number of samples and the current„ÄÇBatch„ÄÇSo should be 100„ÄÇ And then we say the number of correct„ÄÇ So the correct predictions equals„ÄÇ And here we can say predictions equals equals the actual labels and then dot sum and then dot item„ÄÇ

 So for each correct predictionÔºå we will add plus one„ÄÇAnd thenÔºå of course„ÄÇ we have to say plus equals the number of correct„ÄÇValues„ÄÇ And then when we are done with the loop„ÄÇ we calculate the total accuracy by saying a equals 100 times the number of correct„ÄÇAnd predictions divided by the number of samples„ÄÇ So this is the accuracy in percent„ÄÇ

 And now let's print this„ÄÇ So print„ÄÇÂóØ„ÄÇAnd we want to print accuracy equals„ÄÇ And here we simply say a„ÄÇ And then we are done„ÄÇ So now let's save this and clear this and let's run this and hope that everything is working„ÄÇSo now our training startsÔºå and we should see that the loss should be increased with every step„ÄÇ Sometimes it will also increase again„ÄÇ But finallyÔºå it should get lower and lower„ÄÇAnd„ÄÇNow„ÄÇ

 it should be done and testing is very fast„ÄÇ So now we see that the accuracy is 94„ÄÇ9„ÄÇ So it worked„ÄÇ Our first feet forward model is done„ÄÇ And yeahÔºå I hope you understood everything„ÄÇ And you enjoyed this„ÄÇ If you like itÔºå please subscribe to the channel and see you next timeÔºå bye„ÄÇ

![](img/ee06be2433f88c7e880533bc41c954a0_54.png)