# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P17ÔºöL3.1- KerasÊ∑±Â∫¶Â≠¶‰π†ÂíåÁ•ûÁªèÁΩëÁªúÁºñÁ®ã‰ªãÁªç - ShowMeAI - BV15f4y1w7b8

Hi this is Jeffy and welcome to applications of Deep neural networkss with Washington University in this video we're going to have a general introduction to deep neural networks and conceptually how they work„ÄÇ

This will allow us to build upon the Python that we learn in previous parts of this course and to actually construct neural networks for the latest on my AI course and projects„ÄÇ

 click subscribe in the bell next to it to be notified of every new video„ÄÇ

 neuralural networks have been around for a while„ÄÇ Deep learning is just the ability to train neural networks that are very deep„ÄÇ

üòä„ÄÇ

![](img/8444b070a2a80e4ecb951689e6339e17_1.png)

You might have seen other machine learning models and other classes like support vector machines and gradient boostting and XG Bot and light GBBM and all these various ways of training a model based on data„ÄÇ

Neurural networks can be drop in replacements for other models like this„ÄÇ

The neural network simply takes in the data that you would have normally sent to a support vector machine or other model„ÄÇ

And outputs either a classification result where it attempts to classify or determine the type of what the input was sent to it„ÄÇ

Or it can be regression and it outputs a number based on the data that you're trying to have the neural network predict from„ÄÇ

HoweverÔºå neural networksÔºå their real power lies in their ability to take end data and produce data in ways that don't fit into your typical classification and regression„ÄÇ

Type modelsÔºå for exampleÔºå you can input an image into a neural network„ÄÇ

 You can even have a neural networkÔºå accept an image and produce another image„ÄÇSo it gets very„ÄÇ

Expressive what you can actually do with the neural network is the input can be just about anything and the output can be just about anything„ÄÇ

 and the input and output by no means have to be of the same type„ÄÇIn other modelsÔºå you would„ÄÇ

Simply send in a one dimensional vector so a list of predictors„ÄÇYou can alsoÔºå though„ÄÇ

 with a neural networkÔºå pass in a 2D matrix„ÄÇ NowÔºå this is where you start to pass in„ÄÇ

 say an image with a grid of pixels„ÄÇ The power of the neural network is that it realizes the pixels that are near each other are more important to each other„ÄÇ

 whereas the other model types„ÄÇ change in the order of the input factor really has no effect on anything„ÄÇ

It can also pass in a 3D matrix or 3D tensor„ÄÇThis is would essentially be a color image where that third dimension is specifying the color of the individual pixels that you're passing in„ÄÇ

And dimensionsÔºå it can be used in several different ways with neural networks when you talk about the dimensionality of the neural network„ÄÇ

Usually you're talking about what the input vector or input matrix looks like„ÄÇ

 how many input neurons do you have and how are they arrangedÔºå are they a grid or they a boxÔºü

Dimenssions can also refer to the number of weights that are in a neural network„ÄÇNow„ÄÇ

 traditional models„ÄÇYou would talk about regression and classification and neural networks do this too„ÄÇ

 the output neurons of the neural network become either the single regression output or the classification„ÄÇ

So a regression neural network like you see hereÔºå now these are two example neural networks that I put together„ÄÇ

 I work in the life insurance industry so you'll see a number of examples from me sort of in an insure tech sort of way where you've got„ÄÇ

Inputs that are related to medical records and other things that you'd be interested in for life insurance„ÄÇ

Here you're asking the neural network to predict the maximum face amount„ÄÇ

 So how much should we insure somebody forÔºå what's the maximum amount we would go on the risk for with an individual„ÄÇ

Classification neural networksÔºå those produce classes„ÄÇ

 so we would maybe have a preferred standard substandard or a decline„ÄÇ

That just puts the the potential insurance app into the correct bucket„ÄÇ So regression classification„ÄÇ

 These are your traditional types of modelÔºå and we'll see later on in this class that neural networks can have much more complicated outputs than just„ÄÇ

Classification or regressionÔºå a neural network can even be both classification and regression at the same time„ÄÇ

The output neuronsÔºå so the ones on the far rightÔºå here there's just one„ÄÇ

 if it's a regression neural networkÔºå it's always going to be just one output neuron„ÄÇ

Here we have additional output neuronsÔºå one output neuron for each class„ÄÇ

That's how you can tell a classification neural network if it's a binary classification neural network„ÄÇ

 meaning it's only classifying between two thingsÔºå usually it'll just have one output neuron and that specifies the probability of it being one of those classes„ÄÇ

HoweverÔºå if you're dealing with a multi class classification with„ÄÇWith three or more classes„ÄÇ

 usually you're going to„ÄÇYou're going to have one output neuron per class„ÄÇ

 You would never have a single class classification neural network because there's just one class„ÄÇ

 it would always be that classÔºå so you can have to have at least two classes so that there's at least something to differentiate between for the neural network to classify„ÄÇ

This is the structure of a neural network„ÄÇThey have multiple layers„ÄÇ

 Now we'll see that there are additional layer types and other things that will make this more complicated„ÄÇ

 but for tabular neural networksÔºå this is where the input to it looks sort of like rows and columns from Excel„ÄÇ

 This is what it'll look like„ÄÇYou're going to have your input layer„ÄÇ

 These input neurons are the values that come into the neural network„ÄÇ

 Then you're going to have several hidden layersÔºå finally going to the output layer„ÄÇ

These are bias neuronsÔºå you don't send input into those directlyÔºå they are simply there„ÄÇ2„ÄÇ

To give the neural network additional predictive power„ÄÇ

 we'll see exactly what bias neurons are for in a momentÔºå they handle„ÄÇ

They handle a situation where the inputs are both zero„ÄÇ

 but you don't necessarily want the output to also be zero„ÄÇ

The arrows are the weights between the various entities in the„ÄÇAnd the neural network„ÄÇ

In hidden layersÔºå you can have lots of hidden layers in deep learningÔºå hundreds of them„ÄÇ

It's usually four types of neurons and a neural network input neurons take in the input to the rest of the neural network„ÄÇ

Hidden neuronsÔºå neither they're hidden because they're between the input and output neurons„ÄÇ

 input neurons receive the input for the neural network„ÄÇ

 outputput neurons receive the output that are sent out of the neural network hidden or between that„ÄÇ

Context neurons will see more about those when we get into time series and recurrent neural networks„ÄÇ

 they maintain state between calls to the neural network„ÄÇAnd then bias neurons„ÄÇ

 they are essentially like the y intercept and„ÄÇTraditional mathematicsÔºå linear„ÄÇLinear equations„ÄÇ

You also have several layer types that these neurons go into„ÄÇ

 there's the input layer that receives the inputÔºå the output layer that sends the output from the neural network and then the hidden layers between that„ÄÇ

In a later partÔºå we're going to manually calculate the output from a neural network„ÄÇBut for now„ÄÇ

 we'll see that the calculation that a neural network actually goes through is not that complex„ÄÇ

It's essentially a weighted sum passed into a activation function„ÄÇ

So the input to calculating one hidden neuron„ÄÇOr an output neuron in the neural network„ÄÇ

Essentially takes in the feature vector or the vector coming into it„ÄÇ

 So if we were calculating for this neuron down here„ÄÇThe input vector would be 1Ôºå2Ôºå3„ÄÇ these values„ÄÇ

 and essentiallyÔºå you would multiply input 1 times weight 1Ôºå input 2 times weight 2„ÄÇ

And put three times weight three„ÄÇSome those altogether„ÄÇ

That value then gets passed into the activation functionÔºå and that becomes the output of that neuron„ÄÇ

And that's what this equation is basically showing you here„ÄÇ

 It' essentially the summation of all of thetasÔºå thetas or weightsÔºå timesÔºå all of the x's„ÄÇ

 the x's are inputsÔºå and then phi is the the activation function„ÄÇ

 And this is basically done over and over and over again to calculate every hidden and output neuron„ÄÇ

In the neural network„ÄÇThis gives you an example of thisÔºå the input is one and 2„ÄÇ

Now this third neuron hereÔºå that's actually your bias neuron„ÄÇ

And the way that it gets represented as the bias neuroon is we cancateate a one onto the end of this„ÄÇ

 So one is going into one first neuron 2 goes into the second and this one„ÄÇGoes into the third„ÄÇ

Neuron that becomes basically the bias„ÄÇ So whatever this weight is just gets added to it kind of like an intercept„ÄÇ

 Since this is oneÔºå we're multiplying one times the third weight„ÄÇ

 That's basically how bias neurons neurons work„ÄÇWe might have this as our weights„ÄÇ

 So those might be the three weight values„ÄÇ The third one would be called the bias value„ÄÇ

 We multiply each of these inputs by each of these weight values in summit„ÄÇ and this 0„ÄÇ

8 becomes the summation that is then passed to the activation function„ÄÇ

Activation functions are just functions that introduce non linearity into the neural network„ÄÇ

 so that everything's not linear rectified linear unit„ÄÇ

 we will see is the most popular of the activation functions in modern deep„ÄÇ

Deep learning and other family members related to rectified linear units like the leaky„ÄÇRalue„ÄÇ

Soft Mac is always used in classification neural networks for the final output„ÄÇOf the neural network„ÄÇ

 this ensures that all of the output neurons sum to one„ÄÇ

 because you'd like those output neurons be the probability of each of the classes that the neural network is trying to classify in softftmax just ensures that all those probabilities are truly probabilities and they add up to one as probabilities usually do„ÄÇ

The rectified linear unit is a pretty simple activation functionÔºå but extremely effective„ÄÇ

You're essentially taking the max of0 and xÔºå so x is the value that was passed in„ÄÇ

Softmax looks like thisÔºå againÔºå it's essentially summing them together„ÄÇ

 dividing each one by the summationÔºå that normalization is what ensures that they add up all to 1„ÄÇ0„ÄÇ

By the wayÔºå if you'd like to experiment with the softftmax and see how these individual values are created„ÄÇ

 I have a JavaScript example that I have a link to right here„ÄÇAnd why is the rectified linear unitÔºü

So popularÔºå why is this such a popularÔºüActation function„ÄÇUsed to be before the deep learning days„ÄÇ

 the most common activation functions were the hyperbolic tangent and sigmoid„ÄÇSigmoid is shown here„ÄÇ

So as you would input values into itÔºå it would have the squashing effect„ÄÇ

As you went to negative infinity or positive infinityÔºå And that was desired behavior that worked„ÄÇ

 that worked quite well„ÄÇWellÔºå you're typically optimizing these neural networks through„ÄÇ

 through gradient descent„ÄÇ So you're taking the derivative of the error function„ÄÇ

 So this is the error function„ÄÇAs we change a weight„ÄÇ So as we change one weightÔºå the error goes up„ÄÇ

 The error goes down„ÄÇYou'd like to get the error at the minimum location„ÄÇ

 except you can't see this entire graph at once„ÄÇ You'd have to literally calculate the neural network for every number between„ÄÇ

A reasonable range„ÄÇ You only really see the dot that you're actually odd„ÄÇ

 but that's what the derivative is for in calculus„ÄÇ You take the derivative„ÄÇ

 and it gives you the slope or the instantaneous rate of change of wherever the weight is at„ÄÇ

 And you can tell by the slope of this value„ÄÇ This has a negative slope„ÄÇ

 So that means we need to increase the weight to move towards the minimum„ÄÇ

 You're always changing the weight by the inverse of the sign of the of the slope or the gradient„ÄÇ

 This is typically called gradient in machine learning„ÄÇSo if we look at the derivative„ÄÇ

Of the sigmoid functionÔºå you can see it by the dashed line„ÄÇ

Notice how it quickly converges to zero this since that derivative is called a gradient„ÄÇ

The fact that it would go to0 or vanish means we would have a vanishing gradient„ÄÇ

 This is the vanishing gradient problem„ÄÇIn the deep learning solved„ÄÇ

 one of the many problems that it mostly solved„ÄÇBecause this saturates to 0„ÄÇ

 it's not as desirable as the„ÄÇAs the re that the rectified linear unit doesn't saturate to 0„ÄÇ

 like the sigmoid function does in both directions„ÄÇ We'll look at why bias neurons are needed„ÄÇ

EssentiallyÔºå they are the intercept„ÄÇLike when you previously worked with Y equals Mx plus K linear equations„ÄÇ

If you look at this oneÔºå this is an example of when we change the weight„ÄÇ

 If you notice we change the weightÔºå it's effectively changing the slope„ÄÇAnd you really„ÄÇ

 the line always has to pass through zero„ÄÇWhen the input is„ÄÇSo all of these pass through zero„ÄÇ

 there's no way to really shift that„ÄÇWhen you change the bias neuronÔºå now you're shifting„ÄÇ

 you're not affecting the slope„ÄÇ So using those two togetherÔºå you can really„ÄÇ

 you can affect the slope and you can shift it„ÄÇ You can move it„ÄÇ

And then all of those neurons together can contribute in sort of the additive effect of all of the neurons„ÄÇ

Let's this line then break and basically approximate any function„ÄÇ



![](img/8444b070a2a80e4ecb951689e6339e17_3.png)

Thank you for watching this video„ÄÇ Now that we've seen a general introduction to deep neural networks„ÄÇ

 We're ready to start to look at Tensorflow and Cars and see how these are actually implemented in Python so that you can make use of them„ÄÇ

 This content changes often„ÄÇ So subscribe to the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇ

üòä„ÄÇ