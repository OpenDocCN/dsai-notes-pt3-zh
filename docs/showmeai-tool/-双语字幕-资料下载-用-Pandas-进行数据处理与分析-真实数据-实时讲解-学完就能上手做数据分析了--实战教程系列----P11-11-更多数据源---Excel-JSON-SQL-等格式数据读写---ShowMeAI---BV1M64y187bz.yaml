- en: 【双语字幕+资料下载】用 Pandas 进行数据处理与分析！真实数据&实时讲解，学完就能上手做数据分析了！＜实战教程系列＞ - P11：11）更多数据源
    - Excel、JSON、SQL 等格式数据读写 - ShowMeAI - BV1M64y187bz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hey there。 how's it going， everybody。 In this video。 we're gonna be learning
    how to read and write data to different sources。 So we'll learn how to read and
    write data using CV files， Excel files， Json and also SQL databases。 Now， in this
    series so far。 we've been reading data from CV files。 But in data science。
  prefs: []
  type: TYPE_NORMAL
- en: there are so many different ways for data to be stored。 So by the end of this
    video。 you should be able to get your data to and from pandas， no matter what
    data format you're using。 Now， if you're watching this video because you're looking
    for how to read and write a specific file format。 Then I'll be sure to add timestamps
    in the description section below to where we read and write from each different
    format。
  prefs: []
  type: TYPE_NORMAL
- en: Now， I would like to mention that we do have a sponsor for this series videos。
    And that is brilliant。 So I really want to thank brilliant for sponsoring this
    series。 And it would be great if you all can check them out using the link in
    the description section below and support the sponsors。 And I'll talk more about
    their services in just a bit。 So with that said。
  prefs: []
  type: TYPE_NORMAL
- en: let's go ahead and get started。 Okay， so first， let's look at CV files since
    we've already been。😊。Using these throughout the series we should already be familiar
    with reading data in from CSv since that's what we've been doing so far。 But in
    case this is the first video of the series that you're watching。 let's go over
    this one more time and then we'll also learn how to write to a cv file as well。
  prefs: []
  type: TYPE_NORMAL
- en: So up here towards the top of my notebook here we can see that I'm reading in
    this CSv file and this Cv file is within a data folder that is in the same location
    as this Jupyter notebook on the file system Now if you have a cv file loaded elsewhere
    on the system then you'll need to pass in the full path to that file instead of
    just this relative location that we have here。
  prefs: []
  type: TYPE_NORMAL
- en: and we can see that we have different arguments that we can pass in when reading
    our Cv files。 So in this example I'm automatically setting the index to this respondent
    column here which is the respondent ID for each person who answered this survey
    And when I read in the Cv we can see that it sets this data frame equal。
  prefs: []
  type: TYPE_NORMAL
- en: to the data and we can print this data out down here at the bottom。 So that
    is the read CSV method and it allows us to pull data in to pandas。 Now let's learn
    how to write this data back to a CSv So maybe you're gonna make some changes and
    some different analysis here to your data frame and then we want to export this
    back to our file system for later use or so that we can share it with someone
    else or something like that。
  prefs: []
  type: TYPE_NORMAL
- en: So for example， let's filter down for a specific location in this survey you
    know maybe you're doing some analysis for your specific country and you just want
    to see the survey results from that location we've seen this in previous videos。
    but if we want to filter then we can simply say I'll create a filter here and
    just say that I want the country here and I'll grab if the country is equal to
    India so let's say you're doing some analysis and you only want the survey results
    from India So now I'm going create a new data frame here。
  prefs: []
  type: TYPE_NORMAL
- en: I'll call this Indiadf and do a Df do Lo and pass in filter。 So now if I do
    an Indiadf do head to take a look at the beginning of this new data frame if we
    look over here in the country column then we can see that all of these countries
    here are now set to India so now let's say that we want to export this new filter
    data frame to a CSv file So to do this we can use the two CSv method。
  prefs: []
  type: TYPE_NORMAL
- en: So we can say I'll just say India underscore Df， which is our data frame do2
    underscore CSv and now I'm just going to pass it into that same location in that
    data directory and then I'll just call this modified do cv So if I run this we
    can see that we don't get any errors and now if I go back and look at my file
    system here then I have this modified do cv。
  prefs: []
  type: TYPE_NORMAL
- en: so if I click on this。Then we can see that this is know a little bunch together
    since it's a CSV file。 a Ros CSsv file that we're looking at。 but we can see that
    we have all of our column names here。 and then the second row should be the first
    result and I can see here that we have India for that country if I look at the
    second result we can see we have India again and India again down here most likely
    I can't see it but you know we can just assume that it's there it's looking good。
  prefs: []
  type: TYPE_NORMAL
- en: and actually there it is right there。 so we can see that we did actually export
    that data frame where we filtered that down to a new CSv file so that was easy
    enough so now let's look at how to read and write to some other formats。 So one
    thing that you might run into is a tabbedlim file these are almost exactly the
    same thing as CSV files。
  prefs: []
  type: TYPE_NORMAL
- en: but instead of your data being separated by a comma the data is instead separated
    by tabs so to do this。pas， we're still going to use the same CSV methods that
    we've already seen。 but we're going to pass in a custom separator so we can write
    to a tabbedlim file just by changing the file extension here to do TSV and I'm
    also going to specify a separator argument。 So I'm going to sayep SP。 and then
    you want to pass in your separator。
  prefs: []
  type: TYPE_NORMAL
- en: Now you can pass in anything here if you want you know a file that is separated
    by hashes or anything。 but commas and tabs are probably the most common。 So I'm
    going to put a backslash T there because that's how we specified tabs in Python。
    And now if I run this cell。 I'm going to go back to our data directory here。 we
    can see that now we have this modified do TSv。 if I click on that then we can
    see that now this looks almost exactly the same as the comma separated file。
  prefs: []
  type: TYPE_NORMAL
- en: but now we have tabs here instead of commas。Now， if you're reading in tab CSV
    files。 then all you need to do is take this SP equal to backslash T。 and you can
    just add that as an argument up here to read CSV， so it's basically the same thing。Okay。
    so now let's move on to some other file formats。 Now。
  prefs: []
  type: TYPE_NORMAL
- en: a very popular file format when working with this kind of data is Excel。 Now。
    if we want to write to Excel， then we're going to need to Pip install a couple
    of packages。 So I have my terminal open with the current environment that I am
    using this is my Jupyter notebook running here。 let me grab my other terminal。
    So I have the same environment that I'm using within Jupiter。
  prefs: []
  type: TYPE_NORMAL
- en: You want to be sure that you're using that same environment so that your Pip
    installing in the right location。 and now we're going to install a couple of packages。
    So first I'm going say Pip install。 and this is Xl W T。 So Xwt will write to an
    older Xls Excel format。 but if you want to write to a newer Excells X Excel format。
  prefs: []
  type: TYPE_NORMAL
- en: then we'll also need to install open pi XL。 and you can p install multiple。😊，packageages。
    but just by list them all right here。 And finally， we want， if we want to read
    Excel files。 then we can install the X LRD package。 So I think that is the three
    packages that we're going to need in order to work with Excel files here。 So I'll
    go ahead and install all of those and let those finish。And once those are installed。
  prefs: []
  type: TYPE_NORMAL
- en: let's go back to our notebook。 And now let's try to write to an Excel file。
    So to write to an Excel file， I'm just going to write the same modified data frame
    that we have here。 And we are going to use the two underscore Excel method。 and
    this is just as easy as passing in。 let's say I'll save it in that data folder
    again。 I'll call this modified dot Xls X。
  prefs: []
  type: TYPE_NORMAL
- en: So I'm going to write to the newer Excel format。 So if I run this。 then it might
    take a second here for this to work because it's actually creating this Excel
    file on the back end。 So let's let this finish and we can tell it's finished when
    this turns from an asterisk to a number here。Okay， so once that's finished， let's
    flip over to our data folder here and we can see that we do have that dot Xlsx
    file。
  prefs: []
  type: TYPE_NORMAL
- en: Now this likely won't open up in Jupiter because this is an Excel file。 We can
    see here that we can't open this up in the browser。 We actually need Excel。 So
    let me open up my finder window here。 I have this open down here。 And I am within
    this data folder。 and we can see that we have our modified dot Xsx file here。
  prefs: []
  type: TYPE_NORMAL
- en: Now I don't actually have Excel on this machine。 I have numbers。 So I'm going
    to open this up in numbers。 It should basically be the same on Windows。 but you
    can just open it up with Excel。 Now， again。 this might take a second to open up
    because we do still have a lot of rows here in this data。Okay。
  prefs: []
  type: TYPE_NORMAL
- en: so we've got this opened up in Excel。 again， I'm on numbers because I'm on a
    Mac and I don't have Excel installed。 but it should open up find in Excel as well。
    Let me zoom in a little bit here。 So we can see and we can format these if we
    need to。 So， for example。 we can change the column sizes here so that all these
    fit in but we can see here that we have our respondent Is。
  prefs: []
  type: TYPE_NORMAL
- en: if I look over at country， we can see that it did export the filtered data frame
    that we were hoping to export。 So everything looks good here。 Now there are also
    some more advanced things that we can do with Excel as well。 if you're familiar
    with Excel。 then you might know that we have the concept of different sheets where
    we can have multiple spreadsheets and one Excel file。 And if you want to read
    or write to a specific sheet。
  prefs: []
  type: TYPE_NORMAL
- en: then you can pass in a sheet argument to these methods actually I trying to
    scroll over to my notebook here。Let me scroll down here to the bottom。 So like
    I was saying。 if you want to read or write to a specific sheet， then you can pass
    in a sheet argument to these methods。 And there's also a way to start from different
    columns and rows as well。
  prefs: []
  type: TYPE_NORMAL
- en: But I'm not going go into all these little details here。 If you Google this
    method name to Excel。 Then you can find the arguments that you can pass in and
    all the additional details in the documentation。 So for now， let's go ahead and
    move on and see how that we can read in that same Excel file that we just created
    and make sure that this works。 Now， by default， it's going to load in with a default
    index， just like when we read a Cv file。
  prefs: []
  type: TYPE_NORMAL
- en: So we'll have to specify that we want our index column to be that respondent
    column。 So to do that。 I'm just going to call this test since we're going to be
    creating a new data frame here from that Excel file that we just created。 And
    we're going to use the read underscore Excel method here。 and now we。I want to
    pass in the location， and I'll just go ahead and copy this here。
  prefs: []
  type: TYPE_NORMAL
- en: So that is modified Xl S X on my machine。 And now I'm going to set that index
    column equal to and that was respondent on your data that might be different。
    but I want my index column to be equal to that respondent。 So I'm going to run
    that cell and load that in and then I'm going to look at that test data frame。
    Now before I run this， I'm going to make sure that this finishes processing here
    and that this asterisk goes away again。
  prefs: []
  type: TYPE_NORMAL
- en: it can take some time because it's actually you know loading in that data from
    Excel now。 which is a little more tricky than loading it in from a CSv。 So now
    if we look at that test data frame， let me just look at the head here instead
    of looking at the whole thing。 So if I look at the head， then we can see that
    we have the same data frame here that we had up here。
  prefs: []
  type: TYPE_NORMAL
- en: So that was exported to Excel and imported correctly。 Okay。 so now let's cover
    some other popular file。Formats now JSson is also really popular for this kind
    of data。 So let's take a look at that。 First， let's write our modified data frame
    to a Json file。 Now for writing to a Json file， then we can use the two Json method。
  prefs: []
  type: TYPE_NORMAL
- en: So you're probably starting to see a pattern here。 These method names are very
    straightforward。 Now this one is a bit different since there are some different
    orientations that we can use for Json。 So just by using the default arguments。
    I can just say so that was India Df do2 underscore Json。 and now I'll pass in
    a file location here。 but instead of an Excel file， we want a Json file。
  prefs: []
  type: TYPE_NORMAL
- en: Now I'm just going to use the default arguments for now。 and then I'll show
    you how we can change this up a bit。 So if I run this。 we can see that ran very
    quickly。 if I go back to my data folder here。 Then now we have this JSson file。
    if I look within here。Okay。
  prefs: []
  type: TYPE_NORMAL
- en: that took just a second to open up on my machine。 again， we do have a lot of
    data in here。 But if we look in here， then we can see that this is very dictionarylike。
    So we have a main branch key here and then the value for that key are all of the
    responses just for that column And if I was to scroll down here then I would be
    able to find the other keys and the other responses as well。 So by default， this
    is a dictionary like Json。 Now there are also different ways that we can write
    Json files。
  prefs: []
  type: TYPE_NORMAL
- en: again， I'm not going to go into every single little detail here。 but let's say
    that we wanted this Json to be list like instead of dictionary like which is how
    it is by default。 So to do this， we can change the orient argument。 So instead
    let's add one here to our arguments。 And I'm going to say Orient is equal to。
    And if we pass in records。And lines equal to true。
  prefs: []
  type: TYPE_NORMAL
- en: Then this will now make this records like， which is list like。 And this lines
    equal to true。 Let me spell that right。 Well just make each of these on a new
    line。 So it might be a little bit easier to read。 Now， if you want to see the
    exact arguments that you can pass into Orient。 then again， just look up pandas
    to Json method。 and it'll take you to the documentation with all the different
    things that you can pass in here。
  prefs: []
  type: TYPE_NORMAL
- en: So let me run this。 And now let's go back and reload our Json file to see how
    this looks。 And now what we have here is more list like。 So before we had a single
    dictionary where the values were a list of all of the responses。 But now we have
    one response at a time。 So we have the main branch。 And then So this is actually
    this first one here。 If I scroll down。
  prefs: []
  type: TYPE_NORMAL
- en: we can see that this is the second。😊，This is actually the entire first response。
    So we have the main branch and then that answer and then open source and then
    that answer。 and then so on。 And we can see here that for the country， we have
    India。 and each response within this survey is actually on a different line。
  prefs: []
  type: TYPE_NORMAL
- en: So that's a little bit different than how it was before。 but there's just different
    ways that we can export these Json files depending on your needs。 Okay。 so now
    that we've written our data to JSson files。 Now let's also read this JSson file
    so that we can make sure that we know how that's done as well。
  prefs: []
  type: TYPE_NORMAL
- en: Now， since we wrote the Json file with these different arguments here。 then
    we need to use those same arguments when we read the data in as well。 So if you're
    reading in Json files and have any issues。 then you might need to play around
    with the different arguments to fit the data that you're trying to read in。
  prefs: []
  type: TYPE_NORMAL
- en: So in this case， I'm just gonna copy this whole line here。 And I'm going say
    test is equal to。And actually， let me just grab this part。And I'll say PDd。 read
    underscore Json。 and now I'll pass in all those arguments here。So we are reading
    the JSON file from this location。 We know that the Orient is list like instead
    of dictionary like and that all of these are on new lines。
  prefs: []
  type: TYPE_NORMAL
- en: And again， depending on your JSsonN data， you might need to go in and change
    these around depending on how your data looks。 So if I run this， then let's see。If
    we have the same data that we exported before。 and it seems like we do， this looks
    exactly like it did whenever we exported this data。 Okay。 so now the last file
    format that we're gonna look at。
  prefs: []
  type: TYPE_NORMAL
- en: let's learn how we can read and write data from SQL databases。 Now。 this is
    probably the most complicated simply because you have to have the database set
    up and all of that good stuff。 But for the sake of this video， I'm going to assume
    that you already have a database with the correct credentials to log into that
    database。 So I have a Postgres database set up on my machine that will be reading
    and writing to。 So first。
  prefs: []
  type: TYPE_NORMAL
- en: let's see how we would connect to this database。 Now， just like with Excel。
    We're going to need to install a package to do this。 So let me bring up my terminal
    here。 I'll close this numbers file here。 Let's see， let me try to quit out of
    this， actually。 I'll just minimize it。Having trouble shutting down。 Okay。
  prefs: []
  type: TYPE_NORMAL
- en: so let me go back to the terminal that I hope opens to where I can install some
    different packages。 And that's my Jupiter notebook。 Where's my other terminal。
    Here we go。 Okay。 so to connect to our database。 We're going to want to install
    SQL alchemy。 And this is a very popular O RM for Python that allows us to more
    easily work with databases。
  prefs: []
  type: TYPE_NORMAL
- en: If you don't know what an O RM is， it stands for object relational mapper。 And
    it's just a way for us to use Python objects in order to connect to a database。
    I plan on doing a complete video or a complete series on SQL alchemy in the future。
    But for now。 let's go ahead and just install this。 So this is Pip install S QL。Alchemy。And
    I'll install that。
  prefs: []
  type: TYPE_NORMAL
- en: And depending on the database that you're using， you might not need to do anything
    else here。 So。 for example， if you're using SQLite or something like that。 But
    since I'm using a Postgres database for this tutorial。 I also need to install
    the psycho P G2 package that allows us to work with Postgres。
  prefs: []
  type: TYPE_NORMAL
- en: I'm not sure if that's actually how you say that package name。 But that's what
    I've always called it。 So Pip install and to install this package to work with
    Postgres。 it's psycho Pg2 dash binary。 So I'll install that， And with those packages
    installed。 let's go back to our notebook and see if we can connect to this database
    using SQL alchemy。
  prefs: []
  type: TYPE_NORMAL
- en: So first， we're going to want to import everything that we need。 So from。SQL
    alchemy。I'm going to want to import their create engine， and this will allow us
    to connect to the database。 Now I'm also going to want to import psychoPg2。 So
    let me run this cell。 and now that those are imported， we should be able to create
    the engine。
  prefs: []
  type: TYPE_NORMAL
- en: which is basically our database connection。 And again。 I'm going to assume that
    you've already created this database and have a username and password。 So to create
    this， I can say engine is equal to。And use that create engine function that we
    just imported from SQL alchemy and now we need our Postgres connection string
    Now if you don't know how to make Postgres connection strings then you know they
    have this available on the SQL alchemy site as well let me make sure I spelled
    this correctly that is PostgresqL and then we want to pass in the username and
    password for our database now for my case I just made a user of Db user and a
    password of Db pass now another thing here that I'd like to mention is that you
    probably shouldn't put credentials within code like this I'll leave a link in
    the description section below where I show how in Python you should use you something
    like environment variables or a config file to hide this information but for the
    sake of this tutorial I'm just going to put it directly in here。
  prefs: []
  type: TYPE_NORMAL
- en: but if you're doing。And in production code， I would highly recommend using environment
    variables so that。 you know you don't expose your username and passwords within
    your code base。 Okay。 so there we have our username and password。 And now the
    database that we want to connect to So this is on local host。 This is on my local
    machine。 It's running on port 5，4，3，2 and now the name of the database。
  prefs: []
  type: TYPE_NORMAL
- en: Now I have Pg admin open here where I can see my databases。 and we can see that
    I've just created an empty database here called sample underscore Db。 So that
    is the database that I'm going to connect to Okay so if I typed everything correctly
    here。 then I should be able to get a connection to that database。
  prefs: []
  type: TYPE_NORMAL
- en: So now let's try to write our modified database frame to a table in this database。
    and this table doesn't need to currently exist。 by default， it will create this
    table for us。 if it does already exist。 then we'll need to add another argument
    to handle that。But we'll see that in just a second。 So to do this， I can just
    say India underscore Df。
  prefs: []
  type: TYPE_NORMAL
- en: which is the data frame we want to export。 Then this is to underscore S Q O。
    And now the table that we want to write this data too。 I'm just going to call
    this sample underscore table。 Now， again， this doesn't currently exist。 but it
    should create it。 And now we need to pass in our database connection here。
  prefs: []
  type: TYPE_NORMAL
- en: I called mine engine。 So let's pass that in。 And if I run this， let's see if
    this works。Okay。 so we didn't get any errors whenever I read that or whenever
    I wrote that。 But now let's go back to my PG admin here。 and let's see if I can
    see this table。 So first。 I'm just gonna right click and refresh。 I like to do
    that anytime I've made any changes。
  prefs: []
  type: TYPE_NORMAL
- en: And we can see there here that we have a sample table down here。 I'm going to
    right click on that and go to view and edit data and look at all the rows here。And
    we can see it does look like this worked。 I know that this is probably a little
    difficult to see on my screen here。 but we have all of our data written here into
    the database。 Okay。
  prefs: []
  type: TYPE_NORMAL
- en: so that's good that we were able to get this from pandas into SQL。 But now what
    if we updated our data and wanted to rewrite that data to this database。 So let's
    go back to our notebook and see what this would look like。 Now。 if I try to run
    this same line again where we export this to SQL。
  prefs: []
  type: TYPE_NORMAL
- en: then we're actually going to get it an error because this table already exists。
    If you want to write over a table， then we can add in an additional argument。
    And the argument that we want to add in is called if underscore exists equals。
    And now what we want to do if this table already exists。 Now， in my case。
  prefs: []
  type: TYPE_NORMAL
- en: I'm just going to replace that table with our new data。![](img/77e06e113266309258f73542889b4603_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: But there are also other options as well。 we could have it throw an error we
    could which is what it does by default。 we could also append data to a table。
    So if you're doing like a daily script where you're analyzing information then
    you can just append that daily data to your existing table but for this example。
  prefs: []
  type: TYPE_NORMAL
- en: I'm just going to have this replace the table So let's run this and once this
    is finished processing。 then I will go back to PG admin now again， let me come
    up here and refresh this and dig back down into the database and let me close
    this view here and let's see if we still have this data Okay so we can see that
    this worked we were able to rerun that command and it just replaced that data
    that was in that existing table with our new data In this case it was the same。
  prefs: []
  type: TYPE_NORMAL
- en: Da， but that's how you would do that。 Okay， so lastly。 now that we've seen how
    to add our data to a database。 Now let's see how we can read in this same data
    using SQL。 Now if you skip to this part of the video using the timestamps in the
    description sections below。
  prefs: []
  type: TYPE_NORMAL
- en: Then please go back to when we wrote data to our database and see how I set
    up this database connection here because we're going to reuse that same connection
    to read in our data。 Okay， so this is pretty simple now that we actually have
    this database connection set up to do this。
  prefs: []
  type: TYPE_NORMAL
- en: we can just say I'll call this SQl underscore Df。 and we will just say PD do
    read underscore SQl and now we want to pass in the table that we're going to read
    from。 And that was sample， underscore table and now pass in our database connection。
    My connection here。
  prefs: []
  type: TYPE_NORMAL
- en: I called engine。And also， I'm also going to pass in an index column just like
    we did when we read in our CSv。 So I'll say index column is equal to， and that
    is going to be this respondent row right here for your data that might be different。
    So whatever you want to be your index just pass it in there if you want pandas
    to just do a default index。 then you can just leave this off entirely。 Okay， so
    if I run this then let's look at SQL Df dot head to make sure this worked。
  prefs: []
  type: TYPE_NORMAL
- en: And we can see that that worked well， We still have the same data frame here
    that we started off with where we filter down these countries to just be the results
    from India。 Now there might be instances where you don't want to load in an entire
    table but you want to run a specific SQL query in order to load in our data to
    do this。
  prefs: []
  type: TYPE_NORMAL
- en: we can use the method， read， underscore SQL， underscore query。To run a specific
    S Ql query。 So let me just copy what I did here。And paste this down here。 And
    now instead of reading in this entire table， I'm going to actually run a query
    here。 So I'll do read underscore SQL underscore query。 And now instead of the
    table name here。
  prefs: []
  type: TYPE_NORMAL
- en: I'm actually going pass in a SQL query。 Now I'm just going to load in everything
    here。 So I'll say select star from sample underscore table and everything else
    here is going to be the same。 we're still have we still have our database connection
    and we still want our index column to be equal to respondent。 So this is still
    going to grab all the rows， but if you wanted to customize this。
  prefs: []
  type: TYPE_NORMAL
- en: then you could add in a where clause here to filter this down。 So let me run
    this。 And now let's look at our SQL data frame here。 And we can see that that
    worked as well。 So we load in this data using a SQL query instead of just reading
    in the entire table。 So that can be especially useful。You're working with large
    databases where you only want to load in specific data using a query。
  prefs: []
  type: TYPE_NORMAL
- en: Okay， so we're just about finished up here。 But let me show you one more tip
    before we wrap this up So you may have seen people load in data using a URL instead
    of a specific file for some of the methods that we've looked at before。 and we
    can do that。 all you need to do is you need to be sure that you're using the correct
    method for whatever form of data is on the URL。
  prefs: []
  type: TYPE_NORMAL
- en: So， for example， in my flask and django series， I created a Json file of some
    sample posts for the website that werereating in that series。 And I have that
    Json file on my Github page。 Now， if I wanted to bring that into pandas。 then
    I could simply use the read Json method and then pass in that URL。 I wouldn't
    actually have to download that Json first and then pass it in that way。
  prefs: []
  type: TYPE_NORMAL
- en: So I have this open here。 if you didn't know on Github， you can look at the
    raw files。 So we can see that this is a long。URL here， but I will have this code
    posted in the description section below if you'd like to follow along。 So I'm
    just going to copy this URL。 and this isn't on my file system。 And now let's see
    if we can just load this in。 So I'm going to call this post underscore Df。
  prefs: []
  type: TYPE_NORMAL
- en: And I'll set this equal to P D dot read underscore Json since this is Json on
    the URL。 If it was Cv。 then you'd want to use read CSv and so on。 So now I can
    just paste in that URL there。 And now let's just run that cell。 And we can see
    that we didn't get any errors。 So let me。Now。I'll look at the head。Of our data
    frame here。 And we can see that I do have my sample posts here。
  prefs: []
  type: TYPE_NORMAL
- en: These are the sample posts that I used on that website series。 So depending
    on the data in that URL。 you should be able to use the methods that we've seen
    to load in data from a URL just like we did here。 Now， before we end here， I would
    like to thank the sponsor of this video and that is brilliant。 I really enjoy
    the tutorials that brilliant provides and would definitely recommend checking
    them out。
  prefs: []
  type: TYPE_NORMAL
- en: brilliant is a problem solving website that helps you understand underlying
    concepts by actively working through guided lessons。 and brilliant would be an
    excellent way to supplement what you learn here with their handson courses。 They
    have some excellent courses and lessons on data science that do a deep dive on
    how to think about and analyze data correctly。 So if you're watching my panda
    series because you're getting into the data science field。
  prefs: []
  type: TYPE_NORMAL
- en: then I would highly recommend also checking out brilliant and seeing what other
    data science skills you can learn。 They even use Python in their statistics course
    and will quiz you on how to correctly。😊。![](img/77e06e113266309258f73542889b4603_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77e06e113266309258f73542889b4603_4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/77e06e113266309258f73542889b4603_5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/77e06e113266309258f73542889b4603_6.png)'
  prefs: []
  type: TYPE_IMG
- en: ize the data within the language。 Their guided lessons will challenge you。 but
    you also have the ability to get hints or even solutions if you need them。 It's
    really tailored towards understanding the material。 So to support my channel and
    learn more about brilliant。
  prefs: []
  type: TYPE_NORMAL
- en: You can go to brilliant org forms to sign up for free and also the first 200
    people to go to that link will'll get 20% off the annual premium subscription
    and you can find that link in the description section below again。 that's brilliant
    org so I think that's going to do it for this pandas video。
  prefs: []
  type: TYPE_NORMAL
- en: I hope you feel like you got a good idea for how to read and write data from
    multiple different sources。 what we covered here should cover the vast majority
    of file formats that you're going to be seeing and using in the data science field。
    Now I'm probably going to take a break from this pandas series after this video
    and do a few one off videos that I've been wanting to cover。 but I know that there
    are a lot of topics in pandas left to cover and I will get around to those more
    advanced topics in future video。
  prefs: []
  type: TYPE_NORMAL
- en: 😊。![](img/77e06e113266309258f73542889b4603_8.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77e06e113266309258f73542889b4603_9.png)'
  prefs: []
  type: TYPE_IMG
- en: But in the meantime， if you'd like a good source for learning pandas。 then I
    would highly recommend checking out the channel Data school。 That's run by Kevin
    Markham。 and he's done the panda tutorials at Pycon for several years now he didn't
    ask me to suggest his channel or anything like that。 I just think that he does
    a good job。 And his channel is actually completely devoted to pandas and data
    science。
  prefs: []
  type: TYPE_NORMAL
- en: So he's already covered some of the more advanced topics that I do plan to cover
    in future videos。 But if anyone has any questions about what be covered in this
    video then feel free to ask in the comment section below and I'll do my best to
    answer those。
  prefs: []
  type: TYPE_NORMAL
- en: And if you enjoy these tutorials and would like to support them then there are
    several ways you can do that。 the easiest ways to simply like the video and give
    it a thumbs up and also it's a huge help to share these videos with anyone who
    you think would find them useful And if you have the means you can contribute
    through Patreon and there's a link to that page in the description section below。
  prefs: []
  type: TYPE_NORMAL
- en: be sure to subscribe for future videos。 and thank you all for watching。![](img/77e06e113266309258f73542889b4603_11.png)
  prefs: []
  type: TYPE_NORMAL
