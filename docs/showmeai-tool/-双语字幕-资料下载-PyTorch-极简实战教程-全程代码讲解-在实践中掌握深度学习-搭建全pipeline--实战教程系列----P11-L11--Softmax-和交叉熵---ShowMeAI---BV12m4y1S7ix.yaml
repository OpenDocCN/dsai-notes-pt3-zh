- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P11ÔºöL11-
    Softmax Âíå‰∫§ÂèâÁÜµ - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome back to a new Pytorch tutorial„ÄÇ This time„ÄÇ we talk about
    the soft maxs function and the cross entropy loss„ÄÇ These are one of the most common
    functions used in neural networks„ÄÇ So you should know how they work„ÄÇ NowÔºå I will
    teach you the math behind these functions and how we can use them in nuy and then
    pytorrch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And at the endÔºå I will show you how a typical classification neural network
    with those functions look like„ÄÇ So let's start„ÄÇüòäÔºåAnd this is the formula of the
    soft maxs„ÄÇ So it applies the exponential function to each element and normalizes
    it by dividing by the sum of all these exponentials„ÄÇ So what it doesÔºå it basically
    squashes the output to be between 0 and 1„ÄÇ So we get probabilities„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look at an example„ÄÇüòäÔºåLet's say we have a linear layerÔºå which
    has three output values„ÄÇ And these values are so called scores or locks„ÄÇ So they
    are raw values„ÄÇAnd then we apply the soft marks and get probabilities„ÄÇSo each
    value is squash to be between 0 and 1„ÄÇAnd the highest value here gets the highest
    probability here„ÄÇAnd yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if we sum these three probabilities upÔºå then we get one„ÄÇAnd then this is our
    prediction„ÄÇ And then we can choose for the„ÄÇClasss with the highest probability„ÄÇSoÔºå
    yeah„ÄÇ that's how the soft mark works„ÄÇ And now let's have a look at the code„ÄÇ So
    here I already implemented it in numpy„ÄÇ So we can calculate this in one line„ÄÇ
    So first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have the exponentialÔºå and then we divide by the sum„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: Over all these exponentials„ÄÇAnd„ÄÇNowÔºå let's run this„ÄÇ This has the same values
    as in my slide„ÄÇ And then here we also see that the highest valueÔºå the highest
    loet has the highest probability„ÄÇI rounded them them in my slides„ÄÇ So it's slightly
    different„ÄÇ but basically„ÄÇ we see that it's correct„ÄÇAndÔºå of courseÔºå we can also
    calculate it in piytorrch„ÄÇ And for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we create a tenzoar„ÄÇ So let's say x equals torch dot tenzor„ÄÇ and it gets the
    same values as this one„ÄÇAnd then we can say outputs equals torch dot soft max
    of x„ÄÇ And we also must specify the dimensions„ÄÇ So we say dim equals 0„ÄÇ So it computes
    it along the first axis„ÄÇ And now let's print these outputs„ÄÇSo yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: here we see that the result is almost the same„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå this works„ÄÇAnd now let's continues„ÄÇ So a lot of times„ÄÇ the soft max function
    is combined with the so called cross entropy loss„ÄÇSo this measures the performance
    of our classification model„ÄÇ whose output is a probability between 0 and 1„ÄÇAnd
    it can be used in multi class problems„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and the loss increases as the predicted probability diverges from the actual
    label„ÄÇSo the better our predictionÔºå the lower is our loss„ÄÇ So here we have two
    examples„ÄÇSo here„ÄÇ this is a good prediction„ÄÇ And then we have a low cross entropy
    loss„ÄÇ And here this is a bad prediction„ÄÇ And then we have a high across entropy
    loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what we also must know is that in this caseÔºå our y must be hotÔºå one hot
    and coated„ÄÇSo let's say we have three pro three possible classesÔºå class 0Ôºå1 and
    2„ÄÇ And in this case„ÄÇ the correct label is the class0„ÄÇ So here we must put a one
    and for all the other classes„ÄÇ we must put a 0„ÄÇ So this is how we do one hot encoding„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then for the predicted y we must have probabilities„ÄÇ SoÔºå for example„ÄÇ we
    applied the soft maxs here before„ÄÇAnd yeahÔºå so now againÔºå let's have a look at
    the coat„ÄÇ how we do this in nuy„ÄÇ So we can calculate this here„ÄÇ So we have the
    sum over the actual labels times„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7ecf0933f8ff6fd629229f5815a80cb_5.png)'
  prefs: []
  type: TYPE_IMG
- en: The lock of the predicted labels„ÄÇ And then we must put a -1 at the beginning„ÄÇAnd
    to„ÄÇ we can also normalize itÔºå but we don't do this here so we could divide it
    by the number of samples„ÄÇAnd then we create our y„ÄÇ So as I saidÔºå this must be
    one hot encoded„ÄÇ So here we have other examples„ÄÇ So if it is class 1Ôºå then it
    must look like thisÔºå for example„ÄÇAnd then down here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we put our two predictions„ÄÇ So these are now probabilities„ÄÇ So the first one
    has a good prediction because also hereÔºå the class 0 has the highest probability„ÄÇAnd
    the second prediction is a bad prediction„ÄÇ So hereÔºå class 0 gets a very low probability„ÄÇ
    and class 2 gets a high probability„ÄÇAnd now then I compute the entropy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the cross entropy and predict both of them„ÄÇ So let's run this„ÄÇ And here we see
    that the first prediction has a„ÄÇLow loss„ÄÇ And the second prediction has a high
    loss„ÄÇAnd now againÔºå let's see how we can do this in Pytorch„ÄÇSo for thisÔºå firstÔºå
    we create„ÄÇThe loss„ÄÇ So we say loss equals NÔºå N from the neural torch and N„ÄÇModuleÔºå
    NÔºå N dot cross entropy loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7ecf0933f8ff6fd629229f5815a80cb_7.png)'
  prefs: []
  type: TYPE_IMG
- en: And now what we must knowÔºå let's have a look at the slides again„ÄÇSo here we
    have to be careful„ÄÇBecause the cross entropys already applies the lock soft marks„ÄÇ
    and then the negative lock likelihood loss„ÄÇ So we should not or must not implement
    the soft marks layer for ourselves„ÄÇSo this is the first thing we must know„ÄÇ And
    the second thing is that here our y must not be one hot encoded„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we should only put the correct class label here„ÄÇAnd alsoÔºå the why predictions
    has raw scores„ÄÇ So no soft mark here„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: So be careful about this„ÄÇ And now let's see this in practice„ÄÇ So let's say„ÄÇ
    let's create our actual labels„ÄÇ And this is a torch dot tenzor„ÄÇ And now here we
    only put the correct class labels„ÄÇ So let's say in this caseÔºå it's class 0„ÄÇ And
    not one hot encoded any more„ÄÇ And then we have a good prediction„ÄÇ Y prediction
    good equals„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Torch dot tenzor„ÄÇ And then here we must be careful about the size„ÄÇ So this has
    the size number of samples times the number of classes„ÄÇ So let's sayÔºå in our case„ÄÇ
    we have one sample and 3 possible classes„ÄÇ So this is an array of arrays„ÄÇ And
    here we put in 2„ÄÇ01„ÄÇ0 and 0„ÄÇ1„ÄÇAnd rememberÔºå thisÔºå these are the raw values„ÄÇ So
    we didn't apply the soft max„ÄÇ And here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the highest„ÄÇOrÂØπ„ÄÇThe class 0 has the highest value„ÄÇ So this is a good prediction„ÄÇ
    And now let's make a bad prediction„ÄÇ So prediction bad„ÄÇ So here„ÄÇ the very first
    value is a lower valueÔºå let's sayÔºå And the second value is high„ÄÇ And let's change
    this also a little bit„ÄÇAnd now we compute our loss like this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now we call the loss function that we created here„ÄÇ and then we put in the
    y prediction and the actual y and the same with our second„ÄÇ Let's compute a second
    loss with y prediction bad and y„ÄÇAnd now let's print them„ÄÇ So let's print L1„ÄÇDot
    item„ÄÇSo it only has one value„ÄÇ so we can call the item function and also L 2 dot
    item„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's run this„ÄÇ And yeahÔºå here we see that our good prediction has a lower
    cross entropy loss„ÄÇ So this works„ÄÇAnd now to get the actual predictionsÔºå we can
    do it like this„ÄÇ So let's say underscore because we don't need this„ÄÇ and then
    predictions„ÄÇDicctions equals torch„ÄÇ dot max„ÄÇ And then here we put in the prediction„ÄÇ
    So why prediction good„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then along the first dimension„ÄÇAnd alsoÔºå the same with the bad one„ÄÇ So let's
    call this prediction 1 and prediction 2„ÄÇÂóØ„ÄÇAnd let's print our predictions„ÄÇ So
    predictions 1 and print predictions 2„ÄÇSo this will here we see that we choose„ÄÇThe
    highest probability„ÄÇ So in this caseÔºå we choose„ÄÇThis oneÔºå and in in the second
    case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we choose this one„ÄÇ So class number one here„ÄÇSo this is how we get the predictions„ÄÇAnd
    what's also very good is that the loss in Pyto allows for multiple samples„ÄÇ So
    let's increase our samples here„ÄÇ So let's say we have three samples„ÄÇ So three
    possible classes„ÄÇThen our tenzo must have three class labelsÔºå our actual y„ÄÇ SoÔºå
    for exampleÔºå2Ôºå0 and 1„ÄÇAnd thenÔºå our„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Predictions must be here of size number of samples times the number of classes„ÄÇ
    So now this is of size 3 by 3„ÄÇSo let's do this„ÄÇ So here we must put in„ÄÇAnother„ÄÇList
    with three values„ÄÇSo„ÄÇÂóØ„ÄÇLike this and like this„ÄÇ So let's say this one is a good
    prediction„ÄÇ So the first„ÄÇClassÔºå the first correct label is class number 2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this one must have the highest valueÔºå and this one must be low„ÄÇ So let's
    say 001„ÄÇ And here„ÄÇ the very first oneÔºå the first class must have a high value„ÄÇ
    So like this„ÄÇAnd then the value in the middle must have the highest raw values„ÄÇ
    SoÔºå for exampleÔºå like this„ÄÇAnd then„ÄÇWe do the same for our bad prediction„ÄÇAnd„ÄÇLet's
    say here we have this one higher„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And also change this a little bit„ÄÇAnd then we can again„ÄÇ compute the cross the
    cross entropys with multiple samples„ÄÇAnd now let's run this„ÄÇ And then we also
    see here againÔºå our first prediction is good and has a low loss„ÄÇ And the second
    one is not so good„ÄÇAnd yeahÔºå here we get the correct predictions from the first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Prediction tenor„ÄÇ So here we also have 2Ôºå0Ôºå1Ôºå like in the actual y„ÄÇSoÔºå yeah„ÄÇ
    this is how we can use the cross entropylos in Pytorch„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: And nowÔºå let's go back to our slides„ÄÇSo now I want to show you how a typical
    neural network looks like„ÄÇ So here is a typical neural net in a multi class classification
    problem„ÄÇ So here we want to find out what animal our image shows„ÄÇ So we have an
    input layer and then some hidden layers and maybe some activation functions in
    between„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then at the endÔºå we have a linear layer with one output for each class„ÄÇSo
    here we have two outputs„ÄÇ And then at the very end„ÄÇ we apply our soft marks and
    get the probabilities„ÄÇSo nowÔºå as I said in Pytorch„ÄÇ we must be careful because
    we use the cross entropy loss here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we must not use the soft max layer in our neural net„ÄÇ So we must not implement
    this for ourselves„ÄÇ So let's have a look at how this code looks„ÄÇ So in a multi
    class„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: And classificationÔºå our netÔºå for example„ÄÇLooks like this„ÄÇ So we define our layers„ÄÇ
    So we have one linear layerÔºå which gets an input size and then a hidden size„ÄÇ
    Then we have activation function in between„ÄÇ And then our last layerÔºå it gets
    the hidden size„ÄÇ and then the output size is the number of classes„ÄÇ So for each„ÄÇPossible
    class„ÄÇ We have one output„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then in the forward method„ÄÇ So here we only apply our layers and then no
    softm here at the very end„ÄÇ And then we create our modelÔºå and then we used the
    cross entropisÔºå which then applies to softm„ÄÇSo yeahÔºå be careful here„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_15.png)
  prefs: []
  type: TYPE_NORMAL
- en: And„ÄÇSo this example also works for more classes„ÄÇ So if our image couldÔºå for
    example„ÄÇ also be a bird or a mouse or whateverÔºå then this is also the correct
    layout„ÄÇ But if we just have a binary classification problem with two possible
    outputs„ÄÇ Then we can change our layer like this„ÄÇ so„ÄÇNowÔºå we rephrase our question„ÄÇ
    So we just say„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: is it a docÔºå Yes or no„ÄÇ And then here at the endÔºå we have a linear layer with
    only one output„ÄÇ And then we do not use the softm functionÔºå but we use the smoid
    function„ÄÇ which then gets a probability„ÄÇ And if this is higher than 05Ôºå then we
    say yes„ÄÇAnd here in Pytorrch„ÄÇ we use the„ÄÇBC E loss or binary cross entropy loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we must implement the sigmoid function at the end„ÄÇ So let's have a look
    at our neural net in a binary classification case„ÄÇ So again here first„ÄÇ we set
    up our layers and our activation functions„ÄÇ And the last layer has the output
    size 1„ÄÇ So this is always fixed in this case„ÄÇ And then in the forward pass Now
    here after we applied our layers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we also must implement the sigmoid function„ÄÇ So yeahÔºå and then here as a criterion„ÄÇ
    we use the binary cross entropy loss„ÄÇ So be very careful here about these two
    different„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_17.png)
  prefs: []
  type: TYPE_NORMAL
- en: Different possible neural nets„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: And yeahÔºå but that's basically what I wanted to show you„ÄÇSo the last structure
    is also what I used in the logistic regression tutorial so you can check that
    out if you haven't already„ÄÇAnd for nowÔºå that's all I wanted to show you„ÄÇ I hope
    you enjoyed it and understood everything„ÄÇ If you have any questionsÔºå leave them
    in the comments below„ÄÇ And if you like this tutorial„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then please subscribe to the channel and see you next timeÔºå bye by„ÄÇ![](img/e7ecf0933f8ff6fd629229f5815a80cb_21.png)
  prefs: []
  type: TYPE_NORMAL
