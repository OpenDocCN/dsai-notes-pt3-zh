- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P11ï¼šL11-
    Softmax å’Œäº¤å‰ç†µ - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P11ï¼šL11-
    Softmax å’Œäº¤å‰ç†µ - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome back to a new Pytorch tutorialã€‚ This timeã€‚ we talk about
    the soft maxs function and the cross entropy lossã€‚ These are one of the most common
    functions used in neural networksã€‚ So you should know how they workã€‚ Nowï¼Œ I will
    teach you the math behind these functions and how we can use them in nuy and then
    pytorrchã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿å›åˆ°æ–°çš„PyTorchæ•™ç¨‹ã€‚è¿™æ¬¡æˆ‘ä»¬å°†è®¨è®ºsoftmaxå‡½æ•°å’Œäº¤å‰ç†µæŸå¤±ã€‚è¿™äº›æ˜¯ç¥ç»ç½‘ç»œä¸­æœ€å¸¸ç”¨çš„å‡½æ•°ä¹‹ä¸€ï¼Œå› æ­¤ä½ åº”è¯¥çŸ¥é“å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ç°åœ¨ï¼Œæˆ‘å°†æ•™ä½ è¿™äº›å‡½æ•°èƒŒåçš„æ•°å­¦åŸç†ï¼Œä»¥åŠå¦‚ä½•åœ¨numpyå’ŒPyTorchä¸­ä½¿ç”¨å®ƒä»¬ã€‚
- en: And at the endï¼Œ I will show you how a typical classification neural network
    with those functions look likeã€‚ So let's startã€‚ğŸ˜Šï¼ŒAnd this is the formula of the
    soft maxsã€‚ So it applies the exponential function to each element and normalizes
    it by dividing by the sum of all these exponentialsã€‚ So what it doesï¼Œ it basically
    squashes the output to be between 0 and 1ã€‚ So we get probabilitiesã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä¼šå‘ä½ å±•ç¤ºä¸€ä¸ªå…¸å‹çš„åˆ†ç±»ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è¿ä½œçš„ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸ˜Šè¿™æ˜¯softmaxçš„å…¬å¼ã€‚å®ƒå°†æŒ‡æ•°å‡½æ•°åº”ç”¨äºæ¯ä¸ªå…ƒç´ ï¼Œå¹¶é€šè¿‡æ‰€æœ‰è¿™äº›æŒ‡æ•°çš„å’Œè¿›è¡Œå½’ä¸€åŒ–ã€‚å› æ­¤ï¼Œå®ƒåŸºæœ¬ä¸Šå°†è¾“å‡ºå‹ç¼©åˆ°0å’Œ1ä¹‹é—´ï¼Œå¾—åˆ°æ¦‚ç‡ã€‚
- en: So let's have a look at an exampleã€‚ğŸ˜Šï¼ŒLet's say we have a linear layerï¼Œ which
    has three output valuesã€‚ And these values are so called scores or locksã€‚ So they
    are raw valuesã€‚And then we apply the soft marks and get probabilitiesã€‚So each
    value is squash to be between 0 and 1ã€‚And the highest value here gets the highest
    probability hereã€‚And yeahã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜Šå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå®ƒæœ‰ä¸‰ä¸ªè¾“å‡ºå€¼ã€‚è¿™äº›å€¼ç§°ä¸ºåˆ†æ•°æˆ–é”å®šå€¼ã€‚å®ƒä»¬æ˜¯åŸå§‹å€¼ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨softmaxå¹¶å¾—åˆ°æ¦‚ç‡ã€‚æ¯ä¸ªå€¼è¢«å‹ç¼©åˆ°0å’Œ1ä¹‹é—´ï¼Œæœ€é«˜çš„å€¼å¾—åˆ°æœ€é«˜çš„æ¦‚ç‡ã€‚
- en: if we sum these three probabilities upï¼Œ then we get oneã€‚And then this is our
    predictionã€‚ And then we can choose for theã€‚Classs with the highest probabilityã€‚Soï¼Œ
    yeahã€‚ that's how the soft mark worksã€‚ And now let's have a look at the codeã€‚ So
    here I already implemented it in numpyã€‚ So we can calculate this in one lineã€‚
    So firstã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†è¿™ä¸‰ä¸ªæ¦‚ç‡ç›¸åŠ ï¼Œå°±å¾—åˆ°1ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ˜¯softmaxçš„å·¥ä½œåŸç†ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ã€‚æˆ‘å·²ç»åœ¨numpyä¸­å®ç°äº†å®ƒã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸€è¡Œå†…è®¡ç®—ã€‚
- en: we have the exponentialï¼Œ and then we divide by the sumã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_1.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰æŒ‡æ•°ï¼Œç„¶åæˆ‘ä»¬é™¤ä»¥æ€»å’Œã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_1.png)
- en: Over all these exponentialsã€‚Andã€‚Nowï¼Œ let's run thisã€‚ This has the same values
    as in my slideã€‚ And then here we also see that the highest valueï¼Œ the highest
    loet has the highest probabilityã€‚I rounded them them in my slidesã€‚ So it's slightly
    differentã€‚ but basicallyã€‚ we see that it's correctã€‚Andï¼Œ of courseï¼Œ we can also
    calculate it in piytorrchã€‚ And for thisã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›æŒ‡æ•°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚è¿™ä¸æˆ‘å¹»ç¯ç‰‡ä¸­çš„å€¼ç›¸åŒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°æœ€é«˜çš„å€¼å…·æœ‰æœ€é«˜çš„æ¦‚ç‡ã€‚æˆ‘åœ¨å¹»ç¯ç‰‡ä¸­å¯¹å®ƒä»¬è¿›è¡Œäº†å››èˆäº”å…¥ï¼Œæ‰€ä»¥ç¨å¾®ä¸åŒï¼Œä½†åŸºæœ¬ä¸Šæ˜¯æ­£ç¡®çš„ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨PyTorchä¸­è®¡ç®—å®ƒã€‚
- en: we create a tenzoarã€‚ So let's say x equals torch dot tenzorã€‚ and it gets the
    same values as this oneã€‚And then we can say outputs equals torch dot soft max
    of xã€‚ And we also must specify the dimensionsã€‚ So we say dim equals 0ã€‚ So it computes
    it along the first axisã€‚ And now let's print these outputsã€‚So yeahã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¼ é‡ã€‚å‡è®¾xç­‰äºtorch.tensorï¼Œå®ƒå¾—åˆ°ä¸è¿™ä¸ªç›¸åŒçš„å€¼ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´outputsç­‰äºtorch.softmax(x)ã€‚æˆ‘ä»¬è¿˜å¿…é¡»æŒ‡å®šç»´åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬è¯´dimç­‰äº0ã€‚è¿™æ˜¯åœ¨ç¬¬ä¸€ä¸ªè½´ä¸Šè®¡ç®—ã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°è¿™äº›è¾“å‡ºã€‚
- en: here we see that the result is almost the sameã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°ç»“æœå‡ ä¹ç›¸åŒã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_3.png)
- en: Soï¼Œ this worksã€‚And now let's continuesã€‚ So a lot of timesã€‚ the soft max function
    is combined with the so called cross entropy lossã€‚So this measures the performance
    of our classification modelã€‚ whose output is a probability between 0 and 1ã€‚And
    it can be used in multi class problemsã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™æœ‰æ•ˆã€‚ç°åœ¨ç»§ç»­ã€‚å¾ˆå¤šæ—¶å€™ï¼Œsoftmaxå‡½æ•°ä¸æ‰€è°“çš„äº¤å‰ç†µæŸå¤±ç»“åˆä½¿ç”¨ã€‚è¿™æµ‹é‡æˆ‘ä»¬çš„åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶è¾“å‡ºæ˜¯0åˆ°1ä¹‹é—´çš„æ¦‚ç‡ï¼Œå¯ä»¥ç”¨äºå¤šç±»é—®é¢˜ã€‚
- en: and the loss increases as the predicted probability diverges from the actual
    labelã€‚So the better our predictionï¼Œ the lower is our lossã€‚ So here we have two
    examplesã€‚So hereã€‚ this is a good predictionã€‚ And then we have a low cross entropy
    lossã€‚ And here this is a bad predictionã€‚ And then we have a high across entropy
    lossã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é¢„æµ‹æ¦‚ç‡ä¸å®é™…æ ‡ç­¾åç¦»æ—¶ï¼ŒæŸå¤±å¢åŠ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„é¢„æµ‹è¶Šå¥½ï¼ŒæŸå¤±è¶Šä½ã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªä¾‹å­ã€‚è¿™æ˜¯ä¸€ä¸ªå¥½çš„é¢„æµ‹ï¼Œäº¤å‰ç†µæŸå¤±è¾ƒä½ã€‚è€Œè¿™æ˜¯ä¸€ä¸ªå·®çš„é¢„æµ‹ï¼Œäº¤å‰ç†µæŸå¤±è¾ƒé«˜ã€‚
- en: And what we also must know is that in this caseï¼Œ our y must be hotï¼Œ one hot
    and coatedã€‚So let's say we have three pro three possible classesï¼Œ class 0ï¼Œ1 and
    2ã€‚ And in this caseã€‚ the correct label is the class0ã€‚ So here we must put a one
    and for all the other classesã€‚ we must put a 0ã€‚ So this is how we do one hot encodingã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¿…é¡»çŸ¥é“çš„æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ y å¿…é¡»æ˜¯çƒ­çš„ï¼Œç‹¬çƒ­ç¼–ç ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸‰ä¸ªå¯èƒ½çš„ç±»åˆ«ï¼Œç±»åˆ« 0ã€1 å’Œ 2ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­£ç¡®çš„æ ‡ç­¾æ˜¯ç±»åˆ« 0ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å¿…é¡»æ”¾ä¸€ä¸ª
    1ï¼Œæ‰€æœ‰å…¶ä»–ç±»åˆ«æˆ‘ä»¬å¿…é¡»æ”¾ 0ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•è¿›è¡Œç‹¬çƒ­ç¼–ç ã€‚
- en: And then for the predicted y we must have probabilitiesã€‚ Soï¼Œ for exampleã€‚ we
    applied the soft maxs here beforeã€‚And yeahï¼Œ so now againï¼Œ let's have a look at
    the coatã€‚ how we do this in nuyã€‚ So we can calculate this hereã€‚ So we have the
    sum over the actual labels timesã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé¢„æµ‹çš„ yï¼Œæˆ‘ä»¬å¿…é¡»æœ‰æ¦‚ç‡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¹‹å‰åœ¨è¿™é‡Œåº”ç”¨äº†è½¯æœ€å¤§å€¼ã€‚ç°åœ¨å†æ¬¡ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ã€‚æˆ‘ä»¬å¦‚ä½•åœ¨ nuy ä¸­åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œè®¡ç®—ã€‚å› æ­¤æˆ‘ä»¬æœ‰å®é™…æ ‡ç­¾çš„æ€»å’Œä¹˜ä»¥ã€‚
- en: '![](img/e7ecf0933f8ff6fd629229f5815a80cb_5.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7ecf0933f8ff6fd629229f5815a80cb_5.png)'
- en: The lock of the predicted labelsã€‚ And then we must put a -1 at the beginningã€‚And
    toã€‚ we can also normalize itï¼Œ but we don't do this here so we could divide it
    by the number of samplesã€‚And then we create our yã€‚ So as I saidï¼Œ this must be
    one hot encodedã€‚ So here we have other examplesã€‚ So if it is class 1ï¼Œ then it
    must look like thisï¼Œ for exampleã€‚And then down hereã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ ‡ç­¾çš„é”å®šã€‚ç„¶åæˆ‘ä»¬å¿…é¡»åœ¨å¼€å¤´æ”¾ä¸€ä¸ª -1ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½†æˆ‘ä»¬åœ¨è¿™é‡Œä¸è¿™æ ·åšï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†å…¶é™¤ä»¥æ ·æœ¬æ•°é‡ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„ yã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œè¿™å¿…é¡»æ˜¯ç‹¬çƒ­ç¼–ç ã€‚å› æ­¤è¿™é‡Œæˆ‘ä»¬æœ‰å…¶ä»–ç¤ºä¾‹ã€‚å¦‚æœæ˜¯ç±»åˆ«
    1ï¼Œåˆ™å¿…é¡»åƒè¿™æ ·ï¼Œä¾‹å¦‚ã€‚ç„¶ååœ¨ä¸‹é¢ã€‚
- en: we put our two predictionsã€‚ So these are now probabilitiesã€‚ So the first one
    has a good prediction because also hereï¼Œ the class 0 has the highest probabilityã€‚And
    the second prediction is a bad predictionã€‚ So hereï¼Œ class 0 gets a very low probabilityã€‚
    and class 2 gets a high probabilityã€‚And now then I compute the entropyã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé¢„æµ‹ã€‚è¿™äº›ç°åœ¨æ˜¯æ¦‚ç‡ã€‚ç¬¬ä¸€ä¸ªé¢„æµ‹æ˜¯å¥½çš„ï¼Œå› ä¸ºåœ¨è¿™é‡Œï¼Œç±»åˆ« 0 æ‹¥æœ‰æœ€é«˜çš„æ¦‚ç‡ã€‚ç¬¬äºŒä¸ªé¢„æµ‹æ˜¯å·®çš„ã€‚åœ¨è¿™é‡Œï¼Œç±»åˆ« 0 çš„æ¦‚ç‡éå¸¸ä½ï¼Œè€Œç±»åˆ« 2
    çš„æ¦‚ç‡å¾ˆé«˜ã€‚ç„¶åæˆ‘è®¡ç®—ç†µã€‚
- en: the cross entropy and predict both of themã€‚ So let's run thisã€‚ And here we see
    that the first prediction has aã€‚Low lossã€‚ And the second prediction has a high
    lossã€‚And now againï¼Œ let's see how we can do this in Pytorchã€‚So for thisï¼Œ firstï¼Œ
    we createã€‚The lossã€‚ So we say loss equals Nï¼Œ N from the neural torch and Nã€‚Moduleï¼Œ
    Nï¼Œ N dot cross entropy lossã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰ç†µå’Œé¢„æµ‹ä¸¤ä¸ªã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°ç¬¬ä¸€ä¸ªé¢„æµ‹æœ‰ä¸€ä¸ªä½æŸå¤±ï¼Œè€Œç¬¬äºŒä¸ªé¢„æµ‹æœ‰ä¸€ä¸ªé«˜æŸå¤±ã€‚ç°åœ¨å†æ¬¡ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ Pytorch ä¸­åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºæŸå¤±ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´æŸå¤±ç­‰äº
    Nï¼ŒN æ¥è‡ªç¥ç»ç½‘ç»œç«ç‚¬å’Œ Nã€‚æ¨¡å—ï¼ŒNï¼ŒN dot äº¤å‰ç†µæŸå¤±ã€‚
- en: '![](img/e7ecf0933f8ff6fd629229f5815a80cb_7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7ecf0933f8ff6fd629229f5815a80cb_7.png)'
- en: And now what we must knowï¼Œ let's have a look at the slides againã€‚So here we
    have to be carefulã€‚Because the cross entropys already applies the lock soft marksã€‚
    and then the negative lock likelihood lossã€‚ So we should not or must not implement
    the soft marks layer for ourselvesã€‚So this is the first thing we must knowã€‚ And
    the second thing is that here our y must not be one hot encodedã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¿…é¡»çŸ¥é“çš„äº‹æƒ…æ˜¯ï¼Œè®©æˆ‘ä»¬å†æ¬¡æŸ¥çœ‹å¹»ç¯ç‰‡ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¿…é¡»å°å¿ƒã€‚å› ä¸ºäº¤å‰ç†µå·²ç»åº”ç”¨äº†é”å®šçš„è½¯æ ‡è®°ï¼Œç„¶åæ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸åº”è¯¥æˆ–ä¸èƒ½è‡ªå·±å®ç°è½¯æ ‡è®°å±‚ã€‚è¿™æ˜¯æˆ‘ä»¬å¿…é¡»çŸ¥é“çš„ç¬¬ä¸€ä»¶äº‹ã€‚ç¬¬äºŒä»¶äº‹æ˜¯è¿™é‡Œæˆ‘ä»¬çš„
    y ä¸èƒ½æ˜¯ç‹¬çƒ­ç¼–ç ã€‚
- en: so we should only put the correct class label hereã€‚And alsoï¼Œ the why predictions
    has raw scoresã€‚ So no soft mark hereã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_9.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åº”è¯¥åªåœ¨è¿™é‡Œæ”¾æ­£ç¡®çš„ç±»åˆ«æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œä¸ºä»€ä¹ˆé¢„æµ‹æœ‰åŸå§‹åˆ†æ•°ã€‚æ‰€ä»¥è¿™é‡Œæ²¡æœ‰è½¯æ ‡è®°ã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_9.png)
- en: So be careful about thisã€‚ And now let's see this in practiceã€‚ So let's sayã€‚
    let's create our actual labelsã€‚ And this is a torch dot tenzorã€‚ And now here we
    only put the correct class labelsã€‚ So let's say in this caseï¼Œ it's class 0ã€‚ And
    not one hot encoded any moreã€‚ And then we have a good predictionã€‚ Y prediction
    good equalsã€‚ğŸ˜Šã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¦å°å¿ƒè¿™ä¸€ç‚¹ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å®é™…æƒ…å†µã€‚å‡è®¾æˆ‘ä»¬åˆ›å»ºå®é™…æ ‡ç­¾ã€‚è¿™æ˜¯ä¸€ä¸ª torch dot tenzorã€‚è¿™é‡Œæˆ‘ä»¬åªæ”¾æ­£ç¡®çš„ç±»åˆ«æ ‡ç­¾ã€‚å‡è®¾åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œå®ƒæ˜¯ç±»åˆ«
    0ã€‚ä¸å†æ˜¯ç‹¬çƒ­ç¼–ç ã€‚ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªå¥½çš„é¢„æµ‹ã€‚Y é¢„æµ‹å¥½ç­‰äºã€‚ğŸ˜Šã€‚
- en: Torch dot tenzorã€‚ And then here we must be careful about the sizeã€‚ So this has
    the size number of samples times the number of classesã€‚ So let's sayï¼Œ in our caseã€‚
    we have one sample and 3 possible classesã€‚ So this is an array of arraysã€‚ And
    here we put in 2ã€‚01ã€‚0 and 0ã€‚1ã€‚And rememberï¼Œ thisï¼Œ these are the raw valuesã€‚ So
    we didn't apply the soft maxã€‚ And hereã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Torch dot tenzorã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¿…é¡»æ³¨æ„å¤§å°ã€‚æ‰€ä»¥è¿™ä¸ªå¤§å°æ˜¯æ ·æœ¬æ•°é‡ä¹˜ä»¥ç±»åˆ«æ•°é‡ã€‚å‡è®¾åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ ·æœ¬å’Œ 3 ä¸ªå¯èƒ½çš„ç±»åˆ«ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªæ•°ç»„çš„æ•°ç»„ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æ”¾å…¥
    2ã€‚01ã€‚0 å’Œ 0ã€‚1ã€‚è¯·è®°ä½ï¼Œè¿™äº›æ˜¯åŸå§‹å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬æ²¡æœ‰åº”ç”¨è½¯æœ€å¤§å€¼ã€‚ç„¶åè¿™é‡Œã€‚
- en: the highestã€‚Orå¯¹ã€‚The class 0 has the highest valueã€‚ So this is a good predictionã€‚
    And now let's make a bad predictionã€‚ So prediction badã€‚ So hereã€‚ the very first
    value is a lower valueï¼Œ let's sayï¼Œ And the second value is highã€‚ And let's change
    this also a little bitã€‚And now we compute our loss like thisã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€é«˜çš„ã€‚æˆ–è€…å¯¹ã€‚ç±»0å…·æœ‰æœ€é«˜çš„å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªå¥½çš„é¢„æµ‹ã€‚ç°åœ¨è®©æˆ‘ä»¬åšä¸€ä¸ªä¸å¥½çš„é¢„æµ‹ã€‚æ‰€ä»¥ä¸ä½³çš„é¢„æµ‹ã€‚åœ¨è¿™é‡Œï¼Œç¬¬ä¸€ä¸ªå€¼æ˜¯è¾ƒä½çš„å€¼ï¼Œå‡è®¾æ˜¯ï¼Œç¬¬äºŒä¸ªå€¼å¾ˆé«˜ã€‚æˆ‘ä»¬ä¹Ÿç¨å¾®æ”¹å˜ä¸€ä¸‹è¿™ä¸ªã€‚ç°åœ¨æˆ‘ä»¬è¿™æ ·è®¡ç®—æŸå¤±ã€‚
- en: So now we call the loss function that we created hereã€‚ and then we put in the
    y prediction and the actual y and the same with our secondã€‚ Let's compute a second
    loss with y prediction bad and yã€‚And now let's print themã€‚ So let's print L1ã€‚Dot
    itemã€‚So it only has one valueã€‚ so we can call the item function and also L 2 dot
    itemã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç§°è¿™é‡Œåˆ›å»ºçš„æŸå¤±å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬è¾“å…¥é¢„æµ‹çš„yå’Œå®é™…çš„yï¼Œç¬¬äºŒä¸ªä¹Ÿæ˜¯å¦‚æ­¤ã€‚è®©æˆ‘ä»¬è®¡ç®—ç¬¬äºŒä¸ªæŸå¤±ï¼Œé¢„æµ‹çš„yä¸ä½³å’Œyã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°å®ƒä»¬ã€‚æˆ‘ä»¬æ‰“å°L1ã€‚Dot
    itemã€‚å®ƒåªæœ‰ä¸€ä¸ªå€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è°ƒç”¨itemå‡½æ•°å’ŒL2 dot itemã€‚
- en: So let's run thisã€‚ And yeahï¼Œ here we see that our good prediction has a lower
    cross entropy lossã€‚ So this worksã€‚And now to get the actual predictionsï¼Œ we can
    do it like thisã€‚ So let's say underscore because we don't need thisã€‚ and then
    predictionsã€‚Dicctions equals torchã€‚ dot maxã€‚ And then here we put in the predictionã€‚
    So why prediction goodã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬çœ‹åˆ°å¥½çš„é¢„æµ‹æœ‰è¾ƒä½çš„äº¤å‰ç†µæŸå¤±ã€‚æ‰€ä»¥è¿™æ˜¯æœ‰æ•ˆçš„ã€‚ç°åœ¨è¦è·å¾—å®é™…é¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšã€‚å‡è®¾ä¸‹åˆ’çº¿ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªã€‚ç„¶åé¢„æµ‹ã€‚Dicctionsç­‰äºtorch.dot
    maxã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬è¾“å…¥é¢„æµ‹ã€‚å³å¥½çš„é¢„æµ‹yã€‚
- en: And then along the first dimensionã€‚And alsoï¼Œ the same with the bad oneã€‚ So let's
    call this prediction 1 and prediction 2ã€‚å—¯ã€‚And let's print our predictionsã€‚ So
    predictions 1 and print predictions 2ã€‚So this will here we see that we chooseã€‚The
    highest probabilityã€‚ So in this caseï¼Œ we chooseã€‚This oneï¼Œ and in in the second
    caseã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ²¿ç€ç¬¬ä¸€ä¸ªç»´åº¦ã€‚åŒæ ·çš„ä¸ä½³é¢„æµ‹ã€‚æ‰€ä»¥æˆ‘ä»¬ç§°ä¹‹ä¸ºé¢„æµ‹1å’Œé¢„æµ‹2ã€‚å—¯ã€‚è®©æˆ‘ä»¬æ‰“å°æˆ‘ä»¬çš„é¢„æµ‹ã€‚æ‰€ä»¥é¢„æµ‹1å’Œæ‰“å°é¢„æµ‹2ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬é€‰æ‹©äº†æœ€é«˜çš„æ¦‚ç‡ã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€‰æ‹©è¿™ä¸ªï¼Œåœ¨ç¬¬äºŒç§æƒ…å†µä¸‹ã€‚
- en: we choose this oneã€‚ So class number one hereã€‚So this is how we get the predictionsã€‚And
    what's also very good is that the loss in Pyto allows for multiple samplesã€‚ So
    let's increase our samples hereã€‚ So let's say we have three samplesã€‚ So three
    possible classesã€‚Then our tenzo must have three class labelsï¼Œ our actual yã€‚ Soï¼Œ
    for exampleï¼Œ2ï¼Œ0 and 1ã€‚And thenï¼Œ ourã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©è¿™ä¸ªã€‚æ‰€ä»¥è¿™é‡Œæ˜¯ç±»åˆ«ç¼–å·1ã€‚è¿™å°±æ˜¯æˆ‘ä»¬è·å–é¢„æµ‹çš„æ–¹å¼ã€‚è¿˜æœ‰ä¸€ç‚¹éå¸¸å¥½çš„æ˜¯ï¼ŒPytoä¸­çš„æŸå¤±å…è®¸å¤šä¸ªæ ·æœ¬ã€‚æ‰€ä»¥è®©æˆ‘ä»¬åœ¨è¿™é‡Œå¢åŠ æˆ‘ä»¬çš„æ ·æœ¬ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸‰ä¸ªæ ·æœ¬ã€‚æ‰€ä»¥ä¸‰ä¸ªå¯èƒ½çš„ç±»åˆ«ã€‚ç„¶åæˆ‘ä»¬çš„å¼ é‡å¿…é¡»æœ‰ä¸‰ä¸ªç±»åˆ«æ ‡ç­¾ï¼Œæˆ‘ä»¬çš„å®é™…yã€‚å› æ­¤ï¼Œä¾‹å¦‚ï¼Œ2ï¼Œ0å’Œ1ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„ã€‚
- en: Predictions must be here of size number of samples times the number of classesã€‚
    So now this is of size 3 by 3ã€‚So let's do thisã€‚ So here we must put inã€‚Anotherã€‚List
    with three valuesã€‚Soã€‚å—¯ã€‚Like this and like thisã€‚ So let's say this one is a good
    predictionã€‚ So the firstã€‚Classï¼Œ the first correct label is class number 2ã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å¿…é¡»æ˜¯æ ·æœ¬æ•°é‡ä¹˜ä»¥ç±»åˆ«æ•°é‡çš„å¤§å°ã€‚æ‰€ä»¥ç°åœ¨è¿™æ˜¯3ä¹˜3çš„å¤§å°ã€‚è®©æˆ‘ä»¬è¿™æ ·åšã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬å¿…é¡»æ”¾å…¥å¦ä¸€ä¸ªå¸¦æœ‰ä¸‰ä¸ªå€¼çš„åˆ—è¡¨ã€‚æ‰€ä»¥ï¼Œå—¯ã€‚åƒè¿™æ ·ï¼Œåƒè¿™æ ·ã€‚å‡è®¾è¿™æ˜¯ä¸€ä¸ªå¥½çš„é¢„æµ‹ã€‚æ‰€ä»¥ç¬¬ä¸€ä¸ªç±»ï¼Œç¬¬ä¸€ä¸ªæ­£ç¡®æ ‡ç­¾æ˜¯ç±»åˆ«ç¼–å·2ã€‚
- en: So this one must have the highest valueï¼Œ and this one must be lowã€‚ So let's
    say 001ã€‚ And hereã€‚ the very first oneï¼Œ the first class must have a high valueã€‚
    So like thisã€‚And then the value in the middle must have the highest raw valuesã€‚
    Soï¼Œ for exampleï¼Œ like thisã€‚And thenã€‚We do the same for our bad predictionã€‚Andã€‚Let's
    say here we have this one higherã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªå¿…é¡»æ˜¯æœ€é«˜å€¼ï¼Œè¿™ä¸ªå¿…é¡»æ˜¯ä½å€¼ã€‚å‡è®¾æ˜¯001ã€‚åœ¨è¿™é‡Œï¼Œç¬¬ä¸€ä¸ªç±»å¿…é¡»æœ‰é«˜å€¼ã€‚åƒè¿™æ ·ã€‚ç„¶åä¸­é—´çš„å€¼å¿…é¡»æœ‰æœ€é«˜çš„åŸå§‹å€¼ã€‚ä¾‹å¦‚ï¼Œåƒè¿™æ ·ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ä¸ä½³çš„é¢„æµ‹åšåŒæ ·çš„äº‹æƒ…ã€‚å‡è®¾è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ›´é«˜çš„å€¼ã€‚
- en: And also change this a little bitã€‚And then we can againã€‚ compute the cross the
    cross entropys with multiple samplesã€‚And now let's run thisã€‚ And then we also
    see here againï¼Œ our first prediction is good and has a low lossã€‚ And the second
    one is not so goodã€‚And yeahï¼Œ here we get the correct predictions from the firstã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜è¦ç¨å¾®æ”¹å˜ä¸€ä¸‹è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å†æ¬¡è®¡ç®—å¤šä¸ªæ ·æœ¬çš„äº¤å‰ç†µã€‚ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬å†æ¬¡çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé¢„æµ‹æ˜¯å¥½çš„ï¼Œå¹¶ä¸”æœ‰ä½æŸå¤±ã€‚ç¬¬äºŒä¸ªå°±ä¸å¤ªå¥½äº†ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬ä»ç¬¬ä¸€ä¸ªå¾—åˆ°äº†æ­£ç¡®çš„é¢„æµ‹ã€‚
- en: Prediction tenorã€‚ So here we also have 2ï¼Œ0ï¼Œ1ï¼Œ like in the actual yã€‚Soï¼Œ yeahã€‚
    this is how we can use the cross entropylos in Pytorchã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_11.png)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å¼ é‡ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ä¹Ÿæœ‰2ï¼Œ0ï¼Œ1ï¼Œå’Œå®é™…çš„yä¸€æ ·ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨Pytorchä¸­ä½¿ç”¨äº¤å‰ç†µæŸå¤±çš„æ–¹å¼ï¼[](img/e7ecf0933f8ff6fd629229f5815a80cb_11.png)
- en: And nowï¼Œ let's go back to our slidesã€‚So now I want to show you how a typical
    neural network looks likeã€‚ So here is a typical neural net in a multi class classification
    problemã€‚ So here we want to find out what animal our image showsã€‚ So we have an
    input layer and then some hidden layers and maybe some activation functions in
    betweenã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å›åˆ°å¹»ç¯ç‰‡ã€‚æ‰€ä»¥ç°åœ¨æˆ‘æƒ³å±•ç¤ºä¸€ä¸ªå…¸å‹çš„ç¥ç»ç½‘ç»œæ˜¯æ€æ ·çš„ã€‚åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬æƒ³è¦æ‰¾å‡ºæˆ‘ä»¬çš„å›¾åƒæ˜¾ç¤ºçš„æ˜¯å“ªç§åŠ¨ç‰©ã€‚å› æ­¤æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥å±‚ï¼Œç„¶åä¸€äº›éšè—å±‚ï¼Œä¹Ÿè®¸åœ¨ä¸­é—´æœ‰ä¸€äº›æ¿€æ´»å‡½æ•°ã€‚
- en: And then at the endï¼Œ we have a linear layer with one output for each classã€‚So
    here we have two outputsã€‚ And then at the very endã€‚ we apply our soft marks and
    get the probabilitiesã€‚So nowï¼Œ as I said in Pytorchã€‚ we must be careful because
    we use the cross entropy loss hereã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¯ä¸ªç±»åˆ«çš„è¾“å‡ºä¸ºä¸€çš„çº¿æ€§å±‚ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªè¾“å‡ºã€‚åœ¨æœ€åï¼Œæˆ‘ä»¬åº”ç”¨softmaxå¹¶è·å¾—æ¦‚ç‡ã€‚æ‰€ä»¥ç°åœ¨ï¼Œå¦‚æˆ‘æ‰€è¯´ï¼Œåœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨äº¤å‰ç†µæŸå¤±ã€‚
- en: So we must not use the soft max layer in our neural netã€‚ So we must not implement
    this for ourselvesã€‚ So let's have a look at how this code looksã€‚ So in a multi
    classã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_13.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨ç¥ç»ç½‘ç»œä¸­å¿…é¡»ä¸ä½¿ç”¨softmaxå±‚ã€‚å› æ­¤æˆ‘ä»¬å¿…é¡»ä¸è‡ªå·±å®ç°è¿™ä¸ªã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªä»£ç çš„æ ·å­ã€‚åœ¨å¤šåˆ†ç±»ä¸­ã€‚
- en: And classificationï¼Œ our netï¼Œ for exampleã€‚Looks like thisã€‚ So we define our layersã€‚
    So we have one linear layerï¼Œ which gets an input size and then a hidden sizeã€‚
    Then we have activation function in betweenã€‚ And then our last layerï¼Œ it gets
    the hidden sizeã€‚ and then the output size is the number of classesã€‚ So for eachã€‚Possible
    classã€‚ We have one outputã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼Œæˆ‘ä»¬çš„ç½‘ç»œï¼Œä¾‹å¦‚ã€‚çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„å±‚ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå®ƒæ¥æ”¶è¾“å…¥å¤§å°å’Œä¸€ä¸ªéšè—å¤§å°ã€‚ç„¶åæˆ‘ä»¬åœ¨ä¸­é—´æœ‰æ¿€æ´»å‡½æ•°ã€‚æœ€åä¸€å±‚æ¥æ”¶éšè—å¤§å°ï¼Œè¾“å‡ºå¤§å°æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚å¯¹äºæ¯ä¸ªå¯èƒ½çš„ç±»åˆ«ï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªè¾“å‡ºã€‚
- en: And then in the forward methodã€‚ So here we only apply our layers and then no
    softm here at the very endã€‚ And then we create our modelï¼Œ and then we used the
    cross entropisï¼Œ which then applies to softmã€‚So yeahï¼Œ be careful hereã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_15.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰å‘æ–¹æ³•ä¸­ã€‚æˆ‘ä»¬åªåº”ç”¨æˆ‘ä»¬çš„å±‚ï¼Œè€Œåœ¨æœ€åæ²¡æœ‰softmaxã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µï¼Œå®ƒé€‚ç”¨äºsoftmaxã€‚æ‰€ä»¥è¦å°å¿ƒè¿™é‡Œã€‚
- en: Andã€‚So this example also works for more classesã€‚ So if our image couldï¼Œ for
    exampleã€‚ also be a bird or a mouse or whateverï¼Œ then this is also the correct
    layoutã€‚ But if we just have a binary classification problem with two possible
    outputsã€‚ Then we can change our layer like thisã€‚ soã€‚Nowï¼Œ we rephrase our questionã€‚
    So we just sayã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­ä¹Ÿé€‚ç”¨äºæ›´å¤šç±»åˆ«ã€‚å¦‚æœæˆ‘ä»¬çš„å›¾åƒä¹Ÿå¯ä»¥æ˜¯é¸Ÿæˆ–è€é¼ æˆ–å…¶ä»–ï¼Œé‚£ä¹ˆè¿™æ˜¯æ­£ç¡®çš„å¸ƒå±€ã€‚ä½†å¦‚æœæˆ‘ä»¬åªæ˜¯æœ‰ä¸€ä¸ªäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œå…·æœ‰ä¸¤ä¸ªå¯èƒ½çš„è¾“å‡ºã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·æ›´æ”¹æˆ‘ä»¬çš„å±‚ã€‚æ‰€ä»¥ï¼Œç°åœ¨æˆ‘ä»¬é‡æ–°è¡¨è¿°æˆ‘ä»¬çš„é—®æ³•ã€‚æˆ‘ä»¬åªæ˜¯è¯´ã€‚
- en: is it a docï¼Œ Yes or noã€‚ And then here at the endï¼Œ we have a linear layer with
    only one outputã€‚ And then we do not use the softm functionï¼Œ but we use the smoid
    functionã€‚ which then gets a probabilityã€‚ And if this is higher than 05ï¼Œ then we
    say yesã€‚And here in Pytorrchã€‚ we use theã€‚BC E loss or binary cross entropy lossã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä»½æ–‡æ¡£å—ï¼Ÿæ˜¯æˆ–ä¸æ˜¯ã€‚ç„¶ååœ¨æœ€åï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåªæœ‰ä¸€ä¸ªè¾“å‡ºçš„çº¿æ€§å±‚ã€‚æˆ‘ä»¬ä¸ä½¿ç”¨softmaxå‡½æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨sigmoidå‡½æ•°ã€‚ç„¶åå¾—åˆ°ä¸€ä¸ªæ¦‚ç‡ã€‚å¦‚æœè¿™ä¸ªæ¦‚ç‡é«˜äº0.5ï¼Œæˆ‘ä»¬å°±è¯´æ˜¯ã€‚è¿™é‡Œåœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨BCELossæˆ–äºŒå…ƒäº¤å‰ç†µæŸå¤±ã€‚
- en: So here we must implement the sigmoid function at the endã€‚ So let's have a look
    at our neural net in a binary classification caseã€‚ So again here firstã€‚ we set
    up our layers and our activation functionsã€‚ And the last layer has the output
    size 1ã€‚ So this is always fixed in this caseã€‚ And then in the forward pass Now
    here after we applied our layersã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬å¿…é¡»åœ¨æœ€åå®ç°sigmoidå‡½æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹åœ¨äºŒå…ƒåˆ†ç±»æƒ…å†µä¸‹çš„ç¥ç»ç½‘ç»œã€‚æ‰€ä»¥åœ¨è¿™é‡Œé¦–å…ˆã€‚æˆ‘ä»¬è®¾ç½®æˆ‘ä»¬çš„å±‚å’Œæ¿€æ´»å‡½æ•°ã€‚æœ€åä¸€å±‚çš„è¾“å‡ºå¤§å°ä¸º1ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹æ€»æ˜¯å›ºå®šçš„ã€‚ç„¶ååœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œç°åœ¨åœ¨åº”ç”¨æˆ‘ä»¬çš„å±‚åã€‚
- en: we also must implement the sigmoid functionã€‚ So yeahï¼Œ and then here as a criterionã€‚
    we use the binary cross entropy lossã€‚ So be very careful here about these two
    differentã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_17.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¿…é¡»å®ç°sigmoidå‡½æ•°ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œç„¶ååœ¨è¿™é‡Œä½œä¸ºæ ‡å‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±ã€‚æ‰€ä»¥åœ¨è¿™ä¸¤ä¸ªä¸åŒçš„æ–¹é¢è¦éå¸¸å°å¿ƒã€‚
- en: Different possible neural netsã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_19.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒå¯èƒ½çš„ç¥ç»ç½‘ç»œã€‚
- en: And yeahï¼Œ but that's basically what I wanted to show youã€‚So the last structure
    is also what I used in the logistic regression tutorial so you can check that
    out if you haven't alreadyã€‚And for nowï¼Œ that's all I wanted to show youã€‚ I hope
    you enjoyed it and understood everythingã€‚ If you have any questionsï¼Œ leave them
    in the comments belowã€‚ And if you like this tutorialã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™åŸºæœ¬ä¸Šå°±æ˜¯æˆ‘æƒ³è¦å±•ç¤ºçš„å†…å®¹ã€‚æ‰€ä»¥æœ€åçš„ç»“æ„ä¹Ÿæ˜¯æˆ‘åœ¨é€»è¾‘å›å½’æ•™ç¨‹ä¸­ä½¿ç”¨çš„ï¼Œå¦‚æœä½ è¿˜æ²¡çœ‹è¿‡ï¼Œå¯ä»¥å»çœ‹çœ‹ã€‚ç›®å‰ï¼Œæˆ‘æƒ³å±•ç¤ºçš„å°±æ˜¯è¿™äº›ã€‚å¸Œæœ›ä½ å–œæ¬¢ï¼Œå¹¶ä¸”ç†è§£äº†æ‰€æœ‰å†…å®¹ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨ä¸‹é¢çš„è¯„è®ºä¸­ç•™è¨€ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹çš„è¯ã€‚
- en: then please subscribe to the channel and see you next timeï¼Œ bye byã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_21.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¯·è®¢é˜…é¢‘é“ï¼Œä¸‹æ¬¡å†è§ï¼Œæ‹œæ‹œã€‚![](img/e7ecf0933f8ff6fd629229f5815a80cb_21.png)
