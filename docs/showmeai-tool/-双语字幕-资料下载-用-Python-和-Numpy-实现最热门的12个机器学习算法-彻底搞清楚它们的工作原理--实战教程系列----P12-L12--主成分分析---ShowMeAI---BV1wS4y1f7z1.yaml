- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P12ÔºöL12-
    ‰∏ªÊàêÂàÜÂàÜÊûê - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hi everybody„ÄÇ Welcome to our new machine learning from Sct tutorial„ÄÇ Today„ÄÇ
    we are going to implement the principal component analysis or PCA using only Python
    and Numpy„ÄÇThe PCA is a nice tool to get linearly independent features and also
    to reduce the dimensionality of our data set„ÄÇSo the goal is to find a new set
    of dimensions such that all the dimensions are orthogonal and hence linearly independent
    and ranked according to the variance of data along them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this means we want to find a transformationÔºå such that the transform features
    are linearly independent„ÄÇAnd the dimensionality can then be reduced by taking
    only the dimensions with the highest importance„ÄÇAnd those newly fine dimensions
    should minimize the projection error„ÄÇ and the projected point should have a maximum
    spread or which means the maximum variance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's look at an image to understand this better„ÄÇ So let's say our 2D data
    is distributed like this and now we want to project it into 1 d and now what we
    want to do is we want to find the axis that are orthogonal to each other„ÄÇAnd when
    we project our data onto these axesÔºå then our new projected data should have the
    maximum spread„ÄÇSo on the left sideÔºå these are the correct principal axis„ÄÇ So if
    we project them in one D„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So on the largest principal component„ÄÇIf we project our data on this axis„ÄÇ then
    they have the maximum spread„ÄÇ AndÔºå for exampleÔºå if we look on the right side„ÄÇ
    So these are incorrect axis„ÄÇ SoÔºå let's look at how the projected data would look
    like for„ÄÇ so on the right sideÔºå we made it even worse and projected it on the„ÄÇY
    xs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So these are clearly wrong because here we can see that a lot of data is on
    the same spot„ÄÇ so we don't have any more information about them„ÄÇ But here on the
    left side„ÄÇ the projected data has the maximum spread„ÄÇSo we can contain most of
    the information about the data„ÄÇAnd alsoÔºå the projection errorÔºå which means this
    would be„ÄÇThe lines from here to the axis„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is minimalÔºå whereas on the right side„ÄÇ So here we have to make a long„ÄÇ
    very long projection line for each point„ÄÇ So the left side is the correct answer„ÄÇ
    And now how do we find these principal components„ÄÇSo for thisÔºå as I said„ÄÇ we want
    to maximize the varianceÔºå so we need some math„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we need the variance of a sample X and this is calculated as one over the
    number of samples„ÄÇ and then we have the sum over each component minus x bar and
    x bar is the mean value„ÄÇ So we subtract the mean value from our data set„ÄÇAnd now
    we also need a covariance matrix„ÄÇ so this indicates the level to which two variables
    vary together and the covariance matrix of two variables is defined as this so1
    over n and then again the sum and here we subtract the mean and again here also
    the mean and then transposed and in our case we want to have the covariance matrix
    with both of our x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this is also called the auto covariance matrix„ÄÇSo„ÄÇWe have to calculate this
    and then our problem is reduced to an eigenvector or eigenvalue problem„ÄÇ so I
    will not go into detail about eigenvectors here„ÄÇ but I will put some links in
    the description if you want to read more„ÄÇÂóØ„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But what we have to do is we have to find the eigenvectors and eigenvalues of
    this covariance matrix and the eigenvectors point then in the direction of the
    maximum variance and the corresponding eigenvalues indicate the importance of
    its corresponding eigenvector„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now if we have a look at this image again on the left side„ÄÇ So these twovectors
    that I've drawn hereÔºå they correspond to the eigenvectors of the covariance matrix
    of our data set„ÄÇSo this is what we have to do„ÄÇ And here I've written the approach„ÄÇ
    So first we sub the mean value from our x or from our data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we calculate the covariance matrix„ÄÇ Then we have to calculate the eigenvectors
    and eigenvalues„ÄÇThen we sort the eigenvectors in decreasing orderÔºå according to
    their eigenvalues„ÄÇ and then we can specify how many dimensions we want to keep„ÄÇ
    and then we choose only the first K eigenvectors„ÄÇThat will then be the new K dimensions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we transform the original data„ÄÇInto these new dimensions by projecting
    them„ÄÇ just„ÄÇ this is simply a dot product„ÄÇOf our data with the newÔºå with the eigenvectors„ÄÇAnd
    then we are done„ÄÇ So this is all we have to doÔºå and„ÄÇBye„ÄÇOne thing that is very
    nice about this„ÄÇPncple component analysis and the eigenvectors is that they are
    all orthogonal of each other„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This means that our new data is then also linearly independent„ÄÇ So this is a
    nice little bonus of the PCA„ÄÇ![](img/592934822c91be0e4fe905744b84c579_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now we can start„ÄÇ So let's import Ny S and P„ÄÇ And then we create a class
    PC A„ÄÇ This will get an in it with self„ÄÇ And then here we specify the number of
    components„ÄÇ We want to keep„ÄÇAnd then we store them hereÔºå so we say self dot n
    components„ÄÇEquals n„ÄÇComponents„ÄÇAnd„ÄÇWe want to find out the eigenvectors„ÄÇ So let's
    call them components here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is none in the beginning„ÄÇ And we also want to store the mean later„ÄÇ
    So let's say self that mean equals none„ÄÇ And then we define our fit methodÔºå as
    always„ÄÇ So this will get the day that we want to transform„ÄÇAnd then we don't use
    the predict method„ÄÇ So now we call this transform„ÄÇSo this will transform our data
    once we fitted it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is just the projection„ÄÇ But now let's start with the fit methodÔºå so„ÄÇLet's
    say or again„ÄÇ let's write our approach„ÄÇ So we want to have the mean„ÄÇThen we calculate
    the covariance matrix„ÄÇThen we want to calculate the eigen vectorctors and values„ÄÇÂóØ„ÄÇHigenvalue„ÄÇThen
    we sort our een back to„ÄÇ so sort„ÄÇEigenves„ÄÇAnd„ÄÇThenÔºå we store only„ÄÇThe first and„ÄÇEying
    vectors„ÄÇSo this is what we have to do„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we have to„ÄÇProject our data„ÄÇSo„ÄÇYeahÔºå let's do this„ÄÇ So let's say„ÄÇÂóØ„ÄÇHere
    we can say self dot mean equals„ÄÇ and then we just use the Ny mean function of
    our data along the first axis„ÄÇAnd then we sub the mean„ÄÇ So we say x equals x minus
    self dot mean„ÄÇSo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/592934822c91be0e4fe905744b84c579_3.png)'
  prefs: []
  type: TYPE_IMG
- en: We have to do thisÔºå because if we look„ÄÇAgainÔºå and in our formula with the covariance
    matrix„ÄÇ then always thisÔºå the mean is subtracted„ÄÇ![](img/592934822c91be0e4fe905744b84c579_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's just do this here„ÄÇ and then we calculate the co variance matrix„ÄÇ and
    this is called cof„ÄÇ And then we simply use the nuy dot„ÄÇCough function„ÄÇ So this
    will do exactly this if we only put in one input here„ÄÇBut now we have to be careful„ÄÇ
    because let's„ÄÇÂóØ„ÄÇLet's look at our data„ÄÇ So this would be a nuy and D array where
    one row„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the row is one sample„ÄÇAnd one column„ÄÇIs one feature of Eto„ÄÇBut if we have a
    look at the documentationÔºå then for this functionÔºå it's the other way round„ÄÇ So
    one column is one observation or one sample„ÄÇ So we have to transpose it here„ÄÇ
    So please double check it for yourself„ÄÇAnd then we continue„ÄÇ So now we calculate
    the eigen„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: VectctorsÔºå and IÔºå I don't know„ÄÇ it's the other way around„ÄÇ SorryÔºå eigenvalue
    and eigen vectorctors„ÄÇ And for thisÔºå we can also use a function in Nmpy„ÄÇ So lump
    Ny linearal„ÄÇLynn A dot„ÄÇIike„ÄÇüòä„ÄÇAnd then we put in our Covari matrix„ÄÇAnd here againÔºå
    we have to be careful if we„ÄÇIf we look at the documentationÔºå then it says that
    eigenves are returned as column vectors„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So one column„ÄÇWith all„ÄÇ1 column I here is one eigen vector„ÄÇAnd now for to do
    easier calculations„ÄÇ we want to do it the other way around„ÄÇ So we say eigenvectors
    equals eigenvectors dot transposed„ÄÇAnd thenÔºå we sort them„ÄÇSo for thisÔºå we say
    in our sortded indices„ÄÇ are Ny a sort of the eigenvalues now„ÄÇAnd we want to have
    it in decreasing order so we can use slicing here all the way from start to the
    end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we put in my a step-1„ÄÇ So this is a nice little trick to reverse a list„ÄÇAnd
    now we have the indices of the soded eigenve eigenvalues in decreasing order„ÄÇ
    And now we say our soded eigenvalues equals the eigenvalues„ÄÇWith this order and
    the same for the eigenvectors equals eigenvectors„ÄÇWith this indices„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we store the first N eigenvectors„ÄÇ So we say self dot components„ÄÇÂóØ„ÄÇLittle
    typo here„ÄÇ Self do components„ÄÇEqualsÔºå and now we can simply say eigenves„ÄÇFrom
    the start„ÄÇ So from 0 to self dot and components„ÄÇ So this is why we transposed
    it here so we can easily do this transformation and also this transformation„ÄÇSo
    this is the fit method„ÄÇ And now we just have to transform it„ÄÇ And now here we„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Must not forget that we should also subtract the mean here„ÄÇ So we say x„ÄÇEquals
    x minus self„ÄÇDon't mean this is why we started here„ÄÇAnd now we can project it
    and then return it„ÄÇ So we say return nuy dot„ÄÇ and we project X onto our„ÄÇÂóØ„ÄÇComponents„ÄÇ
    so we the dot product with the self dot components„ÄÇ But now againÔºå we have to
    be careful here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Because here we transposed them and now again we want a column vector„ÄÇ So here
    again„ÄÇ we have to transpose it„ÄÇ So againÔºå please double check it for yourself„ÄÇAnd
    now we are done„ÄÇ So this is the whole implementation„ÄÇ And now here I've written
    a little test script that is using the famous Iis data set„ÄÇAnd then I will create
    a PCA instance and I will„ÄÇ so by the way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this will have a dimension of 150 by4 So we have 105 samples and four different
    features here„ÄÇ and now we only want to keep two and dimensions„ÄÇ So we put in two
    in our initializer„ÄÇ Then we fit the dataÔºå and then we transform it„ÄÇAnd then I
    print the shape of both„ÄÇAnd then I will plot them„ÄÇ So now we have a 2D vector
    so we can plot it in 2D„ÄÇSo yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's run this and see if this is working„ÄÇAnd o„ÄÇ‰∏ÄÈ°µ„ÄÇSo I didn't write Python
    3 here„ÄÇ but it worked anyway„ÄÇ So let's test this again„ÄÇ![](img/592934822c91be0e4fe905744b84c579_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå here we have our 4D feature vector transformed or projected into 2D„ÄÇ
    and we see that all are„ÄÇAll the three different classes are plotted in a different
    color„ÄÇ so we can see that we can still have an easy separation of our classes„ÄÇ
    So yeahÔºå that's very nice„ÄÇ And yeahÔºå that's the PC AÔºå I hope you understood everything„ÄÇ
    And if you liked it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: please subscribe to the channel„ÄÇ and see you next timeÔºå bye„ÄÇüòä„ÄÇ![](img/592934822c91be0e4fe905744b84c579_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/592934822c91be0e4fe905744b84c579_10.png)'
  prefs: []
  type: TYPE_IMG
