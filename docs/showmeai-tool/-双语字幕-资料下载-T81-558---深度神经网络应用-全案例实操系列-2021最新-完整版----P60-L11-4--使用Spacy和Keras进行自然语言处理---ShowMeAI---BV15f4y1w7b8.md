# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P60ÔºöL11.4- ‰ΩøÁî®SpacyÂíåKerasËøõË°åËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff„ÄÇ welcome to applications of deep neural networks In this video„ÄÇ we're going to take a look at natural language processing in particular„ÄÇ we're going to use Spacey Cars and word tove together for the latest on my AI course and projects„ÄÇ click subscribe in the bell next to it to be notified of every new video previously we did text generation where we looked at treasure Island„ÄÇ

 and we got the neural network to generate its own pirate stories„ÄÇ We're going to continue with this„ÄÇ but this time we're going to use word level text generationPrevious we use character level text generation„ÄÇ Now there's much debate as far as which of these two you should use„ÄÇ I tend to prefer generating at the character level and going truly end to end with the neural network„ÄÇ

 letting it figure out grammar and letting it figure out sentence structure not really doing a lot of feature engineering on nouns and verbs and that sort of thing„ÄÇ but you can certainly do word level„ÄÇ And if you do word level„ÄÇ then it might be useful to provide additional features to let„ÄÇüòä„ÄÇ

![](img/be18f4bf025fc5921b83d7e947e8af7a_1.png)

No if it's dealing with a noun or a verb or other things„ÄÇ We won't get quite that complicated with this example„ÄÇ but I'll show you how we can use this to generate text at a word by word level this is what we use for captioning„ÄÇ we were generating at the word level„ÄÇ Although you can certainly do captioning at the character level as well„ÄÇ

 So here I imported the packages and libraries that I need both for spacey and for Kis„ÄÇ I'm getting the treasure island text just like I did before„ÄÇ And now I am dealing with spacey So I am going to use spacey now to do the tokenization to break the treasure island text apart so that I have it by individual words„ÄÇ NowÔºå when we did character level encodingÔºå we didn't have to do this tokenization was handled by the neural network„ÄÇ

 It learned what spaces were and it learned how to break the words up here since we're doing word by word„ÄÇ we need to do that actual tokenization and spacey is what does it for us„ÄÇ So we're essentially looping through all of the tokens so„ÄÇAll of the words that it finds this is just codeÔºå this first lineÔºå this is a pretty useful line„ÄÇ

This reduces the characters to just characters in the ASCI code 0 to 127„ÄÇ so this ensures that you are dealing just with ASII characters in those words„ÄÇ so that strips out a lot of the extra junk characters like this copyright symbol in this weird A that removes those from your words then we also strip off any white space because once you removes some of these„ÄÇ there might be white space now that was embedded in there„ÄÇ

 We make sure that the tokens are not digits„ÄÇAnd that they're not URLs„ÄÇ emails or other things like that„ÄÇ Now you might want to handle theseÔºå but for the simple example„ÄÇ we're simply stripping them Now in the character by character„ÄÇ we just let them pass it in and interestinglyÔºå the neural network would learn to generate URLs and email addresses and other things like that total number of words we had when we did that is 6421„ÄÇ

 we print out some of these„ÄÇ Now some of these are numbersÔºå that's a Roman numeral„ÄÇ but we'll just go ahead and let it go in that won't necessarily hurt anything„ÄÇ we're creating to look ups here„ÄÇ a word to index„ÄÇ So this would take a word like alone and convert it into a index„ÄÇ Now if you want to make these more consistent you might want to sort these because then the indexes are going to always be the same„ÄÇ

 So for production situationsÔºå you need to think about that to make sure that these indexes are truly staying the same„ÄÇ OtherwiseÔºå if you train your neural network and you change what these indexes are„ÄÇ It's not going to work„ÄÇThen we tokenize the text that basically goes and takes the original Tresure Island text and replaces each word with the index number„ÄÇSo now it's more ready to be fed into something like an embedding layer or other text translation input„ÄÇ

 We're going to now this is just like what we did in characters„ÄÇ except we're creating sequences of wordsÔºå not sequences of characters so this is essentially the same code we're dealing with word sequences of up to six words step just means that we move forward three words each time otherwise we would have a of a lot of redundancy in here because if the word if the sentences this is a test„ÄÇ

 the first sequence is going to be this is the next sequence you would move forward three words so you wouldn't immediately start with the second second word„ÄÇThe smaller you make this numberÔºå the more of repetition that you will have and then we see what these sequences look like„ÄÇ

 at least the first five So these are the sequences that we're training on we're training the neural network if you have this word„ÄÇ this wordÔºå this wordÔºå this wordÔºå this word and this word then what is the next word then we vectorize it this is essentially just changing this into lumpy arrays so that we can actually train the neural network with it„ÄÇ

 this is exactly the same as from the character level encoding that we did but it is really just building these up„ÄÇSort of one by one and then also building the Y so that we have„ÄÇThe dummy variables coming out on the Y side so that we can predict it these we are using dummy variables„ÄÇ So this is the X shape„ÄÇ This makes it very evident how we're encoding this„ÄÇ

 So the X shape is essentially we have 32005 sequences that we generated the max sequence sizes 6„ÄÇ and then we have this many values because we have dummy variables coming in„ÄÇ essentially„ÄÇ we have a dummy variable for each of these six„ÄÇ So this is really a lot of data that we've pregenerated„ÄÇ This is where' using an embedding layer could make this more efficientÔºå but it fits into Ram„ÄÇ

 So I'm largely happy with it„ÄÇ The y is similar„ÄÇ we don't have the six here because we're not predicting sequences„ÄÇ We're just predicting the next character„ÄÇ So we have the same number of rows„ÄÇ but now we have these dummy variables that tell us which of those 6400 words It's actually going to be„ÄÇ And you can see what the dummies look like here essentially we're creating„ÄÇVery smallÔºå simple LSTM„ÄÇ

Has 128„ÄÇNurons„ÄÇAnd it's layer„ÄÇWe're going to optimize with Rs prop again„ÄÇ keeping it similar to the example that I pulled in from the character encoding Now this is not a formal example from Kira's I just modified the previous example from Kiras for the LSTM generation I just modified this to actually work with words instead of characters we print a summary it's got like 4 million weights that we're training on this is the sample function this is exactly like the characterbased one we're essentially doing a softm function where the temperature determines how conservative it's going to be in terms of the sentences how willing it is to take risks so you get more creative with that is a higher number but more error prone with it as a lower number and this is essentially a softm essentially ensuring that the probabilities of all of those 66400 words that are in the vocabulary that's trying to predict for the next„ÄÇ

That the probabilities of each of those do sum to one because they're probabilities because we're going to take essentially the highest one„ÄÇ and that is what we're going to predict as the next word„ÄÇ This is very similar to the previous example„ÄÇ We're calling this on Epoch end and we're essentially generating some text on each of these iterations as it goes through training„ÄÇ and we can see that the text that it's generating gets better and better and better„ÄÇ

 We're going to generate 100 words for each of these„ÄÇ We're going to build up the input sequence„ÄÇ So we're going to randomly sample six or however big that input vector was for the sequence„ÄÇ I think it was six words if memory serves„ÄÇ YeahÔºå excellent was6„ÄÇ So we're sampling groups of six words and we begin training„ÄÇ We'll scroll down Now this is a lot„ÄÇ

 this was training for a while„ÄÇ This is why I'm not running these for the video„ÄÇ it would take quite a while„ÄÇ So temperatureÔºå this is a pretty conservative temperature and this is well„ÄÇthe training„ÄÇ you can see the pirate story that it's inventing Israel„ÄÇ I ourselves sunk pieces It reply towards make me about among business„ÄÇ thus Morgan news„ÄÇ

 Davy in mint return„ÄÇ I'm not seeing really any grammatical error„ÄÇ So that's really kind of cool„ÄÇ Another thing that it's doing that I just find fascinating is my tokenizer„ÄÇ I probably didn't spend enough time on the tokenizer„ÄÇ but it's taking the I and ill„ÄÇ those are two different words in the vocabulary„ÄÇ That's why there's a space in here because we're putting the spaces in„ÄÇ

 It doesn't know what spaces are„ÄÇ but it figures out that usually after an IÔºå it'll be ill„ÄÇ or it'll be S„ÄÇ it's not like tacking this apostrophe S on fell or something like this„ÄÇ It really is truly figuring out the sentence structure„ÄÇ and that's really„ÄÇ really cool and where natural language processing has really„ÄÇ

 really benefited from some of the deep learning technology„ÄÇSo this is how you would do that same text generator at the word level The word level„ÄÇ I think out of the box without a lot of effort trying to optimize does produce very realistic words and sentence structure and believable text as far as pirate stories„ÄÇ Thank you for watching this video on the next video„ÄÇ

 we're going to look deeper into natural language processing with curs and look at embedding layers„ÄÇüòä„ÄÇ![](img/be18f4bf025fc5921b83d7e947e8af7a_3.png)

This content changes oftenÔºå so subscribe to the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇ