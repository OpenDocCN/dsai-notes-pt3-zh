- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P21ÔºöL3.5- ÊèêÂèñKerasÊùÉÈáçÂπ∂ÊâãÂä®ËøõË°åÁ•ûÁªèÁΩëÁªúËÆ°ÁÆó
    - ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå this is Jeffine„ÄÇ Welcome to applications of deep neural networks with Washington
    University„ÄÇ In this videoÔºå we're going to see how to actually extract the weights
    from a neural network created by Kes and put those weights into an equation so
    that we can actually calculate the output from the neural network and see that
    this there's really no magic to this process„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåThat it is simply weights multiplied against the inputs that produces a final
    output for the latest on my AI course and projects„ÄÇ click subscribe in the bell
    next to it to be notified of every new video„ÄÇ This is code from Ks and TensorflowlowÔºå
    But this applies to really just about anything„ÄÇ I am going to show you how to
    actually extract the weights from the neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so that we can then put them onto a diagram and actually actually calculate
    it and come up with the same number that Tensorflowlow would have„ÄÇüòä„ÄÇ![](img/99c863804a8e2678a48ffb4d5482d3aa_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: For thisÔºå we're going to use an exclusive or neural networkÔºå that's the XOR
    function„ÄÇThese are the inputs to itÔºå0Ôºå0Ôºå the usual truth table for any sort of
    an and or an or„ÄÇ This is the expected output„ÄÇ The thing to remember with X O R
    is if the two inputs are the same„ÄÇ it's going to be 0„ÄÇIf the two outputs are differentÔºå
    zero and 1 versus 10Ôºå it's going to be one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here we set up a neural network„ÄÇ It has two inputs and a hidden layer of two„ÄÇ
    We're going for the absolute smallest neural network just to show that you don't
    really need that much to calculate an excluvo„ÄÇAlsoÔºå since we are going to calculate
    this by hand or just being lazy„ÄÇ we don't I don't want to give you a truly deepÔºå
    deep neural network and then calculate it by hand„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that would not be fine„ÄÇSo we're going to optimize it with mean square error„ÄÇ
    and the final output is going to be one neural networkÔºå it's a regression neural
    network„ÄÇ you can do XOR as classification or regression in this caseÔºå I'm doing
    it as regression„ÄÇI basically train it here„ÄÇ I am training it for100Ôºå000 epochs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It may take longer because there's so few weights in this that your initial
    random values or your weights are really going to have a lot of determination
    on the success of your training„ÄÇ We could do this in way fewer than this many
    epochs„ÄÇ This is basically grilling a cheese sandwich with a gen engineÔºå but„ÄÇI
    am really not trying to show you how to tune these things right now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I just want to get the weights set up so that they're good for a exclusive or
    network„ÄÇ we're going to put those on a diagram and show you that basically you
    can then calculate the same output so you can see the weights behind what Kis
    gives you„ÄÇI train itÔºå I predict itÔºå this is good for the neural network because„ÄÇEach
    of these four correspond to these four up here in the input„ÄÇ This is scientific
    notation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So to the negative 4Ôºå that's 0„ÄÇ000„ÄÇ That number„ÄÇ So these two are effectively0„ÄÇ
    and these two in the middle are effectively one„ÄÇ So this may look weird if you're
    not used to seeing numbers like this„ÄÇ but this is a great output„ÄÇ the neural network
    has trained quite well„ÄÇSo then what we're able to do is I writeÔºå I wrote this
    little program hereÔºå this dumps the weights„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so you're seeing that the layer0 counting starting at zero„ÄÇWeight from the bias
    to the next layer layer 1„ÄÇ againÔºå we're counting with 0„ÄÇ neuron 1„ÄÇ neuron neuron
    0Ôºå neuron 1„ÄÇ These are the biases„ÄÇAnd these are the actual weights„ÄÇSo there's
    not a lot of weights and biases in this neural networkÔºå it's a fairly small one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I'm going to show draw a diagram here in just a minute„ÄÇ and we're going to
    copy all of these values up onto the diagram„ÄÇ Then we're going to calculate it
    by hand„ÄÇ I give you the code down here to calculate it by hand as well„ÄÇ if you
    want to do itÔºå you set the input to 1 and 0Ôºå So1 in 0 and an X O R„ÄÇ Those are
    different„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it should be 1„ÄÇ So it outputs a 0„ÄÇ96„ÄÇ NowÔºå since we're calculating this by
    hand„ÄÇ I truncated quite a few of these numbers„ÄÇ So 1„ÄÇ29 becomes 1„ÄÇ3Ôºå and we'll
    do that when we diagram it„ÄÇ So we lose a little bit of accuracyÔºå but„ÄÇThese are
    the two input neurons„ÄÇWe are going to feed this values 0 and  one„ÄÇ So that's what
    we're calculating for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You could calculate itÔºå reallyÔºå for„ÄÇJust about any other value that you wanted
    to„ÄÇ But for the other four that you would have in the exclusive  four„ÄÇ we're also
    going to have the bias neuron„ÄÇ So like we saw in earlier class videos„ÄÇ you're
    essentially calculating„ÄÇA weighted sum over and over and over again for a neural
    network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to calculate the value for hidden neuron0„ÄÇH0„ÄÇEach zero has inputs
    from those previous three„ÄÇThese all have weights„ÄÇ as we saw from the output from
    the program beforeÔºå so I'm just going to copy those in„ÄÇSo this is going to become
    essentially a weighted sum that we're going to use to calculate what hidden 0
    is actually what its value actually is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So to do thatÔºå we're going to multiply the input 0 times its weightÔºå which is
    1„ÄÇ3„ÄÇÊàëÊòØ„ÄÇ1 times 1„ÄÇ3„ÄÇThat one is from the firstÔºå which is actually the second input
    neuron„ÄÇ And then we have to add the bias„ÄÇ and we basically perform that„ÄÇCalculationÔºå
    which ends up being 0„ÄÇ So the value for hidden in hidden neuron 0 is 0„ÄÇ NextÔºå
    we're going to calculate the value for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Hid neuroron 1Ôºå which is the second one„ÄÇ It has a similar sort of thing going
    on„ÄÇ It has three weights coming into it„ÄÇIs going to copy those values from the
    output that we had from the Kiras program„ÄÇSo that is simply„ÄÇ1„ÄÇ2Ôºå we are calculating
    basically the weighted sum again„ÄÇThe first input I0 drops outÔºå second one contributes
    that one point„ÄÇ1„ÄÇ2 to it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then the intercept or the bias for„ÄÇFor thisÔºå for the second hidden neuron
    is zero„ÄÇThis whole thingÔºå together„ÄÇIs basically equal to 1„ÄÇ2„ÄÇ Then this value„ÄÇ
    we have to pass it through the through the activation function„ÄÇ Both of these
    needed to go through the activation function„ÄÇ That is the rectified linear unit„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that is basically going to be the max„ÄÇOf 0 and 1„ÄÇ2„ÄÇWhich is also 1„ÄÇ2„ÄÇSo the
    output from this hidden neuron here is 1„ÄÇ2„ÄÇSame sort of deal up here to apply
    its activation function„ÄÇ It's going to be maxed because it's also the value„ÄÇ the
    max 0Ôºå0 is 0„ÄÇactivation functions that you can useÔºå I may give you a different
    one for for the midterm examination on this one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The only other ones that you will ever seeÔºå at least in this classÔºå are sigmoid„ÄÇPollic
    tangent„ÄÇThere are othersÔºå but these are for regression„ÄÇIf you're doing a classification„ÄÇ
    you might also see the softm but„ÄÇFocus primarily on these„ÄÇNow the output„ÄÇ the
    final layer so that we actually get what this neural network is providing for
    us„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is going to have similar to before„ÄÇ We do have another bias connection„ÄÇ
    So you're going to have essentially three and bound connections again„ÄÇAnd we're
    essentially going to just copy the weights that we had from the output in Caras„ÄÇThere
    is 0„ÄÇAnd this results in the final equation that ties this whole thing together„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And that is going to basically be„ÄÇIs the output from from hidden hidden 0„ÄÇTimes
    1„ÄÇ6„ÄÇPlusÔºå 1„ÄÇ2„ÄÇTimes 0„ÄÇ8„ÄÇYou can see the 1„ÄÇ2 times the 0„ÄÇ8 and then plus the plus
    the„ÄÇThe bias or the intercept„ÄÇ which is zero„ÄÇAnd then this whole thing equals
    0„ÄÇ96„ÄÇThat's your final output that is approximately equal to 1„ÄÇ0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is what you would have expected from a exclusive or„ÄÇOf„ÄÇZero and what„ÄÇThere
    you have it„ÄÇ That is the calculation for this neural network done using the weights
    that Kiro is actually trained for us„ÄÇ So we can see that there is really no magic
    there„ÄÇüòäÔºåThis simple of a neural network„ÄÇ you could have actually just hand coded
    these weights that would have not been difficult„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'll probably do a video on that as well so that you can see how to actually
    hand code one of these„ÄÇ You do need about this size of neural network„ÄÇ You definitely
    need the hidden layer„ÄÇ![](img/99c863804a8e2678a48ffb4d5482d3aa_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for watching this video„ÄÇ In the next video„ÄÇ we're going to see more
    advanced training techniques that we can use with a neural network to„ÄÇüòä„ÄÇTo get
    better results and to measure or error in a variety of different ways„ÄÇ this content
    changes often„ÄÇ so subscribe to the channel to stay up to date on this course and
    other topics in artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
