# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÂÆòÊñπÊïôÁ®ãÊù•Âï¶ÔºÅ5‰Ωç Hugging Face Â∑•Á®ãÂ∏àÂ∏¶‰Ω†‰∫ÜËß£ Transformers ÂéüÁêÜÁªÜËäÇÂèäNLP‰ªªÂä°Â∫îÁî®ÔºÅÔºúÂÆòÊñπÊïôÁ®ãÁ≥ªÂàóÔºû - P34ÔºöL6.2- LewisÁöÑÂú®Á∫øÁõ¥Êí≠ËÆ≤Ëß£ - ShowMeAI - BV1Jm4y1X7UL

So basically the goal of this session is to go through chapter 2 together and in this chapter what we're going to be doing is diving into the sort of internals of the transformers library and in particular we're going to be looking at the sort of models so there's a set of model APIs that we're going to look at and also the tokenizers which we relied on heavily to convert text into a format that the models can process„ÄÇ

And so you've seen in the first lessons that we have„ÄÇA pipeline API„ÄÇAnd this pipeline API basically wraps all of the complexity of pre processing and post processing text and also fitting it to the model so that you just have to basically give it a sentence and then you could classify„ÄÇ

 for exampleÔºå the sentiment„ÄÇAnd today we just want to sort of unpack what's happening inside this function and also to understand some of the different sort of approaches you can take for tokenizing your text and also how to save and load the models and tokens„ÄÇ

And we'll finish by looking at what you have to kind of do when you're dealing with sentences or texts that have different lengths„ÄÇ because it turns out that in pytorrch and Tensorflow and most deep learning frameworks„ÄÇ we need a kind of standardized sort of rectangular input for our models„ÄÇAnd basically the way we're going to do this is I'm going to go through the sections and then pause for questions„ÄÇ

 but in the meantimeÔºå if you have some sort of very urgent thing that you want to ask„ÄÇ Omar will be here helping us with guidance„ÄÇSo just to give you a taste of like what we're going to be sort of covering but at a higher level„ÄÇ every single model in the Transformers' library has a corresponding modeling file„ÄÇ so for example here what I'm looking at is the modeling file for BERT„ÄÇ

And this this file has all of the source code for all of the different tasks that you can use BRT for so for example„ÄÇ if I look for„ÄÇVt modelÔºå this is the sort of base class that we're going to be looking at today which is responsible for basically creating contextualized embeddings of the inputs so how do we create kind of numerical representations of our text that have some sense of meaning and this kind of class is relatively simple it just has the embedding layer which you saw in the first chapter so the thing that we pass through before we hit the transformer stack and then we have an encoder and this encoder is essentially responsible for converting these tokens or these token embeddings into these contextualized representations„ÄÇ

And I just recommend sort of let your homework have a look through some of this code„ÄÇ so any sort of class that you see us using todayÔºå for exampleÔºå BERT model„ÄÇ have a look at the sort of source code and this really helps you understand how transformers work and at least for me personally it was only by sort of going through this kind of step by step and understanding how all the inputs go through the forward pass that I was really able to understand all the workings of a transformer„ÄÇ

So that's just a little side note„ÄÇSo„ÄÇTo get started„ÄÇ maybe let's have a look at what really happens behind the pipeline„ÄÇ so let's kick start with this video„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_1.png)

What happens inside the pipelinebra functionÔºüIn this video„ÄÇ we'll look at what actually happens when we use the pipeline function of the transformformerss library„ÄÇWell specificallyÔºå well look at the sentiment analysis pipeline„ÄÇ and now it went from the two following sentences so the positive and negative labels were respective scores„ÄÇ

As we've said in the byprint presentation„ÄÇThere are three stages in the pipeline„ÄÇFirst„ÄÇ we convert the verex to numbers the model can make sense of using a tokenizer„ÄÇThen those numbers goes through the model which outputs lows„ÄÇFinally„ÄÇ the first processing steps transform Voor gets into labels and scores„ÄÇ

Let's look in details at those three steps and how to replicate their using the Transformerss library„ÄÇ beginning with the first stage tokenization„ÄÇThe to process has several steps first„ÄÇ the text is split into small chunks called tokens„ÄÇThey can be words„ÄÇ part of words or punctuation symbolsÔºå then the tokenser will add some special tokens if the model expected„ÄÇ

HereÔºå the middle used expect a seal token at the beginning and a sep token at the end of the sentence to classify„ÄÇLastlyÔºå the token ison patches each token to its unique ID in the vocabulary of the pro model„ÄÇTo load the tokenizerÔºå the transformformers library provides the Utokenizer API„ÄÇThe most important method of this class is from Pretrained„ÄÇ

 which will download and cache the configuration and the vocabulary associated to a given checkpoint„ÄÇHereÔºå the checkpoint used by default for the sentiment analysis pipeline is distill belt basin case Fiin tuned SSs2 English„ÄÇ which is a bit of a mouthful„ÄÇWe instant to token associated with a checkpoint and feed it to the two sentences„ÄÇSince the two sentences are not of the same sizeÔºå well need to pad the shest one to be able to build an array„ÄÇ

This is done by the tokenizer with the option padding equal„ÄÇWith trucation equal2„ÄÇ we ensure that any sentence longer than the maximum the model can handle is truncated„ÄÇLastly„ÄÇ the return tensil option tells the tokenizer to return the byythch tensil„ÄÇLooking as a result„ÄÇ we see we have a dictionary with two keysÔºå input ID contains the ideas of both sentences with zero where the padding is applied„ÄÇ

The second key attention mask indicates where petting has been applied„ÄÇ so the model does not pay attention to it„ÄÇThis is all what is inside the took step„ÄÇNow let's have a look at the second step„ÄÇ‰∏â„Åª„Å©„ÄÇAs sponsor to an either„ÄÇ for is a notomodal API with from pretrain methodÔºå it would download lu and cache the configuration of the model„ÄÇ

 as well as the pertrain weight„ÄÇHoweverÔºå the Automod API will only instantiate the body of the model„ÄÇ that is the part of the model that is left once the pro traininging head is removed„ÄÇIt will output a high dimensional tensor that is a representation of the sentences past„ÄÇ but which is not directly useful for our classification product„ÄÇHere the tensor has two sentences„ÄÇ

 each of 16 tokensÔºå and the last dimension is the Indian size of our model 768„ÄÇTo get an output link to our classification problem„ÄÇ we need to use the Automodal for sequence classificationification class„ÄÇIt works exactly as you to model classÔºå except with12 build a model with a classification head„ÄÇüòä„ÄÇ

Praise one auto class for each common NLP task in the transformformers library„ÄÇHere„ÄÇ after giving all models of two sentences„ÄÇWe get a ten of size 2 by2„ÄÇ1 result for each sentence and for each possible level„ÄÇThose outputs are not probabilities yet„ÄÇ we can see they don't sum to one„ÄÇThis is because each model of the transformformer's library returns look it„ÄÇ

To make sense of look itÔºå we need to dig into the third and last step of the pipeline„ÄÇPlus processing„ÄÇTo conduct LoAT into probabilitiesÔºå we need to apply a softmax layers to them„ÄÇAs we can seeÔºå this transforms them into positive number that's a to1„ÄÇSo last step is to know which of those corresponds to the positive of the negative label„ÄÇ

This is given by the IT2lipal field of the model conflictg„ÄÇThe first proba is index0„ÄÇ correspond to the negative level and the seconds index1 correspond to the positive level„ÄÇThis is how our classifier built with the pipeline function peaked with labels and compute those scores„ÄÇüòäÔºåNow that you know how each step worksÔºå you can easily tweak them to your needs„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_3.png)

„ÄÇ

![](img/40873acb06abf924ac4a43fae802679a_5.png)

So„ÄÇLet's see„ÄÇAll rightÔºå so do we have any questions at this stage about the pipelineÔºü

So one of the things that we saw is that there's these kind of three components„ÄÇ there's like a pre processingcess stage„ÄÇOkayÔºå great„ÄÇOkay great so one of the first questions we have is could you please explain the intuition behind the BRT SST2 English checkpoint and what are the different flavors of checkpoints to be used and how did we choose SST2 Okay great so basically each of the transformer models„ÄÇ

 they have sort of pretrained base or pretrained backbone which I think you saw in the first chapter„ÄÇAnd then what we do typically with models like BERT and GP is we fine tune them on a downstream task„ÄÇSo the idea is that you takeÔºå for exampleÔºå BEert which was pre-trained on Wikipedia and the book corpus and then you say„ÄÇ okayÔºå I want to do classification now so I'm going to basically take these weights that I had in my original model and I'm going to add a classification head which is going to just basically be a linear layer that allows us to do the classification task and then we do the fine tuning step on a particular task„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_7.png)

So if you want to understand a little bit about how the„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_9.png)

The models work or the description of the models„ÄÇ If we look at Bert and what was this guy called„ÄÇ this is Bt„ÄÇ‰∏âÂÖ´ÂÖ´ÂÖ´„ÄÇUncasedÔºå fine tuned„ÄÇ So uncased„ÄÇUn tuneed„ÄÇÂóØ„ÄÇAnd then it's S„ÄÇ what was it„ÄÇFine tuned are distber okay„ÄÇThe stillber„ÄÇUncasedÔºå fine tuned„ÄÇWhat are I missing hereÔºüüòî„ÄÇDistill B face and case function indicate„ÄÇ So if we look at this„ÄÇ

Then what we can see is that this is a checkpoint that was fine tuned on a particular task„ÄÇ so this task here is called the T bank task„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_11.png)

![](img/40873acb06abf924ac4a43fae802679a_12.png)

The binary classification benchmarkÔºå and I think from memory this is just like a sentiment analysis task which just has label or data given in terms of just two labels like you know positive or negative„ÄÇSo the basic idea of like why did we choose this or the basic answer is that we were just trying to sort of demonstrate how the pipeline works for sentiment analysis and this is one model which is well suited for that task„ÄÇ

So I hope that answers your questionÔºå DK creative„ÄÇAnd then the other question we have is in this case„ÄÇ we assume that there are only two classes for classification„ÄÇ how do we specify a multiclass problem and what checkpoint would use OkayÔºå great„ÄÇ that's a very good question„ÄÇSo maybe what we can do is let's have a look at the coabab for this chapter„ÄÇ

So here we've got a sentiment analysis pipeline„ÄÇAnd of course„ÄÇ it's just going to predict two classes„ÄÇ And so now I'm just going to instantiate the tokenizer„ÄÇAnd here's the model„ÄÇOkayÔºå so if we look at a model„ÄÇüòäÔºåEvery single model has a config„ÄÇAnd this config tells you thingsÔºå for exampleÔºå like the number of classes„ÄÇ

 so you can see we've got two classes here„ÄÇAnd what you can do when you instantiate a model„ÄÇ you can define the number of classes you would like when you instantiate the thing for text classification„ÄÇ so just to give you an example„ÄÇLet's suppose that I take a checkpoint„ÄÇFor multi class„ÄÇNow I'm going to do two things hereÔºå I'm going to show you first„ÄÇ

 how do we insha a model that we then would fine sharing ourselves„ÄÇ and then I'll show you the sort of simpler case where we have an existing pretrain model„ÄÇSo if I don't haveÔºå imagine I just have my own data set and there's no model in the hub that is suitable for what I want to do„ÄÇ what I might do is I'll sayÔºå okayÔºå I'm going to take distillbert base uncased„ÄÇ

And this is just the pretrain modelÔºå there's nothing sort of special about it„ÄÇ I still have to do some work„ÄÇAnd then what I could do is I can sayÔºå okayÔºå I'm going to take„ÄÇFirst thing to take from transformers„ÄÇI'm going to import an auto model„ÄÇ but now I'm going to do it for sequence classification„ÄÇ

 so this is where anytime you're dealing with like text classification or you know multiclass multilabel these things„ÄÇ this is a sequence classification task„ÄÇAnd then what I'm going to do„ÄÇ I'm going to take my model for sequence classification„ÄÇAnd then I can do my usual from pretrained„ÄÇI take my„ÄÇCheckpoint that I've got now„ÄÇ All this new one„ÄÇ

And then what I can do I can pass keyword arguments that will specify how many labels I'm dealing with so imagine that my data set has six classes that I'm dealing with„ÄÇ so what I can do is I can say the number of labels is six„ÄÇAnd now what will happen„ÄÇ

It will download the base model or the pretrain model for distillvert„ÄÇAnd it will then add a classification head on top of this model and it will configure it with the right number of classes so that you know we can do fine tuning appropriately so now if we look at our config you can see that it's already initialized the model with six different classes„ÄÇ

And we don't know the labels yet because we haven't provided our own data set and our own labeling convention„ÄÇ but we could do that„ÄÇAnd then from here we could then just fine tune and train the model exactly as we've done„ÄÇ or we will do in the next chapterÔºå so that's one way of doing it„ÄÇNow„ÄÇ the other part of the question isÔºå how do I take a sort of pretrain model or fine tune model from the hubÔºü

And this is a little trickier to figure out likeÔºå you know which model is suitable for your task„ÄÇ So the way I usually do it is I lookÔºå for exampleÔºå at text classification„ÄÇ so I do a filter here on text classification„ÄÇAnd then I sort of like ask myselfÔºå okay„ÄÇ maybe I'm dealing with let's see„ÄÇNowÔºå this isn't so easy to find a multi class exampleÔºå so„ÄÇI think„ÄÇ

 in general„ÄÇYeahÔºå so„ÄÇActuallyÔºå finding the multiclass model that is suitable for your task takes a bit of work„ÄÇ I meanÔºå maybe maybe Omar already knows a fast way to get this„ÄÇ but generally speaking all of the models that we have here are in some senseÔºå fine tuned on a task„ÄÇ So for exampleÔºå like this German sentiment Bt„ÄÇpresumably is two classes and one way you could quickly check that is by looking at the files and versions and seeing in the configuration how many labels you have so in this case there's three labels„ÄÇ

But actually searching for this effectively on the H„ÄÇ I'm not sure maybe there is a way of doing this„ÄÇOr maybe this is a good feature we should add in the hub„ÄÇI am HolmesÔºå I hope that sort of partially answers your question„ÄÇBut if not„ÄÇ then feel free to write in the chat„ÄÇYeahÔºå exactly we should add a feature that is great basically I think what we would like is a filter where we could filter between binary classification multiclass and multilaval and then that would allow us to refine things good question awesome„ÄÇ

üòäÔºåOkayÔºå so are there any more questions about the pipelines before we look a bit more at the codeÔºü

OkayÔºå so in that caseÔºå let'sÔºå let's have a sort of walk through this this coab with the pipeline to sort of get a deeper understanding of what's going on„ÄÇ So we've got this„ÄÇËØ∂„ÄÇExample here where we're basically downloading the sentiment analysis pipeline„ÄÇ

And we've got now the classifierÔºå which we can feed these two texts that you saw in one of the earlier chapters„ÄÇBut now what we want to do is we want to understand what really is happening under the wood so remember that the first thing we need to do is we need to process or pre process these raw texts because„ÄÇ

Basically all neural networks can't do operations on raw textex it's kind of like imagine you want to do like matrix multi„ÄÇ how do you do that on like a stringÔºüAnd so what we can do instead„ÄÇIs we use a tokenizerÔºü

And one of the key things that you should remember is that if you're doing any sort of fine tuning or any like sort of inference or predictions„ÄÇ it's really important that the checkpoint you use here is the same for the tokenizer and the model and that's because when these transformers are pre-trained on a large corpus there's a corresponding tokenizer that was also find you're trained in some sense to learn the vocabulary of that corpus and so if you sort of mix and match a checkpoint for one tokenizer and then a different checkpoint for the model„ÄÇ

 basically you'll get a mismatch in the vocabulary and then you'll get kind of garbage in in your outputs so just that's one sort of thing to watch out for„ÄÇOkayÔºå so we've got a tokenizer„ÄÇüòäÔºåAnd now we've got these same raw inputs„ÄÇ and if we basically feed these two sentences into the tokenizer„ÄÇ you get generally there are two things that you just sort of need to remember„ÄÇ

 You're going to get something called input Is„ÄÇAnd these input Is are basically a mapping of every single token in our sequence to a unique number or a unique integer to be precise„ÄÇ and this is basically a mapping in the vocabulary so imagine that I was thinking about like I don't know the whole English language where I'm just dealing with words then I'm going to have probably several hundred thousand words or tokens in my vocabulary and then if I get like the word whole„ÄÇ

 I would like to be able to match that to number that corresponds to this mapping in the vocabulary„ÄÇBut as we sawÔºå I think in the first chapterÔºå in fact we might see it as well today„ÄÇ this kind of like tokenization in terms of words is not very efficient and so what we usually do is something a bit cleverer„ÄÇ but the basic idea is that every single token in this input is going to be mapped to a number and then those numbers allow us to sort of distinguish between different tokens in the sequence„ÄÇ

So that's what input ID are„ÄÇAnd the other thing that you're going to see today in more detail is something called an attention mask„ÄÇ and I'll explain a bit more later on what this is really doing„ÄÇ but you can already see that it's kind of putting a bunch of ones at some part of the sequence and a bunch of zeros towards the end of the sequence„ÄÇ and this will become clearer later on„ÄÇOkayÔºå so we've got the tokenizer„ÄÇ

 so we've now converted our raw text into these IsÔºå these numbers we can operate on„ÄÇAnd then let me just make sure I load the correct checkpoint here„ÄÇSo now we're going to load the modelÔºå so this is the thing that will process these inputs„ÄÇAnd let me just delete this„ÄÇOkayÔºå and so then the question is how do you feed your inputs to your model so the simplest way is to just take this dictionary that we have here„ÄÇ

 which has two keysÔºå it has input IDs and attention mask„ÄÇAnd then we can just use the standard Python unpacking operator to just feed all of the keys and values to the model„ÄÇAnd when we do thisÔºå this will basically feed the inputs to the forward pass of the model to generate the outputs„ÄÇAnd so one way we could look at thatÔºå I think we can probably do this„ÄÇ if we look at the forward„ÄÇ

You can see here in the coab it's showing us basically what the arguments this forward pass can accept„ÄÇ so it tells us we can accept input IDsÔºå we can have an attention mask and then there are like some more kind of sophisticated or advanced things we could also provide but you know we don't need to do them for today but just so you know there are other things that you can do„ÄÇ

So you can see thatÔºå okayÔºå we need to provide at least these input Is in attention mask„ÄÇüòä„ÄÇAnd so when we do the unpacking like hereÔºå this will basically run through the forward pass and produce some outputs„ÄÇAnd as we saw in the video„ÄÇThese outputs are basically called like hidden states and these hidden states are just some sort of like like say compressed representation of the text„ÄÇ so we're taking this raw textÔºå we're converting it first into numbers and then we're taking those numbers and then we're converting those sort of integers into dense vectors so basically every token is now associated with a vector„ÄÇ

And in this case we've got 16 vectors per sentenceÔºå and each vector has 768 dimensions„ÄÇ and that's just because of the way Bt was or distill Bert as well was pre traineded„ÄÇSo let's have a look at one of these vectorsÔºå so we've got outputs„ÄÇSo I'm going to take the first sentenceÔºå so that's the first index„ÄÇ

 and I'm going to look at the first token of this sentence and so if you look at this„ÄÇThey must be slices or integers„ÄÇCa I need to do last hidden state„ÄÇ OkayÔºå good„ÄÇSo actually„ÄÇ let's just take one step back if we just look at the raw outputs„ÄÇYou can see that in transformers„ÄÇ all the outputs from the models are usually wrapped in an object which is kind of something we can then like you know index by attribute name and so here we've got something called the base model output and then this has in this case it's a single attribute called the last hidden state and the Tensor„ÄÇ

So if I want to then access this last hidden state„ÄÇNow I've got a tensor„ÄÇ which has the thing I wanted to do„ÄÇ So I'm going to get the first sentence„ÄÇ I'm going to get the first vector or the first token„ÄÇ sorry the vector corresponding to the first token„ÄÇ And this is now thisÔºå you know„ÄÇ

 huge thing of you knowÔºå numbers fromÔºå you knowÔºå negative to positive„ÄÇ and this should have„ÄÇA size of 768Ôºå where are weÔºüYeah„ÄÇSo this is basically the numerical representation of the first token in the first sequence or the first sentence we passed„ÄÇOkayÔºå so let's just check are there any questionsÔºüOkayÔºå coolÔºå so let's carry on„ÄÇOkay„ÄÇ so this is basically what the numerical representations are produced by the model„ÄÇ

And then as we saw in the videoÔºå these numerical representations by themselves they don't let us do things like text classification„ÄÇ they just say the numerical representation of this token is blah„ÄÇ and now if we want to do classificationÔºå we need to take that vector or these feature vectors and then we need to add them or combine them with a classification head„ÄÇAnd so the whole Transer library is built around this idea of like taking like a model for task X and task X can be things like sequence classification„ÄÇ

 question answeringÔºå summarizationÔºå translationÔºå so on so forth„ÄÇAnd in this case„ÄÇ when we instantiate a model with sequence classificationÔºå as we saw before„ÄÇ this is now going to create„ÄÇA model„ÄÇWhich has„ÄÇA number of labels„ÄÇ so you can see here we've now got a model with two labels because that's what this pre train checkpoint has„ÄÇ

And then when we look at the outputs„ÄÇWe've now got instead of having just these last hidden states„ÄÇWe've got„ÄÇLoges„ÄÇAnd these logicits are basically what happens when you feed these feature vectors through this linear layer„ÄÇ this will now compress these 768 dimensional vectors into just two numbers or project them into two numbers„ÄÇAnd these are the things that we can then use to derive probabilities and figure out for example„ÄÇ

 which class is the most likelyÔºå so you can see here that you know this one here is more likely than this one and vice versa because I think the second example is like a negative sentiment„ÄÇOkayÔºå so that's more or less how we think about the outputs from a model versus a model with a classification head„ÄÇ

And here what we can see is if we want to convert our lodges from into probabilities„ÄÇ we can just take a softmax over them and you may remember that the softmax basically takes all of the inputs„ÄÇ it exponentials them and then it normalizes that exponential by the sum of all the exponentials so you basically end up having something that ranges from zero to1„ÄÇ so it's a good candidate for a probability„ÄÇAnd if we do that„ÄÇ

 we then get now probabilities for each of the two sentiments„ÄÇAnd also we now can see this is the way we can map between the label IDÔºå which says„ÄÇ you know what does zero mean in terms of something that's a bit more meaningfulÔºüOkay„ÄÇ so let's have a look„ÄÇLet's see„ÄÇOkayÔºå greatÔºå So we have a question from SRM Sumya„ÄÇ

Which says the classification model should take the output from the distiller model that's exactly right so in fact's let's have a look at this„ÄÇIf we look at„ÄÇClassÔºå I'm doing this for BertÔºå but it's the same for dist Bt„ÄÇ So if we take Bt model for„ÄÇWhere the full sequence classification„ÄÇSo if you look at what this model actually hasÔºå it has the BRT model that we saw or the distilled Bt model we saw in our example„ÄÇ

 and then it just applies dropout and a linear layer„ÄÇAnd linear layer has a dimension of the hidden size of the 768„ÄÇ and then it's going to compress that into just these two numbers defined by the number of labels„ÄÇAnd so if we look down at what happens inside the forward pass„ÄÇ

 the first thing we do is we get the outputs from the BRT model„ÄÇ So these are just the feature vectorsÔºå these 768 dimensional vectors„ÄÇAnd then you can skip most of this kind of stuff„ÄÇ the main point is that„ÄÇhere„ÄÇWe„ÄÇBut but well„ÄÇ don't worry about the port outputÔºå the main thing is that we feed these outputs into the classification head to produce the logicits„ÄÇ

So that's a great question„ÄÇYes„ÄÇÂóØ„ÄÇËøôËøô„ÄÇOkayÔºå so we've got a question from Platin ChibaÔºå so„ÄÇHow can we see what the token representation means in the textÔºüSo„ÄÇCoolÔºå so maybe just to show you„ÄÇ like„ÄÇSomething that let's seeÔºå maybe we get ahead of ourselvesÔºå but that's okayÔºå okay„ÄÇWe've got these raw inputs which are given by these strings„ÄÇ

 and then we get these input IDs like thisÔºå rightÔºüAnd so one thing we could do„ÄÇIf you want to go backwards and we're going to see this later„ÄÇBut what I could do„ÄÇI can say okay„ÄÇTokenizerÔºå and I'm going to decodeÔºå so I'm going to do the opposite of what I did before„ÄÇAnd now I'm going take my input Ids„ÄÇAnd fingers crossed„ÄÇThis need to do input„ÄÇHies„ÄÇ

And now you can see by using this decocode methodÔºå we're able to kind of reverse the process of the broad text„ÄÇ but what it does is it also introduces some special tokensÔºå one is called the CLS token„ÄÇ which kind of just tells you this is like the start of the sentence„ÄÇ and then we have a Sep token which basically is used to distinguish between pairs of sentences„ÄÇ

So this is one where you can go back from where you started„ÄÇAnd yeahÔºå if you have more questions„ÄÇ we can tackle them as we go ahead„ÄÇOkayÔºå coolÔºå so that's some the the sort of first look at how the pipeline works under the hood„ÄÇSo now what we could do„ÄÇIs let's have a look at like the models in more detail„ÄÇSo I'm going to start by watching this video and then we'll pause for questions and then again look at some code„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_14.png)

![](img/40873acb06abf924ac4a43fae802679a_15.png)

How to instantiate a transforms model„ÄÇIn this video„ÄÇ we'll look at how we can create and use the model from the Transformers library„ÄÇAs we seen before„ÄÇ the Automod class allows you to instantiate a proed model from any checkpoint on the I face app„ÄÇIt will pick the right model class from the library to instant shade the proper architecture and load the weight of the preed model inside„ÄÇ

As we can seeÔºå when given a bird checkpointÔºå we end up with a bird model and similarly for GPT2 or part„ÄÇBeyond the scenesÔºå this API can take the name of a checkpoint on the earth„ÄÇ in which case it will download and cache the configuration file as well as the model weights file„ÄÇYou can also specify the path to a local folder that contains a valid configuration file and a model of weights file„ÄÇ

To instantiate the between modelÔºå the Automodal API will first open the configuration file to look at the configuration class that should be used„ÄÇThe configuration class depends on the type of the modelÔºå BÔºå GPT2 or parttÔºå for instance„ÄÇOnce it has a proper configuration classÔºå it can instantiate that configuration„ÄÇ which is a blueprint to know how to create the model„ÄÇ

It also uses this configuration class to find the proper model class„ÄÇWhich is then combined with the loaded configuration to load the model„ÄÇIts model is not yet a pro modelÔºå as it has just been initialized with randomdom weights„ÄÇThe last step is to load the weight from the model file inside this model„ÄÇ

To easily load the configuration of a model from any checkpoint or a folder containing the configuration file„ÄÇ we can use the autoconfig class„ÄÇLike the Automod class„ÄÇ it will pick the right configuration class from the library„ÄÇWe can also use a specific class corresponding to a checkpoint that well need to change your code each time we want to try a different model architecture„ÄÇ

As we said beforeÔºå the configuration of a model is a blueprint that contains all the information necessary to create the model architecture„ÄÇFor instanceÔºå the bird model associated with a bird based case checkpoint has 12 layers„ÄÇ a hidden side of 768„ÄÇAnd the vocabulary size of 28996„ÄÇOnce we add the configuration„ÄÇ we can create a model that has the same architecture as a checkpointÔºå but is randomly initialized„ÄÇ

We can vet training it from scratch like any by doch model„ÄÇWe can also change any part of the configuration by using keyword arguments„ÄÇSo sequence snippet of code instant shades a randomly initialized layout model with 10 layers instead of 12„ÄÇSaving a model once its trend off fine tune is very easy„ÄÇWe just have to use the safe between method„ÄÇ

HereÔºå the model will be saved in a folder named My beltt model inside the current working directory„ÄÇSuch a model can then be re using the from between method„ÄÇTo learn how to easily approach this model to the aÔºå check out the push to video„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_17.png)

![](img/40873acb06abf924ac4a43fae802679a_18.png)

„ÄÇSo any questions so far about the model like loading and saving models before we dive into some codeÔºü

So„ÄÇJust to sort of summarize what we saw in the video„ÄÇ whenever we do this from pretrained method with a model„ÄÇ we first need to get a config and then we saw that config just a couple of minutes ago„ÄÇ it defines things like the mapping of the labels to the ideas and how many labels the model has and all that kind of stuff„ÄÇ

 how many layers all those thingsÔºå and then that config is then used to load the weights of the model so that it makes sure that everything is kind of configured in the right way„ÄÇAnd then once we have„ÄÇThis modelÔºå we can then save it and then use it for other things„ÄÇSo„ÄÇ

If there's no kind of urgent questions right nowÔºå I'll have a look at„ÄÇThe models code„ÄÇ just as a mentionedÔºå you can watch these videos and your own time and work through this kind of text„ÄÇ but I think it might be sort of more useful if we just have a look„ÄÇ

![](img/40873acb06abf924ac4a43fae802679a_20.png)

![](img/40873acb06abf924ac4a43fae802679a_21.png)

At„ÄÇAt the codeÔºå so„ÄÇÂóØBut but but„ÄÇLet's just checkÔºå I can run transformers„ÄÇokÔºåÊâÄ„ÄÇOne thing maybe to mention is„ÄÇA really common example or situation that youll find yourself in is you basically you've trained a model and now you want to share it in some way and the sharing typically at least when I was working in my previous company„ÄÇ

 it was much more about deploying this model so that you could serve it or produce predictions that other services could consume„ÄÇAnd so once you've saved your modelÔºå the question isÔºå okayÔºå what the hell do I do with this thingÔºüüòä„ÄÇAnd as we can see hereÔºå this save thing will basically save two objects„ÄÇ it will save a configuration JO fileÔºå and it will also save a Pytorrch model do bin file„ÄÇ

 and this is something in Pytorrch called a state dictionary which basically provides all the information for the layers and the weights„ÄÇAnd so if we want to use this in like to produce predictions„ÄÇThe first thing we need to do is what we've always been doing is we take some input text„ÄÇWe convert it into input IDs„ÄÇAnd then we need to convert those input IDs into tensesors„ÄÇ

 which we can then feed to the model„ÄÇAnd so previously what we were doing was using like the tokenizer„ÄÇ and that's exactly what you would also do in practice„ÄÇ but in this example„ÄÇ we're just showing the outputs of the tokenizer„ÄÇSo let's have a look at what that looks like in code„ÄÇSo Is check if there are any questionsÔºå okayÔºüOkayÔºå so„ÄÇüòäÔºåÂóØ„ÄÇüòä„ÄÇ

Maybe just to quickly summarize we've got you can also load your configurations using two different things„ÄÇ you can either load your model directly from one of the default configs in the library and then this will provide you with like you know a kind of summary about the hidden size and so on„ÄÇ

But if you do thisÔºå the model is completely randomly initialized„ÄÇ which means all the weights are just random and this model is going to just be garbage„ÄÇ it's not going to help you make any good predictions and this is what you do actually when you want to pretrain a model or you want to really train a model from scratch„ÄÇÂóØ„ÄÇSo in practiceÔºå most of the timeÔºå what you're really doing is using the from pretrained and then this will initialize the model with the pretrain weights and the correct head if we need it„ÄÇ

So if we wanted to doÔºå say predictionsÔºå let me just instantiate this„ÄÇSo let's suppose that I've got my model and I'm happy with it and so I want to„ÄÇSave it so I can deploy it somewhere„ÄÇSo let's just wait through this model to download„ÄÇOkayÔºå good„ÄÇ So then what I could do is I could save my modelÔºå and this is just some path on your your on your machine so„ÄÇ

If we now look„ÄÇInside the file system„ÄÇWe can see that we've got a directory called directory on my computer„ÄÇ So now if I have a look at what's inside that directory„ÄÇI've got these two files„ÄÇ I've got this config JasonÔºå and I've got this like binary file called Pythtorage model„ÄÇAnd so„ÄÇWhat we can do now is we can take that folder and we can you know wrap it upÔºå zip it up„ÄÇ

 put it on a machine„ÄÇAnd then if we want to get new predictions„ÄÇThen what we do is we take our tokenized inputs„ÄÇWe then feed those or convert them into a tensor because all the ptorch models expect towarch tensors„ÄÇAnd so if we look at this model inputs„ÄÇIt's just going to be a tensor„ÄÇAnd then we feed these inputs to the modelÔºå and then this is now what would constitute a prediction„ÄÇ

 and then you can you know do whatever you want with that prediction„ÄÇ maybe use it to make some sort of decisions or maybe use it to feed a dashboard„ÄÇ basically the skysal limit„ÄÇAnd that's more or less like sort ofÔºå you know„ÄÇ how you generate predictionsÔºå' pretty straightforward„ÄÇSo let's have a look„ÄÇ

 We've got a question here„ÄÇOut of interestÔºå how long would it take to train Bert from scratch and can you do it on coabÔºü

OkayÔºå so„ÄÇI think if you it really depends on the size of the corpus that you want to use So for example„ÄÇ Bert was trained if I'm not mistaken on all of English Wikipedia„ÄÇAnd a corpus called the Books corpusÔºå which is sort of scanned library books„ÄÇAnd„ÄÇI think„ÄÇLet me think„ÄÇSoÔºå you know let's do something like this let's why don't we find the answer so on the fly because I don't remember off the top of my head how long it took them to do it„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_23.png)

![](img/40873acb06abf924ac4a43fae802679a_24.png)

![](img/40873acb06abf924ac4a43fae802679a_25.png)

And there's nothing better than live„ÄÇReading papers„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_27.png)

Okay„ÄÇüòäÔºåSoÔºå here's the book paper„ÄÇAnd let's have a look atÔºå I'm guessing they use TPUs„ÄÇOkayÔºå so„ÄÇThey say here that they trained BerRT base on four cloud TPUsÔºå so this is 16 TPU chips„ÄÇAnd each pre training took four days to complete„ÄÇSo„ÄÇI think from memory„ÄÇ the cloud TPUs you get on coabab are just one TPU chip„ÄÇSo sort of roughly speaking„ÄÇ

 it would take you maybe 16 daysÔºå16 times 4„ÄÇ So 64„ÄÇDays to train on curtL„ÄÇYou know„ÄÇ with the same corpus„ÄÇÂóØ„ÄÇüòäÔºåBut„ÄÇI don't think so„ÄÇ YeahÔºå I'm not sure if there's a quick Bt training„ÄÇ HoweverÔºå I will show you something„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_29.png)

There's a blog post by Huging face„ÄÇLet's see on training„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_31.png)

A model on EsperantoÔºå so I'll chuck this in the chat„ÄÇSo„ÄÇCan I do thatÔºüOkayÔºå so this blog post„ÄÇ it uses a slightly older APIÔºå but the basic idea is to show you that you actually can train in a coab a BERT model as long as your corpus isn't too big„ÄÇ so this is Estoranto which is a special language that is you know has much less text than English„ÄÇBut I think from memoryÔºå this was trained„ÄÇIn just an hour and a halfÔºå maybe a few hours„ÄÇSo let's see„ÄÇ

„Å°„Å¶„ÄÇOkayÔºå maybe we don't see it here„ÄÇ We just have to look at the coab„ÄÇÂóØ„ÄÇLet's see„ÄÇ So the training of this model„ÄÇOkay„ÄÇSo yeahÔºå this training took„ÄÇAlmost three hours„ÄÇSo it really kind of depends on the size of your corpusÔºå so in principle you can„ÄÇ but if you want to do something that's like say as powerful as Bert„ÄÇ

 then you're going to need some some more serious hardware„ÄÇOkay„ÄÇÂóØ„ÄÇSo there's another question by I am homess I understand that transfer learning or using a pretrain model is the way to go instead Yes„ÄÇ that's exactly right so the sort of real power of like transformers and NLP sort of nowadays in general is that we don't really want to do pretraining ourselves because again it's expensive and time and takes a long time so I would almost always use a pretrain model if I can„ÄÇ

The only time you might really be stuck is if you're dealing with like a domain that's very different from any pretm model that exists„ÄÇFor exampleÔºå suppose I was trying to train a model on like source code„ÄÇSo you know„ÄÇ in the early days of transformers that there weren't any pre-trained models on source code„ÄÇ like you know trying to for exampleÔºå understand Python the language and so then you know using BERT base like on English and then trying to transfer to source code might be a bit tricky it might not give you very good results and so if you you trained on a source code corpus that would give you better results„ÄÇ

And the other example where you generally need to find an alternative is if you're dealing with like a language that is not one of the sort of commonly supported ones„ÄÇ so my understanding is that there's like many languages for exampleÔºå in Africa„ÄÇ which aren't really represent it highly in Wikipedia and so then this is hard for people to train models or train transformers on and then you typically need to do some sort of tricks to like take something that is like multilingual like a multilingual version of BERT and try to somehow adapt it to your language but these are generally more advanced things that we can talk about later„ÄÇ

Okay„ÄÇSo„ÄÇLet's see„ÄÇ So where were we We have looked at how we can another question„ÄÇ can we change the config parameters of a pretrain model and use itÔºüËØ∂„ÄÇYesÔºå but with some caveats„ÄÇSo„ÄÇFor example„ÄÇLet's think about what can we change and what can't we changeÔºüSo„ÄÇÂóØ„ÄÇI want to make sure I don't say something silly„ÄÇLet's have a look at the model config we have here„ÄÇ

So this is the„ÄÇThis is the config associated with Bert Bates„ÄÇAnd here you can see that there's a bunch of hyperparameters„ÄÇThat were associated with the pre training of this model„ÄÇSo„ÄÇFor example„ÄÇLet's see„ÄÇSo I have a suspicion that if we change many of these things„ÄÇ

 we're going to break the model in a non trivial way„ÄÇHowever„ÄÇLet me think„ÄÇWhat happens if we change the number of hidden layersÔºüüòîÔºåSo you know what„ÄÇ let's try the usual way of doing things in deep learning is just to try„ÄÇSo„ÄÇI'm going to try to change„ÄÇ SoÔºå so Bt has a number of attention heads„ÄÇ

 so I'm going to see what happens„ÄÇIf I reduce the number of tension heads from 12 to6„ÄÇLet's see„ÄÇIf this works„ÄÇSo let's have a look at the config to make sure that worked„ÄÇSo now we've got attention heads six„ÄÇNowÔºå what happens if we try to feed some inputs to this modelÔºü

Okay„ÄÇüòäÔºåOkayÔºå so„ÄÇInteresting„ÄÇOkayÔºå so„ÄÇIt seems that we can change the config and things work in the sense that we don't get errors„ÄÇBut I have a suspicion that like hacking into this in a pretrain model would affect the the kind of performance in some non trivial way because„ÄÇ

If we think about like what happens when we do something like text classification„ÄÇ we're taking the whole like base model of BRT and then we're just stacking on top of this the classification head„ÄÇAnd if I start kind of like you knowÔºå doing an like dissecting Bt into pieces or somethingÔºå you know„ÄÇ reducing the attention heads or changing the number of transformer layers„ÄÇ

 so Bert has 12 encoder layers„ÄÇI have a suspicion that I would probably have some sort of non trivial or negative impact on the downstream task„ÄÇ like classification that I want to fine tune on„ÄÇBut maybe Omar has a different insight here„ÄÇOkay„ÄÇSoir„ÄÇThat's a good question I've actually never hacked into a pre trend model this way„ÄÇMaybe you could try and see you like do some experiments like what happens if I completely change the number of layers the number of attention heads invert and to try to do classification like sentiment analysis„ÄÇ

 do I get better or worse performanceÔºüI have a feeling it would be worse„ÄÇ but it'd be a cool thing to check and if you do check please share it on the forum„ÄÇOkay„ÄÇSo that was the look at sort of how we generate predictions„ÄÇ let's now have a look at the tokenizers„ÄÇIn more detail„ÄÇSo„ÄÇLet's„ÄÇCcroros our fingers that the Internet still works„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_33.png)

Okay„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_35.png)

In the next few videosÔºå we'll take a look at the tokens„ÄÇIn natural language processing„ÄÇ most of the data that we handle consists of raw text„ÄÇ However„ÄÇ machine learning models cannot read or understand text in its raw form„ÄÇThey can only work with numbers„ÄÇSo the tokenizer objective will be to translate the text into numbers„ÄÇ

There are several possible approaches to this conversion„ÄÇ and the objective is to find the most meaningful representation„ÄÇWe'll take a look at three distinct organizationization algorithms„ÄÇ We compare them one to one„ÄÇ So we recommend you take a look at the videos in the following order„ÄÇ firstÔºå word based„ÄÇ

 followed by character basedÔºå and finallyÔºå sub word based„ÄÇüòä„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_37.png)

Yeah„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_39.png)

ÂóØ„ÄÇOkayÔºå so„ÄÇ„Å§„Åë„Çâ„Çå„Åß„Åô„ÄÇOkay so that was like a high level overview of what we're been talking about that there this general process we have to go through of converting text into into numbers„ÄÇThere's a bunch of videos in this section that you can look at„ÄÇ which show the different types of ways you can tokenize text„ÄÇÂóØ„ÄÇüòäÔºåam I all right„ÄÇ can you guys see me or notÔºüCan you„ÄÇOkayÔºå goodÔºå great„ÄÇOh good„ÄÇYeah„ÄÇüòäÔºåThe joys of Home officeÔºå okay„ÄÇüòä„ÄÇ

What I was saying is there are different approaches or strategies you can take for tokenizing text and the advantages and disadvantages of them just depend on the application you're interested in„ÄÇSo I'm not going to go through the videosÔºå you can watch these yourselves„ÄÇ

 but let's just have a quick look at the sort of three most popular approaches„ÄÇSo the sort of first thing I might imagine is if I got like a text like Jim Henson was a puppeteer„ÄÇThen what I might do is sayÔºå okayÔºå I just want to split this text into words„ÄÇAnd in English„ÄÇ a simple like trick to do that is just to split on white space„ÄÇ So most of the time in English„ÄÇ

 if there's a white spaceÔºå that's the boundary between words„ÄÇAnd then this would convert„ÄÇ for exampleÔºå Jim Henson was a puppeteer into these five tokensÔºå so in this case a word is a token„ÄÇBut there are like several languages where this is like a terrible ideaÔºå so for example„ÄÇ if you have ever learnt Japanese you have characters called Kanji and these kanji don't have any words for any space it's just a sequence of Kanji and in general they're actually not even written from left to right they're written from top to bottom„ÄÇ

So doing this kind of splitting or tokenization in terms of white space just wouldn't work„ÄÇAnd so an alternative approach is to try something called character based„ÄÇ so this would be like imagine you just split every letter in an English sequence into its own token„ÄÇ and this would actually be then quite good for Japanese because every character is a kanji character which then you know we could represent with a token„ÄÇ

And so the kind of thing you can see here is that it really the sort of tokenization strategy seems to really depend on the language that we're studying„ÄÇAnd so„ÄÇThe thing that like a lot of research has gone into is trying to find something that gives you like a good trade off between these two kind of extremes of word tokenization and character tokenization„ÄÇ

And maybe I should also mention a couple of drawbacks before we go into that„ÄÇ so one of the drawbacks with word tokenization„ÄÇIs that this will create a vocabulary which is the size of the number of words in our language„ÄÇ so basically if we have imagine we just tokenize English„ÄÇ then we will need a token for every single word in the English language and this sum this is generally huge it's going to be several hundred thousand tokens which makes it very like computationally expensive„ÄÇ

But the other thing that's kind of not great about this is that it doesn't make any sort of distinction between like like I don't know„ÄÇ dog and dogsÔºå which are kind of likeÔºå you knowÔºå similar words and we're kind of representing them now with two independent tokens„ÄÇ

So that's the drawback with the word ones and the character based ones have the drawback that the model has to basically learn what a word actually means because the only thing it gets now characters or it gets character tokens„ÄÇAnd then it has to figure out over training thatÔºå okay„ÄÇ

 if I put together these characters in this orderÔºå this seems to represent like a more abstract object like a word„ÄÇ and so this at least for English would be not a great strategy„ÄÇSo most tokenizers„ÄÇ they use something called subword tokenization„ÄÇAnd the basic idea is that instead of like just splitting on word boundaries or on characters„ÄÇ you basically split or you decompose a word into sub wordss„ÄÇAnd an example here is like„ÄÇ

 let's take the word annoyinglyÔºå so annoyingly can be represented as maybe two sub wordss„ÄÇ annoying and Lee„ÄÇAnd then what we can do is we can just kind of collect the frequencies of these subwords and then use this to figure out basically what I like the most frequent subwords in the language and then we can use those sub wordss to build back the full word itself„ÄÇ

 so if you know that you've got annoying and LeeÔºå you can then reconstruct annoyingly from these two components„ÄÇAnd so like I guess there's an example hereÔºå you can sort of split„ÄÇ let's do tokenization into these subwords so that you can see this is kind of a mix of a wordtoken with a subwordization„ÄÇ and we've also got the exclamation mark being treated as its own separate token„ÄÇ

And the sort of most common tokenizers that you would seeÔºå there's a good question„ÄÇ I'll get to that are things called word peaceÔºå which is the one that Bert used or sentence piece„ÄÇ which is the one that GT and the GPT models are typically use„ÄÇSo there's a really good question„ÄÇ how do you design the subweb boundariesÔºå is it manualÔºü

So this is more or less determined by the algorithm that you choose to use„ÄÇI think„ÄÇLike in general„ÄÇ it's a mix of like manual rules and also learning a form of learning from the corpus„ÄÇ So let's have a quick look at„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_41.png)

LetSeeÔºå I think it's the sentence piece paper„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_43.png)

ËøôËøôËøô„ÄÇSo this isÔºå I'm going to put this in the chat„ÄÇOkay„ÄÇ so this is one of the most famous papers on tokenization„ÄÇAnd let's have a quick look at„ÄÇSo how are these boundaries„ÄÇOkay„ÄÇÂóØ„ÄÇYeahÔºå that's right„ÄÇüòäÔºåThat's what I remember from this paper„ÄÇ So they say that historicallyÔºå most like tokenization„ÄÇAlgorithms„ÄÇ

 they were they used manual rules and the problem with thisÔºå of course„ÄÇ is that for every language in your during set of rules and it's a real like pain to„ÄÇTo sort of maintain and extend„ÄÇAnd so if I'm not mistaken„ÄÇ sentence piece is kind of like a learned tokenizer„ÄÇ

 so you actually have like a sort of optimization objective and then you train this like you train a model and so by training this on your corpus„ÄÇ you actually learn the word boundaries„ÄÇÂóØ„ÄÇüòäÔºåBut I haven't read this for a few years„ÄÇ and I might be forgetting something but„ÄÇYeahÔºå that's a good question and I think it may something that we can add in a future version of course„ÄÇok„ÄÇüòäÔºåSo„ÄÇWhere were weÔºå So we were looking at these different tokenization strategies„ÄÇ

 So let's maybe look at the coab„ÄÇÂóØ„ÄÇSo„ÄÇOne of the things I often like to do is to sort of capture the outputs in my piping stools on coLab so I don't have this humongous„ÄÇÊ≤°„ÄÇOf installation„ÄÇOkayÔºå so what you can see here is what we were talking about before„ÄÇ

 this is if you just do word peace tokenization oh sorry word splitting into words and now we can have a look at like what the BRT tokenizer does„ÄÇAnd there are two ways you can do this in transformers„ÄÇ you can specify the specific class that you want to use for the tokennova„ÄÇAnd this is if you know happen to be maybe doing something very specific and you really you want to make sure you get the ver organizer„ÄÇ

But the thing that I personally use all the time is just the auto tokenizer because this will automatically convert the tokenizer into this class anyway„ÄÇ so if I provide a checkpoint and it can identify thatÔºå it will then automatically load it this way„ÄÇ

OkayÔºå so if we take a tokenizer„ÄÇIt converts the text into these input IDs„ÄÇBut now let's have a look at something here„ÄÇSo why are we doing this twiceÔºüüòîÔºåOkay„ÄÇOkay„ÄÇ good so what we're doing here is we're just taking a sequence of text and then we're extracting the tokens as a list and so you can see here„ÄÇThat in the case of BERTÔºå which uses this word piece tokenization algorithm„ÄÇ

The way it distinguishes like words from sub wordss is using this double hash symbol„ÄÇSo you can see here that in the vocabulary of the Bt tokenoer„ÄÇIt has learned that it's good to split words between trans and everything else and if we wanted to reconstruct these two words„ÄÇ we just need to know that this double hash means that this former belongs to trans to build transformer„ÄÇ

And so one way you can reconstruct the sentence is you can take your tokens and you can convert them back into input IDs like this„ÄÇ so this will create these IDsÔºå and then you can decode these input IDs to build back the original string„ÄÇAnother way you could do this is let's have a look where we have our inputs„ÄÇ„Å®„Å¶ Ïôú„ÄÇOkay„ÄÇ so another way you could do this is if I take my„ÄÇTokenizer„ÄÇAnd I just tokenized my sequence„ÄÇ

Then this produces what we saw before„ÄÇAnd then what I could do is I could go tokenizer„ÄÇdecode„ÄÇI put my inputs and my input Is„ÄÇAnd this should return what we saw before„ÄÇ And now you can see the difference between this approach and the one here is we don't have these„ÄÇSpecial tokensÔºå so if you don't want these to be presentÔºå I think we can do skip special tokens true„ÄÇ

And then this will give us back the original sequence„ÄÇCool„ÄÇ so that's some more or less like a sort of deep dive into the tokenizers„ÄÇÂóØ„ÄÇüòä„ÄÇMaybe one thing to mentionÔºå let's have a look at a different tokenizer so you get an idea of„ÄÇWhat you might also see„ÄÇ So let's find a GPT model„ÄÇThat is not going to block the collab„ÄÇSoÔºå G to„ÄÇ

Let let's do maybe this one„ÄÇSo I'm going to just take a tiny GPT„ÄÇ you can also copy the name of the checkpointÔºå which is quite handyÔºå so we are here„ÄÇSo what I'm going to do is I just want to show you the difference between the GPT model and the way it tokenizes„ÄÇ so hopefully this works„ÄÇYeahÔºå so GPT has a kind of very quirky„ÄÇ

Tokenizer where it uses this weird symbolÔºå it's like a G with a little like thought on top of it„ÄÇ and this is what it uses to indicate that there's a white space between this token and this one„ÄÇSo you can see that it's sayingÔºå okay„ÄÇUsingÔºå and then there's a white spaceÔºå then R„ÄÇ then there's a white space and then transÔºå but then former has no white space so if we wanted to reconstruct this„ÄÇ

 we just stitch this back to this„ÄÇBut then there's a white space with a network and so on„ÄÇ and so you can see this is kind of quite different to the Bt one which basically treats every token as having a corresponding white space unless we have the two hash symbols„ÄÇ

 whereas GPT2 is kind of the other way around it says„ÄÇ assume no there is no white space unless I put a special symbol like this„ÄÇOkay„ÄÇ so are there any questions about the tokenizersÔºüOkayÔºå okayÔºå so let's now have a look„ÄÇAt how we can handle„ÄÇMultiple sequences together„ÄÇI'll start the video„ÄÇ

How to batch inputs together in this videoÔºå we also see how two batch input sequences together„ÄÇIn share all of the sentences we want to pass through our model won't all have the same length„ÄÇHere we are using the model we saw in the sentiment analysis pipeline and want to classify two sentences„ÄÇWhen tokenizing them and mapping each token to its corresponding input ID„ÄÇ

 we get two lists of different length„ÄÇTrying to create a densor or an Mbi array from the list will result in an error because all arrays and densils should be recangular„ÄÇOne way to overcome this limit is to make the second sentence the same length at the first by adding a special token as many times as necessary„ÄÇ

Another way would be to trun gate the first sequence to the length of the second„ÄÇBut you would then lose a lot of information that may be necessary to properly classify the sentence„ÄÇIn generalÔºå we only truncate sentences when we are longer than the maximum length the model can handle„ÄÇThe value used to pad the circum sentence should not be picked randomly„ÄÇ

The model has been portrayed with a certain padding IDÔºå which you can find in tokenazizer„ÄÇpa tokenite„ÄÇNow that we have p sentencesÔºå we can make a batch with them„ÄÇIf we pass the two sentences to the model separately and patch together however„ÄÇ we notice that we don't get the same results for the sentence that is pad here the second one h is that a bug in the transformers library now if you remember that transformers will all make easy user of attention layers„ÄÇ

 this should not come as a total surprise„ÄÇWhen computing is the contextual representation of each token„ÄÇThe attention layers look at all the other words in the sentence„ÄÇIf you have just a sentence or the sentence with several padic tokens that it„ÄÇ it's logical we don't get the same values„ÄÇTo get the same results with or without padding„ÄÇ

 we need to indicate to the attention layers that we should ignore those padding targets„ÄÇThis is done by creating an attention maskÔºå a tonsil with the same shape as the input IDs with series and ones„ÄÇOnce indicates the tokens the attention layers should consider in the context„ÄÇ and the the tokens we should ignore„ÄÇNowÔºå passing this attention mask along with the input ID will give us the same results as when we send the two sentences individually to the model„ÄÇ

This is all done behind the scenes by the tokenizer when you apply to several sentences with the flag padding equal to„ÄÇIt will apply his bedding with a proper value to the smaller sentences and create the appropriate attention mask„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_45.png)

![](img/40873acb06abf924ac4a43fae802679a_46.png)

Yeah„ÄÇOkayÔºå so I see we have a couple of questions so the first question from IM homesmes„ÄÇ I don't understand why we needed the extra dimension„ÄÇBased on the error message that has returned„ÄÇ how would you troubleshoot to determine that you needed another dimensionÔºüOkay„ÄÇ so I think the best way to look at this is probably with some codeÔºå so let's go here„ÄÇüòîÔºåAnd„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_48.png)

Install transformers„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_50.png)

OkayÔºå so„ÄÇIf I understand the question from O homesÔºå you're talking about this error message„ÄÇ Let's see if it is reproducible„ÄÇOkayÔºå good„ÄÇ So I think you're talking about this index error„ÄÇThat we're getting here„ÄÇSo let's have a look how we might debug this„ÄÇSo the error is saying that the dimension is out of range„ÄÇAnd we expected to be in the range -1 to 0„ÄÇ

 but got one„ÄÇSo let's have a look at what the shape of our inputs are„ÄÇ This is sort of how I would debug this messageÔºå so„ÄÇOkayÔºå so we can see that the„ÄÇThe input IDs have a size of just 14Ôºå and that's just representing the 14 tokens that we get when we tokenize the sequence„ÄÇÂóØ„ÄÇüòäÔºåSo I'm going to show you a dirty secret of how most software engineers debug stuff„ÄÇ

 so you take this„ÄÇAnd you checkuck it into Google„ÄÇAnd then you goÔºå aÔºå this looks like a pythor ch„ÄÇAnd then we have a look and we see if someone can explain what's going on„ÄÇSorry„ÄÇYou can see some messageÔºå someone has a thing hereÔºå getting some dimension at a range error„ÄÇU„ÄÇ and then okayÔºå this looks like it's like a deeper issue in pytorrchÔºå maybe that's going so helpful„ÄÇ

But then maybe let's look a stack overflow this is often where you'll find a good answer„ÄÇ so someone is getting the same kind of error and let's see what someone told them in the answer„ÄÇSo they're saying that you're giving a 1D tensor to this thingÔºå but it expects this kind of object„ÄÇSo let's see if that is kind of relevant to what we're doing„ÄÇSo„ÄÇIf we look at the error here„ÄÇ

 it's saying that at some point in the stack trace„ÄÇWe tried to compute sequence length„ÄÇBy taking the size of the input Is„ÄÇAnd then we actually tried to access the second dimension of the input IDs„ÄÇAnd so if you look at„ÄÇThe the size that we provideÔºå it only has one dimension„ÄÇ so basically„ÄÇ I can access„ÄÇSize 0Ôºå because that's the„ÄÇThe the first first dimension that's available„ÄÇ

 I don't know why coab is so slow„ÄÇHoweverÔºå if I tried to access the second dimension of a one dimensional object„ÄÇ then that's not possibleÔºå so it's going to throw this kind of error„ÄÇFor some reason„ÄÇ CoAab is acting really slowlyÔºå I'm just going to restart it„ÄÇSee if can do that„ÄÇMore interesting„ÄÇSo let's seeÔºå is it intent„ÄÇInterestingÔºå so coababÔºå okay„ÄÇLet's see„ÄÇI spin up curl again„ÄÇ

Maybe I have too many colorss open„ÄÇOkayÔºå let's try again„ÄÇSo„ÄÇ‰Ω† to„ÄÇAndst thisÔºå okay„ÄÇ fingers crossed this works„ÄÇSorry forÔºå okayÔºå good„ÄÇWe're trying to debug this error„ÄÇ and we see that the stack trace is telling us it's here„ÄÇ And the problem is that we're trying to„ÄÇDetermine the sizeÔºå or basically we're trying to pick out the size of the dimension in the second component of the input IDs„ÄÇ

But the problem that we have is that the input Is„ÄÇI only have one dimension„ÄÇ so their size like that 14„ÄÇAnd if we access the first elementÔºå we get 14Ôºå that's good„ÄÇ but if we access the secondÔºå we're going to get the same error„ÄÇAnd so here we're seeing that the problem is that we need to basically provide a batch dimension which says that we're dealing with one sentence„ÄÇ

 which has a sequence of length 14„ÄÇ So basically most of the inputs„ÄÇThe input Is„ÄÇIds should have„ÄÇShape„ÄÇThattch size„ÄÇAnd then secrets like that„ÄÇNo„ÄÇDoes that answer the questionÔºå I'm HolmesÔºü

I think that was a bit of a convoluted way of debuggingÔºå but that's more or less how I would do it„ÄÇÂóØ„ÄÇCoolÔºå so then there's another question from SRM Sma about can we modify the padding techniqueÔºü

So the answer is yesÔºå and let's have a look where we do padding„ÄÇ„ÅÇ„Å£„Åü„ÄÇOkay„ÄÇ I'm going to just make this up„ÄÇOkayÔºå so let's have a look„ÄÇSo I've got a tokenizer„ÄÇAnd if I just take a sequence like this„ÄÇI'm going to get these input IDs„ÄÇAnd so then the question might beÔºå if I add some padding„ÄÇ

Then what what's going to happen to my input is so basically this should I think just in this case„ÄÇ provide no no padding tokens„ÄÇSo what I'm going to do is I'm going to create„ÄÇTwo sequencesÔºå so„ÄÇMy dog„ÄÇIs called photo„ÄÇAnd then another sequenceÔºå which is like my cat is„ÄÇIs cold„ÄÇSomething really coolÔºå like„ÄÇI knowÔºå Eliza„ÄÇSo now I've got these two sequences„ÄÇ

 one is shorter than the other„ÄÇSo now if I pass these to my tokenizer with the padding true„ÄÇYou can see that what happens is in the first sequenceÔºå it takes the normal tokens„ÄÇ and then it adds a bunch of zeros to the end„ÄÇAnd the number of tokens it adds is just designed so that it matches the same length as the longest sequence in the„ÄÇIn the in the inputs„ÄÇNow the question is„ÄÇKind of like what strategy can we doÔºüTo do that„ÄÇ

And let's seeÔºå I think we can do do„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_52.png)

So what I'm going to do to find the actual argument I need is I'm going to go to transformformers„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_54.png)

And I'm going to look at padding because I can't remember how I pad on the left„ÄÇSo maybe is it„ÄÇPtting arguments for pretrain„ÄÇTo„ÄÇSoÔºå look at the source code„ÄÇok„ÄÇSo I can do longest max length„ÄÇInterestingÔºå so okayÔºå that only will give us the options„ÄÇ for exampleÔºå if I do„ÄÇMax length„ÄÇThen this will pad to the maximum length of the whole modelÔºå so the modelÔºå which is Bt„ÄÇ

 can process 512 tokens„ÄÇAnd so this will process huge number of zeroes all the way out to the length of 512„ÄÇAnd the other option that we have here is LongestÔºå which is the default of just the longest example in the inputs„ÄÇBut let's see how we've pad to the left„ÄÇÂóØ„ÄÇI remember„ÄÇWe can do this„ÄÇ„ÅÜ„Çì„ÄÇOkayÔºå so„ÄÇLet's see„ÄÇ So we can take in the input„ÄÇPadding strategy„ÄÇAnd„ÄÇHad to multiple„ÄÇÂóØÂóØ„ÄÇI was„ÄÇ

I was pretty sure that I can pat on both sides„ÄÇSoÔºå let's see„ÄÇThanksÔºå Oma has the answerÔºå great„ÄÇüòäÔºåSo„ÄÇ so then we can do this„ÄÇ We do padding true„ÄÇAnd thenÔºå patting side„ÄÇEquals left„ÄÇInteresting„ÄÇ did I do wrong„ÄÇMaybe it's only for some„ÄÇTchkenizes and we can do this„ÄÇOkayÔºå so padding inside„ÄÇSo here we can see that their default values to pad should be right or left„ÄÇ

And this is for the pre trained tokenizer„ÄÇInteresting okayÔºå let's do thisÔºå so„ÄÇLet's see„ÄÇSorry for this live hacking„ÄÇSo let's have a look„ÄÇ If we look at one of the attributes of the tokenizer„ÄÇ it's called padding side„ÄÇAnd here by default it's right„ÄÇ so I can override this attribute by saying make it left„ÄÇSo it's not a keyword argument„ÄÇ

 it's an attribute of the tokenizerÔºå that's what I was missing„ÄÇAnd so now you can see indeed„ÄÇThat we can pad to the left„ÄÇSo I think that should answer the question from SRM swimmer„ÄÇThanks so much for the help with that„ÄÇÂóØ„ÄÇüòäÔºåÂóØ„ÄÇThanks Dk crazy do you see what it's really like in the real world all right so let's see so the question follow up question is will it have an impact in the attention mechanism Okay that's a really good question so„ÄÇ

üòäÔºåThe reason we haveÔºå so there are two things going on hereÔºå so on the one hand„ÄÇÂóØ„ÄÇüòä„ÄÇThere's the padding which we need to do so that we can make sure all the inputs are basically a rectangular array„ÄÇ and that's just the way that we need to do things like matrix modificationification„ÄÇIn the in the network and„ÄÇSo once we introduce padding„ÄÇ

 we introduce the problem that Sylvan mentioned in the video„ÄÇ which is that the attention mask or sorry the attention in general will attend to every single token in the input„ÄÇ so you know in this case here every single one of these zeros in principle is a token which has its own embedding and then when we calculate attention we basically do a pairwise multiplication of every token with every other token in the sequence„ÄÇAnd that would be a problem because this would sort of say to the modelÔºå hey„ÄÇ

 I've got these three tokensÔºå they seem to be related to each other and then when I construct my kind of representation at the end of the encoder„ÄÇ this would have this kind of like artificial like information from padding and we don't really want that because you know padding was something we just artificially injected„ÄÇ

And so the thing that Sylvan mentioned is that we will get in general something called an attention mask„ÄÇ so I'm going to just call this my tokenized„ÄÇWellÔºå let's call it inputs„ÄÇÂØπÁöÑ„ÄÇSo if I look at my inputs„ÄÇ I've got input Ids„ÄÇAnd I've got an attention mask„ÄÇAnd this attention mask will then when we compute the attention inside the„ÄÇ

 it will say every time you see a zero„ÄÇCompletely ignore that token„ÄÇAnd so what you can see is that the tokenizer has correctly figured out that if I say pat on the left„ÄÇ then make sure that there's a mask for the first three elements or the first three padding tokens„ÄÇ so these zeros will basically be we'll say to hurtÔºå ignore this when you compute attention„ÄÇ

 just compute attention on the actual words we care about„ÄÇSo to answer your questionÔºå Sm Ser„ÄÇ it doesn't have an impact because the attention mask takes care of that and it's all done automatically through the tokenizer„ÄÇThanks I HolÔºå that's very kind„ÄÇ I'm doing one next weekÔºå So I think for session three„ÄÇ you can come join„ÄÇüòäÔºåOkayÔºå so let's see„ÄÇ let's maybe have a look at„ÄÇ

 I think the one of the last sections we have here„ÄÇPutting it all togetherÔºå so„ÄÇÂóØ„ÄÇLet's see„ÄÇYeah„ÄÇ let's walk through a little bit the code this of this section„ÄÇ so this is going to kind of bring together all the things that we've learned in this session„ÄÇAnd I'm going to just„ÄÇGo through the code„ÄÇSoÔºå the idea„ÄÇ

Just to remember what we're doing in this chapterÔºå we're kind of deconstructing the pipeline„ÄÇ looking at all the pieces that go into it„ÄÇ And now we're going to sort of put this all together to sort of have our own custom pipeline„ÄÇSo first thing we just load a checkpointÔºå a tokenizerÔºå and feed some inputs„ÄÇAnd in fact„ÄÇ I'm going to do this one hereÔºå which now produces with two sequences„ÄÇ

And so now we can have a look at the different sort of ways that we could pad„ÄÇ So this is„ÄÇ I think we saw this before„ÄÇ the model inputs now are going to have padding tokens that go all the way„ÄÇSorryÔºå they match the longest in the sequence„ÄÇSo in this caseÔºå this is the longest input„ÄÇSo the first one doesn't get any padding tokensÔºå but the second guy gets all these extra zeros here„ÄÇ

And as we saw before„ÄÇThe attention mask will get all these extra zeros to say„ÄÇ don't pay attention to those documents„ÄÇThen as we saw before„ÄÇWe can put max length„ÄÇ and then this will„ÄÇTo diplomaacy„ÄÇThis will now„ÄÇPut a bazillion zeroes on top of everything„ÄÇ So all these zeroes we saw„ÄÇ So this goes up to 512 extra zeroes for the bird„ÄÇM length„ÄÇ

 and then you can also configure how far you want to add padding to if you want to„ÄÇThe other thing I didn't really mention so far is this concept of truncation„ÄÇSo let's have a look at how that could workÔºå so„ÄÇBasically„ÄÇ what will happen most of the time in like unless you're dealing with like tweets or you know„ÄÇ

 very short textsÔºå a lot of the time your inputs will exceed the maximum length that the transformer can process„ÄÇAnd so this is actually one of the main like challenges with transformers is that they're really good at like kind of short to medium length inputs generally„ÄÇ

 but as we go to very long sequences there's two problems„ÄÇ one is that attention is very computationally intensive and expensive to do and the second is that most of the models just predeefine what is the maximum length that the input can have in the pretraining phase and once you define that you can't exceed it„ÄÇ

So in this caseÔºå I want to show you what would happen if we did that„ÄÇ so I'm going to take a sequence and I'm going to say I don't know„ÄÇLet's take this guy„ÄÇAnd„ÄÇI'm going to try and break the so I'm going to times this„ÄÇ' there's this Pro 14 tokens„ÄÇ let's times it by 1 thousand0„ÄÇSo now I've got a really long sequence„ÄÇAnd I want to see„ÄÇ

What happens if I try to pass„ÄÇThis sequence„ÄÇOops„ÄÇTo my tokenizer„ÄÇJust you know„ÄÇ if I just did nothingÔºå what would happenÔºüOkayÔºå so now what you can see is that we get a warning„ÄÇ the token in sequence length is longer than the maximum sequence length of this model„ÄÇWhere's message to to to„ÄÇSoÔºå it's saying that we've got„ÄÇYou knowÔºå way more tokens than we have„ÄÇ

And then you goÔºå ohÔºå okayÔºå I don't care about warnings„ÄÇ So I'm gonna just say„ÄÇWho cares„ÄÇAnd I'm going to try and return tensor tensesors„ÄÇOkayÔºå so this seems to work„ÄÇ And now what happens if I„ÄÇTake a„ÄÇDo we have a modelÔºå Not okay„ÄÇ So from transformers import auto model„ÄÇAnd now what happens if I try to„ÄÇDo we have a checkpointÔºüüòî„ÄÇ

Where is my checkpointÔºüYesÔºå I' check good„ÄÇLet's try to load my model„ÄÇSo what I'm trying to do here is I'm trying to see„ÄÇ I want to show you what happens if we just naively try to pass a 14000„ÄÇALong sequence to both„ÄÇAnd so nowÔºå if I'm not mistakenÔºå our inputs„ÄÇShould break the model„ÄÇIndeed„ÄÇ

So here we can see we've got an index errorÔºå the index error is out of range„ÄÇAnd that is essentially trying to say to usÔºå lookÔºå you tried to do this embedding operation somewhere in the code„ÄÇAnd you are trying to pass an input which violates the constraints set by the model„ÄÇAnd so this is an example whereÔºå you knowÔºå we just broke the model because we gave it something that was too long„ÄÇ

The solution to that is to use the truncation parent parameter and set it to true„ÄÇAnd what this will do is this will now„ÄÇTake our inputsÔºå and truncate them„ÄÇSo let's get the input IDs„ÄÇAnd thenÔºå the size„ÄÇAnd this will now„ÄÇThis has now converted them to the maximum size of the model or trunceted them to the maximum size„ÄÇAnd so now I can feed this to my model and it's happy„ÄÇ

So this is one way you can deal with the the problem of when your text is too long„ÄÇ you can just truncate it„ÄÇ and my experience is generally that truncation works okay for„ÄÇLike classification tasksÔºå so if you're doing like you know multiclass with this kind of thing generally like a lot of the information is actually at the start of the review or the tweet„ÄÇBut you don't want to do this for things like question answering because you know the answer might be somewhere towards the end of the text and then if you truncate it you lose it and we'll probably see later on in another„ÄÇ

Iteration of the courseÔºå how you handle that„ÄÇAnd also in summarization„ÄÇ it's a little bit depends on the caseÔºå sometimes I've been able to truncate and get okay results„ÄÇ but sometimes you have to do clever things like you knowÔºå split the text into different pieces„ÄÇ truncate those piecesÔºå and then kind of aggregate the results„ÄÇOkay„ÄÇ

 so that's truncation and padding„ÄÇÂóØ„ÄÇAnd yeahÔºå I think„ÄÇYeah„ÄÇI think that's pretty much what we need to do for this„ÄÇSo are there any questions at the stageÔºüOkay„ÄÇ so one thing I would like to mention is„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_56.png)

You've probably seen we have the forums and on the forumsÔºå we have a course category„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_58.png)

So if you have any questions that come to mind after this session or just questions in general about transformers„ÄÇ just you can ask them here and one of us will respond„ÄÇI think a really cool thing to do if you have the time is to share your projects„ÄÇ so in fact we can see DK Cr De here sharing his awesome data set for the Model hub„ÄÇ

And you know as you start learning transformersÔºå a really cool way to sort of get some feedback on how you're going is to kind of share your work and at least for me personally„ÄÇ you know I come from a non-computer science background I studied physics and then decided to switch into machine learning for me„ÄÇ

 this kind of sharing was a very powerful way of getting some feedback from the community and also trying to like you know learn how to communicate„ÄÇWhich is a really important part of you know doing any data science in the real world or in general„ÄÇSoÔºå for exampleÔºå a cool experiment would be this one that Tom was asked today of like„ÄÇWhat happens if I change the con figure of the modelÔºå does it break things„ÄÇ

 That would be a cool thing to show or in general„ÄÇLikeÔºå you know„ÄÇ any any sort of training experiments you do reveal awesome awesome„ÄÇÁªßÁª≠„ÄÇOkay„ÄÇ there's one last question from Rash MashikÔºå how to check the default model„ÄÇOkay„ÄÇ so let's have a look at this„ÄÇOkayÔºå so„ÄÇLet's take a pipeline„ÄÇ

So I probably need to go from transformers„ÄÇImport pipeline„ÄÇAnd then I'm going to create a pipeline„ÄÇFor sentiment analysis„ÄÇFor example„ÄÇSo the question is„ÄÇHow do we check what model is being used so the pipe line object it has a bunch of different attributes and the attribute that is interesting is the model„ÄÇIn this caseÔºå so if we look at thisÔºå it's going to tell usÔºå okayÔºå it's like some output„ÄÇ

 but maybe we can just look at the config of the model„ÄÇAnd then we can see that in this case„ÄÇ the model that we're using for sentiment analysis by default is distillbert and it's this checkpoint that we saw earlier in the class„ÄÇSo I hope that answers the question Rash MashikÔºå you can do this for any other pipeline„ÄÇ let's have a quick look what happens if I do question answering„ÄÇ„ÅÜ„ÅÜ„ÅÑ„ÄÇ

So the default model in question answering is going to be„ÄÇDistillber base case on squad„ÄÇSo that's the long we have„ÄÇCoolÔºå so there's another question are the recordings available for session one from Silvan„ÄÇ I was not able to find it on the YouTube channelÔºå I believe„ÄÇüòäÔºåThey will be„ÄÇ but I guess I'll have to check with Silvin later„ÄÇ So I'm pretty sure we try to put everything on YouTube that we can„ÄÇ

 so„ÄÇI'll let you know it up„ÄÇSo yeah thanks a lot for your really cool questions„ÄÇ it's a real pleasure having people interact otherwise I'll just be talking to the screen by myself„ÄÇThanks for your input„ÄÇAnd„ÄÇWe'll see you for the next session„ÄÇ so Sylvane is giving the session tomorrow„ÄÇWhich is the same one as today and next week we kick off with chapter3„ÄÇ



![](img/40873acb06abf924ac4a43fae802679a_60.png)

ÏïÑ„ÄÇ![](img/40873acb06abf924ac4a43fae802679a_62.png)