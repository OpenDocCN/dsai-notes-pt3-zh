- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- è®­ç»ƒç®¡é“ï¼šæ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨
    - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- è®­ç»ƒç®¡é“ï¼šæ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨
    - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome back to your new Pytorch tutorialã€‚ In the last tutorialã€‚
    we implemented logistic regression from scratch and then learned how we can use
    Pyt to calculate the gradients for us with Beck propagationã€‚ Nowï¼Œ we will continue
    where we left offã€‚ And now we are going to replace the manually computed loss
    and parameter updates by using the loss and optimizer classes in Pytorchã€‚ğŸ˜Šï¼ŒAnd
    then we also replaced the manually computed model prediction by implementing a
    pytorch modelã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ã€‚æ¬¢è¿å›åˆ°æ–°çš„ PyTorch æ•™ç¨‹ã€‚åœ¨ä¸Šä¸€ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»å¤´å®ç°äº†é€»è¾‘å›å½’ï¼Œç„¶åå­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ Pyt æ¥è®¡ç®—æ¢¯åº¦ï¼Œç°åœ¨ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä¹‹å‰çš„å†…å®¹ã€‚ç°åœ¨æˆ‘ä»¬è¦ç”¨
    PyTorch ä¸­çš„æŸå¤±å’Œä¼˜åŒ–å™¨ç±»æ›¿ä»£æ‰‹åŠ¨è®¡ç®—çš„æŸå¤±å’Œå‚æ•°æ›´æ–°ã€‚ğŸ˜Šç„¶åæˆ‘ä»¬è¿˜é€šè¿‡å®ç°ä¸€ä¸ª PyTorch æ¨¡å‹æ›¿æ¢äº†æ‰‹åŠ¨è®¡ç®—çš„æ¨¡å‹é¢„æµ‹ã€‚
- en: Then Pytorch can do the complete pipeline for usã€‚ So this video covers steps
    3 and 4ã€‚And please watch the previous tutorial first to see the steps 1 and 2ã€‚
    So now let's startã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_1.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶å PyTorch å¯ä»¥ä¸ºæˆ‘ä»¬å®Œæˆæ•´ä¸ªç®¡é“ã€‚æ‰€ä»¥è¿™ä¸ªè§†é¢‘æ¶µç›–äº†æ­¥éª¤ 3 å’Œ 4ã€‚è¯·å…ˆè§‚çœ‹ä¹‹å‰çš„æ•™ç¨‹ä»¥äº†è§£æ­¥éª¤ 1 å’Œ 2ã€‚é‚£ä¹ˆç°åœ¨è®©æˆ‘ä»¬å¼€å§‹å§ï¼![](img/2b520fac31b9d1c4d2989f76ec98bd5f_1.png)
- en: And firstï¼Œ I want to talk about the general training pipeline in Pytorchã€‚ So
    typicallyã€‚ we have three stepsã€‚ So the first step is to design our modelã€‚So we
    design the number of inputs and outputsã€‚ So input size and output sizeã€‚And then
    alsoã€‚ we designed the forward pass with all the different operations or all the
    different layersã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘æƒ³è°ˆè°ˆ PyTorch ä¸­çš„ä¸€èˆ¬è®­ç»ƒç®¡é“ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬æœ‰ä¸‰ä¸ªæ­¥éª¤ã€‚ç¬¬ä¸€æ­¥æ˜¯è®¾è®¡æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬è®¾è®¡è¾“å…¥å’Œè¾“å‡ºçš„æ•°é‡ï¼Œå³è¾“å…¥å¤§å°å’Œè¾“å‡ºå¤§å°ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†å‰å‘ä¼ æ’­åŠæ‰€æœ‰ä¸åŒçš„æ“ä½œæˆ–ä¸åŒçš„å±‚ã€‚
- en: Then as a second stepï¼Œ we design or we come up with the so we construct the
    loss and the optimizerã€‚ and then as a last stepï¼Œ we do our training loopã€‚So this
    is the training loopã€‚ So we start by doing our forward passã€‚ So here we computeã€‚Or
    let's write this downã€‚ Compute the predictionã€‚Then we do the backward passï¼Œ backward
    passã€‚ So we get the gradientsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½œä¸ºç¬¬äºŒæ­¥ï¼Œæˆ‘ä»¬è®¾è®¡æˆ–æ„å»ºæŸå¤±å’Œä¼˜åŒ–å™¨ï¼Œæœ€åä¸€æ­¥æ˜¯è¿›è¡Œè®­ç»ƒå¾ªç¯ã€‚è¿™æ˜¯è®­ç»ƒå¾ªç¯ã€‚æˆ‘ä»¬ä»å‰å‘ä¼ æ’­å¼€å§‹ã€‚è¿™é‡Œæˆ‘ä»¬è®¡ç®—ï¼Œæˆ–è€…è®©æˆ‘ä»¬å†™ä¸‹æ¥ã€‚è®¡ç®—é¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œåå‘ä¼ æ’­ï¼Œè·å–æ¢¯åº¦ã€‚
- en: and Pytorch can do everything for usã€‚ We only have to define or to design our
    modelã€‚ Soã€‚ and after we have the gradientsã€‚ We can then update our weightsã€‚So
    now we update our weightsã€‚ and then we iterate this a couple of time until we
    are doneã€‚ And that's the whole pipelineã€‚So now let's continueã€‚ and now let's replace
    the loss and the optimizationã€‚So for thisã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch å¯ä»¥ä¸ºæˆ‘ä»¬åšä¸€åˆ‡ã€‚æˆ‘ä»¬åªéœ€å®šä¹‰æˆ–è®¾è®¡æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç„¶ååœ¨è·å¾—æ¢¯åº¦åï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°æƒé‡ã€‚ç°åœ¨æˆ‘ä»¬æ›´æ–°æƒé‡ï¼Œç„¶åè¿­ä»£å‡ æ¬¡ç›´åˆ°å®Œæˆã€‚è¿™å°±æ˜¯æ•´ä¸ªç®¡é“ã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­ï¼Œæ›¿æ¢æŸå¤±å’Œä¼˜åŒ–ã€‚
- en: we import the neural network moduleï¼Œ so we import torch dot N N S and Nã€‚ so
    we can use some functions from thisã€‚And now we don't want to define the loss manually
    anymore so we can simply delete thisã€‚And nowã€‚Down here before our trainingï¼Œ we
    still need to define our lossã€‚ so we can say loss equalsã€‚ And here we can use
    a loss which is provided from pytorchã€‚ So we can say N N dot M E lossã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œå› æ­¤æˆ‘ä»¬å¯¼å…¥ torch dot N N S å’Œ Nï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨å…¶ä¸­çš„ä¸€äº›å‡½æ•°ã€‚ç°åœ¨æˆ‘ä»¬ä¸å†æƒ³æ‰‹åŠ¨å®šä¹‰æŸå¤±ï¼Œæ‰€ä»¥å¯ä»¥ç®€å•åœ°åˆ é™¤è¿™ä¸€éƒ¨åˆ†ã€‚åœ¨æˆ‘ä»¬è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬ä»éœ€è¦å®šä¹‰æˆ‘ä»¬çš„æŸå¤±ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥è¯´æŸå¤±ç­‰äºã€‚è¿™é‡Œå¯ä»¥ä½¿ç”¨
    PyTorch æä¾›çš„æŸå¤±ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´ N N dot M E lossã€‚
- en: which is exactly what we implement beforeã€‚ So this is the mean squared errorã€‚
    And this is a callable functionã€‚ And then we also want a optr from p chargechã€‚
    So we say optr equals torch dot optim from the optimization moduleï¼Œ And then here
    we use S GDã€‚ which stands for stochastic radiant descentï¼Œ which will need some
    parameterssã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯æˆ‘ä»¬ä¹‹å‰å®ç°çš„ã€‚æ‰€ä»¥è¿™æ˜¯å‡æ–¹è¯¯å·®ã€‚è¿™æ˜¯ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬è¿˜æƒ³ä» p chargech è·å–ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ optr ç­‰äº torch
    dot optim æ¥è‡ªä¼˜åŒ–æ¨¡å—ï¼Œç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ S GDï¼Œä»£è¡¨éšæœºæ¢¯åº¦ä¸‹é™ï¼Œéœ€è¦ä¸€äº›å‚æ•°ã€‚
- en: some parameters that it should optimizeã€‚ and it will need this as a listã€‚ So
    we put our W hereã€‚ And then it also needs the Lrã€‚ So the learning rateï¼Œ which
    isã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›éœ€è¦ä¼˜åŒ–çš„å‚æ•°ï¼Œå¹¶ä¸”å®ƒå°†éœ€è¦ä½œä¸ºåˆ—è¡¨ã€‚å› æ­¤æˆ‘ä»¬æŠŠ W æ”¾åœ¨è¿™é‡Œã€‚ç„¶åå®ƒè¿˜éœ€è¦ Lrï¼Œå³å­¦ä¹ ç‡ï¼Œå…·ä½“æ˜¯ã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_3.png)
- en: Our previously defined learning rateã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_5.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„å­¦ä¹ ç‡ã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_5.png)
- en: And then in our training loopã€‚ So the loss computation is now still the same
    because this is a callable functionã€‚ which gets the actual y and the predicted
    yã€‚And then we don't need to manually update our weights any more so we can simply
    say optr dot stepã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¸­ã€‚æŸå¤±è®¡ç®—ç°åœ¨ä»ç„¶æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°ï¼Œå®ƒè·å–å®é™…çš„ y å’Œé¢„æµ‹çš„ yã€‚ç„¶åæˆ‘ä»¬ä¸å†éœ€è¦æ‰‹åŠ¨æ›´æ–°æˆ‘ä»¬çš„æƒé‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´
    optr.dot.stepã€‚
- en: which will do an optimization stepã€‚ And then we also still have to empty our
    gradients after the optimization stepã€‚ So we can say optimizationr dot0 graã€‚And
    now we are done with step 3ã€‚ So let's run this to see if this is workingã€‚Andã€‚Soï¼Œ
    yeahï¼Œ still workingã€‚ Our prediction is good after the trainingã€‚And let's continue
    with step 4 and replace our manually implemented forward method with the with
    a pytorch modelã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¿›è¡Œä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤ã€‚ç„¶åæˆ‘ä»¬åœ¨ä¼˜åŒ–æ­¥éª¤ä¹‹åä»ç„¶éœ€è¦æ¸…ç©ºæˆ‘ä»¬çš„æ¢¯åº¦ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥è¯´ optimizer.dot0.graã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†æ­¥éª¤ 3ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªçœ‹çœ‹æ˜¯å¦æœ‰æ•ˆã€‚å—¯ï¼Œæ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œä»ç„¶æœ‰æ•ˆã€‚æˆ‘ä»¬çš„é¢„æµ‹åœ¨è®­ç»ƒåæ˜¯ä¸é”™çš„ã€‚è®©æˆ‘ä»¬ç»§ç»­è¿›è¡Œæ­¥éª¤
    4ï¼Œå¹¶ç”¨ pytorch æ¨¡å‹æ›¿æ¢æˆ‘ä»¬æ‰‹åŠ¨å®ç°çš„å‰å‘æ–¹æ³•ã€‚
- en: soã€‚For thisï¼Œ let's we also don't need our weights anymore because then our pie
    touch model knows the parametersã€‚So here we say model equals N N dot linearã€‚ So
    usually we had have to design this for ourselvesã€‚But since this is very trivial
    for linear regressionã€‚ So this is only one layerã€‚ this is already provided in
    pytorchã€‚ So this is N N dot linearã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå¯¹äºè¿™ä¸ªï¼Œæˆ‘ä»¬ä¹Ÿä¸å†éœ€è¦æˆ‘ä»¬çš„æƒé‡ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å·²ç»çŸ¥é“äº†å‚æ•°ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬è¯´ model ç­‰äº N N.dot.linearã€‚é€šå¸¸æˆ‘ä»¬éœ€è¦è‡ªå·±è®¾è®¡è¿™ä¸ªã€‚ä½†ç”±äºè¿™å¯¹äºçº¿æ€§å›å½’æ¥è¯´éå¸¸ç®€å•ã€‚è¿™åªæœ‰ä¸€å±‚ï¼Œè¿™åœ¨
    pytorch ä¸­å·²ç»æä¾›äº†ã€‚æ‰€ä»¥è¿™æ˜¯ N N.dot.linearã€‚
- en: and this needs an input size and an output size of our featuresã€‚And for thisã€‚
    we need to do some modificationã€‚ So now ourã€‚X and y need to have a different shapeã€‚
    So this must be a 2 D array Nowï¼Œ where the number of rows is the number of samplesã€‚
    And for each rowï¼Œ we have the number of or the the featuresã€‚ So this has a new
    shapeã€‚Sorryã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éœ€è¦æˆ‘ä»¬ç‰¹å¾çš„è¾“å…¥å¤§å°å’Œè¾“å‡ºå¤§å°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åšä¸€äº›ä¿®æ”¹ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬çš„ X å’Œ y éœ€è¦æœ‰ä¸åŒçš„å½¢çŠ¶ã€‚å› æ­¤è¿™å¿…é¡»æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œç°åœ¨è¡Œæ•°æ˜¯æ ·æœ¬çš„æ•°é‡ã€‚æ¯è¡Œå¯¹åº”ç‰¹å¾æ•°é‡ã€‚å› æ­¤è¿™ä¸ªæœ‰ä¸€ä¸ªæ–°å½¢çŠ¶ã€‚æŠ±æ­‰ã€‚
- en: In new shapeã€‚That looks like thisã€‚And the same for our yã€‚ So our y is the same
    shape nowï¼Œ So2ã€‚4ï¼Œ6ã€‚And 8ã€‚ So now let's get the shapeã€‚ So this is yã€‚ I have to
    be careful nowã€‚ So we can say number of samples and number of featuresã€‚Equals
    x dot shapeã€‚ And now let's print thisã€‚ So print the number of samples and the
    number of featuresã€‚ And now let's run thisã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ–°å½¢çŠ¶çœ‹èµ·æ¥åƒè¿™æ ·ã€‚y çš„å½¢çŠ¶ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥æ˜¯ 2ã€4ã€6 å’Œ 8ã€‚æ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬è·å–å½¢çŠ¶ã€‚æ‰€ä»¥æ˜¯ yã€‚æˆ‘ç°åœ¨å¾—å°å¿ƒã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´æ ·æœ¬æ•°é‡å’Œç‰¹å¾æ•°é‡ç­‰äº
    x.dot.shapeã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°è¿™ä¸ªã€‚æ‰€ä»¥æ‰“å°æ ·æœ¬æ•°é‡å’Œç‰¹å¾æ•°é‡ã€‚ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚
- en: So this will run into an errorã€‚ But I think we get until hereã€‚ So the shape
    is now 4 by oneã€‚ So we have four samples and one feature for each sampleã€‚ And
    now we define our modelã€‚ So this needs an input and an output sizeã€‚ So the inputã€‚Input
    size equals the number of featuresã€‚ and the output sizeï¼Œ output size is still
    the sameã€‚ So this is also the number of featuresã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šå¯¼è‡´ä¸€ä¸ªé”™è¯¯ã€‚ä½†æˆ‘æƒ³æˆ‘ä»¬å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥å½¢çŠ¶ç°åœ¨æ˜¯ 4Ã—1ã€‚æˆ‘ä»¬æœ‰å››ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªç‰¹å¾ã€‚ç°åœ¨æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚è¿™éœ€è¦ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡ºå¤§å°ã€‚è¾“å…¥å¤§å°ç­‰äºç‰¹å¾çš„æ•°é‡ï¼Œè¾“å‡ºå¤§å°ä»ç„¶æ˜¯ä¸€æ ·çš„ã€‚æ‰€ä»¥è¿™ä¹Ÿæ˜¯ç‰¹å¾çš„æ•°é‡ã€‚
- en: So this is one as an input size and one as an output sizeã€‚ Now we need to give
    this to our modelã€‚ So we say here input sizeã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_7.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªè¾“å…¥å¤§å°å’Œä¸€ä¸ªè¾“å‡ºå¤§å°ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦æŠŠè¿™ä¸ªç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œè¯´è¾“å…¥å¤§å°ã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_7.png)
- en: And output sizeã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_9.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºå¤§å°ã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_9.png)
- en: And then one moreï¼Œ then when weã€‚Want to get the predictionã€‚ We can simply say
    we can call the modelã€‚ but now this cannot have a float valueã€‚ So this must be
    a tenzorã€‚ So let's create a test tenzorã€‚ Let's say Xã€‚Testã€‚Equals torch dot Tenorï¼Œ
    which gets only one sample with5ã€‚ And then it gets a data type ofï¼Œ sayï¼Œ torch
    dot float 32ã€‚And then here we pass the test sampleã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå†ä¸€æ¬¡ï¼Œå½“æˆ‘ä»¬æƒ³è¦è·å–é¢„æµ‹æ—¶ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´æˆ‘ä»¬å¯ä»¥è°ƒç”¨æ¨¡å‹ã€‚ä½†ç°åœ¨è¿™ä¸èƒ½æ˜¯ä¸€ä¸ªæµ®ç‚¹å€¼ã€‚å› æ­¤è¿™å¿…é¡»æ˜¯ä¸€ä¸ªå¼ é‡ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæµ‹è¯•å¼ é‡ã€‚å‡è®¾ X.Test
    ç­‰äº torch.dot.Tensorï¼Œå®ƒåªè·å–ä¸€ä¸ªæ ·æœ¬ï¼Œå€¼ä¸º 5ã€‚ç„¶åå®ƒçš„æ•°æ®ç±»å‹ä¸º torch.dot.float32ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬ä¼ é€’æµ‹è¯•æ ·æœ¬ã€‚
- en: And since this is only one has only one valueï¼Œ we can call the dot item to get
    the actual float value thenã€‚ So now let's copy and paste this down hereã€‚å—¯ã€‚And
    now we also have to modify ourã€‚Optimizer hereã€‚ So we don't have our weights nowã€‚
    So this lists with the parameters hereã€‚ we can simply say model dotã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ä¸ªåªæœ‰ä¸€ä¸ªå€¼ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ç‚¹é¡¹ç›®æ¥è·å–å®é™…çš„æµ®ç‚¹å€¼ã€‚æ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬æŠŠè¿™æ®µå¤åˆ¶ç²˜è´´åˆ°è¿™é‡Œã€‚å—¯ã€‚ç°åœ¨æˆ‘ä»¬è¿˜éœ€è¦ä¿®æ”¹æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬æ²¡æœ‰æƒé‡ã€‚æ‰€ä»¥è¿™ä¸ªåˆ—è¡¨ä¸­åŒ…å«å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´
    model.dotã€‚
- en: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_11.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_11.png)'
- en: Terrameterï¼Œ and call this functionã€‚And now hereï¼Œ forï¼Œ for the predictionï¼Œ we
    alsoã€‚ we simply call the modelã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_13.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Terrameterï¼Œå¹¶è°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚ç°åœ¨åœ¨è¿™é‡Œï¼Œå¯¹äºé¢„æµ‹ï¼Œæˆ‘ä»¬ä¹Ÿç®€å•åœ°è°ƒç”¨æ¨¡å‹ã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_13.png)
- en: And now we are doneã€‚ So now we are usingã€‚The pieytor model to get thisã€‚ And
    also down hereã€‚ Nowã€‚ if we want to print them againï¼Œ we have to unpack themã€‚ So
    let's sayã€‚W and an optional bias equals model parametersã€‚This will unpack themã€‚
    And then if we want to print the actualï¼Œ this will be a list of listsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨pieytoræ¨¡å‹æ¥è·å¾—è¿™ä¸ªã€‚æ­¤å¤–åœ¨è¿™é‡Œï¼Œå¦‚æœæˆ‘ä»¬æƒ³å†æ¬¡æ‰“å°å®ƒä»¬ï¼Œæˆ‘ä»¬å¿…é¡»è§£åŒ…å®ƒä»¬ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´Wå’Œä¸€ä¸ªå¯é€‰çš„åç½®ç­‰äºæ¨¡å‹å‚æ•°ã€‚è¿™å°†è§£åŒ…å®ƒä»¬ã€‚å¦‚æœæˆ‘ä»¬æƒ³æ‰“å°å®é™…çš„ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ã€‚
- en: So let's get the first or the actual first weight with this and we can also
    call the item because we don't want to see the tenorã€‚And now I think we are doneã€‚
    So let's run this to see if this is workingã€‚ And yeahï¼Œ soã€‚The final output is
    not perfectã€‚ So this might be because the initialization now is randomlyã€‚ And
    alsoï¼Œ this optimizer technique might be a little differentã€‚ So you might want
    to play aroundã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬ç”¨è¿™ä¸ªè·å–ç¬¬ä¸€ä¸ªæˆ–å®é™…çš„ç¬¬ä¸€ä¸ªæƒé‡ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è°ƒç”¨è¿™ä¸ªé¡¹ç›®ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æƒ³çœ‹åˆ°å¼ é‡ã€‚ç°åœ¨æˆ‘è§‰å¾—æˆ‘ä»¬å®Œæˆäº†ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æœ‰æ•ˆã€‚æ˜¯çš„ï¼Œæœ€ç»ˆè¾“å‡ºå¹¶ä¸å®Œç¾ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºåˆå§‹åŒ–ç°åœ¨æ˜¯éšæœºçš„ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªä¼˜åŒ–æŠ€æœ¯å¯èƒ½æœ‰ç‚¹ä¸åŒã€‚æ‰€ä»¥ä½ å¯èƒ½æƒ³è¦å°è¯•ä¸€ä¸‹ã€‚
- en: play around with the learning rate and the number of iterationsã€‚ but basicallyï¼Œ
    it worksã€‚ and it gets better and betterã€‚With every stepã€‚And yeahã€‚ so this is how
    we can construct the whole training pipelineã€‚ And one more thingã€‚ So in this caseã€‚
    we didn't have to upï¼Œ have to come up with the model for ourselvesã€‚ So here we
    only had one layerã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä¸€ä¸‹å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ã€‚ä½†åŸºæœ¬ä¸Šï¼Œå®ƒæœ‰æ•ˆã€‚éšç€æ¯ä¸€æ­¥ï¼Œå®ƒå˜å¾—è¶Šæ¥è¶Šå¥½ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•æ„å»ºæ•´ä¸ªè®­ç»ƒç®¡é“ã€‚è¿˜æœ‰ä¸€ä»¶äº‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸å¿…è‡ªå·±æå‡ºæ¨¡å‹ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬åªæœ‰ä¸€å±‚ã€‚
- en: And this was already provided in Pytorchã€‚ But let's say we need a custom modelã€‚
    So let's write a custom linear regression modelã€‚Then we have to derive this from
    Nï¼Œ N dot moduleã€‚ and this will get a in itã€‚Methtã€‚Which has selfï¼Œ and which gets
    theã€‚Inputã€‚Dimmensionsã€‚And the output dimensionsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å·²ç»åœ¨Pytorchä¸­æä¾›äº†ã€‚ä½†å‡è®¾æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹ã€‚è®©æˆ‘ä»¬å†™ä¸€ä¸ªè‡ªå®šä¹‰çš„çº¿æ€§å›å½’æ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬å¿…é¡»ä»Nï¼ŒN.dot.moduleæ´¾ç”Ÿã€‚è¿™å°†å¾—åˆ°ä¸€ä¸ªåˆå§‹åŒ–çš„æ–¹æ³•ã€‚å®ƒæœ‰selfï¼Œå¹¶è·å–è¾“å…¥ç»´åº¦å’Œè¾“å‡ºç»´åº¦ã€‚
- en: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_15.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_15.png)'
- en: And then here we call superï¼Œ the super classï¼Œ So super of linear regression
    with selfã€‚ and then dotã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_17.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬è°ƒç”¨superï¼Œè¶…ç±»ã€‚æ‰€ä»¥çº¿æ€§å›å½’çš„superå’Œselfã€‚ç„¶åç‚¹ã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_17.png)
- en: In itï¼Œ this is how we call the superconorã€‚ And here we would define our layersã€‚So
    in this caseã€‚ we say our self dot line or linear layer equals Nn dot linearã€‚ and
    this will get the input dimension and the output dimension and then we store them
    hereã€‚ and then we also have to implement the forward pass in our model class so
    self and xã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•è°ƒç”¨è¶…ç±»ã€‚æˆ‘ä»¬ä¼šå®šä¹‰æˆ‘ä»¬çš„å±‚ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯´æˆ‘ä»¬çš„self.dotçº¿æ€§å±‚ç­‰äºNn.dot.linearã€‚è¿™å°†è·å–è¾“å…¥ç»´åº¦å’Œè¾“å‡ºç»´åº¦ï¼Œç„¶åæˆ‘ä»¬å°†å®ƒä»¬å­˜å‚¨åœ¨è¿™é‡Œã€‚ç„¶åæˆ‘ä»¬è¿˜å¿…é¡»åœ¨æˆ‘ä»¬çš„æ¨¡å‹ç±»ä¸­å®ç°å‰å‘ä¼ é€’ï¼Œæ‰€ä»¥selfå’Œxã€‚
- en: and here we can simply return self dot linear of xã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_19.png)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿”å›self.dot.linearçš„xã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_19.png)
- en: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_20.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_20.png)'
- en: And this is the whole thingã€‚ And now we can say our model equals linear regression
    with the input size and the output sizeã€‚ And now this will do the same thingã€‚
    So now this is just a dummy exampleã€‚ because this is a simple wrapper that will
    do exactly the sameã€‚ But basicallyã€‚ this is how we design our pieyto modelã€‚ So
    now let's comment this out and use this class to see if this is workingã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æ•´ä¸ªå†…å®¹ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬çš„æ¨¡å‹ç­‰åŒäºçº¿æ€§å›å½’ï¼Œè¾“å…¥å¤§å°å’Œè¾“å‡ºå¤§å°ã€‚ç°åœ¨è¿™å°†åšåŒæ ·çš„äº‹æƒ…ã€‚æ‰€ä»¥ç°åœ¨è¿™åªæ˜¯ä¸€ä¸ªè™šæ‹Ÿç¤ºä¾‹ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç®€å•çš„åŒ…è£…å™¨ï¼Œå®ƒå°†å®Œå…¨ç›¸åŒã€‚ä½†åŸºæœ¬ä¸Šï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è®¾è®¡pieytoæ¨¡å‹çš„æ–¹å¼ã€‚æ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬æ³¨é‡Šæ‰è¿™ä¸ªï¼Œä½¿ç”¨è¿™ä¸ªç±»æ¥çœ‹çœ‹å®ƒæ˜¯å¦æœ‰æ•ˆã€‚
- en: Andã€‚Yeahï¼Œ so it's still workingã€‚ So that's all for nowã€‚ And nowï¼Œ Pyto can do
    most of the work for usã€‚ Of courseï¼Œ we still have to design our model and have
    to know which loss and optimr we want to useã€‚ but we don't have to worry about
    the underlying algorithms anymoreï¼Œ soã€‚Yeahã€‚ you can find all the code on Giubã€‚
    And if you like thisã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥å®ƒä»ç„¶æœ‰æ•ˆã€‚ç°åœ¨å°±è¿™äº›ã€‚ç°åœ¨ï¼ŒPytoå¯ä»¥ä¸ºæˆ‘ä»¬å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦è®¾è®¡æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶ä¸”éœ€è¦çŸ¥é“æˆ‘ä»¬æƒ³è¦ä½¿ç”¨å“ªä¸ªæŸå¤±å’Œä¼˜åŒ–å™¨ï¼Œä½†æˆ‘ä»¬ä¸å¿…å†æ‹…å¿ƒåº•å±‚ç®—æ³•äº†ã€‚æ‰€ä»¥ï¼Œä½ å¯ä»¥åœ¨Giubä¸Šæ‰¾åˆ°æ‰€æœ‰ä»£ç ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªã€‚
- en: please subscribe to the channel and see you next timeï¼Œ byeã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_22.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®¢é˜…é¢‘é“ï¼Œä¸‹ä¸€æ¬¡è§ï¼Œæ‹œæ‹œã€‚![](img/2b520fac31b9d1c4d2989f76ec98bd5f_22.png)
