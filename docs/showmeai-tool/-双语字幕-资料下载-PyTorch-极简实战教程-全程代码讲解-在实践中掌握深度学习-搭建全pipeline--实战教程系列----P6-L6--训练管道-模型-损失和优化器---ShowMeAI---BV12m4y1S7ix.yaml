- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P6ÔºöL6- ËÆ≠ÁªÉÁÆ°ÈÅìÔºöÊ®°Âûã„ÄÅÊçüÂ§±Âíå‰ºòÂåñÂô®
    - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome back to your new Pytorch tutorial„ÄÇ In the last tutorial„ÄÇ
    we implemented logistic regression from scratch and then learned how we can use
    Pyt to calculate the gradients for us with Beck propagation„ÄÇ NowÔºå we will continue
    where we left off„ÄÇ And now we are going to replace the manually computed loss
    and parameter updates by using the loss and optimizer classes in Pytorch„ÄÇüòäÔºåAnd
    then we also replaced the manually computed model prediction by implementing a
    pytorch model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then Pytorch can do the complete pipeline for us„ÄÇ So this video covers steps
    3 and 4„ÄÇAnd please watch the previous tutorial first to see the steps 1 and 2„ÄÇ
    So now let's start„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: And firstÔºå I want to talk about the general training pipeline in Pytorch„ÄÇ So
    typically„ÄÇ we have three steps„ÄÇ So the first step is to design our model„ÄÇSo we
    design the number of inputs and outputs„ÄÇ So input size and output size„ÄÇAnd then
    also„ÄÇ we designed the forward pass with all the different operations or all the
    different layers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then as a second stepÔºå we design or we come up with the so we construct the
    loss and the optimizer„ÄÇ and then as a last stepÔºå we do our training loop„ÄÇSo this
    is the training loop„ÄÇ So we start by doing our forward pass„ÄÇ So here we compute„ÄÇOr
    let's write this down„ÄÇ Compute the prediction„ÄÇThen we do the backward passÔºå backward
    pass„ÄÇ So we get the gradients„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and Pytorch can do everything for us„ÄÇ We only have to define or to design our
    model„ÄÇ So„ÄÇ and after we have the gradients„ÄÇ We can then update our weights„ÄÇSo
    now we update our weights„ÄÇ and then we iterate this a couple of time until we
    are done„ÄÇ And that's the whole pipeline„ÄÇSo now let's continue„ÄÇ and now let's replace
    the loss and the optimization„ÄÇSo for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we import the neural network moduleÔºå so we import torch dot N N S and N„ÄÇ so
    we can use some functions from this„ÄÇAnd now we don't want to define the loss manually
    anymore so we can simply delete this„ÄÇAnd now„ÄÇDown here before our trainingÔºå we
    still need to define our loss„ÄÇ so we can say loss equals„ÄÇ And here we can use
    a loss which is provided from pytorch„ÄÇ So we can say N N dot M E loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is exactly what we implement before„ÄÇ So this is the mean squared error„ÄÇ
    And this is a callable function„ÄÇ And then we also want a optr from p chargech„ÄÇ
    So we say optr equals torch dot optim from the optimization moduleÔºå And then here
    we use S GD„ÄÇ which stands for stochastic radiant descentÔºå which will need some
    parameterss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: some parameters that it should optimize„ÄÇ and it will need this as a list„ÄÇ So
    we put our W here„ÄÇ And then it also needs the Lr„ÄÇ So the learning rateÔºå which
    is„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Our previously defined learning rate„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: And then in our training loop„ÄÇ So the loss computation is now still the same
    because this is a callable function„ÄÇ which gets the actual y and the predicted
    y„ÄÇAnd then we don't need to manually update our weights any more so we can simply
    say optr dot step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which will do an optimization step„ÄÇ And then we also still have to empty our
    gradients after the optimization step„ÄÇ So we can say optimizationr dot0 gra„ÄÇAnd
    now we are done with step 3„ÄÇ So let's run this to see if this is working„ÄÇAnd„ÄÇSoÔºå
    yeahÔºå still working„ÄÇ Our prediction is good after the training„ÄÇAnd let's continue
    with step 4 and replace our manually implemented forward method with the with
    a pytorch model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so„ÄÇFor thisÔºå let's we also don't need our weights anymore because then our pie
    touch model knows the parameters„ÄÇSo here we say model equals N N dot linear„ÄÇ So
    usually we had have to design this for ourselves„ÄÇBut since this is very trivial
    for linear regression„ÄÇ So this is only one layer„ÄÇ this is already provided in
    pytorch„ÄÇ So this is N N dot linear„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this needs an input size and an output size of our features„ÄÇAnd for this„ÄÇ
    we need to do some modification„ÄÇ So now our„ÄÇX and y need to have a different shape„ÄÇ
    So this must be a 2 D array NowÔºå where the number of rows is the number of samples„ÄÇ
    And for each rowÔºå we have the number of or the the features„ÄÇ So this has a new
    shape„ÄÇSorry„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In new shape„ÄÇThat looks like this„ÄÇAnd the same for our y„ÄÇ So our y is the same
    shape nowÔºå So2„ÄÇ4Ôºå6„ÄÇAnd 8„ÄÇ So now let's get the shape„ÄÇ So this is y„ÄÇ I have to
    be careful now„ÄÇ So we can say number of samples and number of features„ÄÇEquals
    x dot shape„ÄÇ And now let's print this„ÄÇ So print the number of samples and the
    number of features„ÄÇ And now let's run this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this will run into an error„ÄÇ But I think we get until here„ÄÇ So the shape
    is now 4 by one„ÄÇ So we have four samples and one feature for each sample„ÄÇ And
    now we define our model„ÄÇ So this needs an input and an output size„ÄÇ So the input„ÄÇInput
    size equals the number of features„ÄÇ and the output sizeÔºå output size is still
    the same„ÄÇ So this is also the number of features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is one as an input size and one as an output size„ÄÇ Now we need to give
    this to our model„ÄÇ So we say here input size„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: And output size„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: And then one moreÔºå then when we„ÄÇWant to get the prediction„ÄÇ We can simply say
    we can call the model„ÄÇ but now this cannot have a float value„ÄÇ So this must be
    a tenzor„ÄÇ So let's create a test tenzor„ÄÇ Let's say X„ÄÇTest„ÄÇEquals torch dot TenorÔºå
    which gets only one sample with5„ÄÇ And then it gets a data type ofÔºå sayÔºå torch
    dot float 32„ÄÇAnd then here we pass the test sample„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And since this is only one has only one valueÔºå we can call the dot item to get
    the actual float value then„ÄÇ So now let's copy and paste this down here„ÄÇÂóØ„ÄÇAnd
    now we also have to modify our„ÄÇOptimizer here„ÄÇ So we don't have our weights now„ÄÇ
    So this lists with the parameters here„ÄÇ we can simply say model dot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_11.png)'
  prefs: []
  type: TYPE_IMG
- en: TerrameterÔºå and call this function„ÄÇAnd now hereÔºå forÔºå for the predictionÔºå we
    also„ÄÇ we simply call the model„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now we are done„ÄÇ So now we are using„ÄÇThe pieytor model to get this„ÄÇ And
    also down here„ÄÇ Now„ÄÇ if we want to print them againÔºå we have to unpack them„ÄÇ So
    let's say„ÄÇW and an optional bias equals model parameters„ÄÇThis will unpack them„ÄÇ
    And then if we want to print the actualÔºå this will be a list of lists„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's get the first or the actual first weight with this and we can also
    call the item because we don't want to see the tenor„ÄÇAnd now I think we are done„ÄÇ
    So let's run this to see if this is working„ÄÇ And yeahÔºå so„ÄÇThe final output is
    not perfect„ÄÇ So this might be because the initialization now is randomly„ÄÇ And
    alsoÔºå this optimizer technique might be a little different„ÄÇ So you might want
    to play around„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: play around with the learning rate and the number of iterations„ÄÇ but basicallyÔºå
    it works„ÄÇ and it gets better and better„ÄÇWith every step„ÄÇAnd yeah„ÄÇ so this is how
    we can construct the whole training pipeline„ÄÇ And one more thing„ÄÇ So in this case„ÄÇ
    we didn't have to upÔºå have to come up with the model for ourselves„ÄÇ So here we
    only had one layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this was already provided in Pytorch„ÄÇ But let's say we need a custom model„ÄÇ
    So let's write a custom linear regression model„ÄÇThen we have to derive this from
    NÔºå N dot module„ÄÇ and this will get a in it„ÄÇMetht„ÄÇWhich has selfÔºå and which gets
    the„ÄÇInput„ÄÇDimmensions„ÄÇAnd the output dimensions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_15.png)'
  prefs: []
  type: TYPE_IMG
- en: And then here we call superÔºå the super classÔºå So super of linear regression
    with self„ÄÇ and then dot„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_17.png)
  prefs: []
  type: TYPE_NORMAL
- en: In itÔºå this is how we call the superconor„ÄÇ And here we would define our layers„ÄÇSo
    in this case„ÄÇ we say our self dot line or linear layer equals Nn dot linear„ÄÇ and
    this will get the input dimension and the output dimension and then we store them
    here„ÄÇ and then we also have to implement the forward pass in our model class so
    self and x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and here we can simply return self dot linear of x„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b520fac31b9d1c4d2989f76ec98bd5f_20.png)'
  prefs: []
  type: TYPE_IMG
- en: And this is the whole thing„ÄÇ And now we can say our model equals linear regression
    with the input size and the output size„ÄÇ And now this will do the same thing„ÄÇ
    So now this is just a dummy example„ÄÇ because this is a simple wrapper that will
    do exactly the same„ÄÇ But basically„ÄÇ this is how we design our pieyto model„ÄÇ So
    now let's comment this out and use this class to see if this is working„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And„ÄÇYeahÔºå so it's still working„ÄÇ So that's all for now„ÄÇ And nowÔºå Pyto can do
    most of the work for us„ÄÇ Of courseÔºå we still have to design our model and have
    to know which loss and optimr we want to use„ÄÇ but we don't have to worry about
    the underlying algorithms anymoreÔºå so„ÄÇYeah„ÄÇ you can find all the code on Giub„ÄÇ
    And if you like this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: please subscribe to the channel and see you next timeÔºå bye„ÄÇ![](img/2b520fac31b9d1c4d2989f76ec98bd5f_22.png)
  prefs: []
  type: TYPE_NORMAL
