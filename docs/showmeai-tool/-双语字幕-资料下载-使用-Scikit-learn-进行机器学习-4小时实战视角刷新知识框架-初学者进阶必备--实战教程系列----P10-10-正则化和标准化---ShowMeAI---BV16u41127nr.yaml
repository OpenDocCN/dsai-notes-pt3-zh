- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„Äë‰ΩøÁî® Scikit-learn ËøõË°åÊú∫Âô®Â≠¶‰π†Ôºå4Â∞èÊó∂ÂÆûÊàòËßÜËßíÂà∑Êñ∞Áü•ËØÜÊ°ÜÊû∂ÔºåÂàùÂ≠¶ËÄÖËøõÈò∂ÂøÖÂ§áÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P10Ôºö10ÔºâÊ≠£ÂàôÂåñÂíåÊ†áÂáÜÂåñ
    - ShowMeAI - BV16u41127nr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/ba488996234f1aee5936aea9ea40f4bf_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Well in this video I'm may be talking about two topics„ÄÇ one is regularization
    and other is standardization„ÄÇ standardization is something that we have to use
    a lot in this course and understand„ÄÇ and it's really relatively simpleÔºå but regularization
    is a very complicated topic and might require a lot of time and a more advanced
    machine learning course„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm not trying to get into any math regarding regularization„ÄÇ and I'm just trying
    to try to give the kind of simplest intuition and we aren't going to get deep
    into it„ÄÇ but just wanting to know that that's an important and deeper topic„ÄÇSo
    in terms of things that we've already done„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba488996234f1aee5936aea9ea40f4bf_2.png)'
  prefs: []
  type: TYPE_IMG
- en: We've been using logistic regression a lot and a problem that it has that I
    haven't talked about is that it's very sensitive to scaling„ÄÇAnd so for exampleÔºå
    you might have a data set and there might be some numbers in it with specific
    units„ÄÇAnd you might get one result of due do the classification if you change
    those unitsÔºå so for example„ÄÇ maybe you change miles to feetÔºå you might get a different
    result„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is of course not what we want we just care about the actual kind of information„ÄÇ
    not what units somebody arbitrarily chose to use„ÄÇWhy is that well it's because
    logistic regression is applying this technique to regularizationÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Which tries to use smaller coefficientes and in general„ÄÇ not have a very large
    coefficient on just one of our features„ÄÇYou can imagine that I have lots and lots
    of feature columns„ÄÇThat just by chance„ÄÇ maybe one of them does better on the training
    data than others„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Even if the other feature columns are kind of still somewhat useful„ÄÇ and so
    what I wouldn't want to do is just by chance choose that best one because then
    it won't work well later on a test data set„ÄÇ regularization basically is providing
    a motivation to use multiple features and not consider one too heavily„ÄÇ even if
    that would do better in the short term„ÄÇSo logistic regression does this linear
    regression which was the first model we learned in this course does not„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but there are also things very similar to linear regression that do such as
    ridge regression and lasal regression„ÄÇ we're not trying to talk about those at
    320 but they're important and use all the time and so know that this regularization
    thing is a big deal„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So what would we really likeÔºå we don't want our model to be sensitive to units„ÄÇ
    We would like to standardize it in some way so that we have the same numbers going
    in„ÄÇ regardless of what their original units were„ÄÇSo for this example I just made
    up kind of a fake scenario„ÄÇ we're measuring some sort of quantity in the real
    world three times and based on we're trying to predict what sort of category it's
    in the category will either be true or false„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The underlying rule is that when the true lengthÔºå which we don't know„ÄÇ is bigger
    than 5 than the category is true„ÄÇ It's less than 5„ÄÇ The category is false„ÄÇ And
    so these three kind of noisy measurementsÔºå even though they know tell us exactly
    what the true length is„ÄÇ they give us some information about that they can help
    us guess whether it's true or false„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here I have that dataÔºå that fake data I'm talking aboutÔºå the y column here
    is the category„ÄÇ and then I have my three measurementsÔºå x1 x to an x3„ÄÇLet me just
    talk a little bit about how I'm generating this„ÄÇSo under anumpot random„ÄÇ there
    are a bunch of functions that will generate random dataÔºå I'm doing a normal distribution
    here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can sample from different distributionsÔºå you don't know what that means
    that's fine for this course„ÄÇ but basically I'm generating1000 random numbers with
    an average of four and putting them in here„ÄÇAnd so this will be an array of numbers„ÄÇ
    and then I'm sayingÔºå wellÔºå whatever that's five„ÄÇ greater than fiveÔºå I want to
    have a trueÔºå otherwise I'll have a false„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: When I'm looking at this data frame down hereÔºå truefe does not directly go then
    into any of these columns it's unknown„ÄÇ but category does and categories what
    we're trying to predict„ÄÇSo how are we trying to predict if we don't know true
    feetÔºå why have these three other columns„ÄÇ which are basically just true feet plus
    some random noise„ÄÇ So if I look at it down here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let me look at this first one„ÄÇ All three measurements were less than5„ÄÇ So it
    makes a lot of sense that I'll predict that the y is less than than5„ÄÇ Maybe I
    can even look at some more cases here„ÄÇ I wonder if„ÄÇI can see where it's true„ÄÇLet
    me do that So I can see some other cases here where it's true rightÔºå all in this
    case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: all the measurements were greater than5„ÄÇ So I say it's true„ÄÇ This is kind of
    a more interesting example„ÄÇ This number is very large„ÄÇ right„ÄÇ One measurement
    was like almost of7„ÄÇAnd even though the other two measurements were less than
    5„ÄÇ this was enough of a signal that the model decide it's true„ÄÇ WellÔºå it's true
    overall„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and hopefully the model will decide the same„ÄÇ OkayÔºå so that's the data we're
    working with„ÄÇ And let's see if we can„ÄÇTrain a model to try to predict this„ÄÇ so
    I'm going to create a model and I'm going to use a linear or logistic regression
    model„ÄÇAnd I'm going to fit it„ÄÇTo my dataÔºå and so I may have some x and and Y's
    for my Y's„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm just going to pull out y column for my training data and for my X„ÄÇI want
    to pass in a list of all my columns that contain features so x1 and x2 and x3„ÄÇ
    and I'm going be using these againÔºå so I'm actually try to put this in a variable
    called x columns„ÄÇAnd then I don't have to keep typing that whole long thing every
    time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so I fit it and that's straight and so pretty soon I may look at the coefficients
    for this model„ÄÇ but before that I just want to as an asideÔºå see what accuracy
    it has if I want to see the accuracy of the model„ÄÇ I can just say instead of fit
    I want to score and then to be realistic I shouldn't score it on data that I haven't
    seen before instead of the thing I trained it on right so this is kind of a better
    test and I see that it has 89% accuracy is that good 89% seems high that we would
    be right that often but let me show you why it's not necessarily„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If I look at this Y columnÔºå I see it's almost always false and indeed if I say
    value counts„ÄÇ I could see it's only true less than 20% of the time„ÄÇ and I can
    actually just divide this by the length of tests to see that„ÄÇ And so what this
    means is that even if I had a very naive modelÔºå it just always says false„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I would be getting 81% accuracyÔºå 89% is betterÔºå but in that context it's not
    so amazing just given that there's so much skew in that column„ÄÇOkayÔºå so after
    I look at the accuracy of a modelÔºå the next thing I'll often want to look at are
    my coefficients„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I like to plot those in some wayÔºå ohÔºå my model coefficients„ÄÇAnd I see those
    are right here and and I'd like have some sort of bar plots„ÄÇ I know that these
    things are paired up with these x columns right so this is the coefficient for
    x1 so on and so forth„ÄÇ And so the way I'll often make such a bar plot is I'll
    say PDd do series„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then I will have my Y values„ÄÇAnd then I'll have index equals x values„ÄÇDot
    plotÔºå dot bar„ÄÇSo on the x valuesÔºå I'm going to use those column namesÔºå and I'm
    going to put the coefficients„ÄÇTo basically have the quantities that are going
    to go to the Y axis„ÄÇAnd it's complaining that the length of one of these things
    is one when it was supposed to be three„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and so x columnsÔºå that's pretty simple thereÔºå but if I look at this„ÄÇThis array
    right here what do I see I see there's it's really a twodiional thing I can flatten
    it into a one dimensional array with three numbers or if I say negative one„ÄÇ it'll
    make it one dimensional it'll make it you know it doesn't matter how many numbers
    I have and I'll figure it out so I may put that here now and now I get this plot
    and this is interesting right I was talking about how maybe sometimes just by
    chance we focus more in one problem than another and that happens here right x1
    x2 and x3 we're all equally noisy but it just so happens that based on the training
    data the model thinks that x1 is more kind of more useful right that was just
    by chance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so you could imagine a worse scenario where it picks one column that I really
    likes and ignores all these other ones that have good information in it„ÄÇ and so
    the model will try to avoid doing that„ÄÇ regularization means that we'll try not
    to put too much weight on just one factor we'll try to spread it out a little
    bit and then if you took it to an extreme„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you might imagine that I could have a model where I just look at my intercept„ÄÇMy
    intercept is you can imagine that being like the average and the model could just
    predict animal all my coefficients could be zero in that case„ÄÇ well we all just
    predict the same thingÔºå and we want to have this problem I guess we have another
    problem that just want to be very accurate„ÄÇOkayÔºå so I have that„ÄÇAnd so let me
    head back here and I'm going randomly generate this data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this time I'm just going to change the units on this column and I'm going
    to change the units to be miles and so there's 5„ÄÇ280 feet in a mile so I'm just
    to make a comment here this is feet to miles like that and so I have the same
    kind of data just different units„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so I might hope that my model won't do anything that differently„ÄÇAnd so I'm
    going to run this again„ÄÇ and I see that not too much has changed here„ÄÇ And then
    I want to think about what's going to happen when I rerun this„ÄÇSo in this x2 column„ÄÇThe
    numbers are all much smaller now because it's in miles„ÄÇ and so I might expect
    that to use thisÔºå I might have a bigger coefficient on x2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If I wanted to be just like before„ÄÇIt turns out when I run itÔºå I actually see
    the opposite„ÄÇ It's adverse to having such large coefficients on one column because
    of that regularization thing I talked about„ÄÇ So it actually decidesÔºå heyÔºå I'm
    just going to ignore x2 entirely„ÄÇ I have to put a bigger or a bigger weight on
    that that I'm comfortable doing to have it be a factor„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I just lost some information there„ÄÇ I' using these two columns anymore„ÄÇ NowÔºå
    of course„ÄÇ that's silly rightÔºå putting a bigger coefficient on it isn't really
    waiting it more„ÄÇ It's just canceling out the fact that I have different units
    on it„ÄÇ And so there's different ways to solve this„ÄÇ One is that I could just insist
    that I have the same units for everything„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Another way I could do it is I could try to kind of make this a little more
    uniform in some way„ÄÇ And so that's what I'm going to do here„ÄÇüòäÔºåI'm going head
    back hereÔºå and let me„ÄÇLet me take my training data and my actuallyÔºå where do I
    want to do thisÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I take these x columns even soonerÔºå ActuallyÔºå noÔºå that's fine„ÄÇ I'm going to
    leave that there„ÄÇ so I'm going take my training data and I want to take a slice
    of it and I want to get all the rows and I want to get columns x1„ÄÇThrough R x2„ÄÇAnd
    so this is just my features now well through x3Ôºå sorryÔºå these are just my features„ÄÇAnd
    I want to somehow standardize it so that they all have roughly the same scale„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so I'm to do is I'm just going pull this out into this x variable right
    here„ÄÇ and there's going be two things I'm going to do„ÄÇ one is that I'm going take
    the mean of each of each column just like that„ÄÇ and I'm going subtract these numbers
    off of each column I'm going say that„ÄÇ And so now all of these columns are centered
    at  zero right after I subtracted away the mean the average of every column is0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it turns out that that is also helpful for logistic regression to run faster„ÄÇ
    I'm not trying to get any details about why that's useful And then more importantly
    I want them to be on the same scale„ÄÇ and so oopsÔºå what happen there„ÄÇI jump onto
    a new column or to a new cell and so if I look at this„ÄÇ that's a standard deviation
    of each column and if I have larger numbers while the standard deviation shall
    be higher and so standardization the real key part is that I'm dividing all of
    this by that standard deviation and if I do that I may get a bunch of small numbers
    that have roughly the same scale so after I've done this all of them will have
    the same average0 and then the same standard deviation of one and so this would
    be a better x data and I can actually put this back in to my training data like
    this so I may say this equals my new X data so I make it out here Im going say
    manually standardize the data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so after I do that I run all of this stuff again and now I see that great
    x2 is back in play„ÄÇ even though I have different unitsÔºå it's not getting obsessed
    with these other columns just based on the units„ÄÇ so this was a good thing to
    do okay that's what standardization is„ÄÇNow it turns out that to do this rightÔºå
    I have to calculate this mean and standard deviation on the training data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but then I have to use that same mean and standard deviation on the test data
    I can't retake the mean on the test data and so the methodology of this gets a
    little bit complicated and so generally we won't do this generally we'll have
    ask K learn do that for us and so it turns out that there's a preprocessing step
    called standardization and we're going to use that as so manually doing this so
    I'm going to head back here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And you can see I've already import my standard scalar„ÄÇAnd I'm to run this here„ÄÇAnd
    this is skipping now for my modelÔºå rightÔºå I'll just actually leave this for now„ÄÇ
    and that'll be my bad model„ÄÇ What I'll do is I'll' create a new model„ÄÇ which will
    be a pipeline model„ÄÇAnd in that pipeline model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I want to have a standard scalar followed by a logistic regression„ÄÇJust like
    that and„ÄÇThis one„ÄÇ I may have to actually create them like that„ÄÇI have to give
    them namesÔºå right„ÄÇ So I'm going pass tus here„ÄÇ So I'm going to call that standard
    scalar„ÄÇOkay„ÄÇ I have to put Thomas to separate these things„ÄÇAnd thenÔºå then we go
    have this logistic regression„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Like that And this is my new model and then it turns out all this stuff I was
    doing before of like fitting„ÄÇ for example it can work the same way I can fit just
    like I did before because I this new one also fit and so I can do that I could
    also score it like I did before let me score it now and I get something very similar
    and then what's might be interesting is that when I actually do this when I actually
    try to get this bar plot it should show that it's back in play right even though
    the nonstandardized version is ignoring x2 now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this version should show it so remember this there's gonna be a small error
    here and the problem is that pipeline doesn't have coefficients this pipeline
    as a whole doesn't have coefficients but the logistic regression inside of it
    does have coefficients„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåAnd so how can I get to that it turns out that any pipeline works like a dictionary
    and I can for example I can copy these names and use that like a key and so that
    would get me my standard scalar from the beginning or I could pass in this key
    and that would get me a logistic my logistic regression stage of it and so from
    that then I could actually see well what are the coefficients involved there and
    I would I would paste this here instead of what I originally had and so now I
    can see that when I have the standardization in play as a transformer before my
    estimator it'll automatically do that and then it'll do the right things as well
    if I do my fitting here it'll calculate that mean and a standard deviation and
    I do scoring it's just going to use the same mean in standard deviation from before
    it would not look at that for the task because that would be kind of a methodological
    mistake so we're going to be generally doing whenever we have a logistic regression
    unless we have some very special scenario„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: For exampleÔºå the data has already been standardized„ÄÇ![](img/ba488996234f1aee5936aea9ea40f4bf_4.png)
  prefs: []
  type: TYPE_NORMAL
