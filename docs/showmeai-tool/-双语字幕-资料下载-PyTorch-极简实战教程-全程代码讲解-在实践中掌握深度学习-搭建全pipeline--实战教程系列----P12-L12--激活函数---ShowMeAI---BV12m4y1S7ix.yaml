- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12-
    æ¿€æ´»å‡½æ•° - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12-
    æ¿€æ´»å‡½æ•° - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome to your new Pytorch tutorialã€‚ This timeã€‚ I want to talk
    about activationation functionsã€‚ Actation functions are an extremely important
    feature of neural networksã€‚ So let's have a look at what activationation functions
    areã€‚ why they are used what different types of functions there are and how we
    incorporate them into our pyt modelã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°æ–°çš„PyTorchæ•™ç¨‹ã€‚è¿™æ¬¡ï¼Œæˆ‘æƒ³è°ˆè°ˆæ¿€æ´»å‡½æ•°ã€‚æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œä¸­æå…¶é‡è¦çš„ç‰¹æ€§ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä»€ä¹ˆæ˜¯æ¿€æ´»å‡½æ•°ï¼Œä¸ºä»€ä¹ˆä½¿ç”¨å®ƒä»¬ï¼Œæœ‰å“ªäº›ä¸åŒç±»å‹çš„å‡½æ•°ï¼Œä»¥åŠå¦‚ä½•å°†å®ƒä»¬çº³å…¥æˆ‘ä»¬çš„PyTorchæ¨¡å‹ã€‚
- en: So activationation functions apply a linear transformation to the layer output
    and basically decide whether a neuron should be activated or notã€‚ğŸ˜Šï¼ŒSo why do we
    use themï¼Œ Why is only a linear transformation not good enoughã€‚So typicallyã€‚ we
    would have a linear layer in our network that applies a linear transformationã€‚
    So here it multiplies the input input with some weights and maybe add sub buyers
    and then delivers the outputã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°å¯¹å±‚è¾“å‡ºåº”ç”¨çº¿æ€§å˜æ¢ï¼Œå¹¶åŸºæœ¬å†³å®šä¸€ä¸ªç¥ç»å…ƒæ˜¯å¦åº”è¢«æ¿€æ´»ã€‚ğŸ˜Š é‚£ä¹ˆï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆä½¿ç”¨å®ƒä»¬å‘¢ï¼Ÿä»…ä»…è¿›è¡Œçº¿æ€§å˜æ¢ä¸ºä»€ä¹ˆä¸å¤Ÿå¥½ï¼Ÿé€šå¸¸ï¼Œæˆ‘ä»¬ä¼šåœ¨ç½‘ç»œä¸­æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¯¥å±‚åº”ç”¨çº¿æ€§å˜æ¢ã€‚åœ¨è¿™é‡Œï¼Œå®ƒç”¨æŸäº›æƒé‡ä¹˜ä»¥è¾“å…¥ï¼Œå¹¶å¯èƒ½åŠ ä¸Šåç½®ï¼Œç„¶åè¾“å‡ºç»“æœã€‚
- en: And let's suppose we don't have activationctuation functions in betweenã€‚Then
    we would have only linear transformations after each otherã€‚ So our whole network
    from input to output is essentially just a linear regression modelã€‚ And this linear
    model is not suited for more complex tasksã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬ä¹‹é—´æ²¡æœ‰æ¿€æ´»å‡½æ•°ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬çš„æ•´ä¸ªç½‘ç»œä»è¾“å…¥åˆ°è¾“å‡ºå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ã€‚è€Œè¿™ä¸ªçº¿æ€§æ¨¡å‹å¹¶ä¸é€‚åˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚
- en: So the conclusion is that with non nonlinear transformations in between our
    network can learn better and perform more complex tasksã€‚ So after each layerï¼Œ
    we typically want to apply this activation functionsã€‚ So hereã€‚Firstã€‚ we have our
    normal linear layerï¼Œ and then we also apply this activationctuaation functionã€‚And
    with thisï¼Œ our network can learn betterã€‚And now let's talk about the most popular
    activation functionsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“è®ºæ˜¯ï¼Œé€šè¿‡åœ¨ä¸­é—´åŠ å…¥éçº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå¯ä»¥æ›´å¥½åœ°å­¦ä¹ å¹¶æ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œåœ¨æ¯ä¸€å±‚ä¹‹åï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›åº”ç”¨è¿™äº›æ¿€æ´»å‡½æ•°ã€‚è¿™é‡Œï¼Œé¦–å…ˆæ˜¯æˆ‘ä»¬æ­£å¸¸çš„çº¿æ€§å±‚ï¼Œç„¶ååº”ç”¨è¿™ä¸ªæ¿€æ´»å‡½æ•°ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬çš„ç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¨è®ºæœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ã€‚
- en: So the ones I want to show you is the binary step functionï¼Œ the smoid functionã€‚
    the hyperbolic tangent functionï¼Œ the realloï¼Œ the leaky reulu and the softmã€‚So
    let's start with the simple step functionã€‚ So this will just output one if our
    input is greater than a thresholdã€‚ So here the threshold is 0 and0 otherwiseï¼Œ
    this is not used in practice actuallyã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³å±•ç¤ºçš„å‡½æ•°æ˜¯äºŒè¿›åˆ¶é˜¶è·ƒå‡½æ•°ã€sigmoidå‡½æ•°ã€åŒæ›²æ­£åˆ‡å‡½æ•°ã€ReLUå‡½æ•°ã€Leaky ReLUå’Œsoftmaxå‡½æ•°ã€‚é¦–å…ˆï¼Œä»ç®€å•çš„é˜¶è·ƒå‡½æ•°å¼€å§‹ã€‚å½“æˆ‘ä»¬çš„è¾“å…¥å¤§äºä¸€ä¸ªé˜ˆå€¼æ—¶ï¼Œå®ƒå°†è¾“å‡º1ã€‚è¿™é‡Œçš„é˜ˆå€¼æ˜¯0ï¼Œå¦åˆ™è¾“å‡º0ï¼Œè¿™å®é™…ä¸Šåœ¨å®è·µä¸­å¹¶ä¸å¸¸ç”¨ã€‚
- en: but this should demonstrate the example of if the neuron should be activatedted
    or notã€‚And yeahã€‚ so a more popular choice is the sigoid functionã€‚ And you should
    already know this if you've watched my tutorial about logistic regressionã€‚ So
    the formula is  one over one plus E to the minus xã€‚ and this will output a probability
    between 0 and1ã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™åº”è¯¥æ¼”ç¤ºä¸€ä¸ªç¥ç»å…ƒæ˜¯å¦åº”è¯¥è¢«æ¿€æ´»çš„ä¾‹å­ã€‚è€Œä¸”ï¼Œä¸€ä¸ªæ›´æµè¡Œçš„é€‰æ‹©æ˜¯sigmoidå‡½æ•°ã€‚å¦‚æœä½ çœ‹è¿‡æˆ‘å…³äºé€»è¾‘å›å½’çš„æ•™ç¨‹ï¼Œä½ åº”è¯¥å·²ç»çŸ¥é“è¿™ä¸ªã€‚å®ƒçš„å…¬å¼æ˜¯1é™¤ä»¥1åŠ ä¸Šeçš„è´Ÿxæ¬¡æ–¹ï¼Œè¿™å°†è¾“å‡ºä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ¦‚ç‡ã€‚
- en: And this is typically used in the last layer of a binary classification problemã€‚Soï¼Œ
    yeahã€‚ then we have the hyperbolic tangent function or ton Hã€‚ This is basically
    a scaled sigmoid function and also a little bit shiftedã€‚ So this will output a
    value between-1 and plus oneã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é€šå¸¸ç”¨äºäºŒåˆ†ç±»é—®é¢˜çš„æœ€åä¸€å±‚ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬æœ‰åŒæ›²æ­£åˆ‡å‡½æ•°ï¼Œä¹Ÿç§°ä¸ºtanhã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç¼©æ”¾çš„sigmoidå‡½æ•°ï¼Œå¹¶ä¸”ç¨å¾®æœ‰ç‚¹åç§»ã€‚å› æ­¤ï¼Œå®ƒçš„è¾“å‡ºå€¼åœ¨-1å’Œ1ä¹‹é—´ã€‚
- en: And this is actually a good choice in hidden layersã€‚ So you should know about
    the ton H functionã€‚Then we have the relo functionï¼Œ and this is the most popular
    choice in in most of the networksã€‚ So the relu function will output 0 for negative
    valuesã€‚ and it will simply output the input as output for positive valuesã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™åœ¨éšè—å±‚ä¸­æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚ä½ åº”è¯¥äº†è§£tanhå‡½æ•°ã€‚æ¥ä¸‹æ¥æ˜¯ReLUå‡½æ•°ï¼Œè¿™æ˜¯å¤§å¤šæ•°ç½‘ç»œä¸­æœ€æµè¡Œçš„é€‰æ‹©ã€‚ReLUå‡½æ•°å¯¹è´Ÿå€¼è¾“å‡º0ï¼Œå¯¹æ­£å€¼åˆ™ç›´æ¥è¾“å‡ºè¾“å…¥å€¼ã€‚
- en: So it is actually a linear function for values greater than 0ï¼Œ and it is just0
    for negative valuesã€‚ So it doesn't look that much different from just a linear
    transformationï¼Œ but in factã€‚ it is nonlineararï¼Œ and it is actually the most popular
    choice in the networksã€‚ and its typically a very good choice for an activationation
    functionã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¯¹å¤§äº0çš„å€¼çš„çº¿æ€§å‡½æ•°ï¼Œè€Œå¯¹è´Ÿå€¼åˆ™ä¸º0ã€‚å› æ­¤ï¼Œå®ƒçœ‹èµ·æ¥ä¸çº¿æ€§å˜æ¢æ²¡æœ‰å¤ªå¤§ä¸åŒï¼Œä½†å®é™…ä¸Šå®ƒæ˜¯éçº¿æ€§çš„ï¼Œå®é™…ä¸Šæ˜¯ç½‘ç»œä¸­æœ€æµè¡Œçš„é€‰æ‹©ï¼Œé€šå¸¸æ˜¯æ¿€æ´»å‡½æ•°çš„ä¸€ä¸ªéå¸¸å¥½çš„é€‰æ‹©ã€‚
- en: So the rule of thumb is if you don't know which function you should useã€‚ then
    just use a relo for hidden layersã€‚Yeahï¼Œ so this is the reluï¼Œ very popular choiceã€‚
    Then we also have the leaky relu functionã€‚ So this is a slightly modified and
    slightly improved version of the reluã€‚ So this will still just output the input
    for x greater than 0ã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¡ç»éªŒæ³•åˆ™æ˜¯ï¼Œå¦‚æœä½ ä¸çŸ¥é“åº”è¯¥ä½¿ç”¨å“ªä¸ªå‡½æ•°ï¼Œé‚£ä¹ˆåªéœ€åœ¨éšè—å±‚ä¸­ä½¿ç”¨ReLUã€‚æ˜¯çš„ï¼Œè¿™å°±æ˜¯ReLUï¼Œéå¸¸å—æ¬¢è¿çš„é€‰æ‹©ã€‚ç„¶åæˆ‘ä»¬è¿˜æœ‰æ³„æ¼ReLUå‡½æ•°ã€‚è¿™æ˜¯ReLUçš„ç¨å¾®ä¿®æ”¹å’Œæ”¹è¿›ç‰ˆæœ¬ã€‚å› æ­¤ï¼Œå¯¹äºå¤§äº0çš„xï¼Œå®ƒä»ç„¶åªè¾“å‡ºè¾“å…¥ã€‚
- en: but this will multiply our input with a very small value for negative numbersã€‚
    So here I've written a times x for negative numbersã€‚ and this a is typically very
    smallã€‚ So it'sã€‚ for exampleï¼Œ0001ã€‚And this is an improved version of the relo that
    tries to solve the so called vanish ingredient problemã€‚Because with a normal reluï¼Œ
    our values here are 0ã€‚ And this means that also the gradient later in the back
    propagation is 0ã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯è¿™ä¼šå¯¹è´Ÿæ•°çš„è¾“å…¥ä¹˜ä»¥ä¸€ä¸ªéå¸¸å°çš„å€¼ã€‚å› æ­¤ï¼Œæˆ‘åœ¨è¿™é‡Œä¸ºè´Ÿæ•°å†™äº†aä¹˜ä»¥xï¼Œè€Œè¿™ä¸ªaé€šå¸¸æ˜¯éå¸¸å°çš„ã€‚ä¾‹å¦‚ï¼Œ0.0001ã€‚è¿™æ˜¯ReLUçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œè¯•å›¾è§£å†³æ‰€è°“çš„æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚å› ä¸ºä½¿ç”¨æ­£å¸¸çš„ReLUæ—¶ï¼Œæˆ‘ä»¬è¿™é‡Œçš„å€¼ä¸º0ã€‚è¿™ä¹Ÿæ„å‘³ç€åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæ¢¯åº¦ä¹Ÿæ˜¯0ã€‚
- en: And when the gradient is 0ï¼Œ then this means that these weights will never be
    updatedã€‚ So these neurons won't learn anythingã€‚ And we also say that these neurons
    are deadã€‚And this is why sometimes you want to use the leaky relu functionã€‚ So
    whenever you notice that your weights won't update during trainingã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¢¯åº¦ä¸º0æ—¶ï¼Œè¿™æ„å‘³ç€è¿™äº›æƒé‡å°†æ°¸è¿œä¸ä¼šè¢«æ›´æ–°ã€‚å› æ­¤ï¼Œè¿™äº›ç¥ç»å…ƒä¸ä¼šå­¦ä¹ ä»»ä½•ä¸œè¥¿ã€‚æˆ‘ä»¬ä¹Ÿç§°è¿™äº›ç¥ç»å…ƒä¸ºâ€œæ­»äº¡â€ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰æ—¶ä½ æƒ³ä½¿ç”¨æ³„æ¼ReLUå‡½æ•°çš„åŸå› ã€‚æ‰€ä»¥æ¯å½“ä½ æ³¨æ„åˆ°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æƒé‡ä¸ä¼šæ›´æ–°æ—¶ã€‚
- en: then try to use the leaky relu instead of the normal reloã€‚And yeahï¼Œ then as
    a last functionã€‚ I want to show you the softmax functionã€‚ and you also should
    already know this because I have a whole tutorial about the soft softmax functionã€‚
    So this will just this will basically squash the inputs to be outputs between
    0 and one so that we have a probability as an outputã€‚ And this is typically a
    good choice in the last layer of a multi class classification problemã€‚Soã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°è¯•ä½¿ç”¨æ³„æ¼ReLUä»£æ›¿æ­£å¸¸çš„ReLUã€‚æœ€åï¼Œæˆ‘æƒ³å‘ä½ å±•ç¤ºSoftmaxå‡½æ•°ã€‚ä½ ä¹Ÿåº”è¯¥å·²ç»çŸ¥é“è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘æœ‰ä¸€ä¸ªå…³äºSoftmaxå‡½æ•°çš„å®Œæ•´æ•™ç¨‹ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šä¼šå°†è¾“å…¥å‹ç¼©ä¸º0åˆ°1ä¹‹é—´çš„è¾“å‡ºï¼Œä»¥ä¾¿æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªæ¦‚ç‡ä½œä¸ºè¾“å‡ºã€‚è¿™é€šå¸¸æ˜¯å¤šç±»åˆ†ç±»é—®é¢˜æœ€åä¸€å±‚çš„ä¸€ä¸ªä¸é”™é€‰æ‹©ã€‚
- en: yeahï¼Œ that's the different activationation functions I wanted to show youã€‚ And
    now let's jump to the code and see how we can use them in Pytorchã€‚ So we have
    two optionsã€‚![](img/e9ab40438781f445672555aa3780e63b_1.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘æƒ³å±•ç¤ºçš„ä¸åŒæ¿€æ´»å‡½æ•°ã€‚ç°åœ¨è®©æˆ‘ä»¬è·³åˆ°ä»£ç ï¼Œçœ‹çœ‹å¦‚ä½•åœ¨PyTorchä¸­ä½¿ç”¨å®ƒä»¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªé€‰æ‹©ã€‚![](img/e9ab40438781f445672555aa3780e63b_1.png)
- en: And the first one is to create our functions as N N modulesã€‚ So in our network
    in the init functionã€‚ firstï¼Œ we define all the layers we want to haveã€‚ So hereï¼Œ
    for exampleï¼Œ firstã€‚ we have a linear layerã€‚And then after thatï¼Œ we want to have
    a relu activationctuaation functionã€‚ so we create our relu module hereã€‚And we
    can get that from the torch dot and N moduleã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ˜¯å°†æˆ‘ä»¬çš„å‡½æ•°åˆ›å»ºä¸ºNNæ¨¡å—ã€‚åœ¨æˆ‘ä»¬çš„ç½‘ç»œçš„åˆå§‹åŒ–å‡½æ•°ä¸­ï¼Œé¦–å…ˆå®šä¹‰æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰å±‚ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆæœ‰ä¸€ä¸ªçº¿æ€§å±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºæˆ‘ä»¬çš„ReLUæ¨¡å—ã€‚æˆ‘ä»¬å¯ä»¥ä»torchçš„NNæ¨¡å—ä¸­è·å–ã€‚
- en: So this contains all the different functions I just showed youã€‚And then we have
    the next layer for hereï¼Œ exampleï¼Œ it's the next linear layerã€‚ and then the next
    activationation functionã€‚ So here we have a sigoidid at the endã€‚And then in the
    forward passï¼Œ we simply call all these functions after each otherã€‚ So firstã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŒ…å«äº†æˆ‘åˆšæ‰å±•ç¤ºçš„æ‰€æœ‰ä¸åŒå‡½æ•°ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æœ‰è¿™é‡Œçš„ä¸‹ä¸€å±‚ï¼Œä¾‹å¦‚ï¼Œä¸‹ä¸€å±‚çº¿æ€§å±‚ã€‚ç„¶åæ˜¯ä¸‹ä¸€ä¸ªæ¿€æ´»å‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨æœ€åæœ‰ä¸€ä¸ªSigmoidå‡½æ•°ã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬ç®€å•åœ°ä¾æ¬¡è°ƒç”¨æ‰€æœ‰è¿™äº›å‡½æ•°ã€‚å› æ­¤ï¼Œé¦–å…ˆã€‚
- en: we have the linearï¼Œ the first linear layerï¼Œ which gets an outputã€‚ And then we
    use this output and put it into our reloã€‚ And then againã€‚ we use this output and
    put it in the next linear layer and so onã€‚So this is the first way how we can
    use itã€‚ and the second way is to use these functions directlyã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰çº¿æ€§ï¼Œç¬¬ä¸€å±‚çº¿æ€§ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªè¾“å‡ºå¹¶å°†å…¶æ”¾å…¥æˆ‘ä»¬çš„reloä¸­ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨è¿™ä¸ªè¾“å‡ºï¼Œå°†å…¶æ”¾å…¥ä¸‹ä¸€ä¸ªçº¿æ€§å±‚ï¼Œä¾æ­¤ç±»æ¨ã€‚è¿™æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„ç¬¬ä¸€ç§æ–¹å¼ï¼Œè€Œç¬¬äºŒç§æ–¹å¼æ˜¯ç›´æ¥ä½¿ç”¨è¿™äº›å‡½æ•°ã€‚
- en: So in the init functionï¼Œ we only define our linear layerã€‚ So linear 1 and linear
    2 and then in the forward pass we apply this linear layer and then also call this
    torch dot relu function here and then the torch dot seeoidoid function directlyã€‚
    So this is just from the torch APIã€‚And yeahï¼Œ this is a different way how we can
    use itã€‚ Both ways will achieve the same thingã€‚ It's justã€‚How you prefer your codeã€‚
    And yeahã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨initå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åªå®šä¹‰æˆ‘ä»¬çš„çº¿æ€§å±‚ã€‚æ‰€ä»¥çº¿æ€§1å’Œçº¿æ€§2ï¼Œç„¶ååœ¨å‰å‘ä¼ é€’ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨è¿™ä¸ªçº¿æ€§å±‚ï¼Œç„¶åä¹Ÿåœ¨è¿™é‡Œè°ƒç”¨torch dot reluå‡½æ•°ï¼Œæ¥ç€æ˜¯torch
    dot sigmoidå‡½æ•°ã€‚æ‰€ä»¥è¿™åªæ˜¯æ¥è‡ªtorch APIçš„ä¸åŒä½¿ç”¨æ–¹å¼ã€‚æ²¡é”™ï¼Œè¿™ä¸¤ç§æ–¹å¼å°†å®ç°ç›¸åŒçš„æ•ˆæœã€‚åªæ˜¯ä½ å–œæ¬¢ä½ çš„ä»£ç æ–¹å¼è€Œå·²ã€‚æ²¡é”™ã€‚
- en: so all the functions that I just showed youï¼Œ you can get from the N N moduleã€‚
    So here we have an N reluï¼Œ but we canï¼Œ for exampleã€‚ also have an N dot sigmoid
    and we have an N dot soft marksï¼Œ and we have an N dot ton Hã€‚å—¯ã€‚And alsoã€‚ N and
    dot leaky reluã€‚ So all these functions are available hereã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æˆ‘åˆšåˆšç»™ä½ å±•ç¤ºçš„å‡½æ•°ï¼Œä½ å¯ä»¥ä»N Næ¨¡å—ä¸­è·å¾—ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰N reluï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ï¼Œæ¯”å¦‚è¯´ï¼Œæ‹¥æœ‰N dot sigmoidï¼Œè¿˜æœ‰N dot soft
    marksï¼Œä»¥åŠN dot ton Hã€‚å—¯ã€‚è¿˜æœ‰ï¼ŒNå’Œdot leaky reluã€‚æ‰€ä»¥æ‰€æœ‰è¿™äº›å‡½æ•°åœ¨è¿™é‡Œéƒ½æ˜¯å¯ç”¨çš„ã€‚
- en: And they are also available in the torch API like thisã€‚ So here we have torch
    dot reloã€‚ and we have torch dot sigmoidã€‚ We also have torch dot soft maxã€‚And torch
    dot1n Hã€‚And but sometimes they are not used in the the functions are not available
    in the torch API directlyã€‚ but they are available in torch do n n dot functionalã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬åœ¨torch APIä¸­ä¹Ÿå¯ä»¥è¿™æ ·ä½¿ç”¨ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰torch dot reloï¼Œè¿˜æœ‰torch dot sigmoidã€‚æˆ‘ä»¬ä¹Ÿæœ‰torch dot
    soft maxå’Œtorch dot1n Hã€‚ä½†æœ‰æ—¶è¿™äº›å‡½æ•°åœ¨torch APIä¸­ä¸å¯ç”¨ï¼Œä½†å®ƒä»¬åœ¨torch do n n dot functionalä¸­æ˜¯å¯ä»¥æ‰¾åˆ°çš„ã€‚
- en: So here I import a torch and n functional as Fã€‚ and then I can call hereï¼Œ for
    exampleï¼Œ F dot Reluã€‚ So this is the same as torch dot reluã€‚ but hereï¼Œ for exampleã€‚
    is the torch is F dot leaky relu is only available in this APIã€‚Soï¼Œ yeahã€‚ but that's
    how we can use the activation functions and pytorrchã€‚ And it's actually very easyã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘å¯¼å…¥äº†torchå’Œn functionalä½œä¸ºFã€‚ç„¶åæˆ‘å¯ä»¥åœ¨è¿™é‡Œè°ƒç”¨ï¼Œæ¯”å¦‚è¯´ï¼ŒF dot Reluã€‚è¿™å’Œtorch dot reluæ˜¯ä¸€æ ·çš„ã€‚ä½†è¿™é‡Œï¼Œæ¯”å¦‚è¯´ï¼Œtorchçš„F
    dot leaky reluåªåœ¨è¿™ä¸ªAPIä¸­å¯ç”¨ã€‚æ‰€ä»¥ï¼Œæ²¡é”™ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨æ¿€æ´»å‡½æ•°å’Œpytorchã€‚è€Œä¸”è¿™å®é™…ä¸Šéå¸¸ç®€å•ã€‚
- en: And I hope you understood everything and now feel comfortable with activationation
    functionsã€‚ If you like thisï¼Œ please subscribe to the channel and see you next
    time byã€‚![](img/e9ab40438781f445672555aa3780e63b_3.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ èƒ½ç†è§£æ‰€æœ‰å†…å®¹ï¼Œç°åœ¨å¯¹æ¿€æ´»å‡½æ•°æ„Ÿåˆ°èˆ’é€‚ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œæˆ‘ä»¬ä¸‹æ¬¡è§ï¼
