- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P12ÔºöL12-
    ÊøÄÊ¥ªÂáΩÊï∞ - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to your new Pytorch tutorial„ÄÇ This time„ÄÇ I want to talk
    about activationation functions„ÄÇ Actation functions are an extremely important
    feature of neural networks„ÄÇ So let's have a look at what activationation functions
    are„ÄÇ why they are used what different types of functions there are and how we
    incorporate them into our pyt model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So activationation functions apply a linear transformation to the layer output
    and basically decide whether a neuron should be activated or not„ÄÇüòäÔºåSo why do we
    use themÔºå Why is only a linear transformation not good enough„ÄÇSo typically„ÄÇ we
    would have a linear layer in our network that applies a linear transformation„ÄÇ
    So here it multiplies the input input with some weights and maybe add sub buyers
    and then delivers the output„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And let's suppose we don't have activationctuation functions in between„ÄÇThen
    we would have only linear transformations after each other„ÄÇ So our whole network
    from input to output is essentially just a linear regression model„ÄÇ And this linear
    model is not suited for more complex tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the conclusion is that with non nonlinear transformations in between our
    network can learn better and perform more complex tasks„ÄÇ So after each layerÔºå
    we typically want to apply this activation functions„ÄÇ So here„ÄÇFirst„ÄÇ we have our
    normal linear layerÔºå and then we also apply this activationctuaation function„ÄÇAnd
    with thisÔºå our network can learn better„ÄÇAnd now let's talk about the most popular
    activation functions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the ones I want to show you is the binary step functionÔºå the smoid function„ÄÇ
    the hyperbolic tangent functionÔºå the realloÔºå the leaky reulu and the softm„ÄÇSo
    let's start with the simple step function„ÄÇ So this will just output one if our
    input is greater than a threshold„ÄÇ So here the threshold is 0 and0 otherwiseÔºå
    this is not used in practice actually„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this should demonstrate the example of if the neuron should be activatedted
    or not„ÄÇAnd yeah„ÄÇ so a more popular choice is the sigoid function„ÄÇ And you should
    already know this if you've watched my tutorial about logistic regression„ÄÇ So
    the formula is  one over one plus E to the minus x„ÄÇ and this will output a probability
    between 0 and1„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is typically used in the last layer of a binary classification problem„ÄÇSoÔºå
    yeah„ÄÇ then we have the hyperbolic tangent function or ton H„ÄÇ This is basically
    a scaled sigmoid function and also a little bit shifted„ÄÇ So this will output a
    value between-1 and plus one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is actually a good choice in hidden layers„ÄÇ So you should know about
    the ton H function„ÄÇThen we have the relo functionÔºå and this is the most popular
    choice in in most of the networks„ÄÇ So the relu function will output 0 for negative
    values„ÄÇ and it will simply output the input as output for positive values„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it is actually a linear function for values greater than 0Ôºå and it is just0
    for negative values„ÄÇ So it doesn't look that much different from just a linear
    transformationÔºå but in fact„ÄÇ it is nonlineararÔºå and it is actually the most popular
    choice in the networks„ÄÇ and its typically a very good choice for an activationation
    function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the rule of thumb is if you don't know which function you should use„ÄÇ then
    just use a relo for hidden layers„ÄÇYeahÔºå so this is the reluÔºå very popular choice„ÄÇ
    Then we also have the leaky relu function„ÄÇ So this is a slightly modified and
    slightly improved version of the relu„ÄÇ So this will still just output the input
    for x greater than 0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this will multiply our input with a very small value for negative numbers„ÄÇ
    So here I've written a times x for negative numbers„ÄÇ and this a is typically very
    small„ÄÇ So it's„ÄÇ for exampleÔºå0001„ÄÇAnd this is an improved version of the relo that
    tries to solve the so called vanish ingredient problem„ÄÇBecause with a normal reluÔºå
    our values here are 0„ÄÇ And this means that also the gradient later in the back
    propagation is 0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And when the gradient is 0Ôºå then this means that these weights will never be
    updated„ÄÇ So these neurons won't learn anything„ÄÇ And we also say that these neurons
    are dead„ÄÇAnd this is why sometimes you want to use the leaky relu function„ÄÇ So
    whenever you notice that your weights won't update during training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then try to use the leaky relu instead of the normal relo„ÄÇAnd yeahÔºå then as
    a last function„ÄÇ I want to show you the softmax function„ÄÇ and you also should
    already know this because I have a whole tutorial about the soft softmax function„ÄÇ
    So this will just this will basically squash the inputs to be outputs between
    0 and one so that we have a probability as an output„ÄÇ And this is typically a
    good choice in the last layer of a multi class classification problem„ÄÇSo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: yeahÔºå that's the different activationation functions I wanted to show you„ÄÇ And
    now let's jump to the code and see how we can use them in Pytorch„ÄÇ So we have
    two options„ÄÇ![](img/e9ab40438781f445672555aa3780e63b_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: And the first one is to create our functions as N N modules„ÄÇ So in our network
    in the init function„ÄÇ firstÔºå we define all the layers we want to have„ÄÇ So hereÔºå
    for exampleÔºå first„ÄÇ we have a linear layer„ÄÇAnd then after thatÔºå we want to have
    a relu activationctuaation function„ÄÇ so we create our relu module here„ÄÇAnd we
    can get that from the torch dot and N module„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this contains all the different functions I just showed you„ÄÇAnd then we have
    the next layer for hereÔºå exampleÔºå it's the next linear layer„ÄÇ and then the next
    activationation function„ÄÇ So here we have a sigoidid at the end„ÄÇAnd then in the
    forward passÔºå we simply call all these functions after each other„ÄÇ So first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have the linearÔºå the first linear layerÔºå which gets an output„ÄÇ And then we
    use this output and put it into our relo„ÄÇ And then again„ÄÇ we use this output and
    put it in the next linear layer and so on„ÄÇSo this is the first way how we can
    use it„ÄÇ and the second way is to use these functions directly„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So in the init functionÔºå we only define our linear layer„ÄÇ So linear 1 and linear
    2 and then in the forward pass we apply this linear layer and then also call this
    torch dot relu function here and then the torch dot seeoidoid function directly„ÄÇ
    So this is just from the torch API„ÄÇAnd yeahÔºå this is a different way how we can
    use it„ÄÇ Both ways will achieve the same thing„ÄÇ It's just„ÄÇHow you prefer your code„ÄÇ
    And yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so all the functions that I just showed youÔºå you can get from the N N module„ÄÇ
    So here we have an N reluÔºå but we canÔºå for example„ÄÇ also have an N dot sigmoid
    and we have an N dot soft marksÔºå and we have an N dot ton H„ÄÇÂóØ„ÄÇAnd also„ÄÇ N and
    dot leaky relu„ÄÇ So all these functions are available here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And they are also available in the torch API like this„ÄÇ So here we have torch
    dot relo„ÄÇ and we have torch dot sigmoid„ÄÇ We also have torch dot soft max„ÄÇAnd torch
    dot1n H„ÄÇAnd but sometimes they are not used in the the functions are not available
    in the torch API directly„ÄÇ but they are available in torch do n n dot functional„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here I import a torch and n functional as F„ÄÇ and then I can call hereÔºå for
    exampleÔºå F dot Relu„ÄÇ So this is the same as torch dot relu„ÄÇ but hereÔºå for example„ÄÇ
    is the torch is F dot leaky relu is only available in this API„ÄÇSoÔºå yeah„ÄÇ but that's
    how we can use the activation functions and pytorrch„ÄÇ And it's actually very easy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I hope you understood everything and now feel comfortable with activationation
    functions„ÄÇ If you like thisÔºå please subscribe to the channel and see you next
    time by„ÄÇ![](img/e9ab40438781f445672555aa3780e63b_3.png)
  prefs: []
  type: TYPE_NORMAL
