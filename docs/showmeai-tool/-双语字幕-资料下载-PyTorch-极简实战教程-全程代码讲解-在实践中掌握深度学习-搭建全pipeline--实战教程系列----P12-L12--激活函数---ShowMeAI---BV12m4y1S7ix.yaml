- en: 【双语字幕+资料下载】PyTorch 极简实战教程！全程代码讲解，在实践中掌握深度学习&搭建全pipeline！＜实战教程系列＞ - P12：L12-
    激活函数 - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】PyTorch 极简实战教程！全程代码讲解，在实践中掌握深度学习&搭建全pipeline！＜实战教程系列＞ - P12：L12-
    激活函数 - ShowMeAI - BV12m4y1S7ix
- en: Hi， everybody。 Welcome to your new Pytorch tutorial。 This time。 I want to talk
    about activationation functions。 Actation functions are an extremely important
    feature of neural networks。 So let's have a look at what activationation functions
    are。 why they are used what different types of functions there are and how we
    incorporate them into our pyt model。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，大家好。欢迎来到新的PyTorch教程。这次，我想谈谈激活函数。激活函数是神经网络中极其重要的特性。让我们看看什么是激活函数，为什么使用它们，有哪些不同类型的函数，以及如何将它们纳入我们的PyTorch模型。
- en: So activationation functions apply a linear transformation to the layer output
    and basically decide whether a neuron should be activated or not。😊，So why do we
    use them， Why is only a linear transformation not good enough。So typically。 we
    would have a linear layer in our network that applies a linear transformation。
    So here it multiplies the input input with some weights and maybe add sub buyers
    and then delivers the output。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数对层输出应用线性变换，并基本决定一个神经元是否应被激活。😊 那么，我们为什么使用它们呢？仅仅进行线性变换为什么不够好？通常，我们会在网络中有一个线性层，该层应用线性变换。在这里，它用某些权重乘以输入，并可能加上偏置，然后输出结果。
- en: And let's suppose we don't have activationctuation functions in between。Then
    we would have only linear transformations after each other。 So our whole network
    from input to output is essentially just a linear regression model。 And this linear
    model is not suited for more complex tasks。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们之间没有激活函数。那么，我们的整个网络从输入到输出实际上只是一个线性回归模型。而这个线性模型并不适合更复杂的任务。
- en: So the conclusion is that with non nonlinear transformations in between our
    network can learn better and perform more complex tasks。 So after each layer，
    we typically want to apply this activation functions。 So here。First。 we have our
    normal linear layer， and then we also apply this activationctuaation function。And
    with this， our network can learn better。And now let's talk about the most popular
    activation functions。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 结论是，通过在中间加入非线性变换，我们的网络可以更好地学习并执行更复杂的任务。因此，在每一层之后，我们通常希望应用这些激活函数。这里，首先是我们正常的线性层，然后应用这个激活函数。这样，我们的网络能够更好地学习。现在，让我们讨论最流行的激活函数。
- en: So the ones I want to show you is the binary step function， the smoid function。
    the hyperbolic tangent function， the reallo， the leaky reulu and the softm。So
    let's start with the simple step function。 So this will just output one if our
    input is greater than a threshold。 So here the threshold is 0 and0 otherwise，
    this is not used in practice actually。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我想展示的函数是二进制阶跃函数、sigmoid函数、双曲正切函数、ReLU函数、Leaky ReLU和softmax函数。首先，从简单的阶跃函数开始。当我们的输入大于一个阈值时，它将输出1。这里的阈值是0，否则输出0，这实际上在实践中并不常用。
- en: but this should demonstrate the example of if the neuron should be activatedted
    or not。And yeah。 so a more popular choice is the sigoid function。 And you should
    already know this if you've watched my tutorial about logistic regression。 So
    the formula is  one over one plus E to the minus x。 and this will output a probability
    between 0 and1。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但这应该演示一个神经元是否应该被激活的例子。而且，一个更流行的选择是sigmoid函数。如果你看过我关于逻辑回归的教程，你应该已经知道这个。它的公式是1除以1加上e的负x次方，这将输出一个介于0和1之间的概率。
- en: And this is typically used in the last layer of a binary classification problem。So，
    yeah。 then we have the hyperbolic tangent function or ton H。 This is basically
    a scaled sigmoid function and also a little bit shifted。 So this will output a
    value between-1 and plus one。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于二分类问题的最后一层。那么，我们有双曲正切函数，也称为tanh。它基本上是一个缩放的sigmoid函数，并且稍微有点偏移。因此，它的输出值在-1和1之间。
- en: And this is actually a good choice in hidden layers。 So you should know about
    the ton H function。Then we have the relo function， and this is the most popular
    choice in in most of the networks。 So the relu function will output 0 for negative
    values。 and it will simply output the input as output for positive values。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这在隐藏层中是一个不错的选择。你应该了解tanh函数。接下来是ReLU函数，这是大多数网络中最流行的选择。ReLU函数对负值输出0，对正值则直接输出输入值。
- en: So it is actually a linear function for values greater than 0， and it is just0
    for negative values。 So it doesn't look that much different from just a linear
    transformation， but in fact。 it is nonlinearar， and it is actually the most popular
    choice in the networks。 and its typically a very good choice for an activationation
    function。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个对大于0的值的线性函数，而对负值则为0。因此，它看起来与线性变换没有太大不同，但实际上它是非线性的，实际上是网络中最流行的选择，通常是激活函数的一个非常好的选择。
- en: So the rule of thumb is if you don't know which function you should use。 then
    just use a relo for hidden layers。Yeah， so this is the relu， very popular choice。
    Then we also have the leaky relu function。 So this is a slightly modified and
    slightly improved version of the relu。 So this will still just output the input
    for x greater than 0。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一条经验法则是，如果你不知道应该使用哪个函数，那么只需在隐藏层中使用ReLU。是的，这就是ReLU，非常受欢迎的选择。然后我们还有泄漏ReLU函数。这是ReLU的稍微修改和改进版本。因此，对于大于0的x，它仍然只输出输入。
- en: but this will multiply our input with a very small value for negative numbers。
    So here I've written a times x for negative numbers。 and this a is typically very
    small。 So it's。 for example，0001。And this is an improved version of the relo that
    tries to solve the so called vanish ingredient problem。Because with a normal relu，
    our values here are 0。 And this means that also the gradient later in the back
    propagation is 0。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这会对负数的输入乘以一个非常小的值。因此，我在这里为负数写了a乘以x，而这个a通常是非常小的。例如，0.0001。这是ReLU的改进版本，试图解决所谓的消失梯度问题。因为使用正常的ReLU时，我们这里的值为0。这也意味着在反向传播中，梯度也是0。
- en: And when the gradient is 0， then this means that these weights will never be
    updated。 So these neurons won't learn anything。 And we also say that these neurons
    are dead。And this is why sometimes you want to use the leaky relu function。 So
    whenever you notice that your weights won't update during training。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度为0时，这意味着这些权重将永远不会被更新。因此，这些神经元不会学习任何东西。我们也称这些神经元为“死亡”。这就是为什么有时你想使用泄漏ReLU函数的原因。所以每当你注意到在训练过程中权重不会更新时。
- en: then try to use the leaky relu instead of the normal relo。And yeah， then as
    a last function。 I want to show you the softmax function。 and you also should
    already know this because I have a whole tutorial about the soft softmax function。
    So this will just this will basically squash the inputs to be outputs between
    0 and one so that we have a probability as an output。 And this is typically a
    good choice in the last layer of a multi class classification problem。So。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后尝试使用泄漏ReLU代替正常的ReLU。最后，我想向你展示Softmax函数。你也应该已经知道这一点，因为我有一个关于Softmax函数的完整教程。因此，这基本上会将输入压缩为0到1之间的输出，以便我们得到一个概率作为输出。这通常是多类分类问题最后一层的一个不错选择。
- en: yeah， that's the different activationation functions I wanted to show you。 And
    now let's jump to the code and see how we can use them in Pytorch。 So we have
    two options。![](img/e9ab40438781f445672555aa3780e63b_1.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这就是我想展示的不同激活函数。现在让我们跳到代码，看看如何在PyTorch中使用它们。因此，我们有两个选择。![](img/e9ab40438781f445672555aa3780e63b_1.png)
- en: And the first one is to create our functions as N N modules。 So in our network
    in the init function。 first， we define all the layers we want to have。 So here，
    for example， first。 we have a linear layer。And then after that， we want to have
    a relu activationctuaation function。 so we create our relu module here。And we
    can get that from the torch dot and N module。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是将我们的函数创建为NN模块。在我们的网络的初始化函数中，首先定义我们想要的所有层。例如，这里，我们首先有一个线性层。然后，我们想要一个ReLU激活函数，所以我们在这里创建我们的ReLU模块。我们可以从torch的NN模块中获取。
- en: So this contains all the different functions I just showed you。And then we have
    the next layer for here， example， it's the next linear layer。 and then the next
    activationation function。 So here we have a sigoidid at the end。And then in the
    forward pass， we simply call all these functions after each other。 So first。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这包含了我刚才展示的所有不同函数。接下来我们有这里的下一层，例如，下一层线性层。然后是下一个激活函数。在这里，我们在最后有一个Sigmoid函数。在前向传播中，我们简单地依次调用所有这些函数。因此，首先。
- en: we have the linear， the first linear layer， which gets an output。 And then we
    use this output and put it into our relo。 And then again。 we use this output and
    put it in the next linear layer and so on。So this is the first way how we can
    use it。 and the second way is to use these functions directly。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有线性，第一层线性，得到一个输出。然后我们使用这个输出并将其放入我们的relo中。接着，我们再次使用这个输出，将其放入下一个线性层，依此类推。这是我们可以使用它的第一种方式，而第二种方式是直接使用这些函数。
- en: So in the init function， we only define our linear layer。 So linear 1 and linear
    2 and then in the forward pass we apply this linear layer and then also call this
    torch dot relu function here and then the torch dot seeoidoid function directly。
    So this is just from the torch API。And yeah， this is a different way how we can
    use it。 Both ways will achieve the same thing。 It's just。How you prefer your code。
    And yeah。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在init函数中，我们只定义我们的线性层。所以线性1和线性2，然后在前向传递中，我们应用这个线性层，然后也在这里调用torch dot relu函数，接着是torch
    dot sigmoid函数。所以这只是来自torch API的不同使用方式。没错，这两种方式将实现相同的效果。只是你喜欢你的代码方式而已。没错。
- en: so all the functions that I just showed you， you can get from the N N module。
    So here we have an N relu， but we can， for example。 also have an N dot sigmoid
    and we have an N dot soft marks， and we have an N dot ton H。嗯。And also。 N and
    dot leaky relu。 So all these functions are available here。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所有我刚刚给你展示的函数，你可以从N N模块中获得。所以这里我们有N relu，但我们也可以，比如说，拥有N dot sigmoid，还有N dot soft
    marks，以及N dot ton H。嗯。还有，N和dot leaky relu。所以所有这些函数在这里都是可用的。
- en: And they are also available in the torch API like this。 So here we have torch
    dot relo。 and we have torch dot sigmoid。 We also have torch dot soft max。And torch
    dot1n H。And but sometimes they are not used in the the functions are not available
    in the torch API directly。 but they are available in torch do n n dot functional。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在torch API中也可以这样使用。所以在这里我们有torch dot relo，还有torch dot sigmoid。我们也有torch dot
    soft max和torch dot1n H。但有时这些函数在torch API中不可用，但它们在torch do n n dot functional中是可以找到的。
- en: So here I import a torch and n functional as F。 and then I can call here， for
    example， F dot Relu。 So this is the same as torch dot relu。 but here， for example。
    is the torch is F dot leaky relu is only available in this API。So， yeah。 but that's
    how we can use the activation functions and pytorrch。 And it's actually very easy。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我导入了torch和n functional作为F。然后我可以在这里调用，比如说，F dot Relu。这和torch dot relu是一样的。但这里，比如说，torch的F
    dot leaky relu只在这个API中可用。所以，没错。这就是我们如何使用激活函数和pytorch。而且这实际上非常简单。
- en: And I hope you understood everything and now feel comfortable with activationation
    functions。 If you like this， please subscribe to the channel and see you next
    time by。![](img/e9ab40438781f445672555aa3780e63b_3.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能理解所有内容，现在对激活函数感到舒适。如果你喜欢这个，请订阅频道，我们下次见！
