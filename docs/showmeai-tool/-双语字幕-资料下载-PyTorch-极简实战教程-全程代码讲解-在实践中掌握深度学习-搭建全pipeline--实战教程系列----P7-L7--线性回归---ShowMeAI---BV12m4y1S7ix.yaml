- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P7ÔºöL7- Á∫øÊÄßÂõûÂΩí
    - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome back to a new Pytarch tutorial„ÄÇ This timeÔºå we implement
    linear regression„ÄÇ So we already implemented this step by step in the last couple
    of tutorials„ÄÇ and this should be a repetition where we can apply all the learned
    concepts and quickly implement our algorithm again„ÄÇüòäÔºåSo as I've shown you beforeÔºå
    our typical pieytorch pipeline consists of those three steps„ÄÇFirst„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we design our model„ÄÇ So we define the input and the output sizeÔºå and then the
    forward pass„ÄÇ Then we create our loss and optimizer functions„ÄÇ And then we do
    the actual training loop with the forward pass„ÄÇ the backward pass and the weight
    updates„ÄÇ So let's do this„ÄÇAnd first of all„ÄÇ we import a couple of things that
    we need„ÄÇ So let's import torch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we import torch dot and N S and N„ÄÇ So the neural network module„ÄÇ Then we
    import nuy S and P just to make some data transformations„ÄÇ and then from S K learn„ÄÇ
    we import data set„ÄÇ So we want to generate a regression data set„ÄÇAnd then we also
    want to plot this later„ÄÇ So I say import matplot clip the pie plot as P LT„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we do our three steps„ÄÇ So we design the modelÔºå step number one„ÄÇThen
    step number two„ÄÇ we define the loss and the optimizer„ÄÇAnd then step number 3Ôºå
    our training loop„ÄÇ So let's do this„ÄÇ And first of allÔºå let's do a step 0 where
    we prepare our data„ÄÇ So prepare data„ÄÇ So let's generate a regression data set„ÄÇ
    and we can do this by saying let's call this x nuy and y nuy equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we can use data sets dot make regression„ÄÇ which getsÔºå let's say 100
    samples„ÄÇ So n samples equals 100„ÄÇ and only one feature in this example„ÄÇ So n features
    equals  one„ÄÇ Then we add some noise„ÄÇ and let's also at a random state„ÄÇ Let's say
    this is one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_3.png)'
  prefs: []
  type: TYPE_IMG
- en: And then we want to convert this to a torch tenzoa„ÄÇ So we say x equals„ÄÇ and
    then we can use the function torch dot from numy„ÄÇ and then we say x dot„ÄÇX underscore
    nuy„ÄÇBut we want to convert this to a„ÄÇA float 32 data data type before„ÄÇ So right
    now this is a double data type„ÄÇ So if we use a double here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we will run into some arrows later„ÄÇ So let's just convert this by saying
    S type and then say nuy dot float 32 and we do the same thing for our y„ÄÇ So we
    say y equals the torch tenzo from our nuy array„ÄÇAnd now let's also reshape our
    y„ÄÇ because right now this is a has only one rowÔºå and we want to make it a column
    vector„ÄÇ So we want to put each value in one rowÔºå and the whole shape has only
    one columnÔºå so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let's say y equals y dot view„ÄÇ And here we put in the new size So y dot shape0„ÄÇ
    So the number of of values„ÄÇ and then only one column„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_6.png)'
  prefs: []
  type: TYPE_IMG
- en: So the few method is a built in pi torch methodÔºå which will reshape our tenone„ÄÇAnd
    then let's get the number of samples and the number of features by saying this
    is x dot shape„ÄÇ So we can use this in a second„ÄÇ And now let's do our three steps„ÄÇ
    So now we have the data„ÄÇ Now we define the model„ÄÇAnd in the linear regression
    caseÔºå this is just one layer„ÄÇ So our„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we can use to build in linear model„ÄÇ So we say model equals N N dot linear„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_8.png)
  prefs: []
  type: TYPE_NORMAL
- en: This is the linear layerÔºå which needs a input size of our features and a output
    size„ÄÇ So let's say input size equals„ÄÇ This is the number of features we have„ÄÇ
    So this is just one in our example„ÄÇ and the output size equals one„ÄÇ So we only
    want to have one value for each sample that we want to put in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So our model gets now the input and the output size So input size and output
    size„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_10.png)
  prefs: []
  type: TYPE_NORMAL
- en: And this is all we have to do to set up the model„ÄÇAnd now let's continue with
    the loss and the optimizer„ÄÇ So let's call this criterion„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_12.png)
  prefs: []
  type: TYPE_NORMAL
- en: And here we can use a built in loss function from Pytorch„ÄÇ And in the case of
    linear regression„ÄÇ this is the mean squared error„ÄÇ So we can say this is N N dot
    M S E loss„ÄÇ So will calculate the mean squared error„ÄÇ So this is a callable function„ÄÇAnd
    then we also set up the optimizers„ÄÇ So we say„ÄÇOpttimizer equals„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And let's say torch dot optim dot S G D„ÄÇ So this is stochastic gradient descent„ÄÇ
    and our optr needs the parameters that it should optimize„ÄÇ So here we can simply
    say this is model dot parameters„ÄÇ And then it needs a learning rate„ÄÇ So let's
    define this here as a variable„ÄÇ So let's say learning rate equals„ÄÇ let's say 001„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_14.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_15.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_16.png)'
  prefs: []
  type: TYPE_IMG
- en: And then Lr equals learning rate„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: So this is step number 2„ÄÇ And now let's do our training loop„ÄÇ SoÔºå first of all„ÄÇ
    let's define the number of epochs„ÄÇLet's say we want to do 100 training iterations„ÄÇ
    and now for„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_20.png)
  prefs: []
  type: TYPE_NORMAL
- en: Epoch in range epochs„ÄÇ And now here we do our steps in the training loopÔºå the
    forward pass„ÄÇ the backward pass and the update and the weight updates„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_23.png)'
  prefs: []
  type: TYPE_IMG
- en: So let'sÔºå first of allÔºå do the forward pass and also the loss here„ÄÇThenÔºå the
    backward pass„ÄÇAnd then the update„ÄÇ So the forward pass and the loss hereÔºå we can
    sayÔºå why predict it„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_25.png)
  prefs: []
  type: TYPE_NORMAL
- en: EqualsÔºå and here we call our model„ÄÇ And as a dataÔºå it gets x„ÄÇ So this is the
    forward pass„ÄÇ And then we compute the loss by saying loss equals„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_27.png)
  prefs: []
  type: TYPE_NORMAL
- en: This is our cr we call this criterion„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_29.png)
  prefs: []
  type: TYPE_NORMAL
- en: And this needs the actual labels and the predicted values„ÄÇ So why predict and
    why„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_31.png)
  prefs: []
  type: TYPE_NORMAL
- en: And nowÔºå in the backward pass to calculate the gradientsÔºå we just say lost dot
    backward„ÄÇ So this will do the back propagation and calculate the gradients for
    us„ÄÇAnd then our update here„ÄÇ we simply say optimizer dot step„ÄÇ So this will update
    the weights„ÄÇAnd then before the next iterationÔºå we have to be careful„ÄÇ So we have
    to empty our gradients now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because whenever we call the backward functionÔºå this will sum up the gradients
    into the dot Gr attribute„ÄÇ So now we want to empty this again„ÄÇ And we simply say
    optimizer dot0 gra„ÄÇ So you should never forget this„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_33.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_34.png)'
  prefs: []
  type: TYPE_IMG
- en: And then we are done with the draining loop„ÄÇ Let's also print some information„ÄÇ
    So let's say if„ÄÇEpoch plus 1 modular1s equals equals 0„ÄÇ So every 10th stepÔºå we
    want to print some information„ÄÇ So let's print the epoch„ÄÇ And here we say epoch
    plus one„ÄÇ and let's also print the loss„ÄÇ the loss equals„ÄÇ And here we can can
    say loss dot item„ÄÇ and let's format this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's plot or print only for decimal values„ÄÇü§¢ÔºåSo now we are done„ÄÇ and now
    let's also plot this„ÄÇ so„ÄÇLet's sayÔºå let's get all the predicted values by saying
    predicted equals here„ÄÇ we call our final model nowÔºå model X„ÄÇAnd with all the data„ÄÇAnd
    now we want to convert this to nu pipe back againÔºå but before we do that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we want to detach our tenzoorÔºå so we want to prevent this operation from being
    tracked in our graph and our computational graph„ÄÇBecause right nowÔºå this tenor„ÄÇHere
    I have a typo predicted„ÄÇ So this tenor has the required gradients argument set
    to true„ÄÇ But now we want this to fall to be false„ÄÇ So this will generate a new
    tenzor where our gradient calculation attribute is false„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is our new tenzo„ÄÇ And then we just call the numpy function„ÄÇ Now we convert
    it to numpy and now plot this„ÄÇ So let's say first plot all our data„ÄÇSo„ÄÇ x nuy
    and y nuy„ÄÇAnd we want to plot this asÔºå let's say red dots„ÄÇ And then we want to
    plot our generated or approximated function„ÄÇ So let's say PÔºå L T dot plot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: X nuy on the X axis and our predicted labels on the Y axis„ÄÇAnd let's plot this
    in blue„ÄÇ And then we say P L T dot show„ÄÇ And now let's run this and hope that
    everything is correct„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_36.png)
  prefs: []
  type: TYPE_NORMAL
- en: And now this plot appears here„ÄÇ So now we see that we have a pretty good approximation
    of our data with this line„ÄÇ and we see that this is working„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_38.png)
  prefs: []
  type: TYPE_NORMAL
- en: And yeahÔºå now were done„ÄÇ I hope you enjoyed this„ÄÇ If you like this„ÄÇ please subscribe
    to the channel and see you next timeÔºå bye„ÄÇüòä„ÄÇ![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_40.png)
  prefs: []
  type: TYPE_NORMAL
