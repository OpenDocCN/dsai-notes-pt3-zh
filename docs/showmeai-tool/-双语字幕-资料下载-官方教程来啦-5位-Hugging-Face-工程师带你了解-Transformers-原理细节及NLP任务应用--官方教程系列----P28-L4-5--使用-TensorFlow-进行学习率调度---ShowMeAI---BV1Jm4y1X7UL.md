# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P28ï¼šL4.5- ä½¿ç”¨ TensorFlow è¿›è¡Œå­¦ä¹ ç‡è°ƒåº¦ - ShowMeAI - BV1Jm4y1X7UL

In our other videosï¼Œ we talked about the basics of fine tuning a language model with Tensorflowã€‚ And as alwaysï¼Œ when I refer to videosï¼Œ I'll link them belowã€‚ But stillï¼Œ can we do betterã€‚ So here's the code from our model fine tuning videoã€‚ And whileï¼Œ while it worksã€‚ we could definitely tweak a couple of thingsã€‚ by farã€‚

 the most important thing is the learning w In this videoï¼Œ we'll talk about how to change itã€‚ which will make your training much more consistently successfulã€‚ğŸ˜Šï¼ŒIn factã€‚ there are two things we want to change about the default learning rate for atã€‚ The first is that it's way too high for our modelsã€‚ So by defaultã€‚

 Adam uses a learning rate of 10 to the -3ï¼Œ which is very high for training transformersã€‚ We're going to start at 5 by 10 to the-5ï¼Œ which is 20 times lower than the defaultã€‚ And secondlyã€‚ we don't just want a constant learning rateã€‚ We can get even better performance if we decay the learning rate down to a tiny valueã€‚ or even to 0 over the course of trainingã€‚ So that's what this polynomial decay schedule thing is doingã€‚

 So that name might be intimidatingï¼Œ especially if you only vaguely remember what a polynomial is from Atslasã€‚ So I'll show you what that decay looks like in a secondã€‚ But first we need to tell the scheduler how long training is going to beã€‚ So that it decays of the right speedï¼Œ And that's what this code here is doingã€‚ğŸ˜Šã€‚

So we're computing how many mini batches the model is going to see over the entire training run and to compute thatã€‚ we're taking the size of the training setï¼Œ dividing it by the batch size which gives us the number of batches per epochã€‚ and then we're multiplying that by the number of epochs to get a total number of batchesã€‚ it's going to see over the whole training runã€‚ğŸ˜Šï¼ŒSo once we know how many batchesã€‚

 how many training steps we're takingï¼Œ we just pass all of that information to the scheduler and we're ready to goã€‚So what does the polynomial decay schedule look likeï¼ŸWith default optionsã€‚ it's actually just a linear scheduleï¼Œ so it looks like thisã€‚ It starts at our initial valueã€‚ which is 5 by 10 to the minus5 or 5 e minus5 and then it decays down at a constant rate until it hits zero right at the very end of training So why do they call it polynomial and not linear Well if you tweak the options you can get a higher order at a truly polynomial decay schedule but there's no need to do that right now by defaultã€‚

 you get a linear schedule and if you were aware that a linear function is a special case of a polynomial you can feel proudã€‚ğŸ˜Šï¼ŒSo that asideï¼Œ how do we actually use the schedulerã€‚ So easilyï¼Œ we just pass it to Adamã€‚ You'll notice the first time when we compiled the modelï¼Œ we just pass the string atã€‚ Curris recognizes the names of common optimizers and loss functions if you pass them with strings So it saves time and it avoids imports to do it that wayã€‚

 if you only want the default settingsã€‚ but we're professional machine learners now with our very own learning rate schedule So we have to do things properlyã€‚ So the first thing we do is we import the optimizerã€‚ Then we initialize it with our scheduler in the learning rate argumentã€‚ and then we compile the model using our new optimizer and whatever loss function you wantã€‚

 we'll leave that unchangedã€‚ This will be sparse categorical cross entropyã€‚ if you're following long from the fine tuning videoã€‚ but it can be anything else that you're using yourselfã€‚ğŸ˜Šã€‚So now we have a high performance model ready to goã€‚ all that remains is to fit the model just like we did beforeã€‚And remember because we've compiled the model with the new optimizer at the new learning rateã€‚ we actually don't need to change anything about the fit I call at allã€‚ we just call fit here exactly the same command we used before if you've seen in other videosã€‚ğŸ˜Šã€‚

But now we get a beautiful training with a niceï¼Œ smoothã€‚ a good initial learning rate and a solid learning rate decayingã€‚And you will get much better performance as a resultã€‚![](img/23f9dd7967986f28f1dcc38c80344a1b_1.png)

ã€‚

![](img/23f9dd7967986f28f1dcc38c80344a1b_3.png)