- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P8ï¼šL2.1- ç®¡é“å‡½æ•°å†…éƒ¨ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ(PyTorch) - ShowMeAI - BV1Jm4y1X7UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What happens inside the by functionï¼ŸIn this videoï¼Œ we'll look at what actually
    happens when we use the pipeline function of the Transforms libraryã€‚Now specificallyï¼Œ
    well look at the sentiment analysis pipeline and then it went from the two following
    sentences so the positive and negative labels were respective scoresã€‚
  prefs: []
  type: TYPE_NORMAL
- en: As we've seen in the pipeline plan presentationã€‚There are three stages in the
    pipelineã€‚Firstã€‚ we convert a verex to numbers the model can make signs of using
    a tokenizerã€‚Then those numbers go through the model which outputs lowsã€‚Finallyã€‚
    the first processing steps transform Vo gate into labels and scoresã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Let's look in details at those three steps and how to replicate their music
    the Transformerss libraryã€‚ beginning with the first stage tokenizationã€‚So organization
    process has several steps firstã€‚ the text is split into small chunks called tokensã€‚They
    can be wordsï¼Œ part of wordsã€‚ or punctuation symbolsã€‚Then the tokenizer will add
    some special tokens if the model expectedã€‚Hereã€‚
  prefs: []
  type: TYPE_NORMAL
- en: the model used expect a seal token at the beginning and a sep token at the end
    of the sentence to classifyã€‚Lastlyï¼Œ the tokenazer patches each token to its unique
    ID in the vocabulary of the portrayed modelã€‚To load such a tokenizerï¼Œ the transformformers
    library provides the Utokenizer APIã€‚The most important method of this class is
    from Pretrainedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which will download and cache the configuration and the vocabulary associated
    to a given checkpointã€‚Hereï¼Œ the checkpoint used by default for the sentiment analysis
    pipeline is distber based case fine tuned SS2 Englishã€‚ which is a bit of a mouthfulã€‚We
    instant to tookken associated with a checkpointã€‚ and feed it to the two sentencesã€‚Since
    the two sentences are not of the same sizeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: well need to pad the shest one to be able to build an arrayã€‚This is done by
    the tokenizer with the option padding equal trueã€‚With truation equal2ã€‚ we ensure
    that any sentence longer and the maximum the middle can handle is truncatedã€‚Lastlyã€‚
    the return tensil option tells the tokenizer to return the byytch tensilã€‚Looking
    at a resultã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we see we have a dictionary with two keysï¼Œ input ID contains the ideas of both
    sentences with zero where the padding is appliedã€‚The second key attention mask
    indicates where patting has been appliedã€‚ so the model does not pay attention
    to itã€‚This is all what is inside the token stepã€‚Now let's have a look at the second
    stepã€‚ä¸‰ã‚‚ã©ã€‚As also to anã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there is a notomod API with from prettrain methodã€‚ it would download and cache
    the configuration of the modelï¼Œ as well as the pretrain weightã€‚Howeverã€‚ the Automod
    API will only instantiate the body of the modelã€‚ that is the part of the model
    that is left once the pro traininging head is removedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: It will output a high dimensional tensorï¼Œ that is a representation of the sentence's
    pastã€‚ but which is not directly useful for our classification programã€‚Hereï¼Œ the
    tensor has two sentencesã€‚ each of 16 tokensï¼Œ and the last dimension is the Indian
    size of our modelï¼Œ 768ã€‚To get an output linked to our classification problemã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we need to use the Automodal for sequence classificationification classã€‚It works
    exactly as you to model classï¼Œ except that12 build a model with a classification
    headã€‚ğŸ˜Šã€‚Praise one auto class for each common NLP task in the transformformers
    libraryã€‚Hereã€‚ after giving all models of two sentencesã€‚We get a tensor of size
    2 by2ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: one result for each sentence and for each possible levelã€‚Those outputs are not
    probabilities yetã€‚ we can see they don't sum to oneã€‚This is because each model
    of the transformformers's library returns look itã€‚To make sense of look itï¼Œ we
    need to dig into the third and last step of the pipelineã€‚Plus processingã€‚To conduct
    Lo into probabilitiesï¼Œ we need to apply a softmax layers to themã€‚
  prefs: []
  type: TYPE_NORMAL
- en: As we can seeï¼Œ this transforms them into positive number that's a up to1ã€‚The
    last step is to know which of those corresponds to the positive of the negative
    labelã€‚This is given by the I2lipol field of the model congã€‚The first probabilityba
    is index0ã€‚ correspond to the negative level and the seconds index1 correspond
    to the positive levelã€‚
  prefs: []
  type: TYPE_NORMAL
- en: This is how our classifier built with the pipeline function pickeded with labels
    and compute those scoresã€‚ğŸ˜Šï¼ŒNow that you know how each step worksï¼Œ you can easily
    tweak them to your needsã€‚![](img/be1ca15e381cec8388260069320209cf_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be1ca15e381cec8388260069320209cf_2.png)'
  prefs: []
  type: TYPE_IMG
- en: ã€‚
  prefs: []
  type: TYPE_NORMAL
