- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P31ï¼šL5.5- boostrappingä¸åŸºå‡†è¶…å‚æ•°
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P31ï¼šL5.5- bootstrappingä¸åŸºå‡†è¶…å‚æ•°
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ welcomel to applications of deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ we're going to talk about L1 L2 and dropout
    togetherã€‚ You might be thinking greatã€‚ I had to figure out how many layers to
    have in a neural networkã€‚ how many neurons to put in each of those layersã€‚ Nowã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ä¸€èµ·è®¨è®ºL1ã€L2å’Œdropoutã€‚ä½ å¯èƒ½åœ¨æƒ³å¤ªå¥½äº†ã€‚æˆ‘å¾—å¼„æ¸…æ¥šç¥ç»ç½‘ç»œåº”è¯¥æœ‰å¤šå°‘å±‚ï¼Œæ¯ä¸€å±‚åº”è¯¥æ”¾å¤šå°‘ä¸ªç¥ç»å…ƒã€‚ç°åœ¨ã€‚
- en: I've got all these regularization techniques to figure out as wellã€‚ It's getting
    complicated to architect and neural networkã€‚ Later in this course we'll see that
    we can use Bayesian optimization to help us figure this outã€‚ But for nowï¼Œ I'm
    going to give you some practical tips in this video for the latest on my AI course
    and projectsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜å¾—ææ¸…æ¥šè¿™äº›æ­£åˆ™åŒ–æŠ€å·§ã€‚è®¾è®¡ç¥ç»ç½‘ç»œå˜å¾—å¤æ‚äº†ã€‚åœ¨æœ¬è¯¾ç¨‹åé¢çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ¥å¸®åŠ©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½†ç°åœ¨ï¼Œæˆ‘ä¼šåœ¨è¿™ä¸ªè§†é¢‘ä¸­ç»™ä½ ä¸€äº›å…³äºæˆ‘çš„AIè¯¾ç¨‹å’Œé¡¹ç›®çš„å®ç”¨æç¤ºã€‚
- en: click subscribe in the bell next to it to be notified of every new videoã€‚ So
    we've vet on quite a few hyperpara as we've made our way through this course up
    to this pointã€‚ğŸ˜Šã€‚![](img/7030d8ee32d0baa28197ef0d0c7cdbba_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»æ—è¾¹çš„é“ƒé“›è®¢é˜…ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åœ¨è¿™é—¨è¯¾ç¨‹ä¸­å·²ç»å®¡æŸ¥äº†ä¸å°‘è¶…å‚æ•°ã€‚ğŸ˜Šã€‚![](img/7030d8ee32d0baa28197ef0d0c7cdbba_1.png)
- en: There's the number of layers in a neural networkï¼Œ adjusting these hyperparameters
    can definitely have an impact on the performance of your neural networkã€‚There's
    also how many neurons you have in each layerã€‚ there's what activation functions
    you're using on each layerã€‚Now we've added dropout percent per layer and the L1
    and L2 regularization values per layerã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„å±‚æ•°ï¼Œè°ƒæ•´è¿™äº›è¶…å‚æ•°ç¡®å®ä¼šå½±å“ä½ çš„ç¥ç»ç½‘ç»œçš„æ€§èƒ½ã€‚æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡ï¼Œä»¥åŠä½ åœ¨æ¯å±‚ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»åœ¨æ¯å±‚æ·»åŠ äº†dropoutæ¯”ä¾‹å’ŒL1ã€L2æ­£åˆ™åŒ–å€¼ã€‚
- en: And you know what we're not done yetã€‚ We're going to add even more on as we
    make our way through the rest of the courseã€‚ These are a lot of hyperparametersï¼Œ
    and it can be difficult to determine exactly how you should set theseã€‚In this
    videoï¼Œ I'm not going to show you any techniques for particularly optimizing any
    of these hyperparameterã€‚ but I am going to show you a way that you can evaluate
    what effect your changes are actually havingã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”ä½ çŸ¥é“å—ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰ç»“æŸã€‚éšç€æˆ‘ä»¬ç»§ç»­è¿›è¡Œè¯¾ç¨‹ï¼Œæˆ‘ä»¬ä¼šæ·»åŠ æ›´å¤šå†…å®¹ã€‚è¿™äº›æ˜¯å¾ˆå¤šè¶…å‚æ•°ï¼Œç¡®å®šåº”è¯¥å¦‚ä½•è®¾ç½®å®ƒä»¬å¯èƒ½ä¼šå¾ˆå›°éš¾ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä¸ä¼šå‘ä½ å±•ç¤ºä¼˜åŒ–è¿™äº›è¶…å‚æ•°çš„å…·ä½“æŠ€å·§ï¼Œä½†æˆ‘ä¼šå‘Šè¯‰ä½ å¦‚ä½•è¯„ä¼°ä½ çš„å˜åŒ–å®é™…ä¸Šäº§ç”Ÿäº†ä»€ä¹ˆå½±å“ã€‚
- en: As you saw in previous parts of this module leading up to this pointã€‚ if you
    rerun the neural network a couple of timesï¼Œ even with a fivefold cross validationã€‚
    you're going to get various results from your final accuracy RMSEã€‚ depending on
    if your classification or regressionã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ åœ¨è¿™ä¸ªæ¨¡å—ä¹‹å‰çš„éƒ¨åˆ†çœ‹åˆ°çš„ï¼Œå¦‚æœä½ é‡æ–°è¿è¡Œç¥ç»ç½‘ç»œå‡ æ¬¡ï¼Œå³ä½¿ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯ï¼Œä½ æœ€ç»ˆçš„å‡†ç¡®ç‡RMSEä¹Ÿä¼šæœ‰æ‰€ä¸åŒï¼Œè¿™å–å†³äºä½ æ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’ã€‚
- en: this can make it very difficult since your accuracy or your predictive power
    of the neural network somewhat bounces around naturally before you even change
    anythingã€‚ it can be very difficult to know if the change that you just made to
    one of these hyperparameters has actually had an effect or if you're just seeing
    normal variation in the neural network as a result of the random weights that
    that neural network starts with in this sectionã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½ä¼šè®©äº‹æƒ…å˜å¾—éå¸¸å›°éš¾ï¼Œå› ä¸ºåœ¨ä½ è¿›è¡Œä»»ä½•æ›´æ”¹ä¹‹å‰ï¼Œç¥ç»ç½‘ç»œçš„å‡†ç¡®ç‡æˆ–é¢„æµ‹èƒ½åŠ›åœ¨è‡ªç„¶æ³¢åŠ¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯èƒ½å¾ˆéš¾çŸ¥é“ä½ åˆšåˆšå¯¹å…¶ä¸­ä¸€ä¸ªè¶…å‚æ•°æ‰€åšçš„æ›´æ”¹æ˜¯å¦çœŸçš„äº§ç”Ÿäº†æ•ˆæœï¼Œè¿˜æ˜¯åªæ˜¯çœ‹åˆ°äº†ç¥ç»ç½‘ç»œç”±äºéšæœºæƒé‡è€Œå¯¼è‡´çš„æ­£å¸¸å˜å¼‚ã€‚
- en: we're going to look at something called bootstping Nowã€‚ bootstping is similar
    to cross validationation in that it is a technique that you can use to get trainingã€‚ã¯ã„ã€‚And
    validation sets to work with your neural network onã€‚ But it's different because
    it's not a set number of foldsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¦çœ‹çœ‹ä¸€ä¸ªå«åšbootstrappingçš„ä¸œè¥¿ã€‚bootstrappingç±»ä¼¼äºäº¤å‰éªŒè¯ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ç§å¯ä»¥ç”¨æ¥è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†ä»¥ä¾¿ä¸ä½ çš„ç¥ç»ç½‘ç»œä¸€èµ·å·¥ä½œçš„æŠ€æœ¯ã€‚ä½†å®ƒçš„ä¸åŒä¹‹å¤„åœ¨äºå®ƒä¸æ˜¯å›ºå®šçš„æŠ˜å æ•°ã€‚
- en: You simply keep re grabbing a training and validation set from your initial
    data set over and over and over againã€‚ and you do this with replacementã€‚ Nowï¼Œ
    when you say with replacementã€‚ that means that the first time you pull these elements
    out of the data setã€‚ you put them all right back in and the next time you pull
    from the same groupã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªéœ€ä¸æ–­ä»åˆå§‹æ•°æ®é›†ä¸­é‡æ–°è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚å¹¶ä¸”è¿™æ˜¯åœ¨æœ‰æ”¾å›çš„æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚ç°åœ¨ï¼Œå½“ä½ è¯´â€œæœ‰æ”¾å›â€æ—¶ï¼Œè¿™æ„å‘³ç€ä½ ç¬¬ä¸€æ¬¡ä»æ•°æ®é›†ä¸­æå–è¿™äº›å…ƒç´ åï¼Œå°†å®ƒä»¬å…¨éƒ¨æ”¾å›ï¼Œç„¶åä¸‹ä¸€æ¬¡ä½ ä»åŒä¸€ç»„ä¸­æå–ã€‚
- en: So you could end up with exactly the same training and validation set on multiple
    polls fromã€‚From your datasetï¼Œ But that's fineã€‚ we are going to average all of
    the accuracy or rooting square errors together to get an average performance for
    the neural network and will simply keep pulling more and more and more runs from
    the data set as we keep going and we'll look at how many runsã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯èƒ½ä¼šåœ¨å¤šæ¬¡æŠ½æ ·ä¸­å¾—åˆ°å®Œå…¨ç›¸åŒçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚ä½†è¿™æ²¡å…³ç³»ã€‚æˆ‘ä»¬å°†æŠŠæ‰€æœ‰çš„å‡†ç¡®ç‡æˆ–å‡æ–¹æ ¹è¯¯å·®å¹³å‡åœ¨ä¸€èµ·ï¼Œä»¥è·å¾—ç¥ç»ç½‘ç»œçš„å¹³å‡æ€§èƒ½ï¼Œéšç€æˆ‘ä»¬ä¸æ–­è¿›è¡Œï¼Œæˆ‘ä»¬å°†ç»§ç»­ä»æ•°æ®é›†ä¸­æŠ½å–æ›´å¤šçš„è¿è¡Œï¼Œå¹¶æŸ¥çœ‹æˆ‘ä»¬è¿›è¡Œäº†å¤šå°‘æ¬¡è¿è¡Œã€‚
- en: how many splits of your data it takes before your average error starts to really
    converge to a consistent valueã€‚ We're also going to use early stopping so we will
    stop training the neural network when the validation set no longer improvesã€‚ This
    will also allow us to report on an average number of epochs that is needed for
    this particular data and neural network so that you can start to get an idea of
    how many epochs you should really train with Nowã€‚ since we are using this for
    benchmarkingã€‚We are going to want to report the time that certain things took
    because we might want to run this on Google CoLab or some other cloud based resource
    that will let us have more compute powerã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦å¯¹æ•°æ®è¿›è¡Œå¤šå°‘æ¬¡æ‹†åˆ†ï¼Œå¹³å‡è¯¯å·®æ‰ä¼šçœŸæ­£æ”¶æ•›åˆ°ä¸€ä¸ªä¸€è‡´çš„å€¼ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨æå‰åœæ­¢ï¼Œå› æ­¤å½“éªŒè¯é›†ä¸å†æ”¹å–„æ—¶ï¼Œæˆ‘ä»¬å°†åœæ­¢è®­ç»ƒç¥ç»ç½‘ç»œã€‚è¿™è¿˜å°†å…è®¸æˆ‘ä»¬æŠ¥å‘Šæ‰€éœ€çš„å¹³å‡è½®æ¬¡ï¼Œä»¥ä¾¿ä½ å¯ä»¥å¼€å§‹äº†è§£åº”è¯¥è®­ç»ƒå¤šå°‘è½®ã€‚ç°åœ¨ï¼Œç”±äºæˆ‘ä»¬åœ¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬éœ€è¦æŠ¥å‘ŠæŸäº›æ“ä½œæ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½æƒ³åœ¨Google
    CoLabæˆ–å…¶ä»–å…è®¸æˆ‘ä»¬è·å¾—æ›´å¤šè®¡ç®—èƒ½åŠ›çš„äº‘èµ„æºä¸Šè¿è¡Œè¿™ä¸ªã€‚
- en: I'm just going to define that functionã€‚ It's easy enoughã€‚ Let's look at how
    we'll bootstrap for regressionã€‚ So I'm going to show you a regression and a classification
    example of bootstrapping firstã€‚ and then we'll look at the actual benchmarking
    program for thisï¼Œ since its regressionã€‚ we are attempting to predict the ageã€‚This
    is the simple dataset set that we've used a number of times throughout this courseã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯å®šä¹‰é‚£ä¸ªå‡½æ•°ã€‚å¾ˆç®€å•ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è¿›è¡Œå›å½’çš„è‡ªä¸¾ã€‚æ‰€ä»¥æˆ‘ä¼šå…ˆå‘ä½ å±•ç¤ºä¸€ä¸ªå›å½’å’Œä¸€ä¸ªåˆ†ç±»çš„è‡ªä¸¾ç¤ºä¾‹ï¼Œç„¶åæˆ‘ä»¬å†çœ‹çœ‹è¿™ä¸ªçš„å®é™…åŸºå‡†æµ‹è¯•ç¨‹åºï¼Œå› ä¸ºè¿™æ˜¯å›å½’ã€‚æˆ‘ä»¬è¯•å›¾é¢„æµ‹å¹´é¾„ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­ä½¿ç”¨è¿‡å¤šæ¬¡çš„ç®€å•æ•°æ®é›†ã€‚
- en: I'll go ahead and run thatã€‚ It'll simply load itï¼Œ and it's done loadingã€‚ Now
    I'm going to bootstrap itã€‚ So let me just go ahead and tick this off because it
    takes it a little while to runã€‚ and start to explain it Hereï¼Œ we're defining the
    number of splitsã€‚ This is how many runs through thisã€‚ We're going to goã€‚ So I
    chose 50ã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ç»§ç»­è¿è¡Œå®ƒã€‚å®ƒä¼šç®€å•åœ°åŠ è½½ï¼ŒåŠ è½½å®Œæˆäº†ã€‚ç°åœ¨æˆ‘è¦è¿›è¡Œè‡ªä¸¾ã€‚æ‰€ä»¥è®©æˆ‘æŠŠè¿™ä¸ªå‹¾é€‰æ‰ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ç‚¹æ—¶é—´æ¥è¿è¡Œï¼Œå¹¶å¼€å§‹è§£é‡Šã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰æ‹†åˆ†çš„æ•°é‡ã€‚è¿™æ˜¯æˆ‘ä»¬è¦è¿›è¡Œçš„å¤šå°‘æ¬¡è¿è¡Œã€‚æˆ‘é€‰æ‹©äº†50ã€‚
- en: 50 would be a bit big for a cross validation because you would end up with a
    validation setã€‚ There's one 50th of the data set and on smaller data that that
    would potentially be problemã€‚ But here we're doingï¼Œ So we're doing a shuffle splitã€‚Just
    like with cross validationã€‚ we will use a stratified split when we get to classification
    because we don't want to accidentally change the class balances because that would
    introduce bias that would give us potentially an incorrectly trained modelã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 50å¯¹äº¤å‰éªŒè¯æ¥è¯´æœ‰ç‚¹å¤§ï¼Œå› ä¸ºä½ ä¼šå¾—åˆ°ä¸€ä¸ªéªŒè¯é›†ã€‚æ•°æ®é›†ä¸­æœ‰1/50çš„æ ·æœ¬ï¼Œè€Œåœ¨æ›´å°çš„æ•°æ®é›†ä¸Šï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ã€‚ä½†æ˜¯åœ¨è¿™é‡Œæˆ‘ä»¬è¦åšçš„ï¼Œæ˜¯è¿›è¡Œæ´—ç‰Œæ‹†åˆ†ã€‚å°±åƒäº¤å‰éªŒè¯ä¸€æ ·ï¼Œå½“æˆ‘ä»¬è¿›è¡Œåˆ†ç±»æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åˆ†å±‚æ‹†åˆ†ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æƒ³æ„å¤–æ”¹å˜ç±»åˆ«å¹³è¡¡ï¼Œå› ä¸ºé‚£æ ·ä¼šå¼•å…¥åå·®ï¼Œå¯¼è‡´å¯èƒ½è®­ç»ƒå‡ºä¸€ä¸ªä¸æ­£ç¡®çš„æ¨¡å‹ã€‚
- en: at least it would be slightly off because it would have been trained on the
    wrong balancesã€‚ portions of the classes that is to be classifiedã€‚The 0ã€‚1 tells
    me that I am taking 10% of the data set for a validation setã€‚ and we're using
    42 so that we have consistently random results from sampling themã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘ä¼šæœ‰ä¸€ç‚¹åå·®ï¼Œå› ä¸ºå®ƒä¼šåœ¨é”™è¯¯çš„å¹³è¡¡ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¦åˆ†ç±»çš„ç±»åˆ«éƒ¨åˆ†ã€‚0.1å‘Šè¯‰æˆ‘ï¼Œæˆ‘æ­£åœ¨ä»æ•°æ®é›†ä¸­æŠ½å–10%çš„æ ·æœ¬ä½œä¸ºéªŒè¯é›†ã€‚æˆ‘ä»¬ä½¿ç”¨42ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ä»ä¸­é‡‡æ ·å¾—åˆ°çš„ç»“æœæ˜¯ä¸€è‡´éšæœºçš„ã€‚
- en: It's not as important to do a random state for bootstrapping because each time
    you run through itã€‚ I don't know that you necessarily care that split one is going
    to be the same as split one again when you run itã€‚ but in cross validationï¼Œ it
    can be nice to have those consistent foldsã€‚ We're going to split itã€‚ Run through
    with each of these train and testsã€‚And we're going to construct the neural networkã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼•å¯¼æ—¶åšéšæœºçŠ¶æ€å¹¶ä¸æ˜¯é‚£ä¹ˆé‡è¦ï¼Œå› ä¸ºæ¯æ¬¡è¿è¡Œæ—¶ã€‚æˆ‘ä¸çŸ¥é“ä½ æ˜¯å¦çœŸçš„å…³å¿ƒåˆ†å‰²ä¸€åœ¨å†æ¬¡è¿è¡Œæ—¶ä¼šä¸åˆ†å‰²ä¸€ç›¸åŒã€‚ä½†åœ¨äº¤å‰éªŒè¯ä¸­ï¼Œä¿æŒä¸€è‡´çš„æŠ˜å ä¼šæ¯”è¾ƒå¥½ã€‚æˆ‘ä»¬å°†è¿›è¡Œåˆ†å‰²ã€‚ä½¿ç”¨è¿™äº›è®­ç»ƒå’Œæµ‹è¯•è¿›è¡Œè¿è¡Œã€‚æˆ‘ä»¬å°†æ„å»ºç¥ç»ç½‘ç»œã€‚
- en: So these are your hyperparameters that you'd want to be experimenting withã€‚
    You will want to change these to different values and run it and try to get an
    idea of how effective this particular set of hyperparameters areã€‚ We'll look at
    this later in this videoã€‚ we'll see how we actually construct a benchmarkã€‚ we
    are going to use early stoppingã€‚ So since we're using early stopping and our validation
    data are also from the early stopã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›æ˜¯ä½ æƒ³è¦å°è¯•çš„è¶…å‚æ•°ã€‚ä½ éœ€è¦å°†è¿™äº›æ›´æ”¹ä¸ºä¸åŒçš„å€¼å¹¶è¿è¡Œå®ƒï¼Œå°è¯•äº†è§£è¿™ä¸€ç‰¹å®šè¶…å‚æ•°é›†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç¨ååœ¨è§†é¢‘ä¸­ä¼šæŸ¥çœ‹è¿™ä¸ªï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•æ„å»ºåŸºå‡†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ—©æœŸåœæ­¢ã€‚æ‰€ä»¥ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨æ—©æœŸåœæ­¢ï¼Œè€Œæˆ‘ä»¬çš„éªŒè¯æ•°æ®ä¹Ÿæ˜¯æ¥è‡ªæ—©æœŸåœæ­¢ã€‚
- en: we can't use that validation set as a true indicator of the actual effectiveness
    of the neural network because we're using that same validation set both to stop
    the neural network and evaluateã€‚ but it doesn't matterã€‚ we're more looking just
    for relative values of those from run to runã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸èƒ½å°†é‚£ä¸ªéªŒè¯é›†ä½œä¸ºç¥ç»ç½‘ç»œå®é™…æœ‰æ•ˆæ€§çš„çœŸå®æŒ‡æ ‡ï¼Œå› ä¸ºæˆ‘ä»¬åŒæ—¶ä½¿ç”¨åŒä¸€ä¸ªéªŒè¯é›†æ¥åœæ­¢ç¥ç»ç½‘ç»œå’Œè¿›è¡Œè¯„ä¼°ã€‚ä½†è¿™æ— å…³ç´§è¦ã€‚æˆ‘ä»¬æ›´å…³æ³¨çš„æ˜¯ä¸åŒè¿è¡Œä¹‹é—´çš„ç›¸å¯¹å€¼ã€‚
- en: not the true predictive power of this neural networkã€‚ We're simply trying to
    optimize hyperparametersï¼Œ really at thisã€‚So we will fit it on how remaining many
    steps is neededï¼Œ then we will do our predictionã€‚And we'll track the average error
    and also the average number of epochs neededã€‚Apoox epochï¼Œ howeverã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸æ˜¯è¿™ä¸ªç¥ç»ç½‘ç»œçš„çœŸå®é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬åªæ˜¯æƒ³ä¼˜åŒ–è¶…å‚æ•°ï¼Œå®é™…ä¸Šåœ¨è¿™é‡Œã€‚æ‰€ä»¥æˆ‘ä»¬ä¼šæ‹Ÿåˆéœ€è¦çš„å‰©ä½™æ­¥éª¤ï¼Œç„¶åè¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬å°†è·Ÿè¸ªå¹³å‡è¯¯å·®å’Œæ‰€éœ€çš„å¹³å‡çºªå…ƒæ•°ã€‚å¤§çº¦çºªå…ƒï¼Œç„¶è€Œã€‚
- en: You want to pronounce that wordã€‚ and we also track the standard deviation of
    the error because the standard deviation gives us an idea of how much variance
    the particular set of hyperparameters is giving usã€‚ howï¼Œ how much the neural network
    accuracy or R messy varies from one particular run to anotherã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æƒ³è¦å‘éŸ³é‚£ä¸ªè¯ã€‚æˆ‘ä»¬è¿˜è·Ÿè¸ªè¯¯å·®çš„æ ‡å‡†å·®ï¼Œå› ä¸ºæ ‡å‡†å·®è®©æˆ‘ä»¬äº†è§£ç‰¹å®šè¶…å‚æ•°é›†ç»™æˆ‘ä»¬çš„æ–¹å·®æœ‰å¤šå¤§ã€‚ç¥ç»ç½‘ç»œçš„å‡†ç¡®æ€§æˆ–R messyåœ¨ä¸åŒè¿è¡Œä¹‹é—´çš„å˜åŒ–ã€‚
- en: Now you can see hereï¼Œ we've done quite a few splitsã€‚ So at the beginningã€‚We
    got a score of 0ã€‚688ã€‚ And obviouslyï¼Œ that's still the mean score because there's
    only one of themã€‚ No standard deviation on the first one because it didn't deviate
    from anythingã€‚ And we continue to get our scoresã€‚ You'll notice there is quite
    a bit of jumping around Sc equals 0ã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬åšäº†ç›¸å½“å¤šçš„åˆ†å‰²ã€‚åœ¨å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº†0.688çš„åˆ†æ•°ã€‚æ˜¾ç„¶ï¼Œè¿™ä»ç„¶æ˜¯å‡å€¼åˆ†æ•°ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªã€‚ç¬¬ä¸€ä¸ªæ²¡æœ‰æ ‡å‡†å·®ï¼Œå› ä¸ºå®ƒæ²¡æœ‰åç¦»ä»»ä½•å€¼ã€‚æˆ‘ä»¬ç»§ç»­è·å–åˆ†æ•°ã€‚ä½ ä¼šæ³¨æ„åˆ°ï¼Œåˆ†æ•°ç›¸å½“æ³¢åŠ¨ï¼ŒScç­‰äº0ã€‚
- en: 88 score equals 0ã€‚56 and so on and so forthã€‚ We continue this processã€‚ And by
    the time we get to 36ã€‚ which is how far it made itã€‚ We can see the standard deviation
    is is decent for for this oneã€‚ So it's jumping around by about plus or minus 0ã€‚18ã€‚
    And we can see that the mean scoreã€‚ it has somewhat convergeã€‚ It's right around
    74ã€‚ I meanï¼Œ it it's still jumping around in in the 10sã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 88ï¼Œåˆ†æ•°ç­‰äº0.56ç­‰ç­‰ã€‚æˆ‘ä»¬ç»§ç»­è¿™ä¸ªè¿‡ç¨‹ã€‚å½“æˆ‘ä»¬åˆ°è¾¾36æ—¶ï¼Œä¹Ÿå°±æ˜¯å®ƒè¾¾åˆ°çš„ç¨‹åº¦ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ ‡å‡†å·®ç›¸å½“ä¸é”™ã€‚å› æ­¤ï¼Œæ³¢åŠ¨å¤§çº¦åœ¨Â±0.18ä¹‹é—´ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‡åˆ†æ•°ï¼Œå®ƒæœ‰äº›æ”¶æ•›ã€‚å¤§çº¦åœ¨74å·¦å³ï¼Œä»ç„¶åœ¨10çš„èŒƒå›´å†…æ³¢åŠ¨ã€‚
- en: but you can see really how how many of these you want to average together before
    these start to get prettyã€‚ pretty convergentã€‚ The epochã€‚ It looks like somewhere
    around now 113 to 130ã€‚2ã€‚ these are jumping around quite a bitã€‚ but the mean epochs
    really has converged1ï¼Œ1ï¼Œ8ï¼Œ11ï¼Œ7ã€‚ So mean epochï¼Œ it looks like 117 to 118ã€‚ That's
    the number of epochs to really be training this thing onã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä½ å¯ä»¥çœŸæ­£çœ‹åˆ°ï¼Œåœ¨è¿™äº›å¼€å§‹è¶‹äºç›¸å½“ä¹‹å‰ï¼Œä½ æƒ³è¦å¹³å‡å¤šå°‘ä¸ªã€‚è¿™äº›çºªå…ƒã€‚çœ‹èµ·æ¥å¤§çº¦åœ¨113åˆ°130ä¹‹é—´ã€‚è¿™äº›å€¼å˜åŒ–å¾ˆå¤§ï¼Œä½†å‡å€¼çºªå…ƒå·²ç»æ”¶æ•›åˆ°1ï¼Œ1ï¼Œ8ï¼Œ11ï¼Œ7ã€‚å› æ­¤å‡å€¼çºªå…ƒçœ‹èµ·æ¥æ˜¯117åˆ°118ã€‚è¿™å°±æ˜¯ä½ çœŸæ­£éœ€è¦è®­ç»ƒçš„çºªå…ƒæ•°ã€‚
- en: And then the meanï¼Œ the mean scoreã€‚ It said thereï¼Œ it'sï¼Œ it's not converged a
    whole lotã€‚ it'sã€‚ it's in around 0ã€‚75 to 0ã€‚74ã€‚ So we're in the pretty high7ã€‚4sï¼Œ
    low 7ã€‚5ã€‚ So it'sã€‚ it's somewhere around 0ã€‚75ã€‚ But you can see if you look just
    at the the mean scores hereã€‚ There's a lot of variance in the mean as it'sï¼Œ as
    it's going throughã€‚ That was regressionã€‚ Nowã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¹³å‡åˆ†æ•°ã€‚å®ƒè¯´é‚£é‡Œï¼Œæ²¡æœ‰å¤ªå¤šæ”¶æ•›ã€‚å®ƒå¤§çº¦åœ¨0.75åˆ°0.74ä¹‹é—´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¤„äºç›¸å½“é«˜çš„7.4å’Œä½7.5ä¹‹é—´ã€‚æ‰€ä»¥å®ƒå¤§çº¦åœ¨0.75å·¦å³ã€‚ä½†æ˜¯å¦‚æœä½ ä»…æŸ¥çœ‹è¿™é‡Œçš„å¹³å‡åˆ†æ•°ï¼Œå¯ä»¥çœ‹åˆ°éšç€å®ƒçš„è¿›è¡Œï¼Œå¹³å‡å€¼æœ‰å¾ˆå¤šæ–¹å·®ã€‚è¿™æ˜¯å›å½’ã€‚ç°åœ¨ã€‚
- en: if you want to do classificationã€‚ it's pretty similarï¼Œ really exceptã€‚Well you'll
    load in the data set we'll be predicting on product because it's the classification
    formã€‚Hereï¼Œ thoughï¼Œ we'll use a stratified shuffle splitã€‚ That is going to make
    sure that those classes stay balanced for classificationã€‚ We're using 10%ã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¿›è¡Œåˆ†ç±»ï¼Œè¿™çœŸçš„å¾ˆç›¸ä¼¼ï¼Œé™¤äº†â€¦â€¦å¥½å§ï¼Œä½ ä¼šåŠ è½½æˆ‘ä»¬å°†é¢„æµ‹çš„äº§å“æ•°æ®é›†ï¼Œå› ä¸ºå®ƒæ˜¯åˆ†ç±»å½¢å¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åˆ†å±‚æ´—ç‰Œæ‹†åˆ†ã€‚è¿™å°†ç¡®ä¿è¿™äº›ç±»åœ¨åˆ†ç±»ä¸­ä¿æŒå¹³è¡¡ã€‚æˆ‘ä»¬ä½¿ç”¨10%ã€‚
- en: just like before and also a random state of 42ã€‚ Then we're going to split itã€‚
    But notice we have to pass in in addition to X the products so that we know what
    the classes are so we canã€‚Evenly split thatï¼Œ then we we do the same splitting
    just like beforeã€‚ the rest of this is really prettyï¼Œ pretty much the same thingã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒä¹‹å‰ä¸€æ ·ï¼ŒéšæœºçŠ¶æ€ä¸º42ã€‚ç„¶åæˆ‘ä»¬è¦è¿›è¡Œæ‹†åˆ†ã€‚ä½†æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦é™¤äº†Xä»¥å¤–ï¼Œè¿˜è¦ä¼ å…¥äº§å“ï¼Œä»¥ä¾¿æˆ‘ä»¬çŸ¥é“ç±»æ˜¯ä»€ä¹ˆï¼Œä»¥ä¾¿èƒ½å¤Ÿå‡åŒ€æ‹†åˆ†ï¼Œç„¶åæˆ‘ä»¬å†è¿›è¡Œç›¸åŒçš„æ‹†åˆ†ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ã€‚å…¶ä½™éƒ¨åˆ†å®é™…ä¸Šæ˜¯ç›¸å½“ç›¸ä¼¼çš„ã€‚
- en: We have categorical cross entropy and we set the final output neuron count to
    the number of classesã€‚ so it's the typical classification sort of thingã€‚And we
    print out the same results that we had beforeã€‚ So we're tracking the mean epochs
    and the mean score so that we can get an idea of how long we should train this
    and alsoã€‚Let us know relative to another setting of hyperparametersã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰åˆ†ç±»äº¤å‰ç†µï¼Œå¹¶å°†æœ€ç»ˆè¾“å‡ºç¥ç»å…ƒè®¡æ•°è®¾ç½®ä¸ºç±»çš„æ•°é‡ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯å…¸å‹çš„åˆ†ç±»æ–¹å¼ã€‚æˆ‘ä»¬æ‰“å°å‡ºä¹‹å‰çš„ç›¸åŒç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬è·Ÿè¸ªå¹³å‡å‘¨æœŸå’Œå¹³å‡åˆ†æ•°ï¼Œä»¥ä¾¿èƒ½å¤Ÿäº†è§£æˆ‘ä»¬åº”è¯¥è®­ç»ƒå¤šé•¿æ—¶é—´ï¼Œä»¥åŠâ€¦â€¦ç›¸å¯¹äºå…¶ä»–è¶…å‚æ•°è®¾ç½®ã€‚
- en: how effective the current set of hyperparameters areã€‚ Nowï¼Œ to use this the benchmarkã€‚
    we'll do a regression problemã€‚ So here I'm setting up the data set toã€‚ So for
    benchmarkingã€‚ we're going to do a classification problem So I am setting this
    up basically so that it creates the dummies on the product columnã€‚ We'll evaluate
    it with log lossã€‚ I'll go ahead run that so that we have itã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰è¶…å‚æ•°é›†çš„æœ‰æ•ˆæ€§ã€‚ç°åœ¨ï¼Œä¸ºäº†ä½¿ç”¨è¿™ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬å°†åšä¸€ä¸ªå›å½’é—®é¢˜ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘æ­£åœ¨è®¾ç½®æ•°æ®é›†ã€‚ä¸ºäº†åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å°†åšä¸€ä¸ªåˆ†ç±»é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘åŸºæœ¬ä¸Šæ˜¯åœ¨è®¾ç½®ï¼Œä»¥ä¾¿åœ¨äº§å“åˆ—ä¸Šåˆ›å»ºè™šæ‹Ÿå˜é‡ã€‚æˆ‘ä»¬å°†ç”¨å¯¹æ•°æŸå¤±è¯„ä¼°å®ƒã€‚æˆ‘å°†è¿è¡Œè¿™ä¸ªï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿå¾—åˆ°å®ƒã€‚
- en: I am going to start the benchmarking and explain it as goã€‚ So you'll notice
    this is pretty similarã€‚ we're doing the stratified shuffle split just like beforeã€‚
    number of splitsã€‚ This time is 100 because that gives meã€‚ğŸ˜Šï¼ŒGives me a more converged
    score to actually evaluate this with using a test set size of 10%ã€‚ So we loop
    through just like beforeã€‚ The big difference hereï¼Œ thoughã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å¼€å§‹åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨è¿›è¡Œæ—¶è§£é‡Šã€‚æ‰€ä»¥ä½ ä¼šæ³¨æ„åˆ°è¿™éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬æ­£åœ¨è¿›è¡Œåˆ†å±‚æ´—ç‰Œæ‹†åˆ†ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ã€‚æ‹†åˆ†æ¬¡æ•°ã€‚è¿™æ¬¡æ˜¯100ï¼Œå› ä¸ºè¿™ç»™äº†æˆ‘æ›´æ”¶æ•›çš„åˆ†æ•°ï¼Œä»¥ä¾¿å®é™…ä½¿ç”¨10%çš„æµ‹è¯•é›†å¤§å°è¿›è¡Œè¯„ä¼°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·å¾ªç¯ã€‚è¿™é‡Œæœ€å¤§çš„ä¸åŒæ˜¯ã€‚
- en: is I did spend some time trying to tune theseã€‚You'll notice I'm using a different
    activation functionã€‚ We'll see more about this Preluã€‚ It's like a leaky reluã€‚
    except the amount of leakiness is a parameter that is optimized by the neural
    network as wellã€‚ We'll see some more modern activation functions when we get into
    the kagle module and talk about how to really automatically tune these hyperparameterã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¡®å®èŠ±äº†ä¸€äº›æ—¶é—´å°è¯•è°ƒæ•´è¿™äº›ã€‚ä½ ä¼šæ³¨æ„åˆ°æˆ‘ä½¿ç”¨äº†ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€‚æˆ‘ä»¬å°†çœ‹åˆ°æ›´å¤šå…³äºè¿™ä¸ªPreluçš„å†…å®¹ã€‚å®ƒåƒä¸€ä¸ªæ³„æ¼çš„reluï¼Œåªæ˜¯æ³„æ¼çš„ç¨‹åº¦æ˜¯ä¸€ä¸ªç”±ç¥ç»ç½‘ç»œä¼˜åŒ–çš„å‚æ•°ã€‚å½“æˆ‘ä»¬è¿›å…¥kagleæ¨¡å—å¹¶è°ˆè®ºå¦‚ä½•çœŸæ­£è‡ªåŠ¨è°ƒæ•´è¿™äº›è¶…å‚æ•°æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°ä¸€äº›æ›´ç°ä»£çš„æ¿€æ´»å‡½æ•°ã€‚
- en: So I'm using that type of activation functionã€‚ I do have 50% dropout on the
    first two hidden layersã€‚ but not the thirdã€‚ And that is mostly what I had triedã€‚
    I had tried a couple of different onesã€‚ And this was truly giving me the lowest
    or the best best results for log lossã€‚ because usually want a low log lossã€‚ These
    are some of the other attempts that I got and the various mean scores that I gotã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä½¿ç”¨äº†é‚£ç§æ¿€æ´»å‡½æ•°ã€‚æˆ‘åœ¨å‰ä¸¤ä¸ªéšè—å±‚ä¸Šæœ‰50%çš„ä¸¢å¼ƒï¼Œä½†ç¬¬ä¸‰ä¸ªæ²¡æœ‰ã€‚è¿™å°±æ˜¯æˆ‘å°è¯•çš„ä¸»è¦å†…å®¹ã€‚æˆ‘å°è¯•äº†å‡ ç§ä¸åŒçš„æ–¹å¼ã€‚è¿™å®é™…ä¸Šç»™äº†æˆ‘å¯¹æ•°æŸå¤±æœ€ä½æˆ–æœ€ä½³çš„ç»“æœï¼Œå› ä¸ºé€šå¸¸å¸Œæœ›å¯¹æ•°æŸå¤±è¾ƒä½ã€‚è¿™æ˜¯æˆ‘å¾—åˆ°çš„ä¸€äº›å…¶ä»–å°è¯•å’Œå„ç§å¹³å‡åˆ†æ•°ã€‚
- en: So I was getting this right around it about 0ã€‚65ï¼Œ which is somewhat what the
    the mean score is converging toã€‚Standard deviation is a little bit lowerã€‚ I attribute
    that to using dropout and you can let this runã€‚ It takes it a little whileã€‚So
    this can be a very lengthy procedure I will often have my regular computer going
    and my Google Coab account both working on different sides of a particular problem
    so this will go onã€‚ you'll see this score converge to somewhere about 0ã€‚65ã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¤§çº¦å¾—åˆ°äº†è¿™ä¸ªç»“æœï¼Œæ¥è¿‘0.65ï¼Œè¿™å¤§è‡´æ˜¯å‡å€¼åˆ†æ•°æ”¶æ•›çš„å€¼ã€‚æ ‡å‡†å·®ç¨å¾®ä½ä¸€äº›ã€‚æˆ‘å°†å…¶å½’å› äºä½¿ç”¨äº†dropoutï¼Œå¹¶ä¸”ä½ å¯ä»¥è®©è¿™ä¸ªè¿‡ç¨‹æŒç»­è¿›è¡Œã€‚è¿™éœ€è¦ä¸€ç‚¹æ—¶é—´ã€‚å› æ­¤ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªéå¸¸æ¼«é•¿çš„è¿‡ç¨‹ï¼Œæˆ‘é€šå¸¸ä¼šè®©æˆ‘çš„å¸¸è§„è®¡ç®—æœºå’Œæˆ‘çš„Google
    Coabè´¦æˆ·åŒæ—¶åœ¨ç‰¹å®šé—®é¢˜çš„ä¸åŒæ–¹é¢å·¥ä½œï¼Œå› æ­¤è¿™ä¸ªè¿‡ç¨‹ä¼šç»§ç»­è¿›è¡Œã€‚ä½ ä¼šçœ‹åˆ°è¿™ä¸ªåˆ†æ•°æ”¶æ•›åˆ°å¤§çº¦0.65å·¦å³ã€‚
- en: Thank you for watching this video up to this point in this course we've dealt
    primarily with tabular data and neural networks work fine with thisã€‚ Howeverï¼Œ
    now we're going to start to get into the things that make neural networks really
    a model type that has gained a lot of attentionã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ è§‚çœ‹åˆ°ç›®å‰ä¸ºæ­¢çš„è§†é¢‘ï¼Œåœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å¤„ç†çš„æ˜¯è¡¨æ ¼æ•°æ®ï¼Œç¥ç»ç½‘ç»œåœ¨è¿™æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼Œç°åœ¨æˆ‘ä»¬å°†å¼€å§‹è¿›å…¥ä¸€äº›ä½¿ç¥ç»ç½‘ç»œçœŸæ­£æˆä¸ºä¸€ç§å¤‡å—å…³æ³¨çš„æ¨¡å‹ç±»å‹çš„å†…å®¹ã€‚
- en: '![](img/7030d8ee32d0baa28197ef0d0c7cdbba_3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7030d8ee32d0baa28197ef0d0c7cdbba_3.png)'
- en: Specificallyï¼Œ we're going to start with imagesï¼Œ but we'll also get into time
    seriesã€‚How to actually have images be the output of neural network and other things
    as wellï¼Ÿ
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä»å›¾åƒå¼€å§‹ï¼Œä½†æˆ‘ä»¬ä¹Ÿä¼šè¿›å…¥æ—¶é—´åºåˆ—ã€‚å¦‚ä½•è®©å›¾åƒå®é™…æˆä¸ºç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼Œä»¥åŠå…¶ä»–ç›¸å…³å†…å®¹ï¼Ÿ
- en: This content changes oftenï¼Œ so subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligenceã€‚ This content changes
    oftenã€‚ so subscribe to the channel to stay up to date on this course and other
    topics in artificial intelligenceã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå†…å®¹ç»å¸¸æ›´æ–°ï¼Œæ‰€ä»¥è¯·è®¢é˜…é¢‘é“ä»¥ä¿æŒå¯¹æœ¬è¯¾ç¨‹å’Œäººå·¥æ™ºèƒ½å…¶ä»–ä¸»é¢˜çš„æœ€æ–°äº†è§£ã€‚è¿™ä¸ªå†…å®¹ç»å¸¸æ›´æ–°ï¼Œæ‰€ä»¥è¯·è®¢é˜…é¢‘é“ä»¥ä¿æŒå¯¹æœ¬è¯¾ç¨‹å’Œäººå·¥æ™ºèƒ½å…¶ä»–ä¸»é¢˜çš„æœ€æ–°äº†è§£ã€‚
