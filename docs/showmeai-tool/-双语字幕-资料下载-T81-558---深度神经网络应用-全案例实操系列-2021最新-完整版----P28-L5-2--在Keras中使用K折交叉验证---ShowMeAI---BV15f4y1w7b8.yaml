- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P28ÔºöL5.2- Âú®Keras‰∏≠‰ΩøÁî®KÊäò‰∫§ÂèâÈ™åËØÅ
    - ShowMeAI - BV15f4y1w7b8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå this is Jeff HeatonÔºå welcome to applications of Deep neural networks with
    Washington University In this part„ÄÇ we're going to take a look at how we can use
    Kfold cross validation to get a good estimate of how our neural network will perform
    on data that is outside of the training set This will allow us to produce out
    of sample predictions that is predictions that are not in the training set for
    the entire training set by simply flipping through the training set and letting
    each piece of the data have a chance to be the validation and the training set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This will let us see how well our regularization and other techniques are combating
    overfitting for the latest on my AI course and projects„ÄÇ click subscribe and the
    bell next to it to be notified of every new video„ÄÇ Cross validation often has
    different objectives that you're trying to actually accomplish„ÄÇ The overall idea
    of kfold cross validation is that you're getting multiple„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Training sets and validation sets so you can train the neural network on one
    part of your data and evaluate it on another„ÄÇThe most common reason for doing
    this is generating out of sample predictions from a neural network„ÄÇ We'll see
    how that's doneÔºå but you're able to basically generate an out of sample prediction
    for or every single element in the entire training set„ÄÇ you can also use it to
    estimate a good number of epos to train a neural network for when used in conjunction
    with early stopping„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which we've already seen„ÄÇ FinallyÔºå you can use it to evaluate the effectiveness
    of certain hyperparameters„ÄÇ Maybe you're wanting to test„ÄÇüòäÔºåIf adding another hidden
    layer or some other adjustment to your neural network is actually helping you„ÄÇ
    HoweverÔºå with the randomness of neural networksÔºå often you need to do more than
    just a single set of cross validation folds to really evaluate if what you've
    done has has had any effect„ÄÇWe'll look at a process called bootstrapping to really
    evaluate the effectiveness of changes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is how cross validation works„ÄÇ You essentially take your data set and you
    break it into five folds„ÄÇ usually or10 folds or some number of folds„ÄÇ The number
    of elements in each of these folds should be pretty much even„ÄÇ just due to the
    fact that your data set won't break up evenly always into five folds„ÄÇ or however
    manyÔºå you'll have a few extra elements in each fold„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but largely they're pretty equal to each other„ÄÇ However many folds you have„ÄÇ
    That's how many times you're going to train„ÄÇ So you're literally going to have
    a neural network for each of the folds„ÄÇ And this is some of the the complexity
    in what you do within those five neural networks„ÄÇ And we'll talk about that as
    well„ÄÇBut essentiallyÔºå what you do is each fold you pick„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So each time through these same five folds and they're the same five folds in
    each one„ÄÇ you choose one of the folds to be the validation set„ÄÇ and you use the
    remaining„ÄÇ you cancatenate folds 2Ôºå3Ôºå4 and 5 together„ÄÇ and this is the training
    set„ÄÇ and the highlighted one here„ÄÇ That is the validation set„ÄÇ So you train that
    and you get one model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You train this oneÔºå and you get another model„ÄÇ This lets each of the folds have
    their chance as the validation set„ÄÇ and you're essentially generating predictions
    on the validation set„ÄÇ So if you take all of these yellow regions and concatenate
    them together or you have out of sample predictions If that's what your objective
    one„ÄÇ You can see that hereÔºå you basically do a prediction on each of these concatenate
    those predictions together„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now you have a out of fold prediction„ÄÇüòäÔºåOn the entire training set„ÄÇ and
    that's cool because you didn't have to waste part of your training set for training„ÄÇ
    part of your data set for validationÔºå you're able to predict on the entire thing„ÄÇ
    Now„ÄÇ if you want to bring these togetherÔºå So say you you have trained this and
    now you have five neural networks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There's several ways that you get your final resulting neural network that you're
    going to use for new data that's outside of this all because you're probably creating
    your neural network because you want to train something on an ongoing basis if
    say you're a life insurance company„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you've got historic data and you've created this you've trained it to try to
    classify the risk of these people dying for your life insurance policies„ÄÇ you're
    going to have new people coming in and that's what you really want to do your
    work on„ÄÇ So you need to figure out what are you doing Are you picking the best
    of these models„ÄÇAre you averaging them all together or somehowÔºå and these are
    some of the common ways that you do this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You could choose the model that has the highest validation score„ÄÇ NowÔºå one warning
    signÔºå though„ÄÇ that I tend to look at is how similar are the validation scores
    between these„ÄÇ If one of these models consistently gets„ÄÇworse score than the others„ÄÇ
    That means that there's some outliers that landed in thisÔºå in this validation
    set for say Model 4„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the more you see variances between theseÔºå that means that you have just a
    couple of outliers that are landing in„ÄÇIn particularÔºå validation foldsÔºå and that
    might be something that you need to consider in your data set„ÄÇMaybe you want to
    eliminate those outliersÔºå maybe you„ÄÇ but at the very least you should be aware
    of themÔºå new data coming in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so say your new life insurance applicationsÔºå you could present that to all five
    of these then vote„ÄÇOr average„ÄÇ So voting would be if the first three say no„ÄÇ Don't
    don't insure this individual and 4 and 5 say yesÔºå that majority win„ÄÇ And you'd
    say no„ÄÇ Or if it's giving you some sort of score or age that you think this individual
    will live to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then you average them altogether„ÄÇ This is the one that I like probably the best„ÄÇ
    I prefer either the averaging one or or this one„ÄÇ So you do early stopping on
    each of these„ÄÇ And you keep track of how many epochs were needed for each full„ÄÇ
    So maybe it took 300 epochs to get this one trained to an optimal level an early
    stop to 50 here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 320 hereÔºå howeverÔºå many„ÄÇEpoet tu„ÄÇYou then average that together or you take
    the maximum or there's other techniques„ÄÇ but you look at all the number of epochs
    that are tookck across all of these„ÄÇAnd then you train on the entire training
    set„ÄÇ You don't use a validation„ÄÇ and you train for that number of epochs„ÄÇ And
    then that becomes a single neural network that is hopefully„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The the best of thoseÔºå all you still have the you still have the random nature
    of the neural network„ÄÇ so you probably want to train that one multiple times„ÄÇAnd
    perhaps average those together„ÄÇ Now„ÄÇ let's see some examples„ÄÇOf how to do cross
    validation„ÄÇ So the objective here is going to be out of sample prediction„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to do out of sample regression because the code looks a little
    bit different for each of these„ÄÇSo what you've got toÔºå there's some differences
    in how you do the cross validation between classification and regression regression
    is definitely the more simple case because you just simply break it up into an
    even number of folds„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And and you're doneÔºå but with classificationÔºå you need to make sure that breaking
    those folds up does not mess up your class balances„ÄÇIf you've gotÔºå if you're classifyingÔºå
    say between three things„ÄÇ you want to make sure that the ratio of those three
    things is the same in each of those folds„ÄÇ or you're introducing bias into your
    neural network„ÄÇ And that's not a good thing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So out of sample kfold cross validation for regression„ÄÇ Here we set up the data
    sloting that in„ÄÇ We're using that same simple data set that we had beforeÔºå we're
    going to be predicting the age„ÄÇ like we've done beforeÔºå because this is a regression
    problem„ÄÇ All these other values are coming in to help us predict the age„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to assume 500 epochs for this„ÄÇ We're not going to use early
    stopping because we want our validation fold So really try to give us some sort
    of out of sample prediction„ÄÇ we can use early stopping„ÄÇ So maybe we would do a
    whole separate run just to figure out how many epochs and we could adjust this
    to something more optimal„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But for nowÔºå we're just we're just assuming 500 epochs so that we can get a
    true sort of out of sample prediction of how many„ÄÇOf how good 500 upbox would
    be here we set up the cross validation„ÄÇ We're usingsyit Learnar randomand state
    42„ÄÇ that just means that the folds will be the same time the same each time„ÄÇ Suffle
    is always a good idea„ÄÇ If you're shuffling„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you need a random state so that you can know that your folds are the same but
    your folds are different each time that you run this as you're trying experiments„ÄÇ
    it'll just make things inconsistent you won't notice that say fold one is consistently
    getting an outlier„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It's okay to change this value„ÄÇBut this gives you at least some consistency
    while you're while you're experimenting„ÄÇ then you get the train in the test set
    for the the split„ÄÇ you're passing in the X data set and you're looping through
    each time you're creating a regressionion neural network because that is one„ÄÇ
    and that's mean squared error„ÄÇTraining with Adam„ÄÇAnd you're fitting now with your
    validation set so that you can report a validation error as you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: As you go throughÔºå our out of sample predictionsÔºå we're creating the out of
    sample Ys„ÄÇ that's just so that we're getting the Y's in the same order that our
    folds came in so that we can do a final evaluation and the out of sample predictions
    we're simply appending them together„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So as you do each one of theseÔºå you'll make out a sample predictions and you
    will concatenate the folds together so that you have one uniform one at the very
    end„ÄÇ you report the score so that you have the overall score„ÄÇFor this fold„ÄÇAnd
    the overall score finally comes at the end„ÄÇ You're going to take all of these
    out of sample Ys that you've grouped„ÄÇ concatenate them togetherÔºå the out of sample
    predictions„ÄÇ and you're going to compare the Ys„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which were the values that we wanted it to be with what the predictions were„ÄÇ
    And we're going to report a final out of sample score of whatever the difference
    between these two happens to be„ÄÇ And that's your estimate of how good your your
    neural network actually what„ÄÇ we can also write out the out of sample predictions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If we want them for some further analysis to write them as a Cv file„ÄÇ Now„ÄÇ when
    we classify we have to do something called kfold cross validation„ÄÇ This is just
    because we don't want to say the balance of those of the items in each„ÄÇ So we're
    going to use the same„ÄÇ go ahead and run that loads the data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we're trying to predict the products„ÄÇ So whatever that balance of products
    was„ÄÇ Maybe they have 20% Choose product a„ÄÇüòäÔºåYou don't want to change that balance
    because that's your ground truth„ÄÇ If the neural network is veryÔºå very unsure based
    on your data„ÄÇ it's always going to say product A had 20%„ÄÇ It's going to sayÔºå yeahÔºå
    20% probability„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's a if it has no clue whatsoever based on the input data„ÄÇ maybe the input
    data was particularly hard in that case„ÄÇ So if you're training one neural network
    and the ground truth percent difference„ÄÇ Class A is much bigger and fold a fold1„ÄÇ
    then class A is in full2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that's going to mess with that ground truth and that will mess with the accuracy
    of your neural network„ÄÇSo what we do is stratified kfold„ÄÇ So we stratified kfold
    sampling here„ÄÇ We're going to have our five folds„ÄÇ We're going to shuffle and
    hair a random state„ÄÇ That'll looks the same„ÄÇ That just drops in„ÄÇSame as as kfold„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're going to keep the out of sample wise in predictions just like before„ÄÇ
    but this part's a little different„ÄÇWe're doing the split„ÄÇ So when we do the split
    in the previous oneÔºå we just pass in X„ÄÇ But since we want to balance on the Y'sÔºå
    we have to pass in the y„ÄÇ We have to pass in the class„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it needs to know what class we're trying to balance on„ÄÇ And we pass in the
    original class from the data frame the product We don't want to pass in dummy
    variables for this„ÄÇ It's not designed to work on dummy variables„ÄÇ It needs a class
    like three like ABC or whatever those products are„ÄÇ And it uses that to make sure
    that the balance is the same in each of those folds„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that's very important„ÄÇ You could just put in Kfold hereÔºå just like you did
    in regression„ÄÇ and it would work just fine„ÄÇüòäÔºåBut you would have unbalanced splits„ÄÇAnd
    often„ÄÇ that would work out just fineÔºå but it will probably affect the accuracy
    of your neural network at least a little bit„ÄÇ We use categorical cross entropy
    and softmax and a output neurons equal to the number of classes that we have because
    we are doing a classification neural network„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Everything else is the same as regression„ÄÇWe report the individual accuracy
    on each score„ÄÇ We're using accuracy instead of ourMSA because we are doing classification„ÄÇ
    Then we print the final accuracy and we're we're basically done with that„ÄÇ Now
    you can run this and it begins to it'll go through each of the folds and it'll
    report a fold score and then ultimately the final accuracy for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the first accuracy was about 66% rounded„ÄÇ And now you can see that we have
    essentially gone through all the five folds and we produce a final accuracy score
    of„ÄÇ66%Ôºå which is prettyÔºå pretty similar range to these„ÄÇ The most difficult fold
    was the fifth one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So there might be some sort of an outlier or something in that one that is making
    that one more difficult„ÄÇ or this simply could be the random nature of the neural
    network„ÄÇ IlyÔºå you'd want to run that again„ÄÇSee if that one is consistently hard„ÄÇ
    And againÔºå that's why you want to use that random seed so that the folds are consistent„ÄÇ
    FinallyÔºå you can trade with a cross validation and a holdout set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is where you set out a holdout before you even do the five folds„ÄÇ and that
    lets you do that„ÄÇ So to see so this gives you then a holdout set that is different
    than the five folds that you're dealing with„ÄÇIf we run this„ÄÇThis is also regression„ÄÇ
    We're trying to predict the age„ÄÇ We can then run it and we I'll get it running
    so that we can see that while I'm doing that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you have the you set it for the five folds just like before and you loop through
    the entire thing„ÄÇ1 regression„ÄÇ you're training on the mean square error with the
    atom update rule„ÄÇ It's really pretty similar„ÄÇ The folds are all the same„ÄÇ It's
    just you're keeping this final holdout set that you're going to do the predictions
    on and you're going to report a root mean square on that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you'll see a different one between the two„ÄÇ We run through all of this„ÄÇ And
    we will get our final holdout value„ÄÇ this is training just like before„ÄÇ except
    that holdout set is simply not in there„ÄÇ And here's our final holdout notice that
    it is worse than the other ones„ÄÇ Now so it seems like we're doing all this extra
    effort just to get a worse score„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But this is a more accurate score„ÄÇs no there's less overfitting in that„ÄÇ Thank
    you for watching this video„ÄÇ in the next video„ÄÇ we're going to see how to actually
    use L1 and L2„ÄÇ and also a combination of the two„ÄÇ in Cars„ÄÇ This content changes
    often„ÄÇ So subscribe to the channel to stay up to date on this course and other
    topics and artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_3.png)'
  prefs: []
  type: TYPE_IMG
