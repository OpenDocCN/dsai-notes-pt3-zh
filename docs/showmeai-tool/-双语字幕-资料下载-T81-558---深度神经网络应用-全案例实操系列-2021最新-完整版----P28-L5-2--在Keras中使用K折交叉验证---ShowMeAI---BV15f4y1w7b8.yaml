- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P28ï¼šL5.2- åœ¨Kerasä¸­ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hiï¼Œ this is Jeff Heatonï¼Œ welcome to applications of Deep neural networks with
    Washington University In this partã€‚ we're going to take a look at how we can use
    Kfold cross validation to get a good estimate of how our neural network will perform
    on data that is outside of the training set This will allow us to produce out
    of sample predictions that is predictions that are not in the training set for
    the entire training set by simply flipping through the training set and letting
    each piece of the data have a chance to be the validation and the training setã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: This will let us see how well our regularization and other techniques are combating
    overfitting for the latest on my AI course and projectsã€‚ click subscribe and the
    bell next to it to be notified of every new videoã€‚ Cross validation often has
    different objectives that you're trying to actually accomplishã€‚ The overall idea
    of kfold cross validation is that you're getting multipleã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: Training sets and validation sets so you can train the neural network on one
    part of your data and evaluate it on anotherã€‚The most common reason for doing
    this is generating out of sample predictions from a neural networkã€‚ We'll see
    how that's doneï¼Œ but you're able to basically generate an out of sample prediction
    for or every single element in the entire training setã€‚ you can also use it to
    estimate a good number of epos to train a neural network for when used in conjunction
    with early stoppingã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: which we've already seenã€‚ Finallyï¼Œ you can use it to evaluate the effectiveness
    of certain hyperparametersã€‚ Maybe you're wanting to testã€‚ğŸ˜Šï¼ŒIf adding another hidden
    layer or some other adjustment to your neural network is actually helping youã€‚
    Howeverï¼Œ with the randomness of neural networksï¼Œ often you need to do more than
    just a single set of cross validation folds to really evaluate if what you've
    done has has had any effectã€‚We'll look at a process called bootstrapping to really
    evaluate the effectiveness of changesã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: This is how cross validation worksã€‚ You essentially take your data set and you
    break it into five foldsã€‚ usually or10 folds or some number of foldsã€‚ The number
    of elements in each of these folds should be pretty much evenã€‚ just due to the
    fact that your data set won't break up evenly always into five foldsã€‚ or however
    manyï¼Œ you'll have a few extra elements in each foldã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: but largely they're pretty equal to each otherã€‚ However many folds you haveã€‚
    That's how many times you're going to trainã€‚ So you're literally going to have
    a neural network for each of the foldsã€‚ And this is some of the the complexity
    in what you do within those five neural networksã€‚ And we'll talk about that as
    wellã€‚But essentiallyï¼Œ what you do is each fold you pickã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: So each time through these same five folds and they're the same five folds in
    each oneã€‚ you choose one of the folds to be the validation setã€‚ and you use the
    remainingã€‚ you cancatenate folds 2ï¼Œ3ï¼Œ4 and 5 togetherã€‚ and this is the training
    setã€‚ and the highlighted one hereã€‚ That is the validation setã€‚ So you train that
    and you get one modelã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: You train this oneï¼Œ and you get another modelã€‚ This lets each of the folds have
    their chance as the validation setã€‚ and you're essentially generating predictions
    on the validation setã€‚ So if you take all of these yellow regions and concatenate
    them together or you have out of sample predictions If that's what your objective
    oneã€‚ You can see that hereï¼Œ you basically do a prediction on each of these concatenate
    those predictions togetherã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: And now you have a out of fold predictionã€‚ğŸ˜Šï¼ŒOn the entire training setã€‚ and
    that's cool because you didn't have to waste part of your training set for trainingã€‚
    part of your data set for validationï¼Œ you're able to predict on the entire thingã€‚
    Nowã€‚ if you want to bring these togetherï¼Œ So say you you have trained this and
    now you have five neural networksã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: There's several ways that you get your final resulting neural network that you're
    going to use for new data that's outside of this all because you're probably creating
    your neural network because you want to train something on an ongoing basis if
    say you're a life insurance companyã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: you've got historic data and you've created this you've trained it to try to
    classify the risk of these people dying for your life insurance policiesã€‚ you're
    going to have new people coming in and that's what you really want to do your
    work onã€‚ So you need to figure out what are you doing Are you picking the best
    of these modelsã€‚Are you averaging them all together or somehowï¼Œ and these are
    some of the common ways that you do thisã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: You could choose the model that has the highest validation scoreã€‚ Nowï¼Œ one warning
    signï¼Œ thoughã€‚ that I tend to look at is how similar are the validation scores
    between theseã€‚ If one of these models consistently getsã€‚worse score than the othersã€‚
    That means that there's some outliers that landed in thisï¼Œ in this validation
    set for say Model 4ã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: So the more you see variances between theseï¼Œ that means that you have just a
    couple of outliers that are landing inã€‚In particularï¼Œ validation foldsï¼Œ and that
    might be something that you need to consider in your data setã€‚Maybe you want to
    eliminate those outliersï¼Œ maybe youã€‚ but at the very least you should be aware
    of themï¼Œ new data coming inã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: so say your new life insurance applicationsï¼Œ you could present that to all five
    of these then voteã€‚Or averageã€‚ So voting would be if the first three say noã€‚ Don't
    don't insure this individual and 4 and 5 say yesï¼Œ that majority winã€‚ And you'd
    say noã€‚ Or if it's giving you some sort of score or age that you think this individual
    will live toã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: then you average them altogetherã€‚ This is the one that I like probably the bestã€‚
    I prefer either the averaging one or or this oneã€‚ So you do early stopping on
    each of theseã€‚ And you keep track of how many epochs were needed for each fullã€‚
    So maybe it took 300 epochs to get this one trained to an optimal level an early
    stop to 50 hereã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 320 hereï¼Œ howeverï¼Œ manyã€‚Epoet tuã€‚You then average that together or you take
    the maximum or there's other techniquesã€‚ but you look at all the number of epochs
    that are tookck across all of theseã€‚And then you train on the entire training
    setã€‚ You don't use a validationã€‚ and you train for that number of epochsã€‚ And
    then that becomes a single neural network that is hopefullyã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The the best of thoseï¼Œ all you still have the you still have the random nature
    of the neural networkã€‚ so you probably want to train that one multiple timesã€‚And
    perhaps average those togetherã€‚ Nowã€‚ let's see some examplesã€‚Of how to do cross
    validationã€‚ So the objective here is going to be out of sample predictionã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to do out of sample regression because the code looks a little
    bit different for each of theseã€‚So what you've got toï¼Œ there's some differences
    in how you do the cross validation between classification and regression regression
    is definitely the more simple case because you just simply break it up into an
    even number of foldsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: And and you're doneï¼Œ but with classificationï¼Œ you need to make sure that breaking
    those folds up does not mess up your class balancesã€‚If you've gotï¼Œ if you're classifyingï¼Œ
    say between three thingsã€‚ you want to make sure that the ratio of those three
    things is the same in each of those foldsã€‚ or you're introducing bias into your
    neural networkã€‚ And that's not a good thingã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: So out of sample kfold cross validation for regressionã€‚ Here we set up the data
    sloting that inã€‚ We're using that same simple data set that we had beforeï¼Œ we're
    going to be predicting the ageã€‚ like we've done beforeï¼Œ because this is a regression
    problemã€‚ All these other values are coming in to help us predict the ageã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Now we're going to assume 500 epochs for thisã€‚ We're not going to use early
    stopping because we want our validation fold So really try to give us some sort
    of out of sample predictionã€‚ we can use early stoppingã€‚ So maybe we would do a
    whole separate run just to figure out how many epochs and we could adjust this
    to something more optimalã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: But for nowï¼Œ we're just we're just assuming 500 epochs so that we can get a
    true sort of out of sample prediction of how manyã€‚Of how good 500 upbox would
    be here we set up the cross validationã€‚ We're usingsyit Learnar randomand state
    42ã€‚ that just means that the folds will be the same time the same each timeã€‚ Suffle
    is always a good ideaã€‚ If you're shufflingã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: you need a random state so that you can know that your folds are the same but
    your folds are different each time that you run this as you're trying experimentsã€‚
    it'll just make things inconsistent you won't notice that say fold one is consistently
    getting an outlierã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: It's okay to change this valueã€‚But this gives you at least some consistency
    while you're while you're experimentingã€‚ then you get the train in the test set
    for the the splitã€‚ you're passing in the X data set and you're looping through
    each time you're creating a regressionion neural network because that is oneã€‚
    and that's mean squared errorã€‚Training with Adamã€‚And you're fitting now with your
    validation set so that you can report a validation error as youã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: As you go throughï¼Œ our out of sample predictionsï¼Œ we're creating the out of
    sample Ysã€‚ that's just so that we're getting the Y's in the same order that our
    folds came in so that we can do a final evaluation and the out of sample predictions
    we're simply appending them togetherã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: So as you do each one of theseï¼Œ you'll make out a sample predictions and you
    will concatenate the folds together so that you have one uniform one at the very
    endã€‚ you report the score so that you have the overall scoreã€‚For this foldã€‚And
    the overall score finally comes at the endã€‚ You're going to take all of these
    out of sample Ys that you've groupedã€‚ concatenate them togetherï¼Œ the out of sample
    predictionsã€‚ and you're going to compare the Ysã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: which were the values that we wanted it to be with what the predictions wereã€‚
    And we're going to report a final out of sample score of whatever the difference
    between these two happens to beã€‚ And that's your estimate of how good your your
    neural network actually whatã€‚ we can also write out the out of sample predictionsã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: If we want them for some further analysis to write them as a Cv fileã€‚ Nowã€‚ when
    we classify we have to do something called kfold cross validationã€‚ This is just
    because we don't want to say the balance of those of the items in eachã€‚ So we're
    going to use the sameã€‚ go ahead and run that loads the data setã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: And now we're trying to predict the productsã€‚ So whatever that balance of products
    wasã€‚ Maybe they have 20% Choose product aã€‚ğŸ˜Šï¼ŒYou don't want to change that balance
    because that's your ground truthã€‚ If the neural network is veryï¼Œ very unsure based
    on your dataã€‚ it's always going to say product A had 20%ã€‚ It's going to sayï¼Œ yeahï¼Œ
    20% probabilityã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: it's a if it has no clue whatsoever based on the input dataã€‚ maybe the input
    data was particularly hard in that caseã€‚ So if you're training one neural network
    and the ground truth percent differenceã€‚ Class A is much bigger and fold a fold1ã€‚
    then class A is in full2ã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: that's going to mess with that ground truth and that will mess with the accuracy
    of your neural networkã€‚So what we do is stratified kfoldã€‚ So we stratified kfold
    sampling hereã€‚ We're going to have our five foldsã€‚ We're going to shuffle and
    hair a random stateã€‚ That'll looks the sameã€‚ That just drops inã€‚Same as as kfoldã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: we're going to keep the out of sample wise in predictions just like beforeã€‚
    but this part's a little differentã€‚We're doing the splitã€‚ So when we do the split
    in the previous oneï¼Œ we just pass in Xã€‚ But since we want to balance on the Y'sï¼Œ
    we have to pass in the yã€‚ We have to pass in the classã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: So it needs to know what class we're trying to balance onã€‚ And we pass in the
    original class from the data frame the product We don't want to pass in dummy
    variables for thisã€‚ It's not designed to work on dummy variablesã€‚ It needs a class
    like three like ABC or whatever those products areã€‚ And it uses that to make sure
    that the balance is the same in each of those foldsã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: So that's very importantã€‚ You could just put in Kfold hereï¼Œ just like you did
    in regressionã€‚ and it would work just fineã€‚ğŸ˜Šï¼ŒBut you would have unbalanced splitsã€‚And
    oftenã€‚ that would work out just fineï¼Œ but it will probably affect the accuracy
    of your neural network at least a little bitã€‚ We use categorical cross entropy
    and softmax and a output neurons equal to the number of classes that we have because
    we are doing a classification neural networkã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Everything else is the same as regressionã€‚We report the individual accuracy
    on each scoreã€‚ We're using accuracy instead of ourMSA because we are doing classificationã€‚
    Then we print the final accuracy and we're we're basically done with thatã€‚ Now
    you can run this and it begins to it'll go through each of the folds and it'll
    report a fold score and then ultimately the final accuracy for thisã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: So the first accuracy was about 66% roundedã€‚ And now you can see that we have
    essentially gone through all the five folds and we produce a final accuracy score
    ofã€‚66%ï¼Œ which is prettyï¼Œ pretty similar range to theseã€‚ The most difficult fold
    was the fifth oneã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: So there might be some sort of an outlier or something in that one that is making
    that one more difficultã€‚ or this simply could be the random nature of the neural
    networkã€‚ Ilyï¼Œ you'd want to run that againã€‚See if that one is consistently hardã€‚
    And againï¼Œ that's why you want to use that random seed so that the folds are consistentã€‚
    Finallyï¼Œ you can trade with a cross validation and a holdout setã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: This is where you set out a holdout before you even do the five foldsã€‚ and that
    lets you do thatã€‚ So to see so this gives you then a holdout set that is different
    than the five folds that you're dealing withã€‚If we run thisã€‚This is also regressionã€‚
    We're trying to predict the ageã€‚ We can then run it and we I'll get it running
    so that we can see that while I'm doing thatã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: you have the you set it for the five folds just like before and you loop through
    the entire thingã€‚1 regressionã€‚ you're training on the mean square error with the
    atom update ruleã€‚ It's really pretty similarã€‚ The folds are all the sameã€‚ It's
    just you're keeping this final holdout set that you're going to do the predictions
    on and you're going to report a root mean square on thatã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è®¾ç½®äº”ä¸ªæŠ˜å ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ï¼Œå¹¶ä¸”å¾ªç¯æ•´ä¸ªè¿‡ç¨‹ã€‚1å›å½’ã€‚ä½ æ­£åœ¨è®­ç»ƒå‡æ–¹è¯¯å·®ï¼Œä½¿ç”¨åŸå­æ›´æ–°è§„åˆ™ã€‚è¿™å…¶å®éå¸¸ç›¸ä¼¼ã€‚æ‰€æœ‰æŠ˜å éƒ½æ˜¯ä¸€æ ·çš„ã€‚åªä¸è¿‡ä½ ä¿ç•™äº†è¿™ä¸ªæœ€ç»ˆçš„ä¿ç•™é›†ï¼Œä»¥ä¾¿è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å°†æŠ¥å‘Šä¸€ä¸ªå‡æ–¹æ ¹ã€‚
- en: So you'll see a different one between the twoã€‚ We run through all of thisã€‚ And
    we will get our final holdout valueã€‚ this is training just like beforeã€‚ except
    that holdout set is simply not in thereã€‚ And here's our final holdout notice that
    it is worse than the other onesã€‚ Now so it seems like we're doing all this extra
    effort just to get a worse scoreã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ ä¼šçœ‹åˆ°ä¸¤è€…ä¹‹é—´çš„ä¸åŒã€‚æˆ‘ä»¬ä¼šèµ°è¿‡è¿™ä¸€åˆ‡ã€‚æˆ‘ä»¬å°†å¾—åˆ°æœ€ç»ˆçš„ä¿ç•™å€¼ï¼Œè¿™å°±åƒä»¥å‰çš„è®­ç»ƒï¼Œåªæ˜¯ä¿ç•™é›†ä¸åœ¨å…¶ä¸­ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬çš„æœ€ç»ˆä¿ç•™ï¼Œæ³¨æ„å®ƒæ¯”å…¶ä»–çš„æ›´å·®ã€‚ç°åœ¨çœ‹æ¥ï¼Œæˆ‘ä»¬ä¼¼ä¹ä»˜å‡ºäº†é¢å¤–çš„åŠªåŠ›ï¼Œåªæ˜¯ä¸ºäº†å¾—åˆ°ä¸€ä¸ªæ›´ç³Ÿçš„åˆ†æ•°ã€‚
- en: But this is a more accurate scoreã€‚s no there's less overfitting in thatã€‚ Thank
    you for watching this videoã€‚ in the next videoã€‚ we're going to see how to actually
    use L1 and L2ã€‚ and also a combination of the twoã€‚ in Carsã€‚ This content changes
    oftenã€‚ So subscribe to the channel to stay up to date on this course and other
    topics and artificial intelligenceã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™æ˜¯ä¸€ä¸ªæ›´å‡†ç¡®çš„è¯„åˆ†ã€‚è¿™é‡Œçš„è¿‡æ‹Ÿåˆæ›´å°‘ã€‚æ„Ÿè°¢æ‚¨è§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•å®é™…ä½¿ç”¨L1å’ŒL2ï¼Œä»¥åŠå®ƒä»¬çš„ç»„åˆã€‚è¿™äº›å†…å®¹ç»å¸¸å˜åŒ–ï¼Œå› æ­¤è¯·è®¢é˜…é¢‘é“ä»¥éšæ—¶äº†è§£æœ¬è¯¾ç¨‹å’Œå…¶ä»–ä¸»é¢˜åŠäººå·¥æ™ºèƒ½ã€‚
- en: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_3.png)'
