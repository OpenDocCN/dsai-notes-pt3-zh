# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P21ï¼šL21- è°ƒæ•´å­¦ä¹ ç‡ä»¥è·å¾—æ›´å¥½çš„ç»“æœ - ShowMeAI - BV12m4y1S7ix

Heyï¼Œ guysï¼Œ welcome to another Pytorch tutorialã€‚ Todayã€‚ I'll show you a simple but powerful technique that you can use in your Pytorch code to improve the optimization process during the trainingã€‚ So what we want to use here is a so called learning rate chatularã€‚ This means that we adjust our learning rate during the training loopã€‚

 either based on the number of epochs or based on validation measurementsã€‚ So let's roll the introã€‚ and then I'll show you how to do thisã€‚ğŸ˜Šã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_1.png)

![](img/4ac8ae1274e2571af0e178a8cbc54a12_2.png)

ğŸ¼ï¼ŒAs you should knowï¼Œ the learning rate is one of the most important type of parameters that you should tweak during the trainingã€‚ and it's highly recommended to adjust the learning rate a little bit during the training to get better resultsã€‚ Pytorarch provides several methods in their API to do thisã€‚ And it's really not that hard to implement this in your codeã€‚

 So let me show you how one thing I want to mention is that by adjusting the learning rateã€‚ Most of the time we want to decrease the learning rateã€‚ So not increase itã€‚ But of courseã€‚ it always depends on your specific problemã€‚ So Pytorarch provides some so-called learning rate scheduleular in the optimization or optim moduleã€‚ So let's go through the API documentation and show some examplesã€‚

 So here we are in the API doc of the optim moduleã€‚ and here in this section how to adjust the learning rate it explains how we canã€‚do thisã€‚ And of courseï¼Œ I will put the link in the description So it tells us that torch Opim Lr schedule provides several methods to adjust the learning rate based on the number of epochsã€‚ or we also have methods that adjust the learning rate based on some validation measurementsã€‚

And we also have this important sentenceã€‚ Learn rate scheduleuling should be applied after Opimrs updateã€‚ So our coach should look something like thisã€‚ So we create a scheduleerã€‚ Then we have our training loop where we have our epochsã€‚ and then we do the training stepã€‚ So this might be something like lost dot backwardã€‚ Then we might have a validation stepã€‚

 and then we call Shaular dot stepã€‚ So it's that simpleã€‚ But of courseã€‚ we have to create a schedulerã€‚ And for thisï¼Œ we have different optionsã€‚ So let's go through the documentationï¼Œ and then I show you the different methods we haveã€‚So the first one is the socalled lambmbda Lrã€‚ So this sets the learning rate of each parameter group to the initial learning rate times a given functionã€‚

 So we might even have different parameter groupsï¼Œ which we don't care about for nowã€‚ but you should know that then you can use multiple functions hereã€‚ So what we want to do is we create a lambda functionã€‚ So this is basically a one line functionã€‚ And this can be dependent on the epochã€‚ So here in this example we divide the epoch by 30ã€‚

 And then we create our lambda Lr with an optrã€‚ and then this lambda function that we create itã€‚ So let's go to the code and let me show you an actual exampleã€‚ So what you want to do here is you want to import torch optim Lr scheduler as Lr schedulerã€‚ğŸ˜Šã€‚

![](img/4ac8ae1274e2571af0e178a8cbc54a12_4.png)

Then we have a learning rateã€‚ So in this caseï¼Œ it's 0ã€‚1 in the beginningã€‚ Then we have a modelã€‚ So in this caseï¼Œ a simple linear modelã€‚ Then you also need an optimizerã€‚ And then here we create this lambda functionã€‚ So this is a one line function and dependent on the epochã€‚ So we divide the epoch by 10ã€‚And then we multiply this with the initial learning rateã€‚

 So in the first epoch we have one divided by 10 times our learning rate in the second epochã€‚ we have two divided by 10 times our initial learning rate and so on and then we create our scheduler like thisã€‚ So we give it the optimizer and the lambda functionã€‚ And in this exampleã€‚ our learning rate is actually increasingã€‚ But in all the next examplesï¼Œ it will be decreasing thenã€‚

 So then here we have our typical training loopã€‚ and first of allã€‚ I want to print optimizer state di to show you how this looks likeã€‚ Then here we mightï¼Œ for exampleã€‚ do lost dot backwardsï¼Œ then we call optimizer step then we might have some validation step and then we call this schedule stepã€‚ and then here I want to print the actual learning rate and we canã€‚

Access this by calling optimizer state ditã€‚ And then here we access the key Para groupsã€‚ and here we only use oneã€‚ So we access this one with index0ã€‚ And then we use the key learning rateã€‚ So this will give us the actual learning rateã€‚ So let's run this code by saying Python lambda L Rã€‚ And then here we see we have the optimizer state dit with the initial learning rate of 01ã€‚

 and then here the first epochã€‚ we have one over 10 times 0ã€‚1ã€‚ So we have thisã€‚ the second epochã€‚ we have this and onã€‚ So I hope that it's clear how this worksã€‚ So let's look at the next exampleã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_6.png)

So the next example is the socalled multiplicative Lrã€‚ This basically works the sameã€‚ But here we multiply the learning rate of each parameter group by the factor given in the specified functionã€‚ So againï¼Œ we create a lambda function that may be dependent on the epochã€‚ So here we just return a valueã€‚ So it actually doesn't change with the number of epochsã€‚

 But now this is multiplicativeã€‚ So each epoch this will be multiplied to the last epochã€‚ So let's go to the code againã€‚ and then let's have a look at how this looks likeã€‚ So here we have the same code as beforeã€‚ But now we use the multiplicative L R with simply this factor of 095ã€‚ And then each time it will be multiplied to the last learning rateã€‚ So let's printã€‚



![](img/4ac8ae1274e2571af0e178a8cbc54a12_8.png)

This oneï¼Œ so let's clear this one and let's run Python multi Lr dot piã€‚ and then we see we have our initial learning rate of 0ã€‚1ã€‚ Then we multiply it with 0ã€‚95 which will give us 0ã€‚095ã€‚ and the next one we again multiply it with 0ã€‚95 and this will give us this learning rate and then this and this and so onã€‚

 So again the big difference here is that here we multiply it with the learning rateã€‚ and with the lambda Lrï¼Œ we just use the initial learning rate and then multiply it with this functionã€‚ So then if we go further then we have the step Lrï¼Œ this is probably the most easiest to understandã€‚ So here it says it decays the learning rate of each parameter group by gamma every step size epoch soã€‚



![](img/4ac8ae1274e2571af0e178a8cbc54a12_10.png)

It might look something like thisã€‚ So we have our step Lr with the optimizer then a step size and then this gammaã€‚ So as I saidï¼Œ typically we want to decrease itã€‚ So we set this to something smaller than oneã€‚ And then here we can see for the first 30 epochs we have our initial learning rate 0ã€‚05ã€‚ then we multiplied it with 0ã€‚1ã€‚ So we have 0ã€‚005 for the next 30 steps and then again we multiplied it with our gammaã€‚

 So we have this learning rate for the next 30 steps and so on So yeahã€‚ this is one of the simplest oneã€‚ but it's actually really powerfulã€‚ So I used this very often myselfã€‚ Then we have this multi step Lr which decay the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestonesã€‚So basicallyï¼Œ hereï¼Œ instead of stepsï¼Œ we just we can give it milestonesã€‚

 and then it will do the sameã€‚ So here for epoch 30ï¼Œ it will apply our gammaï¼Œ then for epoch 80ã€‚ it will apply our gamma and so onã€‚ So this gives us a little bit more variationã€‚ if we don't want to use the same step size all the timeã€‚ Then we have this exponential Lr hereã€‚ it decays the learning rate by gammaï¼Œ every epochã€‚ So this is essentially the same as the step Lrã€‚

 if we use a step size of one hereã€‚ So here we simply apply this every epochã€‚ and then we don't have to care about these stepsã€‚ So yeahï¼Œ that's this oneã€‚ then we also have this cosine uning Lrï¼Œ which I won't go over for nowï¼Œ but you canï¼Œ of courseã€‚ have a look at that yourselfã€‚And yeahï¼Œ but then let's have a look at this one againã€‚

 So this one is called reduce L R on plateauã€‚ So now hereï¼Œ this is not dependent on the epochã€‚ but here insteadï¼Œ it's dependent on some measurements or metricsã€‚ So it reduces the learning rate when a metric has stopped improvingã€‚ So here we want to reduce our learning rate and Py tells us that models often benefit from reducing the learning rate by a factor of 2 between 2 and 10 Once learning stagnateatesã€‚

 This shaular here reads a metric quantityã€‚ And if no improvement is seen for a patient's number of epochsã€‚ The learning rate is reducedã€‚ So what we need here is we need a optrã€‚ we need a mode and a factor and a patientsã€‚ğŸ˜Šï¼ŒSo the mode will be min or maxã€‚ So in min modeã€‚ learning rate will be reduced when the quantity monitored has stopped decreasing in max modeã€‚

 it will be reduced when the quantity has stopped increasingã€‚ Then we need this factorã€‚ So the factor by which the learning rate will be reduced default is 0ã€‚1ã€‚ and we need the patientsã€‚ So how long we wait for no improvementã€‚ So number of epochs with no improvement after which learning rate will be reducedã€‚ For exampleï¼Œ if patients equals2ã€‚ Then we will ignore the first two epochs with no improvement and will only decrease the learning rate after the third epochã€‚

 If the loss still hasn't improvedã€‚ So our code might look something like thisã€‚ we have our optimizerã€‚ We have this reduce L R on plaã€‚With our optimizer and in this caseã€‚ it just uses the default factor and patienceã€‚ and then we have our training loop where we do the training stepã€‚ We calculate the lossã€‚ And after the validationï¼Œ we calculate chatular dot step with this validation lossã€‚

 So if this doesn't get better for the number of epochs we want to waitã€‚ then we reduce the learning rateã€‚ and this really can help our model to stop stagnating and further improve the optimizationã€‚ So yeahï¼Œ then we also have this cyclic Lr and one moreï¼Œ I guess this onecycl Lrã€‚ which I won't go over here as wellï¼Œ but you can check that out for yourselfã€‚ But yeahã€‚

 that's what I wanted to show you for nowã€‚ So againã€‚ you can go to this API documentation and then check out the different optimizersã€‚So yeahã€‚ that's all I wanted to show you for now againã€‚ this can really help your model during the training processã€‚ and it's not that hard to implementã€‚ So just pick a scheduler and try out different ones for yourselfã€‚

 and then call the scheduler That's dot stepã€‚ And yeahï¼Œ then you're good to goã€‚ And I hope you enjoyed this tutorial And if you liked it then please subscribe to the channelã€‚ and then I hope to see you next time byeã€‚![](img/4ac8ae1274e2571af0e178a8cbc54a12_12.png)

![](img/4ac8ae1274e2571af0e178a8cbc54a12_13.png)