# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- TorchText - ShowMeAI - BV1ZZ4y1U7dg

ğŸ¼ã€‚![](img/aaa011e1dc72912baa590f68c0c043cd_1.png)

Hello everyoneï¼Œ welcome to PyTchDeveloper Day for 2020ã€‚My name is Georgeã€‚ I'm a software engineer at Facebookï¼Œ I work for the text domain in Pytotã€‚My job at Facebook is to support the PyToch userï¼Œ especially in the text domain for research to productionã€‚ so in this talk I will go over some major update in 2020 and help you understand how our work can facilitate your research and production for the PyTochã€‚



![](img/aaa011e1dc72912baa590f68c0c043cd_3.png)

So why we want to have a text domain in addition to Pythã€‚Firstã€‚We want to accelerate NLP research and provide some reusful orthogonal and correct building block for cutting edge researchã€‚Based on our knowledge for the text domain and the research communityã€‚We want to work with the both internal team and external open source community to build a pipeline like can better supportã€‚

Both the Facebook products and external researchã€‚Secondã€‚ğŸ˜Šã€‚We want to provide a solution to transfer from research to productionã€‚What we mean here is we integrate those pipeline and module with a wide range of py capabilities such as touchscript quantizationã€‚ distribute data parallel and mobile with this goal we want to have a better support for research to production transition for most end an NLP pipeline thirdly we want to engage with the community and Disc novel technologyã€‚

As people notesï¼Œ the NRP domainã€‚Move very fastã€‚ So the text domain in Pyth team want to develop a good technology understandingã€‚In the NLP area and build a new research collaborationã€‚For the future communityã€‚With those go in mindï¼Œ we provide those easy access to data setsã€‚Text processing pipelineã€‚And some NLP related moduleã€‚So if you have timeï¼Œ please go over those one by oneã€‚

 our row text data setï¼Œ the transformã€‚And our more some NP related moduleï¼Œ I will goã€‚ go over this one by one with youã€‚![](img/aaa011e1dc72912baa590f68c0c043cd_5.png)

Okayï¼Œ so the new data set in To textã€‚We have also rewrite a few existing data set in To text nightly releaseã€‚So hereã€‚For the nightly releaseï¼Œ we consider those new stuff as a prototypeã€‚ So we will release those new data sets with ourã€‚ğŸ˜Šï¼ŒBetter release very soonã€‚So the new data set show here are fully compatible with data loader in Pyã€‚

User will have the flexibility to build the data processing pipeline with some standard tokenizer and vocabulary blocksã€‚Okayï¼Œ here I list all the new data set available to our user in our beta releaseã€‚ at sameã€‚ you may wonderingï¼Œ once I have those raw dataï¼Œ what should I do to convert this raw data intoã€‚

![](img/aaa011e1dc72912baa590f68c0c043cd_7.png)

A tensor like can be used to train a model okayï¼Œ so here we provide some improved performance we provide some data pipeline with improved performanceã€‚With some C+ plus extensionã€‚So the goal here is we want to have an easy transfer to productionã€‚Here's the overview of some end to end pipeline with Ptoch and Torch textã€‚The row data string read and send to a field transform here here you can see like tokenizerã€‚

 vocabularyï¼Œ vector took up and convertive cancerï¼Œ rightï¼Ÿ

So currently we are rewriting this data processing transform as an orthogonal building block with the G portã€‚So after this after this stepï¼Œ we call this a pre processingã€‚ The data are sent to data loader and samplerï¼Œ where we generateã€‚They have bachelorã€‚Then the data are ready for the modelã€‚So we are do our best to write those a building blockã€‚

 individual building blockï¼Œ like so you will have the full flexibility to combine them togetherã€‚And with the C++ extensionï¼Œ we are able to support the Gt for all this transformã€‚And we believe that's a better support for the productionã€‚Okayï¼Œ so now we go to theã€‚The NLP related moduleï¼Œ so we released a new multiha attention module in Tor textã€‚

 so in addition to the dropping replacementï¼Œ if you are using the multiha attention in Pytoch co libraryã€‚ we support the dropping replacement so you can easily switch from a Pytoch multiha attention to To text multiha attentionã€‚In additionã€‚The new multi attention container also for touch scriptã€‚Based on the feedback from our userï¼Œ we add the incremental decoding and the broadcasting supportã€‚

The idea for this new multi high attention container is to facilitate user with some novel research idea under the transformer architectureã€‚Right nowï¼Œ the transformer architecture is very popular across the textï¼Œ audio and vision domainã€‚We hope here like we can provide a very flexible multiha attention module so our user can apply this with different idea here I give you an example like how you switch from the Pych multiha attention to our Tosh text multiha attention containerã€‚

![](img/aaa011e1dc72912baa590f68c0c043cd_9.png)

Just with this a few linesã€‚User has more flexibility to try different custom component with the concept multi attentionã€‚You can putã€‚Custom in projection containerï¼Œ multihaitation containerã€‚O skilled dog productsã€‚ you can apply different idea with this multi hair attention containerã€‚

![](img/aaa011e1dc72912baa590f68c0c043cd_11.png)

Okayï¼Œ lastï¼Œ but not leastã€‚On our websiteï¼Œ we have several text related tutorialsã€‚Including the one to show how to use the new data set for text classification analysisã€‚Please check out those tutorial and have some idea about how to write those into an NLP pipelineã€‚Keep in mindï¼Œ like we will also update this tutorial and to show how to build the N to N pipeline with theã€‚

With the new Tosht library for different NLP task at the endï¼Œ thank you for watching this videoã€‚ and I hope you enjoy the Pytor Develop day for this yearï¼Œ and I will see you aroundã€‚![](img/aaa011e1dc72912baa590f68c0c043cd_13.png)

![](img/aaa011e1dc72912baa590f68c0c043cd_14.png)

![](img/aaa011e1dc72912baa590f68c0c043cd_15.png)