# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL1.6- Transformerï¼šè§£ç å™¨ - ShowMeAI - BV1Jm4y1X7UL

In this videoï¼Œ we'll study the decoder architectureã€‚

An example of a popular decoder only architecture is GPT2ã€‚In order to understand how decoders workã€‚

 we recommend taking a look at the video regarding encodersï¼Œ they're extremely similar to decodersã€‚

One can use a decoder for most of the same tasks as an encoderã€‚

 albeit with generally a little loss of performanceã€‚

Let's take the same approach we have taken with the encoder to try and understand the architectural differences between an encoder and ID decorã€‚

We'll use a small example using three wordsã€‚ We pass them through their decoderã€‚

We retrieve a numerical representation for each worldã€‚Hereï¼Œ for exampleã€‚

 the decoder converts the three words welcomee to NYC and these are three sequences of numbersã€‚

The decoder outputs exactly one sequence of numbers per input wordã€‚

This numerical representation can also be called a feature vector or a feature tensorã€‚

Let's dive in this representationã€‚It contains one vector per word that was passed through the decoderã€‚

Each of these vectors is a numerical representation of the word in questionã€‚ğŸ˜Šã€‚

The dimension of that vector is defined by the architecture of the modelã€‚

Whether a decoder differs from the encoder is principally with its self attention mechanismã€‚

 it's using what is called masked self attentionã€‚Hereï¼Œ for exampleï¼Œ if we focus on the word 2ã€‚

 we'll see that this vector is absolutely unmodified by the NYC wordã€‚

That's because all the words on the rightï¼Œ also known as the right context of the word is maskedã€‚

Rather than benefiting from all the words on the left and rightï¼Œ so the bidirectional contextã€‚

 decoders only have access to a single contextã€‚Which can be the left context or the right contextã€‚

The mask self attention mechanism differs from the self attention mechanism by using an additional mask to hide the context on either side of the wordã€‚

The words numerical representation will not be affected by the words in the hidden contextã€‚

So when should one use a decoder decoders like encoders can be used as standalone modelsã€‚

 as they generate a numerical representationï¼Œ they can also be used in a wide variety of tasksã€‚

 Howeverï¼Œ the strength of a decoder lies in the way a word can only have access to its left contextã€‚

Having only access to their left contactsï¼Œ they are inherently good at tax generationã€‚

 the ability to generate a word or a sequence of wordsï¼Œ given a known sequence of wordsã€‚ğŸ˜Šã€‚

This is known as causal language modeling or natural language generationã€‚

Here's an example of how causal language modeling worksã€‚ We start with an initial wordï¼Œ which is myã€‚

We use this as input for the decoderã€‚ğŸ˜Šï¼ŒThe model outputs a vector of numbersã€‚

 and this vector contains information about the sequenceï¼Œ which is here a single wordã€‚

We apply a small transformation to that vector so that it maps to all the words known by the modelã€‚

 which is a mapping that will seeator called a language modeling headã€‚

We identify that the model believes that the most probable following word is nameã€‚

We then take that new word and add it to the initial sequenceã€‚From myï¼Œ we are now at my nameã€‚

This is where the autoregressive aspect comes inã€‚ğŸ˜Šã€‚

Outtoregressive models reuse their past outputs as inputs and the following stepsã€‚Once againã€‚

 we do the exact same operationã€‚We cast that sequence through the decoder and retrieve the most probable following wordã€‚

In this caseï¼Œ it is the word isã€‚We repeat the operation until we're satisfiedã€‚

Starting from a single wordï¼Œ we've now generated a false sentenceã€‚ We decided to stop thereã€‚

 but we could continue for a whileã€‚ GP T2ï¼Œ for exampleï¼Œ has a maximum context size of 1024ã€‚

 We could eventually generate up to 1024 wordsï¼Œ and the decoder would still have some memory of the first words and as sequenceã€‚

ğŸ˜Šã€‚

![](img/dfd8577ffd03c9ab167585435c2c2709_1.png)

å—¯ã€‚

![](img/dfd8577ffd03c9ab167585435c2c2709_3.png)