- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P11ÔºöL11-
    ÈöèÊú∫Ê£ÆÊûó - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to your new machine learning from Sct tutorial„ÄÇ Today„ÄÇ
    we are going to implement a random forest using only built in Python modules and
    Ny„ÄÇ The random forest algorithm is one of the most powerful and most popular algorithm„ÄÇ
    So I'm very excited that we can finally implement it in this tutorial in the last
    tutorial„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I explained how a single decision tree works„ÄÇ So if you haven't watched the
    previous tutorial yet„ÄÇ then please do so„ÄÇ because our random forest model and
    also the implementation is based on the decision tree model from the last time„ÄÇüòäÔºåSo
    if you have understood the decision treesÔºå then the approach of the random forest
    is very easy to understand„ÄÇIf we have a look at this image hereÔºå then this shows
    the whole idea„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the idea is to combine multiple trees into a forest„ÄÇ so we train multiple
    trees and each tree gets a random subset of the training data„ÄÇ thus the word random„ÄÇ
    We then make a prediction with each of the trees at the end„ÄÇAnd we make a maturity
    vote then to get the final prediction„ÄÇSo this is the whole idea„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the random forest has some advantages compared to only one treeÔºå for example„ÄÇ
    by building more treesÔºå we have more chances to get the correct prediction and
    we also reduce the chance of overfitting with a single tree so typically the accuracy
    of a random forest is higher than with a single tree and that's why it's so powerful„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå yeahÔºå now we can jump right to the implementation„ÄÇ SoÔºå of courseÔºå we import„ÄÇ![](img/657eaccf75713176fcbe8f6ff6eea819_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: NampyÔºå SÔºå and P„ÄÇAnd then we also import the decision tree class from the last
    time„ÄÇ so we say from decision treeÔºå import decision tree„ÄÇAnd now we can start„ÄÇ
    We create our class decision3„ÄÇAnd or sorryÔºå nowÔºå now we have the random forest„ÄÇ
    So now we create our random forest class„ÄÇAnd this will„ÄÇGet an in it„ÄÇSo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and this will get the number of trees we want to have in our forest„ÄÇ So let's
    say a number of trees equals 100 by default„ÄÇAnd then it also gets all the parameters
    from our decision tree initializeza„ÄÇ So it gets the minimum samples required for
    a split„ÄÇ It gets the maximum depth„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And it gets an optional number of features for some more randomness„ÄÇSo let's
    just copy and paste this here„ÄÇ and then we store all of them„ÄÇ So we say self dot
    number„ÄÇOf trees equals and treesÔºå self dot min sample split equals min sample
    split self dot max depth equals max depth and self dot N„ÄÇFats equalsÔºå and feats„ÄÇAnd
    then we implement our predict and our fit methodsÔºå so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The predict method has the„ÄÇTest data„ÄÇ And we start with the fit method„ÄÇ So we
    say fit„ÄÇSelf„ÄÇ and this will have the training data and the training labels„ÄÇAnd
    one more thing that we want to haveÔºå and I can put it here first so we want to
    have an empty array of trees where we want to store each single tree that we now
    are going to create„ÄÇ So we say self dot trees equals and this will be an empty
    list„ÄÇAnd then in the fit method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we want to make sure that the list is empty again„ÄÇAnd nowÔºå we start„ÄÇTraining
    our tree„ÄÇ So we say four underscore because we don't need this in range self dot
    number of trees„ÄÇ And now we create our tree„ÄÇ So we say3 equals decision  tree„ÄÇAnd
    this will get all the parameters„ÄÇ So it gets min sample split equals self dot
    min sample split„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then it will have the max depth equal self dotÔºå max depth and the number of
    features equal self dot„ÄÇNumber of features„ÄÇAnd„ÄÇNowÔºå what we want to do is we want
    to give our tree a random subset„ÄÇ So let's define a global function here„ÄÇ Let's
    call this„ÄÇ This is also called bootstping„ÄÇ So let's call this„ÄÇBoott sampleÔºå which
    will get X and y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we just or first look at how many different number of samples we have„ÄÇ
    So n samples equals x dot shape„ÄÇSo as alwaysÔºå this is a Ny and D where the first
    dimension is the number of samples and the second dimension and number of features„ÄÇAnd
    now we make a random choice„ÄÇ So we say indices equals numpy random choice„ÄÇAnd
    here we put in the number of samples as integer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This means that it will make a random choice between 0 and the number of samples„ÄÇ
    So our indices lie in this rate rangeÔºå and the size will be of size number of
    samplesÔºå too„ÄÇBut we also say replace equals true„ÄÇ So this means that some of the
    indices can be there multiple times„ÄÇ and others get dropped„ÄÇ So we randomly drop
    some of the samples and use only a subset„ÄÇAnd then„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we return„ÄÇThe x„ÄÇÂì¶„ÄÇThese indices„ÄÇAnd alsoÔºå the why of these indices„ÄÇSo now we
    only have these selected samples„ÄÇüéºAnd now we can train our tree with this„ÄÇ So
    first„ÄÇ we say x sample and y sample equals bootstrap sample with X and y„ÄÇ And
    then we say tree dot fit„ÄÇX sampleÔºå and y„ÄÇSample„ÄÇAnd then we simply append this
    to our tree list„ÄÇ So we say self dot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: T st a pent„ÄÇ3„ÄÇAnd now we are done with the training phase„ÄÇ And now when we predict
    it„ÄÇ So we make a prediction with each of our trees„ÄÇ So we say tree pres or tree
    predictions equals„ÄÇAnd here I will use a list comprehension and then convert this
    to a nuier array„ÄÇ So here I say tree dot predict„ÄÇX 43 in self dot trees„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So for each trees now we make what we call the tree predict method„ÄÇ And now
    we want to do the maturity vote„ÄÇ But now we have to be careful because what we
    get here is„ÄÇLet's sayÔºå for exampleÔºå we have three trees and four samples„ÄÇAnd then
    let's say our first tree for simplicity just does once as predictions„ÄÇ So for
    each sample„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it will have a one here„ÄÇ and the second tree just makes zeros„ÄÇ So we have zeros„ÄÇ
    and then the third tree also just predicts one„ÄÇSoÔºå and then againÔºå this would
    be„ÄÇAn array„ÄÇ This would be an arrayÔºå and this would be an array in this array
    then„ÄÇ But„ÄÇ and now we want to do the maturity vote„ÄÇ So now what we„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Actually one is we want a arrays that look like thisÔºå so we want to have 10Ôºå1Ôºå10Ôºå1Ôºå101
    and 101„ÄÇSo„ÄÇ this one„ÄÇThis one from all of the trees„ÄÇWe want to have the corresponding
    predictions„ÄÇ So we convert it to this structure„ÄÇ And there's a very nice function
    from Nmpy that is doing exactly this„ÄÇ So we say tree pres equals Nmpy swap„ÄÇAs„ÄÇÂóØ„ÄÇWith
    this three predictions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we swap axis0 and axis 1„ÄÇ So this is doing exactly this„ÄÇAnd now we
    can do the maturity vote„ÄÇ So we say why prediction equals„ÄÇ And now we predict
    the most common label for each of these three predictionsÔºå so„ÄÇNowÔºå if we have
    or„ÄÇ1Ôºå0Ôºå1Ôºå1Ôºå0Ôºå1Ôºå1Ôºå0Ôºå1„ÄÇ So now we go over them„ÄÇAnd make a maturity vote then over
    them and then over them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we use less comprehension again„ÄÇ And we sayÔºå most common label„ÄÇOf a tree
    prediction for each tree prediction in tree predictions„ÄÇAnd then we convert this
    to a nuy„ÄÇArray and return it„ÄÇAnd now the only thing left that we need is the most
    common label function„ÄÇ And we also needed this in the decision tree class„ÄÇ So
    here we have the most common label function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: As a class function„ÄÇ So hereÔºå as you seeÔºå we need this a lot of time„ÄÇ So it
    might be better to do this as a global function„ÄÇSo we put this here and you might
    even put it in another file and call this from a helpback class or something„ÄÇ
    but we just put it hereÔºå so„ÄÇI will not explain this again if you don't know how
    this works„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then please watch the last tutorial so we don't need self anymore„ÄÇAnd we also
    need to import from collections„ÄÇImport the counter module„ÄÇAnd now we can do the
    maturity vote and now we are done„ÄÇ so I have a little test script here to test
    our class„ÄÇ so I will import the breast cancer data set from the Ecal learn module„ÄÇ
    Then I will generate some training and test labelsÔºå Then I will create our random
    forest instance„ÄÇAnd here I just use three trees because training might take some
    time„ÄÇ and we didn't optimize our codeÔºå so„ÄÇIn our videoÔºå I just use three now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then I will fit the data and I will predict the test data and then calculate
    the accuracy„ÄÇ So let's run this and hope that everything is working„ÄÇAnd„ÄÇWe made
    a mistakeÔºå so we say self„ÄÇOh„ÄÇ we want to append it to our treesÔºå of course„ÄÇ And
    now one more try„ÄÇFingers crosseds„ÄÇAnd now we have the accuracy„ÄÇ So now we see
    that our model is working„ÄÇ And yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I hope you understood everything„ÄÇ And if you liked it„ÄÇ please subscribe to the
    channel and see you next timeÔºå bye„ÄÇ![](img/657eaccf75713176fcbe8f6ff6eea819_3.png)
  prefs: []
  type: TYPE_NORMAL
