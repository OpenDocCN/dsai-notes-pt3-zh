- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P3ÔºöL3- Á∫øÊÄßÂõûÂΩí
    - ShowMeAI - BV1wS4y1f7z1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to a new tutorial„ÄÇ This is the second video of the machine
    learning from Sc tutorial series„ÄÇ In this seriesÔºå we are going to implement popular
    machine learning algorithms using only built and Python modules and nuy„ÄÇ TodayÔºå
    we are going to implement the linear regression algorithm„ÄÇ So let's talk about
    the concept of linear regression first„ÄÇ So in regression„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we want to predict continuous valuesÔºå whereas in classification„ÄÇ we want to
    predict a discrete value like a class label 0 or one„ÄÇ So if we have a look at
    this example plot„ÄÇ Then we have our dataÔºå the blue dots„ÄÇ and we want to approximate
    this data with a linear function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that's why it's called linear regression„ÄÇ So we use a linear function to predict
    the values„ÄÇüòä„ÄÇSo we can define the approximation as y hat equals w times x plus
    B„ÄÇ So this is the line equation where W or our weights is the slope and B is the
    bias or just the shift along the Y x's in the2 D case„ÄÇSo this is the approximation„ÄÇ
    And now we have to come up with this W and the B„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And how do we find this„ÄÇSo for thisÔºå we define a cost function„ÄÇ And in linear
    regression„ÄÇ this is the mean squared error„ÄÇ So this is the„ÄÇDiffence„ÄÇBetween the
    actual value and the approximated value„ÄÇ So the actual value for this„ÄÇ we need
    training samples„ÄÇAnd then we square this difference and sum over all the samples
    and then divide by the number of samples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this wayÔºå we get the mean error„ÄÇThis is the cost function of theÔºå so this
    is the error„ÄÇ And„ÄÇ of courseÔºå we want to have the error as small as possible„ÄÇ
    So we have to find the minimum of this function„ÄÇ And how do we find the minimum„ÄÇ
    So for this„ÄÇ we need to calculate the derivative or the gradient„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we calculate the gradient of our cost function with respect to W and with
    respect to B„ÄÇSo this is the formula of the gradient„ÄÇ Please check this for yourself„ÄÇ
    and I will also put some links in the description with some further readings„ÄÇ
    but I will not go into detail now„ÄÇAnd now with this gradient„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we use a technique that is called gradient descent„ÄÇ So this is an iterative
    method to get to the minimum„ÄÇ So if we have our object or our cost function hereÔºå
    then we start somewhere„ÄÇ So we have some initialization of the weights and the
    bias„ÄÇAnd then we want to go into the direction of the steepest descent„ÄÇAnd the
    steepest de is also the gradient„ÄÇ So we want to go into the direction of the„ÄÇ
    into the negative direction of the gradient„ÄÇAnd we do this iteratively until we
    finally reach the minimum„ÄÇAnd with each iterationÔºå we have a update rule for the
    new weights and new bias„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the new W is the old W minus alpha times the derivative„ÄÇüòä„ÄÇSo minus because
    we want to go into the negative direction„ÄÇAnd then this alpha is the so-called
    learning rate„ÄÇ And this is an important parameter for our model„ÄÇ So the learning
    rate defines how far we go how far we go into this direction with each iteration
    step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå for exampleÔºå if we use a small learning rateÔºå then it may take longer„ÄÇ but
    it can finally reach the minimumÔºå And if we reach or if we use a big learning
    rate„ÄÇ then it might be fasterÔºå but it might also jump around like this and never
    find the minimum„ÄÇ So this is an important parameter that we have to specify„ÄÇ and
    please keep that in mind„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now that we know our update rulesÔºå I've written the formulas for the derivatives
    again here and„ÄÇThen simplify them a little bit„ÄÇ Please check that for yourself„ÄÇ
    So these are the formulas for the update rules and the derivatives„ÄÇ And this is
    all we need to know„ÄÇ So now we can get started„ÄÇ So now let's define a class called
    linear regression„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4667748848c4687cace578f8ebabf6_1.png)'
  prefs: []
  type: TYPE_IMG
- en: And this willÔºå of courseÔºå has an in it method„ÄÇOr double underscore score in
    it„ÄÇAnd then it has self„ÄÇ and it gets the learning rate„ÄÇ And I will give this a
    default value„ÄÇ So usually this is a very small value„ÄÇ So I will give it0001„ÄÇ And
    then I will give it a number of iterations„ÄÇ So how many iterations we use in our
    gradient decent method„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I will also give this a default value„ÄÇ So I will say this is 1000„ÄÇ And then
    I will simply store them here„ÄÇ So I will say self L R equals L R and self„ÄÇAnd„ÄÇItders
    equals and its„ÄÇ And then laterÔºå we have to come up with the weights„ÄÇ But here
    at the beginningÔºå I will simply say self weights equals none and self dot bias
    equals none„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we have to define two functionsÔºå and we will follow the conventions
    of other machine learning libraries here„ÄÇ So we will define a fit methodÔºå which
    takes the training samples and the labels for them„ÄÇ So this will involve the training
    step and the gradient descent„ÄÇAnd then we will define a predict method„ÄÇSo then
    when it gets new„ÄÇTest samples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then it can approximate the value and return the value„ÄÇ So these are the functions
    we have to implement„ÄÇ And before we go on„ÄÇ let's have a quick look at the data
    X and Y„ÄÇ So how does this look„ÄÇAnd for this„ÄÇ I've written a little example scriptÔºå
    and I used the psychic learn module to generate some example data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I will split the data into training and test samples and also training and
    test labels„ÄÇ So first of allÔºå let's have a look at how this data looks„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: So here is the plot„ÄÇ So this is how our data looks„ÄÇ And now we want to find
    a function somewhere here that approximates the value„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: And let's have a look at the„ÄÇShape of our x and y„ÄÇ So let's run this„ÄÇ And I
    don't want this plot here anymore„ÄÇÂóØ„ÄÇSo if we see that our x is a N D array of
    size 80 by1„ÄÇ And this is because I put in hereÔºå I want to have 100 samples„ÄÇAnd
    one feature for each sample„ÄÇ And then I will split this„ÄÇ So our training samples
    only has 80 samples in it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is a ND array of size 80 by one and our training labels is just a 1
    D vector of also of size 80„ÄÇ So for each training sampleÔºå we have one value„ÄÇSo
    this is how our data looks„ÄÇ And now let's continue„ÄÇ So let's implement the fit
    method„ÄÇ SoÔºå as I said„ÄÇ we need to implement the gradient decentcent method here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and the gradient decent always needs to start somewhere„ÄÇ So we need to have
    some initialization„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4667748848c4687cace578f8ebabf6_8.png)'
  prefs: []
  type: TYPE_IMG
- en: So„ÄÇLet's do„ÄÇ let's in it our parameters„ÄÇ And for this first„ÄÇ let's get the number
    of samples and the number of features„ÄÇ We can get this by saying this is x dot
    shape„ÄÇAnd then we simply initialize all the weights with0„ÄÇ So we can say self
    weights equals nuy0s„ÄÇOf size and feature„ÄÇAnd features„ÄÇ So for each component„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we put in a0 and self du bias equals 0„ÄÇ This is just a value„ÄÇ So you can also
    use random values hereÔºå but 0 is just fine„ÄÇSo let's use 0 here„ÄÇ and then we use
    the gradient descent„ÄÇ So this is an iterative process„ÄÇ So we use a fall loop„ÄÇ
    So for IÔºå oh actuallyÔºå we don't need this„ÄÇ So for underscore in range and then
    self dot and its„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And nowÔºå what we need„ÄÇWe first have to approximate„ÄÇOr let's have a look at the
    formula again„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_10.png)
  prefs: []
  type: TYPE_NORMAL
- en: SoÔºå the formula for our„ÄÇAnd new weights is the old weight minus the learning
    rate times the derivative„ÄÇAnd the derivative with respect to W is one over n„ÄÇAnd
    then we have the sum and the sum over two times X I„ÄÇTimesÔºå and then„ÄÇThe difference
    here of this approximated and the actual values„ÄÇ So let's first„ÄÇFirst„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: calculate this approximation„ÄÇ So we have this formula here„ÄÇThe approximation
    is„ÄÇThe weights times our x plus the bias„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_12.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's do this„ÄÇ Let's sayÔºå and let's call this why„ÄÇPredic it equals„ÄÇ And then
    we can use N dot dot„ÄÇAnd then x and self dot weights„ÄÇPlus„ÄÇBsÔºå so this will multiply
    the x with the weights„ÄÇAnd now that we have the approximationÔºå we can calculate
    the derivative with respect to W„ÄÇ And this is„ÄÇ let's againÔºå have a look at this
    formula„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4667748848c4687cace578f8ebabf6_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 1 over n„ÄÇAnd then„ÄÇThe sum„ÄÇ and then inside the sumÔºå we have the product of x
    times this„ÄÇSoÔºå we say„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_16.png)
  prefs: []
  type: TYPE_NORMAL
- en: 1 over„ÄÇAnd samples„ÄÇ So we already got the number of samples here„ÄÇAnd then times„ÄÇ
    And then we have the sum product„ÄÇ So this is nothing elseÔºå but also the dot product„ÄÇ
    So N P dot dot„ÄÇ But now we have to be careful„ÄÇ So what we did here„ÄÇÂóØ„ÄÇHere we multiply
    each weight component with the feature vector component and sum it up„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we do this for all samples and then get one value for each sample„ÄÇAnd here
    we want to get one value for each feature vector component„ÄÇ So we multiply each
    sample with a predicted value and sum it up„ÄÇSo„ÄÇ and then we do this for each feature
    vector component and get one value for each component„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the other way round„ÄÇ So this is along the other axis„ÄÇ And we can„ÄÇ
    So we have to use x dot transposed hereÔºå x dot T„ÄÇAnd this is the dot product of
    the transposed X„ÄÇ And then we have y predicted minus the actual y„ÄÇ So please check
    the nuy dot function for yourself„ÄÇSoÔºå this is the„ÄÇThe of the W and the derivative
    of the bias is also one„ÄÇ Or again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's have a look at the formula„ÄÇ So this is the same„ÄÇ except that we don't
    have the x here„ÄÇ So this is one over n„ÄÇ and then just the sum„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: Of this difference„ÄÇ And by the wayÔºå I„ÄÇI left the two out„ÄÇ So this is just a
    scaling factor that we can omit„ÄÇSo here one over n and then the sum over the difference„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_20.png)
  prefs: []
  type: TYPE_NORMAL
- en: So againÔºå one over number of samples„ÄÇ And then we can say nuy dot sum and the
    sum of y predicted minus actual y„ÄÇSo this areÔºå these are our derivatives„ÄÇ And
    now we update our weights„ÄÇ So we say self weights minus equals self dot learning
    rate times this„ÄÇDivative„ÄÇAnd self„ÄÇ the bias equals self„ÄÇMinus equals self dot
    learning rate times„ÄÇThe derivative„ÄÇAnd„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this is the gradient descent„ÄÇAnd now we need the predict method„ÄÇ SoÔºå againÔºå
    we approximate„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: We approximate the values with this formulaÔºå so„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_24.png)
  prefs: []
  type: TYPE_NORMAL
- en: We already have this here„ÄÇ So this is„ÄÇThe dot product of x and the weightsÔºå
    plus the buyers„ÄÇAnd then„ÄÇ we simply returned this„ÄÇSo this is the whole implementation
    that we needÔºå and„ÄÇI forgot to import Ny„ÄÇ of course„ÄÇ So I sayÔºå let's sayÔºå import
    Ny S and P so that we can use this„ÄÇAnd nowÔºå let's test this„ÄÇSo„ÄÇÂóØ„ÄÇLet's import
    this class„ÄÇ So let's sayÔºå from„ÄÇLinear regression„ÄÇImport linear regression„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then create some regresso equals„ÄÇLinear regression„ÄÇAnd then we say regressor
    dot fit„ÄÇ and we want to fit the training samples and the training labels„ÄÇAnd then
    we can say we can get predicted values equals„ÄÇRegress or„ÄÇDot predict„ÄÇ And now
    we want to predict the„ÄÇTest„ÄÇÂóØ„ÄÇSamples„ÄÇAnd now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: in order to calculate the or to see how our model performs„ÄÇ now we can't„ÄÇ we
    can't use the accuracy measure„ÄÇ But here we use the mean squared error„ÄÇ So as
    I said„ÄÇ this is our cost functionÔºå the mean square error that tells us how big„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_26.png)
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the actual value and the approximated value is„ÄÇ so let's
    define the mean squared arrow„ÄÇ Let's say theef MÔºå EÔºå and this will get„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_28.png)
  prefs: []
  type: TYPE_NORMAL
- en: The actual valuesÔºå and the predicted values„ÄÇAnd this is„ÄÇThe numpy hereÔºå we can
    use Nampy mean„ÄÇAnd thenÔºå simplyÔºå the difference„ÄÇTo the power of two„ÄÇ So why true„ÄÇMus
    y predicted to the power of2„ÄÇAnd then we want to return this„ÄÇSoÔºå let's see„ÄÇ let's
    say„ÄÇM S E value equals ME„ÄÇOff„ÄÇW„ÄÇTest„ÄÇAnd the predicted„ÄÇValuesÔºå and let's print
    this„ÄÇSo now if we run this„ÄÇÂóØ„ÄÇSeeÔºå it's not running„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Bias is not defined„ÄÇ So let's say what we„ÄÇWhat did we miss hereÔºå dot biasas„ÄÇAnd
    let's run this again„ÄÇSo„ÄÇLÔºå27„ÄÇOhÔºå sorry„ÄÇ I copied this and forgot this yearÔºå too„ÄÇ
    So next try„ÄÇNow„ÄÇ we see that our„ÄÇPerformanceÔºå so the mean squaredarrow is 783„ÄÇ
    So this is pretty high„ÄÇ So let's use another learning rate here„ÄÇ So let's say
    L R equals„ÄÇt0Ôºå1„ÄÇAnd let's run this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we see that our arrow is smaller„ÄÇ And let's actually plot this„ÄÇSo„ÄÇLet's
    plot„ÄÇFirst„ÄÇ with the original learning rateÔºå let's see our„ÄÇHow the plot looks„ÄÇSoÔºå
    now„ÄÇOur plot looks like this„ÄÇ So it's„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_30.png)
  prefs: []
  type: TYPE_NORMAL
- en: Almost like the right slopeÔºå but not exactly„ÄÇ And now let's have the other learning
    rate„ÄÇ So let's use this learning rate„ÄÇ and let's run this„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_32.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4667748848c4687cace578f8ebabf6_33.png)'
  prefs: []
  type: TYPE_IMG
- en: And now our plot looks like this„ÄÇ and this looks pretty goodÔºå actually„ÄÇ So this
    is a pretty good„ÄÇFit a pretty good approximation of this data with a linear function„ÄÇ
    So we see that our implementation is working„ÄÇ and I hope you enjoyed this tutorial
    and see you in the next tutorial„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4667748848c4687cace578f8ebabf6_35.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ce4667748848c4687cace578f8ebabf6_36.png)'
  prefs: []
  type: TYPE_IMG
- en: Bye„ÄÇHiÔºå everybody„ÄÇ Welcome to a new machine learning from scratch tutorial„ÄÇ
    Today„ÄÇ we are going to implement logistic regression using only built and Python
    modules and Nmpy„ÄÇ If you haven't watched my previous video about linear regression„ÄÇ
    I will highly recommend to watch it first„ÄÇ Since I explain some of the concepts
    a little bit more in detail there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But I will try to cover all the important concepts again in this video„ÄÇ So let's
    talk about the concepts of logistic regression„ÄÇüòäÔºåSo„ÄÇAs you rememberÔºå in linear
    regression„ÄÇ we model our data with a linear function W times x plus B„ÄÇSo this
    will output continuous values„ÄÇAnd nowÔºå in logistic regressionÔºå we don't want continuous
    valuesÔºå but we want a probability„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And in order to model this probabilityÔºå we apply the sigmite function to our
    linear model„ÄÇSo the sigmoid function is one over one plus the exponential function
    of minus x„ÄÇSo this is the soid function and Min and x„ÄÇ in our caseÔºå is then our
    our linear model„ÄÇSo„ÄÇ and this isÔºå this will output a probability between 0 and1„ÄÇ
    So if we plot the sigmoid function„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then you can see this is between 0 and one„ÄÇSo with this function„ÄÇ we can model
    a probability of our data„ÄÇAnd nowÔºå with this„ÄÇApproximate it„ÄÇOutputÔºå and we can
    then„ÄÇWe must then come up with the parameters W„ÄÇ So our weight and our bias„ÄÇAnd
    how do we do that„ÄÇ So againÔºå I already explained this in the previous video„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we apply a method that is called gradient descent„ÄÇ So first of allÔºå we need
    a cost function„ÄÇ And here we don't have the mean squared arrow any„ÄÇ but we use
    a function that is called the cross entropy„ÄÇ So this is the formula„ÄÇI will not
    go into detail about thisÔºå but I will put some further readings in the description„ÄÇSo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: with this formula„ÄÇWhat we doÔºå we want to optimize this with respect to our parametersÔºå
    W and B„ÄÇSo we use gradient descent„ÄÇ and this means we start at some point and
    then iteratively update our parameters„ÄÇ So we have to calculate the derivative„ÄÇAnd
    then go into the direction of this derivative until we finally reach the minimum„ÄÇSoÔºå
    and then we also need to define a so called learning rate„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the learning rate determines how far we go into this direction with each
    step„ÄÇ So this is an important parameter„ÄÇ It shouldn't be too high since then it
    might jump around and never find the minimum„ÄÇ but it also shouldn't be too low„ÄÇSorry„ÄÇAnd
    now with this gradient descent„ÄÇWhat we have is we have our update rules„ÄÇ So our„ÄÇOur
    weight is our new weight is the old weight minus„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: because we want to go into the negative directionÔºå minus our learning rate times
    the derivative„ÄÇAnd the same is for our bias„ÄÇAnd then here are the formulas that
    we need for our derivatives„ÄÇ So these are the formulasÔºå and they are actually
    the same derivativesÔºå like in„ÄÇLar regression„ÄÇ so you can check the math behind
    it yourself„ÄÇ I will also put some links in the description„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is all we need now to get startedÔºå and now we can implement our„ÄÇ![](img/ce4667748848c4687cace578f8ebabf6_38.png)
  prefs: []
  type: TYPE_NORMAL
