- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPySpark Â§ßÊï∞ÊçÆÂ§ÑÁêÜÂÖ•Èó®ÔºåÂ∏¶‰Ω†Áé©ËΩ¨Python+SparkÂ§ßÊï∞ÊçÆÊìç‰Ωú‰∏éÂàÜÊûêÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P1ÔºöL1- Pyspark
    ‰ªãÁªçÂíåÂÆâË£Ö - ShowMeAI - BV1sL4y147dP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/ee846c25d771f30e25168217f8991826_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Hello all„ÄÇ My name is Krisna and welcome to my YouTube channel„ÄÇ So guys„ÄÇ we
    are going to start Apache Spark seriesÔºå and specifically if I talk about Spark„ÄÇ
    we will be focusing on how we can use Spk with Python„ÄÇ So we are going to discuss
    about the library call Pi Spk„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We will try to understand everything why Spk is actually required and probably
    we also try to cover a lot of things„ÄÇ There is something called as Mlib Spk MlibÔºå
    which will basically say that how you can apply machine learning„ÄÇ you know in
    Apache Spk itself with the help of this Spk API called as Pipark libraries and
    apart from that we'll also try to see in the future once we understand the basics
    of the Pipark library„ÄÇ how we can actually preprocess our data setÔºå how we can
    use the Pipar data frames we'll also try to see„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: How we can implement or how we can use Pipark in cloud platforms like data breaksÔºå
    AmazonÔºå Aws„ÄÇ you knowÔºå so all these kind of clouds will try to cover and remember
    Apache spark is quite handy„ÄÇ Let me tell you just let me just give you some of
    the reasons So why Aache spark is pretty much good because understand„ÄÇ suppose
    if you have a huge amount of data Okay suppose if I say that I'm having 64 gb
    dataÔºå1„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 8 gb dataÔºå you knowÔºå we may have some kind of systemsÔºå a standalone systemsÔºå
    you know„ÄÇ where we can have 32 gb of Ram probably 64 gb of Ram right now in the
    workstation that I'm working in it has 64 gb Ram„ÄÇ So max to MacÔºå it can directly
    upload a data set of 32 gb„ÄÇ48 gb But what if if we have a data set of 1„ÄÇ8 g you
    know that is the time guys„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we don't just depend on a local system will try to preprocess that particular
    data perform any kind of operation in distribute„ÄÇuted systems right distributeded
    system basically means that all there will be multiple systems„ÄÇ you knowÔºå where
    we can actually run this kind of jobs or process or try to do any kind of activities
    that we really want and definitely Apache Sp will actually help us to do that
    and this has been pretty much amazing And yes„ÄÇ people wanted this kind of videos
    a lot So how we are going to go through this specific playlistes that will try
    to first of all„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: start with the installation„ÄÇ We'll try to use Pipark because that is also Apache
    spark it is a spark API with Python when you are actually working with Python
    we basically use Pipark library And yes„ÄÇ we can also use spark with other programming
    languages like Java scale and all right and will try to understand from basics
    you know from basic how do we read a data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: how do we connect to a data source probably how do we play with the data frames
    know in this Apache Sp that is your Pi„ÄÇüòäÔºåAlso they provide you data structures
    like data frames„ÄÇ which is pretty much similar to the pans data frameÔºå but yes„ÄÇ
    different kind of operations are supported over there which we'll be looking one
    by one as we go ahead and then we'll try to enter into Mlib Apache Sp Mlib so
    basically it is called as spark Mlib which will actually help us to perform machine
    learning which where we'll be able to perform some machine learning algorithm
    task where we'll be able to do regression classification clustering and finally
    well try to see how we can actually do the same operation in cloud where I'll
    try to show you some examples where we will be having a huge data set we will
    try to do the operation in the clusters of system you know in a distributed system
    and we'll try to see how we can use spark in that right so all those things will
    basically get covered now some of the advantages of Apache Sp and white is very
    much famous because it runs workloads hundred multiplied by„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: times fasterÔºå you knowÔºå that basically means And if you know about big data
    guys„ÄÇ when we talk about big dataÔºå we are basically talking about huge data setÔºå
    right„ÄÇ and there if you have heard of this terminology called as map produceÔºå
    right„ÄÇTrusush mean Apache Spk is much more faster 00 times faster than map produce
    also„ÄÇ okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and it is some of the more advantage that it is ease of use„ÄÇ you can write application
    quickly in Java scalear Python or RÔºå as I said„ÄÇ well be focusing on Python where
    we'll be using a library called Pipark„ÄÇ Then you can also combine SQL streaming
    and complex analytics„ÄÇ When I talk about complex analytics„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm basically talking about this Mlib machine learning libraries that will work
    definitely well with the Apache Spk„ÄÇAnd Apache sparks can run on Hadoop Apache
    meso Knet standalone in or in the clouds cloud„ÄÇ different types of cloud guys
    when I talk about Aws data breaksÔºå all these things„ÄÇ we can definitely work„ÄÇ and
    it actually runs in a cluster cluster basically means in a distributed So these
    are some of the examples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now if I go with respect to which version ofpar spark will be usingpar 3„ÄÇ1„ÄÇ1
    will be using will try to work and if you just go and search for here you can
    see SQL and data frames and all here you can see spark streaming machineli that
    is called as machine learning in all And apart from that if I go and see the overview
    here you can see that Apache spark is a fast and general purpose cluster computing
    system„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it provides highle APIs in scale or Java and Python that makes parallel job
    easy to write an optimized engine that supports general competition graph„ÄÇ So
    it is basically to work with huge amount of data„ÄÇüòäÔºåAnd shortÔºå you know„ÄÇ and that
    is pretty much handy„ÄÇ will'll try to work„ÄÇ NowÔºå if I go and search for this park
    in Python„ÄÇ you knowÔºå this page will get basically go open and this things will
    try to discuss how to install it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And in this video will try to install the Pipar library„ÄÇ if I talk about Pipar
    library you'll be able to see that Pipar library is pretty much amazing„ÄÇ this
    library is if you really want to work if you want to work this spark functionality
    with Python„ÄÇ you basically use the specific library and let's proceed and let's
    try to see that how we can quickly„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåYeahÔºå how we can quickly you know install the specific libraries and check
    out like what all things we can actually do„ÄÇ OkayÔºå so all these things we'll try
    to see„ÄÇ So let's begin„ÄÇ please make sure that you create a new environment when
    you are working with Pi So I have created a new environment called my and here„ÄÇ
    first of allÔºå I'll try to install the Pi libraries„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I'll just install Pipar and let's see in this we'll focus on installation„ÄÇ
    we'll focus on reading some data sets and try to see that what all things we can
    actually do„ÄÇ Okay and after doing this what we can actually do is that you can
    see that our Pi has been installed in order to check whether the installation
    is perfect or not I'll just write import Pi So this this looks perfectly find
    it is working know we are able to see that the Pi is basically installed properly
    Now you may be facing some kind of problems„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: is with respect to Pipar So that is the reason why I'm telling you create a
    new environment If you' are facing some kind„ÄÇüòäÔºåIsueÔºå just let me know what is
    the error that you are getting probably writing in the comment section„ÄÇ OkayÔºå
    now let's do one thing„ÄÇ I'll just open Excel sheet Okay„ÄÇ and probably I'll just
    try to create a„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee846c25d771f30e25168217f8991826_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Some data setsÔºå I'll say nameÔºå probablyÔºå I'll just say name„ÄÇAnd HÔºå right„ÄÇ And
    suppose my name over here that I'm going to write is squ and also 31„ÄÇ I'm going
    to say Suan S„ÄÇRightÔºå Shoan Shu„ÄÇ I will just sayÔºå okayÔºå 30„ÄÇ and probably„ÄÇ I'll
    just write some more names like Sunny„ÄÇ Probably I'll also give the data as 29„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this three data will just try to see how we can read this specific file„ÄÇ
    Okay„ÄÇ I'm just going to save it„ÄÇLet's see„ÄÇ I'll save it in the same location where
    my Jupiter notebook is guys„ÄÇ E I created a folderÔºå I guess„ÄÇYou can save it in
    any location where your notebook file is open„ÄÇ rightÔºå So it is not necessary and
    just making sure that you don't see any of my files„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I'm just saving it„ÄÇ OkayÔºå I'm saving it as test 1„ÄÇ Here you can see„ÄÇ I'm
    saving it as test 1 dot C SV„ÄÇ So I'll save it„ÄÇ Let's keep this particular file
    saved„ÄÇ OkayÔºå now„ÄÇ if I probably want toÔºå you know„ÄÇüòä„ÄÇ![](img/ee846c25d771f30e25168217f8991826_4.png)
  prefs: []
  type: TYPE_NORMAL
- en: Read with the panda„ÄÇ So what we writeÔºå we write P D dot read underscore CSVÔºå
    right„ÄÇ And I basically use this particular data sets called as„ÄÇtestest1„ÄÇDot CS
    Sv„ÄÇ right So when I am executing this hereÔºå you will be able to see this specific
    information„ÄÇ Now„ÄÇ when I really want to work with PiparÔºå alwaysÔºå first of allÔºå
    remember„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we need to start a spark session„ÄÇ And in order to start a spark session„ÄÇ first
    of all„ÄÇ let me create some more fields„ÄÇ Just see this„ÄÇ Just follow this particular
    steps with respect to creating a pass session„ÄÇ So Ill write from Pipar„ÄÇüòäÔºåDot sqL„ÄÇImport„ÄÇSpark
    session„ÄÇ OkayÔºå and then I'll execute this„ÄÇ You can see that it is exhibiting fine„ÄÇ
    Then I'll writeÔºå sorry„ÄÇI don't know what has opened„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so Ill Ill create a variable called a spark and probably Ill use the spark session
    dot builder„ÄÇAnd I'll say app name and hereÔºå I'll just give my session name„ÄÇ OkayÔºå
    so it'll be like practice„ÄÇ suppose I'm practicing these things„ÄÇ And then I can
    say get or create„ÄÇ So when I actually execute thisÔºå you'll be able to see a spark
    session will get created„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And if you' are executing for the first timeÔºå itll probably take some amount
    of time other than that if I executed multiple times„ÄÇ then you'll be able to work
    it now here you can definitely see that in this when you're executing in a local„ÄÇ
    therell always be only one cluster„ÄÇ But when you are actually working in a cloud„ÄÇ
    you can create multiple clusters and instances„ÄÇ Okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so the spark version that you'll be using is v 3„ÄÇ1„ÄÇ1 here you can see that this
    is basically present in the master when probably you'll be working in multiple
    instances„ÄÇ there you'll be seeing masters and cluster one cluster to all those
    kind of information Okay so this is with respect to Sp Now let's I'll just write
    Df of pipar where I will try to read a data set„ÄÇüòäÔºåWith respect to spark„ÄÇ OkayÔºå
    now in order to read a reader set what I can write„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I can write like the spark dot„ÄÇReadÔºå dot„ÄÇThere is a lot of options like CSV
    format„ÄÇ JDBC parque scheme or table textÔºå lot of options there„ÄÇ So here we are
    going to take CSV and here Im just going to write tips1„ÄÇTeps 1Ôºå do CÔºå right„ÄÇ And
    if I just try to execute it hereÔºå I'm getting some error saying that this particular
    file does not exist„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Let me see„ÄÇI think this file is present„ÄÇÂóØÂóØÂóØ„ÄÇJust let me see guys„ÄÇ why this is
    not getting executed„ÄÇ Ti 1„ÄÇD F file open„ÄÇ here I can see test 1 dot C V„ÄÇ OkayÔºå
    sorryÔºå I did not write that C SV file„ÄÇ I guess test 1 dot C SV„ÄÇOkayÔºå this has
    now work„ÄÇ NowÔºå if I go and see D dots Pipar„ÄÇ it is showing these two stringsÔºå
    right this two column C0 and C1„ÄÇ Now here you can see that guys„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I've created this particular Cv file right and it is just taking this A B as
    a default column probably So it is saying C0 and C1 So what we can actually do
    is that and probably if you really want to see your entire data set„ÄÇ you can basically
    see like this Df underscore Pipar dot show here we'll be able to see name and
    age this this information I really want to make my column name or age as my main
    column right But when I'm directly reading this Cv file„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: probably we are getting underscore c0 underscore C1„ÄÇ So in order to solve this
    what I will do is that we have a different technique So I'll write spark dot read
    dot option there is something called as option and inside this option what you
    can basically give is that there'll be an option with respect to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåHeaderÔºå like I seeÔºå there will be something like key value that you will be
    providing in option„ÄÇ So what you can doÔºå you can just write header„ÄÇCommer true„ÄÇ
    So whatever value„ÄÇ the first column first row value will be thereÔºå that will be
    considered as your header„ÄÇ And if I write Csv with respect to test1„ÄÇ nowÔºå I'm
    just going to read this test1 dataset set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Test 1 dot CS Sv„ÄÇ NowÔºå once I execute this hereÔºå you'll be able to see that
    I am able to get now name string H string„ÄÇ OkayÔºå but let's see our complete data
    set„ÄÇ So here if I execute this now„ÄÇ I'll be able to see the entire data set with
    this particular columns„ÄÇ Okay„ÄÇ so let me just quickly save this in my Df underscore
    Pi spark„ÄÇüòäÔºåAnd now„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's go and see the type of„ÄÇD F underscore pi spaÔºå okay„ÄÇNowÔºå when I execute
    this here„ÄÇ you'll be able to see guys when I was reading this Df right when I
    was„ÄÇ if I go and see the type of this with the help of pandas here you'll be able
    to see that there is pandas dot code do frame do data frame„ÄÇ But here you'll be
    seeing that when you are reading this particular data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it is of type Pipar do sQl do data frame do data frame„ÄÇ Yes„ÄÇ so that is pandas
    data frame is sQL dot data frame data frame„ÄÇ Yes„ÄÇ most of the Apis are almost
    same„ÄÇ The functionalities are same There a lot of things that we will be learning
    as we go ahead„ÄÇ but„ÄÇüòäÔºåIf I quickly want to see my„ÄÇProbablyÔºå I don't know whether
    head will work„ÄÇ Let's see„ÄÇ Yes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: head is also working„ÄÇ So if I use dot headÔºå probably you'll be able to see the
    rose information I basically shown over here„ÄÇ NowÔºå if I really want to see the
    more information regarding my columns I will be able to use something called as
    print schema now in this this print schema is just like Df dot info„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which will actually tell about your columns like name is string and age string
    Okay so all these are some basic operations that you have actually done after
    installer again„ÄÇ the main thing why I'm focusing on this is that just try to install
    this spice spark and keep it ready for my next session I will be trying to show
    you how we can change the data type how we can work with data frames„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: how we can actually do data precrossingÔºå how we can handle null valuesÔºå missing
    values„ÄÇ how we can delete the columnsÔºå how we can do various things„ÄÇ all those
    things will basically really discussing over thereÔºå how to drop columns and So
    I hope„ÄÇüòä„ÄÇ![](img/ee846c25d771f30e25168217f8991826_6.png)
  prefs: []
  type: TYPE_NORMAL
- en: Like this particular video„ÄÇ So this is just my pie spark introductionÔºå okay„ÄÇAnd
    we will continue the next session„ÄÇ Probably„ÄÇ I'll also give you this information
    in my Github where we will probably„ÄÇ okayÔºå Pi Sp is already there„ÄÇ![](img/ee846c25d771f30e25168217f8991826_8.png)
  prefs: []
  type: TYPE_NORMAL
- en: Êàë„ÄÇBasic introductionÔºå fineÔºå So we will try to do this and we'll try to cover
    this a entire thing as we go ahead in the next session„ÄÇ RememberÔºå guysÔºå againÔºå
    our main aim is basically to make you understand how probably well be working
    in clouds„ÄÇ And before thatÔºå we really need to know all the basic stuffs that we
    need to understand regarding Pipar„ÄÇ But yesÔºå it is an amazing library itll actually
    help us to implement all the spark APIpis functionalityities that are that basically
    supports with Python„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So everything all the things that are related to machine learning we can also
    do with the help of Pi„ÄÇ So I hope you like this particular videoll see all in
    the next video have a great day„ÄÇ Thank you on doll„ÄÇüòä„ÄÇ![](img/ee846c25d771f30e25168217f8991826_10.png)
  prefs: []
  type: TYPE_NORMAL
