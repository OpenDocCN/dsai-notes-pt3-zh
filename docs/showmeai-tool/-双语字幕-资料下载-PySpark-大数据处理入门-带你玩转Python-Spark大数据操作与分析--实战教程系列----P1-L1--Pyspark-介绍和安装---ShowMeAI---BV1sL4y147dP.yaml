- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PySpark å¤§æ•°æ®å¤„ç†å…¥é—¨ï¼Œå¸¦ä½ ç©è½¬Python+Sparkå¤§æ•°æ®æ“ä½œä¸åˆ†æï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P1ï¼šL1- Pyspark
    ä»‹ç»å’Œå®‰è£… - ShowMeAI - BV1sL4y147dP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PySparkå¤§æ•°æ®å¤„ç†å…¥é—¨ï¼Œå¸¦ä½ ç©è½¬Python+Sparkå¤§æ•°æ®æ“ä½œä¸åˆ†æï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P1ï¼šL1- Pysparkä»‹ç»å’Œå®‰è£…
    - ShowMeAI - BV1sL4y147dP
- en: '![](img/ee846c25d771f30e25168217f8991826_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee846c25d771f30e25168217f8991826_0.png)'
- en: Hello allã€‚ My name is Krisna and welcome to my YouTube channelã€‚ So guysã€‚ we
    are going to start Apache Spark seriesï¼Œ and specifically if I talk about Sparkã€‚
    we will be focusing on how we can use Spk with Pythonã€‚ So we are going to discuss
    about the library call Pi Spkã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæˆ‘çš„åå­—æ˜¯Krisnaï¼Œæ¬¢è¿æ¥åˆ°æˆ‘çš„YouTubeé¢‘é“ã€‚é‚£ä¹ˆï¼Œå¤§å®¶ï¼Œæˆ‘ä»¬å°†å¼€å§‹Apache Sparkç³»åˆ—ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæˆ‘è°ˆåˆ°Sparkï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå¦‚ä½•ä½¿ç”¨Sparkä¸Pythonã€‚æ‰€ä»¥æˆ‘ä»¬å°†è®¨è®ºä¸€ä¸ªå«åšPi
    Sparkçš„åº“ã€‚ğŸ˜Š
- en: We will try to understand everything why Spk is actually required and probably
    we also try to cover a lot of thingsã€‚ There is something called as Mlib Spk Mlibï¼Œ
    which will basically say that how you can apply machine learningã€‚ you know in
    Apache Spk itself with the help of this Spk API called as Pipark libraries and
    apart from that we'll also try to see in the future once we understand the basics
    of the Pipark libraryã€‚ how we can actually preprocess our data setï¼Œ how we can
    use the Pipar data frames we'll also try to seeã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å°è¯•ç†è§£ä¸ºä»€ä¹ˆå®é™…ä¸Šéœ€è¦Sparkï¼Œå¹¶ä¸”æˆ‘ä»¬å¯èƒ½è¿˜ä¼šè¦†ç›–å¾ˆå¤šå†…å®¹ã€‚æœ‰ä¸€ä¸ªå«åšMlib Spark Mlibçš„ä¸œè¥¿ï¼Œå®ƒåŸºæœ¬ä¸Šä¼šå‘Šè¯‰ä½ å¦‚ä½•åœ¨Apache
    Sparkä¸­åº”ç”¨æœºå™¨å­¦ä¹ ã€‚ä½ çŸ¥é“ï¼Œé€šè¿‡è¿™ä¸ªå«åšPiparkåº“çš„Spark APIã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¼šåœ¨æœªæ¥çœ‹åˆ°ï¼Œä¸€æ—¦æˆ‘ä»¬ç†è§£äº†Piparkåº“çš„åŸºç¡€çŸ¥è¯†ï¼Œæˆ‘ä»¬å¦‚ä½•å®é™…é¢„å¤„ç†æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œå¦‚ä½•ä½¿ç”¨Piparkæ•°æ®æ¡†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå°è¯•äº†è§£ã€‚
- en: How we can implement or how we can use Pipark in cloud platforms like data breaksï¼Œ
    Amazonï¼Œ Awsã€‚ you knowï¼Œ so all these kind of clouds will try to cover and remember
    Apache spark is quite handyã€‚ Let me tell you just let me just give you some of
    the reasons So why Aache spark is pretty much good because understandã€‚ suppose
    if you have a huge amount of data Okay suppose if I say that I'm having 64 gb
    dataï¼Œ1ã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•åœ¨äº‘å¹³å°å¦‚Databricksã€Amazonã€AWSä¸­å®æ–½æˆ–ä½¿ç”¨Piparkï¼Œä½ çŸ¥é“ï¼Œå› æ­¤æ‰€æœ‰è¿™äº›äº‘æˆ‘ä»¬éƒ½ä¼šå°è¯•è¦†ç›–ï¼Œå¹¶ä¸”è®°ä½Apache Sparkéå¸¸æ–¹ä¾¿ã€‚è®©æˆ‘å‘Šè¯‰ä½ ï¼Œç»™ä½ ä¸€äº›ç†ç”±ï¼Œä¸ºä»€ä¹ˆApache
    Sparkéå¸¸å¥½ï¼Œå› ä¸ºä½ è¦ç†è§£ï¼Œå‡è®¾ä½ æœ‰å¤§é‡çš„æ•°æ®ã€‚å¥½çš„ï¼Œå‡è®¾æˆ‘è¯´æˆ‘æœ‰64GBçš„æ•°æ®ï¼Œ1ã€‚
- en: 8 gb dataï¼Œ you knowï¼Œ we may have some kind of systemsï¼Œ a standalone systemsï¼Œ
    you knowã€‚ where we can have 32 gb of Ram probably 64 gb of Ram right now in the
    workstation that I'm working in it has 64 gb Ramã€‚ So max to Macï¼Œ it can directly
    upload a data set of 32 gbã€‚48 gb But what if if we have a data set of 1ã€‚8 g you
    know that is the time guysã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 8GBçš„æ•°æ®ï¼Œä½ çŸ¥é“ï¼Œæˆ‘ä»¬å¯èƒ½æœ‰ä¸€äº›ç‹¬ç«‹ç³»ç»Ÿï¼ŒçŸ¥é“ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯èƒ½æœ‰32GBæˆ–64GBçš„å†…å­˜ã€‚ç°åœ¨æˆ‘æ­£åœ¨å·¥ä½œçš„å·¥ä½œç«™æœ‰64GBçš„å†…å­˜ã€‚å› æ­¤ï¼Œæœ€å¤§åˆ°æœ€å¤§ï¼Œå®ƒå¯ä»¥ç›´æ¥ä¸Šä¼ ä¸€ä¸ª32GBæˆ–48GBçš„æ•°æ®é›†ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ª1.8GBçš„æ•°æ®é›†ï¼Œä½ çŸ¥é“ï¼Œé‚£ä¸ªæ—¶å€™ï¼Œå¤§å®¶ã€‚
- en: we don't just depend on a local system will try to preprocess that particular
    data perform any kind of operation in distributeã€‚uted systems right distributeded
    system basically means that all there will be multiple systemsã€‚ you knowï¼Œ where
    we can actually run this kind of jobs or process or try to do any kind of activities
    that we really want and definitely Apache Sp will actually help us to do that
    and this has been pretty much amazing And yesã€‚ people wanted this kind of videos
    a lot So how we are going to go through this specific playlistes that will try
    to first of allã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä»…ä¾èµ–äºæœ¬åœ°ç³»ç»Ÿï¼Œè¿˜å°†å°è¯•åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­é¢„å¤„ç†ç‰¹å®šæ•°æ®å¹¶æ‰§è¡Œä»»ä½•ç±»å‹çš„æ“ä½œã€‚åˆ†å¸ƒå¼ç³»ç»ŸåŸºæœ¬ä¸Šæ„å‘³ç€å°†ä¼šæœ‰å¤šä¸ªç³»ç»Ÿï¼Œä½ çŸ¥é“ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™äº›ç³»ç»Ÿä¸Šè¿è¡Œè¿™ç§ä½œä¸šæˆ–å¤„ç†ï¼Œæˆ–å°è¯•åšæˆ‘ä»¬çœŸæ­£æƒ³åšçš„ä»»ä½•æ´»åŠ¨ï¼ŒApache
    Sparkè‚¯å®šä¼šå¸®åŠ©æˆ‘ä»¬åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿™çœŸçš„æ˜¯éå¸¸æ£’ã€‚è€Œä¸”ï¼Œæ˜¯çš„ï¼Œäººä»¬éå¸¸æƒ³è¦è¿™ç§ç±»å‹çš„è§†é¢‘ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å¦‚ä½•è¿›è¡Œè¿™ä¸ªç‰¹å®šçš„æ’­æ”¾åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†é¦–å…ˆã€‚
- en: start with the installationã€‚ We'll try to use Pipark because that is also Apache
    spark it is a spark API with Python when you are actually working with Python
    we basically use Pipark library And yesã€‚ we can also use spark with other programming
    languages like Java scale and all right and will try to understand from basics
    you know from basic how do we read a dataã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å®‰è£…å¼€å§‹ã€‚æˆ‘ä»¬å°†å°è¯•ä½¿ç”¨Piparkï¼Œå› ä¸ºå®ƒä¹Ÿæ˜¯Apache Sparkï¼Œè¿™æ˜¯ä¸€ä¸ªä¸Pythonä¸€èµ·ä½¿ç”¨çš„Spark APIã€‚å½“ä½ å®é™…ä½¿ç”¨Pythonæ—¶ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šä½¿ç”¨Piparkåº“ã€‚è€Œä¸”ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨Sparkä¸å…¶ä»–ç¼–ç¨‹è¯­è¨€ï¼Œå¦‚Javaã€Scalaç­‰ï¼Œæˆ‘ä»¬å°†ä»åŸºç¡€çŸ¥è¯†å¼€å§‹ï¼Œäº†è§£æˆ‘ä»¬å¦‚ä½•è¯»å–æ•°æ®ã€‚
- en: how do we connect to a data source probably how do we play with the data frames
    know in this Apache Sp that is your Piã€‚ğŸ˜Šï¼ŒAlso they provide you data structures
    like data framesã€‚ which is pretty much similar to the pans data frameï¼Œ but yesã€‚
    different kind of operations are supported over there which we'll be looking one
    by one as we go ahead and then we'll try to enter into Mlib Apache Sp Mlib so
    basically it is called as spark Mlib which will actually help us to perform machine
    learning which where we'll be able to perform some machine learning algorithm
    task where we'll be able to do regression classification clustering and finally
    well try to see how we can actually do the same operation in cloud where I'll
    try to show you some examples where we will be having a huge data set we will
    try to do the operation in the clusters of system you know in a distributed system
    and we'll try to see how we can use spark in that right so all those things will
    basically get covered now some of the advantages of Apache Sp and white is very
    much famous because it runs workloads hundred multiplied byã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•è¿æ¥åˆ°æ•°æ®æºï¼Œå¯èƒ½æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨æ•°æ®æ¡†ï¼Œåœ¨è¿™ä¸ªApache Sparkä¸­ï¼Œè¿™æ˜¯ä½ çš„Piã€‚ğŸ˜Šä»–ä»¬è¿˜æä¾›ä½ æ•°æ®ç»“æ„ï¼Œæ¯”å¦‚æ•°æ®æ¡†ï¼Œè¿™ä¸Pandasæ•°æ®æ¡†éå¸¸ç›¸ä¼¼ï¼Œä½†æ”¯æŒçš„æ“ä½œæœ‰æ‰€ä¸åŒï¼Œæˆ‘ä»¬å°†é€ä¸€æŸ¥çœ‹ï¼Œç„¶åæˆ‘ä»¬å°†å°è¯•è¿›å…¥Mlib
    Apache Spark Mlibï¼ŒåŸºæœ¬ä¸Šç§°ä¸ºSpark Mlibï¼Œå®ƒå°†å¸®åŠ©æˆ‘ä»¬æ‰§è¡Œæœºå™¨å­¦ä¹ ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å°†èƒ½å¤Ÿæ‰§è¡Œä¸€äº›æœºå™¨å­¦ä¹ ç®—æ³•ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›å½’ã€åˆ†ç±»ã€èšç±»ï¼Œæœ€åæˆ‘ä»¬å°†å°è¯•çœ‹çœ‹å¦‚ä½•åœ¨äº‘ä¸­æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œæˆ‘ä¼šå‘ä½ å±•ç¤ºä¸€äº›ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ä¸ªåºå¤§çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å°è¯•åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿçš„é›†ç¾¤ä¸­è¿›è¡Œæ“ä½œï¼Œå¹¶çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨Sparkï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥è¿™äº›å†…å®¹åŸºæœ¬ä¸Šéƒ½ä¼šè¦†ç›–åˆ°ã€‚ç°åœ¨Apache
    Sparkçš„ä¸€äº›ä¼˜åŠ¿ä»¥åŠä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤è‘—åï¼Œå› ä¸ºå®ƒçš„å·¥ä½œè´Ÿè½½é€Ÿåº¦æ˜¯æ™®é€šçš„ç™¾å€ã€‚
- en: times fasterï¼Œ you knowï¼Œ that basically means And if you know about big data
    guysã€‚ when we talk about big dataï¼Œ we are basically talking about huge data setï¼Œ
    rightã€‚ and there if you have heard of this terminology called as map produceï¼Œ
    rightã€‚Trusush mean Apache Spk is much more faster 00 times faster than map produce
    alsoã€‚ okayã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¿«ï¼Œä½ çŸ¥é“ï¼Œè¿™åŸºæœ¬ä¸Šæ„å‘³ç€ã€‚å¦‚æœä½ äº†è§£å¤§æ•°æ®ï¼Œå½“æˆ‘ä»¬è°ˆè®ºå¤§æ•°æ®æ—¶ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯åœ¨è°ˆè®ºå·¨å¤§çš„æ•°æ®é›†ï¼Œå¯¹å§ï¼Ÿå¦‚æœä½ å¬è¯´è¿‡è¿™ä¸ªå«åšMapReduceçš„æœ¯è¯­ï¼ŒApache
    Sparkçš„é€Ÿåº¦æ¯”MapReduceå¿«å¾—å¤šï¼Œå¿«100å€ã€‚å¥½çš„ã€‚
- en: and it is some of the more advantage that it is ease of useã€‚ you can write application
    quickly in Java scalear Python or Rï¼Œ as I saidã€‚ well be focusing on Python where
    we'll be using a library called Piparkã€‚ Then you can also combine SQL streaming
    and complex analyticsã€‚ When I talk about complex analyticsã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å®ƒçš„ä¸€äº›ä¼˜åŠ¿åœ¨äºæ˜“ç”¨æ€§ã€‚ä½ å¯ä»¥å¿«é€Ÿç”¨Javaã€Scalaã€Pythonæˆ–Rç¼–å†™åº”ç”¨ç¨‹åºï¼Œæ­£å¦‚æˆ‘æ‰€è¯´çš„ã€‚æˆ‘ä»¬å°†ä¸“æ³¨äºPythonï¼Œå¹¶å°†ä½¿ç”¨ä¸€ä¸ªå«åšPiparkçš„åº“ã€‚ç„¶åä½ è¿˜å¯ä»¥ç»“åˆSQLæµå¤„ç†å’Œå¤æ‚åˆ†æã€‚å½“æˆ‘è°ˆè®ºå¤æ‚åˆ†ææ—¶ã€‚
- en: I'm basically talking about this Mlib machine learning libraries that will work
    definitely well with the Apache Spkã€‚And Apache sparks can run on Hadoop Apache
    meso Knet standalone in or in the clouds cloudã€‚ different types of cloud guys
    when I talk about Aws data breaksï¼Œ all these thingsã€‚ we can definitely workã€‚ and
    it actually runs in a cluster cluster basically means in a distributed So these
    are some of the examplesã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åŸºæœ¬ä¸Šæ˜¯åœ¨è°ˆè®ºè¿™ä¸ªMlibæœºå™¨å­¦ä¹ åº“ï¼Œå®ƒè‚¯å®šèƒ½ä¸Apache Sparkè‰¯å¥½åä½œã€‚Apache Sparkå¯ä»¥åœ¨Hadoopã€Apache Mesosã€Kubernetesã€ç‹¬ç«‹æ¨¡å¼æˆ–äº‘ç«¯è¿è¡Œã€‚å½“æˆ‘è°ˆåˆ°AWS
    DataBricksç­‰å„ç§äº‘æœåŠ¡æ—¶ï¼Œæˆ‘ä»¬è‚¯å®šå¯ä»¥å·¥ä½œã€‚å®ƒå®é™…ä¸Šåœ¨é›†ç¾¤ä¸­è¿è¡Œï¼Œé›†ç¾¤åŸºæœ¬ä¸Šæ„å‘³ç€åˆ†å¸ƒå¼ã€‚å› æ­¤ï¼Œè¿™äº›éƒ½æ˜¯ä¸€äº›ç¤ºä¾‹ã€‚
- en: Now if I go with respect to which version ofpar spark will be usingpar 3ã€‚1ã€‚1
    will be using will try to work and if you just go and search for here you can
    see SQL and data frames and all here you can see spark streaming machineli that
    is called as machine learning in all And apart from that if I go and see the overview
    here you can see that Apache spark is a fast and general purpose cluster computing
    systemã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘è°ˆåˆ°æˆ‘ä»¬å°†ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬çš„Sparkï¼Œæˆ‘ä»¬å°†ä½¿ç”¨3.1.1ï¼Œå¹¶å°è¯•è¿›è¡Œå·¥ä½œã€‚å¦‚æœä½ åœ¨è¿™é‡Œæœç´¢ä¸€ä¸‹ï¼Œä½ å¯ä»¥çœ‹åˆ°SQLå’Œæ•°æ®æ¡†æ¶ï¼Œä»¥åŠSparkæµå¤„ç†ï¼Œæœºå™¨å­¦ä¹ è¢«ç§°ä¸ºæœºå™¨å­¦ä¹ ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¦‚æœæˆ‘æŸ¥çœ‹æ¦‚è¿°ï¼Œä½ ä¼šçœ‹åˆ°Apache
    Sparkæ˜¯ä¸€ä¸ªå¿«é€Ÿçš„é€šç”¨é›†ç¾¤è®¡ç®—ç³»ç»Ÿã€‚
- en: it provides highle APIs in scale or Java and Python that makes parallel job
    easy to write an optimized engine that supports general competition graphã€‚ So
    it is basically to work with huge amount of dataã€‚ğŸ˜Šï¼ŒAnd shortï¼Œ you knowã€‚ and that
    is pretty much handyã€‚ will'll try to workã€‚ Nowï¼Œ if I go and search for this park
    in Pythonã€‚ you knowï¼Œ this page will get basically go open and this things will
    try to discuss how to install itã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæä¾›äº†é«˜æ•ˆçš„APIï¼Œå¯ä»¥åœ¨Javaå’ŒPythonä¸­è¿›è¡Œæ‰©å±•ï¼Œè¿™ä½¿å¾—å¹¶è¡Œä½œä¸šçš„ç¼–å†™å˜å¾—ç®€å•ï¼Œæ”¯æŒé€šç”¨è®¡ç®—å›¾çš„ä¼˜åŒ–å¼•æ“ã€‚æ‰€ä»¥å®ƒåŸºæœ¬ä¸Šæ˜¯ç”¨æ¥å¤„ç†å¤§é‡æ•°æ®çš„ã€‚ğŸ˜Šè€Œä¸”ç®€çŸ­ï¼Œä½ çŸ¥é“çš„ã€‚è¿™éå¸¸æ–¹ä¾¿ã€‚æˆ‘ä»¬å°†å°è¯•å·¥ä½œã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘å»æœç´¢è¿™ä¸ªPythonä¸­çš„Sparkï¼Œä½ çŸ¥é“ï¼Œè¿™ä¸ªé¡µé¢åŸºæœ¬ä¸Šä¼šæ‰“å¼€ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•å®‰è£…å®ƒã€‚
- en: And in this video will try to install the Pipar libraryã€‚ if I talk about Pipar
    library you'll be able to see that Pipar library is pretty much amazingã€‚ this
    library is if you really want to work if you want to work this spark functionality
    with Pythonã€‚ you basically use the specific library and let's proceed and let's
    try to see that how we can quicklyã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å°è¯•å®‰è£…Piparåº“ã€‚å¦‚æœæˆ‘è°ˆåˆ°Piparåº“ï¼Œä½ ä¼šå‘ç°Piparåº“éå¸¸æ£’ã€‚è¿™ä¸ªåº“æ˜¯å¦‚æœä½ çœŸçš„æƒ³åœ¨Pythonä¸­ä½¿ç”¨SparkåŠŸèƒ½çš„è¯ï¼Œä½ åŸºæœ¬ä¸Šä¼šä½¿ç”¨è¿™ä¸ªç‰¹å®šçš„åº“ã€‚è®©æˆ‘ä»¬ç»§ç»­ï¼Œçœ‹çœ‹æˆ‘ä»¬èƒ½å¤šå¿«åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: ğŸ˜Šï¼ŒYeahï¼Œ how we can quickly you know install the specific libraries and check
    out like what all things we can actually doã€‚ Okayï¼Œ so all these things we'll try
    to seeã€‚ So let's beginã€‚ please make sure that you create a new environment when
    you are working with Pi So I have created a new environment called my and hereã€‚
    first of allï¼Œ I'll try to install the Pi librariesã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šæ˜¯çš„ï¼Œæˆ‘ä»¬å¦‚ä½•å¿«é€Ÿå®‰è£…ç‰¹å®šçš„åº“ï¼Œå¹¶æ£€æŸ¥æˆ‘ä»¬å¯ä»¥å®é™…åšäº›ä»€ä¹ˆã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å°è¯•çœ‹çœ‹æ‰€æœ‰è¿™äº›å†…å®¹ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬å¼€å§‹å§ã€‚è¯·ç¡®ä¿åœ¨ä½¿ç”¨Piæ—¶åˆ›å»ºä¸€ä¸ªæ–°ç¯å¢ƒã€‚æ‰€ä»¥æˆ‘åˆ›å»ºäº†ä¸€ä¸ªåä¸ºmyçš„æ–°ç¯å¢ƒã€‚åœ¨è¿™é‡Œï¼Œé¦–å…ˆï¼Œæˆ‘å°†å°è¯•å®‰è£…Piåº“ã€‚
- en: So I'll just install Pipar and let's see in this we'll focus on installationã€‚
    we'll focus on reading some data sets and try to see that what all things we can
    actually doã€‚ Okay and after doing this what we can actually do is that you can
    see that our Pi has been installed in order to check whether the installation
    is perfect or not I'll just write import Pi So this this looks perfectly find
    it is working know we are able to see that the Pi is basically installed properly
    Now you may be facing some kind of problemsã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†å®‰è£…Piparï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åœ¨è¿™æ–¹é¢æˆ‘ä»¬å°†ä¸“æ³¨äºå®‰è£…ã€‚æˆ‘ä»¬å°†ä¸“æ³¨äºè¯»å–ä¸€äº›æ•°æ®é›†ï¼Œå¹¶å°è¯•çœ‹çœ‹æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åšäº›ä»€ä¹ˆã€‚å¥½çš„ï¼Œå®Œæˆè¿™ä¸ªä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„Piå·²ç»å®‰è£…ã€‚ä¸ºäº†æ£€æŸ¥å®‰è£…æ˜¯å¦å®Œç¾ï¼Œæˆ‘å°†å†™import
    Piã€‚æ‰€ä»¥è¿™çœ‹èµ·æ¥å®Œå…¨æ­£å¸¸ï¼Œå®ƒæ­£åœ¨å·¥ä½œã€‚æˆ‘ä»¬èƒ½å¤Ÿçœ‹åˆ°PiåŸºæœ¬ä¸Šå·²æ­£ç¡®å®‰è£…ã€‚ç°åœ¨ä½ å¯èƒ½ä¼šé‡åˆ°æŸç§é—®é¢˜ã€‚
- en: is with respect to Pipar So that is the reason why I'm telling you create a
    new environment If you' are facing some kindã€‚ğŸ˜Šï¼ŒIsueï¼Œ just let me know what is
    the error that you are getting probably writing in the comment sectionã€‚ Okayï¼Œ
    now let's do one thingã€‚ I'll just open Excel sheet Okayã€‚ and probably I'll just
    try to create aã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å…³äºPiparçš„ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘å‘Šè¯‰ä½ åˆ›å»ºä¸€ä¸ªæ–°ç¯å¢ƒçš„åŸå› ã€‚å¦‚æœä½ é‡åˆ°æŸç§é—®é¢˜ï¼ŒğŸ˜Šè¯·å‘Šè¯‰æˆ‘ä½ é‡åˆ°çš„é”™è¯¯ï¼Œå¯èƒ½åœ¨è¯„è®ºåŒºå†™ä¸€ä¸‹ã€‚å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬æ¥åšä¸€ä»¶äº‹ã€‚æˆ‘ä¼šæ‰“å¼€Excelè¡¨æ ¼ã€‚å¥½çš„ï¼Œå¯èƒ½æˆ‘ä¼šè¯•ç€åˆ›å»ºä¸€ä¸ªã€‚
- en: '![](img/ee846c25d771f30e25168217f8991826_2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee846c25d771f30e25168217f8991826_2.png)'
- en: Some data setsï¼Œ I'll say nameï¼Œ probablyï¼Œ I'll just say nameã€‚And Hï¼Œ rightã€‚ And
    suppose my name over here that I'm going to write is squ and also 31ã€‚ I'm going
    to say Suan Sã€‚Rightï¼Œ Shoan Shuã€‚ I will just sayï¼Œ okayï¼Œ 30ã€‚ and probablyã€‚ I'll
    just write some more names like Sunnyã€‚ Probably I'll also give the data as 29ã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ•°æ®é›†ï¼Œæˆ‘ä¼šè¯´åå­—ï¼Œå¯èƒ½æˆ‘ä¼šè¯´åå­—ã€‚è¿˜æœ‰Hï¼Œå¯¹å§ã€‚å‡è®¾æˆ‘åœ¨è¿™é‡Œå†™çš„åå­—æ˜¯squanï¼Œè¿˜æœ‰31ã€‚æˆ‘ä¼šè¯´Suan Sã€‚å¯¹å§ï¼ŒShoan Shuã€‚æˆ‘ä¼šè¯´ï¼Œå¥½çš„ï¼Œ30ã€‚å¯èƒ½æˆ‘ä¼šå†å†™ä¸€äº›åå­—ï¼Œæ¯”å¦‚Sunnyã€‚å¯èƒ½æˆ‘è¿˜ä¼šç»™å‡ºæ•°æ®29ã€‚
- en: So this three data will just try to see how we can read this specific fileã€‚
    Okayã€‚ I'm just going to save itã€‚Let's seeã€‚ I'll save it in the same location where
    my Jupiter notebook is guysã€‚ E I created a folderï¼Œ I guessã€‚You can save it in
    any location where your notebook file is openã€‚ rightï¼Œ So it is not necessary and
    just making sure that you don't see any of my filesã€‚Okayã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸‰ä¸ªæ•°æ®æˆ‘ä»¬å°†å°è¯•çœ‹çœ‹å¦‚ä½•è¯»å–è¿™ä¸ªç‰¹å®šçš„æ–‡ä»¶ã€‚å¥½çš„ï¼Œæˆ‘è¦ä¿å­˜å®ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘ä¼šå°†å…¶ä¿å­˜åœ¨ä¸æˆ‘çš„Jupyterç¬”è®°æœ¬ç›¸åŒçš„ä½ç½®ã€‚å—¯ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œæˆ‘æƒ³ã€‚ä½ å¯ä»¥å°†å…¶ä¿å­˜åœ¨ä»»ä½•æ‰“å¼€ç¬”è®°æœ¬æ–‡ä»¶çš„ä½ç½®ï¼Œå¯¹å§ï¼Œæ‰€ä»¥è¿™ä¸æ˜¯å¿…è¦çš„ï¼Œåªæ˜¯ç¡®ä¿ä½ æ²¡æœ‰çœ‹åˆ°æˆ‘çš„æ–‡ä»¶ã€‚å¥½çš„ã€‚
- en: and I'm just saving itã€‚ Okayï¼Œ I'm saving it as test 1ã€‚ Here you can seeã€‚ I'm
    saving it as test 1 dot C SVã€‚ So I'll save itã€‚ Let's keep this particular file
    savedã€‚ Okayï¼Œ nowã€‚ if I probably want toï¼Œ you knowã€‚ğŸ˜Šã€‚![](img/ee846c25d771f30e25168217f8991826_4.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ­£åœ¨ä¿å­˜å®ƒã€‚å¥½çš„ï¼Œæˆ‘å°†å…¶ä¿å­˜ä¸ºtest 1ã€‚åœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°ã€‚æˆ‘å°†å…¶ä¿å­˜ä¸ºtest 1.dot CSVã€‚æ‰€ä»¥æˆ‘ä¼šä¿å­˜å®ƒã€‚è®©æˆ‘ä»¬ä¿æŒè¿™ä¸ªç‰¹å®šçš„æ–‡ä»¶å·²ä¿å­˜ã€‚å¥½çš„ï¼Œç°åœ¨å¦‚æœæˆ‘å¯èƒ½æƒ³è¦ï¼Œä½ çŸ¥é“ã€‚ğŸ˜Šï¼[](img/ee846c25d771f30e25168217f8991826_4.png)
- en: Read with the pandaã€‚ So what we writeï¼Œ we write P D dot read underscore CSVï¼Œ
    rightã€‚ And I basically use this particular data sets called asã€‚testest1ã€‚Dot CS
    Svã€‚ right So when I am executing this hereï¼Œ you will be able to see this specific
    informationã€‚ Nowã€‚ when I really want to work with Piparï¼Œ alwaysï¼Œ first of allï¼Œ
    rememberã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨pandasè¯»å–ã€‚æ‰€ä»¥æˆ‘ä»¬å†™çš„æ˜¯`pd.read_csv`ï¼Œå¯¹å§ã€‚æˆ‘åŸºæœ¬ä¸Šä½¿ç”¨è¿™ä¸ªæ•°æ®é›†ï¼Œå«åš`testest1.CSV`ï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥å½“æˆ‘åœ¨è¿™é‡Œæ‰§è¡Œæ—¶ï¼Œä½ å°†èƒ½çœ‹åˆ°è¿™ä¸ªç‰¹å®šä¿¡æ¯ã€‚ç°åœ¨ï¼Œå½“æˆ‘çœŸçš„æƒ³ç”¨Piparæ—¶ï¼Œé¦–å…ˆï¼Œè®°ä½ã€‚
- en: we need to start a spark sessionã€‚ And in order to start a spark sessionã€‚ first
    of allã€‚ let me create some more fieldsã€‚ Just see thisã€‚ Just follow this particular
    steps with respect to creating a pass sessionã€‚ So Ill write from Piparã€‚ğŸ˜Šï¼ŒDot sqLã€‚Importã€‚Spark
    sessionã€‚ Okayï¼Œ and then I'll execute thisã€‚ You can see that it is exhibiting fineã€‚
    Then I'll writeï¼Œ sorryã€‚I don't know what has openedã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å¯åŠ¨ä¸€ä¸ªsparkä¼šè¯ã€‚ä¸ºäº†å¯åŠ¨sparkä¼šè¯ï¼Œé¦–å…ˆè®©æˆ‘åˆ›å»ºæ›´å¤šå­—æ®µã€‚è¯·çœ‹è¿™ä¸ªã€‚è¯·éµå¾ªåˆ›å»ºpassä¼šè¯çš„ç‰¹å®šæ­¥éª¤ã€‚æ‰€ä»¥æˆ‘å°†å†™`from Pipar`ï¼Œ`Dot
    sqL`ï¼Œ`Import Spark session`ã€‚å¥½çš„ï¼Œç„¶åæˆ‘å°†æ‰§è¡Œè¿™ä¸ªã€‚ä½ å¯ä»¥çœ‹åˆ°å®ƒè¡¨ç°å¾—å¾ˆå¥½ã€‚ç„¶åæˆ‘ä¼šå†™ï¼ŒæŠ±æ­‰ã€‚æˆ‘ä¸çŸ¥é“æ‰“å¼€äº†ä»€ä¹ˆã€‚
- en: so Ill Ill create a variable called a spark and probably Ill use the spark session
    dot builderã€‚And I'll say app name and hereï¼Œ I'll just give my session nameã€‚ Okayï¼Œ
    so it'll be like practiceã€‚ suppose I'm practicing these thingsã€‚ And then I can
    say get or createã€‚ So when I actually execute thisï¼Œ you'll be able to see a spark
    session will get createdã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†åˆ›å»ºä¸€ä¸ªåä¸ºsparkçš„å˜é‡ï¼Œå¯èƒ½æˆ‘å°†ä½¿ç”¨`spark session.builder`ã€‚æˆ‘ä¼šè¯´åº”ç”¨åç§°ï¼Œè¿™é‡Œæˆ‘åªä¼šç»™æˆ‘çš„ä¼šè¯å‘½åã€‚å¥½çš„ï¼Œæ‰€ä»¥å®ƒå°†æ˜¯ç»ƒä¹ ã€‚å‡è®¾æˆ‘åœ¨ç»ƒä¹ è¿™äº›ä¸œè¥¿ã€‚ç„¶åæˆ‘å¯ä»¥è¯´è·å–æˆ–åˆ›å»ºã€‚æ‰€ä»¥å½“æˆ‘å®é™…æ‰§è¡Œæ—¶ï¼Œä½ å°†èƒ½çœ‹åˆ°ä¸€ä¸ªsparkä¼šè¯ä¼šè¢«åˆ›å»ºã€‚
- en: And if you' are executing for the first timeï¼Œ itll probably take some amount
    of time other than that if I executed multiple timesã€‚ then you'll be able to work
    it now here you can definitely see that in this when you're executing in a localã€‚
    therell always be only one clusterã€‚ But when you are actually working in a cloudã€‚
    you can create multiple clusters and instancesã€‚ Okayã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œå®ƒå¯èƒ½ä¼šèŠ±ä¸€äº›æ—¶é—´ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œå¦‚æœæˆ‘å¤šæ¬¡æ‰§è¡Œï¼Œé‚£ä½ å°±èƒ½ä½¿ç”¨å®ƒã€‚ç°åœ¨åœ¨è¿™é‡Œä½ ç»å¯¹å¯ä»¥çœ‹åˆ°ï¼Œå½“ä½ åœ¨æœ¬åœ°æ‰§è¡Œæ—¶ï¼Œæ€»æ˜¯åªæœ‰ä¸€ä¸ªé›†ç¾¤ã€‚ä½†å½“ä½ åœ¨äº‘ä¸­å·¥ä½œæ—¶ï¼Œä½ å¯ä»¥åˆ›å»ºå¤šä¸ªé›†ç¾¤å’Œå®ä¾‹ã€‚å¥½çš„ã€‚
- en: so the spark version that you'll be using is v 3ã€‚1ã€‚1 here you can see that this
    is basically present in the master when probably you'll be working in multiple
    instancesã€‚ there you'll be seeing masters and cluster one cluster to all those
    kind of information Okay so this is with respect to Sp Now let's I'll just write
    Df of pipar where I will try to read a data setã€‚ğŸ˜Šï¼ŒWith respect to sparkã€‚ Okayï¼Œ
    now in order to read a reader set what I can writeã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å°†ä½¿ç”¨çš„sparkç‰ˆæœ¬æ˜¯v 3.1.1ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°ï¼Œè¿™åŸºæœ¬ä¸Šå­˜åœ¨äºä¸»èŠ‚ç‚¹ï¼Œå½“ä½ å¯èƒ½åœ¨å¤šä¸ªå®ä¾‹ä¸­å·¥ä½œæ—¶ï¼Œä½ å°†çœ‹åˆ°ä¸»èŠ‚ç‚¹å’Œé›†ç¾¤çš„ä¿¡æ¯ã€‚å¥½çš„ï¼Œè¿™ä¸Sparkæœ‰å…³ã€‚ç°åœ¨æˆ‘å°†å†™`Df
    of pipar`ï¼Œæˆ‘å°†å°è¯•è¯»å–ä¸€ä¸ªæ•°æ®é›†ã€‚ğŸ˜Šï¼Œå…³äºsparkã€‚å¥½çš„ï¼Œç°åœ¨ä¸ºäº†è¯»å–ä¸€ä¸ªæ•°æ®é›†ï¼Œæˆ‘å¯ä»¥å†™ä»€ä¹ˆã€‚
- en: I can write like the spark dotã€‚Readï¼Œ dotã€‚There is a lot of options like CSV
    formatã€‚ JDBC parque scheme or table textï¼Œ lot of options thereã€‚ So here we are
    going to take CSV and here Im just going to write tips1ã€‚Teps 1ï¼Œ do Cï¼Œ rightã€‚ And
    if I just try to execute it hereï¼Œ I'm getting some error saying that this particular
    file does not existã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥å†™`spark.Read`ï¼Œç„¶åæœ‰å¾ˆå¤šé€‰é¡¹ï¼Œæ¯”å¦‚CSVæ ¼å¼ã€JDBCã€parquetã€schemeæˆ–æ–‡æœ¬ç­‰ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å°†é‡‡ç”¨CSVï¼Œæˆ‘åªæ˜¯å†™`tips1.Teps
    1 do C`ï¼Œå¯¹å§ã€‚å¦‚æœæˆ‘è¯•ç€åœ¨è¿™é‡Œæ‰§è¡Œï¼Œæˆ‘æ”¶åˆ°ä¸€äº›é”™è¯¯ï¼Œè¯´è¿™ä¸ªç‰¹å®šæ–‡ä»¶ä¸å­˜åœ¨ã€‚
- en: Let me seeã€‚I think this file is presentã€‚å—¯å—¯å—¯ã€‚Just let me see guysã€‚ why this is
    not getting executedã€‚ Ti 1ã€‚D F file openã€‚ here I can see test 1 dot C Vã€‚ Okayï¼Œ
    sorryï¼Œ I did not write that C SV fileã€‚ I guess test 1 dot C SVã€‚Okayï¼Œ this has
    now workã€‚ Nowï¼Œ if I go and see D dots Piparã€‚ it is showing these two stringsï¼Œ
    right this two column C0 and C1ã€‚ Now here you can see that guysã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘çœ‹çœ‹ã€‚æˆ‘æƒ³è¿™ä¸ªæ–‡ä»¶æ˜¯å­˜åœ¨çš„ã€‚å—¯å—¯å—¯ã€‚è®©æˆ‘çœ‹çœ‹ï¼Œä¸ºä»€ä¹ˆè¿™ä¸ªæ²¡æœ‰æ‰§è¡Œã€‚`Ti 1.D F file open`ã€‚è¿™é‡Œæˆ‘èƒ½çœ‹åˆ°`test 1.CV`ã€‚å¥½çš„ï¼ŒæŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å†™é‚£ä¸ªCSVæ–‡ä»¶ã€‚æˆ‘æƒ³æ˜¯`test
    1.CSV`ã€‚å¥½çš„ï¼Œè¿™ç°åœ¨å·¥ä½œäº†ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘å»çœ‹`D dots Pipar`ï¼Œå®ƒæ˜¾ç¤ºè¿™ä¸¤ä¸ªå­—ç¬¦ä¸²ï¼Œå¯¹å§ï¼Œè¿™ä¸¤åˆ—C0å’ŒC1ã€‚ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°ï¼Œä¼™è®¡ä»¬ã€‚
- en: I've created this particular Cv file right and it is just taking this A B as
    a default column probably So it is saying C0 and C1 So what we can actually do
    is that and probably if you really want to see your entire data setã€‚ you can basically
    see like this Df underscore Pipar dot show here we'll be able to see name and
    age this this information I really want to make my column name or age as my main
    column right But when I'm directly reading this Cv fileã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åˆ›å»ºäº†è¿™ä¸ªç‰¹å®šçš„ Cv æ–‡ä»¶ï¼Œå®ƒåªæ˜¯å°† A B ä½œä¸ºé»˜è®¤åˆ—ï¼Œå¯èƒ½å®ƒåœ¨è¯´ C0 å’Œ C1ã€‚æ‰€ä»¥æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åšçš„æ˜¯ï¼Œå¯èƒ½å¦‚æœä½ çœŸçš„æƒ³çœ‹åˆ°æ•´ä¸ªæ•°æ®é›†ï¼Œä½ å¯ä»¥åŸºæœ¬ä¸Šåƒè¿™æ ·çœ‹åˆ°
    `Df underscore Pipar dot show`ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬èƒ½çœ‹åˆ° name å’Œ ageï¼Œæˆ‘çœŸçš„æƒ³æŠŠæˆ‘çš„åˆ—åæˆ– age ä½œä¸ºæˆ‘çš„ä¸»è¦åˆ—ã€‚ä½†å½“æˆ‘ç›´æ¥è¯»å–è¿™ä¸ª
    Cv æ–‡ä»¶æ—¶ã€‚
- en: probably we are getting underscore c0 underscore C1ã€‚ So in order to solve this
    what I will do is that we have a different technique So I'll write spark dot read
    dot option there is something called as option and inside this option what you
    can basically give is that there'll be an option with respect toã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æˆ‘ä»¬å¾—åˆ°äº† `underscore c0 underscore C1`ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä¼šä½¿ç”¨ä¸åŒçš„æŠ€æœ¯ã€‚æ‰€ä»¥æˆ‘ä¼šå†™ `spark dot read
    dot option`ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå«åš option çš„ä¸œè¥¿ï¼Œåœ¨è¿™ä¸ª option ä¸­ï¼Œä½ å¯ä»¥åŸºæœ¬ä¸Šæä¾›ä¸€ä¸ªé€‰é¡¹ã€‚
- en: ğŸ˜Šï¼ŒHeaderï¼Œ like I seeï¼Œ there will be something like key value that you will be
    providing in optionã€‚ So what you can doï¼Œ you can just write headerã€‚Commer trueã€‚
    So whatever valueã€‚ the first column first row value will be thereï¼Œ that will be
    considered as your headerã€‚ And if I write Csv with respect to test1ã€‚ nowï¼Œ I'm
    just going to read this test1 dataset setã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼ŒHeaderï¼Œåƒæˆ‘çœ‹åˆ°çš„ï¼Œä¼šæœ‰ä¸€äº›åƒ key value ä½ å°†åœ¨ option ä¸­æä¾›çš„ä¸œè¥¿ã€‚æ‰€ä»¥ä½ å¯ä»¥è¿™æ ·åšï¼Œä½ å¯ä»¥å†™ `header.commer
    true`ã€‚è¿™æ ·ï¼Œç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œçš„å€¼å°†è¢«è§†ä¸ºä½ çš„ headerã€‚å¦‚æœæˆ‘å†™ `Csv` ä»¥æµ‹è¯•1ï¼Œç°åœ¨ï¼Œæˆ‘å°±è¦è¯»å–è¿™ä¸ª test1 æ•°æ®é›†ã€‚
- en: Test 1 dot CS Svã€‚ Nowï¼Œ once I execute this hereï¼Œ you'll be able to see that
    I am able to get now name string H stringã€‚ Okayï¼Œ but let's see our complete data
    setã€‚ So here if I execute this nowã€‚ I'll be able to see the entire data set with
    this particular columnsã€‚ Okayã€‚ so let me just quickly save this in my Df underscore
    Pi sparkã€‚ğŸ˜Šï¼ŒAnd nowã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`Test 1 dot CS Sv`ã€‚ç°åœ¨ï¼Œä¸€æ—¦æˆ‘åœ¨è¿™é‡Œæ‰§è¡Œè¿™ä¸ªï¼Œä½ ä¼šçœ‹åˆ°æˆ‘èƒ½å¤Ÿå¾—åˆ°ç°åœ¨çš„ name å­—ç¬¦ä¸²å’Œ H å­—ç¬¦ä¸²ã€‚å¥½çš„ï¼Œä½†è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„å®Œæ•´æ•°æ®é›†ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œå¦‚æœæˆ‘ç°åœ¨æ‰§è¡Œè¿™ä¸ªï¼Œæˆ‘å°†èƒ½å¤Ÿçœ‹åˆ°æ•´ä¸ªæ•°æ®é›†ä¸è¿™äº›ç‰¹å®šçš„åˆ—ã€‚å¥½çš„ã€‚æ‰€ä»¥è®©æˆ‘è¿…é€Ÿåœ°å°†å…¶ä¿å­˜åœ¨æˆ‘çš„
    `Df underscore Pi spark` ä¸­ã€‚ğŸ˜Šï¼Œç°åœ¨ã€‚'
- en: let's go and see the type ofã€‚D F underscore pi spaï¼Œ okayã€‚Nowï¼Œ when I execute
    this hereã€‚ you'll be able to see guys when I was reading this Df right when I
    wasã€‚ if I go and see the type of this with the help of pandas here you'll be able
    to see that there is pandas dot code do frame do data frameã€‚ But here you'll be
    seeing that when you are reading this particular data setã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å»çœ‹çœ‹ `type of D F underscore pi spa`ã€‚å¥½çš„ã€‚ç°åœ¨ï¼Œå½“æˆ‘åœ¨è¿™é‡Œæ‰§è¡Œè¿™ä¸ªæ—¶ã€‚ä½ ä»¬ä¼šçœ‹åˆ°ï¼Œå¤§å®¶ï¼Œå½“æˆ‘åœ¨é˜…è¯»è¿™ä¸ª Df
    çš„æ—¶å€™ã€‚å¯¹ï¼Œå½“æˆ‘ä½¿ç”¨ pandas çš„å¸®åŠ©æ¥æŸ¥çœ‹è¿™ä¸ªç±»å‹æ—¶ï¼Œä½ ä¼šçœ‹åˆ°æœ‰ `pandas dot code do frame do data frame`ã€‚ä½†åœ¨è¿™é‡Œï¼Œä½ ä¼šçœ‹åˆ°ï¼Œå½“ä½ åœ¨è¯»å–è¿™ä¸ªç‰¹å®šæ•°æ®é›†æ—¶ã€‚
- en: it is of type Pipar do sQl do data frame do data frameã€‚ Yesã€‚ so that is pandas
    data frame is sQL dot data frame data frameã€‚ Yesã€‚ most of the Apis are almost
    sameã€‚ The functionalities are same There a lot of things that we will be learning
    as we go aheadã€‚ butã€‚ğŸ˜Šï¼ŒIf I quickly want to see myã€‚Probablyï¼Œ I don't know whether
    head will workã€‚ Let's seeã€‚ Yesã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯ `Pipar do sQl do data frame do data frame` ç±»å‹ã€‚æ˜¯çš„ã€‚æ‰€ä»¥ pandas æ•°æ®æ¡†æ˜¯ `sQL dot
    data frame data frame`ã€‚æ˜¯çš„ã€‚å¤§å¤šæ•° API å‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚åŠŸèƒ½ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚éšç€æˆ‘ä»¬ç»§ç»­å‰è¿›ï¼Œè¿˜æœ‰å¾ˆå¤šä¸œè¥¿æˆ‘ä»¬ä¼šå­¦ä¹ ã€‚ä½†ã€‚ğŸ˜Šï¼Œå¦‚æœæˆ‘å¿«é€Ÿæƒ³çœ‹åˆ°æˆ‘çš„ã€‚å¯èƒ½ï¼Œæˆ‘ä¸çŸ¥é“
    `head` æ˜¯å¦ä¼šå·¥ä½œã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æ˜¯çš„ã€‚
- en: head is also workingã€‚ So if I use dot headï¼Œ probably you'll be able to see the
    rose information I basically shown over hereã€‚ Nowï¼Œ if I really want to see the
    more information regarding my columns I will be able to use something called as
    print schema now in this this print schema is just like Df dot infoã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`head` ä¹Ÿå¯ä»¥ä½¿ç”¨ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä½¿ç”¨ `dot head`ï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°æˆ‘åœ¨è¿™é‡ŒåŸºæœ¬å±•ç¤ºçš„è¡Œä¿¡æ¯ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘çœŸçš„æƒ³çœ‹åˆ°å…³äºæˆ‘çš„åˆ—çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå«åš
    `print schema` çš„ä¸œè¥¿ï¼Œç°åœ¨è¿™ä¸ª `print schema` å°±åƒ `Df dot info`ã€‚'
- en: which will actually tell about your columns like name is string and age string
    Okay so all these are some basic operations that you have actually done after
    installer againã€‚ the main thing why I'm focusing on this is that just try to install
    this spice spark and keep it ready for my next session I will be trying to show
    you how we can change the data type how we can work with data framesã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®é™…ä¸Šä¼šå‘Šè¯‰ä½ å…³äºåˆ—çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ name æ˜¯å­—ç¬¦ä¸²ï¼Œage ä¹Ÿæ˜¯å­—ç¬¦ä¸²ã€‚å¥½çš„ï¼Œè¿™äº›éƒ½æ˜¯ä½ åœ¨å®‰è£…ä¹‹åå®é™…æ‰§è¡Œçš„ä¸€äº›åŸºæœ¬æ“ä½œã€‚æˆ‘è¦å¼ºè°ƒçš„ä¸»è¦åŸå› æ˜¯ï¼Œå°½é‡å®‰è£…è¿™ä¸ª
    Sparkï¼Œå¹¶ä¸ºæˆ‘çš„ä¸‹ä¸€ä¸ªè¯¾ç¨‹åšå¥½å‡†å¤‡ï¼Œæˆ‘ä¼šå°è¯•å‘ä½ å±•ç¤ºå¦‚ä½•æ›´æ”¹æ•°æ®ç±»å‹ä»¥åŠå¦‚ä½•å¤„ç†æ•°æ®æ¡†ã€‚
- en: how we can actually do data precrossingï¼Œ how we can handle null valuesï¼Œ missing
    valuesã€‚ how we can delete the columnsï¼Œ how we can do various thingsã€‚ all those
    things will basically really discussing over thereï¼Œ how to drop columns and So
    I hopeã€‚ğŸ˜Šã€‚![](img/ee846c25d771f30e25168217f8991826_6.png)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å®é™…è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå¦‚ä½•å¤„ç†ç©ºå€¼å’Œç¼ºå¤±å€¼ã€‚æˆ‘ä»¬å¦‚ä½•åˆ é™¤åˆ—ï¼Œå¦‚ä½•åšå„ç§äº‹æƒ…ã€‚è¿™äº›å†…å®¹åŸºæœ¬ä¸Šéƒ½ä¼šåœ¨é‚£å„¿è®¨è®ºï¼Œå¦‚ä½•åˆ é™¤åˆ—ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›ã€‚ğŸ˜Šã€‚![](img/ee846c25d771f30e25168217f8991826_6.png)
- en: Like this particular videoã€‚ So this is just my pie spark introductionï¼Œ okayã€‚And
    we will continue the next sessionã€‚ Probablyã€‚ I'll also give you this information
    in my Github where we will probablyã€‚ okayï¼Œ Pi Sp is already thereã€‚![](img/ee846c25d771f30e25168217f8991826_8.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åƒè¿™ä¸ªç‰¹å®šçš„è§†é¢‘ã€‚è¿™åªæ˜¯æˆ‘å¯¹PySparkçš„ä»‹ç»ï¼Œå¥½çš„ã€‚æˆ‘ä»¬å°†ç»§ç»­ä¸‹ä¸€ä¸ªç¯èŠ‚ã€‚å¯èƒ½ã€‚æˆ‘è¿˜ä¼šåœ¨æˆ‘çš„GitHubä¸Šæä¾›è¿™äº›ä¿¡æ¯ï¼Œé‚£é‡Œæˆ‘ä»¬å¯èƒ½ä¼šï¼Œå¥½çš„ï¼ŒPySparkå·²ç»åœ¨é‚£å„¿äº†ã€‚![](img/ee846c25d771f30e25168217f8991826_8.png)
- en: æˆ‘ã€‚Basic introductionï¼Œ fineï¼Œ So we will try to do this and we'll try to cover
    this a entire thing as we go ahead in the next sessionã€‚ Rememberï¼Œ guysï¼Œ againï¼Œ
    our main aim is basically to make you understand how probably well be working
    in cloudsã€‚ And before thatï¼Œ we really need to know all the basic stuffs that we
    need to understand regarding Piparã€‚ But yesï¼Œ it is an amazing library itll actually
    help us to implement all the spark APIpis functionalityities that are that basically
    supports with Pythonã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ã€‚åŸºæœ¬ä»‹ç»ï¼Œå¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å°è¯•å®Œæˆè¿™ä¸ªï¼Œæˆ‘ä»¬ä¼šå°½é‡æ¶µç›–æ‰€æœ‰å†…å®¹ã€‚åœ¨ä¸‹ä¸€ä¸ªç¯èŠ‚ä¸­ã€‚è®°ä½ï¼Œå¤§å®¶ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡åŸºæœ¬ä¸Šæ˜¯è®©ä½ ç†è§£æˆ‘ä»¬å¦‚ä½•åœ¨äº‘ç«¯å·¥ä½œã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬çœŸçš„éœ€è¦äº†è§£æ‰€æœ‰åŸºæœ¬çŸ¥è¯†ï¼Œå…³äºPiparã€‚ä½†ç¡®å®ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¤äººæƒŠå¹çš„åº“ï¼Œå®ƒå°†å¸®åŠ©æˆ‘ä»¬å®ç°æ‰€æœ‰ä¸Pythonç›¸å…³çš„Spark
    APIåŠŸèƒ½ã€‚
- en: So everything all the things that are related to machine learning we can also
    do with the help of Piã€‚ So I hope you like this particular videoll see all in
    the next video have a great dayã€‚ Thank you on dollã€‚ğŸ˜Šã€‚![](img/ee846c25d771f30e25168217f8991826_10.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä¸æœºå™¨å­¦ä¹ ç›¸å…³çš„å†…å®¹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å€ŸåŠ©Piæ¥å®Œæˆã€‚æ‰€ä»¥æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªè§†é¢‘ï¼Œæˆ‘ä»¬ä¸‹ä¸ªè§†é¢‘å†è§ï¼Œç¥ä½ æœ‰ä¸ªæ„‰å¿«çš„ä¸€å¤©ã€‚è°¢è°¢å¤§å®¶ã€‚ğŸ˜Šã€‚![](img/ee846c25d771f30e25168217f8991826_10.png)
