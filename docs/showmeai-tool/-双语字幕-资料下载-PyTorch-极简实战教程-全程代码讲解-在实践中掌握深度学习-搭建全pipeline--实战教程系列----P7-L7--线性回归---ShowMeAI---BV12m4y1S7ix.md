# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- çº¿æ€§å›å½’ - ShowMeAI - BV12m4y1S7ix

Hiï¼Œ everybodyã€‚ Welcome back to a new Pytarch tutorialã€‚ This timeï¼Œ we implement linear regressionã€‚ So we already implemented this step by step in the last couple of tutorialsã€‚ and this should be a repetition where we can apply all the learned concepts and quickly implement our algorithm againã€‚ğŸ˜Šï¼ŒSo as I've shown you beforeï¼Œ our typical pieytorch pipeline consists of those three stepsã€‚Firstã€‚

 we design our modelã€‚ So we define the input and the output sizeï¼Œ and then the forward passã€‚ Then we create our loss and optimizer functionsã€‚ And then we do the actual training loop with the forward passã€‚ the backward pass and the weight updatesã€‚ So let's do thisã€‚And first of allã€‚ we import a couple of things that we needã€‚ So let's import torchã€‚

 Then we import torch dot and N S and Nã€‚ So the neural network moduleã€‚ Then we import nuy S and P just to make some data transformationsã€‚ and then from S K learnã€‚ we import data setã€‚ So we want to generate a regression data setã€‚And then we also want to plot this laterã€‚ So I say import matplot clip the pie plot as P LTã€‚

And then we do our three stepsã€‚ So we design the modelï¼Œ step number oneã€‚Then step number twoã€‚ we define the loss and the optimizerã€‚And then step number 3ï¼Œ our training loopã€‚ So let's do thisã€‚ And first of allï¼Œ let's do a step 0 where we prepare our dataã€‚ So prepare dataã€‚ So let's generate a regression data setã€‚ and we can do this by saying let's call this x nuy and y nuy equalsã€‚

 and then we can use data sets dot make regressionã€‚ which getsï¼Œ let's say 100 samplesã€‚ So n samples equals 100ã€‚ and only one feature in this exampleã€‚ So n features equals  oneã€‚ Then we add some noiseã€‚ and let's also at a random stateã€‚ Let's say this is oneã€‚

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_1.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_2.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_3.png)

And then we want to convert this to a torch tenzoaã€‚ So we say x equalsã€‚ and then we can use the function torch dot from numyã€‚ and then we say x dotã€‚X underscore nuyã€‚But we want to convert this to aã€‚A float 32 data data type beforeã€‚ So right now this is a double data typeã€‚ So if we use a double hereã€‚

 then we will run into some arrows laterã€‚ So let's just convert this by saying S type and then say nuy dot float 32 and we do the same thing for our yã€‚ So we say y equals the torch tenzo from our nuy arrayã€‚And now let's also reshape our yã€‚ because right now this is a has only one rowï¼Œ and we want to make it a column vectorã€‚ So we want to put each value in one rowï¼Œ and the whole shape has only one columnï¼Œ soã€‚

Let's say y equals y dot viewã€‚ And here we put in the new size So y dot shape0ã€‚ So the number of of valuesã€‚ and then only one columnã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_5.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_6.png)

So the few method is a built in pi torch methodï¼Œ which will reshape our tenoneã€‚And then let's get the number of samples and the number of features by saying this is x dot shapeã€‚ So we can use this in a secondã€‚ And now let's do our three stepsã€‚ So now we have the dataã€‚ Now we define the modelã€‚And in the linear regression caseï¼Œ this is just one layerã€‚ So ourã€‚

 so we can use to build in linear modelã€‚ So we say model equals N N dot linearã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_8.png)

This is the linear layerï¼Œ which needs a input size of our features and a output sizeã€‚ So let's say input size equalsã€‚ This is the number of features we haveã€‚ So this is just one in our exampleã€‚ and the output size equals oneã€‚ So we only want to have one value for each sample that we want to put inã€‚

 So our model gets now the input and the output size So input size and output sizeã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_10.png)

And this is all we have to do to set up the modelã€‚And now let's continue with the loss and the optimizerã€‚ So let's call this criterionã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_12.png)

And here we can use a built in loss function from Pytorchã€‚ And in the case of linear regressionã€‚ this is the mean squared errorã€‚ So we can say this is N N dot M S E lossã€‚ So will calculate the mean squared errorã€‚ So this is a callable functionã€‚And then we also set up the optimizersã€‚ So we sayã€‚Opttimizer equalsã€‚

And let's say torch dot optim dot S G Dã€‚ So this is stochastic gradient descentã€‚ and our optr needs the parameters that it should optimizeã€‚ So here we can simply say this is model dot parametersã€‚ And then it needs a learning rateã€‚ So let's define this here as a variableã€‚ So let's say learning rate equalsã€‚ let's say 001ã€‚



![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_14.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_15.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_16.png)

And then Lr equals learning rateã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_18.png)

So this is step number 2ã€‚ And now let's do our training loopã€‚ Soï¼Œ first of allã€‚ let's define the number of epochsã€‚Let's say we want to do 100 training iterationsã€‚ and now forã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_20.png)

Epoch in range epochsã€‚ And now here we do our steps in the training loopï¼Œ the forward passã€‚ the backward pass and the update and the weight updatesã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_22.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_23.png)

So let'sï¼Œ first of allï¼Œ do the forward pass and also the loss hereã€‚Thenï¼Œ the backward passã€‚And then the updateã€‚ So the forward pass and the loss hereï¼Œ we can sayï¼Œ why predict itã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_25.png)

Equalsï¼Œ and here we call our modelã€‚ And as a dataï¼Œ it gets xã€‚ So this is the forward passã€‚ And then we compute the loss by saying loss equalsã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_27.png)

This is our cr we call this criterionã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_29.png)

And this needs the actual labels and the predicted valuesã€‚ So why predict and whyã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_31.png)

And nowï¼Œ in the backward pass to calculate the gradientsï¼Œ we just say lost dot backwardã€‚ So this will do the back propagation and calculate the gradients for usã€‚And then our update hereã€‚ we simply say optimizer dot stepã€‚ So this will update the weightsã€‚And then before the next iterationï¼Œ we have to be carefulã€‚ So we have to empty our gradients nowã€‚

 because whenever we call the backward functionï¼Œ this will sum up the gradients into the dot Gr attributeã€‚ So now we want to empty this againã€‚ And we simply say optimizer dot0 graã€‚ So you should never forget thisã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_33.png)

![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_34.png)

And then we are done with the draining loopã€‚ Let's also print some informationã€‚ So let's say ifã€‚Epoch plus 1 modular1s equals equals 0ã€‚ So every 10th stepï¼Œ we want to print some informationã€‚ So let's print the epochã€‚ And here we say epoch plus oneã€‚ and let's also print the lossã€‚ the loss equalsã€‚ And here we can can say loss dot itemã€‚ and let's format thisã€‚

 So let's plot or print only for decimal valuesã€‚ğŸ¤¢ï¼ŒSo now we are doneã€‚ and now let's also plot thisã€‚ soã€‚Let's sayï¼Œ let's get all the predicted values by saying predicted equals hereã€‚ we call our final model nowï¼Œ model Xã€‚And with all the dataã€‚And now we want to convert this to nu pipe back againï¼Œ but before we do thatã€‚

 we want to detach our tenzoorï¼Œ so we want to prevent this operation from being tracked in our graph and our computational graphã€‚Because right nowï¼Œ this tenorã€‚Here I have a typo predictedã€‚ So this tenor has the required gradients argument set to trueã€‚ But now we want this to fall to be falseã€‚ So this will generate a new tenzor where our gradient calculation attribute is falseã€‚

 So this is our new tenzoã€‚ And then we just call the numpy functionã€‚ Now we convert it to numpy and now plot thisã€‚ So let's say first plot all our dataã€‚Soã€‚ x nuy and y nuyã€‚And we want to plot this asï¼Œ let's say red dotsã€‚ And then we want to plot our generated or approximated functionã€‚ So let's say Pï¼Œ L T dot plotã€‚

X nuy on the X axis and our predicted labels on the Y axisã€‚And let's plot this in blueã€‚ And then we say P L T dot showã€‚ And now let's run this and hope that everything is correctã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_36.png)

And now this plot appears hereã€‚ So now we see that we have a pretty good approximation of our data with this lineã€‚ and we see that this is workingã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_38.png)

And yeahï¼Œ now were doneã€‚ I hope you enjoyed thisã€‚ If you like thisã€‚ please subscribe to the channel and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚![](img/ad2afccfb99dbe5b3a127586f9ddc4a3_40.png)