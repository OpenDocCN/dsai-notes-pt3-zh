# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P20ï¼šL20- RNN & LSTM & GRU - å¾ªç¯ç¥ç»ç½‘ç»œ - ShowMeAI - BV12m4y1S7ix

Hi and welcome to your new Pytorch tutorialã€‚ Todayã€‚ I show you how we can implement a recurrent neural net using the built in R and N moduleã€‚ In the last tutorialï¼Œ we implemented the R and N from scratchã€‚ and I highly recommend to watch this one first to understand the internal architecture of R Nsã€‚ğŸ˜Šã€‚

And now todayï¼Œ we focus on the implementation with Pytorch's own moduleã€‚ so we don't have to implement everything by ourselvesã€‚ I will show you how to use the RnN moduleã€‚ And then at the endï¼Œ I also show you how easily we can switch our Rn N model and use special kinds of RNs like LSTM and Gã€‚ So let's startã€‚ and we are going to use my tutorial about a simple neural net as starting pointã€‚

 So this is tutorial number 13 from my Pytorch beginner courseã€‚ You can find the link to the video and also to the code on Gitub in the description belowã€‚ So as I saidï¼Œ this is tutorial number 13ã€‚ So I already grab this code and copied it to my editorã€‚ And then this exampleï¼Œ we are doing diit classification on the Mnes data setsã€‚

 And I have to say that image classification is not the typical example for RnNsã€‚ But what I want to demonstrate here is how we must treat our inputã€‚ğŸ˜Šã€‚![](img/4923e3db142c0d58fa13de82f2b481db_1.png)

As a sequence and then set up the correct shapesï¼Œ and it also shows that RnNs can indeed be used to get a high accuracy on this classification taskã€‚ So last time we learned that the special thing about RnNs is that we work on sequences of vectors hereã€‚

 So we treat our input as a sequenceã€‚ and there can be different types of Rnnsã€‚ So in this example we used this many to one architectureã€‚ So here we have a sequence as a input and then only one output at the endã€‚ and this is our class label in this caseã€‚ So let's jump to the codeã€‚

 and the first thing we must change is the hyperparameterã€‚ So the Ms data set consists of images of size 28 by 28 pixelsã€‚ and last time we squeeze that into one dimensionsã€‚Our input size was 28 times 28 or 784ã€‚ and this timeï¼Œ as I saidï¼Œ we treat the image as a sequenceã€‚

 So what we do here now is we treat one image dimension as one sequence and the other image dimension as the input or feature sizeã€‚So you can see this as that we are looking at one row at a timeã€‚ So let's comment and this outã€‚ and let's create a new oneã€‚ So let's say our input size equalsã€‚ And nowï¼Œ as I saidã€‚ we are looking at one row at a timeã€‚ So this is 28ã€‚ And then we also create the sequence lengthã€‚

 And this is also 28ã€‚ And then we change the hidden size to be 128ã€‚ So you can try our different sizes hereã€‚ And we add another hyperparmeterã€‚ and this is the number of layersã€‚ And here I set this to2ã€‚ So by defaultï¼Œ it is oneã€‚ And this means that we are stacking in this caseï¼Œ two R and Ns togetherã€‚

 and the second R and N takes the output from the first R and n as an inputã€‚ So this can further improve our modelã€‚And now we want to implement the R and N classã€‚ So let's change the name to R and Nã€‚ and also in this super methodã€‚And then our model down here also now is the R and Nã€‚

 And now let's delete all of this to start with a newï¼Œ fresh implementationã€‚So now our R and N has still has the input size and the hidden size and the number of classes as parametersã€‚ And it also gets the new parameter number of layersã€‚ So let's put it in hereã€‚ So let's say the number of layers hereã€‚ And thenï¼Œ of courseã€‚

 we must also pass it to our model when we create itã€‚ So this is our hyperparameterã€‚ğŸ˜Šã€‚And then what we want to do here first is we simply want to store the number of layers and the hidden sizeã€‚ So let's say self nu layers equals nu layersã€‚And alsoï¼Œ self dotã€‚Hiddenã€‚Size equals hidden sizeã€‚And then we create the R and N model and use the built in Pytorch R and N moduleã€‚

 So you can find this here on the official documentationã€‚ So this is the R and N class that Pytorch provides for usã€‚ So we are going to use thisã€‚ So we create an R and N and say self R and N equalsã€‚ And now this is in the NN moduleã€‚ So NN dot R and Nã€‚![](img/4923e3db142c0d58fa13de82f2b481db_3.png)

![](img/4923e3db142c0d58fa13de82f2b481db_4.png)

And the R and N needs the input sizeã€‚ It needs the hidden sizeã€‚ and it needs the number of layers in this orderã€‚ And then we also use a parameter that is called batch first and set this to trueã€‚ So this just means that we must have the batch as a first dimensionã€‚ So our input needs to have the shapeã€‚Btch sizeã€‚Bch size and then the sequence lengthã€‚

 and then the input or feature sizeã€‚ So this is the shape that we need to have for our inputã€‚![](img/4923e3db142c0d58fa13de82f2b481db_6.png)

And againï¼Œ you can find this in the documentationã€‚ So if you set batch first to trueã€‚ then here you need this shapeã€‚![](img/4923e3db142c0d58fa13de82f2b481db_8.png)

So now what we want to do is before we pass the images to our modelã€‚ So last time we reshaped it to be this sizeã€‚ So originally our batch or our images have the size the batch size than a one and then 28 and then 28 againã€‚ So this time we only want to have our batch size and then 28 by 28ã€‚ So here we reshape it to be this size and then the 28ã€‚

 the first one is our sequence length and the second one is our input sizeã€‚ So these are both 28ã€‚ and the same in our so this is in our training loop and then later in our evaluation loopã€‚ we do the sameã€‚ So here we also have to reshape it toã€‚This sizeã€‚ So now we have our input in the correct shapeã€‚And now we need one more layerã€‚ So as I saidã€‚

 we are using this many to one architectureã€‚ So in the end we have a classification taskã€‚ So this means that we are using a linear layer and then later the soft marks and the crossenttropisã€‚ So let's create one more linear layerã€‚ So let's say self dot Fc for fully connected equals nn dot linearã€‚ And now here we want to be carefulã€‚ So for the input sizeã€‚

 we use the hidden size and the output size is the number of classesã€‚ and I will explain this later againï¼Œ but basically as we can see in this image or also in this image we only use the last time step of ourã€‚ence to do the classificationã€‚ So we only need the last hidden size as the input size for the linear layerã€‚ So this is basically the whole init functionã€‚ And nowï¼Œ of courseã€‚

 we also need to implement the forward passã€‚ So our R and Nã€‚ If we have a look at the documentationã€‚![](img/4923e3db142c0d58fa13de82f2b481db_10.png)

Then it needs two inputsï¼Œ and the one is the the first one is the inputã€‚ and the second one is the initial hidden stateã€‚ So we need this in the correct shapeã€‚ And so let's create an a tensor with just0ã€‚ So we say H0 equalsã€‚ And then let's say torch dot0sã€‚ And then here the first one is the number of layersã€‚ The second one is the batch sizeã€‚

 So we get this by saying x dot size0ã€‚ The next dimension is the hidden sizeã€‚ So we say self dot hidden sizeã€‚ And then we also want to push it to the deviceã€‚ If you're using oneã€‚![](img/4923e3db142c0d58fa13de82f2b481db_12.png)

So now this is our initial hidden stateï¼Œ and now we can call our R and N modelã€‚ So we say out and then a underscore because we don't need thisã€‚ and then we say self dot R and Nã€‚ and this gets x and H0ã€‚ So againï¼Œ let's have a look at the documentationã€‚ So it delivers two outputsã€‚ and the first tenor contains the output features or the hidden states from all the time stepsã€‚

 And the other one is just the hidden state for the step Nã€‚ So we don't need this in this caseã€‚ So now we have the output and the output is of sizeã€‚ This is batchã€‚![](img/4923e3db142c0d58fa13de82f2b481db_14.png)

![](img/4923e3db142c0d58fa13de82f2b481db_15.png)

Btch sizeï¼Œ and then we have the sequence lengthï¼Œ and then we have the hidden sizeã€‚ So this is the new shape of our outputã€‚And now what we want to do is we want to decocode the hidden state only of the last time stepã€‚So what we have here againï¼Œ let's write this in numberã€‚ So this is nã€‚ and then 28ã€‚ and our hidden size is 128ã€‚ And now we only want the last time stepã€‚

 So we want to have our out to be in n and then 128ã€‚ So we get this by saying out equals outã€‚ And then we use this slicing here and take allã€‚The samples in our batchã€‚ and then only the last time stepã€‚ So we can say-1ã€‚ And then againã€‚ a colon for all the features in the hidden sizeã€‚ So now we have our out in this sizeã€‚

 And now that's why we need the hidden size as the input size for our linear layerã€‚ So now we can call thatã€‚ So now we can say out equal self dot fully connected with our outã€‚ And then we return the outã€‚ So now this is the whole implementation that we need for our R and Nã€‚ Soã€‚Everything else stays the same in our training and evaluation loopã€‚ And againã€‚

 what we have to be careful here is to treat our input as a sequenceã€‚ And then when we use the build in R and N that we use the correct shapeã€‚ and then we need the initial hidden state also in the correct shapeã€‚ And then we have to reshape it before we pass it to our fully connected layerã€‚ So let's try it outã€‚

 So let's say Python main dot piã€‚ğŸ˜Šï¼ŒAl rightï¼Œ so now training is done and as you can seeã€‚ we get a accuracy of 93%ã€‚ so our R and N works and you can see that it can be applied on this classification taskã€‚![](img/4923e3db142c0d58fa13de82f2b481db_17.png)

And now at the endï¼Œ I also want to show you two more R and N modulesã€‚ So two special kindsã€‚ The first one is the GR U or gated recurrent unitã€‚And the second one is the LSTM or long short term memoryã€‚ So both both are also very popular R and Nsã€‚ And I will not explain the theory about them right nowã€‚

 I will just show you how easily we can use them as well with this implementationã€‚ So let's use the R U firstã€‚ So we can simply say N N dot G R Uã€‚ And let's also call this self dot G R U and down here self dot G R Uã€‚ And everything else stay exactly the sameã€‚ So it takes the same input parametersã€‚

 It also needs this hidden stateã€‚ And then the output is in the same shapeã€‚ So now let's run this with the G R U and test thisã€‚![](img/4923e3db142c0d58fa13de82f2b481db_19.png)

Alrightï¼Œ so now as you can seeï¼Œ the GR U works tooã€‚ So the accuracy was even higher hereã€‚ And now as last thingï¼Œ let's also try the LSTMã€‚ So as you might know for the LSTMã€‚ we need an initial cell stateã€‚ So let's use the LSTMã€‚ So let's first call the cell dot LSTMã€‚ And then here we use Nn dot LSTMã€‚ The input parameters are still the sameã€‚

 And then here we need an initial tenor for the cell stateã€‚ So let's call this C0ã€‚ And this has the same shapeã€‚ And then here we call the self dot LSTMã€‚ And this needs the hidden state and the cell state as a as a tuple hereã€‚ğŸ˜Šã€‚![](img/4923e3db142c0d58fa13de82f2b481db_21.png)

![](img/4923e3db142c0d58fa13de82f2b481db_22.png)

So now this is all we need to apply the LSTMã€‚ So let's clear this and run it one more timeã€‚Alrightã€‚ so this one work 2ã€‚ and you can see the accuracy is 97%ã€‚ Soï¼Œ yeahã€‚ so now you know how you can implement a R and N in Pyar using the build in R and N moduleã€‚ and you also know how you can use the tier U and the LSTMã€‚ And yeahã€‚

 I hope you enjoyed this tutorialã€‚ If you liked itã€‚ Then please consider subscribing to the channel and hit the like button and see you next time byeã€‚ğŸ˜Šã€‚![](img/4923e3db142c0d58fa13de82f2b481db_24.png)