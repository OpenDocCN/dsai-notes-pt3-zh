# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P22ÔºöL22- PyTorch LightningÔºö‰∏ìÈó®‰∏∫Êú∫Âô®Â≠¶‰π†Á†îÁ©∂ËÄÖÂºÄÂèëÁöÑPyTorchËΩªÈáèÂåÖË£ÖÂô® - ShowMeAI - BV12m4y1S7ix

HeyÔºå guysÔºå welcomel to your new Pytor tutorial„ÄÇ TodayÔºå we'll be talking about Pytor lightning„ÄÇ Pytorch lightning is a lightweight pytor wrapper„ÄÇ It aims to reduce boiler plate codes so that implementing the algorithm and also improving your model„ÄÇ So tweaking and optimizing it can be way faster„ÄÇ You don't have to remember all the tiny details of the Pytor framework„ÄÇ because lightning takes care of all that„ÄÇüòäÔºåAnd another great feature is that it prints out warnings and gives you helpful machine learning tips or hints„ÄÇ

 if you make mistakes„ÄÇ So we will see all that later„ÄÇ So usually I'm skeptical when it comes to wrapper frameworks that abstract away a lot of things„ÄÇ But this timeÔºå I can really recommend it„ÄÇ I still advise that you learn the underlying basics first„ÄÇ and if you haven't yetÔºå then you can check out my free Pytor course here on YouTube„ÄÇ

 The link is in the description„ÄÇ But if you are already familiar with Pytorch„ÄÇ then you can check out Pytoch lightning and see if you like it or not„ÄÇ So today„ÄÇ I'll be converting one of the codes from my Pytor course to Pytorch lightning„ÄÇ So you can get a feel how this framework works and how you can use it„ÄÇ AllrightÔºå let's jump into it„ÄÇ

 So first of allÔºå Pytorch lightning is open source„ÄÇ So you can find it here on Github„ÄÇ And they also have a website with a nice documentation that you get you started„ÄÇ I will put„ÄÇüòä„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_1.png)

The links in the description as well„ÄÇ And now here are some of the details that you no longer have to worry about with Pytorch lightning„ÄÇ so you no longer have to worry about when to set your model to training or evaluation mode you don't have to worry about using a device for GPU support and then push your model and all your tenss to the device you can easily turn off GPu or even GPU support with lightning and you can easily scale it up then you also have to no longer care about the zero grad or calling backward and optimize a step you no longer have to worry about using torch no grad or detach and as a bonus you have integrate a tenor or support„ÄÇ

And it also prints out tips or hintsÔºå which we will see later„ÄÇ So let's jump to the code„ÄÇ And for this exampleÔºå we'll be taking one of the codes from my Pytoch tutorial„ÄÇ So you can find it on Github„ÄÇ and then in this caseÔºå we take tutorial number 13„ÄÇ So this is a simple feed forward neural netÔºå which is applied to the Mist data set to do diit classification„ÄÇ

 So now let's grab some of the code and write a new code with Pytoch lightning„ÄÇ So first of all„ÄÇ so let me delete this„ÄÇ and now let's start with a fresh Python script„ÄÇ And by the way„ÄÇ when you want to install Pytoch lightningÔºå you have two common options„ÄÇ The first one is to use Pip„ÄÇ So you just say Pip install Pytoch lightning„ÄÇ Or if you use in Conda„ÄÇ

 then you can grab this command„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_3.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_4.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_5.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_6.png)

So I already did this in my terminal„ÄÇ So Pytoch lightning is already insult here„ÄÇ So let's go back to the code and copy and paste some of the things„ÄÇ So we want to have all the same import statements„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_8.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_9.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_10.png)

And as an addition nowÔºå we also import Pytorch lightning„ÄÇ So we say import Pytorrch lightning with an underscore S PÔºå L„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_12.png)

And now we want to convert our modelÔºå our neural net to a lightning model„ÄÇ So we grab this code„ÄÇ and then we copy and paste it in here And now instead of deriving from N and dot module„ÄÇ we now derive from P dot lightning module„ÄÇ This will give us the same functions as the original model„ÄÇ but it will also give us some more functionsÔºå which we will see in a second„ÄÇ

 So now the in it function is still the sameÔºå and the forward function also is still the same„ÄÇ So we then need all the hyperparameters„ÄÇ So let's grab them as well„ÄÇ So let's grab these hyperparameters and paste them in here„ÄÇüòä„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_14.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_15.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_16.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_17.png)

HereAnd now we need to implement some more functions„ÄÇ So let's quickly go to the official website Here„ÄÇ you see a step by step guideÔºå by the way„ÄÇ And you also have a nice comparison how Pytch code looks versus Pytoch lightning and what it abstracts away„ÄÇ So as you can seeÔºå we also need to define a training step„ÄÇ

 a configure optimizes function and a train data loader„ÄÇ So let's copy all of these in here„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_19.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_20.png)

And now for theÔºå let's start with the simplest one„ÄÇ So the configure optimizers there you simply put in the optimizer that you created„ÄÇ So in our original scriptÔºå we set up the optimizer here„ÄÇ So this is the atom optimizer„ÄÇ And then you return it„ÄÇ So we can return this one in one line„ÄÇ So this will be our optimizer„ÄÇ

 our atom optimizer with the learning rate from our hyperparmeters„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_22.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_23.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_24.png)

And then we want to in implement the training step„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_26.png)

So for the training stepÔºå we are doing exactly what we are doing in our training loop„ÄÇ and now we don't have no longer have to worry about our fall loops about unpackking our images and labels and then hear the optimizer0 grabs the backward and the step„ÄÇ

 So we simply grab this part„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_28.png)

And paste it in here„ÄÇ And as you see in the training step functionÔºå we get a batch and a batch index„ÄÇ So we unpack our batch into x and y„ÄÇ and then let me copy the code„ÄÇ So„ÄÇThis is now our images and our labels„ÄÇ And then we have to reshape our images„ÄÇ and we no longer have to push it to the device„ÄÇ So we remove all of those things„ÄÇ

And then we do the forward pass„ÄÇ And here we can actually use self because we are inside of our model class„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_30.png)

And then we apply the criterion„ÄÇ and in our original code„ÄÇ we set up the criterion here as N and dot cross entropis„ÄÇ But we can also use it instead if we use the functional module„ÄÇ So for thisÔºå we say„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_32.png)

Import torch dot NÔºå NÔºå dot funk„ÄÇChannel S F„ÄÇAnd then down hereÔºå we say our loss equals„ÄÇ and now we can call„ÄÇF dot cross entropy with our outputs and the labels„ÄÇ And these are all the things„ÄÇ And for nowÔºå we only want to return a dictionary with this loss„ÄÇ So Pyto lightning needs this dictionary so that it can show you the loss during the training„ÄÇ

And this is all we need in our training step„ÄÇ And then we also have to implement the train data loader function„ÄÇ So what we want to do here is we want to do what we did in the beginning here we set up the training data set and the training data loader„ÄÇ

 So let's copy this and paste this in the function here„ÄÇ So first our training data set„ÄÇ So this is this one„ÄÇ And then the train data loader„ÄÇ So this one„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_34.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_35.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_36.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_37.png)

And then we want to return the train data loader or train loader„ÄÇ And now this is all we need to start a training„ÄÇ And now what we want to do is we say if„ÄÇName equals equalsÔºå underscore underscore main„ÄÇThen we have to set up a lightning trainer„ÄÇ So for thisÔºå we import a trainer„ÄÇ So we say from Pytorch lightning import trainer„ÄÇ

 Then we want to set up a trainer„ÄÇ So here we say our trainer equals our lightning trainer„ÄÇ And for nowÔºå I can show you a trick that is very helpful during development„ÄÇ So you say fast defron equals true„ÄÇ This will run a single batch through train„ÄÇ And also through validation„ÄÇ if we have a validation step„ÄÇ And with this„ÄÇ

 you can test if your model works„ÄÇ So now we also have to set up our model„ÄÇ So we say model equals„ÄÇ And let's call this lit model to make it clear that this is a lightning model„ÄÇ So lit„ÄÇüòä„ÄÇuralNeAnd here we say let neural net with the hyperparameter„ÄÇ So it needs the input size„ÄÇ It needs the hidden sizeÔºå and it needs the number of classes„ÄÇ

 which we all have here for our hyperparameters„ÄÇ Then we have to fit it to our trainer„ÄÇ So we say trainer dot fit our model„ÄÇ And now we can already run it and see if this is working„ÄÇ So now„ÄÇLet's go to the console„ÄÇ And now let's run this file„ÄÇ So we say Python lightning dot pi„ÄÇHere we get a error„ÄÇ SoÔºå of courseÔºå I rename the lital net„ÄÇ So I also have to say„ÄÇ

Let neural net here„ÄÇ And now againÔºå let's run it„ÄÇAnd now we see it starting„ÄÇ so we see that we have no GP support„ÄÇ We also get an overview of our different layers and the parameters„ÄÇ Then since I'm running it the first timeÔºå it's downloading the data set„ÄÇAnd then here is the training„ÄÇ So it worked„ÄÇ So it only did one batch because we set fast defffron equals true„ÄÇ

 So let's clear it and test it one more time„ÄÇ So now it shouldn't download it anymore„ÄÇAnd this was way faster„ÄÇ And now hereÔºå for exampleÔºå we get one warningÔºå so it says„ÄÇThe data loader train data loader does not have many workersÔºå which may be a bottleneck„ÄÇ Consider increasing the value of nu workers argument„ÄÇ Try 4„ÄÇ

 So this is actually the first tip that can help us to speed up our code„ÄÇ So here„ÄÇ what we want to do is in our train loader„ÄÇ We can also give it the argument„ÄÇ nu workers equals 4„ÄÇ And now let's clear this and run it again„ÄÇAnd now our warning is goneÔºå So now„ÄÇHere we see the overview„ÄÇ We now only used one epoch„ÄÇ and here we have the loss„ÄÇ

 So now if you want to have a full trainingÔºå we can also give our trainer„ÄÇ the argument max epoch equals„ÄÇ and then the number of epochs that we defined„ÄÇ epochs„ÄÇ So this is one of the hyperparameters„ÄÇAnd now let's set this to false again„ÄÇ So now this should do a full training„ÄÇ And nowÔºå let's see if this is working„ÄÇ

And now this should take„ÄÇMax eposÔºå sorry„ÄÇNow this should take a little longer„ÄÇAnd yeahÔºå we see„ÄÇ So we get a nice progress barÔºå and we see our training worksÔºå and our loss should slowly decrease„ÄÇ So this is working„ÄÇüòäÔºåSo nowÔºå let me stop this for now„ÄÇ And now the next thing we want to do is to do our evaluation or testing„ÄÇ

 So similar to the training step and training data loader„ÄÇ We now also want to add a validation step and a validation data loader„ÄÇ So let me copy this„ÄÇ And let me paste it in here„ÄÇ And this has to be called validation step„ÄÇAnd here we are doing the same thing actually„ÄÇ So as in the test train training step„ÄÇ

 So we reshape our imagesÔºå then we do the forward pass and calculate the loss„ÄÇ And this time we use the key well loss or validation loss„ÄÇ and now let's set our fast defron to true again„ÄÇ and let me run it again„ÄÇ So now if we run it„ÄÇ we should get an error„ÄÇ And yeahÔºå here we get the exception„ÄÇ you have defined validation step„ÄÇ

 but have not passed in a validation data loader„ÄÇ So now we know what we have to do„ÄÇ We also have to use a validation data loader„ÄÇ So let me copy and paste this„ÄÇAnd this has to be called well data loaddown„ÄÇ So if you're not sure you can go to the official„ÄÇ

![](img/8f1dbba751e44013b8cee2c7d55082c5_39.png)

DocumentationÔºå and then scroll to the validation loop„ÄÇ There„ÄÇ you see the„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_41.png)

3 functions that you needÔºå so„ÄÇNowÔºå first of allÔºå now we have our„ÄÇValidation data set„ÄÇ So here we want to sayÔºå or we want to grab„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_43.png)

The next or the test data set„ÄÇ So in this caseÔºå I call the test data set„ÄÇ but actually„ÄÇ you want to have„ÄÇA split of your data to a training data set„ÄÇ a validation data set and a test data set„ÄÇ So the validation data set is used to get an unbiased evaluation of the model performance while tuning the hyperparameters„ÄÇAnd then the test dataset set is used at the very end to provide an unbiased evaluation with data that the model has never seen before„ÄÇ

 So in this exampleÔºå weÔºå let's say we only have two splits of our data set„ÄÇ Now„ÄÇ we only use the first two steps training and validation„ÄÇ So let me copy this and paste it in here„ÄÇ And let's rename this to a validation data set„ÄÇ And then let's grab the validation data loader„ÄÇ

![](img/8f1dbba751e44013b8cee2c7d55082c5_45.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_46.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_47.png)

And rename it to Val loader and then return the v loader„ÄÇAnd now let's run it again„ÄÇ So let's say Python lightning dot pi„ÄÇAnd we get anarrow„ÄÇ So data set equals well data set„ÄÇ of course„ÄÇ And let's clear this and run it again and then it should do one pass through training and validation„ÄÇ and we see it worked„ÄÇ And now we againÔºå get the same warning„ÄÇ

 So here we use the nu workers equals 4„ÄÇ So let's clear it and try it again„ÄÇ So you see it already starts giving us hints when we make some mistakes„ÄÇ And now we see it worked„ÄÇ And now it's also to suggesting to define a validation epoch and method for accumulating stats„ÄÇ So let's go to the documentation and grab this function to see how it looks like So„ÄÇ



![](img/8f1dbba751e44013b8cee2c7d55082c5_49.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_50.png)

We copy and paste this„ÄÇ So this is the function that is executed after each validation epoch„ÄÇ And here we want to calculate the average loss„ÄÇ So actuallyÔºå in our example„ÄÇ we want to do exactly the same„ÄÇ So we want to call„ÄÇ calculate the average loss by saying this is torch stack„ÄÇ And then since we„ÄÇ

Called the key validation loss„ÄÇ We can access it here and then calculate the mean over all the„ÄÇLossesÔºå and nowÔºå for nowÔºå let's not worry about this„ÄÇ And then we return the average loss as a dictionary„ÄÇAnd then we are done„ÄÇ So nowÔºå if we clear it„ÄÇThenÔºå we should get a„ÄÇ1 pass now no more warning„ÄÇAnd now we have everything that we need for our coat„ÄÇ

 So this is all we need with pie touch lightning„ÄÇAnd nowÔºå let me show you one more hint„ÄÇ what we can get„ÄÇ SoÔºå for exampleÔºå in our validation loader„ÄÇ if we accidentally set shuffle equals true and then try to run it„ÄÇ we should get a warning or a hint„ÄÇSo yeahÔºå here we see user warning„ÄÇ

 Yourve data loader has shuffle equals true„ÄÇ It is best practice to turn this off for validation and test data loadus„ÄÇ So you seeÔºå we can actually already improve our code with this framework„ÄÇAnd now„ÄÇ let's compare this with our„ÄÇOriginal code„ÄÇ So if we have a look at this code in Github„ÄÇ

![](img/8f1dbba751e44013b8cee2c7d55082c5_52.png)

Then we no longer need this device functionality„ÄÇAnd we no longer needs to have the manual for loop over all epochs and over all the batches„ÄÇ we no longer have to care about these two device calls„ÄÇ we no longer have to care about the optimizer and the backward pass and also we can leave out the whole validation loop because we can calculate this with this validation step and now„ÄÇ for exampleÔºå if you also want a training stepÔºå we can easily edit the same way as we are doing with this function and with this function and now„ÄÇ

 for exampleÔºå if you want to use GPusÔºå then we can easily do this by giving our trainer the argument„ÄÇ GPus equalsÔºå for exampleÔºå one and when you want to scale it up and have more GPus available„ÄÇ you can also use„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_54.png)

2 or let's say4Ôºå or you can use even T support„ÄÇ So it's really easy to switch from CPU to GP and then scale it up„ÄÇ And you can also have a distributed backend„ÄÇ So DPÔºå for example„ÄÇYou can also easily switch to 16 bit precision„ÄÇYou can lock the GP memory„ÄÇ So a lot of features that you can now very easily test with this framework„ÄÇ

 You just have to use all the different arguments for the trainer„ÄÇ AndÔºå for example„ÄÇ one more helpful function that you can try is auto L R„ÄÇFind equals true„ÄÇ This will run a„ÄÇAlgorithm to find the best learning rate„ÄÇ So you can try this one„ÄÇYou can also setÔºå for example„ÄÇ deterministic equals true„ÄÇ So this allows it to reproduce your results„ÄÇ

 or you can apply gradient clipping very easily by just passing in the argument Claient gripve equals„ÄÇ and then some value between 0 and oneÔºå I guessÔºå So for thisÔºå you again„ÄÇ have to check out the official documentation„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_56.png)

There you find all the different features and use cases„ÄÇ And now let me show you the full training and validation„ÄÇ So let's remove this„ÄÇ And now let's set the argument fast defffron to false again„ÄÇ So again„ÄÇ I'm not having a high number of epochs„ÄÇ So it's only two because it takes a long time here without the GP„ÄÇ

 But let's run it anyway„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_58.png)

So againÔºå we see the progress in our first epoch here and how our loss is decreasing„ÄÇ And when this is doneÔºå we should a faster progress bar for the validation loop„ÄÇSo now be careful„ÄÇ Now here we see the validation loop„ÄÇ and now our second training epoch starts„ÄÇ SoÔºå yeah„ÄÇ this is working„ÄÇAnd now the second validation and now it's done„ÄÇ

 and we can see and inspect the final loss„ÄÇ So this worked„ÄÇ And now as the last thing„ÄÇ let me show you the integrated tenor board support„ÄÇ So as you can see„ÄÇ there is all already a folderÔºå which is called lightning lock„ÄÇ So it's already automatically saving some checkpoint„ÄÇüòäÔºåSo and now„ÄÇ

 if we want to inspect the different losses during in the tensor boardÔºå then what we have to do is„ÄÇ for exampleÔºå if you want to inspect the training„ÄÇ Then in our training step„ÄÇ we create another dictionary„ÄÇ and let's call this tenor board locks equals„ÄÇ and this is an again„ÄÇ a dictionary„ÄÇ and here let's call this train loss as a key and as valueÔºå it's the same loss„ÄÇ

 And then we also append the tensor board loss to our dictionary„ÄÇ And we have to give it the key lock and then lock„ÄÇ And this is our tensor board locks„ÄÇAnd now the same in our validation epoch end„ÄÇ So here we create the tensboard locks„ÄÇ And here let's call it average validation loss„ÄÇ And this is the average„ÄÇAverage loss„ÄÇ

 And then here againÔºå we used the key lock and then the tenzo board locks„ÄÇ And now let's run it one more time to save all those things„ÄÇSo now we have to wait until this is done„ÄÇSo now our training and validation is done„ÄÇ So now it should have„ÄÇSave all of those to the lock directory„ÄÇ

 And now we can start the tensor board by saying tensor board minus minus lock„ÄÇD equals„ÄÇ and then light ning locks„ÄÇ And by the wayÔºå I also have a full tutorial about how you can work with the Tensor board„ÄÇ I will also put the link in the description„ÄÇAnd with lightning„ÄÇ you don't have to install it manually become because it comes with the requirements of lightning„ÄÇ

 So this should automatically work now„ÄÇ So now if we hit run„ÄÇ and sorryÔºå this was no underscore„ÄÇinus minus loar equals lightning locks„ÄÇAnd now it works„ÄÇ So now our tensor board is running here„ÄÇ So now if we open up this„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_60.png)

Then we should inspect the epochs„ÄÇ Then we have the average validation loss„ÄÇ So since we only use two„ÄÇEpochsÔºå we don't see a lot of difference here„ÄÇ So it's only a one line„ÄÇ But for the training lossÔºå this is what we looked after each„ÄÇTraining step„ÄÇ So here we see a nice graph how our training loss decreased„ÄÇ So yeahÔºå you see„ÄÇ

 we have automatic tensor board integrationÔºå and it's very easy to work with this tool„ÄÇ So yeah„ÄÇ these are most of the features I wanted to show you„ÄÇ and let me know in the comments if you like this framework„ÄÇ And if you also think that it can simplify your development process„ÄÇ And if you like this video„ÄÇ

 then please hit the like button and consider subscribing to the channel„ÄÇ this helps me a lot and see next time by„ÄÇ![](img/8f1dbba751e44013b8cee2c7d55082c5_62.png)

![](img/8f1dbba751e44013b8cee2c7d55082c5_63.png)