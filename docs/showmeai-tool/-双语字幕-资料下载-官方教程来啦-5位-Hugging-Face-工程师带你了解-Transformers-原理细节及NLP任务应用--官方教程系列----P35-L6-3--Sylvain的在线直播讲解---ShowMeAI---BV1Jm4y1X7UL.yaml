- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P35：L6.3- Sylvain的在线直播讲解 - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P35：L6.3- Sylvain的在线直播讲解 - ShowMeAI - BV1Jm4y1X7UL
- en: Yeah。Welcome to the live session where we'll go over chapter2 of the Iing this
    course。 I'm joined by Lewis who on the chat and we' is going to answer all your
    question quicker than I will and don't hesitate to ask all your questions because
    I'm going to read themlo and answer them on the live stream that's like the main
    advantage of following this live stream instead of just watching the course by
    yourself。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，欢迎来到直播环节，我们将讨论本课程的第二章。我和聊天中的路易斯一起，他会比我更快回答你的所有问题，不要犹豫，尽量问出你的问题，因为我会在直播中阅读并回答它们，这也是跟随这个直播而不是自己观看课程的主要优势。
- en: So over inside this chapter2， we all look at the pipeline object that we used
    at length during chapter 1 on all NLP tasks。And we'll see exactly how it works，
    we'll see how it loads a model。 how it preprocesses the inputs with a tokenizer，
    and then how it processes with output to get the predictions and probabilities
    that we got during chapterpt1。So we'll watch a few videos that answer all the
    questions that you have and we'll do more life coding than in chapter 1 because。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们将详细查看在第一章中用于所有 NLP 任务的管道对象。我们将确切了解它是如何工作的，如何加载模型，如何使用分词器预处理输入，以及如何处理输出以获取我们在第一章中得到的预测和概率。因此，我们将观看一些视频，回答你所有的问题，并且我们会比第一章进行更多的现场编码。
- en: Because chapter 1 was just a general introductionction and varies a lot of more
    code in chapter 2。So as an introduction， as you may know， again face is mainly
    known for its Transformers library。 which is a library containing a lot of transformers
    model。 and it provides an easy API to download pre trade model and to use the
    different architectures。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因为第一章只是一个总体介绍，第二章包含了更多的代码。所以作为一个介绍，正如你所知，Hugging Face 主要以其 Transformers 库而闻名。这是一个包含众多变换模型的库，它提供了一个简单的
    API 来下载预训练模型并使用不同的架构。
- en: I think there are more than 60 architectures now available into the library。And。It
    all exposes unified API but inside and provides you with either a torch module
    or a tons of flu care model that you can use by yourself or that you can。A train
    with the API that the library also provides。And the goal is the views flexibility
    and simplicity and the library doesn't contain any abstraction at all。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为现在库中可用的架构超过60个。它全部暴露了统一的 API，并提供了一个 torch 模块或大量的 Flu Care 模型，供你自行使用或通过库提供的
    API 进行训练。目标是实现灵活性和简单性，库中没有任何抽象。
- en: it's not a library composed of building blocks， every model。 every one of the
    60 architectures I was talking about is completely defined in its own modeling
    files so we can have a quick look for instance at the modeling B file which contains
    also the code of the bird model inside the library as if you look at the input
    you see like there are just torch imports and then some internal classes that
    this uses but we share between all models which are mainly the output types that
    we use will see exactly what the outputs are a little bit later when we could
    but there is no other inputs where is not like an attention block that we reuse
    the Que model the attention block for the B model is defined inside this modeling
    file so we have the B buildingsddings you have the B self attention etca。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个由构建模块组成的库，每个模型、我提到的60个架构中的每一个都是在自己的建模文件中完全定义的。因此我们可以快速查看建模 B 文件，它也包含库内鸟模型的代码。如果你查看输入，你会发现只是一些
    torch 导入，然后是它使用的一些内部类，但我们在所有模型中共享的主要是我们使用的输出类型。我们稍后会确切看到输出，但没有其他输入，像重用的注意力模块对于
    B 模型的注意力块是在这个建模文件内部定义的，所以我们有 B 构建，你有 B 自注意力等等。
- en: etc。![](img/0e882caa799deede0a88d67b58737bfe_1.png)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！![](img/0e882caa799deede0a88d67b58737bfe_1.png)
- en: The idea is that if you want to play around with the model and change a line
    of code inside the model you won't have to learn 50 different files。 it's not
    subclassing something， that subclassing something， that' subclass something， everything。
    oh， I'm guessing I'm just realizing I'm hiding a bits okay， I guess I'm just going
    to scroll faster。You have everything in that modeling belt file and you can play
    around and modify everything you want if you want to experiment with it。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，如果你想玩弄模型并修改模型内部的代码行，你不需要学习50个不同的文件。并不是要进行多重继承，每一层都要继承，我突然意识到我隐藏了一些东西，看来我只是要更快地滚动。你在建模带文件中拥有所有内容，可以随意玩弄和修改，如果你想实验的话。
- en: and that's really one of the strengths of the transformformer library。 one of
    the features that our users have said very accurate。![](img/0e882caa799deede0a88d67b58737bfe_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是transformers库的一个优势，我们的用户表示这个功能非常准确。![](img/0e882caa799deede0a88d67b58737bfe_3.png)
- en: Which is why I wanted to show it to you briefly。And。So yeah。 we'll see how the
    pipeline API loads that B model。 it was actually a digitaltill B model that we
    use in chapter 1 and the corresponding dokenizer。And how to do everything that
    this pipeline function was doing by hand so that you can tweak any of those steps
    if you need to on your own tasks。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我想简要展示给你的原因。所以，是的，我们将看看管道API如何加载B模型。实际上，这是我们在第一章中使用的数字B模型及相应的分词器。并且如何手动执行这个管道函数所做的一切，以便你可以在自己的任务中调整这些步骤。
- en: So let's begin with the first section and first video we're going to watch this
    introductory video that' is going to present what's happening behind the pipeline
    I'm just not going to stream them from YouTube because that's making this the
    live stream like a lot I'm just I have all the videos look at it So if you give
    me just one minute。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们开始第一部分，以及我们要观看的第一段视频，这段介绍视频将展示管道背后发生的事情。我不会直接从YouTube播放，因为这会使直播变得很复杂，我已经拥有所有视频。如果你给我一分钟时间。
- en: I'm going to extract it。![](img/0e882caa799deede0a88d67b58737bfe_5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我要提取它。![](img/0e882caa799deede0a88d67b58737bfe_5.png)
- en: And played from my computer。![](img/0e882caa799deede0a88d67b58737bfe_7.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从我的电脑播放。![](img/0e882caa799deede0a88d67b58737bfe_7.png)
- en: Yeah。What happens inside the pipeline function？In this video。 we'll look at
    what actually happens when we use the pipeline function of the transformforms
    library。Now specifically， well look at the sentiment analysis pipeline and then
    we went from the two following sentences to the positive and negative labels with
    respective scores。As we've seen in the pipeline presentation， there are three
    stages in the pipeline。First。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，管道函数内部发生了什么？在这个视频中，我们将看看使用transformers库的管道函数时实际上发生了什么。现在具体来说，我们将查看情感分析管道，然后我们将从以下两个句子获得相应的正面和负面标签及其分数。正如我们在管道演示中看到的，管道有三个阶段。首先。
- en: we convert the redex to numbers the model can make signs of using a tokenizer。Then
    those numbers goes through the model， which outputs loads。Finally。 the best processing
    steps transform those delegates into labels and skull。Let's look in details at
    those three steps and how to replicate them using the Transform library。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重写表达式转换为模型可以使用的数字，并通过分词器进行处理。然后，这些数字经过模型，输出大量内容。最后，最佳处理步骤将这些代表转化为标签和分数。让我们详细了解这三个步骤，以及如何使用Transform库复制它们。
- en: beginning with the first stage tokenization。So to process has several steps
    first。 the text is split into small chunks called tokens。They can be words。 part
    of words or punctuation symbols， then the tokensizer will add some special tokens
    in the model expect。Here， the middle used expect a CLS token at the beginning
    and a S token at the end of the sentence to classify。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一阶段的分词开始。这个处理过程有几个步骤，首先，文本被拆分成称为令牌的小块。它们可以是单词、单词的一部分或标点符号，然后分词器将添加一些模型所期望的特殊令牌。在这里，中间使用的模型在句子开始时期望一个CLS令牌，在句子结束时期望一个S令牌，以进行分类。
- en: Lastly， the token isone patches each token to its unique ID in the vocabulary
    of the pro model。To load the tokenizer， the transformformers library provides
    the autotokenizer API。The most important method of this class is from Pretrained。
    which will download and cache the configuration and the vocabulary associated
    to a given checkpoint。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，令牌将每个令牌映射到专业模型词汇中的唯一ID。要加载分词器，transformers库提供了autotokenizer API。这个类中最重要的方法是from
    Pretrained，它会下载并缓存与给定检查点相关的配置和词汇。
- en: Here， the checkpoint used by default for the supplement analysis pipeline is
    distillbel baseline case5 tune SS2 English。 which is a bit of a mouthful。We instance
    to tookken as with a checkpoint。 and feed it to the two sentences。Since the two
    sentences are not the same size。 well need to pad the shed oneand to be able to
    build an array。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，补充分析管道默认使用的检查点是 distillbel baseline case5 tune SS2 English。这有点拗口。我们将实例化为
    token，作为一个检查点，并将其输入到两个句子中。由于这两个句子大小不同，我们需要对较小的句子进行填充，以便构建一个数组。
- en: This is done by the tokenizer with the option padding equal true。With truation
    equal true。 we ensure that any sentence longer and the maximum the model can handle
    is truncated。Lastly。 the return tensil option gal the tokenizer to return the
    bytch tensil。Looking as a result。 we see we have a dictionary with two keys， input
    ID contains the ideas of both sentences with zero where the padding is applied。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过将 padding 选项设置为 true 的 tokenizer 完成的。通过将 truncation 设置为 true，我们确保任何超过模型最大处理能力的句子都会被截断。最后，返回张量选项允许
    tokenizer 返回批处理张量。最终结果显示，我们得到了一个包含两个键的字典，input ID 包含两个句子的 ID，并在填充应用的地方用零替代。
- en: The second key attention mask indicates where petting has been applied。 so the
    model does not pay attention to it。This is always is inside the took step。Now
    let's have a look at the second step。这不懂。As for tokenizer。 or is a nottomod API
    over from pretrained method， it will download lu and cache the configuration of
    the model as well as the pretrain weight。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个关键注意力掩码指示了应用了填充的位置，以便模型不关注它。这始终是在 token 步骤内部。现在让我们来看第二步。这有点不明白。至于 tokenizer，或者说是一个基于预训练方法的
    nottomod API，它将下载并缓存模型的配置以及预训练权重。
- en: However， the autotomodl API will only instantiate the body of the model。 that
    is the part of the model that is left once the pro traininging head is removed。It
    will output a high dimensional tensor， that is a representation of the sentence's
    past。 but which is not directly useful for a classification problem。Here， the
    tensor has two sentences。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，autotomodl API 只会实例化模型的主体部分，也就是移除预训练头后的模型部分。它将输出一个高维张量，即句子的过去表示，但这对于分类问题并不直接有用。在这里，张量包含两个句子。
- en: each of 16 tokens， and the last dimension is the Indian size of our model， 768。To
    get an output link to our classification problem。 we need to use the Automodal
    for sequence classificationification class。It works exactly as zero to model class，
    except by 12 built a model with a classification head。😊。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 16 个 tokens，最后一个维度是我们模型的输入大小，768。要获取与我们的分类问题相关的输出链接，我们需要使用用于序列分类的 Automodal
    类。它的工作方式与 zero 到模型类完全相同，只是构建了一个带有分类头的模型。😊
- en: Praise one auto class for each common NLP task in the transformformers library。Here。
    after giving our models the two sentences， we get a tensor of size 2 by 2。 one
    result for each sentence and for each possible level。Those outputs are not probabilities
    yet。 we can see they don't sum to one。This is because each model of the transformformer's
    library returns look it。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 transformformers 库中为每个常见的 NLP 任务表扬一个自动类。在这里，在给我们的模型两个句子后，我们得到了一个大小为 2x2 的张量，每个句子和每个可能的级别都有一个结果。这些输出尚未是概率，我们可以看到它们的总和不为一。这是因为
    transformformers 库中的每个模型返回的是 logits。
- en: To make sense of those looks， we need to dig into the third and last step of
    the pipeline plus processing。To conduct Lo into probabilities， we need to apply
    a softmax layers to them。As we can see。 this transforms them into positive number
    that's a up to1。The last step is to know which of those corresponds to the positive
    or the negative label。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些 logits，我们需要深入分析管道处理的第三步和最后一步。为了将 logits 转换为概率，我们需要对它们应用 softmax 层。正如我们所看到的，这将它们转变为正数，总和为
    1。最后一步是确定哪些对应于正标签或负标签。
- en: this is given by the ID2lipal field of the model conflict。The first proba is
    index0。 correspond to the negative level and the second index1 correspond to the
    positive level。This is how our classifier built with the pipeline function peaked
    with labels and compute those scores。😊，Now that you know how each step works，
    you can easily tweak them to your needs。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由模型冲突的 ID2lipal 字段给出的。第一个概率是索引 0，代表负级别，第二个索引 1 代表正级别。这就是我们的分类器如何通过管道函数与标签结合并计算这些分数。😊
    现在你知道每一步是如何运作的，你可以轻松调整它们以满足你的需求。
- en: '![](img/0e882caa799deede0a88d67b58737bfe_9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_9.png)'
- en: 嗯。![](img/0e882caa799deede0a88d67b58737bfe_11.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。![](img/0e882caa799deede0a88d67b58737bfe_11.png)
- en: Yeah。![](img/0e882caa799deede0a88d67b58737bfe_13.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。![](img/0e882caa799deede0a88d67b58737bfe_13.png)
- en: 嗯对。哎呀。Back。嗯。I think that something。Wronong with webcam。 Let me just double
    double check。Okay。 somehow my head disappeared。 I don't know why exactly。 if you
    can see it。 feel free to say it in the chat。In the meantime， there is a question。
    what does the triple that signify in from that activations import act to function
    in the PY files shownone？
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，对。哎呀。回来了。嗯。我觉得摄像头出了点问题。让我再检查一下。好的，我的头不见了。我不知道为什么。如果你能看到，请在聊天中告诉我。与此同时，有一个问题。来自“从激活导入
    act 到 function”中的三重符号表示什么，在这些 PY 文件中展示？
- en: Very good question， so this is standard Python if you're building the package。In
    Python。 and you're trying to import things。You have several levels。 so it's because
    of the structure of the transformer repo。 let me try to pull it back and quit
    my VS code。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 很好的问题，这在构建包时是标准的 Python。在 Python 中，你尝试导入东西时，有几个层级。这是因为 transformer 仓库的结构。让我试着拉回去，退出我的
    VS Code。
- en: '![](img/0e882caa799deede0a88d67b58737bfe_15.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_15.png)'
- en: But。Rise either Here we are。 So you as a transformers folder and then inside
    that transformer models。 you have a model subfold and then a bird subfold and
    when the modeling file is here And since we've organized the good that way to
    avoid everything all the files directly in the transformers folder because as
    I said we have like 60 different architecture。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但是。好的，我们来了。所以你有一个 transformers 文件夹，然后里面是 transformer models，你有一个模型子文件夹，然后是 bird
    子文件夹，模型文件就在这里。由于我们将代码组织成这样，以避免所有文件直接放在 transformers 文件夹中，因为正如我所说，我们有大约 60 种不同的架构。
- en: So that would be a little of files there are organized like this And when you're
    trying to import when you say from dot it's to go back inside the structure。 So
    from import blah blah blah would be in the bird foldert is in the model folder
    and then thet gets back to the transformers folder。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些文件的组织方式是这样的，当你尝试导入时，使用“from dot”是为了回到结构内部。因此，从“import blah blah blah”会在
    bird 文件夹中，位于模型文件夹中，然后再返回到 transformers 文件夹。
- en: So it's just a way to come back to the root of the post directory。嗯。I I didn't
    see any of questions。 but don't hesitate to ask at any time in the chat your questions
    and I'll answer as best as I can and as I expected my webca is not showing anymore
    and I have no idea why because。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是一种回到帖子目录根目录的方式。嗯。我没有看到任何问题，但在聊天中随时提问，我会尽力回答。而且正如我预期的，我的摄像头不再显示，我也不知道为什么。
- en: '![](img/0e882caa799deede0a88d67b58737bfe_17.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_17.png)'
- en: I just。S the video and it' is not working。Let me just try something。Here we
    are。 sorry about that hair too just shut it down and restart started it。So we're
    good to continue our expression behind the pipeline function and so we'll just
    take a little bit at the code that we just doing in the video I'm not going to
    show it from the section side but remember that in most sections in all the sections
    that have could you have an open coll button at the top like this which I opened
    a little bit earlier just to execute the first cell which is installing everything
    and can take a bit of time and then the second cell which is download the model
    I executed it already so that we have the result instantaneously here and we are
    ready to look at the rest of the notebook so like in the video let me move myself
    on the other side of the screen because the code。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才看了这个视频，但它没有工作。让我尝试一下。这里我们来了，抱歉，刚刚把摄像头关掉再重启。所以我们可以继续讨论管道函数背后的表达式，我们将稍微查看一下视频中正在做的代码。我不会从代码部分展示，但请记住，在大多数部分，所有部分都有一个开放的按钮在顶部，就像这个，我稍早打开过，以执行第一个单元，安装所有内容，这可能需要一点时间，然后第二个单元是下载模型，我已经执行过了，所以我们这里有结果，我们准备查看笔记本的其余部分。就像在视频中一样，让我把自己移到屏幕的另一侧，因为代码。
- en: Is mostly on the left。And。So like in the video， we all look exactly at the code
    that is executed when we try to use the sentiment analysis pipeline on two sentences
    like that。And see how we get to the results on those labels。 So as we've seen
    in the video。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分内容在左侧。所以像视频中一样，我们都准确查看了在对两句话使用情感分析管道时执行的代码。看看我们如何得到这些标签的结果。正如我们在视频中看到的。
- en: the first step the prepossessing is done by a tokenizer so we'll look into the
    tokens in detail a little bit further ahead but for not just you just need to
    know that the tokenizer texts the input text so those two sentences I've been
    waiting for the U face calls my own life and I edit it so much it's going to take
    those two sentences and convert that them into numbers because the model doesn't
    understand text it understand numbers So basically behind the scenes it's going
    to split that text into small chunks that we call token and so will tokenizer
    and then associate each of us token to a unique ID which is the numbers that were
    gonna to see。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，预处理由令牌器完成，所以我们稍后将详细查看令牌，但你只需知道令牌器处理输入文本。这两句话是“我等着你面对我自己的生活”，我编辑了很多，它将这两句话转换成数字，因为模型不理解文本，它理解数字。所以在幕后，它将文本拆分成我们称之为令牌的小块，然后令牌器将每个令牌与一个唯一ID关联，这些数字就是我们将要看到的。
- en: To load the tokenizer， we need to know the identifier of the tokenizer。So here
    this is the identifier of the model that is used by sentiment analysis pipeline
    by default。 and then we just call autotokenize order from pretrained and。It's
    gonna download if you I've the files already downloaded because I executed this
    first。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载令牌器，我们需要知道令牌器的标识符。所以这里是默认用于情感分析管道的模型标识符。然后我们只需调用autotokenize，顺序来自预训练的模型。如果你已经下载了文件，因为我执行了这个步骤，它会下载。
- en: but if it's the first time you're executing this， it's going to download the
    files of the tokenizer and in particular the vocabulary which contains the mapping
    token to unique ID and instantiate it and once you have that objective available。
    you can fit it your input directly like this so raw input inside the tokenizer
    and we all exactly explain what for spaing and tru mean。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果这是你第一次执行，它将下载令牌器的文件，特别是包含令牌与唯一ID映射的词汇表，并实例化它。一旦你有了那个目标，就可以像这样直接处理输入，将原始输入放入令牌器，我们将确切解释“spaing”和“tru”的含义。
- en: A little bit for and we tell it to return tensilors and since we are using Pytoch
    here。 we tell it to returns Pytoch tensor with viPT。You can also say TF of tons
    of litensils。 N for N arrays or Fls for flex， which is， I guess for Fl， it's also
    an n arrays。And if we execute it， we can see that we get an output which is a
    dictionary with input ID and attention mask we'll explain what the attention mask
    means a little bit for in the course。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有点不同，我们告诉它返回张量，由于我们在这里使用Pytoch，所以我们告诉它返回Pytoch张量和viPT。你也可以说TF，表示大量的张量。N表示N个数组，或者Fl表示flex，我想对于Fl，它也是N个数组。如果我们执行它，我们可以看到输出是一个字典，包含输入ID和注意力掩码，我们将在课程中稍后解释注意力掩码的含义。
- en: In this last session， sorry， and the input I are the unique numbers I was talking
    about。 So it's converted that text into small chunks of petaccle tokens and each
    of us token have been associated to unique number And once we are that we can。Use
    as the model。On this input。Just if we want to have a look。Back。Why are you annoying
    me if we want to have a look back at how those Is correspond to the text that
    we had at the beginning。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个会话中，抱歉，输入的“I”是我所提到的唯一数字。所以它把文本转换成小块的petaccle令牌，每个令牌都被关联到唯一的数字。一旦我们有了这个，我们就可以作为模型使用这个输入。只要我们想回顾一下，为什么你要烦我，如果我们想回顾一下这些与一开始的文本是如何对应的。
- en: we can use the decocode method of autokenizer， So I type tokenizer the Decode。And
    then。 I'm gonna to。Take my inputs， grab the key input I。Ohello。D di。Like this，
    so this is a tensor。 so I'm going to convert it to a list。And take this is a list
    of lists because I have two the two sentences in my sentence in my tensor sorry。
    so I'm just taking the first one， for instance， and if I that execute that sorry
    I can see my original text which has been a bit prepossessed。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用autotokenizer的decocode方法，所以我输入tokenizer的Decode。然后，我将获取我的输入，抓取关键输入I。Ohello。D
    di。像这样，所以这是一个张量。我将其转换为列表。因为我在张量中有两个句子，所以这是一个列表的列表。举个例子，我只是获取第一个句子，如果我执行这个，抱歉，我可以看到我的原始文本经过了一些预处理。
- en: there is no capital anymore for the I for instance。And we can also see that
    the tokenizer added something at the beginning and something at the end。 This
    is perfectly logical， as the tokenizer is adding those。The token is always adding
    for tokens because the model expects them。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在“我”不再是大写字母了。例如，令牌器在开头和结尾添加了一些内容。这是完全合乎逻辑的，因为令牌器正在添加这些。令牌总是为令牌添加，因为模型期待它们。
- en: So I'm just going to pause here for questions before I go into the model part
    of the code。What does in Ututuanizer mean？And what type of tokenazizer is used。
    So the auto for auto tokenizer mean in the auto tokenazizer class means that you
    can。Load any tokenizer corresponding to any architecture using that API。 So for
    instance。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入代码的模型部分之前，我将在这里暂停以便提问。Ututuanizer是什么意思？使用了什么类型的tokenizer？auto tokenizer类中的auto意味着你可以使用该API加载任何对应于任何架构的tokenizer。例如。
- en: here our model is a distilled bird model。 So the tokenizer is going to be a
    distilled bird tokenizer can double check that by just adding a console So if
    I type。Dkenizer。And print the output。It's going to tell me it's a pretend tokener
    fast。 which is not super useful。M。But the representation is not purposeable。 but
    the type should be a distill belt tokenizer fast。嗯。And if I had used a bird checkpoint。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们的模型是一个蒸馏鸟模型。所以tokenizer将是一个蒸馏鸟tokenizer，可以通过添加控制台进行双重检查。如果我输入tokenizer并打印输出，它会告诉我这是一个假tokenizer
    fast，这并不是特别有用。但表示形式不是可用的，类型应该是蒸馏鸟tokenizer fast。如果我使用了鸟checkpoint。
- en: I would have a bird tokenizer fast， if I had used， I don't know about checkpoint。
    I would have a badt tokenizer fast， etc cea etcettera。 so the U in autotokenazizer
    means that that class is going to pick the right so class of tokenazizer so when
    corresponding to the model used by your checkpoint automatically so your code
    here tokenazizer equals auto tokenizer that from between checkpoint is going to
    work from for any checkpoint on the model whatever the class of your model as
    long as it's a class of model that has been implemented in transformers it's going
    to work on it。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我使用了鸟tokenizer，我会有一个快速的tokenizer，不知道关于checkpoint的事。我会有一个糟糕的快速tokenizer等等。所以自动tokenizer中的U意味着该类将自动选择与使用的checkpoint对应的正确tokenizer子类。因此，你的代码这里tokenizer等于auto
    tokenizer，这将适用于任何checkpoint模型，只要它是transformers中实现的模型类。
- en: Whereas if you were using here Distill B tokenizer。 you would have to change
    the class used if you change the type of checkpoint， for instance。 if you would
    use the belt model， you would have to change it to bird tokenizer if you were
    using a GT2 checkpoint。 you would need to change it to GT2 tokenizer， etca， etca。嗯。The
    second question was。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 而如果你在这里使用Distill B tokenizer，改变checkpoint类型时，你就需要更改使用的类。例如，如果你使用的是belt模型，你需要将其更改为鸟tokenizer；如果你使用GT2
    checkpoint，则需要更改为GT2 tokenizer，等等。
- en: will I be broken into I and M and the answer to that is yes， we'll see a little
    bit。 I'm just going to push an answer more completely a little bit later when
    we see exactly the different type of tokenizer。And what is the difference between
    a fast tokenizer and a standard tokenizer Very good question So we usually have
    for each model two tokens one that is called slow or standout and the ver that
    is called fast the fast tokenizer is backed by the cookingface tokenizer library
    which is not written in Python but in rust and which in turn because you may have
    known that Python is a slow language and so if you do the wall tokenization in
    pure Python it can be a bit slow when you have lots of lots of text whereas the
    tokenizer fast backed by rust is going to be extremely fast。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我会被分解成I和M吗？答案是肯定的，我们稍后会看到一点。我将稍后更完整地推送答案，届时我们会看到不同类型的tokenizer。快速tokenizer和标准tokenizer之间有什么区别？非常好的问题。每个模型通常有两个token，一个叫慢token或突出token，另一个叫快速token。快速tokenizer基于cookingface
    tokenizer库，它不是用Python编写的，而是用Rust编写的。因为你可能知道，Python是一种较慢的语言，所以如果在纯Python中进行整个tokenization，当文本量非常大时，会稍显缓慢，而基于Rust的快速tokenizer则会非常快速。
- en: So the main difference， that's the main difference between the two of them if
    you are just processing one text。 you won't see any difference， but if you're
    processing 10000 of texts at the same time as the tokenator fast is going to be
    much faster than the python tokenizer。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于，如果你只是处理一段文本，你不会看到任何差别，但如果你同时处理10000段文本，快速tokenizer将比Python tokenizer快得多。
- en: And that's also the question we have for the moment。 I'm going to continue and
    look at the model。So the same way we have nout to kal API， we have a newto modelal
    API and again the auto in automod API means that this class is going to pick the
    right subclass belt model。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是目前我们的问题。我将继续并查看模型。与我们有nout to kal API的方式一样，我们有一个新模型API，同样，automod API中的auto意味着这个类将选择正确的子类模型。
- en: GT2 model， distill belt model， etc depending on the checkpoint that it receives
    so here since it's a dist belt checkpoint it's gonna。I would put distill button
    model， I can just show you here。Ma that model， and it should。I have a nice wrap
    we can see here， this steel bird bottle。嗯。I'm gonna to remove that cell because
    it's a bit annoying。Thank you for that。嗯。So， that model。Is。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GT2模型、蒸馏模型等，具体取决于接收到的检查点，因此这里由于是蒸馏检查点，它会。我会放置蒸馏模型，我可以在这里给你展示。那个模型，它应该。我们可以看到这个蒸馏模型。嗯。我会删除那个单元，因为有点烦人。谢谢你。嗯。所以，那个模型。
- en: Coming with again， we didn't download any file here because when we executed
    the pipeline instruction at the very beginning。 we downloaded everything we needed。
    so the model is already cached。 that's why we don't see any download here and
    we get the warning which I'm going explain just after this。 which is because。This
    automod class is going to give us the base pretrained model。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，我们没有下载任何文件，因为当我们在一开始执行管道指令时，我们下载了所需的一切。所以模型已经被缓存。这就是为什么我们在这里看不到任何下载，以及我将要解释的警告，原因是这个automod类将给我们提供基础预训练模型。
- en: and this space pretrained model doesn't output classification of a sentence
    between positive and negative。 it outputs the hidden state of the pretrained model，
    which is a dimension of 168。So that's why here we have a warning because that
    model auto model is missing a classification head and as the warning was saying。
    some weights of the checkpoint were not used when initializing the model。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个空间预训练模型不输出句子的正负分类。它输出的是预训练模型的隐藏状态，维度为168。所以这里有一个警告，因为该模型的自动模型缺少分类头，正如警告所说，某些检查点的权重在初始化模型时未被使用。
- en: specifically classifier bias， classifier weights， etca。 which are all the weights
    of the classifier head。So Automod is something that can be useful if you just
    want the tensor or hidden features outputted by your pre model。 but here we want
    to classify our sentences between positive and negative so we need a model with
    a classification head and that's given to us by the automod for sequence classification
    class。And this one， when I execute the cell is not going to output any warninging
    because all the weights are going to be used and there is not going to be anything
    problematic when you dig the checkpoint。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是分类器偏置、分类器权重等，这些都是分类头的所有权重。因此，如果你只想获取预模型输出的张量或隐藏特征，Automod是有用的，但我们想将句子分类为正面或负面，因此我们需要一个带有分类头的模型，而这正是由用于序列分类的automod提供的。当我执行这个单元时，不会输出任何警告，因为所有权重都会被使用，检查点时不会出现任何问题。
- en: And if we。G our input to thatmo and look at the shape and we can see that it's
    going to be a tons of size 2 by 2。One little comment about the output is that
    the output of transformer models。 so that was the thing that were imported at
    the beginning of our modeling file。 if you remember like we had a lot of import
    from the the dot model outputs。Moule。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输入传递给该模型并查看形状，我们可以看到它将是一个2x2的大小。关于输出的小评论是，变换器模型的输出。那是我们在建模文件开头导入的内容。如果你还记得，我们从模型输出中进行了很多导入。
- en: So those outputs are a bit of an hybrid between the name to and the dictionary
    so you can access everything either by doing dot like this So output dot logits。
    you can also access things。By。嗯。Asking。With the key。So， outputs。And when we ask
    for the logicit key。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些输出在名称和字典之间有点混合，因此你可以通过这样做来访问一切，即输出点logits。你也可以通过键来访问。嗯。通过问。使用键。所以，输出。当我们请求逻辑密钥时。
- en: like it would if like a dictionary， which works as well。嗯。And if we just ask
    for the base representation， we can see it's a sequence classifier output which
    contain Lu sheet and thistensor。 so here it contains only one thing but most of
    the transformer model can return lots of things as output。 for instance if I added
    labels here it would return a loss。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 就像字典一样，这也能正常工作。嗯。如果我们只是请求基本表示，我们可以看到它是一个序列分类器的输出，包含Lu表和这个张量。因此这里只包含一项，但大多数变换器模型可以返回许多输出。例如，如果我在这里添加标签，它将返回损失。
- en: we could also ask the model to return all the hiddenden states or all the attentions
    results and in which case our output is becoming a bit quoted。 which is why it's
    organized as this spec class that behaves likeedt and an imable。So those legits。
    because those names of what we get are numbers which appear a bit random。 they
    don't really look like probabilities and we will need to do one last step of post
    processing as we saw in the video to convert them into probabilities。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以要求模型返回所有隐藏状态或所有注意力结果，这样我们的输出就变得有些冗长。这就是为什么它被组织为这种像 EDT 和一个 IMable 的规格类。所以那些合法性。因为我们得到的名称是一些看起来有些随机的数字。它们并不真正像概率，我们需要进行最后一步后处理，如视频中所示，将它们转换为概率。
- en: that is apply the surfmax。And if we just import from torturer soft max function
    plate。 we can see that now we get the exact same score that we had at the beginning。
    so for instance for the second sentence like we have that 0。99 so 99。95% and if
    we look back at the result of the pipeline we can see that we had 99。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用 softmax。如果我们只是从 `torturer` 导入 softmax 函数板块。我们可以看到，现在得到的分数与一开始的完全相同。例如，对于第二个句子，我们得到了
    0.99，即 99.95%。如果我们回顾管道的结果，可以看到我们得到了 99。
- en: 945 yeah so the exact same scores。And to know which which one was the negative
    and which one was positively label。The pipeline is using that field from the model
    configuration， so for each model。 the configuration file associated to it is accessible
    via the configure attributebute and the ID2 level field contains a correspondence
    between integers and levels。So let's see if we have any questions。哎呀。First question
    is。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确切的分数。所以要知道哪个是负面标签，哪个是正面标签。管道使用模型配置中的那个字段，因此对于每个模型，关联的配置文件可以通过配置属性访问，ID2级字段包含整数与级别之间的对应关系。那我们看看有没有问题。哎呀，第一个问题是。
- en: is there any reason to use the standard Python to can？
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准 Python 有什么理由吗？
- en: I work at a game phase so I'm a little bit biased and I'm going to say no you
    have to using the fast tokenser is always going be better。 so it's going to be
    even if you don't have many many text。 it's going to be at the same speed at the
    very minimum maybe faster than the Python tokenazer standardt Python tokenneer。
    but also it has many more features， so we'll look at them in the second part of
    the course mainly but it has features that have been designed specifically for
    tasks like like token classification or question answering that allow you to know
    for instance。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我在游戏阶段工作，所以我有点偏见，我要说不，使用快速的分词器总是更好。所以即使你没有很多文本，它的速度也至少与标准 Python 分词器相同，甚至可能更快。此外，它还有更多功能，我们将在课程的第二部分主要讨论这些功能，这些功能专门为像标记分类或问答这样的任务设计，让你知道例如。
- en: if from which word the token comes from or to exactly which span of text both
    tokens represent in the original text which fit that are a little bit not a little
    bit way out together to get with the slow tokener。Another question is。Are there
    any similar tutorials or resources for sentiment analysis for multilabels that
    or regression tasks for sequences not right now which is that's a very good question
    so not right now and we should definitely work on that so the main thing you would
    have to change is the processing at the end instead of applying softmax you would
    apply for a regression you wouldn't apply anything I guess and then for multiplelabel
    so multiple label multiple possible labels for each of your sentence you would
    apply probably a stingoid to your result。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: token 来源于哪个单词，或两个 token 在原始文本中分别代表哪个文本跨度，这些功能的设计是为了克服慢速分词器的一些限制。另一个问题是，有没有类似的教程或资源用于多标签情感分析或序列的回归任务？现在没有，这是个非常好的问题，所以现在没有，我们确实应该对此进行一些工作。你需要改变的主要内容是最后的处理，而不是应用
    softmax，对于回归来说，你可能不需要应用任何东西。对于多标签，每个句子可能有多个标签，你可能需要对结果应用一个 `sigmoid`。
- en: And then last question is the first two input ID， the two lines are the same。
    but the words are not why is that so we have to look back at the decoding because
    the two words are indeed the same so the sentence here。Begins with 101 and 101
    which is the idea that CSO。 so that's why you have the first two input id that
    the same and then the 1045 correspond to the I because the two sentences begin
    with I and then it starts being different because you have the ver or8 for the
    two sentences。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个问题是前两个输入ID，这两行是相同的，但单词不同，为什么会这样呢？我们需要回头看看解码，因为这两个词确实相同，因此这里的句子以101和101开始，这是CSO的概念。这就是为什么你有前两个输入ID相同，然后1045对应于I，因为这两个句子都以I开头，然后开始不同，因为你有两个句子的ver或8。
- en: And another question would it be possible to do a video explaining the code
    structure of the library and the ID behind it so that's to maybe make it easier
    to contribute very。 very good question and it's actually scheduled for the last
    part of the course in the part of the course we have a chapter that's going to
    be dedicated to how to contribute to againface libraries and particular the transformformers
    library and then well have video explaining the good structure of all the libraries
    of the again phase ecosystem。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个问题，是否有可能做一个视频，解释库的代码结构和背后的ID，这样可能会更容易贡献？非常好的问题，实际上这已安排在课程的最后部分，在课程的一部分中，我们有一个章节将专门讨论如何为againface库贡献，特别是transformers库，然后我们将有视频解释again
    phase生态系统中所有库的良好结构。
- en: Again， don't hesitate to ask any questions I'm going to pose regularly to answer
    it。So that's pretty much everything that was behind the pipeline and we've seen
    it in detail in the code。 So now let's have a look at the main object inside the
    pipeline， which is the model。So again。 we have a short video that I'm going to
    show from my computer and then we'll look at the code in detail and if you have
    questions。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，请不要犹豫，随时提问，我将定期提出问题来回答。所以，这就是管道背后的所有内容，我们在代码中详细看过了。现在，让我们看一下管道内的主要对象，也就是模型。所以，我们还有一个短视频，我将从我的电脑上展示，然后我们将详细查看代码，如果你有问题。
- en: I can answer them and live code with you。Let me just grab the video。呃。Which
    of course。 I can't find easily otherwise， but would be too easy。 Why did it disappear。And
    one was ver poormiss。 I just moved it。Recently。![](img/0e882caa799deede0a88d67b58737bfe_19.png)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以回答这些问题并与你实时编程。让我先找一下视频。呃，当然，我找不到很容易，不然太简单了。为什么它消失了。还有一个是ver poormiss。我刚刚移动了它。最近。![](img/0e882caa799deede0a88d67b58737bfe_19.png)
- en: And。![](img/0e882caa799deede0a88d67b58737bfe_21.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 而且。![](img/0e882caa799deede0a88d67b58737bfe_21.png)
- en: How to instantiate a transforms model。In this video。 we'll look at how we can
    create and use the model from the Transformers library。As we' seen before。 the
    Automodal class allows you to instantiate a portrayed model from any checkpoint
    on the I face app。It will pick the right model class from the library to instant
    shade the proper architecture and load the weights of the preed model inside。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如何实例化变换模型。在这个视频中，我们将探讨如何创建和使用来自Transformers库的模型。正如我们之前看到的，Automodal类允许你从I face应用中的任何检查点实例化一个表现模型。它会从库中选择合适的模型类以即时调整正确的架构，并加载预训练模型的权重。
- en: As we can see， when given a bird checkpoint， we end up with a bird model and
    similarly for GPT2 or part。Beyond the scenes， this APII can take the name of a
    checkpoint on the U。 in which case it will download and cache the configuration
    file as well as the model weights file。You can also specify the path to a local
    folder that contains a valid configuration file and a model of waste file。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，当给定一个bird检查点时，我们最终得到了一个bird模型，对于GPT2或part也是如此。在幕后，这个API可以接受U上的检查点名称，在这种情况下，它将下载并缓存配置文件以及模型权重文件。你还可以指定包含有效配置文件和模型权重文件的本地文件夹的路径。
- en: To instant shade the between model， the Automodal API will first open the configuration
    file to look at the configuration class that should be used。The configuration
    class depends on the type of the model， B， GPPT2， or Bt， for instance。Once it
    has a proper configuration class， it can instantiate that configuration。 which
    is a blueprint to know how to create the model。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实例化between模型，Automodal API将首先打开配置文件，以查看应该使用的配置类。配置类取决于模型的类型，比如B，GPPT2或Bt等。一旦有了合适的配置类，就可以实例化该配置，这是一种了解如何创建模型的蓝图。
- en: It also uses this configuration class to find the proper model class。 which
    is when combined with the root configuration to load the model。This model is not
    yet a portraytrain model， as it has just been initialized with random weights。The
    last step is to load the weight from the model file inside this model。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它还使用这个配置类找到合适的模型类，当与根配置结合时加载模型。这个模型尚未是可训练模型，因为它只是用随机权重初始化的。最后一步是从模型文件中加载权重。
- en: To easily load the configuration of a model from any checkpoint or a folder
    containing the configuration file。 we can use the autoconfigug class。Like the
    Automod class。 it will pick the right configuration class from the library。We
    can also use a specific class corresponding to a checkpoint。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松从任何检查点或包含配置文件的文件夹加载模型的配置，我们可以使用自动配置类。就像自动模型类一样，它将从库中选择正确的配置类。我们也可以使用与检查点对应的特定类。
- en: but well need to change your code each time we want to try a different model
    architecture。As we said before， the configuration of a model is a blueprint that
    contains all the information necessary to create the model architecture。For instance，
    the bird model associated with a birth based case checkpoint as 12 layers。 the
    hidden side of 768。And the vocabulary size of 28996。Once we add a configuration。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但是每次我们想尝试不同的模型架构时，都需要更改你的代码。正如我们之前所说，模型的配置是一个蓝图，包含创建模型架构所需的所有信息。例如，鸟类模型与基于出生的案例检查点相关联，具有12层，隐藏层为768，词汇表大小为28996。一旦我们添加了配置。
- en: we can create a model that has the same architecture as a checkpoint， but is
    randomly initialized。We can vet training it from scratch like any by doch model。We
    can also change any part of the configuration by using keyword arguments。So sequence
    one sniet of code， instant sheets a randomly initialized B model with 10 layers
    instead of 12。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个与检查点具有相同架构的模型，但它是随机初始化的。我们可以像任何标准模型一样，从头开始进行训练。我们还可以通过使用关键字参数更改配置的任何部分。因此，序列中的一段代码，瞬间生成一个随机初始化的B模型，层数为10，而不是12。
- en: Saving a model once its trend off fine is very easy。We just have to use the
    safe between method。Here。 the model will be saved in a folder named My belt model
    inside the current working directory。Such a model can then be re using the from
    between method。To learn how to easily approach this model to the web， check out
    the push to video。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型的趋势稳定，保存模型非常简单。我们只需使用保存方法。在这里，模型将保存在当前工作目录中的名为“My belt model”的文件夹中。这样的模型可以通过使用从文件中加载的方法重新使用。要了解如何轻松将此模型推送到网络，请查看推送视频。
- en: '![](img/0e882caa799deede0a88d67b58737bfe_23.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_23.png)'
- en: 嗯。![](img/0e882caa799deede0a88d67b58737bfe_25.png)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。![](img/0e882caa799deede0a88d67b58737bfe_25.png)
- en: Okay， so let's see if we have any questions， not just yet。 Don't hesitate to
    ask any question in the chat and I'll answer them。Regularly， and for this。 let's
    open the code app。And look a little bit at the code behind the Automod API。And
    in particular。 we'll see， for instance， I told you a little bit earlier that our
    model could return more venture surligit and it could return。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么我们来看一下是否有任何问题，目前还没有。请随时在聊天中提问，我会定期回答。为了这个，我们打开代码应用程序，稍微看一下Automod API背后的代码。特别是，我们会看到，例如，我早些时候告诉过你，我们的模型可以返回更多的潜在结果，并且可以返回。
- en: for instance， all the hidden states or things like that and we'll see how to
    do that just here。![](img/0e882caa799deede0a88d67b58737bfe_27.png)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，所有的隐藏状态或类似的东西，我们将在这里看到如何做到这一点。![](img/0e882caa799deede0a88d67b58737bfe_27.png)
- en: Yeses。![](img/0e882caa799deede0a88d67b58737bfe_29.png)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。![](img/0e882caa799deede0a88d67b58737bfe_29.png)
- en: So。If you had any questions， that would be an need all time because I didn't
    execute this notebook in advance。 so we need to wait for it to install everything。Okay，
    that didn't think so。嗯。So。To create a random model that looks exactly like the
    per model。 we can just instantiate the default configuration and use that configuration
    inside the model。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你有任何问题，那将是非常必要的，因为我没有提前执行这个笔记本。因此，我们需要等它安装所有内容。好的，我不这样认为。嗯。所以，要创建一个看起来与之前模型完全相同的随机模型，我们只需实例化默认配置，并在模型中使用该配置。
- en: Like we saw in the video， the config contains lots of fields that are related
    to what's happening inside the model。 so for instance we have the hidden size
    configured。 we have the number of words that our model can taken， we have the
    vocabulary size。The mobile type。 which is built， the activation it' used， which
    is Kiu， etctera， etca。嗯。So， that model。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在视频中看到的，配置包含许多与模型内部发生的事情相关的字段。例如，我们有配置好的隐藏层大小，我们有模型可以接受的单词数量，还有词汇表大小。模型类型，构建时使用的激活函数是Kiu，等等。嗯。所以，这个模型。
- en: Using just random usage the config is going to be randomly initialized and there
    is nothing to download there if we。Want to use a pretrained model we have to use
    the from pretrained method。 which is going to download the exact config and then
    the model weights and as we saw in the video。 it's going to use the config to
    first instantiate randomly initialized model and then load the weights from that
    checkpoint inside the model we have。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机配置时，配置将被随机初始化，并且那里没有任何下载。如果我们想使用预训练模型，我们必须使用 `from pretrained` 方法。这将下载确切的配置和模型权重，正如我们在视频中看到的，它将首先使用配置来实例化一个随机初始化的模型，然后从我们拥有的那个检查点加载权重。
- en: And if we。Want to change anything。いのもの。More specifically in its configuration。
    we can say it in several places。 So， for instance， we can start with。呃。A config
    that is exactly like birds。So con to convicted from pre traineded。B pesquiist。Which
    is gonna download， and it's already downloaded from here。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要更改任何内容。更具体地说是在其配置中。我们可以在多个地方说出来。例如，我们可以从一个配置开始。呃。这个配置与鸟类完全相同。我们要从预训练的B
    `pesquiist`中获取，这个配置会下载，而且这里已经下载过了。
- en: Where you mean the two configurefig is not defined。 Oh yeah， I have only use
    B config。 So let's continue with that。So。B con from between。 which is gonna reuse
    the con that was download here。 So this is the configuration of the per model。
    And if we want to change anything inside it， we saw the video， for instance。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你是说 `two configurefig` 没有定义。哦，是的，我只使用了B配置。那我们继续这个。所以，B从 `between` 来的配置，将会重用这里下载的配置。这是每个模型的配置。如果我们想更改其中的任何内容，我们在视频中看到了，例如。
- en: to change certain number of hidden layers。 but let's say that。I want to change
    the fact that I want my model to return all the hidden states。Which I would say
    with outputs in states equal to。So I can do this in the config and then instant
    shape my model with model equal bad model config。 I can also directly change this
    when I do。That model that from betweentrained here。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更改某些隐藏层的数量。但假设我想更改我希望我的模型返回所有隐藏状态的事实。我会用 `outputs` 的 `states` 等于某个值来实现。所以我可以在配置中做到这一点，然后用
    `model` 等于 `bad model config` 来实例化我的模型。当我执行从 `betweentrained` 这里的模型时，我也可以直接更改这个。
- en: So since if I were to change here the number of hidden layers it wouldn't work
    anymore。 the command would fail because I would then try to load a checkpoints
    that has been defined with 12 layers inside the model with 10 layers so bytoch
    would complete I mean it would probably work but I would have a warning with like
    the widths not being used and the model would probably not get super useful results
    but for something like outputed in states which doesn't really change the way
    the model was pretrained this is going to work super nicely。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果我在这里更改隐藏层的数量，它将不再有效。命令会失败，因为我试图加载一个定义了12层的检查点，而模型只有10层。所以`bytoch`可能会工作，但我会收到警告，提示未使用的宽度，模型可能不会得到非常有用的结果。不过对于像
    `outputed in states` 这样不真正改变预训练模型方式的内容，这将非常有效。
- en: And if I try to take inputs。So let's define some random input and then pass
    it to a tokenizer。 So I'll have to。Use the part toagonizer and then instantiate
    it with a form betweentrain method。Should oh， yes， it's finding it here。Should
    have executeded the elder or or。So if I'm creating a do like this。And then， applying
    it。呃。To my inputs。🎼Return a tensor。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我尝试输入一些内容。那我们先定义一些随机输入，然后传递给分词器。我要使用部分分词器，然后用一个形式的 `betweentrain` 方法实例化它。嗯，应该是这里在找到它。应该执行先前的那个。
- en: I don't need to put the padding and location that we saw before， because there
    is only one sentence。 I'll see why a little bit earlier。 So once I have done that，
    I can look at my。Outputs。And it should have。Now， two keys。Still one key。YeahWith
    a little bit more because the bird model has a pull output you on top of the luggit
    but oh and it's not luets anymore sorry it's less hidden states because this is
    not the classification model it's a base model I used a be model which is the
    same as using automod not a be model for second classification so I get a last
    hidden states instead of look key the puller output is specific to B so it's always
    as that and then I can say have a last key with hidden states and a list of all
    the turnsult which correspond to all the hidden states of my model so this is
    how you change the configuration of your model on the fly either insides when
    we create a config if you are trying to initialize a randomly initialized model
    or to the form pretrain letter if you are trying to use。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我不需要放置之前看到的填充和位置，因为这里只有一句话。我稍早一点会解释原因。所以一旦我完成了这一步，我就可以查看我的输出。现在应该有两个键。仍然是一个键。是的，多出了一点，因为鸟模型在luggit顶部有一个输出，但哦，不再是luets了，抱歉，隐藏状态更少，因为这不是分类模型，而是我使用的基础模型。我使用的是be模型，这与使用automod不一样，不是用于第二次分类的be模型，所以我得到的是最后的隐藏状态，而不是看键，puller输出是特定于B的，所以它总是这样，然后我可以说有一个最后的键，包含隐藏状态和与我的模型的所有隐藏状态对应的结果列表，这就是如何在创建配置时实时更改模型配置的方式，如果你尝试初始化一个随机初始化的模型，或者使用预训练的版本，如果你尝试使用。
- en: A pre model in particular， if you're using a classification model， for instance。
    a sequence classification model， you can specify the most important argument is
    going to be nu levels。Because when you add your classification head， you want
    to control how many outputs that classification head hass。 so you would do that
    with a new labels argument。So that model。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是如果你使用的是分类模型，例如序列分类模型，你可以指定最重要的参数，即nu levels。因为当你添加分类头时，你想控制这个分类头有多少个输出。所以你可以通过new
    labels参数来实现。这样模型。
- en: And then once you finish training or a tuning your model。 you can use safe pretrain
    to save it on the floor on your。On your hard drive and you can use Pushtb。 which
    we just released today actually。So on your model to directly upload your model
    on the Higing face hub so that anyone in the world can use it。Don't see any questions
    again， don't hesitate to ask any questions。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了训练或微调你的模型，你可以使用safe pretrain将其保存在你的硬盘上，并且你可以使用Pushtb。我们今天刚发布的实际上。所以在你的模型上直接上传到Higing
    face hub，以便全世界的人都可以使用它。如果还有其他问题，欢迎随时提问。
- en: I'm going to answer them regularly。And so this is all we have to。 this is all
    we various intersection for models and then let's look at tokenizer。 which is
    responsible for prepoing the input I'm going to move myself， oh not to screen。![](img/0e882caa799deede0a88d67b58737bfe_31.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我会定期回答这些问题。这就是我们所需要的。关于模型的各种交叉点，然后我们来看一下负责处理输入的标记器，我将移动自己，哦，不要看屏幕。![](img/0e882caa799deede0a88d67b58737bfe_31.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_32.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_32.png)'
- en: Come back。Going to move myself。Back on the left。Because。And we。 we look at this
    section here and look that the code inside the code app。嗯。So tokenizer。Oh yeah
    let's look at the video with tokens of first and then I'll comment everything
    that's happening in this section。T can as introduction video。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 回来吧。我要移动自己。回到左边。因为。我们。我们看看这里这一部分，并看看代码应用中的代码。嗯。所以标记器。哦，是的，我们先看一下带有标记的视频，然后我会评论这一部分发生的所有事情。这可以作为介绍视频。
- en: '![](img/0e882caa799deede0a88d67b58737bfe_34.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_34.png)'
- en: '![](img/0e882caa799deede0a88d67b58737bfe_35.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_35.png)'
- en: Yeah。In the next few minutes， we'll take a look at the tokens。😊，In natural language
    processing。 most of the data that we handle consists of raw text； however。 machine
    learning models cannot read or understand text in its raw form。They can only work
    with numbers。So the tokenizers objective will be to translate the text into numbers。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。在接下来的几分钟里，我们将查看标记。😊 在自然语言处理领域，我们处理的大多数数据都是原始文本；然而，机器学习模型无法以原始形式读取或理解文本。它们只能处理数字。因此，标记器的目标是将文本转换为数字。
- en: There are several possible approaches to this conversion。 and the objective
    is to find the most meaningful representation。😊。We'll take a look at three distinct
    organization algorithms， we compare them one to one。 so we recommend you take
    a look at the videos in the following order， first word based。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种可能的方法来进行此转换，目标是找到最有意义的表示。😊 我们将查看三种不同的组织算法，逐一比较，因此我们建议你按以下顺序查看视频，首先是基于单词的。
- en: followed by character based and finally sub word based。![](img/0e882caa799deede0a88d67b58737bfe_37.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是基于字符的，最后是基于子词的。![](img/0e882caa799deede0a88d67b58737bfe_37.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_38.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_38.png)'
- en: Yeah。So we won't look at the video。 Actually， we're gonna look directly at the
    text inside inside the the。The course and I'll comment because we won't have time
    to watch all those videos in the slide we。So world based tokens so you can look
    at the video in your free time but we're gonna explain it a little bit more in
    depth with what I' to do。 but the world based tokenizer is just going to split
    your sentence by word so the easiest way to do that is to take all the spaces
    and then split your text onto those spaces more advanced would be to include some
    walls to split and punctuation so for instance the exclam mark separated it from
    tokenization or here let's split it between let and aworth as。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。所以我们不会看视频。实际上，我们将直接查看课程中的文本，我会进行评论，因为我们没有时间观看所有这些幻灯片中的视频。所以基于单词的标记器，你可以在闲暇时观看视频，但我们将更深入地解释我要做的事情。基于单词的标记器只是将你的句子按单词分割，因此最简单的方法是取所有空格，然后按这些空格拆分文本，更高级的方式是包括一些墙壁来分割和标点符号，例如感叹号将其从标记化中分离，或者在这里我们将其分割为“let”和“aworth”。
- en: So we can see this on this example with Chi and Sun weather Preter。 which is
    separated into five words here。So the world organizers are。Were used a lot before
    transformers， mostly the advantage is that。You split naturally your text onto
    the spaces and punctuation see disadvantage is that you end up with pretty large
    vocabularies because there there are lots of different worlds in English and every
    time someone makes a typo in some world you end up with in a new world in your
    vocabulary。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这个例子中看到“Chi”和“Sun weather Preter”，它被分成了五个单词。所以基于单词的组织者是。之前在变换器中使用得很多，主要的优点是。你自然地将文本拆分成空格和标点，缺点是你最终会有相当大的词汇量，因为英语中有很多不同的单词，每次有人在某个单词中打错字，你就会在词汇表中增加一个新单词。
- en: So each word gets assigned in ID starting from0 going up to the size of the
    vocabulary and then since we can't guarantee that the user is never going to make
    a tape or anything。 there is a special rule， if we encounter a token that doesn't
    exist in the vocabulary。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个单词从0开始分配一个ID，直到词汇表的大小，然后由于我们不能保证用户永远不会犯错，因此有一个特殊规则，如果我们遇到一个在词汇表中不存在的标记。
- en: it's usually replaced by something called the unknown token which is usually
    something that looks like that and between brackets。So this is one of the other
    drawbacks of the world based tokennea。 so the first one is that we have very largeocabularies。
    the second one is that we need to learn that the word Do and the word Dos are
    very similar。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它通常被称为未知标记来替代，通常看起来像这样，并在括号中。因此，这是基于单词的标记器的另一个缺点，第一个是我们有非常大的词汇量，第二个是我们需要学习“Do”和“Dos”这两个单词非常相似。
- en: they won't know that from scratch because when the model is initialized randomly。
    it's going to have a set of abidding for that word dos and another one for that
    word dogss and it's going to need to learn by seeing lots and lots of data that
    those two world look a little a bitlike。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 他们不知道从头开始，因为模型是随机初始化的。它将有一组针对那个单词“dos”的约束，还有另一组针对“dogss”的约束，并且它需要通过看到大量数据来学习这两个词看起来有点相似。
- en: And the last disadvantage is that Ung token so every word that the depo is going
    to end up like this and the more can learn in your representation of that。 it
    it says if you had just deleted the world in the sentence。So another way is to
    just split your text on all characters which is the what character best organizes
    to in this case your vocabulary is not going to be very large because 256 SI characters。
    for instance， a little bit more if you take the wall any good thing。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个缺点是Ung token，所以每个单词最终都会这样，更多的是在你的表示中学习。它的意思是，如果你删除句子中的某个单词会怎样。另一种方式是按所有字符来分割文本，这就是字符最佳组织的方式。在这种情况下，你的词汇量不会很大，因为只有256个字符。例如，如果你处理更大的词汇。
- en: but you're not going to end up with models but have a vocabulary size of I don't
    know 300。000 or something like that。So this is better for the vocabulary size。
    you probably won't get a known token because you all see all the different character
    possible。But the drawback is that now the representation is based on character，
    so the model as to on that。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但你不会得到一个模型，其词汇量达到300,000或类似的数量。因此，这对于词汇量更好，你可能不会得到未知的token，因为你会看到所有可能的不同字符。但缺点是，现在表示基于字符，因此模型必须适应这一点。
- en: for instance， the letter E is not does not mean the same thing when it's between
    an a T compared to the letter E here with the key and the n beside the word organization。So
    is a representation of each letter is less meaningful， that's what I't trying
    to say。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，字母E在字母T之间的含义与字母E在字母K和N旁边的“组织”中的含义不同。因此，每个字母的表示意义较弱，这就是我想表达的。
- en: And compared to what we had we've worked。The overall drawback is that we end
    up with very long sentences。 for instance， for leads to tokenization， if we look
    back with two world based tokenization it was split into five words with the tokenization
    with the character based tokenization it splits in much more concor here but it's
    between 15 and 20。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们所拥有的相比，我们的工作有了进展。整体的缺点是句子变得非常长。例如，基于词的分词法将其分为五个词，而基于字符的分词法则分得更多，通常在15到20之间。
- en: let's say so we end up with longer sentences and our transformer models are
    usually constrained by a maximum lengths。 So for instance， the built model can
    only do5 can only treats 512 tokens at a time。 So using a character based tokenization
    algorithm would。Make sure the maximum sentence。 you can feed them all pretty short。So
    that's why transformmonology usually use a compromise between word and character
    based tocanization。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们最终得到更长的句子，而我们的变换模型通常受到最大长度的限制。例如，构建的模型一次只能处理512个token。因此，使用基于字符的分词算法将确保最大句子长度相对较短。这就是为什么变换技术通常在基于词和基于字符的分词之间找到折衷。
- en: which is the world tokenization。So sub organization。As the name indicates。 it's
    going to split your text into subwors， so it's still split between words。 but
    some wordss are got into， for instance here， you've got lets do and then token
    and Iization are separated into。Notice that you get the small and。With like the
    animals know to say that in English。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是tokenization。子组织，如其名称所示，会将你的文本拆分成子词，因此仍然在词之间拆分。但是某些词会被拆分，例如这里的lets do，然后token和Iization被分开。注意到你得到了小写的和。像动物在英语中会这样说。
- en: but that special thing between an inferior superior sign with slash W。 which
    means that it's the end of a world。 So Tuken doesn't have it because that's for
    the。That's because we want the model to be able to differentiate token as a single
    world and token followed by something else like tokenization。 so the Iization
    as the specificx that says here its in the other world that token does not。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但在较小和较大的符号之间有个特殊的符号斜杠W，表示这是一个单词的结尾。因此，token没有这个，因为我们希望模型能够区分单个单词的token和后面跟着其他东西的token，例如tokenization。所以Iization作为特定的表示，在其他单词中token并没有。
- en: And so that depends on the convention used by the tokenizer。 some tokens have
    a thing at the beginning of world。 some tokens have a thing at the end of the
    world。And so this approach。 this approach allows you to have a vocabulary that's
    not going to be too huge and the two can still have some meaningfulmantic some
    semantic meaning that's more meaningful than just characters。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于分词器使用的约定。有些token在单词的开头，有些则在单词的末尾。因此，这种方法允许你拥有一个不会太庞大的词汇，同时仍能保持某种语义上的意义，超过单纯的字符。
- en: And the last thing is that for worlds based tokens， for instance， Doug and dogs。
    well two separate wordss here， Do as dogs would probably be split into Do and
    nest。 the same way tokenization is split between token and Iization so it can
    learn that they have the same prefix and then the sization is going to be used
    in another world like modernization。And the twoken can the model sorry can then
    make sense of the Su fixes and learn that they are always kind of the same。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点是，对于基于世界的令牌，例如，Doug和dogs。这两个单词应该被分开，Do和dogs可能会被拆分为Do和nest。令牌化的方式与token和Iization之间的分割类似，因此它可以学习它们具有相同的前缀，然后sization将在另一个词中使用，如modernization。模型可以理解后缀，并学习它们总是某种相同的。
- en: And so in the next part of the course， we'll look at into detail as a different
    because there are three different。Suborgan algorithm that are by level word piece
    and sentence piece will explain exactly the difference between them in the next
    part of the course。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在课程的下一部分，我们将详细查看不同的算法，因为有三种不同的子组织算法，按级别的word piece和sentence piece将在课程的下一部分中准确解释它们之间的差异。
- en: So let's see if we have any questions before we look at oh the tokenazer work
    practice。Yes。 I'm just going to put myself here properly。Tuckenneer breaks a sentence
    into tokens。 but no limatizations or stming is performed before learning。嗯。You
    know。It's。Lets meet the bullsh。I would say no， but you should ask the questions
    from where people that are more competent than me can answer you because I'm not
    completely sure。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看看令牌器在实践中的工作之前，让我们看看是否有任何问题。是的。我只是想好好把自己放在这里。令牌器将句子分解为令牌，但在学习之前不会进行词形还原或stemming。嗯。你知道。是的。让我们不扯淡。我会说没有，但你应该向那些比我更有能力的人提问，因为我不完全确定。
- en: Could you provide intuition into word piece usingbed and sentence piece type
    tos？I could。 but it's going to take a bit of time so again I'm going to readdirect
    you on the form where I can take the time to properly answer you and there is
    also maybe Louisis can share it here phrase his the tokenazal summary in the transformal
    documentation that explains the difference between what piece and the sentence
    piece which is using Uniigram behind the scene and the key difference between
    the two。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你能提供关于word piece和sentence piece类型的直观理解吗？我可以。但这会花一点时间，所以我再次会引导你去那个可以让我好好回答你的地方，或许Louisis可以在这里分享他的tokenizer总结，以及在变换文档中解释word
    piece和sentence piece之间区别的内容，后者在后台使用Uniigram，二者之间的关键区别。
- en: Does the W slashW tag add any information in the subwe tokenization， so yes。
    as I said it's what allows them all all to know the difference between a single
    world like I mean between token used as a single world or token inside a world
    like tokenization or tokenizer。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: W slash W标签在子令牌化中添加了任何信息吗？是的。正如我所说的，这使得所有人都知道单个单词之间的区别，例如token作为单个单词或token在一个单词内部，如tokenization或tokenizer。
- en: And then let's see how the tokener work in practice。呃。So can we have seen to
    load the tokenizer using the form pretrain method。And what it returns。 and we'll
    now quickly look at the video on the tokenization pipeline。 which is going to
    explain what's happened when we feed the tokener sequence like that and how it
    returns those numbers。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们看看令牌器在实践中的工作。呃。因此我们已经看到使用form预训练方法加载令牌器。它返回的内容是什么。现在我们将快速查看有关令牌化管道的视频，它将解释当我们像这样输入令牌序列时发生了什么，以及它如何返回那些数字。
- en: Let me just grab it from my computer， and then I'll continue answering questions。嗯。So
    took a nice pipeline。In this video， while look at how tokenizer converts ver text
    to numbers that a transformer model can make sense of。 like when we execute this
    good。Here is a quick overview of what happens inside the tokenizer object。First，
    the text is split into tuets， which are words， parts of words， or punctuation
    symbols。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我从我的电脑上抓取一下，然后我将继续回答问题。嗯。因此，这是一个不错的管道。在这个视频中，我们将看看令牌器如何将文本转换为变换模型可以理解的数字。例如，当我们执行这个时。这里是令牌器对象内部发生的事情的快速概述。首先，文本被拆分成tuets，通常是单词、单词的一部分或标点符号。
- en: Then the tokenizer adds potential special tos and converts each token to our
    unique respective ID。 as defined by the touckenizer's vocabulary。As we'll see
    it doesn't quite happen in this order。 but doing it like this is better for her
    understandings。The first step is to split our input text into tokens， we use the
    tokenized method for this。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后tokenizer添加潜在的特殊token并将每个token转换为我们独特的相应ID，正如tokenizer的词汇所定义的。正如我们将看到的，这个过程并不是严格按照这个顺序进行的，但这样做更有利于理解。第一步是将我们的输入文本拆分成tokens，我们为此使用tokenized方法。
- en: To do that， the tokenizer may first perform some operations like lower casing
    or words。 then follow a set of rules to split the result in small chunks of text。Most
    of the transformable models use a word organization algorithm。Which means that
    one given word can be split in several tokens， like tokens here。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，tokenizer可能首先执行一些操作，比如小写或单词，然后遵循一套规则将结果拆分成小块文本。大多数可转换模型使用单词组织算法。这意味着一个给定的单词可以被拆分成几个tokens，像这里的tokens一样。
- en: Look at the Tokenization algorithms video linked below for more information。The
    ash ash prefix we see in front of I is a convention used by bird to indicate thistoken
    is not the beginning of the world。Other organrs may use different convention however。For
    instance。 Albert tokenizers will add a long end score in front of all the tokens
    that had its space before them。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看下面链接的Tokenization算法视频以获取更多信息。我们在“I”前看到的ash ash前缀是鸟类用来指示该token不是单词开头的惯例。其他组织可能使用不同的惯例。举例来说，阿尔伯特tokenizers会在所有前面有空格的token前加一个长的结束分数。
- en: Which is a convention shared by all sentence based tors。The second step of the
    tokenization pipeline is to map those tokens to respective IDs。 as defined by
    the vocabulary of the tokenizer。This is why we need to download the file when
    we instant hit a tokenizer with the form pre method。We have to make sure we use
    the same mapping as when the model was portrayed。To do this。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个所有基于句子的tors共享的惯例。tokenization管道的第二步是将这些token映射到相应的ID，正如tokenizer的词汇所定义的。这就是为什么当我们用form
    pre方法即时调用tokenizer时需要下载文件。我们必须确保使用与模型展现时相同的映射。为此。
- en: we use the converttugans to IDs method。You may have noticed that we don't have
    the exact same results as in our first slide。Or note as this look like a list
    of random numbers anyway。 in which case allow me to refresh your memory。We the
    number at the beginning and the number at the end that are missing。Those are the
    special tickets。So special tokens are added by the proper formalal method。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用converttugans到IDs的方法。你可能注意到我们与第一张幻灯片的结果不完全相同。或者说这看起来像是一串随机数字。在这种情况下，请允许我刷新你的记忆。缺失的就是开头和结尾的数字。那些是特殊的token。所以特殊token是通过适当的formalal方法添加的。
- en: which knows the indices of a token in the vocabulary and just adds the proper
    numbers in the input IDs list。You can look at the special tokens and more generally
    at how the tokenizer has changed your text by using the decocode method and the
    outputs of the tokenizer object。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 它知道token在词汇中的索引，并在输入ID列表中添加适当的数字。你可以使用decocode方法查看特殊tokens，以及更一般地查看tokenizer如何改变你的文本，借助tokenizer对象的输出。
- en: As for the prefix for beginning of worlds part of worlds。 both special token
    vary depending on which tor you are using。So belt tokener uses CLS on。 but the
    Robertta tokener uses HTMLl like on calls S and/lash S。Now that you know how the
    tocanazer works， you can forget all was intermediately admitted and don remember
    that you have to call it on your input texts。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 至于单词开始部分的前缀，两个特殊token会根据你使用的tor而有所不同。所以belt tokener使用CLS，而Robertta tokener则使用类似于HTML的S和/lash
    S。现在你知道tokenizer是如何工作的，你可以忘记所有中间步骤，并记住必须在输入文本上调用它。
- en: The output of the decokenizer don't just contain the input ID， however。To learn
    where the attention mask is， check out the batch input Together video。To learn
    about targettype ideas， you get the process spells of start video。![](img/0e882caa799deede0a88d67b58737bfe_40.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，decokenizer的输出不仅仅包含输入ID。要了解注意力掩码的位置，请查看batch input Together视频。要了解targettype的想法，你可以获得start视频的处理拼写。![](img/0e882caa799deede0a88d67b58737bfe_40.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_41.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_41.png)'
- en: Yeah。So we have one questions that's linked to what we were seeing just before
    the video are token slash W and token going to have separate representation IDs
    and yes。 we are going to have separate representation IDs because we are not the
    same token。As which is the。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。所以我们有一个问题，和我们在视频之前看到的有关，标记/ W和标记会有单独的表示ID吗？是的。我们会有单独的表示ID，因为我们不是同一个标记。正如所说的。
- en: the the whole meaning of that slashable you2an。Ne and sorry specific。So tokenizer
    or the tokenization pipeline， I'm not going to livecode was intermediately admitted
    because you shouldn't really learn them。 we're just showing them to show you the
    steps inside the pipeline the main thing to remember is that you just have to
    call your tokenizer on your input like this because this is the main E that's
    the most useful and now we'll look at what the attention mask is and what padding
    and Fun means the arguments that we had at the very beginning so that we fully
    explain what's happening inside all the tokenizer。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这整个可分割的意思是你能。对不起，具体一点。所以标记器或标记化管道，我不会实时编码，因为你实际上不应该学习它们。我们只是展示它们，以便你看到管道内部的步骤，记住的主要内容是你只需要像这样在输入上调用你的标记器，因为这是主要的E，这是最有用的。现在我们来看看什么是注意力掩码，填充和Fun意味着什么，这些都是我们在最开始时提到的参数，这样我们就可以完全解释所有标记器内部发生的事情。
- en: Oh， another question， is there a reason you would save a proed organizer or
    then better to just have a local copy。Very good question。 So， yeah， there is no
    real reason to save your patron organizer if you don't need to。If you didn't make
    any change inside it and you always you would always have a local copy because
    auto tokenizer that from pretrained is going to cache the files to avoid you download
    them again so there is no reason to save it the one exception is when youre creating
    folder that you want to push to the model hub in which case you should save your
    tokenizer inside that folder so that when you push user you push your model the
    configuration and the tokenizer that's used with it and we have all those threethink。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，还有一个问题，你是否有理由保存一个项目组织者，或者说最好只是保留一个本地副本。非常好的问题。所以，是的，如果你不需要，实际上没有理由保存你的项目组织者。如果你没有对它做任何更改，并且你总是会有一个本地副本，因为自动标记器会从预训练中缓存文件，以避免你再次下载它们，因此没有理由去保存。唯一的例外是当你创建一个文件夹并希望推送到模型库时，在这种情况下，你应该将你的标记器保存在该文件夹中，这样当你推送用户时，你推送的是你的模型、配置和与之使用的标记器，我们有所有这三者。
- en: The the the A face website is going to be able to apply in front APII in your
    model and you will be able to play with the Wichat online。other than that you
    won't really need to use the safe pretrain method on the tokener。 it's mostly
    for the model that's going to be super useful or and we will see part two closer
    to do that if you're training a tokener from scratch because you're pretraining
    a model。 for instance， in a new language， then youll need to use the safe pretrain
    method to save the result of your tokener。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: A面网站将能够在你的模型前应用API，你将能够在线玩Wichat。除此之外，你真的不需要在标记器上使用安全预训练方法。这主要是针对将非常有用的模型，我们将在稍后接近此部分时看到。如果你从头开始训练标记器，因为你正在预训练一个模型，比如说在一种新语言中，那么你需要使用安全预训练方法来保存你的标记器结果。
- en: So we're going to watch the last video for today live session about batching
    inputs to cover and then we'll look more crisly at the good to。 let me just。Launch
    the collab first so that we don't have to wait after the video and then we'll
    watch the video together。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将观看今天的最后一个视频，关于批处理输入的现场会议，然后我们将更仔细地看一下好的部分。让我先启动协作工具，这样我们就不用在视频之后等待，然后我们一起观看视频。
- en: 嗯。Come on。Sth。Yes， I want to run it。And let me grab the video， patching and
    put together。![](img/0e882caa799deede0a88d67b58737bfe_43.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。来吧。某些事情。是的，我想运行它。让我抓取视频、打补丁并整理在一起。![](img/0e882caa799deede0a88d67b58737bfe_43.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_44.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_44.png)'
- en: 嗯。Yeah。How to batch inputs together in this video， well see how2 batch input
    sequences together。In general all， the sentences we want to pass through our model
    won't all have the same length。Here we are using the model we saw in the sentiment
    analysis pipeline and want to classify two sentences。When tokenizing them and
    mapping each token to its corresponding input I。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。是的。在这个视频中，如何将输入一起批处理，我们将看到如何将输入序列批处理在一起。一般来说，我们希望通过模型传递的所有句子不会都是相同的长度。这里我们使用的是我们在情感分析管道中看到的模型，并希望对两个句子进行分类。当对它们进行标记化并将每个标记映射到其对应的输入时。
- en: we get two lists of different length。Trying to create a densor or an newbi array
    from the two will result in an error because all arrays and densilrs should be
    a recangro。One way to overcome this limit is to make the second sentence the same
    length at the first by adding a special token as many times as necessary。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到两个不同长度的列表。从这两个列表创建一个张量或新的数组将导致错误，因为所有数组和张量应是规则的。克服此限制的一种方法是通过添加特殊标记使第二个句子的长度与第一个句子相同，直到需要的次数。
- en: Another way would be to truk the first sequence to the length of the second。But
    we would then lose a lot of information that may be necessary to properly classify
    the sentence。In general， we only truncate sentences when we are longer than the
    maximum length the model can handle。The value used to pad the circums should not
    be picked randomly。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将第一个序列截断到第二个的长度。但这样我们将失去许多可能对正确分类句子至关重要的信息。一般来说，我们只在句子超过模型能够处理的最大长度时才会截断句子。用于填充的值不应随意选择。
- en: The model has been portrayed with a certain padding ID， which you can find in
    tokenizer。pa tokenite。Now that we have better sentences， we can make a batch with
    them。If we pass the two sentences to the model separately and patched together
    however。 we notice that we don't get the same results for the sentence that is
    pad here the second one is that the bug in the transformerers library now if you
    remember that transformers will all make easy use of attention layers。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已经被描绘为具有特定的填充 ID，你可以在 tokenizer 中找到。现在我们有了更好的句子，我们可以用它们创建一个批次。如果我们单独将两个句子传递给模型并合并在一起，我们会注意到，对于填充的句子，即第二个句子，我们没有得到相同的结果，这是
    transformer 库中的一个错误。如果你记得，transformers 会轻松使用注意力层。
- en: this should not come as a total surprise。When computing is the contextual representation
    of each token。The attention layers look at all the other words in the sentence。If
    you have just a sentence or the sentence with soball padic tos added。 each logicalal
    don't get the same values。To get the same results with or without padding。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应让人感到完全意外。当计算每个标记的上下文表示时，注意力层会查看句子中的所有其他单词。如果你只有一个句子或句子与填充标记，每个逻辑标记都不会获得相同的值。要获得相同的结果，无论是否有填充。
- en: we need to indicate to the attention layers that we should ignore those padding
    tickets。This is done by creating an attention mask， a tonsil with the same shape
    as the input IDs with series and ones。Once indicates the tokens the attention
    layers should consider in the context。 and the the tokens which should ignore。Now，
    passing this attention mask along with the input ID will give us the same results
    as when we send the two sentences individually to the model。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要向注意力层指示忽略那些填充标记。这是通过创建一个注意力掩码完成的，该掩码与输入 ID 具有相同的形状，包含零和一。一旦指示了注意力层应考虑的标记，以及应忽略的标记。现在，将此注意力掩码与输入
    ID 一起传递将给我们与单独将两个句子发送给模型时相同的结果。
- en: This is all done behind the scenes by the tokenizer when you apply to several
    sentences with a flag bedding equal true。It will apply his bedding with a proper
    value to the smaller sentences and create the appropriate attention mask。![](img/0e882caa799deede0a88d67b58737bfe_46.png)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是由 tokenizer 在后台完成的，当你将几个句子应用于标志填充等于真时。它会为较小的句子应用适当的填充值，并创建适当的注意力掩码。![](img/0e882caa799deede0a88d67b58737bfe_46.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_47.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_47.png)'
- en: 。So， let's。Look at the same thing in collab。Let for any questions。 no， I don't
    see any questions。 don't hesitate to ask your questions in the chat again。 and
    let's look at the same code that we had to look again at what the padding and
    attention mask are exactly。So as we saw in the video， if we try to。Apply our model。Directly。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 。所以，让我们。看看在协作中的相同内容。有什么问题请问。没有，我没有看到任何问题。请随时在聊天中再次提出你的问题。接下来，让我们再次看看我们需要关注的代码，具体来说是什么填充和注意力掩码。正如我们在视频中看到的，如果我们尝试。直接应用我们的模型。
- en: Oh no it it not take the exact same thing as in the video。 If we try to apply
    our model directly on just one sentence that we recognize and converted to to
    ideas like that using the the same code as in the previous video。 it's gonna fail
    because the model wants batches of input so it wants。It's actually the dokenizer
    even if you have just one sentence is adding one dimension。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，不，它并没有像视频中那样精确。如果我们直接在一个被识别并转换为 ID 的句子上应用我们的模型，使用与前一个视频相同的代码，它会失败，因为模型想要输入的批次。因此，实际上即使你只有一个句子，tokenizer
    也会添加一个维度。
- en: you can see like there are two pairs of brackets surrounding that so here this
    is a tonsor of shape1 by a guess 60。And so if you want to pass just one sentence
    that you processed manually to a model。 you have two had one dimension， for instance，
    by adding a pair of brackets here。This is for a separate example， so now looking
    at two sentences together， so if we have two lists。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到有两个括号包围着这个，所以这里是一个形状为1乘60的张量。如果你想将手动处理的单个句子传递给模型，你需要增加一个维度，例如，在这里添加一对括号。这是一个单独的示例，现在来看两个句子一起，所以如果我们有两个列表。
- en: so let's say we have both id and both ID and we want to make a pair of sentences。We
    can't we can create a list of lists like that， but we can't create an array with
    the two of them because then don't have the same shape。So we need to add a padding
    index so we could ever hide it at the handle at the beginning。 most of the Transal
    model expects the padding to be applied on the right with the exception of ExcelNe
    which expects it at the beginning。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们同时有id和ID，并且想要形成一对句子。我们可以创建一个列表的列表，但不能创建一个包含这两个句子的数组，因为它们的形状不一致。所以我们需要添加一个填充索引，这样我们就可以在开头隐藏它。大多数Transal模型期望填充应用在右侧，ExcelNe则期望在开头。
- en: but the tokenizer should be responsible to apply the padding because the tokenizer
    knows what the model wants and is going to apply it on the right side。嗯。So once
    you have thosepat I， you can create。You can sorry。 create done source from them
    and then pass them through the model。And if we look like we we've just done in
    the video at the outputs。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 但tokenizer应该负责应用填充，因为tokenizer知道模型需要什么，并将其应用在右侧。所以一旦你有了这些pat I，你就可以创建。对不起，从中创建源，然后传递给模型。如果我们像在视频中看到的那样查看输出。
- en: if we pass the sentences separately， So we still have to add power brackets
    if we want the model to be applied on them because the model expects a batch。
    So the batch can have just one thing inside it， but it needs to be something that
    has two dimension。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分别传递句子，仍然需要添加括号，如果我们希望模型对它们进行处理，因为模型期望一个批次。因此，批次内部可以只有一项，但它需要是具有两个维度的东西。
- en: So if we pass sequence1 and sequence 2， we get those two results and if we pass
    the patch with the two sentences。 we get the same result for the first sentence，
    but different result for the second sentence we can see that Ph2 are difference
    here which is because the circumst with a paddingken if we don't do anything special
    the model is not going to compute the same results it's not going to properly
    ignore the paddingken so that's so in the video is because of the attention layers
    the attention layers are call the transformers model and are we explain that at
    length during chapter1 there are layers that not computer presentation not just
    of one word but one word within its context because the transformer models were
    designed for translation at the very beginning and if we want to translate the
    word we don't just need to pay attention to that word but all the words around
    it for instance to know which genders the word hard。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们传递sequence1和sequence2，就会得到这两个结果；如果我们传递这两个句子的patch，第一次句子的结果是相同的，但第二句的结果不同。我们可以看到这里的Ph2有所不同，这是由于填充的上下文。如果我们不做特别处理，模型不会计算出相同的结果，它不会正确忽略填充。所以在视频中提到的就是由于注意力层，注意力层被称为transformers模型，我们在第一章详细解释过。这些层不仅计算单词的表示，还计算单词在其上下文中的表示，因为transformer模型一开始就是为翻译设计的。如果我们想翻译一个单词，就不仅需要关注那个单词，还需要关注周围的所有单词，例如了解“hard”的性别。
- en: It's a singular or parole known， things like that。And so if we don't say to
    if we don't tell the attention layer that this is not a real token it just affect
    token we needed to to have a rectangle and make a batch the attention layer is
    going to pay attention to the token and computer contextual representation that
    adds that token into account So to tell the attention layer now that's not a real
    token don't pay attention to it we have to create what is called an attention
    mask so here we put ones where we want the attention layer to pay attention and
    zero where we want the attention layer to ignore so is the same shape as the batch
    adss that we are using and the zero is put where we have the token I token ID。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个单一的或已知的符号之类的东西。因此，如果我们不告诉注意力层这不是一个真实的标记，而仅仅是一个影响标记，我们需要一个矩形来形成一个批次，注意力层将会关注这个标记并计算上下文表示，这样就考虑了该标记。为了告诉注意力层这不是一个真实的标记，不要关注它，我们必须创建所谓的注意力掩码，因此在这里，我们在想要注意力层关注的地方放置1，而在想要注意力层忽略的地方放置0，因此它与我们使用的批次的形状相同，而0放置在我们有标记ID的地方。
- en: And if we do that and pass those through the model now we see that we get the
    same output。 so here the first sentence is still the same because there was not
    bedding。 but this output here is the same as this output here。 which was the output
    of the model with the second sentence。So this is what's done by the tokenizer。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做并将这些传递给模型，现在我们看到我们得到了相同的输出。因此，第一个句子仍然是相同的，因为没有填充，但这里的输出与这里的输出是相同的，后者是模型处理第二个句子的输出。这就是标记器所完成的工作。
- en: if you remember when we were passinging all two sentences， it was returning
    something with two keys。 one was the input Is which correspond to this thing here。
    and therefore one was the attention mask which corresponds to this thing here。The
    truncation argument， so when we passed the two sentences to auto at the very beginning。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得我们传递的两个句子，它返回了带有两个键的内容。一个是输入，它对应于这里的内容，另一个是注意力掩码，它对应于这里的内容。截断参数，因此当我们将这两个句子传递给自动模型时，正是这样。
- en: we said padding equal true and truncation equal2， so truncation equal true is
    going to truncate very very long sentences because for be model。 for instance，
    can only handle sequences that have a length of 512 maximum。So if we have a sequence
    that's longer than that， the model is going to fail and we need to truncate it。
    you shouldn't truncate your input for any other reason when making them shorter
    than the maximum length the model can handle otherwise because when you trun sorry
    you remove information so for padding we are adding something and we can tell
    the attention layer to ignore it so at the end we get the exact same results with
    or without padding with truncation you're ignoring information but you just can't
    recover so you will never be able to get the same results without truncation。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说填充为真，截断为2，所以截断为真将会截断非常非常长的句子，因为对于模型来说，例如，只能处理长度最大为512的序列。因此，如果我们有一个比这更长的序列，模型将会失败，我们需要截断它。在使输入短于模型能够处理的最大长度时，不应该因为其他原因而进行截断，因为当你截断时，会丢失信息；而对于填充，我们是添加一些东西，并且可以告诉注意力层忽略它，因此最后我们得到的结果与是否填充是完全相同的。通过截断，你是在忽略信息，但你无法恢复，所以没有截断的情况下，你将永远无法得到相同的结果。
- en: But sadly it's needed because transformers have a maximum length。 so if you
    have very long inputs that are greater than than at maximum length you need to
    locate them。And if we go back looking at the course in the putting all。Putting
    it all section。We have the various patting on from strategy。 So here。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 但遗憾的是，这是必要的，因为变压器有最大长度。因此，如果你有非常长的输入超过最大长度，你需要将其定位。如果我们回到课程中并查看“把所有东西放在一起”的部分，我们有各种填充策略。因此在这里。
- en: So if we go back with the two sentences we are using at the very beginning of
    this left session and pass them we can pass them to the skener。 which is going
    to then output list of list， but we won't be able to patch them together without
    applying padding so we have various strategies to apply padding when we do padding
    equal through it's the same as doing padding equal longest which is going to pad
    the sequences up to the maximum length inside the samples that you're passing
    so here。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们回到一开始这段对话中使用的两个句子并传递它们，我们可以将它们传递给筛选器，它将输出一个列表的列表，但我们无法将它们拼接在一起而不应用填充。因此，我们有各种策略来应用填充。当我们填充为真时，它与填充为最长是一样的，都是将序列填充到你所传递的样本中的最大长度。
- en: We have two sequences with patting equal longest， it's going to add the second
    sentences。 the second sentence here to the length of the longer sentence。 If we
    go back to the notebook behind pipeline， we can see that here。With all those zeros
    being added so that the circum sentence inside the batch is the exact same length
    at the first。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个序列，填充等于最长，这会将第二个句子添加到较长句子的长度。如果我们回到管道后面的笔记本，我们可以看到这里。所有这些零被添加，以确保批处理中的句子长度与第一个句子完全相同。
- en: You can also say patting equal max length max length refer to the model max
    length， so for instance。 it's 512 for be or distber， so this is going to add every
    sentence to 512 the maximum length of the model of adoption should not be considered
    when。When you have short sentences， for instance， its in general it's better to
    add to the longest thing you have unless you need to use fixed shaped for some
    reason。 for instance， TPUus like fixed shapes better in which case you would use
    the max length option and which is a bit inefficient but for instance with the
    TPU accelerator。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以说填充等于最大长度，最大长度指的是模型的最大长度，所以例如，对于be或distber，它是512，因此这会将每个句子添加到模型的最大长度512。对于短句子而言，模型的采用不应被视为固定形状，通常最好是添加到你拥有的最长句子，除非出于某种原因需要使用固定形状。例如，TPU更喜欢固定形状，在这种情况下你会使用最大长度选项，尽管这有点低效，但对于TPU加速器而言是如此。
- en: it's the only way to get some real speed because they need all the inputs towards
    the exact same shape。And you can also specify a max length that you want， so for
    instance。 if you know that in your dataset set， all your sentences are shorter
    than 1 around28。 you could say patting equal max length max length equal 1028。And
    so you should also use truncation。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是获得真实速度的唯一方法，因为它们需要所有输入具有完全相同的形状。你也可以指定一个你想要的最大长度，所以例如，如果你知道在你的数据集中，所有句子都短于大约28，你可以说填充等于最大长度，最大长度等于1028。因此，你还应该使用截断。
- en: as we said， because you need to truncate when your inputs when you there are
    longer than the maximum length a model can handle。 you can also specify the maximum
    length to which you want to truncate your inputs。So that's it for padding on。
    Do we have any questions。嗯。No questions， Small links shared by Lewiss。So。The last
    thing is， as we saw before， you can tell you token to return bytoch tensor and
    soft flu tensor on entire array。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，因为当输入超过模型可以处理的最大长度时，你需要进行截断。你也可以指定希望截断输入的最大长度。所以关于填充就这些。我们有任何问题吗？嗯，没有问题，Lewis分享了一些小链接。所以，最后一件事是，正如我们之前看到的，你可以告诉你的标记返回bytoch张量和软流张量在整个数组上。
- en: And as we saw in one of the videos， the tokenizer， the tokener pipeline video
    specifically。 the tokenazer adds the special token at the beginning and at the
    end of our sentence。 so it depends exactly on which model youre using， but the
    tokener will all know what token's special token the model expects and will put
    them at the beginning and at the end of your sentences。And so to conclude this
    chapter， here is the world code， it's just missing the post processing。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在其中一个视频中看到的，特别是tokenizer视频，tokenizer会在句子的开始和结束添加特殊标记。因此，这完全取决于你使用的模型，但tokenizer会知道模型期望的特殊标记，并将它们放在句子的开始和结束。为总结这一章节，这里是代码，只是缺少后处理。
- en: but here is the world code that was executed behind the scene by the pipeline。嗯。So
    you import auto tokenizer on automod for sequence classification。 the auto beam
    meaning that you can use any checkpoint from the hub。 it's going to pick the right
    architecture for you。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是管道后台执行的代码。嗯。所以你导入用于序列分类的auto tokenizer和automod。auto beam意味着你可以从中心使用任何检查点。它会为你选择正确的架构。
- en: The checkpoint that was used by the pipeline is this1。 you can use any of our
    checkpoint on the model hub that correspond to a sentiment analysis task。We can
    load the tokenazizer with the from betweentrain method。 we can load our model
    with the from betweenttrain method and if we have two sentences。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 管道使用的检查点是this1。你可以使用我们模型中心上任何对应于情感分析任务的检查点。我们可以通过betweentrain方法加载tokenizer。我们可以通过betweenttrain方法加载模型，如果我们有两个句子。
- en: we can tokenize them together with padding equal2 con equal2 which on tensor
    source equal piy toch if we're using py toch tensor flu which using tensor flu
    and then we pass the tokens to the model and to get the plus process results it's
    just missing the soft max here to have the exact same result as pipeline。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将它们与填充一起标记为equal2 con equal2，这在张量源上等于piy toch，如果我们使用py toch张量流，然后将标记传递给模型以获得加法过程的结果，这里只是缺少soft
    max，以便得到与管道完全相同的结果。
- en: One last question I'm confused between padding you call longest and dynamic
    padding。 Is it similar。 Yes， it's very， very similar。 Well look at dynamic padding
    in the next chapter when doing training。 but for those who don't know what it
    is dynamic padding is when you。When you go through your training data and beL
    batches。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个问题，我对你提到的最长填充和动态填充感到困惑。它们相似吗？是的，非常相似。我们将在下一章中学习动态填充的内容，但对于那些不知道它是什么的人来说，动态填充是在你遍历训练数据时进行的。
- en: dynamic padding means each time you have to build a batch。 you pad your sentences
    to the maximum length inside the batch。 and so that's what padding equal longest
    will do if you pass your sentences a small batch of sentences is going to create
    a batch with for instance eight sentences and the second dimension is going to
    be the maximum length inside that batch。Whereas patting equal max length is going
    to add everything to a fixed max length。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 动态填充意味着每次你需要构建一个批次时，你将句子填充到该批次内的最大长度。因此，如果你传递一小批句子，填充equal longest将会创建一个包含例如八个句子的批次，第二个维度将是该批次内的最大长度。而填充equal
    max length则会将所有内容填充到一个固定的最大长度。
- en: either the maximum length of model canon all or the maximum length you passed
    along to the dekenizer。And so that's it for the basic use of models and tokenizers。Thank
    you for following the live stream so now that weve seen that chapter together
    you should be able to complete the questionnaire at the hand and then before going
    to chapter 3。 it's useful if you try to run again the pipeline collab that we
    saw together on the batching input togetherab that we looked at together and try
    to understand what every cell is doing and maybe even try to redo it yourself。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最大长度可以是模型所能处理的最大长度，也可以是你传递给解码器的最大长度。这就是模型和标记器的基本用法。感谢你关注这次直播，现在我们一起看完这一章后，你应该能够完成手头的问卷，然后在进入第三章之前，如果你尝试再次运行我们一起看到的批处理输入的pipeline
    colab，并试着理解每个单元的功能，甚至尝试自己重新做一遍，那将是很有用的。
- en: And then try to think of a problem you want to work on on a classification with
    a text classification problem you want to work on and try to use a base model
    and an organizer on that problem to be able to get outputs from some inputs and
    we'll see in the next chapter of to actually find tuneni model on your given problem
    and in chapter 4 we'll see how to upload as a result to the bundle hub so that
    you can venture that model to with the rest of the community and use the widgets
    of the inference API online to have demos of。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后尝试想一个你想要解决的分类问题，特别是文本分类问题，并尝试在该问题上使用基本模型和组织器，以便从一些输入中获得输出。我们将在下一章中实际找到适合你给定问题的模型，并在第四章中看到如何将结果上传到bundle
    hub，以便你能与社区其他成员分享该模型，并在线使用推理API的小部件进行演示。
- en: Of your model。 let me look at the questions before we end this live stream。Oh。
    following up from the previous question with patting equal whole longest。 we don't
    need to use that decorator with padding。 That's not entirely true。 And we' all
    see exactly why in the next chapter， we。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 关于你的模型。在我们结束这次直播之前，让我看看问题。哦，接着上一个关于填充equal whole longest的问题。我们不需要在填充时使用那个装饰器。这并不完全正确。我们将在下一章中确切地看到原因。
- en: we need to be in the dynamic of training a model， but。If you are playing padding
    all longest while you' doing tokenization on your wall data set。 it's going to
    add to the longest element in your data set。 so that's not really the same thing
    as doing dynamic padding。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要处于训练模型的动态中，但如果你在整个数据集上执行最长填充时进行标记化，它将增加数据集中的最长元素。所以这与动态填充并不完全相同。
- en: But yeah well we'll dive into that in the next chapter。Otherwise。 thank you
    all for following this live stream and next week on Cha 3 there are four different
    live streams because Cha 3 is very different for Pytch and Tensorflow since it's
    about training and functioning until now the code add some little differences
    and you can see that you have a switch here for the course if you want to switch
    between Pytch and Tensofflow if you want to look at exactly what' difference but
    for next chapter is going to be very different so you have live session specifically
    for Pythtch and live section specifically for Tensorflow be sure to check on the
    formss which one you want to attend to and market into your calendar。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们将在下一章深入探讨这个问题。谢谢大家关注这个直播，下一周在Cha 3将有四个不同的直播，因为Cha 3对于Pytch和Tensorflow非常不同，主要是关于训练和功能。到目前为止，代码有一些小差异，你可以看到这里有一个切换选项，可以在Pytch和Tensorflow之间切换，如果你想了解它们的具体区别。不过下一章将会非常不同，所以会有专门针对Pytch的直播和专门针对Tensorflow的直播，务必查看一下表格，选择你想参加的课程，并记在你的日历上。
- en: And yeah， let's first this up。 Thanks a lot for coming， bye bye。![](img/0e882caa799deede0a88d67b58737bfe_49.png)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，让我们先结束这一段。非常感谢大家的到来，再见！![](img/0e882caa799deede0a88d67b58737bfe_49.png)
- en: 。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![](img/0e882caa799deede0a88d67b58737bfe_51.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_51.png)'
