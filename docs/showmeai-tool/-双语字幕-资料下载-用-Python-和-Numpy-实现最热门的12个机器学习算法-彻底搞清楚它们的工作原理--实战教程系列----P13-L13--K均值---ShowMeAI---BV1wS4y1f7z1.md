# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P13ÔºöL13- KÂùáÂÄº - ShowMeAI - BV1wS4y1f7z1

Hi everybody and welcome to a new machine learning from Sct tutorial Today„ÄÇ

 we are going to implement the K means algorithm using only built and Python modules and Nmpy„ÄÇ

 The goal of the K means is to cluster a data set into K different clusters and here we have a unlabeled dataset set„ÄÇ

 So we are dealing with an unsupervised learning technique and each sample should be assigned to the cluster with the nearest mean So let's have a look at some images to see what this means„ÄÇ

 So here we have our unlabeled data and now in this case we want to find three different clusters So should look like this„ÄÇ

And then we assign the labels to the closest clusterÔºå so to the center of the closest cluster„ÄÇ

So yeahÔºå this is what we want to do and how are we going to achieve this So this is an iterative optimization technique„ÄÇ

 so first we initialize our cluster centers so we randomly pick some samples and say these are our first centers and then we do these two steps until we are conversed„ÄÇ

So firstÔºå we update our cluster labelsÔºå which means we assign the points to the nearest cluster center„ÄÇ

And the cluster sender is also called centroid„ÄÇAnd nextÔºå we update our centroids„ÄÇ

 So now we set the centerÔºå the new center to the mean of each cluster„ÄÇ

 and we iteratively do this until there's no more change„ÄÇ So let's again„ÄÇ

 have a look at the images to see how this is working„ÄÇ So firstÔºå we have our unlabeled data set„ÄÇ

And now we randomly pick three centroids„ÄÇ So here I've drawn them„ÄÇ So I hope you can see them„ÄÇ

 So these are our initial centroids„ÄÇ And now we assign the labels to the„ÄÇOf the data to the„ÄÇ

 to the label of the closest centroid„ÄÇ So this is our first initialization„ÄÇ

 And now we start optimizing„ÄÇ So now we update our centroids„ÄÇ

 So we calculate the new mean for each cluster„ÄÇ So I think this centroid will be moved to something like here„ÄÇ

 And this will maybeÔºå maybe be here„ÄÇ And the green centroid will be moved to maybe here„ÄÇ

 So let's see what's happening„ÄÇ YeahÔºå so this is the new centroids„ÄÇAnd now we update our label„ÄÇ

 our labels„ÄÇ So now we check which is the closest centroid for each label„ÄÇ

 so maybe these will become blue and more of these will become orange„ÄÇSo yeahÔºå this is the next step„ÄÇ

 And now againÔºå we update our centroid„ÄÇ So I think this one will further move to to the right„ÄÇ

And this one will move up here and this one will maybe stay the same„ÄÇ So yeahÔºå this is the next step„ÄÇ

 And now againÔºå we update the labels„ÄÇ So I think these will become orange now„ÄÇ

 and these will become blue„ÄÇ and now we are almost done„ÄÇ So maybe here are some slight shifts„ÄÇ

 So againÔºå update the centroids„ÄÇ then againÔºå the labels then the centroids than the labels And now there's no more change„ÄÇ

 So now we are converged„ÄÇAnd yeahÔºå so this is the whole approach„ÄÇ

 And the only math that we need for this is the Euclidean distance„ÄÇ

 So I already showed you this in the tutorial about the Can nearest nava„ÄÇAlgorithm„ÄÇ

 the Euclidean distance between two vectors is defined as the square root of the sum over the squared distances„ÄÇ



![](img/b78b3a13800e496c831fda09c2e14c17_1.png)

So this is all we need„ÄÇ So now we can start„ÄÇ So let's import nuy as N PÔºå of course„ÄÇ

 And then I will sayÔºå I will set a random seat toÔºå let's say 42Ôºå you don't need this„ÄÇ

 but I want to reproduce my data later„ÄÇ And since we are using a random initialization„ÄÇ I want to„ÄÇ

 to have the same resultsÔºå or I want to„ÄÇTo reproduce my results„ÄÇ So I will set a random seat here„ÄÇ

And now let's define our Euclidean distance function first„ÄÇ So Euclidean distance of two vectors„ÄÇ

 x1 and x2„ÄÇ So this will be a global function„ÄÇ And here we have to implement the formula that I just showed you„ÄÇ

 and we can do this in one line„ÄÇ So we say return and nuy firstÔºå we have the square root off„ÄÇ

 And then we have the sum nu pi sum„ÄÇ And then we have the sum over all the distances„ÄÇ

 So we can say x1 minus x2 to the power of2„ÄÇ So the square distances„ÄÇ So this is the function„ÄÇ

 And now we can implement our K means class„ÄÇ So this will have an in it„ÄÇWhich has self„ÄÇ

 And then it will get a K„ÄÇ So this will be the number of clusters„ÄÇ And by defaultÔºå let's say it's 5„ÄÇ

 Then it will get a max its„ÄÇ So this is the maximum number of iterations„ÄÇ

 we want to do for our optimization„ÄÇ And by defaultÔºå let's say it's 100„ÄÇ

 And then it will also get a Boolean plot steps equals false„ÄÇ So you don't need this„ÄÇ

 but I'm going to implement an additional function here to plot the different steps like we've just seen„ÄÇ

 So„ÄÇYeahÔºå so firstÔºå we set them or we store them„ÄÇ So we say self dot K equals K self dot max iters equals max iters and self dot plot steps equals plot steps„ÄÇ

 And now we create our empty clusters and centroids„ÄÇ So self dot„ÄÇClus equals„ÄÇOops„ÄÇEquals„ÄÇ

 and now here we want to be careful because this is important„ÄÇ So this is a list of lists„ÄÇ

 So a list of sample indices„ÄÇFor each cluster„ÄÇ and in the beginningÔºå each cluster has an empty list„ÄÇ

 So we sayÔºå and we use list comprehension and then say we say have an empty list for underscore in range self dot K„ÄÇ

 So for each clusterÔºå we initialize an empty list„ÄÇAnd then we say our self dot centroids equals„ÄÇ

 and this will also be an empty list„ÄÇ And here we are going to store the feature vectors or the mean feature vector for each cluster„ÄÇ

 So mean feature vector for each cluster„ÄÇ So here we are having actual samples„ÄÇ

 And here we just store the indices„ÄÇ So this is important„ÄÇAnd now we can continue„ÄÇ

 So now usually we would implement a fit and a predict method„ÄÇ

 But since we are having a unsupervised learning technique hereÔºå we and we don't have any label data„ÄÇ

 we just have to implement the predict method„ÄÇ SoÔºå and we don't need this the fit method„ÄÇ

 So let's define our predict method„ÄÇWith self and X„ÄÇAnd hereÔºå firstÔºå let's store our data„ÄÇ

 self x equals x„ÄÇ and then the dimensions„ÄÇ So self number of samples and self dot number of features equals x dot shape because we need this later„ÄÇ

 And as alwaysÔºå this is a nuy and D array„ÄÇAnd soÔºå yeah„ÄÇ

 So now let's do the steps we just talked about„ÄÇ So firstÔºå we have to„ÄÇInitialize our„ÄÇOur centroids„ÄÇ

And then we do a optimization„ÄÇ So here we can say four underscore in range self dot max its„ÄÇ

 And now here in our loop firstÔºå we update our clusters„ÄÇSoÔºå let's sayÔºå update clusters„ÄÇAnd then„ÄÇ

 we update the cents„ÄÇAnd then we check for conversiongs„ÄÇCheck„ÄÇIf converged„ÄÇAnd if soÔºå then we break„ÄÇ

 And at the endÔºå we want to classify the samples as the index of their clusters„ÄÇ

 So here we say return„ÄÇReturn„ÄÇCluster labels„ÄÇ So this is what we have to do„ÄÇSo now let's start„ÄÇ

 So let's say we want to initialize our centroids„ÄÇ So we want to randomly pick some samples„ÄÇ

 So let's say random sample indices equals„ÄÇ and now we use list comprehension„ÄÇ So or noÔºå sorry„ÄÇ

 here I can use nuy dot random dot choice„ÄÇAnd this will get self dot number of samplesÔºå and self dot„ÄÇ

KÔºå and we also have to say replace equals false because we don't want to pick the same indices twice„ÄÇ

 So this will be an array of size self dot K„ÄÇ And for each entry„ÄÇ

 it will pick a random choice between 0 and the number of samples„ÄÇAnd nowÔºå we assign„ÄÇ

The according sample that belongs to this index to our centroids„ÄÇ

 So we say self dot centroids equals„ÄÇ And now here we use list comprehension„ÄÇ So we say self dot X„ÄÇ

Of the current index for index in random samplingdices„ÄÇ

So this is the initialization of our centroids„ÄÇ And now we can do the optimization„ÄÇ So first„ÄÇ

 we say we update our clusters„ÄÇ So we say self dot clusters equals self dotÔºå create clusters„ÄÇ

 and this will get self dot centroids„ÄÇ So this is a help of function„ÄÇ

 create clusters that we are going to implement now„ÄÇ

 So define create clusters with self and the cents„ÄÇ

So here we assign the samples to the closest centroids to create our clusters„ÄÇ So first we have„ÄÇ

An empty list of lists for our clusters„ÄÇAnd now we iterate over our data„ÄÇ

 So we say for index and sample in enumerate self dot X„ÄÇ

 So this enumerate function will give us the current index and the current sample„ÄÇ

 Now we want to get the closest centroid„ÄÇ And we want to have the index of this„ÄÇ

 So we say centroid index equals self dot closest„ÄÇCentroid„ÄÇAnd this will getÔºå get the current sample„ÄÇ

 and then it will get the centroid„ÄÇ So this will be another helper function that we„ÄÇ

 we will create in a second„ÄÇ But now let's continue here„ÄÇ So when we have the centroid index„ÄÇ

 we append„ÄÇOr we take the current„ÄÇClusterÔºå so the clusters of this„ÄÇCentroid index„ÄÇ

 And then we append„ÄÇThe current index„ÄÇ So we put the current sample index in the closest cluster„ÄÇ

And then we return our clusters„ÄÇSo this is how we create our clusters„ÄÇ

 and now we need the define closest centroid function„ÄÇ

 which gets self and a sample and the centroids„ÄÇAnd„ÄÇ

Here we calculate the distances of the current sample to each centroid„ÄÇAnd thenÔºå want to get the„ÄÇ

The centroid or the index of the centroidÔºå which has theÔºå the closest distance„ÄÇ

 So let's calculate all the distances with list comprehension„ÄÇ

 So here we use the Euclidean distance function we already have„ÄÇ

 So the Euclidean distance of a sample and of each„ÄÇCentroid point„ÄÇ So point4 point in cents„ÄÇ

And then we want we so we have all the distances here„ÄÇ

 and now we want to see which is the minimum or the index with the minimum distance„ÄÇ

 So we can use Ny arc min here„ÄÇ So we say closest index equals Ny arc min of this distances„ÄÇAnd then„ÄÇ

 you simply„ÄÇReturn it„ÄÇ So return the closest index„ÄÇ So now we have this„ÄÇ

 and now we created our clustersÔºå and now we can continue with our optimization„ÄÇ

 So here we update our centroids as the next steps„ÄÇ But before we want to store the centroids„ÄÇ

 So let's say centroids alts equals self dot centroids so that we can check for convergence later„ÄÇ

 And then we say self dot centroids equals self dot get„ÄÇCentroids„ÄÇOf this„ÄÇ

 And this will get the self clusters„ÄÇ So this is another function„ÄÇ

This will assign the mean value of the clusters to the centroid„ÄÇ So for each cluster„ÄÇ

 we now calculate the mean„ÄÇ So let's define thisÔºå define„ÄÇGet„ÄÇÂèØ„ÄÇNotice do I get cents„ÄÇ

Which gets self and the clusters„ÄÇSo„ÄÇHereÔºå we initialize our centroids with zeros in the beginning„ÄÇ

 So let's say centroids„ÄÇEquals nuai zeros„ÄÇ And this will be of size self K and„ÄÇSelf dot N„ÄÇFeatures„ÄÇ

 and here this should be a two pull„ÄÇ So we have to be careful here„ÄÇ So for each cluster„ÄÇ

 we will store the feature Ea„ÄÇ So that's why it has to have this dimensions„ÄÇ

And now we iterate over the clusters„ÄÇ So we say four cluster index and cluster in enumerate„ÄÇCluters„ÄÇ

And„ÄÇThen we calculate the cluster mean„ÄÇ So cluster mean equals N pi mean of our self dot x of this cluster indices„ÄÇ

ü§¢ÔºåAnd this should be along the first axis„ÄÇSo againÔºå let's have a look at what this means„ÄÇ So our„ÄÇ

 as I saidÔºå our clusters is a list of lists„ÄÇ So if we have just a current cluster„ÄÇ

 then this is a listÔºå and this is a list of the indices that are in this cluster„ÄÇ

 So if we call self dot X with this indices„ÄÇThen it will only return the samples that are in the current clusters„ÄÇ

In the current cluster„ÄÇ And then we calculate the mean„ÄÇ

 So this is what what what's going to happen here„ÄÇ And now once we have our mean„ÄÇ

 we assign it to the current cluster So to the current centroids„ÄÇ So we say centroids of the current„ÄÇ

Cluster index equals cluster„ÄÇMean„ÄÇAnd then we are done and can return the centroids„ÄÇSoÔºå yeah„ÄÇ

 So now we have our new centroids„ÄÇ and now we check if we are converged„ÄÇ So we say if self dot„ÄÇ

Is converged„ÄÇThen we will break so we can stop here„ÄÇAnd this will get the centroids„ÄÇ

Old and the new new ones„ÄÇ So self do cents„ÄÇ And this is another helper function„ÄÇThis will simply„ÄÇ

Calculate the distances between each old and new centroids for all the cents„ÄÇ and check if this is0„ÄÇ

SoÔºå define is converged„ÄÇSelf and centsÔºå old and new centroids„ÄÇSo again„ÄÇ

 we calculate the distances with less comprehenion„ÄÇ This turns as equals„ÄÇ

 And here we calculate calculate the Eucladan distance of centroids old of I„ÄÇ

 So the current centroid vector„ÄÇAnd the new oneÔºå So centroids„ÄÇI„ÄÇ4 I in range self dot k„ÄÇ

üéºSo for each clusterÔºå it will look at the old and the new centroid vector and calculate the Euclidean distance and store it in this list„ÄÇ

 And then we can return some„ÄÇDistances„ÄÇEquals equals 0„ÄÇ

 So this is a build in function that will iterate over these entries and calculate or sum it up„ÄÇ

 And so if this is0Ôºå then there is no more change in our centroids„ÄÇ So we say it is converged„ÄÇSo„ÄÇ

 yeahÔºå so now we have this„ÄÇ And now we want to return the cluster labels„ÄÇ

 So let's say return self dot„ÄÇGet cluster labels„ÄÇAnd this will get self thought„ÄÇCluters„ÄÇ

So here for each sampleÔºå we will get the label of the cluster it was assigned to„ÄÇ

 So let's create this right here„ÄÇ So define get cluster labels with and the clusters„ÄÇAnd so first„ÄÇ

 we say our labels equals a nuy„ÄÇEty array„ÄÇOf sizeÔºå self dot number of samples„ÄÇ So for each sample„ÄÇ

 we want to returnÔºå which is the„ÄÇThe cluster it was assigned toÔºå But be careful here„ÄÇ

 because these labels are not the actual labels of our data because we don't know them„ÄÇ

 So this is just the index of the cluster it was assigned to„ÄÇSoÔºå yeah„ÄÇ

 So now we iterate over the cluster„ÄÇ So for cluster index„ÄÇAnd alsoÔºå cluster in enumerate„ÄÇCluters„ÄÇ

And then we iterate over the current cluster„ÄÇ So for sample index in cluster„ÄÇSo again„ÄÇ

 this is a list of listÔºå and each cluster has a list has all the the sample indices of the samples that are in this cluster„ÄÇ

 So for each sample index in this current clusterÔºå we say labels of the sample index equals„ÄÇ

The current cluster index„ÄÇAnd then we return our labels„ÄÇ And now we are done„ÄÇ

So one more thing that I want to implementÔºå but you don't need this is to have a plot function„ÄÇ

 So define find plot and self„ÄÇAnd here we are going to use matpl lab so I can sayÔºå import„ÄÇMuch„ÄÇ

Ploot lip dot pi plot S P L TÔºå And I'm not going to explain the details here„ÄÇ

 but if you want to see more tutorials about my plot lipÔºå then please leave some comments„ÄÇ

So let's implement the plot method„ÄÇ So hereÔºå I simply want to„ÄÇPloot the„ÄÇ

 the data and to which cluster it belongs„ÄÇ And then also the centroids„ÄÇ So let's create our figure„ÄÇ

 So F a X equals PÔºå L T dot sub„ÄÇPlootsÔºå and let's give this a fixed size of size„ÄÇ Let's sayÔºå12 by 8„ÄÇ

 And then we iterate over our clusters„ÄÇ So for I and index in„ÄÇSell inÔºå in nom„ÄÇ

Marade self dot clusters„ÄÇ And now we get the current point„ÄÇ So point equals self dot X„ÄÇOf the index„ÄÇ

 but we have to transpose it here„ÄÇAnd now we scatter the point„ÄÇ So A X starts scatter„ÄÇ

 And here I unpack the point„ÄÇAnd nowÔºå so this will plot all the points„ÄÇAnd for each cluster„ÄÇ

 it will use a different color„ÄÇAnd now we do theÔºå we plot all the centroids„ÄÇ

 So for point in self dot cents„ÄÇA x dot scatter„ÄÇ And againÔºå we unfold our point„ÄÇ

 and then we say marker equals„ÄÇXÔºå so it has a marker signÔºå and color equals„ÄÇ

Black and line width equals 2„ÄÇSoÔºå and then we have to say PLT dot showÔºå of course„ÄÇAnd„ÄÇ

So this is the plot function„ÄÇ And nowÔºå during our optimizationÔºå if we set plot steps to true„ÄÇ

Then we want to plot after we updated our clustersÔºå so we say„ÄÇIf self dot„ÄÇPloot steps„ÄÇ

 then self dot plot„ÄÇAnd againÔºå we also want to plot after we updated our centroids„ÄÇ

So we put it right here„ÄÇ and now we are done„ÄÇ So now let's run this„ÄÇ

 and I hope that everything is working„ÄÇ So let's clear this and run our script„ÄÇAnd„ÄÇOh„ÄÇ

 I missed a comma hereÔºå line 37„ÄÇUpdate centÔºå the centroids old equals„ÄÇThe current ones„ÄÇ



![](img/b78b3a13800e496c831fda09c2e14c17_3.png)

So againÔºå let's try this„ÄÇ And now it's working„ÄÇ So here we have„ÄÇ

 I created a data set with four different clusters„ÄÇ

 and we can see that it correctly could identify them„ÄÇ



![](img/b78b3a13800e496c831fda09c2e14c17_5.png)

So let's set plot steps to true and don't plot it at the end„ÄÇ And now let's run it again„ÄÇ



![](img/b78b3a13800e496c831fda09c2e14c17_7.png)

To see the different steps„ÄÇ So hereÔºå this is after our initialization„ÄÇ

 So we randomly picked some samples and said these are our first centroids„ÄÇ

 So maybe the initialization is not very good hereÔºå but„ÄÇ

You could find out the clusters correctly later„ÄÇ So let's see what's happening„ÄÇ

 So now we are so then we are starting to optimize„ÄÇ So now in the next step„ÄÇ

 we are calculating the new centroids„ÄÇSo I think the centroid of the blue cluster is moving to maybe over here and the orange centroid is moving to down here„ÄÇ

 So let's look at the next step„ÄÇ So yeahÔºå and now we are updating the labels„ÄÇ

 So I think these here are no longer orange but red and green„ÄÇSo„ÄÇYeah„ÄÇ

 and also some of them are now red„ÄÇ And now we update our centroids again„ÄÇ

 So I think this will further move over here„ÄÇ This will move down here„ÄÇ

And these will move a little bit„ÄÇ So yeah„ÄÇAnd now we again update our labels„ÄÇ

 So I think more of them will become red and more of them here will become green„ÄÇSoÔºå and now again„ÄÇ

 we update our centroid„ÄÇ So I think this will further move over here„ÄÇ

 The red centroid will move over here„ÄÇ This is already good„ÄÇ And the green 1 or moves over here„ÄÇ

 I think„ÄÇSoÔºå yepÔºå that's that„ÄÇ And now againÔºå we update the labels„ÄÇ now again„ÄÇ

 the centroids and the labels„ÄÇ And now I think we are converged„ÄÇ SoÔºå yeah„ÄÇ

 this is how the K meanss works„ÄÇ And I hope you understood everything„ÄÇ And if you liked it„ÄÇ

 please subscribe to the channel and see you next time by„ÄÇ



![](img/b78b3a13800e496c831fda09c2e14c17_9.png)

![](img/b78b3a13800e496c831fda09c2e14c17_10.png)