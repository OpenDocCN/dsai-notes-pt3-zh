# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P50ï¼šL9.4- è‡ªç„¶è¯­è¨€å¤„ç†å’ŒKerasçš„è¿ç§»å­¦ä¹  - ShowMeAI - BV15f4y1w7b8

Hiï¼Œ this is Jeff Heatonã€‚ We to applications of deep neural networks with Washington Universityã€‚

 In this videoï¼Œ we're going to look at natural language pretrained neural networks that you can transfer into your current project for the latest on my AI course and projectsã€‚

 click subscribe and the bell next to it to be notified of every new videoã€‚

 Transfer learning is also commonly used with natural language processingã€‚

 Now we've got an entire module of natural language processing coming upã€‚

 this just gives us a brief introduction to thisã€‚ Usually what you're doing with transfer learning is you're adding an embedding layer to your neural networkã€‚

 This basically takes raw textï¼Œ such as sentences and encodes it into some sort of a vector that will go into your neural networkã€‚

 and that vector can be predicted upon because it's a numeric input its fixed lengthã€‚

 whereas the normal sentences can have very numbers of wordsã€‚

 and you have to also really think about how you want to actually encode yourã€‚ğŸ˜Šã€‚



![](img/ca3b1efa82105f12ffd73e17655ac6cb_1.png)

There's a variety of different embedding layers or encodersã€‚

 basically models that you'll transfer inã€‚ and we'll see more about that in a couple of modules once we get to the natural language processing module for this courseã€‚

 I have listed some of the sources here that I use to put together this pretty simple example showing how we can analyze the Internet movie database and try to predict if a review is positive or negativeã€‚

 Now to make use of this exampleï¼Œ you'll need to have Tensorflow hub and Tensorflow datas installedã€‚

 you can run those two PP installsã€‚ if you're running Google coab every time you restart your coab you're going to have to rerun theseã€‚

 I already have these installed on my environment I'm currently running locallyã€‚

 the first thing I'm going to do is load in the Internet movie database reviewã€‚

 So this has reviews for all sorts of moviesï¼Œ Some are positiveï¼Œ some are negativeã€‚

 the idea is we're going to try to train the neural network to know if these reviews are positive or negative just by reading theã€‚

exNow we're going to make use of a pretrained embedding module called Google news swivel20 dimensionã€‚

 So this is a relatively low resolutionã€‚ So this is a relatively low dimension encoderã€‚

 It's only 20 dimensions is definitely a lot higher like word'reve and other ones that will see when we get to the natural language processing moduleã€‚

 I'm going go ahead and load in the pretrained neural network for thatã€‚'s built into Cursã€‚

 So they make that pretty easy for usã€‚ Let's look at the first three movie reviewsã€‚

 So these are the first three movie reviewsã€‚ You can see they are just pretty much freeform textã€‚

 This is unstructured data and structured data is particularly hard to use with a neural networkã€‚

 Not to worryï¼Œ we can take that unstructured data Basically right here is what we use previously to print it outã€‚

 We're going to pass it into the hub layerã€‚ The hub layer is the layer that we just transferred in from Tensorflow hub that isã€‚

The Google trained embedding layerã€‚ So now let's see what those three reviews look like hereã€‚

 Now you can see they actual vectorsï¼Œ vectors of 20 numbers eachã€‚

 that's a lot easier to train and deal with a neural network onã€‚

 you can basically just put raw text into the beginning of the neural network and it's going to process on these vectors that are created from those sentencesã€‚

 Nowï¼Œ what do the individual numbers meanï¼Œ that's very hard to sayï¼Œ but the vectors in the distanceã€‚

 So if you were to take the distance between two of these vectors for two different sentences more similar sentences will typically be closer togetherã€‚

 We'll see more how we can do linear or algebra on these vectors that the embedding layers are creating when we talk about word to Vã€‚

 Now let's go ahead and add the hub layer to our neural network that we're building and some other layersã€‚

Using sigmoid and only one output neuron because this is a binary classifierã€‚

 either it's a positive review or it's notã€‚ We'll compile the neural networkã€‚

 It's binary cross entropy and we'll use atom for trainingã€‚

 We're going to split into a training and test set so that we can train on one and validate on the other and we're going to go ahead and fit it using a relatively large batch size of 512ã€‚

 So this is going to go through a number of epochs we'll fast forward thisã€‚

 Okay the neural network is trainedã€‚ We can go ahead and evaluate itã€‚

 This is running it on the test setã€‚ can see it's getting an accuracy of around 85%ã€‚

 Let's use a rock chartã€‚ Let's look at how the training progressedã€‚

 you'll notice up here that when I fit it I kept the history that can be useful to look at because you can chart how the training progressed I'll run this this is very interesting because you can see that this reallyã€‚

 really shows overfitting an actionã€‚You can see that the training loss is going downï¼Œ downï¼Œ downã€‚

And continues to go downã€‚ But the validation loss plateausã€‚ and eventually starts to go upã€‚

 That is almost a textbook description of overfitting and why you use early stoppingã€‚

 So the loss was going downã€‚ This is pretty much just the mirror of image of thatã€‚

 The accuracy is going upã€‚ And againï¼Œ you can see where overfitting starts to occurã€‚

 and the validation set begins to plateauï¼Œ even though the train set will just keep getting better and betterã€‚

 Thank you for watching this video on transfer learningã€‚ This content changes oftenã€‚

 So subscribe to the channel to stay up to date on this course and other topics and artificial intelligenceã€‚



![](img/ca3b1efa82105f12ffd73e17655ac6cb_3.png)