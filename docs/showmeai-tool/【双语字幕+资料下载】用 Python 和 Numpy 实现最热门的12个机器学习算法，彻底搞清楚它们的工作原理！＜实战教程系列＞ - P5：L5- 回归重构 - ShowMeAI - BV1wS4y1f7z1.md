# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- å›å½’é‡æ„ - ShowMeAI - BV1wS4y1f7z1

Hi everybodyã€‚ welcomelcome to your new Python tutorialã€‚ And this videoã€‚

 I will simply refactor the code of the last two videosã€‚

 So if you haven't watched the previous two videos about linear regression and logistic regressionã€‚

 then please do soã€‚So now if you compare the codeï¼Œ then you will see that is almost similarã€‚

So both classes have exactly the same in it methodã€‚And both have almost the same fit methodã€‚

 So in both classesï¼Œ we in it our parametersï¼Œ and then we do the same gradient descentã€‚

 except that in the linear regressionï¼Œ we have simply this linear model for our approximated y and in logistic regressionã€‚

 we also have this linear modelã€‚ But then we also apply the sigmoid functionã€‚

And the same difference is in the predict methodã€‚ So in linear regressionã€‚

 we simply apply the linear modelã€‚ and in logistic regressionã€‚

 we apply the linear model and then the sigmoid functionã€‚ and then say if it's 1 or 0ã€‚

And we have this help a functionã€‚But a lot of this code is similarã€‚ So let's refactor thisã€‚ And yetã€‚

 let's create a base class and call it baseã€‚Regression and our two class classes will be derived from this base regressionã€‚

 So let's sayã€‚Linear regression derived from base regressionã€‚

And the same for our logistic regressionã€‚And then we can cut the init methodã€‚

And put it in the base regressionï¼Œ because this is the same for bothã€‚Soï¼Œ let's cut this hereã€‚

And then we also cut the fit methodã€‚So this is almost the sameã€‚ So we don't need this hereã€‚

And put it up hereã€‚å—¯ã€‚The only thing now that we have toã€‚That is different is the why predictedã€‚

 because one time we need simply the linear model and one time we need the linear model and the sigmoid functionã€‚

So let's call let's create a helper function and call it underscore approximationã€‚

 which will get our data Xï¼Œ or samplesï¼Œ and then it also gets the weights and the biasã€‚

 and in our base classï¼Œ we will raise a not implemented errorã€‚

 So this have has to be implemented in the derived classesã€‚ So now in the linear regressionã€‚å—¯ã€‚Classã€‚

 let's also create thisã€‚ And here we will implement thisã€‚ and in ourã€‚Linear regressionã€‚Modelã€‚

 this is simply the linear modelã€‚ so we can return thisã€‚

Return the do product of x and W plus the biasersã€‚And thenï¼Œ we also have toã€‚å—¯ã€‚OrWe can cutã€‚

The fit method from our logistic regression method and implement the approximation methodã€‚ So hereã€‚

 let's copy thisã€‚æ˜¯å“¦ã€‚So all let's remember that we need to have the linear model and then apply the sigoidoid functionã€‚

 So let's cut thisã€‚And let's create our underscoreã€‚Uã€‚Exximation method with selfã€‚Xï¼Œ Wï¼Œ and the biasã€‚

And then we create our linear modelï¼Œ which is nuyã€‚Dotã€‚Xã€‚Wã€‚Plusï¼Œ biasã€‚

And then we apply the sigmoid function and return itã€‚

 So let's return self dot underscore sigmoid of our linear modelã€‚So this is the approximationã€‚

 And now the predict method is a little bit differentï¼Œ soã€‚Let'sã€‚In our base classï¼Œ let'sã€‚

Toine this predict methodã€‚å—¯ã€‚And here we will implement aã€‚Help her functionï¼Œ and we call itã€‚Undercoreã€‚

 predictã€‚Which will get self X and also W and Bã€‚ And also in the base class hereã€‚

 we will simply raise a not implemented errorã€‚ So this now has to be implemented in the derived classesã€‚

 So we call and return this in ourã€‚Predict method in the base classã€‚

 So let's return self dot underscoreï¼Œ predictã€‚ and with the test samplesã€‚

 And then now with our calculated weights and the calculated biasã€‚

And now in our linear regression classï¼Œ we define this underscore predictï¼Œ which will get xã€‚

 W and biasã€‚Soï¼Œ and hereï¼Œ it will getã€‚So this will be the same codeã€‚Soã€‚The dot productã€‚

 dot product of x and w plus the biasã€‚ And we can return this in this lineã€‚ So we don't need thisã€‚

And then alsoï¼Œ in the logistic regressionï¼Œ weã€‚Deine this as underscoreï¼Œ predict with Xï¼Œ W and Bã€‚

 And then we have the same code hereã€‚ So weã€‚Use W and Bï¼Œ then apply the sigoid itã€‚

 Define if it's  one or 0 and return itã€‚And nowï¼Œ we canã€‚

Coopied this here and put it in the same file hereã€‚



![](img/8cc66a6687567bf6ad9be22465110a63_1.png)

And then we are doneã€‚ So now we have two modelsï¼Œ the linear regression and the logistic regressionã€‚

 just in 60 lines of Pythonã€‚ And it looks much cleaner nowã€‚ Soï¼Œ yeahï¼Œ that's itã€‚

 I hope you enjoyed this tutorial and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚



![](img/8cc66a6687567bf6ad9be22465110a63_3.png)

![](img/8cc66a6687567bf6ad9be22465110a63_4.png)