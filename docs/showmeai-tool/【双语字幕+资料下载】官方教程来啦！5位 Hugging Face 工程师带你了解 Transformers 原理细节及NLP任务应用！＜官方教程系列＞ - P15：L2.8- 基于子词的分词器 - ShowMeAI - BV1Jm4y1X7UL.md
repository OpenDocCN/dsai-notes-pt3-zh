# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL2.8- åŸºäºå­è¯çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL

Let's take a look at subward based tokenizationã€‚Understanding why sub word based tokenization is interesting requires understanding the flaws of word based and character based tokenizationã€‚

If you haven't seen the first videos on word based and character based tokenizationã€‚

 we recommend you check them out before looking at this videoã€‚

Subward based tokenization lies in between characteror based and word based tokenization algorithmsã€‚

The idea is to find a middle ground between very large vocabulariesã€‚

 a large quantity of out of vocabulary tokensï¼Œ and a loss of meaning across very similar wordsã€‚

 for word based tokens and very long sequences as well as less meaningful individual tokens for character based tokenizersã€‚

These algorithms rely on the following principleã€‚Frequently used words should not be splin into smaller subwordsã€‚

 while rare words should be decomposed into meaningful subsã€‚An example is the word dogã€‚

 we would like to have our tokenizer to have a single ID for the word dogã€‚

 rather than splitting it into characters DONGã€‚Howeverï¼Œ when encountering the word dogsã€‚

 we would like our tokenizer to understand that at the rootã€‚

 this is still the word dog with an added S that slightly changes the meaning while keeping the original ideaã€‚

Another example is a complex word like tokenizationï¼Œ which can be split into meaningful subwordsã€‚ğŸ˜Šã€‚

The root of the word is tokenï¼Œ and Iization completes the root to give it a slightly different meaningã€‚

ğŸ˜Šï¼ŒIt makes sense to split the word into two token as the root of the word labeled as the start of the word andization as additional information labeled as a completion of the wordã€‚

In turnï¼Œ the model will now be able to make sense of token in different situationsã€‚

It will understand that the words tokenï¼Œ tokensï¼Œ tokenizing and tokenizations have a similar meaning and are linkedã€‚

It will also understand that tokenizationï¼Œ modernizationï¼Œ and immunizationã€‚

 which all have the same suffixesï¼Œ are probably used in these same syntactic situationsã€‚

Subword based tokenrs generally have a way to identify which tokens are a start of word and which tokens complete start of wordssã€‚

So here token as the start of a word and hash hashization as completion of the wordã€‚

Here the hash hash prefix indicates that Iization is part of award word rather than the beginning of itã€‚

ğŸ˜Šï¼ŒThe hash hash comes from the bird tokenizer based on the word peace algorithmã€‚

Other tokens use other prefixes which can be placed to indicate part of words like in here or start of words insteadã€‚

There are a lot of different algorithms that can be used for subway tokenizationã€‚

 and most models obtaining state of the art results in English today use some kind of subward tokenization algorithmsã€‚

These approaches help in reducing the vocabulary sizes by sharing information across different wordsã€‚

 having the ability to have prefixes and suffixes understood as suchã€‚

They keep meaning across very similar words by recognizing similar tokensï¼Œ making them upã€‚



![](img/682dd3ed84761ab915a2a8c8a20c6be7_1.png)

![](img/682dd3ed84761ab915a2a8c8a20c6be7_2.png)

Yeahã€‚