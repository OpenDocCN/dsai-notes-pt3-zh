# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P4ï¼šL1.4- Transformeræ¶æ„ - ShowMeAI - BV1Jm4y1X7UL

Let's study the transformer architectureã€‚This video is the introductory video to the encodersã€‚ decoders and Ender decoder series of videosã€‚In the seriesã€‚ we'll try to understand what makes a transformer network and we'll try to explain it in simple high level termsã€‚No advanced understanding of neural networks is necessaryã€‚

 but an understanding of basic vectors and tensors may helpã€‚To get startedã€‚ we'll take up this diagram from the original transformformer paper entitledAttention is all you need by Pawa it upã€‚As we'll see hereï¼Œ we can leverage only some parts of it according to what we're trying to doã€‚We want to dive into the specific layersï¼Œ building up that architectureã€‚

 but we'll try to understand the different ways this architecture can be usedã€‚Let's first start by splitting that architecture into two parts on the leftã€‚ you have the encoder and on the rightï¼Œ the decoderã€‚These two can be used togetherã€‚ but they can also be used independentlyã€‚Let's understand how these workã€‚

The encoder accepts inputs that represent textï¼Œ it converts these textã€‚ these words into numerical representationsã€‚These numerical representations can also be called embedding or featuresã€‚We'll see that it uses the self attention mechanism as its main componentã€‚And we recommend you check out the video on encoders specifically to understand what is this numerical presentation as well as how it worksã€‚

ğŸ˜Šï¼ŒWe'll study the self attention mechanism in more detail as well as its bidirectional propertiesã€‚ğŸ˜Šã€‚The decoder is similar to the encodeï¼Œ it can also accept text inputsã€‚It uses a similar mechanism as the encoderï¼Œ which is the masked self attention as wellã€‚It differs from the encoder due to its unitite directional feature and is traditionally used in an autoregressive mannerã€‚

Here tooï¼Œ we recommend you check out the video on decodersã€‚ especially to understand how all of this worksã€‚Combining the two parts results in what is known as an anchor decoder or a sequence to sequence transformã€‚The encoder accepts inputs and computes a high level representation of those inputsã€‚These outputs are then passed to the decoderã€‚ The decoder uses the encoder's outputs alongside other inputs to generate a predictionã€‚

And then predict an outputï¼Œ which is will reuse in future iterationsï¼Œ hence the term autoregressiveã€‚Finallyï¼Œ to get an understanding of encoder decoders as a wholeã€‚ we recommend you check out the video on encoder decoderã€‚![](img/d08fc3568a2c2da170bab8fd128110ce_1.png)

![](img/d08fc3568a2c2da170bab8fd128110ce_2.png)

Yeahã€‚