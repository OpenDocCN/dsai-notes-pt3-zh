# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P8ÔºöL8- ÈÄªËæëÂõûÂΩí - ShowMeAI - BV12m4y1S7ix

HiÔºå everybody„ÄÇ Welcome back to a new Pytorch tutorial„ÄÇ This timeÔºå we implement logistic regression„ÄÇ

 If you have watched the previous tutorialsÔºå then this should be very easy Now„ÄÇ Once again„ÄÇ

 we implement our typical pytorch pipeline with those three steps„ÄÇ So firstÔºå we set up our model„ÄÇ

 We define the input and output size and the forward pass„ÄÇ

 Then we create the loss and the optimizer functions„ÄÇ

 and then we do the actual training loop with the forward pass„ÄÇ

 the backward pass and the weight updates„ÄÇüòäÔºåThe code here should be very similar to the code in the last tutorial where we implemented linear regression„ÄÇ

 We only have to make slight adjustments for the model and the loss function„ÄÇ

 So we add one more layer to our modelÔºå and we select a different loss function from Pytor built in functions„ÄÇ

SoÔºå let's start„ÄÇFirst of allÔºå let let's import some things that we need„ÄÇ So we import torch„ÄÇ

 of courseÔºå and we import torch dot nn as an n„ÄÇ So the neural network module„ÄÇ

 Then we import Ny S NP P to make some data transformations„ÄÇ Then from Sk learn„ÄÇ

 we import data sets to load a binary classification data set„ÄÇThen from SK learn dot pre processing„ÄÇ

 we want to import standard Scalar because we want to scale our features„ÄÇ

 and then from SK learn dot model selectionÔºå we import train test split because we want to have a separation of training and testing data„ÄÇ

And now let's do our three steps„ÄÇ So firstÔºå we want to set up the model„ÄÇ

 Then we want to set up the loss and the optimizer„ÄÇ And then in the third step„ÄÇ

 we do the actual training loop„ÄÇAnd as a step 0„ÄÇWe want to prepare the data„ÄÇSo let's do this„ÄÇ

 So let's load the breast cancer data set from S K learn so we can say B equals data sets dot load breast cancer„ÄÇ

 This is a binary classification problem where we can predict cancer based on the input features„ÄÇ

 So let's say X and Y equals B dot data and B dot target„ÄÇAnd then we want to sayÔºå ohÔºå first of all„ÄÇ

 let's the get the number of samples and the number of features by saying this is„ÄÇX dot shape„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_1.png)

So let's print this first So print the number of samples and the number of features to see how our data set looks like„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_3.png)

And„ÄÇWe see we have 569 samples and 30 different features„ÄÇ So a lot of features here„ÄÇ

And now let's continue„ÄÇ And let's split our data when we say X train and X test and„ÄÇ

X test and y train and y test equals here„ÄÇ We can use the train test split function where we put in x and y„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_5.png)

![](img/d4aae67709503eb865c057216c0046f3_6.png)

And we want to be the test„ÄÇSize„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_8.png)

To be 20%„ÄÇ So this is 02„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_10.png)

And let's also give this a random state equalsÔºå let's sayÔºå1Ôºå2Ôºå3Ôºå4„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_12.png)

And there should be a small S„ÄÇAnd now let's convert„ÄÇ orÔºå first of all„ÄÇ

 now we want to scale our features„ÄÇ Sc them Here„ÄÇ we set up a standard scalar S C equals standard scalar„ÄÇ

 which will„ÄÇMake our features to have zero mean and unit variance„ÄÇ

 This is always recommended to do when we want to deal with a logistic regression„ÄÇ

So now we scale our dataÔºå So we say x train equals sc dot fit transform„ÄÇ

 and then as an input we put in x train„ÄÇ and then we want to do the same thing with our test data„ÄÇ

 So we say x test equals SC dot here we only transform it„ÄÇÂóØ„ÄÇAnd here we put in X test„ÄÇ Now„ÄÇ

 we scaled our data„ÄÇ Now we want to convert it to torch tenzoos„ÄÇ So let's say x train„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_14.png)

Equals torch dot„ÄÇ And here we can use the function from nuy„ÄÇ

 And then we put an x train and cast this to a flow 32 data type„ÄÇ So we say x train dot S„ÄÇType„ÄÇ

Numpy dot float 32„ÄÇ because right nowÔºå this is of type double„ÄÇ

 and then we would run into some errorss later„ÄÇ So let's cast this and convert this to a tenor„ÄÇ

 And now let's do this with all the other arrays„ÄÇ So let's say x„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_16.png)

Test„ÄÇEquals„ÄÇThis„ÄÇAnd our Y train„ÄÇAnd alsoÔºå our y test tenzon„ÄÇW test„ÄÇAnd now„ÄÇ

 as the last thing to prepare our data is to reshape our why„ÄÇTenzo„ÄÇ So Y train equals Y train dot V„ÄÇ

 This is a built in function from Pytorarch that will reshape our Tzo with the given size„ÄÇ

 So it gets the size„ÄÇ Y train„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_18.png)

That shapeÔºå0„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_20.png)

And one„ÄÇ So right nowÔºå our y has only one rowÔºå and we want to make it a column vector„ÄÇ

 So we want to put each value in one row with only one column„ÄÇ So this will do exactly this„ÄÇ

 and also for our y test„ÄÇ So Y test equals this y test„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_22.png)

![](img/d4aae67709503eb865c057216c0046f3_23.png)

And nowÔºå we are„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_25.png)

Think we are done with our data preparing„ÄÇSo now let's set up our model„ÄÇ

 and here our model is a linear combination of weights and a bias„ÄÇ

 And then in the logistic regression caseÔºå we apply a sigmoid function at the end„ÄÇSo let's do this„ÄÇ

 And for thisÔºå we want to write our own class„ÄÇ So let's call this model„ÄÇ

 or we can also call this logistic regression„ÄÇChatic„ÄÇRegression„ÄÇ

And this must be derived from N and dot module„ÄÇAnd then this will get a in it„ÄÇWhich has self„ÄÇ

 And then it gets the number of input„ÄÇFeatures„ÄÇAnd hereÔºå firstÔºå we call the super in it„ÄÇ

 So let's say super„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_27.png)

Logistic regression and self dot in it„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_29.png)

And then here we define our layer„ÄÇ So we only have one layer self dot linear equals„ÄÇ

 And here we can use the build in layer N N dot linear„ÄÇ And this gets the input size„ÄÇ

 So an input features„ÄÇ and the output size is just one„ÄÇ So we only want to have one value„ÄÇ

 one class label at the end„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_31.png)

![](img/d4aae67709503eb865c057216c0046f3_32.png)

![](img/d4aae67709503eb865c057216c0046f3_33.png)

And then we also have to implement the forward pass hereÔºå which has self and the data„ÄÇ

 And our forward pass is firstÔºå we apply the linear layer and then the sigmoid function„ÄÇ

 So here we say why predict it„ÄÇEquals torch dot sitete„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_35.png)

So this is also a build and function that we can use„ÄÇ And here we apply ourselves to a linear layer„ÄÇ

 So linear layer with our data X„ÄÇ and then we return our y predicted„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_37.png)

![](img/d4aae67709503eb865c057216c0046f3_38.png)

So this is our model„ÄÇ And now let's create this„ÄÇ So model equals logistic regression of size„ÄÇ

 And here we put in the number of features that we have„ÄÇ So now our layer is of size 30 by  one„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_40.png)

![](img/d4aae67709503eb865c057216c0046f3_41.png)

And„ÄÇNoÔºå sorryÔºå30 input features and one output feature„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_43.png)

And now we have our model on„ÄÇAnd now we can continue with the loss and the optimizer„ÄÇ So for a loss„ÄÇ

 the loss function now is different than in the linear regression case„ÄÇ

 So here we say criterion equals NÔºå N dot BÔºå E loss„ÄÇ So the binary cross entropy loss here„ÄÇ

And our optimizer is the same„ÄÇ So this can be„ÄÇÂóØ„ÄÇThis is torch dot optim dot S G D for stochastic gradient descent„ÄÇ

 And this gets some parameters that we want to optimize„ÄÇ So here we just say model dot parameters„ÄÇ

 And it also needs a learning rate„ÄÇ So let's say our„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_45.png)

![](img/d4aae67709503eb865c057216c0046f3_46.png)

![](img/d4aae67709503eb865c057216c0046f3_47.png)

Learning rate equals „ÄÇ01„ÄÇ And then here we say L R equals learning rate„ÄÇ

 So now this is step 2 and now step 3„ÄÇ So let's define some number of epochs„ÄÇEqualsÔºå let's say„ÄÇ

 100 iterations„ÄÇ And now we do our training loop„ÄÇSo now we do four„ÄÇEpoch in range nu epochs„ÄÇ

 And then firstÔºå we do the forward pass forward„ÄÇPass and the loss calculation„ÄÇ

 Then we do the backward passÔºå and then we do the updates„ÄÇ

So let's say y predicted equals here we call our model and as thetaÔºå it gets x train„ÄÇ

 and then we say loss equals criterionÔºå and this will get a the y predicted and the actual y training„ÄÇ

 So the training samples or the training labels„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_49.png)

![](img/d4aae67709503eb865c057216c0046f3_50.png)

![](img/d4aae67709503eb865c057216c0046f3_51.png)

![](img/d4aae67709503eb865c057216c0046f3_52.png)

AndNow we do the backward pass and calculate the gradientsÔºå and again„ÄÇ

 we simply have to call lost dot dot backward and piytch will do all the calculations for us„ÄÇ

And now we update our weights„ÄÇ So here we simply have to say optimizer dot step„ÄÇ And again„ÄÇ

 Pytorch will do all the update calculations for us„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_54.png)

And then we don't„ÄÇ orÔºå we must not forget to empty our gradients againÔºå so„ÄÇWant to0 the gradients„ÄÇ

 because the backward function here will always add up all the gradients into the dot gra attribute„ÄÇ

 So let's empty them again before the next iteration„ÄÇ And we simply must say optimizer dot0 gra„ÄÇ

And thenÔºå let's also„ÄÇPrint some information If epoch plus  one modo 10 equals equals 0„ÄÇ

 So every 10th stepÔºå we want to print some information„ÄÇ Let's use an F string here„ÄÇ Let's sayÔºå epoch„ÄÇ

And here we can use epoch plus one„ÄÇ And then we also want to see the loss„ÄÇ

 So the loss equals loss dot item„ÄÇ And let's format this to sayÔºå to only print four decimal values„ÄÇ

And yeahÔºå so now we are done„ÄÇ This is our logistic regression implementation„ÄÇ

 And now let's evaluate our model„ÄÇ So the evaluation should not be part of our computational graph where we want to track the history„ÄÇ

 So we want to say with torch dot no Gr„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_56.png)

![](img/d4aae67709503eb865c057216c0046f3_57.png)

![](img/d4aae67709503eb865c057216c0046f3_58.png)

And then do our evaluation here„ÄÇ So here I want to get the accuracy„ÄÇ

 So let's get all the predicted classes from our test samples„ÄÇ So let's say this is model„ÄÇ

 And here we put an X test„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_60.png)

![](img/d4aae67709503eb865c057216c0046f3_61.png)

And then let's convert this to class labels„ÄÇ So 0 or 1„ÄÇ So remember„ÄÇ

 the seeoid function here will return a value between 0 and 1„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_63.png)

![](img/d4aae67709503eb865c057216c0046f3_64.png)

And„ÄÇIf this is larger than 05Ôºå we say this is class 1„ÄÇ and otherwise it's class 0„ÄÇ

 So let's say why predicted classes equals y predicted dot round„ÄÇ

 So here we can use a build and function again„ÄÇAnd this will do exactly this„ÄÇAnd yeahÔºå so if we„ÄÇ

Do don't use this statement„ÄÇ Then this would be part of the computational graph„ÄÇ

And it would track the gradient calculations for us„ÄÇ So here we don't want this„ÄÇ

 We don't need this because we are done„ÄÇ So that's why we used this with statement here„ÄÇ

 And now let's calculate the accuracy by saying act equals y predicted classes„ÄÇ

 And here we can call the equal function equals y test„ÄÇAnd then thes sum„ÄÇ So we want to sum„ÄÇ

Them up for every prediction that is correct„ÄÇ It will add„ÄÇPlus 1„ÄÇ

 And then we divide this by the number of samples of test samples„ÄÇ So why test dot shape„ÄÇ0„ÄÇ

This will return the number of test samplesÔºå and then let's print our accuracy print„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_66.png)

![](img/d4aae67709503eb865c057216c0046f3_67.png)

Accuracy equals„ÄÇÂóØ„ÄÇAc dot4 FÔºå also only for decimal values„ÄÇ

 And now let's run this and hope that everything is correct„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_69.png)

![](img/d4aae67709503eb865c057216c0046f3_70.png)

And„ÄÇStandard scalr has no attribute transform„ÄÇ So here I have a typo„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_72.png)

Transform„ÄÇNowÔºå let's run this again„ÄÇTrans„ÄÇForm„ÄÇOne more try„ÄÇAnd now we are done„ÄÇ

 and we have a accuracy of 089„ÄÇ So it's okayÔºå it's goodÔºå but it's not perfect„ÄÇ

 So you might want to play around withÔºå for exampleÔºå the number of iterations„ÄÇ

 and where do we have it„ÄÇ

![](img/d4aae67709503eb865c057216c0046f3_74.png)

![](img/d4aae67709503eb865c057216c0046f3_75.png)

![](img/d4aae67709503eb865c057216c0046f3_76.png)

The number of epochs or the learning rateÔºå for example„ÄÇ

 or also maybe try out a different optimizer here„ÄÇ but basically yeah that's how we implement logistic regression„ÄÇ

 I hope you liked it„ÄÇ If you liked itÔºå please subscribe to the channel and see you next time bye„ÄÇ



![](img/d4aae67709503eb865c057216c0046f3_78.png)