- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P25ï¼šL4.4- åå‘ä¼ æ’­ã€NesterovåŠ¨é‡å’ŒADAMè®­ç»ƒ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P25ï¼šL4.4- åå‘ä¼ æ’­ã€NesterovåŠ¨é‡å’ŒADAMè®­ç»ƒ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ Welcom to applications of deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ I am going to show you howã€‚ğŸ˜Šï¼ŒThe back propagation
    algorithm works internallyã€‚ We'll look at back propagationï¼Œ classicï¼Œ also atomã€‚
    Netroov momentum and other techniques that are commonly used to train deep neural
    networks for the latest on among AI course and projectsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯Jeff Heatonã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºå†…éƒ¨å·¥ä½œçš„åå‘ä¼ æ’­ç®—æ³•ã€‚æˆ‘ä»¬å°†çœ‹çœ‹ç»å…¸çš„åå‘ä¼ æ’­ï¼Œè¿˜æœ‰åŸå­ï¼ŒNetroovåŠ¨é‡å’Œå…¶ä»–å¸¸ç”¨çš„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„æŠ€æœ¯ï¼Œè¿™æ˜¯AIè¯¾ç¨‹å’Œé¡¹ç›®ä¸­æœ€æ–°çš„å†…å®¹ã€‚
- en: Click subscribe in the bell next to it to be notified of every new videoã€‚ So
    classic back propagationã€‚ Back propagation has been around for a whileã€‚ It is
    Jeffrey Henton contributed quite a bit to itã€‚ and werebo as wellã€‚ So a variety
    of people introduced aspects of back propagationã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»æ—è¾¹çš„é“ƒé“›è®¢é˜…ï¼Œä»¥æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚å¦‚æ­¤ç»å…¸çš„åå‘ä¼ æ’­ã€‚åå‘ä¼ æ’­å·²ç»å­˜åœ¨äº†ä¸€æ®µæ—¶é—´ã€‚Jeffrey Hentonå¯¹æ­¤åšå‡ºäº†ç›¸å½“å¤§çš„è´¡çŒ®ã€‚ä»¥åŠwereboä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼Œè®¸å¤šäººä»‹ç»äº†åå‘ä¼ æ’­çš„å„ä¸ªæ–¹é¢ã€‚
- en: and has been continued to be built upon over many yearsã€‚ This equation that
    you see here is sort of your very general training equationã€‚ This just says that
    Tã€‚ Now T is the current epoch or timeã€‚ This is saying that the weightsã€‚ which
    are the weights of thetaã€‚ The weights for the current time are equal to the weights
    from the previous time minus V the current timeã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å¤šå¹´æ¥ä¸€ç›´åœ¨ç»§ç»­å»ºç«‹ã€‚ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„è¿™ä¸ªæ–¹ç¨‹æ˜¯ä½ éå¸¸ä¸€èˆ¬çš„è®­ç»ƒæ–¹ç¨‹ã€‚è¿™åªæ˜¯è¯´Tã€‚ç°åœ¨Tæ˜¯å½“å‰çš„æ—¶æœŸæˆ–æ—¶é—´ã€‚è¿™æ˜¯åœ¨è¯´æƒé‡ã€‚è¿™äº›æ˜¯thetaçš„æƒé‡ã€‚å½“å‰æ—¶é—´çš„æƒé‡ç­‰äºå‰ä¸€æ—¶é—´çš„æƒé‡å‡å»å½“å‰æ—¶é—´çš„Vã€‚
- en: Vã€‚ğŸ˜Šã€‚![](img/6491afaa3cf5a798f8c7f93f7b8bf376_1.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Vã€‚ğŸ˜Šã€‚![](img/6491afaa3cf5a798f8c7f93f7b8bf376_1.png)
- en: From the current time is just a vectorã€‚ All of these are vectors that holds
    the amounts that we're going to change each of the weights byã€‚ So this doesn't
    tell us a whole lot by itselfã€‚ It just says that we are going to change the weightsã€‚
    And each time by this vector of change amountsã€‚ Nowï¼Œ the vector of change amountsã€‚
    V sub T will see a variety of functions that show us how to calculate v sub Tã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰æ—¶é—´åªæ˜¯ä¸€ä¸ªå‘é‡ã€‚æ‰€æœ‰è¿™äº›éƒ½æ˜¯å‘é‡ï¼Œå®ƒä»¬ä¿æŒäº†æˆ‘ä»¬å°†æ”¹å˜æ¯ä¸ªæƒé‡çš„æ•°é‡ã€‚æ‰€ä»¥å•ç‹¬çš„è¿™ä¸ªå¹¶ä¸èƒ½å‘Šè¯‰æˆ‘ä»¬å¤ªå¤šã€‚å®ƒåªæ˜¯è¯´æˆ‘ä»¬å°†æ”¹å˜æƒé‡ã€‚æ¯æ¬¡éƒ½æ˜¯é€šè¿‡è¿™ä¸ªå˜åŒ–é‡çš„å‘é‡ã€‚ç°åœ¨ï¼Œå˜åŒ–é‡çš„å‘é‡ã€‚Vå­Tå°†ä¼šçœ‹åˆ°ä¸€ç³»åˆ—çš„å‡½æ•°ï¼Œæ˜¾ç¤ºæˆ‘ä»¬å¦‚ä½•è®¡ç®—Vå­Tã€‚
- en: The first is classic back propagationã€‚ So if we look at thisï¼Œ this is gradient
    descentã€‚ So you have Eta multiplied by nowï¼Œ Eta is multiplied by the rest of thisã€‚
    This is essentially one unitã€‚ This is notã€‚This thing multiplied by the J functionã€‚This
    is the Nabla or the upside down Delta or the harp shaped operatorã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯ç»å…¸çš„åå‘ä¼ æ’­ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬çœ‹è¿™ä¸ªï¼Œè¿™æ˜¯æ¢¯åº¦ä¸‹é™ã€‚æ‰€ä»¥ä½ æœ‰Etaä¹˜ä»¥ç°åœ¨ï¼ŒEtaä¹˜ä»¥å…¶ä½™çš„éƒ¨åˆ†ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå•å…ƒã€‚è¿™ä¸æ˜¯ã€‚è¿™ä¸ªä¸œè¥¿ä¹˜ä»¥Jå‡½æ•°ã€‚è¿™æ˜¯Nablaæˆ–å€’ç½®çš„Deltaæˆ–è€…åƒç«–ç´å½¢çš„è¿ç®—ç¬¦ã€‚
- en: This says take the gradients of the loss function with respect to the weights
    from the previous time step or the previous epochã€‚ So this is essentially giving
    you all of the gradientsã€‚Multiplied by the learning rate at a common values for
    the learning rate are these 0ã€‚10ã€‚01ã€‚ rarely Would you ever want to make that one
    so that you're fully adding the gradients to the weights that would that would
    simply be too chaoticã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ç”¨æŸå¤±å‡½æ•°çš„æ¢¯åº¦å¯¹å…ˆå‰æ—¶é—´æ­¥æˆ–å…ˆå‰æ—¶æœŸçš„æƒé‡è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æ‰€ä»¥è¿™åŸºæœ¬ä¸Šç»™å‡ºäº†æ‰€æœ‰çš„æ¢¯åº¦ã€‚ä¹˜ä»¥å­¦ä¹ ç‡ï¼Œå­¦ä¹ ç‡çš„å¸¸è§å€¼æ˜¯0.10.01ã€‚å¾ˆå°‘ä½ ä¼šæƒ³è¦ä½¿é‚£ä¸€ä¸ªï¼Œè¿™æ ·ä½ å°±å®Œå…¨å°†æ¢¯åº¦æ·»åŠ åˆ°æƒé‡ä¸­ï¼Œé‚£å°†ä¼šæ˜¯å¤ªæ··ä¹±çš„ã€‚
- en: Nowï¼Œ let's see really what the gradient is and how it's actually usedã€‚ This
    is a derivativeã€‚ It's a partial derivativeï¼Œ So you always take the partial derivative
    of one multivariate calculusã€‚ you take the derivative of one single variable and
    a multivariable expressionã€‚ So one single weight with all other weights held constantã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ¢¯åº¦åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒæ˜¯å¦‚ä½•å®é™…ä½¿ç”¨çš„ã€‚è¿™æ˜¯ä¸€ä¸ªå¯¼æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªåå¯¼æ•°ï¼Œæ‰€ä»¥ä½ æ€»æ˜¯å–ä¸€ä¸ªå¤šå…ƒå¾®ç§¯åˆ†çš„åå¯¼æ•°ã€‚ä½ ç”¨æ‰€æœ‰å…¶ä»–æƒé‡ä¿æŒä¸å˜çš„æƒ…å†µä¸‹å–ä¸€ä¸ªå•ä¸€å˜é‡çš„å¯¼æ•°å’Œä¸€ä¸ªå¤šå˜é‡è¡¨è¾¾å¼ã€‚å› æ­¤ï¼Œä¸€ä¸ªå•ä¸€çš„æƒé‡ä¸æ‰€æœ‰å…¶ä»–æƒé‡ä¿æŒä¸å˜ã€‚
- en: So this shows you essentially the error function for a particular weightã€‚ So
    the error for this weight as you adjust the weightã€‚ So if you make the weight0ã€‚
    the error is going to be hereï¼Œ then it goes up and then it swings way downã€‚ This
    is potentially true as you vary just one weight in the neural networkã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™åŸºæœ¬ä¸Šå±•ç¤ºäº†ç‰¹å®šæƒé‡çš„è¯¯å·®å‡½æ•°ã€‚å½“ä½ è°ƒæ•´æƒé‡æ—¶ï¼Œè¿™ä¸ªæƒé‡çš„è¯¯å·®ã€‚æ‰€ä»¥å¦‚æœä½ ä½¿æƒé‡0ï¼Œè¯¯å·®å°±ä¼šåœ¨è¿™é‡Œï¼Œç„¶åä¸Šå‡ï¼Œç„¶åæ€¥å‰§ä¸‹é™ã€‚è¿™åœ¨ä½ ä»…æ”¹å˜ç¥ç»ç½‘ç»œä¸­çš„ä¸€ä¸ªæƒé‡æ—¶å¯èƒ½æ˜¯çœŸå®çš„ã€‚
- en: the error function will go up down and change its valueã€‚ Typicallyï¼Œ we're doing
    gradientã€‚So we want to get the weight all the way to the lowest point hereã€‚ Nowï¼Œ
    we don't want to graphã€‚ generate this entire graph and and sample the neural network
    at each of these pointsã€‚ that would be computationally expensive because there
    is a different chart of this for every single weight in the entire neural networkã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é”™è¯¯å‡½æ•°ä¼šä¸Šä¸‹å˜åŒ–å…¶å€¼ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬æ­£åœ¨åšæ¢¯åº¦ã€‚æ‰€ä»¥æˆ‘ä»¬æƒ³è¦æŠŠæƒé‡å®Œå…¨ç§»åˆ°è¿™é‡Œçš„æœ€ä½ç‚¹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä¸æƒ³è¦ç»˜åˆ¶è¿™ä¸ªæ•´ä¸ªå›¾è¡¨å¹¶ä¸”åœ¨æ¯ä¸€ä¸ªç‚¹ä¸Šå¯¹ç¥ç»ç½‘ç»œè¿›è¡ŒæŠ½æ ·ã€‚å› ä¸ºæ•´ä¸ªç¥ç»ç½‘ç»œä¸­æ¯ä¸€ä¸ªæƒé‡éƒ½æœ‰ä¸åŒçš„å›¾è¡¨ï¼Œè¿™ä¼šéå¸¸æ˜‚è´µã€‚
- en: And as you change one weightï¼Œ all all the others potentially change2ã€‚ This is
    where you have to do the partial derivativeã€‚ We're doing the partial derivative
    for just one weightã€‚ sayy the weight was currently at 1ã€‚5ã€‚ So all we would know
    is that the error was hereã€‚ we don't have the rest of the lead up or continuation
    of the chartã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ æ”¹å˜ä¸€ä¸ªæƒé‡æ—¶ï¼Œæ‰€æœ‰å…¶ä»–çš„æƒé‡ä¹Ÿå¯èƒ½æ”¹å˜ã€‚è¿™å°±æ˜¯ä½ å¿…é¡»è¿›è¡Œåå¯¼æ•°çš„åœ°æ–¹ã€‚æˆ‘ä»¬åªä¸ºä¸€ä¸ªæƒé‡åšåå¯¼æ•°ã€‚å‡è®¾æƒé‡ç›®å‰ä¸º1.5ã€‚æ‰€ä»¥æˆ‘ä»¬åªçŸ¥é“è¯¯å·®åœ¨è¿™é‡Œã€‚æˆ‘ä»¬ä¸çŸ¥é“å›¾è¡¨çš„å…¶ä½™éƒ¨åˆ†æˆ–ç»§ç»­ã€‚
- en: we just have this one pointï¼Œ that point doesn't tell us too much until we take
    the derivative of the loss functionã€‚ then that tells us the instantaneous rate
    of changeã€‚ So youve got the slopeã€‚ this point right here on the error function
    curveã€‚ Notice this line has a negative slopeã€‚ But if we if we just added that
    gradient to itï¼Œ the negative valueï¼Œ whatever that negative slope isã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæœ‰è¿™ä¸€ä¸ªç‚¹ï¼Œé‚£ä¸ªç‚¹ä¸å‘Šè¯‰æˆ‘ä»¬å¤ªå¤šï¼Œç›´åˆ°æˆ‘ä»¬å¯¹æŸå¤±å‡½æ•°å–å¯¼æ•°ã€‚ç„¶åé‚£å‘Šè¯‰æˆ‘ä»¬ç¬æ—¶å˜åŒ–ç‡ã€‚æ‰€ä»¥ä½ å¾—åˆ°äº†æ–œç‡ã€‚è¿™ä¸ªé”™è¯¯å‡½æ•°æ›²çº¿ä¸Šçš„è¿™ä¸ªç‚¹ã€‚æ³¨æ„è¿™æ¡çº¿æœ‰ä¸€ä¸ªè´Ÿæ–œç‡ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬åªæ˜¯æŠŠæ¢¯åº¦åŠ ä¸Šå»ï¼Œè¿™ä¸ªè´Ÿå€¼ï¼Œæ— è®ºé‚£ä¸ªè´Ÿæ–œç‡æ˜¯å¤šå°‘ã€‚
- en: that would decrease the weightã€‚ It would go in the wrong directionã€‚ So if if
    you truly want to go this directionï¼Œ because were past just a little bit of the
    crest of that hillã€‚ if you want to go in this directionï¼Œ you need to take the
    opposite of the slopeã€‚ That is why up here we'reã€‚Ting the sub T because if thisï¼Œ
    I meanï¼Œ say we're right hereã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£å°†å‡å°æƒé‡ã€‚å®ƒä¼šæœé”™è¯¯çš„æ–¹å‘èµ°ã€‚æ‰€ä»¥å¦‚æœä½ çœŸçš„æƒ³è¦èµ°è¿™ä¸ªæ–¹å‘ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è¿‡äº†é‚£ä¸ªå±±ä¸˜çš„é¡¶éƒ¨ä¸€ç‚¹ç‚¹ã€‚å¦‚æœä½ æƒ³æœè¿™ä¸ªæ–¹å‘èµ°ï¼Œä½ éœ€è¦å–æ–œç‡çš„åæ–¹å‘ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨è¿™é‡Œæˆ‘ä»¬è¿›è¡Œäº†å­Tï¼Œå› ä¸ºå¦‚æœè¿™æ ·ï¼Œæˆ‘æ˜¯è¯´ï¼Œå‡è®¾æˆ‘ä»¬å°±åœ¨è¿™é‡Œã€‚
- en: then the line would be very much this this directionã€‚ and that would be a very
    negatively sloped lineã€‚ you would want a positive numberã€‚ So you continued on
    your way downã€‚ If we were right hereï¼Œ then the slope would be positiveã€‚ but we
    would want to subtract one from the weight to continue along this directionã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™æ¡çº¿çš„æ–¹å‘ä¼šéå¸¸ç±»ä¼¼è¿™ä¸ªæ–¹å‘ã€‚å¹¶ä¸”é‚£å°†æ˜¯ä¸€æ¡éå¸¸è´Ÿæ–œç‡çš„çº¿ã€‚ä½ ä¼šæƒ³è¦ä¸€ä¸ªæ­£æ•°ã€‚æ‰€ä»¥ä½ ç»§ç»­èµ°ä¸‹å»ã€‚å¦‚æœæˆ‘ä»¬å°±åœ¨è¿™é‡Œï¼Œé‚£ä¹ˆæ–œç‡å°†æ˜¯æ­£çš„ã€‚ä½†æˆ‘ä»¬å¸Œæœ›ä»æƒé‡ä¸­å‡å»ä¸€ä¸ªæ¥ç»§ç»­æ²¿ç€è¿™ä¸ªæ–¹å‘ã€‚
- en: So that is classicã€‚Back propagationã€‚ It's governed by the learning rateã€‚ If
    you made your learning rate too big instead set a gradient dissenting down hereã€‚
    you'd probably jump completely to the other side of itã€‚ and you would never find
    your way down to this lower valueã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç»å…¸çš„åå‘ä¼ æ’­ã€‚å®ƒç”±å­¦ä¹ ç‡æ§åˆ¶ã€‚å¦‚æœä½ çš„å­¦ä¹ ç‡è®¾ç½®å¾—å¤ªå¤§ï¼Œè€Œä¸æ˜¯è®¾ç½®ä¸€ä¸ªæ¢¯åº¦é™ä¸‹æ¥ï¼Œä½ å¯èƒ½å®Œå…¨è·³åˆ°å®ƒçš„å¦ä¸€ä¾§ã€‚ä½ æ°¸è¿œæ‰¾ä¸åˆ°é€šå‘è¿™ä¸ªæ›´ä½å€¼çš„è·¯ã€‚
- en: So learning rate just describes how quickly we're going to attempt to push the
    weights to optimal valuesã€‚ And this link is pretty handyã€‚ It shows a javascript
    application that I wrote that takes you through all the steps of a classic back
    propagationã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡æè¿°äº†æˆ‘ä»¬å°è¯•å°†æƒé‡æ¨å‘æœ€ä¼˜å€¼çš„é€Ÿåº¦ã€‚è¿™ä¸ªé“¾æ¥éå¸¸æœ‰ç”¨ã€‚å®ƒå±•ç¤ºäº†ä¸€ä¸ªæˆ‘å†™çš„JavaScriptåº”ç”¨ç¨‹åºï¼Œå¸¦ä½ ç»å†ç»å…¸åå‘ä¼ æ’­çš„æ‰€æœ‰æ­¥éª¤ã€‚
- en: So you can see literally how an entire neural network is calculated for X O
    Rã€‚ Next is momentum propagationã€‚ back propagationã€‚ So momentum is something that
    was added to back propagation to prevent from being from settling into a local
    minimaã€‚ So local minima could be right hereã€‚ It might be that further over hereã€‚
    there would be an even more optimal valueã€‚ But once the weight gets settled into
    hereã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ å¯ä»¥çœ‹åˆ°æ•´ä¸ªç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•ä¸ºXORè®¡ç®—çš„ã€‚æ¥ä¸‹æ¥æ˜¯åŠ¨é‡ä¼ æ’­ã€‚åå‘ä¼ æ’­ã€‚æ‰€ä»¥åŠ¨é‡æ˜¯æ·»åŠ åˆ°åå‘ä¼ æ’­ä¸­çš„ä¸œè¥¿ï¼Œä»¥é˜²æ­¢é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚å±€éƒ¨æœ€å°å€¼å¯èƒ½åœ¨è¿™é‡Œã€‚å¯èƒ½åœ¨è¿™è¾¹æ›´è¿›ä¸€æ­¥ä¼šæœ‰æ›´ä¼˜çš„å€¼ã€‚ä½†ä¸€æ—¦æƒé‡å®šå±…åˆ°è¿™é‡Œã€‚
- en: It's really hard for it to push its way completely out of that valley and continue
    onã€‚ã„ã«ï¼Ÿ
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçœŸçš„å¾ˆéš¾å®Œå…¨æ¨å‡ºé‚£ä¸ªå±±è°·å¹¶ç»§ç»­å‰è¡Œã€‚
- en: We have the case hereã€‚ The weightï¼Œ which is where that ball currently is atã€‚
    is essentially stuck in a local minimumã€‚ There might be a global minimum hereã€‚
    It's really hard to know where the global minimum isã€‚ It's usually almost impossibleã€‚
    So this weight would have been continuing downï¼Œ down downï¼Œ but it would potentially
    get stuck hereã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ç§æƒ…å†µã€‚æƒé‡ï¼Œä¹Ÿå°±æ˜¯çƒå½“å‰çš„ä½ç½®ï¼Œå®é™…ä¸Šå¡åœ¨äº†ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼ã€‚è¿™é‡Œå¯èƒ½æœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼ã€‚å¾ˆéš¾çŸ¥é“å…¨å±€æœ€å°å€¼åœ¨å“ªé‡Œã€‚è¿™é€šå¸¸å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚å› æ­¤ï¼Œè¿™ä¸ªæƒé‡æœ¬åº”è¯¥ä¸€ç›´ä¸‹é™ï¼Œä½†å¯èƒ½ä¼šå¡åœ¨è¿™é‡Œã€‚
- en: If not for the momentum that pushes it over the hump and allows it to continueã€‚
    Moment just as its name impliesã€‚ You can think of these weights as moving through
    high dimension spaceã€‚ Moment just gives the gives the weight push and continues
    that push as it maintains its momentumã€‚ This is the formula for momentumã€‚ Now
    we have two hyperparametersã€‚ We have Etaã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸æ˜¯åŠ¨é‡æ¨åŠ¨å®ƒè¶Šè¿‡è¿™ä¸ªå±±å³°å¹¶è®©å®ƒç»§ç»­ã€‚åŠ¨é‡æ­£å¦‚å…¶åã€‚ä½ å¯ä»¥æŠŠè¿™äº›æƒé‡æƒ³è±¡æˆåœ¨é«˜ç»´ç©ºé—´ä¸­ç§»åŠ¨ã€‚åŠ¨é‡åªæ˜¯ç»™æƒé‡ä¸€ä¸ªæ¨åŠ¨ï¼Œå¹¶åœ¨å®ƒä¿æŒåŠ¨é‡çš„åŒæ—¶æŒç»­è¿™ç§æ¨åŠ¨ã€‚è¿™æ˜¯åŠ¨é‡çš„å…¬å¼ã€‚ç°åœ¨æˆ‘ä»¬æœ‰ä¸¤ä¸ªè¶…å‚æ•°ã€‚æˆ‘ä»¬æœ‰Etaã€‚
- en: which is the learning rateã€‚ but we also have lambdaï¼Œ which is the momentum rateã€‚
    The first part of this is completely the same as classic back propagationã€‚ You
    are simply taking the gradientã€‚Multiply by the learning rateã€‚ But you have this
    additional term hereã€‚ And this is the momentum termã€‚ This is lambda times V t
    -1ã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡æ˜¯**å­¦ä¹ ç‡**ã€‚ä½†æˆ‘ä»¬è¿˜æœ‰lambdaï¼Œå®ƒæ˜¯**åŠ¨é‡ç‡**ã€‚è¿™ä¸€éƒ¨åˆ†ä¸ç»å…¸çš„åå‘ä¼ æ’­å®Œå…¨ç›¸åŒã€‚ä½ åªéœ€è®¡ç®—æ¢¯åº¦ã€‚ä¹˜ä»¥å­¦ä¹ ç‡ã€‚ä½†è¿™é‡Œæœ‰ä¸€ä¸ªé¢å¤–çš„é¡¹ã€‚è¿™å°±æ˜¯åŠ¨é‡é¡¹ã€‚å®ƒæ˜¯lambdaä¹˜ä»¥V
    t -1ã€‚
- en: So whatever our previous delta our previous update wasï¼Œ we're scaling it by
    lambda and adding itã€‚ You're just adding the last update scaled right onto the
    equation with everything elseã€‚ That's really all that momentum isã€‚ So as you were
    moving down thisã€‚ you would have built up a lot of momentum because you would
    potentially be moving down fairly fastã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ— è®ºæˆ‘ä»¬ä¹‹å‰çš„deltaæˆ–æ›´æ–°æ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬éƒ½åœ¨ç”¨lambdaè¿›è¡Œç¼©æ”¾å¹¶æ·»åŠ å®ƒã€‚ä½ åªéœ€å°†æœ€åçš„æ›´æ–°ç¼©æ”¾åç›´æ¥åŠ åˆ°æ–¹ç¨‹ä¸­ä¸å…¶ä»–å†…å®¹ä¸€èµ·ã€‚è¿™å°±æ˜¯åŠ¨é‡çš„å…¨éƒ¨ã€‚å› æ­¤ï¼Œå½“ä½ å‘ä¸‹ç§»åŠ¨æ—¶ï¼Œä½ ä¼šç§¯ç´¯å¾ˆå¤šåŠ¨é‡ï¼Œå› ä¸ºä½ å¯èƒ½ä¼šæ¯”è¾ƒå¿«åœ°å‘ä¸‹ç§»åŠ¨ã€‚
- en: Then that change is very much a positive weight that would keep getting added
    on the weight and hopefully push it over the hump and on to possibly out of the
    local minima and onto a better situationã€‚ hopefullyï¼Œ by the wayï¼Œ very common value
    for momentum is 0ã€‚9ã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™ä¸ªå˜åŒ–å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä¸€ä¸ªæ­£çš„æƒé‡ï¼Œåº”è¯¥ä¸æ–­æ·»åŠ åˆ°æƒé‡ä¸Šï¼Œå¸Œæœ›èƒ½æŠŠå®ƒæ¨è¿‡å±±å³°ï¼Œå¯èƒ½ä»å±€éƒ¨æœ€å°å€¼ä¸­èµ°å‡ºæ¥ï¼Œè¿›å…¥æ›´å¥½çš„çŠ¶æ€ã€‚é¡ºä¾¿æä¸€ä¸‹ï¼ŒåŠ¨é‡çš„ä¸€ä¸ªéå¸¸å¸¸è§çš„å€¼æ˜¯0.9ã€‚
- en: They usually favor a fair amount of momentumã€‚ learning rate is often much smallerã€‚
    It's usually 0ã€‚10ã€‚01 or some other negative power of 10ã€‚ Nextï¼Œ we're going to
    look atã€‚And online batch propagation so this is an important concept for propagation
    trainingã€‚ We'll see later that we can configure these values in TensorF and determine
    what the batch sizes will be batchch is simply how many training set elements
    do you need to go through so each of these each of these gradients that we're
    calculating is for a single train set elementã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬é€šå¸¸åå‘äºè¾ƒå¤§çš„åŠ¨é‡ã€‚å­¦ä¹ ç‡é€šå¸¸è¦å°å¾—å¤šã€‚é€šå¸¸æ˜¯0.10ã€0.01æˆ–å…¶ä»–è´Ÿçš„10çš„å¹‚ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å…³æ³¨åœ¨çº¿æ‰¹é‡ä¼ æ’­ï¼Œè¿™æ˜¯ä¼ æ’­è®­ç»ƒçš„é‡è¦æ¦‚å¿µã€‚ç¨åæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨TensorFä¸­é…ç½®è¿™äº›å€¼ï¼Œå¹¶ç¡®å®šæ‰¹é‡å¤§å°ã€‚æ‰¹é‡å¤§å°å°±æ˜¯ä½ éœ€è¦å¤„ç†å¤šå°‘ä¸ªè®­ç»ƒé›†å…ƒç´ ï¼Œå› æ­¤æˆ‘ä»¬è®¡ç®—çš„æ¯ä¸€ä¸ªæ¢¯åº¦éƒ½æ˜¯é’ˆå¯¹å•ä¸ªè®­ç»ƒé›†å…ƒç´ çš„ã€‚
- en: So you might have1 thousand elements in your training setã€‚ How many of those
    you don't have to literally update the weights every single time you calculate
    a training row and get the deltas to the weightã€‚ you can batch those up and you
    batch them up simply by summing up the gradients So you process the first row
    of training training data and you get a vector gradients equal to or changes the
    V equal to the size of the weights and you just you can calculate the next training
    row and you add those gradients onto whatever read had beforeã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯èƒ½åœ¨è®­ç»ƒé›†ä¸­æœ‰1000ä¸ªå…ƒç´ ã€‚åœ¨ä½ è®¡ç®—æ¯ä¸€è¡Œè®­ç»ƒæ•°æ®å¹¶å¾—åˆ°æƒé‡çš„deltasæ—¶ï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½æ›´æ–°æƒé‡ã€‚ä½ å¯ä»¥å°†å®ƒä»¬æ‰¹é‡å¤„ç†ï¼Œæ–¹æ³•æ˜¯ç®€å•åœ°å°†æ¢¯åº¦ç›¸åŠ ã€‚å› æ­¤ï¼Œä½ å¤„ç†è®­ç»ƒæ•°æ®çš„ç¬¬ä¸€è¡Œï¼Œç„¶åå¾—åˆ°ä¸€ä¸ªæ¢¯åº¦å‘é‡ï¼Œå®ƒç­‰äºæƒé‡çš„å¤§å°ï¼Œä½ å¯ä»¥è®¡ç®—ä¸‹ä¸€è¡Œè®­ç»ƒæ•°æ®ï¼Œå¹¶å°†è¿™äº›æ¢¯åº¦æ·»åŠ åˆ°ä¹‹å‰çš„å€¼ä¸Šã€‚
- en: you keep vector adding the gradients until you've made it to the batch sizeã€‚
    So a batch sizeã€‚ if you had a batch size of 10ï¼Œ that means as it's going through
    the training setã€‚ itll make it through 10 elementsï¼Œ and then at the end of the
    10 elementsã€‚ it has the gradients that are basically the sum of that whole runã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¸æ–­åœ°å‘æ¢¯åº¦ä¸­æ·»åŠ å‘é‡ï¼Œç›´åˆ°è¾¾åˆ°æ‰¹é‡å¤§å°ã€‚æ‰€ä»¥æ‰¹é‡å¤§å°ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªæ‰¹é‡å¤§å°ä¸º10ï¼Œè¿™æ„å‘³ç€åœ¨å¤„ç†è®­ç»ƒé›†æ—¶ï¼Œå®ƒå°†å¤„ç†10ä¸ªå…ƒç´ ï¼Œç„¶ååœ¨è¿™10ä¸ªå…ƒç´ ç»“æŸæ—¶ï¼Œå®ƒçš„æ¢¯åº¦åŸºæœ¬ä¸Šæ˜¯æ•´æ¬¡è¿è¡Œçš„æ€»å’Œã€‚
- en: and then it will apply the changes to the weightsã€‚ Online training simply applies
    the change to the weight as soonã€‚You calculate the gradient just one at a timeï¼Œ
    calculate a gradient for one training rowã€‚ apply it to the weightsï¼Œ move on to
    the next training rowï¼Œ calculate its gradientsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒä¼šå°†æ›´æ”¹åº”ç”¨åˆ°æƒé‡ä¸Šã€‚åœ¨çº¿è®­ç»ƒä»…åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œå°†æ›´æ”¹å°½å¿«åº”ç”¨åˆ°æƒé‡ä¸Šï¼Œä½ ä¸€æ¬¡åªè®¡ç®—ä¸€ä¸ªæ¢¯åº¦ï¼Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬è®¡ç®—æ¢¯åº¦ã€‚å°†å…¶åº”ç”¨åˆ°æƒé‡ä¸­ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè®¡ç®—å®ƒçš„æ¢¯åº¦ã€‚
- en: add it to the weights and continue Having the batch sizes this can provide considerable
    efficiency to the training of the neural network this is also very big data compliant
    because if you've got a veryã€‚ very large data set you just need to randomly sample
    many batches from it so many batch training that's another very common technique
    for training neural networks many batches are typically between 32 and 64 in size
    so they're relatively small step and iteration that is just how many training
    cycles has has the neural network gone through step iteration or even and then
    all right now we'll look at stochastic gradient descent which is often used in
    conjunction with many batch training stochastic gradient descent is used to it
    provides a very stochastic orã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å…¶æ·»åŠ åˆ°æƒé‡ä¸­å¹¶ç»§ç»­è¿›è¡Œã€‚æ‹¥æœ‰æ‰¹é‡å¤§å°å¯ä»¥æ˜¾è‘—æé«˜ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡ï¼Œè¿™ä¹Ÿæ˜¯éå¸¸ç¬¦åˆå¤§æ•°æ®çš„ï¼Œå› ä¸ºå¦‚æœä½ æœ‰ä¸€ä¸ªéå¸¸ã€éå¸¸å¤§çš„æ•°æ®é›†ï¼Œä½ åªéœ€ä»ä¸­éšæœºæŠ½å–è®¸å¤šæ‰¹æ¬¡ã€‚å› æ­¤ï¼Œè®¸å¤šæ‰¹é‡è®­ç»ƒæ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„å¦ä¸€ç§éå¸¸å¸¸è§çš„æŠ€æœ¯ï¼Œè®¸å¤šæ‰¹é‡çš„å¤§å°é€šå¸¸åœ¨32åˆ°64ä¹‹é—´ï¼Œå› æ­¤å®ƒä»¬ç›¸å¯¹è¾ƒå°ã€‚æ­¥é•¿å’Œè¿­ä»£å°±æ˜¯ç¥ç»ç½‘ç»œç»å†äº†å¤šå°‘è®­ç»ƒå‘¨æœŸï¼Œæ­¥é•¿è¿­ä»£ç”šè‡³æ˜¯ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹éšæœºæ¢¯åº¦ä¸‹é™ï¼Œå®ƒé€šå¸¸ä¸è®¸å¤šæ‰¹é‡è®­ç»ƒç»“åˆä½¿ç”¨ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ç”¨äºæä¾›éå¸¸éšæœºçš„ç»“æœã€‚
- en: scentWhat's happening is rather than calculating the gradients with the current
    with the entire data setã€‚ you just pick small groups and you keep going through
    these random samples with replacementã€‚ of the neural network training data and
    as you go through each of these one by one by one the error will decrease it'll
    go up sometimes sometimes you'll pick particularly bad sets of training data sometimes
    you'll pick particularly good setsã€‚ It just all pretty much dependsã€‚ So stochastic
    gradient descent is often used alone or as part of another trainingã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ç”Ÿçš„äº‹æƒ…æ˜¯ï¼Œä¸ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è®¡ç®—æ¢¯åº¦ç›¸æ¯”ï¼Œä½ åªé€‰æ‹©å°ç»„ï¼Œå¹¶ä¸”ä¸æ–­å¤„ç†è¿™äº›éšæœºæ ·æœ¬ï¼Œå¸¦æœ‰æ›¿æ¢ã€‚éšç€ä½ ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å¤„ç†è¿™äº›ç¥ç»ç½‘ç»œè®­ç»ƒæ•°æ®ï¼Œæ¯æ¬¡è¯¯å·®éƒ½ä¼šå‡å°‘ï¼Œæœ‰æ—¶ä¼šå¢åŠ ï¼Œæœ‰æ—¶ä½ ä¼šé€‰æ‹©ç‰¹åˆ«ç³Ÿç³•çš„è®­ç»ƒæ•°æ®é›†ï¼Œæœ‰æ—¶ä½ ä¼šé€‰æ‹©ç‰¹åˆ«å¥½çš„é›†åˆã€‚è¿™å‡ ä¹å®Œå…¨å–å†³äºæƒ…å†µã€‚å› æ­¤ï¼Œéšæœºæ¢¯åº¦ä¸‹é™é€šå¸¸å•ç‹¬ä½¿ç”¨æˆ–ä½œä¸ºå…¶ä»–è®­ç»ƒçš„ä¸€éƒ¨åˆ†ã€‚
- en: It's computationally efficient and it decreases overfitting by focusing only
    on a small number of relatively good weights and also there's a variety of other
    techniques like I saidã€‚ back propagation and gradient descentã€‚ that's just the
    main backbone some of these other techniquesã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•åœ¨è®¡ç®—ä¸Šæ˜¯é«˜æ•ˆçš„ï¼Œå¹¶é€šè¿‡åªå…³æ³¨å°‘é‡ç›¸å¯¹è¾ƒå¥½çš„æƒé‡æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚æ­¤å¤–ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–æŠ€æœ¯ï¼Œæ¯”å¦‚æˆ‘è¯´çš„åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™ï¼Œè¿™åªæ˜¯ä¸€äº›ä¸»è¦çš„åŸºç¡€æŠ€æœ¯ã€‚
- en: what they attempt to solve is the learning rate and momentumã€‚ These are both
    hyperparameterã€‚ These are numbers that you need to tune along with everything
    elseã€‚ you thought it was bad enough that you had to pick how many hidden letters
    and howã€‚neurons you want on each head layer Now you've got a learning rate of
    momentumã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬è¯•å›¾è§£å†³çš„æ˜¯å­¦ä¹ ç‡å’ŒåŠ¨é‡ã€‚è¿™ä¸¤ä¸ªéƒ½æ˜¯è¶…å‚æ•°ã€‚è¿™äº›æ˜¯ä½ éœ€è¦ä¸å…¶ä»–æ‰€æœ‰å‚æ•°ä¸€èµ·è°ƒæ•´çš„æ•°å­—ã€‚ä½ ä»¥ä¸ºä»…ä»…éœ€è¦é€‰æ‹©éšè—å±‚çš„ç¥ç»å…ƒæ•°é‡å°±å·²ç»å¤Ÿç³Ÿç³•äº†ï¼Œç°åœ¨ä½ è¿˜å¾—è€ƒè™‘å­¦ä¹ ç‡å’ŒåŠ¨é‡ã€‚
- en: and you need to figure out what the best learning rate isã€‚ what the best momentum
    is so that you will be able to effectively train this neural networkã€‚ The problem
    is the learning rateï¼Œ if you adjust it too smallã€‚ it's never going to accurately
    train your neural network it's just not taking enough risk if you make it too
    large your neural network will be veryã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦å¼„æ¸…æ¥šæœ€ä½³å­¦ä¹ ç‡å’Œæœ€ä½³åŠ¨é‡ï¼Œä»¥ä¾¿èƒ½å¤Ÿæœ‰æ•ˆåœ°è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œã€‚é—®é¢˜æ˜¯å­¦ä¹ ç‡ï¼Œå¦‚æœä½ è°ƒæ•´å¾—å¤ªå°ï¼Œå®ƒæ°¸è¿œæ— æ³•å‡†ç¡®åœ°è®­ç»ƒä½ çš„ç¥ç»ç½‘ç»œï¼Œå®ƒåªæ˜¯æ²¡æœ‰æ‰¿æ‹…è¶³å¤Ÿçš„é£é™©ã€‚å¦‚æœä½ æŠŠå®ƒè°ƒå¾—å¤ªå¤§ï¼Œä½ çš„ç¥ç»ç½‘ç»œå°†ä¼šéå¸¸ä¸ç¨³å®šã€‚
- en: very erratic and momentumï¼Œ same thing if you make it too largeï¼Œ things become
    erraticã€‚ if you make it too smallï¼Œ it's not really having effectã€‚ Also if you
    think about itã€‚ this learning rate is being applied to every every weight in the
    entire neural networkã€‚ maybe a single learning rate is not enoughï¼Œ maybe some
    neurons are learning faster than othersã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸ä¸ç¨³å®šå’ŒåŠ¨é‡ï¼Œå¦‚æœä½ æŠŠå®ƒåšå¾—å¤ªå¤§ï¼Œäº‹æƒ…ä¼šå˜å¾—ä¸ç¨³å®šã€‚å¦‚æœä½ æŠŠå®ƒåšå¾—å¤ªå°ï¼Œå®ƒå®é™…ä¸Šæ²¡æœ‰ä»€ä¹ˆæ•ˆæœã€‚å¦å¤–ï¼Œå¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œè¿™ä¸ªå­¦ä¹ ç‡æ˜¯åº”ç”¨äºæ•´ä¸ªç¥ç»ç½‘ç»œä¸­çš„æ¯ä¸€ä¸ªæƒé‡çš„ã€‚ä¹Ÿè®¸ä¸€ä¸ªå•ä¸€çš„å­¦ä¹ ç‡æ˜¯ä¸å¤Ÿçš„ï¼Œä¹Ÿè®¸ä¸€äº›ç¥ç»å…ƒå­¦ä¹ å¾—æ¯”å…¶ä»–çš„å¿«ã€‚
- en: So they like a concept of putting on multiple learning rates or you can also
    sometimes you will see that they will just automatically decrease the learning
    rate as training progresses So we're trying to move away from having every weight
    having a global learning rate and momentum and then alsoã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»–ä»¬å–œæ¬¢æŠŠå¤šä¸ªå­¦ä¹ ç‡çš„æ¦‚å¿µæ”¾åœ¨ä¸€èµ·ï¼Œæˆ–è€…æœ‰æ—¶ä½ ä¼šçœ‹åˆ°ä»–ä»¬ä¼šéšç€è®­ç»ƒçš„è¿›è¡Œè‡ªåŠ¨å‡å°‘å­¦ä¹ ç‡ã€‚æ‰€ä»¥æˆ‘ä»¬è¯•å›¾æ‘†è„±æ¯ä¸ªæƒé‡éƒ½æœ‰ä¸€ä¸ªå…¨å±€å­¦ä¹ ç‡å’ŒåŠ¨é‡çš„æƒ…å†µï¼Œç„¶åä¹Ÿã€‚
- en: Move to making those those values very sensitiveï¼Œ very nonsensitive are very
    accommodating to values that weren't chosen so wellã€‚ These are some other training
    techniques that we that I've worked with in the pastã€‚ There's resilient propagationã€‚
    It works pretty wellã€‚ It basically recognizes that the sign of the gradient is
    probably the most important thingã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬å‘ä½¿è¿™äº›å€¼éå¸¸æ•æ„Ÿï¼Œéå¸¸ä¸æ•æ„Ÿï¼Œæˆ–è€…éå¸¸é€‚åº”é‚£äº›é€‰æ‹©å¾—ä¸å¥½çš„å€¼ã€‚è¿™äº›æ˜¯æˆ‘è¿‡å»ä½¿ç”¨è¿‡çš„ä¸€äº›å…¶ä»–è®­ç»ƒæŠ€æœ¯ã€‚æœ‰å¼¹æ€§ä¼ æ’­ã€‚å®ƒæ•ˆæœä¸é”™ã€‚å®ƒåŸºæœ¬ä¸Šæ‰¿è®¤æ¢¯åº¦çš„ç¬¦å·å¯èƒ½æ˜¯æœ€é‡è¦çš„äº‹æƒ…ã€‚
- en: It tells you which direction the weight should move to better optimizeã€‚ It also
    does not need a learning rate and momentumã€‚ So it was popular back in the dayã€‚
    It's not seen as much use with a deep learning N of accelerated a gradientã€‚ What
    that does is with stochastic gradient descentã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå‘Šè¯‰ä½ æƒé‡åº”è¯¥æœç€å“ªä¸ªæ–¹å‘ç§»åŠ¨ä»¥æ›´å¥½åœ°ä¼˜åŒ–ã€‚å®ƒä¹Ÿä¸éœ€è¦å­¦ä¹ ç‡å’ŒåŠ¨é‡ã€‚æ‰€ä»¥å®ƒåœ¨æ—©äº›æ—¶å€™å¾ˆæµè¡Œã€‚åœ¨æ·±åº¦å­¦ä¹ åŠ é€Ÿçš„æ¢¯åº¦ä¸­å¹¶ä¸å¸¸è§ã€‚å®ƒåšçš„æ˜¯ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ã€‚
- en: It helps to mitigate the risk of just picking a really bad mini batch that then
    damages the rest of the training that you' that you've done There's addedgrad
    and Ana deltaã€‚ These are bothã€‚At a gradï¼Œ basically it keeps a per weight decaying
    learning rateã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœ‰åŠ©äºå‡å°‘åªé€‰æ‹©ä¸€ä¸ªéå¸¸ç³Ÿç³•çš„å°æ‰¹é‡æ•°æ®çš„é£é™©ï¼Œç„¶åæŸå®³ä½ å·²ç»è¿›è¡Œçš„å…¶ä½™è®­ç»ƒã€‚æœ‰æ·»åŠ æ¢¯åº¦å’Œé˜¿çº³å¾·å°”å¡”ã€‚è¿™ä¸¤è€…éƒ½æ˜¯ã€‚åœ¨æ¢¯åº¦ä¸­ï¼ŒåŸºæœ¬ä¸Šå®ƒä¿æŒä¸€ä¸ªæŒ‰æƒé‡è¡°å‡çš„å­¦ä¹ ç‡ã€‚
- en: But it's monoomically decreasingï¼Œ it never increases againã€‚ so that's why added
    Delta was created to address atgrads issues where that that learning rate could
    just go in a direction and decrease to pretty much zeroã€‚There's also nongraient
    methodsã€‚ If you can't take a derivative of your last functionã€‚ these might be
    usefulã€‚ This includes simulated annelingï¼Œ genetic algorithmsï¼Œ particle swarmã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒæ˜¯å•è°ƒé€’å‡çš„ï¼Œå®ƒæ°¸è¿œä¸ä¼šå†å¢åŠ ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåˆ›å»ºäº†æ·»åŠ Deltaæ¥è§£å†³åœ¨æ¢¯åº¦é—®é¢˜ä¸­çš„é‚£ä¸ªå­¦ä¹ ç‡å¯èƒ½æœä¸€ä¸ªæ–¹å‘å»ï¼Œå¹¶å‡å°‘åˆ°å‡ ä¹ä¸ºé›¶çš„é—®é¢˜ã€‚è¿˜æœ‰ä¸€äº›éæ¢¯åº¦æ–¹æ³•ã€‚å¦‚æœä½ ä¸èƒ½å¯¹ä½ çš„æœ€åå‡½æ•°è¿›è¡Œå¯¼æ•°æ“ä½œï¼Œè¿™äº›å¯èƒ½ä¼šæœ‰ç”¨ã€‚è¿™åŒ…æ‹¬æ¨¡æ‹Ÿé€€ç«ã€é—ä¼ ç®—æ³•ã€ç²’å­ç¾¤ã€‚
- en: Nedermeed and many moreã€‚ So the atom update rule talked about before classic
    back propagationã€‚ It's just another way of calculating V to put into this weight
    update algorithm that we have beforeã€‚ Now the atom update ruleã€‚ what is very nice
    about it is it doesn't have you don't really have to focus too much on a learning
    rateã€‚ there is a learning rate present but of the values that you that you haveã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Nedermeedå’Œè®¸å¤šå…¶ä»–ã€‚å› æ­¤ï¼ŒåŸå­æ›´æ–°è§„åˆ™ä¹‹å‰è°ˆè®ºçš„ç»å…¸åå‘ä¼ æ’­ã€‚è¿™åªæ˜¯æˆ‘ä»¬ä¹‹å‰å°†Vè®¡ç®—åˆ°è¿™ä¸ªæƒé‡æ›´æ–°ç®—æ³•ä¸­çš„å¦ä¸€ç§æ–¹å¼ã€‚ç°åœ¨åŸå­æ›´æ–°è§„åˆ™ã€‚å®ƒä¹‹æ‰€ä»¥å¾ˆå¥½çš„ä¸€ä¸ªåŸå› æ˜¯ä½ ä¸éœ€è¦è¿‡å¤šåœ°å…³æ³¨å­¦ä¹ ç‡ã€‚è™½ç„¶å­˜åœ¨ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œä½†æ˜¯ä½ æ‹¥æœ‰çš„é‚£äº›å€¼ã€‚
- en: the authors of the original paper King Mubaï¼Œ they give some good recommendations
    for the hyperparametersã€‚ and I rarelyï¼Œ even that learning rate at the end tend
    to the negative8ã€‚ they don't you often do not have to have to change thoseã€‚ This
    is a relatively new training algorithmã€‚ It was introduced in 2014ã€‚ It's a popular
    oneã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹è®ºæ–‡çš„ä½œè€…King Mubaï¼Œä»–ä»¬å¯¹è¶…å‚æ•°ç»™å‡ºäº†ä¸€äº›å¥½çš„å»ºè®®ã€‚è€Œä¸”æˆ‘å¾ˆå°‘ï¼Œç”šè‡³æ˜¯æœ€åé‚£ä¸ªå­¦ä¹ ç‡ä¹Ÿå€¾å‘äºè´Ÿ8ã€‚ä»–ä»¬é€šå¸¸ä¸å¿…æ”¹å˜é‚£äº›ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„è®­ç»ƒç®—æ³•ã€‚å®ƒæ˜¯åœ¨2014å¹´å¼•å…¥çš„ã€‚å®ƒæ˜¯ä¸€ä¸ªæµè¡Œçš„ç®—æ³•ã€‚
- en: It deals with training for a sparse for sparse dataã€‚We lots of missing values
    and then also stochastic error function the error function is stochastic because
    we're doing stochastic gradient descent so we're constantly randomly subsampling
    a batch size of many batch and updating the neural network based on that So your
    the changes that you make from one iteration to the other might not help because
    you're grabbing a different set of set of training data on each one the paper
    for this for this is given here at Cornell University the archive if you haven't
    dealt with archive before that's the Greek letter chi so a archive archive and
    moment moment momentest method of moments that you see here it's a way of estimating
    each of the moments of of a distribution of values so that's the mean the variance
    and reliance and so on so let's look atã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¤„ç†ç¨€ç–æ•°æ®çš„è®­ç»ƒã€‚æˆ‘ä»¬æœ‰å¾ˆå¤šç¼ºå¤±å€¼ï¼Œè¿˜æœ‰éšæœºè¯¯å·®å‡½æ•°ï¼Œè¯¯å·®å‡½æ•°æ˜¯éšæœºçš„ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿›è¡Œéšæœºæ¢¯åº¦ä¸‹é™ï¼Œå› æ­¤æˆ‘ä»¬ä¸æ–­éšæœºæŠ½æ ·ä¸€ä¸ªæ‰¹é‡å¤§å°å¹¶åŸºäºæ­¤æ›´æ–°ç¥ç»ç½‘ç»œã€‚å› æ­¤ï¼Œä½ ä»ä¸€æ¬¡è¿­ä»£åˆ°å¦ä¸€è¿­ä»£çš„æ”¹å˜å¯èƒ½æ²¡æœ‰å¸®åŠ©ï¼Œå› ä¸ºä½ åœ¨æ¯æ¬¡æŠ½å–çš„éƒ½æ˜¯ä¸åŒçš„è®­ç»ƒæ•°æ®ã€‚å…³äºè¿™ä¸ªçš„è®ºæ–‡åœ¨åº·å¥ˆå°”å¤§å­¦çš„æ¡£æ¡ˆé‡Œï¼Œå¦‚æœä½ ä¹‹å‰æ²¡æœ‰æ¥è§¦è¿‡æ¡£æ¡ˆï¼Œé‚£æ˜¯å¸Œè…Šå­—æ¯Ï‡ï¼Œæ‰€ä»¥æ˜¯æ¡£æ¡ˆæ¡£æ¡ˆå’ŒçŸ©çš„çŸ©çš„çŸ©ä¼°è®¡æ–¹æ³•ï¼Œä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ï¼Œè¿™æ˜¯ä¼°è®¡ä¸€ç»„å€¼çš„æ¯ä¸ªçŸ©çš„æ–¹æ³•ï¼Œæ‰€ä»¥å°±æ˜¯å‡å€¼ã€æ–¹å·®ç­‰ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚
- en: The actual paper for thisï¼Œ this is very similar to the code that we've seen
    for the other onesã€‚ so this is just t equals t plus one that's indicating that
    we're moving through the time we're initializing the first and second moment the
    first moment is the meanã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡è®ºæ–‡å®é™…ä¸Šä¸æˆ‘ä»¬çœ‹åˆ°çš„å…¶ä»–ä»£ç éå¸¸ç›¸ä¼¼ã€‚æ‰€ä»¥è¿™åªæ˜¯tç­‰äºtåŠ 1ï¼Œè¡¨ç¤ºæˆ‘ä»¬åœ¨æ—¶é—´ä¸Šç§»åŠ¨ï¼Œæˆ‘ä»¬æ­£åœ¨åˆå§‹åŒ–ç¬¬ä¸€å’Œç¬¬äºŒçŸ©ï¼Œç¬¬ä¸€çŸ©æ˜¯å‡å€¼ã€‚
- en: so the mean of the gradients that you're trying to estimateã€‚And V is the varianceã€‚
    so the second moment there is a third and additional momentsã€‚ but all we're dealing
    with are these two the first two momentsã€‚ by the way that's where the name add
    comes from adaptive moment estimation and then we initialize the time set to zeroã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ è¯•å›¾ä¼°è®¡çš„æ¢¯åº¦çš„å‡å€¼ã€‚Væ˜¯æ–¹å·®ã€‚ç¬¬äºŒçŸ©æ˜¯ç¬¬ä¸‰å’Œé™„åŠ çŸ©ã€‚ä½†æˆ‘ä»¬å¤„ç†çš„ä»…æ˜¯è¿™ä¸¤ä¸ªï¼Œç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªçŸ©ã€‚é¡ºä¾¿æä¸€ä¸‹ï¼Œè¿™å°±æ˜¯â€œåŠ â€çš„åå­—æ¥æºï¼Œè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼Œç„¶åæˆ‘ä»¬å°†æ—¶é—´è®¾ç½®åˆå§‹åŒ–ä¸ºé›¶ã€‚
- en: so these two initial estimates are zeroã€‚We are going to calculate the Gï¼Œ the
    gradientã€‚ so this is the gradientï¼Œ just like we're doing beforeï¼Œ very similar
    to classic back propagationã€‚ a little bit different terminology instead of the
    J that I use they use F so this is this is the loss function with the weights
    that's exactly the notation from the module for the previous oneã€‚And we are going
    toã€‚Get that gradientã€‚ These two values deal with a bias on theã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸¤ä¸ªåˆå§‹ä¼°è®¡å€¼ä¸ºé›¶ã€‚æˆ‘ä»¬å°†è®¡ç®—Gï¼Œæ¢¯åº¦ã€‚æ‰€ä»¥è¿™ä¸ªæ¢¯åº¦ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰æ‰€åšçš„ï¼Œéå¸¸ç±»ä¼¼äºç»å…¸åå‘ä¼ æ’­ã€‚æœ¯è¯­ç¨æœ‰ä¸åŒï¼Œæˆ‘ç”¨çš„Jæ”¹æˆäº†Fï¼Œå› æ­¤è¿™æ˜¯æŸå¤±å‡½æ•°ï¼ŒåŒ…å«æƒé‡ï¼Œæ­£æ˜¯å‰ä¸€ä¸ªæ¨¡å—ä¸­çš„ç¬¦å·ã€‚æˆ‘ä»¬å°†è·å–é‚£ä¸ªæ¢¯åº¦ã€‚è¿™ä¸¤ä¸ªå€¼å¤„ç†åå·®ã€‚
- en: That occurs early on since these are initialized to 0ï¼Œ that creates a tremendous
    bias towards  zeroã€‚ So these are just twoã€‚å—¯ã€‚I'm sorry the hats down here are actually
    that that's where they're calculating the they're dealing with the the initialã€‚Fact
    that these are starting out at zeroã€‚ So M hat and V hat M and VT themselvesã€‚ this
    is essentially updating each of theseã€‚ so you'll notice that it's M based on the
    previous Tã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™äº›å€¼åˆå§‹åŒ–ä¸º0ï¼Œå› æ­¤æ—©æœŸä¼šäº§ç”Ÿå·¨å¤§çš„åå·®ï¼Œå› æ­¤è¿™åªæ˜¯ä¸¤ä¸ªã€‚å—¯ï¼ŒæŠ±æ­‰ï¼Œè¿™é‡Œçš„å¸½å­å®é™…ä¸Šæ˜¯åœ¨è®¡ç®—ï¼Œå®ƒä»¬å¤„ç†çš„æ˜¯æœ€åˆçš„äº‹å®ï¼Œå³è¿™äº›å€¼ä»é›¶å¼€å§‹ã€‚å› æ­¤Må¸½å’ŒVå¸½Må’ŒVTæœ¬èº«ï¼ŒåŸºæœ¬ä¸Šæ˜¯åœ¨æ›´æ–°æ¯ä¸€ä¸ªã€‚ä½ ä¼šæ³¨æ„åˆ°è¿™æ˜¯åŸºäºä¸Šä¸€ä¸ªTçš„Mã€‚
- en: They're updating as they go through itï¼Œ they are essentially creating more and
    more of a estimate of the first and second momentsã€‚ and these are vectorsã€‚Obviously
    they're across the weightsã€‚ so that's essentially creating almost a learning rate
    for eachã€‚The first one is on the first power and then second powerï¼Œ so this is
    squaredã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¸æ–­æ›´æ–°ï¼Œå®é™…ä¸Šæ­£åœ¨é€æ¸åˆ›å»ºå¯¹ç¬¬ä¸€å’Œç¬¬äºŒçŸ©çš„ä¼°è®¡ã€‚è¿™äº›éƒ½æ˜¯å‘é‡ï¼Œæ˜¾ç„¶å®ƒä»¬ä¸æƒé‡ç›¸å…³ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯åœ¨ä¸ºæ¯ä¸ªæƒé‡åˆ›å»ºä¸€ä¸ªå‡ ä¹æ˜¯å­¦ä¹ ç‡çš„ä¸œè¥¿ã€‚ç¬¬ä¸€ä¸ªæ˜¯ä¸€æ¬¡å¹‚ï¼Œç„¶åæ˜¯äºŒæ¬¡å¹‚ï¼Œæ‰€ä»¥è¿™æ˜¯å¹³æ–¹çš„ã€‚
- en: We calculate these adjustment values just to deal with the fact that these started
    at a 0ã€‚ and then we're going to update the weightsã€‚ So weight tï¼Œ this is very
    simpleã€‚ So this part is exactly like what we had from trying to highlight the
    bottom rowã€‚ But up to here is exactly what we had for the for classicã€‚We're subtracting
    the gradientã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—è¿™äº›è°ƒæ•´å€¼åªæ˜¯ä¸ºäº†å¤„ç†è¿™äº›å€¼æœ€å¼€å§‹ä¸º0çš„äº‹å®ï¼Œç„¶åæˆ‘ä»¬å°†æ›´æ–°æƒé‡ã€‚æ‰€ä»¥æƒé‡tï¼Œè¿™ä¸ªéå¸¸ç®€å•ã€‚è¿™éƒ¨åˆ†ä¸æˆ‘ä»¬å°è¯•çªå‡ºåº•è¡Œæ—¶å®Œå…¨ç›¸åŒã€‚ä½†åˆ°è¿™é‡Œæ­£æ˜¯æˆ‘ä»¬åœ¨ç»å…¸ç®—æ³•ä¸­æ‰€ç”¨çš„ã€‚æˆ‘ä»¬åœ¨å‡å»æ¢¯åº¦ã€‚
- en: but instead of subtracting the gradientï¼Œ we're using this formula to update
    the parameters or weightsã€‚Alpha is the learning rateï¼Œ so usually learning rates
    or step sizeï¼Œ as they call itã€‚ you're multiplying by the rest of this to scale
    thisã€‚And you're putting in the two hat valuesã€‚ which were based on these just
    to adjust them so that they're not so biased initially towards zeroã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¸æ˜¯å‡å»æ¢¯åº¦ï¼Œè€Œæ˜¯ä½¿ç”¨è¿™ä¸ªå…¬å¼æ¥æ›´æ–°å‚æ•°æˆ–æƒé‡ã€‚Alpha æ˜¯å­¦ä¹ ç‡ï¼Œæ‰€ä»¥é€šå¸¸ç§°ä¸ºå­¦ä¹ ç‡æˆ–æ­¥é•¿ã€‚ä½ åœ¨è¿™ä¸ªåŸºç¡€ä¸Šè¿›è¡Œä¹˜æ³•æ¥è¿›è¡Œç¼©æ”¾ã€‚ç„¶åä½ æ”¾å…¥ä¸¤ä¸ªå¸½å€¼ï¼Œè¿™äº›å¸½å€¼æ˜¯åŸºäºè¿™äº›æ•°æ®ï¼Œä»¥ä¾¿è°ƒæ•´å®ƒä»¬ï¼Œä½¿å®ƒä»¬æœ€åˆä¸ä¼šè¿‡äºåå‘é›¶ã€‚
- en: That is essentially Adamï¼Œ it's a little bit more complicated than your classic
    back propagationã€‚ but not a whole lot not I mean this is not terrible to implement
    andã€‚In Java or another programming languageï¼Œ they discuss the algorithmã€‚And they
    talk about that initial bias correction where why that is needed and how you are
    calculating the two hat values that are very necessary for thatã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šè¿™æ˜¯ Adamï¼Œå®ƒæ¯”ç»å…¸çš„åå‘ä¼ æ’­ç¨å¾®å¤æ‚ä¸€äº›ï¼Œä½†å¹¶ä¸æ˜¯ç‰¹åˆ«å¤æ‚ã€‚æˆ‘æ˜¯è¯´ï¼Œå®æ–½èµ·æ¥å¹¶ä¸éš¾ã€‚åœ¨ Java æˆ–å…¶ä»–ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œä»–ä»¬è®¨è®ºäº†ç®—æ³•ã€‚ä»–ä»¬è°ˆåˆ°äº†åˆå§‹åå·®æ ¡æ­£ï¼Œä¸ºä»€ä¹ˆéœ€è¦ï¼Œä»¥åŠå¦‚ä½•è®¡ç®—è¿™ä¸¤ä¸ªå¸½å€¼ï¼Œè¿™æ˜¯éå¸¸å¿…è¦çš„ã€‚
- en: I will admit I have not read through this part and find it somewhat very complexã€‚
    they're doing some analysis of the convergenceï¼Œ this is essentially proof for
    why it worksã€‚And then they quote related work and then experiments where they
    sort of empirically prove through experiment why it works and with E theorems
    they attempt to talk about why it sort of proofwise why it worksã€‚ related workï¼Œ
    R Pro and Agradï¼Œ those are two other training very similar training techniques
    that existed prior to Adam and this is just straight sort of from the paperã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ‰¿è®¤æˆ‘æ²¡æœ‰ä»”ç»†é˜…è¯»è¿™éƒ¨åˆ†ï¼Œè§‰å¾—å®ƒæœ‰äº›å¤æ‚ã€‚ä»–ä»¬åœ¨åˆ†ææ”¶æ•›ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯ä¸ºä»€ä¹ˆå®ƒæœ‰æ•ˆçš„è¯æ˜ã€‚ç„¶åä»–ä»¬å¼•ç”¨ç›¸å…³å·¥ä½œå’Œå®éªŒï¼Œé€šè¿‡å®éªŒå®è¯è¯æ˜ä¸ºä»€ä¹ˆå®ƒæœ‰æ•ˆï¼Œå¹¶ç”¨
    E å®šç†è¯•å›¾è°ˆè®ºå®ƒä¸ºä½•æœ‰æ•ˆçš„è¯æ˜ã€‚ç›¸å…³å·¥ä½œï¼ŒR Pro å’Œ Agradï¼Œé‚£æ˜¯ä¸¤ç§åœ¨ Adam ä¹‹å‰å­˜åœ¨çš„éå¸¸ç›¸ä¼¼çš„è®­ç»ƒæŠ€æœ¯ï¼Œè¿™éƒ¨åˆ†æ˜¯ç›´æ¥æ¥è‡ªè®ºæ–‡ã€‚
- en: but I like looking the pseudocode a little bit betterã€‚ but I reproduced it there
    if you're interested in itã€‚This is a really good diagramã€‚ I did not create itï¼Œ
    I have links to where this is where I found itã€‚Yeahã€‚So anywayã€‚ and then he cites
    it to the original author canã€‚Definitely want to give credit where credit is dueã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘æ›´å–œæ¬¢çœ‹ä¼ªä»£ç ã€‚å¦‚æœä½ å¯¹æ­¤æ„Ÿå…´è¶£ï¼Œæˆ‘åœ¨è¿™é‡Œå¤ç°äº†å®ƒã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„å›¾è¡¨ã€‚æˆ‘æ²¡æœ‰åˆ›å»ºå®ƒï¼Œæˆ‘æœ‰é“¾æ¥åˆ°æˆ‘æ‰¾åˆ°çš„åœ°æ–¹ã€‚æ˜¯çš„ã€‚æ‰€ä»¥ï¼Œæ— è®ºå¦‚ä½•ï¼Œç„¶åä»–å¼•ç”¨äº†åŸä½œè€…ï¼Œç¡®å®è¦ç»™äºˆåº”æœ‰çš„ä¿¡ç”¨ã€‚
- en: This is a veryï¼Œ very good diagram of thisã€‚ It's an animated gã€‚ So these areã€‚
    this is essentially a search spaceã€‚ So you can think of this as a rugged sort
    of two dimensional planeã€‚ Star is the lowest pointã€‚ That's where you want to get
    toã€‚ And it's likeã€‚ğŸ˜Šã€‚It's sort of like marbles rolling down a ridgeã€‚This is stochastic
    gradient descentã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸å¥½çš„å›¾è¡¨ã€‚è¿™æ˜¯ä¸€ä¸ªåŠ¨æ€å›¾ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæœç´¢ç©ºé—´ã€‚ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªå´å²–çš„äºŒç»´å¹³é¢ã€‚æ˜Ÿæ˜Ÿæ˜¯æœ€ä½ç‚¹ã€‚é‚£æ˜¯ä½ æƒ³è¦åˆ°è¾¾çš„åœ°æ–¹ã€‚è¿™å°±åƒğŸ˜Šã€‚å°±åƒå¼¹ç æ²¿ç€å±±è„Šæ»šè½ã€‚è¿™å°±æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ã€‚
- en: which is the slowest oneã€‚ So that's your classic back propagationã€‚ It's eventually
    getting thereã€‚ The one that gets there really first is add Delta andã€‚It seems
    like greenã€‚Momentum starts out slowã€‚ but then reallyï¼Œ as you can seeï¼Œ builds up
    the momentum and then blasts right past itã€‚ So just your different different training
    methodsã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€æ…¢çš„ä¸€ç§ã€‚æ‰€ä»¥è¿™æ˜¯ä½ ç»å…¸çš„åå‘ä¼ æ’­ã€‚æœ€ç»ˆå®ƒæ˜¯å¯ä»¥åˆ°è¾¾çš„ã€‚æœ€æ—©åˆ°è¾¾çš„é‚£ä¸ªæ˜¯åŠ  Deltaï¼Œå®ƒçœ‹èµ·æ¥åƒç»¿è‰²çš„ã€‚åŠ¨é‡å¼€å§‹æ—¶å¾ˆæ…¢ï¼Œä½†å¦‚ä½ æ‰€è§ï¼Œç¡®å®ä¼šç§¯ç´¯åŠ¨é‡ï¼Œç„¶åå¿«é€Ÿè¶…è¶Šã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯ä½ ä¸åŒçš„è®­ç»ƒæ–¹æ³•ã€‚
- en: Adam is out on here because when this was createdã€‚ Adam did not exist or at
    least was not all that common yetã€‚ But you canã€‚ you can definitely see some of
    them like momentumã€‚ It's very obvious that it'sã€‚It starts to really build that
    momentumï¼Œ the little green ball and shoots right past itã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Adam åœ¨è¿™é‡Œå‡ºç°æ˜¯å› ä¸ºåœ¨åˆ›å»ºè¿™ä¸ªæ—¶ï¼ŒAdam è¿˜ä¸å­˜åœ¨ï¼Œæˆ–è€…è‡³å°‘ä¸å¸¸è§ã€‚ä½†ä½ å¯ä»¥çœ‹åˆ°å…¶ä¸­ä¸€äº›ï¼Œæ¯”å¦‚åŠ¨é‡ã€‚æ˜¾ç„¶å®ƒå¼€å§‹çœŸçš„åœ¨ç§¯ç´¯åŠ¨é‡ï¼Œå°ç»¿çƒè¿…é€Ÿè¶…è¿‡ã€‚
- en: SGD is just slowly methodically working there and it doesn't even make itã€‚ they
    give up and reset it because everybody else has made itã€‚Kras and Tensorflowlow
    has a variety of these these techniques available for youã€‚ Can you the ones that
    are highlighted they haveï¼Œ so they have stochastic gradient descent in Adamã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: SGD åªæ˜¯æ…¢æ…¢æœ‰æ¡ä¸ç´Šåœ°åœ¨å·¥ä½œï¼Œç”šè‡³éƒ½æ²¡èƒ½å®Œæˆã€‚ä»–ä»¬æ”¾å¼ƒå¹¶é‡ç½®ï¼Œå› ä¸ºå…¶ä»–äººéƒ½å®Œæˆäº†ã€‚Kras å’Œ Tensorflowlow æä¾›äº†å¤šç§è¿™äº›æŠ€æœ¯ã€‚ä½ å¯ä»¥çœ‹åˆ°é‚£äº›è¢«çªå‡ºæ˜¾ç¤ºçš„ï¼Œå®ƒä»¬æœ‰éšæœºæ¢¯åº¦ä¸‹é™å’Œ
    Adamã€‚
- en: I believe there's other ways you can get to some of theseã€‚ I'm sorryã€‚ these
    are all of the ones that Tensorflowlow hasã€‚ I honestly have no idea what FTRL
    is or I'm familiar with the other onesã€‚ the ones that I have highlighted are probably
    the ones you're the most interested inã€‚ I would be tempted to highlight RMSs Pro
    as wellã€‚ you can you can definitely experiment with these and get potentially
    better results where you specify it is down here with the optimizerã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡è¿˜æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥è¾¾åˆ°è¿™äº›ç›®æ ‡ã€‚æˆ‘å¾ˆæŠ±æ­‰ï¼Œè¿™äº›éƒ½æ˜¯ Tensorflowlow æä¾›çš„ã€‚æˆ‘è€å®è¯´ä¸çŸ¥é“ FTRL æ˜¯ä»€ä¹ˆï¼Œæˆ–è€…æˆ‘ä¸ç†Ÿæ‚‰å…¶ä»–çš„é€‰é¡¹ã€‚æˆ‘æ‰€å¼ºè°ƒçš„è¿™äº›å¯èƒ½æ˜¯ä½ æœ€æ„Ÿå…´è¶£çš„ã€‚æˆ‘ä¹Ÿä¼šæƒ³å¼ºè°ƒ
    RMSs Proã€‚ä½ å¯ä»¥å°è¯•è¿™äº›ï¼Œå¯èƒ½ä¼šå¾—åˆ°æ›´å¥½çš„ç»“æœï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œå’Œä¼˜åŒ–å™¨ä¸€èµ·æŒ‡å®šã€‚
- en: If you specify one of these that means a learning rate you'll pass in a class
    here and actually pass in the parameters as well you can go to the Tensorflow
    documentation and they show you ourki is anyway and they they give you the actual
    class names for thisã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æŒ‡å®šäº†å…¶ä¸­ä¸€ä¸ªï¼Œè¿™æ„å‘³ç€ä½ å°†åœ¨è¿™é‡Œä¼ é€’ä¸€ä¸ªå­¦ä¹ ç‡çš„ç±»ï¼Œå¹¶ä¸”å®é™…ä¸Šä¹Ÿä¼ é€’å‚æ•°ã€‚ä½ å¯ä»¥æŸ¥çœ‹ Tensorflow çš„æ–‡æ¡£ï¼Œä»–ä»¬ä¼šå‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ï¼Œå¹¶ç»™å‡ºå®é™…çš„ç±»åã€‚
- en: but you can just use any of them just by putting its name in here you'll need
    to put an object in there if you want to specify even additional parameters these
    are some of the results that I got trying these out so you can see that on the
    miles per gallon data set we got basically different results and some like unit's
    momentum needed a lot more training than say atã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ å¯ä»¥é€šè¿‡å°†å…¶åç§°æ”¾åœ¨è¿™é‡Œç›´æ¥ä½¿ç”¨ä»»ä½•ä¸€ä¸ªï¼Œå¦‚æœä½ æƒ³æŒ‡å®šæ›´å¤šçš„å‚æ•°ï¼Œä½ éœ€è¦æ”¾ä¸€ä¸ªå¯¹è±¡ã€‚è¿™æ˜¯æˆ‘å°è¯•è¿™äº›æ—¶å¾—åˆ°çš„ä¸€äº›ç»“æœï¼Œå› æ­¤ä½ å¯ä»¥çœ‹åˆ°åœ¨æ¯åŠ ä»‘è‹±é‡Œæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸åŒçš„ç»“æœï¼Œè€Œåƒå•ä½åŠ¨é‡åˆ™éœ€è¦æ¯”è¯´çš„è¦å¤šå¾—å¤šçš„è®­ç»ƒã€‚
- en: Showing the last successful or the last number of iterationsã€‚ So Adam converges
    pretty quickã€‚ I I'm pretty fond of that training techniqueï¼Œ and I use it often
    At agrad is also pretty good as wellã€‚ So at agradï¼Œ I like RMS Pro definitelyï¼Œ
    definitely I have seen it T's usesã€‚ I willã€‚ğŸ˜Šã€‚I will try between these and look
    at them when I am training neural networks and I often start with the atomã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºæœ€åä¸€æ¬¡æˆåŠŸæˆ–æœ€åå‡ æ¬¡è¿­ä»£ã€‚å› æ­¤ï¼ŒAdam æ”¶æ•›å¾—å¾ˆå¿«ã€‚æˆ‘å¯¹è¿™ç§è®­ç»ƒæŠ€æœ¯å¾ˆæœ‰å¥½æ„Ÿï¼Œå¹¶ä¸”ç»å¸¸ä½¿ç”¨å®ƒã€‚At agrad ä¹Ÿç›¸å½“ä¸é”™ã€‚å› æ­¤åœ¨ agrad
    ä¸Šï¼Œæˆ‘éå¸¸å–œæ¬¢ RMS Proï¼Œç¡®å®ï¼Œæˆ‘å·²ç»çœ‹åˆ°å®ƒçš„ç”¨é€”ã€‚æˆ‘ä¼šã€‚ğŸ˜Šæˆ‘ä¼šåœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶åœ¨è¿™äº›ä¹‹é—´è¿›è¡Œå°è¯•ï¼Œå¹¶ä¸”æˆ‘é€šå¸¸ä» Adam å¼€å§‹ã€‚
- en: but then we'll try at agrad or our MSPã€‚There are probably betterã€‚ more scientific
    ways to pick thoseï¼Œ but that tends to work pretty well for meã€‚ All rightã€‚ That
    is the end of this moduleã€‚![](img/6491afaa3cf5a798f8c7f93f7b8bf376_3.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å°†å°è¯• agrad æˆ–è€…æˆ‘ä»¬çš„ MSPã€‚å¯èƒ½æœ‰æ›´å¥½ã€æ›´ç§‘å­¦çš„æ–¹æ³•æ¥é€‰æ‹©è¿™äº›ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸æ•ˆæœä¸é”™ã€‚å¥½çš„ï¼Œè¿™ä¸ªæ¨¡å—åˆ°æ­¤ç»“æŸã€‚![](img/6491afaa3cf5a798f8c7f93f7b8bf376_3.png)
- en: Thank you for watching this video on how to train a neural networkã€‚ In the next
    videoã€‚ we're going to literally calculate a neural network from scratchã€‚ We're
    going to see how to export the weights from Kes and use those to actually calculate
    what the output of the neural network would have beenã€‚ This removes all magic
    from the processã€‚ This content changes oftenã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢è§‚çœ‹è¿™ä¸ªå…³äºå¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œçš„è§†é¢‘ã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ä»å¤´å¼€å§‹å®é™…è®¡ç®—ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä» Keras å¯¼å‡ºæƒé‡ï¼Œå¹¶ä½¿ç”¨è¿™äº›æƒé‡å®é™…è®¡ç®—ç¥ç»ç½‘ç»œçš„è¾“å‡ºã€‚è¿™æ¶ˆé™¤äº†è¿‡ç¨‹ä¸­çš„æ‰€æœ‰é­”æ³•ã€‚è¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ã€‚
- en: So subscribe to the channel to stay up to date on this course and other topics
    and artificial intelligenceã€‚ğŸ˜Šã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æœ¬è¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚ğŸ˜Š
