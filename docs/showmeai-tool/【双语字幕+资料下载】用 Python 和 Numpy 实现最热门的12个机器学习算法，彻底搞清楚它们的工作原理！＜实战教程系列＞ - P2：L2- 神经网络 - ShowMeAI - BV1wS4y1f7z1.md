# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÁî® Python Âíå Numpy ÂÆûÁé∞ÊúÄÁÉ≠Èó®ÁöÑ12‰∏™Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÂΩªÂ∫ïÊêûÊ∏ÖÊ•öÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P2ÔºöL2- Á•ûÁªèÁΩëÁªú - ShowMeAI - BV1wS4y1f7z1

HiÔºå everybody„ÄÇ Welcome to a new tutorial„ÄÇ This is the first video of a new series called machine learning from scratch„ÄÇ

 In this seriesÔºå we are going to implement popular machine learning algorithms using only built in Python modules and Ny„ÄÇ

 So todayÔºå we will start with the key nearest neighbor algorithm or short K and N algorithm„ÄÇüòä„ÄÇ

The concept of K And N is fairly easy„ÄÇ A sample is classified by a popularity vote of its nearest neighbors„ÄÇ

 So let's consider an example with two classes and a two dimensional feature vector„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_1.png)

So let's have a look at this figure„ÄÇ So here we have two classes„ÄÇ

 the blue class and the orange classÔºå and we have feature vectors with two dimensions„ÄÇ

 So we have x1 on this axis and x 2 on this axis„ÄÇAnd what we do here is we have some training samples„ÄÇ

And then for each new sample that we want to classify„ÄÇ

 we calculate the distance of this sample to each of the training samples„ÄÇ

 And then we have a look at the nearest neighbors„ÄÇ So in this case„ÄÇ

 we have a look at the three nearest neighbors„ÄÇ So these ones„ÄÇ

 And then we choose or predict the label based on the most common„ÄÇClass labels here„ÄÇ

 So we have two blue glasses and one orange glass„ÄÇ So this then will be a blue class„ÄÇ

And this is the whole concept of the K and N„ÄÇAnd what we also have to know is in order to calculate the distances„ÄÇ

We used the Euclidean distance„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_3.png)

So in a 2D exampleÔºå the Euclidean distance of two points is defined as the square root over„ÄÇ

 and then we have for each„ÄÇü§¢ÔºåFeature vector component„ÄÇ we have the squared„ÄÇDifference„ÄÇ

 so we have x 2 minus x1 squared plus y2 minus y1 squared„ÄÇ

 So this is the Euclidean distance in a 2D case„ÄÇ and in a more general caseÔºå the formula„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_5.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_6.png)

Is defined as„ÄÇThisÔºå so it's the square root over the sum from I equals 0 to n„ÄÇ

 where n is the number of dimensions„ÄÇ And then we have the sum over„ÄÇEach„ÄÇComponent„ÄÇ

 and for each componentÔºå we calculate the squared distance or the square difference„ÄÇ

So this is the Euclidean distanceÔºå and this is all we have to know in order to implement the K and N„ÄÇ

 So let's start„ÄÇ And firstÔºå let's define a class called K and N„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_8.png)

And this hasÔºå of courseÔºå an innate method„ÄÇSo in itselfÔºå and this will get a K„ÄÇ

 So this is the number of nearest neighbors we want to consider„ÄÇ

 And this will also get a default value„ÄÇ So the default is 3„ÄÇAnd in the in it„ÄÇOhÔºå sorry„ÄÇ

We define the in it„ÄÇ So in the inÔºå we simply want to store the K„ÄÇ So we say self dot K equals k„ÄÇ

And then what we want to implement here„ÄÇ And here we want to follow the conventions of other machine learning libraries„ÄÇ

 For exampleÔºå the psychic Learn library„ÄÇ So we have a„ÄÇÂóØ„ÄÇFit method„ÄÇSo this„ÄÇ

 this will fit the traininging samples and some„ÄÇTraining labels„ÄÇ

 And this will usually involve a the training step„ÄÇ So we want to implement this one„ÄÇ

 And then we also want to implement a predict method„ÄÇSorryÔºå this also has self„ÄÇ

And the predict method„ÄÇ So here we want to predict new samples„ÄÇ

So these are the methods we want to implement„ÄÇAnd nowÔºå before we go onÔºå first of all„ÄÇ

 let's have a look at how our data looks„ÄÇ So what is this X and this YÔºå And for this„ÄÇ

 I wrote some test script„ÄÇ So here I used the famous irris data sets„ÄÇ

 So you might have heard about this already„ÄÇ So I can get this from the ps could learn module„ÄÇ

 And then I will generate some training samples and some test samples„ÄÇ

 and the associated training labels and test labels„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_10.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_11.png)

So let's first have a look at how this„ÄÇTraining samples„ÄÇ look„ÄÇ

 so we want to print the shape of this one„ÄÇSo this is an an nuy N D array of shapeÔºå120 by 4„ÄÇ

 So 120 is the number of samples„ÄÇ and four is the number of features for each sample„ÄÇ So for example„ÄÇ

 let's print the first sample„ÄÇSo this is four features in it„ÄÇ

So this is how our training samples look„ÄÇ And now let's have a look at our training labels„ÄÇSo„ÄÇ

This is a 1 D row vectorÔºå also of size 120„ÄÇ So for each of our training samplesÔºå we have the„ÄÇ

The label for it„ÄÇSo if we print thisÔºå then we see this will be a 1 D vector with only one row„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_13.png)

And now let's what we see here is we have label 0Ôºå1 and 2„ÄÇ So this is a three class problem„ÄÇ

 So let's also plot this„ÄÇÂóØ„ÄÇAnd nowÔºå for exampleÔºå what I plot here is I only plot the first two„ÄÇ

Feature us so that we have a 2D case„ÄÇSo this is how our data looks„ÄÇ So we have three classesÔºå red„ÄÇ

 green and blue and„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_15.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_16.png)

Yeah„ÄÇSo this is how our data looks„ÄÇ And now we can continue by implementing this„ÄÇ

 So in our fit method„ÄÇSo in the K and N algorithmÔºå this doesn't involve a training step„ÄÇ

 So what we do here is we simply want to store our training samples and then we can use them later„ÄÇ

 so we can sayÔºå let's store them„ÄÇ So let's say self„ÄÇ

 and then we call this X train equals X and self Y train„ÄÇEquals y„ÄÇSo this is all for our fit method„ÄÇ

And nowÔºå for our predict method„ÄÇSoÔºå this will get„ÄÇCan get multiple samples hereÔºå So we„ÄÇÂóØ„ÄÇ

Can see thisÔºå because weÔºå we use a capital x for this„ÄÇSo this can have multiple samples„ÄÇ

 so we can write a little helper method„ÄÇ So we want to do this for each of the samples„ÄÇ

 We want to say we want to get the predicted„ÄÇLabels„ÄÇEqualsÔºå and then„ÄÇWe use„ÄÇ

Or we write a helper method that we call underscore predict„ÄÇ And this will only get one sample„ÄÇ

So here we use list comprehension„ÄÇ So what we want to do then is we want to call this self„ÄÇPredict„ÄÇ

With one sample X„ÄÇ And then we want to do this for all of our samples„ÄÇIn our„ÄÇTests samples here„ÄÇ

 So for small x in capital x„ÄÇAnd then this will be a list„ÄÇ So let's convert this to a nuy arrayÔºå and„ÄÇ

Then„ÄÇThis is our predict method„ÄÇ AndÔºå of courseÔºå we have to import Nmpy„ÄÇ So we say import Ny S and P„ÄÇ

And nowÔºå how does our underscore predict method look likeÔºå So now again„ÄÇLet's have a look„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_18.png)

At the figure here„ÄÇ So what we do is„ÄÇWe want to calculate all the distances„ÄÇ

And then we have a look at the nearest neighbours and the labels of the nearest neighbors„ÄÇ

 And then we do a maturity vote and choose the most common class label„ÄÇ

 So let's write some comments here„ÄÇ SoÔºå first of allÔºå we want to compute the distances„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_20.png)

ThenÔºå we want to„ÄÇGet the can nearest neighborsÔºå So get Ca„ÄÇNearest samplesÔºå and„ÄÇWe want to get the„ÄÇ

AlsoÔºå want to get the labels„ÄÇ And then we do a maturity mode„ÄÇSo we want to get the most„ÄÇ

Come on class label„ÄÇSo„ÄÇLet's do this„ÄÇ So let's sayÔºå distances equals„ÄÇAnd nowÔºå as I said„ÄÇ

 we use the Euclidean distance here„ÄÇ So let's define this„ÄÇ

 and we want to define this as a global function„ÄÇ So you might want to write this in a separate file or call this in some utility class„ÄÇ

 So here I will simply do it in the same file„ÄÇ So I say„ÄÇË¶Å„ÄÇKiyian„ÄÇThis tense„ÄÇOf two feature vectors„ÄÇ

 So let's say x1 and x 2„ÄÇ And now againÔºå let's have a look at the formula„ÄÇ

 So this is the square root„ÄÇ and then the sum over each square distance„ÄÇ So we have the square root„ÄÇ

 So we can say„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_22.png)

![](img/46ae3dd4f47308ef10799ea81647c68c_23.png)

Nampai dot SÔºå QÔºå R T„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_25.png)

And then we have the sum„ÄÇ so we can use numpy dot sum„ÄÇ So this will calculate the sum for over each„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_27.png)

Feature vector component„ÄÇ And here we have the„ÄÇSquared difference„ÄÇ

 So we can say x 1 minus x2 to the power of 2„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_29.png)

So this is all we need„ÄÇ And we want to return thisÔºå of course„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_31.png)

And now in our predict methods„ÄÇWhat we want to do is„ÄÇ

We want to calculate the distances of this one new sample to all the training samples„ÄÇ

 So we also use list comprehensions hereÔºå and we call this Euclidean distance with our new test sample„ÄÇ

SoÔºå and then to each of the training samples„ÄÇ So we say small x train„ÄÇ

 And then we want to calculate this for„ÄÇExtrain in capitalÔºå or in self dot„ÄÇX train„ÄÇ

So now we have all the distances„ÄÇAnd now we want to get the key nearest samples and the labels„ÄÇ

So what we do here is we sort our distances and„ÄÇWe can do this„ÄÇ So let's call this decay„ÄÇIndices„ÄÇ

And this„ÄÇ And here we use nuumpy dot„ÄÇArk sort„ÄÇSo this will sort the distances and„ÄÇ

Will return the indices„ÄÇOf how this is sorted„ÄÇ So here we call„ÄÇDistances„ÄÇAnd this will be an array„ÄÇ

 And we also wantÔºå we only want to have the K closest samples„ÄÇ So let's use slicing here„ÄÇ

 And let's start at the beginning„ÄÇ So 0„ÄÇ or we can omit this„ÄÇ And this is goes only until self dot K„ÄÇ

 So this will be the„ÄÇIndices of the K nearest samples„ÄÇ And now let's get the labels„ÄÇ So we get the„ÄÇ

Kay„ÄÇNearest„ÄÇLabels equals„ÄÇ And here we can„ÄÇ we also use list comprehensions„ÄÇ

 and then we get the label of each training and labels„ÄÇWith this in the indexÔºå So the index I„ÄÇ

 and then for I in K indcs„ÄÇSo now we have the labels of our nearest neighbors„ÄÇ

 and then we use a maturity vote and get the most common class label„ÄÇ

 So let's call this most common equals„ÄÇ And for thisÔºå we use another Python module„ÄÇ

 So we use the counter module„ÄÇ So we say from collections import„ÄÇCounter„ÄÇAnd then„ÄÇWeii„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_33.png)

Get there„ÄÇOr we get a counter of theÔºå nearest labels„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_35.png)

And then this is a method called most common„ÄÇ And we only want to have the first or the very most common„ÄÇ

ÂóØ„ÄÇAnd now let's have a look how this looksÔºå so„ÄÇIf I comment this out and let's write a short example what the collections or the counter module will do„ÄÇ

 So let's say we have a list A equalsÔºå and this is some values in it„ÄÇ So 1Ôºå1Ôºå1Ôºå2Ôºå2„ÄÇ

 and then some error„ÄÇLabelsÔºå as though also from collections import„ÄÇCounter„ÄÇ

And now we get the most common equalsÔºå so„ÄÇWe create a counter of this listÔºå and then one most common„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_37.png)

So„ÄÇLet's print this„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_39.png)

So„ÄÇI have to„ÄÇClose this first„ÄÇ So let's run this again„ÄÇÂóØ„ÄÇSo this will be a list„ÄÇ

 and then we have a tuple of the most common item„ÄÇ SoÔºå for exampleÔºå if weÔºå in this case„ÄÇ

 we only haveÔºå we want to have oneÔºå the one most common item„ÄÇ and this is a tuple„ÄÇ

 and the first item here is the most common item„ÄÇ So this is one„ÄÇ and the second item is the„ÄÇ

Number of timesÔºå this is in our list„ÄÇ So it three times  one„ÄÇ SoÔºå for exampleÔºå if you use two„ÄÇ

 then it will also put in the second most common items„ÄÇ In this caseÔºå it is2„ÄÇ

 and we have two times 2„ÄÇ So we only want„ÄÇOne most common„ÄÇSo as I saidÔºå this is a list„ÄÇ

 So in order to get the first itemÔºå we use the index 0„ÄÇ

 So now we have the tuple and now to get the actual items againÔºå we use the first index„ÄÇ

 and then we have one„ÄÇSo here we have to do the same thingÔºå and we want to return this„ÄÇ

 So let's return most common of index 0 of index 0„ÄÇ

And this is all our implementation of the K and N algorithm„ÄÇSoÔºå let's try this outÔºå so„ÄÇLet's„ÄÇ

 in our test exampleÔºå we already have our test file„ÄÇ We already have our training and test samples„ÄÇ

 So let's use our„ÄÇOur„ÄÇK and N classÔºå So we can say from K And N import„ÄÇK and N„ÄÇ

 and then create a classifier„ÄÇ So CÔºå F equals„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_41.png)

K and N„ÄÇAnd soÔºå let's use„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_43.png)

K equals 3„ÄÇAnd then we want to call the fit method first„ÄÇ So we want to fit our X train„ÄÇAnd the„ÄÇ

Why train„ÄÇAnd then we get the predictions„ÄÇ And this is„ÄÇBy we do this by calling CF dot predict„ÄÇ

 And then we want to predict„ÄÇOur„ÄÇTest sample samples„ÄÇ So x test„ÄÇAnd now let's calculate the accuracy„ÄÇ

 So accuracy„ÄÇ So this is defined by how many of our predictions„ÄÇAre correctly classified„ÄÇ

So this is the„ÄÇ So here we use the sum„ÄÇÂóØ„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_45.png)

The sum„ÄÇAnd hereÔºå we write„ÄÇPredictions equalsÔºå equals„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_47.png)

Why test„ÄÇSo for each predictionÔºå that is„ÄÇThe right„ÄÇOr the same as the correct labelÔºå it„ÄÇEs1„ÄÇ

And then we divide this by the number of test samples„ÄÇ So we divide this by LngÔºå y„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_49.png)

Test„ÄÇAnd let's print our accuracy„ÄÇ

![](img/46ae3dd4f47308ef10799ea81647c68c_51.png)

And see if this works„ÄÇSoÔºå yeahÔºå in this caseÔºå it's 1„ÄÇ0„ÄÇ So all of our predictions are„ÄÇCorrect„ÄÇ

 so let's use another„ÄÇNumber of neighbors„ÄÇ So K equals 5„ÄÇ

 So usually you want you want to use an odd number here„ÄÇ So let's run this„ÄÇOhÔºå sorry„ÄÇ

 So in this caseÔºå it's 096Ôºå so„ÄÇNot as good as with three neighboursÔºå but also very good„ÄÇ And yeah„ÄÇ

 we see that this is working„ÄÇ And this is our whole implementation of the K And M„ÄÇ And yeah„ÄÇ

 I hope you enjoyed this tutorial and see you in the next tutorialÔºå bye„ÄÇüòä„ÄÇ



![](img/46ae3dd4f47308ef10799ea81647c68c_53.png)