# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P70ÔºöL13.4- ‰ΩïÊó∂ÈáçÊñ∞ËÆ≠ÁªÉÊÇ®ÁöÑÁ•ûÁªèÁΩëÁªú - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff Heaton„ÄÇ Welcome to applications of deep neural Network with Washington University„ÄÇ In this videoÔºå we're going to take a look at howÔºå you know when this neural network that you've moved into production is perhaps not being as efficient as it had been in the past„ÄÇ

 We're going to look at model drift and what to do about it For the latest on my AI course and projects„ÄÇ Click subscribe in the bell next to it to be notified of every new video continuing along with our topic of deployment in this module„ÄÇ We're going to look at when we should retrain a neural network„ÄÇ Now„ÄÇ if you just have a data set like the auto Miles per gallon or the Iis or any of these sample data sets that you download and work with„ÄÇ

üòä„ÄÇ![](img/de3c5a0770101a2c095d10c9f95e8614_1.png)

You're not going to know really when you need to retrain your model„ÄÇ For example„ÄÇ the miles per gallon data set that we've worked with a number of times it has stats on cars that help you determine what the miles per gallon is„ÄÇ most of the data from this data set is from the late 1970s and early 1980s„ÄÇ Yes„ÄÇ you almost certainly need to retrain that model from modern data„ÄÇ but for that closed data set„ÄÇ

 the model is as good as it's ever going to get„ÄÇ The problem is that as time marches on changes occur in the distributions of your data„ÄÇ and your incoming new data that you're trying to get real scores for is different„ÄÇ And this is a very common problem that you need to deal with as you develop models that are going to be used in real business ongoing applications„ÄÇ I work in the life insurance industryÔºå and I'll give you an example of it for this„ÄÇ

 So as your data for an individual So we're trying to determine what the risk of ensuring„ÄÇIndividual for life insurance isÔºå so that's the risk essentially of them dying mortality risk we often call it if the person is really fairly average in terms of the input data that you're giving on they're not a smoke or their average height their average weight or average build your neural network is going to tend towards giving predictions that are really pretty average for this person Well average tends to change as you look at health trends over the years„ÄÇ

 mortality is improving more and more people tend to be living towards advanced years„ÄÇ the advanced year is still relatively in the same range that it has been for a while„ÄÇ but more people are living to that high end of that range and also smoking is declining all these things together„ÄÇ change over time and this affect with the ground truth so to speak is that your neural network predicting towards so as this ground truth moves you need to potentially retrain your neural network with newer data every„ÄÇ

F years look at how you determine when this is happening„ÄÇ Now„ÄÇ when you will see this in a closed dataset„ÄÇ So a dataset where you are given the data and you're never going to see any new data for this ever again„ÄÇ That's not a realistic situation in business when you're doing this in the real world„ÄÇ Usually you're building a model because you expect new data if you didn't expect new data„ÄÇ

 why would you build the model in the first placeÔºå predicting closed data sets is interesting„ÄÇ but it lacks a lot of practical application NowÔºå the only time when you will use these techniques on a closed data is where you have a defined test and training set„ÄÇ

 Some of the data sets that we've seenÔºå particularly some of the academic ones„ÄÇ they define what your training set looks like and what your test looks like„ÄÇ So you want to look at has any shift occurred between your training in your test„ÄÇ This is often done in kle„ÄÇ So let's look at this diagram here„ÄÇ

 This is a diagram that appears I gave„ÄÇsource to it„ÄÇ it's from a paper written specifically on looking at data and covariance shifts„ÄÇ These are to names for this sort of phenomenonÔºå somewhat different„ÄÇ but the green„ÄÇ the learned function„ÄÇ So this is using a very linear model„ÄÇ

 So you'll have a lot of bias in this becauses very linear modelÔºå but that's okay the true function„ÄÇ So what really it should be learning is this red„ÄÇ but look what happens essentially over time„ÄÇ And unfortunatelyÔºå the test samples occur a little bit later in time than the blue training samples„ÄÇ So you learn the function on the blue dots„ÄÇ But the reality is and even the true function you don't have much representation of it over here„ÄÇ

 you have no representation at all hardly here„ÄÇ So you're going learn this very linear function„ÄÇ Now there's definitely some noise„ÄÇ nothing's right on the linear function hardly„ÄÇ But nonetheless this is trying to minimize your residual so that the distances between every dot and the line is about averaged„ÄÇthe dots above and the dots below„ÄÇ But then when you get out here„ÄÇ

 you get to a different part of this function„ÄÇ for some reason in time„ÄÇ usually it's some variable that you're simply not capturing or you can't capture„ÄÇ now the trend is more this direction so you probably need to collect more data and reffi it and use a nonlinear model so that you can get this curvature built into your model and get more predictive data„ÄÇ because if if you look at where this red line is now goingÔºå who knows is it going continue down„ÄÇ

 maybe it's going to retake this shape here„ÄÇ And if it does„ÄÇ then our first model would not be so bad„ÄÇ but you need probably something to look at that new segment in time that the ground truth has now shifted for„ÄÇ So how do we measure this drift„ÄÇ I give you a whole bunch of different techniques for this„ÄÇ I'm not going to go through every one of these in the course„ÄÇ

 but just to give you an initial sort of literature review of those those are some of the ones to potentially look at this is a particularly good paper„ÄÇifying view and data shift This tries to look at really all of these and come up with the commonalities of those„ÄÇ

 Now to look at this since it's difficult really for me to do a inclass example where we're getting a training set and then collecting new data and continue thought about doing this with the autompG data and we might get into this So we're going to use a Cagel data and we're going to analyze and see how different the distributions of some of these predictors change between the training set and the test set this becomes an important consideration when you're competing and a Cagel„ÄÇ

 So we're going to use the sharebank Russian housing market data set just to show you that kgle it is essentially looking at can you predict Real price fluctuation in Russia's volatile market if you look at the data They give you a whole bunch really of input values here„ÄÇ

 they scroll across„ÄÇ So those are all the columns in the distributions„ÄÇ We're going to be looking at those there are„ÄÇHred total columns„ÄÇ So it's got a lot of columns that you're dealing with„ÄÇ I always like to go to the overview and see what the evaluation is„ÄÇ

 They're using RMSLE RMS logithmic error„ÄÇ This is it's a regression error„ÄÇ It's not a real common one„ÄÇ And I had to go to the Cagel forms actually to find this The link on the competition actually results in a 404 page not found According to Cagel on definition from this RMSL penalizes under predicted estimate greater than an over predicteddicted„ÄÇ

 So its it's like RMS more log scale and then really not going to deal with it a great deal because we are just looking at the shift between the train in the test„ÄÇ So we won't even really look at the target„ÄÇ So the evaluation for this cale is really not important for the example that I'm going to give you Now you will have to download this data because it comes from Cale„ÄÇ

 I can't just build it into the course getthub„ÄÇ So I'm going to run this„ÄÇ I already had my„ÄÇ![](img/de3c5a0770101a2c095d10c9f95e8614_3.png)

Train and test loaded„ÄÇ I'm literally pulling it right out of the downloads directory on my math„ÄÇ That's a temporary location for meÔºå but it does what I what I wanted to do„ÄÇ There's a lot of examples on this particular kggle of how to preproces the data I do a fairly basic preprocessing So if it's an object type that means that it's some sort of a category So I am going to fill Ns with the mode or the most common value„ÄÇ if it's an integer afloat then we're going to fill in the media media is good to use over mean because medium is less sensitive to outliers„ÄÇ

 Then we're going to go through and label and code essentially all of the categorical So we're not creating dummies we're actually label encoding them so that we end up with a integer value specifying what location they are„ÄÇ I won't get into why that was actually chosenÔºå but these were the common encodings that were used in this particular competition„ÄÇ

 I believe they shied away from dummies because they were simply too many dimensions„ÄÇ So we'll run this so those two functions are defined or that one function is defined and then we'll preprocess the data„ÄÇWe're going to drop the target because we're not trying to predict at this point next„ÄÇ we're going to calculate something called the chaoss statistic„ÄÇ NowÔºå when I first heard this„ÄÇ

 I thought of chaos statistic like COS„ÄÇ but this is the chaos Ks statistic It is essentially looking at how similar are the distributions between two things„ÄÇ So let's just do a sanity check„ÄÇ If I just run this one„ÄÇ I am saying what is the chaos statistic between the kitchen square feet and the kitchen square feet„ÄÇ So what is the chaos statistic between itself„ÄÇ So P valueÔºå it has to be below a certain threshold„ÄÇ

 So this is not below 0„ÄÇ05 This is very high„ÄÇ So this means that it's very unlikely that there are any differences between these two distributions„ÄÇ YeahÔºå because they're the same And then the statistic here„ÄÇ negative0 math majors always love negative zerosÔºå but computer scientists have them„ÄÇ So this is 0 there is no difference between the two values and sense the P value is is very high that„ÄÇ

Means the null hypothesis really cannot be rejected in this case Now let's look at the same column„ÄÇ but we're looking at the column in the training set versus the test set„ÄÇ Now the chaos statistic is a little different„ÄÇ our P value is very low0 and the statistic shows us that there's a 0„ÄÇ25 difference between them So this is interesting that the distribution of the kitchen square feet is quite different between the training set and the test set„ÄÇ

 the next thing we're going to do is essentially run this on all the columns so we'll run a chaos statistic on everything willll only report the times where the p value is less than 0„ÄÇ05 and the case statistic is greater than 0„ÄÇ1 Now the statistic this is somewhat tied to the units of the measurement so you have to look at these really relative to everything else in the values that you're actually measuring and as it runs through you can see this works better if I shrink the font size but then it's harder to read but the show„ÄÇ

All of the columns and the ones that had substantial enough differences„ÄÇ Next„ÄÇ we're going to look at how to detect the drift„ÄÇ Now„ÄÇ this is a very interesting technique that I've seen in a number of kggles„ÄÇ What we're going to do is simply sample the training test set into smaller sets„ÄÇ

 we're going to take these data sets and essentially add in another column that you have down here that tells us where they came from„ÄÇ We're going to see if we can fit a modelÔºå a random forest in this case that can predict when all the data are jumbble back together„ÄÇ

 can it predict or an individual itemÔºå if it came from the test set or the training set„ÄÇ If these are truly uniform random sampled value So there is no difference between the training set and the test set„ÄÇ we simply divided it that wayÔºå you should not be able to predict where an individual row came from if you can predict it„ÄÇ then theres there's differences in the distributions there that really help it to lock on„ÄÇ

What set it actually came from„ÄÇ Let's go ahead and just run this so we can see sort of what it looks like„ÄÇ We're gonna label the two data sets„ÄÇ We're going combine them together and re randomdomize them„ÄÇ Now we need to break out the X and Y so that we're ready to train„ÄÇ This is a classification„ÄÇ So we're using area under the curve„ÄÇre considering anything above 0„ÄÇ75„ÄÇ that's not a real good AU„ÄÇ

 but it's a good enough A you that it's breaking that it's getting onto something that it can really determine what that is„ÄÇ We're going to use each of the columns so that we can evaluate the errors differently„ÄÇ we can see how good each of those columns is it actually predicting it„ÄÇ So we're using the columns one by one to form that prediction„ÄÇ And as we start to run this„ÄÇ

 we'll see different ones that really vary„ÄÇ Now some of this really make sense„ÄÇ The I you're not going to be using the I anyway to predict the I is ever increasing„ÄÇ So yeah„ÄÇ it's going to be very different between the train and test set„ÄÇ same thing for timets„ÄÇ especially if they were taken„ÄÇAt different regions in time„ÄÇ

 So the fact that the timestamp is that differentÔºå is that predictive means that it's probably not uniformly sampled across the full time frame and we see the other values here that are also quite able to be predicted if they're in the training versus the test set this can be very useful information in a kle that you know how to balance this to some degree„ÄÇ

 but it's also very useful if you were collecting new data coming in„ÄÇ you don't need the outcome„ÄÇ you do not need the target for your new dataÔºå you can compare it to your original data set and see if a random forest is able to predict if it's old data of its new data„ÄÇ

 if the random forest can predict with decent accuracy like 87 Auc here„ÄÇ then the underlying data has probably shifted and you need to retrain your model„ÄÇ This content changes often„ÄÇ So subscribe to the channel to stay up to date on this course and other topics in artificial intelligence„ÄÇ

![](img/de3c5a0770101a2c095d10c9f95e8614_5.png)