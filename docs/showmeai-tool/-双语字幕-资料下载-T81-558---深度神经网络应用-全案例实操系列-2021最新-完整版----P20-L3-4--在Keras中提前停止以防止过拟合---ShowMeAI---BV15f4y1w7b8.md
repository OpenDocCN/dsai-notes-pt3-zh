# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëT81-558 ÔΩú Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÂ∫îÁî®-ÂÖ®Ê°à‰æãÂÆûÊìçÁ≥ªÂàó(2021ÊúÄÊñ∞¬∑ÂÆåÊï¥Áâà) - P20ÔºöL3.4- Âú®Keras‰∏≠ÊèêÂâçÂÅúÊ≠¢‰ª•Èò≤Ê≠¢ËøáÊãüÂêà - ShowMeAI - BV15f4y1w7b8

HiÔºå this is Jeff HaytonÔºå welcome to App of Deep neuralural Networks with Washington University„ÄÇIn this videoÔºå we're going to see our first technique for fighting against the constant enemy of the neural network programmer„ÄÇ And that's overfitting„ÄÇ We're going to see how to use early stopping„ÄÇ rather than trying to decide how many epochs„ÄÇ you want to train your neural network 4„ÄÇ

 you can set aside a validation set and train the neural network until that validation set No longer the error on it no longer improves for the latest or among AI course and projects„ÄÇ Click subscribe in the bell next to it to be notified of every new video„ÄÇ

 Earl stopping is a method that helps to prevent overfitting„ÄÇ We'll see other methods to help prevent overfitting as well as we progressed this class„ÄÇüòä„ÄÇ![](img/4c43fcf66b6cd3d24858fb5574a8cd57_1.png)

But what early stopping really tries to do is keep you from training the neural network too far„ÄÇ This is a common visualization to show what ill effects overfitting can have as you train more and more„ÄÇ X axis shows you the error„ÄÇ The Y axis shows you the training time„ÄÇ The dashed line is the validation set and the black line is the training set„ÄÇ

 So what is happening to the error„ÄÇ of your training values versus your validation values as you train longer and longer and longer„ÄÇ GeneralÔºå the validation and training will be pretty similar„ÄÇ The validation might be a little bit worse as you begin to train because„ÄÇThe training set is going to be somewhat overfit because the neural network has those values available„ÄÇ

 It's memorizing some of them„ÄÇ It's fitting very well to the training set„ÄÇ So the validation set will usually be a little bit worse„ÄÇ as you're progressing„ÄÇ Then as the training set gets better and better and better at some point„ÄÇ the validation set will start to almost converge away from this to a higher error value„ÄÇ

 And you'll get worse and worse values on your validation„ÄÇ you'll get better and better values on your training„ÄÇ This is result of overfitting„ÄÇ at this point hereÔºå overfitting really started to occur„ÄÇ Now„ÄÇ you may not recognize it till somewhere out here„ÄÇ because you have to thinkÔºå okayÔºå it's„ÄÇ

 it's getting worse„ÄÇ Maybe it'll get better„ÄÇ Maybe it'll get better„ÄÇ Now it's probably not getting not getting better„ÄÇ NowÔºå there are cases where you'll go out this far„ÄÇ And then itll it'll come back down here„ÄÇ This is a value you will see in this class called patience„ÄÇ How patient are you„ÄÇTo see these values start to move towards convergence again„ÄÇ

 rather than diverge away into validationÔºå just overfitting andÔºå and not getting a better value„ÄÇ This is what early stopping is„ÄÇ You stop early„ÄÇ You stop at this point„ÄÇ and we'll see that it's important that we capture the weights here where we had„ÄÇEssentially„ÄÇ the best or the lowest validation errorÔºå which is probably right in here where we had the lowest validation error„ÄÇ

And then we save those weights so that we have them so that even when we decided to stop„ÄÇ maybe here or hereÔºå depending how patient we wereÔºå we go back to here„ÄÇSo that's an important thing to rememberÔºå tooÔºå about patients„ÄÇ patients will really not hurt you on the overall best score that you'll get at the end„ÄÇ

 you can be as patient as you want„ÄÇ It's just going to take a lot longer„ÄÇTo train because you're going to have to go much further out if you set a gigantic patience number of epochs that you're willing to wait„ÄÇ NowÔºå to do thisÔºå we need to segment the original data set into several data sets„ÄÇ Now„ÄÇ Often we'll just have the first twoÔºå but sometimes we'll have a holdout set as well„ÄÇ

 You may even split it up into more than just these threeÔºå depending really on what you're doing„ÄÇ It also depends on how much how much data you have„ÄÇ If you have a lot of data„ÄÇ then you may split this into many training sets and really ensure that you're not overfitting that you're not ever evaluating based on anything that you are trained on„ÄÇYou can make this very pureÔºå and you may even split out a separate validation set just to be used for nothing other than early stop„ÄÇ

 but every time you split theseÔºå you lose some of your training data„ÄÇ So it's it's a trade off between how purest do you want to be in terms of your„ÄÇNever crossing training and validationÔºå never validating on anything that the neural network is trained on„ÄÇ how much of your training set you want to train on because your neural network will typically perform better„ÄÇ

 at least up to a pointÔºå the bigger the training set that you give it„ÄÇIn kle competitions„ÄÇ for example„ÄÇI will usually accept some small cross contamination of training and validation„ÄÇ meaning I might use a validation set for early stopping„ÄÇJust because I know that I have that holdout set that Kaggle will eventually give me„ÄÇ

I can't do that too much or I overfit to even Kaggles's holdout set to some degree„ÄÇSo it's really just a trade off and we'll talk more about this as we get into hyperparameter tuning and other things where you do need to create additional set„ÄÇ The ultimate insurance policyÔºå thoughÔºå is you set that final holdout set that you keep until the very end and you don't you don't use this for anything during the training process until you say„ÄÇ okayÔºå I am I'm very happy with my modelÔºå let me see what happens when I„ÄÇTry the final„ÄÇ

 final holdout set„ÄÇ And that'sÔºå that's the error rate that you usually decide to commit to as far as how„ÄÇHow well trained your model actually is„ÄÇSo this is what it really looks like„ÄÇIn terms of something like an early stopping„ÄÇWhat you're going to do is take your data set„ÄÇBreak it into maybe 80% and 20% this 20% up here becomes the validation„ÄÇ

 the 80% becomes the training„ÄÇAnd you're going to fit or train your model on this 80%„ÄÇ and then you're going to validate it on this 20%„ÄÇSo you'll get a training error here„ÄÇ That is how well it was fit to your training setÔºå but then you take what you trained on that 80% and validate it with that remaining 20%„ÄÇ and that gives you greater confidence that this is truly the error that your neural network is going to experience when it sees new data„ÄÇ

 Now we're going to look at early stopping and early stopping does require a sort of validation set„ÄÇNow that causes your validation set to now be used for early stopping so now it's part of the training set„ÄÇ so this is where it's good to have that final holdout set that you might want to ultimately evaluate on just be aware of that if you're truly using your validation set as your early stopping„ÄÇ then you probably need another validation set to actually commit to an error on or you can accept that maybe you've got some inflation in your validation set and you can truly use whatever score you stopped on as an estimate of what the final neural network might do„ÄÇ

 but be aware that you have crossed at that pointÔºå your early stopping validation set to your training„ÄÇIs what we're going to do is just monitor this„ÄÇ And as soon as that validation setÔºå no longer„ÄÇIs decreasing and our patience expires„ÄÇ We're going to stop„ÄÇ We're going to early stop„ÄÇ I'm going to show you two examples of how you will do this„ÄÇ

We'll do early stopping with classification and early stopping with regression„ÄÇ The code's mostly the sameÔºå but this gives you some good starting point code to look at for this„ÄÇ So for this oneÔºå we're going to use the iris data set„ÄÇ So we the supple linked width„ÄÇAnd pal length and width„ÄÇLet's go ahead and run this so that we load the data in„ÄÇ

Since as the star goes awayÔºå it's loadedÔºå okayÔºå the data is loaded„ÄÇAnd we we ran it„ÄÇ And you can see the results here„ÄÇ We'll look at the results first„ÄÇ And then I'll show you kind of what happened„ÄÇ We trained and trained and trained„ÄÇ You can see the training of validation loss„ÄÇ And then it says that right around 101„ÄÇ

 we decided to early stopÔºå and we restore the model weights of that last best epoch„ÄÇ And we set up our neural networkÔºå largely just like we did before„ÄÇIt is going to have its classificationÔºå so we're using the Y shape„ÄÇ we're using the number of classes and soft Mac and categorical cross entropy„ÄÇ

 those three together tell you that this is a classification neural network„ÄÇ make sure you set it up like that„ÄÇWe're creating a monitor now this is the part that does the early stopping„ÄÇWe are telling it that we have a patience of five„ÄÇ so we're going to only wait  five epochs for to quit improving„ÄÇ verbose is one„ÄÇ So that'll give us that little notice that we saw down there„ÄÇ

 and we're telling it trulyÔºå restore the best weights„ÄÇ And now we're going to fit it„ÄÇ But notice we're giving it X train Y train„ÄÇ and we're passing in validation data„ÄÇ We haven't done this before„ÄÇ So the validation data„ÄÇ The X test Y test versus the X train Y train„ÄÇ that came from up here„ÄÇ We're splitting into validation and training sets„ÄÇ

 So we're splitting 25% is going to the test set„ÄÇ The remaining 75% is going to validation„ÄÇ So you have X trainÔºå X test Y train„ÄÇ Those are the expected valuesÔºå the Y's„ÄÇAnd why test„ÄÇ So now you have the four measurements split into a train and a test„ÄÇ and you have the expected classÔºå the type of ir that it actually was split into„ÄÇTrain and test„ÄÇ

The random state 42Ôºå that just gives us a way to consistently split it„ÄÇ and then the only other thing we do un fit down here is we pass a call backÔºå monitor„ÄÇ so that's how this early stopping object that we created actually plugs into the training„ÄÇOr the fit process when we call fit as we train„ÄÇ So this is how you implement early stopping for classification„ÄÇ

 It's pretty much the same exact process for regression„ÄÇ You create a monitor„ÄÇ you pass it into the callbacks„ÄÇ and you need to provide validation data„ÄÇ I just give you the exact code for this so that you can quickly make use of it„ÄÇ By the way„ÄÇ if we do want to see the accuracy with that we got for early stopping„ÄÇ

 notice the accuracy is very good„ÄÇüòäÔºåSo we didn't have to guess at the number of epochs„ÄÇ It trained until it saw that„ÄÇWe had trained enough„ÄÇ so this is a way that you can estimate and not have to guess at how many epochs you want to give your neural network„ÄÇThis is a way to have one fewer hyperparameterÔºå hyperparameters„ÄÇ

 all these things you have to pick about your neural network that will influence behavior like the number of epochs„ÄÇ how many layers you haveÔºå we'll see how to pick layers and other things automatically later in this class„ÄÇ but this is one way that you can at least„ÄÇUse mathematical accuracy to some degree to pick your number of epochs to train and a simple example of early stopping with regression„ÄÇ we use the auto miles per gallon„ÄÇ This is the same setup that we've done before„ÄÇ

 We're splitting it into x and Y so that y is the miles per gallon„ÄÇ That's what we're trying to predict X is all these other values that we're using to predict miles per gallon„ÄÇAnd then we use exactly the same code that we used in the previous one for classification„ÄÇ We split it into train and testÔºå 25% split„ÄÇNow we're doing regression though„ÄÇ

 so notice we have one output in neuron„ÄÇWe have mean squared error that tells you that this is a regression neural network„ÄÇ and this is all the same we're giving we're passing the monitor in„ÄÇAnd we're giving it X and Y„ÄÇBoth for validation and for training„ÄÇ So you're giving it that two sidesÔºå the training„ÄÇAnd the validation„ÄÇ And now we can run it„ÄÇAnd it trainsÔºå trains trains„ÄÇ You can see the testing„ÄÇ

The the train„ÄÇLoss and the validation loss both are decreasing„ÄÇAnd at some pointÔºå it decides to stop„ÄÇIt restores the final weightsÔºå so it only made it to 31 epochs before it lost patients here„ÄÇThen you can measure the final root mean square„ÄÇNot a particularly good root mean square„ÄÇ12 now„ÄÇ rememberÔºå neural networks start with random weights„ÄÇ

 So this gives me a chance to quickly illustrate something interesting„ÄÇ Remember Ro mean square was 12„ÄÇ Let's retrain the whole thing„ÄÇ NowÔºå let's recalculate this„ÄÇ4our„ÄÇNotice that huge varianceÔºå this is really annoying about neural networks„ÄÇ but since they start with random weightsÔºå they will get sometimes different error values„ÄÇ

 This makes it very hard to try to tune something„ÄÇ Imagine if you had thrown in an extra hidden layer„ÄÇWhen you did have 12Ôºå but then you throw in a hidden layer oh my gosh I went down to four„ÄÇ that's greatÔºå no it was just the normal variance you have in neural networks„ÄÇWe'll see in the next module that something called bootstrapping will use„ÄÇ

 you're just literally going to have to run this a few times an average together„ÄÇIs how you handle that and things like„ÄÇDrop outÔºå help mitigate this at least a little bit„ÄÇThis is just one of the facts of life dealing with neural networks„ÄÇ They give a great deal of variance in in the score that you get from the output„ÄÇ



![](img/4c43fcf66b6cd3d24858fb5574a8cd57_3.png)

Thank you for watching this video On early stopping„ÄÇ In the next part„ÄÇ we're going to see how to actually extract the weights from a saved Kara's neural network and put those weights into a mathematical equation and actually calculate the values„ÄÇ This removes all the magic from the processÔºå And you can see that it's simply mathematical calculations„ÄÇ This content changes often„ÄÇ So subscribe to the channel to stay up to date on this course and other topics and artificial intelligence„ÄÇ

üòä„ÄÇ