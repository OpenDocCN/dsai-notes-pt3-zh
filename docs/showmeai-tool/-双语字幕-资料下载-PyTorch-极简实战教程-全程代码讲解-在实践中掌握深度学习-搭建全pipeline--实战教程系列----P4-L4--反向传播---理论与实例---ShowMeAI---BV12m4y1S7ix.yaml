- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëPyTorch ÊûÅÁÆÄÂÆûÊàòÊïôÁ®ãÔºÅÂÖ®Á®ã‰ª£Á†ÅËÆ≤Ëß£ÔºåÂú®ÂÆûË∑µ‰∏≠ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†&Êê≠Âª∫ÂÖ®pipelineÔºÅÔºúÂÆûÊàòÊïôÁ®ãÁ≥ªÂàóÔºû - P4ÔºöL4- ÂèçÂêë‰º†Êí≠
    - ÁêÜËÆ∫‰∏éÂÆû‰æã - ShowMeAI - BV12m4y1S7ix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HiÔºå everybody„ÄÇ Welcome to a new Pytorch tutorial„ÄÇ In this video„ÄÇ I am going
    to explain the famous Beck propagation algorithm and how we can calculate gradients
    with it„ÄÇüòäÔºåI explain the necessary concepts of this technique„ÄÇ and then I will
    walk you through a concrete example with some numbers„ÄÇAnd at the end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we will then see how easy it is to apply back propagation in Pytorarch„ÄÇ So let's
    start„ÄÇAnd the first concept we must know is the chain rule„ÄÇ So let's say we have
    two operations or two functions„ÄÇSo firstÔºå we have the input X„ÄÇ and then we apply
    a function A and get an output Y„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we use this output as the input for our second function„ÄÇ So the second
    function B„ÄÇ And then we get the final output C„ÄÇAnd now we want to minimize our
    C„ÄÇ So we want to know the derivative of C with respect to our x here in the beginning„ÄÇAnd
    we can do this using the so called chain rule„ÄÇ So for this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we first compute the derivative of C with respect to y and multiply this with
    the derivative or of y with respect to X„ÄÇ And then we get the final derivative
    we want„ÄÇSo firstÔºå here„ÄÇ we compute the derivative at this position„ÄÇSo the derivative
    of this output with respect to this input„ÄÇ And then hereÔºå the derivative of this
    output with respect to this input„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we multiply them together and get the final gradient we are interested
    in„ÄÇ So that's the chain rule„ÄÇAnd now the next concept is the so called computational
    graph„ÄÇSo for every operation we do with our tens sourceÔºå Pyto will create a graph
    for us„ÄÇ So where at each noteÔºå we apply one operation or one function with some
    inputs and then get an output„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here in this caseÔºå in this exampleÔºå we use a multiplication operation„ÄÇ So
    we multiply x and y and then get C„ÄÇAnd now at these notes„ÄÇ we can calculate so
    called local gradientsÔºå and we can use them later in the chain rule to get the
    final gradient„ÄÇ So hereÔºå the local gradientsÔºå we can compute two gradientsÔºå the
    gradient of C with respect to x„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is simple„ÄÇ since we know this function here„ÄÇ So this is the gradient
    gradient of x times y with respect to xÔºå which is y„ÄÇ And here in the bottom„ÄÇ we
    compute the derivative of x times y with respect to yÔºå which is x„ÄÇSo local gradients
    are easy because we know this function„ÄÇ And why do we want them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Because typically our graph has more operations„ÄÇ And at the very end„ÄÇ we calculate
    a loss function that we want to minimize„ÄÇ So we have to calculate the gradient
    of this loss with respect to our parameter X in the beginning„ÄÇAnd„ÄÇNowÔºå let's suppose
    at this positionÔºå we already know the derivative of the loss with respect to our
    C„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then we can get the final gradient we want„ÄÇ So the with the chain rule„ÄÇ
    So the gradient of the loss with respect to X is then the gradient of loss with
    respect to C„ÄÇTimes our local gradient„ÄÇ So the derivative of C with respect to
    x„ÄÇAnd yeah„ÄÇ this is how we get the final gradient thenÔºå and„ÄÇNowÔºå the whole concept
    consists of three steps„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So firstÔºå we do a forward pass where we apply all the functions and compute
    the loss„ÄÇThen at each noteÔºå we calculate the local gradients„ÄÇ and then we do a
    so called backward pass where we compute the gradient of the loss with respect
    to our weights or parameters using the chain rule„ÄÇSo these are the three steps
    we are going do„ÄÇ And now we look at a concrete example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here we want to use linear regression„ÄÇAnd if you don't know how this works„ÄÇ
    then I highly recommend my machine learning from scratch tutorial about linear
    regression„ÄÇI will put the link in the description„ÄÇSo basically„ÄÇ we model our output
    with a linear combination of some weights and an input„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So our y hat or y predicted is w times x„ÄÇAnd then we formulate some loss function„ÄÇ
    So in this case„ÄÇ this is the squared error„ÄÇ ActuallyÔºå it should be the mean squared
    errorÔºå but for simplicity„ÄÇ we just use the squared error„ÄÇ OtherwiseÔºå we would
    have another operation to get the mean„ÄÇ So the loss is the difference of the predicted
    y minus the actual yÔºå and then we square it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we want to minimize our loss„ÄÇ So we want to know the derivative of the
    loss with respect to our weights„ÄÇ And how do we get thatÔºå So we apply our three
    steps„ÄÇ First„ÄÇ we do a forward pass and put in the x and the W„ÄÇ And then here we
    put in the y„ÄÇAnd apply our functions here„ÄÇ And then we get the loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Then we calculate the the local gradients at each note„ÄÇ So here„ÄÇ the gradient
    of the loss with respect to our S„ÄÇ Then here„ÄÇ the gradient of the S with respect
    to our y hat„ÄÇAnd here at this note„ÄÇ the gradient of Y hat with respect to our
    W„ÄÇAnd then we do a backward pass„ÄÇ So we start at the end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we have the firstÔºå we have the derivative of the loss with respect
    to our S„ÄÇAnd then we use themÔºå and we also use the chain rule to get the derivative
    of the loss with respect of the Y hat„ÄÇAnd then againÔºå we use this and the chain
    rule to get the final gradient of the loss with respect to our W„ÄÇSo let's do this
    with some concrete numbers„ÄÇ So let's say we have x and y is given„ÄÇ So x is  one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and y is 2 in the beginning„ÄÇAnd so these are our training samples„ÄÇ and we initialize
    our weight„ÄÇ So let's sayÔºå for exampleÔºå we say our w is one in the beginning„ÄÇAnd
    then we do the forward pass„ÄÇ So here at the first nodeÔºå we multiply x and W„ÄÇ So
    we get Y hat equals one„ÄÇThen at the next note„ÄÇ we do a subion„ÄÇ So y hat y is 1-2
    equals -1„ÄÇAnd at the very endÔºå so we square our S„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we have have S squaredÔºå so„ÄÇOur lossÔºå thenÔºå is one„ÄÇAnd now we calculate the
    local gradient„ÄÇ So at the last noteÔºå we have the gradient of the loss with respect
    to S„ÄÇ And this is simple because we know the function„ÄÇ So this is the gradient
    of S squared„ÄÇ So this is just 2 S„ÄÇAnd then at the next noteÔºå we have the gradient
    of S with respect to Y hat„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is the gradient of the function Y hat minus y with respect to Y hatÔºå which
    is just one„ÄÇAnd then here at the last nodeÔºå we have the derivative of Y hat with
    respect to W„ÄÇ So this is the derivative of„ÄÇW times x with respect to wÔºå which
    is x„ÄÇAnd also„ÄÇ notice that we don't need to go„ÄÇ Don't need to know the derivatives
    in this„ÄÇGraph lines„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we don't„ÄÇNeed to know what is the derivative of S with respect to Y„ÄÇ And
    also here„ÄÇ we don't need this because our x and our y are fixed„ÄÇ So we are only
    interested in our parameters that we want to update here„ÄÇAnd yeah„ÄÇ and then we
    do the backward pass„ÄÇ So firstÔºå now we use our local gradients„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we want to compute the derivative of the loss with respect to Y hat„ÄÇ And
    here we use the chain rule with our two local gradients that we just computed„ÄÇ
    which is 2 S times 1„ÄÇ and S is -1Ôºå which we calculated up here„ÄÇ And then so this
    is  -2„ÄÇAnd now we use this derivative and also this local gradient to then get
    the final gradient„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the gradient of the loss with respect to our WÔºå which is the gradient of the
    loss with respect to Y hat times the gradient of Y hat with respect to W„ÄÇ which
    is -2 times x and x is 1„ÄÇ So the final gradient is -2„ÄÇSo this is the final gradient
    then that we not want to know„ÄÇ And yeah„ÄÇ that's all how back propagation works„ÄÇ
    And let's jump over to our code and verify that Pyto get these exact numbers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's remember x is 1 Y is 2 and w is 1„ÄÇ And then our first gradient should
    be -2„ÄÇ![](img/84bf83ae2a8c01617b4cf06b2bc402cb_1.png)
  prefs: []
  type: TYPE_NORMAL
- en: So„ÄÇ![](img/84bf83ae2a8c01617b4cf06b2bc402cb_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can use this in pie torch„ÄÇ AndÔºå first of allÔºå we import torchÔºå
    of course„ÄÇThen we create our vector or tenor„ÄÇ So we say x equals torch dot tenor„ÄÇ
    and this is1„ÄÇ and then our y equals torch dot tenor„ÄÇWith two„ÄÇAnd then our initial
    weight is a tenzor„ÄÇAlso„ÄÇ with one„ÄÇSo 1„ÄÇ0 to make it a float„ÄÇ And hereÔºå and with
    our weight„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we are interested in the gradient„ÄÇ So we need to specify require Sc equals true„ÄÇAnd
    then we do the forward pass„ÄÇAnd gets and compute the loss„ÄÇSo we simply sayÔºå why
    hat equals„ÄÇW times xÔºå which is our function„ÄÇ And then we say loss equals y„ÄÇHats„ÄÇMinus„ÄÇThe
    actual y„ÄÇ And then we square thisÔºå So we say„ÄÇThis to the power of two„ÄÇAnd nowÔºå
    let's print our loss„ÄÇAnd see„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is one in the beginning„ÄÇ And now we want to do the backward pass„ÄÇ So let's
    do the backward pass„ÄÇ and pi touch will compute the local gradients automatically
    for us and also computes the backward pass automatically for us„ÄÇ So the only thing
    that we have to call is say loss backward„ÄÇ So this is the whole gradient computation„ÄÇ
    And now our w has this dot Gr attribute„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we can print this„ÄÇ And now this is the first gradient„ÄÇ in the after the
    first forward and backward pass„ÄÇ And rememberÔºå this should be-2 in the beginning„ÄÇ
    And here we see we have a tensor with -2„ÄÇ So this is working„ÄÇAnd the next steps
    would be„ÄÇ for exampleÔºå Now we update our weightsÔºå and then we do the next forward
    and backward pass and do this for a couple of iterations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And yeahÔºå that's how Beck propagation works and howÔºå and also how easy it is
    to use it in Pytorch„ÄÇ And I hope you enjoyed this tutorial„ÄÇ Please subscribe to
    the channel and see you next timeÔºå bye„ÄÇüòä„ÄÇ![](img/84bf83ae2a8c01617b4cf06b2bc402cb_5.png)
  prefs: []
  type: TYPE_NORMAL
