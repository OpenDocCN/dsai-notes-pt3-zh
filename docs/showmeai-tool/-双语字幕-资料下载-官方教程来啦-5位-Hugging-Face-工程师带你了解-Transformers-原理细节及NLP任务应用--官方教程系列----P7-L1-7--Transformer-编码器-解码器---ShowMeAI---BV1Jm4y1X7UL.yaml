- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P7ï¼šL1.7- Transformerï¼šç¼–ç å™¨-è§£ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this videoï¼Œ we'll study the encoder decoder architectureã€‚An example of a
    popular encoder decoder model is T5ã€‚In order to understand how the encoder decoder
    worksã€‚ we recommend you check out the videos on encoders and decoders as the standalone
    modelsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how they work individually will help understanding how an encoder
    decoder worksã€‚Let's start from what we've seen about the encoderã€‚The encoder takes
    words as inputsã€‚ casts them through the encoderï¼Œ and retrieves a numerical representation
    for each word cast through itã€‚We now know that this numerical representation holds
    information about the meaning of the sequenceã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Let's put this aside and add the decoder to the diagramã€‚In this scenarioã€‚ we're
    using the decoder in a manner that we haven't seen beforeã€‚We're passing the outputs
    of the encoder directly to itã€‚Additionally to the encoder outputsã€‚ we also give
    the decoder a sequenceã€‚When prompting the decoder for an output with no initial
    sequenceã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we can give it the value that indicates the start of a sequenceã€‚ğŸ˜Šã€‚And that's
    where the anchor decor magic happensã€‚ğŸ˜Šï¼ŒThe encoder accepts a sequence as inputã€‚It
    computes a prediction and outputs a numerical representationã€‚ğŸ˜Šã€‚Then it sends that
    over to the decoderã€‚It hasï¼Œ in a senseï¼Œ encoded that sequenceã€‚And the decoderã€‚
  prefs: []
  type: TYPE_NORMAL
- en: in turnï¼Œ using this input alongside its usual sequence input will take a stab
    at decoding the sequenceã€‚The decoder dedes a sequenceï¼Œ and outputs a wordã€‚As of
    nowã€‚ we don't really need to make sense of that wordï¼Œ but we can understand that
    the decoder is essentially decoding what the encoder has outputã€‚The start sequence
    hereï¼Œ the startup sequence word here indicates that it should start decoding the
    sequenceã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have both the encoder numerical representation and an initial generated
    wordã€‚ we don't need the encoder anymoreã€‚As we have seen before with the decoderã€‚
    it can act in an autoregressive mannerã€‚The word it has just output can now be
    used as an inputã€‚Thisã€‚ in combination with the numerical representation output
    by the encoderã€‚
  prefs: []
  type: TYPE_NORMAL
- en: can now be used to generate a second wordã€‚Please note that the first word is
    still here as the model still outputs itã€‚ Howeverï¼Œ we have grade it out as we
    have no need for it anymoreã€‚ğŸ˜Šï¼ŒWe can continue on and onã€‚ for exampleï¼Œ until the
    decoder outputs a value that we consider a stopping valueã€‚ like a dot meaning
    the end of a sequenceã€‚Here we've seen the full mechanism of the encoder decoder
    transformerã€‚
  prefs: []
  type: TYPE_NORMAL
- en: let's go over one more timeã€‚ We have an initial sequence that is sent to the
    encoderã€‚ğŸ˜Šã€‚That encoder output is then sent to the decoder for it to be decodedã€‚While
    it can now discard the encoder after a single useã€‚ the decoder will be used several
    times until we have generated every word that we needã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So let's see a concrete example with translation language modelingï¼Œ also called
    transductionã€‚ which is the act of translating a sequenceã€‚Here we would like to
    translate this English sequence welcome to NYYC in Frenchã€‚We're using a transformer
    model that is trained for that task explicitlyã€‚ we use the encoder to create a
    representation of the English sentenceã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we cast this to the decoder with the use of the start sequence wordã€‚ we ask
    it to output the first wordã€‚It outputs B avenueï¼Œ which means welcomeã€‚And we then
    use B avenue as the input sequence for the decoderã€‚This alongside the encoder
    numerical representationï¼Œ allows the decoder to predict the second word aã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which is two in Englishã€‚ğŸ˜Šï¼ŒFinallyï¼Œ we ask the decoder to predict a third wordï¼Œ
    it predicts NYCã€‚ which is correctï¼Œ we've translated the sentenceã€‚Where the encoder
    decoder really shines is that we have an encoder and a decoderã€‚ which often do
    not share weightsã€‚Thereforeï¼Œ we have an entire blockã€‚ the encoder that can be
    trained to understand the sequence and extract the relevant informationã€‚
  prefs: []
  type: TYPE_NORMAL
- en: For the translation scenario we've seen earlierï¼Œ for exampleã€‚ this would mean
    parsing and understanding what was said in the English languageã€‚It would mean
    extracting information from that language and putting all of that in a vector
    dense in informationã€‚ğŸ˜Šï¼ŒOn the other handï¼Œ we have the decoder whose sole purpose
    is to decode the numerical representation output by the encoderã€‚
  prefs: []
  type: TYPE_NORMAL
- en: This decoder can be specialized in a completely different language or even modality
    like images or speechã€‚Encosï¼Œ decoders are special for several reasonsã€‚Firstlyã€‚
    they are able to manage sequence to sequence tasks like translation that we have
    just seenã€‚Secondlyã€‚ the weights between the encoder and the decoder parts are
    not necessarily sharedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Let's take another example of translationã€‚Here where translating transformers
    are powerful in Frenchã€‚Firstlyï¼Œ this means that from a sequence of three wordsã€‚
    we're able to generate a sequence of four wordsã€‚One could argue that this could
    be handled with a decoder that would generate the translation in an autoregressive
    mannerã€‚And they would be rightã€‚Another example of where sequence to sequence transformers
    shine is in summarizationã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Here we have very very long sequenceï¼Œ generally a full textï¼Œ and we want to
    summarize itã€‚Since if the encoder and decoders are separatedï¼Œ we can have different
    context lengthsï¼Œ for exampleã€‚ a very long context for the encoder which handles
    the text and a smaller context for the decoderã€‚ which handles the summarized sequenceã€‚There
    are a lot of sequence to sequence modelsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: This contains a few examples of popular encoder decoder models available in
    the Transformers libraryã€‚Additionallyï¼Œ you can load an encoder and a decoder inside
    an encoder decoder modelã€‚ğŸ˜Šï¼ŒThereforeã€‚ according to the specific task you are targetingï¼Œ
    you may choose to use specific encoders and decoders which have proven their worth
    on these specific tasksã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cce5d73b05cbf648b5982557ff9b6810_1.png)'
  prefs: []
  type: TYPE_IMG
- en: ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cce5d73b05cbf648b5982557ff9b6810_3.png)'
  prefs: []
  type: TYPE_IMG
