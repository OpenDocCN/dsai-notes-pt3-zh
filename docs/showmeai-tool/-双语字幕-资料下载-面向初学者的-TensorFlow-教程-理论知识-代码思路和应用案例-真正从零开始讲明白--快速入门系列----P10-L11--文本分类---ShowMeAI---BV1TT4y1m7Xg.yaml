- en: „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëÈù¢ÂêëÂàùÂ≠¶ËÄÖÁöÑ TensorFlow ÊïôÁ®ãÔºåÁêÜËÆ∫Áü•ËØÜ„ÄÅ‰ª£Á†ÅÊÄùË∑ØÂíåÂ∫îÁî®Ê°à‰æãÔºåÁúüÊ≠£‰ªéÈõ∂ÂºÄÂßãËÆ≤ÊòéÁôΩÔºÅÔºúÂø´ÈÄüÂÖ•Èó®Á≥ªÂàóÔºû - P10ÔºöL11-
    ÊñáÊú¨ÂàÜÁ±ª - ShowMeAI - BV1TT4y1m7Xg
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/cf77d9b01a43fab5f293c40eb502060f_0.png)'
  prefs: []
  type: TYPE_IMG
- en: üéºÔºåHeyÔºå guysÔºå and welcome to another Tensorflow tutorial„ÄÇ In this video„ÄÇ we will
    learn how to use an R and N for text classification„ÄÇ So last time I gave you a
    quick overview and showed you how we treat our input as a sequence„ÄÇ and then create
    an R and N model„ÄÇ And now we apply this to a very interesting task„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is text classification from real worldor data„ÄÇ So we analyze Twitter tweets
    and want to predict if the text is about a disaster event or not„ÄÇ So here I'm
    in a two pointer notebook„ÄÇ and I already imported the things we need„ÄÇ And then
    the data set we are going to useÔºå is available on kgggle„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I put the link in the descriptionÔºå of course„ÄÇ And this is called the disaster
    tweets„ÄÇ So we want to predict which tweets are about real disasters and which
    are not„ÄÇüòä„ÄÇAnd there are two files available„ÄÇ so the training and testing CSsv„ÄÇ
    but the testing CSsv doesn't include the labels„ÄÇ So this is for the submission
    if you want to participate in this competition„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I only downloaded this train do CSsv and then put it in my folder already
    and then we use pans to load the data so we can call panndas read CSsv and then
    the name of the file„ÄÇ And now we if we have a look at the shapeÔºå then we see we
    have 7630 samples and then five columns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now let's have a look at the first five rows„ÄÇ So here we see we get some
    additional information like the I„ÄÇ the keyword and the location which we don't
    need now„ÄÇ and then we have the text„ÄÇ So this is the actual tweet„ÄÇAnd then the
    target label„ÄÇ So zero for no disaster and one for this is a disaster tweet„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's also analyze how many of both classes we have„ÄÇSo we have this many„ÄÇ
    which are about an actual disaster and this manyÔºå which are not about a disaster„ÄÇ
    So I guess it's pretty much balanced here„ÄÇ I think that's that's okay„ÄÇ And yeah„ÄÇ
    So now we can go ahead and want to to preproces this text a little bit before
    we can use an R and N later„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the first thing I want to do is to clean this data a little bit„ÄÇ And I want
    to remove URL„ÄÇ because it doesn't give us any information„ÄÇ And I also want to
    remove punctuation„ÄÇSo for this„ÄÇ I already implemented these helper functions„ÄÇ
    So this one is using regular expressions„ÄÇ And if you want to learn more about
    thisÔºå then I also have a full guide on my channel that you can check out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And yeahÔºå so now let's define these two functions„ÄÇ And hereÔºå for example„ÄÇ these
    are all the punctuation characters that we want to remove„ÄÇSo now if we have a
    look at this exampleÔºå then here it finds one tweet with a URL„ÄÇ and if we remove
    thisÔºå then it has only this format„ÄÇSo now with these two functions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can simply for our data frames or penda this data frame„ÄÇ we can call this
    map function and only for this„ÄÇText column„ÄÇ So we say data frame dot text dot
    map„ÄÇ And then these two functions„ÄÇ And then againÔºå we assign it to the text column„ÄÇ
    So now this means we remove all the URLs and all the punctuation characters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is the first prepro I want to do„ÄÇ and then I also want to remove stop
    words„ÄÇ And for this„ÄÇ we are going to use the famous N LTK libraries„ÄÇ So this is
    a very popular library and Python for natural language processing„ÄÇ and then you
    also probably have to say N LT K dot download„ÄÇ And by the way„ÄÇ you can simply
    install it with Pip„ÄÇAnd then here I want to get all the stop words and then remove
    them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So by definition hereÔºå a stop word is commonly is a commonly used words„ÄÇ such
    as D A N in that a search engine has been programmed to ignore„ÄÇ So we want to
    ignore these stop words„ÄÇ So we get all the stop words from N L T K„ÄÇ And then we
    remove it again with a little helper function„ÄÇ And let's here print the stop words„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here you see all the different stop wordss„ÄÇAnd then again„ÄÇ we call this map
    function with this function„ÄÇ and on the text column„ÄÇ So this removes all the stop
    words„ÄÇ And now let's have a look at some example text„ÄÇ So this is the text column„ÄÇAnd
    now we want to prepare this text so that we can use it for a R and N„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So weÔºå we cannot use it like this with all the strings„ÄÇ So we somehow have to
    transform this to a representation that our model understands„ÄÇAnd for this„ÄÇ the
    first thing we want to do is count all the different words„ÄÇ And here we want to
    make use of a very nice objectÔºå the counter object„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is available in the collections module in Python„ÄÇ and then we count all
    the unique words„ÄÇ So we iterate over over the text columnÔºå and then we say for
    each text„ÄÇ So basically for each line in this text columnÔºå we say we iterate over
    each line„ÄÇ and then for each lineÔºå we split it„ÄÇ So we get an array of all the
    different words„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then we iterate over all the words and put it in our counter„ÄÇ And then each
    time this word appearsÔºå we increase the counter by one„ÄÇ So now if we do this and
    then return the counter and apply this function for the data frame text column„ÄÇ
    then we get the counter and we get the length of the counter„ÄÇ So this is the number
    of unique words„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we see that we have almost 18000 different words in all these tweets„ÄÇSo let's
    also have a look at the counter itself„ÄÇ So this is basically looks like a dictionary„ÄÇ
    So here the keys are the different words„ÄÇ and then we have the count of this word
    here as a value„ÄÇ So this is how the counter looks„ÄÇ And what's also very nice with
    this counter object„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We can call this most common function„ÄÇ So here we can have a look at the fiveÔºå
    most common words„ÄÇ So we see that the word like is the most common one and appears
    345 times„ÄÇüòäÔºåAnd yeahÔºå then again„ÄÇ let's assign this length of the counter to the
    to a variable and call this nu unique words„ÄÇ So we need this later„ÄÇ And now I
    want to split the data set into training and validation set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and for thisÔºå I define an integer of„ÄÇThis should be 80% of our whole data we
    want to use as training„ÄÇ and then the rest of this„ÄÇ So 20% for validation„ÄÇ and
    then we can use the slicing on the data frame„ÄÇ and then the first 80% of the samples
    is used for training and the rest for validation„ÄÇ So now we get that„ÄÇ And then
    we also So right now we still have the whole data frame„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now we want to split the text and the labels and we can simply do this by
    assessing the different columns„ÄÇ So we say train data frame dot text dots to nuy
    and the same for data frame dot target„ÄÇ So we say these are our training sentences
    and our training labels„ÄÇ And then again„ÄÇ we do the same for the validation set„ÄÇSo
    if we have a look at both the training sentences shape and the validation sentences
    shape„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we see that we clearly have more in our training set„ÄÇ and now the next
    thing we want to do is to apply a tokenizer„ÄÇ So with tokenization we vectorize
    a text corpus by turning each text into a sequence of integers„ÄÇ So you will see
    an example in a second which makes this more clearer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But for now we want to import this tokenizer from Tensorflowcars dot preprocessing
    dot text„ÄÇ and then we create a tokenizer object„ÄÇ And for this we need to give
    it the number of unique words„ÄÇ So that's why we calculate this earlier„ÄÇ And then
    we have to call tokenizer dot fit on texts and then„ÄÇthe training sentences„ÄÇ So
    only the training data here„ÄÇ And now when we did this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can get this word index„ÄÇ So here each word has a unique index„ÄÇ So let's say
    word index equals tokenizer dot word index and then have a look at this word index„ÄÇAnd
    then we see each of these words has a unique index„ÄÇ So this is what this tokenizer
    does„ÄÇAnd then we can convert these text to a sequenceÔºå so we can call tokenizer
    dot texts to sequences and then give it the sentences„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So don't get confused by this„ÄÇ we have the sentences„ÄÇ then„ÄÇ So this is the original
    text„ÄÇ And then we get a sequence„ÄÇ So this has the same size„ÄÇ But now it has these
    unique indices„ÄÇ So now we do this for the training set and the validation set„ÄÇ
    And now hereÔºå for example„ÄÇ I compare five samples of the training sentences and
    the corresponding sequences„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's print this„ÄÇAnd this is how it looks like„ÄÇ So now it maybe gets a little
    bit clearer„ÄÇ So the sentence is the normal text„ÄÇ And then after we applied the
    tokenization„ÄÇ we get this sequence„ÄÇ So now we have the same length of this array
    as the text„ÄÇ But now we have an index for each word„ÄÇSo now now we have that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now we want to do one more thing because right now the sequences can have
    a different length„ÄÇ So that's what you can see here„ÄÇ but we want to have the same
    length for every sequence„ÄÇ And for this we apply padding„ÄÇ So againÔºå we import
    this we import Pat sequences from tensofflowcars do preprocessing do sequence„ÄÇ
    and then we have to specify a maximum length„ÄÇ So in this case we say it's 20 but
    you can play around with more or other ones here or maybe a tweet might be even
    longer than 20 different words„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you might increase this a little bit„ÄÇ but then also your training might be
    longer„ÄÇ So now when we specify thisÔºå we can call this function pad sequences and
    we call this with the„ÄÇRining sequences and our specified max length„ÄÇAnd here we
    say padding and truncating equals posts„ÄÇ So this means it just uses zeros„ÄÇ So
    then we do this for the training and validation sequence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then if we have a look at the shape hereÔºå then we see that they all have
    the shape 20 in the second dimension„ÄÇ because this is the max length„ÄÇAnd now if
    we print one padded sequence„ÄÇ we see that it's used zero padding here„ÄÇ So now
    again„ÄÇ let's print one sample of our training sentencesÔºå one sample of our training
    sequences and one sample of the padded sequence so that you see the difference„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then againÔºå here you see for the sentenceÔºå we have all the words for the
    sequence„ÄÇ we have these indicesÔºå and then for the padded sequenceÔºå we use0 padding„ÄÇSo
    now we can check if we„ÄÇ if this is actually correct„ÄÇ So if we can reverse this„ÄÇ
    So for this„ÄÇ we create a dictionary where we flip around the keys and the values
    in this word index„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if we have a look against this is how this word index looks as key„ÄÇ we have
    the word and as valueÔºå we have this index„ÄÇ and now we want to store this in another
    dictionary and do it the other way around„ÄÇ So now we say the key is the index
    and the word is the value„ÄÇSo this is our reverse dictionary„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So let's have a look at that„ÄÇ And then we see all these indices are our keys„ÄÇ
    and all these words are the values„ÄÇ And now we can define this decocode function„ÄÇ
    which gets a sequence„ÄÇ And then we simply„ÄÇüòäÔºåCall the get function with for each
    index in the sequence„ÄÇ So this returns the corresponding value„ÄÇ And if it's not
    available„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then it should return a question mark„ÄÇ but so this is if we get new indices„ÄÇ
    but if we stay in the same training data set then it should find a corresponding
    word for each index„ÄÇ So this is the decocode function„ÄÇ and then let's try it out„ÄÇ
    So we call the deco function for one sample of the training sequences„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this one and then I print the original sequence and the decoded text„ÄÇ and
    here we see we have this sequence and this decoded sequence and I think so we
    say we have three people„ÄÇDight heat way far„ÄÇ So if we have a look at where did
    I already print it„ÄÇ So here I printed the samples from 10 to 15„ÄÇ So that's the
    original sentence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we see that our decoding is correct„ÄÇSo now that we have that„ÄÇ we can come
    to the actual implementation of the model„ÄÇ So last time I showed you that we can
    very easily create an simple R N model or a LSTM model or a GR U model„ÄÇ So in
    this caseÔºå we use an LSTM„ÄÇ So for this we create a first a sequential model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And now since we use text dataÔºå we also use this embedding layer„ÄÇ So word embeddings„ÄÇ
    give us a way to use an efficient dense representation in which similar words
    have a similar encoding„ÄÇ So if you want to learn more about thisÔºå I can recommend
    this official guide in the on the Tensorflow website„ÄÇ So here you see that another
    representation might be one hot encoding„ÄÇ and„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here we simply use a0 or a1„ÄÇ but then there's also this embedding representation„ÄÇ
    So with this embedding layerÔºå we get this representation„ÄÇ So a dense vector of
    floating point values„ÄÇ So right now we still have this padded sequence with all
    the word indices„ÄÇ And now this embedding layer turns this indices into a dense
    vector of fixed size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that's why we needed this tokenization first„ÄÇ and now we can use this embedding
    layer„ÄÇ So this gets the number of unique words„ÄÇ And then a size that you specify
    and then also the maximum length„ÄÇ and that we specified„ÄÇ So this is the input
    length„ÄÇAnd now after we define this„ÄÇ then we can apply our LSTM or R and N layerÔºå
    like the last time where we only specify the number of output units„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here I also said dropout equals 10%„ÄÇAnd then since we want to classify this„ÄÇ
    So a0 or one classificationÔºå we use a dense layer with only one output at the
    end„ÄÇ And then we also apply the sigmoid function„ÄÇ So let's do this and print the
    model summary„ÄÇ So we see after our embedding„ÄÇ we get the output shape of this„ÄÇ
    So the number of batches„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then the 20 is the maximum length and the 32 is just the size that we specified
    here as output size„ÄÇThen our LSTM has this output shape because we specified 64
    output units„ÄÇ and then we have our dense layer„ÄÇAnd now since we use binary classification„ÄÇ
    we use this binary cross entropy loss and here we say from Loit equals falses
    because we already used the activation function here and then again we use a optimizer
    and define the metrics that we want to track and compile the model and then we
    simply train it so we fit it and here we want to use the padded sequence and then
    the corresponding labels and then the epochs and now this is also new„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think I didn't use this before so you can in this fit method you can use the
    validation data parameter and this is a tuple and here we use the validation padded
    sequence and the validation label and now if we do this„ÄÇ then it automatically
    during training uses a validation„ÄÇData set to do the fine tuning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is a nice tip to keep in mind that you can already automatically do
    this validation if you specify the validation data here„ÄÇSo now let's train this„ÄÇAl
    rightÔºå and training is done„ÄÇ And as you can see„ÄÇ the final accuracy on the training
    data is 98%„ÄÇ so pretty good„ÄÇ But for the validation accuracy„ÄÇ and we only have
    73%„ÄÇ So this might be a sign of overfitting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this might be a homework for you that you can further improve or tweak„ÄÇ the
    model a little bit so that this one also gets higher„ÄÇ But as we can see our„ÄÇ we
    did the correct preprocesing with our text data„ÄÇ and we set up a nice LSTM model
    and then get a very nice accuracy here„ÄÇ So let's do some prediction„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we simply call model predict on the„ÄÇüòäÔºåTraining padded sequence in this case„ÄÇ
    And as I said„ÄÇ we used the sigmoid function at the end„ÄÇ So we still have to convert
    this to a label 0 or1„ÄÇ So we simply say if our predicted output probability is
    higher than 0„ÄÇ5„ÄÇ then it's one and otherwise 0„ÄÇ And now let's print some original
    training sentences„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and the corresponding labels and the predictions„ÄÇSo hereÔºå yeah„ÄÇ so we see five
    of these are classified as a as a disaster and the other one are no disasters
    and all of our corrections are correct in this case„ÄÇ So we say for so we seeÔºå
    for example here we we have three people diedÔºå blahÔºå blahÔºå blah„ÄÇ So this is a
    disaster„ÄÇ And here Ta getting floodedÔºå also a disaster„ÄÇ And here at the end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have some a lovelyÔºå no disaster„ÄÇ So yeahÔºå it looks good„ÄÇ And yeah„ÄÇ I think
    we learned a lot in this tutorial„ÄÇ Now you know how to apply some basic and natural
    language processing techniques and then use an LSTM for text classification„ÄÇAnd
    I hope you enjoyed this tutorial„ÄÇ If you liked it„ÄÇ then please hit the like button
    and consider subscribing to the channel„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then I hope to see you in the next videoÔºå bye„ÄÇüòä„ÄÇ![](img/cf77d9b01a43fab5f293c40eb502060f_2.png)
  prefs: []
  type: TYPE_NORMAL
