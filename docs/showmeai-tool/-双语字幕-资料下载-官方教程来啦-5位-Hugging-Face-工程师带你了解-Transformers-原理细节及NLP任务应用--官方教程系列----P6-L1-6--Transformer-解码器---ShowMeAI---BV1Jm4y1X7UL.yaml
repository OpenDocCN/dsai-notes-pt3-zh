- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P6ï¼šL1.6- Transformerï¼šè§£ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½Hugging Faceå·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£TransformersåŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL1.6-
    Transformerï¼šè§£ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
- en: In this videoï¼Œ we'll study the decoder architectureã€‚An example of a popular
    decoder only architecture is GPT2ã€‚In order to understand how decoders workã€‚ we
    recommend taking a look at the video regarding encodersï¼Œ they're extremely similar
    to decodersã€‚One can use a decoder for most of the same tasks as an encoderã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ®µè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶è§£ç å™¨æ¶æ„ã€‚ä¸€ä¸ªæµè¡Œçš„ä»…è§£ç å™¨æ¶æ„ç¤ºä¾‹æ˜¯GPT2ã€‚ä¸ºäº†ç†è§£è§£ç å™¨çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹æœ‰å…³ç¼–ç å™¨çš„è§†é¢‘ï¼Œå®ƒä»¬ä¸è§£ç å™¨éå¸¸ç›¸ä¼¼ã€‚è§£ç å™¨å¯ä»¥ç”¨äºå¤§å¤šæ•°ä¸ç¼–ç å™¨ç›¸åŒçš„ä»»åŠ¡ã€‚
- en: albeit with generally a little loss of performanceã€‚Let's take the same approach
    we have taken with the encoder to try and understand the architectural differences
    between an encoder and ID decorã€‚We'll use a small example using three wordsã€‚ We
    pass them through their decoderã€‚We retrieve a numerical representation for each
    worldã€‚Hereï¼Œ for exampleã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡é€šå¸¸ä¼šæœ‰ä¸€ç‚¹æ€§èƒ½æŸå¤±ã€‚è®©æˆ‘ä»¬é‡‡ç”¨ä¸ç¼–ç å™¨ç›¸åŒçš„æ–¹æ³•ï¼Œè¯•å›¾ç†è§£ç¼–ç å™¨å’ŒIDè§£ç å™¨ä¹‹é—´çš„æ¶æ„å·®å¼‚ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸‰ä¸ªå•è¯çš„å°ç¤ºä¾‹ã€‚æˆ‘ä»¬å°†å®ƒä»¬ä¼ é€’é€šè¿‡è§£ç å™¨ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªå•è¯æ£€ç´¢æ•°å€¼è¡¨ç¤ºã€‚è¿™é‡Œï¼Œä¾‹å¦‚ã€‚
- en: the decoder converts the three words welcomee to NYC and these are three sequences
    of numbersã€‚The decoder outputs exactly one sequence of numbers per input wordã€‚This
    numerical representation can also be called a feature vector or a feature tensorã€‚Let's
    dive in this representationã€‚It contains one vector per word that was passed through
    the decoderã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨å°†ä¸‰ä¸ªå•è¯â€œwelcome to NYCâ€è½¬æ¢ä¸ºä¸‰ä¸ªæ•°å­—åºåˆ—ã€‚è§£ç å™¨ä¸ºæ¯ä¸ªè¾“å…¥å•è¯è¾“å‡ºæ°å¥½ä¸€ä¸ªæ•°å­—åºåˆ—ã€‚è¿™ç§æ•°å€¼è¡¨ç¤ºä¹Ÿå¯ä»¥ç§°ä¸ºç‰¹å¾å‘é‡æˆ–ç‰¹å¾å¼ é‡ã€‚è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨è¿™ç§è¡¨ç¤ºã€‚å®ƒåŒ…å«æ¯ä¸ªé€šè¿‡è§£ç å™¨ä¼ é€’çš„å•è¯çš„ä¸€ä¸ªå‘é‡ã€‚
- en: Each of these vectors is a numerical representation of the word in questionã€‚ğŸ˜Šã€‚The
    dimension of that vector is defined by the architecture of the modelã€‚Whether a
    decoder differs from the encoder is principally with its self attention mechanismã€‚
    it's using what is called masked self attentionã€‚Hereï¼Œ for exampleï¼Œ if we focus
    on the word 2ã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å‘é‡æ˜¯æ‰€è®¨è®ºå•è¯çš„æ•°å€¼è¡¨ç¤ºã€‚ğŸ˜Š å‘é‡çš„ç»´åº¦ç”±æ¨¡å‹çš„æ¶æ„å®šä¹‰ã€‚è§£ç å™¨ä¸ç¼–ç å™¨çš„ä¸»è¦åŒºåˆ«åœ¨äºå…¶è‡ªæ³¨æ„æœºåˆ¶ï¼Œå®ƒä½¿ç”¨æ‰€è°“çš„æ©ç è‡ªæ³¨æ„ã€‚è¿™é‡Œï¼Œä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å…³æ³¨å•è¯â€œ2â€ã€‚
- en: we'll see that this vector is absolutely unmodified by the NYC wordã€‚That's because
    all the words on the rightï¼Œ also known as the right context of the word is maskedã€‚Rather
    than benefiting from all the words on the left and rightï¼Œ so the bidirectional
    contextã€‚ decoders only have access to a single contextã€‚Which can be the left context
    or the right contextã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¼šçœ‹åˆ°è¿™ä¸ªå‘é‡ç»å¯¹ä¸å—â€œNYCâ€ä¸€è¯çš„å½±å“ã€‚è¿™æ˜¯å› ä¸ºå³ä¾§çš„æ‰€æœ‰å•è¯ï¼Œä¹Ÿç§°ä¸ºå•è¯çš„å³ä¸Šä¸‹æ–‡ï¼Œéƒ½è¢«æ©ç å¤„ç†ã€‚è§£ç å™¨åªè®¿é—®å•ä¸ªä¸Šä¸‹æ–‡ï¼Œå¯èƒ½æ˜¯å·¦ä¸Šä¸‹æ–‡æˆ–å³ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯ä»å·¦å³ä¸¤ä¾§çš„æ‰€æœ‰å•è¯ä¸­å—ç›Šã€‚
- en: The mask self attention mechanism differs from the self attention mechanism
    by using an additional mask to hide the context on either side of the wordã€‚The
    words numerical representation will not be affected by the words in the hidden
    contextã€‚So when should one use a decoder decoders like encoders can be used as
    standalone modelsã€‚ as they generate a numerical representationï¼Œ they can also
    be used in a wide variety of tasksã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ©ç è‡ªæ³¨æ„æœºåˆ¶é€šè¿‡ä½¿ç”¨é¢å¤–çš„æ©ç æ¥éšè—å•è¯ä¸¤ä¾§çš„ä¸Šä¸‹æ–‡ï¼Œä¸è‡ªæ³¨æ„æœºåˆ¶æœ‰æ‰€ä¸åŒã€‚å•è¯çš„æ•°å€¼è¡¨ç¤ºä¸ä¼šå—åˆ°éšè—ä¸Šä¸‹æ–‡ä¸­å•è¯çš„å½±å“ã€‚å› æ­¤ï¼Œä½•æ—¶ä½¿ç”¨è§£ç å™¨å°±æ˜¾å¾—é‡è¦ï¼Œè§£ç å™¨å’Œç¼–ç å™¨ä¸€æ ·å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å‹ä½¿ç”¨ã€‚ç”±äºå®ƒä»¬ç”Ÿæˆæ•°å€¼è¡¨ç¤ºï¼Œå› æ­¤å¯ä»¥åº”ç”¨äºå„ç§ä»»åŠ¡ã€‚
- en: Howeverï¼Œ the strength of a decoder lies in the way a word can only have access
    to its left contextã€‚Having only access to their left contactsï¼Œ they are inherently
    good at tax generationã€‚ the ability to generate a word or a sequence of wordsï¼Œ
    given a known sequence of wordsã€‚ğŸ˜Šã€‚This is known as causal language modeling or
    natural language generationã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè§£ç å™¨çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå•è¯ä»…èƒ½è®¿é—®å…¶å·¦ä¸Šä¸‹æ–‡ã€‚ä»…èƒ½è®¿é—®å·¦ä¾§ä¸Šä¸‹æ–‡ï¼Œä½¿å…¶åœ¨ç”Ÿæˆæ–‡æœ¬æ–¹é¢å¤©ç”Ÿä¼˜ç§€ï¼Œèƒ½å¤Ÿæ ¹æ®å·²çŸ¥å•è¯åºåˆ—ç”Ÿæˆå•è¯æˆ–å•è¯åºåˆ—ã€‚è¿™è¢«ç§°ä¸ºå› æœè¯­è¨€å»ºæ¨¡æˆ–è‡ªç„¶è¯­è¨€ç”Ÿæˆã€‚ğŸ˜Š
- en: Here's an example of how causal language modeling worksã€‚ We start with an initial
    wordï¼Œ which is myã€‚We use this as input for the decoderã€‚ğŸ˜Šï¼ŒThe model outputs a vector
    of numbersã€‚ and this vector contains information about the sequenceï¼Œ which is
    here a single wordã€‚We apply a small transformation to that vector so that it maps
    to all the words known by the modelã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› æœè¯­è¨€å»ºæ¨¡å¦‚ä½•å·¥ä½œçš„ä¸€ä¸ªä¾‹å­ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªåˆå§‹å•è¯å¼€å§‹ï¼Œå³â€œmyâ€ã€‚æˆ‘ä»¬å°†å…¶ä½œä¸ºè§£ç å™¨çš„è¾“å…¥ã€‚ğŸ˜Š æ¨¡å‹è¾“å‡ºä¸€ä¸ªæ•°å­—å‘é‡ï¼Œè¿™ä¸ªå‘é‡åŒ…å«å…³äºåºåˆ—çš„ä¿¡æ¯ï¼Œåœ¨è¿™é‡Œæ˜¯ä¸€ä¸ªå•è¯ã€‚æˆ‘ä»¬å¯¹è¿™ä¸ªå‘é‡è¿›è¡Œå°çš„è½¬æ¢ï¼Œä½¿å…¶æ˜ å°„åˆ°æ¨¡å‹å·²çŸ¥çš„æ‰€æœ‰å•è¯ã€‚
- en: which is a mapping that will seeator called a language modeling headã€‚We identify
    that the model believes that the most probable following word is nameã€‚We then
    take that new word and add it to the initial sequenceã€‚From myï¼Œ we are now at my
    nameã€‚This is where the autoregressive aspect comes inã€‚ğŸ˜Šã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§æ˜ å°„ï¼Œè¢«ç§°ä¸ºè¯­è¨€æ¨¡å‹å¤´ã€‚æˆ‘ä»¬ç¡®å®šæ¨¡å‹è®¤ä¸ºæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå•è¯æ˜¯â€œnameâ€ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™ä¸ªæ–°å•è¯æ·»åŠ åˆ°åˆå§‹åºåˆ—ä¸­ã€‚ä»â€œmyâ€å¼€å§‹ï¼Œæˆ‘ä»¬ç°åœ¨çš„åºåˆ—æ˜¯â€œmy
    nameâ€ã€‚è¿™å°±æ˜¯è‡ªå›å½’æ–¹é¢çš„ä½“ç°ã€‚ğŸ˜Šã€‚
- en: Outtoregressive models reuse their past outputs as inputs and the following
    stepsã€‚Once againã€‚ we do the exact same operationã€‚We cast that sequence through
    the decoder and retrieve the most probable following wordã€‚In this caseï¼Œ it is
    the word isã€‚We repeat the operation until we're satisfiedã€‚Starting from a single
    wordï¼Œ we've now generated a false sentenceã€‚ We decided to stop thereã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªå›å½’æ¨¡å‹å°†è¿‡å»çš„è¾“å‡ºé‡ç”¨ä½œä¸ºè¾“å…¥ï¼Œæ¥ä¸‹æ¥çš„æ­¥éª¤ä¹Ÿä¸€æ ·ã€‚æˆ‘ä»¬å†æ¬¡è¿›è¡Œç›¸åŒçš„æ“ä½œã€‚æˆ‘ä»¬é€šè¿‡è§£ç å™¨å¤„ç†è¯¥åºåˆ—ï¼Œå¹¶æå–å‡ºæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯å•è¯â€œisâ€ã€‚æˆ‘ä»¬é‡å¤è¿™ä¸€æ“ä½œï¼Œç›´åˆ°æ„Ÿåˆ°æ»¡æ„ã€‚ä»ä¸€ä¸ªå•è¯å¼€å§‹ï¼Œæˆ‘ä»¬ç°åœ¨ç”Ÿæˆäº†ä¸€ä¸ªé”™è¯¯çš„å¥å­ã€‚æˆ‘ä»¬å†³å®šåœ¨æ­¤åœæ­¢ã€‚
- en: but we could continue for a whileã€‚ GP T2ï¼Œ for exampleï¼Œ has a maximum context
    size of 1024ã€‚ We could eventually generate up to 1024 wordsï¼Œ and the decoder would
    still have some memory of the first words and as sequenceã€‚ğŸ˜Šã€‚![](img/dfd8577ffd03c9ab167585435c2c2709_1.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å¯ä»¥ç»§ç»­ä¸€æ®µæ—¶é—´ã€‚ä¾‹å¦‚ï¼ŒGPT-2çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ä¸º1024ã€‚æˆ‘ä»¬æœ€ç»ˆå¯ä»¥ç”Ÿæˆå¤šè¾¾1024ä¸ªå•è¯ï¼Œè§£ç å™¨ä»ç„¶ä¼šå¯¹å‰é¢çš„å•è¯å’Œåºåˆ—æœ‰ä¸€å®šçš„è®°å¿†ã€‚ğŸ˜Šã€‚![](img/dfd8577ffd03c9ab167585435c2c2709_1.png)
- en: å—¯ã€‚![](img/dfd8577ffd03c9ab167585435c2c2709_3.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚![](img/dfd8577ffd03c9ab167585435c2c2709_3.png)
