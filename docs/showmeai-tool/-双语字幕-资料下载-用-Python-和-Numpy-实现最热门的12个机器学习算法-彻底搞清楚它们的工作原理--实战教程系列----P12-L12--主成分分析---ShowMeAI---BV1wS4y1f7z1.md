# ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12- ä¸»æˆåˆ†åˆ†æ - ShowMeAI - BV1wS4y1f7z1

Hi everybodyã€‚ Welcome to our new machine learning from Sct tutorialã€‚ Todayã€‚ we are going to implement the principal component analysis or PCA using only Python and Numpyã€‚The PCA is a nice tool to get linearly independent features and also to reduce the dimensionality of our data setã€‚So the goal is to find a new set of dimensions such that all the dimensions are orthogonal and hence linearly independent and ranked according to the variance of data along themã€‚

 So this means we want to find a transformationï¼Œ such that the transform features are linearly independentã€‚And the dimensionality can then be reduced by taking only the dimensions with the highest importanceã€‚And those newly fine dimensions should minimize the projection errorã€‚ and the projected point should have a maximum spread or which means the maximum varianceã€‚

 So let's look at an image to understand this betterã€‚ So let's say our 2D data is distributed like this and now we want to project it into 1 d and now what we want to do is we want to find the axis that are orthogonal to each otherã€‚And when we project our data onto these axesï¼Œ then our new projected data should have the maximum spreadã€‚So on the left sideï¼Œ these are the correct principal axisã€‚ So if we project them in one Dã€‚

 So on the largest principal componentã€‚If we project our data on this axisã€‚ then they have the maximum spreadã€‚ Andï¼Œ for exampleï¼Œ if we look on the right sideã€‚ So these are incorrect axisã€‚ Soï¼Œ let's look at how the projected data would look like forã€‚ so on the right sideï¼Œ we made it even worse and projected it on theã€‚Y xsã€‚

 So these are clearly wrong because here we can see that a lot of data is on the same spotã€‚ so we don't have any more information about themã€‚ But here on the left sideã€‚ the projected data has the maximum spreadã€‚So we can contain most of the information about the dataã€‚And alsoï¼Œ the projection errorï¼Œ which means this would beã€‚The lines from here to the axisã€‚

 this is minimalï¼Œ whereas on the right sideã€‚ So here we have to make a longã€‚ very long projection line for each pointã€‚ So the left side is the correct answerã€‚ And now how do we find these principal componentsã€‚So for thisï¼Œ as I saidã€‚ we want to maximize the varianceï¼Œ so we need some mathã€‚

 so we need the variance of a sample X and this is calculated as one over the number of samplesã€‚ and then we have the sum over each component minus x bar and x bar is the mean valueã€‚ So we subtract the mean value from our data setã€‚And now we also need a covariance matrixã€‚ so this indicates the level to which two variables vary together and the covariance matrix of two variables is defined as this so1 over n and then again the sum and here we subtract the mean and again here also the mean and then transposed and in our case we want to have the covariance matrix with both of our xã€‚

 so this is also called the auto covariance matrixã€‚Soã€‚We have to calculate this and then our problem is reduced to an eigenvector or eigenvalue problemã€‚ so I will not go into detail about eigenvectors hereã€‚ but I will put some links in the description if you want to read moreã€‚å—¯ã€‚

But what we have to do is we have to find the eigenvectors and eigenvalues of this covariance matrix and the eigenvectors point then in the direction of the maximum variance and the corresponding eigenvalues indicate the importance of its corresponding eigenvectorã€‚

 So now if we have a look at this image again on the left sideã€‚ So these twovectors that I've drawn hereï¼Œ they correspond to the eigenvectors of the covariance matrix of our data setã€‚So this is what we have to doã€‚ And here I've written the approachã€‚ So first we sub the mean value from our x or from our data setã€‚

 Then we calculate the covariance matrixã€‚ Then we have to calculate the eigenvectors and eigenvaluesã€‚Then we sort the eigenvectors in decreasing orderï¼Œ according to their eigenvaluesã€‚ and then we can specify how many dimensions we want to keepã€‚ and then we choose only the first K eigenvectorsã€‚That will then be the new K dimensionsã€‚

And then we transform the original dataã€‚Into these new dimensions by projecting themã€‚ justã€‚ this is simply a dot productã€‚Of our data with the newï¼Œ with the eigenvectorsã€‚And then we are doneã€‚ So this is all we have to doï¼Œ andã€‚Byeã€‚One thing that is very nice about thisã€‚Pncple component analysis and the eigenvectors is that they are all orthogonal of each otherã€‚

 This means that our new data is then also linearly independentã€‚ So this is a nice little bonus of the PCAã€‚![](img/592934822c91be0e4fe905744b84c579_1.png)

And now we can startã€‚ So let's import Ny S and Pã€‚ And then we create a class PC Aã€‚ This will get an in it with selfã€‚ And then here we specify the number of componentsã€‚ We want to keepã€‚And then we store them hereï¼Œ so we say self dot n componentsã€‚Equals nã€‚Componentsã€‚Andã€‚We want to find out the eigenvectorsã€‚ So let's call them components hereã€‚

And this is none in the beginningã€‚ And we also want to store the mean laterã€‚ So let's say self that mean equals noneã€‚ And then we define our fit methodï¼Œ as alwaysã€‚ So this will get the day that we want to transformã€‚And then we don't use the predict methodã€‚ So now we call this transformã€‚So this will transform our data once we fitted itã€‚

And this is just the projectionã€‚ But now let's start with the fit methodï¼Œ soã€‚Let's say or againã€‚ let's write our approachã€‚ So we want to have the meanã€‚Then we calculate the covariance matrixã€‚Then we want to calculate the eigen vectorctors and valuesã€‚å—¯ã€‚Higenvalueã€‚Then we sort our een back toã€‚ so sortã€‚Eigenvesã€‚Andã€‚Thenï¼Œ we store onlyã€‚The first andã€‚Eying vectorsã€‚So this is what we have to doã€‚

 And here we have toã€‚Project our dataã€‚Soã€‚Yeahï¼Œ let's do thisã€‚ So let's sayã€‚å—¯ã€‚Here we can say self dot mean equalsã€‚ and then we just use the Ny mean function of our data along the first axisã€‚And then we sub the meanã€‚ So we say x equals x minus self dot meanã€‚Soã€‚

![](img/592934822c91be0e4fe905744b84c579_3.png)

We have to do thisï¼Œ because if we lookã€‚Againï¼Œ and in our formula with the covariance matrixã€‚ then always thisï¼Œ the mean is subtractedã€‚![](img/592934822c91be0e4fe905744b84c579_5.png)

So let's just do this hereã€‚ and then we calculate the co variance matrixã€‚ and this is called cofã€‚ And then we simply use the nuy dotã€‚Cough functionã€‚ So this will do exactly this if we only put in one input hereã€‚But now we have to be carefulã€‚ because let'sã€‚å—¯ã€‚Let's look at our dataã€‚ So this would be a nuy and D array where one rowã€‚

 the row is one sampleã€‚And one columnã€‚Is one feature of Etoã€‚But if we have a look at the documentationï¼Œ then for this functionï¼Œ it's the other way roundã€‚ So one column is one observation or one sampleã€‚ So we have to transpose it hereã€‚ So please double check it for yourselfã€‚And then we continueã€‚ So now we calculate the eigenã€‚

Vectctorsï¼Œ and Iï¼Œ I don't knowã€‚ it's the other way aroundã€‚ Sorryï¼Œ eigenvalue and eigen vectorctorsã€‚ And for thisï¼Œ we can also use a function in Nmpyã€‚ So lump Ny linearalã€‚Lynn A dotã€‚Iikeã€‚ğŸ˜Šã€‚And then we put in our Covari matrixã€‚And here againï¼Œ we have to be careful if weã€‚If we look at the documentationï¼Œ then it says that eigenves are returned as column vectorsã€‚

 So one columnã€‚With allã€‚1 column I here is one eigen vectorã€‚And now for to do easier calculationsã€‚ we want to do it the other way aroundã€‚ So we say eigenvectors equals eigenvectors dot transposedã€‚And thenï¼Œ we sort themã€‚So for thisï¼Œ we say in our sortded indicesã€‚ are Ny a sort of the eigenvalues nowã€‚And we want to have it in decreasing order so we can use slicing here all the way from start to the endã€‚

 And we put in my a step-1ã€‚ So this is a nice little trick to reverse a listã€‚And now we have the indices of the soded eigenve eigenvalues in decreasing orderã€‚ And now we say our soded eigenvalues equals the eigenvaluesã€‚With this order and the same for the eigenvectors equals eigenvectorsã€‚With this indicesã€‚

And now we store the first N eigenvectorsã€‚ So we say self dot componentsã€‚å—¯ã€‚Little typo hereã€‚ Self do componentsã€‚Equalsï¼Œ and now we can simply say eigenvesã€‚From the startã€‚ So from 0 to self dot and componentsã€‚ So this is why we transposed it here so we can easily do this transformation and also this transformationã€‚So this is the fit methodã€‚ And now we just have to transform itã€‚ And now here weã€‚

Must not forget that we should also subtract the mean hereã€‚ So we say xã€‚Equals x minus selfã€‚Don't mean this is why we started hereã€‚And now we can project it and then return itã€‚ So we say return nuy dotã€‚ and we project X onto ourã€‚å—¯ã€‚Componentsã€‚ so we the dot product with the self dot componentsã€‚ But now againï¼Œ we have to be careful hereã€‚

Because here we transposed them and now again we want a column vectorã€‚ So here againã€‚ we have to transpose itã€‚ So againï¼Œ please double check it for yourselfã€‚And now we are doneã€‚ So this is the whole implementationã€‚ And now here I've written a little test script that is using the famous Iis data setã€‚And then I will create a PCA instance and I willã€‚ so by the wayã€‚

 this will have a dimension of 150 by4 So we have 105 samples and four different features hereã€‚ and now we only want to keep two and dimensionsã€‚ So we put in two in our initializerã€‚ Then we fit the dataï¼Œ and then we transform itã€‚And then I print the shape of bothã€‚And then I will plot themã€‚ So now we have a 2D vector so we can plot it in 2Dã€‚So yeahã€‚

 let's run this and see if this is workingã€‚And oã€‚ä¸€é¡µã€‚So I didn't write Python 3 hereã€‚ but it worked anywayã€‚ So let's test this againã€‚![](img/592934822c91be0e4fe905744b84c579_7.png)

So yeahï¼Œ here we have our 4D feature vector transformed or projected into 2Dã€‚ and we see that all areã€‚All the three different classes are plotted in a different colorã€‚ so we can see that we can still have an easy separation of our classesã€‚ So yeahï¼Œ that's very niceã€‚ And yeahï¼Œ that's the PC Aï¼Œ I hope you understood everythingã€‚ And if you liked itã€‚

 please subscribe to the channelã€‚ and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚![](img/592934822c91be0e4fe905744b84c579_9.png)

![](img/592934822c91be0e4fe905744b84c579_10.png)