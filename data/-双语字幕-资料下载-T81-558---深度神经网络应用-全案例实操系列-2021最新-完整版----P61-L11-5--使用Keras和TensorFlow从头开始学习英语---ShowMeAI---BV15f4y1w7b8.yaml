- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P61ï¼šL11.5- ä½¿ç”¨Keraså’ŒTensorFlowä»å¤´å¼€å§‹å­¦ä¹ è‹±è¯­
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P61ï¼šL11.5- ä½¿ç”¨Keraså’ŒTensorFlowä»å¤´å¼€å§‹å­¦ä¹ è‹±è¯­
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeffineã€‚ welcomelcom to applications of deep neural networks with
    Washington Universityã€‚ You know whatï¼Œ Maybe we humans are overratedã€‚After allã€‚
    Al 0 was able to master chess in hours without even the benefit of human knowledgeã€‚
    We try to do natural language processingã€‚ we use all these fancy addins like an
    L T K and Space E and other thingsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯Jeffineã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚ä½ çŸ¥é“å—ï¼Œä¹Ÿè®¸æˆ‘ä»¬äººç±»è¢«é«˜ä¼°äº†ã€‚æ¯•ç«Ÿï¼ŒAIåœ¨å‡ å°æ—¶å†…å°±èƒ½æŒæ¡å›½é™…è±¡æ£‹ï¼Œç”šè‡³ä¸éœ€è¦äººç±»çŸ¥è¯†çš„å¸®åŠ©ã€‚æˆ‘ä»¬å°è¯•è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä½¿ç”¨äº†å„ç§é«˜çº§æ’ä»¶ï¼Œæ¯”å¦‚L
    T Kå’ŒSpace Eç­‰ã€‚
- en: Maybe we should just let the neural networksï¼Œ try to learn English from scratch
    or other languages as wellã€‚ This is called end to end neural networksã€‚ For the
    latest on my AI course and projectsã€‚ Click subscribe and the bell next to it to
    be notified of every new videoã€‚ Connuing onwardã€‚ We're going to look at another
    type ofã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘ä»¬åº”è¯¥è®©ç¥ç»ç½‘ç»œä»é›¶å¼€å§‹å­¦ä¹ è‹±è¯­æˆ–å…¶ä»–è¯­è¨€ã€‚è¿™è¢«ç§°ä¸ºç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œã€‚æœ‰å…³æˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®ï¼Œè¯·ç‚¹å‡»è®¢é˜…ï¼Œå¹¶ç‚¹å‡»æ—è¾¹çš„é“ƒé“›ä»¥æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚ç»§ç»­å‰è¿›ã€‚æˆ‘ä»¬å°†çœ‹å¦ä¸€ç§ç±»å‹çš„ã€‚ğŸ˜Š
- en: '![](img/511897e72e03ef43ebc5740c467bd96e_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511897e72e03ef43ebc5740c467bd96e_1.png)'
- en: Welcome to applications of deep learning with Washington University and see
    how we can teach a neural network to learn to read So these are the two papers
    that I am pulling this from based on and then I'm also using the code from the
    Kira's examples where the creator of Kiuras implemented intoto end neural networks
    as an example of cururas this will probably eventually be built intokira's layer
    specific type so that you can use this directly but for now you see some of the
    plumbing used to actually implement this network type and there's also an online
    JavaScript demo that you can use to play with this if you would like to as well
    we are going to go ahead and actually create a end to end neural network here
    and make use of it so these are just some necessary imports and utility functions
    that were created by in the Ki's example I made some changes to these to make
    it work a little bit better and eliminate some of the warnings that Hatheng was
    generated because it was generated possibly on an older version of Kis I also
    added a Q&A part at the endã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦å­¦ä¹ åº”ç”¨ï¼Œçœ‹çœ‹æˆ‘ä»¬å¦‚ä½•æ•™ç¥ç»ç½‘ç»œå­¦ä¹ é˜…è¯»ã€‚è¿™æ˜¯æˆ‘ä»è¿™ä¸¤ç¯‡è®ºæ–‡ä¸­æå–çš„å†…å®¹ï¼ŒåŒæ—¶æˆ‘ä¹Ÿä½¿ç”¨äº†Kiraç¤ºä¾‹ä¸­çš„ä»£ç ï¼ŒKiraçš„åˆ›å»ºè€…å®ç°äº†ç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œä½œä¸ºcururasçš„ä¸€ä¸ªç¤ºä¾‹ï¼Œè¿™æœ€ç»ˆå¯èƒ½ä¼šæ„å»ºæˆKiraçš„ç‰¹å®šå±‚ç±»å‹ï¼Œä»¥ä¾¿ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä½†ç°åœ¨ä½ ä¼šçœ‹åˆ°ä¸€äº›ç”¨äºå®é™…å®ç°è¿™ç§ç½‘ç»œç±»å‹çš„åº•å±‚ä»£ç ï¼Œè¿˜æœ‰ä¸€ä¸ªåœ¨çº¿JavaScriptæ¼”ç¤ºï¼Œä½ ä¹Ÿå¯ä»¥ç”¨æ¥ç©ä¸€ç©ã€‚å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å°†å®é™…åˆ›å»ºä¸€ä¸ªç«¯åˆ°ç«¯ç¥ç»ç½‘ç»œå¹¶åŠ ä»¥åˆ©ç”¨ï¼Œè¿™äº›åªæ˜¯ä¸€äº›å¿…è¦çš„å¯¼å…¥å’Œå·¥å…·å‡½æ•°ï¼Œå®ƒä»¬æ˜¯æ ¹æ®Kiraçš„ç¤ºä¾‹åˆ›å»ºçš„ï¼Œæˆ‘å¯¹è¿™äº›è¿›è¡Œäº†äº›è®¸æ›´æ”¹ï¼Œä½¿å…¶å·¥ä½œå¾—æ›´å¥½ï¼Œå¹¶æ¶ˆé™¤äº†ä¸€äº›Hathengç”Ÿæˆçš„è­¦å‘Šï¼Œå› ä¸ºè¿™äº›è­¦å‘Šå¯èƒ½æ˜¯åœ¨Kisçš„è¾ƒæ—§ç‰ˆæœ¬ä¸Šç”Ÿæˆçš„ã€‚æˆ‘è¿˜åœ¨æœ€åæ·»åŠ äº†ä¸€ä¸ªé—®ç­”éƒ¨åˆ†ã€‚
- en: The original code it trained the network and it gave you the accuracy but it
    didn't give you any way to use itã€‚ So I created that part at the endã€‚ Let's go
    ahead and run thisã€‚ It tells you it's using Tensorflows the back end and then
    we see that it is still runningã€‚ it takes it a little while to little end but
    while it's doing that let me show you some of the parts that we're doing hereã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ä»£ç è®­ç»ƒäº†ç½‘ç»œå¹¶ç»™å‡ºäº†å‡†ç¡®ç‡ï¼Œä½†æ²¡æœ‰æä¾›ä»»ä½•ä½¿ç”¨å®ƒçš„æ–¹æ³•ã€‚æ‰€ä»¥æˆ‘åœ¨æœ€ååˆ›å»ºäº†é‚£éƒ¨åˆ†ã€‚è®©æˆ‘ä»¬ç»§ç»­è¿è¡Œå®ƒã€‚å®ƒå‘Šè¯‰ä½ æ­£åœ¨ä½¿ç”¨Tensorflowä½œä¸ºåç«¯ï¼Œç„¶åæˆ‘ä»¬çœ‹åˆ°å®ƒä»åœ¨è¿è¡Œã€‚å®ƒéœ€è¦ä¸€ç‚¹æ—¶é—´æ‰èƒ½å®Œæˆï¼Œä½†åœ¨æ­¤æœŸé—´ï¼Œè®©æˆ‘å‘ä½ å±•ç¤ºä¸€ä¸‹æˆ‘ä»¬æ­£åœ¨åšçš„ä¸€äº›éƒ¨åˆ†ã€‚
- en: This is the tokenized function it uses a regular expression to split apart the
    words of a sentence so this would turn something like Bob D the apple period where
    is the apple question mark so this is very much in the tasks form but this gives
    you an array of words rather than just one big sentence So this is called tokenization
    it tokenizes the sentence and breaks it apart for you This par is the stories
    So this takes stories that are in Babbby task format and breaks them into the
    three parts So you will get and sort of separated by tab delineation I won't take
    you through all of the steps of this but basically what it is doing is taking
    the three parts the parts are you will have a story where it'll say John walk
    to the bathroom Mary went to the bedroom that's essentiallyã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ ‡è®°åŒ–çš„å‡½æ•°ï¼Œå®ƒä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†ç¦»å¥å­ä¸­çš„å•è¯ï¼Œå› æ­¤è¿™ä¼šå°†â€œBob D the apple. Where is the apple?â€è½¬æ¢ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¤§å¥å­ã€‚è¿™è¢«ç§°ä¸ºæ ‡è®°åŒ–ï¼Œå®ƒå°†å¥å­æ ‡è®°åŒ–å¹¶ä¸ºä½ åˆ†è§£ã€‚è¿™ä¸€éƒ¨åˆ†æ˜¯æ•…äº‹ï¼Œå› æ­¤å®ƒå°†ä»¥Babbbyä»»åŠ¡æ ¼å¼å­˜åœ¨çš„æ•…äº‹åˆ†è§£ä¸ºä¸‰éƒ¨åˆ†ã€‚ä½ å°†ä¼šçœ‹åˆ°ï¼Œå®ƒä»¬æ˜¯é€šè¿‡åˆ¶è¡¨ç¬¦åˆ†éš”çš„ï¼Œæˆ‘ä¸ä¼šè¯¦ç»†è®²è§£æ‰€æœ‰æ­¥éª¤ï¼Œä½†åŸºæœ¬ä¸Šï¼Œå®ƒçš„ä½œç”¨æ˜¯æå–ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬â€œçº¦ç¿°èµ°è¿›æµ´å®¤ï¼Œç›ä¸½èµ°è¿›å§å®¤â€çš„æ•…äº‹ã€‚
- en: First part of it the first of three the second part is in the question where
    is John or some other some other thing and then the third is the answer the answer
    to this type of neural network is always a single word so you are asking a questions
    about the story and then you then give it a question about it and it gives you
    a word back the important thing to realize about the training of the neural network
    is the weights and the training of the neural network are learning to read not
    learning the specific sentences because after the neural network is trained you'll
    give it new sentences and it should be able to answer questions about them because
    the training data has all sorts of instances of Mary's in the bathroom Marys in
    the bedroom all different locations those are specific to that particular training
    example it's not the locations that it's trying to learn it's trying to learn
    how to read new stories and be able to answer about them and then this function
    here that we call will get the stories by calling the other functions that that
    we just had the parse stories and it returns the stories in a vector format that
    can be passed to the neural network findã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€éƒ¨åˆ†æ˜¯ä¸‰éƒ¨åˆ†ä¸­çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯å…³äºâ€œçº¦ç¿°åœ¨å“ªé‡Œâ€æˆ–å…¶ä»–é—®é¢˜ï¼Œç¬¬ä¸‰éƒ¨åˆ†æ˜¯ç­”æ¡ˆï¼Œè¿™ç§ç±»å‹çš„ç¥ç»ç½‘ç»œçš„ç­”æ¡ˆæ€»æ˜¯ä¸€ä¸ªå•è¯ã€‚å› æ­¤ï¼Œä½ åœ¨é—®ä¸€ä¸ªæ•…äº‹çš„é—®é¢˜ï¼Œç„¶åç»™å‡ºä¸€ä¸ªç›¸å…³çš„é—®é¢˜ï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ªå•è¯ã€‚é‡è¦çš„æ˜¯è¦æ„è¯†åˆ°ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ¶‰åŠæƒé‡ï¼Œè€Œä¸æ˜¯å­¦ä¹ ç‰¹å®šçš„å¥å­ï¼Œå› ä¸ºåœ¨ç¥ç»ç½‘ç»œè®­ç»ƒåï¼Œä½ ä¼šç»™å®ƒæ–°çš„å¥å­ï¼Œå®ƒåº”è¯¥èƒ½å¤Ÿå›ç­”å…³äºè¿™äº›å¥å­çš„é—®é¢˜ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸­åŒ…å«äº†å„ç§æƒ…å†µï¼Œæ¯”å¦‚â€œç›ä¸½åœ¨æµ´å®¤â€å’Œâ€œç›ä¸½åœ¨å§å®¤â€ï¼Œè¿™äº›éƒ½æ˜¯ç‰¹å®šäºæŸä¸ªè®­ç»ƒç¤ºä¾‹çš„ä½ç½®ï¼Œå®ƒå¹¶ä¸æ˜¯åœ¨å­¦ä¹ è¿™äº›ä½ç½®ï¼Œè€Œæ˜¯åœ¨å­¦ä¹ å¦‚ä½•é˜…è¯»æ–°æ•…äº‹å¹¶èƒ½å¯¹å…¶è¿›è¡Œå›ç­”ã€‚ç„¶åï¼Œè¿™é‡Œæˆ‘ä»¬ç§°ä¹‹ä¸ºçš„å‡½æ•°ä¼šé€šè¿‡è°ƒç”¨å…¶ä»–å‡½æ•°è·å–æ•…äº‹ï¼Œè¿™äº›å‡½æ•°åˆšæ‰æåˆ°çš„è§£ææ•…äº‹ï¼Œå¹¶å°†æ•…äº‹ä»¥å‘é‡æ ¼å¼è¿”å›ï¼Œè¿™å¯ä»¥ä¼ é€’ç»™ç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ã€‚
- en: vectorize the stories I will show what this specifically looks likeã€‚ but essentially
    what it's doing is it's taking all of the sentencesï¼Œ the storiesã€‚ figuring out
    how many unique words there are and then vectorizing them and changing them into
    pure numeric form where each word has a numeric index that it gets replaced by
    and we'll see how that works in just a little bit this block of code here basically
    downloads the data so we're getting the data from AWS and downloading it to this
    GZ file then we extract out the parts that we want we're doing the 10k challengeã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ•…äº‹è¿›è¡Œå‘é‡åŒ–ï¼Œæˆ‘å°†å±•ç¤ºè¿™å…·ä½“æ˜¯æ€æ ·çš„ã€‚ä½†æœ¬è´¨ä¸Šï¼Œå®ƒçš„ä½œç”¨æ˜¯æå–æ‰€æœ‰çš„å¥å­ã€æ•…äº‹ï¼Œè®¡ç®—æœ‰å¤šå°‘ä¸ªç‹¬ç‰¹çš„å•è¯ï¼Œç„¶åè¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶è½¬æ¢æˆçº¯æ•°å­—å½¢å¼ï¼Œæ¯ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ªæ•°å­—ç´¢å¼•æ¥æ›¿æ¢ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚è¿™æ®µä»£ç åŸºæœ¬ä¸Šä¸‹è½½æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»AWSè·å–æ•°æ®å¹¶ä¸‹è½½åˆ°è¿™ä¸ªGZæ–‡ä»¶ä¸­ï¼Œç„¶åæå–å‡ºæˆ‘ä»¬æƒ³è¦çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ­£åœ¨è¿›è¡Œ10KæŒ‘æˆ˜ã€‚
- en: This is a smaller dataï¼Œ it's still fairly big that keeps us able to run it on
    our computers I should point out the computer that I normally run these examples
    from we're using a AWS instance like I discussed before I basically hit my website
    you won't be able to use that URL because it's my AWS instance and it it costs
    me money not a lot but I normally use a8 gigabã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†ï¼Œè™½ç„¶ä»ç„¶ç›¸å½“å¤§ï¼Œä½†å®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç”µè„‘ä¸Šè¿è¡Œã€‚æˆ‘è¦æŒ‡å‡ºçš„æ˜¯ï¼Œæˆ‘é€šå¸¸è¿è¡Œè¿™äº›ç¤ºä¾‹çš„ç”µè„‘ä½¿ç”¨çš„æ˜¯AWSå®ä¾‹ï¼Œæ­£å¦‚æˆ‘ä¹‹å‰è®¨è®ºçš„é‚£æ ·ï¼Œæˆ‘åŸºæœ¬ä¸Šè®¿é—®æˆ‘çš„ç½‘ç«™ï¼Œä½ æ— æ³•ä½¿ç”¨é‚£ä¸ªURLï¼Œå› ä¸ºå®ƒæ˜¯æˆ‘çš„AWSå®ä¾‹ï¼Œè¿™ä¼šè®©æˆ‘äº§ç”Ÿä¸€äº›è´¹ç”¨ï¼Œè™½ç„¶ä¸å¤šï¼Œä½†æˆ‘é€šå¸¸ä½¿ç”¨çš„æ˜¯8GBçš„å†…å­˜ã€‚
- en: instancest to do this class session since there's considerably more I use a
    I actually use a 16 gigteã€‚ if I was using the full the bigger data sets forms
    of theseã€‚ I would need a much much more ra might work on a computer with8TP so
    we run this I am going to go ahead and run this portion since I ran the previous
    portionã€‚ it is extracting the stories for the challenge and it gets them Now I
    had already run this it had already downloaded them if you were rent this for
    the first time it will probably have to download thoseã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–è¿™ä¸ªè¯¾å ‚ç¯èŠ‚ï¼Œå› ä¸ºæœ‰ç›¸å½“å¤šçš„å†…å®¹ï¼Œæˆ‘å®é™…ä¸Šä½¿ç”¨äº†ä¸€ä¸ª16GBçš„å†…å­˜ã€‚å¦‚æœæˆ‘ä½¿ç”¨çš„æ˜¯æ›´å¤§çš„æ•°æ®é›†ï¼Œæˆ‘ä¼šéœ€è¦æ›´é«˜çš„å†…å­˜ï¼Œå¯èƒ½åœ¨ä¸€å°8TBçš„è®¡ç®—æœºä¸Šè¿è¡Œã€‚å› æ­¤æˆ‘ä»¬æ¥è¿è¡Œè¿™ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä¼šç»§ç»­è¿è¡Œè¿™ä¸€éƒ¨åˆ†ï¼Œå› ä¸ºæˆ‘å·²ç»è¿è¡Œäº†ä¹‹å‰çš„éƒ¨åˆ†ã€‚å®ƒæ­£åœ¨æå–æŒ‘æˆ˜çš„æ•…äº‹ï¼Œå¹¶ä¸”ç°åœ¨æˆ‘å·²ç»è¿è¡Œè¿‡äº†ï¼Œå®ƒå·²ç»ä¸‹è½½äº†å®ƒä»¬ã€‚å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œè¿™ä¸ªï¼Œå®ƒå¯èƒ½éœ€è¦ä¸‹è½½è¿™äº›ã€‚
- en: Let me run this so you can see what the data actually looks like So here's a
    story Mary moved to the bathroom John went to the hallway where is Mary answer
    is bathroom Here's another story and he gonna be multiple sentencesã€‚ Mary moved
    to the bathroom John went to the hallway Daniel went back to the hallway Sand
    move to the garden Where is Daniel So it needs to read these and find the last
    location that he said that he moved to and they describe it different waysã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘è¿è¡Œè¿™ä¸ªï¼Œè®©ä½ çœ‹çœ‹æ•°æ®å®é™…ä¸Šæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªæ•…äº‹ï¼Œç›ä¸½æ¬åˆ°äº†æµ´å®¤ï¼Œçº¦ç¿°å»äº†èµ°å»Šï¼Œç›ä¸½åœ¨å“ªé‡Œï¼Œç­”æ¡ˆæ˜¯æµ´å®¤ã€‚è¿™é‡Œè¿˜æœ‰å¦ä¸€ä¸ªæ•…äº‹ï¼Œè€Œä¸”å°†ä¼šæœ‰å¤šä¸ªå¥å­ã€‚ç›ä¸½æ¬åˆ°äº†æµ´å®¤ï¼Œçº¦ç¿°å»äº†èµ°å»Šï¼Œä¸¹å°¼å°”å›åˆ°äº†èµ°å»Šï¼Œæ¡‘å¾·æ¬åˆ°äº†èŠ±å›­ï¼Œä¸¹å°¼å°”åœ¨å“ªé‡Œï¼Ÿæ‰€ä»¥å®ƒéœ€è¦è¯»å–è¿™äº›å¹¶æ‰¾åˆ°ä»–æœ€åè¯´ä»–æ¬åˆ°çš„ä½ç½®ï¼Œå¹¶ç”¨ä¸åŒçš„æ–¹å¼æè¿°å®ƒã€‚
- en: back to move to went to So it really does need to learn to read this areã€‚Training
    examples I need to emphasize again the neural network is not remembering that
    Mary is in the bathroomã€‚ That's just unique to this particular sentenceã€‚ and there's
    a whole bunch of theseã€‚ there's like 10000 of these that it trains over and the
    answer is always a single word so it learns to read a sentence like this and we'll
    see that in the end we can make up our own ad hoc story for it it will read it
    and it will answer something about it but we'll see that it has some very important
    limitations So first we need to build the vocabulary and this is a very important
    limitation of this program we will run that and let me show you what that looks
    like It builds the vocabulary very quickly What it did is it went over all 10000
    of these training stories and the 1000 test stories so we're going to train it
    on these and we're going to test it on these and we're going to see how well it
    can learn to read it pulls out some very important maximums here that are going
    to be implied are imposed on the trained neural network We have 22 unique words
    that's going to be the vocabulary if you look over all of those sentences and
    youã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°â€œæ¬åˆ°â€å’Œâ€œå»â€çš„é—®é¢˜ã€‚æ‰€ä»¥å®ƒç¡®å®éœ€è¦å­¦ä¹ è¯»å–è¿™äº›è®­ç»ƒç¤ºä¾‹ã€‚æˆ‘éœ€è¦å†æ¬¡å¼ºè°ƒï¼Œç¥ç»ç½‘ç»œå¹¶ä¸è®°å¾—ç›ä¸½åœ¨æµ´å®¤ã€‚è¿™ä»…ä»…æ˜¯è¿™ä¸ªç‰¹å®šå¥å­çš„ç‹¬ç‰¹æ€§ã€‚è¿˜æœ‰ä¸€å¤§å †è¿™æ ·çš„å¥å­ï¼Œå¤§çº¦æœ‰10000ä¸ªä¾›å…¶è®­ç»ƒï¼Œç­”æ¡ˆæ€»æ˜¯ä¸€ä¸ªå•è¯ï¼Œå› æ­¤å®ƒå­¦ä¹ è¯»å–è¿™æ ·çš„å¥å­ã€‚æœ€ç»ˆæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºå®ƒç¼–é€ è‡ªå·±çš„ä¸´æ—¶æ•…äº‹ï¼Œå®ƒä¼šè¯»å–å¹¶å›ç­”ä¸€äº›å†…å®¹ï¼Œä½†æˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒæœ‰ä¸€äº›éå¸¸é‡è¦çš„å±€é™æ€§ã€‚æ‰€ä»¥é¦–å…ˆæˆ‘ä»¬éœ€è¦æ„å»ºè¯æ±‡ï¼Œè¿™æ˜¯è¿™ä¸ªç¨‹åºä¸€ä¸ªéå¸¸é‡è¦çš„é™åˆ¶ã€‚æˆ‘ä»¬å°†è¿è¡Œå®ƒï¼Œè®©æˆ‘ç»™ä½ çœ‹çœ‹é‚£æ˜¯ä»€ä¹ˆæ ·å­ã€‚å®ƒéå¸¸å¿«é€Ÿåœ°æ„å»ºè¯æ±‡ã€‚å®ƒå¯¹æ‰€æœ‰è¿™10000ä¸ªè®­ç»ƒæ•…äº‹å’Œ1000ä¸ªæµ‹è¯•æ•…äº‹è¿›è¡Œäº†å¤„ç†ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†åœ¨è¿™äº›ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¿™äº›ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œçœ‹çœ‹å®ƒèƒ½å­¦ä¼šå¤šå¥½åœ°è¯»å–ã€‚å®ƒæå–äº†ä¸€äº›éå¸¸é‡è¦çš„æå€¼ï¼Œè¿™äº›å°†ä¼šè¢«éšå«æˆ–æ–½åŠ åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šã€‚æˆ‘ä»¬æœ‰22ä¸ªç‹¬ç‰¹çš„å•è¯ï¼Œè¿™å°†æ˜¯è¯æ±‡ï¼Œå¦‚æœä½ æŸ¥çœ‹æ‰€æœ‰é‚£äº›å¥å­ã€‚
- en: Mary move to bathroom all those words and you find out how many uniqueã€‚ There's
    only 22 unique wordsã€‚ This neural network gets veryï¼Œ very memory intense when
    you add more words because those are basically becoming dummy variables so that
    is a big limitation of this but if you throw a memory at it and you throw a computer
    at it there are examples of this that can learn much bigger vocabularies the maximum
    story length is 68 so we're going to essentially create an input that is 68 words
    by 22 dummy variables that's the embedding and another input of the question so
    the query or the question the maximum length that any query was in there is four
    wordsã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç›ä¸½æ¬åˆ°æµ´å®¤ï¼Œæ‰€æœ‰é‚£äº›å•è¯ï¼Œä½ ä¼šå‘ç°æœ‰å¤šå°‘ä¸ªç‹¬ç‰¹çš„ã€‚åªæœ‰22ä¸ªç‹¬ç‰¹çš„å•è¯ã€‚å½“ä½ æ·»åŠ æ›´å¤šå•è¯æ—¶ï¼Œè¿™ä¸ªç¥ç»ç½‘ç»œå˜å¾—éå¸¸éå¸¸å ç”¨å†…å­˜ï¼Œå› ä¸ºè¿™äº›åŸºæœ¬ä¸Šå˜æˆäº†è™šæ‹Ÿå˜é‡ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„é™åˆ¶ã€‚ä½†æ˜¯å¦‚æœä½ ç»™å®ƒæ›´å¤šçš„å†…å­˜å’Œè®¡ç®—æœºï¼Œç¡®å®æœ‰ç¤ºä¾‹å¯ä»¥å­¦ä¹ æ›´å¤§çš„è¯æ±‡ã€‚æœ€å¤§æ•…äº‹é•¿åº¦æ˜¯68ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†åŸºæœ¬ä¸Šåˆ›å»ºä¸€ä¸ªè¾“å…¥ï¼Œç”±68ä¸ªå•è¯å’Œ22ä¸ªè™šæ‹Ÿå˜é‡ç»„æˆï¼Œé‚£å°±æ˜¯åµŒå…¥ï¼Œå¦ä¸€ä¸ªè¾“å…¥æ˜¯é—®é¢˜ï¼Œå› æ­¤æŸ¥è¯¢æˆ–é—®é¢˜çš„æœ€å¤§é•¿åº¦æ˜¯4ä¸ªå•è¯ã€‚
- en: So the input to this neural network is going to become the story which is a
    maximum of 68 words by dummy by 22 dummies and the query length is four words
    by 22 so it effectively becomes 7268 plus4 times the 22 dummies is the input you
    can see this gets big quickly if you start to add additional words for that and
    this is what the vocabulary looks likeã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ä¸ªç¥ç»ç½‘ç»œçš„è¾“å…¥å°†æˆä¸ºæ•…äº‹ï¼Œæœ€å¤§ä¸º68ä¸ªå•è¯ï¼Œæ—è¾¹æ˜¯22ä¸ªè™šæ‹Ÿå€¼ï¼ŒæŸ¥è¯¢é•¿åº¦æ˜¯å››ä¸ªå•è¯ï¼Œæ—è¾¹ä¹Ÿæ˜¯22ä¸ªè™šæ‹Ÿå€¼ã€‚å› æ­¤ï¼Œå®ƒå®é™…ä¸Šå˜æˆ7268åŠ ä¸Š4ä¹˜ä»¥22ä¸ªè™šæ‹Ÿå€¼çš„è¾“å…¥ã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœä½ å¼€å§‹æ·»åŠ é¢å¤–çš„å•è¯ï¼Œè¿™ä¸ªä¼šè¿…é€Ÿå˜å¾—å¾ˆå¤§ï¼Œè¿™å°±æ˜¯è¯æ±‡çš„æ ·å­ã€‚
- en: Now this neural network will take a while to train it will take up to on the
    faster instance that I'm running this onã€‚ it will take about four minutes to train
    but on slower instances with with fewer compute cores this could take longer so
    we want to save the neural network We also want to save this vocabulary because
    we need the lookup This is based on maps and Python can change the ordering for
    maps you're not guaranteed that putting things into the map will always be the
    same size so this is the lookup table this is telling us that zero is period two
    is Daniel and so on and so forth so when we vectorize these what it's going to
    do is it'll change these words into lookups so Mary is4 Mary will become a4 moved
    15 so that'll become 415 etc so to build the training set let me go ahead and
    execute this part of it it tells you what the input is going to look like so you
    have a 10000 times 68 that's the input vector 10000 times4 and then the answers
    which is going to be the vocab size let me show you some individual trainã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™ä¸ªç¥ç»ç½‘ç»œéœ€è¦ä¸€äº›æ—¶é—´æ¥è®­ç»ƒï¼Œåœ¨æˆ‘è¿è¡Œçš„è¿™ä¸ªè¾ƒå¿«å®ä¾‹ä¸Šï¼Œå®ƒå¤§çº¦éœ€è¦å››åˆ†é’Ÿè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨è®¡ç®—æ ¸å¿ƒè¾ƒå°‘çš„è¾ƒæ…¢å®ä¾‹ä¸Šï¼Œè¿™å¯èƒ½ä¼šèŠ±è´¹æ›´é•¿æ—¶é—´ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›ä¿å­˜è¿™ä¸ªç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬è¿˜å¸Œæœ›ä¿å­˜è¿™ä¸ªè¯æ±‡è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æŸ¥æ‰¾ã€‚è¿™æ˜¯åŸºäºæ˜ å°„çš„ï¼Œè€ŒPythonä¸­çš„æ˜ å°„é¡ºåºæ˜¯å¯ä»¥æ”¹å˜çš„ï¼Œå› æ­¤ä½ ä¸èƒ½ä¿è¯å°†ä¸œè¥¿æ”¾å…¥æ˜ å°„åæ€»æ˜¯ä¼šæœ‰ç›¸åŒçš„å¤§å°ã€‚è¿™æ˜¯æŸ¥æ‰¾è¡¨ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬é›¶å¯¹åº”çš„æ˜¯å¥å·ï¼ŒäºŒå¯¹åº”çš„æ˜¯ä¸¹å°¼å°”ï¼Œä¾æ­¤ç±»æ¨ã€‚å½“æˆ‘ä»¬å¯¹è¿™äº›è¿›è¡Œå‘é‡åŒ–æ—¶ï¼Œå®ƒä¼šå°†è¿™äº›è¯è½¬å˜ä¸ºæŸ¥æ‰¾å€¼ï¼Œä¾‹å¦‚ç›ä¸½æ˜¯4ï¼Œç›ä¸½ä¼šå˜æˆ4ï¼Œç§»åŠ¨15ï¼Œæ‰€ä»¥å®ƒä¼šå˜æˆ415ç­‰ç­‰ã€‚å› æ­¤ï¼Œä¸ºäº†æ„å»ºè®­ç»ƒé›†ï¼Œè®©æˆ‘ç»§ç»­æ‰§è¡Œè¿™éƒ¨åˆ†ï¼Œå®ƒå‘Šè¯‰ä½ è¾“å…¥å°†ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼Œæ‰€ä»¥ä½ æœ‰ä¸€ä¸ª10000ä¹˜ä»¥68çš„è¾“å…¥å‘é‡ï¼Œä¸€ä¸ª10000ä¹˜ä»¥4çš„è¾“å‡ºï¼Œç„¶åç­”æ¡ˆå°†æ˜¯è¯æ±‡å¤§å°ï¼Œè®©æˆ‘ç»™ä½ å±•ç¤ºä¸€äº›å•ç‹¬çš„è®­ç»ƒå…ƒç´ ã€‚
- en: ElementsSo this shows you the story so you'll notice we're always making this
    be the same lengthã€‚ so the 68ï¼Œ which was the maximum word size of any of the stories
    and it's paddedã€‚ it's padded to the left So all zeros at the beginning and then
    the actual the actual words that you get here Now one important thing to show
    you these zeros the way the padding works and we have to adjust for this at the
    endã€‚ it's a very important part and I will show you this at the end but notice
    how it says notice how we have 21 words if you count these two things if you were
    paying attention you probably saw 22 unique words so the math doesn't quite add
    up the additional value that we have is really the empty So null 0 is actually
    null0 is the first the first value in that map but really the value is going in
    get one added to it so0 would be one and so on we see that we have to adjust for
    that at the end but that is why we have zeros here So those are actually nulls
    and then we get into the actual values of of the lookup same thing here for the
    question Now this question was the maximum of fourã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç¤ºäº†æ•…äº‹ï¼Œæ‰€ä»¥ä½ ä¼šæ³¨æ„åˆ°æˆ‘ä»¬å§‹ç»ˆä¿æŒè¿™ä¸ªé•¿åº¦ç›¸åŒã€‚68æ˜¯æ‰€æœ‰æ•…äº‹ä¸­å•è¯çš„æœ€å¤§é•¿åº¦ï¼Œå¹¶ä¸”è¿›è¡Œäº†å¡«å……ã€‚å¡«å……æ˜¯åœ¨å·¦è¾¹ï¼Œæ‰€ä»¥å¼€å¤´æ˜¯å…¨é›¶ï¼Œç„¶åæ˜¯ä½ åœ¨è¿™é‡Œå¾—åˆ°çš„å®é™…å•è¯ã€‚ç°åœ¨ä¸€ä¸ªé‡è¦çš„äº‹æƒ…æ˜¯è¦å±•ç¤ºè¿™äº›é›¶ï¼Œå¡«å……çš„å·¥ä½œæ–¹å¼ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æœ€åè¿›è¡Œè°ƒæ•´ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†ï¼Œæˆ‘ä¼šåœ¨æœ€åç»™ä½ å±•ç¤ºï¼Œä½†è¯·æ³¨æ„æˆ‘ä»¬æœ‰21ä¸ªå•è¯ï¼Œå¦‚æœä½ æ•°ä¸€æ•°è¿™ä¸¤ä¸ªä¸œè¥¿ï¼Œå¦‚æœä½ åœ¨å…³æ³¨ï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°22ä¸ªç‹¬ç‰¹çš„å•è¯ï¼Œæ‰€ä»¥æ•°å­¦ä¸å¤ªå¯¹åŠ²ã€‚æˆ‘ä»¬æœ‰çš„é¢å¤–å€¼å®é™…ä¸Šæ˜¯ç©ºå€¼ã€‚æ‰€ä»¥null
    0å®é™…ä¸Šæ˜¯æ˜ å°„ä¸­çš„ç¬¬ä¸€ä¸ªå€¼ï¼Œä½†å®é™…ä¸Šè¿™ä¸ªå€¼åŠ 1ï¼Œæ‰€ä»¥0ä¼šå˜æˆ1ï¼Œä¾æ­¤ç±»æ¨ã€‚æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬å¿…é¡»åœ¨æœ€åè¿›è¡Œè°ƒæ•´ï¼Œä½†è¿™å°±æ˜¯æˆ‘ä»¬è¿™é‡Œæœ‰é›¶çš„åŸå› ã€‚å› æ­¤é‚£äº›å®é™…ä¸Šæ˜¯ç©ºå€¼ï¼Œç„¶åæˆ‘ä»¬è¿›å…¥æŸ¥æ‰¾çš„å®é™…å€¼ï¼Œé—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç°åœ¨è¿™ä¸ªé—®é¢˜çš„æœ€å¤§é•¿åº¦æ˜¯å››ã€‚
- en: So we have no zero padding now we need to compile the neural network because
    Karas does not natively support the end to end neural network there is basically
    the compute graph has to be created here at least to some degree I will show you
    a little bit of what this looks like we won't go really through all the details
    I would have to go I would have to take you through the entire paper really to
    get through that but we're basically creating three encoders two for the sentences
    and then two for the you sort of extract facts out of the sentence and then you
    encode those facts into with the question to try to build up the answer so that's
    why you need two encoders for for the input which are there and then you have
    another encoder for the question and the input dimension becomes the vocabulary
    size so they can build out those dummy variables to hold that output dimension
    then becomes essentially the number of values in in the next layer then we feed
    these all together and eventually all funnels into an LSTM we're using dropouts
    regularizationã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ç°åœ¨æ²¡æœ‰é›¶å¡«å……ï¼Œæˆ‘ä»¬éœ€è¦ç¼–è¯‘ç¥ç»ç½‘ç»œï¼Œå› ä¸ºKerasæœ¬èº«ä¸æ”¯æŒç«¯åˆ°ç«¯çš„ç¥ç»ç½‘ç»œï¼ŒåŸºæœ¬ä¸Šå¿…é¡»åœ¨è¿™é‡Œåˆ›å»ºè®¡ç®—å›¾ï¼Œè‡³å°‘åœ¨æŸç§ç¨‹åº¦ä¸Šã€‚æˆ‘å°†å‘ä½ å±•ç¤ºè¿™çœ‹èµ·æ¥æœ‰ç‚¹åƒä»€ä¹ˆï¼Œæˆ‘ä»¬ä¸ä¼šçœŸæ­£æ·±å…¥æ‰€æœ‰ç»†èŠ‚ï¼Œæˆ‘éœ€è¦å¸¦ä½ çœ‹æ•´ç¯‡è®ºæ–‡æ‰èƒ½äº†è§£ï¼Œä½†æˆ‘ä»¬åŸºæœ¬ä¸Šåˆ›å»ºäº†ä¸‰ä¸ªç¼–ç å™¨ï¼Œå…¶ä¸­ä¸¤ä¸ªç”¨äºå¥å­ï¼Œç„¶åä¸¤ä¸ªç”¨äºä»å¥å­ä¸­æå–äº‹å®ï¼Œç„¶åå°†è¿™äº›äº‹å®ä¸é—®é¢˜ç¼–ç åœ¨ä¸€èµ·ï¼Œè¯•å›¾æ„å»ºç­”æ¡ˆã€‚æ‰€ä»¥ä½ éœ€è¦ä¸¤ä¸ªç”¨äºè¾“å…¥çš„ç¼–ç å™¨ï¼Œè€Œä½ è¿˜æœ‰ä¸€ä¸ªç”¨äºé—®é¢˜çš„ç¼–ç å™¨ï¼Œè¾“å…¥ç»´åº¦å˜æˆè¯æ±‡å¤§å°ï¼Œä»¥ä¾¿æ„å»ºé‚£äº›å ä½å˜é‡ï¼Œè¾“å‡ºç»´åº¦åŸºæœ¬ä¸Šæ˜¯ä¸‹ä¸€å±‚ä¸­çš„å€¼çš„æ•°é‡ã€‚ç„¶åæˆ‘ä»¬å°†è¿™äº›å…¨éƒ¨ç»“åˆåœ¨ä¸€èµ·ï¼Œæœ€ç»ˆæ±‡èšåˆ°ä¸€ä¸ªLSTMä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†dropoutæ­£åˆ™åŒ–ã€‚
- en: Throughout this and then it finally goes into a softmax on the answerã€‚ which
    is essentially the classification of which of the vocabulary does it feel is the
    answer to thisã€‚ And then finally we create the model and train it or in compile
    itã€‚ we don't train it This part runs pretty quickly It creates basically the neural
    network and the neural network is now built and compiled This is where we're going
    to actually train it we're going to use a batch size of 32 and 120 epos Now this
    is very importantã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œå®ƒæœ€ç»ˆè¿›å…¥äº†ç­”æ¡ˆçš„softmaxï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯å¯¹è¯æ±‡ä¸­å“ªä¸ªæ˜¯ç­”æ¡ˆçš„åˆ†ç±»ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºæ¨¡å‹å¹¶è®­ç»ƒæˆ–ç¼–è¯‘å®ƒï¼Œæˆ‘ä»¬å¹¶ä¸è®­ç»ƒã€‚è¿™ä¸ªéƒ¨åˆ†è¿è¡Œå¾—éå¸¸å¿«ï¼ŒåŸºæœ¬ä¸Šåˆ›å»ºäº†ç¥ç»ç½‘ç»œï¼Œç¥ç»ç½‘ç»œç°åœ¨å·²ç»æ„å»ºå¹¶ç¼–è¯‘ã€‚è¿™æ˜¯æˆ‘ä»¬å°†å®é™…è®­ç»ƒå®ƒçš„åœ°æ–¹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨32çš„æ‰¹é‡å¤§å°å’Œ120çš„epochsã€‚ç°åœ¨è¿™ä¸€ç‚¹éå¸¸é‡è¦ã€‚
- en: We're going to save the neural network so that since it takes a while to build
    thisã€‚ we don't want to just throw it away so we save the neural networkã€‚ I added
    this code from the original exampleã€‚ they did not save the neural network but
    we save it to a H5 file which is a binary fileã€‚ I also save the vocabulary to
    a pickle file Pickle is your standard serialization in Python and I report on
    the elapsed time So I am going to go ahead and run this and this will I'll speed
    this upã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¿å­˜ç¥ç»ç½‘ç»œï¼Œå› ä¸ºæ„å»ºè¿™ä¸ªç½‘ç»œéœ€è¦ä¸€æ®µæ—¶é—´ï¼Œæˆ‘ä»¬ä¸æƒ³å°±è¿™æ ·ä¸¢å¼ƒå®ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¿å­˜ç¥ç»ç½‘ç»œã€‚æˆ‘ä»åŸå§‹ç¤ºä¾‹ä¸­æ·»åŠ äº†è¿™æ®µä»£ç ï¼Œä»–ä»¬æ²¡æœ‰ä¿å­˜ç¥ç»ç½‘ç»œï¼Œä½†æˆ‘ä»¬å°†å…¶ä¿å­˜ä¸ºH5æ–‡ä»¶ï¼Œè¿™æ˜¯ä¸€ç§äºŒè¿›åˆ¶æ–‡ä»¶ã€‚æˆ‘è¿˜å°†è¯æ±‡ä¿å­˜ä¸ºpickleæ–‡ä»¶ï¼Œpickleæ˜¯Pythonä¸­çš„æ ‡å‡†åºåˆ—åŒ–æ ¼å¼ï¼Œæˆ‘æŠ¥å‘Šäº†ç»è¿‡çš„æ—¶é—´ã€‚æ‰€ä»¥æˆ‘å°†ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œå¹¶åŠ é€Ÿå¤„ç†ã€‚
- en: but itll could take4 to 8 minutes to actually do this you see the epoch speeding
    along here and it's going through the training setã€‚ which is100 elementsï¼Œ so it
    just continues to go through thisã€‚Using any sort of early stopping it is just
    going to stop at 120 Okay not too badã€‚ about four minutes with this faster AWS
    instanceã€‚ This code I'm not going to execute the neural network was saved the
    chatbot was saved H5 and the vocabulary to a pickle file if you were running this
    and you already had the chat bot and the pickle file saved you could basically
    just run this part instead of the training we're going to evaluate the accuracy
    noticeice that we we do this very similar to the code that we've done before except
    look at the input the input due to the way that that compute graph was structured
    earlierã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®é™…ä¸Šå®Œæˆè¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦4åˆ°8åˆ†é’Ÿï¼Œä½ å¯ä»¥çœ‹åˆ°æ—¶ä»£æ­£åœ¨å¿«é€Ÿæ¨è¿›ï¼Œå¹¶ä¸”å®ƒæ­£åœ¨å¤„ç†è®­ç»ƒé›†ã€‚è®­ç»ƒé›†åŒ…å«100ä¸ªå…ƒç´ ï¼Œæ‰€ä»¥å®ƒåªæ˜¯ç»§ç»­è¿›è¡Œã€‚ä½¿ç”¨ä»»ä½•å½¢å¼çš„æå‰åœæ­¢ï¼Œå®ƒå°†åœ¨120æ—¶åœæ­¢ï¼Œå¥½çš„ï¼Œè¿˜ä¸é”™ã€‚ä½¿ç”¨è¿™ä¸ªæ›´å¿«çš„AWSå®ä¾‹å¤§çº¦éœ€è¦å››åˆ†é’Ÿã€‚è¿™ä¸ªä»£ç æˆ‘ä¸ä¼šæ‰§è¡Œï¼Œç¥ç»ç½‘ç»œå·²ä¿å­˜ï¼ŒèŠå¤©æœºå™¨äººå·²ä¿å­˜ä¸ºH5æ ¼å¼ï¼Œè¯æ±‡ä¹Ÿä¿å­˜ä¸ºpickleæ–‡ä»¶ã€‚å¦‚æœä½ æ­£åœ¨è¿è¡Œè¿™ä¸ªå¹¶ä¸”å·²ç»ä¿å­˜äº†èŠå¤©æœºå™¨äººå’Œpickleæ–‡ä»¶ï¼Œä½ åŸºæœ¬ä¸Šå¯ä»¥åªè¿è¡Œè¿™éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯è®­ç»ƒï¼Œæˆ‘ä»¬å°†è¯„ä¼°å‡†ç¡®æ€§ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬çš„åšæ³•ä¸ä¹‹å‰çš„ä»£ç éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯çœ‹è¾“å…¥ï¼Œç”±äºè®¡ç®—å›¾æ—©æœŸçš„ç»“æ„æ–¹å¼ï¼Œè¾“å…¥æœ‰æ‰€ä¸åŒã€‚
- en: the input is actually two vectors it is the inputs and the queries so the inputs
    are the stories Mary went to the hallway etc queries are the questions are where
    is Mary and we'll run it and we'll print out the predictions so this is the typical
    probability that that we've had before you'll see now we're not seeing all of
    the values but each row is basically showing you the 22 vocab words that you had
    is showing you what the probability ofã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å®é™…ä¸Šæ˜¯ä¸¤ä¸ªå‘é‡ï¼Œä¸€ä¸ªæ˜¯è¾“å…¥ï¼Œå¦ä¸€ä¸ªæ˜¯æŸ¥è¯¢ï¼Œæ‰€ä»¥è¾“å…¥æ˜¯æ•…äº‹ï¼Œç›ä¸½å»äº†èµ°å»Šï¼Œç­‰ç­‰ï¼ŒæŸ¥è¯¢æ˜¯é—®é¢˜ï¼Œæ¯”å¦‚ç›ä¸½åœ¨å“ªé‡Œï¼Œæˆ‘ä»¬å°†è¿è¡Œå®ƒå¹¶æ‰“å°å‡ºé¢„æµ‹ç»“æœï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„å…¸å‹æ¦‚ç‡ã€‚ä½ ç°åœ¨ä¼šçœ‹åˆ°æˆ‘ä»¬å¹¶æ²¡æœ‰çœ‹åˆ°æ‰€æœ‰çš„å€¼ï¼Œä½†æ¯ä¸€è¡ŒåŸºæœ¬ä¸Šæ˜¾ç¤ºçš„æ˜¯ä½ æ‹¥æœ‰çš„22ä¸ªè¯æ±‡çš„æ¦‚ç‡ã€‚
- en: of those words being the answerã€‚ Now you can see if you use a decentsized vocabulary
    where you have maybe 20000 wordsã€‚ you can the inputs to this type of network become
    veryï¼Œ very largeã€‚ So just like before we're gonna take the arg maxes of these
    the arg max now is basically the index of each of these so that's showing vocabular
    word 12 was the expected answer9 was the expected answer there are 151215 not
    all of the words and the vocab list tend to have tend to be the answer very often
    you can definitely see that certain numbers are chosen more often than others
    we can also print out our final accuracy95%'s actually very good Now I want to
    show you how to do an ad hoc query the original example from Cars that I copy
    these from didn't do didn't display the accuracy or show you how to break these
    vectors apart so that's all added by this class if you happen to look at the original
    the original example that I have a link to just above they also don't show you
    how to do an ad hoc query which I think is probably the coolest part of this you've
    trained this great neural network that learns how to read so you want to try to
    testã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¯æ˜¯ç­”æ¡ˆã€‚ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœä½ ä½¿ç”¨ä¸€ä¸ªåˆé€‚å¤§å°çš„è¯æ±‡é‡ï¼Œæ¯”å¦‚20000ä¸ªè¯ï¼Œè¿™ç§ç±»å‹çš„ç½‘ç»œçš„è¾“å…¥ä¼šå˜å¾—éå¸¸éå¸¸å¤§ã€‚å› æ­¤ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†å–è¿™äº›çš„arg
    maxï¼Œç°åœ¨çš„arg maxåŸºæœ¬ä¸Šæ˜¯æ¯ä¸ªçš„ç´¢å¼•ï¼Œæ‰€ä»¥è¿™æ˜¾ç¤ºè¯æ±‡å•è¯12æ˜¯æœŸæœ›çš„ç­”æ¡ˆï¼Œ9æ˜¯æœŸæœ›çš„ç­”æ¡ˆï¼Œ151215ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„è¯æ±‡åˆ—è¡¨ä¸­çš„è¯éƒ½ç»å¸¸æ˜¯ç­”æ¡ˆã€‚ä½ è‚¯å®šèƒ½çœ‹åˆ°æŸäº›æ•°å­—æ¯”å…¶ä»–æ•°å­—æ›´å¸¸è¢«é€‰ä¸­ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰“å°å‡ºæˆ‘ä»¬çš„æœ€ç»ˆå‡†ç¡®ç‡ï¼Œ95%å®é™…ä¸Šéå¸¸å¥½ã€‚ç°åœ¨æˆ‘æƒ³å‘Šè¯‰ä½ å¦‚ä½•è¿›è¡Œä¸´æ—¶æŸ¥è¯¢ã€‚æˆ‘ä»æ±½è½¦ä¸­å¤åˆ¶çš„åŸå§‹ç¤ºä¾‹æ²¡æœ‰æ˜¾ç¤ºå‡†ç¡®ç‡æˆ–å‘Šè¯‰ä½ å¦‚ä½•åˆ†è§£è¿™äº›å‘é‡ï¼Œå› æ­¤è¿™äº›éƒ½æ˜¯è¿™ä¸ªç±»æ·»åŠ çš„ã€‚å¦‚æœä½ ç¢°å·§æŸ¥çœ‹æˆ‘ä¸Šé¢é“¾æ¥çš„åŸå§‹ç¤ºä¾‹ï¼Œä»–ä»¬ä¹Ÿæ²¡æœ‰å‘Šè¯‰ä½ å¦‚ä½•è¿›è¡Œä¸´æ—¶æŸ¥è¯¢ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯æœ€é…·çš„éƒ¨åˆ†ã€‚ä½ è®­ç»ƒäº†è¿™ä¸ªä¼Ÿå¤§çš„ç¥ç»ç½‘ç»œï¼Œå®ƒå­¦ä¼šäº†å¦‚ä½•é˜…è¯»ï¼Œæ‰€ä»¥ä½ æƒ³å°è¯•æµ‹è¯•ã€‚
- en: So I am giving it literally raw text right hereã€‚ It prints out its vocabularyã€‚
    It's reminding youã€‚ remember I only know these words and I tell it Mary moved
    to the bathroomã€‚ John went to the gardenã€‚ etcã€‚ Where is Maryï¼Œ and I run it the
    ad hoc query This is an area that I added to the program so you'll see Mary moved
    to the gardenã€‚ John went to the garden That's an easy one I just kind of made
    that up and I say where is Maryã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘åœ¨è¿™é‡Œç»™å‡ºçš„æ˜¯å­—é¢ä¸Šçš„åŸå§‹æ–‡æœ¬ã€‚å®ƒä¼šè¾“å‡ºå…¶è¯æ±‡é‡ã€‚å®ƒåœ¨æé†’ä½ ã€‚è®°ä½ï¼Œæˆ‘åªçŸ¥é“è¿™äº›è¯ï¼Œæˆ‘å‘Šè¯‰å®ƒç›ä¸½å»æ´—æ‰‹é—´äº†ã€‚çº¦ç¿°å»äº†èŠ±å›­ã€‚ç­‰ç­‰ã€‚ç›ä¸½åœ¨å“ªé‡Œï¼Œæˆ‘è¿è¡Œä¸´æ—¶æŸ¥è¯¢ã€‚è¿™æ˜¯æˆ‘æ·»åŠ åˆ°ç¨‹åºä¸­çš„ä¸€ä¸ªåŒºåŸŸï¼Œæ‰€ä»¥ä½ ä¼šçœ‹åˆ°ç›ä¸½å»äº†èŠ±å›­ã€‚çº¦ç¿°å»äº†èŠ±å›­ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œæˆ‘åªæ˜¯éšä¾¿ç¼–é€ çš„ï¼Œç„¶åæˆ‘é—®ï¼Œç›ä¸½åœ¨å“ªé‡Œã€‚
- en: they're both in the garden So if if I run that it'll say gardenã€‚ I can change
    this to Mary went to the bathroom and it better change its example or it's answer
    thereã€‚ a live demo bathroom good Mary traveled and there's several ways you can
    spell travel depending on the English systemã€‚ they're using the two L so I have
    to used the2 Ls but it doesn't matter it's still understand that John went to
    the gardenã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬éƒ½åœ¨èŠ±å›­é‡Œã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œå®ƒä¼šè¯´èŠ±å›­ã€‚æˆ‘å¯ä»¥æŠŠå®ƒæ”¹æˆç›ä¸½å»äº†æ´—æ‰‹é—´ï¼Œå®ƒä¼šæ›´å¥½åœ°æ”¹å˜ç¤ºä¾‹æˆ–ç­”æ¡ˆã€‚ä¸€ä¸ªå®æ—¶æ¼”ç¤ºï¼Œæ´—æ‰‹é—´å¥½ï¼Œç›ä¸½æ—…è¡Œäº†ï¼Œæ ¹æ®è‹±è¯­ç³»ç»Ÿï¼Œ"æ—…è¡Œ"å¯ä»¥æœ‰å‡ ç§æ‹¼å†™æ–¹å¼ã€‚ä»–ä»¬ä½¿ç”¨ä¸¤ä¸ªLï¼Œæ‰€ä»¥æˆ‘å¿…é¡»ä½¿ç”¨ä¸¤ä¸ªLï¼Œä½†è¿™æ²¡å…³ç³»ï¼Œå®ƒä»ç„¶ç†è§£çº¦ç¿°å»äº†èŠ±å›­ã€‚
- en: there's additional people that you can add into here I can add Daniel and we're
    moving Mary to the hallway and we can run it and it's not retraining the neural
    network It's using the neural network that is learned to read these type of sentences
    and answering the questionsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–äººï¼Œæˆ‘å¯ä»¥æ·»åŠ ä¸¹å°¼å°”ï¼Œæˆ‘ä»¬æŠŠç›ä¸½ç§»åˆ°èµ°å»Šï¼Œç„¶åæˆ‘ä»¬å¯ä»¥è¿è¡Œå®ƒï¼Œè€Œå®ƒå¹¶æ²¡æœ‰é‡æ–°è®­ç»ƒç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯ä½¿ç”¨å·²ç»å­¦ä¼šé˜…è¯»è¿™äº›å¥å­çš„ç¥ç»ç½‘ç»œæ¥å›ç­”é—®é¢˜ã€‚
- en: I can also change the queryã€‚Say where is Daniel and it adjust it for thatã€‚ I'll
    try Now let me show you a few things that don't work so wellã€‚ If I say Daniel
    went to the gameã€‚ that's outside the vocabularyã€‚ it will blow up on that because
    that's not in the dictionary to look up so we can't say Daniel went to the game
    I have not honestly tried this one but I'm going to try hereã€‚ I'm just curious
    what it will do does it have a concept of time So if I say Daniel went to the
    hallwayã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹Ÿå¯ä»¥æ”¹å˜æŸ¥è¯¢ï¼Œæ¯”å¦‚é—®ä¸¹å°¼å°”åœ¨å“ªé‡Œï¼Œå®ƒä¼šä¸ºæ­¤è¿›è¡Œè°ƒæ•´ã€‚è®©æˆ‘è¯•è¯•ã€‚ç°åœ¨è®©æˆ‘ç»™ä½ å±•ç¤ºä¸€äº›æ•ˆæœä¸å¤ªå¥½çš„äº‹æƒ…ã€‚å¦‚æœæˆ‘è¯´ä¸¹å°¼å°”å»äº†æ¯”èµ›ï¼Œé‚£å°±è¶…å‡ºäº†è¯æ±‡èŒƒå›´ã€‚å› ä¸ºåœ¨å­—å…¸ä¸­æ‰¾ä¸åˆ°ï¼Œæ‰€ä»¥ä¼šå´©æºƒã€‚æˆ‘è€å®è¯´è¿˜æ²¡æœ‰å°è¯•è¿™ä¸ªï¼Œä½†æˆ‘æƒ³è¯•è¯•ã€‚æˆ‘åªæ˜¯å¥½å¥‡å®ƒä¼šæ€ä¹ˆåšï¼Œå®ƒæ˜¯å¦æœ‰æ—¶é—´çš„æ¦‚å¿µã€‚å¦‚æœæˆ‘è¯´ä¸¹å°¼å°”å»äº†èµ°å»Šã€‚
- en: Mary went to the bathroomã€‚ Daniel went to the bedroom Daniel has changed his
    location he was in the hallwayã€‚ then he went to the bedroomã€‚ what happens if I
    ask it about that it's smartã€‚ it figures that out because if I change this Daniel
    was in the hallwayã€‚ he went to the bathroom and then it updates it for the other
    oneã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç›ä¸½å»äº†æ´—æ‰‹é—´ã€‚ä¸¹å°¼å°”å»äº†å§å®¤ã€‚ä¸¹å°¼å°”æ”¹å˜äº†ä»–çš„åœ°ç‚¹ï¼Œä»–æ›¾åœ¨èµ°å»Šï¼Œç„¶åå»äº†å§å®¤ã€‚å¦‚æœæˆ‘é—®å®ƒè¿™ä¸ªé—®é¢˜ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå®ƒå¾ˆèªæ˜ã€‚å®ƒèƒ½ç†è§£ï¼Œå› ä¸ºå¦‚æœæˆ‘æ”¹å˜è¿™ä¸ªï¼Œä¸¹å°¼å°”åœ¨èµ°å»Šï¼Œä»–å»äº†æ´—æ‰‹é—´ï¼Œç„¶åå®ƒä¼šä¸ºå¦ä¸€ä¸ªæ›´æ–°ã€‚
- en: if you think about writing a traditional program to do thisã€‚ you would be parsing
    this apart and keeping some sort of state of where everybody is at The neural
    network is just figuring out how to do that and it figures out how to override
    it with subsequent answers and I bet there's examples in the training set that
    teach it this teach it that the last answer is correct you could adjust the training
    set so that the first answer is correct likeã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è€ƒè™‘ç¼–å†™ä¸€ä¸ªä¼ ç»Ÿç¨‹åºæ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ éœ€è¦è§£æè¿™äº›å†…å®¹å¹¶ä¿æŒæŸç§çŠ¶æ€ï¼Œä»¥äº†è§£æ¯ä¸ªäººçš„ä½ç½®ã€‚ç¥ç»ç½‘ç»œåªæ˜¯å¼„æ¸…æ¥šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å®ƒä¼šéšç€åç»­ç­”æ¡ˆçš„å‡ºç°è€Œè¦†ç›–è¿™äº›çŠ¶æ€ã€‚æˆ‘æ•¢æ‰“èµŒï¼Œè®­ç»ƒé›†ä¸­æœ‰ä¸€äº›ç¤ºä¾‹æ•™å®ƒè¿™ä¸€ç‚¹ï¼Œæ•™å®ƒæœ€åä¸€ä¸ªç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚ä½ å¯ä»¥è°ƒæ•´è®­ç»ƒé›†ï¼Œä»¥ä¾¿ç¬¬ä¸€ä¸ªç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚
- en: groundund information it would learn that you could also put the word originally
    in there and train it for Daniel went to the hallwayã€‚ Mary went to the bathroomã€‚
    Daniel went to the bedroomã€‚ Where was Daniel originally and it would very likely
    learn to know that that was the first location that the given person was inã€‚ I'm
    going to also try something else mean to itã€‚ I'm going to ask it where is Sandra
    who does not even appear in this in this storyã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç¡€ä¿¡æ¯ï¼Œå®ƒä¼šå­¦ä¹ åˆ°ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒåŠ å…¥â€œæœ€åˆâ€è¿™ä¸ªè¯ï¼Œå¹¶ä¸ºâ€œä¸¹å°¼å°”å»äº†èµ°å»Šã€‚ç›ä¸½å»äº†æ´—æ‰‹é—´ã€‚ä¸¹å°¼å°”å»äº†å§å®¤ã€‚â€è¿›è¡Œè®­ç»ƒã€‚é—®ä¸¹å°¼å°”æœ€åˆåœ¨å“ªé‡Œï¼Œå®ƒå¾ˆå¯èƒ½ä¼šå­¦ä¼šçŸ¥é“é‚£æ˜¯è¯¥äººæœ€åˆæ‰€åœ¨çš„ä½ç½®ã€‚æˆ‘è¿˜æƒ³å°è¯•ä¸€äº›å…¶ä»–çš„äº‹æƒ…ï¼Œé—®å®ƒæ¡‘å¾·æ‹‰åœ¨å“ªé‡Œï¼Œå¥¹åœ¨è¿™ä¸ªæ•…äº‹ä¸­æ ¹æœ¬æ²¡æœ‰å‡ºç°ã€‚
- en: So it can't say I don't knowã€‚ it's saying that she's in the gardenã€‚ if I had
    to wager and let me actually go ahead and print it's predictionã€‚ So that's the
    probability except I had switched it to a Max So I'm printing out just the probability
    it's confidence almost So it is showing that yeah it's pretty sure it's 90% sure
    that's the highest probability that I see here yeah if I had to venture a guess
    it is saying that Sandra is in the garden because in the train set that's probably
    where she most commonly wasã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒä¸èƒ½è¯´æˆ‘ä¸çŸ¥é“ã€‚å®ƒè¯´å¥¹åœ¨èŠ±å›­é‡Œã€‚å¦‚æœæˆ‘å¿…é¡»æ‰“èµŒï¼Œè®©æˆ‘å®é™…ä¸Šå»æ‰“å°å®ƒçš„é¢„æµ‹ã€‚é‚£ä¹ˆè¿™å°±æ˜¯æ¦‚ç‡ï¼Œé™¤äº†æˆ‘å°†å…¶åˆ‡æ¢åˆ°æœ€å¤§å€¼ï¼Œæ‰€ä»¥æˆ‘åªæ‰“å°å‡ºæ¦‚ç‡ï¼Œå‡ ä¹æ˜¯å®ƒçš„ç½®ä¿¡åº¦ã€‚æ‰€ä»¥å®ƒæ˜¾ç¤ºå‡ºæ˜¯çš„ï¼Œå®ƒç›¸å½“ç¡®å®šï¼Œ90%çš„ç½®ä¿¡åº¦æ˜¯æˆ‘åœ¨è¿™é‡Œçœ‹åˆ°çš„æœ€é«˜æ¦‚ç‡ã€‚å¦‚æœæˆ‘å¿…é¡»çŒœæµ‹ï¼Œå®ƒåœ¨è¯´æ¡‘å¾·æ‹‰åœ¨èŠ±å›­é‡Œï¼Œå› ä¸ºåœ¨è®­ç»ƒé›†ä¸­ï¼Œå¥¹æœ€å¸¸å¾…çš„åœ°æ–¹å¯èƒ½å°±æ˜¯é‚£é‡Œã€‚
- en: I don't know I would have to I would have to verify that that hypothesis but
    this is really a pretty cool technology This isã€‚You could play with this pretty
    easily and train it to learn your own types of sentencesã€‚ You could do things
    like whereas Daniel originally like I had told you thereã€‚ you just need to generate
    the appropriate training data for this and it will literally learn to read those
    sentences that you have the more complicated those sentences become the more difficult
    the bigger the neural network needs needs to be so the neural network back up
    here I didn't really show you a lot of its architecture without having to go into
    into the paper but you would need to increase that size of 32 and then some of
    these dense layers here right now it's just doing a dense layer the vocab size
    so that's what's fed into the softmax but you could create some additional dense
    and regularization layers In fact the cares author is even noting that you would
    probably need to add those So this is showing you how you can basically teach
    the neural network to read completely from scratch it's limited it's got a small
    vocabulary but for four minutes on a 16 gigab computer this is actually pretty
    Thank you for watching the video andã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“ï¼Œæˆ‘éœ€è¦éªŒè¯é‚£ä¸ªå‡è®¾ï¼Œä½†è¿™çœŸçš„æ˜¯ä¸€ä¸ªç›¸å½“é…·çš„æŠ€æœ¯ã€‚ä½ å¯ä»¥è½»æ¾åœ°ç©è¿™ä¸ªï¼Œå¹¶è®­ç»ƒå®ƒå­¦ä¹ ä½ è‡ªå·±çš„å¥å‹ã€‚ä½ å¯ä»¥åšä¸€äº›äº‹æƒ…ï¼Œæ¯”å¦‚ä¸¹å°¼å°”æœ€åˆæåˆ°çš„é‚£æ ·ã€‚ä½ åªéœ€ç”Ÿæˆé€‚å½“çš„è®­ç»ƒæ•°æ®ï¼Œå®ƒå°±ä¼šçœŸçš„å­¦ä¼šé˜…è¯»ä½ æä¾›çš„é‚£äº›å¥å­ã€‚å¥å­è¶Šå¤æ‚ï¼Œç¥ç»ç½‘ç»œéœ€è¦çš„è§„æ¨¡å°±è¶Šå¤§ï¼Œæ‰€ä»¥ç¥ç»ç½‘ç»œçš„æ¶æ„æˆ‘æ²¡æœ‰è¯¦ç»†å±•ç¤ºï¼Œè€Œæ˜¯ç›´æ¥è¯´åˆ°ä½ éœ€è¦å°†å¤§å°ä»32å¢åŠ ï¼Œç„¶ååœ¨è¿™äº›å¯†é›†å±‚ä¸­ï¼Œç°åœ¨åªæ˜¯åœ¨åšä¸€ä¸ªå¯†é›†å±‚ï¼Œè¯æ±‡å¤§å°å°±æ˜¯è¾“å…¥åˆ°softmaxçš„å†…å®¹ï¼Œä½†ä½ å¯ä»¥åˆ›å»ºä¸€äº›é¢å¤–çš„å¯†é›†å±‚å’Œæ­£åˆ™åŒ–å±‚ã€‚äº‹å®ä¸Šï¼Œä½œè€…ç”šè‡³æŒ‡å‡ºä½ å¯èƒ½éœ€è¦æ·»åŠ è¿™äº›ã€‚å› æ­¤ï¼Œè¿™æ˜¾ç¤ºäº†ä½ å¦‚ä½•åŸºæœ¬ä¸Šä»é›¶å¼€å§‹æ•™ç¥ç»ç½‘ç»œé˜…è¯»ï¼Œå®ƒçš„èƒ½åŠ›æœ‰é™ï¼Œè¯æ±‡é‡ä¹Ÿå°ï¼Œä½†åœ¨16GBçš„è®¡ç®—æœºä¸Šè¿è¡Œå››åˆ†é’Ÿï¼Œè¿™å®é™…ä¸Šè¿˜ä¸é”™ã€‚æ„Ÿè°¢è§‚çœ‹è§†é¢‘ã€‚
- en: '![](img/511897e72e03ef43ebc5740c467bd96e_3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511897e72e03ef43ebc5740c467bd96e_3.png)'
- en: Next module we're going to look at reinforcement learning this content changes
    often so subscribe to the channel to stay up to date on this course and other
    topics in artificial intelligenceã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„æ¨¡å—æˆ‘ä»¬å°†æ¢è®¨å¼ºåŒ–å­¦ä¹ ï¼Œè¿™ä¸ªå†…å®¹å˜åŒ–é¢‘ç¹ï¼Œå› æ­¤è¯·è®¢é˜…é¢‘é“ä»¥ä¾¿åŠæ—¶äº†è§£æœ¬è¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚
