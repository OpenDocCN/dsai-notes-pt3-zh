- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- å›å½’é‡æ„
    - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- å›å½’é‡æ„
    - ShowMeAI - BV1wS4y1f7z1
- en: Hi everybodyã€‚ welcomelcome to your new Python tutorialã€‚ And this videoã€‚ I will
    simply refactor the code of the last two videosã€‚ So if you haven't watched the
    previous two videos about linear regression and logistic regressionã€‚ then please
    do soã€‚So now if you compare the codeï¼Œ then you will see that is almost similarã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°ä½ çš„æ–°Pythonæ•™ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘å°†ç®€å•åœ°é‡æ„å‰ä¸¤ä¸ªè§†é¢‘ä¸­çš„ä»£ç ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰è§‚çœ‹å…³äºçº¿æ€§å›å½’å’Œé€»è¾‘å›å½’çš„å‰ä¸¤ä¸ªè§†é¢‘ï¼Œè¯·å…ˆå»çœ‹ã€‚æ‰€ä»¥ç°åœ¨å¦‚æœä½ æ¯”è¾ƒä»£ç ï¼Œä½ ä¼šå‘ç°å®ƒä»¬å‡ ä¹ç›¸ä¼¼ã€‚
- en: So both classes have exactly the same in it methodã€‚And both have almost the
    same fit methodã€‚ So in both classesï¼Œ we in it our parametersï¼Œ and then we do the
    same gradient descentã€‚ except that in the linear regressionï¼Œ we have simply this
    linear model for our approximated y and in logistic regressionã€‚ we also have this
    linear modelã€‚ But then we also apply the sigmoid functionã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸¤ä¸ªç±»çš„`in it`æ–¹æ³•å®Œå…¨ç›¸åŒã€‚ä¸¤ä¸ªç±»çš„`fit`æ–¹æ³•ä¹Ÿå‡ ä¹ç›¸åŒã€‚æ‰€ä»¥åœ¨è¿™ä¸¤ä¸ªç±»ä¸­ï¼Œæˆ‘ä»¬åœ¨`in it`ä¸­ä¼ å…¥å‚æ•°ï¼Œç„¶åæ‰§è¡Œç›¸åŒçš„æ¢¯åº¦ä¸‹é™ã€‚é™¤äº†åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä»…æœ‰è¿™ä¸ªçº¿æ€§æ¨¡å‹æ¥é€¼è¿‘yï¼Œè€Œåœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæœ‰è¿™ä¸ªçº¿æ€§æ¨¡å‹ï¼Œä½†éšåæˆ‘ä»¬è¿˜åº”ç”¨sigmoidå‡½æ•°ã€‚
- en: And the same difference is in the predict methodã€‚ So in linear regressionã€‚ we
    simply apply the linear modelã€‚ and in logistic regressionã€‚ we apply the linear
    model and then the sigmoid functionã€‚ and then say if it's 1 or 0ã€‚And we have this
    help a functionã€‚But a lot of this code is similarã€‚ So let's refactor thisã€‚ And
    yetã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·çš„å·®å¼‚å‡ºç°åœ¨é¢„æµ‹æ–¹æ³•ä¸­ã€‚æ‰€ä»¥åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ç®€å•åœ°åº”ç”¨çº¿æ€§æ¨¡å‹ï¼Œè€Œåœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨çº¿æ€§æ¨¡å‹ï¼Œç„¶åæ˜¯sigmoidå‡½æ•°ï¼Œæ¥ç€åˆ¤æ–­å®ƒæ˜¯1è¿˜æ˜¯0ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°ã€‚ä½†å¾ˆå¤šä»£ç æ˜¯ç›¸ä¼¼çš„ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿›è¡Œé‡æ„ã€‚ç„¶è€Œã€‚
- en: let's create a base class and call it baseã€‚Regression and our two class classes
    will be derived from this base regressionã€‚ So let's sayã€‚Linear regression derived
    from base regressionã€‚And the same for our logistic regressionã€‚And then we can
    cut the init methodã€‚And put it in the base regressionï¼Œ because this is the same
    for bothã€‚Soï¼Œ let's cut this hereã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŸºç±»ï¼Œç§°ä¸º`base.Regression`ï¼Œæˆ‘ä»¬çš„ä¸¤ä¸ªç±»å°†ä»è¿™ä¸ªåŸºå›å½’æ´¾ç”Ÿã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ï¼Œçº¿æ€§å›å½’ä»åŸºå›å½’æ´¾ç”Ÿï¼Œé€»è¾‘å›å½’ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åˆ‡åˆ†`init`æ–¹æ³•ï¼Œå°†å…¶æ”¾å…¥åŸºå›å½’ä¸­ï¼Œå› ä¸ºè¿™å¯¹ä¸¤è€…éƒ½æ˜¯ç›¸åŒçš„ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬åœ¨è¿™é‡Œåˆ‡åˆ†è¿™ä¸ªã€‚
- en: And then we also cut the fit methodã€‚So this is almost the sameã€‚ So we don't
    need this hereã€‚And put it up hereã€‚å—¯ã€‚The only thing now that we have toã€‚That is
    different is the why predictedã€‚ because one time we need simply the linear model
    and one time we need the linear model and the sigmoid functionã€‚So let's call let's
    create a helper function and call it underscore approximationã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä¹Ÿåˆ‡åˆ†`fit`æ–¹æ³•ã€‚è¿™å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œä¸éœ€è¦è¿™ä¸ªï¼Œæ”¾åˆ°ä¸Šé¢å»ã€‚å—¯ï¼Œç°åœ¨å”¯ä¸€éœ€è¦ä¸åŒçš„æ˜¯`why predicted`ï¼Œå› ä¸ºæœ‰æ—¶æˆ‘ä»¬ä»…éœ€è¦çº¿æ€§æ¨¡å‹ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦çº¿æ€§æ¨¡å‹å’Œsigmoidå‡½æ•°ã€‚æ‰€ä»¥è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç§°ä¸º`_approximation`ã€‚
- en: which will get our data Xï¼Œ or samplesï¼Œ and then it also gets the weights and
    the biasã€‚ and in our base classï¼Œ we will raise a not implemented errorã€‚ So this
    have has to be implemented in the derived classesã€‚ So now in the linear regressionã€‚å—¯ã€‚Classã€‚
    let's also create thisã€‚ And here we will implement thisã€‚ and in ourã€‚Linear regressionã€‚Modelã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è·å–æˆ‘ä»¬çš„æ•°æ®Xï¼Œæˆ–æ ·æœ¬ï¼Œç„¶åå®ƒè¿˜ä¼šè·å–æƒé‡å’Œåç½®ã€‚åœ¨æˆ‘ä»¬çš„åŸºç±»ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å‘ä¸€ä¸ªæœªå®ç°çš„é”™è¯¯ã€‚æ‰€ä»¥è¿™å¿…é¡»åœ¨æ´¾ç”Ÿç±»ä¸­å®ç°ã€‚ç°åœ¨åœ¨çº¿æ€§å›å½’ç±»ä¸­ï¼Œè®©æˆ‘ä»¬ä¹Ÿåˆ›å»ºè¿™ä¸ªã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å°†å®ç°å®ƒï¼Œå¹¶åœ¨æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹ä¸­ã€‚
- en: this is simply the linear modelã€‚ so we can return thisã€‚Return the do product
    of x and W plus the biasersã€‚And thenï¼Œ we also have toã€‚å—¯ã€‚OrWe can cutã€‚The fit method
    from our logistic regression method and implement the approximation methodã€‚ So
    hereã€‚ let's copy thisã€‚æ˜¯å“¦ã€‚So all let's remember that we need to have the linear
    model and then apply the sigoidoid functionã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç®€å•å°±æ˜¯çº¿æ€§æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿”å›å®ƒã€‚è¿”å›`x`å’Œ`W`çš„ç‚¹ç§¯åŠ ä¸Šåç½®ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿˜éœ€è¦ï¼Œå—¯ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åˆ‡åˆ†ã€‚å°†é€»è¾‘å›å½’æ–¹æ³•ä¸­çš„`fit`æ–¹æ³•æå–å‡ºæ¥å¹¶å®ç°é€¼è¿‘æ–¹æ³•ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œè®©æˆ‘ä»¬å¤åˆ¶è¿™ä¸ªã€‚æ˜¯å“¦ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬è®°ä½æˆ‘ä»¬éœ€è¦çº¿æ€§æ¨¡å‹ï¼Œç„¶ååº”ç”¨sigmoidå‡½æ•°ã€‚
- en: So let's cut thisã€‚And let's create our underscoreã€‚Uã€‚Exximation method with selfã€‚Xï¼Œ
    Wï¼Œ and the biasã€‚And then we create our linear modelï¼Œ which is nuyã€‚Dotã€‚Xã€‚Wã€‚Plusï¼Œ
    biasã€‚And then we apply the sigmoid function and return itã€‚ So let's return self
    dot underscore sigmoid of our linear modelã€‚So this is the approximationã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬åˆ‡åˆ†è¿™ä¸ªã€‚å¹¶åˆ›å»ºæˆ‘ä»¬çš„`_U`é€¼è¿‘æ–¹æ³•ï¼Œå‚æ•°ä¸º`self`ï¼Œ`X`ï¼Œ`W`å’Œåç½®ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„çº¿æ€§æ¨¡å‹ï¼Œè¡¨ç¤ºä¸º`nuy.Dot.X.W.Plus`åç½®ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨sigmoidå‡½æ•°å¹¶è¿”å›å®ƒã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿”å›`self.dot._sigmoid`çš„çº¿æ€§æ¨¡å‹ã€‚è¿™æ˜¯é€¼è¿‘å€¼ã€‚
- en: And now the predict method is a little bit differentï¼Œ soã€‚Let'sã€‚In our base classï¼Œ
    let'sã€‚Toine this predict methodã€‚å—¯ã€‚And here we will implement aã€‚Help her functionï¼Œ
    and we call itã€‚Undercoreã€‚ predictã€‚Which will get self X and also W and Bã€‚ And
    also in the base class hereã€‚ we will simply raise a not implemented errorã€‚ So
    this now has to be implemented in the derived classesã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œé¢„æµ‹æ–¹æ³•æœ‰ç‚¹ä¸åŒï¼Œæ‰€ä»¥ã€‚è®©æˆ‘ä»¬ã€‚åœ¨æˆ‘ä»¬çš„åŸºç±»ä¸­ï¼Œå®šä¹‰è¿™ä¸ªé¢„æµ‹æ–¹æ³•ã€‚å—¯ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå¹¶ç§°ä¹‹ä¸º`underscore_predict`ï¼Œå®ƒå°†è·å–`self.X`å’Œ`W`ä»¥åŠ`B`ã€‚åœ¨åŸºç±»ä¸­ï¼Œæˆ‘ä»¬å°†ç®€å•åœ°æŠ›å‡ºä¸€ä¸ªæœªå®ç°çš„é”™è¯¯ã€‚æ‰€ä»¥è¿™å¿…é¡»åœ¨æ´¾ç”Ÿç±»ä¸­å®ç°ã€‚
- en: So we call and return this in ourã€‚Predict method in the base classã€‚ So let's
    return self dot underscoreï¼Œ predictã€‚ and with the test samplesã€‚ And then now with
    our calculated weights and the calculated biasã€‚And now in our linear regression
    classï¼Œ we define this underscore predictï¼Œ which will get xã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨åŸºç±»çš„`Predict`æ–¹æ³•ä¸­è°ƒç”¨å¹¶è¿”å›è¿™ä¸ªã€‚è®©æˆ‘ä»¬è¿”å›`self.underscore_predict`ï¼Œå¹¶ä½¿ç”¨æµ‹è¯•æ ·æœ¬ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†è®¡ç®—å‡ºçš„æƒé‡å’Œåå·®ã€‚åœ¨æˆ‘ä»¬çš„çº¿æ€§å›å½’ç±»ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰è¿™ä¸ª`underscore_predict`ï¼Œå®ƒå°†è·å–`x`ã€‚
- en: W and biasã€‚Soï¼Œ and hereï¼Œ it will getã€‚So this will be the same codeã€‚Soã€‚The dot
    productã€‚ dot product of x and w plus the biasã€‚ And we can return this in this
    lineã€‚ So we don't need thisã€‚And then alsoï¼Œ in the logistic regressionï¼Œ weã€‚Deine
    this as underscoreï¼Œ predict with Xï¼Œ W and Bã€‚ And then we have the same code hereã€‚
    So weã€‚Use W and Bï¼Œ then apply the sigoid itã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`W`å’Œåå·®ã€‚æ‰€ä»¥ï¼Œåœ¨è¿™é‡Œï¼Œå®ƒå°†è·å–ã€‚æ‰€ä»¥è¿™å°†æ˜¯ç›¸åŒçš„ä»£ç ã€‚ç‚¹ç§¯ã€‚`x`å’Œ`w`çš„ç‚¹ç§¯åŠ ä¸Šåå·®ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸€è¡Œè¿”å›å®ƒã€‚æ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªã€‚åœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬å°†å…¶å®šä¹‰ä¸º`underscore_predict`ï¼Œå¸¦æœ‰`X`ã€`W`å’Œ`B`ã€‚ç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œæœ‰ç›¸åŒçš„ä»£ç ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨`W`å’Œ`B`ï¼Œç„¶ååº”ç”¨sigmoidå‡½æ•°ã€‚'
- en: Define if it's  one or 0 and return itã€‚And nowï¼Œ we canã€‚Coopied this here and
    put it in the same file hereã€‚![](img/8cc66a6687567bf6ad9be22465110a63_1.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰å®ƒæ˜¯1è¿˜æ˜¯0å¹¶è¿”å›ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªå¤åˆ¶åˆ°è¿™é‡Œï¼Œå¹¶æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚![](img/8cc66a6687567bf6ad9be22465110a63_1.png)
- en: And then we are doneã€‚ So now we have two modelsï¼Œ the linear regression and the
    logistic regressionã€‚ just in 60 lines of Pythonã€‚ And it looks much cleaner nowã€‚
    Soï¼Œ yeahï¼Œ that's itã€‚ I hope you enjoyed this tutorial and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚![](img/8cc66a6687567bf6ad9be22465110a63_3.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®Œæˆäº†ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼Œçº¿æ€§å›å½’å’Œé€»è¾‘å›å½’ï¼Œä»…ç”¨60è¡ŒPythonä»£ç ã€‚çœ‹èµ·æ¥å¹²å‡€å¤šäº†ã€‚é‚£ä¹ˆï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯å…¨éƒ¨ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œä¸‹æ¬¡è§ï¼Œæ‹œã€‚ğŸ˜Šï¼[](img/8cc66a6687567bf6ad9be22465110a63_3.png)
- en: '![](img/8cc66a6687567bf6ad9be22465110a63_4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cc66a6687567bf6ad9be22465110a63_4.png)'
