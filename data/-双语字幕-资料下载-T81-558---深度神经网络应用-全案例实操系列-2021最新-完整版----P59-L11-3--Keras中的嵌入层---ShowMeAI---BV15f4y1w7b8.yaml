- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P59：L11.3- Keras中的嵌入层 -
    ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P59：L11.3- Keras中的嵌入层 -
    ShowMeAI - BV15f4y1w7b8
- en: '![](img/6c6a4e57e3618c11a532abc5118438e7_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c6a4e57e3618c11a532abc5118438e7_0.png)'
- en: Hi， this is Jeff Heaton。 Wel to applications of deep neural networks with Washington
    University。 So what are Kira's embedding layers。 This is another layer type that
    you can use in Kias。 But what do they do， They're used with natural language processing
    for the latest on my AI course and projects。 click subscribe in the bell next
    to it to be notified of every new video。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是杰夫·希顿。欢迎来到华盛顿大学的深度神经网络应用课程。那么，Kira的嵌入层是什么？这是你可以在Keras中使用的另一种层类型。但是它们有什么用呢？它们用于自然语言处理，关于我最新的AI课程和项目，请点击旁边的小铃铛订阅，以便收到每个新视频的通知。
- en: Kis provides something called an embedding layer。 These are very often used
    with natural language processing in Kiras。 However， they don't really have to
    be used just with NLP。 really。 how I think of an embedding layer is almost an
    alternative to one hot encoding with one hot encoding you or dummy variables or
    whatever you want to call that where you take a categorical value So say you have
    100 different possibilities for that categorical value Now you need a way to encode
    that into。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一种叫做嵌入层的东西。这些通常在Keras中用于自然语言处理。然而，它们并不一定只与NLP一起使用。实际上，我认为嵌入层几乎是单热编码的替代品，单热编码就是虚拟变量，或者你想称之为的，处理分类值。所以假设你有100种不同的可能性用于该分类值，现在你需要一种方法来对其进行编码。
- en: say dummy variables。 You're going to have 100 dummy variables。 That gets impractical
    if you deal with extremely large cardinalities for。😊。![](img/6c6a4e57e3618c11a532abc5118438e7_2.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 说到虚拟变量，你将有100个虚拟变量。如果你处理的是极大的基数，这就变得不切实际。😊！[](img/6c6a4e57e3618c11a532abc5118438e7_2.png)
- en: Those categoricals， particularlyicular if you're dealing with words in the English
    language。 Think about how you would dummy and code just English words。 You would
    need one dummy variable for every English word that you had。 Now。 some of the
    options in spacey could be useful for that。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分类，特别是如果你处理的是英语单词。想想你如何对英语单词进行虚拟编码。你需要为每个英语单词准备一个虚拟变量。现在，spacey中的一些选项可能对你有用。
- en: You could turn words into their stem words like having could be transformed
    into have that way you just have one of those to deal with a lot of the verbs
    you could transform like bra。 you could always have a run。 So that way you don't
    have to brought could always be bring or bringing also going to the reword of
    brought。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将词转化为它们的词干，比如“could”可以被转化为“have”，这样你就可以处理许多动词，例如“bra”，你总是可以有一个“run”。这样你就不必处理“brought”，它总是可以是“bring”或“bringing”。
- en: But those are those are some of the things you can do to get that dimension
    down。 But the embedding layer， you can actually learn an embedding layer for your
    words or whatever vocabulary or categorical。 you want to send it towards。 Now，
    this is most often used on sequences。 The type that we would send into an LsTM
    or a temporal convolution neural network， but。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些都是你可以用来降低维度的一些方法。不过，嵌入层实际上可以学习你所需的单词或任何你想传递的词汇或分类。现在，这通常用于序列。我们会将其发送到LSTM或时序卷积神经网络中，但。
- en: That also does not have to be the case。 So let's look at a simple embedding
    layer。 Now。 we're going to see that when we create an embedding layer like here。
    And by the way。 you can see another example of this back in our image captioning
    example。 we made use of embedding and we loaded the glove embedding layer directly
    into it。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不一定是必要的。那么让我们看看一个简单的嵌入层。现在，我们将看到当我们像这里那样创建一个嵌入层时的情况。顺便提一下，你可以在我们的图像字幕示例中看到另一个例子，我们使用了嵌入，并直接加载了GloVe嵌入层。
- en: but we didn't talk a lot about what the embedding layer actually did。 It's all
    exactly like what you're going to see here。 So now we're learning how to actually
    even train an embedding layer。 So here we've defined this embedding layer。 and
    our input dimension count is going to be 10。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们并没有详细讨论嵌入层实际上做了什么。一切都和你在这里看到的一样。所以现在我们要学习如何训练一个嵌入层。在这里，我们定义了这个嵌入层，我们的输入维度计数将是10。
- en: So input dimension。 that's essentially how many categories or how many words。
    what's your vocabulary size。 If you were using one hot encoding。 you would have
    ended up with 10 dummy variables here。 However。 we're going to sort of dimension
    reduce this a little bit， not really a dimension reduction。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 输入维度。基本上是类别的数量或单词的数量。你的词汇量有多大。如果你使用独热编码，你将在这里得到10个虚拟变量。然而，我们将稍微减少这个维度，实际上并不是一个维度减少。
- en: but we're going to encode these into four number of vectors rather than the
    10 number vector that a dummy。Normally have and it's not zeros and ones。 All of
    these four elements are going to be used in that vector。 Now， the input link。
    This is kind of interesting。 This is essentially your sequence link。 If you're
    dealing with natural language processing。 So in this case。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将把这些编码成四个数字的向量，而不是通常的10个数字的虚拟变量。并且它不是零和一。所有这四个元素都将在那个向量中使用。现在，输入链接。这有点有趣。这基本上是你的序列链接。如果你在处理自然语言处理，那么在这种情况下。
- en: we're just going to have two of these because this is a really， really simple
    example。 and you'll notice this neural network。 and I use the term loosely only
    has one embedding layer。 So this neural network is going to essentially just kick
    out the embedding directly to the output layer and you'll see it。 I'm saying atom
    and mean square error， but that really doesn't matter。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只会有两个这样的，因为这是一个非常、非常简单的例子。你会注意到这个神经网络。我用这个术语是比较宽泛的，它只有一个嵌入层。因此，这个神经网络实际上将嵌入直接输出到输出层，你会看到它。我说的是原子和均方误差，但这其实并不重要。
- en: we're not going to train this very simple neural network that we're creating。
    I'll go ahead and run this。 and it doesn't really do anything other than define
    this model。 Now you should really think of the embedding layer as a lookup table。
    So we've got these 10 input dimensions。 And each of those 10 categorical values
    that you're going to pass in。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会训练这个非常简单的神经网络。我要继续运行这个，它实际上并不做任何事情，只是定义这个模型。现在你应该真正把嵌入层看作一个查找表。所以我们有这10个输入维度。而且你将传入的每一个这10个分类值。
- en: each one of those will return a different unique。Set of four numbers from the
    output dimensions。 So this lookup table， you can really think of it as 10 rows
    and four columns。 It's a lookup table。 That is all an embedding layer is is a
    lookup table。 We're going to go ahead and now run this。 We're going to give it
    some input data。 The input data is just going to be a little sequence here of
    one。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将返回输出维度中的一组不同的唯一四个数字。所以这个查找表，你可以把它想象成有10行和四列。这是一个查找表。嵌入层就是一个查找表。我们现在将继续运行这个。我们将给它一些输入数据。输入数据只是这里的一小段序列。
- en: 2，1 and two are both well within that input range， and it is going to change
    these two input categoricals。 these two integers。 The input into these as always
    integer。 So you're transforming your characters or your words。 and this is most
    often used for words Le often for characters。 So that doesn't have to be the case。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2，1 和 2 都在输入范围内，它将改变这两个输入分类。这两个整数。输入总是整数。所以你在转换字符或单词。这通常用于单词，但也常常用于字符。因此这并不一定是这样。
- en: You transform you always provide integers。 because they're basically lookups。
    These are essentially the rows in that weight matrix That is the embedding layer。
    And then we're going to request to predict this。 and we're going to print out
    the shape of the input data and also the prediction that came back。 The lookup
    table。 You might have expected that to be all zero。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你转换的内容总是提供整数。因为它们基本上是查找表。这些实际上是权重矩阵中的行，也就是嵌入层。然后我们将请求进行预测，并打印出输入数据的形状以及返回的预测结果。查找表。你可能会期待它全部为零。
- en: Because we never defined a lookup table。 We never trained this neural network。
    So where are these numbers coming from， They're random initializations， essentially。
    So they're like the random weights that all layers of a neural network have。 This
    doesn't really make a lot of sense until you actually look at the embedding weights。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们从未定义查找表。我们从未训练这个神经网络。那么这些数字从哪里来，它们实际上是随机初始化的。所以它们就像神经网络所有层的随机权重。在你实际查看嵌入权重之前，这并没有太多意义。
- en: So if we look at the embedding weights。 notice there's 10 rows and four columns
    So these are the 10 vocabulary elements And then we we just requested there be
    four of these that four is arbitrary we could have made that six or8 or 102 wouldn't
    really matter。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们查看嵌入权重，注意有10行和四列。这是10个词汇元素。然后我们要求有四个，这个四是任意的，我们可以设为六、八或十，这并不重要。
- en: It's sort of a dimension reduction concept， though though not exactly。 now what
    we're going to do But again， along the lines of why I call it a dimension reduction
    is because instead of having the 10 dummy variables you would have now you have
    these four values。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点像维度减少的概念，尽管不完全是。现在我们要做的，但再次强调我称之为维度减少是因为，代替有10个虚拟变量，你现在有这四个值。
- en: Now let's see what these weights actually mean。 So this first one that one that
    corresponds to this very。 very first column and this whole thing can be。is a column
    or dimension。 vector， notice the 0。4763。 Not it is exactly the same vector as
    this one right here。 the second one。 This is row。 assuming you count with 0 as
    you're starting number，0，1， This is1，2 is the next one negative 2，70，2。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这些权重实际上意味着什么。所以第一个对应于这一列的权重，整个东西可以是一个列或维度向量，注意0.4763。它与这里的这个向量完全相同。第二个。这是行。假设你从0开始计数，0，1，这是1，2是下一个负2，70，2。
- en: 7。 Look at that。 It's just a lookup table。 That's all the embedding layer really
    is。 So the glove embeddings that we used in earlier in this class。 that is essentially
    just a table for a large number of English words， I forget how many。 And I think
    it had vector sizes of 200。 if my memory serves。 Don't quote me on that。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个。这只是一个查找表。这就是嵌入层的全部。因此，我们在本课中使用的手套嵌入，实际上只是一个包含大量英语单词的表，我忘记有多少了。我认为它的向量大小为200，如果我没记错的话。别引用我。
- en: but it had some arbitrary vector length for each of those glove embeddings。
    We just took that matrix and loaded it right into the weights。 We called set weights
    on it。 It's all we did。 And we defined this embedding layer。 Now， when you train
    that neural network。 You want to mark those embedding weight。😊，As nontrainable，
    otherwise。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但每个手套嵌入都有一些任意的向量长度。我们只需将该矩阵直接加载到权重中。我们称其为设置权重。这就是我们所做的。然后我们定义了这个嵌入层。现在，当你训练这个神经网络时，你想要将这些嵌入权重标记为非可训练的，*否则*。
- en: they'll start to get pulled away from the values that they were originally set
    at by whoever trained it。 And if you're doing transfer learning， you probably
    don't want those weights modified。 We'll see more about how to train these in
    a moment。 Now， I compared this to dummy variables。 So usually what you want to
    do to prove that something is equivalent to something else is see if you can emulate
    that thing in something else。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它们将开始从原本的设置值中被拉离。如果你在进行迁移学习，你可能不希望这些权重被修改。我们稍后将看到更多关于如何训练这些权重的信息。现在，我将其与虚拟变量进行比较。通常，你想证明某物等同于其他事物时，可以看看能否在其他事物中模拟该东西。
- en: So we're going to use an embedding layer to basically provide dummy variables
    for us。 So what I am doing here is I am creating an input dimension 3。 So that
    would be a categorical variable that had three possible values。 The dummies for
    this would look like this。 essentially the diagonal that you see with dummy variables
    because dummies just a briefly review essentially one of the values is is one
    or hot。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将使用嵌入层基本上为我们提供虚拟变量。我在这里做的是创建一个输入维度3。这将是一个具有三个可能值的分类变量。虚拟变量看起来是这样的。基本上是你看到的虚拟变量的对角线，因为虚拟变量，简要回顾一下，本质上一个值是1或热编码。
- en: That's why it's called one hot。 And the rest are 0。 And this is a simple way
    that you can encode categorical。Values the output dimension is also going to be
    three because there's three columns in what we're encoding it to。 If you're doing
    dummy variables， these will always be the same。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么它被称为“一热编码”。其他值都是0。这是你可以编码分类值的一种简单方法。输出维度也将是3，因为我们编码的内容有三列。如果你在做虚拟变量，这些值将始终保持不变。
- en: And this is why dummy variables are so inefficient， because say your input dimensions
    was 100。 You had 100 categories。 You could still make this very small。 You want
    to want to make it too small。 but you can make it say 4 or 8 and train for it。
    We'll see how we can do that in just a moment。 Then input length， that's your
    sequence length。 So that's how many of these you want to encode at a time。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是虚拟变量如此低效的原因，因为假设你的输入维度是 100。你有 100 个类别。你仍然可以将其压缩得非常小。你不想太小，但可以将其设置为 4 或 8
    并进行训练。我们将很快看看如何做到这一点。然后输入长度就是你的序列长度。所以这就是你希望一次编码多少个。
- en: Then we're going compile it with atom and MSC again， we're going to never train
    this neural network。 So these two really don't matter。 but we're going to do set
    weights on the embedding layer。 Now we do have to transform this look up up here
    into a list because you can potentially not going to really get into that。 but
    you can you can have multiple lookup matrices for this if it's going sort of in
    multiple directions。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将使用 atom 和 MSC 重新编译它，我们将不会训练这个神经网络。因此，这两个真的无关紧要。但我们会在嵌入层设置权重。现在，我们确实需要将这里的查找转换为列表，因为你可能不会真正深入研究这一点，但如果它朝多个方向发展，你可以有多个查找矩阵。
- en: but that would be a more advanced set。 you can refer to the Kira's documentation
    if you're interested in。Exactly， that is a list。 Let's go ahead and run it。 But
    for now。 just always embed your matrix list and you'll be good to go gonna go
    ahead and run that。 Now we have created essentially our dummy emulator as an embedding
    layer。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但那将是一个更高级的设置。如果你感兴趣，可以参考 Kira 的文档。没错，那是一个列表。让我们继续运行它。但现在，始终嵌入你的矩阵列表，你就可以顺利进行。接下来，我们实际上创建了一个作为嵌入层的虚拟模拟器。
- en: I'm going to go ahead and run it down here， we're going to encode these two
    categoricals and run it and essentially look what it's doing。 There's the dummy
    variables。 so you could put one of these on the front of your neural network and
    not even have to encode your dummy variables。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我将继续在这里简要介绍一下，我们将对这两个类别进行编码，并查看它的运行情况。这里是虚拟变量。这样你可以将其中一个放在神经网络的前面，甚至不需要编码虚拟变量。
- en: there's better ways of going about it。 but this is one way that you you could
    do that。 if you wanted to make your neural network truly so that you could pass
    in these enter your values and have it automatically transform these into dummies。
    This is cool。 you'll do this kind of thing a lot， this is when you want to use
    transfer learning to bring your dummy variables in。 However， the real fun gets
    in。 maybe it's not the real fun。😊，You can train these yourself。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有更好的方法来处理这个，但这是你可以做到的一种方式。如果你想使你的神经网络真正能够输入这些值，并自动将其转换为虚拟变量。这很酷。你会经常做这样的事情，这就是你想要使用迁移学习来引入你的虚拟变量的时候。然而，真正的乐趣在于，也许这并不是最真实的乐趣。😊你可以自己训练这些。
- en: and this is a great way to deal with if your neural network needs to take in
    a high dimensionmen categorical that does not have an easy way that you can transform
    it into dummies。 say you have I don't know a 20000 Carnality categorical。 you
    could literally just define it as a embedding pick some arbitrary number of dimensions
    like I don't know。2040。 It's a hyperparameter you'd have to play with it and literally
    the atom update rule or back propagation。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理神经网络需要输入高维类别的好方法，而这些类别没有简单的方法可以转换为虚拟变量。假设你有一个 20000 维的类别，你可以将其定义为嵌入，选择一些任意的维度数，比如
    2040。这是一个超参数，你需要进行调整，实际上是 atom 更新规则或反向传播。
- en: All of them。 it'll do gradient descent and it will train your embeddings for
    you。 Let's go ahead and see how we can do this。 This uses some of the curs functions
    for natural language processing。 This shows really how easily you can build these
    NLP neural networks now So here are 10 restaurant reviews。 The first ones are
    all bad。 Never coming back。 horriblerrible service。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的变量。它将执行梯度下降，并为你训练嵌入。让我们看看如何做到这一点。这使用了一些自然语言处理的函数。这实际上展示了现在你可以多么轻松地构建这些 NLP
    神经网络。所以这里有 10 条餐厅评论。前几条都是负面的。再也不来了。糟糕的服务。
- en: rude waitress Col food horrible food these other guys。Re liked it。 Awesome，
    awesomewesome service。 Rocks。 Poor work couldn't have done better。 So these are
    all just different。 different values that you can choose for this。 And notice
    I put in random exclamation points。 and then even just a sort of random one that
    was more applying to say， evaluating contractors。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 无礼的女服务员，食物糟糕，这些其他人。Re喜欢它。很棒，超棒的服务。太棒了。糟糕的工作，做得不错。所以这些只是你可以选择的不同值。注意我放入了随机的感叹号。还有一个稍微随机的例子，更多是应用于评估承包商。
- en: but noise is good。 One means negative 0 is positive。 So these are the labels。
    This is the y。 we're going train a neural network on it。 So we're gonna say our
    vocabulary size is 50。 We can just pick that to be whatever the heck we want。
    We don't have to really count the number of words in there。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但噪声是好的。1表示负，0表示正。所以这些是标签。这是y。我们将在其上训练一个神经网络。因此我们将说我们的词汇大小为50。我们可以任意选择这个数字。我们不必真正计算其中的单词数量。
- en: And we're going to use the car is one hot。 The car is one hot is kind of cool。😊。It
    sort of scares me a little bit， but it's good for examples。 It is basically going
    in there and tokenizing for you。 So breaking these words into breaking these sentences
    into words。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用汽车的一热编码。汽车的一热编码很酷。😊。这让我有点害怕，但对于示例来说很好。它基本上是在为你进行标记。因此，将这些词分解，将这些句子分解为单词。
- en: and then assigning each to an index。 So it's doing a lot in the background。Normally。
    I like to have a little more control over that， I want to know how it's being
    tokenized and I would like to know how it is assigning these indexes to it。 But
    this is good for for an example。 when we get into showing how to connect one of
    these to an API that others will use sort of in a production environment。 we'll
    see that we really care about locking down what these index values are。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将每个分配给一个索引。所以它在后台做了很多事情。通常，我喜欢对这些有更多的控制，我想知道它是如何被标记的，并且我想知道它是如何将这些索引分配给它的。但这对于示例来说是好的。当我们展示如何将这些连接到其他人在生产环境中使用的API时，我们会看到我们非常关心锁定这些索引值。
- en: I wouldn't want if'm if I'm deploying this in a real worldor corporate situation。
    I wouldn't want coming to become an index of， say，5 one time retrain an now for。
    and potentially the data coming in is now encoded wrong。So you have to be careful
    with all of that。 Then I am going to go ahead and go ahead and run this part。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在现实世界或企业环境中部署这个，我不希望某次变得索引为5，进行重训练，而现在为4。并且潜在的数据现在编码错误。所以你必须对所有这些小心。然后我将继续运行这部分。
- en: We're going to go ahead and run it and encode these。 These are all of your sequences。
    We would like these to all be of consistent length because that's how sequences
    work。 We're going to do the max length of4。 Look at that。 They're all nice and
    zero padded。 Thank you。 Kiras， We're going to create a very simple sequential
    neural network。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续运行并编码这些。这些是你所有的序列。我们希望这些序列的长度一致，因为这就是序列的工作方式。我们将使用最大长度4。看看吧。它们都很好，并且进行了零填充。谢谢你。Kiras，我们将创建一个非常简单的顺序神经网络。
- en: We are going to have one dense layer at the end。 So there is learning going
    on in here。 There is one weight matrix。 But then we're also learning in the embedding
    layer。 The neural network is going to actually learn how to create these embeddings
    so that they are a way that separate those words and map them into Euclidean space
    in a meaningful way。 This saves you having to deal with tons and tons of dummy
    variables and creating a very complex neural network。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在最后有一个稠密层。因此这里面有学习。这里有一个权重矩阵。但我们也在嵌入层中进行学习。神经网络实际上将学习如何创建这些嵌入，以便将这些词以有意义的方式分离并映射到欧几里得空间。这可以让你避免处理大量的虚拟变量，并创建一个非常复杂的神经网络。
- en: embeddings are great for NLP。 go ahead and run it We'll print a summary。 There
    you see it。 go ahead and fit it。 I'm just running at。😊，100 epochs， very， very
    fast to train。 Let's look at essentially the embeddings。 Each line is an embedding
    for a different word。 I'm not even going to try to explain the rhyme or reason
    for this。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入对于NLP来说很棒。继续运行，我们将打印一个摘要。你看到了吗？继续拟合它。我只是运行它。😊，100个周期，非常非常快地训练。让我们看一下本质上的嵌入。每一行都是一个不同单词的嵌入。我甚至不会尝试解释其中的原因。
- en: There' essentially like weights that were calculated in the same way that the
    weights were for the actual layers。 and there is one layer on here that's learning
    as well。 but the embeddings learn right or long with the other weights in the
    neural network。 and then we can evaluate this neural network at the end。 accuracy
    is perfect。 actually。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里基本上就像是权重，这些权重的计算方式与实际层的权重是相同的。这里有一层也在学习。但是嵌入权重与神经网络中的其他权重一起学习。然后我们可以在最后评估这个神经网络。准确性实际上是完美的。
- en: The reason accuracy is perfect is I really didn't put any overlap here。 All
    the words for negative reviews werere not in the positive reviews。 This is just
    a toy example。 Thank you for watching this video and the next video we're going
    to look at end to end natural language processing This content changes often。
    So subscribe to the channel to stay up to date on this course and other topics
    in artificial intelligence。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性完美的原因是我在这里没有放置任何重叠。负面评论中的所有单词都不在正面评论中。这只是一个玩具示例。感谢观看这个视频，在下一个视频中我们将研究端到端的自然语言处理。这个内容经常变化。所以请订阅频道以便及时了解该课程及其他人工智能主题。
- en: '![](img/6c6a4e57e3618c11a532abc5118438e7_4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c6a4e57e3618c11a532abc5118438e7_4.png)'
