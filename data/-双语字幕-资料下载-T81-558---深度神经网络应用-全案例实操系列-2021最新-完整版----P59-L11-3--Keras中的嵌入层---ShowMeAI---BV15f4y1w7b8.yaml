- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P59ï¼šL11.3- Kerasä¸­çš„åµŒå…¥å±‚ -
    ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P59ï¼šL11.3- Kerasä¸­çš„åµŒå…¥å±‚ -
    ShowMeAI - BV15f4y1w7b8
- en: '![](img/6c6a4e57e3618c11a532abc5118438e7_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c6a4e57e3618c11a532abc5118438e7_0.png)'
- en: Hiï¼Œ this is Jeff Heatonã€‚ Wel to applications of deep neural networks with Washington
    Universityã€‚ So what are Kira's embedding layersã€‚ This is another layer type that
    you can use in Kiasã€‚ But what do they doï¼Œ They're used with natural language processing
    for the latest on my AI course and projectsã€‚ click subscribe in the bell next
    to it to be notified of every new videoã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚é‚£ä¹ˆï¼ŒKiraçš„åµŒå…¥å±‚æ˜¯ä»€ä¹ˆï¼Ÿè¿™æ˜¯ä½ å¯ä»¥åœ¨Kerasä¸­ä½¿ç”¨çš„å¦ä¸€ç§å±‚ç±»å‹ã€‚ä½†æ˜¯å®ƒä»¬æœ‰ä»€ä¹ˆç”¨å‘¢ï¼Ÿå®ƒä»¬ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå…³äºæˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®ï¼Œè¯·ç‚¹å‡»æ—è¾¹çš„å°é“ƒé“›è®¢é˜…ï¼Œä»¥ä¾¿æ”¶åˆ°æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚
- en: Kis provides something called an embedding layerã€‚ These are very often used
    with natural language processing in Kirasã€‚ Howeverï¼Œ they don't really have to
    be used just with NLPã€‚ reallyã€‚ how I think of an embedding layer is almost an
    alternative to one hot encoding with one hot encoding you or dummy variables or
    whatever you want to call that where you take a categorical value So say you have
    100 different possibilities for that categorical value Now you need a way to encode
    that intoã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Kerasæä¾›äº†ä¸€ç§å«åšåµŒå…¥å±‚çš„ä¸œè¥¿ã€‚è¿™äº›é€šå¸¸åœ¨Kerasä¸­ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¹¶ä¸ä¸€å®šåªä¸NLPä¸€èµ·ä½¿ç”¨ã€‚å®é™…ä¸Šï¼Œæˆ‘è®¤ä¸ºåµŒå…¥å±‚å‡ ä¹æ˜¯å•çƒ­ç¼–ç çš„æ›¿ä»£å“ï¼Œå•çƒ­ç¼–ç å°±æ˜¯è™šæ‹Ÿå˜é‡ï¼Œæˆ–è€…ä½ æƒ³ç§°ä¹‹ä¸ºçš„ï¼Œå¤„ç†åˆ†ç±»å€¼ã€‚æ‰€ä»¥å‡è®¾ä½ æœ‰100ç§ä¸åŒçš„å¯èƒ½æ€§ç”¨äºè¯¥åˆ†ç±»å€¼ï¼Œç°åœ¨ä½ éœ€è¦ä¸€ç§æ–¹æ³•æ¥å¯¹å…¶è¿›è¡Œç¼–ç ã€‚
- en: say dummy variablesã€‚ You're going to have 100 dummy variablesã€‚ That gets impractical
    if you deal with extremely large cardinalities forã€‚ğŸ˜Šã€‚![](img/6c6a4e57e3618c11a532abc5118438e7_2.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´åˆ°è™šæ‹Ÿå˜é‡ï¼Œä½ å°†æœ‰100ä¸ªè™šæ‹Ÿå˜é‡ã€‚å¦‚æœä½ å¤„ç†çš„æ˜¯æå¤§çš„åŸºæ•°ï¼Œè¿™å°±å˜å¾—ä¸åˆ‡å®é™…ã€‚ğŸ˜Šï¼[](img/6c6a4e57e3618c11a532abc5118438e7_2.png)
- en: Those categoricalsï¼Œ particularlyicular if you're dealing with words in the English
    languageã€‚ Think about how you would dummy and code just English wordsã€‚ You would
    need one dummy variable for every English word that you hadã€‚ Nowã€‚ some of the
    options in spacey could be useful for thatã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åˆ†ç±»ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ å¤„ç†çš„æ˜¯è‹±è¯­å•è¯ã€‚æƒ³æƒ³ä½ å¦‚ä½•å¯¹è‹±è¯­å•è¯è¿›è¡Œè™šæ‹Ÿç¼–ç ã€‚ä½ éœ€è¦ä¸ºæ¯ä¸ªè‹±è¯­å•è¯å‡†å¤‡ä¸€ä¸ªè™šæ‹Ÿå˜é‡ã€‚ç°åœ¨ï¼Œspaceyä¸­çš„ä¸€äº›é€‰é¡¹å¯èƒ½å¯¹ä½ æœ‰ç”¨ã€‚
- en: You could turn words into their stem words like having could be transformed
    into have that way you just have one of those to deal with a lot of the verbs
    you could transform like braã€‚ you could always have a runã€‚ So that way you don't
    have to brought could always be bring or bringing also going to the reword of
    broughtã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†è¯è½¬åŒ–ä¸ºå®ƒä»¬çš„è¯å¹²ï¼Œæ¯”å¦‚â€œcouldâ€å¯ä»¥è¢«è½¬åŒ–ä¸ºâ€œhaveâ€ï¼Œè¿™æ ·ä½ å°±å¯ä»¥å¤„ç†è®¸å¤šåŠ¨è¯ï¼Œä¾‹å¦‚â€œbraâ€ï¼Œä½ æ€»æ˜¯å¯ä»¥æœ‰ä¸€ä¸ªâ€œrunâ€ã€‚è¿™æ ·ä½ å°±ä¸å¿…å¤„ç†â€œbroughtâ€ï¼Œå®ƒæ€»æ˜¯å¯ä»¥æ˜¯â€œbringâ€æˆ–â€œbringingâ€ã€‚
- en: But those are those are some of the things you can do to get that dimension
    downã€‚ But the embedding layerï¼Œ you can actually learn an embedding layer for your
    words or whatever vocabulary or categoricalã€‚ you want to send it towardsã€‚ Nowï¼Œ
    this is most often used on sequencesã€‚ The type that we would send into an LsTM
    or a temporal convolution neural networkï¼Œ butã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™äº›éƒ½æ˜¯ä½ å¯ä»¥ç”¨æ¥é™ä½ç»´åº¦çš„ä¸€äº›æ–¹æ³•ã€‚ä¸è¿‡ï¼ŒåµŒå…¥å±‚å®é™…ä¸Šå¯ä»¥å­¦ä¹ ä½ æ‰€éœ€çš„å•è¯æˆ–ä»»ä½•ä½ æƒ³ä¼ é€’çš„è¯æ±‡æˆ–åˆ†ç±»ã€‚ç°åœ¨ï¼Œè¿™é€šå¸¸ç”¨äºåºåˆ—ã€‚æˆ‘ä»¬ä¼šå°†å…¶å‘é€åˆ°LSTMæˆ–æ—¶åºå·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œä½†ã€‚
- en: That also does not have to be the caseã€‚ So let's look at a simple embedding
    layerã€‚ Nowã€‚ we're going to see that when we create an embedding layer like hereã€‚
    And by the wayã€‚ you can see another example of this back in our image captioning
    exampleã€‚ we made use of embedding and we loaded the glove embedding layer directly
    into itã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸ä¸€å®šæ˜¯å¿…è¦çš„ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç®€å•çš„åµŒå…¥å±‚ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å½“æˆ‘ä»¬åƒè¿™é‡Œé‚£æ ·åˆ›å»ºä¸€ä¸ªåµŒå…¥å±‚æ—¶çš„æƒ…å†µã€‚é¡ºä¾¿æä¸€ä¸‹ï¼Œä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„å›¾åƒå­—å¹•ç¤ºä¾‹ä¸­çœ‹åˆ°å¦ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åµŒå…¥ï¼Œå¹¶ç›´æ¥åŠ è½½äº†GloVeåµŒå…¥å±‚ã€‚
- en: but we didn't talk a lot about what the embedding layer actually didã€‚ It's all
    exactly like what you're going to see hereã€‚ So now we're learning how to actually
    even train an embedding layerã€‚ So here we've defined this embedding layerã€‚ and
    our input dimension count is going to be 10ã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å¹¶æ²¡æœ‰è¯¦ç»†è®¨è®ºåµŒå…¥å±‚å®é™…ä¸Šåšäº†ä»€ä¹ˆã€‚ä¸€åˆ‡éƒ½å’Œä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ä¸€æ ·ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬è¦å­¦ä¹ å¦‚ä½•è®­ç»ƒä¸€ä¸ªåµŒå…¥å±‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ªåµŒå…¥å±‚ï¼Œæˆ‘ä»¬çš„è¾“å…¥ç»´åº¦è®¡æ•°å°†æ˜¯10ã€‚
- en: So input dimensionã€‚ that's essentially how many categories or how many wordsã€‚
    what's your vocabulary sizeã€‚ If you were using one hot encodingã€‚ you would have
    ended up with 10 dummy variables hereã€‚ Howeverã€‚ we're going to sort of dimension
    reduce this a little bitï¼Œ not really a dimension reductionã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥ç»´åº¦ã€‚åŸºæœ¬ä¸Šæ˜¯ç±»åˆ«çš„æ•°é‡æˆ–å•è¯çš„æ•°é‡ã€‚ä½ çš„è¯æ±‡é‡æœ‰å¤šå¤§ã€‚å¦‚æœä½ ä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼Œä½ å°†åœ¨è¿™é‡Œå¾—åˆ°10ä¸ªè™šæ‹Ÿå˜é‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†ç¨å¾®å‡å°‘è¿™ä¸ªç»´åº¦ï¼Œå®é™…ä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªç»´åº¦å‡å°‘ã€‚
- en: but we're going to encode these into four number of vectors rather than the
    10 number vector that a dummyã€‚Normally have and it's not zeros and onesã€‚ All of
    these four elements are going to be used in that vectorã€‚ Nowï¼Œ the input linkã€‚
    This is kind of interestingã€‚ This is essentially your sequence linkã€‚ If you're
    dealing with natural language processingã€‚ So in this caseã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å°†æŠŠè¿™äº›ç¼–ç æˆå››ä¸ªæ•°å­—çš„å‘é‡ï¼Œè€Œä¸æ˜¯é€šå¸¸çš„10ä¸ªæ•°å­—çš„è™šæ‹Ÿå˜é‡ã€‚å¹¶ä¸”å®ƒä¸æ˜¯é›¶å’Œä¸€ã€‚æ‰€æœ‰è¿™å››ä¸ªå…ƒç´ éƒ½å°†åœ¨é‚£ä¸ªå‘é‡ä¸­ä½¿ç”¨ã€‚ç°åœ¨ï¼Œè¾“å…¥é“¾æ¥ã€‚è¿™æœ‰ç‚¹æœ‰è¶£ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä½ çš„åºåˆ—é“¾æ¥ã€‚å¦‚æœä½ åœ¨å¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œé‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ã€‚
- en: we're just going to have two of these because this is a reallyï¼Œ really simple
    exampleã€‚ and you'll notice this neural networkã€‚ and I use the term loosely only
    has one embedding layerã€‚ So this neural network is going to essentially just kick
    out the embedding directly to the output layer and you'll see itã€‚ I'm saying atom
    and mean square errorï¼Œ but that really doesn't matterã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªä¼šæœ‰ä¸¤ä¸ªè¿™æ ·çš„ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸ã€éå¸¸ç®€å•çš„ä¾‹å­ã€‚ä½ ä¼šæ³¨æ„åˆ°è¿™ä¸ªç¥ç»ç½‘ç»œã€‚æˆ‘ç”¨è¿™ä¸ªæœ¯è¯­æ˜¯æ¯”è¾ƒå®½æ³›çš„ï¼Œå®ƒåªæœ‰ä¸€ä¸ªåµŒå…¥å±‚ã€‚å› æ­¤ï¼Œè¿™ä¸ªç¥ç»ç½‘ç»œå®é™…ä¸Šå°†åµŒå…¥ç›´æ¥è¾“å‡ºåˆ°è¾“å‡ºå±‚ï¼Œä½ ä¼šçœ‹åˆ°å®ƒã€‚æˆ‘è¯´çš„æ˜¯åŸå­å’Œå‡æ–¹è¯¯å·®ï¼Œä½†è¿™å…¶å®å¹¶ä¸é‡è¦ã€‚
- en: we're not going to train this very simple neural network that we're creatingã€‚
    I'll go ahead and run thisã€‚ and it doesn't really do anything other than define
    this modelã€‚ Now you should really think of the embedding layer as a lookup tableã€‚
    So we've got these 10 input dimensionsã€‚ And each of those 10 categorical values
    that you're going to pass inã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè®­ç»ƒè¿™ä¸ªéå¸¸ç®€å•çš„ç¥ç»ç½‘ç»œã€‚æˆ‘è¦ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œå®ƒå®é™…ä¸Šå¹¶ä¸åšä»»ä½•äº‹æƒ…ï¼Œåªæ˜¯å®šä¹‰è¿™ä¸ªæ¨¡å‹ã€‚ç°åœ¨ä½ åº”è¯¥çœŸæ­£æŠŠåµŒå…¥å±‚çœ‹ä½œä¸€ä¸ªæŸ¥æ‰¾è¡¨ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰è¿™10ä¸ªè¾“å…¥ç»´åº¦ã€‚è€Œä¸”ä½ å°†ä¼ å…¥çš„æ¯ä¸€ä¸ªè¿™10ä¸ªåˆ†ç±»å€¼ã€‚
- en: each one of those will return a different uniqueã€‚Set of four numbers from the
    output dimensionsã€‚ So this lookup tableï¼Œ you can really think of it as 10 rows
    and four columnsã€‚ It's a lookup tableã€‚ That is all an embedding layer is is a
    lookup tableã€‚ We're going to go ahead and now run thisã€‚ We're going to give it
    some input dataã€‚ The input data is just going to be a little sequence here of
    oneã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å°†è¿”å›è¾“å‡ºç»´åº¦ä¸­çš„ä¸€ç»„ä¸åŒçš„å”¯ä¸€å››ä¸ªæ•°å­—ã€‚æ‰€ä»¥è¿™ä¸ªæŸ¥æ‰¾è¡¨ï¼Œä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆæœ‰10è¡Œå’Œå››åˆ—ã€‚è¿™æ˜¯ä¸€ä¸ªæŸ¥æ‰¾è¡¨ã€‚åµŒå…¥å±‚å°±æ˜¯ä¸€ä¸ªæŸ¥æ‰¾è¡¨ã€‚æˆ‘ä»¬ç°åœ¨å°†ç»§ç»­è¿è¡Œè¿™ä¸ªã€‚æˆ‘ä»¬å°†ç»™å®ƒä¸€äº›è¾“å…¥æ•°æ®ã€‚è¾“å…¥æ•°æ®åªæ˜¯è¿™é‡Œçš„ä¸€å°æ®µåºåˆ—ã€‚
- en: 2ï¼Œ1 and two are both well within that input rangeï¼Œ and it is going to change
    these two input categoricalsã€‚ these two integersã€‚ The input into these as always
    integerã€‚ So you're transforming your characters or your wordsã€‚ and this is most
    often used for words Le often for charactersã€‚ So that doesn't have to be the caseã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¼Œ1 å’Œ 2 éƒ½åœ¨è¾“å…¥èŒƒå›´å†…ï¼Œå®ƒå°†æ”¹å˜è¿™ä¸¤ä¸ªè¾“å…¥åˆ†ç±»ã€‚è¿™ä¸¤ä¸ªæ•´æ•°ã€‚è¾“å…¥æ€»æ˜¯æ•´æ•°ã€‚æ‰€ä»¥ä½ åœ¨è½¬æ¢å­—ç¬¦æˆ–å•è¯ã€‚è¿™é€šå¸¸ç”¨äºå•è¯ï¼Œä½†ä¹Ÿå¸¸å¸¸ç”¨äºå­—ç¬¦ã€‚å› æ­¤è¿™å¹¶ä¸ä¸€å®šæ˜¯è¿™æ ·ã€‚
- en: You transform you always provide integersã€‚ because they're basically lookupsã€‚
    These are essentially the rows in that weight matrix That is the embedding layerã€‚
    And then we're going to request to predict thisã€‚ and we're going to print out
    the shape of the input data and also the prediction that came backã€‚ The lookup
    tableã€‚ You might have expected that to be all zeroã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è½¬æ¢çš„å†…å®¹æ€»æ˜¯æä¾›æ•´æ•°ã€‚å› ä¸ºå®ƒä»¬åŸºæœ¬ä¸Šæ˜¯æŸ¥æ‰¾è¡¨ã€‚è¿™äº›å®é™…ä¸Šæ˜¯æƒé‡çŸ©é˜µä¸­çš„è¡Œï¼Œä¹Ÿå°±æ˜¯åµŒå…¥å±‚ã€‚ç„¶åæˆ‘ä»¬å°†è¯·æ±‚è¿›è¡Œé¢„æµ‹ï¼Œå¹¶æ‰“å°å‡ºè¾“å…¥æ•°æ®çš„å½¢çŠ¶ä»¥åŠè¿”å›çš„é¢„æµ‹ç»“æœã€‚æŸ¥æ‰¾è¡¨ã€‚ä½ å¯èƒ½ä¼šæœŸå¾…å®ƒå…¨éƒ¨ä¸ºé›¶ã€‚
- en: Because we never defined a lookup tableã€‚ We never trained this neural networkã€‚
    So where are these numbers coming fromï¼Œ They're random initializationsï¼Œ essentiallyã€‚
    So they're like the random weights that all layers of a neural network haveã€‚ This
    doesn't really make a lot of sense until you actually look at the embedding weightsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬ä»æœªå®šä¹‰æŸ¥æ‰¾è¡¨ã€‚æˆ‘ä»¬ä»æœªè®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œã€‚é‚£ä¹ˆè¿™äº›æ•°å­—ä»å“ªé‡Œæ¥ï¼Œå®ƒä»¬å®é™…ä¸Šæ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚æ‰€ä»¥å®ƒä»¬å°±åƒç¥ç»ç½‘ç»œæ‰€æœ‰å±‚çš„éšæœºæƒé‡ã€‚åœ¨ä½ å®é™…æŸ¥çœ‹åµŒå…¥æƒé‡ä¹‹å‰ï¼Œè¿™å¹¶æ²¡æœ‰å¤ªå¤šæ„ä¹‰ã€‚
- en: So if we look at the embedding weightsã€‚ notice there's 10 rows and four columns
    So these are the 10 vocabulary elements And then we we just requested there be
    four of these that four is arbitrary we could have made that six or8 or 102 wouldn't
    really matterã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŸ¥çœ‹åµŒå…¥æƒé‡ï¼Œæ³¨æ„æœ‰10è¡Œå’Œå››åˆ—ã€‚è¿™æ˜¯10ä¸ªè¯æ±‡å…ƒç´ ã€‚ç„¶åæˆ‘ä»¬è¦æ±‚æœ‰å››ä¸ªï¼Œè¿™ä¸ªå››æ˜¯ä»»æ„çš„ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ä¸ºå…­ã€å…«æˆ–åï¼Œè¿™å¹¶ä¸é‡è¦ã€‚
- en: It's sort of a dimension reduction conceptï¼Œ though though not exactlyã€‚ now what
    we're going to do But againï¼Œ along the lines of why I call it a dimension reduction
    is because instead of having the 10 dummy variables you would have now you have
    these four valuesã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰ç‚¹åƒç»´åº¦å‡å°‘çš„æ¦‚å¿µï¼Œå°½ç®¡ä¸å®Œå…¨æ˜¯ã€‚ç°åœ¨æˆ‘ä»¬è¦åšçš„ï¼Œä½†å†æ¬¡å¼ºè°ƒæˆ‘ç§°ä¹‹ä¸ºç»´åº¦å‡å°‘æ˜¯å› ä¸ºï¼Œä»£æ›¿æœ‰10ä¸ªè™šæ‹Ÿå˜é‡ï¼Œä½ ç°åœ¨æœ‰è¿™å››ä¸ªå€¼ã€‚
- en: Now let's see what these weights actually meanã€‚ So this first one that one that
    corresponds to this veryã€‚ very first column and this whole thing can beã€‚is a column
    or dimensionã€‚ vectorï¼Œ notice the 0ã€‚4763ã€‚ Not it is exactly the same vector as
    this one right hereã€‚ the second oneã€‚ This is rowã€‚ assuming you count with 0 as
    you're starting numberï¼Œ0ï¼Œ1ï¼Œ This is1ï¼Œ2 is the next one negative 2ï¼Œ70ï¼Œ2ã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›æƒé‡å®é™…ä¸Šæ„å‘³ç€ä»€ä¹ˆã€‚æ‰€ä»¥ç¬¬ä¸€ä¸ªå¯¹åº”äºè¿™ä¸€åˆ—çš„æƒé‡ï¼Œæ•´ä¸ªä¸œè¥¿å¯ä»¥æ˜¯ä¸€ä¸ªåˆ—æˆ–ç»´åº¦å‘é‡ï¼Œæ³¨æ„0.4763ã€‚å®ƒä¸è¿™é‡Œçš„è¿™ä¸ªå‘é‡å®Œå…¨ç›¸åŒã€‚ç¬¬äºŒä¸ªã€‚è¿™æ˜¯è¡Œã€‚å‡è®¾ä½ ä»0å¼€å§‹è®¡æ•°ï¼Œ0ï¼Œ1ï¼Œè¿™æ˜¯1ï¼Œ2æ˜¯ä¸‹ä¸€ä¸ªè´Ÿ2ï¼Œ70ï¼Œ2ã€‚
- en: 7ã€‚ Look at thatã€‚ It's just a lookup tableã€‚ That's all the embedding layer really
    isã€‚ So the glove embeddings that we used in earlier in this classã€‚ that is essentially
    just a table for a large number of English wordsï¼Œ I forget how manyã€‚ And I think
    it had vector sizes of 200ã€‚ if my memory servesã€‚ Don't quote me on thatã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹è¿™ä¸ªã€‚è¿™åªæ˜¯ä¸€ä¸ªæŸ¥æ‰¾è¡¨ã€‚è¿™å°±æ˜¯åµŒå…¥å±‚çš„å…¨éƒ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨æœ¬è¯¾ä¸­ä½¿ç”¨çš„æ‰‹å¥—åµŒå…¥ï¼Œå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªåŒ…å«å¤§é‡è‹±è¯­å•è¯çš„è¡¨ï¼Œæˆ‘å¿˜è®°æœ‰å¤šå°‘äº†ã€‚æˆ‘è®¤ä¸ºå®ƒçš„å‘é‡å¤§å°ä¸º200ï¼Œå¦‚æœæˆ‘æ²¡è®°é”™çš„è¯ã€‚åˆ«å¼•ç”¨æˆ‘ã€‚
- en: but it had some arbitrary vector length for each of those glove embeddingsã€‚
    We just took that matrix and loaded it right into the weightsã€‚ We called set weights
    on itã€‚ It's all we didã€‚ And we defined this embedding layerã€‚ Nowï¼Œ when you train
    that neural networkã€‚ You want to mark those embedding weightã€‚ğŸ˜Šï¼ŒAs nontrainableï¼Œ
    otherwiseã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ¯ä¸ªæ‰‹å¥—åµŒå…¥éƒ½æœ‰ä¸€äº›ä»»æ„çš„å‘é‡é•¿åº¦ã€‚æˆ‘ä»¬åªéœ€å°†è¯¥çŸ©é˜µç›´æ¥åŠ è½½åˆ°æƒé‡ä¸­ã€‚æˆ‘ä»¬ç§°å…¶ä¸ºè®¾ç½®æƒé‡ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ‰€åšçš„ã€‚ç„¶åæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ªåµŒå…¥å±‚ã€‚ç°åœ¨ï¼Œå½“ä½ è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œæ—¶ï¼Œä½ æƒ³è¦å°†è¿™äº›åµŒå…¥æƒé‡æ ‡è®°ä¸ºéå¯è®­ç»ƒçš„ï¼Œ*å¦åˆ™*ã€‚
- en: they'll start to get pulled away from the values that they were originally set
    at by whoever trained itã€‚ And if you're doing transfer learningï¼Œ you probably
    don't want those weights modifiedã€‚ We'll see more about how to train these in
    a momentã€‚ Nowï¼Œ I compared this to dummy variablesã€‚ So usually what you want to
    do to prove that something is equivalent to something else is see if you can emulate
    that thing in something elseã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å°†å¼€å§‹ä»åŸæœ¬çš„è®¾ç½®å€¼ä¸­è¢«æ‹‰ç¦»ã€‚å¦‚æœä½ åœ¨è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œä½ å¯èƒ½ä¸å¸Œæœ›è¿™äº›æƒé‡è¢«ä¿®æ”¹ã€‚æˆ‘ä»¬ç¨åå°†çœ‹åˆ°æ›´å¤šå…³äºå¦‚ä½•è®­ç»ƒè¿™äº›æƒé‡çš„ä¿¡æ¯ã€‚ç°åœ¨ï¼Œæˆ‘å°†å…¶ä¸è™šæ‹Ÿå˜é‡è¿›è¡Œæ¯”è¾ƒã€‚é€šå¸¸ï¼Œä½ æƒ³è¯æ˜æŸç‰©ç­‰åŒäºå…¶ä»–äº‹ç‰©æ—¶ï¼Œå¯ä»¥çœ‹çœ‹èƒ½å¦åœ¨å…¶ä»–äº‹ç‰©ä¸­æ¨¡æ‹Ÿè¯¥ä¸œè¥¿ã€‚
- en: So we're going to use an embedding layer to basically provide dummy variables
    for usã€‚ So what I am doing here is I am creating an input dimension 3ã€‚ So that
    would be a categorical variable that had three possible valuesã€‚ The dummies for
    this would look like thisã€‚ essentially the diagonal that you see with dummy variables
    because dummies just a briefly review essentially one of the values is is one
    or hotã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨åµŒå…¥å±‚åŸºæœ¬ä¸Šä¸ºæˆ‘ä»¬æä¾›è™šæ‹Ÿå˜é‡ã€‚æˆ‘åœ¨è¿™é‡Œåšçš„æ˜¯åˆ›å»ºä¸€ä¸ªè¾“å…¥ç»´åº¦3ã€‚è¿™å°†æ˜¯ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªå¯èƒ½å€¼çš„åˆ†ç±»å˜é‡ã€‚è™šæ‹Ÿå˜é‡çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚åŸºæœ¬ä¸Šæ˜¯ä½ çœ‹åˆ°çš„è™šæ‹Ÿå˜é‡çš„å¯¹è§’çº¿ï¼Œå› ä¸ºè™šæ‹Ÿå˜é‡ï¼Œç®€è¦å›é¡¾ä¸€ä¸‹ï¼Œæœ¬è´¨ä¸Šä¸€ä¸ªå€¼æ˜¯1æˆ–çƒ­ç¼–ç ã€‚
- en: That's why it's called one hotã€‚ And the rest are 0ã€‚ And this is a simple way
    that you can encode categoricalã€‚Values the output dimension is also going to be
    three because there's three columns in what we're encoding it toã€‚ If you're doing
    dummy variablesï¼Œ these will always be the sameã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒè¢«ç§°ä¸ºâ€œä¸€çƒ­ç¼–ç â€ã€‚å…¶ä»–å€¼éƒ½æ˜¯0ã€‚è¿™æ˜¯ä½ å¯ä»¥ç¼–ç åˆ†ç±»å€¼çš„ä¸€ç§ç®€å•æ–¹æ³•ã€‚è¾“å‡ºç»´åº¦ä¹Ÿå°†æ˜¯3ï¼Œå› ä¸ºæˆ‘ä»¬ç¼–ç çš„å†…å®¹æœ‰ä¸‰åˆ—ã€‚å¦‚æœä½ åœ¨åšè™šæ‹Ÿå˜é‡ï¼Œè¿™äº›å€¼å°†å§‹ç»ˆä¿æŒä¸å˜ã€‚
- en: And this is why dummy variables are so inefficientï¼Œ because say your input dimensions
    was 100ã€‚ You had 100 categoriesã€‚ You could still make this very smallã€‚ You want
    to want to make it too smallã€‚ but you can make it say 4 or 8 and train for itã€‚
    We'll see how we can do that in just a momentã€‚ Then input lengthï¼Œ that's your
    sequence lengthã€‚ So that's how many of these you want to encode at a timeã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯è™šæ‹Ÿå˜é‡å¦‚æ­¤ä½æ•ˆçš„åŸå› ï¼Œå› ä¸ºå‡è®¾ä½ çš„è¾“å…¥ç»´åº¦æ˜¯ 100ã€‚ä½ æœ‰ 100 ä¸ªç±»åˆ«ã€‚ä½ ä»ç„¶å¯ä»¥å°†å…¶å‹ç¼©å¾—éå¸¸å°ã€‚ä½ ä¸æƒ³å¤ªå°ï¼Œä½†å¯ä»¥å°†å…¶è®¾ç½®ä¸º 4 æˆ– 8
    å¹¶è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†å¾ˆå¿«çœ‹çœ‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚ç„¶åè¾“å…¥é•¿åº¦å°±æ˜¯ä½ çš„åºåˆ—é•¿åº¦ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä½ å¸Œæœ›ä¸€æ¬¡ç¼–ç å¤šå°‘ä¸ªã€‚
- en: Then we're going compile it with atom and MSC againï¼Œ we're going to never train
    this neural networkã€‚ So these two really don't matterã€‚ but we're going to do set
    weights on the embedding layerã€‚ Now we do have to transform this look up up here
    into a list because you can potentially not going to really get into thatã€‚ but
    you can you can have multiple lookup matrices for this if it's going sort of in
    multiple directionsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ atom å’Œ MSC é‡æ–°ç¼–è¯‘å®ƒï¼Œæˆ‘ä»¬å°†ä¸ä¼šè®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œã€‚å› æ­¤ï¼Œè¿™ä¸¤ä¸ªçœŸçš„æ— å…³ç´§è¦ã€‚ä½†æˆ‘ä»¬ä¼šåœ¨åµŒå…¥å±‚è®¾ç½®æƒé‡ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ç¡®å®éœ€è¦å°†è¿™é‡Œçš„æŸ¥æ‰¾è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œå› ä¸ºä½ å¯èƒ½ä¸ä¼šçœŸæ­£æ·±å…¥ç ”ç©¶è¿™ä¸€ç‚¹ï¼Œä½†å¦‚æœå®ƒæœå¤šä¸ªæ–¹å‘å‘å±•ï¼Œä½ å¯ä»¥æœ‰å¤šä¸ªæŸ¥æ‰¾çŸ©é˜µã€‚
- en: but that would be a more advanced setã€‚ you can refer to the Kira's documentation
    if you're interested inã€‚Exactlyï¼Œ that is a listã€‚ Let's go ahead and run itã€‚ But
    for nowã€‚ just always embed your matrix list and you'll be good to go gonna go
    ahead and run thatã€‚ Now we have created essentially our dummy emulator as an embedding
    layerã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é‚£å°†æ˜¯ä¸€ä¸ªæ›´é«˜çº§çš„è®¾ç½®ã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥å‚è€ƒ Kira çš„æ–‡æ¡£ã€‚æ²¡é”™ï¼Œé‚£æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚è®©æˆ‘ä»¬ç»§ç»­è¿è¡Œå®ƒã€‚ä½†ç°åœ¨ï¼Œå§‹ç»ˆåµŒå…¥ä½ çš„çŸ©é˜µåˆ—è¡¨ï¼Œä½ å°±å¯ä»¥é¡ºåˆ©è¿›è¡Œã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®é™…ä¸Šåˆ›å»ºäº†ä¸€ä¸ªä½œä¸ºåµŒå…¥å±‚çš„è™šæ‹Ÿæ¨¡æ‹Ÿå™¨ã€‚
- en: I'm going to go ahead and run it down hereï¼Œ we're going to encode these two
    categoricals and run it and essentially look what it's doingã€‚ There's the dummy
    variablesã€‚ so you could put one of these on the front of your neural network and
    not even have to encode your dummy variablesã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ç»§ç»­åœ¨è¿™é‡Œç®€è¦ä»‹ç»ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†å¯¹è¿™ä¸¤ä¸ªç±»åˆ«è¿›è¡Œç¼–ç ï¼Œå¹¶æŸ¥çœ‹å®ƒçš„è¿è¡Œæƒ…å†µã€‚è¿™é‡Œæ˜¯è™šæ‹Ÿå˜é‡ã€‚è¿™æ ·ä½ å¯ä»¥å°†å…¶ä¸­ä¸€ä¸ªæ”¾åœ¨ç¥ç»ç½‘ç»œçš„å‰é¢ï¼Œç”šè‡³ä¸éœ€è¦ç¼–ç è™šæ‹Ÿå˜é‡ã€‚
- en: there's better ways of going about itã€‚ but this is one way that you you could
    do thatã€‚ if you wanted to make your neural network truly so that you could pass
    in these enter your values and have it automatically transform these into dummiesã€‚
    This is coolã€‚ you'll do this kind of thing a lotï¼Œ this is when you want to use
    transfer learning to bring your dummy variables inã€‚ Howeverï¼Œ the real fun gets
    inã€‚ maybe it's not the real funã€‚ğŸ˜Šï¼ŒYou can train these yourselfã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ›´å¥½çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªï¼Œä½†è¿™æ˜¯ä½ å¯ä»¥åšåˆ°çš„ä¸€ç§æ–¹å¼ã€‚å¦‚æœä½ æƒ³ä½¿ä½ çš„ç¥ç»ç½‘ç»œçœŸæ­£èƒ½å¤Ÿè¾“å…¥è¿™äº›å€¼ï¼Œå¹¶è‡ªåŠ¨å°†å…¶è½¬æ¢ä¸ºè™šæ‹Ÿå˜é‡ã€‚è¿™å¾ˆé…·ã€‚ä½ ä¼šç»å¸¸åšè¿™æ ·çš„äº‹æƒ…ï¼Œè¿™å°±æ˜¯ä½ æƒ³è¦ä½¿ç”¨è¿ç§»å­¦ä¹ æ¥å¼•å…¥ä½ çš„è™šæ‹Ÿå˜é‡çš„æ—¶å€™ã€‚ç„¶è€Œï¼ŒçœŸæ­£çš„ä¹è¶£åœ¨äºï¼Œä¹Ÿè®¸è¿™å¹¶ä¸æ˜¯æœ€çœŸå®çš„ä¹è¶£ã€‚ğŸ˜Šä½ å¯ä»¥è‡ªå·±è®­ç»ƒè¿™äº›ã€‚
- en: and this is a great way to deal with if your neural network needs to take in
    a high dimensionmen categorical that does not have an easy way that you can transform
    it into dummiesã€‚ say you have I don't know a 20000 Carnality categoricalã€‚ you
    could literally just define it as a embedding pick some arbitrary number of dimensions
    like I don't knowã€‚2040ã€‚ It's a hyperparameter you'd have to play with it and literally
    the atom update rule or back propagationã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤„ç†ç¥ç»ç½‘ç»œéœ€è¦è¾“å…¥é«˜ç»´ç±»åˆ«çš„å¥½æ–¹æ³•ï¼Œè€Œè¿™äº›ç±»åˆ«æ²¡æœ‰ç®€å•çš„æ–¹æ³•å¯ä»¥è½¬æ¢ä¸ºè™šæ‹Ÿå˜é‡ã€‚å‡è®¾ä½ æœ‰ä¸€ä¸ª 20000 ç»´çš„ç±»åˆ«ï¼Œä½ å¯ä»¥å°†å…¶å®šä¹‰ä¸ºåµŒå…¥ï¼Œé€‰æ‹©ä¸€äº›ä»»æ„çš„ç»´åº¦æ•°ï¼Œæ¯”å¦‚
    2040ã€‚è¿™æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä½ éœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œå®é™…ä¸Šæ˜¯ atom æ›´æ–°è§„åˆ™æˆ–åå‘ä¼ æ’­ã€‚
- en: All of themã€‚ it'll do gradient descent and it will train your embeddings for
    youã€‚ Let's go ahead and see how we can do thisã€‚ This uses some of the curs functions
    for natural language processingã€‚ This shows really how easily you can build these
    NLP neural networks now So here are 10 restaurant reviewsã€‚ The first ones are
    all badã€‚ Never coming backã€‚ horriblerrible serviceã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„å˜é‡ã€‚å®ƒå°†æ‰§è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå¹¶ä¸ºä½ è®­ç»ƒåµŒå…¥ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚è¿™ä½¿ç”¨äº†ä¸€äº›è‡ªç„¶è¯­è¨€å¤„ç†çš„å‡½æ•°ã€‚è¿™å®é™…ä¸Šå±•ç¤ºäº†ç°åœ¨ä½ å¯ä»¥å¤šä¹ˆè½»æ¾åœ°æ„å»ºè¿™äº› NLP
    ç¥ç»ç½‘ç»œã€‚æ‰€ä»¥è¿™é‡Œæœ‰ 10 æ¡é¤å…è¯„è®ºã€‚å‰å‡ æ¡éƒ½æ˜¯è´Ÿé¢çš„ã€‚å†ä¹Ÿä¸æ¥äº†ã€‚ç³Ÿç³•çš„æœåŠ¡ã€‚
- en: rude waitress Col food horrible food these other guysã€‚Re liked itã€‚ Awesomeï¼Œ
    awesomewesome serviceã€‚ Rocksã€‚ Poor work couldn't have done betterã€‚ So these are
    all just differentã€‚ different values that you can choose for thisã€‚ And notice
    I put in random exclamation pointsã€‚ and then even just a sort of random one that
    was more applying to sayï¼Œ evaluating contractorsã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ— ç¤¼çš„å¥³æœåŠ¡å‘˜ï¼Œé£Ÿç‰©ç³Ÿç³•ï¼Œè¿™äº›å…¶ä»–äººã€‚Reå–œæ¬¢å®ƒã€‚å¾ˆæ£’ï¼Œè¶…æ£’çš„æœåŠ¡ã€‚å¤ªæ£’äº†ã€‚ç³Ÿç³•çš„å·¥ä½œï¼Œåšå¾—ä¸é”™ã€‚æ‰€ä»¥è¿™äº›åªæ˜¯ä½ å¯ä»¥é€‰æ‹©çš„ä¸åŒå€¼ã€‚æ³¨æ„æˆ‘æ”¾å…¥äº†éšæœºçš„æ„Ÿå¹å·ã€‚è¿˜æœ‰ä¸€ä¸ªç¨å¾®éšæœºçš„ä¾‹å­ï¼Œæ›´å¤šæ˜¯åº”ç”¨äºè¯„ä¼°æ‰¿åŒ…å•†ã€‚
- en: but noise is goodã€‚ One means negative 0 is positiveã€‚ So these are the labelsã€‚
    This is the yã€‚ we're going train a neural network on itã€‚ So we're gonna say our
    vocabulary size is 50ã€‚ We can just pick that to be whatever the heck we wantã€‚
    We don't have to really count the number of words in thereã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å™ªå£°æ˜¯å¥½çš„ã€‚1è¡¨ç¤ºè´Ÿï¼Œ0è¡¨ç¤ºæ­£ã€‚æ‰€ä»¥è¿™äº›æ˜¯æ ‡ç­¾ã€‚è¿™æ˜¯yã€‚æˆ‘ä»¬å°†åœ¨å…¶ä¸Šè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œã€‚å› æ­¤æˆ‘ä»¬å°†è¯´æˆ‘ä»¬çš„è¯æ±‡å¤§å°ä¸º50ã€‚æˆ‘ä»¬å¯ä»¥ä»»æ„é€‰æ‹©è¿™ä¸ªæ•°å­—ã€‚æˆ‘ä»¬ä¸å¿…çœŸæ­£è®¡ç®—å…¶ä¸­çš„å•è¯æ•°é‡ã€‚
- en: And we're going to use the car is one hotã€‚ The car is one hot is kind of coolã€‚ğŸ˜Šã€‚It
    sort of scares me a little bitï¼Œ but it's good for examplesã€‚ It is basically going
    in there and tokenizing for youã€‚ So breaking these words into breaking these sentences
    into wordsã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ±½è½¦çš„ä¸€çƒ­ç¼–ç ã€‚æ±½è½¦çš„ä¸€çƒ­ç¼–ç å¾ˆé…·ã€‚ğŸ˜Šã€‚è¿™è®©æˆ‘æœ‰ç‚¹å®³æ€•ï¼Œä½†å¯¹äºç¤ºä¾‹æ¥è¯´å¾ˆå¥½ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯åœ¨ä¸ºä½ è¿›è¡Œæ ‡è®°ã€‚å› æ­¤ï¼Œå°†è¿™äº›è¯åˆ†è§£ï¼Œå°†è¿™äº›å¥å­åˆ†è§£ä¸ºå•è¯ã€‚
- en: and then assigning each to an indexã€‚ So it's doing a lot in the backgroundã€‚Normallyã€‚
    I like to have a little more control over thatï¼Œ I want to know how it's being
    tokenized and I would like to know how it is assigning these indexes to itã€‚ But
    this is good for for an exampleã€‚ when we get into showing how to connect one of
    these to an API that others will use sort of in a production environmentã€‚ we'll
    see that we really care about locking down what these index values areã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†æ¯ä¸ªåˆ†é…ç»™ä¸€ä¸ªç´¢å¼•ã€‚æ‰€ä»¥å®ƒåœ¨åå°åšäº†å¾ˆå¤šäº‹æƒ…ã€‚é€šå¸¸ï¼Œæˆ‘å–œæ¬¢å¯¹è¿™äº›æœ‰æ›´å¤šçš„æ§åˆ¶ï¼Œæˆ‘æƒ³çŸ¥é“å®ƒæ˜¯å¦‚ä½•è¢«æ ‡è®°çš„ï¼Œå¹¶ä¸”æˆ‘æƒ³çŸ¥é“å®ƒæ˜¯å¦‚ä½•å°†è¿™äº›ç´¢å¼•åˆ†é…ç»™å®ƒçš„ã€‚ä½†è¿™å¯¹äºç¤ºä¾‹æ¥è¯´æ˜¯å¥½çš„ã€‚å½“æˆ‘ä»¬å±•ç¤ºå¦‚ä½•å°†è¿™äº›è¿æ¥åˆ°å…¶ä»–äººåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨çš„APIæ—¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æˆ‘ä»¬éå¸¸å…³å¿ƒé”å®šè¿™äº›ç´¢å¼•å€¼ã€‚
- en: I wouldn't want if'm if I'm deploying this in a real worldor corporate situationã€‚
    I wouldn't want coming to become an index ofï¼Œ sayï¼Œ5 one time retrain an now forã€‚
    and potentially the data coming in is now encoded wrongã€‚So you have to be careful
    with all of thatã€‚ Then I am going to go ahead and go ahead and run this partã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘åœ¨ç°å®ä¸–ç•Œæˆ–ä¼ä¸šç¯å¢ƒä¸­éƒ¨ç½²è¿™ä¸ªï¼Œæˆ‘ä¸å¸Œæœ›æŸæ¬¡å˜å¾—ç´¢å¼•ä¸º5ï¼Œè¿›è¡Œé‡è®­ç»ƒï¼Œè€Œç°åœ¨ä¸º4ã€‚å¹¶ä¸”æ½œåœ¨çš„æ•°æ®ç°åœ¨ç¼–ç é”™è¯¯ã€‚æ‰€ä»¥ä½ å¿…é¡»å¯¹æ‰€æœ‰è¿™äº›å°å¿ƒã€‚ç„¶åæˆ‘å°†ç»§ç»­è¿è¡Œè¿™éƒ¨åˆ†ã€‚
- en: We're going to go ahead and run it and encode theseã€‚ These are all of your sequencesã€‚
    We would like these to all be of consistent length because that's how sequences
    workã€‚ We're going to do the max length of4ã€‚ Look at thatã€‚ They're all nice and
    zero paddedã€‚ Thank youã€‚ Kirasï¼Œ We're going to create a very simple sequential
    neural networkã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç»§ç»­è¿è¡Œå¹¶ç¼–ç è¿™äº›ã€‚è¿™äº›æ˜¯ä½ æ‰€æœ‰çš„åºåˆ—ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›åºåˆ—çš„é•¿åº¦ä¸€è‡´ï¼Œå› ä¸ºè¿™å°±æ˜¯åºåˆ—çš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æœ€å¤§é•¿åº¦4ã€‚çœ‹çœ‹å§ã€‚å®ƒä»¬éƒ½å¾ˆå¥½ï¼Œå¹¶ä¸”è¿›è¡Œäº†é›¶å¡«å……ã€‚è°¢è°¢ä½ ã€‚Kirasï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªéå¸¸ç®€å•çš„é¡ºåºç¥ç»ç½‘ç»œã€‚
- en: We are going to have one dense layer at the endã€‚ So there is learning going
    on in hereã€‚ There is one weight matrixã€‚ But then we're also learning in the embedding
    layerã€‚ The neural network is going to actually learn how to create these embeddings
    so that they are a way that separate those words and map them into Euclidean space
    in a meaningful wayã€‚ This saves you having to deal with tons and tons of dummy
    variables and creating a very complex neural networkã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨æœ€åæœ‰ä¸€ä¸ªç¨ å¯†å±‚ã€‚å› æ­¤è¿™é‡Œé¢æœ‰å­¦ä¹ ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæƒé‡çŸ©é˜µã€‚ä½†æˆ‘ä»¬ä¹Ÿåœ¨åµŒå…¥å±‚ä¸­è¿›è¡Œå­¦ä¹ ã€‚ç¥ç»ç½‘ç»œå®é™…ä¸Šå°†å­¦ä¹ å¦‚ä½•åˆ›å»ºè¿™äº›åµŒå…¥ï¼Œä»¥ä¾¿å°†è¿™äº›è¯ä»¥æœ‰æ„ä¹‰çš„æ–¹å¼åˆ†ç¦»å¹¶æ˜ å°„åˆ°æ¬§å‡ é‡Œå¾—ç©ºé—´ã€‚è¿™å¯ä»¥è®©ä½ é¿å…å¤„ç†å¤§é‡çš„è™šæ‹Ÿå˜é‡ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªéå¸¸å¤æ‚çš„ç¥ç»ç½‘ç»œã€‚
- en: embeddings are great for NLPã€‚ go ahead and run it We'll print a summaryã€‚ There
    you see itã€‚ go ahead and fit itã€‚ I'm just running atã€‚ğŸ˜Šï¼Œ100 epochsï¼Œ veryï¼Œ very
    fast to trainã€‚ Let's look at essentially the embeddingsã€‚ Each line is an embedding
    for a different wordã€‚ I'm not even going to try to explain the rhyme or reason
    for thisã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥å¯¹äºNLPæ¥è¯´å¾ˆæ£’ã€‚ç»§ç»­è¿è¡Œï¼Œæˆ‘ä»¬å°†æ‰“å°ä¸€ä¸ªæ‘˜è¦ã€‚ä½ çœ‹åˆ°äº†å—ï¼Ÿç»§ç»­æ‹Ÿåˆå®ƒã€‚æˆ‘åªæ˜¯è¿è¡Œå®ƒã€‚ğŸ˜Šï¼Œ100ä¸ªå‘¨æœŸï¼Œéå¸¸éå¸¸å¿«åœ°è®­ç»ƒã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æœ¬è´¨ä¸Šçš„åµŒå…¥ã€‚æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªä¸åŒå•è¯çš„åµŒå…¥ã€‚æˆ‘ç”šè‡³ä¸ä¼šå°è¯•è§£é‡Šå…¶ä¸­çš„åŸå› ã€‚
- en: There' essentially like weights that were calculated in the same way that the
    weights were for the actual layersã€‚ and there is one layer on here that's learning
    as wellã€‚ but the embeddings learn right or long with the other weights in the
    neural networkã€‚ and then we can evaluate this neural network at the endã€‚ accuracy
    is perfectã€‚ actuallyã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡ŒåŸºæœ¬ä¸Šå°±åƒæ˜¯æƒé‡ï¼Œè¿™äº›æƒé‡çš„è®¡ç®—æ–¹å¼ä¸å®é™…å±‚çš„æƒé‡æ˜¯ç›¸åŒçš„ã€‚è¿™é‡Œæœ‰ä¸€å±‚ä¹Ÿåœ¨å­¦ä¹ ã€‚ä½†æ˜¯åµŒå…¥æƒé‡ä¸ç¥ç»ç½‘ç»œä¸­çš„å…¶ä»–æƒé‡ä¸€èµ·å­¦ä¹ ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æœ€åè¯„ä¼°è¿™ä¸ªç¥ç»ç½‘ç»œã€‚å‡†ç¡®æ€§å®é™…ä¸Šæ˜¯å®Œç¾çš„ã€‚
- en: The reason accuracy is perfect is I really didn't put any overlap hereã€‚ All
    the words for negative reviews werere not in the positive reviewsã€‚ This is just
    a toy exampleã€‚ Thank you for watching this video and the next video we're going
    to look at end to end natural language processing This content changes oftenã€‚
    So subscribe to the channel to stay up to date on this course and other topics
    in artificial intelligenceã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®æ€§å®Œç¾çš„åŸå› æ˜¯æˆ‘åœ¨è¿™é‡Œæ²¡æœ‰æ”¾ç½®ä»»ä½•é‡å ã€‚è´Ÿé¢è¯„è®ºä¸­çš„æ‰€æœ‰å•è¯éƒ½ä¸åœ¨æ­£é¢è¯„è®ºä¸­ã€‚è¿™åªæ˜¯ä¸€ä¸ªç©å…·ç¤ºä¾‹ã€‚æ„Ÿè°¢è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­æˆ‘ä»¬å°†ç ”ç©¶ç«¯åˆ°ç«¯çš„è‡ªç„¶è¯­è¨€å¤„ç†ã€‚è¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ã€‚æ‰€ä»¥è¯·è®¢é˜…é¢‘é“ä»¥ä¾¿åŠæ—¶äº†è§£è¯¥è¯¾ç¨‹åŠå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚
- en: '![](img/6c6a4e57e3618c11a532abc5118438e7_4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c6a4e57e3618c11a532abc5118438e7_4.png)'
