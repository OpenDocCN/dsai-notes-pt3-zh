- en: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P4：L4- 使用 PyTorch
    构建模型 - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P4：L4- 使用 PyTorch
    构建模型 - ShowMeAI - BV19L4y1t7tu
- en: Hi and welcome to the next video in the Pytorrch Training series on building
    models with Pytorrch。Specifically in this video， we're going to discuss the module
    and parameter classes in P Torch。 which encapsulate your machine learning models
    and learning weights respectively and how they work together。Common neural network
    layer types， including linear， convolutional。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，欢迎来到 PyTorch 训练系列的下一个视频，主题是使用 PyTorch 构建模型。具体来说，在本视频中，我们将讨论 P Torch 中的模块和参数类，它们分别封装了你的机器学习模型和学习权重，以及它们如何协同工作。常见的神经网络层类型，包括线性层和卷积层。
- en: recurrent and transformer networks。Other layers and functions such as batch
    norming。 dropout and activation functions and loss functions。![](img/7935fe219fa1194c20d621ebcc5e488b_1.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络和变换器网络。其他层和功能，例如批归一化、丢弃和激活函数以及损失函数。![](img/7935fe219fa1194c20d621ebcc5e488b_1.png)
- en: Model building in Pytorrch is centered around two classes in the Torchdo N N
    module。 The module class and the parameter class。The module class encapsulates
    models and model components。 such as neural network layers。 The parameter class
    is a subclass of torch dot tensor that represents learning weights。Moduuleles
    and parameters work together when a parameter is assigned as an attribute of a
    module。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pytorrch 中构建模型围绕 Torch.nn.Module 中的两个类进行。模块类和参数类。模块类封装了模型和模型组件，例如神经网络层。参数类是
    torch.Tensor 的子类，表示学习权重。当参数被分配为模块的属性时，模块和参数一起工作。
- en: The parameter object gets registered with that module。If you register an instance
    of a module subclass as an attribute of a module。 the contained module's parameters
    are also registered as parameters of the owning class。This might be simpler with
    an example。 Let's have a look at this small model。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 参数对象与该模块注册。如果将模块子类的实例作为模块的属性注册，则包含的模块的参数也会作为拥有类的参数注册。这可能通过一个例子更简单。让我们看看这个小模型。
- en: It has two fully connected neural network layers， an activation function in
    between them and a softm at the end。This modelddle shows the common structure
    of a pi torch model。 First。 note that it's a sub class of torchsha and in dot
    module。There is an init method that defines the structure of the model。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它有两个全连接神经网络层，中间有一个激活函数，最后是一个 softmax。这一模型显示了一个 PyTorch 模型的常见结构。首先，请注意它是 torch.nn.Module
    的子类。这里有一个初始化方法，定义了模型的结构。
- en: the layers and functions that make it up。There's also a forward method。 which
    composes those layers and functions into the actual computation。When we create
    an instance of this model and print it。We see that not only does it know its own
    layers and the attributes there are assigned to。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 组成它的层和函数。还有一个前向方法，将这些层和函数组合成实际的计算。当我们创建这个模型的实例并打印时，我们会发现它不仅知道自己的层，还知道分配给这些层的属性。
- en: but also the order in which we registered them。We print out just one of the
    layers。We get a description of just that layer。Our tiny model and our linear layers
    are all subclasses of Torchsha and N dot module。 so we can access their parameters
    through the parameters method。Here。We've printed out。The learning parameters for
    the whole model and port only the linear2 layer note that the tensors。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以及我们注册它们的顺序。我们只打印出其中一层。我们得到了该层的描述。我们的微型模型和线性层都是 torch.nn.Module 的子类，因此我们可以通过参数方法访问它们的参数。在这里，我们打印出了整个模型的学习参数以及线性2层的参数，注意这些张量。
- en: making up linear 2's parameters are the same as the last parameters of the whole
    model。 The model registers parameters of sub modules it owns recursively。This
    is important because the model has to pass all of these parameters to the optimizer
    during training。Next， let's take a look at some of the layer types available in
    Pytorrch。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 线性2的参数与整个模型的最后参数相同。模型递归地注册它拥有的子模块的参数。这一点很重要，因为模型在训练期间必须将所有这些参数传递给优化器。接下来，让我们看看
    Pytorrch 中可用的一些层类型。
- en: Pytorrch has classes encapsulating the common layer types used in modern machine
    learning models。The most basic type is the fully connected or linear layer， which
    we saw in the example above。 This is a layer where every input influences every
    output， hence calling it fully connected。And that influence is to a degree determined
    by the layer's weights。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorrch 有封装现代机器学习模型中常用层类型的类。最基本的类型是全连接层或线性层，我们在上面的例子中看到过。这是一个每个输入影响每个输出的层，因此称其为全连接层。而这种影响在一定程度上由层的权重决定。
- en: If a layer has M inputs and N outputs， its weights will be an M by N matrix。As
    a simple example。 here is a tiny linear layer that takes it three element input
    and yields a two element output。And there's a random three element vector we'll
    use as input。Passing this input gives us a two element output vector。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个层有 M 个输入和 N 个输出，它的权重将是一个 M x N 的矩阵。作为一个简单的例子，这里有一个小型线性层，它接收三元素输入并产生两个元素输出。还有一个随机的三元素向量，我们将用作输入。传递这个输入将给我们一个两元素的输出向量。
- en: If you go ahead and do the matrix multiplication of x times the weights。 and
    then you add the two biases， you should get the output Y。Also。 note that when
    we print the parameters， it lets us know that they require gradients。 That is
    that they're tracking computation history so we can compute gradients for learning。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续进行 x 乘以权重的矩阵乘法，然后加上两个偏置，你应该得到输出 Y。此外，请注意，当我们打印参数时，它告诉我们这些参数需要梯度。也就是说，它们在跟踪计算历史，以便我们可以计算学习的梯度。
- en: Pararameter is a subclass of torchdot tensor， but this default behavior of setting
    autog to true is different from what the tensor class does。Linear layers are widely
    used in deep learning models。 One common place you'll see them is at the end of
    classifier models with the last layer or last few layers will be linear layers。Convolutional
    layers are meant to address data that is strongly correlated in space。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Pararameter 是 torchdot tensor 的一个子类，但将 autog 设置为 true 的默认行为与 tensor 类的行为不同。线性层在深度学习模型中被广泛使用。你常会看到它们出现在分类模型的最后一层或最后几层，通常都是线性层。卷积层旨在处理空间上高度相关的数据。
- en: They're common in computer vision models where they can be used to detect close
    clusters of interesting features and compose them into larger features or recognized
    objects。 They come up in other contexts， too， such as N LP applications。 Often。
    the intent of a word is influenced by the words near it。In an earlier video。 we
    had to look at Lynette 5。 Let's take a closer look at how this computation is
    structured。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在计算机视觉模型中很常见，可以用于检测有趣特征的近聚类并将其组合成更大的特征或识别对象。在其他上下文中也会出现，例如 N LP 应用程序中。通常，一个词的意图会受到其周围词的影响。在之前的视频中，我们看到了
    Lynette 5。让我们更仔细地看看这个计算是如何构建的。
- en: Lette 5 is meant to take in black and white，32 by 32 pixel image tiles of handwritten
    numbers and classify them。 according to which diit is represented。Looking。At the
    first convolutional layer in the model。We can see his arguments are 1，6 and 5。
    The first argument is the number of input channels For us。 that's going to be
    one because a black and white image only has one channel of data。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Lette 5 旨在接收黑白 32 x 32 像素的手写数字图像块并进行分类，根据所代表的数字。看看模型中的第一个卷积层，我们可以看到它的参数是 1、6
    和 5。第一个参数是输入通道的数量，对我们来说，只有一个，因为黑白图像只有一个数据通道。
- en: The second argument 6 is the number of features we want this layer to learn。So
    it can recognize up to six different arrangements of pixels。In and the input。And
    finally。 the five is the size of the convolution kernel。You can think of this
    like a window like it scanned over the input。 collecting features inside this5
    pixel window。 The output of this convolutional layer is an activation map that
    is a spatial map of where it found certain features。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数 6 是我们希望这个层学习的特征数量。因此它可以识别多达六种不同的像素排列。在输入中。最后，五是卷积核的大小。你可以把它想象成一个窗口，像是扫描输入，收集这个
    5 像素窗口内的特征。这个卷积层的输出是一个激活图，是一个空间图，显示了它在哪里发现了某些特征。
- en: The second convolutional layer。Is similar， it takes the first layers in output
    as input。So that's why its first argument is6， we recognize six features in the
    first conve。And we take those as our six input channels to the next convolutional
    layer。This layer。 we're asking to learn 16 different features， which it makes
    by composing the features from the first layer。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个卷积层类似，它将第一个层的输出作为输入。这就是为什么它的第一个参数是6，因为我们在第一个卷积层中识别出六个特征。我们将这些特征作为下一卷积层的六个输入通道。这个层我们要求学习16种不同的特征，这些特征是通过组合第一个层的特征生成的。
- en: And we're only using a three element window。For the convolution card。After the
    second convolution layer has composed its features into a higher level activation
    map。 we pass the output to a set of linear layers that act as a classifier， with
    the final layer。 having 10 outputs representing the probabilities that the input
    represents one of the 10 digits。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用三个元素的窗口进行卷积操作。在第二个卷积层将特征组合成更高级别的激活图后，我们将输出传递给一组线性层，作为分类器，最后一层有10个输出，表示输入代表10个数字之一的概率。
- en: Pytorch has convolutional neural network layers for1，2 and three dimensional
    inputs。 There are more optional arguments that you can look up in the documentation。
    such as stride length and padding。![](img/7935fe219fa1194c20d621ebcc5e488b_3.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch为一维、二维和三维输入提供了卷积神经网络层。还有更多可选参数可以在文档中查找，例如步幅和填充。![](img/7935fe219fa1194c20d621ebcc5e488b_3.png)
- en: '![](img/7935fe219fa1194c20d621ebcc5e488b_4.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7935fe219fa1194c20d621ebcc5e488b_4.png)'
- en: The current neural networks are neural networks designed for use with sequential
    data。 such as a string of words in a natural language sentence。 or a string of
    real time measurements from an instrument。And R and N does this by keeping a hidden
    state。 This acts as a sort of memory for what its seems so far in the sequence。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的神经网络是为处理序列数据而设计的，例如自然语言句子中的单词串，或来自仪器的实时测量串。RNN通过保持一个隐藏状态来实现这一点。这充当了序列中已观察到内容的一种记忆。
- en: The internal structure of an R N N layer or its variance。 the long short term
    memory or LSTM and the gated recurrent unit or G R U。Is pretty complex and outside
    the scope of this video。 but we can show you what it looks like in action with
    this long。 short term memory based part of speech tagger。The constructor has four
    arguments。It has the size of the input vocabulary that is the size of the entire
    inventory of words it's meant to recognize。Each of these words is represented
    as the index of a one hot vector in a vocab size dimensional space。The tag set
    size is the number of tags。That you want the model to recognize an output。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RNN层或其变体（如长短期记忆（LSTM）和门控递归单元（GRU））的内部结构相当复杂，超出了本视频的范围。但我们可以通过这个基于长短期记忆的词性标注器来展示它的实际效果。构造函数有四个参数。它有输入词汇表的大小，也就是它要识别的所有单词的总数。每个单词在词汇大小维空间中表示为一个独热向量的索引。标签集大小是你希望模型识别和输出的标签数量。
- en: Embbedding dim is the size of the embedding space for the vocabulary。 and embedding
    maps of vocabulary down to a lower dimensional space where words with similar
    meanings are close together in that space。And hidden dim is the size of the LSTM's
    memory。The input will be a sentence with words represented as indices of one hot
    vectors。 The embedding layer will map these down to the embedding dim dimensional
    space。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入维度是词汇表的嵌入空间的大小，嵌入将词汇表映射到一个较低维度的空间，其中具有相似含义的单词在该空间中相互接近。隐藏维度是LSTM内存的大小。输入将是一个句子，单词以独热向量的索引表示。嵌入层将这些映射到嵌入维度的空间中。
- en: And the LSTM takes a sequence of embeddings and iterates over it。Yielding an
    output vector of length hidden den。The final linear layer acts as a classifier
    applying log softmax to the output。 and the final layer converts the output into
    a normalized set of estimated probabilities that a given word maps to a given
    part of speech tag。If you'd like to see this network in action， there's a tutorial
    on it on Pytor。org。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM接受一系列嵌入并对其进行迭代，产生一个长度为隐藏层维度的输出向量。最终的线性层作为分类器，应用对数软最大化到输出，最后一层将输出转换为一个标准化的概率集，表示给定单词映射到给定词性标签的估计概率。如果你想看到这个网络的实际效果，可以在Pytorch.org上找到相关教程。
- en: Transformers are multipopoous neural networks， but we see them very often these
    days in natural language applications following the success of BERT。 which is
    a transformable model。Now a discussion of transformer architecture which is kind
    of complex。 is outside the scope of this video， but know that Pytorrch has a transformer
    class that allows you to define the overall parameters of a transformer model。That
    is the number of encoder and decoder layers， the number of attention heads。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是多头神经网络，但我们今天在自然语言应用中经常看到它们，尤其是在BERT成功之后，这是一个可变形的模型。关于Transformer架构的讨论相对复杂，超出了本视频的范围，但请知道Pytorch有一个Transformer类，可以让你定义Transformer模型的整体参数，包括编码器和解码器层的数量以及注意力头的数量。
- en: dropout in activation functions， etc。You can even using the pi torch transformer
    class。 build the BRT model from this single class with the right parameters。Pytorch
    also has classes to encapsulate the individual components of a transformer。 such
    as the encoder and decoder and the layers that make them up。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在激活函数中使用dropout等。你甚至可以使用Pytorch的Transformer类，从这个单一的类构建BRT模型，只需正确的参数。Pytorch还提供了类来封装Transformer的各个组成部分，如编码器、解码器及其组成层。
- en: '![](img/7935fe219fa1194c20d621ebcc5e488b_6.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7935fe219fa1194c20d621ebcc5e488b_6.png)'
- en: '![](img/7935fe219fa1194c20d621ebcc5e488b_7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7935fe219fa1194c20d621ebcc5e488b_7.png)'
- en: There are non learning layer types that perform important functions in models。One
    example is max pooling and it's twin min pullinging。These functions reduce a tensor
    by combining cells together and assigning the maximum value of those input cells
    to the output cell。This is one of those things that's probably easier explained
    by example。So if you look closely here。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些非学习层类型在模型中执行重要功能。一个例子是最大池化及其双胞胎最小池化。这些函数通过将单元组合在一起并将这些输入单元的最大值分配给输出单元来减少张量。这种情况可能通过示例更容易解释。因此，如果你仔细看这里。
- en: we have a 6 by6 matrix， which we use max pool to reduce to a 4 by four matrix。The4
    by four matrix。 the four elements of it， each one contains the maximum value of
    a3 by three quadrant from the input。Normalization layers recententer and normalize
    the output of one layer before feeding it to another。Centering and scaling intermediate
    tensors inside your computation has a number of beneficial effects。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个6x6的矩阵，我们使用最大池化将其减少到一个4x4的矩阵。这个4x4的矩阵中的四个元素，每个元素包含来自输入的3x3区域的最大值。归一化层会重新中心并归一化一个层的输出，然后再将其输入到另一个层。对计算中的中间张量进行中心化和缩放有许多有益的效果。
- en: such as letting you use higher learning rates without problems of vanishing
    and exploding gradients。Running a cell above， you've added a large scaling factor
    and an offset to a random input tensor。You should see that the input tensors mean
    is somewhere in the neighborhood of 15。After we run it through the normalization
    layer， you can see that the values are all smaller and grouped around 0。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 比如让你使用更高的学习率，而不会出现梯度消失和爆炸的问题。在上面的单元运行后，你已经为一个随机输入张量添加了一个大的缩放因子和偏移量。你应该能看到输入张量的均值大约在15左右。经过归一化层处理后，你会发现这些值都变得更小，并且聚集在0附近。
- en: In fact， the mean of this should be very smaller。This is good because a lot
    of activation functions。 which we'll discuss in a little bit， have their strongest
    gradients near zero。But they sometimes suffer from vanishing or exploding gradient
    for inputs that drive them far away from0。Keeping the data centered around the
    area's steepest gradient means that learning will tend to happen faster and converge
    faster。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个均值应该非常小。这很好，因为很多激活函数（我们稍后会讨论）在接近零的地方具有最强的梯度。但它们有时会因输入使它们远离0而遭受梯度消失或爆炸。保持数据围绕最陡梯度的区域中心化意味着学习会更快发生，收敛也会更快。
- en: and higher learning rates will be feasible for your training。Drop out layers
    are a tool for encouraging sparse representations in your model that is pushing
    it to do inference with less data。Drop out layers work by randomly setting parts
    of the input tensor to 0 During training。 Drop out layers are always turned off
    for inference。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的学习率将对你的训练是可行的。Dropout层是一种工具，用于鼓励模型中的稀疏表示，促使其用更少的数据进行推理。Dropout层通过在训练期间随机将输入张量的部分设为0来工作。在推理时，Dropout层始终是关闭的。
- en: This forces the model to learn how to do inference against masked or reduced
    debt input data。So as an example。If I take a random input tensor and I pass it
    through a dropout layer。Twice。 you should see that。I get the same to put Tensor
    back with some random elements。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这迫使模型学习如何在掩蔽或减少的输入数据上进行推断。作为一个例子。如果我拿一个随机输入张量并将其通过Dropout层传递两次，你应该看到我得到相同的输入张量，里面有一些随机元素。
- en: '![](img/7935fe219fa1194c20d621ebcc5e488b_9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7935fe219fa1194c20d621ebcc5e488b_9.png)'
- en: Set to zero。![](img/7935fe219fa1194c20d621ebcc5e488b_11.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为零。![](img/7935fe219fa1194c20d621ebcc5e488b_11.png)
- en: Drop out layers， Heba model learns sparse representations by pushing it to do
    inference with less input data。Drop out white layer's work by randomly setting
    parts of the input tensor to zero during training。Dropout layers are always turned
    off for inference。This forces the models to learn against a masked or reduced
    data set。So as an example。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout层，Heba模型通过迫使其在较少的输入数据上进行推断来学习稀疏表示。Dropout层通过在训练期间随机将输入张量的部分设置为零来工作。在推断时，Dropout层始终关闭。这迫使模型在掩蔽或减少的数据集上学习。因此作为一个例子。
- en: I'll create a random input tensor。And pass it through a dropout layer， and I'll
    do it twice。And you'll see that there are some zeros， and there are some values。
    but the values are always identical。 It's randomly setting the zeros throughout
    the tensor。You can use the optional P argument to set the probability here， we
    send it to 40%， the default is 0。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我会创建一个随机输入张量，并将其传递通过一个Dropout层，我会这样做两次。你会看到里面有一些零，还有一些值，但这些值始终是相同的。它是在张量中随机设置零。你可以使用可选的P参数在这里设置概率，我们将其设为40%，默认值为0。
- en: 5。The final ingredients we need to build our models are activation functions
    and loss functions。 Activation functions are part of what make deep learning impossible。
    If you recall the linear layer example earlier， it was just a simple matrix multiplication
    to take an input vector and get an output vector。And if we stack many such layers
    together， no matter how many layers we do。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 5。构建模型所需的最终成分是激活函数和损失函数。激活函数是使深度学习成为可能的一部分。如果你回忆之前的线性层示例，它只是一个简单的矩阵乘法，将输入向量转化为输出向量。如果我们将许多这样的层堆叠在一起，无论我们堆叠多少层。
- en: we can always reduce that to a single matrix multiplication。 which means that
    we can only ever simulate linear equations。With our machine learning model。This
    is where activation functions come in by inserting a nonlinear activation function
    between layers。We developed the ability to simulate nonlinear equations。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是可以将其简化为单个矩阵乘法。这意味着我们只能模拟线性方程。通过我们的机器学习模型，这就是激活函数的作用，通过在层之间插入非线性激活函数，使我们能够模拟非线性方程。
- en: Tsia N dot module offers all the major activation functions。 including the rectified
    linear unit in many variants， hyperbolic tangent， hard hyperbolic tangent。 sigmoid，
    etc ce。It includes other functions such as Sockms that are most useful at the
    output stage of a model。Py torch has a variety of common loss functions， including
    mean squared error。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Tsia N dot 模块提供了所有主要的激活函数，包括多种变体的修正线性单元、双曲正切、硬双曲正切、sigmoid等。它还包括其他在模型输出阶段最有用的函数，如Sockms。PyTorch有多种常见的损失函数，包括均方误差。
- en: which same as the L2 norm and cross entrytropy loss and negative likelihood
    loss。 which are useful for classifiers and others。![](img/7935fe219fa1194c20d621ebcc5e488b_13.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这与L2范数、交叉熵损失和负对数似然损失相同，这些对于分类器及其他模型都是有用的。![](img/7935fe219fa1194c20d621ebcc5e488b_13.png)
