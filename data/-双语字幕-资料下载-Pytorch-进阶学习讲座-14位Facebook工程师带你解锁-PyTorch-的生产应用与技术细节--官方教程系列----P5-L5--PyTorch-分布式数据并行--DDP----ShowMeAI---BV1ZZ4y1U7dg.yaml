- en: 【双语字幕+资料下载】Pytorch 进阶学习讲座！14位Facebook工程师带你解锁 PyTorch 的生产应用与技术细节 ＜官方教程系列＞ - P5：L5-
    PyTorch 分布式数据并行 (DDP) - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】Pytorch 进阶学习讲座！14位Facebook工程师带你解锁 PyTorch 的生产应用与技术细节 ＜官方教程系列＞ - P5：L5-
    PyTorch 分布式数据并行 (DDP) - ShowMeAI - BV1ZZ4y1U7dg
- en: 🎼。![](img/8bb021252445710b64cf368008f50dc4_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 🎼。![](img/8bb021252445710b64cf368008f50dc4_1.png)
- en: Hey everybody， this is Priam themania and I'm a software engineer at Facebook
    working on Pythtos Disbut I'm going to talk a little bit more about Pyths Dis
    today。In terms of agenda， I'm going to talk a little bit about distributed data
    parallelable。 which is DP and CTND， which is a distributed communication library。
    and then I'll talk a little bit about like future work in terms of what's coming
    in the future for PythOs distributed。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，我是Priam themania，我是Facebook的一名软件工程师，正在研究Pythtos Dis，但今天我将详细讲讲Pyths Dis。关于议程，我会谈谈分布式数据并行，简称DP，以及CTND，这是一种分布式通信库。然后我会讨论一下PythOs分布式的未来工作。
- en: '![](img/8bb021252445710b64cf368008f50dc4_3.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb021252445710b64cf368008f50dc4_3.png)'
- en: So let's take a quick refresher of distributed data parallel first。 so if you
    have a single model that's small enough to fit on a single GPU。 what youd do is
    like you'd use distributed data parallel to train this on a large scale in terms
    of large amount of data and large amount of GPUs。 so you would replicate this
    model on multiple GPUs。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速回顾一下分布式数据并行。如果你有一个足够小的模型可以适应单个GPU，你会使用分布式数据并行在大规模的数据和多个GPU上进行训练。你会在多个GPU上复制这个模型。
- en: run the forward and backward Pass and parallel and then once you have the gradients。
    you'd have like a synchronized gradients operation which all of the ranks will
    enter to kind of aggregate all of the gradients。 and then you'd kind of continue
    other iterations where you run more forward and backward passes and synchronized
    gradients。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前向和后向传播并行处理，得到梯度后，所有的rank会进入一个同步梯度操作，以聚合所有的梯度。接着，你会继续其他迭代，进行更多的前向和后向传播以及同步梯度操作。
- en: '![](img/8bb021252445710b64cf368008f50dc4_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb021252445710b64cf368008f50dc4_5.png)'
- en: So that was a quick overview now let's talk about what's coming what are kind
    of some of the new improvements in DP so the first one is DP communication hook
    So what this does it allows you to completely override the synchronized gradient
    operation that I just talked about so you can register a Python colable and then
    have some arbitrary logic in terms of how you want to like aggregate the gradients
    So one example here is if youll actually want to do FP16 compression of gradients
    before you communicate that you could have like a callable like this where you
    kind of compress the gradients you convert them to float 16 you all reduce and
    float 16 and then finally you decompress back to float 32 so there's one example
    you could do more fancier things like gossip gridd which is an non full sync SD
    algorithm。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个快速概述，现在让我们谈谈未来的改进，DP的一些新进展。第一个是DP通信钩子。这个功能允许你完全覆盖我刚才提到的同步梯度操作，因此你可以注册一个Python可调用对象，然后根据你想要如何聚合梯度来添加任意逻辑。这里有一个例子，如果你想在通信之前对梯度进行FP16压缩，你可以有这样的一个可调用对象，首先压缩梯度，将其转换为float
    16，进行全局归约，最后再解压回float 32。这是一个例子，你还可以做更复杂的事情，比如gossip gridd，这是一种非全同步的SD算法。
- en: Okay the next item is support for uneven inputs in DP so if you have like uneven
    number of batches across different ranks。 what would typically happen is some
    ranks which have like finished their data would not enter like this synchronized
    gradient call while other ranks which are still kind of processing data would
    enter this call and as a result this will lead to either a hang or some sort of
    timeout so this has been a longstanding issue in DP that a lot of Pythto users
    have complained about so now we do have a fix for this you can use this modeled
    or joint context wrapper that's shown in the example here so what this ensures
    is that once some ranks are finished with their data theyll kind of do a bunch
    of dummy synchronized operations to kind of match other ranks which are still
    processing data and this guarantees all ranks complete their processing altogether
    so this is a nice。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，下一个项目是DP对不均匀输入的支持，所以如果你在不同的rank中有不均匀数量的批次。通常发生的情况是，一些已经完成其数据的rank不会进入这个同步梯度调用，而其他仍在处理数据的rank则会进入这个调用，结果会导致挂起或某种超时。因此，这在DP中一直是一个长期存在的问题，许多Pythto用户对此表示不满，现在我们对此有了修复，你可以使用这个在这里显示的模型或联合上下文包装器。这确保了一旦某些rank完成了数据处理，它们会执行一系列虚假的同步操作，以匹配仍在处理数据的其他rank，这确保所有rank能够同时完成他们的处理，这真是太好了。
- en: wayay to kind of deal with uneven inputs across your training。Then we have some
    memory optimizations for DP， so DP today creates a bunch of buckets and to kind
    of batch parameters together for an all reduceduce call which is much more efficient
    but what DP does is it creates entire copy of the gradients for these buckets
    so if you have a one gigabyte model you'll have like one gigyte of parameters。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理训练中不均匀输入的一种方法。然后我们有一些针对DP的内存优化，因此今天的DP会创建一堆桶，将参数批处理在一起以进行一次更高效的全归约调用，但DP所做的是为这些桶创建整个梯度的副本，因此如果你有一个一GB的模型，你将拥有大约一GB的参数。
- en: one gigabyte of gradients and the DP would take another gigabyte because it
    creates a copy of these gradients so to get around this we have a new parameter
    and D called gradient as bucket view so what this does is it makes the dot grad
    field of your parameter a view of the bucket so that way we kind of have only
    one copy of the gradients。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一GB的梯度，DP会再占用一GB，因为它会创建这些梯度的副本。为了避免这种情况，我们引入了一个新的参数D，称为梯度桶视图，因此这使得你的参数的dot grad字段成为桶的一个视图，这样我们就只拥有一份梯度副本。
- en: Then I'd like to talk about combining DP and RPpC so Shan in his talk kind of
    described the RPpC framework and how it can be used for distributed model parallelism
    and DDP is used for distributed data parallelism so now we can kind of combine
    both of these frameworks together to kind of have more complicated training paradigms
    so as you can see in this example we have a DP model where it's a model wrapped
    in DP here so that model is replicated and then we have some remote parameters
    here on worker1 so now if you'd like to train this model you set up your distributed
    optimizer in your forward pass you retrieve the RF which is typically retrieve
    via RPpC and then you feed that into DP you compute the loss and then you run
    your backward and optimizer step so this will kind of run the backward it will
    kind of aggregate the gradients across all of the replicas and then it'll also
    update the remote parameters。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我想谈谈将DP与RPpC结合使用，因此在他的演讲中，Shan描述了RPpC框架以及它如何用于分布式模型并行，而DDP用于分布式数据并行，所以现在我们可以将这两个框架结合起来，形成更复杂的训练范式。如你所见，在这个例子中，我们有一个DP模型，这个模型被包装在DP中，因此这个模型被复制，然后我们在worker1上有一些远程参数，所以现在如果你想训练这个模型，你需要在前向传播中设置分布式优化器，你检索RF，通常是通过RPpC来检索的，然后将其输入到DP中，计算损失，然后运行反向传播和优化步骤，这将执行反向传播，它将跨所有副本聚合梯度，然后还会更新远程参数。
- en: And it'll kind of run the optimizer remotery as well。 So as you can see。 you
    can like combine both of these frameworks pretty seamlessly。![](img/8bb021252445710b64cf368008f50dc4_7.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它也会远程运行优化器。如你所见，你可以非常顺利地结合这两个框架。![](img/8bb021252445710b64cf368008f50dc4_7.png)
- en: Okay now I' like to talk about dynamic bucketing in DP so DP would kind of split
    the parameters into multiple buckets as I just mentioned and it kind of assumes
    that the order of the backward pass is the reverse of model do parameters when
    it kind of builds these buckets so if this if this order is not true and in many
    models this is the case what would happen is like the buckets are not built in
    the optimal order and as a result maybe bucket2 gets ready before bucket one and
    then bucket one needs to wait before it can kind of schedule it's all reduce so
    this kind of results in suboptimal performance to kind of get around this DP now
    records the order of the parameters in the first backward pass and then rebuilds
    the buckets in the optimal order so that you can now schedule the all reduces
    optimally so this kind of showed about like3 to7% speed up in models like B and
    Roberta。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我想谈谈 DP 中的动态分桶。DP 会将参数分成多个桶，如我刚才提到的，它假设反向传播的顺序是模型参数的反向顺序。当它构建这些桶时，如果这个顺序不成立，而在许多模型中确实是这样，那么桶就不会以最佳顺序构建，结果可能是桶
    2 在桶 1 之前准备好，随后桶 1 需要等待才能调度它的全归约。这会导致性能 suboptimal。为了解决这个问题，DP 现在记录了第一次反向传播中的参数顺序，并以最佳顺序重建桶，这样你就可以更优地调度全归约。因此，在像
    B 和 Roberta 这样的模型中，性能提高了大约 3% 到 7%。
- en: Then finally a few miscellaneous improvements we've added better error handling
    in niel by a couple of environment variables so you can look at the documentation
    for these for more details we had like a distributed keyvalue store in CD this
    is mostly used for Rndezvous and coordination so we've mostly just formalized
    this API added some good documentation around it for users we've added Windows
    support for CTD and I'd like to thank Microsoft for this contribution。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，最后是一些杂项改进。我们在 niel 中添加了更好的错误处理，通过几个环境变量进行配置。你可以查看文档以获取更多细节。我们在 C&D 中有一个分布式键值存储，主要用于会面和协调，所以我们主要只是规范化了这个
    API，并为用户添加了一些好的文档。我们为 CTD 添加了 Windows 支持，我想感谢 Microsoft 的贡献。
- en: So now what's coming soon in Py distributed so this is probably in the short
    term maybe like Pytch 1。8 or Pytch 1。9 we're adding point to point communication
    support and process group and C1D so this is built upon nichels point to point
    send and receive support we're adding native GPU support for the RPpC framework
    so you can send and receive GPU tensors over RPC seamlessly we're going to add
    a remote module remote device kind of API for distributed model parallelism so
    you don't have to use the RC framework directly if you want to like just place
    a module or part of a model on a different host different GPU you can just use
    remote module to kind of do that and it will be a nice highlevel API。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在在 Py 分布式中即将推出的内容是什么呢？这可能是在短期内，比如 Pytch 1.8 或 Pytch 1.9，我们将添加点对点通信支持和进程组以及
    C1D。这是基于 nichels 的点对点发送和接收支持。我们还将为 RPpC 框架添加原生 GPU 支持，这样你就可以无缝地通过 RPC 发送和接收 GPU
    张量。我们将为分布式模型并行性添加一个远程模块和远程设备类型的 API，因此如果你想将一个模块或模型的一部分放在不同的主机或 GPU 上，你只需使用远程模块来实现，这将是一个不错的高级
    API。
- en: We're going to add pipeline parallelism to Pytch， so this is a very popular
    way of training models which don't fit on a single GPU。 so it' will be very useful
    for training much larger models。And then finally。 we're going to add a C&D extension
    to support third party collective communication libraries and I like to sorry。
    thank Intel for this contribution。Okay， so that was short term now。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为 Pytch 添加管道并行性，这是一种非常流行的训练模型的方法，适用于不能在单个 GPU 上运行的模型。因此，这对于训练更大模型将非常有用。最后，我们将添加
    C&D 扩展以支持第三方集体通信库，我想对此表示感谢。感谢 Intel 的贡献。好的，这就是短期的计划。
- en: if we think about more longer term， maybe a year from now or even longer what
    we are thinking about in Pyth distributed。 so we think about adding zero style
    training framework for really large model so this was like a very interesting
    paradigm that was。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑更长期的规划，也许是一年之后或更长时间，我们正在考虑在 Pyth 分布式中添加零样式训练框架，以支持真正的大型模型。这是一个非常有趣的范式。
- en: That was proposed by Microsoft and we are trying to incorporate that into Py
    distributed then were planning to add intralaopalism so this is a very interesting
    technique that was used by megatron from NviDdia to kind of train large transformer
    models then we're also planning to add like tocr support for C1D APIs。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由微软提议的，我们正在尝试将其纳入Py分布式，然后计划添加内部并行性，因此这是一种非常有趣的技术，曾被NviDdia的Megatron用于训练大型变换器模型，我们还计划为C1D
    API添加类似的支持。
- en: So today if you have like a very complex model with some collective communication
    within the model you can't really tocr that model because this is not possible
    for C1D API so we're planning to add support for that then we have like auto tuunning
    for DDP so DDP has many parameters that need to be tuned manually today for example
    like the bucket size for the buckets that I talked about so we're planning to
    add some auto tuning where users don't have to tune DP for the particular environments。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以今天如果你有一个非常复杂的模型，模型内部有一些集体通信，你无法真正对该模型进行处理，因为这对于C1D API来说是不可能的，因此我们计划添加对这一点的支持，此外我们还计划为DDP添加自动调优，因为DDP有许多参数需要手动调优，例如我提到的桶的大小，因此我们计划添加一些自动调优功能，用户不必为特定环境调优DP。
- en: Then we have an idea called hybrid parallelism where we're planning to make
    sure that things like pipeline parallelism。 modism， data parallelism， and things
    like even intralaopalism work together seamlessly so users can kind of mix and
    match and figure out what's the best training paradigm for them。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有一个叫做混合并行性的想法，我们计划确保像管道并行性、模型并行性、数据并行性以及内部并行性能够无缝协作，以便用户可以灵活组合，找出最适合他们的训练范式。
- en: And then the next step to that is like once you have hybrid parallelism。 can
    we kind of automate this completely in terms of like the user just gives us a
    model and their training resources and then we figure out what combination of
    hybrid parallelism kind of works best for this model and the user doesn't have
    to worry about this。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，一旦你有了混合并行性，我们是否可以完全自动化这一过程，用户只需提供模型和训练资源，然后我们就可以找出最适合该模型的混合并行性组合，用户无需担心。
- en: '![](img/8bb021252445710b64cf368008f50dc4_9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bb021252445710b64cf368008f50dc4_9.png)'
- en: Finally， I would like to share this page of like distributed overview on Pyth。
    so it's a place where you have all the information for Pyths distributed。 I'll
    definitely recommend that you check this out。![](img/8bb021252445710b64cf368008f50dc4_11.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想分享这页关于Pyth分布式概述的页面。这是一个包含Pyth分布式所有信息的地方。我一定推荐你去查看一下。![](img/8bb021252445710b64cf368008f50dc4_11.png)
- en: 🎼That's all I had。 Thank you very much， and thank you for watching。![](img/8bb021252445710b64cf368008f50dc4_13.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 🎼这就是我所要说的。非常感谢你们的观看！![](img/8bb021252445710b64cf368008f50dc4_13.png)
