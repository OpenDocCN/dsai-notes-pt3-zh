- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- çº¿æ€§å›å½’
    - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- çº¿æ€§å›å½’
    - ShowMeAI - BV1wS4y1f7z1
- en: Hiï¼Œ everybodyã€‚ Welcome to a new tutorialã€‚ This is the second video of the machine
    learning from Sc tutorial seriesã€‚ In this seriesï¼Œ we are going to implement popular
    machine learning algorithms using only built and Python modules and nuyã€‚ Todayï¼Œ
    we are going to implement the linear regression algorithmã€‚ So let's talk about
    the concept of linear regression firstã€‚ So in regressionã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°æ–°çš„æ•™ç¨‹ã€‚è¿™æ˜¯æœºå™¨å­¦ä¹ ç³»åˆ—æ•™ç¨‹çš„ç¬¬äºŒä¸ªè§†é¢‘ã€‚åœ¨è¿™ä¸ªç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨å†…ç½®å’ŒPythonæ¨¡å—ä»¥åŠNumpyå®ç°æµè¡Œçš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†å®ç°çº¿æ€§å›å½’ç®—æ³•ã€‚æ‰€ä»¥è®©æˆ‘ä»¬é¦–å…ˆè°ˆè°ˆçº¿æ€§å›å½’çš„æ¦‚å¿µã€‚åœ¨å›å½’ä¸­ã€‚
- en: we want to predict continuous valuesï¼Œ whereas in classificationã€‚ we want to
    predict a discrete value like a class label 0 or oneã€‚ So if we have a look at
    this example plotã€‚ Then we have our dataï¼Œ the blue dotsã€‚ and we want to approximate
    this data with a linear functionã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦é¢„æµ‹è¿ç»­å€¼ï¼Œè€Œåœ¨åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦é¢„æµ‹ä¸€ä¸ªç¦»æ•£å€¼ï¼Œæ¯”å¦‚ç±»æ ‡ç­¾0æˆ–1ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªç¤ºä¾‹å›¾ï¼Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„æ•°æ®ï¼Œè“ç‚¹ã€‚æˆ‘ä»¬æƒ³ç”¨ä¸€ä¸ªçº¿æ€§å‡½æ•°æ¥é€¼è¿‘è¿™äº›æ•°æ®ã€‚
- en: that's why it's called linear regressionã€‚ So we use a linear function to predict
    the valuesã€‚ğŸ˜Šã€‚So we can define the approximation as y hat equals w times x plus
    Bã€‚ So this is the line equation where W or our weights is the slope and B is the
    bias or just the shift along the Y x's in the2 D caseã€‚So this is the approximationã€‚
    And now we have to come up with this W and the Bã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå«åšçº¿æ€§å›å½’ã€‚æˆ‘ä»¬ä½¿ç”¨çº¿æ€§å‡½æ•°æ¥é¢„æµ‹å€¼ã€‚ğŸ˜Šã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†é€¼è¿‘å®šä¹‰ä¸ºyå¸½ç­‰äºwä¹˜ä»¥xåŠ Bã€‚è¿™æ˜¯ç›´çº¿æ–¹ç¨‹ï¼Œå…¶ä¸­Wæˆ–æˆ‘ä»¬çš„æƒé‡æ˜¯æ–œç‡ï¼ŒBæ˜¯åå·®æˆ–ä»…æ˜¯2Dæƒ…å†µä¸‹æ²¿Yè½´çš„å¹³ç§»ã€‚å› æ­¤ï¼Œè¿™æ˜¯é€¼è¿‘ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦å¾—å‡ºè¿™ä¸ªWå’ŒBã€‚
- en: And how do we find thisã€‚So for thisï¼Œ we define a cost functionã€‚ And in linear
    regressionã€‚ this is the mean squared errorã€‚ So this is theã€‚Diffenceã€‚Between the
    actual value and the approximated valueã€‚ So the actual value for thisã€‚ we need
    training samplesã€‚And then we square this difference and sum over all the samples
    and then divide by the number of samplesã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°è¿™ä¸ªå‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæˆæœ¬å‡½æ•°ã€‚åœ¨çº¿æ€§å›å½’ä¸­ï¼Œè¿™å°±æ˜¯å‡æ–¹è¯¯å·®ã€‚è¿™æ˜¯å®é™…å€¼ä¸é€¼è¿‘å€¼ä¹‹é—´çš„å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒæ ·æœ¬æ¥è·å–è¿™ä¸ªå®é™…å€¼ã€‚ç„¶åæˆ‘ä»¬å¹³æ–¹è¿™ä¸ªå·®å¼‚ï¼Œå¯¹æ‰€æœ‰æ ·æœ¬æ±‚å’Œï¼Œç„¶åé™¤ä»¥æ ·æœ¬æ•°é‡ã€‚
- en: So this wayï¼Œ we get the mean errorã€‚This is the cost function of theï¼Œ so this
    is the errorã€‚ Andã€‚ of courseï¼Œ we want to have the error as small as possibleã€‚
    So we have to find the minimum of this functionã€‚ And how do we find the minimumã€‚
    So for thisã€‚ we need to calculate the derivative or the gradientã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¹³å‡è¯¯å·®ã€‚è¿™æ˜¯æˆæœ¬å‡½æ•°ï¼Œæ‰€ä»¥è¿™æ˜¯è¯¯å·®ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¸Œæœ›è¯¯å·®å°½å¯èƒ½å°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°æœ€å°å€¼å‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å¯¼æ•°æˆ–æ¢¯åº¦ã€‚
- en: So we calculate the gradient of our cost function with respect to W and with
    respect to Bã€‚So this is the formula of the gradientã€‚ Please check this for yourselfã€‚
    and I will also put some links in the description with some further readingsã€‚
    but I will not go into detail nowã€‚And now with this gradientã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬è®¡ç®—æˆ‘ä»¬çš„æˆæœ¬å‡½æ•°å…³äºWå’ŒBçš„æ¢¯åº¦ã€‚è¿™æ˜¯æ¢¯åº¦çš„å…¬å¼ã€‚è¯·è‡ªè¡Œæ£€æŸ¥ï¼Œæˆ‘ä¹Ÿä¼šåœ¨æè¿°ä¸­æ”¾ä¸€äº›é“¾æ¥ä»¥ä¾›è¿›ä¸€æ­¥é˜…è¯»ï¼Œä½†æˆ‘ç°åœ¨ä¸æ‰“ç®—è¯¦ç»†è®²è§£ã€‚ç°åœ¨æœ‰äº†è¿™ä¸ªæ¢¯åº¦ã€‚
- en: we use a technique that is called gradient descentã€‚ So this is an iterative
    method to get to the minimumã€‚ So if we have our object or our cost function hereï¼Œ
    then we start somewhereã€‚ So we have some initialization of the weights and the
    biasã€‚And then we want to go into the direction of the steepest descentã€‚And the
    steepest de is also the gradientã€‚ So we want to go into the direction of theã€‚
    into the negative direction of the gradientã€‚And we do this iteratively until we
    finally reach the minimumã€‚And with each iterationï¼Œ we have a update rule for the
    new weights and new biasã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸€ç§å«åšæ¢¯åº¦ä¸‹é™çš„æŠ€æœ¯ã€‚è¿™æ˜¯ä¸€ç§è¿­ä»£æ–¹æ³•ï¼Œç”¨äºè¾¾åˆ°æœ€å°å€¼ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œæœ‰æˆ‘ä»¬çš„ç›®æ ‡æˆ–æˆæœ¬å‡½æ•°ï¼Œæˆ‘ä»¬å°±ä»æŸä¸ªåœ°æ–¹å¼€å§‹ã€‚æˆ‘ä»¬åˆå§‹åŒ–æƒé‡å’Œåå·®ï¼Œç„¶åæˆ‘ä»¬æƒ³æœç€æœ€é™¡çš„ä¸‹é™æ–¹å‘å‰è¿›ã€‚è€Œæœ€é™¡çš„ä¸‹é™ä¹Ÿæ˜¯æ¢¯åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³æ²¿ç€æ¢¯åº¦çš„è´Ÿæ–¹å‘å‰è¿›ã€‚æˆ‘ä»¬ä¼šè¿­ä»£è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æœ€ç»ˆè¾¾åˆ°æœ€å°å€¼ã€‚æ¯æ¬¡è¿­ä»£æ—¶ï¼Œæˆ‘ä»¬éƒ½æœ‰æ›´æ–°è§„åˆ™æ¥è®¡ç®—æ–°çš„æƒé‡å’Œæ–°çš„åå·®ã€‚
- en: So the new W is the old W minus alpha times the derivativeã€‚ğŸ˜Šã€‚So minus because
    we want to go into the negative directionã€‚And then this alpha is the so-called
    learning rateã€‚ And this is an important parameter for our modelã€‚ So the learning
    rate defines how far we go how far we go into this direction with each iteration
    stepã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ–°çš„Wæ˜¯æ—§çš„Wå‡å»alphaä¹˜ä»¥å¯¼æ•°ã€‚ğŸ˜Šã€‚æ‰€ä»¥å‡å»æ˜¯å› ä¸ºæˆ‘ä»¬æƒ³è¿›å…¥è´Ÿæ–¹å‘ã€‚ç„¶åè¿™ä¸ªalphaæ˜¯æ‰€è°“çš„å­¦ä¹ ç‡ã€‚è¿™æ˜¯æˆ‘ä»¬æ¨¡å‹çš„ä¸€ä¸ªé‡è¦å‚æ•°ã€‚å› æ­¤ï¼Œå­¦ä¹ ç‡å®šä¹‰äº†æˆ‘ä»¬æ¯æ¬¡è¿­ä»£æ­¥é•¿ä¸­å‰è¿›çš„è·ç¦»ã€‚
- en: Soï¼Œ for exampleï¼Œ if we use a small learning rateï¼Œ then it may take longerã€‚ but
    it can finally reach the minimumï¼Œ And if we reach or if we use a big learning
    rateã€‚ then it might be fasterï¼Œ but it might also jump around like this and never
    find the minimumã€‚ So this is an important parameter that we have to specifyã€‚ and
    please keep that in mindã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œé‚£ä¹ˆå¯èƒ½éœ€è¦æ›´é•¿çš„æ—¶é—´ã€‚ä½†å®ƒæœ€ç»ˆå¯ä»¥è¾¾åˆ°æœ€å°å€¼ã€‚å¦‚æœæˆ‘ä»¬è¾¾åˆ°ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡ã€‚é‚£ä¹ˆå¯èƒ½ä¼šæ›´å¿«ï¼Œä½†ä¹Ÿå¯èƒ½ä¼šåƒè¿™æ ·å››å¤„è·³åŠ¨ï¼Œæ°¸è¿œæ‰¾ä¸åˆ°æœ€å°å€¼ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šã€‚è¯·è®°ä½è¿™ä¸€ç‚¹ã€‚
- en: So now that we know our update rulesï¼Œ I've written the formulas for the derivatives
    again here andã€‚Then simplify them a little bitã€‚ Please check that for yourselfã€‚
    So these are the formulas for the update rules and the derivativesã€‚ And this is
    all we need to knowã€‚ So now we can get startedã€‚ So now let's define a class called
    linear regressionã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†æ›´æ–°è§„åˆ™ï¼Œæˆ‘åœ¨è¿™é‡Œå†æ¬¡å†™ä¸‹äº†å¯¼æ•°çš„å…¬å¼ã€‚ç„¶åç¨å¾®ç®€åŒ–äº†ä¸€ä¸‹ã€‚è¯·ä½ è‡ªå·±æ£€æŸ¥ä¸€ä¸‹ã€‚è¿™äº›æ˜¯æ›´æ–°è§„åˆ™å’Œå¯¼æ•°çš„å…¬å¼ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦çŸ¥é“çš„ä¸€åˆ‡ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ã€‚ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåä¸ºçº¿æ€§å›å½’çš„ç±»ã€‚
- en: '![](img/ce4667748848c4687cace578f8ebabf6_1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4667748848c4687cace578f8ebabf6_1.png)'
- en: And this willï¼Œ of courseï¼Œ has an in it methodã€‚Or double underscore score in
    itã€‚And then it has selfã€‚ and it gets the learning rateã€‚ And I will give this a
    default valueã€‚ So usually this is a very small valueã€‚ So I will give it0001ã€‚ And
    then I will give it a number of iterationsã€‚ So how many iterations we use in our
    gradient decent methodã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè¿™é‡Œé¢ä¼šæœ‰ä¸€ä¸ªæ–¹æ³•ã€‚æˆ–è€…åŒä¸‹åˆ’çº¿åˆ†æ•°åœ¨å…¶ä¸­ã€‚ç„¶åå®ƒæœ‰selfï¼Œå¹¶ä¸”è·å–å­¦ä¹ ç‡ã€‚æˆ‘å°†ç»™è¿™ä¸ªä¸€ä¸ªé»˜è®¤å€¼ã€‚æ‰€ä»¥é€šå¸¸è¿™æ˜¯ä¸€ä¸ªéå¸¸å°çš„å€¼ã€‚å› æ­¤æˆ‘å°†å…¶è®¾ä¸º`0001`ã€‚ç„¶åæˆ‘å°†ç»™å®ƒä¸€ä¸ªè¿­ä»£æ¬¡æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­ä½¿ç”¨å¤šå°‘æ¬¡è¿­ä»£ã€‚
- en: and I will also give this a default valueã€‚ So I will say this is 1000ã€‚ And then
    I will simply store them hereã€‚ So I will say self L R equals L R and selfã€‚Andã€‚Itders
    equals and itsã€‚ And then laterï¼Œ we have to come up with the weightsã€‚ But here
    at the beginningï¼Œ I will simply say self weights equals none and self dot bias
    equals noneã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜ä¼šç»™è¿™ä¸ªä¸€ä¸ªé»˜è®¤å€¼ã€‚æˆ‘ä¼šè¯´è¿™æ˜¯`1000`ã€‚ç„¶åæˆ‘ä¼šç®€å•åœ°å°†å®ƒä»¬å­˜å‚¨åœ¨è¿™é‡Œã€‚å› æ­¤æˆ‘ä¼šè¯´self L Rç­‰äºL Rå’Œselfã€‚Itdersç­‰äºå®ƒçš„ã€‚ç„¶åç¨åï¼Œæˆ‘ä»¬å¿…é¡»å¾—å‡ºæƒé‡ã€‚ä½†æ˜¯åœ¨è¿™é‡Œä¸€å¼€å§‹ï¼Œæˆ‘ä¼šç®€å•åœ°è¯´self
    weightsç­‰äºnoneï¼Œself dot biasç­‰äºnoneã€‚
- en: And then we have to define two functionsï¼Œ and we will follow the conventions
    of other machine learning libraries hereã€‚ So we will define a fit methodï¼Œ which
    takes the training samples and the labels for themã€‚ So this will involve the training
    step and the gradient descentã€‚And then we will define a predict methodã€‚So then
    when it gets newã€‚Test samplesã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¿…é¡»å®šä¹‰ä¸¤ä¸ªå‡½æ•°ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†åœ¨è¿™é‡Œéµå¾ªå…¶ä»–æœºå™¨å­¦ä¹ åº“çš„æƒ¯ä¾‹ã€‚å› æ­¤æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªfitæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¥æ”¶è®­ç»ƒæ ·æœ¬åŠå…¶æ ‡ç­¾ã€‚è¿™å°†æ¶‰åŠè®­ç»ƒæ­¥éª¤å’Œæ¢¯åº¦ä¸‹é™ã€‚ç„¶åæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªpredictæ–¹æ³•ã€‚å› æ­¤ï¼Œå½“å®ƒè·å–æ–°çš„æµ‹è¯•æ ·æœ¬æ—¶ã€‚
- en: then it can approximate the value and return the valueã€‚ So these are the functions
    we have to implementã€‚ And before we go onã€‚ let's have a quick look at the data
    X and Yã€‚ So how does this lookã€‚And for thisã€‚ I've written a little example scriptï¼Œ
    and I used the psychic learn module to generate some example dataã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒå¯ä»¥è¿‘ä¼¼è¿™ä¸ªå€¼å¹¶è¿”å›è¯¥å€¼ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦å®ç°çš„å‡½æ•°ã€‚åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹æ•°æ®Xå’ŒYã€‚è¿™çœ‹èµ·æ¥å¦‚ä½•ã€‚ä¸ºæ­¤ï¼Œæˆ‘å†™äº†ä¸€ä¸ªå°ç¤ºä¾‹è„šæœ¬ï¼Œå¹¶ä½¿ç”¨äº†psychic
    learnæ¨¡å—ç”Ÿæˆäº†ä¸€äº›ç¤ºä¾‹æ•°æ®ã€‚
- en: and I will split the data into training and test samples and also training and
    test labelsã€‚ So first of allï¼Œ let's have a look at how this data looksã€‚![](img/ce4667748848c4687cace578f8ebabf6_3.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šå°†æ•°æ®åˆ†æˆè®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬ï¼Œä»¥åŠè®­ç»ƒæ ‡ç­¾å’Œæµ‹è¯•æ ‡ç­¾ã€‚å› æ­¤é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›æ•°æ®çš„æ ·å­ã€‚![](img/ce4667748848c4687cace578f8ebabf6_3.png)
- en: So here is the plotã€‚ So this is how our data looksã€‚ And now we want to find
    a function somewhere here that approximates the valueã€‚![](img/ce4667748848c4687cace578f8ebabf6_5.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œæ˜¯å›¾è¡¨ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„æ•°æ®çœ‹èµ·æ¥çš„æ ·å­ã€‚ç°åœ¨æˆ‘ä»¬æƒ³åœ¨è¿™é‡Œæ‰¾åˆ°ä¸€ä¸ªå‡½æ•°æ¥è¿‘ä¼¼è¿™ä¸ªå€¼ã€‚![](img/ce4667748848c4687cace578f8ebabf6_5.png)
- en: And let's have a look at theã€‚Shape of our x and yã€‚ So let's run thisã€‚ And I
    don't want this plot here anymoreã€‚å—¯ã€‚So if we see that our x is a N D array of
    size 80 by1ã€‚ And this is because I put in hereï¼Œ I want to have 100 samplesã€‚And
    one feature for each sampleã€‚ And then I will split thisã€‚ So our training samples
    only has 80 samples in itã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬ x å’Œ y çš„å½¢çŠ¶ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚æˆ‘ä¸æƒ³å†çœ‹åˆ°è¿™ä¸ªå›¾äº†ã€‚å—¯ã€‚å¦‚æœæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„ x æ˜¯ä¸€ä¸ªå¤§å°ä¸º 80Ã—1 çš„ N D æ•°ç»„ã€‚è¿™æ˜¯å› ä¸ºæˆ‘åœ¨è¿™é‡Œè¾“å…¥ï¼Œæˆ‘æƒ³è¦
    100 ä¸ªæ ·æœ¬ã€‚æ¯ä¸ªæ ·æœ¬ä¸€ä¸ªç‰¹å¾ã€‚ç„¶åæˆ‘ä¼šæ‹†åˆ†è¿™ä¸ªã€‚æ‰€ä»¥æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬åªæœ‰ 80 ä¸ªæ ·æœ¬ã€‚
- en: So this is a ND array of size 80 by one and our training labels is just a 1
    D vector of also of size 80ã€‚ So for each training sampleï¼Œ we have one valueã€‚So
    this is how our data looksã€‚ And now let's continueã€‚ So let's implement the fit
    methodã€‚ Soï¼Œ as I saidã€‚ we need to implement the gradient decentcent method hereã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¤§å°ä¸º 80Ã—1 çš„ N D æ•°ç»„ï¼Œè€Œæˆ‘ä»¬çš„è®­ç»ƒæ ‡ç­¾åªæ˜¯ä¸€ä¸ªå¤§å°ä¸º 80 çš„ 1 D å‘é‡ã€‚æ‰€ä»¥å¯¹äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå€¼ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„æ•°æ®çš„æ ·å­ã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å®ç°
    fit æ–¹æ³•ã€‚æ­£å¦‚æˆ‘æ‰€è¯´ã€‚æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œå®ç°æ¢¯åº¦ä¸‹é™æ³•ã€‚
- en: and the gradient decent always needs to start somewhereã€‚ So we need to have
    some initializationã€‚![](img/ce4667748848c4687cace578f8ebabf6_7.png)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ€»æ˜¯éœ€è¦ä»æŸå¤„å¼€å§‹ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€äº›åˆå§‹åŒ–ã€‚![](img/ce4667748848c4687cace578f8ebabf6_7.png)
- en: '![](img/ce4667748848c4687cace578f8ebabf6_8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4667748848c4687cace578f8ebabf6_8.png)'
- en: Soã€‚Let's doã€‚ let's in it our parametersã€‚ And for this firstã€‚ let's get the number
    of samples and the number of featuresã€‚ We can get this by saying this is x dot
    shapeã€‚And then we simply initialize all the weights with0ã€‚ So we can say self
    weights equals nuy0sã€‚Of size and featureã€‚And featuresã€‚ So for each componentã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ã€‚è®©æˆ‘ä»¬æ¥åšã€‚è®©æˆ‘ä»¬åˆå§‹åŒ–æˆ‘ä»¬çš„å‚æ•°ã€‚å¯¹äºè¿™ä¸€éƒ¨åˆ†ã€‚è®©æˆ‘ä»¬å¾—åˆ°æ ·æœ¬çš„æ•°é‡å’Œç‰¹å¾çš„æ•°é‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯´è¿™æ˜¯ x.dot.shape æ¥è·å–ã€‚ç„¶åæˆ‘ä»¬ç®€å•åœ°å°†æ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸º
    0ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´ self.weights ç­‰äº nuy0sã€‚å¤§å°å’Œç‰¹å¾ã€‚å¯¹äºæ¯ä¸ªåˆ†é‡ã€‚
- en: we put in a0 and self du bias equals 0ã€‚ This is just a valueã€‚ So you can also
    use random values hereï¼Œ but 0 is just fineã€‚So let's use 0 hereã€‚ and then we use
    the gradient descentã€‚ So this is an iterative processã€‚ So we use a fall loopã€‚
    So for Iï¼Œ oh actuallyï¼Œ we don't need thisã€‚ So for underscore in range and then
    self dot and itsã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¾“å…¥ a0 å’Œ self.du.bias ç­‰äº 0ã€‚è¿™åªæ˜¯ä¸€ä¸ªå€¼ã€‚æ‰€ä»¥ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨éšæœºå€¼ï¼Œä½† 0 å°±å¾ˆå¥½ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ 0ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€‚è¿™æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¾ªç¯ã€‚æ‰€ä»¥å¯¹äº
    Iï¼Œå“¦ï¼Œå®é™…ä¸Šï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªã€‚æ‰€ä»¥å¯¹äºä¸‹åˆ’çº¿åœ¨èŒƒå›´å†…ï¼Œç„¶å self.dot å’Œå®ƒçš„ã€‚
- en: And nowï¼Œ what we needã€‚We first have to approximateã€‚Or let's have a look at the
    formula againã€‚![](img/ce4667748848c4687cace578f8ebabf6_10.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯ä»€ä¹ˆã€‚æˆ‘ä»¬é¦–å…ˆå¿…é¡»è¿›è¡Œè¿‘ä¼¼ã€‚æˆ–è€…è®©æˆ‘ä»¬å†çœ‹ä¸€ä¸‹å…¬å¼ã€‚![](img/ce4667748848c4687cace578f8ebabf6_10.png)
- en: Soï¼Œ the formula for ourã€‚And new weights is the old weight minus the learning
    rate times the derivativeã€‚And the derivative with respect to W is one over nã€‚And
    then we have the sum and the sum over two times X Iã€‚Timesï¼Œ and thenã€‚The difference
    here of this approximated and the actual valuesã€‚ So let's firstã€‚Firstã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„å…¬å¼ã€‚æ–°çš„æƒé‡æ˜¯æ—§çš„æƒé‡å‡å»å­¦ä¹ ç‡ä¹˜ä»¥å¯¼æ•°ã€‚å…³äº W çš„å¯¼æ•°æ˜¯ 1 over nã€‚ç„¶åæˆ‘ä»¬æœ‰æ€»å’Œï¼Œå’Œæ€»å’Œä¸ºä¸¤ä¸ª X I çš„ä¹˜ç§¯ã€‚ç„¶åï¼Œè¿™ä¸ªè¿‘ä¼¼å€¼å’Œå®é™…å€¼ä¹‹é—´çš„å·®å¼‚ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å…ˆã€‚é¦–å…ˆã€‚
- en: calculate this approximationã€‚ So we have this formula hereã€‚The approximation
    isã€‚The weights times our x plus the biasã€‚![](img/ce4667748848c4687cace578f8ebabf6_12.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¿™ä¸ªè¿‘ä¼¼å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰è¿™ä¸ªå…¬å¼ã€‚è¿‘ä¼¼å€¼æ˜¯ã€‚æƒé‡ä¹˜ä»¥æˆ‘ä»¬çš„ x åŠ ä¸Šåç½®ã€‚![](img/ce4667748848c4687cace578f8ebabf6_12.png)
- en: So let's do thisã€‚ Let's sayï¼Œ and let's call this whyã€‚Predic it equalsã€‚ And then
    we can use N dot dotã€‚And then x and self dot weightsã€‚Plusã€‚Bsï¼Œ so this will multiply
    the x with the weightsã€‚And now that we have the approximationï¼Œ we can calculate
    the derivative with respect to Wã€‚ And this isã€‚ let's againï¼Œ have a look at this
    formulaã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬è¿™æ ·åšã€‚å‡è®¾æˆ‘ä»¬å«å®ƒ yã€‚é¢„æµ‹ç­‰äºã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ N ç‚¹ç‚¹ã€‚ç„¶å x å’Œ self ç‚¹æƒé‡ã€‚åŠ ä¸Š Bsï¼Œè¿™å°†æŠŠ x ä¸æƒé‡ç›¸ä¹˜ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿‘ä¼¼å€¼ï¼Œå¯ä»¥è®¡ç®—å…³äº
    W çš„å¯¼æ•°ã€‚è¿™æ˜¯ã€‚è®©æˆ‘ä»¬å†çœ‹çœ‹è¿™ä¸ªå…¬å¼ã€‚
- en: '![](img/ce4667748848c4687cace578f8ebabf6_14.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4667748848c4687cace578f8ebabf6_14.png)'
- en: 1 over nã€‚And thenã€‚The sumã€‚ and then inside the sumï¼Œ we have the product of x
    times thisã€‚Soï¼Œ we sayã€‚![](img/ce4667748848c4687cace578f8ebabf6_16.png)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 1 over nã€‚ç„¶åã€‚æ€»å’Œã€‚ç„¶ååœ¨æ€»å’Œå†…éƒ¨ï¼Œæˆ‘ä»¬æœ‰ x ä¹˜ä»¥è¿™ä¸ªçš„ä¹˜ç§¯ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬è¯´ã€‚![](img/ce4667748848c4687cace578f8ebabf6_16.png)
- en: 1 overã€‚And samplesã€‚ So we already got the number of samples hereã€‚And then timesã€‚
    And then we have the sum productã€‚ So this is nothing elseï¼Œ but also the dot productã€‚
    So N P dot dotã€‚ But now we have to be carefulã€‚ So what we did hereã€‚å—¯ã€‚Here we multiply
    each weight component with the feature vector component and sum it upã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 1 overã€‚æ ·æœ¬ã€‚æˆ‘ä»¬å·²ç»å¾—åˆ°äº†æ ·æœ¬çš„æ•°é‡ã€‚ç„¶åä¹˜ä»¥ã€‚ç„¶åæˆ‘ä»¬æœ‰äº†æ€»ä¹˜ç§¯ã€‚æ‰€ä»¥è¿™æ²¡åˆ«çš„ï¼Œä¹Ÿå°±æ˜¯ç‚¹ç§¯ã€‚æ‰€ä»¥ N P ç‚¹ç‚¹ã€‚ä½†ç°åœ¨æˆ‘ä»¬å¾—å°å¿ƒã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œåšäº†ä»€ä¹ˆã€‚å—¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæƒé‡åˆ†é‡ä¸ç‰¹å¾å‘é‡åˆ†é‡ç›¸ä¹˜å¹¶æ±‚å’Œã€‚
- en: And we do this for all samples and then get one value for each sampleã€‚And here
    we want to get one value for each feature vector componentã€‚ So we multiply each
    sample with a predicted value and sum it upã€‚Soã€‚ and then we do this for each feature
    vector component and get one value for each componentã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œè¿™ä¸ªæ“ä½œï¼Œç„¶åä¸ºæ¯ä¸ªæ ·æœ¬å¾—åˆ°ä¸€ä¸ªå€¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºæ¯ä¸ªç‰¹å¾å‘é‡ç»„ä»¶å¾—åˆ°ä¸€ä¸ªå€¼ã€‚æ‰€ä»¥æˆ‘ä»¬å°†æ¯ä¸ªæ ·æœ¬ä¸é¢„æµ‹å€¼ç›¸ä¹˜å¹¶æ±‚å’Œã€‚æ‰€ä»¥ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªç‰¹å¾å‘é‡ç»„ä»¶è¿™æ ·åšï¼Œå¹¶ä¸ºæ¯ä¸ªç»„ä»¶å¾—åˆ°ä¸€ä¸ªå€¼ã€‚
- en: So this is the other way roundã€‚ So this is along the other axisã€‚ And we canã€‚
    So we have to use x dot transposed hereï¼Œ x dot Tã€‚And this is the dot product of
    the transposed Xã€‚ And then we have y predicted minus the actual yã€‚ So please check
    the nuy dot function for yourselfã€‚Soï¼Œ this is theã€‚The of the W and the derivative
    of the bias is also oneã€‚ Or againã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯ç›¸åçš„æ–¹å‘ã€‚æ²¿ç€å¦ä¸€ä¸ªè½´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œä½¿ç”¨xçš„è½¬ç½®ï¼Œx dot Tã€‚è¿™æ˜¯è½¬ç½®Xçš„ç‚¹ç§¯ã€‚ç„¶åæˆ‘ä»¬æœ‰é¢„æµ‹çš„yå‡å»å®é™…çš„yã€‚æ‰€ä»¥è¯·è‡ªå·±æ£€æŸ¥nuy
    dotå‡½æ•°ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯Wçš„å’Œåå·®çš„å¯¼æ•°ä¹Ÿæ˜¯1ã€‚æˆ–è€…å†æ¬¡ã€‚
- en: let's have a look at the formulaã€‚ So this is the sameã€‚ except that we don't
    have the x hereã€‚ So this is one over nã€‚ and then just the sumã€‚![](img/ce4667748848c4687cace578f8ebabf6_18.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå…¬å¼ã€‚æ‰€ä»¥è¿™å°±æ˜¯ç›¸åŒçš„ã€‚åªæ˜¯è¿™é‡Œæ²¡æœ‰xã€‚æ‰€ä»¥è¿™æ˜¯1/nï¼Œç„¶åå°±æ˜¯æ±‚å’Œã€‚![](img/ce4667748848c4687cace578f8ebabf6_18.png)
- en: Of this differenceã€‚ And by the wayï¼Œ Iã€‚I left the two outã€‚ So this is just a
    scaling factor that we can omitã€‚So here one over n and then the sum over the differenceã€‚![](img/ce4667748848c4687cace578f8ebabf6_20.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå·®å¼‚ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ã€‚æˆ‘æŠŠä¸¤ä¸ªçœç•¥æ‰äº†ã€‚æ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªå¯ä»¥å¿½ç•¥çš„ç¼©æ”¾å› å­ã€‚æ‰€ä»¥è¿™é‡Œ1/nï¼Œç„¶åæ˜¯å·®å¼‚çš„å’Œã€‚![](img/ce4667748848c4687cace578f8ebabf6_20.png)
- en: So againï¼Œ one over number of samplesã€‚ And then we can say nuy dot sum and the
    sum of y predicted minus actual yã€‚So this areï¼Œ these are our derivativesã€‚ And
    now we update our weightsã€‚ So we say self weights minus equals self dot learning
    rate times thisã€‚Divativeã€‚And selfã€‚ the bias equals selfã€‚Minus equals self dot
    learning rate timesã€‚The derivativeã€‚Andã€‚Yeahã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å†ä¸€æ¬¡ï¼Œæ ·æœ¬çš„å€’æ•°ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´nuy dot sumå’Œé¢„æµ‹çš„yå‡å»å®é™…çš„yçš„å’Œã€‚æ‰€ä»¥è¿™äº›æ˜¯æˆ‘ä»¬çš„å¯¼æ•°ã€‚ç°åœ¨æˆ‘ä»¬æ›´æ–°æˆ‘ä»¬çš„æƒé‡ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´self
    weightså‡å»ç­‰äºself dotå­¦ä¹ ç‡ä¹˜ä»¥è¿™ä¸ªå¯¼æ•°ã€‚å¹¶ä¸”selfçš„åå·®ç­‰äºselfã€‚å‡å»ç­‰äºself dotå­¦ä¹ ç‡ä¹˜ä»¥å¯¼æ•°ã€‚è¿˜æœ‰ã€‚æ˜¯çš„ã€‚
- en: so this is the gradient descentã€‚And now we need the predict methodã€‚ Soï¼Œ againï¼Œ
    we approximateã€‚![](img/ce4667748848c4687cace578f8ebabf6_22.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯æ¢¯åº¦ä¸‹é™ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦é¢„æµ‹æ–¹æ³•ã€‚æ‰€ä»¥ï¼Œå†ä¸€æ¬¡ï¼Œæˆ‘ä»¬è¿‘ä¼¼ã€‚![](img/ce4667748848c4687cace578f8ebabf6_22.png)
- en: We approximate the values with this formulaï¼Œ soã€‚![](img/ce4667748848c4687cace578f8ebabf6_24.png)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨è¿™ä¸ªå…¬å¼è¿‘ä¼¼å€¼ï¼Œæ‰€ä»¥ã€‚![](img/ce4667748848c4687cace578f8ebabf6_24.png)
- en: We already have this hereã€‚ So this isã€‚The dot product of x and the weightsï¼Œ
    plus the buyersã€‚And thenã€‚ we simply returned thisã€‚So this is the whole implementation
    that we needï¼Œ andã€‚I forgot to import Nyã€‚ of courseã€‚ So I sayï¼Œ let's sayï¼Œ import
    Ny S and P so that we can use thisã€‚And nowï¼Œ let's test thisã€‚Soã€‚å—¯ã€‚Let's import
    this classã€‚ So let's sayï¼Œ fromã€‚Linear regressionã€‚Import linear regressionã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æœ‰äº†è¿™ä¸ªã€‚æ‰€ä»¥è¿™æ˜¯ã€‚xå’Œæƒé‡çš„ç‚¹ç§¯ï¼ŒåŠ ä¸Šåå·®ã€‚ç„¶åã€‚æˆ‘ä»¬ç®€å•åœ°è¿”å›è¿™ä¸ªã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬éœ€è¦çš„æ•´ä¸ªå®ç°ï¼Œæˆ‘å¿˜è®°å¯¼å…¥Nyäº†ã€‚å½“ç„¶ã€‚æ‰€ä»¥æˆ‘è¯´ï¼Œå‡è®¾ï¼Œå¯¼å…¥Ny
    Så’ŒPï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ã€‚æ‰€ä»¥ã€‚å—¯ã€‚è®©æˆ‘ä»¬å¯¼å…¥è¿™ä¸ªç±»ã€‚æ‰€ä»¥å‡è®¾ï¼Œä»ã€‚çº¿æ€§å›å½’ã€‚å¯¼å…¥çº¿æ€§å›å½’ã€‚
- en: And then create some regresso equalsã€‚Linear regressionã€‚And then we say regressor
    dot fitã€‚ and we want to fit the training samples and the training labelsã€‚And then
    we can say we can get predicted values equalsã€‚Regress orã€‚Dot predictã€‚ And now
    we want to predict theã€‚Testã€‚å—¯ã€‚Samplesã€‚And nowã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºä¸€äº›å›å½’ç­‰äºã€‚çº¿æ€§å›å½’ã€‚ç„¶åæˆ‘ä»¬è¯´å›å½’å™¨dot fitã€‚æˆ‘ä»¬æƒ³è¦æ‹Ÿåˆè®­ç»ƒæ ·æœ¬å’Œè®­ç»ƒæ ‡ç­¾ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬å¯ä»¥å¾—åˆ°é¢„æµ‹å€¼ç­‰äºã€‚å›å½’å™¨ã€‚ç‚¹é¢„æµ‹ã€‚ç°åœ¨æˆ‘ä»¬æƒ³é¢„æµ‹ã€‚æµ‹è¯•ã€‚å—¯ã€‚æ ·æœ¬ã€‚ç°åœ¨ã€‚
- en: in order to calculate the or to see how our model performsã€‚ now we can'tã€‚ we
    can't use the accuracy measureã€‚ But here we use the mean squared errorã€‚ So as
    I saidã€‚ this is our cost functionï¼Œ the mean square error that tells us how bigã€‚![](img/ce4667748848c4687cace578f8ebabf6_26.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—æˆ–æŸ¥çœ‹æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°ã€‚ç°åœ¨æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨å‡†ç¡®æ€§åº¦é‡ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®ã€‚æ‰€ä»¥å¦‚æˆ‘æ‰€è¯´ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æˆæœ¬å‡½æ•°ï¼Œå‡æ–¹è¯¯å·®å‘Šè¯‰æˆ‘ä»¬æœ‰å¤šå¤§ã€‚![](img/ce4667748848c4687cace578f8ebabf6_26.png)
- en: The difference between the actual value and the approximated value isã€‚ so let's
    define the mean squared arrowã€‚ Let's say theef Mï¼Œ Eï¼Œ and this will getã€‚![](img/ce4667748848c4687cace578f8ebabf6_28.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…å€¼ä¸è¿‘ä¼¼å€¼ä¹‹é—´çš„å·®å¼‚ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å®šä¹‰å‡æ–¹è¯¯å·®ã€‚å‡è®¾Mï¼ŒEï¼Œè¿™å°†å¾—åˆ°ã€‚![](img/ce4667748848c4687cace578f8ebabf6_28.png)
- en: The actual valuesï¼Œ and the predicted valuesã€‚And this isã€‚The numpy hereï¼Œ we can
    use Nampy meanã€‚And thenï¼Œ simplyï¼Œ the differenceã€‚To the power of twoã€‚ So why trueã€‚Mus
    y predicted to the power of2ã€‚And then we want to return thisã€‚Soï¼Œ let's seeã€‚ let's
    sayã€‚M S E value equals MEã€‚Offã€‚Wã€‚Testã€‚And the predictedã€‚Valuesï¼Œ and let's print
    thisã€‚So now if we run thisã€‚å—¯ã€‚Seeï¼Œ it's not runningã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…å€¼å’Œé¢„æµ‹å€¼ã€‚è¿™å°±æ˜¯ã€‚è¿™é‡Œçš„numpyï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Numpyå‡å€¼ã€‚ç„¶åï¼Œç®€å•åœ°ï¼Œå·®å€¼ã€‚å¹³æ–¹ã€‚å› æ­¤ï¼Œä¸ºä»€ä¹ˆçœŸå®å€¼ã€‚æ˜¯æˆ‘çš„é¢„æµ‹å€¼çš„å¹³æ–¹ã€‚ç„¶åæˆ‘ä»¬æƒ³è¿”å›è¿™ä¸ªã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚å‡è®¾ã€‚MSEå€¼ç­‰äºMEã€‚Offã€‚Wã€‚æµ‹è¯•ã€‚å’Œé¢„æµ‹å€¼ï¼Œè®©æˆ‘ä»¬æ‰“å°è¿™ä¸ªã€‚æ‰€ä»¥ç°åœ¨å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚å—¯ã€‚çœ‹çœ‹ï¼Œå®ƒæ²¡æœ‰è¿è¡Œã€‚
- en: Bias is not definedã€‚ So let's say what weã€‚What did we miss hereï¼Œ dot biasasã€‚And
    let's run this againã€‚Soã€‚Lï¼Œ27ã€‚Ohï¼Œ sorryã€‚ I copied this and forgot this yearï¼Œ tooã€‚
    So next tryã€‚Nowã€‚ we see that ourã€‚Performanceï¼Œ so the mean squaredarrow is 783ã€‚
    So this is pretty highã€‚ So let's use another learning rate hereã€‚ So let's say
    L R equalsã€‚t0ï¼Œ1ã€‚And let's run thisã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åå·®æœªå®šä¹‰ã€‚é‚£ä¹ˆæˆ‘ä»¬è¯´æˆ‘ä»¬ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œé—æ¼äº†ä»€ä¹ˆï¼Œç‚¹åå·®ã€‚è®©æˆ‘ä»¬å†æ¬¡è¿è¡Œè¿™ä¸ªã€‚æ‰€ä»¥ã€‚Lï¼Œ27ã€‚å“¦ï¼Œå¯¹ä¸èµ·ã€‚æˆ‘å¤åˆ¶äº†è¿™ä¸ªå¹¶å¿˜è®°äº†è¿™ä¸€å¹´ã€‚æ‰€ä»¥ä¸‹ä¸€æ¬¡å°è¯•ã€‚ç°åœ¨ã€‚æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„ã€‚æ€§èƒ½ï¼Œå› æ­¤å‡æ–¹è¯¯å·®ä¸º783ã€‚è¿™ç›¸å½“é«˜ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å¦ä¸€ä¸ªå­¦ä¹ ç‡ã€‚å‡è®¾LRç­‰äºã€‚t0ï¼Œ1ã€‚ç„¶åè®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚
- en: And now we see that our arrow is smallerã€‚ And let's actually plot thisã€‚Soã€‚Let's
    plotã€‚Firstã€‚ with the original learning rateï¼Œ let's see ourã€‚How the plot looksã€‚Soï¼Œ
    nowã€‚Our plot looks like thisã€‚ So it'sã€‚![](img/ce4667748848c4687cace578f8ebabf6_30.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„è¯¯å·®æ›´å°äº†ã€‚è®©æˆ‘ä»¬å®é™…ç»˜åˆ¶è¿™ä¸ªã€‚æ‰€ä»¥ã€‚é¦–å…ˆç»˜åˆ¶ã€‚ç”¨åŸå§‹å­¦ä¹ ç‡ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„ã€‚å›¾çœ‹èµ·æ¥å¦‚ä½•ã€‚ç°åœ¨ã€‚æˆ‘ä»¬çš„å›¾çœ‹èµ·æ¥åƒè¿™æ ·ã€‚æ‰€ä»¥æ˜¯ã€‚![](img/ce4667748848c4687cace578f8ebabf6_30.png)
- en: Almost like the right slopeï¼Œ but not exactlyã€‚ And now let's have the other learning
    rateã€‚ So let's use this learning rateã€‚ and let's run thisã€‚![](img/ce4667748848c4687cace578f8ebabf6_32.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹åƒæ­£ç¡®çš„æ–œç‡ï¼Œä½†ä¸å®Œå…¨ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨å¦ä¸€ä¸ªå­¦ä¹ ç‡ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå­¦ä¹ ç‡ã€‚å¹¶è¿è¡Œè¿™ä¸ªã€‚![](img/ce4667748848c4687cace578f8ebabf6_32.png)
- en: '![](img/ce4667748848c4687cace578f8ebabf6_33.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4667748848c4687cace578f8ebabf6_33.png)'
- en: And now our plot looks like thisã€‚ and this looks pretty goodï¼Œ actuallyã€‚ So this
    is a pretty goodã€‚Fit a pretty good approximation of this data with a linear functionã€‚
    So we see that our implementation is workingã€‚ and I hope you enjoyed this tutorial
    and see you in the next tutorialã€‚ğŸ˜Šã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„å›¾çœ‹èµ·æ¥åƒè¿™æ ·ã€‚å®é™…ä¸Šè¿™çœ‹èµ·æ¥ç›¸å½“ä¸é”™ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªç›¸å½“ä¸é”™çš„ã€‚ç”¨çº¿æ€§å‡½æ•°å¯¹è¿™ä¸ªæ•°æ®çš„è‰¯å¥½æ‹Ÿåˆã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„å®ç°æ˜¯æœ‰æ•ˆçš„ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œä¸‹æ¬¡è§ã€‚ğŸ˜Š
- en: '![](img/ce4667748848c4687cace578f8ebabf6_35.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4667748848c4687cace578f8ebabf6_35.png)'
- en: '![](img/ce4667748848c4687cace578f8ebabf6_36.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4667748848c4687cace578f8ebabf6_36.png)'
- en: Byeã€‚Hiï¼Œ everybodyã€‚ Welcome to a new machine learning from scratch tutorialã€‚
    Todayã€‚ we are going to implement logistic regression using only built and Python
    modules and Nmpyã€‚ If you haven't watched my previous video about linear regressionã€‚
    I will highly recommend to watch it firstã€‚ Since I explain some of the concepts
    a little bit more in detail thereã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°æ–°çš„æœºå™¨å­¦ä¹ åŸºç¡€æ•™ç¨‹ã€‚ä»Šå¤©ã€‚æˆ‘ä»¬å°†ä»…ä½¿ç”¨æ„å»ºçš„Pythonæ¨¡å—å’ŒNumpyæ¥å®ç°é€»è¾‘å›å½’ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰è§‚çœ‹æˆ‘å…³äºçº¿æ€§å›å½’çš„å‰ä¸€ä¸ªè§†é¢‘ã€‚æˆ‘å¼ºçƒˆå»ºè®®ä½ å…ˆè§‚çœ‹ï¼Œå› ä¸ºæˆ‘åœ¨é‚£é‡Œå¯¹ä¸€äº›æ¦‚å¿µè¿›è¡Œäº†æ›´è¯¦ç»†çš„è§£é‡Šã€‚
- en: But I will try to cover all the important concepts again in this videoã€‚ So let's
    talk about the concepts of logistic regressionã€‚ğŸ˜Šï¼ŒSoã€‚As you rememberï¼Œ in linear
    regressionã€‚ we model our data with a linear function W times x plus Bã€‚So this
    will output continuous valuesã€‚And nowï¼Œ in logistic regressionï¼Œ we don't want continuous
    valuesï¼Œ but we want a probabilityã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä¼šåœ¨è¿™ä¸ªè§†é¢‘ä¸­å†æ¬¡æ¶µç›–æ‰€æœ‰é‡è¦æ¦‚å¿µã€‚æ‰€ä»¥è®©æˆ‘ä»¬è°ˆè°ˆé€»è¾‘å›å½’çš„æ¦‚å¿µã€‚ğŸ˜Šæ‰€ä»¥ã€‚å¦‚ä½ æ‰€è®°å¾—ï¼Œåœ¨çº¿æ€§å›å½’ä¸­ã€‚æˆ‘ä»¬ç”¨çº¿æ€§å‡½æ•°Wä¹˜ä»¥xåŠ Bæ¥å»ºæ¨¡æˆ‘ä»¬çš„æ•°æ®ã€‚è¿™å°†è¾“å‡ºè¿ç»­å€¼ã€‚è€Œåœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¸æƒ³è¦è¿ç»­å€¼ï¼Œè€Œæ˜¯æƒ³è¦æ¦‚ç‡ã€‚
- en: And in order to model this probabilityï¼Œ we apply the sigmite function to our
    linear modelã€‚So the sigmoid function is one over one plus the exponential function
    of minus xã€‚So this is the soid function and Min and xã€‚ in our caseï¼Œ is then our
    our linear modelã€‚Soã€‚ and this isï¼Œ this will output a probability between 0 and1ã€‚
    So if we plot the sigmoid functionã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¯¹è¿™ç§æ¦‚ç‡å»ºæ¨¡ï¼Œæˆ‘ä»¬å°†sigmoidå‡½æ•°åº”ç”¨äºæˆ‘ä»¬çš„çº¿æ€§æ¨¡å‹ã€‚sigmoidå‡½æ•°æ˜¯1é™¤ä»¥1åŠ ä¸Šè´Ÿxçš„æŒ‡æ•°å‡½æ•°ã€‚æ‰€ä»¥è¿™æ˜¯sigmoidå‡½æ•°ï¼ŒMinå’Œxã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œå°±æ˜¯æˆ‘ä»¬çš„çº¿æ€§æ¨¡å‹ã€‚æ‰€ä»¥ã€‚è¿™å°†è¾“å‡º0åˆ°1ä¹‹é—´çš„æ¦‚ç‡ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬ç»˜åˆ¶sigmoidå‡½æ•°ã€‚
- en: Then you can see this is between 0 and oneã€‚So with this functionã€‚ we can model
    a probability of our dataã€‚And nowï¼Œ with thisã€‚Approximate itã€‚Outputï¼Œ and we can
    thenã€‚We must then come up with the parameters Wã€‚ So our weight and our biasã€‚And
    how do we do thatã€‚ So againï¼Œ I already explained this in the previous videoã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°è¿™æ˜¯åœ¨0å’Œ1ä¹‹é—´ã€‚æ‰€ä»¥é€šè¿‡è¿™ä¸ªå‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æˆ‘ä»¬çš„æ•°æ®å»ºæ¨¡æ¦‚ç‡ã€‚ç°åœ¨ï¼Œé€šè¿‡è¿™ä¸ªã€‚è¿‘ä¼¼è¾“å‡ºï¼Œæˆ‘ä»¬å¿…é¡»å¾—å‡ºå‚æ•°Wã€‚å› æ­¤æˆ‘ä»¬çš„æƒé‡å’Œåå·®ã€‚æˆ‘ä»¬æ€ä¹ˆåšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥å†æ¬¡ï¼Œæˆ‘åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­å·²ç»è§£é‡Šè¿‡è¿™ä¸ªã€‚
- en: So we apply a method that is called gradient descentã€‚ So first of allï¼Œ we need
    a cost functionã€‚ And here we don't have the mean squared arrow anyã€‚ but we use
    a function that is called the cross entropyã€‚ So this is the formulaã€‚I will not
    go into detail about thisï¼Œ but I will put some further readings in the descriptionã€‚Soã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åº”ç”¨ä¸€ç§å«åšæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæˆæœ¬å‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸å†ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ä¸ªå«åšäº¤å‰ç†µçš„å‡½æ•°ã€‚è¿™å°±æ˜¯å…¬å¼ã€‚æˆ‘ä¸ä¼šè¯¦ç»†è®¨è®ºè¿™ä¸ªï¼Œä½†æˆ‘ä¼šåœ¨æè¿°ä¸­æ”¾ä¸€äº›è¿›ä¸€æ­¥é˜…è¯»çš„ææ–™ã€‚
- en: with this formulaã€‚What we doï¼Œ we want to optimize this with respect to our parametersï¼Œ
    W and Bã€‚So we use gradient descentã€‚ and this means we start at some point and
    then iteratively update our parametersã€‚ So we have to calculate the derivativeã€‚And
    then go into the direction of this derivative until we finally reach the minimumã€‚Soï¼Œ
    and then we also need to define a so called learning rateã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªå…¬å¼ã€‚æˆ‘ä»¬æƒ³è¦ä¼˜åŒ–æˆ‘ä»¬çš„å‚æ•°Wå’ŒBã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä»æŸä¸ªç‚¹å¼€å§‹ï¼Œç„¶åè¿­ä»£åœ°æ›´æ–°æˆ‘ä»¬çš„å‚æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è®¡ç®—å¯¼æ•°ã€‚ç„¶åæ²¿ç€è¿™ä¸ªå¯¼æ•°çš„æ–¹å‘å‰è¿›ï¼Œç›´åˆ°æœ€ç»ˆè¾¾åˆ°æœ€å°å€¼ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®šä¹‰ä¸€ä¸ªæ‰€è°“çš„å­¦ä¹ ç‡ã€‚
- en: So the learning rate determines how far we go into this direction with each
    stepã€‚ So this is an important parameterã€‚ It shouldn't be too high since then it
    might jump around and never find the minimumã€‚ but it also shouldn't be too lowã€‚Sorryã€‚And
    now with this gradient descentã€‚What we have is we have our update rulesã€‚ So ourã€‚Our
    weight is our new weight is the old weight minusã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å­¦ä¹ ç‡å†³å®šäº†æˆ‘ä»¬åœ¨æ¯ä¸€æ­¥ä¸­å‘è¿™ä¸ªæ–¹å‘èµ°å¤šè¿œã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å‚æ•°ã€‚å®ƒä¸åº”è¯¥å¤ªé«˜ï¼Œå¦åˆ™å¯èƒ½ä¼šè·³æ¥è·³å»ï¼Œæ°¸è¿œæ‰¾ä¸åˆ°æœ€å°å€¼ï¼Œä½†ä¹Ÿä¸åº”è¯¥å¤ªä½ã€‚æŠ±æ­‰ã€‚ç°åœ¨ï¼Œåˆ©ç”¨è¿™ä¸ªæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„æ›´æ–°è§„åˆ™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–°æƒé‡æ˜¯æ—§æƒé‡å‡å»ã€‚
- en: because we want to go into the negative directionï¼Œ minus our learning rate times
    the derivativeã€‚And the same is for our biasã€‚And then here are the formulas that
    we need for our derivativesã€‚ So these are the formulasï¼Œ and they are actually
    the same derivativesï¼Œ like inã€‚Lar regressionã€‚ so you can check the math behind
    it yourselfã€‚ I will also put some links in the descriptionã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬æƒ³è¦æœè´Ÿæ–¹å‘èµ°ï¼Œå‡å»æˆ‘ä»¬çš„å­¦ä¹ ç‡ä¹˜ä»¥å¯¼æ•°ã€‚åç½®ä¹Ÿæ˜¯ä¸€æ ·ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬æ‰€éœ€çš„å¯¼æ•°å…¬å¼ã€‚è¿™äº›å…¬å¼å®é™…ä¸Šå’Œçº¿æ€§å›å½’ä¸­çš„å¯¼æ•°æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥ä½ å¯ä»¥è‡ªå·±æ£€æŸ¥èƒŒåçš„æ•°å­¦ã€‚æˆ‘ä¹Ÿä¼šåœ¨æè¿°ä¸­æ”¾ä¸€äº›é“¾æ¥ã€‚
- en: And this is all we need now to get startedï¼Œ and now we can implement ourã€‚![](img/ce4667748848c4687cace578f8ebabf6_38.png)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬ç°åœ¨éœ€è¦çš„å…¨éƒ¨ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥å¼€å§‹å®ç°æˆ‘ä»¬çš„ã€‚![](img/ce4667748848c4687cace578f8ebabf6_38.png)
