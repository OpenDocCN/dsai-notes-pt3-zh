- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P50ï¼šL9.4- è‡ªç„¶è¯­è¨€å¤„ç†å’ŒKerasçš„è¿ç§»å­¦ä¹ 
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P50ï¼šL9.4- è‡ªç„¶è¯­è¨€å¤„ç†å’ŒKerasçš„è¿ç§»å­¦ä¹ 
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ We to applications of deep neural networks with Washington
    Universityã€‚ In this videoï¼Œ we're going to look at natural language pretrained
    neural networks that you can transfer into your current project for the latest
    on my AI course and projectsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æˆ‘ä»¬ä¸åç››é¡¿å¤§å­¦ä¸€èµ·ç ”ç©¶æ·±åº¦ç¥ç»ç½‘ç»œçš„åº”ç”¨ã€‚åœ¨è¿™æ®µè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹è‡ªç„¶è¯­è¨€é¢„è®­ç»ƒç¥ç»ç½‘ç»œï¼Œæ‚¨å¯ä»¥å°†å…¶è¿ç§»åˆ°æ‚¨å½“å‰çš„é¡¹ç›®ä¸­ï¼Œäº†è§£æˆ‘æœ€æ–°çš„äººå·¥æ™ºèƒ½è¯¾ç¨‹å’Œé¡¹ç›®ã€‚
- en: click subscribe and the bell next to it to be notified of every new videoã€‚ Transfer
    learning is also commonly used with natural language processingã€‚ Now we've got
    an entire module of natural language processing coming upã€‚ this just gives us
    a brief introduction to thisã€‚ Usually what you're doing with transfer learning
    is you're adding an embedding layer to your neural networkã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»è®¢é˜…å’Œæ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚è¿ç§»å­¦ä¹ ä¹Ÿå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€‚ç°åœ¨æˆ‘ä»¬å°†æœ‰ä¸€ä¸ªå®Œæ•´çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å—å³å°†ä¸Šçº¿ã€‚è¿™åªæ˜¯ç»™æˆ‘ä»¬ä¸€ä¸ªç®€è¦çš„ä»‹ç»ã€‚é€šå¸¸ï¼Œä½¿ç”¨è¿ç§»å­¦ä¹ æ—¶ï¼Œæ‚¨æ˜¯åœ¨ç¥ç»ç½‘ç»œä¸­æ·»åŠ ä¸€ä¸ªåµŒå…¥å±‚ã€‚
- en: This basically takes raw textï¼Œ such as sentences and encodes it into some sort
    of a vector that will go into your neural networkã€‚ and that vector can be predicted
    upon because it's a numeric input its fixed lengthã€‚ whereas the normal sentences
    can have very numbers of wordsã€‚ and you have to also really think about how you
    want to actually encode yourã€‚ğŸ˜Šã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ˜¯å°†åŸå§‹æ–‡æœ¬ï¼ˆä¾‹å¦‚å¥å­ï¼‰ç¼–ç æˆæŸç§å‘é‡ï¼Œä»¥ä¾¿è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚è¿™ä¸ªå‘é‡å¯ä»¥è¢«é¢„æµ‹ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„æ•°å€¼è¾“å…¥ï¼Œè€Œæ™®é€šå¥å­çš„å•è¯æ•°é‡å¯ä»¥å˜åŒ–å¾ˆå¤§ã€‚æ‚¨è¿˜éœ€è¦ä»”ç»†è€ƒè™‘å¦‚ä½•å®é™…ç¼–ç æ‚¨çš„æ–‡æœ¬ã€‚ğŸ˜Š
- en: '![](img/ca3b1efa82105f12ffd73e17655ac6cb_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca3b1efa82105f12ffd73e17655ac6cb_1.png)'
- en: There's a variety of different embedding layers or encodersã€‚ basically models
    that you'll transfer inã€‚ and we'll see more about that in a couple of modules
    once we get to the natural language processing module for this courseã€‚ I have
    listed some of the sources here that I use to put together this pretty simple
    example showing how we can analyze the Internet movie database and try to predict
    if a review is positive or negativeã€‚ Now to make use of this exampleï¼Œ you'll need
    to have Tensorflow hub and Tensorflow datas installedã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤šç§ä¸åŒçš„åµŒå…¥å±‚æˆ–ç¼–ç å™¨ï¼ŒåŸºæœ¬ä¸Šæ˜¯æ‚¨å°†è¿ç§»çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†åœ¨å‡ ä¸ªæ¨¡å—åçœ‹åˆ°æ›´å¤šå…³äºè¿™ä¸€ç‚¹çš„å†…å®¹ï¼Œä¸€æ—¦æˆ‘ä»¬è¿›å…¥æœ¬è¯¾ç¨‹çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å—ã€‚æˆ‘åœ¨è¿™é‡Œåˆ—å‡ºäº†ä¸€äº›æˆ‘ç”¨æ¥ç»„åˆè¿™ä¸ªç®€å•ç¤ºä¾‹çš„æ¥æºï¼Œå±•ç¤ºæˆ‘ä»¬å¦‚ä½•åˆ†æäº’è”ç½‘ç”µå½±æ•°æ®åº“å¹¶å°è¯•é¢„æµ‹è¯„è®ºæ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæçš„ã€‚è¦ä½¿ç”¨è¿™ä¸ªç¤ºä¾‹ï¼Œæ‚¨éœ€è¦å®‰è£…Tensorflow
    hubå’ŒTensorflow datasã€‚
- en: you can run those two PP installsã€‚ if you're running Google coab every time
    you restart your coab you're going to have to rerun theseã€‚ I already have these
    installed on my environment I'm currently running locallyã€‚ the first thing I'm
    going to do is load in the Internet movie database reviewã€‚ So this has reviews
    for all sorts of moviesï¼Œ Some are positiveï¼Œ some are negativeã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥è¿è¡Œè¿™ä¸¤ä¸ªPPå®‰è£…ã€‚å¦‚æœæ‚¨æ¯æ¬¡é‡æ–°å¯åŠ¨Google Coabï¼Œæ‚¨éƒ½éœ€è¦é‡æ–°è¿è¡Œè¿™äº›ã€‚æˆ‘å·²ç»åœ¨æˆ‘å½“å‰çš„æœ¬åœ°ç¯å¢ƒä¸­å®‰è£…äº†è¿™äº›ã€‚æˆ‘è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯åŠ è½½äº’è”ç½‘ç”µå½±æ•°æ®åº“çš„è¯„è®ºã€‚è¿™äº›è¯„è®ºæ¶µç›–äº†å„ç§ç”µå½±ï¼Œæœ‰äº›æ˜¯ç§¯æçš„ï¼Œæœ‰äº›æ˜¯æ¶ˆæçš„ã€‚
- en: the idea is we're going to try to train the neural network to know if these
    reviews are positive or negative just by reading theã€‚exNow we're going to make
    use of a pretrained embedding module called Google news swivel20 dimensionã€‚ So
    this is a relatively low resolutionã€‚ So this is a relatively low dimension encoderã€‚
    It's only 20 dimensions is definitely a lot higher like word'reve and other ones
    that will see when we get to the natural language processing moduleã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æƒ³æ³•æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œï¼Œé€šè¿‡é˜…è¯»è¿™äº›è¯„è®ºæ¥åˆ¤æ–­å®ƒä»¬æ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæçš„ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåä¸ºGoogle news swivelçš„é¢„è®­ç»ƒåµŒå…¥æ¨¡å—ï¼Œç»´åº¦ä¸º20ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒä½åˆ†è¾¨ç‡çš„ç¼–ç å™¨ï¼Œåªæœ‰20ä¸ªç»´åº¦ï¼Œç¡®å®æ¯”æˆ‘ä»¬åœ¨è¿›å…¥è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å—æ—¶çœ‹åˆ°çš„word'reveå’Œå…¶ä»–ç¼–ç å™¨è¦ä½å¾—å¤šã€‚
- en: I'm going go ahead and load in the pretrained neural network for thatã€‚'s built
    into Cursã€‚ So they make that pretty easy for usã€‚ Let's look at the first three
    movie reviewsã€‚ So these are the first three movie reviewsã€‚ You can see they are
    just pretty much freeform textã€‚ This is unstructured data and structured data
    is particularly hard to use with a neural networkã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†åŠ è½½é¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œå®ƒæ˜¯å†…ç½®äºCursä¸­çš„ã€‚å› æ­¤ä»–ä»¬ä¸ºæˆ‘ä»¬æä¾›äº†ç›¸å½“ç®€å•çš„ä½¿ç”¨æ–¹å¼ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å‰ä¸‰æ¡ç”µå½±è¯„è®ºã€‚è¿™æ˜¯å‰ä¸‰æ¡ç”µå½±è¯„è®ºã€‚æ‚¨å¯ä»¥çœ‹åˆ°å®ƒä»¬å‡ ä¹éƒ½æ˜¯è‡ªç”±æ ¼å¼çš„æ–‡æœ¬ã€‚è¿™æ˜¯éç»“æ„åŒ–æ•°æ®ï¼Œè€Œç»“æ„åŒ–æ•°æ®åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨ç‰¹åˆ«å›°éš¾ã€‚
- en: Not to worryï¼Œ we can take that unstructured data Basically right here is what
    we use previously to print it outã€‚ We're going to pass it into the hub layerã€‚
    The hub layer is the layer that we just transferred in from Tensorflow hub that
    isã€‚The Google trained embedding layerã€‚ So now let's see what those three reviews
    look like hereã€‚ Now you can see they actual vectorsï¼Œ vectors of 20 numbers eachã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬å¯ä»¥å¤„ç†é‚£äº›éç»“æ„åŒ–æ•°æ®ã€‚åŸºæœ¬ä¸Šï¼Œè¿™é‡Œæ˜¯æˆ‘ä»¬ä¹‹å‰ç”¨äºæ‰“å°çš„å†…å®¹ã€‚æˆ‘ä»¬å°†æŠŠå®ƒä¼ é€’åˆ°ä¸­å¿ƒå±‚ã€‚ä¸­å¿ƒå±‚æ˜¯æˆ‘ä»¬åˆšåˆšä» Tensorflow Hub
    è½¬ç§»è¿‡æ¥çš„å±‚ï¼Œå³è°·æ­Œè®­ç»ƒçš„åµŒå…¥å±‚ã€‚é‚£ä¹ˆç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸‰æ¡è¯„è®ºåœ¨è¿™é‡Œçš„æ ·å­ã€‚ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°å®ƒä»¬å®é™…çš„å‘é‡ï¼Œæ¯ä¸ªå‘é‡ç”± 20 ä¸ªæ•°å­—ç»„æˆã€‚
- en: that's a lot easier to train and deal with a neural network onã€‚ you can basically
    just put raw text into the beginning of the neural network and it's going to process
    on these vectors that are created from those sentencesã€‚ Nowï¼Œ what do the individual
    numbers meanï¼Œ that's very hard to sayï¼Œ but the vectors in the distanceã€‚ So if
    you were to take the distance between two of these vectors for two different sentences
    more similar sentences will typically be closer togetherã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ–¹é¢è®­ç»ƒå’Œå¤„ç†ç¥ç»ç½‘ç»œè¦å®¹æ˜“å¾—å¤šã€‚ä½ åŸºæœ¬ä¸Šå¯ä»¥å°†åŸå§‹æ–‡æœ¬è¾“å…¥åˆ°ç¥ç»ç½‘ç»œçš„å¼€å¤´ï¼Œå®ƒå°†åŸºäºé‚£äº›å¥å­ç”Ÿæˆçš„å‘é‡è¿›è¡Œå¤„ç†ã€‚é‚£ä¹ˆï¼Œå•ä¸ªæ•°å­—æ„å‘³ç€ä»€ä¹ˆï¼Œè¿™å¾ˆéš¾è¯´ï¼Œä½†å‘é‡ä¹‹é—´çš„è·ç¦»æ˜¯æœ‰æ„ä¹‰çš„ã€‚æ‰€ä»¥å¦‚æœä½ è®¡ç®—ä¸¤ä¸ªä¸åŒå¥å­çš„å‘é‡ä¹‹é—´çš„è·ç¦»ï¼Œæ›´ç›¸ä¼¼çš„å¥å­é€šå¸¸ä¼šæ›´é è¿‘ã€‚
- en: We'll see more how we can do linear or algebra on these vectors that the embedding
    layers are creating when we talk about word to Vã€‚ Now let's go ahead and add the
    hub layer to our neural network that we're building and some other layersã€‚Using
    sigmoid and only one output neuron because this is a binary classifierã€‚ either
    it's a positive review or it's notã€‚ We'll compile the neural networkã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨è°ˆè®ºè¯å‘é‡åˆ° V çš„è¿‡ç¨‹ä¸­æ›´æ·±å…¥åœ°äº†è§£å¦‚ä½•å¯¹è¿™äº›åµŒå…¥å±‚åˆ›å»ºçš„å‘é‡è¿›è¡Œçº¿æ€§æˆ–ä»£æ•°è¿ç®—ã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­å‘æˆ‘ä»¬æ­£åœ¨æ„å»ºçš„ç¥ç»ç½‘ç»œä¸­æ·»åŠ ä¸­å¿ƒå±‚ä»¥åŠå…¶ä»–ä¸€äº›å±‚ã€‚ä½¿ç”¨
    sigmoid æ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸”åªæœ‰ä¸€ä¸ªè¾“å‡ºç¥ç»å…ƒï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ã€‚å®ƒè¦ä¹ˆæ˜¯æ­£é¢è¯„ä»·ï¼Œè¦ä¹ˆä¸æ˜¯ã€‚æˆ‘ä»¬å°†ç¼–è¯‘è¿™ä¸ªç¥ç»ç½‘ç»œã€‚
- en: It's binary cross entropy and we'll use atom for trainingã€‚ We're going to split
    into a training and test set so that we can train on one and validate on the other
    and we're going to go ahead and fit it using a relatively large batch size of
    512ã€‚ So this is going to go through a number of epochs we'll fast forward thisã€‚
    Okay the neural network is trainedã€‚ We can go ahead and evaluate itã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‡ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†ä½¿ç”¨ Adam ä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ªä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¦ä¸€ä¸ªä¸Šè¿›è¡ŒéªŒè¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸å¯¹è¾ƒå¤§çš„æ‰¹é‡å¤§å°
    512 æ¥è¿›è¡Œæ‹Ÿåˆã€‚å› æ­¤ï¼Œè¿™å°†ç»å†è‹¥å¹²ä¸ªè®­ç»ƒå‘¨æœŸï¼Œæˆ‘ä»¬å°†å¿«è¿›è¿™ä¸ªè¿‡ç¨‹ã€‚å¥½çš„ï¼Œç¥ç»ç½‘ç»œå·²å®Œæˆè®­ç»ƒã€‚æˆ‘ä»¬å¯ä»¥è¿›è¡Œè¯„ä¼°ã€‚
- en: This is running it on the test setã€‚ can see it's getting an accuracy of around
    85%ã€‚ Let's use a rock chartã€‚ Let's look at how the training progressedã€‚ you'll
    notice up here that when I fit it I kept the history that can be useful to look
    at because you can chart how the training progressed I'll run this this is very
    interesting because you can see that this reallyã€‚ really shows overfitting an
    actionã€‚You can see that the training loss is going downï¼Œ downï¼Œ downã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œçš„ç»“æœã€‚å¯ä»¥çœ‹åˆ°å…¶å‡†ç¡®ç‡çº¦ä¸º 85%ã€‚è®©æˆ‘ä»¬ä½¿ç”¨å›¾è¡¨ï¼Œçœ‹çœ‹è®­ç»ƒçš„è¿›å±•æƒ…å†µã€‚ä½ ä¼šæ³¨æ„åˆ°åœ¨æˆ‘æ‹Ÿåˆæ—¶ä¿å­˜äº†å†å²è®°å½•ï¼Œè¿™å¯¹äºæŸ¥çœ‹è®­ç»ƒçš„è¿›å±•éå¸¸æœ‰ç”¨ã€‚æˆ‘å°†è¿è¡Œè¿™ä¸ªå›¾è¡¨ï¼Œè¿™å¾ˆæœ‰è¶£ï¼Œå› ä¸ºä½ å¯ä»¥çœ‹åˆ°è¿™ç¡®å®æ˜¾ç¤ºäº†è¿‡æ‹Ÿåˆçš„ç°è±¡ã€‚ä½ å¯ä»¥çœ‹åˆ°è®­ç»ƒæŸå¤±åœ¨ä¸æ–­ä¸‹é™ã€‚
- en: And continues to go downã€‚ But the validation loss plateausã€‚ and eventually starts
    to go upã€‚ That is almost a textbook description of overfitting and why you use
    early stoppingã€‚ So the loss was going downã€‚ This is pretty much just the mirror
    of image of thatã€‚ The accuracy is going upã€‚ And againï¼Œ you can see where overfitting
    starts to occurã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”éªŒè¯æŸå¤±æŒç»­ä¸‹é™ï¼Œä½†æœ€ç»ˆå¼€å§‹å¹³ç¨³ã€‚è¿™å‡ ä¹æ˜¯è¿‡æ‹Ÿåˆçš„æ•™ç§‘ä¹¦å¼æè¿°ï¼Œè¿™ä¹Ÿæ˜¯ä½ ä½¿ç”¨æ—©åœæ³•çš„åŸå› ã€‚å› æ­¤ï¼ŒæŸå¤±åœ¨ä¸‹é™ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯é‚£ä¸ªå›¾åƒçš„é•œåƒã€‚å‡†ç¡®ç‡åœ¨ä¸Šå‡ã€‚ä½ åŒæ ·å¯ä»¥çœ‹åˆ°è¿‡æ‹Ÿåˆå¼€å§‹å‘ç”Ÿçš„åœ°æ–¹ã€‚
- en: and the validation set begins to plateauï¼Œ even though the train set will just
    keep getting better and betterã€‚ Thank you for watching this video on transfer
    learningã€‚ This content changes oftenã€‚ So subscribe to the channel to stay up to
    date on this course and other topics and artificial intelligenceã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: éªŒè¯é›†å¼€å§‹å¹³ç¨³ï¼Œå°½ç®¡è®­ç»ƒé›†å°†ä¸æ–­å˜å¾—æ›´å¥½ã€‚æ„Ÿè°¢è§‚çœ‹è¿™æ®µå…³äºè¿ç§»å­¦ä¹ çš„è§†é¢‘ã€‚è¯¥å†…å®¹ç»å¸¸å˜åŒ–ã€‚å› æ­¤è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿éšæ—¶äº†è§£è¯¥è¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜çš„æœ€æ–°åŠ¨æ€ã€‚
- en: '![](img/ca3b1efa82105f12ffd73e17655ac6cb_3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca3b1efa82105f12ffd73e17655ac6cb_3.png)'
