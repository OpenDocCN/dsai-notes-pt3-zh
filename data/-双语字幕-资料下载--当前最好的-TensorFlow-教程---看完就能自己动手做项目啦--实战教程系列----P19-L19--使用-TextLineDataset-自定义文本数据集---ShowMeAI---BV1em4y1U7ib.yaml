- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘â€œå½“å‰æœ€å¥½çš„ TensorFlow æ•™ç¨‹ï¼â€ï¼Œçœ‹å®Œå°±èƒ½è‡ªå·±åŠ¨æ‰‹åšé¡¹ç›®å•¦ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P19ï¼šL19- ä½¿ç”¨ TextLineDataset
    è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®é›† - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘â€œå½“å‰æœ€å¥½çš„ TensorFlow æ•™ç¨‹ï¼â€ï¼Œçœ‹å®Œå°±èƒ½è‡ªå·±åŠ¨æ‰‹åšé¡¹ç›®å•¦ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P19ï¼šL19- ä½¿ç”¨ TextLineDataset
    è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®é›† - ShowMeAI - BV1em4y1U7ib
- en: What is going onï¼Œ guysï¼Œ Welcome back for another videoã€‚ So in this videoã€‚ we're
    gonna take a look at how we can use Tensorflow textline data to be able to load
    our own custom text dataset that we might be working with for our projectã€‚ So
    let me show you the dataset that we're mainly going work with in this videoã€‚ So
    this dataset by the way if you're familiar is a sentiment analysis data where
    we get a movie review and we're supposed to tell if the review is positive or
    negativeã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿å›æ¥è§‚çœ‹å¦ä¸€ä¸ªè§†é¢‘ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Tensorflow çš„æ–‡æœ¬è¡Œæ•°æ®æ¥åŠ è½½æˆ‘ä»¬è‡ªå·±å®šåˆ¶çš„æ–‡æœ¬æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†å¯èƒ½æ˜¯æˆ‘ä»¬é¡¹ç›®ä¸­æ­£åœ¨ä½¿ç”¨çš„ã€‚è®©æˆ‘ç»™ä½ å±•ç¤ºä¸€ä¸‹æˆ‘ä»¬ä¸»è¦åœ¨è¿™ä¸ªè§†é¢‘ä¸­ä½¿ç”¨çš„æ•°æ®é›†ã€‚è¿™ä¸ªæ•°æ®é›†ï¼Œé¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œå¦‚æœä½ ç†Ÿæ‚‰çš„è¯ï¼Œæ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†ææ•°æ®ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€æ¡ç”µå½±è¯„è®ºï¼Œå¹¶ä¸”æˆ‘ä»¬åº”è¯¥åˆ¤æ–­è¯¥è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ã€‚
- en: All rightï¼Œ so for exampleï¼Œ in this caseï¼Œ we have once againï¼Œ Mrã€‚ Cosner has
    dragged out a movie for far longer than necessaryï¼Œ and then it goes onã€‚ then we
    also have a column for the label if it's negative or a positive review And then
    we haveã€‚ if it belongs to the test set or the training setã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä¾‹å¦‚åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å†æ¬¡çœ‹åˆ°ï¼Œç§‘æ–¯çº³å…ˆç”Ÿå°†ä¸€éƒ¨ç”µå½±æ‹–å¾—æ¯”å¿…è¦çš„æ—¶é—´æ›´é•¿ï¼Œç„¶åç»§ç»­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€åˆ—ç”¨äºæ ‡ç­¾ï¼Œæ ‡è¯†æ˜¯è´Ÿé¢è¿˜æ˜¯æ­£é¢è¯„è®ºï¼Œç„¶åæˆ‘ä»¬æœ‰å®ƒæ˜¯å±äºæµ‹è¯•é›†è¿˜æ˜¯è®­ç»ƒé›†ã€‚
- en: one thing to keep in mind here is that this dataset actually has unsupervised
    reviews as wellã€‚ So that is one thing that we're gonna keep in mind when we're
    cleaning and filtering this dataset to be able toã€‚![](img/fd1d96600c664ffac3a740a72c447896_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦è®°ä½çš„ä¸€ä»¶äº‹æ˜¯ï¼Œè¿™ä¸ªæ•°æ®é›†å®é™…ä¸Šä¹ŸåŒ…å«æ— ç›‘ç£çš„è¯„è®ºã€‚æ‰€ä»¥åœ¨æ¸…ç†å’Œè¿‡æ»¤è¿™ä¸ªæ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¦è€ƒè™‘è¿™ä¸€ç‚¹ã€‚![](img/fd1d96600c664ffac3a740a72c447896_1.png)
- en: for So one thing I do want to say is that every data is going to be differently
    structuredã€‚ but I think that showing you how to deal with this data in particularã€‚
    using textline data is going to give you an understanding of dealing with text
    data that extends to manyã€‚ many different kinds of data I'm also at the end of
    the video going to show you a couple of ideas of what to do when dealing with
    dataset sets that aren't structured in a similar fashion as this one So let's
    get startedã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³è¯´çš„æ˜¯ï¼Œæ¯ä¸ªæ•°æ®çš„ç»“æ„éƒ½å°†æœ‰æ‰€ä¸åŒã€‚ä½†æˆ‘è®¤ä¸ºï¼Œå‘ä½ å±•ç¤ºå¦‚ä½•å¤„ç†è¿™ç§ç‰¹å®šæ•°æ®ï¼Œä½¿ç”¨æ–‡æœ¬è¡Œæ•°æ®å°†ä½¿ä½ ç†è§£å¤„ç†æ–‡æœ¬æ•°æ®çš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°è®¸å¤šä¸åŒç§ç±»çš„æ•°æ®ã€‚åœ¨è§†é¢‘çš„æœ€åï¼Œæˆ‘è¿˜ä¼šå‘ä½ å±•ç¤ºä¸€äº›åœ¨å¤„ç†ä¸è¿™ä¸ªæ•°æ®é›†ç»“æ„ä¸ç›¸ä¼¼çš„æ•°æ®é›†æ—¶çš„æƒ³æ³•ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: '![](img/fd1d96600c664ffac3a740a72c447896_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_3.png)'
- en: Alrightï¼Œ so let's start with creating a text line data setã€‚ So we're going to
    do Ds train is TF dot data do text line data setã€‚And we're going to send in that
    emdbã€‚csv file that we just took a look atã€‚ and then as you saw this file right
    here is going to contain the example both from the training and the test setã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œè®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºä¸€ä¸ªæ–‡æœ¬è¡Œæ•°æ®é›†ã€‚æ‰€ä»¥æˆ‘ä»¬å°†è¿›è¡Œ Ds train æ˜¯ TF.dot.data.do.text line data setã€‚æˆ‘ä»¬å°†ä¼ å…¥æˆ‘ä»¬åˆšåˆšæŸ¥çœ‹çš„
    emdb.csv æ–‡ä»¶ã€‚å¦‚ä½ æ‰€è§ï¼Œè¿™ä¸ªæ–‡ä»¶å°†åŒ…å«æ¥è‡ªè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç¤ºä¾‹ã€‚
- en: so what we can do is we can create another one T test to be Tf data text line
    data set of imMDbã€‚ cvã€‚Oh IMDb do CSV and then end quotes So what we're going do
    here is for exampleï¼Œ for the Ds trainã€‚ we're going to try to filter out all of
    the examples that that belongs to the test set and then the opposite for for the
    test that we're going to try to filter out all that belongs to the training set
    So before doing anything it might be good to just know that this works So we're
    going to do for line in DSs train and we're going to print line All right and
    if we do this it's going to print every line sequentially so what we might we
    might not want to do thatã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åˆ›å»ºå¦ä¸€ä¸ª T æµ‹è¯•ï¼Œä½œä¸º Tf æ•°æ®æ–‡æœ¬è¡Œæ•°æ®é›†çš„ imMDbã€‚cvã€‚å“¦ï¼ŒIMDb çš„ CSVï¼Œç„¶åç»“æŸå¼•å·ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œè¦åšçš„æ˜¯ï¼Œä¾‹å¦‚ï¼Œå¯¹äº
    Ds trainï¼Œæˆ‘ä»¬å°†å°è¯•è¿‡æ»¤å‡ºæ‰€æœ‰å±äºæµ‹è¯•é›†çš„ç¤ºä¾‹ï¼Œåä¹‹äº¦ç„¶ï¼Œå¯¹äºæµ‹è¯•é›†ï¼Œæˆ‘ä»¬å°†å°è¯•è¿‡æ»¤å‡ºæ‰€æœ‰å±äºè®­ç»ƒé›†çš„ç¤ºä¾‹ã€‚åœ¨åšä»»ä½•äº‹æƒ…ä¹‹å‰ï¼Œäº†è§£è¿™ä¸€ç‚¹æ˜¯å¥½çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å¯¹
    Ds train ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œå¤„ç†ï¼Œå¹¶æ‰“å°å‡ºè¿™ä¸€è¡Œã€‚å¥½å§ï¼Œå¦‚æœæˆ‘ä»¬è¿™æ ·åšï¼Œå®ƒä¼šæŒ‰é¡ºåºæ‰“å°æ¯ä¸€è¡Œï¼Œå› æ­¤æˆ‘ä»¬å¯èƒ½ä¸æƒ³è¿™æ ·åšã€‚
- en: So what we can do is we can do first of all dot skip1 this is going to skip
    the one that sort of the first row which describes the all of the columns and
    then we can do dot take and we're just going do dot take5 so that means that we're
    gonna just take five examples So let's look at and so let's run that and see how
    it looks like So as you can see we get the first example or index01 and then we
    get the the review right hereã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å…ˆæ‰§è¡Œ`dot skip1`ï¼Œè¿™å°†è·³è¿‡æè¿°æ‰€æœ‰åˆ—çš„ç¬¬ä¸€è¡Œï¼Œç„¶åæˆ‘ä»¬å¯ä»¥æ‰§è¡Œ`dot take`ï¼Œæˆ‘ä»¬åªéœ€æ‰§è¡Œ`dot take 5`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å°†ä»…è·å–äº”ä¸ªç¤ºä¾‹ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹ï¼Œè¿è¡Œä¸€ä¸‹çœ‹çœ‹ç»“æœå¦‚ä½•ã€‚å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬å¾—åˆ°ç¬¬ä¸€ä¸ªç¤ºä¾‹æˆ–ç´¢å¼•01ï¼Œç„¶åå¾—åˆ°è¯„è®ºã€‚
- en: we get the test set and then we get that negative review So what we want here
    is this right here we want this and we can get it we can get that by doing printf
    dot string dot split that line and then we can do is separated by by a comma and's
    essentially going split the line by a comma and in this case it's actually going
    to split the review as well So what we could doã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è·å–æµ‹è¯•é›†ï¼Œç„¶åå¾—åˆ°è´Ÿé¢è¯„è®ºã€‚æˆ‘ä»¬æƒ³è¦çš„æ˜¯è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡`printf.string.split`å°†å…¶åˆ†å‰²ï¼Œç„¶åç”¨é€—å·åˆ†éš”ï¼Œè¿™å®é™…ä¸Šä¼šå°†è¯„è®ºä¹Ÿåˆ†å‰²ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: To avoid that is if we could send in an argument right here to be max split
    equals 4ã€‚ So it's gonna split this oneï¼Œ this oneï¼Œ this oneï¼Œ this oneã€‚ and then
    it's going to split all the restï¼Œ which is the review just onceã€‚ So it's not gonna
    continue splitting the the reviewã€‚ So we can rerun that and see how it looks likeã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œå¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œä¼ å…¥ä¸€ä¸ªå‚æ•°`max split = 4`ã€‚é‚£ä¹ˆå®ƒå°†åˆ†å‰²è¿™ä¸ªï¼Œè¿™ä¸ªï¼Œè¿™ä¸ªï¼Œè¿™ä¸ªï¼Œç„¶ååªä¼šå°†å‰©ä¸‹çš„è¯„è®ºåˆ†å‰²ä¸€æ¬¡ã€‚å› æ­¤ï¼Œå®ƒä¸ä¼šç»§ç»­åˆ†å‰²è¯„è®ºã€‚æˆ‘ä»¬å¯ä»¥é‡æ–°è¿è¡Œå¹¶çœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚
- en: And as we can see here now it's doing that on a single lineã€‚Alrightï¼Œ so what
    we want hereã€‚ first of allï¼Œ is the test setï¼Œ rightï¼Œ and that would be the the
    first index hereã€‚So the thing we're going to do is we're going to create a function
    and we're going to call it filter trainã€‚And we're going to send in one lineã€‚And
    the first thing we're going to do is we're going to split line and we're going
    to do TF stringã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç°åœ¨å®ƒåœ¨å•è¡Œä¸Šæ‰§è¡Œè¿™ä¸ªæ“ä½œã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æƒ³è¦çš„é¦–å…ˆæ˜¯æµ‹è¯•é›†ï¼Œå¯¹å§ï¼Œè¿™å°†æ˜¯è¿™é‡Œçš„ç¬¬ä¸€ä¸ªç´¢å¼•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¦åšçš„äº‹æƒ…æ˜¯åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œç§°ä¸º`filter
    train`ï¼Œå¹¶ä¼ å…¥ä¸€è¡Œã€‚æˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯åˆ†å‰²è¡Œï¼Œå¹¶æ‰§è¡Œ`TF string.start split`ã€‚
- en: start split the lineï¼Œ and we're going to split it by a commaã€‚ And then we can
    send that send in that max split equals 4ã€‚So then we can first of allã€‚ see what
    is the belonging of that particular example so we could call it data belongingã€‚
    so this is either going to be training or test setã€‚So we're going to do that by
    split line index1ã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€å§‹åˆ†å‰²è¡Œï¼Œç”¨é€—å·åˆ†å‰²ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å‘é€`max split = 4`ã€‚æ‰€ä»¥é¦–å…ˆæˆ‘ä»¬å¯ä»¥çœ‹çœ‹é‚£ä¸ªç‰¹å®šç¤ºä¾‹çš„å½’å±ï¼Œæˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸º`data belonging`ã€‚æ‰€ä»¥è¿™å¯èƒ½æ˜¯è®­ç»ƒé›†æˆ–æµ‹è¯•é›†ã€‚æˆ‘ä»¬å°†é€šè¿‡`split
    line index1`æ¥å®ç°ã€‚
- en: But then we can also get the sentiment categoryï¼Œ which is split line of index
    2ã€‚ so this is either going to be positive negative or it's going to be unsupervised
    and we don't want the unsupervised ones to be in our data set and so we're going
    to filter based on these two right here and the data belonging can be train or
    test as we saw in the as we saw previously All rightã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬ä¹Ÿå¯ä»¥å¾—åˆ°æƒ…æ„Ÿç±»åˆ«ï¼Œè¿™æ˜¯ç´¢å¼•2çš„åˆ†å‰²è¡Œã€‚å› æ­¤ï¼Œè¿™å¯èƒ½æ˜¯æ­£é¢æˆ–è´Ÿé¢ï¼Œæˆ–è€…æ˜¯æ— ç›‘ç£çš„ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ— ç›‘ç£çš„æ ·æœ¬å‡ºç°åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šæ ¹æ®è¿™ä¸¤ä¸ªæ¥è¿‡æ»¤ï¼Œæ•°æ®å¯ä»¥æ˜¯è®­ç»ƒé›†æˆ–æµ‹è¯•é›†ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€çœ‹åˆ°çš„ã€‚
- en: so what we're going to do here is we're just going to return trueã€‚Let's see
    if we can do it like thisã€‚ return trueã€‚Trueï¼Œ ifã€‚Data set belonging is equal to
    trainã€‚And if the sentiment categoryã€‚Is not unsupervised All rightã€‚ then that's
    an example that we want to have in our Ds trainã€‚So then if it's notã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯è¿”å›`true`ã€‚è®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦è¿™æ ·åšã€‚è¿”å›`true`ï¼Œå¦‚æœæ•°æ®é›†å½’å±ç­‰äºè®­ç»ƒé›†ã€‚å¦‚æœæƒ…æ„Ÿç±»åˆ«ä¸æ˜¯æ— ç›‘ç£çš„ã€‚é‚£ä¹ˆè¿™æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨`Ds
    train`ä¸­æ‹¥æœ‰çš„ç¤ºä¾‹ã€‚å¦‚æœä¸æ˜¯çš„è¯ã€‚
- en: we're going to return falseã€‚So what we can do here then is we can do dot filter
    and we can send it through this filter functionã€‚ which we you just called filter
    trainã€‚So if we would now do the same thing we're calling DSstrain againã€‚ we can
    see right here that we're just obtaining the ones that belong to the training
    set and that's exactly what we want for the DSs train So what we want to do and
    now is we want to have an identical one for the the test set so we're just going
    to call it filter test and then the rest should be exactly the same except this
    should be test right there and then we're going to do the same thing on a test
    set filter and then we're gonna send it through filter test All right so one thing
    here is that you might actually want to do this beforehand so you might want to
    just split them into two different CSsv files so that you would have for exampleã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿”å›falseã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨dot filterï¼Œé€šè¿‡è¿™ä¸ªåä¸ºfilter trainçš„è¿‡æ»¤å‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨å†æ¬¡è°ƒç”¨DSstrainï¼Œå°±ä¼šå‘ç°æˆ‘ä»¬åªè·å¾—å±äºè®­ç»ƒé›†çš„éƒ¨åˆ†ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºæµ‹è¯•é›†åˆ›å»ºä¸€ä¸ªç›¸åŒçš„åŠŸèƒ½ï¼Œç§°ä¸ºfilter
    testï¼Œå…¶ä»–éƒ¨åˆ†åº”è¯¥å®Œå…¨ç›¸åŒï¼Œåªéœ€å°†è¿™é‡Œæ”¹ä¸ºtestï¼Œç„¶åå¯¹æµ‹è¯•é›†è¿›è¡Œç›¸åŒçš„filteræ“ä½œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½ å¯èƒ½æƒ³è¦æå‰å°†å®ƒä»¬åˆ†å‰²æˆä¸¤ä¸ªä¸åŒçš„CSsvæ–‡ä»¶ã€‚
- en: trained CSsv and then test csv and in this way you're going to save on some
    compute by not having to filter the examples sort of when you're actually training
    the model but you might need to use this filter function for something else so
    it's good toã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒçš„CSsvå’Œæµ‹è¯•çš„csvï¼Œé€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ å¯ä»¥èŠ‚çœè®¡ç®—èµ„æºï¼Œå› ä¸ºä¸å¿…åœ¨å®é™…è®­ç»ƒæ¨¡å‹æ—¶è¿‡æ»¤ç¤ºä¾‹ï¼Œä½†ä½ å¯èƒ½éœ€è¦åœ¨å…¶ä»–åœ°æ–¹ä½¿ç”¨è¿™ä¸ªè¿‡æ»¤å‡½æ•°ï¼Œå› æ­¤äº†è§£å®ƒçš„å­˜åœ¨å’Œç”¨æ³•æ˜¯å¥½çš„ã€‚
- en: Kknowow that it exists and how to use itã€‚ So now that we've actually done thatã€‚
    what we want to do is we want to let's see if we can make some kind of to do listã€‚So
    we want toã€‚ first of allï¼Œ we want to create some kind of vocabulary right If we're
    gonna send this into our modelã€‚ we want to create a vocabularyã€‚ first of allï¼Œ
    and I've shown how to do this in a previous video in Tensorflow covering Tensorflow
    dataã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†è¿™ä¸€æ­¥ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æƒ³çœ‹çœ‹èƒ½å¦åšä¸€äº›å¾…åŠäº‹é¡¹åˆ—è¡¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¦åˆ›å»ºæŸç§è¯æ±‡è¡¨ã€‚å¦‚æœæˆ‘ä»¬è¦å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»é¦–å…ˆåˆ›å»ºä¸€ä¸ªè¯æ±‡è¡¨ã€‚æˆ‘åœ¨ä¹‹å‰çš„TensorFlowè§†é¢‘ä¸­å·²ç»å±•ç¤ºäº†å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: but I'm going to show it again because I think it's very valuable and could
    help you outã€‚ So then when we've created the vocabularyã€‚ we want to be able to
    to be able to numericalize your your text stringã€‚ sort of the text string to indices
    right and we're going to use token see if it's token text encoderã€‚ I believe it's
    called let me check thatã€‚ Yeahï¼Œ so it's called token text encoder that's going
    do that for usã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å†æ¬¡å±•ç¤ºè¿™ä¸ªè¿‡ç¨‹ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºå®ƒéå¸¸æœ‰ä»·å€¼ï¼Œèƒ½å¸®åŠ©åˆ°ä½ ã€‚å½“æˆ‘ä»¬åˆ›å»ºäº†è¯æ±‡è¡¨åï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå°†æ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—ç´¢å¼•ã€‚æˆ‘ä»¬å°†ä½¿ç”¨token text
    encoderï¼Œæˆ‘ç›¸ä¿¡å®ƒå°±æ˜¯è¿™ä¸ªï¼Œç¨ç­‰æˆ‘ç¡®è®¤ä¸€ä¸‹ã€‚æ˜¯çš„ï¼Œå®ƒå«token text encoderï¼Œè¿™ä¼šä¸ºæˆ‘ä»¬å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚
- en: And then lastlyï¼Œ we're going to use we want to pad de batches so we can send
    inã€‚S inã€‚To and R and Nã€‚ for exampleã€‚Rightï¼Œ so that's sort of what we want to do
    nowã€‚ And to be able to create the vocabularyï¼Œ we first want to have a tokenizerã€‚
    and there are a couple of different tokensã€‚ The one I'm going to use is tokenizer
    at Tensorflow dataetã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è¦å¡«å……æ‰¹æ¬¡ï¼Œä»¥ä¾¿å¯ä»¥å‘é€ã€‚æ¯”å¦‚è¯´Sã€Rå’ŒNã€‚å¯¹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ç°åœ¨æƒ³åšçš„ã€‚ä¸ºäº†åˆ›å»ºè¯æ±‡è¡¨ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€ä¸ªåˆ†è¯å™¨ã€‚è¿™é‡Œæœ‰å‡ ç§ä¸åŒçš„åˆ†è¯å™¨ã€‚æˆ‘å°†ä½¿ç”¨çš„æ˜¯TensorFlow
    datasetä¸­çš„tokenizerã€‚
- en: TFTS do features do text dot tokenizerã€‚And we're going to create a functionã€‚
    So we're going to call build vocabularyã€‚ And so the tokenizer is going to split
    a sentenceã€‚ So let's say I have a sentenceã€‚ I love bananaã€‚ Then that's going split
    it into a list with I loveã€‚Bananaï¼Œ alrightï¼Œ And then that's gonna then be able
    to get tokenizedã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TFTSåŠŸèƒ½æ˜¯text.dot.tokenizerã€‚æˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œç§°ä¸ºbuild vocabularyã€‚è¿™ä¸ªåˆ†è¯å™¨ä¼šå°†å¥å­æ‹†åˆ†ã€‚æ¯”å¦‚è¯´æˆ‘æœ‰ä¸€ä¸ªå¥å­ï¼šæˆ‘çˆ±é¦™è•‰ã€‚å®ƒä¼šå°†å…¶æ‹†åˆ†ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼šæˆ‘çˆ±ï¼Œé¦™è•‰ï¼Œå¥½çš„ï¼Œç„¶åå°±å¯ä»¥è¿›è¡Œæ ‡è®°åŒ–ã€‚
- en: Or rather sorry that's going to be able to be numericalized that's our step2
    to something like the indexã€‚ the index for this particular word in our vocabulary
    so that's let's say we just have those three words maybe that would be 0ã€‚1 and
    2 and then this is something that can be sent in to our R andN later on so for
    the building of the vocabulary we're going to send in some data and maybe DSs
    train we're going to send in DSs train we're going send in some threshold and
    we're going send in some threshold because we want if if this word occurs this
    many times then we want to include it in our dataã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…æ›´ç¡®åˆ‡åœ°è¯´ï¼ŒæŠ±æ­‰ï¼Œè¿™å°†èƒ½å¤Ÿè¿›è¡Œæ•°å­—åŒ–ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„æ­¥éª¤2ï¼Œç±»ä¼¼äºç´¢å¼•ã€‚è¿™ä¸ªç‰¹å®šå•è¯åœ¨æˆ‘ä»¬è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ï¼Œæ‰€ä»¥å‡è®¾æˆ‘ä»¬åªæœ‰è¿™ä¸‰ä¸ªå•è¯ï¼Œä¹Ÿè®¸å®ƒä»¬çš„ç´¢å¼•æ˜¯0ã€1å’Œ2ï¼Œç„¶åè¿™äº›å¯ä»¥åœ¨ç¨åå‘é€åˆ°æˆ‘ä»¬çš„Rå’ŒNä¸­ã€‚å› æ­¤ï¼Œåœ¨æ„å»ºè¯æ±‡è¡¨æ—¶ï¼Œæˆ‘ä»¬å°†å‘é€ä¸€äº›æ•°æ®ï¼Œä¹Ÿè®¸æ˜¯DSs
    trainï¼Œæˆ‘ä»¬å°†å‘é€DSs trainï¼Œå¹¶ä¸”å°†å‘é€ä¸€äº›é˜ˆå€¼ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›å¦‚æœè¿™ä¸ªè¯å‡ºç°äº†è¿™ä¹ˆå¤šæ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬æƒ³è¦å°†å…¶åŒ…å«åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­ã€‚
- en: And in this caseï¼Œ we're going to chooseï¼Œ I don't knowï¼Œ let's say 200ã€‚ this is
    going to be dependent on dataï¼Œ but let's just choose 200 because it's a relatively
    large data setã€‚And also there are different ways to build a vocabularyã€‚ and you
    can imagine doing this in multiple waysï¼Œ and I'm perhaps showing you a more simple
    oneã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é€‰æ‹©ï¼Œæˆ‘ä¸çŸ¥é“ï¼Œå‡è®¾æ˜¯200ã€‚è¿™å°†å–å†³äºæ•°æ®ï¼Œä½†æˆ‘ä»¬å°±é€‰æ‹©200ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§çš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸åŒçš„æ–¹æ³•æ¥æ„å»ºè¯æ±‡è¡¨ã€‚ä½ å¯ä»¥æƒ³è±¡ä»¥å¤šç§æ–¹å¼è¿›è¡Œæ­¤æ“ä½œï¼Œæˆ‘å¯èƒ½ä¼šå‘ä½ å±•ç¤ºä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ã€‚
- en: So first of allï¼Œ what we're going to need is frequenciesï¼Œ which is going to
    be just a dictionaryã€‚Then we're going to do vocabularyï¼Œ and we're going to do
    a setã€‚And first of allã€‚ we're going to do vocabulary dot updateã€‚And we're going
    to send in some start token all right start of sentence token and that's just
    what we call it in this exampleã€‚ And then we have vocabulary dot update and we
    have end of sentence right so EOS for end of sentence and then token So we want
    to have those in our vocabulary and then we're going to do four line in DS train
    dot skip of one So we're going to skip the first row and we're going to do split
    line Tf string dot split lineã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯é¢‘ç‡ï¼Œè¿™å°†åªæ˜¯ä¸€ä¸ªå­—å…¸ã€‚ç„¶åæˆ‘ä»¬å°†å¤„ç†è¯æ±‡è¡¨ï¼Œå¹¶è¿›è¡Œä¸€ä¸ªé›†åˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ‰§è¡Œvocabulary.dot updateã€‚æˆ‘ä»¬å°†å‘é€ä¸€äº›å¼€å§‹æ ‡è®°ï¼Œå¥½å§ï¼Œå¥å­å¼€å§‹æ ‡è®°ï¼Œè¿™åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­å°±æ˜¯æˆ‘ä»¬æ‰€ç§°çš„ã€‚ç„¶åæˆ‘ä»¬å°†æ‰§è¡Œvocabulary.dot
    updateï¼Œæ·»åŠ å¥å­ç»“æŸæ ‡è®°ï¼Œæ‰€ä»¥EOSè¡¨ç¤ºå¥å­ç»“æŸï¼Œæ¥ç€æ˜¯æ ‡è®°ã€‚æˆ‘ä»¬å¸Œæœ›å°†è¿™äº›æ·»åŠ åˆ°æˆ‘ä»¬çš„è¯æ±‡è¡¨ä¸­ï¼Œç„¶åæˆ‘ä»¬å°†æ‰§è¡Œfor line in DS train.dot
    skip of oneï¼Œå› æ­¤æˆ‘ä»¬å°†è·³è¿‡ç¬¬ä¸€è¡Œï¼Œç„¶åæ‰§è¡Œsplit line Tf string.dot split lineã€‚
- en: Coma and then max split equals 4ã€‚ Then we're going to obtain the text or the
    reviewã€‚ and we're going to do that by split line of index 4ã€‚Then we're going to
    get the tokenized textã€‚And to get itï¼Œ we're going to do tokenizer dot tokenize
    of textã€‚ and we need to convert it to numpy as wellã€‚ And then we're also going
    to do dot lower because if the letters or if the word is capitalizedã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¦ç‚¹ç„¶åæœ€å¤§æ‹†åˆ†ç­‰äº4ã€‚ç„¶åæˆ‘ä»¬å°†è·å¾—æ–‡æœ¬æˆ–è¯„è®ºã€‚æˆ‘ä»¬å°†é€šè¿‡æ‹†åˆ†ç´¢å¼•4çš„è¡Œæ¥å®ç°ã€‚ç„¶åæˆ‘ä»¬å°†è·å¾—æ ‡è®°åŒ–çš„æ–‡æœ¬ã€‚ä¸ºäº†è·å¾—å®ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨tokenizer.dot
    tokenizeæ–‡æœ¬ã€‚æˆ‘ä»¬è¿˜éœ€è¦å°†å…¶è½¬æ¢ä¸ºnumpyã€‚æ¥ç€ï¼Œæˆ‘ä»¬è¿˜è¦è¿›è¡Œdot lowerï¼Œå› ä¸ºå¦‚æœå­—æ¯æˆ–å•è¯æ˜¯å¤§å†™çš„ã€‚
- en: then it doesn't really mean anythingã€‚ We're just going to make everything lowercase
    to reduce the number of words in our vocabularyã€‚ then we're going to do four wordã€‚In
    tokenized textã€‚We're going to do if we're not in this frequencies dictionaryã€‚We're
    going to add itã€‚ so we're going to do frequencies of wordã€‚ we're going to set
    that to oneã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™å®é™…ä¸Šå¹¶æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ã€‚æˆ‘ä»¬åªä¼šå°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå°å†™ï¼Œä»¥å‡å°‘è¯æ±‡è¡¨ä¸­çš„å•è¯æ•°é‡ã€‚ç„¶åæˆ‘ä»¬å°†å¤„ç†å››ä¸ªå•è¯ã€‚åœ¨æ ‡è®°åŒ–æ–‡æœ¬ä¸­ã€‚å¦‚æœè¿™ä¸ªè¯ä¸åœ¨é¢‘ç‡å­—å…¸ä¸­ï¼Œæˆ‘ä»¬å°†æ·»åŠ å®ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œfrequencies
    of wordï¼Œå¹¶å°†å…¶è®¾ç½®ä¸º1ã€‚
- en: But if it is in our vocabularyï¼Œ so else we're going to do frequencies of wordã€‚
    we're going to add one to thatã€‚Then we're gonna check if we've reached the thresholdï¼Œ
    rightã€‚ That's the one we sent to the top hereã€‚Soï¼Œ if frequenciesã€‚Of wordã€‚Is equal
    to some thresholdã€‚Then we're going to do vocabulary dot update and then tokenize
    textã€‚Alrightã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœå®ƒåœ¨æˆ‘ä»¬çš„è¯æ±‡è¡¨ä¸­ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†æ‰§è¡Œfrequencies of wordã€‚æˆ‘ä»¬å°†å…¶åŠ ä¸€ã€‚ç„¶åæˆ‘ä»¬å°†æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†é˜ˆå€¼ï¼Œå¯¹å§ã€‚é‚£æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œå‘é€çš„é˜ˆå€¼ã€‚æ‰€ä»¥ï¼Œå¦‚æœfrequencies.of
    wordç­‰äºæŸä¸ªé˜ˆå€¼ã€‚é‚£ä¹ˆæˆ‘ä»¬å°†æ‰§è¡Œvocabulary.dot updateï¼Œç„¶åæ˜¯tokenizeæ–‡æœ¬ã€‚å¥½å§ã€‚
- en: And then at the end of thisï¼Œ we're just going to return the vocabularyã€‚It's
    quite a simple functionã€‚And we're just going to do that by sending in our DSs
    trainã€‚ So what we can do then is we can do vocabularyã€‚Equals build vocabularyã€‚So
    calling that functionã€‚ and we're just going to send in DSs trainã€‚Nowï¼Œ we might
    not want to do this every time because it might take a lot ofã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æœ€åï¼Œæˆ‘ä»¬å°†è¿”å›è¯æ±‡ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å½“ç®€å•çš„å‡½æ•°ã€‚æˆ‘ä»¬å°†é€šè¿‡å‘é€æˆ‘ä»¬çš„`DSs train`æ¥å®Œæˆè¿™ä»¶äº‹ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼Œè¯æ±‡ç­‰äºæ„å»ºè¯æ±‡ã€‚å› æ­¤è°ƒç”¨é‚£ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å°†ä¼ å…¥`DSs
    train`ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³æ¯æ¬¡éƒ½è¿™æ ·åšï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ã€‚
- en: might take some time to compute thisã€‚ So what we can do then is we can do vocab
    fileã€‚ We can open vocabulary dotã€‚Oï¼Œ PGï¼Œ and then we can doã€‚Right byã€‚We're going
    to do pickleã€‚ so we're going to use pickleã€‚ that's why we imported itã€‚ We're going
    to do pickle do dump vocabularyï¼Œ and then vocab fileã€‚Alrightï¼Œ so this wouldã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¿™ä¸ªå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ‰“å¼€`vocabulary.OBG`ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥è¿™æ ·åšã€‚æˆ‘ä»¬å°†ä½¿ç”¨`pickle`ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦å¯¼å…¥å®ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨`pickle.dump`è¯æ±‡ï¼Œç„¶åæ˜¯è¯æ±‡æ–‡ä»¶ã€‚å¥½çš„ï¼Œè¿™æ ·å°±å¯ä»¥äº†ã€‚
- en: Build vocabulary and save it to vocabulary dot O B Gã€‚ rightï¼Œ So then if we've
    created itã€‚ maybe we want to have something for loading it so we could haveã€‚We
    can do by vocab file asã€‚Openã€‚Vocabababularyï¼Œ that objectã€‚ and then ourã€‚RB for
    reading itã€‚ and then vocabulary is pickle dot load that vocab fileã€‚Alrightï¼Œ in
    this caseã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºè¯æ±‡å¹¶å°†å…¶ä¿å­˜åˆ°`vocabulary.OBG`ã€‚å¥½çš„ï¼Œå¦‚æœæˆ‘ä»¬åˆ›å»ºäº†å®ƒï¼Œä¹Ÿè®¸æˆ‘ä»¬æƒ³è¦ä¸€äº›åŠ è½½å®ƒçš„ä¸œè¥¿ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰“å¼€è¯æ±‡æ–‡ä»¶æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚è¯æ±‡å¯¹è±¡å’Œæˆ‘ä»¬çš„`RB`ç”¨äºè¯»å–ï¼Œç„¶åè¯æ±‡æ˜¯é€šè¿‡`pickle.load`åŠ è½½è¯¥è¯æ±‡æ–‡ä»¶ã€‚å¥½çš„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ã€‚
- en: let's just uncommon thatã€‚ first of allï¼Œ So we're just gonna build a vocabulary
    and then we can save it if we want to run it at some other timeã€‚All rightï¼Œ so
    then we're going to create some encoder that can do the numericalizingã€‚ which
    is converting the tokenized string to indicesã€‚So we do that by encoder is Tfdsã€‚featuresã€‚textã€‚
    token text encoderã€‚And first of allï¼Œ we're going to send in the list of the vocabularyã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å…ˆå–æ¶ˆæ³¨é‡Šã€‚å› æ­¤æˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªè¯æ±‡ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨å…¶ä»–æ—¶é—´è¿è¡Œå®ƒï¼Œä¹Ÿå¯ä»¥ä¿å­˜å®ƒã€‚å¥½çš„ï¼Œç„¶åæˆ‘ä»¬å°†åˆ›å»ºä¸€äº›ç¼–ç å™¨ï¼Œå¯ä»¥è¿›è¡Œæ•°å­—åŒ–ã€‚è¿™æ˜¯å°†åˆ†è¯å­—ç¬¦ä¸²è½¬æ¢ä¸ºç´¢å¼•ã€‚æ‰€ä»¥æˆ‘ä»¬é€šè¿‡`encoder
    = Tfds.features.text.token_text_encoder`æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¼ å…¥è¯æ±‡çš„åˆ—è¡¨ã€‚
- en: Then we're gonna specify some end of vocabulary tokenã€‚ and we're gonna specify
    that to be theã€‚Unknownã€‚And those are going to be all the words that didn'tã€‚That
    wasn't inside of our data set of a frequency of at least 200ã€‚ Then we're going
    to do lowercase equals trueã€‚ and we're also going to set the tokenizerã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†æŒ‡å®šä¸€äº›è¯æ±‡ç»“æŸæ ‡è®°ï¼Œå¹¶å°†å…¶æŒ‡å®šä¸ºâ€œæœªçŸ¥â€ã€‚è¿™äº›å°†æ˜¯æ‰€æœ‰æœªåŒ…å«åœ¨æˆ‘ä»¬æ•°æ®é›†ä¸­ï¼Œä¸”é¢‘ç‡è‡³å°‘ä¸º200çš„å•è¯ã€‚ç„¶åæˆ‘ä»¬å°†è®¾ç½®`lowercase =
    true`ï¼Œå¹¶ä¸”è¿˜å°†è®¾ç½®åˆ†è¯å™¨ã€‚
- en: which is just tokenizer in this caseã€‚ Allrightï¼Œ then we're going to define my
    encoderã€‚And what we get here is we're gonna get some text tensorï¼Œ and we're gonna
    get some labelã€‚ And the first thing we're gonna to do is encoded text is justï¼Œ
    we're gonna use our encoderï¼Œ rightã€‚ this token text encoderã€‚canSo to numericalize
    this text tensor that we send inã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä»…ä»…æ˜¯ä¸€ä¸ªåˆ†è¯å™¨ã€‚å¥½çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦å®šä¹‰æˆ‘çš„ç¼–ç å™¨ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ˜¯ä¸€äº›æ–‡æœ¬å¼ é‡ï¼Œä»¥åŠä¸€äº›æ ‡ç­¾ã€‚æˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ç¼–ç æ–‡æœ¬ï¼Œå°±æ˜¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬çš„ç¼–ç å™¨ï¼Œå¯¹å§ã€‚è¿™ç§ä»¤ç‰Œæ–‡æœ¬ç¼–ç å™¨å¯ä»¥å°†æˆ‘ä»¬å‘é€çš„æ–‡æœ¬å¼ é‡æ•°å­—åŒ–ã€‚
- en: So we do that by encoder dot encodeï¼Œ and we do texttensorã€‚ nuyã€‚And then we're
    just going to return the encoded text comma labelã€‚All rightã€‚ now one thing we
    need to do in Tensorflowï¼Œ which is a little bit unnecessarily complexï¼Œ perhapsã€‚
    but we need to make sure that this Python function is a part of our computational
    graphã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬é€šè¿‡ç¼–ç å™¨çš„`encode`æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶ä¼ å…¥`texttensor`ã€‚ç„¶åæˆ‘ä»¬å°†è¿”å›ç¼–ç çš„æ–‡æœ¬å’Œæ ‡ç­¾ã€‚å¥½çš„ï¼Œç°åœ¨åœ¨TensorFlowä¸­ï¼Œæˆ‘ä»¬éœ€è¦åšä¸€ä»¶äº‹ï¼Œè¿™å¯èƒ½æœ‰ç‚¹ä¸å¿…è¦çš„å¤æ‚ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿è¿™ä¸ªPythonå‡½æ•°æ˜¯æˆ‘ä»¬è®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†ã€‚
- en: So we need to define another functionã€‚ We're gonna call it encodeã€‚Map functionã€‚
    And we're gonna send in a line right here and that's gonna be just the long line
    of of the sort of the labelã€‚ the review and all of that that we saw and printed
    beforeã€‚ So the first thing we're gonna do a split lineã€‚ We're going do TF strings
    that split line comma separator and then max split equals 4 againã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦å®šä¹‰å¦ä¸€ä¸ªå‡½æ•°ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º`encode.Map`å‡½æ•°ã€‚æˆ‘ä»¬å°†ä¼ å…¥ä¸€è¡Œæ–‡æœ¬ï¼Œå°±æ˜¯ä¹‹å‰æˆ‘ä»¬çœ‹åˆ°å¹¶æ‰“å°çš„æ ‡ç­¾ã€è¯„è®ºç­‰ç­‰çš„é•¿è¡Œã€‚æˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯æ‹†åˆ†è¡Œã€‚æˆ‘ä»¬å°†ä½¿ç”¨`TF
    strings.split`æ–¹æ³•ï¼Œä»¥é€—å·ä¸ºåˆ†éš”ç¬¦ï¼Œæœ€å¤§åˆ†å‰²æ•°ä¸º4ã€‚
- en: then we're gonna get the label stringï¼Œ that's going be the split line of index
    2ã€‚ So I'm not going print the line againã€‚ and just believe me this is index 2
    of of thatã€‚ If we did the split that's going to be index 2ã€‚ So this is going to
    be negative or positiveã€‚And that is going to be that because we also removed all
    new ones that are unsupervisedã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å¾—åˆ°æ ‡ç­¾å­—ç¬¦ä¸²ï¼Œå®ƒå°†æ˜¯ç´¢å¼•2çš„åˆ†å‰²è¡Œã€‚æ‰€ä»¥æˆ‘ä¸ä¼šå†æ‰“å°è¿™ä¸€è¡Œã€‚åªéœ€ç›¸ä¿¡æˆ‘è¿™æ˜¯ç´¢å¼•2ã€‚å¦‚æœæˆ‘ä»¬è¿›è¡Œäº†åˆ†å‰²ï¼Œè¿™å°†æ˜¯ç´¢å¼•2ã€‚å› æ­¤è¿™å°†æ˜¯è´Ÿé¢æˆ–ç§¯æçš„ã€‚å¹¶ä¸”å°±æ˜¯è¿™æ ·ï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿåˆ é™¤äº†æ‰€æœ‰æœªæ ‡è®°çš„éƒ¨åˆ†ã€‚
- en: Then the review is going to beï¼Œ and we're going to add a start token right here
    in the beginningã€‚ we're going to add it with split line of of index 4ã€‚ So that's
    sort of the last oneã€‚ which is going to be a long sort of text stringã€‚And then
    we're going toï¼Œ in the endã€‚ also add end of sentence tokenã€‚ Allrightï¼Œ so this
    is our reviewã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¯„è®ºå°†æ˜¯ï¼Œæˆ‘ä»¬å°†åœ¨å¼€å§‹æ—¶æ·»åŠ ä¸€ä¸ªèµ·å§‹æ ‡è®°ã€‚æˆ‘ä»¬å°†ä¸ç´¢å¼•4çš„åˆ†å‰²è¡Œä¸€èµ·æ·»åŠ å®ƒã€‚æ‰€ä»¥é‚£æ˜¯æœ€åä¸€ä¸ªï¼Œå°†æ˜¯ä¸€ä¸ªè¾ƒé•¿çš„æ–‡æœ¬å­—ç¬¦ä¸²ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜å°†æ·»åŠ å¥å­ç»“æŸæ ‡è®°ã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„è¯„è®ºã€‚
- en: and this is going to then be sent to this token text encoderã€‚ which is going
    to be the do the tokenizationã€‚ It's going to do lowercaseã€‚ It's going to then
    map it to some numerical valuesã€‚ and we can later send that into our modelã€‚So
    the label is going to be oneã€‚The label string is equal to positiveã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™å°†è¢«å‘é€åˆ°è¿™ä¸ªæ ‡è®°æ–‡æœ¬ç¼–ç å™¨ã€‚å®ƒå°†è¿›è¡Œæ ‡è®°åŒ–ã€‚å®ƒå°†è½¬ä¸ºå°å†™ã€‚ç„¶åå°†å…¶æ˜ å°„åˆ°ä¸€äº›æ•°å­—å€¼ã€‚æˆ‘ä»¬å¯ä»¥ç¨åå°†å…¶å‘é€åˆ°æˆ‘ä»¬çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œæ ‡ç­¾å°†æ˜¯1ã€‚æ ‡ç­¾å­—ç¬¦ä¸²ç­‰äºç§¯æã€‚
- en: otherwise it's going to be zero for negative encoded textï¼Œ comma labelã€‚Is going
    to be equals to Tf that pi functionã€‚And we're going to do my encoder the one be
    defined aboveã€‚ the input is going to be the review and also the the labelã€‚Then
    we're going need to specify the output and the output in this case is going to
    be a T F in 64 and then T f into 32ã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™å¯¹äºè´Ÿç¼–ç æ–‡æœ¬ï¼Œå°†æ˜¯é›¶ï¼Œé€—å·æ ‡ç­¾ã€‚å°†ç­‰äºTfé‚£piå‡½æ•°ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä¸Šé¢å®šä¹‰çš„ç¼–ç å™¨ã€‚è¾“å…¥å°†æ˜¯è¯„è®ºä»¥åŠæ ‡ç­¾ã€‚ç„¶åæˆ‘ä»¬éœ€è¦æŒ‡å®šè¾“å‡ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¾“å‡ºå°†æ˜¯T
    F in 64å’ŒT f in 32ã€‚
- en: So this is for theã€‚The the sentenceï¼Œ the review and this is going to be for
    the label which is just going to be0 or1 then the encoded text dot set shapeã€‚
    this is something we need to do for the computational graph we're going to set
    that to none in a list and we're going to set it to none because the review length
    can be varied a review could be just you know 500 characters or it could be 100
    characters or then we're going do label dot set shape and we're just going to
    send in a list because it's only going to be one value then we're going to return
    the encoded text and we're going to return the label as wellã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™éƒ¨åˆ†æ˜¯ä¸ºäº†ã€‚è¿™ä¸ªå¥å­ï¼Œå®¡æŸ¥ï¼Œä»¥åŠè¿™ä¸ªå°†ç”¨äºæ ‡ç­¾ï¼Œå®ƒåªä¼šæ˜¯0æˆ–1ï¼Œç„¶åæ˜¯ç¼–ç æ–‡æœ¬çš„ç‚¹é›†å½¢çŠ¶ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦ä¸ºè®¡ç®—å›¾åšçš„ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒè®¾ç½®ä¸ºä¸€ä¸ªåˆ—è¡¨ä¸­çš„noneï¼Œæˆ‘ä»¬å°†è®¾ç½®ä¸ºnoneï¼Œå› ä¸ºè¯„è®ºé•¿åº¦æ˜¯å¯ä»¥å˜åŒ–çš„ï¼Œè¯„è®ºå¯èƒ½ä»…ä»…æ˜¯500ä¸ªå­—ç¬¦ï¼Œæˆ–è€…å¯èƒ½æ˜¯100ä¸ªå­—ç¬¦ï¼Œç„¶åæˆ‘ä»¬å°†è¿›è¡Œæ ‡ç­¾çš„ç‚¹é›†å½¢çŠ¶ï¼Œå¹¶ä¸”æˆ‘ä»¬åªä¼šå‘é€ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºå®ƒåªä¼šæœ‰ä¸€ä¸ªå€¼ï¼Œç„¶åæˆ‘ä»¬å°†è¿”å›ç¼–ç æ–‡æœ¬ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¹Ÿå°†è¿”å›æ ‡ç­¾ã€‚
- en: Alrightï¼Œ so now we're sort of to a point where we have defined everything that
    we need to process our dataã€‚ and we're just going to perform these mappings on
    the on the DS train and the DS test data setã€‚So first of allï¼Œ we're going to do
    autotune and you you're hopefully familiar with all of these steps right here
    from from a Tensorflow data set tutorialã€‚ So we're going to do Tf data do experimental
    dot autotuneã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†å¤„ç†æ•°æ®æ‰€éœ€çš„ä¸€åˆ‡ã€‚æˆ‘ä»¬åªéœ€å¯¹DS trainå’ŒDS testæ•°æ®é›†æ‰§è¡Œè¿™äº›æ˜ å°„ã€‚æ‰€ä»¥é¦–å…ˆï¼Œæˆ‘ä»¬å°†è¿›è¡Œè‡ªåŠ¨è°ƒä¼˜ï¼Œå¸Œæœ›ä½ å¯¹è¿™é‡Œæ‰€æœ‰æ­¥éª¤éƒ½å¾ˆç†Ÿæ‚‰ï¼Œæ¥è‡ªTensorflowæ•°æ®é›†æ•™ç¨‹ã€‚å› æ­¤æˆ‘ä»¬å°†æ‰§è¡ŒTf
    data do experimental.dot autotuneã€‚
- en: We're going to do DSstrain equals DSstrain dot mapã€‚ and we're going to map it
    through that in map functionã€‚And we're gonna specify the number of parallel callsã€‚
    And this is just to for if we'reã€‚This is just going to do them in parallel and
    make for faster data loadingã€‚And thenã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åšDSstrainç­‰äºDSstrain.dot mapã€‚æˆ‘ä»¬å°†é€šè¿‡mapå‡½æ•°æ¥æ˜ å°„å®ƒã€‚æˆ‘ä»¬å°†æŒ‡å®šå¹¶è¡Œè°ƒç”¨çš„æ•°é‡ã€‚è¿™åªæ˜¯ä¸ºäº†ï¼Œå¦‚æœæˆ‘ä»¬ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†è®©å®ƒä»¬å¹¶è¡Œè¿è¡Œï¼Œä»è€ŒåŠ å¿«æ•°æ®åŠ è½½é€Ÿåº¦ã€‚ç„¶åã€‚
- en: We can call dot cache as wellï¼Œ and this is going to cache some in memory to
    make it faster for the next oneã€‚So we need to do also a shuffle so that as you
    sawã€‚ all of the examples are sort of negative and then positiveï¼Œ and we want to
    have randomizedã€‚ So the data set is 25000 in length for the trainingã€‚ So we're
    gonna do shuffle 25000ã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥è°ƒç”¨dot cacheï¼Œè¿™å°†ç¼“å­˜ä¸€äº›å†…å®¹åˆ°å†…å­˜ä¸­ï¼Œä»¥ä¾¿ä¸‹ä¸€ä¸ªæ›´å¿«ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è¿›è¡Œæ´—ç‰Œï¼Œå¦‚ä½ æ‰€è§ï¼Œæ‰€æœ‰çš„ç¤ºä¾‹éƒ½æ˜¯è´Ÿé¢çš„ï¼Œç„¶åæ˜¯ç§¯æçš„ï¼Œæˆ‘ä»¬å¸Œæœ›éšæœºåŒ–ã€‚å› æ­¤ï¼Œæ•°æ®é›†çš„é•¿åº¦ä¸º25000ç”¨äºè®­ç»ƒã€‚å› æ­¤æˆ‘ä»¬å°†è¿›è¡Œæ´—ç‰Œ25000ã€‚
- en: Now perhaps 25000 is a bit muchã€‚ you could have done something like 5000ã€‚ but
    let's justã€‚Do shuffle 2 about 5000 to make sure it's completely randomã€‚Then we
    need to batch this and remember all of the reviews can be different in lengthã€‚
    so we're going to need to do padding and we do that by doing padded batchã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¹Ÿè®¸25000æœ‰ç‚¹å¤šã€‚ä½ å¯ä»¥é€‰æ‹©5000ï¼Œä½†æˆ‘ä»¬å°±è¿™æ ·å§ã€‚åšä¸€ä¸‹æ‰“ä¹±ï¼Œå¤§çº¦5000ï¼Œç¡®ä¿å®ƒå®Œå…¨éšæœºã€‚ç„¶åæˆ‘ä»¬éœ€è¦è¿›è¡Œæ‰¹å¤„ç†ï¼Œè®°ä½æ‰€æœ‰è¯„è®ºçš„é•¿åº¦å¯èƒ½ä¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›è¡Œå¡«å……ï¼Œä½¿ç”¨å¡«å……æ‰¹å¤„ç†æ¥å®ç°ã€‚
- en: Specifying the batch size 32ã€‚And then we specify padded shapes and on newer
    tenflow versionsã€‚ you don't need to do thisã€‚But for older onesï¼Œ I think before
    2ã€‚2 perhapsã€‚ then you need to specify thisã€‚And we needï¼Œ we're gonna specify none
    here because we don't know the review lengthã€‚ and then we're gonna specify just
    an empty tupleã€‚ And that's going to be for the the label because it's only going
    to be an integerã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡å®šæ‰¹å¤„ç†å¤§å°ä¸º32ã€‚ç„¶åæˆ‘ä»¬æŒ‡å®šå¡«å……å½¢çŠ¶ï¼Œåœ¨æ›´æ–°çš„TensorFlowç‰ˆæœ¬ä¸­ï¼Œä½ ä¸éœ€è¦è¿™æ ·åšã€‚ä½†åœ¨æ—§ç‰ˆæœ¬ä¸­ï¼Œæˆ‘æƒ³åœ¨2.2ä¹‹å‰å¯èƒ½éœ€è¦è¿™æ ·åšã€‚æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡ŒæŒ‡å®šä¸ºNoneï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“è¯„è®ºçš„é•¿åº¦ï¼Œç„¶åæˆ‘ä»¬åªæŒ‡å®šä¸€ä¸ªç©ºå…ƒç»„ã€‚è¿™å°†ç”¨äºæ ‡ç­¾ï¼Œå› ä¸ºå®ƒåªä¼šæ˜¯ä¸€ä¸ªæ•´æ•°ã€‚
- en: Okayï¼Œ so now that we have thatï¼Œ we can do the same thing for the test setã€‚ but
    we're not gonna need to map it and shuffle itã€‚ so we can just doã€‚This right hereã€‚
    mapping it through the encode map function and then padded batchã€‚32ã€‚ and againï¼Œ
    setting the shapesã€‚ which shouldn't beenï¼Œ you shouldn't have to do thisã€‚ but if
    you're using older versionsã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æµ‹è¯•é›†åšåŒæ ·çš„äº‹æƒ…ã€‚ä½†æˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œæ˜ å°„å’Œæ‰“ä¹±ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥è¿™æ ·åšã€‚é€šè¿‡ç¼–ç æ˜ å°„å‡½æ•°ï¼Œç„¶åè¿›è¡Œå¡«å……æ‰¹å¤„ç†32ã€‚å†æ¬¡è®¾ç½®å½¢çŠ¶ï¼Œè™½ç„¶ä¸åº”è¯¥éœ€è¦è¿™æ ·åšï¼Œä½†å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ—§ç‰ˆæœ¬ã€‚
- en: you might have toã€‚ So I'm just including it hereã€‚ I'm gonna do this quicklyã€‚
    and I'm gonna create a modelã€‚ We've actually done this exactly in the previous
    videoã€‚Alrightã€‚ so pretty quicklyï¼Œ we're just creating a very simple modelã€‚ and
    we're actually just doing the embeddingï¼Œ and then we're doing a global average
    pulling across those embeddingsã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½éœ€è¦ã€‚æ‰€ä»¥æˆ‘åœ¨è¿™é‡ŒåŒ…å«å®ƒã€‚æˆ‘ä¼šå¿«é€Ÿå®Œæˆè¿™ä¸ªã€‚æˆ‘å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­å®é™…ä¸Šå°±åšè¿‡è¿™ä¸ªã€‚å¥½çš„ï¼Œæ‰€ä»¥ç›¸å½“å¿«ï¼Œæˆ‘ä»¬åªæ˜¯åˆ›å»ºäº†ä¸€ä¸ªéå¸¸ç®€å•çš„æ¨¡å‹ï¼Œå®é™…ä¸Šåªæ˜¯åšåµŒå…¥ï¼Œç„¶åå¯¹è¿™äº›åµŒå…¥åšå…¨å±€å¹³å‡æ± åŒ–ã€‚
- en: the outputs from those embeddingsã€‚ and then we're just mapping that through
    a dense layer and then output node for one single oneã€‚ And then we're specifying
    a binary cross entropy because we just have two classes with the autumn atom optimizerã€‚
    Alrightï¼Œ so hopefully there are no errors And this should runã€‚Alrightï¼Œ and it
    doesn'tã€‚ So let's seeã€‚ textã€‚All rightï¼Œ so we actually got there pretty early in
    this build vocabulary functionã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åµŒå…¥çš„è¾“å‡ºã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¨ å¯†å±‚æ˜ å°„ï¼Œæœ€åè¾“å‡ºä¸€ä¸ªèŠ‚ç‚¹ã€‚æ¥ç€æˆ‘ä»¬æŒ‡å®šäº†äºŒå…ƒäº¤å‰ç†µï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªç±»ï¼Œä½¿ç”¨çš„æ˜¯è‡ªé€‚åº”åŠ¨é‡ä¼˜åŒ–å™¨ã€‚å¥½çš„ï¼Œå¸Œæœ›æ²¡æœ‰é”™è¯¯ï¼Œè¿™ä¸ªåº”è¯¥å¯ä»¥è¿è¡Œã€‚å¥½çš„ï¼Œä½†å®ƒæ²¡æœ‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æ–‡æœ¬ã€‚å¥½çš„ï¼Œæˆ‘ä»¬å®é™…ä¸Šåœ¨è¿™ä¸ªæ„å»ºè¯æ±‡è¡¨çš„å‡½æ•°ä¸­å¾—åˆ°äº†ç›¸å½“æ—©çš„ç»“æœã€‚
- en: So let's see what we got fromã€‚ Yeahï¼Œ so here wrote text do Nã€‚ we should have
    the reviewã€‚So we want to convert that toumpï¼Œ we want to make a lowercaseã€‚ and
    then we want to tokenize the actual reviewã€‚So hopefully let's see if that runsã€‚Alrightã€‚
    so pretty amazingly this actually ran so what we can see is that we gotã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¾—åˆ°äº†ä»€ä¹ˆã€‚æ˜¯çš„ï¼Œè¿™é‡Œå†™çš„æ˜¯æ–‡æœ¬åšNã€‚æˆ‘ä»¬åº”è¯¥æœ‰è¯„è®ºã€‚æˆ‘ä»¬æƒ³æŠŠå®ƒè½¬æ¢ä¸ºå°å†™ï¼Œç„¶åæƒ³å¯¹å®é™…çš„è¯„è®ºè¿›è¡Œæ ‡è®°åŒ–ã€‚æ‰€ä»¥å¸Œæœ›è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦å¯ä»¥è¿è¡Œã€‚å¥½çš„ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™å®é™…ä¸ŠæˆåŠŸäº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°äº†ã€‚
- en: let's see 97% after 15 e box in the training and then on the test set we got
    88% but I mean it doesn't really matter what performance we had the important
    thing is that it actually works so one thing is that when building the vocabularyã€‚
    perhaps you could change it so that it just takes the I don't knowã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹åœ¨è®­ç»ƒä¸­ç»è¿‡15ä¸ªå›åˆåè¾¾åˆ°äº†97%ï¼Œè€Œåœ¨æµ‹è¯•é›†ä¸Šå¾—åˆ°äº†88%ã€‚ä½†æˆ‘è§‰å¾—æˆ‘ä»¬è¡¨ç°å¦‚ä½•å¹¶ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯å®ƒå®é™…ä¸Šå¯ä»¥è¿è¡Œã€‚å› æ­¤åœ¨æ„å»ºè¯æ±‡è¡¨æ—¶ï¼Œæˆ–è®¸ä½ å¯ä»¥ä¿®æ”¹å®ƒï¼Œè®©å®ƒä»…ä»…ä½¿ç”¨ã€‚
- en: top 1000 words and then you wouldn't have to set this threshold and I think
    that should be pretty easy to implement as well and as I said in the beginning
    how this works is going to be different for every for every data set because this
    structure is going to be different so I want to give an example of also if you
    would have some kind of different structure so let's see I have I have a test
    example with three CSV files and it's actually let's see it's actually the exact
    sameã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å‰1000ä¸ªå•è¯ï¼Œç„¶åä½ å°±ä¸éœ€è¦è®¾ç½®è¿™ä¸ªé˜ˆå€¼ï¼Œæˆ‘è®¤ä¸ºè¿™åº”è¯¥å¾ˆå®¹æ˜“å®ç°ã€‚æ­£å¦‚æˆ‘ä¸€å¼€å§‹æ‰€è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯¹äºæ¯ä¸ªæ•°æ®é›†æ¥è¯´éƒ½ä¸åŒï¼Œå› ä¸ºç»“æ„ä¼šæœ‰æ‰€ä¸åŒï¼Œæ‰€ä»¥æˆ‘æƒ³ç»™å‡ºä¸€ä¸ªä¸åŒç»“æ„çš„ä¾‹å­ã€‚æ¯”å¦‚è¯´ï¼Œæˆ‘æœ‰ä¸€ä¸ªæµ‹è¯•ç¤ºä¾‹ï¼ŒåŒ…å«ä¸‰ä¸ªCSVæ–‡ä»¶ï¼Œå®é™…ä¸Šå®ƒä»¬æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚
- en: '![](img/fd1d96600c664ffac3a740a72c447896_5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_5.png)'
- en: '![](img/fd1d96600c664ffac3a740a72c447896_6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_6.png)'
- en: And bring itï¼Œ it's also the IMDB data it's just that we have a few examples
    on one fileã€‚ we have a few examples on another fileï¼Œ and then we have another
    few examples on a third file So what you can do in that scenario if you have it
    split up into you know several CSV filesã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å¸¦ä¸Šå®ƒï¼Œè¿™ä¹Ÿæ˜¯IMDBæ•°æ®ï¼Œåªæ˜¯æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­æœ‰å‡ ä¸ªç¤ºä¾‹ï¼Œåœ¨å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ä¹Ÿæœ‰å‡ ä¸ªç¤ºä¾‹ï¼Œç„¶ååœ¨ç¬¬ä¸‰ä¸ªæ–‡ä»¶ä¸­è¿˜æœ‰å‡ ä¸ªç¤ºä¾‹ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœä½ å°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªCSVæ–‡ä»¶ï¼Œä½ å¯ä»¥è¿™æ ·åšã€‚
- en: which might be very common in practice if you're working with large datasetsã€‚![](img/fd1d96600c664ffac3a740a72c447896_8.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶å¯èƒ½æ˜¯éå¸¸å¸¸è§çš„ã€‚![](img/fd1d96600c664ffac3a740a72c447896_8.png)
- en: '![](img/fd1d96600c664ffac3a740a72c447896_9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_9.png)'
- en: Then let's do justï¼Œ I import system and see that exitã€‚ so it doesn't run the
    code belowã€‚ Then what you could do is you could specify the file namesã€‚ So in
    this caseï¼Œ that'sã€‚Test example1 that CSV test example2 that CSvï¼Œ and then test
    example 3 do CSvã€‚So what we could do is we could simply do data set is TF data
    text lineã€‚Data set of those file namesã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®©æˆ‘ä»¬åšä¸€ä¸‹ï¼Œæˆ‘å¯¼å…¥ç³»ç»Ÿï¼Œç„¶åçœ‹ä¸€ä¸‹é€€å‡ºã€‚æ‰€ä»¥å®ƒä¸ä¼šè¿è¡Œä¸‹é¢çš„ä»£ç ã€‚ç„¶åä½ å¯ä»¥æŒ‡å®šæ–‡ä»¶åã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†åˆ«æ˜¯test example1é‚£CSVã€test
    example2é‚£CSVï¼Œç„¶åtest example3é‚£CSVã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç®€å•åœ°åšdata setæ˜¯TFæ•°æ®æ–‡æœ¬è¡Œæ•°æ®é›†ï¼Œæ–‡ä»¶åçš„æ•°æ®é›†ã€‚
- en: and it's going to work out all of itã€‚ So it's gonna going to be like you had
    them all in one CSV fileã€‚ which is pretty amazingã€‚ So that is one thing you can
    doã€‚ We can justï¼Œ to make sure that it worksã€‚ We can just print lineã€‚ğŸ˜Šï¼ŒAnd as you
    can seeï¼Œ it printed all of themï¼Œ including theã€‚The the first one right hereã€‚ and
    it's actually printing them for all of themã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”ä¸€åˆ‡éƒ½ä¼šæ­£å¸¸å·¥ä½œã€‚æ‰€ä»¥å°±åƒä½ æŠŠå®ƒä»¬éƒ½æ”¾åœ¨ä¸€ä¸ªCSVæ–‡ä»¶ä¸­ä¸€æ ·ï¼Œè¿™çœŸæ˜¯å¤ªç¥å¥‡äº†ã€‚è¿™å°±æ˜¯ä½ å¯ä»¥åšçš„ä¸€ä»¶äº‹ã€‚æˆ‘ä»¬å¯ä»¥åªæ˜¯ä¸ºäº†ç¡®ä¿å®ƒæ­£å¸¸å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥æ‰“å°è¡Œã€‚ğŸ˜Šï¼Œæ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå®ƒæ‰“å°äº†æ‰€æœ‰çš„ï¼ŒåŒ…æ‹¬è¿™é‡Œçš„ç¬¬ä¸€ä¸ªã€‚å®é™…ä¸Šï¼Œå®ƒä¸ºæ‰€æœ‰çš„éƒ½æ‰“å°äº†ã€‚
- en: So maybe what you would want to doã€‚Is remove this right here for those CSV filesã€‚But
    so that is one way that is quite convenient for the if you have multiple filesã€‚Now
    one thing is that you might want to do different preprocessing on all of those
    CSV filesã€‚ they might not be equally equally in structured like they were here
    and then what you could do is you could specify just data1 is TF data text line
    data set and then we could do test example one that CSV and then let's do skip
    one just for the first row but perhaps you would do something you know in general
    you would do dot map and then preprocess one if if you would have sort of different
    processing so let's just copy thatã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä¹Ÿè®¸ä½ æƒ³è¦åšçš„æ˜¯ï¼Œä¸ºé‚£äº›CSVæ–‡ä»¶ç§»é™¤è¿™ä¸ªã€‚ä½†æ˜¯è¿™å¯¹äºå¤šä¸ªæ–‡ä»¶æ¥è¯´æ˜¯ä¸€ç§ç›¸å½“æ–¹ä¾¿çš„æ–¹æ³•ã€‚ç°åœ¨ï¼Œæœ‰ä¸€ç‚¹æ˜¯ä½ å¯èƒ½æƒ³å¯¹æ‰€æœ‰è¿™äº›CSVæ–‡ä»¶è¿›è¡Œä¸åŒçš„é¢„å¤„ç†ã€‚å®ƒä»¬çš„ç»“æ„å¯èƒ½ä¸åƒè¿™é‡Œé‚£æ ·å®Œå…¨ç›¸åŒï¼Œç„¶åä½ å¯ä»¥æŒ‡å®šdata1ä¸ºTFæ•°æ®æ–‡æœ¬è¡Œæ•°æ®é›†ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¤„ç†test
    example1é‚£CSVï¼Œç„¶åæˆ‘ä»¬å¯ä»¥è·³è¿‡ç¬¬ä¸€è¡Œï¼Œä½†ä¹Ÿè®¸ä½ ä¼šåšä¸€äº›ï¼Œæ¯”å¦‚é€šå¸¸ä½ ä¼šåšdot mapï¼Œç„¶åé¢„å¤„ç†ä¸€ä¸‹ï¼Œå¦‚æœä½ æœ‰ä¸åŒçš„å¤„ç†æ–¹å¼ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±å¤åˆ¶ä¸€ä¸‹ã€‚
- en: For all of themã€‚ So we're gonna do 2ï¼Œ3ï¼Œ2ï¼Œ3ã€‚And then to combine them when you've
    now processed them differentlyã€‚ depending on the structure of those individual
    CSV fileï¼Œ you would do something like data isã€‚Data set1 dot concatenate and then
    data set2 and then concatenate data set 3ã€‚ and in this way we now obtain the entire
    data set againã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å°†åš2ï¼Œ3ï¼Œ2ï¼Œ3ã€‚ç„¶ååœ¨ä½ å¤„ç†å®ƒä»¬ä¹‹åï¼Œç»“åˆå®ƒä»¬ï¼Œå…·ä½“å–å†³äºè¿™äº›å•ç‹¬çš„CSVæ–‡ä»¶çš„ç»“æ„ï¼Œä½ å¯ä»¥åšä¸€äº›ï¼Œæ¯”å¦‚dataæ˜¯dataset1
    dot concatenateï¼Œç„¶ådataset2ï¼Œç„¶åconcatenate dataset 3ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å†æ¬¡è·å¾—æ•´ä¸ªæ•°æ®é›†ã€‚
- en: so if we do four line in data and then print that lineï¼ŒWe get the entire data
    setã€‚Which is including all of those three CSV fileã€‚ Alrightï¼Œ so that is if you
    would haveã€‚ So this would be example if you have multiple filesã€‚And againï¼Œ let's
    do import thisã€‚This this start exitã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœæˆ‘ä»¬åœ¨æ•°æ®ä¸­åšå››è¡Œç„¶åæ‰“å°é‚£è¡Œï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ•´ä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ‰€æœ‰è¿™ä¸‰ä¸ªCSVæ–‡ä»¶ã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯å¦‚æœä½ æœ‰å¤šä¸ªæ–‡ä»¶çš„æƒ…å†µã€‚å†è¯´ä¸€æ¬¡ï¼Œè®©æˆ‘ä»¬å¯¼å…¥è¿™ä¸ªã€‚è¿™å¼€å§‹é€€å‡ºã€‚
- en: '![](img/fd1d96600c664ffac3a740a72c447896_11.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_11.png)'
- en: So let's say we had a kind of translation data setã€‚So I'm actually going to
    create this ourã€‚ we're just going to do English and then we're going to do Swedish
    al right so here we have them we have English CSV and then we have Swedish CSVã€‚
    Alrightï¼Œ so let's do a language1 and let's just do I love tuna I love potatoã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å‡è®¾æˆ‘ä»¬æœ‰ä¸€ç§ç¿»è¯‘æ•°æ®é›†ã€‚å®é™…ä¸Šï¼Œæˆ‘å°†åˆ›å»ºè¿™ä¸ªï¼Œæˆ‘ä»¬åªæ˜¯åšè‹±è¯­ï¼Œç„¶åæˆ‘ä»¬åšç‘å…¸è¯­ï¼Œå¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰å®ƒä»¬ï¼Œæˆ‘ä»¬æœ‰è‹±è¯­CSVï¼Œç„¶åæˆ‘ä»¬æœ‰ç‘å…¸è¯­CSVã€‚å¥½çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬åšlanguage1ï¼Œç„¶åæˆ‘ä»¬å°±è¯´æˆ‘çˆ±é‡‘æªé±¼ï¼Œæˆ‘çˆ±åœŸè±†ã€‚
- en: '![](img/fd1d96600c664ffac3a740a72c447896_13.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_13.png)'
- en: I love baconã€‚ Then we can do languageï¼Œ tooã€‚In Swedishã€‚ So you wouldnt people
    both to understand its probably yeahï¼Œ got what did we write tunaã€‚Toun fikã€‚ yog
    and scar Potisï¼Œ yog and scarã€‚Baconï¼Œ so now we have those threeï¼Œ rightï¼Œ so we haveã€‚![](img/fd1d96600c664ffac3a740a72c447896_15.png)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çˆ±åŸ¹æ ¹ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åšè¯­è¨€ï¼Œç‘å…¸è¯­ã€‚æ‰€ä»¥ä½ ä¸ä¼šå¸Œæœ›äººä»¬éƒ½èƒ½ç†è§£ï¼Œå¯èƒ½æ˜¯çš„ï¼Œæˆ‘ä»¬å†™çš„æ˜¯ä»€ä¹ˆï¼Œé‡‘æªé±¼ã€‚Toun fikã€‚yog å’Œ scar Potisï¼Œyog
    å’Œ scarã€‚åŸ¹æ ¹ï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰è¿™ä¸‰æ ·ï¼Œå¯¹å§ï¼Œæˆ‘ä»¬æœ‰ã€‚![](img/fd1d96600c664ffac3a740a72c447896_15.png)
- en: Just three examples in this caseï¼Œ but how we would load themã€‚Is we canï¼Œ first
    of allã€‚We can specify some tokenizerï¼Œ you knowï¼Œ as we did previouslyã€‚Which would
    be TFDS do features do text tokenizerã€‚Then we would do English is TF do data doc
    text line data setã€‚And we would have that English do CSvã€‚ and then for the Swedishã€‚Wem
    would have TF dataã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œåªæ˜¯ä¸‰ä¸ªç¤ºä¾‹ï¼Œä½†æˆ‘ä»¬å°†å¦‚ä½•åŠ è½½å®ƒä»¬ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šä¸€äº›åˆ†è¯å™¨ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„ã€‚ä½¿ç”¨ TFDS åšç‰¹å¾å¤„ç†å’Œæ–‡æœ¬åˆ†è¯å™¨ã€‚ç„¶åæˆ‘ä»¬å¤„ç†è‹±è¯­æ˜¯ TF
    çš„æ•°æ®æ–‡æ¡£æ–‡æœ¬è¡Œæ•°æ®é›†ã€‚æˆ‘ä»¬ä¼šæœ‰é‚£ä¸ªè‹±è¯­åš CSvã€‚å¯¹äºç‘å…¸è¯­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ TF æ•°æ®ã€‚
- en: text line data setï¼Œ Swedish dot TSVã€‚And then what you could do is you can do
    Dï¼Œ D Sã€‚Our data set is T of that dataï¼Œ that data set that zipã€‚And then you can
    do English and then Swedishã€‚So now we could do four English and Swedish in data
    setã€‚ and then we can just skip the first row againã€‚Then we could do something
    like printã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬è¡Œæ•°æ®é›†ï¼Œç‘å…¸ç‚¹ TSVã€‚ç„¶åä½ å¯ä»¥åš Dï¼ŒD Sã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯é‚£ä¸ªæ•°æ®çš„ Tï¼Œé‚£ä¸ªæ•°æ®é›†çš„ zipã€‚ç„¶åä½ å¯ä»¥å¤„ç†è‹±è¯­å’Œç‘å…¸è¯­ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®é›†ä¸­åšå››ä¸ªè‹±è¯­å’Œç‘å…¸è¯­ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å†æ¬¡è·³è¿‡ç¬¬ä¸€è¡Œã€‚æ¥ç€æˆ‘ä»¬å¯ä»¥åšç±»ä¼¼æ‰“å°çš„æ“ä½œã€‚
- en: tokenizer do tokenizeã€‚English dot Naiã€‚And then we could do tokenizer do tokenize
    Swedish dot numpyã€‚And then we need to also do decode to UTF 8 because we use special
    charactersã€‚ So special characters would be something likeã€‚This right hereã€‚So that's
    just we're why we're doing Decode UTF 8ã€‚But let's run that and just see how it
    looks likeã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: tokenizer åšåˆ†è¯ã€‚è‹±è¯­ç‚¹ Naiã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åš tokenizer åšåˆ†è¯ ç‘å…¸ç‚¹ numpyã€‚ç„¶åæˆ‘ä»¬è¿˜éœ€è¦åš UTF 8 è§£ç ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦ã€‚æ‰€ä»¥ç‰¹æ®Šå­—ç¬¦ä¼šæ˜¯ç±»ä¼¼è¿™æ ·çš„ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦åš
    UTF 8 è§£ç ã€‚ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸‹ï¼Œçœ‹çœ‹ç»“æœå¦‚ä½•ã€‚
- en: And then we can see that it's splitting those examples that we hadã€‚ which this
    is one pairï¼Œ rightã€‚ This is the translation of this first oneã€‚And what you would
    do need to do then is you would need to do sort ofã€‚ first of allï¼Œ you need a vocabularyã€‚And
    then you would need for each languageã€‚Then you would need to do tokenize and miracleize
    wordsã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ­£åœ¨åˆ†å‰²æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé…å¯¹ï¼Œå¯¹å§ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªçš„ç¿»è¯‘ã€‚æ¥ä¸‹æ¥ä½ éœ€è¦åšçš„æ˜¯ï¼Œä½ éœ€è¦é¦–å…ˆå‡†å¤‡ä¸€ä¸ªè¯æ±‡è¡¨ã€‚ç„¶åä½ éœ€è¦ä¸ºæ¯ç§è¯­è¨€è¿›è¡Œåˆ†è¯å’Œå¥‡è¿¹åŒ–å•è¯ã€‚
- en: And so this is hopefully what I've shown you how to do so you would be relatively
    confident that you can be able to use those twoã€‚ and then we need to do padded
    batch to get batchesã€‚U and so onã€‚ and then we also need toï¼Œ you knowã€‚ create a
    model so create modelã€‚Andã€‚The model in this case is probably something like a
    sequence sequence model that might be a a bit too advanced for you to implementã€‚
    or it could even be a transformer networkã€‚And I haven't covered any of those because
    I think they're a little bit too advancedã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯æˆ‘å¸Œæœ›å‘ä½ å±•ç¤ºçš„å†…å®¹ï¼Œä½ åº”è¯¥ç›¸å¯¹è‡ªä¿¡èƒ½å¤Ÿä½¿ç”¨è¿™ä¸¤ä¸ªã€‚ç„¶åæˆ‘ä»¬éœ€è¦åšå¡«å……æ‰¹æ¬¡æ¥è·å–æ‰¹æ¬¡ã€‚ç­‰ç­‰ã€‚ç„¶åæˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œåˆ›å»ºæ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹åœ¨è¿™ç§æƒ…å†µä¸‹å¯èƒ½ç±»ä¼¼äºåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œè¿™å¯èƒ½å¯¹ä½ æ¥è¯´æœ‰ç‚¹è¿‡äºå¤æ‚ã€‚æˆ–è€…å®ƒç”šè‡³å¯èƒ½æ˜¯ä¸€ä¸ªå˜æ¢å™¨ç½‘ç»œã€‚æˆ‘æ²¡æœ‰æ¶‰åŠè¿™äº›ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºå®ƒä»¬æœ‰ç‚¹è¿‡äºé«˜çº§ã€‚
- en: And is beyond this videoã€‚ So what we wanted to focus on in this video which
    hopefully has been as clear as I can make it is how you would load custom data
    sets for textã€‚ using this textline dataset and hopefully I've also shown you some
    different ways depending on how your dataset is structured So I've said this before
    no data set is exactly the sameã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¶…å‡ºäº†æœ¬è§†é¢‘çš„èŒƒå›´ã€‚æˆ‘ä»¬åœ¨è¿™ä¸ªè§†é¢‘ä¸­æƒ³è¦é‡ç‚¹å…³æ³¨çš„å†…å®¹ï¼Œå¸Œæœ›èƒ½å°½é‡æ¸…æ™°ï¼Œå°±æ˜¯å¦‚ä½•åŠ è½½è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®é›†ã€‚ä½¿ç”¨è¿™ä¸ªæ–‡æœ¬è¡Œæ•°æ®é›†ï¼Œå¸Œæœ›æˆ‘ä¹Ÿå‘ä½ å±•ç¤ºäº†ä¸€äº›ä¸åŒçš„æ–¹æ³•ï¼Œå…·ä½“å–å†³äºä½ çš„æ•°æ®é›†ç»“æ„ã€‚æ‰€ä»¥æˆ‘ä¹‹å‰è¯´è¿‡ï¼Œæ²¡æœ‰å“ªä¸ªæ•°æ®é›†æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚
- en: All the data is going to be differently structuredã€‚ So how you load it is going
    to depend on your datasetã€‚ but hopefully you've learned some principles of sort
    of the general pattern of how you should load the data that you can apply to your
    own projectã€‚ So that's it for this videoï¼Œ hopefully you found the video useful
    Thank you so much for watching and I hope to see you in the next videoã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ•°æ®çš„ç»“æ„éƒ½å°†æœ‰æ‰€ä¸åŒã€‚æ‰€ä»¥åŠ è½½å®ƒçš„æ–¹æ³•å°†å–å†³äºä½ çš„æ•°æ®é›†ã€‚ä½†æ„¿ä½ å­¦åˆ°äº†ä¸€äº›åŠ è½½æ•°æ®çš„ä¸€èˆ¬åŸåˆ™ï¼Œè¿™äº›åŸåˆ™å¯ä»¥åº”ç”¨åˆ°ä½ è‡ªå·±çš„é¡¹ç›®ä¸­ã€‚è§†é¢‘åˆ°æ­¤ä¸ºæ­¢ï¼Œå¸Œæœ›ä½ è§‰å¾—è¿™ä¸ªè§†é¢‘æœ‰ç”¨ï¼Œéå¸¸æ„Ÿè°¢ä½ çš„è§‚çœ‹ï¼Œå¸Œæœ›ä¸‹ä¸ªè§†é¢‘è§ã€‚
- en: '![](img/fd1d96600c664ffac3a740a72c447896_17.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd1d96600c664ffac3a740a72c447896_17.png)'
