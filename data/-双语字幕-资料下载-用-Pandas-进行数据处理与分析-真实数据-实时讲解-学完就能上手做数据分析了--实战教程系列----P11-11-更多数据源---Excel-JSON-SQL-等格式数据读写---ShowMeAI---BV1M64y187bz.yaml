- en: 【双语字幕+资料下载】用 Pandas 进行数据处理与分析！真实数据&实时讲解，学完就能上手做数据分析了！＜实战教程系列＞ - P11：11）更多数据源
    - Excel、JSON、SQL 等格式数据读写 - ShowMeAI - BV1M64y187bz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】用 Pandas 进行数据处理与分析！真实数据&实时讲解，学完就能上手做数据分析了！＜实战教程系列＞ - P11：11）更多数据源
    - Excel、JSON、SQL 等格式数据读写 - ShowMeAI - BV1M64y187bz
- en: Hey there。 how's it going， everybody。 In this video。 we're gonna be learning
    how to read and write data to different sources。 So we'll learn how to read and
    write data using CV files， Excel files， Json and also SQL databases。 Now， in this
    series so far。 we've been reading data from CV files。 But in data science。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，大家好，最近怎么样？在这个视频中，我们将学习如何读取和写入不同来源的数据。所以我们将学习如何使用 CV 文件、Excel 文件、Json 以及 SQL
    数据库进行数据的读取和写入。到目前为止，在这个系列中，我们一直在从 CV 文件中读取数据。但在数据科学中。
- en: there are so many different ways for data to be stored。 So by the end of this
    video。 you should be able to get your data to and from pandas， no matter what
    data format you're using。 Now， if you're watching this video because you're looking
    for how to read and write a specific file format。 Then I'll be sure to add timestamps
    in the description section below to where we read and write from each different
    format。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储的方式有很多种。因此，在这个视频结束时，你应该能够将数据进出 pandas，无论你使用什么数据格式。现在，如果你观看这个视频是因为你在寻找如何读取和写入特定文件格式，那么我会在下面的描述部分添加时间戳，指向我们从每种不同格式读取和写入的地方。
- en: Now， I would like to mention that we do have a sponsor for this series videos。
    And that is brilliant。 So I really want to thank brilliant for sponsoring this
    series。 And it would be great if you all can check them out using the link in
    the description section below and support the sponsors。 And I'll talk more about
    their services in just a bit。 So with that said。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想提到的是我们确实有一个赞助商来支持这一系列视频。那就是 **brilliant**。所以我非常感谢 **brilliant** 赞助这个系列。如果大家能通过下面描述部分的链接查看他们，并支持赞助商，那就太好了。我会在稍后更多地谈论他们的服务。因此，话说回来。
- en: let's go ahead and get started。 Okay， so first， let's look at CV files since
    we've already been。😊。Using these throughout the series we should already be familiar
    with reading data in from CSv since that's what we've been doing so far。 But in
    case this is the first video of the series that you're watching。 let's go over
    this one more time and then we'll also learn how to write to a cv file as well。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。好的，首先，让我们看看 CV 文件，因为我们已经在这一系列中使用过这些😊。我们应该已经熟悉从 CSV 中读取数据，因为到目前为止我们一直在做这个。但如果这是你观看的系列的第一部视频，让我们再复习一遍，然后我们也将学习如何写入
    CV 文件。
- en: So up here towards the top of my notebook here we can see that I'm reading in
    this CSv file and this Cv file is within a data folder that is in the same location
    as this Jupyter notebook on the file system Now if you have a cv file loaded elsewhere
    on the system then you'll need to pass in the full path to that file instead of
    just this relative location that we have here。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在我笔记本的顶部，我们可以看到我正在读取这个 CSV 文件，这个 Cv 文件位于与文件系统中的 Jupyter 笔记本相同位置的一个数据文件夹中。如果你在系统的其他地方加载了一个
    cv 文件，那么你需要传递该文件的完整路径，而不仅仅是我们这里的相对位置。
- en: and we can see that we have different arguments that we can pass in when reading
    our Cv files。 So in this example I'm automatically setting the index to this respondent
    column here which is the respondent ID for each person who answered this survey
    And when I read in the Cv we can see that it sets this data frame equal。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在读取我们的 Cv 文件时可以传递不同的参数。所以在这个例子中，我将索引自动设置为此受访者列，即每个参与此调查的人的受访者 ID。当我读取
    Cv 时，我们可以看到它将这个数据框架设置为相等。
- en: to the data and we can print this data out down here at the bottom。 So that
    is the read CSV method and it allows us to pull data in to pandas。 Now let's learn
    how to write this data back to a CSv So maybe you're gonna make some changes and
    some different analysis here to your data frame and then we want to export this
    back to our file system for later use or so that we can share it with someone
    else or something like that。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到数据，我们可以在底部打印出这些数据。所以这是读取 CSV 的方法，它允许我们将数据导入到 pandas 中。现在让我们学习如何将这些数据写回到 CSV
    中。也许你会对数据框进行一些更改和不同的分析，然后我们想将其导出回文件系统以便后续使用，或者与其他人分享之类的。
- en: So for example， let's filter down for a specific location in this survey you
    know maybe you're doing some analysis for your specific country and you just want
    to see the survey results from that location we've seen this in previous videos。
    but if we want to filter then we can simply say I'll create a filter here and
    just say that I want the country here and I'll grab if the country is equal to
    India so let's say you're doing some analysis and you only want the survey results
    from India So now I'm going create a new data frame here。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以过滤这个调查中的特定位置，可能你正在为你的特定国家进行一些分析，你只想看到来自该位置的调查结果，我们在之前的视频中见过这一点。但是如果我们想过滤，我们可以简单地说我会在这里创建一个过滤器，只要说我想要这里的国家，如果国家等于印度。所以假设你在做一些分析，你只想要来自印度的调查结果，现在我要在这里创建一个新的数据框。
- en: I'll call this Indiadf and do a Df do Lo and pass in filter。 So now if I do
    an Indiadf do head to take a look at the beginning of this new data frame if we
    look over here in the country column then we can see that all of these countries
    here are now set to India so now let's say that we want to export this new filter
    data frame to a CSv file So to do this we can use the two CSv method。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我将称之为 Indiadf 并做一个 Df do Lo 并传入过滤器。所以现在如果我做一个 Indiadf do head 来查看这个新数据框的开头，如果我们在国家列中查看，那么我们可以看到这里所有的国家现在都设置为印度。所以现在假设我们想将这个新的过滤数据框导出到一个
    CSv 文件。因此，为此我们可以使用两个 CSv 方法。
- en: So we can say I'll just say India underscore Df， which is our data frame do2
    underscore CSv and now I'm just going to pass it into that same location in that
    data directory and then I'll just call this modified do cv So if I run this we
    can see that we don't get any errors and now if I go back and look at my file
    system here then I have this modified do cv。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以说我会说印度下划线 Df，这就是我们的数据框 do2 下划线 CSv，现在我将把它传入那个数据目录中的相同位置，然后我将称之为这个修改后的
    do cv。所以如果我运行这个，我们可以看到没有任何错误，现在如果我回去看看我的文件系统，那么我有这个修改后的 do cv。
- en: so if I click on this。Then we can see that this is know a little bunch together
    since it's a CSV file。 a Ros CSsv file that we're looking at。 but we can see that
    we have all of our column names here。 and then the second row should be the first
    result and I can see here that we have India for that country if I look at the
    second result we can see we have India again and India again down here most likely
    I can't see it but you know we can just assume that it's there it's looking good。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我点击这个。那么我们可以看到这现在是一个小块在一起，因为这是一个 CSV 文件。我们正在查看一个 Ros CSsv 文件。但我们可以看到我们有所有的列名在这里。然后第二行应该是第一个结果，我可以看到这里我们有印度作为那个国家，如果我查看第二个结果，我们可以看到我们再次有印度，并且在这里再次有印度，我可能看不到，但你知道我们可以假设它在那里，看起来很好。
- en: and actually there it is right there。 so we can see that we did actually export
    that data frame where we filtered that down to a new CSv file so that was easy
    enough so now let's look at how to read and write to some other formats。 So one
    thing that you might run into is a tabbedlim file these are almost exactly the
    same thing as CSV files。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，它就在那儿。所以我们可以看到我们确实将这个数据框导出到一个新的 CSv 文件中，所以这很简单。现在让我们看看如何读取和写入一些其他格式。所以你可能会遇到一个制表符分隔的文件，这几乎与
    CSV 文件完全相同。
- en: but instead of your data being separated by a comma the data is instead separated
    by tabs so to do this。pas， we're still going to use the same CSV methods that
    we've already seen。 but we're going to pass in a custom separator so we can write
    to a tabbedlim file just by changing the file extension here to do TSV and I'm
    also going to specify a separator argument。 So I'm going to sayep SP。 and then
    you want to pass in your separator。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你的数据不是用逗号分隔，而是用制表符分隔。因此，为此，我们仍将使用我们已经看到的相同的 CSV 方法，但我们将传入一个自定义分隔符，所以我们可以通过将文件扩展名更改为
    TSV 来写入一个制表符分隔的文件，我还将指定一个分隔符参数。所以我会说ep SP。然后你想传入你的分隔符。
- en: Now you can pass in anything here if you want you know a file that is separated
    by hashes or anything。 but commas and tabs are probably the most common。 So I'm
    going to put a backslash T there because that's how we specified tabs in Python。
    And now if I run this cell。 I'm going to go back to our data directory here。 we
    can see that now we have this modified do TSv。 if I click on that then we can
    see that now this looks almost exactly the same as the comma separated file。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想传入任何内容，例如用井号分隔的文件或其他内容，但逗号和制表符可能是最常见的。所以我将在这里放一个反斜杠 T，因为这就是我们在 Python 中指定制表符的方式。现在，如果我运行这个单元，我将返回到我们的数据目录，我们可以看到现在我们有这个修改后的
    do TSv。如果我点击它，我们可以看到这看起来几乎和逗号分隔的文件一样。
- en: but now we have tabs here instead of commas。Now， if you're reading in tab CSV
    files。 then all you need to do is take this SP equal to backslash T。 and you can
    just add that as an argument up here to read CSV， so it's basically the same thing。Okay。
    so now let's move on to some other file formats。 Now。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在我们这里有制表符而不是逗号。现在，如果你正在读取制表符分隔的 CSV 文件，你只需要将 SP 设置为反斜杠 T。你可以将其作为参数添加到这里的
    read CSV，这基本上是相同的。好的，现在让我们继续其他文件格式。
- en: a very popular file format when working with this kind of data is Excel。 Now。
    if we want to write to Excel， then we're going to need to Pip install a couple
    of packages。 So I have my terminal open with the current environment that I am
    using this is my Jupyter notebook running here。 let me grab my other terminal。
    So I have the same environment that I'm using within Jupiter。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理这种数据时，非常流行的文件格式是 Excel。现在，如果我们想写入 Excel，那么我们需要 Pip 安装几个软件包。所以我打开了我的终端，当前环境是我在使用的
    Jupyter notebook。让我抓取我的另一个终端。所以我在 Jupiter 中使用的是相同的环境。
- en: You want to be sure that you're using that same environment so that your Pip
    installing in the right location。 and now we're going to install a couple of packages。
    So first I'm going say Pip install。 and this is Xl W T。 So Xwt will write to an
    older Xls Excel format。 but if you want to write to a newer Excells X Excel format。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你想确保使用相同的环境，以便你的 Pip 安装在正确的位置。现在我们要安装几个软件包。首先，我要说 Pip install。这是 Xl W T。Xwt
    将写入较旧的 Xls Excel 格式。但如果你想写入较新的 Excel 格式。
- en: then we'll also need to install open pi XL。 and you can p install multiple。😊，packageages。
    but just by list them all right here。 And finally， we want， if we want to read
    Excel files。 then we can install the X LRD package。 So I think that is the three
    packages that we're going to need in order to work with Excel files here。 So I'll
    go ahead and install all of those and let those finish。And once those are installed。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们还需要安装 open pi XL。你可以通过列出所有软件包来安装多个。😊，但只需在这里列出它们。最后，如果我们想读取 Excel 文件，那么我们可以安装
    X LRD 包。所以我认为这是我们在处理 Excel 文件时所需的三个软件包。所以我将继续安装所有这些，让它们完成。一旦安装完成。
- en: let's go back to our notebook。 And now let's try to write to an Excel file。
    So to write to an Excel file， I'm just going to write the same modified data frame
    that we have here。 And we are going to use the two underscore Excel method。 and
    this is just as easy as passing in。 let's say I'll save it in that data folder
    again。 I'll call this modified dot Xls X。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的笔记本。现在让我们尝试写入一个 Excel 文件。为了写入 Excel 文件，我将写入我们这里的相同修改后的数据框。我们将使用两个下划线
    Excel 方法。这就像传入一个参数一样简单。比如说，我将把它保存在那个数据文件夹中。我将其命名为 modified dot Xls X。
- en: So I'm going to write to the newer Excel format。 So if I run this。 then it might
    take a second here for this to work because it's actually creating this Excel
    file on the back end。 So let's let this finish and we can tell it's finished when
    this turns from an asterisk to a number here。Okay， so once that's finished， let's
    flip over to our data folder here and we can see that we do have that dot Xlsx
    file。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将写入较新的 Excel 格式。如果我运行这个，它可能需要一段时间来完成，因为它实际上是在后台创建这个 Excel 文件。让我们等它完成，当这个从星号变为数字时，我们就可以知道它完成了。好的，完成后，让我们翻到数据文件夹，我们可以看到确实有那个
    dot Xlsx 文件。
- en: Now this likely won't open up in Jupiter because this is an Excel file。 We can
    see here that we can't open this up in the browser。 We actually need Excel。 So
    let me open up my finder window here。 I have this open down here。 And I am within
    this data folder。 and we can see that we have our modified dot Xsx file here。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这在 Jupiter 中可能无法打开，因为这是一个 Excel 文件。我们可以看到，我们无法在浏览器中打开它。我们实际上需要 Excel。所以让我打开我的查找器窗口。我在这里打开了它。我在这个数据文件夹中，我们可以看到我们有修改后的
    dot Xsx 文件。
- en: Now I don't actually have Excel on this machine。 I have numbers。 So I'm going
    to open this up in numbers。 It should basically be the same on Windows。 but you
    can just open it up with Excel。 Now， again。 this might take a second to open up
    because we do still have a lot of rows here in this data。Okay。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我实际上在这台机器上没有 Excel，我有 Numbers。所以我将在 Numbers 中打开它。它在 Windows 上基本上应该是一样的，但你可以直接用
    Excel 打开。再次说明，这可能需要一秒钟来打开，因为我们这里的数据仍然有很多行。好的。
- en: so we've got this opened up in Excel。 again， I'm on numbers because I'm on a
    Mac and I don't have Excel installed。 but it should open up find in Excel as well。
    Let me zoom in a little bit here。 So we can see and we can format these if we
    need to。 So， for example。 we can change the column sizes here so that all these
    fit in but we can see here that we have our respondent Is。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们已经在 Excel 中打开了这个文件。再次说明，我用的是 Numbers，因为我在 Mac 上，没有安装 Excel。但它在 Excel 中也应该能正常打开。让我放大一点，以便我们可以看到，如果需要，我们可以格式化这些内容。例如，我们可以在这里更改列的大小，使所有内容适应，但我们可以看到我们的回应者是。
- en: if I look over at country， we can see that it did export the filtered data frame
    that we were hoping to export。 So everything looks good here。 Now there are also
    some more advanced things that we can do with Excel as well。 if you're familiar
    with Excel。 then you might know that we have the concept of different sheets where
    we can have multiple spreadsheets and one Excel file。 And if you want to read
    or write to a specific sheet。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我查看国家，我们可以看到它确实导出了我们希望导出的过滤数据框。所以一切看起来都很好。现在，我们还可以对 Excel 做一些更高级的操作。如果你熟悉
    Excel，可能知道我们有不同工作表的概念，可以在一个 Excel 文件中有多个电子表格。如果你想读取或写入特定工作表。
- en: then you can pass in a sheet argument to these methods actually I trying to
    scroll over to my notebook here。Let me scroll down here to the bottom。 So like
    I was saying。 if you want to read or write to a specific sheet， then you can pass
    in a sheet argument to these methods。 And there's also a way to start from different
    columns and rows as well。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你可以将工作表参数传递给这些方法。实际上，我正在尝试滚动到我的笔记本上。让我滚动到最底部。正如我所说的，如果你想读取或写入特定工作表，你可以将工作表参数传递给这些方法。还有一种方法可以从不同的列和行开始。
- en: But I'm not going go into all these little details here。 If you Google this
    method name to Excel。 Then you can find the arguments that you can pass in and
    all the additional details in the documentation。 So for now， let's go ahead and
    move on and see how that we can read in that same Excel file that we just created
    and make sure that this works。 Now， by default， it's going to load in with a default
    index， just like when we read a Cv file。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但我不打算在这里深入探讨这些小细节。如果你在谷歌上搜索这个方法名称与 Excel 相关的内容，你可以找到可以传入的参数和文档中的所有额外细节。所以现在，让我们继续，看看如何读取我们刚创建的同一个
    Excel 文件，并确保这能够正常工作。默认情况下，它会加载一个默认索引，就像我们读取 Cv 文件时一样。
- en: So we'll have to specify that we want our index column to be that respondent
    column。 So to do that。 I'm just going to call this test since we're going to be
    creating a new data frame here from that Excel file that we just created。 And
    we're going to use the read underscore Excel method here。 and now we。I want to
    pass in the location， and I'll just go ahead and copy this here。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要指定我们的索引列是那个回应者列。为了做到这一点，我就叫它测试，因为我们将从刚创建的 Excel 文件中创建一个新的数据框。我们将使用 `read_excel`
    方法，并且我想传入位置，我会复制这个位置。
- en: So that is modified Xl S X on my machine。 And now I'm going to set that index
    column equal to and that was respondent on your data that might be different。
    but I want my index column to be equal to that respondent。 So I'm going to run
    that cell and load that in and then I'm going to look at that test data frame。
    Now before I run this， I'm going to make sure that this finishes processing here
    and that this asterisk goes away again。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我机器上的修改版 Xl S X。现在我要将索引列设置为回应者，在你的数据中可能会不同，但我希望我的索引列等于那个回应者。所以我将运行那个单元格并加载它，然后查看测试数据框。在我运行这个之前，我会确保这个处理完成，并且这个星号消失。
- en: it can take some time because it's actually you know loading in that data from
    Excel now。 which is a little more tricky than loading it in from a CSv。 So now
    if we look at that test data frame， let me just look at the head here instead
    of looking at the whole thing。 So if I look at the head， then we can see that
    we have the same data frame here that we had up here。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些时间，因为它实际上是在从Excel加载数据，这比从CSV加载要复杂一些。那么现在如果我们查看这个测试数据框，让我只看一下头部，而不是查看整个内容。如果我查看头部，那么我们可以看到这里有一个与上面相同的数据框。
- en: So that was exported to Excel and imported correctly。 Okay。 so now let's cover
    some other popular file。Formats now JSson is also really popular for this kind
    of data。 So let's take a look at that。 First， let's write our modified data frame
    to a Json file。 Now for writing to a Json file， then we can use the two Json method。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这被导出到Excel并正确导入了。好的。那么现在我们来介绍一些其他流行的文件格式。现在JSon在这种数据中也非常流行。那么我们来看一下。首先，让我们将修改后的数据框写入一个Json文件。对于写入Json文件，我们可以使用to_json方法。
- en: So you're probably starting to see a pattern here。 These method names are very
    straightforward。 Now this one is a bit different since there are some different
    orientations that we can use for Json。 So just by using the default arguments。
    I can just say so that was India Df do2 underscore Json。 and now I'll pass in
    a file location here。 but instead of an Excel file， we want a Json file。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能开始看到一个模式。这些方法名称非常直接。现在这个稍微有点不同，因为我们可以为Json使用一些不同的方向。因此，只需使用默认参数，我可以说，这就是India_Df.to_json。然后我将传入一个文件位置，但不是Excel文件，而是Json文件。
- en: Now I'm just going to use the default arguments for now。 and then I'll show
    you how we can change this up a bit。 So if I run this。 we can see that ran very
    quickly。 if I go back to my data folder here。 Then now we have this JSson file。
    if I look within here。Okay。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将暂时使用默认参数。然后我会告诉你如何稍微改变一下。所以如果我运行这个。我们可以看到它运行得非常快。如果我返回到我的数据文件夹。那么现在我们有了这个JSon文件。如果我在这里查看。好的。
- en: that took just a second to open up on my machine。 again， we do have a lot of
    data in here。 But if we look in here， then we can see that this is very dictionarylike。
    So we have a main branch key here and then the value for that key are all of the
    responses just for that column And if I was to scroll down here then I would be
    able to find the other keys and the other responses as well。 So by default， this
    is a dictionary like Json。 Now there are also different ways that we can write
    Json files。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这只花了一秒钟就能在我的机器上打开。再说，我们这里确实有很多数据。但是如果我们在这里查看，就会发现这非常像字典。所以我们这里有一个主分支键，然后该键的值是仅针对该列的所有响应。如果我向下滚动，就能找到其他键和其他响应。因此，默认情况下，这就像一个字典类型的Json。现在，我们也可以用不同的方式来编写Json文件。
- en: again， I'm not going to go into every single little detail here。 but let's say
    that we wanted this Json to be list like instead of dictionary like which is how
    it is by default。 So to do this， we can change the orient argument。 So instead
    let's add one here to our arguments。 And I'm going to say Orient is equal to。
    And if we pass in records。And lines equal to true。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我不会深入到每一个细节。但假设我们希望这个Json是列表形式而不是默认的字典形式。那么为了做到这一点，我们可以更改orient参数。所以我们在参数中添加一个。并且我会说orient等于。如果我们传入records，并且lines等于true。
- en: Then this will now make this records like， which is list like。 And this lines
    equal to true。 Let me spell that right。 Well just make each of these on a new
    line。 So it might be a little bit easier to read。 Now， if you want to see the
    exact arguments that you can pass into Orient。 then again， just look up pandas
    to Json method。 and it'll take you to the documentation with all the different
    things that you can pass in here。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这将使其变成记录形式，即列表形式。并且lines等于true。让我拼写正确。好吧，只是让每个都在新的一行上。这样可能更容易阅读。如果你想查看可以传递给orient的确切参数，那么再次查看pandas的to_json方法。它会带你到文档中，那里有所有可以传入的不同内容。
- en: So let me run this。 And now let's go back and reload our Json file to see how
    this looks。 And now what we have here is more list like。 So before we had a single
    dictionary where the values were a list of all of the responses。 But now we have
    one response at a time。 So we have the main branch。 And then So this is actually
    this first one here。 If I scroll down。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我运行这个。现在让我们返回并重新加载我们的Json文件，看看它的样子。现在我们这里有的更像是列表形式。因此在之前我们有一个单一的字典，其中的值是所有响应的列表。但是现在我们一次只有一个响应。所以我们有主分支。然后这实际上是这里的第一个。如果我向下滚动。
- en: we can see that this is the second。😊，This is actually the entire first response。
    So we have the main branch and then that answer and then open source and then
    that answer。 and then so on。 And we can see here that for the country， we have
    India。 and each response within this survey is actually on a different line。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这是第二个。😊这实际上是整个第一个响应。所以我们有主要分支，然后是答案，然后是开源，然后是答案，依此类推。我们在这里看到，对于国家，我们有印度。每个调查中的响应实际上都在不同的行上。
- en: So that's a little bit different than how it was before。 but there's just different
    ways that we can export these Json files depending on your needs。 Okay。 so now
    that we've written our data to JSson files。 Now let's also read this JSson file
    so that we can make sure that we know how that's done as well。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的情况有点不同，但我们可以根据需要以不同的方式导出这些 Json 文件。好的。那么现在我们已经将数据写入 Json 文件。现在让我们也读取这个
    Json 文件，以确保我们知道如何做到这一点。
- en: Now， since we wrote the Json file with these different arguments here。 then
    we need to use those same arguments when we read the data in as well。 So if you're
    reading in Json files and have any issues。 then you might need to play around
    with the different arguments to fit the data that you're trying to read in。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们在这里使用这些不同的参数写了 Json 文件。那么在读取数据时，我们也需要使用相同的参数。所以，如果你在读取 Json 文件时遇到任何问题，那么你可能需要调整不同的参数以适应你尝试读取的数据。
- en: So in this case， I'm just gonna copy this whole line here。 And I'm going say
    test is equal to。And actually， let me just grab this part。And I'll say PDd。 read
    underscore Json。 and now I'll pass in all those arguments here。So we are reading
    the JSON file from this location。 We know that the Orient is list like instead
    of dictionary like and that all of these are on new lines。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我要复制整行。然后我会说 test 等于。实际上，让我先抓取这部分。我会说 pd.read_json，然后将所有这些参数传递在这里。所以我们从这个位置读取
    JSON 文件。我们知道 Orient 是列表而不是字典，并且这些都是在新行上。
- en: And again， depending on your JSsonN data， you might need to go in and change
    these around depending on how your data looks。 So if I run this， then let's see。If
    we have the same data that we exported before。 and it seems like we do， this looks
    exactly like it did whenever we exported this data。 Okay。 so now the last file
    format that we're gonna look at。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，根据你的 JSON 数据，你可能需要根据数据的外观来调整这些设置。所以如果我运行这个，看看。如果我们有之前导出的相同数据。看起来是这样的，这看起来与我们导出数据时完全相同。好的。那么我们要查看的最后一种文件格式。
- en: let's learn how we can read and write data from SQL databases。 Now。 this is
    probably the most complicated simply because you have to have the database set
    up and all of that good stuff。 But for the sake of this video， I'm going to assume
    that you already have a database with the correct credentials to log into that
    database。 So I have a Postgres database set up on my machine that will be reading
    and writing to。 So first。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何从 SQL 数据库读取和写入数据。现在，这可能是最复杂的，因为你需要先设置数据库以及所有相关的配置。但为了本视频的目的，我将假设你已经有一个带有正确登录凭据的数据库。因此，我在我的机器上设置了一个
    Postgres 数据库，将用于读取和写入。
- en: let's see how we would connect to this database。 Now， just like with Excel。
    We're going to need to install a package to do this。 So let me bring up my terminal
    here。 I'll close this numbers file here。 Let's see， let me try to quit out of
    this， actually。 I'll just minimize it。Having trouble shutting down。 Okay。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何连接到这个数据库。现在，就像使用 Excel 一样，我们需要安装一个包来实现这一点。所以让我打开我的终端。我要关闭这个数字文件。让我试着退出这个，实际上，我会把它最小化。关闭时遇到了一些麻烦。好的。
- en: so let me go back to the terminal that I hope opens to where I can install some
    different packages。 And that's my Jupiter notebook。 Where's my other terminal。
    Here we go。 Okay。 so to connect to our database。 We're going to want to install
    SQL alchemy。 And this is a very popular O RM for Python that allows us to more
    easily work with databases。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我回到我希望能打开以安装一些不同包的终端。这是我的 Jupyter notebook。我的另一个终端在哪里？来了。好的。那么要连接到我们的数据库，我们需要安装
    SQL Alchemy。这是一个非常流行的 Python ORM，它使我们更容易处理数据库。
- en: If you don't know what an O RM is， it stands for object relational mapper。 And
    it's just a way for us to use Python objects in order to connect to a database。
    I plan on doing a complete video or a complete series on SQL alchemy in the future。
    But for now。 let's go ahead and just install this。 So this is Pip install S QL。Alchemy。And
    I'll install that。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道什么是 ORM，它是对象关系映射的缩写。这只是我们用 Python 对象连接到数据库的一种方式。我计划将来制作一个关于 SQLAlchemy
    的完整视频或系列教程。但现在，让我们继续安装这个。所以这是 Pip install SQLAlchemy。我将安装它。
- en: And depending on the database that you're using， you might not need to do anything
    else here。 So。 for example， if you're using SQLite or something like that。 But
    since I'm using a Postgres database for this tutorial。 I also need to install
    the psycho P G2 package that allows us to work with Postgres。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的数据库，你可能在这里不需要做其他任何事情。例如，如果你使用的是 SQLite 或类似的东西。但由于我在这个教程中使用的是 Postgres
    数据库，我还需要安装 psycopg2 包，以便我们能够使用 Postgres。
- en: I'm not sure if that's actually how you say that package name。 But that's what
    I've always called it。 So Pip install and to install this package to work with
    Postgres。 it's psycho Pg2 dash binary。 So I'll install that， And with those packages
    installed。 let's go back to our notebook and see if we can connect to this database
    using SQL alchemy。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定这是否是这个包的正确发音，但我一直都是这么称呼它的。所以 Pip install 以安装这个与 Postgres 一起使用的包，它是 psycopg2-binary。所以我将安装它，安装了这些包后，让我们回到我们的笔记本，看看能否使用
    SQLAlchemy 连接到这个数据库。
- en: So first， we're going to want to import everything that we need。 So from。SQL
    alchemy。I'm going to want to import their create engine， and this will allow us
    to connect to the database。 Now I'm also going to want to import psychoPg2。 So
    let me run this cell。 and now that those are imported， we should be able to create
    the engine。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所有需要的内容。所以从 SQLAlchemy 中，我将导入他们的创建引擎，这将允许我们连接到数据库。现在我还需要导入 psycopg2。所以让我运行这个单元，现在这些都已导入，我们应该能够创建引擎。
- en: which is basically our database connection。 And again。 I'm going to assume that
    you've already created this database and have a username and password。 So to create
    this， I can say engine is equal to。And use that create engine function that we
    just imported from SQL alchemy and now we need our Postgres connection string
    Now if you don't know how to make Postgres connection strings then you know they
    have this available on the SQL alchemy site as well let me make sure I spelled
    this correctly that is PostgresqL and then we want to pass in the username and
    password for our database now for my case I just made a user of Db user and a
    password of Db pass now another thing here that I'd like to mention is that you
    probably shouldn't put credentials within code like this I'll leave a link in
    the description section below where I show how in Python you should use you something
    like environment variables or a config file to hide this information but for the
    sake of this tutorial I'm just going to put it directly in here。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是我们的数据库连接。再次假设你已经创建了这个数据库并拥有用户名和密码。为了创建它，我可以说 engine 等于，并使用我们刚刚从 SQLAlchemy
    导入的 create_engine 函数，现在我们需要我们的 Postgres 连接字符串。如果你不知道如何创建 Postgres 连接字符串，他们在 SQLAlchemy
    网站上也有相关信息。让我确保我拼写正确，应该是 PostgresQL，然后我们要传入数据库的用户名和密码。在我的案例中，我只创建了一个名为 Db_user
    的用户和一个密码 Db_pass。还有一点我想提到的是，你可能不应该像这样在代码中放置凭据。我会在下面的描述部分留一个链接，展示如何在 Python 中使用环境变量或配置文件来隐藏这些信息，但为了本教程的方便，我就直接放在这里。
- en: but if you're doing。And in production code， I would highly recommend using environment
    variables so that。 you know you don't expose your username and passwords within
    your code base。 Okay。 so there we have our username and password。 And now the
    database that we want to connect to So this is on local host。 This is on my local
    machine。 It's running on port 5，4，3，2 and now the name of the database。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你在做生产代码，我强烈建议使用环境变量，这样你就不会在代码中暴露用户名和密码。好的，所以我们有了我们的用户名和密码。现在是我们想要连接的数据库。这是在本地主机上。这是在我的本地机器上。它运行在
    5432 端口，现在是数据库的名称。
- en: Now I have Pg admin open here where I can see my databases。 and we can see that
    I've just created an empty database here called sample underscore Db。 So that
    is the database that I'm going to connect to Okay so if I typed everything correctly
    here。 then I should be able to get a connection to that database。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我在这里打开了 Pg 管理器，可以看到我的数据库。我们可以看到我刚刚创建了一个名为 sample underscore Db 的空数据库。这就是我将要连接的数据库。好的，如果我在这里输入的内容正确，那么我应该能够连接到这个数据库。
- en: So now let's try to write our modified database frame to a table in this database。
    and this table doesn't need to currently exist。 by default， it will create this
    table for us。 if it does already exist。 then we'll need to add another argument
    to handle that。But we'll see that in just a second。 So to do this， I can just
    say India underscore Df。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试将修改后的数据库框架写入这个数据库中的一个表。这个表不需要当前存在。默认情况下，它会为我们创建这个表。如果它已经存在，我们需要添加另一个参数来处理。但我们马上就会看到这一点。为了做到这一点，我可以直接说
    India underscore Df。
- en: which is the data frame we want to export。 Then this is to underscore S Q O。
    And now the table that we want to write this data too。 I'm just going to call
    this sample underscore table。 Now， again， this doesn't currently exist。 but it
    should create it。 And now we need to pass in our database connection here。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们想要导出的数据框。接着是 to underscore S Q O。现在我们想要写入此数据的表。我将其称为 sample underscore table。现在，这个表目前不存在，但它应该会被创建。现在我们需要传入我们的数据库连接。
- en: I called mine engine。 So let's pass that in。 And if I run this， let's see if
    this works。Okay。 so we didn't get any errors whenever I read that or whenever
    I wrote that。 But now let's go back to my PG admin here。 and let's see if I can
    see this table。 So first。 I'm just gonna right click and refresh。 I like to do
    that anytime I've made any changes。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我称之为引擎。现在让我们传入它。如果我运行这个，我们来看看它是否有效。好的，所以在我读取或写入时没有出现任何错误。但现在让我们回到我的 PG 管理器，看看我能否看到这个表。首先，我只需右键单击并刷新。我喜欢在进行任何更改时这样做。
- en: And we can see there here that we have a sample table down here。 I'm going to
    right click on that and go to view and edit data and look at all the rows here。And
    we can see it does look like this worked。 I know that this is probably a little
    difficult to see on my screen here。 but we have all of our data written here into
    the database。 Okay。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里看到，我们在下面有一个样本表。我将右键单击它，选择查看和编辑数据，查看所有行。我们可以看到这确实有效。我知道在我的屏幕上可能有点难以看到，但我们的所有数据都写入了数据库。好的。
- en: so that's good that we were able to get this from pandas into SQL。 But now what
    if we updated our data and wanted to rewrite that data to this database。 So let's
    go back to our notebook and see what this would look like。 Now。 if I try to run
    this same line again where we export this to SQL。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以能够将数据从 pandas 导入 SQL 很好。但是如果我们更新了数据并希望将其重写到这个数据库中怎么办？让我们回到我们的笔记本，看看这将是什么样子。现在，如果我尝试再次运行这行，将其导出到
    SQL。
- en: then we're actually going to get it an error because this table already exists。
    If you want to write over a table， then we can add in an additional argument。
    And the argument that we want to add in is called if underscore exists equals。
    And now what we want to do if this table already exists。 Now， in my case。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实际上会得到一个错误，因为这个表已经存在。如果你想覆盖一个表，我们可以添加一个额外的参数。我们想要添加的参数称为 if underscore exists
    等于。现在，如果这个表已经存在，我们该怎么办。现在，在我的情况下。
- en: I'm just going to replace that table with our new data。![](img/77e06e113266309258f73542889b4603_1.png)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把那个表替换为我们的新数据。![](img/77e06e113266309258f73542889b4603_1.png)
- en: But there are also other options as well。 we could have it throw an error we
    could which is what it does by default。 we could also append data to a table。
    So if you're doing like a daily script where you're analyzing information then
    you can just append that daily data to your existing table but for this example。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有其他选项。我们可以让它抛出一个错误，这就是默认情况下的行为。我们也可以将数据追加到一个表中。因此，如果你在进行每日脚本以分析信息，你可以将每日数据追加到现有表中，但对于这个例子来说。
- en: I'm just going to have this replace the table So let's run this and once this
    is finished processing。 then I will go back to PG admin now again， let me come
    up here and refresh this and dig back down into the database and let me close
    this view here and let's see if we still have this data Okay so we can see that
    this worked we were able to rerun that command and it just replaced that data
    that was in that existing table with our new data In this case it was the same。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是要用这个替换表。现在运行这个，处理完成后我会回到PG admin，再次让我上去刷新一下，深入数据库，关闭这个视图，看看我们是否仍然有这个数据。好的，我们可以看到这个有效，我们能够重新运行那个命令，并且用我们的新数据替换了原有表中的数据。在这个情况下，它是相同的。
- en: Da， but that's how you would do that。 Okay， so lastly。 now that we've seen how
    to add our data to a database。 Now let's see how we can read in this same data
    using SQL。 Now if you skip to this part of the video using the timestamps in the
    description sections below。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，但这就是你要怎么做的。好了，最后，现在我们已经看到如何将数据添加到数据库。现在让我们看看如何使用SQL读取相同的数据。如果你跳到视频的这一部分，使用下面描述部分的时间戳。
- en: Then please go back to when we wrote data to our database and see how I set
    up this database connection here because we're going to reuse that same connection
    to read in our data。 Okay， so this is pretty simple now that we actually have
    this database connection set up to do this。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那么请回去查看我们如何将数据写入数据库，并看看我在这里是如何设置这个数据库连接的，因为我们将重用那个连接来读取我们的数据。好了，所以这现在很简单，因为我们实际上已经设置好了这个数据库连接来做到这一点。
- en: we can just say I'll call this SQl underscore Df。 and we will just say PD do
    read underscore SQl and now we want to pass in the table that we're going to read
    from。 And that was sample， underscore table and now pass in our database connection。
    My connection here。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地说我会称之为`SQL_df`。然后我们会说`pd.read_sql`，现在我们想传入我们要读取的表。那个是`sample_table`，现在传入我们的数据库连接。我这里的连接。
- en: I called engine。And also， I'm also going to pass in an index column just like
    we did when we read in our CSv。 So I'll say index column is equal to， and that
    is going to be this respondent row right here for your data that might be different。
    So whatever you want to be your index just pass it in there if you want pandas
    to just do a default index。 then you can just leave this off entirely。 Okay， so
    if I run this then let's look at SQL Df dot head to make sure this worked。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我称之为`engine`。而且，我还会传入一个索引列，就像我们在读取CSV时所做的那样。所以我会说索引列等于，这将是你数据中的这个`respondent`行，可能会有所不同。因此，无论你想要什么作为索引，都可以传入。如果你想让pandas使用默认索引，那么你可以完全省略这个。好的，如果我运行这个，那么让我们看看`SQL_df.head`以确保这个有效。
- en: And we can see that that worked well， We still have the same data frame here
    that we started off with where we filter down these countries to just be the results
    from India。 Now there might be instances where you don't want to load in an entire
    table but you want to run a specific SQL query in order to load in our data to
    do this。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这运行得很好，我们仍然有与开始时相同的数据框，我们将这些国家筛选为仅来自印度的结果。现在可能有一些情况，你不想加载整个表，而是想运行一个特定的SQL查询来加载我们的数据。
- en: we can use the method， read， underscore SQL， underscore query。To run a specific
    S Ql query。 So let me just copy what I did here。And paste this down here。 And
    now instead of reading in this entire table， I'm going to actually run a query
    here。 So I'll do read underscore SQL underscore query。 And now instead of the
    table name here。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用方法`read_sql_query`来运行一个特定的SQL查询。所以让我复制我在这里所做的，并粘贴到这里。现在我不打算读取整个表，而是实际上要在这里运行一个查询。所以我会做`read_sql_query`。现在在这里不再是表名。
- en: I'm actually going pass in a SQL query。 Now I'm just going to load in everything
    here。 So I'll say select star from sample underscore table and everything else
    here is going to be the same。 we're still have we still have our database connection
    and we still want our index column to be equal to respondent。 So this is still
    going to grab all the rows， but if you wanted to customize this。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上要传入一个SQL查询。现在我只是要在这里加载所有内容。所以我会说`select * from sample_table`，其他所有内容都将是相同的。我们仍然有我们的数据库连接，并且我们仍然希望索引列等于`respondent`。所以这仍然会抓取所有行，但如果你想自定义这个。
- en: then you could add in a where clause here to filter this down。 So let me run
    this。 And now let's look at our SQL data frame here。 And we can see that that
    worked as well。 So we load in this data using a SQL query instead of just reading
    in the entire table。 So that can be especially useful。You're working with large
    databases where you only want to load in specific data using a query。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以在这里添加一个 where 子句来过滤数据。让我来运行一下。现在我们来看一下我们的 SQL 数据框。我们可以看到这也成功了。因此，我们使用 SQL
    查询加载这些数据，而不是简单地读取整个表。这在处理大数据库时特别有用，你只想通过查询加载特定数据。
- en: Okay， so we're just about finished up here。 But let me show you one more tip
    before we wrap this up So you may have seen people load in data using a URL instead
    of a specific file for some of the methods that we've looked at before。 and we
    can do that。 all you need to do is you need to be sure that you're using the correct
    method for whatever form of data is on the URL。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们快要完成了。但在结束之前让我再给你一个提示。你可能见过一些人使用 URL 而不是特定文件加载数据，这对于我们之前看过的一些方法是可以的。你只需确保使用正确的方法来处理
    URL 上的数据格式。
- en: So， for example， in my flask and django series， I created a Json file of some
    sample posts for the website that werereating in that series。 And I have that
    Json file on my Github page。 Now， if I wanted to bring that into pandas。 then
    I could simply use the read Json method and then pass in that URL。 I wouldn't
    actually have to download that Json first and then pass it in that way。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我的 Flask 和 Django 系列中，我创建了一个 Json 文件，其中包含该系列网站的一些示例帖子。这个 Json 文件在我的 GitHub
    页面上。如果我想将其引入 pandas，那么我可以简单地使用读取 Json 方法，并传入那个 URL。实际上我不需要先下载那个 Json，然后再以那种方式传入。
- en: So I have this open here。 if you didn't know on Github， you can look at the
    raw files。 So we can see that this is a long。URL here， but I will have this code
    posted in the description section below if you'd like to follow along。 So I'm
    just going to copy this URL。 and this isn't on my file system。 And now let's see
    if we can just load this in。 So I'm going to call this post underscore Df。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里打开这个。如果你不知道，GitHub 上可以查看原始文件。我们可以看到这里有一个很长的 URL，但我会在下面的描述部分发布这段代码，方便你跟进。所以我将复制这个
    URL。这不在我的文件系统上。现在看看我们是否可以直接加载它。我将这个命名为 post_df。
- en: And I'll set this equal to P D dot read underscore Json since this is Json on
    the URL。 If it was Cv。 then you'd want to use read CSv and so on。 So now I can
    just paste in that URL there。 And now let's just run that cell。 And we can see
    that we didn't get any errors。 So let me。Now。I'll look at the head。Of our data
    frame here。 And we can see that I do have my sample posts here。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我将其设置为 P D.dot.read_json，因为这是 URL 上的 Json。如果是 Cv，你需要使用 read_csv，依此类推。现在我可以在这里粘贴那个
    URL。现在让我们运行这个单元格。我们可以看到没有出现错误。现在我来查看一下我们数据框的头部。我们可以看到我确实有我的示例帖子。
- en: These are the sample posts that I used on that website series。 So depending
    on the data in that URL。 you should be able to use the methods that we've seen
    to load in data from a URL just like we did here。 Now， before we end here， I would
    like to thank the sponsor of this video and that is brilliant。 I really enjoy
    the tutorials that brilliant provides and would definitely recommend checking
    them out。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我在那个网站系列中使用的示例帖子。因此，根据那个 URL 中的数据，你应该能够使用我们看到的方法，从 URL 加载数据，就像我们在这里做的一样。在结束之前，我想感谢本视频的赞助商，那就是
    Brilliant。我非常喜欢 Brilliant 提供的教程，绝对推荐你去看看。
- en: brilliant is a problem solving website that helps you understand underlying
    concepts by actively working through guided lessons。 and brilliant would be an
    excellent way to supplement what you learn here with their handson courses。 They
    have some excellent courses and lessons on data science that do a deep dive on
    how to think about and analyze data correctly。 So if you're watching my panda
    series because you're getting into the data science field。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Brilliant 是一个问题解决网站，通过主动完成指导课程来帮助你理解基础概念。Brilliant 是补充你在这里学到的内容的绝佳方式，提供动手课程。他们在数据科学方面有一些优秀的课程和课程，深入探讨如何正确思考和分析数据。因此，如果你在看我的
    Pandas 系列，是因为你想进入数据科学领域。
- en: then I would highly recommend also checking out brilliant and seeing what other
    data science skills you can learn。 They even use Python in their statistics course
    and will quiz you on how to correctly。😊。![](img/77e06e113266309258f73542889b4603_3.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我强烈建议你也去查看 brilliant，看看你可以学习哪些其他的数据科学技能。他们在统计课程中甚至使用 Python，并会考你如何正确使用。😊。![](img/77e06e113266309258f73542889b4603_3.png)
- en: '![](img/77e06e113266309258f73542889b4603_4.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77e06e113266309258f73542889b4603_4.png)'
- en: '![](img/77e06e113266309258f73542889b4603_5.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77e06e113266309258f73542889b4603_5.png)'
- en: '![](img/77e06e113266309258f73542889b4603_6.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77e06e113266309258f73542889b4603_6.png)'
- en: ize the data within the language。 Their guided lessons will challenge you。 but
    you also have the ability to get hints or even solutions if you need them。 It's
    really tailored towards understanding the material。 So to support my channel and
    learn more about brilliant。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在该语言中组织数据。它们的指导课程会挑战你，但你也可以获得提示甚至解决方案，如果你需要的话。它真的很注重理解材料。所以支持我的频道并了解更多关于 brilliant
    的信息。
- en: You can go to brilliant org forms to sign up for free and also the first 200
    people to go to that link will'll get 20% off the annual premium subscription
    and you can find that link in the description section below again。 that's brilliant
    org so I think that's going to do it for this pandas video。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以去 brilliant.org 注册免费账户，并且前 200 位访问该链接的人将获得年度高级订阅 20% 的折扣，你可以在下面的描述部分找到这个链接。再次提醒，那是
    brilliant.org，所以我认为这段 pandas 视频就到这里。
- en: I hope you feel like you got a good idea for how to read and write data from
    multiple different sources。 what we covered here should cover the vast majority
    of file formats that you're going to be seeing and using in the data science field。
    Now I'm probably going to take a break from this pandas series after this video
    and do a few one off videos that I've been wanting to cover。 but I know that there
    are a lot of topics in pandas left to cover and I will get around to those more
    advanced topics in future video。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能对如何从多个不同来源读取和写入数据有一个清晰的认识。我们在这里讨论的内容应该涵盖你在数据科学领域会看到和使用的大多数文件格式。现在，在这段视频之后，我可能会暂时中断这个
    pandas 系列，制作几个我一直想要覆盖的单独视频。但我知道 pandas 还有很多主题需要探讨，我会在未来的视频中涉及那些更高级的话题。
- en: 😊。![](img/77e06e113266309258f73542889b4603_8.png)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/77e06e113266309258f73542889b4603_8.png)
- en: '![](img/77e06e113266309258f73542889b4603_9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77e06e113266309258f73542889b4603_9.png)'
- en: But in the meantime， if you'd like a good source for learning pandas。 then I
    would highly recommend checking out the channel Data school。 That's run by Kevin
    Markham。 and he's done the panda tutorials at Pycon for several years now he didn't
    ask me to suggest his channel or anything like that。 I just think that he does
    a good job。 And his channel is actually completely devoted to pandas and data
    science。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但与此同时，如果你想找到一个学习 pandas 的好资源，那么我强烈建议你查看 Data School 频道。这个频道由 Kevin Markham 运营，他已经在
    Pycon 上做了几年的 pandas 教程，他并没有让我推荐他的频道，我只是认为他做得很好。其实他的频道完全致力于 pandas 和数据科学。
- en: So he's already covered some of the more advanced topics that I do plan to cover
    in future videos。 But if anyone has any questions about what be covered in this
    video then feel free to ask in the comment section below and I'll do my best to
    answer those。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 他已经涵盖了一些我计划在未来视频中讨论的更高级主题。如果有人对本视频中的内容有任何问题，请随时在评论区提问，我会尽力回答。
- en: And if you enjoy these tutorials and would like to support them then there are
    several ways you can do that。 the easiest ways to simply like the video and give
    it a thumbs up and also it's a huge help to share these videos with anyone who
    you think would find them useful And if you have the means you can contribute
    through Patreon and there's a link to that page in the description section below。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这些教程并想要支持他们，还有几种方式可以做到这一点。最简单的方法是给视频点个赞，并分享这些视频给任何你认为会觉得有用的人。如果你有能力的话，可以通过
    Patreon 贡献支持，相关链接在下面的描述部分。
- en: be sure to subscribe for future videos。 and thank you all for watching。![](img/77e06e113266309258f73542889b4603_11.png)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 确保订阅以获取未来的视频。感谢大家的观看。![](img/77e06e113266309258f73542889b4603_11.png)
