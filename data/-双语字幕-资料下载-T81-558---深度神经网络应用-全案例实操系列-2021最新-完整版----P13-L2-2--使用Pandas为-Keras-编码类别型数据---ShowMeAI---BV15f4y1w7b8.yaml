- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P13ï¼šL2.2- ä½¿ç”¨Pandasä¸º Keras
    ç¼–ç ç±»åˆ«å‹æ•°æ® - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P13ï¼šL2.2- ä½¿ç”¨Pandasä¸º Keras
    ç¼–ç ç±»åˆ«å‹æ•°æ® - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Eatonã€‚Welcome to applications of deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ we're going to look at how to deal with
    categorical valuesã€‚ A categorical value is something that is textual and not numericã€‚ğŸ˜Šã€‚These
    require special handling to be inputed into a neural networkã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·ä¼Šé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™æ®µè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•å¤„ç†åˆ†ç±»å€¼ã€‚åˆ†ç±»å€¼æ˜¯æŒ‡æ–‡æœ¬å‹è€Œéæ•°å€¼å‹çš„å†…å®¹ã€‚è¿™äº›éœ€è¦ç‰¹åˆ«å¤„ç†æ‰èƒ½è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚
- en: There are many classic ways of handling thisï¼Œ such as dummy variablesã€‚ howeverã€‚
    we're going to look at a number of different ways to actually encode this type
    of value for a neural network for the latest on my AI course and projectsã€‚ click
    subscribe and the bell next to it to be notified of every new video categorical
    and continuous values are two types of data that you'll run into frequently and
    tabular data setsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šç»å…¸çš„æ–¹æ³•æ¥å¤„ç†è¿™äº›ï¼Œæ¯”å¦‚è™šæ‹Ÿå˜é‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ä¸€äº›ä¸åŒçš„æ–¹æ³•æ¥å®é™…ç¼–ç è¿™ç§ç±»å‹çš„å€¼ï¼Œä»¥ä¾¿ç”¨äºç¥ç»ç½‘ç»œã€‚æœ‰å…³æˆ‘çš„AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»è®¢é˜…åŠæ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ã€‚åˆ†ç±»å€¼å’Œè¿ç»­å€¼æ˜¯ä½ ä¼šé¢‘ç¹é‡åˆ°çš„ä¸¤ç§æ•°æ®ç±»å‹ï¼Œå°¤å…¶åœ¨è¡¨æ ¼æ•°æ®é›†ä¸­ã€‚
- en: '![](img/8a2d61d5d493f9c40cbab3980011f3a5_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a2d61d5d493f9c40cbab3980011f3a5_1.png)'
- en: We can really divide the type of data that you will run intoã€‚In four categories
    in allã€‚al nominal ordinal interval and ratioã€‚Character dataã€‚ So strings you'll
    think of as either nominal or ordinalã€‚ Nominal are just pure class values like
    redï¼Œ greenï¼Œ blueï¼Œ orangeï¼Œ indigoã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥å°†ä½ ä¼šé‡åˆ°çš„æ•°æ®ç±»å‹åˆ†ä¸ºå››ç±»ï¼šåä¹‰å‹ã€åºæ•°å‹ã€åŒºé—´å‹å’Œæ¯”ç‡å‹ã€‚å­—ç¬¦æ•°æ®ã€‚æ‰€ä»¥å­—ç¬¦ä¸²ä½ å¯ä»¥çœ‹ä½œæ˜¯åä¹‰å‹æˆ–åºæ•°å‹ã€‚åä¹‰å‹åªæ˜¯çº¯ç²¹çš„åˆ†ç±»å€¼ï¼Œæ¯”å¦‚çº¢è‰²ã€ç»¿è‰²ã€è“è‰²ã€æ©™è‰²ã€é›è“ã€‚
- en: like colors or something like thatã€‚ Nowï¼Œ colorï¼Œ you might encode that asã€‚As
    RGB values or something else rather than dummies butã€‚Typicallyã€‚A nominal value
    is just something that there's no particular order that you can think of for itã€‚
    You should always try to come up with an orderingï¼Œ if you canã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åƒé¢œè‰²è¿™æ ·çš„ä¸œè¥¿ã€‚ç°åœ¨ï¼Œé¢œè‰²ï¼Œä½ å¯èƒ½ä¼šå°†å…¶ç¼–ç ä¸ºRGBå€¼æˆ–å…¶ä»–å€¼ï¼Œè€Œä¸æ˜¯è™šæ‹Ÿå˜é‡ã€‚ä½†é€šå¸¸æƒ…å†µä¸‹ï¼Œåä¹‰å€¼åªæ˜¯æ²¡æœ‰ç‰¹å®šé¡ºåºçš„ä¸œè¥¿ã€‚ä½ åº”è¯¥å°½é‡æƒ³å‡ºä¸€ä¸ªæ’åºï¼Œå¦‚æœå¯ä»¥çš„è¯ã€‚
- en: And then that makes the nominal value an ordinalã€‚ and ordinals are also textual
    data in your data setsã€‚ but they can beï¼Œ they can be orderedã€‚Now you may have
    occasionally just unbounded text like a note column or a product nameã€‚These are
    neither nominal nor oralï¼Œ I guess they would be nominalã€‚ but you would have way
    too many dummy variablesã€‚ So there's other ways that we'll see when we get it
    a natural language processing that you deal with free form text on the numeric
    data sideã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™ä½¿å¾—åä¹‰å€¼å˜æˆäº†åºæ•°å‹ã€‚åºæ•°å‹ä¹Ÿæ˜¯ä½ æ•°æ®é›†ä¸­çš„æ–‡æœ¬æ•°æ®ï¼Œä½†å®ƒä»¬æ˜¯å¯ä»¥æ’åºçš„ã€‚ç°åœ¨ä½ å¯èƒ½å¶å°”ä¼šæœ‰ä¸€äº›æ²¡æœ‰ç•Œé™çš„æ–‡æœ¬ï¼Œæ¯”å¦‚å¤‡æ³¨åˆ—æˆ–äº§å“åç§°ã€‚è¿™äº›æ—¢ä¸æ˜¯åä¹‰å‹ä¹Ÿä¸æ˜¯åºæ•°å‹ï¼Œæˆ‘æƒ³å®ƒä»¬ä¼šæ˜¯åä¹‰å‹ï¼Œä½†ä½ ä¼šæœ‰å¤ªå¤šè™šæ‹Ÿå˜é‡ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬å¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†æ—¶ï¼Œä¼šçœ‹åˆ°å…¶ä»–å¤„ç†è‡ªç”±æ–‡æœ¬çš„æ–¹æ³•ã€‚
- en: There's interval and ratioã€‚ These two types really are rarely handled differentlyã€‚
    But interval basically our numeric values that have no defined startï¼Œ you can
    think of temperatureã€‚ In my example down thereï¼Œ you would never say yesterday
    was twice as hot as todayã€‚ because there's really noï¼Œ I meanï¼Œ short of Kelvinã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰åŒºé—´å‹å’Œæ¯”ç‡å‹ã€‚è¿™ä¸¤ç§ç±»å‹åœ¨å¤„ç†ä¸Šå¾ˆå°‘æœ‰æ‰€ä¸åŒã€‚ä½†åŒºé—´å‹åŸºæœ¬ä¸Šæ˜¯æ²¡æœ‰å®šä¹‰èµ·ç‚¹çš„æ•°å€¼ï¼Œä½ å¯ä»¥æƒ³è±¡æˆæ¸©åº¦ã€‚åœ¨æˆ‘ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œä½ ä¸ä¼šè¯´æ˜¨å¤©çš„æ¸©åº¦æ˜¯ä»Šå¤©çš„ä¸¤å€ï¼Œå› ä¸ºå…¶å®æ²¡æœ‰ï¼Œé™¤éæ˜¯å¼€å°”æ–‡ã€‚
- en: there's really no defined start for how low a temperature can goã€‚ Fahrenheit
    was originally createdã€‚ assuming that zero was absolute0 andã€‚That was wrongï¼Œ but
    that would have been aã€‚That would have been a good example of making it ratioã€‚
    So a ratio is a numeric value that has clearly defined startã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°å€¼æ•°æ®è€Œè¨€ï¼Œæ¸©åº¦çš„æœ€ä½å€¼å¹¶æ²¡æœ‰æ˜ç¡®çš„èµ·ç‚¹ã€‚åæ°åº¦æœ€åˆå‡è®¾é›¶æ˜¯ç»å¯¹é›¶åº¦ï¼Œè€Œè¿™æ˜¾ç„¶æ˜¯é”™è¯¯çš„ï¼Œä½†è¿™æœ¬å¯ä»¥æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ¯”ç‡çš„ä¾‹å­ã€‚æ‰€ä»¥æ¯”ç‡æ˜¯ä¸€ä¸ªæœ‰æ˜ç¡®èµ·ç‚¹çš„æ•°å€¼ã€‚
- en: You could say that one car is going twice as fast as the second because miles
    per hourã€‚ the speed of a carã€‚ the lowest it can go a 0ã€‚ So it has a defined starting
    pointã€‚ I meanã€‚ I guess technically negative 10 could be going backwardsï¼Œ10 miles
    an hourï¼Œ but we're notã€‚ we're not considering that for this exampleã€‚ So let's
    first look at how do we encode continuous valuesã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è¯´ä¸€è¾†è½¦çš„é€Ÿåº¦æ˜¯å¦ä¸€è¾†è½¦çš„ä¸¤å€ï¼Œå› ä¸ºæ˜¯ä»¥è‹±é‡Œæ¯å°æ—¶æ¥è®¡ã€‚è½¦é€Ÿï¼Œæœ€ä½å¯ä»¥æ˜¯0ã€‚å› æ­¤å®ƒæœ‰ä¸€ä¸ªæ˜ç¡®çš„èµ·å§‹ç‚¹ã€‚æˆ‘æ˜¯è¯´ï¼Œä»æŠ€æœ¯ä¸Šè®²ï¼Œè´Ÿ10å¯ä»¥å€’é€€10è‹±é‡Œæ¯å°æ—¶ï¼Œä½†æˆ‘ä»¬ä¸è€ƒè™‘è¿™ä¸ªä¾‹å­ã€‚æ‰€ä»¥é¦–å…ˆæˆ‘ä»¬æ¥çœ‹å¦‚ä½•å¯¹è¿ç»­å€¼è¿›è¡Œç¼–ç ã€‚
- en: continuous valuesã€‚ sometimes you'll want to normalize themã€‚ Nowã€‚ why you potentially
    need to normalize values is considerï¼Œ you met a friend on campus and they saidã€‚
    ohï¼Œ I didï¼Œ I took this exam yesterdayï¼Œ and I got 60 pointsã€‚ Wellï¼Œ is that a good
    scoreã€‚ is that a bad scoreã€‚ that's completely unnormalizeã€‚ They told you how many
    points they gotã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿ç»­å€¼ï¼Œæœ‰æ—¶ä½ éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œæ ‡å‡†åŒ–ã€‚ç°åœ¨ï¼Œä¸ºä»€ä¹ˆä½ å¯èƒ½éœ€è¦æ ‡å‡†åŒ–å€¼å‘¢ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨æ ¡å›­é‡Œé‡åˆ°ä¸€ä¸ªæœ‹å‹ï¼Œä»–ä»¬è¯´ï¼šâ€œå“¦ï¼Œæˆ‘æ˜¨å¤©å‚åŠ äº†è¿™åœºè€ƒè¯•ï¼Œå¾—äº†60åˆ†ã€‚â€é‚£ä¹ˆï¼Œè¿™ä¸ªåˆ†æ•°æ˜¯å¥½æ˜¯åï¼Ÿå®Œå…¨æ²¡æœ‰æ ‡å‡†åŒ–ã€‚ä»–ä»¬å‘Šè¯‰ä½ ä»–ä»¬å¾—äº†å¤šå°‘åˆ†ã€‚
- en: how many points are available on the entire examã€‚ If there's 100ã€‚ then a score
    of 60 points is really not so goodã€‚ if there were only 60 points availableã€‚On
    that examï¼Œ then that score of 60 is perfectã€‚ Z scores let you do thisã€‚ So Z scores
    normalize a0 z score means that you're exactly at the mean a negative one z score
    means your one standard deviation below the mean Similarlyly a plus one means
    your one standard deviation above So this is kind of like expressing something
    as a percent but it lets you know rather it has the standard deviation baked into
    it so that that makes it more applicable for normalization Now let's look at how
    we could normalize something in the miles per gallon database So we're loading
    the miles per gallon database and we're going to basically take the MPg column
    and normalize it to a z score So notice the MPg is right these are all negative
    numbers and you're not seeing very large one this car here is negative one standardã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä¸ªè€ƒè¯•å¯ç”¨çš„åˆ†æ•°æœ‰å¤šå°‘ã€‚å¦‚æœæ€»åˆ†æ˜¯100ï¼Œé‚£ä¹ˆ60åˆ†å¹¶ä¸ç®—å¥½ï¼›å¦‚æœé‚£åœºè€ƒè¯•åªæœ‰60åˆ†ï¼Œé‚£ä¹ˆ60åˆ†å°±æ˜¯æ»¡åˆ†ã€‚Zåˆ†æ•°è®©ä½ å¯ä»¥è¿™æ ·åšã€‚å› æ­¤ï¼ŒZåˆ†æ•°å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼ŒZåˆ†æ•°ä¸º0æ„å‘³ç€ä½ æ°å¥½åœ¨å‡å€¼ï¼Œ-1çš„Zåˆ†æ•°æ„å‘³ç€ä½ ä½äºå‡å€¼ä¸€ä¸ªæ ‡å‡†å·®ã€‚åŒæ ·ï¼Œ+1çš„Zåˆ†æ•°æ„å‘³ç€ä½ é«˜äºå‡å€¼ä¸€ä¸ªæ ‡å‡†å·®ã€‚è¿™å°±åƒå°†æŸç§ä¸œè¥¿ä»¥ç™¾åˆ†æ¯”çš„å½¢å¼è¡¨è¾¾ï¼Œä½†å®ƒè®©ä½ çŸ¥é“æ˜¯å¦åŒ…å«äº†æ ‡å‡†å·®ï¼Œä½¿å…¶åœ¨æ ‡å‡†åŒ–æ—¶æ›´å…·é€‚ç”¨æ€§ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨æ¯åŠ ä»‘è‹±é‡Œæ•°æ®åº“ä¸­è¿›è¡Œæ ‡å‡†åŒ–ã€‚
- en: viation below the average miles per gallonã€‚ This lets you know very quickly
    which ones are above and below the meanã€‚ Another advantage to doing this is sometimes
    when we want to redact data so make the data hard so it's secret so that you can't
    really tell what's going on with the data yet you can still build models on it
    Z score can be a great way to do this like people's ages if you make that a Z
    scoreã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½äºå¹³å‡æ¯åŠ ä»‘è‹±é‡Œæ•°çš„åå·®ã€‚è¿™è®©ä½ å¾ˆå¿«çŸ¥é“å“ªäº›é«˜äºå’Œä½äºå‡å€¼ã€‚è¿™æ ·åšçš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œå½“æˆ‘ä»¬æƒ³è¦å±è”½æ•°æ®ï¼Œä½¿å…¶å˜å¾—å›°éš¾ä»¥ä¿æŠ¤éšç§æ—¶ï¼Œä½ ä»ç„¶å¯ä»¥åœ¨å…¶ä¸Šæ„å»ºæ¨¡å‹ï¼ŒZåˆ†æ•°å¯ä»¥å¾ˆå¥½åœ°å®ç°è¿™ä¸€ç‚¹ï¼Œæ¯”å¦‚äººçš„å¹´é¾„ï¼Œå¦‚æœä½ æŠŠå¹´é¾„è½¬åŒ–ä¸ºZåˆ†æ•°ã€‚
- en: somebody looking at your data may not guess that that is actually the person's
    age because they'll see values that look just like that miles per gallonã€‚ whereas
    if you see values that range between 18 or 21 and 85 you might guess that that
    is somebody's age and it's useful to redact private information like people's
    ages and other thing So thats that's another sometimes used for Z scores But what's
    good about this is if we were to put every one of these columns into Z scores
    they'd be a lot less readable in a way because you wouldn't see these actual weights
    andã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä½ æ•°æ®çš„äººå¯èƒ½ä¸ä¼šçŒœåˆ°è¿™å®é™…ä¸Šæ˜¯æŸäººçš„å¹´é¾„ï¼Œå› ä¸ºä»–ä»¬ä¼šçœ‹åˆ°ç±»ä¼¼çš„å€¼ï¼Œå¦‚æ¯åŠ ä»‘è‹±é‡Œæ•°ã€‚ç„¶è€Œï¼Œå¦‚æœä½ çœ‹åˆ°çš„å€¼åœ¨18åˆ°21å’Œ85ä¹‹é—´ï¼Œä½ å¯èƒ½ä¼šçŒœæµ‹è¿™æ˜¯æŸäººçš„å¹´é¾„ï¼ŒéšåŒ¿ç§äººä¿¡æ¯ï¼ˆä¾‹å¦‚å¹´é¾„ï¼‰æ˜¯æœ‰ç”¨çš„ã€‚å› æ­¤ï¼ŒZåˆ†æ•°åœ¨æŸäº›æƒ…å†µä¸‹è¢«ç”¨äºæ­¤ã€‚ä½†å¥½çš„ä¸€ç‚¹æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†è¿™äº›åˆ—çš„æ¯ä¸€ä¸ªéƒ½è½¬åŒ–ä¸ºZåˆ†æ•°ï¼Œå®ƒä»¬åœ¨æŸç§ç¨‹åº¦ä¸Šä¼šå˜å¾—ä¸é‚£ä¹ˆå¯è¯»ï¼Œå› ä¸ºä½ çœ‹ä¸åˆ°è¿™äº›å®é™…çš„æƒé‡ã€‚
- en: Things like thatï¼Œ but you would be able to very quickly spot things that are
    above are below the mean Also these will tend to cause these values to be fairly
    evenly distributed about zeroã€‚ which in some cases will help neural networks predict
    power Also you will have all these values in a similar range so the weights of
    the car could very much overwhelm the cylinders of the car which is a much much
    lower number so that's some of the advantages of using Z scores for continuous
    value andcoding Now for categoricalsã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†ä½ ä¼šå¾ˆå¿«å‘ç°å“ªäº›å€¼é«˜äºæˆ–ä½äºå‡å€¼ã€‚æ­¤å¤–ï¼Œè¿™äº›å€¼å¾€å¾€ä¼šåœ¨é›¶é™„è¿‘å‡åŒ€åˆ†å¸ƒï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹ä¼šå¸®åŠ©ç¥ç»ç½‘ç»œé¢„æµ‹åŠŸç‡ã€‚åŒæ—¶ï¼Œä½ ä¼šå‘ç°æ‰€æœ‰è¿™äº›å€¼çš„èŒƒå›´ç›¸ä¼¼ï¼Œå› æ­¤æ±½è½¦çš„é‡é‡å¯èƒ½ä¼šå¤§å¤§è¶…è¿‡æ°”ç¼¸çš„æ•°é‡ï¼Œè¿™ä¸ªæ•°å­—è¦å°å¾—å¤šï¼Œæ‰€ä»¥ä½¿ç”¨Zåˆ†æ•°å¯¹è¿ç»­å€¼è¿›è¡Œç¼–ç æœ‰ä¸€äº›ä¼˜åŠ¿ã€‚ç°åœ¨æ¥çœ‹çœ‹åˆ†ç±»å€¼ã€‚
- en: the tried and true one that you're always going to hear about is dummies and
    if we load the sample data set that we've seen beforeã€‚ this is the data set that
    has the job codes areas incomes predicting which of several products the person'
    is going to buy if we look at this one we can create and we've seen this previously
    in this course we can basically look at how many areas there are there are four
    and we can create the dummyã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç»å¸¸ä¼šå¬åˆ°çš„ç»å…¸æ–¹æ³•æ˜¯è™šæ‹Ÿå˜é‡ï¼Œå¦‚æœæˆ‘ä»¬åŠ è½½ä¹‹å‰çœ‹åˆ°çš„ç¤ºä¾‹æ•°æ®é›†ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«å·¥ä½œä»£ç ã€åŒºåŸŸå’Œæ”¶å…¥ï¼Œç”¨äºé¢„æµ‹ä¸€ä¸ªäººå°†è¦è´­ä¹°çš„å‡ ç§äº§å“ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªæ•°æ®é›†ï¼Œå¯ä»¥åˆ›å»ºè™šæ‹Ÿå˜é‡ï¼Œä¹‹å‰åœ¨è¯¾ç¨‹ä¸­ä¹Ÿè§è¿‡ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šå¯ä»¥æŸ¥çœ‹åŒºåŸŸçš„æ•°é‡ï¼Œæ€»å…±æœ‰å››ä¸ªï¼Œå¯ä»¥åˆ›å»ºè™šæ‹Ÿå˜é‡ã€‚
- en: For you get the AB So for the dummiesï¼Œ what this is doing is this is showing
    you essentially the lookup table for theseã€‚So for the first one areaï¼Œ if it's
    going to be aã€‚The dummyï¼Œ the first dummy will be trueã€‚ The rest falseï¼Œ Similarlylyï¼Œ
    for Bï¼Œ the second andï¼Œ and so onã€‚ Nowã€‚ if we want to actually encode the data
    using that sort of lookup tableï¼Œ we do thisã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè™šæ‹Ÿå˜é‡ï¼Œè¿™å®é™…ä¸Šæ˜¯ä¸ºè¿™äº›å˜é‡å±•ç¤ºæŸ¥æ‰¾è¡¨ã€‚æ‰€ä»¥å¯¹äºç¬¬ä¸€ä¸ªåŒºåŸŸï¼Œå¦‚æœå®ƒæ˜¯ä¸€ä¸ªè™šæ‹Ÿå˜é‡ï¼Œç¬¬ä¸€ä¸ªè™šæ‹Ÿå˜é‡å°†ä¸ºçœŸï¼Œå…¶ä½™ä¸ºå‡ã€‚åŒæ ·åœ°ï¼Œå¯¹äºç¬¬äºŒä¸ªï¼Œä»¥æ­¤ç±»æ¨ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨è¿™ç§æŸ¥æ‰¾è¡¨å¯¹æ•°æ®è¿›è¡Œç¼–ç ï¼Œå°±è¿™æ ·åšã€‚
- en: And these are the dummies that were createdã€‚ So the firstã€‚ the first few rows
    were probably all value Cã€‚ Then we had a value Dã€‚ And we can actually see this
    then byã€‚Running thisï¼Œ all of these first ones where C and similarlyã€‚ C is filled
    in Dï¼Œ C for the areaã€‚ And now we have the dummies actually added to itã€‚ Nowã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯åˆ›å»ºçš„è™šæ‹Ÿå˜é‡ã€‚å› æ­¤å‰å‡ è¡Œå¯èƒ½å…¨æ˜¯å€¼Cã€‚ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªå€¼Dã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œè¿™äº›æ•°æ®çœ‹åˆ°ï¼Œæ‰€æœ‰è¿™äº›ç¬¬ä¸€è¡Œçš„å€¼éƒ½æ˜¯Cã€‚åŒæ ·ï¼ŒCå¡«å……äº†Dï¼ŒåŒºåŸŸä¹Ÿä¸ºCã€‚ç°åœ¨æˆ‘ä»¬å·²å°†è™šæ‹Ÿå˜é‡å®é™…æ·»åŠ åˆ°å…¶ä¸­ã€‚
- en: our ultimate goal is to get this completely numericã€‚ So we need to drop the
    areaã€‚One from thisã€‚ and we do thisã€‚And we can see now that area has been removedã€‚
    So this is one step closer to to getting this into a form that we can actually
    present to a neural networkã€‚ We still have to do a similar encoding on jobã€‚ Nowã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„**æœ€ç»ˆç›®æ ‡**æ˜¯å°†æ•°æ®å®Œå…¨è½¬åŒ–ä¸ºæ•°å­—æ ¼å¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­å»æ‰åŒºåŸŸä¸€ã€‚æˆ‘ä»¬è¿™æ ·åšåï¼Œå¯ä»¥çœ‹åˆ°åŒºåŸŸå·²è¢«ç§»é™¤ã€‚è¿™æ˜¯æˆ‘ä»¬å°†æ•°æ®è½¬åŒ–ä¸ºå¯ä»¥å‘ˆç°ç»™ç¥ç»ç½‘ç»œçš„ä¸€æ­¥ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦å¯¹å·¥ä½œç±»åˆ«è¿›è¡Œç±»ä¼¼çš„ç¼–ç ã€‚
- en: there's other ways to encode for dummy variablesã€‚ There's another way to categoricalsã€‚
    other than dummiesã€‚ You can use something called target encodingã€‚ Nowã€‚ target
    encoding is a little more advanced of a techniqueã€‚ It's commonly used on kgglesã€‚
    So it'sã€‚It's potentially quite powerfulï¼Œ but it's potentially quite dangerous
    because you can easilyã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè™šæ‹Ÿå˜é‡ï¼Œè¿˜æœ‰å…¶ä»–ç¼–ç æ–¹å¼ã€‚é™¤äº†è™šæ‹Ÿå˜é‡ï¼Œè¿˜æœ‰å…¶ä»–åˆ†ç±»æ–¹æ³•ã€‚ä½ å¯ä»¥ä½¿ç”¨ç§°ä¸ºç›®æ ‡ç¼–ç çš„æ–¹å¼ã€‚ç›®æ ‡ç¼–ç æ˜¯ä¸€ç§æ›´é«˜çº§çš„æŠ€æœ¯ï¼Œå¸¸ç”¨äºKaggleç«èµ›ã€‚å› æ­¤ï¼Œå®ƒå¯èƒ½éå¸¸å¼ºå¤§ï¼Œä½†ä¹Ÿå¯èƒ½éå¸¸å±é™©ï¼Œå› ä¸ºä½ å¾ˆå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚
- en: easily overfit with thisï¼Œ so be very careful with itã€‚ You should always be using
    some sort of a holdoutet or a cross validation technique to make sure that you're
    not getting artificially high accuracy in results because you might be overfitã€‚Let's
    create a sample data set here real quickã€‚ We look at this data setã€‚ We have a
    continuous value hereï¼Œ which we actually won't useã€‚ And then two categoricalsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå› æ­¤è¦éå¸¸å°å¿ƒã€‚ä½ åº”è¯¥å§‹ç»ˆä½¿ç”¨æŸç§ç•™å‡ºæ³•æˆ–äº¤å‰éªŒè¯æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç»“æœçš„å‡†ç¡®ç‡ä¸æ˜¯äººä¸ºæé«˜çš„ï¼Œå› ä¸ºä½ å¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚è®©æˆ‘ä»¬å¿«é€Ÿåˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†ã€‚æˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªæ•°æ®é›†ï¼Œæœ‰ä¸€ä¸ªè¿ç»­å€¼ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸ä¼šä½¿ç”¨ï¼Œç„¶åæ˜¯ä¸¤ä¸ªç±»åˆ«å˜é‡ã€‚
- en: categoricals 0 and categorical  oneã€‚ And then yï¼Œ which is the target that we're
    trying to predictã€‚ we could create dummy variables for dogï¼Œ catï¼Œ Wolf and tiã€‚
    Nowï¼Œ againï¼Œ rememberï¼Œ each of theseã€‚ each of these has two classesã€‚ Soã€‚It would
    be ti and Wolf for cat 1 and dog and cat for cat 0ã€‚ What we're going to do if
    we run this line hereï¼Œ this is going to group by cat 0ã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«å˜é‡0å’Œç±»åˆ«å˜é‡1ï¼Œç„¶åæ˜¯ç›®æ ‡å˜é‡yï¼Œæˆ‘ä»¬è¯•å›¾é¢„æµ‹çš„å˜é‡ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºç‹—ã€çŒ«ã€ç‹¼å’Œè™åˆ›å»ºè™šæ‹Ÿå˜é‡ã€‚å†æ¬¡æé†’ï¼Œæ¯ä¸ªç±»åˆ«éƒ½æœ‰ä¸¤ä¸ªç±»ã€‚å› æ­¤ï¼ŒçŒ«1æ˜¯è™å’Œç‹¼ï¼ŒçŒ«0æ˜¯ç‹—å’ŒçŒ«ã€‚å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œè¿è¡Œè¿™ä¸€è¡Œï¼Œå°†æŒ‰çŒ«0è¿›è¡Œåˆ†ç»„ã€‚
- en: On y to look at the meanã€‚ So it's going to tell you what the mean value is of
    the target of y for dog and catã€‚ So we run that cat is 0ã€‚2ã€‚ Dog is 0ã€‚8ã€‚ This is
    what you like to see when you're evaluating these for target encodingã€‚ Fortunatelyï¼Œ
    these two are differentiatedã€‚ This tells you something right thereã€‚ The output
    from thisã€‚Cat or dog is very predictive of what the outputs going to be if this
    was closer to the midpoint between the two targetsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨yä¸ŠæŸ¥çœ‹å‡å€¼ã€‚æ‰€ä»¥å®ƒä¼šå‘Šè¯‰ä½ ç‹—å’ŒçŒ«çš„ç›®æ ‡yçš„å¹³å‡å€¼æ˜¯å¤šå°‘ã€‚æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼ŒçŒ«æ˜¯0.2ï¼Œç‹—æ˜¯0.8ã€‚å½“ä½ è¯„ä¼°è¿™äº›ç›®æ ‡ç¼–ç æ—¶ï¼Œè¿™æ˜¯ä½ å¸Œæœ›çœ‹åˆ°çš„æƒ…å†µã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªæ˜¯å¯ä»¥åŒºåˆ†çš„ã€‚è¿™å‘Šè¯‰ä½ ä¸€äº›äº‹æƒ…ã€‚è¿™ä¸€è¾“å‡ºï¼ŒçŒ«æˆ–ç‹—éå¸¸å…·æœ‰é¢„æµ‹æ€§ï¼Œå¦‚æœè¿™ä¸ªå€¼æ›´æ¥è¿‘ä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„ä¸­ç‚¹ã€‚
- en: so the targets are 0 and oneï¼Œ if this was closer to 0ã€‚5ã€‚Target encoding is probably
    not going to help you on that particular categoryã€‚ Similarlylyï¼Œ dog is 0ã€‚8ã€‚ So
    that'sï¼Œ and since the the target itself is oneï¼Œ these should sum to the targetã€‚
    So essentially what you can think of target encoding doing is very sumã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç›®æ ‡æ˜¯0å’Œ1ï¼Œå¦‚æœè¿™ä¸ªå€¼æ›´æ¥è¿‘0.5ï¼Œç›®æ ‡ç¼–ç å¯èƒ½ä¸ä¼šå¯¹è¿™ä¸ªç‰¹å®šç±»åˆ«æœ‰å¸®åŠ©ã€‚ç±»ä¼¼åœ°ï¼Œç‹—æ˜¯0.8ã€‚æ‰€ä»¥è¿™äº›ç›®æ ‡çš„æ€»å’Œåº”è¯¥ç­‰äºç›®æ ‡å€¼ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šä½ å¯ä»¥è®¤ä¸ºç›®æ ‡ç¼–ç åšçš„å°±æ˜¯éå¸¸ç®€å•çš„æ€»å’Œã€‚
- en: We're going to keep cat at cat 0 as a single columnã€‚ We're not going to blow
    it out to two like we would do with dummy variablesã€‚And we're going to essentiallyã€‚Put
    0ã€‚2 in for all the catsï¼Œ 0ã€‚8 in for all the dogsã€‚And we have now changed that
    into a numberï¼Œ and that number will help to predict the y outã€‚ Nowã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çŒ«ä¿æŒä¸ºcat 0ä½œä¸ºä¸€ä¸ªå•åˆ—ã€‚æˆ‘ä»¬ä¸ä¼šåƒç”¨è™šæ‹Ÿå˜é‡é‚£æ ·æ‹†åˆ†æˆä¸¤åˆ—ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šä¼šç»™æ‰€æœ‰çš„çŒ«èµ‹å€¼0.2ï¼Œç»™æ‰€æœ‰çš„ç‹—èµ‹å€¼0.8ã€‚æˆ‘ä»¬ç°åœ¨å·²ç»å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªæ•°å­—ï¼Œè¿™ä¸ªæ•°å­—å°†æœ‰åŠ©äºé¢„æµ‹yã€‚
- en: this is breaking an absolutely cardinal rule of data scienceã€‚ You are now using
    the target forã€‚Your prediction for before you're actually trainingã€‚That is potentially
    dangerousã€‚ so be very careful with this techniqueï¼Œ and I'll show you kind of how
    we can mitigate that risk a little bit and where this risk comes in is when you
    have a very small number of one classã€‚ we only have one tiger and the tiger is
    zeroã€‚ So the average on the tiger is going to be zeroã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¿åäº†æ•°æ®ç§‘å­¦çš„ç»å¯¹åŸºæœ¬è§„åˆ™ã€‚ä½ ç°åœ¨åœ¨å®é™…è®­ç»ƒä¹‹å‰å°±ä½¿ç”¨äº†ç›®æ ‡è¿›è¡Œé¢„æµ‹ã€‚è¿™æ˜¯æ½œåœ¨çš„å±é™©ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨è¿™ä¸ªæŠ€æœ¯æ—¶è¦éå¸¸å°å¿ƒï¼Œæˆ‘ä¼šå‘ä½ å±•ç¤ºå¦‚ä½•é™ä½è¿™ç§é£é™©ï¼Œè€Œè¿™ç§é£é™©çš„å‡ºç°æ˜¯åœ¨ä½ æœ‰éå¸¸å°æ•°é‡çš„ä¸€ä¸ªç±»åˆ«æ—¶ã€‚æˆ‘ä»¬åªæœ‰ä¸€åªè€è™ï¼Œè€Œè€è™çš„å€¼æ˜¯é›¶ã€‚æ‰€ä»¥è€è™çš„å¹³å‡å€¼å°†æ˜¯é›¶ã€‚
- en: So any place that we replace the tiger with a zeroï¼Œ this one row is going to
    be absolutely correctã€‚Just based on that one columnï¼Œ so it's going to correlate
    100% from Cat one to the targetã€‚The riskier this is is when you have low cardinality
    or low numbers of one particular classã€‚ the way we do that is by keeping count
    of how many of each of these we do and specifying a weight that says how much
    in these low cases will we tend towards the average of y overallã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨ä»»ä½•åœ°æ–¹æˆ‘ä»¬ç”¨é›¶æ›¿æ¢è€è™æ—¶ï¼Œè¿™ä¸€è¡Œå°†ç»å¯¹æ­£ç¡®ã€‚ä»…ä»…åŸºäºè¿™ä¸€åˆ—ï¼Œæ‰€ä»¥å®ƒä¸ç›®æ ‡çš„ç›¸å…³æ€§å°†è¾¾åˆ°100%ã€‚å½“ä½ æœ‰ä½åŸºæ•°æˆ–ç‰¹å®šç±»åˆ«çš„æ•°é‡å¾ˆå°‘æ—¶ï¼Œè¿™ç§æƒ…å†µå°±æ›´åŠ å±é™©ã€‚æˆ‘ä»¬å¤„ç†è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡ï¼Œå¹¶æŒ‡å®šä¸€ä¸ªæƒé‡ï¼Œè¡¨æ˜åœ¨è¿™äº›ä½æ¡ˆä¾‹ä¸­æˆ‘ä»¬å°†æœ‰å¤šå¤§ç¨‹åº¦ä¸Šè¶‹å‘äºyçš„æ•´ä½“å¹³å‡å€¼ã€‚
- en: so we take the overall average of y and we which can be calculated hereã€‚ So
    0ã€‚5 So there's exactly half as many of eachã€‚And for cases like the tiã€‚ we would
    tend very much towards 0ã€‚5 rather than 0ã€‚ So you've got these two factors as part
    of thisã€‚ you have the average for Wolffer for tiï¼Œ but then you also have the overall
    average for whyã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—yçš„æ•´ä½“å¹³å‡å€¼ï¼Œè¿™å¯ä»¥åœ¨è¿™é‡Œè®¡ç®—å‡º0.5ã€‚å› æ­¤æ¯ä¸ªç±»åˆ«çš„æ•°é‡æ­£å¥½æ˜¯ä¸€åŠã€‚å¯¹äºåƒtiè¿™æ ·çš„æƒ…å†µï¼Œæˆ‘ä»¬ä¼šæ›´å€¾å‘äº0.5è€Œä¸æ˜¯0ã€‚æ‰€ä»¥ä½ æœ‰è¿™ä¸¤ä¸ªå› ç´ ä½œä¸ºè¿™ä¸€éƒ¨åˆ†ã€‚ä½ æœ‰Wolfferçš„tiçš„å¹³å‡å€¼ï¼Œä½†ä½ ä¹Ÿæœ‰yçš„æ•´ä½“å¹³å‡å€¼ã€‚
- en: So we calculate a smooth mean so we try to smooth it out and the smoothing factor
    is passed in hereã€‚ You're going to pass in two data frames that you want modifiedã€‚
    The reason I have two data frames is because you're probably going to have a training
    and also a test setã€‚You may want even more than thatã€‚ You can modify the functionã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è®¡ç®—ä¸€ä¸ªå¹³æ»‘çš„å‡å€¼ï¼Œå°è¯•å°†å…¶å¹³æ»‘åŒ–ï¼Œå¹³æ»‘å› å­åœ¨è¿™é‡Œä¼ é€’ã€‚ä½ å°†ä¼ é€’ä¸¤ä¸ªéœ€è¦ä¿®æ”¹çš„æ•°æ®æ¡†ã€‚ä¹‹æ‰€ä»¥æœ‰ä¸¤ä¸ªæ•°æ®æ¡†ï¼Œæ˜¯å› ä¸ºä½ å¯èƒ½ä¼šæœ‰ä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªæµ‹è¯•é›†ã€‚ä½ ç”šè‡³å¯èƒ½éœ€è¦æ›´å¤šã€‚ä½ å¯ä»¥ä¿®æ”¹è¿™ä¸ªå‡½æ•°ã€‚
- en: That's a change that I made from the original function that I copied from Max
    Halfordã€‚ which I've given a link to his original one here Iã€‚Fixed a few things
    and also made that modificationã€‚You pass in the name of the categorical variable
    that you're going to modify what the name of the target is and then the weighting
    if this weight is zeroã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»Max Halfordå¤åˆ¶çš„åŸå§‹å‡½æ•°æ‰€åšçš„æ›´æ”¹ï¼Œæˆ‘åœ¨è¿™é‡Œæä¾›äº†ä»–çš„åŸå§‹é“¾æ¥ã€‚æˆ‘ä¿®å¤äº†ä¸€äº›é—®é¢˜ï¼Œå¹¶è¿›è¡Œäº†ä¿®æ”¹ã€‚ä½ éœ€è¦ä¼ å…¥å°†è¦ä¿®æ”¹çš„åˆ†ç±»å˜é‡çš„åç§°ã€ç›®æ ‡çš„åç§°ä»¥åŠæƒé‡ï¼Œå¦‚æœæƒé‡ä¸ºé›¶ã€‚
- en: Then it's going to essentiallyï¼Œ the risk of overfitting is much higher because
    it is going more towards what the average for each of those individual categories
    isã€‚And if D F2ï¼Œ if you don't have a second data frameï¼Œ just pass inï¼Œ pass in none
    for thatã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿‡æ‹Ÿåˆçš„é£é™©å°†ä¼šæ˜¾è‘—å¢åŠ ï¼Œå› ä¸ºå®ƒæ›´æ¥è¿‘äºæ¯ä¸ªå•ç‹¬ç±»åˆ«çš„å¹³å‡å€¼ã€‚å¦‚æœæ²¡æœ‰ç¬¬äºŒä¸ªæ•°æ®æ¡†DF2ï¼Œå¯ä»¥ä¼ å…¥ç©ºå€¼ã€‚
- en: So when setting the weightï¼Œ it's important to keep in mind that the stronger
    the weight value passed inã€‚ the categories with a smaller number of values will
    tend towards the overall average of yã€‚ which means less chances for overfittingã€‚Weaker
    weight will have a greater potential to overfit Nowã€‚ encoding categorical values
    as ordinalsï¼Œ this is a very strong techniqueã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è®¾ç½®æƒé‡æ—¶ï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œä¼ å…¥çš„æƒé‡å€¼è¶Šå¼ºï¼Œå€¼è¾ƒå°‘çš„ç±»åˆ«ä¼šæ›´å€¾å‘äºyçš„æ•´ä½“å¹³å‡å€¼ï¼Œè¿™æ„å‘³ç€è¿‡æ‹Ÿåˆçš„æœºä¼šæ›´å°ã€‚è¾ƒå¼±çš„æƒé‡åˆ™æœ‰æ›´å¤§çš„è¿‡æ‹Ÿåˆæ½œåŠ›ã€‚ç°åœ¨ï¼Œå°†åˆ†ç±»å€¼ç¼–ç ä¸ºåºæ•°æ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„æŠ€æœ¯ã€‚
- en: no real chance of overfittingã€‚ say you had all of these education levels like
    kindergarten up through postdoctorateã€‚You could create dummy variables for all
    of theseï¼Œ but you're actually losing some information because these are orderedã€‚
    you don't do fifth grade before postdoctorateï¼Œ so you could assign a number 0
    through 20 for each of these and put in that number in place of the dummy variables
    so instead of having 21 of these you will have just the one value that you put
    into thereã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰çœŸæ­£çš„è¿‡æ‹Ÿåˆæœºä¼šã€‚å‡è®¾ä½ æœ‰æ‰€æœ‰è¿™äº›æ•™è‚²æ°´å¹³ï¼Œä»å¹¼å„¿å›­åˆ°åšå£«åã€‚ä½ å¯ä»¥ä¸ºè¿™äº›åˆ›å»ºè™šæ‹Ÿå˜é‡ï¼Œä½†ä½ å®é™…ä¸Šæ˜¯åœ¨å¤±å»ä¸€äº›ä¿¡æ¯ï¼Œå› ä¸ºè¿™äº›æ˜¯æœ‰åºçš„ã€‚ä½ ä¸ä¼šåœ¨åšå£«åä¹‹å‰ä¸Šäº”å¹´çº§ï¼Œå› æ­¤å¯ä»¥ä¸ºæ¯ä¸ªçº§åˆ«åˆ†é…ä¸€ä¸ª0åˆ°20çš„æ•°å­—ï¼Œå¹¶ç”¨è¯¥æ•°å­—ä»£æ›¿è™šæ‹Ÿå˜é‡ï¼Œè¿™æ ·ä½ å°±åªä¼šæœ‰ä¸€ä¸ªå€¼è€Œä¸æ˜¯21ä¸ªã€‚
- en: And then the other thing you may want to do is you may want to change the weighting
    because say a graduate studentã€‚ you might be a graduate student longer than you
    areï¼Œ say in sixth gradeã€‚ one year versus who knows how many years for the graduate
    Thank you for watching this video on categorical valuesã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½ å¯èƒ½è¿˜æƒ³æ”¹å˜æƒé‡ï¼Œå› ä¸ºä¾‹å¦‚ç ”ç©¶ç”Ÿçš„æƒ…å†µã€‚ä½ å¯èƒ½åœ¨ç ”ç©¶ç”Ÿé˜¶æ®µçš„æ—¶é—´æ¯”åœ¨å…­å¹´çº§çš„æ—¶é—´æ›´é•¿ï¼Œä¸€å¹´ä¸è°çŸ¥é“å¤šå°‘å¹´çš„ç ”ç©¶ç”Ÿç»å†ã€‚æ„Ÿè°¢è§‚çœ‹è¿™æ®µå…³äºåˆ†ç±»å€¼çš„è§†é¢‘ã€‚
- en: '![](img/8a2d61d5d493f9c40cbab3980011f3a5_3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a2d61d5d493f9c40cbab3980011f3a5_3.png)'
- en: In the next videoï¼Œ we're going to see how to do other preprocess with pandas
    such as grouping sorting and shuffling of the dataset setã€‚ This content changes
    oftenï¼Œ so subscribe to the channel to stay up to date on this course and other
    topics in artificial intelligenceã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨pandasè¿›è¡Œå…¶ä»–é¢„å¤„ç†ï¼Œå¦‚æ•°æ®é›†çš„åˆ†ç»„ã€æ’åºå’Œæ‰“ä¹±ã€‚è¯¥å†…å®¹ç»å¸¸å˜åŒ–ï¼Œå› æ­¤è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æœ¬è¯¾ç¨‹åŠå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚
