- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P5ï¼šL1.5- Transformerï¼šç¼–ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P5ï¼šL1.5- Transformerï¼šç¼–ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
- en: In this videoï¼Œ we'll study the encoder architectureã€‚An example of a popular
    anchor only architecture is Btï¼Œ which is the most popular model of its kindã€‚Let's
    first start by understanding how it worksã€‚We'll use a small example using three
    wordsã€‚ We use these as inputs and pass them through the encoderã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ç¼–ç å™¨æ¶æ„ã€‚ä¸€ä¸ªæµè¡Œçš„ç‹¬ç«‹æ¶æ„çš„ä¾‹å­æ˜¯Btï¼Œå®ƒæ˜¯è¯¥ç±»å‹ä¸­æœ€å—æ¬¢è¿çš„æ¨¡å‹ã€‚è®©æˆ‘ä»¬å…ˆäº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå°ä¾‹å­ï¼Œä½¿ç”¨ä¸‰ä¸ªå•è¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬é€šè¿‡ç¼–ç å™¨ä¼ é€’ã€‚
- en: We retrieve a numerical representation of each wordã€‚Hereï¼Œ for exampleã€‚ the encode
    converts those three words welcome to NYC in these three sequences of numbersã€‚The
    encoder outputs exactly one sequence of numbers per input wordã€‚This numerical
    representation can also be called a feature vector or a feature tensorã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ£€ç´¢æ¯ä¸ªå•è¯çš„æ•°å€¼è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œç¼–ç å™¨å°†è¿™ä¸‰ä¸ªå•è¯â€œæ¬¢è¿æ¥åˆ°çº½çº¦â€è½¬æ¢ä¸ºè¿™ä¸‰ä¸ªæ•°å­—åºåˆ—ã€‚ç¼–ç å™¨ä¸ºæ¯ä¸ªè¾“å…¥å•è¯è¾“å‡ºä¸€ä¸ªæ•°å­—åºåˆ—ã€‚è¿™ä¸ªæ•°å€¼è¡¨ç¤ºä¹Ÿå¯ä»¥ç§°ä¸ºç‰¹å¾å‘é‡æˆ–ç‰¹å¾å¼ é‡ã€‚
- en: Let's dive in this representationã€‚ It contains one vector per word that was
    passed through the encoderã€‚Each of these vector is a numerical representation
    of the word in questionã€‚The dimension of that vector is defined by the architecture
    of the model for the base bird modelã€‚ it is 768ã€‚These representations contain
    the value of a wordï¼Œ but contextualizedã€‚For exampleã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶è¿™ä¸ªè¡¨ç¤ºã€‚å®ƒåŒ…å«æ¯ä¸ªé€šè¿‡ç¼–ç å™¨ä¼ é€’çš„å•è¯çš„ä¸€ä¸ªå‘é‡ã€‚æ¯ä¸ªå‘é‡éƒ½æ˜¯æ‰€è®¨è®ºå•è¯çš„æ•°å€¼è¡¨ç¤ºã€‚è¯¥å‘é‡çš„ç»´åº¦ç”±æ¨¡å‹çš„æ¶æ„å®šä¹‰ï¼Œå¯¹äºåŸºç¡€Bertæ¨¡å‹ï¼Œå®ƒæ˜¯768ã€‚è¿™äº›è¡¨ç¤ºåŒ…å«äº†å•è¯çš„å€¼ï¼Œä½†åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚ä¾‹å¦‚ã€‚
- en: the vector attributed to the word2 isn't the representation of only the two
    wordã€‚It also takes into account the words around itï¼Œ which we call the contextã€‚As
    in it looks to the left contextï¼Œ the words on the left of the one we studyingã€‚
    hear the word welcomeï¼Œ and the context on the rightï¼Œ here the wordnyCã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†é…ç»™word2çš„å‘é‡ä¸ä»…æ˜¯ä¸¤ä¸ªå•è¯çš„è¡¨ç¤ºã€‚å®ƒè¿˜è€ƒè™‘åˆ°å‘¨å›´çš„å•è¯ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ‰€ç§°çš„ä¸Šä¸‹æ–‡ã€‚å®ƒæŸ¥çœ‹å·¦ä¾§ä¸Šä¸‹æ–‡ï¼Œå³æˆ‘ä»¬ç ”ç©¶çš„å•è¯å·¦è¾¹çš„å•è¯ã€‚åœ¨è¿™é‡Œæ˜¯â€œæ¬¢è¿â€è¿™ä¸ªè¯ï¼Œä»¥åŠå³ä¾§çš„ä¸Šä¸‹æ–‡ï¼Œè¿™é‡Œæ˜¯â€œçº½çº¦â€è¿™ä¸ªè¯ã€‚
- en: and it outputs a value for the word given its contextã€‚It is therefore a contextualized
    valueã€‚ğŸ˜Šã€‚One could say that the vector of 768 values holds the meaning of the word
    within the textã€‚It does this thanks to the self attention mechanismã€‚The self attention
    mechanism relates to different positions or different words in a single sequence
    in order to compute a representation of that sequenceã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¸ºç»™å®šä¸Šä¸‹æ–‡çš„å•è¯è¾“å‡ºä¸€ä¸ªå€¼ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡åŒ–çš„å€¼ã€‚ğŸ˜Šå¯ä»¥è¯´ï¼Œ768ä¸ªå€¼çš„å‘é‡åœ¨æ–‡æœ¬ä¸­æ‰¿è½½äº†å•è¯çš„æ„ä¹‰ã€‚å®ƒæ˜¯é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°çš„ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸å•ä¸ªåºåˆ—ä¸­ä¸åŒä½ç½®æˆ–ä¸åŒå•è¯ç›¸å…³è”ï¼Œä»¥è®¡ç®—è¯¥åºåˆ—çš„è¡¨ç¤ºã€‚
- en: As we've seen beforeï¼Œ this means that the resulting representation of a word
    has been affected by other words in the sequenceã€‚ğŸ˜Šï¼ŒWe won't dive into these specifics
    hereï¼Œ but we' offer some further readings if you want to get a better understanding
    at what happens under the hoodã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è§ï¼Œè¿™æ„å‘³ç€å•è¯çš„æœ€ç»ˆè¡¨ç¤ºå—åˆ°äº†åºåˆ—ä¸­å…¶ä»–å•è¯çš„å½±å“ã€‚ğŸ˜Šæˆ‘ä»¬åœ¨è¿™é‡Œä¸ä¼šæ·±å…¥è¿™äº›ç»†èŠ‚ï¼Œä½†å¦‚æœä½ æƒ³æ›´å¥½åœ°ç†è§£å…¶å†…éƒ¨æœºåˆ¶ï¼Œæˆ‘ä»¬ä¼šæä¾›ä¸€äº›è¿›ä¸€æ­¥çš„é˜…è¯»èµ„æ–™ã€‚
- en: So when should one use an encodeï¼ŸEncodeders can be used as tenderalone models
    in a wide variety of tasksã€‚ğŸ˜Šï¼ŒFor exampleï¼Œ Bertï¼Œ arguably the most famous transformer
    modelï¼Œ is a standalone anchor modelã€‚ and at the time of release it be the state
    of the art in many sequence classification tasksã€‚ question answering tasksï¼Œ and
    masked language modelling to only cite a fewã€‚ğŸ˜Šã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨ç¼–ç å™¨å‘¢ï¼Ÿç¼–ç å™¨å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­ä½¿ç”¨ã€‚ğŸ˜Šä¾‹å¦‚ï¼ŒBertï¼Œæ¯«æ— ç–‘é—®æ˜¯æœ€è‘—åçš„å˜æ¢å™¨æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åŸºå‡†æ¨¡å‹ã€‚åœ¨å‘å¸ƒæ—¶ï¼Œå®ƒåœ¨è®¸å¤šåºåˆ—åˆ†ç±»ä»»åŠ¡ã€é—®ç­”ä»»åŠ¡å’Œæ©è”½è¯­è¨€å»ºæ¨¡ç­‰æ–¹é¢å¤„äºæœ€å…ˆè¿›æ°´å¹³ï¼Œä»…ä¸¾å‡ ä¾‹ã€‚ğŸ˜Š
- en: The idea is that encoders are very powerful at extracting vectors that carry
    meaningful information about a sequenceã€‚This vector can then be handled down the
    road by additional neurons to make sense of themã€‚Let's take a look at some examples
    where encodes really shineã€‚First of allã€‚ mask language modeling or MLMã€‚It's the
    task of predicting a hidden word and a sequence of wordã€‚Hereã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œç¼–ç å™¨åœ¨æå–æºå¸¦åºåˆ—æœ‰æ„ä¹‰ä¿¡æ¯çš„å‘é‡æ–¹é¢éå¸¸å¼ºå¤§ã€‚ç„¶åï¼Œè¿™ä¸ªå‘é‡å¯ä»¥ç”±åç»­ç¥ç»å…ƒè¿›è¡Œå¤„ç†ï¼Œä»¥ç†è§£è¿™äº›ä¿¡æ¯ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›ç¼–ç å™¨è¡¨ç°å‡ºè‰²çš„ä¾‹å­ã€‚é¦–å…ˆï¼Œæ©è”½è¯­è¨€å»ºæ¨¡æˆ–MLMã€‚è¿™æ˜¯é¢„æµ‹åºåˆ—ä¸­éšè—è¯è¯­å’Œå•è¯åºåˆ—çš„ä»»åŠ¡ã€‚åœ¨è¿™é‡Œã€‚
- en: for exampleï¼Œ we have hidden the word between my and isã€‚This is one of the objectives
    with which Bert was trainedã€‚ It was trained to predict hidden words in a sequenceã€‚Encodes
    shine in this scenario in particularã€‚ as bidirectional information is crucial
    hereã€‚If we didn't have the words on the right is Silva and the dotã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬éšè—äº†â€œmyâ€å’Œâ€œisâ€ä¹‹é—´çš„è¯ã€‚è¿™æ˜¯Bertè®­ç»ƒçš„ç›®æ ‡ä¹‹ä¸€ã€‚å®ƒè¢«è®­ç»ƒç”¨äºé¢„æµ‹åºåˆ—ä¸­çš„éšè—è¯ã€‚ç¼–ç å™¨åœ¨è¿™ä¸ªåœºæ™¯ä¸­ç‰¹åˆ«å‡ºè‰²ï¼Œå› ä¸ºåŒå‘ä¿¡æ¯åœ¨è¿™é‡Œè‡³å…³é‡è¦ã€‚å¦‚æœæˆ‘ä»¬æ²¡æœ‰å³è¾¹çš„å•è¯â€œis
    Silvaâ€å’Œå¥å·ã€‚
- en: then there is very little chance that Bt would have been able to identify name
    as the correct wordã€‚The encoder needs to have a good understanding of the sequence
    in order to predict a masked wordã€‚ as even if the text is grammatically correctã€‚It
    does not necessarily make sense in the context of the sequenceã€‚As mentioned earlierï¼Œ
    encodes are good at doing sequence classificationã€‚ğŸ˜Šã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆBtå‡ ä¹æ²¡æœ‰å¯èƒ½èƒ½å¤Ÿè¯†åˆ«â€œnameâ€ä½œä¸ºæ­£ç¡®çš„å•è¯ã€‚ç¼–ç å™¨éœ€è¦å¯¹åºåˆ—æœ‰è‰¯å¥½çš„ç†è§£ï¼Œä»¥ä¾¿é¢„æµ‹ä¸€ä¸ªè¢«å±è”½çš„è¯ï¼Œå› ä¸ºå³ä½¿æ–‡æœ¬åœ¨è¯­æ³•ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œå®ƒåœ¨åºåˆ—çš„ä¸Šä¸‹æ–‡ä¸­ä¹Ÿä¸ä¸€å®šæœ‰æ„ä¹‰ã€‚æ­£å¦‚ä¹‹å‰æåˆ°çš„ï¼Œç¼–ç å™¨æ“…é•¿è¿›è¡Œåºåˆ—åˆ†ç±»ã€‚ğŸ˜Š
- en: Sentiment analysis is an example of sequence classificationã€‚The model's aim
    is to identify the sentiment of a sequenceã€‚It can range from giving a sequence
    a rating from one to five stars if doing review analysis to giving a positive
    or negative rating to a sequenceã€‚ which is what is shown hereã€‚For exampleï¼Œ hereï¼Œ
    given the two sequencesã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ…æ„Ÿåˆ†ææ˜¯åºåˆ—åˆ†ç±»çš„ä¸€ä¸ªä¾‹å­ã€‚æ¨¡å‹çš„ç›®æ ‡æ˜¯è¯†åˆ«åºåˆ—çš„æƒ…æ„Ÿã€‚å®ƒå¯ä»¥ç”¨äºè¯„ä¼°åˆ†ææ—¶ç»™åºåˆ—æ‰“åˆ†ï¼ˆä»ä¸€åˆ°äº”é¢—æ˜Ÿï¼‰ï¼Œæˆ–å¯¹åºåˆ—è¿›è¡Œæ­£é¢æˆ–è´Ÿé¢çš„è¯„ä»·ï¼Œè¿™å°±æ˜¯è¿™é‡Œæ‰€å±•ç¤ºçš„ã€‚ä¾‹å¦‚ï¼Œç»™å®šè¿™ä¸¤ä¸ªåºåˆ—ã€‚
- en: we use the model to compute a prediction and to classify the sequences among
    these two classesã€‚ positive and negativeã€‚While the two sequences are very similar
    containing the same wordsã€‚ the meaning is entirely differentï¼Œ and the encoder
    model is able to grasp that differenceã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹æ¥è®¡ç®—é¢„æµ‹ï¼Œå¹¶å°†åºåˆ—åˆ†ç±»ä¸ºè¿™ä¸¤ç±»ï¼šæ­£é¢å’Œè´Ÿé¢ã€‚è™½ç„¶è¿™ä¸¤ä¸ªåºåˆ—éå¸¸ç›¸ä¼¼ï¼ŒåŒ…å«ç›¸åŒçš„å•è¯ï¼Œä½†æ„ä¹‰å®Œå…¨ä¸åŒï¼Œè€Œç¼–ç å™¨æ¨¡å‹èƒ½å¤ŸæŠ“ä½è¿™ç§å·®å¼‚ã€‚
- en: '![](img/9effa565b433264b98944ee23b41a7f0_1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9effa565b433264b98944ee23b41a7f0_1.png)'
- en: ã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/9effa565b433264b98944ee23b41a7f0_3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9effa565b433264b98944ee23b41a7f0_3.png)'
