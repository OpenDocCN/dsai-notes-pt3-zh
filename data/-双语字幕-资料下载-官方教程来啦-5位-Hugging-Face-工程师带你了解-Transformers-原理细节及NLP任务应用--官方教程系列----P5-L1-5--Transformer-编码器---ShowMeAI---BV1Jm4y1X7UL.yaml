- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P5：L1.5- Transformer：编码器 - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P5：L1.5- Transformer：编码器 - ShowMeAI - BV1Jm4y1X7UL
- en: In this video， we'll study the encoder architecture。An example of a popular
    anchor only architecture is Bt， which is the most popular model of its kind。Let's
    first start by understanding how it works。We'll use a small example using three
    words。 We use these as inputs and pass them through the encoder。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将研究编码器架构。一个流行的独立架构的例子是Bt，它是该类型中最受欢迎的模型。让我们先了解它是如何工作的。我们将使用一个小例子，使用三个单词作为输入，并将它们通过编码器传递。
- en: We retrieve a numerical representation of each word。Here， for example。 the encode
    converts those three words welcome to NYC in these three sequences of numbers。The
    encoder outputs exactly one sequence of numbers per input word。This numerical
    representation can also be called a feature vector or a feature tensor。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索每个单词的数值表示。例如，这里编码器将这三个单词“欢迎来到纽约”转换为这三个数字序列。编码器为每个输入单词输出一个数字序列。这个数值表示也可以称为特征向量或特征张量。
- en: Let's dive in this representation。 It contains one vector per word that was
    passed through the encoder。Each of these vector is a numerical representation
    of the word in question。The dimension of that vector is defined by the architecture
    of the model for the base bird model。 it is 768。These representations contain
    the value of a word， but contextualized。For example。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入研究这个表示。它包含每个通过编码器传递的单词的一个向量。每个向量都是所讨论单词的数值表示。该向量的维度由模型的架构定义，对于基础Bert模型，它是768。这些表示包含了单词的值，但在上下文中。例如。
- en: the vector attributed to the word2 isn't the representation of only the two
    word。It also takes into account the words around it， which we call the context。As
    in it looks to the left context， the words on the left of the one we studying。
    hear the word welcome， and the context on the right， here the wordnyC。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给word2的向量不仅是两个单词的表示。它还考虑到周围的单词，这就是我们所称的上下文。它查看左侧上下文，即我们研究的单词左边的单词。在这里是“欢迎”这个词，以及右侧的上下文，这里是“纽约”这个词。
- en: and it outputs a value for the word given its context。It is therefore a contextualized
    value。😊。One could say that the vector of 768 values holds the meaning of the word
    within the text。It does this thanks to the self attention mechanism。The self attention
    mechanism relates to different positions or different words in a single sequence
    in order to compute a representation of that sequence。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它为给定上下文的单词输出一个值。因此，这是一个上下文化的值。😊可以说，768个值的向量在文本中承载了单词的意义。它是通过自注意力机制实现的。自注意力机制与单个序列中不同位置或不同单词相关联，以计算该序列的表示。
- en: As we've seen before， this means that the resulting representation of a word
    has been affected by other words in the sequence。😊，We won't dive into these specifics
    here， but we' offer some further readings if you want to get a better understanding
    at what happens under the hood。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，这意味着单词的最终表示受到了序列中其他单词的影响。😊我们在这里不会深入这些细节，但如果你想更好地理解其内部机制，我们会提供一些进一步的阅读资料。
- en: So when should one use an encode？Encodeders can be used as tenderalone models
    in a wide variety of tasks。😊，For example， Bert， arguably the most famous transformer
    model， is a standalone anchor model。 and at the time of release it be the state
    of the art in many sequence classification tasks。 question answering tasks， and
    masked language modelling to only cite a few。😊。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么时候应该使用编码器呢？编码器可以作为独立模型在多种任务中使用。😊例如，Bert，毫无疑问是最著名的变换器模型，是一个独立的基准模型。在发布时，它在许多序列分类任务、问答任务和掩蔽语言建模等方面处于最先进水平，仅举几例。😊
- en: The idea is that encoders are very powerful at extracting vectors that carry
    meaningful information about a sequence。This vector can then be handled down the
    road by additional neurons to make sense of them。Let's take a look at some examples
    where encodes really shine。First of all。 mask language modeling or MLM。It's the
    task of predicting a hidden word and a sequence of word。Here。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，编码器在提取携带序列有意义信息的向量方面非常强大。然后，这个向量可以由后续神经元进行处理，以理解这些信息。让我们看看一些编码器表现出色的例子。首先，掩蔽语言建模或MLM。这是预测序列中隐藏词语和单词序列的任务。在这里。
- en: for example， we have hidden the word between my and is。This is one of the objectives
    with which Bert was trained。 It was trained to predict hidden words in a sequence。Encodes
    shine in this scenario in particular。 as bidirectional information is crucial
    here。If we didn't have the words on the right is Silva and the dot。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们隐藏了“my”和“is”之间的词。这是Bert训练的目标之一。它被训练用于预测序列中的隐藏词。编码器在这个场景中特别出色，因为双向信息在这里至关重要。如果我们没有右边的单词“is
    Silva”和句号。
- en: then there is very little chance that Bt would have been able to identify name
    as the correct word。The encoder needs to have a good understanding of the sequence
    in order to predict a masked word。 as even if the text is grammatically correct。It
    does not necessarily make sense in the context of the sequence。As mentioned earlier，
    encodes are good at doing sequence classification。😊。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么Bt几乎没有可能能够识别“name”作为正确的单词。编码器需要对序列有良好的理解，以便预测一个被屏蔽的词，因为即使文本在语法上是正确的，它在序列的上下文中也不一定有意义。正如之前提到的，编码器擅长进行序列分类。😊
- en: Sentiment analysis is an example of sequence classification。The model's aim
    is to identify the sentiment of a sequence。It can range from giving a sequence
    a rating from one to five stars if doing review analysis to giving a positive
    or negative rating to a sequence。 which is what is shown here。For example， here，
    given the two sequences。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是序列分类的一个例子。模型的目标是识别序列的情感。它可以用于评估分析时给序列打分（从一到五颗星），或对序列进行正面或负面的评价，这就是这里所展示的。例如，给定这两个序列。
- en: we use the model to compute a prediction and to classify the sequences among
    these two classes。 positive and negative。While the two sequences are very similar
    containing the same words。 the meaning is entirely different， and the encoder
    model is able to grasp that difference。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模型来计算预测，并将序列分类为这两类：正面和负面。虽然这两个序列非常相似，包含相同的单词，但意义完全不同，而编码器模型能够抓住这种差异。
- en: '![](img/9effa565b433264b98944ee23b41a7f0_1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9effa565b433264b98944ee23b41a7f0_1.png)'
- en: 。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![](img/9effa565b433264b98944ee23b41a7f0_3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9effa565b433264b98944ee23b41a7f0_3.png)'
