- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- æ„ŸçŸ¥å™¨
    - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P7ï¼šL7- æ„ŸçŸ¥å™¨
    - ShowMeAI - BV1wS4y1f7z1
- en: Hiï¼Œ everybodyã€‚ Welcome to a new machine learning from scratch tutorialã€‚ Todayã€‚
    we are going to implement a perceptron using only built and Python modules and
    Nyã€‚ The perceptron can be seen as one single unit of an artificial neural networkã€‚
    So the perceptron is a simplified model of a biological neuronã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°æ–°çš„ä»é›¶å¼€å§‹çš„æœºå™¨å­¦ä¹ æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨å†…ç½®å’ŒPythonæ¨¡å—å®ç°ä¸€ä¸ªæ„ŸçŸ¥å™¨ã€‚æ„ŸçŸ¥å™¨å¯ä»¥çœ‹ä½œæ˜¯äººå·¥ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªå•å…ƒã€‚å› æ­¤ï¼Œæ„ŸçŸ¥å™¨æ˜¯ç”Ÿç‰©ç¥ç»å…ƒçš„ç®€åŒ–æ¨¡å‹ã€‚
- en: and it simulates the behavior of only one cellã€‚ So let's have a look at this
    image hereã€‚ we have one cellã€‚ and our cell gets an inputï¼Œ so it gets input signalsã€‚
    and they are weighted and summed upã€‚ And if the whole input signal then reaches
    a certain thresholdã€‚ Our cell fires a signal and delivers an outputã€‚ So in our
    caseï¼Œ it either fires a1 or a0ã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ¨¡æ‹Ÿäº†ä»…ä¸€ä¸ªç»†èƒçš„è¡Œä¸ºã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå›¾åƒï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç»†èƒã€‚æˆ‘ä»¬çš„ç»†èƒæ¥æ”¶è¾“å…¥ä¿¡å·ï¼Œè¿™äº›ä¿¡å·ç»è¿‡åŠ æƒå¹¶ç›¸åŠ ã€‚å¦‚æœæ•´ä¸ªè¾“å…¥ä¿¡å·è¾¾åˆ°æŸä¸ªé˜ˆå€¼ï¼Œæˆ‘ä»¬çš„ç»†èƒå°±ä¼šå‘å‡ºä¿¡å·å¹¶æä¾›è¾“å‡ºã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒè¦ä¹ˆå‘å‡º1ï¼Œè¦ä¹ˆå‘å‡º0ã€‚ğŸ˜Š
- en: And now if we model this mathematicallyï¼Œ then it looks like thisã€‚ so we have
    our input features and they are multiplied with some weights and then summed upã€‚
    and then we apply an activation function and get our output classã€‚So this is the
    model and now the linear partï¼Œ the linear model simply looks like thisã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¦‚æœæˆ‘ä»¬ä»æ•°å­¦ä¸Šå»ºæ¨¡ï¼Œé‚£ä¹ˆçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚æˆ‘ä»¬æœ‰è¾“å…¥ç‰¹å¾ï¼Œå®ƒä»¬ä¸ä¸€äº›æƒé‡ç›¸ä¹˜å¹¶ç›¸åŠ ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œå¾—åˆ°æˆ‘ä»¬çš„è¾“å‡ºç±»åˆ«ã€‚è¿™å°±æ˜¯æ¨¡å‹ï¼Œçº¿æ€§éƒ¨åˆ†ï¼Œçº¿æ€§æ¨¡å‹çœ‹èµ·æ¥å°±æ˜¯è¿™æ ·ã€‚
- en: so this is just a linear functionã€‚W transpose times x plus Bã€‚ So here we multiply
    and sum up our weights and the biasã€‚ So the bias is the W 0 here in this pictureã€‚And
    after this linear modelã€‚ we apply the activation functionã€‚And in the simplest
    caseã€‚ we simply use the so called unit step functionã€‚ and this is defined as it's
    either one if our input reaches a certainã€‚ a certain threshold or serial otherwiseã€‚So
    in this pictureï¼Œ the threshold is 0ã€‚ So if the input is larger than 0ï¼Œ then the
    output is oneã€‚ and otherwiseï¼Œ it's 0ã€‚And now this is all we need to model the
    outputã€‚ And now the whole output is looks like thisã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚Wçš„è½¬ç½®ä¹˜ä»¥xåŠ ä¸ŠBã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æƒé‡å’Œåç½®ç›¸ä¹˜å¹¶ç›¸åŠ ã€‚å› æ­¤ï¼Œåç½®æ˜¯è¿™ä¸ªå›¾ä¸­çš„W 0ã€‚åœ¨è¿™ä¸ªçº¿æ€§æ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬åº”ç”¨æ¿€æ´»å‡½æ•°ã€‚åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€è°“çš„å•ä½é˜¶è·ƒå‡½æ•°ã€‚å®ƒè¢«å®šä¹‰ä¸ºå¦‚æœæˆ‘ä»¬çš„è¾“å…¥è¾¾åˆ°æŸä¸ªé˜ˆå€¼ï¼Œåˆ™è¾“å‡ºä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œé˜ˆå€¼æ˜¯0ã€‚å› æ­¤ï¼Œå¦‚æœè¾“å…¥å¤§äº0ï¼Œåˆ™è¾“å‡ºä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚ç°åœ¨è¿™å°±æ˜¯æˆ‘ä»¬å»ºæ¨¡è¾“å‡ºæ‰€éœ€çš„å…¨éƒ¨ã€‚æœ€ç»ˆçš„è¾“å‡ºçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚
- en: So firstï¼Œ we apply the linear functionï¼Œ and then we apply the activationation
    functionã€‚And now we have to come up with the weights and the biasã€‚ and for thisã€‚
    we use a simple update rule that is called the Perceptron ruleã€‚So we look at each
    training sample X Iï¼Œ and for each training sampleï¼Œ we then apply the update stepã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åº”ç”¨çº¿æ€§å‡½æ•°ï¼Œç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦è®¾å®šæƒé‡å’Œåç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ›´æ–°è§„åˆ™ï¼Œç§°ä¸ºæ„ŸçŸ¥å™¨è§„åˆ™ã€‚æˆ‘ä»¬æŸ¥çœ‹æ¯ä¸ªè®­ç»ƒæ ·æœ¬X Iï¼Œç„¶åå¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬åº”ç”¨æ›´æ–°æ­¥éª¤ã€‚
- en: and this is defined as the new weight is the old weight plus the delelta weight
    and the deelta weight or delta W is defined as alpha times the actualã€‚Label minus
    the predicted label times the training sample Xã€‚ and here alpha is a learning
    rate between 0 and1ã€‚ So this is just a scaling factorã€‚And now let's have a look
    at what this update will meanã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«å®šä¹‰ä¸ºæ–°æƒé‡ç­‰äºæ—§æƒé‡åŠ ä¸Šå¢é‡æƒé‡ï¼Œè€Œå¢é‡æƒé‡æˆ–delta Wè¢«å®šä¹‰ä¸ºalphaä¹˜ä»¥å®é™…æ ‡ç­¾å‡å»é¢„æµ‹æ ‡ç­¾ä¹˜ä»¥è®­ç»ƒæ ·æœ¬Xã€‚è¿™é‡Œçš„alphaæ˜¯ä»‹äº0å’Œ1ä¹‹é—´çš„å­¦ä¹ ç‡ã€‚æ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ›´æ–°æ„å‘³ç€ä»€ä¹ˆã€‚
- en: So let's have a look at the four possible cases in a two class problemã€‚ So our
    output can be oneã€‚ the actual label can be one and the predictedã€‚Label is also
    oneã€‚ Then the difference is 0ã€‚ So we have no change for our weights hereã€‚And the
    same is if the actual class is 0 and the predicted class is also 0ã€‚ so correctly
    classified and the difference is 0ã€‚ And againï¼Œ no change for our weightsã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹åœ¨ä¸¤ä¸ªç±»åˆ«é—®é¢˜ä¸­å¯èƒ½å‡ºç°çš„å››ç§æƒ…å†µã€‚æˆ‘ä»¬çš„è¾“å‡ºå¯ä»¥æ˜¯1ï¼Œå®é™…æ ‡ç­¾å¯ä»¥æ˜¯1ï¼Œè€Œé¢„æµ‹æ ‡ç­¾ä¹Ÿæ˜¯1ã€‚é‚£ä¹ˆå·®å¼‚ä¸º0ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçš„æƒé‡æ²¡æœ‰å˜åŒ–ã€‚å¦‚æœå®é™…ç±»åˆ«æ˜¯0ï¼Œè€Œé¢„æµ‹ç±»åˆ«ä¹Ÿæ˜¯0ï¼ŒåŒæ ·ä¹Ÿæ˜¯æ­£ç¡®åˆ†ç±»ï¼Œå·®å¼‚ä¸º0ã€‚å› æ­¤ï¼Œæƒé‡æ²¡æœ‰å˜åŒ–ã€‚
- en: But now what happens if we have a misclassificationã€‚ So if the actual class
    is one and the predicted class is 0ã€‚ this means that our weights are too lowã€‚
    and then we see that the difference is oneã€‚ So our weights are increased hereã€‚
    And if the actual class is 0ï¼Œ and the predicted class is 1ã€‚ then our weights are
    too highã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç°åœ¨å¦‚æœæˆ‘ä»¬æœ‰è¯¯åˆ†ç±»ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ‰€ä»¥å¦‚æœå®é™…ç±»åˆ«æ˜¯ 1ï¼Œé¢„æµ‹ç±»åˆ«æ˜¯ 0ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„æƒé‡å¤ªä½ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°å·®å€¼æ˜¯ 1ã€‚å› æ­¤æˆ‘ä»¬çš„æƒé‡åœ¨è¿™é‡Œè¢«å¢åŠ ã€‚å¦‚æœå®é™…ç±»åˆ«æ˜¯
    0ï¼Œé¢„æµ‹ç±»åˆ«æ˜¯ 1ã€‚é‚£ä¹ˆæˆ‘ä»¬çš„æƒé‡å°±å¤ªé«˜äº†ã€‚
- en: and we see that the difference is-1ã€‚ So then our weights are decreasedã€‚ So the
    weights are pushed towards the positive or negative class in case of a misclassificationã€‚And
    this is a simple and intuitive ruleï¼Œ but it worksã€‚And this is all we needã€‚ So
    we look at each training sample and then apply the update ruleã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°å·®å€¼æ˜¯ -1ã€‚å› æ­¤æˆ‘ä»¬çš„æƒé‡è¢«å‡å°‘ã€‚å› æ­¤ï¼Œåœ¨è¯¯åˆ†ç±»çš„æƒ…å†µä¸‹ï¼Œæƒé‡æœç€æ­£ç±»æˆ–è´Ÿç±»ç§»åŠ¨ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•ç›´è§‚çš„è§„åˆ™ï¼Œä½†å®ƒæœ‰æ•ˆã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ‰€éœ€çš„ä¸€åˆ‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æŸ¥çœ‹æ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç„¶ååº”ç”¨æ›´æ–°è§„åˆ™ã€‚
- en: and then we do this a couple of timesã€‚ So we iterate for a certain number of
    iterationsã€‚ and then we have the final weights and are doneã€‚ So this is all we
    need to knowã€‚ and now we can get started and implement itã€‚![](img/47427fb8775e23c1bec39e64630a9c33_1.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æ‰§è¡Œå‡ æ¬¡ã€‚å› æ­¤æˆ‘ä»¬è¿­ä»£ä¸€å®šæ¬¡æ•°ã€‚ç„¶åæˆ‘ä»¬å°±æœ‰äº†æœ€ç»ˆæƒé‡ï¼Œå®Œæˆäº†ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦çŸ¥é“çš„ä¸€åˆ‡ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹å®ç°å®ƒã€‚![](img/47427fb8775e23c1bec39e64630a9c33_1.png)
- en: Soï¼Œ first of allï¼Œ of courseï¼Œ we use numpyã€‚ So we import numpy S and Pã€‚ And then
    we create a class and call it perceptronã€‚Perceptronã€‚And it gets an innate methodã€‚
    of courseã€‚And here it has selfã€‚ and it gets the learning rateã€‚ And I will give
    this a default of 0ã€‚01ã€‚ Then it gets a number of iterationsã€‚ So n itsã€‚ And I will
    also give this a defaultã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å½“ç„¶ä½¿ç”¨ numpyã€‚å› æ­¤æˆ‘ä»¬å¯¼å…¥ numpy S å’Œ Pã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç±»å¹¶ç§°ä¹‹ä¸ºæ„ŸçŸ¥å™¨ã€‚æ„ŸçŸ¥å™¨ã€‚å®ƒæœ‰ä¸€ä¸ªå›ºæœ‰çš„æ–¹æ³•ï¼Œå½“ç„¶ã€‚è¿™é‡Œå®ƒæœ‰è‡ªæˆ‘ï¼Œå¹¶è·å¾—å­¦ä¹ ç‡ã€‚æˆ‘å°†é»˜è®¤å€¼è®¾ä¸º
    0.01ã€‚ç„¶åå®ƒè·å¾—ä¸€ä¸ªè¿­ä»£æ¬¡æ•°ã€‚æ‰€ä»¥ n æ¬¡ã€‚æˆ‘ä¹Ÿä¼šç»™è¿™ä¸ªä¸€ä¸ªé»˜è®¤å€¼ã€‚
- en: Let's say 1000ã€‚ Then I will simply store themã€‚ So I will say self dot L R equals
    learning rate and self dot nã€‚Its equalsï¼Œ and itsã€‚Then we create a the activationation
    functionã€‚ So let's say self dot activationation funã€‚Equalsï¼Œ and now let's create
    this hereï¼Œ andã€‚As I saidã€‚ the activation function is simply the unit step functionã€‚
    So let's call thisã€‚Unit stepã€‚Fkã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ 1000ã€‚ç„¶åæˆ‘ä¼šç®€å•åœ°å­˜å‚¨å®ƒä»¬ã€‚å› æ­¤æˆ‘ä¼šè¯´ self.dot.L R ç­‰äºå­¦ä¹ ç‡ï¼Œself.dot.n.å…¶ equalsï¼Œå’Œå…¶ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºæ¿€æ´»å‡½æ•°ã€‚å› æ­¤å‡è®¾
    self.dot.activationation funã€‚ç­‰äºï¼Œç°åœ¨è®©æˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºå®ƒã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæ¿€æ´»å‡½æ•°å°±æ˜¯å•ä½é˜¶è·ƒå‡½æ•°ã€‚å› æ­¤æˆ‘ä»¬ç§°ä¹‹ä¸ºã€‚Unit stepã€‚Fkã€‚
- en: With self and an input Xï¼Œ andã€‚We could simply say return oneã€‚ if x is larger
    or equal than 0 and else return 0ã€‚Butã€‚We see later that weã€‚ this would only work
    for one singleã€‚Sampleï¼Œ but we see later that we want to apply the activation function
    in the predict method for all the test sampleã€‚ So we want to apply this for a
    N D array as wellã€‚ And for thisã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è‡ªæˆ‘å’Œè¾“å…¥ Xï¼Œä»¥åŠã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´å¦‚æœ x å¤§äºæˆ–ç­‰äº 0ï¼Œåˆ™è¿”å› 1ï¼Œå¦åˆ™è¿”å› 0ã€‚ä½†ã€‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°ï¼Œè¿™åªå¯¹å•ä¸ªæ ·æœ¬æœ‰æ•ˆï¼Œä½†æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨é¢„æµ‹æ–¹æ³•ä¸­å¯¹æ‰€æœ‰æµ‹è¯•æ ·æœ¬åº”ç”¨æ¿€æ´»å‡½æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›å¯¹
    N D æ•°ç»„åº”ç”¨è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è¿™ä¸€ç‚¹ã€‚
- en: we can use a simple function that is called numpy dot whereã€‚ So we return nuy
    dot whereã€‚ And this will get a conditionã€‚ So x is larger or equal than 0ã€‚ And
    if the condition is trueã€‚ then we return one and otherwise 0ã€‚So this will work
    for one single sampleã€‚ but also for multiple samples in one vectorã€‚So now this
    is our activation functionã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œç§°ä¸º numpy.dot.whereã€‚å› æ­¤æˆ‘ä»¬è¿”å› nuy.dot.whereã€‚è¿™å°†è·å–ä¸€ä¸ªæ¡ä»¶ã€‚å› æ­¤ x å¤§äºæˆ–ç­‰äº 0ã€‚å¦‚æœæ¡ä»¶ä¸ºçœŸï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿”å›
    1ï¼Œå¦åˆ™è¿”å› 0ã€‚å› æ­¤è¿™å°†å¯¹å•ä¸ªæ ·æœ¬æœ‰æ•ˆï¼Œä½†ä¹Ÿå¯¹ä¸€ä¸ªå‘é‡ä¸­çš„å¤šä¸ªæ ·æœ¬æœ‰æ•ˆã€‚å› æ­¤ç°åœ¨è¿™æ˜¯æˆ‘ä»¬çš„æ¿€æ´»å‡½æ•°ã€‚
- en: So now we can say self dot activationation fun equals self dot unit step funã€‚And
    now let's also create the weightsã€‚ say self dot weights equals none and self dotã€‚Bs
    equals noneã€‚ So now that we know that we have to implement themã€‚Or get themã€‚ And
    now we implement two functionsã€‚ As alwaysï¼Œ we implement the fit and the predict
    methodã€‚ So firstã€‚Define the fit method with X and yã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´ self.dot.activationation fun ç­‰äº self.dot.unit step funã€‚ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºæƒé‡ã€‚è¯´ self.dot.weights
    ç­‰äº noneï¼Œself.dot.Bs ç­‰äº noneã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬çŸ¥é“å¿…é¡»å®ç°å®ƒä»¬ã€‚æˆ–è€…è·å–å®ƒä»¬ã€‚ç°åœ¨æˆ‘ä»¬å®ç°ä¸¤ä¸ªå‡½æ•°ã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å®ç° fit å’Œ
    predict æ–¹æ³•ã€‚å› æ­¤é¦–å…ˆã€‚å®šä¹‰ fit æ–¹æ³•ï¼Œå¸¦æœ‰ X å’Œ yã€‚
- en: So this gets the training samples and the training labelsã€‚And thenï¼Œ of courseï¼Œ
    ohã€‚ I forgot this selfã€‚And thenï¼Œ weã€‚Alsoï¼Œ define theã€‚Predict methodï¼Œ which gets
    selfã€‚ and then the test samplesã€‚ And now we startã€‚With this predict methodï¼Œ because
    this is very simpleã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™è·å–è®­ç»ƒæ ·æœ¬å’Œè®­ç»ƒæ ‡ç­¾ã€‚ç„¶åï¼Œå½“ç„¶ï¼Œå“¦ã€‚æˆ‘å¿˜äº†è¿™ä¸ª selfã€‚ç„¶åï¼Œæˆ‘ä»¬ã€‚è¿˜å®šä¹‰äº†ã€‚é¢„æµ‹æ–¹æ³•ï¼Œå®ƒè·å– selfã€‚ç„¶åæ˜¯æµ‹è¯•æ ·æœ¬ã€‚ç°åœ¨æˆ‘ä»¬å¼€å§‹ã€‚ä½¿ç”¨è¿™ä¸ªé¢„æµ‹æ–¹æ³•ï¼Œå› ä¸ºè¿™éå¸¸ç®€å•ã€‚
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47427fb8775e23c1bec39e64630a9c33_3.png)'
- en: Let's have a look at the approximation of our output againã€‚ So here firstã€‚ we
    apply this linear function and then the activation functionã€‚So let's do thisã€‚
    So firstã€‚ the linear functionsã€‚ So let's say linearã€‚![](img/47427fb8775e23c1bec39e64630a9c33_5.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†æ¬¡æŸ¥çœ‹æˆ‘ä»¬è¾“å‡ºçš„è¿‘ä¼¼å€¼ã€‚è¿™é‡Œé¦–å…ˆã€‚æˆ‘ä»¬åº”ç”¨è¿™ä¸ªçº¿æ€§å‡½æ•°ï¼Œç„¶åæ˜¯æ¿€æ´»å‡½æ•°ã€‚æˆ‘ä»¬å¼€å§‹å§ã€‚æ‰€ä»¥é¦–å…ˆï¼Œçº¿æ€§å‡½æ•°ã€‚å‡è®¾æ˜¯linearã€‚![](img/47427fb8775e23c1bec39e64630a9c33_5.png)
- en: Output equalsã€‚ and this is w transpose times x plus the biasã€‚ and W transpose
    times x is nothing else but the dot productã€‚ so we can usempie dotã€‚offã€‚X and self
    dotã€‚Self dot weights plus self dot biasã€‚ So now we have the linear functionã€‚ and
    now we apply the activation functionã€‚ So we say why predict equals self dot activation
    functionã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç­‰äºã€‚è¿™æ˜¯wè½¬ç½®ä¹˜ä»¥xåŠ ä¸Šåç½®ï¼Œè€ŒWè½¬ç½®ä¹˜ä»¥xä¸è¿‡æ˜¯ç‚¹ç§¯ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨np.dotã€‚offã€‚Xå’Œself dotã€‚self dot weightsåŠ ä¸Šself
    dot biasã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†çº¿æ€§å‡½æ•°ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬åº”ç”¨æ¿€æ´»å‡½æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´yé¢„æµ‹ç­‰äºself dotæ¿€æ´»å‡½æ•°ã€‚
- en: And as in inputï¼Œ it gets the linear output hereã€‚ And then we simply return the
    y predictedã€‚ So this is the whole predict methodã€‚ And now let's jump to the fit
    methodã€‚ So first of allã€‚ let's get the dimensions of the x vectorã€‚ So this is
    an N D array of size M times N where M or the number of rows is the number of
    samples and N or the number of columns is the number of featuresã€‚So we say N samples
    and Nã€‚Features equals x dot shapeã€‚And now we in it our weightsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè¾“å…¥ï¼Œå®ƒè·å–è¿™é‡Œçš„çº¿æ€§è¾“å‡ºã€‚ç„¶åæˆ‘ä»¬ç®€å•è¿”å›yé¢„æµ‹ã€‚è¿™å°±æ˜¯æ•´ä¸ªé¢„æµ‹æ–¹æ³•ã€‚ç°åœ¨è®©æˆ‘ä»¬è·³åˆ°æ‹Ÿåˆæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è·å–xå‘é‡çš„ç»´åº¦ã€‚è¿™æ˜¯ä¸€ä¸ªå¤§å°ä¸ºMä¹˜Nçš„Nç»´æ•°ç»„ï¼Œå…¶ä¸­Mæˆ–è¡Œæ•°æ˜¯æ ·æœ¬æ•°ï¼ŒNæˆ–åˆ—æ•°æ˜¯ç‰¹å¾æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´Næ ·æœ¬å’ŒNç‰¹å¾ç­‰äºx
    dot shapeã€‚ç°åœ¨æˆ‘ä»¬åˆå§‹åŒ–æˆ‘ä»¬çš„æƒé‡ã€‚
- en: we have to give them an initial valueï¼Œ and we can simply set them to 0 in the
    beginningã€‚ So we say self dot weights equals nuy dot0sã€‚Of sizeï¼Œ number of featuresã€‚
    So for each featureã€‚ we put a0 here for our weightã€‚And alsoï¼Œ ourï¼Œ the biasï¼Œ this
    is simply 0ã€‚And now we can start or one more thing we have to do is we want to
    make sure that our y only consists of classes 0 and1ã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»ç»™å®ƒä»¬ä¸€ä¸ªåˆå§‹å€¼ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†å®ƒä»¬è®¾ç½®ä¸º0ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´self dot weightsç­‰äºnuy dot zerosã€‚å¤§å°ä¸ºç‰¹å¾æ•°ã€‚æ‰€ä»¥å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ”¾ä¸€ä¸ª0ä½œä¸ºæˆ‘ä»¬çš„æƒé‡ã€‚è€Œä¸”ï¼Œåç½®ï¼Œç®€å•ä¸º0ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹ï¼Œè¿˜æœ‰ä¸€ä»¶äº‹æˆ‘ä»¬è¦åšçš„æ˜¯ç¡®ä¿æˆ‘ä»¬çš„yåªåŒ…å«ç±»åˆ«0å’Œ1ã€‚
- en: so let's sayã€‚Y underscore equalsã€‚ And now let's convert all the values to 0
    or oneã€‚ If this is not already the caseã€‚ So we use list comprehension for thisã€‚
    So we say one if I is larger than 0 elseï¼Œ it's 0ï¼Œ4 I in yã€‚And now let's convert
    this to Ny arrayã€‚Soã€‚ nowï¼Œ we have ourã€‚Whyï¼Œ and now we can start the trainingã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å‡è®¾Yä¸‹åˆ’çº¿ç­‰äºã€‚ç°åœ¨æˆ‘ä»¬å°†æ‰€æœ‰å€¼è½¬æ¢ä¸º0æˆ–1ã€‚å¦‚æœæƒ…å†µå¹¶éå¦‚æ­¤ã€‚æˆ‘ä»¬ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ¥å®ç°ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å¦‚æœiå¤§äº0åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œé’ˆå¯¹yä¸­çš„iã€‚ç°åœ¨å°†å…¶è½¬æ¢ä¸ºNyæ•°ç»„ã€‚æ‰€ä»¥ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„Yï¼Œç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒã€‚
- en: So let's again have a look at this update ruleã€‚ So we want to look at each training
    sample and we also want to do this a couple of iterationsã€‚ So we need twoï¼Œ four
    loops hereã€‚ So let's say the first one for underscore because we don't need this
    in range self dot and itsã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å†æ¬¡æŸ¥çœ‹è¿™ä¸ªæ›´æ–°è§„åˆ™ã€‚æˆ‘ä»¬å¸Œæœ›æŸ¥çœ‹æ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜æƒ³è¿›è¡Œå‡ æ¬¡è¿­ä»£ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œéœ€è¦ä¸¤ä¸ªã€å››ä¸ªå¾ªç¯ã€‚æˆ‘ä»¬å‡è®¾ç¬¬ä¸€ä¸ªå¾ªç¯æ˜¯ä¸‹åˆ’çº¿ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨èŒƒå›´å†…ä¸éœ€è¦è¿™ä¸ªself
    dotå’Œå®ƒçš„ã€‚
- en: ğŸ¤¢ã€‚![](img/47427fb8775e23c1bec39e64630a9c33_7.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤¢ã€‚![](img/47427fb8775e23c1bec39e64630a9c33_7.png)
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47427fb8775e23c1bec39e64630a9c33_8.png)'
- en: So this is the number of iterations we definedã€‚ and then our second loopã€‚ And
    for thisã€‚ I used the enumererate methodï¼Œ so I can say fourã€‚Index and also Xï¼Œ Y
    inã€‚En nuerate Xã€‚ So I want to iterate over the drainingã€‚Lmb training samples and
    the enumererate function will give me the index and then also the current sampleã€‚So
    these are our two loopsï¼Œ and nowã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å®šä¹‰çš„è¿­ä»£æ¬¡æ•°ï¼Œç„¶åæ˜¯æˆ‘ä»¬çš„ç¬¬äºŒä¸ªå¾ªç¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä½¿ç”¨äº†enumerateæ–¹æ³•ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥è¯´å››ä¸ªã€‚ç´¢å¼•å’ŒXï¼ŒYåœ¨ã€‚enumerate Xä¸­ã€‚æ‰€ä»¥æˆ‘æƒ³éå†è®­ç»ƒæ ·æœ¬ï¼Œenumerateå‡½æ•°å°†ç»™æˆ‘ç´¢å¼•ä»¥åŠå½“å‰æ ·æœ¬ã€‚è¿™æ˜¯æˆ‘ä»¬çš„ä¸¤ä¸ªå¾ªç¯ï¼Œç°åœ¨ã€‚
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_10.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47427fb8775e23c1bec39e64630a9c33_10.png)'
- en: Sorryã€‚Let's applyã€‚This update ruleã€‚ So let'sï¼Œ againã€‚ we have to calculate the
    predicted value and then apply the updateã€‚ So let's say the linear output equals
    nuy dot ofã€‚![](img/47427fb8775e23c1bec39e64630a9c33_12.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ±æ­‰ã€‚è®©æˆ‘ä»¬åº”ç”¨ã€‚è¿™ä¸ªæ›´æ–°è§„åˆ™ã€‚æ‰€ä»¥ï¼Œå†æ¬¡ã€‚æˆ‘ä»¬å¿…é¡»è®¡ç®—é¢„æµ‹å€¼ç„¶ååº”ç”¨æ›´æ–°ã€‚æˆ‘ä»¬å‡è®¾çº¿æ€§è¾“å‡ºç­‰äºnuy dot ofã€‚![](img/47427fb8775e23c1bec39e64630a9c33_12.png)
- en: The current sampleã€‚And ourselvesï¼Œ that awaitsã€‚Plusï¼Œ the self of biasã€‚Then we
    apply the activation function and get the predicted valueã€‚ So why predict equals
    self dot activation function of the linear outputã€‚ So here we can see that in
    this caseï¼Œ we use it for only one sampleã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰æ ·æœ¬ã€‚è¿˜æœ‰æˆ‘ä»¬çš„ selfï¼Œé‚£ç­‰å¾…ã€‚åŠ ä¸Š self çš„åç½®ã€‚ç„¶åæˆ‘ä»¬åº”ç”¨æ¿€æ´»å‡½æ•°å¹¶å¾—åˆ°é¢„æµ‹å€¼ã€‚æ‰€ä»¥ y é¢„æµ‹ç­‰äº self.dot.activation_function
    çš„çº¿æ€§è¾“å‡ºã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªä¸ºä¸€ä¸ªæ ·æœ¬ä½¿ç”¨å®ƒã€‚
- en: And here down in the predict methodï¼Œ we use the activation function for multiple
    samplesã€‚ And that's why we need this nuy where function here in our activation
    functionã€‚So yeahã€‚ and now let's continueã€‚ So now we have our predicted yã€‚ and
    now let's have a look at the formula againã€‚ So we have the learning rate times
    the differenceã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ predict æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªæ ·æœ¬ä½¿ç”¨æ¿€æ´»å‡½æ•°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨æ¿€æ´»å‡½æ•°ä¸­éœ€è¦è¿™ä¸ª nuy çš„åŸå› ã€‚æ‰€ä»¥æ˜¯çš„ã€‚ç°åœ¨ç»§ç»­ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†é¢„æµ‹çš„ yã€‚ç°åœ¨æˆ‘ä»¬å†çœ‹çœ‹å…¬å¼ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰å­¦ä¹ ç‡ä¹˜ä»¥å·®å€¼ã€‚
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_14.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47427fb8775e23c1bec39e64630a9c33_14.png)'
- en: '![](img/47427fb8775e23c1bec39e64630a9c33_15.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47427fb8775e23c1bec39e64630a9c33_15.png)'
- en: And thenï¼Œ times xï¼Œ soã€‚Let's call this update equals self dot learning rate timesã€‚
    And here we have the actualã€‚Label So y underscore of this current index minus
    y predict thatã€‚And thenã€‚We say selfã€‚Dot weights plus equals update times Xï¼Œ Iã€‚And
    self dot bias plus equals update times 1ã€‚ So we don't need this times 1ã€‚And now
    we are doneã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä¹˜ä»¥ xã€‚æ‰€ä»¥ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªæ›´æ–°ä¸º self.dot.learning_rate ä¹˜ä»¥ã€‚è¿™é‡Œæˆ‘ä»¬æœ‰å®é™…çš„ã€‚æ ‡ç­¾ï¼Œæ‰€ä»¥ y_ä¸‹åˆ’çº¿ å½“å‰ç´¢å¼• å‡å» y
    é¢„æµ‹ã€‚ç„¶åã€‚æˆ‘ä»¬è¯´ self.dot.weights åŠ ä¸Šç­‰äºæ›´æ–°ä¹˜ä»¥ Xï¼Œæˆ‘ã€‚å’Œ self.dot.bias åŠ ä¸Šç­‰äºæ›´æ–°ä¹˜ä»¥ 1ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è¿™ä¸ªä¹˜ä»¥
    1ã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚
- en: So this is the whole implementation of the perceptronã€‚And now let's test thisã€‚
    So here I've writtenã€‚ I've already written a little test script hereï¼Œ so I importpart
    the perceptron hereã€‚And then I will create twoã€‚Two blopsã€‚ so I can use this from
    the SK learn moduleã€‚ I can use a function that is called make blopsã€‚ So this will
    create two classesã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æ„ŸçŸ¥å™¨çš„æ•´ä½“å®ç°ã€‚ç°åœ¨è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ã€‚æ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå†™äº†ã€‚æˆ‘å·²ç»å†™äº†ä¸€ä¸ªå°æµ‹è¯•è„šæœ¬ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå¯¼å…¥æ„ŸçŸ¥å™¨ã€‚ç„¶åæˆ‘ä¼šåˆ›å»ºä¸¤ä¸ªã€‚ä¸¤ä¸ª blopsã€‚æ‰€ä»¥æˆ‘å¯ä»¥ä½¿ç”¨
    SK learn æ¨¡å—ä¸­çš„ä¸€ä¸ªå« make blops çš„å‡½æ•°ã€‚è¿™å°†åˆ›å»ºä¸¤ä¸ªç±»åˆ«ã€‚
- en: And then I will split our data into training samples and test samples and trainingã€‚Training
    labels and test labelsã€‚ Then I will create a perceptronã€‚ I will fit the training
    dataã€‚ and then I will predict the test labelsã€‚And then I will calculate the accuracyã€‚
    and I will also plot thisã€‚ So let's run thisã€‚And I hope I didn't forget anythingã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å°†æŠŠæ•°æ®åˆ†æˆè®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬ï¼Œä»¥åŠè®­ç»ƒæ ‡ç­¾å’Œæµ‹è¯•æ ‡ç­¾ã€‚ç„¶åæˆ‘å°†åˆ›å»ºä¸€ä¸ªæ„ŸçŸ¥å™¨ã€‚æˆ‘ä¼šæ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚ç„¶åæˆ‘å°†é¢„æµ‹æµ‹è¯•æ ‡ç­¾ã€‚ç„¶åæˆ‘å°†è®¡ç®—å‡†ç¡®ç‡ã€‚è¿˜ä¼šç»˜åˆ¶è¿™ä¸ªã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œå®ƒã€‚æˆ‘å¸Œæœ›æˆ‘æ²¡æœ‰å¿˜è®°ä»»ä½•äº‹æƒ…ã€‚
- en: So now here is the plotã€‚ So here we see our two plotlopsã€‚ And here we have our
    linear decision boundaryã€‚ So this is theã€‚Decision function at our perceptron generatedã€‚And
    we see that it separates our two classes perfect hereã€‚ Soï¼Œ and we also see that
    our accuracy is oneã€‚ So it's perfect in this caseã€‚And yeahã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™æ˜¯ç»˜å›¾ã€‚å› æ­¤æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„ä¸¤ä¸ª plotlopsã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„çº¿æ€§å†³ç­–è¾¹ç•Œã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ„ŸçŸ¥å™¨ç”Ÿæˆçš„å†³ç­–å‡½æ•°ã€‚æˆ‘ä»¬çœ‹åˆ°å®ƒå®Œç¾åœ°åˆ†å¼€äº†æˆ‘ä»¬çš„ä¸¤ä¸ªç±»åˆ«ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°æˆ‘ä»¬çš„å‡†ç¡®ç‡æ˜¯
    1ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯å®Œç¾çš„ã€‚æ˜¯çš„ã€‚
- en: we see that the perceptron worksã€‚ and one thing that we should be aware of is
    that the perceptron only works for linearly separable classesã€‚ So if classes can
    be separated with a linear functionï¼Œ like here in our caseã€‚ let's have a look
    at this againã€‚Then it works very wellï¼Œ but otherwise not so muchã€‚And for further
    improvements means we can try out different activationation functionsã€‚ For exampleã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°æ„ŸçŸ¥å™¨å¯ä»¥å·¥ä½œã€‚æˆ‘ä»¬éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæ„ŸçŸ¥å™¨ä»…é€‚ç”¨äºçº¿æ€§å¯åˆ†çš„ç±»åˆ«ã€‚æ‰€ä»¥å¦‚æœç±»åˆ«å¯ä»¥ç”¨çº¿æ€§å‡½æ•°åˆ†å¼€ï¼Œæ¯”å¦‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ã€‚æˆ‘ä»¬å†çœ‹ä¸€ä¸‹è¿™ä¸ªã€‚é‚£ä¹ˆå®ƒå·¥ä½œå¾—éå¸¸å¥½ï¼Œä½†å¦åˆ™æ•ˆæœå°±ä¸é‚£ä¹ˆç†æƒ³ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ã€‚
- en: the sigmoid function and then apply a gradientï¼Œ decent method rather than the
    perception rule in order to update our weightsã€‚But yeahï¼Œ for nowï¼Œ that should
    be all I wanted to show youã€‚ And I hope you enjoyed this tutorial and see you
    next timeï¼Œ byeã€‚![](img/47427fb8775e23c1bec39e64630a9c33_17.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid å‡½æ•°ï¼Œç„¶ååº”ç”¨æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œè€Œä¸æ˜¯æ„ŸçŸ¥è§„åˆ™æ¥æ›´æ–°æˆ‘ä»¬çš„æƒé‡ã€‚ä¸è¿‡ï¼Œç°åœ¨æˆ‘æƒ³å±•ç¤ºçš„åº”è¯¥å°±æ˜¯è¿™äº›ã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œä¸‹æ¬¡å†è§ï¼Œæ‹œæ‹œã€‚![](img/47427fb8775e23c1bec39e64630a9c33_17.png)
