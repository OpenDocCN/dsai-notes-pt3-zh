- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P66：L12.5- 非游戏TF-Agent的强化学习
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P66：L12.5- 非游戏TF-Agent的强化学习
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeffine and welcome to App of Deep neural Networks with Washington
    University。So reinforcement learning， most demos that you've seen of this probably
    lets you play some sort of Atari game or run a mountain car up a hill or balance
    a pole or something fun like that。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是Jeffine，欢迎来到华盛顿大学的深度神经网络应用课程。所以强化学习，你见过的大多数演示可能让你玩某种Atari游戏，或者把山地车开上山，或者平衡一个杆子之类的有趣事情。
- en: What I haven't seen a lot of examples of is how to actually apply this to something
    that is not an Atari game or is not some sort of physics simulation。What I'm going
    to show you in this video is how to actually apply reinforcement learning using
    TF agents to a problem of your own design。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有看到很多实例来说明如何将这一点实际应用于不是Atari游戏或者某种物理模拟的东西。在这个视频中，我将向你展示如何使用TF agents将强化学习应用于你自己设计的问题。
- en: Now， reinforcement learning is outside of the area of normal classification
    and regression。 really it's something where you're constantly performing actions
    on an environment of some sort and receiving feedback for this。The example that
    I'm going to show in this section is how to create sort of a financial simulation。
    We're going to look at net wealth building and sort of personal finance。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，强化学习超出了正常分类和回归的范围。实际上，这是一种你不断对某种环境执行动作并获取反馈的过程。我将在本节中展示的例子是如何创建一种金融模拟。我们将关注净财富的积累和个人理财。
- en: This is really a pretty simple financial simulation。 This is not meant to guide
    you and real life or how to do any sort of real financial investing。 but it shows
    you how to actually put something that is not a video game into the context that
    it can be solved and worked with with。Reinforcement learning， and you can design
    an agent that can actually learn how to do this to see all my videos about Cale。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个相当简单的金融模拟。这并不是为了指导你在现实生活中如何进行任何形式的真实投资，但它展示了如何将一些不是视频游戏的东西放入可以被解决和处理的上下文中。强化学习，你可以设计一个代理，真正学会如何做到这一点，看看我所有关于Cale的视频。
- en: neural networks and other AI topics， click the subscribe button and the bell
    next to it and select all to be notified of every new video So now let's try to
    apply reinforcement learning to something that is not just in the open AI gym
    or an Atari game or something like that。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络和其他人工智能主题，请点击订阅按钮及旁边的铃铛，并选择全部以便接收每个新视频的通知。那么现在，让我们尝试将强化学习应用于一些不仅仅是在OpenAI
    gym或Atari游戏中的东西。
- en: I'm going to create this problem literally from scratch。 Now it's not going
    to be the most profound problem that I'm going to apply this to。 It's really the
    idea of this demonstration is to show you how you can create your own environment
    and apply deep reinforcement learning to it。 We're going to also look at the continuous
    action space and see how you can deal with having one or more actions that is
    a number。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从零开始创建这个问题。现在这并不是我将要应用的最深奥的问题。这个演示的真正目的是向你展示如何创建自己的环境，并将深度强化学习应用于其中。我们还将查看连续动作空间，并了解如何处理一个或多个数值动作。
- en: like how how far do you want to press down the accelerator pedal。 say in a car
    or how hard to apply the brakes， because you can do both of those at the same
    time。 For this example that I'm going to put together。 It's going to be a finance
    kind of personal finance example where you are。😊。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 像是你想把油门踏板踩多深，比如在车里，或者刹车要用多大的力，因为你可以同时进行这两项。对于我将要整合的这个例子，它将是一个个人理财的金融示例。😊
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_1.png)'
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_2.png)'
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_3.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_3.png)'
- en: Letttting the agent be a person that is trying to invest and save for retirement
    it'll be a pretty simplified example where they can choose between putting their
    money in a tax deferred account like a United States IRA or they could choose
    a taxable account and there's limits you can only put so much in that tax deferred
    account you can also choose do you want to pay for your house you want to pay
    the mortgage off early。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让代理成为一个试图投资和为退休储蓄的人，这将是一个相当简化的例子，他们可以选择把钱放入一个像美国IRA这样的税延账户，或者选择一个应税账户，并且有一定的限制，你只能在那个税延账户中放入这么多钱。你也可以选择，你是想为你的房子付款，还是想提前还清抵押贷款。
- en: all these kind of things So these are kind of step by step by step actions that
    are really conducive to reinforcement learning if I just wanted to the computer
    to look at a list of stocks and give me the best allocation for investing that
    would be more like a traditional neural network so we're gonna create an environment
    entirely of our own This is a class in TF agents an objectoriented class here
    are the rules you've got to make sure that you accomplish these things or you
    won't be able to use your new class It has to be a child of Jim environment then
    you've got to。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些事情都是一步一步的动作，实际上对强化学习非常有利。如果我只是想让计算机查看一份股票列表并给出最佳投资分配，那更像是传统的神经网络。因此，我们将完全创建一个自己的环境。这是在TF
    Agents中的一个面向对象的类，以下是规则：你必须确保完成这些事情，否则将无法使用你的新类。它必须是Jim环境的子类，然后你必须。
- en: Iplement a seed function that allows your environment to be seeed。 if you give
    it a constant seed。 then you should get constant results。 You need to implement
    a reset function that basically starts your environment over again is this is
    done often by the reinforcement learning algorithms。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个种子函数，使你的环境能够被设定。如果你给它一个常量种子，那么你应该得到恒定的结果。你需要实现一个重置函数，基本上是重新启动你的环境，这通常是强化学习算法执行的操作。
- en: It's always turning back the clock and hitting reset and starting from the beginning
    This is not as critical。 but you need to implement a render function。 This draws
    one frame of your simulation。 Now we're doing a finance simulation here。 and I
    kept it pretty simple。 We're just outputting text to a window。 same sort of thing
    step is a function that will take you through each step in the you apply an action
    to that step and then you see how the environment changes and we'll get to what
    the environment is in a moment。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它总是回到起点并重置，从头开始。这并不是特别关键，但你需要实现一个渲染函数。这绘制了模拟的一个帧。现在我们在这里做的是财务模拟，我保持得相当简单。我们只是向一个窗口输出文本。类似的步骤是一个函数，它会带你经历每一步，你对该步骤应用一个动作，然后你会看到环境的变化，我们稍后会讨论环境是什么。
- en: you also need to register your environment and we see the code to do this as
    well That's so that Tf agents can actually find your environment when when you
    want to spin it up。 So here's the simulation that。Going to produce Now this is
    a very simple simulation。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要注册你的环境，我们也可以看到用于此的代码。这样TF Agents才能在你想要启动时找到你的环境。因此，这里是将要生成的模拟。现在这是一个非常简单的模拟。
- en: This is just to show you how to get your own environment up and running。 I was
    tempted to just create my own mini video game or something like that。 but I might
    save that for a future example。 but there's already enough examples of doing this
    I want something where we're playing a financial simulation doing personal finances
    is like the old Milton Bradley game of life a bit not to be confused with Conway's
    game of life which which is something else。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是为了向你展示如何让自己的环境启动和运行。我曾想简单地创建一个迷你视频游戏或类似的东西，但我可能会把这个留到未来的示例中。不过，已经有足够多的例子来做到这一点。我想要一些我们在进行财务模拟的东西，做个人理财有点像老迈尔顿·布拉德利的《人生游戏》，而且不要与康威的《生命游戏》混淆，那是另外一回事。
- en: So you have a random starting salary between 40k and 60k dollars you have a
    home loan that starts out so you have to buy a home's I'm keeping it simple that
    is 1。5 times and four times you're starting salary so maybe you are very conservative
    or maybe kind of went nuts so this is looking at how much how much house this
    person can potentially afford and they'll need to adjust their strategy depending
    on how crazy they went on real estate purchases The home loan is always an amortized
    30 year fixed loan with a fixed monthly payment。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你的起始薪资在4万到6万美元之间，你有一个开始时的房贷，所以你必须买房，我把它简化为是1.5到4倍的起始薪资，所以也许你非常保守，或者有点疯狂，这里是看这个人可能负担得起多少房子，他们需要根据在房地产购买上花费的疯狂程度调整策略。房贷始终是一个30年固定利率的摊还贷款，固定的月供。
- en: If you don't make that payment， bad things will start to happen to you Pay higher
    than the homes monthly payment pays down the loan quicker Pay below the monthly
    price results in missed payments When you reach a certain number of payments。
    I think I set it around 15。 they take your house away You don't lose the value
    but they sell it at auction and you lose。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不按时付款，坏事就会开始发生。支付高于房屋月供的金额会更快地偿还贷款，低于月供的支付会导致错过付款。当你达到某个付款次数时，我记得我设置在15次左右，他们会收走你的房子。你不会失去价值，但他们会在拍卖中出售，你会损失。
- en: I think half of your equity。 you can allocate income between luxury purchases。
    which in this game does absolutely nothing for you home payments either above
    or below the payment amount as well as a taxable and a tax advantage savings account。
    It might have been really interesting that if you spent more on luxury or are
    happier and you lived longer therefore you got longer to invest I don't know。
    might have been an interesting twist on this I'll probably take this game of life
    thing a bit further and do a separate video on it outside of the course where
    we look at a fairly complicated financial simulation So the state the state is
    composed of the following values。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为你的一半资产可以分配在奢侈消费之间，而在这个游戏中这对你没有任何帮助，房屋支付无论高于还是低于支付金额，以及一个应税和一个税收优惠的储蓄账户。如果你在奢侈品上花费更多或更快乐，并因此活得更久，因此你有更长的时间投资，我不知道。可能这是一个有趣的转折，我可能会将这个生命游戏再深入一点，并在课程之外做一个单独的视频，看看一个相当复杂的金融模拟。因此，状态由以下值组成。
- en: This is what the program is looking at to determine what it should do。The age
    of the person in months， the salary Now the salary goes up slowly relative to
    inflation。 no chances for promotions or anything like that。 So it's pretty simple
    simulation。 the home value。 This will increase relative to inflation。 This is
    the required home payment。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是程序在查看以确定它应该做什么。人的年龄（月数），薪水。现在薪水相对于通货膨胀缓慢增长，没有晋升机会或其他类似的东西。所以这是一个相当简单的模拟。房屋价值，这将相对于通货膨胀增加。这是所需的房屋支付。
- en: This will stay the same mostly the only time this will change is when you pay
    off your home then it goes to zero。 This is the account balance for the tax advantage。
    This is similar to a United States IRA account。 individual retirement account。
    and then the taxable account。 This is similar to just a standard money market
    savings account。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上会保持不变，唯一会改变的情况是当你还清房贷时，它会变为零。这是税收优惠的账户余额，这类似于美国的个人退休账户（IRA），然后是应税账户，这类似于标准的货币市场储蓄账户。
- en: It does pay you some actually pays you pretty good interest compared to what
    a money market would do。 So this assumes you probably have this invested in a
    independent brokerage account。 selflf-directed brokerage account。 least that's
    what they call them in in the US。 then the action。 So the action is floating point
    because we're doing a continuous action space。 Unlike previously。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实给你一些收益，实际上比货币市场的收益要好。因此这假设你可能将其投资于一个独立的经纪账户，自我管理的经纪账户，至少在美国他们称之为这样。然后是行动。因此，行动是浮动的，因为我们在做一个连续的行动空间。不像之前。
- en: The agent puts a number into each of these four。 And then I normalize them so
    that they some。To to one to get percents。 So if you put 11，111 on all of these。
    that would normalize out to 25% into each。 So this is just letting you pick your
    percents that you want to devote of your salary to the home loan to the savings
    tax advantage account to the normal taxable savings account into luxuries。 Now
    this doesn't guarantee that you'll get all of these percents in there because
    there's various things going on here。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在这四个中输入一个数字。然后我对它们进行归一化，使它们的总和为1，以便得到百分比。所以如果你在所有这些上输入11,111，它将归一化为每个25%。所以这只是让你选择希望将工资的百分比分配给房贷、税收优惠储蓄账户、正常应税储蓄账户和奢侈品。现在这并不保证你能将所有这些百分比投入，因为这里有各种事情发生。
- en: you do have to pay tax on your salary。 So thatll decrease the salary sum you
    might still be allocating money towards your home loan。 but you might have paid
    your house off。 So any value that you put in here is basically ignored。 It's just
    going to go to0。 You might have run out of of salary before you can even put it
    into the taxable account or the savings account。 Also， you can only put so much
    in the tax advantage account。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你的工资确实需要缴税。所以这会减少你可能仍在分配用于房贷的工资总额。但你可能已经还清了房子。因此，你在这里投入的任何价值基本上都会被忽略。它只会变成0。你可能在把工资投入应税账户或储蓄账户之前就已经用完了工资。此外，你只能在税收优惠账户中投入有限的金额。
- en: at least in the United States that's sort of on the trick with IRAs。 There's
    a limit to how much you can put in there per year。 There's a limit to how much
    you can put into your 401 any year， all these all these。Of ramifications and I
    try to put some of those in there as well。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 至少在美国，这对IRA来说是一个难题。每年你可以投入的金额是有限的。每年你可以投入到401(k)的金额也是有限的，所有这些都有各种影响，我也尽量把一些放进去。
- en: The limit that I actually put in for the tax advantage account is similar to4
    and K。 So here is the program。 or at least the class。 This is a object oriented
    class。 It inherits from gym environment metadata you must just present that for
    display I went with pretty much what most of the classes in gym go with one video
    frame per second Def not a highend video game there by any stretch and it's going
    to display in RGb form。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我为税收优惠账户设定的限额类似于401(k)。所以这里是程序，或者说是类。这是一个面向对象的类。它继承自gym环境元数据，你必须为显示呈现它，我几乎遵循了gym中大多数类的做法，每秒一个视频帧，绝对不是高端视频游戏，按任何标准来说，它将以RGB形式显示。
- en: The number of state elements that we have are 7 and it's the first seven of
    these a few of these are calculated values that I do add to the output just for
    debugging purposes。 you'll find that when you create a complex environment you'll
    spend a lot of time just debugging the environment before you even get around
    to hooking it up to Tf agents So here are the positions。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的状态元素数量是7，前七个是这些，几个是我添加到输出中的计算值，仅用于调试目的。你会发现，当你创建一个复杂环境时，会花很多时间调试环境，而不是直接连接到TensorFlow代理。所以这里是这些位置。
- en: So the state is essentially a vector。 The first element in the vector is age
    than salary and so on and so forth。 This is also another constant I put in there
    for a million' see that。That for normalization。 I normalize all the dollar amounts
    to millions， even though salary is much。 much smaller than that it's still a fractional
    piece of a million。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态本质上是一个向量。向量中的第一个元素是年龄，接下来是工资，依此类推。这也是我放入的另一个常数，以便于归一化。我将所有美元金额归一化为百万，尽管工资远远小于这个，它仍然是百万的一小部分。
- en: that helps to keep these numbers from bigger than what the neural network would
    necessarily want to deal with。 These are the values that I put in for inflation
    interest tax rate。 Some of these are normalized to're not normalized but divided
    by 12 because they're monthly values。 this is expenses。 you have no choice you
    have to pay the expenses that's your food clothing。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于保持这些数字不会大于神经网络可能想要处理的数字。这些是我为通货膨胀、利率、税率设定的值。其中一些是归一化的，或者说是除以12，因为它们是月度值。这是支出。你没有选择，你必须支付支出，那是你的食物、衣物。
- en: water internet connection net， all the criticals。 Then your salary range low
    and high starting age and retiring age or essentially age that you die at。 This
    is the constructor I set verbo mode default fall setting ver most mode to true
    just causes my program to print out some logging information so that you can help
    debug it I define the action space。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 水、电、网络连接和所有关键费用。然后是你的薪资范围的低值和高值，起始年龄和退休年龄，或者说是你去世的年龄。这是构造函数，我设置了verbo模式，默认的fall设置将ver
    most模式设为true，这只是导致我的程序打印出一些日志信息，以便你可以帮助调试。我定义了动作空间。
- en: The action spaces between  zero and1。 It's important that you define this so
    that the reinforcement learning algorithm no。What those ranges are and can assign
    random numbers and other things as well。 This is the observation space。 I set
    the low and high values there。 It can potentially go higher than2 million it might
    be clipping it that might be a good idea to put that to maybe4 or10 or something
    like that but anyway that's the ranges there。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间在0和1之间。重要的是你要定义这个空间，以便强化学习算法能够理解这些范围，并可以分配随机数和其他内容。这就是观察空间。我在这里设置了低值和高值。它可能会超过200万，可能会限制在那儿，最好将其设置为4、10或类似的值，但无论如何，这就是范围。
- en: we go ahead and seed and reset I set the log empty the log is my own thing that
    is basically just giving me information that I can convert into a pandas data
    frame later so that I can debug and make sure that the simulation is doing what
    I actually wanted to do This is a handy function that I wrote the calculates the
    person's net worth looking at the home value。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行种子和重置，我设置了日志为空，日志是我自己的东西，基本上就是给我提供一些信息，以便我可以稍后将其转换为pandas数据框，从而进行调试，确保模拟在按我想要的方式运行。这是我编写的一个方便的函数，它计算个人的净资产，查看房屋价值。
- en: the principal amount still on the still owed on the home so you subtract the
    amount that you owe on the home from the value of the home and that that gives
    you that component of the net worth and then we add in the worth also as far as
    the tax advantage account and taxable again I'm making a very simple here you
    mainly save the taxes going into the account not so much coming out。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然欠在房屋上的本金，所以你从房屋的价值中减去你所欠的金额，这样就得到净资产的一个组成部分，然后我们也加上税收优惠账户和应税账户的价值。我在这里做得很简单，主要是在账户里节省税收，而不是在取出时。
- en: Not like in the USA RothRA。 This is where we evaluate the action。 So remember
    I said that these four action values that you get back。 They're just numbers。
    I can't force that they necessarily all sum up to 1。0 for 100% So what I'm going
    to do here is apply some of this I'm going to look at the home payment if it's
    approach 0 I am going to calculate so this is the total this is the total amount
    so they requested a number on all of these。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不像美国的RothRA。这是我们评估动作的地方。所以记得我说过，这四个返回的动作值只是数字。我不能强迫它们都必须总和为1.0（100%）。所以我在这里要做的是应用一些这个，我会查看家庭支付，如果接近0，我将进行计算，这就是总额，他们在这些所有请求了一个数字。
- en: I need to convert those numbers to percent。 so I sum it up and then I'm going
    to divide each of these by this total account Now I don't want to cause a division
    by zero。 So if the total account has fallen below or is approaching0 then I just
    return these all to 0。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要将这些数字转换为百分比。所以我将它们相加，然后将每个值除以这个总金额。现在我不想导致除以零的情况。如果总金额降到或接近0，我就将这些全部返回为0。
- en: this is kind of an error situation， but it does happen when bad strategies are
    evaluated。 so we have to just quickly set those all to  zero and we're done otherwise
    we divide them all this is how we normalize it into something that sums up to
    1。0 and then we return these four values and these are。Real percents After we've
    converted these larger numbers， the TF agents gives you into percents。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这算是一种错误情况，但在评估不良策略时确实会发生。所以我们必须迅速将这些全部设为0，否则我们就将它们全部除以，这就是我们如何将其标准化为总和为1.0，然后返回这四个值。这些是真实的百分比，在我们将这些较大的数字转换为百分比后，TF代理给出的结果。
- en: This is the step function。 What this is doing is basically performing the simulation
    so we've got the action。 We're going to use the action values that were passed
    in and pass it to that evaluate action function that we just saw。 we're going
    also break apart the values that are in the state vector so that we have then
    I don't want to constantly be looking into a hash map that would be annoying so
    we deal first with expenses。 that's that certain percent that you have to pay
    that's necessities like internet connection。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是步骤函数。基本上，它执行模拟，所以我们有了行动。我们将使用传入的行动值，并将其传递给我们刚才看到的评估行动函数。我们还会分开状态向量中的值，这样我就不需要不断查找哈希映射，这会很烦人，因此我们首先处理支出。那是你必须支付的某个百分比，像互联网连接这样的必需品。
- en: at least it is to me。 But yeah， food water， those kind of things。 This deals
    with first the tax advantage deposit account。 We deal with that one first because
    you are limited in how much you can put into that in a given year。 if you start
    to exceedcc to that then it just decreases you so that you cannot。 This is like
    a 401。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 至少对我来说是这样。不过，食物、水等这些东西。我们首先处理税收优惠存款账户。我们首先处理这个账户，因为你在一年内对存款金额有限制。如果你开始超出这个限额，那么你就会被减少，直到不能再存。这就像401计划。
- en: there is a company match。 We calculate how much tax the person owes deduct that
    the home payment。Kind of complicated because we are using amortization。 so I put
    in the amortization formulas so that you basically got that continuously decreasing
    interest payment and continuously increasing principal payment amount。If you had
    a late payment you're in trouble and we count that up。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有公司配对。我们计算个人欠缴的税款，扣除住房付款。因为我们使用了摊销，所以这有点复杂。因此，我放入了摊销公式，使得你基本上拥有持续减少的利息支付和持续增加的本金支付金额。如果你有逾期付款，你就麻烦了，我们会将其统计起来。
- en: if it gets more than 15 late payments， they will foreclose on you and forecloing。
    basically they just sell your house off。 you lose 20000 in fees that they just
    charge for the trouble of selling your house and then so you lose a considerable
    amount。't you don't really want to go there。 Otherwise， if it's just a normal
    late payment。 you're charged a $10% late fee based on the amount of your payment。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果逾期付款超过15次，他们将会止赎你，止赎基本上是他们把你的房子卖掉。你将损失20000元的费用，这只是为了卖掉房子所造成的麻烦，因此你将损失相当可观的金额。你不希望这样发生。否则，如果只是正常的逾期付款，你将根据付款金额收取10%的滞纳金。
- en: not a 10% late fee based on the principal value or house that'd be horrendous。
    And then we update the house value to reflect the payment。 we make the deposits
    into the taxable account and we make sure to credit investment returns。 the investment
    returns are pretty similar， we do randomly vary those a bit。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不要基于本金价值收取10%的滞纳金，否则那将是可怕的。然后我们更新房屋价值以反映付款情况。我们将存款放入应税账户，并确保计算投资回报。投资回报大致相似，我们会随机稍微调整一下。
- en: just like we randomly vary the inflation amount， this just put some randomness
    into the game。 which makes it harder but this is also what reinforcement learning
    is quite good at doing dealing with the random numbers。 we deal with yearly events
    So every time we're at month 12 monthly events。Not too much there。 We're just
    aging the agent by one。 It's a time to retire。 I mean， they retire at 80。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们随机调整通货膨胀金额一样，这在游戏中增加了一些随机性。这使得游戏更具挑战性，但强化学习在处理随机数字方面非常擅长。我们处理年度事件，因此每当我们到达第12个月时就会发生每月事件。没什么特别的。我们只是让代理人老去一年。是时候退休了。我的意思是，他们在80岁时退休。
- en: I guess I probably should have said that this is more like when they died or
    something I'm making everybody quit at the same amount。 but also the person works
    right up to 80 so again this is a simplified simulator and then we normalize and
    finish up So for the state I'm dividing each of these state values your accounts
    by multiples of a million that gets it into much smaller values and it makes it
    easier for the neural network to deal with usually you don't want to be throwing
    in large multimillion dollar numbers into a neural network that just doesn't work
    that well when we perform the yearly we are basically counting the investment
    rate or the inflation rate we're adding some randomness into that we give them
    the salary increase we increase the home value by the amount that inflation dictates
    and so on this is reset it starts it over're reborn so to speak we set the late
    count to0 all all these values to zero so that we are starting over from。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我可能应该说这更像是他们去世时的情况，我让每个人在同样的时间点退出。 但同时这个人工作到80岁，所以这又是一个简化的模拟器，然后我们归一化并结束。
    所以在这个状态中，我将每个状态值除以百万的倍数，这样就会得到更小的值，并且这使得神经网络处理起来更容易。通常你不想把大额百万美元的数字丢进神经网络，这样效果不好。我们进行年度计算时，基本上是在计算投资回报率或通货膨胀率，我们在其中添加一些随机性，给他们薪水增加，按照通货膨胀的规定增加房屋价值，等等。这个是重置的，重新开始，可以说我们将计数器重置为0，所有这些值重置为0，以便从头开始。
- en: Geting we generate a random salary based on between those ranges。 We calculate
    that random house multiple between 1。5 and 4。 We calculate the mortgage amount
    so that we know the amount of the principal and there's that famous amortization
    formula and then we set this all into the state。 Render is pretty simple。 Rler
    is using pill so it's returning an image basically just display all of these values
    in a green font with a black background。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这些范围之间生成随机薪水。 我们计算这个随机房屋的倍数，介于1.5和4之间。 我们计算抵押贷款金额，以便知道本金的数量，那个著名的摊销公式，然后我们把这些都设置到状态中。
    渲染相当简单。 Rler使用Pillow，所以基本上是返回一张图像，将所有这些值以绿色字体在黑色背景上显示出来。
- en: '''ll see what that looks like in a moment and print those all out。 And that''s
    it at that point you have an environment Now you need to register your environment
    so that you can see it and I call this just simple game of life again not conway''s
    game of life I''m kind of thinking of the old board game life。'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会看看那是什么样子，并打印出来。 到那时，你就有了一个环境。 现在你需要注册你的环境，以便能看到它，我把这个称为简单的生命游戏，而不是康威的生命游戏，我有点想起老牌桌游《生命》。
- en: All right so let's test the environment， make sure that it works。 I instantiate
    it and create it just like this I do a reset and all I'm doing here is just playing
    the game of life so to speak with fixed values So one in one I am basically paying
    off the home loan and investing into the tax advantage account which is actually
    a very good strategy I didn't quite make this。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们测试环境，确保它能正常工作。 我实例化并创建它，就像这样，我做一个重置，这里我只是玩生命游戏，用固定的值。 所以在这里，我基本上是在偿还房屋贷款并投资到税收优势账户，这实际上是一个非常好的策略，我没有做到这一点。
- en: Simulator complicated enough that there is a tremendous amount for the deep
    reinforcement learning to learn。 but this alone performs quite well。 You can see
    that it generates a net worth of about $8 million at the end。 so thats that's
    good。 Also remember that's not inflation adjusted So over 80 years that might
    not be quite quite as good。 hyperpara。 I adjusted these a little bit to make it
    so that it it learns relatively well。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟器没有复杂到足以让深度强化学习学习大量内容。 但光是这点就表现得相当不错。 你可以看到到最后它生成了约800万美元的净资产，所以这很好。 另外，请记住这不是通货膨胀调整后的数值，因此在80年后这可能就没那么好。
    我稍微调整了一下超参数，以使其学习相对较好。
- en: You could definitely do some more work here。 The main thing that I change is
    we do 50000 iterations。 and we do 50 collection steps on each of those iterations。
    So that's how many episodes are actually run just so that we have enough data
    each time to work this on。 batch size is 64。 We log every 2500 and we evaluate
    every 5000 Now。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你确实可以在这里做更多的工作。 我改变的主要内容是我们进行50000次迭代，并且在每次迭代中进行50次收集步骤。 所以这实际上运行了多少集，以便每次都有足够的数据来处理。批量大小为64。我们每2500次记录一次，每5000次评估一次。
- en: this is what our environment looks like if we want to just render it so that
    we can see it。 this is what it looks like。 And later we'll look at the video and
    we can watch this person age to 80 and see what they're doing。 We create the two
    environments so that we can both evaluate this and train it and not。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想渲染环境以便看到它，这就是我们环境的样子。这看起来是这样的。稍后我们会观看视频，看到这个人老到80岁时的状态，以及他们在做什么。我们创建了两个环境，以便同时评估和训练它。
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_5.png)'
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_6.png)'
- en: Step on each other。 Now we are using not a DQN。 We had a deep Q learning network
    before the problem with a deep QN。 It's not a major problem， but in this case
    it is， is that the DQN only supports discrete action space So think of a joy stick
    forcing it up down left right。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 互相踩踏。现在我们不再使用DQN。之前我们使用了深度Q学习网络，深度QN的问题在于它只支持离散动作空间。所以想象一下一个操纵杆，控制上下左右。
- en: not how hard you're pressing it we have these four accounts and I would like
    to be able to give a variable amount into each of these four retirement accounts。
    So to do this we need to use a new type of neural networkbased reinforcement learning
    called a DtPg algorithm。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不是你施加的压力有多大，我们有这四个账户，我希望能够给这四个退休账户提供不同的金额。因此，我们需要使用一种新的基于神经网络的强化学习类型，称为DtPg算法。
- en: deep deterministic polyc gradients I won't go too deep into how that algorithm
    actually works if you want to read the original paper on it。 It's here it's I
    have not gone through and implemented one of these myself but they do work quite
    well for continuous action space reinforcement learning This is what the structure
    of it sort of looks like your environment is。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度算法，我不会深入讲解这个算法的工作原理，如果你想阅读原始论文，可以在这里找到。我自己还没有实现过这些，但它们在连续动作空间的强化学习中效果很好。这就是它的结构，你的环境是。
- en: Giving you back your state， which is X。 that same state goes into the critic
    and the actor。 So there's two neural networks at play here。 This is kind of like
    agan where you have basically one neural network that's generating images a second
    one that is discriminating telling it if those images are real or not This is
    similar。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让你恢复你的状态，即X。那个状态进入评论者和行动者。所以这里有两个神经网络在发挥作用。这有点像对抗生成网络，基本上有一个神经网络生成图像，第二个则是在判断那些图像是否真实。这是相似的。
- en: you've got two adversarial neural networks you've got the actor the really adversarial
    of each other。 What it's really doing is the actor is returning in this case for
    the game of life it would return for floating point numbers and those are the
    actions that you're going to get。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你有两个对抗性神经网络，行动者和彼此的真正对抗者。实际上，行动者在这种情况下返回浮点数，这些就是你将得到的动作。
- en: The reason a deep Q network like that like we had before could not do continuous
    values is you need this nice table where you have where you're potentially getting
    a value for every single one of those actions。 What we have now is we feed in
    the current state and the actor return the step that wants it to take the action
    and the critic。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前的深度Q网络之所以无法处理连续值，是因为你需要一个漂亮的表格，在每个动作中都有一个潜在的值。我们现在做的是输入当前状态，行动者返回想要采取的动作步骤，以及评论者。
- en: Evaluates how well the actor did on picking continuous values for the action。
    So you're picking those four continuous values for the accounts。 You're not picking
    which account。 you're picking the four continuous values。 How well those continuous
    values maximized the eventual reward and that's basically how actor critic reinforcement
    neural network works。 The TD error is important to。 That's the temporal difference
    error So that's just looking at how different was the actor's estimation of future
    reward versus what the environment actually gave it back as the future reward。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 评估行动者在选择连续值时的表现。因此，你选择这四个连续值，而不是选择哪个账户。你选择的是这四个连续值。这些连续值如何最大化最终奖励，这基本上就是行动者-评论者强化神经网络的工作原理。时序差分误差也很重要。这是时间差分误差，它只是看行动者对未来奖励的估计与环境实际给予的未来奖励之间的差异。
- en: That is what the objective function is trying to minimize These are the hyperparameters
    for these two neural networks we create those two neural networks and then we
    create the so we create the actor first。 and then we create the critic and then
    we put both of those together into the DDPG agent initialize it and we're basically
    ready to go。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是目标函数试图最小化的内容。这两个神经网络的超参数，我们创建这两个神经网络，然后我们先创建演员，然后创建评论员，再将这两者结合成DDPG代理，初始化它，我们基本上就准备好了。
- en: We're going to use the same metric that we've used in previous。les I've given
    you。 This is where we take a certain number of episodes。 and we look at the average
    return over those episodes。 We do our initial data collection。 just like previous
    examples。 We've got to run through the simulator a couple of times。 and actually。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前例子中使用的相同指标。在这里，我们取一定数量的回合，并查看这些回合的平均回报。我们进行初步数据收集，就像之前的例子一样。我们必须通过模拟器运行几次。
- en: in this case a thousand times and get some sample environment changes based
    on these actions and actual rewards。 That's what we will begin to train the neural
    network on。 And again。 this is the same code that I showed you in previous examples
    that we've done a reinforcement learning。 The main point of this example is how
    to create your own environment。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，进行一千次，并根据这些动作和实际奖励获取一些样本环境变化。这就是我们将开始训练神经网络的内容。而且，这段代码与我在之前强化学习示例中展示的相同。这一例子的主要点是如何创建你自己的环境。
- en: Now we're going train the agent again， this is the same code。 And by the way。
    this code changes really so little that you could actually switch out So up here。
    you could switch out my simple game of life and put in the mountain car continuous
    which is a continuous environment that they have in the gym。 and it would work
    just fine。 It would train the continuous mountain car。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将再次训练代理，这与之前的代码相同。顺便提一下，这段代码的变化非常小，以至于你实际上可以替换掉我的简单生命游戏，放入山地车连续环境，这是他们在健身房里的一个连续环境，它也能正常工作。它会训练连续山地车。
- en: So back down to here where we're training the agent。 It goes through all the
    iterations requested。😊。Coects the requested number of data and this is the output。
    If you want to go through line by line on these my previous examples on Tensorflowlow
    agents learning is shows you exactly how this works。 but it's just a loop going
    through and printing out the results as as it trains and this is going up to 50000
    steps the visualization looks like this So it learns it really quite quickly and
    then reaches a plateau and this is not that hard of an optimization problem for
    it to learn the videos for this if we play one of these you'll see basically it's
    counting up their aging 2122 and you can see it learned to apply。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以回到这里，我们在训练代理。它完成了所有请求的迭代。😊。收集请求的数据量，这是输出。如果你想逐行查看我的TensorFlow代理学习的以前例子，它会准确地告诉你这是如何工作的，但这只是一个循环，遍历并打印出结果，因为它在训练，这个过程最多达到50000步，结果的可视化看起来像这样。所以它学习得非常快，然后达到一个平台，这对它来说不是一个很难的优化问题。如果我们播放这些视频中的一个，你基本上会看到它在计数21122，并且你可以看到它学会了应用。
- en: this is a pretty boring agent actually it just gives up on the house。s it be
    confiscated and then it just goes into investing in the tax advantage savings
    it doesn't quite figure out that there's a limit to that it probably should have
    put the rest in the tax textable and as you retrain it。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代理实际上很无聊，它只是在房子被没收后放弃，然后就去投资于税收优惠储蓄。它没有弄明白这一点的限制，可能应该把剩余部分放在应税账户里，随着你重新训练它。
- en: it learns。![](img/bd52a9eda0cfe550681151d31dca6c38_8.png)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它学习。![](img/bd52a9eda0cfe550681151d31dca6c38_8.png)
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_9.png)'
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_10.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_10.png)'
- en: Different approaches so I've run this a couple of times。 there are smarter agents
    that trains this one this one's okay。 I mean it's building up the net worth and
    you can went to zero because it sold it I do hit them with rent or something like
    that but anyway is creating your。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的方法，所以我运行了几次。有一些更智能的代理在训练，这个还可以。我是说它在增加净资产，但你可能会因为出售而降到零。我确实用租金或者其他方式击中了它，但无论如何，它在创造你的。
- en: '![](img/bd52a9eda0cfe550681151d31dca6c38_12.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd52a9eda0cfe550681151d31dca6c38_12.png)'
