- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ä½¿ç”¨ Scikit-learn è¿›è¡Œæœºå™¨å­¦ä¹ ï¼Œ4å°æ—¶å®æˆ˜è§†è§’åˆ·æ–°çŸ¥è¯†æ¡†æ¶ï¼Œåˆå­¦è€…è¿›é˜¶å¿…å¤‡ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P10ï¼š10ï¼‰æ­£åˆ™åŒ–å’Œæ ‡å‡†åŒ–
    - ShowMeAI - BV16u41127nr
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ä½¿ç”¨ Scikit-learn è¿›è¡Œæœºå™¨å­¦ä¹ ï¼Œ4å°æ—¶å®æˆ˜è§†è§’åˆ·æ–°çŸ¥è¯†æ¡†æ¶ï¼Œåˆå­¦è€…è¿›é˜¶å¿…å¤‡ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P10ï¼š10ï¼‰æ­£åˆ™åŒ–å’Œæ ‡å‡†åŒ–
    - ShowMeAI - BV16u41127nr
- en: '![](img/ba488996234f1aee5936aea9ea40f4bf_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba488996234f1aee5936aea9ea40f4bf_0.png)'
- en: Well in this video I'm may be talking about two topicsã€‚ one is regularization
    and other is standardizationã€‚ standardization is something that we have to use
    a lot in this course and understandã€‚ and it's really relatively simpleï¼Œ but regularization
    is a very complicated topic and might require a lot of time and a more advanced
    machine learning courseã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘å¯èƒ½ä¼šè°ˆè®ºä¸¤ä¸ªä¸»é¢˜ï¼Œä¸€ä¸ªæ˜¯æ­£åˆ™åŒ–ï¼Œå¦ä¸€ä¸ªæ˜¯æ ‡å‡†åŒ–ã€‚æ ‡å‡†åŒ–æ˜¯æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­ç»å¸¸ä½¿ç”¨å¹¶ç†è§£çš„å†…å®¹ï¼Œç›¸å¯¹ç®€å•ï¼Œä½†æ­£åˆ™åŒ–æ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„è¯é¢˜ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´å’Œæ›´é«˜çº§çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ã€‚
- en: I'm not trying to get into any math regarding regularizationã€‚ and I'm just trying
    to try to give the kind of simplest intuition and we aren't going to get deep
    into itã€‚ but just wanting to know that that's an important and deeper topicã€‚So
    in terms of things that we've already doneã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸æƒ³æ·±å…¥æ¢è®¨æ­£åˆ™åŒ–çš„æ•°å­¦ï¼Œæˆ‘åªæ˜¯æƒ³ç»™å‡ºæœ€ç®€å•çš„ç›´è§‰ï¼Œæˆ‘ä»¬ä¸ä¼šæ·±å…¥ç ”ç©¶ï¼Œä½†å¸Œæœ›å¤§å®¶çŸ¥é“è¿™æ˜¯ä¸€ä¸ªé‡è¦ä¸”æ·±åˆ»çš„è¯é¢˜ã€‚è‡³äºæˆ‘ä»¬å·²ç»åšè¿‡çš„äº‹æƒ…ã€‚
- en: '![](img/ba488996234f1aee5936aea9ea40f4bf_2.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba488996234f1aee5936aea9ea40f4bf_2.png)'
- en: We've been using logistic regression a lot and a problem that it has that I
    haven't talked about is that it's very sensitive to scalingã€‚And so for exampleï¼Œ
    you might have a data set and there might be some numbers in it with specific
    unitsã€‚And you might get one result of due do the classification if you change
    those unitsï¼Œ so for exampleã€‚ maybe you change miles to feetï¼Œ you might get a different
    resultã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»é¢‘ç¹ä½¿ç”¨é€»è¾‘å›å½’ï¼Œä½†å®ƒæœ‰ä¸€ä¸ªæˆ‘è¿˜æ²¡è°ˆåˆ°çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯å®ƒå¯¹ç¼©æ”¾éå¸¸æ•æ„Ÿã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›ç‰¹å®šå•ä½çš„æ•°å­—ã€‚å¦‚æœä½ æ”¹å˜è¿™äº›å•ä½ï¼Œæ¯”å¦‚æŠŠè‹±é‡Œæ¢æˆè‹±å°ºï¼Œå¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„åˆ†ç±»ç»“æœã€‚
- en: which is of course not what we want we just care about the actual kind of informationã€‚
    not what units somebody arbitrarily chose to useã€‚Why is that well it's because
    logistic regression is applying this technique to regularizationï¼Ÿ
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œæˆ‘ä»¬å…³å¿ƒçš„æ˜¯çœŸå®çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æŸäººéšæ„é€‰æ‹©çš„å•ä½ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºé€»è¾‘å›å½’å°†è¿™ç§æŠ€æœ¯åº”ç”¨äºæ­£åˆ™åŒ–ã€‚
- en: Which tries to use smaller coefficientes and in generalã€‚ not have a very large
    coefficient on just one of our featuresã€‚You can imagine that I have lots and lots
    of feature columnsã€‚That just by chanceã€‚ maybe one of them does better on the training
    data than othersã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¯•å›¾ä½¿ç”¨è¾ƒå°çš„ç³»æ•°ï¼Œé€šå¸¸ä¸å¸Œæœ›åœ¨æŸä¸€ä¸ªç‰¹å¾ä¸Šæœ‰ä¸€ä¸ªéå¸¸å¤§çš„ç³»æ•°ã€‚ä½ å¯ä»¥æƒ³è±¡æˆ‘æœ‰å¾ˆå¤šç‰¹å¾åˆ—ï¼Œæˆ–è®¸å…¶ä¸­ä¹‹ä¸€åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾—æ¯”å…¶ä»–çš„å¥½ã€‚
- en: Even if the other feature columns are kind of still somewhat usefulã€‚ and so
    what I wouldn't want to do is just by chance choose that best one because then
    it won't work well later on a test data setã€‚ regularization basically is providing
    a motivation to use multiple features and not consider one too heavilyã€‚ even if
    that would do better in the short termã€‚So logistic regression does this linear
    regression which was the first model we learned in this course does notã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿å…¶ä»–ç‰¹å¾åˆ—ä»ç„¶æœ‰äº›æœ‰ç”¨ï¼Œæˆ‘ä¸å¸Œæœ›ä»…ä»…å› ä¸ºè¿æ°”é€‰æ‹©äº†é‚£ä¸ªæœ€å¥½çš„ç‰¹å¾ï¼Œå› ä¸ºè¿™æ ·åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šæ•ˆæœä¸å¥½ã€‚æ­£åˆ™åŒ–åŸºæœ¬ä¸Šæ˜¯æä¾›äº†ä¸€ç§åŠ¨æœºï¼Œä½¿ç”¨å¤šä¸ªç‰¹å¾ï¼Œè€Œä¸å¤ªé‡è§†å…¶ä¸­ä¸€ä¸ªï¼Œå³ä¾¿å®ƒåœ¨çŸ­æœŸå†…è¡¨ç°æ›´å¥½ã€‚å› æ­¤ï¼Œé€»è¾‘å›å½’å®ç°äº†è¿™ç§çº¿æ€§å›å½’ï¼Œè€Œè¿™æ˜¯æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­å­¦ä¹ çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ã€‚
- en: but there are also things very similar to linear regression that do such as
    ridge regression and lasal regressionã€‚ we're not trying to talk about those at
    320 but they're important and use all the time and so know that this regularization
    thing is a big dealã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿˜æœ‰ç±»ä¼¼äºçº¿æ€§å›å½’çš„ä¸œè¥¿ï¼Œæ¯”å¦‚å²­å›å½’å’Œå¥—ç´¢å›å½’ã€‚æˆ‘ä»¬åœ¨320ä¸­ä¸æ‰“ç®—è®¨è®ºè¿™äº›ï¼Œä½†å®ƒä»¬å¾ˆé‡è¦ï¼Œä¸”ç»å¸¸ä½¿ç”¨ï¼Œå› æ­¤è¦çŸ¥é“è¿™ä¸ªæ­£åˆ™åŒ–çš„æ¦‚å¿µéå¸¸é‡è¦ã€‚
- en: So what would we really likeï¼Œ we don't want our model to be sensitive to unitsã€‚
    We would like to standardize it in some way so that we have the same numbers going
    inã€‚ regardless of what their original units wereã€‚So for this example I just made
    up kind of a fake scenarioã€‚ we're measuring some sort of quantity in the real
    world three times and based on we're trying to predict what sort of category it's
    in the category will either be true or falseã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬çœŸæ­£å¸Œæœ›çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬ä¸å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹å¯¹å•ä½æ•æ„Ÿã€‚æˆ‘ä»¬å¸Œæœ›ä»¥æŸç§æ–¹å¼æ ‡å‡†åŒ–ï¼Œä½¿è¾“å…¥çš„æ•°å­—ç›¸åŒï¼Œè€Œä¸ç®¡å®ƒä»¬åŸæ¥çš„å•ä½æ˜¯ä»€ä¹ˆã€‚ä»¥æ­¤ä¾‹å­ï¼Œæˆ‘ç¼–é€ äº†ä¸€ä¸ªè™šæ„çš„åœºæ™¯ï¼Œæˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­æµ‹é‡æŸç§æ•°é‡ä¸‰æ¬¡ï¼Œå¹¶æ ¹æ®è¿™äº›æ•°æ®é¢„æµ‹å®ƒå±äºå“ªä¸ªç±»åˆ«ï¼Œç±»åˆ«å°†æ˜¯â€œçœŸâ€æˆ–â€œå‡â€ã€‚
- en: The underlying rule is that when the true lengthï¼Œ which we don't knowã€‚ is bigger
    than 5 than the category is trueã€‚ It's less than 5ã€‚ The category is falseã€‚ And
    so these three kind of noisy measurementsï¼Œ even though they know tell us exactly
    what the true length isã€‚ they give us some information about that they can help
    us guess whether it's true or falseã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬è§„åˆ™æ˜¯ï¼Œå½“çœŸå®é•¿åº¦ï¼ˆæˆ‘ä»¬ä¸çŸ¥é“ï¼‰å¤§äº5æ—¶ï¼Œç±»åˆ«ä¸ºçœŸï¼›å¦‚æœå°äº5ï¼Œç±»åˆ«ä¸ºå‡ã€‚å› æ­¤è¿™ä¸‰ä¸ªæœ‰äº›å˜ˆæ‚çš„æµ‹é‡å€¼ï¼Œå°½ç®¡å®ƒä»¬æ²¡æœ‰ç¡®åˆ‡å‘Šè¯‰æˆ‘ä»¬çœŸå®é•¿åº¦æ˜¯ä»€ä¹ˆï¼Œä½†å®ƒä»¬ç»™æˆ‘ä»¬æä¾›äº†ä¸€äº›ä¿¡æ¯ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬çŒœæµ‹å®ƒæ˜¯çœŸè¿˜æ˜¯å‡ã€‚
- en: So here I have that dataï¼Œ that fake data I'm talking aboutï¼Œ the y column here
    is the categoryã€‚ and then I have my three measurementsï¼Œ x1 x to an x3ã€‚Let me just
    talk a little bit about how I'm generating thisã€‚So under anumpot randomã€‚ there
    are a bunch of functions that will generate random dataï¼Œ I'm doing a normal distribution
    hereã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘æœ‰æˆ‘æ‰€è¯´çš„é‚£äº›è™šå‡æ•°æ®ï¼Œyåˆ—æ˜¯ç±»åˆ«ã€‚ç„¶åæˆ‘æœ‰æˆ‘çš„ä¸‰ä¸ªæµ‹é‡å€¼ï¼Œx1ã€x2å’Œx3ã€‚è®©æˆ‘è°ˆè°ˆæˆ‘æ˜¯å¦‚ä½•ç”Ÿæˆè¿™äº›çš„ã€‚åœ¨numpyçš„éšæœºå‡½æ•°ä¸‹ï¼Œæœ‰è®¸å¤šå‡½æ•°å¯ä»¥ç”Ÿæˆéšæœºæ•°æ®ï¼Œæˆ‘åœ¨è¿™é‡Œåšçš„æ˜¯æ­£æ€åˆ†å¸ƒã€‚
- en: you can sample from different distributionsï¼Œ you don't know what that means
    that's fine for this courseã€‚ but basically I'm generating1000 random numbers with
    an average of four and putting them in hereã€‚And so this will be an array of numbersã€‚
    and then I'm sayingï¼Œ wellï¼Œ whatever that's fiveã€‚ greater than fiveï¼Œ I want to
    have a trueï¼Œ otherwise I'll have a falseã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä»ä¸åŒçš„åˆ†å¸ƒä¸­æŠ½æ ·ï¼Œå¦‚æœä½ ä¸çŸ¥é“è¿™æ„å‘³ç€ä»€ä¹ˆï¼Œé‚£å¯¹è¿™é—¨è¯¾æ¥è¯´æ²¡å…³ç³»ã€‚ä½†åŸºæœ¬ä¸Šï¼Œæˆ‘åœ¨è¿™é‡Œç”Ÿæˆäº†1000ä¸ªéšæœºæ•°ï¼Œå¹³å‡å€¼ä¸º4ï¼Œç„¶åå°†å®ƒä»¬æ”¾å…¥è¿™é‡Œã€‚è¿™å°†æ˜¯ä¸€ä¸ªæ•°å­—æ•°ç»„ã€‚ç„¶åæˆ‘åœ¨è¯´ï¼Œä»»ä½•å¤§äº5çš„ï¼Œæˆ‘å¸Œæœ›å®ƒä¸ºçœŸï¼Œå¦åˆ™æˆ‘å°±ä¼šå¾—åˆ°å‡ã€‚
- en: When I'm looking at this data frame down hereï¼Œ truefe does not directly go then
    into any of these columns it's unknownã€‚ but category does and categories what
    we're trying to predictã€‚So how are we trying to predict if we don't know true
    feetï¼Œ why have these three other columnsã€‚ which are basically just true feet plus
    some random noiseã€‚ So if I look at it down hereã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘æŸ¥çœ‹ä¸‹é¢çš„æ•°æ®æ¡†æ—¶ï¼Œtruefeå¹¶æ²¡æœ‰ç›´æ¥è¿›å…¥è¿™äº›åˆ—ï¼Œå®ƒæ˜¯æœªçŸ¥çš„ï¼Œä½†ç±»åˆ«æ˜¯å·²çŸ¥çš„ï¼Œè€Œç±»åˆ«æ˜¯æˆ‘ä»¬è¯•å›¾é¢„æµ‹çš„ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•è¿›è¡Œé¢„æµ‹å‘¢ï¼Ÿæ—¢ç„¶æˆ‘ä»¬ä¸çŸ¥é“çœŸå®çš„é•¿åº¦ï¼Œä¸ºä»€ä¹ˆè¿˜ä¼šæœ‰å¦å¤–ä¸‰ä¸ªåˆ—å‘¢ï¼Ÿå®ƒä»¬åŸºæœ¬ä¸Šæ˜¯å®é™…é•¿åº¦åŠ ä¸Šä¸€äº›éšæœºå™ªå£°ã€‚æ‰€ä»¥å¦‚æœæˆ‘åœ¨ä¸‹é¢æŸ¥çœ‹ä¸€ä¸‹ã€‚
- en: let me look at this first oneã€‚ All three measurements were less than5ã€‚ So it
    makes a lot of sense that I'll predict that the y is less than than5ã€‚ Maybe I
    can even look at some more cases hereã€‚ I wonder ifã€‚I can see where it's trueã€‚Let
    me do that So I can see some other cases here where it's true rightï¼Œ all in this
    caseã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘çœ‹çœ‹ç¬¬ä¸€ä¸ªæµ‹é‡å€¼ã€‚æ‰€æœ‰ä¸‰ä¸ªæµ‹é‡å€¼éƒ½å°äº5ã€‚æ‰€ä»¥æˆ‘é¢„æµ‹yä¼šå°äº5æ˜¯éå¸¸åˆç†çš„ã€‚ä¹Ÿè®¸æˆ‘è¿˜å¯ä»¥æŸ¥çœ‹ä¸€äº›å…¶ä»–æƒ…å†µã€‚æˆ‘æƒ³çŸ¥é“æ˜¯å¦èƒ½çœ‹åˆ°å®ƒä¸ºçœŸçš„åœ°æ–¹ã€‚è®©æˆ‘æ¥åšä¸€ä¸‹ï¼Œè¿™æ ·æˆ‘å¯ä»¥çœ‹åˆ°å…¶ä»–ä¸€äº›æƒ…å†µï¼Œè¿™äº›æƒ…å†µåœ¨è¿™ä¸ªä¾‹å­ä¸­éƒ½æ˜¯æ­£ç¡®çš„ã€‚
- en: all the measurements were greater than5ã€‚ So I say it's trueã€‚ This is kind of
    a more interesting exampleã€‚ This number is very largeã€‚ rightã€‚ One measurement
    was like almost of7ã€‚And even though the other two measurements were less than
    5ã€‚ this was enough of a signal that the model decide it's trueã€‚ Wellï¼Œ it's true
    overallã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„æµ‹é‡å€¼éƒ½å¤§äº5ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºè¿™æ˜¯çœŸçš„ã€‚è¿™æ˜¯ä¸€ä¸ªæ›´æœ‰è¶£çš„ä¾‹å­ã€‚è¿™ä¸ªæ•°å­—éå¸¸å¤§ï¼Œå¯¹å§ã€‚ä¸€ä¸ªæµ‹é‡å€¼å‡ ä¹æ˜¯7ã€‚å³ä½¿å…¶ä»–ä¸¤ä¸ªæµ‹é‡å€¼å°äº5ï¼Œè¿™ä¹Ÿè¶³ä»¥è®©æ¨¡å‹åˆ¤æ–­ä¸ºçœŸã€‚å¥½å§ï¼Œæ€»ä½“ä¸Šå®ƒæ˜¯çœŸçš„ã€‚
- en: and hopefully the model will decide the sameã€‚ Okayï¼Œ so that's the data we're
    working withã€‚ And let's see if we canã€‚Train a model to try to predict thisã€‚ so
    I'm going to create a model and I'm going to use a linear or logistic regression
    modelã€‚And I'm going to fit itã€‚To my dataï¼Œ and so I may have some x and and Y's
    for my Y'sã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›æ¨¡å‹ä¼šå¾—å‡ºç›¸åŒçš„ç»“è®ºã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„æ•°æ®ã€‚è®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥å°è¯•é¢„æµ‹è¿™ä¸ªã€‚å› æ­¤ï¼Œæˆ‘å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘å°†ä½¿ç”¨çº¿æ€§æˆ–é€»è¾‘å›å½’æ¨¡å‹ã€‚æˆ‘ä¼šå°†å…¶æ‹Ÿåˆåˆ°æˆ‘çš„æ•°æ®ä¸­ï¼Œæ‰€ä»¥æˆ‘å¯èƒ½ä¼šæœ‰ä¸€äº›Xå’ŒYçš„å€¼ã€‚
- en: I'm just going to pull out y column for my training data and for my Xã€‚I want
    to pass in a list of all my columns that contain features so x1 and x2 and x3ã€‚
    and I'm going be using these againï¼Œ so I'm actually try to put this in a variable
    called x columnsã€‚And then I don't have to keep typing that whole long thing every
    timeã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä»æˆ‘çš„è®­ç»ƒæ•°æ®ä¸­æå–yåˆ—ï¼Œå¯¹äºæˆ‘çš„Xï¼Œæˆ‘æƒ³ä¼ å…¥ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç‰¹å¾åˆ—çš„åˆ—è¡¨ï¼Œæ‰€ä»¥æ˜¯x1ã€x2å’Œx3ã€‚æˆ‘ä¼šå†æ¬¡ä½¿ç”¨è¿™äº›ï¼Œæ‰€ä»¥æˆ‘å®é™…ä¸Šä¼šæŠŠå®ƒæ”¾å…¥ä¸€ä¸ªåä¸ºx
    columnsçš„å˜é‡ä¸­ã€‚è¿™æ ·æˆ‘å°±ä¸éœ€è¦æ¯æ¬¡éƒ½è¾“å…¥é‚£ä¹ˆé•¿çš„å†…å®¹ã€‚
- en: And so I fit it and that's straight and so pretty soon I may look at the coefficients
    for this modelã€‚ but before that I just want to as an asideï¼Œ see what accuracy
    it has if I want to see the accuracy of the modelã€‚ I can just say instead of fit
    I want to score and then to be realistic I shouldn't score it on data that I haven't
    seen before instead of the thing I trained it on right so this is kind of a better
    test and I see that it has 89% accuracy is that good 89% seems high that we would
    be right that often but let me show you why it's not necessarilyã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è¿›è¡Œäº†æ‹Ÿåˆï¼Œè¿™å¾ˆç®€å•ï¼Œå¾ˆå¿«æˆ‘å¯èƒ½ä¼šæŸ¥çœ‹è¿™ä¸ªæ¨¡å‹çš„ç³»æ•°ã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘æƒ³é¡ºä¾¿çœ‹çœ‹å¦‚æœæˆ‘æƒ³æŸ¥çœ‹æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œå®ƒæœ‰å¤šé«˜ã€‚æˆ‘å¯ä»¥è¯´ï¼Œä¸æ˜¯æ‹Ÿåˆï¼Œè€Œæ˜¯è¯„åˆ†ï¼Œç„¶åä¸ºäº†ç°å®èµ·è§ï¼Œæˆ‘ä¸åº”è¯¥åœ¨æˆ‘ä¹‹å‰æ²¡è§è¿‡çš„æ•°æ®ä¸Šè¿›è¡Œè¯„åˆ†ï¼Œè€Œæ˜¯æˆ‘è®­ç»ƒçš„ä¸œè¥¿ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ç§æ›´å¥½çš„æµ‹è¯•ï¼Œæˆ‘çœ‹åˆ°å®ƒæœ‰
    89% çš„å‡†ç¡®ç‡ï¼Œè¿™æ ·å¥½ï¼Ÿ89% ä¼¼ä¹å¾ˆé«˜ï¼Œæˆ‘ä»¬ä¼šé‚£ä¹ˆé¢‘ç¹åœ°æ­£ç¡®ï¼Œä½†è®©æˆ‘ç»™ä½ çœ‹çœ‹ä¸ºä»€ä¹ˆè¿™ä¸ä¸€å®šã€‚
- en: If I look at this Y columnï¼Œ I see it's almost always false and indeed if I say
    value countsã€‚ I could see it's only true less than 20% of the timeã€‚ and I can
    actually just divide this by the length of tests to see thatã€‚ And so what this
    means is that even if I had a very naive modelï¼Œ it just always says falseã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘æŸ¥çœ‹è¿™ä¸ª Y åˆ—ï¼Œæˆ‘å‘ç°å®ƒå‡ ä¹æ€»æ˜¯ä¸ºå‡ï¼Œå®é™…ä¸Šå¦‚æœæˆ‘è¯´å€¼è®¡æ•°ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°å®ƒåªæœ‰å°‘äº 20% çš„æ—¶é—´ä¸ºçœŸã€‚æˆ‘å®é™…ä¸Šå¯ä»¥å°†å…¶é™¤ä»¥æµ‹è¯•çš„é•¿åº¦æ¥çœ‹è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œè¿™æ„å‘³ç€å³ä½¿æˆ‘æœ‰ä¸€ä¸ªéå¸¸ç®€å•çš„æ¨¡å‹ï¼Œå®ƒæ€»æ˜¯è¯´å‡ã€‚
- en: I would be getting 81% accuracyï¼Œ 89% is betterï¼Œ but in that context it's not
    so amazing just given that there's so much skew in that columnã€‚Okayï¼Œ so after
    I look at the accuracy of a modelï¼Œ the next thing I'll often want to look at are
    my coefficientsã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šå¾—åˆ° 81% çš„å‡†ç¡®ç‡ï¼Œ89% æ›´å¥½ï¼Œä½†åœ¨é‚£ç§æƒ…å†µä¸‹å¹¶ä¸æ˜¯é‚£ä¹ˆæƒŠäººï¼Œå› ä¸ºè¿™ä¸€åˆ—çš„åå·®å¤ªå¤§ã€‚å¥½å§ï¼Œæ‰€ä»¥åœ¨æˆ‘æŸ¥çœ‹æ¨¡å‹çš„å‡†ç¡®ç‡åï¼Œæ¥ä¸‹æ¥æˆ‘é€šå¸¸æƒ³æŸ¥çœ‹çš„æ˜¯æˆ‘çš„ç³»æ•°ã€‚
- en: and I like to plot those in some wayï¼Œ ohï¼Œ my model coefficientsã€‚And I see those
    are right here and and I'd like have some sort of bar plotsã€‚ I know that these
    things are paired up with these x columns right so this is the coefficient for
    x1 so on and so forthã€‚ And so the way I'll often make such a bar plot is I'll
    say PDd do seriesã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å–œæ¬¢ä»¥æŸç§æ–¹å¼ç»˜åˆ¶è¿™äº›ï¼Œæˆ‘çš„æ¨¡å‹ç³»æ•°ã€‚è€Œä¸”æˆ‘çœ‹åˆ°è¿™äº›å°±åœ¨è¿™é‡Œï¼Œæˆ‘æƒ³è¦æŸç§æ¡å½¢å›¾ã€‚æˆ‘çŸ¥é“è¿™äº›ä¸è¿™äº› x åˆ—æ˜¯æˆå¯¹çš„ï¼Œæ‰€ä»¥è¿™æ˜¯ x1 çš„ç³»æ•°ï¼Œä»¥æ­¤ç±»æ¨ã€‚å› æ­¤æˆ‘é€šå¸¸åˆ¶ä½œè¿™æ ·çš„æ¡å½¢å›¾çš„æ–¹æ³•æ˜¯è¯´
    PDd do seriesã€‚
- en: And then I will have my Y valuesã€‚And then I'll have index equals x valuesã€‚Dot
    plotï¼Œ dot barã€‚So on the x valuesï¼Œ I'm going to use those column namesï¼Œ and I'm
    going to put the coefficientsã€‚To basically have the quantities that are going
    to go to the Y axisã€‚And it's complaining that the length of one of these things
    is one when it was supposed to be threeã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä¼šæœ‰æˆ‘çš„ Y å€¼ã€‚ç„¶åæˆ‘ä¼šæœ‰ç´¢å¼•ç­‰äº x å€¼ã€‚ç‚¹å›¾ï¼Œç‚¹æ¡å½¢å›¾ã€‚åœ¨ x å€¼ä¸Šï¼Œæˆ‘å°†ä½¿ç”¨è¿™äº›åˆ—åï¼Œå¹¶å°†ç³»æ•°æ”¾å…¥å…¶ä¸­ã€‚åŸºæœ¬ä¸Šä¼šæœ‰é‡å€¼ç”¨äº Y è½´ã€‚è€Œä¸”å®ƒåœ¨æŠ±æ€¨å…¶ä¸­ä¸€ä¸ªçš„é•¿åº¦æ˜¯
    1ï¼Œè€Œå®ƒåº”è¯¥æ˜¯ 3ã€‚
- en: and so x columnsï¼Œ that's pretty simple thereï¼Œ but if I look at thisã€‚This array
    right here what do I see I see there's it's really a twodiional thing I can flatten
    it into a one dimensional array with three numbers or if I say negative oneã€‚ it'll
    make it one dimensional it'll make it you know it doesn't matter how many numbers
    I have and I'll figure it out so I may put that here now and now I get this plot
    and this is interesting right I was talking about how maybe sometimes just by
    chance we focus more in one problem than another and that happens here right x1
    x2 and x3 we're all equally noisy but it just so happens that based on the training
    data the model thinks that x1 is more kind of more useful right that was just
    by chanceã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ x åˆ—ï¼Œè¿™é‡Œéå¸¸ç®€å•ï¼Œä½†å¦‚æœæˆ‘çœ‹çœ‹è¿™ä¸ªã€‚è¿™ä¸€æ•°ç»„ï¼Œæˆ‘çœ‹åˆ°å®ƒå…¶å®æ˜¯ä¸€ä¸ªäºŒç»´çš„ä¸œè¥¿ï¼Œæˆ‘å¯ä»¥å°†å…¶å±•å¹³ä¸ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ•°å­—çš„ä¸€ç»´æ•°ç»„ï¼Œæˆ–è€…å¦‚æœæˆ‘è¯´è´Ÿä¸€ï¼Œå®ƒä¼šå˜æˆä¸€ç»´çš„ï¼Œä¸ç®¡æˆ‘æœ‰å¤šå°‘ä¸ªæ•°å­—ï¼Œå®ƒéƒ½ä¼šæå®šï¼Œæ‰€ä»¥æˆ‘å¯ä»¥æŠŠå®ƒæ”¾åœ¨è¿™é‡Œï¼Œç°åœ¨æˆ‘å¾—åˆ°äº†è¿™ä¸ªå›¾ï¼Œè¿™å¾ˆæœ‰è¶£ï¼Œå¯¹å§ï¼Ÿæˆ‘åœ¨è°ˆè®ºæœ‰æ—¶å€™ä»…ä»…æ˜¯å¶ç„¶ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªé—®é¢˜ä¸Šæ¯”å¦ä¸€ä¸ªé—®é¢˜æ›´å…³æ³¨ï¼Œè¿™ç§æƒ…å†µåœ¨è¿™é‡Œå‘ç”Ÿäº†ï¼Œå¯¹å§ï¼Ÿx1ã€x2
    å’Œ x3 éƒ½æ˜¯åŒæ ·å˜ˆæ‚çš„ï¼Œä½†æ°å¥½åŸºäºè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹è®¤ä¸º x1 æ›´æœ‰ç”¨ï¼Œå¯¹å§ï¼Ÿè¿™çº¯ç²¹æ˜¯å¶ç„¶ã€‚
- en: And so you could imagine a worse scenario where it picks one column that I really
    likes and ignores all these other ones that have good information in itã€‚ and so
    the model will try to avoid doing thatã€‚ regularization means that we'll try not
    to put too much weight on just one factor we'll try to spread it out a little
    bit and then if you took it to an extremeã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥æƒ³è±¡ä¸€ä¸ªæ›´ç³Ÿç³•çš„æƒ…å†µï¼Œå®ƒé€‰æ‹©äº†æˆ‘éå¸¸å–œæ¬¢çš„ä¸€åˆ—ï¼Œå¿½ç•¥äº†æ‰€æœ‰å…¶ä»–å…·æœ‰è‰¯å¥½ä¿¡æ¯çš„åˆ—ã€‚å› æ­¤æ¨¡å‹ä¼šå°½é‡é¿å…è¿™ç§æƒ…å†µã€‚æ­£åˆ™åŒ–æ„å‘³ç€æˆ‘ä»¬ä¼šå°½é‡ä¸å¯¹å•ä¸€å› ç´ èµ‹äºˆè¿‡å¤šæƒé‡ï¼Œæˆ‘ä»¬ä¼šå°è¯•ç¨å¾®åˆ†æ•£ä¸€ä¸‹ï¼Œå¦‚æœä½ å°†å…¶æ¨å‘æç«¯ã€‚
- en: you might imagine that I could have a model where I just look at my interceptã€‚My
    intercept is you can imagine that being like the average and the model could just
    predict animal all my coefficients could be zero in that caseã€‚ well we all just
    predict the same thingï¼Œ and we want to have this problem I guess we have another
    problem that just want to be very accurateã€‚Okayï¼Œ so I have thatã€‚And so let me
    head back here and I'm going randomly generate this dataã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½æƒ³è±¡æˆ‘å¯ä»¥æœ‰ä¸€ä¸ªåªçœ‹æˆ‘çš„æˆªè·çš„æ¨¡å‹ã€‚æˆ‘çš„æˆªè·å¯ä»¥æƒ³è±¡æˆå¹³å‡å€¼ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹åŠ¨ç‰©ï¼Œæ‰€æœ‰æˆ‘çš„ç³»æ•°éƒ½å¯ä»¥ä¸ºé›¶ã€‚å¥½å§ï¼Œæˆ‘ä»¬éƒ½åªé¢„æµ‹åŒæ ·çš„ä¸œè¥¿ï¼Œæˆ‘æƒ³æˆ‘ä»¬æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œåªæƒ³éå¸¸å‡†ç¡®ã€‚å¥½çš„ï¼Œæˆ‘æœ‰è¿™ä¸ªã€‚æ‰€ä»¥è®©æˆ‘å›å»ï¼Œæˆ‘å°†éšæœºç”Ÿæˆè¿™äº›æ•°æ®ã€‚
- en: but this time I'm just going to change the units on this column and I'm going
    to change the units to be miles and so there's 5ã€‚280 feet in a mile so I'm just
    to make a comment here this is feet to miles like that and so I have the same
    kind of data just different unitsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™æ¬¡æˆ‘åªæ˜¯æ”¹å˜è¿™ä¸€åˆ—çš„å•ä½ï¼Œæˆ‘å°†å•ä½æ”¹ä¸ºè‹±é‡Œï¼Œå› æ­¤ä¸€è‹±é‡Œæœ‰5280è‹±å°ºã€‚æˆ‘åœ¨è¿™é‡Œæƒ³åšä¸€ä¸ªè¯„è®ºï¼Œè¿™æ˜¯è‹±å°ºåˆ°è‹±é‡Œçš„è½¬æ¢ï¼Œæ‰€ä»¥æˆ‘æœ‰ç›¸åŒç±»å‹çš„æ•°æ®ï¼Œåªæ˜¯å•ä½ä¸åŒã€‚
- en: so I might hope that my model won't do anything that differentlyã€‚And so I'm
    going to run this againã€‚ and I see that not too much has changed hereã€‚ And then
    I want to think about what's going to happen when I rerun thisã€‚So in this x2 columnã€‚The
    numbers are all much smaller now because it's in milesã€‚ and so I might expect
    that to use thisï¼Œ I might have a bigger coefficient on x2ã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¸Œæœ›æˆ‘çš„æ¨¡å‹ä¸ä¼šæœ‰ä»€ä¹ˆä¸åŒçš„å˜åŒ–ã€‚æ‰€ä»¥æˆ‘å°†å†æ¬¡è¿è¡Œè¿™ä¸ªã€‚æˆ‘çœ‹åˆ°è¿™é‡Œæ²¡æœ‰å¤ªå¤šå˜åŒ–ã€‚ç„¶åæˆ‘æƒ³æƒ³é‡æ–°è¿è¡Œæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ª x2 åˆ—ä¸­ï¼Œæ•°å­—ç°åœ¨éƒ½å°å¾—å¤šï¼Œå› ä¸ºå®ƒæ˜¯ä»¥è‹±é‡Œä¸ºå•ä½ã€‚å› æ­¤ï¼Œæˆ‘å¯èƒ½æœŸæœ›ä½¿ç”¨è¿™ä¸ªæ—¶ï¼Œx2
    çš„ç³»æ•°ä¼šæ›´å¤§ã€‚
- en: If I wanted to be just like beforeã€‚It turns out when I run itï¼Œ I actually see
    the oppositeã€‚ It's adverse to having such large coefficients on one column because
    of that regularization thing I talked aboutã€‚ So it actually decidesï¼Œ heyï¼Œ I'm
    just going to ignore x2 entirelyã€‚ I have to put a bigger or a bigger weight on
    that that I'm comfortable doing to have it be a factorã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘æƒ³å’Œä»¥å‰ä¸€æ ·ã€‚ç»“æœå‘ç°å½“æˆ‘è¿è¡Œå®ƒæ—¶ï¼Œæˆ‘å®é™…ä¸Šçœ‹åˆ°äº†ç›¸åçš„æƒ…å†µã€‚ç”±äºæˆ‘ä¹‹å‰æåˆ°çš„æ­£åˆ™åŒ–å› ç´ ï¼Œæ‹¥æœ‰ä¸€ä¸ªåˆ—ä¸Šå¦‚æ­¤å¤§çš„ç³»æ•°æ˜¯æœ‰å®³çš„ã€‚å› æ­¤ï¼Œå®ƒå®é™…ä¸Šå†³å®šï¼Œå˜¿ï¼Œæˆ‘å°†å®Œå…¨å¿½ç•¥
    x2ã€‚æˆ‘å¿…é¡»åœ¨æˆ‘èƒ½å¤Ÿæ¥å—çš„æƒ…å†µä¸‹ï¼Œç»™å®ƒä¸€ä¸ªæ›´å¤§çš„æƒé‡ã€‚
- en: So I just lost some information thereã€‚ I' using these two columns anymoreã€‚ Nowï¼Œ
    of courseã€‚ that's silly rightï¼Œ putting a bigger coefficient on it isn't really
    waiting it moreã€‚ It's just canceling out the fact that I have different units
    on itã€‚ And so there's different ways to solve thisã€‚ One is that I could just insist
    that I have the same units for everythingã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘åœ¨è¿™é‡Œä¸¢å¤±äº†ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä¸å†ä½¿ç”¨è¿™ä¸¤åˆ—ã€‚ç°åœ¨ï¼Œå½“ç„¶ï¼Œè¿™å¾ˆå‚»ï¼Œå¯¹å§ï¼ŒæŠŠæ›´å¤§çš„ç³»æ•°æ”¾ä¸Šå»å¹¶æ²¡æœ‰çœŸæ­£å¢åŠ æƒé‡ã€‚å®ƒåªæ˜¯æŠµæ¶ˆäº†æˆ‘æœ‰ä¸åŒå•ä½çš„äº‹å®ã€‚å› æ­¤ï¼Œæœ‰ä¸åŒçš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸€ç§æ˜¯æˆ‘å¯ä»¥åšæŒæ‰€æœ‰ä¸œè¥¿éƒ½æœ‰ç›¸åŒçš„å•ä½ã€‚
- en: Another way I could do it is I could try to kind of make this a little more
    uniform in some wayã€‚ And so that's what I'm going to do hereã€‚ğŸ˜Šï¼ŒI'm going head
    back hereï¼Œ and let meã€‚Let me take my training data and my actuallyï¼Œ where do I
    want to do thisï¼Ÿ
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ–¹æ³•æ˜¯æˆ‘å¯ä»¥å°è¯•ä»¥æŸç§æ–¹å¼è®©è¿™ä¸ªæ›´ç»Ÿä¸€ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘åœ¨è¿™é‡Œè¦åšçš„ã€‚ğŸ˜Šï¼Œæˆ‘å°†å›åˆ°è¿™é‡Œï¼Œè®©æˆ‘ã€‚è®©æˆ‘æ‹¿åˆ°æˆ‘çš„è®­ç»ƒæ•°æ®ï¼Œå®é™…ä¸Šï¼Œæˆ‘æƒ³åœ¨å“ªé‡Œåšè¿™ä¸ªï¼Ÿ
- en: I take these x columns even soonerï¼Œ Actuallyï¼Œ noï¼Œ that's fineã€‚ I'm going to
    leave that thereã€‚ so I'm going take my training data and I want to take a slice
    of it and I want to get all the rows and I want to get columns x1ã€‚Through R x2ã€‚And
    so this is just my features now well through x3ï¼Œ sorryï¼Œ these are just my featuresã€‚And
    I want to somehow standardize it so that they all have roughly the same scaleã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆå¿«å°±ä¼šå¤„ç†è¿™äº› x åˆ—ï¼Œå®é™…ä¸Šï¼Œä¸ï¼Œè¿™æ²¡é—®é¢˜ã€‚æˆ‘æ‰“ç®—æŠŠå®ƒç•™åœ¨é‚£é‡Œã€‚å› æ­¤ï¼Œæˆ‘å°†æå–æˆ‘çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶æƒ³å–ä¸€ä¸ªåˆ‡ç‰‡ï¼Œæˆ‘æƒ³è·å–æ‰€æœ‰è¡Œï¼Œå¹¶æå–åˆ— x1ã€‚é€šè¿‡
    R x2ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘çš„ç‰¹å¾ï¼Œç°åœ¨é€šè¿‡ x3ï¼ŒæŠ±æ­‰ï¼Œè¿™äº›åªæ˜¯æˆ‘çš„ç‰¹å¾ã€‚æˆ‘æƒ³ä»¥æŸç§æ–¹å¼æ ‡å‡†åŒ–å®ƒä»¬ï¼Œä»¥ä¾¿å®ƒä»¬å¤§è‡´å…·æœ‰ç›¸åŒçš„å°ºåº¦ã€‚
- en: And so I'm to do is I'm just going pull this out into this x variable right
    hereã€‚ and there's going be two things I'm going to doã€‚ one is that I'm going take
    the mean of each of each column just like thatã€‚ and I'm going subtract these numbers
    off of each column I'm going say thatã€‚ And so now all of these columns are centered
    at  zero right after I subtracted away the mean the average of every column is0ã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è¦åšçš„æ˜¯å°†å…¶æå–åˆ°è¿™ä¸ª x å˜é‡ä¸­ã€‚æˆ‘è¦åšä¸¤ä»¶äº‹ã€‚ä¸€æ˜¯è®¡ç®—æ¯åˆ—çš„å¹³å‡å€¼ï¼Œå°±è¿™æ ·ã€‚æˆ‘å°†ä»æ¯åˆ—ä¸­å‡å»è¿™äº›æ•°å­—ã€‚æˆ‘ä¼šè¯´è¿™æ ·ã€‚å› æ­¤ç°åœ¨æ‰€æœ‰è¿™äº›åˆ—éƒ½ä»¥é›¶ä¸ºä¸­å¿ƒï¼Œå‡å»å‡å€¼åï¼Œæ¯åˆ—çš„å¹³å‡å€¼éƒ½æ˜¯0ã€‚
- en: it turns out that that is also helpful for logistic regression to run fasterã€‚
    I'm not trying to get any details about why that's useful And then more importantly
    I want them to be on the same scaleã€‚ and so oopsï¼Œ what happen thereã€‚I jump onto
    a new column or to a new cell and so if I look at thisã€‚ that's a standard deviation
    of each column and if I have larger numbers while the standard deviation shall
    be higher and so standardization the real key part is that I'm dividing all of
    this by that standard deviation and if I do that I may get a bunch of small numbers
    that have roughly the same scale so after I've done this all of them will have
    the same average0 and then the same standard deviation of one and so this would
    be a better x data and I can actually put this back in to my training data like
    this so I may say this equals my new X data so I make it out here Im going say
    manually standardize the dataã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: äº‹å®è¯æ˜ï¼Œè¿™å¯¹é€»è¾‘å›å½’çš„è¿è¡Œé€Ÿåº¦ä¹Ÿå¾ˆæœ‰å¸®åŠ©ã€‚æˆ‘å¹¶ä¸æƒ³äº†è§£ä¸ºä»€ä¹ˆè¿™æ˜¯æœ‰ç”¨çš„ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘å¸Œæœ›å®ƒä»¬åœ¨åŒä¸€å°ºåº¦ä¸Šã€‚é‚£ä¹ˆï¼Œå“ï¼Œå‘ç”Ÿäº†ä»€ä¹ˆäº‹ï¼Ÿæˆ‘è·³åˆ°äº†æ–°çš„ä¸€åˆ—æˆ–æ–°çš„ä¸€å•å…ƒæ ¼ï¼Œæ‰€ä»¥å¦‚æœæˆ‘æŸ¥çœ‹è¿™ä¸ªï¼Œé‚£æ˜¯æ¯åˆ—çš„æ ‡å‡†å·®ï¼Œå¦‚æœæˆ‘æœ‰æ›´å¤§çš„æ•°å­—ï¼Œé‚£ä¹ˆæ ‡å‡†å·®å°±ä¼šæ›´é«˜ï¼Œå› æ­¤æ ‡å‡†åŒ–çš„å…³é”®éƒ¨åˆ†æ˜¯æˆ‘å°†æ‰€æœ‰è¿™äº›é™¤ä»¥é‚£ä¸ªæ ‡å‡†å·®ï¼Œå¦‚æœæˆ‘è¿™æ ·åšï¼Œæˆ‘å¯èƒ½ä¼šå¾—åˆ°ä¸€å †å°æ•°å­—ï¼Œå®ƒä»¬å¤§è‡´åœ¨åŒä¸€å°ºåº¦ä¸Šï¼Œæ‰€ä»¥åœ¨æˆ‘å®Œæˆè¿™ä¸€åˆ‡åï¼Œå®ƒä»¬éƒ½ä¼šæœ‰ç›¸åŒçš„å‡å€¼å’Œæ ‡å‡†å·®ä¸º1ï¼Œå› æ­¤è¿™å°†æ˜¯æ›´å¥½çš„xæ•°æ®ï¼Œæˆ‘å®é™…ä¸Šå¯ä»¥åƒè¿™æ ·æŠŠå®ƒæ”¾å›æˆ‘çš„è®­ç»ƒæ•°æ®ä¸­ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥è¯´è¿™ç­‰äºæˆ‘çš„æ–°xæ•°æ®ï¼Œæ‰€ä»¥æˆ‘æ‰‹åŠ¨å°†æ•°æ®æ ‡å‡†åŒ–ã€‚
- en: And so after I do that I run all of this stuff again and now I see that great
    x2 is back in playã€‚ even though I have different unitsï¼Œ it's not getting obsessed
    with these other columns just based on the unitsã€‚ so this was a good thing to
    do okay that's what standardization isã€‚Now it turns out that to do this rightï¼Œ
    I have to calculate this mean and standard deviation on the training dataã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨æˆ‘å®Œæˆåï¼Œæˆ‘å†æ¬¡è¿è¡Œæ‰€æœ‰è¿™äº›ï¼Œç°åœ¨æˆ‘çœ‹åˆ°x2é‡æ–°å‘æŒ¥ä½œç”¨ã€‚å°½ç®¡æˆ‘æœ‰ä¸åŒçš„å•ä½ï¼Œå®ƒå¹¶æ²¡æœ‰è¿‡äºå…³æ³¨å…¶ä»–åˆ—çš„å•ä½ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸é”™çš„åšæ³•ï¼Œå¥½çš„ï¼Œè¿™å°±æ˜¯æ ‡å‡†åŒ–çš„æ„ä¹‰ã€‚ç°åœ¨ç»“æœæ˜¯ï¼Œè¦æ­£ç¡®åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å¿…é¡»åœ¨è®­ç»ƒæ•°æ®ä¸Šè®¡ç®—è¿™ä¸ªå‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: but then I have to use that same mean and standard deviation on the test data
    I can't retake the mean on the test data and so the methodology of this gets a
    little bit complicated and so generally we won't do this generally we'll have
    ask K learn do that for us and so it turns out that there's a preprocessing step
    called standardization and we're going to use that as so manually doing this so
    I'm going to head back hereã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘å¿…é¡»åœ¨æµ‹è¯•æ•°æ®ä¸Šä½¿ç”¨ç›¸åŒçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œæˆ‘ä¸èƒ½é‡æ–°è®¡ç®—æµ‹è¯•æ•°æ®ä¸Šçš„å‡å€¼ï¼Œå› æ­¤è¿™ä¸€æ–¹æ³•è®ºä¼šå˜å¾—æœ‰äº›å¤æ‚ï¼Œæ‰€ä»¥ä¸€èˆ¬æˆ‘ä»¬ä¸ä¼šè¿™æ ·åšï¼Œæˆ‘ä»¬ä¼šè¯·Kå­¦ä¹ ä¸ºæˆ‘ä»¬å®Œæˆï¼Œæ‰€ä»¥ç»“æœæ˜¯æœ‰ä¸€ä¸ªé¢„å¤„ç†æ­¥éª¤å«åšæ ‡å‡†åŒ–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªï¼Œè€Œä¸æ˜¯æ‰‹åŠ¨è¿›è¡Œï¼Œæ‰€ä»¥æˆ‘å°†å›åˆ°è¿™é‡Œã€‚
- en: And you can see I've already import my standard scalarã€‚And I'm to run this hereã€‚And
    this is skipping now for my modelï¼Œ rightï¼Œ I'll just actually leave this for nowã€‚
    and that'll be my bad modelã€‚ What I'll do is I'll' create a new modelã€‚ which will
    be a pipeline modelã€‚And in that pipeline modelã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°æˆ‘å·²ç»å¯¼å…¥äº†æˆ‘çš„æ ‡å‡†åŒ–å™¨ã€‚æˆ‘å°†åœ¨è¿™é‡Œè¿è¡Œè¿™ä¸ªã€‚è¿™ç°åœ¨è·³è¿‡äº†æˆ‘çš„æ¨¡å‹ï¼Œå¯¹å§ï¼Œæˆ‘ç°åœ¨å°±å…ˆä¸å¤„ç†è¿™ä¸ªã€‚è¿™å°†æ˜¯æˆ‘çš„ç³Ÿç³•æ¨¡å‹ã€‚æˆ‘å°†åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ªç®¡é“æ¨¡å‹ã€‚åœ¨è¿™ä¸ªç®¡é“æ¨¡å‹ä¸­ã€‚
- en: I want to have a standard scalar followed by a logistic regressionã€‚Just like
    that andã€‚This oneã€‚ I may have to actually create them like thatã€‚I have to give
    them namesï¼Œ rightã€‚ So I'm going pass tus hereã€‚ So I'm going to call that standard
    scalarã€‚Okayã€‚ I have to put Thomas to separate these thingsã€‚And thenï¼Œ then we go
    have this logistic regressionã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³è¦ä¸€ä¸ªæ ‡å‡†åŒ–å™¨ï¼Œåé¢è·Ÿç€é€»è¾‘å›å½’ã€‚å°±è¿™æ ·ã€‚è¿™ä¸€ä¸ªï¼Œæˆ‘å¯èƒ½ç¡®å®è¦è¿™æ ·åˆ›å»ºå®ƒä»¬ã€‚æˆ‘å¿…é¡»ç»™å®ƒä»¬å‘½åï¼Œå¯¹å§ã€‚æ‰€ä»¥æˆ‘å°†æŠŠå®ƒç§°ä¸ºæ ‡å‡†åŒ–å™¨ã€‚å¥½çš„ã€‚æˆ‘å¿…é¡»ç”¨é€—å·åˆ†éš”è¿™äº›ä¸œè¥¿ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æœ‰è¿™ä¸ªé€»è¾‘å›å½’ã€‚
- en: Like that And this is my new model and then it turns out all this stuff I was
    doing before of like fittingã€‚ for example it can work the same way I can fit just
    like I did before because I this new one also fit and so I can do that I could
    also score it like I did before let me score it now and I get something very similar
    and then what's might be interesting is that when I actually do this when I actually
    try to get this bar plot it should show that it's back in play right even though
    the nonstandardized version is ignoring x2 nowã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒè¿™æ ·ï¼Œè¿™æ˜¯æˆ‘çš„æ–°æ¨¡å‹ï¼Œç„¶åå‘ç°ä¹‹å‰æˆ‘åšçš„æ‰€æœ‰æ‹Ÿåˆå·¥ä½œï¼Œæ¯”å¦‚è¯´ï¼Œå®ƒå¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼å·¥ä½œï¼Œæˆ‘å¯ä»¥åƒä»¥å‰ä¸€æ ·è¿›è¡Œæ‹Ÿåˆï¼Œå› ä¸ºè¿™ä¸ªæ–°çš„æ¨¡å‹ä¹Ÿå¯ä»¥æ‹Ÿåˆï¼Œæ‰€ä»¥æˆ‘å¯ä»¥è¿™æ ·åšï¼Œæˆ‘ä¹Ÿå¯ä»¥åƒä»¥å‰ä¸€æ ·å¯¹å®ƒè¿›è¡Œè¯„åˆ†ï¼Œç°åœ¨è®©æˆ‘æ¥è¯„åˆ†ï¼Œæˆ‘å¾—åˆ°çš„ç»“æœéå¸¸ç›¸ä¼¼ï¼Œç„¶åå¯èƒ½æœ‰è¶£çš„æ˜¯ï¼Œå½“æˆ‘å®é™…è¿™æ ·åšæ—¶ï¼Œå½“æˆ‘å®é™…å°è¯•è·å–è¿™ä¸ªæ¡å½¢å›¾æ—¶ï¼Œå®ƒåº”è¯¥æ˜¾ç¤ºå®ƒé‡æ–°å‘æŒ¥ä½œç”¨ï¼Œå°½ç®¡éæ ‡å‡†åŒ–çš„ç‰ˆæœ¬ç°åœ¨å¿½ç•¥äº†x2ã€‚
- en: this version should show it so remember this there's gonna be a small error
    here and the problem is that pipeline doesn't have coefficients this pipeline
    as a whole doesn't have coefficients but the logistic regression inside of it
    does have coefficientsã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç‰ˆæœ¬åº”è¯¥ä¼šæ˜¾ç¤ºå‡ºæ¥ï¼Œæ‰€ä»¥è®°ä½è¿™é‡Œä¼šæœ‰ä¸€ä¸ªå°é”™è¯¯ï¼Œé—®é¢˜åœ¨äºç®¡é“æ²¡æœ‰ç³»æ•°ï¼Œæ•´ä¸ªç®¡é“æ²¡æœ‰ç³»æ•°ï¼Œä½†é‡Œé¢çš„é€»è¾‘å›å½’æ˜¯æœ‰ç³»æ•°çš„ã€‚
- en: ğŸ˜Šï¼ŒAnd so how can I get to that it turns out that any pipeline works like a dictionary
    and I can for example I can copy these names and use that like a key and so that
    would get me my standard scalar from the beginning or I could pass in this key
    and that would get me a logistic my logistic regression stage of it and so from
    that then I could actually see well what are the coefficients involved there and
    I would I would paste this here instead of what I originally had and so now I
    can see that when I have the standardization in play as a transformer before my
    estimator it'll automatically do that and then it'll do the right things as well
    if I do my fitting here it'll calculate that mean and a standard deviation and
    I do scoring it's just going to use the same mean in standard deviation from before
    it would not look at that for the task because that would be kind of a methodological
    mistake so we're going to be generally doing whenever we have a logistic regression
    unless we have some very special scenarioã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œé‚£ä¹ˆæˆ‘æ€ä¹ˆèƒ½å‘ç°ä»»ä½•ç®¡é“éƒ½åƒå­—å…¸ä¸€æ ·å‘¢ï¼Ÿæˆ‘å¯ä»¥å¤åˆ¶è¿™äº›åç§°å¹¶ç”¨ä½œé”®ï¼Œè¿™æ ·å°±èƒ½ä»ä¸€å¼€å§‹è·å¾—æˆ‘çš„æ ‡å‡†åŒ–å™¨ï¼Œæˆ–è€…æˆ‘å¯ä»¥ä¼ å…¥è¿™ä¸ªé”®ï¼Œè¿™æ ·å°±èƒ½å¾—åˆ°æˆ‘çš„é€»è¾‘å›å½’é˜¶æ®µï¼Œå› æ­¤æˆ‘å¯ä»¥çœ‹åˆ°æ¶‰åŠçš„ç³»æ•°ã€‚æˆ‘ä¼šæŠŠè¿™ä¸ªç²˜è´´åˆ°è¿™é‡Œï¼Œè€Œä¸æ˜¯æˆ‘åŸæœ¬çš„å†…å®¹ï¼Œè¿™æ ·æˆ‘å°±èƒ½çœ‹åˆ°åœ¨æ ‡å‡†åŒ–ä½œä¸ºå˜æ¢å™¨çš„æƒ…å†µä¸‹ï¼Œå®ƒä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œç„¶åå¦‚æœæˆ‘åœ¨è¿™é‡Œè¿›è¡Œæ‹Ÿåˆï¼Œå®ƒå°†è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè€Œåœ¨è¯„åˆ†æ—¶ï¼Œå®ƒåªä¼šä½¿ç”¨ä¹‹å‰çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè€Œä¸ä¼šè€ƒè™‘ä»»åŠ¡ï¼Œå› ä¸ºé‚£ä¼šæ˜¯ä¸€ä¸ªæ–¹æ³•è®ºé”™è¯¯ã€‚å› æ­¤ï¼Œåªè¦æ²¡æœ‰éå¸¸ç‰¹æ®Šçš„æƒ…å†µï¼Œæˆ‘ä»¬é€šå¸¸ä¼šè¿™æ ·åšï¼Œå°¤å…¶æ˜¯åœ¨é€»è¾‘å›å½’ä¸­ã€‚
- en: For exampleï¼Œ the data has already been standardizedã€‚![](img/ba488996234f1aee5936aea9ea40f4bf_4.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ•°æ®å·²ç»è¢«æ ‡å‡†åŒ–ã€‚![](img/ba488996234f1aee5936aea9ea40f4bf_4.png)
