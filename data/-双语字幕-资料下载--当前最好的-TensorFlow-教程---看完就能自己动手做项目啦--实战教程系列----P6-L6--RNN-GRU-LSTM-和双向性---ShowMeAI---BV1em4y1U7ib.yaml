- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘â€œå½“å‰æœ€å¥½çš„ TensorFlow æ•™ç¨‹ï¼â€ï¼Œçœ‹å®Œå°±èƒ½è‡ªå·±åŠ¨æ‰‹åšé¡¹ç›®å•¦ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- RNNã€GRUã€LSTM
    å’ŒåŒå‘æ€§ - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘â€œå½“å‰æœ€å¥½çš„ TensorFlow æ•™ç¨‹ï¼â€ï¼Œçœ‹å®Œå°±èƒ½è‡ªå·±åŠ¨æ‰‹åšé¡¹ç›®å•¦ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- RNNã€GRUã€LSTM
    å’ŒåŒå‘æ€§ - ShowMeAI - BV1em4y1U7ib
- en: What is going on guys hope you're doing awesomeï¼Œ roll that intro and then let's
    do some RNN and shitã€‚![](img/7a25be1c8cdf0d1403f6e221e59c1458_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œå¸Œæœ›ä½ ä»¬è¿‡å¾—ä¸é”™ï¼Œæ”¾ä¸Šå¼•è¨€ï¼Œç„¶åæˆ‘ä»¬æ¥èŠèŠ RNN ç­‰ç­‰ã€‚![](img/7a25be1c8cdf0d1403f6e221e59c1458_1.png)
- en: '![](img/7a25be1c8cdf0d1403f6e221e59c1458_2.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a25be1c8cdf0d1403f6e221e59c1458_2.png)'
- en: '![](img/7a25be1c8cdf0d1403f6e221e59c1458_3.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a25be1c8cdf0d1403f6e221e59c1458_3.png)'
- en: Alrightï¼Œ so I got the usual imports that we normally haveã€‚ This is for ignoring
    the Tensorflow messages that can be quite annoyingã€‚ although we'll still get error
    messages and then Tensorflowlow Kas layers to construct our layers and then the
    emminis data set and then this is just so that the if you have any trouble running
    on a GPU these two lines will most likely help youã€‚ Allrightï¼Œ so let's let's start
    with what we actually want to doã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæˆ‘æœ‰æˆ‘ä»¬é€šå¸¸éœ€è¦çš„å¯¼å…¥ã€‚è¿™æ˜¯ä¸ºäº†å¿½ç•¥å¯èƒ½ç›¸å½“çƒ¦äººçš„ Tensorflow æ¶ˆæ¯ã€‚å°½ç®¡æˆ‘ä»¬ä»ç„¶ä¼šæ”¶åˆ°é”™è¯¯æ¶ˆæ¯ï¼Œç„¶åä½¿ç”¨ Tensorflow çš„å±‚æ„å»ºæˆ‘ä»¬çš„å±‚ï¼Œè¿˜æœ‰
    emminis æ•°æ®é›†ã€‚è¿™åªæ˜¯ä¸ºäº†å¦‚æœä½ åœ¨ GPU ä¸Šè¿è¡Œæ—¶é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¿™ä¸¤è¡Œé€šå¸¸ä¼šå¸®åŠ©ä½ ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æ¥å¼€å§‹æˆ‘ä»¬å®é™…æƒ³åšçš„äº‹æƒ…ã€‚
- en: we're going to start with loading our data so we're going to do Xtrainã€‚Y train
    and then X testã€‚ Y testã€‚Is equal to Ms dot load dataï¼Œ then we're going to do xtrain
    equals xtrain as type float 32ã€‚ Currently it's float 64 just to save on own computationï¼Œ
    we're going to convert itã€‚ and then we're going to normalize by dividing by 255ï¼Œ
    So it's in between 0 and 1ã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹åŠ è½½æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬å°†åš Xtrainï¼ŒY trainï¼Œç„¶å X testï¼ŒY testã€‚ç­‰äº Ms.dot.load dataï¼Œç„¶åæˆ‘ä»¬å°† Xtrain
    è®¾ç½®ä¸º float 32 ç±»å‹ã€‚å½“å‰æ˜¯ float 64ï¼Œä¸ºäº†èŠ‚çœè®¡ç®—ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ã€‚ç„¶åæˆ‘ä»¬å°†é€šè¿‡é™¤ä»¥ 255 æ¥å½’ä¸€åŒ–ï¼Œä½¿å…¶ä½äº 0 åˆ° 1 ä¹‹é—´ã€‚
- en: And let's do the same for the the test setã€‚ So flow 32ï¼Œ divided by 255ã€‚ So what
    we're doing here is that we haveã€‚We have an image of 28 by 28 pixels and how this
    is going to work when we're going to send it in to an RN or a GRU or an LSTMã€‚
    we're going to do all three of thoseï¼Œ but we're essentially going to view for
    each time step you're going to sort of unroll one row of the image at a time So
    for a particular time step let's say the first time step It's going to take the
    first row of the image and send that in and then for the second time step it's
    going to take the second row and send that in and just to be clear you wouldn't
    use sequence models to handle imagesã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæµ‹è¯•é›†ä¹ŸåšåŒæ ·çš„å¤„ç†ã€‚æ‰€ä»¥æµ 32ï¼Œé™¤ä»¥ 255ã€‚é‚£ä¹ˆæˆ‘ä»¬è¿™é‡Œåšçš„æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬æœ‰ä¸€å¼  28x28 åƒç´ çš„å›¾åƒï¼Œå½“æˆ‘ä»¬æŠŠå®ƒé€å…¥ RNNã€GRU æˆ–
    LSTM æ—¶ï¼Œä¼šæ˜¯æ€æ ·çš„å‘¢ï¼Ÿæˆ‘ä»¬ä¼šç”¨è¿™ä¸‰ç§æ¨¡å‹ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬ä¼šé’ˆå¯¹æ¯ä¸ªæ—¶é—´æ­¥å±•å¼€å›¾åƒçš„ä¸€è¡Œã€‚æ¯”å¦‚å¯¹äºç‰¹å®šçš„æ—¶é—´æ­¥ï¼Œå‡è®¾ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒå°†å–å›¾åƒçš„ç¬¬ä¸€è¡Œå¹¶å‘é€ï¼Œç„¶åå¯¹äºç¬¬äºŒä¸ªæ—¶é—´æ­¥ï¼Œå®ƒå°†å–ç¬¬äºŒè¡Œå¹¶å‘é€ã€‚ä¸ºäº†æ¾„æ¸…ï¼Œä½ ä¸ä¼šä½¿ç”¨åºåˆ—æ¨¡å‹æ¥å¤„ç†å›¾åƒã€‚
- en: It's not the best model for itï¼Œ you would use a commnet that we covered two
    videos ago butã€‚It works to use RNNã€‚ and as we'll seeï¼Œ will'll get reasonable performanceã€‚
    although this is more to illustrate how you would actuallyã€‚Implement an R N and
    a GR U and LSTM in Tensorflowã€‚ and the data set is not the optimal oneã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨ä¸¤æ®µè§†é¢‘å‰è®¨è®ºçš„ CNNï¼Œä½†ä½¿ç”¨ RNN ä¹Ÿèƒ½å¥æ•ˆã€‚æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œå°½ç®¡æ•°æ®é›†å¹¶ä¸ç†æƒ³ï¼Œä½†æˆ‘ä»¬å°†è·å¾—åˆç†çš„æ€§èƒ½ã€‚è¿™æ›´å¤šæ˜¯ä¸ºäº†è¯´æ˜å¦‚ä½•åœ¨
    Tensorflow ä¸­å®é™…å®ç° RNNã€GRU å’Œ LSTMï¼Œè€Œæ•°æ®é›†å¹¶ä¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚
- en: but we're just picking in a simple one to illustrate this exampleã€‚ Al rightï¼Œ
    so with that saidã€‚ let's actually do our modelï¼Œ we're going to do kos dot sequentialã€‚We're
    going to do model that add and then specify the inputã€‚ in this caseã€‚ we're going
    to specify none and then 28ã€‚So what we're specifying none here is because we dont
    have to have a specific number of time step right so we have 28 pixels in each
    time step and then in this case we actually have 28 time step but we don't have
    to specify that dimensionã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡æˆ‘ä»¬åªæ˜¯é€‰æ‹©ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜ã€‚å¥½çš„ï¼Œè¯ä¸å¤šè¯´ï¼Œå®é™…ä¸Šæˆ‘ä»¬è¦åšæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ keras çš„ sequentialã€‚æˆ‘ä»¬ä¼šæ·»åŠ æ¨¡å‹ï¼Œç„¶åæŒ‡å®šè¾“å…¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æŒ‡å®š
    none å’Œ 28ã€‚è¿™é‡ŒæŒ‡å®š none æ˜¯å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ä¸€ä¸ªç‰¹å®šçš„æ—¶é—´æ­¥æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¯ä¸ªæ—¶é—´æ­¥æœ‰ 28 ä¸ªåƒç´ ï¼Œè€Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šæœ‰ 28 ä¸ªæ—¶é—´æ­¥ï¼Œä½†ä¸éœ€è¦æŒ‡å®šè¯¥ç»´åº¦ã€‚
- en: And then we're going to do modelã€‚ add and then layers simple Rnã€‚ So that's just
    for a basic Rnã€‚ and then let's say 512 nodesã€‚And then as an additional argument
    we can do return sequences equals true so that it's returning the output from
    each time step and in that way we can stack multiple RnN layers on top of each
    other so the output from this RnN is going to be 512 nodes and then return sequences
    it's going to output 512 for each time step in this case we're going to have 28
    time steps and then we can also do activation we can set it to Reluã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ã€‚æ·»åŠ ç®€å•çš„ Rn å±‚ã€‚æ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªåŸºç¡€çš„ Rnã€‚ç„¶åå‡è®¾æœ‰ 512 ä¸ªèŠ‚ç‚¹ã€‚ä½œä¸ºé™„åŠ å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½® return sequences
    ä¸º trueï¼Œè¿™æ ·å®ƒå°†è¿”å›æ¯ä¸ªæ—¶é—´æ­¥éª¤çš„è¾“å‡ºï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥åœ¨å¤šä¸ª RnN å±‚ä¹‹é—´å åŠ ï¼Œæ‰€ä»¥è¿™ä¸ª RnN çš„è¾“å‡ºå°†æ˜¯ 512 ä¸ªèŠ‚ç‚¹ï¼Œç„¶åè¿”å›åºåˆ—åœ¨è¿™ç§æƒ…å†µä¸‹ä¼šè¾“å‡ºæ¯ä¸ªæ—¶é—´æ­¥éª¤çš„
    512ï¼Œå› ä¸ºæˆ‘ä»¬å°†æœ‰ 28 ä¸ªæ—¶é—´æ­¥éª¤ï¼Œç„¶åæˆ‘ä»¬ä¹Ÿå¯ä»¥è¿›è¡Œæ¿€æ´»ï¼Œå°†å…¶è®¾ç½®ä¸º Reluã€‚
- en: And then we can add another oneï¼Œ we can demand let add layers simple R nï¼Œ and
    let's do 512 againã€‚ and we're going to set activation equals Reluã€‚And then for
    the output layer we're going to do model add layersã€‚ dense and we're going to
    have 10 output nodesï¼Œ so you would notice here that we're not doing return sequences
    on this second simple Rnn so here for the output we don't have return sequences
    equals trueã€‚ meaning that it's going to pass every time step and then at the last
    the last output of this simple Rnn hereã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å†æ·»åŠ ä¸€ä¸ªï¼Œå¯ä»¥è¦æ±‚æ·»åŠ ç®€å•çš„ Rn å±‚ï¼Œæˆ‘ä»¬å†åšä¸€æ¬¡ 512ã€‚æˆ‘ä»¬å°†è®¾ç½®æ¿€æ´»ä¸º Reluã€‚å¯¹äºè¾“å‡ºå±‚ï¼Œæˆ‘ä»¬å°†æ·»åŠ æ¨¡å‹å±‚ã€‚denseï¼Œæˆ‘ä»¬å°†æœ‰
    10 ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œå› æ­¤ä½ ä¼šæ³¨æ„åˆ°åœ¨è¿™ä¸ªç¬¬äºŒä¸ªç®€å•çš„ RnN ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰è®¾ç½® return sequencesã€‚å› æ­¤å¯¹äºè¾“å‡ºæˆ‘ä»¬æ²¡æœ‰ return sequences
    ç­‰äº trueã€‚æ„å‘³ç€å®ƒå°†ä¼ é€’æ¯ä¸ªæ—¶é—´æ­¥éª¤ï¼Œç„¶ååœ¨è¿™ä¸ªç®€å•çš„ RnN çš„æœ€åè¾“å‡ºã€‚
- en: we're going to take a layer dense on top of that one and we're going to have
    10 output nodesã€‚Let's do print model summary first so as we can see here on the
    model summary for this first Rnn we're gonna to have none none and then 512 so
    we're going have 512 output node and then we're going have for each time step
    here and the reason that we have none and none is that we have the first one for
    the batches or one of these are for the batches and one of them are for the hidden
    states I think this one is for the batches this one is for the hidden states and
    then at the second one we're not having return sequences equals true so we only
    have none for the batches and then we have 512 nodes from sort of the last hidden
    state when it's past all of the input and then at the end we just have a layer
    on top of that one So all that's left now is for us to do model do compile and
    we're going to specify a loss function and ks do losses sparse categoryã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ä¸Šé¢æ·»åŠ ä¸€ä¸ª dense å±‚ï¼Œå¹¶å°†æœ‰ 10 ä¸ªè¾“å‡ºèŠ‚ç‚¹ã€‚é¦–å…ˆåšæ‰“å°æ¨¡å‹æ‘˜è¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„æ¨¡å‹æ‘˜è¦ï¼Œå¯¹äºç¬¬ä¸€ä¸ª RnNï¼Œæˆ‘ä»¬å°†æœ‰ none
    noneï¼Œç„¶åæ˜¯ 512ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æœ‰ 512 ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éª¤éƒ½æœ‰ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥æœ‰ none å’Œ noneï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬æœ‰ä¸€ä¸ªæ˜¯é’ˆå¯¹æ‰¹æ¬¡çš„ï¼Œå¦ä¸€ä¸ªæ˜¯é’ˆå¯¹éšè—çŠ¶æ€çš„ã€‚æˆ‘è®¤ä¸ºè¿™ä¸ªæ˜¯é’ˆå¯¹æ‰¹æ¬¡çš„ï¼Œå¦ä¸€ä¸ªæ˜¯é’ˆå¯¹éšè—çŠ¶æ€çš„ã€‚ç„¶ååœ¨ç¬¬äºŒä¸ªå±‚ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰è®¾ç½®
    return sequences ä¸º trueï¼Œå› æ­¤æˆ‘ä»¬åªæœ‰ none å¯¹äºæ‰¹æ¬¡ï¼Œç„¶åä»æœ€åçš„éšè—çŠ¶æ€å¾—åˆ° 512 ä¸ªèŠ‚ç‚¹ï¼Œå½“å®ƒå¤„ç†æ‰€æœ‰è¾“å…¥æ—¶ã€‚æœ€åæˆ‘ä»¬åªéœ€è¦åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ ä¸€å±‚ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬è¦åšçš„å°±æ˜¯æ¨¡å‹ç¼–è¯‘ï¼Œå¹¶æŒ‡å®šæŸå¤±å‡½æ•°ï¼Œä½¿ç”¨
    ks çš„æŸå¤± sparse categoryã€‚
- en: croros entropy then we're going to set from logics equals true because we do
    not have a softmax on our dense layer at the end and then optimizerã€‚ we're going
    to Kas do optimizers do atomã€‚And let's set the learning rate to 0ã€‚001ã€‚And then
    metricsã€‚ we're just going keep track of the accuracyã€‚ All that's left now is for
    us to do model that's fit on the training setã€‚And then specifying the batch sizeï¼Œ
    let's say 64ã€‚ and then let's run for 10posã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: croros entropyï¼Œç„¶åæˆ‘ä»¬å°†è®¾ç½® from logics ä¸º trueï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æœ€åçš„ dense å±‚æ²¡æœ‰ softmaxã€‚ç„¶åä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
    ks çš„ä¼˜åŒ–å™¨ï¼Œè®¾ç½®ä¸º atomã€‚æˆ‘ä»¬å°†å­¦ä¹ ç‡è®¾ç½®ä¸º 0.001ã€‚ç„¶åæŒ‡æ ‡ï¼Œæˆ‘ä»¬åªéœ€è·Ÿè¸ªå‡†ç¡®æ€§ã€‚ç°åœ¨æˆ‘ä»¬è¦åšçš„å°±æ˜¯åœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨æ¨¡å‹è¿›è¡Œæ‹Ÿåˆã€‚ç„¶åæŒ‡å®šæ‰¹é‡å¤§å°ï¼Œå‡è®¾ä¸º
    64ã€‚ç„¶åè®©æˆ‘ä»¬è¿è¡Œ 10 æ¬¡ã€‚
- en: And verbo equals2 just for printing every epochã€‚ And then at the endã€‚ we want
    to do an evaluation on our test setã€‚ So we're going to send an X test and then
    the labels Y testã€‚We're also going to specify the batch size 64ï¼Œ and then again
    verboos equalsã€‚Equals 2ã€‚Alrightã€‚ so let's run that and see if this worksã€‚Alrightï¼Œ
    so after 10 epos we got 98% on on the training set and almost 98% on the test
    set as wellã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œ verbo ç­‰äº 2 åªæ˜¯ä¸ºäº†æ¯ä¸ªå‘¨æœŸæ‰“å°ã€‚ç„¶ååœ¨æœ€åï¼Œæˆ‘ä»¬æƒ³å¯¹æˆ‘ä»¬çš„æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å‘é€ X æµ‹è¯•å’Œæ ‡ç­¾ Y æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜å°†æŒ‡å®šæ‰¹é‡å¤§å°ä¸º
    64ï¼Œç„¶åå†æ¬¡è®¾ç½® verboos ç­‰äº 2ã€‚å¥½å§ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥è¿è¡Œä¸€ä¸‹ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½æˆåŠŸã€‚å¥½å§ï¼Œåœ¨ 10 ä¸ªå‘¨æœŸåï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šå¾—åˆ°äº† 98%ï¼Œåœ¨æµ‹è¯•é›†ä¸Šä¹Ÿæ¥è¿‘
    98%ã€‚
- en: I just want to say here that we used an activation RELã€‚ the default when training
    recurrent networks is that you use 10hã€‚ so I don't know if that would work better
    in this caseï¼Œ but anywaysã€‚Just wanted to mention thatã€‚ So if you wouldn't specify
    an activation functionã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³åœ¨è¿™é‡Œæåˆ°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ReLUæ¿€æ´»å‡½æ•°ã€‚è®­ç»ƒé€’å½’ç½‘ç»œçš„é»˜è®¤è®¾ç½®æ˜¯ä½¿ç”¨tanhã€‚æˆ‘ä¸çŸ¥é“åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯å¦ä¼šæ›´å¥½ï¼Œä½†æ— è®ºå¦‚ä½•ï¼Œæˆ‘åªæ˜¯æƒ³æä¸€ä¸‹ã€‚å¦‚æœä½ ä¸æŒ‡å®šæ¿€æ´»å‡½æ•°ã€‚
- en: it would default B 10h when when building these recurrent netsã€‚ So also one
    thing here is that it took a little bit longer than I thought to run this so let's
    just use 256 for our next models what we want to do now is pretty much the same
    thing but we want to build a GRU instead and all we got to do to do that is we
    just got us change this this simple Rn to a GRU and that's pretty much that's
    all you have to do so if we now rerun that we can see what it gets and I guess
    this is not really a fair comparison but GRU should perform better than simple
    Rnns although now we're using half of the units and then we're using 10h instead
    of RElu but the point is not really to compare the to just see that it works and
    and show you how to use simple Rnn GRU and thenã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºè¿™äº›é€’å½’ç½‘ç»œæ—¶ï¼Œé»˜è®¤æ˜¯ä½¿ç”¨tanhã€‚å› æ­¤ï¼Œè¿˜æœ‰ä¸€ç‚¹æ˜¯ï¼Œè¿è¡Œè¿™ä¸ªè¿‡ç¨‹æ¯”æˆ‘æƒ³è±¡çš„è¦é•¿ä¸€äº›ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ä¸‹ä¸€ä¸ªæ¨¡å‹ä¸­ä½¿ç”¨256ä¸ªå•å…ƒã€‚æˆ‘ä»¬ç°åœ¨æƒ³åšçš„åŸºæœ¬ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œä½†æˆ‘ä»¬æƒ³æ„å»ºä¸€ä¸ªGRUã€‚æˆ‘ä»¬è¦åšçš„å°±æ˜¯å°†è¿™ä¸ªç®€å•çš„RNNæ›´æ”¹ä¸ºGRUï¼Œè¿™å‡ ä¹å°±æ˜¯ä½ éœ€è¦åšçš„å…¨éƒ¨ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ç°åœ¨é‡æ–°è¿è¡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹ç»“æœã€‚æˆ‘æƒ³è¿™ä¸æ˜¯ä¸€ä¸ªå…¬å¹³çš„æ¯”è¾ƒï¼Œä½†GRUçš„æ€§èƒ½åº”è¯¥ä¼˜äºç®€å•çš„RNNï¼Œå°½ç®¡æˆ‘ä»¬ç°åœ¨ä½¿ç”¨äº†ä¸€åŠçš„å•å…ƒï¼Œå¹¶ä¸”ä½¿ç”¨tanhè€Œä¸æ˜¯ReLUï¼Œä½†é‡ç‚¹å¹¶ä¸æ˜¯æ¯”è¾ƒï¼Œè€Œæ˜¯å±•ç¤ºå®ƒçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ç®€å•çš„RNNå’ŒGRUã€‚
- en: LSTMï¼Œ which is quite simple as wellï¼Œ we're just going to change this to an LSTMï¼Œ
    but anywaysã€‚ then I also want to show you how to do a bidirectional layerã€‚Alrightï¼Œ
    so after 10pos we had 99ã€‚5% on the training and we get close to 99% on the test
    setã€‚ which is quite good actually it's two layered GRus with 256 units I mean
    to get that is pretty decent actually so let's now change this to an LSTM and
    see if it's an improvement soã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMç›¸å¯¹ç®€å•ï¼Œæˆ‘ä»¬åªéœ€å°†å…¶æ›´æ”¹ä¸ºLSTMï¼Œä½†æ— è®ºå¦‚ä½•ã€‚æˆ‘è¿˜æƒ³å±•ç¤ºå¦‚ä½•å®ç°åŒå‘å±‚ã€‚å¥½çš„ï¼Œåœ¨10ä¸ªepochåï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šè¾¾åˆ°äº†99.5%ï¼Œåœ¨æµ‹è¯•é›†ä¸Šæ¥è¿‘99%ã€‚è¿™å®é™…ä¸Šç›¸å½“ä¸é”™ï¼Œä¸¤ä¸ªå±‚çš„GRUï¼Œ256ä¸ªå•å…ƒï¼Œèƒ½è¾¾åˆ°è¿™ä¸ªæ°´å¹³ç¡®å®ä¸é”™ï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸ºLSTMï¼Œçœ‹çœ‹æ˜¯å¦æœ‰æ‰€æ”¹å–„ã€‚
- en: LSTMs and GRUs are equivalent in terms of performanceã€‚ I think LSTMs are a little
    bit better than GRUsï¼Œ but let's see if that's the case on this data setã€‚Alrightï¼Œ
    so it seems that we get pretty much the identical performanceã€‚ LSTMs were a little
    bit betterï¼Œ perhaps on the test headï¼Œ but pretty much the sameã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMå’ŒGRUåœ¨æ€§èƒ½ä¸Šæ˜¯ç­‰æ•ˆçš„ã€‚æˆ‘è®¤ä¸ºLSTMç¨å¾®ä¼˜äºGRUï¼Œä½†è®©æˆ‘ä»¬çœ‹çœ‹åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šæ˜¯å¦å¦‚æ­¤ã€‚å¥½çš„ï¼Œä¼¼ä¹æˆ‘ä»¬çš„æ€§èƒ½å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚LSTMç¨å¾®å¥½ä¸€äº›ï¼Œä¹Ÿè®¸åœ¨æµ‹è¯•é›†ä¸Šï¼Œä½†å¤§ä½“ä¸Šæ˜¯ä¸€æ ·çš„ã€‚
- en: So what we want to do now is we want to add instead of using just a one directionional
    LSTMã€‚ We want to use a bidirectional and it's pretty easy to add that as wellã€‚
    we're just going do layers do bidirectional and let's do it like thisã€‚ and then
    like thisã€‚ So we're just gonna add layers bidirectional and then we're gonna send
    in that LSTM layerã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ç°åœ¨æƒ³åšçš„æ˜¯ï¼Œä¸ä»…ä½¿ç”¨å•å‘LSTMï¼Œè€Œæ˜¯ä½¿ç”¨åŒå‘LSTMï¼Œæ·»åŠ å®ƒéå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€æ·»åŠ åŒå‘å±‚ï¼Œå°±åƒè¿™æ ·ã€‚ç„¶ååƒè¿™æ ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ·»åŠ åŒå‘å±‚ï¼Œç„¶åå‘é€LSTMå±‚ã€‚
- en: And let's do that for the second oneã€‚OrRatherï¼Œ let's doã€‚Alrightã€‚ so let's let's
    first do Mo print a model that summary and just see how it looks likeã€‚Alrightã€‚
    so I'm not going to let this trainã€‚ So what we get here is as you can see since
    we added this layer bidirectionalã€‚ we're going to get 512 nodes instead of this
    256ã€‚ So what it's doing here is we're specifying the number of nodes for each
    hidden for each of its computation for each hidden state in the LSTM to be 256
    nodesã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸ºç¬¬äºŒä¸ªèŠ‚ç‚¹è¿™æ ·åšã€‚æˆ–è€…è¯´ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚é‚£ä¹ˆæˆ‘ä»¬å…ˆæ‰“å°ä¸€ä¸ªæ¨¡å‹æ€»ç»“ï¼Œçœ‹çœ‹å®ƒçš„æ ·å­ã€‚å¥½çš„ï¼Œæˆ‘ä¸æ‰“ç®—è®©å®ƒè®­ç»ƒã€‚æ‰€ä»¥ä»è¿™é‡Œå¯ä»¥çœ‹å‡ºï¼Œç”±äºæˆ‘ä»¬æ·»åŠ äº†è¿™ä¸ªåŒå‘å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°512ä¸ªèŠ‚ç‚¹ï¼Œè€Œä¸æ˜¯256ä¸ªã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨LSTMä¸­ä¸ºæ¯ä¸ªéšè—çŠ¶æ€çš„è®¡ç®—æŒ‡å®šçš„èŠ‚ç‚¹æ•°é‡æ˜¯256ã€‚
- en: but since we add this bidirectional we're going have one going forward and one
    going backwards So this is going get doubled in the amount of nodes as we see
    hereã€‚ So what we can do then is for the second one we can do also a layers that
    bidirectional and thenã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œç”±äºæˆ‘ä»¬æ·»åŠ äº†åŒå‘ç»“æ„ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ä¸ªå‘å‰å’Œä¸€ä¸ªå‘åçš„èŠ‚ç‚¹ã€‚æ‰€ä»¥è¿™é‡Œçš„èŠ‚ç‚¹æ•°é‡ä¼šåŠ å€ã€‚å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ç¬¬äºŒä¸ªèŠ‚ç‚¹ä¹Ÿè¿›è¡ŒåŒå‘å±‚çš„æ“ä½œã€‚
- en: We can add thatã€‚ğŸ˜”ï¼ŒRight there and that's also going to have 512 nodesã€‚ So let's
    run that and see if the bidirectional is any better than the one direction just
    having one direction on the LSTMã€‚After 10 eposï¼Œ we see that the performance is
    about the same as just using one direction so the bidirectional didn't really
    help andã€‚And I'm not really sure why that is maybe it just needs more training
    or it just doesn't help that muchã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ·»åŠ è¿™ä¸ªã€‚ğŸ˜”ï¼Œå°±åœ¨è¿™é‡Œï¼Œè¿™ä¹Ÿå°†æœ‰512ä¸ªèŠ‚ç‚¹ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥è¿è¡Œä¸€ä¸‹ï¼Œçœ‹çœ‹åŒå‘æ˜¯å¦æ¯”å•å‘çš„LSTMå¥½ã€‚ç»è¿‡10ä¸ªepochï¼Œæˆ‘ä»¬çœ‹åˆ°æ€§èƒ½ä¸å•å‘ä½¿ç”¨åŸºæœ¬ç›¸åŒï¼Œæ‰€ä»¥åŒå‘å¹¶æ²¡æœ‰çœŸæ­£èµ·åˆ°å¸®åŠ©ä½œç”¨ã€‚æˆ‘ä¹Ÿä¸å¤ªç¡®å®šä¸ºä»€ä¹ˆï¼Œå¯èƒ½åªæ˜¯éœ€è¦æ›´å¤šçš„è®­ç»ƒï¼Œæˆ–è€…å®ƒå¹¶æ²¡æœ‰é‚£ä¹ˆå¤§çš„å¸®åŠ©ã€‚
- en: For this particular datasetï¼Œ but in general using bidirectional as more of a
    default is a good optionã€‚ but anyways that was the basics of how to do a simple
    RN a GRU LTM and then also how to add bidirectality and in this scenario we use
    the MNIS dataset so we made it very easy for ourselves when using the Ms data
    and when training on more complex data you need to think about more things such
    as padding the data and masking the data for each batch and we're going to cover
    that in future videos when we're going to load more complex data and loading more
    custom dataã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç‰¹å®šçš„æ•°æ®é›†ï¼Œä½†é€šå¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŒå‘ä½œä¸ºé»˜è®¤è®¾ç½®æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚ä¸è¿‡æ— è®ºå¦‚ä½•ï¼Œè¿™å°±æ˜¯å¦‚ä½•è¿›è¡Œç®€å•çš„RNã€GRUå’ŒLSTMçš„åŸºç¡€çŸ¥è¯†ï¼Œä»¥åŠå¦‚ä½•æ·»åŠ åŒå‘æ€§ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†MNISæ•°æ®é›†ï¼Œæ‰€ä»¥ä½¿ç”¨Msæ•°æ®æ—¶æˆ‘ä»¬ç»™è‡ªå·±æä¾›äº†å¾ˆå¤§çš„ä¾¿åˆ©ã€‚åœ¨è®­ç»ƒæ›´å¤æ‚çš„æ•°æ®æ—¶ï¼Œä½ éœ€è¦è€ƒè™‘æ›´å¤šçš„äº‹æƒ…ï¼Œæ¯”å¦‚ä¸ºæ¯ä¸ªæ‰¹æ¬¡å¡«å……å’Œå±è”½æ•°æ®ï¼Œæˆ‘ä»¬å°†åœ¨æœªæ¥çš„è§†é¢‘ä¸­è®¨è®ºï¼Œå½“æˆ‘ä»¬åŠ è½½æ›´å¤æ‚å’Œè‡ªå®šä¹‰çš„æ•°æ®æ—¶ã€‚
- en: So with that said thank you for watching and I hope to see you in the next videoã€‚![](img/7a25be1c8cdf0d1403f6e221e59c1458_5.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢è§‚çœ‹ï¼Œå¸Œæœ›åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘è§åˆ°ä½ ï¼![](img/7a25be1c8cdf0d1403f6e221e59c1458_5.png)
