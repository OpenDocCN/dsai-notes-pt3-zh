- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P55ï¼šL10.4- ä½¿ç”¨Keraså’ŒTensorFlowè¿›è¡Œå›¾åƒæè¿°ç”Ÿæˆ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P55ï¼šL10.4- ä½¿ç”¨Keraså’ŒTensorFlowè¿›è¡Œå›¾åƒæè¿°ç”Ÿæˆ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ Welcome to applications of Deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ we're going to look at an interesting combination
    of convolution neural networks in LSTM that allows us to caption imagesã€‚ to see
    several things going on in the image and describe it in a sentence for the latest
    on my AI course and projectsã€‚ Click subscribe in the bell next to it to be notified
    of every new videoã€‚ imageage captioningã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œè¿™æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦æ·±åº¦ç¥ç»ç½‘ç»œçš„åº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å·ç§¯ç¥ç»ç½‘ç»œä¸LSTMçš„æœ‰è¶£ç»“åˆï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºå›¾åƒæ·»åŠ æè¿°ï¼Œä»¥ä¾¿çœ‹åˆ°å›¾åƒä¸­çš„å¤šä¸ªå…ƒç´ å¹¶ç”¨ä¸€å¥è¯æ¥æè¿°å®ƒã€‚è¦è·å–æˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®ï¼Œè¯·ç‚¹å‡»æ—è¾¹çš„é“ƒé“›è®¢é˜…ï¼Œä»¥ä¾¿æ”¶åˆ°æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚å›¾åƒæè¿°ç”Ÿæˆã€‚
- en: at least where I first saw it was with Andre Carpathy's dissertationã€‚ Nowã€‚ we
    talked about this guy before in the last sectionï¼Œ some of the LSTM text generation
    came from himã€‚ and he did some veryï¼Œ very interesting work with that as wellã€‚
    So a very interesting guy researcher now works for Teslaã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°çš„æ˜¯å®‰å¾·çƒˆÂ·å¡å¸•è¥¿çš„è®ºæ–‡ã€‚æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­æåˆ°è¿‡è¿™ä¸ªäººï¼Œä¸€äº›LSTMæ–‡æœ¬ç”Ÿæˆçš„å†…å®¹æ¥è‡ªäºä»–ã€‚ä»–åœ¨è¿™æ–¹é¢ä¹Ÿåšäº†ä¸€äº›éå¸¸æœ‰è¶£çš„å·¥ä½œã€‚æ‰€ä»¥è¿™ä½ç ”ç©¶è€…éå¸¸æœ‰è¶£ï¼Œç°åœ¨ä¸ºç‰¹æ–¯æ‹‰å·¥ä½œã€‚
- en: who created a lot of the code for some of this computer vision software that
    we're dealing with right nowã€‚ This is an image actually from his dissertationï¼Œ
    or at least from his website showing what the captioning doesã€‚ğŸ˜Šã€‚![](img/2287d406430b06732c757e98a8dcb007_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä¸ºæˆ‘ä»¬ç°åœ¨å¤„ç†çš„ä¸€äº›è®¡ç®—æœºè§†è§‰è½¯ä»¶åˆ›å»ºäº†å¤§é‡ä»£ç ã€‚è¿™å®é™…ä¸Šæ˜¯ä»–è®ºæ–‡ä¸­çš„ä¸€å¹…å›¾åƒï¼Œæˆ–è€…è‡³å°‘æ˜¯ä»–ç½‘ç«™ä¸Šçš„ä¸€å¹…å›¾åƒï¼Œå±•ç¤ºäº†æè¿°ç”Ÿæˆçš„æ•ˆæœã€‚ğŸ˜Šã€‚![](img/2287d406430b06732c757e98a8dcb007_1.png)
- en: Typically before captioningï¼Œ we would try to just classify something as say
    a catã€‚ Wellã€‚ there's also a skateboard in hereã€‚ and there's a few other things
    as well door and they're just partial piecesã€‚ later then we got into multi-imaging
    classification So we'd say cat skateboard kind of like yellow Now we're wanting
    to actually combine that text generation that we have with the image classificationã€‚
    be able to actually write a caption for theseã€‚ Now we're going to use heavy transfer
    learning because this would take forever to train this thing from the ground up
    even with the transfer learningã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸åœ¨ç”Ÿæˆæè¿°ä¹‹å‰ï¼Œæˆ‘ä»¬ä¼šå°è¯•å°†æŸç‰©åˆ†ç±»ä¸ºçŒ«ã€‚ç„¶è€Œï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªæ»‘æ¿ï¼Œä»¥åŠå…¶ä»–ä¸€äº›éƒ¨åˆ†ï¼Œæ¯”å¦‚é—¨ã€‚åæ¥æˆ‘ä»¬è¿›å…¥äº†å¤šå›¾åƒåˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šè¯´çŒ«å’Œæ»‘æ¿ï¼Œå°±åƒæ˜¯é»„è‰²ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦å®é™…å°†æ–‡æœ¬ç”Ÿæˆä¸å›¾åƒåˆ†ç±»ç»“åˆèµ·æ¥ï¼Œèƒ½å¤Ÿä¸ºè¿™äº›å›¾åƒå†™æè¿°ã€‚æˆ‘ä»¬å°†ä½¿ç”¨é‡åº¦è¿ç§»å­¦ä¹ ï¼Œå› ä¸ºä»å¤´å¼€å§‹è®­ç»ƒè¿™ä¸ªä¸œè¥¿ä¼šè€—è´¹å¾ˆé•¿æ—¶é—´ï¼Œå³ä½¿ä½¿ç”¨è¿ç§»å­¦ä¹ ã€‚
- en: we're going to use a relatively small image set and it's not going to be perfectã€‚
    but it will generate captions that have some that have pretty good meaning for
    what they're looking at they won't be perfectã€‚ We'll see that when we use images
    like I have a set of imagesã€‚ the photos directory that's in the Githubã€‚Repository
    for this course that are just personal family photos that I use in machine learningã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„å›¾åƒé›†ï¼Œå®ƒä¸ä¼šå®Œç¾ï¼Œä½†ä¼šç”Ÿæˆä¸€äº›å…·æœ‰ç›¸å½“å¥½æ„ä¹‰çš„æè¿°ã€‚å®ƒä»¬ä¸ä¼šå®Œç¾ã€‚å½“æˆ‘ä»¬ä½¿ç”¨æˆ‘åœ¨è¿™ä¸ªè¯¾ç¨‹çš„GitHubä¸Šå­˜æ”¾çš„ä¸ªäººå®¶åº­ç…§ç‰‡æ—¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: you've seen a number of them in this classï¼Œ some of them are by dog Hickory
    and these are good tests for that because they're completely outside of imagenet
    and they're just things that I have that are pictures that I chose because I thought
    they are interesting from a machine learning aspect that some of them might not
    be as easy for these neural networks to classify Now the two things that we're
    going to transfer from are inception V3 and glove Inception v3 that's another
    one of the many imagenet trained neural networks mobilenet we've used a couple
    of times in this class up till now I do have it set so that you can use mobilenet
    instead of inception but for this one inception works better I'll get into what
    the actual differences when we get down to that part and then glove that is a
    natural language processing embeddingã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åœ¨è¿™ä¸ªè¯¾ç¨‹ä¸­è§è¿‡å¾ˆå¤šè¿™æ ·çš„ä¾‹å­ï¼Œå…¶ä¸­ä¸€äº›æ¥è‡ªç‹—Hickoryï¼Œå®ƒä»¬æ˜¯éå¸¸å¥½çš„æµ‹è¯•ï¼Œå› ä¸ºå®ƒä»¬å®Œå…¨ä¸åœ¨Imagenetä¸­ï¼Œè€Œä¸”éƒ½æ˜¯æˆ‘é€‰æ‹©çš„ç…§ç‰‡ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒä»¬å¾ˆæœ‰è¶£ï¼Œæœ‰äº›å¯èƒ½å¹¶ä¸å®¹æ˜“è¢«è¿™äº›ç¥ç»ç½‘ç»œåˆ†ç±»ã€‚ç°åœ¨æˆ‘ä»¬å°†ä»Inception
    V3å’ŒGloVeè¿›è¡Œè¿ç§»ã€‚Inception V3æ˜¯ä¼—å¤šç»è¿‡Imagenetè®­ç»ƒçš„ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼ŒMobileNetæˆ‘ä»¬åœ¨è¿™ä¸ªè¯¾ç¨‹ä¸­ä¹Ÿç”¨è¿‡å‡ æ¬¡ã€‚ç°åœ¨æˆ‘å·²è®¾ç½®ä¸ºå¯ä»¥ä½¿ç”¨MobileNetè€Œä¸æ˜¯Inceptionï¼Œä½†åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­Inceptionè¡¨ç°æ›´å¥½ã€‚æˆ‘ä¼šåœ¨åˆ°è¾¾é‚£éƒ¨åˆ†æ—¶è¯¦ç»†è¯´æ˜å®é™…çš„åŒºåˆ«ï¼ŒGloVeæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„åµŒå…¥ã€‚
- en: We'll be learning more about those in the next module when we get into natural
    language processingã€‚ This is essentially how this is going to workã€‚ It's actually
    pretty similar to the text generation that we did in the previous chapterã€‚ But
    instead of just generating random sort of nonsense sentencesã€‚ now we're going
    to actually generate us sentencesã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªæ¨¡å—ä¸­å­¦ä¹ æ›´å¤šå…³äºè¿™äº›å†…å®¹çš„çŸ¥è¯†ï¼Œå½“æˆ‘ä»¬è¿›å…¥è‡ªç„¶è¯­è¨€å¤„ç†æ—¶ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯å®ƒçš„å·¥ä½œæ–¹å¼ã€‚å®é™…ä¸Šï¼Œè¿™ä¸æˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­è¿›è¡Œçš„æ–‡æœ¬ç”Ÿæˆéå¸¸ç›¸ä¼¼ã€‚ä½†ç°åœ¨æˆ‘ä»¬ä¸ä»…ä»…æ˜¯ç”Ÿæˆéšæœºçš„æ— æ„ä¹‰å¥å­ï¼Œè€Œæ˜¯å®é™…ä¸Šè¦ç”Ÿæˆæœ‰æ„ä¹‰çš„å¥å­ã€‚
- en: you can see why we started with the previous part and learned how to just teach
    the neural networksã€‚ The basics of grammar and how to string things togetherã€‚
    And reallyã€‚ we're just guiding them to the knowledge of grammarã€‚ We're not actually
    teaching them grammarã€‚ believe meï¼Œ the early days of natural language process
    was obsessed with teaching the neural networks all about or preprocessing based
    on grammar rules and codifying the grammar rulesã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç†è§£æˆ‘ä»¬ä¸ºä»€ä¹ˆä»å‰é¢çš„éƒ¨åˆ†å¼€å§‹ï¼Œå¹¶å­¦ä¹ å¦‚ä½•ä»…æ•™æˆç¥ç»ç½‘ç»œã€‚è¯­æ³•çš„åŸºç¡€ä»¥åŠå¦‚ä½•å°†å†…å®¹ç»„åˆåœ¨ä¸€èµ·ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬åªæ˜¯åœ¨å¼•å¯¼å®ƒä»¬äº†è§£è¯­æ³•çŸ¥è¯†ã€‚æˆ‘ä»¬å¹¶ä¸æ˜¯åœ¨çœŸæ­£æ•™å®ƒä»¬è¯­æ³•ã€‚ç›¸ä¿¡æˆ‘ï¼Œæ—©æœŸçš„è‡ªç„¶è¯­è¨€å¤„ç†ç—´è¿·äºæ•™ç¥ç»ç½‘ç»œæ‰€æœ‰å…³äºè¯­æ³•è§„åˆ™çš„çŸ¥è¯†ï¼Œå¹¶å°†è¯­æ³•è§„åˆ™ç¼–ç åŒ–ã€‚
- en: and believe meï¼Œ thats that's very difficultã€‚ Nowï¼Œ the idea is to use big dataã€‚
    large corpes of text and to be able to let the neural networkã€‚Learn the language
    for themselvesã€‚ This is how it worksã€‚ Nowï¼Œ previouslyï¼Œ what we would do when we
    were generating the nonsense sentences is we would put in maybe 20 charactersã€‚
    Well now we're not doing this character basedã€‚ We're doing it word basedã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸ä¿¡æˆ‘ï¼Œè¿™éå¸¸å›°éš¾ã€‚ç°åœ¨ï¼Œæƒ³æ³•æ˜¯ä½¿ç”¨å¤§æ•°æ®ï¼Œå¤§é‡æ–‡æœ¬ï¼Œè®©ç¥ç»ç½‘ç»œè‡ªå·±å­¦ä¹ è¯­è¨€ã€‚è¿™å°±æ˜¯å®ƒçš„å·¥ä½œæ–¹å¼ã€‚ä»¥å‰ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆæ— æ„ä¹‰å¥å­æ—¶ä¼šè¾“å…¥å¤§çº¦20ä¸ªå­—ç¬¦ã€‚ç°åœ¨æˆ‘ä»¬ä¸å†åŸºäºå­—ç¬¦ï¼Œè€Œæ˜¯åŸºäºè¯ã€‚
- en: We could do captions character basedã€‚ it would require potentially more trainingã€‚
    and this is just simply the technique I am using at this pointã€‚ if you'd like
    to try it in character basedï¼Œ Id definitely encourage youã€‚ and I'd be curious
    to see what results you gotã€‚ you could definitely use this code as a starting
    point for thisã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åŸºäºå­—ç¬¦åšå­—å¹•ã€‚è¿™å¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒï¼Œè€Œè¿™å°±æ˜¯æˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨çš„æŠ€æœ¯ã€‚å¦‚æœä½ æƒ³å°è¯•åŸºäºå­—ç¬¦çš„æ–¹æ³•ï¼Œæˆ‘ç»å¯¹é¼“åŠ±ä½ ã€‚æˆ‘ä¹Ÿå¾ˆå¥½å¥‡ä½ ä¼šå¾—åˆ°ä»€ä¹ˆç»“æœã€‚ä½ å¯ä»¥ä½¿ç”¨è¿™æ®µä»£ç ä½œä¸ºèµ·ç‚¹ã€‚
- en: but since it is now word basedï¼Œ we are going to present a vectorã€‚ So this this
    is going into the neural networkã€‚ and the individual elements are not charactersã€‚
    they're words and we have a special token word called startï¼Œ this is what kicks
    the whole thing offã€‚ So instead of giving it a seed like we did beforeã€‚ since
    we needed to generate the entire captionã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ç”±äºç°åœ¨æ˜¯åŸºäºè¯çš„ï¼Œæˆ‘ä»¬å°†å‘ˆç°ä¸€ä¸ªå‘é‡ã€‚è¿™å°†è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œå•ä¸ªå…ƒç´ ä¸æ˜¯å­—ç¬¦ï¼Œè€Œæ˜¯å•è¯ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ ‡è®°è¯å«åšâ€œstartâ€ï¼Œè¿™å°±æ˜¯ä¸€åˆ‡çš„èµ·ç‚¹ã€‚å› æ­¤ï¼Œä¸ä¹‹å‰ç»™ç§å­ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ç”Ÿæˆæ•´ä¸ªå­—å¹•ã€‚
- en: we really can't seed it because we don't we don't know how the captionsã€‚start
    so all captions start with the word startï¼Œ which is and not even the real word
    startã€‚ This is just token that we're usingã€‚ And then we initially send it just
    start but here's the trickã€‚ neural networks can accept many different inputs and
    multiple thingsã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœŸçš„æ— æ³•ç»™å®ƒç§å­ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“å­—å¹•æ˜¯å¦‚ä½•å¼€å§‹çš„ã€‚æ‰€æœ‰å­—å¹•éƒ½ä»¥å•è¯â€œstartâ€å¼€å¤´ï¼Œè€Œè¿™ç”šè‡³ä¸æ˜¯å®é™…çš„å•è¯â€œstartâ€ã€‚è¿™åªæ˜¯æˆ‘ä»¬ä½¿ç”¨çš„ä¸€ä¸ªæ ‡è®°ã€‚ç„¶åæˆ‘ä»¬æœ€åˆåªå‘é€â€œstartâ€ï¼Œä½†è¿™æ˜¯è¯€çªã€‚ç¥ç»ç½‘ç»œå¯ä»¥æ¥å—è®¸å¤šä¸åŒçš„è¾“å…¥å’Œå¤šä¸ªå†…å®¹ã€‚
- en: So we're input to this is really very similar to the previous partã€‚ but we're
    putting an entire image into it too So there's two inputs that are coming in and
    we'll see that the Kira's functional API as opposed to sequence is absolutely
    necessary for thisã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çš„è¾“å…¥å®é™…ä¸Šä¸å‰é¢çš„éƒ¨åˆ†éå¸¸ç›¸ä¼¼ï¼Œä½†æˆ‘ä»¬ä¹Ÿè¾“å…¥äº†æ•´å¼ å›¾åƒã€‚æ‰€ä»¥æœ‰ä¸¤ä¸ªè¾“å…¥è¿›æ¥ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°Kiraçš„åŠŸèƒ½æ€§APIä¸åºåˆ—ç›¸æ¯”åœ¨è¿™é‡Œç»å¯¹æ˜¯å¿…è¦çš„ã€‚
- en: So we use the model and Kira's functional API for Resnet and for a couple of
    other things so far in this courseã€‚ but we will definitely be using model and
    functional API for this because this is a fairly complicated neural network are
    giving it a picture and then we gradually build this up just like we did in the
    previous partã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹å’ŒKiraçš„åŠŸèƒ½æ€§APIæ¥å¤„ç†Resnetä»¥åŠç›®å‰è¯¾ç¨‹ä¸­çš„å…¶ä»–å‡ ä¸ªéƒ¨åˆ†ã€‚ä½†æˆ‘ä»¬è‚¯å®šä¼šä½¿ç”¨æ¨¡å‹å’ŒåŠŸèƒ½æ€§APIï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªç›¸å½“å¤æ‚çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ç»™å®ƒä¸€å¼ å›¾ç‰‡ï¼Œç„¶ååƒåœ¨å‰é¢çš„éƒ¨åˆ†ä¸€æ ·é€æ­¥æ„å»ºè¿™ä¸ªç½‘ç»œã€‚
- en: So we pass its startã€‚A picture of a dog running in the grassã€‚ that's actually
    my dogã€‚ And then the neural network takes both of thoseã€‚ and it returns probabilitiesã€‚
    And it'll sayï¼Œ okayã€‚ I think maybe the nextï¼Œ the next one is runsã€‚ Now this is
    if we had put in a and dog hereã€‚ So start a dog runs would be the highest probabilityã€‚
    Nowã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä»å¼€å§‹éƒ¨åˆ†å¼€å§‹ã€‚ä¸€å¼ ç‹—åœ¨è‰åœ°ä¸Šå¥”è·‘çš„ç…§ç‰‡ã€‚é‚£å®é™…ä¸Šæ˜¯æˆ‘çš„ç‹—ã€‚ç„¶åç¥ç»ç½‘ç»œä¼šæ¥æ”¶è¿™ä¸¤è€…ï¼Œå¹¶è¿”å›æ¦‚ç‡ã€‚ç„¶åå®ƒä¼šè¯´ï¼Œå¥½å§ã€‚æˆ‘æƒ³ä¸‹ä¸€æ­¥å¯èƒ½æ˜¯â€œè·‘â€ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œè¾“å…¥â€œaâ€å’Œâ€œç‹—â€ï¼Œé‚£ä¹ˆâ€œå¼€å§‹ä¸€åªç‹—è·‘â€å°†æ˜¯æœ€é«˜æ¦‚ç‡ã€‚ç°åœ¨ã€‚
- en: this is really long because this list of dummy variables is every single word
    that is in this thing vocabularyã€‚ we'll see the size of the vocabulary in a momentã€‚
    but a lot of dummy variablesã€‚ So this is the overall structure of what we're going
    to buildã€‚ and we're going to train this neural networkã€‚ obviously using the transferred
    learning so that we don't have to train this literally from scratchã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœŸçš„å¾ˆé•¿ï¼Œå› ä¸ºè¿™ç»„è™šæ‹Ÿå˜é‡åŒ…å«äº†è¿™ä¸ªè¯æ±‡è¡¨ä¸­çš„æ¯ä¸€ä¸ªå•è¯ã€‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°è¯æ±‡è¡¨çš„å¤§å°ï¼Œä½†æœ‰å¾ˆå¤šè™šæ‹Ÿå˜é‡ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬è¦æ„å»ºçš„æ•´ä½“ç»“æ„ï¼Œæˆ‘ä»¬å°†è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œï¼Œæ˜¾ç„¶ä½¿ç”¨è¿ç§»å­¦ä¹ ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…ä»å¤´å¼€å§‹è®­ç»ƒã€‚
- en: that would be that would be Hughes always use transferred learning if you canã€‚
    I am not going to actually run this codeã€‚ It takes this probably I would seem
    to remember about two hoursã€‚3 hoursã€‚ maybe for to actually train this neural networkã€‚
    So I've got it allã€‚Ran and the version that is up on Github should be pre-ran
    with thisã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä½ åº”è¯¥æ€»æ˜¯ä½¿ç”¨è¿ç§»å­¦ä¹ ï¼Œå¦‚æœå¯ä»¥çš„è¯ã€‚æˆ‘ä¸ä¼šå®é™…è¿è¡Œè¿™æ®µä»£ç ã€‚æ ¹æ®æˆ‘çš„è®°å¿†ï¼Œå¤§çº¦éœ€è¦ä¸¤ä¸ªå°æ—¶æˆ–ä¸‰å°æ—¶æ¥è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œã€‚æ‰€ä»¥æˆ‘å·²ç»æŠŠå®ƒå…¨éƒ¨è¿è¡Œè¿‡äº†ï¼ŒGithubä¸Šä¸Šä¼ çš„ç‰ˆæœ¬åº”è¯¥æ˜¯é¢„å…ˆè¿è¡Œè¿‡çš„ã€‚
- en: Now you might see some different captionsã€‚ if you compare what you're seeing
    in this video to the actual websiteã€‚ because if I rerun thisï¼Œ this is all stochasticã€‚
    So you can get you can definitely get different resultsã€‚ And I tend to rerun these
    as the semester progresses as different versions of Tensorflow come out So you'll
    see different resultsã€‚ these are all the imports that you needã€‚ the only thing
    that's somewhat interesting here is these are the start in in tokensã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯èƒ½ä¼šçœ‹åˆ°ä¸€äº›ä¸åŒçš„æ ‡é¢˜ã€‚å¦‚æœä½ å°†è¿™æ®µè§†é¢‘ä¸­çš„å†…å®¹ä¸å®é™…ç½‘ç«™è¿›è¡Œæ¯”è¾ƒï¼Œå› ä¸ºå¦‚æœæˆ‘é‡æ–°è¿è¡Œï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯éšæœºçš„ã€‚æ‰€ä»¥ä½ ç»å¯¹å¯ä»¥å¾—åˆ°ä¸åŒçš„ç»“æœã€‚éšç€å­¦æœŸçš„è¿›å±•ï¼Œæˆ‘ä¼šå€¾å‘äºé‡æ–°è¿è¡Œè¿™äº›ï¼Œå› ä¸ºä¸åŒç‰ˆæœ¬çš„Tensorflowä¼šå‡ºç°ã€‚æ‰€ä»¥ä½ ä¼šçœ‹åˆ°ä¸åŒçš„ç»“æœã€‚è¿™äº›æ˜¯ä½ éœ€è¦çš„æ‰€æœ‰å¯¼å…¥ï¼Œå”¯ä¸€æœ‰ç‚¹æœ‰è¶£çš„æ˜¯è¿™äº›æ˜¯å¼€å§‹å’Œâ€œinâ€æ ‡è®°ã€‚
- en: So we start it up here with the start token and it keeps adding additional wordsã€‚
    So we put in just startï¼Œ it would hopefully give us a then we would give a start
    and a it would hopefully give us dog then we give a start a and dog and it would
    hopefully give us runs and it continuesã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œç”¨å¼€å§‹æ ‡è®°å¯åŠ¨å®ƒï¼Œå¹¶ä¸æ–­æ·»åŠ é¢å¤–çš„å•è¯ã€‚å¦‚æœæˆ‘ä»¬ä»…è¾“å…¥â€œå¼€å§‹â€ï¼Œå®ƒä¼šå¸Œæœ›ç»™æˆ‘ä»¬ä¸€ä¸ªï¼Œç„¶åæˆ‘ä»¬å†è¾“å…¥â€œå¼€å§‹â€å’Œâ€œaâ€ï¼Œå®ƒä¼šå¸Œæœ›ç»™æˆ‘ä»¬â€œç‹—â€ï¼Œç„¶åæˆ‘ä»¬ç»™å‡ºâ€œå¼€å§‹â€ã€â€œaâ€å’Œâ€œç‹—â€ï¼Œå®ƒä¼šå¸Œæœ›ç»™æˆ‘ä»¬â€œè·‘â€ï¼Œç„¶åç»§ç»­ã€‚
- en: eventually it'll give us an in tokenã€‚ So either we hit the wall here and run
    out of spaceã€‚ that'll stop us or we get a in tokenã€‚ So this is how the output
    of the neural network can be very not fixed lengthã€‚If you need a neural network
    to give you a to generate a sentence or to give you something that's not fixed
    lengthã€‚ This is what you're typically needing to doã€‚ You build it piece by piece
    by piece and let the neural network keep adding another element on it for you
    Epochs that's simply how many epos we're going train it for we're not using early
    stopping or anything like that use inception is true if you want to try mobilenet
    just put false in there doesn't work as wellã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå®ƒä¼šç»™æˆ‘ä»¬ä¸€ä¸ªâ€œinâ€æ ‡è®°ã€‚æ‰€ä»¥è¦ä¹ˆæˆ‘ä»¬åœ¨è¿™é‡Œæ’å¢™è€Œæ²¡æœ‰ç©ºé—´ï¼Œè¦ä¹ˆæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªâ€œinâ€æ ‡è®°ã€‚è¿™å°±æ˜¯ç¥ç»ç½‘ç»œçš„è¾“å‡ºå¯ä»¥æ˜¯éå¸¸ä¸å›ºå®šé•¿åº¦çš„åŸå› ã€‚å¦‚æœä½ éœ€è¦ä¸€ä¸ªç¥ç»ç½‘ç»œç”Ÿæˆä¸€ä¸ªå¥å­æˆ–æä¾›ä¸€äº›ä¸å›ºå®šé•¿åº¦çš„å†…å®¹ï¼Œè¿™å°±æ˜¯ä½ é€šå¸¸éœ€è¦åšçš„ã€‚ä½ éœ€è¦é€æ­¥æ„å»ºï¼Œè®©ç¥ç»ç½‘ç»œä¸ºä½ ä¸æ–­æ·»åŠ æ–°çš„å…ƒç´ ã€‚EpochsæŒ‡çš„æ˜¯æˆ‘ä»¬å°†è®­ç»ƒå®ƒçš„å‘¨æœŸæ•°ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨æå‰åœæ­¢æˆ–å…¶ä»–ä»»ä½•ä¸œè¥¿ã€‚å¦‚æœä½ æƒ³å°è¯•mobilenetï¼Œåªéœ€å°†è¿™é‡Œçš„å€¼è®¾ä¸ºfalseï¼Œè¿™æ ·æ•ˆæœä¸å¥½ã€‚
- en: I have my hours minutes string because we we time how long these things take
    definitely use coab for thisã€‚ the GPU is your friend or if you have your own faster
    GPU definitely use thatã€‚ You're going to need to download some data sets for this
    So here I have the path content my drive Now you might need to change this if
    you're putting your stuff in different locationsã€‚ but you'll need to create the
    directories for for each of those in that folderã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰æˆ‘çš„å°æ—¶å’Œåˆ†é’Ÿå­—ç¬¦ä¸²ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šè®¡æ—¶è¿™äº›äº‹æƒ…èŠ±è´¹äº†å¤šé•¿æ—¶é—´ï¼Œç»å¯¹è¦ä½¿ç”¨coabã€‚GPUæ˜¯ä½ çš„æœ‹å‹ï¼Œå¦‚æœä½ æœ‰è‡ªå·±çš„æ›´å¿«çš„GPUï¼Œç»å¯¹è¦ä½¿ç”¨å®ƒã€‚ä½ éœ€è¦ä¸‹è½½ä¸€äº›æ•°æ®é›†ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘æœ‰è·¯å¾„å†…å®¹åœ¨æˆ‘çš„é©±åŠ¨å™¨ä¸Šã€‚ç°åœ¨ï¼Œå¦‚æœä½ å°†ä½ çš„ä¸œè¥¿æ”¾åœ¨ä¸åŒçš„ä½ç½®ï¼Œå¯èƒ½éœ€è¦æ›´æ”¹è¿™ä¸ªè·¯å¾„ï¼Œä½†ä½ éœ€è¦ä¸ºè¯¥æ–‡ä»¶å¤¹ä¸­çš„æ¯ä¸€ä¸ªç›®å½•åˆ›å»ºç›®å½•ã€‚
- en: you can see from the source code they're all just named this and then you need
    to create a data directory that's where it's going to create theã€‚Output files
    those should all be directly off of captionsã€‚ So unzip these these two and put
    them thereã€‚ By the wayã€‚ getting a hold of the flicker8k data set for this can
    be a little trickyã€‚ read the article hereã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä»æºä»£ç ä¸­çœ‹åˆ°ï¼Œå®ƒä»¬çš„åç§°éƒ½æ˜¯è¿™ä¸ªï¼Œç„¶åä½ éœ€è¦åˆ›å»ºä¸€ä¸ªæ•°æ®ç›®å½•ï¼Œè¾“å‡ºæ–‡ä»¶å°†åˆ›å»ºåœ¨è¿™é‡Œã€‚è¿™äº›æ–‡ä»¶åº”è¯¥ç›´æ¥æ¥è‡ªæ ‡é¢˜ã€‚å› æ­¤ï¼Œè§£å‹è¿™ä¸¤ä¸ªæ–‡ä»¶å¹¶æ”¾åˆ°é‚£é‡Œã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè·å–
    flicker8k æ•°æ®é›†å¯èƒ½æœ‰ç‚¹æ£˜æ‰‹ã€‚å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»ç›¸å…³æ–‡ç« ã€‚
- en: there's some copyright questions around that oneã€‚ So its it's difficultã€‚ It's
    not difficult to get a hold ofã€‚ I can't put it into my Github repository because
    it it's not mineã€‚ So if you click that linkï¼Œ you can find out exactly how to get
    a hold of itã€‚ So we're going to clean this data set and begin to process it the
    data set by the wayã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™ä¸ªé—®é¢˜æœ‰ä¸€äº›ç‰ˆæƒé—®é¢˜ã€‚å› æ­¤ï¼Œè¿™å¾ˆå¤æ‚ã€‚å¹¶ä¸æ˜¯è¯´å¾ˆéš¾è·å¾—ã€‚æˆ‘æ— æ³•å°†å…¶æ”¾å…¥æˆ‘çš„ GitHub å­˜å‚¨åº“ï¼Œå› ä¸ºå®ƒä¸æ˜¯æˆ‘çš„ã€‚æ‰€ä»¥å¦‚æœä½ ç‚¹å‡»é‚£ä¸ªé“¾æ¥ï¼Œä½ å¯ä»¥å‡†ç¡®æ‰¾åˆ°å¦‚ä½•è·å–å®ƒçš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ¸…ç†è¿™ä¸ªæ•°æ®é›†ï¼Œå¹¶å¼€å§‹å¤„ç†å®ƒï¼Œé¡ºä¾¿è¯´ä¸€å¥ã€‚
- en: what this data isï¼Œ is it's from Flickrã€‚ it has 8000 images and captions for
    themã€‚ So it's exactly what we need and we need to basically break this up and
    what we're doing here is we're cleaning up the descriptionsã€‚ We're essentially
    converting them to lowerã€‚re for this null punctuationã€‚ These are punctuations
    we're removing certain punctuationã€‚ We are essentially removingã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•°æ®æ¥è‡ª Flickrã€‚å®ƒæœ‰ 8000 å¼ å›¾åƒåŠå…¶å¯¹åº”çš„æ ‡é¢˜ã€‚å› æ­¤ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶æ‹†åˆ†ï¼Œè€Œæˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„å°±æ˜¯æ¸…ç†æè¿°ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šå°†å…¶è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶å»æ‰æ ‡ç‚¹ç¬¦å·ã€‚è¿™äº›æ˜¯æˆ‘ä»¬æ­£åœ¨ç§»é™¤çš„æŸäº›æ ‡ç‚¹ç¬¦å·ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯åœ¨ç§»é™¤ã€‚
- en: Very short words hereã€‚ We're removing words that do not have alpha alphabetic
    components to themã€‚ and we're gradually figuring out the lengthã€‚ So we need to
    know what that maximum caption size is because that's going to be our sequence
    lengthã€‚ and we gradually build up our dictionary of theseã€‚ Then we can print out
    what we collectã€‚ So look up is essentially the number of unique words and then
    the number of words in our dictionary and the max lengthã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€äº›éå¸¸ç®€çŸ­çš„è¯ã€‚æˆ‘ä»¬æ­£åœ¨ç§»é™¤é‚£äº›æ²¡æœ‰å­—æ¯ç»„æˆçš„è¯ï¼Œå¹¶é€æ¸ç¡®å®šé•¿åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æœ€å¤§æ ‡é¢˜é•¿åº¦ï¼Œå› ä¸ºè¿™å°†æ˜¯æˆ‘ä»¬çš„åºåˆ—é•¿åº¦ã€‚ç„¶åæˆ‘ä»¬é€æ¸æ„å»ºè¿™äº›è¯çš„å­—å…¸ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºæˆ‘ä»¬æ”¶é›†åˆ°çš„å†…å®¹ã€‚å› æ­¤ï¼ŒæŸ¥æ‰¾çš„å®è´¨æ˜¯ç‹¬ç‰¹è¯æ±‡çš„æ•°é‡ä»¥åŠæˆ‘ä»¬å­—å…¸ä¸­çš„è¯æ±‡æ•°é‡å’Œæœ€å¤§é•¿åº¦ã€‚
- en: the max caption lengthã€‚ Then we load the glove embeddingsã€‚ Now what the glove
    embeddings are is essentially vectors for each of the words in the vocabularyã€‚And
    each of those vocabulary words is going to have a corresponding vectorã€‚ and those
    are the features that well put into the neural network to actually do the predictions
    rather than doingã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§æ ‡é¢˜é•¿åº¦ã€‚ç„¶åæˆ‘ä»¬åŠ è½½ GloVe åµŒå…¥ã€‚GloVe åµŒå…¥åŸºæœ¬ä¸Šæ˜¯è¯æ±‡ä¸­æ¯ä¸ªè¯çš„å‘é‡ã€‚è¿™äº›è¯æ±‡ä¸­çš„æ¯ä¸ªè¯éƒ½å°†æœ‰ä¸€ä¸ªç›¸åº”çš„å‘é‡ï¼Œè¿™äº›å°±æ˜¯æˆ‘ä»¬æ”¾å…¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯ç›´æ¥è¿›è¡Œã€‚
- en: say the index numbers or dummiesã€‚ This is much better than using a dummy variable
    for each vocabulary word because there's a lot of vocabulary words and that would
    be a lot of additional data thereã€‚ We do use dummies for the vocabulary words
    on the output from the neural networkã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´ç´¢å¼•å·ç æˆ–è™šæ‹Ÿå˜é‡ã€‚è¿™æ¯”ä¸ºæ¯ä¸ªè¯æ±‡ä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿå˜é‡è¦å¥½å¾—å¤šï¼Œå› ä¸ºè¯æ±‡ä¸­æœ‰å¾ˆå¤šè¯ï¼Œè¿™å°†ä¼šäº§ç”Ÿå¤§é‡çš„é¢å¤–æ•°æ®ã€‚æˆ‘ä»¬ç¡®å®åœ¨ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸­ä½¿ç”¨äº†è¯æ±‡çš„è™šæ‹Ÿå˜é‡ã€‚
- en: but we do not on the inputã€‚ The other advantage is those vectors of the wordsï¼Œ
    similar wordsã€‚ the vectors will be closer to each other in Euclidean spaceã€‚ So
    using linear algebra where you're basically calculating distances between two
    vectorsã€‚ two similar words will be fairly close in spaceã€‚ we read in all of the
    image namesã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¸ä¼šåœ¨è¾“å…¥ä¸­ä½¿ç”¨ã€‚å¦ä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œè¿™äº›è¯çš„å‘é‡ï¼Œç›¸ä¼¼çš„è¯ï¼Œå‘é‡åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­ä¼šæ›´æ¥è¿‘ã€‚å› æ­¤ï¼Œåˆ©ç”¨çº¿æ€§ä»£æ•°ï¼Œä½ åŸºæœ¬ä¸Šæ˜¯åœ¨è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»ã€‚ä¸¤ä¸ªç›¸ä¼¼çš„è¯åœ¨ç©ºé—´ä¸­ä¼šç›¸å½“æ¥è¿‘ã€‚æˆ‘ä»¬è¯»å–æ‰€æœ‰çš„å›¾åƒåç§°ã€‚
- en: We're basically getting ready to load everythingã€‚ We have 6000 images in the
    training set 1000 in the testã€‚ and then we build up all of these descriptions
    So thereã€‚Are going to start with the start tokenã€‚ Then they're going to have the
    actual words of the caption and they're going to end with stopã€‚ So this is how
    we basically start and stop the captioning process as we build the sequence like
    we saw up in the diagram earlierã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŸºæœ¬ä¸Šå‡†å¤‡åŠ è½½æ‰€æœ‰å†…å®¹ã€‚è®­ç»ƒé›†ä¸­æœ‰ 6000 å¼ å›¾åƒï¼Œæµ‹è¯•é›†ä¸­æœ‰ 1000 å¼ ã€‚ç„¶åæˆ‘ä»¬æ„å»ºæ‰€æœ‰è¿™äº›æè¿°ã€‚å› æ­¤ï¼Œå°†ä»å¼€å§‹æ ‡è®°å¼€å§‹ã€‚æ¥ç€ä¼šæœ‰æ ‡é¢˜çš„å®é™…è¯æ±‡ï¼Œå¹¶ä»¥åœæ­¢ç»“æŸã€‚è¿™å°±æ˜¯æˆ‘ä»¬åŸºæœ¬ä¸Šå¦‚ä½•å¼€å§‹å’Œç»“æŸæ ‡é¢˜ç”Ÿæˆè¿‡ç¨‹çš„æ–¹æ³•ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„å›¾è¡¨ä¸­æ‰€çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: Nowï¼Œ I have the code here to use inception or to use mobilenetã€‚ this number
    in the output dimensionsï¼Œ that is really the reason whyã€‚ So this turns each of
    the imagesã€‚ So using inceptionï¼Œ if you use inception just straight upã€‚ it would
    return 1000 probabilities because there's 1000 images in imagenet and each of
    those imagesã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘è¿™é‡Œæœ‰ä»£ç å¯ä»¥ä½¿ç”¨inceptionæˆ–mobilenetã€‚è¿™è¾“å‡ºç»´åº¦ä¸­çš„è¿™ä¸ªæ•°å­—ï¼Œç¡®å®æ˜¯åŸå› æ‰€åœ¨ã€‚æ‰€ä»¥è¿™ä¼šè½¬æ¢æ¯ä¸€å¼ å›¾ç‰‡ã€‚ä½¿ç”¨inceptionï¼Œå¦‚æœä½ ç›´æ¥ä½¿ç”¨inceptionï¼Œå®ƒä¼šè¿”å›1000ä¸ªæ¦‚ç‡ï¼Œå› ä¸ºimagenetä¸­æœ‰1000å¼ å›¾ç‰‡ï¼Œæ¯ä¸€å¼ å›¾ç‰‡ã€‚
- en: it would give you the probability that the image is one of thoseã€‚ We strip that
    layer off and below that is a 2048 densely connected layerã€‚ And we use basically
    those outputs like we saw earlier in feature engineering from transferã€‚Learning
    we use that as feature engineeringã€‚ so this 2048 vector that comes out out of
    inception with the top top layers sheared offã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šç»™ä½ è¿™ä¸ªå›¾åƒæ˜¯å…¶ä¸­ä¹‹ä¸€çš„æ¦‚ç‡ã€‚æˆ‘ä»¬å»æ‰äº†é‚£ä¸€å±‚ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ª2048ç»´çš„å¯†é›†è¿æ¥å±‚ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šåƒä¹‹å‰åœ¨è¿ç§»å­¦ä¹ ä¸­çš„ç‰¹å¾å·¥ç¨‹ä¸­çœ‹åˆ°çš„é‚£æ ·ä½¿ç”¨è¿™äº›è¾“å‡ºã€‚æˆ‘ä»¬å°†å…¶ç”¨ä½œç‰¹å¾å·¥ç¨‹ã€‚å› æ­¤ï¼Œè¿™ä¸ªä»inceptionä¸­è¾“å‡ºçš„2048ç»´å‘é‡ï¼Œä¸Šé¢çš„ä¸€äº›å±‚è¢«å»æ‰äº†ã€‚
- en: And that's what we're doing hereã€‚ we're removing two layersã€‚ Those become essentially
    engineered features for those imagesã€‚ And againã€‚ it it's like with glove that
    2048 vector similar images should be closer together in vector spaceã€‚ notice how
    many of the output dimension is hereã€‚ There's not densely connected layers there
    because the mobile net is trying to be very compatible with mobile devices and
    power consumptionã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œåšçš„ã€‚æˆ‘ä»¬å»æ‰äº†ä¸¤ä¸ªå±‚ã€‚è¿™äº›æœ¬è´¨ä¸Šæˆä¸ºäº†è¿™äº›å›¾åƒçš„å·¥ç¨‹ç‰¹å¾ã€‚å†è¯´ä¸€æ¬¡ï¼Œå°±åƒä½¿ç”¨gloveä¸€æ ·ï¼Œ2048ç»´çš„ç›¸ä¼¼å›¾åƒåœ¨å‘é‡ç©ºé—´ä¸­åº”è¯¥æ›´æ¥è¿‘ã€‚æ³¨æ„è¿™é‡Œè¾“å‡ºç»´åº¦æœ‰å¤šå°‘ã€‚å› ä¸ºmobile
    netè¯•å›¾ä¸ç§»åŠ¨è®¾å¤‡å’ŒåŠŸè€—éå¸¸å…¼å®¹ï¼Œæ‰€ä»¥é‚£é‡Œæ²¡æœ‰å¯†é›†è¿æ¥çš„å±‚ã€‚
- en: So your number of dimensions when you shear off those top layers explodes and
    you've got 50 That's not ideal for feature engineering because that's going to
    be a very sparse vectorã€‚ at least when I've inspected on many of those will be0
    and you won't get quite you just will not get as good a results as using the 2048ã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“ä½ å»æ‰é‚£äº›é¡¶å±‚æ—¶ï¼Œä½ çš„ç»´åº¦æ•°é‡ä¼šæ¿€å¢ï¼Œè€Œä½ æœ‰50ã€‚è¿™å¯¹äºç‰¹å¾å·¥ç¨‹æ¥è¯´å¹¶ä¸æ˜¯ç†æƒ³çš„ï¼Œå› ä¸ºè¿™ä¼šæ˜¯ä¸€ä¸ªéå¸¸ç¨€ç–çš„å‘é‡ã€‚è‡³å°‘æˆ‘åœ¨è®¸å¤šå®ä¾‹ä¸­æ£€æŸ¥æ—¶ï¼Œä¼šå‘ç°å¾ˆå¤šç»´åº¦ä¸º0ï¼Œç»“æœä¸ä¼šé‚£ä¹ˆå¥½ï¼Œä½¿ç”¨2048ç»´çš„ç»“æœä¼šæ›´å¥½ã€‚
- en: you're welcome to experiment with it though and the code should all workã€‚ These
    are the key constantã€‚That you need to change the height and the width because
    different transferred neural networks are trained for different image sizes and
    it prints out a summaryã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ä½ å¯ä»¥è¿›è¡Œå®éªŒï¼Œä»£ç åº”è¯¥éƒ½èƒ½æ­£å¸¸å·¥ä½œã€‚è¿™äº›æ˜¯ä½ éœ€è¦æ›´æ”¹çš„å…³é”®å¸¸é‡ã€‚ä½ éœ€è¦æ”¹å˜é«˜åº¦å’Œå®½åº¦ï¼Œå› ä¸ºä¸åŒçš„è¿ç§»ç¥ç»ç½‘ç»œæ˜¯ä¸ºä¸åŒçš„å›¾åƒå¤§å°è®­ç»ƒçš„ï¼Œå®ƒä¼šæ‰“å°å‡ºæ‘˜è¦ã€‚
- en: This is quite longã€‚ It's a long neural network that we transfer inã€‚ We're going
    to create the training setsã€‚ So for each imageï¼Œ we need to encode itã€‚ and we're
    basically encoding the image to whatever that output dimension size isã€‚ Now this
    is what it isã€‚ you can't change that constant and change the output sizeã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç›¸å½“é•¿ã€‚å®ƒæ˜¯æˆ‘ä»¬è¿ç§»çš„ä¸€ä¸ªé•¿ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†åˆ›å»ºè®­ç»ƒé›†ã€‚å› æ­¤ï¼Œå¯¹äºæ¯å¼ å›¾åƒï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯å°†å›¾åƒç¼–ç ä¸ºè¾“å‡ºç»´åº¦çš„å¤§å°ã€‚ç°åœ¨å°±æ˜¯è¿™æ ·ã€‚ä½ æ— æ³•æ”¹å˜é‚£ä¸ªå¸¸é‡å’Œè¾“å‡ºå¤§å°ã€‚
- en: So we're essentially taking the imageã€‚ we're resizing it to a standard sizeã€‚
    We're not worried about keeping the aspect ratioã€‚ The transferred neural network
    tends to figure that outã€‚ We do any preprocessing that the transferred neural
    network needsã€‚ We expand the dimensions hereã€‚ We're essentially taking the long
    string that these images are loaded inã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æœ¬è´¨ä¸Šæ˜¯åœ¨å¤„ç†å›¾åƒã€‚æˆ‘ä»¬å°†å…¶è°ƒæ•´ä¸ºæ ‡å‡†å¤§å°ã€‚æˆ‘ä»¬ä¸æ‹…å¿ƒä¿æŒå®½é«˜æ¯”ã€‚è¿ç§»çš„ç¥ç»ç½‘ç»œå¾€å¾€ä¼šè§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œè¿ç§»ç¥ç»ç½‘ç»œéœ€è¦çš„ä»»ä½•é¢„å¤„ç†ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ‰©å±•ç»´åº¦ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯å°†è¿™äº›å›¾åƒåŠ è½½æ—¶çš„é•¿å­—ç¬¦ä¸²å¤„ç†ã€‚
- en: and putting it back into the grid that a image really should beã€‚ Here is where
    we perform any preprocessing actually not up hereã€‚ that's essentially convertingã€‚To
    an array and then we call the either mobilenet inception to predictã€‚ that's where
    it turns into that 2048 vectorã€‚ and then we reshape it so it's in the right sizeã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å°†å…¶æ”¾å›åˆ°å›¾åƒåº”è¯¥çœŸæ­£å¤„äºçš„ç½‘æ ¼ä¸­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®é™…ä¸Šæ‰§è¡Œä»»ä½•é¢„å¤„ç†ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡Œã€‚é‚£åŸºæœ¬ä¸Šæ˜¯è½¬æ¢ä¸ºæ•°ç»„ï¼Œç„¶åæˆ‘ä»¬è°ƒç”¨mobilenetæˆ–inceptionè¿›è¡Œé¢„æµ‹ã€‚é‚£æ˜¯å¦‚ä½•å˜æˆ2048ç»´å‘é‡çš„ã€‚ç„¶åæˆ‘ä»¬å°†å…¶é‡å¡‘ä¸ºæ­£ç¡®çš„å¤§å°ã€‚
- en: essentially for prediction or for input into the neural network This is where
    we generate the training set This is where we call this over and over this can
    take some time So we actually pickle the training set after we load so we're loading
    all those JpeEgs or PGs or whatever that image data is and turning them into the
    2048 vectors that this thing crunches them down intoã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šæ˜¯ä¸ºäº†é¢„æµ‹æˆ–è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ã€‚è¿™æ˜¯æˆ‘ä»¬ç”Ÿæˆè®­ç»ƒé›†çš„åœ°æ–¹ã€‚è¿™æ˜¯æˆ‘ä»¬åå¤è°ƒç”¨çš„åœ°æ–¹ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚å› æ­¤ï¼Œåœ¨åŠ è½½åæˆ‘ä»¬å®é™…ä¸Šä¼šå°†è®­ç»ƒé›†è¿›è¡Œåºåˆ—åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬æ­£åœ¨åŠ è½½æ‰€æœ‰è¿™äº›JpeEgsæˆ–PGsï¼Œæˆ–è€…é‚£äº›å›¾åƒæ•°æ®ï¼Œå¹¶å°†å®ƒä»¬è½¬åŒ–ä¸ºè¿™ä¸ªä¸œè¥¿å‹ç¼©æˆçš„2048ä¸ªå‘é‡ã€‚
- en: So this is a lot of image process doesn't take necessarily a tremendous amount
    of time but it can take a while this is where we process the captions and we get
    them into a similar structure Now we have 30000 captions because there's up to
    five different captions provided for each image that's just the way the data is
    So each image is actually multiple caption which which is kind of nice We're going
    to get rid ofã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¤§é‡çš„å›¾åƒå¤„ç†ä¸ä¸€å®šéœ€è¦èŠ±è´¹å·¨å¤§çš„æ—¶é—´ï¼Œä½†å¯èƒ½ä¼šèŠ±ä¸€äº›æ—¶é—´ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¤„ç†æ ‡é¢˜çš„åœ°æ–¹ï¼Œæˆ‘ä»¬å°†å®ƒä»¬æ•´ç†æˆç›¸ä¼¼çš„ç»“æ„ã€‚ç°åœ¨æˆ‘ä»¬æœ‰30000ä¸ªæ ‡é¢˜ï¼Œå› ä¸ºæ¯å¼ å›¾åƒæœ€å¤šæä¾›äº”ä¸ªä¸åŒçš„æ ‡é¢˜ï¼Œè¿™å°±æ˜¯æ•°æ®çš„æ–¹å¼ã€‚å› æ­¤ï¼Œæ¯å¼ å›¾åƒå®é™…ä¸Šéƒ½æœ‰å¤šä¸ªæ ‡é¢˜ï¼Œè¿™æ ·æŒºä¸é”™çš„ã€‚æˆ‘ä»¬å°†å»æ‰ä¸€äº›ã€‚
- en: Wds that don't occur that oftenã€‚ So our vocabulary drops down to just 1651ã€‚
    That helps a lotã€‚ And we build up two indexesã€‚ each of those 1651 words that we're
    dealing withã€‚ which we also add to the tokensã€‚ We have one that takes an index
    number and gives you a word back and a similar one that takes a word and gives
    you an index backã€‚ So you've got a double directional sort of dictionary to look
    these words up inã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ç»å¸¸å‡ºç°çš„å•è¯ã€‚å› æ­¤æˆ‘ä»¬çš„è¯æ±‡é‡é™åˆ°äº†1651ã€‚è¿™å¸®åŠ©äº†å¾ˆå¤šã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªç´¢å¼•ã€‚æ¯ä¸€ä¸ªæˆ‘ä»¬å¤„ç†çš„1651ä¸ªå•è¯ï¼Œè¿™ä¹Ÿæ·»åŠ åˆ°æ ‡è®°ä¸­ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªæ ¹æ®ç´¢å¼•å·è¿”å›å•è¯çš„ç´¢å¼•ï¼Œè¿˜æœ‰ä¸€ä¸ªæ ¹æ®å•è¯è¿”å›ç´¢å¼•çš„ç±»ä¼¼çš„ç´¢å¼•ã€‚å› æ­¤ï¼Œä½ æœ‰ä¸€ä¸ªåŒå‘çš„å­—å…¸æ¥æŸ¥æ‰¾è¿™äº›å•è¯ã€‚
- en: We do add two to the max length that accounts for this start and end tokenã€‚
    And then this is what it looks like really using the data generatorã€‚ We're going
    to start would just startã€‚ It should so we're calling that neural network multiple
    timesã€‚ call it would just start it adds a So this is what the training set actually
    looks likeã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¼šå°†æœ€å¤§é•¿åº¦åŠ ä¸Šä¸¤ä¸ªï¼Œä»¥è€ƒè™‘è¿™ä¸ªå¼€å§‹å’Œç»“æŸæ ‡è®°ã€‚ç„¶åè¿™å°±æ˜¯ä½¿ç”¨æ•°æ®ç”Ÿæˆå™¨çš„å®é™…æ ·å­ã€‚æˆ‘ä»¬å°†å¼€å§‹ï¼Œåªéœ€å¼€å§‹ã€‚å®ƒåº”è¯¥ï¼Œæ‰€ä»¥æˆ‘ä»¬å¤šæ¬¡è°ƒç”¨é‚£ä¸ªç¥ç»ç½‘ç»œã€‚è°ƒç”¨å®ƒä¼šå¼€å§‹æ·»åŠ ä¸€ä¸ªã€‚æ‰€ä»¥è¿™å°±æ˜¯è®­ç»ƒé›†çš„å®é™…æ ·å­ã€‚
- en: So just one pictureã€‚ and one captionã€‚ The idea here is this training sets going
    to be giganticã€‚ So for each imageã€‚ This is just one imageã€‚ This is hickoryã€‚ my
    dog running on the grassã€‚ He's not in the data setï¼Œ but I'm using him as an exampleã€‚
    for this oneã€‚ we would have five different captions of himã€‚ We would have maybe
    likeã€‚Budog runs on the grassã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åªæ˜¯ä¸€å¼ å›¾ç‰‡å’Œä¸€ä¸ªæ ‡é¢˜ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯è¿™ä¸ªè®­ç»ƒé›†å°†æ˜¯å·¨å¤§çš„ã€‚å› æ­¤å¯¹äºæ¯å¼ å›¾åƒï¼Œè¿™åªæ˜¯ä¸€ä¸ªå›¾åƒã€‚è¿™æ˜¯æˆ‘çš„ç‹—åœ¨è‰åœ°ä¸Šè·‘çš„æƒ…æ™¯ã€‚å®ƒä¸åœ¨æ•°æ®é›†ä¸­ï¼Œä½†æˆ‘ç”¨å®ƒä½œä¸ºä¾‹å­ã€‚å¯¹äºè¿™ä¸€å¼ ï¼Œæˆ‘ä»¬å°†æœ‰äº”ä¸ªä¸åŒçš„æ ‡é¢˜ã€‚æˆ‘ä»¬å¯èƒ½ä¼šæœ‰åƒâ€œç‹—åœ¨è‰åœ°ä¸Šè·‘â€è¿™æ ·çš„æ ‡é¢˜ã€‚
- en: dog running all these different variants of what the caption could be But for
    each of theseã€‚ this is showing just one captionã€‚ We need to generate all the phases
    of itã€‚ We need to generate with this image and just start return A with this image
    start an a return dog with this imageã€‚ start a dog return runsã€‚ there are a lot
    of data in this training setã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç‹—è·‘åŠ¨çš„æ‰€æœ‰è¿™äº›ä¸åŒå˜ä½“å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„æ ‡é¢˜ã€‚ä½†å¯¹äºæ¯ä¸€ä¸ªï¼Œè¿™é‡Œåªå±•ç¤ºä¸€ä¸ªæ ‡é¢˜ã€‚æˆ‘ä»¬éœ€è¦ç”Ÿæˆå®ƒçš„æ‰€æœ‰é˜¶æ®µã€‚æˆ‘ä»¬éœ€è¦ç”Ÿæˆè¿™ä¸ªå›¾åƒå¹¶å¼€å§‹è¿”å›â€œç‹—â€ï¼Œç„¶åç”¨è¿™ä¸ªå›¾åƒå¼€å§‹è¿”å›â€œç‹—â€ã€‚å¼€å§‹â€œç‹—â€ï¼Œè¿”å›â€œè·‘â€ã€‚åœ¨è¿™ä¸ªè®­ç»ƒé›†ä¸­æœ‰å¾ˆå¤šæ•°æ®ã€‚
- en: So we're going to use something called a generator to make it not so insane
    in terms of the RA requirementã€‚ and then you'd get your second image because the
    data set has 5ï¼Œ6000 of theseï¼Œ So each one of thoseã€‚ youd be you'd have to literally
    duplicate the image in the training set five times for each of the five captionsã€‚
    and then each of the captions gets a number of additional entryã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§å«åšç”Ÿæˆå™¨çš„ä¸œè¥¿ï¼Œä»¥ä½¿å…¶åœ¨RAè¦æ±‚æ–¹é¢ä¸é‚£ä¹ˆç–¯ç‹‚ã€‚ç„¶åä½ å°†å¾—åˆ°ä½ çš„ç¬¬äºŒå¼ å›¾ç‰‡ï¼Œå› ä¸ºæ•°æ®é›†ä¸­æœ‰56000ä¸ªè¿™æ ·çš„ã€‚æ‰€ä»¥æ¯ä¸€ä¸ªï¼Œä½ å¿…é¡»çœŸçš„åœ¨è®­ç»ƒé›†ä¸­å°†å›¾åƒå¤åˆ¶äº”æ¬¡ï¼Œä»¥å¯¹åº”æ¯ä¸ªæ ‡é¢˜çš„äº”ä¸ªç‰ˆæœ¬ã€‚ç„¶åæ¯ä¸ªæ ‡é¢˜è¿˜ä¼šæœ‰ä¸€äº›é¢å¤–çš„æ¡ç›®ã€‚
- en: So you would need to duplicate that picture get again for each of these intermediate
    placesã€‚ It's a lot of dataã€‚ So we use a data generatorã€‚ The data generator is
    what we're going toã€‚not generate this big matrix like we did before to pass inã€‚
    And essentially what's going on here is it's looping through all of the keysã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ éœ€è¦ä¸ºæ¯ä¸€ä¸ªä¸­é—´ä½ç½®å¤åˆ¶é‚£å¼ å›¾ç‰‡ã€‚è¿™æ˜¯å¤§é‡çš„æ•°æ®ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨æ•°æ®ç”Ÿæˆå™¨ã€‚æ•°æ®ç”Ÿæˆå™¨æ˜¯æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„ã€‚ä¸ä¼šåƒä¹‹å‰é‚£æ ·ç”Ÿæˆè¿™ä¸ªå¤§çŸ©é˜µæ¥ä¼ å…¥ã€‚åŸºæœ¬ä¸Šï¼Œè¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…æ˜¯å®ƒåœ¨éå†æ‰€æœ‰çš„é”®ã€‚
- en: Those are essentially all of the images that we haveã€‚ And then for each one
    we're looping through all the descriptionã€‚ So there's five of themã€‚ And then we
    generateã€‚ So these are percent of the dimensions we need to generate one for each
    pictureã€‚ one for each descriptionã€‚ And then one for each combination of those
    words in there so that we catch all the intermediate formã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬æ‹¥æœ‰çš„æ‰€æœ‰å›¾åƒã€‚ç„¶åå¯¹äºæ¯ä¸€ä¸ªå›¾åƒï¼Œæˆ‘ä»¬å¾ªç¯éå†æ‰€æœ‰æè¿°ã€‚å› æ­¤æœ‰äº”ä¸ªæè¿°ã€‚ç„¶åæˆ‘ä»¬ç”Ÿæˆã€‚è¿™äº›æ˜¯æˆ‘ä»¬éœ€è¦ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆä¸€ä¸ªã€ä¸ºæ¯ä¸ªæè¿°ç”Ÿæˆä¸€ä¸ªçš„ç»´åº¦çš„ç™¾åˆ†æ¯”ã€‚ç„¶åæ˜¯è¿™äº›è¯çš„æ¯ç§ç»„åˆï¼Œä»¥ä¾¿æˆ‘ä»¬æ•æ‰åˆ°æ‰€æœ‰çš„ä¸­é—´å½¢å¼ã€‚
- en: And we're also mindful of how many photos we want per batch that's a training
    hyperparameterã€‚ The way that this works is this is this big loop is not being
    ran just straight out and generateã€‚ it would just dump a ton of data if it didã€‚
    That's what the yield command in Python doesã€‚ This is essentially a dynamic collection
    that you're buildingã€‚ and every time you hit yieldã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è€ƒè™‘åˆ°æ¯æ‰¹æ¬¡å¸Œæœ›æœ‰å¤šå°‘å¼ ç…§ç‰‡ï¼Œè¿™æ˜¯ä¸€ä¸ªè®­ç»ƒè¶…å‚æ•°ã€‚è¿™ä¸ªå¤§çš„å¾ªç¯å¹¶ä¸æ˜¯ç›´æ¥è¿è¡Œå¹¶ç”Ÿæˆçš„ã€‚å¦‚æœè¿™æ ·åšï¼Œå®ƒä¼šå€¾å€’å¤§é‡æ•°æ®ã€‚è¿™å°±æ˜¯Pythonä¸­yieldå‘½ä»¤çš„ä½œç”¨ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªåŠ¨æ€é›†åˆï¼Œä½ æ­£åœ¨æ„å»ºï¼Œæ¯æ¬¡ä½ è§¦å‘yieldæ—¶ã€‚
- en: It essentially keeps this loop sort of in freeze dry eye modeã€‚ So it just freezes
    itã€‚ And lets the program go on with what it's doingã€‚ And then the next time this
    getsã€‚CalledIt goes right back to here and restarts the loops exactly where they
    were and returnsã€‚ returns itã€‚ These are the glove embedding so that we have those
    available so that we can turn those words into the 200 per wordã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåŸºæœ¬ä¸Šè®©è¿™ä¸ªå¾ªç¯ä¿æŒåœ¨å†·å†»å¹²ç‡¥æ¨¡å¼ã€‚æ‰€ä»¥å®ƒåªæ˜¯å†»ç»“å®ƒï¼Œå¹¶è®©ç¨‹åºç»§ç»­è¿›è¡Œå®ƒçš„å·¥ä½œã€‚ç„¶åä¸‹æ¬¡è¿™ä¸ªè¢«è°ƒç”¨æ—¶ï¼Œå®ƒä¼šå›åˆ°è¿™é‡Œï¼Œå¹¶ä»ä¸Šæ¬¡å¾ªç¯çš„ç¡®åˆ‡ä½ç½®é‡æ–°å¼€å§‹å¹¶è¿”å›ã€‚è¿”å›å®ƒã€‚è¿™äº›æ˜¯GloVeåµŒå…¥ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å•è¯è½¬æ¢ä¸ºæ¯ä¸ªå•è¯çš„200ä¸ªå€¼ã€‚
- en: You have to set that constantã€‚ You can't change that that is fixed by gloveã€‚
    And then we essentially just build the inputs for all of those caption words that
    we have and look them all upã€‚ And you can see then essentially the shape of thisã€‚
    So we're using a embedding layerã€‚ We have 1652 wordsã€‚ and each of those 1652 words
    has 200 elementsã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¿…é¡»è®¾ç½®é‚£ä¸ªå¸¸é‡ã€‚ä½ ä¸èƒ½æ”¹å˜å®ƒï¼Œå› ä¸ºå®ƒæ˜¯ç”±**GloVe**å›ºå®šçš„ã€‚ç„¶åæˆ‘ä»¬åŸºæœ¬ä¸Šåªæ˜¯æ„å»ºæ‰€æœ‰é‚£äº›æˆ‘ä»¬æ‹¥æœ‰çš„æ ‡é¢˜è¯çš„è¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬å…¨éƒ¨æŸ¥æ‰¾ã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™ä¸ªçš„åŸºæœ¬å½¢çŠ¶ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåµŒå…¥å±‚ã€‚æˆ‘ä»¬æœ‰1652ä¸ªå•è¯ï¼Œæ¯ä¸ªå•è¯éƒ½æœ‰200ä¸ªå…ƒç´ ã€‚
- en: Those 200 elements are the vectors that glove turns each of the words into so
    that similar words will be closer in vector spaceã€‚ This is using something called
    akira's embedding layerã€‚ We will learn more about this in the next module when
    you get an NLPã€‚ that these are greatã€‚ This lets you do this whole lookup inside
    of ks and let ks do it internallyã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™200ä¸ªå…ƒç´ æ˜¯GloVeå°†æ¯ä¸ªå•è¯è½¬åŒ–ä¸ºçš„å‘é‡ï¼Œå› æ­¤ç›¸ä¼¼çš„å•è¯åœ¨å‘é‡ç©ºé—´ä¸­ä¼šæ›´æ¥è¿‘ã€‚è¿™ä½¿ç”¨äº†ä¸€ç§å«åš**Akira**çš„åµŒå…¥å±‚ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªæ¨¡å—ä¸­å­¦ä¹ æ›´å¤šå…³äºè¿™ä¸ªçš„çŸ¥è¯†ï¼Œå½“ä½ å­¦ä¹ NLPæ—¶ã€‚è¿™äº›éå¸¸å¥½ã€‚è¿™è®©ä½ åœ¨å†…éƒ¨è¿›è¡Œæ•´ä¸ªæŸ¥æ‰¾ã€‚
- en: This is what the neural network looks likeã€‚ğŸ˜Šï¼ŒSo input1 is going to be your image
    input2 is that gradually increasing caption that you're going to send in each
    timeã€‚ This is where the embedding layer comes inã€‚ It uses the vocabulary size
    and it uses this embedding matrix that we had created that essentially becomes
    the weights of that layer we'll see that we transfer this in in a moment When
    we create it here we don't transfer it inã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç¥ç»ç½‘ç»œçš„æ ·å­ã€‚ğŸ˜Š è¾“å…¥1æ˜¯ä½ çš„å›¾åƒï¼Œè¾“å…¥2æ˜¯ä½ å°†æ¯æ¬¡å‘é€çš„é€æ¸å¢åŠ çš„æ ‡é¢˜ã€‚è¿™å°±æ˜¯åµŒå…¥å±‚çš„ä½œç”¨ã€‚å®ƒä½¿ç”¨è¯æ±‡é‡å¤§å°ï¼Œå¹¶åˆ©ç”¨æˆ‘ä»¬åˆ›å»ºçš„åµŒå…¥çŸ©é˜µï¼Œè¿™ä¸ªçŸ©é˜µåŸºæœ¬ä¸Šæˆä¸ºé‚£ä¸ªå±‚çš„æƒé‡ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°æˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºå®ƒæ—¶å¹¶æ²¡æœ‰è½¬ç§»å®ƒã€‚
- en: it's initialized with random weightsï¼Œ but it's essentially a matrix of this
    size so that it can do those lookups for youã€‚ it'll look up each of those words
    and put in the correct 200 valuesã€‚ we've got some dropout layers going on and
    we've got a 256 LSTM really very similar to the type of LSTM layer that we used
    for O text generation in the previous part we set up we basically add these to
    the neural network we had a final dense 256 layer and then the final output layer
    is going to be the vocab size because you've got dummy variables coming out ofã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç”¨éšæœºæƒé‡åˆå§‹åŒ–ï¼Œä½†åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªè¿™ç§å¤§å°çš„çŸ©é˜µï¼Œä»¥ä¾¿ä¸ºä½ æ‰§è¡Œé‚£äº›æŸ¥æ‰¾ã€‚å®ƒä¼šæŸ¥æ‰¾æ¯ä¸ªå•è¯å¹¶å¡«å…¥æ­£ç¡®çš„200ä¸ªå€¼ã€‚æˆ‘ä»¬æœ‰ä¸€äº›ä¸¢å¼ƒå±‚ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰256çš„LSTMï¼Œè¿™å®é™…ä¸Šéå¸¸ç±»ä¼¼äºæˆ‘ä»¬åœ¨å‰ä¸€éƒ¨åˆ†ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„LSTMå±‚ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šå°†è¿™äº›æ·»åŠ åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€ç»ˆçš„256å¯†é›†å±‚ï¼Œæœ€åçš„è¾“å‡ºå±‚å°†æ˜¯è¯æ±‡å¤§å°ï¼Œå› ä¸ºä½ æœ‰è™šæ‹Ÿå˜é‡è¾“å‡ºã€‚
- en: Essentiallyï¼Œ then we create a model so this is using the kas functional API
    so that we can have the inputsã€‚ we can have multiple inputsï¼Œ inputs one and inputs
    multiple inputs here when I'm talking about it is like maybe three pictures coming
    in or one picture in a caption or three pictures in a caption who knows however
    you want to set it upã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œè¿™ä½¿ç”¨çš„æ˜¯KerasåŠŸèƒ½æ€§APIï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æœ‰è¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥æœ‰å¤šä¸ªè¾“å…¥ï¼Œè¾“å…¥ä¸€ä¸ªå’Œå¤šä¸ªè¾“å…¥ï¼Œå½“æˆ‘è°ˆè®ºå®ƒæ—¶ï¼Œæ¯”å¦‚è¯´å¯èƒ½æœ‰ä¸‰å¼ å›¾ç‰‡è¿›æ¥ï¼Œæˆ–è€…ä¸€å¼ å›¾ç‰‡å’Œä¸€ä¸ªæ ‡é¢˜ï¼Œæˆ–è€…ä¸‰å¼ å›¾ç‰‡åŠ ä¸€ä¸ªæ ‡é¢˜ï¼Œéšä½ å¦‚ä½•è®¾ç½®ã€‚
- en: This is the summary of our network that I just describedã€‚ This is very importantã€‚
    This is where we're basically taking that embedding matrix from glove and putting
    that we are just putting that right into the weights of the neural networkã€‚ So
    we're overr the weights of the neural networkã€‚ and it becomes a lookup for usã€‚
    whenever it sees word 5ã€‚ for exampleï¼Œ itll go to the column rowã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘åˆšæ‰æè¿°çš„æˆ‘ä»¬ç½‘ç»œçš„æ‘˜è¦ã€‚è¿™éå¸¸é‡è¦ã€‚è¿™æ˜¯æˆ‘ä»¬åŸºæœ¬ä¸Šä»GloVeè·å–åµŒå…¥çŸ©é˜µå¹¶å°†å…¶ç›´æ¥æ”¾å…¥ç¥ç»ç½‘ç»œæƒé‡ä¸­çš„åœ°æ–¹ã€‚æ‰€ä»¥æˆ‘ä»¬è¦†ç›–äº†ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚å®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæŸ¥æ‰¾ï¼Œå½“å®ƒçœ‹åˆ°å•è¯5æ—¶ï¼Œä¾‹å¦‚ï¼Œå®ƒä¼šå»åˆ°åˆ—è¡Œã€‚
- en: depending on on the orientation that matrix and place that into the feature
    vectorã€‚ we can pile the entire neural network for categorical cross entropy because
    it is a classification neural networkã€‚ We're now going to train it the batch size
    of threeã€‚Do I got this from some of the original papersã€‚ We're basically doing
    itã€‚ So we have those 10 epochsã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®çŸ©é˜µçš„æ–¹å‘ï¼Œå°†å…¶æ”¾å…¥ç‰¹å¾å‘é‡ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºåˆ†ç±»äº¤å‰ç†µæ„å»ºæ•´ä¸ªç¥ç»ç½‘ç»œï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬ç°åœ¨å°†ä»¥æ‰¹é‡å¤§å°ä¸ºä¸‰è¿›è¡Œè®­ç»ƒã€‚è¿™æ˜¯æˆ‘ä»ä¸€äº›åŸå§‹è®ºæ–‡ä¸­è·å¾—çš„ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ­£åœ¨è¿›è¡Œè¿™ä¸ªã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰10ä¸ªè®­ç»ƒå‘¨æœŸã€‚
- en: We are going to do 20 epochs at this learning rate and then a final epochã€‚ we're
    sort of decreasing the learning rateã€‚ we could also use a scheduler for thisã€‚
    but this is pretty pretty straightforwardã€‚ Now we do save the neural networkã€‚
    If we see it already existsï¼Œ we just load itã€‚ we don't rebuild it because this
    part's going to take a bit of timeã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨è¿™ä¸ªå­¦ä¹ ç‡ä¸‹è¿›è¡Œ20ä¸ªè®­ç»ƒå‘¨æœŸï¼Œç„¶åæ˜¯æœ€åä¸€ä¸ªå‘¨æœŸã€‚æˆ‘ä»¬æ­£åœ¨å‡å°‘å­¦ä¹ ç‡ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸ºæ­¤ä½¿ç”¨è°ƒåº¦ç¨‹åºï¼Œä½†è¿™ç›¸å½“ç®€å•ã€‚ç°åœ¨æˆ‘ä»¬ä¿å­˜ç¥ç»ç½‘ç»œã€‚å¦‚æœæˆ‘ä»¬çœ‹åˆ°å®ƒå·²ç»å­˜åœ¨ï¼Œæˆ‘ä»¬åªéœ€åŠ è½½å®ƒã€‚æˆ‘ä»¬ä¸é‡å»ºå®ƒï¼Œå› ä¸ºè¿™éƒ¨åˆ†ä¼šèŠ±è´¹ä¸€äº›æ—¶é—´ã€‚
- en: Nowï¼Œ when we need to actually generate the captionï¼Œ this is the function that's
    going to do itã€‚ This is actually somewhat similar to the function that we had
    in the previous part where we were just generate random textã€‚ it's going to go
    in a range up to the max length that is going to build a sequence essentially
    just with the just with the start tag startingã€‚ it is going to pad the sequence
    because it has to go to the to the endã€‚ essentially with with zerosã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå½“æˆ‘ä»¬éœ€è¦å®é™…ç”Ÿæˆæ ‡é¢˜æ—¶ï¼Œè¿™å°±æ˜¯å°†è¦æ‰§è¡Œçš„å‡½æ•°ã€‚è¿™å®é™…ä¸Šä¸æˆ‘ä»¬åœ¨å‰é¢éƒ¨åˆ†ç”Ÿæˆéšæœºæ–‡æœ¬çš„å‡½æ•°æœ‰äº›ç›¸ä¼¼ã€‚å®ƒå°†æ„å»ºä¸€ä¸ªåºåˆ—ï¼Œé•¿åº¦è¾¾åˆ°æœ€å¤§å€¼ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯ä»èµ·å§‹æ ‡ç­¾å¼€å§‹ã€‚å®ƒä¼šç”¨é›¶å¡«å……åºåˆ—ï¼Œå› ä¸ºå¿…é¡»åˆ°è¾¾æœ«å°¾ã€‚
- en: We're going to request a predictionã€‚ Our max gets us of thoseã€‚Those predictionsã€‚
    which of them is has the highest probabilityï¼Œ because that's the word that we're
    going to addã€‚ Then we add a space to itã€‚ If we've gotten the stop tokenï¼Œ then
    we stop and we continueã€‚ and then finallyï¼Œ we split this out and return a textual
    string that tells us essentially what the caption wasã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¯·æ±‚ä¸€ä¸ªé¢„æµ‹ã€‚æˆ‘ä»¬çš„æœ€å¤§å€¼è·å–è¿™äº›é¢„æµ‹ä¸­æ¦‚ç‡æœ€é«˜çš„é‚£ä¸ªï¼Œå› ä¸ºé‚£å°±æ˜¯æˆ‘ä»¬è¦æ·»åŠ çš„å•è¯ã€‚ç„¶åæˆ‘ä»¬åŠ ä¸€ä¸ªç©ºæ ¼ã€‚å¦‚æœæˆ‘ä»¬å¾—åˆ°äº†åœæ­¢æ ‡è®°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±åœæ­¢å¹¶ç»§ç»­ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å…¶åˆ†ç¦»å¹¶è¿”å›ä¸€ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼Œå‘Šè¯‰æˆ‘ä»¬æ ‡é¢˜åŸºæœ¬ä¸Šæ˜¯ä»€ä¹ˆã€‚
- en: Now if we call this and evaluate itã€‚ These are some actual resultsã€‚ So you see
    these two people riding on a bike togetherã€‚ It says man and white shirt is standing
    by a woman in a blue hatã€‚ Okay closeã€‚ it had a decent idea what's going onã€‚ If
    we look at some more of theseï¼Œ another inside of hereã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¦‚æœæˆ‘ä»¬è°ƒç”¨è¿™ä¸ªå¹¶è¯„ä¼°å®ƒã€‚è¿™æ˜¯ä¸€äº›å®é™…ç»“æœã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™ä¸¤ä¸ªäººä¸€èµ·éª‘è‡ªè¡Œè½¦ã€‚å®ƒè¯´ï¼Œç©¿ç™½è¡¬è¡«çš„ç”·äººç«™åœ¨ä¸€ä¸ªæˆ´è“è‰²å¸½å­çš„å¥³äººæ—è¾¹ã€‚å¥½å§ï¼Œæ¥è¿‘ã€‚å®ƒå¯¹æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…æœ‰ä¸ªä¸é”™çš„æƒ³æ³•ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ›´å¤šè¿™äº›ï¼Œè¿™é‡Œè¿˜æœ‰å¦å¤–ä¸€ä¸ªã€‚
- en: There is a dog being barraged by tennis ballsã€‚ dog is chasing a ballã€‚ Okayï¼Œ
    yeahã€‚ he's kind of jumping at itã€‚ I'll buy thatã€‚ There' is a dog on concrete or
    maybe snowã€‚ Two dogs are running through grassã€‚ Okayï¼Œ new is a dogã€‚ black and
    white dogs running through the snowã€‚ This is the most spot on one that I've seen
    yet of the threeã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€åªç‹—è¢«ç½‘çƒè½°ç‚¸ã€‚ç‹—åœ¨è¿½é€ä¸€ä¸ªçƒã€‚å¥½å§ï¼Œæ˜¯çš„ã€‚å®ƒæœ‰ç‚¹è·³è·ƒç€å»è¿½ã€‚å¯ä»¥æ¥å—ã€‚æœ‰ä¸€åªç‹—åœ¨æ··å‡åœŸä¸Šæˆ–è€…å¯èƒ½æ˜¯åœ¨é›ªä¸Šã€‚ä¸¤åªç‹—åœ¨è‰åœ°ä¸Šå¥”è·‘ã€‚å¥½å§ï¼Œç°åœ¨æ˜¯ä¸€åªç‹—ã€‚é»‘ç™½ç‹—åœ¨é›ªä¸­å¥”è·‘ã€‚è¿™æ˜¯æˆ‘çœ‹åˆ°çš„ä¸‰ä¸ªä¸­æœ€å‡†ç¡®çš„ä¸€ä¸ªã€‚
- en: So that's very goodã€‚ Againï¼Œ these are not perfectã€‚Would have to expend considerably
    more training and probably get a bigger data set to really get these a lot a lot
    betterã€‚ Look like two guys walking with some strange graffiti on the groundã€‚ man
    in black coat is standing next to women in black jacket fairly closeã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¾ˆå¥½ã€‚å†è¯´ä¸€æ¬¡ï¼Œè¿™äº›å¹¶ä¸å®Œç¾ã€‚éœ€è¦æŠ•å…¥æ›´å¤šè®­ç»ƒï¼Œå¯èƒ½è¿˜è¦æ›´å¤§çš„æ•°æ®é›†ï¼Œæ‰èƒ½è®©è¿™äº›æ›´å¥½ã€‚çœ‹èµ·æ¥æœ‰ä¸¤ä¸ªå®¶ä¼™èµ°åœ¨åœ°ä¸Šï¼Œæ—è¾¹æœ‰ä¸€äº›å¥‡æ€ªçš„æ¶‚é¸¦ã€‚ç©¿é»‘è‰²å¤–å¥—çš„äººç«™åœ¨ç©¿é»‘è‰²å¤¹å…‹çš„å¥³äººæ—è¾¹ï¼Œè·ç¦»ç›¸å½“è¿‘ã€‚
- en: It wasn't figuring out the gendersã€‚ but it got the gender right on one guyã€‚
    two kids playing on a trampolineã€‚ little boy in red shirt is jumping off of a
    swingã€‚ I can get thatã€‚ maybe he's maybe there's a swing back thereã€‚ but I did
    rerun this a few times and it was picking up on the trampolineã€‚ So that's kind
    of neat that it can see that sometimes two women in a bikini near a shorelineã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ²¡æœ‰ææ¸…æ¥šæ€§åˆ«ï¼Œä½†åœ¨ä¸€ä¸ªç”·å­©èº«ä¸Šæå¯¹äº†æ€§åˆ«ã€‚ä¸¤ä¸ªå­©å­åœ¨è¹¦åºŠä¸Šç©ã€‚ç©¿çº¢è‰²è¡¬è¡«çš„å°ç”·å­©æ­£åœ¨ä»ç§‹åƒä¸Šè·³ä¸‹æ¥ã€‚æˆ‘å¯ä»¥ç†è§£ã€‚ä¹Ÿè®¸é‚£å„¿æœ‰ä¸ªç§‹åƒã€‚ä½†æˆ‘é‡æ–°è¿è¡Œäº†å‡ æ¬¡ï¼Œå®ƒç¡®å®æ³¨æ„åˆ°äº†è¹¦åºŠã€‚æ‰€ä»¥å®ƒèƒ½çœ‹è§è¿™äº›ä¸œè¥¿ï¼ŒçœŸä¸é”™ï¼Œæœ‰æ—¶æœ‰ä¸¤ä¸ªç©¿æ¯”åŸºå°¼çš„å¥³äººåœ¨æµ·å²¸çº¿é™„è¿‘ã€‚
- en: So group of young peopleã€‚ Okay I'll buy that are climbing up rock into the waterã€‚
    So you can see it's figuring out sees rocks it sees waterã€‚ It's neat that it's
    getting the grammar right into the waterã€‚ so I mean it's putting articles in front
    of things like it shouldã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç¾¤å¹´è½»äººã€‚å¥½å§ï¼Œæˆ‘ç›¸ä¿¡ä»–ä»¬æ­£åœ¨çˆ¬ä¸Šå²©çŸ³è¿›å…¥æ°´ä¸­ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°å®ƒæ­£åœ¨å¼„æ¸…æ¥šï¼Œçœ‹åˆ°å²©çŸ³ï¼Œçœ‹åˆ°äº†æ°´ã€‚å¾ˆæœ‰è¶£çš„æ˜¯å®ƒæŠŠè¯­æ³•å¼„å¯¹äº†ï¼Œè¿›å…¥æ°´ä¸­ã€‚æ‰€ä»¥æˆ‘æƒ³å®ƒåœ¨ä¸œè¥¿å‰åŠ äº†å† è¯ï¼Œåšå¾—å¾ˆå¥½ã€‚
- en: It's a dog dog is running through grassã€‚ So okay on these pictures that it wasã€‚Nowã€‚
    it wasn't trained with theseã€‚ These are from the same setã€‚ This is from the test
    setã€‚ If you evaluate them on some of my photosï¼Œ which are here from from Githubã€‚
    it doesn't do quite as wellã€‚ Nowï¼Œ this is kind of meanã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åªç‹—ï¼Œç‹—åœ¨è‰åœ°ä¸Šè·‘ã€‚æ‰€ä»¥å¥½å§ï¼Œè¿™äº›ç…§ç‰‡æ˜¯è¿™æ ·çš„ã€‚ç°åœ¨ã€‚å®ƒå¹¶æ²¡æœ‰ç”¨è¿™äº›è¿›è¡Œè®­ç»ƒã€‚è¿™äº›æ¥è‡ªåŒä¸€ç»„ã€‚è¿™æ˜¯æµ‹è¯•é›†ã€‚å¦‚æœä½ åœ¨æˆ‘ä¸€äº›ç…§ç‰‡ä¸Šè¯„ä¼°å®ƒä»¬ï¼Œè¿™äº›æ¥è‡ªGitHubï¼Œå®ƒçš„è¡¨ç°ä¸å¤ªå¥½ã€‚ç°åœ¨ï¼Œè¿™æœ‰ç‚¹æ®‹é…·ã€‚
- en: but this is what you do to test these things out and to show the limitationsã€‚
    I am standing next to this is at a university in Floridaã€‚ this is actually the
    university that I graduated with my doctorate thatã€‚ And this that's a tardisã€‚
    if you've ever watched Doctorã€‚ Hoï¼Œ that is that's a whole thing in Drã€‚ Hoï¼Œ But
    it's a phone boothã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å°±æ˜¯ä½ æµ‹è¯•è¿™äº›ä¸œè¥¿å¹¶å±•ç¤ºå±€é™æ€§çš„æ–¹æ³•ã€‚æˆ‘ç«™åœ¨è¿™é‡Œï¼Œè¿™æ˜¯ä½›ç½—é‡Œè¾¾çš„ä¸€æ‰€å¤§å­¦ã€‚å…¶å®è¿™æ˜¯æˆ‘è·å¾—åšå£«å­¦ä½çš„å¤§å­¦ã€‚é‚£æ˜¯ä¸ªå¡”è¿ªæ–¯ï¼ˆTARDISï¼‰ã€‚å¦‚æœä½ çœ‹è¿‡ã€Šç¥ç§˜åšå£«ã€‹ï¼Œé‚£å°±æ˜¯é‡Œé¢çš„ä¸œè¥¿ã€‚ä½†è¿™åªæ˜¯ä¸€ä¸ªç”µè¯äº­ã€‚
- en: probably didn't have any tardiss let maybe phone booths and it' training setã€‚
    So man in black shirtã€‚ it's actually a blue shirtï¼Œ but I blame the camera for
    thatã€‚ and jeansï¼Œ it's seen my jeansã€‚ So that's cool on the streetã€‚ Okayï¼Œ it probably
    thought these things usually occur on a streetã€‚ but it was actually inside of
    a buildingï¼Œ but that's actually pretty goodã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æ²¡æœ‰å¡”è¿ªæ–¯ï¼Œåªæœ‰ç”µè¯äº­åœ¨è®­ç»ƒé›†ä¸­ã€‚æ‰€ä»¥ç©¿é»‘è‰²è¡¬è¡«çš„äººã€‚å…¶å®æ˜¯è“è‰²è¡¬è¡«ï¼Œä½†æˆ‘æŠŠè´£ä»»æ¨ç»™äº†ç›¸æœºã€‚è¿˜æœ‰ç‰›ä»”è£¤ï¼Œæˆ‘çš„ç‰›ä»”è£¤å¯è§ã€‚æ‰€ä»¥åœ¨è¡—ä¸Šçœ‹èµ·æ¥å¾ˆé…·ã€‚å¥½å§ï¼Œå®ƒå¯èƒ½è®¤ä¸ºè¿™äº›äº‹æƒ…é€šå¸¸å‘ç”Ÿåœ¨è¡—ä¸Šã€‚ä½†å®é™…ä¸Šæ˜¯åœ¨å»ºç­‘ç‰©å†…éƒ¨ï¼Œä½†è¿™å®é™…ä¸Šè¿˜ä¸é”™ã€‚
- en: This is me sitting there man in black shirtã€‚Not even closeã€‚ and tieã€‚ I only
    wear a tie if I am forced to cast drinkã€‚ I amï¼Œ I'm not drinkingã€‚ This is my mother's
    dogã€‚ Two dogs are fighting in the grassã€‚ I don't know the dog maybe a split personality
    going on Now I noticed in the flickr data setã€‚ a lot of them had people doing
    actionsã€‚ There's no people in hereã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ååœ¨é‚£é‡Œçš„æ ·å­ï¼Œç©¿é»‘è‰²è¡¬è¡«çš„äººã€‚ç”šè‡³ä¸€ç‚¹ä¹Ÿä¸æ¥è¿‘ã€‚æˆ‘åªåœ¨è¢«è¿«å‡ºå¸­æ—¶æ‰ä¼šç³»é¢†å¸¦ã€‚æˆ‘ç°åœ¨ä¸å–é…’ã€‚è¿™æ˜¯æˆ‘æ¯äº²çš„ç‹—ã€‚ä¸¤åªç‹—åœ¨è‰åœ°ä¸Šæ‰“æ–—ã€‚æˆ‘ä¸çŸ¥é“é‚£åªç‹—ï¼Œä¹Ÿè®¸æ­£åœ¨ç»å†åŒé‡äººæ ¼ã€‚æˆ‘æ³¨æ„åˆ°åœ¨Flickræ•°æ®é›†ä¸­ï¼Œå¾ˆå¤šç…§ç‰‡éƒ½æœ‰äººçš„åŠ¨ä½œã€‚è¿™å„¿æ²¡æœ‰äººã€‚
- en: This is a bed and breakfast that my wife and I visitedï¼Œ and we just took a picture
    of itã€‚ So it's completely just a landscape shotã€‚ There's no workersã€‚ there are
    steps hereã€‚ So maybe this is my wifeï¼Œ I and my dog man in a red shirtã€‚ So it's
    talking about my wife who is not a manã€‚ She is a woman is sitting on a stool with
    his shoesã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å’Œå¦»å­å‚è§‚çš„ä¸€å®¶æ°‘å®¿ï¼Œæˆ‘ä»¬åˆšæ‹äº†å¼ ç…§ç‰‡ã€‚æ‰€ä»¥è¿™å®Œå…¨æ˜¯ä¸€å¹…é£æ™¯ç…§ã€‚æ²¡æœ‰å·¥ä½œäººå‘˜ã€‚è¿™å„¿æœ‰å°é˜¶ã€‚ä¹Ÿè®¸è¿™æ˜¯æˆ‘ã€æˆ‘çš„å¦»å­å’Œç©¿çº¢è‰²è¡¬è¡«çš„ç‹—ã€‚è¿™é‡Œè¯´çš„æ˜¯æˆ‘çš„å¦»å­ï¼Œå¥¹ä¸æ˜¯ç”·äººã€‚å¥¹æ˜¯ä¸ªå¥³äººï¼Œååœ¨å‡³å­ä¸Šï¼Œç©¿ç€ä»–çš„é‹å­ã€‚
- en: not even closeã€‚ This is me sitting onã€‚ I like this rockã€‚ It's at Washington
    Universityã€‚ I call it the github rock because it looks pretty similar to Github
    with all those green squaresã€‚ man in swim trunksã€‚ I am not in swim trunks in the
    middle War shoeã€‚ I would might lose my job is holding drink in his handã€‚ Yeah
    it really wants me to getã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿œè¿œä¸å¤Ÿã€‚è¿™æ˜¯æˆ‘åç€çš„æ ·å­ã€‚æˆ‘å–œæ¬¢è¿™å—çŸ³å¤´ã€‚å®ƒåœ¨åç››é¡¿å¤§å­¦ã€‚æˆ‘ç§°å®ƒä¸º GitHub å²©çŸ³ï¼Œå› ä¸ºå®ƒçœ‹èµ·æ¥å¾ˆåƒ GitHub çš„é‚£äº›ç»¿è‰²æ–¹å—ã€‚ç©¿ç€æ³³è£¤çš„äººã€‚æˆ‘å¹¶ä¸æ˜¯ç©¿ç€æ³³è£¤åœ¨æˆ˜äº‰é‹ä¸­é—´ã€‚æˆ‘å¯èƒ½ä¼šå› ä¸ºæ‰‹é‡Œæ‹¿ç€é¥®æ–™è€Œå¤±å»å·¥ä½œã€‚æ˜¯çš„ï¼Œå®ƒçœŸçš„è®©æˆ‘æƒ³è¦å¾—åˆ°ã€‚
- en: I am not in the middle of the university with swim trunks and a drink so anywayã€‚
    these are just trying it on some of the images so up to date on this course and
    other topics and artificial intelligenceã€‚![](img/2287d406430b06732c757e98a8dcb007_3.png)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¹¶ä¸æ˜¯ç©¿ç€æ³³è£¤ã€æ‰‹é‡Œæ‹¿ç€é¥®æ–™åœ¨å¤§å­¦çš„ä¸­é—´ï¼Œæ‰€ä»¥æ— è®ºå¦‚ä½•ã€‚è¿™åªæ˜¯å°è¯•åœ¨ä¸€äº›å›¾ç‰‡ä¸Šï¼Œæ‰€ä»¥åœ¨è¿™é—¨è¯¾ç¨‹å’Œå…¶ä»–ä¸»é¢˜åŠäººå·¥æ™ºèƒ½æ–¹é¢ä¿æŒæœ€æ–°ã€‚![](img/2287d406430b06732c757e98a8dcb007_3.png)
