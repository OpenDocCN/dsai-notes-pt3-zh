- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P33：L6.1- Sylvain的在线直播讲解 - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P33：L6.1- Sylvain的在线直播讲解 - ShowMeAI - BV1Jm4y1X7UL
- en: But yeah I think it's time to begin so welcome to thefi course and we're going
    to let you the chapter 1 together and this is really the place to ask any questions
    you might have so please do so in the chat I'm going to make some booth every
    five to 10 minutes and go through the chats read the question and answer them
    to the best of my knowledge。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 不过我觉得是时候开始了，欢迎来到这个课程，我们将一起学习第一章，这里是提问的最佳场所，请在聊天中提问，我会每五到十分钟查看一次聊天，阅读问题并尽量回答。
- en: So the urging for his course。And this session will go over chapter1 as I said。
    and this chapter is meant as a very general introduction to what transformer models
    can do so you don't need to worry about setting up be just yet a few good samples
    will to see how you can run them in coab directly。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个课程的目的如我所说，这一节将涵盖第一章，这一章旨在非常一般性地介绍变换模型的功能，所以你不需要担心目前的设置，几个好的样本将展示如何在coab中直接运行它们。
- en: Or the Greenface website。I'm not going to watch review the introduction video
    because you can do that on your own time and there's not really anything informative
    in it。 and so the goal of the first section of the course， which is probably right
    now。 which is this section in pink is to。Introduce you to transform models the
    web Strer went today and then teach you how to download the pretrain model from
    the Eb。 find unit on your own data for text classification tasks。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个是Greenface网站。我不会再看介绍视频，因为你可以在自己的时间里观看，视频内容没有什么信息。第一部分的目标，可能就是现在这个粉色部分，是为了向你介绍变换模型，今天的网络结构，并教你如何从Eb下载预训练模型，找出自己的数据以进行文本分类任务。
- en: And thenluute the result back to the model heb。And then see section 2 will dive
    a little bit deeper and consider all NLP tag can adjust text classification and
    when the last part of the course is going to dive even a little bit deeper。 so
    part two should be released in the fall and part three should be released at the
    beginning of next year。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将结果传回模型。在第二部分，我们将更深入探讨所有NLP任务如何调整文本分类，而课程的最后一部分将进一步深入。因此第二部分将在秋季发布，第三部分将在明年初发布。
- en: And so once you've finished this part of the course。 you should be able to download
    the between model Once you it on your own problem。 and then。Create either a smaller
    uploadlet the result back to the hub。 And so you should。Look at the forum。Which
    I'm gonna。Show you dressed here at discuss the phase。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了这部分课程，你应该能够下载该模型，并在自己的问题上进行使用。然后，创建一个较小的上传，结果将传回中心。因此你应该查看论坛，我将展示如何讨论这个阶段。
- en: go which is where you will be able to ask any question you have after this live
    stream in the course category there is one to big per chapter for all your questions
    and then there is also one to pick I'm going to follow very closely which is。So
    chair your projects topic on which you should definitely share anything you built
    after following the first section of the course。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将在这个直播结束后，可以在课程类别中提出任何问题，每章都有一个讨论区供你提问，并且我会密切关注另一个讨论区，所以请分享你的项目主题，确保分享在学习课程第一部分后所做的任何构建。
- en: 嗯。Alrighty。So let's dive in case I don't see any general questions for now with。The
    beginning of chapter1。So。Transformer models are all about doing NLP task andLP
    stands for natural language processing。 and it's a field that's related to everything
    language。So the the goal of NLP task is either to classify some text。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，好的。那么让我们深入探讨吧。如果现在没有任何一般性问题，我们就开始第一章。变换模型主要用于处理NLP任务，而NLP代表自然语言处理，这是一个与语言相关的领域。因此，NLP任务的目标是对一些文本进行分类。
- en: so for instance getting the sentiment of a review， detecting if an email is
    spam。 detecting if a comment someone posted online is rather nice or rather not
    nice if a sentence is grammatically correct。 etctera， etcter。Our task could be
    classifying edge word in the text， so for instance。 being able to pass the gramatic
    component to be able to see if that word is a person or a location or an organization。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，获取评论的情感，检测邮件是否为垃圾邮件，判断某个在线评论是友好还是不友好，判断句子是否语法正确，等等。我们的任务可以是对文本中的词汇进行分类，例如，能够解析语法成分，以判断这个词是人名、地点还是组织。
- en: Another type of NLP task is generated text content。 so completing a prompt which
    is what Jo smartF does when you're trying to compose a message。 usually it suggests
    using X and even Gmail does that now on email。嗯。Fillling the B syn text。 which
    is another kind of text generation。Another kind of task is to extract an answer
    from a text。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种NLP任务是生成文本内容。因此，完成一个提示是Jo smartF在你尝试撰写消息时所做的。通常它会建议使用X，甚至Gmail现在在邮件中也这样做。嗯。填充B
    syn文本，这是另一种文本生成。另一种任务是从文本中提取答案。
- en: so given a very long text and then a question being able a model。 being able
    to extract the answer to that model。The answer sort of that question。 sorry from
    the context or generating these new sentences from an input text。 So this is for
    instance， generating translation of a text summarizing a text I saw not over recently
    on making a new text in and of style。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定一个非常长的文本，然后一个问题，模型能够提取该问题的答案。答案是从上下文中提取，或根据输入文本生成这些新句子。例如，生成文本的翻译，总结我最近看到的一个新文本的内容。
- en: for instance， a casual style or more。嗯。Foral style。So those are all the kind
    of enLP testque tackle in the course， especially in section2。And so it's rather
    challenging because a computer doesn't process information the same way as we
    do。 And so a new transformer model let you deep learning to。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一种随意风格或更正式的风格。所以这些都是课程中，特别是第2部分将要处理的各种enLP任务。而且这相当具有挑战性，因为计算机处理信息的方式与我们不同。因此，一个新的变换模型让你可以深入学习。
- en: Are able to from like a few from a few samples that you have label for are able
    to generalize properly without you having to to create。Rs for instance， so before
    deep learning was all the rage。 one thing would be to pass the text and have some
    special rules if I see this world maybe it means its's positive if I see this
    world maybe it means it's negative etctera right。 so that's not what's done now
    this is mostly using transformable models with a again face library。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 能够从你有标签的几个样本中恰当地进行概括，而不需要你去创建。例如，在深度学习盛行之前，一件事情是解析文本并有一些特殊规则，如果我看到这个词，也许意味着这是积极的；如果我看到这个词，也许意味着这是消极的，等等。对吧。所以这不是现在所做的，主要是使用可变换模型和再次使用face库。
- en: And those models， you should， if you have not yet you should follow an introduction
    to a deep learning course。 those model are usually trained and they don't follow
    a set of given role that has been written by your human。 they have weights， which
    follow an algorithm called gradient descent and following the training data that
    are fed they make valueless function or ametric a little bit better at each step
    and so we get at the end a model that is kind of a black box。But we can use and
    generalize fairly well on data， but looks like the data on the training set。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型，如果你还没有，你应该跟随一个深度学习课程的介绍。这些模型通常是经过训练的，并不遵循人类编写的一组特定规则。它们有权重，遵循一种称为梯度下降的算法，并根据输入的训练数据，它们在每一步使损失函数或指标变得更好，因此我们最终得到的模型有点像一个黑匣子。但我们可以使用并在数据上进行良好的概括，但看起来与训练集的数据相似。
- en: So let's look at what this model can do in practice。 so there are two ways we'll
    be able to do that the first way is by clicking on this open in color button。So
    Collab is a platform maintained by Google which provides free access to resources
    like GPUs or TUs in a Jupy down the book environment。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看这个模型在实践中能做什么。因此，我们有两种方式可以做到这一点，第一种方式是点击这个“在颜色中打开”按钮。所以Collab是由谷歌维护的平台，提供免费的资源访问，例如在Jupyter
    Notebook环境中的GPU或TPU。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_1.png)'
- en: So， you can execute。Good directly。![](img/1b5e5fce582c60e42117fe3a72e59e6a_3.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可以直接执行。好。![](img/1b5e5fce582c60e42117fe3a72e59e6a_3.png)
- en: Inside it。And so the first cell needs to be executed to install all the library
    I'm going to let this run where we go through a little bit of the course and then
    we'll be able to run the other good samples I'm going to come back to that window
    a little bit later So transformers models are everywhere they are used by a lot
    of companies now those are examples of company that are using the Gface model
    app to have to share as models we have pretrained and probably to use them internally
    as well。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中。因此，第一个单元需要执行以安装我将要使用的所有库。我会让它运行，我们会稍微了解一下课程，然后我们就能运行其他好的样本，我稍后会回到那个窗口。所以变换模型无处不在，现在许多公司都在使用它们，这些是使用Gface模型的公司的示例，我们有预训练的模型，并可能在内部使用它们。
- en: And the Transformless library is the main interface library that provide access
    to those model。And the bottle up。Which we're going to have a quick look。And a
    little bit later。 the model app is where all post Sptrain model as taught is part
    of the evening phase ecosystem and we will be able to run the same code as wellll
    try in the Coab notebook on the that model app a little bit later。The thing you
    will need to do to access the forum and to be able to play around with all the
    models is create give a second following the link here on the 6 of the chapter
    1。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 库是提供访问这些模型的主要接口库。我们将快速浏览一下。稍后模型应用是所有后训练模型的所在，它是整个生态系统的一部分，我们将能够在稍后的模型应用中运行与在
    Coab 笔记本上尝试的相同代码。您需要做的事情是访问论坛并能够玩弄所有模型，点击章节 1 中的链接。
- en: And so we'll have a look first at the higher higher level API object。 which
    is called the pipeline plan function。![](img/1b5e5fce582c60e42117fe3a72e59e6a_5.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们首先来看更高层次的 API 对象，称为管道计划函数。![](img/1b5e5fce582c60e42117fe3a72e59e6a_5.png)
- en: I'm going just give a littleelps few minutes。Just going to give a few minutes
    for the video to both。For the video to load itself a little bit and take the time
    to answer any questions that you have let miss horse through the chat。I so， there
    were a few。So the first question I see is there still a place for LSTM models
    approaches like OMF。 if so where do they outperform transformer models or at least
    should be considered that's a very very good questions。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我会给大家几分钟时间。只是想给视频一些加载时间，并且回答你们在聊天中提出的任何问题。所以，我看到的第一个问题是，LSTM 模型（如 OMF）是否仍然有用？如果有，它们在哪些方面优于变压器模型，或者至少应该被考虑？这是一个非常好的问题。
- en: very good question right and there is definitely still a place for LSTM model。The
    main reason transformer models are heavily used right now is that the computation
    is more efficient especially on hardwarePUs and TUs。 because LSTMs rely on a recurrent
    mechanism and that recurt mechanism is a little bit harder to optimize but for
    instance when in fit as a very good state of the art results on the EMDB data
    set。 which is classifying movie reviews and it's been only very recently that
    it was performedform that transformers and that task in which I think is linked
    to the fact that the EMDB reviews very long and transformer models are good with
    input that go up to the sequence like we can manage。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 非常好的问题，确实仍然有 LSTM 模型的应用。当前变压器模型被广泛使用的主要原因是计算效率更高，尤其是在硬件 TPU 和 GPU 上，因为 LSTM
    依赖于递归机制，而这种机制的优化相对较难。不过，例如，LSTM 在 EMDB 数据集上取得了非常好的先进结果，该数据集用于电影评论分类。最近才发现变压器模型在该任务上的表现，我认为这与
    EMDB 评论非常长有关，而变压器模型在处理较长序列时表现良好。
- en: usually it's 512 but for 12 to sorry， but yeah if you have things that are longer
    than that it looks like LSTM models can see。It'll be a very good fit。Another question，
    I'm going to answer it just after this video because it's about the pipeline。So
    yeah， let's watch the video， I'm going to disappear from the screen so you can
    watch with me being in the way and I'll come back after the video。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，它是 512，但对于 12 来说，如果有超过这个长度的内容，似乎 LSTM 模型可以看到。这将是一个非常好的选择。另一个问题，我会在这个视频之后回答，因为它是关于管道的。所以，嗯，让我们先看视频，我会从屏幕上消失，这样你们可以不受干扰地观看，视频结束后我会回来。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_7.png)'
- en: 。It's pipe end function。The pipeline function is the most I level API of the
    Transformers library。It groupgroup together all the steps to go from moretex to
    usable predictions。The model used is at the core a pipeline， but the pipeline
    also include all the necessary prepoing since the model does not expect texts
    but number。As well as some post processing to make the output of the model human
    readable。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是管道结束函数。管道函数是 Transformers 库中最高层次的 API。它将所有步骤组合在一起，从文本到可用的预测。使用的模型本质上是一个管道，但管道还包含所有必要的预处理，因为模型并不期望文本，而是数字。同时还包括一些后处理，使模型的输出易于人类理解。
- en: Let's look at the first phase on Paul with the sentiment analysis pipeline。This
    pipeline performs text classification on a given input and determines if it's
    positive or negative。Here it attributed the positive label on the given text，
    with a confidence of 95%。You can pass multiple texts to the same pipeline， which
    will be processed and passed through the model together as a batch。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下保罗与情感分析管道的第一阶段。该管道对给定输入执行文本分类，并确定它是正面还是负面。这里它将正面标签归因于给定文本，置信度为 95%。你可以将多个文本传递给同一管道，这些文本将作为批处理一起处理并传递给模型。
- en: The output is a list of individual results in the same order as the input text。Here
    we find the same label on score for the first text。 and the second text is church
    negative with a confidence of 99。9%。The zero shot classification pipeline is a
    more general text classification pipeline。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是与输入文本顺序相同的个别结果列表。这里我们发现第一个文本的分数标签相同。第二个文本的标签是 church negative，置信度为 99.9%。零-shot
    分类管道是一种更通用的文本分类管道。
- en: It allows you to provide the labels you want。Here we want to classify our input
    text along the labels education。 politics， and business。The pipeline successfully
    recognizes it's more about education than the yoga labels。With a confidence of
    84%。Moving on to our tasks， the text generation pipeline will to complete a given
    prompt。The output is generated with a bit of randomness， so it changes each time
    you call the generator object on a given prompt。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它允许你提供想要的标签。这里我们想沿着教育、政治和商业的标签对输入文本进行分类。管道成功识别出它与教育标签的关系比与瑜伽标签更密切，置信度为 84%。接下来，我们的任务是文本生成管道，它将完成给定提示。输出是带有一点随机性的生成，因此每次在给定提示上调用生成器对象时都会有所不同。
- en: Up until now， we've used the Biman APIpi with a default model associated to
    each task。But you can use it with any model that has been freetrained or fine
    tune on this task。Oh。Going on the model H， again F dogo slash models。You can filter
    the available models by task。The defaultform adults used in our previous example
    was GPT2。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们一直使用与每个任务相关的默认模型的 Biman APIpi。但你可以使用任何已经经过自由训练或微调的模型。哦。继续使用模型 H，再次是 F
    dogo slash models。你可以按任务过滤可用模型。我们之前示例中使用的默认成人模型是 GPT2。
- en: but were many more models available and not just in English。Let's go back to
    the next generation pipeline and load it with another model， distill JPT2。This
    is a lighter version of GT2 created by the Eingface team。When applying the pipeline
    to a given prompt， we can specify several arguments。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有许多其他可用模型，而不仅仅是英文。让我们回到下一个生成管道，并用另一个模型 distill JPT2 加载它。这是由 Eingface 团队创建的
    GT2 的轻量版本。将管道应用于给定提示时，我们可以指定多个参数。
- en: Such as the maximum length of the generated text， all the number of sentences
    we want to return since there is some randomness in the generation。Generating
    text by guess the X12 in is intense， why the pertraining objective of GPT2。The
    field mass pipeline is a pering objective of Bro， which is to guess the value
    of mass qua。In this case， we ask the two most likely values for the missing words
    according to the model。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如生成文本的最大长度，以及我们想返回的句子数，因为生成中存在一些随机性。通过猜测 X12 生成文本是强度很大的，这也是 GPT2 的预训练目标。领域质量管道是
    Bro 的一个预训练目标，目的是猜测质量的值。在这种情况下，我们根据模型询问两个最可能缺失单词的值。
- en: and get mathematical or computational as possible answers。And a task transform
    most model than form is to classify each word in a sentence instead of the sentence
    as a wall。One example of this is named entity recognition。Which is the task of
    identifying entities such as persons。 organizations or locations in a sentence。Here，
    the model correctly finds the person， sva。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 并尽可能获得数学或计算方面的答案。任务转换模型的形式是对句子中的每个单词进行分类，而不是将整个句子作为一个整体。例如，这就是命名实体识别，它的任务是在句子中识别如人、组织或地点等实体。这里，模型正确识别了人，sva。
- en: the organization， Gface， as well as the location Brooklyn inside the input text。The
    group entities equal2 argument use is to make the pipeline group together the
    different walls linked to the same entity。 such as eggging and face here。novel
    task available with the byg API is extractive question and sorry。Providing a context
    and a question， the model will identify the span of text in the context containing
    the answer to the question。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 组织 Gface 以及输入文本中的布鲁克林位置。该组实体 equal2 参数的使用是将与同一实体相关的不同墙壁聚集在一起，例如这里的 eggging 和
    face。通过 byg API 提供的新任务是提取式问题和对不起。提供一个上下文和一个问题，模型将识别上下文中包含答案的文本范围。
- en: Getting short summaries of very longats is also something the Transers library
    can help with with the summarization bike。Finally， the last task supported by
    the B API is translation。Here we use a French English model found on the Mo hub
    to get the English version of our quick text。Here is a brief summary of all the
    tasks we've looked into in this video try them out through the influence switchts
    in the modern hub。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 获取非常长文本的简短摘要也是Transformers库可以帮助的，特别是在摘要方面。最后，B API支持的最后一个任务是翻译。在这里，我们使用一个法英模型，从Mo
    Hub找到，用于获取我们快速文本的英文版本。这里是我们在这个视频中研究的所有任务的简要总结，可以通过现代Hub中的影响切换进行尝试。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_9.png)'
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_10.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_10.png)'
- en: Yeah。Sorry about the song at the beginning so I'm trying to get better wrong
    A I can't have my head on at the same time I'm playing the video so that part
    seem to have been dealt with and for the fact that it's leggy and be angle a lot
    trying to find a solution but I don't know what it could be it so it could be
    a little bit better the next video I tried minimizing OS and footage could be
    better。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。抱歉一开始的歌曲，我试着让自己更好，我不能在播放视频的同时专心，所以那部分似乎已经处理过了，对于延迟的问题，我尝试找到解决方案，但我不知道可能是什么，所以下一个视频可能会更好，我试着最小化操作系统，画面可能会更好。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_12.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_12.png)'
- en: So the question I skipped before the video was when you do pipeline sentiment
    analysis。 how can you determine which model it is using the dog string doesn't
    help much。 Pat true its not in the documentation and we should probably make an
    effort to have it better document it the best way to do that is to inspect a source
    code for now。Which I'm going to show you in one second。So， you need to go。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我在视频前跳过的问题是，当你进行管道情感分析时，如何确定使用的是哪个模型，狗字符串并没有太大帮助。确实，它没有在文档中，我们可能应该努力使其更好地记录，最好的方法是检查源代码，现在我将给你展示这一点。所以，你需要继续。
- en: Into pipelines in it and that's a great question we actually make it more accessible
    and easier to see。 but you need to go in the source code in the pipelines module
    and in the in file and where you will see for each pipeline the default model
    that is used so here we are using the text classification and sentimentalizes
    have the same default as the model that was used is this still well based in case
    find you in SS2 English。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 进入管道，这是个很好的问题，实际上我们使其更易于访问和查看，但你需要进入管道模块的源代码，在文件中，你会看到每个管道所使用的默认模型，所以在这里我们使用文本分类和情感分析的默认模型是基于SS2英语的。
- en: Don't know why the person didn't pick a short name， but。That's the name of it。Let
    me see if there are any other questions。Yes， I'll send the links of the next video
    on the chat。 or you can follow on the on the course chapter at the same time and
    look at them directly in and of our brother。Okay， it looks like there are number
    of our questions， so let's continue。So this chapter。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不知道为什么那个人没有选择一个短名字，但这就是它的名字。让我看看还有没有其他问题。是的，我会在聊天中发送下一个视频的链接，或者你可以同时在课程章节中跟随并直接查看它们，好的，看起来有很多其他问题，所以让我们继续。那么这一章。
- en: So the rest of this section is just a good samples but shown in the video。 So
    I'm not going to go through that Instead， I'm going to look at my call up， which
    hopefully。Should as。![](img/1b5e5fce582c60e42117fe3a72e59e6a_14.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以本节的其余部分只是视频中展示的良好示例。所以我不打算详细介绍，而是要看看我的调用，希望。应该是。![](img/1b5e5fce582c60e42117fe3a72e59e6a_14.png)
- en: I need to restart own time， all lovely。![](img/1b5e5fce582c60e42117fe3a72e59e6a_16.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要重启自己的时间，所有都是美好的。![](img/1b5e5fce582c60e42117fe3a72e59e6a_16.png)
- en: So hopefully it's not going to take a long time to reinstall everything。好好好。Why
    are you being with me。Okay。So for could be a little bit smoother for you。So once
    we have installed the Transformers library， we can run the first cell， for instance。
    which add the same good as in the video and yeah。V is a book with the last version
    of To show which was just released now。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 希望重新安装所有内容不会花太长时间。好好好。你为什么要和我在一起？好的。这样对你会更顺畅一点。一旦我们安装了Transformers库，我们可以运行第一个单元，例如。与视频中的内容相同，没错。V是刚发布的最新版本的书。
- en: and so we can't run anything on Collab。So that's also lovely。Not sure if we
    can get this。you know your version somehow。あ。Okay， so hopefully the Pytoch works
    folks will fix the bug very soon and you will all be able to run all the collapse
    pretty easily in the meantime。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们无法在Collab上运行任何东西。这也很棒。不确定我们能否以某种方式获取你的版本。あ。好的，所以希望Pytoch的工作人员能尽快修复这个bug，你们都能在此期间轻松运行所有的collapse。
- en: I'm going to try to show you the same thing in a tripyter notebook and'm going
    where I'm not going do something that can run it。Ge。So let me just go。 I'm not
    gonna be here。So if you prefer winning notebooks locally。 everything is in the
    report called Notes inside the Eing F Or and with this notebook you have a course
    sub folder and when you have chapter by chapter or directly for the videos。So
    let's look。At the same notebook I was trying to run in Collab。Zoom in a little
    bit。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我将尝试在tripyter笔记本中向你展示同样的事情，我不打算做可以运行的事情。Ge。所以让我去。我不会在这里。所以如果你更喜欢在本地赢得笔记本。所有内容都在名为Notes的报告中，里面有一个Eing
    F Or，使用这个笔记本你有一个课程子文件夹，当你按章节或直接用于视频时。所以让我们看看。我们尝试在Collab中运行的同一个笔记本。放大一点。
- en: Let is done less， okay。So after running the install。 which I don't need to do
    because it's my have everything installed。 you can run the code that we were seeing
    on the video you can play around and put another sentence。 try with several sentences
    at the same time。Twice as there wrote classificationification pipeline and all
    the pipeline were saw in the video。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们减少一点，好吗。所以在运行安装后。我不需要这样做，因为我已经安装好一切。你可以运行我们在视频中看到的代码，可以随意更改，试试其他句子，同时尝试多个句子。两次那里写的分类管道，和我们在视频中看到的所有管道。
- en: 😊，As the next。The other way you could try all of that is model ever。嗯。spec ed。不得有你怎么样。诶。Where
    you can go so the model appears do co s models and where you can click on any
    model。 So for instance， if we go and the distill be based on case fine turn SS
    to English。 which we saw as a default model for the sentiment analysis pipeline。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，接下来。你可以尝试所有这些的另一种方式是模型。嗯。规范教育。不得有你怎么样。诶。你可以去的地方，所以模型会出现，你可以点击任何模型。例如，如果我们去，并将蒸馏基于案例好的转成英语。这是我们看到的情感分析管道的默认模型。
- en: we can go there and whereas a small wet where we can。Try it on any sentence。And
    gets the same results。So。You've got oh meta table。 so you've got at least three
    different ways of trying all the good samples。 So on the with the inference API
    and the website， sometimes when you were when you're trying a model you have a
    small progress by here while it's loading and then you can try it on b or sentences
    in a。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以去那里，在一个小湿地方，我们可以。在任何句子上试试。并得到相同的结果。所以。你有哦元表。所以你至少有三种不同的方法来尝试所有的好样本。在推理API和网站上，有时当你尝试一个模型时，你会看到小进度条在加载，然后你可以在b或句子中试试。
- en: So。Let's go。Back to the course。On the next section。About how transformer models
    work so transformer models are pretty recent。 So the architecture in itself was
    released in a paper in 2017 and then the first pretrain model was G。And was released
    by Open AI in June 2018， a second betweentrain model Westbro released by Google
    in October 2018 and then it kind of accelerated like here are just a few samples
    of model。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以。让我们。回到课程。下一部分。关于变换器模型如何工作的，变换器模型是相当新的。所以架构本身是在2017年的一篇论文中发布的，第一个预训练模型是G。由Open
    AI在2018年6月发布，第二个预训练模型Westbro在2018年10月由谷歌发布，然后它有点加速了，这里只是几个模型的示例。
- en: but there are many， many more。That were released and even though so we would
    need at least like to to have this image be twice or three times as big to be
    able to put every model that was released between 2019 and right now。It's really
    hard to follow the base at which everything is released。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有很多，很多更多。那些已经发布，尽管如此，我们至少需要这个图像变成两倍或三倍大，以便能够放下自2019年至今发布的每一个模型。真的很难跟上发布的基础。
- en: but the transformformers library as I think 60 different architectures now and
    is trying to add as soon as the papers are released。And so what we're going to
    look at here is not what's inside a transformable model in details。 in details，
    but a little bit of an overview and there are three types of transformer models
    which we're going to explore in this section。 so we are GT model GT like model
    that are called ons or autoregressive models。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为变压器库现在有大约60种不同的架构，并在论文发布后尽快添加新模型。因此我们将在这里讨论的不是变压器模型内部的细节，而是一个概览。在本节中，我们将探讨三种类型的变压器模型，包括被称为自回归模型的GT模型。
- en: Wwhichch are transformable models that are basically to generate text。 look
    so what we were talking about before， like when your phone is trying to suggest
    you ideas of words that you could put in the sentence afterward。The other type
    of transformer model is the be like model a GT like sorry it's not the noncodeders
    model it's a decoders model。 sorry autoregressive or decoders model for GT bet
    like its encodecoder model or autoencoing models。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变压器模型基本上是用于生成文本的。例如，当你的手机试图建议你可以在后续句子中使用的单词时，之前讨论的内容就是这种情况。另一种类型的变压器模型是类似于GT的模型，抱歉，不是编码器模型，而是解码器模型，抱歉，是自回归或解码器模型。
- en: so those models are best suited to classify things so generate a9 representation
    of the sentence which allows you to either classify the whole sentence。 classify
    every words in the sentence are they also very good for the extractive question
    and throwinging task we talked about earlier。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些模型最适合分类事物，生成句子的a9表示，使你能够分类整个句子或句子中的每个单词，它们在我们之前讨论的提取式问答任务中也非常有效。
- en: And the last type of model are the sequence to sequence transformer models are
    encode decos model。 which has the ones that are better suited for the sequence
    to sequence text such as translation。 sumorization， basically writing a new text
    from an input text。So all those transformer models are language models and the
    main difference between the first two types。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种模型是序列到序列的变压器模型，即编码解码模型，它更适合处理序列到序列的文本，如翻译、摘要，基本上是从输入文本写出新的文本。因此，所有这些变压器模型都是语言模型，而前两种类型之间的主要区别在于。
- en: the encodeders and the decoders is that the decoders are portrayed by kissing
    the next world。 so that's why we are very good at generating text because that's
    how they are betrayed。Whereas the en those models are usually betweened with feeling
    some random masks in the sentence。 so we have two examples here。And those transformformers
    models are super big so we started with like open AGTD with tests rather than
    tell millions of parameters and now we have models with like billions or tens
    of billions of parameters。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器的区别在于解码器通过预测下一个单词来进行操作。这就是我们在生成文本方面表现出色的原因，因为这就是它们的运作方式。而编码器模型通常是通过在句子中填充一些随机掩码来进行训练。因此我们这里有两个例子。这些变压器模型非常庞大，我们最开始的开放AGTD模型仅有数百万个参数，而现在我们有数十亿或数百亿个参数的模型。
- en: Which is why it's very important to reuse this model， which is the whole point
    of transfer learning。 so transfer learning is when you take your betweentrain
    model。 which is huge and between on lots and lots of amount of data which took
    a lot of compute so a lot of later bra animated a lot of CO2 and you reuse that
    model to find unit on a new tasks。 the task that you want to work with and。Yeah，
    so by reducing the model instead of training a new one from scratch。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么重用这个模型非常重要的原因，这就是迁移学习的全部意义。迁移学习是指你使用一个训练好的大模型，该模型在大量数据上进行训练，这需要消耗大量计算资源，排放了大量二氧化碳，然后重用该模型来解决新的任务。你希望处理的任务。因此，通过减少模型，而不是从头开始训练一个新的模型。
- en: you're saving compute， you're saving money and you need less data than what
    the model always be trained on。Let you see our new questions。Very one question
    is GT3 the only model that allows you to do zero short learning？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你节省了计算资源，节省了金钱，并且需要的数据比模型训练时所需的少。让我们来看一下我们新的问题。第一个问题是GT3是否是唯一允许零样本学习的模型？
- en: So that is how this model was advertised， it's not the only one because T5 for
    instance。 has the same kind of prompt where you tell it， for instance。 summarize
    this text or translate this text from this language to this language and so it
    can also do some sort of zero learning。And the link to the notebook but it was
    answered already So let's go and watch the transfer learning video I hope this
    one is going to be a little bit better than the previous one and or in the chat
    like will share the link if you want to watch it directly if it's too leg。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这个模型的推广方式，但并不是唯一的，因为 T5 也有类似的提示，例如让你总结这段文本或将这段文本从一种语言翻译成另一种语言，因此它也可以进行某种零学习。链接已经被回答过，所以让我们去看迁移学习的视频，希望这次比之前的要好一些，如果你想直接观看，我会在聊天中分享链接。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_18.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_18.png)'
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_19.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_19.png)'
- en: What is transfer learning？The idea of transfer learning is to leverage signal
    knowledge acquired by a model train with lotss of data on another task。The Model
    A will be trained specifically for task A。Now let's say you want to train Ad all
    B for a different task。One option would be to train the model from scratch。This
    could take lots of computation。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是迁移学习？迁移学习的理念是利用在另一个任务上通过大量数据训练的模型所获得的知识。模型 A 将专门为任务 A 进行训练。现在假设你想为不同的任务训练模型
    B，一个选择是从头开始训练模型。这可能需要大量计算。
- en: time and data。Instead， we could initialize Model B with the same weights as
    Model A。 transferring the knowledge of Model A on T P。When trend from scratch。
    all the middle's weights are initialized randomly。In this example。 we are training
    a belt model on the task of recognizing if two sentences are similar on it。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 时间和数据。相反，我们可以用与模型 A 相同的权重初始化模型 B，从而转移模型 A 在 T P 上的知识。当从头开始训练时，所有中间的权重都是随机初始化的。在这个例子中，我们正在训练一个模型，以识别两个句子是否相似。
- en: On the left， it's trained from scratch。And on the right， it's venting a proed
    model。As we can see。 using transfer learning on the preed model yields better
    results。And it doesn't matter if we train longer， so training from scratch is
    kept around 70% accuracy。 or the betweentrain would all beat the 86% easily。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是从头开始训练的模型，右侧是经过预训练的模型。可以看出，使用迁移学习的预训练模型产生了更好的结果。无论我们训练多长时间，从头训练的模型保持在 70%
    的准确率，而经过迁移学习的模型轻松超过 86%。
- en: This is because portraytrained models are usually trained on large amounts of
    data but provides a model with a statistical understanding of the language used
    during portraytraining。In computer visions， transfer learning has been applied
    successfully for almost 10 years。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为预训练模型通常在大量数据上进行训练，但为模型提供了对训练期间使用的语言的统计理解。在计算机视觉领域，迁移学习成功应用了近 10 年。
- en: Models are frequently proed on ImageNet by a asset containing 1。2 million of
    photo images。Each image is classified by one of 1000 levels。Training like this。
    unlabeled data is called supervised learning。In natural language processing。 transfer
    learning is a bit more recent。A key difference with ImageNet is that the training
    is usually self supervised。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型经常在 ImageNet 上经过训练，该数据集包含 120 万张照片。每张图像被分类为 1000 个级别之一。这样的训练方法被称为监督学习，而在自然语言处理领域，迁移学习则相对较新。与
    ImageNet 的一个关键区别在于，训练通常是自我监督的。
- en: Which means it doesn't require humanation for the labels。A very common portraying
    objective is to guess the next word in a sentence。Which only requires lots and
    lots of text。GPT2， for instance。 was retrieveed this way using the content of
    45 million links posted by users on web。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它不需要人工标注标签。一个非常常见的目标是预测句子中的下一个单词，这只需要大量文本。例如，GPT-2 就是通过使用用户在网络上发布的 4500
    万个链接的内容进行训练的。
- en: Another example of selfupvised per training objective is to predict the value
    of randomly massed quals。Which is similar to fit in blood B tests you may have
    done in school。Brt was between this way using the English Wikiped and had100 and
    published books。In practice。 transfer learning is applied on a given model by
    throwing away its head。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个自我监督预训练目标的例子是预测随机掩盖的单词。这类似于你在学校中可能做过的血液测试。这种方法使用了英文维基百科和 100 本已出版的书籍进行训练。在实践中，通过抛弃给定模型的头部来应用迁移学习。
- en: that is its last layers focused on the per training objective。And' with a new
    randomly initialized a suitable for the desk attempt。For instance。 when invented
    in the build model earlier， we remove the a that classified Mque and replaced
    it with a classifier with two outputposts since our task at two levels。To be as
    efficient as possible， the protrained model used should be as similar as possible
    to the task it's fine tune on。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其最后的层专注于预训练目标。并且使用一个新的随机初始化的适合任务的模型。例如，在之前构建模型时，我们移除了分类Mque的层，并用一个有两个输出的分类器替换，因为我们的任务在两个层面上。为了尽可能高效，使用的预训练模型应与其微调的任务尽可能相似。
- en: For instance， if the problem is to classify German sentences。It's best to use
    a German portraying model。But with the good comes to bad。 the proed model does
    not only transfer its knowledge， but also any bias it may contain。ImageNe mostly
    contains images coming from the United States and Western Europe。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果问题是对德语句子进行分类，最好使用一个德语模型。但好事与坏事相伴，预训练模型不仅转移了它的知识，还包括它可能包含的任何偏见。ImageNe主要包含来自美国和西欧的图像。
- en: so models fine tuned with it usually will perform better on images from these
    countries。But Beni also studied the bias in the prediction of its Gpyy3。 which
    was between using the guess and X world objectives。Changing the gender of the
    prone from E Westbury to she Westbury changed the predictions from mostly neutral
    objectives。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用微调模型通常会在这些国家的图像上表现更好。但是贝尼也研究了其**Gpyy3**预测中的偏差，涉及使用猜测和**X世界目标**。将**E Westbury**的性别从“他”更改为“她”会改变预测结果，主要为中立目标。
- en: To almost only physical ones。In the model code of the GT2 Mor。 Open AI also
    acknowledges its bias and discourages its use in systems that interact with humans。![](img/1b5e5fce582c60e42117fe3a72e59e6a_21.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎只针对物理模型。在**GT2 Mor**的模型代码中，OpenAI也承认其偏见，并不鼓励在与人类互动的系统中使用它。![](img/1b5e5fce582c60e42117fe3a72e59e6a_21.png)
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_22.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_22.png)'
- en: 嗯。![](img/1b5e5fce582c60e42117fe3a72e59e6a_24.png)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。![](img/1b5e5fce582c60e42117fe3a72e59e6a_24.png)
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_25.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_25.png)'
- en: Okay， so let me see if there are any questions on the video。😔，I don't think
    so。Okay。 so this was what transfer learning is very high level introductionions
    and this is what's usually done with transformerss model。And for instance， the
    model we used before distill B fine tune necess to English。 I'm forgetting alphat
    name， as the name indicates it was fine tuned on the62 dataset set which is a
    dataset set containing sentences and you have to classify them between positive
    and negative。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我看看视频上是否有任何问题。😔 我认为没有。好的，这就是迁移学习的非常高层次的介绍，这通常是用变换器模型完成的。例如，我们之前使用的模型**distill
    B**，它在**62**数据集上进行了微调，这个数据集包含句子，你需要将它们分类为正面或负面。
- en: And。Unlesss are are any new questions， I don't think so we're already to dive
    into the transformer architecture a little bit more so Omar will post the link
    of that video in the chat。 I hope it's not too leggy again， so this video is going
    to introduce as the difference between Odo decos and sequence to sequence transformer
    model we briefly talked about before。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 除非有新的问题，否则我认为我们已经可以稍微深入探讨变换器架构，因此奥马尔将在聊天中发布该视频的链接。我希望这次不会再卡顿，因此这个视频将介绍我们之前简要讨论过的**Odo解码器**与**序列到序列变换器模型**之间的区别。
- en: Let's study the transformer architecture。This video is the introductory video
    to the Ens。 decoders and Ender decoder series of videos。In the series。 we'll try
    to understand what makes a transformer network and we'll try to explain it in
    simple high level terms。No advanced understanding of neural networks is necessary。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们研究变换器架构。这个视频是关于**Ens解码器**和**Ender解码器**系列视频的介绍。在这一系列中，我们将尝试理解变换器网络的构成，并试图用简单的高层次术语进行解释。对神经网络的深入理解并不是必要的。
- en: but an understanding of basic vectors and tensors may help。To get started。 we'll
    take up this diagram from the original transformer paper entitledAttention is
    All you need by Vawaing it up。As we'll see here， we can leverage only some parts
    of it according to what we're trying to do。We want to dive into these specific
    layers， building up that architecture。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但对基本向量和张量的理解可能会有所帮助。为了开始，我们将引用原始变换器论文中的这一图表，题为**“注意力即一切”**，通过对其进行阐述。如我们所见，我们只能根据我们的目标利用其中的部分内容。我们希望深入探讨这些特定的层，构建出该架构。
- en: but we' try to understand the different ways this architecture can be used。Let's
    first start by splitting that architecture into two parts on the left you have
    the encoder and on the right。 the decoder。These two can be used together， but
    they can also be used independently。Let's understand how these work。The encoder
    accepts inputs that represent text。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将尝试理解这种架构的不同使用方式。让我们首先将架构分为两部分，左边是编码器，右边是解码器。这两者可以一起使用，但也可以独立使用。让我们理解它们是如何工作的。编码器接受表示文本的输入。
- en: it converts these text， these words into numerical representations。These numerical
    representations can also be called embeddings or features。We'll see that it uses
    the self attention mechanism as its main component。And we recommend you check
    out the video on encoders specifically to understand what is this numerical presentation
    as well as how it works。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 它将这些文本，这些词转换为数值表示。这些数值表示也可以称为嵌入或特征。我们将看到它使用自注意力机制作为其主要组件。我们建议你专门查看关于编码器的视频，以理解这种数值表示是什么以及它是如何运作的。
- en: 😊，We'll study the self attention mechanism in more detail， as well as its bidirectional
    properties。😊。The decoder is similar to the encoder， it can also accept text inputs。It
    uses a similar mechanism as the encoder， which is the masked self attention as
    well。It differs from the encoder due to its unititedirectional feature and is
    traditionally used in an autoregressive manner。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我们将更详细地研究自注意力机制及其双向特性。😊。解码器与编码器类似，它也可以接受文本输入。它使用与编码器相似的机制，即掩蔽自注意力。由于其单向特性，它与编码器有所不同，传统上以自回归方式使用。
- en: Here too， we recommend you check out the video on decoders。 especially to understand
    how all of this works。Combining the two parts results in what is known as an anchor
    decoder or a sequence to sequence transform。The encoder accepts inputs and computes
    a high level representation of those inputs。These outputs are then passed to the
    decoder。 The decoder uses the encoder's outputs and alongside other inputs to
    generate a prediction。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们也建议你查看关于解码器的视频，特别是为了理解这一切是如何运作的。将这两个部分结合起来，形成了所谓的锚解码器或序列到序列转换。编码器接受输入并计算这些输入的高级表示。这些输出随后传递给解码器。解码器使用编码器的输出和其他输入来生成预测。
- en: And then predict an output， which is will reuse in future iterations， hence
    the term autoregressive。Finally， to get an understanding of encoder decoders as
    a whole。 we recommend you check out the video on encoder decoder。![](img/1b5e5fce582c60e42117fe3a72e59e6a_27.png)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后预测一个输出，这将在未来的迭代中重用，因此称为自回归。最后，为了全面理解编码器解码器，我们建议你查看关于编码器解码器的视频。![](img/1b5e5fce582c60e42117fe3a72e59e6a_27.png)
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_28.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_28.png)'
- en: 。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_30.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_30.png)'
- en: Okay so that was the general introduction and so basically this is the diagram
    you have to remember about the general transformformers architecture and so what
    is an encode decoder or an encode decoder model we dive into that a little bit
    more just before going there we briefly talked about attention which was in the。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这就是一般介绍，基本上这是你需要记住的关于一般变换器架构的图表。那么什么是编码解码器或编码解码器模型呢？在深入了解之前，我们稍微讨论了注意力的内容。
- en: diagramia for the original paper， so the core of the architecture the transformer
    model is this layer called multied attention。 which is a layer that basically
    as indicates base attention。 so it's going to look at your wall sentence。sequence
    of input and for each word it's going to compute some score li to pay attention
    to this or this word of is a world so why is that it's because the transformer
    architecture was originally designed for translation and when you're translating
    if you're translating a given word you need that word but you need to understand
    the context around it so for instance you might need the gender of if the word
    are none you may need its gender which would be the word before you may need some
    words after。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文的图表，所以架构的核心，变换器模型是这个称为多头注意力的层。这一层基本上表明了基本注意力。因此，它将查看你的整个句子。输入序列中的每个词都将计算一个分数，以关注这个词或其他词。这是因为变换器架构最初是为翻译设计的，当你翻译一个特定的词时，你需要这个词，但你需要理解其周围的上下文。例如，你可能需要在词的性别上下文中，如果词是名词，你可能需要其性别，即前一个词，或者后面的某些词。
- en: So that attention layer is where on top of computing some contextual representation
    of a given world。 it's there to tell them all all for this specific word also
    in to this one in particular this world not so useful but this one you should
    really pay attention to it will have videos coming in section 2 and section 3
    of the calls that go into more details of what attention layer is but that's the
    I level1 introduction。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以注意力层是计算给定世界的一些上下文表示的地方。它用于告诉所有与这个特定单词相关的内容，尤其是这个单词，虽然这个世界不太有用，但这个你真的应该关注，它将在第2节和第3节中有视频，详细讲解注意力层是什么，但这就是我对第一层的介绍。
- en: And the key difference between encode codeos and decoders is thattting on codes
    as the attention mechanism is allowed to look at every word in the sentence。 So
    the word before and the word after， that's because like the bird model when you
    need to。Detect。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器之间的关键区别在于，编码器的注意力机制可以查看句子中的每个单词。因此，前面的单词和后面的单词，因为像鸟模型那样，当你需要检测时。
- en: sorry to guess what is the value of a mask word， it's useful to look at what
    was before and also what we after。The decoders models like GT have to predict
    the next world。 so if they were allowed to look at the word after it would be
    cheating。 so in those models the attention layer is allowed to look at what was
    before in the sentence so for instance when trying to guess silver as the attention
    layer can only look at my name and is they can't look at what is after in the
    sentence。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，要猜测一个掩码词的值，查看前面的内容以及后面的内容是有用的。解码器模型如GT必须预测下一个单词。所以如果它们被允许查看后面的单词，那将是作弊。因此在这些模型中，注意力层只允许查看句子中前面的内容。例如，当试图猜测“银”时，注意力层只能查看“我的名字”，而不能查看句子中的后面内容。
- en: Yeahそ。We are going to switch to the next。Section of this chapter on Ocuoss models，
    but before that。 let me see if there are any questions。On the decoder diagram。
    what does the output shifted right refer to， please。Let me check that diagram。This。
    so that's because。Here。We are training a translation model remember this is the
    original architecture of the transformer model so when you translate a phrase
    sentence in English and the output is a sentence in French so here the labels
    your this part of the transformer model is the decoder and it will try to guess
    the next word so it's going to begin with nothing and then when to try to begin
    the first word of the sentence then it will have the first word of the sentence
    to try begin to get the second word of the sentence then it will have the first
    word in the sentence to get the third word in the sentence。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们将切换到本章关于Ocuoss模型的下一节，但在此之前，让我看看是否有任何问题。在解码器图中，输出右移指的是什么，请问。让我检查一下那个图。这里。因为我们正在训练一个翻译模型，请记住这是变换器模型的原始架构，所以当你翻译一个英文短语句子时，输出是一个法语句子。因此，这里的标签你这个变换器模型的部分是解码器，它会尝试猜测下一个单词，所以它会从无开始，然后尝试开始句子的第一个单词，然后它将有句子的第一个单词来尝试获取句子的第二个单词，然后它将有句子的第一个单词来获取句子的第三个单词。
- en: etctera et cetera so that's where the shift right means it means that here we
    have the output so the text in the desired language shifted one token to the right。And
    another question was， can you provide a quick overview intuition of oh a digital
    version of a model like distber is able to maintain accuracy while being significantly
    sorry。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，所以右移的意思是我们有输出，即所需语言的文本右移一个标记。还有另一个问题，你能否提供一个快速的直观概述，说明数字版本的模型如distber如何在显著减轻的情况下保持准确性。
- en: more lightweight so quick overview intuition。Sure。In general。 transformer models
    or deep learningruning models have lots of parameters， several millions。 but a
    lot of those parameters are either redundant or not necessarily really useful
    and so for instance varies a lot of research into pruning transformers networks
    so pruning means removing some of those weights to be able to go faster especially
    at eachfer。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 快速概述直观。好的。一般来说，变换器模型或深度学习模型有很多参数，数百万个。但其中很多参数要么是冗余的，要么并不一定真正有用，因此例如有很多研究致力于修剪变换器网络，修剪意味着移除一些权重以便能够更快，尤其是在每个时刻。
- en: distillation is another way of reducing the size so the process is to have a
    smaller model try to get some output as a bigger model and it works reasonably
    well to still have some good performance。So that's because the intriition behind
    it is that a very big model as lots of parameters。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏是另一种减少模型大小的方法，其过程是让一个较小的模型尝试输出一个较大模型的结果，并且它的性能仍然相当不错。因此，这背后的直觉是，一个非常大的模型有很多参数。
- en: but not all of those parameters are really useful。And so let's dive into the
    encoders model so we have three more videos， encoders。 decos and then sequence
    to sequence model。![](img/1b5e5fce582c60e42117fe3a72e59e6a_32.png)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但并不是所有这些参数都真的有用。因此，让我们深入探讨编码器模型，我们还有三段视频，编码器、解码器，然后是序列到序列模型。![](img/1b5e5fce582c60e42117fe3a72e59e6a_32.png)
- en: We put it in full screen， and design。![](img/1b5e5fce582c60e42117fe3a72e59e6a_34.png)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其置于全屏并进行设计。![](img/1b5e5fce582c60e42117fe3a72e59e6a_34.png)
- en: In this video， we'll study the encoder architecture。An example of a popular
    encoder only architecture is Bt， which is the most popular model of its kind。Let's
    first start by understanding how it works。We'll use a small example using three
    words。 We use these as inputs and pass them through the encoder。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将研究编码器架构。一个流行的仅编码器架构的例子是Bt，这是同类中最受欢迎的模型。让我们首先了解它是如何工作的。我们将使用三个单词的小示例，将它们作为输入并传递给编码器。
- en: We retrieve a numerical representation of each word。Here， for example。 the encode
    converts those three words welcomee to NYC in these three sequences of numbers。The
    encode outputs exactly one sequence of numbers per input word。This numerical representation
    can also be called a feature vector or a feature tensor。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检索每个单词的数值表示。例如，这里的编码器将三个单词“欢迎”、“到”、“NYC”转换为这三个数字序列。编码器对每个输入单词输出恰好一个数字序列。这个数值表示也可以称为特征向量或特征张量。
- en: Let's dive in this representation。 It contains one vector per word that was
    passed through the encoder。Each of these vector is a numerical representation
    of the word in question。The dimension of that vector is defined by the architecture
    of the model for the base bird model。 it is 768。These representations contain
    the value of a word， but contextualized。For example。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这个表示。它包含一个经过编码器处理的每个单词的向量。每个向量是该单词的数值表示。该向量的维度由基础鸟模型的架构定义，维度为768。这些表示包含了一个单词的值，但具有上下文化。例如。
- en: the vector attributed to the word2 isn't the representation of only the two
    word。It also takes into account the words around it， which we call the context。As
    in it looks to the left context， the words on the left of the one we studying。
    hear the word welcome， and the context on the right， here the word NYC。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与单词2相关的向量不仅是两个单词的表示。它还考虑了周围的单词，我们称之为上下文。例如，它查看左侧上下文，即我们正在研究的单词左边的单词；听到单词“欢迎”，以及右侧的上下文，这里是单词“NYC”。
- en: and it outputs a value for the word given its context。It is therefore a contextualized
    value。😊。One could say that the vector of 768 values holds the meaning of the word
    within the text。It does this thanks to the self attention mechanism。The self attention
    mechanism relates to different positions or different words in a single sequence
    in order to compute a representation of that sequence。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它输出一个值，用于给定上下文中的单词。因此，这是一个上下文化的值。😊可以说，768个值的向量在文本中承载了单词的含义。它依赖于自注意力机制。自注意力机制与单个序列中的不同位置或不同单词相关，以便计算该序列的表示。
- en: As we've seen before， this means that the originalot representation of a word
    has been affected by other words in the sequence。We won't dive into these specifics
    here， but we' offer some further readings if you want to get a better understanding
    at what happens under the hood。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，这意味着一个单词的原始表示受到序列中其他单词的影响。我们在这里不会深入这些细节，但如果你想更好地理解幕后发生了什么，我们会提供一些进一步的阅读材料。
- en: So when should one use an encode？Encodeders can be used as tenderalone models
    in a wide variety of tasks。For example， Bert， arguably the most famous transformer
    model， is a standalone anchor model。 and at the time of release it be the state
    of the art in many sequence classification tasks。 question answering tasks， and
    masked language modeling to only cite a few。😊。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么时候应该使用编码器？编码器可以作为独立模型在多种任务中使用。例如，Bert，无疑是最著名的变换模型，是一个独立的锚定模型。在发布时，它在许多序列分类任务、问答任务和掩码语言建模中是最先进的，仅举几例。😊
- en: The idea is that encoders are very powerful at extracting vectors that carry
    meaningful information about a sequence。This vector can then be handled down the
    road by additional neurons to make sense of it。Let's take a look at some examples
    where encodes really shine。First of all。 mask language modeling or MLM。It's the
    task of predicting a hidden word and a sequence of word。Here。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是编码器在提取携带有意义信息的向量方面非常强大。这个向量可以通过额外的神经元进一步处理，以使其有意义。让我们来看一些编码器真正闪光的例子。首先是掩码语言建模或MLM。这是预测序列中隐藏词的任务。在这里。
- en: for example， we have hidden the word between my and is。This is one of the objectives
    with which Bert was trained。 It was trained to predict hidden words in a sequence。Encodes
    shine in this scenario in particular as bidirectional information is crucial here。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们隐藏了“my”和“is”之间的词。这是Bert训练的目标之一。它的训练目的是预测序列中的隐藏词。编码器在这种情况下特别出色，因为双向信息在这里至关重要。
- en: If we didn't have the words on the right， is it Silva and the dot。 then there
    is very little chance that Bt would have been able to identify name as the correct
    word。The encoder needs to have a good understanding of the sequence in order to
    predict a masked word。 as even if the text is grammatically correct。It does not
    necessarily make sense in the context of the sequence。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有右边的词，是Silva和点。那Bt几乎没有机会将名称识别为正确的词。编码器需要对序列有很好的理解，以便预测被掩盖的词，因为即使文本在语法上是正确的，也不一定在序列的上下文中有意义。
- en: As mentioned earlier， encodes are good at doing sequence classification。😊。Sentiment
    analysis is an example of sequence classification。The model's aim is to identify
    the sentiment of a sequence。It can range from giving a sequence a rating from
    one to five stars if doing review analysis to giving a positive or negative rating
    to a sequence。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，编码器擅长进行序列分类。😊情感分析就是序列分类的一个例子。模型的目标是识别序列的情感。它可以为序列评分，从一到五颗星，或者为序列给予正面或负面的评分。
- en: which is what is shown here。For example， here， given the two sequences。 we use
    the model to compute a prediction and to classify the sequences among these two
    classes。 positive and negative。While the two sequences are very similar containing
    the same words。 the meaning is entirely different， and the encoder model is able
    to grasp that difference。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这里所展示的。例如，给定这两个序列。我们使用模型来计算预测，并将这两个序列分类为正类和负类。虽然这两个序列非常相似，包含相同的单词，但含义完全不同，而编码模型能够把握这种差异。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_36.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_36.png)'
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_37.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_37.png)'
- en: 嗯。So that was it forcodo's model， some examples are Albertt， Bt， distill Bt，
    Elect， Roberta。Let's have a look at the next section， which is going to be about
    decoderos and just before that let me check if there are any questions。Don't see
    any new questions in the chat， don't forget that you can ask any questions in
    the chat that's kind of the point of having the live session。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。这就是forcodo模型，一些例子是Albertt、Bt、distill Bt、Elect、Roberta。我们来看看下一部分，将讨论解码器。在此之前，让我检查一下是否有任何问题。聊天中没有看到新的问题，别忘了你可以在聊天中提出任何问题，这正是直播会议的目的。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_39.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_39.png)'
- en: Yeah。![](img/1b5e5fce582c60e42117fe3a72e59e6a_41.png)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。![](img/1b5e5fce582c60e42117fe3a72e59e6a_41.png)
- en: In this video， we'll study the decoder architecture。An example of a popular
    decoder only architecture is GPT2。In order to understand how decoders work。 we
    recommend taking a look at the video regarding encoders， they're extremely similar
    to decoders。One can use a decoder for most of the same tasks as an encoder。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将研究解码器架构。一个流行的仅解码器架构的例子是GPT2。为了理解解码器是如何工作的，我们建议观看关于编码器的视频，它们与解码器非常相似。解码器可以用于大多数与编码器相同的任务。
- en: albeit with generally a little loss of performance。Let's take the same approach
    we have taken with the encoder to try and understand the architectural differences
    between an encoder and ID coder。We'll use a small example using three words。 We
    pass them through their decoder。We retrieve a numerical representation for each
    word。Here， for example。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通常性能会稍有损失。让我们采取与编码器相同的方法，尝试理解编码器和ID解码器之间的架构差异。我们将使用一个包含三个单词的小例子。我们将它们传递给解码器。我们为每个单词检索一个数值表示。在这里，例如。
- en: the decoder converts the three words welcomee to NYC and these are three sequences
    of numbers。The decoder outputs exactly one sequence of numbers per input word。This
    numerical representation can also be called a feature vector or a feature tensor。Let's
    dive in this representation。It contains one vector per word that was passed through
    the decoder。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器将“welcomee to NYC”这三个词转换为三个数字序列。解码器每个输入词准确输出一个数字序列。这个数值表示也可以称为特征向量或特征张量。让我们深入研究这个表示。它包含通过解码器传递的每个词的一个向量。
- en: Each of these vectors is a numerical representation of the word in question。😊。The
    dimension of that vector is defined by the architecture of the model。Whether a
    decoder differs from the encoder is principally with its self attention mechanism。
    it's using what is called masked self attention。Here， for example， if we focus
    on the word 2。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量中的每一个都是相关词的数值表示。😊该向量的维度由模型的架构决定。解码器与编码器的主要区别在于其自注意力机制。它使用被称为掩蔽自注意力的机制。在这里，例如，如果我们专注于词“2”。
- en: we'll see that this vector is absolutely unmodified by the NYC word。That's because
    all the words on the right， also known as the right context of the word is masked。Rather
    than benefiting from all the words on the left and right， so the bidirectional
    context。 decoders only have access to a single context。Which can be the left context
    or the right context。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会看到这个向量在纽约市的词汇中完全没有被修改。这是因为右侧的所有词语，也称为该词的右上下文，都被屏蔽了。因此，解码器并没有利用左右两侧的所有词语，也就是双向上下文。解码器只能访问单一的上下文，可以是左上下文或右上下文。
- en: The mask self attention mechanism differs from the self attention mechanism
    by using an additional mask to hide the context on either side of the word。The
    words numerical representation will not be affected by the words in the hidden
    context。So when should one use a decoder decoders like encoders can be used as
    standalone models。 as they generate a numerical representation， they can also
    be used in a wide variety of tasks。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 掩蔽自注意力机制与自注意力机制的不同之处在于使用了额外的掩蔽，以隐藏词语两侧的上下文。词语的数值表示不会受到隐藏上下文中词语的影响。那么，何时使用解码器呢？解码器可以像编码器一样作为独立模型使用。因为它们生成数值表示，也可以用于各种任务。
- en: however， the strength of a decoder lies in the way a word can only have access
    to its left context。Having only access to their left context， they are inherently
    good at text generationration。 the ability to generate a word or a sequence of
    words， given a known sequence of words。😊。This is known as causal language modeling
    or natural language generation。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解码器的强大之处在于词语只能访问其左侧的上下文。仅访问左侧上下文，使它们在文本生成方面具有天生的优势。给定一个已知的词序列，生成一个词或一系列词的能力。😊这被称为因果语言建模或自然语言生成。
- en: Here's an example of how causal language modeling works。 We start with an initial
    word， which is my。We use this as input for the decoder。😊，The model outputs a vector
    of numbers。 and this vector contains information about the sequence， which is
    here a single word。We apply a small transformation to that vector so that it maps
    to all the words known by the model。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个因果语言建模如何工作的例子。我们以初始词“my”开始。我们将其用作解码器的输入。😊，模型输出一个数字向量。这个向量包含有关序列的信息，这里是一个单词。我们对该向量应用一个小的变换，以使其映射到模型已知的所有词。
- en: which is a mapping that we'll see later called a language modeling head。We identify
    that the model believes that the most probable following word is name。We then
    take that new word and add it to the initial sequence。From my， we are now at my
    name。This is where the autoregressive aspect comes in。😊。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们稍后将看到的一个映射，称为语言模型头。我们识别出模型认为最可能的下一个词是“name”。然后，我们将这个新词添加到初始序列中。从“my”开始，我们现在得到了“my
    name”。这就是自回归特性的体现。😊
- en: Outtoregressive models reuse their past outputs as inputs and the following
    steps。Once again。 we do the exact same operation。We cast that sequence through
    the decoder and retrieve the most probable following word。In this case， it is
    the word is。We repeat the operation until we're satisfied。Starting from a single
    word， we've now generated a full sentence。We decided to stop there。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型将过去的输出重用为输入和后续步骤。再一次，我们执行完全相同的操作。我们通过解码器处理该序列，并检索出最可能的下一个词。在这种情况下，它是“is”这个词。我们重复这个操作，直到满意为止。从一个单词开始，我们现在生成了一个完整的句子。我们决定在此停止。
- en: but we could continue for a while。 GP T 2， for example， has a maximum context
    size of 1024。 We could eventually generate up to 1024 words， and the decoder would
    still have some memory of their first words and that sequence。😊，Yeah。게임。Let me
    look at some questions on the decoders model Oh one question is it possible to
    use larger sentences more than 512 words as inputs for encodes。 great question
    so it depends but most of the time no for instance spt as a maximum length of
    512 so you can't use larger sentences than that。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以继续一段时间。比如说，GPT-2 的最大上下文大小是 1024。我们最终可以生成多达 1024 个单词，而解码器仍然会记住它们的前几个单词和那个序列。😊，是的。游戏。让我看看关于解码器模型的一些问题。哦，有一个问题是，是否可以使用超过
    512 个单词的较长句子作为编码器的输入。好问题，这取决于情况，但大多数时候不行。例如，spt 的最大长度是 512，因此您不能使用比这更长的句子。
- en: Some newer models like Longformer， for instance， can accept a longer context。
    so you should look at the documentation， but first would be very specific on codes。
    like not all ons can do that。Another thing that you can do is split your sentence
    into several parts of 512 worlds and then if you get a representation。 you pass
    each of those chunk to the model so you get a representation for each of those
    checks and you can。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一些较新的模型，比如 Longformer，可以接受更长的上下文。因此，您应该查看文档，但首先要对代码非常具体，比如并不是所有的模型都能做到这一点。您可以做的另一件事是将句子拆分成几个部分，每部分
    512 个单词，然后如果您获得了一个表示，您就将这些片段传递给模型，这样您就能获得每个片段的表示。
- en: for instance， average where you get at the end to try to train a classifier
    for larger sentences。 but that's kind of the only way， either a specific model
    that has been trained to have longer inputs such as longform or splitting your
    inputs input bits。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您最终在尝试为较长的句子训练分类器时的平均情况。但这大概是唯一的方式，要么是经过特定训练的模型，可以处理较长的输入，比如 Longformer，要么是将输入拆分成多个部分。
- en: So another question， how does the mask language modeling objective deal with
    long gray worlds that end up being a multi doken world。 example sea planee with
    Pipe becomes。Ci。😊，AshAsh plane。 so you will see about that for those who are more
    beginners。 you will see about that separation in the next chapter with the Tokenals
    video。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，掩码语言建模目标如何处理长灰色单词，这最终会成为一个多令牌单词。例如，海上飞机与管道结合时会变成 Ci。😊，AshAsh plane。对于那些初学者，您将在下一个章节的视频中看到关于这种分离的内容。
- en: If not using wall wall masking then the model can cheat and see parts of the
    longer world。 so that is absolutely correct and that's why you will see that you
    have several versions of PE there is one that has been pretrained with wall wall
    masking and another one that has been trained without that it would be cheating
    for the ash ash plane because the model。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用全掩码，那么模型就会作弊，看到较长单词的部分。因此，这是绝对正确的，这就是为什么您会看到有多个版本的 PE，其中一个经过全掩码预训练，而另一个则没有。如果模型看到
    ash ash plane，那就会作弊。
- en: I mean it would be cheating where we're kissing C， sorry when you've got the
    ash ash plane in the context。 when you're trying to guess ash ash plane and you've
    got the shifts because in the context it's less cheating because the model has
    to guess that the model did not hand there。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，如果我们在上下文中猜测 ash ash plane，这就是作弊，抱歉，因为您有 ash ash plane 的上下文。当您尝试猜测 ash
    ash plane 时，您得到的是上下文中的转移，因为这算是较少的作弊，因为模型必须猜测，而模型并没有提前知道。
- en: but it's true that using world wall masking would remove that kind of specific
    bias and cheating we've seen。Weve the upper ris。Okay， so let's look at the last
    video and then I'll answer to more question after the last video on sequence to
    sequence transformable laws which combine both theionders and the decoders。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 但确实，使用全掩码会消除我们所看到的特定偏见和作弊情况。我们已经到了上层。好吧，让我们看看最后一个视频，然后我会在最后一个关于序列到序列的变换模型的视频中回答更多问题，这种模型结合了编码器和解码器。
- en: In this video， we'll study the encoder decoder architecture。An example of a
    popular encoder decoder model is T5。In order to understand how the encoder decoder
    works。 we recommend you check out the videos on encoders and decoders as a standalone
    models。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将研究编码器-解码器架构。一个流行的编码器-解码器模型的例子是 T5。为了理解编码器-解码器是如何工作的，我们建议您查看有关编码器和解码器的独立模型视频。
- en: Understanding how they work individually will help understanding how an encoder
    decoder works。Let's start from what we've seen about the encoder。The encoder takes
    words as inputs。 casts them through the encoder， and retrieves a numerical representation
    for each word casts through it。We now know that this numerical representation
    holds information about the meaning of the sequence。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 了解它们如何单独工作将有助于理解编码器-解码器的工作原理。让我们从我们对编码器的了解开始。编码器以单词作为输入，通过编码器进行处理，并为每个经过它的单词检索出数值表示。我们现在知道，这个数值表示包含了序列的意义信息。
- en: Let's put this aside and add the decoder to the diagram。In this scenario。 we're
    using the decoder in a manner that we haven't seen before。We're passing the outputs
    of the encoder directly to it。Additionally to the encoder outputs。 we also give
    the decoder a sequence。When prompting the decoder for an output with no initial
    sequence。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其搁置，并将解码器添加到图表中。在这种情况下，我们以一种前所未见的方式使用解码器。我们直接将编码器的输出传递给它。除了编码器的输出外，我们还给解码器一个序列。在没有初始序列的情况下提示解码器输出。
- en: we can give it the value that indicates the start of a sequence。😊。And that's
    where the anchor to decor magic happens。😊，The encoder accepts a sequence as input。It
    computes a prediction and outputs a numerical representation。😊。Then it sends that
    over to the decoder。It has， in a sense， encoded that sequence。And the decoder。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以给它一个值，以指示序列的开始。😊。而这就是解码魔法的锚点所在。😊，编码器将一个序列作为输入。它计算一个预测并输出一个数值表示。😊。然后将其发送给解码器。可以说，它已经对该序列进行了编码。而解码器。
- en: in turn， using this input alongside its usual sequence input will take a stab
    at decoding the sequence。The decoder dedes a sequence， and outputs a word。As of
    now。 we don't really need to make sense of that word， but we can understand that
    the decoder is essentially decoding what the encoder has output。The startup sequence
    here， the startup sequence word here indicates that it should start decoding the
    sequence。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，使用这个输入与它通常的序列输入一起，解码器将尝试解码这个序列。解码器解码一个序列并输出一个单词。到目前为止，我们并不需要理解这个单词，但我们可以理解解码器实际上是在解码编码器的输出。这里的启动序列，启动序列单词表明它应该开始解码这个序列。
- en: Now that we have both the encoder numerical representation and an initial generated
    word。 we don't need the encoder anymore。As we have seen before with the decoder。
    it can act in an autoregressive manner。The word it has just output can now be
    used as an input。This。 in combination with the numerical representation output
    by the encoder。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了编码器的数值表示和一个初始生成的单词。我们不再需要编码器。正如我们之前看到的，解码器可以以自回归的方式进行工作。它刚刚输出的单词现在可以用作输入。这与编码器输出的数值表示结合在一起。
- en: can now be used to generate a second word。Please note that the first word is
    still here as the model still outputs it。 however， we have grade it out as we
    have no need for it anymore。😊，We can continue on and on。 for example， on the decoder
    outputs a value that we consider a stopping value。 like a dot meaning the end
    of a sequence。Here we've seen the full mechanism of the encoder decoder transformer。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以用来生成第二个单词。请注意，第一个单词仍然在这里，因为模型仍然输出它。然而，我们将其标记为无效，因为我们不再需要它。😊，我们可以继续进行。例如，当解码器输出一个我们认为是停止值的值时，比如一个点，表示序列的结束。这里我们已经看到了编码器-解码器变换器的完整机制。
- en: let's go over one more time。 we have an initial sequence that is sent to the
    encoder。😊。That encoder output is then sent to the decoder for it to be decoded。While
    it can now discard the encoder after a single use。 the decoder will be used several
    times until we have generated every word that we need。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再复习一次。我们有一个初始序列发送给编码器。😊。该编码器的输出然后被发送给解码器进行解码。虽然在单次使用后可以丢弃编码器，但解码器将被多次使用，直到我们生成所需的每个单词。
- en: So let's see a concrete example with translation language modeling， also called
    transduction。 which is the act of translating a sequence。Here we would like to
    translate this English sequence We to NOIC in French。We're using a transformer
    model that is trained for that task explicitly。 we use the encoder to create a
    representation of the English sentence。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子，涉及翻译语言建模，也称为转导。这是将一个序列翻译的行为。在这里，我们想将这个英文序列"We"翻译成法语的"NOIC"。我们使用一个专门为此任务训练的变换器模型。我们使用编码器来创建英文句子的表示。
- en: we cast this to the decoder with the use of the start sequence word。 we ask
    it to output the first word。It outputs the avenue， which means' welcome。And we
    then use B avenue as the input sequence for the decoder。This alongside the encoder
    numerical representation， allows the decoder to predict the second word a。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其传递给解码器，使用开始序列词。我们要求它输出第一个词。它输出了“avenue”，这意味着“欢迎”。然后我们使用“B avenue”作为解码器的输入序列。这与编码器的数值表示相结合，使解码器能够预测第二个词“a”。
- en: which is two in English。😊，Finally， we ask the decoder to predict a third word，
    it predicts NYC。 which is correct， we've translated the sentence。Where the encoder
    decoder really shines is that we have an encoder and a decoder。 which often do
    not share weights。Therefore， we have an entire block。 the encoder that can be
    trained to understand the sequence and extract the relevant information。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这在英语中是两个。😊最后，我们要求解码器预测第三个词，它预测了NYC，这是正确的，我们已经翻译了句子。编码器-解码器真正闪光的地方在于我们有一个编码器和一个解码器，通常不共享权重。因此，我们有一个完整的块，编码器可以训练以理解序列并提取相关信息。
- en: For the translation scenario we've seen earlier， for example。 this would mean
    parsing and understanding what was said in the English language。It would mean
    extracting information from that language and putting all of that in a vector
    dense in information。😊，On the other hand， we have the decoder whose sole purpose
    is to decode the numerical representation output by the encoder。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前看到的翻译场景中，例如，这意味着解析和理解英语中所说的话。这意味着从该语言中提取信息，并将所有信息放入信息密集的向量中。😊另一方面，我们有解码器，其唯一目的是解码编码器输出的数值表示。
- en: This decoder can be specialized in a completely different language or even modality
    like images or speech。😊，Encos， decoders are special for several reasons。Firstly。
    theyre able to manage sequence to sequence tasks like translation that we have
    just seen。Secondly。 the weights between the encoder and the decoder parts are
    not necessarily shared。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解码器可以专门用于完全不同的语言，甚至是图像或语音等模态。😊编码器和解码器在几个方面是特殊的。首先，它们能够处理像我们刚刚看到的翻译这样的序列到序列任务。其次，编码器和解码器部分之间的权重不一定是共享的。
- en: Let's take another example of translation。Here where translating transformers
    are powerful in French。Firstly， this means that from a sequence of three words。
    we're able to generate a sequence of four words。One could argue that this could
    be handled with a decoder that would generate the translation in an autoregressive
    manner。And they would be right。Another example of where sequence to sequence transformers
    shine is in summarization。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个翻译的例子。在这里，翻译变换器在法语中非常强大。首先，这意味着从三个词的序列中，我们能够生成四个词的序列。可以说，这可以由一个以自回归方式生成翻译的解码器来处理。他们是对的。另一个序列到序列变换器发光的例子是摘要。
- en: 😊，Here we have very very long sequence， generally a full text， and we want to
    summarize it。Since the encoder and decoders are separated， we can have different
    context lengths， for example。 a very long context for the encoder which handles
    the text and a smaller context for the decoder。 which handles the summarized sequence。There
    are a lot of sequence to sequence models。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 😊在这里我们有非常非常长的序列，通常是一整篇文本，我们想要总结它。由于编码器和解码器是分开的，我们可以有不同的上下文长度，例如。编码器处理文本的非常长的上下文和解码器处理摘要序列的小上下文。有很多序列到序列模型。
- en: This contains a few examples of popular encoder decoder models available in
    the Transformers library。Additionally， you can load an encoder and a decoder inside
    an encoder decoder model。😊，Therefore。 according to the specific task you're targeting，
    you may choose to use specific encoders and decoders which have proven their worth
    on these specific tasks。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这包含了一些在Transformers库中可用的流行编码器-解码器模型的示例。此外，你可以在编码器-解码器模型中加载一个编码器和一个解码器。😊因此，根据你针对的具体任务，你可以选择在这些特定任务中已证明其价值的特定编码器和解码器。
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_43.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_43.png)'
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_44.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_44.png)'
- en: Yeah。Okay， so let's see if we have any questions。Oh， okay。 is pararaing kind
    of decodder problem or both encodeder decodder problem？
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。好的，那我们来看看有没有问题。哦，好的。是段落解码器问题还是编码器解码器问题？
- en: And is there any pipeline available for same？So paraphrasing so generating a
    similar task to the input will be more like an encode or decoder pronym。 however，
    it seems that it's going to be very hard to train a model on that task specifically
    because it's going to have a tendency to want to include the same thing it received。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有任何可用的管道来实现同样的功能？所以改写生成与输入类似的任务，更像是一个编码器或解码器原型。然而，似乎在这个特定任务上训练模型会非常困难，因为它倾向于想要包含接收到的相同内容。
- en: Unless you want to parase in another style， for instance。 going from formal
    style to very casual style。There is no pipeline available provide that desk out
    of the books in the transformformers library。So let's go to the next section of
    Cha1， which is the last section of the chapter and talk a little bit about bias
    and limitations of the transformer models。So like any deep learning model in general
    transformers are just one specific case。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你想以另一种风格进行改写，例如从正式风格到非常随意的风格。目前在变换器库中没有提供实现该任务的管道。所以让我们进入 Cha1 的下一部分，这是这一章的最后一部分，谈谈变换器模型的偏见和局限性。像任何深度学习模型一样，变换器只是一个特定的案例。
- en: but it's really a problem for all kind of deeping model so there are powerful
    tools and as you know probably if you take a deep learning introduction course
    via powerful tools but you don't really control or a get from the input to the
    output this was more controlled by the training they cut and the training data
    they receive but if you don't take any precautions you can have those models make
    predictions that you don't necessarily want in the app deploy so for instance。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 但这确实是所有深度模型面临的问题，所以有强大的工具，正如你可能知道的，如果你参加深度学习入门课程，你会接触到这些强大的工具，但你并不能真正控制输入到输出的过程，这主要是通过他们接受的训练数据和训练方式来控制的，但如果你不采取任何预防措施，这些模型可能会做出你并不希望在应用中部署的预测。
- en: let's just have a very quick look at what B which has been retrained on the
    objective of thin maskQu and just have a similar example to the one we saw in
    the transfer learning video with GPT where we just change the gender of the sentence
    so if you put this man works as a。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下经过重新训练的 B 模型，其目标是 thin maskQu，并且有一个类似于我们在 GPT 的迁移学习视频中看到的例子，我们只需改变句子的性别，所以如果你说“这个男人作为一个工作”。
- en: Cass the word or this woman man works as a guess the word for man。 we get like
    some kind of neutral jobs。Probably moree stere stetype， stereotypically male，
    for women。 we get very stereotypically female jobs and prostitute。 which is necessarily
    something we would want our model to output as one of the top five possibilities。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 猜这个词或“这个女人作为一个”猜“男人”的词。我们得到一些中性的工作。可能更符合刻板印象的是男性的工作，而女性则得到非常刻板印象的女性工作，甚至是性工作者。这显然不是我们希望模型输出的前五个可能性之一。
- en: So and that model was not trained on data but was particularly flagged as problematic。
    more like usually considered neutral， it's only Wikipedia and a corus of un publishedlish
    book。So you have to be like GBT， for instance， is more known because it's been
    trained of kind of the wall of the internet according to editit users。 which is
    considered a little bit more sexist or。Oxyenophobic or thing like that so you
    have to be very careful because that bias is in the pretrained model so this is
    done with the bird paste and case checkpoint specifically and completely reproducible
    because there is no randomness in the prediction so if you're go a notebook on
    tris you're going to get those results。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以该模型并不是在数据上训练的，但被特别标记为有问题的。更像是通常被认为是中立的，只有维基百科和一些未出版的书籍。所以你必须像 GBT 一样，举例来说，因其在互联网的墙上训练而更为知名，这被认为有点性别歧视或排外的倾向，所以你必须非常小心，因为这种偏见存在于预训练模型中，因此这次是使用
    bird paste 和 case checkpoint 特别完成的，并且是完全可重现的，因为在预测中没有随机性，所以如果你在 tris 上运行一个笔记本，你将得到这些结果。
- en: So this is present in any model in some way or novel and it's also going to
    persist after your fine tu。 so you have always have to be super careful that when
    you fine your model in your training data there are going to be enough samples
    of the outputs you'd like to see and you should always roll out your model into
    production by taking some care to analyze the results that you get and potentially
    if you see that you have some predictions you would like to avoid try to correct
    your training data to add more samples。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在某种程度上，这在任何模型中都是存在的，或者说是新的，并且在你的微调后也会持续存在。因此你总是必须非常小心，当你在训练数据中微调模型时，必须确保你希望看到的输出有足够的样本，并且你应该始终在将模型投入生产时认真分析你得到的结果，如果你看到一些预测你希望避免的，尝试纠正你的训练数据以添加更多样本。
- en: To add more samples that would correct the bias of your model。And that's it
    for chapter one。 let me see if there are more questions。In the chat otherwise
    we're going to be ready to tune out so once you have finished this video don't
    hesitate to take a quiz I'm not going to do that on the video to be sure that
    you understood all the terms that were all the content sorry but we saw in this
    chapter there are two left sessions。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多样本以纠正你模型的偏差。这就是第一章的内容。让我看看还有没有更多问题。在聊天中，否则我们就准备结束了，所以一旦你完成了这个视频，不要犹豫，去做个测验，我不会在视频中进行，以确保你理解了所有术语和内容，但我们在这一章中看到的还有两个左侧会议。
- en: That you can find again on the forum。 This is not the forum anymore。There are
    two live session next week for Cha 2， be sure to try and wide on the other so
    there's going to be one with me and next Wednesday hopefully all the technical
    issues will be resolved because I'm going to take a little bit of care to make
    sure it works a little bit better especially for the videos and so。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在论坛上再次找到。这已经不是论坛了。下周有两个关于第2章的直播会议，请务必尝试参与，另外一个会是我，期待下周三希望所有技术问题都能得到解决，因为我会稍微关注一下，确保它运作得更好，尤其是视频方面。
- en: The first one is going to be with Lewis on morning European time on Wednesday
    and the second one is going to be on next first day at the same time as this one。Take
    the time to create an account there if hub to play with the hopefully the bug
    in codeab is going to be sold very soon and you can play with also code on that
    platform。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个会议将是与Lewis在欧洲时间周三的早晨，第二个会议将在下周四的同一时间进行。请花时间在那里创建一个账户，如果hub上有问题，希望codeab的bug能很快解决，你也可以在那个平台上进行代码实验。
- en: otherwise you can always use。The inference API on the website and also think
    about a project that you'd like to run。And to try your skills on as you learn
    more using the course。 so of anything related to text classification， because
    that's what will focus in this part of the course。 but try to think of yeah what
    kind of model you would like to build doing something that you could then share
    with the community and then in the next fews will help you with the course to
    do that in practice。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 否则你总是可以使用网站上的推理API，并考虑一个你想要运行的项目。在你学习课程的过程中，尝试你的技能，因此与文本分类相关的任何内容，因为这部分课程将集中于此。但试着想想你想构建什么样的模型，做一些你可以分享给社区的事情，然后在接下来的几周内，这将帮助你在实践中完成课程。
- en: Let me check when last time we got questions。And。Yeah， I think that's kind of
    whole。Okay。 thanks a lot for following and see you next week。![](img/1b5e5fce582c60e42117fe3a72e59e6a_46.png)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我检查一下上次我们有什么问题。嗯，我想这大概就是全部了。好的，非常感谢你的关注，下周见！![](img/1b5e5fce582c60e42117fe3a72e59e6a_46.png)
- en: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_47.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b5e5fce582c60e42117fe3a72e59e6a_47.png)'
- en: 嗯。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。
