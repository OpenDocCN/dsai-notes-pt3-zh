- en: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P12：L12- TensorFlow
    数据集 - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P12：L12- TensorFlow
    数据集 - ShowMeAI - BV1em4y1U7ib
- en: In this video， we're going to look at Tensorflow data sets and how we can load
    different data sets。 do preprocessing and load the data efficiently。![](img/7bf38e208736fe558252a86bf7cde851_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将查看 TensorFlow 数据集，以及如何加载不同的数据集，进行预处理并高效加载数据。![](img/7bf38e208736fe558252a86bf7cde851_1.png)
- en: '![](img/7bf38e208736fe558252a86bf7cde851_2.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bf38e208736fe558252a86bf7cde851_2.png)'
- en: Alright so here are the dataset sets that are available through through Ka datasets。
    which is what we've been primarily working with so far。 and as you can see it's
    relatively few right we've been working with Cycipher 10 and MNist and if we now
    just look at the comparison of Tensorflow data as we can see here we have a bunch
    more dataset so we have dataset sets for audio。 images， image I don't know really
    difference between image image classification， object detection。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，下面是通过 Ka datasets 提供的数据集，这也是我们到目前为止主要使用的。正如你所看到的，数据集相对较少，我们一直在使用 Cycipher
    10 和 MNist。如果我们现在只看 TensorFlow 数据的比较，如此可见，我们有许多其他数据集，包括音频、图像、图像分类和目标检测。
- en: question answering， structure， summarization text。Translate video。 I mean。 there
    are so many dataset sets on this one。 So it makes sense for us to learn it。 and
    it also becomes so we can also load the data extremely efficiently。 Now I feel
    it's important to mention that what we're going to work with isn't the same as
    Tf do data。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 问答、结构、文本摘要、视频翻译。我的意思是，这个数据集上有很多种类。因此我们学习它是有意义的，同时也使我们能够极高效地加载数据。现在我觉得有必要提到的是，我们将要处理的内容与
    Tf 数据并不相同。
- en: So Tensorflow datas makes it easy to load datas and it uses Tf data。 But if
    you have。 let's say custom data you want to load you're going to want to use Tf
    data。 So Tensorflow datas is a highlevel wrapper intended to make it very easy
    to load commonly used datas。 And in future videos， we're gonna to look at how
    to use Tf do data to build an input pipeline for data sets that perhaps you've
    gathered on your own or scrape from the Internet or something like that。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 数据使加载数据变得简单，它使用 Tf 数据。但是，如果你有自定义数据需要加载，那么你需要使用 Tf 数据。因此，TensorFlow
    数据是一个高层封装器，旨在使加载常用数据变得非常简单。在接下来的视频中，我们将看看如何使用 Tf 数据来构建数据集的输入管道，或许是你自己收集的，或是从互联网上抓取的等等。
- en: with that set， there's going to be similarities since Tf datas is a wrapper
    for Tf data。 So what we learn in this video is still going to be useful for when
    we load our own custom datas。![](img/7bf38e208736fe558252a86bf7cde851_4.png)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，会有一些相似性，因为 Tf 数据是 Tf 数据的封装器。因此我们在这个视频中学到的内容在加载自定义数据时仍然会有用。![](img/7bf38e208736fe558252a86bf7cde851_4.png)
- en: Alright， enough talking。 So what we're gonna do now is actually take a look
    at the code。 So what you have right here is just the standard imports we've been
    using。 There are。 I guess this one is new and this one from import Tensorflow
    datas as TFDS。 that's new。 So if you don't have those， you can Google how to download
    them for So for this one is just Pip install Tensorflow datas。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，够了，不再多说了。现在我们要做的是看看代码。你现在看到的就是我们一直在使用的标准导入。这个是新的，来自 TensorFlow 数据的导入 TFDS，这个是新的。如果你没有这些，可以搜索一下如何下载。所以对于这个，只需执行
    Pip install TensorFlow 数据。
- en: therere also gonna be in the description for the links for annaconda。 and then
    also the command for Pip。All right， so when you've downloaded that。 what we're
    going to do is。We're gonna first just load the Ms data set。 So we've been working
    a lot with Ms and we're all getting tired of Ms I think。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 说明中也会提供 Anaconda 的链接，以及 Pip 的命令。好的，当你下载完毕后，我们要做的是先加载 Ms 数据集。我们在 Ms 上工作了很多，我想我们都感到厌倦了。
- en: but this is really just for a simple use case of how it actually works to load
    it。 So what we're going do is we're going to do DS train DS test So we're going
    to get from this Tensorful data sets。 we're gonna to get a data set for training，
    a data set for test。 And then we're also going get some DSS info about the dataset。
    So how we do this is TfDS do load。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这实际上只是一个简单的用例，展示它是如何工作的。所以我们将进行 DS train 和 DS test。我们将从 TensorFlow 数据集中获取一个训练数据集和一个测试数据集。然后，我们还会获取一些关于数据集的
    DSS 信息。我们这样做的方式是通过 TfDS 加载。
- en: And then here we're going to do Mnist。So then we do split and this right here。
    this string is just what you find on that the Tensor data catalog。 and then you
    just write the dataset name as the string。And then we're going to do split and
    we're going to do train。And test。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行 Mnist。所以我们进行分割，这里的字符串就是你在 Tensor 数据目录中找到的内容。然后你只需将数据集名称写成字符串。接着我们将进行分割，并且将进行训练和测试。
- en: So this is to this sort of we're doing DS Strain as the first output in this
    twople right here。 So that's why we're doing split train first and then test。
    Some data sets also have a validation。 So then you would also do add a validation
    string here。 Emmins doesn't have that。 But so you would have to check for the
    specific data set。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们将 DS Strain 作为这个元组中第一个输出的原因。因此我们先进行训练分割，然后进行测试。一些数据集也有验证集，因此你还需要在这里添加一个验证字符串。Emmins
    没有这个，但你需要检查特定的数据集。
- en: sort of what you have what the splits are for that data set。Then we're going
    to do shuffle files equals true So what Tensorflow data sets usually do is store
    things in something called TF records and usually they it in multiple files so
    let's say they have 1000 example per file and the reason is that that way it can
    be streamed through so if you're working with a server or something like that
    it can be streamed over internet and then it can be loaded simultaneously while
    it's training so that's quite useful if you're training on something like Google
    TpUs or something like that and so what we want to do is we want to shuffle those
    files so that and we want to shuffle those so that it doesn't see the exact same
    sequence of files even though the batches inside of that sort of 1000 examples
    are going to be shuffled and randomized we still want to shuffle the files so
    that the ordering is not the same。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要了解该数据集的分割情况。然后我们将设置 shuffle files 为 true。Tensorflow 数据集通常将内容存储在称为 TF records
    的东西中，通常分成多个文件，比如说每个文件有 1000 个示例，这样做的原因是可以流式传输。因此，如果你在服务器上工作，可以通过互联网进行流式传输，然后在训练时同时加载，这在使用
    Google TpUs 等进行训练时非常有用。因此我们希望打乱这些文件，以便不再看到完全相同的文件序列，尽管那些 1000 个示例内的批次将被打乱和随机化，我们仍希望打乱文件，以便顺序不同。
- en: Alright， and then we're going to do as supervise equals true。 And this means
    that it will return a tuple of sort of the so of a tuple of image comma label。
    Otherwise it's going to return a dictionary。And then we're going to do with info
    equals true。 So that's why we get this Ds info。 If we remove this one。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，然后我们将设置为 supervise equals true。这意味着它将返回一个元组，即图像和标签的元组。否则，它将返回一个字典。然后我们将设置
    info 为 true。所以这就是我们获得 Ds info 的原因。如果我们移除这个。
- en: we set it to false we're just gonna get Ds train and Ds test。 So first thing
    usually we just do print DSs info so that we actually see what the data set looks
    like in this case I think we already know it but so the image shape are 28 to
    281 and then the D type is Tf U in 8 and then we have label it has no shape it's
    just an integer and it's Tf in 64 and then we have a number of classes 10。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其设置为 false，我们只会获取 Ds train 和 Ds test。首先，通常我们会打印 DS 的信息，以便实际查看数据集的样子。在这种情况下，我想我们已经知道了，但图像形状是
    28x28，数据类型是 Tf U in 8，标签没有形状，它只是一个整数，类型是 Tf in 64，并且我们有 10 个类别。
- en: Total examples are 70，0，10 k and 60k for our training， and then you can sort
    of see citation。 et cetera if you're using this for a paper or something like
    that。All right。If you also have newer versions of Tensorflow， I think it's 2。3
    and up。 then you can also do something like figure equals Tftds that show examples
    DSs train DSs info so we're going set in sort of the test set and then information
    about the test set or training set rather and then rows and we can set columns
    and you'll see what this is going to doright yeah so what you also need to do
    is you can't do this as supervised it's expecting to get a dictionary for this。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总例数为 70,000、10,000 和 60,000 用于我们的训练，然后你可以看到引用等。如果你将其用于论文或类似的事情。好的。如果你还有更新版本的
    Tensorflow，我认为是 2.3 及以上。然后你也可以做类似的事情，figure equals Tftds 显示示例 DSs train DSs info，所以我们将设置测试集及其信息，而不是训练集，然后设置行和列，你会看到这将如何运行。
- en: So let's just rerun it as supervised equals false and then as you'll see it
    will look something like this。 where you can see some examples of your dataset
    set So here we have four rows and four columns as we specified and then we can
    see what the beneath we can see the name and then the labels so moving on what
    we need to do now is we're gonna create a function So we're going to define normal
    image we're gonna to take in an image。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们重新运行它，将 supervised 设置为 false，然后你会看到它会看起来像这样，你可以看到你的数据集的一些示例。在这里，我们有四行四列，如我们指定的那样，然后我们可以看到下面的名称和标签。接下来，我们需要做的就是创建一个函数，因此我们将定义
    normal image，并传入一张图像。
- en: '![](img/7bf38e208736fe558252a86bf7cde851_6.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bf38e208736fe558252a86bf7cde851_6.png)'
- en: '![](img/7bf38e208736fe558252a86bf7cde851_7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bf38e208736fe558252a86bf7cde851_7.png)'
- en: and a label。 And what we're gonna do here we're just going to normalize images。
    So。 so essentially we're gonna make sure the image is TF flow 32。 and then we're
    gonna just divide by 255 to get it in that0 to1 range。 So we're gonna return TF
    dot ca image。 and then TF dot flow 32。 and then divide by 255。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以及一个标签。我们要做的就是对图像进行归一化。因此，本质上我们要确保图像为 TF flow 32，然后将其除以 255，使其在 0 到 1 的范围内。因此我们将返回
    TF.dot ca image，然后 TF.dot flow 32，再除以 255。
- en: and then we're gonna return the label。And so。What we can do is now is we can
    do Ds train equals DSstrain。 map。And then， normalize image。Alright， so we're essentially
    what this is going to do it's going to map every single example and run it through
    this function first。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将返回标签。那么，现在我们可以执行 Ds train 等于 DSstrain.map，然后归一化图像。好的，这本质上会对每一个示例进行映射，并首先运行这个函数。
- en: And then we can also specify nu parallel calls since to send in to this normalized
    image there' is no inherent sequence that needs to be done in so this can be done
    in parallel so then we specify how many parallel calls it's going to do and so
    you could specify this yourself so let's say5 or 10 whatever it's essentially
    going to be a hyperparameter of your model one cool thing is that Tensorflow lets
    you do auto tuneune so it's going to sort of find what Tensorflow thinks is best
    and then how to get autoune you would do Tf do data that experimental dot auto
    tune。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们还可以指定 nu parallel calls，因为在发送到这个归一化图像时，没有固有的序列需要执行，因此可以并行处理，然后我们指定要进行多少个并行调用，你可以自己指定，比如
    5 或 10，基本上这将是模型的一个超参数。一个很酷的事情是 TensorFlow 允许你进行自动调优，因此它会找出 TensorFlow 认为最好的方法，想要进行自动调优，你可以执行
    Tf do data that experimental.dot auto tune。
- en: And then we're going to do Dstrain equals Dstrain dot cache and this cache is
    essentially after the first time it's loaded data。 it's going to keep track of
    some of them in memory so that it's going to be faster for the next time。And then
    DSstrain equals DSstrain dot shuffle and we can essentially set the buffer sort
    of the shuffle buffer size and let's say 1000。 so this means that it's not going
    to see the entire range of the dataset。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将执行 Dstrain 等于 Dstrain.dot cache，这个缓存本质上是在第一次加载数据后，它会在内存中跟踪其中的一些数据，以便下次加载更快。接着，DSstrain
    等于 DSstrain.dot shuffle，我们可以设置缓冲区的大小，比如设置为 1000，这意味着它不会看到数据集的整个范围。
- en: but it's going to also this number kind of depends on this size of the files
    that in this case Tf datasets has stored them in but what you could do here also
    is you could do DSs info dot splits and you could obtain the train and then you
    could just do do nu examples and in this way we're going be sure that it's going
    shuffle them randomly。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个数字在某种程度上也依赖于文件的大小，在这种情况下，Tf 数据集将它们存储起来，但你也可以在这里执行 DSs info.dot splits，然后获取训练数据，接着你可以执行
    nu examples，这样我们就可以确保随机打乱它们。
- en: And then we're going to do Dstrain equals Dstrain dot batch， and then we can
    set some batch size。And let's set the batch size above here。 So batch size， let's
    do 64。Oh。Al right。Get that back。 and then。Also， we're gonna do D Strain dot prefetch。
    and then we're gonna also gonna set auto tune on that。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将执行 Dstrain 等于 Dstrain.dot batch，然后可以设置一些批处理大小。让我们在这里设置批处理大小，假设批处理大小为 64。好的，回到这里。另外，我们还将执行
    D Strain.dot prefetch，然后我们也会在这上面设置自动调优。
- en: And then also here on the prefetch essentially， it's gonna while it's running
    on the GPU。 it's gonna prefetch 64 examples so that they are ready to be run instantly
    after the the GPU calls are done。 And then all we gotta do here is we gotta do
    it for the test set as well。 So we gotta do。Pretty much the same thing。Pretty
    much the same thing。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在预取阶段，当它在GPU上运行时，会预取64个示例，以便在GPU调用完成后可以立即运行。我们在测试集上也要做同样的事情。所以我们要做的基本上是相同的操作。
- en: So we're gonna do D S test is D S test dot map。Normalize image， and then nonpar
    calls is auto tune。And then we're gonna do。Bch， so we're not going to shuffle
    it for the test set。 And then we're going to do Lastly， we're going to do pre
    fetchtch。Prefet fetch， and then auto2。Alright， so this is for the actual data
    processing and this is going to be similar to when weve when we're loading the
    data ourselves using TF data。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将执行`D S test = D S test.map(normalize_image)`，并且`nonpar`设置为`auto tune`。然后我们将执行`Bch`，因此我们不会在测试集上打乱数据。最后，我们将执行`prefetch`，并设置为`auto2`。好的，这部分是实际数据处理，这与我们使用TF数据加载数据时会非常相似。
- en: so this is just this is so this is already a Tensorflow data which has been
    loaded very conveniently by this thing。 but after that point， so this is exactly
    what we're going to do when we have custom data sets and so on。And then let's
    see。 What we need to do now is just create a simple model。 So let's do careas
    that sequential。 and let's do Kaas input 28。28，1。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这已经是由这个工具方便加载的Tensorflow数据了。但在那之后，当我们有自定义数据集时，这正是我们要做的。接下来我们需要做的是创建一个简单的模型。让我们用`keras.Sequential`来创建，并设置输入为`28，28，1`。
- en: And then let's just do one comm layer。32 output channels，3 kernel sides， and
    then。Rello。And let's do flattenten。And then one then layer。And that's it。So all
    we got to do now is model compile。 and you've， you've seen all of this before。
    So model compile optimizer equals cares that。Optimizers dot Adam， We can set the
    learning rate。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需添加一个卷积层，32个输出通道，3的卷积核大小，然后是`relu`激活。接着我们将展平。然后再添加一层。就是这样。现在我们要做的就是编译模型。你之前都见过这些。所以模型编译时优化器设置为`keras.optimizers.Adam`，我们可以设置学习率。
- en: And then loss equals krisat losses dot spars。cateategorical。Ccroross entropy。And
    then metrics is just going to be accuracy。And then。We're going to do model do
    fit on the Ds train， so normally you would have to send in x and y。 so if you
    have x and y you would do x and then y。 Now since we have everything in this Ds
    train。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后损失设置为`keras.losses.sparse_categorical_crossentropy`，指标将是准确率。接着，我们将在训练集上调用`model.fit`，通常你需要传入`x`和`y`。所以如果你有`x`和`y`，你将传入`x`然后是`y`。现在因为我们在训练集中有了所有数据。
- en: we can just send that in and that's going contain twople of the x and Y labels
    and then we can do epochs set it to5 or something like that ver're both equals
    2 and then model that evaluate on the test set。And if there are no errors， let's
    see。 And this hopefully works。Yeah， one thing we had to do。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接传入，这将包含`x`和`y`标签，然后我们可以将`epochs`设置为5或者其他值，然后在测试集上评估模型。如果没有错误，让我们看看。希望这能正常工作。是的，我们需要做的事情有一件。
- en: we did as supervise equals false when we wanted to do this show examples。 but
    we gotta do supervise equals true for this to work。 So let's rerun it。And let's
    see what's wrong。 So what we gotta do here is we gotta do from logicit equals
    true。 Otherwise， this is not gonna train， so。Think that's what we were missing。Alright。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在想要展示示例时设置了`supervise equals false`。但为了让它工作，我们必须将`supervise`设置为`true`。所以让我们重新运行一下，看看问题出在哪里。我们需要做的是将`logicit`设置为`true`。否则，这将无法训练。我想这就是我们缺少的部分。好的。
- en: so we get 98% on the test set。 And so that's an example of Mist with images。
    I was thinking that we could also look at something a little bit。More。 I guess
    something different in text classification。 So this is gonna be， I guess。 a little
    bit more advanced because it's also gonna to be how to process the text and so
    on。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在测试集上得到了98%的准确率。这是一个与图像相关的示例。我在想我们也可以看看一些不同的文本分类内容。这将稍微复杂一些，因为这也涉及文本的处理等。
- en: we're gonna do it very simple and try to focus on a data。 but we're gonna so
    we're gonna look at IMDB data reviews。 So essentially that that reviews of movies
    and and so what we want to do is a sentiment analysis on these reviews of these
    movies and tell and sort of interpret if the comment is a positive comment or
    a negative one。 So let's just give an example。 I mean， some comment might be this
    movie was terrible and then we would give that a zero since this is negative。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将非常简单地进行操作，专注于数据。但我们将查看 IMDB 数据评论。因此，基本上这些是关于电影的评论，我们想要对这些评论进行情感分析，并判断评论是积极的还是消极的。举个例子，有些评论可能是“这部电影太糟糕了”，那么我们会给它一个零，因为这是负面的。
- en: And then if something someone said this movie was really good。 then we would
    set this to maybe one right So that's sort of the the data we're working with
    we're gonna do is similar to what we did before。 D S trained D S test。We're going
    to do and then DS info， we're going to TFDS dot load。 and we're going to specify
    IMDB reviews。We're going to do split is。Train， and then test。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果有人说“这部电影真的很好”，那么我们可能会将其设置为一。所以这就是我们正在处理的数据，和之前类似。D S train，D S test。我们将进行
    TFDS dot load，然后指定 IMDB 评论。我们将拆分为 train 和 test。
- en: And then we're going to do shuffle files equals true as supervised equals true。And
    then with info。 equals true。All right， so the same as what we just did。Now， what
    we need to do also is since what。 if we just so what we can do， first of all，
    actually， we can do print D S info and then we can do。 I du't know， something
    like。For text， comma label in the S train。 We can do print text。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将设置 shuffle 文件为 true，supervised 也为 true。然后设置 info 为 true。好的，就和我们刚刚做的一样。现在我们还需要做的是，如果我们只是...那么我们可以做的第一件事是，实际上，我们可以打印
    D S info，然后我们可以做。我不知道，类似于...对于 S train 中的文本和标签，我们可以打印文本。
- en: And then let's just quit。After one single example。 So just just so begin get。We
    get an interpretation of how it looks like。 And then we' gotta do。Split。Alright。
    so we get some information here。 We get test train。 So we have 25 k examples for
    training。25 k for test。 And then this unsupervised， I think are just comments
    that we don't have a label for。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们就结束吧。只用一个例子。所以开始理解一下它的样子。然后我们要进行拆分。好的。我们获得了一些信息。我们有测试和训练。所以我们有 25k 个训练示例，25k
    个测试示例。然后这个无监督的，我想是我们没有标签的评论。
- en: And then， yeah， and then we get great fun。 I went with eight friends to sneak
    preview viewing of this film。Yeah， so you can sort of。It's quite a long comment，
    but anyways。And。So that's how it looks like what we need to do first of all is
    actually tokenize it so that we don't get an entire sentence because that's we
    can't send an entire sentence to our model。 we need to first tokenize it so that
    let's say we have a string hello， know I love this。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，没错，我们会很有趣。我和八个朋友一起参加了这部电影的提前观看。是的，你可以看到。这是一个相当长的评论，不过无论如何。然后，这就是我们首先需要做的，实际上是进行分词，以便我们不能将整个句子发送到我们的模型中。我们需要先进行分词，假设我们有一个字符串“你好，我爱这个”。
- en: I love this movie。Movie。 And then we， we're gotta do so tokenization。 And essentially。
    the output is then gonna be a list of。Sort of the specific independent words。
    So I love。And then this etc， you get the point and then the next point is we need
    to actually numericalize it so that we we can't send in a string here。 we need
    to convert each of these words to an index using some vocabulary。Alright。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我爱这部电影。然后我们要进行分词。基本上，输出将是一个独立单词的列表。所以我爱，然后等等，你明白我的意思。接下来我们需要做的是将其数字化，因为我们不能在这里发送字符串。我们需要使用某个词汇将这些单词转换为索引。好的。
- en: so what we need to do is we need to do tokenizer and we can use Tensorflow datas
    for that。 We can do features text do tokenizer。 And one thing here is that so
    Tensorflowlow has a bunch of different ways to process text。 and it's a bit confusing，
    to be honest。 you have Tensorflow datas that you can preprocess it CAs has a tokenization。
    you can preprocess it And then there's also a library called Tensorflow text that
    there isn't really that much information on。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要做的是进行分词，并且我们可以使用 Tensorflow 数据来完成这个任务。我们可以对特征文本进行分词。而这里有一点就是，Tensorflow
    有很多不同的方式来处理文本，老实说，有点让人困惑。你可以用 Tensorflow 数据进行预处理，CAs 也有一个分词功能。你可以对其进行预处理，然后还有一个名为
    Tensorflow text 的库，但实际上关于它的信息并不多。
- en: So yeah， I'm not really sure which one of these three are best so far this Tensorflow
    data seems to work fine。But anyways， after we get thisken tokenizer， we're going
    to do define build vocabulary。And we're gonna do a vocabulary。 iss gonna be a
    set。And then we're going to do four text。 and then we don't need the labels in
    DS S train。 We're going to do vocabulary do update。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以是的，我目前不太确定这三种中哪一种最好，这个 Tensorflow 数据似乎运作得很好。不过无论如何，在我们获取这个分词器之后，我们将定义构建词汇表。然后我们将创建一个词汇表，它将是一个集合。接着我们将处理文本，而在
    DS S 训练中我们不需要标签。我们将更新词汇表。
- en: tokenizer do tokenize。Of that text。 and then we got to convert it to numpy。
    and then let's do it lower so that it doesn't really matter if the characters
    are uppercase or lowercase。And one thing， if you're very observant is that we're
    adding every word now in our vocabulary and that is not ideal。 right， normally
    it would set some frequency。 let's say if it occurs five times in our data Then
    we add it to our vocabulary because that's then it's an important word or something
    like that。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器对该文本进行分词。然后我们需要将其转换为 numpy。然后我们将其转换为小写，这样字母大小写就不太重要了。有一件事，如果你很观察的话，我们现在在我们的词汇表中添加每一个单词，这并不是理想的。对吧，通常会设置一些频率。假设它在我们的数据中出现五次，然后我们将其添加到词汇表，因为那样它就是一个重要的词。
- en: So this is not really for efficiency， this is not for accuracy。 I just want
    to show you a very simple example of how you would do this。 And I think you can
    try to make this better and。Yeah。 do something like check how many times that
    specific word occurs and then add it if it。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这并不是为了效率，也不是为了准确性。我只是想给你展示一个如何做的非常简单的例子。我想你可以尝试让它更好。是的，可以做一些检查，看看那个特定单词出现了多少次，然后如果它出现，就添加进去。
- en: if it occurs a certain amount of times， something like that。And then we're going
    to return that vocabulary。And so we're going to do vocabulary equals build vocabulary。And
    then one thing we're going to do now is we're going to do an encoder。 So as I
    said。 we need to numericalize these specific all of the tokenized words。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个词出现了一定次数，类似于这个情况。然后我们将返回这个词汇表。所以我们将词汇表等于构建词汇表。然后我们现在要做的一件事是我们将进行编码器。如我所说，我们需要对所有已分词的单词进行数字化处理。
- en: and that's going to be done with TFTS do features dot text do token text encoder。And
    then we're going to send in our vocabulary， first of all。 and then we're going
    to send specify out of vocabulary token。 so if we get a word that's not in a vocabulary，
    in this case。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过 TFTS 进行特征处理，即文本进行分词编码。然后我们将首先发送我们的词汇表，接着我们将指定超出词汇的标记。如果我们得到一个不在词汇表中的单词，在这种情况下。
- en: we're not going to get any right because we're adding every single word， but
    anyways。 and then we're going to do lower case equals true。And tokenizer， we're
    gonna specify our tokenizer。Alright， so now we can do define my encoding。And we're
    going to get some text tensor and we're going to get some label and what we're
    going to do is we're going return encoder do encode that text tensor do nuy。 So
    we're going to convert it to Ny first and then we're going to return the label
    So essentially what this encoder is going to do is it's going to tokenize it and
    then turn it into an index based on this vocabulary right here And so it's going
    to do all of the things we need essentially。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会得到任何正确的结果，因为我们在添加每一个单词，但无论如何。然后我们将设置小写为真。分词器，我们将指定我们的分词器。好的，现在我们可以定义我的编码。我们将获取一些文本张量，然后获取一些标签，我们将做的是返回编码器来编码该文本张量。我们将先将其转换为
    Ny，然后返回标签。所以这个编码器的本质是对其进行分词，然后根据这个词汇表将其转换为索引。所以它将完成我们所需的一切。
- en: And then we got to do another function。 So we're gonna to encode map。And we're
    gonna send in some text and label。And the thing is here that the data loading
    is also part of the Tensorflow graph。 so this is a python function， and we need
    to do a function to we need to specify the input and the outputs of this function
    so that it's part of the graph so we're going to do encoded text and then label
    is Tf py function so we essentially we're going to specify that we're going to
    send it through some python function and we're going to specify the function so
    my encoding we also need to specify the input so this is because of its part of
    the because it's part of the graph we're going to do text and then label and then
    we're going to T out is is Tf in 64 Tf in 64 so because when to numericalize is's
    going to become an integer representing the word in our vocabulary。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要做另一个函数。所以我们将进行编码映射。我们将发送一些文本和标签。这里的事情是数据加载也是 Tensorflow 图的一部分。所以这是一个 Python
    函数，我们需要做一个函数以指定此函数的输入和输出，使其成为图的一部分。因此我们将做编码文本，然后标签是 Tf py 函数，所以我们实际上将指定我们将通过某个
    Python 函数发送它，并且我们将指定该函数，因此我的编码，我们还需要指定输入，这是因为它是图的一部分，我们将做文本和标签，然后我们将 T out 设置为
    Tf in 64 Tf in 64，因为在进行数字化时，它将变成代表我们词汇表中单词的整数。
- en: And both are going to be integers。And then we also got to do encode and text
    dot set shape。 so we got to specify the shape and we're going to specify none。
    and why we're specifying none is because we essentially have a sequence and that
    sequence can be arbitrary length。Although the label are just going to be a single
    integer of 0 and1。Alright， and then at the end。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 并且这两个都将是整数。然后我们还需要进行编码和文本点设置形状。因此我们需要指定形状，并且我们将指定为 none。我们之所以指定 none 是因为我们实际上有一个序列，而该序列可以是任意长度。尽管标签只会是单个整数
    0 和 1。好吧，然后在最后。
- en: return encoded text comma label。 So this can feel a little bit， I't know，unk
    clunky。 like unnecessary， but。And there might be better ways to do it as well。
    This is just。How I managed to get it to work and and it seems to， to be that this
    is a standard way of doing it。Alright， so then we're going to do auto tuneune
    Tf that the experimental dot auto tuneune。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 返回编码文本，逗号，标签。因此这可能会感觉有点，我不知道，笨拙。像是不必要的，但。可能还有更好的方法来做到这一点。这只是。我设法让它工作，似乎这是一个标准的做法。好吧，然后我们将进行自动调整
    Tf，那是实验点自动调整。
- en: We're going to do Ds train is DSstrain dot map。 We're going to send it through
    encode map。That's what we call it right in code mapap。And then， nu parallel。Calls
    is autotune。And then we' got to do this。Let's do cache。 So dot cache。And then
    we're going to do D S train equals Ds train dot shuffle。 And let's just write
    10000。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做 DS 训练是 DSstrain 点映射。我们将通过编码映射发送它。我们在代码映射中称其为。然后，nu 并行。调用是自动调整。然后我们要做这个。让我们做缓存。所以点缓存。然后我们将
    D S 训练设置为 DS 训练点洗牌。我们写 10000。
- en: And then what we're going to do now is D S train。That padded batch。And we got
    to do a padded batch because all of the sequences in our batch are going to be
    different in length。 so we need to pad it so that pad it to the longest example。And。What
    we do here then is we do padded batch 32， and then we gotta specify padded shapes。嗯。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要做的是 D S 训练。那个填充批处理。我们必须进行填充批处理，因为我们批中的所有序列长度都不同。所以我们需要填充到最长的示例。然后我们在这里做填充批处理
    32，并且需要指定填充形状。嗯。
- en: So we got to do none。And then just a tuple。So on new versions on Tensorflow。
    this part is not necessary， but essentially what we're doing here is we're specifying
    which of the of the shapes that are going to be padded。 So when we specify none
    here， those are for the images or rather the text sequences。 and that's the one
    we we want to pad。 So that's why we write none on that one。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要做 none。然后只是一个元组。在 Tensorflow 的新版本中，这部分不是必需的，但我们在这里所做的基本上是指定将要填充的形状。因此，当我们在这里指定
    none 时，那些是用于图像或文本序列的。这就是我们想要填充的。因此我们在那个上写 none。
- en: And then Tensorflowlow is going to know that we want to pad that one and then
    we're got to do DSs train。Thatt prefetch。 And then again。Auto tune。And similarly
    for our test set。 we're just going to do DSs test。Dot map and code map。Encode
    map。 That's it。 And then we're gonna do padded batch on that one as well。 So D
    S has thatt padded batch。32。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 Tensorflowlow 将会知道我们想要填充那个，然后我们要进行 DS 的训练。那是预取。然后再一次。自动调整。同样对于我们的测试集，我们只需进行
    DS 的测试。点映射和代码映射。编码映射。就是这样。然后我们也会在那个上进行填充批处理。所以 D S 拥有那个填充批处理。32。
- en: And then again， padded shapes。A're going to be none and just a tuple。Allright。
    so that was it for the preprocessing of the text and the data。What we got to do
    now is create our model。 So we're just going to create a very simple model。And
    then we can， we're gonna do， first of all， we're gonna do layer start masking。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次，填充形状。A将为无，并且只是一个元组。好的。这就是文本和数据的预处理。我们现在要做的是创建我们的模型。所以我们只是要创建一个非常简单的模型。然后我们首先要做的是层开始掩蔽。
- en: Mask value equals 0。 so essentially。So essentially here we're telling TensorF
    that the values that are padded they're going to be padded with index of 0。Those
    values are going to be ignored in its computation。 So let's say we have one sequence
    that's 1000 in length and we have one that's 20。 then that 20 in length is going
    to be padded by 980 of zeros and performing the computation for all of those are
    going to be quite unnecessary。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 掩蔽值等于0。因此，基本上。在这里，我们告诉TensorF，填充的值将用索引0填充。这些值将在计算中被忽略。假设我们有一个长度为1000的序列，还有一个长度为20。那么长度为20的序列将被980个零填充，执行所有这些的计算是完全不必要的。
- en: So when we're doing this layer that masking， we're just letting Tensorflow know
    that just ignore those values of0 don't perform any computation。 Yeah， so。Then
    we're gonna do layers dot embedding。 We're gonna do input dimension is going to
    be the length of the vocabulary。 and then plus 2 plus 2。 because we added one
    when we did this padded batch with index of 0 to our vocabulary。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们做这个层掩蔽时，我们只是让Tensorflow知道忽略那些值为0的，不进行任何计算。是的，所以。然后我们要做层嵌入。输入维度将是词汇表的长度，再加上2加2。因为我们在进行这个填充批次时将索引0添加到了我们的词汇表中。
- en: And then we also have one out of， out of。We also have one index for out of vocabulary
    words。 So we're just going to add two on that。And then let's specify some output
    I mentioned。 we're just going to specify 32， which is very small for an embedding
    size。 Normally。 you would have 300 or something like that。But as， again， this
    is just for illustration。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个超出，超出词汇的索引。因此，我们只是将其加2。然后让我们指定一些输出维度，我们将指定32，这对于嵌入大小来说非常小。通常，你会有300或者类似的。但再次强调，这只是为了说明。
- en: And then what you would also do is you would do some LCSTM or some sequence
    model on this。 In this case， all we're gonna do is we're gonna do global， global
    average。Pulling 1 D。 So let's just say we had 1000 words in our sequence。 And
    then what we did is we map this each。 each index， each word， we mapped that to
    an output dimension of 32。 So essentially， we had。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你还会做一些LCSTM或某种序列模型。在这种情况下，我们要做的是全局，全球平均。1D池化。假设我们的序列中有1000个单词。然后我们将每个索引，每个单词映射到输出维度32。因此，基本上我们有。
- en: you know， batch size。And then times 1000 in that case。 And then what we did
    after this embedding we're going to get batch size times 100 times 32。 So each
    word has been mapped to an output dimension of 32。 Then after this after this
    average pooling， we're just going to get batch size times 32。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，批量大小。在这种情况下乘以1000。然后在嵌入之后，我们将得到批量大小乘以100乘以32。所以每个单词都映射到输出维度32。然后在这个平均池化之后，我们将得到批量大小乘以32。
- en: essentially taking the average of all of the sequences for all of the examples。Yeah。So
    then we're going to do layer start dense。64 activation equals re。And then at the
    end。 we're just going to add one dense layer。So let's just say it's gonna output
    a single float float value。 And then if it's if it's less than0， if it's less
    than 0， then it's negative。So less than zero。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上是对所有示例的所有序列取平均。是的。所以然后我们要做层开始密集。64的激活等于re。然后在最后。我们只需添加一个密集层。假设它将输出一个单一的浮点值。如果它小于0，那么就是负数。所以小于零。
- en: Negative， greater than0。Positive or maybe greater or equal。So what we're going
    to do here is we're going to use binary cross entropy and that's going to use
    the sigmoid activation function。So yeah， I guess this， this part of is a bit more
    advanced。 So if you're not really following that。 that's okay。 But yeah， so what
    we're gonna do now is we're do model compile。嗯。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 负数，大于0。正数或者可能大于或等于。所以我们要做的是使用二元交叉熵，这将使用sigmoid激活函数。所以，是的，我想这一部分稍微有点高级。如果你没完全跟上，也没关系。但现在我们要做的是模型编译。
- en: I guess also one thing you could do is you could do as we have done previously
    where it would output two nodes。 and then you could use sparse。Srse categorical
    cross entropy， as we've normally done。 It's just that when we have two classes，
    we can instead use another loss function。 and that loss function is care that
    loss that binary。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我想你还可以做的一件事是，可以像我们之前做的那样输出两个节点。然后你可以使用`sparse categorical cross entropy`，就像我们通常所做的那样。只是当我们有两个类别时，我们可以使用另一种损失函数，而这个损失函数就是`binary
    cross entropy`。
- en: Cross entropy and then from logicit equals true。And then we can specify the
    optimizer。 cars that optimizes。 atom。3 e-4。 And then we can specify the clipping
    value so that they we don't get exploding gradient problems。And then metrics，
    we're gonna do。Accuracy。Alright， And then in the end。 we're just gonna do model
    that fit D S train。 Let's do it for 1nypos。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵，然后从`logicit`等于真。然后我们可以指定优化器，优化器使用`atom`，学习率为`3e-4`。然后我们可以指定剪切值，以避免梯度爆炸的问题。然后在指标方面，我们将使用`Accuracy`。好的，最后，我们只需执行`model.fit(D,
    S_train)`。我们就这样做1nypos。
- en: And then we're going to evaluate it on the test set。 So again。 this is not in
    any way for accuracy or something it's just for demonstration of how we would
    take this data set from Tensorflowlow data。 Billy vocabulary， preprocess the text，
    load it efficiently and then create a simple model just to show that it works。So
    let's run it and see what we get。Alright， so first of all。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将对测试集进行评估。因此，再次强调，这不是为了准确性或其他什么，仅仅是为了演示我们如何从TensorFlow数据集中获取数据，构建词汇表，预处理文本，进行高效加载，然后创建一个简单的模型来展示它的工作原理。所以让我们运行它，看看结果。好的，首先。
- en: can we just talk about how that ran without any errors on the first try。 I mean。
    I wrote like all of this code and it ran on the first try。 That got to be the
    first time that is actually that has actually happened。 So on the test set。 we
    get 89% at the end。 And let's see if we can scroll up。This is why I set the verbose
    equals 2。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能不能先聊聊它第一次运行时没有任何错误的事情。我是说，我写了所有这些代码，它第一次运行就成功了。这可能是第一次真正发生这种情况。所以在测试集上，我们最终得到了89%。让我们看看能否向上滚动。这就是我设置`verbose`为2的原因。
- en: by the way， and then we get 96% on the training about so。WellWe can， I mean。
    there's some room for regularization and there's also some room。 we can try and
    know train this for a little bit longer。 Of course。 we can make the model bigger
    and so on。 But for this case， this that， that's fine。 So alright。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，然后在训练集上我们得到了大约96%。所以，好吧，我们可以，有一些正则化的空间，也有一些空间。我们可以尝试再训练一段时间。当然，我们可以让模型变得更大等等。但在这种情况下，这样就可以了。所以好吧。
- en: so hopefully this video was useful。 I know this last part might have been。Li
    little bit advanced。 but I also wanted to show you how you would do it for text
    and I also want to mention that there are a bunch of different ways to do this。
    This is one way。 And if you have an alternate way that you think is better to
    something like this to process the text then please leave a comment。 But anyways，
    thank you so much for watching the video。 And I hope to see you in the next one。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以希望这个视频对你有帮助。我知道最后这一部分可能有点复杂，但我也想展示一下如何处理文本，并且我还想提到有很多不同的方法可以做到这一点。这是其中一种。如果你有觉得更好的替代方法来处理这些文本，请在评论中告诉我。不管怎样，非常感谢你观看这个视频。我希望在下一个视频中见到你。
- en: '![](img/7bf38e208736fe558252a86bf7cde851_9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bf38e208736fe558252a86bf7cde851_9.png)'
