- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P9：L2.2- 管道函数内部会发生什么？(TensorFlow) - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P9：L2.2- 管道函数内部会发生什么？(TensorFlow) - ShowMeAI - BV1Jm4y1X7UL
- en: '![](img/77009730db8ae748615a56ed0788b98a_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_0.png)'
- en: '![](img/77009730db8ae748615a56ed0788b98a_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_1.png)'
- en: What happens inside the pipelineplan function？In this video。 we will look at
    what actually happens when we use the pipeline function of the transformformerss
    library。More specifically， we will look at the scientific analysis pipeline and
    know it went from the two following sentences to the positive and negative labels
    with the respective scores。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在pipelineplan函数内部发生了什么？在这个视频中，我们将看看当我们使用transformers库的pipeline函数时，实际发生了什么。更具体地说，我们将查看科学分析管道，了解它是如何从以下两个句子转化为积极和消极标签及其相应的分数。
- en: '![](img/77009730db8ae748615a56ed0788b98a_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_3.png)'
- en: As we have seen in the pipeline video， there are three stages in the pipeline。First。We
    conduct the Rotex through numbers the model can make sense of using a tokenizer。Then
    was number goess with the model which outputs legs。Finally。 the processinging
    steps transform thoselogs into labels and skull。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在pipeline视频中看到的，管道中有三个阶段。首先，我们通过使用标记器将文本转化为模型可以理解的数字。然后，这些数字与模型进行交互，输出标签。最后，处理步骤将这些标签转化为分类和得分。
- en: '![](img/77009730db8ae748615a56ed0788b98a_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_5.png)'
- en: Let's look in detail about threes steps and now to replicate them in the transformformers
    library。 beginning with the first stage tokenization。![](img/77009730db8ae748615a56ed0788b98a_7.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这三个步骤以及如何在transformers库中复制它们，首先是第一阶段的标记化。![](img/77009730db8ae748615a56ed0788b98a_7.png)
- en: The tokenization process has several steps first， the text is split into small
    chunks called token。They get me words， part of words， or punctuation symbols。Then
    the tokenizer will add some special tokens to the model expected。Here。 the model
    used expects a CS token at the beginning and a s token at the end of the sentence
    to classify。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化过程包含几个步骤，首先，文本被拆分成称为token的小块。它们可以是单词、词的一部分或标点符号。然后，标记器会添加一些模型所期望的特殊标记。在这里，所使用的模型在句子的开头期望一个CS标记，在结尾期望一个s标记以进行分类。
- en: Lastly， the tokenizer matches each token to its unique ID in the vocabulary
    of the protrained model。![](img/77009730db8ae748615a56ed0788b98a_9.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，标记器将每个标记与预训练模型词汇中的唯一ID进行匹配。![](img/77009730db8ae748615a56ed0788b98a_9.png)
- en: To load such a tokenizer， the transformformer library provides the autotokenizer
    API。The most important method of this gas is from betray trade。Which will download
    and cache the configuration and the vocabulary associateded to a given checkpoint。Here
    the check used by default for the sentiment analysis pipeline is dist belt paste
    andcase fine tune SS through English。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载这样的标记器，transformers库提供了autotokenizer API。该API中最重要的方法是from betray trade，它将下载并缓存与给定检查点相关的配置和词汇。在这里，默认用于情感分析管道的检查点是dist
    belt paste和case fine tune SS through English。
- en: which is a bit of a mouth。We instance shade tookken associated with a checkpoint。
    then feed it the two sentences。Since those two sentences are not of the same size。
    we will need to pad the short one to be able to be an array。This is done by the
    tokenizer with ytion， patting equal。With truation equal through。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点难以表达。我们实例化与检查点关联的标记，然后输入这两个句子。由于这两个句子的长度不同，我们需要对较短的句子进行填充，以便能够形成一个数组。这是通过标记器进行的，使用ytion，填充相等。通过truation相等。
- en: we ensure that any sentence longer than the maximum the model can handle is
    truncated。Lastly。 the returntensil option tells the tukener return a by doch tensil。Looking
    at the result。 we see we have a dictionary with two keys。Input IDs contains the
    IDs of both sentences with zeros where the padding is applied。The second key attention
    mask indicates where patting has been applied。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保任何超过模型最大处理能力的句子都会被截断。最后，returntensil选项告诉标记器返回一个由doch tensil构成的数组。查看结果，我们可以看到我们有一个包含两个键的字典。Input
    IDs包含两个句子的ID，其中填充的部分为零。第二个键attention mask指示填充的应用位置。
- en: so the model does not pay attention to it。![](img/77009730db8ae748615a56ed0788b98a_11.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型不会对此给予关注。![](img/77009730db8ae748615a56ed0788b98a_11.png)
- en: This is always what is inside the organization step。![](img/77009730db8ae748615a56ed0788b98a_13.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这始终是组织步骤中的内容。![](img/77009730db8ae748615a56ed0788b98a_13.png)
- en: Now let's have a look at the second step。The model。![](img/77009730db8ae748615a56ed0788b98a_15.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下第二步。模型。![](img/77009730db8ae748615a56ed0788b98a_15.png)
- en: As for the tokenezizer produce the newmod API with a from between method。It
    will download and cache the configuration of the model， as well as the pretrained
    weights。However。 the Auto API will only instants the body of the model。That is
    the part of the model that is left once the pro tradinging edge is remove。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 至于tokenizer，使用newmod API和between方法。它将下载并缓存模型的配置以及预训练权重。然而，Auto API仅实例化模型的主体。即在去除预训练边缘后剩下的模型部分。
- en: It will output a high dimensional tensor that is a representation of the sentences
    past。 but which is not directly useful for a classification problem。Here the tennsor
    has two sentences。 each of 16 token， and the last dimension is ill in size of
    a model，768。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出一个高维张量，表示句子的过去，但这对于分类问题并没有直接用处。这里的张量有两个句子，每个有16个标记，最后的维度是模型的大小，768。
- en: '![](img/77009730db8ae748615a56ed0788b98a_17.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_17.png)'
- en: To get an output link to our classification problem。 we need to use the Automod
    for sequence classification class。It works exactly as zero to model class， except
    that it will build a model with a classification head。Price1 not a class for each
    common LLP task in the Transformers library。Here。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得与我们的分类问题相关的输出链接，我们需要使用序列分类类的Automod。它的工作方式与零到模型类完全相同，只是它将构建一个带有分类头的模型。价格1不是Transformers库中每个常见LLP任务的类。
- en: after giving our model the two sentences， we get the tens source size2 by 2。
    one result for each sentence and for each possible label。Those outputs are not
    probabilities yet。 we can see they don't sell up to one。This is because each model
    of the transformer library returns lugg it。To make sense of us look it， we need
    to dig into the field and last step of the pipeline。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在给我们的模型两个句子后，我们得到了大小为2x2的张量源。每个句子和每个可能标签的一个结果。这些输出还不是概率，我们可以看到它们的总和不为1。这是因为transformer库的每个模型返回logits。为了理解这些，我们需要深入挖掘管道的最后一步。
- en: plus processing。To convert logits into probabilities， we need to apply a C max
    layer to them。As we can see， this transforms them into positive numbers， by's
    them up to one。The last step is to know which of those correspond to the positive
    or the negative label。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 加上处理。为了将logits转换为概率，我们需要对其应用一个C max层。正如我们所看到的，这将其转换为正数，使它们的总和为1。最后一步是知道哪些对应于正标签或负标签。
- en: '![](img/77009730db8ae748615a56ed0788b98a_19.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_19.png)'
- en: This is given by the ID2lipold field of the model config。The first proba is
    index 0。 correspond to the negative level and the seconds index 1 correspond to
    the positive level。This is how our classifier built with the pipeline function，
    peaks+ labels and computed vicos。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这由模型配置的ID2lipold字段给出。第一个概率是索引0，对应于负标签，第二个索引1对应于正标签。这就是我们的分类器如何通过管道函数构建的，选择标签并计算vicos。
- en: '![](img/77009730db8ae748615a56ed0788b98a_21.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_21.png)'
- en: Now that you know how each steps works， you can easily tweak them to your needs。![](img/77009730db8ae748615a56ed0788b98a_23.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道每个步骤是如何工作的，你可以轻松调整它们以满足你的需求。![](img/77009730db8ae748615a56ed0788b98a_23.png)
- en: '![](img/77009730db8ae748615a56ed0788b98a_24.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77009730db8ae748615a56ed0788b98a_24.png)'
- en: 嗯。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。
