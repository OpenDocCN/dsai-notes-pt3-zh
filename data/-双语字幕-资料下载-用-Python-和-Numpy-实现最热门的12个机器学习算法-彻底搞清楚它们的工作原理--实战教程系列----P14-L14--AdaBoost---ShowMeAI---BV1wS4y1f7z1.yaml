- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P14ï¼šL14-
    AdaBoost - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„ 12 ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P14ï¼šL14-
    AdaBoost - ShowMeAI - BV1wS4y1f7z1
- en: Heyï¼Œ guysï¼Œ welcome to another machine learning from Sc tutorialã€‚ Todayã€‚ we are
    going to implement the addda boost algorithm using only Nmpy and built in Python
    modulesã€‚ğŸ˜Šã€‚A a boost uses the boosting approachï¼Œ which follows the simple idea
    to combine multiple weak classifier into one strong classifierã€‚ And this approach
    works really well in practiceã€‚ So let's start with a theory before we jump to
    the codeã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å˜¿ï¼Œå¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°å¦ä¸€ä¸ªæœºå™¨å­¦ä¹ çš„ Sc æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨ Numpy å’Œå†…ç½®çš„ Python æ¨¡å—å®ç° AdaBoost ç®—æ³•ã€‚ğŸ˜Š AdaBoost
    ä½¿ç”¨æå‡æ–¹æ³•ï¼Œè¿™ä¸ªç®€å•çš„æƒ³æ³•æ˜¯å°†å¤šä¸ªå¼±åˆ†ç±»å™¨ç»„åˆæˆä¸€ä¸ªå¼ºåˆ†ç±»å™¨ã€‚è¿™ä¸ªæ–¹æ³•åœ¨å®è·µä¸­æ•ˆæœéå¸¸å¥½ã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬è·³åˆ°ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆä»ç†è®ºå¼€å§‹ã€‚
- en: So let's have a look at this 2 D example here to understand the conceptã€‚ So
    here we have our samples with only two different features on the X axis and on
    the Y axisã€‚ And now the first classifier makes a split based on the Y axis in
    this exampleã€‚ So it draws a horizontal decision line at some thresholdã€‚ So the
    dashed line that we can see hereã€‚ğŸ˜Šã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªäºŒç»´ç¤ºä¾‹ï¼Œä»¥ç†è§£è¿™ä¸ªæ¦‚å¿µã€‚è¿™é‡Œæˆ‘ä»¬çš„æ ·æœ¬åªæœ‰ä¸¤ä¸ªä¸åŒçš„ç‰¹å¾ï¼Œåˆ†åˆ«åœ¨ X è½´å’Œ Y è½´ä¸Šã€‚ç°åœ¨ï¼Œç¬¬ä¸€ä¸ªåˆ†ç±»å™¨æ ¹æ® Y è½´è¿›è¡Œåˆ’åˆ†ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒåœ¨æŸä¸ªé˜ˆå€¼å¤„ç»˜åˆ¶äº†ä¸€æ¡æ°´å¹³å†³ç­–çº¿ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„è™šçº¿å°±æ˜¯ã€‚ğŸ˜Š
- en: And we can see that some predictions are correctï¼Œ but we also have misclassificationsã€‚
    And now with these misclassificationsï¼Œ we can then calculate a performance measureã€‚
    So the accuracy for this classifierã€‚ And with this measureã€‚ we calculate and update
    weights for all training samplesã€‚ And now the second classifier comes inã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›é¢„æµ‹æ˜¯æ­£ç¡®çš„ï¼Œä½†ä¹Ÿæœ‰è¯¯åˆ†ç±»ã€‚ç°åœ¨ï¼Œåˆ©ç”¨è¿™äº›è¯¯åˆ†ç±»ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ€§èƒ½æŒ‡æ ‡ã€‚ä¹Ÿå°±æ˜¯è¿™ä¸ªåˆ†ç±»å™¨çš„å‡†ç¡®åº¦ã€‚é€šè¿‡è¿™ä¸ªæŒ‡æ ‡ï¼Œæˆ‘ä»¬è®¡ç®—å¹¶æ›´æ–°æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„æƒé‡ã€‚ç„¶åç¬¬äºŒä¸ªåˆ†ç±»å™¨å°±ä¼šå‡ºç°ã€‚
- en: and it uses these weights and finds a different and possibly better decision
    boundaryã€‚ So the second classifier in this example here chooses a feature on the
    x axis and draws a vertical lineã€‚ And then againï¼Œ we calculate the performance
    and update the weightsã€‚ And then we repeat the step for as many classifiers as
    we wantã€‚ğŸ˜Šï¼ŒAnd then here at the very endã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåˆ©ç”¨è¿™äº›æƒé‡ï¼Œå¯»æ‰¾ä¸€ä¸ªä¸åŒçš„ã€å¯èƒ½æ›´å¥½çš„å†³ç­–è¾¹ç•Œã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œç¬¬äºŒä¸ªåˆ†ç±»å™¨é€‰æ‹©äº† X è½´ä¸Šçš„ä¸€ä¸ªç‰¹å¾å¹¶ç»˜åˆ¶äº†ä¸€æ¡å‚ç›´çº¿ã€‚ç„¶åå†æ¬¡è®¡ç®—æ€§èƒ½å¹¶æ›´æ–°æƒé‡ã€‚æ¥ç€æˆ‘ä»¬é‡å¤è¿™ä¸ªæ­¥éª¤ï¼Œç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æƒ³è¦çš„åˆ†ç±»å™¨æ•°é‡ã€‚ğŸ˜Š
    ç„¶ååœ¨æœ€åã€‚
- en: we have all the different decision linesï¼Œ and we also have all the different
    classifier performancesã€‚ and then we combine all our classifiers so we can make
    a weighted sum with the calculated performancesã€‚ And this allows us to draw the
    perfect decision line that we can see hereã€‚ which can be more complex than a simple
    linear decision lineã€‚ğŸ˜Šã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰æ‰€æœ‰ä¸åŒçš„å†³ç­–çº¿ï¼Œä»¥åŠæ‰€æœ‰ä¸åŒçš„åˆ†ç±»å™¨æ€§èƒ½ã€‚ç„¶åæˆ‘ä»¬å°†æ‰€æœ‰åˆ†ç±»å™¨ç»“åˆèµ·æ¥ï¼Œå¯ä»¥ç”¨è®¡ç®—å¾—å‡ºçš„æ€§èƒ½è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç»˜åˆ¶å‡ºæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„å®Œç¾å†³ç­–çº¿ï¼Œè¿™å¯èƒ½æ¯”ç®€å•çš„çº¿æ€§å†³ç­–çº¿æ›´å¤æ‚ã€‚ğŸ˜Š
- en: And the idea with the way that some here he at the end means that the better
    the classifier is the more impact it has for the final outcomeã€‚ So this is basically
    the conceptã€‚ And now let's look at all the different steps and also the math behind
    this in detailã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œåˆ†ç±»å™¨è¶Šå¥½ï¼Œå¯¹æœ€ç»ˆç»“æœçš„å½±å“å°±è¶Šå¤§ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯è¿™ä¸ªæ¦‚å¿µã€‚ç°åœ¨è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹æ‰€æœ‰ä¸åŒçš„æ­¥éª¤ä»¥åŠèƒŒåçš„æ•°å­¦ã€‚
- en: So the first thing that we need is a weak classifierã€‚ and this is also called
    weak learnerã€‚So a weak learner is always a very simple classifierã€‚ And in the
    case of the addda boostã€‚ we use a so called decision stamp for thisã€‚ So a decision
    stamp is basically a decision tree with only one splitã€‚ So what we can see hereã€‚
    So we look at only one feature of our samples and only at one thresholdã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦çš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä¸€ä¸ªå¼±åˆ†ç±»å™¨ï¼Œè¿™ä¹Ÿè¢«ç§°ä¸ºå¼±å­¦ä¹ å™¨ã€‚å¼±å­¦ä¹ å™¨æ€»æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„åˆ†ç±»å™¨ã€‚åœ¨ AdaBoost çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ‰€è°“çš„å†³ç­–æ ‘æ¡©ã€‚å†³ç­–æ ‘æ¡©åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªåªæœ‰ä¸€ä¸ªåˆ’åˆ†çš„å†³ç­–æ ‘ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„å†…å®¹æ˜¯ï¼Œæˆ‘ä»¬åªå…³æ³¨æ ·æœ¬çš„ä¸€ä¸ªç‰¹å¾å’Œä¸€ä¸ªé˜ˆå€¼ã€‚
- en: And then based on if our feature value is greater or smaller than the thresholdã€‚
    We say that it is class -1 or class plus oneã€‚ğŸ˜Šï¼ŒSo this is the decision stumpã€‚
    And then we need the formula for the errorã€‚ So the first timeã€‚ the very first
    time during our iterationï¼Œ the error is calculated as the number of misclassifications
    divided by the total number of samplesã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ ¹æ®æˆ‘ä»¬çš„ç‰¹å¾å€¼æ˜¯å¦å¤§äºæˆ–å°äºé˜ˆå€¼ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å®ƒæ˜¯ç±»åˆ« -1 æˆ–ç±»åˆ« +1ã€‚ğŸ˜Š æ‰€ä»¥è¿™å°±æ˜¯å†³ç­–æ ‘æ¡©ã€‚ç„¶åæˆ‘ä»¬éœ€è¦è®¡ç®—è¯¯å·®çš„å…¬å¼ã€‚æ‰€ä»¥ç¬¬ä¸€æ¬¡ï¼Œåœ¨æˆ‘ä»¬è¿­ä»£çš„ç¬¬ä¸€æ¬¡ï¼Œè¯¯å·®æ˜¯é€šè¿‡è¯¯åˆ†ç±»çš„æ•°é‡é™¤ä»¥æ ·æœ¬æ€»æ•°æ¥è®¡ç®—çš„ã€‚
- en: And this is the natural approach for the errorã€‚ So if we have a look at our
    example againã€‚ Then we can see that we have 10 samples in this caseã€‚ and in the
    first in the first classifierã€‚ we have three misclassificationsã€‚ So this means
    that our error rate is 0ã€‚3 or 30%ã€‚So this is the first timeã€‚ But the next time
    we also want to take into account the weightsã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤„ç†è¯¯å·®çš„è‡ªç„¶æ–¹æ³•ã€‚å¦‚æœæˆ‘ä»¬å†æ¬¡æŸ¥çœ‹æˆ‘ä»¬çš„ä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æœ‰10ä¸ªæ ·æœ¬ã€‚åœ¨ç¬¬ä¸€ä¸ªåˆ†ç±»å™¨ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸‰æ¬¡è¯¯åˆ†ç±»ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„è¯¯å·®ç‡æ˜¯0.3æˆ–30%ã€‚è¿™æ˜¯ç¬¬ä¸€æ¬¡ã€‚ä½†ä¸‹æ¬¡æˆ‘ä»¬ä¹Ÿæƒ³è€ƒè™‘æƒé‡ã€‚
- en: So if a sample was mis misclassifiedï¼Œ we give it a higher weight for the next
    iterationã€‚ And this means that our formula is then calculatedã€‚As the sum over
    the weights for all misclassificationsã€‚ And if our error is greater than 05ã€‚ we
    simply flip the errorã€‚ So we flip all decisionï¼Œ all decisionsï¼Œ and we also flip
    our errorã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€ä¸ªæ ·æœ¬è¢«è¯¯åˆ†ç±»ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ç»™äºˆå®ƒæ›´é«˜çš„æƒé‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„å…¬å¼å°†è®¡ç®—ä¸ºæ‰€æœ‰è¯¯åˆ†ç±»æƒé‡çš„æ€»å’Œã€‚å¦‚æœæˆ‘ä»¬çš„è¯¯å·®å¤§äº0.5ï¼Œæˆ‘ä»¬å°±ç®€å•åœ°ç¿»è½¬è¯¯å·®ã€‚æˆ‘ä»¬ç¿»è½¬æ‰€æœ‰å†³ç­–ï¼Œä¹Ÿç¿»è½¬æˆ‘ä»¬çš„è¯¯å·®ã€‚
- en: So it is then1 minus the errorã€‚So this is the errorã€‚ and now we need the weightsã€‚
    So the weights are initially set to one over n for each sampleã€‚ And this also
    matches the error calculation in the first stepã€‚ So if we say we calculate the
    error as the sum over all misclassified weightsã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯1å‡å»è¯¯å·®ã€‚è¿™å°±æ˜¯è¯¯å·®ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦æƒé‡ã€‚æ¯ä¸ªæ ·æœ¬çš„æƒé‡æœ€åˆè®¾ç½®ä¸º1/nã€‚è¿™ä¹Ÿä¸ç¬¬ä¸€æ­¥çš„è¯¯å·®è®¡ç®—ç›¸åŒ¹é…ã€‚å¦‚æœæˆ‘ä»¬è¯´è®¡ç®—è¯¯å·®ä¸ºæ‰€æœ‰è¯¯åˆ†ç±»æƒé‡çš„æ€»å’Œã€‚
- en: and we also say that each weight is one over n in the beginningã€‚ then it is
    equal to the number of misclassifications divided by the number of samples like
    hereã€‚Soã€‚ yeahï¼Œ that's why the initial weights are one over n for each sampleã€‚And
    then we also need the update rule that is defined hereã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è¯´æ¯ä¸ªæƒé‡ä¸€å¼€å§‹æ˜¯1/nã€‚å› æ­¤å®ƒç­‰äºè¯¯åˆ†ç±»æ•°é‡é™¤ä»¥æ ·æœ¬æ•°é‡ã€‚æ˜¯çš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆåˆå§‹æƒé‡æ˜¯æ¯ä¸ªæ ·æœ¬1/nã€‚ç„¶åæˆ‘ä»¬è¿˜éœ€è¦è¿™é‡Œå®šä¹‰çš„æ›´æ–°è§„åˆ™ã€‚
- en: So we have the old weight times the exponential function of minus alpha times
    the actual y times h of xã€‚ where h of x is our predictionã€‚And alpha is the accuracy
    of the classifierã€‚ So if this is-1ã€‚ we have a misclassificationã€‚ and if this is
    plus one hereï¼Œ then we have a correct classificationã€‚ and this whole formula basically
    makes sure that misclassified samples have a higher impact for the next classifierã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰æ—§æƒé‡ä¹˜ä»¥è´Ÿé˜¿å°”æ³•ä¹˜ä»¥å®é™…yä¹˜ä»¥h(x)ï¼Œå…¶ä¸­h(x)æ˜¯æˆ‘ä»¬çš„é¢„æµ‹ã€‚é˜¿å°”æ³•æ˜¯åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ã€‚å¦‚æœè¿™æ˜¯-1ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯¯åˆ†ç±»ã€‚å¦‚æœè¿™æ˜¯+1ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰ä¸€ä¸ªæ­£ç¡®åˆ†ç±»ã€‚è¿™ä¸ªå…¬å¼ç¡®ä¿äº†è¢«è¯¯åˆ†ç±»çš„æ ·æœ¬å¯¹ä¸‹ä¸€ä¸ªåˆ†ç±»å™¨æœ‰æ›´å¤§çš„å½±å“ã€‚
- en: So yeahï¼Œ this is what you should remember from the weightsã€‚And now the performanceã€‚
    So we need to calculate the performance or alpha for each classifierï¼Œ and we can
    do thisã€‚ and we need this for the final prediction thenï¼Œ and the formula for the
    performance is calculated as thisã€‚ So it's 0ã€‚5 times the lock of one minus the
    error divided by the errorã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™å°±æ˜¯ä½ åº”è¯¥è®°ä½çš„æƒé‡ã€‚ç°åœ¨æ˜¯æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªåˆ†ç±»å™¨è®¡ç®—æ€§èƒ½æˆ–é˜¿å°”æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚è¿™æ˜¯æœ€ç»ˆé¢„æµ‹æ‰€éœ€çš„ï¼Œæ€§èƒ½çš„å…¬å¼è®¡ç®—å¦‚ä¸‹ã€‚æ‰€ä»¥æ˜¯0.5ä¹˜ä»¥1å‡å»è¯¯å·®çš„å¯¹æ•°é™¤ä»¥è¯¯å·®ã€‚
- en: So let me make this a little bit larger for youã€‚So this is the performanceã€‚And
    our error is always between 0 and1ã€‚ So I plotted alpha for different errors in
    this range hereã€‚ And we can see that it is equally distributed somewhere between
    a positive value here and a negative value hereã€‚So with a low errorï¼Œ we have a
    high positive valueï¼Œ and with a high error here close to oneã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘æŠŠè¿™ä¸ªæ”¾å¤§ä¸€ç‚¹ã€‚è¿™å°±æ˜¯æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯¯å·®å§‹ç»ˆåœ¨0å’Œ1ä¹‹é—´ã€‚æ‰€ä»¥æˆ‘åœ¨è¿™ä¸ªèŒƒå›´å†…ç»˜åˆ¶äº†ä¸åŒè¯¯å·®ä¸‹çš„é˜¿å°”æ³•ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒåœ¨ä¸€ä¸ªæ­£å€¼å’Œä¸€ä¸ªè´Ÿå€¼ä¹‹é—´å‡åŒ€åˆ†å¸ƒã€‚è¯¯å·®ä½æ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé«˜çš„æ­£å€¼ï¼Œè€Œå½“è¯¯å·®æ¥è¿‘1æ—¶ï¼Œæƒ…å†µåˆ™ç›¸åã€‚
- en: we have a high negative valueã€‚ So since we are flipping the decision then this
    will then be correct classifications again with a high contribution to the negative
    sideã€‚ So the side hereï¼Œ where the class is  -1ã€‚ So this is the concept of the
    alphaã€‚ And now we need the prediction So now if if we have understood all of thisã€‚
    then the final prediction is very easy to understandã€‚ So we just choose the sign
    hereã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ªå¾ˆé«˜çš„è´Ÿå€¼ã€‚å› æ­¤ï¼Œç”±äºæˆ‘ä»¬æ­£åœ¨ç¿»è½¬å†³ç­–ï¼Œè¿™å°†å†æ¬¡äº§ç”Ÿå¯¹è´Ÿé¢è´¡çŒ®è¾ƒé«˜çš„æ­£ç¡®åˆ†ç±»ã€‚æ‰€ä»¥è¿™é‡Œçš„ç±»åˆ«æ˜¯-1ã€‚è¿™å°±æ˜¯é˜¿å°”æ³•çš„æ¦‚å¿µã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦é¢„æµ‹ã€‚å¦‚æœæˆ‘ä»¬ç†è§£äº†è¿™ä¸€åˆ‡ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„é¢„æµ‹å°±å¾ˆå®¹æ˜“ç†è§£ã€‚æˆ‘ä»¬åªéœ€é€‰æ‹©è¿™é‡Œçš„ç¬¦å·ã€‚
- en: the sign of the sum over all predictions where we weigh each prediction with
    the performance of the classifierã€‚ So alpha times the prediction hereã€‚So the better
    our classifierã€‚ the more impact it has for the final predictionã€‚And the better
    the classifierã€‚ the more it points into the negative or positive sideã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰é¢„æµ‹æ€»å’Œçš„ç¬¦å·ï¼Œæˆ‘ä»¬ç”¨åˆ†ç±»å™¨çš„æ€§èƒ½æ¥åŠ æƒæ¯ä¸ªé¢„æµ‹ã€‚å› æ­¤ï¼Œè¿™é‡Œæ˜¯`alpha`ä¹˜ä»¥é¢„æµ‹ã€‚æˆ‘ä»¬çš„åˆ†ç±»å™¨è¶Šå¥½ï¼Œå¯¹æœ€ç»ˆé¢„æµ‹çš„å½±å“å°±è¶Šå¤§ã€‚åˆ†ç±»å™¨è¶Šå¥½ï¼Œå®ƒçš„æŒ‡å‘å°±è¶Šå€¾å‘äºè´Ÿä¾§æˆ–æ­£ä¾§ã€‚
- en: And then we take the better side as prediction for our classã€‚ Soï¼Œ yeahã€‚ that's
    the concept of the predictionã€‚And it can be a bit confusing with the different
    formulas and the side flippingã€‚ but the basic concept is not so difficultã€‚ And
    let's summarize all the different training steps that we must do in the codeã€‚
    Soï¼Œ first of allï¼Œ we initializeï¼Œ initialize over weights for each sample and set
    the value to one over Nã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†æ›´å¥½çš„ä¾§ä½œä¸ºæˆ‘ä»¬ç±»åˆ«çš„é¢„æµ‹ã€‚å› æ­¤ï¼Œæ˜¯çš„ã€‚è¿™å°±æ˜¯é¢„æµ‹çš„æ¦‚å¿µã€‚å¯¹äºä¸åŒçš„å…¬å¼å’Œä¾§ç¿»å¯èƒ½ä¼šæœ‰ç‚¹å›°æƒ‘ï¼Œä½†åŸºæœ¬æ¦‚å¿µå¹¶ä¸éš¾ã€‚ç°åœ¨è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹åœ¨ä»£ç ä¸­å¿…é¡»æ‰§è¡Œçš„ä¸åŒè®­ç»ƒæ­¥éª¤ã€‚å› æ­¤ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬åˆå§‹åŒ–æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼Œå¹¶å°†å€¼è®¾ç½®ä¸º1/Nã€‚
- en: Then we choose the number of weak learners we wantã€‚ And then we iterate over
    thisã€‚ And then we train each decision stampumpã€‚ So we do a greedy search to find
    the best split feature and the best split thresholdã€‚Then we calculate the error
    for this decision stumpã€‚ So this is with the formulaã€‚ the sum over the misclassified
    weightsã€‚ Then we also flip the error and the decision if it is greater than 05ã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬é€‰æ‹©æƒ³è¦çš„å¼±å­¦ä¹ è€…æ•°é‡ã€‚æ¥ç€æˆ‘ä»¬è¿›è¡Œè¿­ä»£ã€‚ç„¶åæˆ‘ä»¬è®­ç»ƒæ¯ä¸ªå†³ç­–æ ‘æ¡©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œè´ªå©ªæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„åˆ†è£‚ç‰¹å¾å’Œæœ€ä½³çš„åˆ†è£‚é˜ˆå€¼ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—è¿™ä¸ªå†³ç­–æ ‘æ¡©çš„é”™è¯¯ã€‚å› æ­¤ï¼Œè¿™æ˜¯ç”¨å…¬å¼è¡¨ç¤ºçš„ï¼Œé”™è¯¯æ˜¯åˆ†ç±»é”™è¯¯æƒé‡çš„æ€»å’Œã€‚ç„¶åå¦‚æœé”™è¯¯å¤§äº0.5ï¼Œæˆ‘ä»¬è¿˜è¦ç¿»è½¬é”™è¯¯å’Œå†³ç­–ã€‚
- en: Then we calculate the alpha with the formulaã€‚And then we need the predictionsã€‚
    And then with the predictions and the alphaï¼Œ we can then calculateã€‚ We can then
    update the weightsã€‚So this is what we must do in the code nowã€‚ And yeahã€‚ I promise
    you that since now that we have all the formulas and all the training steps hereã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ç”¨å…¬å¼è®¡ç®—`alpha`ã€‚æ¥ç€æˆ‘ä»¬éœ€è¦é¢„æµ‹ã€‚ç„¶ååˆ©ç”¨é¢„æµ‹å’Œ`alpha`ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥æ›´æ–°æƒé‡ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ç°åœ¨å¿…é¡»åœ¨ä»£ç ä¸­åšçš„äº‹æƒ…ã€‚æ˜¯çš„ã€‚æˆ‘å‘ä½ ä¿è¯ï¼Œä»ç°åœ¨å¼€å§‹æˆ‘ä»¬æœ‰äº†æ‰€æœ‰çš„å…¬å¼å’Œæ‰€æœ‰çš„è®­ç»ƒæ­¥éª¤ã€‚
- en: the implementation is pretty straightforward and should not be so hardã€‚ So let's
    jump to the codeã€‚![](img/736cbfb71e784efea36f6f12e912a093_1.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°æ˜¯ç›¸å½“ç›´æ¥çš„ï¼Œåº”è¯¥ä¸éš¾ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è·³åˆ°ä»£ç ã€‚![](img/736cbfb71e784efea36f6f12e912a093_1.png)
- en: So the first thing we do is import Nyã€‚ So import Ny S and Pã€‚ And this is the
    only module that we're gonna to needã€‚And now we create a class for the decision
    stampumpã€‚ So class decision stampumpã€‚And this gets an in itã€‚ So define an in itã€‚
    And this only has selfã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å¯¼å…¥`Ny`ã€‚æ‰€ä»¥å¯¼å…¥`Ny S`å’Œ`P`ã€‚è¿™æ˜¯æˆ‘ä»¬å°†éœ€è¦çš„å”¯ä¸€æ¨¡å—ã€‚ç„¶åæˆ‘ä»¬ä¸ºå†³ç­–æ ‘æ¡©åˆ›å»ºä¸€ä¸ªç±»ã€‚æ‰€ä»¥`class decision
    stampump`ã€‚ç„¶åè¿›è¡Œåˆå§‹åŒ–ã€‚æ‰€ä»¥å®šä¹‰ä¸€ä¸ªåˆå§‹åŒ–ï¼Œè¿™é‡Œåªéœ€è¦`self`ã€‚
- en: And here we want to store a couple of thingsã€‚ So the first thing that we want
    to store is the so called polarityã€‚ So self dot polarity equals oneã€‚ And this
    tells us if the sample should be classified or as-1 or plus one for the given
    thresholdã€‚ So if we want to look at the right or the left sideã€‚ And this is needed
    because if we want to flip the errorï¼Œ then we also must flip the polarityã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³è¦å­˜å‚¨å‡ ä¸ªä¸œè¥¿ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¦å­˜å‚¨æ‰€è°“çš„ææ€§ã€‚æ‰€ä»¥`self.dot polarity`ç­‰äº1ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬æ ·æœ¬åº”è¯¥è¢«åˆ†ç±»ä¸º-1è¿˜æ˜¯+1ï¼Œå–å†³äºç»™å®šçš„é˜ˆå€¼ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æƒ³æŸ¥çœ‹å·¦ä¾§æˆ–å³ä¾§ã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬æƒ³ç¿»è½¬é”™è¯¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå¿…é¡»ç¿»è½¬ææ€§ã€‚
- en: So this gets clearer in a secondã€‚And now the second thing that we want to store
    here is the feature indexã€‚ So self dot feature index equals none in the beginningã€‚
    And we also want to store the thresholdã€‚ So the split threshold self dot threshold
    equals none in the beginningã€‚ And we also want to store the variable for the performanceã€‚
    So the alphaã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šåœ¨ä¸€ç§’é’Ÿå†…å˜å¾—æ›´æ¸…æ™°ã€‚ç°åœ¨æˆ‘ä»¬è¦åœ¨è¿™é‡Œå­˜å‚¨çš„ç¬¬äºŒä»¶äº‹æ˜¯ç‰¹å¾ç´¢å¼•ã€‚æ‰€ä»¥`self.dot feature index`åœ¨å¼€å§‹æ—¶ç­‰äºNoneã€‚æˆ‘ä»¬è¿˜æƒ³å­˜å‚¨é˜ˆå€¼ã€‚å› æ­¤ï¼Œåˆ†è£‚é˜ˆå€¼`self.dot
    threshold`åœ¨å¼€å§‹æ—¶ç­‰äºNoneã€‚æˆ‘ä»¬è¿˜æƒ³å­˜å‚¨æ€§èƒ½å˜é‡ã€‚æ‰€ä»¥`alpha`ã€‚
- en: So we say self dot alpha equals noneã€‚ So this is the things that we want to
    storeã€‚ And then we also define a predict method for the decision stampumpã€‚ So
    we say define predict and it gets self and it gets xã€‚ So the samples that it should
    predictã€‚ And now what we want to do here isï¼Œ simply look at only one feature of
    this sample and then compare it with the threshold and say if it's smaller than
    it's  minus-1 and otherwise it's plus oneã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´self.dot alphaç­‰äºnoneã€‚è¿™æ˜¯æˆ‘ä»¬æƒ³è¦å­˜å‚¨çš„å†…å®¹ã€‚ç„¶åæˆ‘ä»¬è¿˜å®šä¹‰ä¸€ä¸ªé¢„æµ‹æ–¹æ³•ç”¨äºå†³ç­–æ ‘æ¡©ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å®šä¹‰é¢„æµ‹ï¼Œå®ƒæ¥å—selfå’Œxã€‚æ‰€ä»¥å®ƒåº”è¯¥é¢„æµ‹çš„æ ·æœ¬ã€‚ç°åœ¨æˆ‘ä»¬æƒ³åœ¨è¿™é‡Œåšçš„äº‹æƒ…æ˜¯ï¼ŒåªæŸ¥çœ‹è¿™ä¸ªæ ·æœ¬çš„ä¸€ä¸ªç‰¹å¾ï¼Œç„¶åå°†å…¶ä¸é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹çœ‹å¦‚æœå®ƒå°äºé˜ˆå€¼ï¼Œåˆ™ä¸º-1ï¼Œå¦åˆ™ä¸º+1ã€‚
- en: So that's the whole concept of the decision stampumpã€‚ So let's do thisã€‚ So let's
    say the number of samples equals X dot shape0ã€‚ and then let's get only this featureã€‚
    So let's say x column equals xã€‚![](img/736cbfb71e784efea36f6f12e912a093_3.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯å†³ç­–æ ‘æ¡©çš„æ•´ä¸ªæ¦‚å¿µã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿™æ ·åšã€‚æˆ‘ä»¬å¯ä»¥è¯´æ ·æœ¬çš„æ•°é‡ç­‰äºXçš„å½¢çŠ¶0ã€‚ç„¶åæˆ‘ä»¬åªè·å–è¿™ä¸ªç‰¹å¾ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´x columnç­‰äºxã€‚![](img/736cbfb71e784efea36f6f12e912a093_3.png)
- en: '![](img/736cbfb71e784efea36f6f12e912a093_4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/736cbfb71e784efea36f6f12e912a093_4.png)'
- en: And then we can use a colonã€‚ so we still want all the samplesã€‚ but only this
    feature index that we calculate later during the trainingã€‚ So self dot feature
    indexã€‚ and now we make our predictionsã€‚ So we say predictions equalsã€‚ and by defaultï¼Œ
    we say this is oneã€‚ So let's say nuy ones with the size of the number of samplesã€‚
    And then we must check the polarityã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å†’å·ã€‚æ‰€ä»¥æˆ‘ä»¬ä»ç„¶å¸Œæœ›æ‰€æœ‰çš„æ ·æœ¬ï¼Œä½†åªæœ‰è¿™ä¸ªç‰¹å¾ç´¢å¼•ï¼Œæˆ‘ä»¬ç¨ååœ¨è®­ç»ƒæœŸé—´è®¡ç®—ã€‚æ‰€ä»¥self.dot feature indexã€‚ç°åœ¨æˆ‘ä»¬åšå‡ºé¢„æµ‹ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´predictionsç­‰äºã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯´è¿™æ˜¯1ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´nuy
    onesçš„å¤§å°ç­‰äºæ ·æœ¬æ•°é‡ã€‚ç„¶åæˆ‘ä»¬å¿…é¡»æ£€æŸ¥ææ€§ã€‚
- en: So we say if self dot polarity equals equals oneã€‚ So this is the default caseã€‚
    then we say that all the predictions that are smaller where the feature vector
    is smaller than the threshold than it's minus-1ã€‚Soï¼Œ let's sayï¼Œ predictionsã€‚And
    then at these indexesã€‚ where x column is smaller than self dot threshold Then
    these predictions are -1ã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´å¦‚æœself.dot polarityç­‰äº1ã€‚è¿™æ˜¯é»˜è®¤æƒ…å†µã€‚é‚£ä¹ˆæˆ‘ä»¬è¯´æ‰€æœ‰é¢„æµ‹ä¸­ï¼Œç‰¹å¾å‘é‡å°äºé˜ˆå€¼çš„å°±æ˜¯-1ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œé¢„æµ‹ã€‚åœ¨è¿™äº›ç´¢å¼•å¤„ï¼Œx
    columnå°äºself.dot thresholdã€‚é‚£ä¹ˆè¿™äº›é¢„æµ‹å°±æ˜¯-1ã€‚
- en: And in the other caseï¼Œ elseã€‚ So if our polarity is -1ã€‚ then we want to do it
    exactly the other way aroundã€‚ So let me copy thisã€‚ But we want to say if the x
    value is greater than our thresholdã€‚ then these are the -1 predictionsã€‚Soï¼Œ yeahï¼Œ
    this is the all that our the decision stump is doingã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¦ä¸€ç§æƒ…å†µä¸‹ï¼Œelseã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬çš„ææ€§æ˜¯-1ã€‚é‚£ä¹ˆæˆ‘ä»¬æƒ³è¦å®Œå…¨åè¿‡æ¥åšã€‚æ‰€ä»¥è®©æˆ‘å¤åˆ¶è¿™ä¸ªã€‚ä½†æˆ‘ä»¬æƒ³è¯´å¦‚æœxå€¼å¤§äºæˆ‘ä»¬çš„é˜ˆå€¼ã€‚é‚£ä¹ˆè¿™äº›å°±æ˜¯-1çš„é¢„æµ‹ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„å†³ç­–æ ‘æ¡©æ‰€åšçš„æ‰€æœ‰äº‹æƒ…ã€‚
- en: and then we can return the predictionsã€‚So this is the class for the decision
    decision stampã€‚ And now we need a class for the actual add boost algorithmã€‚ So
    let's say class add boostã€‚And let's make this a small letterã€‚ And now we need
    a in it firstã€‚ So define a in itã€‚ And this gets selfã€‚ and the only parameter it
    gets is the number of classifiers that we wantã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è¿”å›é¢„æµ‹ç»“æœã€‚æ‰€ä»¥è¿™æ˜¯å†³ç­–æ ‘æ¡©çš„ç±»ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®é™…çš„åŠ æ³•å¢å¼ºç®—æ³•çš„ç±»ã€‚æˆ‘ä»¬å¯ä»¥è¯´class add boostã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªåå­—çš„å°å†™å¤„ç†ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ä¸ªåˆå§‹åŒ–å‡½æ•°ã€‚æ‰€ä»¥å®šä¹‰ä¸€ä¸ªåˆå§‹åŒ–å‡½æ•°ã€‚å®ƒæ¥å—selfï¼Œè€Œå”¯ä¸€çš„å‚æ•°æ˜¯æˆ‘ä»¬æƒ³è¦çš„åˆ†ç±»å™¨æ•°é‡ã€‚
- en: So let's say N Cï¼Œ L F equals5 by defaultã€‚ And then in the in itï¼Œ we store this
    numberã€‚ So we say self dot N Cï¼Œ L F equals the number of classifierã€‚ And then
    as alwaysã€‚ we want to implement the fit and the predict methodã€‚ So let's start
    with the fit methodã€‚ So let's say define fitã€‚ and it has self and it has X and
    yã€‚ So the training samples and the labelsã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´N Cï¼ŒL Fé»˜è®¤ä¸º5ã€‚ç„¶ååœ¨åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬å­˜å‚¨è¿™ä¸ªæ•°å­—ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´self.dot N Cï¼ŒL Fç­‰äºåˆ†ç±»å™¨çš„æ•°é‡ã€‚ç„¶ååƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬æƒ³è¦å®ç°fitå’Œpredictæ–¹æ³•ã€‚æˆ‘ä»¬å…ˆä»fitæ–¹æ³•å¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥è¯´å®šä¹‰fitï¼Œå®ƒæ¥å—selfã€Xå’Œyã€‚æ‰€ä»¥è®­ç»ƒæ ·æœ¬å’Œæ ‡ç­¾ã€‚
- en: And nowï¼Œ the first thing we do is to get the shape of this vectorã€‚ So the number
    of samples and also the number of featuresã€‚Features equals x dot shapeã€‚And then
    we want to inï¼Œ initialize our weightsã€‚ So in it the weightsã€‚ And as I saidã€‚ all
    the weights for each sample is set to one over n in the beginningã€‚ So let's say
    w equals nuyã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšçš„æ˜¯è·å–è¿™ä¸ªå‘é‡çš„å½¢çŠ¶ã€‚æ‰€ä»¥æ ·æœ¬çš„æ•°é‡ä»¥åŠç‰¹å¾çš„æ•°é‡ã€‚ç‰¹å¾ç­‰äºxçš„å½¢çŠ¶ã€‚ç„¶åæˆ‘ä»¬è¦åˆå§‹åŒ–æˆ‘ä»¬çš„æƒé‡ã€‚å› æ­¤ï¼Œåˆå§‹åŒ–æƒé‡ã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæ¯ä¸ªæ ·æœ¬çš„æƒé‡åœ¨å¼€å§‹æ—¶éƒ½è®¾ç½®ä¸º1/nã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´wç­‰äºnuyã€‚
- en: And then we can use a a method from numpy that is called fullã€‚ So nuumpy fullã€‚
    and it gets the size number of samplesã€‚ And then it gets an initial valueã€‚ And
    here we say oneã€‚Over the number of samplesã€‚ So this sets each value to this calculated
    valueã€‚And thenã€‚ this is our initializationã€‚ And now let's iterate through all
    the classifiers and do the trainingã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¥è‡ªnumpyçš„æ–¹æ³•ï¼Œå«åšfullã€‚æ‰€ä»¥numpy fullã€‚å®ƒè·å–æ ·æœ¬çš„å¤§å°æ•°å­—ã€‚ç„¶åå®ƒè·å–ä¸€ä¸ªåˆå§‹å€¼ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯´æ˜¯1ã€‚é™¤ä»¥æ ·æœ¬æ•°ã€‚è¿™å°†æ¯ä¸ªå€¼è®¾ç½®ä¸ºè¿™ä¸ªè®¡ç®—å€¼ã€‚ç„¶åï¼Œè¿™æ˜¯æˆ‘ä»¬çš„åˆå§‹åŒ–ã€‚ç°åœ¨è®©æˆ‘ä»¬éå†æ‰€æœ‰åˆ†ç±»å™¨å¹¶è¿›è¡Œè®­ç»ƒã€‚
- en: So firstï¼Œ we create a list where we want to store all the classifiersã€‚ So let's
    say self dot Cã€‚ L F'sã€‚ğŸ˜Šï¼Œè¯¶ã€‚And this is an empty list in the beginningã€‚ And now
    let's do the iterationã€‚ So let's say4 underscore in rangeã€‚ And here we have the
    number of classifiers that we specifyã€‚ So self dot Nï¼Œ Cï¼Œ Lï¼Œ Fã€‚And now what we
    want to do here is we want to do the greedy searchã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰åˆ†ç±»å™¨ã€‚å‡è®¾self.dot.C.LF'sã€‚ğŸ˜Šï¼Œè¿™æ˜¯ä¸€ä¸ªç©ºåˆ—è¡¨ã€‚ä¸€å¼€å§‹ã€‚ç°åœ¨è®©æˆ‘ä»¬è¿›è¡Œè¿­ä»£ã€‚å‡è®¾4_åœ¨èŒƒå›´å†…ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰æŒ‡å®šçš„åˆ†ç±»å™¨æ•°é‡ã€‚æ‰€ä»¥self.dot.N.C.L.Fã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦åšçš„æ˜¯è´ªå©ªæœç´¢ã€‚
- en: So we want to iterate over all the features and all the thresholdã€‚ So this is
    similar to the decision tree implementation that I did in another tutorialã€‚ And
    I recommend that you check that outï¼Œ tooã€‚ So we want to do a similar thing hereã€‚
    Soã€‚Firstã€‚ we create our classifierã€‚ So let's say C LF equals decision stumpã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³éå†æ‰€æœ‰ç‰¹å¾å’Œæ‰€æœ‰é˜ˆå€¼ã€‚è¿™ä¸æˆ‘åœ¨å¦ä¸€ä¸ªæ•™ç¨‹ä¸­å®ç°çš„å†³ç­–æ ‘ç±»ä¼¼ã€‚æˆ‘å»ºè®®ä½ ä¹ŸæŸ¥çœ‹ä¸€ä¸‹ã€‚æ‰€ä»¥æˆ‘ä»¬æƒ³åœ¨è¿™é‡Œåšç±»ä¼¼çš„äº‹æƒ…ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„åˆ†ç±»å™¨ã€‚å‡è®¾C.LFç­‰äºdecision
    stumpã€‚
- en: And now let's define a min error in the beginningã€‚ So we want to find the best
    feature valueã€‚ the split feature and the split threshold where this error then
    is minimumã€‚ So in the beginningã€‚ we just say this is floatã€‚Inï¼Œ so this is a very
    high numberã€‚ And now let's iterate over all the featuresã€‚ So let's say for featureã€‚For
    featureã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¸€å¼€å§‹å®šä¹‰ä¸€ä¸ªæœ€å°è¯¯å·®ã€‚æˆ‘ä»¬æƒ³æ‰¾åˆ°æœ€ä½³ç‰¹å¾å€¼ã€åˆ†å‰²ç‰¹å¾å’Œåˆ†å‰²é˜ˆå€¼ï¼Œä½¿å¾—è¿™ä¸ªè¯¯å·®æœ€å°ã€‚æ‰€ä»¥ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬åªæ˜¯è¯´è¿™æ˜¯floatã€‚åœ¨è¿™é‡Œï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸é«˜çš„æ•°å­—ã€‚ç°åœ¨è®©æˆ‘ä»¬éå†æ‰€æœ‰ç‰¹å¾ã€‚å‡è®¾å¯¹äºç‰¹å¾ã€‚å¯¹äºç‰¹å¾ã€‚
- en: I in the range ofã€‚And here we have the number of features that we got in the
    beginningã€‚And then we want to get only this featureã€‚ So let's say X columnã€‚ So
    this is similar to what we did hereã€‚So we can do the same thing and say x columnã€‚Equals
    thisã€‚ So all the samplesï¼Œ but only this feature indexã€‚ So I call it feature Iï¼Œ
    in this caseã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨èŒƒå›´å†…ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€å¼€å§‹è·å–çš„ç‰¹å¾æ•°é‡ã€‚ç„¶åæˆ‘ä»¬åªæƒ³è·å–è¿™ä¸ªç‰¹å¾ã€‚å‡è®¾Xåˆ—ã€‚è¿™ä¸æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„ç›¸ä¼¼ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åšåŒæ ·çš„äº‹æƒ…ï¼Œè®¾å®šxåˆ—ç­‰äºè¿™ä¸ªã€‚æ‰€ä»¥æ‰€æœ‰æ ·æœ¬ï¼Œä½†ä»…ä»…æ˜¯è¿™ä¸ªç‰¹å¾ç´¢å¼•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ç§°ä¹‹ä¸ºç‰¹å¾iã€‚
- en: And then we get one to get only the unique valuesã€‚ And these are our thresholdsã€‚
    So let's say thresholds equals Nai dot uniqueã€‚And hereï¼Œ the unique values of our
    columnã€‚ So x columnã€‚ And now we iterate over all the thresholdsã€‚ So let's say
    for threshold inã€‚Thresholdsã€‚And now what we want to do is we want to predict with
    the polarity 1 first and then calculate the error with the formulas that I showed
    you in the beginningã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è·å–å”¯ä¸€å€¼ã€‚å®ƒä»¬å°±æ˜¯æˆ‘ä»¬çš„é˜ˆå€¼ã€‚å‡è®¾é˜ˆå€¼ç­‰äºNai.dot.uniqueã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„åˆ—çš„å”¯ä¸€å€¼ã€‚æ‰€ä»¥xåˆ—ã€‚ç°åœ¨æˆ‘ä»¬éå†æ‰€æœ‰é˜ˆå€¼ã€‚å‡è®¾å¯¹äºé˜ˆå€¼åœ¨é˜ˆå€¼ä¸­ã€‚ç°åœ¨æˆ‘ä»¬æƒ³åšçš„æ˜¯å…ˆç”¨ææ€§1è¿›è¡Œé¢„æµ‹ï¼Œç„¶åç”¨æˆ‘åœ¨å¼€å§‹æ—¶å±•ç¤ºçš„å…¬å¼è®¡ç®—è¯¯å·®ã€‚
- en: So let's say our polarity equals  oneã€‚ And then let's do the predictionsã€‚ So
    predictions equalsã€‚ And this is similar to what we did hereã€‚ So in the beginningï¼Œ
    justï¼Œ it's just oneã€‚And then we use this formula hereã€‚ So since our polarity is  oneã€‚
    we have to compare it by saying if it's smaller than our thresholdã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬çš„ææ€§ç­‰äº1ã€‚ç„¶åè®©æˆ‘ä»¬è¿›è¡Œé¢„æµ‹ã€‚æ‰€ä»¥é¢„æµ‹ç­‰äºã€‚è¿™ä¸æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„ç›¸ä¼¼ã€‚æ‰€ä»¥ä¸€å¼€å§‹ï¼Œå®ƒåªæ˜¯1ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå…¬å¼ã€‚å› æ­¤ï¼Œç”±äºæˆ‘ä»¬çš„ææ€§æ˜¯1ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡æ¯”è¾ƒå®ƒæ˜¯å¦å°äºé˜ˆå€¼æ¥è¿›è¡Œæ¯”è¾ƒã€‚
- en: So predictions where our column valueï¼Œ our feature value is smaller than our
    threshold than there our predictions are -1ã€‚So now we predicted all the samples
    and now we want to calculate the error and as I saidã€‚ the error is the sum over
    the weights of the misclassified samplesã€‚ So let's get the misclassified weightã€‚
    So let's say misclassified equals W and the w where our y our training labels
    is not equal to the predictions that we just didã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥é¢„æµ‹ç»“æœæ˜¯æˆ‘ä»¬çš„åˆ—å€¼ï¼Œæˆ‘ä»¬çš„ç‰¹å¾å€¼å°äºé˜ˆå€¼æ—¶ï¼Œé¢„æµ‹ç»“æœä¸º-1ã€‚ç°åœ¨æˆ‘ä»¬é¢„æµ‹äº†æ‰€æœ‰æ ·æœ¬ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æƒ³è®¡ç®—è¯¯å·®ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œè¯¯å·®æ˜¯é”™è¯¯åˆ†ç±»æ ·æœ¬æƒé‡çš„æ€»å’Œã€‚è®©æˆ‘ä»¬è·å–é”™è¯¯åˆ†ç±»çš„æƒé‡ã€‚å‡è®¾é”™è¯¯åˆ†ç±»ç­‰äºWï¼Œä¸”wæ˜¯æˆ‘ä»¬çš„yï¼Œè®­ç»ƒæ ‡ç­¾ä¸ç­‰äºæˆ‘ä»¬åˆšæ‰çš„é¢„æµ‹ã€‚
- en: So these are the misclassified weightsã€‚ And now we want to simply calculate
    this sum over these weightsã€‚ So arrowã€‚Eror equals the sum over this misclassified
    weightã€‚ So this is the errorã€‚ And now we also want to flip our error If it is
    greater than 05ã€‚ So we say if error is greater than 05ã€‚ We simply say that our
    new error equals 1 the error and then we also flip the polarityã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯è¢«é”™è¯¯åˆ†ç±»çš„æƒé‡ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦ç®€å•åœ°è®¡ç®—è¿™äº›æƒé‡çš„æ€»å’Œã€‚æ‰€ä»¥ arrow.Eror ç­‰äºè¿™äº›è¢«é”™è¯¯åˆ†ç±»æƒé‡çš„æ€»å’Œã€‚è¿™æ˜¯è¯¯å·®ã€‚ç°åœ¨å¦‚æœè¯¯å·®å¤§äº 0.5ï¼Œæˆ‘ä»¬ä¹Ÿè¦ç¿»è½¬æˆ‘ä»¬çš„è¯¯å·®ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å¦‚æœè¯¯å·®å¤§äº
    0.5ï¼Œæˆ‘ä»¬ç®€å•åœ°è¯´æˆ‘ä»¬çš„æ–°è¯¯å·®ç­‰äº 1 å‡å»è¯¯å·®ï¼Œç„¶åæˆ‘ä»¬ä¹Ÿç¿»è½¬ææ€§ã€‚
- en: So we say P equals-1ã€‚So now we have our error and now we check if our error
    is smaller than the min errorã€‚ So let's say if error is smaller than the min errorã€‚Then
    this is our new min eraã€‚ So we say min error equals errorã€‚ And now this is the
    best current fit for our decision stampã€‚ So we want to store thisã€‚ So we say CF
    dot polarity equals P and sorryï¼Œ only Pã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´ P ç­‰äº -1ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†è¯¯å·®ï¼Œæ¥ä¸‹æ¥æ£€æŸ¥æˆ‘ä»¬çš„è¯¯å·®æ˜¯å¦å°äºæœ€å°è¯¯å·®ã€‚å¦‚æœè¯¯å·®å°äºæœ€å°è¯¯å·®ï¼Œé‚£ä¹ˆè¿™æ˜¯æˆ‘ä»¬çš„æ–°æœ€å°è¯¯å·®ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´æœ€å°è¯¯å·®ç­‰äºè¯¯å·®ã€‚è¿™æ˜¯æˆ‘ä»¬å½“å‰çš„æœ€ä½³æ‹Ÿåˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¦ä¿å­˜è¿™ä¸ªç»“æœã€‚æˆ‘ä»¬è¯´
    CF.dot.polarity ç­‰äº Pï¼Œå¯¹ä¸èµ·ï¼Œåªæœ‰ Pã€‚
- en: and we also want to store the threshold and the featureã€‚ So CF dot threshold
    equals the current threshold and C F dot feature index equals feature Iã€‚And yeahï¼Œ
    so this is the whole training loop for a classifierã€‚And now when we are done with
    post four loopsï¼Œ what we want to do here is have to check if I'm on the right
    indentã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æƒ³ä¿å­˜é˜ˆå€¼å’Œç‰¹å¾ã€‚å› æ­¤ï¼ŒCF.dot.threshold ç­‰äºå½“å‰é˜ˆå€¼ï¼ŒCF.dot.feature index ç­‰äºç‰¹å¾ Iã€‚æ˜¯çš„ï¼Œè¿™å°±æ˜¯åˆ†ç±»å™¨çš„æ•´ä¸ªè®­ç»ƒå¾ªç¯ã€‚å½“æˆ‘ä»¬å®Œæˆåå››ä¸ªå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬è¦æ£€æŸ¥æˆ‘æ˜¯å¦åœ¨æ­£ç¡®çš„ç¼©è¿›ä¸Šã€‚
- en: So now what we have to do is to calculate the performanceã€‚ so calculate alphaã€‚
    So we say and CF dot alpha equalsã€‚ and then we need this formula hereã€‚![](img/736cbfb71e784efea36f6f12e912a093_6.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦è®¡ç®—æ€§èƒ½ã€‚è®¡ç®— alphaã€‚å› æ­¤æˆ‘ä»¬è¯´ CF.dot.alpha ç­‰äºã€‚ç„¶åæˆ‘ä»¬éœ€è¦è¿™ä¸ªå…¬å¼ã€‚![](img/736cbfb71e784efea36f6f12e912a093_6.png)
- en: So 05 times the lock of1 minus the error divided by the errorã€‚![](img/736cbfb71e784efea36f6f12e912a093_8.png)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ 0.5 ä¹˜ä»¥ 1 å‡å»è¯¯å·®ï¼Œå†é™¤ä»¥è¯¯å·®ã€‚![](img/736cbfb71e784efea36f6f12e912a093_8.png)
- en: And we also use a little epsilon so that we don't divide by 0ã€‚ So let's say
    Es equals this small valueã€‚å—¯ã€‚And this is our epsilonã€‚ And now let's use the formulaã€‚
    So 05 times and nuy dot the lockã€‚And here we have one minus the errorã€‚And then
    divide by hereã€‚ let's say error plus our epsilonã€‚ And let's wrap this in another
    parenthsesã€‚ So this oneã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸€ä¸ªå°çš„ epsilonï¼Œä»¥é¿å…é™¤ä»¥ 0ã€‚æ‰€ä»¥æˆ‘ä»¬å‡è®¾ Es ç­‰äºè¿™ä¸ªå°å€¼ã€‚å—¯ã€‚è¿™æ˜¯æˆ‘ä»¬çš„ epsilonã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå…¬å¼ã€‚æ‰€ä»¥ 0.5
    ä¹˜ä»¥ nuy.dot.logã€‚è¿™é‡Œæœ‰ 1 å‡å»è¯¯å·®ã€‚ç„¶ååœ¨è¿™é‡Œé™¤ä»¥ï¼Œå‡è®¾è¯¯å·®åŠ ä¸Šæˆ‘ä»¬çš„ epsilonã€‚å¹¶å°†å…¶åŒ…è£¹åœ¨å¦ä¸€ä¸ªæ‹¬å·ä¸­ã€‚æ‰€ä»¥è¿™ä¸ªã€‚
- en: And let me check if this is correctã€‚ So let's do another one around this one
    hereã€‚And thenã€‚ this should be fineã€‚So this is our alphaã€‚ And now we want to update
    the weightsã€‚ And for thisã€‚ we also need the predictionã€‚ So let's check the formula
    formula againã€‚ So this is the old weight times the exponential function of minus
    the alpha that we just calculated times the actual predictions or the actual labels
    times the predictionsã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘æ£€æŸ¥ä¸€ä¸‹è¿™æ˜¯å¦æ­£ç¡®ã€‚å› æ­¤è®©æˆ‘ä»¬åœ¨è¿™ä¸ªå‘¨å›´å†åšä¸€ä¸ªæ£€æŸ¥ã€‚è¿™åº”è¯¥æ²¡é—®é¢˜ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çš„ alphaã€‚ç°åœ¨æˆ‘ä»¬æƒ³æ›´æ–°æƒé‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿˜éœ€è¦é¢„æµ‹å€¼ã€‚å› æ­¤è®©æˆ‘ä»¬å†æ¬¡æ£€æŸ¥å…¬å¼ã€‚æ‰€ä»¥è¿™æ˜¯æ—§æƒé‡ä¹˜ä»¥åˆšåˆšè®¡ç®—çš„
    alpha çš„æŒ‡æ•°å‡½æ•°ï¼Œä¹˜ä»¥å®é™…é¢„æµ‹å€¼æˆ–å®é™…æ ‡ç­¾ä¹˜ä»¥é¢„æµ‹å€¼ã€‚
- en: And then we normalize itã€‚ So this is the formula that we needã€‚ So let's write
    this hereã€‚ And let's first get the predictionsã€‚ So we can say predictions equalsã€‚
    and we already implemented this so we can simply say CF dot predictionã€‚![](img/736cbfb71e784efea36f6f12e912a093_10.png)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¿›è¡Œå½’ä¸€åŒ–ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦çš„å…¬å¼ã€‚è®©æˆ‘ä»¬åœ¨è¿™é‡Œå†™ä¸‹è¿™ä¸ªå…¬å¼ã€‚é¦–å…ˆè·å–é¢„æµ‹å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´ predictions ç­‰äºã€‚æˆ‘ä»¬å·²ç»å®ç°äº†è¿™ä¸€ç‚¹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´
    CF.dot.predictionã€‚![](img/736cbfb71e784efea36f6f12e912a093_10.png)
- en: '![](img/736cbfb71e784efea36f6f12e912a093_11.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/736cbfb71e784efea36f6f12e912a093_11.png)'
- en: Xã€‚Xï¼Œ yeahã€‚ So we get the column up hereã€‚ so we can put the whole x hereã€‚ And
    now we have the predictionsï¼Œ and now we can use them and update the weightsã€‚ So
    we say our weights is multiplied equalsã€‚ And then we say nuy Xã€‚ So the exponential
    functionã€‚ And then minus C F dot alphaã€‚Timesï¼Œ and here the actual labelsã€‚ and
    then times the predictionsã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Xã€‚Xï¼Œæ²¡é”™ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œè·å–åˆ—ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥æŠŠæ•´ä¸ª x æ”¾åœ¨è¿™é‡Œã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†é¢„æµ‹ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒä»¬å¹¶æ›´æ–°æƒé‡ã€‚å› æ­¤æˆ‘ä»¬è¯´æˆ‘ä»¬çš„æƒé‡ä¹˜ä»¥ç­‰äºã€‚ç„¶åæˆ‘ä»¬è¯´
    nuy Xã€‚å³æŒ‡æ•°å‡½æ•°ã€‚ç„¶åå‡å» CF.dot.alphaã€‚ä¹˜ä»¥ï¼Œå’Œè¿™é‡Œçš„å®é™…æ ‡ç­¾ã€‚å†ä¹˜ä»¥é¢„æµ‹å€¼ã€‚
- en: The timesï¼Œ predictionsã€‚Soï¼Œ and then we want to normalize itã€‚ So we divide it
    by the sum over this weightã€‚ So we say w divided equalsã€‚ And then here we say
    nuy dot sum Wã€‚And now we are doneã€‚ So we updated our weightsã€‚ and then we want
    to store this classifierã€‚ So we want to save itã€‚ So we say self dot Cï¼Œ Lã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¡æ•°ï¼Œé¢„æµ‹ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬æƒ³è¦è¿›è¡Œå½’ä¸€åŒ–ã€‚æˆ‘ä»¬å°†å…¶é™¤ä»¥è¿™ä¸ªæƒé‡çš„æ€»å’Œã€‚æ‰€ä»¥æˆ‘ä»¬è¯´wé™¤ä»¥ç­‰äºã€‚ç„¶åè¿™é‡Œæˆ‘ä»¬è¯´nuyçš„sum Wã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚æˆ‘ä»¬æ›´æ–°äº†æƒé‡ï¼Œç„¶åæˆ‘ä»¬æƒ³è¦å­˜å‚¨è¿™ä¸ªåˆ†ç±»å™¨ã€‚æˆ‘ä»¬æƒ³è¦ä¿å­˜å®ƒã€‚æ‰€ä»¥æˆ‘ä»¬è¯´selfçš„Cï¼ŒLã€‚
- en: F dot append the current classifierã€‚ So we append Cï¼Œ L Fã€‚And now we are done
    with the fit methodã€‚ So this is the whole training of our add boost classifierã€‚Andã€‚Nowï¼Œ
    what we also need isï¼Œ of courseã€‚ we want to have the predict methodã€‚ So let's
    implement this down hereã€‚ So let's say defineï¼Œ predictã€‚ and here it gets self
    and it also gets Xã€‚ And now this is the formula that I showed you hereã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Fç‚¹è¿½åŠ å½“å‰åˆ†ç±»å™¨ã€‚æ‰€ä»¥æˆ‘ä»¬è¿½åŠ Cï¼ŒL Fã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†fitæ–¹æ³•ã€‚è¿™æ˜¯æˆ‘ä»¬add booståˆ†ç±»å™¨çš„æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å½“ç„¶ä¹Ÿéœ€è¦predictæ–¹æ³•ã€‚è®©æˆ‘ä»¬åœ¨è¿™é‡Œå®ç°å®ƒã€‚å‡è®¾å®šä¹‰predictï¼Œè¿™é‡Œå¾—åˆ°selfï¼Œä¹Ÿå¾—åˆ°Xã€‚è¿™æ˜¯æˆ‘ä¹‹å‰ç»™ä½ å±•ç¤ºçš„å…¬å¼ã€‚
- en: So we look at the s of the sumã€‚ And here we multiply each alpha with the with
    the predictionã€‚ Yeahã€‚ So let's do thisã€‚![](img/736cbfb71e784efea36f6f12e912a093_13.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŸ¥çœ‹æ€»å’Œçš„sã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å°†æ¯ä¸ªalphaä¸é¢„æµ‹ç›¸ä¹˜ã€‚æ˜¯çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥åšè¿™ä¸ªã€‚![](img/736cbfb71e784efea36f6f12e912a093_13.png)
- en: '![](img/736cbfb71e784efea36f6f12e912a093_14.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/736cbfb71e784efea36f6f12e912a093_14.png)'
- en: So let's say Cï¼Œ L F dot Prats equalsã€‚ And here we use list comprehensionã€‚ And
    then we do this for each of the classifiersã€‚ So we say Cï¼Œ L F dot alpha timesã€‚
    And here we use the predictions Cï¼Œ L F dot predictã€‚ And here we want to predict
    Xã€‚ And we want to do this for each of the start classifiersã€‚ So we say for Cï¼Œ
    L F in self dot Cï¼Œ L Fsã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾Cï¼ŒL Fçš„Pratsç­‰äºã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªåˆ†ç±»å™¨è¿›è¡Œæ­¤æ“ä½œã€‚æˆ‘ä»¬è¯´Cï¼ŒL Fçš„alphaä¹˜ä»¥ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨é¢„æµ‹Cï¼ŒL Fçš„predictã€‚æˆ‘ä»¬æƒ³è¦å¯¹Xè¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬è¦å¯¹æ¯ä¸ªåˆå§‹åˆ†ç±»å™¨è¿™æ ·åšã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å¯¹äºCï¼ŒL
    Fåœ¨selfçš„Cï¼ŒL Fsä¸­ã€‚
- en: So these are all the predictions in the sumã€‚ And now we need to calculate the
    sumã€‚ So we say y pre equals and then nuy dot sumã€‚ And here we say Cï¼Œ L F pres
    and along the axis 0ã€‚ So now we have the sumã€‚ And now the very last thing that
    we need to do is to look at the sã€‚ So we say y pret equals nuy dot s sign of thisï¼Œ
    Y pretã€‚And this is our final predictionã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›éƒ½æ˜¯é¢„æµ‹çš„æ€»å’Œã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦è®¡ç®—æ€»å’Œã€‚æˆ‘ä»¬è¯´y preç­‰äºnuyçš„sumã€‚è¿™é‡Œæˆ‘ä»¬è¯´Cï¼ŒL F presï¼Œå¹¶æ²¿ç€axis 0ã€‚ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†æ€»å’Œã€‚æœ€åæˆ‘ä»¬éœ€è¦æŸ¥çœ‹sã€‚æ‰€ä»¥æˆ‘ä»¬è¯´y
    pretç­‰äºnuyçš„sign of thisï¼ŒY pretã€‚è¿™æ˜¯æˆ‘ä»¬çš„æœ€ç»ˆé¢„æµ‹ã€‚
- en: And then we can return thisã€‚ So let's return why pretã€‚And now we should be doneã€‚
    So now we have the fit method and the predict methodã€‚And now here I've already
    written a little test scriptã€‚ So here I import this class that we just createdã€‚
    So from addda boost to import adddabuã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è¿”å›è¿™ä¸ªã€‚æˆ‘ä»¬è¯´è¿”å›y pretã€‚ç°åœ¨æˆ‘ä»¬åº”è¯¥å®Œæˆäº†ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†fitæ–¹æ³•å’Œpredictæ–¹æ³•ã€‚è¿™é‡Œæˆ‘å·²ç»å†™äº†ä¸€ä¸ªå°çš„æµ‹è¯•è„šæœ¬ã€‚æ‰€ä»¥æˆ‘ä»addda
    boostä¸­å¯¼å…¥æˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„ç±»ã€‚
- en: Then I also have a accuracy measure hereã€‚ And then in this exampleã€‚ we load
    the breast concert data set from the SQ learnide assetsã€‚ and then the important
    thing that we must do here is to set all the labels that are 0 at the moment to-1
    because add boost needs the labels as-1 and plus1ã€‚ and then we do a train test
    split as alwaysï¼Œ And then here we create a add boost classifierã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªå‡†ç¡®ç‡æµ‹é‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»SQ learnide assetsåŠ è½½ä¹³è…ºç™Œæ•°æ®é›†ã€‚é‡è¦çš„æ˜¯æˆ‘ä»¬å¿…é¡»å°†ç›®å‰æ‰€æœ‰æ ‡ç­¾ä¸º0çš„è®¾ç½®ä¸º-1ï¼Œå› ä¸ºadd
    boostéœ€è¦æ ‡ç­¾ä¸º-1å’Œ+1ã€‚ç„¶åæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·è¿›è¡Œè®­ç»ƒæµ‹è¯•åˆ†å‰²ï¼Œç„¶ååœ¨è¿™é‡Œåˆ›å»ºä¸€ä¸ªadd booståˆ†ç±»å™¨ã€‚
- en: and in this caseï¼Œ I put in five classifierã€‚ Then we call the fit methodã€‚ and
    then we call the predict method and then we calculate the accuracyã€‚So this is
    the test scriptã€‚ So let's run this and hope that everything's workingã€‚ So let's
    say Python a boost testã€‚ it's called and hit enterï¼Œ and it's runningï¼Œ and it's
    calculatingã€‚And I hope that it's workingã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æ”¾å…¥äº†äº”ä¸ªåˆ†ç±»å™¨ã€‚ç„¶åæˆ‘ä»¬è°ƒç”¨fitæ–¹æ³•ã€‚æ¥ç€æˆ‘ä»¬è°ƒç”¨predictæ–¹æ³•ï¼Œç„¶åè®¡ç®—å‡†ç¡®ç‡ã€‚æ‰€ä»¥è¿™æ˜¯æµ‹è¯•è„šæœ¬ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå¸Œæœ›ä¸€åˆ‡æ­£å¸¸ã€‚å‡è®¾æˆ‘ä»¬ç”¨Pythonæ¥è¿›è¡Œboostæµ‹è¯•ï¼Œè°ƒç”¨å®ƒå¹¶æŒ‰ä¸‹å›è½¦ï¼Œå®ƒæ­£åœ¨è¿è¡Œï¼Œå¹¶åœ¨è®¡ç®—ã€‚æˆ‘å¸Œæœ›å®ƒèƒ½æ­£å¸¸å·¥ä½œã€‚
- en: And now we are doneã€‚ Soï¼Œ yeahï¼Œ so here we have a accuracy accuracyã€‚ and it's
    pretty good in this exampleã€‚ So we have 094ã€‚ So we see that it's workingã€‚And yeahã€‚
    I hope you enjoyed the tutorial and see you next timeï¼Œ byeã€‚![](img/736cbfb71e784efea36f6f12e912a093_16.png)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå‡†ç¡®ç‡ï¼Œè€Œä¸”åœ¨è¿™ä¸ªä¾‹å­ä¸­è¡¨ç°ç›¸å½“ä¸é”™ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰094ã€‚æˆ‘ä»¬çœ‹åˆ°å®ƒåœ¨å·¥ä½œã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œä¸‹æ¬¡è§ï¼Œæ‹œï¼![](img/736cbfb71e784efea36f6f12e912a093_16.png)
