- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P40：L7.4- 在Keras中用于半监督学习的GANS
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P40：L7.4- 在Keras中用于半监督学习的GANS
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeffine， welcome to applications of deep neural Networks with Washington
    University In this video we're going to look at how we can use GANs to generate
    additional training data for the latest on my AI course and projects click subscribe
    and the bell next to it to be notified of every new video GANs have a wide array
    of uses beyond just the face generation that you often see them used for they
    can definitely generate other types of images but they can also work on tabular
    data and really any sort of data where you are attempting to have a neural network
    that is generating data that should be real or or could be classified as fake
    the key element to having something as a GN is having that discriminator that
    tells the difference and the generator that actuallys the data Another area that
    we are seeing GANs used for a great deal is in the area of semisupervised training
    So let's first talk about what semisupervised training actually is and see how
    a GAN can be used to implement this First let's talk about supervised。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是Jeffine，欢迎来到华盛顿大学的深度神经网络应用。在这段视频中，我们将探讨如何使用GAN生成额外的训练数据。想了解我最新的AI课程和项目，请点击订阅并按旁边的铃铛，以便接收每个新视频的通知。GAN有广泛的用途，不仅仅是你常见的面部生成，它们确实可以生成其他类型的图像，但也可以处理表格数据和任何试图让神经网络生成应该是真实的或可能被分类为虚假的数据。使其成为GAN的关键要素是有一个区分器来区分，并且生成器实际生成数据。我们看到GAN被大量用于半监督训练。因此，让我们先谈谈什么是半监督训练，并看看如何使用GAN来实现这一点。首先，让我们谈谈监督训练。
- en: Training and unsupervised training， which you've probably seen in previous machine
    learning literature。 but just in case you haven't supervised training is what
    we've been doing up to this point。 I would say probably the vast majority of this
    class is in the area of supervised learning This is where you have multiple Xs
    in the case of tabular data or grids and other things in the case of image data
    but you have some sort of input coming in。 which is the X。And you know what the
    correct Ys are。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和无监督训练，你可能在之前的机器学习文献中见过。但以防你没有，监督训练就是我们到目前为止所做的。我会说本课程绝大部分内容都是在监督学习领域。在这里，你有多个X，在表格数据或图像数据的情况下，你有某种输入进来，也就是X。而你知道正确的Y是什么。
- en: you are going to train the model to produce these Y's when you have these x's
    because later on you're going to have x's coming in where you don't know what
    the Y is and that's where you want the neural network or other model to be able
    to give you some estimate as far as what the Y value is going to actually be unsupervised
    training is where we have the X's。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将训练模型以在拥有这些x时产生这些Y，因为后来你会有未知Y的x进来，而这正是你希望神经网络或其他模型能够给出Y值估计的地方，无监督训练是指我们拥有X的情况。
- en: it could look just like this， it would work with image data tabular or really
    just about anything。 but there is no Y where letting the neural network or whatever
    model it is and you don't typically by the way use neural networks for unsupervised
    training this is usually the area of things like K means。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 它可能看起来就像这样，它可以处理图像数据、表格数据或几乎任何东西。但没有Y，允许神经网络或任何模型工作，通常你不使用神经网络进行无监督训练，这通常是像K均值这样的领域。
- en: clustering and other things。Your classic unsupervised training is just going
    to take the inputs and cluster them in such a way so that similar ones are together
    these could be similar images。 these could be similar inputs in tabular data a
    variety of things semisupervised training it's actually much closer to supervised
    training I would say than unsupervised and this is where GAs really shine and
    semisupervised training you have x's just like you have in these others but you
    don't have a label or a Y for every single one of them you might have a small
    number of them by no means have the complete data set labeled traditionally what
    would be done is these values that were not labeled would be left out because
    there was no way to feed them into traditional supervised learning or you would
    train it on the ones that you did have Y's for with classic back propagation or
    however you were training that particular model then you would create predict。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和其他方法。你的经典无监督训练只会将输入进行聚类，使得相似的样本在一起，这些可能是相似的图像，也可能是表格数据中的相似输入等多种情况。半监督训练实际上比无监督训练更接近有监督训练，我认为这就是GA的优势所在。在半监督训练中，你有x，就像在其他情况下一样，但并不是每一个都有标签或Y，你可能只有少量数据，并没有完整的数据集按照传统方式标记。通常情况下，未标记的值会被忽略，因为没有办法将它们输入到传统的有监督学习中，或者你会在你拥有Y的那些数据上进行训练，使用经典的反向传播或任何你在训练特定模型时使用的方法，然后你会创建预测。
- en: Why predictions for all the missing values and then retrain the whole thing
    on the predictive values with the others In practice。 I never had a great deal
    of success with that technique。 but there is some theoretical basis for it with
    semisupervised training and GNs will see that there's a way that we are able to
    actually make use of these Now semiupervised training。 this does make sense from
    a biological standpoint if you think about a child who is seeing all sorts of
    vehicles as they go about their daily lives with their parents or whoever they're
    with and they're seeing all these vehicles as they pass on the street and they're
    not labeled。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要预测所有缺失值，然后在预测值和其他值上重新训练整个模型。在实践中，我在这种技术上从未取得很大的成功，但在半监督训练和GNs中确实有一些理论基础。如果你考虑一个孩子在日常生活中看到各种交通工具的情景，他们与父母或其他人在一起，看到街上经过的各种车辆，而这些车辆并没有标签。
- en: nobody is telling them even that that's a vehicle seeing just a barrage of images
    as as they grow up。 they learn edges， they learn other sorts of things they learn
    how to classify if something is on top of something else just by observing there's
    no particular labels。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人告诉他们那是交通工具，他们只是看到大量的图像。随着他们的成长，他们学习边缘，学习其他各种事物，他们通过观察学习如何分类某物是否在其他物体之上，而不需要特定的标签。
- en: '![](img/f767f7ef74d84f44593821b6fdff2880_1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f767f7ef74d84f44593821b6fdff2880_1.png)'
- en: '![](img/f767f7ef74d84f44593821b6fdff2880_2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f767f7ef74d84f44593821b6fdff2880_2.png)'
- en: eventually somebody says hey， that's a car， that's a bus that's a train that's
    a bicycle using that small handful of labels that they're given when somebody
    actually tells them what they're looking at or they verify it independently that
    is semi my supervised training because it is building on those years and years
    of having unlabeled data that they didn't know what they were looking at。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，有人说，“嘿，那是一辆车，那是一辆公交车，那是一列火车，那是一辆自行车。”利用他们得到的那小部分标签，当有人告诉他们自己在看什么或他们独立验证时，这就是半监督训练，因为它建立在多年来未标记数据的基础上，而他们不知道自己在看什么。
- en: but they knew they were looking at something and it just it gives them additional
    reference。 that's exactly the same thing with supervised training these values，
    even though we don't have Y。 they're still valuable for the neural network to
    be learning structure in this data as it is learning to predict the ones that
    we do actually in fact have the wise car so let's look at the structure for this
    this is the structure of a normal imagegene GA baseline so to speak where they
    research started we saw this before but just to quickly review we have actual
    images they go into a discri。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但他们知道自己在看着某样东西，这给他们提供了额外的参考。这与有监督训练完全相同，即使我们没有Y，这些值对神经网络学习数据结构仍然是有价值的，因为它在学习预测我们实际上拥有的那些信息。所以让我们来看一下结构，这是一个普通的图像生成GA基线的结构，可以说是研究的起点。我们之前见过这个，但为了快速回顾，我们有实际的图像，它们进入一个判别器。
- en: '![](img/f767f7ef74d84f44593821b6fdff2880_4.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f767f7ef74d84f44593821b6fdff2880_4.png)'
- en: And we have the generated images that the generator。 So the cyan pieces。 Those
    are the two neural networks random seed values are causing that generator to generate
    images。 The discriminator is learning to better and better discriminate between
    actual and generated the generator is learning to create better and better images
    that fool the discriminator Now once this is all done。 you keep the generator
    because it generates images for you and you likely throw away the discriminator
    it was just there for the generator to practice against we'll see that this flips
    for semisupervised learning in semisupervised learning we care about the discriminator
    and not so much the generator we typically throw the generator away。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有生成的图像，这些图像是由生成器产生的。所以青色部分。这两个神经网络的随机种子值导致生成器生成图像。鉴别器正在不断学习，更好地区分真实图像和生成图像，而生成器也在不断学习，创造出更好更能欺骗鉴别器的图像。一旦这一切完成，你就保留生成器，因为它为你生成图像，而你可能会丢掉鉴别器，它只是用来让生成器进行练习的。我们会看到在半监督学习中这一点会有所改变，在半监督学习中，我们更关心鉴别器，而不是生成器，通常我们会丢掉生成器。
- en: This is how you would train a semisupervised classification neural network。
    It's very。 very similar to the diagram that we just looked at in this case。 we're
    looking at how we would train it on tabular data， say medical record。 the discriminator
    would learn to tell。The difference between a fake medical record or whatever the
    generator is generating。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你训练一个半监督分类神经网络的方式。这与我们刚才看到的图非常相似。在这种情况下，我们关注的是如何在表格数据上进行训练，比如医疗记录。鉴别器会学习识别生成的虚假医疗记录和真实医疗记录之间的区别。
- en: this part's all the same as the previous one as is this part。 the difference
    is we're training it now to tell not just the difference between fake and real。
    these are the real and this is fake。We're teaching it to learn classes so there's
    four different classes of say medical record that we're looking at maybe four
    different health levels we're teaching it as a classification neural network to
    classify between five things the four classes that we're actually interested in
    and is it a fake once we're done training the whole thing we now have this discriminator
    that can tell the difference between fake and what what the classes are we also
    have the generator that is able to generate these fake medical records but we
    can then throw away the generator and we'll use the discriminator really truly
    as our actual neural network now for the medical records where we don't have the
    why so we're missing this we still feed those in it's just now we're evaluating
    it not based on if it classified it correctly but just if it knew the difference
    between fake and real the streethouse view data set is a image data that is often
    used to demonstrate semisupervised game。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分与之前的一样，这部分也是。不同之处在于我们现在训练它，不仅仅是区分虚假和真实。这些是真实的，而这是虚假的。我们在教它学习类别，因此我们正在关注的可能是四种不同类别的医疗记录，或许是四种不同的健康水平。我们在教它作为分类神经网络，区分五个事物：四个我们真正感兴趣的类别，以及它是否是虚假的。训练完成后，我们得到了这个能够区分虚假和真实类别的鉴别器，我们还拥有一个能够生成这些虚假医疗记录的生成器，但我们可以丢掉生成器，真正使用鉴别器作为我们的实际神经网络，现在用于医疗记录，尤其是当我们缺少标签时。所以我们仍然将这些数据输入进去，只是现在我们评估它时，不是基于它是否正确分类，而是基于它是否能够区分虚假和真实。街道房屋视图数据集是一种图像数据，常用于展示半监督学习。
- en: Lning and I have a link to Akira's example external to to this class that demonstrates
    this if you're interested in this sort of technology but what this does is you
    have data on these addresses from images that were taken on the sides of buildings
    and not all of those are labeled or you simulate them not all being labeled and
    you see that theganN is capable of learning to classify these 10 digit types even
    though it doesn't have labels on each of those Now if you want to do the same
    thing for regression it becomes very similar you have two outputs so you have
    a multi output neural network one is the actual regression value that you're trying
    to train it on and the other is the probability that it's a fake record being
    generated Now these I'm doing tabular as just the example again these could be
    medical records and perhaps the regression output would be a health level or maybe
    a guess。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Lning 和我有一个链接到 Akira 的示例，外部的类展示了这一点，如果你对这种技术感兴趣的话。这个示例使用了建筑物侧面拍摄的图像上的数据，并不是所有的数据都有标签，或者你可以模拟这些数据没有标签的情况，你会发现
    ganN 能够学习分类这些 10 位数字类型，即使每个数字都没有标签。如果你想为回归做同样的事情，这会变得非常相似，你会有两个输出，所以你有一个多输出神经网络，一个是你试图训练的实际回归值，另一个是生成的假记录的概率。这些我正在用表格作为示例，这些可以是医疗记录，也许回归输出将是健康水平，或者是患者的年龄，或其他值，或许是他们当前是否有疾病的预测。
- en: How old the patient is or some other value， perhaps if they have a current disease
    or not a prediction so it' it's doing these same two things when you feed in medical
    records where we don't know the Y output then we want to see that this regression
    on the fake record when we're feeding in values where we have the medical record
    we don't have the Y we just want to make sure that the probability that the fake
    record is fairly high and that's built into the training。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在处理医疗记录时，当我们不知道 Y 输出时，这两个相同的操作。当我们输入医疗记录时，我们希望看到这个假记录的回归，当我们输入值时，我们有医疗记录但没有
    Y，我们只想确保假记录的概率相当高，而这被纳入了训练中。
- en: we don't so much care about what it's regressing on or what the regression output
    is for ones where we do have it。 we're penalizing it based on how close or how
    far away。 it was from the expected why from this and just like the classification
    one when we're all done with this we throw away the generator and the discriminator
    becomes the semisupervised neural network that was trained on this Now if you
    want to go further with this semiupervised learning technique I've given you a
    couple of links of articles that I。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不太在乎它在回归什么，或者对于我们确实有回归输出的情况，回归输出是什么。我们会根据它与期望的 Y 之间的距离来惩罚它。就像分类问题一样，当我们完成这一切时，我们会抛弃生成器，鉴别器成为在此基础上训练的半监督神经网络。如果你想进一步了解这种半监督学习技术，我给了你几篇文章的链接。
- en: Found useful for this There is a link to the actual house dataset set。 That's
    a pretty interesting dataset set to look at。 It has all house numbers above。 You
    can deal with it several ways。 you can deal with classifying the individual digits
    give thetangles the bounding rectangle of the entire set of digits if you want
    to so you can be classifying digits so you can be classifying the entire address。
    It just depends on how you want to set up the problem The examples that I give
    you here we're using digit is the paper that first started looking at this on
    supervised representation learning with deep convolutional generative G adversarial
    network to in the this're going to take a look at some of the cutting edge and
    a very active area of research content changes often so。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对此我发现很有用。有一个链接到实际的房屋数据集。这是一个相当有趣的数据集，值得关注。它包含所有房屋号码。你可以用几种方式处理它。你可以处理单个数字的分类，给出整个数字集的边界矩形，如果你愿意的话，所以你可以分类数字，也可以分类整个地址。这完全取决于你如何设置问题。我在这里给你的例子使用数字，这是首个开始研究无监督表示学习的论文，使用深度卷积生成对抗网络。我们将看看一些前沿的、非常活跃的研究领域，内容常常变化。
- en: '![](img/f767f7ef74d84f44593821b6fdff2880_6.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f767f7ef74d84f44593821b6fdff2880_6.png)'
- en: To the channel to stay up to date on this course and other topics in artificial
    intelligence。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请关注该频道，以获取有关本课程和人工智能其他主题的最新信息。
