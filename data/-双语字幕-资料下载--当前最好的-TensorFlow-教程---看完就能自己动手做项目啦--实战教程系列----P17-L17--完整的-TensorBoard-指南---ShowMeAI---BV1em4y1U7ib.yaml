- en: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P17：L17- 完整的 TensorBoard
    指南 - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P17：L17- 完整的 TensorBoard
    指南 - ShowMeAI - BV1em4y1U7ib
- en: The goal of this video is for you to get a thorough understanding of how to
    use Tensorboard to easier understand the bug and modify our models。This is a long
    video because there is so much to cover when it comes to Tensor board。 So let
    me first of all show you an overview of the things we're going to learn how to
    do in this tutorial。 We're going to start off learning about what is perhaps the
    most basic but also probably the most useful thing which is obtaining these accuracy
    and these loss plots。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本视频的目标是让你充分理解如何使用 TensorBoard 来更轻松地理解错误并修改我们的模型。这个视频较长，因为涉及的内容太多，关于 TensorBoard
    的内容。因此，让我首先给你展示一下我们将在本教程中学习的内容概览。我们将从学习或许是最基础但也是最有用的事情开始，即获取这些准确度和损失图。
- en: and in this example right here， you see the accuracy plot when training our
    model with varying learning rates。 and then we're going to move on and learn about
    how to visualize images So in this example here we're performed some minor data
    augmentation in turning a very few percentage of the images to grayscale and so
    visualizing these changes can be very useful to make sure that what we intend
    to happen is actually what is going on。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，你会看到训练我们模型时使用不同学习率的准确度图。接下来，我们将学习如何可视化图像。在这个示例中，我们对图像进行了少量的数据增强，将极少数图像转换为灰度，因此可视化这些变化将非常有用，以确保我们期望发生的事情确实正在发生。
- en: So for example， here we have a horse that has been converted to a grayscale
    image Also we're going to see how to both create and。these confusion matrices
    to tensor board where we have the predicted label on the y axis and then we have
    the true label on the X axis in this way we can see what and where the model is
    misclassifying images。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，在这里我们有一匹马，它已经被转换为灰度图像。我们还将看到如何创建混淆矩阵并将其发送到 TensorBoard，在这里我们将预测标签放在 Y 轴上，真实标签放在
    X 轴上，这样我们可以看到模型在何处错误分类图像。
- en: So for example， here we have that it correctly predicts airplane 65% of the
    time but in 7% of the time it's misclassifying a bird as an airplane Also we can
    see that it predicts cat correctly 40% of the time where the largest mistake happens
    when it misclassifies it as a dog in about 21% of the cases other useful things
    is that you can visualize the graph of our model so here we have a very。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '比如，在这里我们看到它正确预测飞机的概率为 65%，但在 7% 的时间里，它错误地将鸟类分类为飞机。我们还可以看到它正确预测猫的概率为 40%，而最大的错误发生在它将猫误分类为狗的情况，大约占
    21%。其他有用的功能是你可以可视化我们模型的图形，在这里我们有一个非常。  '
- en: very simple model which takes in two inputs X and Y performs a matrix multiply
    and then send that through a re and that will be the output of the model。And so
    this can be useful if you're building more complicated or complex models like
    Resnets where you want to make sure that all of the skip connections are at the
    correct places。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的模型，它接受两个输入 X 和 Y，执行矩阵乘法，然后将其通过一个激活函数，这将是模型的输出。因此，如果你正在构建更复杂的模型，比如 Resnets，这种模型可以确保所有跳跃连接位于正确的位置，这将是非常有用的。
- en: or if you just want to get a better understanding and see more visually how
    the model looks like other cool things that you can do with Tensor board is then
    you can visualize and see the distribution of the various parameters of your model
    so this can be useful to debug if you get error messages and you want to see exactly
    where in the model the error is occurring then these distribution plots can be
    useful to know which layer the default lies in but we also have a lot more things
    to look at so were for example going to do hyperparameter search using the Hpars
    API of Tensor board with this you can see what parameters to use for your model。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你只是想更好地理解并更直观地查看模型的外观，TensorBoard 还能做其他酷事，你可以可视化并查看模型各种参数的分布，这对调试非常有用。如果你收到错误信息并想确切知道错误发生在模型的哪个地方，那么这些分布图将有助于你了解错误发生在哪一层。我们还有更多的内容要查看，比如我们将使用
    TensorBoard 的 Hpars API 进行超参数搜索，这样你就可以看到哪些参数适合你的模型。
- en: By seeing the different correlations between in this case the dropout。 the learning
    rate and then the number of units and then how that finally corresponds to the
    accuracy after a single epoC run so to easier understand this right here we can
    restrict our attention to only looking at what corresponds to the best acuracies
    and what we can see here is that the model preferably wants quite low dropout
    and then it wants a very high learning rate and also high number of units so in
    this case we're checking the accuracy on the training set so that's probably white
    wants very low levels of dropout and we are also going to learn how to use the
    projector tab of Tensor board which is pretty awesome visually and you can use
    different algorithms like the TSneE and then the PCA to get an understanding of
    how your model is learning to represent these images。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察在这种情况下丢弃率、学习率和单元数之间的不同相关性，以及这些因素如何最终影响单次训练后准确率。为了更容易理解这一点，我们可以将注意力限制在仅关注与最佳准确率对应的部分。我们在这里看到的是，模型更倾向于使用相对较低的丢弃率，然后希望有一个非常高的学习率和较高的单元数。在这种情况下，我们正在检查训练集上的准确率，因此这可能是模型希望使用非常低的丢弃率的原因。我们还将学习如何使用TensorBoard的投影器选项卡，这在视觉上非常棒，你可以使用不同的算法，如t-SNE和PCA，以了解模型如何学习表示这些图像。
- en: Projected down from a very high dimensional space to 3D where we can easily
    visualize them。 For example， here we are running it on the MNIS data set and we
    can， for example。 use the TS& algorithm and if we for example now search for I
    don't know zero then we can see that there are clusters of that zero right here
    and similarly if we would search for two。 we would see that there's a cluster
    over here of the number two and so this can be useful to understand how the model
    works and also how the model create its representation the Tensorflow profiler
    is also a very useful tool that allows you to see what is taking up the most amount
    of time and sort of what aspects you should aim to improve in your training so
    for example here we're given a summary of the different parts and how much time
    it takes out of our training So for example in this graph right here we can see
    that the input is taking。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个非常高维空间投影到3D空间，在这里我们可以轻松可视化它们。例如，这里我们在MNIST数据集上运行它，我们可以，例如，使用TS&算法，如果我们现在搜索零，那么我们可以看到这里有零的聚类。同样，如果我们搜索数字二，我们会看到这里有一个数字二的聚类，这对理解模型的工作原理以及模型如何创建其表示是非常有用的。TensorFlow分析器也是一个非常有用的工具，它允许你查看哪些部分占用了最多的时间，以及在训练中你应该关注哪些方面进行改进。例如，在这里我们给出了不同部分的总结，以及它们在训练中占用的时间。因此在这里的图表中，我们可以看到输入占用了很大一部分时间。
- en: Very big part of the time and so what they also do is give you some recommendations
    and they say that the program is highly input bound because about 84% of the total
    step time is waiting for input and so what we should try to focus on in this particular
    program is improving the efficiency of our input pipeline and there are also other
    parts of the profiler where you can see many different other parts as well。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 占用了很大一部分时间，因此他们还给出了一些建议，表示该程序高度依赖输入，因为约84%的总步骤时间都在等待输入。因此，在这个特定的程序中，我们应该努力改善输入管道的效率，分析器的其他部分也可以看到许多不同的部分。
- en: but more on that later now you have an idea of the different parts and areas
    of Tensor board that we're going to cover so let's get started。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_1.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不过稍后再讲，现在你对我们将要覆盖的TensorBoard的不同部分和领域有了一个概念，所以让我们开始吧。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_1.png)
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_2.png)'
- en: Alright， so to make this video at least relatively well structured。 I've divided
    it into different files where if each file we're going to focus on one specific
    part of Tensor board。And for each of them I have a starter code and my thinking
    is that I'm going to go through this starter code which is based on code for many
    of the previous tutorials I've done。 so I'm going to go through it relatively
    quickly and then you can check out the previous videos and I'm going to refer
    to those to go into more depth in each of the different parts。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，为了让这个视频至少相对结构清晰，我将其分成不同的文件，每个文件专注于TensorBoard的一个特定部分。对于每个部分，我有一个起始代码，我的想法是快速讲解这个基于我之前许多教程的代码。所以我会相对快速地讲解，然后你可以查看之前的视频，我会引用那些视频以便对每个部分进行更深入的探讨。
- en: but so we're going to base everything from this starter code and then we're
    just going to modify it or add some parts to write to Tensor board。So to start
    off with these are the basic imports that we're going to need and then this is
    what we've seen previously。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将一切基于这个起始代码，然后我们将修改它或添加一些部分以写入 TensorBoard。首先，这是我们需要的基本导入，然后这是我们之前看到的内容。
- en: we're going to add those two lines just to avoid any GPp errors。We're loading
    the Cypher 10 data set from Tensorflowlow datas。 we're normalizing those images
    by dividing by 255。 and then we have some data augmentation on those images we
    also then we do the mapping and the cache and then the batching and prefeting
    on those training and test set。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将添加这两行，以避免任何 GPp 错误。我们正在从 TensorFlow 数据集中加载 CIFAR 10 数据集。我们通过除以 255 来规范化这些图像。然后我们对这些图像进行了数据增强，然后进行映射、缓存、批处理和预取训练和测试集。
- en: Although we're not shuffling on the test set。And then we have the class names
    right here。 and these are used later on in the projector tab to see the correct
    label for each respective image。Then we have the a model right here， very simple
    model using comb layers， two comb layers。 a max pullinging， a flatten and then
    two dense layers which they drop out in between。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有对测试集进行随机打乱。然后我们在这里有类名，这些将在投影仪标签中用于查看每个相应图像的正确标签。接下来我们有一个模型，这里是一个非常简单的模型，使用了组合层，两个组合层，一个最大池化层，一个展平层，然后是两个稠密层，中间有
    dropout。
- en: And then right here we have some setup for custom training loops so we're getting
    the model first of all。 then we have the loss function， we have the optimizer
    at at in this case and then we have a accuracyysymmetric to keep track of the
    current accuracy。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这里我们有一些自定义训练循环的设置，所以我们首先获取模型。接着我们有损失函数，我们有优化器，在这种情况下，我们有一个准确性对称体来跟踪当前的准确性。
- en: And then we have these two train riders， which is what we're going to use to
    write to attentionure board and these are going to be train and for the test so
    for each of them we're going to keep track of a step for the train step and the
    test step。All right， and then we have custom loop。We we're going through for epoch
    in range of nu Epos and then for batching that in DS train and then so this is
    pretty basic。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有这两个训练记录器，这将用于写入 TensorBoard，这些将是训练和测试，因此我们将为每个记录训练步骤和测试步骤。好的，然后我们有自定义循环。我们将遍历每个周期，范围是
    nu Epos，然后在 DS train 中进行分批，这个过程非常基础。
- en: we're doing for propagation， backward propagation。 and then we're doing gradient
    into or an optimizer step right here。Then we're doing the same thing for the training
    set， although we're not doing back propagation。 we're just checking what the accuracy
    is。Al right， so for this first file。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在进行前向传播，反向传播。然后我们在这里进行梯度更新或优化步骤。然后我们对训练集做同样的事情，尽管我们没有进行反向传播。我们只是在检查准确性。好的，对于这个第一个文件。
- en: I'm actually going to do something that may look a little bit weird。 but we're
    going to remove a lot of this code。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_4.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上我将要做一些看起来可能有点奇怪的事情，但我们会去掉很多代码。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_4.png)
- en: So we're going to go just to here， I think and。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_6.png)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想我们就到这里。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_6.png)
- en: The idea here is that that starter code is what we're going to base pretty much
    every of those files you can see right here。 but this specific one we want to
    use callbacks and this is a more simple way where we're not using custom training
    loops so I want to show that first and so after we get the model we're just going
    to do model dot compile we're going to specify our optimizer in this case we're
    just going to use Adam。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，这段起始代码是我们几乎所有文件的基础，你可以在这里看到。但我们希望在这个特定的文件中使用回调，这是一个更简单的方法，我们不使用自定义训练循环，所以我想首先展示这一点。因此，在获取模型后，我们将直接执行
    model.dot compile，我们将指定我们的优化器，在这种情况下我们只会使用 Adam。
- en: And then we're going to specify the loss。 In this case， we're going to use sparse
    categorical。Cross entropy， and then from log， it equals true。We're also going
    to specify the metric。 which in this case is going to be the accuracy。Alright，
    and then we're going to do。Tensor board callback。So this is perhaps the most simple
    way to use Tensor board and I'm going to show you exactly what it does。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将指定损失。在这种情况下，我们将使用稀疏分类交叉熵，并且 log 设为 true。我们还将指定指标，在这种情况下将是准确率。好的，然后我们将做
    Tensor board 回调。所以这可能是使用 Tensor board 的最简单方法，我将给你展示它的具体作用。
- en: but it's very convenient so all you have to do is you need to do cars。 callllbacks
    do Tensor board you got to specify a log directory where it's going to keep track
    of all the logs and so let's just do that let's call it TBb callback directory。And
    then we specify some histogram frequency and this is just， we're going to set
    that to one。 and that is for the distribution plots of the gradients and all of
    that stuff that I showed you previously。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但这非常方便，所以你所要做的就是需要做 cars。回调 Tensor board，你必须指定一个日志目录，它会跟踪所有日志，所以我们就这样做，称之为 TBb
    回调目录。然后我们指定一些直方图频率，这个我们将设置为一。这个用于我之前展示的梯度分布图等所有东西。
- en: Alright， so late lastly we're going to do the model that fit， so we're going
    to send in the DS train。 we're going to send the number of epochs， let's say I
    don't know a5。 then we're going to do validation data we're going to send in the
    test set actually I mean normally you would let's just for purposes let's just
    pretend the test set is the validation data I know this is very incorrect to do
    but so normally you would split the training data into a validation data and then
    send that in but that doesn't really matter for this purpose。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，最后我们将进行模型拟合，所以我们将传入 DS train。我们将传入 epoch 数，假设我不知道 a5。然后我们将做验证数据，实际上我们将传入测试集，通常你会这样做，所以为了目的，我们就假装测试集是验证数据。我知道这样做非常不正确，但这对于这个目的来说并不重要。
- en: And then we're going to specify the callback， we're going to do Tensor board
    callback。And then we're going to specify verbos equals 2。Alright， so that's it。
    let's run this and I'm going to show you then how it looks like in Tensor board。
    But actually。 before we do that， I have to show you how you would actually run
    it。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将指定回调，我们将做 Tensor board 回调。然后我们将指定 verbose 设为 2。好的，就这样。让我们运行这个，我将向你展示在 Tensor
    board 中的样子。但实际上，在我们这样做之前，我需要向你展示如何实际运行它。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_8.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_8.png)'
- en: So how you would run this is you would go to the folder where you have。Thats
    specific where you're running this specific script。 and then you're going to also
    have activated the environment that you have Tensorflow on。And then you would
    do rather tensor board， you would do log De。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你如何运行这个，你需要进入你有的文件夹。具体来说，就是你运行这个特定脚本的地方。然后你还需要激活你安装了 Tensorflow 的环境。然后你会做的是，tensor
    board，你会做 log De。
- en: and then you would specify the one that you called right here， so TB callback
    directory。And if you run that。We're going to get back a URL， which is local hosts
    6006 in this case。 and so if we go to that。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_10.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要指定你在这里称为的那个，TB 回调目录。如果你运行这个，我们将返回一个 URL，这在本例中是本地主机 6006。所以如果我们去查看这个。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_10.png)
- en: And we update it we'll get something that looks like this where we have the
    training and then the validation accuracy for each epoC and then we also have
    it for the loss function so as you can see which is perhaps a little bit strange
    have that the validation accuracy is actually higher but just keep in mind that
    we're using dropout and then we're also using quite I mean and we're also using
    some data augmentation which might make it more difficult for the model to overfit
    on the training data so just for example we're converting some of them to grayscale
    and of course that that could be quite more challenging so that's probably why
    normally you would see this in reverse that the training is higher than the validation。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后，我们将得到这样的结果，其中包含每个 epoch 的训练和验证准确度，同时我们也有损失函数的结果。因此你可以看到，验证准确度实际上更高，这可能有点奇怪，但请记住，我们正在使用
    dropout，同时也在使用数据增强，这可能使模型在训练数据上更难过拟合。例如，我们将一些图像转换为灰度，当然，这可能更具挑战性，所以这大概就是为什么通常你会看到这种情况反转，训练准确度高于验证准确度。
- en: Allright but as we can see right up on this tab right here we have four different
    ones。 so that's just for the scrs， that's just for the loss and the accuracy。
    you also have the graph that sort of tells us how it looks like right we have
    our input。 two comb layers max pull flat and dense drop out and then another denses。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但正如我们在这个标签上看到的，我们有四个不同的内容。这只是用于 SCRS 的，关于损失和准确度的。你还有图形，告诉我们它看起来如何，我们有输入、两个组合层、最大池化、扁平化和稠密的
    dropout，然后是另一个稠密层。
- en: So you can inspect it right here， then we have the distributions that I showed
    you previously where you can see the gradients of all of the parameters。Or rather
    the distribution of the parameters。Alright。 so that's the most basic way to set
    up Tensor board using callbacks and what I'm going to move on to is show you how
    to do it from custom training loops and so that's what we'll be working with from
    now on。 but just know that most of the things that I'm going to show you you can
    also do using callbacks and using model that fit but using custom training loops
    allows us to have more flexibility makes a lot of things actually more simple
    to do。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这样你可以在这里检查它，然后我们有之前展示给你的分布，你可以看到所有参数的梯度。或者更确切地说，是参数的分布。好的。这是使用回调设置 Tensor board
    的最基本方式，我将要展示的是如何从自定义训练循环中实现它，因此从现在开始我们将专注于此。但请知道我将向你展示的大多数内容也可以使用回调和模型拟合来完成，但使用自定义训练循环使我们拥有更多灵活性，实际上使很多事情变得更简单。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_12.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_12.png)'
- en: So let's do full screen on this。And again， this is just the starter code that
    you saw previously and all that we're going to do is we're going to move down
    to the part where we have the custom training loops。And here we want to now add
    so that we're printing to Tensor board。 And so one idea could be that。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们全屏查看这个。再一次，这只是你之前看到的起始代码，我们要做的就是移动到自定义训练循环的部分。这里我们想要添加打印到 Tensor board
    的功能。因此，一个想法是。
- en: Either you're writing it to Tensor board for every batch and so you could do
    it here。 or you could do it right here after this， that would work。 but perhaps
    it would be cleaner a cleaner graph if you do it every epoC so what we could do
    is right here we're going to do with the train rider as default。Then we want to
    send in the loss and we also want to send in the the accuracy and just one thing
    right here is that we're going to send in the loss。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要么你为每个批次写入 Tensor board，所以你可以在这里执行。或者你可以在这里执行，这样也可以。但也许每个 epoch 执行会使图形更清晰，因此我们可以在这里使用训练者作为默认值。然后我们想要发送损失值，同时也想发送准确度，这里有一件事是我们要发送损失值。
- en: Just like this and this is actually just going to be the last loss of that batch
    so just a note you would probably want to send in the mean loss over that batch
    and so you would have a list of the losses and then you would take the mean of
    that and send that in but this works just to show how you can write a tensor board
    so we're going to send in the loss right there and then we're going to send in
    step and we can use the epoC as our step。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样，这实际上只是该批次的最后损失，所以要注意，你可能希望发送该批次的平均损失。因此你会有一个损失列表，然后取其均值发送，但这可以展示如何写入 Tensor
    board，因此我们会在那里发送损失，然后发送步骤，我们可以用 epoch 作为我们的步骤。
- en: Then we're going to do the same thing， dot scalar。We're going to do it with
    let' see with accuracy。 so let me scroll down， we're going to do it with sending
    the accuracy and the accuracy are going to be stored in this accuracy metric dot
    result。And keep in mind here that that's reset in between the epochs。 and it's
    also reset in between the training and the testing。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将做同样的事情，使用 dot scalar。我们将使用准确度，所以让我滚动到下方，我们将发送准确度，准确度将存储在这个 accuracy metric
    的结果中。请记住，在每个 epoch 之间它会重置，并且在训练和测试之间也会重置。
- en: So this should be itated through the test set。All right。 and then we're going
    to add chsmetryduct result。We're going to specify the step again as epoch。 and
    then that's it。Now we've done it from the training we also want to do it for the
    test so what we could do here since we're going to do pretty much the same thing。
    we could just copy that and paste that in and we've got to make sure that we're
    changing all of this stuff test writer step is still epoch step is still epoC
    and yeah so that should be it hopefully there are no mistakes in this right now。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是通过测试集进行迭代的。好的，然后我们要添加 chsmetryduct 结果。我们将再次指定步数为 epoch。现在我们完成了训练，我们也想为测试做同样的事情。因此，我们可以在这里复制并粘贴，并确保更改所有这些内容，测试写入器的步数仍然是
    epoch，步数仍然是 epoch，所以希望现在没有错误。
- en: So one thing to keep in mind here is that we're now using another。A log directory
    so here we're using logs and in logs we're storing two subfolders for the train
    and the test So let's run this first of all and again we're going to open up this
    tensor this Ananaconda prompt right again。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里需要记住的一点是，我们现在使用了另一个日志目录。在这里，我们使用日志，并在日志中存储两个子文件夹，分别用于训练和测试。首先让我们运行这个，接着我们要再次打开这个
    Anaconda 提示符。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_14.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_14.png)'
- en: And we're going to use this， but we're going to specify the log gear to instead
    be just logs and then it's automatically going to find the training and the testing。
    So if we run that and we then go to fire Firefox or your browser and go to Tboard
    to on that URL。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个，但我们会指定日志文件夹只是 logs，然后它将自动找到训练和测试。如果我们运行它，然后去打开 Firefox 或其他浏览器，访问 Tboard
    的那个网址。
- en: local hosts 6006。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_16.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本地地址 6006。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_16.png)
- en: And I guess we just have to wait a little bit here because it's going write
    every epoch so allright。 so here we can now see the accuracy plot for example
    and also the last plot and that these are going to be very you know clear discrete
    steps because we're just recording it every epoch。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我们在这里只需等一下，因为它会写入每个 epoch，所以好的。在这里，我们现在可以看到准确度图，例如最后的图，这些将是非常清晰的离散步骤，因为我们只是每个
    epoch 记录一次。
- en: but of course you could as I said， do it every batch and that would give you
    a much smoother graph。But what is perhaps so？What is perhaps a little bit odd
    here is that the test set has higher accuracy than the training and I would imagine
    that has to do with the augmentation I chose to do for example I made some of
    the RGB on the training to grayscale and that is of course going to make it a
    lot harder for the model to train on those so that's probably the reason why so
    that's what I pretty much wanted to show you on that you could also do more complicated
    or more complex things and you could for example use it for hyperparameter search
    you could do something like for learning rate in and then you could specify at
    learning rates you want something like this。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，正如我所说，你可以每个批次进行记录，这样会得到更平滑的图形。但这里有一点奇怪的是，测试集的准确率高于训练集，我想这可能与我选择的增强方法有关。例如，我将训练中的一些
    RGB 转换为灰度，这当然会让模型在这些数据上训练得更困难，所以这可能就是原因。这就是我想向你展示的内容，你还可以做更复杂的事情，比如进行超参数搜索，可以指定学习率之类的。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_18.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_18.png)'
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_19.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_19.png)'
- en: And。And let's just indent。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_21.png)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们缩进一下。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_21.png)
- en: At one step。 And so what you could do then is。You could。sortrt of train a model
    for specific number of vpoOs with each respective learning rate and here I'm using
    a grid search just on those five different learning rate so let's do that we can
    do train step equals test step and we're just going to set that to zero every
    time。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个步骤中。所以你可以做的是。你可以为每个相应的学习率训练一个特定数量的vpoOs模型，这里我只对这五个不同的学习率进行网格搜索，所以我们来做这个，我们可以设置训练步骤等于测试步骤，每次都将其设置为零。
- en: And then we're going to do the train rid is T F dot summary dot create file
    writer。 And then we're going to do logs train。 But then we're also going to add。Let's
    see。 I're also going to add string of learning rate just to make sort of we can
    distinguish between the different runs。 So let's do that again and let's do the
    test writer。And here we're going to do test。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将执行训练日志，使用TF.dot.summary.dot.create_file_writer。然后我们将进行日志训练。但是我们还将添加。让我们看看。我还会添加学习率的字符串，以便我们可以区分不同的运行。所以让我们再做一次，并进行测试写入。这里我们将执行测试。
- en: And then we need to get the model because we want to sort of refresh the we
    want to reset the weight for each time。 so we're going to do get model， then we're
    going to do optimizer is ks。 optimizeims。 Adam。So we're also， you know， we need
    to re initialitialize the optimizer with this specific learning rate。And then
    you could do it this way and you can write the test writer like this from some
    epochs。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要获取模型，因为我们想要刷新，每次都重置权重。因此，我们将执行获取模型，然后优化器是ks.optimizeims.Adam。所以我们也需要使用这个特定的学习率重新初始化优化器。你可以这样做，并且可以像这样从一些纪元中写入测试写入器。
- en: but do let's just do it for every batch， as I said。That might make for a smoother
    graph so what we're going to do here we're going to specify the train step rather
    than the epoch and then after here we're going to iterate up the train step by
    one and that's going to be sort of one discrete step in the plot and then we're
    going to do that for the test writer as well。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但就像我说的，让我们对每个批次执行一次。这可能会使图表更平滑，因此我们在这里要做的是指定训练步骤，而不是纪元，然后在这里我们将训练步骤迭代增加一，这将是图表中的一个离散步骤，然后我们也会对测试写入器执行同样的操作。
- en: so we got to change this to the test let's see。Test step。 and we've got to do
    it for all of these to make sure that we don't do anything wrong here。We're going
    to iterate up that by one as well。Al right。 so let's think is there anything else
    now we need to change。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们得把这个改成测试，让我们看看。测试步骤。我们必须对所有这些进行处理，以确保我们不会在这里犯任何错误。我们还将将其迭代增加一。好的，让我们想想还有什么需要更改的。
- en: we could decrease the number of epochs just to make it faster。So let me run
    this and I'll show you what it looks like in Tensor board。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_23.png)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以减少纪元的数量，以加快速度。让我运行一下，然后我会向你展示它在Tensor板上的样子。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_23.png)
- en: All right， so looking at the tensor board right here we and I've just separated
    out for the train so you could just write train right here and you can get all
    of the different ones and these correspond to different learning rates and you
    can also do some smoothing right here so you can get you know the loss just varies
    a lot from e for batch from batch to batch and so you could use some smoothing
    to make some better looking plots and then so for example。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，看着这个张量板，我刚刚将训练部分分离出来，所以你可以在这里写“train”，你可以获取所有不同的模型，这些对应于不同的学习率，你也可以在这里进行一些平滑处理，这样你就可以知道损失在每个批次之间变化很大，因此你可以使用一些平滑处理来制作更好看的图表，然后，比如说。
- en: just in this case， if we compare we would have that the learning rates the initial
    learning rate should probably be between 0。001 and then 0。0001 between this。This
    orange and this red one right here。 And so that's it for sort of getting lost
    plot and accuracy plots and you can play around with that。 and you can that is
    very， very useful。 So let's move on to getting images So what we're going to start
    with right here is using just visualizing images and probably the first thing
    we're going to do here is we're going to add some data augmentation we're going
    to use matplot Lib and to plot it。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就在这种情况下，如果我们进行比较，初始学习率应该在0.001和0.0001之间，这两个学习率之间是橙色和红色的。所以关于损失图和准确率图就是这样，你可以随意玩玩，这非常有用。那么让我们继续获取图像。所以我们这里要做的第一件事是可视化图像，可能我们要做的第一件事是添加一些数据增强，我们将使用matplotlib来绘制它。
- en: they want values between0 and1 So we can matpl1。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_25.png)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 他们想要在0和1之间的值。所以我们可以使用matpl1。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_25.png)
- en: Values， so what we can do is we can do image is T F clip。By value。Image and
    then clip value min is0 clip value max is 1 and so by the way。 if you're wondering
    why it would be under or over this0 and one range since we divide by 255 that
    should be the natural range but because of this data augmentation some pixel values
    can become lower or greater than1 so we just want to make sure at the end that
    they are actually between0 and 1 and that's going to be used later on but I'm
    going to show you y in just a moment。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 值，所以我们可以使用`T F clip`。通过值，图像然后裁剪值最小为0，裁剪值最大为1。顺便说一句，如果你在想为什么会低于或高于这个0和1的范围，因为我们除以255，这应该是自然范围，但由于数据增强，某些像素值可能会低于或高于1，所以我们只是想确保最后它们实际上在0和1之间，这在后面会用到，但我马上会向你展示。
- en: So what we could do here is we could actually remove the training part so we
    could just do for batch like this and what we could do is we could iterate through
    for batch XY in enumerate DS train。And so what we're going to do first of all
    is we're going to create a figure and we're going to create an image grid。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以在这里实际移除训练部分，这样我们就可以这样处理批次，我们可以通过`enumerate DS train`来迭代每个批次的XY。首先我们将创建一个图形，并创建一个图像网格。
- en: so we're going to send in x and Y to that and then class names so this right
    here is a function we're going to create in this u function right here。But we're
    going to create an image grid， and then we're going to do width ridr as default。
    We're going to do Tfsmary。 image。And we're going to send in visualized images。And
    we're going to do plot to image， and we're going to send in the figure and then
    the step。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将传入x和Y，以及类名。这里是一个我们将在这个u函数中创建的函数。我们将创建一个图像网格，然后将宽度设置为默认值。我们将做`Tfsmary image`。然后我们将发送可视化图像。我们将做绘图到图像，并发送图形和步骤。
- en: It's just equals to step。And so we could。In this case。 we're just going to just
    have a single writer and do it like this。And instead of using train step。 we're
    just going to call it step。Like this。 And then that's pretty much all we're going
    to do。 we're going iterate up step by one All right， so this is how it's going
    to look like in this file。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这等于步骤。因此在这种情况下，我们只会使用一个单独的写入器，就这样做。而不是使用训练步骤，我们只称之为步骤。就这样。然后这就是我们将要做的所有事情。我们将逐步迭代。
- en: but of course we need to create this image grid and we need to create this plot
    to image I just want to say first of all that there are easy ways to do this and
    what I'm doing here by we're going to create this image grid in this plot to image。I
    unnecessarily complicated， but it helps or rather。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当然我们需要创建这个图像网格，并且我们需要创建这个图像绘图。我想先说有更简单的方法可以做到这一点，而我在这里所做的确实使这个图像网格的创建变得复杂，但这有助于或者说更好。
- en: if you want it in a clean way so that you get it in a grid and you get it with
    the labels above it and so on。Then this is how you're going to want to do it there
    are easier ways to just plot the images。 but they don't look very nice and of
    course we want it to look nice。 so let's go to u and what we're going to first
    of all is I'm just going to copy in some imports that are going to be used in
    this file。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想以整洁的方式呈现，使其以网格形式显示，并在上方有标签。那么这就是你想要的做法，虽然还有更简单的方法来绘制图像，但效果可能不够美观，当然我们希望它看起来不错。那么让我们开始，我首先要复制一些将在此文件中使用的导入内容。
- en: And the thing we're going to do first， so we had， let's see， we had the defined
    plot to image。And we're going to send in some figure。So let's to pass on that
    first of all。 and then we had another image rather another functions we had image
    grid， and we send in some data。 we send in some labels and we send in some class
    names。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的事情是，我们有，嗯，让我看看，我们定义了绘图到图像。然后我们将发送一些图形。因此，让我们先处理那个。然后我们还有另一个图像，实际上是另一个函数，我们有图像网格，发送一些数据，发送一些标签，并发送一些类名。
- en: And what I'm going to do is I'm going to copy in from TensorFlow official tutorials
    this plot to image。 so this is stolen from TensorFlow official guide， you can
    see the URL right here but essentially so it converts the mapplot lib plot specified
    by figure to a PNG image and returns it this applied figure is closed and inaccessible
    after this call so basically it's converting the mapplot lib plot into a PNG in
    memory and then it gets decoded to a tensor which can then be sent to the a tensor
    board so here we're using IO to save the plot in memory。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从 TensorFlow 官方教程中复制这个绘图到图像。这是从 TensorFlow 官方指南中偷来的，你可以在这里看到 URL，但基本上，它将指定的
    mapplot lib 绘图转换为 PNG 图像并返回它，这个应用的图形在此调用后被关闭且不可访问，所以基本上它是在内存中将 mapplot lib 绘图转换为
    PNG，然后解码为张量，接着可以发送到 TensorBoard，因此在这里我们使用 IO 将绘图保存在内存中。
- en: Then we're closing the figure preventing from being displayed directly inside
    the notebook。And then we're converting PNG buffer to TF image by image。 decocode
    PNG。And then lastly。 we're adding the batch dimension。So I mean I don't want to
    focus too much on this。 this is not really relevant I would say this is just some
    boiler play code just to make sure that it's in the right format。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们关闭图形，防止它直接在笔记本中显示。然后，我们通过 image.decode PNG 将 PNG 缓冲区转换为 TF 图像。最后，我们添加批量维度。我不想过多关注这一点，我认为这并不相关，这只是一些样板代码，以确保它处于正确格式。
- en: which is a tensor and then this image grid is what we're going to do first so
    we're going to create image grid which is going to create this matpllib figure
    then we're going to send it to this plot to image and that's going to then be
    sent to Tensor board。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个张量，然后这个图像网格是我们将首先创建的，因此我们将创建图像网格，这将创建这个 Matplotlib 图形，然后我们将其发送到这个绘图到图像，接着会发送到
    TensorBoard。
- en: So first of all， the data data should be in this format， so we should have batch
    size。 HW and then channels。 So what we could do first of all。 you see we could
    just assert data dot number of dimensions is4 what we're going to do is we're
    going to create a figure So figure is PLT dot figure of fig size and this is just
    Matpllib right that's the import。Right here importpl pipeline as PLT。So we're
    creating that with a fig size of 10 and 10 and this is in I think this is in inches
    so it can be a bit confusing。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据应该采用这种格式，因此我们需要有批量大小。HW 然后是通道。所以我们可以首先断言数据的维度为 4，我们要做的是创建一个图形，所以图形是 PLT.dot.figure
    的 fig size，这只是 Matplotlib 的导入。就在这里，importpl pipeline as PLT。所以我们以 10 和 10 的 fig
    size 创建它，这我认为是以英寸为单位，因此可能会有点混淆。
- en: but I've tried out different values and this is just what looks nice so but
    anyways then number of images are going to be data shape0 I guess this is specific
    to sort of a specific batch size the fig size that we're using here and what I've
    tried for is batch 32 you can also use larger batch size and it's still going
    to look relatively okay what I'm doing here is that I'm creating the number of
    images to be data shape of0 which is you know a general format that's not specific
    to 32 then we're going to do size is。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试过不同的值，这就是看起来不错的效果，所以无论如何，图像数量将是数据的形状 0，我想这对于特定的批量大小来说是特定的，我们在这里使用的 fig size，我尝试过的是批量
    32，你也可以使用更大的批量大小，它看起来仍然相对不错。我在这里做的是将图像数量设为数据形状的 0，这是一种通用格式，并不特定于 32。
- en: Integer of N do seal of MP square root of nu images and why we're doing this
    is because we want a nice looking grid。 a square grid， so the size of the grid
    is going to be the square root of the number of images just sealed to the up1
    integer value。And then what we're going to do is we're going to iterate for I
    in range of data。Shape of 0。So for each image in our data， we're going to create
    a subplot for that one。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 整数 N 为 MP 的平方根的上取整图像，因为我们想要一个好看的网格，一个正方形网格，因此网格的大小将是图像数量的平方根，上取整到下一个整数值。接着我们将迭代范围为数据的形状
    0 的 I，因此对于数据中的每个图像，我们将为该图像创建一个子图。
- en: and we're going the size of the subplot is just going to be size。Size and size
    just for the number of images in total。 and the specific image right now is going
    to be specified by this index， which is i+1。And the title of this plot is going
    to be class names。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 子图的大小将是大小。大小和大小，正好是图像总数，而当前特定图像将由这个索引指定，即 i+1。该图的标题将是类别名称。
- en: labels of so the labels are the values of the index for that one。 and then we're
    doing class names of that index right there to get the name of it then we're going
    to do xt and we're just going to send in an empty list and we're going to do yt
    and we're going to do similar and then we're going to do plot grid equals or plot
    grid false。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是该索引的值。然后我们在这里使用该索引的类名称来获取它的名称，接下来我们将做`xt`，并发送一个空列表，然后我们将做`yt`，并做类似的事情，最后我们将设置`plot
    grid`为`false`。
- en: And。Then we're going to do if data shape of three is one。 we're going to do
    plotted and we're going to do C map is PLT dot Cm dot binary just to get gray
    scale right。 So if gray scale。This is what we're going to do and if it's RGB。
    we're just going to do plot I show of data of I。And then in the end。Right， let's
    see。 right there。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将检查数据形状的第三个维度是否为1。我们将进行绘图，并将CMap设置为`PLT.dot.Cm.dot.binary`以获取灰度图。如果是灰度图，我们将这样处理；如果是RGB，我们将直接使用`plot
    I show of data of I`。最后，看看这里的内容。
- en: we're gonna return the figure。And here we don't have to specify any CMap because
    RGB is by default。So this is a lot of Matlolib and I don't want to go into the
    specifics of that but you get the general picture right here。 even if you're not
    too familiar with Matplthlib I would think we're creating a figure and then we're
    creating a subplot which are going to be these specific training images and the
    size of it is size so row the number of rows is size and that is dependent on
    the number of images that we send in。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将返回图形。在这里我们不需要指定任何CMap，因为默认情况下是RGB。所以这里有很多Matplotlib的内容，我不想深入细节，但你能理解整体概念吧。即使你对Matplotlib不太熟悉，我认为我们正在创建一个图形，然后创建一个子图，这将是这些特定的训练图像，大小为`size`，行数由我们传入的图像数量决定。
- en: so let's say we send in 16 images then the grid is going to be 4 by4 and each
    of those cells in that4 by4 grid is going to be an image from our training set。So
    now we have the image grid and then we have plot to image， so if we go back。This
    should most likely work now so we're doing first image grid then we're doing plot
    to image and here we are writing every single image in our batch。So that wouldn't
    really be necessary， perhaps you want to do it every epoch or something。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以假设我们传入16张图像，那么网格将是4乘4，每个单元格将是我们训练集中的一张图像。现在我们有了图像网格，然后我们有`plot to image`，所以如果我们回到这里，这应该可以正常工作。我们先做图像网格，然后做`plot
    to image`，这里我们正在写入我们批次中的每一张图像。这并不是特别必要，也许你想在每个epoch时执行一次。
- en: but this could be useful just to check all of the the images， so let's run it。And
    let's see what it looks like。 Alright， and then right here we need to import it
    right。 so we need to do right here we need to do from us import plot to image
    and then image grid。And rerun it and hopefully it works。Al right， writer as the
    is not defined so。I think this should be。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但这可能很有用，可以查看所有图像，所以让我们运行它。看看它的样子。好吧，在这里我们需要导入它。所以我们需要在这里做`from us import plot
    to image`和`image grid`。然后重新运行它，希望能成功。好吧，writer未定义，所以我想这应该是。
- en: 😔，Wrier dot as default。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_27.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 😔，Wrier dot为默认设置。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_27.png)
- en: Alright so if we open up the tensor board now we get it in a nice format。 we
    get the labels printed above it and as you can see here this is a square grid
    so for example here it's six by6 and then these last ones aren't covered by any
    image and that's because we did that seal and round it upwards but I would say
    this looks pretty good this is how this is a nice way to visualize the images
    and then you can see the step right here for each of the batches one thing here
    is that if you scroll back just one you're going to see that it's actually six
    steps。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果我们现在打开TensorBoard，我们会得到一个漂亮的格式。标签会打印在上方，正如你所看到的，这是一种正方形网格，例如这里是6乘6，然后这些最后的图像没有被任何图像覆盖，因为我们进行了密封并向上取整。但我觉得这看起来很好，这是一种可视化图像的好方法，你可以看到这里每个批次的步骤。一件事是，如果你向后滚动一次，你会看到实际上是六个步骤。
- en: And。If you call when you in Ananaconda， you call Tensor board。 you have another
    argument you can send in， which is samples per plugin and that would specify how
    many images you want in this step slider right here so for example if you just
    specify it so if you specify that for example to 94。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Anaconda中调用TensorBoard，你可以发送另一个参数，即`samples per plugin`，这将指定你希望在此步骤滑块中显示多少张图像。例如，如果你将其指定为94。
- en: then that would mean that one slide step right here is for one batch rather
    than jumping you know in this case about 25 batches。So that's just one thing to
    keep in mind if you are wondering about that。But anyways。 that's how we can visualize
    images。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_29.png)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这里的一个滑动步骤是针对一个批次，而不是像这种情况下大约 25 个批次。所以如果你对此感到困惑，这只是需要记住的一点。但无论如何，这就是我们如何可视化图像的方式。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_29.png)
- en: Now the next thing is we want to create this confusion matrix and this is also
    a little bit complicated。 but as you saw previously it's going to turn out pretty
    good so let's go through it step by step and again this is the starter code as
    usual how this is going to work is that normally you would create the confusion
    matrix by sending in all of the images at the same time with rather the outputs
    and the y labels at the same time and then that would create a confusion matrix
    but in this case we're creating it or rather we want to update it as we do it
    for every batch right let's say we have an enormous data set we can't send it
    in at the same time so what we're going do。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要创建这个混淆矩阵，这也有点复杂。但是正如你之前看到的，它会表现得相当不错，所以让我们一步一步来，再次这是启动代码。通常你会通过同时发送所有图像以及输出和
    y 标签来创建混淆矩阵，但在这种情况下，我们想要在每个批次处理时更新它，假设我们有一个庞大的数据集，无法一次性发送。
- en: As we're going to create a confusion matrix。That is initially empty。 so we're
    going to do MP zeros and then length of class names。And then length of class names，
    right。 that's going to be the the the number of rows and the number of columns。And
    what we're going to do is we're going to update this throughout going through
    our batch。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个最初为空的混淆矩阵。所以我们将使用 MP.zeros，长度为类名。然后类名的长度，这将是行数和列数。我们将在遍历我们的批次时更新它。
- en: So we're going to go through here and we're going to do confusion is plus equals。A
    function get confusion matrix， and we're going to send in why， Y prediction。 and
    then the class names。And that's going to then add up to that confusion matrix。
    and we might get some numerical roundup errors and so on。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将要在这里进行混淆矩阵的计算，使用的是 confusion is plus equals 函数。我们将发送输入 y 和 Y 预测，以及类名。然后这将累加到混淆矩阵中，可能会产生一些数值的四舍五入误差等。
- en: but at least this approximation is very good and it's going to be enough to
    get a good representation of what the model is learning。Then after that batch，
    we're going to do width train rider do as default。We're going to do Tf。summary。im。And
    we in we're going to specify the name， which is confusion matrix。Then we're going
    to do。App plot confusion matrix， and again this is going to be another function。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但至少这个近似值非常好，足以获取模型学习的良好表示。然后在这个批次之后，我们将使用默认的宽度训练进行操作。我们将使用 Tf.summary.im，并指定名称为混淆矩阵。然后我们会做
    App plot confusion matrix，这又将是另一个函数。
- en: And we're going to send in confusion and we're going to divide by the batch
    index because we're going to get a confusion matrix here at every update that
    is going to be have its own probabilities between zero and 1。 but because this
    is a linear operator， then we can divide by the batch index。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将发送混淆矩阵，并按批次索引进行除法，因为我们将在每次更新时获得一个具有自己概率的混淆矩阵，范围在零到一之间。但由于这是一个线性操作，我们可以按批次索引进行除法。
- en: And we will get an average of the probabilities for the entire epoC Of course。
    you know you could get overflow by this since you you're essentially let's say
    you're adding one every single batch。 but to get overflow you would need to have
    an enormously large data set。You know even if you have 100，000 samples， having
    100，000 is very far away from overflowing。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将得到整个 epoch 的概率平均值。当然，你知道这样做可能会导致溢出，因为你实际上是在每个批次中加一，但为了导致溢出，你需要有一个非常大的数据集。即使你有
    100,000 个样本，100,000 也远离溢出。
- en: so this is okay， just keep that in mind though and then we have class names。And
    then we're going to specify this step as the epoch。 so we're just plotting the
    confusion matrix every epoch because this can also be quite you know quite expensive
    all right。 and then we don't have to do it for the test set， but of course you
    can do it for the test set if you want。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这没问题，只需记住这一点，然后我们有类名。接下来我们将把这一步指定为epoch。因此，我们每个epoch绘制混淆矩阵，因为这也可能相当昂贵。好的，然后我们不必为测试集做这个，但当然如果你愿意，也可以为测试集做。
- en: but I'm just going to remove this actually。Like this because we don't， that's
    not really needed。And we can save on some compute， So what we want to do now is
    we want to go to the u function and we want to create those two functions。Alright，
    so we're going to first to get confusion matrix。 We're going to send in y y labels。
    we're going to send in Los， we're going to send in class names。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但我实际上将去掉这个。像这样，因为我们不需要那样，这样可以节省一些计算资源。所以我们现在想要去到u函数，并创建那两个函数。好的，我们首先要获取混淆矩阵。我们将传入y标签，传入Los，传入类名。
- en: So this right here are the correct labels。 This is just what the output from
    the model is。And these are the class names。let's pass on that just quickly and
    then we also want to do define plot confusion matrix and here we want to send
    it as input。 some confusion matrix and I'm just going to write CM we're also going
    to send in some class names so maybe first of all we want to actually create the
    confusion matrix and you know to not do this from scratch we're going to use SK
    learn to produce the confusion matrix。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里是正确的标签。这就是模型的输出。这些是类名。让我们快速处理一下，然后我们还想定义绘制混淆矩阵的方法，并将其作为输入。某些混淆矩阵，我将其称为CM，我们还将传入一些类名，所以也许首先我们想要实际创建混淆矩阵，并且为了不从头开始，我们将使用SK
    learn来生成混淆矩阵。
- en: creating confusion matrix would be a separate video I guess。But we would do
    the predictions are the nuy arg max of logicits and then x is1。 So we're converting
    it to Numpy here as well。 So of going from nuy to Tensorflow is pretty seamless。
    And so we just have to do nu argm directly and then。That will be the predictions。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 创建混淆矩阵可能会是一个单独的视频。我想。但我们将进行预测，即逻辑回归的nuy arg max，然后x是1。因此我们在这里也将其转换为Numpy。所以从nuy到Tensorflow的转换相当顺利。因此我们只需直接做nu
    argm，然后那将是预测结果。
- en: Then we're going to do the confusion matrix is SK learnarn dot matrix dot confusion
    matrix。And we're going to send in the y labels， we're going to send in the predictions。And
    then we're going to send in labels as NP a range of length of class names。 and
    this is essentially to make sure that we're getting a confusion matrix which is
    length of class names in both the rows and the columns because let's say we have
    a batch size of 32 we would not perhaps get one example from each class every
    and that would destroy our confusion matrix。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要使用SK learn的`matrix.confusion_matrix`来生成混淆矩阵。我们将传入y标签，传入预测结果。然后我们将传入标签，作为NP的类名长度范围。这基本上是为了确保我们得到的混淆矩阵在行和列上都与类名的长度相等，因为假设我们的批量大小为32，我们可能不会从每个类中得到一个样本，这会破坏我们的混淆矩阵。
- en: so we're sending this in just to make sure that it's actually the size we want。And
    then we're just going to return the confusion matrix。Alright。 and then we want
    to do the plot confusion matrix and what we're going to do here is we're going
    to do size is the length of number of class names。We're going to create a figure，
    which is PLT dot figure， and then fig size is size， comma size。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们这样做只是为了确保它的确是我们想要的大小。然后我们将返回混淆矩阵。好的。然后我们想要绘制混淆矩阵，这里我们将根据类名的数量设置大小。我们将创建一个图形，即PLT.figure，然后fig
    size是size, size。
- en: We're going to do plot。 in show and we're going to do sending the confusion
    matrix。 we're going to do interpolation interpolation is nearest。And you can read
    documentation for what this is exactly doing。 I'm not entirely sure what the different
    interpolation algorithms are。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行绘图，并展示混淆矩阵。我们将进行插值，插值方法是最近邻。你可以查看文档了解这到底是在做什么。我不太确定不同的插值算法是什么。
- en: but this seems to be a default one and then we're going to use CM is PLt。cm。bls。So
    this is just gonna make the。Sort of the color of it blue。 we're going to see exactly
    how it looks like later on， but then we're going to PL T back title。 We're going
    to specify the confusion matrix。And then the indices are going to be NPR range
    of length of class names。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但这似乎是一个默认值，然后我们将使用CM是PLt.cm.bls。这只是让颜色变成蓝色。我们稍后会看到它的具体样子，但接下来我们将使用PLT.back.title。我们将指定混淆矩阵，然后索引将是NPR范围内的类名长度。
- en: All right。And then we're going to do plot do x ticks。 We're gonna to send in
    the indices。 That's how many。Sort of x values。 How many。How many ones we're going
    to have and then we can specify the names of them and we're going to do class
    names and then to have them look in a nice way and so that the texts don't overlap。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。然后我们将进行plot.do.x_ticks。我们将发送索引。这就是有多少种x值。我们将有多少个，然后可以指定它们的名称，我们将使用类名，确保它们看起来不错，并且文本不会重叠。
- en: we're going to specify a rotation to be 45 degrees。And then we're going to do
    ytics。 and we're going to do indices and then class names。Alright。 and then we
    want to make sure that we normalize， so we want to normalize confusion matrix。And
    how we do that is we do CM is NP dot around。We're going to do CMm type。Float。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定旋转为45度。然后我们将进行ytics，并进行索引和类名的设置。好的。然后我们希望确保进行归一化，因此我们要归一化混淆矩阵。我们通过CM是NP.dot.around来实现。我们将使用CMm类型Float。
- en: And we're going to divide by seeing that sum x is 1。And we're going to domp
    donuax。 and we're going to specify the number of decimals。U。So this just to make
    sure that everything adds up to one。For the sort of the predictions for a specific
    one can only add up to one over all of the different labels。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们会通过看到总和x等于1来进行除法。我们将要做domp donuax，并指定小数位数U。这只是为了确保所有内容加起来为1。对于特定的预测，所有不同标签的和只能等于1。
- en: So that's what we're doing there then we want to print or yeah。 so we want to
    have the text of the probability for that specific one。So we're going to first
    do threshold is CM dot max。 and we're going to divide it by  two and we're going
    to do something pretty cool with this。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们要做的，我们想要打印，或者说是的。我们希望得到特定概率的文本。因此，我们首先会设定阈值为CM.dot.max，并将其除以二，然后我们将用这个做一些相当酷的事情。
- en: but you're going to see it in just a second。So we're going to go through for
    I in range of size。Then we're going to do 4 J in range of size， right。 we need
    to go through every row and then we need to go through every column and we're
    going to do first color is white if the CMI J is greater than the threshold Ls
    black。So if it's over some threshold， then we're going to make it white because
    then the background color of that cell is going to be relatively dark。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 但你将在一秒钟内看到。我们将遍历大小范围内的I，然后进行4 J的遍历，没错。我们需要遍历每一行，然后遍历每一列，首先如果CMI J大于阈值，颜色为白色，否则为黑色。所以如果它超过某个阈值，我们将把它设为白色，因为那个单元格的背景色会相对较暗。
- en: so then we're going to make it white， otherwise if it's lower then it means
    it's relatively light cell or a light colored cell。 sort of a more white cell，
    then we're going to make the text black。And then we're going to do plot do text，
    We're going to specify the position as IJ。 and then we're going to send in the
    actual value， which is CMIj。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将把颜色设为白色，否则如果值较低，则意味着它是相对较浅的单元格或浅色单元格。某种更白的单元格，因此我们将把文本颜色设为黑色。然后我们将使用plot.do.text，指定位置为IJ，然后发送实际值，即CMIj。
- en: And we're going to do horizontal alignment。 We're going to do center just to
    center the text。And then we're going to do color equals color， and that is the
    thing we specified over here。Alright。 and then after that， we're going to do plot
    tight。Layout， and this is just to get some nice format。 And then we're going to
    do plot X label， and we're going do true label as the。X values。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行水平对齐，选择中心以居中文本。然后我们将设定颜色为color，这就是我们在这里指定的内容。好的，接下来我们将使用plot.tight.layout，这只是为了获得一些良好的格式。然后我们将使用plot.x_label，将真实标签作为x值。
- en: and then the plot that y label is the predicted。Predicted label。And similarly，
    as we did before。This is now a matlolib figure to get it nice to tensor board。
    It needs to be converted to a tensor。 so we can reuse the plot to image so we
    can deploy plot to image of that figure。And then we can return CMm image。Alright，
    so now we can go back and hopefully this should now work。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 y 轴的标签是预测的。预测标签。同样，正如我们之前所做的。这现在是一个 matplot 图形，为了使其在 TensorBoard 中显示良好，它需要转换为张量。因此，我们可以重用该图形以生成图像，然后返回
    CMm 图像。好的，现在我们可以回去，希望这现在能工作。
- en: So if we run this now， this is not going to work actually right we need to import
    this stuff as well from Us import what was it called get confusion matrix and
    then plot confusion matrix and hopefully that will run now So let's see。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_31.png)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们现在运行，这实际上是无法工作的，对吧，我们还需要从 Us 导入这些内容，导入 confusion matrix 的名称，然后是 plot confusion
    matrix，希望这现在可以运行。那么我们看看。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_31.png)
- en: All right so as you can see it seems to be working。 of course we have only run
    it for a single epoOP now so our model is quite bad and this is represented of
    course in the values of the diagonal。 which is how much well the probability of
    it accurately classifying each specific class so in this case airplane about 50%
    and so on so I mean let's increase the number of epochs。 let's say to5 and rerun
    it and then you'll see as this improves over the epochs。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如你所见，它似乎在工作。当然，我们现在只运行了一次 epoch，因此我们的模型相当糟糕，这在对角线的值中得到了体现。这是它准确分类每个特定类别的概率，所以在这种情况下，飞机大约是
    50% 等等。因此，我的意思是让我们增加 epoch 的数量，假设到 5，并重新运行它，然后你会看到随着 epoch 的增加，这一情况有所改善。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_33.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_33.png)'
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_34.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_34.png)'
- en: Alright， so as you can see， it's become better， the model is still pretty bad。
    we're using a very small model。And so you can still see that it's not perfect，
    but anyways。 the visualization seems to work。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_36.png)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，如你所见，情况变得更好了，但模型仍然很糟糕。我们使用的是一个非常小的模型。因此，你仍然可以看到它并不完美，但无论如何，可视化似乎有效。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_36.png)
- en: Let's now move on to doing a graph。This is going to be a guess。A little bit
    simpler。 So let's see here。 I'm actually not going to need anything of this。 so
    we're going to remove all of the data set stuff right here。We're going to remove
    all of it。So the thing we're actually going to do is we're going to do writer。
    we're going do T F summary。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转到创建图形。这将是一个猜测，稍微简单一点。那么让我们看看。我实际上不需要这些东西。因此，我们将移除这里的所有数据集内容。我们将全部删除。我们实际上要做的是使用
    writer。我们将使用 TF summary。
- en: create。File writer。We're going to specify logs， and then we're going to specify
    graph。Viz。 let's just call it that。And then we're going to just define some function。
    my function is's going to take x and y's input， then she're going to turn Tf。nn。
    Relu and then Tf。t mat Mole and then x and y。So it's just doing matrix multiply
    and then taking the re of that。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建。文件写入器。我们将指定日志，然后指定图形。就叫它这个吧。接下来，我们将定义一些函数。我的函数将接收 x 和 y 的输入，然后返回 Tf.nn.ReLU，然后是
    Tf.matmul，然后是 x 和 y。因此，它只是在进行矩阵乘法，然后获取结果。
- en: And we're going to do x is Tf random uniform shape 3 by 3， and y is just similarly
    Tf random uniform。Three by three as well。And then we'll have to create， we need
    to do add Tf dot function。So since Tenflow 2。0， eager mode is executed by default。But
    creating these graphs is actually not possible to do in eager mode so we need
    to make sure that it's run in a static graph。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做 x 是 Tf.random.uniform 形状为 3x3，y 也是类似的 Tf.random.uniform 三维矩阵。然后我们需要创建，添加
    Tf.function。因此，自从 TensorFlow 2.0 起，默认情况下以 eager 模式执行。但在 eager 模式下实际上无法创建这些图形，因此我们需要确保它在静态图中运行。
- en: and so this at Tia function， makes sure that the function is converted and run
    in a static graph。And when that is done。Now we're going to do Tf that summary
    that trace on。 and we're going to do graph true profiler true。Then we're going
    to do out is my function of that x and y that we just send in。Then we're going
    do with writer as default。Default。We're going to do TF summary dot trace export。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个 Tia 函数确保函数被转换并在静态图中运行。当这完成后。现在我们将执行 Tf.summary.trace，并设置 graph 为 true，profiler
    也为 true。然后我们将执行我的函数，输入我们刚刚发送的 x 和 y。然后我们将使用 writer 作为默认值。我们将执行 TF summary.dot
    trace export。
- en: and we're going to do name， we're just going to call it function trace。😔。We're
    going to do step equals0 because we're just doing one and then profiler。Our directory。Is
    going to be。Logs， and then we can't do it in this way， so we need to do it like
    this。 just a weird thing。That we need to do and then graph visualization and let's
    see。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的名字，我们就叫它函数跟踪。😔。我们将步骤设为0，因为我们只做一次，然后是分析器。我们的目录将是。日志，然后我们不能这样做，所以我们需要这样做。只是一个奇怪的事情。我们需要这样做，然后进行图形可视化，让我们看看。
- en: We also need to do it like this， if I recall correctly。And that should work
    now if you of course if you want to visualize a model this is still going to work
    so I mean if you have a model just to return a model of x here so just create
    your model here using Kaara sequential and so on and then just to return model
    of X and that should still work what Im want to do here is just show you a very
    minimal simple example but of course this this can be used in general so if we
    run this and then go to Tensor board or let's see that it's no error first of
    all yeah so we get some warnings here and I'm not really sure what to do about
    him I mean。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我没记错的话，我们还需要这样做。如果你想可视化模型，这样应该可以工作，所以我的意思是，如果你有一个模型，只需在这里返回X的模型，所以只需在这里使用Keras的顺序模型创建你的模型，然后返回X的模型，这样仍然可以工作。我想在这里做的只是给你展示一个非常简单的例子，但当然这可以广泛使用，所以如果我们运行这个，然后去TensorBoard，或者让我们先看看没有错误，是的，我们在这里收到一些警告，我不太确定该怎么处理。我是说。
- en: Sometimes in Tensorflow， you just get errors， but if stuff works。Like it should
    then。I don't know。I don't know what to do about this Anyways， we're just going
    to look at the graph and it's going to look no data set name train。 al right。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_38.png)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在TensorFlow中，你会遇到错误，但如果一切正常。我不知道。我不知道该怎么处理这个。不过，我们只是看看图形，它会显示没有数据集名称训练。好吧！[](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_38.png)
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_39.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_39.png)'
- en: And then we have this x and Y matrix multiply Re， and then that's the output。Sometimes
    these graphs can look a little bit weird。 like I'm not sure why we have two identity
    right here。嗯。But yeah。So anyways。 you can see sort of the most relevant parts
    here that we're doing this and then re and personally I haven't used this too
    much but。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有这个X和Y矩阵相乘Re，然后就是输出。有时这些图形看起来有点奇怪。我不确定为什么我们在这里有两个身份。嗯。不过，是的。所以无论如何。你可以看到我们正在做的最相关部分，然后Re，个人来说，我没有使用太多，但。
- en: It can be useful， I'd imagine if you want to debug some very complex model。But
    anyways。 let's move back to thecode so we're going to want to do hyperparameters
    now and this is something that's very useful。 I would say if you want to do a
    hyperparameter search and you get it visualized very nicely so it looks very good。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我想这在调试一些非常复杂的模型时可能很有用。但是无论如何。让我们回到代码，所以我们现在要处理超参数，这非常有用。我会说如果你想进行超参数搜索，并且可以很漂亮地可视化。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_41.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_41.png)'
- en: And yeah， so we're going to again， just work from the start code and we're going
    to modify it。 And the thing we're going to modify now is。We're going to create
    sort of。A function。 and we're going to call it train model let。One epoch。And then
    we're going to send in something called H PRs and actually we need to import that。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，是的，我们将再次从起始代码开始，并进行修改。现在我们要修改的内容是。我们要创建一种。一个函数。我们称之为训练模型，让我们进行一个周期。然后我们将传入一些叫做H
    PR的内容，实际上我们需要导入这个。
- en: first of all， so what we need to import is from Tensorboard dot plugins。Plugins
    that H PR import API as H。 That's just for the H parameters API。 and we're going
    to import it as H。 And so let's move back down to that function。 and what we're
    going to do here。As。Let's say in our model right here we're gonna copy in that
    model and let's say we want to sort of play around with when it comes to the model。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入的是来自Tensorboard插件的内容。插件中的H PR导入API作为H。这只是为了H参数API。我们将其导入为H。那么让我们回到那个函数。我们将在这里做的。假设在我们的模型中，我们会复制那个模型，假设我们想在模型上玩一下。
- en: let's say we want to play around with this right here let's just call a number
    of units in the dense layer right here and of course you could imagine doing this
    for different values and then so on but I'm just going to use it that for this
    example and what I'm going to show you is how to get the Hpm from that and let's
    just say we want to play around with the dropout as well。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想在这里进行一些尝试，我们就称之为密集层中的单位数量，当然你可以想象对不同值进行这样的操作等等，但我在这个例子中只使用这个。我将向你展示如何从中获取Hpm，并且假设我们也想尝试一下dropout。
- en: So wait， let's remove this and。Drop out of some drop rate。And。Then let's also
    say we want to play around with the learning rate。 So from this h parameterss。
    we're going to get units， we're going to do H parameterss。 and then we're going
    to call it H nu units。We're going to get a drop rate and of course。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以等一下，先移除这个，然后使用某个drop rate。然后我们还想尝试一下学习率。从这个h参数中，我们将获得单位，做H参数，然后称之为H nu units。我们将获得一个drop
    rate，当然。
- en: we haven't defined these， but we're going to do it later on。 so we're going
    to get H dropout and then we're also going to get learning rate。And that is Kaos
    optimizeimrs。adom。<|OTHER|>The learning rate will be set to learning rate。 and
    we need to get the learning rate as well。And that is from H's H learning rate。Alright。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有定义这些，但我们稍后会进行。因此我们将获得H dropout，然后我们也将获得学习率。这是Kaos optimizeimrs.adom。学习率将设置为学习率。我们也需要获取学习率。这来自H的H学习率。好的。
- en: so now that we have those， we want we have the the oh wait， that's on the learning
    rate optimizer。 that's the optimizer， and we're using the learning rate that we
    got from H primes。And then we're creating the model using this units in this drop
    rate。 Then we're going to actually create。One training epoch， and we're going
    to do it with this。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这些，我们想要的是，哦等一下，那是学习率优化器。那是优化器，我们使用的是从H primes得到的学习率。然后我们使用这个单位和这个drop
    rate来创建模型。然后我们实际上要创建一个训练epoch，我们将用这个来进行。
- en: So we're going to copy that and put it over here。Says this functionalityity
    oh， not this one。This one， train model 1 epoch。 This is going to train it for
    one epoch so we can paste that and。That should be it。Then we're going to want
    to write to Tensor board， so write to TBb。We're going to do that by specifying
    their run directory。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要复制这个并放到这里。说这个功能，不，不是这个。这是训练模型1个epoch。这将训练1个epoch，所以我们可以粘贴它。这就应该是这样。然后我们要写入Tensor
    board，所以写入TBb。我们将通过指定它们的运行目录来实现。
- en: And since we're doing many different runs with different values of the learning
    rate and drop rate and so on。 we're gonna to first specify logs and then train
    and then we're going do。Plus string of units。Then we're going to do units， so
    this is just going to be an integer。 then we're going to be units and we're going
    to underscore underscore just to make it readable。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们进行许多不同的运行，使用不同的学习率和drop rate等。我们将首先指定日志，然后是训练，然后我们将进行。加上单位的字符串。然后我们将进行单位，所以这只是一个整数。然后我们将进行单位，并且我们将使用下划线来使其可读。
- en: then we're going to do string of drop rate。And then we're going to do drop out。Then
    we're going to do string of learning rate。😔，And then we're going to do。Learning
    rate。Allright so this might look a little bit weird， this is just not to get one
    line too long。 so this looks all right。We can easily read it。And then when we
    have the run directory。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行drop rate的字符串处理。然后我们将进行drop out。然后我们将进行学习率的字符串处理。😔 然后我们将进行学习率。那么这看起来可能有点奇怪，这只是为了不让一行太长，所以这样看起来还不错。我们可以轻松阅读它。当我们有运行目录时。
- en: which is going to be different depending on the unit drop rate and the learning
    rate。 we're going to do w。T F dot summary， dot create file。Rer of that run directory。
    we're going to do as default。So notice here that we're not using a train writer。
    we're creating the file writer as we sort of write it and so that's going to be
    different for everyone。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个将根据单位drop rate和学习率而有所不同。我们将进行w.T F点summary，点create file。Rer of that run directory。我们将其设为默认。所以请注意，我们没有使用train
    writer。我们创建文件写入器时会写入，因此这对每个人来说都是不同的。
- en: And then we're going to do h parameters of h parameterss。And。We're going to
    do accuracy is accuracy metric dot result。 So this right here is just going to
    record the values that was used for this run。 and then we have the accuracy。 and
    then we have Tf summary do scalar。Accuracy。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行h参数的h参数。我们将准确性定义为准确性指标结果。所以这将记录此运行中使用的值。然后我们有准确性。然后我们有Tf摘要做标量。准确性。
- en: and then we're going to send in that accuracy， and we're just going to do step
    one。And then after that， we've got to do reset the states of thecsymmetric。All
    right， now we can。 I guess remove this get model because we created the model
    for everyone。Then we we。Don't have to have that。😔，We don't have to have the number
    of Vpo or yeah， we could。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将传入那个准确性，我们只会进行第一步。之后，我们需要重置对称状态。好了，现在我们可以。我想我们可以删除这个获取模型，因为我们为每个人创建了模型。然后我们就不需要那个。😔，我们不需要Vpo的数量，或者是的，我们可以。
- en: We could have the number of epochs， but I'm actually going to just run it for
    a single epoch anyways。 so I'm going to remove that。Then we want to have the loss。
    we want to have the optimizer we want to have the acrosymmetric。 and then we don't
    need those because we're writing it already using a new bright every time。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有纪元数量，但我实际上会将其仅运行一个纪元。因此我要删除那个。然后我们想要有损失。我们想要有优化器，我们想要有对称的。然后我们不需要那些，因为我们每次都已经用新方法写了它。
- en: But then we're going to specify H nu units。We're going to do H。t H per Ram。AndWe'm
    going to do nu units。That's what we're going to call it。 then we're going to do
    Hp dot discrete。And we're going to send in 32， 64， 128。So what this means is that
    we're choosing the number of units in in that particular dense layer to be 32
    64 or 128。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将指定H nu单位。我们将进行H。t H每个Ram。我们将做nu单位。我们将称之为这个。然后我们将进行Hp dot离散。我们将传入32，64，128。这意味着我们选择该特定密集层中的单元数量为32、64或128。
- en: So we're kind of using yeah， so we're kind of using a kind of a grid search
    here and if you are familiar with hyperparameter search you probably know that
    using a random search is much more is better。And I'm not going to explain why，
    but essentially using random search is better。Of course。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这里有点像使用网格搜索，如果你熟悉超参数搜索，你可能知道使用随机搜索更好。我不打算解释原因，但本质上使用随机搜索更好。当然。
- en: what you would do here is that you would perhaps random sample the values first
    using from some maybe using random search and and then that's what you would put
    into these discrete values but anyways。 just for the illustration here I'm just
    using it in a very simple way。In discrete steps。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里需要做的是，首先可能使用随机搜索对值进行随机抽样，然后将这些值放入离散值中，但无论如何，这里只是为了说明，我用一种非常简单的方式来处理。以离散步骤进行。
- en: In a grid search manner， and then we're going to do H dropout， and we're going
    to do H dot H PRm。 we're going to send in dropout。We're going to do HB。t discrete
    and we're going to specify 0。1。O。2，0。3。And up five， so let's say we have those
    four values and then we have the HP learning rate。And let's do HB do H parameter。<|OTHER|>And
    it's called learning rate。And again。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以网格搜索的方式进行，然后我们将进行H dropout，做H dot H PRm。我们将传入dropout。我们将做HB。t离散，并指定0.1、0.2、0.3和0.5，所以假设我们有这四个值，然后我们有HP学习率。我们来做HB进行H参数。它被称为学习率。再一次。
- en: we're going to HP out discrete。嗯。And then1 e minus3，1 e minus5 and 1 e minus5，
    4 and5。 So anyways。 what you can also do here is that there is something called
    H dot int interval and then I think it's real interval。And then you can sample
    from those。But。Yeah， I don't really see the point of those really。 I would just
    sample from them myself and then find the values and then put that in as discrete
    values。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将HP出离散。嗯。然后1 e minus3，1 e minus5和1 e minus5，4和5。所以无论如何，你在这里还可以做的是，有一个叫做H dot
    int间隔的东西，我认为它是实际间隔。然后你可以从中抽样。但是。我真的看不出这些的意义。我只是自己抽样，然后找到值，然后将其作为离散值放入。
- en: So I think this this would work well in general as well。 but there might be
    some use case that I'm not really familiar with。So anyways， when we have those。We
    can change this right here， we're not going to use the normal training loop that
    we've used previously。 what we're rather going to do is we're going to do full
    learning rate in HP learning rate。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为这在一般情况下也会很好，但可能有一些我并不熟悉的用例。因此，当我们有这些时。我们可以在这里更改，我们不会使用之前用过的正常训练循环。我们要做的是使用HP学习率的完整学习率。
- en: domainin values。And we're going to do four units in H nu units that domain values。And
    also for rate in HP dropout。The domain values。In this way we're just iterating
    through the loan rate units and rate in these discrete values。Then。We're going
    to run it just for a single epoch。 but otherwise you would do you know for epoC
    in range of the number of epos right here。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 域值。我们将以 H nu 单位进行四个单元的计算，这些域值。还有在 HP dropout 中的速率。域值。通过这种方式，我们只是在这些离散值中迭代贷款利率单元。然后。我们将仅运行一次训练周期。但否则，你会在这个地方为
    epoC 的范围内进行多次训练。
- en: But we're just going to do it for one。 so then we're going to do h parameters
    is equal to a dictionary。 and then we're going to specify H learning rate is equal
    to a learning rate。And what is wrong here？
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们只会进行一次。所以我们将 h 参数等于一个字典。然后我们将指定 H 学习率等于一个学习率。那么这里有什么问题呢？
- en: 😔，Yeah， it needs to be not equal to like this and then Hum units。And we're going
    to specify that as units。An HP dropout is just rate。Alright。 and then we're gonna
    call that function， which is train model 1 epoch of H per。 So actually。 one thing
    here is that you couldn't。 you wouldn't do for epoch here because that would。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 😔，是的，需要使其不等于这样，然后 Hum 单元。我们将其指定为单元。HP dropout 仅仅是速率。好的。然后我们将调用那个函数，也就是训练模型 1
    个 H 周期。所以实际上。这一点是你不能。在这里进行多次训练，因为那样会。
- en: Sort of reset this。 Yeah， what you have to do then is modify this。 So you would
    have to add one loop in here。 I would imagine， but。Anyways。 this works just to
    illustrate how you would do it for a single epoch with different hyperparameters。This
    might take a while to run， so I'm just going to run it and hopefully we don't
    get any errors as I say that。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微重置一下。是的，你需要做的是修改这个。所以你需要在这里添加一个循环。我想象中是这样的，但是。无论如何。这只是为了说明如何在不同的超参数下进行单次训练周期。这可能需要一段时间才能运行，所以我会运行它，希望我们不会遇到任何错误。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_43.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_43.png)'
- en: Alright， so we get the H parameters right here and I'm going to just make this
    a little bit larger and you're going to see how it updates as it's getting more
    data points。All right， now we get all of these hyperparameter and that this might
    look a little bit confusing。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们在这里获得 H 参数，我会稍微放大一点，你会看到它随着数据点的增加而更新。好的，现在我们得到了所有这些超参数，这可能看起来有点混乱。
- en: So what you could do， see if I can do it， you can just specify sort of。Some
    the top ones。 the top accuracies and。In this case we can see that the top ones
    correspond to having high number of units right a larger model is always better
    basically with a learning rate of 0。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以做的就是，看看我能否做到，你可以指定某种。前几个。最高的准确性。在这种情况下，我们可以看到前几个对应于高单位数，基本上较大的模型在学习率为 0
    的情况下总是更好。
- en: 001 so the highest of the ones we chose and then preferably with the lowest
    level of dropout and again this is because we're doing it on the training set
    you would actually want to do it on the validation set to make sure that this
    is correct。But again， we just want to illustrate how this works in Tensor board。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 001，所以我们选择的这些中最高的那个，然后最好是选择最低的 dropout 级别，这样做是因为我们是在训练集上进行的，实际上你应该在验证集上进行，以确保这是正确的。但再次强调，我们只是想演示这在
    Tensor board 中是如何工作的。
- en: So that's it for using the H parameters。 Now we want to move on to doing the
    projector and the projector is very cool and can be very useful so let's move
    to that file and is we actually don't need these things So I'm going to remove
    the model and I'm going to remove let's see pretty much everything here。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所以使用 H 参数就到此为止。现在我们想要继续做投影器，而投影器非常酷且可以非常有用，所以让我们转到那个文件，其实我们不需要这些东西。所以我要删除模型，并且我将删除这里几乎所有的内容。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_45.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_45.png)'
- en: And we're not going need。Anything， really， we're going to need one batch， one
    X batch， one y batch。And we can do that by using next iter of the S strain。Then
    we're going to send that to some function。 which we're going to create plot to
    projector。We're going to send in X batch and we're going to send in X batch again。
    I'm going to explain this in a second， and then we're going to send in Y batch。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们其实不需要。任何东西，真的，我们只需要一个批次，一个 X 批次，一个 y 批次。我们可以通过使用 S strain 的下一次迭代来实现。然后我们将把它发送到某个函数。我们将创建一个绘图到投影器的函数。我们将发送
    X 批次，并且我们将再次发送 X 批次。我稍后会解释这一点，然后我们将发送 Y 批次。
- en: We're going to send in class names。We're also going to send in some log directory。
    we're just going to call it。Proge like that projector。 And so why we're sending
    in exppat to times here is because。This function we're going to create is going
    to obviously take in some some data points， right。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将发送类名。我们还会发送一些日志目录。我们就叫它Proge，像投影仪那样。因此，我们在这里发送exppat两次的原因是，这个我们要创建的函数显然会接收一些数据点，对吧。
- en: the images。And it's going to take in some labels but it's also going to take
    in some feature vector。 so normally you would have sent in your your images， your
    data。To your model and then out from that model， you would take some feature vector。
    which might be in some arbitrary number of dimensions。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像。它将接收一些标签，但也会接收一些特征向量。通常，你会将你的图像、数据发送到模型，然后从模型中获取一些特征向量，特征向量可能是任意维度的数字。
- en: So then this projector is going to take in this feature vector and try to obtain
    a projection down to the images and see sort of what of the features and use one
    of these algorithms like TS&E or PCA to project it down and see what does our
    model sort of think are in similar groups depending on what the feature vector
    says。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这个投影仪将接收这个特征向量，并试图将其投影到图像上，看看哪些特征，以及使用像TS&E或PCA这样的算法进行投影，看看我们的模型认为在特征向量所描述的相似组中有什么。
- en: And what we're going to do in this case is we're not going to send it into some
    model。 So what that means is we're going to use the images itself。 We're going
    to reshape the images。To just be one long feature vector with the pixel values
    for that image。 and that's going to be our feature vector。Again， as most of this
    stuff in this tutorial it's been for illustrating how to use Tensorboard so this
    is how to make it simpler for us。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 而我们在这种情况下要做的是不将其发送到某个模型。这意味着我们将使用图像本身。我们将重塑图像。将其变为一个长的特征向量，包含该图像的像素值。这将是我们的特征向量。同样，正如本教程中大多数内容一样，它是为了说明如何使用Tensorboard，因此这就是为了简化我们的操作。
- en: but normally this would be a feature vector so just to be U clear， you would
    feature vector。 you would do model of Xpat， you would get some feature vector
    and here you would call in with that feature vector。Now we're not going to do
    that， as I said， so all we need to do is create this plot to projector。And before
    I forget， let's do from U's import plot to projector。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常这将是一个特征向量，为了让你清楚，你会得到特征向量。你会用Xpat做模型，你会得到一个特征向量，然后在这里你会调用那个特征向量。现在我们不会这样做，正如我所说，我们需要做的就是创建这个plot
    to projector。在我忘记之前，让我们从U中导入plot to projector。
- en: And then we're going to go to these u functions again。And we're going to need
    two things。First of all， we're going to need something called a create sprite
    image and a sprite image is essentially that we're going to take the image。 the
    input or the X patch， the input X patch。And we're going to create a sprite image
    and a sprite image is that it's going to have one image with all of the images
    in that batch that we send in。 So let me see if I can bring up one example of
    that。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将再次使用这些u函数。我们需要两个东西。首先，我们需要一个称为创建精灵图像的东西，精灵图像本质上是我们将提取的图像，输入或X patch，输入的X
    patch。我们将创建一个精灵图像，它将包含我们发送的那个批次中的所有图像。让我看看我能否举一个例子。
- en: So a sprite image of emnist is going or this is fashion emnist。 So this right
    here。 this is a sprite image。 as you can see it's a sort of a collection of of
    many， many。 many different images inside that single one and。That's going to be
    useful to send to Tensor board so that we just have to send one image rather than
    many。 and so that's what we need to do first of all。And。I'm not going to do this。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以emnist的精灵图像是这样的，或者说这是时尚emnist。所以就在这里。这是一个精灵图像。正如你所看到的，这是一组许多、许多不同图像的集合，集中在一个单一的图像中。这将对发送到Tensor
    board非常有用，这样我们只需发送一张图像，而不是多张。因此，这就是我们首先需要做的。我不会这样做。
- en: I'm just going to steal it and I'm going to copy paste it。 so this is stolen
    from this guy。 Andrew B。 Martin and。Yeah， you can read through this， we're just
    going to use it。 I'm going not going to focus on what this does。SoBut essentially，
    it creates this sprite image and。So basically it does some reshaping and some
    stuff and adds padding to get this image。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我打算直接偷用它，我会复制粘贴。所以这段内容是从这个人那里偷来的。安德鲁·B·马丁。是的，你可以阅读一下，我们就要使用它。我不会专注于它的功能。基本上，它创建了这个精灵图像，并进行一些重塑和处理，添加填充以获得这个图像。
- en: We're going to create this define plot to projector function and what we're
    going to send in is X。 that feature vector I was talking about， Y， class names。
    and we're going to specify some log directory。As default。 let's just say default
    log trajectoryy and then let' see we're going to also get Me file。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建这个定义投影到投影仪的函数，我们要传入的是 X。就是我所说的特征向量，Y，类名。我们将指定一些日志目录。默认情况下。我们就称之为默认日志轨迹，然后我们还将获得
    Me 文件。
- en: And we're going to specify metadata。 TSv。All right， so this is going to make
    a lot more sense。For what we're going to use them for。So first of all。 we need
    to sort of make sure that the number of x dimensions is four。Because what we're
    going to do is we're going to assume it's of this form where we have the batch
    size。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定元数据。TSV。好吧，这样会让我们用它们的目的更清晰。因此首先，我们需要确保 X 维度的数量为四。因为我们将假设它的形式是这样的，我们有批量大小。
- en: we have the height， the width， and then the channels。And one thing that can
    be super frustrating is that。You can't run this if youve already if you have a
    directory with this log directory already and so what we're going to do is we're
    going to do if OSpa is directory。 so if there is a directory in log direction
    that means that there's some old projector files there。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有高度、宽度和通道。非常令人沮丧的一点是，如果你已经有一个包含此日志目录的目录，你是无法运行这个的。因此，我们将进行检查，如果 OS.path 是目录。也就是说，如果日志方向中存在目录，则意味着那里有一些旧的投影仪文件。
- en: so what we're going to do then is we're going to do shuttle。 remove3 and remove
    tree of that log directory。And then， we're gonna make their。Make that directory
    of that log directory。 So essentially， we're making it clean from scratch。 So
    we're gonna。Create a new， clean， fresh folder。Alright， then we're going to do。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行 shuttle。移除 3 和移除树的日志目录。然后，我们将创建日志目录。基本上，我们从头开始清理。所以我们将创建一个新的、干净的、全新的文件夹。好吧，然后我们将继续。
- en: I'm going to specify the Sprites file， and that's going to be OS path join of
    log directory。 and then we're just going to call it sprite sub PNG。And we're going
    to do sprite is create sprite of x。Of input the input data。 then we're going to
    do CM M V2， that inrite spites file。What the sprites file and then the sprite。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我将指定 Sprites 文件，它将是日志目录的操作系统路径连接，然后我们将称之为 sprite_sub.png。接着我们将生成 sprite 是输入数据的创建精灵。然后我们将做
    CM M V2，写入精灵文件。精灵文件和精灵。
- en: All right， then we want to generate the label names。So we're going to do that
    by doing labels is class names of y of I。 so each respective integer target value
    for each training example， so I is a training example。 then we're going to do
    for I in range of int of y shape of0。Like that。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，然后我们想要生成标签名称。我们将通过将标签设置为 Y 的类名来实现。因此每个训练示例的相应整数目标值，I 是训练示例。然后我们将在 Y 的形状范围内进行循环。
- en: Then we're going to write to that metadata TSV file and that's used for plotting
    in the labels in the projector。 so we're going to do open OS Pat join， log directory
    and that metadata met file。 that's what we called it， and then we're going to
    write to that one。And we're going to open it as F。So for label in labels， we're
    going to do Ft right。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将写入元数据 TSV 文件，这将用于在投影仪中绘制标签。所以我们将打开操作系统路径连接，日志目录和元数据文件。这就是我们所称的，然后我们将写入这个文件。我们将以
    F 的方式打开它。因此，对于标签中的每一个，我们将进行 Ft 写入。
- en: We're going to do new line and we're going to do dot format label， all？
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行换行，并使用点格式标签，全都吗？
- en: So we're just going to send in the label and then we're going to make a new
    line and that's just the structure tensor word wanted in。And now what I'm going
    to do is we're going to do the feature vector。 So we're going to do if feature
    vector dot n dimension is not equal to 2。 that would mean that it's not a feature
    vector so that would mean that， for example。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们只会传入标签，然后我们将换行，这只是我们想要的结构张量。现在我将做的是处理特征向量。如果特征向量的维度不等于 2，那就意味着这不是一个特征向量，比如说。
- en: like we are doing it， we're sending in exppat two times。 So if that is the case。
    then we're going to do print。<|OTHER|>And we're going to do see note。Feature vector
    is not a form batch comma featured。And then we can do reshaping to try and get
    it to this form。And then what we're going to do is feature vector。Is equal to
    Tf do reshape。And then feature vector。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们现在所做的，我们两次发送 exppat。所以如果是这样的话，我们要做 print。然后我们要看注释。特征向量不是一个形式批次逗号特征。然后我们可以进行重塑，尝试把它变成这个形式。然后我们要做的是特征向量。等于
    Tf do reshape。然后特征向量。
- en: and then。Feature vector dot shape0， and then minus-1。Allright， and then we're
    going to do。Feature vector is T F variable。 So we're just gonna convert it to
    a T F variable。And then we're going to do checkpoint is Tf train dot checkpoint
    of embedding equals feature vector。And then checkpoint dot save and then OS path
    showing log directory。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后。特征向量 dot shape0，然后是 -1。好的，然后我们要做。特征向量是 T F 变量。所以我们只需将其转换为 T F 变量。然后我们要做 checkpoint
    是 Tf train dot checkpoint 的嵌入等于特征向量。接着 checkpoint dot save，然后 OS path 显示日志目录。
- en: And then we're going to specify embeddings。ckKP。嗯。So we're basically saving
    that feature vector into the log directory and then embedding that CKPT thats
    what。Tenssor board wants to be called and then。We're going to set up a config
    that's going to be right that's going to write to to the projector。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将指定 embeddings. ckKP。嗯。基本上我们将特征向量保存到日志目录，然后嵌入那个 CKPT，这就是 TensorBoard 想要的名称。然后我们将设置一个配置，这个配置将写入
    projector。
- en: So we're going to config is projector dot projector config。Embbedding is config
    do embeddings add。And then we're going to do embedding dot tensor name， we're
    going to specify embedding。 and then we need to do this right here， we need to
    do dot attribute。 and then variable name variable value。And then we need to do
    embedding that met data path。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要配置的是 projector dot projector config。嵌入是 config do embeddings add。然后我们要做
    embedding dot tensor name，我们要指定嵌入。然后我们需要在这里做，做 dot attribute。然后是变量名变量值。接着我们需要做嵌入的
    met data path。
- en: We need to do meta file。And。Yeah， if you feel this is a little bit clumsy， I
    mean。 the code is kind of。There's a lot of code to do this。I wouldn't focus too
    much on the specific code in Pythtor， for example this is done in one line。 literally
    one line， but in TensorFlow this is a little bit more difficult the embedding
    projector is usually done to project embeddings makes sense right。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做 meta file。是的，如果你觉得这有点笨拙，我的意思是。代码有点。要做这个有很多代码。我不会太关注具体的 Pythtor 代码，例如这在一行中完成。字面上是一行，但在
    TensorFlow 中这有点困难，嵌入投影器通常用于投影嵌入，这没错吧。
- en: When we're doing it for images， we need to do it in this。Very convoluted way。
    but anyways this is how we do it， but if you find a better way than do comment。
    but this is the best way that I've found so far。Then we're going to do embedding。
    sprite。 image path， and then we're going to do sprite。p andng。 So we're just specifying
    the image path。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为图像这样做时，我们需要以这种。非常复杂的方式来做。但无论如何，这就是我们的做法，如果你发现更好的方法，请留言。但这是我目前找到的最好方法。然后我们将做嵌入。sprite。图像路径，然后我们将做
    sprite。p 和 ng。所以我们只是在指定图像路径。
- en: And then we're doing embedding thatt sp single image dimension extend。 And then
    we need to specify x shape of1 and then x shape of 2。 So we're specifying the。At
    the height and the width。And lastly， we got to do projector， visualize embedding。
    and we're going to send in log directory， and then the config and thats。That's
    it。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们正在做嵌入 thatt sp 单图像维度扩展。然后我们需要指定 x 形状为 1，接着 x 形状为 2。所以我们指定了高度和宽度。最后，我们必须做
    projector，visualize embedding。我们要发送日志目录，然后是配置，这样就好了。
- en: so now hopefully if there are no errors。This should run。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_47.png)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在希望如果没有错误的话。这应该可以运行。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_47.png)
- en: And this is not right。😔。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_49.png)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 而这并不正确。😔。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_49.png)
- en: So the error is that we when we're normalizing the images， when we're creating
    these sprite images。 we need to make sure that we don't divide by 255 and。If we
    rerun it。We should hopefully obtain the correct pixel values now。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_51.png)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所以错误是当我们对图像进行归一化时，当我们创建这些精灵图像时。我们需要确保不除以 255。如果我们重新运行它。我们现在应该能够获得正确的像素值。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_51.png)
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_52.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_52.png)'
- en: And as you can see here， we have the images and we can see right there that's
    a dog and then that's a boat or a ship。That's a horse， I think， and let's see
    what else。Yeah， anyways， and you can play around with this。 We have very few images
    here， right， So what you would probably want to do is you would change the batch
    size to something like 500。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们有这些图像，我们可以看到那是一只狗，然后那是一艘船。我认为那是一匹马，让我们看看还有什么。是的，反正你可以玩玩这个。我们这里的图像很少，所以你可能想做的是将批量大小更改为500。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_54.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_54.png)'
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_55.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_55.png)'
- en: And if we rerun it， this is quite quick， actually。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_57.png)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新运行，这个速度实际上很快。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_57.png)
- en: Alright， so what you need to do here， I think if you rerun it， you need to do。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_59.png)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，所以我认为你在这里需要做的，如果你重新运行，你需要做。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_59.png)
- en: Sort of a new one。 So if we rerun it now。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_61.png)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有点像一个新的。所以如果我们现在重新运行它。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_61.png)
- en: And then we open it again in Tensor board。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_63.png)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在Tensorboard中再次打开它。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_63.png)
- en: Sometimes this works， sometimes it doesn't。But now it works。 allright。 So now
    we get 500 images and then you could do， you know， use the T S and E PC A and
    this also。 I've tried to make it。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_65.png)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有时这有效，有时无效。但现在它有效。好吧。现在我们得到500张图像，然后你可以使用T-SNE和PCA，这也是我尝试制作的。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_65.png)
- en: General， in the way that we could just replace this by Mist and we could just
    run it directly and then if we now check it。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_67.png)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通用，按我们可以将其替换为Mist的方式，我们可以直接运行它，然后如果我们现在检查它。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_67.png)
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_68.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_68.png)'
- en: All right。Yeah。Then of course we can't do this data augmentation。 so let's remove
    the data augmentation completely， but we can't convert something to gray scale
    that's already grayscale。 but other than that， I think this should be general
    in the way that we can just change that to Mist。 and then you would be able to
    project。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。是的。那么当然我们无法进行数据增强，所以让我们完全移除数据增强，但我们无法将已经是灰度的东西转换为灰度。不过除此之外，我认为这应该是通用的，我们可以将其更改为Mist。然后你将能够进行投影。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_70.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_70.png)'
- en: Any type that you like。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_72.png)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢的任何类型。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_72.png)
- en: Al right， white wait， there's one more thing。 actually。 Yeah， so this is the
    last file of this one。 but。I want to show you the other thing too in a Tensorflowlow
    Pror。 so Tensorflow Pror is available since Tensorflowlow 2。2 and if you follow
    these tutorials you know that I'm using TensorFlow 2。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，等等，还有一件事，实际上。是的，这是最后一个文件。但我也想在TensorFlow Pro中展示另一个东西。TensorFlow Pro自TensorFlow
    2.2以来可用，如果你跟着这些教程，你会知道我使用的是TensorFlow 2。
- en: 1 on the GPU and so I can't use it and it's actually necessary that you have
    a GPU to run。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_74.png)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我在GPU上运行，因此我无法使用它，实际上你需要一个GPU来运行。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_74.png)
- en: So what I did is I did it in。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_76.png)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我所做的是。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_76.png)
- en: Google Collab， and this is following very closely to one of their official tutorials。But
    so essentially， I'm going to just step through this and perhaps you're watching
    this video。In the future when you can easily install TensorFlow 2。2 and above
    with Ananaconda。 and so you can probably do this and create just a new file。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab，这与他们的官方教程非常接近。但实际上，我将逐步进行，也许你在未来观看这个视频时，你可以轻松安装TensorFlow 2.2及以上版本与Anaconda。所以你可能可以做到这一点，并创建一个新的文件。
- en: so you wouldn't have to do this on Google Collab， but this works just to illustrate。All
    right。 so first of all， we just have some import right here and then we got to
    do this Pip install Tensor board plugin profile。We're importing Tensorflow and
    here we're making sure that we're running on the GPU， as I said。 that's a requirement。And
    then TensorFlow， I think it works on the CPU， but for some functionality。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你不需要在Google Collab上执行这个，但这样做只是为了说明。首先，我们在这里有一些导入，然后我们需要执行这个Pip安装Tensorboard插件profile。我们正在导入Tensorflow，在这里我们确保我们在GPU上运行，正如我所说的。这是一个要求。然后TensorFlow，我认为它在CPU上也可以工作，但某些功能需要。
- en: there's a requirement that you do it on the GPU。So then we're doing tensor data
    sets。 we're importing MNIS， very similar to what you've。To what you have seen
    throughout this tutorial。Then we're normalizing the image， we're doing some map
    and then batch。 then we create some very simple model with a model compile。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个要求是你必须在GPU上执行。所以我们在做tensor数据集。我们正在导入MNIST，非常类似于你在这个教程中看到的。然后我们对图像进行归一化，进行一些映射，然后批处理。接着我们创建一个非常简单的模型并进行模型编译。
- en: And then we create some logs and they create these logs。 this is from Tensflowlow
    official tutorial using this daytime now。 which is kind of clever so that you
    would get a new file。 new log file every time you run it because the time has
    changed。So that's a pretty good way to avoid those errors instead of doing in
    the projector way of removing the folder if it exists。 then doing something like
    this could be easier， I think。All right。 so then we havesor tensor board callback
    and we're just creating that and all they're doing is doing this profile batch
    and they're doing it 500 to 520 and。So this is using it with Tensor board callbacks，
    but there are multiple ways you can do it if you have custom loops as well。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一些日志，并生成这些日志。这是来自Tensorflow官方教程，使用当前时间的日志，这种方法非常聪明，因为每次运行时会生成一个新的日志文件，因为时间发生了变化。所以这是一种很好的避免错误的方法，而不是以项目的方式删除文件夹，如果存在的话。这样做可能更简单，我认为。好吧，然后我们有tensor
    board回调，我们只是在创建它，他们所做的就是执行这个profile批次，他们执行500到520。所以这是使用Tensor board回调，但如果你有自定义循环，也有多种方式可以做到这一点。
- en: I'm going to reference to that official tutorial in the description of this
    video if you want to check out more how you would do it。And then we're just doing
    model that fit and again using this tensor board to call back。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_78.png)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多如何实现，我将引用这个官方教程链接在视频描述中。然后我们只是在做模型拟合，并再次使用这个tensor board进行回调。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_78.png)
- en: And then if we then do load extension of Tensor boards。 so we open up Tensor
    board and we run it with log de equal logs。Then we can get all of these different
    things， so we get an overview page。 and maybe I need to rerun this。I think so，
    so we're gonna。Factory reset runtime。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果我们扩展Tensorboard的负载。我们打开Tensorboard并使用日志de equal logs运行它。然后我们可以得到这些不同的东西，所以我们得到了一个概览页面。也许我需要重新运行这个。我想是的，所以我们将进行工厂重置运行时。
- en: and then we're going to run all。So then we can open up the profiler and what
    we'll get is we have some different tools here we have an overview page and to
    be honest I haven't looked into these in detail。 but I'll show you just an overview
    of what they do so this is the one that's perhaps the easiest to look at and the
    easiest to sort of understand here they give a summary of all the different parts
    so you can see the timing for all of the different components of the training。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将运行所有内容。所以然后我们可以打开分析器，我们将得到一些不同的工具，我们有一个概览页面，老实说，我还没有详细查看这些。但我会给你展示它们的概述，这可能是最容易查看和理解的地方，他们给出了所有不同部分的摘要，所以你可以看到训练的所有不同组件的时间。
- en: you can also see different things， the placement let's number of TF ops executed
    on the host and the device。 not really sure what the difference between host and
    devices here。So again。 I'm no expert in terms profiler， but maybe someone can
    comment what this difference is。But other things you can see here is like the
    device compute precision， if you're using  16 bit。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到不同的东西，放置让TF操作在主机和设备上执行的数量。我不太确定这里主机和设备之间的区别是什么。所以再次强调，我不是性能分析的专家，但也许有人可以评论这个区别。不过，你可以在这里看到的其他内容包括设备计算精度，如果你使用的是16位。
- en: how many percentage of the computation is done on 16 bit？And then。You can see
    some recommendations here and this graph is quite useful as well。 so you can see
    for example， the input is highly input bound， 85% is waiting for input。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_80.png)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 计算中有多少百分比是在16位上完成的？然后，你可以看到一些推荐，这个图表也相当有用。比如说，你可以看到输入高度输入绑定，85%在等待输入。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_80.png)
- en: So what we could do， for example， is now that we know that we could do dot dot
    cache。 what could we do else we could do non parallel calls is autotune。And we
    could do that for both。 so none。😔，Parallel calls is auto tune， and then we can
    do。Ado cache。And then we could also do dot pre fetch and we can do auto tuneune
    dot pre fetchch， pre fetchch。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，现在我们知道可以做点缓存，我们还能做什么呢？可以进行非并行调用的自动调优。我们可以对两者都这样做。所以没有。😔，并行调用是自动调优，然后我们可以做。缓存。然后我们还可以做点预取，可以进行自动调优的点预取。
- en: And we could again send in auto tune。 and yeah， I guess I'll just。Rerun this
    again。So。Run time。 run all， and we'll see if there's any difference in the。In
    the output from the Tensorflowlow Pror。Oh by the way， this profile batch is between
    which batch and this says that you want to profile so right here we want to profile
    between the index batch of 500 two index batch of 520 so we're just doing the
    profile for 20 batches so if you do it for too many batches you could get out
    of memory errors so that's why they recommend doing just 20 or 10 batches and
    also not to put it in the initial batches rather just to do it in the middle of
    training because it could take some time to initialize things。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次发送自动调优，是的，我想我会重新运行这个。所以。运行时间，运行全部，我们将看看输出是否有任何差异。在TensorFlow分析器中，顺便说一下，这个分析批次是在两个批次之间，这里说明你想要分析的内容，所以在这里我们想要分析从500的索引批次到520的索引批次，所以我们只是对20个批次进行分析，如果你做得太多批次，可能会出现内存不足的错误，因此他们建议只做20个或10个批次，并且最好不要放在初始批次中，而是在训练中间进行，因为初始化某些东西可能需要时间。
- en: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_82.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_82.png)'
- en: Alright， so let's again look at the profiler and see if there's any difference
    so right now we can see that your program is moderately input bound because 160%
    is waiting for input。 but that's a major improvement over the 85% that we just
    had。So let's see what more you have。
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我们再看看分析器，看看是否有什么不同，现在我们可以看到你的程序是适度地输入绑定，因为160%在等待输入。但这比我们刚刚的85%有了很大的改善。那我们再看看你还有什么。
- en: Actually， you know what， you can play around with this yourself and you can
    see。 and I'm also going to reference to a Tensor flow。Official tutorial worthy
    going into more depth on this Tensorflow profiler。But yeah， so for example here
    you can see sort of the matrix multiply that is taking up 25%。 the biggest chunk
    of the operation time， and then you have different things like trace viewer and
    that's you can get a very。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你知道吗，你可以自己尝试一下，你会看到。我还将参考一个TensorFlow的官方教程，值得深入研究这个TensorFlow分析器。但是，是的，举个例子，你可以看到矩阵乘法占用了25%。这是操作时间中最大的一块，然后还有不同的东西，比如追踪查看器，你可以获得一个非常。
- en: very low level view of all of the operations done on the CPU and GP and you
    can see exactly what is taking up time。So this is a very， very cool tool， this
    is more of an introductory to that and just show you how to get it up and running
    and yeah so。That's it。 This is going to be a very long video。 I can see that。
    and hopefully。This video is at least useful in understanding Tensor board。Yeah。
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 低级别的视图，查看CPU和GPU上执行的所有操作，确切地看到哪些占用了时间。所以这是一个非常非常酷的工具，这更多是一个入门介绍，展示如何让它运行，是的。所以就是这样。这将是一个非常长的视频。我可以看到这一点。希望这个视频至少在理解TensorBoard方面是有用的。是的。
- en: thank you so much for watching the video and I hope to see you in the next one。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_84.png)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你观看这个视频，希望在下一个视频中见到你。![](img/41a5fdcb23b17bc7ced9ef9ac3dfb245_84.png)
