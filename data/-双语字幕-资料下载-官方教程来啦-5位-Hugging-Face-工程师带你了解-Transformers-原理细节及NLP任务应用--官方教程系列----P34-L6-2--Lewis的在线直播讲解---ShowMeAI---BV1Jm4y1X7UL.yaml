- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P34ï¼šL6.2- Lewisçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥äº†ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠ NLP ä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P34ï¼šL6.2- Lewis çš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
- en: So basically the goal of this session is to go through chapter 2 together and
    in this chapter what we're going to be doing is diving into the sort of internals
    of the transformers library and in particular we're going to be looking at the
    sort of models so there's a set of model APIs that we're going to look at and
    also the tokenizers which we relied on heavily to convert text into a format that
    the models can processã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ¬¡ä¼šè®®çš„åŸºæœ¬ç›®æ ‡æ˜¯ä¸€èµ·é˜…è¯»ç¬¬äºŒç« ï¼Œåœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å˜æ¢å™¨åº“çš„å†…éƒ¨ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬å°†å…³æ³¨æ¨¡å‹çš„ç±»å‹ã€‚æœ‰ä¸€ç»„æ¨¡å‹ API æˆ‘ä»¬å°†è¦æŸ¥çœ‹ï¼ŒåŒæ—¶ä¹Ÿä¼šçœ‹åˆ°æˆ‘ä»¬åœ¨å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼æ—¶æ‰€ä¾èµ–çš„åˆ†è¯å™¨ã€‚
- en: And so you've seen in the first lessons that we haveã€‚A pipeline APIã€‚And this
    pipeline API basically wraps all of the complexity of pre processing and post
    processing text and also fitting it to the model so that you just have to basically
    give it a sentence and then you could classifyã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€è¯¾ä¸­ä½ çœ‹åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªç®¡é“ APIã€‚è¿™ä¸ªç®¡é“ API åŸºæœ¬ä¸Šå°è£…äº†æ–‡æœ¬çš„é¢„å¤„ç†å’Œåå¤„ç†çš„æ‰€æœ‰å¤æ‚æ€§ï¼ŒåŒæ—¶ä¹Ÿå°†å…¶é€‚é…åˆ°æ¨¡å‹ä¸­ï¼Œè¿™æ ·ä½ åªéœ€è¦ç®€å•åœ°ç»™å®ƒä¸€å¥è¯ï¼Œç„¶åå°±å¯ä»¥è¿›è¡Œåˆ†ç±»ã€‚
- en: for exampleï¼Œ the sentimentã€‚And today we just want to sort of unpack what's happening
    inside this function and also to understand some of the different sort of approaches
    you can take for tokenizing your text and also how to save and load the models
    and tokensã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚æƒ…æ„Ÿåˆ†æã€‚ä»Šå¤©æˆ‘ä»¬å¸Œæœ›è§£è¯»è¿™ä¸ªå‡½æ•°å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¹¶ç†è§£ä½ å¯ä»¥é‡‡ç”¨çš„ä¸€äº›ä¸åŒæ–¹æ³•æ¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œä»¥åŠå¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å’Œæ ‡è®°ã€‚
- en: And we'll finish by looking at what you have to kind of do when you're dealing
    with sentences or texts that have different lengthsã€‚ because it turns out that
    in pytorrch and Tensorflow and most deep learning frameworksã€‚ we need a kind of
    standardized sort of rectangular input for our modelsã€‚And basically the way we're
    going to do this is I'm going to go through the sections and then pause for questionsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€šè¿‡æŸ¥çœ‹åœ¨å¤„ç†é•¿åº¦ä¸åŒçš„å¥å­æˆ–æ–‡æœ¬æ—¶éœ€è¦åšçš„äº‹æƒ…æ¥ç»“æŸè¿™èŠ‚è¯¾ã€‚å› ä¸ºåœ¨ pytorch å’Œ Tensorflow ä»¥åŠå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ ‡å‡†åŒ–çš„çŸ©å½¢è¾“å…¥æ ¼å¼ä¾›æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ã€‚åŸºæœ¬ä¸Šæˆ‘ä»¬å°†é€šè¿‡è®²è§£å„ä¸ªéƒ¨åˆ†ç„¶åæš‚åœæé—®æ¥è¿›è¡Œã€‚
- en: but in the meantimeï¼Œ if you have some sort of very urgent thing that you want
    to askã€‚ Omar will be here helping us with guidanceã€‚So just to give you a taste
    of like what we're going to be sort of covering but at a higher levelã€‚ every single
    model in the Transformers' library has a corresponding modeling fileã€‚ so for example
    here what I'm looking at is the modeling file for BERTã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨æ­¤æœŸé—´ï¼Œå¦‚æœä½ æœ‰ä¸€äº›éå¸¸ç´§æ€¥çš„é—®é¢˜æƒ³è¦è¯¢é—®ï¼ŒOmar ä¼šåœ¨è¿™é‡Œæä¾›æŒ‡å¯¼ã€‚æ‰€ä»¥è®©æˆ‘ç»™ä½ ä¸€ä¸ªé«˜å±‚æ¬¡çš„æ¦‚å¿µï¼Œå±•ç¤ºæˆ‘ä»¬å°†è¦è¦†ç›–çš„å†…å®¹ã€‚å˜æ¢å™¨åº“ä¸­çš„æ¯ä¸€ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªç›¸åº”çš„å»ºæ¨¡æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œæˆ‘ç°åœ¨æŸ¥çœ‹çš„æ˜¯
    BERT çš„å»ºæ¨¡æ–‡ä»¶ã€‚
- en: And this this file has all of the source code for all of the different tasks
    that you can use BRT for so for exampleã€‚ if I look forã€‚Vt modelï¼Œ this is the sort
    of base class that we're going to be looking at today which is responsible for
    basically creating contextualized embeddings of the inputs so how do we create
    kind of numerical representations of our text that have some sense of meaning
    and this kind of class is relatively simple it just has the embedding layer which
    you saw in the first chapter so the thing that we pass through before we hit the
    transformer stack and then we have an encoder and this encoder is essentially
    responsible for converting these tokens or these token embeddings into these contextualized
    representationsã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†æ‰€æœ‰ä¸åŒä»»åŠ¡çš„æºä»£ç ï¼Œä½ å¯ä»¥ä½¿ç”¨ BRTã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘æŸ¥æ‰¾ Vt æ¨¡å‹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ä»Šå¤©è¦å…³æ³¨çš„åŸºæœ¬ç±»ï¼Œå®ƒè´Ÿè´£åŸºæœ¬ä¸Šåˆ›å»ºè¾“å…¥çš„ä¸Šä¸‹æ–‡åµŒå…¥ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•åˆ›å»ºä¸€äº›å…·æœ‰æ„ä¹‰çš„æ–‡æœ¬çš„æ•°å€¼è¡¨ç¤ºå‘¢ï¼Ÿè¿™ä¸ªç±»ç›¸å¯¹ç®€å•ï¼Œå®ƒåªæœ‰åµŒå…¥å±‚ï¼Œä½ åœ¨ç¬¬ä¸€ç« çœ‹åˆ°è¿‡ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿›å…¥å˜æ¢å™¨å †æ ˆä¹‹å‰ä¼ é€’çš„å†…å®¹ï¼Œç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ªç¼–ç å™¨ï¼Œè¿™ä¸ªç¼–ç å™¨æœ¬è´¨ä¸Šè´Ÿè´£å°†è¿™äº›æ ‡è®°æˆ–æ ‡è®°åµŒå…¥è½¬æ¢ä¸ºè¿™äº›ä¸Šä¸‹æ–‡åŒ–çš„è¡¨ç¤ºã€‚
- en: And I just recommend sort of let your homework have a look through some of this
    codeã€‚ so any sort of class that you see us using todayï¼Œ for exampleï¼Œ BERT modelã€‚
    have a look at the sort of source code and this really helps you understand how
    transformers work and at least for me personally it was only by sort of going
    through this kind of step by step and understanding how all the inputs go through
    the forward pass that I was really able to understand all the workings of a transformerã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å»ºè®®ä½ å¯¹è¿™äº›ä»£ç è¿›è¡Œä¸€äº›ç ”ç©¶ï¼Œå› æ­¤ä»Šå¤©æˆ‘ä»¬ä½¿ç”¨çš„ä»»ä½•ç±»ï¼Œä¾‹å¦‚BERTæ¨¡å‹ï¼Œéƒ½å¯ä»¥æŸ¥çœ‹å…¶æºä»£ç ï¼Œè¿™çœŸçš„æœ‰åŠ©äºç†è§£transformersçš„å·¥ä½œåŸç†ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œåªæœ‰é€æ­¥äº†è§£æ‰€æœ‰è¾“å…¥å¦‚ä½•é€šè¿‡å‰å‘ä¼ é€’ï¼Œæ‰èƒ½çœŸæ­£ç†è§£transformerçš„æ‰€æœ‰å·¥ä½œæœºåˆ¶ã€‚
- en: So that's just a little side noteã€‚Soã€‚To get startedã€‚ maybe let's have a look
    at what really happens behind the pipelineã€‚ so let's kick start with this videoã€‚![](img/40873acb06abf924ac4a43fae802679a_1.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªå°æ’æ›²ã€‚å› æ­¤ï¼Œä¸ºäº†å¼€å§‹ï¼Œæˆ–è®¸æˆ‘ä»¬å¯ä»¥çœ‹çœ‹åœ¨pipelineåé¢å®é™…å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæ¥å¯åŠ¨è¿™ä¸ªè§†é¢‘ã€‚![](img/40873acb06abf924ac4a43fae802679a_1.png)
- en: What happens inside the pipelinebra functionï¼ŸIn this videoã€‚ we'll look at what
    actually happens when we use the pipeline function of the transformformerss libraryã€‚Well
    specificallyï¼Œ well look at the sentiment analysis pipelineã€‚ and now it went from
    the two following sentences so the positive and negative labels were respective
    scoresã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipelinebra`å‡½æ•°å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿåœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ä½¿ç”¨transformersåº“çš„pipelineå‡½æ•°æ—¶å®é™…å‘ç”Ÿçš„äº‹æƒ…ã€‚æˆ‘ä»¬å°†ç‰¹åˆ«å…³æ³¨æƒ…æ„Ÿåˆ†æpipelineã€‚å®ƒçš„æ­£é¢å’Œè´Ÿé¢æ ‡ç­¾å¯¹åº”äºä»¥ä¸‹ä¸¤ä¸ªå¥å­çš„åˆ†æ•°ã€‚'
- en: As we've said in the byprint presentationã€‚There are three stages in the pipelineã€‚Firstã€‚
    we convert the verex to numbers the model can make sense of using a tokenizerã€‚Then
    those numbers goes through the model which outputs lowsã€‚Finallyã€‚ the first processing
    steps transform Voor gets into labels and scoresã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨æ¼”ç¤ºä¸­æ‰€è¯´ï¼Œpipelineä¸­æœ‰ä¸‰ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†è¯å™¨å°†è¯æ±‡è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚ç„¶åï¼Œè¿™äº›æ•°å­—é€šè¿‡æ¨¡å‹è¾“å‡ºã€‚æœ€åï¼Œç¬¬ä¸€æ­¥å¤„ç†å°†è¾“å‡ºè½¬æ¢ä¸ºæ ‡ç­¾å’Œåˆ†æ•°ã€‚
- en: Let's look in details at those three steps and how to replicate their using
    the Transformerss libraryã€‚ beginning with the first stage tokenizationã€‚The to
    process has several steps firstã€‚ the text is split into small chunks called tokensã€‚They
    can be wordsã€‚ part of words or punctuation symbolsï¼Œ then the tokenser will add
    some special tokens if the model expectedã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯¦ç»†äº†è§£è¿™ä¸‰ä¸ªæ­¥éª¤ä»¥åŠå¦‚ä½•ä½¿ç”¨Transformersåº“å¤åˆ¶å®ƒä»¬ï¼Œé¦–å…ˆæ˜¯åˆ†è¯é˜¶æ®µã€‚è¯¥è¿‡ç¨‹æœ‰å‡ ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç§°ä¸ºtokensçš„å°å—ã€‚è¿™äº›å¯ä»¥æ˜¯å•è¯ã€å•è¯çš„ä¸€éƒ¨åˆ†æˆ–æ ‡ç‚¹ç¬¦å·ï¼Œå¦‚æœæ¨¡å‹é¢„æœŸçš„è¯ï¼Œåˆ†è¯å™¨ä¼šæ·»åŠ ä¸€äº›ç‰¹æ®Šçš„tokensã€‚
- en: Hereï¼Œ the middle used expect a seal token at the beginning and a sep token at
    the end of the sentence to classifyã€‚Lastlyï¼Œ the token ison patches each token
    to its unique ID in the vocabulary of the pro modelã€‚To load the tokenizerï¼Œ the
    transformformers library provides the Utokenizer APIã€‚The most important method
    of this class is from Pretrainedã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œä¸­é—´æ¨¡å‹æœŸæœ›åœ¨å¥å­å¼€å¤´æœ‰ä¸€ä¸ªèµ·å§‹tokenï¼Œå¥å­æœ«å°¾æœ‰ä¸€ä¸ªåˆ†éš”tokenã€‚æœ€åï¼Œåˆ†è¯å™¨å°†æ¯ä¸ªtokenæ˜ å°„åˆ°å…¶åœ¨æ¨¡å‹è¯æ±‡ä¸­çš„å”¯ä¸€IDã€‚è¦åŠ è½½åˆ†è¯å™¨ï¼Œtransformersåº“æä¾›äº†Utokenizer
    APIã€‚è¿™ä¸ªç±»ä¸­æœ€é‡è¦çš„æ–¹æ³•æ˜¯from_pretrainedã€‚
- en: which will download and cache the configuration and the vocabulary associated
    to a given checkpointã€‚Hereï¼Œ the checkpoint used by default for the sentiment analysis
    pipeline is distill belt basin case Fiin tuned SSs2 Englishã€‚ which is a bit of
    a mouthfulã€‚We instant to token associated with a checkpoint and feed it to the
    two sentencesã€‚Since the two sentences are not of the same sizeï¼Œ well need to pad
    the shest one to be able to build an arrayã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ä¸‹è½½å¹¶ç¼“å­˜ä¸ç»™å®šæ£€æŸ¥ç‚¹ç›¸å…³çš„é…ç½®å’Œè¯æ±‡ã€‚åœ¨è¿™é‡Œï¼Œæƒ…æ„Ÿåˆ†æpipelineé»˜è®¤ä½¿ç”¨çš„æ£€æŸ¥ç‚¹æ˜¯distilbert-base-uncased-finetuned-sst-2-englishï¼Œå¬èµ·æ¥æœ‰ç‚¹å¤æ‚ã€‚æˆ‘ä»¬å®ä¾‹åŒ–ä¸æ£€æŸ¥ç‚¹ç›¸å…³çš„tokenï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ä¸¤ä¸ªå¥å­ä¸­ã€‚ç”±äºè¿™ä¸¤ä¸ªå¥å­å¤§å°ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¾ƒå°çš„å¥å­è¿›è¡Œå¡«å……ï¼Œä»¥ä¾¿æ„å»ºæ•°ç»„ã€‚
- en: This is done by the tokenizer with the option padding equalã€‚With trucation equal2ã€‚
    we ensure that any sentence longer than the maximum the model can handle is truncatedã€‚Lastlyã€‚
    the return tensil option tells the tokenizer to return the byythch tensilã€‚Looking
    as a resultã€‚ we see we have a dictionary with two keysï¼Œ input ID contains the
    ideas of both sentences with zero where the padding is appliedã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é€šè¿‡åˆ†è¯å™¨ä»¥paddingé€‰é¡¹å®Œæˆçš„ã€‚ä½¿ç”¨truncation=2ï¼Œæˆ‘ä»¬ç¡®ä¿ä»»ä½•è¶…è¿‡æ¨¡å‹æœ€å¤§å¤„ç†èƒ½åŠ›çš„å¥å­éƒ½ä¼šè¢«æˆªæ–­ã€‚æœ€åï¼Œreturn_tensorsé€‰é¡¹å‘Šè¯‰åˆ†è¯å™¨è¿”å›æ‰¹æ¬¡å¼ é‡ã€‚ç»“æœï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­ä¸¤ä¸ªé”®input_idsåŒ…å«äº†ä¸¤ä¸ªå¥å­çš„IDï¼Œé›¶åˆ™æ˜¯å¡«å……çš„ä½ç½®ã€‚
- en: The second key attention mask indicates where petting has been appliedã€‚ so the
    model does not pay attention to itã€‚This is all what is inside the took stepã€‚Now
    let's have a look at the second stepã€‚ä¸‰ã»ã©ã€‚As sponsor to an eitherã€‚ for is a notomodal
    API with from pretrain methodï¼Œ it would download lu and cache the configuration
    of the modelã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå…³é”®æ³¨æ„åŠ›æ©ç æŒ‡ç¤ºå·²ç»åº”ç”¨äº†å“ªç§å¤„ç†ï¼Œä»¥ä¾¿æ¨¡å‹ä¸å¯¹å…¶è¿›è¡Œå…³æ³¨ã€‚è¿™å°±æ˜¯tookæ­¥éª¤ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹ç¬¬äºŒæ­¥ã€‚ä¸‰ã»ã©ã€‚ä½œä¸ºä¸€ä¸ªæä¾›é¢„è®­ç»ƒæ–¹æ³•çš„notomodal
    APIï¼Œå®ƒå°†ä¸‹è½½luå¹¶ç¼“å­˜æ¨¡å‹çš„é…ç½®ã€‚
- en: as well as the pertrain weightã€‚Howeverï¼Œ the Automod API will only instantiate
    the body of the modelã€‚ that is the part of the model that is left once the pro
    traininging head is removedã€‚It will output a high dimensional tensor that is a
    representation of the sentences pastã€‚ but which is not directly useful for our
    classification productã€‚Here the tensor has two sentencesã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠpertrainæƒé‡ã€‚ç„¶è€Œï¼ŒAutomod APIåªä¼šå®ä¾‹åŒ–æ¨¡å‹çš„ä¸»ä½“ã€‚å³åœ¨å»é™¤pro trainingingå¤´åå‰©ä¸‹çš„æ¨¡å‹éƒ¨åˆ†ã€‚å®ƒå°†è¾“å‡ºä¸€ä¸ªé«˜ç»´å¼ é‡ï¼Œè¡¨ç¤ºå¥å­çš„è¿‡å»ï¼Œä½†å¯¹äºæˆ‘ä»¬çš„åˆ†ç±»äº§å“å¹¶æ²¡æœ‰ç›´æ¥ç”¨å¤„ã€‚è¿™é‡Œçš„å¼ é‡æœ‰ä¸¤ä¸ªå¥å­ã€‚
- en: each of 16 tokensï¼Œ and the last dimension is the Indian size of our model 768ã€‚To
    get an output link to our classification problemã€‚ we need to use the Automodal
    for sequence classificationification classã€‚It works exactly as you to model classï¼Œ
    except with12 build a model with a classification headã€‚ğŸ˜Šã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª16ä¸ªæ ‡è®°ï¼Œæœ€åä¸€ä¸ªç»´åº¦æ˜¯æˆ‘ä»¬æ¨¡å‹çš„å°åº¦å¤§å°768ã€‚ä¸ºäº†è·å¾—ä¸æˆ‘ä»¬çš„åˆ†ç±»é—®é¢˜ç›¸å…³çš„è¾“å‡ºé“¾æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨Automodalè¿›è¡Œåºåˆ—åˆ†ç±»ç±»ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ä¸æ‚¨ä½¿ç”¨çš„æ¨¡å‹ç±»å®Œå…¨ç›¸åŒï¼Œåªæ˜¯æ„å»ºäº†ä¸€ä¸ªå¸¦åˆ†ç±»å¤´çš„æ¨¡å‹ã€‚ğŸ˜Šã€‚
- en: Praise one auto class for each common NLP task in the transformformers libraryã€‚Hereã€‚
    after giving all models of two sentencesã€‚We get a ten of size 2 by2ã€‚1 result for
    each sentence and for each possible levelã€‚Those outputs are not probabilities
    yetã€‚ we can see they don't sum to oneã€‚This is because each model of the transformformer's
    library returns look itã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºtransformersåº“ä¸­æ¯ä¸ªå¸¸è§çš„NLPä»»åŠ¡ï¼Œèµæ‰¬ä¸€ä¸ªè‡ªåŠ¨åˆ†ç±»ã€‚åœ¨è¿™é‡Œï¼Œç»™å‡ºä¸¤ä¸ªå¥å­çš„æ‰€æœ‰æ¨¡å‹åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª2x2çš„ç»“æœï¼Œæ¯ä¸ªå¥å­å’Œæ¯ä¸ªå¯èƒ½çš„çº§åˆ«ã€‚è¿™äº›è¾“å‡ºè¿˜ä¸æ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬çš„æ€»å’Œä¸ä¸º1ã€‚è¿™æ˜¯å› ä¸ºtransformersåº“ä¸­çš„æ¯ä¸ªæ¨¡å‹è¿”å›çš„æ˜¯look
    itã€‚
- en: To make sense of look itï¼Œ we need to dig into the third and last step of the
    pipelineã€‚Plus processingã€‚To conduct LoAT into probabilitiesï¼Œ we need to apply
    a softmax layers to themã€‚As we can seeï¼Œ this transforms them into positive number
    that's a to1ã€‚So last step is to know which of those corresponds to the positive
    of the negative labelã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç†è§£look itï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥ç®¡é“çš„ç¬¬ä¸‰ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªæ­¥éª¤ã€‚åŠ ä¸Šå¤„ç†ã€‚è¦å°†LoATè½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬åº”ç”¨softmaxå±‚ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¿™å°†å®ƒä»¬è½¬åŒ–ä¸ºä¸€ä¸ªæ­£æ•°ï¼Œæ€»å’Œä¸º1ã€‚æ‰€ä»¥æœ€åä¸€æ­¥æ˜¯çŸ¥é“è¿™äº›å¯¹åº”äºæ­£æ ‡ç­¾è¿˜æ˜¯è´Ÿæ ‡ç­¾ã€‚
- en: This is given by the IT2lipal field of the model conflictgã€‚The first proba is
    index0ã€‚ correspond to the negative level and the seconds index1 correspond to
    the positive levelã€‚This is how our classifier built with the pipeline function
    peaked with labels and compute those scoresã€‚ğŸ˜Šï¼ŒNow that you know how each step
    worksï¼Œ you can easily tweak them to your needsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”±æ¨¡å‹conflictgçš„IT2lipalå­—æ®µç»™å‡ºçš„ã€‚ç¬¬ä¸€ä¸ªæ¦‚ç‡æ˜¯index0ï¼Œå¯¹åº”è´Ÿæ ‡ç­¾ï¼Œç¬¬äºŒä¸ªindex1å¯¹åº”æ­£æ ‡ç­¾ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ„å»ºçš„åˆ†ç±»å™¨å¦‚ä½•ä½¿ç”¨ç®¡é“å‡½æ•°ä¸æ ‡ç­¾è¿›è¡Œäº¤äº’å¹¶è®¡ç®—è¿™äº›åˆ†æ•°ã€‚ğŸ˜Šï¼Œç°åœ¨æ‚¨çŸ¥é“æ¯ä¸ªæ­¥éª¤æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_3.png)'
- en: ã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_5.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_5.png)'
- en: Soã€‚Let's seeã€‚All rightï¼Œ so do we have any questions at this stage about the
    pipelineï¼Ÿ
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚å¥½çš„ï¼Œæ‰€ä»¥åœ¨è¿™ä¸ªé˜¶æ®µæˆ‘ä»¬å¯¹ç®¡é“è¿˜æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
- en: So one of the things that we saw is that there's these kind of three componentsã€‚
    there's like a pre processingcess stageã€‚Okayï¼Œ greatã€‚Okay great so one of the first
    questions we have is could you please explain the intuition behind the BRT SST2
    English checkpoint and what are the different flavors of checkpoints to be used
    and how did we choose SST2 Okay great so basically each of the transformer modelsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°çš„ä¸€ä»¶äº‹æ˜¯æœ‰è¿™ä¸‰ç§æˆåˆ†ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªé¢„å¤„ç†é˜¶æ®µã€‚å¥½çš„ï¼Œå¤ªå¥½äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯èƒ½å¦è¯·æ‚¨è§£é‡Šä¸€ä¸‹BRT SST2è‹±è¯­æ£€æŸ¥ç‚¹èƒŒåçš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Œä»¥åŠæœ‰å“ªäº›ä¸åŒç±»å‹çš„æ£€æŸ¥ç‚¹å¯ä»¥ä½¿ç”¨ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬é€‰æ‹©SST2ã€‚å¥½çš„ï¼Œæ€»çš„æ¥è¯´ï¼Œæ¯ä¸ªtransformeræ¨¡å‹ã€‚
- en: they have sort of pretrained base or pretrained backbone which I think you saw
    in the first chapterã€‚And then what we do typically with models like BERT and GP
    is we fine tune them on a downstream taskã€‚So the idea is that you takeï¼Œ for exampleï¼Œ
    BEert which was pre-trained on Wikipedia and the book corpus and then you sayã€‚
    okayï¼Œ I want to do classification now so I'm going to basically take these weights
    that I had in my original model and I'm going to add a classification head which
    is going to just basically be a linear layer that allows us to do the classification
    task and then we do the fine tuning step on a particular taskã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æœ‰é¢„è®­ç»ƒçš„åŸºç¡€æˆ–é¢„è®­ç»ƒçš„éª¨å¹²ï¼Œæˆ‘æƒ³ä½ åœ¨ç¬¬ä¸€ç« çœ‹åˆ°äº†ã€‚ç„¶åæˆ‘ä»¬é€šå¸¸ä¼šå¯¹åƒBERTå’ŒGPè¿™æ ·çš„æ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒã€‚æ‰€ä»¥åŸºæœ¬æ€è·¯æ˜¯ï¼Œä½ æ‹¿ä¾‹å¦‚åœ¨ç»´åŸºç™¾ç§‘å’Œä¹¦ç±è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„BERTï¼Œç„¶åè¯´ã€‚å¥½çš„ï¼Œæˆ‘ç°åœ¨æƒ³è¿›è¡Œåˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘å°†åŸºæœ¬ä¸Šä½¿ç”¨åŸå§‹æ¨¡å‹ä¸­çš„è¿™äº›æƒé‡ï¼Œç„¶åæ·»åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå¯ä»¥è®©æˆ‘ä»¬è¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œç„¶åæˆ‘ä»¬åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_7.png)'
- en: So if you want to understand a little bit about how theã€‚![](img/40873acb06abf924ac4a43fae802679a_9.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ æƒ³äº†è§£ä¸€ä¸‹å¦‚ä½•è¿›è¡Œã€‚![](img/40873acb06abf924ac4a43fae802679a_9.png)
- en: The models work or the description of the modelsã€‚ If we look at Bert and what
    was this guy calledã€‚ this is Btã€‚ä¸‰å…«å…«å…«ã€‚Uncasedï¼Œ fine tunedã€‚ So uncasedã€‚Un tuneedã€‚å—¯ã€‚And
    then it's Sã€‚ what was itã€‚Fine tuned are distber okayã€‚The stillberã€‚Uncasedï¼Œ fine
    tunedã€‚What are I missing hereï¼ŸğŸ˜”ã€‚Distill B face and case function indicateã€‚ So
    if we look at thisã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å·¥ä½œåŸç†æˆ–æ¨¡å‹çš„æè¿°ã€‚å¦‚æœæˆ‘ä»¬çœ‹çœ‹Bertï¼Œè¿™å®¶ä¼™å«ä»€ä¹ˆæ¥ç€ã€‚è¿™æ˜¯Btã€‚ä¸‰å…«å…«å…«ã€‚æ— å¤§å°å†™ï¼Œå¾®è°ƒã€‚æ— å¤§å°å†™ã€‚æœªå¾®è°ƒã€‚å—¯ã€‚é‚£ä¹ˆå®ƒæ˜¯Sã€‚é‚£æ˜¯ä»€ä¹ˆã€‚å¾®è°ƒçš„distberï¼Œå¥½å§ã€‚ä»ç„¶æ˜¯æ— å¤§å°å†™ï¼Œå¾®è°ƒã€‚æˆ‘è¿™é‡Œæ¼æ‰äº†ä»€ä¹ˆï¼ŸğŸ˜”ã€‚Distill
    Bé¢å’Œå¤§å°å†™åŠŸèƒ½æŒ‡ç¤ºã€‚å¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªã€‚
- en: Then what we can see is that this is a checkpoint that was fine tuned on a particular
    taskã€‚ so this task here is called the T bank taskã€‚![](img/40873acb06abf924ac4a43fae802679a_11.png)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒçš„æ£€æŸ¥ç‚¹ã€‚æ‰€ä»¥è¿™ä¸ªä»»åŠ¡å«åšT bankä»»åŠ¡ã€‚![](img/40873acb06abf924ac4a43fae802679a_11.png)
- en: '![](img/40873acb06abf924ac4a43fae802679a_12.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_12.png)'
- en: The binary classification benchmarkï¼Œ and I think from memory this is just like
    a sentiment analysis task which just has label or data given in terms of just
    two labels like you know positive or negativeã€‚So the basic idea of like why did
    we choose this or the basic answer is that we were just trying to sort of demonstrate
    how the pipeline works for sentiment analysis and this is one model which is well
    suited for that taskã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒå…ƒåˆ†ç±»åŸºå‡†ï¼Œæˆ‘è®°å¾—è¿™å°±åƒä¸€ä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼Œåªæœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼Œå¦‚æ­£é¢æˆ–è´Ÿé¢ã€‚æˆ‘ä»¬é€‰æ‹©è¿™ä¸ªçš„åŸºæœ¬åŸå› æ˜¯ï¼Œæˆ‘ä»¬åªæ˜¯æƒ³å±•ç¤ºæƒ…æ„Ÿåˆ†æçš„ç®¡é“å¦‚ä½•å·¥ä½œï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆè¯¥ä»»åŠ¡çš„æ¨¡å‹ã€‚
- en: So I hope that answers your questionï¼Œ DK creativeã€‚And then the other question
    we have is in this caseã€‚ we assume that there are only two classes for classificationã€‚
    how do we specify a multiclass problem and what checkpoint would use Okayï¼Œ greatã€‚
    that's a very good questionã€‚So maybe what we can do is let's have a look at the
    coabab for this chapterã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¸Œæœ›è¿™èƒ½å›ç­”ä½ çš„é—®é¢˜ï¼ŒDK creativeã€‚ç„¶åæˆ‘ä»¬è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ã€‚æˆ‘ä»¬å‡è®¾åªæœ‰ä¸¤ä¸ªç±»åˆ«ç”¨äºåˆ†ç±»ã€‚æˆ‘ä»¬å¦‚ä½•æŒ‡å®šå¤šç±»åˆ«é—®é¢˜ï¼Œä»¥åŠä¼šä½¿ç”¨ä»€ä¹ˆæ£€æŸ¥ç‚¹ï¼Ÿå¥½çš„ï¼Œå¾ˆå¥½ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ã€‚æ‰€ä»¥ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥çœ‹çœ‹è¿™ä¸€ç« çš„å†…å®¹ã€‚
- en: So here we've got a sentiment analysis pipelineã€‚And of courseã€‚ it's just going
    to predict two classesã€‚ And so now I'm just going to instantiate the tokenizerã€‚And
    here's the modelã€‚Okayï¼Œ so if we look at a modelã€‚ğŸ˜Šï¼ŒEvery single model has a configã€‚And
    this config tells you thingsï¼Œ for exampleï¼Œ like the number of classesã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæƒ…æ„Ÿåˆ†æç®¡é“ã€‚æ˜¾ç„¶ï¼Œå®ƒåªä¼šé¢„æµ‹ä¸¤ä¸ªç±»åˆ«ã€‚ç°åœ¨æˆ‘å°†å®ä¾‹åŒ–åˆ†è¯å™¨ã€‚ä¸‹é¢æ˜¯æ¨¡å‹ã€‚å¥½çš„ï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹æ¨¡å‹ã€‚ğŸ˜Šï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé…ç½®ã€‚è¿™ä¸ªé…ç½®å‘Šè¯‰ä½ ä¸€äº›ä¿¡æ¯ï¼Œä¾‹å¦‚ç±»åˆ«æ•°é‡ã€‚
- en: so you can see we've got two classes hereã€‚And what you can do when you instantiate
    a modelã€‚ you can define the number of classes you would like when you instantiate
    the thing for text classificationã€‚ so just to give you an exampleã€‚Let's suppose
    that I take a checkpointã€‚For multi classã€‚Now I'm going to do two things hereï¼Œ
    I'm going to show you firstã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰ä¸¤ä¸ªç±»åˆ«ã€‚å½“ä½ å®ä¾‹åŒ–æ¨¡å‹æ—¶ï¼Œå¯ä»¥å®šä¹‰ä½ å¸Œæœ›çš„ç±»åˆ«æ•°é‡ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚ä¸ºäº†ç»™ä½ ä¸€ä¸ªä¾‹å­ã€‚å‡è®¾æˆ‘ä½¿ç”¨ä¸€ä¸ªå¤šç±»åˆ«çš„æ£€æŸ¥ç‚¹ã€‚ç°åœ¨æˆ‘è¦åšä¸¤ä»¶äº‹ï¼Œæˆ‘å°†é¦–å…ˆå±•ç¤ºç»™ä½ ã€‚
- en: how do we insha a model that we then would fine sharing ourselvesã€‚ and then
    I'll show you the sort of simpler case where we have an existing pretrain modelã€‚So
    if I don't haveï¼Œ imagine I just have my own data set and there's no model in the
    hub that is suitable for what I want to doã€‚ what I might do is I'll sayï¼Œ okayï¼Œ
    I'm going to take distillbert base uncasedã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•è®©æ¨¡å‹ä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿè‡ªå·±å¾®è°ƒã€‚ç„¶åæˆ‘ä¼šå±•ç¤ºä¸€ä¸ªæ›´ç®€å•çš„æ¡ˆä¾‹ï¼Œå³æˆ‘ä»¬æœ‰ä¸€ä¸ªç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœæˆ‘æ²¡æœ‰ï¼Œå‡è®¾æˆ‘åªæœ‰è‡ªå·±çš„æ•°æ®é›†ï¼Œè€Œåœ¨ä¸­å¿ƒæ²¡æœ‰é€‚åˆæˆ‘æƒ³åšçš„æ¨¡å‹ã€‚é‚£æˆ‘å¯èƒ½ä¼šè¯´ï¼Œå¥½å§ï¼Œæˆ‘è¦ä½¿ç”¨distillbert
    base uncasedã€‚
- en: And this is just the pretrain modelï¼Œ there's nothing sort of special about itã€‚
    I still have to do some workã€‚And then what I could do is I can sayï¼Œ okayï¼Œ I'm
    going to takeã€‚First thing to take from transformersã€‚I'm going to import an auto
    modelã€‚ but now I'm going to do it for sequence classificationã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ã€‚æˆ‘ä»ç„¶éœ€è¦åšä¸€äº›å·¥ä½œã€‚ç„¶åæˆ‘å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œæˆ‘è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä»transformerså¯¼å…¥ä¸€ä¸ªè‡ªåŠ¨æ¨¡å‹ï¼Œä½†ç°åœ¨æˆ‘è¦ä¸ºåºåˆ—åˆ†ç±»æ¥åšã€‚
- en: so this is where anytime you're dealing with like text classification or you
    know multiclass multilabel these thingsã€‚ this is a sequence classification taskã€‚And
    then what I'm going to doã€‚ I'm going to take my model for sequence classificationã€‚And
    then I can do my usual from pretrainedã€‚I take myã€‚Checkpoint that I've got nowã€‚
    All this new oneã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨å¤„ç†æ–‡æœ¬åˆ†ç±»æˆ–å¤šç±»å¤šæ ‡ç­¾æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šä½¿ç”¨æˆ‘çš„åºåˆ—åˆ†ç±»æ¨¡å‹ã€‚ç„¶åæˆ‘å¯ä»¥ä»é¢„è®­ç»ƒå¼€å§‹ã€‚æˆ‘æ‹¿åˆ°æˆ‘çš„æ£€æŸ¥ç‚¹ï¼Œç°åœ¨æ˜¯è¿™ä¸ªæ–°çš„æ¨¡å‹ã€‚
- en: And then what I can do I can pass keyword arguments that will specify how many
    labels I'm dealing with so imagine that my data set has six classes that I'm dealing
    withã€‚ so what I can do is I can say the number of labels is sixã€‚And now what will
    happenã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¯ä»¥ä¼ é€’å…³é”®å­—å‚æ•°ï¼Œè¿™å°†æŒ‡å®šæˆ‘æ­£åœ¨å¤„ç†å¤šå°‘ä¸ªæ ‡ç­¾ï¼Œæ‰€ä»¥æƒ³è±¡ä¸€ä¸‹æˆ‘çš„æ•°æ®é›†æœ‰å…­ä¸ªç±»åˆ«ã€‚æˆ‘å¯ä»¥è¯´æ ‡ç­¾çš„æ•°é‡æ˜¯å…­ã€‚é‚£ä¹ˆæ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿ
- en: It will download the base model or the pretrain model for distillvertã€‚And it
    will then add a classification head on top of this model and it will configure
    it with the right number of classes so that you know we can do fine tuning appropriately
    so now if we look at our config you can see that it's already initialized the
    model with six different classesã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šä¸‹è½½distillvertçš„åŸºç¡€æ¨¡å‹æˆ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨è¿™ä¸ªæ¨¡å‹ä¸Šæ·»åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼Œå¹¶ç”¨æ­£ç¡®çš„ç±»åˆ«æ•°é‡è¿›è¡Œé…ç½®ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿé€‚å½“åœ°è¿›è¡Œå¾®è°ƒã€‚æ‰€ä»¥ç°åœ¨å¦‚æœæˆ‘ä»¬æŸ¥çœ‹é…ç½®ï¼Œä½ ä¼šå‘ç°å®ƒå·²ç»ç”¨å…­ä¸ªä¸åŒçš„ç±»åˆ«åˆå§‹åŒ–äº†æ¨¡å‹ã€‚
- en: And we don't know the labels yet because we haven't provided our own data set
    and our own labeling conventionã€‚ but we could do thatã€‚And then from here we could
    then just fine tune and train the model exactly as we've doneã€‚ or we will do in
    the next chapterï¼Œ so that's one way of doing itã€‚Nowã€‚ the other part of the question
    isï¼Œ how do I take a sort of pretrain model or fine tune model from the hubï¼Ÿ
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜ä¸çŸ¥é“æ ‡ç­¾ï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰æä¾›è‡ªå·±çš„æ•°æ®é›†å’Œæ ‡ç­¾çº¦å®šã€‚ä½†æˆ‘ä»¬å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ç„¶åä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥å¾®è°ƒå’Œè®­ç»ƒæ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬å·²ç»åšçš„é‚£æ ·ï¼Œæˆ–è€…åœ¨ä¸‹ä¸€ç« ä¸­æˆ‘ä»¬å°†è¦åšçš„é‚£æ ·ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ç§æ–¹æ³•ã€‚ç°åœ¨ï¼Œé—®é¢˜çš„å¦ä¸€éƒ¨åˆ†æ˜¯ï¼Œæˆ‘å¦‚ä½•ä»ä¸­å¿ƒè·å–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æˆ–å¾®è°ƒæ¨¡å‹ï¼Ÿ
- en: And this is a little trickier to figure out likeï¼Œ you know which model is suitable
    for your taskã€‚ So the way I usually do it is I lookï¼Œ for exampleï¼Œ at text classificationã€‚
    so I do a filter here on text classificationã€‚And then I sort of like ask myselfï¼Œ
    okayã€‚ maybe I'm dealing with let's seeã€‚Nowï¼Œ this isn't so easy to find a multi
    class exampleï¼Œ soã€‚I thinkã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æœ‰ç‚¹éš¾ä»¥å¼„æ¸…æ¥šï¼Œå“ªä¸ªæ¨¡å‹é€‚åˆä½ çš„ä»»åŠ¡ã€‚æ‰€ä»¥æˆ‘é€šå¸¸çš„æ–¹æ³•æ˜¯ï¼Œä¸¾ä¸ªä¾‹å­ï¼ŒæŸ¥çœ‹æ–‡æœ¬åˆ†ç±»ã€‚æ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå¯¹æ–‡æœ¬åˆ†ç±»è¿›è¡Œè¿‡æ»¤ã€‚ç„¶åæˆ‘é—®è‡ªå·±ï¼Œå¥½å§ï¼Œä¹Ÿè®¸æˆ‘æ­£åœ¨å¤„ç†ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ã€‚ç°åœ¨ï¼Œè¿™ä¸æ˜¯é‚£ä¹ˆå®¹æ˜“æ‰¾åˆ°ä¸€ä¸ªå¤šç±»çš„ä¾‹å­ï¼Œæ‰€ä»¥ã€‚æˆ‘æƒ³ã€‚
- en: in generalã€‚Yeahï¼Œ soã€‚Actuallyï¼Œ finding the multiclass model that is suitable
    for your task takes a bit of workã€‚ I meanï¼Œ maybe maybe Omar already knows a fast
    way to get thisã€‚ but generally speaking all of the models that we have here are
    in some senseï¼Œ fine tuned on a taskã€‚ So for exampleï¼Œ like this German sentiment
    Btã€‚presumably is two classes and one way you could quickly check that is by looking
    at the files and versions and seeing in the configuration how many labels you
    have so in this case there's three labelsã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥ã€‚å®é™…ä¸Šï¼Œæ‰¾åˆ°é€‚åˆä½ ä»»åŠ¡çš„å¤šç±»æ¨¡å‹éœ€è¦ä¸€äº›å·¥ä½œã€‚æˆ‘æ˜¯è¯´ï¼Œä¹Ÿè®¸å¥¥é©¬å°”å·²ç»çŸ¥é“ä¸€ç§å¿«é€Ÿè·å–çš„æ–¹æ³•ï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬è¿™é‡Œæ‰€æœ‰çš„æ¨¡å‹åœ¨æŸç§æ„ä¹‰ä¸Šéƒ½æ˜¯é’ˆå¯¹ä¸€ä¸ªä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒçš„ã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ªå¾·å›½æƒ…æ„ŸBtï¼Œå¯èƒ½æœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œä¸€ç§å¿«é€Ÿæ£€æŸ¥çš„æ–¹æ³•æ˜¯æŸ¥çœ‹æ–‡ä»¶å’Œç‰ˆæœ¬ï¼Œçœ‹çœ‹é…ç½®ä¸­æœ‰å¤šå°‘ä¸ªæ ‡ç­¾ï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­æœ‰ä¸‰ä¸ªæ ‡ç­¾ã€‚
- en: But actually searching for this effectively on the Hã€‚ I'm not sure maybe there
    is a way of doing thisã€‚Or maybe this is a good feature we should add in the hubã€‚I
    am Holmesï¼Œ I hope that sort of partially answers your questionã€‚But if notã€‚ then
    feel free to write in the chatã€‚Yeahï¼Œ exactly we should add a feature that is great
    basically I think what we would like is a filter where we could filter between
    binary classification multiclass and multilaval and then that would allow us to
    refine things good question awesomeã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®é™…ä¸Šåœ¨ H ä¸Šæœ‰æ•ˆåœ°æœç´¢è¿™ä¸ªï¼Œæˆ‘ä¸ç¡®å®šï¼Œä¹Ÿè®¸æœ‰åŠæ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ–è€…ä¹Ÿè®¸è¿™æ˜¯æˆ‘ä»¬åº”è¯¥åœ¨ä¸­å¿ƒæ·»åŠ çš„ä¸€ä¸ªå¥½åŠŸèƒ½ã€‚æˆ‘æ˜¯éœå§†æ–¯ï¼Œæˆ‘å¸Œæœ›è¿™éƒ¨åˆ†å›ç­”äº†ä½ çš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£å°±è¯·éšæ—¶åœ¨èŠå¤©ä¸­å†™ä¸‹ä½ çš„é—®é¢˜ã€‚æ˜¯çš„ï¼Œæ²¡é”™ï¼Œæˆ‘ä»¬åº”è¯¥æ·»åŠ ä¸€ä¸ªåŠŸèƒ½ï¼ŒåŸºæœ¬ä¸Šæˆ‘è®¤ä¸ºæˆ‘ä»¬æƒ³è¦çš„æ˜¯ä¸€ä¸ªè¿‡æ»¤å™¨ï¼Œå¯ä»¥åœ¨äºŒå…ƒåˆ†ç±»ã€å¤šç±»å’Œå¤šçº§ä¹‹é—´è¿›è¡Œè¿‡æ»¤ï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½æ›´å¥½åœ°ç»†åŒ–å†…å®¹ï¼Œé—®é¢˜å¾ˆå¥½ï¼Œå¤ªæ£’äº†ã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ so are there any more questions about the pipelines before we look a
    bit more at the codeï¼Ÿ
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Š å¥½çš„ï¼Œåœ¨æˆ‘ä»¬æ›´æ·±å…¥åœ°çœ‹ä¸€ä¸‹ä»£ç ä¹‹å‰ï¼Œè¿˜æœ‰å…³äºç®¡é“çš„å…¶ä»–é—®é¢˜å—ï¼Ÿ
- en: Okayï¼Œ so in that caseï¼Œ let'sï¼Œ let's have a sort of walk through this this coab
    with the pipeline to sort of get a deeper understanding of what's going onã€‚ So
    we've got thisã€‚è¯¶ã€‚Example here where we're basically downloading the sentiment
    analysis pipelineã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬é€šè¿‡è¿™ä¸ªç®¡é“çš„ç¤ºä¾‹æ¥æ›´æ·±å…¥åœ°ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰äº†è¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæ­£åœ¨ä¸‹è½½æƒ…æ„Ÿåˆ†æç®¡é“ã€‚
- en: And we've got now the classifierï¼Œ which we can feed these two texts that you
    saw in one of the earlier chaptersã€‚But now what we want to do is we want to understand
    what really is happening under the wood so remember that the first thing we need
    to do is we need to process or pre process these raw texts becauseã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†åˆ†ç±»å™¨ï¼Œå¯ä»¥å°†ä½ åœ¨å‰é¢çš„ç« èŠ‚ä¸­çœ‹åˆ°çš„è¿™ä¸¤æ®µæ–‡æœ¬è¾“å…¥è¿›å»ã€‚ä½†ç°åœ¨æˆ‘ä»¬æƒ³åšçš„æ˜¯ç†è§£åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæ‰€ä»¥è¯·è®°ä½ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†æˆ–é¢„å¤„ç†è¿™äº›åŸå§‹æ–‡æœ¬ï¼Œå› ä¸ºã€‚
- en: Basically all neural networks can't do operations on raw textex it's kind of
    like imagine you want to do like matrix multiã€‚ how do you do that on like a stringï¼ŸAnd
    so what we can do insteadã€‚Is we use a tokenizerï¼Ÿ
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šæ‰€æœ‰ç¥ç»ç½‘ç»œéƒ½æ— æ³•å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œæ“ä½œï¼Œæƒ³è±¡ä¸€ä¸‹ï¼Œä½ æƒ³è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚ä½ å¦‚ä½•åœ¨å­—ç¬¦ä¸²ä¸Šåšåˆ°è¿™ä¸€ç‚¹ï¼Ÿæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç”¨åˆ†è¯å™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: And one of the key things that you should remember is that if you're doing any
    sort of fine tuning or any like sort of inference or predictionsã€‚ it's really
    important that the checkpoint you use here is the same for the tokenizer and the
    model and that's because when these transformers are pre-trained on a large corpus
    there's a corresponding tokenizer that was also find you're trained in some sense
    to learn the vocabulary of that corpus and so if you sort of mix and match a checkpoint
    for one tokenizer and then a different checkpoint for the modelã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥è®°ä½çš„ä¸€ä¸ªå…³é”®ç‚¹æ˜¯ï¼Œå¦‚æœä½ æ­£åœ¨è¿›è¡Œä»»ä½•å½¢å¼çš„å¾®è°ƒï¼Œæˆ–è€…ä»»ä½•æ¨æ–­æˆ–é¢„æµ‹ï¼Œç¡®ä¿ä½ åœ¨è¿™é‡Œä½¿ç”¨çš„æ£€æŸ¥ç‚¹ä¸åˆ†è¯å™¨å’Œæ¨¡å‹æ˜¯ç›¸åŒçš„ï¼Œè¿™å¾ˆé‡è¦ã€‚å› ä¸ºå½“è¿™äº›å˜æ¢å™¨åœ¨ä¸€ä¸ªå¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œä¼šæœ‰ä¸€ä¸ªç›¸åº”çš„åˆ†è¯å™¨ï¼Œä¹Ÿåœ¨æŸç§æ„ä¹‰ä¸Šè¿›è¡Œå¾®è°ƒä»¥å­¦ä¹ è¯¥è¯­æ–™åº“çš„è¯æ±‡ï¼Œæ‰€ä»¥å¦‚æœä½ æ··åˆä½¿ç”¨ä¸€ä¸ªåˆ†è¯å™¨çš„æ£€æŸ¥ç‚¹å’Œæ¨¡å‹çš„ä¸åŒæ£€æŸ¥ç‚¹ã€‚
- en: basically you'll get a mismatch in the vocabulary and then you'll get kind of
    garbage in in your outputs so just that's one sort of thing to watch out forã€‚Okayï¼Œ
    so we've got a tokenizerã€‚ğŸ˜Šï¼ŒAnd now we've got these same raw inputsã€‚ and if we
    basically feed these two sentences into the tokenizerã€‚ you get generally there
    are two things that you just sort of need to rememberã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šä½ ä¼šåœ¨è¯æ±‡è¡¨ä¸­é‡åˆ°ä¸åŒ¹é…ï¼Œç„¶åä½ çš„è¾“å‡ºä¸­ä¼šå‡ºç°ä¸€äº›åƒåœ¾ä¿¡æ¯ï¼Œæ‰€ä»¥è¿™æ˜¯éœ€è¦æ³¨æ„çš„ä¸€ç‚¹ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ªåˆ†è¯å™¨ã€‚ğŸ˜Š ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†è¿™äº›ç›¸åŒçš„åŸå§‹è¾“å…¥ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™ä¸¤å¥è¯è¾“å…¥åˆ°åˆ†è¯å™¨ä¸­ï¼Œé€šå¸¸æœ‰ä¸¤ä»¶äº‹æƒ…ä½ éœ€è¦è®°ä½ã€‚
- en: You're going to get something called input Isã€‚And these input Is are basically
    a mapping of every single token in our sequence to a unique number or a unique
    integer to be preciseã€‚ and this is basically a mapping in the vocabulary so imagine
    that I was thinking about like I don't know the whole English language where I'm
    just dealing with words then I'm going to have probably several hundred thousand
    words or tokens in my vocabulary and then if I get like the word wholeã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†ä¼šå¾—åˆ°ä¸€ä¸ªå«åšè¾“å…¥ Is çš„ä¸œè¥¿ã€‚è¿™äº›è¾“å…¥ Is åŸºæœ¬ä¸Šæ˜¯å°†æˆ‘ä»¬åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªæ ‡è®°æ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—ï¼Œæˆ–è€…æ›´å‡†ç¡®åœ°è¯´æ˜¯ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯åœ¨è¯æ±‡è¡¨ä¸­çš„ä¸€ç§æ˜ å°„ï¼Œæ‰€ä»¥æƒ³è±¡ä¸€ä¸‹æˆ‘åœ¨æ€è€ƒæ•´ä¸ªè‹±è¯­è¯­è¨€æ—¶ï¼Œåªæ˜¯åœ¨å¤„ç†å•è¯ï¼Œé‚£ä¹ˆæˆ‘å¯èƒ½ä¼šåœ¨æˆ‘çš„è¯æ±‡ä¸­æœ‰å¥½å‡ åä¸‡ä¸ªå•è¯æˆ–æ ‡è®°ï¼Œç„¶åå¦‚æœæˆ‘å¾—åˆ°åƒâ€œwholeâ€è¿™æ ·çš„è¯ã€‚
- en: I would like to be able to match that to number that corresponds to this mapping
    in the vocabularyã€‚But as we sawï¼Œ I think in the first chapterï¼Œ in fact we might
    see it as well todayã€‚ this kind of like tokenization in terms of words is not
    very efficient and so what we usually do is something a bit clevererã€‚ but the
    basic idea is that every single token in this input is going to be mapped to a
    number and then those numbers allow us to sort of distinguish between different
    tokens in the sequenceã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›èƒ½å¤Ÿå°†å…¶ä¸è¯æ±‡è¡¨ä¸­å¯¹åº”äºæ­¤æ˜ å°„çš„æ•°å­—è¿›è¡ŒåŒ¹é…ã€‚ä½†æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­çœ‹åˆ°çš„ï¼Œå®é™…ä¸Šæˆ‘ä»¬ä»Šå¤©å¯èƒ½ä¹Ÿä¼šçœ‹åˆ°ï¼Œè¿™ç§æŒ‰è¯è¿›è¡Œçš„æ ‡è®°åŒ–å¹¶ä¸æ˜¯å¾ˆé«˜æ•ˆï¼Œå› æ­¤æˆ‘ä»¬é€šå¸¸ä¼šåšä¸€äº›æ›´èªæ˜çš„äº‹æƒ…ã€‚ä½†åŸºæœ¬çš„æƒ³æ³•æ˜¯ï¼Œè¾“å…¥ä¸­çš„æ¯ä¸€ä¸ªæ ‡è®°éƒ½å°†æ˜ å°„åˆ°ä¸€ä¸ªæ•°å­—ï¼Œç„¶åè¿™äº›æ•°å­—ä½¿æˆ‘ä»¬èƒ½å¤ŸåŒºåˆ†åºåˆ—ä¸­çš„ä¸åŒæ ‡è®°ã€‚
- en: So that's what input ID areã€‚And the other thing that you're going to see today
    in more detail is something called an attention maskã€‚ and I'll explain a bit more
    later on what this is really doingã€‚ but you can already see that it's kind of
    putting a bunch of ones at some part of the sequence and a bunch of zeros towards
    the end of the sequenceã€‚ and this will become clearer later onã€‚Okayï¼Œ so we've
    got the tokenizerã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯è¾“å…¥ ID çš„å«ä¹‰ã€‚ä»Šå¤©ä½ å°†æ›´è¯¦ç»†åœ°çœ‹åˆ°çš„å¦ä¸€ä»¶äº‹æ˜¯è¢«ç§°ä¸ºæ³¨æ„åŠ›æ©ç çš„ä¸œè¥¿ã€‚æˆ‘ç¨åä¼šæ›´è¯¦ç»†åœ°è§£é‡Šè¿™å®é™…ä¸Šæ˜¯åšä»€ä¹ˆçš„ã€‚ä½†ä½ å¯ä»¥å·²ç»çœ‹åˆ°ï¼Œå®ƒåœ¨åºåˆ—çš„æŸäº›éƒ¨åˆ†æ”¾ç½®äº†ä¸€å †ä¸€ï¼Œè€Œåœ¨åºåˆ—çš„æœ«å°¾æ”¾ç½®äº†ä¸€å †é›¶ã€‚è¿™ä¸€ç‚¹ç¨åä¼šå˜å¾—æ›´æ¸…æ™°ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æœ‰äº†åˆ†è¯å™¨ã€‚
- en: so we've now converted our raw text into these Isï¼Œ these numbers we can operate
    onã€‚And then let me just make sure I load the correct checkpoint hereã€‚So now we're
    going to load the modelï¼Œ so this is the thing that will process these inputsã€‚And
    let me just delete thisã€‚Okayï¼Œ and so then the question is how do you feed your
    inputs to your model so the simplest way is to just take this dictionary that
    we have hereã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ç°åœ¨å·²ç»å°†åŸå§‹æ–‡æœ¬è½¬æ¢æˆäº†è¿™äº›å¯ä»¥æ“ä½œçš„æ•°å­—ã€‚ç„¶åè®©æˆ‘ç¡®ä¿æˆ‘åœ¨è¿™é‡ŒåŠ è½½äº†æ­£ç¡®çš„æ£€æŸ¥ç‚¹ã€‚ç°åœ¨æˆ‘ä»¬è¦åŠ è½½æ¨¡å‹ï¼Œè¿™å°†å¤„ç†è¿™äº›è¾“å…¥ã€‚è®©æˆ‘å…ˆåˆ é™¤è¿™ä¸ªã€‚å¥½çš„ï¼Œé‚£ä¹ˆé—®é¢˜æ˜¯ï¼Œå¦‚ä½•å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ç›´æ¥ä½¿ç”¨æˆ‘ä»¬è¿™é‡Œçš„è¿™ä¸ªå­—å…¸ã€‚
- en: which has two keysï¼Œ it has input IDs and attention maskã€‚And then we can just
    use the standard Python unpacking operator to just feed all of the keys and values
    to the modelã€‚And when we do thisï¼Œ this will basically feed the inputs to the forward
    pass of the model to generate the outputsã€‚And so one way we could look at thatï¼Œ
    I think we can probably do thisã€‚ if we look at the forwardã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœ‰ä¸¤ä¸ªé”®ï¼Œåˆ†åˆ«æ˜¯è¾“å…¥ ID å’Œæ³¨æ„åŠ›æ©ç ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„ Python è§£åŒ…æ“ä½œç¬¦å°†æ‰€æœ‰çš„é”®å’Œå€¼ä¼ é€’ç»™æ¨¡å‹ã€‚å½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼ŒåŸºæœ¬ä¸Šä¼šå°†è¾“å…¥ä¼ é€’åˆ°æ¨¡å‹çš„å‰å‘ä¼ æ’­ä¸­ä»¥ç”Ÿæˆè¾“å‡ºã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥è¿™æ ·æ¥çœ‹è¿™ä¸ªé—®é¢˜ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹å‰å‘ä¼ æ’­ã€‚
- en: You can see here in the coab it's showing us basically what the arguments this
    forward pass can acceptã€‚ so it tells us we can accept input IDsï¼Œ we can have an
    attention mask and then there are like some more kind of sophisticated or advanced
    things we could also provide but you know we don't need to do them for today but
    just so you know there are other things that you can doã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ CoLab ä¸­çœ‹åˆ°ï¼Œå®ƒåŸºæœ¬ä¸Šå‘æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸ªå‰å‘ä¼ æ’­å¯ä»¥æ¥å—çš„å‚æ•°ã€‚å› æ­¤ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬å¯ä»¥æ¥å—è¾“å…¥ IDï¼Œå¯ä»¥æœ‰æ³¨æ„åŠ›æ©ç ï¼Œç„¶åè¿˜æœ‰ä¸€äº›æ›´å¤æ‚æˆ–é«˜çº§çš„ä¸œè¥¿æˆ‘ä»¬ä¹Ÿå¯ä»¥æä¾›ï¼Œä½†ä»Šå¤©æˆ‘ä»¬ä¸éœ€è¦å¤„ç†è¿™äº›ï¼Œä¸è¿‡è¯·ä½ çŸ¥é“ï¼Œè¿˜æœ‰å…¶ä»–ä½ å¯ä»¥åšçš„äº‹æƒ…ã€‚
- en: So you can see thatï¼Œ okayï¼Œ we need to provide at least these input Is in attention
    maskã€‚ğŸ˜Šã€‚And so when we do the unpacking like hereï¼Œ this will basically run through
    the forward pass and produce some outputsã€‚And as we saw in the videoã€‚These outputs
    are basically called like hidden states and these hidden states are just some
    sort of like like say compressed representation of the textã€‚ so we're taking this
    raw textï¼Œ we're converting it first into numbers and then we're taking those numbers
    and then we're converting those sort of integers into dense vectors so basically
    every token is now associated with a vectorã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬éœ€è¦æä¾›è‡³å°‘è¿™äº›è¾“å…¥ ID å’Œæ³¨æ„åŠ›æ©ç ã€‚ğŸ˜Šã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬åƒè¿™é‡Œé‚£æ ·è¿›è¡Œè§£åŒ…æ—¶ï¼Œè¿™åŸºæœ¬ä¸Šä¼šè¿è¡Œå‰å‘ä¼ æ’­å¹¶ç”Ÿæˆä¸€äº›è¾“å‡ºã€‚æ­£å¦‚æˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œè¿™äº›è¾“å‡ºåŸºæœ¬ä¸Šè¢«ç§°ä¸ºéšè—çŠ¶æ€ï¼Œè€Œè¿™äº›éšè—çŠ¶æ€åªæ˜¯æŸç§å‹ç¼©çš„æ–‡æœ¬è¡¨ç¤ºã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ï¼Œç„¶åå°†è¿™äº›æ•°å­—å†è½¬æ¢ä¸ºå¯†é›†å‘é‡ï¼Œå› æ­¤æ¯ä¸ªæ ‡è®°ç°åœ¨éƒ½ä¸ä¸€ä¸ªå‘é‡ç›¸å…³è”ã€‚
- en: And in this case we've got 16 vectors per sentenceï¼Œ and each vector has 768
    dimensionsã€‚ and that's just because of the way Bt was or distill Bert as well
    was pre trainededã€‚So let's have a look at one of these vectorsï¼Œ so we've got outputsã€‚So
    I'm going to take the first sentenceï¼Œ so that's the first indexã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ¯ä¸ªå¥å­æœ‰16ä¸ªå‘é‡ï¼Œæ¯ä¸ªå‘é‡æœ‰768ç»´ã€‚è¿™åªæ˜¯å› ä¸ºBtæˆ–Distill Bertçš„é¢„è®­ç»ƒæ–¹å¼ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™äº›å‘é‡ä¹‹ä¸€ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¾“å‡ºã€‚æ‰€ä»¥æˆ‘å°†å–ç¬¬ä¸€å¥è¯ï¼Œè¿™å°±æ˜¯ç¬¬ä¸€ä¸ªç´¢å¼•ã€‚
- en: and I'm going to look at the first token of this sentence and so if you look
    at thisã€‚They must be slices or integersã€‚Ca I need to do last hidden stateã€‚ Okayï¼Œ
    goodã€‚So actuallyã€‚ let's just take one step back if we just look at the raw outputsã€‚You
    can see that in transformersã€‚ all the outputs from the models are usually wrapped
    in an object which is kind of something we can then like you know index by attribute
    name and so here we've got something called the base model output and then this
    has in this case it's a single attribute called the last hidden state and the
    Tensorã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æŸ¥çœ‹è¿™ä¸ªå¥å­çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼Œæ‰€ä»¥å¦‚æœä½ çœ‹è¿™ä¸ªã€‚å®ƒä»¬å¿…é¡»æ˜¯åˆ‡ç‰‡æˆ–æ•´æ•°ã€‚æˆ‘éœ€è¦å¤„ç†æœ€åçš„éšè—çŠ¶æ€ã€‚å¥½çš„ï¼Œå¾ˆå¥½ã€‚å®é™…ä¸Šï¼Œè®©æˆ‘ä»¬é€€ä¸€æ­¥ï¼Œå¦‚æœæˆ‘ä»¬åªçœ‹åŸå§‹è¾“å‡ºã€‚ä½ å¯ä»¥çœ‹åˆ°åœ¨å˜å‹å™¨ä¸­ï¼Œæ¨¡å‹çš„æ‰€æœ‰è¾“å‡ºé€šå¸¸éƒ½æ˜¯åŒ…è£…åœ¨ä¸€ä¸ªå¯¹è±¡ä¸­çš„ï¼Œè¿™ä¸ªå¯¹è±¡æˆ‘ä»¬å¯ä»¥æ ¹æ®å±æ€§åç§°è¿›è¡Œç´¢å¼•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå«åšåŸºç¡€æ¨¡å‹è¾“å‡ºçš„ä¸œè¥¿ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒæœ‰ä¸€ä¸ªå±æ€§å«åšæœ€åçš„éšè—çŠ¶æ€å’Œå¼ é‡ã€‚
- en: So if I want to then access this last hidden stateã€‚Now I've got a tensorã€‚ which
    has the thing I wanted to doã€‚ So I'm going to get the first sentenceã€‚ I'm going
    to get the first vector or the first tokenã€‚ sorry the vector corresponding to
    the first tokenã€‚ And this is now thisï¼Œ you knowã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœæˆ‘æƒ³è®¿é—®è¿™ä¸ªæœ€åçš„éšè—çŠ¶æ€ã€‚ç°åœ¨æˆ‘å¾—åˆ°äº†ä¸€ä¸ªå¼ é‡ã€‚å®ƒåŒ…å«æˆ‘æƒ³è¦åšçš„ä¸œè¥¿ã€‚æ‰€ä»¥æˆ‘è¦è·å–ç¬¬ä¸€å¥è¯ã€‚æˆ‘å°†è·å–ç¬¬ä¸€ä¸ªå‘é‡æˆ–ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚æŠ±æ­‰ï¼Œæ˜¯å¯¹åº”äºç¬¬ä¸€ä¸ªæ ‡è®°çš„å‘é‡ã€‚ç°åœ¨è¿™ä¸ªå°±æ˜¯ï¼Œä½ çŸ¥é“çš„ã€‚
- en: huge thing of you knowï¼Œ numbers fromï¼Œ you knowï¼Œ negative to positiveã€‚ and this
    should haveã€‚A size of 768ï¼Œ where are weï¼ŸYeahã€‚So this is basically the numerical
    representation of the first token in the first sequence or the first sentence
    we passedã€‚Okayï¼Œ so let's just check are there any questionsï¼ŸOkayï¼Œ coolï¼Œ so let's
    carry onã€‚Okayã€‚ so this is basically what the numerical representations are produced
    by the modelã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“çš„ï¼Œè¿™æ˜¯ä¸€å¤§å †æ•°å­—ï¼Œä»è´Ÿåˆ°æ­£ã€‚è¿™åº”è¯¥æœ‰768çš„å¤§å°ï¼Œæˆ‘ä»¬åœ¨å“ªé‡Œï¼Ÿæ˜¯çš„ã€‚æ‰€ä»¥è¿™åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬ä¼ å…¥çš„ç¬¬ä¸€ä¸ªåºåˆ—æˆ–ç¬¬ä¸€å¥è¯ä¸­ç¬¬ä¸€ä¸ªæ ‡è®°çš„æ•°å€¼è¡¨ç¤ºã€‚å¥½çš„ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜ï¼Ÿå¥½çš„ï¼Œå¾ˆå¥½ï¼Œç»§ç»­ã€‚å¥½çš„ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯æ¨¡å‹ç”Ÿæˆçš„æ•°å€¼è¡¨ç¤ºã€‚
- en: And then as we saw in the videoï¼Œ these numerical representations by themselves
    they don't let us do things like text classificationã€‚ they just say the numerical
    representation of this token is blahã€‚ and now if we want to do classificationï¼Œ
    we need to take that vector or these feature vectors and then we need to add them
    or combine them with a classification headã€‚And so the whole Transer library is
    built around this idea of like taking like a model for task X and task X can be
    things like sequence classificationã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ­£å¦‚æˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œè¿™äº›æ•°å€¼è¡¨ç¤ºæœ¬èº«å¹¶ä¸èƒ½è®©æˆ‘ä»¬è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚å®ƒä»¬åªæ˜¯è¯´è¿™ä¸ªæ ‡è®°çš„æ•°å€¼è¡¨ç¤ºæ˜¯ blahã€‚å¦‚æœæˆ‘ä»¬æƒ³è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬éœ€è¦å°†é‚£ä¸ªå‘é‡æˆ–è¿™äº›ç‰¹å¾å‘é‡ä¸åˆ†ç±»å¤´è¿›è¡Œç›¸åŠ æˆ–ç»“åˆã€‚å› æ­¤ï¼Œæ•´ä¸ªTranseråº“å°±æ˜¯å›´ç»•ç€åƒè¿™æ ·å°†ä»»åŠ¡Xçš„æ¨¡å‹è¿›è¡Œæ„å»ºï¼Œè€Œä»»åŠ¡Xå¯ä»¥æ˜¯è¯¸å¦‚åºåˆ—åˆ†ç±»è¿™æ ·çš„äº‹æƒ…ã€‚
- en: question answeringï¼Œ summarizationï¼Œ translationï¼Œ so on so forthã€‚And in this caseã€‚
    when we instantiate a model with sequence classificationï¼Œ as we saw beforeã€‚ this
    is now going to createã€‚A modelã€‚Which hasã€‚A number of labelsã€‚ so you can see here
    we've now got a model with two labels because that's what this pre train checkpoint
    hasã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ç­‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“æˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ªåºåˆ—åˆ†ç±»æ¨¡å‹æ—¶ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œè¿™ç°åœ¨å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ã€‚å®ƒæœ‰å¤šä¸ªæ ‡ç­¾ã€‚å› æ­¤ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæœ‰ä¸¤ä¸ªæ ‡ç­¾çš„æ¨¡å‹ï¼Œå› ä¸ºè¿™å°±æ˜¯è¿™ä¸ªé¢„è®­ç»ƒæ£€æŸ¥ç‚¹æ‰€å…·å¤‡çš„ã€‚
- en: And then when we look at the outputsã€‚We've now got instead of having just these
    last hidden statesã€‚We've gotã€‚Logesã€‚And these logicits are basically what happens
    when you feed these feature vectors through this linear layerã€‚ this will now compress
    these 768 dimensional vectors into just two numbers or project them into two numbersã€‚And
    these are the things that we can then use to derive probabilities and figure out
    for exampleã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå½“æˆ‘ä»¬æŸ¥çœ‹è¾“å‡ºæ—¶ã€‚æˆ‘ä»¬ç°åœ¨å¾—åˆ°çš„ä¸æ˜¯ä»…æœ‰è¿™äº›æœ€åçš„éšè—çŠ¶æ€ã€‚æˆ‘ä»¬å¾—åˆ°äº†logitsã€‚è¿™äº›logitsåŸºæœ¬ä¸Šæ˜¯å°†è¿™äº›ç‰¹å¾å‘é‡è¾“å…¥åˆ°è¿™ä¸ªçº¿æ€§å±‚åçš„ç»“æœï¼Œè¿™å°†æŠŠè¿™äº›768ç»´çš„å‘é‡å‹ç¼©ä¸ºä¸¤ä¸ªæ•°å­—ï¼Œæˆ–æŠ•å½±ä¸ºä¸¤ä¸ªæ•°å­—ã€‚è¿™äº›å°±æ˜¯æˆ‘ä»¬å¯ä»¥ç”¨æ¥æ¨å¯¼æ¦‚ç‡çš„ä¸œè¥¿ï¼Œä¾‹å¦‚ã€‚
- en: which class is the most likelyï¼Œ so you can see here that you know this one here
    is more likely than this one and vice versa because I think the second example
    is like a negative sentimentã€‚Okayï¼Œ so that's more or less how we think about the
    outputs from a model versus a model with a classification headã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å“ªä¸ªç±»æœ€æœ‰å¯èƒ½ï¼Œæ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œè¿™ä¸ªæ¯”è¿™ä¸ªæ›´å¯èƒ½ï¼Œåä¹‹äº¦ç„¶ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºç¬¬äºŒä¸ªç¤ºä¾‹åƒæ˜¯è´Ÿé¢æƒ…ç»ªã€‚å¥½çš„ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•çœ‹å¾…æ¨¡å‹è¾“å‡ºä¸å¸¦æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€‚
- en: And here what we can see is if we want to convert our lodges from into probabilitiesã€‚
    we can just take a softmax over them and you may remember that the softmax basically
    takes all of the inputsã€‚ it exponentials them and then it normalizes that exponential
    by the sum of all the exponentials so you basically end up having something that
    ranges from zero to1ã€‚ so it's a good candidate for a probabilityã€‚And if we do
    thatã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°†æˆ‘ä»¬çš„é€»è¾‘è½¬æ¢ä¸ºæ¦‚ç‡ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥å¯¹å®ƒä»¬è¿›è¡Œ softmaxï¼Œä½ å¯èƒ½è¿˜è®°å¾— softmax åŸºæœ¬ä¸Šå¤„ç†æ‰€æœ‰è¾“å…¥ã€‚å®ƒå¯¹å®ƒä»¬è¿›è¡ŒæŒ‡æ•°è¿ç®—ï¼Œç„¶åé€šè¿‡æ‰€æœ‰æŒ‡æ•°çš„æ€»å’Œæ¥æ ‡å‡†åŒ–ï¼Œæ‰€ä»¥ä½ æœ€ç»ˆå¾—åˆ°çš„èŒƒå›´æ˜¯ä»é›¶åˆ°ä¸€ã€‚æ‰€ä»¥å®ƒæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ¦‚ç‡å€™é€‰ã€‚å¦‚æœæˆ‘ä»¬è¿™æ ·åšã€‚
- en: we then get now probabilities for each of the two sentimentsã€‚And also we now
    can see this is the way we can map between the label IDï¼Œ which saysã€‚ you know
    what does zero mean in terms of something that's a bit more meaningfulï¼ŸOkayã€‚ so
    let's have a lookã€‚Let's seeã€‚Okayï¼Œ greatï¼Œ So we have a question from SRM Sumyaã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°æ¯ä¸ªæƒ…ç»ªçš„æ¦‚ç‡ã€‚ç°åœ¨æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°è¿™æ˜¯æˆ‘ä»¬å¯ä»¥åœ¨æ ‡ç­¾ ID ä¹‹é—´æ˜ å°„çš„æ–¹å¼ï¼Œå®ƒè¡¨ç¤ºã€‚ä½ çŸ¥é“é›¶åœ¨æŸç§æ›´æœ‰æ„ä¹‰çš„ä¸œè¥¿ä¸­æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿå¥½çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ã€‚å¥½çš„ï¼Œå¤ªå¥½äº†ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¥è‡ª
    SRM Sumya çš„é—®é¢˜ã€‚
- en: Which says the classification model should take the output from the distiller
    model that's exactly right so in fact's let's have a look at thisã€‚If we look atã€‚Classï¼Œ
    I'm doing this for Bertï¼Œ but it's the same for dist Btã€‚ So if we take Bt model
    forã€‚Where the full sequence classificationã€‚So if you look at what this model actually
    hasï¼Œ it has the BRT model that we saw or the distilled Bt model we saw in our
    exampleã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¡¨ç¤ºåˆ†ç±»æ¨¡å‹åº”è¯¥æ¥å—æ¥è‡ªè’¸é¦æ¨¡å‹çš„è¾“å‡ºï¼Œè¿™æ­£æ˜¯æ­£ç¡®çš„ï¼Œå®é™…ä¸Šæˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªã€‚å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹ã€‚ç±»ï¼Œæˆ‘ä¸º Bert åšè¿™ä¸ªï¼Œä½†å¯¹äº dist Bt ä¹Ÿæ˜¯ä¸€æ ·ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æ‹¿
    Bt æ¨¡å‹ã€‚å…¨åºåˆ—åˆ†ç±»ã€‚æ‰€ä»¥å¦‚æœä½ çœ‹çœ‹è¿™ä¸ªæ¨¡å‹å®é™…åŒ…å«ä»€ä¹ˆï¼Œå®ƒåŒ…å«æˆ‘ä»¬åœ¨ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ BRT æ¨¡å‹æˆ–è’¸é¦çš„ Bt æ¨¡å‹ã€‚
- en: and then it just applies dropout and a linear layerã€‚And linear layer has a dimension
    of the hidden size of the 768ã€‚ and then it's going to compress that into just
    these two numbers defined by the number of labelsã€‚And so if we look down at what
    happens inside the forward passã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒåªéœ€åº”ç”¨ dropout å’Œçº¿æ€§å±‚ã€‚çº¿æ€§å±‚çš„ç»´åº¦æ˜¯ 768 çš„éšè—å¤§å°ã€‚ç„¶åå®ƒå°†æŠŠè¿™ä¸ªå‹ç¼©æˆä»…ç”±æ ‡ç­¾æ•°é‡å®šä¹‰çš„ä¸¤ä¸ªæ•°å­—ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å¾€ä¸‹çœ‹ï¼Œåœ¨å‰å‘ä¼ é€’ä¸­å‘ç”Ÿäº†ä»€ä¹ˆã€‚
- en: the first thing we do is we get the outputs from the BRT modelã€‚ So these are
    just the feature vectorsï¼Œ these 768 dimensional vectorsã€‚And then you can skip
    most of this kind of stuffã€‚ the main point is thatã€‚hereã€‚Weã€‚But but wellã€‚ don't
    worry about the port outputï¼Œ the main thing is that we feed these outputs into
    the classification head to produce the logicitsã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå¾—åˆ° BRT æ¨¡å‹çš„è¾“å‡ºã€‚è¿™äº›åªæ˜¯ç‰¹å¾å‘é‡ï¼Œè¿™äº› 768 ç»´çš„å‘é‡ã€‚ç„¶åä½ å¯ä»¥è·³è¿‡å¤§éƒ¨åˆ†è¿™æ ·çš„å†…å®¹ã€‚ä¸»è¦çš„è§‚ç‚¹æ˜¯ã€‚åœ¨è¿™é‡Œã€‚æˆ‘ä»¬ã€‚ä½†æ˜¯ä¸ç”¨æ‹…å¿ƒç«¯å£è¾“å‡ºï¼Œä¸»è¦æ˜¯æˆ‘ä»¬å°†è¿™äº›è¾“å‡ºè¾“å…¥åˆ°åˆ†ç±»å¤´ä¸­ä»¥äº§ç”Ÿé€»è¾‘ã€‚
- en: So that's a great questionã€‚Yesã€‚å—¯ã€‚è¿™è¿™ã€‚Okayï¼Œ so we've got a question from Platin
    Chibaï¼Œ soã€‚How can we see what the token representation means in the textï¼ŸSoã€‚Coolï¼Œ
    so maybe just to show youã€‚ likeã€‚Something that let's seeï¼Œ maybe we get ahead of
    ourselvesï¼Œ but that's okayï¼Œ okayã€‚We've got these raw inputs which are given by
    these stringsã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚æ˜¯çš„ã€‚å—¯ã€‚è¿™è¿™ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¥è‡ª Platin Chiba çš„é—®é¢˜ã€‚æˆ‘ä»¬å¦‚ä½•æŸ¥çœ‹æ ‡è®°è¡¨ç¤ºåœ¨æ–‡æœ¬ä¸­çš„æ„ä¹‰ï¼Ÿæ‰€ä»¥ã€‚å¾ˆå¥½ï¼Œæˆ–è®¸åªæ˜¯ä¸ºäº†å±•ç¤ºç»™ä½ ã€‚æ¯”å¦‚ã€‚æˆ‘ä»¬å¯èƒ½æœ‰ç‚¹æ€¥äºæ±‚æˆï¼Œä½†è¿™æ²¡å…³ç³»ï¼Œå¥½çš„ã€‚æˆ‘ä»¬æœ‰è¿™äº›åŸå§‹è¾“å…¥ï¼Œå®ƒä»¬ç”±è¿™äº›å­—ç¬¦ä¸²ç»™å‡ºã€‚
- en: and then we get these input IDs like thisï¼Œ rightï¼ŸAnd so one thing we could doã€‚If
    you want to go backwards and we're going to see this laterã€‚But what I could doã€‚I
    can say okayã€‚Tokenizerï¼Œ and I'm going to decodeï¼Œ so I'm going to do the opposite
    of what I did beforeã€‚And now I'm going take my input Idsã€‚And fingers crossedã€‚This
    need to do inputã€‚Hiesã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°è¿™äº›è¾“å…¥ IDï¼Œå°±åƒè¿™æ ·ï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åšä¸€ä»¶äº‹ã€‚å¦‚æœä½ æƒ³è¦å›é€€ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ä½†æˆ‘å¯ä»¥è¿™æ ·åšã€‚æˆ‘å¯ä»¥è¯´å¥½å§ã€‚Tokenizerï¼Œæˆ‘è¦è§£ç ï¼Œæ‰€ä»¥æˆ‘è¦åšæˆ‘ä¹‹å‰æ‰€åšçš„äº‹æƒ…çš„ç›¸åã€‚ç°åœ¨æˆ‘è¦æ‹¿æˆ‘çš„è¾“å…¥
    IDã€‚å¸Œæœ›èƒ½æˆåŠŸã€‚è¿™éœ€è¦è¾“å…¥ã€‚Hiesã€‚
- en: And now you can see by using this decocode methodï¼Œ we're able to kind of reverse
    the process of the broad textã€‚ but what it does is it also introduces some special
    tokensï¼Œ one is called the CLS tokenã€‚ which kind of just tells you this is like
    the start of the sentenceã€‚ and then we have a Sep token which basically is used
    to distinguish between pairs of sentencesã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ä½¿ç”¨è¿™ä¸ªè§£ç æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€†è½¬å¹¿æ³›æ–‡æœ¬çš„è¿‡ç¨‹ã€‚ä½†å®ƒçš„ä½œç”¨æ˜¯å¼•å…¥ä¸€äº›ç‰¹æ®Šçš„æ ‡è®°ï¼Œå…¶ä¸­ä¸€ä¸ªç§°ä¸º CLS æ ‡è®°ï¼Œå®ƒè¡¨ç¤ºå¥å­çš„å¼€å§‹ã€‚ç„¶åæˆ‘ä»¬æœ‰ä¸€ä¸ª
    Sep æ ‡è®°ï¼Œå®ƒåŸºæœ¬ä¸Šç”¨äºåŒºåˆ†å¥å­å¯¹ä¹‹é—´çš„åŒºåˆ«ã€‚
- en: So this is one where you can go back from where you startedã€‚And yeahï¼Œ if you
    have more questionsã€‚ we can tackle them as we go aheadã€‚Okayï¼Œ coolï¼Œ so that's some
    the the sort of first look at how the pipeline works under the hoodã€‚So now what
    we could doã€‚Is let's have a look at like the models in more detailã€‚So I'm going
    to start by watching this video and then we'll pause for questions and then again
    look at some codeã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¯ä»¥å›åˆ°ä½ å¼€å§‹çš„åœ°æ–¹çš„æ¨¡å‹ã€‚å¦‚æœä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿›è¡Œæ—¶è§£å†³ã€‚å¥½çš„ï¼Œé…·ï¼Œæ‰€ä»¥è¿™å°±æ˜¯å…³äºç®¡é“å¦‚ä½•åœ¨å†…éƒ¨å·¥ä½œçš„åˆæ­¥è§‚å¯Ÿã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åšçš„ï¼Œæ˜¯æ›´è¯¦ç»†åœ°çœ‹çœ‹æ¨¡å‹ã€‚æ‰€ä»¥æˆ‘å°†å¼€å§‹è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼Œç„¶åæˆ‘ä»¬å°†æš‚åœä»¥æé—®ï¼Œå†çœ‹çœ‹ä¸€äº›ä»£ç ã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_14.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_14.png)'
- en: '![](img/40873acb06abf924ac4a43fae802679a_15.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_15.png)'
- en: How to instantiate a transforms modelã€‚In this videoã€‚ we'll look at how we can
    create and use the model from the Transformers libraryã€‚As we seen beforeã€‚ the
    Automod class allows you to instantiate a proed model from any checkpoint on the
    I face appã€‚It will pick the right model class from the library to instant shade
    the proper architecture and load the weight of the preed model insideã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•å®ä¾‹åŒ–ä¸€ä¸ªå˜æ¢æ¨¡å‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä» Transformers åº“åˆ›å»ºå’Œä½¿ç”¨æ¨¡å‹ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼ŒAutomod ç±»å…è®¸ä½ ä» I
    face åº”ç”¨çš„ä»»ä½•æ£€æŸ¥ç‚¹å®ä¾‹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚å®ƒä¼šä»åº“ä¸­é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹ç±»ï¼Œä»¥å®ä¾‹åŒ–é€‚å½“çš„æ¶æ„ï¼Œå¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ã€‚
- en: As we can seeï¼Œ when given a bird checkpointï¼Œ we end up with a bird model and
    similarly for GPT2 or partã€‚Beyond the scenesï¼Œ this API can take the name of a
    checkpoint on the earthã€‚ in which case it will download and cache the configuration
    file as well as the model weights fileã€‚You can also specify the path to a local
    folder that contains a valid configuration file and a model of weights fileã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå½“ç»™å®šä¸€ä¸ªé¸Ÿç±»æ£€æŸ¥ç‚¹æ—¶ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªé¸Ÿç±»æ¨¡å‹ï¼Œå¯¹äº GPT2 æˆ– part ä¹Ÿæ˜¯ç±»ä¼¼çš„ã€‚åœ¨åå°ï¼Œè¿™ä¸ª API å¯ä»¥æ¥æ”¶åœ°çƒä¸Šçš„æ£€æŸ¥ç‚¹åç§°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†ä¸‹è½½å¹¶ç¼“å­˜é…ç½®æ–‡ä»¶ä»¥åŠæ¨¡å‹æƒé‡æ–‡ä»¶ã€‚ä½ è¿˜å¯ä»¥æŒ‡å®šåŒ…å«æœ‰æ•ˆé…ç½®æ–‡ä»¶å’Œæ¨¡å‹æƒé‡æ–‡ä»¶çš„æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„ã€‚
- en: To instantiate the between modelï¼Œ the Automodal API will first open the configuration
    file to look at the configuration class that should be usedã€‚The configuration
    class depends on the type of the modelï¼Œ Bï¼Œ GPT2 or parttï¼Œ for instanceã€‚Once it
    has a proper configuration classï¼Œ it can instantiate that configurationã€‚ which
    is a blueprint to know how to create the modelã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®ä¾‹åŒ–ä¹‹é—´æ¨¡å‹ï¼ŒAutomodal API é¦–å…ˆä¼šæ‰“å¼€é…ç½®æ–‡ä»¶ï¼Œä»¥æŸ¥çœ‹åº”ä½¿ç”¨çš„é…ç½®ç±»ã€‚é…ç½®ç±»ä¾èµ–äºæ¨¡å‹ç±»å‹ï¼Œä¾‹å¦‚ Bã€GPT2 æˆ– partã€‚ä¸€æ—¦æœ‰äº†åˆé€‚çš„é…ç½®ç±»ï¼Œå®ƒå°±å¯ä»¥å®ä¾‹åŒ–è¯¥é…ç½®ï¼Œè¿™æ˜¯ä¸€ç§çŸ¥é“å¦‚ä½•åˆ›å»ºæ¨¡å‹çš„è“å›¾ã€‚
- en: It also uses this configuration class to find the proper model classã€‚Which is
    then combined with the loaded configuration to load the modelã€‚Its model is not
    yet a pro modelï¼Œ as it has just been initialized with randomdom weightsã€‚The last
    step is to load the weight from the model file inside this modelã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿˜ä½¿ç”¨è¿™ä¸ªé…ç½®ç±»æ¥æ‰¾åˆ°é€‚å½“çš„æ¨¡å‹ç±»ã€‚ç„¶åå°†å…¶ä¸åŠ è½½çš„é…ç½®ç›¸ç»“åˆï¼Œä»¥åŠ è½½æ¨¡å‹ã€‚å®ƒçš„æ¨¡å‹å°šæœªæˆä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºå®ƒåˆšåˆšç”¨éšæœºæƒé‡åˆå§‹åŒ–ã€‚æœ€åä¸€æ­¥æ˜¯ä»æ¨¡å‹æ–‡ä»¶ä¸­åŠ è½½æƒé‡åˆ°è¯¥æ¨¡å‹ä¸­ã€‚
- en: To easily load the configuration of a model from any checkpoint or a folder
    containing the configuration fileã€‚ we can use the autoconfig classã€‚Like the Automod
    classã€‚ it will pick the right configuration class from the libraryã€‚We can also
    use a specific class corresponding to a checkpoint that well need to change your
    code each time we want to try a different model architectureã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è½»æ¾åŠ è½½æ¥è‡ªä»»ä½•æ£€æŸ¥ç‚¹æˆ–åŒ…å«é…ç½®æ–‡ä»¶çš„æ–‡ä»¶å¤¹çš„æ¨¡å‹é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ autoconfig ç±»ã€‚åƒ Automod ç±»ä¸€æ ·ï¼Œå®ƒå°†ä»åº“ä¸­é€‰æ‹©æ­£ç¡®çš„é…ç½®ç±»ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ä¸æ£€æŸ¥ç‚¹å¯¹åº”çš„ç‰¹å®šç±»ï¼Œè¿™æ ·æ¯æ¬¡æƒ³å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„æ—¶ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´æ”¹ä»£ç ã€‚
- en: As we said beforeï¼Œ the configuration of a model is a blueprint that contains
    all the information necessary to create the model architectureã€‚For instanceï¼Œ the
    bird model associated with a bird based case checkpoint has 12 layersã€‚ a hidden
    side of 768ã€‚And the vocabulary size of 28996ã€‚Once we add the configurationã€‚ we
    can create a model that has the same architecture as a checkpointï¼Œ but is randomly
    initializedã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¯´ï¼Œæ¨¡å‹çš„é…ç½®æ˜¯ä¸€ä¸ªè“å›¾ï¼ŒåŒ…å«åˆ›å»ºæ¨¡å‹æ¶æ„æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œä¸åŸºäºé¸Ÿçš„æ¡ˆä¾‹æ£€æŸ¥ç‚¹ç›¸å…³çš„é¸Ÿæ¨¡å‹æœ‰12å±‚ï¼Œéšè—å±‚å¤§å°ä¸º768ï¼Œè¯æ±‡é‡å¤§å°ä¸º28996ã€‚ä¸€æ—¦æˆ‘ä»¬æ·»åŠ äº†é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªä¸æ£€æŸ¥ç‚¹å…·æœ‰ç›¸åŒæ¶æ„ä½†éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ã€‚
- en: We can vet training it from scratch like any by doch modelã€‚We can also change
    any part of the configuration by using keyword argumentsã€‚So sequence snippet of
    code instant shades a randomly initialized layout model with 10 layers instead
    of 12ã€‚Saving a model once its trend off fine tune is very easyã€‚We just have to
    use the safe between methodã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åƒä»»ä½•å…¶ä»–æ¨¡å‹ä¸€æ ·ä»å¤´å¼€å§‹è®­ç»ƒå®ƒã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ä½¿ç”¨å…³é”®å­—å‚æ•°æ›´æ”¹é…ç½®çš„ä»»ä½•éƒ¨åˆ†ã€‚å› æ­¤ï¼Œè¿™æ®µä»£ç ç‰‡æ®µå®ä¾‹åŒ–äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å…·æœ‰10å±‚çš„å¸ƒå±€æ¨¡å‹ï¼Œè€Œä¸æ˜¯12å±‚ã€‚ä¸€æ—¦æ¨¡å‹ç»è¿‡å¾®è°ƒï¼Œä¿å­˜å®ƒæ˜¯éå¸¸ç®€å•çš„ã€‚æˆ‘ä»¬åªéœ€ä½¿ç”¨ä¿å­˜æ–¹æ³•ã€‚
- en: Hereï¼Œ the model will be saved in a folder named My beltt model inside the current
    working directoryã€‚Such a model can then be re using the from between methodã€‚To
    learn how to easily approach this model to the aï¼Œ check out the push to videoã€‚![](img/40873acb06abf924ac4a43fae802679a_17.png)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨å½“å‰å·¥ä½œç›®å½•ä¸­çš„åä¸ºMy beltt modelçš„æ–‡ä»¶å¤¹å†…ã€‚è¿™æ ·ä¸€ä¸ªæ¨¡å‹å¯ä»¥é€šè¿‡from betweenæ–¹æ³•é‡æ–°ä½¿ç”¨ã€‚è¦äº†è§£å¦‚ä½•è½»æ¾è®¿é—®è¿™ä¸ªæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹æ¨é€åˆ°è§†é¢‘ã€‚![](img/40873acb06abf924ac4a43fae802679a_17.png)
- en: '![](img/40873acb06abf924ac4a43fae802679a_18.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_18.png)'
- en: ã€‚So any questions so far about the model like loading and saving models before
    we dive into some codeï¼Ÿ
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢å…³äºæ¨¡å‹åŠ è½½å’Œä¿å­˜æ¨¡å‹çš„ä»»ä½•é—®é¢˜å—ï¼Œåœ¨æˆ‘ä»¬æ·±å…¥ä¸€äº›ä»£ç ä¹‹å‰ï¼Ÿ
- en: Soã€‚Just to sort of summarize what we saw in the videoã€‚ whenever we do this from
    pretrained method with a modelã€‚ we first need to get a config and then we saw
    that config just a couple of minutes agoã€‚ it defines things like the mapping of
    the labels to the ideas and how many labels the model has and all that kind of
    stuffã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä¸ºäº†æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚æ¯å½“æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è·å–ä¸€ä¸ªé…ç½®ï¼Œç„¶åæˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„é…ç½®åœ¨å‡ åˆ†é’Ÿå‰å®šä¹‰äº†æ ‡ç­¾åˆ°æ ‡è¯†ç¬¦çš„æ˜ å°„ã€æ¨¡å‹çš„æ ‡ç­¾æ•°é‡ä»¥åŠæ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: how many layers all those thingsï¼Œ and then that config is then used to load
    the weights of the model so that it makes sure that everything is kind of configured
    in the right wayã€‚And then once we haveã€‚This modelï¼Œ we can then save it and then
    use it for other thingsã€‚Soã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå°‘å±‚ç­‰ç­‰ï¼Œç„¶åè¯¥é…ç½®ç”¨äºåŠ è½½æ¨¡å‹çš„æƒé‡ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡ä»¥æ­£ç¡®çš„æ–¹å¼é…ç½®ã€‚ç„¶åï¼Œä¸€æ—¦æˆ‘ä»¬æœ‰äº†è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä¿å­˜å®ƒå¹¶ç”¨äºå…¶ä»–äº‹æƒ…ã€‚æ‰€ä»¥ã€‚
- en: If there's no kind of urgent questions right nowï¼Œ I'll have a look atã€‚The models
    codeã€‚ just as a mentionedï¼Œ you can watch these videos and your own time and work
    through this kind of textã€‚ but I think it might be sort of more useful if we just
    have a lookã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç°åœ¨æ²¡æœ‰ä»€ä¹ˆç´§æ€¥çš„é—®é¢˜ï¼Œæˆ‘æ¥çœ‹çœ‹æ¨¡å‹çš„ä»£ç ã€‚æ­£å¦‚æˆ‘æåˆ°çš„ï¼Œä½ å¯ä»¥åœ¨è‡ªå·±çš„æ—¶é—´è§‚çœ‹è¿™äº›è§†é¢‘ï¼Œå¹¶é€šè¿‡è¿™ç±»æ–‡æœ¬è¿›è¡Œå­¦ä¹ ã€‚ä½†æˆ‘è®¤ä¸ºå¦‚æœæˆ‘ä»¬ç›´æ¥çœ‹ä¸€ä¸‹å¯èƒ½æ›´æœ‰ç”¨ã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_20.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_20.png)'
- en: '![](img/40873acb06abf924ac4a43fae802679a_21.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_21.png)'
- en: Atã€‚At the codeï¼Œ soã€‚å—¯But but butã€‚Let's just checkï¼Œ I can run transformersã€‚okï¼Œæ‰€ã€‚One
    thing maybe to mention isã€‚A really common example or situation that youll find
    yourself in is you basically you've trained a model and now you want to share
    it in some way and the sharing typically at least when I was working in my previous
    companyã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»£ç å¤„ï¼Œå—¯ï¼Œä½†è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ï¼Œæˆ‘å¯ä»¥è¿è¡Œtransformersã€‚å¥½çš„ï¼Œæ‰€ã€‚ä¸€ä»¶å€¼å¾—æåŠçš„äº‹æƒ…æ˜¯ï¼Œä½ å¯èƒ½ä¼šå‘ç°è‡ªå·±å¤„äºä¸€ç§éå¸¸å¸¸è§çš„æƒ…å†µï¼šåŸºæœ¬ä¸Šä½ å·²ç»è®­ç»ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œç°åœ¨æƒ³ä»¥æŸç§æ–¹å¼åˆ†äº«å®ƒï¼Œè€Œè¿™ç§åˆ†äº«é€šå¸¸æ˜¯åœ¨æˆ‘ä¹‹å‰å·¥ä½œçš„å…¬å¸ä¸­ã€‚
- en: it was much more about deploying this model so that you could serve it or produce
    predictions that other services could consumeã€‚And so once you've saved your modelï¼Œ
    the question isï¼Œ okayï¼Œ what the hell do I do with this thingï¼ŸğŸ˜Šã€‚And as we can see
    hereï¼Œ this save thing will basically save two objectsã€‚ it will save a configuration
    JO fileï¼Œ and it will also save a Pytorrch model do bin fileã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ›´å¤šæ˜¯å…³äºéƒ¨ç½²è¿™ä¸ªæ¨¡å‹ï¼Œä»¥ä¾¿ä½ å¯ä»¥ä¸ºå…¶ä»–æœåŠ¡æä¾›æœåŠ¡æˆ–ç”Ÿæˆé¢„æµ‹ã€‚å› æ­¤ï¼Œä¸€æ—¦ä½ ä¿å­˜äº†æ¨¡å‹ï¼Œé—®é¢˜æ˜¯ï¼Œå¥½çš„ï¼Œæˆ‘åˆ°åº•è¯¥å¦‚ä½•å¤„ç†è¿™ä¸ªä¸œè¥¿ï¼ŸğŸ˜Šã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè¿™ä¸ªä¿å­˜åŠŸèƒ½åŸºæœ¬ä¸Šä¼šä¿å­˜ä¸¤ä¸ªå¯¹è±¡ã€‚å®ƒä¼šä¿å­˜ä¸€ä¸ªé…ç½®JOæ–‡ä»¶ï¼Œå¹¶ä¸”è¿˜ä¼šä¿å­˜ä¸€ä¸ªPytorchæ¨¡å‹çš„do
    binæ–‡ä»¶ã€‚
- en: and this is something in Pytorrch called a state dictionary which basically
    provides all the information for the layers and the weightsã€‚And so if we want
    to use this in like to produce predictionsã€‚The first thing we need to do is what
    we've always been doing is we take some input textã€‚We convert it into input IDsã€‚And
    then we need to convert those input IDs into tensesorsã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨ Pytorrch ä¸­è¢«ç§°ä¸ºçŠ¶æ€å­—å…¸ï¼Œå®ƒæä¾›äº†æ‰€æœ‰å±‚å’Œæƒé‡çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ç”¨è¿™ä¸ªæ¥ç”Ÿæˆé¢„æµ‹ï¼Œæˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹å°±æ˜¯åƒä»¥å‰ä¸€æ ·è·å–è¾“å…¥æ–‡æœ¬ï¼Œå°†å…¶è½¬æ¢ä¸ºè¾“å…¥
    IDï¼Œç„¶åå°†è¿™äº›è¾“å…¥ ID è½¬æ¢ä¸ºå¼ é‡ã€‚
- en: which we can then feed to the modelã€‚And so previously what we were doing was
    using like the tokenizerã€‚ and that's exactly what you would also do in practiceã€‚
    but in this exampleã€‚ we're just showing the outputs of the tokenizerã€‚So let's
    have a look at what that looks like in codeã€‚So Is check if there are any questionsï¼Œ
    okayï¼ŸOkayï¼Œ soã€‚ğŸ˜Šï¼Œå—¯ã€‚ğŸ˜Šã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™äº›å¼ é‡æä¾›ç»™æ¨¡å‹ã€‚ä¹‹å‰æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯åˆ†è¯å™¨ï¼Œè¿™æ­£æ˜¯ä½ åœ¨å®é™…ä¸­ä¼šåšçš„ã€‚ä½†åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯å±•ç¤ºäº†åˆ†è¯å™¨çš„è¾“å‡ºã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä»£ç ä¸­é‚£æ˜¯ä»€ä¹ˆæ ·å­ã€‚æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰é—®é¢˜ï¼Œå¥½å—ï¼Ÿå¥½çš„ï¼Œå—¯ã€‚ğŸ˜Šï¼Œå—¯ã€‚ğŸ˜Šã€‚
- en: Maybe just to quickly summarize we've got you can also load your configurations
    using two different thingsã€‚ you can either load your model directly from one of
    the default configs in the library and then this will provide you with like you
    know a kind of summary about the hidden size and so onã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸å¿«é€Ÿæ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ–¹å¼åŠ è½½é…ç½®ã€‚ä½ å¯ä»¥ç›´æ¥ä»åº“ä¸­çš„é»˜è®¤é…ç½®åŠ è½½æ¨¡å‹ï¼Œè¿™å°†ä¸ºä½ æä¾›æœ‰å…³éšè—å±‚å¤§å°ç­‰çš„*æ‘˜è¦*ã€‚
- en: But if you do thisï¼Œ the model is completely randomly initializedã€‚ which means
    all the weights are just random and this model is going to just be garbageã€‚ it's
    not going to help you make any good predictions and this is what you do actually
    when you want to pretrain a model or you want to really train a model from scratchã€‚å—¯ã€‚So
    in practiceï¼Œ most of the timeï¼Œ what you're really doing is using the from pretrained
    and then this will initialize the model with the pretrain weights and the correct
    head if we need itã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœä½ è¿™æ ·åšï¼Œæ¨¡å‹å°†ä¼šå®Œå…¨éšæœºåˆå§‹åŒ–ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰æƒé‡éƒ½æ˜¯éšæœºçš„ï¼Œè¿™ä¸ªæ¨¡å‹å°†æ¯«æ— ç”¨å¤„ï¼Œå®ƒä¸ä¼šå¸®åŠ©ä½ åšå‡ºä»»ä½•å¥½çš„é¢„æµ‹ã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯ä½ åœ¨æƒ³è¦å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒæˆ–è€…ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶æ‰€åšçš„äº‹æƒ…ã€‚å—¯ã€‚å› æ­¤åœ¨å®è·µä¸­ï¼Œå¤§å¤šæ•°æ—¶å€™ï¼Œä½ çœŸæ­£åšçš„æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿™å°†ç”¨é¢„è®­ç»ƒæƒé‡å’Œéœ€è¦çš„æ­£ç¡®å¤´åˆå§‹åŒ–æ¨¡å‹ã€‚
- en: So if we wanted to doï¼Œ say predictionsï¼Œ let me just instantiate thisã€‚So let's
    suppose that I've got my model and I'm happy with it and so I want toã€‚Save it
    so I can deploy it somewhereã€‚So let's just wait through this model to downloadã€‚Okayï¼Œ
    goodã€‚ So then what I could do is I could save my modelï¼Œ and this is just some
    path on your your on your machine soã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘å…ˆå®ä¾‹åŒ–ä¸€ä¸‹ã€‚å‡è®¾æˆ‘å·²ç»æœ‰äº†æˆ‘çš„æ¨¡å‹å¹¶ä¸”å¯¹å®ƒæ»¡æ„ï¼Œå› æ­¤æˆ‘æƒ³ä¿å­˜å®ƒä»¥ä¾¿å¯ä»¥éƒ¨ç½²åˆ°æŸå¤„ã€‚è®©æˆ‘ä»¬ç­‰å¾…æ¨¡å‹ä¸‹è½½ã€‚å¥½çš„ï¼Œå¥½çš„ã€‚é‚£ä¹ˆæˆ‘å¯ä»¥åšçš„å°±æ˜¯ä¿å­˜æˆ‘çš„æ¨¡å‹ï¼Œè¿™åªæ˜¯ä½ æœºå™¨ä¸Šçš„ä¸€ä¸ªè·¯å¾„ã€‚
- en: If we now lookã€‚Inside the file systemã€‚We can see that we've got a directory
    called directory on my computerã€‚ So now if I have a look at what's inside that
    directoryã€‚I've got these two filesã€‚ I've got this config Jasonï¼Œ and I've got this
    like binary file called Pythtorage modelã€‚And soã€‚What we can do now is we can take
    that folder and we can you know wrap it upï¼Œ zip it upã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ç°åœ¨æŸ¥çœ‹æ–‡ä»¶ç³»ç»Ÿï¼Œå¯ä»¥çœ‹åˆ°æˆ‘è®¡ç®—æœºä¸Šæœ‰ä¸€ä¸ªåä¸º directory çš„ç›®å½•ã€‚é‚£ä¹ˆç°åœ¨å¦‚æœæˆ‘æŸ¥çœ‹é‚£ä¸ªç›®å½•é‡Œé¢æœ‰ä»€ä¹ˆï¼Œæˆ‘æœ‰è¿™ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯ config
    Jasonï¼Œå¦ä¸€ä¸ªæ˜¯ä¸€ä¸ªåä¸º Pythtorage model çš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æŠŠé‚£ä¸ªæ–‡ä»¶å¤¹æ‰“åŒ…ï¼Œå‹ç¼©ã€‚
- en: put it on a machineã€‚And then if we want to get new predictionsã€‚Then what we
    do is we take our tokenized inputsã€‚We then feed those or convert them into a tensor
    because all the ptorch models expect towarch tensorsã€‚And so if we look at this
    model inputsã€‚It's just going to be a tensorã€‚And then we feed these inputs to the
    modelï¼Œ and then this is now what would constitute a predictionã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¾åˆ°ä¸€å°æœºå™¨ä¸Šã€‚å¦‚æœæˆ‘ä»¬æƒ³è·å¾—æ–°çš„é¢„æµ‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¦åšçš„æ˜¯è·å–æˆ‘ä»¬çš„åˆ†è¯è¾“å…¥ï¼Œç„¶åå°†è¿™äº›è¾“å…¥è½¬æ¢ä¸ºå¼ é‡ï¼Œå› ä¸ºæ‰€æœ‰çš„ ptorch æ¨¡å‹éƒ½æœŸæœ›è¾“å…¥å¼ é‡ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªæ¨¡å‹è¾“å…¥ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªå¼ é‡ã€‚ç„¶åæˆ‘ä»¬å°†è¿™äº›è¾“å…¥æä¾›ç»™æ¨¡å‹ï¼Œè¿™æ ·å°±æ„æˆäº†ä¸€ä¸ªé¢„æµ‹ã€‚
- en: and then you can you know do whatever you want with that predictionã€‚ maybe use
    it to make some sort of decisions or maybe use it to feed a dashboardã€‚ basically
    the skysal limitã€‚And that's more or less like sort ofï¼Œ you knowã€‚ how you generate
    predictionsï¼Œ' pretty straightforwardã€‚So let's have a lookã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å¯ä»¥å¯¹è¿™ä¸ªé¢„æµ‹åšä»»ä½•æƒ³åšçš„äº‹æƒ…ï¼Œå¯èƒ½ç”¨å®ƒæ¥åšæŸç§å†³ç­–ï¼Œæˆ–è€…ç”¨æ¥è¾“å…¥åˆ°ä»ªè¡¨æ¿ã€‚åŸºæœ¬ä¸Šæ˜¯æ²¡æœ‰é™åˆ¶çš„ã€‚è¿™å°±æ˜¯ç”Ÿæˆé¢„æµ‹çš„è¿‡ç¨‹ï¼Œ*éå¸¸ç®€å•*ã€‚æ‰€ä»¥è®©æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚
- en: We've got a question hereã€‚Out of interestï¼Œ how long would it take to train Bert
    from scratch and can you do it on coabï¼Ÿ
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿™é‡Œæœ‰ä¸€ä¸ªé—®é¢˜ã€‚å‡ºäºå…´è¶£ï¼Œä»å¤´è®­ç»ƒBertéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿä½ å¯ä»¥åœ¨coabä¸Šåšåˆ°å—ï¼Ÿ
- en: Okayï¼Œ soã€‚I think if you it really depends on the size of the corpus that you
    want to use So for exampleã€‚ Bert was trained if I'm not mistaken on all of English
    Wikipediaã€‚And a corpus called the Books corpusï¼Œ which is sort of scanned library
    booksã€‚Andã€‚I thinkã€‚Let me thinkã€‚Soï¼Œ you know let's do something like this let's
    why don't we find the answer so on the fly because I don't remember off the top
    of my head how long it took them to do itã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™çœŸçš„å–å†³äºä½ æƒ³ä½¿ç”¨çš„è¯­æ–™åº“çš„å¤§å°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘æ²¡è®°é”™çš„è¯ï¼ŒBertæ˜¯åœ¨æ•´ä¸ªè‹±è¯­ç»´åŸºç™¾ç§‘å’Œä¸€ä¸ªç§°ä¸ºBooks corpusçš„è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œåè€…æ˜¯æ‰«æè¿‡çš„å›¾ä¹¦é¦†ä¹¦ç±ã€‚è€Œä¸”ã€‚æˆ‘æƒ³æƒ³ã€‚æ‰€ä»¥ï¼Œä½ çŸ¥é“ï¼Œè®©æˆ‘ä»¬è¿™æ ·åšï¼Œä¸ºä»€ä¹ˆä¸ç°åœºæ‰¾åˆ°ç­”æ¡ˆï¼Œå› ä¸ºæˆ‘ä¸è®°å¾—ä»–ä»¬èŠ±äº†å¤šé•¿æ—¶é—´å»åšè¿™ä¸ªã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_23.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_23.png)'
- en: '![](img/40873acb06abf924ac4a43fae802679a_24.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_24.png)'
- en: '![](img/40873acb06abf924ac4a43fae802679a_25.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_25.png)'
- en: And there's nothing better than liveã€‚Reading papersã€‚![](img/40873acb06abf924ac4a43fae802679a_27.png)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ä»€ä¹ˆæ¯”ç°åœºé˜…è¯»è®ºæ–‡æ›´å¥½çš„äº†ã€‚![](img/40873acb06abf924ac4a43fae802679a_27.png)
- en: Okayã€‚ğŸ˜Šï¼ŒSoï¼Œ here's the book paperã€‚And let's have a look atï¼Œ I'm guessing they
    use TPUsã€‚Okayï¼Œ soã€‚They say here that they trained BerRT base on four cloud TPUsï¼Œ
    so this is 16 TPU chipsã€‚And each pre training took four days to completeã€‚Soã€‚I
    think from memoryã€‚ the cloud TPUs you get on coabab are just one TPU chipã€‚So sort
    of roughly speakingã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚ğŸ˜Šé‚£ä¹ˆï¼Œè¿™æ˜¯ä¹¦ç±è®ºæ–‡ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼Œæˆ‘çŒœä»–ä»¬ä½¿ç”¨TPUã€‚å¥½çš„ï¼Œä»–ä»¬åœ¨è¿™é‡Œè¯´ä»–ä»¬åœ¨å››ä¸ªäº‘TPUä¸Šè®­ç»ƒäº†BERTåŸºç¡€ç‰ˆï¼Œæ‰€ä»¥è¿™å°±æ˜¯16ä¸ªTPUèŠ¯ç‰‡ã€‚æ¯æ¬¡é¢„è®­ç»ƒéœ€è¦å››å¤©æ‰èƒ½å®Œæˆã€‚æ‰€ä»¥ã€‚æˆ‘è®°å¾—ï¼Œcoababä¸Šçš„äº‘TPUåªæ˜¯ä¸€ä¸ªTPUèŠ¯ç‰‡ã€‚å› æ­¤ï¼Œå¤§è‡´æ¥è¯´ã€‚
- en: it would take you maybe 16 daysï¼Œ16 times 4ã€‚ So 64ã€‚Days to train on curtLã€‚You
    knowã€‚ with the same corpusã€‚å—¯ã€‚ğŸ˜Šï¼ŒButã€‚I don't think soã€‚ Yeahï¼Œ I'm not sure if there's
    a quick Bt trainingã€‚ Howeverï¼Œ I will show you somethingã€‚![](img/40873acb06abf924ac4a43fae802679a_29.png)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½éœ€è¦ä½ å¤§çº¦16å¤©ï¼Œ16ä¹˜ä»¥4ã€‚æ‰€ä»¥64å¤©ï¼Œåœ¨curtLä¸Šè®­ç»ƒã€‚ä½ çŸ¥é“çš„ã€‚å—¯ã€‚ğŸ˜Šä½†æ˜¯ã€‚æˆ‘ä¸è¿™ä¹ˆè®¤ä¸ºã€‚æ˜¯çš„ï¼Œæˆ‘ä¸ç¡®å®šæ˜¯å¦æœ‰å¿«é€Ÿçš„Btè®­ç»ƒã€‚ä¸è¿‡ï¼Œæˆ‘ä¼šç»™ä½ å±•ç¤ºä¸€äº›ä¸œè¥¿ã€‚![](img/40873acb06abf924ac4a43fae802679a_29.png)
- en: There's a blog post by Huging faceã€‚Let's see on trainingã€‚![](img/40873acb06abf924ac4a43fae802679a_31.png)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Faceæœ‰ä¸€ç¯‡å…³äºè®­ç»ƒçš„åšå®¢æ–‡ç« ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚![](img/40873acb06abf924ac4a43fae802679a_31.png)
- en: A model on Esperantoï¼Œ so I'll chuck this in the chatã€‚Soã€‚Can I do thatï¼ŸOkayï¼Œ
    so this blog postã€‚ it uses a slightly older APIï¼Œ but the basic idea is to show
    you that you actually can train in a coab a BERT model as long as your corpus
    isn't too bigã€‚ so this is Estoranto which is a special language that is you know
    has much less text than Englishã€‚But I think from memoryï¼Œ this was trainedã€‚In just
    an hour and a halfï¼Œ maybe a few hoursã€‚So let's seeã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…³äºä¸–ç•Œè¯­çš„æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘æŠŠè¿™ä¸ªæ‰”è¿›èŠå¤©ä¸­ã€‚é‚£ä¹ˆã€‚æˆ‘å¯ä»¥è¿™æ ·åšå—ï¼Ÿå¥½çš„ï¼Œè¿™ç¯‡åšå®¢æ–‡ç« ã€‚å®ƒä½¿ç”¨äº†ç¨æ—§çš„APIï¼Œä½†åŸºæœ¬çš„æƒ³æ³•æ˜¯å‘Šè¯‰ä½ ï¼Œåªè¦ä½ çš„è¯­æ–™åº“ä¸æ˜¯å¤ªå¤§ï¼Œä½ å®é™…ä¸Šå¯ä»¥åœ¨coabä¸Šè®­ç»ƒä¸€ä¸ªBERTæ¨¡å‹ã€‚æ‰€ä»¥è¿™æ˜¯ä¸–ç•Œè¯­ï¼Œå®ƒæ˜¯ä¸€ç§ç‰¹æ®Šçš„è¯­è¨€ï¼Œæ–‡æœ¬é‡è¿œå°‘äºè‹±è¯­ã€‚ä½†æˆ‘è®°å¾—ï¼Œè¿™æ˜¯åœ¨ä¸€ä¸ªåŠå°æ—¶å†…è®­ç»ƒå®Œæˆçš„ï¼Œä¹Ÿè®¸å‡ ä¸ªå°æ—¶ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹ã€‚
- en: ã¡ã¦ã€‚Okayï¼Œ maybe we don't see it hereã€‚ We just have to look at the coabã€‚å—¯ã€‚Let's
    seeã€‚ So the training of this modelã€‚Okayã€‚So yeahï¼Œ this training tookã€‚Almost three
    hoursã€‚So it really kind of depends on the size of your corpusï¼Œ so in principle
    you canã€‚ but if you want to do something that's like say as powerful as Bertã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ã¡ã¦ã€‚å¥½çš„ï¼Œä¹Ÿè®¸æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹ä¸åˆ°ã€‚æˆ‘ä»¬åªéœ€æŸ¥çœ‹coabã€‚å—¯ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚é‚£ä¹ˆè¿™ä¸ªæ¨¡å‹çš„è®­ç»ƒã€‚å¥½çš„ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œè¿™æ¬¡è®­ç»ƒèŠ±äº†å°†è¿‘ä¸‰ä¸ªå°æ—¶ã€‚æ‰€ä»¥è¿™çœŸçš„å–å†³äºä½ çš„è¯­æ–™åº“çš„å¤§å°ï¼Œæ‰€ä»¥åŸåˆ™ä¸Šä½ å¯ä»¥ã€‚ä½†å¦‚æœä½ æƒ³åšä¸€äº›åƒBerté‚£ä¹ˆå¼ºå¤§çš„äº‹æƒ…ã€‚
- en: then you're going to need some some more serious hardwareã€‚Okayã€‚å—¯ã€‚So there's
    another question by I am homess I understand that transfer learning or using a
    pretrain model is the way to go instead Yesã€‚ that's exactly right so the sort
    of real power of like transformers and NLP sort of nowadays in general is that
    we don't really want to do pretraining ourselves because again it's expensive
    and time and takes a long time so I would almost always use a pretrain model if
    I canã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä½ å°†éœ€è¦ä¸€äº›æ›´å¼ºå¤§çš„ç¡¬ä»¶ã€‚å¥½çš„ã€‚å—¯ã€‚è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘æ˜ç™½è¿ç§»å­¦ä¹ æˆ–ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œæ˜¯çš„ã€‚è¿™æ­£æ˜¯æ­£ç¡®çš„ï¼Œæ‰€ä»¥åƒå¦‚ä»Šçš„å˜å‹å™¨å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„çœŸæ­£åŠ›é‡åœ¨äºï¼Œæˆ‘ä»¬å¹¶ä¸æƒ³è‡ªå·±è¿›è¡Œé¢„è®­ç»ƒï¼Œå› ä¸ºè¿™æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œå› æ­¤æˆ‘å‡ ä¹æ€»æ˜¯ä¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚æœå¯ä»¥çš„è¯ã€‚
- en: The only time you might really be stuck is if you're dealing with like a domain
    that's very different from any pretm model that existsã€‚For exampleï¼Œ suppose I
    was trying to train a model on like source codeã€‚So you knowã€‚ in the early days
    of transformers that there weren't any pre-trained models on source codeã€‚ like
    you know trying to for exampleï¼Œ understand Python the language and so then you
    know using BERT base like on English and then trying to transfer to source code
    might be a bit tricky it might not give you very good results and so if you you
    trained on a source code corpus that would give you better resultsã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çœŸæ­£å¯èƒ½ä¼šé™·å…¥å›°å¢ƒçš„å”¯ä¸€æ—¶åˆ»æ˜¯å½“ä½ å¤„ç†çš„é¢†åŸŸä¸ä»»ä½•ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹éå¸¸ä¸åŒçš„æ—¶å€™ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘è¯•å›¾åœ¨æºä»£ç ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä½ çŸ¥é“ï¼Œåœ¨è½¬æ¢å™¨çš„æ—©æœŸé˜¶æ®µï¼Œæ²¡æœ‰ä»»ä½•é¢„è®­ç»ƒçš„æºä»£ç æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œç†è§£Pythonè¯­è¨€ï¼Œé‚£ä¹ˆä½ çŸ¥é“åœ¨è‹±æ–‡çš„BERTåŸºç¡€ä¸Šè¿›è¡Œè½¬ç§»åˆ°æºä»£ç å¯èƒ½ä¼šæœ‰ç‚¹æ£˜æ‰‹ï¼Œå¯èƒ½ä¸ä¼šç»™ä½ éå¸¸å¥½çš„ç»“æœï¼Œå› æ­¤å¦‚æœä½ åœ¨æºä»£ç è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œä¼šå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚
- en: And the other example where you generally need to find an alternative is if
    you're dealing with like a language that is not one of the sort of commonly supported
    onesã€‚ so my understanding is that there's like many languages for exampleï¼Œ in
    Africaã€‚ which aren't really represent it highly in Wikipedia and so then this
    is hard for people to train models or train transformers on and then you typically
    need to do some sort of tricks to like take something that is like multilingual
    like a multilingual version of BERT and try to somehow adapt it to your language
    but these are generally more advanced things that we can talk about laterã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé€šå¸¸éœ€è¦å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆçš„ä¾‹å­æ˜¯å½“ä½ å¤„ç†ä¸€ç§ä¸å¸¸è§çš„è¯­è¨€æ—¶ã€‚ä¾‹å¦‚ï¼Œæˆ‘äº†è§£åˆ°éæ´²æœ‰è®¸å¤šè¯­è¨€åœ¨ç»´åŸºç™¾ç§‘ä¸Šå¹¶æ²¡æœ‰å¾—åˆ°å¾ˆé«˜çš„ä»£è¡¨æ€§ï¼Œå› æ­¤è¿™è®©äººä»¬è®­ç»ƒæ¨¡å‹æˆ–è½¬æ¢å™¨æ—¶å˜å¾—å›°éš¾ï¼Œé€šå¸¸éœ€è¦ä¸€äº›æŠ€å·§ï¼Œæ¯”å¦‚ä½¿ç”¨å¤šè¯­è¨€çš„BERTç‰ˆæœ¬ï¼Œè¯•å›¾ä»¥æŸç§æ–¹å¼å°†å…¶é€‚é…åˆ°ä½ çš„è¯­è¨€ï¼Œä½†è¿™äº›é€šå¸¸æ˜¯æ›´é«˜çº§çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥ç¨åè®¨è®ºã€‚
- en: Okayã€‚Soã€‚Let's seeã€‚ So where were we We have looked at how we can another questionã€‚
    can we change the config parameters of a pretrain model and use itï¼Ÿè¯¶ã€‚Yesï¼Œ but
    with some caveatsã€‚Soã€‚For exampleã€‚Let's think about what can we change and what
    can't we changeï¼ŸSoã€‚å—¯ã€‚I want to make sure I don't say something sillyã€‚Let's have
    a look at the model config we have hereã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚é‚£ä¹ˆã€‚æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚é‚£ä¹ˆæˆ‘ä»¬åˆ°å“ªå„¿äº†ï¼Ÿæˆ‘ä»¬å·²ç»çœ‹è¿‡äº†å¦ä¸€ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥æ›´æ”¹é¢„è®­ç»ƒæ¨¡å‹çš„é…ç½®å‚æ•°å¹¶ä½¿ç”¨å®ƒå—ï¼Ÿè¯¶ã€‚å¯ä»¥ï¼Œä½†æœ‰ä¸€äº›æ³¨æ„äº‹é¡¹ã€‚æ‰€ä»¥ã€‚æ¯”å¦‚è¯´ã€‚è®©æˆ‘ä»¬æƒ³æƒ³æˆ‘ä»¬èƒ½æ”¹å˜ä»€ä¹ˆï¼Œä¸èƒ½æ”¹å˜ä»€ä¹ˆï¼Ÿå—¯ã€‚æˆ‘æƒ³ç¡®ä¿æˆ‘ä¸ä¼šè¯´ä¸€äº›æ„šè ¢çš„è¯ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬è¿™é‡Œçš„æ¨¡å‹é…ç½®ã€‚
- en: So this is theã€‚This is the config associated with Bert Batesã€‚And here you can
    see that there's a bunch of hyperparametersã€‚That were associated with the pre
    training of this modelã€‚Soã€‚For exampleã€‚Let's seeã€‚So I have a suspicion that if
    we change many of these thingsã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸Bert Batesç›¸å…³çš„é…ç½®ã€‚è¿™é‡Œå¯ä»¥çœ‹åˆ°ä¸€å †ä¸è¯¥æ¨¡å‹é¢„è®­ç»ƒç›¸å…³çš„è¶…å‚æ•°ã€‚æ‰€ä»¥ã€‚ä¾‹å¦‚ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æ‰€ä»¥æˆ‘æ€€ç–‘å¦‚æœæˆ‘ä»¬æ›´æ”¹è®¸å¤šè¿™äº›ä¸œè¥¿ã€‚
- en: we're going to break the model in a non trivial wayã€‚Howeverã€‚Let me thinkã€‚What
    happens if we change the number of hidden layersï¼ŸğŸ˜”ï¼ŒSo you know whatã€‚ let's try
    the usual way of doing things in deep learning is just to tryã€‚Soã€‚I'm going to
    try to changeã€‚ Soï¼Œ so Bt has a number of attention headsã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»¥ä¸€ç§éå¹³å‡¡çš„æ–¹å¼æ‰“ç ´æ¨¡å‹ã€‚ä¸è¿‡ï¼Œè®©æˆ‘æƒ³æƒ³ã€‚å¦‚æœæˆ‘ä»¬æ”¹å˜éšè—å±‚çš„æ•°é‡ä¼šå‘ç”Ÿä»€ä¹ˆï¼ŸğŸ˜”ï¼Œæ‰€ä»¥ä½ çŸ¥é“å—ï¼Ÿè®©æˆ‘ä»¬å°è¯•ä¸€ä¸‹æ·±åº¦å­¦ä¹ ä¸­çš„å¸¸è§„åšæ³•ï¼Œå°±æ˜¯è¯•è¯•ã€‚å› æ­¤ï¼Œæˆ‘æ‰“ç®—è¿›è¡Œæ›´æ”¹ã€‚Btæœ‰å¤šä¸ªæ³¨æ„åŠ›å¤´ã€‚
- en: so I'm going to see what happensã€‚If I reduce the number of tension heads from
    12 to6ã€‚Let's seeã€‚If this worksã€‚So let's have a look at the config to make sure
    that workedã€‚So now we've got attention heads sixã€‚Nowï¼Œ what happens if we try to
    feed some inputs to this modelï¼Ÿ
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³çœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å¦‚æœæˆ‘å°†æ³¨æ„åŠ›å¤´çš„æ•°é‡ä»12å‡å°‘åˆ°6ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚å¦‚æœè¿™èƒ½å·¥ä½œã€‚è®©æˆ‘ä»¬æŸ¥çœ‹é…ç½®ï¼Œç¡®ä¿æ›´æ”¹æˆåŠŸã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰å…­ä¸ªæ³¨æ„åŠ›å¤´ã€‚é‚£ä¹ˆï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ç»™è¿™ä¸ªæ¨¡å‹è¾“å…¥ä¸€äº›å†…å®¹ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿ
- en: Okayã€‚ğŸ˜Šï¼ŒOkayï¼Œ soã€‚Interestingã€‚Okayï¼Œ soã€‚It seems that we can change the config
    and things work in the sense that we don't get errorsã€‚But I have a suspicion that
    like hacking into this in a pretrain model would affect the the kind of performance
    in some non trivial way becauseã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚ğŸ˜Šï¼Œå¥½çš„ï¼Œæ‰€ä»¥ã€‚æœ‰è¶£ã€‚å¥½çš„ï¼Œæ‰€ä»¥ã€‚ä¼¼ä¹æˆ‘ä»¬å¯ä»¥æ›´æ”¹é…ç½®ï¼Œä¸”ä¸€åˆ‡éƒ½èƒ½æ­£å¸¸å·¥ä½œï¼Œæ„å‘³ç€æ²¡æœ‰é”™è¯¯ã€‚ä½†æˆ‘æ€€ç–‘åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­è¿›è¡Œè¿™ç§é»‘å®¢è¡Œä¸ºä¼šä»¥æŸç§éå¹³å‡¡çš„æ–¹å¼å½±å“æ€§èƒ½ï¼Œå› ä¸ºã€‚
- en: If we think about like what happens when we do something like text classificationã€‚
    we're taking the whole like base model of BRT and then we're just stacking on
    top of this the classification headã€‚And if I start kind of like you knowï¼Œ doing
    an like dissecting Bt into pieces or somethingï¼Œ you knowã€‚ reducing the attention
    heads or changing the number of transformer layersã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ€è€ƒä¸€ä¸‹åšæ–‡æœ¬åˆ†ç±»æ—¶å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæˆ‘ä»¬å°±ä¼šæŠŠæ•´ä¸ªBRTåŸºç¡€æ¨¡å‹æ‹¿æ¥ï¼Œç„¶ååœ¨ä¸Šé¢å åŠ åˆ†ç±»å¤´ã€‚å¦‚æœæˆ‘å¼€å§‹åˆ†è§£Btæˆä¸åŒçš„éƒ¨åˆ†ï¼Œå‡å°‘æ³¨æ„åŠ›å¤´æˆ–æ”¹å˜å˜å‹å™¨å±‚çš„æ•°é‡ã€‚
- en: so Bert has 12 encoder layersã€‚I have a suspicion that I would probably have
    some sort of non trivial or negative impact on the downstream taskã€‚ like classification
    that I want to fine tune onã€‚But maybe Omar has a different insight hereã€‚Okayã€‚Soirã€‚That's
    a good question I've actually never hacked into a pre trend model this wayã€‚Maybe
    you could try and see you like do some experiments like what happens if I completely
    change the number of layers the number of attention heads invert and to try to
    do classification like sentiment analysisã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥Bertæœ‰12ä¸ªç¼–ç å±‚ã€‚æˆ‘æ€€ç–‘è¿™å¯èƒ½ä¼šå¯¹æˆ‘æƒ³è¦å¾®è°ƒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰äº§ç”Ÿä¸€äº›éå¹³å‡¡æˆ–è´Ÿé¢çš„å½±å“ã€‚ä½†ä¹Ÿè®¸Omaråœ¨è¿™æ–¹é¢æœ‰ä¸åŒçš„è§è§£ã€‚å¥½çš„ã€‚è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œæˆ‘å®é™…ä¸Šä»æœªä»¥è¿™ç§æ–¹å¼ç ´è§£è¿‡é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¹Ÿè®¸ä½ å¯ä»¥è¯•è¯•çœ‹ï¼Œåšä¸€äº›å®éªŒï¼Œæ¯”å¦‚å¦‚æœæˆ‘å®Œå…¨æ”¹å˜å±‚çš„æ•°é‡ã€æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œå°è¯•è¿›è¡Œåˆ†ç±»ï¼Œæ¯”å¦‚æƒ…æ„Ÿåˆ†æï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚
- en: do I get better or worse performanceï¼ŸI have a feeling it would be worseã€‚ but
    it'd be a cool thing to check and if you do check please share it on the forumã€‚Okayã€‚So
    that was the look at sort of how we generate predictionsã€‚ let's now have a look
    at the tokenizersã€‚In more detailã€‚Soã€‚Let'sã€‚Ccroros our fingers that the Internet
    still worksã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ€§èƒ½ä¼šæ›´å¥½è¿˜æ˜¯æ›´å·®ï¼Ÿæˆ‘æ„Ÿè§‰å¯èƒ½ä¼šæ›´å·®ï¼Œä½†è¿™ç¡®å®æ˜¯ä¸ªå¾ˆé…·çš„äº‹æƒ…ã€‚å¦‚æœä½ æ£€æŸ¥äº†ï¼Œè¯·åœ¨è®ºå›ä¸Šåˆ†äº«ã€‚å¥½çš„ã€‚è¿™æ˜¯æˆ‘ä»¬å¦‚ä½•ç”Ÿæˆé¢„æµ‹çš„å›é¡¾ã€‚ç°åœ¨è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°çœ‹çœ‹åˆ†è¯å™¨ã€‚å¸Œæœ›äº’è”ç½‘ä»ç„¶æœ‰æ•ˆã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_33.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_33.png)'
- en: Okayã€‚![](img/40873acb06abf924ac4a43fae802679a_35.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚![](img/40873acb06abf924ac4a43fae802679a_35.png)
- en: In the next few videosï¼Œ we'll take a look at the tokensã€‚In natural language
    processingã€‚ most of the data that we handle consists of raw textã€‚ Howeverã€‚ machine
    learning models cannot read or understand text in its raw formã€‚They can only work
    with numbersã€‚So the tokenizer objective will be to translate the text into numbersã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„è§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å…³æ³¨ä»¤ç‰Œã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¤„ç†çš„å¤§å¤šæ•°æ•°æ®ç”±åŸå§‹æ–‡æœ¬ç»„æˆã€‚ç„¶è€Œï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ— æ³•ä»¥åŸå§‹å½¢å¼è¯»å–æˆ–ç†è§£æ–‡æœ¬ã€‚å®ƒä»¬åªèƒ½å¤„ç†æ•°å­—ã€‚å› æ­¤ï¼Œåˆ†è¯å™¨çš„ç›®æ ‡æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ã€‚
- en: There are several possible approaches to this conversionã€‚ and the objective
    is to find the most meaningful representationã€‚We'll take a look at three distinct
    organizationization algorithmsã€‚ We compare them one to oneã€‚ So we recommend you
    take a look at the videos in the following orderã€‚ firstï¼Œ word basedã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§å¯èƒ½çš„è½¬æ¢æ–¹æ³•ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°æœ€æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚æˆ‘ä»¬å°†æŸ¥çœ‹ä¸‰ç§ä¸åŒçš„ç»„ç»‡ç®—æ³•ï¼Œé€ä¸€æ¯”è¾ƒã€‚æˆ‘ä»¬å»ºè®®ä½ æŒ‰ä»¥ä¸‹é¡ºåºè§‚çœ‹è§†é¢‘ï¼Œé¦–å…ˆæ˜¯åŸºäºè¯çš„ã€‚
- en: followed by character basedï¼Œ and finallyï¼Œ sub word basedã€‚ğŸ˜Šã€‚![](img/40873acb06abf924ac4a43fae802679a_37.png)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯åŸºäºå­—ç¬¦çš„ï¼Œæœ€åæ˜¯åŸºäºå­è¯çš„ã€‚ğŸ˜Šã€‚![](img/40873acb06abf924ac4a43fae802679a_37.png)
- en: Yeahã€‚![](img/40873acb06abf924ac4a43fae802679a_39.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚![](img/40873acb06abf924ac4a43fae802679a_39.png)
- en: å—¯ã€‚Okayï¼Œ soã€‚ã¤ã‘ã‚‰ã‚Œã§ã™ã€‚Okay so that was like a high level overview of what we're
    been talking about that there this general process we have to go through of converting
    text into into numbersã€‚There's a bunch of videos in this section that you can
    look atã€‚ which show the different types of ways you can tokenize textã€‚å—¯ã€‚ğŸ˜Šï¼Œam I
    all rightã€‚ can you guys see me or notï¼ŸCan youã€‚Okayï¼Œ goodï¼Œ greatã€‚Oh goodã€‚Yeahã€‚ğŸ˜Šï¼ŒThe
    joys of Home officeï¼Œ okayã€‚ğŸ˜Šã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œå¥½å§ï¼Œæ‰€ä»¥ã€‚è¿™æ˜¯æˆ‘ä»¬è®¨è®ºçš„é«˜å±‚æ¬¡æ¦‚è¿°ï¼Œå…³äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—çš„è¿‡ç¨‹ã€‚è¿™ä¸€éƒ¨åˆ†æœ‰è®¸å¤šè§†é¢‘å±•ç¤ºä¸åŒçš„åˆ†è¯æ–¹å¼ã€‚å—¯ã€‚ğŸ˜Šï¼Œæˆ‘æ²¡é—®é¢˜å§ã€‚ä½ ä»¬èƒ½çœ‹åˆ°æˆ‘å—ï¼Ÿå¯ä»¥å—ã€‚å¥½çš„ï¼Œå¾ˆå¥½ã€‚å“¦ï¼Œå¤ªå¥½äº†ã€‚æ˜¯çš„ã€‚ğŸ˜Šï¼Œåœ¨å®¶åŠå…¬çš„ä¹è¶£ï¼Œå¥½çš„ã€‚ğŸ˜Šã€‚
- en: What I was saying is there are different approaches or strategies you can take
    for tokenizing text and the advantages and disadvantages of them just depend on
    the application you're interested inã€‚So I'm not going to go through the videosï¼Œ
    you can watch these yourselvesã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ‰€è¯´çš„æ˜¯ï¼Œæœ‰ä¸åŒçš„åˆ†è¯ç­–ç•¥ï¼Œä¼˜ç¼ºç‚¹å–å†³äºä½ æ„Ÿå…´è¶£çš„åº”ç”¨ã€‚æ‰€ä»¥æˆ‘ä¸ä¼šé€ä¸ªè§†é¢‘è®²è§£ï¼Œä½ ä»¬å¯ä»¥è‡ªå·±è§‚çœ‹ã€‚
- en: but let's just have a quick look at the sort of three most popular approachesã€‚So
    the sort of first thing I might imagine is if I got like a text like Jim Henson
    was a puppeteerã€‚Then what I might do is sayï¼Œ okayï¼Œ I just want to split this text
    into wordsã€‚And in Englishã€‚ a simple like trick to do that is just to split on
    white spaceã€‚ So most of the time in Englishã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è®©æˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹ä¸‰ç§æœ€æµè¡Œçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘å¯èƒ½ä¼šæƒ³è±¡ï¼Œå¦‚æœæˆ‘æœ‰ä¸€æ®µæ–‡æœ¬ï¼Œæ¯”å¦‚â€œå‰å§†Â·äº¨æ£®æ˜¯ä¸€ä¸ªæœ¨å¶å¸ˆâ€ã€‚ç„¶åæˆ‘å¯èƒ½ä¼šè¯´ï¼Œå¥½å§ï¼Œæˆ‘åªæƒ³æŠŠè¿™æ®µæ–‡æœ¬æ‹†åˆ†æˆå•è¯ã€‚åœ¨è‹±è¯­ä¸­ï¼Œç®€å•çš„æ–¹æ³•å°±æ˜¯åœ¨ç©ºæ ¼ä¸Šæ‹†åˆ†ã€‚å› æ­¤ï¼Œåœ¨è‹±è¯­ä¸­å¤§å¤šæ•°æ—¶å€™ã€‚
- en: if there's a white spaceï¼Œ that's the boundary between wordsã€‚And then this would
    convertã€‚ for exampleï¼Œ Jim Henson was a puppeteer into these five tokensï¼Œ so in
    this case a word is a tokenã€‚But there are like several languages where this is
    like a terrible ideaï¼Œ so for exampleã€‚ if you have ever learnt Japanese you have
    characters called Kanji and these kanji don't have any words for any space it's
    just a sequence of Kanji and in general they're actually not even written from
    left to right they're written from top to bottomã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰ç©ºæ ¼ï¼Œé‚£å°±æ˜¯å•è¯ä¹‹é—´çš„è¾¹ç•Œã€‚ç„¶åè¿™ä¼šè½¬æ¢ã€‚ä¾‹å¦‚ï¼Œå‰å§†Â·äº¨æ£®æ˜¯ä¸€ä¸ªæœ¨å¶å¸ˆï¼Œå°†å…¶åˆ†ä¸ºè¿™äº”ä¸ªæ ‡è®°ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå•è¯å°±æ˜¯ä¸€ä¸ªæ ‡è®°ã€‚ä½†æ˜¯åœ¨å‡ ç§è¯­è¨€ä¸­ï¼Œè¿™ç§æ–¹æ³•æ˜¯ä¸ªç³Ÿç³•çš„ä¸»æ„ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ å­¦ä¹ è¿‡æ—¥è¯­ï¼Œä½ ä¼šå‘ç°æ±‰å­—æ²¡æœ‰ç©ºæ ¼çš„ä»»ä½•å•è¯ï¼Œåªæ˜¯ä¸€ç³»åˆ—çš„æ±‰å­—ï¼Œå¹¶ä¸”é€šå¸¸æ˜¯ä»ä¸Šåˆ°ä¸‹ä¹¦å†™ï¼Œè€Œä¸æ˜¯ä»å·¦åˆ°å³ã€‚
- en: So doing this kind of splitting or tokenization in terms of white space just
    wouldn't workã€‚And so an alternative approach is to try something called character
    basedã€‚ so this would be like imagine you just split every letter in an English
    sequence into its own tokenã€‚ and this would actually be then quite good for Japanese
    because every character is a kanji character which then you know we could represent
    with a tokenã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ç§åŸºäºç©ºæ ¼çš„æ‹†åˆ†æˆ–æ ‡è®°åŒ–æ ¹æœ¬æ— æ³•å®ç°ã€‚å¦ä¸€ç§æ›¿ä»£æ–¹æ³•æ˜¯å°è¯•å­—ç¬¦åŸºç¡€çš„æ–¹æ³•ã€‚æ‰€ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œä½ å°†è‹±æ–‡åºåˆ—ä¸­çš„æ¯ä¸ªå­—æ¯æ‹†åˆ†æˆè‡ªå·±çš„æ ‡è®°ã€‚è¿™æ ·åšå¯¹æ—¥è¯­ä¹Ÿå¾ˆæœ‰æ•ˆï¼Œå› ä¸ºæ¯ä¸ªå­—ç¬¦éƒ½æ˜¯ä¸€ä¸ªæ±‰å­—ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ ‡è®°æ¥è¡¨ç¤ºã€‚
- en: And so the kind of thing you can see here is that it really the sort of tokenization
    strategy seems to really depend on the language that we're studyingã€‚And soã€‚The
    thing that like a lot of research has gone into is trying to find something that
    gives you like a good trade off between these two kind of extremes of word tokenization
    and character tokenizationã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°çš„äº‹æƒ…æ˜¯ï¼Œæ ‡è®°åŒ–ç­–ç•¥ç¡®å®ä¼¼ä¹ä¾èµ–äºæˆ‘ä»¬ç ”ç©¶çš„è¯­è¨€ã€‚å› æ­¤ï¼Œè®¸å¤šç ”ç©¶è‡´åŠ›äºå¯»æ‰¾ä¸€ç§åœ¨å•è¯æ ‡è®°åŒ–å’Œå­—ç¬¦æ ‡è®°åŒ–è¿™ä¸¤ç§æç«¯ä¹‹é—´æä¾›è‰¯å¥½æŠ˜è¡·çš„æ–¹æ³•ã€‚
- en: And maybe I should also mention a couple of drawbacks before we go into thatã€‚
    so one of the drawbacks with word tokenizationã€‚Is that this will create a vocabulary
    which is the size of the number of words in our languageã€‚ so basically if we have
    imagine we just tokenize Englishã€‚ then we will need a token for every single word
    in the English language and this sum this is generally huge it's going to be several
    hundred thousand tokens which makes it very like computationally expensiveã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ·±å…¥è®¨è®ºä¹‹å‰ï¼Œæˆ‘è¿˜åº”è¯¥æåˆ°å‡ ä¸ªç¼ºç‚¹ã€‚å› æ­¤ï¼ŒåŸºäºå•è¯æ ‡è®°åŒ–çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œè¿™å°†åˆ›å»ºä¸€ä¸ªè¯æ±‡è¡¨ï¼Œå…¶å¤§å°ç­‰äºæˆ‘ä»¬è¯­è¨€ä¸­çš„å•è¯æ•°é‡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾åªæ ‡è®°è‹±è¯­ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªè‹±è¯­å•è¯åˆ›å»ºä¸€ä¸ªæ ‡è®°ï¼Œè¿™é€šå¸¸æ˜¯å‡ åä¸‡ä¸ªæ ‡è®°ï¼Œè¿™åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µã€‚
- en: But the other thing that's kind of not great about this is that it doesn't make
    any sort of distinction between like like I don't knowã€‚ dog and dogsï¼Œ which are
    kind of likeï¼Œ you knowï¼Œ similar words and we're kind of representing them now
    with two independent tokensã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦ä¸€ä¸ªä¸å¤ªç†æƒ³çš„åœ°æ–¹æ˜¯ï¼Œå®ƒæ²¡æœ‰åŒºåˆ†ç±»ä¼¼çš„è¯ï¼Œæ¯”å¦‚ç‹—å’Œç‹—ç‹—ï¼Œè¿™äº›è¯æ˜¯ç›¸ä¼¼çš„ï¼Œè€Œæˆ‘ä»¬ç°åœ¨å°†å®ƒä»¬è¡¨ç¤ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„æ ‡è®°ã€‚
- en: So that's the drawback with the word ones and the character based ones have
    the drawback that the model has to basically learn what a word actually means
    because the only thing it gets now characters or it gets character tokensã€‚And
    then it has to figure out over training thatï¼Œ okayã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åŸºäºå•è¯çš„æ–¹æ³•çš„ç¼ºç‚¹ï¼Œè€ŒåŸºäºå­—ç¬¦çš„æ–¹æ³•çš„ç¼ºç‚¹æ˜¯æ¨¡å‹å¿…é¡»å­¦ä¹ ä¸€ä¸ªå•è¯å®é™…æ„å‘³ç€ä»€ä¹ˆï¼Œå› ä¸ºå®ƒå¾—åˆ°çš„åªæœ‰å­—ç¬¦æˆ–å­—ç¬¦æ ‡è®°ã€‚ç„¶åï¼Œå®ƒå¿…é¡»é€šè¿‡è®­ç»ƒå¼„æ¸…æ¥šï¼Œå¥½çš„ã€‚
- en: if I put together these characters in this orderï¼Œ this seems to represent like
    a more abstract object like a wordã€‚ and so this at least for English would be
    not a great strategyã€‚So most tokenizersã€‚ they use something called subword tokenizationã€‚And
    the basic idea is that instead of like just splitting on word boundaries or on
    charactersã€‚ you basically split or you decompose a word into sub wordssã€‚And an
    example here is likeã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘å°†è¿™äº›å­—ç¬¦æŒ‰è¿™ä¸ªé¡ºåºç»„åˆèµ·æ¥ï¼Œè¿™ä¼¼ä¹ä»£è¡¨äº†ä¸€ä¸ªæ›´æŠ½è±¡çš„å¯¹è±¡ï¼Œæ¯”å¦‚ä¸€ä¸ªå•è¯ã€‚å› æ­¤ï¼Œè‡³å°‘å¯¹äºè‹±è¯­æ¥è¯´ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½çš„ç­–ç•¥ã€‚å¤§å¤šæ•°åˆ†è¯å™¨ä½¿ç”¨ä¸€ç§ç§°ä¸ºå­è¯åˆ†è¯çš„æ–¹æ³•ã€‚åŸºæœ¬çš„æƒ³æ³•æ˜¯ï¼Œä¸ä»…ä»…åœ¨å•è¯è¾¹ç•Œæˆ–å­—ç¬¦ä¸Šè¿›è¡Œåˆ†å‰²ï¼Œè€Œæ˜¯å°†ä¸€ä¸ªå•è¯åˆ†è§£ä¸ºå­è¯ã€‚è¿™é‡Œçš„ä¸€ä¸ªä¾‹å­æ˜¯ã€‚
- en: let's take the word annoyinglyï¼Œ so annoyingly can be represented as maybe two
    sub wordssã€‚ annoying and Leeã€‚And then what we can do is we can just kind of collect
    the frequencies of these subwords and then use this to figure out basically what
    I like the most frequent subwords in the language and then we can use those sub
    wordss to build back the full word itselfã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»¥â€œannoyinglyâ€ä¸ºä¾‹ï¼Œâ€œannoyinglyâ€å¯ä»¥è¡¨ç¤ºä¸ºå¯èƒ½ä¸¤ä¸ªå­è¯ã€‚ â€œannoyingâ€å’Œâ€œlyâ€ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æ”¶é›†è¿™äº›å­è¯çš„é¢‘ç‡ï¼Œç„¶åç”¨è¿™äº›é¢‘ç‡æ¥åŸºæœ¬ä¸Šæ‰¾å‡ºè¯­è¨€ä¸­æœ€å¸¸ç”¨çš„å­è¯ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ç”¨è¿™äº›å­è¯é‡æ–°æ„å»ºå®Œæ•´çš„å•è¯ã€‚
- en: so if you know that you've got annoying and Leeï¼Œ you can then reconstruct annoyingly
    from these two componentsã€‚And so like I guess there's an example hereï¼Œ you can
    sort of splitã€‚ let's do tokenization into these subwords so that you can see this
    is kind of a mix of a wordtoken with a subwordizationã€‚ and we've also got the
    exclamation mark being treated as its own separate tokenã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ çŸ¥é“ä½ æœ‰â€œannoyingâ€å’Œâ€œlyâ€ï¼Œé‚£ä¹ˆä½ å¯ä»¥ä»è¿™ä¸¤ä¸ªç»„æˆéƒ¨åˆ†é‡æ–°æ„å»ºâ€œannoyinglyâ€ã€‚æˆ‘æƒ³è¿™é‡Œæœ‰ä¸ªä¾‹å­ï¼Œä½ å¯ä»¥å°†åˆ†è¯åˆ†æˆè¿™äº›å­è¯ï¼Œè¿™æ ·ä½ å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€ç§å•è¯ä¸å­è¯æ··åˆçš„æƒ…å†µã€‚è€Œä¸”æˆ‘ä»¬è¿˜å°†æ„Ÿå¹å·è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„æ ‡è®°ã€‚
- en: And the sort of most common tokenizers that you would seeï¼Œ there's a good questionã€‚
    I'll get to that are things called word peaceï¼Œ which is the one that Bert used
    or sentence pieceã€‚ which is the one that GT and the GPT models are typically useã€‚So
    there's a really good questionã€‚ how do you design the subweb boundariesï¼Œ is it
    manualï¼Ÿ
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šçœ‹åˆ°çš„æœ€å¸¸è§çš„åˆ†è¯å™¨ï¼Œå¤§æ¦‚æœ‰ä¸€ä¸ªå¥½é—®é¢˜ã€‚æˆ‘ä¼šæåˆ°çš„æœ‰ç§°ä¸º word piece çš„æ–¹æ³•ï¼Œè¿™æ˜¯ Bert ä½¿ç”¨çš„ï¼Œæˆ–è€…æ˜¯ sentence pieceï¼Œè¿™æ˜¯
    GT å’Œ GPT æ¨¡å‹é€šå¸¸ä½¿ç”¨çš„ã€‚æ‰€ä»¥è¿™æ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚ä½ å¦‚ä½•è®¾è®¡å­è¯è¾¹ç•Œï¼Œæ˜¯æ‰‹åŠ¨çš„å—ï¼Ÿ
- en: So this is more or less determined by the algorithm that you choose to useã€‚I
    thinkã€‚Like in generalã€‚ it's a mix of like manual rules and also learning a form
    of learning from the corpusã€‚ So let's have a quick look atã€‚![](img/40873acb06abf924ac4a43fae802679a_41.png)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ç”±ä½ é€‰æ‹©ä½¿ç”¨çš„ç®—æ³•å†³å®šçš„ã€‚æˆ‘è®¤ä¸ºã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ç§æ‰‹åŠ¨è§„åˆ™ä¸ä»è¯­æ–™åº“å­¦ä¹ çš„ç»“åˆã€‚æ‰€ä»¥æˆ‘ä»¬å¿«é€Ÿçœ‹ä¸€ä¸‹ã€‚![](img/40873acb06abf924ac4a43fae802679a_41.png)
- en: LetSeeï¼Œ I think it's the sentence piece paperã€‚![](img/40873acb06abf924ac4a43fae802679a_43.png)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘çœ‹çœ‹ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ sentence piece è®ºæ–‡ã€‚![](img/40873acb06abf924ac4a43fae802679a_43.png)
- en: è¿™è¿™è¿™ã€‚So this isï¼Œ I'm going to put this in the chatã€‚Okayã€‚ so this is one of the
    most famous papers on tokenizationã€‚And let's have a quick look atã€‚So how are these
    boundariesã€‚Okayã€‚å—¯ã€‚Yeahï¼Œ that's rightã€‚ğŸ˜Šï¼ŒThat's what I remember from this paperã€‚
    So they say that historicallyï¼Œ most like tokenizationã€‚Algorithmsã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¿™è¿™ã€‚æ‰€ä»¥è¿™æ˜¯ï¼Œæˆ‘è¦æŠŠè¿™ä¸ªæ”¾åœ¨èŠå¤©ä¸­ã€‚å¥½çš„ã€‚è¿™æ˜¯å…³äºåˆ†è¯çš„æœ€è‘—åè®ºæ–‡ä¹‹ä¸€ã€‚æˆ‘ä»¬å¿«é€Ÿçœ‹ä¸€ä¸‹ã€‚é‚£ä¹ˆè¿™äº›è¾¹ç•Œæ˜¯æ€æ ·çš„ã€‚å¥½çš„ã€‚å—¯ã€‚æ˜¯çš„ï¼Œæ²¡é”™ã€‚ğŸ˜Šè¿™æ˜¯æˆ‘ä»è¿™ç¯‡è®ºæ–‡ä¸­è®°å¾—çš„ã€‚æ‰€ä»¥ä»–ä»¬è¯´ï¼Œä»å†å²ä¸Šçœ‹ï¼Œå¤§å¤šæ•°åˆ†è¯ç®—æ³•ã€‚
- en: they were they used manual rules and the problem with thisï¼Œ of courseã€‚ is that
    for every language in your during set of rules and it's a real like pain toã€‚To
    sort of maintain and extendã€‚And so if I'm not mistakenã€‚ sentence piece is kind
    of like a learned tokenizerã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ä½¿ç”¨äº†æ‰‹åŠ¨è§„åˆ™ï¼Œè¿™ä¸ªé—®é¢˜å½“ç„¶åœ¨äºï¼Œå¯¹äºæ¯ç§è¯­è¨€ï¼Œä½ éƒ½éœ€è¦ä¸€å¥—è§„åˆ™ï¼Œè¿™çœŸçš„æ˜¯ä¸ªéº»çƒ¦äº‹ã€‚è¦ç»´æŠ¤å’Œæ‰©å±•è¿™äº›è§„åˆ™ã€‚å¦‚æœæˆ‘æ²¡è®°é”™çš„è¯ï¼Œsentence piece
    å°±åƒæ˜¯ä¸€ä¸ªå­¦ä¹ çš„åˆ†è¯å™¨ã€‚
- en: so you actually have like a sort of optimization objective and then you train
    this like you train a model and so by training this on your corpusã€‚ you actually
    learn the word boundariesã€‚å—¯ã€‚ğŸ˜Šï¼ŒBut I haven't read this for a few yearsã€‚ and I might
    be forgetting something butã€‚Yeahï¼Œ that's a good question and I think it may something
    that we can add in a future version of courseã€‚okã€‚ğŸ˜Šï¼ŒSoã€‚Where were weï¼Œ So we were
    looking at these different tokenization strategiesã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å®é™…ä¸Šæœ‰ä¸€ä¸ªä¼˜åŒ–ç›®æ ‡ï¼Œç„¶åä½ å°±åƒè®­ç»ƒæ¨¡å‹ä¸€æ ·è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œé€šè¿‡åœ¨ä½ çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½ å®é™…ä¸Šå­¦ä¼šäº†å•è¯çš„è¾¹ç•Œã€‚å—¯ã€‚ğŸ˜Šä½†æ˜¯æˆ‘å·²ç»å¥½å‡ å¹´æ²¡çœ‹è¿‡è¿™ä¸ªäº†ã€‚å¯èƒ½ä¼šå¿˜è®°ä¸€äº›ä¸œè¥¿ï¼Œä½†ã€‚æ˜¯çš„ï¼Œè¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œæˆ‘è®¤ä¸ºè¿™å¯èƒ½æ˜¯æˆ‘ä»¬å¯ä»¥åœ¨æœªæ¥ç‰ˆæœ¬ä¸­æ·»åŠ çš„å†…å®¹ã€‚å¥½çš„ã€‚ğŸ˜Šé‚£ä¹ˆã€‚æˆ‘ä»¬åœ¨å“ªå„¿ï¼Ÿæˆ‘ä»¬åœ¨çœ‹è¿™äº›ä¸åŒçš„åˆ†è¯ç­–ç•¥ã€‚
- en: So let's maybe look at the coabã€‚å—¯ã€‚Soã€‚One of the things I often like to do is
    to sort of capture the outputs in my piping stools on coLab so I don't have this
    humongousã€‚æ²¡ã€‚Of installationã€‚Okayï¼Œ so what you can see here is what we were talking
    about beforeã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿè®¸å¯ä»¥çœ‹çœ‹ CoLabã€‚å—¯ã€‚æ‰€ä»¥ã€‚æˆ‘ç»å¸¸å–œæ¬¢åšçš„ä¸€ä»¶äº‹æ˜¯æ•è·æˆ‘åœ¨ CoLab ä¸­çš„ç®¡é“å·¥å…·çš„è¾“å‡ºï¼Œè¿™æ ·æˆ‘å°±ä¸éœ€è¦æœ‰è¿™ä¸ªå·¨å¤§çš„ã€‚æ²¡ã€‚å®‰è£…çš„å†…å®¹ã€‚å¥½çš„ï¼Œæ‰€ä»¥ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„æ˜¯æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„å†…å®¹ã€‚
- en: this is if you just do word peace tokenization oh sorry word splitting into
    words and now we can have a look at like what the BRT tokenizer doesã€‚And there
    are two ways you can do this in transformersã€‚ you can specify the specific class
    that you want to use for the tokennovaã€‚And this is if you know happen to be maybe
    doing something very specific and you really you want to make sure you get the
    ver organizerã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦‚æœä½ åªæ˜¯è¿›è¡Œå•è¯çš„è¯å—åˆ†å‰²ï¼Œå“¦ï¼Œå¯¹ä¸èµ·ï¼Œæ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆå•è¯ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ BERT åˆ†è¯å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥åœ¨ transformers
    ä¸­åšåˆ°è¿™ä¸€ç‚¹ã€‚ä½ å¯ä»¥æŒ‡å®šä½ æƒ³è¦ä½¿ç”¨çš„å…·ä½“ç±»ç”¨äº tokennovaã€‚å¦‚æœä½ æ°å¥½åœ¨åšä¸€äº›éå¸¸ç‰¹å®šçš„äº‹æƒ…ï¼Œå¹¶ä¸”ä½ çœŸçš„æƒ³ç¡®ä¿ä½ è·å¾—äº†åˆé€‚çš„ç»„ç»‡è€…ã€‚
- en: But the thing that I personally use all the time is just the auto tokenizer
    because this will automatically convert the tokenizer into this class anywayã€‚
    so if I provide a checkpoint and it can identify thatï¼Œ it will then automatically
    load it this wayã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä¸ªäººä¸€ç›´ä½¿ç”¨çš„æ–¹å¼æ˜¯è‡ªåŠ¨åˆ†è¯å™¨ï¼Œå› ä¸ºè¿™ä¼šè‡ªåŠ¨å°†åˆ†è¯å™¨è½¬æ¢ä¸ºè¿™ä¸ªç±»ã€‚æ‰€ä»¥å¦‚æœæˆ‘æä¾›ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”å®ƒèƒ½å¤Ÿè¯†åˆ«å‡ºæ¥ï¼Œå®ƒå°±ä¼šä»¥è¿™ç§æ–¹å¼è‡ªåŠ¨åŠ è½½ã€‚
- en: Okayï¼Œ so if we take a tokenizerã€‚It converts the text into these input IDsã€‚But
    now let's have a look at something hereã€‚So why are we doing this twiceï¼ŸğŸ˜”ï¼ŒOkayã€‚Okayã€‚
    good so what we're doing here is we're just taking a sequence of text and then
    we're extracting the tokens as a list and so you can see hereã€‚That in the case
    of BERTï¼Œ which uses this word piece tokenization algorithmã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬è·å–ä¸€ä¸ªåˆ†è¯å™¨ã€‚å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºè¿™äº›è¾“å…¥ IDã€‚ä½†ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™é‡Œçš„æŸäº›å†…å®¹ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦åšä¸¤æ¬¡ï¼ŸğŸ˜”ï¼Œå¥½çš„ã€‚å¥½çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„åªæ˜¯è·å–ä¸€æ®µæ–‡æœ¬ï¼Œç„¶åå°†å…¶æå–ä¸ºä¸€ä¸ªæ ‡è®°åˆ—è¡¨ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ã€‚åœ¨
    BERT çš„æƒ…å†µä¸‹ï¼Œå®ƒä½¿ç”¨è¿™ä¸ª word piece åˆ†è¯ç®—æ³•ã€‚
- en: The way it distinguishes like words from sub wordss is using this double hash
    symbolã€‚So you can see here that in the vocabulary of the Bt tokenoerã€‚It has learned
    that it's good to split words between trans and everything else and if we wanted
    to reconstruct these two wordsã€‚ we just need to know that this double hash means
    that this former belongs to trans to build transformerã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåŒºåˆ†å•è¯å’Œå­è¯çš„æ–¹å¼æ˜¯ä½¿ç”¨è¿™ä¸ªåŒå“ˆå¸Œç¬¦å·ã€‚æ‰€ä»¥ä½ å¯ä»¥åœ¨ BERT åˆ†è¯å™¨çš„è¯æ±‡è¡¨ä¸­çœ‹åˆ°ï¼Œå®ƒå·²ç»å­¦ä¼šäº†åœ¨ trans å’Œå…¶ä»–å†…å®¹ä¹‹é—´åˆ†å‰²å•è¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦é‡å»ºè¿™ä¸¤ä¸ªå•è¯ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“è¿™ä¸ªåŒå“ˆå¸Œæ„å‘³ç€å‰é¢çš„å†…å®¹å±äº
    transï¼Œä»¥æ„å»º transformerã€‚
- en: And so one way you can reconstruct the sentence is you can take your tokens
    and you can convert them back into input IDs like thisã€‚ so this will create these
    IDsï¼Œ and then you can decode these input IDs to build back the original stringã€‚Another
    way you could do this is let's have a look where we have our inputsã€‚ã¨ã¦ ì™œã€‚Okayã€‚
    so another way you could do this is if I take myã€‚Tokenizerã€‚And I just tokenized
    my sequenceã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œé‡å»ºå¥å­çš„ä¸€ç§æ–¹å¼æ˜¯ä½ å¯ä»¥è·å–ä½ çš„æ ‡è®°ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢å›è¾“å…¥ IDï¼Œåƒè¿™æ ·ã€‚æ‰€ä»¥è¿™å°†åˆ›å»ºè¿™äº› IDï¼Œç„¶åä½ å¯ä»¥è§£ç è¿™äº›è¾“å…¥ ID ä»¥é‡å»ºåŸå§‹å­—ç¬¦ä¸²ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æœ‰çš„è¾“å…¥ã€‚ã¨ã¦
    ì™œã€‚å¥½çš„ã€‚é‚£ä¹ˆå¦ä¸€ç§æ–¹æ³•æ˜¯å¦‚æœæˆ‘è·å–æˆ‘çš„åˆ†è¯å™¨ã€‚æˆ‘åªæ˜¯å¯¹æˆ‘çš„åºåˆ—è¿›è¡Œåˆ†è¯ã€‚
- en: Then this produces what we saw beforeã€‚And then what I could do is I could go
    tokenizerã€‚decodeã€‚I put my inputs and my input Isã€‚And this should return what we
    saw beforeã€‚ And now you can see the difference between this approach and the one
    here is we don't have theseã€‚Special tokensï¼Œ so if you don't want these to be presentï¼Œ
    I think we can do skip special tokens trueã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™ä¼šäº§ç”Ÿæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„å†…å®¹ã€‚æ¥ä¸‹æ¥æˆ‘å¯ä»¥åšçš„æ˜¯ä½¿ç”¨ tokenizer.decodeã€‚æˆ‘è¾“å…¥æˆ‘çš„è¾“å…¥ï¼Œè€Œæˆ‘çš„è¾“å…¥æ˜¯ã€‚è¿™åº”è¯¥è¿”å›æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„å†…å®¹ã€‚ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°è¿™ç§æ–¹æ³•å’Œè¿™é‡Œçš„æ–¹æ³•ä¹‹é—´çš„åŒºåˆ«æ˜¯æˆ‘ä»¬æ²¡æœ‰è¿™äº›ç‰¹æ®Šæ ‡è®°ï¼Œæ‰€ä»¥å¦‚æœä½ ä¸å¸Œæœ›è¿™äº›å­˜åœ¨ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥å°†
    skip special tokens è®¾ç½®ä¸º trueã€‚
- en: And then this will give us back the original sequenceã€‚Coolã€‚ so that's some more
    or less like a sort of deep dive into the tokenizersã€‚å—¯ã€‚ğŸ˜Šã€‚Maybe one thing to mentionï¼Œ
    let's have a look at a different tokenizer so you get an idea ofã€‚What you might
    also seeã€‚ So let's find a GPT modelã€‚That is not going to block the collabã€‚Soï¼Œ
    G toã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™å°†ç»™æˆ‘ä»¬è¿”å›åŸå§‹åºåˆ—ã€‚é…·ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯å¯¹åˆ†è¯å™¨çš„ä¸€ç§æ·±å…¥æ¢è®¨ã€‚å—¯ã€‚ğŸ˜Šã€‚ä¹Ÿè®¸å€¼å¾—æåˆ°çš„ä¸€ä»¶äº‹æ˜¯ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦ä¸€ä¸ªåˆ†è¯å™¨ï¼Œè¿™æ ·ä½ å°±èƒ½å¯¹ä½ å¯èƒ½çœ‹åˆ°çš„å†…å®¹æœ‰ä¸ªäº†è§£ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬æ‰¾ä¸€ä¸ªä¸ä¼šé˜»æ­¢
    colab çš„ GPT æ¨¡å‹ã€‚æ‰€ä»¥ï¼ŒG toã€‚
- en: Let let's do maybe this oneã€‚So I'm going to just take a tiny GPTã€‚ you can also
    copy the name of the checkpointï¼Œ which is quite handyï¼Œ so we are hereã€‚So what
    I'm going to do is I just want to show you the difference between the GPT model
    and the way it tokenizesã€‚ so hopefully this worksã€‚Yeahï¼Œ so GPT has a kind of very
    quirkyã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬åšè¿™ä¸ªå§ã€‚æˆ‘å°†åªæ˜¯æ‹¿ä¸€ä¸ªå°çš„GPTã€‚ä½ ä¹Ÿå¯ä»¥å¤åˆ¶æ£€æŸ¥ç‚¹çš„åç§°ï¼Œè¿™å¾ˆæ–¹ä¾¿ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œã€‚é‚£ä¹ˆæˆ‘æƒ³åšçš„å°±æ˜¯å±•ç¤ºGPTæ¨¡å‹ä¸å®ƒçš„åˆ†è¯æ–¹å¼ä¹‹é—´çš„åŒºåˆ«ã€‚å¸Œæœ›è¿™æ ·èƒ½æˆåŠŸã€‚æ˜¯çš„ï¼ŒGPTæœ‰ä¸€ç§éå¸¸å¤æ€ªçš„å¤„ç†æ–¹å¼ã€‚
- en: Tokenizer where it uses this weird symbolï¼Œ it's like a G with a little like
    thought on top of itã€‚ and this is what it uses to indicate that there's a white
    space between this token and this oneã€‚So you can see that it's sayingï¼Œ okayã€‚Usingï¼Œ
    and then there's a white spaceï¼Œ then Rã€‚ then there's a white space and then transï¼Œ
    but then former has no white space so if we wanted to reconstruct thisã€‚
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨ä½¿ç”¨è¿™ä¸ªå¥‡æ€ªçš„ç¬¦å·ï¼Œå°±åƒä¸€ä¸ªä¸Šé¢æœ‰å°æ€è€ƒçš„Gã€‚å®ƒç”¨æ¥è¡¨ç¤ºè¿™ä¸ªæ ‡è®°ä¸é‚£ä¸ªæ ‡è®°ä¹‹é—´æœ‰ç©ºæ ¼ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°å®ƒåœ¨è¯´ï¼Œå¥½çš„ï¼Œä½¿ç”¨ï¼Œç„¶åæœ‰ç©ºæ ¼ï¼Œç„¶åæ˜¯Rï¼Œç„¶åæœ‰ç©ºæ ¼ï¼Œç„¶åæ˜¯transï¼Œä½†æ˜¯formeræ²¡æœ‰ç©ºæ ¼ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬æƒ³é‡æ„è¿™ä¸ªã€‚
- en: we just stitch this back to thisã€‚But then there's a white space with a network
    and so onã€‚ and so you can see this is kind of quite different to the Bt one which
    basically treats every token as having a corresponding white space unless we have
    the two hash symbolsã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªéœ€å°†å…¶æ‹¼æ¥å›å»ã€‚ä½†æ˜¯é‚£æ ·ç½‘ç»œä¹‹é—´æœ‰ç©ºæ ¼ç­‰ç­‰ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°è¿™ä¸Btæ¨¡å‹æœ‰å¾ˆå¤§ä¸åŒï¼Œåè€…åŸºæœ¬ä¸Šå°†æ¯ä¸ªæ ‡è®°è§†ä¸ºæœ‰ç›¸åº”çš„ç©ºæ ¼ï¼Œé™¤éæˆ‘ä»¬æœ‰ä¸¤ä¸ªäº•å·ã€‚
- en: whereas GPT2 is kind of the other way around it saysã€‚ assume no there is no
    white space unless I put a special symbol like thisã€‚Okayã€‚ so are there any questions
    about the tokenizersï¼ŸOkayï¼Œ okayï¼Œ so let's now have a lookã€‚At how we can handleã€‚Multiple
    sequences togetherã€‚I'll start the videoã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è€ŒGPT2åˆ™æ˜¯å¦ä¸€ç§æƒ…å†µã€‚å‡è®¾æ²¡æœ‰ç©ºæ ¼ï¼Œé™¤éæˆ‘æ”¾å…¥è¿™æ ·çš„ç‰¹æ®Šç¬¦å·ã€‚å¥½çš„ã€‚é‚£ä¹ˆå…³äºåˆ†è¯å™¨æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿå¥½çš„ï¼Œå¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ã€‚æˆ‘å°†å¼€å§‹è¿™ä¸ªè§†é¢‘ã€‚
- en: How to batch inputs together in this videoï¼Œ we also see how two batch input
    sequences togetherã€‚In share all of the sentences we want to pass through our model
    won't all have the same lengthã€‚Here we are using the model we saw in the sentiment
    analysis pipeline and want to classify two sentencesã€‚When tokenizing them and
    mapping each token to its corresponding input IDã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•å°†è¾“å…¥æ‰¹é‡å¤„ç†åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šçœ‹åˆ°å¦‚ä½•å°†ä¸¤ä¸ªæ‰¹é‡è¾“å…¥åºåˆ—ç»“åˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬æƒ³é€šè¿‡æ¨¡å‹ä¼ é€’çš„æ‰€æœ‰å¥å­å¹¶ä¸éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨åœ¨æƒ…æ„Ÿåˆ†æç®¡é“ä¸­çœ‹åˆ°çš„æ¨¡å‹ï¼Œå¹¶å¸Œæœ›å¯¹ä¸¤ä¸ªå¥å­è¿›è¡Œåˆ†ç±»ã€‚å½“å¯¹å®ƒä»¬è¿›è¡Œåˆ†è¯å¹¶å°†æ¯ä¸ªæ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”çš„è¾“å…¥IDæ—¶ã€‚
- en: we get two lists of different lengthã€‚Trying to create a densor or an Mbi array
    from the list will result in an error because all arrays and densils should be
    recangularã€‚One way to overcome this limit is to make the second sentence the same
    length at the first by adding a special token as many times as necessaryã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°ä¸¤ä¸ªä¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚å°è¯•ä»åˆ—è¡¨åˆ›å»ºä¸€ä¸ªå¯†é›†æ•°ç»„æˆ–Mbiæ•°ç»„ä¼šå¯¼è‡´é”™è¯¯ï¼Œå› ä¸ºæ‰€æœ‰æ•°ç»„å’Œå¯†é›†æ•°ç»„åº”è¯¥æ˜¯çŸ©å½¢çš„ã€‚å…‹æœè¿™ä¸ªé™åˆ¶çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡æ·»åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œä½¿ç¬¬äºŒä¸ªå¥å­çš„é•¿åº¦ä¸ç¬¬ä¸€ä¸ªå¥å­ç›¸åŒï¼Œæ·»åŠ å¿…è¦çš„æ¬¡æ•°ã€‚
- en: Another way would be to trun gate the first sequence to the length of the secondã€‚But
    you would then lose a lot of information that may be necessary to properly classify
    the sentenceã€‚In generalï¼Œ we only truncate sentences when we are longer than the
    maximum length the model can handleã€‚The value used to pad the circum sentence
    should not be picked randomlyã€‚
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ–¹æ³•æ˜¯å°†ç¬¬ä¸€ä¸ªåºåˆ—æˆªæ–­åˆ°ç¬¬äºŒä¸ªçš„é•¿åº¦ã€‚ä½†æ˜¯ä½ ä¼šå¤±å»å¾ˆå¤šå¯èƒ½å¯¹æ­£ç¡®åˆ†ç±»å¥å­å¿…è¦çš„ä¿¡æ¯ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåªæœ‰å½“å¥å­è¶…è¿‡æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬æ‰ä¼šæˆªæ–­å¥å­ã€‚ç”¨æ¥å¡«å……å¥å­çš„å€¼ä¸åº”è¯¥éšæœºé€‰æ‹©ã€‚
- en: The model has been portrayed with a certain padding IDï¼Œ which you can find in
    tokenazizerã€‚pa tokeniteã€‚Now that we have p sentencesï¼Œ we can make a batch with
    themã€‚If we pass the two sentences to the model separately and patch together howeverã€‚
    we notice that we don't get the same results for the sentence that is pad here
    the second one h is that a bug in the transformers library now if you remember
    that transformers will all make easy user of attention layersã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å·²è¢«æç»˜ä¸ºå…·æœ‰ç‰¹å®šçš„å¡«å……IDï¼Œä½ å¯ä»¥åœ¨tokenazizerä¸­æ‰¾åˆ°ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†pä¸ªå¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒä»¬åˆ›å»ºä¸€ä¸ªæ‰¹æ¬¡ã€‚å¦‚æœæˆ‘ä»¬å•ç‹¬å°†ä¸¤ä¸ªå¥å­ä¼ é€’ç»™æ¨¡å‹å¹¶è¿›è¡Œæ‹¼æ¥ï¼Œæˆ‘ä»¬ä¼šæ³¨æ„åˆ°ï¼Œå¯¹äºå¡«å……çš„å¥å­ï¼Œç¬¬äºŒä¸ªå¥å­å¾—åˆ°çš„ç»“æœä¸ä¸€æ ·ã€‚è¿™æ˜¯å˜æ¢å™¨åº“ä¸­çš„ä¸€ä¸ªbugå—ï¼Ÿç°åœ¨å¦‚æœä½ è®°å¾—ï¼Œå˜æ¢å™¨ä¼šè®©ç”¨æˆ·è½»æ¾ä½¿ç”¨æ³¨æ„åŠ›å±‚ã€‚
- en: this should not come as a total surpriseã€‚When computing is the contextual representation
    of each tokenã€‚The attention layers look at all the other words in the sentenceã€‚If
    you have just a sentence or the sentence with several padic tokens that itã€‚ it's
    logical we don't get the same valuesã€‚To get the same results with or without paddingã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥å¹¶ä¸ä»¤äººå®Œå…¨æƒŠè®¶ã€‚å½“è®¡ç®—æ¯ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºæ—¶ï¼Œæ³¨æ„åŠ›å±‚ä¼šæŸ¥çœ‹å¥å­ä¸­çš„æ‰€æœ‰å…¶ä»–å•è¯ã€‚å¦‚æœä½ åªæœ‰ä¸€ä¸ªå¥å­æˆ–è€…å¥å­ä¸­æœ‰å‡ ä¸ªå¡«å……æ ‡è®°ï¼Œé‚£é€»è¾‘ä¸Šæˆ‘ä»¬ä¸ä¼šå¾—åˆ°ç›¸åŒçš„å€¼ã€‚ä¸ºäº†è·å¾—ç›¸åŒçš„ç»“æœï¼Œæ— è®ºæœ‰æ— å¡«å……ã€‚
- en: we need to indicate to the attention layers that we should ignore those padding
    targetsã€‚This is done by creating an attention maskï¼Œ a tonsil with the same shape
    as the input IDs with series and onesã€‚Once indicates the tokens the attention
    layers should consider in the contextã€‚ and the the tokens we should ignoreã€‚Nowï¼Œ
    passing this attention mask along with the input ID will give us the same results
    as when we send the two sentences individually to the modelã€‚
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å‘æ³¨æ„åŠ›å±‚æŒ‡ç¤ºåº”è¯¥å¿½ç•¥é‚£äº›å¡«å……ç›®æ ‡ã€‚è¿™æ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªæ³¨æ„åŠ›æ©ç æ¥å®Œæˆçš„ï¼Œè¯¥æ©ç ä¸è¾“å…¥ ID å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼Œé‡Œé¢æ˜¯ç³»åˆ—çš„ 0 å’Œ 1ã€‚1 è¡¨ç¤ºæ³¨æ„åŠ›å±‚åº”è¯¥åœ¨ä¸Šä¸‹æ–‡ä¸­è€ƒè™‘çš„æ ‡è®°ï¼Œè€Œ
    0 è¡¨ç¤ºæˆ‘ä»¬åº”è¯¥å¿½ç•¥çš„æ ‡è®°ã€‚ç°åœ¨ï¼Œå°†è¿™ä¸ªæ³¨æ„åŠ›æ©ç ä¸è¾“å…¥ ID ä¸€èµ·ä¼ é€’ï¼Œå°†ç»™æˆ‘ä»¬ä¸å°†ä¸¤ä¸ªå¥å­å•ç‹¬å‘é€ç»™æ¨¡å‹æ—¶ç›¸åŒçš„ç»“æœã€‚
- en: This is all done behind the scenes by the tokenizer when you apply to several
    sentences with the flag padding equal toã€‚It will apply his bedding with a proper
    value to the smaller sentences and create the appropriate attention maskã€‚![](img/40873acb06abf924ac4a43fae802679a_45.png)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€åˆ‡éƒ½æ˜¯ç”±æ ‡è®°å™¨åœ¨ä½ å°†å¤šä¸ªå¥å­åº”ç”¨äºå¡«å……æ ‡å¿—æ—¶åœ¨åå°å®Œæˆçš„ã€‚å®ƒå°†ä¸ºè¾ƒå°çš„å¥å­åº”ç”¨é€‚å½“çš„å¡«å……å€¼ï¼Œå¹¶åˆ›å»ºç›¸åº”çš„æ³¨æ„åŠ›æ©ç ã€‚![](img/40873acb06abf924ac4a43fae802679a_45.png)
- en: '![](img/40873acb06abf924ac4a43fae802679a_46.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_46.png)'
- en: Yeahã€‚Okayï¼Œ so I see we have a couple of questions so the first question from
    IM homesmesã€‚ I don't understand why we needed the extra dimensionã€‚Based on the
    error message that has returnedã€‚ how would you troubleshoot to determine that
    you needed another dimensionï¼ŸOkayã€‚ so I think the best way to look at this is
    probably with some codeï¼Œ so let's go hereã€‚ğŸ˜”ï¼ŒAndã€‚
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚å¥½çš„ï¼Œæˆ‘çœ‹åˆ°æˆ‘ä»¬æœ‰å‡ ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªé—®é¢˜æ¥è‡ª IM homesmesã€‚æˆ‘ä¸æ˜ç™½ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦é¢å¤–çš„ç»´åº¦ã€‚æ ¹æ®è¿”å›çš„é”™è¯¯æ¶ˆæ¯ï¼Œä½ ä¼šå¦‚ä½•æ’æŸ¥ä»¥ç¡®å®šä½ éœ€è¦å¦ä¸€ä¸ªç»´åº¦ï¼Ÿå¥½çš„ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºæŸ¥çœ‹è¿™ä¸ªçš„æœ€ä½³æ–¹å¼å¯èƒ½æ˜¯ä¸€äº›ä»£ç ï¼Œæ‰€ä»¥æˆ‘ä»¬å»è¿™é‡Œã€‚ğŸ˜”ï¼Œå¹¶ã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_48.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_48.png)'
- en: Install transformersã€‚![](img/40873acb06abf924ac4a43fae802679a_50.png)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£… transformersã€‚![](img/40873acb06abf924ac4a43fae802679a_50.png)
- en: Okayï¼Œ soã€‚If I understand the question from O homesï¼Œ you're talking about this
    error messageã€‚ Let's see if it is reproducibleã€‚Okayï¼Œ goodã€‚ So I think you're talking
    about this index errorã€‚That we're getting hereã€‚So let's have a look how we might
    debug thisã€‚So the error is saying that the dimension is out of rangeã€‚And we expected
    to be in the range -1 to 0ã€‚
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæ‰€ä»¥ã€‚å¦‚æœæˆ‘ç†è§£ O homes çš„é—®é¢˜ï¼Œä½ åœ¨è°ˆè®ºè¿™ä¸ªé”™è¯¯æ¶ˆæ¯ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦å¯ä»¥é‡ç°ã€‚å¥½çš„ï¼Œå¾ˆå¥½ã€‚æ‰€ä»¥æˆ‘æƒ³ä½ åœ¨è°ˆè®ºè¿™ä¸ªç´¢å¼•é”™è¯¯ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¾—åˆ°çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¯èƒ½å¦‚ä½•è°ƒè¯•è¿™ä¸ªã€‚é”™è¯¯æ˜¯è¯´ç»´åº¦è¶…å‡ºèŒƒå›´ã€‚æˆ‘ä»¬é¢„è®¡å®ƒåº”è¯¥åœ¨
    -1 åˆ° 0 çš„èŒƒå›´å†…ã€‚
- en: but got oneã€‚So let's have a look at what the shape of our inputs areã€‚ This is
    sort of how I would debug this messageï¼Œ soã€‚Okayï¼Œ so we can see that theã€‚The input
    IDs have a size of just 14ï¼Œ and that's just representing the 14 tokens that we
    get when we tokenize the sequenceã€‚å—¯ã€‚ğŸ˜Šï¼ŒSo I'm going to show you a dirty secret
    of how most software engineers debug stuffã€‚
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¾—åˆ°äº†ä¸€ä¸ªã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„è¾“å…¥æ˜¯ä»€ä¹ˆå½¢çŠ¶ã€‚è¿™å¤§æ¦‚æ˜¯æˆ‘è°ƒè¯•è¿™ä¸ªæ¶ˆæ¯çš„æ–¹å¼ï¼Œæ‰€ä»¥ã€‚å¥½çš„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ã€‚è¾“å…¥ ID çš„å¤§å°åªæœ‰ 14ï¼Œè¿™ä»…ä»…è¡¨ç¤ºåœ¨å¯¹åºåˆ—è¿›è¡Œæ ‡è®°æ—¶æˆ‘ä»¬è·å¾—çš„
    14 ä¸ªæ ‡è®°ã€‚å—¯ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘å°†å‘ä½ å±•ç¤ºå¤§å¤šæ•°è½¯ä»¶å·¥ç¨‹å¸ˆè°ƒè¯•å†…å®¹çš„ä¸€ä¸ªå°ç§˜å¯†ã€‚
- en: so you take thisã€‚And you checkuck it into Googleã€‚And then you goï¼Œ aï¼Œ this looks
    like a pythor chã€‚And then we have a look and we see if someone can explain what's
    going onã€‚Sorryã€‚You can see some messageï¼Œ someone has a thing hereï¼Œ getting some
    dimension at a range errorã€‚Uã€‚ and then okayï¼Œ this looks like it's like a deeper
    issue in pytorrchï¼Œ maybe that's going so helpfulã€‚
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ æ‹¿è¿™ä¸ªã€‚ç„¶åä½ åœ¨è°·æ­Œä¸ŠæŸ¥ä¸€ä¸‹ã€‚ç„¶åä½ å°±ä¼šè¯´ï¼Œå•Šï¼Œè¿™çœ‹èµ·æ¥åƒä¸ª PyTorchã€‚ç„¶åæˆ‘ä»¬çœ‹çœ‹æœ‰æ²¡æœ‰äººèƒ½è§£é‡Šå‘ç”Ÿäº†ä»€ä¹ˆã€‚æŠ±æ­‰ã€‚ä½ å¯ä»¥çœ‹åˆ°ä¸€äº›æ¶ˆæ¯ï¼Œæœ‰äººè¿™é‡Œæœ‰ä¸ªä¸œè¥¿ï¼Œé‡åˆ°äº†ä¸€äº›ç»´åº¦èŒƒå›´é”™è¯¯ã€‚å—¯ã€‚ç„¶åå¥½å§ï¼Œè¿™çœ‹èµ·æ¥åƒæ˜¯åœ¨
    PyTorch ä¸­çš„ä¸€ä¸ªæ›´æ·±å±‚æ¬¡çš„é—®é¢˜ï¼Œå¯èƒ½ä¼šæœ‰å¸®åŠ©ã€‚
- en: But then maybe let's look a stack overflow this is often where you'll find a
    good answerã€‚ so someone is getting the same kind of error and let's see what someone
    told them in the answerã€‚So they're saying that you're giving a 1D tensor to this
    thingï¼Œ but it expects this kind of objectã€‚So let's see if that is kind of relevant
    to what we're doingã€‚Soã€‚If we look at the error hereã€‚
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¹Ÿè®¸è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å †æ ˆæº¢å‡ºï¼Œè¿™é€šå¸¸æ˜¯ä½ èƒ½æ‰¾åˆ°å¥½ç­”æ¡ˆçš„åœ°æ–¹ã€‚æœ‰äººé‡åˆ°äº†åŒæ ·çš„é”™è¯¯ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åˆ«äººç»™ä»–ä»¬çš„ç­”æ¡ˆã€‚ä»–ä»¬è¯´ä½ ç»™äº†è¿™ä¸ªä¸œè¥¿ä¸€ä¸ª1Då¼ é‡ï¼Œä½†å®ƒæœŸæœ›è¿™ç§ç±»å‹çš„å¯¹è±¡ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯å¦å’Œæˆ‘ä»¬æ­£åœ¨åšçš„ç›¸å…³ã€‚æ‰€ä»¥ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™é‡Œçš„é”™è¯¯ã€‚
- en: it's saying that at some point in the stack traceã€‚We tried to compute sequence
    lengthã€‚By taking the size of the input Isã€‚And then we actually tried to access
    the second dimension of the input IDsã€‚And so if you look atã€‚The the size that
    we provideï¼Œ it only has one dimensionã€‚ so basicallyã€‚ I can accessã€‚Size 0ï¼Œ because
    that's theã€‚The the first first dimension that's availableã€‚
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¯´åœ¨å †æ ˆè·Ÿè¸ªçš„æŸä¸ªç‚¹ã€‚æˆ‘ä»¬è¯•å›¾é€šè¿‡è·å–è¾“å…¥çš„å¤§å°æ¥è®¡ç®—åºåˆ—é•¿åº¦ã€‚ç„¶åæˆ‘ä»¬å®é™…ä¸Šè¯•å›¾è®¿é—®è¾“å…¥IDçš„ç¬¬äºŒä¸ªç»´åº¦ã€‚æ‰€ä»¥å¦‚æœä½ çœ‹ä¸€ä¸‹ã€‚æˆ‘ä»¬æä¾›çš„å¤§å°ï¼Œå®ƒåªæœ‰ä¸€ä¸ªç»´åº¦ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šã€‚æˆ‘å¯ä»¥è®¿é—®ã€‚å¤§å°0ï¼Œå› ä¸ºé‚£æ˜¯å¯ç”¨çš„ç¬¬ä¸€ä¸ªç»´åº¦ã€‚
- en: I don't know why coab is so slowã€‚Howeverï¼Œ if I tried to access the second dimension
    of a one dimensional objectã€‚ then that's not possibleï¼Œ so it's going to throw
    this kind of errorã€‚For some reasonã€‚ CoAab is acting really slowlyï¼Œ I'm just going
    to restart itã€‚See if can do thatã€‚More interestingã€‚So let's seeï¼Œ is it intentã€‚Interestingï¼Œ
    so coababï¼Œ okayã€‚Let's seeã€‚I spin up curl againã€‚
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆcoabè¿™ä¹ˆæ…¢ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘å°è¯•è®¿é—®ä¸€ç»´å¯¹è±¡çš„ç¬¬äºŒä¸ªç»´åº¦ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥å®ƒä¼šæŠ›å‡ºè¿™ç§é”™è¯¯ã€‚ä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚CoAabçš„ååº”çœŸçš„å¾ˆæ…¢ï¼Œæˆ‘å°±è¦é‡å¯å®ƒã€‚çœ‹çœ‹èƒ½ä¸èƒ½åšåˆ°ã€‚è¿™æ›´æœ‰è¶£ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ï¼Œè¿™æ˜¯å¦æœ‰æ„å›¾ã€‚å¾ˆæœ‰è¶£ï¼Œæ‰€ä»¥coababï¼Œå¥½çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘å†å¯åŠ¨ä¸€æ¬¡curlã€‚
- en: Maybe I have too many colorss openã€‚Okayï¼Œ let's try againã€‚Soã€‚ä½  toã€‚Andst thisï¼Œ
    okayã€‚ fingers crossed this worksã€‚Sorry forï¼Œ okayï¼Œ goodã€‚We're trying to debug this
    errorã€‚ and we see that the stack trace is telling us it's hereã€‚ And the problem
    is that we're trying toã€‚Determine the sizeï¼Œ or basically we're trying to pick
    out the size of the dimension in the second component of the input IDsã€‚
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘æ‰“å¼€äº†å¤ªå¤šé¢œè‰²ã€‚å¥½å§ï¼Œå†è¯•ä¸€æ¬¡ã€‚æ‰€ä»¥ã€‚ä½ å»ã€‚Andstè¿™ä¸ªï¼Œå¥½å§ã€‚ç¥å¥½è¿ï¼Œå¸Œæœ›è¿™èƒ½å¥æ•ˆã€‚æŠ±æ­‰ï¼Œå¥½çš„ã€‚æˆ‘ä»¬æ­£åœ¨å°è¯•è°ƒè¯•è¿™ä¸ªé”™è¯¯ï¼Œæˆ‘ä»¬çœ‹åˆ°å †æ ˆè·Ÿè¸ªå‘Šè¯‰æˆ‘ä»¬æ˜¯åœ¨è¿™é‡Œã€‚é—®é¢˜æ˜¯æˆ‘ä»¬æ­£åœ¨å°è¯•ç¡®å®šå¤§å°ï¼Œæˆ–è€…åŸºæœ¬ä¸Šæˆ‘ä»¬åœ¨å°è¯•æŒ‘é€‰è¾“å…¥IDç¬¬äºŒä¸ªç»„ä»¶çš„ç»´åº¦çš„å¤§å°ã€‚
- en: But the problem that we have is that the input Isã€‚I only have one dimensionã€‚
    so their size like that 14ã€‚And if we access the first elementï¼Œ we get 14ï¼Œ that's
    goodã€‚ but if we access the secondï¼Œ we're going to get the same errorã€‚And so here
    we're seeing that the problem is that we need to basically provide a batch dimension
    which says that we're dealing with one sentenceã€‚
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬é¢ä¸´çš„é—®é¢˜æ˜¯è¾“å…¥æ˜¯ã€‚æˆ‘åªæœ‰ä¸€ä¸ªç»´åº¦ã€‚æ‰€ä»¥å®ƒä»¬çš„å¤§å°æ˜¯14ã€‚å¦‚æœæˆ‘ä»¬è®¿é—®ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬å¾—åˆ°14ï¼Œè¿™å¾ˆå¥½ã€‚ä½†å¦‚æœæˆ‘ä»¬è®¿é—®ç¬¬äºŒä¸ªï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ç›¸åŒçš„é”™è¯¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šéœ€è¦æä¾›ä¸€ä¸ªæ‰¹å¤„ç†ç»´åº¦ï¼Œè¯´æ˜æˆ‘ä»¬æ­£åœ¨å¤„ç†ä¸€ä¸ªå¥å­ã€‚
- en: which has a sequence of length 14ã€‚ So basically most of the inputsã€‚The input
    Isã€‚Ids should haveã€‚Shapeã€‚Thattch sizeã€‚And then secrets like thatã€‚Noã€‚Does that
    answer the questionï¼Œ I'm Holmesï¼Ÿ
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªåºåˆ—çš„é•¿åº¦éƒ½æ˜¯14ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šå¤§å¤šæ•°è¾“å…¥ã€‚è¾“å…¥æ˜¯ã€‚IDåº”è¯¥æœ‰ã€‚å½¢çŠ¶ã€‚åŒ¹é…å¤§å°ã€‚ç„¶ååƒè¿™æ ·çš„ç§˜å¯†ã€‚æ²¡æœ‰ã€‚è¿™å›ç­”äº†é—®é¢˜å—ï¼Œæˆ‘æ˜¯éœå§†æ–¯ï¼Ÿ
- en: I think that was a bit of a convoluted way of debuggingï¼Œ but that's more or
    less how I would do itã€‚å—¯ã€‚Coolï¼Œ so then there's another question from SRM Sma about
    can we modify the padding techniqueï¼Ÿ
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ç§æœ‰ç‚¹å¤æ‚çš„è°ƒè¯•æ–¹å¼ï¼Œä½†è¿™å¤§è‡´å°±æ˜¯æˆ‘ä¼šæ€ä¹ˆåšã€‚å—¯ã€‚å¾ˆå¥½ï¼Œè¿˜æœ‰ä¸€ä¸ªæ¥è‡ªSRM Smaçš„é—®é¢˜ï¼Œå…³äºæˆ‘ä»¬æ˜¯å¦å¯ä»¥ä¿®æ”¹å¡«å……æŠ€æœ¯ï¼Ÿ
- en: So the answer is yesï¼Œ and let's have a look where we do paddingã€‚ã‚ã£ãŸã€‚Okayã€‚ I'm
    going to just make this upã€‚Okayï¼Œ so let's have a lookã€‚So I've got a tokenizerã€‚And
    if I just take a sequence like thisã€‚I'm going to get these input IDsã€‚And so then
    the question might beï¼Œ if I add some paddingã€‚
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åœ¨å“ªé‡Œè¿›è¡Œå¡«å……ã€‚å•Šï¼Œå¥½çš„ã€‚æˆ‘åªæ˜¯éšä¾¿æƒ³çš„ã€‚å¥½å§ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ã€‚æˆ‘æœ‰ä¸€ä¸ªåˆ†è¯å™¨ã€‚å¦‚æœæˆ‘åªå–è¿™æ ·ä¸€ä¸ªåºåˆ—ï¼Œæˆ‘ä¼šå¾—åˆ°è¿™äº›è¾“å…¥IDã€‚é‚£ä¹ˆé—®é¢˜å¯èƒ½æ˜¯ï¼Œå¦‚æœæˆ‘æ·»åŠ ä¸€äº›å¡«å……ã€‚
- en: Then what what's going to happen to my input is so basically this should I think
    just in this caseã€‚ provide no no padding tokensã€‚So what I'm going to do is I'm
    going to createã€‚Two sequencesï¼Œ soã€‚My dogã€‚Is called photoã€‚And then another sequenceï¼Œ
    which is like my cat isã€‚Is coldã€‚Something really coolï¼Œ likeã€‚I knowï¼Œ Elizaã€‚So now
    I've got these two sequencesã€‚
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘çš„è¾“å…¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼ŒåŸºæœ¬ä¸Šåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™åº”è¯¥ä¸æä¾›ä»»ä½•å¡«å……æ ‡è®°ã€‚æ‰€ä»¥æˆ‘å°†åˆ›å»ºä¸¤ä¸ªåºåˆ—ã€‚æˆ‘çš„ç‹—å«ç…§ç‰‡ã€‚ç„¶åå¦ä¸€ä¸ªåºåˆ—ï¼Œåƒæˆ‘çš„çŒ«æ˜¯ã€‚æ˜¯å†·çš„ã€‚çœŸçš„å¾ˆé…·ï¼Œæ¯”å¦‚ã€‚æˆ‘çŸ¥é“ï¼Œä¼Šä¸½èã€‚æ‰€ä»¥ç°åœ¨æˆ‘æœ‰è¿™ä¸¤ä¸ªåºåˆ—ã€‚
- en: one is shorter than the otherã€‚So now if I pass these to my tokenizer with the
    padding trueã€‚You can see that what happens is in the first sequenceï¼Œ it takes
    the normal tokensã€‚ and then it adds a bunch of zeros to the endã€‚And the number
    of tokens it adds is just designed so that it matches the same length as the longest
    sequence in theã€‚In the in the inputsã€‚Now the question isã€‚Kind of like what strategy
    can we doï¼ŸTo do thatã€‚
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¡æ¯”å¦ä¸€æ¡çŸ­ã€‚æ‰€ä»¥ç°åœ¨å¦‚æœæˆ‘å°†è¿™äº›ä¼ é€’ç»™æˆ‘çš„åˆ†è¯å™¨å¹¶è®¾ç½®å¡«å……ä¸ºçœŸã€‚ä½ å¯ä»¥çœ‹åˆ°å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œåœ¨ç¬¬ä¸€ä¸ªåºåˆ—ä¸­ï¼Œå®ƒé‡‡ç”¨äº†æ­£å¸¸çš„æ ‡è®°ã€‚ç„¶ååœ¨æœ«å°¾æ·»åŠ äº†ä¸€å †é›¶ã€‚å®ƒæ·»åŠ çš„é›¶çš„æ•°é‡æ­£æ˜¯ä¸ºäº†åŒ¹é…è¾“å…¥ä¸­æœ€é•¿åºåˆ—çš„é•¿åº¦ã€‚ç°åœ¨é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä»€ä¹ˆç­–ç•¥ï¼Ÿæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: And let's seeï¼Œ I think we can do doã€‚![](img/40873acb06abf924ac4a43fae802679a_52.png)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ï¼Œæˆ‘æƒ³æˆ‘ä»¬å¯ä»¥åšåˆ°ã€‚![](img/40873acb06abf924ac4a43fae802679a_52.png)
- en: So what I'm going to do to find the actual argument I need is I'm going to go
    to transformformersã€‚![](img/40873acb06abf924ac4a43fae802679a_54.png)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æ¥ä¸‹æ¥è¦åšçš„å°±æ˜¯æ‰¾åˆ°æˆ‘éœ€è¦çš„å®é™…å‚æ•°ï¼Œæˆ‘å°†å»transformformersã€‚![](img/40873acb06abf924ac4a43fae802679a_54.png)
- en: And I'm going to look at padding because I can't remember how I pad on the leftã€‚So
    maybe is itã€‚Ptting arguments for pretrainã€‚Toã€‚Soï¼Œ look at the source codeã€‚okã€‚So
    I can do longest max lengthã€‚Interestingï¼Œ so okayï¼Œ that only will give us the optionsã€‚
    for exampleï¼Œ if I doã€‚Max lengthã€‚Then this will pad to the maximum length of the
    whole modelï¼Œ so the modelï¼Œ which is Btã€‚
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æŸ¥çœ‹å¡«å……ï¼Œå› ä¸ºæˆ‘ä¸è®°å¾—æˆ‘æ˜¯å¦‚ä½•åœ¨å·¦ä¾§è¿›è¡Œå¡«å……çš„ã€‚é‚£ä¹ˆä¹Ÿè®¸æ˜¯ã€‚é¢„è®­ç»ƒçš„å¡«å……å‚æ•°ã€‚å»ã€‚å¥½å§ã€‚çœ‹çœ‹æºä»£ç ã€‚å¥½çš„ã€‚æ‰€ä»¥æˆ‘å¯ä»¥åšæœ€é•¿æœ€å¤§é•¿åº¦ã€‚æœ‰è¶£ï¼Œæ‰€ä»¥å¥½çš„ï¼Œè¿™åªä¼šç»™æˆ‘ä»¬é€‰é¡¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘åšã€‚æœ€å¤§é•¿åº¦ã€‚é‚£ä¹ˆè¿™å°†å¡«å……åˆ°æ•´ä¸ªæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯Btæ¨¡å‹ã€‚
- en: can process 512 tokensã€‚And so this will process huge number of zeroes all the
    way out to the length of 512ã€‚And the other option that we have here is Longestï¼Œ
    which is the default of just the longest example in the inputsã€‚But let's see how
    we've pad to the leftã€‚å—¯ã€‚I rememberã€‚We can do thisã€‚ã†ã‚“ã€‚Okayï¼Œ soã€‚Let's seeã€‚ So we
    can take in the inputã€‚Padding strategyã€‚Andã€‚Had to multipleã€‚å—¯å—¯ã€‚I wasã€‚
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥å¤„ç†512ä¸ªæ ‡è®°ã€‚æ‰€ä»¥è¿™å°†å¤„ç†å¤§é‡çš„é›¶ï¼Œä¸€ç›´åˆ°512çš„é•¿åº¦ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œçš„å¦ä¸€ä¸ªé€‰é¡¹æ˜¯æœ€é•¿ï¼Œè¿™å°±æ˜¯è¾“å…¥ä¸­æœ€é•¿ç¤ºä¾‹çš„é»˜è®¤å€¼ã€‚ä½†æ˜¯è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦‚ä½•åœ¨å·¦ä¾§å¡«å……çš„ã€‚å—¯ã€‚æˆ‘è®°å¾—ã€‚æˆ‘ä»¬å¯ä»¥åšåˆ°ã€‚ã†ã‚“ã€‚å¥½çš„ï¼Œæ‰€ä»¥ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¾“å…¥ã€‚å¡«å……ç­–ç•¥ã€‚å’Œã€‚å¤šä¸ªã€‚å—¯å—¯ã€‚æˆ‘ä¹‹å‰æ˜¯ã€‚
- en: I was pretty sure that I can pat on both sidesã€‚Soï¼Œ let's seeã€‚Thanksï¼Œ Oma has
    the answerï¼Œ greatã€‚ğŸ˜Šï¼ŒSoã€‚ so then we can do thisã€‚ We do padding trueã€‚And thenï¼Œ patting
    sideã€‚Equals leftã€‚Interestingã€‚ did I do wrongã€‚Maybe it's only for someã€‚Tchkenizes
    and we can do thisã€‚Okayï¼Œ so padding insideã€‚So here we can see that their default
    values to pad should be right or leftã€‚
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆç¡®å®šæˆ‘å¯ä»¥åœ¨ä¸¤ä¾§è¿›è¡Œå¡«å……ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ã€‚è°¢è°¢ï¼ŒOmaç»™å‡ºäº†ç­”æ¡ˆï¼Œå¤ªå¥½äº†ã€‚ğŸ˜Šï¼Œæ‰€ä»¥ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·åšã€‚æˆ‘ä»¬åšå¡«å……ä¸ºçœŸã€‚ç„¶åï¼Œå¡«å……ä¾§ã€‚ç­‰äºå·¦ä¾§ã€‚æœ‰è¶£ã€‚æˆ‘åšé”™äº†å—ï¼Ÿä¹Ÿè®¸è¿™ä»…é€‚ç”¨äºæŸäº›ã€‚åˆ†è¯å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åšåˆ°ã€‚å¥½çš„ï¼Œæ‰€ä»¥å¡«å……ä¾§ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé»˜è®¤å€¼åº”è¯¥å¡«å……ä¸ºå³ä¾§æˆ–å·¦ä¾§ã€‚
- en: And this is for the pre trained tokenizerã€‚Interesting okayï¼Œ let's do thisï¼Œ soã€‚Let's
    seeã€‚Sorry for this live hackingã€‚So let's have a lookã€‚ If we look at one of the
    attributes of the tokenizerã€‚ it's called padding sideã€‚And here by default it's
    rightã€‚ so I can override this attribute by saying make it leftã€‚So it's not a keyword
    argumentã€‚
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é’ˆå¯¹é¢„è®­ç»ƒçš„åˆ†è¯å™¨ã€‚æœ‰è¶£ï¼Œå¥½çš„ï¼Œè®©æˆ‘ä»¬è¿™æ ·åšã€‚è®©æˆ‘ä»¬çœ‹çœ‹ã€‚æŠ±æ­‰è¿›è¡Œç°åœºé»‘å®¢ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹åˆ†è¯å™¨çš„ä¸€ä¸ªå±æ€§ã€‚å®ƒè¢«ç§°ä¸ºå¡«å……ä¾§ã€‚åœ¨è¿™é‡Œé»˜è®¤æ˜¯å³ä¾§ã€‚æ‰€ä»¥æˆ‘å¯ä»¥é€šè¿‡è®¾ç½®ä¸ºå·¦ä¾§æ¥è¦†ç›–è¿™ä¸ªå±æ€§ã€‚æ‰€ä»¥è¿™ä¸æ˜¯ä¸€ä¸ªå…³é”®å­—å‚æ•°ã€‚
- en: it's an attribute of the tokenizerï¼Œ that's what I was missingã€‚And so now you
    can see indeedã€‚That we can pad to the leftã€‚So I think that should answer the question
    from SRM swimmerã€‚Thanks so much for the help with thatã€‚å—¯ã€‚ğŸ˜Šï¼Œå—¯ã€‚Thanks Dk crazy do
    you see what it's really like in the real world all right so let's see so the
    question follow up question is will it have an impact in the attention mechanism
    Okay that's a really good question soã€‚
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åˆ†è¯å™¨çš„ä¸€ä¸ªå±æ€§ï¼Œè¿™æ­£æ˜¯æˆ‘æ‰€ç¼ºå°‘çš„ã€‚æ‰€ä»¥ç°åœ¨ä½ ç¡®å®å¯ä»¥çœ‹åˆ°ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å·¦ä¾§å¡«å……ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºè¿™åº”è¯¥å›ç­”SRMæ¸¸æ³³è€…çš„é—®é¢˜ã€‚éå¸¸æ„Ÿè°¢ä½ çš„å¸®åŠ©ã€‚å—¯ã€‚ğŸ˜Šï¼Œå—¯ã€‚è°¢è°¢Dkï¼Œç–¯ç‹‚ï¼Œä½ æ˜¯å¦çœ‹åˆ°ç°å®ä¸–ç•Œçš„çœŸå®æ ·å­ï¼Œå¥½å§ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹ï¼Œåç»­é—®é¢˜æ˜¯å®ƒä¼šå¯¹æ³¨æ„æœºåˆ¶äº§ç”Ÿå½±å“å—ï¼Ÿè¿™æ˜¯ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼Œæ‰€ä»¥ã€‚
- en: ğŸ˜Šï¼ŒThe reason we haveï¼Œ so there are two things going on hereï¼Œ so on the one handã€‚å—¯ã€‚ğŸ˜Šã€‚There's
    the padding which we need to do so that we can make sure all the inputs are basically
    a rectangular arrayã€‚ and that's just the way that we need to do things like matrix
    modificationificationã€‚In the in the network andã€‚So once we introduce paddingã€‚
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘ä»¬æœ‰çš„åŸå› æ˜¯ï¼Œè¿™é‡Œæœ‰ä¸¤ä»¶äº‹æƒ…å‘ç”Ÿï¼Œä¸€æ–¹é¢ã€‚å—¯ã€‚ğŸ˜Šã€‚æœ‰å¡«å……ï¼Œæˆ‘ä»¬éœ€è¦è¿™æ ·åšï¼Œä»¥ç¡®ä¿æ‰€æœ‰è¾“å…¥åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªçŸ©å½¢æ•°ç»„ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦è¿›è¡ŒçŸ©é˜µä¿®æ”¹ç­‰æ“ä½œçš„æ–¹å¼ã€‚åœ¨ç½‘ç»œä¸­ã€‚æ‰€ä»¥ä¸€æ—¦æˆ‘ä»¬å¼•å…¥äº†å¡«å……ã€‚
- en: we introduce the problem that Sylvan mentioned in the videoã€‚ which is that the
    attention mask or sorry the attention in general will attend to every single token
    in the inputã€‚ so you know in this case here every single one of these zeros in
    principle is a token which has its own embedding and then when we calculate attention
    we basically do a pairwise multiplication of every token with every other token
    in the sequenceã€‚And that would be a problem because this would sort of say to
    the modelï¼Œ heyã€‚
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»‹ç»äº†Sylvanåœ¨è§†é¢‘ä¸­æåˆ°çš„é—®é¢˜ã€‚å³æ³¨æ„åŠ›æ©ç ï¼Œæˆ–è€…è¯´ä¸€èˆ¬çš„æ³¨æ„åŠ›å°†å…³æ³¨è¾“å…¥ä¸­çš„æ¯ä¸€ä¸ªæ ‡è®°ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™é‡Œçš„æ¯ä¸€ä¸ªé›¶åŸåˆ™ä¸Šéƒ½æ˜¯ä¸€ä¸ªå…·æœ‰è‡ªå·±åµŒå…¥çš„æ ‡è®°ï¼Œç„¶åå½“æˆ‘ä»¬è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼ŒåŸºæœ¬ä¸Šæ˜¯å°†æ¯ä¸ªæ ‡è®°ä¸åºåˆ—ä¸­çš„æ¯ä¸ªå…¶ä»–æ ‡è®°è¿›è¡Œæˆå¯¹ç›¸ä¹˜ã€‚è¿™å°†æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºè¿™ä¼šå¯¹æ¨¡å‹è¯´ï¼Œå˜¿ã€‚
- en: I've got these three tokensï¼Œ they seem to be related to each other and then
    when I construct my kind of representation at the end of the encoderã€‚ this would
    have this kind of like artificial like information from padding and we don't really
    want that because you know padding was something we just artificially injectedã€‚
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰è¿™ä¸‰ä¸ªæ ‡è®°ï¼Œå®ƒä»¬ä¼¼ä¹å½¼æ­¤ç›¸å…³ï¼Œç„¶åå½“æˆ‘åœ¨ç¼–ç å™¨æœ«å°¾æ„å»ºæˆ‘çš„è¡¨ç¤ºæ—¶ï¼Œè¿™å°†æœ‰ä¸€äº›å¡«å……çš„äººå·¥ä¿¡æ¯ï¼Œè€Œæˆ‘ä»¬å¹¶ä¸å¸Œæœ›è¿™æ ·ï¼Œå› ä¸ºå¡«å……æ˜¯æˆ‘ä»¬äººä¸ºæ³¨å…¥çš„ä¸œè¥¿ã€‚
- en: And so the thing that Sylvan mentioned is that we will get in general something
    called an attention maskã€‚ so I'm going to just call this my tokenizedã€‚Wellï¼Œ let's
    call it inputsã€‚å¯¹çš„ã€‚So if I look at my inputsã€‚ I've got input Idsã€‚And I've got an
    attention maskã€‚And this attention mask will then when we compute the attention
    inside theã€‚
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥Sylvanæåˆ°çš„äº‹æƒ…æ˜¯æˆ‘ä»¬é€šå¸¸ä¼šå¾—åˆ°ä¸€ä¸ªå«åšæ³¨æ„åŠ›æ©ç çš„ä¸œè¥¿ã€‚æ‰€ä»¥æˆ‘å°±å«å®ƒæˆ‘çš„åˆ†è¯è¿‡çš„è¾“å…¥ã€‚å¯¹çš„ã€‚æ‰€ä»¥å¦‚æœæˆ‘çœ‹æˆ‘çš„è¾“å…¥ï¼Œæˆ‘æœ‰è¾“å…¥IDï¼Œè¿˜æœ‰ä¸€ä¸ªæ³¨æ„åŠ›æ©ç ã€‚è¿™ä¸ªæ³¨æ„åŠ›æ©ç åœ¨æˆ‘ä»¬å†…éƒ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä¼šä½¿ç”¨ã€‚
- en: it will say every time you see a zeroã€‚Completely ignore that tokenã€‚And so what
    you can see is that the tokenizer has correctly figured out that if I say pat
    on the leftã€‚ then make sure that there's a mask for the first three elements or
    the first three padding tokensã€‚ so these zeros will basically be we'll say to
    hurtï¼Œ ignore this when you compute attentionã€‚
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šè¯´æ¯å½“ä½ çœ‹åˆ°ä¸€ä¸ªé›¶æ—¶ï¼Œå®Œå…¨å¿½ç•¥é‚£ä¸ªæ ‡è®°ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°åˆ†è¯å™¨æ­£ç¡®åœ°æ„è¯†åˆ°ï¼Œå¦‚æœæˆ‘è¯´å·¦è¾¹çš„patï¼Œé‚£ä¹ˆç¡®ä¿å¯¹å‰ä¸‰ä¸ªå…ƒç´ æˆ–å‰ä¸‰ä¸ªå¡«å……æ ‡è®°æœ‰ä¸€ä¸ªæ©ç ã€‚å› æ­¤ï¼Œè¿™äº›é›¶åŸºæœ¬ä¸Šä¼šå¯¹æ³¨æ„åŠ›è®¡ç®—è¯´ï¼Œå¿½ç•¥è¿™ä¸ªã€‚
- en: just compute attention on the actual words we care aboutã€‚So to answer your questionï¼Œ
    Sm Serã€‚ it doesn't have an impact because the attention mask takes care of that
    and it's all done automatically through the tokenizerã€‚Thanks I Holï¼Œ that's very
    kindã€‚ I'm doing one next weekï¼Œ So I think for session threeã€‚ you can come joinã€‚ğŸ˜Šï¼ŒOkayï¼Œ
    so let's seeã€‚ let's maybe have a look atã€‚
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: åªè®¡ç®—æˆ‘ä»¬å…³å¿ƒçš„å®é™…å•è¯çš„æ³¨æ„åŠ›ã€‚å›ç­”ä½ çš„é—®é¢˜ï¼ŒSm Serã€‚å®ƒæ²¡æœ‰å½±å“ï¼Œå› ä¸ºæ³¨æ„åŠ›æ©ç å¤„ç†äº†è¿™ä¸€åˆ‡ï¼Œéƒ½æ˜¯é€šè¿‡åˆ†è¯å™¨è‡ªåŠ¨å®Œæˆçš„ã€‚è°¢è°¢ä½ ï¼ŒHolï¼ŒçœŸå¥½ã€‚æˆ‘ä¸‹å‘¨è¿˜æœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºç¬¬ä¸‰èŠ‚ä½ å¯ä»¥æ¥å‚åŠ ã€‚ğŸ˜Šå¥½çš„ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ã€‚
- en: I think the one of the last sections we have hereã€‚Putting it all togetherï¼Œ soã€‚å—¯ã€‚Let's
    seeã€‚Yeahã€‚ let's walk through a little bit the code this of this sectionã€‚ so this
    is going to kind of bring together all the things that we've learned in this sessionã€‚And
    I'm going to justã€‚Go through the codeã€‚Soï¼Œ the ideaã€‚
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºæˆ‘ä»¬è¿™é‡Œæœ‰æœ€åå‡ èŠ‚çš„å†…å®¹ã€‚æŠŠå®ƒå…¨éƒ¨ç»“åˆèµ·æ¥ï¼Œæ‰€ä»¥ã€‚å—¯ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸€èŠ‚çš„ä»£ç ï¼Œè¿™å°†æŠŠæˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­å­¦åˆ°çš„æ‰€æœ‰å†…å®¹ç»“åˆèµ·æ¥ã€‚æˆ‘å°†é€æ­¥æµè§ˆä»£ç ã€‚æ‰€ä»¥ï¼Œæƒ³æ³•æ˜¯ã€‚
- en: Just to remember what we're doing in this chapterï¼Œ we're kind of deconstructing
    the pipelineã€‚ looking at all the pieces that go into itã€‚ And now we're going to
    sort of put this all together to sort of have our own custom pipelineã€‚So first
    thing we just load a checkpointï¼Œ a tokenizerï¼Œ and feed some inputsã€‚And in factã€‚
    I'm going to do this one hereï¼Œ which now produces with two sequencesã€‚
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦è®°ä½æˆ‘ä»¬åœ¨è¿™ä¸€ç« ä¸­åšçš„äº‹æƒ…ï¼Œæˆ‘ä»¬åœ¨è§£æ„ç®¡é“ï¼ŒæŸ¥çœ‹æ‰€æœ‰ç»„æˆéƒ¨åˆ†ã€‚ç°åœ¨æˆ‘ä»¬å°†æŠŠè¿™ä¸€åˆ‡ç»“åˆèµ·æ¥ï¼Œæ„å»ºè‡ªå·±çš„è‡ªå®šä¹‰ç®¡é“ã€‚ç¬¬ä¸€ä»¶äº‹æ˜¯åŠ è½½æ£€æŸ¥ç‚¹ã€åˆ†è¯å™¨å¹¶æä¾›ä¸€äº›è¾“å…¥ã€‚äº‹å®ä¸Šï¼Œæˆ‘è¦åšçš„è¿™ä¸ªå°†ç”Ÿæˆä¸¤ä¸ªåºåˆ—ã€‚
- en: And so now we can have a look at the different sort of ways that we could padã€‚
    So this isã€‚ I think we saw this beforeã€‚ the model inputs now are going to have
    padding tokens that go all the wayã€‚Sorryï¼Œ they match the longest in the sequenceã€‚So
    in this caseï¼Œ this is the longest inputã€‚So the first one doesn't get any padding
    tokensï¼Œ but the second guy gets all these extra zeros hereã€‚
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ä¸åŒçš„å¡«å……æ–¹å¼ã€‚æ‰€ä»¥è¿™æ˜¯ã€‚æˆ‘æƒ³æˆ‘ä»¬ä¹‹å‰è§è¿‡è¿™ä¸ªã€‚æ¨¡å‹è¾“å…¥ç°åœ¨å°†æœ‰å¡«å……æ ‡è®°ï¼Œç›´åˆ°æœ€é•¿åºåˆ—åŒ¹é…ã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™æ˜¯æœ€é•¿çš„è¾“å…¥ã€‚ç¬¬ä¸€ä¸ªæ²¡æœ‰å¡«å……æ ‡è®°ï¼Œä½†ç¬¬äºŒä¸ªåˆ™å¾—åˆ°æ‰€æœ‰è¿™äº›é¢å¤–çš„é›¶ã€‚
- en: And as we saw beforeã€‚The attention mask will get all these extra zeros to sayã€‚
    don't pay attention to those documentsã€‚Then as we saw beforeã€‚We can put max lengthã€‚
    and then this willã€‚To diplomaacyã€‚This will nowã€‚Put a bazillion zeroes on top of
    everythingã€‚ So all these zeroes we sawã€‚ So this goes up to 512 extra zeroes for
    the birdã€‚M lengthã€‚
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ã€‚æ³¨æ„æ©ç ä¼šæ·»åŠ æ‰€æœ‰è¿™äº›é¢å¤–çš„é›¶ï¼Œè¡¨ç¤ºã€‚ä¸è¦å…³æ³¨é‚£äº›æ–‡æ¡£ã€‚ç„¶åï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ã€‚æˆ‘ä»¬å¯ä»¥è®¾ç½®æœ€å¤§é•¿åº¦ã€‚ç„¶åè¿™å°†ã€‚è‡³å¤–äº¤ã€‚è¿™ç°åœ¨å°†ã€‚ç»™æ‰€æœ‰ä¸œè¥¿åŠ ä¸Šä¸€å †é›¶ã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°çš„æ‰€æœ‰è¿™äº›é›¶ã€‚æ‰€ä»¥è¿™ä¸Šå‡åˆ°512ä¸ªé¢å¤–é›¶ç”¨äºbirdã€‚Mé•¿åº¦ã€‚
- en: and then you can also configure how far you want to add padding to if you want
    toã€‚The other thing I didn't really mention so far is this concept of truncationã€‚So
    let's have a look at how that could workï¼Œ soã€‚Basicallyã€‚ what will happen most
    of the time in like unless you're dealing with like tweets or you knowã€‚
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ è¿˜å¯ä»¥é…ç½®ä½ æƒ³è¦æ·»åŠ å¡«å……çš„è·ç¦»ï¼Œå¦‚æœä½ æƒ³çš„è¯ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æ²¡æœ‰çœŸæ­£æåˆ°çš„å¦ä¸€ä»¶äº‹æ˜¯æˆªæ–­çš„æ¦‚å¿µã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™å¦‚ä½•å·¥ä½œï¼ŒåŸºæœ¬ä¸Šã€‚é€šå¸¸ä¼šå‘ç”Ÿçš„äº‹æƒ…ï¼Œé™¤éä½ åœ¨å¤„ç†åƒæ¨æ–‡é‚£æ ·çš„å†…å®¹ã€‚
- en: very short textsï¼Œ a lot of the time your inputs will exceed the maximum length
    that the transformer can processã€‚And so this is actually one of the main like
    challenges with transformers is that they're really good at like kind of short
    to medium length inputs generallyã€‚
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆçŸ­çš„æ–‡æœ¬ï¼Œå¾ˆå¤šæ—¶å€™ä½ çš„è¾“å…¥å°†è¶…è¿‡å˜æ¢å™¨å¯ä»¥å¤„ç†çš„æœ€å¤§é•¿åº¦ã€‚å› æ­¤ï¼Œè¿™å®é™…ä¸Šæ˜¯å˜æ¢å™¨çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒä»¬åœ¨å¤„ç†çŸ­åˆ°ä¸­ç­‰é•¿åº¦çš„è¾“å…¥æ—¶è¡¨ç°éå¸¸å¥½ã€‚
- en: but as we go to very long sequences there's two problemsã€‚ one is that attention
    is very computationally intensive and expensive to do and the second is that most
    of the models just predeefine what is the maximum length that the input can have
    in the pretraining phase and once you define that you can't exceed itã€‚
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å½“æˆ‘ä»¬å¤„ç†éå¸¸é•¿çš„åºåˆ—æ—¶ï¼Œä¼šæœ‰ä¸¤ä¸ªé—®é¢˜ã€‚ç¬¬ä¸€ä¸ªæ˜¯æ³¨æ„åŠ›è®¡ç®—å¯†é›†ä¸”æ˜‚è´µï¼Œç¬¬äºŒä¸ªæ˜¯å¤§å¤šæ•°æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µåªé¢„å®šä¹‰è¾“å…¥çš„æœ€å¤§é•¿åº¦ï¼Œä¸€æ—¦ä½ å®šä¹‰äº†ï¼Œå°±ä¸èƒ½è¶…è¿‡å®ƒã€‚
- en: So in this caseï¼Œ I want to show you what would happen if we did thatã€‚ so I'm
    going to take a sequence and I'm going to say I don't knowã€‚Let's take this guyã€‚Andã€‚I'm
    going to try and break the so I'm going to times thisã€‚' there's this Pro 14 tokensã€‚
    let's times it by 1 thousand0ã€‚So now I've got a really long sequenceã€‚And I want
    to seeã€‚
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æƒ³ç»™ä½ å±•ç¤ºå¦‚æœæˆ‘ä»¬è¿™æ ·åšä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ‰€ä»¥æˆ‘å°†å–ä¸€ä¸ªåºåˆ—ï¼Œç„¶åè¯´æˆ‘ä¸çŸ¥é“ã€‚è®©æˆ‘ä»¬æ‹¿è¿™ä¸ªå®¶ä¼™ã€‚ç„¶åï¼Œæˆ‘è¦å°è¯•æ‰“ç ´è¿™ä¸ªï¼Œæ‰€ä»¥æˆ‘å°†å…¶ä¹˜ä»¥ã€‚'è¿™é‡Œæœ‰è¿™ä¸ªPro
    14ä¸ªæ ‡è®°ã€‚è®©æˆ‘ä»¬ä¹˜ä»¥1000ã€‚æ‰€ä»¥ç°åœ¨æˆ‘æœ‰äº†ä¸€ä¸ªéå¸¸é•¿çš„åºåˆ—ã€‚æˆ‘æƒ³çœ‹çœ‹ã€‚
- en: What happens if I try to passã€‚This sequenceã€‚Oopsã€‚To my tokenizerã€‚Just you knowã€‚
    if I just did nothingï¼Œ what would happenï¼ŸOkayï¼Œ so now what you can see is that
    we get a warningã€‚ the token in sequence length is longer than the maximum sequence
    length of this modelã€‚Where's message to to toã€‚Soï¼Œ it's saying that we've gotã€‚You
    knowï¼Œ way more tokens than we haveã€‚
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘å°è¯•ä¼ é€’ã€‚è¿™ä¸ªåºåˆ—ã€‚å“å‘€ã€‚ç»™æˆ‘çš„åˆ†è¯å™¨ã€‚ä½ çŸ¥é“çš„ã€‚å¦‚æœæˆ‘ä»€ä¹ˆéƒ½ä¸åšï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå¥½çš„ï¼Œæ‰€ä»¥ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªè­¦å‘Šã€‚åºåˆ—ä¸­çš„æ ‡è®°é•¿åº¦è¶…è¿‡äº†è¯¥æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚åœ¨å“ªé‡Œçš„æ¶ˆæ¯ã€‚å®ƒåœ¨è¯´ï¼Œæˆ‘ä»¬æœ‰ã€‚ä½ çŸ¥é“ï¼Œè¿œè¿œè¶…è¿‡æˆ‘ä»¬æ‹¥æœ‰çš„æ ‡è®°ã€‚
- en: And then you goï¼Œ ohï¼Œ okayï¼Œ I don't care about warningsã€‚ So I'm gonna just sayã€‚Who
    caresã€‚And I'm going to try and return tensor tensesorsã€‚Okayï¼Œ so this seems to
    workã€‚ And now what happens if Iã€‚Take aã€‚Do we have a modelï¼Œ Not okayã€‚ So from transformers
    import auto modelã€‚And now what happens if I try toã€‚Do we have a checkpointï¼ŸğŸ˜”ã€‚
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ ä¼šè¯´ï¼Œå“¦ï¼Œå¥½å§ï¼Œæˆ‘ä¸åœ¨ä¹è­¦å‘Šã€‚æ‰€ä»¥æˆ‘å°†è¯´ã€‚è°åœ¨ä¹ã€‚æˆ‘å°†å°è¯•è¿”å›å¼ é‡ã€‚å¥½çš„ï¼Œæ‰€ä»¥è¿™ä¼¼ä¹æœ‰æ•ˆã€‚é‚£ä¹ˆå¦‚æœæˆ‘ã€‚æ‹¿ä¸€ä¸ªã€‚æˆ‘ä»¬æœ‰æ¨¡å‹å—ï¼Œä¸å¥½ã€‚æ‰€ä»¥ä»transformerså¯¼å…¥è‡ªåŠ¨æ¨¡å‹ã€‚é‚£ä¹ˆå¦‚æœæˆ‘å°è¯•ã€‚æˆ‘ä»¬æœ‰æ£€æŸ¥ç‚¹å—ï¼ŸğŸ˜”
- en: Where is my checkpointï¼ŸYesï¼Œ I' check goodã€‚Let's try to load my modelã€‚So what
    I'm trying to do here is I'm trying to seeã€‚ I want to show you what happens if
    we just naively try to pass a 14000ã€‚ALong sequence to bothã€‚And so nowï¼Œ if I'm
    not mistakenï¼Œ our inputsã€‚Should break the modelã€‚Indeedã€‚
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ£€æŸ¥ç‚¹åœ¨å“ªé‡Œï¼Ÿæ˜¯çš„ï¼Œæˆ‘æ£€æŸ¥è¿‡äº†ã€‚è®©æˆ‘ä»¬å°è¯•åŠ è½½æˆ‘çš„æ¨¡å‹ã€‚æ‰€ä»¥æˆ‘åœ¨è¿™é‡Œæƒ³åšçš„æ˜¯ï¼Œæˆ‘æƒ³çœ‹çœ‹ã€‚å¦‚æœæˆ‘ä»¬å¤©çœŸåœ°å°è¯•ä¼ é€’ä¸€ä¸ª14000ã€‚é•¿åºåˆ—ç»™ä¸¤è€…ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘æ²¡æé”™ï¼Œæˆ‘ä»¬çš„è¾“å…¥ã€‚åº”è¯¥æ‰“ç ´æ¨¡å‹ã€‚ç¡®å®ã€‚
- en: So here we can see we've got an index errorï¼Œ the index error is out of rangeã€‚And
    that is essentially trying to say to usï¼Œ lookï¼Œ you tried to do this embedding
    operation somewhere in the codeã€‚And you are trying to pass an input which violates
    the constraints set by the modelã€‚And so this is an example whereï¼Œ you knowï¼Œ we
    just broke the model because we gave it something that was too longã€‚
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬é‡åˆ°äº†ç´¢å¼•é”™è¯¯ï¼Œç´¢å¼•é”™è¯¯è¶…å‡ºèŒƒå›´ã€‚åŸºæœ¬ä¸Šè¿™æ˜¯åœ¨å‘Šè¯‰æˆ‘ä»¬ï¼Œçœ‹ï¼Œä½ è¯•å›¾åœ¨ä»£ç çš„æŸä¸ªåœ°æ–¹æ‰§è¡Œè¿™ä¸ªåµŒå…¥æ“ä½œã€‚ä½ æ­£åœ¨å°è¯•ä¼ é€’ä¸€ä¸ªè¿åæ¨¡å‹çº¦æŸçš„è¾“å…¥ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°±è¿™æ ·æ‰“ç ´äº†æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬ç»™äº†å®ƒè¿‡é•¿çš„ä¸œè¥¿ã€‚
- en: The solution to that is to use the truncation parent parameter and set it to
    trueã€‚And what this will do is this will nowã€‚Take our inputsï¼Œ and truncate themã€‚So
    let's get the input IDsã€‚And thenï¼Œ the sizeã€‚And this will nowã€‚This has now converted
    them to the maximum size of the model or trunceted them to the maximum sizeã€‚And
    so now I can feed this to my model and it's happyã€‚
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æˆªæ–­çˆ¶å‚æ•°å¹¶å°†å…¶è®¾ç½®ä¸ºtrueã€‚è¿™æ ·åšå°†ä¼šå¤„ç†æˆ‘ä»¬çš„è¾“å…¥å¹¶è¿›è¡Œæˆªæ–­ã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆè·å–è¾“å…¥IDï¼Œç„¶åæ˜¯å¤§å°ã€‚ç°åœ¨è¿™å·²ç»å°†å®ƒä»¬è½¬æ¢ä¸ºæ¨¡å‹çš„æœ€å¤§å¤§å°æˆ–æˆªæ–­åˆ°æœ€å¤§å¤§å°ã€‚å› æ­¤æˆ‘å¯ä»¥å°†å…¶è¾“å…¥åˆ°æˆ‘çš„æ¨¡å‹ä¸­ï¼Œå®ƒä¼šå¾ˆé«˜å…´ã€‚
- en: So this is one way you can deal with the the problem of when your text is too
    longã€‚ you can just truncate itã€‚ and my experience is generally that truncation
    works okay forã€‚Like classification tasksï¼Œ so if you're doing like you know multiclass
    with this kind of thing generally like a lot of the information is actually at
    the start of the review or the tweetã€‚But you don't want to do this for things
    like question answering because you know the answer might be somewhere towards
    the end of the text and then if you truncate it you lose it and we'll probably
    see later on in anotherã€‚
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§å¤„ç†æ–‡æœ¬è¿‡é•¿é—®é¢˜çš„æ–¹æ³•ã€‚ä½ å¯ä»¥ç›´æ¥æˆªæ–­å®ƒã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼Œæˆªæ–­é€šå¸¸å¯¹åˆ†ç±»ä»»åŠ¡æ•ˆæœä¸é”™ã€‚æ‰€ä»¥å¦‚æœä½ åšçš„æ˜¯å¤šç±»ä»»åŠ¡ï¼Œé€šå¸¸å¾ˆå¤šä¿¡æ¯å®é™…ä¸Šåœ¨è¯„è®ºæˆ–æ¨æ–‡çš„å¼€å¤´ã€‚ä½†å¯¹äºé—®ç­”è¿™ç±»ä»»åŠ¡å°±ä¸åº”è¯¥è¿™æ ·åšï¼Œå› ä¸ºç­”æ¡ˆå¯èƒ½åœ¨æ–‡æœ¬çš„åé¢ï¼Œå¦‚æœæˆªæ–­äº†å°±ä¼šä¸¢å¤±ï¼Œä¹‹åæˆ‘ä»¬å¯èƒ½ä¼šåœ¨å…¶ä»–åœ°æ–¹çœ‹åˆ°ã€‚
- en: Iteration of the courseï¼Œ how you handle thatã€‚And also in summarizationã€‚ it's
    a little bit depends on the caseï¼Œ sometimes I've been able to truncate and get
    okay resultsã€‚ but sometimes you have to do clever things like you knowï¼Œ split
    the text into different piecesã€‚ truncate those piecesï¼Œ and then kind of aggregate
    the resultsã€‚Okayã€‚
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹çš„è¿­ä»£ï¼Œä½ æ˜¯å¦‚ä½•å¤„ç†çš„ï¼Ÿè¿˜æœ‰åœ¨æ€»ç»“æ—¶ã€‚è¿™æœ‰ç‚¹å–å†³äºå…·ä½“æƒ…å†µï¼Œæœ‰æ—¶æˆ‘èƒ½æˆªæ–­æ–‡æœ¬å¹¶å¾—åˆ°ä¸é”™çš„ç»“æœï¼Œä½†æœ‰æ—¶ä½ éœ€è¦åšä¸€äº›å·§å¦™çš„äº‹æƒ…ï¼Œæ¯”å¦‚æŠŠæ–‡æœ¬åˆ†æˆä¸åŒçš„éƒ¨åˆ†ã€‚æˆªæ–­è¿™äº›éƒ¨åˆ†ï¼Œç„¶åå†æ±‡æ€»ç»“æœã€‚å¥½çš„ã€‚
- en: so that's truncation and paddingã€‚å—¯ã€‚And yeahï¼Œ I thinkã€‚Yeahã€‚I think that's pretty
    much what we need to do for thisã€‚So are there any questions at the stageï¼ŸOkayã€‚
    so one thing I would like to mention isã€‚![](img/40873acb06abf924ac4a43fae802679a_56.png)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯æˆªæ–­å’Œå¡«å……ã€‚å—¯ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºã€‚è¿™åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬éœ€è¦åšçš„ã€‚é‚£ä¹ˆè¿™ä¸ªé˜¶æ®µæœ‰æ²¡æœ‰é—®é¢˜ï¼Ÿå¥½çš„ã€‚æˆ‘æƒ³æåˆ°çš„ä¸€ä»¶äº‹æ˜¯ã€‚![](img/40873acb06abf924ac4a43fae802679a_56.png)
- en: You've probably seen we have the forums and on the forumsï¼Œ we have a course
    categoryã€‚![](img/40873acb06abf924ac4a43fae802679a_58.png)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»çœ‹åˆ°æˆ‘ä»¬æœ‰è®ºå›ï¼Œåœ¨è®ºå›ä¸Šï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯¾ç¨‹åˆ†ç±»ã€‚![](img/40873acb06abf924ac4a43fae802679a_58.png)
- en: So if you have any questions that come to mind after this session or just questions
    in general about transformersã€‚ just you can ask them here and one of us will respondã€‚I
    think a really cool thing to do if you have the time is to share your projectsã€‚
    so in fact we can see DK Cr De here sharing his awesome data set for the Model
    hubã€‚
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ åœ¨è¿™æ¬¡ä¼šè®®åæƒ³åˆ°ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…ä¸€èˆ¬å…³äºå˜å‹å™¨çš„é—®é¢˜ï¼Œå¯ä»¥åœ¨è¿™é‡Œè¯¢é—®ï¼Œæˆ‘ä»¬ä¸­çš„ä¸€ä¸ªäººä¼šå›å¤ä½ ã€‚å¦‚æœä½ æœ‰æ—¶é—´ï¼Œåˆ†äº«ä½ çš„é¡¹ç›®æ˜¯ä¸€ä¸ªå¾ˆé…·çš„äº‹æƒ…ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°DK
    Cr Deåœ¨è¿™é‡Œåˆ†äº«ä»–å‡ºè‰²çš„æ•°æ®é›†ï¼Œç”¨äºæ¨¡å‹ä¸­å¿ƒã€‚
- en: And you know as you start learning transformersï¼Œ a really cool way to sort of
    get some feedback on how you're going is to kind of share your work and at least
    for me personallyã€‚ you know I come from a non-computer science background I studied
    physics and then decided to switch into machine learning for meã€‚
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¼€å§‹å­¦ä¹ å˜å‹å™¨æ—¶ï¼Œè·å¾—åé¦ˆçš„ä¸€ç§å¾ˆé…·çš„æ–¹æ³•å°±æ˜¯åˆ†äº«ä½ çš„å·¥ä½œï¼Œè‡³å°‘å¯¹æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ¥è‡ªéè®¡ç®—æœºç§‘å­¦èƒŒæ™¯ï¼Œæˆ‘å­¦ä¹ äº†ç‰©ç†ï¼Œç„¶åå†³å®šè½¬å‘æœºå™¨å­¦ä¹ ã€‚
- en: this kind of sharing was a very powerful way of getting some feedback from the
    community and also trying to like you know learn how to communicateã€‚Which is a
    really important part of you know doing any data science in the real world or
    in generalã€‚Soï¼Œ for exampleï¼Œ a cool experiment would be this one that Tom was asked
    today of likeã€‚What happens if I change the con figure of the modelï¼Œ does it break
    thingsã€‚
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åˆ†äº«æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ–¹å¼ï¼Œå¯ä»¥è·å¾—ç¤¾åŒºçš„åé¦ˆï¼ŒåŒæ—¶ä¹Ÿåœ¨å­¦ä¹ å¦‚ä½•æ²Ÿé€šã€‚è¿™æ˜¯è¿›è¡Œä»»ä½•æ•°æ®ç§‘å­¦å·¥ä½œéå¸¸é‡è¦çš„ä¸€éƒ¨åˆ†ã€‚æ‰€ä»¥ï¼Œæ¯”å¦‚ï¼Œä»Šå¤©æ±¤å§†è¢«é—®åˆ°çš„ä¸€ä¸ªæœ‰è¶£å®éªŒæ˜¯ï¼Œæ”¹å˜æ¨¡å‹çš„é…ç½®ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå®ƒä¼šå‡ºé”™å—ã€‚
- en: That would be a cool thing to show or in generalã€‚Likeï¼Œ you knowã€‚ any any sort
    of training experiments you do reveal awesome awesomeã€‚ç»§ç»­ã€‚Okayã€‚ there's one last
    question from Rash Mashikï¼Œ how to check the default modelã€‚Okayã€‚ so let's have
    a look at thisã€‚Okayï¼Œ soã€‚Let's take a pipelineã€‚
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå±•ç¤ºçš„é…·ç‚¹ï¼Œæˆ–è€…ä¸€èˆ¬æ¥è¯´ã€‚ä½ çŸ¥é“ï¼Œä»»ä½•ä½ è¿›è¡Œçš„è®­ç»ƒå®éªŒéƒ½ä¼šæ­ç¤ºå‡ºéå¸¸æ£’çš„å†…å®¹ã€‚ç»§ç»­ã€‚å¥½çš„ï¼Œè¿˜æœ‰ä¸€ä¸ªæ¥è‡ªæ‹‰ä»€Â·é©¬å¸Œå…‹çš„æœ€åä¸€ä¸ªé—®é¢˜ï¼Œå¦‚ä½•æ£€æŸ¥é»˜è®¤æ¨¡å‹ã€‚å¥½çš„ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªã€‚å¥½çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬æ‹¿ä¸€ä¸ªç®¡é“ã€‚
- en: So I probably need to go from transformersã€‚Import pipelineã€‚And then I'm going
    to create a pipelineã€‚For sentiment analysisã€‚For exampleã€‚So the question isã€‚How
    do we check what model is being used so the pipe line object it has a bunch of
    different attributes and the attribute that is interesting is the modelã€‚In this
    caseï¼Œ so if we look at thisï¼Œ it's going to tell usï¼Œ okayï¼Œ it's like some outputã€‚
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¯èƒ½éœ€è¦ä»transformerså¯¼å…¥ç®¡é“ã€‚ç„¶åæˆ‘å°†åˆ›å»ºä¸€ä¸ªæƒ…æ„Ÿåˆ†æçš„ç®¡é“ã€‚ä¾‹å¦‚ã€‚é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬å¦‚ä½•æ£€æŸ¥æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹ï¼Œå› æ­¤ç®¡é“å¯¹è±¡æœ‰è®¸å¤šä¸åŒçš„å±æ€§ï¼Œè€Œæœ‰è¶£çš„å±æ€§æ˜¯æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªï¼Œå®ƒä¼šå‘Šè¯‰æˆ‘ä»¬ï¼Œå¥½çš„ï¼Œè¾“å‡ºæ˜¯è¿™æ ·çš„ã€‚
- en: but maybe we can just look at the config of the modelã€‚And then we can see that
    in this caseã€‚ the model that we're using for sentiment analysis by default is
    distillbert and it's this checkpoint that we saw earlier in the classã€‚So I hope
    that answers the question Rash Mashikï¼Œ you can do this for any other pipelineã€‚
    let's have a quick look what happens if I do question answeringã€‚ã†ã†ã„ã€‚
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥å…ˆæŸ¥çœ‹æ¨¡å‹çš„é…ç½®ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé»˜è®¤ç”¨äºæƒ…æ„Ÿåˆ†æçš„æ¨¡å‹æ˜¯distillbertï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬åœ¨è¯¾å ‚ä¸Šçœ‹åˆ°çš„é‚£ä¸ªæ£€æŸ¥ç‚¹ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›è¿™èƒ½å›ç­”æ‹‰ä»€Â·é©¬å¸Œå…‹çš„é—®é¢˜ï¼Œä½ å¯ä»¥å¯¹ä»»ä½•å…¶ä»–ç®¡é“åšè¿™ä¸ªã€‚æˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹å¦‚æœæˆ‘è¿›è¡Œé—®ç­”ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ã†ã†ã„ã€‚
- en: So the default model in question answering is going to beã€‚Distillber base case
    on squadã€‚So that's the long we haveã€‚Coolï¼Œ so there's another question are the
    recordings available for session one from Silvanã€‚ I was not able to find it on
    the YouTube channelï¼Œ I believeã€‚ğŸ˜Šï¼ŒThey will beã€‚ but I guess I'll have to check
    with Silvin laterã€‚ So I'm pretty sure we try to put everything on YouTube that
    we canã€‚
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨é—®ç­”ä¸­çš„é»˜è®¤æ¨¡å‹å°†æ˜¯DistillberåŸºäºsquadçš„æ¡ˆä¾‹ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„ã€‚é…·ï¼Œæ‰€ä»¥è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œè¥¿å°”ä¸‡çš„ç¬¬ä¸€åœºä¼šè®®çš„å½•éŸ³æ˜¯å¦å¯ç”¨ã€‚æˆ‘åœ¨YouTubeé¢‘é“ä¸Šæ‰¾ä¸åˆ°å®ƒï¼Œæˆ‘ç›¸ä¿¡ã€‚ğŸ˜Šï¼Œä¼šæœ‰çš„ã€‚ä½†æˆ‘æƒ³æˆ‘å¾—ç¨åå’Œè¥¿å°”æ–‡ç¡®è®¤ã€‚æ‰€ä»¥æˆ‘å¾ˆç¡®å®šæˆ‘ä»¬å°½é‡å°†èƒ½æ”¾ä¸ŠYouTubeçš„å†…å®¹éƒ½æ”¾ä¸Šå»ã€‚
- en: soã€‚I'll let you know it upã€‚So yeah thanks a lot for your really cool questionsã€‚
    it's a real pleasure having people interact otherwise I'll just be talking to
    the screen by myselfã€‚Thanks for your inputã€‚Andã€‚We'll see you for the next sessionã€‚
    so Sylvane is giving the session tomorrowã€‚Which is the same one as today and next
    week we kick off with chapter3ã€‚
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä¼šè®©ä½ çŸ¥é“è¿™ä¸ªæƒ…å†µã€‚æ‰€ä»¥éå¸¸æ„Ÿè°¢ä½ ä»¬æå‡ºçš„éå¸¸é…·çš„é—®é¢˜ã€‚èƒ½å¤Ÿä¸å¤§å®¶äº’åŠ¨çœŸæ˜¯ä¸€ç§å¿«ä¹ï¼Œå¦åˆ™æˆ‘å°±åªèƒ½ä¸€ä¸ªäººå¯¹ç€å±å¹•è¯´è¯ã€‚æ„Ÿè°¢ä½ çš„å‚ä¸ã€‚æˆ‘ä»¬ä¸‹æ¬¡è§ã€‚æ‰€ä»¥è¥¿å°”ä¸‡æ˜å¤©ä¼šè¿›è¡Œä¼šè®®ï¼Œè¿™å’Œä»Šå¤©æ˜¯ä¸€æ ·çš„ï¼Œä¸‹å‘¨æˆ‘ä»¬å°†å¼€å§‹ç¬¬ä¸‰ç« ã€‚
- en: '![](img/40873acb06abf924ac4a43fae802679a_60.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40873acb06abf924ac4a43fae802679a_60.png)'
- en: ì•„ã€‚![](img/40873acb06abf924ac4a43fae802679a_62.png)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ì•„ã€‚![](img/40873acb06abf924ac4a43fae802679a_62.png)
