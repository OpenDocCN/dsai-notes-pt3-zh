- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P63ï¼šL12.2- ç”¨äºæ¸¸æˆå­¦ä¹ çš„Q-Learningç®—æ³•ç®€ä»‹
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P63ï¼šL12.2- ç”¨äºæ¸¸æˆå­¦ä¹ çš„Q-Learningç®—æ³•ç®€ä»‹
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Haytonã€‚ welcome to applications of deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ we're going to build on using Gã€‚ We saw
    open AI gym in the previous videoã€‚ Now we're going to apply Q learning to thatã€‚
    And we're going to see how we can build up using machine learning a basic lookup
    table that shows us what action to take for any state that the environment might
    work onã€‚ Then we're going to continue this and show how deep neural network can
    become a very complex representation of a table so that we don't have to store
    literally every combination in thereã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·æµ·é¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†åŸºäºä½¿ç”¨ Gã€‚æˆ‘ä»¬åœ¨ä¸Šä¸€ä¸ªè§†é¢‘ä¸­çœ‹åˆ°äº† Open AI gymã€‚ç°åœ¨æˆ‘ä»¬å°†å¯¹å…¶åº”ç”¨
    Q å­¦ä¹ ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åˆ©ç”¨æœºå™¨å­¦ä¹ æ„å»ºä¸€ä¸ªåŸºæœ¬çš„æŸ¥æ‰¾è¡¨ï¼Œä»¥å±•ç¤ºç¯å¢ƒå¯èƒ½ä½œç”¨äºçš„ä»»ä½•çŠ¶æ€åº”é‡‡å–çš„è¡ŒåŠ¨ã€‚ç„¶åæˆ‘ä»¬å°†ç»§ç»­å±•ç¤ºæ·±åº¦ç¥ç»ç½‘ç»œå¦‚ä½•æˆä¸ºä¸€ä¸ªéå¸¸å¤æ‚çš„è¡¨çš„è¡¨ç¤ºï¼Œä»è€Œæ— éœ€é€ä¸ªå­˜å‚¨æ¯ç§ç»„åˆã€‚
- en: And that begins the deep Q neural network to see all my videos about Cale neural
    networks and other AI topicsã€‚ Click the subscribe button and the bell next to
    it and select all to be notified of every new videoã€‚ So let's have a look at Q
    learningã€‚ I'm going to run this in Google coababã€‚ This really does not require
    GPUã€‚ The code here actually is not even designed to take advantage of a GPã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ‡å¿—ç€æ·±åº¦ Q ç¥ç»ç½‘ç»œçš„å¼€å§‹ã€‚æƒ³è¦æŸ¥çœ‹æˆ‘å…³äº Cale ç¥ç»ç½‘ç»œå’Œå…¶ä»– AI ä¸»é¢˜çš„æ‰€æœ‰è§†é¢‘ï¼Œè¯·ç‚¹å‡»è®¢é˜…æŒ‰é’®å’Œæ—è¾¹çš„é“ƒé“›ï¼Œå¹¶é€‰æ‹©å…¨éƒ¨ä»¥æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹
    Q å­¦ä¹ ã€‚æˆ‘å°†åœ¨ Google coabab ä¸­è¿è¡Œè¿™ä¸ªã€‚è¿™å®é™…ä¸Šå¹¶ä¸éœ€è¦ GPUã€‚è¿™é‡Œçš„ä»£ç ç”šè‡³æ²¡æœ‰è®¾è®¡ç”¨æ¥åˆ©ç”¨ GPUã€‚
- en: ğŸ˜Šã€‚![](img/30d4f6c3c88418548a35fcff3628ad27_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šã€‚![](img/30d4f6c3c88418548a35fcff3628ad27_1.png)
- en: I'm going to give you just a Python implementation of doing a tablebased Q learning
    lookupã€‚ which is really kind of how Q learning works In the next partã€‚ we will
    extend this and we'll actually use Kra's TF agents to actually implement this
    in deep learning and then go of completely beyond this and use something that
    has a much more complex environment like an Atari game So here I have the setup
    for Google Coab we definitely want to use Tensorflow 2ã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ç»™ä½ ä¸€ä¸ªåŸºäº Python çš„è¡¨æ ¼ Q å­¦ä¹ æŸ¥æ‰¾å®ç°ã€‚è¿™å®é™…ä¸Šå°±æ˜¯ Q å­¦ä¹ çš„å·¥ä½œåŸç†ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ‰©å±•è¿™ä¸ªå¹¶ä¸”å®é™…ä½¿ç”¨ Kra çš„ TF
    agents åœ¨æ·±åº¦å­¦ä¹ ä¸­å®ç°è¿™ä¸€ç‚¹ï¼Œç„¶åå®Œå…¨è¶…è¶Šè¿™ä¸€ç‚¹ï¼Œä½¿ç”¨æ›´å¤æ‚çš„ç¯å¢ƒï¼Œæ¯”å¦‚ä¸€ä¸ª Atari æ¸¸æˆã€‚è¿™é‡Œæ˜¯ Google Coab çš„è®¾ç½®ï¼Œæˆ‘ä»¬è‚¯å®šè¦ä½¿ç”¨
    Tensorflow 2ã€‚
- en: 0 and I have some additional code here that if you're running coabab will execute
    because coab at least as it was as of the recording of this video does not include
    TF agents and some other things that I want so that I'm able to display videos
    literally write in the browser here TF agents provides a very good way to take
    a snapshot of just a single image from an environment running as it's rendering
    and show it to you there's other waysã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›é¢å¤–çš„ä»£ç ï¼Œå¦‚æœä½ åœ¨è¿è¡Œ coababï¼Œå®ƒå°†ä¼šæ‰§è¡Œï¼Œå› ä¸ºæˆªè‡³å½•åˆ¶è¿™æ®µè§†é¢‘æ—¶ï¼Œcoab å¹¶ä¸åŒ…å« TF agents å’Œä¸€äº›æˆ‘æƒ³è¦çš„å…¶ä»–åŠŸèƒ½ï¼Œè¿™æ ·æˆ‘æ‰èƒ½åœ¨æµè§ˆå™¨ä¸­ç›´æ¥å±•ç¤ºè§†é¢‘ã€‚TF
    agents æä¾›äº†ä¸€ç§éå¸¸å¥½çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç¯å¢ƒæ¸²æŸ“æ—¶ä»…ä»å•ä¸€å›¾åƒä¸­æ•æ‰å¿«ç…§å¹¶å±•ç¤ºç»™ä½ ï¼Œè¿˜æœ‰å…¶ä»–æ–¹å¼ã€‚
- en: Get around itï¼Œ but TF agents makes that real easy other than that we're not
    using TF agents for this videoã€‚ We'll use TF agents in the next oneã€‚ I will go
    ahead and run this partã€‚ it will say note using Google coab once this actually
    runs that I run this part to install all of the libraries that I would like to
    have this can take a bit so we'll fast forward through it Now Q learning there's
    a couple of terms that I'll refer to as we're going through this So it's good
    to go ahead and go through those now agent think of this in terms of a video game
    although in the very last part of this chapter we will do something that's not
    a video game but the agent that's your player that your computer player that is
    going through the environment playing the video game now the environment is the
    universe that the agent exists in the environment has state and the state of the
    environment changes as the agent interacts with it and the agent interacts with
    the environment byã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç»•è¿‡è¿™ä¸ªï¼Œä½†æ˜¯TFä»£ç†ä½¿å¾—è¿™ä¸€åˆ‡å˜å¾—å¾ˆç®€å•ï¼Œé™¤æ­¤ä¹‹å¤–æˆ‘ä»¬åœ¨è¿™ä¸ªè§†é¢‘ä¸­ä¸ä½¿ç”¨TFä»£ç†ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ä½¿ç”¨TFä»£ç†ã€‚æˆ‘å°†ç»§ç»­è¿è¡Œè¿™ä¸€éƒ¨åˆ†ã€‚å®ƒä¼šæ˜¾ç¤ºä½¿ç”¨Google
    Colabï¼Œä¸€æ—¦è¿™ä¸ªéƒ¨åˆ†çœŸæ­£è¿è¡Œï¼Œæˆ‘å°±ä¼šè¿è¡Œè¿™éƒ¨åˆ†ä»¥å®‰è£…æˆ‘å¸Œæœ›æ‹¥æœ‰çš„æ‰€æœ‰åº“ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šå¿«è¿›ã€‚åœ¨Qå­¦ä¹ ä¸­ï¼Œæœ‰å‡ ä¸ªæœ¯è¯­æˆ‘ä¼šåœ¨æˆ‘ä»¬è¿›è¡Œæ—¶æåˆ°ï¼Œæ‰€ä»¥ç°åœ¨äº†è§£è¿™äº›æ˜¯æœ‰å¥½å¤„çš„ã€‚ä»£ç†å¯ä»¥æƒ³è±¡æˆä¸€ä¸ªè§†é¢‘æ¸¸æˆï¼Œå°½ç®¡åœ¨æœ¬ç« çš„æœ€åéƒ¨åˆ†æˆ‘ä»¬å°†åšä¸€äº›ä¸æ˜¯è§†é¢‘æ¸¸æˆçš„äº‹æƒ…ï¼Œä½†ä»£ç†æ˜¯ä½ çš„ç©å®¶ï¼Œå³åœ¨ç¯å¢ƒä¸­è¿›è¡Œæ¸¸æˆçš„è®¡ç®—æœºç©å®¶ã€‚ç¯å¢ƒæ˜¯ä»£ç†å­˜åœ¨çš„å®‡å®™ï¼Œç¯å¢ƒæœ‰çŠ¶æ€ï¼Œéšç€ä»£ç†çš„äº’åŠ¨ï¼Œç¯å¢ƒçš„çŠ¶æ€ä¼šå˜åŒ–ï¼Œè€Œä»£ç†é€šè¿‡ä¸ç¯å¢ƒçš„äº’åŠ¨æ¥ä¸ä¹‹äº’åŠ¨ã€‚
- en: '![](img/30d4f6c3c88418548a35fcff3628ad27_3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30d4f6c3c88418548a35fcff3628ad27_3.png)'
- en: '![](img/30d4f6c3c88418548a35fcff3628ad27_4.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30d4f6c3c88418548a35fcff3628ad27_4.png)'
- en: First of allï¼Œ just seeing the state of the environment in this part this videoã€‚
    the environment state is just going to be a vector of some numbers that tell us
    the position of a cart or how fast it's going things like that but when we get
    to Atari the environment is actually a grid that's the video screen as the game
    is being played a step is one cycle through the environment so you take an action
    that's like move upã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåœ¨è¿™ä¸ªè§†é¢‘çš„è¿™ä¸€éƒ¨åˆ†ï¼Œçœ‹çœ‹ç¯å¢ƒçš„çŠ¶æ€ã€‚ç¯å¢ƒçŠ¶æ€åªæ˜¯ä¸€ä¸ªæ•°å­—å‘é‡ï¼Œå‘Šè¯‰æˆ‘ä»¬å°è½¦çš„ä½ç½®æˆ–é€Ÿåº¦ç­‰ä¿¡æ¯ï¼Œä½†å½“æˆ‘ä»¬è¿›å…¥Atariæ—¶ï¼Œç¯å¢ƒå®é™…ä¸Šæ˜¯ä¸€ä¸ªç½‘æ ¼ï¼Œå°±æ˜¯è§†é¢‘å±å¹•ï¼Œæ¸¸æˆæ­£åœ¨è¿›è¡Œï¼Œä¸€æ­¥æ˜¯é€šè¿‡ç¯å¢ƒçš„ä¸€ä¸ªå‘¨æœŸï¼Œæ‰€ä»¥ä½ é‡‡å–ä¸€ä¸ªåŠ¨ä½œï¼Œå°±åƒå‘ä¸Šç§»åŠ¨ã€‚
- en: move downï¼Œ move leftï¼Œ move right and then see what that does to the environment
    and one complete cycle of performing some action and seeing the results that's
    a step all of the steps until you accomplish your goal or get killed or the simulation
    ends that's called an episode so that's like one play of the video game an Epoch
    is a training construct it can be some number of episodes depending on how you
    configure figure things and then the terminal state is the step that you get to
    that causes the game to endã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ä¸‹ç§»åŠ¨ï¼Œå‘å·¦ç§»åŠ¨ï¼Œå‘å³ç§»åŠ¨ï¼Œç„¶åçœ‹çœ‹è¿™å¯¹ç¯å¢ƒäº§ç”Ÿäº†ä»€ä¹ˆå½±å“ã€‚ä¸€ä¸ªå®Œæ•´çš„æ‰§è¡ŒæŸä¸ªåŠ¨ä½œå¹¶æŸ¥çœ‹ç»“æœçš„å‘¨æœŸå°±æ˜¯ä¸€æ­¥ï¼Œæ‰€æœ‰æ­¥éª¤ç›´åˆ°ä½ å®Œæˆç›®æ ‡æˆ–è¢«æ€æ­»ï¼Œæˆ–è€…æ¨¡æ‹Ÿç»“æŸï¼Œè¿™å«åšä¸€ä¸ªå›åˆï¼Œè¿™å°±åƒæ˜¯ä¸€åœºè§†é¢‘æ¸¸æˆçš„æ¸¸ç©ã€‚ä¸€è½®æ˜¯ä¸€ä¸ªè®­ç»ƒæ„é€ ï¼Œå®ƒå¯ä»¥æ˜¯ä¸€äº›å›åˆï¼Œå…·ä½“å–å†³äºä½ å¦‚ä½•é…ç½®ï¼Œç„¶åç»ˆæ­¢çŠ¶æ€æ˜¯å¯¼è‡´æ¸¸æˆç»“æŸçš„æ­¥éª¤ã€‚
- en: you winï¼Œ you run out of livesï¼Œ something like thatã€‚ So Q learning basically
    builds up table so that every state in this environmentã€‚ it gives us a indication
    of which action is going to be the most rewarding because as you're going through
    this you get a reward at the end of each step and that tells you how how effective
    that step was some gains are mean they don't give you any reward until the veryã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ èµ¢äº†ï¼Œç”Ÿå‘½ç”¨å°½ï¼Œç±»ä¼¼çš„æƒ…å†µã€‚æ‰€ä»¥Qå­¦ä¹ åŸºæœ¬ä¸Šå»ºç«‹äº†ä¸€ä¸ªè¡¨ï¼Œä»¥ä¾¿åœ¨è¿™ä¸ªç¯å¢ƒä¸­çš„æ¯ä¸ªçŠ¶æ€ç»™æˆ‘ä»¬æŒ‡ç¤ºå“ªä¸ªåŠ¨ä½œå°†æ˜¯æœ€æœ‰å¥–åŠ±çš„ï¼Œå› ä¸ºåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä½ åœ¨æ¯ä¸€æ­¥ç»“æŸæ—¶ä¼šè·å¾—å¥–åŠ±ï¼Œè¿™å‘Šè¯‰ä½ è¿™ä¸€æ­¥çš„æœ‰æ•ˆæ€§ã€‚æœ‰äº›æ¸¸æˆæ„å‘³ç€åœ¨æœ€åæ‰ä¼šç»™ä½ å¥–åŠ±ã€‚
- en: very end and only if you want itã€‚ those can be a little harder to optimize forã€‚
    but Q learning still does thatã€‚ that would be almost like going through your foury
    degree and at the very endã€‚ either you get a diploma or you don't And if you don't
    who knows why you did something wrongã€‚ But that's the torture that we put computers
    throughã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨ä½ æƒ³è¦çš„æ—¶å€™æ‰ä¼šåˆ°è¾¾æœ€åçš„ç»ˆç‚¹ã€‚é‚£äº›å¯èƒ½ä¼šæœ‰ç‚¹éš¾ä»¥ä¼˜åŒ–ã€‚ä½†Qå­¦ä¹ ä»ç„¶å¯ä»¥åšåˆ°ã€‚è¿™å‡ ä¹å°±åƒä½ å®Œæˆå››å¹´å­¦ä½è¯¾ç¨‹ï¼Œæœ€åè¦ä¹ˆè·å¾—æ–‡å‡­ï¼Œè¦ä¹ˆä¸è·å¾—ã€‚å¦‚æœæ²¡æœ‰ï¼Œè°çŸ¥é“ä½ åšé”™äº†ä»€ä¹ˆã€‚ä½†è¿™å°±æ˜¯æˆ‘ä»¬è®©è®¡ç®—æœºç»å†çš„æŠ˜ç£¨ã€‚
- en: So we're going to work with something called the mountain carã€‚ This is a classic
    reinforcement learning hello world kind of thing it is essentially a mountain
    car Now if you run this code hereã€‚ I'm not going toã€‚On it because it's already
    renderedã€‚ But this will show you one frame of whatever environment you're specifyingã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå«åšå±±åœ°è½¦çš„ä¸œè¥¿ã€‚è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„å¼ºåŒ–å­¦ä¹ â€œä½ å¥½ï¼Œä¸–ç•Œâ€ç±»å‹çš„ä¸œè¥¿ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯å±±åœ°è½¦ã€‚ç°åœ¨å¦‚æœä½ åœ¨è¿™é‡Œè¿è¡Œè¿™æ®µä»£ç ï¼Œæˆ‘ä¸ä¼šè¿è¡Œå®ƒï¼Œå› ä¸ºå®ƒå·²ç»æ¸²æŸ“äº†ã€‚ä½†è¿™ä¼šå‘ä½ å±•ç¤ºä½ æ‰€æŒ‡å®šçš„ç¯å¢ƒçš„ä¸€ä¸ªå¸§ã€‚
- en: I'm specifying the mountain carã€‚ If I put in space invadersï¼Œ it would draw the
    space invadersã€‚ all the little aliens coming down to be blastedã€‚ So this is how
    mountain car worksã€‚ The idea of this game is your mountain car essentiallyï¼Œ you
    can provide forward thrustã€‚ So you really only have three actions you can take
    One action is no actionã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ­£åœ¨æŒ‡å®šå±±åœ°è½¦ã€‚å¦‚æœæˆ‘è¾“å…¥å¤ªç©ºä¾µç•¥è€…ï¼Œå®ƒå°†ç»˜åˆ¶å¤ªç©ºä¾µç•¥è€…ã€‚æ‰€æœ‰å°å¤–æ˜Ÿäººéƒ½åœ¨ä¸‹æ¥è¢«å‡»æ¯ã€‚æ‰€ä»¥è¿™å°±æ˜¯å±±åœ°è½¦çš„å·¥ä½œåŸç†ã€‚è¿™ä¸ªæ¸¸æˆçš„æƒ³æ³•æ˜¯ä½ çš„å±±åœ°è½¦æœ¬è´¨ä¸Šå¯ä»¥æä¾›å‘å‰çš„æ¨åŠ›ã€‚å› æ­¤ä½ å®é™…ä¸Šåªæœ‰ä¸‰ä¸ªå¯ä»¥é‡‡å–çš„è¡ŒåŠ¨ï¼Œå…¶ä¸­ä¸€ä¸ªè¡ŒåŠ¨æ˜¯æ— åŠ¨ä½œã€‚
- en: And then the other two actions are either thrust this way up the hill or thrust
    this way up this side of the hillã€‚ The trick is your little car is just not strong
    enough to make it up the hillã€‚ try to just blast up the hillã€‚ I'll show you that
    in a momentã€‚ You're not that successfulã€‚ So what you've got to do is blast up
    the hillã€‚ get as far as you can go and then you'll roll back but you'll roll back
    and build up some potential energy up the hill on this sideã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¦å¤–ä¸¤ä¸ªåŠ¨ä½œè¦ä¹ˆæ˜¯å‘è¿™è¾¹åŠ é€Ÿä¸Šå¡ï¼Œè¦ä¹ˆæ˜¯å‘è¿™è¾¹åŠ é€Ÿä¸Šå¡ã€‚è¯€çªæ˜¯ä½ çš„è½¦å­åŠ›é‡ä¸å¤Ÿï¼Œæ— æ³•ä¸Šå¡ã€‚è¯•ç€ç›´æ¥å†²ä¸Šå»ã€‚æˆ‘ç¨åä¼šç»™ä½ å±•ç¤ºã€‚ä½ å¹¶ä¸å¤ªæˆåŠŸã€‚æ‰€ä»¥ä½ éœ€è¦åšçš„æ˜¯å†²ä¸Šå¡ï¼Œå°½å¯èƒ½å¾€ä¸Šèµ°ï¼Œç„¶åä½ ä¼šå›æ»šï¼Œä½†ä½ ä¼šåœ¨è¿™ä¸ªå¡çš„ä¸€ä¾§ç§¯ç´¯ä¸€äº›åŠ¿èƒ½ã€‚
- en: and then you chargeï¼Œ charge straight through and goã€‚ğŸ˜Šï¼ŒUp againã€‚ maybe usually
    you'll make it in two shotsã€‚ but the idea is to get all the way up here using
    potential energy from that other hill and conserving your momentum thereã€‚ so you
    can apply a left forceï¼Œ you can apply no force or you can apply a right force
    So those are the numbers that specify those discrete actions the way the Q table
    works you can only apply a discrete action So these are the three things there's
    no like apply maximum force left apply half force like your car like your accelerator
    when you press that down flooring the car gives you a completely different result
    than tapping it this is possible with Q learning but you need to use some special
    tricks and other thingsã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ åŠ é€Ÿï¼Œç›´æ¥å†²ä¸Šå»ã€‚ğŸ˜Š å†å¾€ä¸Šã€‚é€šå¸¸ä½ å¯èƒ½ä¼šåœ¨ä¸¤æ¬¡å°è¯•ä¸­å®Œæˆã€‚ä½†æƒ³æ³•æ˜¯åˆ©ç”¨æ¥è‡ªå¦ä¸€ä¸ªå±±ä¸˜çš„åŠ¿èƒ½ï¼Œä¿æŒä½ çš„åŠ¨é‡ï¼Œä»è€Œä¸€è·¯åˆ°è¾¾è¿™é‡Œã€‚å› æ­¤ï¼Œä½ å¯ä»¥æ–½åŠ å·¦ä¾§çš„åŠ›é‡ï¼Œæˆ–è€…ä¸æ–½åŠ åŠ›é‡ï¼Œæˆ–è€…æ–½åŠ å³ä¾§çš„åŠ›é‡ã€‚è¿™äº›æ•°å­—æŒ‡å®šäº†è¿™äº›ç¦»æ•£åŠ¨ä½œçš„æ–¹å¼ï¼ŒQè¡¨çš„å·¥ä½œåŸç†æ˜¯ä½ åªèƒ½æ–½åŠ ç¦»æ•£åŠ¨ä½œã€‚å› æ­¤è¿™ä¸‰ä»¶äº‹æƒ…æ˜¯æ²¡æœ‰åƒæ–½åŠ æœ€å¤§å·¦ä¾§åŠ›é‡æˆ–æ–½åŠ ä¸€åŠåŠ›é‡çš„æƒ…å†µï¼Œåƒä½ çš„æ±½è½¦ä¸€æ ·ï¼Œå½“ä½ æŒ‰ä¸‹æ²¹é—¨æ—¶ï¼Œå…¨åŠ›åŠ é€Ÿç»™ä½ å®Œå…¨ä¸åŒçš„ç»“æœï¼Œè€Œè½»è½»è¸©ä¸‹åˆ™æ˜¯å¦ä¸€å›äº‹ã€‚è¿™åœ¨Qå­¦ä¹ ä¸­æ˜¯å¯èƒ½çš„ï¼Œä½†ä½ éœ€è¦ä½¿ç”¨ä¸€äº›ç‰¹æ®Šçš„æŠ€å·§å’Œå…¶ä»–ä¸œè¥¿ã€‚
- en: we'll get into that when we get into deep Q learning using the deep neural network
    but typically it's assumed that these are discrete actions but continuous actions
    are fine you just have to use different algorithms for those and in our very last
    part of this module willã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è¿›å…¥ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„æ·±åº¦Qå­¦ä¹ æ—¶ï¼Œæˆ‘ä»¬ä¼šè¯¦ç»†è®²è§£è¿™ä¸ªï¼Œä½†é€šå¸¸å‡è®¾è¿™äº›æ˜¯ç¦»æ•£åŠ¨ä½œï¼Œä½†è¿ç»­åŠ¨ä½œä¹Ÿå¯ä»¥ï¼Œåªéœ€ä¸ºè¿™äº›ä½¿ç”¨ä¸åŒçš„ç®—æ³•ã€‚åœ¨æœ¬æ¨¡å—çš„æœ€åéƒ¨åˆ†æˆ‘ä»¬å°†è®¨è®ºã€‚
- en: Look at an application of deep neural networksï¼Œ deep queue networks that is
    not a gameã€‚ and it does take continuous actionsã€‚ Nowï¼Œ the mountain car environmentã€‚
    essentially all you have is the position of this carã€‚ I believe zero is right
    here and then negative and positive and then the velocityã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹çœ‹æ·±åº¦ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªåº”ç”¨ï¼Œæ·±åº¦Qç½‘ç»œï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ¸¸æˆã€‚å®ƒç¡®å®é‡‡å–äº†è¿ç»­åŠ¨ä½œã€‚ç°åœ¨ï¼Œå±±åœ°è½¦ç¯å¢ƒã€‚åŸºæœ¬ä¸Šä½ æ‰€æ‹¥æœ‰çš„å°±æ˜¯è¿™è¾†è½¦çš„ä½ç½®ã€‚æˆ‘ç›¸ä¿¡é›¶å°±åœ¨è¿™é‡Œï¼Œç„¶åæ˜¯è´Ÿå€¼å’Œæ­£å€¼ï¼Œè¿˜æœ‰é€Ÿåº¦ã€‚
- en: How fast is it going and that's positive or negative too and those are continuous
    valuesã€‚ So the velocity that you're going this way or the velocity that you're
    going that wayã€‚ negative versus positiveã€‚ Now this is using some code that allows
    you to actually visualize it and see it go up I don't have the code the video
    from last timeã€‚ So let me go ahead and run thisã€‚ So I'm going to run this and
    what this does is it gives you a couple of functions to show the video and actually
    see the video in a web browser which is handy because a lot of these you'll require
    a GPU and you may want to actually run those in Google coab to get advantage of
    the free GPU but you can also display it in a browser if you're running it locally
    or a little screenã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçš„é€Ÿåº¦æœ‰å¤šå¿«ï¼Œè¿™ä¹Ÿæ˜¯æ­£å€¼æˆ–è´Ÿå€¼ï¼Œè¿™äº›éƒ½æ˜¯è¿ç»­çš„å€¼ã€‚å› æ­¤ï¼Œä½ å¾€è¿™è¾¹èµ°çš„é€Ÿåº¦æˆ–å¾€é‚£è¾¹èµ°çš„é€Ÿåº¦ï¼Œè´Ÿå€¼ä¸æ­£å€¼ã€‚ç°åœ¨è¿™æ˜¯ä½¿ç”¨ä¸€äº›ä»£ç ï¼Œå…è®¸ä½ å®é™…å¯è§†åŒ–å¹¶çœ‹åˆ°å®ƒä¸Šå‡ï¼Œæˆ‘æ²¡æœ‰ä¸Šæ¬¡è§†é¢‘ä¸­çš„ä»£ç ã€‚è®©æˆ‘æ¥è¿è¡Œè¿™ä¸ªã€‚å› æ­¤ï¼Œæˆ‘å°†è¿è¡Œå®ƒï¼Œå®ƒç»™ä½ æä¾›å‡ ä¸ªå‡½æ•°æ¥æ˜¾ç¤ºè§†é¢‘ï¼Œå®é™…ä¸Šå¯ä»¥åœ¨ç½‘é¡µæµè§ˆå™¨ä¸­æŸ¥çœ‹è§†é¢‘ï¼Œè¿™å¾ˆæ–¹ä¾¿ï¼Œå› ä¸ºå¾ˆå¤šæ—¶å€™ä½ éœ€è¦ä¸€ä¸ªGPUï¼Œä½ å¯èƒ½æƒ³åœ¨Google
    Colabä¸Šè¿è¡Œè¿™äº›ä»¥åˆ©ç”¨å…è´¹çš„GPUï¼Œä½†å¦‚æœä½ æœ¬åœ°è¿è¡Œï¼Œä¹Ÿå¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºã€‚
- en: ğŸ˜Šï¼Œ'll typically pop up and show it to youã€‚ So now let me go ahead and run itã€‚
    So what this doesã€‚ This is a mountain car that very stubbornly just tries to apply
    maximum acceleration to the rightã€‚ So it just it just floors it well there is
    no maximum it just pushes with full power to the rightã€‚ tries to scale the mountain
    and ultimately is not strong enoughã€‚ Now this output you see hereã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œé€šå¸¸ä¼šå¼¹å‡ºå¹¶å±•ç¤ºç»™ä½ ã€‚é‚£ä¹ˆç°åœ¨è®©æˆ‘æ¥è¿è¡Œå®ƒã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é¡½å›ºçš„å±±åœ°è½¦ï¼Œè¯•å›¾å°†æœ€å¤§åŠ é€Ÿåº¦æ–½åŠ åˆ°å³ä¾§ã€‚æ‰€ä»¥å®ƒå°±æ˜¯å…¨åŠ›ä»¥èµ´åœ°å¾€å³æ¨ã€‚è¯•å›¾æ”€ç™»å±±å³°ï¼Œæœ€ç»ˆåŠ›é‡ä¸è¶³ã€‚ç°åœ¨ä½ çœ‹åˆ°çš„è¿™ä¸ªè¾“å‡ºã€‚
- en: you can see the stateã€‚ So the state is showing essentially where the car so
    it's velocity how fast it's goingã€‚ it kind of reaches full velocity and varies
    a little bitã€‚ this is a little bit randomã€‚ And then this is the positionã€‚ you
    can see it's basically going further and further and further up the hill and at
    some point yeah right around thereã€‚ it starts to slide back downã€‚ pictureture
    is worth a thousand wordsã€‚ So if you look at thisã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°çŠ¶æ€ã€‚å› æ­¤çŠ¶æ€æ˜¾ç¤ºäº†æ±½è½¦çš„ä½ç½®ï¼Œå®ƒçš„é€Ÿåº¦æœ‰å¤šå¿«ã€‚å®ƒè¾¾åˆ°å®Œå…¨é€Ÿåº¦å¹¶æœ‰æ‰€æ³¢åŠ¨ï¼Œè¿™æœ‰ç‚¹éšæœºã€‚ç„¶åè¿™æ˜¯ä½ç½®ã€‚ä½ å¯ä»¥çœ‹åˆ°å®ƒåŸºæœ¬ä¸Šè¶Šæ¥è¶Šå¾€å±±ä¸Šèµ°ï¼Œåœ¨æŸä¸ªæ—¶å€™ï¼Œç¡®å®å°±åœ¨é‚£é™„è¿‘ï¼Œå®ƒå¼€å§‹æ»‘å›ä¸‹æ¥ã€‚ä¸€å¹…å›¾èƒœè¿‡åƒè¨€ä¸‡è¯­ã€‚æ‰€ä»¥å¦‚æœä½ çœ‹è¿™ä¸ªã€‚
- en: this is what it does it tries it tries who's the Greek guy that pushes the boulder
    up the hill sisyphus if I remember my history classes is rightã€‚That's basically
    an impossible taskã€‚ Nowï¼Œ what I do is I use old school programmingã€‚ I program
    a car to solve thisã€‚ And for a simple case like thisã€‚ yeahã€‚ I can I can write
    a completely deterministic programï¼Œ just give it some rulesã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å®ƒçš„ä½œç”¨ï¼Œè¯•å›¾ï¼Œè¯•å›¾æ˜¯è°çš„å¸Œè…Šäººï¼ŒæŠŠåœ†çŸ³æ¨ä¸Šå±±ï¼Ÿå¦‚æœæˆ‘æ²¡è®°é”™çš„è¯æ˜¯è¥¿è¥¿å¼—æ–¯ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªä¸å¯èƒ½å®Œæˆçš„ä»»åŠ¡ã€‚ç°åœ¨ï¼Œæˆ‘ç”¨ä¼ ç»Ÿç¼–ç¨‹çš„æ–¹æ³•ã€‚æˆ‘ç¼–å†™äº†ä¸€ä¸ªè½¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¯¹äºè¿™æ ·çš„ç®€å•æƒ…å†µï¼Œæ˜¯çš„ã€‚æˆ‘å¯ä»¥å†™ä¸€ä¸ªå®Œå…¨ç¡®å®šæ€§çš„ç¨‹åºï¼Œåªéœ€ç»™å®ƒä¸€äº›è§„åˆ™ã€‚
- en: and the rule essentially that I'm going to look atã€‚ I'm going to look at the
    stateã€‚ and I'm going to only pay attention to the stateã€‚ I am not going to pay
    attention to the velocityã€‚ I don't careã€‚ I want to know just the position of the
    stateã€‚ and this position if the position is greater than0 zeroã€‚ So this wayã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è§„åˆ™åŸºæœ¬ä¸Šæ˜¯æˆ‘å°†è¦è§‚å¯Ÿçš„çŠ¶æ€ã€‚æˆ‘åªå…³æ³¨çŠ¶æ€ï¼Œä¸ä¼šå…³æ³¨é€Ÿåº¦ã€‚æˆ‘ä¸åœ¨ä¹ã€‚æˆ‘åªæƒ³çŸ¥é“çŠ¶æ€çš„ä½ç½®ã€‚å¦‚æœè¿™ä¸ªä½ç½®å¤§äºé›¶ï¼Œå°±è¿™æ ·ã€‚
- en: then I'm going to push this wayã€‚ if the car is going this wayã€‚ I am going to
    push this way and try to getï¼Œ try to get it as far up this side as possible to
    build up as much potential energy as it possibly canã€‚ and the program to do this
    is really pretty simpleã€‚ If the position So the first element of the state is
    greater than zeroï¼Œ I apply toã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å°†å‘è¿™ä¸ªæ–¹å‘æ¨ã€‚å¦‚æœè½¦æ˜¯å¾€è¿™è¾¹èµ°ï¼Œæˆ‘å°†å‘è¿™ä¸ªæ–¹å‘æ¨ï¼Œå°½é‡è®©å®ƒå°½å¯èƒ½åœ°å¾€è¿™ä¸€è¾¹èµ°ï¼Œä»¥ç§¯ç´¯å°½å¯èƒ½å¤šçš„åŠ¿èƒ½ã€‚è¿™ä¸ªç¨‹åºå…¶å®éå¸¸ç®€å•ã€‚å¦‚æœä½ç½®ï¼ŒçŠ¶æ€çš„ç¬¬ä¸€ä¸ªå…ƒç´ å¤§äºé›¶ï¼Œæˆ‘å°±æ–½åŠ ã€‚
- en: which is push it to the rightã€‚ Otherwiseï¼Œ I apply zeroï¼Œ which is push it to
    the leftã€‚ One is no power at allã€‚ I don't believe any bra is apply butã€‚We really
    don't need to break in order to achieve our goal or ever let up on the engineã€‚
    So here we run itã€‚ We see the same stepsã€‚ Notice there is no rewardã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨å‘å³è¾¹ã€‚å¦åˆ™ï¼Œæˆ‘æ–½åŠ é›¶ï¼Œå³æ¨å‘å·¦è¾¹ã€‚ä¸€æ˜¯å®Œå…¨æ²¡æœ‰åŠ¨åŠ›ã€‚æˆ‘è®¤ä¸ºæ²¡æœ‰ä»»ä½•åˆ¹è½¦æ–½åŠ ï¼Œä½†æˆ‘ä»¬çœŸçš„ä¸éœ€è¦åˆ¹è½¦æ¥å®ç°æˆ‘ä»¬çš„ç›®æ ‡ï¼Œæˆ–è€…è®©å‘åŠ¨æœºå‡é€Ÿã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œè¿è¡Œå®ƒã€‚æˆ‘ä»¬çœ‹åˆ°ç›¸åŒçš„æ­¥éª¤ã€‚æ³¨æ„æ²¡æœ‰å¥–åŠ±ã€‚
- en: This is like trying to get through your four years of college and the only feedback
    you get as you graduate or you don'tã€‚ but we get all the away to the endã€‚ eventuallyï¼Œ
    ohï¼Œ and I don't display the last stepsã€‚ So you don't even see your reward that
    you that you ultimately get when this thing winsã€‚ But let's take a look at how
    my programmed car worksã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±åƒè¯•å›¾å®Œæˆä½ å››å¹´çš„å¤§å­¦ï¼Œè€Œä½ å”¯ä¸€å¾—åˆ°çš„åé¦ˆæ˜¯ä½ æ¯•ä¸šäº†è¿˜æ˜¯æ²¡æœ‰ã€‚ä½†æˆ‘ä»¬ä¼šä¸€ç›´åˆ°æœ€åï¼Œæœ€ç»ˆï¼Œå“¦ï¼Œæˆ‘æ²¡æœ‰å±•ç¤ºæœ€åçš„æ­¥éª¤ã€‚æ‰€ä»¥ä½ ç”šè‡³çœ‹ä¸åˆ°ä½ æœ€ç»ˆå¾—åˆ°çš„å¥–åŠ±ï¼Œå½“è¿™ä¸ªä¸œè¥¿è·èƒœæ—¶ã€‚ä½†è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ç¼–ç¨‹çš„æ±½è½¦æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: You can see it's a bit more successful and it winsã€‚ All rightï¼Œ that is a hardcod
    ruleã€‚ Imagine if the Tesla engineer is set there and try to think of every possible
    case that would ever come up when you're driving an actual carã€‚ it would be quite
    miserableã€‚ and it just wouldn't workã€‚ You would never think of everythingã€‚ I meanã€‚
    this is almost a1 B world like flatwor because I can't go really up or downã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°å®ƒå˜å¾—æ›´æˆåŠŸå¹¶ä¸”èµ¢å¾—äº†æ¯”èµ›ã€‚å¥½å§ï¼Œé‚£æ˜¯ä¸€ä¸ªç¡¬ç¼–ç çš„è§„åˆ™ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœç‰¹æ–¯æ‹‰çš„å·¥ç¨‹å¸ˆååœ¨é‚£é‡Œï¼Œè¯•å›¾è€ƒè™‘åˆ°ä½ åœ¨å®é™…é©¾é©¶æ±½è½¦æ—¶å¯èƒ½å‡ºç°çš„æ¯ä¸€ç§æƒ…å†µï¼Œé‚£å°†æ˜¯ç›¸å½“ç—›è‹¦çš„ã€‚è€Œä¸”è¿™æ ¹æœ¬è¡Œä¸é€šã€‚ä½ æ°¸è¿œä¹Ÿæƒ³ä¸åˆ°æ‰€æœ‰çš„äº‹æƒ…ã€‚æˆ‘çš„æ„æ€æ˜¯ï¼Œè¿™å‡ ä¹æ˜¯ä¸€ä¸ª
    A1 B ä¸–ç•Œï¼Œåƒæ˜¯å¹³å¦çš„ä¸–ç•Œï¼Œå› ä¸ºæˆ‘ä¸èƒ½çœŸæ­£å‘ä¸Šæˆ–å‘ä¸‹ç§»åŠ¨ã€‚
- en: I'm only going left and rightã€‚ So reinforcement learningã€‚ how this worksã€‚ is
    we have our agent and we have the environmentã€‚Agent takes some action on the environmentã€‚
    And there's two outputs from thatã€‚ There is the state for the next frameã€‚ and
    then there's the reward that you receive from that action for the mountain carã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªåœ¨å·¦å³ç§»åŠ¨ã€‚å› æ­¤ï¼Œå¼ºåŒ–å­¦ä¹ çš„å·¥ä½œæ–¹å¼æ˜¯æˆ‘ä»¬æœ‰ä»£ç†å’Œç¯å¢ƒã€‚ä»£ç†åœ¨ç¯å¢ƒä¸­é‡‡å–ä¸€äº›åŠ¨ä½œã€‚ç„¶åä»ä¸­æœ‰ä¸¤ä¸ªè¾“å‡ºã€‚ä¸€ä¸ªæ˜¯ä¸‹ä¸€ä¸ªå¸§çš„çŠ¶æ€ï¼Œå¦ä¸€ä¸ªæ˜¯ä½ ä»è¿™ä¸ªåŠ¨ä½œä¸­è·å¾—çš„å¥–åŠ±ï¼Œé’ˆå¯¹å±±åœ°æ±½è½¦ã€‚
- en: you only receive a reward when you actually succeed and make it all the way
    up the hillã€‚ Nowã€‚ I'll mention this briefly because this is sometimes done in
    reinforcement learning is if the environment doesn't necessarily give you a rewardã€‚
    you can sometimes get greater effects by engineering your own rewardã€‚ And that
    might be doing something like sayingï¼Œ okayã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªæœ‰åœ¨æˆåŠŸå¹¶ä¸”å®Œå…¨ä¸Šå±±æ—¶æ‰èƒ½è·å¾—å¥–åŠ±ã€‚ç°åœ¨ï¼Œæˆ‘ä¼šç®€è¦æä¸€ä¸‹ï¼Œå› ä¸ºåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæœ‰æ—¶å¦‚æœç¯å¢ƒä¸ä¸€å®šç»™ä½ å¥–åŠ±ï¼Œä½ å¯ä»¥é€šè¿‡è®¾è®¡è‡ªå·±çš„å¥–åŠ±æ¥è·å¾—æ›´å¤§çš„æ•ˆæœã€‚è¿™å¯èƒ½æ˜¯åšä¸€äº›äº‹æƒ…ï¼Œæ¯”å¦‚è¯´ï¼Œå¥½çš„ã€‚
- en: I'm going to give it a reward for maybe getting up the hill further than it's
    ever gotten before or somethingã€‚ So use your your mindï¼Œ your heuristics as a human
    to actually come up with some intermediate reward just maybe to give it betterã€‚
    better resultsã€‚ Now this line here means that training occurs and it adjusts itã€‚
    And then the next state goes into the agentã€‚ and it continuesã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šç»™å®ƒä¸€ä¸ªå¥–åŠ±ï¼Œå¯èƒ½æ˜¯å› ä¸ºå®ƒæ¯”ä»¥å‰ä¸Šå±±æ›´è¿›ä¸€æ­¥ä¹‹ç±»çš„ã€‚æ‰€ä»¥ç”¨ä½ ä½œä¸ºäººç±»çš„æ€ç»´å’Œå¯å‘å¼ï¼Œå®é™…æƒ³å‡ºä¸€äº›ä¸­é—´å¥–åŠ±ï¼Œä»¥ä¾¿ç»™å®ƒæ›´å¥½çš„ç»“æœã€‚ç°åœ¨è¿™æ¡çº¿è¡¨ç¤ºè®­ç»ƒå‘ç”Ÿå¹¶ä¸”è¿›è¡Œè°ƒæ•´ã€‚ç„¶åä¸‹ä¸€ä¸ªçŠ¶æ€è¿›å…¥ä»£ç†ï¼Œç»§ç»­è¿›è¡Œã€‚
- en: It continues and continues and continuesã€‚Nowï¼Œ this equation is how Q learning
    is typically representedã€‚ It looks kind of big and crazy if you've not seen a
    lot of these kind of equations beforeã€‚ but it's not that badã€‚ And I want to take
    you through the parts of it because this is actually quite importantã€‚ So the way
    Q learning works is it's keeping that tableã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç»§ç»­ï¼Œä¸€ç›´ç»§ç»­ï¼Œç°åœ¨ï¼Œè¿™ä¸ªæ–¹ç¨‹æ˜¯ Q å­¦ä¹ é€šå¸¸çš„è¡¨ç¤ºæ–¹å¼ã€‚å¦‚æœä½ ä¹‹å‰æ²¡è§è¿‡å¾ˆå¤šè¿™ç§æ–¹ç¨‹ï¼Œå®ƒçœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚å’Œç–¯ç‹‚ï¼Œä½†å…¶å®æ²¡é‚£ä¹ˆç³Ÿã€‚æˆ‘æƒ³å¸¦ä½ é€æ­¥äº†è§£å®ƒçš„å„ä¸ªéƒ¨åˆ†ï¼Œå› ä¸ºè¿™å®é™…ä¸Šç›¸å½“é‡è¦ã€‚å› æ­¤ï¼ŒQ
    å­¦ä¹ çš„å·¥ä½œæ–¹å¼æ˜¯ä¿æŒé‚£ä¸ªè¡¨æ ¼ã€‚
- en: So a table for every possible state that the environment can be inã€‚ Now in this
    caseã€‚ this is a grid of all the possible positions and all the possible velocities
    because those are your two states that you have and state Tã€‚ that just means all
    of those statesã€‚ So S is essentially a array holding those two values in the case
    of this problemã€‚ and for each action that you might takeã€‚ So each of those different
    states in that gridã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä¸ºç¯å¢ƒå¯èƒ½å¤„äºçš„æ¯ä¸€ä¸ªçŠ¶æ€å‡†å¤‡ä¸€ä¸ªè¡¨æ ¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™æ˜¯æ‰€æœ‰å¯èƒ½ä½ç½®å’Œæ‰€æœ‰å¯èƒ½é€Ÿåº¦çš„ç½‘æ ¼ï¼Œå› ä¸ºè¿™å°±æ˜¯ä½ æ‹¥æœ‰çš„ä¸¤ä¸ªçŠ¶æ€ï¼Œä»¥åŠçŠ¶æ€ Tã€‚è¿™æ„å‘³ç€æ‰€æœ‰è¿™äº›çŠ¶æ€ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªé—®é¢˜ä¸­ï¼ŒS
    å®è´¨ä¸Šæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œä¿å­˜è¿™ä¸¤ä¸ªå€¼ï¼Œé’ˆå¯¹ä½ å¯èƒ½é‡‡å–çš„æ¯ä¸ªåŠ¨ä½œã€‚å› æ­¤ï¼Œåœ¨é‚£ä¸ªç½‘æ ¼ä¸­çš„æ¯ä¸ªä¸åŒçŠ¶æ€ã€‚
- en: there's three actions you can takeã€‚ So this is really a cube every value on
    the grid is essentially the three actions and anticipated reward that you would
    get for for being thereã€‚ So what this is doing what this is saying is theã€‚Value
    is going to be changedã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é‡‡å–ä¸‰ç§åŠ¨ä½œã€‚æ‰€ä»¥è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªç«‹æ–¹ä½“ï¼Œç½‘æ ¼ä¸Šçš„æ¯ä¸ªå€¼æœ¬è´¨ä¸Šæ˜¯ä¸‰ç§åŠ¨ä½œä»¥åŠä½ åœ¨é‚£é‡Œçš„é¢„æœŸå¥–åŠ±ã€‚æ‰€ä»¥è¿™å®é™…ä¸Šæ˜¯è¯´ï¼Œå€¼å°†ä¼šè¢«æ”¹å˜ã€‚
- en: It is essentially going to be added toã€‚ So this this old valueï¼Œ this is almost
    like in programmingã€‚ a equals a plus plus something else plus sub delta a equals
    a plus1ã€‚ if this whole thing was oneã€‚ So this whole part over here is the delta
    We're calculating how much we want to change that Q value of Iã€‚ This is very typical
    of machine learningã€‚ This is how I don't even know how many algorithms I studied
    have exactly this formatã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šå°†ä¼šè¢«æ·»åŠ ã€‚æ‰€ä»¥è¿™ä¸ªæ—§å€¼ï¼Œå‡ ä¹å°±åƒç¼–ç¨‹ä¸­çš„é‚£æ ·ã€‚a ç­‰äº a åŠ ä¸ŠæŸäº›å…¶ä»–å€¼åŠ ä¸Šå­ delta a ç­‰äº a åŠ  1ã€‚å¦‚æœæ•´ä¸ªäº‹æƒ…æ˜¯ä¸€ä¸ªã€‚æ‰€ä»¥è¿™é‡Œçš„æ•´ä¸ªéƒ¨åˆ†å°±æ˜¯æˆ‘ä»¬è®¡ç®—çš„
    deltaï¼Œæˆ‘ä»¬è¦æ”¹å˜çš„ Q å€¼ã€‚è¿™åœ¨æœºå™¨å­¦ä¹ ä¸­éå¸¸å…¸å‹ã€‚æˆ‘ç”šè‡³ä¸çŸ¥é“æˆ‘å­¦ä¹ è¿‡å¤šå°‘ç®—æ³•å…·æœ‰å®Œå…¨ç›¸åŒçš„æ ¼å¼ã€‚
- en: Like the weight in a neural networkï¼Œ the weight equals the weight plus essentially
    the gradient times a learning rateã€‚ The learning rate is there to scale it backã€‚
    Learn to walk before you learn to runã€‚ If you didn't have this learning rate hereï¼Œ
    then this whole big value would just be flipping those Q values all over the placeã€‚
    It would be too aggressive of a changeã€‚ And it would be like the learning algorithm
    got super excited about every little new thing that it learned that it completely
    overrode most of what it had learned previouslyã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒç¥ç»ç½‘ç»œä¸­çš„æƒé‡ï¼Œæƒé‡ç­‰äºæƒé‡åŠ ä¸Šæ¢¯åº¦ä¹˜ä»¥å­¦ä¹ ç‡ã€‚å­¦ä¹ ç‡ç”¨äºç¼©æ”¾å®ƒã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªå­¦ä¹ ç‡ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•´ä¸ªå¤§çš„å€¼å°±ä¼šåœ¨å„ä¸ªåœ°æ–¹ç¿»è½¬è¿™äº› Q å€¼ã€‚è¿™æ ·ä¼šæ˜¯ä¸€ä¸ªè¿‡äºæ¿€è¿›çš„å˜åŒ–ã€‚å°±åƒå­¦ä¹ ç®—æ³•å¯¹å®ƒæ‰€å­¦åˆ°çš„æ¯ä¸€ä¸ªå°æ–°äº‹ç‰©éƒ½è¶…çº§å…´å¥‹ï¼Œä»¥è‡³äºå®Œå…¨è¦†ç›–äº†ä¹‹å‰å­¦åˆ°çš„å¤§éƒ¨åˆ†å†…å®¹ã€‚
- en: So your alpha is your learning rateã€‚ I use a learning rate in this example of01ã€‚
    So one10th of whatever we're going to calculate for this delta is what we're going
    to applyã€‚ And they call it a temporal differenceã€‚ I took this right from Wikipediaã€‚
    The equationã€‚ Now you get back a rewardã€‚ So let's look at how that reward is everything
    hinges on the reward actually happensã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ çš„ alpha æ˜¯ä½ çš„å­¦ä¹ ç‡ã€‚æˆ‘åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨çš„å­¦ä¹ ç‡æ˜¯ 0.1ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¦è®¡ç®—çš„è¿™ä¸ª delta çš„ååˆ†ä¹‹ä¸€å°±æ˜¯æˆ‘ä»¬è¦åº”ç”¨çš„ã€‚ä»–ä»¬ç§°ä¹‹ä¸ºæ—¶é—´å·®ã€‚æˆ‘ç›´æ¥ä»ç»´åŸºç™¾ç§‘ä¸Šæ‹¿çš„è¿™ä¸ªæ–¹ç¨‹ã€‚ç°åœ¨ä½ å¾—åˆ°äº†ä¸€ä¸ªå¥–åŠ±ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå¥–åŠ±ï¼Œå› ä¸ºä¸€åˆ‡éƒ½ä¾èµ–äºå¥–åŠ±å®é™…ä¸Šæ˜¯å¦‚ä½•å‘ç”Ÿçš„ã€‚
- en: I like to look at these equationsã€‚ğŸ˜Šï¼Œin blocks and then rip the block apartã€‚
    So inside of this delta that is being multiplied by the learning rateï¼Œ there's
    really two partsã€‚ Notice this old valueã€‚ That's the same old value that you add
    hereã€‚ You could do some algebraic simplification and do some canceling and not
    have thatã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å–œæ¬¢æŠŠè¿™äº›æ–¹ç¨‹çœ‹ä½œæ˜¯å—ï¼Œç„¶åæŠŠå—æ’•å¼€ã€‚æ‰€ä»¥åœ¨è¿™ä¸ªä¹˜ä»¥å­¦ä¹ ç‡çš„ delta å†…éƒ¨ï¼Œå®é™…ä¸Šæœ‰ä¸¤ä¸ªéƒ¨åˆ†ã€‚æ³¨æ„è¿™ä¸ªæ—§å€¼ã€‚é‚£æ˜¯ä½ åœ¨è¿™é‡Œæ·»åŠ çš„ç›¸åŒæ—§å€¼ã€‚ä½ å¯ä»¥åšä¸€äº›ä»£æ•°ç®€åŒ–å’Œæ¶ˆå»ï¼Œè€Œä¸éœ€è¦é‚£ä¸ªã€‚
- en: but that's just the way this is writtenã€‚ basically we want this whole thing
    to be a delta to be added to thisã€‚ so we have to subtract off the old value so
    that basically this is our new valueã€‚ This is the old value so the difference
    between these twoã€‚ We desire to get to hear this new valueã€‚ So we have to subtract
    the old value from it so that we need to see the magnitude changeã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™åªæ˜¯å†™æˆè¿™æ ·çš„æ–¹å¼ã€‚åŸºæœ¬ä¸Šæˆ‘ä»¬å¸Œæœ›æ•´ä¸ªä¸œè¥¿æ˜¯ä¸€ä¸ª delta è¢«æ·»åŠ åˆ°è¿™ä¸ªã€‚å› æ­¤æˆ‘ä»¬å¿…é¡»å‡å»æ—§å€¼ï¼Œè¿™æ ·åŸºæœ¬ä¸Šè¿™æ˜¯æˆ‘ä»¬çš„æ–°å€¼ã€‚è¿™æ˜¯æ—§å€¼ï¼Œæ‰€ä»¥è¿™ä¸¤è€…ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬å¸Œæœ›å¾—åˆ°è¿™ä¸ªæ–°å€¼ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦ä»ä¸­å‡å»æ—§å€¼ï¼Œä»¥ä¾¿çœ‹åˆ°å¹…åº¦çš„å˜åŒ–ã€‚
- en: and then we scale it by the learning rate and at itã€‚ So let's look at the new
    value because that's the interesting partã€‚ that is just setting up the deltaã€‚
    We need to know how much we want to change it byã€‚ So this is the theoretical new
    value that we would like to eventually take it toã€‚ So the rewardã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ç”¨å­¦ä¹ ç‡è¿›è¡Œç¼©æ”¾å¹¶æ·»åŠ å®ƒã€‚å› æ­¤è®©æˆ‘ä»¬çœ‹çœ‹æ–°å€¼ï¼Œå› ä¸ºé‚£æ˜¯æœ‰è¶£çš„éƒ¨åˆ†ã€‚è¿™åªæ˜¯è®¾ç½® deltaã€‚æˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘ä»¬å¸Œæœ›æ”¹å˜å¤šå°‘ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬æœ€ç»ˆæƒ³è¦è¾¾åˆ°çš„ç†è®ºæ–°å€¼ã€‚å› æ­¤ï¼Œå¥–åŠ±ã€‚
- en: Re is importantã€‚The reward is what we were given for applying this state and
    action that we currently didã€‚ So we're updating the Q value for whatever action
    we took on the current stateã€‚ So the reward is I mean fundamentally the reward
    would be the estimate of the future value that we're going to get So those values
    in the tableã€‚ they tell you the reward for each of those actions at a particular
    stateã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Re æ˜¯é‡è¦çš„ã€‚å¥–åŠ±æ˜¯æˆ‘ä»¬å› åº”ç”¨å½“å‰çŠ¶æ€å’Œè¡ŒåŠ¨è€Œè·å¾—çš„ã€‚å› æ­¤æˆ‘ä»¬åœ¨æ›´æ–°å½“å‰çŠ¶æ€ä¸‹æˆ‘ä»¬æ‰€é‡‡å–çš„ä»»ä½•è¡ŒåŠ¨çš„ Q å€¼ã€‚å¥–åŠ±ï¼ŒåŸºæœ¬ä¸Šï¼Œå¥–åŠ±å°±æ˜¯æˆ‘ä»¬å°†è¦è·å¾—çš„æœªæ¥ä»·å€¼çš„ä¼°è®¡ã€‚æ‰€ä»¥è¡¨ä¸­çš„é‚£äº›å€¼ã€‚å®ƒä»¬å‘Šè¯‰ä½ åœ¨ç‰¹å®šçŠ¶æ€ä¸‹æ¯ä¸ªè¡ŒåŠ¨çš„å¥–åŠ±ã€‚
- en: usually you just take the highest of those actionsã€‚ but we want to evaluate
    the other two actions that we're not taking now we want to get to the eventual
    reward that we're going to get So what we do is after we've applied so we've done
    our action we've gotten a reward or not then we take this discount factor this
    is set to 0ã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ä½ åªæ˜¯é‡‡å–é‚£äº›åŠ¨ä½œä¸­çš„æœ€é«˜ä¸€ä¸ªã€‚ä½†æˆ‘ä»¬æƒ³è¦è¯„ä¼°ç°åœ¨ä¸é‡‡å–çš„å¦å¤–ä¸¤ä¸ªåŠ¨ä½œï¼Œæˆ‘ä»¬æƒ³è¦å¾—åˆ°æœ€ç»ˆå°†è·å¾—çš„å¥–åŠ±ã€‚æ‰€ä»¥æˆ‘ä»¬æ‰€åšçš„å°±æ˜¯åœ¨åº”ç”¨å®ŒåŠ¨ä½œåï¼Œæ— è®ºè·å¾—äº†å¥–åŠ±ä¸å¦ï¼Œç„¶åæˆ‘ä»¬é‡‡ç”¨è¿™ä¸ªæŠ˜æ‰£å› å­ï¼Œè¿™ä¸ªå€¼è®¾ä¸º0ã€‚
- en: 95 so we're taking 95% of whatever the maximum Q value was for this next state
    that we're moving into based on that action So whatever that actionã€‚ultted in
    this new stateã€‚ We look at the Q values for this new state and we take the maximum
    of those action Q valuesã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 95ï¼Œå› æ­¤æˆ‘ä»¬æ­£åœ¨åŸºäºè¯¥åŠ¨ä½œè¿›å…¥çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œè€ƒè™‘95%çš„æœ€å¤§Qå€¼ã€‚æ‰€ä»¥ä¸è®ºè¯¥åŠ¨ä½œå¯¼è‡´çš„æ˜¯ä»€ä¹ˆæ–°çŠ¶æ€ã€‚æˆ‘ä»¬æŸ¥çœ‹è¯¥æ–°çŠ¶æ€çš„Qå€¼ï¼Œå¹¶å–è¿™äº›åŠ¨ä½œQå€¼çš„æœ€å¤§å€¼ã€‚
- en: So that is the maximum that is an estimateï¼Œ as it says of the optimal future
    reward for the next the next step in all of thisã€‚ You don't want to necessarily
    apply this directly to it because then its mind is too much to the futureã€‚ and
    is the here and now is quite important The reward that we just got it's a balance
    between these twoã€‚ but a very common setting for this oneã€‚ and the one I'm using
    is 95%ã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªæœ€å¤§å€¼çš„ä¼°è®¡ï¼Œæ­£å¦‚å®ƒæ‰€è¯´çš„ï¼Œè¿™æ˜¯ä¸‹ä¸€æ­¥çš„**æœ€ä½³æœªæ¥å¥–åŠ±**ã€‚ä½ ä¸ä¸€å®šæƒ³ç›´æ¥åº”ç”¨è¿™ä¸ªï¼Œå› ä¸ºé‚£æ ·çš„è¯æ€ç»´å°±å¤ªè¿‡äºæœªæ¥äº†ï¼Œè€Œæ­¤æ—¶æ­¤åˆ»æ˜¯ç›¸å½“é‡è¦çš„ã€‚æˆ‘ä»¬åˆšåˆšå¾—åˆ°çš„å¥–åŠ±æ˜¯åœ¨è¿™ä¸¤è€…ä¹‹é—´çš„å¹³è¡¡ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„è®¾ç½®ï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯95%ã€‚
- en: So we're taking both of these into consideration considerablyã€‚ So that's itã€‚
    That is how this equation worksã€‚ and I wanted to take you through each step of
    it because this is a very common format equation for machine learning and I give
    you a description of what these values areã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè€ƒè™‘äº†è¿™ä¸¤è€…ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯è¿™ä¸ªæ–¹ç¨‹å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘æƒ³å¸¦ä½ é€æ­¥äº†è§£ï¼Œå› ä¸ºè¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªéå¸¸å¸¸è§çš„æ ¼å¼æ–¹ç¨‹ï¼Œæˆ‘ç»™ä½ æè¿°äº†è¿™äº›å€¼çš„å«ä¹‰ã€‚
- en: we just went through all of thoseã€‚ So let's look at the Q learning carã€‚ the
    Q learning car is going to basically learn to perform similar to the preprogrammed
    car thatã€‚I gaveve youã€‚ the trick is it's learned by a machine that has no concept
    of gravity or hills or anything like thatã€‚ It just plays with the car until it
    finally gets its reward and distributes that across the actions and eventually
    gets a pretty good programã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšç»å†äº†æ‰€æœ‰è¿™äº›ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹Qå­¦ä¹ çš„æ±½è½¦ã€‚Qå­¦ä¹ çš„æ±½è½¦åŸºæœ¬ä¸Šä¼šå­¦ä¹ ä»¥ç±»ä¼¼äºæˆ‘ç»™ä½ çš„**é¢„ç¼–ç¨‹æ±½è½¦**çš„æ–¹å¼è¿›è¡Œæ“ä½œã€‚å…³é”®æ˜¯å®ƒæ˜¯ç”±ä¸€ä¸ªå¯¹é‡åŠ›ã€å¡åº¦æˆ–å…¶ä»–ä»»ä½•äº‹ç‰©æ²¡æœ‰æ¦‚å¿µçš„æœºå™¨å­¦ä¹ çš„ã€‚å®ƒåªæ˜¯å’Œæ±½è½¦ç©ï¼Œç›´åˆ°æœ€ç»ˆè·å¾—å¥–åŠ±ï¼Œå¹¶å°†å…¶åˆ†é…åˆ°è¡ŒåŠ¨ä¸­ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç›¸å½“å¥½çš„ç¨‹åºã€‚
- en: So this is the Q learning carã€‚ This is calculating to the discrete stateã€‚ this
    is just putting the position into one of those 10 bucketsã€‚ This is running the
    gameã€‚ So while we're not doneï¼Œ we are going to basically this is a very important
    part to understand hereã€‚ This is how epsilon is usedã€‚ Epsilon is a valueã€‚ One
    means make the car move completely erratically random0 means make the car move
    completely according to the Q tableã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯Qå­¦ä¹ çš„æ±½è½¦ã€‚å®ƒæ­£åœ¨è®¡ç®—ç¦»æ•£çŠ¶æ€ï¼Œè¿™åªæ˜¯å°†ä½ç½®æ”¾å…¥è¿™10ä¸ªæ¡¶ä¸­çš„ä¸€ä¸ªã€‚è¿™æ˜¯åœ¨è¿è¡Œæ¸¸æˆã€‚æ‰€ä»¥è™½ç„¶æˆ‘ä»¬è¿˜æ²¡æœ‰å®Œæˆï¼Œä½†è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†ï¼Œéœ€è¦ç†è§£ã€‚è¿™ä¸ªæ˜¯epsilonçš„ä½¿ç”¨ã€‚Epsilonæ˜¯ä¸€ä¸ªå€¼ã€‚1æ„å‘³ç€è®©æ±½è½¦å®Œå…¨ä¸è§„åˆ™åœ°ç§»åŠ¨ï¼Œ0æ„å‘³ç€è®©æ±½è½¦å®Œå…¨æŒ‰ç…§Qè¡¨ç§»åŠ¨ã€‚
- en: 5 means do it kind of half in halfã€‚ This epsilon value starts out very random
    at firstã€‚ and then we decay it as we go so that it goes closer and closer to zeroã€‚
    and then eventually for the last partï¼Œ it is performing completely from the Q
    tableã€‚And learning completely on the Q tableã€‚ The randomness forces it to explore
    and try out radical new ideasã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 5æ„å‘³ç€åšå¾—æœ‰ç‚¹ä¸€åŠä¸€åŠã€‚è¿™ä¸ªepsilonå€¼ä¸€å¼€å§‹æ˜¯éå¸¸éšæœºçš„ï¼Œç„¶åæˆ‘ä»¬åœ¨å‰è¿›è¿‡ç¨‹ä¸­é€æ¸å‡å°ï¼Œä½¿å…¶è¶Šæ¥è¶Šæ¥è¿‘é›¶ã€‚æœ€åï¼Œåœ¨æœ€åéƒ¨åˆ†ï¼Œå®ƒå®Œå…¨ä»Qè¡¨ä¸­æ‰§è¡Œï¼Œå¹¶å®Œå…¨ä¾èµ–äºQè¡¨ã€‚éšæœºæ€§è¿«ä½¿å®ƒæ¢ç´¢å¹¶å°è¯•æ¿€è¿›çš„æ–°æƒ³æ³•ã€‚
- en: And then once you've locked in on a pretty good overall techniqueã€‚ then the
    machine learning can take over and refine that simulated andneing is a very similar
    process to this and other Monte Carlo techniques as wellã€‚ We get our reward by
    calling the environmentã€‚ Con the state into a discrete valueã€‚ We check to see
    if we've reached the goal position if we have we're doneã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä¸€æ—¦ä½ é”å®šäº†ä¸€ä¸ªç›¸å½“å¥½çš„æ•´ä½“æŠ€æœ¯ï¼Œæœºå™¨å­¦ä¹ å°±å¯ä»¥æ¥ç®¡å¹¶ç»†åŒ–æ¨¡æ‹Ÿï¼Œè¿™ä¸å…¶ä»–è’™ç‰¹å¡æ´›æŠ€æœ¯éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬é€šè¿‡è°ƒç”¨ç¯å¢ƒè·å¾—å¥–åŠ±ã€‚å°†çŠ¶æ€è½¬åŒ–ä¸ºç¦»æ•£å€¼ã€‚æˆ‘ä»¬æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†ç›®æ ‡ä½ç½®ï¼Œå¦‚æœè¾¾åˆ°äº†æˆ‘ä»¬å°±å®Œæˆäº†ã€‚
- en: and then we update the Q valueã€‚ This is that big hair equation from before written
    in Pythonã€‚ and then we switch the state to to the new state that we have moved
    toã€‚ So this is basically running the gameã€‚ This runs one step of the gameã€‚ and
    these are all these hyperparametersã€‚ Learn rate is the learning rate for the algorithmã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æ›´æ–°Qå€¼ã€‚è¿™æ˜¯ä¹‹å‰æåˆ°çš„å¤§çš„Qå€¼æ–¹ç¨‹ï¼Œç”¨Pythonå†™çš„ã€‚æ¥ç€æˆ‘ä»¬å°†çŠ¶æ€åˆ‡æ¢åˆ°æˆ‘ä»¬ç§»åŠ¨åˆ°çš„æ–°çŠ¶æ€ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯åœ¨è¿è¡Œæ¸¸æˆã€‚è¿™æ˜¯æ¸¸æˆçš„ä¸€ä¸ªæ­¥éª¤ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯è¶…å‚æ•°ã€‚å­¦ä¹ ç‡æ˜¯ç®—æ³•çš„å­¦ä¹ ç‡ã€‚
- en: how quickly we update those Q values Disc I told you before that's the 95% How
    many episodes we want to run how often do we want an update I set that to every
    thousand discrete grid sizeã€‚Using just 10 so that whole left or right between
    the two mountains is effectively broken into into 10 values just to keep it a
    discrete input into Q learningã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ›´æ–°Qå€¼çš„é€Ÿåº¦ã€‚æˆ‘ä¹‹å‰å‘Šè¯‰è¿‡ä½ ï¼Œè¿™å°±æ˜¯95%ã€‚æˆ‘ä»¬æƒ³è¿è¡Œå¤šå°‘å›åˆï¼Œå¤šä¹…æ›´æ–°ä¸€æ¬¡ã€‚æˆ‘æŠŠè¿™ä¸ªè®¾ç½®ä¸ºæ¯ä¸€åƒæ¬¡ç¦»æ•£ç½‘æ ¼å¤§å°ã€‚åªä½¿ç”¨10ï¼Œå› æ­¤ä¸¤ä¸ªå±±ä¹‹é—´çš„æ•´ä¸ªå·¦å³åŒºåŸŸæœ‰æ•ˆåœ°è¢«åˆ’åˆ†ä¸º10ä¸ªå€¼ï¼Œä»¥ä¿æŒQå­¦ä¹ çš„ç¦»æ•£è¾“å…¥ã€‚
- en: which is how key learning typically works We can get Q learning to work on continuous
    values but that that gets into other algorithms that we'll see later in this chapter
    This is the decay for the epsilon like I told you aboutã€‚So we load up the mountain
    car environmentã€‚ We set up the epsilon change so that it can decay eventually
    to0 over the episodes that we wanted toã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å…³é”®å­¦ä¹ é€šå¸¸æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚æˆ‘ä»¬å¯ä»¥è®©Qå­¦ä¹ åœ¨è¿ç»­å€¼ä¸Šå·¥ä½œï¼Œä½†è¿™æ¶‰åŠåˆ°åé¢ç« èŠ‚ä¸­æˆ‘ä»¬ä¼šçœ‹åˆ°çš„å…¶ä»–ç®—æ³•ã€‚è¿™æ˜¯æˆ‘ä¹‹å‰æåˆ°çš„epsilonè¡°å‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åŠ è½½å±±è½¦ç¯å¢ƒã€‚æˆ‘ä»¬è®¾ç½®epsilonå˜åŒ–ï¼Œä»¥ä¾¿å®ƒæœ€ç»ˆèƒ½å¤Ÿåœ¨æˆ‘ä»¬æƒ³è¦çš„å›åˆä¸­è¡°å‡åˆ°0ã€‚
- en: We're gonna loop until we're done with the episodesã€‚ When we hit the show every
    so show every 1000 in the case that I gave youã€‚ it will show you what's going
    on and run a gameã€‚ you'll only see the game if you're running this locallyã€‚ we
    count the successes and we run itã€‚ So you can see from the first thousand episodesã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¾ªç¯ï¼Œç›´åˆ°å›åˆç»“æŸã€‚å½“æˆ‘ä»¬æ¯éš”1000æ¬¡æ˜¾ç¤ºä¸€æ¬¡æ—¶ï¼Œå®ƒä¼šå‘Šè¯‰ä½ å‘ç”Ÿäº†ä»€ä¹ˆå¹¶è¿è¡Œä¸€ä¸ªæ¸¸æˆã€‚åªæœ‰åœ¨æœ¬åœ°è¿è¡Œæ—¶ï¼Œä½ æ‰èƒ½çœ‹åˆ°æ¸¸æˆã€‚æˆ‘ä»¬ç»Ÿè®¡æˆåŠŸæ¬¡æ•°å¹¶è¿è¡Œå®ƒã€‚å› æ­¤ï¼Œä½ å¯ä»¥ä»å‰ä¸€åƒä¸ªå›åˆä¸­çœ‹åˆ°ã€‚
- en: there were no successes at allã€‚ We don't get any successes until 4000ã€‚ And through
    a lot of this since there's no feedbackã€‚ The only really capability it has here
    is trying random approachesã€‚ And finallyã€‚ it gets some successã€‚ And that is why
    the epsilon is decaying because we want that randomness to start to go awayã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨æ²¡æœ‰æˆåŠŸã€‚æˆ‘ä»¬åœ¨4000æ¬¡ä¹‹å‰æ²¡æœ‰ä»»ä½•æˆåŠŸã€‚åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œç”±äºæ²¡æœ‰åé¦ˆï¼Œå®ƒå”¯ä¸€çœŸæ­£çš„èƒ½åŠ›æ˜¯å°è¯•éšæœºæ–¹æ³•ã€‚æœ€åï¼Œå®ƒå–å¾—äº†ä¸€äº›æˆåŠŸã€‚è¿™å°±æ˜¯epsilonè¡°å‡çš„åŸå› ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›éšæœºæ€§å¼€å§‹æ¶ˆå¤±ã€‚
- en: And once it starts to get some learning to refine what it's learnedã€‚ You can
    see here of the thousand episodesï¼Œ it finally is completely successfulã€‚ Howeverã€‚
    there's a lot of randomã€‚to the environmentã€‚ So it'sï¼Œ it's not learned completely
    yetã€‚ I like to let it go until it gets to fairly consistent success like hereã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®ƒå¼€å§‹å­¦ä¹ ä»¥ç²¾ç‚¼æ‰€å­¦å†…å®¹ï¼Œä½ å¯ä»¥çœ‹åˆ°åœ¨è¿™åƒä¸ªå›åˆä¸­ï¼Œå®ƒæœ€ç»ˆæ˜¯å®Œå…¨æˆåŠŸçš„ã€‚ç„¶è€Œï¼Œç¯å¢ƒä¸­æœ‰å¾ˆå¤šéšæœºæ€§ã€‚å› æ­¤ï¼Œå®ƒè¿˜æ²¡æœ‰å®Œå…¨å­¦ä¹ ã€‚æˆ‘å–œæ¬¢è®©å®ƒç»§ç»­ï¼Œç›´åˆ°å®ƒè¾¾åˆ°ç›¸å¯¹ä¸€è‡´çš„æˆåŠŸï¼Œå¦‚æ­¤å¤„æ‰€ç¤ºã€‚
- en: you could modify this so that the code actually looks at this and stops based
    on the the thousands getting consistentã€‚ Howeverï¼Œ the problem you have then isã€‚You
    increase the total number of episodes and then your decay of the epsilon doesn't
    work as wellã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä¿®æ”¹è¿™ä¸ªï¼Œä½¿å¾—ä»£ç å®é™…ä¸ŠæŸ¥çœ‹è¿™ä¸ªå¹¶æ ¹æ®å›åˆçš„ä¸€è‡´æ€§åœæ­¢ã€‚ç„¶è€Œï¼Œä½ é¢ä¸´çš„é—®é¢˜æ˜¯ï¼Œä½ å¢åŠ äº†æ€»å›åˆæ•°ï¼Œç„¶åepsilonçš„è¡°å‡æ•ˆæœå°±ä¸å¥½äº†ã€‚
- en: because you don't know how deep you're going at the very beginningã€‚ Now to run
    itã€‚ we can see it run hereã€‚ and it rocksï¼Œ and it's not quite as good as the preprogram
    cardã€‚ but it does itã€‚ Now let's look inside a deep let's look inside a Q learning's
    brain and see what's going on here I can dump the tableã€‚ So the velocity is discreetized
    as is the positionã€‚ We have to discreteet these twoã€‚ Otherwiseã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºåœ¨å¼€å§‹æ—¶ä½ ä¸çŸ¥é“è‡ªå·±è¦èµ°å¤šæ·±ã€‚ç°åœ¨è¿è¡Œå®ƒã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°å®ƒè¿è¡Œï¼Œå¹¶ä¸”å®ƒè¡¨ç°å¾—ä¸é”™ï¼Œè™½ç„¶æ²¡æœ‰é¢„ç¼–ç¨‹çš„å¡è½¦å¥½ï¼Œä½†å®ƒç¡®å®è¿è¡Œäº†ã€‚ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹Qå­¦ä¹ çš„å†…éƒ¨ï¼Œçœ‹çœ‹è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘å¯ä»¥å¯¼å‡ºè¡¨æ ¼ã€‚é€Ÿåº¦å’Œä½ç½®éƒ½è¢«ç¦»æ•£åŒ–ã€‚æˆ‘ä»¬å¿…é¡»ç¦»æ•£è¿™ä¸¤è€…ã€‚å¦åˆ™ã€‚
- en: how would we have a tableï¼Œ we would have infinite values hereã€‚ Nowã€‚ when we
    get into deep Q learningï¼Œ we can deal quite easily out of the box with continuous
    values for both of these twoã€‚ Howeverï¼Œ deep Q learning does require some further
    algorithms added to it if we want to continuous actions so that basically we can
    apply the accelerator maybe halfway or three quarterss away or something like
    thatã€‚ Now just looking at the gridï¼Œ this doesn't tell me a lotã€‚ first of allã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•æ„å»ºä¸€ä¸ªè¡¨å‘¢ï¼Ÿè¿™é‡Œå°†æœ‰æ— é™çš„å€¼ã€‚ç°åœ¨ï¼Œå½“æˆ‘ä»¬è¿›å…¥æ·±åº¦ Q å­¦ä¹ æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾å¤„ç†è¿™ä¸¤ä¸ªè¿ç»­å€¼ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦è¿ç»­çš„åŠ¨ä½œï¼Œæ·±åº¦ Q å­¦ä¹ ç¡®å®éœ€è¦ä¸€äº›é¢å¤–çš„ç®—æ³•ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬å¯ä»¥å°†åŠ é€Ÿå™¨åº”ç”¨åˆ°ä¸€åŠæˆ–ä¸‰åˆ†ä¹‹ä¸‰çš„åœ°æ–¹ï¼Œæˆ–è€…ç±»ä¼¼çš„æƒ…å†µã€‚ç°åœ¨ï¼Œçœ‹çœ‹è¿™ä¸ªç½‘æ ¼ï¼Œè¿™å¹¶æ²¡æœ‰å‘Šè¯‰æˆ‘å¾ˆå¤šï¼Œé¦–å…ˆã€‚
- en: I don't feel that velocity is even all that useful of an input for this problem
    When I did my preprogrammed car I used only positionã€‚ So what I did was I took
    the mean of these so that I can see kind of row and column at a time because like
    I said for this problem I'm not seeing really the value of the velocity and you
    can see when I take the means look these line up you can see that it's learned
    almost a gradient and it learns particular this is the one that I'm more interested
    inã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—é€Ÿåº¦åœ¨è¿™ä¸ªé—®é¢˜ä¸­ç”šè‡³ä¸æ˜¯ä¸€ä¸ªå¾ˆæœ‰ç”¨çš„è¾“å…¥ã€‚å½“æˆ‘ç¼–å†™æˆ‘çš„é¢„ç¼–ç¨‹æ±½è½¦æ—¶ï¼Œæˆ‘åªä½¿ç”¨äº†ä½ç½®ã€‚å› æ­¤ï¼Œæˆ‘æ‰€åšçš„æ˜¯å–è¿™äº›å€¼çš„å¹³å‡å€¼ï¼Œè¿™æ ·æˆ‘å¯ä»¥ä¸€æ¬¡æŸ¥çœ‹ä¸€è¡Œå’Œä¸€åˆ—ï¼Œå› ä¸ºæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œå¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å¹¶æ²¡æœ‰çœ‹åˆ°é€Ÿåº¦çš„çœŸæ­£ä»·å€¼ã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œå½“æˆ‘å–å¹³å‡å€¼æ—¶ï¼Œè¿™äº›å€¼å¯¹é½äº†ï¼Œä½ å¯ä»¥çœ‹åˆ°å®ƒå‡ ä¹å­¦ä¹ åˆ°äº†ä¸€ä¸ªæ¢¯åº¦ï¼Œå®ƒå­¦ä¹ äº†ç‰¹åˆ«çš„è¿™ä¸ªæ˜¯æˆ‘æ›´æ„Ÿå…´è¶£çš„ã€‚
- en: it's learning to apply more of it and less well one direction versus the other
    direction up here versus down hereã€‚ Now up here these last two those are a bit
    anomalies but that could also because it's already hit the goal stated it's already
    crashing to the top of the mountain it may not really matter what it's doing at
    that point and that's Q learning Q learning is tablebased will'll look more at
    this with deep learning where we can get deal better with continuous values first
    inã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ­£åœ¨å­¦ä¹ å¦‚ä½•åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šåº”ç”¨æ›´å¤šï¼Œè€Œåœ¨å¦ä¸€ä¸ªæ–¹å‘ä¸Šåˆ™è¾ƒå°‘ï¼Œä¸Šé¢ä¸ä¸‹é¢çš„æ¯”è¾ƒã€‚ç°åœ¨ï¼Œä¸Šé¢è¿™æœ€åä¸¤ä¸ªæœ‰ç‚¹å¼‚å¸¸ï¼Œä½†è¿™ä¹Ÿå¯èƒ½æ˜¯å› ä¸ºå®ƒå·²ç»è¾¾åˆ°äº†æ—¢å®šç›®æ ‡ï¼Œå·²ç»æ’ä¸Šäº†å±±é¡¶ï¼Œæ­¤æ—¶å®ƒçš„è¡Œä¸ºå¯èƒ½å¹¶ä¸é‡è¦ã€‚è¿™å°±æ˜¯
    Q å­¦ä¹ ï¼ŒQ å­¦ä¹ æ˜¯åŸºäºè¡¨æ ¼çš„ï¼Œæˆ‘ä»¬å°†åœ¨æ·±åº¦å­¦ä¹ ä¸­æ›´æ·±å…¥åœ°æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°å¤„ç†è¿ç»­å€¼ã€‚
- en: '![](img/30d4f6c3c88418548a35fcff3628ad27_6.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30d4f6c3c88418548a35fcff3628ad27_6.png)'
- en: St and then also in the actionsã€‚ Thank you for watching this videoã€‚ In the next
    videoã€‚ we're going to look at how to extend this table into deep Q learning that
    way we don't have to represent literally every combination we can let the neural
    network learn to generalizeã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Stï¼Œä»¥åŠåœ¨åŠ¨ä½œæ–¹é¢ã€‚æ„Ÿè°¢æ‚¨è§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•å°†è¿™ä¸ªè¡¨æ‰©å±•åˆ°æ·±åº¦ Q å­¦ä¹ ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…é€ä¸€è¡¨ç¤ºæ¯ç§ç»„åˆï¼Œè€Œæ˜¯è®©ç¥ç»ç½‘ç»œå­¦ä¹ å¦‚ä½•è¿›è¡Œæ³›åŒ–ã€‚
