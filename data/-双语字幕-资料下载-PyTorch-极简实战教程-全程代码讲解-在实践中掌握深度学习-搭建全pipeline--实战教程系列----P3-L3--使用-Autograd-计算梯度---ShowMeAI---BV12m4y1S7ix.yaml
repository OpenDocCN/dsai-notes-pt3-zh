- en: 【双语字幕+资料下载】PyTorch 极简实战教程！全程代码讲解，在实践中掌握深度学习&搭建全pipeline！＜实战教程系列＞ - P3：L3- 使用
    Autograd 计算梯度 - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】PyTorch 极简实战教程！全程代码讲解，在实践中掌握深度学习&搭建全pipeline！＜实战教程系列＞ - P3：L3- 使用
    Autograd 计算梯度 - ShowMeAI - BV12m4y1S7ix
- en: Hi， everybody。 Welcome to a new pytorrch tutorial。 Today。 we learn about the
    autograd package in pytorrch and how we can calculate gradients with it。 Grds
    are essential for our model optimization。 So this is a very important concept
    that we should understand。Luckily， Pytorch provides the autograd package， which
    can do all the computations for us。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，大家好。欢迎来到新的 pytorrch 教程。今天，我们将学习 pytorrch 中的 autograd 包，以及我们如何用它计算梯度。梯度对于我们的模型优化至关重要。因此，这是一个我们应该理解的重要概念。幸运的是，Pytorch
    提供了 autograd 包，它可以为我们执行所有计算。
- en: We just have to know how to use it。 So let's start to see how we can calculate
    gradients in pieytorrch。 So first of all， we import torch。 Of course。😊，And now
    let's create a tenor x equals torch dot R n of size 3。 And now let's print our
    x。 So this is a tenzor with three values。 So three random values。And now。 let's
    say later， we want to calculate the gradients of some function with respect to
    x。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要知道如何使用它。因此，让我们开始看看如何在 pytorrch 中计算梯度。首先，我们导入 torch。当然。😊 然后让我们创建一个张量 x 等于
    torch dot R n，大小为 3。现在让我们打印我们的 x。这是一个包含三个值的张量。三个随机值。现在。假设我们稍后想计算某个函数相对于 x 的梯度。
- en: Then what we have to do is we must specify the argument requires Gr equals true。
    So by default。 this is false。And now if we run this again， then we see that also
    Pyto tracks that it requires the gradient。And now。Whenever we do operations with
    this tenor， Py toch will create a so called computational graph for us。 So now
    let's say we do the operation x plus 2， and we start this in an output。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们要做的就是指定参数 requires Gr 等于 true。默认情况下，这是 false。现在如果我们再次运行这个，我们会看到 Pyto 也跟踪它需要梯度。现在，每当我们对这个张量进行操作时，Pytoch
    会为我们创建一个所谓的计算图。因此，现在假设我们进行操作 x 加 2，并将其存储在输出中。
- en: So we say y equals x plus 2。 Then this will create the computational graph。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_1.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们说 y 等于 x 加 2。这将创建计算图。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_1.png)
- en: And this looks like this。So for each node， we have a for each operation。 we
    have a node with inputs and an output。 So here the operation is the plus。 So in
    addition。And our inputs are x and 2， and the output is y。And now with this graph
    and the technique that is called back propagation。 we can then calculate the gradients。I
    will explain the concept of Beck propagation in detail in the next video。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像这样。对于每个节点，我们有一个遍历操作。我们有一个带输入和输出的节点。所以这里的操作是加法。因此，输入是 x 和 2，输出是 y。现在通过这个图和称为反向传播的技术，我们可以计算梯度。我将在下一个视频中详细解释反向传播的概念。
- en: But for now， it's fine to just know that we or how we can use it。So first， we
    do a forward pass。 So here we apply this operation。 And in the forward pass， we
    calculate the output y。And since we specified that it requires the gradient。 Pytoch
    will then automatically create and store a function for us。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，只需知道我们如何使用它就可以了。因此，首先我们进行前向传递。在这里，我们应用这个操作。在前向传递中，我们计算输出 y。由于我们指定它需要梯度，Pytoch
    会自动为我们创建并存储一个函数。
- en: And this function is then used in the back proagation and to get the gradients。
    So here y has an attribute Gr underscore F N。 So this will point to a gradient
    function。 And in this case， it's called at add backward。And with this function。
    we can then calculate the gradients in the so called backward pass。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数随后用于反向传播以获取梯度。因此，y 拥有一个属性 Gr underscore F N。这将指向一个梯度函数。在这种情况下，称为 at add
    backward。通过这个函数，我们可以在所谓的反向传递中计算梯度。
- en: So this will calculate the gradient of y with respect to x in this case。 So
    now if we print y。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将计算 y 对 x 的梯度。在这种情况下。如果我们打印 y。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_3.png)
- en: Then we will see exactly this grat Fn attribute。 And here， this is an at backward
    function。 So because here， our operation was a plus。And then， our。Then we do the
    back propagation later。 So that's why it's called at backward。And let's do some
    more operation with our tensrs。So let's say we have C equals y times y times 2，
    for example。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将看到这个 grat Fn 属性。这里，这是一个反向函数。因为在这里，我们的操作是加法。然后，我们再进行反向传播。因此，这就是为什么称之为反向。让我们用我们的张量做一些更多的操作。假设我们有
    C 等于 y 乘以 y 乘以 2。
- en: So this tensor then also has this gra function attribute So here Grt Fn equals
    mile backward because here our operation is a multiplication。 And for example，
    we can say C equals C dot mean。 So we can apply a mean operation。 And then our
    gradient function is the mean backward。And now。 when we want to calculate the
    gradients， the only thing that we must do is to call C dot backward。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个张量也有这个 gra 函数属性。因此这里 Grt Fn 等于 mile backward，因为这里我们的操作是乘法。例如，我们可以说 C 等于
    C.mean。这样我们就可以应用均值操作。然后我们的梯度函数就是 mean backward。现在，当我们想要计算梯度时，我们必须做的唯一事情就是调用 C.backward。
- en: So this will then calculate the gradient of C with respect to x。 So x then has
    a gradient。 a dot Gr attribute where the gradients are stored。 so we can print
    this。And now if you run this。 then we see that we have the gradients here in this
    tenor。 So this is all we have to do。And now let's have a look what happens when
    we don't specify this argument。 So first of all。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这将计算 C 关于 x 的梯度。所以 x 具有一个梯度，一个 dot Gr 属性，其中存储了梯度。我们可以打印它。现在如果你运行这个。那么我们看到在这个张量中有梯度。因此，这就是我们必须做的所有事情。现在让我们看看当我们不指定这个参数时会发生什么。所以首先。
- en: if we print our tenzos。Then we see that they don't have this gra function attribute。
    And if we try to call the backward function， then this will produce an  error。
    So it says tens does not require gr and does not have the gr function。So remember
    that we must specify this argument and then it will work。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印我们的张量。然后我们看到它们没有这个 gra 函数属性。如果我们尝试调用反向函数，那么这将产生一个错误。因此，它显示张量不需要 gr 并且没有
    gr 函数。所以记住，我们必须指定这个参数，然后它将正常工作。
- en: And one thing that we should also know is so in the background what this basically
    does。 this will create a socal vector Jacobcobian product to get the gradients。
    So this will look like this。 I will not go into the mathematical details。 but
    we should know that we have the Jacobcobian matrix。 with the partial derivatives。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该知道的一件事是，在后台这基本上做了什么。这将创建一个标量向量雅可比乘积以获得梯度。所以它将看起来像这样。我不会深入数学细节。但我们应该知道，我们有雅可比矩阵与偏导数。
- en: And then we multiply this with a gradient vector。 and then we will get the final
    the final gradients that we are interested in。 So this is also called the chain
    rule。 And I will also explain this more in detail in the next video But yeah。
    we should know that actually we must multiply it with a vector。 So in this case。
    since our C is a scala value， We don't have to put the don't have to use an argument。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将其与梯度向量相乘。然后我们将得到我们感兴趣的最终梯度。这也称为链式法则。我将在下一个视频中更详细地解释这一点。但我们应该知道，实际上我们必须将其与一个向量相乘。因此在这种情况下，由于我们的
    C 是一个标量值，我们不需要传递参数。
- en: '![](img/cecd5af4135b2f12f1bfa0bb78e5d673_5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cecd5af4135b2f12f1bfa0bb78e5d673_5.png)'
- en: And here for our backward function。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_7.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里对于我们的反向函数。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_7.png)
- en: So our C here has only v value。 So this is fine。 But let's say we didn't apply
    the mean operation。 So now our C has more than one value in it。 So it's also size
    1 by 3。 And now when we try to call the backward function like this。 And this
    will produce an error。 So Gr can be implicitly created only for scala outputs。
    So in this case， we have to。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的 C 这里只有一个标量值。这是可以的。但假设我们没有应用均值操作。那么现在我们的 C 中有多个值。所以它的大小也是 1x3。现在，当我们尝试像这样调用反向函数时。这将产生一个错误。因此，Gr
    只能为标量输出隐式创建。在这种情况下，我们必须。
- en: Give it the gradient argument。 So we have to create a vector of the same size，
    so。Let's say V equals torch dot1or。 And here we put， for example，011。0 and0。0，0，1。
    and we give it a data type of torch dot float 32。 and then we must pass this vector
    to our backward function。 and now it will work again。So now， if we run this。Then
    this is okay。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 给它梯度参数。所以我们必须创建一个相同大小的向量。所以，假设 V 等于 torch.dot(1)。在这里我们放置，例如，011.0 和 0.0，0，1。并且我们将数据类型设置为
    torch.float32。然后我们必须将这个向量传递给我们的反向函数。现在它将再次正常工作。所以现在，如果我们运行这个。那么这是可以的。
- en: So we should know that in the background， this is a a vector Jacobbian product。
    And a lot of times。 the last operation is some operation that will create a scala
    value。 So this is。 it's okay to call it like this without an argument。 But if
    this is not an a scala。 Then we must give it the the vector。And yeah。Then some
    other thing that we should know is how we can prevent Pyth from tracking the history
    and calculating this gra f n attribute。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该知道，在后台，这是一个向量雅可比乘积。很多时候，最后一个操作是某个操作，将创建一个标量值。因此，可以在没有参数的情况下像这样调用。但如果这不是标量，我们必须提供向量。还有一些其他事情我们应该知道，如何防止Pyth跟踪历史并计算这个grad_fn属性。
- en: So for example， sometimes during our training loop when we want to update our
    weights。 Then this operation should not be part of the gradient computation。 So
    in one of the next tutorials。 I will give a concrete example of how we apply this
    autocrad package。 And then it will become clearer， maybe。 But yeah， for now。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，有时在我们的训练循环中，当我们想更新权重时，这个操作不应该是梯度计算的一部分。在接下来的教程中，我会给出一个具体的示例，说明我们如何应用这个autograd包。然后可能会变得更清晰。但现在是这样。
- en: we should know how we can prevent this from from tracking the gradients。 and
    we have three option for this。 So the first one is to call the requires。Grat。
    underscore function， and set this to false。The second option is to call X dot
    det。 So this will create a new Tenzoor that doesn't require the gradient。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该知道如何防止跟踪梯度，并且我们有三个选项。第一个是调用requires_grad下划线函数，并将其设置为false。第二个选项是调用x dot
    detach。因此这将创建一个不需要梯度的新张量。
- en: And the second option would be to wrap this in a width statement。 So with torch，
    dot， no gr。 And then we can do our operations。So， yeah， let's try each of these。
    So first。We can say。 x dot requires。Grat， underscore and set this to false。 So
    whenever a function has a trailing underscore in Pytorch。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项是将其包装在一个宽度语句中。因此使用torch，dot，不需要梯度。然后我们可以进行操作。是的，让我们尝试每一个。这是第一步。我们可以说x dot需要梯度，带下划线，并将其设置为false。因此，每当一个函数在Pytorch中有一个尾随的下划线时。
- en: then this means that it will modify our variable in place。 So now， if we print
    X。Then we will see that it doesn't have this require Gr attribute anymore。 So
    now this is false。So this is the first option， and the second option would be
    to call x detaach。 So we say y equals x do detaach。 So this will create a new
    vector with the same or a newtenzo with the same values。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它将在原地修改我们的变量。因此，现在，如果我们打印x。然后我们会看到它不再具有这个requires_grad属性。所以现在这是false。这是第一个选项，第二个选项是调用x.detach。因此我们说y等于x.detach。这将创建一个具有相同值的新向量或新张量。
- en: but it doesn't require the gradient。 So here we see that our y has the same
    values。 but doesn't require the gradients。And the last option is to wrap it in
    a torch in a width with statement with torch dot no Gr。 And then we can do some
    operations， for example， y equals。X plus 2。And now， if we print our y。 then we
    see that it doesn't have the gradient function attribute here。 So， yeah。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但它不需要梯度。因此在这里我们看到我们的y具有相同的值，但不需要梯度。最后一个选项是将其包装在一个torch的with语句中，使用torch.no_grad。然后我们可以进行一些操作，例如y等于x加2。现在，如果我们打印我们的y，我们会看到它在这里没有梯度函数属性。因此，是的。
- en: if we don't use this and would run it like this。Then our y has the gradient
    function。 So these are the three ways how we can stop Pythot from creating this
    gradient functions and tracking the history in our computational graph。And now
    one more very important thing that we should also know is that whenever we call
    the backward function。 then the gradient for this tenzoor will be accumulated
    into the dot gra attribute。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用这个并且像这样运行它。那么我们的y就会有梯度函数。因此，这三种方法可以阻止Pytorch创建梯度函数并跟踪我们计算图中的历史。而现在还有一个非常重要的事情，我们也应该知道的是，每当我们调用backward函数时，梯度将被累积到dot_grad属性中。
- en: So the values will be summed up。 So here we we must be very careful。 So let's
    create some dummy training example。 where we have some have some weights。 So this
    is a a tenzoor with ones in it of size， let's say4， and they require the gradient。
    So requires gra equals true。 And now let's say we have a training loop where we
    say four epo in range and first。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这样值就会被求和。在这里我们必须非常小心。因此让我们创建一些虚拟的训练示例，其中我们有一些权重。这是一个全为1的张量，大小为4，并且需要梯度。因此requires_grad等于true。现在假设我们有一个训练循环，我们说对于每个epoch在范围内。
- en: let's only do one iteration。 And here we do， let's say model output。Equals，
    let's say。Weights times 3 dot sum。 So this is just a dummy operation， which will
    simulate some model output。 And then we want to calculate the gradients。 So we
    say model output dot backward。And now we have the gradient， so we can call weight
    dot Gr。And print this。嗯。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先进行一次迭代。这里我们假设模型输出等于权重乘以3的总和。这只是一个虚拟操作，用来模拟一些模型输出。然后我们想计算梯度。所以我们调用模型输出的`.backward()`。现在我们有了梯度，可以调用`weights.Gr`并打印出来。
- en: So I what gradients here are 3。So the tensor is filled with threes。 And now
    if we do another iteration。 So if we say we have two iterations。 then the second
    backward call will again accumulate the values and write them into the Gr attribute。
    So now our grs has sixes in it。 And now if we do a third iteration。 Then it has
    nines in it。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的梯度是3，所以张量填充了三的值。如果我们进行第二次迭代，那么第二次的`backward`调用将再次累加值并写入`Gr`属性。现在我们的`grs`中有六。再进行第三次迭代后，它将变为九。
- en: So all the values are summed up。 And now our weights or our gradients are clearly
    incorrect。 So before we do the next iteration and optimization step。 we must empty
    the gradients。 So we must call weights dot Gr dot0 underscore。 And now if we run
    this then our gradients are correct again。So this is one very important。Th that
    we must know during our training steps。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有值都已被求和。现在我们的权重或梯度显然不正确。在进行下一次迭代和优化步骤之前，我们必须清空梯度。因此，我们必须调用`weights.Gr.0_`。现在如果我们运行这个，梯度将再次正确。这是我们在训练步骤中必须了解的一个非常重要的事项。
- en: and later we will work with the Pytorch built in Opr。 So let's say we have a
    optr from the torch optimization package。 So torch do optim do SGD for stochastic
    gradient descent。 which has our weights as parameters and some learning rate and
    now with this optimizer。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将使用Pytorch内置的优化器。假设我们有一个来自torch优化包的优化器。torch的optim模块提供了随机梯度下降（SGD），它的参数包括我们的权重和一些学习率，现在有了这个优化器。
- en: we can call or do a optimization step and then before we do the next iteration。
    we must call the optimizer do0 gra function， which will do exactly the same。So，
    yeah。 we will talk about the optimizes in some later tutorials。 But yeah， for
    now。 the things you should remember is that whenever we want to calculate the
    gradients。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用或进行一个优化步骤，然后在进行下一次迭代之前，必须调用优化器的`do0_gra`函数，它会执行完全相同的操作。所以，是的。我们将在后面的教程中讨论优化器。但现在，你应该记住的是，每当我们想计算梯度时。
- en: We must specify the require gr parameter and set this to true。 Then we can simply
    calculate the gradients with calling the backward function and before we want
    to do the next operation or the next iteration in our optimization steps。 we must
    empty our gradients。 So we must call the0 function again。And we also should know
    how we can prevent some operations from being tracked in the computational graph。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指定`require_gr`参数并设置为true。然后我们可以简单地调用`backward`函数来计算梯度。在进行下一个操作或优化步骤的下一次迭代之前，我们必须清空我们的梯度。因此，我们必须再次调用`the0`函数。我们还应该知道如何防止某些操作在计算图中被跟踪。
- en: And that's all I wanted to show you for now with the autograd package。 And I
    hope you liked it。 Please subscribe to the channel and see you next time， bye。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_9.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我想向你展示的有关autograd包的内容。希望你喜欢。请订阅频道，下次再见，拜。![](img/cecd5af4135b2f12f1bfa0bb78e5d673_9.png)
