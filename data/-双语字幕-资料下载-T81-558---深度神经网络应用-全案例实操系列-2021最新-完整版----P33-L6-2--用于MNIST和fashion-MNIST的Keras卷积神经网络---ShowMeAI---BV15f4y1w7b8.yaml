- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P33ï¼šL6.2- ç”¨äºMNISTå’Œfashion-MNISTçš„Keraså·ç§¯ç¥ç»ç½‘ç»œ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P33ï¼šL6.2- ç”¨äºMNISTå’Œfashion-MNISTçš„Keraså·ç§¯ç¥ç»ç½‘ç»œ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ welcome to applications of Deep neural Network with
    Washington University In this videoã€‚ we're going to look at how to use convolution
    neural networksã€‚ and we're going to start with one of the classic neural network
    data sets that has been used on many differentã€‚ğŸ˜Šï¼ŒModels other than neural networks
    as wellï¼Œ this is the Mins digits data setã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯Jeff Heatonã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†ä»ä¸€ä¸ªç»å…¸çš„ç¥ç»ç½‘ç»œæ•°æ®é›†å¼€å§‹ï¼Œè¿™ä¸ªæ•°æ®é›†åœ¨è®¸å¤šä¸åŒçš„æ¨¡å‹ä¸­ä½¿ç”¨ï¼Œé™¤äº†ç¥ç»ç½‘ç»œä¹‹å¤–ï¼Œè¿™å°±æ˜¯MNSæ•°å­—æ•°æ®é›†ã€‚
- en: then we're going to look at a newer data set that is very similarã€‚ it's meant
    to be a drop in replacement for Minstã€‚ it is the mens fashion data setã€‚After we
    get through these we'll be able to see more advanced topics for convolution neural
    networks for the latest on my AI course and projectsã€‚ click subscribe and the
    bell next to it to be notified of every new videoã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ä¸€ä¸ªéå¸¸ç›¸ä¼¼çš„æ–°æ•°æ®é›†ã€‚å®ƒè¢«è®¾è®¡ä¸ºMNSçš„æ›¿ä»£å“ï¼Œè¿™å°±æ˜¯ç”·æ€§æ—¶å°šæ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬å®Œæˆè¿™äº›ä¹‹åï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿçœ‹åˆ°å·ç§¯ç¥ç»ç½‘ç»œçš„æ›´é«˜çº§ä¸»é¢˜ã€‚æƒ³äº†è§£æˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®ï¼Œè¯·ç‚¹å‡»è®¢é˜…ï¼Œå¹¶ç‚¹å‡»æ—è¾¹çš„é“ƒé“›ä»¥è·å¾—æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚
- en: so you'll notice that I am using Google CoLb rather than just simply my laptop
    like I've done in a number of these videosã€‚![](img/194e506320f5e9abc6ca37065bbd9036_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ ä¼šæ³¨æ„åˆ°ï¼Œæˆ‘ä½¿ç”¨Google Coabè€Œä¸ä»…ä»…æ˜¯ç®€å•åœ°ä½¿ç”¨æˆ‘çš„ç¬”è®°æœ¬ç”µè„‘ï¼Œå°±åƒæˆ‘åœ¨è®¸å¤šè¿™äº›è§†é¢‘ä¸­æ‰€åšçš„é‚£æ ·ã€‚![](img/194e506320f5e9abc6ca37065bbd9036_1.png)
- en: We're using Google coab because we're going to need to make use of a GPUã€‚ If
    we don't make use of the GPUï¼Œ this is gonna to run very slowlyã€‚ We'll literally
    have things that would take probably five minutes with the GPU that could take
    two hours without the GPUã€‚ So it's very important that you have that GPU available
    to youã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨Google Coabæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦ä½¿ç”¨GPUã€‚å¦‚æœä¸ä½¿ç”¨GPUï¼Œè¿™å°†è¿è¡Œå¾—éå¸¸æ…¢ã€‚å®é™…ä¸Šï¼Œæœ‰äº›äº‹æƒ…åœ¨GPUä¸Šå¯èƒ½éœ€è¦äº”åˆ†é’Ÿï¼Œè€Œæ²¡æœ‰GPUå¯èƒ½éœ€è¦ä¸¤ä¸ªå°æ—¶ã€‚å› æ­¤ï¼Œæ‹¥æœ‰å¯ç”¨çš„GPUéå¸¸é‡è¦ã€‚
- en: I have a complete video that I'll link to thisã€‚ It's linked with the class that
    shows how to use Google coabab in conjunction with this with this courseã€‚ Basicallyï¼Œ
    if you want to run any of my course material It's best just to open up the Github
    repository with Google coab and pull the file inã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ä¸€ä¸ªå®Œæ•´çš„è§†é¢‘ï¼Œæˆ‘ä¼šé“¾æ¥åˆ°è¿™ä¸ªã€‚å®ƒä¸è¿™ä¸ªè¯¾ç¨‹ç›¸å…³ï¼Œå±•ç¤ºäº†å¦‚ä½•å°†Google Coabä¸æ­¤è¯¾ç¨‹ç»“åˆä½¿ç”¨ã€‚åŸºæœ¬ä¸Šï¼Œå¦‚æœä½ æƒ³è¿è¡Œæˆ‘çš„ä»»ä½•è¯¾ç¨‹ææ–™ï¼Œæœ€å¥½æ‰“å¼€å¸¦æœ‰Google
    Coabçš„GitHubä»“åº“ï¼Œå¹¶å°†æ–‡ä»¶å¯¼å…¥ã€‚
- en: that's exactly what I've done hereã€‚ You can see up hereã€‚ I've connected to to
    Github and I've pulled in this class 6ã€‚ Nowã€‚ let's look at how to use this with
    Kira's and convolution neural networksã€‚ First of allã€‚ let's go ahead and change
    the runtime typeã€‚ We do want to useã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯æˆ‘åœ¨è¿™é‡Œæ‰€åšçš„ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ã€‚æˆ‘å·²è¿æ¥åˆ°GitHubï¼Œå¹¶å¯¼å…¥äº†è¿™ä¸ªç±»6ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†å…¶ä¸Kiraå’Œå·ç§¯ç¥ç»ç½‘ç»œä¸€èµ·ä½¿ç”¨ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ã€‚æˆ‘ä»¬ç¡®å®æƒ³è¦ä½¿ç”¨ã€‚
- en: '![](img/194e506320f5e9abc6ca37065bbd9036_3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/194e506320f5e9abc6ca37065bbd9036_3.png)'
- en: So it's saying in Python 3ï¼Œ6ï¼Œ Washington Universityã€‚ that's something localã€‚
    So it would complain about that as soon as I tried to run it because Google Coabb
    doesn't know what that isã€‚ That's a local Python environment that I haveã€‚ I'm
    going switch it to Python 3 hardware acceleratorã€‚ We want a GPUã€‚ We'll get into
    TUus laterã€‚ and let's go ahead and saveã€‚ Nowã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒåœ¨Python 3.6ä¸­è¯´ï¼Œåç››é¡¿å¤§å­¦ã€‚è¿™æ˜¯ä¸€äº›æœ¬åœ°å†…å®¹ã€‚å› æ­¤ï¼Œå½“æˆ‘å°è¯•è¿è¡Œæ—¶ï¼Œå®ƒä¼šå¯¹æ­¤æŠ±æ€¨ï¼Œå› ä¸ºGoogle Coabä¸çŸ¥é“é‚£æ˜¯ä»€ä¹ˆã€‚è¿™æ˜¯æˆ‘æ‹¥æœ‰çš„æœ¬åœ°Pythonç¯å¢ƒã€‚æˆ‘å°†å…¶åˆ‡æ¢åˆ°Python
    3ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚æˆ‘ä»¬æƒ³è¦GPUã€‚æˆ‘ä»¬ç¨åä¼šè¿›å…¥TUusï¼Œè®©æˆ‘ä»¬ç»§ç»­ä¿å­˜ã€‚ç°åœ¨ã€‚
- en: whenever I run things in thisã€‚ It'll run it with the GPUã€‚ So we're going to
    look at computer vision data setsã€‚ There's a couple of them that we see in thisã€‚ğŸ˜Šã€‚![](img/194e506320f5e9abc6ca37065bbd9036_5.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å½“æˆ‘åœ¨è¿™é‡Œè¿è¡Œä¸œè¥¿æ—¶ã€‚å®ƒä¼šç”¨GPUè¿è¡Œã€‚æ‰€ä»¥æˆ‘ä»¬å°†æŸ¥çœ‹è®¡ç®—æœºè§†è§‰æ•°æ®é›†ã€‚åœ¨è¿™ä¸ªä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å‡ ä¸ªã€‚ğŸ˜Šã€‚![](img/194e506320f5e9abc6ca37065bbd9036_5.png)
- en: Course moduleï¼Œ and then we get into some even more advanced ones later in this
    semesterã€‚The classic Ho world computer vision data set is the minsed digitsã€‚These
    are basically handwritten digits that are 28 by 28 pixelsã€‚ they have been studied
    and used to death in researchã€‚Papersã€‚As a resultã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹æ¨¡å—ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†åœ¨æœ¬å­¦æœŸåé¢è¿›å…¥ä¸€äº›æ›´é«˜çº§çš„å†…å®¹ã€‚ç»å…¸çš„è®¡ç®—æœºè§†è§‰æ•°æ®é›†æ˜¯MNSæ•°å­—ã€‚è¿™äº›åŸºæœ¬ä¸Šæ˜¯28x28åƒç´ çš„æ‰‹å†™æ•°å­—ï¼Œå®ƒä»¬åœ¨ç ”ç©¶è®ºæ–‡ä¸­è¢«åå¤ç ”ç©¶å’Œä½¿ç”¨ã€‚å› æ­¤ã€‚
- en: there's been a very closely related data set added to this called Minced fashionã€‚
    which looks very much like thisï¼Œ except its handbags and shoes and shirtsã€‚ So
    you try to detect an article of clothing rather than rather than digitsã€‚But we
    need to go through the the mins digitsã€‚ We want to see how this works because
    it'sã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªä¸æ­¤éå¸¸ç›¸å…³çš„æ•°æ®é›†è¢«æ·»åŠ åˆ°è¿™ä¸ªåä¸º**Minced fashion**çš„é¡¹ç›®ä¸­ã€‚å®ƒçœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ï¼Œåªä¸è¿‡æ˜¯æ‰‹è¢‹ã€é‹å­å’Œè¡¬è¡«ã€‚æ‰€ä»¥ä½ è¯•å›¾æ£€æµ‹çš„æ˜¯è¡£ç‰©ï¼Œè€Œä¸æ˜¯æ•°å­—ã€‚ä½†æˆ‘ä»¬éœ€è¦é€šè¿‡è¿™äº›**mins**æ•°å­—ã€‚æˆ‘ä»¬æƒ³çœ‹çœ‹è¿™æ˜¯å¦‚ä½•è¿ä½œçš„ï¼Œå› ä¸ºå®ƒæ˜¯ã€‚
- en: it's your typical hello world forã€‚For machine learning and for computer visionã€‚
    now where these digits came fromã€‚Standardized testsï¼Œ more and moreï¼Œ this is becoming
    online butã€‚You guys have probably done fill in the bubble sort of standardized
    testsã€‚When I was in grade schoolã€‚ high school and getting ready for ACT exams
    and satï¼Œ getting ready for collegeã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä½ å…¸å‹çš„**hello world**ç¨‹åºã€‚å¯¹äºæœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æ¥è¯´ã€‚è¿™äº›æ•°å­—æ¥è‡ªå“ªé‡Œã€‚æ ‡å‡†åŒ–è€ƒè¯•ï¼Œè¶Šæ¥è¶Šå¤šåœ°ï¼Œè¿™æ­£åœ¨å˜å¾—åœ¨çº¿ã€‚ä½†ä½ ä»¬å¯èƒ½åšè¿‡å¡«æ³¡çš„é‚£ç§æ ‡å‡†åŒ–æµ‹è¯•ã€‚å½“æˆ‘ä¸Šå°å­¦ã€é«˜ä¸­ï¼Œå¹¶ä¸ºACTå’ŒSATè€ƒè¯•å‡†å¤‡æ—¶ï¼Œå‡†å¤‡ä¸Šå¤§å­¦ã€‚
- en: this is the only way you took this kind of thing literally they know that these
    students were creating perfect computer vision training data because the students
    would draw like a three end here and then fill in the blankã€‚ so you've got your
    X which is going to be the what they drew and the Yã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å”¯ä¸€çš„æ–¹å¼ï¼Œä½ å¯ä»¥å­—é¢ç†è§£è¿™äº›å­¦ç”Ÿæ­£åœ¨åˆ›å»ºå®Œç¾çš„è®¡ç®—æœºè§†è§‰è®­ç»ƒæ•°æ®ï¼Œå› ä¸ºå­¦ç”Ÿä»¬ä¼šåœ¨è¿™é‡Œç”»ä¸ªä¸‰ï¼Œç„¶åå¡«ç©ºã€‚æ‰€ä»¥ä½ å¾—åˆ°äº†Xï¼Œå³ä»–ä»¬ç”»çš„å†…å®¹å’ŒYã€‚
- en: the expected labels of what they filled in the little blank withã€‚Now there's
    noise in this dataset setã€‚ Some studentsï¼Œ me in particularã€‚ when I was taking
    these things had really bad handwritingã€‚ Some students would lieã€‚ they would draw
    a three here and fill in a four hereã€‚ R's a legitimate mistakeã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬å¡«å…¥å°ç©ºç™½çš„æœŸæœ›æ ‡ç­¾ã€‚ç°åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­æœ‰å™ªå£°ã€‚ä¸€äº›å­¦ç”Ÿï¼Œç‰¹åˆ«æ˜¯æˆ‘ã€‚åœ¨å‚åŠ è¿™äº›è€ƒè¯•æ—¶å­—è¿¹éå¸¸ç³Ÿç³•ã€‚æœ‰äº›å­¦ç”Ÿä¼šæ’’è°ã€‚ä»–ä»¬ä¼šåœ¨è¿™é‡Œç”»ä¸ªä¸‰ï¼Œç„¶ååœ¨è¿™é‡Œå¡«ä¸ªå››ã€‚Ræ˜¯ä¸€ä¸ªåˆæ³•çš„é”™è¯¯ã€‚
- en: So that is that is basically how this worksã€‚ I also remember some of these exams
    that I took back in the innocent days before identity theftã€‚ You'd literally put
    your social security number right on the exam paperã€‚ What could possibly go wrongã€‚
    So that is the mined digitsã€‚ Minsed fashion is basically articles of clothing
    in exactly the same format as the mins digitsã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™åŸºæœ¬ä¸Šå°±æ˜¯å®ƒçš„å·¥ä½œåŸç†ã€‚æˆ‘è¿˜è®°å¾—ä¸€äº›æˆ‘åœ¨èº«ä»½ç›—çªƒå‰çš„å¤©çœŸæ—¶ä»£å‚åŠ çš„è€ƒè¯•ã€‚ä½ ä¼šåœ¨è€ƒè¯•çº¸ä¸Šå†™ä¸Šä½ çš„ç¤¾ä¼šå®‰å…¨å·ç ã€‚å¯èƒ½ä¼šå‡ºä»€ä¹ˆé—®é¢˜å‘¢ã€‚æ‰€ä»¥è¿™å°±æ˜¯è¢«æŒ–æ˜çš„æ•°å­—ã€‚**Minced
    fashion**åŸºæœ¬ä¸Šæ˜¯ä¸**mins digits**å®Œå…¨ç›¸åŒæ ¼å¼çš„è¡£ç‰©ã€‚
- en: So it's a drop in replacementã€‚ and we will take a look at this dataï¼Œ because
    it it's greatã€‚ and it's more difficult than minst for the machine learningã€‚ Mach
    learning has gotten smarterã€‚ So we need smarterï¼Œ not smarter but harder data sets
    for the machine learning to tackleã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ç§å¯æ›¿ä»£çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†æŸ¥çœ‹è¿™äº›æ•°æ®ï¼Œå› ä¸ºå®ƒéå¸¸å‡ºè‰²ï¼Œå¹¶ä¸”å¯¹äºæœºå™¨å­¦ä¹ æ¥è¯´æ¯”**minst**æ›´å…·æŒ‘æˆ˜æ€§ã€‚æœºå™¨å­¦ä¹ å˜å¾—æ›´èªæ˜äº†ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ›´èªæ˜ï¼Œè€Œä¸æ˜¯æ›´èªæ˜ï¼Œè€Œæ˜¯æ›´å›°éš¾çš„æ•°æ®é›†æ¥æŒ‘æˆ˜æœºå™¨å­¦ä¹ ã€‚
- en: '![](img/194e506320f5e9abc6ca37065bbd9036_7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/194e506320f5e9abc6ca37065bbd9036_7.png)'
- en: You can see there's shoesï¼Œ there's pantsï¼Œ there's shirtsï¼Œ dressesã€‚![](img/194e506320f5e9abc6ca37065bbd9036_9.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°æœ‰é‹å­ã€è£¤å­ã€è¡¬è¡«ã€è£™å­ã€‚![](img/194e506320f5e9abc6ca37065bbd9036_9.png)
- en: Fashion items basically for the neural network to figure outã€‚ So instead of
    10 different digitsã€‚ you have 10 different types of fashionã€‚ It's also the CR
    data setã€‚ C 10ï¼Œ we will use becauseã€‚![](img/194e506320f5e9abc6ca37065bbd9036_11.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶å°šç‰©å“åŸºæœ¬ä¸Šæ˜¯è®©ç¥ç»ç½‘ç»œå»è¯†åˆ«çš„ã€‚æ‰€ä»¥ä½ æœ‰10ç§ä¸åŒç±»å‹çš„æ—¶å°šï¼Œè€Œä¸æ˜¯10ä¸ªä¸åŒçš„æ•°å­—ã€‚å®ƒä¹Ÿæ˜¯CRæ•°æ®é›†ã€‚C 10ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨å®ƒï¼Œå› ä¸ºã€‚![](img/194e506320f5e9abc6ca37065bbd9036_11.png)
- en: We're going to look at resnets later in this course and Resnetsã€‚This is the
    data set that they really distinguish themselves on for the first timeã€‚ so we'll
    see how you can apply a resnet to these full colorã€‚Data set here where you've
    got different types of airplanes and different types of carsï¼Œ catsï¼Œ birdsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨æœ¬è¯¾ç¨‹ç¨åæŸ¥çœ‹**resnets**ï¼Œè¿™æ˜¯ä»–ä»¬é¦–æ¬¡çœŸæ­£å±•ç¤ºè‡ªå·±èƒ½åŠ›çš„æ•°æ®é›†ã€‚æ‰€ä»¥æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†**resnet**åº”ç”¨äºè¿™äº›å…¨å½©è‰²çš„æ•°æ®é›†ï¼Œè¿™é‡Œä½ æœ‰ä¸åŒç±»å‹çš„é£æœºã€æ±½è½¦ã€çŒ«ã€é¸Ÿã€‚
- en: All these kind of thingsï¼Œ by the wayï¼Œ a common complaint of some of these machine
    learning databases that later ones have tried to fix is what percentage of these
    are animalsã€‚Airplane is not birdï¼Œ catï¼Œ deer dogï¼Œ frogï¼Œ horseã€‚ Okayï¼Œ so a good
    60% of thatã€‚Is an animalã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè¿™äº›äº‹æƒ…çš„å…±åŒæŠ±æ€¨æ˜¯ä¸€äº›æœºå™¨å­¦ä¹ æ•°æ®åº“åæ¥çš„ç‰ˆæœ¬è¯•å›¾ä¿®å¤çš„é—®é¢˜ï¼Œå³è¿™äº›æ•°æ®ä¸­æœ‰å¤šå°‘ç™¾åˆ†æ¯”æ˜¯åŠ¨ç‰©ã€‚é£æœºä¸æ˜¯é¸Ÿï¼ŒçŒ«ï¼Œé¹¿ï¼Œç‹—ï¼Œé’è›™ï¼Œé©¬ã€‚å¥½çš„ï¼Œæ‰€ä»¥å¤§çº¦60%æ˜¯åŠ¨ç‰©ã€‚
- en: So not not completely balanced as far as the the features that it's learningã€‚
    other resources that I do want to show youã€‚At Stanfordã€‚ there's an entire course
    just on convolution neural networksã€‚ And just like this courseã€‚ they make all
    of the material available on the Internetã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨å­¦ä¹ çš„ç‰¹å¾æ–¹é¢å¹¶ä¸æ˜¯å®Œå…¨å¹³è¡¡ã€‚æˆ‘æƒ³å‘ä½ å±•ç¤ºå…¶ä»–èµ„æºã€‚åœ¨æ–¯å¦ç¦ï¼Œæœ‰ä¸€æ•´é—¨è¯¾ç¨‹ä¸“é—¨è®²å·ç§¯ç¥ç»ç½‘ç»œã€‚å°±åƒè¿™é—¨è¯¾ç¨‹ä¸€æ ·ï¼Œä»–ä»¬å°†æ‰€æœ‰ææ–™éƒ½æ”¾åœ¨ç½‘ä¸Šã€‚
- en: So you'll want to check that out if you're particularly interested in convolution
    neural networksã€‚ Andre Kpathy isã€‚I've followed his research since he was a grad
    student and very interesting guy who does a lot with computer visionã€‚ his his
    dissertation was on captioningã€‚Now you could have a picture of a cat riding a
    skateboard and the neural network would literally write out in textã€‚ Cat riding
    skateboardã€‚ That was the image that I think he had as one of the figures in hisã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ ç‰¹åˆ«å¯¹å·ç§¯ç¥ç»ç½‘ç»œæ„Ÿå…´è¶£ï¼Œæƒ³è¦æŸ¥çœ‹ä¸€ä¸‹ã€‚Andre Kpathyæ˜¯ä¸€ä¸ªæˆ‘ä»ä»–è¯»ç ”ç©¶ç”Ÿæ—¶å°±å…³æ³¨çš„ç ”ç©¶è€…ï¼Œä»–æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„äººï¼Œåšäº†å¾ˆå¤šè®¡ç®—æœºè§†è§‰çš„å·¥ä½œã€‚ä»–çš„è®ºæ–‡æ˜¯å…³äºå›¾åƒå­—å¹•çš„ã€‚ä½ å¯èƒ½æœ‰ä¸€å¼ çŒ«éª‘æ»‘æ¿çš„å›¾ç‰‡ï¼Œç¥ç»ç½‘ç»œä¼šçœŸçš„ä»¥æ–‡æœ¬å½¢å¼å†™å‡ºæ¥ï¼šçŒ«éª‘æ»‘æ¿ã€‚è¿™æ˜¯æˆ‘è®¤ä¸ºä»–è®ºæ–‡ä¸­çš„ä¸€ä¸ªå›¾åƒã€‚
- en: in his dissertationã€‚He taught the course at Stanford initiallyã€‚I think he works
    for Tesla nowã€‚ so he probably doesn't teach that course anymore for Stanford University
    and all the information is is online He also wrote Convenet JSã€‚There's an early
    convolution neural network library completely in JavaScriptã€‚ probably these days
    you would use the Google Tensorflow for JavaScriptã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–æœ€åˆåœ¨æ–¯å¦ç¦æ•™æˆè¿™é—¨è¯¾ç¨‹ã€‚æˆ‘æƒ³ä»–ç°åœ¨åœ¨ç‰¹æ–¯æ‹‰å·¥ä½œï¼Œæ‰€ä»¥ä»–å¯èƒ½ä¸å†æ•™è¿™é—¨è¯¾ç¨‹ï¼Œæ–¯å¦ç¦å¤§å­¦çš„æ‰€æœ‰ä¿¡æ¯éƒ½åœ¨çº¿ä¸Šã€‚ä»–è¿˜å†™äº†Convenet JSã€‚è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç”¨JavaScriptç¼–å†™çš„æ—©æœŸå·ç§¯ç¥ç»ç½‘ç»œåº“ã€‚ç°åœ¨ä½ å¯èƒ½ä¼šä½¿ç”¨Googleçš„Tensorflow
    for JavaScriptã€‚
- en: but it's worth taking a look at so we'll look at convolution neural networksã€‚
    convolution neural networksï¼Œ a lot of the application to deep learning and to
    neural networks in generalã€‚ came from Janang Laonï¼Œ who is one of the corecipients
    of the Tring Award for deep learning that was awarded to himã€‚Yshua Benjiio and
    Jeffrey Hintonã€‚What thisï¼ŸDoes that is really powerful compared to neural networks
    beforeã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å€¼å¾—ä¸€çœ‹ï¼Œå› æ­¤æˆ‘ä»¬å°†ç ”ç©¶å·ç§¯ç¥ç»ç½‘ç»œã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼Œå¾ˆå¤šæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„ä¸€èˆ¬åº”ç”¨ï¼Œæ¥æºäºJanang Laonï¼Œä»–æ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸçš„Tringå¥–çš„å…±åŒè·å¾—è€…ä¹‹ä¸€ï¼Œè·å¾—è€…è¿˜åŒ…æ‹¬Yshua
    Benjiioå’ŒJeffrey Hintonã€‚è¿™ä¸ªæœ‰ä»€ä¹ˆï¼Ÿä¸ä¹‹å‰çš„ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒçœŸçš„å¾ˆå¼ºå¤§ã€‚
- en: And other traditional machine learning models like support vector machines is
    it can scanã€‚So we've got the digit A here and notice this little box connected
    to the convolution layerã€‚The convolution layer is one of a couple of new types
    of layers that we have nowã€‚ we had dense layersã€‚Now we get convolution layers
    and max pooling layersã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ¯”å¦‚æ”¯æŒå‘é‡æœºï¼Œå¯ä»¥è¿›è¡Œæ‰«æã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œæœ‰æ•°å­—Aï¼Œæ³¨æ„è¿™ä¸ªå°æ¡†ä¸å·ç§¯å±‚ç›¸è¿ã€‚å·ç§¯å±‚æ˜¯æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰çš„å‡ ç§æ–°ç±»å‹å±‚ä¹‹ä¸€ã€‚æˆ‘ä»¬æœ‰å¯†é›†å±‚ï¼Œç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†å·ç§¯å±‚å’Œæœ€å¤§æ± åŒ–å±‚ã€‚
- en: We also got dropout layers previously in this courseï¼Œ but these are the two
    new onesã€‚ The convolution layer takes this squareã€‚That is used to scanã€‚ so you
    specify the size of that squareï¼Œ the size of the scanning regionã€‚And it scans
    acrossã€‚The entire image areaï¼Œ and it learnsã€‚As it goesã€‚So if the square was right
    hereã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é—¨è¯¾ç¨‹ä¸­ä¹‹å‰ä¹Ÿä»‹ç»äº†dropoutå±‚ï¼Œä½†è¿™ä¸¤ä¸ªæ˜¯æ–°çš„ã€‚å·ç§¯å±‚ä½¿ç”¨è¿™ä¸ªæ­£æ–¹å½¢ã€‚ç”¨äºæ‰«æï¼Œæ‰€ä»¥ä½ æŒ‡å®šé‚£ä¸ªæ­£æ–¹å½¢çš„å¤§å°ï¼Œæ‰«æåŒºåŸŸçš„å¤§å°ã€‚å®ƒä¼šåœ¨æ•´ä¸ªå›¾åƒåŒºåŸŸå†…æ‰«æï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­å­¦ä¹ ã€‚å¦‚æœæ­£æ–¹å½¢å°±åœ¨è¿™é‡Œã€‚
- en: it would learn about the bump on the top of the a sort of an angleã€‚ Nowã€‚ other
    letters might have a feature such as that and theseã€‚Filters they' typically called
    or neuronsã€‚ they're roughly equivalent to the neurons in a normal hidden layerã€‚Would
    learn each of these attributesã€‚ Maybe one of them would learn a line that stops
    hereã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šå­¦ä¹ é¡¶éƒ¨éš†èµ·çš„ç‰¹å¾ï¼Œå‘ˆç°å‡ºä¸€ä¸ªè§’åº¦ã€‚ç°åœ¨ï¼Œå…¶ä»–å­—æ¯å¯èƒ½ä¹Ÿæœ‰è¿™æ ·çš„ç‰¹å¾ï¼Œè¿™äº›é€šå¸¸è¢«ç§°ä¸ºè¿‡æ»¤å™¨æˆ–ç¥ç»å…ƒã€‚å®ƒä»¬å¤§è‡´ç›¸å½“äºæ™®é€šéšè—å±‚ä¸­çš„ç¥ç»å…ƒã€‚ä¼šå­¦ä¹ è¿™äº›å±æ€§ã€‚ä¹Ÿè®¸å…¶ä¸­ä¸€ä¸ªä¼šå­¦ä¹ ä¸€æ¡åœ¨è¿™é‡Œåœæ­¢çš„çº¿ã€‚
- en: Maybe one of them learns a line going at an angleã€‚ One learns a somewhat perpendicular
    connection to the lineã€‚ The bump at the topã€‚ All those would be learned by the
    feature mapsï¼Œ and that's in the convolution neural networkã€‚ The convolution layersã€‚
    feature map is another term for a convolution layerã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸å…¶ä¸­ä¸€ä¸ªå­¦ä¹ äº†ä¸€ä¸ªå€¾æ–œçš„çº¿ï¼Œå¦ä¸€ä¸ªå­¦ä¹ äº†ä¸çº¿å‚ç›´çš„è¿æ¥ã€‚é¡¶éƒ¨çš„éš†èµ·ã€‚è¿™äº›éƒ½ä¼šè¢«ç‰¹å¾å›¾å­¦ä¹ ï¼Œè€Œè¿™äº›åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ã€‚å·ç§¯å±‚ã€‚ç‰¹å¾å›¾æ˜¯å·ç§¯å±‚çš„å¦ä¸€ç§è¯´æ³•ã€‚
- en: Then after you've learned thisï¼Œ you might want to sub sampleample it and decrease
    it to a lower resolutionã€‚ That is the max pooling layerã€‚ We'll see all of these
    layers work in a momentï¼Œ exactlyã€‚ğŸ˜Šã€‚Then you have some additional convolution layers
    now learn to find features on this much reduced mapã€‚ So since it's smaller resolutionï¼Œ
    what you're really looking for now is the building blocks that this first layer
    learnedã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨ä½ å­¦ä¹ å®Œè¿™ä¸ªåï¼Œä½ å¯èƒ½æƒ³è¦è¿›è¡Œä¸‹é‡‡æ ·å¹¶é™ä½åˆ°æ›´ä½çš„åˆ†è¾¨ç‡ã€‚è¿™å°±æ˜¯æœ€å¤§æ± åŒ–å±‚ã€‚æˆ‘ä»¬ç¨åå°†çœ‹åˆ°æ‰€æœ‰è¿™äº›å±‚å¦‚ä½•å·¥ä½œï¼Œç¡®åˆ‡åœ°è¯´ã€‚ğŸ˜Šç„¶åä½ æœ‰ä¸€äº›é¢å¤–çš„å·ç§¯å±‚ï¼Œç°åœ¨å­¦ä¹ åœ¨è¿™ä¸ªå¤§å¤§ç¼©å°çš„å›¾ä¸Šæ‰¾åˆ°ç‰¹å¾ã€‚æ‰€ä»¥ç”±äºå®ƒçš„åˆ†è¾¨ç‡è¾ƒå°ï¼Œä½ ç°åœ¨çœŸæ­£è¦å¯»æ‰¾çš„æ˜¯ç¬¬ä¸€å±‚å­¦ä¹ åˆ°çš„æ„å»ºå—ã€‚
- en: So this first layer maybe learn bump Perpendicular connection line and end of
    a lineã€‚ now using just those building blocksï¼Œ this layer then builds higher level
    abstractionsã€‚ So maybe it learnsï¼Œ okayï¼Œ bump connected to two linesã€‚ and then
    we pass through some fully connected layersï¼Œ these are dense layersã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªç¬¬ä¸€å±‚å¯èƒ½å­¦ä¹ çš„æ˜¯å‚ç›´è¿æ¥çº¿å’Œçº¿çš„æœ«ç«¯ã€‚ç°åœ¨ä»…ä»…ä½¿ç”¨è¿™äº›æ„å»ºå—ï¼Œè¿™ä¸€å±‚ç„¶åæ„å»ºæ›´é«˜å±‚æ¬¡çš„æŠ½è±¡ã€‚æ‰€ä»¥ä¹Ÿè®¸å®ƒå­¦ä¹ åˆ°ï¼Œå¥½çš„ï¼Œå‡¸èµ·è¿æ¥åˆ°ä¸¤æ¡çº¿ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ä¸€äº›å…¨è¿æ¥å±‚ï¼Œè¿™äº›æ˜¯ç¨ å¯†å±‚ã€‚
- en: there's a variety of terminologies floating about on theseã€‚ğŸ˜Šã€‚Dense layer is
    pretty much what I've seen them called at least in the last five years worth worth
    of literature and then Gaussian connectionsã€‚ that is pretty much being used then
    for classificationã€‚ So the output has 10ã€‚ So there are 10 different types of digitsã€‚
    It's interesting that he specifies 10 outputsã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¸Šé¢æœ‰å„ç§æœ¯è¯­åœ¨æµä¼ ã€‚ğŸ˜Šç¨ å¯†å±‚åŸºæœ¬ä¸Šå°±æ˜¯æˆ‘åœ¨è¿‡å»äº”å¹´æ–‡çŒ®ä¸­çœ‹åˆ°çš„ç§°è°“ï¼Œç„¶åé«˜æ–¯è¿æ¥ã€‚åŸºæœ¬ä¸Šç”¨äºåˆ†ç±»ã€‚æ‰€ä»¥è¾“å‡ºæœ‰10ã€‚å› æ­¤æœ‰10ç§ä¸åŒç±»å‹çš„æ•°å­—ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»–æŒ‡å®šäº†10ä¸ªè¾“å‡ºã€‚
- en: but has a letter here you'd expect maybe 26 if he was teaching it onã€‚ but the
    paper that this was used a lot in was using mined digitsã€‚ but you could draw an
    a and it would probably might think it's a9 with a weird line on itã€‚ nonetheless
    this is the original diagram from La's original paper in 1998 that set the framework
    for a lot of what was to comeã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯åœ¨è¿™é‡Œæœ‰ä¸ªå­—æ¯ï¼Œå¦‚æœä»–åœ¨æ•™çš„æ—¶å€™ï¼Œå¯èƒ½ä¼šæœŸå¾…26ä¸ªå­—æ¯ã€‚ä½†è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æŒ–æ˜çš„æ•°å­—ã€‚ä½ å¯ä»¥ç”»ä¸€ä¸ªâ€œAâ€ï¼Œå®ƒå¯èƒ½ä¼šè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªâ€œA9â€ï¼Œä¸Šé¢æœ‰å¥‡æ€ªçš„çº¿æ¡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™æ˜¯æ¥è‡ªLaåœ¨1998å¹´åŸå§‹è®ºæ–‡ä¸­çš„åŸå§‹å›¾è¡¨ï¼Œä¸ºåæ¥çš„å¾ˆå¤šå†…å®¹å¥ å®šäº†æ¡†æ¶ã€‚
- en: Convolution layersã€‚ What you need to specify on them is the number of filtersã€‚
    The filter sizeã€‚ That's the size of that square that's scanning acrossã€‚Side is
    how many pixels that square jumps as it is scanning across padding is essentially
    a border that you put around the image because when that square hits the edgeã€‚
    you don't necessarily one it just falling off the edge of the image and not having
    a full set of pixels so you can put a padding around it zeros basically a black
    background and you also need to specify some activation functionã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯å±‚ã€‚ä½ éœ€è¦åœ¨å®ƒä»¬ä¸Šé¢æŒ‡å®šçš„æ˜¯è¿‡æ»¤å™¨çš„æ•°é‡ã€‚è¿‡æ»¤å™¨çš„å¤§å°ã€‚é‚£æ˜¯æ‰«æçš„æ­£æ–¹å½¢çš„å¤§å°ã€‚ä¾§è¾¹æ˜¯é‚£ä¸ªæ­£æ–¹å½¢åœ¨æ‰«ææ—¶è·³è·ƒçš„åƒç´ æ•°ï¼Œå¡«å……åŸºæœ¬ä¸Šæ˜¯ä½ æ”¾ç½®åœ¨å›¾åƒå‘¨å›´çš„è¾¹æ¡†ï¼Œå› ä¸ºå½“é‚£ä¸ªæ­£æ–¹å½¢åˆ°è¾¾è¾¹ç¼˜æ—¶ã€‚ä½ å¹¶ä¸å¸Œæœ›å®ƒå°±è¿™æ ·æ‰å‡ºå›¾åƒçš„è¾¹ç¼˜ï¼Œè€Œæ²¡æœ‰å®Œæ•´çš„åƒç´ é›†ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨å®ƒå‘¨å›´æ”¾ç½®å¡«å……ï¼ŒåŸºæœ¬ä¸Šæ˜¯é»‘è‰²èƒŒæ™¯çš„é›¶ï¼ŒåŒæ—¶ä½ è¿˜éœ€è¦æŒ‡å®šä¸€äº›æ¿€æ´»å‡½æ•°ã€‚
- en: usually re or one of the variance like like prelu or leaky re one of that family
    typically convolution layers are basically add weights and add parameters just
    like the other layersã€‚ the amount of parameters that it would add would be the
    filter size times the filter size because the scanning square is always square
    so the filter size is assumed to be the same horizontal and vertically times the
    number of filters and that's just basically weights that are inside of that convolution
    layerã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æ˜¯ReLUæˆ–ç±»ä¼¼çš„å˜ä½“ï¼Œå¦‚PReLUæˆ–Leaky ReLUï¼Œé‚£ä¸€ç±»ã€‚é€šå¸¸å·ç§¯å±‚åŸºæœ¬ä¸Šæ·»åŠ æƒé‡å’Œå‚æ•°ï¼Œå°±åƒå…¶ä»–å±‚ä¸€æ ·ã€‚å®ƒæ·»åŠ çš„å‚æ•°é‡å°†æ˜¯è¿‡æ»¤å™¨å¤§å°ä¹˜ä»¥è¿‡æ»¤å™¨å¤§å°ï¼Œå› ä¸ºæ‰«ææ­£æ–¹å½¢æ€»æ˜¯æ­£æ–¹å½¢ï¼Œå› æ­¤è¿‡æ»¤å™¨å¤§å°è¢«å‡å®šä¸ºæ°´å¹³å’Œå‚ç›´ç›¸åŒï¼Œä¹˜ä»¥è¿‡æ»¤å™¨çš„æ•°é‡ï¼Œè¿™åŸºæœ¬ä¸Šå°±æ˜¯å·ç§¯å±‚å†…éƒ¨çš„æƒé‡ã€‚
- en: Once it scans across the entire region of the imageã€‚ they're often called shared
    weights because that means that something detected up here in the image can also
    be detected down here in the imageã€‚Positional invarianceã€‚ Basicallyï¼Œ you can something
    could moveã€‚ and it's stillï¼Œ it's still detectedã€‚ This is essentially how you would
    think of a convolution layer as workingã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®ƒæ‰«ææ•´ä¸ªå›¾åƒåŒºåŸŸã€‚å®ƒä»¬é€šå¸¸è¢«ç§°ä¸ºå…±äº«æƒé‡ï¼Œå› ä¸ºè¿™æ„å‘³ç€åœ¨å›¾åƒä¸Šæ–¹æ£€æµ‹åˆ°çš„ä¸œè¥¿ä¹Ÿå¯ä»¥åœ¨å›¾åƒä¸‹æ–¹è¢«æ£€æµ‹åˆ°ã€‚ä½ç½®ä¸å˜æ€§ã€‚åŸºæœ¬ä¸Šï¼Œä½ å¯ä»¥è®©æŸä¸ªä¸œè¥¿ç§»åŠ¨ï¼Œè€Œå®ƒä»ç„¶å¯ä»¥è¢«æ£€æµ‹åˆ°ã€‚è¿™å°±æ˜¯ä½ å¯ä»¥è€ƒè™‘å·ç§¯å±‚å¦‚ä½•å·¥ä½œçš„æœ¬è´¨ã€‚
- en: You would have this squareã€‚ Nowï¼Œ this is dealing with gray scaleã€‚ So there's
    not three individual elements of redï¼Œ green and blue on each of these pixelsã€‚
    If this were colorï¼Œ thered basically just be a depth component and you would have
    three numbers for each of theseã€‚ But this blue region is basically the scan and
    it goes across as itï¼Œ as it scans andã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†æœ‰è¿™ä¸ªæ–¹å½¢ã€‚ç°åœ¨ï¼Œè¿™æ˜¯å¤„ç†ç°åº¦å›¾åƒã€‚å› æ­¤ï¼Œæ¯ä¸ªåƒç´ ä¸Šå¹¶æ²¡æœ‰çº¢ã€ç»¿ã€è“ä¸‰ä¸ªç‹¬ç«‹å…ƒç´ ã€‚å¦‚æœè¿™æ˜¯å½©è‰²å›¾åƒï¼ŒåŸºæœ¬ä¸Šåªä¼šæœ‰ä¸€ä¸ªæ·±åº¦ç»„ä»¶ï¼Œæ¯ä¸ªåƒç´ å°†æœ‰ä¸‰ä¸ªæ•°å­—ã€‚ä½†è¿™ä¸ªè“è‰²åŒºåŸŸåŸºæœ¬ä¸Šæ˜¯æ‰«æï¼Œå®ƒåœ¨æ‰«ææ—¶æ¨ªè·¨è€Œè¿‡ã€‚
- en: It has to go completely through the entire image for each prediction or each
    step in the trainingã€‚ Now max pooling layersã€‚ This is where you reduce the resolutionã€‚
    So say you wanted to cut the resolution in halfã€‚ You would take essentially a
    6 by 6ã€‚ and you would take it down to a 3 by 3ã€‚ So it would divide it into these
    regionsã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¿…é¡»åœ¨æ¯æ¬¡é¢„æµ‹æˆ–è®­ç»ƒçš„æ¯ä¸€æ­¥ä¸­å®Œå…¨éå†æ•´ä¸ªå›¾åƒã€‚ç°åœ¨æ˜¯æœ€å¤§æ± åŒ–å±‚ã€‚è¿™é‡Œæ˜¯ä½ å‡å°‘åˆ†è¾¨ç‡çš„åœ°æ–¹ã€‚æ‰€ä»¥è¯´ä½ æƒ³å°†åˆ†è¾¨ç‡å‡åŠã€‚ä½ å°†åŸºæœ¬ä¸Šå–ä¸€ä¸ª6ä¹˜6çš„åŒºåŸŸï¼Œé™åˆ°3ä¹˜3ã€‚æ‰€ä»¥å®ƒå°†åˆ†æˆè¿™äº›åŒºåŸŸã€‚
- en: and you would take the maximum hence max pooling layerã€‚ maximum in this cell
    is 8ã€‚ So you get an8ã€‚ Max in this cell is 2ã€‚ you get a 2ã€‚ so onï¼Œ and I do have
    a link here to giveã€‚ if you want more information on the lower levels of convolution
    neural networksã€‚ So let's have a look at how we would handle the digits data set
    I'm going to go ahead and run this region hereã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†å–æœ€å¤§å€¼ï¼Œå› æ­¤æ˜¯æœ€å¤§æ± åŒ–å±‚ã€‚è¿™ä¸ªå•å…ƒä¸­çš„æœ€å¤§å€¼æ˜¯8ã€‚æ‰€ä»¥ä½ å¾—åˆ°8ã€‚è¿™ä¸ªå•å…ƒä¸­çš„æœ€å¤§å€¼æ˜¯2ã€‚ä½ å¾—åˆ°2ã€‚ä¾æ­¤ç±»æ¨ï¼Œæˆ‘è¿™é‡Œæœ‰ä¸€ä¸ªé“¾æ¥å¯ä»¥æä¾›ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå·ç§¯ç¥ç»ç½‘ç»œä½å±‚çš„ä¿¡æ¯ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å¤„ç†æ•°å­—æ•°æ®é›†ï¼Œæˆ‘å°†ç»§ç»­è¿è¡Œè¿™é‡Œçš„åŒºåŸŸã€‚
- en: and it displays the informationã€‚ So we've got 60000 in the training set and
    10000 in the test setã€‚ Nowï¼Œ notice we are notã€‚Having to split train and test on
    our ownã€‚ they don't want you to for the immense data setã€‚ This is because this
    was used almost in a competitive way for papersã€‚ So they wanted everybody using
    the same training and the same test set so that you could have a reasonable comparison
    So if you said that you got a certain result on itã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¾ç¤ºäº†ä¿¡æ¯ã€‚æ‰€ä»¥æˆ‘ä»¬çš„è®­ç»ƒé›†æœ‰60000ä¸ªï¼Œæµ‹è¯•é›†æœ‰10000ä¸ªã€‚ç°åœ¨ï¼Œæ³¨æ„æˆ‘ä»¬å¹¶ä¸éœ€è¦è‡ªå·±æ‹†åˆ†è®­ç»ƒå’Œæµ‹è¯•ã€‚ä»–ä»¬ä¸å¸Œæœ›ä½ è¿™æ ·åšï¼Œä»¥åº”å¯¹åºå¤§çš„æ•°æ®é›†ã€‚è¿™æ˜¯å› ä¸ºè¿™ä¸ªæ•°æ®é›†å‡ ä¹ä»¥ç«äº‰çš„æ–¹å¼ç”¨äºè®ºæ–‡ã€‚æ‰€ä»¥ä»–ä»¬å¸Œæœ›æ¯ä¸ªäººéƒ½ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•é›†ï¼Œä»¥ä¾¿èƒ½å¤Ÿè¿›è¡Œåˆç†çš„æ¯”è¾ƒã€‚å› æ­¤ï¼Œå¦‚æœä½ è¯´ä½ åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°äº†æŸä¸ªç»“æœã€‚
- en: It's not just because you picked a lucky break between train and testã€‚ Nowã€‚
    if we want to display the digitsï¼Œ we can simply run thisã€‚ This shows you what's
    actually in this this data setã€‚ Nowï¼Œ noticeï¼Œ tooã€‚ we did pull the data set directly
    from Kisã€‚ Kis provides this for common dataset sets as a convenienceã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä»…ä»…æ˜¯å› ä¸ºä½ åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´é€‰æ‹©äº†ä¸€ä¸ªå¹¸è¿çš„ç»“æœã€‚å¦‚æœæˆ‘ä»¬æƒ³æ˜¾ç¤ºæ•°å­—ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿è¡Œè¿™ä¸ªã€‚è¿™æ˜¾ç¤ºäº†è¿™ä¸ªæ•°æ®é›†ä¸­å®é™…åŒ…å«çš„å†…å®¹ã€‚ç°åœ¨ï¼Œæ³¨æ„ï¼Œæˆ‘ä»¬ç¡®å®æ˜¯ç›´æ¥ä»Kisæå–æ•°æ®é›†çš„ã€‚Kisä¸ºå¸¸è§çš„æ•°æ®é›†æä¾›äº†è¿™ä¸ªä¾¿åˆ©ã€‚
- en: but this can be a real painã€‚ if you're trying to use your own imagesã€‚ And we'll
    talk about that in a future video how to pull in literally your own images raw
    Jpegs and P and Gsã€‚But for nowï¼Œ we'll use the convenience methods Hereã€‚ you can
    see one of the imagesã€‚ It's very bigã€‚ You can see probably some sort of a spiral
    thereã€‚ If you want to actually display itã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ å°è¯•ä½¿ç”¨è‡ªå·±çš„å›¾åƒï¼Œè¿™å¯èƒ½ä¼šéå¸¸éº»çƒ¦ã€‚æˆ‘ä»¬å°†åœ¨æœªæ¥çš„è§†é¢‘ä¸­è®¨è®ºå¦‚ä½•æå–ä½ è‡ªå·±çš„åŸå§‹JPEGå’ŒPNGå›¾åƒã€‚ä¸è¿‡ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™é‡Œçš„ä¾¿åˆ©æ–¹æ³•ã€‚ä½ å¯ä»¥çœ‹åˆ°å…¶ä¸­ä¸€å¼ å›¾åƒã€‚å®ƒéå¸¸å¤§ã€‚ä½ å¯èƒ½ä¼šçœ‹åˆ°æŸç§èºæ—‹å½¢ã€‚å¦‚æœä½ æƒ³å®é™…æ˜¾ç¤ºå®ƒã€‚
- en: You can run thisã€‚And it displays it some sort of a oneã€‚That's the 105th digit
    if you want to pickã€‚The next digit it would display itï¼Œ which is a sixã€‚ So this
    is essentially heat map that Matplot Live provides usã€‚ It provides a convenient
    way to visualize theseã€‚ If you want to see a whole bunchã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªã€‚å®ƒä»¥æŸç§æ–¹å¼æ˜¾ç¤ºå‡ºæ¥ã€‚è¿™æ˜¯ç¬¬105ä¸ªæ•°å­—ï¼Œå¦‚æœä½ æƒ³é€‰æ‹©ä¸‹ä¸€ä¸ªæ•°å­—ï¼Œå®ƒå°†æ˜¾ç¤ºå‡º6ã€‚æ‰€ä»¥è¿™æœ¬è´¨ä¸Šæ˜¯Matplot Liveæä¾›çš„çƒ­å›¾ã€‚å®ƒæä¾›äº†ä¸€ç§æ–¹ä¾¿çš„å¯è§†åŒ–æ–¹å¼ã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹å¾ˆå¤šæ•°æ®ã€‚
- en: you just do subplotsã€‚In Mapllibï¼Œ you run thatã€‚ and it will show you a whole
    bunch of digitsã€‚ So this is a good way to visualize some of these data sets and
    show that they really are just images that are being pulled in for youã€‚ You basically
    have to take the raw P and G and Jpeg files and turn them into tensors into cubes
    heightthe width by the color depthã€‚ Nowï¼Œ let's go ahead and build a neural network
    to train this onã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åªéœ€åšå­å›¾ã€‚åœ¨Mapllibä¸­è¿è¡Œå®ƒã€‚å®ƒä¼šæ˜¾ç¤ºä¸€å †æ•°å­—ã€‚æ‰€ä»¥è¿™æ˜¯å¯è§†åŒ–è¿™äº›æ•°æ®é›†çš„ä¸€ç§å¥½æ–¹æ³•ï¼Œæ˜¾ç¤ºå®ƒä»¬ç¡®å®åªæ˜¯ä¸ºä½ æå–çš„å›¾åƒã€‚ä½ åŸºæœ¬ä¸Šéœ€è¦å°†åŸå§‹PNGå’ŒJPEGæ–‡ä»¶è½¬æ¢ä¸ºå¼ é‡ï¼Œå˜æˆé«˜å®½åº¦ä¹˜ä»¥é¢œè‰²æ·±åº¦çš„ç«‹æ–¹ä½“ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è®­ç»ƒè¿™ä¸ªã€‚
- en: This is where we'll be glad we have a GPUã€‚ğŸ˜Šï¼ŒDoesn't take too much time to actually
    build thisã€‚ It shows some of our hyperparameter We're choosing a batch size of
    128ã€‚ We are doing some basic transformation on on the data set so that everything
    is in the right order that Tensorflow wants itã€‚ Typicallyï¼Œ weï¼Œ we need a height
    by width byã€‚The color depthã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬ä¼šå¾ˆé«˜å…´æœ‰GPUã€‚ğŸ˜Š å®é™…ä¸Šæ„å»ºè¿™ä¸ªå¹¶ä¸éœ€è¦å¤ªå¤šæ—¶é—´ã€‚å®ƒæ˜¾ç¤ºäº†æˆ‘ä»¬çš„ä¸€äº›è¶…å‚æ•°ï¼Œæˆ‘ä»¬é€‰æ‹©çš„æ‰¹å¤„ç†å¤§å°æ˜¯128ã€‚æˆ‘ä»¬å¯¹æ•°æ®é›†è¿›è¡Œä¸€äº›åŸºæœ¬è½¬æ¢ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡ç¬¦åˆTensorflowçš„è¦æ±‚ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé«˜åº¦ä¹˜ä»¥å®½åº¦ä¹˜ä»¥é¢œè‰²æ·±åº¦ã€‚
- en: And we will see how to actually do that whenã€‚How to do this on raw images When
    we get into dealing with loading our own images for thisã€‚ this is importantã€‚This
    is doing some basic normalization on itã€‚ So I said that neural networks deal best
    withã€‚Data that's always in the same rangeã€‚ Wellã€‚ you're always in the same range
    of 0 to 255 on the individual color components of RGBã€‚But here itã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å®é™…åšåˆ°è¿™ä¸€ç‚¹ã€‚å¦‚ä½•å¤„ç†åŸå§‹å›¾åƒã€‚å½“æˆ‘ä»¬åŠ è½½è‡ªå·±çš„å›¾åƒæ—¶ï¼Œè¿™æ˜¯é‡è¦çš„ã€‚è¿™æ˜¯åœ¨è¿›è¡Œä¸€äº›åŸºæœ¬è§„èŒƒåŒ–ã€‚æ‰€ä»¥æˆ‘è¯´ç¥ç»ç½‘ç»œæœ€å¥½å¤„ç†çš„æ•°æ®æ€»æ˜¯åœ¨ç›¸åŒèŒƒå›´å†…ã€‚å¥½å§ï¼Œä½ çš„RGBæ¯ä¸ªé¢œè‰²åˆ†é‡å§‹ç»ˆåœ¨0åˆ°255çš„ç›¸åŒèŒƒå›´å†…ã€‚ä½†è¿™é‡Œã€‚
- en: this is getting everything between 0 and 1ã€‚ since the normal range was 0 to
    2ã€‚55 for a little better resultï¼Œ you could even potentially center this about
    0ã€‚ So you'd probably subtract 128 and divide by 128ã€‚ We'll see an example of thatã€‚Later
    onã€‚We print out how many train sizes we've gotã€‚Here is where we build up the neural
    networkã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è·å–æ‰€æœ‰åœ¨0åˆ°1ä¹‹é—´çš„å€¼ã€‚å› ä¸ºæ­£å¸¸èŒƒå›´æ˜¯0åˆ°2.55ï¼Œä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œä½ ç”šè‡³å¯ä»¥å°†å…¶å›´ç»•0å±…ä¸­ã€‚æ‰€ä»¥ä½ å¯èƒ½éœ€è¦å‡å»128å¹¶é™¤ä»¥128ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°ä¸€ä¸ªä¾‹å­ã€‚æˆ‘ä»¬æ‰“å°å‡ºæœ‰å¤šå°‘è®­ç»ƒæ ·æœ¬ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬æ„å»ºç¥ç»ç½‘ç»œçš„åœ°æ–¹ã€‚
- en: We add in some convolution to the layersã€‚You don't really have a convolution
    3Dï¼Œ there might be oneã€‚ but typically you're dealing with 2D because you're recognizing
    2D imagesã€‚ it wouldn't surprise me if somebody has figured out a way to send in
    3D images for recognitionã€‚ be interesting how you would capture those you'd probably
    need two camerasã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å±‚ä¸­æ·»åŠ äº†ä¸€äº›å·ç§¯ã€‚ä½ å®é™…ä¸Šæ²¡æœ‰3Då·ç§¯ï¼Œå¯èƒ½ä¼šæœ‰ä¸€ä¸ªï¼Œä½†é€šå¸¸ä½ å¤„ç†çš„æ˜¯2Dï¼Œå› ä¸ºä½ åœ¨è¯†åˆ«2Då›¾åƒã€‚å¦‚æœæœ‰äººæ‰¾åˆ°äº†å‘é€3Då›¾åƒè¿›è¡Œè¯†åˆ«çš„æ–¹æ³•ï¼Œæˆ‘ä¸ä¼šæ„Ÿåˆ°æƒŠè®¶ã€‚ä½ å¯èƒ½éœ€è¦ä¸¤ä¸ªç›¸æœºæ¥æ•æ‰è¿™äº›ã€‚
- en: But we'll deal just with 2D imagesï¼Œ not 3Dã€‚ Then we put in the max pooling layerã€‚
    add some dropoutã€‚ The flattening layer is always needed when you move from these
    2D to the dense layers like we've had beforeã€‚ So always put that flatten in there
    because that basically now you can't go back once you flattenedã€‚ you can't use
    the convolution layers againï¼Œ or at least not without some extraordinary reshaping
    in thereã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬åªå¤„ç†2Då›¾åƒï¼Œè€Œä¸æ˜¯3Dã€‚ç„¶åæˆ‘ä»¬åŠ å…¥æœ€å¤§æ± åŒ–å±‚ï¼Œæ·»åŠ ä¸€äº›dropoutã€‚åœ¨ä»è¿™äº›2Då±‚è½¬æ¢åˆ°æˆ‘ä»¬ä¹‹å‰æåˆ°çš„å¯†é›†å±‚æ—¶ï¼Œæ€»æ˜¯éœ€è¦æ‰å¹³å±‚ã€‚æ‰€ä»¥ä¸€å®šè¦åŠ ä¸Šè¿™ä¸ªæ‰å¹³å±‚ï¼Œå› ä¸ºä¸€æ—¦æ‰å¹³åŒ–ï¼Œä½ å°±ä¸èƒ½å†å›å»ï¼Œæˆ–è€…è‡³å°‘æ²¡æœ‰ä¸€äº›éå¸¸ç‰¹æ®Šçš„é‡å¡‘ã€‚
- en: never say neverã€‚ And then you use a dense layerã€‚ and this is classificationã€‚
    So we're using the classes categorical crossenttropy and softmã€‚ just just like
    we've had many times beforeã€‚ Nowï¼Œ when you want to train and fit itã€‚ and notice
    these times that I have hereï¼Œ this takes nearly two hours on the CPU and 13 minutes
    on the GPã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ°¸è¿œä¸è¦è¯´æ°¸è¿œä¸ã€‚ç„¶åä½ ä½¿ç”¨ä¸€ä¸ªå¯†é›†å±‚ã€‚è¿™æ˜¯åˆ†ç±»ã€‚å› æ­¤æˆ‘ä»¬ä½¿ç”¨ç±»çš„ç±»åˆ«äº¤å‰ç†µå’Œsoftmaxï¼Œå°±åƒæˆ‘ä»¬ä»¥å‰çœ‹åˆ°çš„é‚£æ ·ã€‚ç°åœ¨ï¼Œå½“ä½ æƒ³è®­ç»ƒå¹¶é€‚åº”å®ƒæ—¶ï¼Œæ³¨æ„æˆ‘è¿™é‡Œçš„æ—¶é—´ï¼Œè¿™åœ¨CPUä¸Šéœ€è¦è¿‘ä¸¤ä¸ªå°æ—¶ï¼Œè€Œåœ¨GPUä¸Šåªéœ€13åˆ†é’Ÿã€‚
- en: It's not gonna to take you that much time on the GPã€‚ this Google GP that they
    give you for free is better than my GP that I that I ran this on about a year
    ago when I took those timesã€‚ So it's awesome what you getã€‚For freeã€‚Let's go ahead
    and run thisã€‚ we're going to go ahead and train itï¼Œ this is training code just
    like we've seen beforeã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨GPUä¸Šå¹¶ä¸ä¼šèŠ±ä½ å¤ªå¤šæ—¶é—´ã€‚è¿™ä¸ªè°·æ­Œå…è´¹æä¾›çš„GPUæ¯”æˆ‘å¤§çº¦ä¸€å¹´å‰è¿è¡Œæ—¶ç”¨çš„GPUè¦å¥½ã€‚æ‰€ä»¥ä½ å…è´¹è·å¾—çš„ä¸œè¥¿çœŸæ˜¯å¤ªæ£’äº†ã€‚è®©æˆ‘ä»¬ç»§ç»­è¿è¡Œè¿™ä¸ªã€‚æˆ‘ä»¬å°†ç»§ç»­è®­ç»ƒï¼Œè¿™æ®µä»£ç ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„è®­ç»ƒä»£ç ä¸€æ ·ã€‚
- en: I do time itï¼Œ so we'll see exactly how much time this takesã€‚ Now you want to
    run thatã€‚ It tends to take it a little bit of time to get goingï¼Œ but we're already
    on the first epochã€‚And it'sï¼Œ it's really pretty quickã€‚ We're on our second epochã€‚
    and itï¼Œ it took reallyã€‚ it's taking only about 4 seconds a epochã€‚ So I will go
    ahead and fast forward thisï¼Œ but it'sã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šè®¡æ—¶ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ç¡®åˆ‡çœ‹åˆ°è¿™éœ€è¦å¤šå°‘æ—¶é—´ã€‚ç°åœ¨ä½ æƒ³è¿è¡Œå®ƒã€‚å¯åŠ¨æ—¶é€šå¸¸éœ€è¦ä¸€ç‚¹æ—¶é—´ï¼Œä½†æˆ‘ä»¬å·²ç»è¿›å…¥ç¬¬ä¸€ä¸ªçºªå…ƒã€‚å®ƒï¼ŒçœŸçš„å¾ˆå¿«ã€‚æˆ‘ä»¬åœ¨ç¬¬äºŒä¸ªçºªå…ƒï¼Œå®ƒï¼Œå®é™…ä¸Šåªéœ€è¦å¤§çº¦4ç§’æ¯ä¸ªçºªå…ƒã€‚æ‰€ä»¥æˆ‘ä¼šç»§ç»­å¿«è¿›ï¼Œä½†å®ƒæ˜¯ã€‚
- en: it's not gonna take much timeã€‚ Just think this could be two hours on some CPUusã€‚
    And there we are 53 secondsã€‚ I love GPusã€‚ Nowï¼Œ there's two types of GPU that you're
    dealing with right nowã€‚ there's the K 80 that I think that's what it's called
    that Google gives you for for freeã€‚ That's the cheaper oneã€‚ It's probably around
    a $500 to $700 GPUã€‚ This is in 2019ã€‚ğŸ˜Šï¼ŒWellã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä¼šèŠ±è´¹å¤ªå¤šæ—¶é—´ã€‚æƒ³æƒ³çœ‹ï¼Œè¿™å¯èƒ½åœ¨ä¸€äº› CPU ä¸Šéœ€è¦ä¸¤ä¸ªå°æ—¶ã€‚è€Œæˆ‘ä»¬åªç”¨äº† 53 ç§’ã€‚æˆ‘å–œæ¬¢ GPUã€‚ç°åœ¨ï¼Œä½ æ­£åœ¨å¤„ç†ä¸¤ç§ç±»å‹çš„ GPUã€‚æœ‰ K80ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯å®ƒçš„åç§°ï¼ŒGoogle
    å…è´¹æä¾›ç»™ä½ ã€‚é‚£æ˜¯æ›´ä¾¿å®œçš„ï¼Œå¯èƒ½åœ¨ $500 åˆ° $700 çš„ GPU èŒƒå›´å†…ã€‚è¿™æ˜¯ 2019 å¹´çš„æƒ…å†µã€‚ğŸ˜Šï¼Œå¥½å§ã€‚
- en: I willm sure I will have updated the video by the time this that changes radicallyã€‚
    There's also a V 100 more advanced enterprise 1ã€‚ I use that on Amazon cloudã€‚ And
    that's probably a $6 or $7000 GPUã€‚ You would think that it would run even faster
    on that oneã€‚ but it does notã€‚ This is not a complicated enough neural networkã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šç¡®ä¿åœ¨è¿™ä¸ªè§†é¢‘å‘ç”Ÿé‡å¤§å˜åŒ–æ—¶å·²ç»æ›´æ–°äº†å®ƒã€‚è¿˜æœ‰ä¸€ä¸ªæ›´å…ˆè¿›çš„ V100 ä¼ä¸šçº§ GPUã€‚æˆ‘åœ¨äºšé©¬é€Šäº‘ä¸Šä½¿ç”¨è¿™ä¸ªã€‚å®ƒå¤§æ¦‚æ˜¯ $6000 æˆ– $7000
    çš„ GPUã€‚ä½ å¯èƒ½ä¼šè®¤ä¸ºåœ¨é‚£ä¸ªä¸Šè¿è¡Œä¼šæ›´å¿«ï¼Œä½†å¹¶ä¸ä¼šã€‚è¿™ä¸ªç¥ç»ç½‘ç»œå¹¶ä¸å¤æ‚ã€‚
- en: It runs about the same speed on this GPU versus the otherã€‚So when you are running
    on something like Amazon Cloudã€‚ you need to balance really and not overuse to
    advanced of a GPU or or you're simply wasting your moneyã€‚ You're dealing with
    something the more advanced GPU on Amazon Cloudï¼Œ at least in today's dollarsã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ª GPU ä¸Šçš„è¿è¡Œé€Ÿåº¦ä¸å¦ä¸€ä¸ªå·®ä¸å¤šã€‚å› æ­¤ï¼Œå½“ä½ åœ¨åƒäºšé©¬é€Šäº‘è¿™æ ·çš„ç¯å¢ƒä¸­è¿è¡Œæ—¶ï¼Œä½ éœ€è¦çœŸæ­£åœ°è¿›è¡Œå¹³è¡¡ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨é«˜çº§ GPUï¼Œå¦åˆ™ä½ åªæ˜¯åœ¨æµªè´¹é‡‘é’±ã€‚ä½ æ­£åœ¨å¤„ç†äºšé©¬é€Šäº‘ä¸Šæ›´å…ˆè¿›çš„
    GPUï¼Œè‡³å°‘åœ¨ä»Šå¤©çš„ä»·æ ¼ä¸Šã€‚
- en: it's about $4 an hourã€‚Versus under1 dollar an hour for this GPU that Google's
    given you for freeã€‚ so definitely use the Google oneã€‚For some of the assignments
    that I give youã€‚ you will need GPU level processing performance otherwise you're
    going to spend hours and hours and hours training and it'll take you a long timeã€‚
    so I'll discuss that when we get into some of those assignments because there's
    separate videos and explanations for each of the assignments Now let's evaluate
    the accuracyã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å°æ—¶å¤§çº¦ $4ï¼Œè€Œ Google æä¾›ç»™ä½ çš„è¿™ä¸ª GPU æ¯å°æ—¶ä¸åˆ° $1ã€‚å› æ­¤ï¼Œè‚¯å®šè¦ä½¿ç”¨ Google çš„é‚£ä¸ªã€‚åœ¨æˆ‘ç»™ä½ çš„ä¸€äº›ä½œä¸šä¸­ï¼Œä½ å°†éœ€è¦ GPU
    çº§åˆ«çš„å¤„ç†æ€§èƒ½ï¼Œå¦åˆ™ä½ å°†èŠ±è´¹æ•°å°æ—¶è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å°†èŠ±è´¹å¾ˆé•¿æ—¶é—´ã€‚å› æ­¤ï¼Œåœ¨è®¨è®ºè¿™äº›ä½œä¸šæ—¶ï¼Œæˆ‘ä¼šè¯´æ˜ï¼Œå› ä¸ºæ¯ä¸ªä½œä¸šéƒ½æœ‰å•ç‹¬çš„è§†é¢‘å’Œè§£é‡Šã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¯„ä¼°å‡†ç¡®æ€§ã€‚
- en: Let's just run this and look at the accuracyï¼Œ99%ã€‚ This is why fashion minst
    was introducedã€‚ because this this is a decent hello world that what are you going
    to do if you're trying to do any sort of research on thisã€‚ I meanï¼Œ you're going
    to be at 99999ã€‚ it's pointless to say that you increase the accuracy from 99%
    and 99ã€‚99999%ã€‚Now this is another thing that's useful to know tooï¼Œ when you're
    scoringã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªå¹¶çœ‹çœ‹å‡†ç¡®ç‡ï¼Œ99%ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¼•å…¥ Fashion MNIST çš„åŸå› ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸é”™çš„ Hello Worldï¼Œå¦‚æœä½ æƒ³å¯¹è¿™ä¸ªè¿›è¡Œä»»ä½•ç ”ç©¶ï¼Œä½ å°†ä¼šå¾—åˆ°
    99.9999 çš„ç»“æœã€‚è¯´ä½ å°†å‡†ç¡®ç‡ä» 99% æé«˜åˆ° 99.9999% æ˜¯æ¯«æ— æ„ä¹‰çš„ã€‚ç°åœ¨ï¼Œè¿˜æœ‰ä¸€ä»¶äº‹åœ¨è¯„åˆ†æ—¶ä¹Ÿå¾ˆæœ‰ç”¨ã€‚
- en: that is sending data to get values back predictions backï¼Œ if you're using a
    GPUã€‚ you might get a resource exhaust error that is simply because you have thrown
    too much data at the GPU for it to score at onceã€‚So you have a couple of options
    thereï¼Œ this usually won't come up during training because you're using mini batchesã€‚
    but if you're trying to score a big block of like a million rowsï¼Œ it might not
    fit in the GPUã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åœ¨å‘é€æ•°æ®ä»¥è·å–é¢„æµ‹å€¼æ—¶ï¼Œå¦‚æœä½ åœ¨ä½¿ç”¨ GPUï¼Œä½ å¯èƒ½ä¼šé‡åˆ°èµ„æºè€—å°½é”™è¯¯ï¼Œè¿™ä»…ä»…æ˜¯å› ä¸ºä½ ä¸€æ¬¡æ€§å‘ GPU æŠ•å…¥äº†å¤ªå¤šæ•°æ®è¿›è¡Œè¯„åˆ†ã€‚å› æ­¤ï¼Œä½ æœ‰å‡ ä¸ªé€‰æ‹©ï¼Œè¿™é€šå¸¸åœ¨è®­ç»ƒæœŸé—´ä¸ä¼šå‡ºç°ï¼Œå› ä¸ºä½ ä½¿ç”¨çš„æ˜¯å°æ‰¹é‡ã€‚ä½†æ˜¯å¦‚æœä½ è¯•å›¾å¯¹åƒä¸€ç™¾ä¸‡è¡Œè¿™æ ·çš„å¤§æ•°æ®å—è¿›è¡Œè¯„åˆ†ï¼Œå®ƒå¯èƒ½æ— æ³•é€‚åº”
    GPUã€‚
- en: coupleuple of options thereï¼Œ scoring is usually pretty quickã€‚ you can just send
    that to the CPU and it doesn't matter how big it is within reasonã€‚Or you can break
    it up into pieces and send each of those pieces one by one to the GPUã€‚ And you
    would use this kind of code to do it hereã€‚ if I wanted to just score the first
    100ã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰å‡ ä¸ªé€‰æ‹©ï¼Œè¯„åˆ†é€šå¸¸éå¸¸å¿«ã€‚ä½ å¯ä»¥å°†å…¶å‘é€åˆ° CPUï¼Œæ— è®ºå¤§å°ï¼Œåªè¦åœ¨åˆç†èŒƒå›´å†…éƒ½æ²¡å…³ç³»ã€‚æˆ–è€…ä½ å¯ä»¥å°†å…¶åˆ†æˆå‡ éƒ¨åˆ†ï¼Œä¸€æ¬¡å°†æ¯éƒ¨åˆ†å‘é€åˆ° GPUã€‚ä½ å¯ä»¥ä½¿ç”¨è¿™æ ·çš„ä»£ç æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘åªæƒ³è¯„åˆ†å‰
    100 ä¸ªã€‚
- en: So I give you an example of of doing thatã€‚ Nowï¼Œ let's look at mensed fashionã€‚
    Againã€‚ we're using convenience methods to load this in so that we don't have to
    have the data sets of all those fashion itemsã€‚ we're going go ahead and run thatã€‚
    it is downloading itã€‚ So when you run some of these for the first timeï¼Œ it will
    download itã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç»™ä½ ä¸¾ä¸ªä¾‹å­ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ—¶å°šæ•°æ®é›†ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ä¾¿æ·çš„æ–¹æ³•åŠ è½½æ•°æ®ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…æ‹¥æœ‰æ‰€æœ‰æ—¶å°šé¡¹ç›®çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ç»§ç»­è¿è¡Œï¼Œè¿™æ­£åœ¨ä¸‹è½½ã€‚å› æ­¤ï¼Œå½“ä½ ç¬¬ä¸€æ¬¡è¿è¡Œè¿™äº›æ—¶ï¼Œå®ƒä¼šä¸‹è½½æ•°æ®ã€‚
- en: if we want to display these just like beforeï¼Œ this is exactlyï¼Œ by the wayã€‚ the
    same code as the digitsã€‚ So I'm not going to explainï¼Œ reexplain it allã€‚ just going
    kind of show you what it looks likeã€‚Let's go ahead and run thisã€‚ I shouldn't really
    say digit9ã€‚ this is a fashion apparel number nineï¼Œ which is a shoeã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³åƒä¹‹å‰ä¸€æ ·æ˜¾ç¤ºè¿™äº›ï¼Œè¿™æ­£æ˜¯ï¼Œé¡ºä¾¿è¯´ä¸€ä¸‹ã€‚ä¸æ•°å­—çš„ä»£ç å®Œå…¨ç›¸åŒã€‚å› æ­¤æˆ‘ä¸æ‰“ç®—é‡æ–°è§£é‡Šè¿™ä¸€åˆ‡ã€‚åªæ˜¯ç¨å¾®ç»™ä½ å±•ç¤ºä¸€ä¸‹å®ƒçš„æ ·å­ã€‚è®©æˆ‘ä»¬æ¥è¿è¡Œä¸€ä¸‹ã€‚æˆ‘ä¸åº”è¯¥çœŸçš„è¯´æ˜¯æ•°å­—9ã€‚è¿™æ˜¯æ—¶å°šæœè£…çš„ç¬¬ä¹å·ï¼Œä¹Ÿå°±æ˜¯ä¸€åŒé‹ã€‚
- en: And if we want to display a whole range of them just like the digits we canã€‚
    So there's a block of fashion items that it's trying toã€‚Againï¼Œ pretty similarã€‚Training
    timesã€‚ I'm going to go ahead and defineã€‚The neural network just like beforeã€‚It's
    definedã€‚We're going to now train itã€‚And we'll see that the training time is really
    pretty comparableã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³åƒæ•°å­—ä¸€æ ·æ˜¾ç¤ºä¸€æ•´ç³»åˆ—çš„ç‰©å“ï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥çš„ã€‚å› æ­¤ï¼Œæœ‰ä¸€ç»„æ—¶å°šç‰©å“æ­£è¯•å›¾å±•ç¤ºå‡ºæ¥ã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™éå¸¸ç›¸ä¼¼ã€‚è®­ç»ƒæ—¶é—´ã€‚æˆ‘å°†ç»§ç»­å®šä¹‰ã€‚ç¥ç»ç½‘ç»œå°±åƒä¹‹å‰é‚£æ ·ã€‚å®ƒå·²ç»å®šä¹‰å¥½äº†ã€‚æˆ‘ä»¬ç°åœ¨å°†è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä¼šå‘ç°è®­ç»ƒæ—¶é—´ç¡®å®éå¸¸ç›¸ä¼¼ã€‚
- en: To what we are dealing with beforeã€‚ Notice the accuracy is not just pegging
    right up to 99%ã€‚ It's around 89%ã€‚90%ã€‚ So this is definitely more difficultã€‚ You
    would have toã€‚ you'd have to work a lot more to get this up to the higher levels
    of accuracyã€‚ And I have not lookedã€‚ This is not too much of a real data setã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æˆ‘ä»¬ä¹‹å‰å¤„ç†çš„æƒ…å†µç›¸æ¯”ã€‚æ³¨æ„ï¼Œå‡†ç¡®ç‡å¹¶æ²¡æœ‰ç›´æ¥ä¸Šå‡åˆ°99%ã€‚å¤§çº¦åœ¨89%ã€‚90%ã€‚æ‰€ä»¥è¿™æ˜¾ç„¶æ›´å›°éš¾ã€‚ä½ éœ€è¦ã€‚ä½ å¿…é¡»ä»˜å‡ºæ›´å¤šåŠªåŠ›æ‰èƒ½å°†å…¶æå‡åˆ°æ›´é«˜çš„å‡†ç¡®ç‡æ°´å¹³ã€‚è€Œä¸”æˆ‘æ²¡æœ‰æŸ¥çœ‹ã€‚è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå¤ªçœŸå®çš„æ•°æ®é›†ã€‚
- en: I suppose it's used some in researchï¼Œ but I have not looked at whereã€‚Some of
    the the more advanced researchers have gotten this up toã€‚ but it looks like we're
    going to be right at around 92% of validation accuracyã€‚ and it looks like it'sã€‚![](img/194e506320f5e9abc6ca37065bbd9036_13.png)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³å®ƒåœ¨ç ”ç©¶ä¸­æœ‰ä¸€äº›ä½¿ç”¨ï¼Œä½†æˆ‘æ²¡æœ‰æŸ¥çœ‹è¿‡å…·ä½“åœ¨å“ªé‡Œã€‚ä¸€äº›æ›´å…ˆè¿›çš„ç ”ç©¶äººå‘˜å°†å…¶æå‡åˆ°å“ªé‡Œã€‚ä½†çœ‹èµ·æ¥æˆ‘ä»¬å¤§çº¦åœ¨92%çš„éªŒè¯å‡†ç¡®ç‡ã€‚å¹¶ä¸”çœ‹èµ·æ¥å®ƒæ˜¯ã€‚![](img/194e506320f5e9abc6ca37065bbd9036_13.png)
- en: Kind of stopping thereã€‚ Thank you for watching this videoã€‚ In the next videoã€‚
    we're going to continue with computer vision and look at resnetsã€‚ This content
    changes oftenã€‚ so subscribe to the channel to stay up to date on this course and
    other topics in artificial intelligenceã€‚ğŸ˜Šã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è‡´åœåœ¨è¿™é‡Œã€‚æ„Ÿè°¢ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ã€‚æˆ‘ä»¬å°†ç»§ç»­è®¨è®ºè®¡ç®—æœºè§†è§‰ï¼Œå¹¶ç ”ç©¶æ®‹å·®ç½‘ç»œï¼ˆresnetsï¼‰ã€‚è¿™éƒ¨åˆ†å†…å®¹ç»å¸¸æ›´æ–°ã€‚å› æ­¤è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿éšæ—¶äº†è§£æœ¬è¯¾ç¨‹åŠå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚ğŸ˜Š
