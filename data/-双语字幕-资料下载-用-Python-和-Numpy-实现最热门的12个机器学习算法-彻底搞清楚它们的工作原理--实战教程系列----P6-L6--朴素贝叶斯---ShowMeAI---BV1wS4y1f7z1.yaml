- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- æœ´ç´ è´å¶æ–¯
    - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P6ï¼šL6- æœ´ç´ è´å¶æ–¯
    - ShowMeAI - BV1wS4y1f7z1
- en: Hiï¼Œ everybodyã€‚ Welcome to a new machine learning from scratch tutorialã€‚ Todayã€‚
    we are going to implement the naive by classifier using only built and Python
    modules and Nyã€‚ğŸ˜Šã€‚So the naive bias classifier is based on the biaseth theoremï¼Œ
    which says that if we have two eventsã€‚ A and Bï¼Œ then the probability of event
    Aï¼Œ given that B has already happenedã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°ä¸€ä¸ªæ–°çš„ä»é›¶å¼€å§‹çš„æœºå™¨å­¦ä¹ æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨æ„å»ºå’ŒPythonæ¨¡å—åŠNumpyå®ç°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€‚ğŸ˜Š æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯åŸºäºè´å¶æ–¯å®šç†çš„ï¼Œè¯¥å®šç†è¡¨æ˜ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªäº‹ä»¶Aå’ŒBï¼Œé‚£ä¹ˆäº‹ä»¶Açš„æ¦‚ç‡ï¼Œç»™å®šBå·²ç»å‘ç”Ÿã€‚
- en: is equal to the probability of Bï¼Œ given that A has happenedã€‚ times the probability
    of a divided by the probability of Bã€‚And if we apply this to our caseã€‚ then our
    formula is the probability of y of our class Yã€‚ given the feature vector X is
    equal to the probability of x given y times P of y divided by P of xã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰äºäº‹ä»¶Bçš„æ¦‚ç‡ï¼Œç»™å®šäº‹ä»¶Aå‘ç”Ÿçš„æ¦‚ç‡ï¼Œä¹˜ä»¥äº‹ä»¶Açš„æ¦‚ç‡é™¤ä»¥äº‹ä»¶Bçš„æ¦‚ç‡ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„æ¡ˆä¾‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„å…¬å¼æ˜¯ç±»Yçš„æ¦‚ç‡ï¼Œç»™å®šç‰¹å¾å‘é‡Xï¼Œç­‰äºç»™å®šYçš„Xçš„æ¦‚ç‡ä¹˜ä»¥P(y)ï¼Œå†é™¤ä»¥P(x)ã€‚
- en: And where x is our feature of vectorï¼Œ which consists of several featuresã€‚And
    now it's called naive by years because now we make the assumption that all features
    are mutually independentã€‚ which meansï¼Œ for exampleï¼Œ if you want to predict the
    probability that a person is going out for a runã€‚Given the feature that the sun
    is shining and alsoï¼Œ given the feature that the person is healthyã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œxæ˜¯æˆ‘ä»¬çš„ç‰¹å¾å‘é‡ï¼Œå®ƒç”±å¤šä¸ªç‰¹å¾ç»„æˆã€‚ç°åœ¨ä¹‹æ‰€ä»¥ç§°ä¸ºæœ´ç´ æ˜¯å› ä¸ºæˆ‘ä»¬å‡è®¾æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚è¿™æ„å‘³ç€ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³é¢„æµ‹ä¸€ä¸ªäººå‡ºå»è·‘æ­¥çš„æ¦‚ç‡ï¼Œç»™å®šå¤ªé˜³åœ¨ç…§è€€ï¼Œä»¥åŠç»™å®šè¿™ä¸ªäººå¥åº·çš„ç‰¹å¾ã€‚
- en: Then both of these features might be independentï¼Œ but both contribute to this
    probability that the person goes outã€‚So in real lifeï¼Œ a lot of features are not
    mutually independentã€‚ but this assumption works fine for a lot of problemsã€‚And
    with this assumptionï¼Œ we can splitã€‚Thisã€‚Probability into the and use the chain
    ruleã€‚ So we calculate the probability for eachã€‚Featureã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™ä¸¤ä¸ªç‰¹å¾å¯èƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä½†éƒ½å¯¹è¿™ä¸ªäººå‡ºå»çš„æ¦‚ç‡æœ‰è´¡çŒ®ã€‚å› æ­¤åœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œå¾ˆå¤šç‰¹å¾å¹¶ä¸æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä½†è¿™ä¸ªå‡è®¾åœ¨å¾ˆå¤šé—®é¢˜ä¸Šæ•ˆæœè‰¯å¥½ã€‚æ ¹æ®è¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ¦‚ç‡åˆ†å‰²ã€‚å¹¶ä½¿ç”¨é“¾å¼æ³•åˆ™ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸ºæ¯ä¸ªç‰¹å¾è®¡ç®—æ¦‚ç‡ã€‚
- en: Given why and multiply eachã€‚And then we multiply it with p of y and divided
    by p of xã€‚And by the wayã€‚ this P of y given x is called the posterior probabilityã€‚
    P of x given y is called the class conditional probabilityã€‚ and P of y is called
    the prior probability of yï¼Œ and P of x is called the prior probability of xã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šyï¼Œå¹¶ä¹˜ä»¥æ¯ä¸€ä¸ªã€‚ç„¶åæˆ‘ä»¬å°†å…¶ä¹˜ä»¥P(y)å¹¶é™¤ä»¥P(x)ã€‚é¡ºä¾¿æä¸€ä¸‹ï¼ŒP(y|x)è¢«ç§°ä¸ºåéªŒæ¦‚ç‡ï¼ŒP(x|y)è¢«ç§°ä¸ºç±»æ¡ä»¶æ¦‚ç‡ï¼ŒP(y)è¢«ç§°ä¸ºyçš„å…ˆéªŒæ¦‚ç‡ï¼Œè€ŒP(x)è¢«ç§°ä¸ºxçš„å…ˆéªŒæ¦‚ç‡ã€‚
- en: And now we want to make a classificationã€‚ So given this posterior probabilityã€‚
    we want to select the class with the highest probabilityã€‚ So we choose yï¼Œ which
    is the arc max of yã€‚Of this posterior probabilityã€‚And now we can apply our formulaã€‚
    And since we are only interested in Yï¼Œ we don't need this P of x so we can cross
    this outã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³è¦è¿›è¡Œåˆ†ç±»ã€‚ç»™å®šè¿™ä¸ªåéªŒæ¦‚ç‡ï¼Œæˆ‘ä»¬å¸Œæœ›é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»ã€‚æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©yï¼Œå³yçš„å¼§æœ€å¤§å€¼ã€‚åœ¨è¿™ä¸ªåéªŒæ¦‚ç‡ä¸­ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åº”ç”¨æˆ‘ä»¬çš„å…¬å¼ã€‚å› ä¸ºæˆ‘ä»¬åªå¯¹Yæ„Ÿå…´è¶£ï¼Œæ‰€ä»¥ä¸éœ€è¦è¿™ä¸ªP(x)ï¼Œæ‰€ä»¥å¯ä»¥å°†å…¶åˆ’å»ã€‚
- en: And then our formula is thisï¼Œ soã€‚Why is the arc max ofã€‚And then we multiply
    each class conditional probabilityï¼Œ and then the prior probabilityã€‚And then we
    use a little trickã€‚Since all these values are are probabilities between 0 and
    1ã€‚ So if we multiply a lot of these valuesï¼Œ then we get very small numbersã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çš„å…¬å¼æ˜¯è¿™æ ·ï¼Œä¸ºä»€ä¹ˆæ˜¯å¼§æœ€å¤§å€¼ã€‚ç„¶åæˆ‘ä»¬ä¹˜ä»¥æ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡ï¼Œå†ä¹˜ä»¥å…ˆéªŒæ¦‚ç‡ã€‚ç„¶åæˆ‘ä»¬ç”¨ä¸€ä¸ªå°æŠ€å·§ã€‚å› ä¸ºæ‰€æœ‰è¿™äº›å€¼éƒ½æ˜¯åœ¨0åˆ°1ä¹‹é—´çš„æ¦‚ç‡ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬ä¹˜ä»¥å¾ˆå¤šè¿™äº›å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¼šå¾—åˆ°éå¸¸å°çš„æ•°å­—ã€‚
- en: and we might run into overflow problemsã€‚ So in order to prevent thisï¼Œ we apply
    the lock functionã€‚ So we apply the lock for each of these probabilitiesã€‚ and with
    the lock or the rules for logarithmussï¼Œ we can change the multiplication sign
    into an a plus signã€‚ So now we have an addition hereã€‚And now we have this formula
    that we needã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°æº¢å‡ºé—®é¢˜ã€‚å› æ­¤ï¼Œä¸ºäº†é˜²æ­¢è¿™ä¸ªï¼Œæˆ‘ä»¬åº”ç”¨é”å®šåŠŸèƒ½ã€‚æˆ‘ä»¬å¯¹æ¯ä¸€ä¸ªæ¦‚ç‡åº”ç”¨é”å®šã€‚é€šè¿‡é”å®šæˆ–å¯¹æ•°çš„è§„åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¹˜æ³•ç¬¦å·è½¬æ¢ä¸ºåŠ æ³•ç¬¦å·ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªåŠ æ³•ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬éœ€è¦çš„è¿™ä¸ªå…¬å¼ã€‚
- en: and now we need to come up with thisã€‚AP probabilityï¼Œ So the prior probability
    is just the frequencyã€‚We can see this in a secondã€‚ And then what is this class
    conditional probabilityï¼Œ P of x given yã€‚And here we model this with a gausian
    distributionã€‚ So here we can see the formulaã€‚ So this is one overã€‚ and then the
    square root of2 pi times the variance of yã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦æå‡ºè¿™ä¸ªã€‚AP æ¦‚ç‡ï¼Œæ‰€ä»¥å…ˆéªŒæ¦‚ç‡å°±æ˜¯é¢‘ç‡ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ä¸€ç¬é—´çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ç„¶åè¿™ä¸ªç±»åˆ«æ¡ä»¶æ¦‚ç‡ï¼Œå³ Pï¼ˆx | yï¼‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨é«˜æ–¯åˆ†å¸ƒæ¥å»ºæ¨¡ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªå…¬å¼ã€‚æ‰€ä»¥è¿™æ˜¯
    1 é™¤ä»¥ï¼Œç„¶åæ˜¯ 2Ï€çš„å¹³æ–¹æ ¹ä¹˜ä»¥ y çš„æ–¹å·®ã€‚
- en: Times the exponential function of minus x minus the mean value squared divided
    by two times the varianceã€‚And here we see a plot of the Gaussian function for
    different means and variations variancesã€‚So this is a probability that is always
    between0 and one andã€‚Yeahï¼Œ with this formulasã€‚ this is all we need to get startedã€‚
    So now we canã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹˜ä»¥æŒ‡æ•°å‡½æ•°çš„è´Ÿ x å‡å»å‡å€¼çš„å¹³æ–¹é™¤ä»¥ 2 ä¹˜ä»¥æ–¹å·®ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸åŒå‡å€¼å’Œæ–¹å·®çš„é«˜æ–¯å‡½æ•°çš„å›¾ã€‚è¿™ä¸ªæ¦‚ç‡æ€»æ˜¯åœ¨ 0 å’Œ 1 ä¹‹é—´ï¼Œæ˜¯çš„ï¼Œä½¿ç”¨è¿™äº›å…¬å¼ã€‚è¿™æ˜¯æˆ‘ä»¬å¼€å§‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å¯ä»¥ã€‚
- en: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_1.png)'
- en: Start and implement itã€‚ And first of allï¼Œ of courseï¼Œ we import Ny S N Pã€‚And
    then we create a class called naiveã€‚But isã€‚And it doesn't need an in it method
    so we can implement the fit method firstã€‚ so we want to fit training data and
    training labelsã€‚ And then we also want to implement a predict methodã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€å§‹å¹¶å®ç°å®ƒã€‚é¦–å…ˆï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬å¯¼å…¥ Ny S N Pã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåä¸º naive çš„ç±»ã€‚ä½†å®ƒä¸éœ€è¦ä¸€ä¸ª init æ–¹æ³•ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é¦–å…ˆå®ç° fit
    æ–¹æ³•ã€‚æˆ‘ä»¬å¸Œæœ›æ‹Ÿåˆè®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ ‡ç­¾ã€‚ç„¶åæˆ‘ä»¬è¿˜æƒ³å®ç°ä¸€ä¸ªé¢„æµ‹æ–¹æ³•ã€‚
- en: So here we predict the test labels or test samplesã€‚And now let's startã€‚ So let's
    start with the fit methodã€‚ So what we can do hereã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_3.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬é¢„æµ‹æµ‹è¯•æ ‡ç­¾æˆ–æµ‹è¯•æ ·æœ¬ã€‚ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ã€‚æ‰€ä»¥æˆ‘ä»¬ä» fit æ–¹æ³•å¼€å§‹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œåšä»€ä¹ˆã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_3.png)
- en: Is so we need the priorsï¼Œ and we can calculate them in this fit methodã€‚ and
    we need the class conditionalã€‚ So here we need the mean for each class and the
    variance for each classã€‚ So we can also calculate theseã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_5.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆéªŒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ª fit æ–¹æ³•ä¸­è®¡ç®—å®ƒä»¬ã€‚æˆ‘ä»¬éœ€è¦ç±»åˆ«æ¡ä»¶ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦æ¯ä¸ªç±»åˆ«çš„å‡å€¼å’Œæ–¹å·®ã€‚å› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥è®¡ç®—è¿™äº›ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_5.png)
- en: So let's do thisã€‚ So let's get the number of samples and the number of features
    firstã€‚And by the wayã€‚ our input hereï¼Œ the X is a nuy N D array where the first
    dimension is the number of samplesã€‚ and the second dimension or the number of
    rows is the number of samples and the number of columns is the number of featuresã€‚
    so we can unpack this and say this is x dot shapeã€‚And our y is a 1 D row vector
    also of sizeã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬è¿™æ ·åšã€‚é¦–å…ˆè·å–æ ·æœ¬æ•°é‡å’Œç‰¹å¾æ•°é‡ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ä»¬çš„è¾“å…¥ X æ˜¯ä¸€ä¸ª nuy N D æ•°ç»„ï¼Œå…¶ä¸­ç¬¬ä¸€ç»´æ˜¯æ ·æœ¬æ•°é‡ï¼Œç¬¬äºŒç»´æˆ–è¡Œæ•°æ˜¯æ ·æœ¬æ•°é‡ï¼Œåˆ—æ•°æ˜¯ç‰¹å¾æ•°é‡ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è§£åŒ…è¿™ä¸ªå¹¶è¯´è¿™æ˜¯
    x.shapeã€‚æˆ‘ä»¬çš„ y æ˜¯ä¸€ä¸ª 1D è¡Œå‘é‡ï¼Œå¤§å°ä¹Ÿä¸ºã€‚
- en: the number of samplesã€‚ So this is our inputã€‚ And now let's get the unique classesã€‚
    let's say self classes equals nuy unique of yã€‚ So this will find the unique elements
    of an arrayã€‚ So if we have two classesï¼Œ0 and 1ã€‚ then this will be an arrayã€‚Just
    with 1ï¼Œ0 and 1 y in it and1ã€‚1 in itã€‚Then let's say the number of classes equalsã€‚ğŸ¤¢ï¼ŒThe
    length of this selfã€‚Classesã€‚And nowã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬æ•°é‡ã€‚è¿™æ˜¯æˆ‘ä»¬çš„è¾“å…¥ã€‚ç°åœ¨è®©æˆ‘ä»¬è·å–å”¯ä¸€ç±»åˆ«ã€‚å‡è®¾ self.classes ç­‰äº nuy.unique(y)ã€‚è¿™å°†æ‰¾åˆ°æ•°ç»„çš„å”¯ä¸€å…ƒç´ ã€‚å¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œ0
    å’Œ 1ã€‚é‚£ä¹ˆè¿™å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œé‡Œé¢ä»…åŒ…å« 1ã€0 å’Œ 1 yï¼Œä»¥åŠ 1ã€‚å‡è®¾ç±»åˆ«çš„æ•°é‡ç­‰äºã€‚ğŸ¤¢ï¼Œè¿™ä¸ª self.classes çš„é•¿åº¦ã€‚ç°åœ¨ã€‚
- en: let's in it orã€‚In it meanã€‚Variarianance and priorsã€‚ So let's say self dot mean
    equalsã€‚ And we want to in them with zeros at firstã€‚ And it gets the sizeã€‚ It has
    number of classes and number of features as tuple hereã€‚ So it alsoã€‚For eachã€‚Classã€‚It
    hasã€‚The same number ofã€‚Or for each classã€‚We needã€‚Means for each featureã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆå§‹åŒ–æˆ–ã€‚åˆå§‹åŒ–å‡å€¼ã€æ–¹å·®å’Œå…ˆéªŒã€‚å‡è®¾ self.mean ç­‰äºã€‚æˆ‘ä»¬æƒ³å…ˆç”¨é›¶åˆå§‹åŒ–ã€‚å®ƒçš„å¤§å°ä¸ºç±»åˆ«æ•°é‡å’Œç‰¹å¾æ•°é‡çš„å…ƒç»„ã€‚æ‰€ä»¥å®ƒä¹Ÿã€‚å¯¹äºæ¯ä¸ªã€‚ç±»åˆ«ã€‚å®ƒæœ‰ã€‚æ¯ä¸ªç‰¹å¾æ‰€éœ€çš„å‡å€¼æ•°é‡ã€‚
- en: And we want to get this or give this a data type of float nuy dot float 64ã€‚And
    we want to do this with the same for the variancesã€‚ So let's say self doã€‚ç©å„¿ã€‚Equals
    thisã€‚ And then we want to do self dotã€‚Priers equals N dot zerosã€‚ And here for
    each classã€‚ we want one priorã€‚ So this is just a 1 D vector of size number of
    classes with a data type of N dot float 64ã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦å°†è¿™ä¸ªæˆ–ç»™å®ƒä¸€ä¸ª float ç±»å‹ï¼Œå³ nuy.float64ã€‚æˆ‘ä»¬æƒ³å¯¹æ–¹å·®åšåŒæ ·çš„äº‹æƒ…ã€‚å‡è®¾ self.doã€‚ç©å„¿ã€‚ç­‰äºè¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬å¸Œæœ› self.priers
    ç­‰äº N.zerosã€‚åœ¨è¿™é‡Œï¼Œå¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ä¸ªå…ˆéªŒã€‚æ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªå¤§å°ä¸ºç±»åˆ«æ•°é‡çš„ 1D å‘é‡ï¼Œæ•°æ®ç±»å‹ä¸º N.float64ã€‚
- en: And now let's calculate themã€‚ So for each class in self dot classesï¼Œ weã€‚Nowã€‚
    only we only want the samples that has this class as labelsã€‚ So let's call this
    Xï¼Œ C equals Xã€‚ and then whereã€‚C equals equalsã€‚Whyã€‚And now we can calculate the
    mean for each class and fill our self do meanã€‚ So we want to fillã€‚Thisã€‚Row and
    all columns hereã€‚ And we sayï¼Œ this is xã€‚Xï¼Œ Cã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è®¡ç®—å®ƒä»¬ã€‚å› æ­¤å¯¹äºself dot classesä¸­çš„æ¯ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬ã€‚ç°åœ¨ã€‚æˆ‘ä»¬åªæƒ³è¦å°†è¿™ä¸ªç±»åˆ«ä½œä¸ºæ ‡ç­¾çš„æ ·æœ¬ã€‚å› æ­¤æˆ‘ä»¬å°†å…¶ç§°ä¸ºXï¼ŒCç­‰äºXã€‚ç„¶ååœ¨å“ªé‡Œã€‚Cç­‰äºç­‰äºã€‚Yã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªç±»åˆ«è®¡ç®—å‡å€¼å¹¶å¡«å……æˆ‘ä»¬çš„self
    do meanã€‚å› æ­¤æˆ‘ä»¬æƒ³å¡«å……ã€‚è¿™ä¸€è¡Œå’Œæ‰€æœ‰åˆ—ã€‚æˆ‘ä»¬è¯´ï¼Œè¿™æ˜¯xã€‚Xï¼ŒCã€‚
- en: dot and nuly has a or a N Dã€‚Aray has to built in mean functionsã€‚ So we can say
    mean along the axis0ã€‚ So please check the meanã€‚Function for yourselfã€‚And we want
    to do the same thing for varã€‚ So self var in this row for each columnã€‚Is X C dot
    varã€‚ So Numpy also has a var methodã€‚And then we calculate the priorï¼Œ So selfã€‚Friersã€‚Of
    this classã€‚Is equal toã€‚ And nowã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: dotå’Œnulyæœ‰ä¸€ä¸ªæˆ–ä¸€ä¸ªN Dã€‚æ•°ç»„å¿…é¡»å†…ç½®å‡å€¼å‡½æ•°ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥æ²¿ç€è½´0è®¡ç®—å‡å€¼ã€‚å› æ­¤è¯·è‡ªå·±æ£€æŸ¥å‡å€¼ã€‚å‡½æ•°ã€‚æˆ‘ä»¬æƒ³å¯¹varåšåŒæ ·çš„äº‹æƒ…ã€‚å› æ­¤self
    varåœ¨è¿™ä¸€è¡Œçš„æ¯ä¸€åˆ—ã€‚æ˜¯X C dot varã€‚å› æ­¤Numpyä¹Ÿæœ‰ä¸€ä¸ªvaræ–¹æ³•ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—å…ˆéªŒï¼Œå› æ­¤selfã€‚Priersã€‚è¿™ä¸ªç±»åˆ«ã€‚ç­‰äºã€‚ç°åœ¨ã€‚
- en: what information do we already haveã€‚If we have the the training samples and
    training labelsã€‚ so we can say the prior probability that this class will occur
    is equal to the frequency of this class in the training samplesã€‚Soï¼Œ we sayã€‚We
    get Xï¼Œ Cã€‚The shapeã€‚0ï¼Œ so onlyã€‚This will get the numberã€‚Of samples with this labelã€‚And
    then we divide it by the number of total samplesã€‚ Soã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æŒæ¡äº†å“ªäº›ä¿¡æ¯ã€‚å¦‚æœæˆ‘ä»¬æœ‰è®­ç»ƒæ ·æœ¬å’Œè®­ç»ƒæ ‡ç­¾ã€‚æˆ‘ä»¬å¯ä»¥è¯´è¿™ä¸ªç±»åˆ«å‘ç”Ÿçš„å…ˆéªŒæ¦‚ç‡ç­‰äºè¯¥ç±»åˆ«åœ¨è®­ç»ƒæ ·æœ¬ä¸­çš„é¢‘ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯´ã€‚æˆ‘ä»¬å¾—åˆ°Xï¼ŒCã€‚å½¢çŠ¶ä¸º0ï¼Œå› æ­¤åªæœ‰ã€‚è¿™å°†è·å¾—å¸¦æœ‰æ­¤æ ‡ç­¾çš„æ ·æœ¬æ•°ã€‚ç„¶åæˆ‘ä»¬å°†å…¶é™¤ä»¥æ€»æ ·æœ¬æ•°ã€‚å› æ­¤ã€‚
- en: and we have to convert this to a float because we don't want integers hereã€‚
    So let's say float number of samplesã€‚So this is the frequencyã€‚ How often this
    class C is occurringã€‚And now this is all for our fit methodã€‚ And now let's implement
    the predict methodã€‚ So and for thisã€‚ we create a little helper methodã€‚ So let's
    call this underscore predict selfã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»å°†å…¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›è¿™é‡Œæ˜¯æ•´æ•°ã€‚å› æ­¤æˆ‘ä»¬è¯´æµ®ç‚¹æ•°æ ·æœ¬æ•°ã€‚æ‰€ä»¥è¿™æ˜¯é¢‘ç‡ã€‚è¿™ä¸ªç±»åˆ«Cå‡ºç°çš„é¢‘ç‡ã€‚ç°åœ¨è¿™å°±æ˜¯æˆ‘ä»¬fitæ–¹æ³•çš„å…¨éƒ¨ã€‚ç°åœ¨è®©æˆ‘ä»¬å®ç°predictæ–¹æ³•ã€‚å› æ­¤ä¸ºäº†è¿™ä¸ªã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå°çš„è¾…åŠ©æ–¹æ³•ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸º_é¢„æµ‹selfã€‚
- en: and this will only get one samplesã€‚ So here we have we can have multiple samplesã€‚
    So here we say why predict equals and then we use list comprehensionã€‚And called
    this underscoreã€‚ predict methodã€‚For only one sampleã€‚ And then we do this for each
    sample in our test samplesã€‚And thenï¼Œ we return themã€‚And now we have to implement
    our underscore predict methodã€‚So hereã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªä¼šè·å–ä¸€ä¸ªæ ·æœ¬ã€‚å› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥æœ‰å¤šä¸ªæ ·æœ¬ã€‚å› æ­¤æˆ‘ä»¬è¯´ä¸ºä»€ä¹ˆé¢„æµ‹ç­‰äºï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨åˆ—è¡¨æ¨å¯¼ã€‚è°ƒç”¨è¿™ä¸ª_é¢„æµ‹æ–¹æ³•ã€‚ä»…ç”¨äºä¸€ä¸ªæ ·æœ¬ã€‚ç„¶åæˆ‘ä»¬å¯¹æµ‹è¯•æ ·æœ¬ä¸­çš„æ¯ä¸ªæ ·æœ¬æ‰§è¡Œæ­¤æ“ä½œã€‚ç„¶åï¼Œæˆ‘ä»¬è¿”å›å®ƒä»¬ã€‚ç°åœ¨æˆ‘ä»¬å¿…é¡»å®ç°æˆ‘ä»¬çš„_é¢„æµ‹æ–¹æ³•ã€‚å› æ­¤åœ¨è¿™é‡Œã€‚
- en: what we need now is we needã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_7.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨éœ€è¦çš„æ˜¯æˆ‘ä»¬éœ€è¦ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_7.png)
- en: To apply this functionã€‚Soã€‚We have to calculate the posterior probabilityã€‚And
    calculate theã€‚Class conditional and the prior for each oneã€‚ And then choose the
    class with the highest probabilityã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_9.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨æ­¤å‡½æ•°ã€‚å› æ­¤ã€‚æˆ‘ä»¬å¿…é¡»è®¡ç®—åéªŒæ¦‚ç‡ã€‚å¹¶è®¡ç®—ã€‚æ¯ä¸ªç±»åˆ«æ¡ä»¶å’Œå…ˆéªŒï¼Œç„¶åé€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_9.png)
- en: So let's create an empty list called posteriorsã€‚Equals an empty listã€‚And now
    let's go over each classã€‚So let's say for index and C inã€‚Enumerateã€‚Self dot classesã€‚
    So here we get also the indexã€‚And the class ladleï¼Œ with this enumerate functionã€‚And
    now we can sayã€‚Firstï¼Œ we get the piaã€‚ So the pia equalsï¼Œ and we already have calculated
    the pã€‚ So this isã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåä¸ºposteriorsçš„ç©ºåˆ—è¡¨ã€‚ç­‰äºä¸€ä¸ªç©ºåˆ—è¡¨ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬éå†æ¯ä¸ªç±»åˆ«ã€‚å‡è®¾å¯¹äºç´¢å¼•å’ŒCåœ¨ã€‚æšä¸¾ã€‚Self dot classesã€‚å› æ­¤è¿™é‡Œæˆ‘ä»¬ä¹Ÿå¾—åˆ°äº†ç´¢å¼•å’Œç±»åˆ«æ ‡ç­¾ï¼Œä½¿ç”¨è¿™ä¸ªæšä¸¾å‡½æ•°ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¾—åˆ°piaã€‚é‚£ä¹ˆpiaç­‰äºï¼Œæˆ‘ä»¬å·²ç»è®¡ç®—å‡ºçš„pã€‚å› æ­¤è¿™æ˜¯ã€‚
- en: The prior of self dotã€‚Priersã€‚With this in current indexã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_11.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: self dot Priersçš„å…ˆéªŒã€‚åœ¨å½“å‰ç´¢å¼•ä¸­ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_11.png)
- en: And nowï¼Œ as I saidï¼Œ then at the endï¼Œ we apply the lock functionã€‚ So let's apply
    this right hereã€‚ So let's say NP P dot lockã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_13.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæœ€åæˆ‘ä»¬åº”ç”¨é”å®šå‡½æ•°ã€‚å› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œåº”ç”¨å®ƒã€‚å› æ­¤æˆ‘ä»¬è¯´NP P dot lockã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_13.png)
- en: And then create the posteriorã€‚Soï¼Œ the posterior equalsã€‚onï¼Œ let's call thisã€‚Classã€‚Conditionalã€‚Equalsã€‚And
    then we apply the gause functionsã€‚ So for thisã€‚ let's create this helper function
    and call this probability density functionã€‚With selfã€‚ And then it getsã€‚The class
    indexã€‚And then it gets xã€‚And here we apply this formula hereã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååˆ›å»ºåéªŒã€‚æ‰€ä»¥ï¼ŒåéªŒç­‰äºã€‚è®©æˆ‘ä»¬ç§°ä¹‹ä¸ºã€‚ç±»ã€‚æ¡ä»¶ã€‚ç­‰äºã€‚ç„¶åæˆ‘ä»¬åº”ç”¨é«˜æ–¯å‡½æ•°ã€‚æ‰€ä»¥å¯¹äºè¿™ä¸ªã€‚è®©æˆ‘ä»¬åˆ›å»ºè¿™ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç§°ä¹‹ä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚ä½¿ç”¨selfã€‚ç„¶åå®ƒè·å–ã€‚ç±»ç´¢å¼•ã€‚ç„¶åå®ƒè·å–xã€‚è¿™é‡Œæˆ‘ä»¬åº”ç”¨è¿™ä¸ªå…¬å¼ã€‚
- en: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_15.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_15.png)'
- en: So we need the mean and the varianceã€‚And thenã€‚å—¯ã€‚Apply this functionã€‚ Soï¼Œ first
    of allã€‚ let's get the mean equalsã€‚ and we already have the meanã€‚ so we can say
    selfã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_17.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦å‡å€¼å’Œæ–¹å·®ã€‚ç„¶åã€‚å—¯ã€‚åº”ç”¨è¿™ä¸ªå‡½æ•°ã€‚æ‰€ä»¥ï¼Œé¦–å…ˆã€‚è®©æˆ‘ä»¬å¾—åˆ°å‡å€¼ç­‰äºã€‚æˆ‘ä»¬å·²ç»æœ‰å‡å€¼äº†ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´selfã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_17.png)
- en: Meanã€‚Of this class index and also the variance equal selfã€‚Vre of this class
    indexã€‚And then let's create a de numeratorï¼Œ which is equals to nuumpy dot xã€‚Soã€‚
    the exponential function of minusã€‚And then we have x minus meanã€‚To the power of
    twoã€‚Divided byã€‚And thenï¼Œ two timesã€‚The variancesã€‚And thenï¼Œ we have theã€‚Dnominatorï¼Œ
    sub denominator equalsã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç±»åˆ«ç´¢å¼•çš„å‡å€¼ä»¥åŠæ–¹å·®ç­‰äºselfã€‚Vreçš„è¯¥ç±»åˆ«ç´¢å¼•ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆ†å­ï¼Œç­‰äºnuumpyç‚¹xã€‚æ‰€ä»¥ã€‚è´Ÿçš„æŒ‡æ•°å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬æœ‰xå‡å»å‡å€¼ã€‚çš„å¹³æ–¹ã€‚é™¤ä»¥ã€‚ç„¶åï¼Œä¸¤å€çš„æ–¹å·®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æœ‰ã€‚åˆ†æ¯ï¼Œæ¬¡åˆ†æ¯ç­‰äºã€‚
- en: And here we have N dotã€‚Sã€‚Co are Ts over the square root ofã€‚Two timesã€‚ And we
    have pieã€‚ We can also get this from Namiï¼Œ and Peter pieã€‚Times thevaï¼Œ and then
    we returnã€‚Nummeratorã€‚ divided byã€‚Do you know me Naã€‚So this is our probability
    density functionã€‚ And then we can say our class conditionalã€‚Isã€‚å—¯ã€‚Selfã€‚Dotã€‚Probability
    density function of our indexã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰Nç‚¹ã€‚Sã€‚Coæ˜¯Tsçš„å¹³æ–¹æ ¹ã€‚ä¸¤å€çš„ã€‚ç„¶åæˆ‘ä»¬æœ‰Ï€ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»Namiå’ŒPeter Ï€ä¸­è·å¾—ã€‚ä¹˜ä»¥thevaï¼Œç„¶åæˆ‘ä»¬è¿”å›ã€‚åˆ†å­ã€‚é™¤ä»¥ã€‚ä½ çŸ¥é“æˆ‘Naã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬çš„ç±»æ¡ä»¶æ˜¯ã€‚å—¯ã€‚Selfã€‚ç‚¹ã€‚æˆ‘ä»¬ç´¢å¼•çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚
- en: And xã€‚And thenã€‚å—¯ã€‚We want toã€‚We want to have the logo rimoreã€‚ So we have says
    N dot lockã€‚And thenã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_19.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åxã€‚ç„¶åã€‚å—¯ã€‚æˆ‘ä»¬æƒ³è¦ã€‚æˆ‘ä»¬æƒ³è¦æœ‰logoçš„è¾¹ç¼˜ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´Nç‚¹é”ã€‚ç„¶åã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_19.png)
- en: We sum all of them upã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_21.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_21.png)
- en: Soï¼Œ we can useã€‚Nampy dot some of thisã€‚å—¯ã€‚And then we say our postã€‚Tior equalsã€‚Pyaï¼Œ
    plusã€‚The class conditionsã€‚Our class conditionalã€‚And then we have penned them to
    our posteriorã€‚ So posteriors dot append posteriorã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_23.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ã€‚Nampyç‚¹ä¸€äº›è¿™ä¸ªã€‚å—¯ã€‚ç„¶åæˆ‘ä»¬è¯´æˆ‘ä»¬çš„åéªŒã€‚Tiorç­‰äºã€‚Pyaï¼ŒåŠ ä¸Šã€‚ç±»æ¡ä»¶ã€‚æˆ‘ä»¬çš„ç±»æ¡ä»¶ã€‚ç„¶åæˆ‘ä»¬å°†å®ƒä»¬é™„åŠ åˆ°æˆ‘ä»¬çš„åéªŒã€‚æ‰€ä»¥posteriorsç‚¹é™„åŠ åéªŒã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_23.png)
- en: And now we use or we apply the arc max of this and choose the class with the
    highest probabilityã€‚ So Ny also has a arc max functionã€‚ So this is very easyï¼Œ
    so we can now say return self dotã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_25.png)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä½¿ç”¨æˆ–åº”ç”¨è¿™ä¸ªçš„arc maxï¼Œå¹¶é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚æ‰€ä»¥Nyä¹Ÿæœ‰ä¸€ä¸ªarc maxå‡½æ•°ã€‚æ‰€ä»¥è¿™éå¸¸ç®€å•ï¼Œæ‰€ä»¥æˆ‘ä»¬ç°åœ¨å¯ä»¥è¯´è¿”å›selfç‚¹ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_25.png)
- en: Classesã€‚Offï¼Œ and the index is nowã€‚è¿™ã€‚Index of theã€‚Poeriors with the highestã€‚Probabilityã€‚So
    we can sayã€‚ N dot Arc maxã€‚Of this posterã€‚And nowï¼Œ we are doneã€‚So this is the whole
    implementationã€‚Andã€‚Now we can run itã€‚ So I already have a littleã€‚Test script where
    I use the p Psych could learn library to load a data set and create a data set
    with 1000 samples and 10 features for each sampleã€‚ And we have two classesã€‚ Then
    we split our data into training and testing samples and the labelsã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ã€‚å…³é—­ï¼Œç´¢å¼•ç°åœ¨æ˜¯ã€‚è¿™ã€‚æœ€é«˜æ¦‚ç‡çš„ã€‚åéªŒã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´ã€‚Nç‚¹Arc maxã€‚è¿™å¼ æµ·æŠ¥ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å®Œæˆäº†ã€‚è¿™å°±æ˜¯æ•´ä¸ªå®ç°ã€‚ç„¶åã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿è¡Œå®ƒã€‚æ‰€ä»¥æˆ‘å·²ç»æœ‰ä¸€ä¸ªå°çš„æµ‹è¯•è„šæœ¬ï¼Œæˆ‘ä½¿ç”¨p
    Psychåº“åŠ è½½æ•°æ®é›†ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬å’Œ10ä¸ªç‰¹å¾çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æœ‰ä¸¤ä¸ªç±»åˆ«ã€‚ç„¶åæˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬åŠå…¶æ ‡ç­¾ã€‚
- en: Then we create our naive bias classifierï¼Œ which I import from this fileã€‚That
    I have hereã€‚And then I fit the training data and the training labelsã€‚ and then
    I predict get the predictions for the test samplesã€‚And then I will calculate the
    accuracyã€‚ So let's run this and see if this happenï¼Œ if this is workingã€‚So yetï¼Œ's
    workingã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œæˆ‘ä»è¿™é‡Œçš„æ–‡ä»¶ä¸­å¯¼å…¥ã€‚ç„¶åæˆ‘æ‹Ÿåˆè®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ ‡ç­¾ã€‚ç„¶åæˆ‘ä¸ºæµ‹è¯•æ ·æœ¬è·å–é¢„æµ‹ã€‚æ¥ç€æˆ‘ä¼šè®¡ç®—å‡†ç¡®åº¦ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥ï¼Œè¿™æ˜¯å¦æœ‰æ•ˆã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™æœ‰æ•ˆã€‚
- en: And we have a classification accuracy of 0ã€‚96ã€‚ So pretty goodã€‚So yeahï¼Œ that's
    itã€‚ I hope you enjoyed this tutorial and see you next timeï¼Œ byeã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_27.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„åˆ†ç±»å‡†ç¡®ç‡ä¸º0ã€‚96ã€‚æ‰€ä»¥ç›¸å½“ä¸é”™ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œå°±è¿™äº›ã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œä¸‹æ¬¡è§ï¼Œå†è§ã€‚![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_27.png)
