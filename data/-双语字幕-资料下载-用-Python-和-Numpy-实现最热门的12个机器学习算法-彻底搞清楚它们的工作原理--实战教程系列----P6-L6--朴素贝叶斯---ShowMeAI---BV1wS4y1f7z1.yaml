- en: 【双语字幕+资料下载】用 Python 和 Numpy 实现最热门的12个机器学习算法，彻底搞清楚它们的工作原理！＜实战教程系列＞ - P6：L6- 朴素贝叶斯
    - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】用 Python 和 Numpy 实现最热门的12个机器学习算法，彻底搞清楚它们的工作原理！＜实战教程系列＞ - P6：L6- 朴素贝叶斯
    - ShowMeAI - BV1wS4y1f7z1
- en: Hi， everybody。 Welcome to a new machine learning from scratch tutorial。 Today。
    we are going to implement the naive by classifier using only built and Python
    modules and Ny。😊。So the naive bias classifier is based on the biaseth theorem，
    which says that if we have two events。 A and B， then the probability of event
    A， given that B has already happened。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。欢迎来到一个新的从零开始的机器学习教程。今天，我们将仅使用构建和Python模块及Numpy实现朴素贝叶斯分类器。😊 朴素贝叶斯分类器是基于贝叶斯定理的，该定理表明，如果我们有两个事件A和B，那么事件A的概率，给定B已经发生。
- en: is equal to the probability of B， given that A has happened。 times the probability
    of a divided by the probability of B。And if we apply this to our case。 then our
    formula is the probability of y of our class Y。 given the feature vector X is
    equal to the probability of x given y times P of y divided by P of x。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 等于事件B的概率，给定事件A发生的概率，乘以事件A的概率除以事件B的概率。如果我们将其应用于我们的案例，那么我们的公式是类Y的概率，给定特征向量X，等于给定Y的X的概率乘以P(y)，再除以P(x)。
- en: And where x is our feature of vector， which consists of several features。And
    now it's called naive by years because now we make the assumption that all features
    are mutually independent。 which means， for example， if you want to predict the
    probability that a person is going out for a run。Given the feature that the sun
    is shining and also， given the feature that the person is healthy。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 而x是我们的特征向量，它由多个特征组成。现在之所以称为朴素是因为我们假设所有特征都是相互独立的。这意味着，例如，如果你想预测一个人出去跑步的概率，给定太阳在照耀，以及给定这个人健康的特征。
- en: Then both of these features might be independent， but both contribute to this
    probability that the person goes out。So in real life， a lot of features are not
    mutually independent。 but this assumption works fine for a lot of problems。And
    with this assumption， we can split。This。Probability into the and use the chain
    rule。 So we calculate the probability for each。Feature。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这两个特征可能是独立的，但都对这个人出去的概率有贡献。因此在现实生活中，很多特征并不是相互独立的，但这个假设在很多问题上效果良好。根据这个假设，我们可以将这个概率分割。并使用链式法则。所以我们为每个特征计算概率。
- en: Given why and multiply each。And then we multiply it with p of y and divided
    by p of x。And by the way。 this P of y given x is called the posterior probability。
    P of x given y is called the class conditional probability。 and P of y is called
    the prior probability of y， and P of x is called the prior probability of x。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 给定y，并乘以每一个。然后我们将其乘以P(y)并除以P(x)。顺便提一下，P(y|x)被称为后验概率，P(x|y)被称为类条件概率，P(y)被称为y的先验概率，而P(x)被称为x的先验概率。
- en: And now we want to make a classification。 So given this posterior probability。
    we want to select the class with the highest probability。 So we choose y， which
    is the arc max of y。Of this posterior probability。And now we can apply our formula。
    And since we are only interested in Y， we don't need this P of x so we can cross
    this out。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要进行分类。给定这个后验概率，我们希望选择概率最高的类。所以我们选择y，即y的弧最大值。在这个后验概率中。现在我们可以应用我们的公式。因为我们只对Y感兴趣，所以不需要这个P(x)，所以可以将其划去。
- en: And then our formula is this， so。Why is the arc max of。And then we multiply
    each class conditional probability， and then the prior probability。And then we
    use a little trick。Since all these values are are probabilities between 0 and
    1。 So if we multiply a lot of these values， then we get very small numbers。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们的公式是这样，为什么是弧最大值。然后我们乘以每个类的条件概率，再乘以先验概率。然后我们用一个小技巧。因为所有这些值都是在0到1之间的概率。所以如果我们乘以很多这些值，那么我们会得到非常小的数字。
- en: and we might run into overflow problems。 So in order to prevent this， we apply
    the lock function。 So we apply the lock for each of these probabilities。 and with
    the lock or the rules for logarithmuss， we can change the multiplication sign
    into an a plus sign。 So now we have an addition here。And now we have this formula
    that we need。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会遇到溢出问题。因此，为了防止这个，我们应用锁定功能。我们对每一个概率应用锁定。通过锁定或对数的规则，我们可以将乘法符号转换为加法符号。所以现在我们有了一个加法。现在我们有了我们需要的这个公式。
- en: and now we need to come up with this。AP probability， So the prior probability
    is just the frequency。We can see this in a second。 And then what is this class
    conditional probability， P of x given y。And here we model this with a gausian
    distribution。 So here we can see the formula。 So this is one over。 and then the
    square root of2 pi times the variance of y。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要提出这个。AP 概率，所以先验概率就是频率。我们可以在一瞬间看到这一点。然后这个类别条件概率，即 P（x | y）。在这里，我们用高斯分布来建模。所以我们可以看到这个公式。所以这是
    1 除以，然后是 2π的平方根乘以 y 的方差。
- en: Times the exponential function of minus x minus the mean value squared divided
    by two times the variance。And here we see a plot of the Gaussian function for
    different means and variations variances。So this is a probability that is always
    between0 and one and。Yeah， with this formulas。 this is all we need to get started。
    So now we can。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以指数函数的负 x 减去均值的平方除以 2 乘以方差。在这里，我们看到不同均值和方差的高斯函数的图。这个概率总是在 0 和 1 之间，是的，使用这些公式。这是我们开始所需的一切。所以现在我们可以。
- en: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_1.png)'
- en: Start and implement it。 And first of all， of course， we import Ny S N P。And
    then we create a class called naive。But is。And it doesn't need an in it method
    so we can implement the fit method first。 so we want to fit training data and
    training labels。 And then we also want to implement a predict method。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 开始并实现它。首先，当然，我们导入 Ny S N P。然后我们创建一个名为 naive 的类。但它不需要一个 init 方法，因此我们可以首先实现 fit
    方法。我们希望拟合训练数据和训练标签。然后我们还想实现一个预测方法。
- en: So here we predict the test labels or test samples。And now let's start。 So let's
    start with the fit method。 So what we can do here。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_3.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们预测测试标签或测试样本。现在让我们开始。所以我们从 fit 方法开始。我们可以在这里做什么。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_3.png)
- en: Is so we need the priors， and we can calculate them in this fit method。 and
    we need the class conditional。 So here we need the mean for each class and the
    variance for each class。 So we can also calculate these。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_5.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要先验，我们可以在这个 fit 方法中计算它们。我们需要类别条件。所以在这里我们需要每个类别的均值和方差。因此我们也可以计算这些。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_5.png)
- en: So let's do this。 So let's get the number of samples and the number of features
    first。And by the way。 our input here， the X is a nuy N D array where the first
    dimension is the number of samples。 and the second dimension or the number of
    rows is the number of samples and the number of columns is the number of features。
    so we can unpack this and say this is x dot shape。And our y is a 1 D row vector
    also of size。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们这样做。首先获取样本数量和特征数量。顺便说一下，我们的输入 X 是一个 nuy N D 数组，其中第一维是样本数量，第二维或行数是样本数量，列数是特征数量。所以我们可以解包这个并说这是
    x.shape。我们的 y 是一个 1D 行向量，大小也为。
- en: the number of samples。 So this is our input。 And now let's get the unique classes。
    let's say self classes equals nuy unique of y。 So this will find the unique elements
    of an array。 So if we have two classes，0 and 1。 then this will be an array。Just
    with 1，0 and 1 y in it and1。1 in it。Then let's say the number of classes equals。🤢，The
    length of this self。Classes。And now。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 样本数量。这是我们的输入。现在让我们获取唯一类别。假设 self.classes 等于 nuy.unique(y)。这将找到数组的唯一元素。如果我们有两个类别，0
    和 1。那么这将是一个数组，里面仅包含 1、0 和 1 y，以及 1。假设类别的数量等于。🤢，这个 self.classes 的长度。现在。
- en: let's in it or。In it mean。Variarianance and priors。 So let's say self dot mean
    equals。 And we want to in them with zeros at first。 And it gets the size。 It has
    number of classes and number of features as tuple here。 So it also。For each。Class。It
    has。The same number of。Or for each class。We need。Means for each feature。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化或。初始化均值、方差和先验。假设 self.mean 等于。我们想先用零初始化。它的大小为类别数量和特征数量的元组。所以它也。对于每个。类别。它有。每个特征所需的均值数量。
- en: And we want to get this or give this a data type of float nuy dot float 64。And
    we want to do this with the same for the variances。 So let's say self do。玩儿。Equals
    this。 And then we want to do self dot。Priers equals N dot zeros。 And here for
    each class。 we want one prior。 So this is just a 1 D vector of size number of
    classes with a data type of N dot float 64。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要将这个或给它一个 float 类型，即 nuy.float64。我们想对方差做同样的事情。假设 self.do。玩儿。等于这个。然后我们希望 self.priers
    等于 N.zeros。在这里，对于每个类别，我们希望有一个先验。所以这只是一个大小为类别数量的 1D 向量，数据类型为 N.float64。
- en: And now let's calculate them。 So for each class in self dot classes， we。Now。
    only we only want the samples that has this class as labels。 So let's call this
    X， C equals X。 and then where。C equals equals。Why。And now we can calculate the
    mean for each class and fill our self do mean。 So we want to fill。This。Row and
    all columns here。 And we say， this is x。X， C。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算它们。因此对于self dot classes中的每个类别，我们。现在。我们只想要将这个类别作为标签的样本。因此我们将其称为X，C等于X。然后在哪里。C等于等于。Y。现在我们可以为每个类别计算均值并填充我们的self
    do mean。因此我们想填充。这一行和所有列。我们说，这是x。X，C。
- en: dot and nuly has a or a N D。Aray has to built in mean functions。 So we can say
    mean along the axis0。 So please check the mean。Function for yourself。And we want
    to do the same thing for var。 So self var in this row for each column。Is X C dot
    var。 So Numpy also has a var method。And then we calculate the prior， So self。Friers。Of
    this class。Is equal to。 And now。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: dot和nuly有一个或一个N D。数组必须内置均值函数。因此我们可以沿着轴0计算均值。因此请自己检查均值。函数。我们想对var做同样的事情。因此self
    var在这一行的每一列。是X C dot var。因此Numpy也有一个var方法。然后我们计算先验，因此self。Priers。这个类别。等于。现在。
- en: what information do we already have。If we have the the training samples and
    training labels。 so we can say the prior probability that this class will occur
    is equal to the frequency of this class in the training samples。So， we say。We
    get X， C。The shape。0， so only。This will get the number。Of samples with this label。And
    then we divide it by the number of total samples。 So。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经掌握了哪些信息。如果我们有训练样本和训练标签。我们可以说这个类别发生的先验概率等于该类别在训练样本中的频率。因此，我们说。我们得到X，C。形状为0，因此只有。这将获得带有此标签的样本数。然后我们将其除以总样本数。因此。
- en: and we have to convert this to a float because we don't want integers here。
    So let's say float number of samples。So this is the frequency。 How often this
    class C is occurring。And now this is all for our fit method。 And now let's implement
    the predict method。 So and for this。 we create a little helper method。 So let's
    call this underscore predict self。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将其转换为浮点数，因为我们不希望这里是整数。因此我们说浮点数样本数。所以这是频率。这个类别C出现的频率。现在这就是我们fit方法的全部。现在让我们实现predict方法。因此为了这个。我们创建一个小的辅助方法。我们将其称为_预测self。
- en: and this will only get one samples。 So here we have we can have multiple samples。
    So here we say why predict equals and then we use list comprehension。And called
    this underscore。 predict method。For only one sample。 And then we do this for each
    sample in our test samples。And then， we return them。And now we have to implement
    our underscore predict method。So here。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这只会获取一个样本。因此在这里我们可以有多个样本。因此我们说为什么预测等于，然后我们使用列表推导。调用这个_预测方法。仅用于一个样本。然后我们对测试样本中的每个样本执行此操作。然后，我们返回它们。现在我们必须实现我们的_预测方法。因此在这里。
- en: what we need now is we need。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_7.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要的是我们需要。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_7.png)
- en: To apply this function。So。We have to calculate the posterior probability。And
    calculate the。Class conditional and the prior for each one。 And then choose the
    class with the highest probability。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_9.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 应用此函数。因此。我们必须计算后验概率。并计算。每个类别条件和先验，然后选择具有最高概率的类别。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_9.png)
- en: So let's create an empty list called posteriors。Equals an empty list。And now
    let's go over each class。So let's say for index and C in。Enumerate。Self dot classes。
    So here we get also the index。And the class ladle， with this enumerate function。And
    now we can say。First， we get the pia。 So the pia equals， and we already have calculated
    the p。 So this is。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个名为posteriors的空列表。等于一个空列表。接下来我们遍历每个类别。假设对于索引和C在。枚举。Self dot classes。因此这里我们也得到了索引和类别标签，使用这个枚举函数。现在我们可以说。首先，我们得到pia。那么pia等于，我们已经计算出的p。因此这是。
- en: The prior of self dot。Priers。With this in current index。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_11.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: self dot Priers的先验。在当前索引中。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_11.png)
- en: And now， as I said， then at the end， we apply the lock function。 So let's apply
    this right here。 So let's say NP P dot lock。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_13.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如我所说，最后我们应用锁定函数。因此我们在这里应用它。因此我们说NP P dot lock。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_13.png)
- en: And then create the posterior。So， the posterior equals。on， let's call this。Class。Conditional。Equals。And
    then we apply the gause functions。 So for this。 let's create this helper function
    and call this probability density function。With self。 And then it gets。The class
    index。And then it gets x。And here we apply this formula here。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后创建后验。所以，后验等于。让我们称之为。类。条件。等于。然后我们应用高斯函数。所以对于这个。让我们创建这个辅助函数，称之为概率密度函数。使用self。然后它获取。类索引。然后它获取x。这里我们应用这个公式。
- en: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_15.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_15.png)'
- en: So we need the mean and the variance。And then。嗯。Apply this function。 So， first
    of all。 let's get the mean equals。 and we already have the mean。 so we can say
    self。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_17.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要均值和方差。然后。嗯。应用这个函数。所以，首先。让我们得到均值等于。我们已经有均值了。所以我们可以说self。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_17.png)
- en: Mean。Of this class index and also the variance equal self。Vre of this class
    index。And then let's create a de numerator， which is equals to nuumpy dot x。So。
    the exponential function of minus。And then we have x minus mean。To the power of
    two。Divided by。And then， two times。The variances。And then， we have the。Dnominator，
    sub denominator equals。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该类别索引的均值以及方差等于self。Vre的该类别索引。然后我们创建一个分子，等于nuumpy点x。所以。负的指数函数。然后我们有x减去均值。的平方。除以。然后，两倍的方差。接着，我们有。分母，次分母等于。
- en: And here we have N dot。S。Co are Ts over the square root of。Two times。 And we
    have pie。 We can also get this from Nami， and Peter pie。Times theva， and then
    we return。Nummerator。 divided by。Do you know me Na。So this is our probability
    density function。 And then we can say our class conditional。Is。嗯。Self。Dot。Probability
    density function of our index。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们有N点。S。Co是Ts的平方根。两倍的。然后我们有π。我们也可以从Nami和Peter π中获得。乘以theva，然后我们返回。分子。除以。你知道我Na。所以这是我们的概率密度函数。然后我们可以说我们的类条件是。嗯。Self。点。我们索引的概率密度函数。
- en: And x。And then。嗯。We want to。We want to have the logo rimore。 So we have says
    N dot lock。And then。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_19.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后x。然后。嗯。我们想要。我们想要有logo的边缘。所以我们说N点锁。然后。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_19.png)
- en: We sum all of them up。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_21.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们全部相加。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_21.png)
- en: So， we can use。Nampy dot some of this。嗯。And then we say our post。Tior equals。Pya，
    plus。The class conditions。Our class conditional。And then we have penned them to
    our posterior。 So posteriors dot append posterior。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_23.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以使用。Nampy点一些这个。嗯。然后我们说我们的后验。Tior等于。Pya，加上。类条件。我们的类条件。然后我们将它们附加到我们的后验。所以posteriors点附加后验。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_23.png)
- en: And now we use or we apply the arc max of this and choose the class with the
    highest probability。 So Ny also has a arc max function。 So this is very easy，
    so we can now say return self dot。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_25.png)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用或应用这个的arc max，并选择概率最高的类别。所以Ny也有一个arc max函数。所以这非常简单，所以我们现在可以说返回self点。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_25.png)
- en: Classes。Off， and the index is now。这。Index of the。Poeriors with the highest。Probability。So
    we can say。 N dot Arc max。Of this poster。And now， we are done。So this is the whole
    implementation。And。Now we can run it。 So I already have a little。Test script where
    I use the p Psych could learn library to load a data set and create a data set
    with 1000 samples and 10 features for each sample。 And we have two classes。 Then
    we split our data into training and testing samples and the labels。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 类。关闭，索引现在是。这。最高概率的。后验。所以我们可以说。N点Arc max。这张海报。现在，我们完成了。这就是整个实现。然后。现在我们可以运行它。所以我已经有一个小的测试脚本，我使用p
    Psych库加载数据集，并为每个样本创建一个包含1000个样本和10个特征的数据集。我们有两个类别。然后我们将数据分为训练样本和测试样本及其标签。
- en: Then we create our naive bias classifier， which I import from this file。That
    I have here。And then I fit the training data and the training labels。 and then
    I predict get the predictions for the test samples。And then I will calculate the
    accuracy。 So let's run this and see if this happen， if this is working。So yet，'s
    working。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建我们的朴素贝叶斯分类器，我从这里的文件中导入。然后我拟合训练数据和训练标签。然后我为测试样本获取预测。接着我会计算准确度。所以让我们运行这个，看看是否可以，这是否有效。所以，是的，这有效。
- en: And we have a classification accuracy of 0。96。 So pretty good。So yeah， that's
    it。 I hope you enjoyed this tutorial and see you next time， bye。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_27.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类准确率为0。96。所以相当不错。所以是的，就这些。我希望你喜欢这个教程，下次见，再见。![](img/0c43fb2d3ef1d0cac23b2a5dddc91acf_27.png)
