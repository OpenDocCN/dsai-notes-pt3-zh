- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P18：L3.2- 深度学习工具库Tensorflow和Keras
    简介 - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P18：L3.2- 深度学习工具库TensorFlow和Keras简介
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeff Heaton， Welcome to applications of Deep neural Network with
    Washington University In this video。 we're going to have an introduction to Tensorflowlow
    and Kes。 and we're going to see how Tensorflowlow can be used directly to perform
    mathematical calculations and Alkis becomes the neural network layer on top of
    the low levelvel Tensorflowlow compute engine for the latest on my AI course and
    projects。 click subscribe in the bell next to it to be notified of every new video
    TensorFlow and Kes。😊。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是杰夫·希顿，欢迎来到华盛顿大学深度神经网络应用课程。在这个视频中，我们将介绍TensorFlow和Keras，并看看TensorFlow如何直接用于进行数学计算，而Keras则成为底层TensorFlow计算引擎之上的神经网络层。有关我AI课程和项目的最新信息，请点击订阅，旁边的铃铛可以通知你每个新视频的发布。
- en: '![](img/73f8c430d7aa8a142b3ebee5c5c859cc_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73f8c430d7aa8a142b3ebee5c5c859cc_1.png)'
- en: So TensorFlow is the low level mathematical library that gives you access to
    CPU， GPU and grid。 computing capabilities， and CAs is the high level abstraction
    that lets you think of these mathematical constructs as neural networks。I give
    you some links here that point to the various locations that are useful for dealing
    with Tensor Flow and Kas。One of the first things that you need to deal with is
    there's a lot of different versions of Tensorflow。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，TensorFlow是一个低级数学库，它让你能够访问CPU、GPU和网格计算能力，而Keras是一个高层抽象，它让你把这些数学结构视为神经网络。我这里给你一些链接，指向处理TensorFlow和Keras的各种有用资源。你需要处理的第一件事是，TensorFlow有很多不同的版本。
- en: and Google is very good or bad， as the case may be at breaking changes。 There's
    a lot of very smart people working on Tensorflow who are very opinionated about
    how the API should be。And as different factions of them win and lose their respective
    battles。 breaking changes happen to the API。 A breaking change means that a new
    version will basically cause previous code written for previous versions to not
    work。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在重大变更方面可能非常出色或糟糕，视情况而定。很多聪明的人在TensorFlow上工作，他们对API应该如何设计有着非常明确的看法。随着不同派系的胜负，API会发生重大变化。重大变更意味着新版本会导致以前为旧版本编写的代码无法工作。
- en: They change the names of things。 They change the capitalizations of things。Lots
    of lots of little。 sometimes just little annoying things， sometimes big structural
    changes。So it's important that you're running the version of Ten flow that is
    specified for this class。This video was recorded with TensorFlow 2。0。Which was
    in its dev stages as this video was recorded。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 他们改变事物的名称。他们改变事物的大小写。有很多很多小的， 有时只是小的烦人事， 有时是大的结构性变化。因此，确保你正在使用本课程指定的Ten flow版本非常重要。这个视频是在TensorFlow
    2.0开发阶段录制的。
- en: but as long as you have a version that is 20 or later， I will be updating the
    course。Videos for the different parts， if something gets broken as a result of
    newer tensorflow。 So you can simply run this and it will show you the version
    that you， that you have。Now installing TensorFlow。I have an entire video just
    on how to install Tensorflow on Windows and another video for Mac。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是只要你有20或更高版本，我会更新课程。如果由于更新的TensorFlow而导致某些内容出现问题，我会更新视频。因此，你只需运行这个，它会显示你拥有的版本。关于安装TensorFlow，我有一整部视频专门讲述如何在Windows上安装TensorFlow，还有一部关于Mac的视频。
- en: Those deal with installing Tensorflow with CPU to get really good performance。
    you need to use your GPU and your GPU needs to be the right kind of GPU。 Some
    of the assignments and examples that will run in class will require a G run on
    a CPU。 but it's going to be slow。And installing a GPU is is not the easiest thing
    in the world。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些涉及到使用CPU安装TensorFlow以获得良好性能。你需要使用GPU，并且你的GPU需要是合适的类型。在课堂上运行的一些作业和示例将需要在CPU上运行，
    但这会很慢。而安装GPU并不是世界上最简单的事情。
- en: I'll probably do a video on that。 I mostly personally run。GPU deep learning
    in the cloud。 where I don't have to install all the hardware and drivers and and
    other things。 but it can certainly be done。What I recommend for this class。 especially
    on the assignments and parts of this class， that require a GPU is Google Coab。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能会做一个相关视频。我个人主要在云端运行GPU深度学习，这样我就不需要安装所有硬件和驱动程序等，但这确实是可行的。我建议这门课的作业和部分内容需要GPU的同学使用Google
    Colab。
- en: Google Collabab is cloud based， You don't have to install Python or anything。
    It comes completely ready to go。 I have a video that is linked on the first module
    on how to use Google Coab with this class。 and we probably talked about it in
    the first class meeting。So this is。 this is probably the recommended way to actually
    run all of the examples and to complete your assignments is Google Coab。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Google Collabab是基于云的，你无需安装Python或其他任何东西。它完全准备就绪。我有一个视频在第一个模块中链接，介绍如何在这个课程中使用Google
    Coab，我们可能在第一次课堂会议中讨论过。因此，这可能是运行所有示例和完成作业的推荐方式。
- en: It's a powerful instance。 It is a dual core with。I believe 12 gig of Ram and
    a GPU。 you can't beat it。 So that's that's my recommended route for， for working
    on this class。 And why did I choose Tensorflowlow for this class， Well。 Tensorflowlow
    is supported by Google has excellent support in Google Google Cloud。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个强大的实例。它是一个双核，拥有12GB的内存和一块GPU。你无法超越它。因此，这就是我推荐的学习此课程的途径。至于我为什么为这个课程选择Tensorflowlow，嗯，Tensorflowlow得到谷歌的支持，在Google
    Cloud中有出色的支持。
- en: It has great CPU and GPU support， and it's in Python。 Python is very quickly
    becoming sort of the the high level language for machine learning and AI。 So you're
    well served in learning Python and learning。Learning deep learning through Ks
    and and through Python。 Now， you can use Tensorflow directly。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它有很好的CPU和GPU支持，并且是用Python编写的。Python正在迅速成为机器学习和AI的高级语言。因此，学习Python和通过Ks以及Python学习深度学习是非常有利的。现在，你可以直接使用Tensorflow。
- en: And if you're writing very custom sort of machine learning models， that might
    be the way to go。 But by and large， the way to the way to access this is through
    Ks。 It makes it makes the deep learning quite a bit easier to。To set up and there's
    really not much of a downside unless you truly do need complete control over the
    computation of the underlying neural networks。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在编写非常自定义的机器学习模型，那可能是最佳选择。但总体而言，访问的方式是通过Ks。这使得深度学习变得更加简单。设置起来其实没有太多缺点，除非你确实需要对基础神经网络的计算拥有完全控制。
- en: and then you would need Tensorflow。And there's other deep learning tools。 I
    have a number of them listed here。If you're particularly the end of Java。 deep
    learning for J is definitely one to look at as is H2O。Tensorflow is。At least in
    previous versions， it was very graph oriented so that it would compute later。🤢。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要Tensorflow，还有其他深度学习工具。我这里列出了几个。如果你特别喜欢Java，深度学习J绝对值得关注，H2O也是。Tensorflow在以前的版本中，至少是非常图形化的，所以它会后期计算。🤢
- en: A big thing about Tensorflow 2。0 is eager execution。 Now， they made a lot of
    changes。 So this is very much。 if you haven't worked with Tensorflow before， don't
    worry。 it's completely changed anyway。 So this。This will get you ready for the
    latest version。And this is just showing tensor board， which helps you visualize
    the neural networks that you that you create。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow 2.0的一个重要特点是即时执行。现在，他们进行了很多更改。因此，如果你之前没有使用过Tensorflow，不用担心，它已经完全改变了。所以这将让你为最新版本做好准备。这只是展示了tensor
    board，它可以帮助你可视化你创建的神经网络。
- en: We're going to look at how to use Tensorflow directly。 and we're only going
    to go through that in in this in this video。 the rest of the videos will be completely
    in Kira。 So what this is going to do is do something called a mandlebro plot。
    you can see it here。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将直接查看如何使用Tensorflow。我们只会在这个视频中进行这个部分，其他视频将完全使用Kira。所以这将做一个叫做Mandlebro图的东西，你可以在这里看到它。
- en: If you've ever worked with Mandlebrots before you definitely recognize that
    that very classic looking plot that you have there。 Essential， this is a set of
    equations， very simple equations that give a。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_3.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前曾与Mandlebrots合作，你一定会认出那幅经典的图。实际上，这是一组非常简单的方程，生成了。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_3.png)
- en: You can keep zooming in on regions of this， and it's almost infinite in the
    complexity of the very cool sort of landscapes that you could zoom into。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_5.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续放大这个区域，几乎在复杂的景观中是无限的，你可以深入探索。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_5.png)
- en: Here I've written the code to basically do a mandlebro plot。 Now you can sees
    it's very simple。 that's the code to produce the manelbro。 And the the functions。
    the display fractal that's actually used after it's completely rendered what we're
    doing actually in Tensorflow is we're defining。Essentially。It's like it's a complex
    number， it's a complex plane and you plot out the Mandlebrot across the entire
    plane and this。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我写了代码来基本上绘制曼德尔布罗特图。现在你可以看到这非常简单。那是生成曼德尔布罗特的代码。显示分形的函数在完全渲染后使用，我们在 Tensorflow
    中做的实际上是定义。基本上就像是一个复数，它是一个复平面，你在整个平面上绘制曼德尔布罗特。
- en: I'm not going to go into the actual math in the Mandlebrot， but there's plenty
    of tutorials on that。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_7.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入曼德尔布罗特的实际数学，但有很多关于这一点的教程。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_7.png)
- en: And we're essentially sweeping across this entire， this entire plane。 We're
    doing 200 iterations of it。 So this gets drawn 200 times each time it gets drawn
    a little bit better。 And it's very much an iterative iterative process。 So each
    time through this。 It's being sent off to tensorflow for processing。 It could
    use the G P or if you wanted to。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上是在这个平面上进行全面的扫描。我们进行了200次迭代。所以每次绘制时，它都会被画得越来越好。这是一个非常迭代的过程。所以每次通过它时，都会发送到
    Tensorflow 进行处理。如果你愿意，可以使用 GPU。
- en: It's so fast that you really， you really don't need the。You don't need the actual
    GP for this。 The CPU alone can do it。 And this is essentially the equation here
    that is that has been calculated to actually plot the the mandlebrot as it goes
    through。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_9.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 速度快得令人难以置信，你真的不需要实际的 GPU。仅用 CPU 就可以做到。这是计算出来的方程，用于绘制曼德尔布罗特图像。![](img/73f8c430d7aa8a142b3ebee5c5c859cc_9.png)
- en: And if you run this， you can see that it actually goes。Very quickly。 Now to
    see a little more exactly what's going on， this is also using Tensorflow directly。
    I'm creating two matrices here。So these are actually vectors。A yeah。 I guess youd
    call this a row matrix， this is a column matrix。Because this goes straight across。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个，你会看到它实际上运行得非常快。现在为了更精确地观察发生了什么，这里也是直接使用 Tensorflow。我在这里创建了两个矩阵。所以这些实际上是向量。嗯，我想你可以称它为行矩阵，这个是列矩阵。因为这个是横着的。
- en: this is two values that are on top of each other， and we're going to do a matrix
    multiply of matrix 1 by matrix 2。 and we print it out。And as you can see， the
    results here。It does the matrix multiply and results in 12。Now I convert it to
    a float so that you don't see all the tensor。Decorations around it。These were
    two constants， so that was constant times constant。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个值叠在一起，我们要对矩阵1和矩阵2进行矩阵乘法，然后打印出来。正如你所看到的，结果是12。现在我把它转换为浮点数，以便你不会看到所有的张量装饰。这是两个常量，所以是常量乘以常量。
- en: And you can tell this is all very linear algebra oriented。 Now we're doing two
    variables and we're going to subtract well we're subtracting a variable from a
    constant。And we do that， and we can basically see that we get the negative 2，
    negative 1 row。Matrix。We can now reassign X because x is a variable so we reassign
    it and we're able to basically recalculate that calculation from up there and
    get a different value。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看出这一切都是非常线性代数导向的。现在我们在处理两个变量，我们要从一个常量中减去一个变量。我们这样做后，可以看到结果是负2，负1的行矩阵。我们现在可以重新赋值
    X，因为 x 是一个变量，所以我们重新赋值，并能够基本上重新计算上面的计算，得到不同的值。
- en: so this this is the building blocks upon which the neural networks are built
    in TensorF Now Kis lets us start to think about the layers of the neural networks
    like we had before。So we're going to use a classic data set， the miles per gallon
    data set。This lets us。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这些是构建神经网络的基础块。现在让我们开始思考神经网络的层，就像之前那样。我们将使用一个经典的数据集，即每加仑多少英里数据集。这让我们能够。
- en: That data set gives us statistics or calculations on various cars and the miles
    per gallon。 so the idea is to build a model that uses cylinder count and displacement
    and other stats on cars to actually come up with the miles per gallon to produce
    a model that does that。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集为我们提供了各种汽车及其每加仑多少英里的统计数据或计算。所以想法是建立一个模型，利用气缸数、排量和其他汽车统计数据来实际得出每加仑多少英里，从而生成一个这样的模型。
- en: So here we are going to basically read in。The auto miles per gallon。We're going
    to keep the names of the cars， the cars themselves。 the names of the cars aren't
    predictive。 they're just the car names。Horse power。 we do have some missing value。
    so we're going to fill in the media。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将基本上读取每加仑的自动里程。我们将保留汽车的名称和汽车本身。汽车名称并没有预测能力，它们只是汽车名称。马力方面，我们确实有一些缺失值，因此我们将用中位数填充。
- en: and just we' we've seen this before， so I'm going through this pretty， pretty
    quickly。And then we create our x and R Y x is the predictors。 so we're predicting
    on cylinders displayplacement in horsepower， weight， acceleration。 year and origin，
    what country is produced in。And we're calculating this on。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前见过这个，所以我会很快地通过。然后我们创建我们的 x 和 y，x 是预测变量，因此我们在气缸、排量、马力、重量、加速度、年份和产地（即生产国家）上进行预测。我们正在计算这个。
- en: Or we're trying to predict the miles per gallon， so this is your x。 which is
    your values used to predict and y is what you're actually predicting。And then
    this is what。Kas actually looks like we create a sequentialial， so that's just
    layers that go in sequence。We create our first dense layer， a dense layer， simply
    a layer where every neuron in the first layer is connected to the next layer。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图预测每加仑的里程，这就是你的 x，用于预测的值，y 是你实际预测的内容。然后这就是 Keras 的实际样子，我们创建一个序列模型，这只是按顺序排列的层。我们创建我们的第一个密集层，密集层就是第一层的每个神经元都连接到下一层。
- en: Ands that's very common。 That's usually how layers actually work。 especially
    for tabular data like we have now。 This will be different when we get into images。The
    input dimensions are going to be the x shape 1。 So that is the number of columns
    that we have。 because X is the input value。 Sha 0 would be how many rows。 Shape
    1 is how many columns。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常常见的。通常这就是层的工作方式，尤其对于像现在这样的表格数据。当我们进入图像时，这将有所不同。输入维度将是 x 的形状 1。因此这是我们拥有的列数，因为
    x 是输入值。形状 0 是行数，形状 1 是列数。
- en: Activ is the rectified linear unit， and we have 25 neurons。 We have 10 in the
    next hidden layer。 We don't have to tell it how many inputs we have as its。 it's
    basically the 25 from the previous layer。 and also， activation is re。Output is
    just one for a regression neural network， the output is always one。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Activ 是修正线性单元，我们有 25 个神经元。我们在下一个隐藏层有 10 个。我们不必告诉它我们有多少输入，实际上就是来自前一层的 25 个。激活也是如此。输出对于回归神经网络来说仅为一个，输出总是一个。
- en: It's also important that for a regression neural network that you always have
    the mean squared error as your as your loss optimizer willll usually use atom
    in this class。 there's other ones you can use that we'll see。Soon。And then we
    fit it。 We're going to fit it for 100 epochs。Epoex is just how long the neural
    network trains for。 That may or may not be enough。 We'll later see how we can
    determine how many epochs is actually enough。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归神经网络，重要的是你始终使用均方误差作为损失函数，优化器通常在这个类中使用 Adam，还有其他一些可以使用的，我们很快就会看到。然后我们进行拟合。我们将拟合
    100 个周期，周期只是神经网络训练的时间。这可能足够，也可能不够。稍后我们将看到如何确定多少周期实际上是足够的。
- en: Verbose means to print out data as we go。 And if we run this。 we can see you
    see the loss shrinks as it continues。 So it gets better and better and more effective
    as it goes。 This is what the verbose value up there says to do。 So this lets you。That
    that lets you control how much data is being dumped to the screen on you。 Now。
    it's not doing particularly good here this is。The hundred is probably not enough
    for it。 We'll later see that we get more advanced， and this actually gets quite
    a bit。Better predictions。And we calculate our root mean square error。24， believe
    me。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 详细模式意味着在进行过程中打印出数据。如果我们运行这个，可以看到损失在继续时缩小。因此，随着时间的推移，它变得越来越好，更有效。这就是上面详细值的作用。所以这让你可以控制输出到屏幕的数据量。现在，这里的表现并不好，这
    100 可能不够。稍后我们将看到我们变得更加高级，这实际上会得到更好的预测。我们计算我们的均方根误差，24，信我。
- en: is not particularly good on miles per gallon because that's the range。So you
    can。 in cases like that， this is not that uncommon。 You can retrain it。 It might
    give you a better result。 Yeah， that's that's a much better result。 So it just
    took a retrain in that case。 But we'll see later how we can control automatically
    the number of epochs that's needed。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在每加仑的里程上效果不是特别好，因为这是范围。所以在这种情况下，这并不罕见。你可以重新训练它。可能会给你更好的结果。是的，这确实是一个更好的结果。所以在这种情况下，重新训练就解决了问题。但稍后我们将看到如何自动控制所需的周期数。
- en: And you can see here this should drop。 Yeah， that's much better。 That' that's
    about as good as it gets。 You'll see it sometimes get as low as3。 And then we
    can print out the various cars and therere miles per gallon or predicted miles
    per gallon。And yeah， you can see now there much。Much closer。You can also do classification。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到这应该下降。是的，这好多了。这是尽可能好的表现。你有时会看到它降到3。然后我们可以打印出各种汽车的每加仑里程或预测的每加仑里程。现在你可以看到它们的差距更小了。你也可以进行分类。
- en: we'll use the classic IIS data set for that， that's where you have four measurements
    that give you any of three different types of iris flour。You're using sepple length，
    Sppple width， pedal length and pedal width。This is what a curas classification
    neural network looks like。 It's very similar。 You have the inputs。 That would
    be four inputs。 You have the。Hidden one or more hidden layers。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用经典的IIS数据集，这里有四个测量值，可以给出三种不同类型的鸢尾花。你使用的是花萼长度、花萼宽度、花瓣长度和花瓣宽度。这就是鸢尾花分类神经网络的样子。它非常相似。你有输入，那将是四个输入。还有一个或多个隐藏层。
- en: however， with a classification， especially a multi class classification。 which
    means you have more than two classes。 you'll do softm and categorical cross entropy。If
    this was a binary classification， which we'll see。In the next module。That would。
    that would be a little different。 You would have a binary。ALo function here。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于分类，尤其是多类分类，这意味着你有两个以上的类别。你将使用softmax和类别交叉熵。如果这是一个二元分类，我们将在下一个模块中看到。那会有些不同。你会有一个二元ALo函数在这里。
- en: But we'll see that when we get to that。Training this one， it trains and decreases
    the loss。You can also print out the species， Those are the types of iris that
    we're trying to classify。 And this is what the output from it looks like。 It's
    in scientific notation。 which makes that a little bit difficult to read。 You can
    tell it not to use。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将在训练时看到这一点。训练这个模型时，它会训练并减少损失。你还可以打印出物种，这些是我们试图分类的鸢尾花类型。它的输出是这样的，采用科学记数法，这使得阅读有些困难。你可以告诉它不要使用。
- en: You can suppress the scientific notation， but essentially。These three values
    are the three irisees in whichever ones the biggest， which is this one。 It was
    classified to be that first iris， which was Satosa。 You can also suppress the
    scientific notation， And then you see the values that are one。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以抑制科学记数法，但本质上，这三个值是三个鸢尾花中最大的那个，这个被分类为第一种鸢尾花，即山鸢尾。你也可以抑制科学记数法，然后你会看到那些值都是1。
- en: or at least very close to one。That means it's all that first class。 Now the
    Iis file is sorted by by the individual irises， so that's why at the top of the
    file。 they're all the same and you can also turn these predictions into just numeric
    value so0 means the first Iis1 is the second and two is is the third。 So that's
    if you just want literally a number that tells you which of the classes was actually
    predicted。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或者至少非常接近1。这意味着它全部是第一类。现在，IIS文件按单个鸢尾花排序，这就是为什么文件顶部的它们都是相同的。你也可以将这些预测转换为数字值，0意味着第一类，1是第二类，2是第三类。这就是如果你只是想要一个字面上的数字，告诉你哪个类别实际上被预测。
- en: You can also print out using this array notation， you can print out the actual
    textual name of the irisees the classes that were chosen。 and you can see the
    accuracy is 98%， so it's it's very。Very good。You can also feed in just ad hoc
    values and have a prediction。 so this is how you'd feed in one value and it predicts
    Iis Vi Jenica for these measurements。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用这个数组表示法打印出鸢尾花类别的实际文本名称。你可以看到准确率为98%，所以这是非常的、非常好的。你还可以输入临时值并进行预测。这就是你如何输入一个值并预测这些测量值的鸢尾花种类。
- en: '![](img/73f8c430d7aa8a142b3ebee5c5c859cc_11.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73f8c430d7aa8a142b3ebee5c5c859cc_11.png)'
- en: And you can feed a bunch of them at once。 This is how you'd feed too。Thank you
    for watching this video。 In the next video。 we're going to see how to load and
    save Kira's neural networks。 This content changes often。 so subscribe to the channel
    to stay up to date on this course and other topics in artificial intelligence。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以一次输入一组数据。这就是你如何输入两个。感谢观看这个视频。在下一个视频中，我们将看到如何加载和保存鸢尾花的神经网络。这个内容经常变化，所以请订阅频道，及时了解本课程及其他人工智能主题。
- en: 😊。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。
