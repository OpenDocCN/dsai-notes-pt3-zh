- en: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P8：L8- 使用 PyTorch
    进行生产推理部署 - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】140分钟入门PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P8：L8- 使用PyTorch进行生产推理部署
    - ShowMeAI - BV19L4y1t7tu
- en: Welcome to the next video in the Pytorrch training series。 This video will talk
    about deploying your piy torch model for inference and production。 In particular，
    this video will talk about。![](img/4d44108b0d360e3d929d7a4ff88b5654_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到PyTorch培训系列的下一个视频。本视频将讨论将你的PyTorch模型用于推理和生产。特别是，本视频将讨论。![](img/4d44108b0d360e3d929d7a4ff88b5654_1.png)
- en: Putting your pi torch model in evaluation mode。Converting your model to torch
    script and performing inference。Using torch script with C plus plus。And deploying
    your model with torchserv。 which is Py Torch's model serving solution。No matter
    which deployment method you use。 the first thing you should always do is put your
    model into evaluation mode。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的PyTorch模型置于评估模式。将模型转换为torch脚本并执行推理。使用C++的torch脚本。以及使用torchserv部署模型，这是PyTorch的模型服务解决方案。无论你使用哪种部署方法，第一件事始终是将模型置于评估模式。
- en: Evaluation mode is the opposite of training mode。 It turns off training related
    behaviors that you don't want during inference time。In particular， it turns off
    autograd， you may recall from the earlier video on autograrad that piytorrch tensors。
    including your model's learning weights， track their computation history to aid
    the rapid computation of backward gradients for learning。This can be expensive
    in terms of both memory and compute and is not something you want to inference
    time。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模式与训练模式相反。它关闭了在推理时不希望的与训练相关的行为。特别是，它关闭了autograd，你可能还记得之前关于autograd的视频中提到的，PyTorch张量（包括模型的学习权重）会跟踪其计算历史，以帮助快速计算反向梯度。这在内存和计算方面可能是昂贵的，并且在推理时并不是你希望的。
- en: EVval mode also changes the behavior of certain modules that contain training
    specific functionality。In particular， dropout layers are only active during training
    time。 setting your model in Eval mode makes drop out aOA。Batch norm layers track
    running stats on their computed mean invaris during training。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模式还会改变某些包含训练特定功能模块的行为。特别是，dropout层仅在训练期间处于活动状态。将模型设置为评估模式使dropout无效。批归一化层在训练期间跟踪其计算的均值的运行统计。
- en: But this behavior is turned off for eval mode。Next。 let's look at the procedure
    for putting your model in evaluation mode。 First。 you'll want to load your model
    for a Python based model that will involve loading the model's state dictionary
    from disk and initializing your object with it。Then you call the eval method on
    your model。And you're done。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在评估模式下，此行为是关闭的。接下来，让我们看一下将模型置于评估模式的过程。首先，你需要加载一个基于Python的模型，这将涉及从磁盘加载模型的状态字典并用它初始化对象。然后，你在模型上调用eval方法。这样就完成了。
- en: Your model has now turned off training related behaviors for inference。It's
    worth noting that the Eval method is actually just an alias for calling the train
    method with an argument of false。You may find this useful if your code already
    contains a flag that indicates whether you are doing training or inference。Once
    you're in Eval mode， you can start sending your model batches of data for inference。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型现在已关闭与推理相关的训练行为。值得注意的是，eval方法实际上只是调用train方法并传递false参数的别名。如果你的代码已经包含一个指示你是在训练还是推理的标志，这可能会很有用。一旦进入评估模式，你可以开始向模型发送数据批次进行推理。
- en: In the rest of this video， we're going to talk about different methods for deploying
    your model for inference。 but for all of them， making sure your model is in evaluation
    mode is your first step。So what is torch script， it's a statically typed subset
    of Python for representing Py torch models。And it's meant to be consumed by the
    jet， the Pytorrch just in time compiler。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本视频的其余部分，我们将讨论不同的方法来部署模型进行推理。但对于所有这些，确保模型处于评估模式是第一步。那么，什么是torch脚本呢？它是一个用于表示PyTorch模型的静态类型Python子集。它旨在被Jet，即PyTorch即时编译器使用。
- en: which performs runtime optimizations to improve the performance of your model。It
    also allows you to save your model and weights in a single file and load them
    as a script module object that you can call just as you would your original model。![](img/4d44108b0d360e3d929d7a4ff88b5654_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 它会执行运行时优化，以提高模型的性能。它还允许你将模型和权重保存在一个文件中，并将其作为脚本模块对象加载，这样你可以像调用原始模型一样调用它。![](img/4d44108b0d360e3d929d7a4ff88b5654_3.png)
- en: So how do you use torch script。Build， test and train your model in Python， as
    you normally would。When you want to export your model for production inference，
    you can use the torch。jet。trace or torch。jet。script calls to convert your model
    to torch script。After that。 you can call the dot save method on your torchscript
    bundle。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何使用 torch script 呢？像往常一样在 Python 中构建、测试和训练你的模型。当你想将模型导出用于生产推理时，可以使用 torch.jet.trace
    或 torch.jet.script 调用将模型转换为 torch script。之后，你可以在 torchscript 包上调用 dot save 方法。
- en: To save it to a single file that contains both the computation graph and the
    learning weights for your model。The just in time compiler executes your torch
    script model。 performing runtime optimizations such as operator fusion and batching
    matrix multiplications。You can also write your own custom extensions to Torch
    script and C++。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将其保存到一个包含计算图和模型学习权重的单一文件中。即时编译器执行你的 torch script 模型，执行运行时优化，例如操作符融合和批处理矩阵乘法。你还可以为
    Torch script 和 C++ 编写自己的自定义扩展。
- en: The code on the right shows what torch script looks like。 But in the general
    case。 you won't have to edit it yourself。 It's generated from your Python code。
    Let's walk through the process of using torch script in more detail。![](img/4d44108b0d360e3d929d7a4ff88b5654_5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的代码展示了 torch script 的样子。但在一般情况下，你不需要自己编辑它。它是从你的 Python 代码生成的。让我们更详细地了解使用 torch
    script 的过程。![](img/4d44108b0d360e3d929d7a4ff88b5654_5.png)
- en: The process starts with the model you built in Python and train to the point
    of readiness for deployment。The next step is to convert your model to torchscript。
    There are two ways to do this。 Torch dotjet dot script and torch dotjet dot trace。
    It's important to note the differences between the two techniques for converting
    your model to torchscript。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从你在 Python 中构建并训练到准备部署的模型开始。下一步是将你的模型转换为 torchscript。有两种方法可以做到这一点：torch.jet.script
    和 torch.jet.trace。重要的是要注意这两种转换模型为 torchscript 技术之间的区别。
- en: Torch。jet doesscript converts your model by directly inspecting your code and
    running it through the torchscript compiler。It preserves control flow， which you'll
    need if your forward function has conditionals or loops。 and it accommodates common
    Python data structures。However。 due to limitations of Python operator support
    in the Torchscript compiler。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Torch.jet.script 通过直接检查你的代码并将其通过 torchscript 编译器运行来转换模型。它保留了控制流，如果你的前向函数有条件或循环，你将需要它，并且它适应常见的
    Python 数据结构。然而，由于 Torchscript 编译器中 Python 操作符支持的限制。
- en: some models won't be convertible using Torch。jetscript。Torch dot Je dot trace
    takes a sample input and traces it through your computation graph to generate
    the torchscript version of your model。This doesn't suffer the operator coverage
    limitations of torchdt script。 but because it only traces a single path through
    your code。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型无法使用 Torch.jetscript 转换。Torch.jet.trace 通过计算图对样本输入进行追踪，以生成模型的 torchscript
    版本。这不受 torchdt script 的操作符覆盖限制，但因为它只追踪代码中的单一路径。
- en: it won't respect conditionals or other control flow structures that might cause
    variable or non deterministic runtime behavior。It's also possible to mix tracing
    and scripting when converting a model。 See the documentation for the Torchdot
    Je module for notes on mixing the two techniques。It's worth looking at the docks
    to see the optional arguments for a script and trace。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它不会尊重可能导致变量或非确定性运行时行为的条件或其他控制流结构。在转换模型时，也可以混合追踪和脚本。有关混合这两种技术的说明，请查看 Torchdot
    Je 模块的文档。查看文档以了解脚本和追踪的可选参数是值得的。
- en: There are extra options for checking the consistency and tolerances of your
    torchscript model。Now。 we'll save our tor script model。 This saves both your computation
    graph and your learning weights in a single file。 which means you don't have to
    ship the Python file with your model's class definition when you want to deploy
    to production。When it's time to do inference， you call torch dotje dot load on
    your model and feed it batches of input in the same way you would the Python version
    of your model。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有额外的选项可用于检查你的 torchscript 模型的一致性和容差。现在，我们将保存我们的 tor script 模型。这将计算图和学习权重保存在一个单一文件中，这意味着在生产部署时你不需要随模型的类定义一起交付
    Python 文件。当进行推理时，你可以在模型上调用 torch.jet.load，并以与模型的 Python 版本相同的方式馈送输入批次。
- en: Everything I've shown you up to now has involved manipulating your model in
    Python code。 even after you've converted it to Torch script。There are situations
    and environments， though。 where you may need high throughput or real time inference
    and would like to do without the overhead of the Python interpreter。 It may also
    be the case that your production environment is already centered around C plus
    plus code。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我所展示的一切都涉及在Python代码中操作你的模型，即使你已经将其转换为Torch script。然而，在某些情况下和环境中，你可能需要高吞吐量或实时推理，并希望尽量避免Python解释器的开销。你的生产环境也可能已经围绕C++代码建立。
- en: and you'd like to continue using C plus plus as much as possible。You may recall
    from an earlier video in this series that the important tensor computations in
    P torch happen in the Libtorrch。 a compiled and optimized C plus plus library。Pytorrch
    also has a C plus plus front end to this library。 This means that you can load
    your torchscript model in C plus+ and run it with no Python runtime dependencies。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望尽可能继续使用C++。你可能还记得在本系列的早期视频中，P torch中的重要张量计算发生在Libtorrch，这是一个编译和优化的C++库。Pytorrch还为这个库提供了C++前端。这意味着你可以在C++中加载你的torchscript模型，并在没有Python运行时依赖的情况下运行它。
- en: The first thing you'll need to do is to go to pytorrch dot org and download
    the latest version of Libtorrch。 Unzip the package and place it where your make
    system can find it。 This slide shows a minimal C make file for a project using
    Libtorrch。 Know that you'll need to be using C plus plus 14 or higher to make
    use of Libtorrch。In Python。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的第一件事是访问pytorrch.org并下载最新版本的Libtorrch。解压缩包并将其放置在你的构建系统可以找到的位置。此幻灯片展示了一个使用Libtorrch的项目的最小CMake文件。请注意，你需要使用C++
    14或更高版本才能使用Libtorrch。在Python中。
- en: you'd import torch， use torch dotjet do load to bring your torchscript model
    into memory and then call your model with an input batch。 The process is not so
    different than C plus plus。First， include torch slash script dot H。 This is your
    one stop include for working with torch script and C plus plus。 Next。 declare
    a torchcht script module variable， then use torch Jet load to load it into memory。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要导入torch，使用torch.load将你的torchscript模型加载到内存中，然后用输入批次调用你的模型。这个过程与C++并没有太大区别。首先，包含torch/script.h。这是你在使用torch
    script和C++时的单一包含。接下来，声明一个torch script模块变量，然后使用torch.load将其加载到内存中。
- en: To get predictions from your model， call its forward method with an appropriate
    input。Here we have created a dummy input with torch ones。 You'd be bringing in
    your own inputs of whatever size your model requires。Once you have your output
    predictions as a tensor， you can manipulate them with the C++ equivalentence of
    the tensor methods you're used to in Pytorrch's Python frontant。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要从你的模型获取预测，调用其forward方法并传入适当的输入。这里我们创建了一个使用torch.ones的虚拟输入。你将根据模型要求的大小引入自己的输入。一旦你获得输出预测作为张量，就可以使用你在Pytorrch的Python前端中习惯的张量方法的C++等效方法来操作它们。
- en: The Piytor。org tutorial section includes content walking you through setting
    up a C++ project。 as well as multiple tutorials demonstrating aspects of the C++
    front end。![](img/4d44108b0d360e3d929d7a4ff88b5654_7.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Piytor.org教程部分包括引导你设置C++项目的内容，以及多个展示C++前端各个方面的教程。![](img/4d44108b0d360e3d929d7a4ff88b5654_7.png)
- en: Setting up a production model serving environment can be complex。 especially
    if you're serving multiple models， working with multiple versions of models require
    scalability or want detailed logging or metrics。Torch Ser is the Pytorrch model
    serving solution that covers all these needs and warmth。Torchserv loads instances
    of your model or models and individual process spaces and distributes incoming
    requests to them。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 设置生产模型服务环境可能很复杂，特别是当你同时服务多个模型、处理多个版本时，要求可扩展性或需要详细的日志记录或指标。Torch Ser是满足所有这些需求的Pytorrch模型服务解决方案。Torchserv加载你的模型实例或模型的独立进程空间，并将传入请求分发给它们。
- en: and has a number of features to make it useful for creating M based web services。It
    has data handlers covering common use cases， including image classification and
    segmentation。 object detection， and text classification。It allows you to set version
    identifiers for models。 and you can manage and simultaneously serve multiple versions
    of a model。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它有许多功能，使其适用于创建基于M的网络服务。它有数据处理程序，涵盖常见用例，包括图像分类和分割、目标检测和文本分类。它允许你为模型设置版本标识符，你可以管理并同时服务多个版本的模型。
- en: It can optionally batch input requests from multiple sources， which can sometimes
    improve throughput。It features robust logging and the ability to log your own
    metrics。And it has separate restful APIs for inference and model management， which
    may be secured with HtTPS。I'll wrap up this video by walking through setting up
    and running Torchserv with one of the examples available at Github。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以选择性地批量处理来自多个来源的输入请求，这有时可以提高吞吐量。它具有强大的日志记录功能以及记录自定义指标的能力，并且它有单独的 RESTful API
    用于推断和模型管理，这可能会通过 HtTPS 进行安全保护。我将通过演示如何设置和运行 Torchserv 来结束这个视频，使用 GitHub 上可用的一个示例。
- en: com/ptorrch/ serve in the examples folder。We'll set up a pretrained image classification
    model for inference。![](img/4d44108b0d360e3d929d7a4ff88b5654_9.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: com/ptorrch/ 在示例文件夹中提供。我们将设置一个预训练的图像分类模型进行推断。![](img/4d44108b0d360e3d929d7a4ff88b5654_9.png)
- en: First， let's install Torchserv。 I'll demonstrate the process for setting it
    up on a Linux or Mac system。 but torch Serf also works on Windows if that's your
    preferred server environment。First。 I'm going to create a new condoda environment
    for torchur。I'm going to clone the source repository because it has convenient
    scripts for correctly installing torchserv dependencies。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装 Torchserv。我将演示如何在 Linux 或 Mac 系统上进行设置，但如果你偏好在 Windows 上，也可以使用 Torchserv。首先，我将为
    Torchserv 创建一个新的 Conda 环境。我会克隆源代码库，因为它提供了方便的脚本来正确安装 Torchserv 依赖项。
- en: When you run the dependency install script on a machine within N Vidia GPus。
    you may need to specify what version of the couda drivers you have installed。Details
    are in the install procedure described and Torch serves Read Me on GitHub。Since
    I'm installing on a Mac， I can skip that。Now， with the dependencies installed。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在配备 NVIDIA GPU 的机器上运行依赖项安装脚本时，可能需要指定你安装的 CUDA 驱动版本。详细信息在 GitHub 上的安装程序和 Torchserv
    的 Read Me 中描述。由于我在 Mac 上安装，因此可以跳过这一步。现在，依赖项已安装。
- en: I can either install from source or use Piperconda。 I'm actually installing
    two programs。 Torch Ser and the torch model Archivr， which we'll get to in a minute。If
    you' are installing with Conda， don't forget to specify the Ptorrch channel with
    D C Ptorrch。The your torchserv environment needs is a model store directory。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以选择从源代码安装或使用 Pipercnda。我实际上正在安装两个程序：Torchserv 和模型归档工具，这部分我们稍后再讲。如果你使用 Conda
    安装，请不要忘记指定 Ptorrch 频道。你的 Torchserv 环境需要一个模型存储目录。
- en: All your models served by torchserv are stored in this folder。 You can name
    it anything you like。 but I'm going to keep that simple。Next， we'll need a model
    to serve。Tsserv expects models to be packaged in a model archive。 which contains
    your model's code and weights， along with any other files needed to support your
    model。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Torchserv 提供的所有模型都存储在这个文件夹中。你可以随意命名，但我将保持简单。接下来，我们需要一个模型来提供。Torchserv 期望模型被打包在一个模型归档中，其中包含你的模型代码和权重，以及支持模型所需的其他文件。
- en: For example， in a natural language application， you might have embeddings or
    vocabularies that you need to package with your model。A model archive is created
    by the model Archivr， which was the package I installed alongside torch Ser above。First，
    we'll need to download some train model weights。Next。 let's create a model archive
    from these weights。Taking these arguments， one at a time。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在自然语言应用程序中，你可能需要与模型一起打包嵌入或词汇。模型归档由模型归档工具创建，这就是我与 Torchserv 一起安装的包。首先，我们需要下载一些训练模型的权重。接下来，让我们从这些权重创建一个模型归档。逐一处理这些参数。
- en: Every model has a name here， Dennet 161。A model needs a version number here
    we just went with 1。0。We're going to be using a Python based model， so we use
    the model file flag to bring in the Python file containing a model class。The serialized
    file argument specifies the file containing the model weights。If we were loading
    a torch script model， we'd skip the model file argument and just specify the serialized
    torch script file here。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型在这里都有一个名称，Dennet 161。模型需要一个版本号，我们这里选择 1。0。我们将使用基于 Python 的模型，因此我们使用模型文件标志引入包含模型类的
    Python 文件。序列化文件参数指定包含模型权重的文件。如果我们加载的是一个 Torch 脚本模型，我们将跳过模型文件参数，仅在这里指定序列化的 Torch
    脚本文件。
- en: We're going to bring in an extra support file， a JSO file containing mappings
    of the model's trained category Is to human readable labels。Finally， every model
    archive needs a handler to transform and prepare incoming data for inference。
    I'm going to use the built in image classifier handler。 but it's also possible
    to write your own handler and specify that file here。Now。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将引入一个额外的支持文件，一个包含模型训练类别与人类可读标签映射的JSO文件。最后，每个模型归档需要一个处理程序来转换和准备传入数据以进行推理。我将使用内置的图像分类器处理程序，但也可以编写自己的处理程序并在此处指定该文件。现在。
- en: you can see we have a dotton mar file。 This is our model archive。 It belongs
    in the model store。 So let's put it there。Now， let's start torchservf。 We'll do
    so with four arguments。The start flag should be self explanatory。By default。 Torcherf
    stores its current configuration and loads its last config on startup and the
    NCS flag suppresses this behavior。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们有一个dotton mar文件。这是我们的模型归档。它应该放在模型存储中。所以让我们把它放到那里。现在，让我们启动torchservf。我们将用四个参数来做这件事。启动标志应该是显而易见的。默认情况下，Torcherf会存储其当前配置，并在启动时加载其最后的配置，而NCS标志抑制这种行为。
- en: The Model Store flag lets us specify our model store folder。And optionally。
    we can tell torch Ser to start with a model loaded。 We'll specify our new model
    archive for Densnet 161。Torter puts out a lot of helpful information。 all of which
    is also saved in log files， Let's have a look at the logs folder now。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Model Store标志让我们指定模型存储文件夹。并且可以选择性地告诉torch服务以加载的模型启动。我们将指定新的Densnet 161模型归档。Torter提供了很多有用的信息，这些信息也保存于日志文件中。现在让我们看看日志文件夹。
- en: Note that a log directory has been created alongside our model store。And here
    you can see we have logs for all torchserv， behavior and metrics。Now。 the torch
    serve is running， let's do some inference。We'll grab a sample image from the source
    rep over our input， and then we'll call curl。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，日志目录已在我们的模型存储旁边创建。这里你可以看到我们有所有torchserv的行为和指标日志。现在，torch服务正在运行，让我们进行一些推理。我们将从源库获取一个示例图像，然后调用curl。
- en: On the torchserv side， the default image classifier model takes care of unpacking
    the image and converting it to a tensor。 feeding it to the model and processing
    the output。 This shows a simple case of using the torchserv inference API over
    H TtP。 but you can also access it via GRPC or use the Kf serving API used by Kubbeflow
    on Kubernetes。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在torchserv一侧，默认的图像分类器模型负责解压图像并将其转换为张量，输入到模型并处理输出。这展示了一个使用torchserv推理API通过HTTP的简单案例，但你也可以通过GRPC访问它，或使用Kubernetes上的Kf服务API。
- en: And here we have the top five classes identified by the model。If we want to
    learn about the status of the server or manage which models we're serving or how
    many worker processes are devoted to each worker。 we can use the management API。Above，
    used a prediction API on his default port of 8080。The default for the management
    API is Port 8081。 Let's use this curl command to see how the server reports what
    models it's serving。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有模型识别的前五个类别。如果我们想了解服务器的状态或管理我们正在提供的模型，或每个工作进程专用的工作进程数量，我们可以使用管理API。上面使用了在其默认端口8080上的预测API。管理API的默认端口是8081。让我们使用这个curl命令来查看服务器报告它正在提供哪些模型。
- en: The model is end point enumerates models being served， which right now is just
    our densenet model。 Let's get a little more detail on it。And here you can see
    it specifies things about this particular model。 including how many workers are
    spun up， et cetera。We can be more specific if we have more than one version of
    the model by adding the version number to the URL。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型端点列举了正在提供的模型，目前仅有我们的densenet模型。让我们获取更多关于它的细节。在这里你可以看到它指定了有关这个特定模型的信息，包括运行的工作进程数量等。如果我们有多个版本的模型，我们可以通过将版本号添加到URL中来更加具体。
- en: This shows the default configuration for a Ser model， with 12 workers running。You
    can also use the management API to alter that configuration。 so let's change the
    number of workers。So I set both the mini max workersers to four。And now。 if I
    ask for。The status of our model again， we should see the number of workers has
    changed。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了一个服务模型的默认配置，运行着12个工作进程。你还可以使用管理API来更改该配置。所以让我们改变工作进程的数量。我把最小和最大工作进程都设置为四个。现在，如果我再次询问我们的模型状态，我们应该看到工作进程的数量已经改变。
- en: The management API lets you register new models from a local model archive or
    from a URL。 It lets you unregister models or set the default version of a model
    to serve or get the status of a model or model that you're serving。![](img/4d44108b0d360e3d929d7a4ff88b5654_11.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 管理 API 允许你从本地模型存档或 URL 注册新模型。它让你可以注销模型或设置服务的模型的默认版本，或者获取你正在服务的模型的状态。![](img/4d44108b0d360e3d929d7a4ff88b5654_11.png)
- en: Finally， we can stop Torser with the stop flag。The Torch Ser GitHub Repo also
    has walkthroughs and examples from many common tasks。 including specific server
    management tasks， setting up HTPS， writing a custom handler and more。![](img/4d44108b0d360e3d929d7a4ff88b5654_13.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过停止标志停止 Torser。Torch Ser GitHub 仓库中也有许多常见任务的操作指南和示例，包括特定的服务器管理任务、设置
    HTPS、编写自定义处理程序等。![](img/4d44108b0d360e3d929d7a4ff88b5654_13.png)
- en: '![](img/4d44108b0d360e3d929d7a4ff88b5654_14.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d44108b0d360e3d929d7a4ff88b5654_14.png)'
- en: And as always， everything I've described here and more is documented fully in
    the documentation and tutorials at Pytorch dot org。![](img/4d44108b0d360e3d929d7a4ff88b5654_16.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我在这里描述的所有内容以及更多信息都在 Pytorch dot org 的文档和教程中有详细记录。![](img/4d44108b0d360e3d929d7a4ff88b5654_16.png)
