- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P11ï¼šL11-
    éšæœºæ£®æ— - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P11ï¼šL11-
    éšæœºæ£®æ— - ShowMeAI - BV1wS4y1f7z1
- en: Hiï¼Œ everybodyã€‚ Welcome to your new machine learning from Sct tutorialã€‚ Todayã€‚
    we are going to implement a random forest using only built in Python modules and
    Nyã€‚ The random forest algorithm is one of the most powerful and most popular algorithmã€‚
    So I'm very excited that we can finally implement it in this tutorial in the last
    tutorialã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°ä½ çš„æ–°æœºå™¨å­¦ä¹ Sctæ•™ç¨‹ã€‚ä»Šå¤©æˆ‘ä»¬å°†ä½¿ç”¨ä»…å†…ç½®çš„Pythonæ¨¡å—å’ŒNyæ¥å®ç°éšæœºæ£®æ—ã€‚éšæœºæ£®æ—ç®—æ³•æ˜¯æœ€å¼ºå¤§ä¸”æœ€å—æ¬¢è¿çš„ç®—æ³•ä¹‹ä¸€ã€‚æ‰€ä»¥æˆ‘å¾ˆå…´å¥‹æˆ‘ä»¬ç»ˆäºèƒ½åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­å®ç°å®ƒï¼Œåœ¨ä¸Šä¸€ä¸ªæ•™ç¨‹ä¸­ã€‚
- en: I explained how a single decision tree worksã€‚ So if you haven't watched the
    previous tutorial yetã€‚ then please do soã€‚ because our random forest model and
    also the implementation is based on the decision tree model from the last timeã€‚ğŸ˜Šï¼ŒSo
    if you have understood the decision treesï¼Œ then the approach of the random forest
    is very easy to understandã€‚If we have a look at this image hereï¼Œ then this shows
    the whole ideaã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§£é‡Šäº†å•æ£µå†³ç­–æ ‘æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰è§‚çœ‹ä¹‹å‰çš„æ•™ç¨‹ï¼Œé‚£ä¹ˆè¯·å»è§‚çœ‹ï¼Œå› ä¸ºæˆ‘ä»¬çš„éšæœºæ£®æ—æ¨¡å‹ä»¥åŠå®ç°æ˜¯åŸºäºä¸Šæ¬¡çš„å†³ç­–æ ‘æ¨¡å‹ã€‚ğŸ˜Šå¦‚æœä½ ç†è§£äº†å†³ç­–æ ‘ï¼Œé‚£ä¹ˆéšæœºæ£®æ—çš„æ–¹å¼å°±éå¸¸å®¹æ˜“ç†è§£ã€‚å¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™é‡Œçš„å›¾åƒï¼Œå®ƒå±•ç¤ºäº†æ•´ä¸ªæƒ³æ³•ã€‚
- en: So the idea is to combine multiple trees into a forestã€‚ so we train multiple
    trees and each tree gets a random subset of the training dataã€‚ thus the word randomã€‚
    We then make a prediction with each of the trees at the endã€‚And we make a maturity
    vote then to get the final predictionã€‚So this is the whole ideaã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªæƒ³æ³•æ˜¯å°†å¤šæ£µæ ‘ç»„åˆæˆä¸€ç‰‡æ£®æ—ã€‚å› æ­¤æˆ‘ä»¬è®­ç»ƒå¤šæ£µæ ‘ï¼Œæ¯æ£µæ ‘è·å–è®­ç»ƒæ•°æ®çš„éšæœºå­é›†ï¼Œå› æ­¤ç§°ä¸ºéšæœºã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨æ¯æ£µæ ‘è¿›è¡Œé¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œæˆç†ŸæŠ•ç¥¨ä»¥è·å¾—æœ€ç»ˆé¢„æµ‹ã€‚è¿™å°±æ˜¯æ•´ä¸ªæ€è·¯ã€‚
- en: And the random forest has some advantages compared to only one treeï¼Œ for exampleã€‚
    by building more treesï¼Œ we have more chances to get the correct prediction and
    we also reduce the chance of overfitting with a single tree so typically the accuracy
    of a random forest is higher than with a single tree and that's why it's so powerfulã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—ç›¸æ¯”å•æ£µæ ‘æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œä¾‹å¦‚ã€‚é€šè¿‡æ„å»ºæ›´å¤šçš„æ ‘ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šæœºä¼šè·å¾—æ­£ç¡®çš„é¢„æµ‹ï¼ŒåŒæ—¶æˆ‘ä»¬ä¹Ÿå‡å°‘äº†å•æ£µæ ‘è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§ï¼Œå› æ­¤éšæœºæ£®æ—çš„å‡†ç¡®æ€§é€šå¸¸é«˜äºå•æ£µæ ‘ï¼Œè¿™å°±æ˜¯å®ƒå¦‚æ­¤å¼ºå¤§çš„åŸå› ã€‚
- en: Soï¼Œ yeahï¼Œ now we can jump right to the implementationã€‚ Soï¼Œ of courseï¼Œ we importã€‚![](img/657eaccf75713176fcbe8f6ff6eea819_1.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ç›´æ¥è·³åˆ°å®ç°éƒ¨åˆ†ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯¼å…¥ã€‚![](img/657eaccf75713176fcbe8f6ff6eea819_1.png)
- en: Nampyï¼Œ Sï¼Œ and Pã€‚And then we also import the decision tree class from the last
    timeã€‚ so we say from decision treeï¼Œ import decision treeã€‚And now we can startã€‚
    We create our class decision3ã€‚And or sorryï¼Œ nowï¼Œ now we have the random forestã€‚
    So now we create our random forest classã€‚And this willã€‚Get an in itã€‚Soã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nampyã€Så’ŒPã€‚ç„¶åæˆ‘ä»¬è¿˜ä»ä¸Šæ¬¡å¯¼å…¥å†³ç­–æ ‘ç±»ã€‚å› æ­¤æˆ‘ä»¬è¯´ä»å†³ç­–æ ‘ä¸­å¯¼å…¥å†³ç­–æ ‘ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ã€‚æˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„ç±»decision3ã€‚æŠ±æ­‰ï¼Œç°åœ¨æˆ‘ä»¬æœ‰äº†éšæœºæ£®æ—ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„éšæœºæ£®æ—ç±»ã€‚è¿™å°†è¿›è¡Œåˆå§‹åŒ–ã€‚
- en: and this will get the number of trees we want to have in our forestã€‚ So let's
    say a number of trees equals 100 by defaultã€‚And then it also gets all the parameters
    from our decision tree initializezaã€‚ So it gets the minimum samples required for
    a splitã€‚ It gets the maximum depthã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è·å–æˆ‘ä»¬æƒ³è¦åœ¨æ£®æ—ä¸­æ‹¥æœ‰çš„æ ‘çš„æ•°é‡ã€‚å› æ­¤ï¼Œé»˜è®¤æƒ…å†µä¸‹æ ‘çš„æ•°é‡ç­‰äº100ã€‚ç„¶åå®ƒè¿˜è·å–æˆ‘ä»¬å†³ç­–æ ‘åˆå§‹åŒ–å™¨çš„æ‰€æœ‰å‚æ•°ã€‚æ‰€ä»¥å®ƒè·å–åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼Œè·å–æœ€å¤§æ·±åº¦ã€‚
- en: And it gets an optional number of features for some more randomnessã€‚So let's
    just copy and paste this hereã€‚ and then we store all of themã€‚ So we say self dot
    numberã€‚Of trees equals and treesï¼Œ self dot min sample split equals min sample
    split self dot max depth equals max depth and self dot Nã€‚Fats equalsï¼Œ and featsã€‚And
    then we implement our predict and our fit methodsï¼Œ soã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè·å–ä¸€äº›æ›´å¤šéšæœºæ€§çš„å¯é€‰ç‰¹å¾æ•°é‡ã€‚æ‰€ä»¥æˆ‘ä»¬å°±å¤åˆ¶ç²˜è´´åœ¨è¿™é‡Œã€‚ç„¶åæˆ‘ä»¬å­˜å‚¨æ‰€æœ‰è¿™äº›ã€‚å› æ­¤æˆ‘ä»¬è¯´self.dot number of treesç­‰äºæ ‘ï¼Œself.dot
    min sample splitç­‰äºmin sample splitï¼Œself.dot max depthç­‰äºmax depthï¼Œself.dot N fatsç­‰äºï¼Œå’Œfeatsã€‚ç„¶åæˆ‘ä»¬å®ç°æˆ‘ä»¬çš„é¢„æµ‹å’Œæ‹Ÿåˆæ–¹æ³•ã€‚
- en: The predict method has theã€‚Test dataã€‚ And we start with the fit methodã€‚ So we
    say fitã€‚Selfã€‚ and this will have the training data and the training labelsã€‚And
    one more thing that we want to haveï¼Œ and I can put it here first so we want to
    have an empty array of trees where we want to store each single tree that we now
    are going to createã€‚ So we say self dot trees equals and this will be an empty
    listã€‚And then in the fit methodã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ–¹æ³•æœ‰æµ‹è¯•æ•°æ®ã€‚æˆ‘ä»¬ä»æ‹Ÿåˆæ–¹æ³•å¼€å§‹ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´æ‹Ÿåˆè‡ªå·±ï¼Œè¿™å°†åŒ…å«è®­ç»ƒæ•°æ®å’Œè®­ç»ƒæ ‡ç­¾ã€‚è¿˜æœ‰ä¸€ä»¶äº‹ï¼Œæˆ‘ä»¬æƒ³è¦æœ‰ä¸€ä¸ªç©ºçš„æ ‘æ•°ç»„ï¼Œç”¨äºå­˜å‚¨æˆ‘ä»¬ç°åœ¨è¦åˆ›å»ºçš„æ¯ä¸€æ£µæ ‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯´
    self.dot trees ç­‰äºä¸€ä¸ªç©ºåˆ—è¡¨ã€‚ç„¶ååœ¨æ‹Ÿåˆæ–¹æ³•ä¸­ã€‚
- en: we want to make sure that the list is empty againã€‚And nowï¼Œ we startã€‚Training
    our treeã€‚ So we say four underscore because we don't need this in range self dot
    number of treesã€‚ And now we create our treeã€‚ So we say3 equals decision  treeã€‚And
    this will get all the parametersã€‚ So it gets min sample split equals self dot
    min sample splitã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³ç¡®ä¿åˆ—è¡¨å†æ¬¡ä¸ºç©ºã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ ‘ã€‚å› æ­¤æˆ‘ä»¬è¯´ for underscoreï¼Œå› ä¸ºæˆ‘ä»¬åœ¨èŒƒå›´ self.dot number of trees
    ä¸­ä¸éœ€è¦è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„æ ‘ã€‚å› æ­¤æˆ‘ä»¬è¯´ 3 ç­‰äºå†³ç­–æ ‘ã€‚è¿™å°†è·å–æ‰€æœ‰å‚æ•°ã€‚å› æ­¤å®ƒè·å–æœ€å°æ ·æœ¬æ‹†åˆ†ç­‰äº self.dot min sample splitã€‚
- en: Then it will have the max depth equal self dotï¼Œ max depth and the number of
    features equal self dotã€‚Number of featuresã€‚Andã€‚Nowï¼Œ what we want to do is we want
    to give our tree a random subsetã€‚ So let's define a global function hereã€‚ Let's
    call thisã€‚ This is also called bootstpingã€‚ So let's call thisã€‚Boott sampleï¼Œ which
    will get X and yã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒå°†å…·æœ‰æœ€å¤§æ·±åº¦ç­‰äº self dotï¼Œæœ€å¤§æ·±åº¦ï¼Œç‰¹å¾æ•°é‡ç­‰äº self dotã€‚ç‰¹å¾æ•°é‡ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æƒ³è¦åšçš„æ˜¯ç»™æˆ‘ä»¬çš„æ ‘ä¸€ä¸ªéšæœºå­é›†ã€‚å› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰ä¸€ä¸ªå…¨å±€å‡½æ•°ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºã€‚è¿™ä¹Ÿç§°ä¸ºå¼•å¯¼ã€‚å› æ­¤æˆ‘ä»¬ç§°ä¹‹ä¸º
    Boott æ ·æœ¬ï¼Œå®ƒå°†è·å– X å’Œ yã€‚
- en: And now we just or first look at how many different number of samples we haveã€‚
    So n samples equals x dot shapeã€‚So as alwaysï¼Œ this is a Ny and D where the first
    dimension is the number of samples and the second dimension and number of featuresã€‚And
    now we make a random choiceã€‚ So we say indices equals numpy random choiceã€‚And
    here we put in the number of samples as integerã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å…ˆçœ‹çœ‹æˆ‘ä»¬æœ‰å¤šå°‘ä¸åŒæ•°é‡çš„æ ·æœ¬ã€‚å› æ­¤ n æ ·æœ¬ç­‰äº x.dot shapeã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œè¿™æ˜¯ä¸€ä¸ª Ny å’Œ Dï¼Œå…¶ä¸­ç¬¬ä¸€ç»´æ˜¯æ ·æœ¬æ•°é‡ï¼Œç¬¬äºŒç»´æ˜¯ç‰¹å¾æ•°é‡ã€‚ç°åœ¨æˆ‘ä»¬è¿›è¡Œéšæœºé€‰æ‹©ã€‚å› æ­¤æˆ‘ä»¬è¯´ç´¢å¼•ç­‰äº
    numpy random choiceã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¾“å…¥æ ·æœ¬æ•°é‡ä½œä¸ºæ•´æ•°ã€‚
- en: This means that it will make a random choice between 0 and the number of samplesã€‚
    So our indices lie in this rate rangeï¼Œ and the size will be of size number of
    samplesï¼Œ tooã€‚But we also say replace equals trueã€‚ So this means that some of the
    indices can be there multiple timesã€‚ and others get droppedã€‚ So we randomly drop
    some of the samples and use only a subsetã€‚And thenã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€å®ƒå°†åœ¨ 0 å’Œæ ·æœ¬æ•°é‡ä¹‹é—´éšæœºé€‰æ‹©ã€‚å› æ­¤æˆ‘ä»¬çš„ç´¢å¼•åœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œå¤§å°ä¹Ÿå°†æ˜¯æ ·æœ¬æ•°é‡ã€‚ä½†æˆ‘ä»¬è¿˜è¯´æ›¿æ¢ç­‰äºçœŸã€‚è¿™æ„å‘³ç€ä¸€äº›ç´¢å¼•å¯ä»¥å¤šæ¬¡å‡ºç°ï¼Œè€Œå…¶ä»–çš„ä¼šè¢«ä¸¢å¼ƒã€‚å› æ­¤æˆ‘ä»¬éšæœºä¸¢å¼ƒä¸€äº›æ ·æœ¬ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªå­é›†ã€‚ç„¶åã€‚
- en: we returnã€‚The xã€‚å“¦ã€‚These indicesã€‚And alsoï¼Œ the why of these indicesã€‚So now we
    only have these selected samplesã€‚ğŸ¼And now we can train our tree with thisã€‚ So
    firstã€‚ we say x sample and y sample equals bootstrap sample with X and yã€‚ And
    then we say tree dot fitã€‚X sampleï¼Œ and yã€‚Sampleã€‚And then we simply append this
    to our tree listã€‚ So we say self dotã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿”å›ã€‚è¿™äº› xã€‚å“¦ã€‚è¿™äº›ç´¢å¼•ã€‚åŒæ—¶ï¼Œä¹Ÿè¿”å›è¿™äº›ç´¢å¼•çš„ yã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬åªæœ‰è¿™äº›é€‰æ‹©çš„æ ·æœ¬ã€‚ğŸ¼ç°åœ¨æˆ‘ä»¬å¯ä»¥ç”¨è¿™äº›æ¥è®­ç»ƒæˆ‘ä»¬çš„æ ‘ã€‚å› æ­¤é¦–å…ˆï¼Œæˆ‘ä»¬è¯´ x
    æ ·æœ¬å’Œ y æ ·æœ¬ç­‰äº bootstrap æ ·æœ¬ä¸ X å’Œ yã€‚ç„¶åæˆ‘ä»¬è¯´æ ‘ dot fitï¼ŒX æ ·æœ¬å’Œ y æ ·æœ¬ã€‚ç„¶åæˆ‘ä»¬ç®€å•åœ°å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„æ ‘åˆ—è¡¨ä¸­ã€‚å› æ­¤æˆ‘ä»¬è¯´
    self dotã€‚
- en: T st a pentã€‚3ã€‚And now we are done with the training phaseã€‚ And now when we predict
    itã€‚ So we make a prediction with each of our treesã€‚ So we say tree pres or tree
    predictions equalsã€‚And here I will use a list comprehension and then convert this
    to a nuier arrayã€‚ So here I say tree dot predictã€‚X 43 in self dot treesã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: T st a pentã€‚3ã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†è®­ç»ƒé˜¶æ®µã€‚ç°åœ¨ï¼Œå½“æˆ‘ä»¬è¿›è¡Œé¢„æµ‹æ—¶ã€‚æˆ‘ä»¬å¯¹æ¯æ£µæ ‘è¿›è¡Œé¢„æµ‹ã€‚å› æ­¤æˆ‘ä»¬è¯´æ ‘é¢„æµ‹æˆ–æ ‘é¢„æµ‹ç­‰äºã€‚åœ¨è¿™é‡Œæˆ‘å°†ä½¿ç”¨åˆ—è¡¨æ¨å¯¼ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºä¸€ä¸ª
    numpy æ•°ç»„ã€‚å› æ­¤åœ¨è¿™é‡Œæˆ‘è¯´æ ‘ dot predictï¼ŒX 43 åœ¨ self.dot trees ä¸­ã€‚
- en: So for each trees now we make what we call the tree predict methodã€‚ And now
    we want to do the maturity voteã€‚ But now we have to be careful because what we
    get here isã€‚Let's sayï¼Œ for exampleï¼Œ we have three trees and four samplesã€‚And then
    let's say our first tree for simplicity just does once as predictionsã€‚ So for
    each sampleã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤å¯¹äºæ¯æ£µæ ‘ï¼Œæˆ‘ä»¬ç°åœ¨åˆ¶ä½œæˆ‘ä»¬æ‰€ç§°çš„æ ‘é¢„æµ‹æ–¹æ³•ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¿›è¡Œæˆç†ŸæŠ•ç¥¨ã€‚ä½†ç°åœ¨æˆ‘ä»¬å¿…é¡»å°å¿ƒï¼Œå› ä¸ºæˆ‘ä»¬å¾—åˆ°çš„æ˜¯ã€‚å‡è®¾ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰ä¸‰æ£µæ ‘å’Œå››ä¸ªæ ·æœ¬ã€‚ç„¶åå‡è®¾æˆ‘ä»¬çš„ç¬¬ä¸€æ£µæ ‘ä¸ºäº†ç®€å•èµ·è§ä»…è¿›è¡Œä¸€æ¬¡é¢„æµ‹ã€‚å› æ­¤å¯¹äºæ¯ä¸ªæ ·æœ¬ã€‚
- en: it will have a one hereã€‚ and the second tree just makes zerosã€‚ So we have zerosã€‚
    and then the third tree also just predicts oneã€‚Soï¼Œ and then againï¼Œ this would
    beã€‚An arrayã€‚ This would be an arrayï¼Œ and this would be an array in this array
    thenã€‚ Butã€‚ and now we want to do the maturity voteã€‚ So now what weã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿™é‡Œä¼šæœ‰ä¸€ä¸ªä¸€ã€‚ç¬¬äºŒæ£µæ ‘åªä¼šç”Ÿæˆé›¶ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰é›¶ã€‚ç„¶åç¬¬ä¸‰æ£µæ ‘ä¹Ÿåªé¢„æµ‹ä¸€ä¸ªã€‚æ‰€ä»¥ï¼Œç„¶åå†ä¸€æ¬¡ï¼Œè¿™å°†æ˜¯ã€‚ä¸€ä¸ªæ•°ç»„ã€‚è¿™å°†æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œè€Œè¿™ä¸ªæ•°ç»„å°†æ˜¯è¿™ä¸ªæ•°ç»„ä¸­çš„ä¸€ä¸ªæ•°ç»„ã€‚ä½†ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¿›è¡Œæˆç†Ÿåº¦æŠ•ç¥¨ã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬ã€‚
- en: Actually one is we want a arrays that look like thisï¼Œ so we want to have 10ï¼Œ1ï¼Œ10ï¼Œ1ï¼Œ101
    and 101ã€‚Soã€‚ this oneã€‚This one from all of the treesã€‚We want to have the corresponding
    predictionsã€‚ So we convert it to this structureã€‚ And there's a very nice function
    from Nmpy that is doing exactly thisã€‚ So we say tree pres equals Nmpy swapã€‚Asã€‚å—¯ã€‚With
    this three predictionsã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªçœ‹èµ·æ¥åƒè¿™æ ·çš„æ•°ç»„ï¼Œæ‰€ä»¥æˆ‘ä»¬æƒ³è¦æœ‰ 10ï¼Œ1ï¼Œ10ï¼Œ1ï¼Œ101 å’Œ 101ã€‚æ‰€ä»¥ã€‚è¿™ä¸ªã€‚ä»æ‰€æœ‰æ ‘ä¸­ã€‚æˆ‘ä»¬æƒ³è¦æœ‰å¯¹åº”çš„é¢„æµ‹ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºè¿™ä¸ªç»“æ„ã€‚å¹¶ä¸”
    Nmpy ä¸­æœ‰ä¸€ä¸ªéå¸¸å¥½çš„å‡½æ•°æ­£å¥½åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ tree pres ç­‰äº Nmpy swapã€‚ä½œä¸ºã€‚å—¯ã€‚ç”¨è¿™ä¸‰ä¸ªé¢„æµ‹ã€‚
- en: and then we swap axis0 and axis 1ã€‚ So this is doing exactly thisã€‚And now we
    can do the maturity voteã€‚ So we say why prediction equalsã€‚ And now we predict
    the most common label for each of these three predictionsï¼Œ soã€‚Nowï¼Œ if we have
    orã€‚1ï¼Œ0ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ1ã€‚ So now we go over themã€‚And make a maturity vote then over
    them and then over themã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬äº¤æ¢ axis0 å’Œ axis 1ã€‚æ‰€ä»¥è¿™æ­£æ˜¯æˆ‘ä»¬æ‰€åšçš„ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡Œæˆç†Ÿåº¦æŠ•ç¥¨ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´é¢„æµ‹ç­‰äºã€‚ç°åœ¨æˆ‘ä»¬ä¸ºè¿™ä¸‰æ¬¡é¢„æµ‹ä¸­çš„æ¯ä¸€ä¸ªé¢„æµ‹é¢„æµ‹æœ€å¸¸è§çš„æ ‡ç­¾ï¼Œæ‰€ä»¥ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æœ‰æˆ–ã€‚1ï¼Œ0ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ1ã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬é€ä¸€æŸ¥çœ‹å®ƒä»¬ã€‚ç„¶åè¿›è¡Œæˆç†Ÿåº¦æŠ•ç¥¨ï¼Œå†å¯¹å®ƒä»¬è¿›è¡ŒæŠ•ç¥¨ã€‚
- en: So we use less comprehension againã€‚ And we sayï¼Œ most common labelã€‚Of a tree
    prediction for each tree prediction in tree predictionsã€‚And then we convert this
    to a nuyã€‚Array and return itã€‚And now the only thing left that we need is the most
    common label functionã€‚ And we also needed this in the decision tree classã€‚ So
    here we have the most common label functionã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å†æ¬¡ä½¿ç”¨åˆ—è¡¨æ¨å¯¼ã€‚æˆ‘ä»¬è¯´ï¼Œæ ‘é¢„æµ‹ä¸­æ¯ä¸ªæ ‘é¢„æµ‹çš„æœ€å¸¸è§æ ‡ç­¾ã€‚ç„¶åæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸º nuyã€‚æ•°ç»„å¹¶è¿”å›å®ƒã€‚ç°åœ¨æˆ‘ä»¬å”¯ä¸€éœ€è¦çš„æ˜¯æœ€å¸¸è§æ ‡ç­¾å‡½æ•°ã€‚æˆ‘ä»¬åœ¨å†³ç­–æ ‘ç±»ä¸­ä¹Ÿéœ€è¦è¿™ä¸ªã€‚å› æ­¤è¿™é‡Œæˆ‘ä»¬æœ‰æœ€å¸¸è§æ ‡ç­¾å‡½æ•°ã€‚
- en: As a class functionã€‚ So hereï¼Œ as you seeï¼Œ we need this a lot of timeã€‚ So it
    might be better to do this as a global functionã€‚So we put this here and you might
    even put it in another file and call this from a helpback class or somethingã€‚
    but we just put it hereï¼Œ soã€‚I will not explain this again if you don't know how
    this worksã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªç±»å‡½æ•°ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬éœ€è¦è¿™ä¸ªå¾ˆå¤šæ¬¡ã€‚å› æ­¤ï¼ŒæŠŠå®ƒä½œä¸ºä¸€ä¸ªå…¨å±€å‡½æ•°å¯èƒ½æ›´å¥½ã€‚æ‰€ä»¥æˆ‘ä»¬æŠŠå®ƒæ”¾åœ¨è¿™é‡Œï¼Œä½ ç”šè‡³å¯ä»¥æŠŠå®ƒæ”¾åœ¨å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå¹¶ä»ä¸€ä¸ªå¸®åŠ©ç±»æˆ–å…¶ä»–åœ°æ–¹è°ƒç”¨å®ƒã€‚ä½†æˆ‘ä»¬åªæ˜¯æŠŠå®ƒæ”¾åœ¨è¿™é‡Œï¼Œæ‰€ä»¥ã€‚å¦‚æœä½ ä¸çŸ¥é“è¿™ä¸ªæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæˆ‘å°±ä¸å†è§£é‡Šäº†ã€‚
- en: then please watch the last tutorial so we don't need self anymoreã€‚And we also
    need to import from collectionsã€‚Import the counter moduleã€‚And now we can do the
    maturity vote and now we are doneã€‚ so I have a little test script here to test
    our classã€‚ so I will import the breast cancer data set from the Ecal learn moduleã€‚
    Then I will generate some training and test labelsï¼Œ Then I will create our random
    forest instanceã€‚And here I just use three trees because training might take some
    timeã€‚ and we didn't optimize our codeï¼Œ soã€‚In our videoï¼Œ I just use three nowã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¯·è§‚çœ‹æœ€åä¸€ä¸ªæ•™ç¨‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦ self äº†ã€‚æˆ‘ä»¬è¿˜éœ€è¦ä» collections ä¸­å¯¼å…¥ã€‚å¯¼å…¥ counter æ¨¡å—ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡Œæˆç†Ÿåº¦æŠ•ç¥¨ï¼Œç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚æ‰€ä»¥æˆ‘è¿™é‡Œæœ‰ä¸€ä¸ªå°æµ‹è¯•è„šæœ¬æ¥æµ‹è¯•æˆ‘ä»¬çš„ç±»ã€‚æˆ‘å°†ä»
    Ecal learn æ¨¡å—å¯¼å…¥ä¹³è…ºç™Œæ•°æ®é›†ã€‚ç„¶åæˆ‘ä¼šç”Ÿæˆä¸€äº›è®­ç»ƒå’Œæµ‹è¯•æ ‡ç­¾ï¼Œç„¶ååˆ›å»ºæˆ‘ä»¬çš„éšæœºæ£®æ—å®ä¾‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘åªä½¿ç”¨ä¸‰æ£µæ ‘ï¼Œå› ä¸ºè®­ç»ƒå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰ä¼˜åŒ–æˆ‘ä»¬çš„ä»£ç ï¼Œæ‰€ä»¥ã€‚åœ¨æˆ‘ä»¬çš„è§†é¢‘ä¸­ï¼Œæˆ‘ç°åœ¨åªä½¿ç”¨ä¸‰æ£µã€‚
- en: Then I will fit the data and I will predict the test data and then calculate
    the accuracyã€‚ So let's run this and hope that everything is workingã€‚Andã€‚We made
    a mistakeï¼Œ so we say selfã€‚Ohã€‚ we want to append it to our treesï¼Œ of courseã€‚ And
    now one more tryã€‚Fingers crossedsã€‚And now we have the accuracyã€‚ So now we see
    that our model is workingã€‚ And yeahã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å°†æ‹Ÿåˆæ•°æ®ï¼Œå¹¶é¢„æµ‹æµ‹è¯•æ•°æ®ï¼Œç„¶åè®¡ç®—å‡†ç¡®æ€§ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå¸Œæœ›ä¸€åˆ‡æ­£å¸¸ã€‚ç„¶åã€‚æˆ‘ä»¬çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œæ‰€ä»¥æˆ‘ä»¬è¯´ selfã€‚å“¦ã€‚æˆ‘ä»¬æƒ³æŠŠå®ƒæ·»åŠ åˆ°æˆ‘ä»¬çš„æ ‘ä¸­ï¼Œå½“ç„¶ã€‚ç°åœ¨å†è¯•ä¸€æ¬¡ã€‚å¸Œæœ›ä¸€åˆ‡é¡ºåˆ©ã€‚ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†å‡†ç¡®æ€§ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹æ­£åœ¨å·¥ä½œã€‚æ˜¯çš„ã€‚
- en: I hope you understood everythingã€‚ And if you liked itã€‚ please subscribe to the
    channel and see you next timeï¼Œ byeã€‚![](img/657eaccf75713176fcbe8f6ff6eea819_3.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ èƒ½ç†è§£ä¸€åˆ‡ã€‚å¦‚æœä½ å–œæ¬¢ï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œä¸‹æ¬¡è§ï¼Œæ‹œæ‹œã€‚![](img/657eaccf75713176fcbe8f6ff6eea819_3.png)
