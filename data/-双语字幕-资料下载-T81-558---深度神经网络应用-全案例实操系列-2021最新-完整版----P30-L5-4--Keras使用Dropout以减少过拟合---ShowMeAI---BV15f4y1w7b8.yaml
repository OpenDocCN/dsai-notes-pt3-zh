- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P30ï¼šL5.4- Kerasä½¿ç”¨Dropoutä»¥å‡å°‘è¿‡æ‹Ÿåˆ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P30ï¼šL5.4- Kerasä½¿ç”¨Dropoutä»¥å‡å°‘è¿‡æ‹Ÿåˆ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Hetonï¼Œ welcome to applications of deep neural networks with
    Washington University In this videoã€‚ I'm going to show you how to make use of
    dropoutï¼Œ which is another type of regularization technique that you can use in
    Carra's neural networks along with L1 and L2 for the latest on my AI course and
    projects click subscribe in the bell next to it to be notified of every new video
    let's look at what dropout is actually doing for neural network Now dropout is
    added layer by layer as you can see hereã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·èµ«é¡¿ï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨dropoutï¼Œè¿™æ˜¯ä¸€ç§æ‚¨å¯ä»¥åœ¨Kerasçš„ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨çš„å¦ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¸L1å’ŒL2ä¸€èµ·ä½¿ç”¨ã€‚è¦äº†è§£æˆ‘çš„AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»æ—è¾¹çš„é“ƒé“›è®¢é˜…ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹dropoutå®é™…ä¸Šåœ¨ç¥ç»ç½‘ç»œä¸­åšäº†ä»€ä¹ˆã€‚dropoutæ˜¯é€å±‚æ·»åŠ çš„ï¼Œæ­£å¦‚æ‚¨åœ¨è¿™é‡Œçœ‹åˆ°çš„ã€‚
- en: dropoutï¼Œ you specify a percentage of the neurons for that layer and in this
    caseï¼Œ this neuronã€‚ this neuron and this neuron would be dropped out Now this changes
    each time through the steps in the epochs that way you're constantly changing
    which which of these neurons is actually available and not available when they're
    dropped outã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: dropoutï¼Œæ‚¨å¯ä»¥ä¸ºè¯¥å±‚æŒ‡å®šä¸€ä¸ªç¥ç»å…ƒçš„ç™¾åˆ†æ¯”ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™ä¸ªç¥ç»å…ƒã€è¿™ä¸ªç¥ç»å…ƒå’Œè¿™ä¸ªç¥ç»å…ƒä¼šè¢«ä¸¢å¼ƒã€‚è¿™æ ·åœ¨æ¯ä¸ªå‘¨æœŸçš„æ­¥éª¤ä¸­éƒ½ä¼šæ”¹å˜ï¼Œæ‚¨ä¸æ–­åœ°æ”¹å˜è¿™äº›ç¥ç»å…ƒçš„å¯ç”¨æ€§ï¼Œå³å“ªäº›ç¥ç»å…ƒè¢«ä¸¢å¼ƒï¼Œå“ªäº›ç¥ç»å…ƒæ˜¯å¯ç”¨çš„ã€‚
- en: they keep their weights but they no longer contribute any values by these dashed
    lines to the next layer So this is a comparison that I've heard of this oftenã€‚![](img/1a9b299d3e905cb95a8e5cd8656ae1bc_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä¿æŒæƒé‡ï¼Œä½†ä¸å†é€šè¿‡è¿™äº›è™šçº¿å¯¹ä¸‹ä¸€å±‚è´¡çŒ®ä»»ä½•å€¼ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ç»å¸¸å¬åˆ°çš„æ¯”è¾ƒä¹‹ä¸€ã€‚![](img/1a9b299d3e905cb95a8e5cd8656ae1bc_1.png)
- en: Would be like if you went to work and every day the CEO of the company told
    half the people to just go home at random that'd be pretty cool actuallyã€‚ but
    what that would do for you is it would make sure that none of the workers become
    particularly specialized at their own tasksã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå¦‚æœæ‚¨å»ä¸Šç­ï¼Œæ¯å¤©å…¬å¸CEOéšæœºå‘Šè¯‰ä¸€åŠçš„äººå›å®¶ï¼Œè¿™å®é™…ä¸Šä¼šå¾ˆé…·ã€‚ä½†è¿™æ ·åšä¼šç¡®ä¿æ²¡æœ‰å·¥äººç‰¹åˆ«ä¸“æ³¨äºè‡ªå·±çš„ä»»åŠ¡ã€‚
- en: They're very agile that way andã€‚Not overfi to the particular tasksã€‚ So this
    kind of keeps the neural network on its toesã€‚ This also attempts toã€‚Simulate having
    a bunch of neural networksï¼Œ each with different configurations of neurons put
    into there to help mitigate the randomness that we saw in the last part where
    the neural networkã€‚ you can train them multiple timesã€‚ and you get completely
    different resultsã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬åœ¨è¿™æ–¹é¢éå¸¸çµæ´»ï¼Œä¸”ä¸å®¹æ˜“è¿‡æ‹Ÿåˆç‰¹å®šä»»åŠ¡ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹å¼ä¿æŒäº†ç¥ç»ç½‘ç»œçš„æ´»åŠ›ã€‚è¿™ä¹Ÿè¯•å›¾æ¨¡æ‹Ÿä¸€å †ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªéƒ½æœ‰ä¸åŒçš„ç¥ç»å…ƒé…ç½®ï¼Œä»¥å¸®åŠ©å‡è½»æˆ‘ä»¬åœ¨ä¸Šä¸€ä¸ªéƒ¨åˆ†çœ‹åˆ°çš„éšæœºæ€§ï¼Œæ‚¨å¯ä»¥å¤šæ¬¡è®­ç»ƒå®ƒä»¬ï¼Œå¹¶è·å¾—å®Œå…¨ä¸åŒçš„ç»“æœã€‚
- en: So this can lower the variability of the output of the neural network to some
    degreeã€‚ it's almost like built in andsemblingã€‚ you have a lot of virtual neural
    networks that are created for each each layer as the dropout is randomly appliedã€‚
    Nowï¼Œ dropout this is important when it's done all of those neurons that were missingã€‚
    which change back and forthï¼Œ all the neurons go backã€‚ So each of those neuronsã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥é™ä½ç¥ç»ç½‘ç»œè¾“å‡ºçš„å˜å¼‚æ€§ã€‚è¿™å‡ ä¹å°±åƒæ˜¯å†…ç½®çš„é›†æˆï¼Œæ‚¨ä¸ºæ¯ä¸€å±‚åˆ›å»ºäº†è®¸å¤šè™šæ‹Ÿç¥ç»ç½‘ç»œï¼Œå› ä¸ºdropoutæ˜¯éšæœºåº”ç”¨çš„ã€‚ç°åœ¨ï¼Œdropoutæ˜¯é‡è¦çš„ï¼Œå½“è¿™äº›ç¼ºå¤±çš„ç¥ç»å…ƒéƒ½å˜åŒ–æ—¶ï¼Œæ‰€æœ‰çš„ç¥ç»å…ƒéƒ½ä¼šå›å½’ã€‚å› æ­¤ï¼Œæ¯ä¸€ä¸ªç¥ç»å…ƒã€‚
- en: those subnet in their return and you have the full neural network then for for
    your use after you're done with fittingã€‚ So dropout is only really affecting the
    neural network during training andã€‚out is yet another hyperparameter that we need
    to optimizeã€‚ which can influence the effectiveness of our neural networkã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å­ç½‘ç»œåœ¨è¿”å›åï¼Œæ‚¨å°±å¾—åˆ°äº†å®Œæ•´çš„ç¥ç»ç½‘ç»œï¼Œä»¥ä¾›æ‚¨åœ¨æ‹Ÿåˆå®Œæˆåä½¿ç”¨ã€‚å› æ­¤ï¼Œdropoutåªåœ¨è®­ç»ƒæœŸé—´å½±å“ç¥ç»ç½‘ç»œã€‚å®ƒæ˜¯å¦ä¸€ä¸ªæˆ‘ä»¬éœ€è¦ä¼˜åŒ–çš„è¶…å‚æ•°ï¼Œå¯èƒ½ä¼šå½±å“æˆ‘ä»¬ç¥ç»ç½‘ç»œçš„æœ‰æ•ˆæ€§ã€‚
- en: This is a pretty good animation that I rather like that shows you essentially
    how how dropout works to some degreeã€‚ Basically as the training iterations are
    going throughã€‚ It's randomly selecting dropout neurons to to go awayã€‚ The white
    neurons are the ones that are still there in the black ones are dropped outã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç›¸å½“ä¸é”™çš„åŠ¨ç”»ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼Œå®ƒå±•ç¤ºäº†dropoutæ˜¯å¦‚ä½•åœ¨æŸç§ç¨‹åº¦ä¸Šå·¥ä½œçš„ã€‚åŸºæœ¬ä¸Šï¼Œåœ¨è®­ç»ƒè¿­ä»£è¿‡ç¨‹ä¸­ï¼Œå®ƒä¼šéšæœºé€‰æ‹©ä¸€äº›dropoutç¥ç»å…ƒæ¥ä¸¢å¼ƒã€‚ç™½è‰²ç¥ç»å…ƒæ˜¯ä»ç„¶å­˜åœ¨çš„ï¼Œè€Œé»‘è‰²ç¥ç»å…ƒæ˜¯è¢«ä¸¢å¼ƒçš„ã€‚
- en: You can see the input and the output neurons remain in the in the neural networkã€‚
    Also biasesã€‚ You don't you don't drop out biasesã€‚ So let's see how we would do
    thisã€‚ We're going to do classificationã€‚ I'm going to go ahead and run this so
    that we get the classification data loadedã€‚ We're going to predict product from
    the sampleã€‚ğŸ˜Šï¼ŒThe simple data set that I that I gave you earlierã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°è¾“å…¥å’Œè¾“å‡ºç¥ç»å…ƒä»ç„¶ä¿ç•™åœ¨ç¥ç»ç½‘ç»œä¸­ã€‚è¿˜æœ‰åç½®ã€‚ä½ ä¸ä¼šä¸¢å¼ƒåç½®ã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¯¥æ€ä¹ˆåšã€‚æˆ‘ä»¬å°†è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä¼šç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œä»¥ä¾¿åŠ è½½åˆ†ç±»æ•°æ®ã€‚æˆ‘ä»¬å°†ä»æ ·æœ¬ä¸­é¢„æµ‹äº§å“ã€‚ğŸ˜Šï¼Œå°±æ˜¯æˆ‘ä¹‹å‰ç»™ä½ çš„ç®€å•æ•°æ®é›†ã€‚
- en: this is essentially how you do dropoutã€‚ It's very simpleã€‚ You add almost a layerã€‚
    a dropout sort of layerï¼Œ it's affecting the layer before itã€‚ So this is causing
    this previous layer 50 to drop out 50% of the neuronsã€‚ you can also add one to
    additional layer usually most literature that I've seen suggest not dropping out
    from your final hidden layerã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¦‚ä½•è¿›è¡Œä¸¢å¼ƒæ³•ã€‚éå¸¸ç®€å•ã€‚ä½ å‡ ä¹æ·»åŠ äº†ä¸€ä¸ªå±‚ï¼Œä¸€ä¸ªä¸¢å¼ƒå±‚ï¼Œå®ƒå½±å“ç€ä¹‹å‰çš„å±‚ã€‚æ‰€ä»¥è¿™å¯¼è‡´å‰é¢çš„å±‚50ä¸¢å¼ƒ50%çš„ç¥ç»å…ƒã€‚ä½ ä¹Ÿå¯ä»¥å†æ·»åŠ ä¸€ä¸ªé¢å¤–çš„å±‚ï¼Œé€šå¸¸æˆ‘è§è¿‡çš„å¤§å¤šæ•°æ–‡çŒ®å»ºè®®ä¸è¦ä»æœ€ç»ˆçš„éšè—å±‚ä¸­ä¸¢å¼ƒã€‚
- en: the layer that is just before the outputã€‚ so we'll follow thatã€‚ but if you wanted
    to add it to that layer too that's that's exactly how you would do itã€‚ go ahead
    and run thisã€‚It is basically setting up to do the Kfold cross validationã€‚And we
    can see now that it's completedï¼Œ our final accuracy was 70%ï¼Œ which is actually
    pretty goodã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åœ¨è¾“å‡ºä¹‹å‰çš„å±‚ã€‚å› æ­¤æˆ‘ä»¬ä¼šéµå¾ªè¿™ä¸ªã€‚ä½†å¦‚æœä½ æƒ³æŠŠå®ƒä¹ŸåŠ åˆ°é‚£ä¸ªå±‚ä¸Šï¼Œè¿™å°±æ˜¯ä½ è¯¥æ€ä¹ˆåšã€‚ç»§ç»­è¿è¡Œè¿™ä¸ªã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯åœ¨ä¸ºKfoldäº¤å‰éªŒè¯åšå‡†å¤‡ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒå·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆå‡†ç¡®ç‡æ˜¯70%ï¼Œè¿™å®é™…ä¸Šæ˜¯ç›¸å½“ä¸é”™çš„ã€‚
- en: If we were to rerun thisï¼Œ we might get a different a different accuracyã€‚ but
    this does help stabilize some of the effects of the of the random weightsã€‚ So
    we'll try thatã€‚ Let's go ahead and run it a second timeã€‚And we can see the resultã€‚
    It'sï¼Œ it's 71%ã€‚ So it actuallyã€‚ we can see that overallã€‚On our sample size of
    two anywayã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬é‡æ–°è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„å‡†ç¡®ç‡ã€‚ä½†è¿™ç¡®å®æœ‰åŠ©äºç¨³å®šéšæœºæƒé‡çš„ä¸€äº›å½±å“ã€‚æˆ‘ä»¬æ¥è¯•ä¸€ä¸‹ã€‚è®©æˆ‘ä»¬å†è¿è¡Œä¸€æ¬¡ã€‚å¯ä»¥çœ‹åˆ°ç»“æœæ˜¯71%ã€‚æ‰€ä»¥å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ€»ä½“ä¸Šï¼Œåœ¨æˆ‘ä»¬ä¸¤ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ã€‚
- en: this dropout does seem to be helping compared to the previous one that we were
    looking at where we were using L1 and L2 in the previous the previous part in
    a different videoã€‚Now it didn't remove the variance completelyï¼Œ we were at 70%
    and removed moved it up to 71 just from rerunning itã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸¢å¼ƒæ³•ä¼¼ä¹ç›¸è¾ƒäºæˆ‘ä»¬ä¹‹å‰åœ¨å…¶ä»–è§†é¢‘ä¸­ä½¿ç”¨L1å’ŒL2æ—¶å¸®åŠ©æ›´å¤§ã€‚ç°åœ¨å®ƒå¹¶æ²¡æœ‰å®Œå…¨æ¶ˆé™¤æ–¹å·®ï¼Œæˆ‘ä»¬çš„å‡†ç¡®ç‡ä»70%æé«˜åˆ°äº†71%ï¼Œä»…ä»…æ˜¯é€šè¿‡é‡æ–°è¿è¡Œã€‚
- en: In the next videoï¼Œ we're going to look at how to useã€‚Boottrapping so that we
    can get better benchmarks by running this through a number of times and averaging
    things togetherã€‚ Thank you for watching this video and the next video we're going
    to look at L1 L2 and drop out all together and get an idea of which you should
    be using and why this content changes often so subscribe to the channel to stay
    up to date on this course and other topics and artificial intelligenceã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è‡ªåŠ©æ³•ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥é€šè¿‡å¤šæ¬¡è¿è¡Œå¹¶å°†ç»“æœå¹³å‡æ¥è·å¾—æ›´å¥½çš„åŸºå‡†ã€‚æ„Ÿè°¢ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­æˆ‘ä»¬å°†ä¸€èµ·çœ‹çœ‹L1ã€L2å’Œä¸¢å¼ƒæ³•ï¼Œäº†è§£ä½ åº”è¯¥ä½¿ç”¨å“ªä¸€ç§ä»¥åŠåŸå› ã€‚è¿™äº›å†…å®¹ç»å¸¸å˜åŒ–ï¼Œæ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£è¿™ä¸ªè¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚
- en: '![](img/1a9b299d3e905cb95a8e5cd8656ae1bc_3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a9b299d3e905cb95a8e5cd8656ae1bc_3.png)'
