- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- å¸¦æœ‰
    Autograd å’Œåå‘ä¼ æ’­çš„æ¢¯åº¦ä¸‹é™ - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P5ï¼šL5- å¸¦æœ‰
    Autograd å’Œåå‘ä¼ æ’­çš„æ¢¯åº¦ä¸‹é™ - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome to a new Pytorch tutorialã€‚ In this tutorialã€‚ I show you
    a concrete example of how we optimize our model with automatic gradient computation
    using the pytorch autogra packageã€‚ So we start by implementing the linear regression
    algorithm from scratchã€‚ where we do every step manuallyã€‚ So we implement the equations
    to calculate the model prediction and the loss functionã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°æ–°çš„Pytorchæ•™ç¨‹ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨pytorch autogradåŒ…è‡ªåŠ¨æ¢¯åº¦è®¡ç®—æ¥ä¼˜åŒ–æˆ‘ä»¬çš„æ¨¡å‹çš„å…·ä½“ä¾‹å­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹å®ç°çº¿æ€§å›å½’ç®—æ³•ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­éƒ½æ‰‹åŠ¨è¿›è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®ç°è®¡ç®—æ¨¡å‹é¢„æµ‹å’ŒæŸå¤±å‡½æ•°çš„æ–¹ç¨‹ã€‚
- en: Then we do a numerical computation of the gradients and implement the formulaã€‚
    And then we implement the gradient decent algorithm to optimize our parametersã€‚
    When this is workingï¼Œ we see how we can replace the manually computed gradients
    with the automatic back propagation algorithm from Pytorchã€‚ğŸ˜Šï¼ŒSo this is step number
    2ã€‚ and in the third stepsï¼Œ we in the third stepã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¿›è¡Œæ¢¯åº¦çš„æ•°å€¼è®¡ç®—å¹¶å®ç°å…¬å¼ã€‚æ¥ç€æˆ‘ä»¬å®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥ä¼˜åŒ–æˆ‘ä»¬çš„å‚æ•°ã€‚å½“è¿™ä¸€åˆ‡æ­£å¸¸æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•å°†æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦æ›¿æ¢ä¸ºPytorchçš„è‡ªåŠ¨åå‘ä¼ æ’­ç®—æ³•ã€‚ğŸ˜Šæ‰€ä»¥è¿™æ˜¯ç¬¬äºŒæ­¥ã€‚åœ¨ç¬¬ä¸‰æ­¥ä¸­ï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸‰æ­¥ã€‚
- en: we replace the manually computed loss and parameter updates by using the loss
    and optimize the classes in Pytorchã€‚ and in the final stepï¼Œ we replaced the manually
    computed model prediction by implementing a pytorch modelã€‚So when we understood
    each of these stepsï¼Œ Pytorch can do most of the work for usã€‚ Of courseã€‚ we still
    have to to design our model and have to know which loss and optimizer we should
    useã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä½¿ç”¨Pytorchä¸­çš„æŸå¤±å’Œä¼˜åŒ–ç±»æ¥æ›¿æ¢æ‰‹åŠ¨è®¡ç®—çš„æŸå¤±å’Œå‚æ•°æ›´æ–°ã€‚åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®ç°ä¸€ä¸ªpytorchæ¨¡å‹æ›¿æ¢äº†æ‰‹åŠ¨è®¡ç®—çš„æ¨¡å‹é¢„æµ‹ã€‚å› æ­¤ï¼Œå½“æˆ‘ä»¬ç†è§£è¿™äº›æ­¥éª¤æ—¶ï¼ŒPytorchå¯ä»¥ä¸ºæˆ‘ä»¬åšå¤§éƒ¨åˆ†å·¥ä½œã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦è®¾è®¡æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶ä¸”è¦çŸ¥é“åº”è¯¥ä½¿ç”¨å“ªä¸ªæŸå¤±å’Œä¼˜åŒ–å™¨ã€‚
- en: But we don't have to worry about the underlying algorithms any moreã€‚So now this
    video will cover steps 1 and 2ã€‚And in the next videoï¼Œ we will see the steps 3
    and 4ã€‚ So let's startã€‚ And I assume that you already know how linear regression
    and gradient decent worksã€‚ And if notï¼Œ then please watch my machine learning from
    scratch tutorial about this algorithmã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¸å¿…å†æ‹…å¿ƒåº•å±‚ç®—æ³•ã€‚å› æ­¤ï¼Œè¿™ä¸ªè§†é¢‘å°†æ¶µç›–æ­¥éª¤1å’Œ2ã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ­¥éª¤3å’Œ4ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬å¼€å§‹ã€‚æˆ‘å‡è®¾ä½ å·²ç»çŸ¥é“çº¿æ€§å›å½’å’Œæ¢¯åº¦ä¸‹é™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚å¦‚æœä¸çŸ¥é“ï¼Œè¯·è§‚çœ‹æˆ‘å…³äºè¿™ä¸ªç®—æ³•çš„ä»é›¶å¼€å§‹çš„æœºå™¨å­¦ä¹ æ•™ç¨‹ã€‚
- en: because now I will not explain all the steps in detailã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_1.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºç°åœ¨æˆ‘ä¸ä¼šè¯¦ç»†è§£é‡Šæ‰€æœ‰æ­¥éª¤ã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_1.png)
- en: But I put the link in the descriptionã€‚So now we do everything from scratchã€‚
    So we use only nuyã€‚ So we import nuy S and Pï¼Œ and then we use linear regressionã€‚
    So we use a function which just just does a linear combination of some weights
    and our inputsã€‚ and we don't care about the bias hereã€‚So in our exampleï¼Œ let's
    say F equalsã€‚Two times xã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘æŠŠé“¾æ¥æ”¾åœ¨æè¿°ä¸­äº†ã€‚å› æ­¤æˆ‘ä»¬ç°åœ¨ä»å¤´å¼€å§‹åšæ‰€æœ‰äº‹æƒ…ã€‚æˆ‘ä»¬åªä½¿ç”¨nuyã€‚ç„¶åæˆ‘ä»¬å¯¼å…¥nuyçš„Så’ŒPï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨çº¿æ€§å›å½’ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»…ä»…è¿›è¡Œä¸€äº›æƒé‡ä¸è¾“å…¥çš„çº¿æ€§ç»„åˆçš„å‡½æ•°ã€‚è¿™é‡Œæˆ‘ä»¬ä¸å…³å¿ƒåå·®ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå‡è®¾Fç­‰äº2å€çš„xã€‚
- en: So our weight must be 2ã€‚ And then let's do some training samplesã€‚ So let's say
    xã€‚Equals numpy dot arrayã€‚ And then we put some tests or training samplesã€‚ So let's
    say 1ï¼Œ2ï¼Œ3ï¼Œ and 4ã€‚ And this will be of numpyã€‚Orï¼Œ let's getã€‚Give this a data type
    onï¼Œ say this is Nmpy dot floatat 32ã€‚And thenï¼Œ we also want a yã€‚Andã€‚Since our formula
    isï¼Œ this is 2 xã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çš„æƒé‡å¿…é¡»æ˜¯2ã€‚ç„¶åæˆ‘ä»¬åšä¸€äº›è®­ç»ƒæ ·æœ¬ã€‚å‡è®¾xç­‰äºnumpy.dot.arrayã€‚ç„¶åæˆ‘ä»¬æ”¾ä¸€äº›æµ‹è¯•æˆ–è®­ç»ƒæ ·æœ¬ã€‚å‡è®¾æ˜¯1ã€2ã€3å’Œ4ã€‚è¿™å°†æ˜¯numpyçš„ã€‚æˆ–è€…ï¼Œè®©æˆ‘ä»¬ç»™å®ƒä¸€ä¸ªæ•°æ®ç±»å‹ï¼Œæ¯”å¦‚è¯´è¿™æ˜¯Nmpy.dot.float32ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¹Ÿæƒ³è¦ä¸€ä¸ªyã€‚å› ä¸ºæˆ‘ä»¬çš„å…¬å¼æ˜¯ï¼Œè¿™å°±æ˜¯2xã€‚
- en: we have to multiply each of the values by 2ã€‚ So 2ï¼Œ4ï¼Œ6 and 8ã€‚And now we initialize
    our weightsã€‚ So we simply say w equals 0 in the beginningã€‚And now we have to calculate
    our model predictionã€‚ And we also have to calculate the lossã€‚And then we have
    to calculate the gradientsã€‚ So now we do each of these steps manuallyã€‚ So let's
    define a functionï¼Œ and we call this forwardã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»å°†æ¯ä¸ªå€¼ä¹˜ä»¥2ã€‚å› æ­¤æ˜¯2ã€4ã€6å’Œ8ã€‚ç°åœ¨æˆ‘ä»¬åˆå§‹åŒ–æˆ‘ä»¬çš„æƒé‡ã€‚æˆ‘ä»¬ç®€å•åœ°è¯´wåœ¨å¼€å§‹æ—¶ç­‰äº0ã€‚ç°åœ¨æˆ‘ä»¬å¿…é¡»è®¡ç®—æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿè¦è®¡ç®—æŸå¤±ã€‚ç„¶åæˆ‘ä»¬è¦è®¡ç®—æ¢¯åº¦ã€‚ç°åœ¨æˆ‘ä»¬æ‰‹åŠ¨æ‰§è¡Œæ¯ä¸€ä¸ªæ­¥éª¤ã€‚é‚£ä¹ˆæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºforwardã€‚
- en: So this is a forward pass to follow the conventions of pytorrchï¼Œ which will
    get xã€‚ and then our model output is simply w times xã€‚So this is the forward passã€‚Nowï¼Œ
    the lossã€‚So here we define the function lossï¼Œ which depends on why and why predictedã€‚Soã€‚The
    this is the model outputã€‚And now here in this caseã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªéµå¾ªpytorchçº¦å®šçš„å‰å‘ä¼ æ’­ï¼Œå°†è·å–xã€‚ç„¶åæˆ‘ä»¬çš„æ¨¡å‹è¾“å‡ºä»…ä»…æ˜¯wä¹˜ä»¥xã€‚è¿™æ˜¯å‰å‘ä¼ æ’­ã€‚ç°åœ¨ï¼ŒæŸå¤±ã€‚è¿™é‡Œæˆ‘ä»¬å®šä¹‰æŸå¤±å‡½æ•°ï¼Œå®ƒä¾èµ–äºyå’Œé¢„æµ‹çš„yã€‚æ‰€ä»¥ï¼Œè¿™æ˜¯æ¨¡å‹è¾“å‡ºã€‚ç°åœ¨åœ¨è¿™ç§æƒ…å†µä¸‹ã€‚
- en: this is the or the loss equals the mean squared error in the case of linear
    regressionã€‚ And we can calculate this by sayingï¼Œ this isã€‚å—¯ã€‚Let's say y predict
    it minus yã€‚ And then to the power of 2ã€‚ And then we do the mean operationã€‚So this
    is the lossã€‚ and now we manually have have to calculate the gradient of the loss
    with respect to our parametersã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ–æŸå¤±ç­‰äºçº¿æ€§å›å½’æƒ…å†µä¸‹çš„å‡æ–¹è¯¯å·®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯´ï¼Œè¿™æ˜¯ã€‚å—¯ã€‚å‡è®¾yé¢„æµ‹å‡å»yã€‚ç„¶åå¹³æ–¹ã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œå¹³å‡æ“ä½œã€‚æ‰€ä»¥è¿™æ˜¯æŸå¤±ã€‚ç°åœ¨æˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—æŸå¤±ç›¸å¯¹äºæˆ‘ä»¬å‚æ•°çš„æ¢¯åº¦ã€‚
- en: So let's have a look at the mean squared errorã€‚ So the formula is1 over n because
    it's the meanã€‚ And then we have our w times x our prediction minus the actual
    value to the power of2ã€‚ And now if you want to have the derivativeã€‚ So the derivative
    of thisã€‚ let's call this j or objective function with respect to w equals1 over
    n And then we have two times xã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹å‡æ–¹è¯¯å·®ã€‚å…¬å¼æ˜¯1é™¤ä»¥nï¼Œå› ä¸ºè¿™æ˜¯å¹³å‡å€¼ã€‚ç„¶åæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„wä¹˜ä»¥xï¼Œæˆ‘ä»¬çš„é¢„æµ‹å‡å»å®é™…å€¼çš„å¹³æ–¹ã€‚ç°åœ¨å¦‚æœä½ æƒ³è¦å¯¼æ•°ã€‚é‚£ä¹ˆè¿™ä¸ªå¯¼æ•°ï¼Œè®©æˆ‘ä»¬ç§°ä¹‹ä¸ºjæˆ–ç›®æ ‡å‡½æ•°å…³äºwç­‰äº1é™¤ä»¥nï¼Œç„¶åæˆ‘ä»¬æœ‰ä¸¤ä¸ªä¹˜ä»¥xã€‚
- en: and then times W times x minus yã€‚ So this is the numerical computationdã€‚Computed
    derivativeã€‚ Also please double check the math for yourselfã€‚And now we implement
    thisã€‚ So we say define gradientã€‚ which is dependentã€‚On x and y and also Y predictedã€‚
    And now we can do this in one lineã€‚ So we return numpy dotã€‚ We need a dot product
    of two times xã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä¹˜ä»¥Wä¹˜ä»¥xå‡å»yã€‚æ‰€ä»¥è¿™æ˜¯æ•°å€¼è®¡ç®—çš„ã€‚è®¡ç®—å‡ºçš„å¯¼æ•°ã€‚è¯·è‡ªè¡Œä»”ç»†æ£€æŸ¥æ•°å­¦ã€‚ç°åœ¨æˆ‘ä»¬å®ç°è¿™ä¸ªã€‚æˆ‘ä»¬è¯´å®šä¹‰æ¢¯åº¦ï¼Œå®ƒä¾èµ–äºxå’Œyä»¥åŠé¢„æµ‹çš„yã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥ä¸€è¡Œå®Œæˆã€‚æˆ‘ä»¬è¿”å›numpy.dotã€‚æˆ‘ä»¬éœ€è¦ä¸¤ä¸ªä¹˜ä»¥xçš„ç‚¹ç§¯ã€‚
- en: '![](img/49d809dfcdcd868a09e2ad42202f36fc_3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49d809dfcdcd868a09e2ad42202f36fc_3.png)'
- en: And then here we have y predicted minus yã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_5.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™é‡Œæˆ‘ä»¬æœ‰é¢„æµ‹çš„yå‡å»yã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_5.png)
- en: Said this is this formula hereã€‚And thenï¼Œ of courseï¼Œ we also need the meanã€‚ So
    let's say this is dot meanã€‚ We can simply do it like this in Nyã€‚And nowã€‚Yeahï¼Œ
    these are theã€‚Things we need Nowï¼Œ let's print ourã€‚Prediction before the trainingã€‚
    So let's printã€‚ and we use an F stringã€‚ So prediction before trainingã€‚And let's
    say we want to predict the value5ã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´è¿™æ˜¯è¿™é‡Œçš„å…¬å¼ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¹³å‡å€¼ã€‚å‡è®¾è¿™æ˜¯ç‚¹å¹³å‡ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°åœ¨Nyä¸­è¿™æ ·åšã€‚ç°åœ¨ï¼Œè¿™äº›æ˜¯æˆ‘ä»¬éœ€è¦çš„ä¸œè¥¿ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨è®­ç»ƒä¹‹å‰æ‰“å°æˆ‘ä»¬çš„é¢„æµ‹ã€‚æ‰€ä»¥æˆ‘ä»¬æ‰“å°ã€‚æˆ‘ä»¬ä½¿ç”¨Få­—ç¬¦ä¸²ã€‚æ‰€ä»¥è®­ç»ƒä¹‹å‰çš„é¢„æµ‹ã€‚å‡è®¾æˆ‘ä»¬æƒ³é¢„æµ‹å€¼5ã€‚
- en: which should be 10ã€‚And hereï¼Œ we can doã€‚In the F stringã€‚ we can actually use
    an expression so we can call this forward method and with fiveã€‚And let's say we
    only want three decimal valuesã€‚And nowï¼Œ let's start our trainingã€‚So let's define
    some parametersã€‚ So we need a learning rateï¼Œ which isï¼Œ let's sayï¼Œ001ã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥æ˜¯10ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨Få­—ç¬¦ä¸²ä¸­åšåˆ°ã€‚åœ¨è¿™ä¸ªè¡¨è¾¾å¼ä¸­æˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™ä¸ªforwardæ–¹æ³•ï¼Œä¼ å…¥5ã€‚å‡è®¾æˆ‘ä»¬åªæƒ³è¦ä¸‰ä½å°æ•°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¼€å§‹è®­ç»ƒã€‚æˆ‘ä»¬å®šä¹‰ä¸€äº›å‚æ•°ã€‚æˆ‘ä»¬éœ€è¦å­¦ä¹ ç‡ï¼Œå‡è®¾æ˜¯0.01ã€‚
- en: And then we need a number of iterationsã€‚ So we say an its equals 10ã€‚And now
    let's do our training loopã€‚ So we say for epoch in range and itersã€‚And then firstï¼Œ
    we do theã€‚Predictionï¼Œ which is the forward passã€‚ So this is the forward passã€‚And
    we can simply do this with our functionsã€‚ So we say y prediction or y pre equals
    forwardã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¿­ä»£æ¬¡æ•°ã€‚æˆ‘ä»¬è¯´itsç­‰äº10ã€‚ç°åœ¨è®©æˆ‘ä»¬è¿›è¡Œè®­ç»ƒå¾ªç¯ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å¯¹äºepochåœ¨èŒƒå›´å†…å’Œiterã€‚ç„¶åé¦–å…ˆï¼Œæˆ‘ä»¬è¿›è¡Œé¢„æµ‹ï¼Œå³å‰å‘ä¼ æ’­ã€‚è¿™æ˜¯å‰å‘ä¼ æ’­ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç”¨æˆ‘ä»¬çš„å‡½æ•°æ¥å®Œæˆè¿™ä¸ªã€‚æ‰€ä»¥æˆ‘ä»¬è¯´yé¢„æµ‹æˆ–y
    preç­‰äºforwardã€‚
- en: And then we put in our capital xã€‚And now we want to have the lossã€‚ So our loss
    L equals the loss ofã€‚The actual yã€‚And our Y predict itã€‚Nowï¼Œ we need to get theã€‚Grasï¼Œ
    so our gradientsï¼Œ with respect to Wã€‚ So D W equals the gradient function that
    we just implementedã€‚ which is dependent on x and y and the y predictedã€‚srryã€‚Why
    pretã€‚Andã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¾“å…¥æˆ‘ä»¬çš„å¤§å†™xã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦æŸå¤±ã€‚æ‰€ä»¥æˆ‘ä»¬çš„æŸå¤±Lç­‰äºå®é™…yçš„æŸå¤±ã€‚å’Œæˆ‘ä»¬çš„é¢„æµ‹yã€‚ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦è·å–ã€‚æ¢¯åº¦ï¼Œå³å…³äºWçš„æ¢¯åº¦ã€‚æ‰€ä»¥D Wç­‰äºæˆ‘ä»¬åˆšåˆšå®ç°çš„æ¢¯åº¦å‡½æ•°ï¼Œä¾èµ–äºxå’Œyä»¥åŠé¢„æµ‹çš„yã€‚
- en: Now we have to update our weightsã€‚ and yesï¼Œ so the update formula in the gradient
    decentcent algorithm is just we go into the negative direction of the training
    of the gradientã€‚ So minus xã€‚ And then hereï¼Œ the step width or the so called learning
    rate times our our gradientã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¿…é¡»æ›´æ–°æˆ‘ä»¬çš„æƒé‡ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­çš„æ›´æ–°å…¬å¼å°±æ˜¯æˆ‘ä»¬æœç€æ¢¯åº¦çš„è´Ÿæ–¹å‘å‰è¿›ã€‚æ‰€ä»¥æ˜¯è´Ÿxã€‚åœ¨è¿™é‡Œï¼Œæ­¥é•¿æˆ–æ‰€è°“çš„å­¦ä¹ ç‡ä¹˜ä»¥æˆ‘ä»¬çš„æ¢¯åº¦ã€‚
- en: So this is the update formulaã€‚And then let's say we also want to print some
    information hereã€‚ So we say if epoch modularï¼Œ just say one hereï¼Œ because now we
    want to print every stepã€‚If this is 0ã€‚ we want to printï¼Œ let's sayã€‚We want to
    print the epochã€‚ And here we print epoch plus 1ã€‚And then we want to get the weightï¼Œ
    which isã€‚The weightã€‚Wï¼Œ alsoï¼Œ just 3ã€‚Deimal numbersã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯æ›´æ–°å…¬å¼ã€‚ç„¶åæˆ‘ä»¬å‡è®¾æˆ‘ä»¬è¿˜æƒ³åœ¨è¿™é‡Œæ‰“å°ä¸€äº›ä¿¡æ¯ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å¦‚æœepochå–æ¨¡ï¼Œå‡è®¾è¿™é‡Œä¸º1ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³åœ¨æ¯ä¸€æ­¥æ‰“å°ã€‚å¦‚æœè¿™æ˜¯0ã€‚æˆ‘ä»¬æƒ³æ‰“å°ï¼Œæ¯”å¦‚è¯´ã€‚æˆ‘ä»¬æƒ³æ‰“å°epochã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æ‰“å°epochåŠ 1ã€‚ç„¶åæˆ‘ä»¬æƒ³å¾—åˆ°æƒé‡ï¼Œå³ã€‚æƒé‡ã€‚Wï¼Œä¿ç•™ä¸‰ä½å°æ•°ã€‚
- en: And then we also want to have the lossã€‚ So the loss equalsã€‚The lossã€‚ And here
    we sayï¼Œ pointã€‚8ã€‚ let's sayã€‚And yeahï¼Œ and then at the endã€‚We want to print the
    prediction after the trainingã€‚Soï¼Œ nowã€‚ let's predictã€‚Prediict a print predictionã€‚After
    trainingã€‚And yeahï¼Œ soã€‚ Nowã€‚ let's run this and see what happensã€‚So everything
    should be workingã€‚ And nowï¼Œ so yeahã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¿˜æƒ³è¦æœ‰æŸå¤±ã€‚æŸå¤±ç­‰äºã€‚æŸå¤±ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯´ï¼Œç‚¹ã€‚8ã€‚å°±è¿™æ ·ã€‚ç„¶ååœ¨æœ€åã€‚æˆ‘ä»¬æƒ³åœ¨è®­ç»ƒåæ‰“å°é¢„æµ‹ã€‚æ‰€ä»¥ï¼Œç°åœ¨ã€‚è®©æˆ‘ä»¬é¢„æµ‹ã€‚é¢„æµ‹å¹¶æ‰“å°é¢„æµ‹ã€‚åœ¨è®­ç»ƒåã€‚æ˜¯çš„ï¼Œé‚£ä¹ˆã€‚ç°åœ¨ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä¸€åˆ‡åº”è¯¥éƒ½èƒ½æ­£å¸¸å·¥ä½œã€‚ç°åœ¨ï¼Œæ˜¯çš„ã€‚
- en: before our trainingï¼Œ the prediction is0ã€‚And then for each stepã€‚ remember that
    our formula should be  two times xã€‚ So our w should be 2 in the beginningã€‚ And
    we see that with each training stepï¼Œ it it increases our weightï¼Œ and it decreases
    our lossã€‚ So it gets better with every stepã€‚ And after the trainingï¼Œ our model
    prediction is 9ã€‚999ã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„è®­ç»ƒä¹‹å‰ï¼Œé¢„æµ‹ä¸º0ã€‚ç„¶åå¯¹äºæ¯ä¸€æ­¥ã€‚è®°ä½æˆ‘ä»¬çš„å…¬å¼åº”è¯¥æ˜¯2å€çš„xã€‚æ‰€ä»¥æˆ‘ä»¬çš„wä¸€å¼€å§‹åº”è¯¥æ˜¯2ã€‚æˆ‘ä»¬çœ‹åˆ°æ¯ä¸ªè®­ç»ƒæ­¥éª¤éƒ½åœ¨å¢åŠ æˆ‘ä»¬çš„æƒé‡ï¼Œå¹¶ä¸”å‡å°‘æˆ‘ä»¬çš„æŸå¤±ã€‚å› æ­¤ï¼Œæ¯ä¸€æ­¥éƒ½ä¼šå˜å¾—æ›´å¥½ã€‚è®­ç»ƒåï¼Œæˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹ä¸º9.999ã€‚
- en: So it's almost thereã€‚ So let's sayï¼Œ for exampleï¼Œ now we want to have more iterations
    hereã€‚ say we only did we only did 10 iterationsï¼Œ which is not muchã€‚ Nowã€‚ if we
    run this and let's print every second step onlyã€‚Then we seeï¼Œ in the endï¼Œ our loss
    is 0ã€‚ and the prediction is correctã€‚Nowï¼Œ this is the implementation where we did
    everything manuallyã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¿«åˆ°äº†ã€‚æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­ï¼Œç°åœ¨æˆ‘ä»¬æƒ³è¦æ›´å¤šçš„è¿­ä»£ã€‚å‡è®¾æˆ‘ä»¬åªè¿›è¡Œäº†10æ¬¡è¿­ä»£ï¼Œè¿™å¹¶ä¸å¤šã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå¹¶ä¸”æ¯éš”ä¸€æ­¥æ‰“å°ä¸€æ¬¡ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°ï¼Œæœ€åæˆ‘ä»¬çš„æŸå¤±æ˜¯0ï¼Œé¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚ç°åœ¨ï¼Œè¿™æ˜¯æˆ‘ä»¬æ‰‹åŠ¨å®ç°çš„éƒ¨åˆ†ã€‚
- en: And now let's replace the gradient calculationã€‚ So let's select all of this
    and copy this into a separate fileã€‚And now we don't use numpy anymoreã€‚ So now
    let's only import torch and do everything with pie torchã€‚Andï¼Œ of courseï¼Œ what
    we now want to get rid of is this gradientï¼Œ the the manually computed gradientã€‚
    So we simply delete thisã€‚ We don't need this anymoreã€‚And now we don't have nuy
    arraysã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ›¿æ¢æ¢¯åº¦è®¡ç®—ã€‚æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©æ‰€æœ‰è¿™äº›å¹¶å°†å…¶å¤åˆ¶åˆ°ä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ä¸­ã€‚ç°åœ¨æˆ‘ä»¬ä¸å†ä½¿ç”¨numpyäº†ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬åªéœ€å¯¼å…¥torchï¼Œå¹¶ä½¿ç”¨torchæ¥å®Œæˆæ‰€æœ‰å·¥ä½œã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ç°åœ¨æƒ³è¦å»æ‰çš„æ˜¯è¿™ä¸ªæ¢¯åº¦ï¼Œå³æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦ã€‚å› æ­¤æˆ‘ä»¬ç®€å•åœ°åˆ é™¤å®ƒã€‚æˆ‘ä»¬ä¸å†éœ€è¦è¿™ä¸ªã€‚ç°åœ¨æˆ‘ä»¬ä¹Ÿæ²¡æœ‰nuyæ•°ç»„äº†ã€‚
- en: So this is now aã€‚Torch dot Tzarã€‚And our data type is now a torch dot float 32
    and the same with our yã€‚ which is now aã€‚Torch dotã€‚Tensor and also the data types
    from the torch moduleã€‚But everything else is the same hereã€‚ So the same syntaxã€‚And
    now our W also has to be a tenorã€‚ So let's say this is a torch dot tenor with0ã€‚0
    in the beginningã€‚ and it also gets a data type ofã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨è¿™æ˜¯ä¸€ä¸ªTorch dot Tzarã€‚æˆ‘ä»¬çš„æ•°æ®ç±»å‹ç°åœ¨æ˜¯torch dot float 32ï¼Œyä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®ƒç°åœ¨æ˜¯ä¸€ä¸ªTorch dot Tensorï¼Œå¹¶ä¸”æ•°æ®ç±»å‹æ¥è‡ªtorchæ¨¡å—ã€‚ä½†å…¶ä»–ä¸€åˆ‡åœ¨è¿™é‡Œéƒ½æ˜¯ä¸€æ ·çš„ã€‚å› æ­¤ï¼Œè¯­æ³•ç›¸åŒã€‚ç°åœ¨æˆ‘ä»¬çš„Wä¹Ÿå¿…é¡»æ˜¯ä¸€ä¸ªå¼ é‡ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´è¿™æ˜¯ä¸€å¼€å§‹ä¸º0.0çš„torch
    dot tensorã€‚å®ƒä¹Ÿä¼šæœ‰ä¸€ä¸ªæ•°æ®ç±»å‹ã€‚
- en: say torch dot float 32ã€‚ And since we are interested in the gradient of our loss
    with respect to this parameterã€‚ we need to specify that this requires the gradient
    computationã€‚ So requires gra equals trueã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_7.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´æ˜¯torch dot float 32ã€‚ç”±äºæˆ‘ä»¬å¯¹æŸå¤±å…³äºè¿™ä¸ªå‚æ•°çš„æ¢¯åº¦æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å®šè¿™éœ€è¦æ¢¯åº¦è®¡ç®—ã€‚æ‰€ä»¥requires grad = trueã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_7.png)
- en: Nowï¼Œ the forward function and the loss function is still the same because we
    can use the same syntax in pytorrchã€‚And nowã€‚In our training loopï¼Œ the forward
    pass is still the sameã€‚ The loss is the sameã€‚ And now the gradientï¼Œ this is the
    equal to the backward passã€‚ So rememberï¼Œ in backward propagationã€‚ we first do
    a forward passã€‚ That's why we used the syntaxã€‚ And then later for the gradientsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå‰å‘å‡½æ•°å’ŒæŸå¤±å‡½æ•°ä»ç„¶ç›¸åŒï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ pytorrch ä¸­ä½¿ç”¨ç›¸åŒçš„è¯­æ³•ã€‚ ç°åœ¨ï¼Œåœ¨æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¸­ï¼Œå‰å‘ä¼ æ’­ä»ç„¶ç›¸åŒã€‚ æŸå¤±æ˜¯ç›¸åŒçš„ã€‚ ç°åœ¨æ¢¯åº¦ç­‰äºåå‘ä¼ æ’­ã€‚
    æ‰€ä»¥è®°ä½ï¼Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œå‰å‘ä¼ æ’­ã€‚ è¿™å°±æ˜¯æˆ‘ä»¬ä½¿ç”¨è¿™ç§è¯­æ³•çš„åŸå› ã€‚ ç„¶åå†è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚
- en: we use the backward passã€‚ So here we simply call L dotã€‚Backwardã€‚And this will
    calculate the gradient of our loss with respect to Wã€‚ So Pyto does all the computations
    for usã€‚ And now we update our weightsã€‚ But here we want to be carefulã€‚ and I explained
    this in the tutorial about the auto gridd packageã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨åå‘ä¼ æ’­ã€‚ æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°è°ƒç”¨ L dotã€‚Backwardã€‚è¿™æ ·å°±ä¼šè®¡ç®—æˆ‘ä»¬æŸå¤±å¯¹ W çš„æ¢¯åº¦ã€‚ æ‰€ä»¥ Pyto ä¸ºæˆ‘ä»¬å®Œæˆæ‰€æœ‰è®¡ç®—ã€‚ ç°åœ¨æˆ‘ä»¬æ›´æ–°æˆ‘ä»¬çš„æƒé‡ã€‚
    ä½†åœ¨è¿™é‡Œæˆ‘ä»¬è¦å°å¿ƒã€‚ æˆ‘åœ¨å…³äº auto gridd åŒ…çš„æ•™ç¨‹ä¸­è§£é‡Šè¿‡è¿™ä¸€ç‚¹ã€‚
- en: Because we don't want to beã€‚ this operation should not be part of ourã€‚Graadient
    trackingrek graphã€‚ So this should not be part of the computational graphã€‚ So we
    need to wrap this in a width torch dotã€‚ no gr statementã€‚And then one more thing
    that we should also knowã€‚ And I also talked about this alreadyï¼Œ we must empty
    or  zero the gradients againã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬ä¸å¸Œæœ›è¿™ä¸ªæ“ä½œæˆä¸ºæˆ‘ä»¬æ¢¯åº¦è¿½è¸ªå›¾çš„ä¸€éƒ¨åˆ†ã€‚ æ‰€ä»¥è¿™ä¸åº”æˆä¸ºè®¡ç®—å›¾çš„ä¸€éƒ¨åˆ†ã€‚ æˆ‘ä»¬éœ€è¦ç”¨ torch dotã€‚no gr è¯­å¥å°†å…¶åŒ…è£…ã€‚ è¿˜æœ‰ä¸€ä»¶æˆ‘ä»¬ä¹Ÿåº”è¯¥çŸ¥é“çš„äº‹æƒ…ã€‚
    æˆ‘ä¹Ÿå·²ç»è°ˆåˆ°è¿‡ï¼Œæˆ‘ä»¬å¿…é¡»å†æ¬¡æ¸…ç©ºæˆ–ç½®é›¶æ¢¯åº¦ã€‚
- en: because whenever we call backwardï¼Œ it will write ourã€‚Grads and accumulate them
    in the W dot Gr attributeã€‚ So before the next iterationã€‚ we want to make sure
    that our gradients are 0 againã€‚ So we can say w times gra times0 underscoreã€‚ So
    this will modify it in placeã€‚And nowï¼Œ we are doneã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæ¯å½“æˆ‘ä»¬è°ƒç”¨ backward æ—¶ï¼Œå®ƒä¼šå†™å…¥æˆ‘ä»¬çš„ Gradsï¼Œå¹¶å°†å…¶ç´¯ç§¯åœ¨ W dot Gr å±æ€§ä¸­ã€‚ æ‰€ä»¥åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹å‰ï¼Œæˆ‘ä»¬è¦ç¡®ä¿æˆ‘ä»¬çš„æ¢¯åº¦å†æ¬¡ä¸º
    0ã€‚ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´ w times gra times 0 underscoreã€‚ è¿™æ ·å°†å°±åœ°ä¿®æ”¹å®ƒã€‚ ç°åœ¨ï¼Œæˆ‘ä»¬å®Œæˆäº†ã€‚
- en: And now let's run this and see if this is workingã€‚And W is not definedã€‚ Ohï¼Œ
    yeahï¼Œ Coï¼Œ Of courseã€‚ this is now W dot graã€‚And let's try this againã€‚ and now it's
    workingã€‚And now we also see that it will increase our Wï¼Œ and it will decrease
    our lossã€‚ And here we said we had 20 iterationsã€‚But it's not correctï¼Œ Not completely
    correctã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æœ‰æ•ˆã€‚ W è¿˜æœªå®šä¹‰ã€‚ å“¦ï¼Œæ˜¯çš„ï¼Œå½“ç„¶ã€‚ ç°åœ¨æ˜¯ W dot graã€‚ è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡ã€‚ ç°åœ¨å®ƒå·¥ä½œæ­£å¸¸ã€‚ ç°åœ¨æˆ‘ä»¬ä¹Ÿçœ‹åˆ°å®ƒä¼šå¢åŠ æˆ‘ä»¬çš„
    Wï¼Œå¹¶ä¸”ä¼šå‡å°‘æˆ‘ä»¬çš„æŸå¤±ã€‚ è¿™é‡Œæˆ‘ä»¬è¯´æˆ‘ä»¬æœ‰ 20 æ¬¡è¿­ä»£ã€‚ ä½†è¿™å¹¶ä¸å®Œå…¨æ­£ç¡®ã€‚
- en: And this is because theã€‚Backward or the back propagation is not as exact as
    the numerical gradient computationã€‚ So let's try some more iterations hereã€‚ Let's
    say we want 100 iterations and print our update every 10th stepã€‚ So let's try
    this againã€‚And now we see after the training is doneã€‚ we have the correct correct
    predictionã€‚Soï¼Œ yeahï¼Œ that's it for this videoã€‚ And in the next videoã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºåå‘ä¼ æ’­å¹¶ä¸åƒæ•°å€¼æ¢¯åº¦è®¡ç®—é‚£æ ·ç²¾ç¡®ã€‚ æ‰€ä»¥è®©æˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•æ›´å¤šçš„è¿­ä»£ã€‚ å‡è®¾æˆ‘ä»¬æƒ³è¦ 100 æ¬¡è¿­ä»£ï¼Œå¹¶åœ¨æ¯ 10 æ­¥æ—¶æ‰“å°æˆ‘ä»¬çš„æ›´æ–°ã€‚ æ‰€ä»¥è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡ã€‚
    ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬æœ‰äº†æ­£ç¡®çš„é¢„æµ‹ã€‚ æ‰€ä»¥ï¼Œæ²¡é”™ï¼Œè¿™å°±æ˜¯æœ¬è§†é¢‘çš„å†…å®¹ã€‚ åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ã€‚
- en: we will continue here and replace the manually computed loss and weight updates
    with Pytorch loss and optimizer classesã€‚So if you like this videoï¼Œ please subscribe
    to the channel and see you next timeï¼Œ byeã€‚![](img/49d809dfcdcd868a09e2ad42202f36fc_9.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨è¿™é‡Œç»§ç»­ï¼Œå°†æ‰‹åŠ¨è®¡ç®—çš„æŸå¤±å’Œæƒé‡æ›´æ–°æ›¿æ¢ä¸º Pytorch çš„æŸå¤±å’Œä¼˜åŒ–å™¨ç±»ã€‚ å¦‚æœä½ å–œæ¬¢è¿™ä¸ªè§†é¢‘ï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œä¸‹æ¬¡è§ï¼Œå†è§ï¼
