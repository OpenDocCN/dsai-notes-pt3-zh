- en: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P2：L2- PyTorch 张量简介
    - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P2：L2- PyTorch 张量简介
    - ShowMeAI - BV19L4y1t7tu
- en: Welcome。In this video， we're going to do a deep dive on pieytorrchtensors。In
    a Pytorch deep learning model。All of your data inputs， outputs， learning weights。
    it is only to be expressed as tensors， multidisional arrays that can contain floating
    point。 integer or boolean data。In particular in this video。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎。在这个视频中，我们将深入探讨 PyTorch 张量。在 PyTorch 深度学习模型中，所有的数据输入、输出和学习权重仅以张量的形式表达，这是一种可以包含浮点数、整数或布尔数据的多维数组。特别是在这个视频中。
- en: we're going to go over some of the ways to create pie torch tensors。How to use
    tensors in mathematical and logical operations alone or with each other。Methods
    for copying tensors。How to move to GPU for hardware acceleration？
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论创建 PyTorch 张量的一些方法。如何在数学和逻辑操作中单独或相互使用张量。复制张量的方法。如何移动到 GPU 进行硬件加速？
- en: Manipulating tensor shapes and the pie torch nuy bridge。If you haven't already。
    I recommend going to the Pytorch examples Repo and downloading the interactive
    notebook that goes with this video。![](img/629ed54063cce12da521281bede5aad8_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 操作张量形状和 PyTorch 的 Nuy bridge。如果你还没有这样做，我建议去 PyTorch 示例库下载与这个视频配套的交互式笔记本。![](img/629ed54063cce12da521281bede5aad8_1.png)
- en: Okay， so in the first cell here， we'll import PyTtorrch。 we're also going to
    import Python's math module to use some constant status has。First thing we're
    going to go over is creating tensors。 so here we have the very simplest way to
    create a tensor， the torchdot empty call。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在这里的第一个单元中，我们将导入 PyTorch。我们还将导入 Python 的 math 模块以使用一些常量。我们将要讨论的第一件事是创建张量。所以这里有创建张量的最简单方法，torch.empty
    调用。
- en: The torch module has multiple factory methods that will let you create tensors
    with and without initial values and whatever data type you need。 this is the most
    basic way to allocate a tensorrch do empty here it's going to create a three by
    four tensor。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Torch 模块有多个工厂方法，可以让你以有或没有初始值以及你需要的任何数据类型创建张量。这是分配张量的最基本方式，torch.empty 这里将创建一个三行四列的张量。
- en: And we can see。That the object itself。Is of type torch dot tensor。Now when you
    run this cell you may see random looking values in the output that's because Torchta
    empty just allocates memory and does not write any values to it。 so whatever happened
    to be memory at the time you allocated this tensor is what you're going to see
    here。One quick note about tensors and their dimensions and terminology。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对象本身是类型 torch.tensor。当你运行这个单元时，你可能会看到输出中出现随机的值，这是因为 torch.empty 仅分配内存，而不写入任何值。因此，在你分配这个张量时，内存中存在的内容就是你在这里看到的。关于张量及其维度和术语的一点简要说明。
- en: sometimes when we have one dimensional tensor， we will call it a vector because
    it's just an ordered tuple of dimensions。A of coordinates。Likewise， a two dimensional
    tensor is often referred to as a matrix。 and anything larger well always call
    a tensor。Now， more often than not。 you'll want to initialize your tensor with
    some value。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有时当我们有一维张量时，我们会称之为向量，因为它只是一个有序的维度元组。二维张量通常被称为矩阵，而任何更大的张量我们都称为张量。现在，更常见的是，你会希望用一些值来初始化你的张量。
- en: Common cases are all zeros are all ones or random values。 And the torch module
    provides factory methods for all of these。 So here if we run the cell。You get
    the things that you might expect in the method names。 you get a two by three tensor
    full of zeros， a  two by three tensor full of ones。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 常见情况是全零、全一或随机值。Torch 模块为所有这些情况提供了工厂方法。所以在这里如果我们运行这个单元，你会得到你在方法名称中可能期望的结果，一个充满零的二行三列张量，一个充满一的二行三列张量。
- en: and then tensor full of random values between 0 and 1。 Now speaking of the random
    tensor。 you might have spotted the call to torchdo manual seed rip before instantiating
    that tensor。So what's that about now， initializing tensors such as your model
    learning weights with random values is very common。 but often you will want your
    results to be reproducible。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是充满 0 到 1 之间随机值的张量。现在说到随机张量。你可能已经注意到在实例化那个张量之前调用了 torch.manual_seed。这是怎么回事呢？现在，用随机值初始化张量，例如你的模型学习权重是很常见的，但通常你会希望你的结果是可重复的。
- en: especially if you're working in a research setting。 So Ptorrch gives you a tool
    for doing it。 the manual seed call。 anytime you call manual seed with a particular
    integer seed you will reinitialize your pseudorandom number generators and get
    the same results again when you call them。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是如果你在研究环境中工作。因此Ptorrch为你提供了一个工具来做到这一点。手动种子调用。每当你使用特定的整数种子调用手动种子时，你将重新初始化你的伪随机数生成器，并在再次调用时获得相同的结果。
- en: So here in the following cell as an example。We call manual seed， we call Torchdot
    Rand。 we get some values out， we call Torchdot Rand again and get some values
    out。Then when we call a manual seed again and do those two torchdot random calls。
    we'll see both times they yield the same values。 So this is how you make sure
    that identical computations that depend on random numbers will provide identical
    results if you need that reproducibility。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在下面的单元格中作为一个例子。我们调用手动种子，我们调用Torchdot Rand。我们输出一些值，然后再次调用Torchdot Rand并输出一些值。接着当我们再次调用手动种子并进行这两个torchdot随机调用时。我们会发现两次的结果都是相同的。因此，这就是确保依赖随机数的相同计算能提供相同结果的方式，如果你需要这种可重复性。
- en: So often， when you are performing operations on two or more tensors。 they will
    need to be of the same shape。 that is having the same number of dimensions and
    the same number of cells in each dimension or the same extent in each dimension。All
    the factory methods I've shown you on the torch module so far have corresponding
    methods。Appended with underscore like。And when you pass in a tensor as an argument
    to empty like or zeros lir。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常当你对两个或多个张量执行操作时。它们需要具有相同的形状。即在每个维度上具有相同数量的维度和相同数量的单元格，或者在每个维度上的相同范围。我迄今为止在torch模块上展示的所有工厂方法都有相应的方法。后面加上下划线。当你将张量作为参数传递给empty或zeros时。
- en: with these other methods， you will get out a tensor initialized as you specify。
    but of the same shape as a tensor you passed in as an argument。So here we've run
    the cell and we can see that our initial tensor was2 by2 by3 and even though we
    specified no shape for the O tensors。 they all will also come out two by two by3
    and initialized in the way you'd expect。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些其他方法，你将得到一个根据你的指定初始化的张量。但它的形状与作为参数传入的张量相同。因此在这里我们运行了单元格，我们可以看到我们的初始张量是2
    by 2 by 3，尽管我们没有为O张量指定形状。它们也都会以你所期望的方式输出为2 by 2 by 3。
- en: When we want to find out the shape of the tensor， we can always query its shape
    property。And this will give us back a list of the dimensions and their extents。Now
    the last way to create a tensor that we're going to cover is to specify its data
    directly from a Ptorrch collection。 So here if you look at these examples， we
    have a nested array and we have a tuple and we have a tuple that contains a tuple
    in a list。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要找出张量的形状时，我们总是可以查询其形状属性。这将给我们返回一个维度及其范围的列表。现在我们将要讨论的创建张量的最后一种方法是直接从Ptorrch集合中指定其数据。因此在这里，如果你看看这些例子，我们有一个嵌套数组和一个元组，以及一个包含元组的元组在列表中。
- en: And when we call Torchdot tensor with any of these collections as an argument。
    we get back a new tensor that is initialized with the data we specified。 So here
    you can see in all three cases， we've gotten back a tensor that is of the shape
    and containing the data that we'd expect。 So torchdot tensor creates a copy of
    the data。 This is important to know the underlying memory representation of a
    Python list is not the same as the underlying memory representation of a tensor。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用Torchdot张量并将这些集合作为参数时。我们会得到一个用我们指定的数据初始化的新张量。因此在这里你可以看到在所有三个案例中，我们得到了一个形状和包含预期数据的张量。因此torchdot张量创建了数据的副本。知道Python列表的底层内存表示与张量的底层内存表示是不同的是很重要的。
- en: So we always copy that data when we're creating a new tensor and initializing
    it with data in this way。Now I mentioned earlier that tensors can have floating
    point or integer Boolean underlying data types。 the simplest way to specify your
    data type is to do it at creation time。So here in this cell I'm creating an int16
    and a float 64 and you'll see A when I print it out is a set of ones represented
    as 16 bit integers。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在以这种方式创建新的张量并用数据初始化时，我们总是复制那些数据。现在我之前提到过，张量可以有浮点或整数布尔底层数据类型。指定数据类型的最简单方法是在创建时进行。因此在这个单元格中，我正在创建一个int16和一个float
    64，你会看到当我打印出来时A是由16位整数表示的一组1。
- en: and you can see none of the ones have that little decimal point after them。
    which is Python's subtle signal that we're dealing with an int rather than the
    float。We also could see that because we overro the default data type。 the default
    is a 32 bit floating point。 When we print the tensor。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到没有任何一个后面有小数点，这表明我们处理的是整数而非浮点数。我们还可以看到，因为我们覆盖了默认数据类型，默认是 32 位浮点数。当我们打印张量时。
- en: Ptorrch helpful reports to us that this is the underlying data type of that
    tensor。Likewise。 when we do a 64 bit float。The other way to change the data type
    of a tensor or to really move it to a new tensor with your required data type。Is
    with the two method。 So here I'm calling B do2 and saying I would rather have
    this data as 32 bit integers。 And if you look closely at the values of B and C
    when they're printed out。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 有用的报告告诉我们这是该张量的基础数据类型。类似地，当我们使用 64 位浮点数时。改变张量的数据类型或将其真正移动到所需数据类型的新张量的另一种方法是使用这两种方法。因此，这里我调用
    B.do2，并表示我希望将此数据作为 32 位整数。仔细查看打印出来的 B 和 C 的值。
- en: the values of C are just the values of B truncated to make them integers。 So
    it's a float to conversion there。The other thing you may have noticed here is
    that here specified the dimensions of the tensor as a tuple。 canonically Pytorch
    expects a tuple for a tensor's dimensions。 but when the dimensions are the first
    argument of a method。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: C 的值只是 B 的值被截断为整数。因此，这是一个浮点到整数的转换。你可能注意到的另一件事是，这里指定了张量的维度为一个元组。典型情况下，PyTorch
    对张量的维度期望一个元组，但当维度是方法的第一个参数时。
- en: it lets us cheat a little and just put in a series of integers。 but here to
    make the code a little more readable， I separate up the tensor's shape as a tuple。The
    data types you can use are Boolean five types of eventss and four types of float。Let's
    look at basic arithmetic first and how we can make tensors interact with scalrs
    now if we run this cell。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们有点侥幸，只需输入一系列整数。但为了让代码更易读，我将张量的形状分离为一个元组。你可以使用的数据类型有布尔值、五种事件类型和四种浮点数类型。首先让我们看看基本的算术运算，以及如何让张量与标量交互，现在如果我们运行这个单元。
- en: See， let's look at the first line here， we're going to create a tensor full
    of zeros。 and we're going to add the integer1 to it。 So what does that mean to
    add an integer to a tensor。 Well here。We're going to be doing this operation element
    wise over every element of the tensor。 so every zero in that tensor should have
    a one added to it。 And if we look at our output。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 看，看这里的第一行，我们将创建一个满是零的张量，并将整数 1 加到其中。这意味着将一个整数加到张量上有什么含义。好吧，在这里。我们将在张量的每个元素上执行此操作，因此该张量中的每个零都应该加上一个
    1。如果我们查看输出。
- en: that is in fact， what we see。Likewise， with multiplication， division subtraction，
    exponentiation。 with integer or floating point powers， I'll also note that because
    the binary operation between a tensor and a scalar puts out a tensor of the same
    shape you had originally。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是我们所看到的。类似地，对于乘法、除法、减法和指数运算。对于整数或浮点幂，我还要指出，因为张量和标量之间的二元运算会输出与最初相同形状的张量。
- en: you can chain together these arithmetic operations intuitively。 and you can
    see that in the line where we created the threes。NowDo these same arithmetic operations
    with two tensors behaves sort of into a like you'd expect。 So we take our our
    twos。 Our little two by two tens are full of two floating point2s。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直观地将这些算术运算链接在一起。你可以在我们创建三的那一行看到这一点。现在，用两个张量执行相同的算术运算行为是你所期望的那样。因此，我们取出我们的二。我们的小
    2x2 张量充满了两个浮点数 2。
- en: We' going to're going to use the exponiation operator。 Now we're going to specify
    the powers 1，2，3。 and 4。 And so here the。Mathematical operation is going to be
    done。Element wise between corresponding elements of each tensor because they're
    of the same shape。And so if we。Run this cell， you can see that in fact， powers
    of two are in the first tensor。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用指数运算符。现在我们将指定幂 1、2、3 和 4。因此，数学运算将在每个张量的对应元素之间逐元素执行，因为它们具有相同的形状。因此，如果我们运行这个单元，你可以看到实际上，二的幂在第一个张量中。
- en: We've added two tensors of ones and fours to get fives， and if we multiply threes
    and fours。 we get12s。A key thing here is that all the tensors that we've shown
    you in these examples of these tensor binary operations are of identical shape。So
    we can see when we run this cell that when we try to do an operation with two
    tensors of different shape。 we get a runtime error， even though these two tensors
    have the exact same number of cells。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了两个全是1和4的张量，得到了5；如果我们将3和4相乘，就得到了12。这里一个关键点是，我们在这些张量二元操作的示例中展示的所有张量都是相同形状的。因此，当我们运行这个单元时，发现当我们尝试对两个不同形状的张量进行操作时，会出现运行时错误，尽管这两个张量的单元数量完全相同。
- en: there's no natural way to map between the two。So in the general case。 your tenors
    will have to be of the same shape， there is one important and useful exception
    to that。 and that is what we call broadcasting。Here is an example， so I created
    a random tensor。 two rows by four columns， and I'm multiplying it here you can
    see by a tensor with one row and four columns and we actually get out something
    like what we'd expect so see our random output in the first print statement in
    our second print statement shows all of that doubled。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 之间没有自然的映射方式。因此在一般情况下，你的张量必须具有相同的形状，但有一个重要且有用的例外，那就是我们所称的广播。这里有一个例子，我创建了一个随机张量，两个行四列，并且在这里你可以看到我与一个一行四列的张量相乘，实际上我们得到了我们预期的结果，所以在第一个打印语句中看到我们的随机输出，而在第二个打印语句中显示了所有结果翻倍。
- en: Well how do we do this， how did we multiply two tensors of different shapes
    and get an intuitive result？
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们该怎么做呢？我们如何将两个不同形状的张量相乘并得到直观的结果？
- en: So broadcasting is a way to perform an operation between tensors that have specific
    similarities in their shapes。So here in the cell preus， the one row four column
    tensor was multiplied element wise by each of the two four column rows of the
    random tensor。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 广播是一种在具有特定相似形状的张量之间执行操作的方法。在这里的单元中，那个一行四列的张量与随机张量的每个两行四列逐元素相乘。
- en: So this is an important operation in deep learning。One common example is using
    batches of inputs。 so your piytorrch machine learning model will in the general
    case not expect a single input for either training or inference。 but we will expect
    a batch of inputs so here applying an operation to each instance in the batch
    separately but returning a tensor of the same shape is what you'd expect so here
    we have in our random tensor we had two rows a random values we multiplied by
    one row of twos doing each row individually and that's akin to the batch operation
    that we're performing some operation on each segment of a tensor separately。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习中的一个重要操作。一个常见的例子是使用输入批次。因此，你的PyTorch机器学习模型在一般情况下不会期望单一输入进行训练或推理，而是会期望一个输入批次。因此，在这里对批次中的每个实例分别应用操作，但返回一个相同形状的张量是你所期望的。在我们的随机张量中，有两行随机值，我们与一行2相乘，逐行操作，这类似于我们对张量每个部分分别执行操作的批处理操作。
- en: There are rules for broadcasting。 The first one is that no empty tensors。 so
    every tensor must have at least one dimension， and then there are some rules for
    the relationship between the dimensions and extents of the two tensors that you
    want to perform an operation on。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 广播有一些规则。第一个是不能有空张量，因此每个张量必须至少有一个维度，然后在你想对其执行操作的两个张量的维度和范围之间有一些规则。
- en: So when we compare the dimension sizes of the two tensors going from the last
    to the first。 have to have either each dimension must be equal， or one of the
    dimensions must be of size 1。 or the dimension doesn't exist in one of the tensors。
    here are some examples that show the rules that I just described。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们从最后一个维度到第一个维度比较两个张量的维度大小时，必须要么每个维度相等，要么其中一个维度的大小为1，或者在一个张量中该维度不存在。以下是一些展示我刚刚描述的规则的例子。
- en: It's probably easier to look at these than to try to reason them out so we start
    with a tensor full of ones。 it's a three dimensional tensor with four layers，
    three rows and two columns。And we will multiply that by a random 3 by two tensor。
    If we look at the output of that。 we can see that we multiplied our random tensor
    by each of the four layers of our original tensor full of ones。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看这些可能比推理更容易，因此我们从一个充满1的张量开始。它是一个三维张量，有四层，三行和两列。我们将其与一个随机的3行2列张量相乘。如果我们查看输出，我们可以看到我们将随机张量与原始的充满1的张量的四层逐一相乘。
- en: And sos what we say， the operation is broadcast over those layers over that
    first dimension。Likewise。 in the following line here we multiply a times another
    random tensor to get C。 This time we're doing a3 by one tensor， and so what does
    that give us。 this follows the rules because in the last dimension one of the
    dimensions is one and the second dimension they match and then the first dimension
    is absent in one of the tensors。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们说，操作在这些层上广播到第一个维度。同样，在接下来的这一行中，我们将A乘以另一个随机张量以得到C。这次我们在做一个3x1的张量，所以这会给我们带来什么？这遵循规则，因为在最后一个维度中，一个维度是1，第二个维度匹配，而在其中一个张量中，第一个维度是缺失的。
- en: The output there。Looks like this。 So if we think of our random tensor that went
    into making C as a three element column vector。 which you can see in the output
    here， when we multiply it by a bunch of ones is that every three element column。In
    our output， tensor is the same， so we broadcast this operation over every three
    element column in our tensor。Likewise， in the final example， multiplying a random
    one by two tensor times are a tensor full of ones。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样。如果我们把进入C的随机张量看作是一个三元素列向量，你可以在输出中看到，当我们将其乘以一组一时，每个三元素列在我们的输出张量中都是相同的，因此我们对张量中的每个三元素列进行了广播这个操作。同样，在最后一个例子中，将一个随机的二张量与一个充满一的张量相乘。
- en: That does something akin to the previous time， except now。 instead of every
    three element column having the operation performed on it。 now every two element
    row has the operation performed on it。Now there's a Pytorch documentation note
    on this topic of broadcasting。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这与上次的情况类似，但现在每个三元素列上执行的操作变成了每个二元素行上执行的操作。现在在这个广播主题上有一个Pytorch文档的说明。
- en: and I urge you to read it if you are interested in more details。Now to give
    you an idea of some operations that will break the rules and not work。 all these
    lines should give you a runtime error so in the first case trying to create B。
    we always compare the dimensions last to first and B's last dimension is three
    or has an extent of three。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对更多细节感兴趣，我强烈建议你阅读它。现在，我想给你一些会打破规则并且无法工作的操作的例子。所有这些行应该会给你一个运行时错误，因此在第一种情况下，尝试创建B。我们总是从最后到第一比较维度，B的最后维度是三或者有一个三的范围。
- en: A's has an extent of two， those don't match we can't broadcast the multiplication
    here。Like what I would see， it the last two dimensions are two and three instead
    of three and two。 they're different that won't work in the final example。We try
    to create an empty tensor and broadcast an operation over。One with dimensions。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: A的范围是2，这些不匹配，我们无法在这里广播乘法。就像我所看到的，最后两个维度是2和3而不是3和2，它们是不同的，这样就不行了。在最后一个例子中，我们尝试创建一个空张量并在其上广播一个操作，维度为。
- en: that doesn't work， we can't do it with an empty tensor。Now。 Piytorrch tensors
    have over 300 mathematical operations that you can perform on them and here are
    a few examples from the major categories。In the first section， we just have some
    con functions that you might use for manipulating numbers。 absolute value ceiling
    floor and a clamp， which sets min and max values for your tensor。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这不行，我们不能用空张量来进行操作。现在，Pytorch张量有超过300种数学操作可以执行，这里是一些主要类别的例子。在第一部分，我们有一些你可能会用来操作数字的函数，如绝对值、向上取整、向下取整和一个设置张量最小值和最大值的钳制函数。
- en: And all those will act on every element of the tensor， Likewise， for triometric
    functions。 So here I've created a tensor full of angles， and I want to get the
    sign of those angles and then get the inverse of that sign。 and you can see from
    running the cell。That we get back what we expect。We can do bitwise logical operations
    on either boolean or integer tensors here I've got two integer tensors and I'm
    performing a bitwise exor on them。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些操作都会作用于张量的每一个元素，三角函数也是如此。因此，我在这里创建了一个充满角度的张量，我想得到这些角度的正弦，然后得到那个正弦的反函数。你可以从运行单元格中看到，我们得到了预期的结果。我们可以对布尔或整数张量进行按位逻辑操作，这里我有两个整数张量，并对它们执行按位异或操作。
- en: And we can see that it does exactly what you'd expect if you' were doing like
    a bit Y as x or in C。 for example。We can do comparisons of tensors as well。So
    we'll get a tensor where you specify some data we'll at tensor fold ones。 we'll
    test their quality。We can see because the tensor D， its first value was one。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这正是你所期望的，如果你在做类似的按位与或C。例如，我们也可以进行张量的比较。因此，我们将得到一个张量，你可以指定一些数据，我们将是一个充满一的张量。我们会测试它们的相等性。我们可以看到，因为张量D的第一个值是1。
- en: But all the rest were different， we can see we had a true and three falses there。
    which was exactly what we'd expect。There are also a number of reduction operations
    that you can perform on a single tensor。So for example， here we can take the maximum
    of a tensor no matter how large the tensor。 this is going to give us back a single
    value。Our tensor with a single value。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但是其余的都不同，我们可以看到那里有一个真值和三个假值，这正是我们所期待的。你还可以对单个张量执行多种归约操作。例如，在这里我们可以获取一个张量的最大值，无论这个张量有多大。这将返回一个单一值。我们的张量中有一个单一值。
- en: if you want to extract that value from that one element output tensor， we use
    the dot item call。And if you look at the output from these reduction knos。First
    we get back a tensor with our value in it， and then after the item call。 we've
    actually extracted the value。You can also do means standard deviations。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从那个单元素输出张量中提取那个值，我们使用点项调用。如果你查看这些归约节点的输出，首先我们得到一个包含我们值的张量，然后在调用项之后，我们实际上提取了这个值。你还可以计算均值和标准差。
- en: there are convenience methods for performing arithmetic operations。 including
    all of the elements of the tensor， so here with the dot Progue call we're taking
    the product of all numbers in the tensor。And we can also， as another example，
    get all the unique elements of a tensor。all these behave more or less as you'd
    expect。Of course。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有便利的方法来执行算术运算，包括张量的所有元素。因此，在这里通过点积调用，我们正在计算张量中所有数字的乘积。我们还可以作为另一个例子，获取张量中的所有唯一元素。这些行为或多或少都是你所期待的。当然。
- en: linear algebra is at the heart of a lot of what we do in deep learning。 so there
    are a lot of vector and matrix and linear algebra operations。 so for example I'll
    create two vectors that correspond to X and Y unit vectors I'll create two matrices。One
    of which is just random and one of which is going to be three times the identity
    matrix。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数是深度学习中很多工作的核心。因此有很多向量、矩阵和线性代数操作。例如，我将创建两个对应于 X 和 Y 单位向量的向量，并将创建两个矩阵。其中一个是随机的，另一个将是三倍的单位矩阵。
- en: We can do some things with them。Torch do cross gets a cross product between
    the two vectors。 So if we cross the Y unit vector with the X unit vector in that
    order。 we should expect back the negative Z unit vector， which is， in fact what
    we got。 we can do a matrix multiplication of our two matrices。 So we have our
    random matrix。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用它们做一些事情。Torch 在两个向量之间计算叉积。所以如果我们按照顺序将 Y 单位向量与 X 单位向量进行叉乘，我们应该得到负的 Z 单位向量，这实际上就是我们得到的。我们可以对我们的两个矩阵进行矩阵乘法。所以我们有了随机矩阵。
- en: And then when we multiply it by three times the identity matrix。 we should expect
    that we get back a matrix that is about three times the value of our input。 and
    in fact we see that。 And you can do more advanced complex operations like singular
    value decomposition as well。And so this is just a very small sampling of the 300
    odd mathematical and logical operations associated with P piyTch tensors。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后当我们将其乘以三倍的单位矩阵时，我们应该期待得到一个大约是我们输入值三倍的矩阵，实际上我们确实看到了这一点。你还可以进行更复杂的高级操作，比如奇异值分解。因此，这只是与
    Pytorch 张量相关的 300 多个数学和逻辑操作中的一小部分。
- en: I urge you look at the documentation to understand the full inventory。Now。 sometimes
    if you're doing a computation with two tensors。 you'll say they're intermediate
    values of some kind。 you may not need those intermediate values when you're done。
    it can be a nice optimization to be able to recycle that memory。 If you need a
    tensor of the same size and data type as the intermediate one you going throw
    away。 So as an example of that here again， I'm going to create a tensor full of
    angles。 I'm going to get signs from them。 And you can see when we run this cell
    and check the output we have a。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你查看文档以了解完整的清单。现在，有时候如果你正在进行两个张量的计算，你会说它们是某种中间值。当你完成时，你可能不需要那些中间值。能够回收内存是一个不错的优化。如果你需要一个与将要丢弃的中间张量大小和数据类型相同的张量。作为这个例子的再次说明，我将创建一个充满角度的张量。我将从中获取正弦值。你可以看到当我们运行这个单元并检查输出时，我们得到了一个。
- en: that's our angles， we have our signs。Of those angles。And then if we look back
    here。 we can see a has not changed。 So here we see Torch dot sign gave us back
    a new tensor and left the old one in place。But because we're acting on a single
    tensor here， we could。 and if we don't need the input values。 we could just put
    the outputs in that tensor itself。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们的角度，我们有这些角度的正弦。如果我们回头看一下，我们可以看到a没有改变。因此，我们看到Torch的sign给我们返回了一个新张量，并保留了旧张量在原地。但是因为我们在这里作用于一个单一张量，如果我们不需要输入值，我们可以直接将输出放在那个张量本身中。
- en: The underscore on a method like sign means that you are altering the tensor
    in place。 that tensor you're putting in， it's an argument。 So now if we do the
    exact same thing。 B is a tensor containing the same angles that A did。 and we
    take do the same operation。 We do a sign on it。We can see there's our initial
    angles， there's the output of that sign operation。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 像sign这样的带下划线的方法意味着你正在就地修改张量。你放入的那个张量，它是一个参数。所以现在如果我们做完全相同的事情，B是一个包含与A相同角度的张量。我们进行同样的操作，对它进行正弦计算。我们可以看到那是我们最初的角度，还有该正弦操作的输出。
- en: but this time B has changed， we told it that we wanted to use B's memory for
    this and it was of a compatible data type and size。 and so B was altered in place。Now
    if you want to do this with binary arithmetic operations。There are functions for
    you that that behave similarly to the the binary pitorrch operators。 So here we'll
    create two by two matrices， A and B。 We can look at their values before。 Now。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次B已经改变，我们告诉它我们想使用B的内存，并且它的数据类型和大小是兼容的。因此B在原地被修改。如果你想对二元算术运算执行此操作，有一些函数的行为与二元pitorch操作符类似。因此，我们将创建两个2x2矩阵，A和B。我们可以查看它们的值，然后。
- en: we'll call the。In place addition method。And you can see here that now a has
    changed。Methods that cover a binary operation。 the calling tensor will be the
    one that is changed in place。And so likewise， here， when we do the same for B，
    if we square the random contents of B。 but do it with the mo underscore， we'll
    get back exactly what we expect。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为就地加法方法。你可以看到这里a已经改变。覆盖二元操作的方法。调用的张量将是被就地改变的那个。因此，同样地，当我们对B进行相同操作时，如果我们对B的随机内容进行平方，但使用mo下划线，我们将得到我们预期的结果。
- en: Note that these in place arithmetic functions on are methods of the torch dot
    tensor objects not attached to torch module like a lot of other functions。 the
    calling tensor， as I said， is the one that gets changed in place。There's another
    option for placing the result of a computation in an existing already allocated
    tensor。Many of the methods and functions we've seen so far， including the creation
    methods for tensors。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些就地算术函数是torch张量对象的方法，而不是像许多其他函数一样附加到torch模块。调用的张量，正如我所说，是那个在原地被改变的张量。还有另一种选择，可以将计算结果放入一个已经分配的现有张量中。到目前为止，我们看到的许多方法和函数，包括张量的创建方法。
- en: have an out argument that lets you specify a tensor to receive the output。 If
    the out tensor is the same shape as the output and the correct data type that
    matches the output data type。 this can happen without a new memory allocation。
    So if we run this cell。We'll create tensors A and B， which are two by two random
    matrices and then C。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 都有一个out参数，可以让你指定一个张量来接收输出。如果out张量与输出具有相同的形状，并且数据类型匹配输出的数据类型，则可以在不分配新内存的情况下发生这种情况。因此，如果我们运行这个单元格。我们将创建张量A和B，它们是两个2x2的随机矩阵，然后是C。
- en: which is a 2 by2 matrix full of zeros， and we'll use the Python ID call to get
    the ID of that object。We'll print it out， we will do a matrix multiplication between
    A and B。 and we'll specify C as that optional out argument。And then if we look
    at our next print of C。 we'll see that it's changed just no longer zeros。So C
    was the same size as both A and B with the same data type 32 bit floating point
    pi tors default。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个充满零的2x2矩阵，我们将使用Python ID调用来获取该对象的ID。我们将打印出来，我们将对A和B进行矩阵乘法，并将C指定为那个可选的输出参数。然后，如果我们查看C的下一个打印结果，会发现它已不再是零。所以C与A和B的大小相同，数据类型都是32位浮点数的默认类型。
- en: And so when we do that multiplication， specify C to receive the output， we see
    that it does。We also assigned the output as a return value to another label D。
    and if we look we'll see that C and D are actually the same object， this assertion
    didn't fire。 so C and D are the same object， and we can also see by an assertion
    that C's ID did not change。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们进行乘法时，指定C来接收输出，我们看到它确实如此。我们还将输出赋值为另一个标签D的返回值。如果我们查看，将会发现C和D实际上是同一个对象，这个断言没有触发。所以C和D是同一个对象，我们还可以通过一个断言看到C的ID没有改变。
- en: we're dealing with the same object in memory。So I just wanted to to give you
    a tangible example of how all that works and it works for creation calls too。
    so when we call Torchdot Rand with an optional out argument， again。 as long as
    the shape and data type or what we want， we will get back a tensor in that same
    object。So we've seen how to create and manipulate tensors。 But what about copying
    them， Now。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理的是内存中的同一个对象。因此，我只是想给你一个具体的例子，说明这一切是如何运作的，它也适用于创建调用。所以当我们调用Torchdot Rand时，带有一个可选的输出参数，只要形状和数据类型符合要求，我们就会得到一个同一对象中的张量。所以我们已经看到如何创建和操作张量。那么，关于复制它们呢？
- en: tensors are like any object in Python， meaning that if you assign it to a variable。
    that variable is just a label for the object， you're not making a copy of the
    object。 create a tensor full of ones in the cell， we'll assign it to A say B equals
    a。 And if look when we change a value of a and print B， the the value within B
    has changed as well。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 张量就像Python中的任何对象，这意味着如果你将其赋值给一个变量，那么该变量只是对象的标签，而不是创建对象的副本。创建一个满是一的张量，我们将其赋值给A，假设B等于A。如果你注意到，当我们改变A的值并打印B时，B中的值也发生了变化。
- en: So these are just two labels for the exact same object。 what if you need a separate
    copy of the data。It may happen if you're building a complex model with multiple
    computation paths。 and so you want to have separate copies of the input to pass
    to different portions of the model。So in this case， you would use the clone method，
    so we're going to do something very similar here。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是同一个对象的两个标签。如果你需要数据的单独副本会怎样？这可能发生在你构建一个具有多个计算路径的复杂模型时，因此你想要对输入进行单独的副本，以传递给模型的不同部分。在这种情况下，你会使用克隆方法，所以我们将在这里做类似的事情。
- en: we're going to create two by two meters full of ones。 we're going to say B is
    in same Josea now。 but now we're going to clone A instead of just doing the assignment。We
    can verify that these are。 in fact different objects in memory with the assertion。
    and we can verify that the contents are the same with a Torrsch EQ call。And when
    we change A， again。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个满是一的二维矩阵。我们将说B现在在同一个Josea中。但现在我们要克隆A，而不是仅仅进行赋值。我们可以通过断言验证它们实际上在内存中是不同的对象。我们还可以通过Torrsch
    EQ调用验证它们的内容是相同的。当我们再次改变A时。
- en: we can verify that B has not changed when we print it out。So there is one important
    thing to be aware of using clone。 which is that if your source tensor has autograd
    enableded， then so will the clone of that tensor。 We're going to cover this more
    deeply in the video on autograd。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证，当我们打印B时，它没有改变。因此，有一件重要的事情需要注意，使用克隆时，如果你的源张量启用了autograd，那么该张量的克隆也会启用。我们将在关于autograd的视频中更深入地讨论这个问题。
- en: But if you want a light version of the details， here it is。嗯。So as an example。
    let's see you you have a complex model with multiple computation paths in its
    forward method。 and the original tensor and its clone or its copy are going to
    contribute to the model's output。 then in order to enable model learning you want
    autograd turned on for both tensors。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你想要详细信息的简化版本，这里就是。嗯。因此作为一个例子，假设你有一个复杂模型，在其前向方法中有多个计算路径，原始张量及其克隆或副本将贡献于模型的输出。为了使模型学习，你希望这两个张量都启用autograd。
- en: If your source tensor has autograd enabled， which it generally will。 if it's
    a set of learning weights or it's derived from a computation involving the weights。Then
    everything else out of redden， it will know already and you'll get the result
    you want。On the other hand， perhaps you are doing a computation where neither
    the original tensor nor its clone need to track gradients。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的源张量启用了autograd，通常情况下是的。如果它是一组学习权重，或者它源于涉及权重的计算。那么其他地方的变化，它会提前知道，你将得到你想要的结果。另一方面，也许你正在进行一个计算，既不需要跟踪原始张量也不需要跟踪其克隆。
- en: In that case， as long as the source tensor has autograd turned off， you're good
    to go。There is。 of course， a third case， so imagine you're performing some computation
    in your model's forward function where gradients are turned on for everything
    by default。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只要源张量关闭了autograd，你就可以继续。还有当然，还有第三种情况，假设你在模型的前向函数中执行一些计算，其中所有内容默认情况下都开启了梯度跟踪。
- en: but you want to pull out some values midstream to generate metrics。And you want
    those to be separate from the data that's being acted on。 So in this case。 you
    wouldn't want the cloned copy of your source tensor to track gradients。Degrades
    performance and doesn't actually do anything for you in this example use case。So
    for this。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你想在中间提取一些值来生成指标。你希望这些与正在处理的数据分开。因此在这种情况下，你不希望源张量的克隆副本跟踪梯度。这会降低性能，并且在这个示例用例中实际上没有任何作用。所以为此。
- en: you can use the detaach method on the source tensor。So if we run。This we'll
    see is what we'll create our two by two tensor of random values。 We will set requires
    grad equals true。 So now every computation subsequent to a will have its history
    tracked。 So we know where it came from and can compute backward derivative itus。
    And that happens with clone。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对源张量使用detach方法。因此，如果我们运行。我们将看到这是我们创建的两个随机值的二乘二张量。我们将设置requires grad等于true。因此，现在每个后续计算都会跟踪其历史。我们知道它的来源，并可以计算反向导数。这是通过clone实现的。
- en: So here we clone B from a in we print B， we see that grad function equals clone
    backwards。 That's telling us B is tracking its history。 Now， instead。 if you look
    at the line where we create C。 we say a dot detached dot clone。And then when we
    print C。 we get the same data except that we don't get the history attached。If
    we print A。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们从A克隆B，当我们打印B时，看到grad函数等于clone backwards。这告诉我们B正在跟踪其历史。现在，如果你查看创建C的那一行，我们说a.dot
    detached.dot clone。然后当我们打印C时，我们得到相同的数据，只是没有附加历史。如果我们打印A。
- en: we'll see that that detach call did not actually alter A at all。 it basically
    just says do everything as if autogriide were turned off。One of the core advantages
    of Pytorch is hardware acceleration。 If you have a coa compatible Nvidia GP and
    the drivers installed。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会看到那个detach调用实际上没有改变A。它基本上只是说，做所有事情就像autograd被关闭一样。Pytorch的一个核心优势是硬件加速。如果你有兼容的Nvidia
    GPU和已安装的驱动程序。
- en: you can radically accelerate your performance for both training and inference。
    Everything we've done so far has been on CPU By default when you create a tensor。
    It's instantiated in CPU and Marine。So how do we move to the faster hardware？First
    things first。 we should check whether that faster hardware is available and we
    do that with Torch。kuudo。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以显著加速训练和推理的性能。到目前为止，我们所做的一切都是在CPU上。默认情况下，当你创建一个张量时，它是在CPU和内存中实例化的。那么我们如何移动到更快的硬件呢？首先，我们应该检查是否有更快的硬件可用，我们通过Torch.kuudo来做到这一点。
- en: is available。So if I run the cell。You see， it will tell us whether or not we
    have a GPU available on this device。Once you determine that one or more GPUs are
    available。 you need to put the data inplace the GPU can see it。Your CPU works
    on data that is lives in your machine's ramp。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可用。所以如果我运行这个单元，你会看到它会告诉我们这个设备上是否有GPU可用。一旦你确定一个或多个GPU可用，你需要将数据放在GPU可以看到的地方。你的CPU处理的是存储在机器RAM中的数据。
- en: your GPU also has dedicated memory attached to it。whichever device you want
    to perform your computation on。 you must move all the data needed for that operation
    to memory accessible by your target device。 Now we know that we have one or more
    GPUus available， we run。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你的GPU还附带专用内存。无论你希望在哪个设备上执行计算，都必须将该操作所需的所有数据移动到目标设备可以访问的内存中。现在我们知道我们有一个或多个GPU可用，我们运行。
- en: Now there are multiple ways to get your data onto your target device。 but the
    easiest way is at creation time， you can see here when we have a KUa GPU available。We
    will create a tensor with an optional argument that says device equals kuda。When
    we create a tensor that we want to move to a GPU。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有多种方法可以将数据传输到目标设备，但最简单的方法是在创建时。你可以在这里看到，当我们有可用的KUa GPU时，我们将创建一个带有可选参数的张量，该参数指示设备等于kuda。当我们创建一个想要移动到GPU的张量时。
- en: As we'll use the optional device argument on the factory with all the factory
    methods I showed you for creating tensors will take this device argument and here
    we're putting in the string kuta to say。 or we would like to move this tensor
    into memory accessible by the GPU when you print the tensor。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在工厂方法中使用可选的设备参数，所有我为你展示的创建张量的工厂方法都会接受这个设备参数，这里我们放入字符串kuta，表示我们希望将这个张量移动到GPU可访问的内存中，当你打印张量时。
- en: you'll see that it reports that the tensor is living on the GPU device。You can
    also query the number of GPUs and if there's more than one。 you can specify them
    by index with a colon after the Kuta string， so Kuda colon 0， Kuta colon 1。 eta，
    would be the strings you put in as your device argument。As a general engineering
    practice。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到它报告张量位于GPU设备上。你也可以查询GPU的数量，如果有多个，你可以通过在Kuda字符串后添加冒号来按索引指定它们，例如Kuda:0，Kuda:1。eta将是你作为设备参数输入的字符串。作为一般工程实践。
- en: it's generally not considered good to specify things with magic constant strings
    all over your code。 so a better practice is to at the beginning of computation。
    choose which device you wanted to do your computation on。And get a handle to that
    device and then pass that handle around here in the next cell。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常不建议在代码中到处使用魔法常量字符串，因此更好的做法是在计算开始时选择你希望在哪个设备上进行计算，并获取该设备的句柄，然后在下一个单元中传递该句柄。
- en: we have an example of that。Where my device， depending on whether or not we have
    a GPU available or not。 is either Torchdot device Kuta or Torchdot device CPU。
    Once we have the handle to that device。 we can pass that in as the optional argument
    to creating a tensor is shown in the last couple of lines there。Thus creating
    a tensor， what if you have an existing tensor。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个例子，其中我的设备取决于是否有可用的GPU，可能是Torch.device("cuda")或Torch.device("cpu")。一旦我们获得了该设备的句柄，我们可以将其作为可选参数传递给创建张量，如最后几行所示。因此创建张量，如果你有一个现有的张量。
- en: it's living in memory for one device and you want to move it to the other device。How
    do you go about that？Well， in this cell， we'll demonstrate you can create if you
    create a tensor in CPU。 for example， and you want to move it to GPU， you can either
    put in the string ka or k colon and 0 cota colon in 1。Or you could pass in a handle
    to a device that you have retrieved earlier。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 它在一个设备的内存中，而你想将其移动到另一个设备。你该如何操作？在这个单元中，我们将演示如何创建一个张量在CPU中，例如，如果你想将其移动到GPU，你可以输入字符串ka或k:0，k:1。或者你可以传递先前获取的设备的句柄。
- en: And you just pass it into the two method like so this is the same two method
    that lets you change the data type of a tensor if you wanted to change both the
    data type and the device。 you would have to specify the names of the arguments
    so D type equals Torch dot flow 16 device equals you know my GPU。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其传递给two方法，如此，这就是允许你更改张量数据类型的same two方法。如果你想同时更改数据类型和设备，则必须指定参数的名称，比如dtype=Torch.float16，device=你的GPU。
- en: But that's how you move all your tensors， learning weights， everything from
    CPU memory to GPU。Sometimes you'll have a tensor and you'll need it to be in a
    different shape。 so we're going to look at a couple of common cases and the tools
    Pytorrch offers to help you handle that。So one case where you might need to change
    the number of dimensions is when you're passing a single instance of input to
    your model。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何将所有张量、学习权重以及从CPU内存移动到GPU的过程。有时你会有一个张量，需要它以不同的形状存在。因此我们将查看几种常见情况以及Pytorch提供的工具来帮助你处理这些情况。你可能需要改变维度数量的一种情况是当你向模型传递单个输入实例时。
- en: Pytorch models expect batches of input rather than single instances。 So， for
    example。 if we had an image classification model that took in a three color image，226
    by 226 pixels。 each input image would be represented as a 3 by 226 by 2，26 tensor。Your
    model is expecting a shape of n times 3 times 2，26 times 2，26。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch模型期望批量输入，而不是单个实例。例如，如果我们有一个图像分类模型，输入为一幅三色图像，226x226像素。每个输入图像将表示为一个3x226x226的张量。你的模型期望的形状是n
    x 3 x 226 x 226。
- en: where n is the number of images in the batch， which might be， for example。8
    or 16 while you're doing training。 But let's see you're doing inference one at
    a time。 How do you make a batch of one。We're going to do that with the unsqueezze
    method。So we start with a random tensor meant to represent our input。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中n是批次中的图像数量，例如在训练时可能是8或16。但假设你在逐个进行推理，如何将其批量化为1？我们将使用unsqueeze方法来实现。我们从一个随机张量开始，代表我们的输入。
- en: the 3 by 226 by 226 image representation， and then we're going to call unsqueeze  zero
    and get that tensor and check its shape。And we'll see it's changed to  one by
    3 by 226 by 2，26。 So we added a dimension at the beginning。 That's what a 0 on
    unsqueezes， says as we want this new dimension to be the first dimension。 the
    one at index 0。 That's unsqueezing。 What we mean then by squeezing here。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 3 x 226 x 226 的图像表示，然后我们将调用 unsqueeze 的零，并获取该张量并检查其形状。我们会看到它变成了 1 x 3 x 226
    x 226。我们在开头添加了一个维度。这就是 unsqueeze 中的 0，表示我们希望这个新维度是第一个维度，即索引 0 的维度。这就是 unsqueeze
    的意思。我们在这里所说的 squeezing 是指什么。
- en: we're taking advantage of the fact that any dimension of extent  one does not
    change the number of elements in the tensor。So for example， here if we。Create
    C， which is a one by one by one by one by one tensor。When we print it， we see
    it only has one element and a lot of square brackets。So continuing the example
    above with our image classifier。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了扩展为 1 的任何维度不会改变张量中的元素数量这一事实。因此，例如，在这里如果我们创建 C，一个 1 x 1 x 1 x 1 x 1 的张量。当我们打印它时，我们看到它只有一个元素，还有很多方括号。所以继续上面的示例，使用我们的图像分类器。
- en: let's say the model's output is a 20 element vector for each input。You then
    expect the output to have a shape of n by 20 where n is the number of instances
    that were in the input batch。 so as many input instances as you put in， you want
    to have that many predictions coming out that means for a single input batch we'll
    get an output of shape 1 by 20。So what if you want to do non batched computation
    with that output。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模型的输出是每个输入的 20 元素向量。你可以期待输出具有 n x 20 的形状，其中 n 是输入批次中的实例数量。因此，你放入的输入实例越多，你希望得到的预测也越多。这意味着对于一个单一的输入批次，我们将获得形状为
    1 x 20 的输出。那么，如果你想对该输出进行非批处理计算呢？
- en: something as just expecting a 20 element vector for that， we have squeeze。So
    what's happening here here we created a random 1 by 20 vector again。 meant to
    stand in as our output tensor。We can check its shape and verify that it is 1 by
    20。 and then we can call squeeze zero on it。And so what that's saying is we want
    to take that dimension of extent1 and squeeze it away。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一点，我们只是期望得到一个 20 元素的向量，所以我们有 squeeze。那么这里发生了什么呢？我们再次创建了一个随机的 1 x 20 向量，作为我们的输出张量。我们可以检查它的形状并验证它是
    1 x 20。然后我们可以对其调用 squeeze 的零。那就是说，我们希望将该扩展为 1 的维度压缩掉。
- en: When we call that we look at the shape of B， following we can see is' just a
    20 element tensor。Now this can only be done with dimensions of extent 1。So in
    the following stanza with the variable C and D， we create a random pi torch tensor。
    and then we try to squeeze the first dimension of it。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用时，我们查看 B 的形状，可以看到它只是一个 20 元素的张量。现在这只能在扩展为 1 的维度下完成。因此，在接下来的诗句中，使用变量 C 和
    D，我们创建一个随机的 PyTorch 张量。然后我们尝试压缩它的第一个维度。
- en: And if you check the shape of the output of squeeze in that case。 you'll find
    it's the same shape you started with。 We didn't lose a dimension as we intended
    because there is no way to do that without destroying data in this case。So squeezing
    unsqueezing will only work with dimensions of extent1。 Another place you might
    use unsqueeze is to help with broadcasting。 if you recall earlier。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查 squeeze 的输出形状，你会发现它与开始时的形状相同。我们没有失去一个维度，因为在这种情况下没有办法做到这一点而不损坏数据。因此，squeezing
    和 unsqueezing 只会在扩展为 1 的维度上有效。你可能在其他地方使用 unsqueeze 来帮助广播。如果你之前记得。
- en: we had some code we were demonstratingmin broadcasting or we took4 by3 by two
    tensor multiplied it by a3 by one tensor and the result once we had the dimensions
    aligned was that every three element column in our original tensor had the same
    operation applied to the same multiplication。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些代码来演示广播，我们将一个 4 x 3 x 2 的张量与一个 3 x 1 的张量相乘，结果是当维度对齐时，原始张量中的每个三元素列都应用了相同的操作进行乘法。
- en: Now， what if instead of3 by one， we had just had a three element column vector
    that we wanted to broadcast sum operation over a？
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们没有 3 x 1，而是有一个我们想要进行广播求和操作的三元素列向量呢？
- en: If we look at the next cell， we can see that if we just look at A and B as they're
    created right away。 Brocasting can't happen there。 the trailing dimensions don't
    line up。 So what do we do。 We use unsqueeze。In the cell。To create an extra dimension
    of extent one。And then when we multiply the random three element vector against
    the larger tensor。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看下一个单元，我们可以看到如果我们直接查看 A 和 B 的创建情况。广播在这里无法发生。尾部维度不匹配。那么我们该怎么办？我们使用 unsqueeze。在单元中创建一个扩展为
    1 的额外维度。然后当我们将随机的三元素向量与较大的张量相乘时。
- en: we can see every three element column in the tensor has a multiplication operation
    broadcast over it。 so this can be a way to manipulate dimensions to get broadcasting
    to work for you without having to transpose dimensions on either of your tensors。TheSqueeze
    and unsqueeezze methods also have in place versions。 like we saw easier earlier
    with the math methods if I have one input instance。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到张量中的每个三元素列都有一个乘法操作在其上广播。因此，这可以是一种操纵维度的方式，以便让广播为你工作，而无需转置任一张量的维度。Squeeze和unsqueeze方法也有就地版本。就像我们早些时候看到的数学方法一样，如果我有一个输入实例。
- en: and I want to make a batch of one instead of calling unsqueeze I can call unsqueezze
    with the underscore and do the operation in place。Now， sometimes you'll want to
    change the shape of a tensor more radically while still preserving the number
    of elements in the tensor in their contents。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我想创建一个批次为1，而不是调用unsqueeze，我可以调用带下划线的unsqueeze，并进行就地操作。现在，有时你会想更激进地改变张量的形状，同时保留张量中元素的数量及其内容。
- en: In one case where this happens is again， taking the example of an image classifier。
    it's common in such models for the beginning of the computation to involve convolutional
    layers and the end the classification piece to involve fully connected or linear
    layers。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况发生在一个例子中，即图像分类器。此类模型的计算开始时通常涉及卷积层，而最后的分类部分则涉及全连接层或线性层。
- en: Now the convolutional layers when they're working with images will usually put
    out a three dimensional tensor。 you will have some horizontal and vertical extent
    meant to map the detection of features onto the image spatially。 and then it will
    have a depth as well， and that will be the number of features that that convolution
    kernel has learned to recognize。The fully connected layers that follow， though，
    are expecting just a one dimensional vector。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，卷积层在处理图像时，通常会输出一个三维张量。你会有一些水平和垂直的范围，用于将特征的检测在图像上进行空间映射。然后，它还会有一个深度，这将是该卷积核学习到的特征数量。接下来的全连接层则只期望一维向量。
- en: So how do we translate between these two cases where we have。An output vector
    becomes an input vector， but it needs to change shape。 but keep the same number
    of cells。Well， all we can we can do that is with the reshape method。So here we'll
    create a 6 by 20 by 20 tensor that's a stand in for our convolutional layer output。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何在这两种情况下进行转换，其中输出向量变为输入向量，但需要改变形状，但保持相同数量的单元呢？好吧，我们能做到的就是使用reshape方法。因此，在这里我们将创建一个6乘20乘20的张量，作为我们卷积层输出的占位符。
- en: And we will reshape that into a one dimensional tensor with6 times 20 times
    20 elements that stand inver the input into our fully connected layer Now when
    it can。 reshape will actually put out a view on the original tensor。So instead
    of creating a。New tensor object with new memory allocation。 It will create a new
    tensor object addressing the same memory underlying the first tensor。 So this
    is important， by the way， if you use reshape and it feeds you back a view of the
    original tensor。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把它重塑为一个一维张量，包含6乘20乘20个元素，这些元素代表我们全连接层的输入。现在，当它可以时，reshape实际上会在原始张量上输出一个视图。因此，它不会创建一个具有新内存分配的新张量对象，而是创建一个新的张量对象，指向第一个张量底层的相同内存。这一点很重要，如果你使用reshape并且返回原始张量的视图。
- en: know changes in the source tensor will be reflected in the new tensor unless
    you clone it。There are conditions beyond the scope of this introduction where
    reshape has to return tensor with the data copied for more information。 there's
    a documentation out on the topic， which I urge you to read。The last topic I wrote
    a cover on is introduction to Tensors。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，源张量中的更改将在新张量中反映，除非你克隆它。在此介绍的范围之外，还有条件需要reshape返回带数据复制的张量，更多信息可以参考相关文档。我最后写的主题是张量简介。
- en: Is the data portability between numpy tensors and pi torch tensors Now， in the
    section above。 we mentioned briefly that pi torches broadcasting semantics are
    just like numps。But the connection between the two libraries goes even deeper
    than that。If you have existing machine learning or scientific code with data stored
    in NI and D arrays。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在numpy张量和PyTorch张量之间的可移植性如何？现在，在上面的部分，我们简要提到PyTorch的广播语义与numpy相似。但这两个库之间的连接更深。如果你有现有的机器学习或科学代码，其中数据存储在NI和D数组中。
- en: you may wish to express that same data as Pytorrch tensors。 whether to take
    advantage of PyTtorrch's GPU acceleration or its efficient abstractions for building
    deep learning models。It's easy to switch between numpy and di rays and Pytorrch
    tensors。So the first cell here I' be importing Ny。And we'll create a nuy array。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望将相同的数据表示为 Pytorrch 张量，以利用 PyTtorrch 的 GPU 加速或其高效的深度学习模型构建抽象。在 Numpy、Di
    Rays 和 Pytorrch 张量之间切换非常简单。所以在这里的第一个单元我将导入 Ny。然后我们将创建一个 Nuy 数组。
- en: two by three matrix full of ones。Now to express that as a pi torch tensor we
    call torched up from Numpy with the Numpy array as an argument。 we get back a
    tensor and we print it out， we will see that it's the same shape。 it contains
    the same data and even goes so far as preserving the 64 bit floating point data
    type。 Numpy's default。And the conversion is just as easy the other way。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个充满一的 2x3 矩阵。现在要将其表示为 Pi Torch 张量，我们用 Numpy 数组作为参数调用 Torch。我们得到一个张量并将其打印出来，我们会看到它的形状相同，包含相同的数据，甚至保留了
    64 位浮点数据类型，这是 Numpy 的默认值。转换在另一方向上同样简单。
- en: so here we'll create a random pi torch tensor。And we'll call dot Numpy on it
    and we'll get back a Numpy and D。Now it's important to know these converted objects
    are using the same underlying memory as their source objects。 meaning that changes
    to one are reflected in the other。So when I run the final cell。What you'll see
    is change the value of one element of a numpy array。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们将创建一个随机的 Pi Torch 张量。然后我们将在其上调用 Numpy，我们会得到一个 Numpy 和 D。现在重要的是要知道这些转换后的对象使用与其源对象相同的底层内存。这意味着对一个的更改会反映在另一个上。所以当我运行最后一个单元时，你会看到更改了
    Numpy 数组中的一个元素的值。
- en: and we see that reflected in the pi torch tensor that we made from it。 Likewise。
    when I change a value in the pi torch tensor we made。 it's reflected in the nuy
    tensor we created from that。So again。 if you have code already that's manipulating
    your data in NPI， moving it over to P Torches a breeze。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在从中创建的 Pi Torch 张量中看到了这一点。同样，当我更改我们制作的 Pi Torch 张量中的一个值时，它会反映在我们从中创建的 Nuy
    张量中。所以，如果你已有代码在操纵 NPI 中的数据，将其迁移到 P Torches 是轻而易举的。
- en: '![](img/629ed54063cce12da521281bede5aad8_3.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/629ed54063cce12da521281bede5aad8_3.png)'
- en: That is our first deep dive on tensors。 The topics we cover today and the documentation
    at Pytororch。 org should be all you need to get going on the videos later in this
    series。 but as well on your own work within Pytorrch。Thank you for listening。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们对张量的第一次深入探讨。我们今天讨论的主题和 Pytororch.org 上的文档应该是你在后续视频中和独立工作中开始所需的一切。感谢你的收听。
