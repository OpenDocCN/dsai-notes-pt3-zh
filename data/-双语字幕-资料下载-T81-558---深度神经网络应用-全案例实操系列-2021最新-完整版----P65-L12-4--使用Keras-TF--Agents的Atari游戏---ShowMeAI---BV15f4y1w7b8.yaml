- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P65ï¼šL12.4- ä½¿ç”¨Keras TF- Agentsçš„Atariæ¸¸æˆ
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P65ï¼šL12.4- ä½¿ç”¨Keras TF- Agentsçš„Atariæ¸¸æˆ
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ Wecome to applications of Deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ we're going to continue learning about deep
    reinforcement learningã€‚ and we're going to see how to apply a Tf agentsbased program
    to Atariã€‚ We're to look at how to play the pawong gameã€‚ Howeverã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯Jeff Heatonã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­å­¦ä¹ æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†åŸºäºTF agentsçš„ç¨‹åºåº”ç”¨äºAtariã€‚æˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ç©pongæ¸¸æˆã€‚ç„¶è€Œã€‚
- en: the same technique can be used for just about any Atari gameã€‚ And then in the
    next partã€‚ we're going to look at something that's not a game and see how we can
    apply deep reinforcement learning to a financial simulationã€‚ to see all my videos
    about Cale neural networks and other AI topicsã€‚ click the subscribe button and
    the bell next to it and select al to be notified of every new videoã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åŒçš„æŠ€æœ¯å‡ ä¹å¯ä»¥ç”¨äºä»»ä½•Atariæ¸¸æˆã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ä¸€äº›ä¸æ˜¯æ¸¸æˆçš„å†…å®¹ï¼Œä»¥åŠå¦‚ä½•å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ åº”ç”¨äºé‡‘èæ¨¡æ‹Ÿã€‚è¦æŸ¥çœ‹æˆ‘å…³äºCaleç¥ç»ç½‘ç»œå’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜çš„æ‰€æœ‰è§†é¢‘ï¼Œè¯·ç‚¹å‡»è®¢é˜…æŒ‰é’®å’Œæ—è¾¹çš„é“ƒé“›ï¼Œå¹¶é€‰æ‹©æ‰€æœ‰ä»¥ä¾¿æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚
- en: So let's look at one of the most common uses for things like deep reinforcement
    learningã€‚ At least the examples that most people get intoã€‚ even if they don't
    get into actually applying this to problems of their ownã€‚ And that's Atari gamesã€‚
    Nowï¼Œ in the next moduleï¼Œ I'm going to show you how to actually adapt this to a
    problem of your own designã€‚ğŸ˜Šã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªæœ€å¸¸è§ç”¨é€”ã€‚è‡³å°‘æ˜¯å¤§å¤šæ•°äººä¼šæ¥è§¦çš„ä¾‹å­ï¼Œå³ä½¿ä»–ä»¬ä¸å®é™…å°†å…¶åº”ç”¨äºè‡ªå·±çš„é—®é¢˜ã€‚è¿™å°±æ˜¯Atariæ¸¸æˆã€‚ç°åœ¨ï¼Œåœ¨ä¸‹ä¸€ä¸ªæ¨¡å—ä¸­ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºå¦‚ä½•å°†å…¶å®é™…è°ƒæ•´ä¸ºä½ è‡ªå·±è®¾è®¡çš„é—®é¢˜ã€‚ğŸ˜Š
- en: '![](img/8a587b916463673ac1aabf070d00efab_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a587b916463673ac1aabf070d00efab_1.png)'
- en: '![](img/8a587b916463673ac1aabf070d00efab_2.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a587b916463673ac1aabf070d00efab_2.png)'
- en: And will create something in reinforcement learning that is not open AIGã€‚ But
    for nowã€‚ let's do the obligatory Atari exampleã€‚ Now I had to build most of this
    sort of from scratch there is not a lot of TF agent examples of Atari games out
    there So I worked on this a fair amount to get it working going open this in Google
    coab because I would like to use a GPU so be deadlyã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å°†åˆ›å»ºä¸€äº›åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä¸æ˜¯å¼€æ”¾AIçš„ä¸œè¥¿ã€‚ä½†ç°åœ¨ï¼Œè®©æˆ‘ä»¬åšä¸€ä¸ªå¿…è¦çš„Atariç¤ºä¾‹ã€‚ç°åœ¨æˆ‘å‡ ä¹æ˜¯ä»é›¶å¼€å§‹æ„å»ºå¤§éƒ¨åˆ†å†…å®¹ï¼Œå¸‚é¢ä¸Šæ²¡æœ‰å¾ˆå¤šTF agentçš„Atariæ¸¸æˆç¤ºä¾‹ã€‚æ‰€ä»¥æˆ‘èŠ±äº†ç›¸å½“å¤šçš„æ—¶é—´æ¥ä½¿å…¶æ­£å¸¸å·¥ä½œï¼Œå»æ‰“å¼€è¿™ä¸ªåœ¨Google
    CoLabä¸Šï¼Œå› ä¸ºæˆ‘æƒ³ä½¿ç”¨GPUï¼Œæ‰€ä»¥è¦éå¸¸å°å¿ƒã€‚
- en: deadly slow if you don't use a GPU Now what I'm going to suggest is go ahead
    and do a run all because there's a lot of code in here and you'll run into an
    errorã€‚ hopefully they fix this by the time you're watching this video but at the
    time you would get an error basically as you you would run through this part here
    where you install all of the necessary software and I'll update this as things
    change because things constantly change with machine learning and then as soon
    as you went down to here and tried to do allã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸ä½¿ç”¨GPUï¼Œé€Ÿåº¦ä¼šéå¸¸æ…¢ã€‚ç°åœ¨æˆ‘å»ºè®®ä½ è¿›è¡Œå…¨éƒ¨è¿è¡Œï¼Œå› ä¸ºè¿™é‡Œæœ‰å¾ˆå¤šä»£ç ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°é”™è¯¯ã€‚å¸Œæœ›åœ¨ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘æ—¶ä»–ä»¬èƒ½è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨å½“æ—¶ä½ åŸºæœ¬ä¸Šä¼šåœ¨å®‰è£…æ‰€æœ‰å¿…è¦è½¯ä»¶çš„éƒ¨åˆ†é‡åˆ°é”™è¯¯ï¼Œæˆ‘ä¼šéšç€æƒ…å†µå˜åŒ–æ›´æ–°è¿™ä¸ªå†…å®¹ï¼Œå› ä¸ºæœºå™¨å­¦ä¹ çš„å†…å®¹ä¸æ–­å˜åŒ–ï¼Œç„¶åä¸€æ—¦ä½ ä¸‹åˆ°è¿™é‡Œå°è¯•åšæ‰€æœ‰æ“ä½œã€‚
- en: These importsï¼Œ you would get an error if you're getting an error in this import
    blockï¼Œ do a runtimeã€‚ restart and then run all againã€‚ Reing the runtime does not
    erase the installation that happened up hereã€‚ Basically the reason you get that
    error is because once you P installed TF agentsã€‚ it updates the version of protocol
    buffer that is installed on your computer and Tensorflow itself needs a restart
    of the Python environment or it's gonna to give you an error at that pointã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯¼å…¥ï¼Œå¦‚æœä½ åœ¨è¿™ä¸ªå¯¼å…¥å—ä¸­é‡åˆ°é”™è¯¯ï¼Œè¿›è¡Œè¿è¡Œæ—¶é‡å¯ï¼Œç„¶åå†æ¬¡è¿è¡Œæ‰€æœ‰ä»£ç ã€‚é‡æ–°å¯åŠ¨è¿è¡Œæ—¶ä¸ä¼šåˆ é™¤ä¹‹å‰çš„å®‰è£…ã€‚åŸºæœ¬ä¸Šï¼Œå‡ºç°è¯¥é”™è¯¯çš„åŸå› æ˜¯ï¼Œä¸€æ—¦ä½ å®‰è£…äº†TF
    agentsï¼Œå®ƒä¼šæ›´æ–°ä½ ç”µè„‘ä¸Šå®‰è£…çš„åè®®ç¼“å†²åŒºç‰ˆæœ¬ï¼Œè€ŒTensorflowæœ¬èº«éœ€è¦é‡æ–°å¯åŠ¨Pythonç¯å¢ƒï¼Œå¦åˆ™ä¼šåœ¨é‚£æ—¶å‡ºç°é”™è¯¯ã€‚
- en: Hopefully as time progresses Google coabã€‚ I'm really hoping they will install
    Tf agents just out of the boxã€‚ I assume they willã€‚ it's a Google productã€‚ It's
    just a new product relatively newã€‚ So let's talk about how we will do thisã€‚ We're
    gonna still use a Dqn a deep Q learning networkã€‚ This is all based on virtual
    Atariï¼Œ which is a completely unrelated project a machine learningã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›éšç€æ—¶é—´çš„æ¨ç§»ï¼ŒGoogle Coabã€‚æˆ‘çœŸçš„å¸Œæœ›ä»–ä»¬èƒ½é»˜è®¤å®‰è£…**Tf agents**ã€‚æˆ‘æƒ³ä»–ä»¬ä¼šçš„ï¼Œè¿™æ˜¯è°·æ­Œçš„äº§å“ï¼Œåªæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„äº§å“ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬è°ˆè°ˆå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬ä»å°†ä½¿ç”¨DQNæ·±åº¦Qå­¦ä¹ ç½‘ç»œã€‚è¿™ä¸€åˆ‡éƒ½æ˜¯åŸºäºè™šæ‹Ÿ**Atari**ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸ç›¸å…³çš„æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚
- en: It was just a emulator so that you could play old Atari 2600ã€‚![](img/8a587b916463673ac1aabf070d00efab_4.png)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ï¼Œä½ å¯ä»¥ç©æ—§çš„**Atari 2600**ã€‚![](img/8a587b916463673ac1aabf070d00efab_4.png)
- en: Gams now I am somewhat oldï¼Œ but this is before really even my timeã€‚ There are
    2600sã€‚ This is called the Atari 2600 out there when I was pretty youngã€‚ but this
    was kind of the older game system out thereã€‚ I did play PAman on this back in
    the dayã€‚ which which was kind of funã€‚ I was more into a system that came out around
    this time called in television a little bit later than the Atari 2600ã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸¸æˆç°åœ¨æˆ‘æœ‰ç‚¹è€äº†ï¼Œä½†è¿™å…¶å®åœ¨æˆ‘çš„æ—¶ä»£ä¹‹å‰ã€‚è¿™æ˜¯2600sã€‚è¿™å°±æ˜¯**Atari 2600**ï¼Œåœ¨æˆ‘å¹´è½»çš„æ—¶å€™å‡ºç°ã€‚ä½†è¿™æ˜¯å½“æ—¶çš„è€æ¸¸æˆç³»ç»Ÿã€‚æˆ‘æ›¾ç»åœ¨è¿™ä¸ªç³»ç»Ÿä¸Šç©è¿‡**Pac-Man**ï¼Œé‚£çœŸçš„å¾ˆæœ‰è¶£ã€‚æˆ‘æ›´å–œæ¬¢å¤§çº¦åœ¨è¿™ä¸ªæ—¶æœŸæ¨å‡ºçš„å¦ä¸€ä¸ªç³»ç»Ÿï¼Œå«åš**Intellivision**ï¼Œå®ƒæ¯”**Atari
    2600**æ™šä¸€ç‚¹ã€‚
- en: and then of courseï¼Œ my absolute true love the Commodore 64ã€‚ So this is the specs
    for an Atari 2600ã€‚ It's not an advanced machine by any stretchã€‚ Your play field
    resolution is 40 by 192 pixelsã€‚ Now they do some creative coding to sort of stretch
    that outã€‚ but you're able to basically just feed that right into convolution neural
    networkã€‚ Alsoã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæˆ‘ç»å¯¹çœŸçˆ±çš„å°±æ˜¯**Commodore 64**ã€‚æ‰€ä»¥è¿™æ˜¯**Atari 2600**çš„è§„æ ¼ã€‚å®ƒå¹¶ä¸æ˜¯ä¸€å°å…ˆè¿›çš„æœºå™¨ã€‚ä½ çš„æ¸¸æˆåœºåˆ†è¾¨ç‡æ˜¯40ä¹˜192åƒç´ ã€‚ç°åœ¨ï¼Œä»–ä»¬é€šè¿‡ä¸€äº›åˆ›é€ æ€§çš„ç¼–ç æ¥æ‰©å±•è¿™ä¸ªåˆ†è¾¨ç‡ï¼Œä½†ä½ åŸºæœ¬ä¸Šå¯ä»¥å°†å…¶ç›´æ¥è¾“å…¥åˆ°å·ç§¯ç¥ç»ç½‘ç»œä¸­ã€‚
- en: the amount of memory in an Atari is not hugeã€‚ So it's measured in killbyteã€‚![](img/8a587b916463673ac1aabf070d00efab_6.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Atari**ä¸­çš„å†…å­˜é‡å¹¶ä¸å¤§ã€‚æ‰€ä»¥å®ƒæ˜¯ä»¥åƒå­—èŠ‚ä¸ºå•ä½è¿›è¡Œæµ‹é‡çš„ã€‚![](img/8a587b916463673ac1aabf070d00efab_6.png)'
- en: '![](img/8a587b916463673ac1aabf070d00efab_7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a587b916463673ac1aabf070d00efab_7.png)'
- en: '![](img/8a587b916463673ac1aabf070d00efab_8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a587b916463673ac1aabf070d00efab_8.png)'
- en: It's not not megabytesï¼Œ actually I think it's less than a kilobyte so you can
    push the active memory that the games are running on into the well the video you
    would push into a convolution neural network the 160 by 192 that's the resolution
    I think you're normally dealing with but you can actually look at the instate
    memory of the game and use that for the state space and there's a lot of information
    in there and that can be used the agent can learn to cheat too because some of
    the memory might be being utilized for the opponent play so the computer controlled
    the NPCs in the game So we do all the imports hyperparameters i tuned this a lot
    to get it to work for pong also did some with breakout and space invadersve got
    you've got to play with these values and get them working for whatever game you're
    going to do the number of iterations thatã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™å¹¶ä¸æ˜¯å…†å­—èŠ‚ï¼Œæˆ‘æƒ³å®ƒç”šè‡³å°äºä¸€ä¸ªåƒå­—èŠ‚ï¼Œå› æ­¤ä½ å¯ä»¥å°†æ¸¸æˆè¿è¡Œçš„æ´»åŠ¨å†…å­˜æ¨é€åˆ°å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œ160ä¹˜192æ˜¯æˆ‘è®¤ä¸ºä½ é€šå¸¸å¤„ç†çš„åˆ†è¾¨ç‡ï¼Œä½†ä½ å®é™…ä¸Šå¯ä»¥æŸ¥çœ‹æ¸¸æˆçš„å³æ—¶å†…å­˜ï¼Œå¹¶å°†å…¶ç”¨äºçŠ¶æ€ç©ºé—´ï¼Œé‡Œé¢æœ‰å¾ˆå¤šä¿¡æ¯ï¼Œä»£ç†å¯ä»¥å­¦ä¹ ä½œå¼Šï¼Œå› ä¸ºæœ‰äº›å†…å­˜å¯èƒ½è¢«å¯¹æ‰‹çš„åŠ¨ä½œå ç”¨ï¼Œæ‰€ä»¥è®¡ç®—æœºæ§åˆ¶çš„NPCåœ¨æ¸¸æˆä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬åšæ‰€æœ‰çš„å¯¼å…¥è¶…å‚æ•°ï¼Œæˆ‘è°ƒä¼˜äº†å¾ˆå¤šä»¥é€‚åº”**Pong**ï¼Œè¿˜å¯¹**Breakout**å’Œ**Space
    Invaders**åšäº†ä¸€äº›å®éªŒï¼Œä½ å¿…é¡»è°ƒæ•´è¿™äº›å€¼ä»¥ä½¿å®ƒä»¬é€‚åº”ä½ è¦è¿›è¡Œçš„ä»»ä½•æ¸¸æˆçš„è¿­ä»£æ¬¡æ•°ã€‚
- en: Is doing nearly a quarter of a million of them hereã€‚ So that is I typically
    ran this thing all nightã€‚ It takes a while to trainï¼Œ even with a GPUã€‚ And I'm
    also doing 10 collection steps per iterationã€‚ replay buffer sizeï¼Œ that didn't
    make too much of a differenceï¼Œ but this learning rateã€‚ this 10 value here and
    the number of iterationsã€‚ Those are the guys that I was really tuning to try to
    get better resultsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œå‡ ä¹å¤„ç†äº†25ä¸‡ä¸ªæ ·æœ¬ã€‚æ‰€ä»¥æˆ‘é€šå¸¸ä¼šæ•´å¤œè¿è¡Œè¿™ä¸ªç¨‹åºã€‚è®­ç»ƒéœ€è¦ä¸€æ®µæ—¶é—´ï¼Œå³ä½¿ä½¿ç”¨GPUã€‚æˆ‘æ¯æ¬¡è¿­ä»£ä¹Ÿè¿›è¡Œ10æ¬¡æ”¶é›†æ­¥éª¤ã€‚é‡æ”¾ç¼“å†²åŒºå¤§å°æ²¡æœ‰å¤ªå¤§å·®åˆ«ï¼Œä½†è¿™ä¸ªå­¦ä¹ ç‡ï¼Œè¿™ä¸ª10çš„å€¼å’Œè¿­ä»£æ¬¡æ•°ï¼Œè¿™äº›æ˜¯æˆ‘çœŸæ­£è°ƒä¼˜ä»¥æœŸè·å¾—æ›´å¥½ç»“æœçš„å‚æ•°ã€‚
- en: I did not mess with the shape of the neural network that could also help you
    as wellã€‚ The stuff does take a lot of time and not just your time compute time
    to try out your different hyperparameter tuningsã€‚ So for an Atari environmentï¼Œ
    we're using pongï¼Œ you can see some of the other games I was playing with here
    as wellã€‚ Pong's relatively simpleã€‚ It is basically two paddles bouncing a ball
    between themselvesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ²¡æœ‰è°ƒæ•´ç¥ç»ç½‘ç»œçš„å½¢çŠ¶ï¼Œè¿™ä¹Ÿå¯ä»¥å¸®åŠ©ä½ ã€‚è°ƒæ•´ä¸åŒçš„è¶…å‚æ•°ç¡®å®éœ€è¦å¾ˆå¤šæ—¶é—´ï¼Œä¸ä»…ä»…æ˜¯ä½ çš„æ—¶é—´ï¼Œè¿˜æœ‰è®¡ç®—æ—¶é—´ã€‚æ‰€ä»¥åœ¨Atariç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹’ä¹“çƒï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘åœ¨è¿™é‡Œç©çš„ä¸€äº›å…¶ä»–æ¸¸æˆã€‚ä¹’ä¹“çƒç›¸å¯¹ç®€å•ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸¤ä¸ªçƒæ‹åœ¨ç›¸äº’å¼¹çƒã€‚
- en: Now pong was before my timeã€‚ I never really got into pong whatsoeverã€‚ That would
    probably be I meanã€‚ that would be the 1970sã€‚A little before I got into into video
    games mid80s is kind of when I got into a lot of this stuffã€‚ So the frame skip
    rateï¼Œ you're not going to look at every single frame that is coming through in
    the Atari This is just things to make the computation time reasonable if you are
    processing every single frame that's going it's going take a lot of processing
    time there is a wrappper that TF agents gives you to load the Atari games I definitely
    recommend you do this because it builds in some of the preprocessing that is necessary
    or not necessaryã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¹’ä¹“çƒæ˜¯åœ¨æˆ‘ä¹‹å‰çš„æ—¶ä»£ã€‚æˆ‘ä»æ¥æ²¡æœ‰çœŸæ­£ç©è¿‡ä¹’ä¹“çƒã€‚é‚£åº”è¯¥æ˜¯1970å¹´ä»£ï¼Œç¨æ—©äºæˆ‘è¿›å…¥ç”µå­æ¸¸æˆçš„æ—¶æœŸï¼Œæˆ‘æ˜¯åœ¨80å¹´ä»£ä¸­æœŸå¼€å§‹æ¥è§¦è¿™äº›ä¸œè¥¿çš„ã€‚æ‰€ä»¥å¸§è·³è¿‡ç‡ï¼Œä½ ä¸ä¼šæŸ¥çœ‹Atariä¸­æ¯ä¸€å¸§çš„ç”»é¢ã€‚è¿™åªæ˜¯ä¸ºäº†è®©è®¡ç®—æ—¶é—´å˜å¾—åˆç†ï¼Œå¦‚æœä½ å¤„ç†æ¯ä¸€å¸§ï¼Œå¤„ç†æ—¶é—´ä¼šéå¸¸é•¿ã€‚TF
    agentsæä¾›äº†ä¸€ä¸ªåŒ…è£…å™¨æ¥åŠ è½½Atariæ¸¸æˆï¼Œæˆ‘ç»å¯¹æ¨èä½ è¿™æ ·åšï¼Œå› ä¸ºå®ƒå†…ç½®äº†ä¸€äº›å¿…è¦æˆ–ä¸å¿…è¦çš„é¢„å¤„ç†ã€‚
- en: but it really helps your neural network be able to learn quickly because the
    idea is I mean look at is the color information really telling you all that much
    and this is the little panel that's going to go up and down the computer the NPC
    the computer control player by the wayã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™ç¡®å®æœ‰åŠ©äºä½ çš„ç¥ç»ç½‘ç»œå¿«é€Ÿå­¦ä¹ ï¼Œå› ä¸ºé—®é¢˜åœ¨äºï¼Œé¢œè‰²ä¿¡æ¯çœŸçš„èƒ½å‘Šè¯‰ä½ å¾ˆå¤šå—ï¼Ÿè¿™æ˜¯ä¸€ä¸ªå°†ä¸Šä¸‹ç§»åŠ¨çš„é¢æ¿ï¼Œç”µè„‘NPCï¼Œé¡ºä¾¿è¯´ä¸€ä¸‹æ˜¯ç”µè„‘æ§åˆ¶çš„ç©å®¶ã€‚
- en: NPC is non-player character I guess I get that from the MMOGs theOOGs this is
    the opponent scoreã€‚This is your score and technically the neural network is looking
    at everythingã€‚ I don't know that it's getting a whole lot from the scoresã€‚ but
    that is potentially something that it uses as wellã€‚ maybe tries harder when it
    sees it losingã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: NPCæ˜¯éç©å®¶è§’è‰²ï¼Œæˆ‘æƒ³æˆ‘ä»MMOGä¸­å¾—åˆ°äº†è¿™ä¸ªï¼ŒOOGsè¿™æ˜¯å¯¹æ‰‹çš„åˆ†æ•°ã€‚è¿™æ˜¯ä½ çš„åˆ†æ•°ï¼Œä¸¥æ ¼æ¥è¯´ï¼Œç¥ç»ç½‘ç»œåœ¨æŸ¥çœ‹ä¸€åˆ‡ã€‚æˆ‘ä¸çŸ¥é“å®ƒæ˜¯å¦ä»åˆ†æ•°ä¸­è·å¾—äº†å¾ˆå¤šï¼Œä½†è¿™ä¹Ÿå¯èƒ½æ˜¯å®ƒä½¿ç”¨çš„ä¸œè¥¿ï¼Œå¯èƒ½åœ¨çœ‹åˆ°è‡ªå·±è¾“çš„æ—¶å€™æ›´åŠªåŠ›ã€‚
- en: I don't know I'm kidding it probably doesn't it probably learns that those scores
    are pretty pretty unimportant So we load the Atari environment we are using a
    separate one for training and evaluation just so that if you run an avval step
    in there that you're not changing the state of the training which could be confusing
    that would beã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“æˆ‘åœ¨å¼€ç©ç¬‘ï¼Œå®ƒå¯èƒ½ä¸ä¼šï¼Œå®ƒå¯èƒ½ä¼šå­¦ä¹ åˆ°é‚£äº›åˆ†æ•°å…¶å®å¹¶ä¸é‡è¦ã€‚æ‰€ä»¥æˆ‘ä»¬åŠ è½½äº†Atariç¯å¢ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„ç¯å¢ƒè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œè¿™æ ·å¦‚æœä½ åœ¨é‚£é‡Œé¢è¿è¡Œä¸€ä¸ªè¯„ä¼°æ­¥éª¤ï¼Œå°±ä¸ä¼šæ”¹å˜è®­ç»ƒçš„çŠ¶æ€ï¼Œè¿™å¯èƒ½ä¼šè®©äººå›°æƒ‘ã€‚
- en: Ohï¼Œ that would be like having an an alcohol blackout almostã€‚ you time would
    skip forwardã€‚ but you would not know what happenedã€‚ Probably a bad example of
    what's going on thereã€‚ But it works for me as a visualã€‚Nowï¼Œ the agentï¼Œ this I
    got from TF agent's exampleã€‚ This is just a wrapper around the normal agentã€‚ You
    can see that we're dividing each of the state valuesã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œè¿™å°±åƒå‡ ä¹æœ‰ä¸€æ¬¡é…’ç²¾å¤±å¿†ä¸€æ ·ã€‚ä½ ä¼šè·³è¿‡æ—¶é—´ï¼Œä½†ä½ ä¸çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆã€‚å¯èƒ½è¿™ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚ä½†å¯¹æˆ‘æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰åŒ–çš„æ–¹å¼ã€‚ç°åœ¨ï¼Œä»£ç†ï¼Œè¿™ä¸ªæˆ‘ä»TF
    agentçš„ç¤ºä¾‹ä¸­å¾—æ¥çš„ã€‚è¿™åªæ˜¯ä¸€ä¸ªå›´ç»•æ­£å¸¸ä»£ç†çš„åŒ…è£…å™¨ã€‚ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åœ¨åˆ’åˆ†æ¯ä¸ªçŠ¶æ€å€¼ã€‚
- en: but by 255ã€‚ What you're doing thereã€‚ Basically is converting that 0 to 255 RGB
    values into a floating point value between 0 and 1ã€‚ and the neural network deals
    with that a little bit betterã€‚ Most of the examples have done thatã€‚ I usually
    do that when I'm dealing with image dataã€‚ don't feed 0 to 255 into a convolution
    neural networkã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é€šè¿‡255ã€‚ä½ åœ¨åšçš„åŸºæœ¬ä¸Šæ˜¯å°†0åˆ°255çš„RGBå€¼è½¬æ¢ä¸º0åˆ°1ä¹‹é—´çš„æµ®ç‚¹å€¼ï¼Œç¥ç»ç½‘ç»œå¯¹è¿™ä¸ªå¤„ç†å¾—æ›´å¥½ã€‚å¤§å¤šæ•°ç¤ºä¾‹éƒ½æ˜¯è¿™æ ·åšçš„ã€‚æˆ‘é€šå¸¸åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶è¿™æ ·åšã€‚ä¸è¦å°†0åˆ°255çš„å€¼è¾“å…¥å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: It can deal with it okay but it really likes those smaller values betterã€‚ This
    information here is importantã€‚ these are actually hyperparameterã€‚ I did not try
    tuning these too muchã€‚ I just took example valuesã€‚ but here these are the fully
    connected layersã€‚ So these are the dense layersã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥åº”å¯¹å¾—å¾ˆå¥½ï¼Œä½†å®ƒç¡®å®æ›´å–œæ¬¢é‚£äº›è¾ƒå°çš„å€¼ã€‚è¿™äº›ä¿¡æ¯å¾ˆé‡è¦ã€‚è¿™äº›å®é™…ä¸Šæ˜¯è¶…å‚æ•°ã€‚æˆ‘æ²¡æœ‰å¯¹è¿™äº›è¿›è¡Œå¤ªå¤šè°ƒä¼˜ã€‚æˆ‘åªæ˜¯å–äº†ç¤ºä¾‹å€¼ã€‚ä½†è¿™é‡Œæ˜¯å®Œå…¨è¿æ¥å±‚ã€‚æ‰€ä»¥è¿™äº›æ˜¯ç¨ å¯†å±‚ã€‚
- en: Most convolution neural networks have dense layers at the veryï¼Œ very endã€‚ So
    we have just a single layer of 512ã€‚ You could add multipleã€‚Ones thereï¼Œ if you
    wantã€‚ And then the convolution layersã€‚ This is the channel count 32ã€‚ scanning
    an8 by8 with a stride of4ã€‚ That's basic convolution informationã€‚ And then several
    of theseã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°å·ç§¯ç¥ç»ç½‘ç»œåœ¨æœ€åéƒ½æœ‰ç¨ å¯†å±‚ã€‚æ‰€ä»¥æˆ‘ä»¬åªæœ‰ä¸€å±‚512ã€‚ä½ å¯ä»¥æ·»åŠ å¤šä¸ªå±‚ï¼Œå¦‚æœä½ æ„¿æ„ã€‚ç„¶åæ˜¯å·ç§¯å±‚ã€‚è¿™é‡Œçš„é€šé“æ•°æ˜¯32ï¼Œä»¥æ­¥å¹…4æ‰«æ8x8ã€‚è¿™æ˜¯åŸºæœ¬çš„å·ç§¯ä¿¡æ¯ã€‚ç„¶åæœ‰å‡ ä¸ªè¿™æ ·çš„å±‚ã€‚
- en: It don't really provide a way to put a max pooling layer in there at least that
    I've seen but this has worked relatively wellã€‚ I did not try tuning this to too
    muchã€‚ or at allï¼Œ reallyã€‚ then you build up the Atari Q network that we had up
    thereã€‚ So this is the wrapper for the neural networkã€‚ I have more text that describes
    kind of what I did because this this example I did literally literally build from
    scratch except for that Q network wrapper that I found We're gonna use an RMS
    prop optimizerã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¹¶æ²¡æœ‰çœŸæ­£æä¾›ä¸€ç§æ–¹å¼æ¥æ”¾ç½®æœ€å¤§æ± åŒ–å±‚ï¼Œè‡³å°‘æˆ‘æ²¡æœ‰çœ‹åˆ°ï¼Œä½†è¿™ç›¸å¯¹æœ‰æ•ˆã€‚æˆ‘æ²¡æœ‰å¯¹å…¶è¿›è¡Œå¤ªå¤šè°ƒä¼˜ï¼Œå®é™…ä¸Šæ ¹æœ¬æ²¡æœ‰ã€‚ç„¶åä½ æ„å»ºæˆ‘ä»¬ä¸Šé¢æåˆ°çš„ Atari Q
    ç½‘ç»œã€‚è¿™æ˜¯ç¥ç»ç½‘ç»œçš„å°è£…å™¨ã€‚æˆ‘æœ‰æ›´å¤šçš„æ–‡æœ¬æè¿°æˆ‘åšäº†ä»€ä¹ˆï¼Œå› ä¸ºè¿™ä¸ªç¤ºä¾‹æˆ‘ç¡®å®æ˜¯ä»å¤´å¼€å§‹æ„å»ºçš„ï¼Œé™¤äº†æˆ‘æ‰¾åˆ°çš„ Q ç½‘ç»œå°è£…å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ RMS prop ä¼˜åŒ–å™¨ã€‚
- en: the learning rate and other things here are very importantã€‚ Those are hyperparameters
    that you'll want to tuneã€‚ we use a global steps so that we have the counter for
    how far we've gone with trainingã€‚ And here is where we create the DQ N agentã€‚
    DQ N is good when you haveã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡å’Œå…¶ä»–å‚æ•°åœ¨è¿™é‡Œéå¸¸é‡è¦ã€‚è¿™äº›æ˜¯ä½ éœ€è¦è°ƒä¼˜çš„è¶…å‚æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨å…¨å±€æ­¥éª¤ï¼Œä»¥ä¾¿æˆ‘ä»¬æœ‰ä¸€ä¸ªè®¡æ•°å™¨æ¥è®°å½•è®­ç»ƒè¿›å±•ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬åˆ›å»º DQN ä»£ç†ã€‚DQN åœ¨ä½ æ‹¥æœ‰æ—¶è¡¨ç°è‰¯å¥½ã€‚
- en: actionction space or discrete action space where it has individual values and
    not some continuous range of numbers In the next partã€‚ the next videoï¼Œ we will
    see how to deal with a continuous range action spaceã€‚ You use a close a close
    relative to the DQN neural network I use the same evaluation that I got from TF
    agentsã€‚ this is a method that I copied a function that I copied basically what
    it's doing is 10 evaluation steps and it calculates the average reward over 10
    stepsã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´æˆ–ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œå…¶ä¸­æœ‰å•ç‹¬çš„å€¼ï¼Œè€Œä¸æ˜¯ä¸€äº›è¿ç»­çš„æ•°å€¼èŒƒå›´ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ï¼Œä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å¤„ç†è¿ç»­èŒƒå›´çš„åŠ¨ä½œç©ºé—´ã€‚ä½ ä½¿ç”¨ä¸ DQN ç¥ç»ç½‘ç»œéå¸¸æ¥è¿‘çš„ä¸€ä¸ªå˜ç§ã€‚æˆ‘ä½¿ç”¨çš„è¯„ä¼°æ–¹å¼ä¸
    TF ä»£ç†ä¸­è·å¾—çš„ç›¸åŒã€‚è¿™æ˜¯ä¸€ä¸ªæˆ‘å¤åˆ¶çš„å‡½æ•°ï¼ŒåŸºæœ¬ä¸Šå®ƒçš„ä½œç”¨æ˜¯è¿›è¡Œ10æ­¥è¯„ä¼°ï¼Œå¹¶è®¡ç®—10æ­¥çš„å¹³å‡å¥–åŠ±ã€‚
- en: Atari games are stochasticã€‚ They have random numbersã€‚ The same strategy won't
    necessarily work for two games in a rowã€‚ but they figure if you played it over
    10 games's that's gonna be reasonable replayplay buffer we're using pretty much
    the same type of replay buffer that I've used in the other Tensorflow T agent
    examples There's nothing too crazy that you need to do here for Atari I use exactly
    the same random collection code that I've used in a coupleã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Atari æ¸¸æˆæ˜¯éšæœºçš„ã€‚å®ƒä»¬æœ‰éšæœºæ•°ã€‚åŒæ ·çš„ç­–ç•¥åœ¨è¿ç»­ä¸¤å±€æ¸¸æˆä¸­ä¸ä¸€å®šæœ‰æ•ˆã€‚ä½†ä»–ä»¬è®¤ä¸ºå¦‚æœä½ ç©è¶…è¿‡10å±€æ¸¸æˆï¼Œé‚£å°†ä¼šæ˜¯ä¸€ä¸ªåˆç†çš„é‡æ”¾ç¼“å†²åŒºã€‚æˆ‘ä»¬ä½¿ç”¨çš„å‡ ä¹æ˜¯ä¸æˆ‘åœ¨å…¶ä»–
    Tensorflow T ä»£ç†ç¤ºä¾‹ä¸­ä½¿ç”¨çš„ç›¸åŒç±»å‹çš„é‡æ”¾ç¼“å†²åŒºã€‚åœ¨ Atari ä¸­ï¼Œä½ ä¸éœ€è¦åšä»»ä½•ç‰¹åˆ«ç–¯ç‹‚çš„äº‹æƒ…ï¼Œæˆ‘ä½¿ç”¨çš„æ­£æ˜¯æˆ‘åœ¨å‡ ä¸ªç¤ºä¾‹ä¸­ç”¨è¿‡çš„éšæœºé›†åˆä»£ç ã€‚
- en: Of examples so farã€‚ and this is code that I got from the TF agents examples
    works quite well for thatã€‚ This is exactly the same trainingã€‚ the agent code that
    I used in previous examplesã€‚ Nothing needs to really change hereã€‚ It's pretty
    boiler plateã€‚ some initial setupã€‚ and then we're going to loop through the number
    of iterations and we're going to see this is where that steps for iterationã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢çš„ç¤ºä¾‹ï¼Œä»¥åŠæˆ‘ä» TF ä»£ç†ç¤ºä¾‹ä¸­è·å¾—çš„ä»£ç ï¼Œå¯¹äºè¿™ä¸ªéå¸¸æœ‰æ•ˆã€‚è¿™æ­£æ˜¯æˆ‘åœ¨ä¹‹å‰ç¤ºä¾‹ä¸­ä½¿ç”¨çš„è®­ç»ƒä»£ç†ä»£ç ã€‚åœ¨è¿™é‡ŒçœŸçš„æ²¡æœ‰ä»€ä¹ˆéœ€è¦æ”¹å˜çš„ã€‚å®ƒæ˜¯ç›¸å½“åŸºç¡€çš„ã€‚è¿›è¡Œä¸€äº›åˆå§‹è®¾ç½®ï¼Œç„¶åæˆ‘ä»¬å°†å¾ªç¯éå†è¿­ä»£æ¬¡æ•°ï¼Œè¿™å°±æ˜¯æ¯æ¬¡è¿­ä»£çš„æ­¥éª¤ã€‚
- en: So setting this to 10ã€‚ each iterationã€‚ we're going to collect 10 steps and use
    that to train the neural network to build to the data that we randomly sample
    from that we're training the dataã€‚ the data collection Here you can see it is
    it is working very hard training Now negative 20 negative 21ã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å°†è¿™ä¸ªè®¾ç½®ä¸º10ã€‚æ¯æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬å°†æ”¶é›†10æ­¥ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥æ„å»ºæˆ‘ä»¬éšæœºé‡‡æ ·çš„æ•°æ®ï¼Œè¿™é‡Œä½ å¯ä»¥çœ‹åˆ°å®ƒæ­£åœ¨åŠªåŠ›è®­ç»ƒï¼Œç°åœ¨æ˜¯è´Ÿ20ï¼Œè´Ÿ21ã€‚
- en: I think it actually is is the worst you can possibly do if you're just if you're
    doing bad at pong everything you're not hitting anything you're shooting like
    stormtroopersã€‚ you're sending you're not hit you're not sending it through the
    other guy he's sending it right back to youã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—è¿™å…¶å®æ˜¯ä½ å¯èƒ½åšå¾—æœ€ç³Ÿç³•çš„ï¼Œå¦‚æœä½ åœ¨ä¹’ä¹“çƒä¸Šè¡¨ç°ä¸å¥½ï¼Œä»€ä¹ˆéƒ½æ²¡å‡»ä¸­ï¼Œå°±åƒå†²é”‹é˜Ÿå‘˜ä¸€æ ·ã€‚ä½ æ ¹æœ¬æ²¡èƒ½æŠŠçƒé€è¿‡å»ï¼Œåè€Œæ˜¯å¯¹æ–¹æŠŠçƒç›´æ¥æ‰“å›ç»™ä½ ã€‚
- en: So I was very depressed with that when I was learning and working through this
    no matter what I would tryã€‚ I would get like negative 21 straight downï¼Œ down downã€‚
    It'd be four hours laterï¼Œ negative 21ã€‚ but as I started to adjust thingsã€‚ I was
    able to get this to improve considerably and you can see this is a decent training
    runã€‚ I'm fairly happy with itã€‚ It got above zero you can do much better than thisã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“æˆ‘å­¦ä¹ å¹¶åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æ—¶ï¼Œæˆ‘éå¸¸æ²®ä¸§ï¼Œæ— è®ºæˆ‘å°è¯•ä»€ä¹ˆï¼Œæˆ‘çš„å¾—åˆ†æ€»æ˜¯åƒè´Ÿ21ä¸€æ ·ï¼Œä¸€ç›´å¾€ä¸‹ã€‚å››ä¸ªå°æ—¶åï¼Œè¿˜æ˜¯è´Ÿ21ã€‚ä½†å½“æˆ‘å¼€å§‹è°ƒæ•´ä¸€äº›ä¸œè¥¿æ—¶ï¼Œæˆ‘èƒ½å¤Ÿä½¿è¿™ä¸ªè¡¨ç°æ˜¾è‘—æ”¹å–„ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™æ˜¯ä¸€æ¬¡ä¸é”™çš„è®­ç»ƒã€‚æˆ‘å¯¹æ­¤ç›¸å½“æ»¡æ„ï¼Œå®ƒå¾—åˆ†è¶…è¿‡äº†é›¶ï¼Œä½ å¯ä»¥åšå¾—æ¯”è¿™å¥½å¤šäº†ã€‚
- en: This is a pretty average run that I have hereã€‚ and I'll show you what what mine
    looks likeã€‚ it does score pointsï¼Œ it does much better than the random agentã€‚ but
    you could definitely refine this if you wanted to spend a lot of time optimizing
    those hyperparameter and throwing compute at it visualizationã€‚ you can see pretty
    good progression upward probably more iterations would help me and then you can
    see the video here I'm let it play It's good but not great Oh I scored scored
    oneã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘è¿™é‡Œçš„ä¸€ä¸ªç›¸å½“å¹³å‡çš„è¿è¡Œï¼Œæˆ‘ä¼šç»™ä½ å±•ç¤ºæˆ‘çš„è¡¨ç°ã€‚å®ƒå¾—åˆ†ï¼Œè¡¨ç°æ¯”éšæœºä»£ç†å¥½å¾—å¤šï¼Œä½†å¦‚æœä½ æƒ³èŠ±å¾ˆå¤šæ—¶é—´ä¼˜åŒ–è¶…å‚æ•°å’Œè¿›è¡Œå¯è§†åŒ–ï¼Œä½ å¯ä»¥è‚¯å®šåœ°è¿›è¡Œæ›´å¥½çš„è°ƒæ•´ã€‚ä½ å¯ä»¥çœ‹åˆ°ä¸é”™çš„å‘ä¸Šè¿›å±•ï¼Œå¯èƒ½æ›´å¤šçš„è¿­ä»£ä¼šå¯¹æˆ‘æœ‰å¸®åŠ©ï¼Œç„¶åä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„è§†é¢‘ï¼Œæˆ‘è®©å®ƒæ’­æ”¾ï¼Œå®ƒä¸é”™ä½†ä¸ç®—ä¼˜ç§€ã€‚å“¦ï¼Œæˆ‘å¾—äº†ä¸€åˆ†ã€‚
- en: score2 I'm ahead even I'm behind So it's playing accept wellã€‚ I never really
    got too much into pong to play this on my own so I don't know howã€‚How good I would
    particularly be doing hereï¼Œ but it'sï¼Œ it's definitely losing overallã€‚ But if you
    try the random agentï¼Œ ohï¼Œ it'sï¼Œ it's catching upï¼Œ this is awesome Am I am my headï¼Œ
    Yesã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—åˆ†2ï¼Œæˆ‘é¢†å…ˆï¼Œå°½ç®¡æˆ‘è½åã€‚æ‰€ä»¥å®ƒè¡¨ç°å¾—ä¸é”™ã€‚æˆ‘ä»æœªçœŸæ­£æ·±å…¥ç©è¿‡ä¹’ä¹“çƒï¼Œæ‰€ä»¥æˆ‘ä¸çŸ¥é“æˆ‘ä¼šç‰¹åˆ«åšå¾—æ€ä¹ˆæ ·ï¼Œä½†æ•´ä½“ä¸Šè‚¯å®šæ˜¯è¾“çš„ã€‚ä¸è¿‡å¦‚æœä½ å°è¯•éšæœºä»£ç†ï¼Œå“¦ï¼Œå®ƒæ­£åœ¨è¿½ä¸Šï¼Œè¿™å¤ªæ£’äº†ï¼Œæˆ‘é¢†å…ˆäº†å—ï¼Ÿæ˜¯çš„ã€‚
- en: so this isã€‚It's it's definitely got some skillã€‚ It has definitely learned some
    things from from this game and isã€‚ is now aheadã€‚ So I'm not going look at the
    whole three minutes that I capturedã€‚ Who knowsã€‚ Maybe it maybe it reallyï¼Œ really
    gets betterã€‚ but let's look at the random agent And you'll see that the reinforcement
    learning that I took some time optimizing really does make it differentã€‚ I meanï¼Œ
    it's getting killedã€‚ It scored oneã€‚ I meanï¼Œ a broke clock is right twice a dayï¼Œ
    rightã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ã€‚å®ƒç»å¯¹æœ‰ä¸€äº›æŠ€å·§ã€‚å®ƒç¡®å®ä»è¿™ä¸ªæ¸¸æˆä¸­å­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ï¼Œå¹¶ä¸”ç°åœ¨é¢†å…ˆã€‚æ‰€ä»¥æˆ‘ä¸æƒ³çœ‹æˆ‘å½•åˆ¶çš„æ•´æ•´ä¸‰åˆ†é’Ÿã€‚è°çŸ¥é“å‘¢ï¼Ÿä¹Ÿè®¸å®ƒçœŸçš„ä¼šå˜å¾—æ›´å¥½ã€‚ä½†è®©æˆ‘ä»¬çœ‹çœ‹éšæœºä»£ç†ï¼Œä½ ä¼šå‘ç°æˆ‘èŠ±äº†ä¸€äº›æ—¶é—´ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ç¡®å®è®©å®ƒæœ‰æ‰€ä¸åŒã€‚æˆ‘æ˜¯è¯´ï¼Œå®ƒè¢«å‡»å€’äº†ã€‚å®ƒå¾—äº†ä¸€åˆ†ã€‚æˆ‘æ˜¯è¯´ï¼Œåé’Ÿä¹Ÿä¸€å¤©å¯¹ä¸¤æ¬¡ï¼Œå¯¹å§ã€‚
- en: So that's that's it playingï¼Œsï¼Œ it doesn't take nearly as many minutesã€‚ that's
    a good example right thereã€‚ Like howï¼Œ how long does it stay playing before you
    hit end of gameã€‚ I meanï¼Œ like when Iï¼Œ when I would go to arcades back in the 80sã€‚
    How long would my $2 worth of quarters last that my mom had given me in in my
    caseã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯æ¸¸æˆï¼Œå®ƒå¹¶æ²¡æœ‰èŠ±è´¹å¤ªå¤šæ—¶é—´ã€‚é‚£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚æ¯”å¦‚ï¼Œåœ¨æ¸¸æˆç»“æŸä¹‹å‰ï¼Œå®ƒèƒ½ç©å¤šä¹…ã€‚æˆ‘æ˜¯è¯´ï¼Œå½“æˆ‘åœ¨80å¹´ä»£å»æ¸¸æˆå…æ—¶ï¼Œæˆ‘çš„2ç¾å…ƒèƒ½ç”¨å¤šå°‘ä¸ªå­£åº¦ï¼Œæˆ‘å¦ˆå¦ˆç»™æˆ‘çš„ã€‚
- en: usually about5 minutesï¼Œ I was not that good at video games everã€‚ really like
    centipedeã€‚ That was probably my favorite Dragon's layer was probably the worseã€‚
    You had to spendï¼Œ I thinkã€‚ an entire dollar and I would get killed in like a minuteã€‚![](img/8a587b916463673ac1aabf070d00efab_10.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸å¤§çº¦5åˆ†é’Ÿï¼Œæˆ‘ä»æ¥ä¸å¤ªæ“…é•¿è§†é¢‘æ¸¸æˆã€‚çœŸçš„å¾ˆå–œæ¬¢èœˆèš£ã€‚è¿™å¯èƒ½æ˜¯æˆ‘æœ€å–œæ¬¢çš„ï¼Œé¾™ä¹‹å±‚å¯èƒ½æ˜¯æœ€ç³Ÿç³•çš„ã€‚ä½ å¿…é¡»èŠ±ï¼Œæˆ‘è®¤ä¸ºï¼Œæ•´æ•´ä¸€ç¾å…ƒï¼Œä½†æˆ‘ä¼šåœ¨ä¸€åˆ†é’Ÿå†…å°±è¢«æ€æ‰ã€‚![](img/8a587b916463673ac1aabf070d00efab_10.png)
- en: '![](img/8a587b916463673ac1aabf070d00efab_11.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a587b916463673ac1aabf070d00efab_11.png)'
- en: Thank you for watching my video and in the next one we're going to look at how
    to do the same sort of thing only with a financial simulationã€‚If you find this
    kind of thing interestingï¼Œ please subscribe to my channelã€‚ Thank you very muchã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢è§‚çœ‹æˆ‘çš„è§†é¢‘ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•é€šè¿‡è´¢åŠ¡æ¨¡æ‹ŸåšåŒæ ·çš„äº‹æƒ…ã€‚å¦‚æœä½ è§‰å¾—è¿™ç±»å†…å®¹æœ‰è¶£ï¼Œè¯·è®¢é˜…æˆ‘çš„é¢‘é“ã€‚éå¸¸æ„Ÿè°¢ã€‚
