- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P7ï¼šL1.7- Transformerï¼šç¼–ç å™¨-è§£ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠ NLP ä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P7ï¼šL1.7- Transformerï¼šç¼–ç å™¨-è§£ç å™¨ - ShowMeAI - BV1Jm4y1X7UL
- en: In this videoï¼Œ we'll study the encoder decoder architectureã€‚An example of a
    popular encoder decoder model is T5ã€‚In order to understand how the encoder decoder
    worksã€‚ we recommend you check out the videos on encoders and decoders as the standalone
    modelsã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚ä¸€ä¸ªæµè¡Œçš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ä¾‹å­æ˜¯ T5ã€‚ä¸ºäº†ç†è§£ç¼–ç å™¨-è§£ç å™¨çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨æŸ¥çœ‹å…³äºç¼–ç å™¨å’Œè§£ç å™¨çš„ç‹¬ç«‹æ¨¡å‹è§†é¢‘ã€‚
- en: Understanding how they work individually will help understanding how an encoder
    decoder worksã€‚Let's start from what we've seen about the encoderã€‚The encoder takes
    words as inputsã€‚ casts them through the encoderï¼Œ and retrieves a numerical representation
    for each word cast through itã€‚We now know that this numerical representation holds
    information about the meaning of the sequenceã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£å®ƒä»¬å„è‡ªçš„å·¥ä½œåŸç†å°†æœ‰åŠ©äºç†è§£ç¼–ç å™¨-è§£ç å™¨çš„å·¥ä½œåŸç†ã€‚è®©æˆ‘ä»¬ä»æˆ‘ä»¬æ‰€äº†è§£çš„ç¼–ç å™¨å¼€å§‹ã€‚ç¼–ç å™¨å°†å•è¯ä½œä¸ºè¾“å…¥ï¼Œç»è¿‡ç¼–ç å™¨å¤„ç†ï¼Œå¹¶ä¸ºæ¯ä¸ªç»è¿‡çš„å•è¯æ£€ç´¢ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚æˆ‘ä»¬ç°åœ¨çŸ¥é“è¿™ä¸ªæ•°å€¼è¡¨ç¤ºåŒ…å«å…³äºåºåˆ—æ„ä¹‰çš„ä¿¡æ¯ã€‚
- en: Let's put this aside and add the decoder to the diagramã€‚In this scenarioã€‚ we're
    using the decoder in a manner that we haven't seen beforeã€‚We're passing the outputs
    of the encoder directly to itã€‚Additionally to the encoder outputsã€‚ we also give
    the decoder a sequenceã€‚When prompting the decoder for an output with no initial
    sequenceã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŠŠè¿™ä¸ªæ”¾åˆ°ä¸€è¾¹ï¼Œç»™å›¾è¡¨æ·»åŠ è§£ç å™¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ã€‚æˆ‘ä»¬ä»¥ä¸€ç§å‰æ‰€æœªè§çš„æ–¹å¼ä½¿ç”¨è§£ç å™¨ã€‚æˆ‘ä»¬å°†ç¼–ç å™¨çš„è¾“å‡ºç›´æ¥ä¼ é€’ç»™å®ƒã€‚æ­¤å¤–ï¼Œé™¤äº†ç¼–ç å™¨è¾“å‡ºä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç»™è§£ç å™¨ä¸€ä¸ªåºåˆ—ã€‚å½“è¯·æ±‚è§£ç å™¨è¾“å‡ºæ²¡æœ‰åˆå§‹åºåˆ—æ—¶ã€‚
- en: we can give it the value that indicates the start of a sequenceã€‚ğŸ˜Šã€‚And that's
    where the anchor decor magic happensã€‚ğŸ˜Šï¼ŒThe encoder accepts a sequence as inputã€‚It
    computes a prediction and outputs a numerical representationã€‚ğŸ˜Šã€‚Then it sends that
    over to the decoderã€‚It hasï¼Œ in a senseï¼Œ encoded that sequenceã€‚And the decoderã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç»™å®ƒä¸€ä¸ªè¡¨ç¤ºåºåˆ—å¼€å§‹çš„å€¼ã€‚ğŸ˜Šã€‚è¿™å°±æ˜¯é”šç‚¹è£…é¥°é­”æ³•å‘ç”Ÿçš„åœ°æ–¹ã€‚ğŸ˜Šï¼Œç¼–ç å™¨æ¥å—ä¸€ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥ã€‚å®ƒè®¡ç®—ä¸€ä¸ªé¢„æµ‹å¹¶è¾“å‡ºä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ğŸ˜Šã€‚ç„¶åå°†å…¶å‘é€ç»™è§£ç å™¨ã€‚ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒå·²ç»å¯¹è¯¥åºåˆ—è¿›è¡Œäº†ç¼–ç ã€‚ç„¶åè§£ç å™¨ã€‚
- en: in turnï¼Œ using this input alongside its usual sequence input will take a stab
    at decoding the sequenceã€‚The decoder dedes a sequenceï¼Œ and outputs a wordã€‚As of
    nowã€‚ we don't really need to make sense of that wordï¼Œ but we can understand that
    the decoder is essentially decoding what the encoder has outputã€‚The start sequence
    hereï¼Œ the startup sequence word here indicates that it should start decoding the
    sequenceã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åè¿‡æ¥ï¼Œä½¿ç”¨è¿™ä¸ªè¾“å…¥å’Œå®ƒé€šå¸¸çš„åºåˆ—è¾“å…¥å°†å°è¯•è§£ç åºåˆ—ã€‚è§£ç å™¨è§£ç ä¸€ä¸ªåºåˆ—ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªå•è¯ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸éœ€è¦çœŸæ­£ç†è§£é‚£ä¸ªå•è¯ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç†è§£è§£ç å™¨æœ¬è´¨ä¸Šæ˜¯åœ¨è§£ç ç¼–ç å™¨çš„è¾“å‡ºã€‚è¿™é‡Œçš„å¼€å§‹åºåˆ—ï¼Œè¿™é‡Œå¯åŠ¨åºåˆ—çš„å•è¯è¡¨æ˜å®ƒåº”è¯¥å¼€å§‹è§£ç åºåˆ—ã€‚
- en: Now that we have both the encoder numerical representation and an initial generated
    wordã€‚ we don't need the encoder anymoreã€‚As we have seen before with the decoderã€‚
    it can act in an autoregressive mannerã€‚The word it has just output can now be
    used as an inputã€‚Thisã€‚ in combination with the numerical representation output
    by the encoderã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ç¼–ç å™¨çš„æ•°å€¼è¡¨ç¤ºå’Œä¸€ä¸ªåˆå§‹ç”Ÿæˆçš„å•è¯ã€‚æˆ‘ä»¬ä¸å†éœ€è¦ç¼–ç å™¨ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œè§£ç å™¨å¯ä»¥ä»¥è‡ªå›å½’çš„æ–¹å¼å·¥ä½œã€‚å®ƒåˆšåˆšè¾“å‡ºçš„å•è¯ç°åœ¨å¯ä»¥ç”¨ä½œè¾“å…¥ã€‚è¿™ä¸ç¼–ç å™¨è¾“å‡ºçš„æ•°å€¼è¡¨ç¤ºç›¸ç»“åˆã€‚
- en: can now be used to generate a second wordã€‚Please note that the first word is
    still here as the model still outputs itã€‚ Howeverï¼Œ we have grade it out as we
    have no need for it anymoreã€‚ğŸ˜Šï¼ŒWe can continue on and onã€‚ for exampleï¼Œ until the
    decoder outputs a value that we consider a stopping valueã€‚ like a dot meaning
    the end of a sequenceã€‚Here we've seen the full mechanism of the encoder decoder
    transformerã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¯ä»¥ç”¨æ¥ç”Ÿæˆç¬¬äºŒä¸ªå•è¯ã€‚è¯·æ³¨æ„ï¼Œç¬¬ä¸€ä¸ªå•è¯ä»ç„¶å­˜åœ¨ï¼Œå› ä¸ºæ¨¡å‹ä»ç„¶è¾“å‡ºå®ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å·²å°†å…¶ç°åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦å®ƒã€‚ğŸ˜Šï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­ä¸‹å»ã€‚ä¾‹å¦‚ï¼Œç›´åˆ°è§£ç å™¨è¾“å‡ºæˆ‘ä»¬è®¤ä¸ºçš„åœæ­¢å€¼ï¼Œæ¯”å¦‚ä¸€ä¸ªå¥å·ï¼Œè¡¨ç¤ºåºåˆ—çš„ç»“æŸã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ç¼–ç å™¨-è§£ç å™¨å˜æ¢å™¨çš„å®Œæ•´æœºåˆ¶ã€‚
- en: let's go over one more timeã€‚ We have an initial sequence that is sent to the
    encoderã€‚ğŸ˜Šã€‚That encoder output is then sent to the decoder for it to be decodedã€‚While
    it can now discard the encoder after a single useã€‚ the decoder will be used several
    times until we have generated every word that we needã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†å›é¡¾ä¸€æ¬¡ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå‘é€åˆ°ç¼–ç å™¨çš„åˆå§‹åºåˆ—ã€‚ğŸ˜Šã€‚ç„¶åï¼Œè¯¥ç¼–ç å™¨è¾“å‡ºè¢«å‘é€åˆ°è§£ç å™¨è¿›è¡Œè§£ç ã€‚è™½ç„¶åœ¨å•æ¬¡ä½¿ç”¨åå®ƒå¯ä»¥ä¸¢å¼ƒç¼–ç å™¨ï¼Œä½†è§£ç å™¨å°†è¢«å¤šæ¬¡ä½¿ç”¨ï¼Œç›´åˆ°ç”Ÿæˆæ‰€éœ€çš„æ¯ä¸€ä¸ªå•è¯ã€‚
- en: So let's see a concrete example with translation language modelingï¼Œ also called
    transductionã€‚ which is the act of translating a sequenceã€‚Here we would like to
    translate this English sequence welcome to NYYC in Frenchã€‚We're using a transformer
    model that is trained for that task explicitlyã€‚ we use the encoder to create a
    representation of the English sentenceã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œå…³äºç¿»è¯‘è¯­è¨€å»ºæ¨¡ï¼Œä¹Ÿç§°ä¸ºè½¬å¯¼ã€‚è¿™æ˜¯ç¿»è¯‘ä¸€ä¸ªåºåˆ—çš„è¡Œä¸ºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³æŠŠè¿™å¥è‹±è¯­åºåˆ—â€œwelcome to NYYCâ€ç¿»è¯‘æˆæ³•è¯­ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¸“é—¨ä¸ºè¿™ä¸ªä»»åŠ¡è®­ç»ƒçš„å˜æ¢å™¨æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ç¼–ç å™¨æ¥åˆ›å»ºè‹±è¯­å¥å­çš„è¡¨ç¤ºã€‚
- en: we cast this to the decoder with the use of the start sequence wordã€‚ we ask
    it to output the first wordã€‚It outputs B avenueï¼Œ which means welcomeã€‚And we then
    use B avenue as the input sequence for the decoderã€‚This alongside the encoder
    numerical representationï¼Œ allows the decoder to predict the second word aã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä½¿ç”¨èµ·å§‹åºåˆ—å•è¯å°†å…¶ä¼ é€’ç»™è§£ç å™¨ã€‚æˆ‘ä»¬è¯·å®ƒè¾“å‡ºç¬¬ä¸€ä¸ªå•è¯ã€‚å®ƒè¾“å‡ºâ€œB avenueâ€ï¼Œè¿™æ„å‘³ç€æ¬¢è¿ã€‚ç„¶åæˆ‘ä»¬å°†â€œB avenueâ€ä½œä¸ºè§£ç å™¨çš„è¾“å…¥åºåˆ—ã€‚è¿™ä¸ªä¸ç¼–ç å™¨çš„æ•°å€¼è¡¨ç¤ºç»“åˆï¼Œå…è®¸è§£ç å™¨é¢„æµ‹ç¬¬äºŒä¸ªå•è¯â€œaâ€ã€‚
- en: which is two in Englishã€‚ğŸ˜Šï¼ŒFinallyï¼Œ we ask the decoder to predict a third wordï¼Œ
    it predicts NYCã€‚ which is correctï¼Œ we've translated the sentenceã€‚Where the encoder
    decoder really shines is that we have an encoder and a decoderã€‚ which often do
    not share weightsã€‚Thereforeï¼Œ we have an entire blockã€‚ the encoder that can be
    trained to understand the sequence and extract the relevant informationã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‹±è¯­ä¸­ï¼Œè¿™æ˜¯ä¸¤ä¸ªå•è¯ã€‚ğŸ˜Š æœ€åï¼Œæˆ‘ä»¬è¯·è§£ç å™¨é¢„æµ‹ç¬¬ä¸‰ä¸ªå•è¯ï¼Œå®ƒé¢„æµ‹äº†â€œNYCâ€ã€‚è¿™æ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä»¬å·²ç»ç¿»è¯‘äº†è¿™ä¸ªå¥å­ã€‚ç¼–ç å™¨å’Œè§£ç å™¨çœŸæ­£å‡ºè‰²çš„åœ°æ–¹åœ¨äºï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç¼–ç å™¨å’Œä¸€ä¸ªè§£ç å™¨ï¼Œå®ƒä»¬é€šå¸¸ä¸å…±äº«æƒé‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå®Œæ•´çš„å—ï¼Œå³ç¼–ç å™¨ï¼Œå¯ä»¥è¢«è®­ç»ƒæ¥ç†è§£åºåˆ—å¹¶æå–ç›¸å…³ä¿¡æ¯ã€‚
- en: For the translation scenario we've seen earlierï¼Œ for exampleã€‚ this would mean
    parsing and understanding what was said in the English languageã€‚It would mean
    extracting information from that language and putting all of that in a vector
    dense in informationã€‚ğŸ˜Šï¼ŒOn the other handï¼Œ we have the decoder whose sole purpose
    is to decode the numerical representation output by the encoderã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ç¿»è¯‘åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œè¿™æ„å‘³ç€è§£æå’Œç†è§£ç”¨è‹±è¯­è¯´çš„å†…å®¹ã€‚è¿™å°†æ„å‘³ç€ä»è¯¥è¯­è¨€ä¸­æå–ä¿¡æ¯ï¼Œå¹¶å°†æ‰€æœ‰è¿™äº›ä¿¡æ¯æ”¾å…¥ä¸€ä¸ªä¿¡æ¯å¯†é›†çš„å‘é‡ä¸­ã€‚ğŸ˜Š å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æœ‰è§£ç å™¨ï¼Œå…¶å”¯ä¸€ç›®çš„æ˜¯è§£ç ç¼–ç å™¨è¾“å‡ºçš„æ•°å€¼è¡¨ç¤ºã€‚
- en: This decoder can be specialized in a completely different language or even modality
    like images or speechã€‚Encosï¼Œ decoders are special for several reasonsã€‚Firstlyã€‚
    they are able to manage sequence to sequence tasks like translation that we have
    just seenã€‚Secondlyã€‚ the weights between the encoder and the decoder parts are
    not necessarily sharedã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè§£ç å™¨å¯ä»¥ä¸“é—¨ç”¨äºå®Œå…¨ä¸åŒçš„è¯­è¨€ï¼Œç”šè‡³æ˜¯å›¾åƒæˆ–è¯­éŸ³ç­‰æ¨¡æ€ã€‚ç¼–ç å™¨å’Œè§£ç å™¨æœ‰å‡ ä¸ªç‰¹åˆ«çš„åŸå› ã€‚é¦–å…ˆï¼Œå®ƒä»¬èƒ½å¤Ÿç®¡ç†åƒæˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„ç¿»è¯‘è¿™æ ·çš„åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†ä¹‹é—´çš„æƒé‡ä¸ä¸€å®šæ˜¯å…±äº«çš„ã€‚
- en: Let's take another example of translationã€‚Here where translating transformers
    are powerful in Frenchã€‚Firstlyï¼Œ this means that from a sequence of three wordsã€‚
    we're able to generate a sequence of four wordsã€‚One could argue that this could
    be handled with a decoder that would generate the translation in an autoregressive
    mannerã€‚And they would be rightã€‚Another example of where sequence to sequence transformers
    shine is in summarizationã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†ä¸¾ä¸€ä¸ªç¿»è¯‘çš„ä¾‹å­ã€‚åœ¨è¿™é‡Œï¼Œç¿»è¯‘å˜æ¢å™¨åœ¨æ³•è¯­ä¸­å¾ˆå¼ºå¤§ã€‚é¦–å…ˆï¼Œè¿™æ„å‘³ç€ä»ä¸‰ä¸ªå•è¯çš„åºåˆ—ä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆå››ä¸ªå•è¯çš„åºåˆ—ã€‚æœ‰äººå¯èƒ½ä¼šäº‰è¾©è¯´ï¼Œè¿™å¯ä»¥é€šè¿‡ä¸€ä¸ªä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆç¿»è¯‘çš„è§£ç å™¨æ¥å¤„ç†ã€‚ä»–ä»¬æ˜¯å¯¹çš„ã€‚å¦ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—å˜æ¢å™¨å‡ºè‰²çš„ä¾‹å­æ˜¯æ‘˜è¦ç”Ÿæˆã€‚
- en: Here we have very very long sequenceï¼Œ generally a full textï¼Œ and we want to
    summarize itã€‚Since if the encoder and decoders are separatedï¼Œ we can have different
    context lengthsï¼Œ for exampleã€‚ a very long context for the encoder which handles
    the text and a smaller context for the decoderã€‚ which handles the summarized sequenceã€‚There
    are a lot of sequence to sequence modelsã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰éå¸¸éå¸¸é•¿çš„åºåˆ—ï¼Œä¸€èˆ¬æ˜¯ä¸€æ•´ç¯‡æ–‡æœ¬ï¼Œæˆ‘ä»¬æƒ³è¦å¯¹å…¶è¿›è¡Œæ€»ç»“ã€‚ç”±äºç¼–ç å™¨å’Œè§£ç å™¨æ˜¯åˆ†å¼€çš„ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä¾‹å¦‚ï¼Œç¼–ç å™¨å¤„ç†æ–‡æœ¬æ—¶çš„éå¸¸é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä»¥åŠè§£ç å™¨å¤„ç†æ‘˜è¦åºåˆ—æ—¶çš„è¾ƒå°ä¸Šä¸‹æ–‡ã€‚åºåˆ—åˆ°åºåˆ—æ¨¡å‹æœ‰å¾ˆå¤šã€‚
- en: This contains a few examples of popular encoder decoder models available in
    the Transformers libraryã€‚Additionallyï¼Œ you can load an encoder and a decoder inside
    an encoder decoder modelã€‚ğŸ˜Šï¼ŒThereforeã€‚ according to the specific task you are targetingï¼Œ
    you may choose to use specific encoders and decoders which have proven their worth
    on these specific tasksã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŒ…å«äº†ä¸€äº›åœ¨å˜æ¢å™¨åº“ä¸­æµè¡Œçš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œä½ å¯ä»¥åœ¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ä¸­åŠ è½½ç¼–ç å™¨å’Œè§£ç å™¨ã€‚ğŸ˜Š å› æ­¤ï¼Œæ ¹æ®ä½ æ‰€é’ˆå¯¹çš„ç‰¹å®šä»»åŠ¡ï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨åœ¨è¿™äº›ç‰¹å®šä»»åŠ¡ä¸Šè¯æ˜å…¶ä»·å€¼çš„ç‰¹å®šç¼–ç å™¨å’Œè§£ç å™¨ã€‚
- en: '![](img/cce5d73b05cbf648b5982557ff9b6810_1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cce5d73b05cbf648b5982557ff9b6810_1.png)'
- en: ã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/cce5d73b05cbf648b5982557ff9b6810_3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cce5d73b05cbf648b5982557ff9b6810_3.png)'
