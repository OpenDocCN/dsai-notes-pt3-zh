- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘140åˆ†é’Ÿå…¥é—¨ PyTorchï¼Œå®˜æ–¹æ•™ç¨‹æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒç¬¬ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- Autogradçš„åŸºç¡€çŸ¥è¯†
    - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘140åˆ†é’Ÿå…¥é—¨PyTorchï¼Œå®˜æ–¹æ•™ç¨‹æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒç¬¬ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼šL3- Autogradçš„åŸºç¡€çŸ¥è¯†
    - ShowMeAI - BV19L4y1t7tu
- en: Hiï¼Œ and welcome back to the Pytorrch trainingin video seriesã€‚ In this videoã€‚
    we're going to cover autographã€‚ just Pytorrch's toolning for rapidly and dynamically
    computing the gradients that drive back propagation based learning in your machine
    learning modelã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæ¬¢è¿å›åˆ°PyTorchè®­ç»ƒè§†é¢‘ç³»åˆ—ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºautogradï¼Œå³PyTorchç”¨äºå¿«é€ŸåŠ¨æ€è®¡ç®—é©±åŠ¨åŸºäºåå‘ä¼ æ’­å­¦ä¹ çš„æ¢¯åº¦çš„å·¥å…·ã€‚
- en: ğŸ˜Šï¼ŒIn particularï¼Œ we're going to go over what Autograd does for youã€‚And why it
    makes Piytorrch so flexible and powerful for machine learningã€‚We'll walk through
    a basic code example to you feel for what autograta is doing under the hoodã€‚Then
    we'll see autograd's role in a training loopã€‚After thatã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†è®¨è®ºAutogradä¸ºä½ åšäº†ä»€ä¹ˆï¼Œä»¥åŠå®ƒä¸ºä½•ä½¿PyTorchåœ¨æœºå™¨å­¦ä¹ ä¸­å¦‚æ­¤çµæ´»å’Œå¼ºå¤§ã€‚æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªåŸºæœ¬çš„ä»£ç ç¤ºä¾‹æ¥äº†è§£autogradåœ¨èƒŒååšäº†ä»€ä¹ˆã€‚ç„¶åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°autogradåœ¨è®­ç»ƒå¾ªç¯ä¸­çš„ä½œç”¨ã€‚ä¹‹åã€‚
- en: we'll talk about why and how to turn the autograd feature off and on for a particular
    tensor or a particular contextã€‚We'll see the autograd profiler in actionï¼Œ and
    we'll look at the autograrad high level API that was released with Pytor 1ã€‚5ã€‚![](img/25eee23f7825074bf201a2a5a23d28cb_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¨è®ºä¸ºä»€ä¹ˆä»¥åŠå¦‚ä½•åœ¨ç‰¹å®šå¼ é‡æˆ–ç‰¹å®šä¸Šä¸‹æ–‡ä¸­æ‰“å¼€æˆ–å…³é—­autogradåŠŸèƒ½ã€‚æˆ‘ä»¬å°†çœ‹åˆ°autogradåˆ†æå™¨çš„å®é™…åº”ç”¨ï¼Œå¹¶æŸ¥çœ‹ä¸PyTorch 1.5ä¸€èµ·å‘å¸ƒçš„autogradé«˜çº§APIï¼[](img/25eee23f7825074bf201a2a5a23d28cb_1.png)
- en: The autograd feature of Pytorrch is a large part of what makes Piytorrch a fast
    and flexible framework for building deep learning projectsã€‚It does this by easing
    the computation of the partial derivativesã€‚ also called gradients that drive back
    propagation based learningã€‚I'm not going to belabor the math hereï¼Œ although if
    you'd like a refresherã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„autogradåŠŸèƒ½æ˜¯ä½¿PyTorchæˆä¸ºä¸€ä¸ªå¿«é€Ÿçµæ´»çš„æ·±åº¦å­¦ä¹ é¡¹ç›®æ„å»ºæ¡†æ¶çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å®ƒé€šè¿‡ç®€åŒ–éƒ¨åˆ†å¯¼æ•°çš„è®¡ç®—ï¼ˆä¹Ÿç§°ä¸ºé©±åŠ¨åå‘ä¼ æ’­å­¦ä¹ çš„æ¢¯åº¦ï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè¯¦ç»†è®¨è®ºæ•°å­¦ï¼Œå°½ç®¡å¦‚æœä½ æƒ³è¦å¤ä¹ ä¸€ä¸‹çš„è¯ã€‚
- en: go ahead and download the notebook and follow along in detailã€‚The important
    concept here is that when we're training our modelï¼Œ we compute a loss functionã€‚
    which tells us how far our model's prediction is from the idealã€‚We then need to
    find the partial derivatives of the loss function with respect to the model's
    learning weightsã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ä¸‹è½½ç¬”è®°æœ¬ï¼Œå¹¶è¯¦ç»†è·Ÿéšã€‚è¿™é‡Œçš„é‡è¦æ¦‚å¿µæ˜¯ï¼Œå½“æˆ‘ä»¬è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è®¡ç®—æŸå¤±å‡½æ•°ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬æ¨¡å‹çš„é¢„æµ‹ä¸ç†æƒ³æƒ…å†µç›¸å·®å¤šè¿œã€‚ç„¶åï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å­¦ä¹ æƒé‡çš„åå¯¼æ•°ã€‚
- en: These derivatives tell us in what direction we have to adjust the weights in
    order to minimize the lossã€‚ This involves the iterative application of the chain
    rule of differential calculus over every path through the computationã€‚Autograd
    makes this computation faster by tracing your computation at runtimeã€‚Every output
    tensor from your model's computation carries with it a history of the operations
    that LED to itã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯¼æ•°å‘Šè¯‰æˆ‘ä»¬éœ€è¦åœ¨å“ªä¸ªæ–¹å‘ä¸Šè°ƒæ•´æƒé‡ï¼Œä»¥æœ€å°åŒ–æŸå¤±ã€‚è¿™æ¶‰åŠåœ¨è®¡ç®—çš„æ¯ä¸ªè·¯å¾„ä¸Šè¿­ä»£åº”ç”¨å¾®ç§¯åˆ†çš„é“¾å¼æ³•åˆ™ã€‚Autogradé€šè¿‡åœ¨è¿è¡Œæ—¶è·Ÿè¸ªè®¡ç®—ï¼Œä½¿è¿™ä¸ªè¿‡ç¨‹æ›´å¿«ã€‚æ¥è‡ªæ¨¡å‹è®¡ç®—çš„æ¯ä¸ªè¾“å‡ºå¼ é‡éƒ½æºå¸¦ç€å¯¼è‡´å…¶äº§ç”Ÿçš„æ“ä½œå†å²ã€‚
- en: This history allows the rapid computation of derivatives over the graphã€‚All
    the way back to your model's learning weightsã€‚In additionã€‚ because this history
    is gathered at runtimeï¼Œ you'll get the correct derivativesã€‚ even if your model
    has a dynamic structure with decision branches and loopsã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå†å²å…è®¸åœ¨å›¾ä¸Šå¿«é€Ÿè®¡ç®—å¯¼æ•°ï¼Œç›´åˆ°æ¨¡å‹çš„å­¦ä¹ æƒé‡ã€‚æ­¤å¤–ï¼Œç”±äºè¿™ä¸ªå†å²æ˜¯åœ¨è¿è¡Œæ—¶æ”¶é›†çš„ï¼Œå³ä½¿æ¨¡å‹å…·æœ‰åŠ¨æ€ç»“æ„ã€å†³ç­–åˆ†æ”¯å’Œå¾ªç¯ï¼Œä½ ä¹Ÿèƒ½è·å¾—æ­£ç¡®çš„å¯¼æ•°ã€‚
- en: This offers a lot of flexibility over tools that depend on analysis of a static
    computation rackã€‚Let's have a look at a simple example of autograd in actionã€‚Firstã€‚
    we'll import pie torch and matte plot lib so we can graph some stuffã€‚Nextã€‚ we'll
    make a one dimensional tensor holding a bunch of values between0 and 2 piã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ºä¾èµ–äºé™æ€è®¡ç®—å›¾åˆ†æçš„å·¥å…·æä¾›äº†å¾ˆå¤§çš„çµæ´»æ€§ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªautogradå®é™…åº”ç”¨çš„ç®€å•ç¤ºä¾‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å¯¼å…¥PyTorchå’ŒMatplotlibï¼Œä»¥ä¾¿å¯ä»¥ç»˜åˆ¶ä¸€äº›å›¾å½¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œå­˜å‚¨ä»0åˆ°2Ï€ä¹‹é—´çš„ä¸€äº›å€¼ã€‚
- en: and we'll add the requires grad equals true flatã€‚Note that when we print Aã€‚
    pitorrch lets us know that A wants gradients computed on any calculation it's
    involved inã€‚Now we'll perform a computation here we'll just take the sign of all
    the values in A and we'll graph that and that looks rightã€‚If you notice the calls
    that attach hereï¼Œ I'll be covering those later in the section on turning autograd
    on and offã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ·»åŠ  requires_grad=True æ ‡å¿—ã€‚è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬æ‰“å° A æ—¶ï¼ŒPytorrch è®©æˆ‘ä»¬çŸ¥é“ A å¸Œæœ›åœ¨ä»»ä½•å‚ä¸çš„è®¡ç®—ä¸­è®¡ç®—æ¢¯åº¦ã€‚ç°åœ¨æˆ‘ä»¬å°†åœ¨è¿™é‡Œæ‰§è¡Œä¸€ä¸ªè®¡ç®—ï¼Œæˆ‘ä»¬å°†å–
    A ä¸­æ‰€æœ‰å€¼çš„æ­£å¼¦ï¼Œå¹¶å°†å…¶å›¾å½¢åŒ–ï¼Œè¿™çœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚å¦‚æœä½ æ³¨æ„åˆ°è¿™é‡Œçš„è°ƒç”¨ï¼Œæˆ‘å°†åœ¨ç¨åçš„éƒ¨åˆ†ä¸­è®¨è®ºå¦‚ä½•æ‰“å¼€å’Œå…³é—­ autogradã€‚
- en: So printing a tensor beã€‚We see that Pytorrch tells us it has a grad functionã€‚
    This means that B came from a computation where at least one of the inputs required
    the calculation of gradientsã€‚ The grad function tells us that B came from the
    s operationã€‚Let's perform a couple of more stepsã€‚ we'll double be and add one
    to it and we do thisï¼Œ we see that the output tensors again contain information
    about the operations that generated them into the grad function propertyã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ‰“å°ä¸€ä¸ªå¼ é‡æ˜¯å¾ˆç®€å•çš„ã€‚æˆ‘ä»¬çœ‹åˆ° Pytorrch å‘Šè¯‰æˆ‘ä»¬å®ƒæœ‰ä¸€ä¸ª grad å‡½æ•°ã€‚è¿™æ„å‘³ç€ B æ¥è‡ªä¸€ä¸ªè®¡ç®—ï¼Œå…¶ä¸­è‡³å°‘æœ‰ä¸€ä¸ªè¾“å…¥éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚grad
    å‡½æ•°å‘Šè¯‰æˆ‘ä»¬ B æ¥è‡ª s æ“ä½œã€‚è®©æˆ‘ä»¬å†è¿›è¡Œå‡ æ­¥ã€‚æˆ‘ä»¬å°†å¯¹ B è¿›è¡ŒåŠ å€å¹¶åŠ ä¸€ï¼Œå½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°è¾“å‡ºå¼ é‡å†æ¬¡åŒ…å«ç”Ÿæˆå®ƒä»¬çš„æ“ä½œçš„ä¿¡æ¯åˆ° grad
    å‡½æ•°å±æ€§ä¸­ã€‚
- en: By defaultï¼Œ Autograd expects the final function in a gradient computation to
    be single valueã€‚This is the case when we're computing the derivatives over learning
    weightsã€‚ The loss function has a single scalar value for its outputã€‚It doesn't
    strictly have to be a single valueï¼Œ but we'll go over that in a minuteã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼ŒAutograd æœŸæœ›æ¢¯åº¦è®¡ç®—ä¸­çš„æœ€ç»ˆå‡½æ•°æ˜¯å•ä¸ªå€¼ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨æˆ‘ä»¬è®¡ç®—å­¦ä¹ æƒé‡çš„å¯¼æ•°æ—¶ã€‚æŸå¤±å‡½æ•°çš„è¾“å‡ºæ˜¯å•ä¸ªæ ‡é‡å€¼ã€‚å®ƒå¹¶ä¸ä¸¥æ ¼éœ€è¦æ˜¯å•ä¸ªå€¼ï¼Œä½†æˆ‘ä»¬ç¨åä¼šå¯¹æ­¤è¿›è¡Œè®¨è®ºã€‚
- en: Here we'll just sum the elements of the tensor and call that the final output
    for this computationã€‚We can actually use the grad function property of any output
    or intermediate tensor to walk back to the beginning of the computational history
    using the grad functions's next functions propertyã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬å°†ç®€å•åœ°å¯¹å¼ é‡çš„å…ƒç´ æ±‚å’Œï¼Œå¹¶å°†å…¶ç§°ä¸ºè¯¥è®¡ç®—çš„æœ€ç»ˆè¾“å‡ºã€‚æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ä½¿ç”¨ä»»ä½•è¾“å‡ºæˆ–ä¸­é—´å¼ é‡çš„ grad å‡½æ•°å±æ€§ï¼Œå‘å›èµ°åˆ°è®¡ç®—å†å²çš„å¼€å¤´ï¼Œåˆ©ç”¨
    grad å‡½æ•°çš„ next å‡½æ•°å±æ€§ã€‚
- en: Here you can see that D knows it came from an addition operationã€‚ which knows
    it came from a multiplication operationï¼Œ so I'm back to Aã€‚A does not have a gridd
    functionï¼Œ it is an input or leaf node of this computation graphã€‚ and so represents
    the target variables for which we want to compute the gradientsã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°ï¼ŒD çŸ¥é“å®ƒæ¥è‡ªä¸€ä¸ªåŠ æ³•æ“ä½œï¼Œè€Œ B çŸ¥é“å®ƒæ¥è‡ªä¸€ä¸ªä¹˜æ³•æ“ä½œï¼Œæ‰€ä»¥æˆ‘åˆå›åˆ°äº† Aã€‚A æ²¡æœ‰ grad å‡½æ•°ï¼Œå®ƒæ˜¯è¿™ä¸ªè®¡ç®—å›¾çš„è¾“å…¥æˆ–å¶èŠ‚ç‚¹ï¼Œå› æ­¤è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›è®¡ç®—æ¢¯åº¦çš„ç›®æ ‡å˜é‡ã€‚
- en: So we've looked a little at the history trackingï¼Œ but how do we actually compute
    gradientsï¼Ÿ
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»ç¨å¾®çœ‹äº†ä¸€ä¸‹å†å²è¿½è¸ªï¼Œä½†æˆ‘ä»¬å®é™…ä¸Šæ˜¯å¦‚ä½•è®¡ç®—æ¢¯åº¦çš„ï¼Ÿ
- en: That's easyï¼Œ just call the backward method on the output Tensor like soã€‚Looking
    back over the computationï¼Œ we have a sineï¼Œ the derivative of which is cosineã€‚We
    have the multiplication by twoï¼Œ which should add a factor of two to our gradientã€‚In
    the additionã€‚ which should not change the derivative at allã€‚Graphing the grad
    property on Aï¼Œ we see thatï¼Œ in factã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆç®€å•ï¼Œåªéœ€å¯¹è¾“å‡ºå¼ é‡è°ƒç”¨ backward æ–¹æ³•ã€‚å›é¡¾è®¡ç®—ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ­£å¼¦å‡½æ•°ï¼Œå…¶å¯¼æ•°æ˜¯ä½™å¼¦ã€‚æˆ‘ä»¬æœ‰ä¹˜ä»¥äºŒçš„æ“ä½œï¼Œè¿™åº”è¯¥ç»™æˆ‘ä»¬çš„æ¢¯åº¦åŠ ä¸Šä¸€ä¸ªäºŒçš„å› å­ã€‚åœ¨åŠ æ³•ä¸­ï¼Œè¿™ä¸åº”è¯¥æ”¹å˜å¯¼æ•°ã€‚å¯¹
    A çš„ grad å±æ€§è¿›è¡Œå›¾å½¢åŒ–ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼Œå®é™…ä¸Šã€‚
- en: the compute gradients are twice the cosineã€‚Be aware that gradients are only
    computed for inputs or leaf nodes of the computationã€‚ intermediate tensors will
    not have gradients attached after the back passã€‚So we peeked into the hood at
    how autogradã€‚Computes gradients in a simple caseã€‚Nextã€‚ we'll examine this role
    in the training loop of a Ptorrch modelã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å‡ºçš„æ¢¯åº¦æ˜¯ä½™å¼¦çš„ä¸¤å€ã€‚è¯·æ³¨æ„ï¼Œæ¢¯åº¦ä»…ä¸ºè®¡ç®—çš„è¾“å…¥æˆ–å¶èŠ‚ç‚¹è®¡ç®—ã€‚ä¸­é—´å¼ é‡åœ¨åå‘ä¼ é€’åå°†ä¸ä¼šé™„å¸¦æ¢¯åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çª¥æ¢äº† autograd å¦‚ä½•åœ¨ä¸€ä¸ªç®€å•çš„æ¡ˆä¾‹ä¸­è®¡ç®—æ¢¯åº¦ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å…¶åœ¨
    Ptorrch æ¨¡å‹è®­ç»ƒå¾ªç¯ä¸­çš„ä½œç”¨ã€‚
- en: To see how autograd works in your trainingï¼Œ let's build a small model and watch
    how it changes over a single training batchã€‚We'll define and instantiate a model
    and create some standard in tensors for the trainingã€‚ input and ideal outputã€‚You
    may have seen that we did not specify requires bright equals true for the model's
    layers within a subclass of Torchã€‚ NN dot moduleï¼Œ the gradient tracking is managed
    for youã€‚If we look at the layers of the modelã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£è‡ªåŠ¨æ±‚å¯¼åœ¨è®­ç»ƒä¸­çš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå°æ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿå®ƒåœ¨å•ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­çš„å˜åŒ–ã€‚æˆ‘ä»¬å°†å®šä¹‰å¹¶å®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸ºè®­ç»ƒåˆ›å»ºä¸€äº›æ ‡å‡†è¾“å…¥å¼ é‡å’Œç†æƒ³è¾“å‡ºã€‚ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬æ²¡æœ‰åœ¨Torchçš„NNæ¨¡å—å­ç±»ä¸­æŒ‡å®š`requires_grad=True`ï¼Œæ¢¯åº¦è·Ÿè¸ªä¼šä¸ºä½ ç®¡ç†ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ¨¡å‹çš„å±‚ã€‚
- en: you can see the randomly initialized weights and that they have no gradients
    computed yetã€‚You might have noticed there's a grad function on the weights I sampledã€‚
    There's no grad function on the weights themselves because they're a leaf node
    of the computation graphã€‚But the slice operation counts as a differentiable operationã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°éšæœºåˆå§‹åŒ–çš„æƒé‡ï¼Œä¸”å®ƒä»¬å°šæœªè®¡ç®—å‡ºæ¢¯åº¦ã€‚ä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œæƒé‡ä¸Šæœ‰ä¸€ä¸ª`grad`å‡½æ•°ã€‚æˆ‘é‡‡æ ·çš„æƒé‡ä¸Šæ²¡æœ‰`grad`å‡½æ•°ï¼Œå› ä¸ºå®ƒä»¬æ˜¯è®¡ç®—å›¾çš„å¶èŠ‚ç‚¹ã€‚ä½†åˆ‡ç‰‡æ“ä½œç®—ä½œä¸€ä¸ªå¯å¾®æ“ä½œã€‚
- en: So my little slice of the weights is a grad function indicating that it came
    from the sliceã€‚So let's see how this changes after one training batchã€‚For a loss
    functionã€‚ we'll use the square of the Euclidean distance between our prediction
    and our ideal outputã€‚We'll also set up a basic optimizer using stochastic gradient
    descentã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘çš„å°æƒé‡åˆ‡ç‰‡ä¸Šæœ‰ä¸€ä¸ª`grad`å‡½æ•°ï¼Œè¡¨ç¤ºå®ƒæ¥è‡ªåˆ‡ç‰‡ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åœ¨ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡åè¿™å¦‚ä½•å˜åŒ–ã€‚å¯¹äºæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é¢„æµ‹å€¼ä¸ç†æƒ³è¾“å‡ºä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»çš„å¹³æ–¹ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™è®¾ç½®ä¸€ä¸ªåŸºæœ¬ä¼˜åŒ–å™¨ã€‚
- en: Note that we initialize the optimizer with the model's learning weights of parametersã€‚
    The optimizer is responsible for adjusting the weightsã€‚So what happens when we
    call lost do backwardï¼ŸWe can see that the weights have not changedã€‚ but that we
    do have gradients computedã€‚ Againï¼Œ these gradients guide the optimizer in determining
    how to adjust the weights to minimize the loss scoreã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹çš„å­¦ä¹ æƒé‡åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€‚ä¼˜åŒ–å™¨è´Ÿè´£è°ƒæ•´æƒé‡ã€‚é‚£ä¹ˆï¼Œå½“æˆ‘ä»¬è°ƒç”¨`loss.backward()`æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æƒé‡æ²¡æœ‰å˜åŒ–ï¼Œä½†ç¡®å®è®¡ç®—å‡ºäº†æ¢¯åº¦ã€‚è¿™äº›æ¢¯åº¦æŒ‡å¯¼ä¼˜åŒ–å™¨ç¡®å®šå¦‚ä½•è°ƒæ•´æƒé‡ä»¥æœ€å°åŒ–æŸå¤±è¯„åˆ†ã€‚
- en: In order to actually update the weightsï¼Œ we have to call optimizerã€‚t stepã€‚And
    we can see that the rates have changedã€‚This is how learning happens in your pie
    torch modelsã€‚There's one more important step in the processã€‚After you call optimizerã€‚stepã€‚
    you need to call optimizerã€‚0rogradã€‚If you don'tï¼Œ the gradients will accumulate
    over every training batchã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®é™…æ›´æ–°æƒé‡ï¼Œæˆ‘ä»¬å¿…é¡»è°ƒç”¨`optimizer.step()`ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æƒé‡å‘ç”Ÿäº†å˜åŒ–ã€‚è¿™å°±æ˜¯ä½ åœ¨PyTorchæ¨¡å‹ä¸­å­¦ä¹ çš„æ–¹å¼ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­è¿˜æœ‰ä¸€æ­¥é‡è¦çš„æ­¥éª¤ã€‚åœ¨è°ƒç”¨`optimizer.step()`åï¼Œä½ éœ€è¦è°ƒç”¨`optimizer.zero_grad()`ã€‚å¦‚æœä¸è¿™æ ·åšï¼Œæ¢¯åº¦å°†ä¼šåœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­ç´¯ç§¯ã€‚
- en: For exampleï¼Œ if we run a training batch five times without calling a zero gradã€‚You
    can see the gradients turn up mostly with much larger magnitudes because they
    were accumulated over each batchã€‚And you can see the calling the zero grad resets
    the gradientsã€‚If your model' is not learning or training is giving you strange
    resultsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬åœ¨ä¸è°ƒç”¨`zero_grad`çš„æƒ…å†µä¸‹è¿è¡Œä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡äº”æ¬¡ï¼Œä½ ä¼šå‘ç°æ¢¯åº¦çš„å¹…åº¦å˜å¾—æ›´å¤§ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­ç´¯ç§¯çš„ã€‚ä½ å¯ä»¥çœ‹åˆ°è°ƒç”¨`zero_grad`ä¼šé‡ç½®æ¢¯åº¦ã€‚å¦‚æœä½ çš„æ¨¡å‹æ²¡æœ‰å­¦ä¹ æˆ–è®­ç»ƒç»™å‡ºå¥‡æ€ªçš„ç»“æœã€‚
- en: the very first thing you should check is whether you're calling zero grad after
    each training stepã€‚Sometimes you'll want to control whether gradients get tracked
    for a calculationã€‚ There are multiple ways to do thisï¼Œ depending on the situationã€‚
    The easiest is just to set the acquire grad flag directlyï¼Œ like soã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥æ£€æŸ¥çš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä½ æ˜¯å¦åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤åè°ƒç”¨äº†`zero_grad`ã€‚æœ‰æ—¶ä½ å¯èƒ½æƒ³æ§åˆ¶æŸä¸ªè®¡ç®—çš„æ¢¯åº¦è·Ÿè¸ªã€‚æœ‰å¤šç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œå…·ä½“å–å†³äºæƒ…å†µã€‚æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ç›´æ¥è®¾ç½®`requires_grad`æ ‡å¿—ï¼Œå¦‚æ­¤ã€‚
- en: And we can see that B1 has a grad functionï¼Œ but B2 does not because we turned
    off history tracking in A prior to computing B2ã€‚If you only need autograd turned
    off temporarilyï¼Œ you can use the Torchã€‚ Norad Con Managerã€‚When we run this cellã€‚We
    can see that history is tracked for all computationsã€‚ except the one in the nograd
    contextã€‚ Nograd can also be used as a function or method decoratorã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°B1æœ‰ä¸€ä¸ª`grad`å‡½æ•°ï¼Œä½†B2æ²¡æœ‰ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è®¡ç®—B2ä¹‹å‰å…³é—­äº†å†å²è·Ÿè¸ªã€‚å¦‚æœä½ åªéœ€è¦æš‚æ—¶å…³é—­è‡ªåŠ¨æ±‚å¯¼ï¼Œå¯ä»¥ä½¿ç”¨Torchçš„`no_grad`ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚å½“æˆ‘ä»¬è¿è¡Œè¿™ä¸ªå•å…ƒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ‰€æœ‰è®¡ç®—çš„å†å²éƒ½ä¼šè¢«è·Ÿè¸ªï¼Œé™¤äº†åœ¨`no_grad`ä¸Šä¸‹æ–‡ä¸­çš„é‚£ä¸€ä¸ªã€‚`no_grad`ä¹Ÿå¯ä»¥ç”¨ä½œå‡½æ•°æˆ–æ–¹æ³•çš„è£…é¥°å™¨ã€‚
- en: causing history tracking to be turned off for computations inside the decorated
    functionã€‚It's a corresponding context managerï¼Œ Torchdot and Agradã€‚For turning
    autograd on in a local contextã€‚ it may also be used as a decoratorã€‚Finallyï¼Œ you
    may have a tensor tracking historyã€‚ but you need a copy that doesn'tã€‚In this caseï¼Œ
    the Tensor object has a detached method which creates a copy of the tensor that
    is detached from the computation historyã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šå¯¼è‡´åœ¨è£…é¥°å‡½æ•°å†…éƒ¨çš„è®¡ç®—ä¸­å…³é—­å†å²è·Ÿè¸ªã€‚å®ƒæ˜¯ä¸€ä¸ªç›¸åº”çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ŒTorchdotå’ŒAgradã€‚ç”¨äºåœ¨å±€éƒ¨ä¸Šä¸‹æ–‡ä¸­æ‰“å¼€autogradã€‚å®ƒä¹Ÿå¯ä»¥ç”¨ä½œè£…é¥°å™¨ã€‚æœ€åï¼Œä½ å¯èƒ½æœ‰ä¸€ä¸ªè·Ÿè¸ªå†å²çš„å¼ é‡ï¼Œä½†ä½ éœ€è¦ä¸€ä¸ªä¸è·Ÿè¸ªå†å²çš„å‰¯æœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒTensorå¯¹è±¡å…·æœ‰ä¸€ä¸ªdetachæ–¹æ³•ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªä¸è®¡ç®—å†å²åˆ†ç¦»çš„å¼ é‡å‰¯æœ¬ã€‚
- en: We did this above when we grabbed some of our tensorsã€‚ Ma Totlib expects a nuy
    arrayã€‚ but the implicit tensor to nuy array conversion is not enabled for tensorors
    tracking historyã€‚ Once we make our attached copyï¼Œ we're good to goã€‚There's one
    more important note about autogra mechanicsã€‚ You have to be careful about using
    in place operations on Tensor's track gradientsã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è·å–ä¸€äº›å¼ é‡æ—¶ï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢åšäº†è¿™ä¸€ç‚¹ã€‚MatplotlibæœŸæœ›ä¸€ä¸ªnuyæ•°ç»„ï¼Œä½†å¯¹è·Ÿè¸ªå†å²çš„å¼ é‡ï¼Œéšå¼å¼ é‡åˆ°nuyæ•°ç»„çš„è½¬æ¢å¹¶æœªå¯ç”¨ã€‚ä¸€æ—¦æˆ‘ä»¬åˆ¶ä½œäº†é™„åŠ å‰¯æœ¬ï¼Œå°±å¯ä»¥ç»§ç»­è¿›è¡Œã€‚å…³äºautogradæœºåˆ¶è¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ³¨æ„äº‹é¡¹ã€‚ä½ å¿…é¡»å°å¿ƒå¯¹è·Ÿè¸ªæ¢¯åº¦çš„å¼ é‡ä½¿ç”¨å°±åœ°æ“ä½œã€‚
- en: Doing so may destroy information you need to correctly do your backward pass
    laterã€‚ In factã€‚ Pytorch will even give you a runtime errorã€‚If you try to perform
    an in place operation on an input tensor that requires gradientsã€‚Autograd tracks
    every step of your tensor computationã€‚ combiningbining that information with time
    measurements would be useful for profiling gradient tract computationsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšå¯èƒ½ä¼šç ´åä½ åœ¨ç¨åæ­£ç¡®è¿›è¡Œåå‘ä¼ æ’­æ‰€éœ€çš„ä¿¡æ¯ã€‚å®é™…ä¸Šï¼Œå¦‚æœä½ å°è¯•å¯¹éœ€è¦æ¢¯åº¦çš„è¾“å…¥å¼ é‡æ‰§è¡Œå°±åœ°æ“ä½œï¼ŒPyTorchç”šè‡³ä¼šç»™å‡ºè¿è¡Œæ—¶é”™è¯¯ã€‚Autogradä¼šè·Ÿè¸ªä½ å¼ é‡è®¡ç®—çš„æ¯ä¸€æ­¥ï¼Œå°†è¿™äº›ä¿¡æ¯ä¸æ—¶é—´æµ‹é‡ç»“åˆèµ·æ¥ï¼Œå¯¹äºåˆ†ææ¢¯åº¦è·Ÿè¸ªè®¡ç®—æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚
- en: and in factï¼Œ this feature is part of autogradã€‚The next code cellã€‚Shows basic
    usage of the profilerã€‚The autograd profiler can also group results by code blocks
    or input shape and can export results for other tracing tools the late documentation
    has full detailsã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™ä¸€ç‰¹æ€§æ˜¯autogradçš„ä¸€éƒ¨åˆ†ã€‚ä¸‹ä¸€ä¸ªä»£ç å•å…ƒå±•ç¤ºäº†åˆ†æå™¨çš„åŸºæœ¬ç”¨æ³•ã€‚autogradåˆ†æå™¨è¿˜å¯ä»¥æŒ‰ä»£ç å—æˆ–è¾“å…¥å½¢çŠ¶åˆ†ç»„ç»“æœï¼Œå¹¶å¯ä»¥ä¸ºå…¶ä»–è¿½è¸ªå·¥å…·å¯¼å‡ºç»“æœï¼Œæœ€æ–°çš„æ–‡æ¡£æä¾›äº†è¯¦ç»†ä¿¡æ¯ã€‚
- en: Pytororch 1ã€‚5 saw the introduction of the Autograd High level APIã€‚Which exposes
    some of the core operations underlying autogradã€‚In order to explain this bestã€‚
    I'm going to go into some more mathematical depth on what autograrad is doing
    under the hoodã€‚So say you have a function with n inputs and M outputsï¼Œ what' say
    y equals a function of xã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.5å¼•å…¥äº†Autogradé«˜å±‚APIï¼Œæš´éœ²äº†autogradèƒŒåçš„æ ¸å¿ƒæ“ä½œã€‚ä¸ºäº†æœ€å¥½åœ°è§£é‡Šè¿™ä¸€ç‚¹ï¼Œæˆ‘å°†æ·±å…¥ä¸€äº›æ•°å­¦å†…å®¹ï¼Œè¯´æ˜autogradåœ¨å¹•ååšäº†ä»€ä¹ˆã€‚æ‰€ä»¥å‡è®¾ä½ æœ‰ä¸€ä¸ªå…·æœ‰nä¸ªè¾“å…¥å’ŒMä¸ªè¾“å‡ºçš„å‡½æ•°ï¼Œè®¾yç­‰äºxçš„ä¸€ä¸ªå‡½æ•°ã€‚
- en: The complete set of partial derivatives of the outputs with respect to the inputs
    is a matrix called the Jacobianã€‚Nowï¼Œ if you have a second functionï¼Œ which we'll
    call Lã€‚ which equals G of y that takes an n dimensional inputã€‚That is the same
    dimensionality as the output of our first function and return to scalar outputã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¾“å…¥çš„è¾“å‡ºçš„å®Œæ•´åå¯¼æ•°é›†åˆæ˜¯ä¸€ä¸ªç§°ä¸ºé›…å¯æ¯”çŸ©é˜µçš„çŸ©é˜µã€‚ç°åœ¨ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªç¬¬äºŒä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºLï¼Œå®ƒç­‰äºG(y)ï¼Œå¹¶ä¸”æ¥å—ä¸€ä¸ªnç»´è¾“å…¥ã€‚è¿™ä¸ç¬¬ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºç»´åº¦ç›¸åŒï¼Œå¹¶è¿”å›æ ‡é‡è¾“å‡ºã€‚
- en: You can express its gradients with respect to Y is a column vectorã€‚It's really
    just a one column Jacobã€‚To tie this back to what we've been talking aboutã€‚ imagine
    the first function as your pitorrch model with potentially many inputs and many
    learning weights and many outputsã€‚ and the second function as a loss function
    with the model's output as input and the loss value as the scalar outputã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†å…¶æ¢¯åº¦è¡¨ç¤ºä¸ºå…³äºYçš„åˆ—å‘é‡ã€‚å®ƒå®é™…ä¸Šåªæ˜¯ä¸€ä¸ªå•åˆ—çš„é›…å¯æ¯”çŸ©é˜µã€‚å°†è¿™ä¸€ç‚¹ä¸æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„å†…å®¹è”ç³»èµ·æ¥ï¼Œæƒ³è±¡ç¬¬ä¸€ä¸ªå‡½æ•°æ˜¯ä½ çš„PyTorchæ¨¡å‹ï¼Œå¯èƒ½æœ‰è®¸å¤šè¾“å…¥ã€è®¸å¤šå­¦ä¹ æƒé‡å’Œè®¸å¤šè¾“å‡ºï¼Œè€Œç¬¬äºŒä¸ªå‡½æ•°æ˜¯ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œä»¥æ¨¡å‹çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œä»¥æŸå¤±å€¼ä½œä¸ºæ ‡é‡è¾“å‡ºã€‚
- en: If we multiply the first functions to Cobian by the gradient of the second function
    and apply the chain ruleã€‚ we get another column vectorã€‚This column vector represents
    the partial derivatives of the second function with respect to the inputs of the
    first functionã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†ç¬¬ä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºä¹˜ä»¥ç¬¬äºŒä¸ªå‡½æ•°çš„æ¢¯åº¦å¹¶åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°å¦ä¸€ä¸ªåˆ—å‘é‡ã€‚è¿™ä¸ªåˆ—å‘é‡è¡¨ç¤ºç¬¬äºŒä¸ªå‡½æ•°ç›¸å¯¹äºç¬¬ä¸€ä¸ªå‡½æ•°è¾“å…¥çš„åå¯¼æ•°ã€‚
- en: Or in the case of our machine learning modelã€‚The partial derivative of loss
    with respect to learning weightsã€‚Tchto autograd is an engine for computing these
    vector Jacobian productsã€‚ This is how we accumulate the gradients over the learning
    weights during the backward passã€‚For this reasonï¼Œ the backward call can also take
    an optional vector inputã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…åœ¨æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æƒ…å†µä¸‹ã€‚æŸå¤±å¯¹å­¦ä¹ æƒé‡çš„åå¯¼æ•°ã€‚Tchto autograd æ˜¯ä¸€ä¸ªç”¨äºè®¡ç®—è¿™äº›å‘é‡é›…å¯æ¯”ä¹˜ç§¯çš„å¼•æ“ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¦‚ä½•ç´¯ç§¯å­¦ä¹ æƒé‡çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œåå‘è°ƒç”¨ä¹Ÿå¯ä»¥æ¥å—ä¸€ä¸ªå¯é€‰çš„å‘é‡è¾“å…¥ã€‚
- en: The vector represents a set of gradients over the output tensorã€‚ which are multiplied
    by the Jacobian of the autograd trace tensor that precedes itã€‚Let's try a specific
    example with a small vectorã€‚If we tried to call Y dot backward nowã€‚ we'd get a
    runtime error and a message that gradientians can only be implicitly computed
    for scalar outputsã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡è¡¨ç¤ºè¾“å‡ºå¼ é‡ä¸Šçš„ä¸€ç»„æ¢¯åº¦ï¼Œè¿™äº›æ¢¯åº¦ä¸å…¶ä¹‹å‰çš„ autograd è·Ÿè¸ªå¼ é‡çš„é›…å¯æ¯”ç›¸ä¹˜ã€‚è®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªå¸¦å°å‘é‡çš„å…·ä½“ç¤ºä¾‹ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨å°è¯•è°ƒç”¨ Y dot
    åå‘ä¼ æ’­ï¼Œä¼šé‡åˆ°è¿è¡Œæ—¶é”™è¯¯ï¼Œå¹¶æ”¶åˆ°ä¸€ä¸ªæ¶ˆæ¯ï¼ŒæŒ‡æ˜æ¢¯åº¦åªèƒ½é’ˆå¯¹æ ‡é‡è¾“å‡ºéšå¼è®¡ç®—ã€‚
- en: For a multidimensional outputï¼Œ Autora expectses to provide the gradients for
    those three outputs that it can multiply into the Jacobiaã€‚Most of the output gradients
    here are all related to the powers of the twoã€‚ which we'd expect from the repeated
    doubling operation in the previous cellã€‚There's an API on Autograd that gives
    you direct access to important differential matrix and vector operationsã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤šç»´è¾“å‡ºï¼ŒAutora æœŸæœ›æä¾›è¿™ä¸‰ä¸ªè¾“å‡ºçš„æ¢¯åº¦ï¼Œä»¥ä¾¿å®ƒå¯ä»¥ä¹˜å…¥é›…å¯æ¯”ã€‚è¿™é‡Œçš„å¤§éƒ¨åˆ†è¾“å‡ºæ¢¯åº¦éƒ½ä¸äºŒçš„å¹‚ç›¸å…³ï¼Œè¿™æ˜¯æˆ‘ä»¬ä»ä¸Šä¸€ä¸ªå•å…ƒçš„é‡å¤åŠ å€æ“ä½œä¸­æ‰€æœŸå¾…çš„ã€‚Autograd
    ä¸Šæœ‰ä¸€ä¸ª APIï¼Œå¯ä»¥ç›´æ¥è®¿é—®é‡è¦çš„å¾®åˆ†çŸ©é˜µå’Œå‘é‡æ“ä½œã€‚
- en: in particularï¼Œ it allows you to calculate the Jacobian and Hesher matrices of
    a particular function for particular inputsã€‚The Hesians like the Jacobanï¼Œ but
    expresses all partial secondaryã€‚Let's take the Jacobian of a single function and
    evaluate it for two single element inputsã€‚And if you look closelyï¼Œ the first output
    should equal2 times e to the x since the derivative of e to the x is the exponential
    itselfã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯ï¼Œå®ƒå…è®¸ä½ ä¸ºç‰¹å®šè¾“å…¥è®¡ç®—ç‰¹å®šå‡½æ•°çš„é›…å¯æ¯”å’Œæµ·æ£®çŸ©é˜µã€‚æµ·æ£®çŸ©é˜µåƒé›…å¯æ¯”ï¼Œä½†è¡¨ç¤ºæ‰€æœ‰çš„åäºŒæ¬¡å¯¼æ•°ã€‚è®©æˆ‘ä»¬å–ä¸€ä¸ªå•ä¸€å‡½æ•°çš„é›…å¯æ¯”ï¼Œå¹¶ä¸ºä¸¤ä¸ªå•å…ƒç´ è¾“å…¥è¿›è¡Œè¯„ä¼°ã€‚å¦‚æœä½ ä»”ç»†è§‚å¯Ÿï¼Œç¬¬ä¸€ä¸ªè¾“å‡ºåº”è¯¥ç­‰äº
    2 ä¹˜ä»¥ e çš„ x æ¬¡æ–¹ï¼Œå› ä¸º e çš„ x æ¬¡æ–¹çš„å¯¼æ•°å°±æ˜¯å®ƒè‡ªèº«ã€‚
- en: And the second value should be threeã€‚Now you canï¼Œ of courseï¼Œ do this with higher
    order tensorsã€‚Here we've computed Jacobbian have that same adding function with
    a different set of inputsã€‚There's also a function to directly compute the vector
    Jacobcoium product if you provide the vectorã€‚Autograd's JVP method does the same
    matrix multiplication as VJP with the operarans reversedã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œç¬¬äºŒä¸ªå€¼åº”è¯¥æ˜¯ä¸‰ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥ä½¿ç”¨æ›´é«˜é˜¶çš„å¼ é‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—äº†å…·æœ‰ä¸åŒè¾“å…¥é›†çš„ç›¸åŒåŠ æ³•å‡½æ•°çš„é›…å¯æ¯”ã€‚è¿˜æœ‰ä¸€ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥è®¡ç®—å‘é‡é›…å¯æ¯”ä¹˜ç§¯ï¼Œå¦‚æœä½ æä¾›å‘é‡çš„è¯ã€‚Autograd
    çš„ JVP æ–¹æ³•æ‰§è¡Œä¸ VJP ç›¸åŒçš„çŸ©é˜µä¹˜æ³•ï¼Œä½†æ“ä½œæ•°æ˜¯åå‘çš„ã€‚
- en: the VHP and HVVP methods do the same for the vector Hesian projectã€‚For more
    informationã€‚ including important performance notesï¼Œ see the documentation for
    the new Autograd Funal APIã€‚![](img/25eee23f7825074bf201a2a5a23d28cb_3.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: VHP å’Œ HVVP æ–¹æ³•å¯¹å‘é‡æµ·æ£®æŠ•å½±ä¹Ÿæ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬é‡è¦çš„æ€§èƒ½è¯´æ˜ï¼Œè¯·å‚é˜…æ–°çš„ Autograd Funal API æ–‡æ¡£ã€‚![](img/25eee23f7825074bf201a2a5a23d28cb_3.png)
- en: '![](img/25eee23f7825074bf201a2a5a23d28cb_4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25eee23f7825074bf201a2a5a23d28cb_4.png)'
