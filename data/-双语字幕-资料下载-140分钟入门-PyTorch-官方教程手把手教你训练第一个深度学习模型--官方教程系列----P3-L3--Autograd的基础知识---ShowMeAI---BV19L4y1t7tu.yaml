- en: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P3：L3- Autograd的基础知识
    - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】140分钟入门PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P3：L3- Autograd的基础知识
    - ShowMeAI - BV19L4y1t7tu
- en: Hi， and welcome back to the Pytorrch trainingin video series。 In this video。
    we're going to cover autograph。 just Pytorrch's toolning for rapidly and dynamically
    computing the gradients that drive back propagation based learning in your machine
    learning model。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，欢迎回到PyTorch训练视频系列。在这个视频中，我们将讨论autograd，即PyTorch用于快速动态计算驱动基于反向传播学习的梯度的工具。
- en: 😊，In particular， we're going to go over what Autograd does for you。And why it
    makes Piytorrch so flexible and powerful for machine learning。We'll walk through
    a basic code example to you feel for what autograta is doing under the hood。Then
    we'll see autograd's role in a training loop。After that。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，特别是，我们将讨论Autograd为你做了什么，以及它为何使PyTorch在机器学习中如此灵活和强大。我们将通过一个基本的代码示例来了解autograd在背后做了什么。然后，我们将看到autograd在训练循环中的作用。之后。
- en: we'll talk about why and how to turn the autograd feature off and on for a particular
    tensor or a particular context。We'll see the autograd profiler in action， and
    we'll look at the autograrad high level API that was released with Pytor 1。5。![](img/25eee23f7825074bf201a2a5a23d28cb_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论为什么以及如何在特定张量或特定上下文中打开或关闭autograd功能。我们将看到autograd分析器的实际应用，并查看与PyTorch 1.5一起发布的autograd高级API！[](img/25eee23f7825074bf201a2a5a23d28cb_1.png)
- en: The autograd feature of Pytorrch is a large part of what makes Piytorrch a fast
    and flexible framework for building deep learning projects。It does this by easing
    the computation of the partial derivatives。 also called gradients that drive back
    propagation based learning。I'm not going to belabor the math here， although if
    you'd like a refresher。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的autograd功能是使PyTorch成为一个快速灵活的深度学习项目构建框架的重要组成部分。它通过简化部分导数的计算（也称为驱动反向传播学习的梯度）来实现这一点。我不会在这里详细讨论数学，尽管如果你想要复习一下的话。
- en: go ahead and download the notebook and follow along in detail。The important
    concept here is that when we're training our model， we compute a loss function。
    which tells us how far our model's prediction is from the ideal。We then need to
    find the partial derivatives of the loss function with respect to the model's
    learning weights。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 请下载笔记本，并详细跟随。这里的重要概念是，当我们训练模型时，我们计算损失函数，它告诉我们模型的预测与理想情况相差多远。然后，我们需要找到损失函数相对于模型学习权重的偏导数。
- en: These derivatives tell us in what direction we have to adjust the weights in
    order to minimize the loss。 This involves the iterative application of the chain
    rule of differential calculus over every path through the computation。Autograd
    makes this computation faster by tracing your computation at runtime。Every output
    tensor from your model's computation carries with it a history of the operations
    that LED to it。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导数告诉我们需要在哪个方向上调整权重，以最小化损失。这涉及在计算的每个路径上迭代应用微积分的链式法则。Autograd通过在运行时跟踪计算，使这个过程更快。来自模型计算的每个输出张量都携带着导致其产生的操作历史。
- en: This history allows the rapid computation of derivatives over the graph。All
    the way back to your model's learning weights。In addition。 because this history
    is gathered at runtime， you'll get the correct derivatives。 even if your model
    has a dynamic structure with decision branches and loops。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个历史允许在图上快速计算导数，直到模型的学习权重。此外，由于这个历史是在运行时收集的，即使模型具有动态结构、决策分支和循环，你也能获得正确的导数。
- en: This offers a lot of flexibility over tools that depend on analysis of a static
    computation rack。Let's have a look at a simple example of autograd in action。First。
    we'll import pie torch and matte plot lib so we can graph some stuff。Next。 we'll
    make a one dimensional tensor holding a bunch of values between0 and 2 pi。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这为依赖于静态计算图分析的工具提供了很大的灵活性。让我们来看一个autograd实际应用的简单示例。首先，我们将导入PyTorch和Matplotlib，以便可以绘制一些图形。接下来，我们将创建一个一维张量，存储从0到2π之间的一些值。
- en: and we'll add the requires grad equals true flat。Note that when we print A。
    pitorrch lets us know that A wants gradients computed on any calculation it's
    involved in。Now we'll perform a computation here we'll just take the sign of all
    the values in A and we'll graph that and that looks right。If you notice the calls
    that attach here， I'll be covering those later in the section on turning autograd
    on and off。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将添加 requires_grad=True 标志。请注意，当我们打印 A 时，Pytorrch 让我们知道 A 希望在任何参与的计算中计算梯度。现在我们将在这里执行一个计算，我们将取
    A 中所有值的正弦，并将其图形化，这看起来是正确的。如果你注意到这里的调用，我将在稍后的部分中讨论如何打开和关闭 autograd。
- en: So printing a tensor be。We see that Pytorrch tells us it has a grad function。
    This means that B came from a computation where at least one of the inputs required
    the calculation of gradients。 The grad function tells us that B came from the
    s operation。Let's perform a couple of more steps。 we'll double be and add one
    to it and we do this， we see that the output tensors again contain information
    about the operations that generated them into the grad function property。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以打印一个张量是很简单的。我们看到 Pytorrch 告诉我们它有一个 grad 函数。这意味着 B 来自一个计算，其中至少有一个输入需要计算梯度。grad
    函数告诉我们 B 来自 s 操作。让我们再进行几步。我们将对 B 进行加倍并加一，当我们这样做时，我们看到输出张量再次包含生成它们的操作的信息到 grad
    函数属性中。
- en: By default， Autograd expects the final function in a gradient computation to
    be single value。This is the case when we're computing the derivatives over learning
    weights。 The loss function has a single scalar value for its output。It doesn't
    strictly have to be a single value， but we'll go over that in a minute。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Autograd 期望梯度计算中的最终函数是单个值。这种情况发生在我们计算学习权重的导数时。损失函数的输出是单个标量值。它并不严格需要是单个值，但我们稍后会对此进行讨论。
- en: Here we'll just sum the elements of the tensor and call that the final output
    for this computation。We can actually use the grad function property of any output
    or intermediate tensor to walk back to the beginning of the computational history
    using the grad functions's next functions property。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们将简单地对张量的元素求和，并将其称为该计算的最终输出。我们实际上可以使用任何输出或中间张量的 grad 函数属性，向回走到计算历史的开头，利用
    grad 函数的 next 函数属性。
- en: Here you can see that D knows it came from an addition operation。 which knows
    it came from a multiplication operation， so I'm back to A。A does not have a gridd
    function， it is an input or leaf node of this computation graph。 and so represents
    the target variables for which we want to compute the gradients。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到，D 知道它来自一个加法操作，而 B 知道它来自一个乘法操作，所以我又回到了 A。A 没有 grad 函数，它是这个计算图的输入或叶节点，因此表示我们希望计算梯度的目标变量。
- en: So we've looked a little at the history tracking， but how do we actually compute
    gradients？
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经稍微看了一下历史追踪，但我们实际上是如何计算梯度的？
- en: That's easy， just call the backward method on the output Tensor like so。Looking
    back over the computation， we have a sine， the derivative of which is cosine。We
    have the multiplication by two， which should add a factor of two to our gradient。In
    the addition。 which should not change the derivative at all。Graphing the grad
    property on A， we see that， in fact。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单，只需对输出张量调用 backward 方法。回顾计算，我们有一个正弦函数，其导数是余弦。我们有乘以二的操作，这应该给我们的梯度加上一个二的因子。在加法中，这不应该改变导数。对
    A 的 grad 属性进行图形化，我们看到，实际上。
- en: the compute gradients are twice the cosine。Be aware that gradients are only
    computed for inputs or leaf nodes of the computation。 intermediate tensors will
    not have gradients attached after the back pass。So we peeked into the hood at
    how autograd。Computes gradients in a simple case。Next。 we'll examine this role
    in the training loop of a Ptorrch model。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的梯度是余弦的两倍。请注意，梯度仅为计算的输入或叶节点计算。中间张量在反向传递后将不会附带梯度。因此，我们窥探了 autograd 如何在一个简单的案例中计算梯度。接下来，我们将检查其在
    Ptorrch 模型训练循环中的作用。
- en: To see how autograd works in your training， let's build a small model and watch
    how it changes over a single training batch。We'll define and instantiate a model
    and create some standard in tensors for the training。 input and ideal output。You
    may have seen that we did not specify requires bright equals true for the model's
    layers within a subclass of Torch。 NN dot module， the gradient tracking is managed
    for you。If we look at the layers of the model。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解自动求导在训练中的工作原理，让我们构建一个小模型，并观察它在单个训练批次中的变化。我们将定义并实例化一个模型，并为训练创建一些标准输入张量和理想输出。你可能已经注意到，我们没有在Torch的NN模块子类中指定`requires_grad=True`，梯度跟踪会为你管理。如果我们查看模型的层。
- en: you can see the randomly initialized weights and that they have no gradients
    computed yet。You might have noticed there's a grad function on the weights I sampled。
    There's no grad function on the weights themselves because they're a leaf node
    of the computation graph。But the slice operation counts as a differentiable operation。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到随机初始化的权重，且它们尚未计算出梯度。你可能注意到，权重上有一个`grad`函数。我采样的权重上没有`grad`函数，因为它们是计算图的叶节点。但切片操作算作一个可微操作。
- en: So my little slice of the weights is a grad function indicating that it came
    from the slice。So let's see how this changes after one training batch。For a loss
    function。 we'll use the square of the Euclidean distance between our prediction
    and our ideal output。We'll also set up a basic optimizer using stochastic gradient
    descent。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我的小权重切片上有一个`grad`函数，表示它来自切片。那么，让我们看看在一个训练批次后这如何变化。对于损失函数，我们将使用预测值与理想输出之间的欧几里得距离的平方。我们还将使用随机梯度下降设置一个基本优化器。
- en: Note that we initialize the optimizer with the model's learning weights of parameters。
    The optimizer is responsible for adjusting the weights。So what happens when we
    call lost do backward？We can see that the weights have not changed。 but that we
    do have gradients computed。 Again， these gradients guide the optimizer in determining
    how to adjust the weights to minimize the loss score。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用模型的学习权重初始化优化器。优化器负责调整权重。那么，当我们调用`loss.backward()`时会发生什么呢？我们可以看到权重没有变化，但确实计算出了梯度。这些梯度指导优化器确定如何调整权重以最小化损失评分。
- en: In order to actually update the weights， we have to call optimizer。t step。And
    we can see that the rates have changed。This is how learning happens in your pie
    torch models。There's one more important step in the process。After you call optimizer。step。
    you need to call optimizer。0rograd。If you don't， the gradients will accumulate
    over every training batch。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际更新权重，我们必须调用`optimizer.step()`。我们可以看到权重发生了变化。这就是你在PyTorch模型中学习的方式。这个过程中还有一步重要的步骤。在调用`optimizer.step()`后，你需要调用`optimizer.zero_grad()`。如果不这样做，梯度将会在每个训练批次中累积。
- en: For example， if we run a training batch five times without calling a zero grad。You
    can see the gradients turn up mostly with much larger magnitudes because they
    were accumulated over each batch。And you can see the calling the zero grad resets
    the gradients。If your model' is not learning or training is giving you strange
    results。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们在不调用`zero_grad`的情况下运行一个训练批次五次，你会发现梯度的幅度变得更大，因为它们是在每个批次中累积的。你可以看到调用`zero_grad`会重置梯度。如果你的模型没有学习或训练给出奇怪的结果。
- en: the very first thing you should check is whether you're calling zero grad after
    each training step。Sometimes you'll want to control whether gradients get tracked
    for a calculation。 There are multiple ways to do this， depending on the situation。
    The easiest is just to set the acquire grad flag directly， like so。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该检查的第一件事是你是否在每个训练步骤后调用了`zero_grad`。有时你可能想控制某个计算的梯度跟踪。有多种方法可以做到这一点，具体取决于情况。最简单的方法就是直接设置`requires_grad`标志，如此。
- en: And we can see that B1 has a grad function， but B2 does not because we turned
    off history tracking in A prior to computing B2。If you only need autograd turned
    off temporarily， you can use the Torch。 Norad Con Manager。When we run this cell。We
    can see that history is tracked for all computations。 except the one in the nograd
    context。 Nograd can also be used as a function or method decorator。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到B1有一个`grad`函数，但B2没有，因为我们在计算B2之前关闭了历史跟踪。如果你只需要暂时关闭自动求导，可以使用Torch的`no_grad`上下文管理器。当我们运行这个单元时，我们可以看到所有计算的历史都会被跟踪，除了在`no_grad`上下文中的那一个。`no_grad`也可以用作函数或方法的装饰器。
- en: causing history tracking to be turned off for computations inside the decorated
    function。It's a corresponding context manager， Torchdot and Agrad。For turning
    autograd on in a local context。 it may also be used as a decorator。Finally， you
    may have a tensor tracking history。 but you need a copy that doesn't。In this case，
    the Tensor object has a detached method which creates a copy of the tensor that
    is detached from the computation history。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致在装饰函数内部的计算中关闭历史跟踪。它是一个相应的上下文管理器，Torchdot和Agrad。用于在局部上下文中打开autograd。它也可以用作装饰器。最后，你可能有一个跟踪历史的张量，但你需要一个不跟踪历史的副本。在这种情况下，Tensor对象具有一个detach方法，可以创建一个与计算历史分离的张量副本。
- en: We did this above when we grabbed some of our tensors。 Ma Totlib expects a nuy
    array。 but the implicit tensor to nuy array conversion is not enabled for tensorors
    tracking history。 Once we make our attached copy， we're good to go。There's one
    more important note about autogra mechanics。 You have to be careful about using
    in place operations on Tensor's track gradients。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们获取一些张量时，我们在上面做了这一点。Matplotlib期望一个nuy数组，但对跟踪历史的张量，隐式张量到nuy数组的转换并未启用。一旦我们制作了附加副本，就可以继续进行。关于autograd机制还有一个重要的注意事项。你必须小心对跟踪梯度的张量使用就地操作。
- en: Doing so may destroy information you need to correctly do your backward pass
    later。 In fact。 Pytorch will even give you a runtime error。If you try to perform
    an in place operation on an input tensor that requires gradients。Autograd tracks
    every step of your tensor computation。 combiningbining that information with time
    measurements would be useful for profiling gradient tract computations。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做可能会破坏你在稍后正确进行反向传播所需的信息。实际上，如果你尝试对需要梯度的输入张量执行就地操作，PyTorch甚至会给出运行时错误。Autograd会跟踪你张量计算的每一步，将这些信息与时间测量结合起来，对于分析梯度跟踪计算是非常有用的。
- en: and in fact， this feature is part of autograd。The next code cell。Shows basic
    usage of the profiler。The autograd profiler can also group results by code blocks
    or input shape and can export results for other tracing tools the late documentation
    has full details。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这一特性是autograd的一部分。下一个代码单元展示了分析器的基本用法。autograd分析器还可以按代码块或输入形状分组结果，并可以为其他追踪工具导出结果，最新的文档提供了详细信息。
- en: Pytororch 1。5 saw the introduction of the Autograd High level API。Which exposes
    some of the core operations underlying autograd。In order to explain this best。
    I'm going to go into some more mathematical depth on what autograrad is doing
    under the hood。So say you have a function with n inputs and M outputs， what' say
    y equals a function of x。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.5引入了Autograd高层API，暴露了autograd背后的核心操作。为了最好地解释这一点，我将深入一些数学内容，说明autograd在幕后做了什么。所以假设你有一个具有n个输入和M个输出的函数，设y等于x的一个函数。
- en: The complete set of partial derivatives of the outputs with respect to the inputs
    is a matrix called the Jacobian。Now， if you have a second function， which we'll
    call L。 which equals G of y that takes an n dimensional input。That is the same
    dimensionality as the output of our first function and return to scalar output。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关于输入的输出的完整偏导数集合是一个称为雅可比矩阵的矩阵。现在，如果你有一个第二个函数，我们称之为L，它等于G(y)，并且接受一个n维输入。这与第一个函数的输出维度相同，并返回标量输出。
- en: You can express its gradients with respect to Y is a column vector。It's really
    just a one column Jacob。To tie this back to what we've been talking about。 imagine
    the first function as your pitorrch model with potentially many inputs and many
    learning weights and many outputs。 and the second function as a loss function
    with the model's output as input and the loss value as the scalar output。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其梯度表示为关于Y的列向量。它实际上只是一个单列的雅可比矩阵。将这一点与我们之前讨论的内容联系起来，想象第一个函数是你的PyTorch模型，可能有许多输入、许多学习权重和许多输出，而第二个函数是一个损失函数，以模型的输出作为输入，以损失值作为标量输出。
- en: If we multiply the first functions to Cobian by the gradient of the second function
    and apply the chain rule。 we get another column vector。This column vector represents
    the partial derivatives of the second function with respect to the inputs of the
    first function。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将第一个函数的输出乘以第二个函数的梯度并应用链式法则，我们会得到另一个列向量。这个列向量表示第二个函数相对于第一个函数输入的偏导数。
- en: Or in the case of our machine learning model。The partial derivative of loss
    with respect to learning weights。Tchto autograd is an engine for computing these
    vector Jacobian products。 This is how we accumulate the gradients over the learning
    weights during the backward pass。For this reason， the backward call can also take
    an optional vector input。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在我们的机器学习模型的情况下。损失对学习权重的偏导数。Tchto autograd 是一个用于计算这些向量雅可比乘积的引擎。这就是我们在反向传播过程中如何累积学习权重的梯度。因此，反向调用也可以接受一个可选的向量输入。
- en: The vector represents a set of gradients over the output tensor。 which are multiplied
    by the Jacobian of the autograd trace tensor that precedes it。Let's try a specific
    example with a small vector。If we tried to call Y dot backward now。 we'd get a
    runtime error and a message that gradientians can only be implicitly computed
    for scalar outputs。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 向量表示输出张量上的一组梯度，这些梯度与其之前的 autograd 跟踪张量的雅可比相乘。让我们尝试一个带小向量的具体示例。如果我们现在尝试调用 Y dot
    反向传播，会遇到运行时错误，并收到一个消息，指明梯度只能针对标量输出隐式计算。
- en: For a multidimensional output， Autora expectses to provide the gradients for
    those three outputs that it can multiply into the Jacobia。Most of the output gradients
    here are all related to the powers of the two。 which we'd expect from the repeated
    doubling operation in the previous cell。There's an API on Autograd that gives
    you direct access to important differential matrix and vector operations。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多维输出，Autora 期望提供这三个输出的梯度，以便它可以乘入雅可比。这里的大部分输出梯度都与二的幂相关，这是我们从上一个单元的重复加倍操作中所期待的。Autograd
    上有一个 API，可以直接访问重要的微分矩阵和向量操作。
- en: in particular， it allows you to calculate the Jacobian and Hesher matrices of
    a particular function for particular inputs。The Hesians like the Jacoban， but
    expresses all partial secondary。Let's take the Jacobian of a single function and
    evaluate it for two single element inputs。And if you look closely， the first output
    should equal2 times e to the x since the derivative of e to the x is the exponential
    itself。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，它允许你为特定输入计算特定函数的雅可比和海森矩阵。海森矩阵像雅可比，但表示所有的偏二次导数。让我们取一个单一函数的雅可比，并为两个单元素输入进行评估。如果你仔细观察，第一个输出应该等于
    2 乘以 e 的 x 次方，因为 e 的 x 次方的导数就是它自身。
- en: And the second value should be three。Now you can， of course， do this with higher
    order tensors。Here we've computed Jacobbian have that same adding function with
    a different set of inputs。There's also a function to directly compute the vector
    Jacobcoium product if you provide the vector。Autograd's JVP method does the same
    matrix multiplication as VJP with the operarans reversed。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 而第二个值应该是三。当然，你可以使用更高阶的张量。在这里，我们计算了具有不同输入集的相同加法函数的雅可比。还有一个函数可以直接计算向量雅可比乘积，如果你提供向量的话。Autograd
    的 JVP 方法执行与 VJP 相同的矩阵乘法，但操作数是反向的。
- en: the VHP and HVVP methods do the same for the vector Hesian project。For more
    information。 including important performance notes， see the documentation for
    the new Autograd Funal API。![](img/25eee23f7825074bf201a2a5a23d28cb_3.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: VHP 和 HVVP 方法对向量海森投影也执行相同的操作。有关更多信息，包括重要的性能说明，请参阅新的 Autograd Funal API 文档。![](img/25eee23f7825074bf201a2a5a23d28cb_3.png)
- en: '![](img/25eee23f7825074bf201a2a5a23d28cb_4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25eee23f7825074bf201a2a5a23d28cb_4.png)'
