- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ä½¿ç”¨ Scikit-learn è¿›è¡Œæœºå™¨å­¦ä¹ ï¼Œ4å°æ—¶å®æˆ˜è§†è§’åˆ·æ–°çŸ¥è¯†æ¡†æ¶ï¼Œåˆå­¦è€…è¿›é˜¶å¿…å¤‡ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼š3ï¼‰è®­ç»ƒæµ‹è¯•æ‹†åˆ†
    - ShowMeAI - BV16u41127nr
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ä½¿ç”¨ Scikit-learn è¿›è¡Œæœºå™¨å­¦ä¹ ï¼Œ4å°æ—¶å®æˆ˜è§†è§’åˆ·æ–°çŸ¥è¯†æ¡†æ¶ï¼Œåˆå­¦è€…è¿›é˜¶å¿…å¤‡ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P3ï¼š3ï¼‰è®­ç»ƒæµ‹è¯•æ‹†åˆ†
    - ShowMeAI - BV16u41127nr
- en: '![](img/816ece20f67a7bafdafc76312ab6259f_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/816ece20f67a7bafdafc76312ab6259f_0.png)'
- en: So last time I talked about this issue of basically evaluating our model on
    the same data we fit it toã€‚ right the model can effectively memorize an answer
    and look like it is doing well even if it didn't really learn anythingã€‚And so
    the way we'll deal with this problem is we'll use something an SK learn called
    a train test splitã€‚ and this is a general strategyï¼Œ but this specific method will
    make it easy for usã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸Šæ¬¡æˆ‘è®¨è®ºäº†åœ¨åŒä¸€æ•°æ®ä¸Šè¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„é—®é¢˜ã€‚æ¨¡å‹å®é™…ä¸Šå¯ä»¥æœ‰æ•ˆåœ°è®°å¿†ç­”æ¡ˆï¼Œçœ‹èµ·æ¥è¡¨ç°è‰¯å¥½ï¼Œå³ä½¿å®ƒå¹¶æ²¡æœ‰çœŸæ­£å­¦åˆ°ä»»ä½•ä¸œè¥¿ã€‚æˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨ä¸€ç§SKå­¦ä¹ ä¸­çš„è®­ç»ƒæµ‹è¯•æ‹†åˆ†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™æ˜¯ä¸€ç§é€šç”¨ç­–ç•¥ï¼Œä½†è¿™ç§ç‰¹å®šæ–¹æ³•ä¼šè®©æˆ‘ä»¬æ›´å®¹æ˜“ã€‚
- en: And what we'll do is I'll take our original data frame and give us back two
    data framesã€‚We'll train our model to one data frame and then we'll evaluate it
    on the other one on the data the model has not seen beforeã€‚ and so it's a better
    test of whether it's doing wellã€‚And so I can see that there's a number of things
    I can pass into this one is I can say well what ratio of my data would I like
    to go into my test set I can say other things like try to stratify the data like
    let's say I was dealing with something categoricalã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™æ ·åšï¼Œæˆ‘å°†è·å–æˆ‘ä»¬çš„åŸå§‹æ•°æ®æ¡†ï¼Œå¹¶è¿”å›ä¸¤ä¸ªæ•°æ®æ¡†ã€‚æˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒåœ¨ä¸€ä¸ªæ•°æ®æ¡†ä¸Šï¼Œç„¶ååœ¨å¦ä¸€ä¸ªæœªè§è¿‡çš„æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°ã€‚å› æ­¤ï¼Œè¿™æ›´å¥½åœ°æµ‹è¯•å®ƒçš„è¡¨ç°ã€‚å› æ­¤æˆ‘å¯ä»¥çœ‹åˆ°æˆ‘å¯ä»¥ä¼ é€’è®¸å¤šå†…å®¹ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯æˆ‘å¯ä»¥è¯´æˆ‘å¸Œæœ›æˆ‘çš„æ•°æ®ä¸­æœ‰å¤šå°‘æ¯”ä¾‹è¿›å…¥æµ‹è¯•é›†ï¼Œæˆ‘è¿˜å¯ä»¥è¯´å…¶ä»–äº‹æƒ…ï¼Œæ¯”å¦‚å°è¯•å¯¹æ•°æ®è¿›è¡Œåˆ†å±‚ï¼Œä¾‹å¦‚å‡è®¾æˆ‘å¤„ç†çš„æ˜¯æŸç§åˆ†ç±»æ•°æ®ã€‚
- en: I might want to make sure that I'd have a similar number of categories on both
    sides I'm not trying to do that hereã€‚ but I will specify a trained test setã€‚![](img/816ece20f67a7bafdafc76312ab6259f_2.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯èƒ½æƒ³ç¡®ä¿ä¸¤è¾¹çš„ç±»åˆ«æ•°é‡ç›¸ä¼¼ï¼Œè™½ç„¶æˆ‘åœ¨è¿™é‡Œå¹¶æ²¡æœ‰å°è¯•è¿™æ ·åšã€‚ä½†æˆ‘ä¼šæŒ‡å®šä¸€ä¸ªè®­ç»ƒæµ‹è¯•é›†ã€‚![](img/816ece20f67a7bafdafc76312ab6259f_2.png)
- en: So let me import this thingï¼Œ I'm going to sayã€‚From SKLn import oh it's SKLnã€‚t
    model selectionã€‚Importã€‚ trainï¼Œ testï¼Œ splitã€‚Okayï¼Œ and then I'm going to say try
    and test splitã€‚And I'm going to try to try split on my data frame that I have
    originallyã€‚Andã€‚Basicallyã€‚ it's returning something called an array with two data
    frames in itã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘å¯¼å…¥è¿™ä¸ªä¸œè¥¿ï¼Œæˆ‘è¦è¯´ã€‚ä»SKLnå¯¼å…¥å“¦æ˜¯SKLnã€‚tæ¨¡å‹é€‰æ‹©ã€‚å¯¼å…¥ã€‚è®­ç»ƒï¼Œæµ‹è¯•ï¼Œæ‹†åˆ†ã€‚å¥½çš„ï¼Œç„¶åæˆ‘è¦è¯´å°è¯•å’Œæµ‹è¯•æ‹†åˆ†ã€‚æˆ‘å°†å°è¯•åœ¨æˆ‘åŸå§‹çš„æ•°æ®æ¡†ä¸Šè¿›è¡Œæ‹†åˆ†ã€‚åŸºæœ¬ä¸Šï¼Œå®ƒè¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ•°æ®æ¡†çš„æ•°ç»„ã€‚
- en: and so the way I can capture this is I can say well I have my train data frame
    and my test data frame is order is to to return them inã€‚And so let me take a look
    at theseã€‚ Here's my train data frameã€‚And then here is my test data frameã€‚And you
    can see that it kind of shuffled things around right if I look at the indexã€‚And
    I'm dealing with different data right so this is the data frame that my model
    is going to learn from and then this other one is the one I'm going to use to
    actually make sure it learns somethingã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¯ä»¥è¿™æ ·ç†è§£ï¼Œæˆ‘æœ‰æˆ‘çš„è®­ç»ƒæ•°æ®æ¡†å’Œæµ‹è¯•æ•°æ®æ¡†ï¼Œå®ƒä»¬çš„é¡ºåºæ˜¯è¿”å›å®ƒä»¬ã€‚è®©æˆ‘çœ‹çœ‹è¿™äº›ã€‚è¿™é‡Œæ˜¯æˆ‘çš„è®­ç»ƒæ•°æ®æ¡†ã€‚ç„¶åè¿™æ˜¯æˆ‘çš„æµ‹è¯•æ•°æ®æ¡†ã€‚ä½ å¯ä»¥çœ‹åˆ°å®ƒä»¬æœ‰ç‚¹æ‰“ä¹±äº†ï¼Œå¦‚æœæˆ‘çœ‹ç´¢å¼•çš„è¯ã€‚æˆ‘æ­£åœ¨å¤„ç†ä¸åŒçš„æ•°æ®ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘çš„æ¨¡å‹å°†è¦å­¦ä¹ çš„æ•°æ®ï¼Œè€Œå¦ä¸€ä¸ªæ˜¯æˆ‘å®é™…ç¡®ä¿å®ƒèƒ½å­¦åˆ°ä¸œè¥¿çš„æ•°æ®ã€‚
- en: And so I can see right now if I look at thisï¼Œ there's 6ï¼Œ300 down here and then
    19ï¼Œ000 up hereã€‚And that's because the default split is 0ã€‚25ï¼Œ so I could do something
    like this if I wantã€‚And then that would make this first one bigger and the next
    one smallerã€‚ So I'm going to do thatã€‚ Ohã€‚ and then let me just see what I did
    wrong hereã€‚ I think I actually have to say test sizeã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨æˆ‘å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœæˆ‘çœ‹è¿™ä¸ªï¼Œä¸‹é¢æ˜¯6300ï¼Œä¸Šé¢æ˜¯19000ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤çš„æ‹†åˆ†æ˜¯0.25ï¼Œæ‰€ä»¥å¦‚æœæˆ‘æ„¿æ„ï¼Œå¯ä»¥è¿™æ ·åšã€‚ç„¶åè¿™ä¼šè®©ç¬¬ä¸€ä¸ªæ•°æ®æ¡†å˜å¤§ï¼Œç¬¬äºŒä¸ªå˜å°ã€‚æ‰€ä»¥æˆ‘å°†è¿™æ ·åšã€‚å“¦ï¼Œæ¥ä¸‹æ¥è®©æˆ‘çœ‹çœ‹æˆ‘åšé”™äº†ä»€ä¹ˆã€‚æˆ‘æƒ³æˆ‘å®é™…ä¸Šéœ€è¦æŒ‡å®šæµ‹è¯•å¤§å°ã€‚
- en: So I'm going say test size equals thatã€‚And then this is going to be a little
    bit biggerã€‚ more data to test onã€‚And then a little bit less data to train on and
    so there's some trade offs thereã€‚Anywayï¼Œ now what I'm going to do is I'm going
    to head back to this example that I had earlier where I was doing my training
    and ideas instead of fitting and scoring on the same dataã€‚ I'm going to fit to
    my training data and then score on my test dataã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†è®¾ç½®æµ‹è¯•å¤§å°ç­‰äºè¿™ä¸ªã€‚ç„¶åè¿™å°†ä¼šå¤§ä¸€ç‚¹ã€‚ç”¨äºæµ‹è¯•çš„æ•°æ®æ›´å¤šã€‚è€Œè®­ç»ƒçš„æ•°æ®åˆ™ç¨å°‘ï¼Œæ‰€ä»¥è¿™é‡Œæœ‰ä¸€äº›æƒè¡¡ã€‚æ— è®ºå¦‚ä½•ï¼Œç°åœ¨æˆ‘å°†å›åˆ°æˆ‘ä¹‹å‰çš„ä¾‹å­ï¼Œåœ¨é‚£é‡Œæˆ‘åšäº†è®­ç»ƒï¼Œè€Œä¸æ˜¯åœ¨åŒä¸€æ•°æ®ä¸Šè¿›è¡Œæ‹Ÿåˆå’Œè¯„åˆ†ã€‚æˆ‘å°†æ‹Ÿåˆæˆ‘çš„è®­ç»ƒæ•°æ®ï¼Œç„¶ååœ¨æˆ‘çš„æµ‹è¯•æ•°æ®ä¸Šè¿›è¡Œè¯„åˆ†ã€‚
- en: so I'm just to do it again from scratch for review and I'm going to say LR equals
    linearã€‚Regressionã€‚![](img/816ece20f67a7bafdafc76312ab6259f_4.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†ä»å¤´å¼€å§‹å†åšä¸€æ¬¡å¤ä¹ ï¼Œæˆ‘è¦è¯´LRç­‰äºçº¿æ€§å›å½’ã€‚![](img/816ece20f67a7bafdafc76312ab6259f_4.png)
- en: And ohï¼Œ there we goï¼Œ and that I say Lrã€‚ fitã€‚And I have my trainã€‚Data frameã€‚
    and then I have a list of columns hereã€‚And then here I may have my Y columnã€‚Rightã€‚
    so that's the general strategyã€‚ And then what I want to do is I want to score
    itã€‚ when I'm scoring itï¼Œ I'm going to give it a Y column and anã€‚And an ex colonyã€‚
    And I'm sorryã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œæ¥äº†ï¼Œæˆ‘è¯´Lr.fitã€‚æˆ‘æœ‰æˆ‘çš„è®­ç»ƒæ•°æ®æ¡†ï¼Œç„¶åæˆ‘æœ‰ä¸€ä¸ªåˆ—çš„åˆ—è¡¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å¯èƒ½æœ‰æˆ‘çš„Yåˆ—ï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥è¿™å°±æ˜¯æ€»ä½“ç­–ç•¥ã€‚ç„¶åæˆ‘æƒ³åšçš„æ˜¯ç»™å®ƒæ‰“åˆ†ã€‚å½“æˆ‘ç»™å®ƒæ‰“åˆ†æ—¶ï¼Œæˆ‘ä¼šæä¾›ä¸€ä¸ªYåˆ—å’Œä¸€ä¸ªxåˆ—ã€‚æŠ±æ­‰ã€‚
- en: I have to sayï¼Œ train Diaf hereã€‚I'm going to give it that same informationã€‚ but
    now I'm going to be doing the test data frameã€‚Where as I may say test data frame
    and then same thing down hereï¼Œ test data frameã€‚And then what I'm going to do is
    for thisã€‚Imra passing in a list of columnsï¼Œ like beforeã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾—è¯´ï¼Œè®­ç»ƒæ•°æ®æ¡†åœ¨è¿™é‡Œã€‚æˆ‘ä¼šç»™å®ƒåŒæ ·çš„ä¿¡æ¯ã€‚ä½†ç°åœ¨æˆ‘å°†å¤„ç†æµ‹è¯•æ•°æ®æ¡†ã€‚æˆ‘å¯èƒ½ä¼šè¯´æµ‹è¯•æ•°æ®æ¡†ï¼Œç„¶ååœ¨è¿™é‡Œä¹Ÿæ˜¯ä¸€æ ·ï¼Œæµ‹è¯•æ•°æ®æ¡†ã€‚ç„¶åæˆ‘å°†ä¸ºæ­¤ä¼ é€’ä¸€ä¸ªåˆ—çš„åˆ—è¡¨ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ã€‚
- en: And what was I doing last time I was looking at this positiveã€‚A seven day averageã€‚
    And somebody' may pass that inã€‚Just like that and the same thing hereã€‚ and then
    over here I can just look at that thing that I'm trying to predict my Yã€‚ which
    was this thingã€‚When me pass that in and then the same hereã€‚ I'm going to do thatã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸Šæ¬¡åœ¨çœ‹è¿™ä¸ªæ­£æ•°æ—¶åœ¨åšä»€ä¹ˆï¼Ÿä¸ƒå¤©çš„å¹³å‡å€¼ã€‚æœ‰äººå¯èƒ½ä¼šå°†å…¶ä¼ é€’è¿‡æ¥ã€‚å°±åƒè¿™æ ·ï¼Œè¿™é‡Œä¹Ÿæ˜¯ä¸€æ ·ã€‚ç„¶ååœ¨è¿™é‡Œï¼Œæˆ‘å¯ä»¥æŸ¥çœ‹æˆ‘è¯•å›¾é¢„æµ‹çš„Yå€¼ï¼Œè¿™å°±æ˜¯è¿™ä¸ªä¸œè¥¿ã€‚å½“æˆ‘æŠŠå®ƒä¼ é€’è¿‡æ¥æ—¶ï¼Œè¿™é‡Œä¹Ÿæ˜¯ä¸€æ ·ã€‚æˆ‘ä¼šè¿™ä¹ˆåšã€‚
- en: and I see thatã€‚ okayï¼Œ my score is 0ã€‚16ã€‚Whereas what would happen if I wanted
    to run it on my original training data frameï¼Ÿ
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœ‹åˆ°è¿™ä¸ªã€‚å¥½å§ï¼Œæˆ‘çš„å¾—åˆ†æ˜¯0.16ã€‚é‚£ä¹ˆå¦‚æœæˆ‘æƒ³åœ¨æˆ‘åŸå§‹çš„è®­ç»ƒæ•°æ®æ¡†ä¸Šè¿è¡Œå®ƒï¼Œä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿ
- en: Like that training data frameã€‚And here it does better on the data I trained
    onã€‚ so this suggests that there is some overfitting hereï¼Œ rightã€‚ the model does
    better the data it learned from instead of some new data it hasn't seen before
    and so that of course is a concernã€‚And so there's different things I can try to
    do to avoid that and we'll eventually talk about some of those things this semesterã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åƒè¿™æ ·çš„è®­ç»ƒæ•°æ®æ¡†ã€‚åœ¨æˆ‘è®­ç»ƒçš„æ•°æ®ä¸Šè¡¨ç°å¾—æ›´å¥½ã€‚è¿™è¡¨æ˜è¿™é‡Œå­˜åœ¨ä¸€äº›è¿‡æ‹Ÿåˆï¼Œå¯¹å§ï¼Ÿæ¨¡å‹åœ¨å®ƒå­¦ä¹ è¿‡çš„æ•°æ®ä¸Šè¡¨ç°å¾—æ›´å¥½ï¼Œè€Œä¸æ˜¯åœ¨ä¸€äº›å®ƒä¹‹å‰æ²¡è§è¿‡çš„æ–°æ•°æ®ä¸Šï¼Œè¿™å½“ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥å°è¯•ä¸åŒçš„æ–¹æ³•æ¥é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šåœ¨è¿™ä¸ªå­¦æœŸè®¨è®ºå…¶ä¸­çš„ä¸€äº›æ–¹æ³•ã€‚
- en: Helloï¼Œ today I'm going to be talking more about how we can determine whether
    or not a model is doing a good job or notã€‚ how can we interpret different scores
    and make sure that we aren't getting a good score just by chanceã€‚And so I may
    be picking up last time with the model that's trying to predict COVID deaths two
    weeks out based on the current number of cases and so let me just quickly review
    what I did last time we were doing a train test split on our data frame and our
    data framework look like this right hereã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘å°†è®¨è®ºå¦‚ä½•åˆ¤æ–­ä¸€ä¸ªæ¨¡å‹æ˜¯å¦è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬å¦‚ä½•è§£è¯»ä¸åŒçš„å¾—åˆ†ï¼Œå¹¶ç¡®ä¿æˆ‘ä»¬ä¸æ˜¯å¶ç„¶å¾—åˆ°ä¸€ä¸ªå¥½å¾—åˆ†ã€‚å› æ­¤ï¼Œæˆ‘å¯èƒ½ä¼šæ¥ä¸Šæ¬¡çš„å†…å®¹ï¼Œæ¨¡å‹è¯•å›¾é¢„æµ‹åŸºäºå½“å‰ç—…ä¾‹æ•°é‡çš„ä¸¤å‘¨åCOVIDæ­»äº¡äººæ•°ï¼Œæ‰€ä»¥è®©æˆ‘å¿«é€Ÿå›é¡¾ä¸€ä¸‹ä¸Šæ¬¡æˆ‘ä»¬åœ¨æ•°æ®æ¡†ä¸Šè¿›è¡Œè®­ç»ƒæµ‹è¯•åˆ†å‰²æ—¶çš„æ“ä½œï¼Œæˆ‘ä»¬çš„æ•°æ®æ¡†çœ‹èµ·æ¥åƒè¿™æ ·ã€‚
- en: And it was per countyã€‚ And then there's every day and the year thereã€‚Then I
    created this linear regression modelï¼Œ which I imported from SK learned linearar
    modelã€‚And then I did two things with itï¼Œ I fit it to my training dataã€‚And then
    I scored it on my testing dataï¼Œ and these were the two pieces my training data
    and testing data came from here and so when I was doing thisã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æŒ‰å¿è®¡ç®—çš„ã€‚ç„¶åæ¯å¤©å’Œä¸€å¹´çš„æ•°æ®åœ¨é‚£é‡Œã€‚ç„¶åæˆ‘åˆ›å»ºäº†è¿™ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œæˆ‘ä»SKå­¦ä¹ çš„çº¿æ€§æ¨¡å‹ä¸­å¯¼å…¥äº†å®ƒã€‚ç„¶åæˆ‘ç”¨å®ƒåšäº†ä¸¤ä»¶äº‹ï¼Œæˆ‘æŠŠå®ƒæ‹Ÿåˆåˆ°æˆ‘çš„è®­ç»ƒæ•°æ®ä¸Šã€‚ç„¶åæˆ‘åœ¨æˆ‘çš„æµ‹è¯•æ•°æ®ä¸Šç»™å®ƒæ‰“åˆ†ï¼Œè¿™ä¸¤éƒ¨åˆ†æˆ‘çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥è‡ªè¿™é‡Œï¼Œå› æ­¤å½“æˆ‘åšè¿™ä¸ªæ—¶ã€‚
- en: what I basically did is I put in my Y valuesï¼Œ which is what I'm ultimately trying
    to predictã€‚And then here I put in my x values or my featuresï¼Œ which are things
    I know right nowï¼Œ so for exampleã€‚ right now I know the seven day rolling average
    of positive test cases and then two weeks out I'm trying to predict well how many
    deaths will there beã€‚This can be a single seriesï¼Œ which is why we just put a stringã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åŸºæœ¬ä¸Šåšçš„æ˜¯è¾“å…¥æˆ‘çš„Yå€¼ï¼Œè¿™æ˜¯æˆ‘æœ€ç»ˆæƒ³è¦é¢„æµ‹çš„ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘è¾“å…¥æˆ‘çš„xå€¼æˆ–ç‰¹å¾ï¼Œè¿™äº›æ˜¯æˆ‘ç°åœ¨çŸ¥é“çš„ä¸œè¥¿ï¼Œä¾‹å¦‚ï¼Œç°åœ¨æˆ‘çŸ¥é“é˜³æ€§ç—…ä¾‹çš„ä¸ƒå¤©æ»šåŠ¨å¹³å‡æ•°ï¼Œç„¶åä¸¤å‘¨åæˆ‘è¯•å›¾é¢„æµ‹ä¼šæœ‰å¤šå°‘æ­»äº¡ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå•ä¸€çš„åºåˆ—ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åªè¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
- en: I'm in the brackets after the data frameã€‚Here we actually have to pass in a
    full data frame because in general we might have multiple features and when I
    pass in a list to the brackets after a data frame while I get a smaller data frameã€‚
    that's why I have the double brackets thereã€‚ So anywayï¼Œ so I have this 0ã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æ•°æ®æ¡†åçš„æ‹¬å·é‡Œã€‚è¿™é‡Œæˆ‘ä»¬å®é™…ä¸Šå¿…é¡»ä¼ å…¥ä¸€ä¸ªå®Œæ•´çš„æ•°æ®æ¡†ï¼Œå› ä¸ºé€šå¸¸æˆ‘ä»¬å¯èƒ½æœ‰å¤šä¸ªç‰¹å¾ï¼Œå½“æˆ‘åœ¨æ•°æ®æ¡†åé¢çš„æ‹¬å·é‡Œä¼ å…¥ä¸€ä¸ªåˆ—è¡¨æ—¶ï¼Œæˆ‘å¾—åˆ°çš„æ˜¯ä¸€ä¸ªè¾ƒå°çš„æ•°æ®æ¡†ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä¼šæœ‰åŒæ‹¬å·ã€‚æ‰€ä»¥æ— è®ºå¦‚ä½•ï¼Œæˆ‘æœ‰è¿™ä¸ª0ã€‚
- en: 2 and and we know that the score will somewhere be be somewhere between 0 and1ã€‚
    So it's a little bit hard to say how good this score is right maybe you always
    get something like 0ã€‚2 by chance do we know So that's one of the things I want
    to talk about today And then the other thing that you might notice is if I rerun
    this a few times Oh and now I'm at 27%  32%ã€‚ 24%ï¼Œ 242826ã€‚So you can see that based
    on how I do this train test splitã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¼Œæˆ‘ä»¬çŸ¥é“åˆ†æ•°ä¼šåœ¨0å’Œ1ä¹‹é—´ã€‚å› æ­¤ï¼Œå¾ˆéš¾è¯´è¿™ä¸ªåˆ†æ•°æœ‰å¤šå¥½ï¼Œå¯¹å§ï¼Ÿä¹Ÿè®¸ä½ æ€»æ˜¯å¶ç„¶å¾—åˆ°0.2ã€‚é‚£ä¹ˆï¼Œè¿™æ˜¯æˆ‘ä»Šå¤©æƒ³è°ˆçš„äº‹æƒ…ä¹‹ä¸€ã€‚ä½ å¯èƒ½è¿˜ä¼šæ³¨æ„åˆ°ï¼Œå¦‚æœæˆ‘å¤šæ¬¡é‡æ–°è¿è¡Œè¿™ä¸ªï¼Œå“¦ï¼Œç°åœ¨æˆ‘å¾—åˆ°äº†27%ã€32%ã€24%ã€24ã€28ã€26ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°ï¼ŒåŸºäºæˆ‘å¦‚ä½•è¿›è¡Œè®­ç»ƒæµ‹è¯•åˆ’åˆ†ï¼Œç»“æœæ˜¯ä¸åŒçš„ã€‚
- en: I can do very different numbers and so that doesn't give us a lot of assurance
    so how can we get some more stable numbers out of the system I'll just give you
    kind of a hint of what the problem is if I look at train DF and I look at this
    column hereã€‚ which is the thing we're trying to predictã€‚å—¯ã€‚Let me do thatã€‚So I
    have all those numbers thereã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥å¾—åˆ°éå¸¸ä¸åŒçš„æ•°å­—ï¼Œå› æ­¤è¿™å¹¶æ²¡æœ‰ç»™æˆ‘ä»¬å¾ˆå¤šä¿éšœã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•æ‰èƒ½ä»ç³»ç»Ÿä¸­è·å¾—æ›´ç¨³å®šçš„æ•°å­—å‘¢ï¼Ÿæˆ‘åªç»™ä½ ä¸€ä¸ªå…³äºé—®é¢˜çš„æç¤ºï¼Œå¦‚æœæˆ‘æŸ¥çœ‹è®­ç»ƒæ•°æ®æ¡†å¹¶æŸ¥çœ‹è¿™ä¸ªåˆ—ï¼Œæ­£æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„å†…å®¹ã€‚å—¯ã€‚è®©æˆ‘æ¥åšè¿™ä¸ªã€‚æ‰€ä»¥æˆ‘æœ‰æ‰€æœ‰è¿™äº›æ•°å­—ã€‚
- en: Let me look at the variance of that column variance are just kind of a measure
    of how different values are from the averageã€‚And then I'm also going to do that
    thingï¼Œ same thing for the testã€‚And so when I do it for the testã€‚ I see that actually
    they have quite different variancesã€‚ and if I run this againã€‚While instrument
    randomly shake out differently now the test data actually has a higher variance
    than than the training dataã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘æŸ¥çœ‹è¯¥åˆ—çš„æ–¹å·®ï¼Œæ–¹å·®åªæ˜¯ä¸åŒå€¼ä¸å¹³å‡å€¼ä¹‹é—´å·®å¼‚çš„ä¸€ä¸ªåº¦é‡ã€‚ç„¶åæˆ‘ä¹Ÿå°†å¯¹æµ‹è¯•é›†åšåŒæ ·çš„äº‹æƒ…ã€‚å› æ­¤ï¼Œå½“æˆ‘å¯¹æµ‹è¯•é›†è¿›è¡Œæ“ä½œæ—¶ï¼Œæˆ‘çœ‹åˆ°å®ƒä»¬å®é™…ä¸Šå…·æœ‰ç›¸å½“ä¸åŒçš„æ–¹å·®ã€‚å¦‚æœæˆ‘å†è¿è¡Œä¸€æ¬¡ï¼ŒéšæœºæŠ½æ ·ç»“æœä¸åŒï¼Œç°åœ¨æµ‹è¯•æ•°æ®çš„æ–¹å·®å®é™…ä¸Šæ¯”è®­ç»ƒæ•°æ®çš„æ–¹å·®è¦é«˜ã€‚
- en: and we eventually look at how this scoring function worksã€‚ but it turns out
    that it's very much based on this varianceã€‚Which is why we have such a noisy measureã€‚
    So let me head over to the slides and try to give a preview of the things we're
    going to talk about todayã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€ç»ˆä¼šçœ‹çœ‹è¿™ä¸ªè¯„åˆ†å‡½æ•°æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ä½†äº‹å®è¯æ˜ï¼Œå®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠåŸºäºè¿™ä¸ªæ–¹å·®ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æœ‰å¦‚æ­¤å˜ˆæ‚çš„åº¦é‡çš„åŸå› ã€‚æ‰€ä»¥è®©æˆ‘å»çœ‹å¹»ç¯ç‰‡ï¼Œå°è¯•é¢„è§ˆä¸€ä¸‹æˆ‘ä»¬ä»Šå¤©è¦è®¨è®ºçš„å†…å®¹ã€‚
- en: '![](img/816ece20f67a7bafdafc76312ab6259f_6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/816ece20f67a7bafdafc76312ab6259f_6.png)'
- en: We're going to be learning for new functions related model evaluationã€‚ so first
    we're going to learn these two functions hereï¼Œ which will let us score our modelsã€‚![](img/816ece20f67a7bafdafc76312ab6259f_8.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å­¦ä¹ ä¸æ¨¡å‹è¯„ä¼°ç›¸å…³çš„æ–°å‡½æ•°ã€‚æ‰€ä»¥é¦–å…ˆæˆ‘ä»¬å°†å­¦ä¹ è¿™ä¸¤ä¸ªå‡½æ•°ï¼Œå®ƒä»¬å°†è®©æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œè¯„åˆ†ã€‚![](img/816ece20f67a7bafdafc76312ab6259f_8.png)
- en: Secondï¼Œ if our model is trying to mediocre like mine is rightï¼Œ I mean 0ã€‚2 is
    not greatã€‚ how can we know if it's not just chance and then we're going to be
    using something called the permutation test score for thatã€‚ and then finallyã€‚To
    get a less noisy measureï¼Œ we're going to be doing something called cross value
    scoringã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒï¼Œå¦‚æœæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¹³åº¸ï¼Œå°±åƒæˆ‘çš„æ¨¡å‹ä¸€æ ·ï¼Œæˆ‘çš„æ„æ€æ˜¯0.2å¹¶ä¸å¥½ã€‚æˆ‘ä»¬æ€ä¹ˆçŸ¥é“è¿™ä¸æ˜¯å¶ç„¶çš„å‘¢ï¼Ÿå› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§å«åšç½®æ¢æ£€éªŒåˆ†æ•°çš„æ–¹æ³•ã€‚æœ€åï¼Œä¸ºäº†è·å¾—ä¸€ä¸ªå™ªå£°è¾ƒå°‘çš„åº¦é‡ï¼Œæˆ‘ä»¬å°†è¿›è¡Œäº¤å‰éªŒè¯è¯„åˆ†ã€‚
- en: '![](img/816ece20f67a7bafdafc76312ab6259f_10.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/816ece20f67a7bafdafc76312ab6259f_10.png)'
- en: And so let me start here with these two metrics hereã€‚ I have this R2 score and
    mean absolute errorã€‚ and we'll talk about how those workã€‚![](img/816ece20f67a7bafdafc76312ab6259f_12.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè®©æˆ‘ä»è¿™ä¸¤ä¸ªæŒ‡æ ‡å¼€å§‹ã€‚æˆ‘æœ‰è¿™ä¸ªRÂ²åˆ†æ•°å’Œå¹³å‡ç»å¯¹è¯¯å·®ã€‚æˆ‘ä»¬å°†è®¨è®ºå®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚![](img/816ece20f67a7bafdafc76312ab6259f_12.png)
- en: If I go back to my slides or go back to my notebook right hereã€‚I'm doing the
    scoring hereã€‚ I can also up hereï¼Œ I can sayï¼Œ from SKL dot matrixã€‚![](img/816ece20f67a7bafdafc76312ab6259f_14.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘å›åˆ°æˆ‘çš„å¹»ç¯ç‰‡æˆ–æˆ‘çš„ç¬”è®°æœ¬ï¼Œæˆ‘åœ¨è¿™é‡Œè¿›è¡Œè¯„åˆ†ã€‚æˆ‘ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œè¯´ï¼Œä»SKL.dotçŸ©é˜µå¼€å§‹ã€‚![](img/816ece20f67a7bafdafc76312ab6259f_14.png)
- en: Importã€‚There's a couple things I want to doã€‚ I want to do the R2 scoreï¼Œ and
    then the meanã€‚Absolute error scoreã€‚And so the way all of these metrics work is
    kind of like thisã€‚ I'll call the metrics functionã€‚And then I'll say something
    likeï¼Œ I meanã€‚ I should just check this here quickã€‚I'll say something like I had
    to make sure the order is different between my true and my predictedã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼å…¥ã€‚ æˆ‘æƒ³åšå‡ ä»¶äº‹ã€‚ æˆ‘æƒ³è®¡ç®— R2 åˆ†æ•°ï¼Œç„¶åæ˜¯å¹³å‡ç»å¯¹è¯¯å·®åˆ†æ•°ã€‚ æ‰€æœ‰è¿™äº›æŒ‡æ ‡çš„å·¥ä½œæ–¹å¼å¤§è‡´å¦‚ä¸‹ã€‚ æˆ‘ä¼šè°ƒç”¨æŒ‡æ ‡å‡½æ•°ã€‚ ç„¶åæˆ‘ä¼šè¯´ç±»ä¼¼äºï¼Œæˆ‘çš„æ„æ€æ˜¯ã€‚
    æˆ‘åº”è¯¥åœ¨è¿™é‡Œå¿«é€Ÿæ£€æŸ¥ä¸€ä¸‹ã€‚ æˆ‘ä¼šè¯´ï¼Œæˆ‘å¿…é¡»ç¡®ä¿æˆ‘çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„é¡ºåºæ˜¯ä¸åŒçš„ã€‚
- en: I'll say you know what are the true values and then my predicted valuesã€‚And
    so for exampleï¼Œ up hereã€‚ I know that these are my true valuesã€‚Andã€‚Then my predicted
    valuesï¼Œ Wellã€‚ how do I forget my predicted valuesï¼Œ I could just sayï¼Œ wellï¼Œ modelã€‚
    please predict for me what these y values should be based on these x values that
    I'm going give youã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šè¯´ï¼ŒçœŸå®å€¼æ˜¯ä»€ä¹ˆï¼Œç„¶åæ˜¯æˆ‘çš„é¢„æµ‹å€¼ã€‚ æ‰€ä»¥ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œã€‚ æˆ‘çŸ¥é“è¿™äº›æ˜¯æˆ‘çš„çœŸå®å€¼ã€‚ ç„¶åæˆ‘çš„é¢„æµ‹å€¼ï¼Œå—¯ã€‚ æˆ‘è¯¥å¦‚ä½•å¿˜è®°æˆ‘çš„é¢„æµ‹å€¼ï¼Œæˆ‘å¯ä»¥ç®€å•åœ°è¯´ï¼Œæ¨¡å‹ã€‚
    è¯·ä¸ºæˆ‘é¢„æµ‹è¿™äº› y å€¼åº”è¯¥åŸºäºæˆ‘å°†ç»™ä½ çš„ x å€¼ã€‚
- en: All so I'm going to do thatï¼Œ and so here I'm putting in x values and it's going
    to return back to me Y valuesã€‚And this weird anything thing that we'll eventually
    talk aboutã€‚ but I could take this and I could put this right hereã€‚And so that
    I could use lots of different metric functions hereï¼Œ I couldï¼Œ for exampleã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†è¿™æ ·åšï¼Œåœ¨è¿™é‡Œæˆ‘è¾“å…¥ x å€¼ï¼Œå®ƒå°†è¿”å›ç»™æˆ‘ Y å€¼ã€‚ è¿˜æœ‰è¿™ä¸ªå¥‡æ€ªçš„ä¸œè¥¿ï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šè°ˆåˆ°ã€‚ ä½†æˆ‘å¯ä»¥æŠŠè¿™ä¸ªæ”¾åœ¨è¿™é‡Œã€‚ è¿™æ ·æˆ‘å°±å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨å¾ˆå¤šä¸åŒçš„æŒ‡æ ‡å‡½æ•°ï¼Œä¾‹å¦‚ã€‚
- en: use the R2 scoreã€‚Right here and guess whatï¼Œ it turns out that this square here
    that's associated with the model is just defaultfiling to use R2 scoreã€‚ there's
    lots of different metrics I could have used insteadï¼Œ but this one is the default
    oneã€‚So let's talk a little bit aboutã€‚Let's talk a little bit about how this were
    computedã€‚ So the idea of it is that this is the thing I'm trying to predict right
    So this is my why columnã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ R2 åˆ†æ•°ã€‚ å°±åœ¨è¿™é‡Œï¼Œç»“æœæ˜¯ä¸æ¨¡å‹ç›¸å…³çš„è¿™ä¸ªå¹³æ–¹é»˜è®¤ä½¿ç”¨ R2 åˆ†æ•°ã€‚ æˆ‘å¯ä»¥ä½¿ç”¨å¾ˆå¤šä¸åŒçš„æŒ‡æ ‡ï¼Œä½†è¿™ä¸ªæ˜¯é»˜è®¤çš„ã€‚ æ‰€ä»¥è®©æˆ‘ä»¬è°ˆè°ˆã€‚ è®©æˆ‘ä»¬ç¨å¾®è°ˆè°ˆè¿™ä¸ªæ˜¯å¦‚ä½•è®¡ç®—çš„ã€‚
    æ‰€ä»¥å®ƒçš„æƒ³æ³•æ˜¯ï¼Œè¿™æ˜¯æˆ‘æƒ³é¢„æµ‹çš„ä¸œè¥¿ã€‚ æ‰€ä»¥è¿™æ˜¯æˆ‘çš„ y åˆ—ã€‚
- en: And somebody going say why is hereã€‚And I can just peek at thatã€‚ What I want
    to do is I want to look at basically the squared residuals of this column relative
    to the meanã€‚ and so what does that meanã€‚ So I can take thisã€‚ and I can subtract
    off the mean of thisã€‚And then if I want toï¼Œ I can square all of thatã€‚So this is
    really a measure of kind of how bad the system is if I add all of these things
    offã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äººä¼šè¯´ï¼Œä¸ºä»€ä¹ˆåœ¨è¿™é‡Œã€‚ æˆ‘å¯ä»¥ç¥ä¸€çœ¼ã€‚ æˆ‘æƒ³åšçš„æ˜¯æŸ¥çœ‹è¿™ä¸€åˆ—ç›¸å¯¹äºå‡å€¼çš„å¹³æ–¹æ®‹å·®ã€‚ é‚£æ˜¯ä»€ä¹ˆæ„æ€ã€‚ æˆ‘å¯ä»¥å–è¿™ä¸ªã€‚ ç„¶åå‡å»è¿™ä¸ªçš„å‡å€¼ã€‚ ç„¶åå¦‚æœæˆ‘æƒ³ï¼Œæˆ‘å¯ä»¥å¯¹æ‰€æœ‰è¿™äº›è¿›è¡Œå¹³æ–¹ã€‚
    æ‰€ä»¥è¿™å®é™…ä¸Šæ˜¯è¡¡é‡ç³»ç»Ÿæœ‰å¤šç³Ÿç³•çš„ä¸€ä¸ªæ ‡å‡†ï¼Œå¦‚æœæˆ‘æŠŠæ‰€æœ‰è¿™äº›ä¸œè¥¿åŠ èµ·æ¥ã€‚
- en: So this is really well the variance of the system except I'm summing instead
    of averagingã€‚Right so this was my original kind of total error or variance in
    the system I might think of it as I'm just adding out so I wasm trying a sum of
    squaresã€‚ğŸ¤§And then what I want to think about is what if instead of vughã€‚Measuring
    the distance from each shnã€‚Each value to the meanã€‚ What if I measure the distance
    to my actual predictionã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å®é™…ä¸Šæ˜¯ç³»ç»Ÿçš„æ–¹å·®ï¼Œåªæ˜¯æˆ‘åœ¨æ±‚å’Œè€Œä¸æ˜¯å¹³å‡ã€‚ å¯¹ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘åŸæœ¬çš„æ€»è¯¯å·®æˆ–ç³»ç»Ÿçš„æ–¹å·®ï¼Œæˆ‘å¯ä»¥è®¤ä¸ºæˆ‘åªæ˜¯åœ¨æ·»åŠ ï¼Œæ‰€ä»¥æˆ‘åœ¨å°è¯•å¹³æ–¹å’Œã€‚ğŸ¤§ç„¶åæˆ‘æƒ³è€ƒè™‘çš„æ˜¯ï¼Œå¦‚æœä¸æ˜¯æµ‹é‡æ¯ä¸ªå€¼ä¸å‡å€¼çš„è·ç¦»ã€‚
    è€Œæ˜¯æµ‹é‡åˆ°æˆ‘å®é™…é¢„æµ‹çš„è·ç¦»ã€‚
- en: So it'll be very similar hereï¼Œ I can sayï¼Œ what is kind of left overã€‚If I subtract
    off my predictionsã€‚ how do I get my predictionsï¼Œ Wellï¼Œ that's this right hereã€‚
    I'm going to grab this pieceã€‚And let me have my predictions hereã€‚Yeahï¼Œ then let
    me take a look at thatã€‚So the way I really think about it is that originally this
    was how much variance I had in the system and this is how much I haveã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œä¼šéå¸¸ç›¸ä¼¼ï¼Œæˆ‘å¯ä»¥è¯´ï¼Œå‰©ä¸‹çš„æ˜¯ä»€ä¹ˆã€‚ å¦‚æœæˆ‘å‡å»æˆ‘çš„é¢„æµ‹ã€‚ æˆ‘å¦‚ä½•è·å¾—æˆ‘çš„é¢„æµ‹ï¼Œå—¯ï¼Œè¿™å°±æ˜¯åœ¨è¿™é‡Œã€‚ æˆ‘ä¼šæŠ“å–è¿™ä¸€éƒ¨åˆ†ã€‚ è®©æˆ‘æŠŠæˆ‘çš„é¢„æµ‹æ”¾åœ¨è¿™é‡Œã€‚
    æ˜¯çš„ï¼Œç„¶åè®©æˆ‘çœ‹çœ‹ã€‚ æ‰€ä»¥æˆ‘çœŸæ­£çš„æƒ³æ³•æ˜¯ï¼Œæœ€åˆè¿™æ˜¯æˆ‘åœ¨ç³»ç»Ÿä¸­çš„æ–¹å·®ï¼Œè€Œè¿™æ˜¯æˆ‘ç°åœ¨çš„æ–¹å·®ã€‚
- en: After Iï¼Œ after I do predictions rather than just subtracting off the meanã€‚And
    so what I could do is what I could sayã€‚å—¯ã€‚I could sayï¼Œ wellï¼Œ how much is remainingã€‚
    so I could say leftover divided by the totalã€‚Andã€‚Why is thatï¼ŸOhï¼Œ let me just put
    that hereã€‚I can say that left over divided by the totalï¼Œ and I'm likeï¼Œ okayã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘åšé¢„æµ‹åï¼Œè€Œä¸æ˜¯ä»…ä»…å‡å»å‡å€¼ã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥è¯´ï¼Œå—¯ï¼Œæˆ‘å¯ä»¥è¯´ï¼Œè¿˜æœ‰å¤šå°‘å‰©ä½™ã€‚æˆ‘å¯ä»¥è¯´å‰©ä½™é™¤ä»¥æ€»æ•°ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿå“¦ï¼Œè®©æˆ‘æŠŠå®ƒæ”¾åœ¨è¿™é‡Œã€‚æˆ‘å¯ä»¥è¯´å‰©ä½™é™¤ä»¥æ€»æ•°ï¼Œç„¶åæˆ‘æƒ³ï¼Œå¥½çš„ã€‚
- en: why left 79% of the variance on the table basicallyï¼ŸWhich means that I took
    away 20%ã€‚ğŸ¤§And you can see that that's exactly what this is up here right so really
    this is the the math that people will really kind of use to evaluate how goodã€‚A
    regression is typically we can do that more simply with R2 scoreã€‚ and even more
    simply that's the default vi say linear a regression dot scoreã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåŸºæœ¬ä¸Šä¿ç•™äº†79%çš„æ–¹å·®ï¼Ÿè¿™æ„å‘³ç€æˆ‘å‡å»äº†20%ã€‚ğŸ¤§ä½ å¯ä»¥çœ‹åˆ°è¿™æ­£æ˜¯ä¸Šé¢çš„å†…å®¹ï¼Œæ‰€ä»¥å®é™…ä¸Šè¿™æ˜¯äººä»¬ç”¨æ¥è¯„ä¼°å›å½’æ•ˆæœçš„æ•°å­¦ã€‚é€šå¸¸æˆ‘ä»¬å¯ä»¥ç”¨Rå¹³æ–¹åˆ†æ•°æ›´ç®€å•åœ°åšåˆ°è¿™ä¸€ç‚¹ï¼Œç”šè‡³æ›´ç®€å•çš„å°±æ˜¯é»˜è®¤çš„vi
    sayçº¿æ€§å›å½’.dotåˆ†æ•°ã€‚
- en: Let me give you an example of another metric people might useã€‚So maybe I want
    to just getï¼Œ wellã€‚ what is the average errorï¼ŸAnd so on that caseï¼Œ I would go back
    to this pieceã€‚RightSo this is all of the errorsã€‚And if I want to get the averageã€‚
    I should probably just think about the absolute errorã€‚And then I could take the
    mean of thatã€‚Rightã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ç»™ä½ ä¸¾ä¸ªä¾‹å­ï¼Œè¯´æ˜äººä»¬å¯èƒ½ä¼šä½¿ç”¨çš„å¦ä¸€ç§æŒ‡æ ‡ã€‚æ‰€ä»¥ä¹Ÿè®¸æˆ‘åªæ˜¯æƒ³çŸ¥é“ï¼Œå—¯ï¼Œå¹³å‡è¯¯å·®æ˜¯å¤šå°‘ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä¼šå›åˆ°è¿™ä¸€éƒ¨åˆ†ã€‚å¯¹ï¼Œè¿™éƒ½æ˜¯é”™è¯¯ã€‚å¦‚æœæˆ‘æƒ³è·å¾—å¹³å‡å€¼ï¼Œåº”è¯¥è€ƒè™‘ç»å¯¹è¯¯å·®ã€‚ç„¶åæˆ‘å¯ä»¥å–é‚£ä¸ªçš„å‡å€¼ã€‚å¯¹ã€‚
- en: so this would be the averageã€‚Absolute errorã€‚And it turns out that that is justã€‚Rather
    than be collectulating that myselfï¼Œ I could just grab this mean absolute error
    hereã€‚ there's lots of different metrics in here and I think if I hit shift tabã€‚Wellã€‚
    maybe it's just regular tabã€‚ I can see all the different metrics that come hereã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°†æ˜¯å¹³å‡ç»å¯¹è¯¯å·®ã€‚ç»“æœè¯æ˜ï¼Œå®é™…ä¸Šï¼Œå¹²è„†å–è¿™ä¸ªå‡å€¼ç»å¯¹è¯¯å·®å°±è¡Œäº†ã€‚è¿™å…¶ä¸­æœ‰å¾ˆå¤šä¸åŒçš„æŒ‡æ ‡ï¼Œæˆ‘æƒ³å¦‚æœæˆ‘æŒ‰ä¸‹Shift Tabï¼Œå—¯ã€‚ä¹Ÿè®¸åªæ˜¯å¸¸è§„Tabã€‚æˆ‘å¯ä»¥çœ‹åˆ°è¿™é‡Œæ‰€æœ‰ä¸åŒçš„æŒ‡æ ‡ã€‚
- en: And most of them I have never usedã€‚ so I paste this here and I seeï¼Œ wellï¼Œ that's
    kind of strangeã€‚ It should be the same It should be the same numberã€‚Why is that
    not the same numberã€‚Because onã€‚I wanted to get the error of my predictions not
    relative to the meanã€‚ and so I'm sorry this was the thing that I wanted toï¼Œ this
    is the thing I had wanted to grabã€‚Rightã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”æˆ‘å¤§å¤šæ•°éƒ½æ²¡æœ‰ä½¿ç”¨è¿‡ã€‚æ‰€ä»¥æˆ‘æŠŠè¿™ä¸ªç²˜è´´åœ¨è¿™é‡Œï¼Œæˆ‘å‘ç°ï¼Œå—¯ï¼Œè¿™æœ‰ç‚¹å¥‡æ€ªã€‚å®ƒåº”è¯¥æ˜¯ç›¸åŒçš„ï¼Œåº”è¯¥æ˜¯åŒä¸€ä¸ªæ•°å­—ã€‚ä¸ºä»€ä¹ˆä¸æ˜¯åŒä¸€ä¸ªæ•°å­—ã€‚å› ä¸ºæˆ‘æƒ³è¦çš„æ˜¯æˆ‘çš„é¢„æµ‹è¯¯å·®ï¼Œè€Œä¸æ˜¯ç›¸å¯¹äºå‡å€¼ã€‚å› æ­¤ï¼Œå¯¹ä¸èµ·ï¼Œè¿™æ˜¯æˆ‘æƒ³è¦çš„ï¼Œè¿™æ˜¯æˆ‘æƒ³æŠ“çš„ä¸œè¥¿ã€‚å¯¹ã€‚
- en: so I wanted to sayã€‚You knowï¼Œ here are all my errorsã€‚ take the absolute value
    of themã€‚ right some errors are positive or negativeã€‚ I just want to have the absolute
    valueã€‚ then I have the averageã€‚And why is thatã€‚å—¯ã€‚In balance syntaxã€‚ that usually
    probably means I have a mismatch in terms of my parenthesesã€‚I seeã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³è¯´ï¼Œä½ çŸ¥é“ï¼Œè¿™é‡Œæ˜¯æˆ‘æ‰€æœ‰çš„é”™è¯¯ã€‚å–å®ƒä»¬çš„ç»å¯¹å€¼ã€‚å¯¹ï¼Œæœ‰äº›é”™è¯¯æ˜¯æ­£çš„æˆ–è´Ÿçš„ã€‚æˆ‘åªæ˜¯æƒ³è¦ç»å¯¹å€¼ã€‚ç„¶åæˆ‘æœ‰å¹³å‡å€¼ã€‚ä¸ºä»€ä¹ˆå‘¢ã€‚å—¯ã€‚åœ¨å¹³è¡¡è¯­æ³•ä¸­ï¼Œè¿™é€šå¸¸å¯èƒ½æ„å‘³ç€æˆ‘çš„æ‹¬å·ä¸åŒ¹é…ã€‚æˆ‘æ˜ç™½äº†ã€‚
- en: So this one has is matched up thereã€‚ So I don't know why I grab that squaredã€‚
    Okayã€‚ so I can see that this is all my errorsã€‚And then I'm taking the absolute
    and I'm picking the averageã€‚ and I'll thankfullyï¼Œ okayï¼Œ well actually get the
    same thing that I have down hereã€‚And so againã€‚ rightï¼Œ this is just a shortcut
    for this kind of more complicated mathã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªåœ¨é‚£å„¿æ˜¯åŒ¹é…çš„ã€‚æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘æŠ“ä½äº†é‚£ä¸ªå¹³æ–¹ã€‚å¥½çš„ã€‚æˆ‘å¯ä»¥çœ‹åˆ°è¿™éƒ½æ˜¯æˆ‘çš„é”™è¯¯ã€‚ç„¶åæˆ‘å–ç»å¯¹å€¼å¹¶è®¡ç®—å¹³å‡å€¼ã€‚æ„Ÿè°¢ä¸Šå¤©ï¼Œå®é™…ä¸Šå¾—åˆ°çš„å’Œæˆ‘åœ¨è¿™é‡Œå†™çš„ç›¸åŒã€‚å› æ­¤ï¼Œå†ä¸€æ¬¡ã€‚å¯¹ï¼Œè¿™åªæ˜¯è¿™ä¸ªæ›´å¤æ‚æ•°å­¦çš„æ·å¾„ã€‚
- en: but it's another metric in terms of how these metrics workã€‚å‘ƒã€‚This one is kind
    of counting all errors more equallyï¼Œ you knowã€‚ error that's twice as big is just
    twice as badã€‚Because this one up here where Im doing the when I'm doing where
    is it right here when I'm doing the R R squared scoreã€‚ because that one is squaring
    my errorsï¼Œ this will tend to make it look worse if I have a few errors that are
    really big as opposed to many small errorsã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™æ˜¯å¦ä¸€ç§æŒ‡æ ‡ï¼Œæ¶‰åŠè¿™äº›æŒ‡æ ‡å¦‚ä½•å·¥ä½œã€‚å‘ƒã€‚è¿™ç§æ–¹æ³•å¯¹æ‰€æœ‰é”™è¯¯çš„è®¡æ•°ç›¸å¯¹å¹³ç­‰ï¼Œä½ çŸ¥é“ã€‚é”™è¯¯çš„ä¸¤å€å¤§åªæ˜¯åçš„ä¸¤å€ã€‚å› ä¸ºåœ¨è¿™é‡Œï¼Œæˆ‘åœ¨åšRå¹³æ–¹åˆ†æ•°æ—¶ï¼Œå¹³æ–¹æˆ‘çš„é”™è¯¯ï¼Œè¿™ä¼šä½¿å®ƒçœ‹èµ·æ¥æ›´ç³Ÿï¼Œå°¤å…¶æ˜¯å½“æˆ‘æœ‰ä¸€äº›éå¸¸å¤§çš„é”™è¯¯ï¼Œè€Œä¸æ˜¯å¾ˆå¤šå°é”™è¯¯æ—¶ã€‚
- en: Okayï¼Œ so those were a couple metricsï¼Œ which was one of the things we wanted
    to answer in the slideã€‚ I'm just going to head back hereã€‚![](img/816ece20f67a7bafdafc76312ab6259f_16.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™äº›æ˜¯å‡ ä¸ªæŒ‡æ ‡ï¼Œè¿™æ˜¯æˆ‘ä»¬æƒ³åœ¨å¹»ç¯ç‰‡ä¸­å›ç­”çš„å†…å®¹ä¹‹ä¸€ã€‚æˆ‘å°†è¿”å›è¿™é‡Œã€‚![](img/816ece20f67a7bafdafc76312ab6259f_16.png)
