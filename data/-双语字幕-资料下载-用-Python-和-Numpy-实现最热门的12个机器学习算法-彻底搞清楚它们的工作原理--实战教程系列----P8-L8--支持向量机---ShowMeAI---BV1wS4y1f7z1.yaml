- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼šL8- æ”¯æŒå‘é‡æœº
    - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼šL8- æ”¯æŒå‘é‡æœº
    - ShowMeAI - BV1wS4y1f7z1
- en: Hiï¼Œ everybodyã€‚ Welcome to your new machine learning from scratcht tutorialã€‚
    Todayã€‚ we are going to implement the SVM algorithm using only built in Python
    modules and Nyã€‚ğŸ˜Šã€‚The SVM or support vector machine is a very popular algorithmã€‚
    It follows the idea to use a linear model and to find a linear decision boundary
    also called a hyperplane that best separates our dataã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°å…¨æ–°çš„æœºå™¨å­¦ä¹ å…¥é—¨æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨å†…ç½®çš„ Python æ¨¡å—å’Œ Numpy æ¥å®ç° SVM ç®—æ³•ã€‚ğŸ˜Š SVM æˆ–æ”¯æŒå‘é‡æœºæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ç®—æ³•ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨çº¿æ€§æ¨¡å‹ï¼Œå¯»æ‰¾æœ€ä½³åˆ†éš”æˆ‘ä»¬æ•°æ®çš„çº¿æ€§å†³ç­–è¾¹ç•Œï¼Œäº¦å³è¶…å¹³é¢ã€‚
- en: And hereï¼Œ the choice as the best hyperplane is the one that represents the largest
    separation or the largest march in between the two classesã€‚ So we choose the hyperplane
    so that the distance from it to the nearest data point on each side is maximizedã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæœ€ä½³è¶…å¹³é¢çš„é€‰æ‹©æ˜¯ä»£è¡¨ä¸¤ä¸ªç±»åˆ«ä¹‹é—´æœ€å¤§åˆ†éš”æˆ–æœ€å¤§é—´éš”çš„é‚£ä¸ªã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©è¶…å¹³é¢ï¼Œä½¿å…¶åˆ°æ¯ä¾§æœ€è¿‘æ•°æ®ç‚¹çš„è·ç¦»æœ€å¤§åŒ–ã€‚
- en: So if we have a look at this imageï¼Œ then we want to find a hyperplane and the
    hyperplane has to satisfy this equation W times x minus B equals 0ã€‚And we want
    toã€‚Find the hyperplane so that the distance to both theã€‚both classes is maximizedã€‚
    So we used the class plus one here and-1 hereã€‚So thisï¼Œ this tense or the margin
    should be maximizedã€‚And firstï¼Œ let's have a look at the math behind itã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå›¾åƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢ï¼Œè€Œè¿™ä¸ªè¶…å¹³é¢å¿…é¡»æ»¡è¶³è¿™ä¸ªæ–¹ç¨‹ W ä¹˜ä»¥ x å‡å» B ç­‰äº 0ã€‚æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°è¶…å¹³é¢ï¼Œä½¿å¾—åˆ°ä¸¤ä¸ªç±»åˆ«çš„è·ç¦»æœ€å¤§åŒ–ã€‚å› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ç±»åˆ«
    +1ï¼Œè€Œåœ¨è¿™é‡Œä½¿ç”¨ -1ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªé—´éš”åº”è¯¥è¢«æœ€å¤§åŒ–ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹èƒŒåçš„æ•°å­¦ã€‚
- en: so it's a little bit more complex than in my previous tutorialsã€‚ but I promise
    that once you have understood itï¼Œ the final implementation is fairly simpleã€‚Soã€‚We
    use the linear modelï¼Œ W times x minus Bã€‚ That should be 0ã€‚ And then ourã€‚å—¯ã€‚Our
    function should also satisfy the condition that W times x minus B should be greater
    or equal than oneã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ¯”æˆ‘ä¹‹å‰çš„æ•™ç¨‹ç¨å¾®å¤æ‚ä¸€äº›ã€‚ä½†æˆ‘ä¿è¯ä¸€æ—¦ä½ ç†è§£äº†ï¼Œæœ€ç»ˆçš„å®ç°ç›¸å¯¹ç®€å•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨çº¿æ€§æ¨¡å‹ï¼ŒW ä¹˜ä»¥ x å‡å» Bï¼Œåº”è¯¥ç­‰äº 0ã€‚ç„¶åæˆ‘ä»¬çš„å‡½æ•°ä¹Ÿåº”è¯¥æ»¡è¶³
    W ä¹˜ä»¥ x å‡å» B åº”è¯¥å¤§äºç­‰äº 1ã€‚
- en: For our class plus oneã€‚ So all the samples here must lie on the left side of
    this equation or this line hereã€‚And allã€‚The samples of the classï¼Œ-1 must lieã€‚On
    the right side from this equationã€‚ So if we put this mathematicallyï¼Œ then we should
    it must satisfy W times x minus B should be greater or equal than1 for class1ã€‚
    or it should be less or equal than-1 for class -1ã€‚So if you put this in only one
    equationã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç±»åˆ«åŠ ä¸€ã€‚å› æ­¤ï¼Œè¿™é‡Œæ‰€æœ‰çš„æ ·æœ¬å¿…é¡»ä½äºè¿™ä¸ªæ–¹ç¨‹æˆ–è¿™æ¡çº¿çš„å·¦ä¾§ã€‚è€Œç±»åˆ« -1 çš„æ‰€æœ‰æ ·æœ¬å¿…é¡»ä½äºè¿™ä¸ªæ–¹ç¨‹çš„å³ä¾§ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬ç”¨æ•°å­¦è¡¨ç¤ºï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥æ»¡è¶³
    W ä¹˜ä»¥ x å‡å» B å¿…é¡»å¤§äºç­‰äº 1 å¯¹äºç±»åˆ« 1ï¼Œæˆ–è€…å¯¹äºç±»åˆ« -1ï¼Œå¿…é¡»å°äºç­‰äº -1ã€‚æ‰€ä»¥å¦‚æœä½ å°†è¿™äº›æ”¾å…¥ä¸€ä¸ªæ–¹ç¨‹ä¸­ã€‚
- en: then we multiply ourã€‚Linear function with the class labelã€‚ And this should be
    greater or equal than oneã€‚So this is the condition that we want to satisfyã€‚And
    now we want to come up with the W and the Bã€‚ So our weights and the biasã€‚And for
    thisã€‚ we use the cost function and then apply gradient descentã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†æˆ‘ä»¬çš„çº¿æ€§å‡½æ•°ä¸ç±»åˆ«æ ‡ç­¾ç›¸ä¹˜ã€‚è¿™åº”è¯¥å¤§äºç­‰äº 1ã€‚è¿™æ˜¯æˆ‘ä»¬å¸Œæœ›æ»¡è¶³çš„æ¡ä»¶ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦å¾—å‡º W å’Œ Bï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æƒé‡å’Œåç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆæœ¬å‡½æ•°ï¼Œç„¶ååº”ç”¨æ¢¯åº¦ä¸‹é™ã€‚
- en: So if you're not familiar with gradient descent alreadyã€‚ then please watch one
    of my previous tutorialsã€‚ For exampleã€‚ the one with linear regression there I
    explain this a little bit more in detailã€‚So now let'sã€‚Conueã€‚ So we use theã€‚Use
    a cost function hereã€‚ And in this caseï¼Œ we use the hinge lossã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ¢¯åº¦ä¸‹é™è¿˜ä¸ç†Ÿæ‚‰ï¼Œè¯·è§‚çœ‹æˆ‘ä¹‹å‰çš„æŸä¸ªæ•™ç¨‹ã€‚ä¾‹å¦‚ï¼Œåœ¨å…³äºçº¿æ€§å›å½’çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘å¯¹æ­¤åšäº†æ›´è¯¦ç»†çš„è§£é‡Šã€‚é‚£ä¹ˆç°åœ¨ï¼Œè®©æˆ‘ä»¬ç»§ç»­ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªæˆæœ¬å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨é“°é“¾æŸå¤±ã€‚
- en: and this is defined as the maximum of 0 andã€‚1 minusã€‚ And here we have our conditionã€‚
    Y I times our linear modelã€‚ So what this means is ifã€‚ if we plot the hinge loss
    and here the blue line is the hinge lossã€‚ So this is either 0ï¼Œ ifã€‚Y timesã€‚F is
    greater or equal than oneã€‚ So if they have the the same signã€‚Then it's 0ã€‚ And
    theï¼Œ if theyã€‚Yeahã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«å®šä¹‰ä¸º 0 å’Œ 1 å‡å»çš„æœ€å¤§å€¼ã€‚è€Œè¿™é‡Œæ˜¯æˆ‘ä»¬çš„æ¡ä»¶ã€‚Y I ä¹˜ä»¥æˆ‘ä»¬çš„çº¿æ€§æ¨¡å‹ã€‚é‚£ä¹ˆè¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬ç»˜åˆ¶é“°é“¾æŸå¤±ï¼Œè¿™é‡Œè“çº¿å°±æ˜¯é“°é“¾æŸå¤±ã€‚å¦‚æœ Y
    ä¹˜ä»¥ F å¤§äºç­‰äº 1ï¼Œé‚£ä¹ˆè¿™æ˜¯ 0ã€‚æ‰€ä»¥å¦‚æœå®ƒä»¬å…·æœ‰ç›¸åŒçš„ç¬¦å·ï¼Œé‚£ä¹ˆç»“æœå°±æ˜¯ 0ã€‚
- en: if they are correctly classified and are larger than oneï¼Œ then our loss is 0ã€‚
    So this means if we have a look at this image againï¼Œ ifã€‚For the green glassï¼Œ if
    itsã€‚ if it lies on this sideã€‚Thenï¼Œ it's 0ã€‚And for the blue classï¼Œ if it lies on
    this sideã€‚ then it's also 0ã€‚And otherwiseï¼Œ then we have a linear functionã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå®ƒä»¬è¢«æ­£ç¡®åˆ†ç±»å¹¶ä¸”å¤§äº1ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„æŸå¤±ä¸º0ã€‚è¿™æ„å‘³ç€å¦‚æœæˆ‘ä»¬å†çœ‹çœ‹è¿™ä¸ªå›¾åƒï¼Œå¦‚æœã€‚å¯¹äºç»¿è‰²çš„ç»ç’ƒï¼Œå¦‚æœå®ƒåœ¨è¿™ä¸€ä¾§ã€‚é‚£ä¹ˆï¼Œå®ƒæ˜¯0ã€‚å¯¹äºè“è‰²ç±»åˆ«ï¼Œå¦‚æœå®ƒåœ¨è¿™ä¸€ä¾§ã€‚é‚£ä¹ˆä¹Ÿæ˜¯0ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚
- en: So the further we are away from our decision boundary lineï¼Œ the higher is our
    lossã€‚And so this is one part of our cost functionã€‚ and the other part isã€‚As I
    already saidã€‚ we want to maximize the margin hereã€‚So between these two classes
    and the margin isã€‚Defined as2 over the magnitude of wã€‚ So this is dependent from
    our weightã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬è·ç¦»å†³ç­–è¾¹ç•Œçº¿è¶Šè¿œï¼ŒæŸå¤±å°±è¶Šé«˜ã€‚è¿™æ˜¯æˆ‘ä»¬æˆæœ¬å‡½æ•°çš„ä¸€éƒ¨åˆ†ã€‚å¦ä¸€éƒ¨åˆ†æ˜¯ã€‚æ­£å¦‚æˆ‘ä¹‹å‰æ‰€è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¿™é‡Œæœ€å¤§åŒ–é—´éš”ã€‚æ‰€ä»¥åœ¨è¿™ä¸¤ä¸ªç±»åˆ«ä¹‹é—´ï¼Œé—´éš”è¢«å®šä¹‰ä¸º2é™¤ä»¥wçš„å¤§å°ã€‚å› æ­¤ï¼Œè¿™å–å†³äºæˆ‘ä»¬çš„æƒé‡ã€‚
- en: dependent on our weight vectorã€‚ So we want to maximize thisï¼Œ and thereforeã€‚
    we want to minimize the magnitudeã€‚ So we put this or add this to ourã€‚Cost functionsã€‚
    So we also put this termã€‚The magnitude of W to the power of two times a lambda
    parameterã€‚And then here we have our hinge lossã€‚So the lambda parameter tries to
    find a trade off between these two termsã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾èµ–äºæˆ‘ä»¬çš„æƒé‡å‘é‡ã€‚å› æ­¤æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¿™ä¸ªï¼Œå› æ­¤ã€‚æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–å¤§å°ã€‚æ‰€ä»¥æˆ‘ä»¬å°†è¿™ä¸ªæˆ–æ·»åŠ åˆ°æˆ‘ä»¬çš„ã€‚æˆæœ¬å‡½æ•°ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜æ·»åŠ è¿™ä¸€é¡¹ã€‚Wçš„å¤§å°çš„å¹³æ–¹ä¹˜ä»¥ä¸€ä¸ªlambdaå‚æ•°ã€‚ç„¶åè¿™é‡Œæ˜¯æˆ‘ä»¬çš„é“°é“¾æŸå¤±ã€‚æ‰€ä»¥lambdaå‚æ•°è¯•å›¾åœ¨è¿™ä¸¤ä¸ªé¡¹ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªæŠ˜è¡·ã€‚
- en: So we it says basically says which is more importantã€‚ So we want toï¼Œ of courseã€‚
    we want to have the right classificationã€‚ We want to lie on the correct side of
    our linesã€‚ but we also want to have the the line such that the margin isã€‚Is maximizedã€‚å—¯ã€‚So
    yeahã€‚ so if we look at the two casesï¼Œ if ourï¼Œ if we are on the correct side of
    the lineã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åŸºæœ¬ä¸Šè¿™è¯´æ˜äº†å“ªä¸ªæ›´é‡è¦ã€‚æ‰€ä»¥æˆ‘ä»¬å½“ç„¶å¸Œæœ›æœ‰æ­£ç¡®çš„åˆ†ç±»ã€‚æˆ‘ä»¬å¸Œæœ›ä½äºçº¿çš„æ­£ç¡®ä¸€ä¾§ã€‚ä½†æˆ‘ä»¬ä¹Ÿå¸Œæœ›çº¿çš„é—´éš”æ˜¯ã€‚æœ€å¤§åŒ–çš„ã€‚å—¯ã€‚æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬çœ‹çœ‹è¿™ä¸¤ç§æƒ…å†µï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ­£ç¡®çš„ä¸€ä¾§ã€‚
- en: So if Y I times F on xï¼Œ F of x is greater or equal than oneã€‚Then we simplyã€‚
    we only have this term because this is the hinge loss is 0ã€‚ And otherwiseã€‚ then
    our cost function is this yearã€‚And now we want to minimize thatã€‚ So we want to
    get the derivatives or the gradients of our cost functionã€‚So in the first caseã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœY Iä¹˜ä»¥Fåœ¨xä¸Šï¼ŒFçš„å€¼å¤§äºæˆ–ç­‰äº1ã€‚é‚£ä¹ˆæˆ‘ä»¬ä»…ä»…æœ‰è¿™ä¸€é¡¹ï¼Œå› ä¸ºé“°é“¾æŸå¤±ä¸º0ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬çš„æˆæœ¬å‡½æ•°å°±æ˜¯è¿™ä¸€é¡¹ã€‚ç°åœ¨æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–è¿™ä¸ªã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³è¦è·å¾—æˆæœ¬å‡½æ•°çš„å¯¼æ•°æˆ–æ¢¯åº¦ã€‚åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ã€‚
- en: if we are greater or equal than oneã€‚Our derivativeã€‚Is only is two times lambda
    timesã€‚Wï¼Œ soã€‚ and here we only look at one component of our Wã€‚ So we get rid of
    the magnitudeã€‚And the derivative with respect to the B is 0ã€‚ So please double
    check that for yourself hereã€‚ I will not explain the derivatives and detailsã€‚And
    in the other caseã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¤§äºæˆ–ç­‰äº1ã€‚æˆ‘ä»¬çš„å¯¼æ•°ä»…ä¸º2å€lambdaä¹˜ä»¥Wã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿™é‡Œåªçœ‹Wçš„ä¸€ä¸ªåˆ†é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»æ‰å¤§å°ã€‚å¯¹Bçš„å¯¼æ•°ä¸º0ã€‚æ‰€ä»¥è¯·è‡ªè¡Œç¡®è®¤ã€‚æˆ‘ä¸ä¼šè§£é‡Šå¯¼æ•°å’Œç»†èŠ‚ã€‚åœ¨å¦ä¸€ç§æƒ…å†µä¸‹ã€‚
- en: so if if Y I times F on x is not greater or equal than oneã€‚Then our derivative
    with respect to the W is this equation hereã€‚ and the derivative with respect to
    our bias is only Y Iã€‚So againã€‚ please double check it for yourselfã€‚And then when
    we have our gradientsï¼Œ we can use the update ruleã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœY Iä¹˜ä»¥Fåœ¨xä¸Šä¸å¤§äºæˆ–ç­‰äº1ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯¹Wçš„å¯¼æ•°æ˜¯è¿™é‡Œçš„æ–¹ç¨‹ã€‚å¯¹æˆ‘ä»¬çš„åå·®çš„å¯¼æ•°ä»…ä¸ºY Iã€‚å› æ­¤ï¼Œè¯·å†æ¬¡è‡ªè¡Œç¡®è®¤ã€‚å½“æˆ‘ä»¬æœ‰äº†æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´æ–°è§„åˆ™ã€‚
- en: So the new weight is the old weight minus because we use gradient deã€‚ So we
    go into negative direction minus the learning rate or the step size times the
    derivativeã€‚ So these are our update rules andã€‚Nowï¼Œ I hope youve understood the
    concept and the math behind thisã€‚ And now we can start implementing itã€‚ So this
    is now straightforwardã€‚ Soï¼Œ first of allã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ–°çš„æƒé‡æ˜¯æ—§çš„æƒé‡å‡å»ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€‚æ‰€ä»¥æˆ‘ä»¬æœè´Ÿæ–¹å‘å‰è¿›ï¼Œå‡å»å­¦ä¹ ç‡æˆ–æ­¥é•¿ä¹˜ä»¥å¯¼æ•°ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ›´æ–°è§„åˆ™ã€‚ç°åœ¨ï¼Œæˆ‘å¸Œæœ›ä½ èƒ½ç†è§£è¿™ä¸ªæ¦‚å¿µåŠå…¶èƒŒåçš„æ•°å­¦ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹å®ç°å®ƒäº†ã€‚è¿™ç°åœ¨å¾ˆç®€å•ã€‚æ‰€ä»¥ï¼Œé¦–å…ˆã€‚
- en: we import Ny S and Pï¼Œ of courseã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_1.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å½“ç„¶å¯¼å…¥Ny Så’ŒPï¼[](img/0c2f47f3d7d980a73cb14308e5d73802_1.png)
- en: And then we create our class S we amï¼Œ which will get an in itã€‚Methodã€‚And here
    I will put in a learning rateï¼Œ which will get a default value of 0001ã€‚ and it
    will get a lambda parameterï¼Œ which will also get a defaultã€‚ And I will say this
    is 001ã€‚ So this is usually also a small valueã€‚And then it will get the number
    of iterations for our optimizationã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„ç±»Sï¼Œæˆ‘ä»¬å°†è·å¾—ä¸€ä¸ªinitæ–¹æ³•ã€‚åœ¨è¿™é‡Œæˆ‘å°†æ”¾å…¥ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œé»˜è®¤å€¼ä¸º0001ã€‚å®ƒå°†è·å¾—ä¸€ä¸ªlambdaå‚æ•°ï¼Œé»˜è®¤å€¼ä¹Ÿå°†å¾—åˆ°ã€‚æˆ‘ä¼šè¯´è¿™æ˜¯001ã€‚æ‰€ä»¥è¿™é€šå¸¸ä¹Ÿæ˜¯ä¸€ä¸ªå°å€¼ã€‚ç„¶åå®ƒå°†è·å¾—æˆ‘ä»¬ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°ã€‚
- en: which will get the default of 1000ã€‚So then I will simply store themã€‚ So I will
    say self dot L R equals learning rateã€‚Self doã€‚Lambda Para equals Lambda Paraã€‚
    So note that I cannot use Lambda here because Lada is a keyword and Python for
    the Lada functionã€‚Soã€‚ yeahï¼Œ them self doã€‚Andã€‚Its equals and itsã€‚ Then I will say
    self dot W equals none and self dot B equals noneã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è·å¾—é»˜è®¤å€¼1000ã€‚æ‰€ä»¥æˆ‘å°†ç®€å•åœ°å­˜å‚¨å®ƒä»¬ã€‚æˆ‘ä¼šè¯´self dot L Rç­‰äºå­¦ä¹ ç‡ã€‚Self doã€‚Lambda Paraç­‰äºLambda Paraã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä¸èƒ½åœ¨è¿™é‡Œä½¿ç”¨Lambdaï¼Œå› ä¸ºLadaæ˜¯Pythonä¸­Ladaå‡½æ•°çš„å…³é”®å­—ã€‚æ‰€ä»¥ã€‚æ˜¯çš„ï¼Œself
    doã€‚ç„¶åå®ƒç­‰äºå¹¶ä¸”å®ƒçš„ã€‚ç„¶åæˆ‘ä¼šè¯´self dot Wç­‰äºnoneï¼Œself dot Bç­‰äºnoneã€‚
- en: So I have to come up with them laterã€‚ and then we define our two functionsã€‚
    So as alwaysã€‚ one is the predict functionï¼Œ where we fit the training samples and
    the training labelsã€‚And theã€‚Sorryï¼Œ this is the fit methodã€‚And the other one is
    the predict methodã€‚Where we predict the labels of the test samplesã€‚And now let's
    start with the predict methodã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¿…é¡»ç¨åæƒ³å‡ºå®ƒä»¬ã€‚ç„¶åæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ä¸¤ä¸ªå‡½æ•°ã€‚åƒå¾€å¸¸ä¸€æ ·ã€‚ä¸€ä¸ªæ˜¯é¢„æµ‹å‡½æ•°ï¼Œæˆ‘ä»¬æ‹Ÿåˆè®­ç»ƒæ ·æœ¬å’Œè®­ç»ƒæ ‡ç­¾ã€‚å¯¹ä¸èµ·ï¼Œè¿™æ˜¯fitæ–¹æ³•ã€‚å¦ä¸€ä¸ªæ˜¯é¢„æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬é¢„æµ‹æµ‹è¯•æ ·æœ¬çš„æ ‡ç­¾ã€‚ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹é¢„æµ‹æ–¹æ³•ã€‚
- en: because this is very shortï¼Œ soã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_3.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™éå¸¸ç®€çŸ­ï¼Œæ‰€ä»¥ã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_3.png)
- en: We want toï¼Œ as I saidï¼Œ if we look at the mathï¼Œ we apply this linear modelã€‚ and
    then we look at the sign of thisã€‚ So if it's positiveï¼Œ then we say it's class
    1ã€‚ and if it's negativeï¼Œ then we say it's class-1ã€‚ So we say linearã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_5.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦ï¼Œå¦‚æˆ‘æ‰€è¯´ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ•°å­¦ï¼Œæˆ‘ä»¬åº”ç”¨è¿™ä¸ªçº¿æ€§æ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªçš„ç¬¦å·ã€‚å¦‚æœæ˜¯æ­£æ•°ï¼Œæˆ‘ä»¬å°±è¯´å®ƒæ˜¯ç±»1ã€‚å¦‚æœæ˜¯è´Ÿæ•°ï¼Œæˆ‘ä»¬å°±è¯´å®ƒæ˜¯ç±»-1ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´çº¿æ€§ã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_5.png)
- en: Output equals Ny dotã€‚Dotï¼Œ So the dot productã€‚Of X and self dot Wï¼Œ minus self
    dotã€‚Bã€‚ and then we choose the sizeã€‚ so we can simply sayï¼Œ returnã€‚Nampy dot sign
    of this linear outputã€‚So this is the whole predict implementationã€‚ And now let's
    continue with the fit methodï¼Œ soã€‚First of allã€‚As I saidï¼Œ we used the classes plus
    1 and-1 hereã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºç­‰äºNy dotã€‚Dotï¼Œæ‰€ä»¥ç‚¹ç§¯ã€‚Xå’Œself dot Wçš„ç»“æœå‡å»self dot Bã€‚ç„¶åæˆ‘ä»¬é€‰æ‹©å¤§å°ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´ï¼Œè¿”å›Nampy
    dotçš„è¿™ä¸ªçº¿æ€§è¾“å‡ºçš„ç¬¦å·ã€‚æ‰€ä»¥è¿™æ˜¯æ•´ä¸ªé¢„æµ‹å®ç°ã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­fitæ–¹æ³•ï¼Œæ‰€ä»¥ã€‚é¦–å…ˆï¼Œå¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ç±»+1å’Œ-1ã€‚
- en: So we want to make sure that our y has only -1 and plus1ã€‚ So oftentimes it has
    0 and1ã€‚ So let's convert thisã€‚ So let's say y underscore equalsã€‚ And here we can
    use nuy dot where this will get a conditionã€‚ So we say yã€‚ if this is less or equal
    than 0ï¼Œ then we put in -1ï¼Œ and otherwise we put in plus1ã€‚Soã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æƒ³ç¡®ä¿æˆ‘ä»¬çš„yåªæœ‰-1å’Œ+1ã€‚æ‰€ä»¥é€šå¸¸å®ƒæœ‰0å’Œ1ã€‚è®©æˆ‘ä»¬è½¬æ¢ä¸€ä¸‹ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´y underscoreç­‰äºã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨nuy dot whereï¼Œè¿™å°†è·å¾—ä¸€ä¸ªæ¡ä»¶ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´yã€‚å¦‚æœå°äºæˆ–ç­‰äº0ï¼Œæˆ‘ä»¬å°±æ”¾-1ï¼Œå¦åˆ™æ”¾+1ã€‚æ‰€ä»¥ã€‚
- en: This will convert all the zeros or smaller numbers to -1 and the other numbers
    to plus 1ã€‚And nowã€‚ let's get the number of samples and the number ofã€‚Featuresã€‚And
    this is simply X dotã€‚Shapeã€‚ because our input vector X is a nuy and DRAã€‚Where
    the number of rows is the number of samples and the number of columns is the number
    of featuresã€‚Then we want to initialize our W and our Bã€‚ And we simply put in zeros
    in the beginningã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æŠŠæ‰€æœ‰çš„é›¶æˆ–æ›´å°çš„æ•°å­—è½¬æ¢ä¸º-1ï¼Œå…¶ä»–æ•°å­—è½¬æ¢ä¸º+1ã€‚ç°åœ¨ã€‚è®©æˆ‘ä»¬è·å–æ ·æœ¬æ•°å’Œç‰¹å¾æ•°ã€‚è¿™ç®€å•åœ°æ˜¯X dot Shapeã€‚å› ä¸ºæˆ‘ä»¬çš„è¾“å…¥å‘é‡Xæ˜¯nuyå’ŒDRAã€‚è¡Œæ•°æ˜¯æ ·æœ¬æ•°ï¼Œåˆ—æ•°æ˜¯ç‰¹å¾æ•°ã€‚ç„¶åæˆ‘ä»¬æƒ³è¦åˆå§‹åŒ–æˆ‘ä»¬çš„Wå’ŒBã€‚æˆ‘ä»¬ä¸€å¼€å§‹ç®€å•åœ°æ”¾å…¥é›¶ã€‚
- en: So we say selfã€‚Tt W equals Ny zeros of size and featuresã€‚ So for eachã€‚Feature
    component we put in a0 for our weight componentã€‚And then we say self dot B equals
    0ã€‚ And now we canã€‚Start with our gradient descentsã€‚ So we sayï¼Œ for underscoreã€‚
    because we don't need this in range self dot and it iterã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´selfã€‚Tt Wç­‰äºNyçš„å¤§å°å’Œç‰¹å¾çš„é›¶ã€‚å¯¹äºæ¯ä¸ªç‰¹å¾ç»„ä»¶ï¼Œæˆ‘ä»¬å°†a0æ”¾å…¥æˆ‘ä»¬çš„æƒé‡ç»„ä»¶ä¸­ã€‚ç„¶åæˆ‘ä»¬è¯´self dot Bç­‰äº0ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹æˆ‘ä»¬çš„æ¢¯åº¦ä¸‹é™ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ï¼Œfor
    underscoreã€‚å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦åœ¨èŒƒå›´self dotä¸­ï¼Œå®ƒè¿­ä»£ã€‚
- en: So the number of iterations we want to do thisã€‚And thenï¼Œ we iterate over ourã€‚Train
    samplesã€‚ So I say4 index and Xï¼Œ I in enumerate Xã€‚So this will give me the current
    index and also the current sampleã€‚And nowã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_7.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æƒ³è¦è¿›è¡Œçš„è¿­ä»£æ¬¡æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿­ä»£æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬ã€‚æ‰€ä»¥æˆ‘è¯´4ç´¢å¼•å’ŒXï¼Œæˆ‘åœ¨æšä¸¾Xä¸­ã€‚è¿™å°†ç»™æˆ‘å½“å‰çš„ç´¢å¼•å’Œå½“å‰çš„æ ·æœ¬ã€‚ç°åœ¨ã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_7.png)
- en: What I want to do now is let'sã€‚Have a look at theã€‚math againã€‚ So I want toã€‚I
    want to calculate the weight or the derivative of our cost function with respect
    to the W and with respect to the biasã€‚And here I firstï¼Œ but at firstï¼Œ I look if
    this condition is satisfiedï¼Œ so I willã€‚Sayã€‚ and the condition is Y I times our
    linear functionã€‚ So I say condition equalsã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç°åœ¨æƒ³åšçš„æ˜¯è®©æˆ‘ä»¬å†çœ‹ä¸€ä¸‹æ•°å­¦ã€‚æ‰€ä»¥æˆ‘æƒ³è®¡ç®—æƒé‡æˆ–æˆ‘ä»¬çš„æˆæœ¬å‡½æ•°å…³äºWå’Œå…³äºåå·®çš„å¯¼æ•°ã€‚è¿™é‡Œæˆ‘é¦–å…ˆï¼Œä½†é¦–å…ˆï¼Œæˆ‘ä¼šæŸ¥çœ‹è¿™ä¸ªæ¡ä»¶æ˜¯å¦æ»¡è¶³ï¼Œæ‰€ä»¥æˆ‘ä¼šè¯´ã€‚æ¡ä»¶æ˜¯Y
    Iä¹˜ä»¥æˆ‘ä»¬çš„çº¿æ€§å‡½æ•°ã€‚æ‰€ä»¥æˆ‘è¯´æ¡ä»¶ç­‰äºã€‚
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_9.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f47f3d7d980a73cb14308e5d73802_9.png)'
- en: Why underscore of the current indexã€‚Timesï¼Œ and then the linear functionï¼Œ soã€‚Nampai
    dotã€‚Of the current sample and our self dot W minus self dotã€‚Bã€‚å—¯ã€‚This should be
    greater or equal than oneã€‚So if this is satisfied and the condition is true and
    otherwise it's falseã€‚ So now I say ifã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå½“å‰ç´¢å¼•ä½¿ç”¨ä¸‹åˆ’çº¿ã€‚ä¹˜ä»¥ï¼Œç„¶åæ˜¯çº¿æ€§å‡½æ•°ï¼Œæ‰€ä»¥ã€‚Nampai.dotã€‚å½“å‰æ ·æœ¬å’Œæˆ‘ä»¬çš„self.dot.Wå‡å»self.dot.Bã€‚å—¯ã€‚è¿™åº”è¯¥å¤§äºæˆ–ç­‰äº1ã€‚æ‰€ä»¥å¦‚æœæ»¡è¶³è¿™ä¸ªæ¡ä»¶å¹¶ä¸”æ¡ä»¶æ˜¯çœŸçš„ï¼Œå¦åˆ™å°±æ˜¯å‡ã€‚æ‰€ä»¥ç°åœ¨æˆ‘è¯´å¦‚æœã€‚
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_11.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f47f3d7d980a73cb14308e5d73802_11.png)'
- en: Conditionï¼Œ so if this is trueã€‚Thenï¼Œ ourã€‚Divatives look like thisã€‚ So the derivative
    with respect to the B is just 0ã€‚ And so we only need thisï¼Œ soã€‚I sayã€‚å—¯ã€‚So it's
    two times lambda times Wã€‚ And then in our updateï¼Œ we go in So we sayã€‚The new way
    is the old weight minus the learning rate times thisã€‚ So I write this in one stepã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡ä»¶ï¼Œæ‰€ä»¥å¦‚æœè¿™æ˜¯çœŸçš„ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬çš„å¯¼æ•°çœ‹èµ·æ¥åƒè¿™æ ·ã€‚å…³äºBçš„å¯¼æ•°å°±æ˜¯0ã€‚æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦è¿™ä¸ªï¼Œæ‰€ä»¥ã€‚æˆ‘è¯´ã€‚å—¯ã€‚æ‰€ä»¥æ˜¯ä¸¤å€lambdaä¹˜ä»¥Wã€‚ç„¶ååœ¨æˆ‘ä»¬çš„æ›´æ–°ä¸­ï¼Œæˆ‘ä»¬å»ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´æ–°çš„æ–¹å¼æ˜¯æ—§çš„æƒé‡å‡å»å­¦ä¹ ç‡ä¹˜ä»¥è¿™ä¸ªã€‚æ‰€ä»¥æˆ‘å°†å…¶å†™åœ¨ä¸€æ­¥ä¸­ã€‚
- en: So I say self dot W minus equal self dot learning rate timesã€‚ And now hereã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_13.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è¯´self.dot.Wå‡å»self.dot.learning rateä¹˜ä»¥ã€‚ç°åœ¨åœ¨è¿™é‡Œã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_13.png)
- en: We haveã€‚Two times self dot Lambda parameter times self dot Wã€‚So this is the
    first updateã€‚Or if our condition is satisfied and we only need this updateã€‚ And
    otherwiseã€‚ we say self dots W minus equal self times L R the learning rate timesã€‚
    and let's againã€‚ have a look at the equationã€‚ So it'sã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸¤å€self.dot.Lambdaå‚æ•°ä¹˜ä»¥self.dot.Wã€‚æ‰€ä»¥è¿™æ˜¯ç¬¬ä¸€æ¬¡æ›´æ–°ã€‚æˆ–è€…å¦‚æœæˆ‘ä»¬çš„æ¡ä»¶æ»¡è¶³ï¼Œæˆ‘ä»¬åªéœ€è¦è¿™ä¸ªæ›´æ–°ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬è¯´self.dots.Wå‡å»selfä¹˜ä»¥L
    Rå­¦ä¹ ç‡ä¹˜ä»¥ã€‚è®©æˆ‘ä»¬å†çœ‹çœ‹è¿™ä¸ªæ–¹ç¨‹ã€‚æ‰€ä»¥æ˜¯ã€‚
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_15.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f47f3d7d980a73cb14308e5d73802_15.png)'
- en: Two times lambda times w minus Yï¼Œ I times Xï¼Œ I soã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_17.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤å€lambdaä¹˜ä»¥wå‡å»Yï¼ŒIä¹˜ä»¥Xï¼ŒIï¼Œæ‰€ä»¥ã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_17.png)
- en: Two timesã€‚Our lambda timesã€‚W minus nuy dotã€‚ So I want to multiply our vectorsï¼Œ
    Xï¼Œ I andã€‚Why I so the y underscore of the current indexã€‚So this is our update
    for the W and our self dotã€‚Bã€‚Is minus equal self times learning rate times the
    derivative and the derivative is onlyã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤å€ã€‚æˆ‘ä»¬çš„lambdaä¹˜ä»¥ã€‚Wå‡å»nuy.dotã€‚æ‰€ä»¥æˆ‘æƒ³ä¹˜ä»¥æˆ‘ä»¬çš„å‘é‡ï¼ŒXï¼ŒIå’ŒY Iï¼Œæ‰€ä»¥å½“å‰ç´¢å¼•çš„yä¸‹åˆ’çº¿ã€‚è¿™æ˜¯æˆ‘ä»¬å¯¹Wçš„æ›´æ–°ï¼Œæˆ‘ä»¬çš„self.dot.Bæ˜¯å‡å»selfä¹˜ä»¥å­¦ä¹ ç‡ä¹˜ä»¥å¯¼æ•°ï¼Œè€Œå¯¼æ•°åªæœ‰ã€‚
- en: '![](img/0c2f47f3d7d980a73cb14308e5d73802_19.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c2f47f3d7d980a73cb14308e5d73802_19.png)'
- en: Or just Y Iï¼Œ So onlyã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_21.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…åªæ˜¯Y Iï¼Œæ‰€ä»¥åªæœ‰ã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_21.png)
- en: Why underscore of the indexã€‚And now we are doneã€‚ So this is the whole implementationã€‚And
    now let's test thisã€‚ So I've written a little test script that willã€‚Import this
    SVM classã€‚ and then it will generate aã€‚Some test samplesã€‚ So it will generate
    two glassesã€‚And then I will create my SVM classifier and fit the dataã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆç´¢å¼•ä½¿ç”¨ä¸‹åˆ’çº¿ã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚è¿™å°±æ˜¯æ•´ä¸ªå®ç°ã€‚ç°åœ¨è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ã€‚æ‰€ä»¥æˆ‘å†™äº†ä¸€ä¸ªå°æµ‹è¯•è„šæœ¬ï¼Œå®ƒä¼šå¯¼å…¥è¿™ä¸ªSVMç±»ï¼Œç„¶åç”Ÿæˆä¸€äº›æµ‹è¯•æ ·æœ¬ã€‚å®ƒå°†ç”Ÿæˆä¸¤ä¸ªç»ç’ƒã€‚ç„¶åæˆ‘ä¼šåˆ›å»ºæˆ‘çš„SVMåˆ†ç±»å™¨å¹¶æ‹Ÿåˆæ•°æ®ã€‚
- en: And then I wrote a little function to visualize this so you can find the code
    on Gitthubï¼Œ by the wayã€‚ so please check it out for yourself and now if we run
    thisã€‚ so let's say Python as we am underscore Te Pã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_23.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å†™äº†ä¸€ä¸ªå°å‡½æ•°æ¥å¯è§†åŒ–è¿™ä¸ªï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ä»£ç ï¼Œé¡ºä¾¿è¯´ä¸€ä¸‹ã€‚è¯·è‡ªå·±æŸ¥çœ‹ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚æ‰€ä»¥æˆ‘ä»¬è¯´Python as we
    am underscore Te Pã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_23.png)
- en: And nowï¼Œ this shouldã€‚Calculate the weights and the biasï¼Œ and it should also
    plot the decisionã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_25.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¿™åº”è¯¥è®¡ç®—æƒé‡å’Œåå·®ï¼Œå¹¶ä¸”è¿˜åº”è¯¥ç»˜åˆ¶å†³ç­–ã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_25.png)
- en: Fun the the yellow line and the two lines on both sides hereã€‚And we see that
    it's workingã€‚ Soï¼Œ yeahã€‚That's all about the S VMã€‚ I hope you enjoyed thisã€‚ And
    if you like thisã€‚ please subscribe to my channel and see you next timeï¼Œ byeã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_27.png)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯è¿™é‡Œçš„é»„çº¿å’Œä¸¤è¾¹çš„ä¸¤æ¡çº¿ã€‚æˆ‘ä»¬çœ‹åˆ°å®ƒåœ¨å·¥ä½œã€‚ æ‰€ä»¥ï¼Œæ˜¯çš„ã€‚è¿™å°±æ˜¯å…³äº S VM çš„å…¨éƒ¨å†…å®¹ã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªã€‚å¦‚æœä½ å–œæ¬¢ï¼Œè¯·è®¢é˜…æˆ‘çš„é¢‘é“ï¼Œä¸‹æ¬¡è§ï¼Œæ‹œã€‚![](img/0c2f47f3d7d980a73cb14308e5d73802_27.png)
