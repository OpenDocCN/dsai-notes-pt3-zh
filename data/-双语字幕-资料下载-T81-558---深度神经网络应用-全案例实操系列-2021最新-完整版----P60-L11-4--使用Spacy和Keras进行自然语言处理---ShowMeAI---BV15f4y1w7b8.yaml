- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P60ï¼šL11.4- ä½¿ç”¨Spacyå’ŒKerasè¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P60ï¼šL11.4- ä½¿ç”¨Spacyå’ŒKerasè¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeffã€‚ welcome to applications of deep neural networks In this videoã€‚
    we're going to take a look at natural language processing in particularã€‚ we're
    going to use Spacey Cars and word tove together for the latest on my AI course
    and projectsã€‚ click subscribe in the bell next to it to be notified of every new
    video previously we did text generation where we looked at treasure Islandã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«ã€‚æ¬¢è¿æ¥åˆ°æ·±åº¦ç¥ç»ç½‘ç»œçš„åº”ç”¨ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†ç»“åˆä½¿ç”¨Spaceyã€Keraså’Œword2vecï¼Œäº†è§£æˆ‘çš„AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°åŠ¨æ€ã€‚ç‚¹å‡»è®¢é˜…ï¼Œå¹¶ç‚¹å‡»æ—è¾¹çš„é“ƒé“›ï¼Œä»¥æ¥æ”¶æ¯ä¸ªæ–°è§†é¢‘çš„é€šçŸ¥ã€‚ä¹‹å‰æˆ‘ä»¬è¿›è¡Œäº†æ–‡æœ¬ç”Ÿæˆï¼ŒæŸ¥çœ‹äº†ã€Šé‡‘é“¶å²›ã€‹ã€‚
- en: and we got the neural network to generate its own pirate storiesã€‚ We're going
    to continue with thisã€‚ but this time we're going to use word level text generationPrevious
    we use character level text generationã€‚ Now there's much debate as far as which
    of these two you should useã€‚ I tend to prefer generating at the character level
    and going truly end to end with the neural networkã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®©ç¥ç»ç½‘ç»œç”Ÿæˆè‡ªå·±çš„æµ·ç›—æ•…äº‹ã€‚æˆ‘ä»¬å°†ç»§ç»­è¿™ä¸ªé¡¹ç›®ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬å°†ä½¿ç”¨å•è¯çº§æ–‡æœ¬ç”Ÿæˆã€‚ä¹‹å‰æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å­—ç¬¦çº§æ–‡æœ¬ç”Ÿæˆã€‚å…³äºè¿™ä¸¤è€…ï¼Œå­˜åœ¨å¾ˆå¤šäº‰è®ºã€‚æˆ‘çš„å€¾å‘æ˜¯ç”Ÿæˆå­—ç¬¦çº§ï¼Œå¹¶ä¸ç¥ç»ç½‘ç»œçœŸæ­£å®ç°ç«¯åˆ°ç«¯ã€‚
- en: letting it figure out grammar and letting it figure out sentence structure not
    really doing a lot of feature engineering on nouns and verbs and that sort of
    thingã€‚ but you can certainly do word levelã€‚ And if you do word levelã€‚ then it
    might be useful to provide additional features to letã€‚ğŸ˜Šã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©å®ƒè‡ªå·±ç†æ¸…è¯­æ³•å’Œå¥å­ç»“æ„ï¼Œè€Œä¸æ˜¯åœ¨åè¯å’ŒåŠ¨è¯ç­‰æ–¹é¢è¿›è¡Œå¤§é‡ç‰¹å¾å·¥ç¨‹ï¼Œä½†ä½ å½“ç„¶å¯ä»¥è¿›è¡Œå•è¯çº§åˆ«çš„å¤„ç†ã€‚å¦‚æœä½ é€‰æ‹©å•è¯çº§åˆ«ï¼Œé‚£ä¹ˆæä¾›é¢å¤–çš„ç‰¹å¾å¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚ğŸ˜Š
- en: '![](img/be18f4bf025fc5921b83d7e947e8af7a_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be18f4bf025fc5921b83d7e947e8af7a_1.png)'
- en: No if it's dealing with a noun or a verb or other thingsã€‚ We won't get quite
    that complicated with this exampleã€‚ but I'll show you how we can use this to generate
    text at a word by word level this is what we use for captioningã€‚ we were generating
    at the word levelã€‚ Although you can certainly do captioning at the character level
    as wellã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è®ºæ˜¯åè¯ã€åŠ¨è¯æˆ–å…¶ä»–ä¸œè¥¿ï¼Œè¿™ä¸ªé—®é¢˜ä¸ç®—å¤æ‚ã€‚æˆ‘ä»¬ä¸ä¼šåœ¨è¿™ä¸ªä¾‹å­ä¸­åšå¾—å¤ªå¤æ‚ï¼Œä½†æˆ‘ä¼šå±•ç¤ºå¦‚ä½•é€å­—ç”Ÿæˆæ–‡æœ¬ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨ç”Ÿæˆå­—å¹•æ—¶æ‰€ç”¨çš„ã€‚å°½ç®¡ä½ ç¡®å®å¯ä»¥åœ¨å­—ç¬¦çº§åˆ«è¿›è¡Œå­—å¹•ç”Ÿæˆã€‚
- en: So here I imported the packages and libraries that I need both for spacey and
    for Kisã€‚ I'm getting the treasure island text just like I did beforeã€‚ And now
    I am dealing with spacey So I am going to use spacey now to do the tokenization
    to break the treasure island text apart so that I have it by individual wordsã€‚
    Nowï¼Œ when we did character level encodingï¼Œ we didn't have to do this tokenization
    was handled by the neural networkã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘å¯¼å…¥äº†éœ€è¦çš„åŒ…å’Œåº“ï¼Œåˆ†åˆ«ç”¨äºspacyå’ŒKerasã€‚æˆ‘è·å–äº†ã€Šé‡‘é“¶å²›ã€‹çš„æ–‡æœ¬ï¼Œå°±åƒä¹‹å‰é‚£æ ·ã€‚ç°åœ¨æˆ‘å¼€å§‹å¤„ç†spacyï¼Œæ‰€ä»¥æˆ‘å°†ä½¿ç”¨spacyè¿›è¡Œåˆ†è¯ï¼Œå°†ã€Šé‡‘é“¶å²›ã€‹çš„æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªå•è¯ã€‚å½“æˆ‘ä»¬è¿›è¡Œå­—ç¬¦çº§ç¼–ç æ—¶ï¼Œä¸éœ€è¦è¿›è¡Œè¿™ç§åˆ†è¯ï¼Œå› ä¸ºåˆ†è¯ç”±ç¥ç»ç½‘ç»œå¤„ç†ã€‚
- en: It learned what spaces were and it learned how to break the words up here since
    we're doing word by wordã€‚ we need to do that actual tokenization and spacey is
    what does it for usã€‚ So we're essentially looping through all of the tokens soã€‚All
    of the words that it finds this is just codeï¼Œ this first lineï¼Œ this is a pretty
    useful lineã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå­¦ä¹ äº†ä»€ä¹ˆæ˜¯ç©ºæ ¼ï¼Œå¹¶å­¦ä¼šäº†å¦‚ä½•å°†å•è¯æ‹†åˆ†å¼€ã€‚ç”±äºæˆ‘ä»¬æŒ‰å•è¯è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œå®é™…çš„åˆ†è¯ï¼Œè€Œspacyæ­£æ˜¯ä¸ºæˆ‘ä»¬å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯éå†æ‰€æœ‰çš„tokenï¼Œå³å®ƒæ‰¾åˆ°çš„æ‰€æœ‰å•è¯ã€‚è¿™åªæ˜¯ä»£ç ï¼Œè¿™ç¬¬ä¸€è¡Œæ˜¯ç›¸å½“æœ‰ç”¨çš„ä¸€è¡Œã€‚
- en: This reduces the characters to just characters in the ASCI code 0 to 127ã€‚ so
    this ensures that you are dealing just with ASII characters in those wordsã€‚ so
    that strips out a lot of the extra junk characters like this copyright symbol
    in this weird A that removes those from your words then we also strip off any
    white space because once you removes some of theseã€‚ there might be white space
    now that was embedded in thereã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å­—ç¬¦å‡å°‘åˆ°ASCIIç 0åˆ°127çš„å­—ç¬¦ã€‚å› æ­¤ï¼Œè¿™ç¡®ä¿ä½ ä»…å¤„ç†è¿™äº›å•è¯ä¸­çš„ASCIIå­—ç¬¦ã€‚è¿™ä¼šå»æ‰å¾ˆå¤šé¢å¤–çš„åƒåœ¾å­—ç¬¦ï¼Œæ¯”å¦‚è¿™ä¸ªå¥‡æ€ªçš„ç‰ˆæƒç¬¦å·å’Œå¥‡æ€ªçš„å­—æ¯Aï¼Œç„¶åæˆ‘ä»¬è¿˜ä¼šå»æ‰ä»»ä½•ç©ºç™½å­—ç¬¦ï¼Œå› ä¸ºä¸€æ—¦ä½ å»æ‰ä¸€äº›å­—ç¬¦ï¼Œå¯èƒ½ä¼šå‡ºç°åµŒå…¥åœ¨å…¶ä¸­çš„ç©ºç™½ã€‚
- en: We make sure that the tokens are not digitsã€‚And that they're not URLsã€‚ emails
    or other things like thatã€‚ Now you might want to handle theseï¼Œ but for the simple
    exampleã€‚ we're simply stripping them Now in the character by characterã€‚ we just
    let them pass it in and interestinglyï¼Œ the neural network would learn to generate
    URLs and email addresses and other things like that total number of words we had
    when we did that is 6421ã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¡®ä¿æ ‡è®°ä¸æ˜¯æ•°å­—ï¼Œä¹Ÿä¸æ˜¯ç½‘å€ã€ç”µå­é‚®ä»¶æˆ–å…¶ä»–ç±»ä¼¼çš„ä¸œè¥¿ã€‚ç°åœ¨ä½ å¯èƒ½æƒ³å¤„ç†è¿™äº›ï¼Œä½†å¯¹äºè¿™ä¸ªç®€å•çš„ä¾‹å­ï¼Œæˆ‘ä»¬åªæ˜¯å°†å®ƒä»¬å‰”é™¤ã€‚ç°åœ¨åœ¨å­—ç¬¦é€ä¸ªå¤„ç†æ—¶ï¼Œæˆ‘ä»¬åªæ˜¯è®©å®ƒä»¬é€šè¿‡ï¼Œæœ‰è¶£çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œä¼šå­¦ä¹ ç”Ÿæˆç½‘å€ã€ç”µå­é‚®ä»¶åœ°å€ç­‰ã€‚å½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œå•è¯çš„æ€»æ•°æ˜¯6421ã€‚
- en: we print out some of theseã€‚ Now some of these are numbersï¼Œ that's a Roman numeralã€‚
    but we'll just go ahead and let it go in that won't necessarily hurt anythingã€‚
    we're creating to look ups hereã€‚ a word to indexã€‚ So this would take a word like
    alone and convert it into a indexã€‚ Now if you want to make these more consistent
    you might want to sort these because then the indexes are going to always be the
    sameã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰“å°å‡ºä¸€äº›è¿™äº›å†…å®¹ã€‚ç°åœ¨å…¶ä¸­ä¸€äº›æ˜¯æ•°å­—ï¼Œé‚£æ˜¯ç½—é©¬æ•°å­—ã€‚ä½†æˆ‘ä»¬ä¼šç»§ç»­è®©å®ƒé€šè¿‡ï¼Œè¿™ä¸ä¼šé€ æˆä»»ä½•ä¼¤å®³ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºä¸¤ä¸ªæŸ¥æ‰¾è¡¨ï¼Œä¸€ä¸ªæ˜¯å•è¯åˆ°ç´¢å¼•ã€‚å› æ­¤ï¼Œè¿™ä¼šå°†å•è¯åƒâ€œå­¤ç‹¬â€è½¬æ¢ä¸ºä¸€ä¸ªç´¢å¼•ã€‚ç°åœ¨å¦‚æœä½ æƒ³ä½¿è¿™äº›æ›´åŠ ä¸€è‡´ï¼Œä½ å¯èƒ½æƒ³å¯¹è¿™äº›è¿›è¡Œæ’åºï¼Œå› ä¸ºé‚£æ ·ç´¢å¼•å°†å§‹ç»ˆä¿æŒç›¸åŒã€‚
- en: So for production situationsï¼Œ you need to think about that to make sure that
    these indexes are truly staying the sameã€‚ Otherwiseï¼Œ if you train your neural
    network and you change what these indexes areã€‚ It's not going to workã€‚Then we
    tokenize the text that basically goes and takes the original Tresure Island text
    and replaces each word with the index numberã€‚So now it's more ready to be fed
    into something like an embedding layer or other text translation inputã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½ éœ€è¦è€ƒè™‘ç¡®ä¿è¿™äº›ç´¢å¼•çœŸçš„ä¿æŒä¸å˜ã€‚å¦åˆ™ï¼Œå¦‚æœä½ è®­ç»ƒä½ çš„ç¥ç»ç½‘ç»œå¹¶æ”¹å˜è¿™äº›ç´¢å¼•ï¼Œå®ƒæ˜¯ä¸ä¼šå·¥ä½œçš„ã€‚ç„¶åæˆ‘ä»¬å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼ŒåŸºæœ¬ä¸Šæ˜¯å°†åŸå§‹ã€Šé‡‘é“¶å²›ã€‹æ–‡æœ¬ä¸­çš„æ¯ä¸ªå•è¯æ›¿æ¢ä¸ºç´¢å¼•å·ã€‚å› æ­¤ç°åœ¨å®ƒæ›´é€‚åˆè¢«è¾“å…¥åˆ°åƒåµŒå…¥å±‚æˆ–å…¶ä»–æ–‡æœ¬ç¿»è¯‘è¾“å…¥ä¸­ã€‚
- en: We're going to now this is just like what we did in charactersã€‚ except we're
    creating sequences of wordsï¼Œ not sequences of characters so this is essentially
    the same code we're dealing with word sequences of up to six words step just means
    that we move forward three words each time otherwise we would have a of a lot
    of redundancy in here because if the word if the sentences this is a testã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†åšçš„è¿™é¡¹å·¥ä½œä¸æˆ‘ä»¬åœ¨å­—ç¬¦ä¸­æ‰€åšçš„ç±»ä¼¼ï¼Œåªä¸è¿‡æˆ‘ä»¬åˆ›å»ºçš„æ˜¯å•è¯åºåˆ—ï¼Œè€Œä¸æ˜¯å­—ç¬¦åºåˆ—ã€‚å› æ­¤ï¼Œè¿™å®é™…ä¸Šæ˜¯ç›¸åŒçš„ä»£ç ï¼Œæˆ‘ä»¬å¤„ç†æœ€å¤šå…­ä¸ªå•è¯çš„å•è¯åºåˆ—ã€‚â€œæ­¥è¿›â€æ„å‘³ç€æˆ‘ä»¬æ¯æ¬¡å‘å‰ç§»åŠ¨ä¸‰ä¸ªå•è¯ï¼Œå¦åˆ™æˆ‘ä»¬åœ¨è¿™é‡Œä¼šæœ‰å¾ˆå¤šå†—ä½™ï¼Œå› ä¸ºå¦‚æœå¥å­æ˜¯â€œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•â€ã€‚
- en: the first sequence is going to be this is the next sequence you would move forward
    three words so you wouldn't immediately start with the second second wordã€‚The
    smaller you make this numberï¼Œ the more of repetition that you will have and then
    we see what these sequences look likeã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªåºåˆ—å°†æ˜¯â€œè¿™æ˜¯â€ï¼Œä¸‹ä¸€ä¸ªåºåˆ—ä½ å°†å‘å‰ç§»åŠ¨ä¸‰ä¸ªå•è¯ï¼Œå› æ­¤ä½ ä¸ä¼šç«‹å³å¼€å§‹äºç¬¬äºŒä¸ªå•è¯ã€‚ä½ å°†è¿™ä¸ªæ•°å­—åšå¾—è¶Šå°ï¼Œä½ å°±ä¼šæœ‰è¶Šå¤šçš„é‡å¤ï¼Œç„¶åæˆ‘ä»¬çœ‹çœ‹è¿™äº›åºåˆ—æ˜¯ä»€ä¹ˆæ ·çš„ã€‚
- en: at least the first five So these are the sequences that we're training on we're
    training the neural network if you have this wordã€‚ this wordï¼Œ this wordï¼Œ this
    wordï¼Œ this word and this word then what is the next word then we vectorize it
    this is essentially just changing this into lumpy arrays so that we can actually
    train the neural network with itã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘å‰äº”ä¸ªã€‚è¿™äº›æ˜¯æˆ‘ä»¬æ­£åœ¨è®­ç»ƒçš„åºåˆ—ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒç¥ç»ç½‘ç»œã€‚å¦‚æœä½ æœ‰è¿™ä¸ªå•è¯ã€è¿™ä¸ªå•è¯ã€è¿™ä¸ªå•è¯ã€è¿™ä¸ªå•è¯ã€è¿™ä¸ªå•è¯å’Œè¿™ä¸ªå•è¯ï¼Œé‚£ä¹ˆä¸‹ä¸€ä¸ªå•è¯æ˜¯ä»€ä¹ˆï¼Ÿç„¶åæˆ‘ä»¬å°†å…¶å‘é‡åŒ–ï¼Œè¿™æœ¬è´¨ä¸Šåªæ˜¯å°†å…¶è½¬æ¢ä¸ºç¬¨é‡çš„æ•°ç»„ï¼Œä»¥ä¾¿æˆ‘ä»¬å®é™…èƒ½å¤Ÿç”¨å®ƒæ¥è®­ç»ƒç¥ç»ç½‘ç»œã€‚
- en: this is exactly the same as from the character level encoding that we did but
    it is really just building these upã€‚Sort of one by one and then also building
    the Y so that we haveã€‚The dummy variables coming out on the Y side so that we
    can predict it these we are using dummy variablesã€‚ So this is the X shapeã€‚ This
    makes it very evident how we're encoding thisã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æˆ‘ä»¬è¿›è¡Œçš„å­—ç¬¦çº§ç¼–ç å®Œå…¨ç›¸åŒï¼Œä½†å®é™…ä¸Šå°±æ˜¯é€ä¸ªæ„å»ºè¿™äº›ã€‚åŒæ—¶ä¹Ÿæ„å»ºYï¼Œä»¥ä¾¿æˆ‘ä»¬æœ‰è™šæ‹Ÿå˜é‡ä»Yç«¯è¾“å‡ºï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨è™šæ‹Ÿå˜é‡ã€‚è¿™å°±æ˜¯Xçš„å½¢çŠ¶ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¦‚ä½•ç¼–ç è¿™ä¸€ç‚¹éå¸¸æ˜æ˜¾ã€‚
- en: So the X shape is essentially we have 32005 sequences that we generated the
    max sequence sizes 6ã€‚ and then we have this many values because we have dummy
    variables coming inã€‚ essentiallyã€‚ we have a dummy variable for each of these sixã€‚
    So this is really a lot of data that we've pregeneratedã€‚ This is where' using
    an embedding layer could make this more efficientï¼Œ but it fits into Ramã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥Xå½¢çŠ¶åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬ç”Ÿæˆçš„32005ä¸ªåºåˆ—ï¼Œæœ€å¤§åºåˆ—å¤§å°æ˜¯6ã€‚ç„¶åæˆ‘ä»¬æœ‰è¿™ä¹ˆå¤šå€¼ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰è™šæ‹Ÿå˜é‡è¿›å…¥ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬ä¸ºè¿™å…­ä¸ªæ¯ä¸ªéƒ½æœ‰ä¸€ä¸ªè™šæ‹Ÿå˜é‡ã€‚æ‰€ä»¥è¿™å®é™…ä¸Šæ˜¯æˆ‘ä»¬é¢„ç”Ÿæˆçš„å¾ˆå¤šæ•°æ®ã€‚è¿™æ˜¯ä½¿ç”¨åµŒå…¥å±‚å¯ä»¥ä½¿å…¶æ›´é«˜æ•ˆçš„åœ°æ–¹ï¼Œä½†å®ƒé€‚åˆå†…å­˜ã€‚
- en: So I'm largely happy with itã€‚ The y is similarã€‚ we don't have the six here because
    we're not predicting sequencesã€‚ We're just predicting the next characterã€‚ So we
    have the same number of rowsã€‚ but now we have these dummy variables that tell
    us which of those 6400 words It's actually going to beã€‚ And you can see what the
    dummies look like here essentially we're creatingã€‚Very smallï¼Œ simple LSTMã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¯¹æ­¤åŸºæœ¬æ»¡æ„ã€‚yæ˜¯ç›¸ä¼¼çš„ã€‚æˆ‘ä»¬è¿™é‡Œæ²¡æœ‰å…­ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬ä¸é¢„æµ‹åºåˆ—ã€‚æˆ‘ä»¬åªæ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ç›¸åŒæ•°é‡çš„è¡Œï¼Œä½†ç°åœ¨æˆ‘ä»¬æœ‰è¿™äº›è™šæ‹Ÿå˜é‡ï¼Œå‘Šè¯‰æˆ‘ä»¬6400ä¸ªå•è¯ä¸­å®é™…ä¼šæ˜¯å“ªä¸ªã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™äº›è™šæ‹Ÿå˜é‡çš„æ ·å­ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬åœ¨åˆ›å»ºéå¸¸å°çš„ã€ç®€å•çš„LSTMã€‚
- en: Has 128ã€‚Nuronsã€‚And it's layerã€‚We're going to optimize with Rs prop againã€‚ keeping
    it similar to the example that I pulled in from the character encoding Now this
    is not a formal example from Kira's I just modified the previous example from
    Kiras for the LSTM generation I just modified this to actually work with words
    instead of characters we print a summary it's got like 4 million weights that
    we're training on this is the sample function this is exactly like the characterbased
    one we're essentially doing a softm function where the temperature determines
    how conservative it's going to be in terms of the sentences how willing it is
    to take risks so you get more creative with that is a higher number but more error
    prone with it as a lower number and this is essentially a softm essentially ensuring
    that the probabilities of all of those 66400 words that are in the vocabulary
    that's trying to predict for the nextã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰128ä¸ªç¥ç»å…ƒã€‚å®ƒçš„å±‚ã€‚æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨Rs propè¿›è¡Œä¼˜åŒ–ï¼Œä¿æŒä¸æˆ‘ä»å­—ç¬¦ç¼–ç ä¸­æå–çš„ç¤ºä¾‹ç±»ä¼¼ã€‚ç°åœ¨è¿™ä¸æ˜¯Kiraçš„æ­£å¼ç¤ºä¾‹ï¼Œæˆ‘åªæ˜¯ä¿®æ”¹äº†Kiraçš„å‰ä¸€ä¸ªç¤ºä¾‹ä»¥è¿›è¡ŒLSTMç”Ÿæˆï¼Œæˆ‘åªæ˜¯ä¿®æ”¹å®ƒä»¥å®é™…å¤„ç†å•è¯è€Œä¸æ˜¯å­—ç¬¦ã€‚æˆ‘ä»¬æ‰“å°ä¸€ä¸ªæ‘˜è¦ï¼Œå®ƒå¤§çº¦æœ‰400ä¸‡ä¸ªæƒé‡ï¼Œæˆ‘ä»¬æ­£åœ¨è®­ç»ƒï¼Œè¿™æ˜¯æ ·æœ¬å‡½æ•°ï¼Œè¿™ä¸åŸºäºå­—ç¬¦çš„å®Œå…¨ç›¸åŒã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šåœ¨åšä¸€ä¸ªsoftmå‡½æ•°ï¼Œæ¸©åº¦å†³å®šäº†å®ƒåœ¨å¥å­æ–¹é¢çš„ä¿å®ˆç¨‹åº¦ï¼Œæ„¿æ„å†’é™©çš„ç¨‹åº¦ï¼Œå› æ­¤æ•°å­—è¶Šé«˜ï¼Œåˆ›é€ åŠ›è¶Šå¼ºï¼Œä½†æ•°å­—è¶Šä½åˆ™æ›´å®¹æ˜“å‡ºé”™ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªsoftmï¼Œç¡®ä¿åœ¨è¯•å›¾é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æ—¶ï¼Œæ‰€æœ‰66400ä¸ªè¯æ±‡çš„æ¦‚ç‡ã€‚
- en: That the probabilities of each of those do sum to one because they're probabilities
    because we're going to take essentially the highest oneã€‚ and that is what we're
    going to predict as the next wordã€‚ This is very similar to the previous exampleã€‚
    We're calling this on Epoch end and we're essentially generating some text on
    each of these iterations as it goes through trainingã€‚ and we can see that the
    text that it's generating gets better and better and betterã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¦‚ç‡çš„æ€»å’Œä¸º1ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æ¦‚ç‡ï¼Œå› ä¸ºæˆ‘ä»¬å°†åŸºæœ¬ä¸Šå–æœ€é«˜çš„é‚£ä¸ªã€‚è¿™å°±æ˜¯æˆ‘ä»¬å°†é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è¿™ä¸ä¹‹å‰çš„ä¾‹å­éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªè¿­ä»£ç»“æŸæ—¶è°ƒç”¨å®ƒï¼ŒåŸºæœ¬ä¸Šåœ¨æ¯æ¬¡è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆä¸€äº›æ–‡æœ¬ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒç”Ÿæˆçš„æ–‡æœ¬è¶Šæ¥è¶Šå¥½ã€‚
- en: We're going to generate 100 words for each of theseã€‚ We're going to build up
    the input sequenceã€‚ So we're going to randomly sample six or however big that
    input vector was for the sequenceã€‚ I think it was six words if memory servesã€‚
    Yeahï¼Œ excellent was6ã€‚ So we're sampling groups of six words and we begin trainingã€‚
    We'll scroll down Now this is a lotã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¸ºæ¯ä¸ªç”Ÿæˆ100ä¸ªå•è¯ã€‚æˆ‘ä»¬å°†æ„å»ºè¾“å…¥åºåˆ—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†éšæœºæŠ½æ ·å…­ä¸ªï¼Œæˆ–è€…æ— è®ºé‚£ä¸ªè¾“å…¥å‘é‡ä¸ºå¤šå°‘ã€‚æˆ‘è®°å¾—æ˜¯å…­ä¸ªå•è¯ã€‚æ˜¯çš„ï¼Œç¡®å®æ˜¯6ã€‚æ‰€ä»¥æˆ‘ä»¬æ­£åœ¨æŠ½æ ·å…­ä¸ªå•è¯çš„ç»„ï¼Œå¼€å§‹è®­ç»ƒã€‚ç°åœ¨æˆ‘ä»¬å°†å‘ä¸‹æ»šåŠ¨ï¼Œè¿™å¾ˆå¤šã€‚
- en: this was training for a whileã€‚ This is why I'm not running these for the videoã€‚
    it would take quite a whileã€‚ So temperatureï¼Œ this is a pretty conservative temperature
    and this is wellã€‚the trainingã€‚ you can see the pirate story that it's inventing
    Israelã€‚ I ourselves sunk pieces It reply towards make me about among businessã€‚
    thus Morgan newsã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å·²ç»è®­ç»ƒäº†ä¸€æ®µæ—¶é—´ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘æ²¡æœ‰ä¸ºè§†é¢‘è¿è¡Œè¿™äº›ã€‚å®ƒä¼šèŠ±è´¹ç›¸å½“é•¿çš„æ—¶é—´ã€‚æ‰€ä»¥æ¸©åº¦ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“ä¿å®ˆçš„æ¸©åº¦ï¼Œè€Œè¿™ä¹Ÿæ˜¯è®­ç»ƒã€‚ä½ å¯ä»¥çœ‹åˆ°å®ƒæ­£åœ¨åˆ›é€ çš„æµ·ç›—æ•…äº‹ã€‚æˆ‘è‡ªå·±æ²‰æ²¡äº†å‡ æ®µï¼Œå®ƒå›å¤è®©æˆ‘åœ¨å•†ä¸šä¸­æ„Ÿåˆ°ä¸å®‰ã€‚å› æ­¤æ‘©æ ¹æ–°é—»ã€‚
- en: Davy in mint returnã€‚ I'm not seeing really any grammatical errorã€‚ So that's
    really kind of coolã€‚ Another thing that it's doing that I just find fascinating
    is my tokenizerã€‚ I probably didn't spend enough time on the tokenizerã€‚ but it's
    taking the I and illã€‚ those are two different words in the vocabularyã€‚ That's
    why there's a space in here because we're putting the spaces inã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Davy åœ¨è–„è·ä¸­å›å½’ã€‚æˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•è¯­æ³•é”™è¯¯ï¼Œè¿™çœŸçš„å¾ˆé…·ã€‚è¿˜æœ‰ä¸€ä»¶æˆ‘è§‰å¾—å¾ˆè¿·äººçš„äº‹æ˜¯æˆ‘çš„åˆ†è¯å™¨ã€‚æˆ‘å¯èƒ½æ²¡æœ‰èŠ±è¶³å¤Ÿçš„æ—¶é—´åœ¨åˆ†è¯å™¨ä¸Šï¼Œä½†å®ƒå°† I å’Œ ill
    è§†ä¸ºè¯æ±‡ä¸­çš„ä¸¤ä¸ªä¸åŒè¯ã€‚è¿™å°±æ˜¯è¿™é‡Œæœ‰ç©ºæ ¼çš„åŸå› ï¼Œå› ä¸ºæˆ‘ä»¬æ·»åŠ äº†ç©ºæ ¼ã€‚
- en: It doesn't know what spaces areã€‚ but it figures out that usually after an Iï¼Œ
    it'll be illã€‚ or it'll be Sã€‚ it's not like tacking this apostrophe S on fell or
    something like thisã€‚ It really is truly figuring out the sentence structureã€‚ and
    that's reallyã€‚ really cool and where natural language processing has reallyã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¸çŸ¥é“ä»€ä¹ˆæ˜¯ç©ºæ ¼ï¼Œä½†å®ƒå‘ç°é€šå¸¸åœ¨ I åé¢ä¼šæ˜¯ illï¼Œæˆ–è€…ä¼šæ˜¯ Sã€‚å®ƒå¹¶ä¸æ˜¯åƒæŠŠè¿™ä¸ªæ’‡å· S é™„åŠ åœ¨ fell ä¸Šé‚£æ ·ï¼Œè€Œæ˜¯çœŸæ­£ç†è§£å¥å­ç»“æ„ã€‚è¿™çœŸçš„éå¸¸é…·ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†åœ¨è¿™ä¸€ç‚¹ä¸Šå–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚
- en: really benefited from some of the deep learning technologyã€‚So this is how you
    would do that same text generator at the word level The word levelã€‚ I think out
    of the box without a lot of effort trying to optimize does produce very realistic
    words and sentence structure and believable text as far as pirate storiesã€‚ Thank
    you for watching this video on the next videoã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„åº”ç”¨ä½¿å¾—æ–‡æœ¬ç”Ÿæˆåœ¨å•è¯å±‚é¢ä¸Šéå¸¸æœ‰æ•ˆã€‚æˆ‘è®¤ä¸ºå³ä½¿ä¸åšå¤ªå¤šä¼˜åŒ–ï¼Œä¹Ÿèƒ½äº§ç”Ÿéå¸¸çœŸå®çš„å•è¯å’Œå¥å­ç»“æ„ï¼Œå°¤å…¶æ˜¯å…³äºæµ·ç›—æ•…äº‹çš„å¯ä¿¡æ–‡æœ¬ã€‚æ„Ÿè°¢è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼ŒæœŸå¾…ä¸‹ä¸€ä¸ªè§†é¢‘ã€‚
- en: we're going to look deeper into natural language processing with curs and look
    at embedding layersã€‚ğŸ˜Šã€‚![](img/be18f4bf025fc5921b83d7e947e8af7a_3.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸å…‰æ ‡ï¼Œå¹¶æŸ¥çœ‹åµŒå…¥å±‚ã€‚ğŸ˜Šã€‚![](img/be18f4bf025fc5921b83d7e947e8af7a_3.png)
- en: This content changes oftenï¼Œ so subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligenceã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ï¼Œæ‰€ä»¥è¯·è®¢é˜…é¢‘é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£è¯¥è¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜çš„æ›´æ–°ã€‚
