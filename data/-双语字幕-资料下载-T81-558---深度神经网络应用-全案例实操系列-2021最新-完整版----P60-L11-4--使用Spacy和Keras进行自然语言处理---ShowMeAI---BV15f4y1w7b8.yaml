- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P60：L11.4- 使用Spacy和Keras进行自然语言处理
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P60：L11.4- 使用Spacy和Keras进行自然语言处理
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeff。 welcome to applications of deep neural networks In this video。
    we're going to take a look at natural language processing in particular。 we're
    going to use Spacey Cars and word tove together for the latest on my AI course
    and projects。 click subscribe in the bell next to it to be notified of every new
    video previously we did text generation where we looked at treasure Island。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是杰夫。欢迎来到深度神经网络的应用。在这个视频中，我们将看看自然语言处理，特别是，我们将结合使用Spacey、Keras和word2vec，了解我的AI课程和项目的最新动态。点击订阅，并点击旁边的铃铛，以接收每个新视频的通知。之前我们进行了文本生成，查看了《金银岛》。
- en: and we got the neural network to generate its own pirate stories。 We're going
    to continue with this。 but this time we're going to use word level text generationPrevious
    we use character level text generation。 Now there's much debate as far as which
    of these two you should use。 I tend to prefer generating at the character level
    and going truly end to end with the neural network。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让神经网络生成自己的海盗故事。我们将继续这个项目，但这次我们将使用单词级文本生成。之前我们使用的是字符级文本生成。关于这两者，存在很多争论。我的倾向是生成字符级，并与神经网络真正实现端到端。
- en: letting it figure out grammar and letting it figure out sentence structure not
    really doing a lot of feature engineering on nouns and verbs and that sort of
    thing。 but you can certainly do word level。 And if you do word level。 then it
    might be useful to provide additional features to let。😊。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让它自己理清语法和句子结构，而不是在名词和动词等方面进行大量特征工程，但你当然可以进行单词级别的处理。如果你选择单词级别，那么提供额外的特征可能是有用的。😊
- en: '![](img/be18f4bf025fc5921b83d7e947e8af7a_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be18f4bf025fc5921b83d7e947e8af7a_1.png)'
- en: No if it's dealing with a noun or a verb or other things。 We won't get quite
    that complicated with this example。 but I'll show you how we can use this to generate
    text at a word by word level this is what we use for captioning。 we were generating
    at the word level。 Although you can certainly do captioning at the character level
    as well。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是名词、动词或其他东西，这个问题不算复杂。我们不会在这个例子中做得太复杂，但我会展示如何逐字生成文本，这是我们在生成字幕时所用的。尽管你确实可以在字符级别进行字幕生成。
- en: So here I imported the packages and libraries that I need both for spacey and
    for Kis。 I'm getting the treasure island text just like I did before。 And now
    I am dealing with spacey So I am going to use spacey now to do the tokenization
    to break the treasure island text apart so that I have it by individual words。
    Now， when we did character level encoding， we didn't have to do this tokenization
    was handled by the neural network。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我导入了需要的包和库，分别用于spacy和Keras。我获取了《金银岛》的文本，就像之前那样。现在我开始处理spacy，所以我将使用spacy进行分词，将《金银岛》的文本拆分为单个单词。当我们进行字符级编码时，不需要进行这种分词，因为分词由神经网络处理。
- en: It learned what spaces were and it learned how to break the words up here since
    we're doing word by word。 we need to do that actual tokenization and spacey is
    what does it for us。 So we're essentially looping through all of the tokens so。All
    of the words that it finds this is just code， this first line， this is a pretty
    useful line。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它学习了什么是空格，并学会了如何将单词拆分开。由于我们按单词进行处理，我们需要进行实际的分词，而spacy正是为我们完成这个任务。因此，我们实际上是遍历所有的token，即它找到的所有单词。这只是代码，这第一行是相当有用的一行。
- en: This reduces the characters to just characters in the ASCI code 0 to 127。 so
    this ensures that you are dealing just with ASII characters in those words。 so
    that strips out a lot of the extra junk characters like this copyright symbol
    in this weird A that removes those from your words then we also strip off any
    white space because once you removes some of these。 there might be white space
    now that was embedded in there。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这将字符减少到ASCII码0到127的字符。因此，这确保你仅处理这些单词中的ASCII字符。这会去掉很多额外的垃圾字符，比如这个奇怪的版权符号和奇怪的字母A，然后我们还会去掉任何空白字符，因为一旦你去掉一些字符，可能会出现嵌入在其中的空白。
- en: We make sure that the tokens are not digits。And that they're not URLs。 emails
    or other things like that。 Now you might want to handle these， but for the simple
    example。 we're simply stripping them Now in the character by character。 we just
    let them pass it in and interestingly， the neural network would learn to generate
    URLs and email addresses and other things like that total number of words we had
    when we did that is 6421。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保标记不是数字，也不是网址、电子邮件或其他类似的东西。现在你可能想处理这些，但对于这个简单的例子，我们只是将它们剔除。现在在字符逐个处理时，我们只是让它们通过，有趣的是，神经网络会学习生成网址、电子邮件地址等。当我们这样做时，单词的总数是6421。
- en: we print out some of these。 Now some of these are numbers， that's a Roman numeral。
    but we'll just go ahead and let it go in that won't necessarily hurt anything。
    we're creating to look ups here。 a word to index。 So this would take a word like
    alone and convert it into a index。 Now if you want to make these more consistent
    you might want to sort these because then the indexes are going to always be the
    same。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印出一些这些内容。现在其中一些是数字，那是罗马数字。但我们会继续让它通过，这不会造成任何伤害。我们在这里创建两个查找表，一个是单词到索引。因此，这会将单词像“孤独”转换为一个索引。现在如果你想使这些更加一致，你可能想对这些进行排序，因为那样索引将始终保持相同。
- en: So for production situations， you need to think about that to make sure that
    these indexes are truly staying the same。 Otherwise， if you train your neural
    network and you change what these indexes are。 It's not going to work。Then we
    tokenize the text that basically goes and takes the original Tresure Island text
    and replaces each word with the index number。So now it's more ready to be fed
    into something like an embedding layer or other text translation input。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在生产环境中，你需要考虑确保这些索引真的保持不变。否则，如果你训练你的神经网络并改变这些索引，它是不会工作的。然后我们对文本进行标记化，基本上是将原始《金银岛》文本中的每个单词替换为索引号。因此现在它更适合被输入到像嵌入层或其他文本翻译输入中。
- en: We're going to now this is just like what we did in characters。 except we're
    creating sequences of words， not sequences of characters so this is essentially
    the same code we're dealing with word sequences of up to six words step just means
    that we move forward three words each time otherwise we would have a of a lot
    of redundancy in here because if the word if the sentences this is a test。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将做的这项工作与我们在字符中所做的类似，只不过我们创建的是单词序列，而不是字符序列。因此，这实际上是相同的代码，我们处理最多六个单词的单词序列。“步进”意味着我们每次向前移动三个单词，否则我们在这里会有很多冗余，因为如果句子是“这是一个测试”。
- en: the first sequence is going to be this is the next sequence you would move forward
    three words so you wouldn't immediately start with the second second word。The
    smaller you make this number， the more of repetition that you will have and then
    we see what these sequences look like。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个序列将是“这是”，下一个序列你将向前移动三个单词，因此你不会立即开始于第二个单词。你将这个数字做得越小，你就会有越多的重复，然后我们看看这些序列是什么样的。
- en: at least the first five So these are the sequences that we're training on we're
    training the neural network if you have this word。 this word， this word， this
    word， this word and this word then what is the next word then we vectorize it
    this is essentially just changing this into lumpy arrays so that we can actually
    train the neural network with it。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 至少前五个。这些是我们正在训练的序列，我们在训练神经网络。如果你有这个单词、这个单词、这个单词、这个单词、这个单词和这个单词，那么下一个单词是什么？然后我们将其向量化，这本质上只是将其转换为笨重的数组，以便我们实际能够用它来训练神经网络。
- en: this is exactly the same as from the character level encoding that we did but
    it is really just building these up。Sort of one by one and then also building
    the Y so that we have。The dummy variables coming out on the Y side so that we
    can predict it these we are using dummy variables。 So this is the X shape。 This
    makes it very evident how we're encoding this。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们进行的字符级编码完全相同，但实际上就是逐个构建这些。同时也构建Y，以便我们有虚拟变量从Y端输出，以便我们可以进行预测。我们使用虚拟变量。这就是X的形状。这使得我们如何编码这一点非常明显。
- en: So the X shape is essentially we have 32005 sequences that we generated the
    max sequence sizes 6。 and then we have this many values because we have dummy
    variables coming in。 essentially。 we have a dummy variable for each of these six。
    So this is really a lot of data that we've pregenerated。 This is where' using
    an embedding layer could make this more efficient， but it fits into Ram。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以X形状基本上是我们生成的32005个序列，最大序列大小是6。然后我们有这么多值，因为我们有虚拟变量进入。基本上，我们为这六个每个都有一个虚拟变量。所以这实际上是我们预生成的很多数据。这是使用嵌入层可以使其更高效的地方，但它适合内存。
- en: So I'm largely happy with it。 The y is similar。 we don't have the six here because
    we're not predicting sequences。 We're just predicting the next character。 So we
    have the same number of rows。 but now we have these dummy variables that tell
    us which of those 6400 words It's actually going to be。 And you can see what the
    dummies look like here essentially we're creating。Very small， simple LSTM。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我对此基本满意。y是相似的。我们这里没有六个，因为我们不预测序列。我们只是预测下一个字符。因此，我们有相同数量的行，但现在我们有这些虚拟变量，告诉我们6400个单词中实际会是哪个。你可以看到这些虚拟变量的样子，基本上我们在创建非常小的、简单的LSTM。
- en: Has 128。Nurons。And it's layer。We're going to optimize with Rs prop again。 keeping
    it similar to the example that I pulled in from the character encoding Now this
    is not a formal example from Kira's I just modified the previous example from
    Kiras for the LSTM generation I just modified this to actually work with words
    instead of characters we print a summary it's got like 4 million weights that
    we're training on this is the sample function this is exactly like the characterbased
    one we're essentially doing a softm function where the temperature determines
    how conservative it's going to be in terms of the sentences how willing it is
    to take risks so you get more creative with that is a higher number but more error
    prone with it as a lower number and this is essentially a softm essentially ensuring
    that the probabilities of all of those 66400 words that are in the vocabulary
    that's trying to predict for the next。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有128个神经元。它的层。我们将再次使用Rs prop进行优化，保持与我从字符编码中提取的示例类似。现在这不是Kira的正式示例，我只是修改了Kira的前一个示例以进行LSTM生成，我只是修改它以实际处理单词而不是字符。我们打印一个摘要，它大约有400万个权重，我们正在训练，这是样本函数，这与基于字符的完全相同。我们基本上在做一个softm函数，温度决定了它在句子方面的保守程度，愿意冒险的程度，因此数字越高，创造力越强，但数字越低则更容易出错。这基本上是一个softm，确保在试图预测下一个单词时，所有66400个词汇的概率。
- en: That the probabilities of each of those do sum to one because they're probabilities
    because we're going to take essentially the highest one。 and that is what we're
    going to predict as the next word。 This is very similar to the previous example。
    We're calling this on Epoch end and we're essentially generating some text on
    each of these iterations as it goes through training。 and we can see that the
    text that it's generating gets better and better and better。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率的总和为1，因为它们是概率，因为我们将基本上取最高的那个。这就是我们将预测的下一个单词。这与之前的例子非常相似。我们在每个迭代结束时调用它，基本上在每次训练过程中生成一些文本。我们可以看到它生成的文本越来越好。
- en: We're going to generate 100 words for each of these。 We're going to build up
    the input sequence。 So we're going to randomly sample six or however big that
    input vector was for the sequence。 I think it was six words if memory serves。
    Yeah， excellent was6。 So we're sampling groups of six words and we begin training。
    We'll scroll down Now this is a lot。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个生成100个单词。我们将构建输入序列。因此，我们将随机抽样六个，或者无论那个输入向量为多少。我记得是六个单词。是的，确实是6。所以我们正在抽样六个单词的组，开始训练。现在我们将向下滚动，这很多。
- en: this was training for a while。 This is why I'm not running these for the video。
    it would take quite a while。 So temperature， this is a pretty conservative temperature
    and this is well。the training。 you can see the pirate story that it's inventing
    Israel。 I ourselves sunk pieces It reply towards make me about among business。
    thus Morgan news。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经训练了一段时间。这就是为什么我没有为视频运行这些。它会花费相当长的时间。所以温度，这是一个相当保守的温度，而这也是训练。你可以看到它正在创造的海盗故事。我自己沉没了几段，它回复让我在商业中感到不安。因此摩根新闻。
- en: Davy in mint return。 I'm not seeing really any grammatical error。 So that's
    really kind of cool。 Another thing that it's doing that I just find fascinating
    is my tokenizer。 I probably didn't spend enough time on the tokenizer。 but it's
    taking the I and ill。 those are two different words in the vocabulary。 That's
    why there's a space in here because we're putting the spaces in。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Davy 在薄荷中回归。我没有看到任何语法错误，这真的很酷。还有一件我觉得很迷人的事是我的分词器。我可能没有花足够的时间在分词器上，但它将 I 和 ill
    视为词汇中的两个不同词。这就是这里有空格的原因，因为我们添加了空格。
- en: It doesn't know what spaces are。 but it figures out that usually after an I，
    it'll be ill。 or it'll be S。 it's not like tacking this apostrophe S on fell or
    something like this。 It really is truly figuring out the sentence structure。 and
    that's really。 really cool and where natural language processing has really。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它不知道什么是空格，但它发现通常在 I 后面会是 ill，或者会是 S。它并不是像把这个撇号 S 附加在 fell 上那样，而是真正理解句子结构。这真的非常酷，自然语言处理在这一点上取得了巨大的进步。
- en: really benefited from some of the deep learning technology。So this is how you
    would do that same text generator at the word level The word level。 I think out
    of the box without a lot of effort trying to optimize does produce very realistic
    words and sentence structure and believable text as far as pirate stories。 Thank
    you for watching this video on the next video。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术的应用使得文本生成在单词层面上非常有效。我认为即使不做太多优化，也能产生非常真实的单词和句子结构，尤其是关于海盗故事的可信文本。感谢观看这个视频，期待下一个视频。
- en: we're going to look deeper into natural language processing with curs and look
    at embedding layers。😊。![](img/be18f4bf025fc5921b83d7e947e8af7a_3.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨自然语言处理与光标，并查看嵌入层。😊。![](img/be18f4bf025fc5921b83d7e947e8af7a_3.png)
- en: This content changes often， so subscribe to the channel to stay up to date on
    this course and other topics in artificial intelligence。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内容经常变化，所以请订阅频道，以便及时了解该课程和其他人工智能主题的更新。
