- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P26ï¼šL4.3- Kerasä»‹ç» - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P26ï¼šL4.3- Kerasä»‹ç» - ShowMeAI - BV1Jm4y1X7UL
- en: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_0.png)'
- en: In this videoï¼Œ I'm going to give you a very quick introduction to how our transformer
    models work together with TensorFlow and Kasã€‚The very short explanation is that
    all of our TensorF models are also Keis model objectsã€‚ and so they have the standard
    KeIS model APIã€‚ğŸ˜Šï¼ŒIf you're an experienced machine learning engineer who's used
    Kais a lotã€‚ that's probably all you need to know to start working with themï¼Œ but
    for everyone elseã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘å°†ç»™ä½ ä¸€ä¸ªå…³äºæˆ‘ä»¬çš„ Transformer æ¨¡å‹å¦‚ä½•ä¸ TensorFlow å’Œ Keras ååŒå·¥ä½œ çš„ç®€è¦ä»‹ç»ã€‚éå¸¸ç®€å•çš„è§£é‡Šæ˜¯ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„
    TensorFlow æ¨¡å‹ä¹Ÿæ˜¯ Keras æ¨¡å‹å¯¹è±¡ï¼Œå› æ­¤å®ƒä»¬æ‹¥æœ‰æ ‡å‡†çš„ Keras æ¨¡å‹ APIã€‚ğŸ˜Š å¦‚æœä½ æ˜¯ä¸€ä¸ªæœ‰ç»éªŒçš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·²ç»é¢‘ç¹ä½¿ç”¨ Kerasï¼Œé‚£å¯èƒ½è¿™å°±æ˜¯ä½ å¼€å§‹ä¸ä¹‹åˆä½œæ‰€éœ€çŸ¥é“çš„ä¸€åˆ‡ï¼Œä½†å¯¹äºå…¶ä»–äººæ¥è¯´ã€‚
- en: including the prodigal Pytorrch engineers out there who are returning to the
    foldã€‚ I'm going to quickly introduce Keis models and how we work with themã€‚In
    other videosã€‚ which I'll link belowï¼Œ I'll run through training with Keis models
    in more detailï¼Œ but firstã€‚ at a high levelï¼Œ what is a Keis modelï¼Ÿ
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…æ‹¬é‚£äº›å›å½’çš„æµªå­ PyTorch å·¥ç¨‹å¸ˆã€‚æˆ‘å°†è¿…é€Ÿä»‹ç» Keras æ¨¡å‹ä»¥åŠæˆ‘ä»¬å¦‚ä½•ä¸å®ƒä»¬åˆä½œã€‚åœ¨æˆ‘ä¸‹é¢é“¾æ¥çš„å…¶ä»–è§†é¢‘ä¸­ï¼Œæˆ‘å°†æ›´è¯¦ç»†åœ°è®²è§£ Keras
    æ¨¡å‹çš„è®­ç»ƒï¼Œä½†é¦–å…ˆï¼Œä»é«˜å±‚æ¬¡ä¸Šçœ‹ï¼ŒKeras æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ
- en: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_2.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_2.png)'
- en: So your model basically contains your entire networkã€‚ it contains the layers
    and the weights for those layers and also tells the model what to do with themã€‚
    so it defines the whole path all the way from your inputs to your outputsã€‚If you've
    used Keis beforeï¼Œ you probably started using model objects by building them out
    by handã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ çš„æ¨¡å‹åŸºæœ¬ä¸ŠåŒ…å«äº†æ•´ä¸ªç½‘ç»œã€‚å®ƒåŒ…å«è¿™äº›å±‚å’Œå±‚çš„æƒé‡ï¼Œå¹¶ä¸”è¿˜å‘Šè¯‰æ¨¡å‹å¦‚ä½•å¤„ç†è¿™äº›æƒé‡ã€‚å› æ­¤ï¼Œå®ƒå®šä¹‰äº†ä»è¾“å…¥åˆ°è¾“å‡ºçš„æ•´ä¸ªè·¯å¾„ã€‚å¦‚æœä½ ä»¥å‰ä½¿ç”¨è¿‡ Kerasï¼Œå¯èƒ½æ˜¯é€šè¿‡æ‰‹åŠ¨æ„å»ºæ¨¡å‹å¯¹è±¡æ¥å¼€å§‹çš„ã€‚
- en: you added one layer after anotherï¼Œ maybe using the model dot ad or the functional
    approachã€‚And there's nothing wrong with thatã€‚ you can lots of great models are
    built that wayã€‚ but you can also preload an entire model weights and allã€‚ And
    this is really helpful because if you as you can see hereã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä¸€ä¸ªå±‚ä¸€ä¸ªå±‚åœ°æ·»åŠ ï¼Œä¹Ÿè®¸ä½¿ç”¨ model.dot.add æˆ–åŠŸèƒ½æ€§æ–¹æ³•ã€‚è¿™æ²¡ä»€ä¹ˆé”™ï¼Œå¾ˆå¤šå‡ºè‰²çš„æ¨¡å‹éƒ½æ˜¯è¿™æ ·æ„å»ºçš„ã€‚ä½†ä½ ä¹Ÿå¯ä»¥ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬æƒé‡ã€‚è¿™éå¸¸æœ‰å¸®åŠ©ï¼Œå› ä¸ºå¦‚ä½ æ‰€è§ã€‚
- en: if you try reading the paper or if you try looking at the codeã€‚ you'll see the
    inside of a transformer is pretty complex and writing it all out from scratch
    and getting it right would be hired even for an experienced machine learning engineerã€‚
    but because it's all packed inside a modelï¼Œ you don't need to worry about that
    complexity on that if you don't want toã€‚ if you're a researcher if you want to
    really dig in thereï¼Œ you canã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å°è¯•é˜…è¯»è®ºæ–‡æˆ–æŸ¥çœ‹ä»£ç ï¼Œä½ ä¼šå‘ç° Transformer çš„å†…éƒ¨ç»“æ„ç›¸å½“å¤æ‚ï¼Œä»å¤´å¼€å§‹ç¼–å†™å¹¶å‡†ç¡®å®ç°è¿™ä¸€åˆ‡å¯¹äºä¸€ä½æœ‰ç»éªŒçš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆæ¥è¯´ä¹Ÿæ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚ä½†å› ä¸ºè¿™ä¸€åˆ‡éƒ½æ‰“åŒ…åœ¨æ¨¡å‹å†…éƒ¨ï¼Œå¦‚æœä½ ä¸æƒ³æ‹…å¿ƒé‚£ç§å¤æ‚æ€§ï¼Œä½ å°±ä¸éœ€è¦æ‹…å¿ƒã€‚å¦‚æœä½ æ˜¯ä¸€åç ”ç©¶äººå‘˜ï¼Œæƒ³æ·±å…¥æ¢è®¨ï¼Œå½“ç„¶å¯ä»¥ã€‚
- en: but you can also just load a pre-trained preconfigured transformer model in
    just one line of codeã€‚ğŸ˜Šã€‚![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_4.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥ä»…ç”¨ä¸€è¡Œä»£ç åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ã€é¢„é…ç½®çš„ Transformer æ¨¡å‹ã€‚ğŸ˜Šã€‚![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_4.png)
- en: And when I mentioned earlier about the KaIS APIï¼Œ the advantage of it is that
    whether you write your own model from scratch or load a pre-trained oneã€‚ you interact
    with the model through that same API so you use exactly the same few methods and
    you're going to see them again and again these methods like fitã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘æ—©äº›æ—¶å€™æåˆ° Keras API æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿åœ¨äºï¼Œæ— è®ºä½ æ˜¯ä»å¤´å¼€å§‹ç¼–å†™è‡ªå·±çš„æ¨¡å‹ï¼Œè¿˜æ˜¯åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œä½ éƒ½é€šè¿‡ç›¸åŒçš„ API ä¸æ¨¡å‹äº’åŠ¨ï¼Œå› æ­¤ä½ ä½¿ç”¨çš„æ­£æ˜¯é‚£äº›å°‘æ•°ç›¸åŒçš„æ–¹æ³•ï¼Œä½ ä¼šä¸€æ¬¡åˆä¸€æ¬¡åœ°çœ‹åˆ°è¿™äº›æ–¹æ³•ï¼Œæ¯”å¦‚
    fitã€‚
- en: compile and predict and like I mentioned will see cover concrete examples of
    how to use those methods in the videos I'll link below For nowã€‚ the key thing
    to take away from this video if you've never seen Keis before is that this neat
    encapsulation means that all the complexity of a huge neuralnet becomes manageable
    because you interact with it in exactly the same way using exactly the same methodsã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–è¯‘å’Œé¢„æµ‹ï¼Œæ­£å¦‚æˆ‘æåˆ°çš„ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å…·ä½“çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨æˆ‘ä¸‹é¢é“¾æ¥çš„è§†é¢‘ä¸­ä½¿ç”¨è¿™äº›æ–¹æ³•ã€‚ç°åœ¨ï¼Œè¦ä»è¿™ä¸ªè§†é¢‘ä¸­å¸¦èµ°çš„å…³é”®ç‚¹æ˜¯ï¼Œå¦‚æœä½ ä»¥å‰ä»æœªè§è¿‡Kerasï¼Œé‚£ä¹ˆè¿™ç§æ•´æ´çš„å°è£…æ„å‘³ç€åºå¤§çš„ç¥ç»ç½‘ç»œçš„æ‰€æœ‰å¤æ‚æ€§å˜å¾—å¯ç®¡ç†ï¼Œå› ä¸ºä½ ä»¥å®Œå…¨ç›¸åŒçš„æ–¹å¼ä¸ä¹‹äº’åŠ¨ï¼Œä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ–¹æ³•ã€‚
- en: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_6.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_6.png)'
- en: Whether it's a huge pretrained language model or a simple model you wrote out
    by handã€‚![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_8.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ˜¯ä¸€ä¸ªå·¨å¤§çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¿˜æ˜¯ä½ æ‰‹åŠ¨ç¼–å†™çš„ç®€å•æ¨¡å‹ã€‚![](img/eb8701c93d4ca852bec8b5e00e0fc6a1_8.png)
- en: ã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
