- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P25：L4.4- 反向传播、Nesterov动量和ADAM训练
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P25：L4.4- 反向传播、Nesterov动量和ADAM训练
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeff Heaton。 Welcom to applications of deep neural networks with
    Washington University。 In this video， I am going to show you how。😊，The back propagation
    algorithm works internally。 We'll look at back propagation， classic， also atom。
    Netroov momentum and other techniques that are commonly used to train deep neural
    networks for the latest on among AI course and projects。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是Jeff Heaton。欢迎来到华盛顿大学深度神经网络应用课程。在这个视频中，我将向你展示内部工作的反向传播算法。我们将看看经典的反向传播，还有原子，Netroov动量和其他常用的训练深度神经网络的技术，这是AI课程和项目中最新的内容。
- en: Click subscribe in the bell next to it to be notified of every new video。 So
    classic back propagation。 Back propagation has been around for a while。 It is
    Jeffrey Henton contributed quite a bit to it。 and werebo as well。 So a variety
    of people introduced aspects of back propagation。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 点击旁边的铃铛订阅，以接收每个新视频的通知。如此经典的反向传播。反向传播已经存在了一段时间。Jeffrey Henton对此做出了相当大的贡献。以及werebo也是如此。因此，许多人介绍了反向传播的各个方面。
- en: and has been continued to be built upon over many years。 This equation that
    you see here is sort of your very general training equation。 This just says that
    T。 Now T is the current epoch or time。 This is saying that the weights。 which
    are the weights of theta。 The weights for the current time are equal to the weights
    from the previous time minus V the current time。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 并且多年来一直在继续建立。你在这里看到的这个方程是你非常一般的训练方程。这只是说T。现在T是当前的时期或时间。这是在说权重。这些是theta的权重。当前时间的权重等于前一时间的权重减去当前时间的V。
- en: V。😊。![](img/6491afaa3cf5a798f8c7f93f7b8bf376_1.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: V。😊。![](img/6491afaa3cf5a798f8c7f93f7b8bf376_1.png)
- en: From the current time is just a vector。 All of these are vectors that holds
    the amounts that we're going to change each of the weights by。 So this doesn't
    tell us a whole lot by itself。 It just says that we are going to change the weights。
    And each time by this vector of change amounts。 Now， the vector of change amounts。
    V sub T will see a variety of functions that show us how to calculate v sub T。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当前时间只是一个向量。所有这些都是向量，它们保持了我们将改变每个权重的数量。所以单独的这个并不能告诉我们太多。它只是说我们将改变权重。每次都是通过这个变化量的向量。现在，变化量的向量。V子T将会看到一系列的函数，显示我们如何计算V子T。
- en: The first is classic back propagation。 So if we look at this， this is gradient
    descent。 So you have Eta multiplied by now， Eta is multiplied by the rest of this。
    This is essentially one unit。 This is not。This thing multiplied by the J function。This
    is the Nabla or the upside down Delta or the harp shaped operator。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是经典的反向传播。所以如果我们看这个，这是梯度下降。所以你有Eta乘以现在，Eta乘以其余的部分。这本质上是一个单元。这不是。这个东西乘以J函数。这是Nabla或倒置的Delta或者像竖琴形的运算符。
- en: This says take the gradients of the loss function with respect to the weights
    from the previous time step or the previous epoch。 So this is essentially giving
    you all of the gradients。Multiplied by the learning rate at a common values for
    the learning rate are these 0。10。01。 rarely Would you ever want to make that one
    so that you're fully adding the gradients to the weights that would that would
    simply be too chaotic。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着用损失函数的梯度对先前时间步或先前时期的权重进行梯度下降。所以这基本上给出了所有的梯度。乘以学习率，学习率的常见值是0.10.01。很少你会想要使那一个，这样你就完全将梯度添加到权重中，那将会是太混乱的。
- en: Now， let's see really what the gradient is and how it's actually used。 This
    is a derivative。 It's a partial derivative， So you always take the partial derivative
    of one multivariate calculus。 you take the derivative of one single variable and
    a multivariable expression。 So one single weight with all other weights held constant。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看梯度到底是什么，以及它是如何实际使用的。这是一个导数。这是一个偏导数，所以你总是取一个多元微积分的偏导数。你用所有其他权重保持不变的情况下取一个单一变量的导数和一个多变量表达式。因此，一个单一的权重与所有其他权重保持不变。
- en: So this shows you essentially the error function for a particular weight。 So
    the error for this weight as you adjust the weight。 So if you make the weight0。
    the error is going to be here， then it goes up and then it swings way down。 This
    is potentially true as you vary just one weight in the neural network。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这基本上展示了特定权重的误差函数。当你调整权重时，这个权重的误差。所以如果你使权重0，误差就会在这里，然后上升，然后急剧下降。这在你仅改变神经网络中的一个权重时可能是真实的。
- en: the error function will go up down and change its value。 Typically， we're doing
    gradient。So we want to get the weight all the way to the lowest point here。 Now，
    we don't want to graph。 generate this entire graph and and sample the neural network
    at each of these points。 that would be computationally expensive because there
    is a different chart of this for every single weight in the entire neural network。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数会上下变化其值。通常，我们正在做梯度。所以我们想要把权重完全移到这里的最低点。现在，我们不想要绘制这个整个图表并且在每一个点上对神经网络进行抽样。因为整个神经网络中每一个权重都有不同的图表，这会非常昂贵。
- en: And as you change one weight， all all the others potentially change2。 This is
    where you have to do the partial derivative。 We're doing the partial derivative
    for just one weight。 sayy the weight was currently at 1。5。 So all we would know
    is that the error was here。 we don't have the rest of the lead up or continuation
    of the chart。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当你改变一个权重时，所有其他的权重也可能改变。这就是你必须进行偏导数的地方。我们只为一个权重做偏导数。假设权重目前为1.5。所以我们只知道误差在这里。我们不知道图表的其余部分或继续。
- en: we just have this one point， that point doesn't tell us too much until we take
    the derivative of the loss function。 then that tells us the instantaneous rate
    of change。 So youve got the slope。 this point right here on the error function
    curve。 Notice this line has a negative slope。 But if we if we just added that
    gradient to it， the negative value， whatever that negative slope is。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有这一个点，那个点不告诉我们太多，直到我们对损失函数取导数。然后那告诉我们瞬时变化率。所以你得到了斜率。这个错误函数曲线上的这个点。注意这条线有一个负斜率。但是如果我们只是把梯度加上去，这个负值，无论那个负斜率是多少。
- en: that would decrease the weight。 It would go in the wrong direction。 So if if
    you truly want to go this direction， because were past just a little bit of the
    crest of that hill。 if you want to go in this direction， you need to take the
    opposite of the slope。 That is why up here we're。Ting the sub T because if this，
    I mean， say we're right here。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那将减小权重。它会朝错误的方向走。所以如果你真的想要走这个方向，因为我们已经过了那个山丘的顶部一点点。如果你想朝这个方向走，你需要取斜率的反方向。这就是为什么在这里我们进行了子T，因为如果这样，我是说，假设我们就在这里。
- en: then the line would be very much this this direction。 and that would be a very
    negatively sloped line。 you would want a positive number。 So you continued on
    your way down。 If we were right here， then the slope would be positive。 but we
    would want to subtract one from the weight to continue along this direction。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这条线的方向会非常类似这个方向。并且那将是一条非常负斜率的线。你会想要一个正数。所以你继续走下去。如果我们就在这里，那么斜率将是正的。但我们希望从权重中减去一个来继续沿着这个方向。
- en: So that is classic。Back propagation。 It's governed by the learning rate。 If
    you made your learning rate too big instead set a gradient dissenting down here。
    you'd probably jump completely to the other side of it。 and you would never find
    your way down to this lower value。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是经典的反向传播。它由学习率控制。如果你的学习率设置得太大，而不是设置一个梯度降下来，你可能完全跳到它的另一侧。你永远找不到通向这个更低值的路。
- en: So learning rate just describes how quickly we're going to attempt to push the
    weights to optimal values。 And this link is pretty handy。 It shows a javascript
    application that I wrote that takes you through all the steps of a classic back
    propagation。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率描述了我们尝试将权重推向最优值的速度。这个链接非常有用。它展示了一个我写的JavaScript应用程序，带你经历经典反向传播的所有步骤。
- en: So you can see literally how an entire neural network is calculated for X O
    R。 Next is momentum propagation。 back propagation。 So momentum is something that
    was added to back propagation to prevent from being from settling into a local
    minima。 So local minima could be right here。 It might be that further over here。
    there would be an even more optimal value。 But once the weight gets settled into
    here。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以看到整个神经网络是如何为XOR计算的。接下来是动量传播。反向传播。所以动量是添加到反向传播中的东西，以防止陷入局部最小值。局部最小值可能在这里。可能在这边更进一步会有更优的值。但一旦权重定居到这里。
- en: It's really hard for it to push its way completely out of that valley and continue
    on。いに？
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它真的很难完全推出那个山谷并继续前行。
- en: We have the case here。 The weight， which is where that ball currently is at。
    is essentially stuck in a local minimum。 There might be a global minimum here。
    It's really hard to know where the global minimum is。 It's usually almost impossible。
    So this weight would have been continuing down， down down， but it would potentially
    get stuck here。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一种情况。权重，也就是球当前的位置，实际上卡在了一个局部最小值。这里可能有一个全局最小值。很难知道全局最小值在哪里。这通常几乎是不可能的。因此，这个权重本应该一直下降，但可能会卡在这里。
- en: If not for the momentum that pushes it over the hump and allows it to continue。
    Moment just as its name implies。 You can think of these weights as moving through
    high dimension space。 Moment just gives the gives the weight push and continues
    that push as it maintains its momentum。 This is the formula for momentum。 Now
    we have two hyperparameters。 We have Eta。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不是动量推动它越过这个山峰并让它继续。动量正如其名。你可以把这些权重想象成在高维空间中移动。动量只是给权重一个推动，并在它保持动量的同时持续这种推动。这是动量的公式。现在我们有两个超参数。我们有Eta。
- en: which is the learning rate。 but we also have lambda， which is the momentum rate。
    The first part of this is completely the same as classic back propagation。 You
    are simply taking the gradient。Multiply by the learning rate。 But you have this
    additional term here。 And this is the momentum term。 This is lambda times V t
    -1。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是**学习率**。但我们还有lambda，它是**动量率**。这一部分与经典的反向传播完全相同。你只需计算梯度。乘以学习率。但这里有一个额外的项。这就是动量项。它是lambda乘以V
    t -1。
- en: So whatever our previous delta our previous update was， we're scaling it by
    lambda and adding it。 You're just adding the last update scaled right onto the
    equation with everything else。 That's really all that momentum is。 So as you were
    moving down this。 you would have built up a lot of momentum because you would
    potentially be moving down fairly fast。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以无论我们之前的delta或更新是什么，我们都在用lambda进行缩放并添加它。你只需将最后的更新缩放后直接加到方程中与其他内容一起。这就是动量的全部。因此，当你向下移动时，你会积累很多动量，因为你可能会比较快地向下移动。
- en: Then that change is very much a positive weight that would keep getting added
    on the weight and hopefully push it over the hump and on to possibly out of the
    local minima and onto a better situation。 hopefully， by the way， very common value
    for momentum is 0。9。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个变化很大程度上是一个正的权重，应该不断添加到权重上，希望能把它推过山峰，可能从局部最小值中走出来，进入更好的状态。顺便提一下，动量的一个非常常见的值是0.9。
- en: They usually favor a fair amount of momentum。 learning rate is often much smaller。
    It's usually 0。10。01 or some other negative power of 10。 Next， we're going to
    look at。And online batch propagation so this is an important concept for propagation
    training。 We'll see later that we can configure these values in TensorF and determine
    what the batch sizes will be batchch is simply how many training set elements
    do you need to go through so each of these each of these gradients that we're
    calculating is for a single train set element。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通常偏向于较大的动量。学习率通常要小得多。通常是0.10、0.01或其他负的10的幂。接下来，我们将关注在线批量传播，这是传播训练的重要概念。稍后我们会看到，我们可以在TensorF中配置这些值，并确定批量大小。批量大小就是你需要处理多少个训练集元素，因此我们计算的每一个梯度都是针对单个训练集元素的。
- en: So you might have1 thousand elements in your training set。 How many of those
    you don't have to literally update the weights every single time you calculate
    a training row and get the deltas to the weight。 you can batch those up and you
    batch them up simply by summing up the gradients So you process the first row
    of training training data and you get a vector gradients equal to or changes the
    V equal to the size of the weights and you just you can calculate the next training
    row and you add those gradients onto whatever read had before。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能在训练集中有1000个元素。在你计算每一行训练数据并得到权重的deltas时，不需要每次都更新权重。你可以将它们批量处理，方法是简单地将梯度相加。因此，你处理训练数据的第一行，然后得到一个梯度向量，它等于权重的大小，你可以计算下一行训练数据，并将这些梯度添加到之前的值上。
- en: you keep vector adding the gradients until you've made it to the batch size。
    So a batch size。 if you had a batch size of 10， that means as it's going through
    the training set。 itll make it through 10 elements， and then at the end of the
    10 elements。 it has the gradients that are basically the sum of that whole run。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你不断地向梯度中添加向量，直到达到批量大小。所以批量大小，如果你有一个批量大小为10，这意味着在处理训练集时，它将处理10个元素，然后在这10个元素结束时，它的梯度基本上是整次运行的总和。
- en: and then it will apply the changes to the weights。 Online training simply applies
    the change to the weight as soon。You calculate the gradient just one at a time，
    calculate a gradient for one training row。 apply it to the weights， move on to
    the next training row， calculate its gradients。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它会将更改应用到权重上。在线训练仅在计算梯度时，将更改尽快应用到权重上，你一次只计算一个梯度，为一个训练样本计算梯度。将其应用到权重中，继续下一个训练样本，计算它的梯度。
- en: add it to the weights and continue Having the batch sizes this can provide considerable
    efficiency to the training of the neural network this is also very big data compliant
    because if you've got a very。 very large data set you just need to randomly sample
    many batches from it so many batch training that's another very common technique
    for training neural networks many batches are typically between 32 and 64 in size
    so they're relatively small step and iteration that is just how many training
    cycles has has the neural network gone through step iteration or even and then
    all right now we'll look at stochastic gradient descent which is often used in
    conjunction with many batch training stochastic gradient descent is used to it
    provides a very stochastic or。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将其添加到权重中并继续进行。拥有批量大小可以显著提高神经网络的训练效率，这也是非常符合大数据的，因为如果你有一个非常、非常大的数据集，你只需从中随机抽取许多批次。因此，许多批量训练是训练神经网络的另一种非常常见的技术，许多批量的大小通常在32到64之间，因此它们相对较小。步长和迭代就是神经网络经历了多少训练周期，步长迭代甚至是现在我们来看看随机梯度下降，它通常与许多批量训练结合使用，随机梯度下降用于提供非常随机的结果。
- en: scentWhat's happening is rather than calculating the gradients with the current
    with the entire data set。 you just pick small groups and you keep going through
    these random samples with replacement。 of the neural network training data and
    as you go through each of these one by one by one the error will decrease it'll
    go up sometimes sometimes you'll pick particularly bad sets of training data sometimes
    you'll pick particularly good sets。 It just all pretty much depends。 So stochastic
    gradient descent is often used alone or as part of another training。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的事情是，与使用整个数据集计算梯度相比，你只选择小组，并且不断处理这些随机样本，带有替换。随着你一个接一个地处理这些神经网络训练数据，每次误差都会减少，有时会增加，有时你会选择特别糟糕的训练数据集，有时你会选择特别好的集合。这几乎完全取决于情况。因此，随机梯度下降通常单独使用或作为其他训练的一部分。
- en: It's computationally efficient and it decreases overfitting by focusing only
    on a small number of relatively good weights and also there's a variety of other
    techniques like I said。 back propagation and gradient descent。 that's just the
    main backbone some of these other techniques。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在计算上是高效的，并通过只关注少量相对较好的权重来减少过拟合。此外，还有许多其他技术，比如我说的反向传播和梯度下降，这只是一些主要的基础技术。
- en: what they attempt to solve is the learning rate and momentum。 These are both
    hyperparameter。 These are numbers that you need to tune along with everything
    else。 you thought it was bad enough that you had to pick how many hidden letters
    and how。neurons you want on each head layer Now you've got a learning rate of
    momentum。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它们试图解决的是学习率和动量。这两个都是超参数。这些是你需要与其他所有参数一起调整的数字。你以为仅仅需要选择隐藏层的神经元数量就已经够糟糕了，现在你还得考虑学习率和动量。
- en: and you need to figure out what the best learning rate is。 what the best momentum
    is so that you will be able to effectively train this neural network。 The problem
    is the learning rate， if you adjust it too small。 it's never going to accurately
    train your neural network it's just not taking enough risk if you make it too
    large your neural network will be very。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要弄清楚最佳学习率和最佳动量，以便能够有效地训练这个神经网络。问题是学习率，如果你调整得太小，它永远无法准确地训练你的神经网络，它只是没有承担足够的风险。如果你把它调得太大，你的神经网络将会非常不稳定。
- en: very erratic and momentum， same thing if you make it too large， things become
    erratic。 if you make it too small， it's not really having effect。 Also if you
    think about it。 this learning rate is being applied to every every weight in the
    entire neural network。 maybe a single learning rate is not enough， maybe some
    neurons are learning faster than others。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 非常不稳定和动量，如果你把它做得太大，事情会变得不稳定。如果你把它做得太小，它实际上没有什么效果。另外，如果你仔细想想，这个学习率是应用于整个神经网络中的每一个权重的。也许一个单一的学习率是不够的，也许一些神经元学习得比其他的快。
- en: So they like a concept of putting on multiple learning rates or you can also
    sometimes you will see that they will just automatically decrease the learning
    rate as training progresses So we're trying to move away from having every weight
    having a global learning rate and momentum and then also。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以他们喜欢把多个学习率的概念放在一起，或者有时你会看到他们会随着训练的进行自动减少学习率。所以我们试图摆脱每个权重都有一个全局学习率和动量的情况，然后也。
- en: Move to making those those values very sensitive， very nonsensitive are very
    accommodating to values that weren't chosen so well。 These are some other training
    techniques that we that I've worked with in the past。 There's resilient propagation。
    It works pretty well。 It basically recognizes that the sign of the gradient is
    probably the most important thing。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 转向使这些值非常敏感，非常不敏感，或者非常适应那些选择得不好的值。这些是我过去使用过的一些其他训练技术。有弹性传播。它效果不错。它基本上承认梯度的符号可能是最重要的事情。
- en: It tells you which direction the weight should move to better optimize。 It also
    does not need a learning rate and momentum。 So it was popular back in the day。
    It's not seen as much use with a deep learning N of accelerated a gradient。 What
    that does is with stochastic gradient descent。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 它告诉你权重应该朝着哪个方向移动以更好地优化。它也不需要学习率和动量。所以它在早些时候很流行。在深度学习加速的梯度中并不常见。它做的是使用随机梯度下降。
- en: It helps to mitigate the risk of just picking a really bad mini batch that then
    damages the rest of the training that you' that you've done There's addedgrad
    and Ana delta。 These are both。At a grad， basically it keeps a per weight decaying
    learning rate。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它有助于减少只选择一个非常糟糕的小批量数据的风险，然后损害你已经进行的其余训练。有添加梯度和阿纳德尔塔。这两者都是。在梯度中，基本上它保持一个按权重衰减的学习率。
- en: But it's monoomically decreasing， it never increases again。 so that's why added
    Delta was created to address atgrads issues where that that learning rate could
    just go in a direction and decrease to pretty much zero。There's also nongraient
    methods。 If you can't take a derivative of your last function。 these might be
    useful。 This includes simulated anneling， genetic algorithms， particle swarm。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但它是单调递减的，它永远不会再增加。所以这就是为什么创建了添加Delta来解决在梯度问题中的那个学习率可能朝一个方向去，并减少到几乎为零的问题。还有一些非梯度方法。如果你不能对你的最后函数进行导数操作，这些可能会有用。这包括模拟退火、遗传算法、粒子群。
- en: Nedermeed and many more。 So the atom update rule talked about before classic
    back propagation。 It's just another way of calculating V to put into this weight
    update algorithm that we have before。 Now the atom update rule。 what is very nice
    about it is it doesn't have you don't really have to focus too much on a learning
    rate。 there is a learning rate present but of the values that you that you have。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Nedermeed和许多其他。因此，原子更新规则之前谈论的经典反向传播。这只是我们之前将V计算到这个权重更新算法中的另一种方式。现在原子更新规则。它之所以很好的一个原因是你不需要过多地关注学习率。虽然存在一个学习率，但是你拥有的那些值。
- en: the authors of the original paper King Muba， they give some good recommendations
    for the hyperparameters。 and I rarely， even that learning rate at the end tend
    to the negative8。 they don't you often do not have to have to change those。 This
    is a relatively new training algorithm。 It was introduced in 2014。 It's a popular
    one。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文的作者King Muba，他们对超参数给出了一些好的建议。而且我很少，甚至是最后那个学习率也倾向于负8。他们通常不必改变那些。这是一个相对较新的训练算法。它是在2014年引入的。它是一个流行的算法。
- en: It deals with training for a sparse for sparse data。We lots of missing values
    and then also stochastic error function the error function is stochastic because
    we're doing stochastic gradient descent so we're constantly randomly subsampling
    a batch size of many batch and updating the neural network based on that So your
    the changes that you make from one iteration to the other might not help because
    you're grabbing a different set of set of training data on each one the paper
    for this for this is given here at Cornell University the archive if you haven't
    dealt with archive before that's the Greek letter chi so a archive archive and
    moment moment momentest method of moments that you see here it's a way of estimating
    each of the moments of of a distribution of values so that's the mean the variance
    and reliance and so on so let's look at。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它处理稀疏数据的训练。我们有很多缺失值，还有随机误差函数，误差函数是随机的，因为我们在进行随机梯度下降，因此我们不断随机抽样一个批量大小并基于此更新神经网络。因此，你从一次迭代到另一迭代的改变可能没有帮助，因为你在每次抽取的都是不同的训练数据。关于这个的论文在康奈尔大学的档案里，如果你之前没有接触过档案，那是希腊字母χ，所以是档案档案和矩的矩的矩估计方法，你在这里看到的，这是估计一组值的每个矩的方法，所以就是均值、方差等。我们来看看。
- en: The actual paper for this， this is very similar to the code that we've seen
    for the other ones。 so this is just t equals t plus one that's indicating that
    we're moving through the time we're initializing the first and second moment the
    first moment is the mean。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文实际上与我们看到的其他代码非常相似。所以这只是t等于t加1，表示我们在时间上移动，我们正在初始化第一和第二矩，第一矩是均值。
- en: so the mean of the gradients that you're trying to estimate。And V is the variance。
    so the second moment there is a third and additional moments。 but all we're dealing
    with are these two the first two moments。 by the way that's where the name add
    comes from adaptive moment estimation and then we initialize the time set to zero。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你试图估计的梯度的均值。V是方差。第二矩是第三和附加矩。但我们处理的仅是这两个，第一个和第二个矩。顺便提一下，这就是“加”的名字来源，自适应矩估计，然后我们将时间设置初始化为零。
- en: so these two initial estimates are zero。We are going to calculate the G， the
    gradient。 so this is the gradient， just like we're doing before， very similar
    to classic back propagation。 a little bit different terminology instead of the
    J that I use they use F so this is this is the loss function with the weights
    that's exactly the notation from the module for the previous one。And we are going
    to。Get that gradient。 These two values deal with a bias on the。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这两个初始估计值为零。我们将计算G，梯度。所以这个梯度，就像我们之前所做的，非常类似于经典反向传播。术语稍有不同，我用的J改成了F，因此这是损失函数，包含权重，正是前一个模块中的符号。我们将获取那个梯度。这两个值处理偏差。
- en: That occurs early on since these are initialized to 0， that creates a tremendous
    bias towards  zero。 So these are just two。嗯。I'm sorry the hats down here are actually
    that that's where they're calculating the they're dealing with the the initial。Fact
    that these are starting out at zero。 So M hat and V hat M and VT themselves。 this
    is essentially updating each of these。 so you'll notice that it's M based on the
    previous T。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些值初始化为0，因此早期会产生巨大的偏差，因此这只是两个。嗯，抱歉，这里的帽子实际上是在计算，它们处理的是最初的事实，即这些值从零开始。因此M帽和V帽M和VT本身，基本上是在更新每一个。你会注意到这是基于上一个T的M。
- en: They're updating as they go through it， they are essentially creating more and
    more of a estimate of the first and second moments。 and these are vectors。Obviously
    they're across the weights。 so that's essentially creating almost a learning rate
    for each。The first one is on the first power and then second power， so this is
    squared。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在执行过程中不断更新，实际上正在逐渐创建对第一和第二矩的估计。这些都是向量，显然它们与权重相关。因此，这基本上是在为每个权重创建一个几乎是学习率的东西。第一个是一次幂，然后是二次幂，所以这是平方的。
- en: We calculate these adjustment values just to deal with the fact that these started
    at a 0。 and then we're going to update the weights。 So weight t， this is very
    simple。 So this part is exactly like what we had from trying to highlight the
    bottom row。 But up to here is exactly what we had for the for classic。We're subtracting
    the gradient。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算这些调整值只是为了处理这些值最开始为0的事实，然后我们将更新权重。所以权重t，这个非常简单。这部分与我们尝试突出底行时完全相同。但到这里正是我们在经典算法中所用的。我们在减去梯度。
- en: but instead of subtracting the gradient， we're using this formula to update
    the parameters or weights。Alpha is the learning rate， so usually learning rates
    or step size， as they call it。 you're multiplying by the rest of this to scale
    this。And you're putting in the two hat values。 which were based on these just
    to adjust them so that they're not so biased initially towards zero。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但不是减去梯度，而是使用这个公式来更新参数或权重。Alpha 是学习率，所以通常称为学习率或步长。你在这个基础上进行乘法来进行缩放。然后你放入两个帽值，这些帽值是基于这些数据，以便调整它们，使它们最初不会过于偏向零。
- en: That is essentially Adam， it's a little bit more complicated than your classic
    back propagation。 but not a whole lot not I mean this is not terrible to implement
    and。In Java or another programming language， they discuss the algorithm。And they
    talk about that initial bias correction where why that is needed and how you are
    calculating the two hat values that are very necessary for that。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上这是 Adam，它比经典的反向传播稍微复杂一些，但并不是特别复杂。我是说，实施起来并不难。在 Java 或其他编程语言中，他们讨论了算法。他们谈到了初始偏差校正，为什么需要，以及如何计算这两个帽值，这是非常必要的。
- en: I will admit I have not read through this part and find it somewhat very complex。
    they're doing some analysis of the convergence， this is essentially proof for
    why it works。And then they quote related work and then experiments where they
    sort of empirically prove through experiment why it works and with E theorems
    they attempt to talk about why it sort of proofwise why it works。 related work，
    R Pro and Agrad， those are two other training very similar training techniques
    that existed prior to Adam and this is just straight sort of from the paper。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我承认我没有仔细阅读这部分，觉得它有些复杂。他们在分析收敛，这本质上是为什么它有效的证明。然后他们引用相关工作和实验，通过实验实证证明为什么它有效，并用
    E 定理试图谈论它为何有效的证明。相关工作，R Pro 和 Agrad，那是两种在 Adam 之前存在的非常相似的训练技术，这部分是直接来自论文。
- en: but I like looking the pseudocode a little bit better。 but I reproduced it there
    if you're interested in it。This is a really good diagram。 I did not create it，
    I have links to where this is where I found it。Yeah。So anyway。 and then he cites
    it to the original author can。Definitely want to give credit where credit is due。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但我更喜欢看伪代码。如果你对此感兴趣，我在这里复现了它。这是一个非常好的图表。我没有创建它，我有链接到我找到的地方。是的。所以，无论如何，然后他引用了原作者，确实要给予应有的信用。
- en: This is a very， very good diagram of this。 It's an animated g。 So these are。
    this is essentially a search space。 So you can think of this as a rugged sort
    of two dimensional plane。 Star is the lowest point。 That's where you want to get
    to。 And it's like。😊。It's sort of like marbles rolling down a ridge。This is stochastic
    gradient descent。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常非常好的图表。这是一个动态图。这本质上是一个搜索空间。你可以把它想象成一个崎岖的二维平面。星星是最低点。那是你想要到达的地方。这就像😊。就像弹珠沿着山脊滚落。这就是随机梯度下降。
- en: which is the slowest one。 So that's your classic back propagation。 It's eventually
    getting there。 The one that gets there really first is add Delta and。It seems
    like green。Momentum starts out slow。 but then really， as you can see， builds up
    the momentum and then blasts right past it。 So just your different different training
    methods。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最慢的一种。所以这是你经典的反向传播。最终它是可以到达的。最早到达的那个是加 Delta，它看起来像绿色的。动量开始时很慢，但如你所见，确实会积累动量，然后快速超越。因此，这就是你不同的训练方法。
- en: Adam is out on here because when this was created。 Adam did not exist or at
    least was not all that common yet。 But you can。 you can definitely see some of
    them like momentum。 It's very obvious that it's。It starts to really build that
    momentum， the little green ball and shoots right past it。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 在这里出现是因为在创建这个时，Adam 还不存在，或者至少不常见。但你可以看到其中一些，比如动量。显然它开始真的在积累动量，小绿球迅速超过。
- en: SGD is just slowly methodically working there and it doesn't even make it。 they
    give up and reset it because everybody else has made it。Kras and Tensorflowlow
    has a variety of these these techniques available for you。 Can you the ones that
    are highlighted they have， so they have stochastic gradient descent in Adam。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 只是慢慢有条不紊地在工作，甚至都没能完成。他们放弃并重置，因为其他人都完成了。Kras 和 Tensorflowlow 提供了多种这些技术。你可以看到那些被突出显示的，它们有随机梯度下降和
    Adam。
- en: I believe there's other ways you can get to some of these。 I'm sorry。 these
    are all of the ones that Tensorflowlow has。 I honestly have no idea what FTRL
    is or I'm familiar with the other ones。 the ones that I have highlighted are probably
    the ones you're the most interested in。 I would be tempted to highlight RMSs Pro
    as well。 you can you can definitely experiment with these and get potentially
    better results where you specify it is down here with the optimizer。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信还有其他方法可以达到这些目标。我很抱歉，这些都是 Tensorflowlow 提供的。我老实说不知道 FTRL 是什么，或者我不熟悉其他的选项。我所强调的这些可能是你最感兴趣的。我也会想强调
    RMSs Pro。你可以尝试这些，可能会得到更好的结果，你可以在这里和优化器一起指定。
- en: If you specify one of these that means a learning rate you'll pass in a class
    here and actually pass in the parameters as well you can go to the Tensorflow
    documentation and they show you ourki is anyway and they they give you the actual
    class names for this。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你指定了其中一个，这意味着你将在这里传递一个学习率的类，并且实际上也传递参数。你可以查看 Tensorflow 的文档，他们会向你展示如何使用，并给出实际的类名。
- en: but you can just use any of them just by putting its name in here you'll need
    to put an object in there if you want to specify even additional parameters these
    are some of the results that I got trying these out so you can see that on the
    miles per gallon data set we got basically different results and some like unit's
    momentum needed a lot more training than say at。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以通过将其名称放在这里直接使用任何一个，如果你想指定更多的参数，你需要放一个对象。这是我尝试这些时得到的一些结果，因此你可以看到在每加仑英里数据集上，我们得到了不同的结果，而像单位动量则需要比说的要多得多的训练。
- en: Showing the last successful or the last number of iterations。 So Adam converges
    pretty quick。 I I'm pretty fond of that training technique， and I use it often
    At agrad is also pretty good as well。 So at agrad， I like RMS Pro definitely，
    definitely I have seen it T's uses。 I will。😊。I will try between these and look
    at them when I am training neural networks and I often start with the atom。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 显示最后一次成功或最后几次迭代。因此，Adam 收敛得很快。我对这种训练技术很有好感，并且经常使用它。At agrad 也相当不错。因此在 agrad
    上，我非常喜欢 RMS Pro，确实，我已经看到它的用途。我会。😊我会在训练神经网络时在这些之间进行尝试，并且我通常从 Adam 开始。
- en: but then we'll try at agrad or our MSP。There are probably better。 more scientific
    ways to pick those， but that tends to work pretty well for me。 All right。 That
    is the end of this module。![](img/6491afaa3cf5a798f8c7f93f7b8bf376_3.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将尝试 agrad 或者我们的 MSP。可能有更好、更科学的方法来选择这些，但对我来说，这种方法通常效果不错。好的，这个模块到此结束。![](img/6491afaa3cf5a798f8c7f93f7b8bf376_3.png)
- en: Thank you for watching this video on how to train a neural network。 In the next
    video。 we're going to literally calculate a neural network from scratch。 We're
    going to see how to export the weights from Kes and use those to actually calculate
    what the output of the neural network would have been。 This removes all magic
    from the process。 This content changes often。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢观看这个关于如何训练神经网络的视频。在下一个视频中，我们将从头开始实际计算一个神经网络。我们将看到如何从 Keras 导出权重，并使用这些权重实际计算神经网络的输出。这消除了过程中的所有魔法。这个内容经常变化。
- en: So subscribe to the channel to stay up to date on this course and other topics
    and artificial intelligence。😊。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所以请订阅频道，以便及时了解本课程和其他人工智能主题。😊
