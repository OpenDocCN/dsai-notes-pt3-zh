- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é¢å‘åˆå­¦è€…çš„ TensorFlow æ•™ç¨‹ï¼Œç†è®ºçŸ¥è¯†ã€ä»£ç æ€è·¯å’Œåº”ç”¨æ¡ˆä¾‹ï¼ŒçœŸæ­£ä»é›¶å¼€å§‹è®²æ˜ç™½ï¼ï¼œå¿«é€Ÿå…¥é—¨ç³»åˆ—ï¼ - P4ï¼šL4- çº¿æ€§å›å½’  å®Œæ•´é¡¹ç›®æ¼”ç»ƒ
    - ShowMeAI - BV1TT4y1m7Xg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é¢å‘åˆå­¦è€…çš„ TensorFlow æ•™ç¨‹ï¼Œç†è®ºçŸ¥è¯†ã€ä»£ç æ€è·¯å’Œåº”ç”¨æ¡ˆä¾‹ï¼ŒçœŸæ­£ä»é›¶å¼€å§‹è®²æ˜ç™½ï¼ï¼œå¿«é€Ÿå…¥é—¨ç³»åˆ—ï¼ - P4ï¼šL4- çº¿æ€§å›å½’  å®Œæ•´é¡¹ç›®æ¼”ç»ƒ
    - ShowMeAI - BV1TT4y1m7Xg
- en: '![](img/938d327f6471f60f92b4ed5bae0092f3_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/938d327f6471f60f92b4ed5bae0092f3_0.png)'
- en: ğŸ¼ï¼ŒHeyï¼Œ guysï¼Œ welcome to the next Tensorlow tutorialã€‚ In the last videoã€‚ we learned
    how to create a neural network and then train and evaluate the model and make
    predictionsã€‚ğŸ˜Šï¼ŒNow in this tutorialï¼Œ we implement our first real world projectã€‚
    so we deal with a regression problemï¼Œ and we learn how we load the dataã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ï¼Œå¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°ä¸‹ä¸€ä¸ª TensorFlow æ•™ç¨‹ã€‚åœ¨ä¸Šä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åˆ›å»ºç¥ç»ç½‘ç»œï¼Œç„¶åè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚ğŸ˜Šï¼Œåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å®ç°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªçœŸå®ä¸–ç•Œé¡¹ç›®ï¼Œå› æ­¤æˆ‘ä»¬å¤„ç†å›å½’é—®é¢˜ï¼Œå­¦ä¹ å¦‚ä½•åŠ è½½æ•°æ®ã€‚
- en: analyze the data and apply some pre processingã€‚And yeahï¼Œ as I said last timeã€‚
    we already used a deep neural networkã€‚ and here we first take a step back and
    apply only a linear regression modelã€‚ So only one layerã€‚ But at the endï¼Œ we extend
    this to againï¼Œ a deep neural networkã€‚ And with thisã€‚ you should get a better understanding
    of the Keerra stance layerã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ææ•°æ®å¹¶è¿›è¡Œä¸€äº›é¢„å¤„ç†ã€‚æ˜¯çš„ï¼Œæ­£å¦‚æˆ‘ä¸Šæ¬¡æ‰€è¯´ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œã€‚è¿™é‡Œæˆ‘ä»¬å…ˆé€€ä¸€æ­¥ï¼Œä»…åº”ç”¨çº¿æ€§å›å½’æ¨¡å‹ã€‚æ‰€ä»¥åªæœ‰ä¸€å±‚ã€‚ä½†æœ€åï¼Œæˆ‘ä»¬å†æ¬¡å°†å…¶æ‰©å±•ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œã€‚é€šè¿‡è¿™ä¸ªï¼Œä½ åº”è¯¥èƒ½æ›´å¥½åœ°ç†è§£
    Keras å±‚ã€‚
- en: and also of activationctuaation functionsã€‚ So here I am in a twopyter notebookã€‚
    And you don't need to worry about thisï¼Œ this just makes life a little bit easier
    for me to explain the code to you and show you the different stepsã€‚ But you can
    code all of this in a normal python fileã€‚ So let's start and firstã€‚ we import
    the things we needã€‚ So againï¼Œ hereï¼Œ I silence some warningsã€‚ Then we import mappllip
    nuyã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ¿€æ´»å‡½æ•°ã€‚å› æ­¤æˆ‘åœ¨ä¸€ä¸ª Jupyter ç¬”è®°æœ¬ä¸­ã€‚ä½ ä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªï¼Œè¿™åªæ˜¯è®©æˆ‘åœ¨å‘ä½ è§£é‡Šä»£ç å’Œå±•ç¤ºä¸åŒæ­¥éª¤æ—¶ç”Ÿæ´»æ›´å®¹æ˜“ä¸€ç‚¹ã€‚ä½†æ˜¯ä½ å¯ä»¥åœ¨æ™®é€šçš„ Python
    æ–‡ä»¶ä¸­ç¼–å†™æ‰€æœ‰è¿™äº›ä»£ç ã€‚é‚£ä¹ˆè®©æˆ‘ä»¬å¼€å§‹ï¼Œé¦–å…ˆå¯¼å…¥æˆ‘ä»¬éœ€è¦çš„ä¸œè¥¿ã€‚è¿™é‡Œï¼Œæˆ‘å†æ¬¡å…³é—­äº†ä¸€äº›è­¦å‘Šã€‚ç„¶åæˆ‘ä»¬å¯¼å…¥ matplotlibã€‚
- en: and then this is newã€‚ So now we use pandasã€‚ and you can install this with Pip
    install pandasã€‚ğŸ˜Šã€‚So this makes it very easy to work with data sets and analyze
    them and modify themã€‚And then here I set some print options to make the outputs
    a little bit nicerã€‚And then we import the things from Tenofflowã€‚ So againï¼Œ we
    import Tensofflow STfã€‚ Then we import Kasã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæ˜¯æ–°çš„ã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ pandasã€‚ä½ å¯ä»¥é€šè¿‡ pip install pandas æ¥å®‰è£…å®ƒã€‚ğŸ˜Šã€‚è¿™ä½¿å¾—å¤„ç†æ•°æ®é›†ã€åˆ†æå®ƒä»¬å’Œä¿®æ”¹å®ƒä»¬å˜å¾—éå¸¸ç®€å•ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘è®¾ç½®äº†ä¸€äº›æ‰“å°é€‰é¡¹ï¼Œè®©è¾“å‡ºçœ‹èµ·æ¥æ›´å¥½ä¸€äº›ã€‚æ¥ç€æˆ‘ä»¬ä»
    TensorFlow å¯¼å…¥ç›¸å…³å†…å®¹ã€‚æ‰€ä»¥å†æ¬¡å¯¼å…¥ TensorFlow çš„ tfï¼Œç„¶åå¯¼å…¥ Kerasã€‚
- en: So the last time I told you about the Kaas API and we already use the ks layersã€‚
    And now this is newã€‚ So we also import from Tenorflowlow ks layers do experimentalã€‚
    we import preprocessing to apply a preprocessing layer to our dataã€‚ So these are
    the imports we needã€‚ And now for the data setã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šæ¬¡æˆ‘å‘Šè¯‰ä½ å…³äº Kaas API çš„äº‹æƒ…ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨äº† ks å±‚ã€‚ç°åœ¨è¿™æ˜¯æ–°çš„ã€‚å› æ­¤æˆ‘ä»¬è¿˜ä» TensorFlow çš„ ks.layers å¯¼å…¥å®éªŒæ¨¡å—ï¼Œå¯¼å…¥
    preprocessing æ¥å¯¹æ•°æ®åº”ç”¨é¢„å¤„ç†å±‚ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦çš„å¯¼å…¥ã€‚ç°åœ¨å¼€å§‹å¤„ç†æ•°æ®é›†ã€‚
- en: So we are going to use this autompg data setã€‚So this is a data set from the
    year 1983ã€‚ and here we have different features and with this we predict thempg
    for a carã€‚ So how many miles a car can travel using one gallon of fuel and so
    here this is a very popular website where you find a lot of machine learning data
    setsã€‚ And now if you click on the data folder then here you get the link to the
    actual dataã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ª autompg æ•°æ®é›†ã€‚è¿™ä¸ªæ•°æ®é›†æ¥è‡ª1983å¹´ï¼Œè¿™é‡Œæœ‰ä¸åŒçš„ç‰¹å¾ï¼Œä½¿ç”¨è¿™äº›ç‰¹å¾æ¥é¢„æµ‹æ±½è½¦çš„ mpgã€‚å› æ­¤ï¼Œæ±½è½¦ä½¿ç”¨ä¸€åŠ ä»‘ç‡ƒæ–™èƒ½è¡Œé©¶å¤šå°‘è‹±é‡Œï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ç½‘ç«™ï¼Œä½ å¯ä»¥åœ¨ä¸Šé¢æ‰¾åˆ°å¾ˆå¤šæœºå™¨å­¦ä¹ æ•°æ®é›†ã€‚å¦‚æœä½ ç‚¹å‡»æ•°æ®æ–‡ä»¶å¤¹ï¼Œä¸‹é¢ä½ ä¼šæ‰¾åˆ°å®é™…æ•°æ®çš„é“¾æ¥ã€‚
- en: So now here in the twopiter notebookï¼Œ this is the exact same URLã€‚ and I already
    wrote the different column names up here and then with pandas we can say pandas
    do read CSv So the ending here is not do csv but it's still in the CSv formã€‚So
    we can use this and you can also use this method if you have a Csv file on your
    diskã€‚ So this works as wellã€‚ So now let's load our data and then we can call data
    dot tail So this prints the last five columns So here we see thempg value that
    we want to predict and the different featuresã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åœ¨è¿™ä¸ª Jupyter ç¬”è®°æœ¬ä¸­ï¼Œè¿™æ˜¯å®Œå…¨ç›¸åŒçš„ URLã€‚æˆ‘å·²ç»åœ¨ä¸Šé¢å†™å¥½äº†ä¸åŒçš„åˆ—åï¼Œç„¶åä½¿ç”¨ pandasï¼Œæˆ‘ä»¬å¯ä»¥è¯´ pandas èƒ½è¯»å– CSVã€‚å› æ­¤è¿™é‡Œçš„ç»“å°¾ä¸æ˜¯
    do csvï¼Œä½†å®ƒä»ç„¶æ˜¯ CSV æ ¼å¼çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œå¦‚æœä½ åœ¨ç£ç›˜ä¸Šæœ‰ CSV æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ã€‚æ‰€ä»¥è¿™ä¹Ÿå¯ä»¥å·¥ä½œã€‚ç°åœ¨è®©æˆ‘ä»¬åŠ è½½æ•°æ®ï¼Œç„¶åå¯ä»¥è°ƒç”¨
    data.tailï¼Œè¿™å°†æ‰“å°æœ€åäº”åˆ—ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°æƒ³è¦é¢„æµ‹çš„ mpg å€¼å’Œä¸åŒçš„ç‰¹å¾ã€‚
- en: So for exampleï¼Œ we have the number of cylindersï¼Œ the displacementï¼Œ the horsepowerã€‚
    the weight acceleration model year and or writtenã€‚ So yeahã€‚ this is our data sets
    and now we clean our data setã€‚ So in here we have some missing numbers and we
    can very simply exclude them by saying data dot drop and a and then we also change
    this last column here becauseã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰æ°”ç¼¸æ•°é‡ã€æ’é‡ã€é©¬åŠ›ã€é‡é‡ã€åŠ é€Ÿåº¦ã€è½¦å‹å¹´ä»½ç­‰ã€‚å› æ­¤ï¼Œæ˜¯çš„ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œç°åœ¨æˆ‘ä»¬æ¸…ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€äº›ç¼ºå¤±å€¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆç®€å•åœ°é€šè¿‡è°ƒç”¨`data.drop`æ¥æ’é™¤å®ƒä»¬ï¼Œç„¶åæˆ‘ä»¬ä¹Ÿä¼šæ”¹å˜æœ€åä¸€åˆ—ï¼Œå› ä¸ºã€‚
- en: This is actually a categorical valueï¼Œ and all of the other ones are numerical
    valuesã€‚ So this might confuse our modelã€‚ So we want to change this in a one hot
    encoded dataã€‚ so we can call data set dot pop or written to remove thisã€‚And here
    in the originã€‚ we have the different countriesï¼Œ USAï¼Œ Europe and Japanã€‚ So we can
    add them like thisã€‚So nowï¼Œ againã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªåˆ†ç±»å€¼ï¼Œå…¶ä»–çš„éƒ½æ˜¯æ•°å€¼å‹å€¼ã€‚æ‰€ä»¥è¿™å¯èƒ½ä¼šè®©æˆ‘ä»¬çš„æ¨¡å‹æ„Ÿåˆ°å›°æƒ‘ã€‚å› æ­¤æˆ‘ä»¬æƒ³å°†å…¶è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥è°ƒç”¨æ•°æ®é›†çš„`pop`æ–¹æ³•æˆ–ç›´æ¥åˆ é™¤å®ƒã€‚åœ¨åŸå§‹æ•°æ®ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸åŒçš„å›½å®¶ï¼Œå¦‚ç¾å›½ã€æ¬§æ´²å’Œæ—¥æœ¬ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·æ·»åŠ å®ƒä»¬ã€‚ç°åœ¨ï¼Œå†æ¬¡è¿›è¡Œæ“ä½œã€‚
- en: if we have a look at our tailï¼Œ then we see we removed the origin value and instead
    included one hot labelsã€‚ So here for label 1ã€‚ our it's the USA for label 2ï¼Œ it's
    Europeã€‚ Then againï¼Œ label 1ã€‚ So againã€‚ USA has the one and so onã€‚ So now we have
    thisã€‚And now we want to split our data into training and test setsã€‚ and we can
    do this by calling dot sample and then use a fraction hereã€‚ So use 80% for trainingã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æŸ¥çœ‹å°¾éƒ¨æ•°æ®ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¼šçœ‹åˆ°æˆ‘ä»¬ç§»é™¤äº†åŸå§‹å€¼ï¼Œè€Œæ˜¯åŒ…å«äº†ç‹¬çƒ­ç¼–ç æ ‡ç­¾ã€‚å› æ­¤ï¼Œå¯¹äºæ ‡ç­¾1ï¼Œä»£è¡¨ç¾å›½ï¼›å¯¹äºæ ‡ç­¾2ï¼Œä»£è¡¨æ¬§æ´²ã€‚ç„¶åï¼Œæ ‡ç­¾1åˆæ˜¯è¿™æ ·ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªã€‚ç°åœ¨æˆ‘ä»¬æƒ³å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨`sample`æ–¹æ³•ï¼Œå¹¶åœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªæ¯”ä¾‹æ¥å®ç°ã€‚å› æ­¤ï¼Œä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†ã€‚
- en: And we againï¼Œ drop the samplesã€‚ So all of the samplesã€‚ all of that we specify
    here are not included in the test setã€‚And then yeahã€‚ let's print print the shape
    and let's also describe our dataset set so we can very easily do this with pandas
    as well by calling the dot describe functionã€‚ And then here we see we have our
    whole data set has 392 samples and then 10 different columns for nowã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å†æ¬¡åˆ é™¤æ ·æœ¬ã€‚å› æ­¤ï¼Œæ‰€æœ‰æˆ‘ä»¬åœ¨è¿™é‡ŒæŒ‡å®šçš„æ ·æœ¬éƒ½ä¸åŒ…æ‹¬åœ¨æµ‹è¯•é›†ä¸­ã€‚ç„¶åï¼Œæ˜¯çš„ï¼Œè®©æˆ‘ä»¬æ‰“å°å½¢çŠ¶ï¼Œå¹¶æè¿°æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨`describe`å‡½æ•°å¾ˆå®¹æ˜“åœ°åšåˆ°è¿™ä¸€ç‚¹ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†æœ‰392ä¸ªæ ·æœ¬ï¼Œç›®å‰æœ‰10ä¸ªä¸åŒçš„åˆ—ã€‚
- en: And then our test dataï¼Œ our training data is 80% so 314 and the rest is for
    testingã€‚And now if we call the describe functionï¼Œ then it analyze some statistics
    like the number of samplesã€‚ the meanï¼Œ the standard deviationï¼Œ the min and maxã€‚So
    this might be useful to analyze itã€‚ And first of allï¼Œ now what we want to do is
    we want to split the features from the labelsã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çš„æµ‹è¯•æ•°æ®ï¼Œè®­ç»ƒæ•°æ®å 80%ï¼Œå³314ï¼Œå‰©ä¸‹çš„æ˜¯ç”¨äºæµ‹è¯•çš„ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬è°ƒç”¨æè¿°å‡½æ•°ï¼Œå®ƒä¼šåˆ†æä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æ ·æœ¬æ•°é‡ã€å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼å’Œæœ€å¤§å€¼ã€‚è¿™å¯èƒ½æœ‰åŠ©äºåˆ†æã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è¦åšçš„æ˜¯å°†ç‰¹å¾å’Œæ ‡ç­¾åˆ†å¼€ã€‚
- en: So we want to this is our label that we predict MPã€‚ So first we make a copyã€‚
    So we say these are our training features and our test featuresã€‚ And then we simply
    pop the MP G column from the training and testing dataã€‚ And when we pop thisã€‚
    then this also returns the dataã€‚ So this is now our training and testing labelsã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦çš„æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾ï¼Œä»¥é¢„æµ‹MPã€‚é¦–å…ˆæˆ‘ä»¬åšä¸€ä¸ªå‰¯æœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯´è¿™äº›æ˜¯æˆ‘ä»¬çš„è®­ç»ƒç‰¹å¾å’Œæµ‹è¯•ç‰¹å¾ã€‚ç„¶åæˆ‘ä»¬ç®€å•åœ°ä»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸­åˆ é™¤MPGåˆ—ã€‚å½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œå®ƒä¹Ÿä¼šè¿”å›æ•°æ®ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬ç°åœ¨çš„è®­ç»ƒå’Œæµ‹è¯•æ ‡ç­¾ã€‚
- en: So now we have thisã€‚Then here's a simple function to plot the data with mappl
    lipã€‚ and now let's simply plot one featureã€‚ So let's plot the horsepower featureã€‚
    And we plot this against thempg valueã€‚ So we see the more horsepower our car has
    also the less is the value of MPgã€‚ and this makes sense because the more power
    our gas has the more fuel it needsã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªã€‚ç„¶åè¿™æ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œç”¨äºç”¨`matplotlib`ç»˜åˆ¶æ•°æ®ã€‚ç°åœ¨è®©æˆ‘ä»¬ç®€å•åœ°ç»˜åˆ¶ä¸€ä¸ªç‰¹å¾ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶é©¬åŠ›ç‰¹å¾ã€‚æˆ‘ä»¬å°†å…¶ä¸MPgå€¼è¿›è¡Œå¯¹æ¯”ã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°ï¼Œæ±½è½¦çš„é©¬åŠ›è¶Šå¤§ï¼ŒMPgå€¼è¶Šä½ã€‚è¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ±½è½¦åŠŸç‡è¶Šå¤§ï¼Œæ‰€éœ€çš„ç‡ƒæ–™ä¹Ÿå°±è¶Šå¤šã€‚
- en: So the less is the MPgã€‚ And so then we also let's plot the weight featureã€‚ and
    this should have a similar distributionã€‚ So againï¼Œ the higher the weight of a
    carã€‚ the more fuel uses itï¼Œ and the less is the number of the MPg valueã€‚Soï¼Œ yeahã€‚So
    this is how our data looks likeã€‚ And now if we go back to the describe function
    and have a look at the different mean valuesã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥MPgå€¼è¶Šä½ã€‚è€Œä¸”æˆ‘ä»¬ä¹Ÿå¯ä»¥ç»˜åˆ¶é‡é‡ç‰¹å¾ï¼Œè¿™åº”è¯¥æœ‰ç±»ä¼¼çš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œæ±½è½¦çš„é‡é‡è¶Šå¤§ï¼Œæ¶ˆè€—çš„ç‡ƒæ–™å°±è¶Šå¤šï¼Œè€ŒMPgå€¼åˆ™è¶Šä½ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„æ•°æ®å‘ˆç°çš„æ ·å­ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬å›åˆ°æè¿°å‡½æ•°ï¼ŒæŸ¥çœ‹ä¸åŒçš„å‡å€¼ã€‚
- en: then we see these all have different ranges and this is againã€‚ a very important
    issue that we have to considerã€‚ So this these different ranges if if we leave
    it like thisã€‚ then it might confuse our modelã€‚ So it's recommend to normalize
    the data first and to normalizeã€‚ So let's print againï¼Œ let's describe our training
    data set and only the but let's print only the mean and the standard deviationã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çœ‹åˆ°è¿™äº›éƒ½æœ‰ä¸åŒçš„èŒƒå›´ï¼Œè¿™å†æ¬¡æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»è€ƒè™‘ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¿™æ ·ä¿æŒè¿™äº›ä¸åŒçš„èŒƒå›´ï¼Œå¯èƒ½ä¼šä½¿æˆ‘ä»¬çš„æ¨¡å‹æ„Ÿåˆ°å›°æƒ‘ã€‚å› æ­¤ï¼Œå»ºè®®å…ˆå¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚è®©æˆ‘ä»¬å†æ¬¡æ‰“å°ï¼Œæè¿°æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†ï¼Œä½†åªæ‰“å°å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: So if we run this again then here we see the different meansã€‚And now to normalize
    itã€‚ we used this normalization layer from the pre processingces moduleã€‚ So this
    one here that we importedã€‚So let's code thisã€‚ So here we create a normalization
    layerã€‚ So we say normalizer equals pre processing dotã€‚Normalizationã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å†æ¬¡è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆè¿™é‡Œæˆ‘ä»¬ä¼šçœ‹åˆ°ä¸åŒçš„å‡å€¼ã€‚ç°åœ¨ä¸ºäº†æ ‡å‡†åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢„å¤„ç†æ¨¡å—ä¸­çš„è¿™ä¸ªæ ‡å‡†åŒ–å±‚ã€‚å› æ­¤è¿™é‡Œæ˜¯æˆ‘ä»¬å¯¼å…¥çš„è¿™ä¸ªã€‚è®©æˆ‘ä»¬ç¼–ç è¿™ä¸ªã€‚å› æ­¤æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ ‡å‡†åŒ–å±‚ã€‚å› æ­¤æˆ‘ä»¬è¯´normalizerç­‰äºpre.processing.dot.Normalizationã€‚
- en: so this is a kas layer for the sequential APIã€‚ And to call thisã€‚ we have to
    adapt it to our to the dataã€‚ So here we sayï¼Œ normalizer dot adaptã€‚ And then we
    want to adapt it to the training featuresã€‚ And right nowã€‚ this is a panda data
    set so we can convert this to a nuy arrayã€‚ and then call the train feeã€‚Cersã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºé¡ºåºAPIçš„kaså±‚ã€‚è¦è°ƒç”¨è¿™ä¸ªï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶é€‚åº”æˆ‘ä»¬çš„æ•°æ®ã€‚å› æ­¤è¿™é‡Œæˆ‘ä»¬è¯´ï¼Œnormalizer.dot.adaptã€‚ç„¶åæˆ‘ä»¬æƒ³è¦å°†å…¶é€‚åº”è®­ç»ƒç‰¹å¾ã€‚ç›®å‰ï¼Œè¿™æ˜¯ä¸€ä¸ªpandasæ•°æ®é›†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†å…¶è½¬æ¢ä¸ºnuyæ•°ç»„ï¼Œç„¶åè°ƒç”¨è®­ç»ƒç‰¹å¾ã€‚
- en: So what this is doing for nowï¼Œ this simply store calculates the mean and the
    variance of this training features and then stores them in the layersã€‚ So nowï¼Œ
    for exampleï¼Œ we can say print and then normalr dot mean dot nuyã€‚ and then if we
    run thisã€‚Then we see here thisã€‚ These are our mean valuesï¼Œ and these are the exact
    same numbers as we see here if we go this first column downã€‚So this simply stores
    itã€‚ And now whenever we apply this layer to our dataã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨è¿™ä¸ªåŠŸèƒ½çš„ä½œç”¨æ˜¯ï¼Œç®€å•åœ°è®¡ç®—è®­ç»ƒç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå°†å…¶å­˜å‚¨åœ¨å±‚ä¸­ã€‚å› æ­¤ï¼Œç°åœ¨ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°ï¼Œç„¶åä½¿ç”¨normalr.dot.mean.dot.nuyã€‚å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå°±å¯ä»¥çœ‹åˆ°è¿™é‡Œã€‚è¿™äº›æ˜¯æˆ‘ä»¬çš„å‡å€¼ï¼Œè¿™äº›æ•°å­—ä¸æˆ‘ä»¬åœ¨ç¬¬ä¸€åˆ—çœ‹åˆ°çš„å®Œå…¨ç›¸åŒã€‚å› æ­¤è¿™åªæ˜¯å°†å…¶å­˜å‚¨ä¸‹æ¥ã€‚ç°åœ¨æ¯å½“æˆ‘ä»¬å°†è¿™ä¸ªå±‚åº”ç”¨äºæˆ‘ä»¬çš„æ•°æ®æ—¶ã€‚
- en: then it normalizes the features such that it computes this this formulaã€‚ So
    it subtracts the mean and then divides by the standard deviationã€‚ and this means
    that our output has a zero mean and unit varianceã€‚ So here let's get some exampleã€‚
    So the first data from the training featuresã€‚Thenï¼Œ let's print this oneã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¼šå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä»¥è®¡ç®—è¿™ä¸ªå…¬å¼ã€‚å› æ­¤ï¼Œå®ƒå‡å»å‡å€¼ï¼Œç„¶åé™¤ä»¥æ ‡å‡†å·®ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„è¾“å‡ºå…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®ã€‚è¿™é‡Œæˆ‘ä»¬æ¥ä¸€äº›ç¤ºä¾‹ã€‚å› æ­¤ï¼Œç¬¬ä¸€ä¸ªæ•°æ®æ¥è‡ªè®­ç»ƒç‰¹å¾ã€‚ç„¶åï¼Œè®©æˆ‘ä»¬æ‰“å°è¿™ä¸ªã€‚
- en: So this is the first data unizedã€‚ And then we also want to print the normalized
    oneã€‚ Soã€‚ let's print norã€‚Maizedã€‚ And then here we have to call this layer normalr
    with our dataã€‚ So firstã€‚ and then convert it to numpyã€‚ So this is how we convert
    from a tensor flow tensor to a nuy arrayã€‚ So let's run thisã€‚ And then we see we
    have theã€‚The first layer with different scales and rangesã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ç¬¬ä¸€ä¸ªæ•°æ®æ ‡å‡†åŒ–ã€‚ç„¶åæˆ‘ä»¬è¿˜æƒ³æ‰“å°æ ‡å‡†åŒ–åçš„æ•°æ®ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬æ‰“å°nor.Maizedã€‚ç„¶åè¿™é‡Œæˆ‘ä»¬éœ€è¦ç”¨æˆ‘ä»¬çš„æ•°æ®è°ƒç”¨è¿™ä¸ªnormalrå±‚ã€‚æ‰€ä»¥é¦–å…ˆã€‚ç„¶åå°†å…¶è½¬æ¢ä¸ºnumpyã€‚è¿™æ˜¯æˆ‘ä»¬å¦‚ä½•å°†TensorFlowå¼ é‡è½¬æ¢ä¸ºnuyæ•°ç»„ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬æœ‰ç¬¬ä¸€å±‚å…·æœ‰ä¸åŒçš„å°ºåº¦å’ŒèŒƒå›´ã€‚
- en: and now our normalized dataã€‚ So they are all somewhere around0 with a unit standard
    deviationã€‚ So this is how we apply a normalizer preprocessing layerã€‚ And now let's
    tackle our regression problemã€‚ So in regressionã€‚ So we somewhere we have this
    distribution that we know from the training dataã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ ‡å‡†åŒ–æ•°æ®ã€‚å› æ­¤å®ƒä»¬éƒ½åœ¨0é™„è¿‘ï¼Œå…·æœ‰å•ä½æ ‡å‡†å·®ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åº”ç”¨æ ‡å‡†åŒ–é¢„å¤„ç†å±‚ã€‚ç°åœ¨è®©æˆ‘ä»¬å¤„ç†æˆ‘ä»¬çš„å›å½’é—®é¢˜ã€‚åœ¨å›å½’ä¸­ã€‚æˆ‘ä»¬åœ¨æŸå¤„æœ‰è¿™ä¸ªä»è®­ç»ƒæ•°æ®å¾—çŸ¥çš„åˆ†å¸ƒã€‚
- en: and now when we get a new dataï¼Œ we want to so for exampleã€‚ here we get a new
    weight sample and then we want to predict how muchmpT we have for this carã€‚ and
    for thisï¼Œ we fit a function So a linear function with this formula so we can approximate
    it withmp times x plus Bã€‚ So this is aã€‚Basically a line equation and we use this
    with the layers with the dense layerã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å½“æˆ‘ä»¬è·å¾—æ–°æ•°æ®æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦ï¼Œä¾‹å¦‚ã€‚è¿™é‡Œæˆ‘ä»¬è·å¾—ä¸€ä¸ªæ–°çš„æƒé‡æ ·æœ¬ï¼Œç„¶åæˆ‘ä»¬æƒ³é¢„æµ‹è¿™è¾†è½¦çš„å¤šå°‘mpTã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ‹Ÿåˆä¸€ä¸ªå‡½æ•°ï¼Œå³ç”¨è¿™ä¸ªå…¬å¼æ‹Ÿåˆä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥ç”¨mpä¹˜ä»¥xåŠ ä¸ŠBæ¥è¿‘ä¼¼ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªä¸å¯†é›†å±‚ä¸€èµ·ã€‚
- en: Sos better understand thisã€‚ Nowï¼Œ firstï¼Œ let's not use all of the different training
    features that we haveã€‚ So all of these so let's just use one so that we can stay
    in the 2D case so in this caseã€‚ for exampleï¼Œ its let's use the horsepower So here
    let's define our feature and this should be the horse power label so I will implement
    it like this so then you can just change the feature here and then you can try
    out different features so we can also for exampleã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ã€‚ç°åœ¨ï¼Œé¦–å…ˆï¼Œè®©æˆ‘ä»¬ä¸è¦ä½¿ç”¨æ‰€æœ‰ä¸åŒçš„è®­ç»ƒç‰¹å¾ã€‚æ‰€ä»¥è¿™äº›ï¼Œè®©æˆ‘ä»¬åªä½¿ç”¨ä¸€ä¸ªï¼Œä»¥ä¾¿ä¿æŒåœ¨äºŒç»´æƒ…å†µä¸‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨é©¬åŠ›ã€‚æ‰€ä»¥åœ¨è¿™é‡Œè®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ç‰¹å¾ï¼Œè¿™åº”è¯¥æ˜¯é©¬åŠ›æ ‡ç­¾ã€‚æˆ‘å°†è¿™æ ·å®ç°ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥åœ¨è¿™é‡Œæ›´æ”¹ç‰¹å¾ï¼Œç„¶åå°è¯•ä¸åŒçš„ç‰¹å¾ã€‚
- en: use weight here so all of these names you can use here as a feature and now
    let's get the single feature or the single data so let'sã€‚Call this single feature
    equals Ny arrayã€‚ And then from our training featuresã€‚ and then we can access it
    with the featureã€‚ This will only return the horsepower featureã€‚So let's print
    thisã€‚ Let's print single feeã€‚Feature dot shapeã€‚ And let's print trainã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œä½¿ç”¨æƒé‡ï¼Œæ‰€ä»¥æ‰€æœ‰è¿™äº›åç§°éƒ½å¯ä»¥ä½œä¸ºç‰¹å¾ä½¿ç”¨ã€‚ç°åœ¨è®©æˆ‘ä»¬è·å–å•ç‰¹å¾æˆ–å•æ•°æ®ï¼Œç§°ä¹‹ä¸ºå•ç‰¹å¾ç­‰äºNyæ•°ç»„ã€‚ä»æˆ‘ä»¬çš„è®­ç»ƒç‰¹å¾ä¸­ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‰¹å¾è®¿é—®å®ƒã€‚è¿™å°†åªè¿”å›é©¬åŠ›ç‰¹å¾ã€‚æ‰€ä»¥è®©æˆ‘ä»¬æ‰“å°è¿™ä¸ªã€‚æ‰“å°å•ç‰¹å¾
    dot shapeã€‚æ‰“å°è®­ç»ƒç‰¹å¾ dot shapeã€‚
- en: features dot shapeã€‚So nowï¼Œ if we run thisã€‚å—¯ã€‚Here I have a typo arrayã€‚Let's run
    it againã€‚ Then we see our single feature only has one featureã€‚ And in the whole
    training dataã€‚ we have 9 different featuresã€‚ So then up here weã€‚Created this preproces
    or normalization layer and then adapted it to all of the featuresã€‚ So basically
    we have to do the same thingï¼Œ but now adapt it only to the horse power featureã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚å—¯ã€‚è¿™é‡Œæˆ‘æœ‰ä¸€ä¸ªæ‹¼å†™é”™è¯¯çš„æ•°ç»„ã€‚è®©æˆ‘ä»¬å†è¿è¡Œä¸€æ¬¡ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„å•ç‰¹å¾åªæœ‰ä¸€ä¸ªç‰¹å¾ã€‚è€Œåœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®ä¸­ï¼Œæˆ‘ä»¬æœ‰9ä¸ªä¸åŒçš„ç‰¹å¾ã€‚ç„¶ååœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºäº†è¿™ä¸ªé¢„å¤„ç†æˆ–å½’ä¸€åŒ–å±‚ï¼Œå¹¶å°†å…¶é€‚åº”äºæ‰€æœ‰ç‰¹å¾ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šæˆ‘ä»¬å¿…é¡»åšåŒæ ·çš„äº‹æƒ…ï¼Œä½†ç°åœ¨ä»…é€‚åº”é©¬åŠ›ç‰¹å¾ã€‚
- en: So let's call this single feature normalizer then againï¼Œ our single feature
    normalizerã€‚ we call the adapt data and adapt it to the single feature single underscore
    featureã€‚ So now we have this and can run this code and here I have one parentheses
    too muchã€‚ So againã€‚ So now we have this single featureã€‚ and now we create our
    sequential model as the last timeã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å†æ¬¡ç§°ä¹‹ä¸ºå•ç‰¹å¾å½’ä¸€åŒ–å™¨ï¼Œæˆ‘ä»¬è°ƒç”¨é€‚åº”æ•°æ®å¹¶é€‚åº”å•ç‰¹å¾å•ä¸‹åˆ’çº¿ç‰¹å¾ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰è¿™ä¸ªï¼Œå¯ä»¥è¿è¡Œè¿™æ®µä»£ç ï¼Œè¿™é‡Œæˆ‘æœ‰ä¸€ä¸ªå¤šä½™çš„æ‹¬å·ã€‚æ‰€ä»¥å†æ¬¡ä¿®æ­£ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰è¿™ä¸ªå•ç‰¹å¾ã€‚ç°åœ¨æˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„é¡ºåºæ¨¡å‹ï¼Œå°±åƒä¸Šæ¬¡ä¸€æ ·ã€‚
- en: So this is very easy with the Kaas APIã€‚å—¯ã€‚Soï¼Œ we sayï¼Œ our singleã€‚Featureã€‚Model
    equalsã€‚ And then we sayï¼Œ Kas dotã€‚Models dot sequentialã€‚ And then here we use a
    list with all the different layersã€‚ So we use this as a first layerã€‚ So the single
    feature normalizerã€‚ And then we only use one layerã€‚ So one dense layerã€‚ and the
    output or also called units number of units is only oneã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½¿ç”¨Kaas APIéå¸¸ç®€å•ã€‚å—¯ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬è¯´ï¼Œæˆ‘ä»¬çš„å•ç‰¹å¾æ¨¡å‹ç­‰äºã€‚ç„¶åæˆ‘ä»¬è¯´ï¼ŒKas dot Models dot sequentialã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸åŒå±‚çš„åˆ—è¡¨ã€‚æ‰€ä»¥æˆ‘ä»¬å°†è¿™ä¸ªä½œä¸ºç¬¬ä¸€å±‚ã€‚å•ç‰¹å¾å½’ä¸€åŒ–å™¨ã€‚ç„¶åæˆ‘ä»¬åªä½¿ç”¨ä¸€å±‚ã€‚æ‰€ä»¥ä¸€ä¸ªå¯†é›†å±‚ï¼Œè¾“å‡ºæˆ–ç§°ä¸ºå•ä½æ•°çš„å•ä½åªæœ‰ä¸€ä¸ªã€‚
- en: So this is all that we need to build a linear regression modelã€‚ So this is a
    linear regression or linearã€‚Model that applies exactly this formulaã€‚ So it has
    some weights and multiplies it with our inputã€‚ And it also has a biasã€‚So this
    is all that we are doing hereã€‚ So let's run thisã€‚ and let's print the model summaryã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬æ„å»ºçº¿æ€§å›å½’æ¨¡å‹æ‰€éœ€çš„å…¨éƒ¨ã€‚è¿™æ˜¯ä¸€ä¸ªçº¿æ€§å›å½’æˆ–çº¿æ€§æ¨¡å‹ï¼Œå®Œå…¨åº”ç”¨è¿™ä¸ªå…¬å¼ã€‚å®ƒæœ‰ä¸€äº›æƒé‡ï¼Œå¹¶å°†å…¶ä¸æˆ‘ä»¬çš„è¾“å…¥ç›¸ä¹˜ã€‚å®ƒè¿˜æœ‰ä¸€ä¸ªåç½®ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„ä¸€åˆ‡ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå¹¶æ‰“å°æ¨¡å‹æ‘˜è¦ã€‚
- en: And then we see we have our normalization layer and our dense layer and only
    five parameterssã€‚5 parametersã€‚ So it's very simpleã€‚ And now as a next stepã€‚ the
    same as last time we define a loss and a optrã€‚ So for the lossï¼Œ we use ks dot
    lossesã€‚ And in the case of a linear regressionï¼Œ we can use the mean absolutearrowã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬çœ‹åˆ°æœ‰å½’ä¸€åŒ–å±‚å’Œå¯†é›†å±‚ï¼Œåªæœ‰äº”ä¸ªå‚æ•°ã€‚5ä¸ªå‚æ•°ã€‚æ‰€ä»¥è¿™éå¸¸ç®€å•ã€‚æ¥ä¸‹æ¥ï¼Œå’Œä¸Šæ¬¡ä¸€æ ·ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæŸå¤±å’Œä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æ‰€ä»¥å¯¹äºæŸå¤±ï¼Œæˆ‘ä»¬ä½¿ç”¨ks dot
    lossesã€‚åœ¨çº¿æ€§å›å½’çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ã€‚
- en: So this is one possibilityï¼Œ we could also use the mean squaredarrowã€‚ So you
    can try out bothã€‚ğŸ˜Šã€‚And so this is simply this one is doing the so the prediction
    y prediction minus the actual yã€‚ and then the absolute value and then sum it up
    over all samples and calculate the mean value and the mean square error is the
    sameã€‚ except that it's using the square here and not the absolute valueã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ç§å¯èƒ½æ€§ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å‡æ–¹è¯¯å·®ã€‚æ‰€ä»¥ä½ å¯ä»¥å°è¯•ä¸¤è€…ã€‚ğŸ˜Šã€‚æ‰€ä»¥è¿™å®é™…ä¸Šæ˜¯é¢„æµ‹å€¼yå‡å»å®é™…å€¼yï¼Œç„¶åå–ç»å¯¹å€¼ï¼Œæœ€åå¯¹æ‰€æœ‰æ ·æœ¬æ±‚å’Œï¼Œè®¡ç®—å‡å€¼ï¼Œå‡æ–¹è¯¯å·®æ˜¯ç›¸åŒçš„ï¼Œåªä¸è¿‡è¿™é‡Œä½¿ç”¨çš„æ˜¯å¹³æ–¹è€Œä¸æ˜¯ç»å¯¹å€¼ã€‚
- en: So yeah try that out for yourselfã€‚And now the optimizer equalsã€‚ So the same
    as last time Kaas dot optimizers dotã€‚ Let's use the Adamom optimizerã€‚ and we have
    to give it a learning rateã€‚ Let's try out point1ã€‚And then after we have thisã€‚
    then we compile our modelã€‚ So single feature model dot compileï¼Œ compileã€‚And here
    we give itã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¯•è¯•çœ‹å§ã€‚ç°åœ¨ä¼˜åŒ–å™¨ç­‰äºã€‚æ‰€ä»¥å’Œä¸Šæ¬¡ä¸€æ ·Kaas.dot.optimizers.dotã€‚è®©æˆ‘ä»¬ä½¿ç”¨Adamä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬éœ€è¦ç»™å®ƒä¸€ä¸ªå­¦ä¹ ç‡ã€‚è®©æˆ‘ä»¬è¯•è¯•0.1ã€‚ç„¶ååœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªä¹‹åï¼Œæˆ‘ä»¬ç¼–è¯‘æˆ‘ä»¬çš„æ¨¡å‹ã€‚æ‰€ä»¥å•ç‰¹å¾æ¨¡å‹.dot.compileï¼Œcompileã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ç»™å®ƒã€‚
- en: The optr equals the optimizer andã€‚Optim and the loss equals the lossã€‚ And yeahã€‚
    last time we also gave it the metrics that we want to track So the accuracyã€‚ but
    the accuracy doesn't make sense here for the regressionã€‚ So we leave it awayã€‚
    And then we simply see the loss laterã€‚ So let's run thisã€‚And now to train the
    modelã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨ç­‰äºä¼˜åŒ–å™¨å’Œã€‚æŸå¤±ç­‰äºæŸå¤±ã€‚æ˜¯çš„ã€‚ä¸Šæ¬¡æˆ‘ä»¬ä¹Ÿç»™äº†å®ƒè¦è·Ÿè¸ªçš„æŒ‡æ ‡ï¼Œæ‰€ä»¥å‡†ç¡®ç‡ã€‚ä½†åœ¨å›å½’ä¸­å‡†ç¡®ç‡æ²¡æœ‰æ„ä¹‰ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸è€ƒè™‘ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ç¨åçœ‹åˆ°æŸå¤±ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚ç°åœ¨æ¥è®­ç»ƒæ¨¡å‹ã€‚
- en: we simply have to call model dot fit like the last time with our training features
    and here we only want the single feature that we define so the horse power in
    this caseã€‚ but we still include all the training labels then we define the epochs
    then here I set verbose to one to see some logging and we can also define a validation
    split so this automatically takes 20% of the training data and then uses it for
    the validation data to tweak the hyperparameterã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªéœ€åƒä¸Šæ¬¡é‚£æ ·è°ƒç”¨model.dot.fitï¼Œä½¿ç”¨æˆ‘ä»¬çš„è®­ç»ƒç‰¹å¾ï¼Œè¿™é‡Œæˆ‘ä»¬åªå¸Œæœ›å®šä¹‰çš„å•ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯é©¬åŠ›ã€‚ä½†æˆ‘ä»¬ä»ç„¶åŒ…æ‹¬æ‰€æœ‰è®­ç»ƒæ ‡ç­¾ï¼Œç„¶åå®šä¹‰å‘¨æœŸï¼Œåœ¨è¿™é‡Œæˆ‘å°†verboseè®¾ç½®ä¸º1ä»¥æŸ¥çœ‹ä¸€äº›æ—¥å¿—ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å®šä¹‰éªŒè¯åˆ†å‰²ï¼Œè¿™æ ·è‡ªåŠ¨å°†20%çš„è®­ç»ƒæ•°æ®ç”¨äºéªŒè¯æ•°æ®ï¼Œä»¥è°ƒæ•´è¶…å‚æ•°ã€‚
- en: So let's train our modelã€‚And nowï¼Œ training is doneã€‚ And now we see that the
    loss decreasedã€‚ and we also see the validation loss decreasingã€‚And at the endï¼Œ
    we have a training loss of 3ã€‚8 and a validation loss of slightly more 4ã€‚1ã€‚ So
    it's not very badã€‚ and by the wayã€‚ if we call model fitï¼Œ then this returns the
    historyï¼Œ where it stores both of these lossesã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ç°åœ¨ï¼Œè®­ç»ƒå®Œæˆäº†ã€‚ç°åœ¨æˆ‘ä»¬çœ‹åˆ°æŸå¤±å‡å°‘äº†ï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°éªŒè¯æŸå¤±åœ¨å‡å°‘ã€‚æœ€åï¼Œæˆ‘ä»¬çš„è®­ç»ƒæŸå¤±æ˜¯3.8ï¼ŒéªŒè¯æŸå¤±ç¨å¾®è¶…è¿‡4.1ã€‚æ‰€ä»¥å¹¶ä¸ç®—å¤ªå·®ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œå¦‚æœæˆ‘ä»¬è°ƒç”¨model.fitï¼Œé‚£ä¹ˆå®ƒå°†è¿”å›å†å²è®°å½•ï¼Œå…¶ä¸­å­˜å‚¨äº†è¿™ä¸¤ä¸ªæŸå¤±ã€‚
- en: So we can assign it to a variableï¼Œ which I call history hereã€‚ and then I simply
    plot the historyã€‚ So then we can access these two losses by calling history dot
    history and then access the loss and the validation lossã€‚So let's plot this and
    then we see that our losses decreasedï¼Œ so it'sï¼Œ it looks very good hereã€‚And then
    to evaluate our modelï¼Œ we simplyï¼Œ like last timeã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†é…ç»™ä¸€ä¸ªå˜é‡ï¼Œæˆ‘åœ¨è¿™é‡Œç§°ä¹‹ä¸ºhistoryã€‚ç„¶åæˆ‘ç®€å•åœ°ç»˜åˆ¶å†å²è®°å½•ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨history.dot.historyæ¥è®¿é—®è¿™ä¸¤ä¸ªæŸå¤±ï¼Œç„¶åè®¿é—®æŸå¤±å’ŒéªŒè¯æŸå¤±ã€‚è®©æˆ‘ä»¬ç»˜åˆ¶è¿™ä¸ªï¼Œç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æŸå¤±å‡å°‘äº†ï¼Œçœ‹èµ·æ¥éå¸¸ä¸é”™ã€‚ç„¶åè¦è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±åƒä¸Šæ¬¡ä¸€æ ·ç®€å•è¿›è¡Œã€‚
- en: call model dot evaluate with our test features only with horsepower and the
    test labelsã€‚So let's run thisã€‚ And this was very fastã€‚ So here we seeã€‚ we have
    a final loss for the test data of 3ã€‚6ã€‚ So not very badã€‚ And now let's predict
    some samplesã€‚ So in this caseï¼Œ I simply createã€‚Test dataã€‚ So all the values betweenã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨model.dot.evaluateï¼Œåªç”¨æˆ‘ä»¬çš„æµ‹è¯•ç‰¹å¾ï¼Œå³é©¬åŠ›å’Œæµ‹è¯•æ ‡ç­¾ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚è¿™éå¸¸å¿«ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°ã€‚æµ‹è¯•æ•°æ®çš„æœ€ç»ˆæŸå¤±ä¸º3.6ã€‚æ‰€ä»¥å¹¶ä¸ç®—å¤ªå·®ã€‚ç°åœ¨è®©æˆ‘ä»¬é¢„æµ‹ä¸€äº›æ ·æœ¬ã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ç®€å•åˆ›å»ºäº†æµ‹è¯•æ•°æ®ã€‚æ‰€æœ‰å€¼åœ¨ã€‚
- en: The min value and the max value and increase the range a little bitã€‚ You could
    also just use a hard coded number hereï¼Œ like from0 to 250ã€‚ But I want to be it
    like a little bit more suited for the used featureã€‚ So we use thisã€‚ And then to
    predictï¼Œ we call model dot predictã€‚ And then our new test data that we want to
    predictã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´ï¼Œå¹¶å¢åŠ èŒƒå›´ä¸€ç‚¹ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œç›´æ¥ä½¿ç”¨ä¸€ä¸ªç¡¬ç¼–ç çš„æ•°å­—ï¼Œæ¯”å¦‚ä»0åˆ°250ã€‚ä½†æˆ‘æƒ³è®©å®ƒæ›´é€‚åˆä½¿ç”¨çš„ç‰¹å¾ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªã€‚ç„¶åè¦è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬è°ƒç”¨model.dot.predictã€‚ç„¶åæ˜¯æˆ‘ä»¬æƒ³è¦é¢„æµ‹çš„æ–°æµ‹è¯•æ•°æ®ã€‚
- en: And then we plot it with our function from the beginningã€‚And this is the same
    plot as in the beginningã€‚ So we plot the horsepower against the MP7 valueã€‚ And
    then these are our new x values that we predictã€‚ and we see that our predictionã€‚
    So here we plot a lineã€‚ and this is a linear line since we use a linear regression
    modelã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ç”¨ä¸€å¼€å§‹çš„å‡½æ•°ç»˜åˆ¶å®ƒã€‚è¿™ä¸ä¸€å¼€å§‹çš„å›¾ç›¸åŒã€‚å› æ­¤æˆ‘ä»¬ç»˜åˆ¶é©¬åŠ›ä¸MP7å€¼çš„å…³ç³»ã€‚è¿™äº›æ˜¯æˆ‘ä»¬é¢„æµ‹çš„æ–°xå€¼ã€‚æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„é¢„æµ‹ã€‚å› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬ç»˜åˆ¶ä¸€æ¡çº¿ã€‚è¿™æ˜¯ä¸€æ¡çº¿æ€§çº¿ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯çº¿æ€§å›å½’æ¨¡å‹ã€‚
- en: And we see that it's not too badã€‚ So we see the same trendã€‚ So the more horsepower
    our car has the lower the MP isã€‚ but for exampleï¼Œ here in this areaã€‚ it's staying
    the same except that our horsepower is further increasingã€‚ So yeahã€‚ in this area
    and also maybe in this area areaï¼Œ it's not perfectã€‚ So but in the restã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°æƒ…å†µè¿˜ä¸é”™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°ç›¸åŒçš„è¶‹åŠ¿ã€‚æˆ‘ä»¬çš„æ±½è½¦é©¬åŠ›è¶Šå¤§ï¼ŒMPè¶Šä½ã€‚ä½†æ˜¯ï¼Œä¾‹å¦‚ï¼Œåœ¨è¿™ä¸ªåŒºåŸŸï¼Œé™¤éæˆ‘ä»¬çš„é©¬åŠ›è¿›ä¸€æ­¥å¢åŠ ï¼Œå¦åˆ™å®ƒä¿æŒä¸å˜ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œåœ¨è¿™ä¸ªåŒºåŸŸï¼Œä¹Ÿè®¸åœ¨è¿™ä¸ªåŒºåŸŸï¼Œå®ƒå¹¶ä¸å®Œç¾ã€‚ä½†æ˜¯åœ¨å…¶ä½™éƒ¨åˆ†ã€‚
- en: it looks it looks okayã€‚Soï¼Œ yeahï¼Œ so this is how we apply a linear modelã€‚ and
    againã€‚ we only need the one dense layer with one output unitã€‚ and we also apply
    this normal normalization layerã€‚ So this is all we need to use linear regressionã€‚
    And now let's extend this to a deep neural networkã€‚ So as I saidï¼Œ with only one
    dense layerã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥è¿˜ä¸é”™ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åº”ç”¨çº¿æ€§æ¨¡å‹ã€‚å†ä¸€æ¬¡ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªè¾“å‡ºå•å…ƒçš„å¯†é›†å±‚ã€‚æˆ‘ä»¬è¿˜åº”ç”¨è¿™ä¸ªå½’ä¸€åŒ–å±‚ã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¿›è¡Œçº¿æ€§å›å½’æ‰€éœ€çš„ä¸€åˆ‡ã€‚ç°åœ¨è®©æˆ‘ä»¬å°†å…¶æ‰©å±•ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œåªæœ‰ä¸€ä¸ªå¯†é›†å±‚ã€‚
- en: we can only use a or only get a linear linear function here like thisï¼Œ which
    is not perfectã€‚ So to further improve thisï¼Œ we can introduce more layers here
    and convert this to a fully feet forward neural netã€‚So we simplyï¼Œ we still use
    the normalization layer and the dense layer at the endã€‚ But in the middleã€‚ we
    use some more dense layersã€‚ So let's sayï¼Œ layers dot denseã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿™é‡Œåªèƒ½ä½¿ç”¨ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œè¿™å¹¶ä¸å®Œç¾ã€‚å› æ­¤ï¼Œä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå¼•å…¥æ›´å¤šå±‚ï¼Œå°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªå®Œå…¨å‰é¦ˆçš„ç¥ç»ç½‘ç»œã€‚æ‰€ä»¥æˆ‘ä»¬ç®€å•åœ°ï¼Œä»ç„¶åœ¨æœ€åä½¿ç”¨å½’ä¸€åŒ–å±‚å’Œå¯†é›†å±‚ã€‚ä½†æ˜¯åœ¨ä¸­é—´ï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´å¤šçš„å¯†é›†å±‚ã€‚å‡è®¾ï¼Œlayers.dot.denseã€‚
- en: And then here you can try out different values for the hidden sizeã€‚ And as I
    said in the last timeã€‚ we also apply activation functions for these layers in
    the middleã€‚ğŸ˜Šï¼ŒSo let's yeahã€‚ let's use the relu againï¼Œ like last timeã€‚ And then
    let's use the same one againã€‚And yeahã€‚ and then at the endï¼Œ we use our dense layer
    with one outputã€‚ And now this is all we needã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨è¿™é‡Œä½ å¯ä»¥å°è¯•ä¸åŒçš„éšè—å±‚å¤§å°ã€‚æ­£å¦‚æˆ‘ä¸Šæ¬¡æ‰€è¯´ï¼Œæˆ‘ä»¬è¿˜å¯¹ä¸­é—´è¿™äº›å±‚åº”ç”¨æ¿€æ´»å‡½æ•°ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘ä»¬æ˜¯çš„ï¼Œè®©æˆ‘ä»¬åƒä¸Šæ¬¡ä¸€æ ·å†ç”¨ä¸€æ¬¡reluã€‚ç„¶åæˆ‘ä»¬å†ä½¿ç”¨åŒæ ·çš„ã€‚æ˜¯çš„ï¼Œç„¶ååœ¨æœ€åï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªè¾“å‡ºçš„å¯†é›†å±‚ã€‚ç°åœ¨è¿™å°±æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„å…¨éƒ¨ã€‚
- en: And now we converted our linear regression model to a deep neural network modelã€‚
    So let's use thisã€‚ And then againï¼Œ we compile it with the loss and againã€‚ a atom
    optimizer and call the summary to see the different layers againã€‚ And then we
    see there is a lot more parameters that we have to train nowã€‚ So now againã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†æˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹è½¬æ¢ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªã€‚ç„¶åå†æ¬¡ï¼Œæˆ‘ä»¬ç”¨æŸå¤±å’Œä¸€ä¸ªadamä¼˜åŒ–å™¨è¿›è¡Œç¼–è¯‘ï¼Œå¹¶è°ƒç”¨æ‘˜è¦ä»¥å†æ¬¡æŸ¥çœ‹ä¸åŒçš„å±‚ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°ç°åœ¨æœ‰å¾ˆå¤šå‚æ•°éœ€è¦è®­ç»ƒã€‚å› æ­¤ç°åœ¨å†æ¬¡ã€‚
- en: let's call model fit with our horsepower feature and start the trainingã€‚ğŸ˜Šï¼ŒAnd
    training is doneã€‚ And we see the final model or final loss is slightly better
    nowã€‚ so slightly lower than beforeã€‚ So let's evaluate our modelã€‚ And now it's
    below threeã€‚ So it's definitely betterã€‚ And now againã€‚ let's make some predictions
    and plot thisã€‚ And now we see a new plotã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨æˆ‘ä»¬çš„é©¬åŠ›ç‰¹å¾è°ƒç”¨æ¨¡å‹æ‹Ÿåˆå¹¶å¼€å§‹è®­ç»ƒã€‚ğŸ˜Šï¼Œè®­ç»ƒå®Œæˆã€‚æˆ‘ä»¬çœ‹åˆ°æœ€ç»ˆæ¨¡å‹æˆ–æœ€ç»ˆæŸå¤±ç°åœ¨ç•¥å¾®æ”¹å–„ï¼Œæ‰€ä»¥æ¯”ä¹‹å‰ç•¥ä½ã€‚è®©æˆ‘ä»¬è¯„ä¼°ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç°åœ¨å®ƒä½äºä¸‰ã€‚å› æ­¤å®ƒè‚¯å®šæ›´å¥½ã€‚ç°åœ¨æˆ‘ä»¬å†åšä¸€äº›é¢„æµ‹å¹¶ç»˜åˆ¶å›¾è¡¨ã€‚ç°åœ¨æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæ–°å›¾ã€‚
- en: And now we see that our function is no longer only linearã€‚ But here we also
    have some nonlinear areasã€‚ğŸ˜Šï¼ŒAnd this is the effects of the activation functions
    that we usedã€‚ And that's why this is such an important thing in neural networks
    to apply activation functionsã€‚ So here againï¼Œ we have this only our linear regression
    model with one dense layerã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„å‡½æ•°ä¸å†åªæ˜¯çº¿æ€§ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬ä¹Ÿæœ‰ä¸€äº›éçº¿æ€§åŒºåŸŸã€‚ğŸ˜Šï¼Œè¿™æ˜¯æˆ‘ä»¬æ‰€ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°çš„æ•ˆæœã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨ç¥ç»ç½‘ç»œä¸­åº”ç”¨æ¿€æ´»å‡½æ•°å¦‚æ­¤é‡è¦ã€‚å› æ­¤åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å†æ¬¡åªæœ‰ä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªå¯†é›†å±‚çš„çº¿æ€§å›å½’æ¨¡å‹ã€‚
- en: So here we can only predict a linear functionã€‚ And now with a deep neural networkã€‚
    we can get a non nonlinearar function as predictionã€‚ So yeahã€‚ I think that's all
    and now so we only used one feature for now so that we can stay in this 2D caseã€‚
    but of courseï¼Œ we can include all of the features and we do it the sameã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬åªèƒ½é¢„æµ‹çº¿æ€§å‡½æ•°ã€‚è€Œç°åœ¨é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—éçº¿æ€§å‡½æ•°ä½œä¸ºé¢„æµ‹ã€‚æ‰€ä»¥æ˜¯çš„ã€‚æˆ‘æƒ³è¿™å°±æ˜¯å…¨éƒ¨ï¼Œç°åœ¨æˆ‘ä»¬åªä½¿ç”¨ä¸€ä¸ªç‰¹å¾ï¼Œä»¥ä¾¿ä¿æŒåœ¨è¿™ä¸ªäºŒç»´æ¡ˆä¾‹ä¸­ï¼Œä½†å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥åŒ…æ‹¬æ‰€æœ‰ç‰¹å¾ï¼Œæ–¹å¼ç›¸åŒã€‚
- en: So we don't have to change anything actuallyã€‚ So we use this normalizer that
    we adapt it to all theã€‚Maining featuresã€‚ And here we have only our single dense
    layerã€‚ Then we compile thisã€‚And then we fit it here to the all of the training
    features and train it andã€‚And nowã€‚ training is doneã€‚ And we see that the loss
    is also below 3ã€‚ So training on all features makes itã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®é™…ä¸Šæˆ‘ä»¬ä¸éœ€è¦æ›´æ”¹ä»»ä½•ä¸œè¥¿ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ ‡å‡†åŒ–å™¨ï¼Œå°†å…¶è°ƒæ•´åˆ°æ‰€æœ‰çš„ä¸»è¦ç‰¹å¾ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå¯†é›†å±‚ã€‚ç„¶åæˆ‘ä»¬ç¼–è¯‘å®ƒã€‚æ¥ç€æˆ‘ä»¬å°†å…¶æ‹Ÿåˆåˆ°æ‰€æœ‰è®­ç»ƒç‰¹å¾ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç°åœ¨ï¼Œè®­ç»ƒå®Œæˆã€‚æˆ‘ä»¬çœ‹åˆ°æŸå¤±ä¹Ÿä½äº3ã€‚å› æ­¤ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾è¿›è¡Œè®­ç»ƒæ•ˆæœå¾ˆå¥½ã€‚
- en: Furtherï¼Œ betterã€‚ And now let's evaluate it as this last thingã€‚ And then we see
    we get the final lossã€‚So yeahï¼Œ so you see you don't the code is actually the sameã€‚
    We just use all of the training features here and also for the normalizerã€‚ we
    adapted it to all of the training featuresã€‚ So yeahã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ›´å¥½ã€‚ç°åœ¨è®©æˆ‘ä»¬å°†å…¶è¯„ä¼°ä¸ºæœ€åä¸€ä»¶äº‹æƒ…ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°æœ€ç»ˆçš„æŸå¤±ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œä½ ä¼šçœ‹åˆ°ä»£ç å®é™…ä¸Šæ˜¯ç›¸åŒçš„ã€‚æˆ‘ä»¬åªæ˜¯åœ¨è¿™é‡Œä½¿ç”¨æ‰€æœ‰çš„è®­ç»ƒç‰¹å¾ï¼Œä»¥åŠæ ‡å‡†åŒ–å™¨ã€‚æˆ‘ä»¬å°†å…¶è°ƒæ•´åˆ°æ‰€æœ‰è®­ç»ƒç‰¹å¾ã€‚å› æ­¤ï¼Œæ˜¯çš„ã€‚
- en: I think now you learned a lot in this tutorial how we can download and load
    CSv data and how we analyze it and preprocess it with the pandas framework and
    then how we use the normalization layer to normalize our data and then how we
    set up a a single linear regression model with this dense layerã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³ä½ ç°åœ¨åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­å­¦åˆ°äº†å¾ˆå¤šï¼Œå…³äºå¦‚ä½•ä¸‹è½½å’ŒåŠ è½½CSVæ•°æ®ï¼Œå¦‚ä½•ç”¨pandasæ¡†æ¶åˆ†æå’Œé¢„å¤„ç†å®ƒï¼Œç„¶åå¦‚ä½•ä½¿ç”¨æ ‡å‡†åŒ–å±‚æ¥æ ‡å‡†åŒ–æˆ‘ä»¬çš„æ•°æ®ï¼Œæ¥ç€å¦‚ä½•ç”¨è¿™ä¸ªå¯†é›†å±‚è®¾ç½®ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ã€‚
- en: And then againï¼Œ training and evaluation is the same like last timeã€‚ And yeahï¼Œ
    and we alsoã€‚ you learned about the effect of the activation functions in a deep
    neural networkã€‚And yeahã€‚ that's it for todayã€‚ I hope you enjoyed this tutorialã€‚
    And if you liked itã€‚ then please hit the like button and consider subscribing
    to the channelã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå†æ¬¡ï¼Œè®­ç»ƒå’Œè¯„ä¼°ä¸ä¸Šæ¬¡ç›¸åŒã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬ä¹Ÿã€‚ä½ äº†è§£äº†æ¿€æ´»å‡½æ•°åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„ä½œç”¨ã€‚å°±è¿™æ ·ï¼Œä»Šå¤©åˆ°æ­¤ä¸ºæ­¢ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚å¦‚æœä½ å–œæ¬¢ï¼Œè¯·ç‚¹å‡»èµæŒ‰é’®ï¼Œå¹¶è€ƒè™‘è®¢é˜…é¢‘é“ã€‚
- en: And I hope to see you in the next videoï¼Œ byeã€‚ğŸ˜Šã€‚![](img/938d327f6471f60f92b4ed5bae0092f3_2.png)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘è§åˆ°ä½ ï¼Œå†è§ã€‚ğŸ˜Šï¼[](img/938d327f6471f60f92b4ed5bae0092f3_2.png)
