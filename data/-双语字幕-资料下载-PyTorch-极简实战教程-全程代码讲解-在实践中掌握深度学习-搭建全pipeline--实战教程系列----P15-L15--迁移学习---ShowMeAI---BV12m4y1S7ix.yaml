- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL15-
    è¿ç§»å­¦ä¹  - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL15-
    è¿ç§»å­¦ä¹  - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ welcomele to your new Pytorch tutorial In this tutorialã€‚ we will
    talk about transfer learning and how it can be applied in Pytorchã€‚ Transfer learning
    is a machine learning method where a model developed for a first task is then
    reused as the starting point for a model on a second taskã€‚ For exampleï¼Œ we can
    train a model to classify birds and cats and then use the same modelã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°ä½ çš„æ–° PyTorch æ•™ç¨‹ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿ç§»å­¦ä¹ ä»¥åŠå¦‚ä½•åœ¨ PyTorch ä¸­åº”ç”¨å®ƒã€‚è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­ä¸ºç¬¬ä¸€ä¸ªä»»åŠ¡å¼€å‘çš„æ¨¡å‹è¢«é‡æ–°ç”¨äºç¬¬äºŒä¸ªä»»åŠ¡çš„èµ·å§‹ç‚¹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥åˆ†ç±»é¸Ÿç±»å’ŒçŒ«ï¼Œç„¶åä½¿ç”¨åŒæ ·çš„æ¨¡å‹ã€‚
- en: modified only a little bit in the last layerï¼Œ and then use the new model to
    classify bees and dogsã€‚ So it's a popular approach in deep learning that allows
    rapid generation of new modelsã€‚ And this is super important because training of
    a completely new model can be very time consumingã€‚ It can take multiple days or
    even weeksã€‚ So if you use a pretrained modelã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…åœ¨æœ€åä¸€å±‚ç¨ä½œä¿®æ”¹ï¼Œç„¶åä½¿ç”¨æ–°æ¨¡å‹æ¥åˆ†ç±»èœœèœ‚å’Œç‹—ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€ç§æµè¡Œçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆæ–°æ¨¡å‹ã€‚è¿™éå¸¸é‡è¦ï¼Œå› ä¸ºè®­ç»ƒä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹å¯èƒ½ä¼šéå¸¸è€—æ—¶ï¼Œå¯èƒ½éœ€è¦å‡ å¤©ç”šè‡³å‡ å‘¨çš„æ—¶é—´ã€‚å› æ­¤ï¼Œå¦‚æœä½ ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚
- en: then we typically exchange only the last layer and then do not need to train
    the whole model againã€‚ Howeverï¼Œ transfer learning can achieve pretty good performance
    resultsã€‚ and that's why it's so popular nowadaysã€‚ğŸ˜Šï¼ŒSo let's have a look at this
    picture hereã€‚ We have a typical CNN architecture that I already showed you in
    the last tutorial and thisã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬é€šå¸¸åªäº¤æ¢æœ€åä¸€å±‚ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿ç§»å­¦ä¹ èƒ½å¤Ÿå–å¾—ç›¸å½“ä¸é”™çš„æ€§èƒ½ç»“æœï¼Œè¿™å°±æ˜¯å®ƒå¦‚ä»Šå¦‚æ­¤å—æ¬¢è¿çš„åŸå› ã€‚ğŸ˜Šæ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™é‡Œçš„è¿™å¼ å›¾ç‰‡ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå…¸å‹çš„
    CNN æ¶æ„ï¼Œæˆ‘åœ¨ä¸Šä¸€ä¸ªæ•™ç¨‹ä¸­å·²ç»å‘ä½ å±•ç¤ºè¿‡ã€‚
- en: let's say this has been already trained on a lot of data and we have the optimized
    weights and now we only want to take the last fully connected layerã€‚ So this one
    here and then modify it and train the last layer on our new dataã€‚So then we have
    a new model that has been trained and tweaked in the last layerã€‚And yeahã€‚ this
    is the concept of transfer learningã€‚ And now let's have a look at a concrete example
    in Pytorchã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚è¯´ï¼Œè¿™ä¸ªæ¨¡å‹å·²ç»åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰ä¼˜åŒ–çš„æƒé‡ï¼Œç°åœ¨æˆ‘ä»¬åªæƒ³ä¿ç•™æœ€åä¸€å±‚çš„å…¨è¿æ¥å±‚ã€‚æ‰€ä»¥å°±æ˜¯è¿™ä¸€å±‚ï¼Œç„¶åä¿®æ”¹å®ƒï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æ–°æ•°æ®ä¸Šè®­ç»ƒæœ€åä¸€å±‚ã€‚è¿™æ ·æˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªåœ¨æœ€åä¸€å±‚ç»è¿‡è®­ç»ƒå’Œè°ƒæ•´çš„æ–°æ¨¡å‹ã€‚æ˜¯çš„ï¼Œè¿™å°±æ˜¯è¿ç§»å­¦ä¹ çš„æ¦‚å¿µã€‚ç°åœ¨è®©æˆ‘ä»¬åœ¨
    PyTorch ä¸­çœ‹çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚
- en: So in this exampleï¼Œ we wantï¼Œ we are using the pretrained Resnet 18 C Nã€‚ This
    is a network that is trained on more than a million images from the Inet databaseã€‚
    And this network is 18 layers deep and can classify images into 1000 object categoriesã€‚
    And now in our exampleï¼Œ we have only two classesï¼Œ So we only want to detect Bs
    and antsã€‚ğŸ˜Šã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æƒ³ä½¿ç”¨é¢„è®­ç»ƒçš„ Resnet 18 CNNã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨è¶…è¿‡ä¸€ç™¾ä¸‡å¼ æ¥è‡ª Inet æ•°æ®åº“çš„å›¾åƒä¸Šè®­ç»ƒçš„ç½‘ç»œã€‚è¿™ä¸ªç½‘ç»œæœ‰ 18 å±‚æ·±ï¼Œå¯ä»¥å°†å›¾åƒåˆ†ç±»ä¸º
    1000 ä¸ªå¯¹è±¡ç±»åˆ«ã€‚è€Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œæ‰€ä»¥æˆ‘ä»¬åªæƒ³æ£€æµ‹èœœèœ‚å’Œèš‚èšã€‚ğŸ˜Š
- en: '![](img/96844ee58fe06a9d803e6f751f732b19_1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96844ee58fe06a9d803e6f751f732b19_1.png)'
- en: And yeahï¼Œ so let's startã€‚ So in this sessionï¼Œ I alreadyã€‚ I also want to show
    you two other new thingsã€‚ So firstï¼Œ the data sets image folderã€‚ how we can use
    this and how use a scheduleula to change the learning rateã€‚And thenï¼Œ of courseã€‚
    how transfer learning is usedã€‚So I already imported the things that we needã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘è¿˜æƒ³ç»™ä½ å±•ç¤ºä¸¤ä¸ªå…¶ä»–çš„æ–°ä¸œè¥¿ã€‚é¦–å…ˆæ˜¯æ•°æ®é›†çš„å›¾åƒæ–‡ä»¶å¤¹ï¼Œæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨å®ƒï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨è°ƒåº¦ç¨‹åºæ¥æ”¹å˜å­¦ä¹ ç‡ã€‚ç„¶åï¼Œå½“ç„¶ï¼Œè¿ç§»å­¦ä¹ æ˜¯å¦‚ä½•è¢«ä½¿ç”¨çš„ã€‚æˆ‘å·²ç»å¯¼å…¥äº†æˆ‘ä»¬éœ€è¦çš„å†…å®¹ã€‚
- en: and now we set up the dataã€‚ and the last time we used the built in data sets
    from the Torch vision data setsã€‚ And now here we use the data sets dot image folder
    because we saved our data in a folder and this has to have the structure like
    thisã€‚ So we have the folder hereã€‚And then we have a training and a validation
    folderã€‚ So train and Valã€‚ And in each oneï¼Œ we have folders for each classã€‚ So
    here we have ans in ants and Bsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è®¾ç½®æ•°æ®ã€‚ä¸Šæ¬¡æˆ‘ä»¬ä½¿ç”¨äº† Torch è§†è§‰æ•°æ®é›†ä¸­çš„å†…ç½®æ•°æ®é›†ï¼Œè€Œç°åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†çš„å›¾åƒæ–‡ä»¶å¤¹ï¼Œå› ä¸ºæˆ‘ä»¬æŠŠæ•°æ®ä¿å­˜åœ¨ä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œå¹¶ä¸”è¿™ä¸ªæ–‡ä»¶å¤¹éœ€è¦å…·æœ‰è¿™æ ·çš„ç»“æ„ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæœ‰ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œç„¶åæœ‰ä¸€ä¸ªè®­ç»ƒæ–‡ä»¶å¤¹å’Œä¸€ä¸ªéªŒè¯æ–‡ä»¶å¤¹ã€‚æ‰€ä»¥æ˜¯
    train å’Œ Valã€‚åœ¨æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæˆ‘ä»¬éƒ½æœ‰æ¯ä¸ªç±»åˆ«çš„æ–‡ä»¶å¤¹ã€‚è¿™é‡Œæˆ‘ä»¬æœ‰èš‚èšå’Œèœœèœ‚ã€‚
- en: and also in the validation folderï¼Œ we have ants and Bsã€‚And now in each folderã€‚
    we have the images hereã€‚ Soï¼Œ for exampleï¼Œ hereï¼Œ we have some antsã€‚ And alsoã€‚ let's
    have a look at some Bsã€‚ So here we have a Bã€‚And yeahã€‚ so you must structure your
    folder like thisã€‚ And then you can call the data sets dot image folder and give
    it the pathã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éªŒè¯æ–‡ä»¶å¤¹ä¸­ï¼Œæˆ‘ä»¬æœ‰èš‚èšå’Œèœœèœ‚ã€‚ç°åœ¨åœ¨æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæˆ‘ä»¬æœ‰è¿™é‡Œçš„å›¾åƒã€‚æ‰€ä»¥ï¼Œä¾‹å¦‚ï¼Œè¿™é‡Œæˆ‘ä»¬æœ‰ä¸€äº›èš‚èšã€‚è¿˜æœ‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›èœœèœ‚ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€åªèœœèœ‚ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥ä½ å¿…é¡»è¿™æ ·ç»„ç»‡ä½ çš„æ–‡ä»¶å¤¹ã€‚ç„¶åä½ å¯ä»¥è°ƒç”¨æ•°æ®é›†.dot
    image folderå¹¶ç»™å‡ºè·¯å¾„ã€‚
- en: And we also give it some transforms hereã€‚And thenï¼Œ we get theã€‚Classesã€‚ the class
    names by calling image setsï¼Œ image data sets dot classesã€‚And yeahã€‚ then here I
    defined the training model where I did the loop and did the training and the evaluationã€‚
    I will not go into detail hereã€‚ you should already know this from the last tutorials
    how at typical training and evaluation loop looks like you can also check the
    whole code on Gitthubã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜åœ¨è¿™é‡Œæä¾›äº†ä¸€äº›å˜æ¢ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è°ƒç”¨image setsï¼Œimage data sets.dot classesæ¥è·å–ç±»åã€‚æ˜¯çš„ï¼Œç„¶ååœ¨è¿™é‡Œæˆ‘å®šä¹‰äº†è®­ç»ƒæ¨¡å‹ï¼Œåœ¨å¾ªç¯ä¸­è¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ã€‚æˆ‘åœ¨è¿™é‡Œä¸ä¼šè¯¦ç»†è¯´æ˜ï¼Œä½ åº”è¯¥å·²ç»ä»ä¸Šæ¬¡çš„æ•™ç¨‹ä¸­çŸ¥é“å…¸å‹çš„è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯æ˜¯æ€æ ·çš„ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨GitHubä¸ŠæŸ¥çœ‹å®Œæ•´ä»£ç ã€‚
- en: so I will provide the link in the descriptionã€‚ So have a look at this yourselfã€‚And
    now let's use transfer learningã€‚ Soï¼Œ first of allï¼Œ we want to import the pre trained
    modelã€‚ So let's set up this model so we can do this by saying modelã€‚So model equalsã€‚
    And this is available in the Torch visionion dot models moduleã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†åœ¨æè¿°ä¸­æä¾›é“¾æ¥ã€‚è¯·è‡ªè¡ŒæŸ¥çœ‹ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨è¿ç§»å­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³å¯¼å…¥é¢„è®­ç»ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬è®¾ç½®è¿™ä¸ªæ¨¡å‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è¯´modelæ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥modelç­‰äºã€‚è¿™åœ¨Torch
    visionion.dot modelsæ¨¡å—ä¸­å¯ç”¨ã€‚
- en: So I imported torch vision models alreadyã€‚ and then I can call models dot Resnet
    16 or sorryã€‚ Resnet 18 hereã€‚ And then I can say pretrained equals trueã€‚ So this
    is already the optimized weights that are trained on the imagenet dataã€‚And now
    what we want to do is we want to exchange the last fully connected layerã€‚ So first
    of allã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å·²ç»å¯¼å…¥äº†torch visionæ¨¡å‹ã€‚ç„¶åæˆ‘å¯ä»¥è°ƒç”¨models.dot Resnet 16æˆ–æŠ±æ­‰ï¼ŒResnet 18ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å¯ä»¥è¯´pretrainedç­‰äºtrueã€‚è¿™å·²ç»æ˜¯é’ˆå¯¹imagenetæ•°æ®è®­ç»ƒçš„ä¼˜åŒ–æƒé‡ã€‚ç°åœ¨æˆ‘ä»¬æƒ³åšçš„æ˜¯äº¤æ¢æœ€åçš„å…¨è¿æ¥å±‚ã€‚é¦–å…ˆã€‚
- en: let's get the number of input features from the last layerã€‚ So let's say nu
    features equals modelã€‚ and we can get this by calling dot F fully connectedã€‚ And
    then the input featuresã€‚ So this is the number of input features for the last
    layer that we needã€‚And then let's create a new layer and assign it to the last
    layerã€‚ So let's say modelã€‚Dot F C equalsã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è·å–æœ€åä¸€å±‚çš„è¾“å…¥ç‰¹å¾æ•°é‡ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´nu featuresç­‰äºmodelã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨dot F fully connectedæ¥è·å–è¿™ä¸ªã€‚ç„¶åæ˜¯è¾“å…¥ç‰¹å¾ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦çš„æœ€åä¸€å±‚çš„è¾“å…¥ç‰¹å¾æ•°é‡ã€‚ç„¶åè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°å±‚å¹¶å°†å…¶åˆ†é…ç»™æœ€åä¸€å±‚ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´model.Dot
    F Cç­‰äºã€‚
- en: And now we give it a new fully connected layer N N dot linearã€‚ And this gets
    the number of input features that we haveã€‚And then as new output featuresã€‚ number
    of outputsï¼Œ we have two because we have two classes nowã€‚And now we send our model
    to the deviceã€‚ If we have GP supportã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»™å®ƒä¸€ä¸ªæ–°çš„å…¨è¿æ¥å±‚N N.dot linearã€‚è¿™è·å–æˆ‘ä»¬æ‹¥æœ‰çš„è¾“å…¥ç‰¹å¾çš„æ•°é‡ã€‚ç„¶åä½œä¸ºæ–°çš„è¾“å‡ºç‰¹å¾ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªè¾“å‡ºï¼Œå› ä¸ºç°åœ¨æœ‰ä¸¤ä¸ªç±»ã€‚ç°åœ¨æˆ‘ä»¬å°†æ¨¡å‹å‘é€åˆ°è®¾å¤‡ã€‚å¦‚æœæˆ‘ä»¬æœ‰GPæ”¯æŒã€‚
- en: So we created our device in the beginningï¼Œ as alwaysã€‚ So this is Kuda or simply
    CPUã€‚And now that we have our new modelï¼Œ we canï¼Œ againï¼Œ as alwaysï¼Œ define our loss
    and optimize usã€‚ So we say criterionã€‚Equals Nï¼Œ N dot cross entropisã€‚And then let's
    say the optr equalsã€‚ This is from the optimization moduleï¼Œ Opim dot S GDï¼Œ stochastic
    gradient descentã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨å¼€å§‹æ—¶åˆ›å»ºäº†æˆ‘ä»¬çš„è®¾å¤‡ï¼Œå’Œå¾€å¸¸ä¸€æ ·ã€‚è¿™æ˜¯Kudaæˆ–ç®€å•çš„CPUã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ–°æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡åƒå¾€å¸¸ä¸€æ ·å®šä¹‰æˆ‘ä»¬çš„æŸå¤±å’Œä¼˜åŒ–å™¨ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´criterionç­‰äºNï¼ŒN.dot
    cross entropisã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´optrç­‰äºã€‚è¿™æ¥è‡ªä¼˜åŒ–æ¨¡å—ï¼ŒOpim.dot S GDï¼Œéšæœºæ¢¯åº¦ä¸‹é™ã€‚
- en: which has to optimize the model parametersã€‚ And we have to specify the learning
    rate equalsã€‚ let's say0001ã€‚Andã€‚Nowï¼Œ as a new thingï¼Œ let's use a scheduleulaã€‚ This
    will update the learning rateã€‚ soã€‚For thisï¼Œ we can say we can create this by saying
    it's called a step L R scheduleularã€‚Equalsã€‚ and the L R scheduletula is available
    also in the torch optimization moduleã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬å¿…é¡»æŒ‡å®šå­¦ä¹ ç‡ç­‰äºã€‚è®©æˆ‘ä»¬è¯´0001ã€‚ç°åœ¨ï¼Œä½œä¸ºä¸€ä»¶æ–°äº‹ç‰©ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨scheduleulaã€‚è¿™å°†æ›´æ–°å­¦ä¹ ç‡ã€‚æ‰€ä»¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¯´å®ƒå«åšstep
    L R scheduleularæ¥åˆ›å»ºè¿™ä¸ªã€‚å¹¶ä¸”L R scheduletulaåœ¨torchä¼˜åŒ–æ¨¡å—ä¸­ä¹Ÿå¯ç”¨ã€‚
- en: So we already imported thisã€‚ And then we can say L R scheduletula dot step L
    Rã€‚ And then here we have to give it the optimizerã€‚ So here we say optimizerã€‚And
    then we say step sizeã€‚ step size equals 7ã€‚ And then we say gamma equals let's
    sayï¼Œ01ã€‚ This means that every 7 epochsã€‚ Our learning rate is multiplied by this
    valueã€‚ So every 7 epochsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å·²ç»å¯¼å…¥äº†è¿™ä¸€ç‚¹ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è¯´å­¦ä¹ ç‡è°ƒåº¦å™¨æ‰§è¡Œå­¦ä¹ ç‡çš„æ­¥éª¤ã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬å¿…é¡»æä¾›ä¼˜åŒ–å™¨ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ä¼˜åŒ–å™¨ã€‚ç„¶åæˆ‘ä»¬è¯´æ­¥é•¿ï¼Œæ­¥é•¿ç­‰äº7ã€‚ç„¶åæˆ‘ä»¬è¯´ä¼½ç›ï¼Œå‡è®¾ä¸º0.1ã€‚è¿™æ„å‘³ç€æ¯7ä¸ªè½®æ¬¡ï¼Œæˆ‘ä»¬çš„å­¦ä¹ ç‡ä¹˜ä»¥è¿™ä¸ªå€¼ã€‚å› æ­¤æ¯7ä¸ªè½®æ¬¡ã€‚
- en: Our learning rate has only 10 is now only updated to 10 per centã€‚Soï¼Œ yeahã€‚ this
    is how we use a scheduletulaã€‚ And then typically what we want to do is in our
    loopã€‚ in our loop over the epochã€‚ So for epoch in rangeï¼Œ let's sayï¼Œ100ã€‚ And then
    typically here we use the trainingï¼Œ where we also doã€‚Theã€‚The optimizer dot stepã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å­¦ä¹ ç‡ç°åœ¨åªæœ‰10ï¼Œä»…æ›´æ–°åˆ°10%ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬ä½¿ç”¨è°ƒåº¦å™¨çš„æ–¹å¼ã€‚é€šå¸¸æˆ‘ä»¬æƒ³åœ¨æˆ‘ä»¬çš„å¾ªç¯ä¸­ï¼Œåœ¨æ¯ä¸ªè½®æ¬¡ä¸­è¿›è¡Œã€‚å‡è®¾å¯¹äºèŒƒå›´å†…çš„è½®æ¬¡ä¸º100ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨è®­ç»ƒï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜è¦æ‰§è¡Œä¼˜åŒ–å™¨çš„æ­¥éª¤ã€‚
- en: optimizer dot stepã€‚ Then we want to evaluate itï¼Œ evaluate itã€‚ And then we also
    have to call scheduler step scheduler stepã€‚ So this is how we use a schedulerã€‚
    Please have a look at the whole loop here yourselfã€‚So yeahã€‚ now we set up the
    scheduleular and let'sã€‚Call the training functionsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨æ‰§è¡Œæ­¥éª¤ã€‚ç„¶åæˆ‘ä»¬æƒ³è¦è¯„ä¼°å®ƒï¼Œè¯„ä¼°å®ƒã€‚æ¥ç€æˆ‘ä»¬è¿˜å¿…é¡»è°ƒç”¨è°ƒåº¦å™¨çš„æ­¥éª¤ã€‚è¿™æ ·æˆ‘ä»¬å°±ä½¿ç”¨äº†è°ƒåº¦å™¨ã€‚è¯·è‡ªå·±æŸ¥çœ‹æ•´ä¸ªå¾ªç¯ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬è®¾ç½®è°ƒåº¦å™¨ï¼Œè®©æˆ‘ä»¬è°ƒç”¨è®­ç»ƒå‡½æ•°ã€‚
- en: So here we say model equals and then train modelã€‚ So this is the function that
    I createdã€‚ And then I have to pass the modelï¼Œ the criterionã€‚The optimizerã€‚ the
    scheduler and also the number of epochsã€‚ So nu epochsï¼Œ let's say 20ã€‚Andã€‚Yeahã€‚
    so this is how we useï¼Œ how we can use transfer learningã€‚ So in this caseã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´æ¨¡å‹ç­‰äºï¼Œç„¶åè®­ç»ƒæ¨¡å‹ã€‚è¿™æ˜¯æˆ‘åˆ›å»ºçš„å‡½æ•°ã€‚ç„¶åæˆ‘å¿…é¡»ä¼ é€’æ¨¡å‹ã€æ ‡å‡†ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ä»¥åŠè®­ç»ƒè½®æ•°ã€‚å‡è®¾è®­ç»ƒè½®æ•°ä¸º20ã€‚é‚£ä¹ˆï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨è¿ç§»å­¦ä¹ ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ã€‚
- en: we use a technique that is called fine tuningã€‚Because hereï¼Œ weã€‚Train the whole
    model againã€‚ but only a little bitã€‚ So we fine tune all the weights based on the
    new dataã€‚And with the new last layerã€‚ So this is one optionã€‚ And the second one
    is for thisã€‚ I copy and paste the same thingã€‚Andã€‚Let's see where does it startã€‚
    So hereã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸€ç§å«åšå¾®è°ƒçš„æŠ€æœ¯ã€‚å› ä¸ºåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å†æ¬¡è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œä½†åªæ˜¯ç¨å¾®è°ƒæ•´ã€‚å› æ­¤æˆ‘ä»¬æ ¹æ®æ–°æ•°æ®å¾®è°ƒæ‰€æœ‰æƒé‡ï¼Œå¹¶ä½¿ç”¨æ–°çš„æœ€åä¸€å±‚ã€‚è¿™æ˜¯ä¸€ä¸ªé€‰é¡¹ã€‚ç¬¬äºŒä¸ªé€‰é¡¹æ˜¯ï¼Œæˆ‘å¤åˆ¶ç²˜è´´ç›¸åŒçš„å†…å®¹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä»å“ªé‡Œå¼€å§‹ã€‚
- en: and then as a second optionï¼Œ what we can do is we can freeze all theã€‚All the
    layers in the beginning and only train the very last layerã€‚ So for thisã€‚ we have
    to loop over all the parameters here after we got our modelã€‚ So we say 4ã€‚Pm in
    modelã€‚Dot parametersã€‚And then we can set there requires gra attribute to fall
    so we can say paraã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¬¬äºŒä¸ªé€‰é¡¹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸€å¼€å§‹å†»ç»“æ‰€æœ‰å±‚ï¼Œä»…è®­ç»ƒæœ€åä¸€å±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨è·å¾—æ¨¡å‹åå¿…é¡»éå†æ‰€æœ‰å‚æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´å¯¹äºæ¨¡å‹çš„å‚æ•°ï¼Œè®¾ç½®å…¶è¦æ±‚æ¢¯åº¦å±æ€§ä¸ºå‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´å‚æ•°ã€‚
- en: Dot requires grã€‚And then sayï¼Œ weï¼Œ sorryï¼Œ Do requires graã€‚Requires gra equals
    falseã€‚ Now we have itã€‚ and this will freeze all the layers in the beginningã€‚ And
    now we set up the new last layerã€‚ We create a new layer hereã€‚ and by defaultï¼Œ
    this has requires gra equals trueã€‚And then againã€‚ we set up the loss and optimizer
    and the schedule in this caseã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ¢¯åº¦ã€‚ç„¶åè¯´ï¼ŒæŠ±æ­‰ï¼Œéœ€è¦æ¢¯åº¦ï¼Œè¦æ±‚æ¢¯åº¦ç­‰äºå‡ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ï¼Œè¿™å°†åœ¨ä¸€å¼€å§‹å†»ç»“æ‰€æœ‰å±‚ã€‚ç°åœ¨æˆ‘ä»¬è®¾ç½®æ–°çš„æœ€åä¸€å±‚ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºä¸€ä¸ªæ–°å±‚ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™ä¸ªå±‚çš„è¦æ±‚æ¢¯åº¦ç­‰äºçœŸã€‚ç„¶åå†æ¬¡è®¾ç½®æŸå¤±ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ã€‚
- en: And then we do the training function againã€‚ And so yeahï¼Œ so this is even more
    fasterã€‚ And let's run this and then have a look at both the evaluationsã€‚ And I
    also print out the time that it tookã€‚Soã€‚Yeahï¼Œ let's save thisã€‚ and let's run this
    by saying Python transfer dot piã€‚Andã€‚This might firstã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å†æ¬¡æ‰§è¡Œè®­ç»ƒå‡½æ•°ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™æ ·ç”šè‡³æ›´å¿«ã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œç„¶åæŸ¥çœ‹ä¸¤ä¸ªè¯„ä¼°ç»“æœã€‚æˆ‘è¿˜ä¼šæ‰“å°å‡ºæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚æ‰€ä»¥ï¼Œå¥½çš„ï¼Œè®©æˆ‘ä»¬ä¿å­˜è¿™ä¸ªï¼Œå¹¶é€šè¿‡è¯´Python
    transfer.pyæ¥è¿è¡Œã€‚è¿™å¯èƒ½æœ€åˆä¼šã€‚
- en: it will download all the imagesï¼Œ and this might take a couple of seconds because
    I don't have GP support here on my MacBookã€‚ So I will skip thisï¼Œ and then I will
    see you in a secondã€‚All rightï¼Œ Soaï¼Œ I am backã€‚ So this took super long on my computerï¼Œ
    so I resetã€‚The number of epochs to just2 in this exampleã€‚ So let's have a look
    at the resultsã€‚ So after only two epochsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†ä¸‹è½½æ‰€æœ‰å›¾åƒï¼Œè¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿï¼Œå› ä¸ºæˆ‘åœ¨æˆ‘çš„MacBookä¸Šæ²¡æœ‰GPUæ”¯æŒã€‚æ‰€ä»¥æˆ‘ä¼šè·³è¿‡è¿™ä¸€ç‚¹ï¼Œç¨åå†è§ã€‚å¥½çš„ï¼Œæˆ‘å›æ¥äº†ã€‚è¿™åœ¨æˆ‘çš„ç”µè„‘ä¸ŠèŠ±äº†è¶…çº§é•¿çš„æ—¶é—´ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™ä¸ªä¾‹å­ä¸­å°†è®­ç»ƒè½®æ•°é‡ç½®ä¸º2ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ç»“æœã€‚åœ¨ä»…ä»…ä¸¤ä¸ªè®­ç»ƒè½®æ¬¡ä¹‹åã€‚
- en: so this is the first training where we did the fine tuning of the wholeã€‚Modelã€‚
    so this took three and a half minutesã€‚ and the best accuracy now is ã€‚92ã€‚ So 92%ã€‚And
    then this is the second training where we only trained the last layerã€‚ So this
    took only one and a half minutes approximatelyã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç¬¬ä¸€æ¬¡è®­ç»ƒï¼Œæˆ‘ä»¬å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚è¿™ä¸ªè¿‡ç¨‹èŠ±äº†ä¸‰åˆ†åŠé’Ÿï¼Œç›®å‰æœ€ä½³å‡†ç¡®ç‡æ˜¯92%ã€‚è¿™æ˜¯ç¬¬äºŒæ¬¡è®­ç»ƒï¼Œæˆ‘ä»¬ä»…è®­ç»ƒäº†æœ€åä¸€å±‚ï¼Œå¤§çº¦åªèŠ±äº†ä¸€åˆ†åŠé’Ÿã€‚
- en: and yeah accuracy is also is already over 80%ã€‚ So of courseã€‚ it's not as good
    as in when we train the whole trainingï¼Œ but still pretty good for only two epochsã€‚
    And now let's imagine if we set the number of epochs even higherã€‚ So yeahã€‚ this
    is why transfer learning is so powerful because we have a pretrained modelã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå‡†ç¡®ç‡å·²ç»è¶…è¿‡80%ã€‚å½“ç„¶ï¼Œè¿™è¿˜ä¸å¦‚æˆ‘ä»¬è®­ç»ƒæ•´ä¸ªæ¨¡å‹æ—¶çš„æ•ˆæœï¼Œä½†å¯¹äºä»…ä»…ä¸¤ä¸ªå‘¨æœŸæ¥è¯´ï¼Œå·²ç»ç›¸å½“ä¸é”™äº†ã€‚ç°åœ¨æˆ‘ä»¬å‡è®¾å°†å‘¨æœŸæ•°è®¾å¾—æ›´é«˜ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ˜¯è¿ç§»å­¦ä¹ å¦‚æ­¤å¼ºå¤§çš„åŸå› ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚
- en: and then we only fine tune it a little bit and do a completely new task and
    then achieve pretty good results tooã€‚ So yeahï¼Œ so now I hope you understood how
    transfer learning can be applied in Pythtorchã€‚ if you enjoyed the tutorialã€‚ Please
    subscribe to the channel and see you next time byã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹ï¼Œå®Œæˆä¸€ä¸ªå…¨æ–°çš„ä»»åŠ¡ï¼Œä¹Ÿèƒ½å–å¾—ç›¸å½“ä¸é”™çš„ç»“æœã€‚æ‰€ä»¥ï¼Œç°åœ¨æˆ‘å¸Œæœ›ä½ ç†è§£äº†å¦‚ä½•åœ¨Pythtorchä¸­åº”ç”¨è¿ç§»å­¦ä¹ ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œæˆ‘ä»¬ä¸‹æ¬¡å†è§ã€‚
- en: '![](img/96844ee58fe06a9d803e6f751f732b19_3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96844ee58fe06a9d803e6f751f732b19_3.png)'
