- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P44ï¼šL8.3- Kerasç¥ç»ç½‘ç»œç»“æ„æ­å»ºç»†èŠ‚ä¸è¶…å‚æ•°
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P44ï¼šL8.3- Kerasç¥ç»ç½‘ç»œç»“æ„æ­å»ºç»†èŠ‚ä¸è¶…å‚æ•°
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonã€‚ welcome to applications of deep neural networks with
    Washington Universityã€‚ In this videoï¼Œ we're going to answer a question that I'm
    sure has been bothering you the entire semesterã€‚ How do you know how many layers
    to use in your neural networkã€‚ How do you know how many neurons to use in your
    neural networkã€‚ğŸ˜Šã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å›ç­”ä¸€ä¸ªæˆ‘ç›¸ä¿¡åœ¨æ•´ä¸ªå­¦æœŸé‡Œå›°æ‰°ç€ä½ çš„é—®é¢˜ã€‚ä½ å¦‚ä½•çŸ¥é“åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å¤šå°‘å±‚ï¼Ÿä½ å¦‚ä½•çŸ¥é“åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å¤šå°‘ä¸ªç¥ç»å…ƒï¼ŸğŸ˜Š
- en: This is a big enough question that we're going to spend the next two videos
    talking about it for the latest on my AI course and projects click subscribe and
    the bell next to it to be notified of every new video hyper is's really no easy
    answer to this one As we've gone through the various modules looking at neural
    networks you'll see that we have various layers activation typesã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªè¶³å¤Ÿå¤§çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†èŠ±æ¥ä¸‹æ¥çš„ä¸¤ä¸ªè§†é¢‘æ¥è®¨è®ºã€‚æœ‰å…³æˆ‘AIè¯¾ç¨‹å’Œé¡¹ç›®çš„æœ€æ–°ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»è®¢é˜…å¹¶æŒ‰æ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ã€‚å¯¹æ­¤é—®é¢˜çœŸçš„æ²¡æœ‰ç®€å•çš„ç­”æ¡ˆã€‚é€šè¿‡å„ç§æ¨¡å—æŸ¥çœ‹ç¥ç»ç½‘ç»œï¼Œä½ ä¼šçœ‹åˆ°æˆ‘ä»¬æœ‰å„ç§å±‚å’Œæ¿€æ´»ç±»å‹ã€‚
- en: different ways to do normalization different activation functions all kinds
    of things you can adjust on your neural network neural networks a lot on its own
    those are referred to as are the weights of the neural network However hyper are
    not learned as part of the neural network training it's up to you the neural network
    practitioner to actually specify these So specifying which activation functions
    you want what sort of regularization you want all this kind of thing you need
    to specify as the neural network practitioner Now in the next part we'll see how
    to use something called Bayesian optimization to help us fine tune the neural
    networkã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸åŒçš„æ–¹æ³•è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œä»¥åŠä½ å¯ä»¥åœ¨ç¥ç»ç½‘ç»œä¸Šè°ƒæ•´çš„å„ç§å‚æ•°ã€‚ç¥ç»ç½‘ç»œè‡ªèº«æœ‰å¾ˆå¤šè¿™äº›è¢«ç§°ä¸ºç¥ç»ç½‘ç»œçš„æƒé‡ã€‚ç„¶è€Œï¼Œè¶…å‚æ•°å¹¶ä¸æ˜¯ä½œä¸ºç¥ç»ç½‘ç»œè®­ç»ƒçš„ä¸€éƒ¨åˆ†å­¦ä¹ çš„ï¼Œè€Œæ˜¯ç”±ä½ è¿™ä¸ªç¥ç»ç½‘ç»œä»ä¸šè€…æ¥æŒ‡å®šçš„ã€‚å› æ­¤ï¼Œä½œä¸ºç¥ç»ç½‘ç»œä»ä¸šè€…ï¼Œä½ éœ€è¦æŒ‡å®šä½ æƒ³è¦çš„æ¿€æ´»å‡½æ•°ã€æ­£åˆ™åŒ–ç­‰æ‰€æœ‰è¿™äº›å†…å®¹ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ç§°ä¸ºè´å¶æ–¯ä¼˜åŒ–çš„æ–¹æ³•æ¥å¸®åŠ©æˆ‘ä»¬å¾®è°ƒç¥ç»ç½‘ç»œã€‚
- en: '![](img/6ffb2dddbc647a884212a626611b3856_1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ffb2dddbc647a884212a626611b3856_1.png)'
- en: But even then there's some degree still of decisionï¼Œ at least on your partã€‚
    to decide which hyperparameters you're going to optimize and how you want to optimize
    them in this partã€‚ I'm going to take you through the major hyperparametersã€‚ layer
    types and other things that we've gone through up to this point and if you that
    we havent just so that you're aware of them So Kira's layer as you build up the
    neural networkã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å³ä¾¿å¦‚æ­¤ï¼Œä»ç„¶æœ‰ä¸€å®šç¨‹åº¦çš„å†³å®šæƒï¼Œè‡³å°‘åœ¨ä½ è¿™è¾¹ã€‚è¦å†³å®šä½ è¦ä¼˜åŒ–å“ªäº›è¶…å‚æ•°ä»¥åŠä½ æƒ³å¦‚ä½•ä¼˜åŒ–å®ƒä»¬ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘å°†å¸¦ä½ äº†è§£åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬è®¨è®ºçš„ä¸»è¦è¶…å‚æ•°ã€å±‚ç±»å‹ä»¥åŠå…¶ä»–å†…å®¹ï¼Œå¦‚æœä½ è§‰å¾—æˆ‘ä»¬æ²¡æœ‰æ¶µç›–åˆ°çš„å†…å®¹ï¼Œä½ ä¹Ÿè¦çŸ¥é“ã€‚å› æ­¤ï¼Œåœ¨ä½ æ„å»ºç¥ç»ç½‘ç»œæ—¶ï¼ŒKerasçš„å±‚ã€‚
- en: you'll see that the various examples add different sorts of layers onto the
    neural network listed here are all of the layer types that you can add into that
    sequence that you are building for the neural network and ultimately perhaps putting
    into a model if you're using the functional API activation this is a layer that
    allows you to specify the activation function I don't tend to use this a whole
    lot I tend to specify the activation function as part of the parameters for the
    denses and other types of layers but you can break this outã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šçœ‹åˆ°å„ç§ç¤ºä¾‹å°†ä¸åŒç±»å‹çš„å±‚æ·»åŠ åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™é‡Œåˆ—å‡ºçš„æ˜¯ä½ å¯ä»¥åœ¨æ„å»ºçš„åºåˆ—ä¸­æ·»åŠ çš„æ‰€æœ‰å±‚ç±»å‹ï¼Œæœ€ç»ˆæˆ–è®¸æ”¾å…¥æ¨¡å‹ä¸­ï¼Œå¦‚æœä½ ä½¿ç”¨åŠŸèƒ½æ€§APIæ¿€æ´»ï¼Œè¿™æ˜¯ä¸€ä¸ªå…è®¸ä½ æŒ‡å®šæ¿€æ´»å‡½æ•°çš„å±‚ã€‚æˆ‘é€šå¸¸ä¸å¤ªä½¿ç”¨è¿™ä¸ªå±‚ï¼Œè€Œæ˜¯å€¾å‘äºå°†æ¿€æ´»å‡½æ•°ä½œä¸ºå…¨è¿æ¥å±‚å’Œå…¶ä»–å±‚ç±»å‹å‚æ•°çš„ä¸€éƒ¨åˆ†æ¥æŒ‡å®šï¼Œä½†ä½ å¯ä»¥å°†å…¶åˆ†å¼€ã€‚
- en: The activation comes just after a layer activity regularizationã€‚ This allows
    you to add L1 and L2 outside of a layerã€‚ Againï¼Œ L1 and L2ã€‚ as far as activity
    and kernel can be added to the individual layer types such as denseã€‚ but you can
    specify this outside of the layer as wellã€‚ if you find that to be more express
    itã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‘ç”Ÿåœ¨å±‚æ´»åŠ¨æ­£åˆ™åŒ–ä¹‹åã€‚è¿™å…è®¸ä½ åœ¨å±‚ä¹‹å¤–æ·»åŠ L1å’ŒL2ã€‚å†è¯´ä¸€éï¼ŒL1å’ŒL2ï¼Œä½œä¸ºæ´»åŠ¨å’Œå†…æ ¸å¯ä»¥æ·»åŠ åˆ°è¯¸å¦‚å…¨è¿æ¥å±‚ç­‰å•ç‹¬çš„å±‚ç±»å‹ä¸­ï¼Œä½†å¦‚æœä½ å‘ç°å°†å…¶æ”¾åœ¨å±‚ä¹‹å¤–æ›´æ–¹ä¾¿ï¼Œä¹Ÿå¯ä»¥è¿™ä¹ˆåšã€‚
- en: The dense layerã€‚ This is the workhorse of tabular data for sureã€‚ this is mostly
    what your neural network is made up of if you're doing tabular dataã€‚ If you're
    doing images and other thingsï¼Œ you'll still have dense layersã€‚ If you're dealing
    with non dense layers like the LSTM layers and the convolution layersã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯†é›†å±‚ã€‚è¿™æ— ç–‘æ˜¯è¡¨æ ¼æ•°æ®çš„ä¸»åŠ›å†›ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä½ ç¥ç»ç½‘ç»œçš„æ„æˆéƒ¨åˆ†ï¼Œå¦‚æœä½ å¤„ç†è¡¨æ ¼æ•°æ®ã€‚å¦‚æœä½ å¤„ç†å›¾åƒå’Œå…¶ä»–äº‹ç‰©ï¼Œä»ç„¶ä¼šæœ‰å¯†é›†å±‚ã€‚å¦‚æœä½ å¤„ç†éå¯†é›†å±‚ï¼Œå¦‚LSTMå±‚å’Œå·ç§¯å±‚ã€‚
- en: you'll typically have to flattenã€‚ That's a layer type down hereã€‚ before you
    feed things into the dense layerã€‚ Dropout layer allows you to use dropout regularizationã€‚
    you specify a percentage usually relatively lowã€‚ maybe 5ï¼Œ10ï¼Œ20%ã€‚ that is theã€‚ğŸ˜Šã€‚Of
    neurons in that layer that are randomly disabled during trainingã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ é€šå¸¸éœ€è¦å…ˆæ‰å¹³åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªå±‚ç±»å‹ï¼Œåœ¨å°†æ•°æ®è¾“å…¥å¯†é›†å±‚ä¹‹å‰ã€‚ä¸¢å¼ƒå±‚å…è®¸ä½ ä½¿ç”¨ä¸¢å¼ƒæ­£åˆ™åŒ–ã€‚ä½ é€šå¸¸æŒ‡å®šä¸€ä¸ªç›¸å¯¹è¾ƒä½çš„ç™¾åˆ†æ¯”ï¼Œå¯èƒ½æ˜¯5%ï¼Œ10%ï¼Œ20%ã€‚è¿™å°±æ˜¯åœ¨è®­ç»ƒæœŸé—´éšæœºç¦ç”¨è¯¥å±‚ä¸­ç¥ç»å…ƒçš„ç™¾åˆ†æ¯”ã€‚
- en: Dropout has no effect when you're actually running the neural networkã€‚ when
    you're scoring the percent of neurons that you're going to drop out is not a learn
    parameterã€‚ You have to specify that when you're building up your neural networkã€‚
    Flten is used to usually used right before dense layer or right before the output
    that is crunchingã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…è¿è¡Œç¥ç»ç½‘ç»œæ—¶ï¼Œä¸¢å¼ƒæ²¡æœ‰æ•ˆæœã€‚å½“ä½ è®¡ç®—è¦ä¸¢å¼ƒçš„ç¥ç»å…ƒç™¾åˆ†æ¯”æ—¶ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚ä½ å¿…é¡»åœ¨æ„å»ºç¥ç»ç½‘ç»œæ—¶æŒ‡å®šè¿™ä¸€ç‚¹ã€‚Flattené€šå¸¸åœ¨å¯†é›†å±‚ä¹‹å‰æˆ–åœ¨è¾“å‡ºä¹‹å‰ä½¿ç”¨ï¼Œç”¨äºå°†å¤šç»´å¼ é‡ï¼ˆä¸æ˜¯å‘é‡ï¼Œå¦‚çŸ©é˜µå’Œç«‹æ–¹ä½“ï¼‰å‹ç¼©ä¸ºå¯ä»¥ä¼ é€’ç»™ä¸è®¾è®¡å¤„ç†å¤šç»´è¾“å…¥çš„å±‚çš„å‘é‡ã€‚
- en: the multidimensional tensors that are not vectors like the matrices and cubes
    down to a vector that can be passed into a layer that's not designed to handle
    something more than one dimensionã€‚ input is the input layer that is where data
    comes into your neural networkã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å±‚æ˜¯æ•°æ®è¿›å…¥ç¥ç»ç½‘ç»œçš„åœ°æ–¹ã€‚
- en: This can be specified as a parameter for the dense or other layersã€‚Lambda layersã€‚
    I really haven't used these a great dealï¼Œ but you can basically specify a Python
    Lambda function to do some transformation to the data as it goes through masking
    layersã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥ä½œä¸ºå‚æ•°ä¸ºå¯†é›†å±‚æˆ–å…¶ä»–å±‚æŒ‡å®šã€‚Lambdaå±‚ã€‚æˆ‘å®é™…ä¸Šæ²¡æœ‰å¤ªå¤šä½¿ç”¨è¿™äº›ï¼Œä½†ä½ åŸºæœ¬ä¸Šå¯ä»¥æŒ‡å®šä¸€ä¸ªPython Lambdaå‡½æ•°ï¼Œå¯¹æ•°æ®è¿›è¡Œä¸€äº›è½¬æ¢ï¼Œé€šè¿‡æ©ç å±‚ã€‚
- en: Againï¼Œ this is something I have not used and all really in in caresã€‚ but it
    is available It's dealing with time stepssã€‚ I'm guessing this has some utility
    when you're dealing with LSTms in recurrent neural networksã€‚ same thing for the
    permuteã€‚ that can be used to add some randomization and changing to a given patternã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®é™…ä¸Šæ˜¯æˆ‘æ²¡æœ‰ä½¿ç”¨è¿‡çš„ä¸œè¥¿ï¼Œæ‰€æœ‰çš„ç¡®åˆ‡ä¿¡æ¯æˆ‘ä¹Ÿä¸å¤ªæ¸…æ¥šã€‚ä½†å®ƒæ˜¯å¯ç”¨çš„ï¼Œå¤„ç†æ—¶é—´æ­¥ã€‚æˆ‘çŒœè¿™åœ¨å¤„ç†é€’å½’ç¥ç»ç½‘ç»œä¸­çš„LSTMæ—¶ä¼šæœ‰ä¸€äº›ç”¨å¤„ã€‚å¯¹äºæ’åˆ—ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå¯ä»¥ç”¨äºä¸ºç»™å®šæ¨¡å¼æ·»åŠ ä¸€äº›éšæœºæ€§å’Œå˜åŒ–ã€‚
- en: I'm not entirely sure if the permuteã€‚ I don't believe the permute is randomã€‚
    So like I just saidã€‚ permuteã€‚ that's a way to change around the dimensions of
    a layer as it's going throughã€‚ This works pretty similar to reshapeã€‚ I usually
    use reshape rather than permuteã€‚ I have not used permute a great dealã€‚ repeat
    vector is a way to duplicate thingsã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸å¤ªç¡®å®šæ’åˆ—æ˜¯å¦éšæœºã€‚æˆ‘è®¤ä¸ºæ’åˆ—ä¸æ˜¯éšæœºçš„ã€‚å°±åƒæˆ‘åˆšæ‰è¯´çš„ï¼Œæ’åˆ—æ˜¯ä¸€ç§åœ¨å±‚å¤„ç†æ—¶æ”¹å˜ç»´åº¦çš„æ–¹å¼ã€‚è¿™ä¸é‡å¡‘å·¥ä½œåŸç†éå¸¸ç›¸ä¼¼ã€‚æˆ‘é€šå¸¸ä½¿ç”¨é‡å¡‘è€Œä¸æ˜¯æ’åˆ—ã€‚æˆ‘æ²¡æœ‰å¤§é‡ä½¿ç”¨æ’åˆ—ï¼Œé‡å¤å‘é‡æ˜¯ä¸€ç§å¤åˆ¶äº‹ç‰©çš„æ–¹å¼ã€‚
- en: have not used that a great dealã€‚ reshapeã€‚ I've used that a great deal that lets
    you change the structure of things as they're going throughã€‚ğŸ˜Šï¼ŒFor exampleï¼Œ you
    had the RGB values in different orders than the neural network expected Permute
    would probably be a very good choice for that reshape is just more changing what
    the actual shape is without necessarily ordering things too much spatial dropout
    is dropout used for convolution layers I have not used these a great deal activation
    functionsã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ²¡æœ‰å¤§é‡ä½¿ç”¨é‚£ç§ã€‚é‡å¡‘ã€‚æˆ‘ä½¿ç”¨å¾—å¾ˆå¤šï¼Œè¿™è®©ä½ åœ¨æ•°æ®å¤„ç†æ—¶æ”¹å˜äº‹ç‰©çš„ç»“æ„ã€‚ğŸ˜Šä¾‹å¦‚ï¼Œä½ çš„RGBå€¼çš„é¡ºåºä¸ç¥ç»ç½‘ç»œé¢„æœŸçš„ä¸åŒï¼Œæ’åˆ—å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼Œè€Œé‡å¡‘åªæ˜¯æ”¹å˜å®é™…å½¢çŠ¶ï¼Œè€Œä¸ä¸€å®šè¿‡äºå…³æ³¨é¡ºåºã€‚ç©ºé—´ä¸¢å¼ƒæ˜¯ç”¨äºå·ç§¯å±‚çš„ä¸¢å¼ƒï¼Œæˆ‘æ²¡æœ‰å¤§é‡ä½¿ç”¨è¿™äº›æ¿€æ´»å‡½æ•°ã€‚
- en: very important topic in neural networks The thing to remember with activation
    functions is there is a lot of history here neural networks have been around for
    a long time and various activation functions have beenã€‚Added over the yearsï¼Œ some
    of these are not used all that much in the modern era of deep learningã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°åœ¨ç¥ç»ç½‘ç»œä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„è¯é¢˜ã€‚éœ€è¦è®°ä½çš„æ˜¯ï¼Œæ¿€æ´»å‡½æ•°æœ‰å¾ˆå¤šå†å²ï¼Œç¥ç»ç½‘ç»œå­˜åœ¨å·²ä¹…ï¼Œå„ç§æ¿€æ´»å‡½æ•°åœ¨è¿™äº›å¹´é‡Œè¢«æ·»åŠ åˆ°ç½‘ç»œä¸­ï¼Œå…¶ä¸­ä¸€äº›åœ¨ç°ä»£æ·±åº¦å­¦ä¹ ä¸­å¹¶ä¸å¸¸ç”¨ã€‚
- en: You'll also notice that I have advanced activation functions down here as wellã€‚
    advanced activation functions are ones that are actually modified as the fit occursã€‚
    The regular activation functionsï¼Œ they are just part of the training processã€‚
    but the training process is not affecting any parameters inside of the actual
    activation functionã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜ä¼šæ³¨æ„åˆ°æˆ‘åœ¨è¿™é‡Œæœ‰å…ˆè¿›çš„æ¿€æ´»å‡½æ•°ã€‚å…ˆè¿›çš„æ¿€æ´»å‡½æ•°æ˜¯åœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­å®é™…ä¸Šè¢«ä¿®æ”¹çš„å‡½æ•°ã€‚å¸¸è§„æ¿€æ´»å‡½æ•°ä»…æ˜¯è®­ç»ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œä½†è®­ç»ƒè¿‡ç¨‹å¹¶ä¸ä¼šå½±å“å®é™…æ¿€æ´»å‡½æ•°å†…éƒ¨çš„ä»»ä½•å‚æ•°ã€‚
- en: So the softm activation functionï¼Œ you typically see that on the output layer
    of a multiclass classificationã€‚ This ensures that all of the outputs from the
    neural networkï¼Œ some to oneã€‚ So they're essentially probabilities not at a lot
    of tuning hereã€‚ either you're using softmax or you're notã€‚ if you're using multiclass
    classificationã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ softm æ¿€æ´»å‡½æ•°é€šå¸¸åœ¨å¤šç±»åˆ†ç±»çš„è¾“å‡ºå±‚ä¸Šçœ‹åˆ°ã€‚è¿™ç¡®ä¿äº†æ¥è‡ªç¥ç»ç½‘ç»œçš„æ‰€æœ‰è¾“å‡ºæ€»å’Œä¸ºä¸€ã€‚å› æ­¤ï¼Œå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯æ¦‚ç‡ï¼Œè¿™é‡Œæ²¡æœ‰å¤ªå¤šè°ƒæ•´ã€‚è¦ä¹ˆä½ åœ¨ä½¿ç”¨
    softmaxï¼Œè¦ä¹ˆä½ ä¸åœ¨ä½¿ç”¨ï¼Œå¦‚æœä½ åœ¨ä½¿ç”¨å¤šç±»åˆ†ç±»ã€‚
- en: likely you're using softmã€‚ Nowï¼Œ A lot of these come from various papers that
    are being reproduced with care as an EU as one of thoseã€‚ That's the exponential
    linear unit I've seenã€‚That in a couple of papers with Gs and other things I have
    not worked with these a great dealã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¯èƒ½ä½ åœ¨ä½¿ç”¨ softmã€‚ç°åœ¨ï¼Œå¾ˆå¤šè¿™äº›æ¥è‡ªå„ç§è¢«ç»†è‡´å¤åˆ¶çš„è®ºæ–‡ï¼ŒEU æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚è¿™æ˜¯æˆ‘è§è¿‡çš„æŒ‡æ•°çº¿æ€§å•å…ƒã€‚æˆ‘åœ¨ä¸€äº›å…³äº Gs å’Œå…¶ä»–å†…å®¹çš„è®ºæ–‡ä¸­çœ‹åˆ°äº†å®ƒä»¬ï¼Œæˆ‘æ²¡æœ‰å¤ªå¤šæ¥è§¦è¿™äº›ã€‚
- en: something that I'm meaning to work with moreã€‚ they do seem to potentially in
    theory anywayã€‚ have some advantages perhaps over re scaled exponential unit is
    similar to ELU except it's scaled by a numeric value that is another hyperparameter
    that you have to specify by what you want to to scale it by scaling factors simply
    multiplied by the SL So plus this is more of a historic one I have not seen this
    one used a lot in later papers not to say it isn't same thing for soft sign rectified
    linear unit Rlu that is absolutely the workhorse of deep learning other than perhaps
    leaky re which receive further down hyperbolic tangent not used a whole lotã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ‰“ç®—æ›´å¤šåœ°ç ”ç©¶è¿™äº›ã€‚å®ƒä»¬åœ¨ç†è®ºä¸Šä¼¼ä¹ç¡®å®æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œå¯èƒ½åœ¨é‡ç¼©æ”¾çš„æŒ‡æ•°å•å…ƒæ–¹é¢ç±»ä¼¼äº ELUï¼Œåªæ˜¯é€šè¿‡ä¸€ä¸ªæ•°å€¼å€¼è¿›è¡Œç¼©æ”¾ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªè¶…å‚æ•°ï¼Œä½ éœ€è¦æŒ‡å®šä½ æƒ³è¦ç¼©æ”¾çš„å› å­ã€‚ç¼©æ”¾å› å­ç®€å•åœ°ä¹˜ä»¥
    SLï¼Œå› æ­¤åŠ ä¸Šè¿™ä¸ªæ˜¯ä¸€ä¸ªå†å²æ€§çš„å‡½æ•°ï¼Œæˆ‘æ²¡æœ‰åœ¨åæ¥çš„è®ºæ–‡ä¸­çœ‹åˆ°å®ƒå¾ˆå¤šä½¿ç”¨ï¼Œsoft sign ä¿®æ­£çº¿æ€§å•å…ƒ Rlu ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œç»å¯¹æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸»åŠ›å†›ï¼Œé™¤äº†å¯èƒ½æ˜¯æ³„æ¼çš„
    reï¼Œåœ¨åé¢æ˜¯åŒæ›²æ­£åˆ‡ï¼Œä½¿ç”¨å¾—ä¸å¤šã€‚
- en: I see those in LSTMs and they were very popular in hidden layers before RE came
    on strong Sigmoidã€‚ same thingã€‚Common in classic neural networks for hidden layers
    not used a whole lot in hidden layers in modern neural networksã€‚ Howeverï¼Œ sigmoid
    is still very common for a logistic regression output neural networkã€‚ which is
    basically a binary classifier classifying between two things hard sigmoid I haven't
    seen this used a lot recentlyã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ LSTM ä¸­çœ‹åˆ°äº†è¿™äº›ï¼Œåœ¨ RE å¼ºåŠ²å‡ºç°ä¹‹å‰ï¼Œå®ƒä»¬åœ¨éšå±‚ä¸­éå¸¸å—æ¬¢è¿ï¼Œsigmoid ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç»å…¸ç¥ç»ç½‘ç»œçš„éšå±‚ä¸­å¸¸è§ï¼Œä½†åœ¨ç°ä»£ç¥ç»ç½‘ç»œçš„éšå±‚ä¸­ä¸å¸¸ç”¨ã€‚ç„¶è€Œï¼Œsigmoid
    åœ¨é€»è¾‘å›å½’è¾“å‡ºç¥ç»ç½‘ç»œä¸­ä»ç„¶éå¸¸å¸¸è§ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªåœ¨ä¸¤è€…ä¹‹é—´åˆ†ç±»çš„äºŒåˆ†ç±»å™¨ï¼Œhard sigmoid æˆ‘æœ€è¿‘æ²¡æœ‰çœ‹åˆ°å¾ˆå¤šä½¿ç”¨ã€‚
- en: it works just like the sigmoid functionï¼Œ the sigmoid function is expensive to
    calculate because there's two EP functions in itã€‚ the hard sigmoid is more of
    an approximation of the sigmoid of the logisticã€‚ So it's potentially cheaper to
    computeã€‚ maybe on mobile devicesï¼Œ this might be a good ideaã€‚ I have not dealt
    with it a great deal exponentialã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçš„å·¥ä½œæ–¹å¼å°±åƒ sigmoid å‡½æ•°ï¼Œsigmoid å‡½æ•°è®¡ç®—å¼€é”€å¾ˆå¤§ï¼Œå› ä¸ºå…¶ä¸­æœ‰ä¸¤ä¸ª EP å‡½æ•°ã€‚hard sigmoid æ›´åƒæ˜¯ logistic
    çš„ sigmoid çš„ä¸€ç§è¿‘ä¼¼ã€‚å› æ­¤ï¼Œå®ƒåœ¨è®¡ç®—ä¸Šå¯èƒ½æ›´ä¾¿å®œã€‚ä¹Ÿè®¸åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šï¼Œè¿™å¯èƒ½æ˜¯ä¸ªå¥½ä¸»æ„ã€‚æˆ‘æ²¡æœ‰æ·±å…¥å¤„ç†è¿™ä¸ªæŒ‡æ•°ã€‚
- en: This is another one that I have not worked with a great dealï¼Œ but it's a base
    Eactation functionã€‚ at least according to the documentation in Kis linearã€‚ This
    is used a lotã€‚ particularly for regression neural networks on the output layer
    I don't think you would necessarily see it anywhere other than the output layer
    unlessã€‚Just doing something very custom and you need to basically pass through
    the activation function on something so that it does not make any changesã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦ä¸€ä¸ªæˆ‘æ²¡æœ‰å¤ªå¤šæ¥è§¦çš„ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªåŸºæœ¬çš„ Eactation å‡½æ•°ã€‚æ ¹æ®æ–‡æ¡£ï¼Œè¿™ä¸ªå‡½æ•°æ˜¯çº¿æ€§çš„ã€‚è¿™åœ¨å›å½’ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ä¸­ä½¿ç”¨å¾—å¾ˆå¤šï¼Œé™¤éä½ æ­£åœ¨åšä¸€äº›éå¸¸è‡ªå®šä¹‰çš„äº‹æƒ…ï¼Œå¦åˆ™æˆ‘è®¤ä¸ºä½ ä¸å¤ªå¯èƒ½åœ¨è¾“å‡ºå±‚ä»¥å¤–çš„åœ°æ–¹çœ‹åˆ°å®ƒã€‚åŸºæœ¬ä¸Šï¼Œä½ éœ€è¦é€šè¿‡æŸä¸ªä¸œè¥¿ä¼ é€’æ¿€æ´»å‡½æ•°ï¼Œä»¥ç¡®ä¿å®ƒä¸åšä»»ä½•æ›´æ”¹ã€‚
- en: leaky re is a variant of of it allows a very small gradient So it has some special
    rules on the gradients during training This is a very popular one I've used it
    to some degree have not really seen a great deal of improvement within it in all
    cases Preu I I have had some good results with this's there's an alpha term in
    the leaky re that you normally have to specify the pre you can actually learn
    that So that's that's a good thing L1 L2 and dropout I don't see L1 and L2 in
    practice used a great deal in deep learning L2 for sure L1 not as much for especially
    in vision networks where I don't think you really want to drop inputs like drop
    an individual pixel so I don't see L1 used a great dealã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU æ˜¯å…¶å˜ä½“ï¼Œå®ƒå…è®¸éå¸¸å°çš„æ¢¯åº¦ï¼Œå› æ­¤åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¢¯åº¦æœ‰ä¸€äº›ç‰¹æ®Šè§„åˆ™ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„é€‰æ‹©ï¼Œæˆ‘åœ¨æŸç§ç¨‹åº¦ä¸Šä½¿ç”¨è¿‡ï¼Œä½†åœ¨æ‰€æœ‰æƒ…å†µä¸‹å¹¶æ²¡æœ‰çœ‹åˆ°æ˜¾è‘—çš„æ”¹å–„ã€‚Preuï¼Œæˆ‘åœ¨è¿™æ–¹é¢æœ‰ä¸€äº›ä¸é”™çš„ç»“æœï¼Œleaky
    ReLU ä¸­æœ‰ä¸€ä¸ª alpha é¡¹ï¼Œä½ é€šå¸¸éœ€è¦æŒ‡å®šï¼Œä½†ä½ å¯ä»¥å®é™…ä¸Šå­¦ä¹ åˆ°è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä»¶å¥½äº‹ã€‚L1ã€L2 å’Œ dropoutï¼Œæˆ‘åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¸å¤ªçœ‹åˆ°
    L1 å’Œ L2 çš„å¹¿æ³›ä½¿ç”¨ï¼ŒL2 æ˜¯è‚¯å®šçš„ï¼ŒL1 åˆ™ä¸é‚£ä¹ˆå¤šï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰ç½‘ç»œä¸­ï¼Œæˆ‘è®¤ä¸ºä½ å¹¶ä¸æƒ³ä¸¢å¼ƒè¾“å…¥ï¼Œæ¯”å¦‚ä¸¢å¼ƒå•ä¸ªåƒç´ ï¼Œæ‰€ä»¥æˆ‘å¹¶ä¸ç»å¸¸çœ‹åˆ° L1 çš„ä½¿ç”¨ã€‚
- en: L2 might have some value but dropout is really the workhorse of these deep neural
    networks the papers that I read in particular you can figure out the dropout percentage
    using some of the Bayesian optimization that I will show you in the next part
    batch normalization I have been playing with that to some degree and have been
    using it more and more it's pretty handy it can typically in practice at least
    I've observed allow me to increase my learning rate to values that would have
    caused instability in the past So if you set your learning rate too high typically
    you will cause overflows in some of the back propagation steps and even forward
    propagation steps depending on your activation functions so that will cause NANs
    not in numbers and pretty much collapse your training So batch normalization can
    be quite useful for that and also as a combat to the vanishing gradient problem
    where your gradientsã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: L2 å¯èƒ½æœ‰ä¸€äº›ä»·å€¼ï¼Œä½† dropout å®é™…ä¸Šæ˜¯è¿™äº›æ·±åº¦ç¥ç»ç½‘ç»œçš„ä¸»åŠ›ã€‚åœ¨æˆ‘é˜…è¯»çš„è®ºæ–‡ä¸­ï¼Œä½ å¯ä»¥åˆ©ç”¨ä¸€äº›è´å¶æ–¯ä¼˜åŒ–æ¥è®¡ç®— dropout ç™¾åˆ†æ¯”ï¼Œæˆ‘å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†å±•ç¤ºã€‚æ‰¹é‡å½’ä¸€åŒ–ï¼Œæˆ‘åœ¨æŸç§ç¨‹åº¦ä¸Šä¹Ÿåœ¨ç©è¿™ä¸ªï¼Œå¹¶ä¸”è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨å®ƒï¼Œå®ƒéå¸¸æ–¹ä¾¿ï¼Œé€šå¸¸åœ¨å®è·µä¸­ï¼Œè‡³å°‘æˆ‘è§‚å¯Ÿåˆ°ï¼Œå¯ä»¥è®©æˆ‘å°†å­¦ä¹ ç‡æé«˜åˆ°è¿‡å»ä¼šå¯¼è‡´ä¸ç¨³å®šçš„å€¼ã€‚å› æ­¤ï¼Œå¦‚æœä½ å°†å­¦ä¹ ç‡è®¾ç½®å¾—è¿‡é«˜ï¼Œé€šå¸¸ä¼šåœ¨ä¸€äº›åå‘ä¼ æ’­æ­¥éª¤ç”šè‡³å‰å‘ä¼ æ’­æ­¥éª¤ä¸­å¯¼è‡´æº¢å‡ºï¼Œè¿™å–å†³äºä½ çš„æ¿€æ´»å‡½æ•°ï¼Œè¿™ä¼šå¯¼è‡´
    NANsï¼ˆéæ•°å­—ï¼‰ï¼Œå‡ ä¹ä¼šä½¿ä½ çš„è®­ç»ƒå´©æºƒã€‚å› æ­¤ï¼Œæ‰¹é‡å½’ä¸€åŒ–å¯¹æ­¤éå¸¸æœ‰ç”¨ï¼Œä¹Ÿæ˜¯è§£å†³æ¶ˆå¤±æ¢¯åº¦é—®é¢˜çš„ä¸€ç§æ–¹æ³•ã€‚
- en: to go to zero and nothingï¼Œ nothing learns you've also got various training parameters
    that you need to make use of in particular batch size and learning rateã€‚ mini
    batches are very popular in deep learningã€‚ So usually this value is 32 or lower
    for the batch size learning rate also also usually pretty small unless you're
    using batch normalizationã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå­¦ä¹ ç‡é™åˆ°é›¶ï¼Œä»€ä¹ˆä¹Ÿå­¦ä¸åˆ°ã€‚ä½ è¿˜éœ€è¦åˆ©ç”¨å„ç§è®­ç»ƒå‚æ•°ï¼Œç‰¹åˆ«æ˜¯æ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡ã€‚å°æ‰¹é‡åœ¨æ·±åº¦å­¦ä¹ ä¸­éå¸¸æµè¡Œã€‚å› æ­¤ï¼Œé€šå¸¸è¿™ä¸ªå€¼æ˜¯ 32 æˆ–æ›´ä½ï¼Œè€Œå­¦ä¹ ç‡é€šå¸¸ä¹Ÿæ¯”è¾ƒå°ï¼Œé™¤éä½ åœ¨ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–ã€‚
- en: but you need to kind of play with the learning rate and batch normalization
    toã€‚See how small you can to see how small it's necessary to make the learning
    rate Learn rate too small will cause the neural network to simply not learn or
    too high will cause your neural network to quickly destabilizedã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ éœ€è¦åœ¨å­¦ä¹ ç‡å’Œæ‰¹é‡å½’ä¸€åŒ–ä¹‹é—´è¿›è¡Œä¸€å®šçš„è°ƒæ•´ï¼Œä»¥æŸ¥çœ‹å­¦ä¹ ç‡éœ€è¦å¤šå°ã€‚å­¦ä¹ ç‡è¿‡å°ä¼šå¯¼è‡´ç¥ç»ç½‘ç»œæ ¹æœ¬æ— æ³•å­¦ä¹ ï¼Œè¿‡é«˜åˆ™ä¼šå¯¼è‡´ç¥ç»ç½‘ç»œè¿…é€Ÿä¸ç¨³å®šã€‚
- en: So if you see the neural network reporting NA AN for the error functionã€‚ it's
    usually because your learning rate is too highã€‚ I almost always specify learning
    rates as powers of 101 E negative 10ï¼Œ soã€‚10 to the power of1 times 10 to the power
    of negative3 in that caseã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ çœ‹åˆ°ç¥ç»ç½‘ç»œçš„è¯¯å·®å‡½æ•°æŠ¥å‘Šä¸º NA ANï¼Œé€šå¸¸æ˜¯å› ä¸ºä½ çš„å­¦ä¹ ç‡å¤ªé«˜ã€‚æˆ‘å‡ ä¹æ€»æ˜¯å°†å­¦ä¹ ç‡æŒ‡å®šä¸º 10 çš„è´Ÿ 10 æ¬¡å¹‚ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ 10
    çš„ 1 æ¬¡å¹‚ä¹˜ä»¥ 10 çš„è´Ÿ 3 æ¬¡å¹‚ã€‚
- en: Now this function that I have hereï¼Œ we will make more use of theseã€‚ We will
    make a lot more use of this in the next sectionã€‚ but let's go ahead and just do
    a quick look at how we can try out some of these hyperparameterã€‚ I'm going go
    ahead and run this that loads a classification job that I want to doã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘è¿™é‡Œçš„è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†æ›´å¤šåœ°ä½¿ç”¨è¿™äº›ã€‚è®©æˆ‘ä»¬å¿«é€Ÿçœ‹çœ‹å¦‚ä½•å°è¯•ä¸€äº›è¶…å‚æ•°ã€‚æˆ‘å°†è¿è¡Œè¿™ä¸ªï¼Œå®ƒä¼šåŠ è½½æˆ‘æƒ³è¦åšçš„åˆ†ç±»ä»»åŠ¡ã€‚
- en: it's using that simple data set that we looked at beforeã€‚ This evaluates the
    neural networkã€‚ Now this is using bootstrapping like we saw beforeã€‚ splits is
    how many bootstrapping splits we actually wantã€‚ I'm using two just so that it
    runs relatively quicklyï¼Œ but you can experiment with thatã€‚ againã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ç®€å•æ•°æ®é›†ã€‚è¿™è¯„ä¼°äº†ç¥ç»ç½‘ç»œã€‚ç°åœ¨è¿™æ˜¯ä½¿ç”¨è‡ªåŠ©æ³•ï¼Œå°±åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ã€‚splitsæ˜¯æˆ‘ä»¬å®é™…æƒ³è¦å¤šå°‘ä¸ªè‡ªåŠ©æ³•åˆ†å‰²ã€‚æˆ‘ä½¿ç”¨ä¸¤ä¸ªï¼Œè¿™æ ·è¿è¡Œç›¸å¯¹å¿«é€Ÿï¼Œä½†ä½ å¯ä»¥å¯¹æ­¤è¿›è¡Œå®éªŒã€‚
- en: when we're tuning hyperparameterï¼Œ sometimes there's parameters usually not very
    many that go into tuning the hyperparameters or almost hyperparameter squaresã€‚
    but that would be one of them that you would need to set as you're searchingã€‚
    This is another oneã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒæ•´è¶…å‚æ•°æ—¶ï¼Œæœ‰æ—¶ä¼šæœ‰ä¸€äº›å‚æ•°ï¼Œé€šå¸¸ä¸å¤ªå¤šï¼Œæ¶‰åŠè¶…å‚æ•°çš„è°ƒæ•´ï¼Œæˆ–è€…å‡ ä¹æ˜¯è¶…å‚æ•°çš„å¹³æ–¹ã€‚ä½†è¿™å°†æ˜¯ä½ åœ¨æœç´¢æ—¶éœ€è¦è®¾ç½®çš„ä¸€ä¸ªå‚æ•°ã€‚è¿™æ˜¯å¦ä¸€ä¸ªã€‚
- en: So what I'm basically doing here is allowing these values to beã€‚As I'm trying
    to make the very complex neural network hyperparameters numeric and a vectorã€‚
    So dropout would be the first value of the four that we're tuning at onceã€‚ Learn
    rate is the learning rate for the neural networkã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™é‡Œåšçš„åŸºæœ¬ä¸Šæ˜¯å…è®¸è¿™äº›å€¼å­˜åœ¨ã€‚æˆ‘è¯•å›¾å°†éå¸¸å¤æ‚çš„ç¥ç»ç½‘ç»œè¶…å‚æ•°è½¬åŒ–ä¸ºæ•°å€¼å’Œå‘é‡ã€‚å› æ­¤ï¼Œdropoutå°†æ˜¯æˆ‘ä»¬ä¸€æ¬¡è°ƒä¼˜çš„å››ä¸ªå€¼ä¸­çš„ç¬¬ä¸€ä¸ªã€‚Learn
    rateæ˜¯ç¥ç»ç½‘ç»œçš„å­¦ä¹ ç‡ã€‚
- en: neuron percent is just what percentage of these 500 do we want to use and neuron
    shrink is what should the layer shrink by each timeã€‚ So we start out with however
    many neuron percent we have for that neuron countã€‚ and then we shrink it by that
    value each timeã€‚ So it could be like 0ã€‚8ã€‚ if you want to use 80% on your next
    oneã€‚ And you can see here that it basically builds up so long as each new layer
    has a neuron count of 25 and is less than 10 Good run this So does's goingã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: neuron percentä»…ä»…æ˜¯æˆ‘ä»¬æƒ³ä½¿ç”¨çš„è¿™500ä¸ªä¸­çš„ç™¾åˆ†æ¯”ï¼Œneuron shrinkæ˜¯æ¯æ¬¡è¯¥å±‚åº”è¯¥æ”¶ç¼©çš„å€¼ã€‚å› æ­¤æˆ‘ä»¬ä»è¯¥ç¥ç»å…ƒè®¡æ•°çš„neuron
    percentå¼€å§‹ï¼Œç„¶åæ¯æ¬¡æŒ‰è¯¥å€¼æ”¶ç¼©ã€‚æ‰€ä»¥å¦‚æœä½ æƒ³åœ¨ä¸‹ä¸€ä¸ªä¸­ä½¿ç”¨80%ï¼Œå¯ä»¥è®¾ä¸º0.8ã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œåªè¦æ¯ä¸ªæ–°å±‚çš„ç¥ç»å…ƒè®¡æ•°ä¸º25ä¸”å°äº10ï¼Œå°±ä¼šæŒç»­æ„å»ºã€‚
- en: It takes a few moments to actually score and it will be made use of in the next
    part because believe me  for Bayesian optimizationã€‚ It's very valuable to have
    your neural network hyperpara is just one vectorã€‚ You put quite a few additional
    hyperparameters in here And if you're dealing with a vision networkã€‚probably want
    to do convolution layer count and other things so this is still running I'll go
    ahead and oh just finished ignore that error I will fix that that simply was trying
    to time how long it took this is a log loss of 0ã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…è¯„åˆ†éœ€è¦å‡ åˆ†é’Ÿï¼Œå¹¶å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä½¿ç”¨ï¼Œå› ä¸ºç›¸ä¿¡æˆ‘ï¼Œè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–æ—¶ï¼Œå°†ç¥ç»ç½‘ç»œè¶…å‚æ•°ä½œä¸ºä¸€ä¸ªå‘é‡æ˜¯éå¸¸æœ‰ä»·å€¼çš„ã€‚ä½ åœ¨è¿™é‡Œæ”¾å…¥äº†ç›¸å½“å¤šçš„é¢å¤–è¶…å‚æ•°ï¼Œå¦‚æœä½ å¤„ç†çš„æ˜¯è§†è§‰ç½‘ç»œï¼Œå¯èƒ½è¿˜æƒ³åšå·ç§¯å±‚è®¡æ•°ç­‰ï¼Œå› æ­¤è¿™ä»åœ¨è¿è¡Œï¼Œæˆ‘ä¼šç»§ç»­ï¼Œå“¦ï¼Œåˆšå®Œæˆï¼Œå¿½ç•¥é‚£ä¸ªé”™è¯¯ï¼Œæˆ‘ä¼šä¿®å¤ï¼Œå®ƒåªæ˜¯å°è¯•è®°å½•è¿™æ‰€éœ€çš„æ—¶é—´ï¼Œè¿™æ˜¯ä¸€ä¸ª0çš„æ—¥å¿—æŸå¤±ã€‚
- en: 7 and we'll see that in the next part we actually tune that to get down to 0ã€‚59
    which is a much better log loss ignore the negatives I'll explain in the next
    part what that's necessary that's just so that we can optimize it because it wants
    to maximize the number and you don't try to maximize a log loss you try to minimize
    itã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 7ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†çœ‹åˆ°ï¼Œå®é™…ä¸Šæˆ‘ä»¬ä¼šè°ƒæ•´å®ƒä»¥é™ä½åˆ°0.59ï¼Œè¿™æ ·çš„æ—¥å¿—æŸå¤±è¦å¥½å¾—å¤šï¼Œå¿½ç•¥è´Ÿå€¼ï¼Œä¸‹ä¸€éƒ¨åˆ†æˆ‘ä¼šè§£é‡Šè¿™ä¸ºä½•å¿…è¦ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ä¼˜åŒ–ï¼Œå› ä¸ºå®ƒæƒ³è¦æœ€å¤§åŒ–è¿™ä¸ªæ•°å­—ï¼Œè€Œä½ å¹¶ä¸æ˜¯è¯•å›¾æœ€å¤§åŒ–æ—¥å¿—æŸå¤±ï¼Œè€Œæ˜¯è¯•å›¾æœ€å°åŒ–å®ƒã€‚
- en: '![](img/6ffb2dddbc647a884212a626611b3856_3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ffb2dddbc647a884212a626611b3856_3.png)'
- en: Well those are the hyperparameters that make up a neural network and it probably
    seems intimidating to try to optimize these on your own we'll see in the next
    video that we can use Bayesian hyperparameter optimization to let machine learning
    do a lot of this work for us this content changes often so subscribe to the channel
    to stay up to date on this course and other topics and artificial intelligenceã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ„æˆç¥ç»ç½‘ç»œçš„è¶…å‚æ•°ï¼Œå¯èƒ½çœ‹èµ·æ¥ä»¤äººç”Ÿç•ï¼Œè¯•å›¾ç‹¬è‡ªä¼˜åŒ–è¿™äº›ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­çœ‹åˆ°ï¼Œå¯ä»¥ä½¿ç”¨è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–ï¼Œè®©æœºå™¨å­¦ä¹ ä¸ºæˆ‘ä»¬å®Œæˆè®¸å¤šå·¥ä½œï¼Œè¿™ä¸ªå†…å®¹ç»å¸¸å˜åŒ–ï¼Œæ‰€ä»¥è®¢é˜…é¢‘é“ä»¥ä¿æŒè¯¾ç¨‹å’Œå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜çš„æœ€æ–°ä¿¡æ¯ã€‚
