- en: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及NLP任务应用！＜官方教程系列＞
    - P17：L2.10- 批处理输入(PyTorch) - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】官方教程来啦！5位 Hugging Face 工程师带你了解 Transformers 原理细节及 NLP 任务应用！＜官方教程系列＞
    - P17：L2.10- 批处理输入(PyTorch) - ShowMeAI - BV1Jm4y1X7UL
- en: How to batch inputs together in this video， we also see how2 batch input sequences
    together。In general all of the sentences we want to pass through our model won't
    all have the same length。Here we are using the model we saw in the sentiment analysis
    pipeline and want to classify two sentences。When tokenizing them and mapping each
    token to its corresponding input ID。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将输入批处理在这个视频中，我们还看到如何将输入序列批处理在一起。一般来说，我们想传递给模型的所有句子长度都不会相同。在这里，我们使用了在情感分析管道中看到的模型，想要对两个句子进行分类。当对它们进行标记并将每个标记映射到其对应的输入
    ID 时。
- en: We get two lists of different length。Trying to create a tensor or an Mbi array
    from the stool list will result in an error because all arrays and densilrs should
    be recangra。One way to overcome this limit is to make the second sentence the
    same length at the first by adding a special token as many times as necessary。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到两个长度不同的列表。试图从这两个列表创建一个张量或 Mbi 数组将导致错误，因为所有数组和张量都应该是规则的。克服这个限制的一种方法是通过添加特殊标记，将第二个句子的长度调整到第一个句子的长度，直到必要的次数。
- en: Another way would be to2k the first sequence to the length of the second。But
    you would then lose a lot of information that might be necessary to properly classify
    the sentence。In general， we only truncate sentences when we are longer than the
    maximum length the model can handle。The value used to pad the circum sentence
    should not be picked randomly。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将第一个序列的长度调整到第二个序列的长度。但是你将会失去很多可能对于正确分类句子必要的信息。一般来说，当句子的长度超过模型可以处理的最大长度时，我们才会截断句子。用于填充句子的值不应随机选择。
- en: The model has been portrayed with a certain padding ID， which you can find in
    tokenazizerpa token8。Now that we have better or sentences， we can make a batch
    with them。If we pass the two sentences to the model separately and patch together
    however。 we notice that we don't get the same results for the sentence that is
    pad here the second one h is that a bug in the transformers library now if you
    remember that transformers will all make easy user of attention layers。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用了某个填充 ID，你可以在 tokenazizerpa token8 中找到。现在我们有了更好的句子，我们可以将它们组合成一个批次。如果我们单独将这两个句子传递给模型，但将它们拼接在一起，我们会注意到填充在这里的第二个句子得不到相同的结果，这在
    transformers 库中算不算一个 bug 呢？如果你还记得，transformers 会很方便地使用注意力层。
- en: this should not come as a total surprise。When computing is the contextual representation
    of each token。The attention layers look at all the other words in the sentence。If
    you have just a sentence or the sentence with several padic tokens that it。 it's
    logical we don't get the same values。To get the same results with or without padding。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应该是一个完全意外的事情。当计算是每个标记的上下文表示时。注意力层会查看句子中的所有其他单词。如果你只有一个句子或几个填充标记的句子，那么，我们得不到相同的值是合乎逻辑的。为了获得有无填充的相同结果。
- en: we need to indicate to the attention layers that we should ignore those padding
    targets。This is done by creating an attention mask， a tonsil with the same shape
    as the input ID with series and ones。Once indicates the tokens the attention layers
    should consider in the context。 and the the tokens which should ignore。Now， passing
    this attention mask along with the input ID will give us the same results as when
    we send the two sentences individually to the model。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要向注意力层指示，我们应该忽略那些填充目标。这是通过创建一个注意力掩码来完成的，它与输入 ID 具有相同的形状，并包含一系列的 0 和 1。1 表示注意力层在上下文中应该考虑的标记，而
    0 表示应该忽略的标记。现在，将这个注意力掩码与输入 ID 一起传递，将会给我们与单独发送两个句子给模型时相同的结果。
- en: This is all done behind the scenes by the tokenizer when you apply it to several
    sentences with the flag padding equal to。It will apply his bedding with a proper
    value to the smaller sentences and create the appropriate attention mask。![](img/bcdf10aa45eed9c71073117b58d40576_1.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是在后台由 tokenizer 完成的，当你将它应用于多个句子并将填充标志设置为时。它会将适当的值应用于较小的句子，并创建相应的注意力掩码。![](img/bcdf10aa45eed9c71073117b58d40576_1.png)
- en: 。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 。
- en: '![](img/bcdf10aa45eed9c71073117b58d40576_3.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcdf10aa45eed9c71073117b58d40576_3.png)'
