- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorch è¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é” PyTorch çš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P13ï¼šL13-
    PyTorch ç§»åŠ¨ç‰ˆ - ShowMeAI - BV1ZZ4y1U7dg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘Pytorchè¿›é˜¶å­¦ä¹ è®²åº§ï¼14ä½Facebookå·¥ç¨‹å¸ˆå¸¦ä½ è§£é”PyTorchçš„ç”Ÿäº§åº”ç”¨ä¸æŠ€æœ¯ç»†èŠ‚ ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼ - P13ï¼šL13-
    PyTorchç§»åŠ¨ç‰ˆ - ShowMeAI - BV1ZZ4y1U7dg
- en: ğŸ¼ã€‚![](img/26aca7c4871d57c8c593b5a764582be0_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ã€‚![](img/26aca7c4871d57c8c593b5a764582be0_1.png)
- en: Hiï¼Œ I'm Davidã€‚I'm an engineer on Pytorchï¼Œ and today I'm going to be talking
    about some of the improvements we've made to Pytorch Mobi over the past yearã€‚Specificallyï¼Œ
    improvements to CPU performanceï¼Œ our prototype support for GPUsã€‚ our expanded
    documentation and tutorialsï¼Œ and some news about mobile inference acceleratorsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯å¤§å«ã€‚æˆ‘æ˜¯Pytorchçš„ä¸€åå·¥ç¨‹å¸ˆï¼Œä»Šå¤©æˆ‘è¦è°ˆè°ˆæˆ‘ä»¬åœ¨è¿‡å»ä¸€å¹´ä¸­å¯¹Pytorch Mobiæ‰€åšçš„ä¸€äº›æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼ŒCPUæ€§èƒ½çš„æå‡ï¼Œæˆ‘ä»¬å¯¹GPUçš„åŸå‹æ”¯æŒï¼Œæ‰©å±•çš„æ–‡æ¡£å’Œæ•™ç¨‹ï¼Œä»¥åŠå…³äºç§»åŠ¨æ¨ç†åŠ é€Ÿå™¨çš„ä¸€äº›æ¶ˆæ¯ã€‚
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_3.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_3.png)'
- en: So firstï¼Œ CPU performanceã€‚CPUs are the most ubiquitous computational units on
    mobile phonesã€‚They are also the most versatile and in some cases also the most
    powerfulã€‚ and so CPU performance will always be a high priority for us on PyTtorch
    Moã€‚![](img/26aca7c4871d57c8c593b5a764582be0_5.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒCPUæ€§èƒ½ã€‚CPUæ˜¯æ‰‹æœºä¸Šæœ€æ™®éçš„è®¡ç®—å•å…ƒã€‚å®ƒä»¬ä¹Ÿæ˜¯æœ€çµæ´»çš„ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿæ˜¯æœ€å¼ºå¤§çš„ã€‚å› æ­¤ï¼ŒCPUæ€§èƒ½å§‹ç»ˆæ˜¯æˆ‘ä»¬åœ¨PyTtorch Mobiä¸Šçš„é«˜ä¼˜å…ˆçº§ï¼![](img/26aca7c4871d57c8c593b5a764582be0_5.png)
- en: And we've made a lot of improvements over the past yearã€‚Here is a comparison
    between PyTtororch 1ã€‚3ã€‚ the first release of PyTtorch Mobiï¼Œ and a latest release
    Pythtororch 1ã€‚7ã€‚As a benchmarkã€‚ we're using a floating point version of the mobileNet
    V2 modelã€‚And initiallyã€‚ we were running this model in about 250 millisecondsï¼Œ
    but now in the latest releaseã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œæˆ‘ä»¬åšäº†å¾ˆå¤šæ”¹è¿›ã€‚è¿™é‡Œæ˜¯PyTtorch 1.3ã€PyTtorch Mobiçš„é¦–æ¬¡å‘å¸ƒå’Œæœ€æ–°å‘å¸ƒçš„Pytororch 1.7ä¹‹é—´çš„æ¯”è¾ƒã€‚ä½œä¸ºåŸºå‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†mobileNet
    V2æ¨¡å‹çš„æµ®ç‚¹ç‰ˆæœ¬ã€‚æœ€åˆï¼Œæˆ‘ä»¬å¤§çº¦åœ¨250æ¯«ç§’å†…è¿è¡Œæ­¤æ¨¡å‹ï¼Œä½†ç°åœ¨åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­ã€‚
- en: we're over 10 times fasterï¼Œ running this model in under 15 millisecondsã€‚And
    so if you compare us to some of the other mobile inference frameworks out thereã€‚
    we're about in the middle of the packï¼Œ which is something we're pretty happy about
    considering the high quality of the other frameworks we're comparing againstã€‚Nowï¼Œ
    mobile phones are more limited than serversï¼Œ and so we can't run all the fancy
    compiler machinery that Pytorch on the server side uses to get the best performance
    out of your modelã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é€Ÿåº¦è¶…è¿‡äº†10å€ï¼Œåœ¨15æ¯«ç§’å†…è¿è¡Œæ­¤æ¨¡å‹ã€‚å› æ­¤ï¼Œå¦‚æœä½ å°†æˆ‘ä»¬ä¸å…¶ä»–ä¸€äº›ç§»åŠ¨æ¨ç†æ¡†æ¶è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¤§çº¦å¤„äºä¸­é—´ä½ç½®ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬ä¸ä¹‹æ¯”è¾ƒçš„å…¶ä»–æ¡†æ¶çš„é«˜è´¨é‡ï¼Œæˆ‘ä»¬å¯¹æ­¤æ„Ÿåˆ°ç›¸å½“æ»¡æ„ã€‚ç°åœ¨ï¼Œæ‰‹æœºçš„é™åˆ¶æ¯”æœåŠ¡å™¨è¦å¤šï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½è¿è¡Œæ‰€æœ‰èŠ±å“¨çš„ç¼–è¯‘å™¨æœºåˆ¶ï¼Œä»¥ä»æ¨¡å‹ä¸­è·å¾—æœ€ä½³æ€§èƒ½ã€‚
- en: you have to do a little bit of upfront preparation to make sure your model runs
    as fast as possibleã€‚![](img/26aca7c4871d57c8c593b5a764582be0_7.png)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦è¿›è¡Œä¸€äº›å‰æœŸå‡†å¤‡ï¼Œä»¥ç¡®ä¿ä½ çš„æ¨¡å‹è¿è¡Œå¾—å°½å¯èƒ½å¿«ã€‚![](img/26aca7c4871d57c8c593b5a764582be0_7.png)
- en: But fortunatelyï¼Œ we've been able to bundle all that up into a single function
    that you can call to get the best performance It's called optimizeize for mobileã€‚
    it's very simple to use as you can see hereï¼Œ you just import it from Pytororchã€‚
    you run it on your model and you save the resulting model to diskã€‚And this will
    ensure that you're running optimizations like folding batch norm operations into
    a prior convolutionã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å·²ç»èƒ½å¤Ÿå°†æ‰€æœ‰è¿™äº›å†…å®¹æ‰“åŒ…æˆä¸€ä¸ªå•ä¸€çš„å‡½æ•°ï¼Œä½ å¯ä»¥è°ƒç”¨å®ƒæ¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚å®ƒè¢«ç§°ä¸º**ä¸ºç§»åŠ¨ä¼˜åŒ–**ï¼Œä½¿ç”¨èµ·æ¥éå¸¸ç®€å•ï¼Œå¦‚ä½ æ‰€è§ï¼Œåªéœ€ä»Pytororchå¯¼å…¥å®ƒã€‚ä½ åœ¨æ¨¡å‹ä¸Šè¿è¡Œå®ƒï¼Œç„¶åå°†ç”Ÿæˆçš„æ¨¡å‹ä¿å­˜åˆ°ç£ç›˜ã€‚è¿™å°†ç¡®ä¿ä½ æ‰§è¡Œè¯¸å¦‚å°†æ‰¹é‡å½’ä¸€åŒ–æ“ä½œæŠ˜å åˆ°å…ˆå‰å·ç§¯ä¸­çš„ä¼˜åŒ–ã€‚
- en: Prepacking model weights to get maximum throughput and running model freezing
    to eliminate unnecessary overheadã€‚ and we hope to add more optimizations to this
    entry point in the futureã€‚So that covers the preparation of your modelï¼Œ but then
    when you run the modelã€‚ there are some other things you can do to improve performance
    as wellã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æ‰“åŒ…æ¨¡å‹æƒé‡ä»¥è·å¾—æœ€å¤§ååé‡ï¼Œå¹¶è¿è¡Œæ¨¡å‹å†»ç»“ä»¥æ¶ˆé™¤ä¸å¿…è¦çš„å¼€é”€ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥å°†æ›´å¤šä¼˜åŒ–æ·»åŠ åˆ°è¿™ä¸ªå…¥å£ç‚¹ã€‚å› æ­¤ï¼Œè¿™æ¶µç›–äº†æ¨¡å‹çš„å‡†å¤‡ï¼Œä½†åœ¨è¿è¡Œæ¨¡å‹æ—¶ï¼Œä½ è¿˜å¯ä»¥åšä¸€äº›å…¶ä»–äº‹æƒ…æ¥æå‡æ€§èƒ½ã€‚
- en: We've recently released a caching allocator for memoryã€‚Nowï¼Œ the default behavior
    on PyTtorchã€‚ both on mobile and on the server sideã€‚Is that as soon as you're done
    with a tensorã€‚ its memory buffer gets immediately released back to the system
    allocator and in some casesã€‚ back to the operating systemï¼Œ and this is great for
    memory efficiencyã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€è¿‘å‘å¸ƒäº†ä¸€ä¸ªç”¨äºå†…å­˜çš„ç¼“å­˜åˆ†é…å™¨ã€‚ç°åœ¨ï¼ŒPyTtorchçš„é»˜è®¤è¡Œä¸ºï¼Œæ— è®ºæ˜¯åœ¨ç§»åŠ¨ç«¯è¿˜æ˜¯æœåŠ¡å™¨ç«¯ï¼Œéƒ½æ˜¯ä¸€æ—¦ä½ å®Œæˆäº†å¼ é‡ï¼Œå®ƒçš„å†…å­˜ç¼“å†²åŒºå°±ä¼šç«‹å³é‡Šæ”¾å›ç³»ç»Ÿåˆ†é…å™¨ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé‡Šæ”¾å›æ“ä½œç³»ç»Ÿï¼Œè¿™å¯¹äºå†…å­˜æ•ˆç‡éå¸¸æœ‰åˆ©ã€‚
- en: but if you're running the same model over and over againã€‚ you can waste a lot
    of time freeing and reallocating these buffers very quickly and the caching allocator
    gives you more explicit control over that memory policyã€‚ğŸ˜Šï¼ŒSo the way you use it
    is fairly simpleï¼Œ you create this caching allocator objectã€‚ normally right around
    the same time as when you're loading your modelã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœä½ ä¸æ–­é‡å¤è¿è¡Œç›¸åŒçš„æ¨¡å‹ï¼Œä½ å¯èƒ½ä¼šæµªè´¹å¤§é‡æ—¶é—´å¿«é€Ÿé‡Šæ”¾å’Œé‡æ–°åˆ†é…è¿™äº›ç¼“å†²åŒºï¼Œè€Œç¼“å­˜åˆ†é…å™¨åˆ™å¯ä»¥è®©ä½ æ›´æ˜ç¡®åœ°æ§åˆ¶å†…å­˜ç­–ç•¥ã€‚ğŸ˜Šä½¿ç”¨æ–¹å¼ç›¸å½“ç®€å•ï¼Œä½ åˆ›å»ºè¿™ä¸ªç¼“å­˜åˆ†é…å™¨å¯¹è±¡ï¼Œé€šå¸¸åœ¨åŠ è½½æ¨¡å‹æ—¶å°±ä¼šåˆ›å»ºã€‚
- en: and then every time you're going to run the modelã€‚ you first create this caching
    allocator guard objectã€‚ which ensures that your caching allocator is being actively
    used during that inferenceã€‚And that's all you need to doï¼Œ we've seen in performance
    improvements on the order of 5 to 20% from using the caching allocatorã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡ä½ è¦è¿è¡Œæ¨¡å‹æ—¶ï¼Œé¦–å…ˆåˆ›å»ºè¿™ä¸ªç¼“å­˜åˆ†é…å™¨ä¿æŠ¤å¯¹è±¡ï¼Œç¡®ä¿ä½ çš„ç¼“å­˜åˆ†é…å™¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¢«ç§¯æä½¿ç”¨ã€‚ä½ åªéœ€è¦è¿™æ ·åšï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½¿ç”¨ç¼“å­˜åˆ†é…å™¨åæ€§èƒ½æå‡åœ¨5%åˆ°20%ä¹‹é—´ã€‚
- en: of courseï¼Œ at the cost of some increased memory usage while the inference is
    running now moving on from CPU to GPUã€‚![](img/26aca7c4871d57c8c593b5a764582be0_9.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè¿™ä¼šå¯¼è‡´åœ¨æ¨ç†è¿è¡Œæ—¶å†…å­˜ä½¿ç”¨é‡å¢åŠ ï¼Œç°åœ¨ä»CPUç§»åŠ¨åˆ°GPUã€‚![](img/26aca7c4871d57c8c593b5a764582be0_9.png)
- en: Many of you probably know that GPUs have been popular for machine learning on
    the server side for quite a while nowã€‚ but almost all mobile devices have a GPU
    as wellï¼Œ and they can be used for accelerating inference on deviceã€‚Nowï¼Œ in high
    end phones with powerful GPUsï¼Œ you can see significant performance wins from using
    the GPUã€‚ but even on devices with more modest GPUsï¼Œ you can get other benefits
    like reducing power consumption and freeing up the CPU to do other intensive operations
    like running a video chat callã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¤šäººå¯èƒ½çŸ¥é“ï¼ŒGPUåœ¨æœåŠ¡å™¨ç«¯çš„æœºå™¨å­¦ä¹ ä¸­å·²ç»æµè¡Œäº†ä¸€æ®µæ—¶é—´ï¼Œä½†å‡ ä¹æ‰€æœ‰çš„ç§»åŠ¨è®¾å¤‡ä¹Ÿéƒ½æœ‰GPUï¼Œå®ƒä»¬å¯ä»¥ç”¨äºåŠ é€Ÿè®¾å¤‡ä¸Šçš„æ¨ç†ã€‚åœ¨é«˜ç«¯æ‰‹æœºä¸Šï¼Œä½¿ç”¨å¼ºå¤§çš„GPUå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä½†å³ä½¿åœ¨é…ç½®è¾ƒä½çš„è®¾å¤‡ä¸Šï¼Œä½ ä¹Ÿèƒ½è·å¾—å…¶ä»–å¥½å¤„ï¼Œæ¯”å¦‚é™ä½åŠŸè€—å’Œé‡Šæ”¾CPUä»¥è¿›è¡Œå…¶ä»–å¯†é›†å‹æ“ä½œï¼Œä¾‹å¦‚è¿›è¡Œè§†é¢‘é€šè¯ã€‚
- en: for exampleã€‚And so today we're releasing prototype support for GPU inference
    using two different APIs on iOS we have metalã€‚ which is Apple's high performance
    low level API for GPU accessï¼Œ and on Android we have Vulcanã€‚ which is the next
    generation open standard for crossplatform GPU accessã€‚The way you make use of
    these backends is with optimized for mobileã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä»Šå¤©æˆ‘ä»¬å‘å¸ƒäº†å¯¹iOSä¸Šä½¿ç”¨ä¸¤ç§ä¸åŒAPIè¿›è¡ŒGPUæ¨ç†çš„åŸå‹æ”¯æŒï¼Œæˆ‘ä»¬æœ‰Metalï¼Œè¿™æ˜¯è‹¹æœçš„é«˜æ€§èƒ½ä½çº§GPUè®¿é—®APIï¼Œè€Œåœ¨Androidä¸Šæˆ‘ä»¬æœ‰Vulkanï¼Œè¿™æ˜¯ä¸‹ä¸€ä»£è·¨å¹³å°GPUè®¿é—®çš„å¼€æ”¾æ ‡å‡†ã€‚ä½¿ç”¨è¿™äº›åç«¯çš„æ–¹å¼æ˜¯é’ˆå¯¹ç§»åŠ¨è®¾å¤‡è¿›è¡Œä¼˜åŒ–ã€‚
- en: the utility function we introduced earlier for getting the best performanceã€‚
    you just have to pass this one other argument to it to tell it which backend you
    want to use and it will perform the appropriate preparations for your modelã€‚Nowï¼Œ
    when you run the modelï¼Œ there's a few extra small steps you need to doã€‚This first
    example is for iOSã€‚When you take your input tensorã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰ä»‹ç»çš„å®ç”¨å‡½æ•°ç”¨äºè·å–æœ€ä½³æ€§èƒ½ï¼Œä½ åªéœ€ä¼ é€’å¦ä¸€ä¸ªå‚æ•°ï¼Œå‘Šè¯‰å®ƒä½ æƒ³ä½¿ç”¨å“ªä¸ªåç«¯ï¼Œå®ƒå°†ä¸ºä½ çš„æ¨¡å‹è¿›è¡Œé€‚å½“çš„å‡†å¤‡ã€‚ç°åœ¨ï¼Œå½“ä½ è¿è¡Œæ¨¡å‹æ—¶ï¼Œè¿˜æœ‰å‡ ä¸ªé¢å¤–çš„å°æ­¥éª¤éœ€è¦åšã€‚è¿™ä¸ªç¬¬ä¸€ä¸ªç¤ºä¾‹æ˜¯é’ˆå¯¹iOSçš„ã€‚å½“ä½ è·å–è¾“å…¥å¼ é‡æ—¶ã€‚
- en: you need to call this dot metal method it to move it to the GPU so that metal
    can access itã€‚ and then after you're finished running the model you need to call
    dot CPUU and your result to bring it back to the CPU for further processingã€‚If
    you are using Vulcan on Android using the C++ APIï¼Œ you'll do something very similarã€‚
    but if you're using our Java APIï¼Œ there's a simpler methodã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦è°ƒç”¨è¿™ä¸ª`.metal`æ–¹æ³•ï¼Œå°†å…¶ç§»åŠ¨åˆ°GPUï¼Œä»¥ä¾¿Metalå¯ä»¥è®¿é—®å®ƒã€‚ç„¶ååœ¨è¿è¡Œå®Œæ¨¡å‹åï¼Œä½ éœ€è¦è°ƒç”¨`.CPUU`å°†ç»“æœå¸¦å›CPUè¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚å¦‚æœä½ åœ¨Androidä¸Šä½¿ç”¨Vulkançš„C++
    APIï¼Œä½ ä¼šåšä¸€äº›éå¸¸ç›¸ä¼¼çš„äº‹æƒ…ã€‚ä½†å¦‚æœä½ ä½¿ç”¨æˆ‘ä»¬çš„Java APIï¼Œä¼šæœ‰æ›´ç®€å•çš„æ–¹æ³•ã€‚
- en: you can just pass this one extra argument when you're loading the modelï¼Œ deviceã€‚Vulcan
    to tell it which device the model is going to run on and it can automatically
    handle moving your input and output to the GPU and back whenever you run your
    modelã€‚Nowï¼Œ the benefits that you get from running on GPU will vary from device
    to device and model to modelã€‚ but just to give one exampleï¼Œ we've seen a 33% improvement
    in performance from switching to metal from our best CPU implementation with a
    ResNe 18 model on an iPhone 11ã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½æ¨¡å‹æ—¶ï¼Œä½ åªéœ€ä¼ é€’ä¸€ä¸ªé¢å¤–çš„å‚æ•°ï¼Œå³è®¾å¤‡ï¼ˆdeviceï¼‰ï¼Œå‘Šè¯‰Vulcanæ¨¡å‹å°†åœ¨å“ªä¸ªè®¾å¤‡ä¸Šè¿è¡Œï¼Œå®ƒå¯ä»¥åœ¨è¿è¡Œæ¨¡å‹æ—¶è‡ªåŠ¨å¤„ç†è¾“å…¥å’Œè¾“å‡ºçš„ç§»åŠ¨åˆ°GPUå’Œå›é€€ã€‚ç°åœ¨ï¼Œä»GPUè¿è¡Œæ‰€è·å¾—çš„å¥½å¤„ä¼šå› è®¾å¤‡å’Œæ¨¡å‹è€Œå¼‚ï¼Œä½†ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å‘ç°å°†ResNet
    18æ¨¡å‹ä»æˆ‘ä»¬æœ€ä½³çš„CPUå®ç°åˆ‡æ¢åˆ°Metalåï¼Œæ€§èƒ½æé«˜äº†33%ï¼Œåœ¨iPhone 11ä¸Šè¡¨ç°å°¤ä¸ºæ˜æ˜¾ã€‚
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_11.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_11.png)'
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_12.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_12.png)'
- en: I think the release I'm most excited about today is not a code release at allã€‚
    but rather our expanded set of documentation and tutorials one of our highest
    priorities on Pytororch mobile is to make it easy to use and accessible and we
    think that documentation is an important way of enabling that so I'll talk about
    some of the doc tutorials that we have available one of the most interesting ones
    is the Pytororch mobile performance recipes this is a onestop shop for performance
    tips and tricks including how to do operator fusion quantizationã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»Šå¤©æœ€æœŸå¾…çš„å‘å¸ƒå¹¶ä¸æ˜¯ä»£ç å‘å¸ƒï¼Œè€Œæ˜¯æˆ‘ä»¬æ‰©å±•çš„æ–‡æ¡£å’Œæ•™ç¨‹é›†ã€‚Pytorchç§»åŠ¨çš„é¦–è¦ä»»åŠ¡ä¹‹ä¸€æ˜¯è®©å…¶æ˜“äºä½¿ç”¨å’Œå¯è®¿é—®ï¼Œæˆ‘ä»¬è®¤ä¸ºæ–‡æ¡£æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„é‡è¦æ–¹å¼ã€‚å› æ­¤ï¼Œæˆ‘å°†ä»‹ç»ä¸€äº›å¯ç”¨çš„æ–‡æ¡£æ•™ç¨‹ï¼Œå…¶ä¸­ä¸€ä¸ªæœ€æœ‰è¶£çš„æ˜¯Pytorchç§»åŠ¨æ€§èƒ½é£Ÿè°±ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºæ€§èƒ½æŠ€å·§å’Œçªé—¨çš„ä¸€ç«™å¼èµ„æºï¼ŒåŒ…æ‹¬å¦‚ä½•è¿›è¡Œæ“ä½œç¬¦èåˆå’Œé‡åŒ–ã€‚
- en: making sure that you're using the best memory formatã€‚![](img/26aca7c4871d57c8c593b5a764582be0_14.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä½ ä½¿ç”¨æœ€ä½³çš„å†…å­˜æ ¼å¼ã€‚![](img/26aca7c4871d57c8c593b5a764582be0_14.png)
- en: Making sure you're reusing memory appropriatelyã€‚And also how to set up your
    benchmarks to make sure that you're able to measure your models and verify you're
    getting the gains that you expect from these optimizationsã€‚We are also releasing
    today tutorials for Vulcan and metal which go into a little bit more depth on
    how to use these APIs appropriately to get access to GPUsã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä½ é€‚å½“åœ°é‡ç”¨å†…å­˜ã€‚åŒæ—¶ï¼Œå¦‚ä½•è®¾ç½®åŸºå‡†æµ‹è¯•ï¼Œä»¥ç¡®ä¿ä½ èƒ½å¤Ÿæµ‹é‡ä½ çš„æ¨¡å‹ï¼Œå¹¶éªŒè¯ä½ ä»è¿™äº›ä¼˜åŒ–ä¸­è·å¾—çš„æ”¶ç›Šã€‚æˆ‘ä»¬ä»Šå¤©è¿˜å‘å¸ƒäº†é’ˆå¯¹Vulcanå’ŒMetalçš„æ•™ç¨‹ï¼Œæ·±å…¥ä»‹ç»å¦‚ä½•æ­£ç¡®ä½¿ç”¨è¿™äº›APIä»¥è·å–GPUçš„è®¿é—®æƒé™ã€‚
- en: And we have a number of demo apps that are being released to give you a live
    example of how you integrate Pythorch mobile into a appã€‚ and these are available
    for both Android and iOS and they cover a set of features like image segmentation
    and machine translationã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€äº›æ¼”ç¤ºåº”ç”¨ï¼Œå±•ç¤ºå¦‚ä½•å°†Pytorchç§»åŠ¨é›†æˆåˆ°åº”ç”¨ç¨‹åºä¸­ï¼Œè¿™äº›åº”ç”¨å¯ç”¨äºAndroidå’ŒiOSï¼Œå¹¶æ¶µç›–äº†ä¸€ç³»åˆ—åŠŸèƒ½ï¼Œå¦‚å›¾åƒåˆ†å‰²å’Œæœºå™¨ç¿»è¯‘ã€‚
- en: We also have a tutorial out that shows how to use a custom operator in an Android
    application this has been a little tricky so far because you have to configure
    your Android NK to use an external dependencyã€‚ but the tutorial should walk you
    through it and simplify the process and we have an example here of using the ROI
    align operator to run the sorry faster RCNN object detection model in this demo
    appã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªæ•™ç¨‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨Androidåº”ç”¨ä¸­ä½¿ç”¨è‡ªå®šä¹‰æ“ä½œç¬¦ï¼Œè¿™åˆ°ç›®å‰ä¸ºæ­¢æœ‰ç‚¹æ£˜æ‰‹ï¼Œå› ä¸ºä½ å¿…é¡»é…ç½®Android NDKä»¥ä½¿ç”¨å¤–éƒ¨ä¾èµ–é¡¹ã€‚ä½†è¿™ä¸ªæ•™ç¨‹å°†å¼•å¯¼ä½ å®Œæˆè¿™ä¸€è¿‡ç¨‹å¹¶ç®€åŒ–æ“ä½œï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªä½¿ç”¨ROIå¯¹é½æ“ä½œç¬¦åœ¨è¿™ä¸ªæ¼”ç¤ºåº”ç”¨ä¸­è¿è¡Œæ›´å¿«çš„RCNNç›®æ ‡æ£€æµ‹æ¨¡å‹çš„ç¤ºä¾‹ã€‚
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_16.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_16.png)'
- en: The last release we're making today is something to give you access to mobile
    inference accelerators these take the benefits of running your model on GPU and
    just bring them to the next level but I'm going to let the next presenter talk
    about the details of this release and I will just leave you with some links to
    further resources from the Pytororchã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»Šå¤©å‘å¸ƒçš„æœ€åä¸€é¡¹æ˜¯è®©ä½ å¯ä»¥è®¿é—®ç§»åŠ¨æ¨ç†åŠ é€Ÿå™¨ï¼Œè¿™äº›åŠ é€Ÿå™¨åˆ©ç”¨åœ¨GPUä¸Šè¿è¡Œæ¨¡å‹çš„å¥½å¤„ï¼Œå°†å…¶æå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼Œä½†æˆ‘å°†è®©ä¸‹ä¸€ä½æ¼”è®²è€…æ¥è¯¦ç»†ä»‹ç»è¿™ä¸€å‘å¸ƒçš„ç»†èŠ‚ï¼Œå¹¶ç•™ä¸‹ç›¸å…³é“¾æ¥ä»¥è·å–æ›´å¤šPytorchèµ„æºã€‚
- en: org website you can click on mobile to see our homepage tutorials then mobile
    to see our in-depth tutorials and tutorials recipes mobile that will take you
    to the recipes which are little quick tips and tricks for how to use specific
    features I hope you have a great time using Pytororch mobile Thank you very muchã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®˜ç½‘ä¸Šï¼Œæ‚¨å¯ä»¥é€šè¿‡æ‰‹æœºç‚¹å‡»æŸ¥çœ‹æˆ‘ä»¬çš„é¦–é¡µæ•™ç¨‹ï¼Œç„¶åæŸ¥çœ‹æˆ‘ä»¬çš„æ·±å…¥æ•™ç¨‹å’Œæ•™ç¨‹é£Ÿè°±ï¼Œç§»åŠ¨è®¾å¤‡å°†å¸¦æ‚¨åˆ°é£Ÿè°±ï¼Œè¿™äº›æ˜¯å…³äºå¦‚ä½•ä½¿ç”¨ç‰¹å®šåŠŸèƒ½çš„å°æŠ€å·§å’Œçªé—¨ã€‚å¸Œæœ›æ‚¨åœ¨ä½¿ç”¨Pytorchç§»åŠ¨ç‰ˆæ—¶ç©å¾—æ„‰å¿«ï¼Œéå¸¸æ„Ÿè°¢ã€‚
- en: '![](img/26aca7c4871d57c8c593b5a764582be0_18.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26aca7c4871d57c8c593b5a764582be0_18.png)'
