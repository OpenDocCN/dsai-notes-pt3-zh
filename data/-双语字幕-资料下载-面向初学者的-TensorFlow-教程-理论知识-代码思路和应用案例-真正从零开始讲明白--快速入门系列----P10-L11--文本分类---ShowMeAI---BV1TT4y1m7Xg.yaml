- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é¢å‘åˆå­¦è€…çš„ TensorFlow æ•™ç¨‹ï¼Œç†è®ºçŸ¥è¯†ã€ä»£ç æ€è·¯å’Œåº”ç”¨æ¡ˆä¾‹ï¼ŒçœŸæ­£ä»é›¶å¼€å§‹è®²æ˜ç™½ï¼ï¼œå¿«é€Ÿå…¥é—¨ç³»åˆ—ï¼ - P10ï¼šL11-
    æ–‡æœ¬åˆ†ç±» - ShowMeAI - BV1TT4y1m7Xg
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘é¢å‘åˆå­¦è€…çš„ TensorFlow æ•™ç¨‹ï¼Œç†è®ºçŸ¥è¯†ã€ä»£ç æ€è·¯å’Œåº”ç”¨æ¡ˆä¾‹ï¼ŒçœŸæ­£ä»é›¶å¼€å§‹è®²æ˜ç™½ï¼ï¼œå¿«é€Ÿå…¥é—¨ç³»åˆ—ï¼ - P10ï¼šL11-
    æ–‡æœ¬åˆ†ç±» - ShowMeAI - BV1TT4y1m7Xg
- en: '![](img/cf77d9b01a43fab5f293c40eb502060f_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf77d9b01a43fab5f293c40eb502060f_0.png)'
- en: ğŸ¼ï¼ŒHeyï¼Œ guysï¼Œ and welcome to another Tensorflow tutorialã€‚ In this videoã€‚ we will
    learn how to use an R and N for text classificationã€‚ So last time I gave you a
    quick overview and showed you how we treat our input as a sequenceã€‚ and then create
    an R and N modelã€‚ And now we apply this to a very interesting taskã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¼ï¼Œå¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°å¦ä¸€ä¸ª Tensorflow æ•™ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ R å’Œ N è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚ä¸Šæ¬¡æˆ‘ç»™ä½ ä»¬å¿«é€Ÿæ¦‚è¿°äº†ä¸€ä¸‹ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•å°†è¾“å…¥è§†ä¸ºä¸€ä¸ªåºåˆ—ï¼Œç„¶ååˆ›å»º
    R å’Œ N æ¨¡å‹ã€‚ç°åœ¨æˆ‘ä»¬å°†æŠŠå®ƒåº”ç”¨åˆ°ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ä»»åŠ¡ä¸Šã€‚
- en: which is text classification from real worldor dataã€‚ So we analyze Twitter tweets
    and want to predict if the text is about a disaster event or notã€‚ So here I'm
    in a two pointer notebookã€‚ and I already imported the things we needã€‚ And then
    the data set we are going to useï¼Œ is available on kgggleã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ¥è‡ªçœŸå®ä¸–ç•Œæ•°æ®çš„æ–‡æœ¬åˆ†ç±»ã€‚æˆ‘ä»¬åˆ†æ Twitter çš„ tweetsï¼Œå¹¶æƒ³é¢„æµ‹æ–‡æœ¬æ˜¯å¦ä¸ç¾éš¾äº‹ä»¶æœ‰å…³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘åœ¨ä¸€ä¸ª Jupyter Notebook
    ä¸­ï¼Œå·²ç»å¯¼å…¥äº†æˆ‘ä»¬éœ€è¦çš„å†…å®¹ã€‚æˆ‘ä»¬è¦ä½¿ç”¨çš„æ•°æ®é›†åœ¨ Kaggle ä¸Šå¯ä»¥æ‰¾åˆ°ã€‚
- en: So I put the link in the descriptionï¼Œ of courseã€‚ And this is called the disaster
    tweetsã€‚ So we want to predict which tweets are about real disasters and which
    are notã€‚ğŸ˜Šã€‚And there are two files availableã€‚ so the training and testing CSsvã€‚
    but the testing CSsv doesn't include the labelsã€‚ So this is for the submission
    if you want to participate in this competitionã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å½“ç„¶æŠŠé“¾æ¥æ”¾åœ¨æè¿°ä¸­ã€‚è¿™å«åšç¾éš¾ tweetsã€‚æˆ‘ä»¬æƒ³é¢„æµ‹å“ªäº› tweets æ˜¯å…³äºçœŸå®ç¾éš¾çš„ï¼Œå“ªäº›ä¸æ˜¯ã€‚ğŸ˜Šè€Œä¸”æœ‰ä¸¤ä¸ªæ–‡ä»¶å¯ç”¨ï¼Œè®­ç»ƒå’Œæµ‹è¯• CSVï¼Œä½†æµ‹è¯•
    CSV ä¸åŒ…æ‹¬æ ‡ç­¾ã€‚è¿™æ˜¯ç”¨äºæäº¤çš„ï¼Œå¦‚æœä½ æƒ³å‚ä¸è¿™ä¸ªæ¯”èµ›çš„è¯ã€‚
- en: So I only downloaded this train do CSsv and then put it in my folder already
    and then we use pans to load the data so we can call panndas read CSsv and then
    the name of the fileã€‚ And now we if we have a look at the shapeï¼Œ then we see we
    have 7630 samples and then five columnsã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘åªä¸‹è½½äº†è¿™ä¸ªè®­ç»ƒæ•°æ® CSVï¼Œç„¶åå·²ç»æ”¾å…¥æˆ‘çš„æ–‡ä»¶å¤¹ä¸­ã€‚æ¥ç€æˆ‘ä»¬ä½¿ç”¨ pandas åŠ è½½æ•°æ®ï¼Œå¯ä»¥è°ƒç”¨ pandas çš„ read_csvï¼Œç„¶åæ–‡ä»¶åã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ•°æ®çš„å½¢çŠ¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æœ‰
    7630 ä¸ªæ ·æœ¬å’Œäº”åˆ—ã€‚
- en: So now let's have a look at the first five rowsã€‚ So here we see we get some
    additional information like the Iã€‚ the keyword and the location which we don't
    need nowã€‚ and then we have the textã€‚ So this is the actual tweetã€‚And then the
    target labelã€‚ So zero for no disaster and one for this is a disaster tweetã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥çœ‹å‰äº”è¡Œã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›é¢å¤–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ IDã€å…³é”®è¯å’Œä½ç½®ï¼Œè¿™äº›æˆ‘ä»¬ç°åœ¨ä¸éœ€è¦ã€‚ç„¶åæ˜¯æ–‡æœ¬ï¼Œè¿™å°±æ˜¯å®é™…çš„ tweetã€‚ç›®æ ‡æ ‡ç­¾æ˜¯é›¶è¡¨ç¤ºæ²¡æœ‰ç¾éš¾ï¼Œ1
    è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªç¾éš¾ tweetã€‚
- en: So let's also analyze how many of both classes we haveã€‚So we have this manyã€‚
    which are about an actual disaster and this manyï¼Œ which are not about a disasterã€‚
    So I guess it's pretty much balanced hereã€‚ I think that's that's okayã€‚ And yeahã€‚
    So now we can go ahead and want to to preproces this text a little bit before
    we can use an R and N laterã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä¹Ÿæ¥åˆ†æä¸€ä¸‹è¿™ä¸¤ç±»å„æœ‰å¤šå°‘ã€‚æˆ‘ä»¬æœ‰è¿™æ ·å¤šå…³äºå®é™…ç¾éš¾çš„ tweetsï¼Œè¿˜æœ‰è¿™æ ·å¤šä¸æ˜¯å…³äºç¾éš¾çš„ tweetsã€‚æˆ‘æƒ³è¿™é‡Œå¤§è‡´æ˜¯å¹³è¡¡çš„ï¼Œæˆ‘è§‰å¾—è¿™æ²¡é—®é¢˜ã€‚å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ç»§ç»­ï¼Œæƒ³åœ¨ç¨åä½¿ç”¨
    R å’Œ N ä¹‹å‰å…ˆå¯¹è¿™äº›æ–‡æœ¬è¿›è¡Œä¸€äº›é¢„å¤„ç†ã€‚
- en: So the first thing I want to do is to clean this data a little bitã€‚ And I want
    to remove URLã€‚ because it doesn't give us any informationã€‚ And I also want to
    remove punctuationã€‚So for thisã€‚ I already implemented these helper functionsã€‚
    So this one is using regular expressionsã€‚ And if you want to learn more about
    thisï¼Œ then I also have a full guide on my channel that you can check outã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ç¨å¾®æ¸…ç†ä¸€ä¸‹æ•°æ®ã€‚æˆ‘æƒ³å»é™¤ URLï¼Œå› ä¸ºå®ƒæ²¡æœ‰æä¾›ä»»ä½•ä¿¡æ¯ã€‚æˆ‘è¿˜æƒ³å»æ‰æ ‡ç‚¹ç¬¦å·ã€‚ä¸ºæ­¤ï¼Œæˆ‘å·²ç»å®ç°äº†è¿™äº›è¾…åŠ©å‡½æ•°ã€‚è¿™ä¸€ä¸ªä½¿ç”¨äº†æ­£åˆ™è¡¨è¾¾å¼ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œæˆ‘åœ¨æˆ‘çš„é¢‘é“ä¸Šä¹Ÿæœ‰ä¸€ä¸ªå®Œæ•´çš„æŒ‡å—å¯ä»¥æŸ¥çœ‹ã€‚
- en: And yeahï¼Œ so now let's define these two functionsã€‚ And hereï¼Œ for exampleã€‚ these
    are all the punctuation characters that we want to removeã€‚So now if we have a
    look at this exampleï¼Œ then here it finds one tweet with a URLã€‚ and if we remove
    thisï¼Œ then it has only this formatã€‚So now with these two functionsã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬æ¥å®šä¹‰è¿™ä¸¤ä¸ªå‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œä¾‹å¦‚ï¼Œè¿™äº›éƒ½æ˜¯æˆ‘ä»¬æƒ³è¦å»é™¤çš„æ ‡ç‚¹ç¬¦å·å­—ç¬¦ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªä¾‹å­ï¼Œå®ƒæ‰¾åˆ°äº†ä¸€æ¡å¸¦æœ‰ URL çš„ tweetã€‚å¦‚æœæˆ‘ä»¬å»æ‰è¿™ä¸ªï¼Œå®ƒå°±åªæœ‰è¿™ç§æ ¼å¼ã€‚æ‰€ä»¥ç°åœ¨æœ‰äº†è¿™ä¸¤ä¸ªå‡½æ•°ã€‚
- en: we can simply for our data frames or penda this data frameã€‚ we can call this
    map function and only for thisã€‚Text columnã€‚ So we say data frame dot text dot
    mapã€‚ And then these two functionsã€‚ And then againï¼Œ we assign it to the text columnã€‚
    So now this means we remove all the URLs and all the punctuation charactersã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„æ•°æ®æ¡†æˆ– Pandas æ•°æ®æ¡†ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™ä¸ª map å‡½æ•°ï¼Œåªé’ˆå¯¹æ–‡æœ¬åˆ—ã€‚å› æ­¤æˆ‘ä»¬è¯´ data frame.dot text.dot mapï¼Œç„¶åæ˜¯è¿™ä¸¤ä¸ªå‡½æ•°ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†å…¶èµ‹å€¼ç»™æ–‡æœ¬åˆ—ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ç§»é™¤äº†æ‰€æœ‰çš„
    URL å’Œæ ‡ç‚¹ç¬¦å·ã€‚
- en: So this is the first prepro I want to doã€‚ and then I also want to remove stop
    wordsã€‚ And for thisã€‚ we are going to use the famous N LTK librariesã€‚ So this is
    a very popular library and Python for natural language processingã€‚ and then you
    also probably have to say N LT K dot downloadã€‚ And by the wayã€‚ you can simply
    install it with Pipã€‚And then here I want to get all the stop words and then remove
    themã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘æƒ³åšçš„ç¬¬ä¸€ä¸ªé¢„å¤„ç†æ­¥éª¤ã€‚æˆ‘è¿˜æƒ³å»é™¤åœç”¨è¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‘—åçš„ NLTK åº“ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„ Python åº“ã€‚æ‚¨å¯èƒ½è¿˜éœ€è¦æ‰§è¡Œ
    NLTK.dot downloadã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæ‚¨å¯ä»¥é€šè¿‡ Pip ç®€å•å®‰è£…å®ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æƒ³è·å–æ‰€æœ‰åœç”¨è¯å¹¶å°†å…¶ç§»é™¤ã€‚
- en: So by definition hereï¼Œ a stop word is commonly is a commonly used wordsã€‚ such
    as D A N in that a search engine has been programmed to ignoreã€‚ So we want to
    ignore these stop wordsã€‚ So we get all the stop words from N L T Kã€‚ And then we
    remove it again with a little helper functionã€‚ And let's here print the stop wordsã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®å®šä¹‰ï¼Œåœç”¨è¯é€šå¸¸æ˜¯ä¸€äº›å¸¸ç”¨è¯ï¼Œä¾‹å¦‚ Dã€Aã€Nï¼Œæœç´¢å¼•æ“è¢«ç¼–ç¨‹ä¸ºå¿½ç•¥å®ƒä»¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³è¦å¿½ç•¥è¿™äº›åœç”¨è¯ã€‚æˆ‘ä»¬ä» NLTK è·å–æ‰€æœ‰çš„åœç”¨è¯ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå°åŠ©æ‰‹å‡½æ•°å°†å…¶ç§»é™¤ã€‚è®©æˆ‘ä»¬åœ¨è¿™é‡Œæ‰“å°å‡ºåœç”¨è¯ã€‚
- en: So here you see all the different stop wordssã€‚And then againã€‚ we call this map
    function with this functionã€‚ and on the text columnã€‚ So this removes all the stop
    wordsã€‚ And now let's have a look at some example textã€‚ So this is the text columnã€‚And
    now we want to prepare this text so that we can use it for a R and Nã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä¸åŒçš„åœç”¨è¯ã€‚ç„¶åæˆ‘ä»¬å†æ¬¡ä½¿ç”¨è¿™ä¸ªå‡½æ•°è°ƒç”¨ map å‡½æ•°ï¼Œä½œç”¨äºæ–‡æœ¬åˆ—ã€‚è¿™å°†ç§»é™¤æ‰€æœ‰åœç”¨è¯ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›ç¤ºä¾‹æ–‡æœ¬ã€‚è¿™å°±æ˜¯æ–‡æœ¬åˆ—ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦å‡†å¤‡è¿™äº›æ–‡æœ¬ï¼Œä»¥ä¾¿å¯ä»¥ç”¨äº
    RNNã€‚
- en: So weï¼Œ we cannot use it like this with all the stringsã€‚ So we somehow have to
    transform this to a representation that our model understandsã€‚And for thisã€‚ the
    first thing we want to do is count all the different wordsã€‚ And here we want to
    make use of a very nice objectï¼Œ the counter objectã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½åƒè¿™æ ·ä½¿ç”¨æ‰€æœ‰å­—ç¬¦ä¸²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶è½¬æ¢ä¸ºæ¨¡å‹ç†è§£çš„è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œç¬¬ä¸€æ­¥æ˜¯è®¡ç®—æ‰€æœ‰ä¸åŒçš„å•è¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¦åˆ©ç”¨ä¸€ä¸ªéå¸¸ä¸é”™çš„å¯¹è±¡ï¼Œå³è®¡æ•°å™¨å¯¹è±¡ã€‚
- en: which is available in the collections module in Pythonã€‚ and then we count all
    the unique wordsã€‚ So we iterate over over the text columnï¼Œ and then we say for
    each textã€‚ So basically for each line in this text columnï¼Œ we say we iterate over
    each lineã€‚ and then for each lineï¼Œ we split itã€‚ So we get an array of all the
    different wordsã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåœ¨ Python çš„ collections æ¨¡å—ä¸­å¯ç”¨ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—æ‰€æœ‰å”¯ä¸€çš„å•è¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éå†æ–‡æœ¬åˆ—ï¼Œç„¶åè¯´å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œæ“ä½œã€‚åŸºæœ¬ä¸Šï¼Œå¯¹äºæ–‡æœ¬åˆ—ä¸­çš„æ¯ä¸€è¡Œï¼Œæˆ‘ä»¬è¯´éå†æ¯ä¸€è¡Œï¼Œç„¶åå¯¹æ¯ä¸€è¡Œè¿›è¡Œåˆ†å‰²ã€‚è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸åŒå•è¯çš„æ•°ç»„ã€‚
- en: And then we iterate over all the words and put it in our counterã€‚ And then each
    time this word appearsï¼Œ we increase the counter by oneã€‚ So now if we do this and
    then return the counter and apply this function for the data frame text columnã€‚
    then we get the counter and we get the length of the counterã€‚ So this is the number
    of unique wordsã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬éå†æ‰€æœ‰çš„å•è¯å¹¶å°†å…¶æ”¾å…¥æˆ‘ä»¬çš„è®¡æ•°å™¨ä¸­ã€‚æ¯å½“è¿™ä¸ªå•è¯å‡ºç°æ—¶ï¼Œæˆ‘ä»¬å°±å°†è®¡æ•°å™¨å¢åŠ ä¸€ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¿™æ ·åšï¼Œç„¶åè¿”å›è®¡æ•°å™¨å¹¶å°†è¿™ä¸ªå‡½æ•°åº”ç”¨äºæ•°æ®æ¡†çš„æ–‡æœ¬åˆ—ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†è®¡æ•°å™¨åŠå…¶é•¿åº¦ã€‚è¿™å°±æ˜¯å”¯ä¸€å•è¯çš„æ•°é‡ã€‚
- en: So we see that we have almost 18000 different words in all these tweetsã€‚So let's
    also have a look at the counter itselfã€‚ So this is basically looks like a dictionaryã€‚
    So here the keys are the different wordsã€‚ and then we have the count of this word
    here as a valueã€‚ So this is how the counter looksã€‚ And what's also very nice with
    this counter objectã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°è¿™äº›æ¨æ–‡ä¸­å‡ ä¹æœ‰ 18000 ä¸ªä¸åŒçš„å•è¯ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿæ¥çœ‹çœ‹è®¡æ•°å™¨æœ¬èº«ã€‚åŸºæœ¬ä¸Šï¼Œå®ƒçœ‹èµ·æ¥åƒä¸€ä¸ªå­—å…¸ï¼Œè¿™é‡Œçš„é”®æ˜¯ä¸åŒçš„å•è¯ï¼Œå€¼æ˜¯æ¯ä¸ªå•è¯çš„è®¡æ•°ã€‚è¿™å°±æ˜¯è®¡æ•°å™¨çš„æ ·å­ï¼Œè®¡æ•°å™¨å¯¹è±¡è¿˜æœ‰ä¸€ä¸ªå¾ˆå¥½çš„ç‰¹æ€§ã€‚
- en: We can call this most common functionã€‚ So here we can have a look at the fiveï¼Œ
    most common wordsã€‚ So we see that the word like is the most common one and appears
    345 timesã€‚ğŸ˜Šï¼ŒAnd yeahï¼Œ then againã€‚ let's assign this length of the counter to the
    to a variable and call this nu unique wordsã€‚ So we need this laterã€‚ And now I
    want to split the data set into training and validation setã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™ä¸ªæœ€å¸¸è§çš„å‡½æ•°ã€‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹äº”ä¸ªæœ€å¸¸è§çš„å•è¯ã€‚å› æ­¤æˆ‘ä»¬çœ‹åˆ°â€œlikeâ€è¿™ä¸ªè¯æ˜¯æœ€å¸¸è§çš„ï¼Œå‡ºç°äº†345æ¬¡ã€‚ğŸ˜Š ç„¶åå†ä¸€æ¬¡ã€‚è®©æˆ‘ä»¬å°†è®¡æ•°å™¨çš„é•¿åº¦åˆ†é…ç»™ä¸€ä¸ªå˜é‡ï¼Œå¹¶ç§°ä¹‹ä¸º
    nu unique wordsã€‚æˆ‘ä»¬ç¨åä¼šéœ€è¦è¿™ä¸ªã€‚ç°åœ¨æˆ‘æƒ³å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚
- en: and for thisï¼Œ I define an integer ofã€‚This should be 80% of our whole data we
    want to use as trainingã€‚ and then the rest of thisã€‚ So 20% for validationã€‚ and
    then we can use the slicing on the data frameã€‚ and then the first 80% of the samples
    is used for training and the rest for validationã€‚ So now we get thatã€‚ And then
    we also So right now we still have the whole data frameã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘å®šä¹‰ä¸€ä¸ªæ•´æ•°ã€‚è¿™åº”è¯¥æ˜¯æˆ‘ä»¬æƒ³è¦ç”¨ä½œè®­ç»ƒçš„å…¨éƒ¨æ•°æ®çš„80%ã€‚ç„¶åå…¶ä½™çš„éƒ¨åˆ†ã€‚å› æ­¤ï¼Œ20%ç”¨äºéªŒè¯ã€‚æ¥ç€æˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®æ¡†è¿›è¡Œåˆ‡ç‰‡ã€‚ç„¶åå‰80%çš„æ ·æœ¬ç”¨äºè®­ç»ƒï¼Œå…¶ä½™éƒ¨åˆ†ç”¨äºéªŒè¯ã€‚ç°åœ¨æˆ‘ä»¬å¾—åˆ°äº†è¿™ä¸ªã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç°åœ¨ä»ç„¶æ‹¥æœ‰æ•´ä¸ªæ•°æ®æ¡†ã€‚
- en: So now we want to split the text and the labels and we can simply do this by
    assessing the different columnsã€‚ So we say train data frame dot text dots to nuy
    and the same for data frame dot targetã€‚ So we say these are our training sentences
    and our training labelsã€‚ And then againã€‚ we do the same for the validation setã€‚So
    if we have a look at both the training sentences shape and the validation sentences
    shapeã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³è¦æ‹†åˆ†æ–‡æœ¬å’Œæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°é€šè¿‡è®¿é—®ä¸åŒçš„åˆ—æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ train data frame.dot text dots to
    nuyï¼ŒåŒæ ·é€‚ç”¨äº data frame.dot targetã€‚å› æ­¤æˆ‘ä»¬è¯´è¿™äº›æ˜¯æˆ‘ä»¬çš„è®­ç»ƒå¥å­å’Œè®­ç»ƒæ ‡ç­¾ã€‚ç„¶åå†æ¬¡ï¼Œæˆ‘ä»¬å¯¹éªŒè¯é›†åšåŒæ ·çš„äº‹æƒ…ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è®­ç»ƒå¥å­å½¢çŠ¶å’ŒéªŒè¯å¥å­å½¢çŠ¶ã€‚
- en: then we see that we clearly have more in our training setã€‚ and now the next
    thing we want to do is to apply a tokenizerã€‚ So with tokenization we vectorize
    a text corpus by turning each text into a sequence of integersã€‚ So you will see
    an example in a second which makes this more clearerã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°è®­ç»ƒé›†ä¸­æœ‰æ›´å¤šçš„æ•°æ®ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„æ˜¯åº”ç”¨åˆ†è¯å™¨ã€‚é€šè¿‡åˆ†è¯ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬è¯­æ–™åº“å‘é‡åŒ–ï¼Œå°†æ¯ä¸ªæ–‡æœ¬è½¬åŒ–ä¸ºæ•´æ•°åºåˆ—ã€‚æ‰€ä»¥ä½ å¾ˆå¿«å°±ä¼šçœ‹åˆ°ä¸€ä¸ªç¤ºä¾‹ï¼Œä½¿è¿™ä¸€ç‚¹æ›´åŠ æ¸…æ™°ã€‚
- en: But for now we want to import this tokenizer from Tensorflowcars dot preprocessing
    dot textã€‚ and then we create a tokenizer objectã€‚ And for this we need to give
    it the number of unique wordsã€‚ So that's why we calculate this earlierã€‚ And then
    we have to call tokenizer dot fit on texts and thenã€‚the training sentencesã€‚ So
    only the training data hereã€‚ And now when we did thisã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ç°åœ¨æˆ‘ä»¬è¦ä» Tensorflowcars.dot preprocessing.dot text å¯¼å…¥è¿™ä¸ªåˆ†è¯å™¨ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨å¯¹è±¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æä¾›å”¯ä¸€å•è¯çš„æ•°é‡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¹‹å‰è¦è®¡ç®—è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬å¿…é¡»è°ƒç”¨
    tokenizer.dot fit åœ¨æ–‡æœ¬å’Œè®­ç»ƒå¥å­ä¸Šã€‚å› æ­¤è¿™é‡Œåªä½¿ç”¨è®­ç»ƒæ•°æ®ã€‚å½“æˆ‘ä»¬å®Œæˆè¿™ä¸ªæ­¥éª¤æ—¶ã€‚
- en: we can get this word indexã€‚ So here each word has a unique indexã€‚ So let's say
    word index equals tokenizer dot word index and then have a look at this word indexã€‚And
    then we see each of these words has a unique indexã€‚ So this is what this tokenizer
    doesã€‚And then we can convert these text to a sequenceï¼Œ so we can call tokenizer
    dot texts to sequences and then give it the sentencesã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¾—åˆ°è¿™ä¸ªå•è¯ç´¢å¼•ã€‚æ‰€ä»¥è¿™é‡Œæ¯ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ç´¢å¼•ã€‚å‡è®¾ word index ç­‰äº tokenizer.dot word indexï¼Œç„¶åçœ‹çœ‹è¿™ä¸ªå•è¯ç´¢å¼•ã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°è¿™äº›å•è¯çš„æ¯ä¸ªéƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ç´¢å¼•ã€‚è¿™å°±æ˜¯è¿™ä¸ªåˆ†è¯å™¨çš„ä½œç”¨ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è°ƒç”¨
    tokenizer.dot texts to sequencesï¼Œç„¶åç»™å‡ºå¥å­ã€‚
- en: So don't get confused by thisã€‚ we have the sentencesã€‚ thenã€‚ So this is the original
    textã€‚ And then we get a sequenceã€‚ So this has the same sizeã€‚ But now it has these
    unique indicesã€‚ So now we do this for the training set and the validation setã€‚
    And now hereï¼Œ for exampleã€‚ I compare five samples of the training sentences and
    the corresponding sequencesã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸è¦å¯¹æ­¤æ„Ÿåˆ°å›°æƒ‘ã€‚æˆ‘ä»¬æœ‰å¥å­ã€‚ç„¶åã€‚è¿™æ˜¯åŸå§‹æ–‡æœ¬ã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåºåˆ—ã€‚å› æ­¤å®ƒçš„å¤§å°ç›¸åŒã€‚ä½†ç°åœ¨å®ƒæœ‰è¿™äº›å”¯ä¸€çš„ç´¢å¼•ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†åšè¿™ä¸ªã€‚ä¾‹å¦‚ï¼Œæˆ‘æ¯”è¾ƒè®­ç»ƒå¥å­çš„äº”ä¸ªæ ·æœ¬å’Œç›¸åº”çš„åºåˆ—ã€‚
- en: So let's print thisã€‚And this is how it looks likeã€‚ So now it maybe gets a little
    bit clearerã€‚ So the sentence is the normal textã€‚ And then after we applied the
    tokenizationã€‚ we get this sequenceã€‚ So now we have the same length of this array
    as the textã€‚ But now we have an index for each wordã€‚So now now we have thatã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬æ‰“å°ä¸€ä¸‹ã€‚è¿™å°±æ˜¯å®ƒçš„æ ·å­ã€‚æ‰€ä»¥ç°åœ¨å¯èƒ½ä¼šæ›´æ¸…æ¥šä¸€äº›ã€‚å¥å­æ˜¯æ­£å¸¸æ–‡æœ¬ã€‚ç„¶ååœ¨åº”ç”¨äº†åˆ†è¯ä¹‹åã€‚æˆ‘ä»¬å¾—åˆ°è¿™ä¸ªåºåˆ—ã€‚å› æ­¤æˆ‘ä»¬ç°åœ¨è¿™ä¸ªæ•°ç»„çš„é•¿åº¦ä¸æ–‡æœ¬ç›¸åŒã€‚ä½†ç°åœ¨æˆ‘ä»¬ä¸ºæ¯ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ªç´¢å¼•ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªã€‚
- en: And now we want to do one more thing because right now the sequences can have
    a different lengthã€‚ So that's what you can see hereã€‚ but we want to have the same
    length for every sequenceã€‚ And for this we apply paddingã€‚ So againï¼Œ we import
    this we import Pat sequences from tensofflowcars do preprocessing do sequenceã€‚
    and then we have to specify a maximum lengthã€‚ So in this case we say it's 20 but
    you can play around with more or other ones here or maybe a tweet might be even
    longer than 20 different wordsã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³åšä¸€ä»¶äº‹ï¼Œå› ä¸ºç°åœ¨åºåˆ—å¯ä»¥æœ‰ä¸åŒçš„é•¿åº¦ã€‚è¿™å°±æ˜¯ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ã€‚ä½†æˆ‘ä»¬å¸Œæœ›æ¯ä¸ªåºåˆ—éƒ½æœ‰ç›¸åŒçš„é•¿åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åº”ç”¨å¡«å……ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å†æ¬¡å¯¼å…¥ï¼Œä»tensorflowå¯¼å…¥å¡«å……åºåˆ—è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶åæˆ‘ä»¬å¿…é¡»æŒ‡å®šä¸€ä¸ªæœ€å¤§é•¿åº¦ã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯´æ˜¯20ï¼Œä½†ä½ å¯ä»¥åœ¨è¿™é‡Œç©æ›´å¤šæˆ–å…¶ä»–çš„ï¼Œæˆ–è€…ä¹Ÿè®¸ä¸€ä¸ªæ¨æ–‡å¯èƒ½ç”šè‡³è¶…è¿‡20ä¸ªä¸åŒçš„å•è¯ã€‚
- en: So you might increase this a little bitã€‚ but then also your training might be
    longerã€‚ So now when we specify thisï¼Œ we can call this function pad sequences and
    we call this with theã€‚Rining sequences and our specified max lengthã€‚And here we
    say padding and truncating equals postsã€‚ So this means it just uses zerosã€‚ So
    then we do this for the training and validation sequenceã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥ç¨å¾®å¢åŠ ä¸€ä¸‹ï¼Œä½†è¿™ä¹Ÿå¯èƒ½ä½¿ä½ çš„è®­ç»ƒæ—¶é—´æ›´é•¿ã€‚å› æ­¤ç°åœ¨å½“æˆ‘ä»¬æŒ‡å®šè¿™ä¸ªæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™ä¸ªå‡½æ•°å¡«å……åºåˆ—ï¼Œå¹¶ç”¨è®­ç»ƒåºåˆ—å’Œæˆ‘ä»¬æŒ‡å®šçš„æœ€å¤§é•¿åº¦è°ƒç”¨å®ƒã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯´å¡«å……å’Œæˆªæ–­ç­‰äºåå¡«å……ã€‚è¿™æ„å‘³ç€å®ƒåªä½¿ç”¨é›¶ã€‚å› æ­¤æˆ‘ä»¬å¯¹è®­ç»ƒå’ŒéªŒè¯åºåˆ—æ‰§è¡Œæ­¤æ“ä½œã€‚
- en: And then if we have a look at the shape hereï¼Œ then we see that they all have
    the shape 20 in the second dimensionã€‚ because this is the max lengthã€‚And now if
    we print one padded sequenceã€‚ we see that it's used zero padding hereã€‚ So now
    againã€‚ let's print one sample of our training sentencesï¼Œ one sample of our training
    sequences and one sample of the padded sequence so that you see the differenceã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™é‡Œçš„å½¢çŠ¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒä»¬åœ¨ç¬¬äºŒç»´åº¦ä¸Šéƒ½æœ‰å½¢çŠ¶20ï¼Œå› ä¸ºè¿™æ˜¯æœ€å¤§é•¿åº¦ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬æ‰“å°ä¸€ä¸ªå¡«å……åºåˆ—ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™é‡Œä½¿ç”¨äº†é›¶å¡«å……ã€‚å› æ­¤ç°åœ¨å†ä¸€æ¬¡ã€‚è®©æˆ‘ä»¬æ‰“å°ä¸€ä¸ªè®­ç»ƒå¥å­æ ·æœ¬ï¼Œä¸€ä¸ªè®­ç»ƒåºåˆ—æ ·æœ¬å’Œä¸€ä¸ªå¡«å……åºåˆ—æ ·æœ¬ï¼Œä»¥ä¾¿ä½ çœ‹åˆ°åŒºåˆ«ã€‚
- en: And then againï¼Œ here you see for the sentenceï¼Œ we have all the words for the
    sequenceã€‚ we have these indicesï¼Œ and then for the padded sequenceï¼Œ we use0 paddingã€‚So
    now we can check if weã€‚ if this is actually correctã€‚ So if we can reverse thisã€‚
    So for thisã€‚ we create a dictionary where we flip around the keys and the values
    in this word indexã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå†æ¬¡çœ‹åˆ°ï¼Œå¯¹äºè¿™ä¸ªå¥å­ï¼Œæˆ‘ä»¬æœ‰åºåˆ—ä¸­çš„æ‰€æœ‰å•è¯ã€‚æˆ‘ä»¬æœ‰è¿™äº›ç´¢å¼•ï¼Œç„¶åå¯¹äºå¡«å……åºåˆ—ï¼Œæˆ‘ä»¬ä½¿ç”¨0å¡«å……ã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ä¸€ä¸‹ï¼Œå¦‚æœè¿™æ˜¯æ­£ç¡®çš„ã€‚å¦‚æœæˆ‘ä»¬èƒ½åè½¬è¿™ä¸ªã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†è¿™ä¸ªå•è¯ç´¢å¼•ä¸­çš„é”®å’Œå€¼ç¿»è½¬ã€‚
- en: So if we have a look against this is how this word index looks as keyã€‚ we have
    the word and as valueï¼Œ we have this indexã€‚ and now we want to store this in another
    dictionary and do it the other way aroundã€‚ So now we say the key is the index
    and the word is the valueã€‚So this is our reverse dictionaryã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹ï¼Œè¿™æ˜¯è¿™ä¸ªå•è¯ç´¢å¼•çš„é”®ã€‚æˆ‘ä»¬æœ‰å•è¯ï¼Œå€¼ä¸ºè¿™ä¸ªç´¢å¼•ã€‚ç°åœ¨æˆ‘ä»¬æƒ³å°†å…¶å­˜å‚¨åœ¨å¦ä¸€ä¸ªå­—å…¸ä¸­ï¼Œå¹¶åå‘æ“ä½œã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬è¯´ï¼Œé”®æ˜¯ç´¢å¼•ï¼Œå€¼æ˜¯å•è¯ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„åå‘å­—å…¸ã€‚
- en: So let's have a look at thatã€‚ And then we see all these indices are our keysã€‚
    and all these words are the valuesã€‚ And now we can define this decocode functionã€‚
    which gets a sequenceã€‚ And then we simplyã€‚ğŸ˜Šï¼ŒCall the get function with for each
    index in the sequenceã€‚ So this returns the corresponding valueã€‚ And if it's not
    availableã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°æ‰€æœ‰è¿™äº›ç´¢å¼•éƒ½æ˜¯æˆ‘ä»¬çš„é”®ï¼Œæ‰€æœ‰è¿™äº›å•è¯éƒ½æ˜¯å€¼ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥å®šä¹‰è¿™ä¸ªè§£ç å‡½æ•°ï¼Œå®ƒè·å–ä¸€ä¸ªåºåˆ—ã€‚ç„¶åæˆ‘ä»¬ç®€å•åœ°ğŸ˜Šï¼Œå¯¹åºåˆ—ä¸­çš„æ¯ä¸ªç´¢å¼•è°ƒç”¨getå‡½æ•°ã€‚è¿™å°†è¿”å›ç›¸åº”çš„å€¼ã€‚å¦‚æœä¸å¯ç”¨ã€‚
- en: then it should return a question markã€‚ but so this is if we get new indicesã€‚
    but if we stay in the same training data set then it should find a corresponding
    word for each indexã€‚ So this is the decocode functionã€‚ and then let's try it outã€‚
    So we call the deco function for one sample of the training sequencesã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒåº”è¯¥è¿”å›ä¸€ä¸ªé—®å·ã€‚ä½†å¦‚æœæˆ‘ä»¬è·å¾—æ–°çš„ç´¢å¼•ï¼Œåˆ™å¦‚æ­¤ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬ä¿æŒåœ¨åŒä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ä¸­ï¼Œé‚£ä¹ˆå®ƒåº”è¯¥ä¸ºæ¯ä¸ªç´¢å¼•æ‰¾åˆ°ç›¸åº”çš„å•è¯ã€‚è¿™å°±æ˜¯è§£ç å‡½æ•°ã€‚ç„¶åè®©æˆ‘ä»¬è¯•ä¸€ä¸‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªè®­ç»ƒåºåˆ—æ ·æœ¬è°ƒç”¨è§£ç å‡½æ•°ã€‚
- en: So this one and then I print the original sequence and the decoded textã€‚ and
    here we see we have this sequence and this decoded sequence and I think so we
    say we have three peopleã€‚Dight heat way farã€‚ So if we have a look at where did
    I already print itã€‚ So here I printed the samples from 10 to 15ã€‚ So that's the
    original sentenceã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªï¼Œç„¶åæˆ‘æ‰“å°åŸå§‹åºåˆ—å’Œè§£ç æ–‡æœ¬ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°è¿™ä¸ªåºåˆ—å’Œè¿™ä¸ªè§£ç åºåˆ—ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬è¯´æˆ‘ä»¬æœ‰ä¸‰ä¸ªäººã€‚Dightçƒ­é‡è¿œã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æˆ‘å·²ç»æ‰“å°åœ¨å“ªé‡Œã€‚æ‰€ä»¥è¿™é‡Œæˆ‘æ‰“å°äº†ä»10åˆ°15çš„æ ·æœ¬ã€‚å› æ­¤é‚£æ˜¯åŸå§‹å¥å­ã€‚
- en: So we see that our decoding is correctã€‚So now that we have thatã€‚ we can come
    to the actual implementation of the modelã€‚ So last time I showed you that we can
    very easily create an simple R N model or a LSTM model or a GR U modelã€‚ So in
    this caseï¼Œ we use an LSTMã€‚ So for this we create a first a sequential modelã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„è§£ç æ˜¯æ­£ç¡®çš„ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œæ¨¡å‹çš„å®é™…å®ç°ã€‚ä¸Šæ¬¡æˆ‘ç»™ä½ å±•ç¤ºè¿‡ï¼Œæˆ‘ä»¬å¯ä»¥éå¸¸è½»æ¾åœ°åˆ›å»ºä¸€ä¸ªç®€å•çš„R Næ¨¡å‹ã€LSTMæ¨¡å‹æˆ–GR
    Uæ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨LSTMã€‚æ‰€ä»¥ä¸ºæ­¤æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªé¡ºåºæ¨¡å‹ã€‚
- en: And now since we use text dataï¼Œ we also use this embedding layerã€‚ So word embeddingsã€‚
    give us a way to use an efficient dense representation in which similar words
    have a similar encodingã€‚ So if you want to learn more about thisï¼Œ I can recommend
    this official guide in the on the Tensorflow websiteã€‚ So here you see that another
    representation might be one hot encodingã€‚ andã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ç”±äºæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨è¿™ä¸ªåµŒå…¥å±‚ã€‚å› æ­¤ï¼Œå•è¯åµŒå…¥ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„ç¨ å¯†è¡¨ç¤ºæ–¹æ³•ï¼Œå…¶ä¸­ç›¸ä¼¼çš„å•è¯å…·æœ‰ç›¸ä¼¼çš„ç¼–ç ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œæˆ‘å¯ä»¥æ¨èTensorflowç½‘ç«™ä¸Šçš„è¿™ä¸ªå®˜æ–¹æŒ‡å—ã€‚åœ¨è¿™é‡Œä½ ä¼šçœ‹åˆ°å¦ä¸€ç§è¡¨ç¤ºå¯èƒ½æ˜¯ç‹¬çƒ­ç¼–ç ã€‚
- en: Here we simply use a0 or a1ã€‚ but then there's also this embedding representationã€‚
    So with this embedding layerï¼Œ we get this representationã€‚ So a dense vector of
    floating point valuesã€‚ So right now we still have this padded sequence with all
    the word indicesã€‚ And now this embedding layer turns this indices into a dense
    vector of fixed sizeã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨a0æˆ–a1ã€‚ä½†æ˜¯è¿˜æœ‰è¿™ä¸ªåµŒå…¥è¡¨ç¤ºã€‚é€šè¿‡è¿™ä¸ªåµŒå…¥å±‚ï¼Œæˆ‘ä»¬è·å¾—äº†è¿™ä¸ªè¡¨ç¤ºã€‚å› æ­¤ï¼Œå¾—åˆ°ä¸€ä¸ªæµ®ç‚¹å€¼çš„ç¨ å¯†å‘é‡ã€‚ç›®å‰æˆ‘ä»¬ä»ç„¶æœ‰è¿™ä¸ªå¡«å……åºåˆ—ï¼ŒåŒ…å«æ‰€æœ‰çš„å•è¯ç´¢å¼•ã€‚ç°åœ¨è¿™ä¸ªåµŒå…¥å±‚å°†è¿™äº›ç´¢å¼•è½¬æ¢ä¸ºå›ºå®šå¤§å°çš„ç¨ å¯†å‘é‡ã€‚
- en: So that's why we needed this tokenization firstã€‚ and now we can use this embedding
    layerã€‚ So this gets the number of unique wordsã€‚ And then a size that you specify
    and then also the maximum lengthã€‚ and that we specifiedã€‚ So this is the input
    lengthã€‚And now after we define thisã€‚ then we can apply our LSTM or R and N layerï¼Œ
    like the last time where we only specify the number of output unitsã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é¦–å…ˆéœ€è¦è¿™ä¸ªæ ‡è®°åŒ–ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåµŒå…¥å±‚ã€‚è¿™æ ·å¯ä»¥å¾—åˆ°å”¯ä¸€å•è¯çš„æ•°é‡ã€‚ç„¶åæ˜¯ä½ æŒ‡å®šçš„å¤§å°ï¼Œè¿˜æœ‰æœ€å¤§é•¿åº¦ã€‚å°±æ˜¯æˆ‘ä»¬æ‰€æŒ‡å®šçš„ã€‚è¿™æ˜¯è¾“å…¥é•¿åº¦ã€‚ç°åœ¨åœ¨å®šä¹‰ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨æˆ‘ä»¬çš„LSTMæˆ–R
    Nå±‚ï¼Œå°±åƒä¸Šæ¬¡ä¸€æ ·ï¼Œæˆ‘ä»¬åªéœ€æŒ‡å®šè¾“å‡ºå•å…ƒçš„æ•°é‡ã€‚
- en: And here I also said dropout equals 10%ã€‚And then since we want to classify thisã€‚
    So a0 or one classificationï¼Œ we use a dense layer with only one output at the
    endã€‚ And then we also apply the sigmoid functionã€‚ So let's do this and print the
    model summaryã€‚ So we see after our embeddingã€‚ we get the output shape of thisã€‚
    So the number of batchesã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä¹Ÿè¯´äº†dropoutç­‰äº10%ã€‚ç„¶åç”±äºæˆ‘ä»¬æƒ³è¿›è¡Œåˆ†ç±»ï¼Œæ‰€ä»¥a0æˆ–1åˆ†ç±»ï¼Œæˆ‘ä»¬ä½¿ç”¨åªæœ‰ä¸€ä¸ªè¾“å‡ºçš„ç¨ å¯†å±‚ã€‚ç„¶åæˆ‘ä»¬è¿˜åº”ç”¨sigmoidå‡½æ•°ã€‚è®©æˆ‘ä»¬è¿™æ ·åšå¹¶æ‰“å°æ¨¡å‹æ‘˜è¦ã€‚æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°åœ¨åµŒå…¥ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¿™ä¸ªè¾“å‡ºå½¢çŠ¶ã€‚å› æ­¤ï¼Œæ‰¹æ¬¡çš„æ•°é‡ã€‚
- en: And then the 20 is the maximum length and the 32 is just the size that we specified
    here as output sizeã€‚Then our LSTM has this output shape because we specified 64
    output unitsã€‚ and then we have our dense layerã€‚And now since we use binary classificationã€‚
    we use this binary cross entropy loss and here we say from Loit equals falses
    because we already used the activation function here and then again we use a optimizer
    and define the metrics that we want to track and compile the model and then we
    simply train it so we fit it and here we want to use the padded sequence and then
    the corresponding labels and then the epochs and now this is also newã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶å20æ˜¯æœ€å¤§é•¿åº¦ï¼Œ32åªæ˜¯æˆ‘ä»¬åœ¨è¿™é‡ŒæŒ‡å®šçš„è¾“å‡ºå¤§å°ã€‚ç„¶åæˆ‘ä»¬çš„LSTMå…·æœ‰è¿™ä¸ªè¾“å‡ºå½¢çŠ¶ï¼Œå› ä¸ºæˆ‘ä»¬æŒ‡å®šäº†64ä¸ªè¾“å‡ºå•å…ƒã€‚æ¥ç€æˆ‘ä»¬æœ‰æˆ‘ä»¬çš„ç¨ å¯†å±‚ã€‚ç°åœ¨ç”±äºæˆ‘ä»¬ä½¿ç”¨äºŒå…ƒåˆ†ç±»ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªäºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼Œè¿™é‡Œæˆ‘ä»¬è¯´from
    Loitç­‰äºfalseï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿™é‡Œå·²ç»ä½¿ç”¨äº†æ¿€æ´»å‡½æ•°ï¼Œç„¶åå†æ¬¡ä½¿ç”¨ä¼˜åŒ–å™¨å¹¶å®šä¹‰æˆ‘ä»¬æƒ³è¦è·Ÿè¸ªçš„æŒ‡æ ‡ï¼Œç¼–è¯‘æ¨¡å‹ï¼Œç„¶åæˆ‘ä»¬ç®€å•åœ°è®­ç»ƒå®ƒï¼Œå› æ­¤æˆ‘ä»¬è¿›è¡Œæ‹Ÿåˆï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æƒ³ä½¿ç”¨å¡«å……åºåˆ—å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œç„¶åæ˜¯epochsï¼Œç°åœ¨è¿™ä¹Ÿæ˜¯æ–°çš„ã€‚
- en: I think I didn't use this before so you can in this fit method you can use the
    validation data parameter and this is a tuple and here we use the validation padded
    sequence and the validation label and now if we do thisã€‚ then it automatically
    during training uses a validationã€‚Data set to do the fine tuningã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³æˆ‘ä¹‹å‰æ²¡æœ‰ä½¿ç”¨è¿‡è¿™ä¸ªï¼Œå› æ­¤åœ¨è¿™ä¸ªfitæ–¹æ³•ä¸­ä½ å¯ä»¥ä½¿ç”¨éªŒè¯æ•°æ®å‚æ•°ï¼Œè¿™æ˜¯ä¸€å¯¹å…ƒç»„ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨éªŒè¯å¡«å……åºåˆ—å’ŒéªŒè¯æ ‡ç­¾ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬è¿™æ ·åšï¼Œé‚£ä¹ˆå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä½¿ç”¨éªŒè¯æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚
- en: So this is a nice tip to keep in mind that you can already automatically do
    this validation if you specify the validation data hereã€‚So now let's train thisã€‚Al
    rightï¼Œ and training is doneã€‚ And as you can seeã€‚ the final accuracy on the training
    data is 98%ã€‚ so pretty goodã€‚ But for the validation accuracyã€‚ and we only have
    73%ã€‚ So this might be a sign of overfittingã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè®°ä½è¿™ä¸€ç‚¹æ˜¯ä¸ªå¥½ä¸»æ„ï¼Œå¦‚æœä½ åœ¨è¿™é‡ŒæŒ‡å®šéªŒè¯æ•°æ®ï¼Œä½ å¯ä»¥è‡ªåŠ¨è¿›è¡Œæ­¤éªŒè¯ã€‚é‚£ä¹ˆç°åœ¨è®©æˆ‘ä»¬è®­ç»ƒè¿™ä¸ªæ¨¡å‹ã€‚å¥½å§ï¼Œè®­ç»ƒå®Œæˆäº†ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œè®­ç»ƒæ•°æ®çš„æœ€ç»ˆå‡†ç¡®ç‡ä¸º
    98%ã€‚ç›¸å½“ä¸é”™ã€‚ä½†éªŒè¯å‡†ç¡®ç‡åªæœ‰ 73%ã€‚è¿™å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚
- en: So this might be a homework for you that you can further improve or tweakã€‚ the
    model a little bit so that this one also gets higherã€‚ But as we can see ourã€‚ we
    did the correct preprocesing with our text dataã€‚ and we set up a nice LSTM model
    and then get a very nice accuracy hereã€‚ So let's do some predictionã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¯èƒ½æ˜¯ä½ çš„ä¸€é¡¹ä½œä¸šï¼Œä½ å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›æˆ–ç¨å¾®è°ƒæ•´æ¨¡å‹ï¼Œä»¥ä½¿å…¶è¡¨ç°æ›´å¥½ã€‚ä½†æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œäº†æ­£ç¡®çš„é¢„å¤„ç†ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªä¸é”™çš„ LSTM
    æ¨¡å‹ï¼Œå› æ­¤è¿™é‡Œå¾—åˆ°äº†å¾ˆå¥½çš„å‡†ç¡®ç‡ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è¿›è¡Œä¸€äº›é¢„æµ‹ã€‚
- en: So we simply call model predict on theã€‚ğŸ˜Šï¼ŒTraining padded sequence in this caseã€‚
    And as I saidã€‚ we used the sigmoid function at the endã€‚ So we still have to convert
    this to a label 0 or1ã€‚ So we simply say if our predicted output probability is
    higher than 0ã€‚5ã€‚ then it's one and otherwise 0ã€‚ And now let's print some original
    training sentencesã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ç®€å•åœ°åœ¨è¿™é‡Œè°ƒç”¨æ¨¡å‹çš„é¢„æµ‹ã€‚ğŸ˜Šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯è®­ç»ƒçš„å¡«å……åºåˆ—ã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬åœ¨æœ€åä½¿ç”¨äº† sigmoid å‡½æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾
    0 æˆ– 1ã€‚æ‰€ä»¥æˆ‘ä»¬ç®€å•åœ°è¯´ï¼Œå¦‚æœæˆ‘ä»¬çš„é¢„æµ‹è¾“å‡ºæ¦‚ç‡é«˜äº 0.5ï¼Œé‚£ä¹ˆå°±æ˜¯ 1ï¼Œå¦åˆ™å°±æ˜¯ 0ã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°ä¸€äº›åŸå§‹çš„è®­ç»ƒå¥å­ã€‚
- en: and the corresponding labels and the predictionsã€‚So hereï¼Œ yeahã€‚ so we see five
    of these are classified as a as a disaster and the other one are no disasters
    and all of our corrections are correct in this caseã€‚ So we say for so we seeï¼Œ
    for example here we we have three people diedï¼Œ blahï¼Œ blahï¼Œ blahã€‚ So this is a
    disasterã€‚ And here Ta getting floodedï¼Œ also a disasterã€‚ And here at the endã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠç›¸åº”çš„æ ‡ç­¾å’Œé¢„æµ‹ç»“æœã€‚åœ¨è¿™é‡Œï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬çœ‹åˆ°äº”ä¸ªè¢«åˆ†ç±»ä¸ºç¾éš¾ï¼Œå¦ä¸€ä¸ªåˆ™ä¸æ˜¯ç¾éš¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ‰€æœ‰ä¿®æ­£éƒ½æ˜¯æ­£ç¡®çš„ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥è¯´ï¼Œä¾‹å¦‚ï¼Œè¿™é‡Œæœ‰ä¸‰ä¸ªäººæ­»äº¡ï¼Œç­‰ç­‰ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªç¾éš¾ã€‚è€Œè¿™é‡Œï¼Œæ°´ç¾ä¹Ÿæ˜¯ä¸€ä¸ªç¾éš¾ã€‚æœ€åè¿™é‡Œã€‚
- en: we have some a lovelyï¼Œ no disasterã€‚ So yeahï¼Œ it looks goodã€‚ And yeahã€‚ I think
    we learned a lot in this tutorialã€‚ Now you know how to apply some basic and natural
    language processing techniques and then use an LSTM for text classificationã€‚And
    I hope you enjoyed this tutorialã€‚ If you liked itã€‚ then please hit the like button
    and consider subscribing to the channelã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€äº›å¯çˆ±çš„ï¼Œæ²¡æœ‰ç¾éš¾çš„ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œçœ‹èµ·æ¥ä¸é”™ã€‚è€Œä¸”æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬åœ¨æœ¬æ•™ç¨‹ä¸­å­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ã€‚ç°åœ¨ä½ çŸ¥é“å¦‚ä½•åº”ç”¨ä¸€äº›åŸºæœ¬çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œç„¶åä½¿ç”¨
    LSTM è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚å¦‚æœä½ å–œæ¬¢ï¼Œè¯·ç‚¹å‡»ç‚¹èµæŒ‰é’®ï¼Œå¹¶è€ƒè™‘è®¢é˜…é¢‘é“ã€‚
- en: And then I hope to see you in the next videoï¼Œ byeã€‚ğŸ˜Šã€‚![](img/cf77d9b01a43fab5f293c40eb502060f_2.png)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¸Œæœ›åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­è§åˆ°ä½ ï¼Œå†è§ã€‚ğŸ˜Šã€‚![](img/cf77d9b01a43fab5f293c40eb502060f_2.png)
