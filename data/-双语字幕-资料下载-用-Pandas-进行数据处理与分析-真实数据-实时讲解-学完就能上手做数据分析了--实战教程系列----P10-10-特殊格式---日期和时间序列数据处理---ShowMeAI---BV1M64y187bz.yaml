- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Pandas è¿›è¡Œæ•°æ®å¤„ç†ä¸åˆ†æï¼çœŸå®æ•°æ®&å®æ—¶è®²è§£ï¼Œå­¦å®Œå°±èƒ½ä¸Šæ‰‹åšæ•°æ®åˆ†æäº†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P10ï¼š10ï¼‰ç‰¹æ®Šæ ¼å¼
    - æ—¥æœŸå’Œæ—¶é—´åºåˆ—æ•°æ®å¤„ç† - ShowMeAI - BV1M64y187bz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Pandas è¿›è¡Œæ•°æ®å¤„ç†ä¸åˆ†æï¼çœŸå®æ•°æ®&å®æ—¶è®²è§£ï¼Œå­¦å®Œå°±èƒ½ä¸Šæ‰‹åšæ•°æ®åˆ†æäº†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P10ï¼š10ï¼‰ç‰¹æ®Šæ ¼å¼
    - æ—¥æœŸå’Œæ—¶é—´åºåˆ—æ•°æ®å¤„ç† - ShowMeAI - BV1M64y187bz
- en: Hey thereã€‚ how's it goingï¼Œ everybodyã€‚ In this videoã€‚ we're gonna be learning
    how to work with date and time series data within pandasã€‚ Nowã€‚ there's a ton of
    interesting stuff that we can do with datetime dataã€‚ and we'll be learning about
    that hereã€‚ So we'll learn how to properly read in our data so that we can use
    date time functionalityã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿåœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åœ¨ Pandas ä¸­å¤„ç†æ—¥æœŸå’Œæ—¶é—´åºåˆ—æ•°æ®ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ—¥æœŸæ—¶é—´æ•°æ®åšå¾ˆå¤šæœ‰è¶£çš„äº‹æƒ…ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œå­¦ä¹ è¿™äº›å†…å®¹ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•æ­£ç¡®è¯»å–æ•°æ®ï¼Œä»¥ä¾¿èƒ½å¤Ÿä½¿ç”¨æ—¥æœŸæ—¶é—´åŠŸèƒ½ã€‚
- en: We'll also see how to filter by date times how to group dates by resampling
    the time frames and we'll also take a look at doing some simple plotting with
    our time series data as wellã€‚ Nowï¼Œ I'd like to mention that we do have a sponsor
    for the series of videosã€‚ And that is brilliantã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†çœ‹åˆ°å¦‚ä½•æŒ‰æ—¥æœŸæ—¶é—´è¿›è¡Œç­›é€‰ï¼Œå¦‚ä½•é€šè¿‡é‡æ–°é‡‡æ ·æ—¶é—´æ¡†æ¶å¯¹æ—¥æœŸè¿›è¡Œåˆ†ç»„ï¼Œæˆ‘ä»¬è¿˜ä¼šçœ‹çœ‹å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œä¸€äº›ç®€å•çš„ç»˜å›¾ã€‚ç°åœ¨ï¼Œæˆ‘æƒ³æåˆ°çš„æ˜¯ï¼Œæˆ‘ä»¬ç¡®å®æœ‰ä¸€ä¸ªèµåŠ©å•†ä¸ºè¿™ä¸ªç³»åˆ—è§†é¢‘æä¾›æ”¯æŒï¼Œé‚£å°±æ˜¯**Brilliant**ã€‚
- en: So I really want to thank brilliant for sponsoring this seriesã€‚ and it would
    be great if you all can check them out using the link in the description section
    below and support the sponsorsã€‚ And I'll talk more about their services in just
    a bitã€‚ So at that saidã€‚ let's go ahead and get startedã€‚ Okayï¼Œ so first of allã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœŸçš„æƒ³æ„Ÿè°¢ **Brilliant** èµåŠ©è¿™ä¸ªç³»åˆ—ã€‚å¦‚æœä½ ä»¬èƒ½é€šè¿‡ä¸‹é¢æè¿°éƒ¨åˆ†çš„é“¾æ¥æŸ¥çœ‹ä»–ä»¬çš„å†…å®¹å¹¶æ”¯æŒèµåŠ©å•†ï¼Œé‚£å°†æ˜¯å¤ªå¥½äº†ã€‚æˆ‘ä¼šåœ¨ç¨åè°ˆåˆ°ä»–ä»¬çš„æœåŠ¡ã€‚æ‰€ä»¥è¯´å®Œè¿™äº›ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚å¥½çš„ï¼Œé¦–å…ˆã€‚
- en: I've been using the stack overflow survey data for this entire series so farã€‚
    But that data set doesn't actually have any date or time series dataã€‚ So I had
    to choose a different data set for this videoã€‚ I downloaded some historicalã€‚ğŸ˜Šã€‚Cryptocurrency
    data that we can analyze for this videoã€‚ and as usualã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä¸€ç›´åœ¨ä½¿ç”¨ Stack Overflow è°ƒæŸ¥æ•°æ®ã€‚ä½†é‚£ä¸ªæ•°æ®é›†å®é™…ä¸Šæ²¡æœ‰ä»»ä½•æ—¥æœŸæˆ–æ—¶é—´åºåˆ—æ•°æ®ã€‚æ‰€ä»¥æˆ‘ä¸å¾—ä¸ä¸ºè¿™ä¸ªè§†é¢‘é€‰æ‹©ä¸€ä¸ªä¸åŒçš„æ•°æ®é›†ã€‚æˆ‘ä¸‹è½½äº†ä¸€äº›å†å²çš„åŠ å¯†è´§å¸æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªè§†é¢‘ä¸­è¿›è¡Œåˆ†æï¼Œå’Œå¾€å¸¸ä¸€æ ·ã€‚
- en: I'm gonna have links to download the data and the notebooks that I'm using in
    the description section belowã€‚ So I've got my notebook opened up here where I'm
    reading in this CV file of data and let's go ahead and take a look at what this
    looks likeã€‚ So we can see here that I'm loading in this cV file and I called this
    ETH underscore1 H and that's because this is historical data for Ethereum which
    is a cryptocurrency and and this data is broken down on one hour segmentsã€‚ So
    if we look down here at the head of this dataã€‚ we can see that we have some columns
    hereã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†æä¾›ä¸‹è½½æ•°æ®å’Œæˆ‘æ­£åœ¨ä½¿ç”¨çš„ç¬”è®°æœ¬çš„é“¾æ¥ã€‚æ‰€ä»¥æˆ‘å·²ç»æ‰“å¼€äº†æˆ‘çš„ç¬”è®°æœ¬ï¼Œæ­£åœ¨è¯»å–è¿™ä¸ª CV æ–‡ä»¶çš„æ•°æ®ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹è¿™æ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘æ­£åœ¨åŠ è½½è¿™ä¸ª
    CV æ–‡ä»¶ï¼Œæˆ‘å°†å…¶å‘½åä¸º ETH_1Hï¼Œå› ä¸ºè¿™æ˜¯ä»¥å°æ—¶ä¸ºå•ä½çš„ä»¥å¤ªåŠå†å²æ•°æ®ï¼Œè¿™æ˜¯ä¸€ç§åŠ å¯†è´§å¸ã€‚è¿™äº›æ•°æ®è¢«åˆ†è§£ä¸ºæ¯å°æ—¶çš„æ®µè½ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªæ•°æ®çš„å¤´éƒ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ä¸€äº›åˆ—åœ¨è¿™é‡Œã€‚
- en: the first one is a date column and these are broken down by the hour we also
    have some other information here like the symbols the open and closing values
    for these hoursã€‚ the highs and lows and also the volume we so all of this here
    is for let's see March 13 and this is for 8 PMm7 PMã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ˜¯æ—¥æœŸåˆ—ï¼Œè¿™äº›æ•°æ®æŒ‰å°æ—¶åˆ†è§£ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€äº›å…¶ä»–ä¿¡æ¯ï¼Œæ¯”å¦‚ç¬¦å·ã€è¿™äº›å°æ—¶çš„å¼€ç›˜å’Œæ”¶ç›˜å€¼ã€æœ€é«˜å’Œæœ€ä½å€¼ï¼Œä»¥åŠäº¤æ˜“é‡ã€‚å› æ­¤ï¼Œè¿™é‡Œæ‰€æœ‰æ•°æ®éƒ½æ˜¯å…³äº3æœˆ13æ—¥çš„ï¼Œæ—¶é—´æ˜¯æ™šä¸Š8ç‚¹ã€7ç‚¹ã€‚
- en: 6 PMm and so onã€‚ Nowï¼Œ rememberï¼Œ if you want to see more information about your
    data frameã€‚ Soã€‚ for exampleï¼Œ how many rows and columns there areï¼Œ I can run Df
    do shapeã€‚ and we can see that there are 23000 rows here almost 24000 So a good
    bit of data for us to work withã€‚ so now let's actually get into working with datetime
    dataã€‚ So we have this date column hereã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 6ç‚¹ç­‰ç­‰ã€‚ç°åœ¨ï¼Œè®°ä½ï¼Œå¦‚æœä½ æƒ³æŸ¥çœ‹æ›´å¤šå…³äºä½ çš„æ•°æ®æ¡†çš„ä¿¡æ¯ï¼Œæ¯”å¦‚æœ‰å¤šå°‘è¡Œå’Œåˆ—ï¼Œæˆ‘å¯ä»¥è¿è¡Œ `df.shape`ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰23000è¡Œï¼Œå·®ä¸å¤š24000ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰ç›¸å½“å¤šçš„æ•°æ®å¯ä»¥å¤„ç†ã€‚ç°åœ¨æˆ‘ä»¬å°±å¼€å§‹å¤„ç†æ—¥æœŸæ—¶é—´æ•°æ®å§ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œæœ‰è¿™ä¸ªæ—¥æœŸåˆ—ã€‚
- en: and it looks like this is just giving us every hour of the dayã€‚ but right now
    this isn't actually a datetime objectã€‚ I can kind of tell this just because it's
    not in a format that date times usually display as but if you want to be sure
    you can always try running ada data pandas datetime method on this to see if it
    worksã€‚ So let me just grab the first row of this data frame and I'll grab that
    date valueã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥åªæ˜¯ç»™æˆ‘ä»¬æ¯ä¸ªå°æ—¶çš„æ•°æ®ï¼Œä½†ç°åœ¨è¿™å®é™…ä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªæ—¥æœŸæ—¶é—´å¯¹è±¡ã€‚æˆ‘ä¹‹æ‰€ä»¥èƒ½è¿™æ ·åˆ¤æ–­ï¼Œæ˜¯å› ä¸ºå®ƒçš„æ ¼å¼å¹¶ä¸æ˜¯æ—¥æœŸæ—¶é—´é€šå¸¸æ˜¾ç¤ºçš„æ ¼å¼ï¼Œä½†å¦‚æœä½ æƒ³ç¡®è®¤ï¼Œå¯ä»¥å°è¯•åœ¨è¿™ä¸ªæ•°æ®ä¸Šè¿è¡Œ
    ada æ•°æ® pandas æ—¥æœŸæ—¶é—´æ–¹æ³•ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰æ•ˆã€‚æ‰€ä»¥è®©æˆ‘å…ˆè·å–è¿™ä¸ªæ•°æ®æ¡†çš„ç¬¬ä¸€è¡Œï¼Œå¹¶æŠ“å–é‚£ä¸ªæ—¥æœŸå€¼ã€‚
- en: So and then I'll go ahead and try to run a datetime methodã€‚ So to grab that
    first valueã€‚ I'm just gonna say Df Lo and we can see here that theã€‚X is just zero
    over hereã€‚ so I'm just going to pass in a0ã€‚ and I want to grab that date column
    thereã€‚ so if I run what we have nowï¼Œ then we can see that I've plucked out that
    first dateã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å°†ç»§ç»­å°è¯•è¿è¡Œä¸€ä¸ªæ—¥æœŸæ—¶é—´æ–¹æ³•ã€‚ä¸ºäº†è·å–ç¬¬ä¸€ä¸ªå€¼ï¼Œæˆ‘åªéœ€è¯´`Df Lo`ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„`.X`åªæ˜¯é›¶ï¼Œå› æ­¤æˆ‘å°†ä¼ å…¥`a0`ï¼Œå¹¶æƒ³è¦æŠ“å–é‚£ä¸ªæ—¥æœŸåˆ—ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘è¿è¡Œç°åœ¨çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘å·²ç»æå–å‡ºäº†ç¬¬ä¸€ä¸ªæ—¥æœŸã€‚
- en: So now let's just try to run a datetime method on this So there's one method
    called day name that will give us the weekday that this date fell on but if I
    run this now and I say okay dot day name for this value hereã€‚ if I run this then
    we can see that we get an error and it says that a string object has no attribute
    day name and that's because we are reading this in as a string currently So how
    do we convert this to a date timeã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å°±è¯•ç€åœ¨è¿™ä¸ªä¸Šè¿è¡Œä¸€ä¸ªæ—¥æœŸæ—¶é—´æ–¹æ³•ã€‚æœ‰ä¸€ä¸ªå«`day_name`çš„æ–¹æ³•å¯ä»¥å‘Šè¯‰æˆ‘ä»¬è¿™ä¸ªæ—¥æœŸæ˜¯æ˜ŸæœŸå‡ ï¼Œä½†å¦‚æœæˆ‘ç°åœ¨è¿è¡Œè¿™ä¸ªï¼Œå¹¶è¯´`ok.day_name`å¯¹è¿™ä¸ªå€¼ï¼Œå¦‚æœæˆ‘è¿è¡Œå®ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‡ºç°äº†ä¸€ä¸ªé”™è¯¯ï¼Œæç¤ºå­—ç¬¦ä¸²å¯¹è±¡æ²¡æœ‰å±æ€§`day_name`ï¼Œå› ä¸ºæˆ‘ä»¬å½“å‰å°†å…¶ä½œä¸ºå­—ç¬¦ä¸²è¯»å–ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å°†å…¶è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´å‘¢ï¼Ÿ
- en: So there's a few different ways that we can do this and we'll go over some of
    those hereã€‚ Now if you want to convert a column like we have here to a date time
    then we can use the pandas to underscore datetime method So to do this I can simply
    say we'll access that date columnã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨å‡ ç§ä¸åŒçš„æ–¹å¼æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œè®¨è®ºå…¶ä¸­ä¸€äº›ã€‚å¦‚æœä½ æƒ³åƒæˆ‘ä»¬è¿™é‡Œçš„é‚£æ ·å°†ä¸€åˆ—è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`pandas`çš„`to_datetime`æ–¹æ³•ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å¯ä»¥ç®€å•åœ°è¯´æˆ‘ä»¬å°†è®¿é—®é‚£ä¸ªæ—¥æœŸåˆ—ã€‚
- en: We'll set this date column equal toã€‚ and then we'll just say PD for what we
    imported pandas as to underscore date timeã€‚ And now I want to pass in that same
    column to convert that to a date timeã€‚ Nowã€‚ I'm not going to run this right nowï¼Œ
    because if I run this as is then pandas would do its best to figure out the formatting
    of the date time and convert it accordinglyã€‚ but the date time that I have here
    is in a pretty different formatã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™ä¸ªæ—¥æœŸåˆ—è®¾ç½®ä¸ºï¼Œç„¶åæˆ‘ä»¬åªéœ€è¯´`pd`ï¼Œè¿™æ˜¯æˆ‘ä»¬å¯¼å…¥çš„`pandas`çš„`to_datetime`ã€‚ç°åœ¨æˆ‘æƒ³ä¼ å…¥é‚£ä¸ªç›¸åŒçš„åˆ—ä»¥å°†å…¶è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ã€‚ç°åœ¨ï¼Œæˆ‘ä¸ä¼šç«‹å³è¿è¡Œè¿™ä¸ªï¼Œå› ä¸ºå¦‚æœæˆ‘è¿™æ ·è¿è¡Œï¼Œpandaså°†å°½åŠ›å¼„æ¸…æ¥šæ—¥æœŸæ—¶é—´çš„æ ¼å¼å¹¶ç›¸åº”åœ°è½¬æ¢ï¼Œä½†æˆ‘è¿™é‡Œçš„æ—¥æœŸæ—¶é—´æ ¼å¼ç›¸å½“ä¸åŒã€‚
- en: So I doubt that this is going workã€‚ but let's go ahead and try it out anywayã€‚
    Okayã€‚ so I expected to get an errorã€‚ if we scroll down and look at the error hereã€‚
    we can see that it says unknown string formatã€‚ So it did not know how to parse
    this dateã€‚ But like I said beforeï¼Œ depending on how your dates are formattedã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€€ç–‘è¿™è¡Œä¸é€šï¼Œä½†æˆ‘ä»¬è¿˜æ˜¯è¯•è¯•çœ‹å§ã€‚å¥½å§ï¼Œæˆ‘é¢„æœŸä¼šå‡ºç°ä¸€ä¸ªé”™è¯¯ã€‚å¦‚æœæˆ‘ä»¬å‘ä¸‹æ»šåŠ¨æŸ¥çœ‹é”™è¯¯ï¼Œå¯ä»¥çœ‹åˆ°å®ƒæç¤ºæœªçŸ¥å­—ç¬¦ä¸²æ ¼å¼ã€‚å› æ­¤ï¼Œå®ƒä¸çŸ¥é“å¦‚ä½•è§£æè¿™ä¸ªæ—¥æœŸã€‚ä½†æ­£å¦‚æˆ‘ä¹‹å‰æ‰€è¯´ï¼Œå–å†³äºä½ çš„æ—¥æœŸæ ¼å¼ã€‚
- en: then that might actually work for youã€‚ this just so happens to be format it
    in a way that pandas can't convert this automaticallyã€‚ without us telling it how
    our date is formattedã€‚ So what we need to do here is pass in a format stringã€‚ifying
    how dates are formatted so that it can parse this correctlyã€‚ Now I went ahead
    and I created the correct string format ahead of time for this specific dateã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™å®é™…ä¸Šå¯èƒ½é€‚åˆä½ ã€‚æ°å¥½æ˜¯ä»¥ä¸€ç§æ ¼å¼æ˜¾ç¤ºï¼Œpandasæ— æ³•è‡ªåŠ¨è½¬æ¢ï¼Œé™¤éæˆ‘ä»¬å‘Šè¯‰å®ƒæˆ‘ä»¬çš„æ—¥æœŸæ˜¯å¦‚ä½•æ ¼å¼åŒ–çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œä¼ å…¥ä¸€ä¸ªæ ¼å¼å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ—¥æœŸæ˜¯å¦‚ä½•æ ¼å¼åŒ–çš„ï¼Œä»¥ä¾¿å®ƒèƒ½å¤Ÿæ­£ç¡®è§£æã€‚ç°åœ¨æˆ‘æå‰ä¸ºè¿™ä¸ªç‰¹å®šæ—¥æœŸåˆ›å»ºäº†æ­£ç¡®çš„å­—ç¬¦ä¸²æ ¼å¼ã€‚
- en: But just to be clearï¼Œ I never really remember these formatting codes off the
    top of my headã€‚ I always need to go and find these codes within the Python documentation
    So I have that page open hereã€‚ And I will leave a link to this in the description
    section as wellã€‚ but however your date is formatted hereã€‚ So ours started with
    the yearã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡è¦æ˜ç¡®çš„æ˜¯ï¼Œæˆ‘ä»æ¥æ²¡æœ‰çœŸçš„è®°ä½è¿™äº›æ ¼å¼åŒ–ä»£ç ã€‚ æˆ‘æ€»æ˜¯éœ€è¦å»Pythonæ–‡æ¡£ä¸­æŸ¥æ‰¾è¿™äº›ä»£ç ã€‚æ‰€ä»¥æˆ‘è¿™é‡Œæ‰“å¼€äº†é‚£ä¸ªé¡µé¢ã€‚æˆ‘ä¼šåœ¨æè¿°éƒ¨åˆ†ç•™ä¸‹è¿™ä¸ªé“¾æ¥ã€‚ä¸ç®¡ä½ çš„æ—¥æœŸæ˜¯å¦‚ä½•æ ¼å¼åŒ–çš„ï¼Œæˆ‘ä»¬çš„æ—¥æœŸæ˜¯ä»¥å¹´ä»½å¼€å¤´çš„ã€‚
- en: so we can see that that is a percent Y and then we have the month day so we
    can find that in hereã€‚ another one is that we have like 8 pm and things like thatã€‚
    so we can see here that these eyes hereã€‚ this I is for a 12 hour clock which is
    what ours is doingã€‚ and then this percent sign P is for the local equivalent of
    a or pmã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‚£æ˜¯`%Y`ï¼Œç„¶åæˆ‘ä»¬æœ‰æœˆæ—¥ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯åƒ`8 PM`è¿™æ ·çš„æ ¼å¼ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„è¿™äº›`I`ï¼Œè¿™ä¸ª`I`æ˜¯æŒ‡12å°æ—¶åˆ¶ï¼Œè¿™æ˜¯æˆ‘ä»¬æ‰€ç”¨çš„ã€‚ç„¶åè¿™ä¸ª`%p`æ˜¯æŒ‡å½“åœ°çš„ä¸Šåˆæˆ–ä¸‹åˆçš„è¡¨ç¤ºã€‚
- en: So those are going to be in our format string but I'll leave a link to thisã€‚Just
    in case your date formatting is different and you need to create your own So the
    format string that I need to pass in here and againã€‚ this is basically just telling
    pandas how to parse our date we're going to say that first we're going to see
    the year and then a dash and then the monthã€‚And then the day with a dash in between
    that and then a space and then percent I was that 12 hour clockã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›å°†ä¼šåœ¨æˆ‘ä»¬çš„æ ¼å¼å­—ç¬¦ä¸²ä¸­ï¼Œä½†æˆ‘ä¼šç•™ä¸€ä¸ªé“¾æ¥ä»¥é˜²ä¸‡ä¸€ä½ çš„æ—¥æœŸæ ¼å¼ä¸åŒï¼Œéœ€è¦ä½ è‡ªå·±åˆ›å»ºã€‚å› æ­¤æˆ‘éœ€è¦ä¼ å…¥çš„æ ¼å¼å­—ç¬¦ä¸²åŸºæœ¬ä¸Šæ˜¯åœ¨å‘Šè¯‰ pandas å¦‚ä½•è§£ææˆ‘ä»¬çš„æ—¥æœŸï¼Œæˆ‘ä»¬å°†å…ˆçœ‹åˆ°å¹´ä»½ï¼Œç„¶åæ˜¯ä¸€ä¸ªç ´æŠ˜å·ï¼Œç„¶åæ˜¯æœˆä»½ï¼Œå†ç„¶åæ˜¯æ—¥æœŸï¼Œä¸­é—´æœ‰ç ´æŠ˜å·ï¼Œç„¶åæ˜¯ä¸€ä¸ªç©ºæ ¼ï¼Œæ¥ç€æ˜¯ç™¾åˆ†æ¯”ï¼Œæˆ‘è®°å¾—æ˜¯12å°æ—¶åˆ¶ã€‚
- en: And then there is a dashã€‚ and then it is percent Pã€‚ So let me go ahead and run
    thisã€‚ And if I put this in correctly then this should workã€‚ Okayï¼Œ so we didn't
    get any errors thereã€‚ But let's go ahead and make sureã€‚ So I'm going to go ahead
    and look at the date column hereã€‚ And we can see that now these look more like
    datetime objects that we might be used to seeing in programmingã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ˜¯ä¸€ä¸ªç ´æŠ˜å·ï¼Œç„¶åæ˜¯ç™¾åˆ†æ¯” Pã€‚æ‰€ä»¥è®©æˆ‘æ¥è¿è¡Œè¿™ä¸ªã€‚å¦‚æœæˆ‘æŠŠå®ƒæ­£ç¡®åœ°è¾“å…¥ï¼Œé‚£ä¹ˆè¿™ä¸ªåº”è¯¥ä¼šå·¥ä½œã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æ²¡æœ‰å¾—åˆ°ä»»ä½•é”™è¯¯ã€‚ä½†è®©æˆ‘ä»¬ç¡®ä¿ä¸€ä¸‹ã€‚å› æ­¤æˆ‘å°†æŸ¥çœ‹è¿™é‡Œçš„æ—¥æœŸåˆ—ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™äº›ç°åœ¨çœ‹èµ·æ¥æ›´åƒæ˜¯æˆ‘ä»¬åœ¨ç¼–ç¨‹ä¸­ä¹ æƒ¯çœ‹åˆ°çš„
    datetime å¯¹è±¡ã€‚
- en: So it converted 11 pm to 23ã€‚ wellï¼Œ I'm sorryï¼Œ I thought 11 pm was the first
    oneã€‚ Noï¼Œ it's 8 pã€‚ Okayã€‚ so it converted 8 pm to 20 and 7 pm to 19 and so onã€‚
    And now that this is converted to a datetimeã€‚ we should be able to run these datetime
    methods that gave us an error beforeã€‚ So up here where we got this error where
    we tried to grab the day name for theseã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒå°†æ™šä¸Š11ç‚¹è½¬æ¢ä¸º23ã€‚æŠ±æ­‰ï¼Œæˆ‘ä»¥ä¸ºæ™šä¸Š11ç‚¹æ˜¯ç¬¬ä¸€ä¸ªã€‚ä¸ï¼Œå®ƒæ˜¯æ™šä¸Š8ç‚¹ã€‚å¥½çš„ã€‚æ‰€ä»¥å®ƒå°†æ™šä¸Š8ç‚¹è½¬æ¢ä¸º20ï¼Œæ™šä¸Š7ç‚¹è½¬æ¢ä¸º19ï¼Œä»¥æ­¤ç±»æ¨ã€‚ç°åœ¨è¿™å·²è½¬æ¢ä¸º
    datetimeï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿè¿è¡Œè¿™äº›ä¹‹å‰ç»™æˆ‘ä»¬å¸¦æ¥é”™è¯¯çš„ datetime æ–¹æ³•ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬å¾—åˆ°äº†è¿™ä¸ªé”™è¯¯ï¼Œå½“æ—¶æˆ‘ä»¬å°è¯•æŠ“å–è¿™äº›çš„æ—¥æœŸåç§°ã€‚
- en: I'm just going to copy that and paste that in down here and now let's try to
    rerun this and we can see that now it's saying that that first date in our series
    hereã€‚ this March 13 was a Friday so that's nice So it looked like it worksã€‚ Now
    the way that we did this here is that we converted this to a date after we loaded
    in our data with this line right here but if we wanted to convert this to a date
    as we're loading in our dataã€‚ then we can also do that as wellã€‚ So if I go up
    here to the top where we loaded this in at this read CSsv line hereã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å¤åˆ¶è¿™ä¸ªå¹¶ç²˜è´´åˆ°è¿™é‡Œï¼Œç°åœ¨è®©æˆ‘ä»¬å°è¯•é‡æ–°è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç°åœ¨å®ƒè¡¨ç¤ºæˆ‘ä»¬ç³»åˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæ—¥æœŸï¼Œè¿™ä¸ª 3 æœˆ 13 æ—¥æ˜¯ä¸€ä¸ªæ˜ŸæœŸäº”ï¼Œæ‰€ä»¥è¿™å¾ˆå¥½ï¼Œçœ‹èµ·æ¥å®ƒå·¥ä½œäº†ã€‚ç°åœ¨æˆ‘ä»¬è¿™æ ·åšçš„æ–¹å¼æ˜¯ï¼Œåœ¨ç”¨è¿™ä¸€è¡ŒåŠ è½½æ•°æ®åå°†å…¶è½¬æ¢ä¸ºæ—¥æœŸï¼Œä½†å¦‚æœæˆ‘ä»¬å¸Œæœ›åœ¨åŠ è½½æ•°æ®æ—¶å°±å°†å…¶è½¬æ¢ä¸ºæ—¥æœŸï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå¯ä»¥è¿™æ ·åšã€‚æ‰€ä»¥å¦‚æœæˆ‘å›åˆ°é¡¶éƒ¨ï¼Œåœ¨è¿™é‡ŒåŠ è½½çš„è¿™è¡Œè¯»å–
    CSVã€‚
- en: then I can actually pass in some arguments to read CSv so that it loads in certain
    columns as date times and then we can pass in our formatting string as well so
    that it parses those as the data is read in so to do this we need to pass in this
    parse dates argument here and now I'm just going pass in a list of the columnsã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¯ä»¥å®é™…ä¸Šä¼ å…¥ä¸€äº›å‚æ•°æ¥è¯»å– CSVï¼Œä»¥ä¾¿å®ƒå°†æŸäº›åˆ—åŠ è½½ä¸ºæ—¥æœŸæ—¶é—´ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä¼ å…¥æˆ‘ä»¬çš„æ ¼å¼å­—ç¬¦ä¸²ï¼Œä»¥ä¾¿åœ¨è¯»å–æ•°æ®æ—¶è§£æå®ƒä»¬ã€‚å› æ­¤ä¸ºæ­¤æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œä¼ å…¥è¿™ä¸ªè§£ææ—¥æœŸå‚æ•°ï¼Œç°åœ¨æˆ‘åªæ˜¯è¦ä¼ å…¥ä¸€ä¸ªåˆ—çš„åˆ—è¡¨ã€‚
- en: arere going to be datesã€‚ We only have one hereï¼Œ so it's just going to be a list
    of one item oopsã€‚ and I meant to put dateï¼Œ not dates andã€‚Nowï¼Œ just like with beforeã€‚
    if your dates are already formatted in a way that pandas can parse themã€‚ then
    you don't need to add anything else hereã€‚ but we already saw before that we need
    to pass in a specific formatã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¼šæœ‰æ—¥æœŸã€‚æˆ‘ä»¬è¿™é‡Œåªæœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥è¿™å°†åªæ˜¯ä¸€ä¸ªåŒ…å«ä¸€ä¸ªé¡¹ç›®çš„åˆ—è¡¨ã€‚æŠ±æ­‰ï¼Œæˆ‘æœ¬æ¥æƒ³å†™æ—¥æœŸï¼Œè€Œä¸æ˜¯æ—¥æœŸä»¬ã€‚ç°åœ¨ï¼Œå°±åƒä¹‹å‰ä¸€æ ·ï¼Œå¦‚æœä½ çš„æ—¥æœŸå·²ç»ä»¥ pandas
    èƒ½è§£æçš„æ–¹å¼æ ¼å¼åŒ–ï¼Œé‚£ä¹ˆä½ è¿™é‡Œä¸éœ€è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚ä½†æˆ‘ä»¬ä¹‹å‰å·²ç»çœ‹åˆ°éœ€è¦ä¼ å…¥ç‰¹å®šçš„æ ¼å¼ã€‚
- en: So to do this hereï¼Œ we can't just pass in a format string We instead need to
    pass in a function that converts each string to a datetime objectã€‚ So firstï¼Œ let's
    create that functionã€‚ And we've seen lambda functions in this series beforeã€‚ but
    just in case you're unfamiliar with thoseã€‚ you can simply create a normal function
    instead if you are more comfortable with thoseã€‚ but this is just a shorter wayã€‚
    So to create this lambda functionã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬ä¸èƒ½ä»…ä»…ä¼ å…¥æ ¼å¼å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬éœ€è¦ä¼ å…¥ä¸€ä¸ªå°†æ¯ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸º datetime å¯¹è±¡çš„å‡½æ•°ã€‚æ‰€ä»¥é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºé‚£ä¸ªå‡½æ•°ã€‚åœ¨è¿™ä¸ªç³»åˆ—ä¸­æˆ‘ä»¬è§è¿‡
    lambda å‡½æ•°ï¼Œä½†ä»¥é˜²ä½ ä¸ç†Ÿæ‚‰ï¼Œä½ ä¹Ÿå¯ä»¥ç®€å•åœ°åˆ›å»ºä¸€ä¸ªæ™®é€šå‡½æ•°ï¼Œå¦‚æœä½ æ›´ä¹ æƒ¯è¿™æ ·ï¼Œä½†è¿™åªæ˜¯ä¸€ç§æ›´ç®€çŸ­çš„æ–¹å¼ã€‚æ‰€ä»¥è¦åˆ›å»ºè¿™ä¸ª lambda å‡½æ•°ã€‚
- en: I'm just going call this D underscore parserï¼Œ I'm going to set this equal to
    a lambda functionã€‚ And I'll just use x as the variable hereã€‚ and now what do we
    want to returnã€‚ So when we used P do2 date time down hereã€‚ we actually pass in
    an entire series to PD2 dateã€‚TimeBut now this is actually just going to be each
    individual string and it's going to send each individual string through this functionã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šæŠŠè¿™ä¸ªå«åš D underscore parserï¼Œæˆ‘å°†æŠŠå®ƒè®¾ç½®ä¸ºä¸€ä¸ª lambda å‡½æ•°ã€‚æˆ‘ä¼šæŠŠ x ç”¨ä½œè¿™é‡Œçš„å˜é‡ã€‚é‚£ä¹ˆæˆ‘ä»¬æƒ³è¦è¿”å›ä»€ä¹ˆå‘¢ï¼Ÿæ‰€ä»¥å½“æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨
    P do2 date time æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯å°†æ•´ä¸ªåºåˆ—ä¼ é€’ç»™ PD2 dateã€‚Timeï¼Œä½†ç°åœ¨è¿™å®é™…ä¸Šåªæ˜¯å°†æ¯ä¸ªå•ç‹¬çš„å­—ç¬¦ä¸²é€šè¿‡è¿™ä¸ªå‡½æ•°ã€‚
- en: So in order to convert thisï¼Œ we can use a function called PDd dot date time
    dot SP timeã€‚ That's how we convert a string to timeã€‚ and then we can just pass
    inã€‚Our string that we went converted to a date time and then the format and already
    had the format down hereã€‚ So I'll just go ahead and copy that and paste that in
    hereã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸ºäº†è¿›è¡Œè½¬æ¢ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªåä¸º PDd dot date time dot SP time çš„å‡½æ•°ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´çš„æ–¹å¼ã€‚ç„¶åæˆ‘ä»¬åªéœ€ä¼ å…¥ã€‚æˆ‘ä»¬å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ï¼Œç„¶åæ ¼å¼ä¹Ÿå·²ç»åœ¨è¿™é‡Œäº†ã€‚æ‰€ä»¥æˆ‘ä¼šç›´æ¥å¤åˆ¶å¹¶ç²˜è´´åˆ°è¿™é‡Œã€‚
- en: And that's all we need for that date parser functionã€‚ So now the argument for
    the date parser is date underscore parserã€‚ and I'm going to set that equal to
    that D pars variable there that is set to our lambda functionã€‚ Okayï¼Œ so now if
    I run this cell hereã€‚ Then we can see that we didn't get any errorsã€‚ So that's
    goodã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬æ‰€éœ€çš„æ—¥æœŸè§£æå‡½æ•°ã€‚å› æ­¤ï¼Œæ—¥æœŸè§£æå™¨çš„å‚æ•°æ˜¯ date underscore parserã€‚æˆ‘å°†æŠŠå®ƒè®¾ç½®ä¸ºé‚£ä¸ª D pars å˜é‡ï¼Œè¯¥å˜é‡è®¾ç½®ä¸ºæˆ‘ä»¬çš„
    lambda å‡½æ•°ã€‚å¥½çš„ï¼Œç°åœ¨å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ²¡æœ‰å‡ºç°é”™è¯¯ã€‚æ‰€ä»¥è¿™å¾ˆå¥½ã€‚
- en: And now if I run this D F dot head hereã€‚ğŸ˜Šï¼ŒThen we can see that now our data
    frame was already loaded in as a date timeã€‚ So we didn't have to do any conversions
    later onã€‚ It just did it as it was reading in that CSv fileã€‚ Okayï¼Œ so now let's
    look at some more useful things that we can do with date timesã€‚ So first I'm going
    to delete the sales that we have below here so that we are not converting these
    columns again since they're already loaded in as datesã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ª D F dot headï¼Œè¿™é‡Œã€‚ğŸ˜Šï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç°åœ¨æˆ‘ä»¬çš„æ•°æ®æ¡†å·²ç»ä»¥æ—¥æœŸæ—¶é—´çš„å½¢å¼åŠ è½½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸å¿…åœ¨åç»­è¿›è¡Œä»»ä½•è½¬æ¢ã€‚å®ƒåœ¨è¯»å–
    CSV æ–‡ä»¶æ—¶å°±å·²ç»å®Œæˆäº†ã€‚å¥½çš„ï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›æˆ‘ä»¬å¯ä»¥å¯¹æ—¥æœŸæ—¶é—´åšçš„æ›´æœ‰ç”¨çš„äº‹æƒ…ã€‚å› æ­¤ï¼Œæˆ‘å°†åˆ é™¤ä¸‹é¢çš„é”€å”®æ•°æ®ï¼Œä»¥ä¾¿ä¸å†è½¬æ¢è¿™äº›åˆ—ï¼Œå› ä¸ºå®ƒä»¬å·²ç»ä½œä¸ºæ—¥æœŸåŠ è½½è¿›æ¥äº†ã€‚
- en: So I'll delete that oneï¼Œ I will delete that one since that was what was converting
    it earlier I'll delete that as wellã€‚ and I'll keep this one here just for reference
    since I will have these up on my Github afterwards Okayã€‚ so before actually right
    hereï¼Œ we saw how to run a datetime method on a single value when we use this day
    name method but what if we want to run that method on our entire series So let's
    say that we wanted to view the day name of this entire date column hereã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¼šåˆ é™¤é‚£ä¸ªï¼Œå› ä¸ºä¹‹å‰æ˜¯ç”¨æ¥è¿›è¡Œè½¬æ¢çš„ï¼Œæˆ‘ä¹Ÿä¼šåˆ é™¤é‚£ä¸ªã€‚æˆ‘ä¼šä¿ç•™è¿™ä¸ªä¾›å‚è€ƒï¼Œå› ä¸ºæˆ‘ä¹‹åä¼šæŠŠè¿™äº›æ”¾åˆ°æˆ‘çš„ GitHub ä¸Šã€‚å¥½çš„ã€‚åœ¨è¿™é‡Œä¹‹å‰ï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•å¯¹å•ä¸ªå€¼è¿è¡Œ
    datetime æ–¹æ³•ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ª day name æ–¹æ³•æ—¶ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³å¯¹æ•´ä¸ªåºåˆ—è¿è¡Œè¯¥æ–¹æ³•å‘¢ï¼Ÿå‡è®¾æˆ‘ä»¬æƒ³æŸ¥çœ‹è¿™ä¸€æ•´åˆ—æ—¥æœŸçš„æ—¥æœŸåç§°ã€‚
- en: So to do this we can access the Dtã€‚Class on the series object and access the
    datetime methods that wayã€‚ so to do thisã€‚We can just say we can first grab that
    series so that date column is going to return a seriesã€‚ If I run thatï¼Œ we can
    see that we get all those valuesã€‚ And now if we wanted to access the Dt class
    on the series object then we can just say dot Dtã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®åºåˆ—å¯¹è±¡ä¸Šçš„ Dt ç±»ï¼Œå¹¶ä»¥è¿™ç§æ–¹å¼è®¿é—® datetime æ–¹æ³•ã€‚æ‰€ä»¥è¿™æ ·åšã€‚æˆ‘ä»¬å¯ä»¥é¦–å…ˆè·å–é‚£ä¸ªåºåˆ—ï¼Œæ‰€ä»¥æ—¥æœŸåˆ—å°†è¿”å›ä¸€ä¸ªåºåˆ—ã€‚å¦‚æœæˆ‘è¿è¡Œå®ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾—åˆ°æ‰€æœ‰é‚£äº›å€¼ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦åœ¨åºåˆ—å¯¹è±¡ä¸Šè®¿é—®
    Dt ç±»ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è¯´ .Dtã€‚
- en: And now the date time method that we want to useã€‚ So if I want to get the day
    name of all these values then I can just do day name thereã€‚ And if I run thatï¼Œ
    then we can see that we get the day of the week for each of the dates in this
    seriesã€‚ So using the Dt class on the series object is very similar to how we access
    the string class or the STR class for the string methods on an entire seriesã€‚
    and we saw that in previous videosã€‚ So this can definitely be pretty usefulã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³ä½¿ç”¨çš„æ—¥æœŸæ—¶é—´æ–¹æ³•ã€‚å¦‚æœæˆ‘æƒ³è·å–æ‰€æœ‰è¿™äº›å€¼çš„æ—¥æœŸåç§°ï¼Œæˆ‘å¯ä»¥ç›´æ¥ç”¨ day nameã€‚ç„¶åå¦‚æœæˆ‘è¿è¡Œå®ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾—åˆ°è¿™ä¸ªåºåˆ—ä¸­æ¯ä¸ªæ—¥æœŸçš„æ˜ŸæœŸå‡ ã€‚å› æ­¤ï¼Œåœ¨åºåˆ—å¯¹è±¡ä¸Šä½¿ç”¨
    Dt ç±»éå¸¸ç±»ä¼¼äºæˆ‘ä»¬å¦‚ä½•åœ¨æ•´ä¸ªåºåˆ—ä¸Šè®¿é—®å­—ç¬¦ä¸²ç±»æˆ– STR ç±»çš„å­—ç¬¦ä¸²æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­çœ‹åˆ°äº†è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥è¿™è‚¯å®šæ˜¯éå¸¸æœ‰ç”¨çš„ã€‚
- en: So let's say that we wanted to you know create another column so that we could
    quickly reference what day all of these trades took placeã€‚ So to do that we could
    just grab what we have hereã€‚ and I could simply create a new columnã€‚ğŸ˜Šã€‚By simply
    like I'm accessing a columnã€‚ so I could call this column day of week and set this
    equal to and paste in that date time method thereã€‚ If I run thisã€‚ and then we
    look at our data frameã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³åˆ›å»ºå¦ä¸€åˆ—ï¼Œä»¥ä¾¿å¿«é€Ÿå‚è€ƒæ‰€æœ‰è¿™äº›äº¤æ˜“å‘ç”Ÿçš„æ—¥æœŸã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥æå–æˆ‘ä»¬è¿™é‡Œçš„å†…å®¹ï¼Œç„¶åæˆ‘å¯ä»¥ç®€å•åœ°åˆ›å»ºä¸€ä¸ªæ–°åˆ—ã€‚ğŸ˜Šã€‚é€šè¿‡ç®€å•åœ°åƒè®¿é—®ä¸€åˆ—é‚£æ ·ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥ç§°è¿™ä¸ªåˆ—ä¸ºæ˜ŸæœŸå‡ ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºå¹¶ç²˜è´´è¯¥æ—¥æœŸæ—¶é—´æ–¹æ³•ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œç„¶åæˆ‘ä»¬æŸ¥çœ‹æˆ‘ä»¬çš„æ•°æ®æ¡†ã€‚
- en: then we can see that now we can quickly see over here on the right that okay
    the 13th was a Friday and then we have these dates down here towards the endã€‚
    this was a Saturdayï¼Œ so it's nice to see about be able to see what days these
    trades actually took placeã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¿«é€Ÿçœ‹åˆ°å³ä¾§ï¼Œå¥½çš„ï¼Œ13å·æ˜¯æ˜ŸæœŸäº”ï¼Œç„¶ååœ¨åº•éƒ¨æœ‰è¿™äº›æ—¥æœŸï¼Œè¿™æ˜¯ä¸€å‘¨å…­ï¼Œæ‰€ä»¥èƒ½çœ‹åˆ°è¿™äº›äº¤æ˜“å®é™…ä¸Šå‘ç”Ÿåœ¨å“ªäº›æ—¥æœŸçœŸæ˜¯ä¸é”™ã€‚
- en: So now let's look at how we can explore our data a bitã€‚ So we can see by looking
    at the indexes here on the far leftã€‚ that there are over 20000 rows in this data
    setã€‚ So let's see how we can view the earliest and latest dates in this data So
    to do thisã€‚ we can use the min and max methods So to see the earliest date I could
    simply accessã€‚ğŸ˜Šã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ¢ç´¢æˆ‘ä»¬çš„æ•°æ®ã€‚é€šè¿‡æŸ¥çœ‹æœ€å·¦ä¾§çš„ç´¢å¼•ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ•°æ®é›†æœ‰è¶…è¿‡20000è¡Œã€‚é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æŸ¥çœ‹è¿™ä¸ªæ•°æ®é›†ä¸­æœ€æ—©å’Œæœ€æ–°çš„æ—¥æœŸã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€å°å’Œæœ€å¤§æ–¹æ³•ã€‚æ‰€ä»¥ä¸ºäº†æŸ¥çœ‹æœ€æ—©çš„æ—¥æœŸï¼Œæˆ‘å¯ä»¥ç®€å•åœ°è®¿é—®ã€‚ğŸ˜Šã€‚
- en: This date series hereã€‚ and I could just run the min method on thisã€‚ And I run
    thatã€‚ then we can see that the earliest date that it gives us is 2017ï¼Œ0ï¼Œ7ï¼Œ0ï¼Œ1ã€‚
    Now if I wanted to see So what is thatã€‚ That's July 1 of 2017ã€‚ So to view the
    most recent date that I haveã€‚ And it should be the date that I downloaded this
    dataã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ—¥æœŸåºåˆ—ã€‚ç„¶åæˆ‘å¯ä»¥ç›´æ¥å¯¹å…¶è¿è¡Œæœ€å°æ–¹æ³•ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç»™å‡ºçš„æœ€æ—©æ—¥æœŸæ˜¯2017ï¼Œ0ï¼Œ7ï¼Œ0ï¼Œ1ã€‚ç°åœ¨å¦‚æœæˆ‘æƒ³çœ‹çœ‹é‚£æ˜¯ä»€ä¹ˆã€‚é‚£æ˜¯2017å¹´7æœˆ1æ—¥ã€‚ä¸ºäº†æŸ¥çœ‹æˆ‘æ‹¥æœ‰çš„æœ€è¿‘æ—¥æœŸï¼Œå®ƒåº”è¯¥æ˜¯æˆ‘ä¸‹è½½è¿™ä¸ªæ•°æ®çš„æ—¥æœŸã€‚
- en: then I can just look at the max value hereã€‚ And if I run thisã€‚ then we can see
    that this is March 13ï¼Œ2020ï¼Œ which actually was the day that I downloaded this
    dataã€‚ And one really cool thing with date times is that we can actually subtract
    dates in order to view the time between those two datesã€‚ And this is called a
    time deltaã€‚ So to get the amount of time that spans between these two dates hereã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¯ä»¥çœ‹çœ‹è¿™é‡Œçš„æœ€å¤§å€¼ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ˜¯2020å¹´3æœˆ13æ—¥ï¼Œå®é™…ä¸Šæ˜¯æˆ‘ä¸‹è½½è¿™ä¸ªæ•°æ®çš„é‚£ä¸€å¤©ã€‚æ—¥æœŸæ—¶é—´çš„ä¸€ä¸ªéå¸¸é…·çš„åœ°æ–¹æ˜¯æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥å‡å»æ—¥æœŸï¼Œä»¥æŸ¥çœ‹è¿™ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„æ—¶é—´ã€‚è¿™è¢«ç§°ä¸ºæ—¶é—´å·®ã€‚æ‰€ä»¥ä¸ºäº†è·å–è¿™ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„æ—¶é—´è·¨åº¦ã€‚
- en: then I could simply say take the max valueã€‚ and then subtract the min valueã€‚ğŸ˜Šï¼ŒAnd
    if I run thisã€‚ then we can see that we get this time delta that says that there
    are almost 1000 days between the earliest date in our data set in the most recentã€‚
    So we have 986 days in this entire data set of cryptocurrency data almost 1000ã€‚
    So that would definitely be a lot of days to look through if we want to find some
    specific rangesã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘å¯ä»¥ç®€å•åœ°è¯´å–æœ€å¤§å€¼ï¼Œç„¶åå‡å»æœ€å°å€¼ã€‚ğŸ˜Šï¼Œå¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªæ—¶é—´å·®ï¼Œå®ƒè¡¨æ˜åœ¨æˆ‘ä»¬æ•°æ®é›†ä¸­æœ€æ—©çš„æ—¥æœŸå’Œæœ€è¿‘çš„æ—¥æœŸä¹‹é—´å‡ ä¹æœ‰1000å¤©ã€‚æ‰€ä»¥åœ¨è¿™æ•´ä¸ªåŠ å¯†è´§å¸æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬æœ‰986å¤©ï¼Œå·®ä¸å¤š1000å¤©ã€‚å¦‚æœæˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€äº›ç‰¹å®šçš„èŒƒå›´ï¼Œè¿™ç¡®å®ä¼šæ˜¯å¾ˆå¤šå¤©éœ€è¦æŸ¥çœ‹ã€‚
- en: So what if we wanted to do some filters by dateã€‚ So for exampleã€‚ let's say that
    we just wanted to view the data for 2020 Now that we have these converted to date
    timesã€‚ we can create filters just like we have in previous videosã€‚ and we should
    be able to use strings that are formatted like date times or we can use actual
    datetime objectsã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æ—¥æœŸåšä¸€äº›è¿‡æ»¤å‘¢ã€‚æ¯”å¦‚è¯´ï¼Œå‡è®¾æˆ‘ä»¬åªæ˜¯æƒ³æŸ¥çœ‹2020å¹´çš„æ•°æ®ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»å°†è¿™äº›è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ï¼Œæˆ‘ä»¬å¯ä»¥åƒåœ¨ä¹‹å‰çš„è§†é¢‘ä¸­é‚£æ ·åˆ›å»ºè¿‡æ»¤å™¨ï¼Œå¹¶ä¸”æˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿä½¿ç”¨æ ¼å¼åƒæ—¥æœŸæ—¶é—´çš„å­—ç¬¦ä¸²ï¼Œæˆ–è€…å¯ä»¥ä½¿ç”¨å®é™…çš„æ—¥æœŸæ—¶é—´å¯¹è±¡ã€‚
- en: We'll take a look at bothã€‚ So let's see an example of this and some code so
    that it makes some more senseã€‚ So firstï¼Œ I'm going create a filter in a separate
    variable like I've done in previous videosã€‚ But you can also do this in line if
    you prefer to do it that wayã€‚ I just think thatã€‚Our filters separate as a little
    bit easier to readã€‚ So let's say that I want ourã€‚Date seriesã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æŸ¥çœ‹è¿™ä¸¤è€…ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªä¾‹å­å’Œä¸€äº›ä»£ç ï¼Œè¿™æ ·å®ƒå°±æ›´æœ‰æ„ä¹‰äº†ã€‚é¦–å…ˆï¼Œæˆ‘å°†åˆ›å»ºä¸€ä¸ªåœ¨å•ç‹¬å˜é‡ä¸­çš„è¿‡æ»¤å™¨ï¼Œå°±åƒæˆ‘åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­æ‰€åšçš„é‚£æ ·ã€‚ä½†å¦‚æœä½ æ›´å–œæ¬¢è¿™æ ·åšï¼Œä¹Ÿå¯ä»¥åœ¨è¡Œå†…è¿›è¡Œã€‚æˆ‘åªæ˜¯è§‰å¾—å°†è¿‡æ»¤å™¨åˆ†å¼€ä¼šç¨å¾®å®¹æ˜“é˜…è¯»ä¸€äº›ã€‚æ‰€ä»¥å‡è®¾æˆ‘æƒ³è¦æˆ‘ä»¬çš„æ—¥æœŸåºåˆ—ã€‚
- en: I want the objects or the rows that are greater thanã€‚ And then I'm just gonna
    pass in a string here for nowã€‚ And I can just pass in a 2020 thereã€‚ And pandas
    will know that I'm talking about the year 2020ã€‚ let's actually do a greater than
    or equal to hereã€‚ Okayï¼Œ so now that I have that filterã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³è¦å¤§äºæŸä¸ªå€¼çš„å¯¹è±¡æˆ–è¡Œã€‚ç„¶åæˆ‘ç°åœ¨åªä¼šä¼ å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚è¿™é‡Œæˆ‘å¯ä»¥ä¼ å…¥2020å¹´ã€‚Pandasä¼šçŸ¥é“æˆ‘åœ¨è¯´2020å¹´ã€‚è®©æˆ‘ä»¬å®é™…è¿›è¡Œä¸€ä¸ªå¤§äºæˆ–ç­‰äºçš„æ“ä½œã€‚å¥½çš„ï¼Œç°åœ¨æˆ‘æœ‰äº†è¿™ä¸ªç­›é€‰ã€‚
- en: let's just do a Df do Loã€‚ againï¼Œ we've seen this in previous videosã€‚ And then
    I'll pass in that filterã€‚ So if I run thisã€‚ then my bottom row here should be
    January 1 of 2020ã€‚ And it isã€‚ And we can see that we have 17000 hours here of
    2020 dataã€‚ or I'm sorry that's 1700 hours of 2020 dataã€‚ Okayã€‚ so the reason that
    this doesn't go above 2020 is simply because you knowï¼Œ our latest data runs outã€‚
    So we're not getting 2021 since 2021 hasn't happened yetã€‚ But what if we wanted
    data for 2019ã€‚ğŸ˜Šã€‚Wellï¼Œ in order to do thatï¼Œ we'd also have to put in an upper bound
    as wellã€‚ So to do thatã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†åšä¸€æ¬¡Df do Loã€‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­è§è¿‡è¿™ä¸ªã€‚ç„¶åæˆ‘ä¼šä¼ å…¥é‚£ä¸ªç­›é€‰ã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘çš„åº•éƒ¨è¡Œåº”è¯¥æ˜¯2020å¹´1æœˆ1æ—¥ã€‚ç¡®å®æ˜¯ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰17000å°æ—¶çš„2020å¹´æ•°æ®ï¼Œæˆ–è€…æŠ±æ­‰ï¼Œæ˜¯1700å°æ—¶çš„2020å¹´æ•°æ®ã€‚å¥½çš„ã€‚è¿™ä¸è¶…è¿‡2020å¹´çš„åŸå› å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬çš„æœ€æ–°æ•°æ®å·²ç»æ²¡æœ‰äº†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ²¡æœ‰2021å¹´çš„æ•°æ®ï¼Œå› ä¸º2021å¹´è¿˜æ²¡æœ‰å‘ç”Ÿã€‚ä½†å¦‚æœæˆ‘ä»¬æƒ³è¦2019å¹´çš„æ•°æ®å‘¢ğŸ˜Šï¼Ÿé‚£ä¹ˆä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®ä¸€ä¸ªä¸Šé™ã€‚
- en: I'm going to sayï¼Œ okayï¼Œ we wantã€‚Our data to be greater than or equal to 2019
    andã€‚And we just want to do an amperign thereã€‚ I'll go ahead and copy this here
    and then just replace this with a less than and we'll say less than 2020ã€‚ if I
    run this then we can see that our bottom row here we have January 1 of 2019 at
    midnight and then our top row here is December 31 at 11 pm of 2019ã€‚ So that gives
    us all the rows of data that we have for 2019ã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†è¯´ï¼Œå¥½å§ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®å¤§äºæˆ–ç­‰äº2019å¹´ã€‚æˆ‘ä»¬åªæ˜¯æƒ³åœ¨è¿™é‡Œè¿›è¡Œä¸€ä¸ªç­›é€‰ã€‚æˆ‘ä¼šæŠŠè¿™ä¸ªå¤åˆ¶è¿‡æ¥ï¼Œç„¶åå°†å…¶æ›¿æ¢ä¸ºå°äºï¼Œå¹¶ä¸”æˆ‘ä»¬ä¼šè¯´å°äº2020å¹´ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åº•éƒ¨è¡Œæ˜¯2019å¹´1æœˆ1æ—¥çš„åˆå¤œï¼Œé¡¶éƒ¨è¡Œæ˜¯2019å¹´12æœˆ31æ—¥æ™šä¸Š11ç‚¹ã€‚æ‰€ä»¥è¿™ç»™äº†æˆ‘ä»¬2019å¹´çš„æ‰€æœ‰æ•°æ®è¡Œã€‚
- en: And right now we're just using strings up here for these comparisons but we
    can use actual date times as wellã€‚ So to do that we could actually say I could
    just say PDd datetime and then let me go ahead and pass in the month and day here
    as wellã€‚ So I'll say that I want this to be greater than 2019 January 1 and then
    I'll just grab this here and replace this 2020 and then I'llã€‚But I want this to
    be less than 2020s January 1 So now if I run this whoops and I got an error here
    it says you know integer is requiredã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åªæ˜¯ä½¿ç”¨å­—ç¬¦ä¸²è¿›è¡Œè¿™äº›æ¯”è¾ƒï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨å®é™…çš„æ—¥æœŸæ—¶é—´ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å¯ä»¥è¯´æˆ‘åªéœ€è¯´PDd datetimeï¼Œç„¶åè®©æˆ‘ä¼ å…¥æœˆä»½å’Œæ—¥æœŸã€‚æ‰€ä»¥æˆ‘ä¼šè¯´æˆ‘æƒ³è¦çš„æ—¥æœŸå¤§äº2019å¹´1æœˆ1æ—¥ï¼Œç„¶åæˆ‘ä¼šæŠ“å–è¿™ä¸ªå¹¶æ›¿æ¢2020å¹´ã€‚å¹¶ä¸”æˆ‘å¸Œæœ›è¿™ä¸ªæ—¥æœŸå°äº2020å¹´1æœˆ1æ—¥ã€‚æ‰€ä»¥ç°åœ¨å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œå“å‘€ï¼Œæˆ‘å¾—åˆ°äº†ä¸€ä¸ªé”™è¯¯ï¼Œè¯´éœ€è¦æ•´æ•°ã€‚
- en: got a string that might not make sense what I did here is I don't want PD date
    time that was my mistake I want to do the same thing that we did before and do
    two date time so that it converts this string here to a date time So let's do
    PDã€‚2 datetime for both of those and run this and now we can see that we get those
    same results as before for all of the rows in 2019 Now one nice feature about
    dates is that if we set our index so that it uses the date which would actually
    be a good idea for this data set since all of these date or timestamps are unique
    then we can actually do this same thing by using slicing instead So let's see
    what this looks like so that it makes more senseã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—åˆ°ä¸€ä¸ªå­—ç¬¦ä¸²å¯èƒ½ä¸å¤ªåˆç†ï¼Œæˆ‘åœ¨è¿™é‡Œæ‰€åšçš„æ˜¯æˆ‘ä¸æƒ³ä½¿ç”¨PD datetimeï¼Œè¿™æ˜¯æˆ‘çš„é”™è¯¯ï¼Œæˆ‘æƒ³åšæˆ‘ä»¬ä¹‹å‰æ‰€åšçš„äº‹æƒ…ï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ï¼Œä»¥ä¾¿å°†è¿™ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´ã€‚æ‰€ä»¥æˆ‘ä»¬æ¥å¯¹è¿™ä¸¤ä¸ªéƒ½ä½¿ç”¨PDã€‚2
    datetimeå¹¶è¿è¡Œå®ƒï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸ä¹‹å‰ç›¸åŒçš„ç»“æœï¼Œ2019å¹´çš„æ‰€æœ‰è¡Œã€‚å…³äºæ—¥æœŸçš„ä¸€ä¸ªå¥½åŠŸèƒ½æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½®ç´¢å¼•ä»¥ä½¿ç”¨æ—¥æœŸï¼Œè¿™å¯¹è¿™ä¸ªæ•°æ®é›†å®é™…ä¸Šæ˜¯ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºæ‰€æœ‰çš„æ—¥æœŸæˆ–æ—¶é—´æˆ³éƒ½æ˜¯å”¯ä¸€çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥é€šè¿‡ä½¿ç”¨åˆ‡ç‰‡æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™çœ‹èµ·æ¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Œä»¥ä¾¿æ›´æ˜“ç†è§£ã€‚
- en: So first let's set our index so that it's usingã€‚This date column hereã€‚ So here
    at the bottomã€‚ I'm going say Df set underscore index and then I'm going to pass
    in that we want to set the index to date and if I run this then that looks good
    we have set it our index to use date here and now that that looks goodã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥é¦–å…ˆè®©æˆ‘ä»¬è®¾ç½®ç´¢å¼•ï¼Œä»¥ä¾¿å®ƒä½¿ç”¨è¿™ä¸ªæ—¥æœŸåˆ—ã€‚æ‰€ä»¥åœ¨åº•éƒ¨ï¼Œæˆ‘ä¼šè¯´Df set underscore indexï¼Œç„¶åæˆ‘å°†ä¼ å…¥æˆ‘ä»¬å¸Œæœ›å°†ç´¢å¼•è®¾ç½®ä¸ºæ—¥æœŸã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆçœ‹èµ·æ¥å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»å°†ç´¢å¼•è®¾ç½®ä¸ºä½¿ç”¨æ—¥æœŸï¼Œç°åœ¨çœ‹èµ·æ¥ä¸é”™ã€‚
- en: it actually didn't make that changeï¼Œ I want to say in place is equal to true
    to make that change permanent so I'll run that and if we look at our data frame
    againã€‚ then now we have that date as our index and now with that date index we
    can actually filter our dates just by passing them into our brackets so if we
    wanted the data for 2019 then I could literally just say that I want the data
    here for 2019 pass that into my brackets if I run that then we can see that we
    get the same thing here we get this value for January 1 and the top valueã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå®é™…ä¸Šå¹¶æ²¡æœ‰è¿›è¡Œé‚£é¡¹æ›´æ”¹ï¼Œæˆ‘æƒ³è¯´å°†`in place`è®¾ç½®ä¸º`true`ä»¥ä½¿æ›´æ”¹æ°¸ä¹…åŒ–ï¼Œæ‰€ä»¥æˆ‘ä¼šè¿è¡Œå®ƒï¼Œå¦‚æœæˆ‘ä»¬å†æ¬¡æŸ¥çœ‹æˆ‘ä»¬çš„æ•°æ®æ¡†ï¼Œé‚£ä¹ˆç°åœ¨æˆ‘ä»¬å°†è¯¥æ—¥æœŸä½œä¸ºç´¢å¼•ï¼Œç°åœ¨é€šè¿‡è¯¥æ—¥æœŸç´¢å¼•ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥é€šè¿‡å°†æ—¥æœŸä¼ é€’åˆ°æˆ‘ä»¬çš„æ‹¬å·ä¸­æ¥è¿‡æ»¤æ—¥æœŸï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬æƒ³è¦2019å¹´çš„æ•°æ®ï¼Œæˆ‘å¯ä»¥ç›´æ¥è¯´æˆ‘æƒ³è¦2019å¹´çš„æ•°æ®ï¼Œå°†å…¶ä¼ é€’åˆ°æˆ‘çš„æ‹¬å·ä¸­ï¼Œå¦‚æœæˆ‘è¿è¡Œå®ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œè¿™é‡Œæˆ‘ä»¬è·å¾—äº†1æœˆ1æ—¥çš„å€¼å’Œé¡¶éƒ¨å€¼ã€‚
- en: Here is for December 31 so it's a bit easier to you know just access these within
    brackets when these are our indexes rather than creating a filter now if you want
    to grab dates for a specific range then you can use a slice so let's say that
    we wanted all of the data for January and February of 2020 so to do that using
    this slicing here then I could say okay I want from 202001 which would be January
    and then I could just do a slice here using that colon and then say okay well
    I want to go up to February of 2020 so if I run this the second value here is
    inclusive so we can see that we have January 1 of 2020 down here at the bottom
    that slices all the way up to February 29 since this was a leap here now this
    can be really usefulã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯12æœˆ31æ—¥ï¼Œæ‰€ä»¥å½“è¿™äº›æ˜¯æˆ‘ä»¬çš„ç´¢å¼•æ—¶ï¼Œç›´æ¥åœ¨æ‹¬å·ä¸­è®¿é—®ä¼šæ›´ç®€å•ï¼Œè€Œä¸æ˜¯åˆ›å»ºè¿‡æ»¤å™¨ã€‚å¦‚æœä½ æƒ³æŠ“å–ç‰¹å®šèŒƒå›´çš„æ—¥æœŸï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨åˆ‡ç‰‡ã€‚å‡è®¾æˆ‘ä»¬æƒ³è¦2020å¹´1æœˆå’Œ2æœˆçš„æ‰€æœ‰æ•°æ®ï¼Œé‚£ä¹ˆä½¿ç”¨è¿™ä¸ªåˆ‡ç‰‡ï¼Œæˆ‘å¯ä»¥è¯´å¥½çš„ï¼Œæˆ‘æƒ³è¦ä»202001å¼€å§‹ï¼Œä¹Ÿå°±æ˜¯1æœˆï¼Œç„¶åæˆ‘å¯ä»¥ä½¿ç”¨å†’å·åšä¸€ä¸ªåˆ‡ç‰‡ï¼Œç„¶åè¯´å¥½çš„ï¼Œæˆ‘æƒ³è¦åˆ°2020å¹´2æœˆï¼Œæ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œç¬¬äºŒä¸ªå€¼æ˜¯åŒ…å«åœ¨å†…çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°2020å¹´1æœˆ1æ—¥åœ¨åº•éƒ¨ï¼Œè¿™ä¸ªåˆ‡ç‰‡ä¸€ç›´åˆ°2æœˆ29æ—¥ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªé—°å¹´ï¼Œç°åœ¨è¿™éå¸¸æœ‰ç”¨ã€‚
- en: For analyzing our dataï¼Œ because let's say that we wanted to get the average
    closing price for Ethereum for all of our rows of these dates to do thatã€‚ we could
    simply grab this close column hereã€‚ and then grab that average or grab that meanã€‚
    So to do thatï¼Œ we can just sayï¼Œ let me copy this part hereã€‚ firstã€‚ let me just
    access that close series thereï¼Œ that column if I run thatã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ†ææˆ‘ä»¬çš„æ•°æ®ï¼Œå› ä¸ºå‡è®¾æˆ‘ä»¬æƒ³è¦è·å–ä»¥å¤ªåŠåœ¨è¿™äº›æ—¥æœŸçš„æ‰€æœ‰è¡Œçš„å¹³å‡æ”¶ç›˜ä»·ï¼Œåšæ³•å¾ˆç®€å•ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥æŠ“å–è¿™ä¸ªæ”¶ç›˜åˆ—ï¼Œç„¶åè®¡ç®—å¹³å‡å€¼æˆ–å‡å€¼ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œè®©æˆ‘å…ˆå¤åˆ¶è¿™ä¸€éƒ¨åˆ†ã€‚è®©æˆ‘è®¿é—®é‚£ä¸ªæ”¶ç›˜åºåˆ—ï¼Œå¦‚æœæˆ‘è¿è¡Œå®ƒã€‚
- en: then we can see that we get all of those closing values on each of those hours
    for all of those daysã€‚ and now to get the mean of thatï¼Œ I can just sayã€‚Dot meanã€‚
    And that gives us the average closing price for all of those rows within that
    time frameã€‚ And rememberï¼Œ each of those days is reporting by the hourã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è·å–åˆ°æ‰€æœ‰è¿™äº›å¤©æ¯ä¸ªå°æ—¶çš„æ”¶ç›˜å€¼ã€‚ç°åœ¨ä¸ºäº†å¾—åˆ°å‡å€¼ï¼Œæˆ‘å¯ä»¥ç›´æ¥è¯´`Dot mean`ã€‚è¿™ç»™äº†æˆ‘ä»¬åœ¨è¯¥æ—¶é—´èŒƒå›´å†…æ‰€æœ‰è¡Œçš„å¹³å‡æ”¶ç›˜ä»·ã€‚å¹¶ä¸”è¦è®°ä½ï¼Œæ¯ä¸€å¤©éƒ½æ˜¯æŒ‰å°æ—¶æŠ¥å‘Šçš„ã€‚
- en: But what if we wanted to see this data in a different wayã€‚ What if we instead
    wanted to look at this data on a daily basis instead of on an hourly basisã€‚ Wellã€‚
    firstï¼Œ we need to think about what would make sense to view on a daily basisã€‚
    So for exampleã€‚ let's say that we wanted toï¼Œ you knowï¼Œ view the highs for each
    dayã€‚ So right nowã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³ä»¥ä¸åŒçš„æ–¹å¼æŸ¥çœ‹è¿™äº›æ•°æ®å‘¢ï¼Ÿå¦‚æœæˆ‘ä»¬æƒ³è¦æŒ‰å¤©è€Œä¸æ˜¯æŒ‰å°æ—¶æŸ¥çœ‹è¿™äº›æ•°æ®å‘¢ï¼Ÿé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ä»€ä¹ˆæ ·çš„è§†å›¾é€‚åˆæŒ‰å¤©æŸ¥çœ‹ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³æŸ¥çœ‹æ¯å¤©çš„é«˜ç‚¹ã€‚é‚£ä¹ˆç°åœ¨ã€‚
- en: we have all of these highs broken down by hourï¼Œ let me actually look at the
    firstã€‚ let me grab this date range hereï¼Œ And let's look at the first 24 of these
    so that we can get 24 hours hereã€‚ So we can see that for February 29ã€‚ We have
    all these different hours here and each hour has a different high valueã€‚ But what
    if we were likeï¼Œ okayï¼Œ wellï¼Œ we see all these different high valuesã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™äº›é«˜ç‚¹æŒ‰å°æ—¶åˆ’åˆ†ï¼Œå®é™…ä¸Šè®©æˆ‘æŸ¥çœ‹ä¸€ä¸‹ç¬¬ä¸€ä¸ªã€‚è®©æˆ‘æŠ“å–è¿™ä¸ªæ—¥æœŸèŒƒå›´ï¼Œç„¶åæˆ‘ä»¬çœ‹ä¸€ä¸‹å‰24ä¸ªï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥è·å–24å°æ—¶çš„æ•°æ®ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨2æœˆ29æ—¥ï¼Œæˆ‘ä»¬æœ‰è¿™äº›ä¸åŒçš„å°æ—¶ï¼Œæ¯ä¸ªå°æ—¶éƒ½æœ‰ä¸åŒçš„é«˜å€¼ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬è¯´ï¼Œå¥½å§ï¼Œæˆ‘ä»¬çœ‹åˆ°æ‰€æœ‰è¿™äº›ä¸åŒçš„é«˜å€¼ã€‚
- en: But what was the highest value of the day So actuallyã€‚ let me just grab a single
    day here and then we will look at the high values for that So instead of doing
    all of these dates here I'm just going grab January 1 of 2020 and then we will
    look at the high values for that day So againã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é‚£å¤©çš„æœ€é«˜å€¼æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæ‰€ä»¥å®é™…ä¸Šï¼Œè®©æˆ‘æŠ“å–ä¸€å¤©çš„é«˜å€¼ï¼Œç„¶åæˆ‘ä»¬çœ‹çœ‹é‚£å¤©çš„é«˜å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸æŸ¥çœ‹æ‰€æœ‰è¿™äº›æ—¥æœŸï¼ŒåªæŠ“å–2020å¹´1æœˆ1æ—¥ï¼Œç„¶åæˆ‘ä»¬å°†æŸ¥çœ‹é‚£ä¸€å¤©çš„é«˜å€¼ã€‚æ‰€ä»¥å†æ¬¡ã€‚
- en: we don't really care what the highs are for each hour of each day we just want
    to know the high for the entire day So to do this all we need to do is grab the
    max value for this series and we saw how to do this it's just like running mean
    right here all we have to do is say dot max and if I run that then we can see
    that the high value for that day was 132ã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¹¶ä¸å…³å¿ƒæ¯ä¸€å¤©æ¯å°æ—¶çš„é«˜å€¼ï¼Œæˆ‘ä»¬åªæƒ³çŸ¥é“æ•´å¤©çš„é«˜å€¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€è·å–è¯¥ç³»åˆ—çš„æœ€å¤§å€¼ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œæ–¹æ³•å°±åƒåœ¨è¿™é‡Œè¿è¡Œmeanä¸€æ ·ï¼Œæˆ‘ä»¬åªéœ€è¯´.dot
    maxï¼Œå¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‚£å¤©çš„é«˜å€¼æ˜¯132ã€‚
- en: 68 so let's remember this value here right nowï¼Œ this 132ã€‚68 because we're going
    to see how we can resample our data so that we can get the highã€‚Ts for each day
    of our dataã€‚ And then we'll use this one here to compare for January 1 of 2020ã€‚
    So againï¼Œ right nowï¼Œ our data is broken down on an hourly basisã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 68 æ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬è®°ä½è¿™ä¸ªå€¼132.68ï¼Œå› ä¸ºæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å¯¹æ•°æ®è¿›è¡Œé‡é‡‡æ ·ï¼Œä»¥ä¾¿è·å¾—æˆ‘ä»¬æ•°æ®ä¸­æ¯ä¸€å¤©çš„é«˜å€¼ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªå€¼æ¥ä¸2020å¹´1æœˆ1æ—¥è¿›è¡Œæ¯”è¾ƒã€‚æ‰€ä»¥ï¼Œç°åœ¨æˆ‘ä»¬çš„æ•°æ®æ˜¯æŒ‰å°æ—¶åˆ’åˆ†çš„ã€‚
- en: So if we want to redo this so that it's instead broken down by day or week or
    monthã€‚ then we'll do this by doing something called resamplingã€‚ So let's see what
    this looks likeã€‚ So if I want to resample this and see the high value by dayã€‚
    Then I can simplyã€‚Access this high column hereã€‚ And then on that seriesï¼Œ I can
    sayï¼Œ okayï¼Œ I want to resample thisã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³é‡æ–°å¤„ç†ï¼Œä½¿å…¶æŒ‰å¤©ã€å‘¨æˆ–æœˆåˆ’åˆ†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡é‡é‡‡æ ·æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯ä»€ä¹ˆæ ·å­ã€‚å¦‚æœæˆ‘æƒ³é‡é‡‡æ ·å¹¶æŸ¥çœ‹æŒ‰å¤©çš„é«˜å€¼ã€‚é‚£ä¹ˆæˆ‘å¯ä»¥ç®€å•åœ°è®¿é—®è¿™ä¸ªé«˜åˆ—ã€‚åœ¨é‚£ä¸ªç³»åˆ—ä¸Šï¼Œæˆ‘å¯ä»¥è¯´ï¼Œå¥½çš„ï¼Œæˆ‘æƒ³å¯¹è¿™ä¸ªè¿›è¡Œé‡é‡‡æ ·ã€‚
- en: and now we have to tell resample how we want to resample this data right nowã€‚
    it's hourly if I put in a Dï¼Œ then it resamples it to daysã€‚ and I can do 1 d or
    2 dã€‚ you can do whatever thereã€‚ you can do a w for weekã€‚ there's all kinds of
    different codes hereã€‚ Now just like with these date time formatsã€‚ I hardly ever
    remember theseã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¿…é¡»å‘Šè¯‰é‡é‡‡æ ·æˆ‘ä»¬æƒ³å¦‚ä½•é‡é‡‡æ ·è¿™äº›æ•°æ®ï¼Œç°åœ¨æ˜¯æŒ‰å°æ—¶çš„ã€‚å¦‚æœæˆ‘è¾“å…¥ä¸€ä¸ªDï¼Œé‚£ä¹ˆå®ƒä¼šé‡é‡‡æ ·ä¸ºæŒ‰å¤©ã€‚å¦‚æœæˆ‘è¾“å…¥1 dæˆ–2 dï¼Œä½ å¯ä»¥éšæ„é€‰æ‹©ã€‚ä½ å¯ä»¥ç”¨wè¡¨ç¤ºå‘¨ï¼Œè¿™é‡Œæœ‰å„ç§ä¸åŒçš„ä»£ç ã€‚å°±åƒè¿™äº›æ—¥æœŸæ—¶é—´æ ¼å¼ä¸€æ ·ï¼Œæˆ‘å‡ ä¹ä»ä¸è®°å¾—è¿™äº›ã€‚
- en: So I always need to look them up in the documentationã€‚ So I've got this pulled
    up in the pandas documentation here for these date offsetsã€‚ and I will leave a
    link to this page in the description section below as wellã€‚ if you all would like
    to try out some of theseã€‚ but we can see we have hour minute second millisecondsã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æ€»æ˜¯éœ€è¦åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾å®ƒä»¬ã€‚æ‰€ä»¥æˆ‘åœ¨è¿™é‡Œæ‹‰å–äº†pandasæ–‡æ¡£ä¸­çš„è¿™äº›æ—¥æœŸåç§»é‡ï¼Œå¹¶ä¸”æˆ‘å°†åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†ä¸­ç•™ä¸‹è¿™ä¸ªé¡µé¢çš„é“¾æ¥ï¼Œå¦‚æœä½ ä»¬æƒ³å°è¯•å…¶ä¸­çš„ä¸€äº›ã€‚ä½†æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æœ‰å°æ—¶ã€åˆ†é’Ÿã€ç§’ã€æ¯«ç§’ã€‚
- en: microsecondsï¼Œ all kinds of thingsã€‚ If you're doing financesã€‚ you can do quarterly
    and things like that soã€‚I want to do this on a daily basisã€‚ So I'm going to put
    a D thereã€‚ And now we have to tell itï¼Œ okayï¼Œ wellã€‚ what do we want to do with
    these resamplings if I'm looking at entire days hereã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®ç§’ï¼Œå„ç§å„æ ·çš„äº‹æƒ…ã€‚å¦‚æœä½ åœ¨åšè´¢åŠ¡ï¼Œä½ å¯ä»¥æŒ‰å­£åº¦ç­‰è¿›è¡Œå¤„ç†ã€‚æ‰€ä»¥æˆ‘æƒ³æ¯å¤©åšä¸€æ¬¡ã€‚å› æ­¤ï¼Œæˆ‘å°†åœ¨è¿™é‡Œæ”¾ä¸€ä¸ªDã€‚ç°åœ¨æˆ‘ä»¬å¿…é¡»å‘Šè¯‰å®ƒï¼Œå¥½çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬æƒ³å¯¹è¿™äº›é‡é‡‡æ ·åšä»€ä¹ˆï¼Œå¦‚æœæˆ‘æŸ¥çœ‹çš„æ˜¯æ•´å¤©çš„æ•°æ®ã€‚
- en: So if I take this entire day of the firstï¼Œ what do I want to do with this high
    valueã€‚ And we're just sayingï¼Œ wellï¼Œ we want the max value for each of those daysã€‚
    So if I run thisã€‚ then we can see that that gives us a series with all of the
    high values for each dayã€‚ So now let's save this series here as a new variable
    and look up these specific date that we used beforeã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘å–è¿™ä¸ªæ•´å¤©çš„ç¬¬ä¸€å¤©ï¼Œæˆ‘æƒ³å¯¹è¿™ä¸ªé«˜å€¼åšä»€ä¹ˆã€‚æˆ‘ä»¬åªæ˜¯è¯´ï¼Œæˆ‘ä»¬æƒ³è¦æ¯ä¸€å¤©çš„æœ€å¤§å€¼ã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ç»™æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç³»åˆ—ï¼ŒåŒ…å«æ¯ä¸€å¤©çš„é«˜å€¼ã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬å°†è¿™ä¸ªç³»åˆ—ä¿å­˜ä¸ºä¸€ä¸ªæ–°å˜é‡ï¼Œå¹¶æŸ¥çœ‹ä¹‹å‰ä½¿ç”¨çš„ç‰¹å®šæ—¥æœŸã€‚
- en: So I'm going to save this as a variable and call that highsã€‚ And thenã€‚Let's
    access that specific date of 20200101 for the highsã€‚ Nowã€‚ what we should get here
    since we're using the same date that we did hereã€‚ we should get this value of
    132ã€‚68ã€‚ So if I run thatã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æŠŠè¿™ä¸ªä¿å­˜ä¸ºä¸€ä¸ªå˜é‡ï¼Œç§°ä¹‹ä¸ºhighsã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬è®¿é—®20200101è¿™ä¸€å¤©çš„é«˜å€¼ã€‚ç°åœ¨ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ç›¸åŒçš„æ—¥æœŸï¼Œæˆ‘ä»¬åº”è¯¥å¾—åˆ°132.68è¿™ä¸ªå€¼ã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªã€‚
- en: then we can see that the high for that day wasï¼Œ in factï¼Œ equal to what we did
    hereï¼Œ So that worksã€‚ But now instead of just getting one day at a time like we
    did hereã€‚ now that we've resampled this now we have those high values for every
    single day in our dataã€‚ Okayã€‚ so why would something like this be usefulï¼Ÿ I meanã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é‚£å¤©çš„é«˜å€¼å®é™…ä¸Šç­‰äºæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„å€¼ï¼Œæ‰€ä»¥è¿™æœ‰æ•ˆã€‚ä½†ç°åœ¨ï¼Œä¸åƒä¹‹å‰ä¸€æ¬¡åªè·å–ä¸€å¤©çš„æ•°æ®ï¼Œç°åœ¨æˆ‘ä»¬é‡é‡‡æ ·åï¼Œæˆ‘ä»¬æœ‰äº†æ•°æ®ä¸­æ¯ä¸€å¤©çš„é«˜å€¼ã€‚å¥½å§ï¼Œé‚£ä¸ºä»€ä¹ˆè¿™ç§åšæ³•æœ‰ç”¨å‘¢ï¼Ÿæˆ‘çš„æ„æ€æ˜¯ã€‚
- en: you know that might be useful just because it's interestingã€‚ but there are other
    things that we can do as wellã€‚ So let's say that maybe we wanted to plot this
    outã€‚ But instead of you know viewing a plot that had these prices broken down
    hour by hourã€‚ now we can just do a plot for the total price broken down by dayã€‚
    So within Jupyterã€‚Notbooksã€‚ it's extremely easy to plot out informationã€‚ I'm actually
    going to do an entire series on plotting with pandasã€‚ so I'm not going to go into
    a ton of details in this videoã€‚ but we will see how we can do a very simple line
    plot here So to do this we first need to use this special line within Jupyter
    notebooks that allows our plots to display within the browser So all we have to
    do is say this is a percent sign here then we can say mappl lib in line now one
    thing that I do want to mention here is that I did have to go and install mapplot
    lib in the virtual environment that I'm using so if you've only installed pandas
    or and that's it then you might want to go back and install mapplotlib or else
    you'll get an import error hereã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œè¿™å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¾ˆæœ‰è¶£ã€‚ä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥åšå…¶ä»–äº‹æƒ…ã€‚å‡è®¾æˆ‘ä»¬æƒ³è¦ç»˜åˆ¶è¿™ä¸ªå›¾ã€‚ä½†ä¸æ˜¯é€å°æ—¶æŸ¥çœ‹è¿™äº›ä»·æ ¼çš„å›¾ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªæŒ‰å¤©åˆ’åˆ†çš„æ€»ä»·æ ¼å›¾ã€‚åœ¨Jupyter
    Notebookä¸­ï¼Œç»˜åˆ¶ä¿¡æ¯éå¸¸ç®€å•ã€‚æˆ‘å®é™…ä¸Šä¼šåšä¸€ä¸ªå…³äºä½¿ç”¨pandasç»˜å›¾çš„æ•´ä¸ªç³»åˆ—ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šåœ¨è¿™ä¸ªè§†é¢‘ä¸­è¯¦ç»†è®²è§£ï¼Œä½†æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åœ¨è¿™é‡Œåšä¸€ä¸ªéå¸¸ç®€å•çš„æŠ˜çº¿å›¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åœ¨Jupyter
    Notebookä¸­ä½¿ç”¨è¿™ä¸ªç‰¹æ®Šè¡Œï¼Œå…è®¸æˆ‘ä»¬çš„å›¾è¡¨åœ¨æµè§ˆå™¨ä¸­æ˜¾ç¤ºã€‚æ‰€ä»¥æˆ‘ä»¬æ‰€è¦åšçš„å°±æ˜¯åœ¨è¿™é‡ŒåŠ ä¸€ä¸ªç™¾åˆ†å·ï¼Œç„¶åå¯ä»¥è¯´matplotlib inlineã€‚æˆ‘æƒ³æåˆ°çš„ä¸€ä»¶äº‹æ˜¯ï¼Œæˆ‘ç¡®å®å¿…é¡»åœ¨æˆ‘ä½¿ç”¨çš„è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…matplotlibï¼Œæ‰€ä»¥å¦‚æœä½ åªå®‰è£…äº†pandasï¼Œé‚£å°±è¦å›å»å®‰è£…matplotlibï¼Œå¦åˆ™ä½ ä¼šåœ¨è¿™é‡Œé‡åˆ°å¯¼å…¥é”™è¯¯ã€‚
- en: but I went and install that in my virtual environment so we can see that that
    worked there and with that one line of code there now weã€‚Can display plots directly
    within our Jupiter notebookã€‚ So I can simply run the plot method on this data
    frame variable that was resampled and get a plot of thatã€‚ So I'm just going to
    sayï¼Œ okayï¼Œ I want highsã€‚Plootted outã€‚ So highs dot plotã€‚ I'll run thatã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘åœ¨æˆ‘çš„è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…äº†è¿™ä¸ªï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒåœ¨é‚£é‡Œå·¥ä½œï¼Œç°åœ¨ç”¨è¿™ä¸€è¡Œä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨æˆ‘ä»¬çš„Jupyterç¬”è®°æœ¬ä¸­æ˜¾ç¤ºå›¾è¡¨ã€‚æ‰€ä»¥æˆ‘å¯ä»¥ç®€å•åœ°å¯¹è¿™ä¸ªé‡é‡‡æ ·çš„æ•°æ®æ¡†å˜é‡è¿è¡Œç»˜å›¾æ–¹æ³•ï¼Œè·å¾—å›¾è¡¨ã€‚æ‰€ä»¥æˆ‘åªæ˜¯è¯´ï¼Œå¥½å§ï¼Œæˆ‘æƒ³è¦é«˜å€¼çš„å›¾è¡¨ã€‚æ‰€ä»¥é«˜å€¼.dotå›¾è¡¨ã€‚æˆ‘ä¼šè¿è¡Œè¿™ä¸ªã€‚
- en: And we can see that we get a nice mattepllib plot hereã€‚ Okayï¼Œ so that'sï¼Œ you
    knowï¼Œ pretty nice forã€‚ you knowï¼Œ just a few lines of code thereã€‚ Now one thing
    that you might be wondering is if it's possible to resample multiple columns at
    onceã€‚ And we can do that by running the reample method on our entire data frame
    instead of one a single seriesã€‚ So for exampleï¼Œ what do I mean by thisã€‚ Okayï¼Œ
    so whenever I sayï¼Œ you knowã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªä¸é”™çš„matplotlibå›¾ã€‚å¥½å§ï¼Œæ‰€ä»¥è¿™å¯¹äºï¼ŒçŸ¥é“å—ï¼Œåªæ˜¯å‡ è¡Œä»£ç æ¥è¯´ï¼ŒæŒºä¸é”™çš„ã€‚ç°åœ¨ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“æ˜¯å¦å¯ä»¥ä¸€æ¬¡é‡é‡‡æ ·å¤šä¸ªåˆ—ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æ•´ä¸ªæ•°æ®æ¡†ä¸Šè¿è¡Œé‡é‡‡æ ·æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè€Œä¸æ˜¯åœ¨å•ä¸ªç³»åˆ—ä¸Šã€‚é‚£ä¹ˆï¼Œä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘è¿™æ˜¯ä»€ä¹ˆæ„æ€ã€‚å¥½å§ï¼Œæ¯å½“æˆ‘è¯´ï¼ŒçŸ¥é“å—ã€‚
- en: resample multiple columns at onceã€‚ I mean that what if we wanted to resample
    this by dayã€‚ But so farï¼Œ we've only seenï¼Œ okayï¼Œ how we got the high valueï¼Œ But
    what if we saidï¼Œ okayã€‚ well I want to reample this by dayã€‚ But I also wantï¼Œ you
    knowã€‚ the average closing cost of that entire dayã€‚ I want the sum of all of these
    volumes for that entire dayã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ¬¡é‡é‡‡æ ·å¤šä¸ªåˆ—ã€‚æˆ‘æ˜¯è¯´å¦‚æœæˆ‘ä»¬æƒ³æŒ‰å¤©é‡é‡‡æ ·ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªçœ‹åˆ°å¦‚ä½•è·å¾—é«˜å€¼ï¼Œä½†å¦‚æœæˆ‘ä»¬è¯´ï¼Œå¥½å§ï¼Œæˆ‘æƒ³æŒ‰å¤©é‡é‡‡æ ·ã€‚ä½†æ˜¯æˆ‘è¿˜æƒ³è¦ï¼ŒçŸ¥é“å—ã€‚æ•´å¤©çš„å¹³å‡æ”¶ç›˜ä»·ã€‚æˆ‘æƒ³è¦é‚£æ•´å¤©æ‰€æœ‰äº¤æ˜“é‡çš„æ€»å’Œã€‚
- en: And then I want the you knowã€‚ğŸ˜Šï¼ŒThe max high valueã€‚ And I want the min low valueã€‚
    So the way that we've done that down here where we just access that single column
    we wouldn't be able to do it using this method that we did hereã€‚ So in order to
    resample and use multiple columns like thatã€‚ here's how we can do thisã€‚ So we
    can do this by running the resample method on our entire data frameã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘æƒ³è¦çŸ¥é“ğŸ˜Šï¼Œæœ€å¤§é«˜å€¼ã€‚æˆ‘æƒ³è¦æœ€å°ä½å€¼ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œé€šè¿‡è®¿é—®å•åˆ—çš„æ–¹å¼æ¥åšè¿™ä»¶äº‹ï¼Œè€Œç”¨è¿™ç§æ–¹æ³•æˆ‘ä»¬æ— æ³•åšåˆ°ã€‚æ‰€ä»¥ä¸ºäº†é‡é‡‡æ ·å¹¶ä½¿ç”¨å¤šä¸ªåˆ—ï¼Œæ–¹æ³•å¦‚ä¸‹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æ•´ä¸ªæ•°æ®æ¡†ä¸Šè¿è¡Œé‡é‡‡æ ·æ–¹æ³•æ¥å®ç°ã€‚
- en: So if you want to use the same aggregation method on all of your columnsã€‚ So
    for exampleã€‚ let's say Df do resampleã€‚ So now we're resampling our entire data
    frame object hereã€‚ And now we're gonna to pass in what we want to reample onã€‚
    instead of day let's change it up and do weakã€‚ Now we'll reample by each weekã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ æƒ³åœ¨æ‰€æœ‰åˆ—ä¸Šä½¿ç”¨ç›¸åŒçš„èšåˆæ–¹æ³•ã€‚ä¸¾ä¸ªä¾‹å­ã€‚å‡è®¾Dfè¿›è¡Œé‡é‡‡æ ·ã€‚ç°åœ¨æˆ‘ä»¬åœ¨è¿™é‡Œé‡é‡‡æ ·æ•´ä¸ªæ•°æ®æ¡†å¯¹è±¡ã€‚ç°åœ¨æˆ‘ä»¬è¦ä¼ å…¥æˆ‘ä»¬æƒ³è¦é‡é‡‡æ ·çš„å†…å®¹ã€‚æˆ‘ä»¬æŠŠâ€œå¤©â€æ¢æˆâ€œå‘¨â€ã€‚ç°åœ¨æˆ‘ä»¬å°†æŒ‰æ¯å‘¨è¿›è¡Œé‡é‡‡æ ·ã€‚
- en: So if you want to use the same aggregation method on everythingã€‚ then you can
    just put in that aggregation method thereã€‚ So if I run thisã€‚ then this is gonna
    give me the mean values forã€‚each of our columns on a weekly basis Now this is
    cool that we can do this and sometimes you might want to do something like thisã€‚
    but in this instanceï¼Œ it doesn't really make sense to use mean to get the average
    of all of our columns So for exampleã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ æƒ³å¯¹æ‰€æœ‰å†…å®¹ä½¿ç”¨ç›¸åŒçš„èšåˆæ–¹æ³•ï¼Œä½ åªéœ€åœ¨é‚£é‡Œæ”¾å…¥è¯¥èšåˆæ–¹æ³•ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆè¿™å°†ç»™æˆ‘æ¯ä¸ªåˆ—çš„å‘¨å‡å€¼ã€‚ç°åœ¨è¿™å¾ˆé…·ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼Œæœ‰æ—¶ä½ å¯èƒ½æƒ³è¿™æ ·åšï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å‡å€¼æ¥è·å–æ‰€æœ‰åˆ—çš„å¹³å‡å€¼å¹¶ä¸åˆç†ã€‚æ‰€ä»¥ä¾‹å¦‚ã€‚
- en: there's no real reason to get the average volume per hour or something like
    thatã€‚ you've probably want to get the sum for the entire time period or for our
    high and low values hereã€‚ these are giving us the average highs and the average
    lows but the point of a high and low value is to know the high for that time period
    and the low for that time periodã€‚ So we probably don't want mean here eitherã€‚
    So how can we resample this to where we can you know resample and use multiple
    columns but also use multiple aggregation methodsã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šæ²¡æœ‰å¿…è¦æ¯å°æ—¶è·å–å¹³å‡äº¤æ˜“é‡ä¹‹ç±»çš„ä¸œè¥¿ã€‚ä½ å¯èƒ½å¸Œæœ›å¯¹æ•´ä¸ªæ—¶é—´æ®µè¿›è¡Œæ±‚å’Œï¼Œæˆ–è€…å¯¹äºæˆ‘ä»¬çš„é«˜ä½å€¼ï¼Œè¿™äº›ç»™å‡ºäº†å¹³å‡é«˜å’Œå¹³å‡ä½ï¼Œä½†é«˜ä½å€¼çš„æ„ä¹‰åœ¨äºäº†è§£è¯¥æ—¶é—´æ®µçš„æœ€é«˜å’Œæœ€ä½å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬å¯èƒ½ä¹Ÿä¸æƒ³åœ¨è¿™é‡Œä½¿ç”¨å‡å€¼ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•é‡é‡‡æ ·ä»¥ä¾¿å¯ä»¥é‡é‡‡æ ·å¹¶ä½¿ç”¨å¤šä¸ªåˆ—ï¼ŒåŒæ—¶ä¹Ÿä½¿ç”¨å¤šä¸ªèšåˆæ–¹æ³•å‘¢ã€‚
- en: Now we've actually seen this in previous videos and use this method But what
    we want to use here is the A G the ag method and the ag method also accepts a
    mapã€‚Of columns and the aggregation functions that we want to run on that columnã€‚
    Soï¼Œ for exampleã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®é™…ä¸Šåœ¨ä¹‹å‰çš„è§†é¢‘ä¸­å·²ç»çœ‹åˆ°è¿‡è¿™ä¸ªæ–¹æ³•ã€‚ä½†æ˜¯æˆ‘ä»¬è¿™é‡Œæƒ³ç”¨çš„æ˜¯AGæ–¹æ³•ï¼Œagæ–¹æ³•è¿˜æ¥å—æˆ‘ä»¬å¸Œæœ›åœ¨è¯¥åˆ—ä¸Šè¿è¡Œçš„åˆ—å’Œèšåˆå‡½æ•°çš„æ˜ å°„ã€‚ä¾‹å¦‚ã€‚
- en: let's do this with the values forï¼Œ let's seeã€‚ We'll do the closing columnã€‚ We'll
    do the high and low columnsï¼Œ and then we'll also do the volume hereã€‚ So I'm going
    to grab this from up hereã€‚And then we'll do D F dot resampleã€‚ and we'll pass in
    a W for a weekly basisã€‚ And nowï¼Œ instead of passing in dot meanã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨è¿™äº›å€¼æ¥åšï¼Œçœ‹çœ‹ã€‚æˆ‘ä»¬å°†å¤„ç†æ”¶ç›˜åˆ—ï¼Œé«˜åˆ—å’Œä½åˆ—ï¼Œç„¶åæˆ‘ä»¬ä¹Ÿå°†å¤„ç†äº¤æ˜“é‡ã€‚æ‰€ä»¥æˆ‘è¦ä»ä¸Šé¢è·å–è¿™äº›ã€‚ç„¶åæˆ‘ä»¬å°†è¿›è¡ŒD F.dot.resampleï¼Œå¹¶ä¼ å…¥Wä»¥è¿›è¡ŒæŒ‰å‘¨è®¡ç®—ã€‚ç°åœ¨ï¼Œä»£æ›¿ä¼ å…¥dot.meanã€‚
- en: like we did up hereï¼Œ I'm going to pass in dot A G Gã€‚ And now I can pass in a
    dictionary of the columns and or the column namesã€‚ And then the values will be
    the aggregation function that we want to use on that columnã€‚ Soã€‚ for exampleï¼Œ
    let's say that for the closing valueã€‚ I do want to grab the mean of thatã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„ï¼Œæˆ‘å°†ä¼ å…¥dot.AGGã€‚ç°åœ¨æˆ‘å¯ä»¥ä¼ å…¥åˆ—åçš„å­—å…¸ï¼Œç„¶åå€¼å°†æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨è¯¥åˆ—ä¸Šä½¿ç”¨çš„èšåˆå‡½æ•°ã€‚æ‰€ä»¥ã€‚ä¾‹å¦‚ï¼Œå‡è®¾å¯¹äºæ”¶ç›˜å€¼ï¼Œæˆ‘ç¡®å®æƒ³è·å–å…¶å‡å€¼ã€‚
- en: And then I'll say for the high columnã€‚ I want to use the maxã€‚Aggregation function
    for thatã€‚ Since we want the max value for the low columnï¼Œ I want to get the minã€‚And
    for volumeã€‚ I'll go ahead and just sum upã€‚All of the volume for that entire time
    periodã€‚ Okayï¼Œ so againã€‚ the keys here for the dictionary that we passed into agï¼Œ
    the ag methodã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä¼šè¯´é«˜åˆ—ã€‚æˆ‘æƒ³ä¸ºæ­¤ä½¿ç”¨æœ€å¤§å€¼èšåˆå‡½æ•°ã€‚å› ä¸ºæˆ‘ä»¬æƒ³è¦ä½åˆ—çš„æœ€å¤§å€¼ï¼Œæ‰€ä»¥æˆ‘æƒ³è·å–æœ€å°å€¼ã€‚å¯¹äºäº¤æ˜“é‡ï¼Œæˆ‘å°†ç›´æ¥å¯¹æ•´ä¸ªæ—¶é—´æ®µçš„äº¤æ˜“é‡è¿›è¡Œæ±‚å’Œã€‚å¥½çš„ï¼Œæ‰€ä»¥å†æ¬¡å¼ºè°ƒï¼Œæˆ‘ä»¬ä¼ é€’ç»™agçš„æ–¹æ³•ä¸­çš„å­—å…¸çš„é”®ã€‚
- en: this is the column name hereã€‚ then this is the aggregation functionã€‚ So we're
    taking the mean of clothesã€‚ We're taking the max for this entire weekly period
    hereã€‚ for the highs the min for the low and then sum for volumeã€‚ So if we run
    thisã€‚ then it gives us this nice weekly overview of the you know the weekly highs
    and the weekly lows hereã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åˆ—åï¼Œè¿™é‡Œæ˜¯èšåˆå‡½æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬æ­£åœ¨è®¡ç®—æ”¶ç›˜ä»·çš„å‡å€¼ã€‚æˆ‘ä»¬åœ¨æ•´ä¸ªå‘¨æœŸé—´è®¡ç®—é«˜ä»·çš„æœ€å¤§å€¼ï¼Œä½ä»·çš„æœ€å°å€¼ï¼Œç„¶åå¯¹äº¤æ˜“é‡æ±‚å’Œã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå®ƒå°†ç»™æˆ‘ä»¬æä¾›æ¯å‘¨çš„é«˜ä½æ¦‚è¿°ã€‚
- en: and also the average closing costs hereã€‚ And we also have the summation of the
    volume of tradesã€‚ Soã€‚ you knowï¼Œ this really touches on what we can do with date
    times and time series data in pandasã€‚ like I said a little bit agoã€‚ I do plan
    on doing a full series on pandas plotting where we'll cover more advanced topics
    you knowã€‚ such as plotting plotting things out and having rollingã€‚Averages for
    data and things like thatã€‚ Nowã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰æ”¶ç›˜æˆæœ¬çš„å¹³å‡å€¼ã€‚åŒæ—¶æˆ‘ä»¬ä¹Ÿæœ‰äº¤æ˜“é‡çš„æ€»å’Œã€‚æ‰€ä»¥ï¼Œä½ çŸ¥é“ï¼Œè¿™çœŸçš„æ¶‰åŠåˆ°æˆ‘ä»¬åœ¨pandasä¸­å¯ä»¥å¯¹æ—¥æœŸæ—¶é—´å’Œæ—¶é—´åºåˆ—æ•°æ®åšçš„äº‹æƒ…ã€‚å°±åƒæˆ‘åˆšæ‰è¯´çš„ï¼Œæˆ‘ç¡®å®è®¡åˆ’åšä¸€ä¸ªå…³äºpandasç»˜å›¾çš„å®Œæ•´ç³»åˆ—ï¼Œæˆ‘ä»¬å°†æ¶µç›–æ›´é«˜çº§çš„è¯é¢˜ï¼Œæ¯”å¦‚ç»˜å›¾å’Œæ»šåŠ¨å¹³å‡ç­‰ã€‚
- en: before we do end hereï¼Œ I do want to thank the sponsor of this videoï¼Œ and that
    is brilliantã€‚ and I really enjoy the tutorials that brilliant provides and would
    definitely recommend checking them outã€‚ So in this seriesï¼Œ we've been learning
    about pandas and how to analyze data in Pythonã€‚ And brilliant would be an excellent
    way to supplement what you learn here with their handson coursesã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»“æŸä¹‹å‰ï¼Œæˆ‘æƒ³æ„Ÿè°¢æœ¬è§†é¢‘çš„èµåŠ©å•†ï¼Œé‚£å°±æ˜¯Brilliantã€‚æˆ‘éå¸¸å–œæ¬¢Brilliantæä¾›çš„æ•™ç¨‹ï¼Œå¹¶ä¸”ç»å¯¹æ¨èä½ ä»¬å»çœ‹çœ‹ã€‚åœ¨è¿™ä¸ªç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†Pandasä»¥åŠå¦‚ä½•åœ¨Pythonä¸­åˆ†ææ•°æ®ã€‚Brilliantå°†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¡¥å……ï¼Œå¸®åŠ©ä½ é€šè¿‡ä»–ä»¬çš„å®è·µè¯¾ç¨‹æ¥æ‰©å±•åœ¨è¿™é‡Œå­¦åˆ°çš„çŸ¥è¯†ã€‚
- en: They have some excellent courses and lessons that do a deep dive on how to think
    about and analyze data correctlyã€‚ for data analysis fundamentalsï¼Œ I would really
    recommend checking out their statistics courseã€‚ which shows you how to analyze
    graphs and determine significance in the dataã€‚ And I would also recommend their
    machine learning courseã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æœ‰ä¸€äº›ä¼˜ç§€çš„è¯¾ç¨‹å’Œè¯¾ä»¶ï¼Œæ·±å…¥æ¢è®¨å¦‚ä½•æ­£ç¡®æ€è€ƒå’Œåˆ†ææ•°æ®ã€‚å¯¹äºæ•°æ®åˆ†æåŸºç¡€ï¼Œæˆ‘å¼ºçƒˆæ¨èä»–ä»¬çš„ç»Ÿè®¡å­¦è¯¾ç¨‹ï¼Œå®ƒæ•™ä½ å¦‚ä½•åˆ†æå›¾è¡¨å¹¶ç¡®å®šæ•°æ®çš„æ˜¾è‘—æ€§ã€‚æˆ‘ä¹Ÿæ¨èä»–ä»¬çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ã€‚
- en: which takes data analysis to a new level where you' learn about the techniques
    being used that allow machines to make decisions where there's just too many variables
    for a human to considerã€‚ So to support my channel and learn more about brilliantã€‚
    You can go to brilliant org forgelash cs to sign up for freeã€‚ And also the first
    200 people they go to that link will get 20% off the annualã€‚ğŸ˜Šã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æ•°æ®åˆ†ææå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼Œä½ å°†å­¦ä¹ è¢«ç”¨æ¥è®©æœºå™¨åšå‡ºå†³ç­–çš„æŠ€æœ¯ï¼Œè€Œè¿™äº›å†³ç­–æ¶‰åŠçš„å˜é‡å¯¹äºäººç±»æ¥è¯´å®åœ¨æ˜¯å¤ªå¤šäº†ã€‚å› æ­¤ï¼Œæ”¯æŒæˆ‘çš„é¢‘é“å¹¶äº†è§£æ›´å¤šå…³äºBrilliantçš„ä¿¡æ¯ï¼Œä½ å¯ä»¥è®¿é—®brilliant.org/forgelash/csï¼Œæ³¨å†Œå…è´¹è´¦æˆ·ã€‚æ­¤å¤–ï¼Œå‰200ä½è®¿é—®è¯¥é“¾æ¥çš„äººå°†è·å¾—20%çš„å¹´è´¹æŠ˜æ‰£ã€‚ğŸ˜Šã€‚
- en: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_1.png)'
- en: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_2.png)'
- en: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_3.png)'
- en: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_4.png)'
- en: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_5.png)'
- en: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfdbb346186d8f4761ed57fbc05d2db1_6.png)'
- en: Premium subscriptionã€‚ And you can find that link in the description section
    belowã€‚ againã€‚ that's brilliantt org Forlash Cã€‚ Okay so I think that's gonna do
    it for this pandas videoã€‚ I hope you feel like you got a good idea for how to
    work with date and time series data within pandasã€‚ And like I saidï¼Œ there's a
    lot more that we can cover with datetime dataã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜çº§è®¢é˜…ã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†æ‰¾åˆ°é‚£ä¸ªé“¾æ¥ã€‚å†æ¬¡å¼ºè°ƒï¼Œé‚£æ˜¯brilliant.org/flashCã€‚å¥½çš„ï¼Œæˆ‘æƒ³è¿™å°±æ˜¯æœ¬æœŸPandasè§†é¢‘çš„å†…å®¹ã€‚æˆ‘å¸Œæœ›ä½ èƒ½å¯¹å¦‚ä½•åœ¨Pandasä¸­å¤„ç†æ—¥æœŸå’Œæ—¶é—´åºåˆ—æ•°æ®æœ‰ä¸ªå¾ˆå¥½çš„ç†è§£ã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬è¿˜æœ‰å¾ˆå¤šå†…å®¹å¯ä»¥è¦†ç›–ä¸æ—¥æœŸæ—¶é—´æ•°æ®ç›¸å…³çš„çŸ¥è¯†ã€‚
- en: But I feel like what we did here should definitely provide you with the basics
    of being able to convert analyze and resample your data so that you can do the
    exact analysis that you needã€‚ Now in the next videoã€‚ we're gonna be learning how
    to read data in pandas from different sourcesã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—æˆ‘ä»¬æ‰€åšçš„åº”è¯¥ä¸ºä½ æä¾›èƒ½å¤Ÿè½¬æ¢ã€åˆ†æå’Œé‡é‡‡æ ·æ•°æ®çš„åŸºç¡€ï¼Œä»¥ä¾¿ä½ èƒ½å¤Ÿè¿›è¡Œæ‰€éœ€çš„ç²¾ç¡®åˆ†æã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä»ä¸åŒæ¥æºè¯»å–Pandasä¸­çš„æ•°æ®ã€‚
- en: So far in this series we've only coveredv filesï¼Œ but we're gonna learn how to
    read in data from Excel websites SQl databases and a few moreã€‚ So be sure to stick
    around for thatã€‚ But if anyone has any questions about what be covered in this
    video feel free to ask in the comment section belowã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨è¿™ä¸ªç³»åˆ—ä¸­æˆ‘ä»¬åªæ¶µç›–äº†æ–‡ä»¶ï¼Œä½†æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä»Excelã€ç½‘ç«™ã€SQLæ•°æ®åº“å’Œå…¶ä»–ä¸€äº›æ¥æºè¯»å–æ•°æ®ã€‚è¯·åŠ¡å¿…å…³æ³¨è¿™ä¸€å†…å®¹ã€‚å¦‚æœæœ‰äººå¯¹æœ¬è§†é¢‘ä¸­æ‰€æ¶‰åŠçš„å†…å®¹æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·éšæ—¶åœ¨ä¸‹é¢çš„è¯„è®ºåŒºæé—®ã€‚
- en: And I'll do my best to answer thoseã€‚ And if you enjoy these tutorialsã€‚ğŸ˜Šï¼ŒLike
    to support themã€‚ Then there are some ways you can do thatã€‚ The easiest ways to
    simply like the video and give it a thumbs upã€‚ And alsoï¼Œ it's a huge help to share
    these videos with anyone who you think would find them usefulã€‚ And if you have
    the meansï¼Œ you can contribute to Patreonã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šå°½åŠ›å›ç­”è¿™äº›é—®é¢˜ã€‚å¦‚æœä½ å–œæ¬¢è¿™äº›æ•™ç¨‹ã€‚ğŸ˜Šï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç‚¹èµæ¥æ”¯æŒå®ƒä»¬ã€‚è¿˜æœ‰ä¸€äº›å…¶ä»–æ–¹å¼å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æœ€ç®€å•çš„æ–¹å¼å°±æ˜¯ç»™è§†é¢‘ç‚¹ä¸ªèµï¼Œå¹¶ä¸”åˆ†äº«è¿™äº›è§†é¢‘ç»™ä½ è®¤ä¸ºä¼šè§‰å¾—æœ‰ç”¨çš„äººã€‚å¦‚æœä½ æœ‰æ¡ä»¶çš„è¯ï¼Œå¯ä»¥æ”¯æŒæˆ‘çš„Patreonã€‚
- en: And there's a link to that page in the description section belowã€‚ Be sure to
    subscribe for future videosã€‚ And thank you all for watchingã€‚ğŸ˜Šã€‚![](img/cfdbb346186d8f4761ed57fbc05d2db1_8.png)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†æœ‰ä¸€ä¸ªé“¾æ¥ã€‚è¯·åŠ¡å¿…è®¢é˜…ä»¥è·å–æœªæ¥çš„è§†é¢‘ã€‚æ„Ÿè°¢å¤§å®¶çš„è§‚çœ‹ã€‚ğŸ˜Šã€‚![](img/cfdbb346186d8f4761ed57fbc05d2db1_8.png)
