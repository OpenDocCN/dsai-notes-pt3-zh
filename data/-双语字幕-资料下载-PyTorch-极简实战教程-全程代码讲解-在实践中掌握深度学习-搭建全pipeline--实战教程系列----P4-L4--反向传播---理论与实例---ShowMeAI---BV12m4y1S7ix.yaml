- en: 【双语字幕+资料下载】PyTorch 极简实战教程！全程代码讲解，在实践中掌握深度学习&搭建全pipeline！＜实战教程系列＞ - P4：L4- 反向传播
    - 理论与实例 - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】PyTorch 极简实战教程！全程代码讲解，在实践中掌握深度学习&搭建全pipeline！＜实战教程系列＞ - P4：L4- 反向传播
    - 理论与实例 - ShowMeAI - BV12m4y1S7ix
- en: Hi， everybody。 Welcome to a new Pytorch tutorial。 In this video。 I am going
    to explain the famous Beck propagation algorithm and how we can calculate gradients
    with it。😊，I explain the necessary concepts of this technique。 and then I will
    walk you through a concrete example with some numbers。And at the end。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，欢迎来到新的 PyTorch 教程。在这个视频中，我将解释著名的反向传播算法，以及我们如何用它计算梯度。😊我将解释这项技术的必要概念，然后带你通过一个具体的示例和一些数字。最后。
- en: we will then see how easy it is to apply back propagation in Pytorarch。 So let's
    start。And the first concept we must know is the chain rule。 So let's say we have
    two operations or two functions。So first， we have the input X。 and then we apply
    a function A and get an output Y。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将看到在 PyTorch 中应用反向传播是多么简单。所以让我们开始。首先，我们必须了解的概念是链式法则。假设我们有两个操作或两个函数。首先，我们有输入
    X，然后我们应用一个函数 A，得到输出 Y。
- en: And then we use this output as the input for our second function。 So the second
    function B。 And then we get the final output C。And now we want to minimize our
    C。 So we want to know the derivative of C with respect to our x here in the beginning。And
    we can do this using the so called chain rule。 So for this。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将这个输出作为我们第二个函数的输入。第二个函数 B。然后我们得到最终输出 C。现在我们想要最小化我们的 C。因此，我们想知道 C 关于我们最开始的
    x 的导数。我们可以使用所谓的链式法则来做到这一点。
- en: we first compute the derivative of C with respect to y and multiply this with
    the derivative or of y with respect to X。 And then we get the final derivative
    we want。So first， here。 we compute the derivative at this position。So the derivative
    of this output with respect to this input。 And then here， the derivative of this
    output with respect to this input。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算 C 关于 y 的导数，并将其与 y 关于 X 的导数相乘。然后我们得到我们想要的最终导数。所以首先，在这里，我们计算这个位置的导数。这个输出相对于这个输入的导数。然后在这里，这个输出相对于这个输入的导数。
- en: And then we multiply them together and get the final gradient we are interested
    in。 So that's the chain rule。And now the next concept is the so called computational
    graph。So for every operation we do with our tens source， Pyto will create a graph
    for us。 So where at each note， we apply one operation or one function with some
    inputs and then get an output。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将它们相乘，得到我们感兴趣的最终梯度。这就是链式法则。接下来要了解的概念是所谓的计算图。对于我们在 PyTorch 中进行的每一个操作，PyTorch
    会为我们创建一个图。在每个节点上，我们应用一个操作或一个函数，并获得输出。
- en: So here in this case， in this example， we use a multiplication operation。 So
    we multiply x and y and then get C。And now at these notes。 we can calculate so
    called local gradients， and we can use them later in the chain rule to get the
    final gradient。 So here， the local gradients， we can compute two gradients， the
    gradient of C with respect to x。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用乘法操作。我们将 x 和 y 相乘，然后得到 C。在这些节点上，我们可以计算所谓的局部梯度，稍后可以在链式法则中使用它们来得到最终梯度。在这里，局部梯度，我们可以计算两个梯度，C
    关于 x 的梯度。
- en: And this is simple。 since we know this function here。 So this is the gradient
    gradient of x times y with respect to x， which is y。 And here in the bottom。 we
    compute the derivative of x times y with respect to y， which is x。So local gradients
    are easy because we know this function。 And why do we want them。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单，因为我们知道这个函数。所以这是 x 和 y 关于 x 的导数的梯度，即 y。在底部，我们计算 x 和 y 关于 y 的导数，结果是 x。局部梯度很简单，因为我们知道这个函数。我们为什么要它们呢？
- en: Because typically our graph has more operations。 And at the very end。 we calculate
    a loss function that we want to minimize。 So we have to calculate the gradient
    of this loss with respect to our parameter X in the beginning。And。Now， let's suppose
    at this position， we already know the derivative of the loss with respect to our
    C。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因为通常我们的图有更多操作。在最后，我们计算一个我们想要最小化的损失函数。因此，我们必须计算这个损失关于我们最开始的参数 X 的梯度。现在，假设在这个位置，我们已经知道损失关于
    C 的导数。
- en: and then we can get the final gradient we want。 So the with the chain rule。
    So the gradient of the loss with respect to X is then the gradient of loss with
    respect to C。Times our local gradient。 So the derivative of C with respect to
    x。And yeah。 this is how we get the final gradient then， and。Now， the whole concept
    consists of three steps。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以得到我们想要的最终梯度。通过链式法则。损失相对于X的梯度是损失相对于C的梯度乘以我们的局部梯度。因此，C相对于x的导数。是的，这就是我们得到最终梯度的方式。现在，整个概念由三个步骤组成。
- en: So first， we do a forward pass where we apply all the functions and compute
    the loss。Then at each note， we calculate the local gradients。 and then we do a
    so called backward pass where we compute the gradient of the loss with respect
    to our weights or parameters using the chain rule。So these are the three steps
    we are going do。 And now we look at a concrete example。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们进行前向传播，应用所有的函数并计算损失。然后在每个节点，我们计算局部梯度。接着我们进行所谓的反向传播，在这里我们使用链式法则计算损失相对于我们的权重或参数的梯度。这就是我们将要做的三个步骤。现在我们看一个具体的例子。
- en: So here we want to use linear regression。And if you don't know how this works。
    then I highly recommend my machine learning from scratch tutorial about linear
    regression。I will put the link in the description。So basically。 we model our output
    with a linear combination of some weights and an input。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这里想要使用线性回归。如果你不知道这如何工作，那么我强烈推荐我的《从零开始的机器学习》教程，关于线性回归。我会在描述中放上链接。因此，基本上，我们用一些权重和输入的线性组合来建模我们的输出。
- en: So our y hat or y predicted is w times x。And then we formulate some loss function。
    So in this case。 this is the squared error。 Actually， it should be the mean squared
    error， but for simplicity。 we just use the squared error。 Otherwise， we would
    have another operation to get the mean。 So the loss is the difference of the predicted
    y minus the actual y， and then we square it。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的y hat或预测的y是w乘以x。然后我们制定一些损失函数。在这种情况下，这就是平方误差。实际上，它应该是均方误差，但为了简单起见，我们只使用平方误差。否则，我们还需要进行另一个操作来获取均值。因此，损失是预测的y与实际y的差值，然后我们对其进行平方。
- en: And now we want to minimize our loss。 So we want to know the derivative of the
    loss with respect to our weights。 And how do we get that， So we apply our three
    steps。 First。 we do a forward pass and put in the x and the W。 And then here we
    put in the y。And apply our functions here。 And then we get the loss。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要最小化我们的损失。我们想知道损失相对于我们权重的导数。我们如何得到这个呢？所以我们应用我们的三个步骤。首先，我们进行前向传播，输入x和W。然后在这里我们输入y，并在这里应用我们的函数。然后我们得到损失。
- en: Then we calculate the the local gradients at each note。 So here。 the gradient
    of the loss with respect to our S。 Then here。 the gradient of the S with respect
    to our y hat。And here at this note。 the gradient of Y hat with respect to our
    W。And then we do a backward pass。 So we start at the end。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在每个节点计算局部梯度。在这里，损失相对于我们的S的梯度。然后这里，S相对于我们的y hat的梯度。在这个节点上，Y hat相对于我们的W的梯度。接着我们进行反向传播，从最后开始。
- en: And here we have the first， we have the derivative of the loss with respect
    to our S。And then we use them， and we also use the chain rule to get the derivative
    of the loss with respect of the Y hat。And then again， we use this and the chain
    rule to get the final gradient of the loss with respect to our W。So let's do this
    with some concrete numbers。 So let's say we have x and y is given。 So x is  one。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们有第一个，我们有损失相对于我们的S的导数。然后我们使用它们，我们还使用链式法则来获取损失相对于Y hat的导数。然后再次，我们使用这个和链式法则来获得损失相对于我们的W的最终梯度。所以让我们用一些具体的数字来做这个。假设我们有x和y。
- en: and y is 2 in the beginning。And so these are our training samples。 and we initialize
    our weight。 So let's say， for example， we say our w is one in the beginning。And
    then we do the forward pass。 So here at the first node， we multiply x and W。 So
    we get Y hat equals one。Then at the next note。 we do a subion。 So y hat y is 1-2
    equals -1。And at the very end， so we square our S。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 而y在开始时是2。所以这些是我们的训练样本。我们初始化我们的权重。假设，例如，我们说我们的w在开始时是1。然后我们进行前向传播。在第一个节点，我们将x和W相乘。所以我们得到Y
    hat等于1。然后在下一个节点，我们进行减法。所以y hat y是1-2等于-1。在最后，我们对我们的S进行平方。
- en: So we have have S squared， so。Our loss， then， is one。And now we calculate the
    local gradient。 So at the last note， we have the gradient of the loss with respect
    to S。 And this is simple because we know the function。 So this is the gradient
    of S squared。 So this is just 2 S。And then at the next note， we have the gradient
    of S with respect to Y hat。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 S 的平方，因此我们的损失是 1。现在我们计算局部梯度。在最后一个节点，我们有损失对 S 的梯度。这很简单，因为我们知道函数。这是 S 的平方的梯度，也就是
    2 S。在下一个节点，我们有 S 对 Y hat 的梯度。
- en: which is the gradient of the function Y hat minus y with respect to Y hat， which
    is just one。And then here at the last node， we have the derivative of Y hat with
    respect to W。 So this is the derivative of。W times x with respect to w， which
    is x。And also。 notice that we don't need to go。 Don't need to know the derivatives
    in this。Graph lines。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是函数 Y hat 减去 y 对 Y hat 的梯度，值为 1。在最后一个节点，我们有 Y hat 对 W 的导数。这是 W 乘以 x 对 w 的导数，结果是
    x。同时，注意我们不需要了解这个图线中的导数。
- en: so we don't。Need to know what is the derivative of S with respect to Y。 And
    also here。 we don't need this because our x and our y are fixed。 So we are only
    interested in our parameters that we want to update here。And yeah。 and then we
    do the backward pass。 So first， now we use our local gradients。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不需要知道 S 对 Y 的导数。而且在这里，我们也不需要这个，因为我们的 x 和 y 是固定的。因此，我们只关注想要在这里更新的参数。然后我们进行反向传播。首先，我们使用我们的局部梯度。
- en: So we want to compute the derivative of the loss with respect to Y hat。 And
    here we use the chain rule with our two local gradients that we just computed。
    which is 2 S times 1。 and S is -1， which we calculated up here。 And then so this
    is  -2。And now we use this derivative and also this local gradient to then get
    the final gradient。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要计算损失对 Y hat 的导数。在这里，我们使用链式法则，结合我们刚刚计算的两个局部梯度，也就是 2 S 乘以 1，而 S 是 -1，这个我们在上面计算过。结果是
    -2。现在我们使用这个导数和这个局部梯度来得到最终的梯度。
- en: the gradient of the loss with respect to our W， which is the gradient of the
    loss with respect to Y hat times the gradient of Y hat with respect to W。 which
    is -2 times x and x is 1。 So the final gradient is -2。So this is the final gradient
    then that we not want to know。 And yeah。 that's all how back propagation works。
    And let's jump over to our code and verify that Pyto get these exact numbers。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 损失对我们的 W 的梯度，就是损失对 Y hat 的梯度乘以 Y hat 对 W 的梯度。结果是 -2 乘以 x，而 x 是 1。因此，最终的梯度是 -2。这就是我们想知道的最终梯度。这就是反向传播的全部工作原理。让我们跳到代码部分，验证
    PyTorch 是否能得到这些确切的数字。
- en: So let's remember x is 1 Y is 2 and w is 1。 And then our first gradient should
    be -2。![](img/84bf83ae2a8c01617b4cf06b2bc402cb_1.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住 x 是 1，y 是 2，而 w 是 1。那么我们的第一个梯度应该是 -2。![](img/84bf83ae2a8c01617b4cf06b2bc402cb_1.png)
- en: So。![](img/84bf83ae2a8c01617b4cf06b2bc402cb_3.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/84bf83ae2a8c01617b4cf06b2bc402cb_3.png)'
- en: Let's see how we can use this in pie torch。 And， first of all， we import torch，
    of course。Then we create our vector or tenor。 So we say x equals torch dot tenor。
    and this is1。 and then our y equals torch dot tenor。With two。And then our initial
    weight is a tenzor。Also。 with one。So 1。0 to make it a float。 And here， and with
    our weight。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 PyTorch 中使用这个。首先，我们当然要导入 torch。然后我们创建我们的向量或张量。因此，我们说 x 等于 torch.dot.tenor。这是
    1。然后我们的 y 等于 torch.dot.tenor，值为 2。接着我们的初始权重是一个张量，值为 1。因此，1.0 使其成为浮点数。然后用我们的权重。
- en: we are interested in the gradient。 So we need to specify require Sc equals true。And
    then we do the forward pass。And gets and compute the loss。So we simply say， why
    hat equals。W times x， which is our function。 And then we say loss equals y。Hats。Minus。The
    actual y。 And then we square this， So we say。This to the power of two。And now，
    let's print our loss。And see。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对梯度感兴趣。因此我们需要指定 `require Sc` 等于 true。然后我们进行前向传播，计算损失。因此我们简单地说，Y hat 等于 W 乘以
    x，这就是我们的函数。然后我们说损失等于 Y hats 减去实际的 y。然后我们将其平方。所以我们说，这个平方。现在，让我们打印我们的损失，看看结果。
- en: This is one in the beginning。 And now we want to do the backward pass。 So let's
    do the backward pass。 and pi touch will compute the local gradients automatically
    for us and also computes the backward pass automatically for us。 So the only thing
    that we have to call is say loss backward。 So this is the whole gradient computation。
    And now our w has this dot Gr attribute。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是开始的内容。现在我们想要进行反向传播。因此，让我们进行反向传播。`pi touch` 将自动为我们计算局部梯度，同时也自动计算反向传播。因此，我们唯一需要调用的就是
    `loss backward`。这就是整个梯度计算。现在我们的 w 有了这个 dot Gr 属性。
- en: And we can print this。 And now this is the first gradient。 in the after the
    first forward and backward pass。 And remember， this should be-2 in the beginning。
    And here we see we have a tensor with -2。 So this is working。And the next steps
    would be。 for example， Now we update our weights， and then we do the next forward
    and backward pass and do this for a couple of iterations。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印这个。现在这是第一个梯度。在第一次前向和后向传播之后。记住，开始时应该是-2。在这里我们看到有一个张量为-2。所以这个是有效的。接下来的步骤是。例如，现在我们更新我们的权重，然后进行下一次前向和后向传播，并进行几次迭代。
- en: And yeah， that's how Beck propagation works and how， and also how easy it is
    to use it in Pytorch。 And I hope you enjoyed this tutorial。 Please subscribe to
    the channel and see you next time， bye。😊。![](img/84bf83ae2a8c01617b4cf06b2bc402cb_5.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这就是反向传播的工作原理，以及在Pytorch中使用它是多么简单。希望你喜欢这个教程。请订阅频道，下次见，拜！😊。![](img/84bf83ae2a8c01617b4cf06b2bc402cb_5.png)
