- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P4ï¼šL4- åå‘ä¼ æ’­
    - ç†è®ºä¸å®ä¾‹ - ShowMeAI - BV12m4y1S7ix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘PyTorch æç®€å®æˆ˜æ•™ç¨‹ï¼å…¨ç¨‹ä»£ç è®²è§£ï¼Œåœ¨å®è·µä¸­æŒæ¡æ·±åº¦å­¦ä¹ &æ­å»ºå…¨pipelineï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P4ï¼šL4- åå‘ä¼ æ’­
    - ç†è®ºä¸å®ä¾‹ - ShowMeAI - BV12m4y1S7ix
- en: Hiï¼Œ everybodyã€‚ Welcome to a new Pytorch tutorialã€‚ In this videoã€‚ I am going
    to explain the famous Beck propagation algorithm and how we can calculate gradients
    with itã€‚ğŸ˜Šï¼ŒI explain the necessary concepts of this techniqueã€‚ and then I will
    walk you through a concrete example with some numbersã€‚And at the endã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°æ–°çš„ PyTorch æ•™ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘å°†è§£é‡Šè‘—åçš„åå‘ä¼ æ’­ç®—æ³•ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•ç”¨å®ƒè®¡ç®—æ¢¯åº¦ã€‚ğŸ˜Šæˆ‘å°†è§£é‡Šè¿™é¡¹æŠ€æœ¯çš„å¿…è¦æ¦‚å¿µï¼Œç„¶åå¸¦ä½ é€šè¿‡ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹å’Œä¸€äº›æ•°å­—ã€‚æœ€åã€‚
- en: we will then see how easy it is to apply back propagation in Pytorarchã€‚ So let's
    startã€‚And the first concept we must know is the chain ruleã€‚ So let's say we have
    two operations or two functionsã€‚So firstï¼Œ we have the input Xã€‚ and then we apply
    a function A and get an output Yã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†çœ‹åˆ°åœ¨ PyTorch ä¸­åº”ç”¨åå‘ä¼ æ’­æ˜¯å¤šä¹ˆç®€å•ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»äº†è§£çš„æ¦‚å¿µæ˜¯é“¾å¼æ³•åˆ™ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ“ä½œæˆ–ä¸¤ä¸ªå‡½æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰è¾“å…¥
    Xï¼Œç„¶åæˆ‘ä»¬åº”ç”¨ä¸€ä¸ªå‡½æ•° Aï¼Œå¾—åˆ°è¾“å‡º Yã€‚
- en: And then we use this output as the input for our second functionã€‚ So the second
    function Bã€‚ And then we get the final output Cã€‚And now we want to minimize our
    Cã€‚ So we want to know the derivative of C with respect to our x here in the beginningã€‚And
    we can do this using the so called chain ruleã€‚ So for thisã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†è¿™ä¸ªè¾“å‡ºä½œä¸ºæˆ‘ä»¬ç¬¬äºŒä¸ªå‡½æ•°çš„è¾“å…¥ã€‚ç¬¬äºŒä¸ªå‡½æ•° Bã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°æœ€ç»ˆè¾“å‡º Cã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦æœ€å°åŒ–æˆ‘ä»¬çš„ Cã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“ C å…³äºæˆ‘ä»¬æœ€å¼€å§‹çš„
    x çš„å¯¼æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ‰€è°“çš„é“¾å¼æ³•åˆ™æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: we first compute the derivative of C with respect to y and multiply this with
    the derivative or of y with respect to Xã€‚ And then we get the final derivative
    we wantã€‚So firstï¼Œ hereã€‚ we compute the derivative at this positionã€‚So the derivative
    of this output with respect to this inputã€‚ And then hereï¼Œ the derivative of this
    output with respect to this inputã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè®¡ç®— C å…³äº y çš„å¯¼æ•°ï¼Œå¹¶å°†å…¶ä¸ y å…³äº X çš„å¯¼æ•°ç›¸ä¹˜ã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„æœ€ç»ˆå¯¼æ•°ã€‚æ‰€ä»¥é¦–å…ˆï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—è¿™ä¸ªä½ç½®çš„å¯¼æ•°ã€‚è¿™ä¸ªè¾“å‡ºç›¸å¯¹äºè¿™ä¸ªè¾“å…¥çš„å¯¼æ•°ã€‚ç„¶ååœ¨è¿™é‡Œï¼Œè¿™ä¸ªè¾“å‡ºç›¸å¯¹äºè¿™ä¸ªè¾“å…¥çš„å¯¼æ•°ã€‚
- en: And then we multiply them together and get the final gradient we are interested
    inã€‚ So that's the chain ruleã€‚And now the next concept is the so called computational
    graphã€‚So for every operation we do with our tens sourceï¼Œ Pyto will create a graph
    for usã€‚ So where at each noteï¼Œ we apply one operation or one function with some
    inputs and then get an outputã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å®ƒä»¬ç›¸ä¹˜ï¼Œå¾—åˆ°æˆ‘ä»¬æ„Ÿå…´è¶£çš„æœ€ç»ˆæ¢¯åº¦ã€‚è¿™å°±æ˜¯é“¾å¼æ³•åˆ™ã€‚æ¥ä¸‹æ¥è¦äº†è§£çš„æ¦‚å¿µæ˜¯æ‰€è°“çš„è®¡ç®—å›¾ã€‚å¯¹äºæˆ‘ä»¬åœ¨ PyTorch ä¸­è¿›è¡Œçš„æ¯ä¸€ä¸ªæ“ä½œï¼ŒPyTorch
    ä¼šä¸ºæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå›¾ã€‚åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬åº”ç”¨ä¸€ä¸ªæ“ä½œæˆ–ä¸€ä¸ªå‡½æ•°ï¼Œå¹¶è·å¾—è¾“å‡ºã€‚
- en: So here in this caseï¼Œ in this exampleï¼Œ we use a multiplication operationã€‚ So
    we multiply x and y and then get Cã€‚And now at these notesã€‚ we can calculate so
    called local gradientsï¼Œ and we can use them later in the chain rule to get the
    final gradientã€‚ So hereï¼Œ the local gradientsï¼Œ we can compute two gradientsï¼Œ the
    gradient of C with respect to xã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹˜æ³•æ“ä½œã€‚æˆ‘ä»¬å°† x å’Œ y ç›¸ä¹˜ï¼Œç„¶åå¾—åˆ° Cã€‚åœ¨è¿™äº›èŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ‰€è°“çš„å±€éƒ¨æ¢¯åº¦ï¼Œç¨åå¯ä»¥åœ¨é“¾å¼æ³•åˆ™ä¸­ä½¿ç”¨å®ƒä»¬æ¥å¾—åˆ°æœ€ç»ˆæ¢¯åº¦ã€‚åœ¨è¿™é‡Œï¼Œå±€éƒ¨æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸¤ä¸ªæ¢¯åº¦ï¼ŒC
    å…³äº x çš„æ¢¯åº¦ã€‚
- en: And this is simpleã€‚ since we know this function hereã€‚ So this is the gradient
    gradient of x times y with respect to xï¼Œ which is yã€‚ And here in the bottomã€‚ we
    compute the derivative of x times y with respect to yï¼Œ which is xã€‚So local gradients
    are easy because we know this functionã€‚ And why do we want themã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“è¿™ä¸ªå‡½æ•°ã€‚æ‰€ä»¥è¿™æ˜¯ x å’Œ y å…³äº x çš„å¯¼æ•°çš„æ¢¯åº¦ï¼Œå³ yã€‚åœ¨åº•éƒ¨ï¼Œæˆ‘ä»¬è®¡ç®— x å’Œ y å…³äº y çš„å¯¼æ•°ï¼Œç»“æœæ˜¯ xã€‚å±€éƒ¨æ¢¯åº¦å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“è¿™ä¸ªå‡½æ•°ã€‚æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦å®ƒä»¬å‘¢ï¼Ÿ
- en: Because typically our graph has more operationsã€‚ And at the very endã€‚ we calculate
    a loss function that we want to minimizeã€‚ So we have to calculate the gradient
    of this loss with respect to our parameter X in the beginningã€‚Andã€‚Nowï¼Œ let's suppose
    at this positionï¼Œ we already know the derivative of the loss with respect to our
    Cã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºé€šå¸¸æˆ‘ä»¬çš„å›¾æœ‰æ›´å¤šæ“ä½œã€‚åœ¨æœ€åï¼Œæˆ‘ä»¬è®¡ç®—ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦æœ€å°åŒ–çš„æŸå¤±å‡½æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è®¡ç®—è¿™ä¸ªæŸå¤±å…³äºæˆ‘ä»¬æœ€å¼€å§‹çš„å‚æ•° X çš„æ¢¯åº¦ã€‚ç°åœ¨ï¼Œå‡è®¾åœ¨è¿™ä¸ªä½ç½®ï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“æŸå¤±å…³äº
    C çš„å¯¼æ•°ã€‚
- en: and then we can get the final gradient we wantã€‚ So the with the chain ruleã€‚
    So the gradient of the loss with respect to X is then the gradient of loss with
    respect to Cã€‚Times our local gradientã€‚ So the derivative of C with respect to
    xã€‚And yeahã€‚ this is how we get the final gradient thenï¼Œ andã€‚Nowï¼Œ the whole concept
    consists of three stepsã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„æœ€ç»ˆæ¢¯åº¦ã€‚é€šè¿‡é“¾å¼æ³•åˆ™ã€‚æŸå¤±ç›¸å¯¹äºXçš„æ¢¯åº¦æ˜¯æŸå¤±ç›¸å¯¹äºCçš„æ¢¯åº¦ä¹˜ä»¥æˆ‘ä»¬çš„å±€éƒ¨æ¢¯åº¦ã€‚å› æ­¤ï¼ŒCç›¸å¯¹äºxçš„å¯¼æ•°ã€‚æ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¾—åˆ°æœ€ç»ˆæ¢¯åº¦çš„æ–¹å¼ã€‚ç°åœ¨ï¼Œæ•´ä¸ªæ¦‚å¿µç”±ä¸‰ä¸ªæ­¥éª¤ç»„æˆã€‚
- en: So firstï¼Œ we do a forward pass where we apply all the functions and compute
    the lossã€‚Then at each noteï¼Œ we calculate the local gradientsã€‚ and then we do a
    so called backward pass where we compute the gradient of the loss with respect
    to our weights or parameters using the chain ruleã€‚So these are the three steps
    we are going doã€‚ And now we look at a concrete exampleã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œåº”ç”¨æ‰€æœ‰çš„å‡½æ•°å¹¶è®¡ç®—æŸå¤±ã€‚ç„¶ååœ¨æ¯ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬è®¡ç®—å±€éƒ¨æ¢¯åº¦ã€‚æ¥ç€æˆ‘ä»¬è¿›è¡Œæ‰€è°“çš„åå‘ä¼ æ’­ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±ç›¸å¯¹äºæˆ‘ä»¬çš„æƒé‡æˆ–å‚æ•°çš„æ¢¯åº¦ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å°†è¦åšçš„ä¸‰ä¸ªæ­¥éª¤ã€‚ç°åœ¨æˆ‘ä»¬çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚
- en: So here we want to use linear regressionã€‚And if you don't know how this worksã€‚
    then I highly recommend my machine learning from scratch tutorial about linear
    regressionã€‚I will put the link in the descriptionã€‚So basicallyã€‚ we model our output
    with a linear combination of some weights and an inputã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œæƒ³è¦ä½¿ç”¨çº¿æ€§å›å½’ã€‚å¦‚æœä½ ä¸çŸ¥é“è¿™å¦‚ä½•å·¥ä½œï¼Œé‚£ä¹ˆæˆ‘å¼ºçƒˆæ¨èæˆ‘çš„ã€Šä»é›¶å¼€å§‹çš„æœºå™¨å­¦ä¹ ã€‹æ•™ç¨‹ï¼Œå…³äºçº¿æ€§å›å½’ã€‚æˆ‘ä¼šåœ¨æè¿°ä¸­æ”¾ä¸Šé“¾æ¥ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬ç”¨ä¸€äº›æƒé‡å’Œè¾“å…¥çš„çº¿æ€§ç»„åˆæ¥å»ºæ¨¡æˆ‘ä»¬çš„è¾“å‡ºã€‚
- en: So our y hat or y predicted is w times xã€‚And then we formulate some loss functionã€‚
    So in this caseã€‚ this is the squared errorã€‚ Actuallyï¼Œ it should be the mean squared
    errorï¼Œ but for simplicityã€‚ we just use the squared errorã€‚ Otherwiseï¼Œ we would
    have another operation to get the meanã€‚ So the loss is the difference of the predicted
    y minus the actual yï¼Œ and then we square itã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çš„y hatæˆ–é¢„æµ‹çš„yæ˜¯wä¹˜ä»¥xã€‚ç„¶åæˆ‘ä»¬åˆ¶å®šä¸€äº›æŸå¤±å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™å°±æ˜¯å¹³æ–¹è¯¯å·®ã€‚å®é™…ä¸Šï¼Œå®ƒåº”è¯¥æ˜¯å‡æ–¹è¯¯å·®ï¼Œä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªä½¿ç”¨å¹³æ–¹è¯¯å·®ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è¿›è¡Œå¦ä¸€ä¸ªæ“ä½œæ¥è·å–å‡å€¼ã€‚å› æ­¤ï¼ŒæŸå¤±æ˜¯é¢„æµ‹çš„yä¸å®é™…yçš„å·®å€¼ï¼Œç„¶åæˆ‘ä»¬å¯¹å…¶è¿›è¡Œå¹³æ–¹ã€‚
- en: And now we want to minimize our lossã€‚ So we want to know the derivative of the
    loss with respect to our weightsã€‚ And how do we get thatï¼Œ So we apply our three
    stepsã€‚ Firstã€‚ we do a forward pass and put in the x and the Wã€‚ And then here we
    put in the yã€‚And apply our functions hereã€‚ And then we get the lossã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³è¦æœ€å°åŒ–æˆ‘ä»¬çš„æŸå¤±ã€‚æˆ‘ä»¬æƒ³çŸ¥é“æŸå¤±ç›¸å¯¹äºæˆ‘ä»¬æƒé‡çš„å¯¼æ•°ã€‚æˆ‘ä»¬å¦‚ä½•å¾—åˆ°è¿™ä¸ªå‘¢ï¼Ÿæ‰€ä»¥æˆ‘ä»¬åº”ç”¨æˆ‘ä»¬çš„ä¸‰ä¸ªæ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè¾“å…¥xå’ŒWã€‚ç„¶ååœ¨è¿™é‡Œæˆ‘ä»¬è¾“å…¥yï¼Œå¹¶åœ¨è¿™é‡Œåº”ç”¨æˆ‘ä»¬çš„å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°æŸå¤±ã€‚
- en: Then we calculate the the local gradients at each noteã€‚ So hereã€‚ the gradient
    of the loss with respect to our Sã€‚ Then hereã€‚ the gradient of the S with respect
    to our y hatã€‚And here at this noteã€‚ the gradient of Y hat with respect to our
    Wã€‚And then we do a backward passã€‚ So we start at the endã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åœ¨æ¯ä¸ªèŠ‚ç‚¹è®¡ç®—å±€éƒ¨æ¢¯åº¦ã€‚åœ¨è¿™é‡Œï¼ŒæŸå¤±ç›¸å¯¹äºæˆ‘ä»¬çš„Sçš„æ¢¯åº¦ã€‚ç„¶åè¿™é‡Œï¼ŒSç›¸å¯¹äºæˆ‘ä»¬çš„y hatçš„æ¢¯åº¦ã€‚åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸Šï¼ŒY hatç›¸å¯¹äºæˆ‘ä»¬çš„Wçš„æ¢¯åº¦ã€‚æ¥ç€æˆ‘ä»¬è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»æœ€åå¼€å§‹ã€‚
- en: And here we have the firstï¼Œ we have the derivative of the loss with respect
    to our Sã€‚And then we use themï¼Œ and we also use the chain rule to get the derivative
    of the loss with respect of the Y hatã€‚And then againï¼Œ we use this and the chain
    rule to get the final gradient of the loss with respect to our Wã€‚So let's do this
    with some concrete numbersã€‚ So let's say we have x and y is givenã€‚ So x is  oneã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ç¬¬ä¸€ä¸ªï¼Œæˆ‘ä»¬æœ‰æŸå¤±ç›¸å¯¹äºæˆ‘ä»¬çš„Sçš„å¯¼æ•°ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨å®ƒä»¬ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨é“¾å¼æ³•åˆ™æ¥è·å–æŸå¤±ç›¸å¯¹äºY hatçš„å¯¼æ•°ã€‚ç„¶åå†æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå’Œé“¾å¼æ³•åˆ™æ¥è·å¾—æŸå¤±ç›¸å¯¹äºæˆ‘ä»¬çš„Wçš„æœ€ç»ˆæ¢¯åº¦ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ç”¨ä¸€äº›å…·ä½“çš„æ•°å­—æ¥åšè¿™ä¸ªã€‚å‡è®¾æˆ‘ä»¬æœ‰xå’Œyã€‚
- en: and y is 2 in the beginningã€‚And so these are our training samplesã€‚ and we initialize
    our weightã€‚ So let's sayï¼Œ for exampleï¼Œ we say our w is one in the beginningã€‚And
    then we do the forward passã€‚ So here at the first nodeï¼Œ we multiply x and Wã€‚ So
    we get Y hat equals oneã€‚Then at the next noteã€‚ we do a subionã€‚ So y hat y is 1-2
    equals -1ã€‚And at the very endï¼Œ so we square our Sã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œyåœ¨å¼€å§‹æ—¶æ˜¯2ã€‚æ‰€ä»¥è¿™äº›æ˜¯æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬ã€‚æˆ‘ä»¬åˆå§‹åŒ–æˆ‘ä»¬çš„æƒé‡ã€‚å‡è®¾ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬è¯´æˆ‘ä»¬çš„wåœ¨å¼€å§‹æ—¶æ˜¯1ã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œå‰å‘ä¼ æ’­ã€‚åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬å°†xå’ŒWç›¸ä¹˜ã€‚æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°Y
    hatç­‰äº1ã€‚ç„¶ååœ¨ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬è¿›è¡Œå‡æ³•ã€‚æ‰€ä»¥y hat yæ˜¯1-2ç­‰äº-1ã€‚åœ¨æœ€åï¼Œæˆ‘ä»¬å¯¹æˆ‘ä»¬çš„Sè¿›è¡Œå¹³æ–¹ã€‚
- en: So we have have S squaredï¼Œ soã€‚Our lossï¼Œ thenï¼Œ is oneã€‚And now we calculate the
    local gradientã€‚ So at the last noteï¼Œ we have the gradient of the loss with respect
    to Sã€‚ And this is simple because we know the functionã€‚ So this is the gradient
    of S squaredã€‚ So this is just 2 Sã€‚And then at the next noteï¼Œ we have the gradient
    of S with respect to Y hatã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ S çš„å¹³æ–¹ï¼Œå› æ­¤æˆ‘ä»¬çš„æŸå¤±æ˜¯ 1ã€‚ç°åœ¨æˆ‘ä»¬è®¡ç®—å±€éƒ¨æ¢¯åº¦ã€‚åœ¨æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬æœ‰æŸå¤±å¯¹ S çš„æ¢¯åº¦ã€‚è¿™å¾ˆç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å‡½æ•°ã€‚è¿™æ˜¯ S çš„å¹³æ–¹çš„æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯
    2 Sã€‚åœ¨ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬æœ‰ S å¯¹ Y hat çš„æ¢¯åº¦ã€‚
- en: which is the gradient of the function Y hat minus y with respect to Y hatï¼Œ which
    is just oneã€‚And then here at the last nodeï¼Œ we have the derivative of Y hat with
    respect to Wã€‚ So this is the derivative ofã€‚W times x with respect to wï¼Œ which
    is xã€‚And alsoã€‚ notice that we don't need to goã€‚ Don't need to know the derivatives
    in thisã€‚Graph linesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å‡½æ•° Y hat å‡å» y å¯¹ Y hat çš„æ¢¯åº¦ï¼Œå€¼ä¸º 1ã€‚åœ¨æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬æœ‰ Y hat å¯¹ W çš„å¯¼æ•°ã€‚è¿™æ˜¯ W ä¹˜ä»¥ x å¯¹ w çš„å¯¼æ•°ï¼Œç»“æœæ˜¯
    xã€‚åŒæ—¶ï¼Œæ³¨æ„æˆ‘ä»¬ä¸éœ€è¦äº†è§£è¿™ä¸ªå›¾çº¿ä¸­çš„å¯¼æ•°ã€‚
- en: so we don'tã€‚Need to know what is the derivative of S with respect to Yã€‚ And
    also hereã€‚ we don't need this because our x and our y are fixedã€‚ So we are only
    interested in our parameters that we want to update hereã€‚And yeahã€‚ and then we
    do the backward passã€‚ So firstï¼Œ now we use our local gradientsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ä¸éœ€è¦çŸ¥é“ S å¯¹ Y çš„å¯¼æ•°ã€‚è€Œä¸”åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿä¸éœ€è¦è¿™ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬çš„ x å’Œ y æ˜¯å›ºå®šçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªå…³æ³¨æƒ³è¦åœ¨è¿™é‡Œæ›´æ–°çš„å‚æ•°ã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œåå‘ä¼ æ’­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„å±€éƒ¨æ¢¯åº¦ã€‚
- en: So we want to compute the derivative of the loss with respect to Y hatã€‚ And
    here we use the chain rule with our two local gradients that we just computedã€‚
    which is 2 S times 1ã€‚ and S is -1ï¼Œ which we calculated up hereã€‚ And then so this
    is  -2ã€‚And now we use this derivative and also this local gradient to then get
    the final gradientã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦è®¡ç®—æŸå¤±å¯¹ Y hat çš„å¯¼æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œç»“åˆæˆ‘ä»¬åˆšåˆšè®¡ç®—çš„ä¸¤ä¸ªå±€éƒ¨æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯ 2 S ä¹˜ä»¥ 1ï¼Œè€Œ S æ˜¯ -1ï¼Œè¿™ä¸ªæˆ‘ä»¬åœ¨ä¸Šé¢è®¡ç®—è¿‡ã€‚ç»“æœæ˜¯
    -2ã€‚ç°åœ¨æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå¯¼æ•°å’Œè¿™ä¸ªå±€éƒ¨æ¢¯åº¦æ¥å¾—åˆ°æœ€ç»ˆçš„æ¢¯åº¦ã€‚
- en: the gradient of the loss with respect to our Wï¼Œ which is the gradient of the
    loss with respect to Y hat times the gradient of Y hat with respect to Wã€‚ which
    is -2 times x and x is 1ã€‚ So the final gradient is -2ã€‚So this is the final gradient
    then that we not want to knowã€‚ And yeahã€‚ that's all how back propagation worksã€‚
    And let's jump over to our code and verify that Pyto get these exact numbersã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å¯¹æˆ‘ä»¬çš„ W çš„æ¢¯åº¦ï¼Œå°±æ˜¯æŸå¤±å¯¹ Y hat çš„æ¢¯åº¦ä¹˜ä»¥ Y hat å¯¹ W çš„æ¢¯åº¦ã€‚ç»“æœæ˜¯ -2 ä¹˜ä»¥ xï¼Œè€Œ x æ˜¯ 1ã€‚å› æ­¤ï¼Œæœ€ç»ˆçš„æ¢¯åº¦æ˜¯ -2ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æƒ³çŸ¥é“çš„æœ€ç»ˆæ¢¯åº¦ã€‚è¿™å°±æ˜¯åå‘ä¼ æ’­çš„å…¨éƒ¨å·¥ä½œåŸç†ã€‚è®©æˆ‘ä»¬è·³åˆ°ä»£ç éƒ¨åˆ†ï¼ŒéªŒè¯
    PyTorch æ˜¯å¦èƒ½å¾—åˆ°è¿™äº›ç¡®åˆ‡çš„æ•°å­—ã€‚
- en: So let's remember x is 1 Y is 2 and w is 1ã€‚ And then our first gradient should
    be -2ã€‚![](img/84bf83ae2a8c01617b4cf06b2bc402cb_1.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®°ä½ x æ˜¯ 1ï¼Œy æ˜¯ 2ï¼Œè€Œ w æ˜¯ 1ã€‚é‚£ä¹ˆæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ¢¯åº¦åº”è¯¥æ˜¯ -2ã€‚![](img/84bf83ae2a8c01617b4cf06b2bc402cb_1.png)
- en: Soã€‚![](img/84bf83ae2a8c01617b4cf06b2bc402cb_3.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/84bf83ae2a8c01617b4cf06b2bc402cb_3.png)'
- en: Let's see how we can use this in pie torchã€‚ Andï¼Œ first of allï¼Œ we import torchï¼Œ
    of courseã€‚Then we create our vector or tenorã€‚ So we say x equals torch dot tenorã€‚
    and this is1ã€‚ and then our y equals torch dot tenorã€‚With twoã€‚And then our initial
    weight is a tenzorã€‚Alsoã€‚ with oneã€‚So 1ã€‚0 to make it a floatã€‚ And hereï¼Œ and with
    our weightã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ PyTorch ä¸­ä½¿ç”¨è¿™ä¸ªã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å½“ç„¶è¦å¯¼å…¥ torchã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„å‘é‡æˆ–å¼ é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯´ x ç­‰äº torch.dot.tenorã€‚è¿™æ˜¯
    1ã€‚ç„¶åæˆ‘ä»¬çš„ y ç­‰äº torch.dot.tenorï¼Œå€¼ä¸º 2ã€‚æ¥ç€æˆ‘ä»¬çš„åˆå§‹æƒé‡æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œå€¼ä¸º 1ã€‚å› æ­¤ï¼Œ1.0 ä½¿å…¶æˆä¸ºæµ®ç‚¹æ•°ã€‚ç„¶åç”¨æˆ‘ä»¬çš„æƒé‡ã€‚
- en: we are interested in the gradientã€‚ So we need to specify require Sc equals trueã€‚And
    then we do the forward passã€‚And gets and compute the lossã€‚So we simply sayï¼Œ why
    hat equalsã€‚W times xï¼Œ which is our functionã€‚ And then we say loss equals yã€‚Hatsã€‚Minusã€‚The
    actual yã€‚ And then we square thisï¼Œ So we sayã€‚This to the power of twoã€‚And nowï¼Œ
    let's print our lossã€‚And seeã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹æ¢¯åº¦æ„Ÿå…´è¶£ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æŒ‡å®š `require Sc` ç­‰äº trueã€‚ç„¶åæˆ‘ä»¬è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤±ã€‚å› æ­¤æˆ‘ä»¬ç®€å•åœ°è¯´ï¼ŒY hat ç­‰äº W ä¹˜ä»¥
    xï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬è¯´æŸå¤±ç­‰äº Y hats å‡å»å®é™…çš„ yã€‚ç„¶åæˆ‘ä»¬å°†å…¶å¹³æ–¹ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ï¼Œè¿™ä¸ªå¹³æ–¹ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ‰“å°æˆ‘ä»¬çš„æŸå¤±ï¼Œçœ‹çœ‹ç»“æœã€‚
- en: This is one in the beginningã€‚ And now we want to do the backward passã€‚ So let's
    do the backward passã€‚ and pi touch will compute the local gradients automatically
    for us and also computes the backward pass automatically for usã€‚ So the only thing
    that we have to call is say loss backwardã€‚ So this is the whole gradient computationã€‚
    And now our w has this dot Gr attributeã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¼€å§‹çš„å†…å®¹ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦è¿›è¡Œåå‘ä¼ æ’­ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬è¿›è¡Œåå‘ä¼ æ’­ã€‚`pi touch` å°†è‡ªåŠ¨ä¸ºæˆ‘ä»¬è®¡ç®—å±€éƒ¨æ¢¯åº¦ï¼ŒåŒæ—¶ä¹Ÿè‡ªåŠ¨è®¡ç®—åå‘ä¼ æ’­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å”¯ä¸€éœ€è¦è°ƒç”¨çš„å°±æ˜¯
    `loss backward`ã€‚è¿™å°±æ˜¯æ•´ä¸ªæ¢¯åº¦è®¡ç®—ã€‚ç°åœ¨æˆ‘ä»¬çš„ w æœ‰äº†è¿™ä¸ª dot Gr å±æ€§ã€‚
- en: And we can print thisã€‚ And now this is the first gradientã€‚ in the after the
    first forward and backward passã€‚ And rememberï¼Œ this should be-2 in the beginningã€‚
    And here we see we have a tensor with -2ã€‚ So this is workingã€‚And the next steps
    would beã€‚ for exampleï¼Œ Now we update our weightsï¼Œ and then we do the next forward
    and backward pass and do this for a couple of iterationsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ‰“å°è¿™ä¸ªã€‚ç°åœ¨è¿™æ˜¯ç¬¬ä¸€ä¸ªæ¢¯åº¦ã€‚åœ¨ç¬¬ä¸€æ¬¡å‰å‘å’Œåå‘ä¼ æ’­ä¹‹åã€‚è®°ä½ï¼Œå¼€å§‹æ—¶åº”è¯¥æ˜¯-2ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°æœ‰ä¸€ä¸ªå¼ é‡ä¸º-2ã€‚æ‰€ä»¥è¿™ä¸ªæ˜¯æœ‰æ•ˆçš„ã€‚æ¥ä¸‹æ¥çš„æ­¥éª¤æ˜¯ã€‚ä¾‹å¦‚ï¼Œç°åœ¨æˆ‘ä»¬æ›´æ–°æˆ‘ä»¬çš„æƒé‡ï¼Œç„¶åè¿›è¡Œä¸‹ä¸€æ¬¡å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå¹¶è¿›è¡Œå‡ æ¬¡è¿­ä»£ã€‚
- en: And yeahï¼Œ that's how Beck propagation works and howï¼Œ and also how easy it is
    to use it in Pytorchã€‚ And I hope you enjoyed this tutorialã€‚ Please subscribe to
    the channel and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚![](img/84bf83ae2a8c01617b4cf06b2bc402cb_5.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™å°±æ˜¯åå‘ä¼ æ’­çš„å·¥ä½œåŸç†ï¼Œä»¥åŠåœ¨Pytorchä¸­ä½¿ç”¨å®ƒæ˜¯å¤šä¹ˆç®€å•ã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚è¯·è®¢é˜…é¢‘é“ï¼Œä¸‹æ¬¡è§ï¼Œæ‹œï¼ğŸ˜Šã€‚![](img/84bf83ae2a8c01617b4cf06b2bc402cb_5.png)
