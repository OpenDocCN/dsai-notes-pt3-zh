- en: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P9：L9- 自定义图层 - ShowMeAI
    - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P9：L9- 自定义图层 - ShowMeAI
    - BV1em4y1U7ib
- en: What is going on， guys， hope you're doing freaking awesome。 So in this video。
    I want to show you how to create custom layers。😊。![](img/2b96deb5c7669b397136afafb6aca66a_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，希望你们过得非常精彩。在这个视频中，我想向你展示如何创建自定义层。😊！[](img/2b96deb5c7669b397136afafb6aca66a_1.png)
- en: '![](img/2b96deb5c7669b397136afafb6aca66a_2.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b96deb5c7669b397136afafb6aca66a_2.png)'
- en: So far we've seen how to build very flexible models using subclassing and now
    we want to go one level deeper and even create the layers by ourselves。 So I'll
    show you what I mean by that but first just to explain the code we have in front
    of us right now we just have the import that we've seen pretty much every video
    and then we have these two lines to avoid any GPU errors and then lastly we're
    just loading the Ms data so this is just to save some time and so what we're going
    to start with is creating our own custom model sort of like we did in the last
    video so it's going to be very simple we're going to do class my model we're going
    to inherit from Kast model we're going to start with finding the init method we're
    going do self and then we could specify the number of classes for Ms 10 and then
    we're going just going call the super super mymod self do in it。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何使用子类化构建非常灵活的模型，现在我们想更进一步，自己创建层。因此，我会向你解释我所指的内容，但首先为了说明我们面前的代码，我们只是引入了在几乎每个视频中看到的内容，然后我们有这两行代码以避免任何GPU错误，最后我们只是在加载Ms数据，这样可以节省一些时间，所以我们将从创建自己的自定义模型开始，像我们在上一个视频中做的一样，这会非常简单，我们将创建类my
    model，继承自Kast模型，首先找到init方法，我们会使用self，然后可以指定Ms的类数为10，接着我们将调用super mymod self的init。
- en: And then we're going to do self dense1， So all we're going to do here is just
    create two dense layers Allright。 we're going to do layers do dense， let's do
    64 nodes and then self dense2 is layers dense of nu classes we're just going to
    do the call So we're going to call self and then x。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行self dense1，这里我们只需创建两个密集层。好的。我们将使用layers do dense，设定为64个节点，然后self dense2是layers
    dense的nu classes，我们将调用self，然后是x。
- en: So the input or let's call input tensor。And then we're going to do self do den
    one of input tensor。And then let's run a Tf。nn。relu on top of that。So this is
    just what we've seen in the last video。 this is why I'm going through it pretty
    quickly， and then we want to return self do then2 of x。 al right。So now let's
    quickly build a。And just a model compile a model fit on that。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 输入或称为输入张量。然后我们将执行self do den one的输入张量。接着在其上运行Tf.nn.relu。这就是我们在上一个视频中看到的内容，因此我快速讲解，然后我们想返回self
    do then2的x。好的。现在让我们快速构建一个模型，编译模型并进行拟合。
- en: So we'll do model equals my model。 We'll do model do compile。Loss equals ks，
    losses， bars。 categorical cross entropy。From logic equals true。😔，And then let's
    do our model that fit。 so you've seen all of this before， so I'm just going to
    write it out pretty quickly。Alright so now we have a custom model using subclassing
    and then we're just defining our compile and our fit and our evaluate so let's
    just make sure that this actually works so if we run this we see that it's actually
    training and this should go relatively quickly because we just have yeah so we
    have a very very small network like 64 nodes and then 10 output nodes let's actually
    talk about what I want to show you in this video so。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行model equals my model。我们将执行model do compile。损失等于ks，损失，bars。分类交叉熵。从逻辑上讲等于true。😔，然后让我们进行模型拟合。你之前见过这些，所以我会快速写出。好的，现在我们有一个使用子类化的自定义模型，然后我们定义了编译、拟合和评估，所以让我们确保这确实有效，如果我们运行这个，我们会看到它实际上在训练，这应该相对较快，因为我们只有是的，我们有一个非常非常小的网络，像64个节点，然后10个输出节点，让我们实际讨论一下我想在这个视频中展示的内容。
- en: We now want to actually create these layers by ourselves right now we're using
    the layers from Ks and that has the dense layers in it and then we're using Tf。n。relu
    and that's all right and you can build very flexible models using that that's
    actually in most use cases that's fine but sometimes and just for understanding
    you want to actually be able to build those layers by yourself so that you really
    understand what's going on a more under the hood so I'm going to show you how
    to do that and let's do class and let's do dense and we're going to inherit from
    layers layer。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想自己创建这些层。目前我们使用的是Ks中的层，里面包含密集层，然后使用Tf.n.relu。这是可以的，你可以用它构建非常灵活的模型，在大多数情况下这都很好。但有时为了理解，你想自己构建这些层，这样可以更深入地理解内部机制。所以我将向你展示如何做到这一点。让我们做一个类，命名为dense，并继承自layers
    layer。
- en: We're going to create our in function， so we're going to do init and then self
    unit。 and then we're going to specify the input dimension。So all we got to do
    then is we got to run this super method， so super dense self thatt in it。Then
    we're going to do self that W is self add weight。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建我们的初始化函数，所以我们将做init，然后是self和unit。接着我们将指定输入维度。然后我们要做的就是运行super方法，所以super
    dense self that init。然后我们将self.W设置为self.add weight。
- en: So there are actually multiple ways of doing this。 this is the more easy way
    you could also do initialize it by yourself with a T dot variable but this is
    the easy way I'm going to show you in this way Now the first thing we got to do
    is we've got to set it to a name let's just call it W and actually this is quite
    important you'll see in the next video how we can save and load models and I found
    out if you don't actually specify a name you can't save the model so this is very
    important and then we're just going to do a shape we're going to specify a shape
    as input dimension and then to unit All right so input dimension is just what
    we have in the beginning it's going to be 784 which is 28 times 28 and then unit
    is just what we're gonna map it to so when we're using layers164 the unit is 64
    then we can also。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有多种方法来做到这一点。这是更简单的方法，你也可以用T点变量自己初始化，但我将以这种简单的方法展示给你。首先，我们需要设置一个名称，我们就叫它W。实际上，这一点非常重要，你会在下一个视频中看到如何保存和加载模型。我发现如果不指定名称，就无法保存模型，所以这非常重要。接下来，我们要指定一个输入维度的形状，然后设置单位。输入维度就是我们一开始的值，784，即28乘以28，然后单位就是我们要映射到的值，所以在使用layers164时，单位是64。
- en: the initializer so we're going to do random normal and you could check out what
    other initialization methods you can do。 and then we're going to specify trainable
    equals true。And so trainable equals true。 this is for layers like batch norm and
    so on where some of the parameters are not actually trainable。 but so all of our
    parameters in this dense layer are going to be trainable then we're going to do
    self。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化器我们将使用随机正态分布，你可以查看其他初始化方法。然后我们将指定trainable等于true。所以trainable等于true。这适用于像batch
    norm这样的层，其中某些参数实际上不是可训练的。但在这个密集层中，我们的所有参数都将是可训练的，然后我们将进行self。
- en: B is self。 add weight。We're going to call it B and then shape is just going
    to be unit。RightSo't have so we're doing the matrix multiply with W and that's
    why it has to have the input dimension。 but then it's just going to be unit nodes
    so we're going to add one for each of them。 that's why we just have units right
    here。Then we can do initializer and we're just going to initialize it as zeros。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: B是self.add weight。我们将其命名为B，形状就是单位。因此，我们进行矩阵乘法时需要W，这就是它必须具有输入维度的原因。然后它将是单位节点，所以我们为每个节点添加一个。这就是我们在这里所用的单位。然后我们可以进行初始化，我们将其初始化为零。
- en: and then this is also a trainable parameter。And lastly， we just have to do the
    call method。 So call of some input。 we're going to return T of dot matrix multiply
    with the input。And then self to W， and lastly， we just got to add B。So now。We
    can actually replace this right here so we can do let's do selfta dense 1 is dense
    and then let's do 64 and then 784。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个可训练参数。最后，我们只需实现call方法。对于一些输入，我们将返回T点矩阵乘以输入。然后self到W，最后我们需要添加B。现在，我们可以在这里替换，所以我们可以做self
    ta dense 1是dense，然后设置64和784。
- en: and then let's do selfta dense 2 is dense of 10 and then 64 as input。So let's
    out that one right there and let's see if this works。 Allright。 so we seem to
    get pretty much equal results as we did on the last one and most importantly it
    actually runs So one thing you can notice here first of all is that on these ones
    we didn't have to specify the input dimension and this is what we're going call
    it making the layers lazy in that you don't have to actually even say what the
    input dimension is it's just going to work that out So that's what we want to
    do now we want to actually remove this part and make it work regardless of the
    input dimension So how we can do that is that we're going to create a build method
    all right so we have our in it right now and we're going to remove the input dimension
    right here。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们来做一个自密集层，输入维度是10，然后是64。现在把这个输出放在那里，看看是否有效。好的，我们似乎得到了与上一个几乎相等的结果，最重要的是它确实可以运行。首先，你可以注意到，在这些例子中，我们不需要指定输入维度，我们称之为懒惰层，这样你根本不需要说明输入维度，它会自动计算出来。所以我们现在想做的是移除这一部分，使其无论输入维度如何都能工作。我们要做到这一点的方法是创建一个构建方法。现在我们有了初始化，我们将在这里移除输入维度。
- en: And then we're going to do a build method。So we're going to do define build。
    we're going to have self， and then we're going to have an input shape。And then
    we're actually going to create the Ws right here。 so we're going to paste that
    in the build method instead。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创建一个构建方法。我们将定义构建，传入self和输入形状。接着，我们将在这里创建Ws，因此我们将把这段代码粘贴到构建方法中。
- en: and now what's so great is that instead of using input input dim we can do input
    shape and then we're going to do sort of the last of those so in this case we
    have the training examples on the first dimension and then we have 784 because
    of the way we've reshaped it right here。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在非常棒的是，使用输入形状而不是输入维度，我们可以处理最后一个维度。在这种情况下，我们在第一维有训练样本，然后由于我们在这里的重塑，我们有784。
- en: so that's why we do minus1 here and then we're going to do units although what
    we're going to do in the in methods is do selft units equals units。And then we
    got to do replace these units right here with self dot units for both of the for
    the W and then self dot B。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这里做-1，然后我们将使用单位，虽然在初始化方法中我们将self.units设置为units。接着，我们需要将这里的单位替换为self.units，W和B都需要如此。
- en: So what's amazing now is that if we would run this。 we wouldn't have to specify
    the input dimension this hopefully we work so we can actually we can do that classes
    So let's run this now and let's see what we get。And it seems to work and now we
    see that this like these two are pretty similar right the functionality of them
    are pretty much identical and then you might be saying well we're still using
    TfN and dot Reello it would be nice to actually create this ourselves as well
    and so that's our next step you can do this in two ways you can create a function
    or you can create a class like we're doing so far I think the most common way
    is actually defining a function but let's just create a class and you can try
    making a function。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在令人惊讶的是，如果我们运行这个，我们就不需要指定输入维度，这样希望可以工作。我们可以运行这些类。现在让我们试试，看看会得到什么。它似乎有效，我们看到这两个功能几乎是相同的。你可能会说，我们仍在使用TfN和dot
    Reello，实际上自己创建这个也不错，这就是我们的下一步。你可以通过两种方式做到这一点，可以创建一个函数或像我们现在这样创建一个类。我认为最常见的方法是定义一个函数，但让我们创建一个类，你可以尝试制作一个函数。
- en: It's going to be pretty much the same。But we're going to do class Mireello。
    and then we're going to do layers taught layer。And we're going to do define in
    it。Of just self。 we've gotta call super of my reello。Self， and then that in it。And
    then for our actual call。We're just going to return Tf。 mathath。 maximum。Of x
    and0。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它几乎是一样的。但我们将做一个类Mireello，然后定义初始化，只需传入self。我们需要调用super的Mireello，self和初始化。然后在我们的实际调用中，我们只需返回Tf.math.maximum(x,
    0)。
- en: this is just going to return the maximum of x or 0， which is exactly the the
    relative， right。 So at this point， you might be saying， well， how would we actually
    create this Tf math maximum function。You might be feeling that this is a way of
    cheating and so what I would say is that this would be even more low level and
    this is something you can try out and you could read the documentation and the
    source code for how they've actually implemented this function and so on but there
    will always be times where you can go even deeper and explore the details and
    so this is where I would draw the line and that we can use these mathematical
    operations on these tensors so when we have this myrelu we can do self。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是返回 x 和 0 中的最大值，这正是相对的，对吧？所以在这一点上，你可能会说，嗯，我们到底如何创建这个 Tf math maximum 函数。你可能觉得这是一种作弊的方式，我想说这甚至更底层，这是你可以尝试的，你可以阅读文档和源代码，了解他们是如何实现这个函数的等等，但总会有机会让你深入探索细节。因此，我会在这里划定界限，我们可以在这些张量上使用这些数学运算，当我们有这个
    myrelu 时，我们可以做 self。
- en: tre is my Relu so we got to instantiate the class although if you use a function
    this wouldn't be the case what we got to do then is we got to replace this Tf
    and then do relu and then we're going to do selftrelu on top of that。So we can
    run this first of all。So now you've seen how to build these models by yourself
    with Kaa subclassing and then also how to actually build these layers like dense
    layers and re functions so these are pretty simple ones but you can imagine building
    more complex ones as well and I also suggest you to try that out Alright so that's
    it for this video thank you so much for watching and I hope to see you in the
    next one。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: tre 是我的 Relu，所以我们必须实例化这个类，虽然如果你使用一个函数，这就不是这种情况。那么我们要做的是替换这个 Tf，然后做 relu，然后在此基础上做
    selftrelu。所以我们可以先运行这个。因此，你现在已经看到如何通过 Kaa 子类化自己构建这些模型，以及如何实际构建这些层，比如稠密层和 re 函数。这些都是相当简单的，但你可以想象构建更复杂的层。我也建议你尝试一下。好的，这就是本视频的全部内容，非常感谢你的观看，希望在下一个视频见到你。
- en: '![](img/2b96deb5c7669b397136afafb6aca66a_4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b96deb5c7669b397136afafb6aca66a_4.png)'
