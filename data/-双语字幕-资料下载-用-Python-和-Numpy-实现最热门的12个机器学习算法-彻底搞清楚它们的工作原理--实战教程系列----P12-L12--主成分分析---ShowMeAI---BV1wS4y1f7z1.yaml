- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12-
    ä¸»æˆåˆ†åˆ†æ - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P12ï¼šL12-
    ä¸»æˆåˆ†åˆ†æ - ShowMeAI - BV1wS4y1f7z1
- en: Hi everybodyã€‚ Welcome to our new machine learning from Sct tutorialã€‚ Todayã€‚
    we are going to implement the principal component analysis or PCA using only Python
    and Numpyã€‚The PCA is a nice tool to get linearly independent features and also
    to reduce the dimensionality of our data setã€‚So the goal is to find a new set
    of dimensions such that all the dimensions are orthogonal and hence linearly independent
    and ranked according to the variance of data along themã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œå¤§å®¶å¥½ã€‚æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„æ–°æœºå™¨å­¦ä¹ æ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨Pythonå’ŒNumpyå®ç°ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€‚PCAæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ï¼Œç”¨äºè·å–çº¿æ€§ç‹¬ç«‹çš„ç‰¹å¾ï¼Œå¹¶å‡å°‘æˆ‘ä»¬æ•°æ®é›†çš„ç»´åº¦ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æ–°çš„ç»´åº¦ï¼Œä½¿æ‰€æœ‰ç»´åº¦å½¼æ­¤æ­£äº¤ï¼Œä»è€Œçº¿æ€§ç‹¬ç«‹ï¼Œå¹¶æ ¹æ®æ•°æ®æ²¿ç€å®ƒä»¬çš„æ–¹å·®è¿›è¡Œæ’åºã€‚
- en: So this means we want to find a transformationï¼Œ such that the transform features
    are linearly independentã€‚And the dimensionality can then be reduced by taking
    only the dimensions with the highest importanceã€‚And those newly fine dimensions
    should minimize the projection errorã€‚ and the projected point should have a maximum
    spread or which means the maximum varianceã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬æƒ³è¦æ‰¾åˆ°ä¸€ä¸ªå˜æ¢ï¼Œä½¿å¾—å˜æ¢åçš„ç‰¹å¾çº¿æ€§ç‹¬ç«‹ã€‚ç„¶åå¯ä»¥é€šè¿‡ä»…ä¿ç•™æœ€é‡è¦çš„ç»´åº¦æ¥å‡å°‘ç»´åº¦ã€‚è¿™äº›æ–°è·å¾—çš„ç»´åº¦åº”è¯¥æœ€å°åŒ–æŠ•å½±è¯¯å·®ï¼ŒæŠ•å½±ç‚¹åº”è¯¥å…·æœ‰æœ€å¤§çš„æ‰©å±•ï¼Œæˆ–æ„å‘³ç€æœ€å¤§çš„æ–¹å·®ã€‚
- en: So let's look at an image to understand this betterã€‚ So let's say our 2D data
    is distributed like this and now we want to project it into 1 d and now what we
    want to do is we want to find the axis that are orthogonal to each otherã€‚And when
    we project our data onto these axesï¼Œ then our new projected data should have the
    maximum spreadã€‚So on the left sideï¼Œ these are the correct principal axisã€‚ So if
    we project them in one Dã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªå›¾åƒï¼Œä»¥æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ã€‚å‡è®¾æˆ‘ä»¬çš„äºŒç»´æ•°æ®åˆ†å¸ƒæ˜¯è¿™æ ·çš„ï¼Œç°åœ¨æˆ‘ä»¬æƒ³å°†å…¶æŠ•å½±åˆ°ä¸€ç»´ã€‚æˆ‘ä»¬æƒ³è¦æ‰¾åˆ°å½¼æ­¤æ­£äº¤çš„è½´ã€‚å½“æˆ‘ä»¬å°†æ•°æ®æŠ•å½±åˆ°è¿™äº›è½´ä¸Šæ—¶ï¼Œæˆ‘ä»¬çš„æ–°æŠ•å½±æ•°æ®åº”è¯¥å…·æœ‰æœ€å¤§çš„æ‰©å±•ã€‚æ‰€ä»¥åœ¨å·¦ä¾§ï¼Œè¿™äº›æ˜¯æ­£ç¡®çš„ä¸»è½´ã€‚å¦‚æœæˆ‘ä»¬å°†å®ƒä»¬æŠ•å½±åˆ°ä¸€ç»´ã€‚
- en: So on the largest principal componentã€‚If we project our data on this axisã€‚ then
    they have the maximum spreadã€‚ Andï¼Œ for exampleï¼Œ if we look on the right sideã€‚
    So these are incorrect axisã€‚ Soï¼Œ let's look at how the projected data would look
    like forã€‚ so on the right sideï¼Œ we made it even worse and projected it on theã€‚Y
    xsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€å¤§çš„ä¸»æˆåˆ†ä¸Šã€‚å¦‚æœæˆ‘ä»¬å°†æ•°æ®æŠ•å½±åˆ°è¿™ä¸ªè½´ä¸Šï¼Œé‚£ä¹ˆå®ƒä»¬å…·æœ‰æœ€å¤§çš„æ‰©å±•ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çœ‹å³ä¾§ã€‚è¿™äº›è½´æ˜¯é”™è¯¯çš„ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æŠ•å½±æ•°æ®çš„æ ·å­ã€‚æ‰€ä»¥åœ¨å³ä¾§ï¼Œæˆ‘ä»¬åšå¾—æ›´ç³Ÿï¼Œå°†å…¶æŠ•å½±åˆ°Yè½´ä¸Šã€‚
- en: So these are clearly wrong because here we can see that a lot of data is on
    the same spotã€‚ so we don't have any more information about themã€‚ But here on the
    left sideã€‚ the projected data has the maximum spreadã€‚So we can contain most of
    the information about the dataã€‚And alsoï¼Œ the projection errorï¼Œ which means this
    would beã€‚The lines from here to the axisã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›æ˜¾ç„¶æ˜¯é”™è¯¯çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾ˆå¤šæ•°æ®é›†ä¸­åœ¨åŒä¸€ä¸ªç‚¹ä¸Šã€‚æ‰€ä»¥æˆ‘ä»¬æ²¡æœ‰æ›´å¤šå…³äºå®ƒä»¬çš„ä¿¡æ¯ã€‚ä½†åœ¨å·¦ä¾§ï¼ŒæŠ•å½±æ•°æ®çš„æ‰©å±•æœ€å¤§ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åŒ…å«å¤§éƒ¨åˆ†å…³äºæ•°æ®çš„ä¿¡æ¯ã€‚åŒæ—¶ï¼ŒæŠ•å½±è¯¯å·®æ„å‘³ç€è¿™ä¼šæ˜¯ã€‚ä»è¿™é‡Œåˆ°è½´çš„çº¿ã€‚
- en: this is minimalï¼Œ whereas on the right sideã€‚ So here we have to make a longã€‚
    very long projection line for each pointã€‚ So the left side is the correct answerã€‚
    And now how do we find these principal componentsã€‚So for thisï¼Œ as I saidã€‚ we want
    to maximize the varianceï¼Œ so we need some mathã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€å°çš„ï¼Œè€Œå³ä¾§åˆ™ç›¸å¯¹è¾ƒé•¿ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªç‚¹åšä¸€æ¡å¾ˆé•¿å¾ˆé•¿çš„æŠ•å½±çº¿ã€‚æ‰€ä»¥å·¦ä¾§æ˜¯æ­£ç¡®çš„ç­”æ¡ˆã€‚ç°åœ¨æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°è¿™äº›ä¸»æˆåˆ†å‘¢ï¼Ÿæ‰€ä»¥ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–æ–¹å·®ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€äº›æ•°å­¦ã€‚
- en: so we need the variance of a sample X and this is calculated as one over the
    number of samplesã€‚ and then we have the sum over each component minus x bar and
    x bar is the mean valueã€‚ So we subtract the mean value from our data setã€‚And now
    we also need a covariance matrixã€‚ so this indicates the level to which two variables
    vary together and the covariance matrix of two variables is defined as this so1
    over n and then again the sum and here we subtract the mean and again here also
    the mean and then transposed and in our case we want to have the covariance matrix
    with both of our xã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ ·æœ¬Xçš„æ–¹å·®ï¼Œè¿™ä¸ªæ–¹å·®æ˜¯é€šè¿‡æ ·æœ¬æ•°é‡çš„å€’æ•°æ¥è®¡ç®—çš„ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªåˆ†é‡æ±‚å’Œï¼Œå‡å»x barï¼Œx baræ˜¯å‡å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬ä»æ•°æ®é›†ä¸­å‡å»å‡å€¼ã€‚ç°åœ¨æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªåæ–¹å·®çŸ©é˜µã€‚è¿™è¡¨ç¤ºä¸¤ä¸ªå˜é‡å…±åŒå˜åŒ–çš„ç¨‹åº¦ï¼Œè€Œä¸¤ä¸ªå˜é‡çš„åæ–¹å·®çŸ©é˜µå®šä¹‰ä¸ºï¼Œè¿™æ ·1/nï¼Œç„¶åå†æ¬¡æ±‚å’Œï¼Œè¿™é‡Œå‡å»å‡å€¼ï¼Œè¿™é‡Œä¹Ÿå‡å»å‡å€¼ï¼Œç„¶åè½¬ç½®ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦ä¸¤ä¸ªxçš„åæ–¹å·®çŸ©é˜µã€‚
- en: so this is also called the auto covariance matrixã€‚Soã€‚We have to calculate this
    and then our problem is reduced to an eigenvector or eigenvalue problemã€‚ so I
    will not go into detail about eigenvectors hereã€‚ but I will put some links in
    the description if you want to read moreã€‚å—¯ã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¹Ÿè¢«ç§°ä¸ºè‡ªåæ–¹å·®çŸ©é˜µã€‚å› æ­¤ã€‚æˆ‘ä»¬å¿…é¡»è®¡ç®—è¿™ä¸ªï¼Œç„¶åæˆ‘ä»¬çš„é—®é¢˜è¢«ç®€åŒ–ä¸ºç‰¹å¾å‘é‡æˆ–ç‰¹å¾å€¼é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè¯¦ç»†è®¨è®ºç‰¹å¾å‘é‡ï¼Œä½†å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œæˆ‘ä¼šåœ¨æè¿°ä¸­æ”¾ä¸€äº›é“¾æ¥ã€‚å—¯ã€‚
- en: But what we have to do is we have to find the eigenvectors and eigenvalues of
    this covariance matrix and the eigenvectors point then in the direction of the
    maximum variance and the corresponding eigenvalues indicate the importance of
    its corresponding eigenvectorã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬éœ€è¦åšçš„æ˜¯æ‰¾åˆ°è¿™ä¸ªåæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼Œç‰¹å¾å‘é‡æŒ‡å‘æœ€å¤§æ–¹å·®çš„æ–¹å‘ï¼Œç›¸åº”çš„ç‰¹å¾å€¼è¡¨ç¤ºå…¶å¯¹åº”ç‰¹å¾å‘é‡çš„é‡è¦æ€§ã€‚
- en: So now if we have a look at this image again on the left sideã€‚ So these twovectors
    that I've drawn hereï¼Œ they correspond to the eigenvectors of the covariance matrix
    of our data setã€‚So this is what we have to doã€‚ And here I've written the approachã€‚
    So first we sub the mean value from our x or from our data setã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç°åœ¨å¦‚æœæˆ‘ä»¬å†çœ‹ä¸€ä¸‹å·¦ä¾§çš„è¿™ä¸ªå›¾åƒã€‚è¿™ä¸¤ä¸ªæˆ‘åœ¨è¿™é‡Œç”»çš„å‘é‡ï¼Œå®ƒä»¬å¯¹åº”äºæˆ‘ä»¬æ•°æ®é›†çš„åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦åšçš„ã€‚è¿™é‡Œæˆ‘å†™ä¸‹äº†è¿™ä¸ªè¿‡ç¨‹ã€‚é¦–å…ˆæˆ‘ä»¬ä»æˆ‘ä»¬çš„
    x æˆ–æ•°æ®é›†ä¸­å‡å»å‡å€¼ã€‚
- en: Then we calculate the covariance matrixã€‚ Then we have to calculate the eigenvectors
    and eigenvaluesã€‚Then we sort the eigenvectors in decreasing orderï¼Œ according to
    their eigenvaluesã€‚ and then we can specify how many dimensions we want to keepã€‚
    and then we choose only the first K eigenvectorsã€‚That will then be the new K dimensionsã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®¡ç®—åæ–¹å·®çŸ©é˜µã€‚æ¥ç€æˆ‘ä»¬å¿…é¡»è®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚ç„¶åæˆ‘ä»¬æŒ‰ç‰¹å¾å€¼çš„é™åºå¯¹ç‰¹å¾å‘é‡è¿›è¡Œæ’åºã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æŒ‡å®šæƒ³è¦ä¿ç•™çš„ç»´åº¦æ•°é‡ã€‚æ¥ç€æˆ‘ä»¬åªé€‰æ‹©å‰
    K ä¸ªç‰¹å¾å‘é‡ã€‚è¿™å°†æ˜¯æ–°çš„ K ç»´åº¦ã€‚
- en: And then we transform the original dataã€‚Into these new dimensions by projecting
    themã€‚ justã€‚ this is simply a dot productã€‚Of our data with the newï¼Œ with the eigenvectorsã€‚And
    then we are doneã€‚ So this is all we have to doï¼Œ andã€‚Byeã€‚One thing that is very
    nice about thisã€‚Pncple component analysis and the eigenvectors is that they are
    all orthogonal of each otherã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬é€šè¿‡æŠ•å½±å°†åŸå§‹æ•°æ®è½¬æ¢åˆ°è¿™äº›æ–°ç»´åº¦ä¸­ã€‚è¿™åªæ˜¯ä¸€ä¸ªç‚¹ç§¯ã€‚æˆ‘ä»¬çš„æ•°æ®ä¸æ–°çš„ç‰¹å¾å‘é‡è¿›è¡Œç‚¹ç§¯ã€‚ç„¶åæˆ‘ä»¬å°±å®Œæˆäº†ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦åšçš„ä¸€åˆ‡ï¼Œå†è§ã€‚æœ‰ä¸€ä¸ªå¾ˆå¥½çš„åœ°æ–¹åœ¨äºä¸»æˆåˆ†åˆ†æå’Œç‰¹å¾å‘é‡ï¼Œå®ƒä»¬å½¼æ­¤æ­£äº¤ã€‚
- en: This means that our new data is then also linearly independentã€‚ So this is a
    nice little bonus of the PCAã€‚![](img/592934822c91be0e4fe905744b84c579_1.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬çš„æ–°æ•°æ®ä¹Ÿæ˜¯çº¿æ€§ç‹¬ç«‹çš„ã€‚å› æ­¤ï¼Œè¿™æ˜¯ PCA çš„ä¸€ä¸ªå°å¥½å¤„ã€‚![](img/592934822c91be0e4fe905744b84c579_1.png)
- en: And now we can startã€‚ So let's import Ny S and Pã€‚ And then we create a class
    PC Aã€‚ This will get an in it with selfã€‚ And then here we specify the number of
    componentsã€‚ We want to keepã€‚And then we store them hereï¼Œ so we say self dot n
    componentsã€‚Equals nã€‚Componentsã€‚Andã€‚We want to find out the eigenvectorsã€‚ So let's
    call them components hereã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¯¼å…¥ Ny S å’Œ Pã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç±» PCAã€‚è¿™å°†åˆå§‹åŒ– selfã€‚æ¥ç€æˆ‘ä»¬æŒ‡å®šè¦ä¿ç•™çš„ç»„ä»¶æ•°é‡ã€‚ç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œå­˜å‚¨å®ƒä»¬ï¼Œæ‰€ä»¥æˆ‘ä»¬è¯´
    self.dot n componentsã€‚ç­‰äº nã€‚Componentsã€‚æˆ‘ä»¬æƒ³è¦æ‰¾å‡ºç‰¹å¾å‘é‡ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬ç§°å®ƒä»¬ä¸ºç»„ä»¶ã€‚
- en: And this is none in the beginningã€‚ And we also want to store the mean laterã€‚
    So let's say self that mean equals noneã€‚ And then we define our fit methodï¼Œ as
    alwaysã€‚ So this will get the day that we want to transformã€‚And then we don't use
    the predict methodã€‚ So now we call this transformã€‚So this will transform our data
    once we fitted itã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€å¼€å§‹éƒ½æ˜¯ Noneã€‚æˆ‘ä»¬è¿˜æƒ³ç¨åå­˜å‚¨å‡å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ self çš„å‡å€¼ç­‰äº Noneã€‚ç„¶åæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·å®šä¹‰æˆ‘ä»¬çš„ fit æ–¹æ³•ã€‚è¿™å°†è·å–æˆ‘ä»¬æƒ³è¦è½¬æ¢çš„æ•°æ®ã€‚ç„¶åæˆ‘ä»¬ä¸ä½¿ç”¨
    predict æ–¹æ³•ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬è°ƒç”¨ transformã€‚è¿™å°†ä¼šåœ¨æˆ‘ä»¬æ‹Ÿåˆåè½¬æ¢æˆ‘ä»¬çš„æ•°æ®ã€‚
- en: And this is just the projectionã€‚ But now let's start with the fit methodï¼Œ soã€‚Let's
    say or againã€‚ let's write our approachã€‚ So we want to have the meanã€‚Then we calculate
    the covariance matrixã€‚Then we want to calculate the eigen vectorctors and valuesã€‚å—¯ã€‚Higenvalueã€‚Then
    we sort our een back toã€‚ so sortã€‚Eigenvesã€‚Andã€‚Thenï¼Œ we store onlyã€‚The first andã€‚Eying
    vectorsã€‚So this is what we have to doã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯æŠ•å½±ã€‚ä½†ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹ä½¿ç”¨ fit æ–¹æ³•ï¼Œæ‰€ä»¥ã€‚æˆ‘ä»¬å†è¯´ä¸€æ¬¡ã€‚è®©æˆ‘ä»¬å†™ä¸‹æˆ‘ä»¬çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æƒ³è¦æœ‰å‡å€¼ã€‚ç„¶åè®¡ç®—åæ–¹å·®çŸ©é˜µã€‚æ¥ç€æˆ‘ä»¬æƒ³è®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚å—¯ï¼Œç‰¹å¾å€¼ã€‚ç„¶åæˆ‘ä»¬å¯¹ç‰¹å¾å‘é‡è¿›è¡Œæ’åºã€‚æ‰€ä»¥æ’åºã€‚ç‰¹å¾å‘é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬åªå­˜å‚¨å‰é¢å’Œç‰¹å¾å‘é‡ã€‚è¿™å°±æ˜¯æˆ‘ä»¬éœ€è¦åšçš„ã€‚
- en: And here we have toã€‚Project our dataã€‚Soã€‚Yeahï¼Œ let's do thisã€‚ So let's sayã€‚å—¯ã€‚Here
    we can say self dot mean equalsã€‚ and then we just use the Ny mean function of
    our data along the first axisã€‚And then we sub the meanã€‚ So we say x equals x minus
    self dot meanã€‚Soã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬å¿…é¡»å¯¹æ•°æ®è¿›è¡ŒæŠ•å½±ã€‚æ‰€ä»¥ï¼Œå—¯ï¼Œæ¥å§ï¼Œè®©æˆ‘ä»¬è¿™æ ·åšã€‚æˆ‘ä»¬å¯ä»¥è¯´self dot meanç­‰äºï¼Œç„¶åæˆ‘ä»¬åªéœ€ä½¿ç”¨Numpyçš„å‡å€¼å‡½æ•°ï¼Œæ²¿ç€ç¬¬ä¸€ä¸ªè½´ã€‚ç„¶åæˆ‘ä»¬å‡å»å‡å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´xç­‰äºxå‡å»self
    dot meanã€‚æ‰€ä»¥ã€‚
- en: '![](img/592934822c91be0e4fe905744b84c579_3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/592934822c91be0e4fe905744b84c579_3.png)'
- en: We have to do thisï¼Œ because if we lookã€‚Againï¼Œ and in our formula with the covariance
    matrixã€‚ then always thisï¼Œ the mean is subtractedã€‚![](img/592934822c91be0e4fe905744b84c579_5.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»è¿™æ ·åšï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬å†æ¬¡æŸ¥çœ‹æˆ‘ä»¬çš„å…¬å¼ä¸åæ–¹å·®çŸ©é˜µï¼Œå‡å€¼æ€»æ˜¯è¢«å‡å»ã€‚![](img/592934822c91be0e4fe905744b84c579_5.png)
- en: So let's just do this hereã€‚ and then we calculate the co variance matrixã€‚ and
    this is called cofã€‚ And then we simply use the nuy dotã€‚Cough functionã€‚ So this
    will do exactly this if we only put in one input hereã€‚But now we have to be carefulã€‚
    because let'sã€‚å—¯ã€‚Let's look at our dataã€‚ So this would be a nuy and D array where
    one rowã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬åœ¨è¿™é‡Œè¿™æ ·åšã€‚ç„¶åæˆ‘ä»¬è®¡ç®—åæ–¹å·®çŸ©é˜µã€‚è¿™è¢«ç§°ä¸ºcofã€‚ç„¶åæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨Numpyçš„Cofå‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬åœ¨è¿™é‡Œåªæ”¾å…¥ä¸€ä¸ªè¾“å…¥ï¼Œå®ƒå°†å®Œå…¨è¿™æ ·åšã€‚ä½†ç°åœ¨æˆ‘ä»¬å¿…é¡»å°å¿ƒã€‚å› ä¸ºè®©æˆ‘ä»¬ï¼Œå—¯ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ•°æ®ã€‚è¿™å°†æ˜¯ä¸€ä¸ªNumpyå’ŒDæ•°ç»„ï¼Œå…¶ä¸­ä¸€è¡Œã€‚
- en: the row is one sampleã€‚And one columnã€‚Is one feature of Etoã€‚But if we have a
    look at the documentationï¼Œ then for this functionï¼Œ it's the other way roundã€‚ So
    one column is one observation or one sampleã€‚ So we have to transpose it hereã€‚
    So please double check it for yourselfã€‚And then we continueã€‚ So now we calculate
    the eigenã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ã€‚æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ã€‚ä½†å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ–‡æ¡£ï¼Œå¯¹äºè¿™ä¸ªå‡½æ•°ï¼Œæƒ…å†µæ˜¯ç›¸åçš„ã€‚æ‰€ä»¥ä¸€åˆ—æ˜¯ä¸€ä¸ªè§‚å¯Ÿå€¼æˆ–ä¸€ä¸ªæ ·æœ¬ã€‚å› æ­¤æˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œè½¬ç½®ã€‚è¯·è‡ªè¡Œä»”ç»†æ£€æŸ¥ã€‚ç„¶åæˆ‘ä»¬ç»§ç»­ã€‚ç°åœ¨æˆ‘ä»¬è®¡ç®—ç‰¹å¾å€¼ã€‚
- en: Vectctorsï¼Œ and Iï¼Œ I don't knowã€‚ it's the other way aroundã€‚ Sorryï¼Œ eigenvalue
    and eigen vectorctorsã€‚ And for thisï¼Œ we can also use a function in Nmpyã€‚ So lump
    Ny linearalã€‚Lynn A dotã€‚Iikeã€‚ğŸ˜Šã€‚And then we put in our Covari matrixã€‚And here againï¼Œ
    we have to be careful if weã€‚If we look at the documentationï¼Œ then it says that
    eigenves are returned as column vectorsã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡ï¼Œæˆ‘ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¿™æ˜¯åè¿‡æ¥çš„ã€‚æŠ±æ­‰ï¼Œç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨Numpyä¸­ä½¿ç”¨ä¸€ä¸ªå‡½æ•°ã€‚æ‰€ä»¥åˆå¹¶Numpyçº¿æ€§ã€‚ç‚¹ç§¯ã€‚åƒè¿™æ ·ã€‚ğŸ˜Šã€‚ç„¶åæˆ‘ä»¬æ”¾å…¥æˆ‘ä»¬çš„åæ–¹å·®çŸ©é˜µã€‚è¿™é‡Œæˆ‘ä»¬ä¹Ÿè¦å°å¿ƒã€‚å¦‚æœæŸ¥çœ‹æ–‡æ¡£ï¼Œå®ƒè¯´ç‰¹å¾å‘é‡ä»¥åˆ—å‘é‡å½¢å¼è¿”å›ã€‚
- en: So one columnã€‚With allã€‚1 column I here is one eigen vectorã€‚And now for to do
    easier calculationsã€‚ we want to do it the other way aroundã€‚ So we say eigenvectors
    equals eigenvectors dot transposedã€‚And thenï¼Œ we sort themã€‚So for thisï¼Œ we say
    in our sortded indicesã€‚ are Ny a sort of the eigenvalues nowã€‚And we want to have
    it in decreasing order so we can use slicing here all the way from start to the
    endã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ—ã€‚æ‰€æœ‰çš„ã€‚è¿™é‡Œçš„ä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚ä¸ºäº†ä¾¿äºè®¡ç®—ï¼Œæˆ‘ä»¬æƒ³åè¿‡æ¥åšã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ç‰¹å¾å‘é‡ç­‰äºç‰¹å¾å‘é‡è½¬ç½®ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œæ’åºã€‚æ‰€ä»¥ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¯´åœ¨æ’åºçš„ç´¢å¼•ä¸­ï¼Œæ˜¯ç‰¹å¾å€¼çš„æ’åºã€‚æˆ‘ä»¬æƒ³æŒ‰é™åºæ’åˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åˆ‡ç‰‡ï¼Œä»å¼€å§‹åˆ°ç»“æŸã€‚
- en: And we put in my a step-1ã€‚ So this is a nice little trick to reverse a listã€‚And
    now we have the indices of the soded eigenve eigenvalues in decreasing orderã€‚
    And now we say our soded eigenvalues equals the eigenvaluesã€‚With this order and
    the same for the eigenvectors equals eigenvectorsã€‚With this indicesã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…ˆè¿›è¡Œæ­¥éª¤1ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªåè½¬åˆ—è¡¨çš„å°æŠ€å·§ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æŒ‰é™åºæ’åˆ—çš„ç‰¹å¾å€¼çš„ç´¢å¼•ã€‚ç°åœ¨æˆ‘ä»¬è¯´æˆ‘ä»¬çš„ç‰¹å¾å€¼ç­‰äºç‰¹å¾å€¼ã€‚ä½¿ç”¨è¿™ç§é¡ºåºï¼Œç‰¹å¾å‘é‡ä¹Ÿç­‰äºç‰¹å¾å‘é‡ã€‚æ ¹æ®è¿™äº›ç´¢å¼•ã€‚
- en: And now we store the first N eigenvectorsã€‚ So we say self dot componentsã€‚å—¯ã€‚Little
    typo hereã€‚ Self do componentsã€‚Equalsï¼Œ and now we can simply say eigenvesã€‚From
    the startã€‚ So from 0 to self dot and componentsã€‚ So this is why we transposed
    it here so we can easily do this transformation and also this transformationã€‚So
    this is the fit methodã€‚ And now we just have to transform itã€‚ And now here weã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å­˜å‚¨å‰Nä¸ªç‰¹å¾å‘é‡ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´self dot componentsã€‚å—¯ã€‚è¿™æœ‰ä¸€ä¸ªå°é”™è¯¯ã€‚self do componentsã€‚ç­‰äºï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´ç‰¹å¾å‘é‡ã€‚ä»å¼€å§‹ã€‚ä¹Ÿå°±æ˜¯ä»0åˆ°self
    dot componentsã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨è¿™é‡Œè½¬ç½®å®ƒï¼Œä»¥ä¾¿å¯ä»¥è½»æ¾è¿›è¡Œè¿™ç§å˜æ¢ï¼Œè¿˜æœ‰è¿™ç§å˜æ¢ã€‚è¿™æ˜¯fitæ–¹æ³•ã€‚ç°åœ¨æˆ‘ä»¬åªéœ€è¿›è¡Œå˜æ¢ã€‚ç°åœ¨åœ¨è¿™é‡Œæˆ‘ä»¬ã€‚
- en: Must not forget that we should also subtract the mean hereã€‚ So we say xã€‚Equals
    x minus selfã€‚Don't mean this is why we started hereã€‚And now we can project it
    and then return itã€‚ So we say return nuy dotã€‚ and we project X onto ourã€‚å—¯ã€‚Componentsã€‚
    so we the dot product with the self dot componentsã€‚ But now againï¼Œ we have to
    be careful hereã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸èƒ½å¿˜è®°æˆ‘ä»¬ä¹Ÿåº”è¯¥åœ¨è¿™é‡Œå‡å»å‡å€¼ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´xã€‚ç­‰äºxå‡å»self dot meanï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œå¼€å§‹çš„åŸå› ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡ŒæŠ•å½±å¹¶è¿”å›ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´è¿”å›Numpy
    dotã€‚ç„¶åæˆ‘ä»¬å°†XæŠ•å½±åˆ°æˆ‘ä»¬çš„ã€‚å—¯ã€‚ç‰¹å¾ä¸Šã€‚æˆ‘ä»¬ä¸self dot componentsè¿›è¡Œç‚¹ç§¯ã€‚ä½†ç°åœ¨æˆ‘ä»¬å†æ¬¡éœ€è¦å°å¿ƒã€‚
- en: Because here we transposed them and now again we want a column vectorã€‚ So here
    againã€‚ we have to transpose itã€‚ So againï¼Œ please double check it for yourselfã€‚And
    now we are doneã€‚ So this is the whole implementationã€‚ And now here I've written
    a little test script that is using the famous Iis data setã€‚And then I will create
    a PCA instance and I willã€‚ so by the wayã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºåœ¨è¿™é‡Œæˆ‘ä»¬è¿›è¡Œäº†è½¬ç½®ï¼Œç°åœ¨æˆ‘ä»¬åˆæƒ³è¦ä¸€ä¸ªåˆ—å‘é‡ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬åˆéœ€è¦è½¬ç½®å®ƒã€‚è¯·è‡ªè¡Œä»”ç»†æ£€æŸ¥ã€‚ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ã€‚è¿™å°±æ˜¯æ•´ä¸ªå®ç°ã€‚ç°åœ¨æˆ‘å†™äº†ä¸€ä¸ªå°æµ‹è¯•è„šæœ¬ï¼Œä½¿ç”¨è‘—åçš„Iisæ•°æ®é›†ã€‚ç„¶åæˆ‘ä¼šåˆ›å»ºä¸€ä¸ªPCAå®ä¾‹ã€‚
- en: this will have a dimension of 150 by4 So we have 105 samples and four different
    features hereã€‚ and now we only want to keep two and dimensionsã€‚ So we put in two
    in our initializerã€‚ Then we fit the dataï¼Œ and then we transform itã€‚And then I
    print the shape of bothã€‚And then I will plot themã€‚ So now we have a 2D vector
    so we can plot it in 2Dã€‚So yeahã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•°æ®çš„ç»´åº¦æ˜¯150ä¹˜ä»¥4ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰105ä¸ªæ ·æœ¬å’Œå››ä¸ªä¸åŒçš„ç‰¹å¾ã€‚ç°åœ¨æˆ‘ä»¬åªæƒ³ä¿ç•™ä¸¤ä¸ªç»´åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨åˆå§‹åŒ–æ—¶è¾“å…¥2ã€‚ç„¶åæˆ‘ä»¬æ‹Ÿåˆæ•°æ®ï¼Œæ¥ç€è¿›è¡Œè½¬æ¢ã€‚ç„¶åæˆ‘æ‰“å°å‡ºå®ƒä»¬çš„å½¢çŠ¶ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šç»˜åˆ¶å®ƒä»¬ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ª2Då‘é‡ï¼Œå¯ä»¥åœ¨2Dä¸­ç»˜åˆ¶å®ƒã€‚
- en: let's run this and see if this is workingã€‚And oã€‚ä¸€é¡µã€‚So I didn't write Python
    3 hereã€‚ but it worked anywayã€‚ So let's test this againã€‚![](img/592934822c91be0e4fe905744b84c579_7.png)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æœ‰æ•ˆã€‚å“¦ï¼Œä¸€é¡µã€‚æ‰€ä»¥æˆ‘æ²¡æœ‰åœ¨è¿™é‡Œå†™Python 3ï¼Œä½†å®ƒä»ç„¶æœ‰æ•ˆã€‚è®©æˆ‘ä»¬å†æµ‹è¯•ä¸€ä¸‹ã€‚![](img/592934822c91be0e4fe905744b84c579_7.png)
- en: So yeahï¼Œ here we have our 4D feature vector transformed or projected into 2Dã€‚
    and we see that all areã€‚All the three different classes are plotted in a different
    colorã€‚ so we can see that we can still have an easy separation of our classesã€‚
    So yeahï¼Œ that's very niceã€‚ And yeahï¼Œ that's the PC Aï¼Œ I hope you understood everythingã€‚
    And if you liked itã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„4Dç‰¹å¾å‘é‡è½¬åŒ–æˆ–æŠ•å½±åˆ°2Dä¸­ã€‚æˆ‘ä»¬çœ‹åˆ°æ‰€æœ‰ä¸‰ä¸ªä¸åŒçš„ç±»åˆ«ç”¨ä¸åŒçš„é¢œè‰²ç»˜åˆ¶ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ç±»åˆ«ä»ç„¶å¾ˆå®¹æ˜“åˆ†å¼€ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œè¿™å¾ˆä¸é”™ã€‚è¿™å°±æ˜¯PCAï¼Œæˆ‘å¸Œæœ›ä½ èƒ½ç†è§£æ‰€æœ‰å†…å®¹ã€‚å¦‚æœä½ å–œæ¬¢çš„è¯ã€‚
- en: please subscribe to the channelã€‚ and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚![](img/592934822c91be0e4fe905744b84c579_9.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®¢é˜…é¢‘é“ï¼Œä¸‹æ¬¡è§ï¼Œæ‹œã€‚ğŸ˜Šã€‚![](img/592934822c91be0e4fe905744b84c579_9.png)
- en: '![](img/592934822c91be0e4fe905744b84c579_10.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/592934822c91be0e4fe905744b84c579_10.png)'
