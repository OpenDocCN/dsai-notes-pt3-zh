- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P22ï¼šL4.1- ä¸ºKerasæ·±åº¦å­¦ä¹ ç¼–ç ç‰¹å¾å‘é‡
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P22ï¼šL4.1- ä¸ºKerasæ·±åº¦å­¦ä¹ ç¼–ç ç‰¹å¾å‘é‡
    - ShowMeAI - BV15f4y1w7b8
- en: Hiï¼Œ this is Jeff Heatonï¼Œ welcomecome to applications of Deep neural Network
    with Washington Universityã€‚In this videoï¼Œ we're going to begin looking at tabular
    dataã€‚This is data that is not a computer image or audio or any really complicated
    data input that deep neural networks are particularly known forã€‚ rather this is
    data that looks like something coming from Microsoft Excel where you've got columns
    and rows and you're going to try to predict one of those columns based on the
    othersã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ï¼Œæ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨è¯¾ç¨‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹æŸ¥çœ‹è¡¨æ ¼æ•°æ®ã€‚è¿™æ˜¯æ•°æ®ï¼Œä¸æ˜¯è®¡ç®—æœºå›¾åƒæˆ–éŸ³é¢‘ï¼Œä¹Ÿä¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œç‰¹åˆ«æ“…é•¿çš„å¤æ‚æ•°æ®è¾“å…¥ï¼Œè€Œæ˜¯çœ‹èµ·æ¥åƒæ¥è‡ªMicrosoft
    Excelçš„æ•°æ®ï¼Œå…¶ä¸­æœ‰åˆ—å’Œè¡Œï¼Œä½ å°†å°è¯•æ ¹æ®å…¶ä»–åˆ—é¢„æµ‹å…¶ä¸­ä¸€åˆ—ã€‚
- en: To put data like that into a deep neural networkï¼Œ usually each row is one input
    or one set of inputs to the input neurons of that neural networkã€‚The set of all
    of those numbers coming into the neural networkï¼Œ one row is called a feature vectorã€‚All
    data that comes into the neural network needs to be a numeric formã€‚ In this videoã€‚
    we're going to see how to build that feature vector for the latest on my AI course
    and projectsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™æ ·çš„æ•°æ®è¾“å…¥æ·±åº¦ç¥ç»ç½‘ç»œæ—¶ï¼Œé€šå¸¸æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¾“å…¥æˆ–ä¸€ç»„è¾“å…¥ï¼Œè¾“å…¥åˆ°è¯¥ç¥ç»ç½‘ç»œçš„è¾“å…¥ç¥ç»å…ƒä¸­ã€‚æ‰€æœ‰è¿›å…¥ç¥ç»ç½‘ç»œçš„æ•°å­—é›†åˆï¼Œå…¶ä¸­ä¸€è¡Œè¢«ç§°ä¸ºç‰¹å¾å‘é‡ã€‚æ‰€æœ‰è¿›å…¥ç¥ç»ç½‘ç»œçš„æ•°æ®éƒ½éœ€è¦æ˜¯æ•°å­—å½¢å¼ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä¸ºæˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®æ„å»ºè¯¥ç‰¹å¾å‘é‡ã€‚
- en: click subscribe and the bell next to it to be notified of every new videoã€‚ For
    this exampleã€‚ we're going to see how to encode a feature vectorã€‚ We're going to
    use the simple data set that we've seen beforeã€‚ğŸ˜Šã€‚![](img/04abe3c48902ea74356b7dd00d915cc9_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»è®¢é˜…å¹¶æŒ‰æ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶æ”¶åˆ°é€šçŸ¥ã€‚å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ç¼–ç ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰è§è¿‡çš„ç®€å•æ•°æ®é›†ã€‚ğŸ˜Šã€‚![](img/04abe3c48902ea74356b7dd00d915cc9_1.png)
- en: I'll run thatï¼Œ and it shows you basicallyã€‚This the makeup of this of this data
    setã€‚ we've already talked about this one beforeï¼Œ but just quicklyã€‚ it has some
    categoricals like job and areaã€‚ So those need to go into dummy variables and the
    value that you're trying to predict is for each of these individualsã€‚ what product
    did they buyï¼Œ ABC or Dã€‚Think it goes up to Dã€‚Nowã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†è¿è¡Œè¿™ä¸ªç¨‹åºï¼Œå®ƒåŸºæœ¬ä¸Šå‘ä½ å±•ç¤ºäº†è¿™ä¸ªæ•°æ®é›†çš„æ„æˆã€‚æˆ‘ä»¬ä¹‹å‰å·²ç»è®¨è®ºè¿‡è¿™ä¸ªæ•°æ®é›†ï¼Œä½†æˆ‘ä¼šå¿«é€Ÿå›é¡¾ä¸€ä¸‹ã€‚å®ƒæœ‰ä¸€äº›åˆ†ç±»å˜é‡ï¼Œæ¯”å¦‚èŒä¸šå’Œåœ°åŒºã€‚å› æ­¤ï¼Œè¿™äº›éœ€è¦è½¬åŒ–ä¸ºè™šæ‹Ÿå˜é‡ï¼Œè€Œä½ è¯•å›¾é¢„æµ‹çš„å€¼æ˜¯å¯¹äºæ¯ä¸ªä¸ªä½“ï¼Œä»–ä»¬è´­ä¹°äº†å“ªä¸ªäº§å“ï¼ŒAã€Bã€Cæˆ–Dã€‚è®°å¾—äº§å“çš„èŒƒå›´æ˜¯åˆ°Dçš„ã€‚ç°åœ¨ã€‚
- en: with this data set to make it for a neural networkï¼Œ there's some things you
    need to considerã€‚ So the column that you seekï¼Œ at least if you're doing classificationï¼Œ
    is the productã€‚ there's an ID column which we have hereã€‚ That's not useful to
    youã€‚ so you're going to get rid of that oneã€‚And then many of these fields are
    numeric and may not need further processingã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†ä¸ºç¥ç»ç½‘ç»œå‡†å¤‡æ•°æ®æ—¶ï¼Œæœ‰ä¸€äº›éœ€è¦è€ƒè™‘çš„äº‹é¡¹ã€‚å› æ­¤ï¼Œå¦‚æœä½ è¿›è¡Œåˆ†ç±»ï¼Œæ‰€éœ€çš„åˆ—æ˜¯äº§å“ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªIDåˆ—ï¼Œè¿™å¯¹ä½ æ²¡æœ‰ç”¨ï¼Œæ‰€ä»¥ä½ å°†å»æ‰è¿™ä¸€åˆ—ã€‚è®¸å¤šå­—æ®µæ˜¯æ•°å­—çš„ï¼Œå¯èƒ½ä¸éœ€è¦è¿›ä¸€æ­¥å¤„ç†ã€‚
- en: But we'll see that's not entirely trueï¼Œ a neural networkã€‚2 things you can do
    to numeric values that will really help the neural networkã€‚ One is put them in
    consistent rangesã€‚ The fact that income that is in a much bigger rangeã€‚ tens of
    thousandsã€‚Versus ageï¼Œ which is under 100 that hurts the predictive power of the
    neural networkã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¼šçœ‹åˆ°è¿™å¹¶ä¸å®Œå…¨æ­£ç¡®ï¼Œç¥ç»ç½‘ç»œã€‚æœ‰ä¸¤ä»¶äº‹å¯ä»¥å¯¹æ•°å­—å€¼è¿›è¡Œå¤„ç†ï¼Œè¿™å°†éå¸¸æœ‰åŠ©äºç¥ç»ç½‘ç»œã€‚ç¬¬ä¸€æ˜¯å°†å®ƒä»¬æ”¾åœ¨ä¸€è‡´çš„èŒƒå›´å†…ã€‚æ”¶å…¥çš„èŒƒå›´è¦å¤§å¾—å¤šï¼Œè¾¾åˆ°æ•°ä¸‡ï¼Œè€Œå¹´é¾„åˆ™åœ¨100ä»¥ä¸‹ï¼Œè¿™ä¼šå‰Šå¼±ç¥ç»ç½‘ç»œçš„é¢„æµ‹èƒ½åŠ›ã€‚
- en: so getting those into consistent ranges will definitely helpã€‚I'll show you an
    example of that in a later video where we adjust just that and the predictiveor
    power goes up considerablyã€‚The other thing you can do is center these about zeroã€‚Nowã€‚
    centering them about zero doesn't help as much as the rangeï¼Œ but it is useful
    to the neural networkã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå°†è¿™äº›æ•°æ®è°ƒæ•´åˆ°ä¸€è‡´çš„èŒƒå›´è‚¯å®šä¼šæœ‰æ‰€å¸®åŠ©ã€‚æˆ‘ä¼šåœ¨ä»¥åçš„ä¸€ä¸ªè§†é¢‘ä¸­ç»™ä½ å±•ç¤ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°±è°ƒæ•´è¿™ä¸€ç‚¹ï¼Œé¢„æµ‹èƒ½åŠ›ä¼šæ˜¾è‘—æå‡ã€‚ä½ è¿˜å¯ä»¥åšçš„å¦ä¸€ä»¶äº‹æ˜¯å°†è¿™äº›æ•°æ®ä¸­å¿ƒåŒ–åˆ°é›¶é™„è¿‘ã€‚ç°åœ¨ï¼Œä¸­å¿ƒåŒ–åˆ°é›¶è™½ç„¶æ²¡æœ‰èŒƒå›´è°ƒæ•´é‚£ä¹ˆæœ‰æ•ˆï¼Œä½†å¯¹ç¥ç»ç½‘ç»œæ˜¯æœ‰ç”¨çš„ã€‚
- en: There you want to have some negative values and some positive values preferably
    about the same number of negative values as you do positiveã€‚ so it's centered
    about 0ã€‚A quick way to do both of these is to use a Z score and encode those to
    the Z score because the z score is plus or minus the standard deviations from
    the meanã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œä½ å¸Œæœ›æœ‰ä¸€äº›è´Ÿå€¼å’Œä¸€äº›æ­£å€¼ï¼Œæœ€å¥½è´Ÿå€¼å’Œæ­£å€¼çš„æ•°é‡å¤§è‡´ç›¸åŒï¼Œå› æ­¤æ•°æ®ä¸­å¿ƒå›´ç»•0ã€‚å¿«é€Ÿå®ç°è¿™ä¸¤ä¸ªç›®æ ‡çš„æ–¹æ³•æ˜¯ä½¿ç”¨Zåˆ†æ•°ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºZåˆ†æ•°ï¼Œå› ä¸ºZåˆ†æ•°æ˜¯è·ç¦»å‡å€¼çš„æ ‡å‡†å·®çš„æ­£è´Ÿå€¼ã€‚
- en: So we'll go ahead and calculate the dummy variables for say jobã€‚When we run
    thisã€‚ we can see that we get all the dummies coming out hereã€‚ each each row is
    oneã€‚Set of dummy variablesã€‚ so you have 2000 rowsï¼Œ a 2000 rows in that original
    data set and 33 dummies because each of thoseã€‚The job had 33 different job typesã€‚The
    next thing you're going to want to do is merge that into the dataset set so that
    you have a data set with those dummy variables in thereã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†ç»§ç»­è®¡ç®—ä¾‹å¦‚å·¥ä½œèŒä½çš„è™šæ‹Ÿå˜é‡ã€‚å½“æˆ‘ä»¬è¿è¡Œè¿™ä¸ªæ—¶ï¼Œå¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„è™šæ‹Ÿå˜é‡éƒ½åœ¨è¿™é‡Œï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè™šæ‹Ÿå˜é‡é›†ï¼Œå› æ­¤ä½ åœ¨åŸå§‹æ•°æ®é›†ä¸­æœ‰2000è¡Œï¼Œè€Œæœ‰33ä¸ªè™šæ‹Ÿå˜é‡ï¼Œå› ä¸ºæ¯ä¸ªå·¥ä½œèŒä½æœ‰33ç§ä¸åŒçš„èŒä½ç±»å‹ã€‚æ¥ä¸‹æ¥ä½ éœ€è¦åšçš„æ˜¯å°†å…¶åˆå¹¶åˆ°æ•°æ®é›†ä¸­ï¼Œè¿™æ ·ä½ å°±ä¼šæœ‰ä¸€ä¸ªåŒ…å«è¿™äº›è™šæ‹Ÿå˜é‡çš„æ•°æ®é›†ã€‚
- en: we're also going to drop the job columnã€‚ Aes one in both of these pieces were
    dealing with columnsã€‚And there you can see the dummy variables added to thisã€‚
    see where it's nice that we had job under bar in front of it is that prefix specified
    specified right hereã€‚Without thatï¼Œ we would have just PQï¼Œ PE just merged right
    into the data setã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜ä¼šåˆ é™¤å·¥ä½œèŒä½è¿™ä¸€åˆ—ã€‚å› ä¸ºåœ¨è¿™ä¸¤ä¸ªéƒ¨åˆ†ä¸­æˆ‘ä»¬éƒ½åœ¨å¤„ç†åˆ—ã€‚ä½ å¯ä»¥çœ‹åˆ°è™šæ‹Ÿå˜é‡å·²æ·»åŠ åˆ°æ­¤å¤„ï¼Œå‰é¢æœ‰å·¥ä½œä¸‹åˆ’çº¿ä½œä¸ºå‰ç¼€ï¼Œæ­£æ˜¯è¿™ä¸ªå‰ç¼€åœ¨è¿™é‡ŒæŒ‡å®šçš„ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªï¼Œæˆ‘ä»¬å°†ä»…ä»…å¾—åˆ°PQï¼ŒPEï¼Œç›´æ¥åˆå¹¶åˆ°æ•°æ®é›†ä¸­ã€‚
- en: and it would be very hard to keep track that those actually came from the jobã€‚We
    put area in by similar means now we have both job and areaã€‚ so this is great that
    we have that prefix area and jobã€‚Otherwiseã€‚ those two would kind of blend together
    a bitã€‚Incomeï¼Œ we do have some missing values for incomeã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”ï¼Œå¾ˆéš¾è·Ÿè¸ªè¿™äº›å®é™…ä¸Šæ¥è‡ªå·¥ä½œèŒä½çš„ä¿¡æ¯ã€‚æˆ‘ä»¬ç”¨ç±»ä¼¼çš„æ–¹æ³•å¼•å…¥åŒºåŸŸï¼Œç°åœ¨æˆ‘ä»¬åŒæ—¶æ‹¥æœ‰å·¥ä½œå’ŒåŒºåŸŸã€‚è¿™æ˜¯å¾ˆå¥½çš„ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰äº†å‰ç¼€åŒºåŸŸå’Œå·¥ä½œã€‚å¦åˆ™ï¼Œè¿™ä¸¤è€…ä¼šæœ‰ç‚¹æ··åˆåœ¨ä¸€èµ·ã€‚å…³äºæ”¶å…¥ï¼Œæˆ‘ä»¬ç¡®å®æœ‰ä¸€äº›ç¼ºå¤±å€¼ã€‚
- en: so we're going to extract the median from income and put that in to the missing
    valuesã€‚So now we have complete values for incomeã€‚ Nowï¼Œ ideally you would probably
    notã€‚ you'd probably want to get a little fancier with this and have multiple mediansã€‚
    maybe you would break the age this income probably correlates to ageã€‚And you wouldã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†ä»æ”¶å…¥ä¸­æå–ä¸­ä½æ•°ï¼Œå¹¶å°†å…¶æ”¾å…¥ç¼ºå¤±å€¼ä¸­ã€‚ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†å®Œæ•´çš„æ”¶å…¥å€¼ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½ä¸ä¼šã€‚ä½ å¯èƒ½å¸Œæœ›åœ¨è¿™æ–¹é¢åšå¾—æ›´å¤æ‚ä¸€äº›ï¼Œæ‹¥æœ‰å¤šä¸ªä¸­ä½æ•°ï¼Œä¹Ÿè®¸ä½ ä¼šå°†å¹´é¾„åˆ’åˆ†ï¼Œè¿™æ ·æ”¶å…¥å¯èƒ½ä¸å¹´é¾„ç›¸å…³ã€‚ä½ ä¼šè¿™æ ·åšã€‚
- en: Break the age into multiple bandsã€‚ You would calculate the median for each of
    those bandsã€‚ and then putã€‚Intelligently choose a medium value to fill in the missing
    valuesã€‚Where you were using that band to pick itã€‚That gives you a more meaningfulã€‚Missing
    valueã€‚We can see that we have all of the columns in the data frameã€‚And we can
    now get our X columnsã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å¹´é¾„åˆ’åˆ†ä¸ºå¤šä¸ªåŒºé—´ã€‚ä½ éœ€è¦è®¡ç®—æ¯ä¸ªåŒºé—´çš„ä¸­ä½æ•°ï¼Œç„¶åæ™ºèƒ½åœ°é€‰æ‹©ä¸€ä¸ªä¸­é—´å€¼æ¥å¡«è¡¥ç¼ºå¤±å€¼ã€‚ä½¿ç”¨è¯¥åŒºé—´æ¥è¿›è¡Œé€‰æ‹©ï¼Œè¿™æ ·å¯ä»¥å¾—åˆ°æ›´æœ‰æ„ä¹‰çš„ç¼ºå¤±å€¼ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®æ¡†ä¸­åŒ…å«äº†æ‰€æœ‰çš„åˆ—ï¼Œç°åœ¨å¯ä»¥è·å–æˆ‘ä»¬çš„Xåˆ—ã€‚
- en: We do that by getting the same columns that we had up hereã€‚ But we're going
    to drop product because productsï¼Œ what we're trying to predictã€‚ Two problems with
    including productã€‚ One isï¼Œ if that's truly what you're trying to predictï¼Œ thenã€‚That
    would be target leakï¼Œ and it would be trivial for your neural network to predict
    itã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡è·å–ä¸ä¹‹å‰ç›¸åŒçš„åˆ—æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯æˆ‘ä»¬å°†åˆ é™¤äº§å“ï¼Œå› ä¸ºäº§å“æ˜¯æˆ‘ä»¬è¯•å›¾é¢„æµ‹çš„å¯¹è±¡ã€‚åŒ…å«äº§å“å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ã€‚å¦‚æœè¿™ç¡®å®æ˜¯ä½ æƒ³é¢„æµ‹çš„å†…å®¹ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ç›®æ ‡æ³„éœ²ï¼Œç¥ç»ç½‘ç»œé¢„æµ‹å®ƒå°†æ˜¯å¾®ä¸è¶³é“çš„ã€‚
- en: You'd probably get a perfect score on your neural networkï¼Œ perfect score from
    the neural networkã€‚ not a perfect score for me because you would be producing
    a useless neural networkã€‚If you're trying to predict which product the person
    bought and you also tell the neural network which product the person boughtã€‚ that's
    targetrget leak andã€‚The neural network can do that perfectly because you've told
    it the answer alreadyã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šåœ¨ç¥ç»ç½‘ç»œä¸Šè·å¾—å®Œç¾çš„åˆ†æ•°ï¼Œæ¥è‡ªç¥ç»ç½‘ç»œçš„å®Œç¾åˆ†æ•°ã€‚ä½†å¯¹æˆ‘æ¥è¯´å¹¶ä¸æ˜¯å®Œç¾çš„åˆ†æ•°ï¼Œå› ä¸ºä½ ä¼šç”Ÿæˆä¸€ä¸ªæ— ç”¨çš„ç¥ç»ç½‘ç»œã€‚å¦‚æœä½ è¯•å›¾é¢„æµ‹ä¸€ä¸ªäººè´­ä¹°äº†å“ªä¸ªäº§å“ï¼ŒåŒæ—¶ä¹Ÿå‘Šè¯‰ç¥ç»ç½‘ç»œè¿™ä¸ªäººè´­ä¹°äº†å“ªä¸ªäº§å“ã€‚è¿™å°±æ˜¯ç›®æ ‡æ³„éœ²ã€‚ç¥ç»ç½‘ç»œå¯ä»¥å®Œç¾åœ°åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºä½ å·²ç»æå‰å‘Šè¯‰äº†å®ƒç­”æ¡ˆã€‚
- en: And you want to drop ID because ID is just numerically increasing valueï¼Œ and
    it's not that usefulã€‚So here's the final setã€‚Of the names of the individual values
    that go into the feature vector for xã€‚Now we need to convert it into a lumpumpy
    array because neural networks like numeric values to come inã€‚We just put in that
    sameã€‚X columns calculation that we do thereã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¦åˆ é™¤IDï¼Œå› ä¸ºIDåªæ˜¯æ•°å€¼é€’å¢çš„å€¼ï¼Œå¹¶ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚è¿™é‡Œæ˜¯æœ€ç»ˆçš„ç‰¹å¾å‘é‡xä¸­å„ä¸ªå€¼çš„åç§°é›†åˆã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå› ä¸ºç¥ç»ç½‘ç»œå–œæ¬¢æ•°å€¼è¾“å…¥ã€‚æˆ‘ä»¬åªéœ€å°†ä¹‹å‰çš„xåˆ—è®¡ç®—æ”¾å…¥å…¶ä¸­ã€‚
- en: We get the actual lumpy matrix for x so that we have a number of rows equal
    to the size of the data set and a number of columns equal to size of the feature
    vectorã€‚We're going to convert the product into dummiesï¼Œ and that's going to become
    the Yã€‚We keep a list of the actual textual values of the productsï¼Œ product ABCã€‚
    since there's only one categoricalï¼Œ one set of dummiesã€‚ We don't put a prefix
    on thisã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†å®é™…çš„numpyçŸ©é˜µxï¼Œä½¿å¾—è¡Œæ•°ç­‰äºæ•°æ®é›†çš„å¤§å°ï¼Œåˆ—æ•°ç­‰äºç‰¹å¾å‘é‡çš„å¤§å°ã€‚æˆ‘ä»¬å°†äº§å“è½¬æ¢ä¸ºè™šæ‹Ÿå˜é‡ï¼Œè¿™å°†æˆä¸ºYã€‚æˆ‘ä»¬ä¿ç•™å®é™…æ–‡æœ¬å€¼çš„åˆ—è¡¨ï¼Œäº§å“ä¸ºABCã€‚ç”±äºåªæœ‰ä¸€ç»„åˆ†ç±»ï¼Œæˆ‘ä»¬ä¸åœ¨å…¶ä¸Šæ·»åŠ å‰ç¼€ã€‚
- en: So we don't do products Aï¼Œ products B and so onã€‚ Then we get the matrix that
    is the y valuesã€‚ So that is going to be a number of rows equal to the size of
    the data set and number of columns equal to the products that we're classifyingã€‚We
    run thisã€‚And we run this so that we can see X and Yã€‚X is very numericã€‚ that's
    just what the neural network likesï¼Œ and Y you can see is definitely dummy variablesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šåšäº§å“Aã€äº§å“Bç­‰ã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°yå€¼çš„çŸ©é˜µã€‚æ‰€ä»¥è¡Œæ•°ç­‰äºæ•°æ®é›†çš„å¤§å°ï¼Œåˆ—æ•°ç­‰äºæˆ‘ä»¬æ­£åœ¨åˆ†ç±»çš„äº§å“ã€‚æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚ä»¥ä¾¿æŸ¥çœ‹Xå’ŒYã€‚Xæ˜¯éå¸¸æ•°å€¼çš„ï¼Œè¿™æ­£æ˜¯ç¥ç»ç½‘ç»œæ‰€å–œæ¬¢çš„ï¼Œè€ŒYåˆ™æ˜æ˜¾æ˜¯è™šæ‹Ÿå˜é‡ã€‚
- en: And that is how you calculate the x and Y for a classification neural networkã€‚
    make sure that for the classification neural networkï¼Œ you're using categorical
    cross entropyã€‚And a softm functionã€‚ Nowï¼Œ there's only two categoriesã€‚ that's kind
    of a special caseã€‚ and we'll see thatã€‚In a later video for this moduleã€‚That you'd
    use a binaryã€‚Type loss functionã€‚Nowã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¦‚ä½•ä¸ºåˆ†ç±»ç¥ç»ç½‘ç»œè®¡ç®—xå’ŒYã€‚ç¡®ä¿åœ¨åˆ†ç±»ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨åˆ†ç±»äº¤å‰ç†µå’Œsoftmaxå‡½æ•°ã€‚ç°åœ¨ï¼Œåªæœ‰ä¸¤ä¸ªç±»åˆ«ã€‚è¿™ç®—æ˜¯ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œæˆ‘ä»¬å°†åœ¨è¯¥æ¨¡å—çš„åç»­è§†é¢‘ä¸­çœ‹åˆ°ã€‚ä½ å°†ä½¿ç”¨äºŒå…ƒç±»å‹æŸå¤±å‡½æ•°ã€‚
- en: ifã€‚You are dealing with a regressionã€‚X and Yã€‚ then you would make Y be the income
    valuesã€‚ and you would make X beã€‚Just like it was previouslyã€‚ So you'd make sure
    that you were calculatingã€‚The actual values for income and not making that dummy
    variablesã€‚ So it's not really a matrixã€‚ It'd be kind of aã€‚A large column matrix
    where you have one column and one row for every single value in the data setã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¤„ç†çš„æ˜¯å›å½’ï¼ŒXå’ŒYï¼Œé‚£ä¹ˆä½ å°†ä½¿Yæˆä¸ºæ”¶å…¥å€¼ï¼Œè€ŒXåˆ™ä¸ä¹‹å‰ä¸€æ ·ã€‚æ‰€ä»¥ä½ éœ€è¦ç¡®ä¿è®¡ç®—çš„æ˜¯æ”¶å…¥çš„å®é™…å€¼ï¼Œè€Œä¸æ˜¯è™šæ‹Ÿå˜é‡ã€‚å› æ­¤ï¼Œå®ƒä¸æ˜¯çœŸæ­£çš„çŸ©é˜µï¼Œè€Œæ˜¯ä¸€ä¸ªå¤§å‹åˆ—çŸ©é˜µï¼Œæ¯ä¸ªæ•°æ®é›†ä¸­çš„æ¯ä¸ªå€¼éƒ½æœ‰ä¸€åˆ—å’Œä¸€è¡Œã€‚
- en: Nowï¼Œ also with incomeï¼Œ if that were the targetï¼Œ be careful because some of the
    income values are missingã€‚ usually if the column is your target and you have missing
    values in your targetã€‚ often you drop those rows for training because you don't
    necessarily want to be training on these in on these median incomes that youã€‚That
    you estimated for itã€‚ Thank you for watching this video on feature vector creationã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæ”¶å…¥æ˜¯ç›®æ ‡ï¼Œè¯·å°å¿ƒï¼Œå› ä¸ºæŸäº›æ”¶å…¥å€¼æ˜¯ç¼ºå¤±çš„ã€‚é€šå¸¸ï¼Œå¦‚æœåˆ—æ˜¯ä½ çš„ç›®æ ‡å¹¶ä¸”åœ¨ç›®æ ‡ä¸­æœ‰ç¼ºå¤±å€¼ï¼Œé€šå¸¸ä¼šåœ¨è®­ç»ƒæ—¶åˆ é™¤è¿™äº›è¡Œï¼Œå› ä¸ºä½ ä¸ä¸€å®šæƒ³åœ¨è¿™äº›ä¸­ä½æ•°æ”¶å…¥ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ„Ÿè°¢è§‚çœ‹æ­¤è§†é¢‘å…³äºç‰¹å¾å‘é‡åˆ›å»ºã€‚
- en: Now that we have a feature vectorã€‚ The next part is going to show you how to
    actually form a neural network around thatã€‚ train it and produce a predictionã€‚
    This content changes oftenã€‚ So subscribe to the channel to stay up to date on
    this course and other topics and artificial intelligenceã€‚ğŸ˜Šã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ç‰¹å¾å‘é‡ã€‚æ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†å‘ä½ å±•ç¤ºå¦‚ä½•å®é™…å›´ç»•å®ƒå½¢æˆä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè®­ç»ƒå®ƒå¹¶ç”Ÿæˆé¢„æµ‹ã€‚æ­¤å†…å®¹ç»å¸¸æ›´æ”¹ã€‚å› æ­¤ï¼Œè¯·è®¢é˜…é¢‘é“ä»¥ä¿æŒæ›´æ–°ï¼Œäº†è§£æœ¬è¯¾ç¨‹åŠå…¶ä»–äººå·¥æ™ºèƒ½ä¸»é¢˜ã€‚ğŸ˜Š
- en: '![](img/04abe3c48902ea74356b7dd00d915cc9_3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04abe3c48902ea74356b7dd00d915cc9_3.png)'
