- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P35ï¼šL6.3- Sylvainçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P35ï¼šL6.3- Sylvainçš„åœ¨çº¿ç›´æ’­è®²è§£ - ShowMeAI - BV1Jm4y1X7UL
- en: Yeahã€‚Welcome to the live session where we'll go over chapter2 of the Iing this
    courseã€‚ I'm joined by Lewis who on the chat and we' is going to answer all your
    question quicker than I will and don't hesitate to ask all your questions because
    I'm going to read themlo and answer them on the live stream that's like the main
    advantage of following this live stream instead of just watching the course by
    yourselfã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ¬¢è¿æ¥åˆ°ç›´æ’­ç¯èŠ‚ï¼Œæˆ‘ä»¬å°†è®¨è®ºæœ¬è¯¾ç¨‹çš„ç¬¬äºŒç« ã€‚æˆ‘å’ŒèŠå¤©ä¸­çš„è·¯æ˜“æ–¯ä¸€èµ·ï¼Œä»–ä¼šæ¯”æˆ‘æ›´å¿«å›ç­”ä½ çš„æ‰€æœ‰é—®é¢˜ï¼Œä¸è¦çŠ¹è±«ï¼Œå°½é‡é—®å‡ºä½ çš„é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä¼šåœ¨ç›´æ’­ä¸­é˜…è¯»å¹¶å›ç­”å®ƒä»¬ï¼Œè¿™ä¹Ÿæ˜¯è·Ÿéšè¿™ä¸ªç›´æ’­è€Œä¸æ˜¯è‡ªå·±è§‚çœ‹è¯¾ç¨‹çš„ä¸»è¦ä¼˜åŠ¿ã€‚
- en: So over inside this chapter2ï¼Œ we all look at the pipeline object that we used
    at length during chapter 1 on all NLP tasksã€‚And we'll see exactly how it worksï¼Œ
    we'll see how it loads a modelã€‚ how it preprocesses the inputs with a tokenizerï¼Œ
    and then how it processes with output to get the predictions and probabilities
    that we got during chapterpt1ã€‚So we'll watch a few videos that answer all the
    questions that you have and we'll do more life coding than in chapter 1 becauseã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†æŸ¥çœ‹åœ¨ç¬¬ä¸€ç« ä¸­ç”¨äºæ‰€æœ‰ NLP ä»»åŠ¡çš„ç®¡é“å¯¹è±¡ã€‚æˆ‘ä»¬å°†ç¡®åˆ‡äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œå¦‚ä½•åŠ è½½æ¨¡å‹ï¼Œå¦‚ä½•ä½¿ç”¨åˆ†è¯å™¨é¢„å¤„ç†è¾“å…¥ï¼Œä»¥åŠå¦‚ä½•å¤„ç†è¾“å‡ºä»¥è·å–æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­å¾—åˆ°çš„é¢„æµ‹å’Œæ¦‚ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è§‚çœ‹ä¸€äº›è§†é¢‘ï¼Œå›ç­”ä½ æ‰€æœ‰çš„é—®é¢˜ï¼Œå¹¶ä¸”æˆ‘ä»¬ä¼šæ¯”ç¬¬ä¸€ç« è¿›è¡Œæ›´å¤šçš„ç°åœºç¼–ç ã€‚
- en: Because chapter 1 was just a general introductionction and varies a lot of more
    code in chapter 2ã€‚So as an introductionï¼Œ as you may knowï¼Œ again face is mainly
    known for its Transformers libraryã€‚ which is a library containing a lot of transformers
    modelã€‚ and it provides an easy API to download pre trade model and to use the
    different architecturesã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºç¬¬ä¸€ç« åªæ˜¯ä¸€ä¸ªæ€»ä½“ä»‹ç»ï¼Œç¬¬äºŒç« åŒ…å«äº†æ›´å¤šçš„ä»£ç ã€‚æ‰€ä»¥ä½œä¸ºä¸€ä¸ªä»‹ç»ï¼Œæ­£å¦‚ä½ æ‰€çŸ¥ï¼ŒHugging Face ä¸»è¦ä»¥å…¶ Transformers åº“è€Œé—»åã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¼—å¤šå˜æ¢æ¨¡å‹çš„åº“ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç®€å•çš„
    API æ¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶ä½¿ç”¨ä¸åŒçš„æ¶æ„ã€‚
- en: I think there are more than 60 architectures now available into the libraryã€‚Andã€‚It
    all exposes unified API but inside and provides you with either a torch module
    or a tons of flu care model that you can use by yourself or that you canã€‚A train
    with the API that the library also providesã€‚And the goal is the views flexibility
    and simplicity and the library doesn't contain any abstraction at allã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºç°åœ¨åº“ä¸­å¯ç”¨çš„æ¶æ„è¶…è¿‡60ä¸ªã€‚å®ƒå…¨éƒ¨æš´éœ²äº†ç»Ÿä¸€çš„ APIï¼Œå¹¶æä¾›äº†ä¸€ä¸ª torch æ¨¡å—æˆ–å¤§é‡çš„ Flu Care æ¨¡å‹ï¼Œä¾›ä½ è‡ªè¡Œä½¿ç”¨æˆ–é€šè¿‡åº“æä¾›çš„
    API è¿›è¡Œè®­ç»ƒã€‚ç›®æ ‡æ˜¯å®ç°çµæ´»æ€§å’Œç®€å•æ€§ï¼Œåº“ä¸­æ²¡æœ‰ä»»ä½•æŠ½è±¡ã€‚
- en: it's not a library composed of building blocksï¼Œ every modelã€‚ every one of the
    60 architectures I was talking about is completely defined in its own modeling
    files so we can have a quick look for instance at the modeling B file which contains
    also the code of the bird model inside the library as if you look at the input
    you see like there are just torch imports and then some internal classes that
    this uses but we share between all models which are mainly the output types that
    we use will see exactly what the outputs are a little bit later when we could
    but there is no other inputs where is not like an attention block that we reuse
    the Que model the attention block for the B model is defined inside this modeling
    file so we have the B buildingsddings you have the B self attention etcaã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¯ä¸€ä¸ªç”±æ„å»ºæ¨¡å—ç»„æˆçš„åº“ï¼Œæ¯ä¸ªæ¨¡å‹ã€æˆ‘æåˆ°çš„60ä¸ªæ¶æ„ä¸­çš„æ¯ä¸€ä¸ªéƒ½æ˜¯åœ¨è‡ªå·±çš„å»ºæ¨¡æ–‡ä»¶ä¸­å®Œå…¨å®šä¹‰çš„ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¿«é€ŸæŸ¥çœ‹å»ºæ¨¡ B æ–‡ä»¶ï¼Œå®ƒä¹ŸåŒ…å«åº“å†…é¸Ÿæ¨¡å‹çš„ä»£ç ã€‚å¦‚æœä½ æŸ¥çœ‹è¾“å…¥ï¼Œä½ ä¼šå‘ç°åªæ˜¯ä¸€äº›
    torch å¯¼å…¥ï¼Œç„¶åæ˜¯å®ƒä½¿ç”¨çš„ä¸€äº›å†…éƒ¨ç±»ï¼Œä½†æˆ‘ä»¬åœ¨æ‰€æœ‰æ¨¡å‹ä¸­å…±äº«çš„ä¸»è¦æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„è¾“å‡ºç±»å‹ã€‚æˆ‘ä»¬ç¨åä¼šç¡®åˆ‡çœ‹åˆ°è¾“å‡ºï¼Œä½†æ²¡æœ‰å…¶ä»–è¾“å…¥ï¼Œåƒé‡ç”¨çš„æ³¨æ„åŠ›æ¨¡å—å¯¹äº
    B æ¨¡å‹çš„æ³¨æ„åŠ›å—æ˜¯åœ¨è¿™ä¸ªå»ºæ¨¡æ–‡ä»¶å†…éƒ¨å®šä¹‰çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰ B æ„å»ºï¼Œä½ æœ‰ B è‡ªæ³¨æ„åŠ›ç­‰ç­‰ã€‚
- en: etcã€‚![](img/0e882caa799deede0a88d67b58737bfe_1.png)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰ç­‰ï¼![](img/0e882caa799deede0a88d67b58737bfe_1.png)
- en: The idea is that if you want to play around with the model and change a line
    of code inside the model you won't have to learn 50 different filesã€‚ it's not
    subclassing somethingï¼Œ that subclassing somethingï¼Œ that' subclass somethingï¼Œ everythingã€‚
    ohï¼Œ I'm guessing I'm just realizing I'm hiding a bits okayï¼Œ I guess I'm just going
    to scroll fasterã€‚You have everything in that modeling belt file and you can play
    around and modify everything you want if you want to experiment with itã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœä½ æƒ³ç©å¼„æ¨¡å‹å¹¶ä¿®æ”¹æ¨¡å‹å†…éƒ¨çš„ä»£ç è¡Œï¼Œä½ ä¸éœ€è¦å­¦ä¹ 50ä¸ªä¸åŒçš„æ–‡ä»¶ã€‚å¹¶ä¸æ˜¯è¦è¿›è¡Œå¤šé‡ç»§æ‰¿ï¼Œæ¯ä¸€å±‚éƒ½è¦ç»§æ‰¿ï¼Œæˆ‘çªç„¶æ„è¯†åˆ°æˆ‘éšè—äº†ä¸€äº›ä¸œè¥¿ï¼Œçœ‹æ¥æˆ‘åªæ˜¯è¦æ›´å¿«åœ°æ»šåŠ¨ã€‚ä½ åœ¨å»ºæ¨¡å¸¦æ–‡ä»¶ä¸­æ‹¥æœ‰æ‰€æœ‰å†…å®¹ï¼Œå¯ä»¥éšæ„ç©å¼„å’Œä¿®æ”¹ï¼Œå¦‚æœä½ æƒ³å®éªŒçš„è¯ã€‚
- en: and that's really one of the strengths of the transformformer libraryã€‚ one of
    the features that our users have said very accurateã€‚![](img/0e882caa799deede0a88d67b58737bfe_3.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯transformersåº“çš„ä¸€ä¸ªä¼˜åŠ¿ï¼Œæˆ‘ä»¬çš„ç”¨æˆ·è¡¨ç¤ºè¿™ä¸ªåŠŸèƒ½éå¸¸å‡†ç¡®ã€‚![](img/0e882caa799deede0a88d67b58737bfe_3.png)
- en: Which is why I wanted to show it to you brieflyã€‚Andã€‚So yeahã€‚ we'll see how the
    pipeline API loads that B modelã€‚ it was actually a digitaltill B model that we
    use in chapter 1 and the corresponding dokenizerã€‚And how to do everything that
    this pipeline function was doing by hand so that you can tweak any of those steps
    if you need to on your own tasksã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘æƒ³ç®€è¦å±•ç¤ºç»™ä½ çš„åŸå› ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ç®¡é“APIå¦‚ä½•åŠ è½½Bæ¨¡å‹ã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­ä½¿ç”¨çš„æ•°å­—Bæ¨¡å‹åŠç›¸åº”çš„åˆ†è¯å™¨ã€‚å¹¶ä¸”å¦‚ä½•æ‰‹åŠ¨æ‰§è¡Œè¿™ä¸ªç®¡é“å‡½æ•°æ‰€åšçš„ä¸€åˆ‡ï¼Œä»¥ä¾¿ä½ å¯ä»¥åœ¨è‡ªå·±çš„ä»»åŠ¡ä¸­è°ƒæ•´è¿™äº›æ­¥éª¤ã€‚
- en: So let's begin with the first section and first video we're going to watch this
    introductory video that' is going to present what's happening behind the pipeline
    I'm just not going to stream them from YouTube because that's making this the
    live stream like a lot I'm just I have all the videos look at it So if you give
    me just one minuteã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹ç¬¬ä¸€éƒ¨åˆ†ï¼Œä»¥åŠæˆ‘ä»¬è¦è§‚çœ‹çš„ç¬¬ä¸€æ®µè§†é¢‘ï¼Œè¿™æ®µä»‹ç»è§†é¢‘å°†å±•ç¤ºç®¡é“èƒŒåå‘ç”Ÿçš„äº‹æƒ…ã€‚æˆ‘ä¸ä¼šç›´æ¥ä»YouTubeæ’­æ”¾ï¼Œå› ä¸ºè¿™ä¼šä½¿ç›´æ’­å˜å¾—å¾ˆå¤æ‚ï¼Œæˆ‘å·²ç»æ‹¥æœ‰æ‰€æœ‰è§†é¢‘ã€‚å¦‚æœä½ ç»™æˆ‘ä¸€åˆ†é’Ÿæ—¶é—´ã€‚
- en: I'm going to extract itã€‚![](img/0e882caa799deede0a88d67b58737bfe_5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¦æå–å®ƒã€‚![](img/0e882caa799deede0a88d67b58737bfe_5.png)
- en: And played from my computerã€‚![](img/0e882caa799deede0a88d67b58737bfe_7.png)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æˆ‘çš„ç”µè„‘æ’­æ”¾ã€‚![](img/0e882caa799deede0a88d67b58737bfe_7.png)
- en: Yeahã€‚What happens inside the pipeline functionï¼ŸIn this videoã€‚ we'll look at
    what actually happens when we use the pipeline function of the transformforms
    libraryã€‚Now specificallyï¼Œ well look at the sentiment analysis pipeline and then
    we went from the two following sentences to the positive and negative labels with
    respective scoresã€‚As we've seen in the pipeline presentationï¼Œ there are three
    stages in the pipelineã€‚Firstã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œç®¡é“å‡½æ•°å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿåœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ä½¿ç”¨transformersåº“çš„ç®¡é“å‡½æ•°æ—¶å®é™…ä¸Šå‘ç”Ÿäº†ä»€ä¹ˆã€‚ç°åœ¨å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹æƒ…æ„Ÿåˆ†æç®¡é“ï¼Œç„¶åæˆ‘ä»¬å°†ä»ä»¥ä¸‹ä¸¤ä¸ªå¥å­è·å¾—ç›¸åº”çš„æ­£é¢å’Œè´Ÿé¢æ ‡ç­¾åŠå…¶åˆ†æ•°ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç®¡é“æ¼”ç¤ºä¸­çœ‹åˆ°çš„ï¼Œç®¡é“æœ‰ä¸‰ä¸ªé˜¶æ®µã€‚é¦–å…ˆã€‚
- en: we convert the redex to numbers the model can make signs of using a tokenizerã€‚Then
    those numbers goes through the modelï¼Œ which outputs loadsã€‚Finallyã€‚ the best processing
    steps transform those delegates into labels and skullã€‚Let's look in details at
    those three steps and how to replicate them using the Transform libraryã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é‡å†™è¡¨è¾¾å¼è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ä½¿ç”¨çš„æ•°å­—ï¼Œå¹¶é€šè¿‡åˆ†è¯å™¨è¿›è¡Œå¤„ç†ã€‚ç„¶åï¼Œè¿™äº›æ•°å­—ç»è¿‡æ¨¡å‹ï¼Œè¾“å‡ºå¤§é‡å†…å®¹ã€‚æœ€åï¼Œæœ€ä½³å¤„ç†æ­¥éª¤å°†è¿™äº›ä»£è¡¨è½¬åŒ–ä¸ºæ ‡ç­¾å’Œåˆ†æ•°ã€‚è®©æˆ‘ä»¬è¯¦ç»†äº†è§£è¿™ä¸‰ä¸ªæ­¥éª¤ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨Transformåº“å¤åˆ¶å®ƒä»¬ã€‚
- en: beginning with the first stage tokenizationã€‚So to process has several steps
    firstã€‚ the text is split into small chunks called tokensã€‚They can be wordsã€‚ part
    of words or punctuation symbolsï¼Œ then the tokensizer will add some special tokens
    in the model expectã€‚Hereï¼Œ the middle used expect a CLS token at the beginning
    and a S token at the end of the sentence to classifyã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç¬¬ä¸€é˜¶æ®µçš„åˆ†è¯å¼€å§‹ã€‚è¿™ä¸ªå¤„ç†è¿‡ç¨‹æœ‰å‡ ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆï¼Œæ–‡æœ¬è¢«æ‹†åˆ†æˆç§°ä¸ºä»¤ç‰Œçš„å°å—ã€‚å®ƒä»¬å¯ä»¥æ˜¯å•è¯ã€å•è¯çš„ä¸€éƒ¨åˆ†æˆ–æ ‡ç‚¹ç¬¦å·ï¼Œç„¶ååˆ†è¯å™¨å°†æ·»åŠ ä¸€äº›æ¨¡å‹æ‰€æœŸæœ›çš„ç‰¹æ®Šä»¤ç‰Œã€‚åœ¨è¿™é‡Œï¼Œä¸­é—´ä½¿ç”¨çš„æ¨¡å‹åœ¨å¥å­å¼€å§‹æ—¶æœŸæœ›ä¸€ä¸ªCLSä»¤ç‰Œï¼Œåœ¨å¥å­ç»“æŸæ—¶æœŸæœ›ä¸€ä¸ªSä»¤ç‰Œï¼Œä»¥è¿›è¡Œåˆ†ç±»ã€‚
- en: Lastlyï¼Œ the token isone patches each token to its unique ID in the vocabulary
    of the pro modelã€‚To load the tokenizerï¼Œ the transformformers library provides
    the autotokenizer APIã€‚The most important method of this class is from Pretrainedã€‚
    which will download and cache the configuration and the vocabulary associated
    to a given checkpointã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä»¤ç‰Œå°†æ¯ä¸ªä»¤ç‰Œæ˜ å°„åˆ°ä¸“ä¸šæ¨¡å‹è¯æ±‡ä¸­çš„å”¯ä¸€IDã€‚è¦åŠ è½½åˆ†è¯å™¨ï¼Œtransformersåº“æä¾›äº†autotokenizer APIã€‚è¿™ä¸ªç±»ä¸­æœ€é‡è¦çš„æ–¹æ³•æ˜¯from
    Pretrainedï¼Œå®ƒä¼šä¸‹è½½å¹¶ç¼“å­˜ä¸ç»™å®šæ£€æŸ¥ç‚¹ç›¸å…³çš„é…ç½®å’Œè¯æ±‡ã€‚
- en: Hereï¼Œ the checkpoint used by default for the supplement analysis pipeline is
    distillbel baseline case5 tune SS2 Englishã€‚ which is a bit of a mouthfulã€‚We instance
    to tookken as with a checkpointã€‚ and feed it to the two sentencesã€‚Since the two
    sentences are not the same sizeã€‚ well need to pad the shed oneand to be able to
    build an arrayã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œè¡¥å……åˆ†æç®¡é“é»˜è®¤ä½¿ç”¨çš„æ£€æŸ¥ç‚¹æ˜¯ distillbel baseline case5 tune SS2 Englishã€‚è¿™æœ‰ç‚¹æ‹—å£ã€‚æˆ‘ä»¬å°†å®ä¾‹åŒ–ä¸º
    tokenï¼Œä½œä¸ºä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ä¸¤ä¸ªå¥å­ä¸­ã€‚ç”±äºè¿™ä¸¤ä¸ªå¥å­å¤§å°ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¾ƒå°çš„å¥å­è¿›è¡Œå¡«å……ï¼Œä»¥ä¾¿æ„å»ºä¸€ä¸ªæ•°ç»„ã€‚
- en: This is done by the tokenizer with the option padding equal trueã€‚With truation
    equal trueã€‚ we ensure that any sentence longer and the maximum the model can handle
    is truncatedã€‚Lastlyã€‚ the return tensil option gal the tokenizer to return the
    bytch tensilã€‚Looking as a resultã€‚ we see we have a dictionary with two keysï¼Œ input
    ID contains the ideas of both sentences with zero where the padding is appliedã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é€šè¿‡å°† padding é€‰é¡¹è®¾ç½®ä¸º true çš„ tokenizer å®Œæˆçš„ã€‚é€šè¿‡å°† truncation è®¾ç½®ä¸º trueï¼Œæˆ‘ä»¬ç¡®ä¿ä»»ä½•è¶…è¿‡æ¨¡å‹æœ€å¤§å¤„ç†èƒ½åŠ›çš„å¥å­éƒ½ä¼šè¢«æˆªæ–­ã€‚æœ€åï¼Œè¿”å›å¼ é‡é€‰é¡¹å…è®¸
    tokenizer è¿”å›æ‰¹å¤„ç†å¼ é‡ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸ï¼Œinput ID åŒ…å«ä¸¤ä¸ªå¥å­çš„ IDï¼Œå¹¶åœ¨å¡«å……åº”ç”¨çš„åœ°æ–¹ç”¨é›¶æ›¿ä»£ã€‚
- en: The second key attention mask indicates where petting has been appliedã€‚ so the
    model does not pay attention to itã€‚This is always is inside the took stepã€‚Now
    let's have a look at the second stepã€‚è¿™ä¸æ‡‚ã€‚As for tokenizerã€‚ or is a nottomod API
    over from pretrained methodï¼Œ it will download lu and cache the configuration of
    the model as well as the pretrain weightã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå…³é”®æ³¨æ„åŠ›æ©ç æŒ‡ç¤ºäº†åº”ç”¨äº†å¡«å……çš„ä½ç½®ï¼Œä»¥ä¾¿æ¨¡å‹ä¸å…³æ³¨å®ƒã€‚è¿™å§‹ç»ˆæ˜¯åœ¨ token æ­¥éª¤å†…éƒ¨ã€‚ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹ç¬¬äºŒæ­¥ã€‚è¿™æœ‰ç‚¹ä¸æ˜ç™½ã€‚è‡³äº tokenizerï¼Œæˆ–è€…è¯´æ˜¯ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒæ–¹æ³•çš„
    nottomod APIï¼Œå®ƒå°†ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹çš„é…ç½®ä»¥åŠé¢„è®­ç»ƒæƒé‡ã€‚
- en: Howeverï¼Œ the autotomodl API will only instantiate the body of the modelã€‚ that
    is the part of the model that is left once the pro traininging head is removedã€‚It
    will output a high dimensional tensorï¼Œ that is a representation of the sentence's
    pastã€‚ but which is not directly useful for a classification problemã€‚Hereï¼Œ the
    tensor has two sentencesã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œautotomodl API åªä¼šå®ä¾‹åŒ–æ¨¡å‹çš„ä¸»ä½“éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯ç§»é™¤é¢„è®­ç»ƒå¤´åçš„æ¨¡å‹éƒ¨åˆ†ã€‚å®ƒå°†è¾“å‡ºä¸€ä¸ªé«˜ç»´å¼ é‡ï¼Œå³å¥å­çš„è¿‡å»è¡¨ç¤ºï¼Œä½†è¿™å¯¹äºåˆ†ç±»é—®é¢˜å¹¶ä¸ç›´æ¥æœ‰ç”¨ã€‚åœ¨è¿™é‡Œï¼Œå¼ é‡åŒ…å«ä¸¤ä¸ªå¥å­ã€‚
- en: each of 16 tokensï¼Œ and the last dimension is the Indian size of our modelï¼Œ 768ã€‚To
    get an output link to our classification problemã€‚ we need to use the Automodal
    for sequence classificationification classã€‚It works exactly as zero to model classï¼Œ
    except by 12 built a model with a classification headã€‚ğŸ˜Šã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª 16 ä¸ª tokensï¼Œæœ€åä¸€ä¸ªç»´åº¦æ˜¯æˆ‘ä»¬æ¨¡å‹çš„è¾“å…¥å¤§å°ï¼Œ768ã€‚è¦è·å–ä¸æˆ‘ä»¬çš„åˆ†ç±»é—®é¢˜ç›¸å…³çš„è¾“å‡ºé“¾æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ç”¨äºåºåˆ—åˆ†ç±»çš„ Automodal
    ç±»ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ä¸ zero åˆ°æ¨¡å‹ç±»å®Œå…¨ç›¸åŒï¼Œåªæ˜¯æ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹ã€‚ğŸ˜Š
- en: Praise one auto class for each common NLP task in the transformformers libraryã€‚Hereã€‚
    after giving our models the two sentencesï¼Œ we get a tensor of size 2 by 2ã€‚ one
    result for each sentence and for each possible levelã€‚Those outputs are not probabilities
    yetã€‚ we can see they don't sum to oneã€‚This is because each model of the transformformer's
    library returns look itã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ transformformers åº“ä¸­ä¸ºæ¯ä¸ªå¸¸è§çš„ NLP ä»»åŠ¡è¡¨æ‰¬ä¸€ä¸ªè‡ªåŠ¨ç±»ã€‚åœ¨è¿™é‡Œï¼Œåœ¨ç»™æˆ‘ä»¬çš„æ¨¡å‹ä¸¤ä¸ªå¥å­åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå¤§å°ä¸º 2x2 çš„å¼ é‡ï¼Œæ¯ä¸ªå¥å­å’Œæ¯ä¸ªå¯èƒ½çš„çº§åˆ«éƒ½æœ‰ä¸€ä¸ªç»“æœã€‚è¿™äº›è¾“å‡ºå°šæœªæ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬çš„æ€»å’Œä¸ä¸ºä¸€ã€‚è¿™æ˜¯å› ä¸º
    transformformers åº“ä¸­çš„æ¯ä¸ªæ¨¡å‹è¿”å›çš„æ˜¯ logitsã€‚
- en: To make sense of those looksï¼Œ we need to dig into the third and last step of
    the pipeline plus processingã€‚To conduct Lo into probabilitiesï¼Œ we need to apply
    a softmax layers to themã€‚As we can seeã€‚ this transforms them into positive number
    that's a up to1ã€‚The last step is to know which of those corresponds to the positive
    or the negative labelã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£è¿™äº› logitsï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥åˆ†æç®¡é“å¤„ç†çš„ç¬¬ä¸‰æ­¥å’Œæœ€åä¸€æ­¥ã€‚ä¸ºäº†å°† logits è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬åº”ç”¨ softmax å±‚ã€‚æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè¿™å°†å®ƒä»¬è½¬å˜ä¸ºæ­£æ•°ï¼Œæ€»å’Œä¸º
    1ã€‚æœ€åä¸€æ­¥æ˜¯ç¡®å®šå“ªäº›å¯¹åº”äºæ­£æ ‡ç­¾æˆ–è´Ÿæ ‡ç­¾ã€‚
- en: this is given by the ID2lipal field of the model conflictã€‚The first proba is
    index0ã€‚ correspond to the negative level and the second index1 correspond to the
    positive levelã€‚This is how our classifier built with the pipeline function peaked
    with labels and compute those scoresã€‚ğŸ˜Šï¼ŒNow that you know how each step worksï¼Œ
    you can easily tweak them to your needsã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”±æ¨¡å‹å†²çªçš„ ID2lipal å­—æ®µç»™å‡ºçš„ã€‚ç¬¬ä¸€ä¸ªæ¦‚ç‡æ˜¯ç´¢å¼• 0ï¼Œä»£è¡¨è´Ÿçº§åˆ«ï¼Œç¬¬äºŒä¸ªç´¢å¼• 1 ä»£è¡¨æ­£çº§åˆ«ã€‚è¿™å°±æ˜¯æˆ‘ä»¬çš„åˆ†ç±»å™¨å¦‚ä½•é€šè¿‡ç®¡é“å‡½æ•°ä¸æ ‡ç­¾ç»“åˆå¹¶è®¡ç®—è¿™äº›åˆ†æ•°ã€‚ğŸ˜Š
    ç°åœ¨ä½ çŸ¥é“æ¯ä¸€æ­¥æ˜¯å¦‚ä½•è¿ä½œçš„ï¼Œä½ å¯ä»¥è½»æ¾è°ƒæ•´å®ƒä»¬ä»¥æ»¡è¶³ä½ çš„éœ€æ±‚ã€‚
- en: '![](img/0e882caa799deede0a88d67b58737bfe_9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_9.png)'
- en: å—¯ã€‚![](img/0e882caa799deede0a88d67b58737bfe_11.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚![](img/0e882caa799deede0a88d67b58737bfe_11.png)
- en: Yeahã€‚![](img/0e882caa799deede0a88d67b58737bfe_13.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚![](img/0e882caa799deede0a88d67b58737bfe_13.png)
- en: å—¯å¯¹ã€‚å“å‘€ã€‚Backã€‚å—¯ã€‚I think that somethingã€‚Wronong with webcamã€‚ Let me just double
    double checkã€‚Okayã€‚ somehow my head disappearedã€‚ I don't know why exactlyã€‚ if you
    can see itã€‚ feel free to say it in the chatã€‚In the meantimeï¼Œ there is a questionã€‚
    what does the triple that signify in from that activations import act to function
    in the PY files shownoneï¼Ÿ
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œå¯¹ã€‚å“å‘€ã€‚å›æ¥äº†ã€‚å—¯ã€‚æˆ‘è§‰å¾—æ‘„åƒå¤´å‡ºäº†ç‚¹é—®é¢˜ã€‚è®©æˆ‘å†æ£€æŸ¥ä¸€ä¸‹ã€‚å¥½çš„ï¼Œæˆ‘çš„å¤´ä¸è§äº†ã€‚æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚å¦‚æœä½ èƒ½çœ‹åˆ°ï¼Œè¯·åœ¨èŠå¤©ä¸­å‘Šè¯‰æˆ‘ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜ã€‚æ¥è‡ªâ€œä»æ¿€æ´»å¯¼å…¥
    act åˆ° functionâ€ä¸­çš„ä¸‰é‡ç¬¦å·è¡¨ç¤ºä»€ä¹ˆï¼Œåœ¨è¿™äº› PY æ–‡ä»¶ä¸­å±•ç¤ºï¼Ÿ
- en: Very good questionï¼Œ so this is standard Python if you're building the packageã€‚In
    Pythonã€‚ and you're trying to import thingsã€‚You have several levelsã€‚ so it's because
    of the structure of the transformer repoã€‚ let me try to pull it back and quit
    my VS codeã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½çš„é—®é¢˜ï¼Œè¿™åœ¨æ„å»ºåŒ…æ—¶æ˜¯æ ‡å‡†çš„ Pythonã€‚åœ¨ Python ä¸­ï¼Œä½ å°è¯•å¯¼å…¥ä¸œè¥¿æ—¶ï¼Œæœ‰å‡ ä¸ªå±‚çº§ã€‚è¿™æ˜¯å› ä¸º transformer ä»“åº“çš„ç»“æ„ã€‚è®©æˆ‘è¯•ç€æ‹‰å›å»ï¼Œé€€å‡ºæˆ‘çš„
    VS Codeã€‚
- en: '![](img/0e882caa799deede0a88d67b58737bfe_15.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_15.png)'
- en: Butã€‚Rise either Here we areã€‚ So you as a transformers folder and then inside
    that transformer modelsã€‚ you have a model subfold and then a bird subfold and
    when the modeling file is here And since we've organized the good that way to
    avoid everything all the files directly in the transformers folder because as
    I said we have like 60 different architectureã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ã€‚å¥½çš„ï¼Œæˆ‘ä»¬æ¥äº†ã€‚æ‰€ä»¥ä½ æœ‰ä¸€ä¸ª transformers æ–‡ä»¶å¤¹ï¼Œç„¶åé‡Œé¢æ˜¯ transformer modelsï¼Œä½ æœ‰ä¸€ä¸ªæ¨¡å‹å­æ–‡ä»¶å¤¹ï¼Œç„¶åæ˜¯ bird
    å­æ–‡ä»¶å¤¹ï¼Œæ¨¡å‹æ–‡ä»¶å°±åœ¨è¿™é‡Œã€‚ç”±äºæˆ‘ä»¬å°†ä»£ç ç»„ç»‡æˆè¿™æ ·ï¼Œä»¥é¿å…æ‰€æœ‰æ–‡ä»¶ç›´æ¥æ”¾åœ¨ transformers æ–‡ä»¶å¤¹ä¸­ï¼Œå› ä¸ºæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬æœ‰å¤§çº¦ 60 ç§ä¸åŒçš„æ¶æ„ã€‚
- en: So that would be a little of files there are organized like this And when you're
    trying to import when you say from dot it's to go back inside the structureã€‚ So
    from import blah blah blah would be in the bird foldert is in the model folder
    and then thet gets back to the transformers folderã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›æ–‡ä»¶çš„ç»„ç»‡æ–¹å¼æ˜¯è¿™æ ·çš„ï¼Œå½“ä½ å°è¯•å¯¼å…¥æ—¶ï¼Œä½¿ç”¨â€œfrom dotâ€æ˜¯ä¸ºäº†å›åˆ°ç»“æ„å†…éƒ¨ã€‚å› æ­¤ï¼Œä»â€œimport blah blah blahâ€ä¼šåœ¨
    bird æ–‡ä»¶å¤¹ä¸­ï¼Œä½äºæ¨¡å‹æ–‡ä»¶å¤¹ä¸­ï¼Œç„¶åå†è¿”å›åˆ° transformers æ–‡ä»¶å¤¹ã€‚
- en: So it's just a way to come back to the root of the post directoryã€‚å—¯ã€‚I I didn't
    see any of questionsã€‚ but don't hesitate to ask at any time in the chat your questions
    and I'll answer as best as I can and as I expected my webca is not showing anymore
    and I have no idea why becauseã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™åªæ˜¯ä¸€ç§å›åˆ°å¸–å­ç›®å½•æ ¹ç›®å½•çš„æ–¹å¼ã€‚å—¯ã€‚æˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•é—®é¢˜ï¼Œä½†åœ¨èŠå¤©ä¸­éšæ—¶æé—®ï¼Œæˆ‘ä¼šå°½åŠ›å›ç­”ã€‚è€Œä¸”æ­£å¦‚æˆ‘é¢„æœŸçš„ï¼Œæˆ‘çš„æ‘„åƒå¤´ä¸å†æ˜¾ç¤ºï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚
- en: '![](img/0e882caa799deede0a88d67b58737bfe_17.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_17.png)'
- en: I justã€‚S the video and it' is not workingã€‚Let me just try somethingã€‚Here we
    areã€‚ sorry about that hair too just shut it down and restart started itã€‚So we're
    good to continue our expression behind the pipeline function and so we'll just
    take a little bit at the code that we just doing in the video I'm not going to
    show it from the section side but remember that in most sections in all the sections
    that have could you have an open coll button at the top like this which I opened
    a little bit earlier just to execute the first cell which is installing everything
    and can take a bit of time and then the second cell which is download the model
    I executed it already so that we have the result instantaneously here and we are
    ready to look at the rest of the notebook so like in the video let me move myself
    on the other side of the screen because the codeã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åˆšæ‰çœ‹äº†è¿™ä¸ªè§†é¢‘ï¼Œä½†å®ƒæ²¡æœ‰å·¥ä½œã€‚è®©æˆ‘å°è¯•ä¸€ä¸‹ã€‚è¿™é‡Œæˆ‘ä»¬æ¥äº†ï¼ŒæŠ±æ­‰ï¼ŒåˆšåˆšæŠŠæ‘„åƒå¤´å…³æ‰å†é‡å¯ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç»§ç»­è®¨è®ºç®¡é“å‡½æ•°èƒŒåçš„è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬å°†ç¨å¾®æŸ¥çœ‹ä¸€ä¸‹è§†é¢‘ä¸­æ­£åœ¨åšçš„ä»£ç ã€‚æˆ‘ä¸ä¼šä»ä»£ç éƒ¨åˆ†å±•ç¤ºï¼Œä½†è¯·è®°ä½ï¼Œåœ¨å¤§å¤šæ•°éƒ¨åˆ†ï¼Œæ‰€æœ‰éƒ¨åˆ†éƒ½æœ‰ä¸€ä¸ªå¼€æ”¾çš„æŒ‰é’®åœ¨é¡¶éƒ¨ï¼Œå°±åƒè¿™ä¸ªï¼Œæˆ‘ç¨æ—©æ‰“å¼€è¿‡ï¼Œä»¥æ‰§è¡Œç¬¬ä¸€ä¸ªå•å…ƒï¼Œå®‰è£…æ‰€æœ‰å†…å®¹ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´ï¼Œç„¶åç¬¬äºŒä¸ªå•å…ƒæ˜¯ä¸‹è½½æ¨¡å‹ï¼Œæˆ‘å·²ç»æ‰§è¡Œè¿‡äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œæœ‰ç»“æœï¼Œæˆ‘ä»¬å‡†å¤‡æŸ¥çœ‹ç¬”è®°æœ¬çš„å…¶ä½™éƒ¨åˆ†ã€‚å°±åƒåœ¨è§†é¢‘ä¸­ä¸€æ ·ï¼Œè®©æˆ‘æŠŠè‡ªå·±ç§»åˆ°å±å¹•çš„å¦ä¸€ä¾§ï¼Œå› ä¸ºä»£ç ã€‚
- en: Is mostly on the leftã€‚Andã€‚So like in the videoï¼Œ we all look exactly at the code
    that is executed when we try to use the sentiment analysis pipeline on two sentences
    like thatã€‚And see how we get to the results on those labelsã€‚ So as we've seen
    in the videoã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§éƒ¨åˆ†å†…å®¹åœ¨å·¦ä¾§ã€‚æ‰€ä»¥åƒè§†é¢‘ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬éƒ½å‡†ç¡®æŸ¥çœ‹äº†åœ¨å¯¹ä¸¤å¥è¯ä½¿ç”¨æƒ…æ„Ÿåˆ†æç®¡é“æ—¶æ‰§è¡Œçš„ä»£ç ã€‚çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•å¾—åˆ°è¿™äº›æ ‡ç­¾çš„ç»“æœã€‚æ­£å¦‚æˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ã€‚
- en: the first step the prepossessing is done by a tokenizer so we'll look into the
    tokens in detail a little bit further ahead but for not just you just need to
    know that the tokenizer texts the input text so those two sentences I've been
    waiting for the U face calls my own life and I edit it so much it's going to take
    those two sentences and convert that them into numbers because the model doesn't
    understand text it understand numbers So basically behind the scenes it's going
    to split that text into small chunks that we call token and so will tokenizer
    and then associate each of us token to a unique ID which is the numbers that were
    gonna to seeã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼Œé¢„å¤„ç†ç”±ä»¤ç‰Œå™¨å®Œæˆï¼Œæ‰€ä»¥æˆ‘ä»¬ç¨åå°†è¯¦ç»†æŸ¥çœ‹ä»¤ç‰Œï¼Œä½†ä½ åªéœ€çŸ¥é“ä»¤ç‰Œå™¨å¤„ç†è¾“å…¥æ–‡æœ¬ã€‚è¿™ä¸¤å¥è¯æ˜¯â€œæˆ‘ç­‰ç€ä½ é¢å¯¹æˆ‘è‡ªå·±çš„ç”Ÿæ´»â€ï¼Œæˆ‘ç¼–è¾‘äº†å¾ˆå¤šï¼Œå®ƒå°†è¿™ä¸¤å¥è¯è½¬æ¢æˆæ•°å­—ï¼Œå› ä¸ºæ¨¡å‹ä¸ç†è§£æ–‡æœ¬ï¼Œå®ƒç†è§£æ•°å­—ã€‚æ‰€ä»¥åœ¨å¹•åï¼Œå®ƒå°†æ–‡æœ¬æ‹†åˆ†æˆæˆ‘ä»¬ç§°ä¹‹ä¸ºä»¤ç‰Œçš„å°å—ï¼Œç„¶åä»¤ç‰Œå™¨å°†æ¯ä¸ªä»¤ç‰Œä¸ä¸€ä¸ªå”¯ä¸€IDå…³è”ï¼Œè¿™äº›æ•°å­—å°±æ˜¯æˆ‘ä»¬å°†è¦çœ‹åˆ°çš„ã€‚
- en: To load the tokenizerï¼Œ we need to know the identifier of the tokenizerã€‚So here
    this is the identifier of the model that is used by sentiment analysis pipeline
    by defaultã€‚ and then we just call autotokenize order from pretrained andã€‚It's
    gonna download if you I've the files already downloaded because I executed this
    firstã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åŠ è½½ä»¤ç‰Œå™¨ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“ä»¤ç‰Œå™¨çš„æ ‡è¯†ç¬¦ã€‚æ‰€ä»¥è¿™é‡Œæ˜¯é»˜è®¤ç”¨äºæƒ…æ„Ÿåˆ†æç®¡é“çš„æ¨¡å‹æ ‡è¯†ç¬¦ã€‚ç„¶åæˆ‘ä»¬åªéœ€è°ƒç”¨autotokenizeï¼Œé¡ºåºæ¥è‡ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ å·²ç»ä¸‹è½½äº†æ–‡ä»¶ï¼Œå› ä¸ºæˆ‘æ‰§è¡Œäº†è¿™ä¸ªæ­¥éª¤ï¼Œå®ƒä¼šä¸‹è½½ã€‚
- en: but if it's the first time you're executing thisï¼Œ it's going to download the
    files of the tokenizer and in particular the vocabulary which contains the mapping
    token to unique ID and instantiate it and once you have that objective availableã€‚
    you can fit it your input directly like this so raw input inside the tokenizer
    and we all exactly explain what for spaing and tru meanã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œå®ƒå°†ä¸‹è½½ä»¤ç‰Œå™¨çš„æ–‡ä»¶ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ä»¤ç‰Œä¸å”¯ä¸€IDæ˜ å°„çš„è¯æ±‡è¡¨ï¼Œå¹¶å®ä¾‹åŒ–å®ƒã€‚ä¸€æ—¦ä½ æœ‰äº†é‚£ä¸ªç›®æ ‡ï¼Œå°±å¯ä»¥åƒè¿™æ ·ç›´æ¥å¤„ç†è¾“å…¥ï¼Œå°†åŸå§‹è¾“å…¥æ”¾å…¥ä»¤ç‰Œå™¨ï¼Œæˆ‘ä»¬å°†ç¡®åˆ‡è§£é‡Šâ€œspaingâ€å’Œâ€œtruâ€çš„å«ä¹‰ã€‚
- en: A little bit for and we tell it to return tensilors and since we are using Pytoch
    hereã€‚ we tell it to returns Pytoch tensor with viPTã€‚You can also say TF of tons
    of litensilsã€‚ N for N arrays or Fls for flexï¼Œ which isï¼Œ I guess for Flï¼Œ it's also
    an n arraysã€‚And if we execute itï¼Œ we can see that we get an output which is a
    dictionary with input ID and attention mask we'll explain what the attention mask
    means a little bit for in the courseã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ç‚¹ä¸åŒï¼Œæˆ‘ä»¬å‘Šè¯‰å®ƒè¿”å›å¼ é‡ï¼Œç”±äºæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨Pytochï¼Œæ‰€ä»¥æˆ‘ä»¬å‘Šè¯‰å®ƒè¿”å›Pytochå¼ é‡å’ŒviPTã€‚ä½ ä¹Ÿå¯ä»¥è¯´TFï¼Œè¡¨ç¤ºå¤§é‡çš„å¼ é‡ã€‚Nè¡¨ç¤ºNä¸ªæ•°ç»„ï¼Œæˆ–è€…Flè¡¨ç¤ºflexï¼Œæˆ‘æƒ³å¯¹äºFlï¼Œå®ƒä¹Ÿæ˜¯Nä¸ªæ•°ç»„ã€‚å¦‚æœæˆ‘ä»¬æ‰§è¡Œå®ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«è¾“å…¥IDå’Œæ³¨æ„åŠ›æ©ç ï¼Œæˆ‘ä»¬å°†åœ¨è¯¾ç¨‹ä¸­ç¨åè§£é‡Šæ³¨æ„åŠ›æ©ç çš„å«ä¹‰ã€‚
- en: In this last sessionï¼Œ sorryï¼Œ and the input I are the unique numbers I was talking
    aboutã€‚ So it's converted that text into small chunks of petaccle tokens and each
    of us token have been associated to unique number And once we are that we canã€‚Use
    as the modelã€‚On this inputã€‚Just if we want to have a lookã€‚Backã€‚Why are you annoying
    me if we want to have a look back at how those Is correspond to the text that
    we had at the beginningã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åä¸€ä¸ªä¼šè¯ä¸­ï¼ŒæŠ±æ­‰ï¼Œè¾“å…¥çš„â€œIâ€æ˜¯æˆ‘æ‰€æåˆ°çš„å”¯ä¸€æ•°å­—ã€‚æ‰€ä»¥å®ƒæŠŠæ–‡æœ¬è½¬æ¢æˆå°å—çš„petaccleä»¤ç‰Œï¼Œæ¯ä¸ªä»¤ç‰Œéƒ½è¢«å…³è”åˆ°å”¯ä¸€çš„æ•°å­—ã€‚ä¸€æ—¦æˆ‘ä»¬æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½œä¸ºæ¨¡å‹ä½¿ç”¨è¿™ä¸ªè¾“å…¥ã€‚åªè¦æˆ‘ä»¬æƒ³å›é¡¾ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆä½ è¦çƒ¦æˆ‘ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å›é¡¾ä¸€ä¸‹è¿™äº›ä¸ä¸€å¼€å§‹çš„æ–‡æœ¬æ˜¯å¦‚ä½•å¯¹åº”çš„ã€‚
- en: we can use the decocode method of autokenizerï¼Œ So I type tokenizer the Decodeã€‚And
    thenã€‚ I'm gonna toã€‚Take my inputsï¼Œ grab the key input Iã€‚Ohelloã€‚D diã€‚Like thisï¼Œ
    so this is a tensorã€‚ so I'm going to convert it to a listã€‚And take this is a list
    of lists because I have two the two sentences in my sentence in my tensor sorryã€‚
    so I'm just taking the first oneï¼Œ for instanceï¼Œ and if I that execute that sorry
    I can see my original text which has been a bit prepossessedã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨autotokenizerçš„decocodeæ–¹æ³•ï¼Œæ‰€ä»¥æˆ‘è¾“å…¥tokenizerçš„Decodeã€‚ç„¶åï¼Œæˆ‘å°†è·å–æˆ‘çš„è¾“å…¥ï¼ŒæŠ“å–å…³é”®è¾“å…¥Iã€‚Ohelloã€‚D
    diã€‚åƒè¿™æ ·ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¼ é‡ã€‚æˆ‘å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨ã€‚å› ä¸ºæˆ‘åœ¨å¼ é‡ä¸­æœ‰ä¸¤ä¸ªå¥å­ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘åªæ˜¯è·å–ç¬¬ä¸€ä¸ªå¥å­ï¼Œå¦‚æœæˆ‘æ‰§è¡Œè¿™ä¸ªï¼ŒæŠ±æ­‰ï¼Œæˆ‘å¯ä»¥çœ‹åˆ°æˆ‘çš„åŸå§‹æ–‡æœ¬ç»è¿‡äº†ä¸€äº›é¢„å¤„ç†ã€‚
- en: there is no capital anymore for the I for instanceã€‚And we can also see that
    the tokenizer added something at the beginning and something at the endã€‚ This
    is perfectly logicalï¼Œ as the tokenizer is adding thoseã€‚The token is always adding
    for tokens because the model expects themã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨â€œæˆ‘â€ä¸å†æ˜¯å¤§å†™å­—æ¯äº†ã€‚ä¾‹å¦‚ï¼Œä»¤ç‰Œå™¨åœ¨å¼€å¤´å’Œç»“å°¾æ·»åŠ äº†ä¸€äº›å†…å®¹ã€‚è¿™æ˜¯å®Œå…¨åˆä¹é€»è¾‘çš„ï¼Œå› ä¸ºä»¤ç‰Œå™¨æ­£åœ¨æ·»åŠ è¿™äº›ã€‚ä»¤ç‰Œæ€»æ˜¯ä¸ºä»¤ç‰Œæ·»åŠ ï¼Œå› ä¸ºæ¨¡å‹æœŸå¾…å®ƒä»¬ã€‚
- en: So I'm just going to pause here for questions before I go into the model part
    of the codeã€‚What does in Ututuanizer meanï¼ŸAnd what type of tokenazizer is usedã€‚
    So the auto for auto tokenizer mean in the auto tokenazizer class means that you
    canã€‚Load any tokenizer corresponding to any architecture using that APIã€‚ So for
    instanceã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›å…¥ä»£ç çš„æ¨¡å‹éƒ¨åˆ†ä¹‹å‰ï¼Œæˆ‘å°†åœ¨è¿™é‡Œæš‚åœä»¥ä¾¿æé—®ã€‚Ututuanizeræ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿä½¿ç”¨äº†ä»€ä¹ˆç±»å‹çš„tokenizerï¼Ÿauto tokenizerç±»ä¸­çš„autoæ„å‘³ç€ä½ å¯ä»¥ä½¿ç”¨è¯¥APIåŠ è½½ä»»ä½•å¯¹åº”äºä»»ä½•æ¶æ„çš„tokenizerã€‚ä¾‹å¦‚ã€‚
- en: here our model is a distilled bird modelã€‚ So the tokenizer is going to be a
    distilled bird tokenizer can double check that by just adding a console So if
    I typeã€‚Dkenizerã€‚And print the outputã€‚It's going to tell me it's a pretend tokener
    fastã€‚ which is not super usefulã€‚Mã€‚But the representation is not purposeableã€‚ but
    the type should be a distill belt tokenizer fastã€‚å—¯ã€‚And if I had used a bird checkpointã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªè’¸é¦é¸Ÿæ¨¡å‹ã€‚æ‰€ä»¥tokenizerå°†æ˜¯ä¸€ä¸ªè’¸é¦é¸Ÿtokenizerï¼Œå¯ä»¥é€šè¿‡æ·»åŠ æ§åˆ¶å°è¿›è¡ŒåŒé‡æ£€æŸ¥ã€‚å¦‚æœæˆ‘è¾“å…¥tokenizerå¹¶æ‰“å°è¾“å‡ºï¼Œå®ƒä¼šå‘Šè¯‰æˆ‘è¿™æ˜¯ä¸€ä¸ªå‡tokenizer
    fastï¼Œè¿™å¹¶ä¸æ˜¯ç‰¹åˆ«æœ‰ç”¨ã€‚ä½†è¡¨ç¤ºå½¢å¼ä¸æ˜¯å¯ç”¨çš„ï¼Œç±»å‹åº”è¯¥æ˜¯è’¸é¦é¸Ÿtokenizer fastã€‚å¦‚æœæˆ‘ä½¿ç”¨äº†é¸Ÿcheckpointã€‚
- en: I would have a bird tokenizer fastï¼Œ if I had usedï¼Œ I don't know about checkpointã€‚
    I would have a badt tokenizer fastï¼Œ etc cea etcetteraã€‚ so the U in autotokenazizer
    means that that class is going to pick the right so class of tokenazizer so when
    corresponding to the model used by your checkpoint automatically so your code
    here tokenazizer equals auto tokenizer that from between checkpoint is going to
    work from for any checkpoint on the model whatever the class of your model as
    long as it's a class of model that has been implemented in transformers it's going
    to work on itã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä½¿ç”¨äº†é¸Ÿtokenizerï¼Œæˆ‘ä¼šæœ‰ä¸€ä¸ªå¿«é€Ÿçš„tokenizerï¼Œä¸çŸ¥é“å…³äºcheckpointçš„äº‹ã€‚æˆ‘ä¼šæœ‰ä¸€ä¸ªç³Ÿç³•çš„å¿«é€Ÿtokenizerç­‰ç­‰ã€‚æ‰€ä»¥è‡ªåŠ¨tokenizerä¸­çš„Uæ„å‘³ç€è¯¥ç±»å°†è‡ªåŠ¨é€‰æ‹©ä¸ä½¿ç”¨çš„checkpointå¯¹åº”çš„æ­£ç¡®tokenizerå­ç±»ã€‚å› æ­¤ï¼Œä½ çš„ä»£ç è¿™é‡Œtokenizerç­‰äºauto
    tokenizerï¼Œè¿™å°†é€‚ç”¨äºä»»ä½•checkpointæ¨¡å‹ï¼Œåªè¦å®ƒæ˜¯transformersä¸­å®ç°çš„æ¨¡å‹ç±»ã€‚
- en: Whereas if you were using here Distill B tokenizerã€‚ you would have to change
    the class used if you change the type of checkpointï¼Œ for instanceã€‚ if you would
    use the belt modelï¼Œ you would have to change it to bird tokenizer if you were
    using a GT2 checkpointã€‚ you would need to change it to GT2 tokenizerï¼Œ etcaï¼Œ etcaã€‚å—¯ã€‚The
    second question wasã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œå¦‚æœä½ åœ¨è¿™é‡Œä½¿ç”¨Distill B tokenizerï¼Œæ”¹å˜checkpointç±»å‹æ—¶ï¼Œä½ å°±éœ€è¦æ›´æ”¹ä½¿ç”¨çš„ç±»ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯beltæ¨¡å‹ï¼Œä½ éœ€è¦å°†å…¶æ›´æ”¹ä¸ºé¸Ÿtokenizerï¼›å¦‚æœä½ ä½¿ç”¨GT2
    checkpointï¼Œåˆ™éœ€è¦æ›´æ”¹ä¸ºGT2 tokenizerï¼Œç­‰ç­‰ã€‚
- en: will I be broken into I and M and the answer to that is yesï¼Œ we'll see a little
    bitã€‚ I'm just going to push an answer more completely a little bit later when
    we see exactly the different type of tokenizerã€‚And what is the difference between
    a fast tokenizer and a standard tokenizer Very good question So we usually have
    for each model two tokens one that is called slow or standout and the ver that
    is called fast the fast tokenizer is backed by the cookingface tokenizer library
    which is not written in Python but in rust and which in turn because you may have
    known that Python is a slow language and so if you do the wall tokenization in
    pure Python it can be a bit slow when you have lots of lots of text whereas the
    tokenizer fast backed by rust is going to be extremely fastã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šè¢«åˆ†è§£æˆIå’ŒMå—ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°ä¸€ç‚¹ã€‚æˆ‘å°†ç¨åæ›´å®Œæ•´åœ°æ¨é€ç­”æ¡ˆï¼Œå±Šæ—¶æˆ‘ä»¬ä¼šçœ‹åˆ°ä¸åŒç±»å‹çš„tokenizerã€‚å¿«é€Ÿtokenizerå’Œæ ‡å‡†tokenizerä¹‹é—´æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿéå¸¸å¥½çš„é—®é¢˜ã€‚æ¯ä¸ªæ¨¡å‹é€šå¸¸æœ‰ä¸¤ä¸ªtokenï¼Œä¸€ä¸ªå«æ…¢tokenæˆ–çªå‡ºtokenï¼Œå¦ä¸€ä¸ªå«å¿«é€Ÿtokenã€‚å¿«é€ŸtokenizeråŸºäºcookingface
    tokenizeråº“ï¼Œå®ƒä¸æ˜¯ç”¨Pythonç¼–å†™çš„ï¼Œè€Œæ˜¯ç”¨Rustç¼–å†™çš„ã€‚å› ä¸ºä½ å¯èƒ½çŸ¥é“ï¼ŒPythonæ˜¯ä¸€ç§è¾ƒæ…¢çš„è¯­è¨€ï¼Œæ‰€ä»¥å¦‚æœåœ¨çº¯Pythonä¸­è¿›è¡Œæ•´ä¸ªtokenizationï¼Œå½“æ–‡æœ¬é‡éå¸¸å¤§æ—¶ï¼Œä¼šç¨æ˜¾ç¼“æ…¢ï¼Œè€ŒåŸºäºRustçš„å¿«é€Ÿtokenizeråˆ™ä¼šéå¸¸å¿«é€Ÿã€‚
- en: So the main differenceï¼Œ that's the main difference between the two of them if
    you are just processing one textã€‚ you won't see any differenceï¼Œ but if you're
    processing 10000 of texts at the same time as the tokenator fast is going to be
    much faster than the python tokenizerã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦åŒºåˆ«åœ¨äºï¼Œå¦‚æœä½ åªæ˜¯å¤„ç†ä¸€æ®µæ–‡æœ¬ï¼Œä½ ä¸ä¼šçœ‹åˆ°ä»»ä½•å·®åˆ«ï¼Œä½†å¦‚æœä½ åŒæ—¶å¤„ç†10000æ®µæ–‡æœ¬ï¼Œå¿«é€Ÿtokenizerå°†æ¯”Python tokenizerå¿«å¾—å¤šã€‚
- en: And that's also the question we have for the momentã€‚ I'm going to continue and
    look at the modelã€‚So the same way we have nout to kal APIï¼Œ we have a newto modelal
    API and again the auto in automod API means that this class is going to pick the
    right subclass belt modelã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæ˜¯ç›®å‰æˆ‘ä»¬çš„é—®é¢˜ã€‚æˆ‘å°†ç»§ç»­å¹¶æŸ¥çœ‹æ¨¡å‹ã€‚ä¸æˆ‘ä»¬æœ‰nout to kal APIçš„æ–¹å¼ä¸€æ ·ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ–°æ¨¡å‹APIï¼ŒåŒæ ·ï¼Œautomod APIä¸­çš„autoæ„å‘³ç€è¿™ä¸ªç±»å°†é€‰æ‹©æ­£ç¡®çš„å­ç±»æ¨¡å‹ã€‚
- en: GT2 modelï¼Œ distill belt modelï¼Œ etc depending on the checkpoint that it receives
    so here since it's a dist belt checkpoint it's gonnaã€‚I would put distill button
    modelï¼Œ I can just show you hereã€‚Ma that modelï¼Œ and it shouldã€‚I have a nice wrap
    we can see hereï¼Œ this steel bird bottleã€‚å—¯ã€‚I'm gonna to remove that cell because
    it's a bit annoyingã€‚Thank you for thatã€‚å—¯ã€‚Soï¼Œ that modelã€‚Isã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GT2æ¨¡å‹ã€è’¸é¦æ¨¡å‹ç­‰ï¼Œå…·ä½“å–å†³äºæ¥æ”¶åˆ°çš„æ£€æŸ¥ç‚¹ï¼Œå› æ­¤è¿™é‡Œç”±äºæ˜¯è’¸é¦æ£€æŸ¥ç‚¹ï¼Œå®ƒä¼šã€‚æˆ‘ä¼šæ”¾ç½®è’¸é¦æ¨¡å‹ï¼Œæˆ‘å¯ä»¥åœ¨è¿™é‡Œç»™ä½ å±•ç¤ºã€‚é‚£ä¸ªæ¨¡å‹ï¼Œå®ƒåº”è¯¥ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªè’¸é¦æ¨¡å‹ã€‚å—¯ã€‚æˆ‘ä¼šåˆ é™¤é‚£ä¸ªå•å…ƒï¼Œå› ä¸ºæœ‰ç‚¹çƒ¦äººã€‚è°¢è°¢ä½ ã€‚å—¯ã€‚æ‰€ä»¥ï¼Œé‚£ä¸ªæ¨¡å‹ã€‚
- en: Coming with againï¼Œ we didn't download any file here because when we executed
    the pipeline instruction at the very beginningã€‚ we downloaded everything we neededã€‚
    so the model is already cachedã€‚ that's why we don't see any download here and
    we get the warning which I'm going explain just after thisã€‚ which is becauseã€‚This
    automod class is going to give us the base pretrained modelã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡è¯´æ˜ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¸‹è½½ä»»ä½•æ–‡ä»¶ï¼Œå› ä¸ºå½“æˆ‘ä»¬åœ¨ä¸€å¼€å§‹æ‰§è¡Œç®¡é“æŒ‡ä»¤æ—¶ï¼Œæˆ‘ä»¬ä¸‹è½½äº†æ‰€éœ€çš„ä¸€åˆ‡ã€‚æ‰€ä»¥æ¨¡å‹å·²ç»è¢«ç¼“å­˜ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹ä¸åˆ°ä»»ä½•ä¸‹è½½ï¼Œä»¥åŠæˆ‘å°†è¦è§£é‡Šçš„è­¦å‘Šï¼ŒåŸå› æ˜¯è¿™ä¸ªautomodç±»å°†ç»™æˆ‘ä»¬æä¾›åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ã€‚
- en: and this space pretrained model doesn't output classification of a sentence
    between positive and negativeã€‚ it outputs the hidden state of the pretrained modelï¼Œ
    which is a dimension of 168ã€‚So that's why here we have a warning because that
    model auto model is missing a classification head and as the warning was sayingã€‚
    some weights of the checkpoint were not used when initializing the modelã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç©ºé—´é¢„è®­ç»ƒæ¨¡å‹ä¸è¾“å‡ºå¥å­çš„æ­£è´Ÿåˆ†ç±»ã€‚å®ƒè¾“å‡ºçš„æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„éšè—çŠ¶æ€ï¼Œç»´åº¦ä¸º168ã€‚æ‰€ä»¥è¿™é‡Œæœ‰ä¸€ä¸ªè­¦å‘Šï¼Œå› ä¸ºè¯¥æ¨¡å‹çš„è‡ªåŠ¨æ¨¡å‹ç¼ºå°‘åˆ†ç±»å¤´ï¼Œæ­£å¦‚è­¦å‘Šæ‰€è¯´ï¼ŒæŸäº›æ£€æŸ¥ç‚¹çš„æƒé‡åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶æœªè¢«ä½¿ç”¨ã€‚
- en: specifically classifier biasï¼Œ classifier weightsï¼Œ etcaã€‚ which are all the weights
    of the classifier headã€‚So Automod is something that can be useful if you just
    want the tensor or hidden features outputted by your pre modelã€‚ but here we want
    to classify our sentences between positive and negative so we need a model with
    a classification head and that's given to us by the automod for sequence classification
    classã€‚And this oneï¼Œ when I execute the cell is not going to output any warninging
    because all the weights are going to be used and there is not going to be anything
    problematic when you dig the checkpointã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯åˆ†ç±»å™¨åç½®ã€åˆ†ç±»å™¨æƒé‡ç­‰ï¼Œè¿™äº›éƒ½æ˜¯åˆ†ç±»å¤´çš„æ‰€æœ‰æƒé‡ã€‚å› æ­¤ï¼Œå¦‚æœä½ åªæƒ³è·å–é¢„æ¨¡å‹è¾“å‡ºçš„å¼ é‡æˆ–éšè—ç‰¹å¾ï¼ŒAutomodæ˜¯æœ‰ç”¨çš„ï¼Œä½†æˆ‘ä»¬æƒ³å°†å¥å­åˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¸¦æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹ï¼Œè€Œè¿™æ­£æ˜¯ç”±ç”¨äºåºåˆ—åˆ†ç±»çš„automodæä¾›çš„ã€‚å½“æˆ‘æ‰§è¡Œè¿™ä¸ªå•å…ƒæ—¶ï¼Œä¸ä¼šè¾“å‡ºä»»ä½•è­¦å‘Šï¼Œå› ä¸ºæ‰€æœ‰æƒé‡éƒ½ä¼šè¢«ä½¿ç”¨ï¼Œæ£€æŸ¥ç‚¹æ—¶ä¸ä¼šå‡ºç°ä»»ä½•é—®é¢˜ã€‚
- en: And if weã€‚G our input to thatmo and look at the shape and we can see that it's
    going to be a tons of size 2 by 2ã€‚One little comment about the output is that
    the output of transformer modelsã€‚ so that was the thing that were imported at
    the beginning of our modeling fileã€‚ if you remember like we had a lot of import
    from the the dot model outputsã€‚Mouleã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†è¾“å…¥ä¼ é€’ç»™è¯¥æ¨¡å‹å¹¶æŸ¥çœ‹å½¢çŠ¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒå°†æ˜¯ä¸€ä¸ª2x2çš„å¤§å°ã€‚å…³äºè¾“å‡ºçš„å°è¯„è®ºæ˜¯ï¼Œå˜æ¢å™¨æ¨¡å‹çš„è¾“å‡ºã€‚é‚£æ˜¯æˆ‘ä»¬åœ¨å»ºæ¨¡æ–‡ä»¶å¼€å¤´å¯¼å…¥çš„å†…å®¹ã€‚å¦‚æœä½ è¿˜è®°å¾—ï¼Œæˆ‘ä»¬ä»æ¨¡å‹è¾“å‡ºä¸­è¿›è¡Œäº†å¾ˆå¤šå¯¼å…¥ã€‚
- en: So those outputs are a bit of an hybrid between the name to and the dictionary
    so you can access everything either by doing dot like this So output dot logitsã€‚
    you can also access thingsã€‚Byã€‚å—¯ã€‚Askingã€‚With the keyã€‚Soï¼Œ outputsã€‚And when we ask
    for the logicit keyã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›è¾“å‡ºåœ¨åç§°å’Œå­—å…¸ä¹‹é—´æœ‰ç‚¹æ··åˆï¼Œå› æ­¤ä½ å¯ä»¥é€šè¿‡è¿™æ ·åšæ¥è®¿é—®ä¸€åˆ‡ï¼Œå³è¾“å‡ºç‚¹logitsã€‚ä½ ä¹Ÿå¯ä»¥é€šè¿‡é”®æ¥è®¿é—®ã€‚å—¯ã€‚é€šè¿‡é—®ã€‚ä½¿ç”¨é”®ã€‚æ‰€ä»¥ï¼Œè¾“å‡ºã€‚å½“æˆ‘ä»¬è¯·æ±‚é€»è¾‘å¯†é’¥æ—¶ã€‚
- en: like it would if like a dictionaryï¼Œ which works as wellã€‚å—¯ã€‚And if we just ask
    for the base representationï¼Œ we can see it's a sequence classifier output which
    contain Lu sheet and thistensorã€‚ so here it contains only one thing but most of
    the transformer model can return lots of things as outputã€‚ for instance if I added
    labels here it would return a lossã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå­—å…¸ä¸€æ ·ï¼Œè¿™ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œã€‚å—¯ã€‚å¦‚æœæˆ‘ä»¬åªæ˜¯è¯·æ±‚åŸºæœ¬è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ˜¯ä¸€ä¸ªåºåˆ—åˆ†ç±»å™¨çš„è¾“å‡ºï¼ŒåŒ…å«Luè¡¨å’Œè¿™ä¸ªå¼ é‡ã€‚å› æ­¤è¿™é‡ŒåªåŒ…å«ä¸€é¡¹ï¼Œä½†å¤§å¤šæ•°å˜æ¢å™¨æ¨¡å‹å¯ä»¥è¿”å›è®¸å¤šè¾“å‡ºã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘åœ¨è¿™é‡Œæ·»åŠ æ ‡ç­¾ï¼Œå®ƒå°†è¿”å›æŸå¤±ã€‚
- en: we could also ask the model to return all the hiddenden states or all the attentions
    results and in which case our output is becoming a bit quotedã€‚ which is why it's
    organized as this spec class that behaves likeedt and an imableã€‚So those legitsã€‚
    because those names of what we get are numbers which appear a bit randomã€‚ they
    don't really look like probabilities and we will need to do one last step of post
    processing as we saw in the video to convert them into probabilitiesã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥è¦æ±‚æ¨¡å‹è¿”å›æ‰€æœ‰éšè—çŠ¶æ€æˆ–æ‰€æœ‰æ³¨æ„åŠ›ç»“æœï¼Œè¿™æ ·æˆ‘ä»¬çš„è¾“å‡ºå°±å˜å¾—æœ‰äº›å†—é•¿ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒè¢«ç»„ç»‡ä¸ºè¿™ç§åƒ EDT å’Œä¸€ä¸ª IMable çš„è§„æ ¼ç±»ã€‚æ‰€ä»¥é‚£äº›åˆæ³•æ€§ã€‚å› ä¸ºæˆ‘ä»¬å¾—åˆ°çš„åç§°æ˜¯ä¸€äº›çœ‹èµ·æ¥æœ‰äº›éšæœºçš„æ•°å­—ã€‚å®ƒä»¬å¹¶ä¸çœŸæ­£åƒæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œæœ€åä¸€æ­¥åå¤„ç†ï¼Œå¦‚è§†é¢‘ä¸­æ‰€ç¤ºï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºæ¦‚ç‡ã€‚
- en: that is apply the surfmaxã€‚And if we just import from torturer soft max function
    plateã€‚ we can see that now we get the exact same score that we had at the beginningã€‚
    so for instance for the second sentence like we have that 0ã€‚99 so 99ã€‚95% and if
    we look back at the result of the pipeline we can see that we had 99ã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åº”ç”¨ softmaxã€‚å¦‚æœæˆ‘ä»¬åªæ˜¯ä» `torturer` å¯¼å…¥ softmax å‡½æ•°æ¿å—ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨å¾—åˆ°çš„åˆ†æ•°ä¸ä¸€å¼€å§‹çš„å®Œå…¨ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¯¹äºç¬¬äºŒä¸ªå¥å­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†
    0.99ï¼Œå³ 99.95%ã€‚å¦‚æœæˆ‘ä»¬å›é¡¾ç®¡é“çš„ç»“æœï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°äº† 99ã€‚
- en: 945 yeah so the exact same scoresã€‚And to know which which one was the negative
    and which one was positively labelã€‚The pipeline is using that field from the model
    configurationï¼Œ so for each modelã€‚ the configuration file associated to it is accessible
    via the configure attributebute and the ID2 level field contains a correspondence
    between integers and levelsã€‚So let's see if we have any questionsã€‚å“å‘€ã€‚First question
    isã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œç¡®åˆ‡çš„åˆ†æ•°ã€‚æ‰€ä»¥è¦çŸ¥é“å“ªä¸ªæ˜¯è´Ÿé¢æ ‡ç­¾ï¼Œå“ªä¸ªæ˜¯æ­£é¢æ ‡ç­¾ã€‚ç®¡é“ä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„é‚£ä¸ªå­—æ®µï¼Œå› æ­¤å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œå…³è”çš„é…ç½®æ–‡ä»¶å¯ä»¥é€šè¿‡é…ç½®å±æ€§è®¿é—®ï¼ŒID2çº§å­—æ®µåŒ…å«æ•´æ•°ä¸çº§åˆ«ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚é‚£æˆ‘ä»¬çœ‹çœ‹æœ‰æ²¡æœ‰é—®é¢˜ã€‚å“å‘€ï¼Œç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ã€‚
- en: is there any reason to use the standard Python to canï¼Ÿ
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ ‡å‡† Python æœ‰ä»€ä¹ˆç†ç”±å—ï¼Ÿ
- en: I work at a game phase so I'm a little bit biased and I'm going to say no you
    have to using the fast tokenser is always going be betterã€‚ so it's going to be
    even if you don't have many many textã€‚ it's going to be at the same speed at the
    very minimum maybe faster than the Python tokenazer standardt Python tokenneerã€‚
    but also it has many more featuresï¼Œ so we'll look at them in the second part of
    the course mainly but it has features that have been designed specifically for
    tasks like like token classification or question answering that allow you to know
    for instanceã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æ¸¸æˆé˜¶æ®µå·¥ä½œï¼Œæ‰€ä»¥æˆ‘æœ‰ç‚¹åè§ï¼Œæˆ‘è¦è¯´ä¸ï¼Œä½¿ç”¨å¿«é€Ÿçš„åˆ†è¯å™¨æ€»æ˜¯æ›´å¥½ã€‚æ‰€ä»¥å³ä½¿ä½ æ²¡æœ‰å¾ˆå¤šæ–‡æœ¬ï¼Œå®ƒçš„é€Ÿåº¦ä¹Ÿè‡³å°‘ä¸æ ‡å‡† Python åˆ†è¯å™¨ç›¸åŒï¼Œç”šè‡³å¯èƒ½æ›´å¿«ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æœ‰æ›´å¤šåŠŸèƒ½ï¼Œæˆ‘ä»¬å°†åœ¨è¯¾ç¨‹çš„ç¬¬äºŒéƒ¨åˆ†ä¸»è¦è®¨è®ºè¿™äº›åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½ä¸“é—¨ä¸ºåƒæ ‡è®°åˆ†ç±»æˆ–é—®ç­”è¿™æ ·çš„ä»»åŠ¡è®¾è®¡ï¼Œè®©ä½ çŸ¥é“ä¾‹å¦‚ã€‚
- en: if from which word the token comes from or to exactly which span of text both
    tokens represent in the original text which fit that are a little bit not a little
    bit way out together to get with the slow tokenerã€‚Another question isã€‚Are there
    any similar tutorials or resources for sentiment analysis for multilabels that
    or regression tasks for sequences not right now which is that's a very good question
    so not right now and we should definitely work on that so the main thing you would
    have to change is the processing at the end instead of applying softmax you would
    apply for a regression you wouldn't apply anything I guess and then for multiplelabel
    so multiple label multiple possible labels for each of your sentence you would
    apply probably a stingoid to your resultã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: token æ¥æºäºå“ªä¸ªå•è¯ï¼Œæˆ–ä¸¤ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­åˆ†åˆ«ä»£è¡¨å“ªä¸ªæ–‡æœ¬è·¨åº¦ï¼Œè¿™äº›åŠŸèƒ½çš„è®¾è®¡æ˜¯ä¸ºäº†å…‹æœæ…¢é€Ÿåˆ†è¯å™¨çš„ä¸€äº›é™åˆ¶ã€‚å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œæœ‰æ²¡æœ‰ç±»ä¼¼çš„æ•™ç¨‹æˆ–èµ„æºç”¨äºå¤šæ ‡ç­¾æƒ…æ„Ÿåˆ†ææˆ–åºåˆ—çš„å›å½’ä»»åŠ¡ï¼Ÿç°åœ¨æ²¡æœ‰ï¼Œè¿™æ˜¯ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼Œæ‰€ä»¥ç°åœ¨æ²¡æœ‰ï¼Œæˆ‘ä»¬ç¡®å®åº”è¯¥å¯¹æ­¤è¿›è¡Œä¸€äº›å·¥ä½œã€‚ä½ éœ€è¦æ”¹å˜çš„ä¸»è¦å†…å®¹æ˜¯æœ€åçš„å¤„ç†ï¼Œè€Œä¸æ˜¯åº”ç”¨
    softmaxï¼Œå¯¹äºå›å½’æ¥è¯´ï¼Œä½ å¯èƒ½ä¸éœ€è¦åº”ç”¨ä»»ä½•ä¸œè¥¿ã€‚å¯¹äºå¤šæ ‡ç­¾ï¼Œæ¯ä¸ªå¥å­å¯èƒ½æœ‰å¤šä¸ªæ ‡ç­¾ï¼Œä½ å¯èƒ½éœ€è¦å¯¹ç»“æœåº”ç”¨ä¸€ä¸ª `sigmoid`ã€‚
- en: And then last question is the first two input IDï¼Œ the two lines are the sameã€‚
    but the words are not why is that so we have to look back at the decoding because
    the two words are indeed the same so the sentence hereã€‚Begins with 101 and 101
    which is the idea that CSOã€‚ so that's why you have the first two input id that
    the same and then the 1045 correspond to the I because the two sentences begin
    with I and then it starts being different because you have the ver or8 for the
    two sentencesã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªé—®é¢˜æ˜¯å‰ä¸¤ä¸ªè¾“å…¥IDï¼Œè¿™ä¸¤è¡Œæ˜¯ç›¸åŒçš„ï¼Œä½†å•è¯ä¸åŒï¼Œä¸ºä»€ä¹ˆä¼šè¿™æ ·å‘¢ï¼Ÿæˆ‘ä»¬éœ€è¦å›å¤´çœ‹çœ‹è§£ç ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªè¯ç¡®å®ç›¸åŒï¼Œå› æ­¤è¿™é‡Œçš„å¥å­ä»¥101å’Œ101å¼€å§‹ï¼Œè¿™æ˜¯CSOçš„æ¦‚å¿µã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ æœ‰å‰ä¸¤ä¸ªè¾“å…¥IDç›¸åŒï¼Œç„¶å1045å¯¹åº”äºIï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå¥å­éƒ½ä»¥Iå¼€å¤´ï¼Œç„¶åå¼€å§‹ä¸åŒï¼Œå› ä¸ºä½ æœ‰ä¸¤ä¸ªå¥å­çš„veræˆ–8ã€‚
- en: And another question would it be possible to do a video explaining the code
    structure of the library and the ID behind it so that's to maybe make it easier
    to contribute veryã€‚ very good question and it's actually scheduled for the last
    part of the course in the part of the course we have a chapter that's going to
    be dedicated to how to contribute to againface libraries and particular the transformformers
    library and then well have video explaining the good structure of all the libraries
    of the again phase ecosystemã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œæ˜¯å¦æœ‰å¯èƒ½åšä¸€ä¸ªè§†é¢‘ï¼Œè§£é‡Šåº“çš„ä»£ç ç»“æ„å’ŒèƒŒåçš„IDï¼Œè¿™æ ·å¯èƒ½ä¼šæ›´å®¹æ˜“è´¡çŒ®ï¼Ÿéå¸¸å¥½çš„é—®é¢˜ï¼Œå®é™…ä¸Šè¿™å·²å®‰æ’åœ¨è¯¾ç¨‹çš„æœ€åéƒ¨åˆ†ï¼Œåœ¨è¯¾ç¨‹çš„ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç« èŠ‚å°†ä¸“é—¨è®¨è®ºå¦‚ä½•ä¸ºagainfaceåº“è´¡çŒ®ï¼Œç‰¹åˆ«æ˜¯transformersåº“ï¼Œç„¶åæˆ‘ä»¬å°†æœ‰è§†é¢‘è§£é‡Šagain
    phaseç”Ÿæ€ç³»ç»Ÿä¸­æ‰€æœ‰åº“çš„è‰¯å¥½ç»“æ„ã€‚
- en: Againï¼Œ don't hesitate to ask any questions I'm going to pose regularly to answer
    itã€‚So that's pretty much everything that was behind the pipeline and we've seen
    it in detail in the codeã€‚ So now let's have a look at the main object inside the
    pipelineï¼Œ which is the modelã€‚So againã€‚ we have a short video that I'm going to
    show from my computer and then we'll look at the code in detail and if you have
    questionsã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œè¯·ä¸è¦çŠ¹è±«ï¼Œéšæ—¶æé—®ï¼Œæˆ‘å°†å®šæœŸæå‡ºé—®é¢˜æ¥å›ç­”ã€‚æ‰€ä»¥ï¼Œè¿™å°±æ˜¯ç®¡é“èƒŒåçš„æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬åœ¨ä»£ç ä¸­è¯¦ç»†çœ‹è¿‡äº†ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç®¡é“å†…çš„ä¸»è¦å¯¹è±¡ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªçŸ­è§†é¢‘ï¼Œæˆ‘å°†ä»æˆ‘çš„ç”µè„‘ä¸Šå±•ç¤ºï¼Œç„¶åæˆ‘ä»¬å°†è¯¦ç»†æŸ¥çœ‹ä»£ç ï¼Œå¦‚æœä½ æœ‰é—®é¢˜ã€‚
- en: I can answer them and live code with youã€‚Let me just grab the videoã€‚å‘ƒã€‚Which
    of courseã€‚ I can't find easily otherwiseï¼Œ but would be too easyã€‚ Why did it disappearã€‚And
    one was ver poormissã€‚ I just moved itã€‚Recentlyã€‚![](img/0e882caa799deede0a88d67b58737bfe_19.png)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥å›ç­”è¿™äº›é—®é¢˜å¹¶ä¸ä½ å®æ—¶ç¼–ç¨‹ã€‚è®©æˆ‘å…ˆæ‰¾ä¸€ä¸‹è§†é¢‘ã€‚å‘ƒï¼Œå½“ç„¶ï¼Œæˆ‘æ‰¾ä¸åˆ°å¾ˆå®¹æ˜“ï¼Œä¸ç„¶å¤ªç®€å•äº†ã€‚ä¸ºä»€ä¹ˆå®ƒæ¶ˆå¤±äº†ã€‚è¿˜æœ‰ä¸€ä¸ªæ˜¯ver poormissã€‚æˆ‘åˆšåˆšç§»åŠ¨äº†å®ƒã€‚æœ€è¿‘ã€‚![](img/0e882caa799deede0a88d67b58737bfe_19.png)
- en: Andã€‚![](img/0e882caa799deede0a88d67b58737bfe_21.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”ã€‚![](img/0e882caa799deede0a88d67b58737bfe_21.png)
- en: How to instantiate a transforms modelã€‚In this videoã€‚ we'll look at how we can
    create and use the model from the Transformers libraryã€‚As we' seen beforeã€‚ the
    Automodal class allows you to instantiate a portrayed model from any checkpoint
    on the I face appã€‚It will pick the right model class from the library to instant
    shade the proper architecture and load the weights of the preed model insideã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•å®ä¾‹åŒ–å˜æ¢æ¨¡å‹ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•åˆ›å»ºå’Œä½¿ç”¨æ¥è‡ªTransformersåº“çš„æ¨¡å‹ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼ŒAutomodalç±»å…è®¸ä½ ä»I faceåº”ç”¨ä¸­çš„ä»»ä½•æ£€æŸ¥ç‚¹å®ä¾‹åŒ–ä¸€ä¸ªè¡¨ç°æ¨¡å‹ã€‚å®ƒä¼šä»åº“ä¸­é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç±»ä»¥å³æ—¶è°ƒæ•´æ­£ç¡®çš„æ¶æ„ï¼Œå¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ã€‚
- en: As we can seeï¼Œ when given a bird checkpointï¼Œ we end up with a bird model and
    similarly for GPT2 or partã€‚Beyond the scenesï¼Œ this APII can take the name of a
    checkpoint on the Uã€‚ in which case it will download and cache the configuration
    file as well as the model weights fileã€‚You can also specify the path to a local
    folder that contains a valid configuration file and a model of waste fileã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå½“ç»™å®šä¸€ä¸ªbirdæ£€æŸ¥ç‚¹æ—¶ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªbirdæ¨¡å‹ï¼Œå¯¹äºGPT2æˆ–partä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨å¹•åï¼Œè¿™ä¸ªAPIå¯ä»¥æ¥å—Uä¸Šçš„æ£€æŸ¥ç‚¹åç§°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†ä¸‹è½½å¹¶ç¼“å­˜é…ç½®æ–‡ä»¶ä»¥åŠæ¨¡å‹æƒé‡æ–‡ä»¶ã€‚ä½ è¿˜å¯ä»¥æŒ‡å®šåŒ…å«æœ‰æ•ˆé…ç½®æ–‡ä»¶å’Œæ¨¡å‹æƒé‡æ–‡ä»¶çš„æœ¬åœ°æ–‡ä»¶å¤¹çš„è·¯å¾„ã€‚
- en: To instant shade the between modelï¼Œ the Automodal API will first open the configuration
    file to look at the configuration class that should be usedã€‚The configuration
    class depends on the type of the modelï¼Œ Bï¼Œ GPPT2ï¼Œ or Btï¼Œ for instanceã€‚Once it
    has a proper configuration classï¼Œ it can instantiate that configurationã€‚ which
    is a blueprint to know how to create the modelã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ä¾‹åŒ–betweenæ¨¡å‹ï¼ŒAutomodal APIå°†é¦–å…ˆæ‰“å¼€é…ç½®æ–‡ä»¶ï¼Œä»¥æŸ¥çœ‹åº”è¯¥ä½¿ç”¨çš„é…ç½®ç±»ã€‚é…ç½®ç±»å–å†³äºæ¨¡å‹çš„ç±»å‹ï¼Œæ¯”å¦‚Bï¼ŒGPPT2æˆ–Btç­‰ã€‚ä¸€æ—¦æœ‰äº†åˆé€‚çš„é…ç½®ç±»ï¼Œå°±å¯ä»¥å®ä¾‹åŒ–è¯¥é…ç½®ï¼Œè¿™æ˜¯ä¸€ç§äº†è§£å¦‚ä½•åˆ›å»ºæ¨¡å‹çš„è“å›¾ã€‚
- en: It also uses this configuration class to find the proper model classã€‚ which
    is when combined with the root configuration to load the modelã€‚This model is not
    yet a portraytrain modelï¼Œ as it has just been initialized with random weightsã€‚The
    last step is to load the weight from the model file inside this modelã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒè¿˜ä½¿ç”¨è¿™ä¸ªé…ç½®ç±»æ‰¾åˆ°åˆé€‚çš„æ¨¡å‹ç±»ï¼Œå½“ä¸æ ¹é…ç½®ç»“åˆæ—¶åŠ è½½æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹å°šæœªæ˜¯å¯è®­ç»ƒæ¨¡å‹ï¼Œå› ä¸ºå®ƒåªæ˜¯ç”¨éšæœºæƒé‡åˆå§‹åŒ–çš„ã€‚æœ€åä¸€æ­¥æ˜¯ä»æ¨¡å‹æ–‡ä»¶ä¸­åŠ è½½æƒé‡ã€‚
- en: To easily load the configuration of a model from any checkpoint or a folder
    containing the configuration fileã€‚ we can use the autoconfigug classã€‚Like the
    Automod classã€‚ it will pick the right configuration class from the libraryã€‚We
    can also use a specific class corresponding to a checkpointã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è½»æ¾ä»ä»»ä½•æ£€æŸ¥ç‚¹æˆ–åŒ…å«é…ç½®æ–‡ä»¶çš„æ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹çš„é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªåŠ¨é…ç½®ç±»ã€‚å°±åƒè‡ªåŠ¨æ¨¡å‹ç±»ä¸€æ ·ï¼Œå®ƒå°†ä»åº“ä¸­é€‰æ‹©æ­£ç¡®çš„é…ç½®ç±»ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸æ£€æŸ¥ç‚¹å¯¹åº”çš„ç‰¹å®šç±»ã€‚
- en: but well need to change your code each time we want to try a different model
    architectureã€‚As we said beforeï¼Œ the configuration of a model is a blueprint that
    contains all the information necessary to create the model architectureã€‚For instanceï¼Œ
    the bird model associated with a birth based case checkpoint as 12 layersã€‚ the
    hidden side of 768ã€‚And the vocabulary size of 28996ã€‚Once we add a configurationã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æ¯æ¬¡æˆ‘ä»¬æƒ³å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„æ—¶ï¼Œéƒ½éœ€è¦æ›´æ”¹ä½ çš„ä»£ç ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¯´ï¼Œæ¨¡å‹çš„é…ç½®æ˜¯ä¸€ä¸ªè“å›¾ï¼ŒåŒ…å«åˆ›å»ºæ¨¡å‹æ¶æ„æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œé¸Ÿç±»æ¨¡å‹ä¸åŸºäºå‡ºç”Ÿçš„æ¡ˆä¾‹æ£€æŸ¥ç‚¹ç›¸å…³è”ï¼Œå…·æœ‰12å±‚ï¼Œéšè—å±‚ä¸º768ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º28996ã€‚ä¸€æ—¦æˆ‘ä»¬æ·»åŠ äº†é…ç½®ã€‚
- en: we can create a model that has the same architecture as a checkpointï¼Œ but is
    randomly initializedã€‚We can vet training it from scratch like any by doch modelã€‚We
    can also change any part of the configuration by using keyword argumentsã€‚So sequence
    one sniet of codeï¼Œ instant sheets a randomly initialized B model with 10 layers
    instead of 12ã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªä¸æ£€æŸ¥ç‚¹å…·æœ‰ç›¸åŒæ¶æ„çš„æ¨¡å‹ï¼Œä½†å®ƒæ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚æˆ‘ä»¬å¯ä»¥åƒä»»ä½•æ ‡å‡†æ¨¡å‹ä¸€æ ·ï¼Œä»å¤´å¼€å§‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ä½¿ç”¨å…³é”®å­—å‚æ•°æ›´æ”¹é…ç½®çš„ä»»ä½•éƒ¨åˆ†ã€‚å› æ­¤ï¼Œåºåˆ—ä¸­çš„ä¸€æ®µä»£ç ï¼Œç¬é—´ç”Ÿæˆä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„Bæ¨¡å‹ï¼Œå±‚æ•°ä¸º10ï¼Œè€Œä¸æ˜¯12ã€‚
- en: Saving a model once its trend off fine is very easyã€‚We just have to use the
    safe between methodã€‚Hereã€‚ the model will be saved in a folder named My belt model
    inside the current working directoryã€‚Such a model can then be re using the from
    between methodã€‚To learn how to easily approach this model to the webï¼Œ check out
    the push to videoã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ¨¡å‹çš„è¶‹åŠ¿ç¨³å®šï¼Œä¿å­˜æ¨¡å‹éå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€ä½¿ç”¨ä¿å­˜æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨å½“å‰å·¥ä½œç›®å½•ä¸­çš„åä¸ºâ€œMy belt modelâ€çš„æ–‡ä»¶å¤¹ä¸­ã€‚è¿™æ ·çš„æ¨¡å‹å¯ä»¥é€šè¿‡ä½¿ç”¨ä»æ–‡ä»¶ä¸­åŠ è½½çš„æ–¹æ³•é‡æ–°ä½¿ç”¨ã€‚è¦äº†è§£å¦‚ä½•è½»æ¾å°†æ­¤æ¨¡å‹æ¨é€åˆ°ç½‘ç»œï¼Œè¯·æŸ¥çœ‹æ¨é€è§†é¢‘ã€‚
- en: '![](img/0e882caa799deede0a88d67b58737bfe_23.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_23.png)'
- en: å—¯ã€‚![](img/0e882caa799deede0a88d67b58737bfe_25.png)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚![](img/0e882caa799deede0a88d67b58737bfe_25.png)
- en: Okayï¼Œ so let's see if we have any questionsï¼Œ not just yetã€‚ Don't hesitate to
    ask any question in the chat and I'll answer themã€‚Regularlyï¼Œ and for thisã€‚ let's
    open the code appã€‚And look a little bit at the code behind the Automod APIã€‚And
    in particularã€‚ we'll seeï¼Œ for instanceï¼Œ I told you a little bit earlier that our
    model could return more venture surligit and it could returnã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜ï¼Œç›®å‰è¿˜æ²¡æœ‰ã€‚è¯·éšæ—¶åœ¨èŠå¤©ä¸­æé—®ï¼Œæˆ‘ä¼šå®šæœŸå›ç­”ã€‚ä¸ºäº†è¿™ä¸ªï¼Œæˆ‘ä»¬æ‰“å¼€ä»£ç åº”ç”¨ç¨‹åºï¼Œç¨å¾®çœ‹ä¸€ä¸‹Automod APIèƒŒåçš„ä»£ç ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œä¾‹å¦‚ï¼Œæˆ‘æ—©äº›æ—¶å€™å‘Šè¯‰è¿‡ä½ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥è¿”å›æ›´å¤šçš„æ½œåœ¨ç»“æœï¼Œå¹¶ä¸”å¯ä»¥è¿”å›ã€‚
- en: for instanceï¼Œ all the hidden states or things like that and we'll see how to
    do that just hereã€‚![](img/0e882caa799deede0a88d67b58737bfe_27.png)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ‰€æœ‰çš„éšè—çŠ¶æ€æˆ–ç±»ä¼¼çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œçœ‹åˆ°å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚![](img/0e882caa799deede0a88d67b58737bfe_27.png)
- en: Yesesã€‚![](img/0e882caa799deede0a88d67b58737bfe_29.png)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚![](img/0e882caa799deede0a88d67b58737bfe_29.png)
- en: Soã€‚If you had any questionsï¼Œ that would be an need all time because I didn't
    execute this notebook in advanceã€‚ so we need to wait for it to install everythingã€‚Okayï¼Œ
    that didn't think soã€‚å—¯ã€‚Soã€‚To create a random model that looks exactly like the
    per modelã€‚ we can just instantiate the default configuration and use that configuration
    inside the modelã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œé‚£å°†æ˜¯éå¸¸å¿…è¦çš„ï¼Œå› ä¸ºæˆ‘æ²¡æœ‰æå‰æ‰§è¡Œè¿™ä¸ªç¬”è®°æœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç­‰å®ƒå®‰è£…æ‰€æœ‰å†…å®¹ã€‚å¥½çš„ï¼Œæˆ‘ä¸è¿™æ ·è®¤ä¸ºã€‚å—¯ã€‚æ‰€ä»¥ï¼Œè¦åˆ›å»ºä¸€ä¸ªçœ‹èµ·æ¥ä¸ä¹‹å‰æ¨¡å‹å®Œå…¨ç›¸åŒçš„éšæœºæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€å®ä¾‹åŒ–é»˜è®¤é…ç½®ï¼Œå¹¶åœ¨æ¨¡å‹ä¸­ä½¿ç”¨è¯¥é…ç½®ã€‚
- en: Like we saw in the videoï¼Œ the config contains lots of fields that are related
    to what's happening inside the modelã€‚ so for instance we have the hidden size
    configuredã€‚ we have the number of words that our model can takenï¼Œ we have the
    vocabulary sizeã€‚The mobile typeã€‚ which is builtï¼Œ the activation it' usedï¼Œ which
    is Kiuï¼Œ etcteraï¼Œ etcaã€‚å—¯ã€‚Soï¼Œ that modelã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œé…ç½®åŒ…å«è®¸å¤šä¸æ¨¡å‹å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ç›¸å…³çš„å­—æ®µã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰é…ç½®å¥½çš„éšè—å±‚å¤§å°ï¼Œæˆ‘ä»¬æœ‰æ¨¡å‹å¯ä»¥æ¥å—çš„å•è¯æ•°é‡ï¼Œè¿˜æœ‰è¯æ±‡è¡¨å¤§å°ã€‚æ¨¡å‹ç±»å‹ï¼Œæ„å»ºæ—¶ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯Kiuï¼Œç­‰ç­‰ã€‚å—¯ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªæ¨¡å‹ã€‚
- en: Using just random usage the config is going to be randomly initialized and there
    is nothing to download there if weã€‚Want to use a pretrained model we have to use
    the from pretrained methodã€‚ which is going to download the exact config and then
    the model weights and as we saw in the videoã€‚ it's going to use the config to
    first instantiate randomly initialized model and then load the weights from that
    checkpoint inside the model we haveã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºé…ç½®æ—¶ï¼Œé…ç½®å°†è¢«éšæœºåˆå§‹åŒ–ï¼Œå¹¶ä¸”é‚£é‡Œæ²¡æœ‰ä»»ä½•ä¸‹è½½ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ `from pretrained` æ–¹æ³•ã€‚è¿™å°†ä¸‹è½½ç¡®åˆ‡çš„é…ç½®å’Œæ¨¡å‹æƒé‡ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œå®ƒå°†é¦–å…ˆä½¿ç”¨é…ç½®æ¥å®ä¾‹åŒ–ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ï¼Œç„¶åä»æˆ‘ä»¬æ‹¥æœ‰çš„é‚£ä¸ªæ£€æŸ¥ç‚¹åŠ è½½æƒé‡ã€‚
- en: And if weã€‚Want to change anythingã€‚ã„ã®ã‚‚ã®ã€‚More specifically in its configurationã€‚
    we can say it in several placesã€‚ Soï¼Œ for instanceï¼Œ we can start withã€‚å‘ƒã€‚A config
    that is exactly like birdsã€‚So con to convicted from pre trainededã€‚B pesquiistã€‚Which
    is gonna downloadï¼Œ and it's already downloaded from hereã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³è¦æ›´æ”¹ä»»ä½•å†…å®¹ã€‚æ›´å…·ä½“åœ°è¯´æ˜¯åœ¨å…¶é…ç½®ä¸­ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å¤šä¸ªåœ°æ–¹è¯´å‡ºæ¥ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸€ä¸ªé…ç½®å¼€å§‹ã€‚å‘ƒã€‚è¿™ä¸ªé…ç½®ä¸é¸Ÿç±»å®Œå…¨ç›¸åŒã€‚æˆ‘ä»¬è¦ä»é¢„è®­ç»ƒçš„B
    `pesquiist`ä¸­è·å–ï¼Œè¿™ä¸ªé…ç½®ä¼šä¸‹è½½ï¼Œè€Œä¸”è¿™é‡Œå·²ç»ä¸‹è½½è¿‡äº†ã€‚
- en: Where you mean the two configurefig is not definedã€‚ Oh yeahï¼Œ I have only use
    B configã€‚ So let's continue with thatã€‚Soã€‚B con from betweenã€‚ which is gonna reuse
    the con that was download hereã€‚ So this is the configuration of the per modelã€‚
    And if we want to change anything inside itï¼Œ we saw the videoï¼Œ for instanceã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ˜¯è¯´ `two configurefig` æ²¡æœ‰å®šä¹‰ã€‚å“¦ï¼Œæ˜¯çš„ï¼Œæˆ‘åªä½¿ç”¨äº†Bé…ç½®ã€‚é‚£æˆ‘ä»¬ç»§ç»­è¿™ä¸ªã€‚æ‰€ä»¥ï¼ŒBä» `between` æ¥çš„é…ç½®ï¼Œå°†ä¼šé‡ç”¨è¿™é‡Œä¸‹è½½çš„é…ç½®ã€‚è¿™æ˜¯æ¯ä¸ªæ¨¡å‹çš„é…ç½®ã€‚å¦‚æœæˆ‘ä»¬æƒ³æ›´æ”¹å…¶ä¸­çš„ä»»ä½•å†…å®¹ï¼Œæˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°äº†ï¼Œä¾‹å¦‚ã€‚
- en: to change certain number of hidden layersã€‚ but let's say thatã€‚I want to change
    the fact that I want my model to return all the hidden statesã€‚Which I would say
    with outputs in states equal toã€‚So I can do this in the config and then instant
    shape my model with model equal bad model configã€‚ I can also directly change this
    when I doã€‚That model that from betweentrained hereã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ”¹æŸäº›éšè—å±‚çš„æ•°é‡ã€‚ä½†å‡è®¾æˆ‘æƒ³æ›´æ”¹æˆ‘å¸Œæœ›æˆ‘çš„æ¨¡å‹è¿”å›æ‰€æœ‰éšè—çŠ¶æ€çš„äº‹å®ã€‚æˆ‘ä¼šç”¨ `outputs` çš„ `states` ç­‰äºæŸä¸ªå€¼æ¥å®ç°ã€‚æ‰€ä»¥æˆ‘å¯ä»¥åœ¨é…ç½®ä¸­åšåˆ°è¿™ä¸€ç‚¹ï¼Œç„¶åç”¨
    `model` ç­‰äº `bad model config` æ¥å®ä¾‹åŒ–æˆ‘çš„æ¨¡å‹ã€‚å½“æˆ‘æ‰§è¡Œä» `betweentrained` è¿™é‡Œçš„æ¨¡å‹æ—¶ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç›´æ¥æ›´æ”¹è¿™ä¸ªã€‚
- en: So since if I were to change here the number of hidden layers it wouldn't work
    anymoreã€‚ the command would fail because I would then try to load a checkpoints
    that has been defined with 12 layers inside the model with 10 layers so bytoch
    would complete I mean it would probably work but I would have a warning with like
    the widths not being used and the model would probably not get super useful results
    but for something like outputed in states which doesn't really change the way
    the model was pretrained this is going to work super nicelyã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå¦‚æœæˆ‘åœ¨è¿™é‡Œæ›´æ”¹éšè—å±‚çš„æ•°é‡ï¼Œå®ƒå°†ä¸å†æœ‰æ•ˆã€‚å‘½ä»¤ä¼šå¤±è´¥ï¼Œå› ä¸ºæˆ‘è¯•å›¾åŠ è½½ä¸€ä¸ªå®šä¹‰äº†12å±‚çš„æ£€æŸ¥ç‚¹ï¼Œè€Œæ¨¡å‹åªæœ‰10å±‚ã€‚æ‰€ä»¥`bytoch`å¯èƒ½ä¼šå·¥ä½œï¼Œä½†æˆ‘ä¼šæ”¶åˆ°è­¦å‘Šï¼Œæç¤ºæœªä½¿ç”¨çš„å®½åº¦ï¼Œæ¨¡å‹å¯èƒ½ä¸ä¼šå¾—åˆ°éå¸¸æœ‰ç”¨çš„ç»“æœã€‚ä¸è¿‡å¯¹äºåƒ
    `outputed in states` è¿™æ ·ä¸çœŸæ­£æ”¹å˜é¢„è®­ç»ƒæ¨¡å‹æ–¹å¼çš„å†…å®¹ï¼Œè¿™å°†éå¸¸æœ‰æ•ˆã€‚
- en: And if I try to take inputsã€‚So let's define some random input and then pass
    it to a tokenizerã€‚ So I'll have toã€‚Use the part toagonizer and then instantiate
    it with a form betweentrain methodã€‚Should ohï¼Œ yesï¼Œ it's finding it hereã€‚Should
    have executeded the elder or orã€‚So if I'm creating a do like thisã€‚And thenï¼Œ applying
    itã€‚å‘ƒã€‚To my inputsã€‚ğŸ¼Return a tensorã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘å°è¯•è¾“å…¥ä¸€äº›å†…å®¹ã€‚é‚£æˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›éšæœºè¾“å…¥ï¼Œç„¶åä¼ é€’ç»™åˆ†è¯å™¨ã€‚æˆ‘è¦ä½¿ç”¨éƒ¨åˆ†åˆ†è¯å™¨ï¼Œç„¶åç”¨ä¸€ä¸ªå½¢å¼çš„ `betweentrain` æ–¹æ³•å®ä¾‹åŒ–å®ƒã€‚å—¯ï¼Œåº”è¯¥æ˜¯è¿™é‡Œåœ¨æ‰¾åˆ°å®ƒã€‚åº”è¯¥æ‰§è¡Œå…ˆå‰çš„é‚£ä¸ªã€‚
- en: I don't need to put the padding and location that we saw beforeï¼Œ because there
    is only one sentenceã€‚ I'll see why a little bit earlierã€‚ So once I have done thatï¼Œ
    I can look at myã€‚Outputsã€‚And it should haveã€‚Nowï¼Œ two keysã€‚Still one keyã€‚YeahWith
    a little bit more because the bird model has a pull output you on top of the luggit
    but oh and it's not luets anymore sorry it's less hidden states because this is
    not the classification model it's a base model I used a be model which is the
    same as using automod not a be model for second classification so I get a last
    hidden states instead of look key the puller output is specific to B so it's always
    as that and then I can say have a last key with hidden states and a list of all
    the turnsult which correspond to all the hidden states of my model so this is
    how you change the configuration of your model on the fly either insides when
    we create a config if you are trying to initialize a randomly initialized model
    or to the form pretrain letter if you are trying to useã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸éœ€è¦æ”¾ç½®ä¹‹å‰çœ‹åˆ°çš„å¡«å……å’Œä½ç½®ï¼Œå› ä¸ºè¿™é‡Œåªæœ‰ä¸€å¥è¯ã€‚æˆ‘ç¨æ—©ä¸€ç‚¹ä¼šè§£é‡ŠåŸå› ã€‚æ‰€ä»¥ä¸€æ—¦æˆ‘å®Œæˆäº†è¿™ä¸€æ­¥ï¼Œæˆ‘å°±å¯ä»¥æŸ¥çœ‹æˆ‘çš„è¾“å‡ºã€‚ç°åœ¨åº”è¯¥æœ‰ä¸¤ä¸ªé”®ã€‚ä»ç„¶æ˜¯ä¸€ä¸ªé”®ã€‚æ˜¯çš„ï¼Œå¤šå‡ºäº†ä¸€ç‚¹ï¼Œå› ä¸ºé¸Ÿæ¨¡å‹åœ¨luggité¡¶éƒ¨æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œä½†å“¦ï¼Œä¸å†æ˜¯luetsäº†ï¼ŒæŠ±æ­‰ï¼Œéšè—çŠ¶æ€æ›´å°‘ï¼Œå› ä¸ºè¿™ä¸æ˜¯åˆ†ç±»æ¨¡å‹ï¼Œè€Œæ˜¯æˆ‘ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä½¿ç”¨çš„æ˜¯beæ¨¡å‹ï¼Œè¿™ä¸ä½¿ç”¨automodä¸ä¸€æ ·ï¼Œä¸æ˜¯ç”¨äºç¬¬äºŒæ¬¡åˆ†ç±»çš„beæ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘å¾—åˆ°çš„æ˜¯æœ€åçš„éšè—çŠ¶æ€ï¼Œè€Œä¸æ˜¯çœ‹é”®ï¼Œpullerè¾“å‡ºæ˜¯ç‰¹å®šäºBçš„ï¼Œæ‰€ä»¥å®ƒæ€»æ˜¯è¿™æ ·ï¼Œç„¶åæˆ‘å¯ä»¥è¯´æœ‰ä¸€ä¸ªæœ€åçš„é”®ï¼ŒåŒ…å«éšè—çŠ¶æ€å’Œä¸æˆ‘çš„æ¨¡å‹çš„æ‰€æœ‰éšè—çŠ¶æ€å¯¹åº”çš„ç»“æœåˆ—è¡¨ï¼Œè¿™å°±æ˜¯å¦‚ä½•åœ¨åˆ›å»ºé…ç½®æ—¶å®æ—¶æ›´æ”¹æ¨¡å‹é…ç½®çš„æ–¹å¼ï¼Œå¦‚æœä½ å°è¯•åˆå§‹åŒ–ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨é¢„è®­ç»ƒçš„ç‰ˆæœ¬ï¼Œå¦‚æœä½ å°è¯•ä½¿ç”¨ã€‚
- en: A pre model in particularï¼Œ if you're using a classification modelï¼Œ for instanceã€‚
    a sequence classification modelï¼Œ you can specify the most important argument is
    going to be nu levelsã€‚Because when you add your classification headï¼Œ you want
    to control how many outputs that classification head hassã€‚ so you would do that
    with a new labels argumentã€‚So that modelã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯å¦‚æœä½ ä½¿ç”¨çš„æ˜¯åˆ†ç±»æ¨¡å‹ï¼Œä¾‹å¦‚åºåˆ—åˆ†ç±»æ¨¡å‹ï¼Œä½ å¯ä»¥æŒ‡å®šæœ€é‡è¦çš„å‚æ•°ï¼Œå³nu levelsã€‚å› ä¸ºå½“ä½ æ·»åŠ åˆ†ç±»å¤´æ—¶ï¼Œä½ æƒ³æ§åˆ¶è¿™ä¸ªåˆ†ç±»å¤´æœ‰å¤šå°‘ä¸ªè¾“å‡ºã€‚æ‰€ä»¥ä½ å¯ä»¥é€šè¿‡new
    labelså‚æ•°æ¥å®ç°ã€‚è¿™æ ·æ¨¡å‹ã€‚
- en: And then once you finish training or a tuning your modelã€‚ you can use safe pretrain
    to save it on the floor on yourã€‚On your hard drive and you can use Pushtbã€‚ which
    we just released today actuallyã€‚So on your model to directly upload your model
    on the Higing face hub so that anyone in the world can use itã€‚Don't see any questions
    againï¼Œ don't hesitate to ask any questionsã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ å®Œæˆäº†è®­ç»ƒæˆ–å¾®è°ƒä½ çš„æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨safe pretrainå°†å…¶ä¿å­˜åœ¨ä½ çš„ç¡¬ç›˜ä¸Šï¼Œå¹¶ä¸”ä½ å¯ä»¥ä½¿ç”¨Pushtbã€‚æˆ‘ä»¬ä»Šå¤©åˆšå‘å¸ƒçš„å®é™…ä¸Šã€‚æ‰€ä»¥åœ¨ä½ çš„æ¨¡å‹ä¸Šç›´æ¥ä¸Šä¼ åˆ°Higing
    face hubï¼Œä»¥ä¾¿å…¨ä¸–ç•Œçš„äººéƒ½å¯ä»¥ä½¿ç”¨å®ƒã€‚å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æé—®ã€‚
- en: I'm going to answer them regularlyã€‚And so this is all we have toã€‚ this is all
    we various intersection for models and then let's look at tokenizerã€‚ which is
    responsible for prepoing the input I'm going to move myselfï¼Œ oh not to screenã€‚![](img/0e882caa799deede0a88d67b58737bfe_31.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šå®šæœŸå›ç­”è¿™äº›é—®é¢˜ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„ã€‚å…³äºæ¨¡å‹çš„å„ç§äº¤å‰ç‚¹ï¼Œç„¶åæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è´Ÿè´£å¤„ç†è¾“å…¥çš„æ ‡è®°å™¨ï¼Œæˆ‘å°†ç§»åŠ¨è‡ªå·±ï¼Œå“¦ï¼Œä¸è¦çœ‹å±å¹•ã€‚![](img/0e882caa799deede0a88d67b58737bfe_31.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_32.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_32.png)'
- en: Come backã€‚Going to move myselfã€‚Back on the leftã€‚Becauseã€‚And weã€‚ we look at this
    section here and look that the code inside the code appã€‚å—¯ã€‚So tokenizerã€‚Oh yeah
    let's look at the video with tokens of first and then I'll comment everything
    that's happening in this sectionã€‚T can as introduction videoã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å›æ¥å§ã€‚æˆ‘è¦ç§»åŠ¨è‡ªå·±ã€‚å›åˆ°å·¦è¾¹ã€‚å› ä¸ºã€‚æˆ‘ä»¬ã€‚æˆ‘ä»¬çœ‹çœ‹è¿™é‡Œè¿™ä¸€éƒ¨åˆ†ï¼Œå¹¶çœ‹çœ‹ä»£ç åº”ç”¨ä¸­çš„ä»£ç ã€‚å—¯ã€‚æ‰€ä»¥æ ‡è®°å™¨ã€‚å“¦ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹å¸¦æœ‰æ ‡è®°çš„è§†é¢‘ï¼Œç„¶åæˆ‘ä¼šè¯„è®ºè¿™ä¸€éƒ¨åˆ†å‘ç”Ÿçš„æ‰€æœ‰äº‹æƒ…ã€‚è¿™å¯ä»¥ä½œä¸ºä»‹ç»è§†é¢‘ã€‚
- en: '![](img/0e882caa799deede0a88d67b58737bfe_34.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_34.png)'
- en: '![](img/0e882caa799deede0a88d67b58737bfe_35.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_35.png)'
- en: Yeahã€‚In the next few minutesï¼Œ we'll take a look at the tokensã€‚ğŸ˜Šï¼ŒIn natural language
    processingã€‚ most of the data that we handle consists of raw textï¼› howeverã€‚ machine
    learning models cannot read or understand text in its raw formã€‚They can only work
    with numbersã€‚So the tokenizers objective will be to translate the text into numbersã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ åˆ†é’Ÿé‡Œï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹æ ‡è®°ã€‚ğŸ˜Š åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæˆ‘ä»¬å¤„ç†çš„å¤§å¤šæ•°æ•°æ®éƒ½æ˜¯åŸå§‹æ–‡æœ¬ï¼›ç„¶è€Œï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ— æ³•ä»¥åŸå§‹å½¢å¼è¯»å–æˆ–ç†è§£æ–‡æœ¬ã€‚å®ƒä»¬åªèƒ½å¤„ç†æ•°å­—ã€‚å› æ­¤ï¼Œæ ‡è®°å™¨çš„ç›®æ ‡æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ã€‚
- en: There are several possible approaches to this conversionã€‚ and the objective
    is to find the most meaningful representationã€‚ğŸ˜Šã€‚We'll take a look at three distinct
    organization algorithmsï¼Œ we compare them one to oneã€‚ so we recommend you take
    a look at the videos in the following orderï¼Œ first word basedã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§å¯èƒ½çš„æ–¹æ³•æ¥è¿›è¡Œæ­¤è½¬æ¢ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°æœ€æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚ğŸ˜Š æˆ‘ä»¬å°†æŸ¥çœ‹ä¸‰ç§ä¸åŒçš„ç»„ç»‡ç®—æ³•ï¼Œé€ä¸€æ¯”è¾ƒï¼Œå› æ­¤æˆ‘ä»¬å»ºè®®ä½ æŒ‰ä»¥ä¸‹é¡ºåºæŸ¥çœ‹è§†é¢‘ï¼Œé¦–å…ˆæ˜¯åŸºäºå•è¯çš„ã€‚
- en: followed by character based and finally sub word basedã€‚![](img/0e882caa799deede0a88d67b58737bfe_37.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯åŸºäºå­—ç¬¦çš„ï¼Œæœ€åæ˜¯åŸºäºå­è¯çš„ã€‚![](img/0e882caa799deede0a88d67b58737bfe_37.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_38.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_38.png)'
- en: Yeahã€‚So we won't look at the videoã€‚ Actuallyï¼Œ we're gonna look directly at the
    text inside inside the theã€‚The course and I'll comment because we won't have time
    to watch all those videos in the slide weã€‚So world based tokens so you can look
    at the video in your free time but we're gonna explain it a little bit more in
    depth with what I' to doã€‚ but the world based tokenizer is just going to split
    your sentence by word so the easiest way to do that is to take all the spaces
    and then split your text onto those spaces more advanced would be to include some
    walls to split and punctuation so for instance the exclam mark separated it from
    tokenization or here let's split it between let and aworth asã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸ä¼šçœ‹è§†é¢‘ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å°†ç›´æ¥æŸ¥çœ‹è¯¾ç¨‹ä¸­çš„æ–‡æœ¬ï¼Œæˆ‘ä¼šè¿›è¡Œè¯„è®ºï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ—¶é—´è§‚çœ‹æ‰€æœ‰è¿™äº›å¹»ç¯ç‰‡ä¸­çš„è§†é¢‘ã€‚æ‰€ä»¥åŸºäºå•è¯çš„æ ‡è®°å™¨ï¼Œä½ å¯ä»¥åœ¨é—²æš‡æ—¶è§‚çœ‹è§†é¢‘ï¼Œä½†æˆ‘ä»¬å°†æ›´æ·±å…¥åœ°è§£é‡Šæˆ‘è¦åšçš„äº‹æƒ…ã€‚åŸºäºå•è¯çš„æ ‡è®°å™¨åªæ˜¯å°†ä½ çš„å¥å­æŒ‰å•è¯åˆ†å‰²ï¼Œå› æ­¤æœ€ç®€å•çš„æ–¹æ³•æ˜¯å–æ‰€æœ‰ç©ºæ ¼ï¼Œç„¶åæŒ‰è¿™äº›ç©ºæ ¼æ‹†åˆ†æ–‡æœ¬ï¼Œæ›´é«˜çº§çš„æ–¹å¼æ˜¯åŒ…æ‹¬ä¸€äº›å¢™å£æ¥åˆ†å‰²å’Œæ ‡ç‚¹ç¬¦å·ï¼Œä¾‹å¦‚æ„Ÿå¹å·å°†å…¶ä»æ ‡è®°åŒ–ä¸­åˆ†ç¦»ï¼Œæˆ–è€…åœ¨è¿™é‡Œæˆ‘ä»¬å°†å…¶åˆ†å‰²ä¸ºâ€œletâ€å’Œâ€œaworthâ€ã€‚
- en: So we can see this on this example with Chi and Sun weather Preterã€‚ which is
    separated into five words hereã€‚So the world organizers areã€‚Were used a lot before
    transformersï¼Œ mostly the advantage is thatã€‚You split naturally your text onto
    the spaces and punctuation see disadvantage is that you end up with pretty large
    vocabularies because there there are lots of different worlds in English and every
    time someone makes a typo in some world you end up with in a new world in your
    vocabularyã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ªä¾‹å­ä¸­çœ‹åˆ°â€œChiâ€å’Œâ€œSun weather Preterâ€ï¼Œå®ƒè¢«åˆ†æˆäº†äº”ä¸ªå•è¯ã€‚æ‰€ä»¥åŸºäºå•è¯çš„ç»„ç»‡è€…æ˜¯ã€‚ä¹‹å‰åœ¨å˜æ¢å™¨ä¸­ä½¿ç”¨å¾—å¾ˆå¤šï¼Œä¸»è¦çš„ä¼˜ç‚¹æ˜¯ã€‚ä½ è‡ªç„¶åœ°å°†æ–‡æœ¬æ‹†åˆ†æˆç©ºæ ¼å’Œæ ‡ç‚¹ï¼Œç¼ºç‚¹æ˜¯ä½ æœ€ç»ˆä¼šæœ‰ç›¸å½“å¤§çš„è¯æ±‡é‡ï¼Œå› ä¸ºè‹±è¯­ä¸­æœ‰å¾ˆå¤šä¸åŒçš„å•è¯ï¼Œæ¯æ¬¡æœ‰äººåœ¨æŸä¸ªå•è¯ä¸­æ‰“é”™å­—ï¼Œä½ å°±ä¼šåœ¨è¯æ±‡è¡¨ä¸­å¢åŠ ä¸€ä¸ªæ–°å•è¯ã€‚
- en: So each word gets assigned in ID starting from0 going up to the size of the
    vocabulary and then since we can't guarantee that the user is never going to make
    a tape or anythingã€‚ there is a special ruleï¼Œ if we encounter a token that doesn't
    exist in the vocabularyã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¯ä¸ªå•è¯ä»0å¼€å§‹åˆ†é…ä¸€ä¸ªIDï¼Œç›´åˆ°è¯æ±‡è¡¨çš„å¤§å°ï¼Œç„¶åç”±äºæˆ‘ä»¬ä¸èƒ½ä¿è¯ç”¨æˆ·æ°¸è¿œä¸ä¼šçŠ¯é”™ï¼Œå› æ­¤æœ‰ä¸€ä¸ªç‰¹æ®Šè§„åˆ™ï¼Œå¦‚æœæˆ‘ä»¬é‡åˆ°ä¸€ä¸ªåœ¨è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°ã€‚
- en: it's usually replaced by something called the unknown token which is usually
    something that looks like that and between bracketsã€‚So this is one of the other
    drawbacks of the world based tokenneaã€‚ so the first one is that we have very largeocabulariesã€‚
    the second one is that we need to learn that the word Do and the word Dos are
    very similarã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒé€šå¸¸è¢«ç§°ä¸ºæœªçŸ¥æ ‡è®°æ¥æ›¿ä»£ï¼Œé€šå¸¸çœ‹èµ·æ¥åƒè¿™æ ·ï¼Œå¹¶åœ¨æ‹¬å·ä¸­ã€‚å› æ­¤ï¼Œè¿™æ˜¯åŸºäºå•è¯çš„æ ‡è®°å™¨çš„å¦ä¸€ä¸ªç¼ºç‚¹ï¼Œç¬¬ä¸€ä¸ªæ˜¯æˆ‘ä»¬æœ‰éå¸¸å¤§çš„è¯æ±‡é‡ï¼Œç¬¬äºŒä¸ªæ˜¯æˆ‘ä»¬éœ€è¦å­¦ä¹ â€œDoâ€å’Œâ€œDosâ€è¿™ä¸¤ä¸ªå•è¯éå¸¸ç›¸ä¼¼ã€‚
- en: they won't know that from scratch because when the model is initialized randomlyã€‚
    it's going to have a set of abidding for that word dos and another one for that
    word dogss and it's going to need to learn by seeing lots and lots of data that
    those two world look a little a bitlikeã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ä¸çŸ¥é“ä»å¤´å¼€å§‹ï¼Œå› ä¸ºæ¨¡å‹æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚å®ƒå°†æœ‰ä¸€ç»„é’ˆå¯¹é‚£ä¸ªå•è¯â€œdosâ€çš„çº¦æŸï¼Œè¿˜æœ‰å¦ä¸€ç»„é’ˆå¯¹â€œdogssâ€çš„çº¦æŸï¼Œå¹¶ä¸”å®ƒéœ€è¦é€šè¿‡çœ‹åˆ°å¤§é‡æ•°æ®æ¥å­¦ä¹ è¿™ä¸¤ä¸ªè¯çœ‹èµ·æ¥æœ‰ç‚¹ç›¸ä¼¼ã€‚
- en: And the last disadvantage is that Ung token so every word that the depo is going
    to end up like this and the more can learn in your representation of thatã€‚ it
    it says if you had just deleted the world in the sentenceã€‚So another way is to
    just split your text on all characters which is the what character best organizes
    to in this case your vocabulary is not going to be very large because 256 SI charactersã€‚
    for instanceï¼Œ a little bit more if you take the wall any good thingã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªç¼ºç‚¹æ˜¯Ung tokenï¼Œæ‰€ä»¥æ¯ä¸ªå•è¯æœ€ç»ˆéƒ½ä¼šè¿™æ ·ï¼Œæ›´å¤šçš„æ˜¯åœ¨ä½ çš„è¡¨ç¤ºä¸­å­¦ä¹ ã€‚å®ƒçš„æ„æ€æ˜¯ï¼Œå¦‚æœä½ åˆ é™¤å¥å­ä¸­çš„æŸä¸ªå•è¯ä¼šæ€æ ·ã€‚å¦ä¸€ç§æ–¹å¼æ˜¯æŒ‰æ‰€æœ‰å­—ç¬¦æ¥åˆ†å‰²æ–‡æœ¬ï¼Œè¿™å°±æ˜¯å­—ç¬¦æœ€ä½³ç»„ç»‡çš„æ–¹å¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ çš„è¯æ±‡é‡ä¸ä¼šå¾ˆå¤§ï¼Œå› ä¸ºåªæœ‰256ä¸ªå­—ç¬¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å¤„ç†æ›´å¤§çš„è¯æ±‡ã€‚
- en: but you're not going to end up with models but have a vocabulary size of I don't
    know 300ã€‚000 or something like thatã€‚So this is better for the vocabulary sizeã€‚
    you probably won't get a known token because you all see all the different character
    possibleã€‚But the drawback is that now the representation is based on characterï¼Œ
    so the model as to on thatã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ ä¸ä¼šå¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œå…¶è¯æ±‡é‡è¾¾åˆ°300,000æˆ–ç±»ä¼¼çš„æ•°é‡ã€‚å› æ­¤ï¼Œè¿™å¯¹äºè¯æ±‡é‡æ›´å¥½ï¼Œä½ å¯èƒ½ä¸ä¼šå¾—åˆ°æœªçŸ¥çš„tokenï¼Œå› ä¸ºä½ ä¼šçœ‹åˆ°æ‰€æœ‰å¯èƒ½çš„ä¸åŒå­—ç¬¦ã€‚ä½†ç¼ºç‚¹æ˜¯ï¼Œç°åœ¨è¡¨ç¤ºåŸºäºå­—ç¬¦ï¼Œå› æ­¤æ¨¡å‹å¿…é¡»é€‚åº”è¿™ä¸€ç‚¹ã€‚
- en: for instanceï¼Œ the letter E is not does not mean the same thing when it's between
    an a T compared to the letter E here with the key and the n beside the word organizationã€‚So
    is a representation of each letter is less meaningfulï¼Œ that's what I't trying
    to sayã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå­—æ¯Eåœ¨å­—æ¯Tä¹‹é—´çš„å«ä¹‰ä¸å­—æ¯Eåœ¨å­—æ¯Kå’ŒNæ—è¾¹çš„â€œç»„ç»‡â€ä¸­çš„å«ä¹‰ä¸åŒã€‚å› æ­¤ï¼Œæ¯ä¸ªå­—æ¯çš„è¡¨ç¤ºæ„ä¹‰è¾ƒå¼±ï¼Œè¿™å°±æ˜¯æˆ‘æƒ³è¡¨è¾¾çš„ã€‚
- en: And compared to what we had we've workedã€‚The overall drawback is that we end
    up with very long sentencesã€‚ for instanceï¼Œ for leads to tokenizationï¼Œ if we look
    back with two world based tokenization it was split into five words with the tokenization
    with the character based tokenization it splits in much more concor here but it's
    between 15 and 20ã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å·¥ä½œæœ‰äº†è¿›å±•ã€‚æ•´ä½“çš„ç¼ºç‚¹æ˜¯å¥å­å˜å¾—éå¸¸é•¿ã€‚ä¾‹å¦‚ï¼ŒåŸºäºè¯çš„åˆ†è¯æ³•å°†å…¶åˆ†ä¸ºäº”ä¸ªè¯ï¼Œè€ŒåŸºäºå­—ç¬¦çš„åˆ†è¯æ³•åˆ™åˆ†å¾—æ›´å¤šï¼Œé€šå¸¸åœ¨15åˆ°20ä¹‹é—´ã€‚
- en: let's say so we end up with longer sentences and our transformer models are
    usually constrained by a maximum lengthsã€‚ So for instanceï¼Œ the built model can
    only do5 can only treats 512 tokens at a timeã€‚ So using a character based tokenization
    algorithm wouldã€‚Make sure the maximum sentenceã€‚ you can feed them all pretty shortã€‚So
    that's why transformmonology usually use a compromise between word and character
    based tocanizationã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°æ›´é•¿çš„å¥å­ï¼Œè€Œæˆ‘ä»¬çš„å˜æ¢æ¨¡å‹é€šå¸¸å—åˆ°æœ€å¤§é•¿åº¦çš„é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œæ„å»ºçš„æ¨¡å‹ä¸€æ¬¡åªèƒ½å¤„ç†512ä¸ªtokenã€‚å› æ­¤ï¼Œä½¿ç”¨åŸºäºå­—ç¬¦çš„åˆ†è¯ç®—æ³•å°†ç¡®ä¿æœ€å¤§å¥å­é•¿åº¦ç›¸å¯¹è¾ƒçŸ­ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå˜æ¢æŠ€æœ¯é€šå¸¸åœ¨åŸºäºè¯å’ŒåŸºäºå­—ç¬¦çš„åˆ†è¯ä¹‹é—´æ‰¾åˆ°æŠ˜è¡·ã€‚
- en: which is the world tokenizationã€‚So sub organizationã€‚As the name indicatesã€‚ it's
    going to split your text into subworsï¼Œ so it's still split between wordsã€‚ but
    some wordss are got intoï¼Œ for instance hereï¼Œ you've got lets do and then token
    and Iization are separated intoã€‚Notice that you get the small andã€‚With like the
    animals know to say that in Englishã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯tokenizationã€‚å­ç»„ç»‡ï¼Œå¦‚å…¶åç§°æ‰€ç¤ºï¼Œä¼šå°†ä½ çš„æ–‡æœ¬æ‹†åˆ†æˆå­è¯ï¼Œå› æ­¤ä»ç„¶åœ¨è¯ä¹‹é—´æ‹†åˆ†ã€‚ä½†æ˜¯æŸäº›è¯ä¼šè¢«æ‹†åˆ†ï¼Œä¾‹å¦‚è¿™é‡Œçš„lets doï¼Œç„¶åtokenå’ŒIizationè¢«åˆ†å¼€ã€‚æ³¨æ„åˆ°ä½ å¾—åˆ°äº†å°å†™çš„å’Œã€‚åƒåŠ¨ç‰©åœ¨è‹±è¯­ä¸­ä¼šè¿™æ ·è¯´ã€‚
- en: but that special thing between an inferior superior sign with slash Wã€‚ which
    means that it's the end of a worldã€‚ So Tuken doesn't have it because that's for
    theã€‚That's because we want the model to be able to differentiate token as a single
    world and token followed by something else like tokenizationã€‚ so the Iization
    as the specificx that says here its in the other world that token does notã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨è¾ƒå°å’Œè¾ƒå¤§çš„ç¬¦å·ä¹‹é—´æœ‰ä¸ªç‰¹æ®Šçš„ç¬¦å·æ–œæ Wï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªå•è¯çš„ç»“å°¾ã€‚å› æ­¤ï¼Œtokenæ²¡æœ‰è¿™ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½å¤ŸåŒºåˆ†å•ä¸ªå•è¯çš„tokenå’Œåé¢è·Ÿç€å…¶ä»–ä¸œè¥¿çš„tokenï¼Œä¾‹å¦‚tokenizationã€‚æ‰€ä»¥Iizationä½œä¸ºç‰¹å®šçš„è¡¨ç¤ºï¼Œåœ¨å…¶ä»–å•è¯ä¸­tokenå¹¶æ²¡æœ‰ã€‚
- en: And so that depends on the convention used by the tokenizerã€‚ some tokens have
    a thing at the beginning of worldã€‚ some tokens have a thing at the end of the
    worldã€‚And so this approachã€‚ this approach allows you to have a vocabulary that's
    not going to be too huge and the two can still have some meaningfulmantic some
    semantic meaning that's more meaningful than just charactersã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å–å†³äºåˆ†è¯å™¨ä½¿ç”¨çš„çº¦å®šã€‚æœ‰äº›tokenåœ¨å•è¯çš„å¼€å¤´ï¼Œæœ‰äº›åˆ™åœ¨å•è¯çš„æœ«å°¾ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•å…è®¸ä½ æ‹¥æœ‰ä¸€ä¸ªä¸ä¼šå¤ªåºå¤§çš„è¯æ±‡ï¼ŒåŒæ—¶ä»èƒ½ä¿æŒæŸç§è¯­ä¹‰ä¸Šçš„æ„ä¹‰ï¼Œè¶…è¿‡å•çº¯çš„å­—ç¬¦ã€‚
- en: And the last thing is that for worlds based tokensï¼Œ for instanceï¼Œ Doug and dogsã€‚
    well two separate wordss hereï¼Œ Do as dogs would probably be split into Do and
    nestã€‚ the same way tokenization is split between token and Iization so it can
    learn that they have the same prefix and then the sization is going to be used
    in another world like modernizationã€‚And the twoken can the model sorry can then
    make sense of the Su fixes and learn that they are always kind of the sameã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ç‚¹æ˜¯ï¼Œå¯¹äºåŸºäºä¸–ç•Œçš„ä»¤ç‰Œï¼Œä¾‹å¦‚ï¼ŒDougå’Œdogsã€‚è¿™ä¸¤ä¸ªå•è¯åº”è¯¥è¢«åˆ†å¼€ï¼ŒDoå’Œdogså¯èƒ½ä¼šè¢«æ‹†åˆ†ä¸ºDoå’Œnestã€‚ä»¤ç‰ŒåŒ–çš„æ–¹å¼ä¸tokenå’ŒIizationä¹‹é—´çš„åˆ†å‰²ç±»ä¼¼ï¼Œå› æ­¤å®ƒå¯ä»¥å­¦ä¹ å®ƒä»¬å…·æœ‰ç›¸åŒçš„å‰ç¼€ï¼Œç„¶åsizationå°†åœ¨å¦ä¸€ä¸ªè¯ä¸­ä½¿ç”¨ï¼Œå¦‚modernizationã€‚æ¨¡å‹å¯ä»¥ç†è§£åç¼€ï¼Œå¹¶å­¦ä¹ å®ƒä»¬æ€»æ˜¯æŸç§ç›¸åŒçš„ã€‚
- en: And so in the next part of the courseï¼Œ we'll look at into detail as a different
    because there are three differentã€‚Suborgan algorithm that are by level word piece
    and sentence piece will explain exactly the difference between them in the next
    part of the courseã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¾ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è¯¦ç»†æŸ¥çœ‹ä¸åŒçš„ç®—æ³•ï¼Œå› ä¸ºæœ‰ä¸‰ç§ä¸åŒçš„å­ç»„ç»‡ç®—æ³•ï¼ŒæŒ‰çº§åˆ«çš„word pieceå’Œsentence pieceå°†åœ¨è¯¾ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ä¸­å‡†ç¡®è§£é‡Šå®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚
- en: So let's see if we have any questions before we look at oh the tokenazer work
    practiceã€‚Yesã€‚ I'm just going to put myself here properlyã€‚Tuckenneer breaks a sentence
    into tokensã€‚ but no limatizations or stming is performed before learningã€‚å—¯ã€‚You
    knowã€‚It'sã€‚Lets meet the bullshã€‚I would say noï¼Œ but you should ask the questions
    from where people that are more competent than me can answer you because I'm not
    completely sureã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çœ‹çœ‹ä»¤ç‰Œå™¨åœ¨å®è·µä¸­çš„å·¥ä½œä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜ã€‚æ˜¯çš„ã€‚æˆ‘åªæ˜¯æƒ³å¥½å¥½æŠŠè‡ªå·±æ”¾åœ¨è¿™é‡Œã€‚ä»¤ç‰Œå™¨å°†å¥å­åˆ†è§£ä¸ºä»¤ç‰Œï¼Œä½†åœ¨å­¦ä¹ ä¹‹å‰ä¸ä¼šè¿›è¡Œè¯å½¢è¿˜åŸæˆ–stemmingã€‚å—¯ã€‚ä½ çŸ¥é“ã€‚æ˜¯çš„ã€‚è®©æˆ‘ä»¬ä¸æ‰¯æ·¡ã€‚æˆ‘ä¼šè¯´æ²¡æœ‰ï¼Œä½†ä½ åº”è¯¥å‘é‚£äº›æ¯”æˆ‘æ›´æœ‰èƒ½åŠ›çš„äººæé—®ï¼Œå› ä¸ºæˆ‘ä¸å®Œå…¨ç¡®å®šã€‚
- en: Could you provide intuition into word piece usingbed and sentence piece type
    tosï¼ŸI couldã€‚ but it's going to take a bit of time so again I'm going to readdirect
    you on the form where I can take the time to properly answer you and there is
    also maybe Louisis can share it here phrase his the tokenazal summary in the transformal
    documentation that explains the difference between what piece and the sentence
    piece which is using Uniigram behind the scene and the key difference between
    the twoã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ èƒ½æä¾›å…³äºword pieceå’Œsentence pieceç±»å‹çš„ç›´è§‚ç†è§£å—ï¼Ÿæˆ‘å¯ä»¥ã€‚ä½†è¿™ä¼šèŠ±ä¸€ç‚¹æ—¶é—´ï¼Œæ‰€ä»¥æˆ‘å†æ¬¡ä¼šå¼•å¯¼ä½ å»é‚£ä¸ªå¯ä»¥è®©æˆ‘å¥½å¥½å›ç­”ä½ çš„åœ°æ–¹ï¼Œæˆ–è®¸Louisiså¯ä»¥åœ¨è¿™é‡Œåˆ†äº«ä»–çš„tokenizeræ€»ç»“ï¼Œä»¥åŠåœ¨å˜æ¢æ–‡æ¡£ä¸­è§£é‡Šword
    pieceå’Œsentence pieceä¹‹é—´åŒºåˆ«çš„å†…å®¹ï¼Œåè€…åœ¨åå°ä½¿ç”¨Uniigramï¼ŒäºŒè€…ä¹‹é—´çš„å…³é”®åŒºåˆ«ã€‚
- en: Does the W slashW tag add any information in the subwe tokenizationï¼Œ so yesã€‚
    as I said it's what allows them all all to know the difference between a single
    world like I mean between token used as a single world or token inside a world
    like tokenization or tokenizerã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: W slash Wæ ‡ç­¾åœ¨å­ä»¤ç‰ŒåŒ–ä¸­æ·»åŠ äº†ä»»ä½•ä¿¡æ¯å—ï¼Ÿæ˜¯çš„ã€‚æ­£å¦‚æˆ‘æ‰€è¯´çš„ï¼Œè¿™ä½¿å¾—æ‰€æœ‰äººéƒ½çŸ¥é“å•ä¸ªå•è¯ä¹‹é—´çš„åŒºåˆ«ï¼Œä¾‹å¦‚tokenä½œä¸ºå•ä¸ªå•è¯æˆ–tokenåœ¨ä¸€ä¸ªå•è¯å†…éƒ¨ï¼Œå¦‚tokenizationæˆ–tokenizerã€‚
- en: And then let's see how the tokener work in practiceã€‚å‘ƒã€‚So can we have seen to
    load the tokenizer using the form pretrain methodã€‚And what it returnsã€‚ and we'll
    now quickly look at the video on the tokenization pipelineã€‚ which is going to
    explain what's happened when we feed the tokener sequence like that and how it
    returns those numbersã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®©æˆ‘ä»¬çœ‹çœ‹ä»¤ç‰Œå™¨åœ¨å®è·µä¸­çš„å·¥ä½œã€‚å‘ƒã€‚å› æ­¤æˆ‘ä»¬å·²ç»çœ‹åˆ°ä½¿ç”¨formé¢„è®­ç»ƒæ–¹æ³•åŠ è½½ä»¤ç‰Œå™¨ã€‚å®ƒè¿”å›çš„å†…å®¹æ˜¯ä»€ä¹ˆã€‚ç°åœ¨æˆ‘ä»¬å°†å¿«é€ŸæŸ¥çœ‹æœ‰å…³ä»¤ç‰ŒåŒ–ç®¡é“çš„è§†é¢‘ï¼Œå®ƒå°†è§£é‡Šå½“æˆ‘ä»¬åƒè¿™æ ·è¾“å…¥ä»¤ç‰Œåºåˆ—æ—¶å‘ç”Ÿäº†ä»€ä¹ˆï¼Œä»¥åŠå®ƒå¦‚ä½•è¿”å›é‚£äº›æ•°å­—ã€‚
- en: Let me just grab it from my computerï¼Œ and then I'll continue answering questionsã€‚å—¯ã€‚So
    took a nice pipelineã€‚In this videoï¼Œ while look at how tokenizer converts ver text
    to numbers that a transformer model can make sense ofã€‚ like when we execute this
    goodã€‚Here is a quick overview of what happens inside the tokenizer objectã€‚Firstï¼Œ
    the text is split into tuetsï¼Œ which are wordsï¼Œ parts of wordsï¼Œ or punctuation
    symbolsã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»æˆ‘çš„ç”µè„‘ä¸ŠæŠ“å–ä¸€ä¸‹ï¼Œç„¶åæˆ‘å°†ç»§ç»­å›ç­”é—®é¢˜ã€‚å—¯ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸é”™çš„ç®¡é“ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ä»¤ç‰Œå™¨å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºå˜æ¢æ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œè¿™ä¸ªæ—¶ã€‚è¿™é‡Œæ˜¯ä»¤ç‰Œå™¨å¯¹è±¡å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…çš„å¿«é€Ÿæ¦‚è¿°ã€‚é¦–å…ˆï¼Œæ–‡æœ¬è¢«æ‹†åˆ†æˆtuetsï¼Œé€šå¸¸æ˜¯å•è¯ã€å•è¯çš„ä¸€éƒ¨åˆ†æˆ–æ ‡ç‚¹ç¬¦å·ã€‚
- en: Then the tokenizer adds potential special tos and converts each token to our
    unique respective IDã€‚ as defined by the touckenizer's vocabularyã€‚As we'll see
    it doesn't quite happen in this orderã€‚ but doing it like this is better for her
    understandingsã€‚The first step is to split our input text into tokensï¼Œ we use the
    tokenized method for thisã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åtokenizeræ·»åŠ æ½œåœ¨çš„ç‰¹æ®Štokenå¹¶å°†æ¯ä¸ªtokenè½¬æ¢ä¸ºæˆ‘ä»¬ç‹¬ç‰¹çš„ç›¸åº”IDï¼Œæ­£å¦‚tokenizerçš„è¯æ±‡æ‰€å®šä¹‰çš„ã€‚æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œè¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æ˜¯ä¸¥æ ¼æŒ‰ç…§è¿™ä¸ªé¡ºåºè¿›è¡Œçš„ï¼Œä½†è¿™æ ·åšæ›´æœ‰åˆ©äºç†è§£ã€‚ç¬¬ä¸€æ­¥æ˜¯å°†æˆ‘ä»¬çš„è¾“å…¥æ–‡æœ¬æ‹†åˆ†æˆtokensï¼Œæˆ‘ä»¬ä¸ºæ­¤ä½¿ç”¨tokenizedæ–¹æ³•ã€‚
- en: To do thatï¼Œ the tokenizer may first perform some operations like lower casing
    or wordsã€‚ then follow a set of rules to split the result in small chunks of textã€‚Most
    of the transformable models use a word organization algorithmã€‚Which means that
    one given word can be split in several tokensï¼Œ like tokens hereã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œtokenizerå¯èƒ½é¦–å…ˆæ‰§è¡Œä¸€äº›æ“ä½œï¼Œæ¯”å¦‚å°å†™æˆ–å•è¯ï¼Œç„¶åéµå¾ªä¸€å¥—è§„åˆ™å°†ç»“æœæ‹†åˆ†æˆå°å—æ–‡æœ¬ã€‚å¤§å¤šæ•°å¯è½¬æ¢æ¨¡å‹ä½¿ç”¨å•è¯ç»„ç»‡ç®—æ³•ã€‚è¿™æ„å‘³ç€ä¸€ä¸ªç»™å®šçš„å•è¯å¯ä»¥è¢«æ‹†åˆ†æˆå‡ ä¸ªtokensï¼Œåƒè¿™é‡Œçš„tokensä¸€æ ·ã€‚
- en: Look at the Tokenization algorithms video linked below for more informationã€‚The
    ash ash prefix we see in front of I is a convention used by bird to indicate thistoken
    is not the beginning of the worldã€‚Other organrs may use different convention howeverã€‚For
    instanceã€‚ Albert tokenizers will add a long end score in front of all the tokens
    that had its space before themã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æŸ¥çœ‹ä¸‹é¢é“¾æ¥çš„Tokenizationç®—æ³•è§†é¢‘ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨â€œIâ€å‰çœ‹åˆ°çš„ash ashå‰ç¼€æ˜¯é¸Ÿç±»ç”¨æ¥æŒ‡ç¤ºè¯¥tokenä¸æ˜¯å•è¯å¼€å¤´çš„æƒ¯ä¾‹ã€‚å…¶ä»–ç»„ç»‡å¯èƒ½ä½¿ç”¨ä¸åŒçš„æƒ¯ä¾‹ã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œé˜¿å°”ä¼¯ç‰¹tokenizersä¼šåœ¨æ‰€æœ‰å‰é¢æœ‰ç©ºæ ¼çš„tokenå‰åŠ ä¸€ä¸ªé•¿çš„ç»“æŸåˆ†æ•°ã€‚
- en: Which is a convention shared by all sentence based torsã€‚The second step of the
    tokenization pipeline is to map those tokens to respective IDsã€‚ as defined by
    the vocabulary of the tokenizerã€‚This is why we need to download the file when
    we instant hit a tokenizer with the form pre methodã€‚We have to make sure we use
    the same mapping as when the model was portrayedã€‚To do thisã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ‰€æœ‰åŸºäºå¥å­çš„torså…±äº«çš„æƒ¯ä¾‹ã€‚tokenizationç®¡é“çš„ç¬¬äºŒæ­¥æ˜¯å°†è¿™äº›tokenæ˜ å°„åˆ°ç›¸åº”çš„IDï¼Œæ­£å¦‚tokenizerçš„è¯æ±‡æ‰€å®šä¹‰çš„ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå½“æˆ‘ä»¬ç”¨form
    preæ–¹æ³•å³æ—¶è°ƒç”¨tokenizeræ—¶éœ€è¦ä¸‹è½½æ–‡ä»¶ã€‚æˆ‘ä»¬å¿…é¡»ç¡®ä¿ä½¿ç”¨ä¸æ¨¡å‹å±•ç°æ—¶ç›¸åŒçš„æ˜ å°„ã€‚ä¸ºæ­¤ã€‚
- en: we use the converttugans to IDs methodã€‚You may have noticed that we don't have
    the exact same results as in our first slideã€‚Or note as this look like a list
    of random numbers anywayã€‚ in which case allow me to refresh your memoryã€‚We the
    number at the beginning and the number at the end that are missingã€‚Those are the
    special ticketsã€‚So special tokens are added by the proper formalal methodã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨converttugansåˆ°IDsçš„æ–¹æ³•ã€‚ä½ å¯èƒ½æ³¨æ„åˆ°æˆ‘ä»¬ä¸ç¬¬ä¸€å¼ å¹»ç¯ç‰‡çš„ç»“æœä¸å®Œå…¨ç›¸åŒã€‚æˆ–è€…è¯´è¿™çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸²éšæœºæ•°å­—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·å…è®¸æˆ‘åˆ·æ–°ä½ çš„è®°å¿†ã€‚ç¼ºå¤±çš„å°±æ˜¯å¼€å¤´å’Œç»“å°¾çš„æ•°å­—ã€‚é‚£äº›æ˜¯ç‰¹æ®Šçš„tokenã€‚æ‰€ä»¥ç‰¹æ®Štokenæ˜¯é€šè¿‡é€‚å½“çš„formalalæ–¹æ³•æ·»åŠ çš„ã€‚
- en: which knows the indices of a token in the vocabulary and just adds the proper
    numbers in the input IDs listã€‚You can look at the special tokens and more generally
    at how the tokenizer has changed your text by using the decocode method and the
    outputs of the tokenizer objectã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçŸ¥é“tokenåœ¨è¯æ±‡ä¸­çš„ç´¢å¼•ï¼Œå¹¶åœ¨è¾“å…¥IDåˆ—è¡¨ä¸­æ·»åŠ é€‚å½“çš„æ•°å­—ã€‚ä½ å¯ä»¥ä½¿ç”¨decocodeæ–¹æ³•æŸ¥çœ‹ç‰¹æ®Štokensï¼Œä»¥åŠæ›´ä¸€èˆ¬åœ°æŸ¥çœ‹tokenizerå¦‚ä½•æ”¹å˜ä½ çš„æ–‡æœ¬ï¼Œå€ŸåŠ©tokenizerå¯¹è±¡çš„è¾“å‡ºã€‚
- en: As for the prefix for beginning of worlds part of worldsã€‚ both special token
    vary depending on which tor you are usingã€‚So belt tokener uses CLS onã€‚ but the
    Robertta tokener uses HTMLl like on calls S and/lash Sã€‚Now that you know how the
    tocanazer worksï¼Œ you can forget all was intermediately admitted and don remember
    that you have to call it on your input textsã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³äºå•è¯å¼€å§‹éƒ¨åˆ†çš„å‰ç¼€ï¼Œä¸¤ä¸ªç‰¹æ®Štokenä¼šæ ¹æ®ä½ ä½¿ç”¨çš„torè€Œæœ‰æ‰€ä¸åŒã€‚æ‰€ä»¥belt tokenerä½¿ç”¨CLSï¼Œè€ŒRobertta tokeneråˆ™ä½¿ç”¨ç±»ä¼¼äºHTMLçš„Så’Œ/lash
    Sã€‚ç°åœ¨ä½ çŸ¥é“tokenizeræ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä½ å¯ä»¥å¿˜è®°æ‰€æœ‰ä¸­é—´æ­¥éª¤ï¼Œå¹¶è®°ä½å¿…é¡»åœ¨è¾“å…¥æ–‡æœ¬ä¸Šè°ƒç”¨å®ƒã€‚
- en: The output of the decokenizer don't just contain the input IDï¼Œ howeverã€‚To learn
    where the attention mask isï¼Œ check out the batch input Together videoã€‚To learn
    about targettype ideasï¼Œ you get the process spells of start videoã€‚![](img/0e882caa799deede0a88d67b58737bfe_40.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œdecokenizerçš„è¾“å‡ºä¸ä»…ä»…åŒ…å«è¾“å…¥IDã€‚è¦äº†è§£æ³¨æ„åŠ›æ©ç çš„ä½ç½®ï¼Œè¯·æŸ¥çœ‹batch input Togetherè§†é¢‘ã€‚è¦äº†è§£targettypeçš„æƒ³æ³•ï¼Œä½ å¯ä»¥è·å¾—startè§†é¢‘çš„å¤„ç†æ‹¼å†™ã€‚![](img/0e882caa799deede0a88d67b58737bfe_40.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_41.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_41.png)'
- en: Yeahã€‚So we have one questions that's linked to what we were seeing just before
    the video are token slash W and token going to have separate representation IDs
    and yesã€‚ we are going to have separate representation IDs because we are not the
    same tokenã€‚As which is theã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå’Œæˆ‘ä»¬åœ¨è§†é¢‘ä¹‹å‰çœ‹åˆ°çš„æœ‰å…³ï¼Œæ ‡è®°/ Wå’Œæ ‡è®°ä¼šæœ‰å•ç‹¬çš„è¡¨ç¤ºIDå—ï¼Ÿæ˜¯çš„ã€‚æˆ‘ä»¬ä¼šæœ‰å•ç‹¬çš„è¡¨ç¤ºIDï¼Œå› ä¸ºæˆ‘ä»¬ä¸æ˜¯åŒä¸€ä¸ªæ ‡è®°ã€‚æ­£å¦‚æ‰€è¯´çš„ã€‚
- en: the the whole meaning of that slashable you2anã€‚Ne and sorry specificã€‚So tokenizer
    or the tokenization pipelineï¼Œ I'm not going to livecode was intermediately admitted
    because you shouldn't really learn themã€‚ we're just showing them to show you the
    steps inside the pipeline the main thing to remember is that you just have to
    call your tokenizer on your input like this because this is the main E that's
    the most useful and now we'll look at what the attention mask is and what padding
    and Fun means the arguments that we had at the very beginning so that we fully
    explain what's happening inside all the tokenizerã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ•´ä¸ªå¯åˆ†å‰²çš„æ„æ€æ˜¯ä½ èƒ½ã€‚å¯¹ä¸èµ·ï¼Œå…·ä½“ä¸€ç‚¹ã€‚æ‰€ä»¥æ ‡è®°å™¨æˆ–æ ‡è®°åŒ–ç®¡é“ï¼Œæˆ‘ä¸ä¼šå®æ—¶ç¼–ç ï¼Œå› ä¸ºä½ å®é™…ä¸Šä¸åº”è¯¥å­¦ä¹ å®ƒä»¬ã€‚æˆ‘ä»¬åªæ˜¯å±•ç¤ºå®ƒä»¬ï¼Œä»¥ä¾¿ä½ çœ‹åˆ°ç®¡é“å†…éƒ¨çš„æ­¥éª¤ï¼Œè®°ä½çš„ä¸»è¦å†…å®¹æ˜¯ä½ åªéœ€è¦åƒè¿™æ ·åœ¨è¾“å…¥ä¸Šè°ƒç”¨ä½ çš„æ ‡è®°å™¨ï¼Œå› ä¸ºè¿™æ˜¯ä¸»è¦çš„Eï¼Œè¿™æ˜¯æœ€æœ‰ç”¨çš„ã€‚ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Œå¡«å……å’ŒFunæ„å‘³ç€ä»€ä¹ˆï¼Œè¿™äº›éƒ½æ˜¯æˆ‘ä»¬åœ¨æœ€å¼€å§‹æ—¶æåˆ°çš„å‚æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å®Œå…¨è§£é‡Šæ‰€æœ‰æ ‡è®°å™¨å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚
- en: Ohï¼Œ another questionï¼Œ is there a reason you would save a proed organizer or
    then better to just have a local copyã€‚Very good questionã€‚ Soï¼Œ yeahï¼Œ there is no
    real reason to save your patron organizer if you don't need toã€‚If you didn't make
    any change inside it and you always you would always have a local copy because
    auto tokenizer that from pretrained is going to cache the files to avoid you download
    them again so there is no reason to save it the one exception is when youre creating
    folder that you want to push to the model hub in which case you should save your
    tokenizer inside that folder so that when you push user you push your model the
    configuration and the tokenizer that's used with it and we have all those threethinkã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œä½ æ˜¯å¦æœ‰ç†ç”±ä¿å­˜ä¸€ä¸ªé¡¹ç›®ç»„ç»‡è€…ï¼Œæˆ–è€…è¯´æœ€å¥½åªæ˜¯ä¿ç•™ä¸€ä¸ªæœ¬åœ°å‰¯æœ¬ã€‚éå¸¸å¥½çš„é—®é¢˜ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œå¦‚æœä½ ä¸éœ€è¦ï¼Œå®é™…ä¸Šæ²¡æœ‰ç†ç”±ä¿å­˜ä½ çš„é¡¹ç›®ç»„ç»‡è€…ã€‚å¦‚æœä½ æ²¡æœ‰å¯¹å®ƒåšä»»ä½•æ›´æ”¹ï¼Œå¹¶ä¸”ä½ æ€»æ˜¯ä¼šæœ‰ä¸€ä¸ªæœ¬åœ°å‰¯æœ¬ï¼Œå› ä¸ºè‡ªåŠ¨æ ‡è®°å™¨ä¼šä»é¢„è®­ç»ƒä¸­ç¼“å­˜æ–‡ä»¶ï¼Œä»¥é¿å…ä½ å†æ¬¡ä¸‹è½½å®ƒä»¬ï¼Œå› æ­¤æ²¡æœ‰ç†ç”±å»ä¿å­˜ã€‚å”¯ä¸€çš„ä¾‹å¤–æ˜¯å½“ä½ åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹å¹¶å¸Œæœ›æ¨é€åˆ°æ¨¡å‹åº“æ—¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥å°†ä½ çš„æ ‡è®°å™¨ä¿å­˜åœ¨è¯¥æ–‡ä»¶å¤¹ä¸­ï¼Œè¿™æ ·å½“ä½ æ¨é€ç”¨æˆ·æ—¶ï¼Œä½ æ¨é€çš„æ˜¯ä½ çš„æ¨¡å‹ã€é…ç½®å’Œä¸ä¹‹ä½¿ç”¨çš„æ ‡è®°å™¨ï¼Œæˆ‘ä»¬æœ‰æ‰€æœ‰è¿™ä¸‰è€…ã€‚
- en: The the the A face website is going to be able to apply in front APII in your
    model and you will be able to play with the Wichat onlineã€‚other than that you
    won't really need to use the safe pretrain method on the tokenerã€‚ it's mostly
    for the model that's going to be super useful or and we will see part two closer
    to do that if you're training a tokener from scratch because you're pretraining
    a modelã€‚ for instanceï¼Œ in a new languageï¼Œ then youll need to use the safe pretrain
    method to save the result of your tokenerã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Aé¢ç½‘ç«™å°†èƒ½å¤Ÿåœ¨ä½ çš„æ¨¡å‹å‰åº”ç”¨APIï¼Œä½ å°†èƒ½å¤Ÿåœ¨çº¿ç©Wichatã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä½ çœŸçš„ä¸éœ€è¦åœ¨æ ‡è®°å™¨ä¸Šä½¿ç”¨å®‰å…¨é¢„è®­ç»ƒæ–¹æ³•ã€‚è¿™ä¸»è¦æ˜¯é’ˆå¯¹å°†éå¸¸æœ‰ç”¨çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨ç¨åæ¥è¿‘æ­¤éƒ¨åˆ†æ—¶çœ‹åˆ°ã€‚å¦‚æœä½ ä»å¤´å¼€å§‹è®­ç»ƒæ ‡è®°å™¨ï¼Œå› ä¸ºä½ æ­£åœ¨é¢„è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæ¯”å¦‚è¯´åœ¨ä¸€ç§æ–°è¯­è¨€ä¸­ï¼Œé‚£ä¹ˆä½ éœ€è¦ä½¿ç”¨å®‰å…¨é¢„è®­ç»ƒæ–¹æ³•æ¥ä¿å­˜ä½ çš„æ ‡è®°å™¨ç»“æœã€‚
- en: So we're going to watch the last video for today live session about batching
    inputs to cover and then we'll look more crisly at the good toã€‚ let me justã€‚Launch
    the collab first so that we don't have to wait after the video and then we'll
    watch the video togetherã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å°†è§‚çœ‹ä»Šå¤©çš„æœ€åä¸€ä¸ªè§†é¢‘ï¼Œå…³äºæ‰¹å¤„ç†è¾“å…¥çš„ç°åœºä¼šè®®ï¼Œç„¶åæˆ‘ä»¬å°†æ›´ä»”ç»†åœ°çœ‹ä¸€ä¸‹å¥½çš„éƒ¨åˆ†ã€‚è®©æˆ‘å…ˆå¯åŠ¨åä½œå·¥å…·ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸ç”¨åœ¨è§†é¢‘ä¹‹åç­‰å¾…ï¼Œç„¶åæˆ‘ä»¬ä¸€èµ·è§‚çœ‹è§†é¢‘ã€‚
- en: å—¯ã€‚Come onã€‚Sthã€‚Yesï¼Œ I want to run itã€‚And let me grab the videoï¼Œ patching and
    put togetherã€‚![](img/0e882caa799deede0a88d67b58737bfe_43.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æ¥å§ã€‚æŸäº›äº‹æƒ…ã€‚æ˜¯çš„ï¼Œæˆ‘æƒ³è¿è¡Œå®ƒã€‚è®©æˆ‘æŠ“å–è§†é¢‘ã€æ‰“è¡¥ä¸å¹¶æ•´ç†åœ¨ä¸€èµ·ã€‚![](img/0e882caa799deede0a88d67b58737bfe_43.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_44.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_44.png)'
- en: å—¯ã€‚Yeahã€‚How to batch inputs together in this videoï¼Œ well see how2 batch input
    sequences togetherã€‚In general allï¼Œ the sentences we want to pass through our model
    won't all have the same lengthã€‚Here we are using the model we saw in the sentiment
    analysis pipeline and want to classify two sentencesã€‚When tokenizing them and
    mapping each token to its corresponding input Iã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æ˜¯çš„ã€‚åœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œå¦‚ä½•å°†è¾“å…¥ä¸€èµ·æ‰¹å¤„ç†ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†è¾“å…¥åºåˆ—æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡æ¨¡å‹ä¼ é€’çš„æ‰€æœ‰å¥å­ä¸ä¼šéƒ½æ˜¯ç›¸åŒçš„é•¿åº¦ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æˆ‘ä»¬åœ¨æƒ…æ„Ÿåˆ†æç®¡é“ä¸­çœ‹åˆ°çš„æ¨¡å‹ï¼Œå¹¶å¸Œæœ›å¯¹ä¸¤ä¸ªå¥å­è¿›è¡Œåˆ†ç±»ã€‚å½“å¯¹å®ƒä»¬è¿›è¡Œæ ‡è®°åŒ–å¹¶å°†æ¯ä¸ªæ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”çš„è¾“å…¥æ—¶ã€‚
- en: we get two lists of different lengthã€‚Trying to create a densor or an newbi array
    from the two will result in an error because all arrays and densilrs should be
    a recangroã€‚One way to overcome this limit is to make the second sentence the same
    length at the first by adding a special token as many times as necessaryã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°ä¸¤ä¸ªä¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚ä»è¿™ä¸¤ä¸ªåˆ—è¡¨åˆ›å»ºä¸€ä¸ªå¼ é‡æˆ–æ–°çš„æ•°ç»„å°†å¯¼è‡´é”™è¯¯ï¼Œå› ä¸ºæ‰€æœ‰æ•°ç»„å’Œå¼ é‡åº”æ˜¯è§„åˆ™çš„ã€‚å…‹æœæ­¤é™åˆ¶çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡æ·»åŠ ç‰¹æ®Šæ ‡è®°ä½¿ç¬¬äºŒä¸ªå¥å­çš„é•¿åº¦ä¸ç¬¬ä¸€ä¸ªå¥å­ç›¸åŒï¼Œç›´åˆ°éœ€è¦çš„æ¬¡æ•°ã€‚
- en: Another way would be to truk the first sequence to the length of the secondã€‚But
    we would then lose a lot of information that may be necessary to properly classify
    the sentenceã€‚In generalï¼Œ we only truncate sentences when we are longer than the
    maximum length the model can handleã€‚The value used to pad the circums should not
    be picked randomlyã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ–¹æ³•æ˜¯å°†ç¬¬ä¸€ä¸ªåºåˆ—æˆªæ–­åˆ°ç¬¬äºŒä¸ªçš„é•¿åº¦ã€‚ä½†è¿™æ ·æˆ‘ä»¬å°†å¤±å»è®¸å¤šå¯èƒ½å¯¹æ­£ç¡®åˆ†ç±»å¥å­è‡³å…³é‡è¦çš„ä¿¡æ¯ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åªåœ¨å¥å­è¶…è¿‡æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§é•¿åº¦æ—¶æ‰ä¼šæˆªæ–­å¥å­ã€‚ç”¨äºå¡«å……çš„å€¼ä¸åº”éšæ„é€‰æ‹©ã€‚
- en: The model has been portrayed with a certain padding IDï¼Œ which you can find in
    tokenizerã€‚pa tokeniteã€‚Now that we have better sentencesï¼Œ we can make a batch with
    themã€‚If we pass the two sentences to the model separately and patched together
    howeverã€‚ we notice that we don't get the same results for the sentence that is
    pad here the second one is that the bug in the transformerers library now if you
    remember that transformers will all make easy use of attention layersã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å·²ç»è¢«æç»˜ä¸ºå…·æœ‰ç‰¹å®šçš„å¡«å…… IDï¼Œä½ å¯ä»¥åœ¨ tokenizer ä¸­æ‰¾åˆ°ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ›´å¥½çš„å¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒä»¬åˆ›å»ºä¸€ä¸ªæ‰¹æ¬¡ã€‚å¦‚æœæˆ‘ä»¬å•ç‹¬å°†ä¸¤ä¸ªå¥å­ä¼ é€’ç»™æ¨¡å‹å¹¶åˆå¹¶åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬ä¼šæ³¨æ„åˆ°ï¼Œå¯¹äºå¡«å……çš„å¥å­ï¼Œå³ç¬¬äºŒä¸ªå¥å­ï¼Œæˆ‘ä»¬æ²¡æœ‰å¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œè¿™æ˜¯
    transformer åº“ä¸­çš„ä¸€ä¸ªé”™è¯¯ã€‚å¦‚æœä½ è®°å¾—ï¼Œtransformers ä¼šè½»æ¾ä½¿ç”¨æ³¨æ„åŠ›å±‚ã€‚
- en: this should not come as a total surpriseã€‚When computing is the contextual representation
    of each tokenã€‚The attention layers look at all the other words in the sentenceã€‚If
    you have just a sentence or the sentence with soball padic tos addedã€‚ each logicalal
    don't get the same valuesã€‚To get the same results with or without paddingã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸åº”è®©äººæ„Ÿåˆ°å®Œå…¨æ„å¤–ã€‚å½“è®¡ç®—æ¯ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºæ—¶ï¼Œæ³¨æ„åŠ›å±‚ä¼šæŸ¥çœ‹å¥å­ä¸­çš„æ‰€æœ‰å…¶ä»–å•è¯ã€‚å¦‚æœä½ åªæœ‰ä¸€ä¸ªå¥å­æˆ–å¥å­ä¸å¡«å……æ ‡è®°ï¼Œæ¯ä¸ªé€»è¾‘æ ‡è®°éƒ½ä¸ä¼šè·å¾—ç›¸åŒçš„å€¼ã€‚è¦è·å¾—ç›¸åŒçš„ç»“æœï¼Œæ— è®ºæ˜¯å¦æœ‰å¡«å……ã€‚
- en: we need to indicate to the attention layers that we should ignore those padding
    ticketsã€‚This is done by creating an attention maskï¼Œ a tonsil with the same shape
    as the input IDs with series and onesã€‚Once indicates the tokens the attention
    layers should consider in the contextã€‚ and the the tokens which should ignoreã€‚Nowï¼Œ
    passing this attention mask along with the input ID will give us the same results
    as when we send the two sentences individually to the modelã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å‘æ³¨æ„åŠ›å±‚æŒ‡ç¤ºå¿½ç•¥é‚£äº›å¡«å……æ ‡è®°ã€‚è¿™æ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªæ³¨æ„åŠ›æ©ç å®Œæˆçš„ï¼Œè¯¥æ©ç ä¸è¾“å…¥ ID å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼ŒåŒ…å«é›¶å’Œä¸€ã€‚ä¸€æ—¦æŒ‡ç¤ºäº†æ³¨æ„åŠ›å±‚åº”è€ƒè™‘çš„æ ‡è®°ï¼Œä»¥åŠåº”å¿½ç•¥çš„æ ‡è®°ã€‚ç°åœ¨ï¼Œå°†æ­¤æ³¨æ„åŠ›æ©ç ä¸è¾“å…¥
    ID ä¸€èµ·ä¼ é€’å°†ç»™æˆ‘ä»¬ä¸å•ç‹¬å°†ä¸¤ä¸ªå¥å­å‘é€ç»™æ¨¡å‹æ—¶ç›¸åŒçš„ç»“æœã€‚
- en: This is all done behind the scenes by the tokenizer when you apply to several
    sentences with a flag bedding equal trueã€‚It will apply his bedding with a proper
    value to the smaller sentences and create the appropriate attention maskã€‚![](img/0e882caa799deede0a88d67b58737bfe_46.png)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€åˆ‡éƒ½æ˜¯ç”± tokenizer åœ¨åå°å®Œæˆçš„ï¼Œå½“ä½ å°†å‡ ä¸ªå¥å­åº”ç”¨äºæ ‡å¿—å¡«å……ç­‰äºçœŸæ—¶ã€‚å®ƒä¼šä¸ºè¾ƒå°çš„å¥å­åº”ç”¨é€‚å½“çš„å¡«å……å€¼ï¼Œå¹¶åˆ›å»ºé€‚å½“çš„æ³¨æ„åŠ›æ©ç ã€‚![](img/0e882caa799deede0a88d67b58737bfe_46.png)
- en: '![](img/0e882caa799deede0a88d67b58737bfe_47.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_47.png)'
- en: ã€‚Soï¼Œ let'sã€‚Look at the same thing in collabã€‚Let for any questionsã€‚ noï¼Œ I don't
    see any questionsã€‚ don't hesitate to ask your questions in the chat againã€‚ and
    let's look at the same code that we had to look again at what the padding and
    attention mask are exactlyã€‚So as we saw in the videoï¼Œ if we try toã€‚Apply our modelã€‚Directlyã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬ã€‚çœ‹çœ‹åœ¨åä½œä¸­çš„ç›¸åŒå†…å®¹ã€‚æœ‰ä»€ä¹ˆé—®é¢˜è¯·é—®ã€‚æ²¡æœ‰ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•é—®é¢˜ã€‚è¯·éšæ—¶åœ¨èŠå¤©ä¸­å†æ¬¡æå‡ºä½ çš„é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å†æ¬¡çœ‹çœ‹æˆ‘ä»¬éœ€è¦å…³æ³¨çš„ä»£ç ï¼Œå…·ä½“æ¥è¯´æ˜¯ä»€ä¹ˆå¡«å……å’Œæ³¨æ„åŠ›æ©ç ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ã€‚ç›´æ¥åº”ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: Oh no it it not take the exact same thing as in the videoã€‚ If we try to apply
    our model directly on just one sentence that we recognize and converted to to
    ideas like that using the the same code as in the previous videoã€‚ it's gonna fail
    because the model wants batches of input so it wantsã€‚It's actually the dokenizer
    even if you have just one sentence is adding one dimensionã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œä¸ï¼Œå®ƒå¹¶æ²¡æœ‰åƒè§†é¢‘ä¸­é‚£æ ·ç²¾ç¡®ã€‚å¦‚æœæˆ‘ä»¬ç›´æ¥åœ¨ä¸€ä¸ªè¢«è¯†åˆ«å¹¶è½¬æ¢ä¸º ID çš„å¥å­ä¸Šåº”ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä½¿ç”¨ä¸å‰ä¸€ä¸ªè§†é¢‘ç›¸åŒçš„ä»£ç ï¼Œå®ƒä¼šå¤±è´¥ï¼Œå› ä¸ºæ¨¡å‹æƒ³è¦è¾“å…¥çš„æ‰¹æ¬¡ã€‚å› æ­¤ï¼Œå®é™…ä¸Šå³ä½¿ä½ åªæœ‰ä¸€ä¸ªå¥å­ï¼Œtokenizer
    ä¹Ÿä¼šæ·»åŠ ä¸€ä¸ªç»´åº¦ã€‚
- en: you can see like there are two pairs of brackets surrounding that so here this
    is a tonsor of shape1 by a guess 60ã€‚And so if you want to pass just one sentence
    that you processed manually to a modelã€‚ you have two had one dimensionï¼Œ for instanceï¼Œ
    by adding a pair of brackets hereã€‚This is for a separate exampleï¼Œ so now looking
    at two sentences togetherï¼Œ so if we have two listsã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°æœ‰ä¸¤ä¸ªæ‹¬å·åŒ…å›´ç€è¿™ä¸ªï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º1ä¹˜60çš„å¼ é‡ã€‚å¦‚æœä½ æƒ³å°†æ‰‹åŠ¨å¤„ç†çš„å•ä¸ªå¥å­ä¼ é€’ç»™æ¨¡å‹ï¼Œä½ éœ€è¦å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œæ·»åŠ ä¸€å¯¹æ‹¬å·ã€‚è¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„ç¤ºä¾‹ï¼Œç°åœ¨æ¥çœ‹ä¸¤ä¸ªå¥å­ä¸€èµ·ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªåˆ—è¡¨ã€‚
- en: so let's say we have both id and both ID and we want to make a pair of sentencesã€‚We
    can't we can create a list of lists like thatï¼Œ but we can't create an array with
    the two of them because then don't have the same shapeã€‚So we need to add a padding
    index so we could ever hide it at the handle at the beginningã€‚ most of the Transal
    model expects the padding to be applied on the right with the exception of ExcelNe
    which expects it at the beginningã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬åŒæ—¶æœ‰idå’ŒIDï¼Œå¹¶ä¸”æƒ³è¦å½¢æˆä¸€å¯¹å¥å­ã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œä½†ä¸èƒ½åˆ›å»ºä¸€ä¸ªåŒ…å«è¿™ä¸¤ä¸ªå¥å­çš„æ•°ç»„ï¼Œå› ä¸ºå®ƒä»¬çš„å½¢çŠ¶ä¸ä¸€è‡´ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªå¡«å……ç´¢å¼•ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨å¼€å¤´éšè—å®ƒã€‚å¤§å¤šæ•°Transalæ¨¡å‹æœŸæœ›å¡«å……åº”ç”¨åœ¨å³ä¾§ï¼ŒExcelNeåˆ™æœŸæœ›åœ¨å¼€å¤´ã€‚
- en: but the tokenizer should be responsible to apply the padding because the tokenizer
    knows what the model wants and is going to apply it on the right sideã€‚å—¯ã€‚So once
    you have thosepat Iï¼Œ you can createã€‚You can sorryã€‚ create done source from them
    and then pass them through the modelã€‚And if we look like we we've just done in
    the video at the outputsã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†tokenizeråº”è¯¥è´Ÿè´£åº”ç”¨å¡«å……ï¼Œå› ä¸ºtokenizerçŸ¥é“æ¨¡å‹éœ€è¦ä»€ä¹ˆï¼Œå¹¶å°†å…¶åº”ç”¨åœ¨å³ä¾§ã€‚æ‰€ä»¥ä¸€æ—¦ä½ æœ‰äº†è¿™äº›pat Iï¼Œä½ å°±å¯ä»¥åˆ›å»ºã€‚å¯¹ä¸èµ·ï¼Œä»ä¸­åˆ›å»ºæºï¼Œç„¶åä¼ é€’ç»™æ¨¡å‹ã€‚å¦‚æœæˆ‘ä»¬åƒåœ¨è§†é¢‘ä¸­çœ‹åˆ°çš„é‚£æ ·æŸ¥çœ‹è¾“å‡ºã€‚
- en: if we pass the sentences separatelyï¼Œ So we still have to add power brackets
    if we want the model to be applied on them because the model expects a batchã€‚
    So the batch can have just one thing inside itï¼Œ but it needs to be something that
    has two dimensionã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åˆ†åˆ«ä¼ é€’å¥å­ï¼Œä»ç„¶éœ€è¦æ·»åŠ æ‹¬å·ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›æ¨¡å‹å¯¹å®ƒä»¬è¿›è¡Œå¤„ç†ï¼Œå› ä¸ºæ¨¡å‹æœŸæœ›ä¸€ä¸ªæ‰¹æ¬¡ã€‚å› æ­¤ï¼Œæ‰¹æ¬¡å†…éƒ¨å¯ä»¥åªæœ‰ä¸€é¡¹ï¼Œä½†å®ƒéœ€è¦æ˜¯å…·æœ‰ä¸¤ä¸ªç»´åº¦çš„ä¸œè¥¿ã€‚
- en: So if we pass sequence1 and sequence 2ï¼Œ we get those two results and if we pass
    the patch with the two sentencesã€‚ we get the same result for the first sentenceï¼Œ
    but different result for the second sentence we can see that Ph2 are difference
    here which is because the circumst with a paddingken if we don't do anything special
    the model is not going to compute the same results it's not going to properly
    ignore the paddingken so that's so in the video is because of the attention layers
    the attention layers are call the transformers model and are we explain that at
    length during chapter1 there are layers that not computer presentation not just
    of one word but one word within its context because the transformer models were
    designed for translation at the very beginning and if we want to translate the
    word we don't just need to pay attention to that word but all the words around
    it for instance to know which genders the word hardã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ä¼ é€’sequence1å’Œsequence2ï¼Œå°±ä¼šå¾—åˆ°è¿™ä¸¤ä¸ªç»“æœï¼›å¦‚æœæˆ‘ä»¬ä¼ é€’è¿™ä¸¤ä¸ªå¥å­çš„patchï¼Œç¬¬ä¸€æ¬¡å¥å­çš„ç»“æœæ˜¯ç›¸åŒçš„ï¼Œä½†ç¬¬äºŒå¥çš„ç»“æœä¸åŒã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„Ph2æœ‰æ‰€ä¸åŒï¼Œè¿™æ˜¯ç”±äºå¡«å……çš„ä¸Šä¸‹æ–‡ã€‚å¦‚æœæˆ‘ä»¬ä¸åšç‰¹åˆ«å¤„ç†ï¼Œæ¨¡å‹ä¸ä¼šè®¡ç®—å‡ºç›¸åŒçš„ç»“æœï¼Œå®ƒä¸ä¼šæ­£ç¡®å¿½ç•¥å¡«å……ã€‚æ‰€ä»¥åœ¨è§†é¢‘ä¸­æåˆ°çš„å°±æ˜¯ç”±äºæ³¨æ„åŠ›å±‚ï¼Œæ³¨æ„åŠ›å±‚è¢«ç§°ä¸ºtransformersæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸€ç« è¯¦ç»†è§£é‡Šè¿‡ã€‚è¿™äº›å±‚ä¸ä»…è®¡ç®—å•è¯çš„è¡¨ç¤ºï¼Œè¿˜è®¡ç®—å•è¯åœ¨å…¶ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç¤ºï¼Œå› ä¸ºtransformeræ¨¡å‹ä¸€å¼€å§‹å°±æ˜¯ä¸ºç¿»è¯‘è®¾è®¡çš„ã€‚å¦‚æœæˆ‘ä»¬æƒ³ç¿»è¯‘ä¸€ä¸ªå•è¯ï¼Œå°±ä¸ä»…éœ€è¦å…³æ³¨é‚£ä¸ªå•è¯ï¼Œè¿˜éœ€è¦å…³æ³¨å‘¨å›´çš„æ‰€æœ‰å•è¯ï¼Œä¾‹å¦‚äº†è§£â€œhardâ€çš„æ€§åˆ«ã€‚
- en: It's a singular or parole knownï¼Œ things like thatã€‚And so if we don't say to
    if we don't tell the attention layer that this is not a real token it just affect
    token we needed to to have a rectangle and make a batch the attention layer is
    going to pay attention to the token and computer contextual representation that
    adds that token into account So to tell the attention layer now that's not a real
    token don't pay attention to it we have to create what is called an attention
    mask so here we put ones where we want the attention layer to pay attention and
    zero where we want the attention layer to ignore so is the same shape as the batch
    adss that we are using and the zero is put where we have the token I token IDã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ˜¯ä¸€ä¸ªå•ä¸€çš„æˆ–å·²çŸ¥çš„ç¬¦å·ä¹‹ç±»çš„ä¸œè¥¿ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ä¸å‘Šè¯‰æ³¨æ„åŠ›å±‚è¿™ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„æ ‡è®°ï¼Œè€Œä»…ä»…æ˜¯ä¸€ä¸ªå½±å“æ ‡è®°ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªçŸ©å½¢æ¥å½¢æˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ³¨æ„åŠ›å±‚å°†ä¼šå…³æ³¨è¿™ä¸ªæ ‡è®°å¹¶è®¡ç®—ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œè¿™æ ·å°±è€ƒè™‘äº†è¯¥æ ‡è®°ã€‚ä¸ºäº†å‘Šè¯‰æ³¨æ„åŠ›å±‚è¿™ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„æ ‡è®°ï¼Œä¸è¦å…³æ³¨å®ƒï¼Œæˆ‘ä»¬å¿…é¡»åˆ›å»ºæ‰€è°“çš„æ³¨æ„åŠ›æ©ç ï¼Œå› æ­¤åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨æƒ³è¦æ³¨æ„åŠ›å±‚å…³æ³¨çš„åœ°æ–¹æ”¾ç½®1ï¼Œè€Œåœ¨æƒ³è¦æ³¨æ„åŠ›å±‚å¿½ç•¥çš„åœ°æ–¹æ”¾ç½®0ï¼Œå› æ­¤å®ƒä¸æˆ‘ä»¬ä½¿ç”¨çš„æ‰¹æ¬¡çš„å½¢çŠ¶ç›¸åŒï¼Œè€Œ0æ”¾ç½®åœ¨æˆ‘ä»¬æœ‰æ ‡è®°IDçš„åœ°æ–¹ã€‚
- en: And if we do that and pass those through the model now we see that we get the
    same outputã€‚ so here the first sentence is still the same because there was not
    beddingã€‚ but this output here is the same as this output hereã€‚ which was the output
    of the model with the second sentenceã€‚So this is what's done by the tokenizerã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¿™æ ·åšå¹¶å°†è¿™äº›ä¼ é€’ç»™æ¨¡å‹ï¼Œç°åœ¨æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°äº†ç›¸åŒçš„è¾“å‡ºã€‚å› æ­¤ï¼Œç¬¬ä¸€ä¸ªå¥å­ä»ç„¶æ˜¯ç›¸åŒçš„ï¼Œå› ä¸ºæ²¡æœ‰å¡«å……ï¼Œä½†è¿™é‡Œçš„è¾“å‡ºä¸è¿™é‡Œçš„è¾“å‡ºæ˜¯ç›¸åŒçš„ï¼Œåè€…æ˜¯æ¨¡å‹å¤„ç†ç¬¬äºŒä¸ªå¥å­çš„è¾“å‡ºã€‚è¿™å°±æ˜¯æ ‡è®°å™¨æ‰€å®Œæˆçš„å·¥ä½œã€‚
- en: if you remember when we were passinging all two sentencesï¼Œ it was returning
    something with two keysã€‚ one was the input Is which correspond to this thing hereã€‚
    and therefore one was the attention mask which corresponds to this thing hereã€‚The
    truncation argumentï¼Œ so when we passed the two sentences to auto at the very beginningã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬ä¼ é€’çš„ä¸¤ä¸ªå¥å­ï¼Œå®ƒè¿”å›äº†å¸¦æœ‰ä¸¤ä¸ªé”®çš„å†…å®¹ã€‚ä¸€ä¸ªæ˜¯è¾“å…¥ï¼Œå®ƒå¯¹åº”äºè¿™é‡Œçš„å†…å®¹ï¼Œå¦ä¸€ä¸ªæ˜¯æ³¨æ„åŠ›æ©ç ï¼Œå®ƒå¯¹åº”äºè¿™é‡Œçš„å†…å®¹ã€‚æˆªæ–­å‚æ•°ï¼Œå› æ­¤å½“æˆ‘ä»¬å°†è¿™ä¸¤ä¸ªå¥å­ä¼ é€’ç»™è‡ªåŠ¨æ¨¡å‹æ—¶ï¼Œæ­£æ˜¯è¿™æ ·ã€‚
- en: we said padding equal true and truncation equal2ï¼Œ so truncation equal true is
    going to truncate very very long sentences because for be modelã€‚ for instanceï¼Œ
    can only handle sequences that have a length of 512 maximumã€‚So if we have a sequence
    that's longer than thatï¼Œ the model is going to fail and we need to truncate itã€‚
    you shouldn't truncate your input for any other reason when making them shorter
    than the maximum length the model can handle otherwise because when you trun sorry
    you remove information so for padding we are adding something and we can tell
    the attention layer to ignore it so at the end we get the exact same results with
    or without padding with truncation you're ignoring information but you just can't
    recover so you will never be able to get the same results without truncationã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯´å¡«å……ä¸ºçœŸï¼Œæˆªæ–­ä¸º2ï¼Œæ‰€ä»¥æˆªæ–­ä¸ºçœŸå°†ä¼šæˆªæ–­éå¸¸éå¸¸é•¿çš„å¥å­ï¼Œå› ä¸ºå¯¹äºæ¨¡å‹æ¥è¯´ï¼Œä¾‹å¦‚ï¼Œåªèƒ½å¤„ç†é•¿åº¦æœ€å¤§ä¸º512çš„åºåˆ—ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¯”è¿™æ›´é•¿çš„åºåˆ—ï¼Œæ¨¡å‹å°†ä¼šå¤±è´¥ï¼Œæˆ‘ä»¬éœ€è¦æˆªæ–­å®ƒã€‚åœ¨ä½¿è¾“å…¥çŸ­äºæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œä¸åº”è¯¥å› ä¸ºå…¶ä»–åŸå› è€Œè¿›è¡Œæˆªæ–­ï¼Œå› ä¸ºå½“ä½ æˆªæ–­æ—¶ï¼Œä¼šä¸¢å¤±ä¿¡æ¯ï¼›è€Œå¯¹äºå¡«å……ï¼Œæˆ‘ä»¬æ˜¯æ·»åŠ ä¸€äº›ä¸œè¥¿ï¼Œå¹¶ä¸”å¯ä»¥å‘Šè¯‰æ³¨æ„åŠ›å±‚å¿½ç•¥å®ƒï¼Œå› æ­¤æœ€åæˆ‘ä»¬å¾—åˆ°çš„ç»“æœä¸æ˜¯å¦å¡«å……æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚é€šè¿‡æˆªæ–­ï¼Œä½ æ˜¯åœ¨å¿½ç•¥ä¿¡æ¯ï¼Œä½†ä½ æ— æ³•æ¢å¤ï¼Œæ‰€ä»¥æ²¡æœ‰æˆªæ–­çš„æƒ…å†µä¸‹ï¼Œä½ å°†æ°¸è¿œæ— æ³•å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚
- en: But sadly it's needed because transformers have a maximum lengthã€‚ so if you
    have very long inputs that are greater than than at maximum length you need to
    locate themã€‚And if we go back looking at the course in the putting allã€‚Putting
    it all sectionã€‚We have the various patting on from strategyã€‚ So hereã€‚
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é—æ†¾çš„æ˜¯ï¼Œè¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå˜å‹å™¨æœ‰æœ€å¤§é•¿åº¦ã€‚å› æ­¤ï¼Œå¦‚æœä½ æœ‰éå¸¸é•¿çš„è¾“å…¥è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œä½ éœ€è¦å°†å…¶å®šä½ã€‚å¦‚æœæˆ‘ä»¬å›åˆ°è¯¾ç¨‹ä¸­å¹¶æŸ¥çœ‹â€œæŠŠæ‰€æœ‰ä¸œè¥¿æ”¾åœ¨ä¸€èµ·â€çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬æœ‰å„ç§å¡«å……ç­–ç•¥ã€‚å› æ­¤åœ¨è¿™é‡Œã€‚
- en: So if we go back with the two sentences we are using at the very beginning of
    this left session and pass them we can pass them to the skenerã€‚ which is going
    to then output list of listï¼Œ but we won't be able to patch them together without
    applying padding so we have various strategies to apply padding when we do padding
    equal through it's the same as doing padding equal longest which is going to pad
    the sequences up to the maximum length inside the samples that you're passing
    so hereã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœæˆ‘ä»¬å›åˆ°ä¸€å¼€å§‹è¿™æ®µå¯¹è¯ä¸­ä½¿ç”¨çš„ä¸¤ä¸ªå¥å­å¹¶ä¼ é€’å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä¼ é€’ç»™ç­›é€‰å™¨ï¼Œå®ƒå°†è¾“å‡ºä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œä½†æˆ‘ä»¬æ— æ³•å°†å®ƒä»¬æ‹¼æ¥åœ¨ä¸€èµ·è€Œä¸åº”ç”¨å¡«å……ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰å„ç§ç­–ç•¥æ¥åº”ç”¨å¡«å……ã€‚å½“æˆ‘ä»¬å¡«å……ä¸ºçœŸæ—¶ï¼Œå®ƒä¸å¡«å……ä¸ºæœ€é•¿æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯å°†åºåˆ—å¡«å……åˆ°ä½ æ‰€ä¼ é€’çš„æ ·æœ¬ä¸­çš„æœ€å¤§é•¿åº¦ã€‚
- en: We have two sequences with patting equal longestï¼Œ it's going to add the second
    sentencesã€‚ the second sentence here to the length of the longer sentenceã€‚ If we
    go back to the notebook behind pipelineï¼Œ we can see that hereã€‚With all those zeros
    being added so that the circum sentence inside the batch is the exact same length
    at the firstã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸¤ä¸ªåºåˆ—ï¼Œå¡«å……ç­‰äºæœ€é•¿ï¼Œè¿™ä¼šå°†ç¬¬äºŒä¸ªå¥å­æ·»åŠ åˆ°è¾ƒé•¿å¥å­çš„é•¿åº¦ã€‚å¦‚æœæˆ‘ä»¬å›åˆ°ç®¡é“åé¢çš„ç¬”è®°æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œã€‚æ‰€æœ‰è¿™äº›é›¶è¢«æ·»åŠ ï¼Œä»¥ç¡®ä¿æ‰¹å¤„ç†ä¸­çš„å¥å­é•¿åº¦ä¸ç¬¬ä¸€ä¸ªå¥å­å®Œå…¨ç›¸åŒã€‚
- en: You can also say patting equal max length max length refer to the model max
    lengthï¼Œ so for instanceã€‚ it's 512 for be or distberï¼Œ so this is going to add every
    sentence to 512 the maximum length of the model of adoption should not be considered
    whenã€‚When you have short sentencesï¼Œ for instanceï¼Œ its in general it's better to
    add to the longest thing you have unless you need to use fixed shaped for some
    reasonã€‚ for instanceï¼Œ TPUus like fixed shapes better in which case you would use
    the max length option and which is a bit inefficient but for instance with the
    TPU acceleratorã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥è¯´å¡«å……ç­‰äºæœ€å¤§é•¿åº¦ï¼Œæœ€å¤§é•¿åº¦æŒ‡çš„æ˜¯æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œæ‰€ä»¥ä¾‹å¦‚ï¼Œå¯¹äºbeæˆ–distberï¼Œå®ƒæ˜¯512ï¼Œå› æ­¤è¿™ä¼šå°†æ¯ä¸ªå¥å­æ·»åŠ åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦512ã€‚å¯¹äºçŸ­å¥å­è€Œè¨€ï¼Œæ¨¡å‹çš„é‡‡ç”¨ä¸åº”è¢«è§†ä¸ºå›ºå®šå½¢çŠ¶ï¼Œé€šå¸¸æœ€å¥½æ˜¯æ·»åŠ åˆ°ä½ æ‹¥æœ‰çš„æœ€é•¿å¥å­ï¼Œé™¤éå‡ºäºæŸç§åŸå› éœ€è¦ä½¿ç”¨å›ºå®šå½¢çŠ¶ã€‚ä¾‹å¦‚ï¼ŒTPUæ›´å–œæ¬¢å›ºå®šå½¢çŠ¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä½ ä¼šä½¿ç”¨æœ€å¤§é•¿åº¦é€‰é¡¹ï¼Œå°½ç®¡è¿™æœ‰ç‚¹ä½æ•ˆï¼Œä½†å¯¹äºTPUåŠ é€Ÿå™¨è€Œè¨€æ˜¯å¦‚æ­¤ã€‚
- en: it's the only way to get some real speed because they need all the inputs towards
    the exact same shapeã€‚And you can also specify a max length that you wantï¼Œ so for
    instanceã€‚ if you know that in your dataset setï¼Œ all your sentences are shorter
    than 1 around28ã€‚ you could say patting equal max length max length equal 1028ã€‚And
    so you should also use truncationã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è·å¾—çœŸå®é€Ÿåº¦çš„å”¯ä¸€æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦æ‰€æœ‰è¾“å…¥å…·æœ‰å®Œå…¨ç›¸åŒçš„å½¢çŠ¶ã€‚ä½ ä¹Ÿå¯ä»¥æŒ‡å®šä¸€ä¸ªä½ æƒ³è¦çš„æœ€å¤§é•¿åº¦ï¼Œæ‰€ä»¥ä¾‹å¦‚ï¼Œå¦‚æœä½ çŸ¥é“åœ¨ä½ çš„æ•°æ®é›†ä¸­ï¼Œæ‰€æœ‰å¥å­éƒ½çŸ­äºå¤§çº¦28ï¼Œä½ å¯ä»¥è¯´å¡«å……ç­‰äºæœ€å¤§é•¿åº¦ï¼Œæœ€å¤§é•¿åº¦ç­‰äº1028ã€‚å› æ­¤ï¼Œä½ è¿˜åº”è¯¥ä½¿ç”¨æˆªæ–­ã€‚
- en: as we saidï¼Œ because you need to truncate when your inputs when you there are
    longer than the maximum length a model can handleã€‚ you can also specify the maximum
    length to which you want to truncate your inputsã€‚So that's it for padding onã€‚
    Do we have any questionsã€‚å—¯ã€‚No questionsï¼Œ Small links shared by Lewissã€‚Soã€‚The last
    thing isï¼Œ as we saw beforeï¼Œ you can tell you token to return bytoch tensor and
    soft flu tensor on entire arrayã€‚
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è¯´ï¼Œå› ä¸ºå½“è¾“å…¥è¶…è¿‡æ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œä½ éœ€è¦è¿›è¡Œæˆªæ–­ã€‚ä½ ä¹Ÿå¯ä»¥æŒ‡å®šå¸Œæœ›æˆªæ–­è¾“å…¥çš„æœ€å¤§é•¿åº¦ã€‚æ‰€ä»¥å…³äºå¡«å……å°±è¿™äº›ã€‚æˆ‘ä»¬æœ‰ä»»ä½•é—®é¢˜å—ï¼Ÿå—¯ï¼Œæ²¡æœ‰é—®é¢˜ï¼ŒLewisåˆ†äº«äº†ä¸€äº›å°é“¾æ¥ã€‚æ‰€ä»¥ï¼Œæœ€åä¸€ä»¶äº‹æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œä½ å¯ä»¥å‘Šè¯‰ä½ çš„æ ‡è®°è¿”å›bytochå¼ é‡å’Œè½¯æµå¼ é‡åœ¨æ•´ä¸ªæ•°ç»„ä¸Šã€‚
- en: And as we saw in one of the videosï¼Œ the tokenizerï¼Œ the tokener pipeline video
    specificallyã€‚ the tokenazer adds the special token at the beginning and at the
    end of our sentenceã€‚ so it depends exactly on which model youre usingï¼Œ but the
    tokener will all know what token's special token the model expects and will put
    them at the beginning and at the end of your sentencesã€‚And so to conclude this
    chapterï¼Œ here is the world codeï¼Œ it's just missing the post processingã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å…¶ä¸­ä¸€ä¸ªè§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œç‰¹åˆ«æ˜¯tokenizerè§†é¢‘ï¼Œtokenizerä¼šåœ¨å¥å­çš„å¼€å§‹å’Œç»“æŸæ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚å› æ­¤ï¼Œè¿™å®Œå…¨å–å†³äºä½ ä½¿ç”¨çš„æ¨¡å‹ï¼Œä½†tokenizerä¼šçŸ¥é“æ¨¡å‹æœŸæœ›çš„ç‰¹æ®Šæ ‡è®°ï¼Œå¹¶å°†å®ƒä»¬æ”¾åœ¨å¥å­çš„å¼€å§‹å’Œç»“æŸã€‚ä¸ºæ€»ç»“è¿™ä¸€ç« èŠ‚ï¼Œè¿™é‡Œæ˜¯ä»£ç ï¼Œåªæ˜¯ç¼ºå°‘åå¤„ç†ã€‚
- en: but here is the world code that was executed behind the scene by the pipelineã€‚å—¯ã€‚So
    you import auto tokenizer on automod for sequence classificationã€‚ the auto beam
    meaning that you can use any checkpoint from the hubã€‚ it's going to pick the right
    architecture for youã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™æ˜¯ç®¡é“åå°æ‰§è¡Œçš„ä»£ç ã€‚å—¯ã€‚æ‰€ä»¥ä½ å¯¼å…¥ç”¨äºåºåˆ—åˆ†ç±»çš„auto tokenizerå’Œautomodã€‚auto beamæ„å‘³ç€ä½ å¯ä»¥ä»ä¸­å¿ƒä½¿ç”¨ä»»ä½•æ£€æŸ¥ç‚¹ã€‚å®ƒä¼šä¸ºä½ é€‰æ‹©æ­£ç¡®çš„æ¶æ„ã€‚
- en: The checkpoint that was used by the pipeline is this1ã€‚ you can use any of our
    checkpoint on the model hub that correspond to a sentiment analysis taskã€‚We can
    load the tokenazizer with the from betweentrain methodã€‚ we can load our model
    with the from betweenttrain method and if we have two sentencesã€‚
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“ä½¿ç”¨çš„æ£€æŸ¥ç‚¹æ˜¯this1ã€‚ä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æ¨¡å‹ä¸­å¿ƒä¸Šä»»ä½•å¯¹åº”äºæƒ…æ„Ÿåˆ†æä»»åŠ¡çš„æ£€æŸ¥ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡betweentrainæ–¹æ³•åŠ è½½tokenizerã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡betweenttrainæ–¹æ³•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªå¥å­ã€‚
- en: we can tokenize them together with padding equal2 con equal2 which on tensor
    source equal piy toch if we're using py toch tensor flu which using tensor flu
    and then we pass the tokens to the model and to get the plus process results it's
    just missing the soft max here to have the exact same result as pipelineã€‚
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä¸å¡«å……ä¸€èµ·æ ‡è®°ä¸ºequal2 con equal2ï¼Œè¿™åœ¨å¼ é‡æºä¸Šç­‰äºpiy tochï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨py tochå¼ é‡æµï¼Œç„¶åå°†æ ‡è®°ä¼ é€’ç»™æ¨¡å‹ä»¥è·å¾—åŠ æ³•è¿‡ç¨‹çš„ç»“æœï¼Œè¿™é‡Œåªæ˜¯ç¼ºå°‘soft
    maxï¼Œä»¥ä¾¿å¾—åˆ°ä¸ç®¡é“å®Œå…¨ç›¸åŒçš„ç»“æœã€‚
- en: One last question I'm confused between padding you call longest and dynamic
    paddingã€‚ Is it similarã€‚ Yesï¼Œ it's veryï¼Œ very similarã€‚ Well look at dynamic padding
    in the next chapter when doing trainingã€‚ but for those who don't know what it
    is dynamic padding is when youã€‚When you go through your training data and beL
    batchesã€‚
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘å¯¹ä½ æåˆ°çš„æœ€é•¿å¡«å……å’ŒåŠ¨æ€å¡«å……æ„Ÿåˆ°å›°æƒ‘ã€‚å®ƒä»¬ç›¸ä¼¼å—ï¼Ÿæ˜¯çš„ï¼Œéå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­å­¦ä¹ åŠ¨æ€å¡«å……çš„å†…å®¹ï¼Œä½†å¯¹äºé‚£äº›ä¸çŸ¥é“å®ƒæ˜¯ä»€ä¹ˆçš„äººæ¥è¯´ï¼ŒåŠ¨æ€å¡«å……æ˜¯åœ¨ä½ éå†è®­ç»ƒæ•°æ®æ—¶è¿›è¡Œçš„ã€‚
- en: dynamic padding means each time you have to build a batchã€‚ you pad your sentences
    to the maximum length inside the batchã€‚ and so that's what padding equal longest
    will do if you pass your sentences a small batch of sentences is going to create
    a batch with for instance eight sentences and the second dimension is going to
    be the maximum length inside that batchã€‚Whereas patting equal max length is going
    to add everything to a fixed max lengthã€‚
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€å¡«å……æ„å‘³ç€æ¯æ¬¡ä½ éœ€è¦æ„å»ºä¸€ä¸ªæ‰¹æ¬¡æ—¶ï¼Œä½ å°†å¥å­å¡«å……åˆ°è¯¥æ‰¹æ¬¡å†…çš„æœ€å¤§é•¿åº¦ã€‚å› æ­¤ï¼Œå¦‚æœä½ ä¼ é€’ä¸€å°æ‰¹å¥å­ï¼Œå¡«å……equal longestå°†ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«ä¾‹å¦‚å…«ä¸ªå¥å­çš„æ‰¹æ¬¡ï¼Œç¬¬äºŒä¸ªç»´åº¦å°†æ˜¯è¯¥æ‰¹æ¬¡å†…çš„æœ€å¤§é•¿åº¦ã€‚è€Œå¡«å……equal
    max lengthåˆ™ä¼šå°†æ‰€æœ‰å†…å®¹å¡«å……åˆ°ä¸€ä¸ªå›ºå®šçš„æœ€å¤§é•¿åº¦ã€‚
- en: either the maximum length of model canon all or the maximum length you passed
    along to the dekenizerã€‚And so that's it for the basic use of models and tokenizersã€‚Thank
    you for following the live stream so now that weve seen that chapter together
    you should be able to complete the questionnaire at the hand and then before going
    to chapter 3ã€‚ it's useful if you try to run again the pipeline collab that we
    saw together on the batching input togetherab that we looked at together and try
    to understand what every cell is doing and maybe even try to redo it yourselfã€‚
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæœ€å¤§é•¿åº¦å¯ä»¥æ˜¯æ¨¡å‹æ‰€èƒ½å¤„ç†çš„æœ€å¤§é•¿åº¦ï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ ä¼ é€’ç»™è§£ç å™¨çš„æœ€å¤§é•¿åº¦ã€‚è¿™å°±æ˜¯æ¨¡å‹å’Œæ ‡è®°å™¨çš„åŸºæœ¬ç”¨æ³•ã€‚æ„Ÿè°¢ä½ å…³æ³¨è¿™æ¬¡ç›´æ’­ï¼Œç°åœ¨æˆ‘ä»¬ä¸€èµ·çœ‹å®Œè¿™ä¸€ç« åï¼Œä½ åº”è¯¥èƒ½å¤Ÿå®Œæˆæ‰‹å¤´çš„é—®å·ï¼Œç„¶ååœ¨è¿›å…¥ç¬¬ä¸‰ç« ä¹‹å‰ï¼Œå¦‚æœä½ å°è¯•å†æ¬¡è¿è¡Œæˆ‘ä»¬ä¸€èµ·çœ‹åˆ°çš„æ‰¹å¤„ç†è¾“å…¥çš„pipeline
    colabï¼Œå¹¶è¯•ç€ç†è§£æ¯ä¸ªå•å…ƒçš„åŠŸèƒ½ï¼Œç”šè‡³å°è¯•è‡ªå·±é‡æ–°åšä¸€éï¼Œé‚£å°†æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚
- en: And then try to think of a problem you want to work on on a classification with
    a text classification problem you want to work on and try to use a base model
    and an organizer on that problem to be able to get outputs from some inputs and
    we'll see in the next chapter of to actually find tuneni model on your given problem
    and in chapter 4 we'll see how to upload as a result to the bundle hub so that
    you can venture that model to with the rest of the community and use the widgets
    of the inference API online to have demos ofã€‚
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°è¯•æƒ³ä¸€ä¸ªä½ æƒ³è¦è§£å†³çš„åˆ†ç±»é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œå¹¶å°è¯•åœ¨è¯¥é—®é¢˜ä¸Šä½¿ç”¨åŸºæœ¬æ¨¡å‹å’Œç»„ç»‡å™¨ï¼Œä»¥ä¾¿ä»ä¸€äº›è¾“å…¥ä¸­è·å¾—è¾“å‡ºã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­å®é™…æ‰¾åˆ°é€‚åˆä½ ç»™å®šé—®é¢˜çš„æ¨¡å‹ï¼Œå¹¶åœ¨ç¬¬å››ç« ä¸­çœ‹åˆ°å¦‚ä½•å°†ç»“æœä¸Šä¼ åˆ°bundle
    hubï¼Œä»¥ä¾¿ä½ èƒ½ä¸ç¤¾åŒºå…¶ä»–æˆå‘˜åˆ†äº«è¯¥æ¨¡å‹ï¼Œå¹¶åœ¨çº¿ä½¿ç”¨æ¨ç†APIçš„å°éƒ¨ä»¶è¿›è¡Œæ¼”ç¤ºã€‚
- en: Of your modelã€‚ let me look at the questions before we end this live streamã€‚Ohã€‚
    following up from the previous question with patting equal whole longestã€‚ we don't
    need to use that decorator with paddingã€‚ That's not entirely trueã€‚ And we' all
    see exactly why in the next chapterï¼Œ weã€‚
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºä½ çš„æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬ç»“æŸè¿™æ¬¡ç›´æ’­ä¹‹å‰ï¼Œè®©æˆ‘çœ‹çœ‹é—®é¢˜ã€‚å“¦ï¼Œæ¥ç€ä¸Šä¸€ä¸ªå…³äºå¡«å……equal whole longestçš„é—®é¢˜ã€‚æˆ‘ä»¬ä¸éœ€è¦åœ¨å¡«å……æ—¶ä½¿ç”¨é‚£ä¸ªè£…é¥°å™¨ã€‚è¿™å¹¶ä¸å®Œå…¨æ­£ç¡®ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­ç¡®åˆ‡åœ°çœ‹åˆ°åŸå› ã€‚
- en: we need to be in the dynamic of training a modelï¼Œ butã€‚If you are playing padding
    all longest while you' doing tokenization on your wall data setã€‚ it's going to
    add to the longest element in your data setã€‚ so that's not really the same thing
    as doing dynamic paddingã€‚
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å¤„äºè®­ç»ƒæ¨¡å‹çš„åŠ¨æ€ä¸­ï¼Œä½†å¦‚æœä½ åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šæ‰§è¡Œæœ€é•¿å¡«å……æ—¶è¿›è¡Œæ ‡è®°åŒ–ï¼Œå®ƒå°†å¢åŠ æ•°æ®é›†ä¸­çš„æœ€é•¿å…ƒç´ ã€‚æ‰€ä»¥è¿™ä¸åŠ¨æ€å¡«å……å¹¶ä¸å®Œå…¨ç›¸åŒã€‚
- en: But yeah well we'll dive into that in the next chapterã€‚Otherwiseã€‚ thank you
    all for following this live stream and next week on Cha 3 there are four different
    live streams because Cha 3 is very different for Pytch and Tensorflow since it's
    about training and functioning until now the code add some little differences
    and you can see that you have a switch here for the course if you want to switch
    between Pytch and Tensofflow if you want to look at exactly what' difference but
    for next chapter is going to be very different so you have live session specifically
    for Pythtch and live section specifically for Tensorflow be sure to check on the
    formss which one you want to attend to and market into your calendarã€‚
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« æ·±å…¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚è°¢è°¢å¤§å®¶å…³æ³¨è¿™ä¸ªç›´æ’­ï¼Œä¸‹ä¸€å‘¨åœ¨Cha 3å°†æœ‰å››ä¸ªä¸åŒçš„ç›´æ’­ï¼Œå› ä¸ºCha 3å¯¹äºPytchå’ŒTensorflowéå¸¸ä¸åŒï¼Œä¸»è¦æ˜¯å…³äºè®­ç»ƒå’ŒåŠŸèƒ½ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä»£ç æœ‰ä¸€äº›å°å·®å¼‚ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªåˆ‡æ¢é€‰é¡¹ï¼Œå¯ä»¥åœ¨Pytchå’ŒTensorflowä¹‹é—´åˆ‡æ¢ï¼Œå¦‚æœä½ æƒ³äº†è§£å®ƒä»¬çš„å…·ä½“åŒºåˆ«ã€‚ä¸è¿‡ä¸‹ä¸€ç« å°†ä¼šéå¸¸ä¸åŒï¼Œæ‰€ä»¥ä¼šæœ‰ä¸“é—¨é’ˆå¯¹Pytchçš„ç›´æ’­å’Œä¸“é—¨é’ˆå¯¹Tensorflowçš„ç›´æ’­ï¼ŒåŠ¡å¿…æŸ¥çœ‹ä¸€ä¸‹è¡¨æ ¼ï¼Œé€‰æ‹©ä½ æƒ³å‚åŠ çš„è¯¾ç¨‹ï¼Œå¹¶è®°åœ¨ä½ çš„æ—¥å†ä¸Šã€‚
- en: And yeahï¼Œ let's first this upã€‚ Thanks a lot for comingï¼Œ bye byeã€‚![](img/0e882caa799deede0a88d67b58737bfe_49.png)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œè®©æˆ‘ä»¬å…ˆç»“æŸè¿™ä¸€æ®µã€‚éå¸¸æ„Ÿè°¢å¤§å®¶çš„åˆ°æ¥ï¼Œå†è§ï¼![](img/0e882caa799deede0a88d67b58737bfe_49.png)
- en: ã€‚
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
- en: '![](img/0e882caa799deede0a88d67b58737bfe_51.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e882caa799deede0a88d67b58737bfe_51.png)'
