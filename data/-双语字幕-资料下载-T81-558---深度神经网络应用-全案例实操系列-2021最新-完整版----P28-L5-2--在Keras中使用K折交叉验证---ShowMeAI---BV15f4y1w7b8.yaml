- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P28：L5.2- 在Keras中使用K折交叉验证
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P28：L5.2- 在Keras中使用K折交叉验证
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeff Heaton， welcome to applications of Deep neural networks with
    Washington University In this part。 we're going to take a look at how we can use
    Kfold cross validation to get a good estimate of how our neural network will perform
    on data that is outside of the training set This will allow us to produce out
    of sample predictions that is predictions that are not in the training set for
    the entire training set by simply flipping through the training set and letting
    each piece of the data have a chance to be the validation and the training set。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是杰夫·希顿，欢迎来到华盛顿大学的深度神经网络应用部分。在这一部分，我们将看看如何使用K折交叉验证来很好地估计我们的神经网络在训练集之外的数据上的表现。这将使我们能够产生样本外预测，即对于整个训练集的预测，仅需翻转训练集，让每一部分数据都有机会成为验证集和训练集。
- en: This will let us see how well our regularization and other techniques are combating
    overfitting for the latest on my AI course and projects。 click subscribe and the
    bell next to it to be notified of every new video。 Cross validation often has
    different objectives that you're trying to actually accomplish。 The overall idea
    of kfold cross validation is that you're getting multiple。😊。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这将让我们看到我们的正则化和其他技术在对抗过拟合方面的效果。有关我的AI课程和项目的最新信息，请点击订阅并点击旁边的铃铛，以便在每个新视频发布时获得通知。交叉验证通常有不同的目标，你实际上是在尝试完成的。k折交叉验证的整体思想是你会得到多个。😊
- en: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_1.png)'
- en: Training sets and validation sets so you can train the neural network on one
    part of your data and evaluate it on another。The most common reason for doing
    this is generating out of sample predictions from a neural network。 We'll see
    how that's done， but you're able to basically generate an out of sample prediction
    for or every single element in the entire training set。 you can also use it to
    estimate a good number of epos to train a neural network for when used in conjunction
    with early stopping。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和验证集，这样你就可以在数据的一部分上训练神经网络，而在另一部分上进行评估。这样做的最常见原因是从神经网络生成样本外预测。我们将看到如何做到这一点，但你基本上可以为整个训练集中的每一个元素生成样本外预测。当与早停结合使用时，你还可以用它来估计训练神经网络所需的合适训练轮次。
- en: which we've already seen。 Finally， you can use it to evaluate the effectiveness
    of certain hyperparameters。 Maybe you're wanting to test。😊，If adding another hidden
    layer or some other adjustment to your neural network is actually helping you。
    However， with the randomness of neural networks， often you need to do more than
    just a single set of cross validation folds to really evaluate if what you've
    done has has had any effect。We'll look at a process called bootstrapping to really
    evaluate the effectiveness of changes。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到过。最后，你可以用它来评估某些超参数的有效性。也许你想要测试。😊，添加另一个隐藏层或其他对你的神经网络的调整是否真的有所帮助。然而，由于神经网络的随机性，通常你需要进行的不仅仅是一组交叉验证折叠，以真正评估你所做的是否产生了任何效果。我们将看看一个称为自助法的过程，以真正评估变化的有效性。
- en: This is how cross validation works。 You essentially take your data set and you
    break it into five folds。 usually or10 folds or some number of folds。 The number
    of elements in each of these folds should be pretty much even。 just due to the
    fact that your data set won't break up evenly always into five folds。 or however
    many， you'll have a few extra elements in each fold。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是交叉验证的工作原理。你基本上将数据集分成五个折叠，通常是10个折叠或其他数量的折叠。每个折叠中的元素数量应该相对均匀。由于你的数据集不会总是均匀地分成五个折叠，或其他数量的折叠，你将会在每个折叠中有一些额外的元素。
- en: but largely they're pretty equal to each other。 However many folds you have。
    That's how many times you're going to train。 So you're literally going to have
    a neural network for each of the folds。 And this is some of the the complexity
    in what you do within those five neural networks。 And we'll talk about that as
    well。But essentially， what you do is each fold you pick。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但在很大程度上，它们彼此之间相当。你有多少个折叠，你就要训练多少次。所以你实际上会为每一个折叠训练一个神经网络。这就是在这五个神经网络中你所做的复杂性之一。我们也会讨论这一点。但基本上，你会在每个折叠中选择。
- en: So each time through these same five folds and they're the same five folds in
    each one。 you choose one of the folds to be the validation set。 and you use the
    remaining。 you cancatenate folds 2，3，4 and 5 together。 and this is the training
    set。 and the highlighted one here。 That is the validation set。 So you train that
    and you get one model。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每次通过这五个相同的折叠，它们在每一个中都是相同的五个折叠。你选择其中一个折叠作为验证集，使用其余的，将折叠2、3、4和5连接在一起。这就是训练集，突出显示的部分就是验证集。因此你训练这个，并得到一个模型。
- en: You train this one， and you get another model。 This lets each of the folds have
    their chance as the validation set。 and you're essentially generating predictions
    on the validation set。 So if you take all of these yellow regions and concatenate
    them together or you have out of sample predictions If that's what your objective
    one。 You can see that here， you basically do a prediction on each of these concatenate
    those predictions together。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你训练这个，就得到了另一个模型。这让每个折叠都有机会作为验证集，你本质上是在验证集上生成预测。因此，如果你把所有这些黄色区域连接在一起，或者你有外样本预测，如果这是你的目标之一。你可以在这里看到，你基本上对每一个都进行了预测，并将这些预测连接在一起。
- en: And now you have a out of fold prediction。😊，On the entire training set。 and
    that's cool because you didn't have to waste part of your training set for training。
    part of your data set for validation， you're able to predict on the entire thing。
    Now。 if you want to bring these together， So say you you have trained this and
    now you have five neural networks。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个外部预测。😊 在整个训练集上，这很好，因为你不必将部分训练集浪费用于训练，部分数据集用于验证，你能够对整个数据集进行预测。现在，如果你想将这些结合在一起，假设你已经训练了这个，现在你有五个神经网络。
- en: There's several ways that you get your final resulting neural network that you're
    going to use for new data that's outside of this all because you're probably creating
    your neural network because you want to train something on an ongoing basis if
    say you're a life insurance company。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以获得最终结果神经网络，用于处理所有这些新数据，因为你可能是因为想要持续训练某些东西而创建神经网络，比如说你是一家人寿保险公司。
- en: you've got historic data and you've created this you've trained it to try to
    classify the risk of these people dying for your life insurance policies。 you're
    going to have new people coming in and that's what you really want to do your
    work on。 So you need to figure out what are you doing Are you picking the best
    of these models。Are you averaging them all together or somehow， and these are
    some of the common ways that you do this。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你有历史数据，并且你已经创建了这个，你训练它以试图分类这些人因你的人寿保险政策而死亡的风险。你将会有新的人进来，而这正是你真正想要工作的内容。所以你需要弄清楚你在做什么，你是在选择这些模型中最好的一个，还是把它们全部平均，或者以某种方式组合在一起，这些都是常见的方法。
- en: You could choose the model that has the highest validation score。 Now， one warning
    sign， though。 that I tend to look at is how similar are the validation scores
    between these。 If one of these models consistently gets。worse score than the others。
    That means that there's some outliers that landed in this， in this validation
    set for say Model 4。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择验证分数最高的模型。不过，有一个警告信号，我倾向于关注的是这些模型之间的验证分数有多相似。如果其中一个模型的得分持续低于其他模型，那就意味着在这个验证集中有一些离群值，比如模型4。
- en: So the more you see variances between these， that means that you have just a
    couple of outliers that are landing in。In particular， validation folds， and that
    might be something that you need to consider in your data set。Maybe you want to
    eliminate those outliers， maybe you。 but at the very least you should be aware
    of them， new data coming in。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你看到这些模型之间的差异越多，意味着你仅仅有几个离群值落在了特别的验证折叠中，这可能是你需要在数据集中考虑的事情。也许你想要消除那些离群值，或者你，但至少你应该意识到它们，新的数据进来。
- en: so say your new life insurance applications， you could present that to all five
    of these then vote。Or average。 So voting would be if the first three say no。 Don't
    don't insure this individual and 4 and 5 say yes， that majority win。 And you'd
    say no。 Or if it's giving you some sort of score or age that you think this individual
    will live to。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说你的新的人寿保险申请，你可以把这些呈现给这五个网络，然后进行投票或者取平均。所以投票的意思是，如果前面三个说不，不要给这个人投保，而第四和第五说是，那么多数派获胜，你就会说不。或者它给你某种评分或年龄，你认为这个人会活到。
- en: then you average them altogether。 This is the one that I like probably the best。
    I prefer either the averaging one or or this one。 So you do early stopping on
    each of these。 And you keep track of how many epochs were needed for each full。
    So maybe it took 300 epochs to get this one trained to an optimal level an early
    stop to 50 here。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将它们全部平均。这是我可能最喜欢的一个。我更倾向于平均法或这个。所以你在每个上都进行提前停止。你要记录每个完整的训练需要多少个周期。也许这个训练到最优水平用了300个周期，这里提前停止到50。
- en: 320 here， however， many。Epoet tu。You then average that together or you take
    the maximum or there's other techniques。 but you look at all the number of epochs
    that are tookck across all of these。And then you train on the entire training
    set。 You don't use a validation。 and you train for that number of epochs。 And
    then that becomes a single neural network that is hopefully。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 320，这里，多少个周期。然后你将它们平均，或者取最大值，或者还有其他技术。但是你查看所有这些所需的训练周期数。然后在整个训练集上进行训练。你不使用验证，按照那个训练周期数进行训练。然后这就成为一个希望是最佳的单一神经网络。
- en: The the best of those， all you still have the you still have the random nature
    of the neural network。 so you probably want to train that one multiple times。And
    perhaps average those together。 Now。 let's see some examples。Of how to do cross
    validation。 So the objective here is going to be out of sample prediction。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些中最好，你仍然有神经网络的随机特性。所以你可能想多次训练这个。也许将它们平均在一起。现在，让我们看看一些如何进行交叉验证的例子。所以这里的目标将是样本外预测。
- en: So we're going to do out of sample regression because the code looks a little
    bit different for each of these。So what you've got to， there's some differences
    in how you do the cross validation between classification and regression regression
    is definitely the more simple case because you just simply break it up into an
    even number of folds。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将进行样本外回归，因为对于每个这些，代码看起来稍微不同。所以你要做的是，分类和回归之间的交叉验证有一些差异，回归肯定是更简单的情况，因为你只需简单地将其划分为相等数量的折叠。
- en: And and you're done， but with classification， you need to make sure that breaking
    those folds up does not mess up your class balances。If you've got， if you're classifying，
    say between three things。 you want to make sure that the ratio of those three
    things is the same in each of those folds。 or you're introducing bias into your
    neural network。 And that's not a good thing。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你完成了，但在分类中，你需要确保打破这些折叠不会影响你的类别平衡。如果你正在分类，比如说三种事物。你要确保这三种事物的比例在每个折叠中都是一样的。否则你会给神经网络引入偏差。这不是件好事。
- en: So out of sample kfold cross validation for regression。 Here we set up the data
    sloting that in。 We're using that same simple data set that we had before， we're
    going to be predicting the age。 like we've done before， because this is a regression
    problem。 All these other values are coming in to help us predict the age。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是用于回归的样本外 k-fold 交叉验证。在这里，我们设置了数据分配。我们使用之前相同的简单数据集，预测年龄。就像之前一样，因为这是一个回归问题。所有这些其他值都是为了帮助我们预测年龄。
- en: Now we're going to assume 500 epochs for this。 We're not going to use early
    stopping because we want our validation fold So really try to give us some sort
    of out of sample prediction。 we can use early stopping。 So maybe we would do a
    whole separate run just to figure out how many epochs and we could adjust this
    to something more optimal。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们假设500个训练周期。我们不打算使用提前停止，因为我们希望我们的验证折叠能真正给我们某种样本外预测。我们可以使用提前停止。所以也许我们会单独进行一次运行，弄清楚需要多少个训练周期，然后可以将其调整为更优的值。
- en: But for now， we're just we're just assuming 500 epochs so that we can get a
    true sort of out of sample prediction of how many。Of how good 500 upbox would
    be here we set up the cross validation。 We're usingsyit Learnar randomand state
    42。 that just means that the folds will be the same time the same each time。 Suffle
    is always a good idea。 If you're shuffling。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，我们只是假设500个周期，以便获得一个真正的样本外预测，了解500个周期的表现。在这里我们设置了交叉验证。我们使用的是随机状态42。那意味着每次的折叠都会是一样的。打乱总是个好主意。如果你在打乱。
- en: you need a random state so that you can know that your folds are the same but
    your folds are different each time that you run this as you're trying experiments。
    it'll just make things inconsistent you won't notice that say fold one is consistently
    getting an outlier。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个随机状态，以便你知道你的折叠是相同的，但每次运行时你的折叠都是不同的，因为你在尝试实验。这将使事情变得不一致，你不会注意到比如说折叠1一直在得到一个异常值。
- en: It's okay to change this value。But this gives you at least some consistency
    while you're while you're experimenting。 then you get the train in the test set
    for the the split。 you're passing in the X data set and you're looping through
    each time you're creating a regressionion neural network because that is one。
    and that's mean squared error。Training with Adam。And you're fitting now with your
    validation set so that you can report a validation error as you。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 更改这个值是可以的。但这至少给你一些一致性，以便在实验时使用。然后你得到训练集和测试集的分割。你传入X数据集，并且每次循环时你都在创建一个回归神经网络，因为这就是其中之一，而这就是均方误差。使用Adam进行训练。现在你正在与验证集进行拟合，以便可以报告验证错误。
- en: As you go through， our out of sample predictions， we're creating the out of
    sample Ys。 that's just so that we're getting the Y's in the same order that our
    folds came in so that we can do a final evaluation and the out of sample predictions
    we're simply appending them together。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进行样本外预测时，我们正在创建样本外Y值。这只是为了确保我们获得的Y值与我们的折叠顺序相同，以便我们可以进行最终评估，并且样本外预测只是简单地将它们连接在一起。
- en: So as you do each one of these， you'll make out a sample predictions and you
    will concatenate the folds together so that you have one uniform one at the very
    end。 you report the score so that you have the overall score。For this fold。And
    the overall score finally comes at the end。 You're going to take all of these
    out of sample Ys that you've grouped。 concatenate them together， the out of sample
    predictions。 and you're going to compare the Ys。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行每个操作时，你会生成样本外的预测，并将各个折叠连接在一起，以便最终得到一个统一的结果。你报告得分，以便得出整体得分。对于这个折叠，整体得分最后会在最后得到。你将把所有分组的样本外Y值连接在一起，样本外预测。然后你将比较这些Y值。
- en: which were the values that we wanted it to be with what the predictions were。
    And we're going to report a final out of sample score of whatever the difference
    between these two happens to be。 And that's your estimate of how good your your
    neural network actually what。 we can also write out the out of sample predictions。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的价值观与预测值之间的差异。我们将报告一个最终的样本外评分，即这两者之间的差异。这就是你对你的神经网络的实际表现的估计。我们还可以列出样本外的预测。
- en: If we want them for some further analysis to write them as a Cv file。 Now。 when
    we classify we have to do something called kfold cross validation。 This is just
    because we don't want to say the balance of those of the items in each。 So we're
    going to use the same。 go ahead and run that loads the data set。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要进一步分析，可以将它们写成一个Cv文件。现在，当我们进行分类时，我们需要做一个叫做k折交叉验证的过程。这是因为我们不想说每一类物品的平衡。所以我们将使用相同的方式，继续运行并加载数据集。
- en: And now we're trying to predict the products。 So whatever that balance of products
    was。 Maybe they have 20% Choose product a。😊，You don't want to change that balance
    because that's your ground truth。 If the neural network is very， very unsure based
    on your data。 it's always going to say product A had 20%。 It's going to say， yeah，
    20% probability。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在尝试预测产品。因此，无论产品的平衡是什么。也许他们有20%的概率选择产品A。😊你不想改变这种平衡，因为那就是你的真实情况。如果神经网络根据你的数据非常不确定，它总是会说产品A的概率是20%。它会说，嗯，20%的概率。
- en: it's a if it has no clue whatsoever based on the input data。 maybe the input
    data was particularly hard in that case。 So if you're training one neural network
    and the ground truth percent difference。 Class A is much bigger and fold a fold1。
    then class A is in full2。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它完全不清楚输入数据的情况。也许输入数据在这种情况下特别困难。所以如果你训练一个神经网络，而真实情况的百分比差异，类A在折叠1中大得多，而在折叠2中则小得多。
- en: that's going to mess with that ground truth and that will mess with the accuracy
    of your neural network。So what we do is stratified kfold。 So we stratified kfold
    sampling here。 We're going to have our five folds。 We're going to shuffle and
    hair a random state。 That'll looks the same。 That just drops in。Same as as kfold。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这会干扰真实情况，从而影响你的神经网络的准确性。所以我们使用分层 Kfold。我们在这里进行分层 Kfold 采样。我们将有五个折叠，我们将随机打乱并设置一个随机状态。这个看起来是一样的，和
    Kfold 一样。
- en: we're going to keep the out of sample wise in predictions just like before。
    but this part's a little different。We're doing the split。 So when we do the split
    in the previous one， we just pass in X。 But since we want to balance on the Y's，
    we have to pass in the y。 We have to pass in the class。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续在样本外进行预测，就像之前一样。但这一部分有点不同。我们正在进行分割。所以在之前的操作中，我们只传入了 X。但由于我们想要在 Y 上进行平衡，我们必须传入
    y。我们必须传入类。
- en: So it needs to know what class we're trying to balance on。 And we pass in the
    original class from the data frame the product We don't want to pass in dummy
    variables for this。 It's not designed to work on dummy variables。 It needs a class
    like three like ABC or whatever those products are。 And it uses that to make sure
    that the balance is the same in each of those folds。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它需要知道我们要平衡的类是什么。我们从数据框中传入原始类。我们不想为此传入虚拟变量。这不是为了处理虚拟变量而设计的。它需要一个像 ABC 这样的类，或者其他产品。它使用这个确保每个折叠中的平衡是相同的。
- en: So that's very important。 You could just put in Kfold here， just like you did
    in regression。 and it would work just fine。😊，But you would have unbalanced splits。And
    often。 that would work out just fine， but it will probably affect the accuracy
    of your neural network at least a little bit。 We use categorical cross entropy
    and softmax and a output neurons equal to the number of classes that we have because
    we are doing a classification neural network。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这非常重要。你可以在这里直接使用 Kfold，就像在回归中一样，它会正常工作。😊但你会有不平衡的分割。而且通常这会正常工作，但可能会对你的神经网络的准确性产生一定影响。我们使用分类交叉熵和
    softmax，以及与我们分类神经网络的类别数相等的输出神经元。
- en: Everything else is the same as regression。We report the individual accuracy
    on each score。 We're using accuracy instead of ourMSA because we are doing classification。
    Then we print the final accuracy and we're we're basically done with that。 Now
    you can run this and it begins to it'll go through each of the folds and it'll
    report a fold score and then ultimately the final accuracy for this。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一切与回归相同。我们报告每个分数的个别准确性。我们使用准确性而不是我们的 MSA，因为我们在做分类。然后我们打印最终的准确性，这样我们基本上就完成了。现在你可以运行这个，它会遍历每个折叠，并报告折叠分数，最后给出最终的准确性。
- en: So the first accuracy was about 66% rounded。 And now you can see that we have
    essentially gone through all the five folds and we produce a final accuracy score
    of。66%， which is pretty， pretty similar range to these。 The most difficult fold
    was the fifth one。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第一个准确性大约是 66%（四舍五入）。现在你可以看到，我们基本上已经经过了所有五个折叠，并产生了一个最终的准确性得分为 66%，这与这些非常相似。最困难的折叠是第五个。
- en: So there might be some sort of an outlier or something in that one that is making
    that one more difficult。 or this simply could be the random nature of the neural
    network。 Ily， you'd want to run that again。See if that one is consistently hard。
    And again， that's why you want to use that random seed so that the folds are consistent。
    Finally， you can trade with a cross validation and a holdout set。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在那个折叠中可能有某种异常值或者其他什么因素使其更加困难。或者这可能仅仅是神经网络的随机特性。通常，你会想要再次运行这个，看看这个折叠是否始终很难。而这也是为什么你想使用随机种子，以确保折叠的一致性。最后，你可以使用交叉验证和保留集进行交易。
- en: This is where you set out a holdout before you even do the five folds。 and that
    lets you do that。 So to see so this gives you then a holdout set that is different
    than the five folds that you're dealing with。If we run this。This is also regression。
    We're trying to predict the age。 We can then run it and we I'll get it running
    so that we can see that while I'm doing that。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你在做五个折叠之前设置的保留数据集，这样你就可以做到这一点。这样你就能看到，这给你一个与五个折叠不同的保留集。如果我们运行这个，这也是回归。我们尝试预测年龄。然后我们可以运行它，我会让它运行，这样我们就能看到。
- en: you have the you set it for the five folds just like before and you loop through
    the entire thing。1 regression。 you're training on the mean square error with the
    atom update rule。 It's really pretty similar。 The folds are all the same。 It's
    just you're keeping this final holdout set that you're going to do the predictions
    on and you're going to report a root mean square on that。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你设置五个折叠，就像之前一样，并且循环整个过程。1回归。你正在训练均方误差，使用原子更新规则。这其实非常相似。所有折叠都是一样的。只不过你保留了这个最终的保留集，以便进行预测，并将报告一个均方根。
- en: So you'll see a different one between the two。 We run through all of this。 And
    we will get our final holdout value。 this is training just like before。 except
    that holdout set is simply not in there。 And here's our final holdout notice that
    it is worse than the other ones。 Now so it seems like we're doing all this extra
    effort just to get a worse score。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会看到两者之间的不同。我们会走过这一切。我们将得到最终的保留值，这就像以前的训练，只是保留集不在其中。这里是我们的最终保留，注意它比其他的更差。现在看来，我们似乎付出了额外的努力，只是为了得到一个更糟的分数。
- en: But this is a more accurate score。s no there's less overfitting in that。 Thank
    you for watching this video。 in the next video。 we're going to see how to actually
    use L1 and L2。 and also a combination of the two。 in Cars。 This content changes
    often。 So subscribe to the channel to stay up to date on this course and other
    topics and artificial intelligence。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一个更准确的评分。这里的过拟合更少。感谢您观看这个视频。在下一个视频中，我们将看看如何实际使用L1和L2，以及它们的组合。这些内容经常变化，因此请订阅频道以随时了解本课程和其他主题及人工智能。
- en: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73e33bd6247dfcb56b27a81fcdc55ac1_3.png)'
