- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Pandas è¿›è¡Œæ•°æ®å¤„ç†ä¸åˆ†æï¼çœŸå®æ•°æ®&å®æ—¶è®²è§£ï¼Œå­¦å®Œå°±èƒ½ä¸Šæ‰‹åšæ•°æ®åˆ†æäº†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼š8ï¼‰åˆ†ç»„å’Œèšåˆ -
    æ•°æ®çš„åˆ†æå’Œæ¢ç´¢ - ShowMeAI - BV1M64y187bz
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Pandas è¿›è¡Œæ•°æ®å¤„ç†ä¸åˆ†æï¼çœŸå®æ•°æ®&å®æ—¶è®²è§£ï¼Œå­¦å®Œå°±èƒ½ä¸Šæ‰‹åšæ•°æ®åˆ†æäº†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P8ï¼š8ï¼‰åˆ†ç»„å’Œèšåˆ -
    æ•°æ®çš„åˆ†æå’Œæ¢ç´¢ - ShowMeAI - BV1M64y187bz
- en: Hey thereã€‚ How's it goingï¼Œ everybodyã€‚ In this videoã€‚ we're gonna be learning
    how we can group and aggregate our dataã€‚ Nowã€‚ if you don't know what grouping
    and aggregating really entailsã€‚ then I'd really recommend sticking around for
    this videoã€‚ because basicallyã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿåœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„å’Œèšåˆã€‚å¦‚æœä½ ä¸çŸ¥é“åˆ†ç»„å’Œèšåˆåˆ°åº•æ˜¯ä»€ä¹ˆï¼Œå»ºè®®ä½ ç»§ç»­è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼Œå› ä¸ºåŸºæœ¬ä¸Šã€‚
- en: this is what most people think of when they think of actually analyzing data
    in a meaningful senseã€‚ So this will be the first video where we actually get some
    statistics back on our data sets and aren't just modifying our data frames in
    different waysã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤§å¤šæ•°äººæƒ³åˆ°æœ‰æ„ä¹‰åœ°åˆ†ææ•°æ®æ—¶çš„æƒ³æ³•ã€‚å› æ­¤ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä»æ•°æ®é›†è·å–ä¸€äº›ç»Ÿè®¡æ•°æ®ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»¥ä¸åŒçš„æ–¹å¼ä¿®æ”¹æ•°æ®æ¡†ã€‚
- en: Soï¼Œ for exampleï¼Œ maybe you want to know what the average salary for a developer
    isã€‚ Or maybe you want to know how many people from each country knows Python or
    another programming languageã€‚ So what we're going learn here is going to allow
    us to answer those types of questionsã€‚ Nowã€‚ I would like to mention that we do
    have a sponsor for this series of videos and that is brilliantã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¹Ÿè®¸ä½ æƒ³çŸ¥é“å¼€å‘è€…çš„å¹³å‡è–ªèµ„æ˜¯å¤šå°‘ã€‚æˆ–è€…ä½ æƒ³çŸ¥é“æ¯ä¸ªå›½å®¶æœ‰å¤šå°‘äººä¼šPythonæˆ–å…¶ä»–ç¼–ç¨‹è¯­è¨€ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œå­¦ä¹ çš„å†…å®¹å°†å¸®åŠ©æˆ‘ä»¬å›ç­”è¿™äº›ç±»å‹çš„é—®é¢˜ã€‚ç°åœ¨ï¼Œæˆ‘æƒ³æåˆ°æˆ‘ä»¬è¿™ä¸€ç³»åˆ—è§†é¢‘æœ‰ä¸€ä¸ªèµåŠ©å•†ï¼Œé‚£å°±æ˜¯Brilliantã€‚
- en: So I really want to think brilliant for sponsoring this seriesã€‚ And it would
    be great if you all could check them out using the link in the description section
    below and support the sponsorsã€‚ And I'll talk more about their services in just
    a bitã€‚ So with that saidã€‚ let's go ahead and get startedã€‚ Okayï¼Œ so before we start
    doing some more advanced data analysisã€‚ğŸ˜Šã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœŸçš„æƒ³æ„Ÿè°¢BrilliantèµåŠ©è¿™ä¸€ç³»åˆ—ã€‚å¦‚æœå¤§å®¶èƒ½é€šè¿‡ä¸‹é¢æè¿°éƒ¨åˆ†çš„é“¾æ¥æŸ¥çœ‹ä»–ä»¬å¹¶æ”¯æŒèµåŠ©å•†ï¼Œé‚£å°†éå¸¸æ£’ã€‚æˆ‘ç¨åä¼šæ›´å¤šåœ°è°ˆè°ˆä»–ä»¬çš„æœåŠ¡ã€‚é‚£ä¹ˆï¼Œæ—¢ç„¶è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¼€å§‹å§ã€‚åœ¨å¼€å§‹è¿›è¡Œä¸€äº›æ›´é«˜çº§çš„æ•°æ®åˆ†æä¹‹å‰ã€‚ğŸ˜Š
- en: Start off slow and build up to the more advanced stuff so that all of this makes
    sense along the wayã€‚ So I have my developer survey data open here that we've been
    using throughout this seriesã€‚ And as usualï¼Œ if you'd like to follow alongï¼Œ then
    I have links to this code and the data in the description section belowã€‚ So let's
    look at some basic aggregationsã€‚ So if you don't know what aggregation meansï¼Œ
    basicallyã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ…¢æ…¢å¼€å§‹ï¼Œç„¶åé€æ­¥è¿›å…¥æ›´é«˜çº§çš„å†…å®¹ï¼Œä»¥ç¡®ä¿è¿™ä¸€åˆ‡éƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘è¿™é‡Œæ‰“å¼€äº†æˆ‘ä»¬åœ¨æ•´ä¸ªç³»åˆ—ä¸­ä½¿ç”¨çš„å¼€å‘è€…è°ƒæŸ¥æ•°æ®ã€‚å¦‚å¾€å¸¸ä¸€æ ·ï¼Œå¦‚æœä½ æƒ³è·Ÿç€åšï¼Œæˆ‘åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†ä¸­æä¾›äº†ä»£ç å’Œæ•°æ®çš„é“¾æ¥ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä¸€äº›åŸºæœ¬çš„èšåˆã€‚å¦‚æœä½ ä¸çŸ¥é“èšåˆçš„æ„æ€ï¼ŒåŸºæœ¬ä¸Šã€‚
- en: it means that we're going to be combining multiple pieces of data into a single
    resultsã€‚ Soã€‚ for exampleï¼Œ if you've ever used a mean median or mode and mathematicsã€‚
    these are aggregate functionsï¼Œ because they take multiple values and give you
    either the mean median or mode of those resultsã€‚ So if we wanted to run some analysis
    on our developer survey hereã€‚ one question we might ask isã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬å°†æŠŠå¤šä¸ªæ•°æ®ç‰‡æ®µåˆå¹¶ä¸ºä¸€ä¸ªç»“æœã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æ›¾ç»åœ¨æ•°å­¦ä¸­ä½¿ç”¨è¿‡å¹³å‡æ•°ã€ä¸­ä½æ•°æˆ–ä¼—æ•°ï¼Œè¿™äº›éƒ½æ˜¯èšåˆå‡½æ•°ï¼Œå› ä¸ºå®ƒä»¬å–å¤šä¸ªå€¼å¹¶ç»™å‡ºè¿™äº›ç»“æœçš„å¹³å‡æ•°ã€ä¸­ä½æ•°æˆ–ä¼—æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å¯¹è¿™æ¬¡å¼€å‘è€…è°ƒæŸ¥è¿›è¡Œä¸€äº›åˆ†æï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé—®çš„é—®é¢˜æ˜¯ã€‚
- en: okayï¼Œ what is a typical salary for developers who answered this surveyã€‚ So that
    might be some good information to have if you're looking for a job and want to
    get an idea of what the salaries look like at the momentã€‚ So to do thisï¼Œ we can
    grab theã€‚Meedn salaries of our data frameã€‚ So firstã€‚ let's look at these salariesã€‚
    So our salary column within this data frame here of all these survey results is
    called converted compã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œå›ç­”è¿™ä¸ªè°ƒæŸ¥çš„å¼€å‘è€…çš„å…¸å‹è–ªèµ„æ˜¯å¤šå°‘ï¼Ÿå¦‚æœä½ æ­£åœ¨æ‰¾å·¥ä½œå¹¶æƒ³äº†è§£ç›®å‰çš„è–ªèµ„æƒ…å†µï¼Œè¿™å¯èƒ½æ˜¯ä¸€äº›å¾ˆå¥½çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è·å–æ•°æ®æ¡†ä¸­çš„å¹³å‡è–ªèµ„ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›è–ªèµ„ã€‚åœ¨è¿™ä¸ªæ•°æ®æ¡†ä¸­çš„è–ªèµ„åˆ—å«åšè½¬æ¢è–ªèµ„ã€‚
- en: And that is converted to US dollarsã€‚ it's actually further over here in the
    surveyã€‚ It is about right hereã€‚ So I'm going to copy thatã€‚ Nowï¼Œ firstï¼Œ let's just
    look at this columnã€‚ So as we've seen beforeï¼Œ we can just access the column just
    like we're accessing a key of a dictionaryã€‚ And I'm going grab the firstï¼Œ let's
    get the first 15 salaries or soã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è½¬æ¢ä¸ºç¾å…ƒã€‚å®é™…ä¸Šåœ¨è°ƒæŸ¥ä¸­è¿˜æœ‰è¿›ä¸€æ­¥çš„ä¿¡æ¯ã€‚å°±åœ¨è¿™é‡Œã€‚æ‰€ä»¥æˆ‘ä¼šå¤åˆ¶å®ƒã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸€åˆ—ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥åƒè®¿é—®å­—å…¸çš„é”®ä¸€æ ·è®¿é—®è¿™ä¸€åˆ—ã€‚æˆ‘å°†æŠ“å–å‰15ä¸ªè–ªèµ„ã€‚
- en: So I'm going to look at the head of the of the results hereã€‚ And these are salaries
    hereã€‚ that developers put down for this surveyã€‚ and these N N values hereã€‚ just
    mean not a number in this contextã€‚ It means that they just skipped that question
    in the surveyã€‚ Okayï¼Œ so we can see the median salary for this surveyã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†æŸ¥çœ‹ç»“æœçš„å‰å‡ è¡Œã€‚è¿™äº›æ˜¯å¼€å‘äººå‘˜åœ¨è¿™æ¬¡è°ƒæŸ¥ä¸­å¡«å†™çš„å·¥èµ„ï¼Œè€Œè¿™äº›N Nå€¼åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­æ„å‘³ç€éæ•°å­—ï¼Œè¡¨ç¤ºä»–ä»¬åœ¨è°ƒæŸ¥ä¸­è·³è¿‡äº†é‚£ä¸ªé—®é¢˜ã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ¬¡è°ƒæŸ¥çš„ä¸­ä½æ•°å·¥èµ„ã€‚
- en: Just by running the median method on this seriesã€‚ So to do thisã€‚ I'm going to
    go ahead and copy what I have hereã€‚ And nowï¼Œ instead of looking at the headã€‚ I
    can just run median on that seriesã€‚ So if I run thisã€‚ Then we can see that the
    median salary for this survey was around 57000ã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€åœ¨è¿™ä¸ªåºåˆ—ä¸Šè¿è¡Œä¸­ä½æ•°æ–¹æ³•ã€‚å› æ­¤ï¼Œæ¥ä¸‹æ¥æˆ‘å°†å¤åˆ¶æˆ‘åœ¨è¿™é‡Œçš„å†…å®¹ã€‚ç°åœ¨ï¼Œä¸å†æŸ¥çœ‹å¤´éƒ¨ï¼Œæˆ‘å¯ä»¥ç›´æ¥åœ¨é‚£ä¸ªåºåˆ—ä¸Šè¿è¡Œä¸­ä½æ•°ã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ¬¡è°ƒæŸ¥çš„ä¸­ä½æ•°å·¥èµ„å¤§çº¦ä¸º57000ã€‚
- en: So that takes all of the salary responses from our survey from this series hereã€‚
    And it gives us the median value of all of those and ignores the N in valuesã€‚
    Soã€‚ this probably doesn't give us as much information as we'd really like to haveã€‚
    Soï¼Œ for exampleã€‚ different countries pay different amounts since there are different
    costs of living and things like thatã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å–è‡ªæˆ‘ä»¬è°ƒæŸ¥ä¸­è¿™ä¸ªåºåˆ—çš„æ‰€æœ‰å·¥èµ„å“åº”ã€‚å®ƒç»™æˆ‘ä»¬æä¾›äº†æ‰€æœ‰è¿™äº›çš„ä¸­ä½æ•°å€¼ï¼Œå¹¶å¿½ç•¥äº†N Nå€¼ã€‚å› æ­¤ï¼Œè¿™å¯èƒ½æ²¡æœ‰æä¾›æˆ‘ä»¬çœŸæ­£æƒ³è¦çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œä¸åŒå›½å®¶çš„è–ªèµ„ä¸åŒï¼Œå› ä¸ºç”Ÿæ´»æˆæœ¬å’Œå…¶ä»–å› ç´ å„ä¸ç›¸åŒã€‚
- en: So it' would be nice if we could look at the median salary broken down by countryã€‚
    And we'll look at that here in a second when we learn about grouping dataã€‚ So
    firstã€‚ I want to cover a few more basic concepts before we move on to groupingã€‚
    So one thing that I'd like to look at is running these aggregate functions on
    our entire data frameã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½æŸ¥çœ‹æŒ‰å›½å®¶åˆ’åˆ†çš„ä¸­ä½æ•°å·¥èµ„å°±å¥½äº†ã€‚å½“æˆ‘ä»¬å­¦ä¹ åˆ†ç»„æ•°æ®æ—¶ï¼Œæˆ‘ä»¬å°†ç¨åæŸ¥çœ‹è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥é¦–å…ˆï¼Œæˆ‘æƒ³åœ¨ç»§ç»­åˆ†ç»„ä¹‹å‰å†è®²å‡ ä¸ªåŸºæœ¬æ¦‚å¿µã€‚æˆ‘æƒ³æŸ¥çœ‹çš„æ˜¯å¯¹æ•´ä¸ªæ•°æ®æ¡†è¿è¡Œè¿™äº›èšåˆå‡½æ•°ã€‚
- en: ğŸ˜Šï¼ŒSo let's see what we get if we just run this median function that we just
    ran on our entire data frame instead of just this single seriesã€‚ So here I'm just
    going to say Df do medianã€‚ So we're no longer accessing just a single columnã€‚
    So if I run thisï¼Œ then it might take a second to spin up hereã€‚ So when I do thisã€‚
    it's going to look through our data frame and find the columns that contain numerical
    values where it can grab a median value and some of these might not make sense
    to use with the median but others might be pretty useful to usã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœå¯¹æ•´ä¸ªæ•°æ®æ¡†è¿è¡Œæˆ‘ä»¬åˆšæ‰è¿è¡Œçš„ä¸­ä½æ•°å‡½æ•°ä¼šå¾—åˆ°ä»€ä¹ˆã€‚æ‰€ä»¥è¿™é‡Œæˆ‘åªæ˜¯è¯´Df do medianã€‚æˆ‘ä»¬ä¸å†ä»…ä»…è®¿é—®å•ä¸€åˆ—ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œå¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´æ¥åŠ è½½ã€‚å½“æˆ‘è¿™æ ·åšæ—¶ï¼Œå®ƒä¼šæŸ¥æ‰¾æˆ‘ä»¬çš„æ•°æ®æ¡†ï¼Œå¹¶æ‰¾åˆ°åŒ…å«æ•°å€¼çš„åˆ—ï¼Œä»¥ä¾¿å¯ä»¥è·å–ä¸­ä½æ•°å€¼ï¼Œå…¶ä¸­ä¸€äº›å¯èƒ½ä¸é€‚åˆç”¨ä¸­ä½æ•°ï¼Œä½†å…¶ä»–çš„å¯èƒ½å¯¹æˆ‘ä»¬å¾ˆæœ‰ç”¨ã€‚
- en: So for exampleï¼Œ we can see that the median age down here at the bottom for this
    survey was 29 years oldã€‚ and the median number of work hours per weekã€‚ that was
    40ï¼Œ which is pretty standardã€‚ So that makes senseã€‚ Nowï¼Œ if you want to get a broad
    overview of your data and a statistical overviewã€‚ we can use the describe method
    on our data frame insteadã€‚ So if Iã€‚Instead runï¼Œ describeã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ¬¡è°ƒæŸ¥çš„ä¸­ä½æ•°å¹´é¾„åœ¨åº•éƒ¨ä¸º29å²ï¼Œè€Œæ¯å‘¨çš„ä¸­ä½å·¥ä½œå°æ—¶æ•°ä¸º40å°æ—¶ï¼Œè¿™å¾ˆæ ‡å‡†ã€‚æ‰€ä»¥è¿™å¾ˆåˆç†ã€‚ç°åœ¨ï¼Œå¦‚æœä½ æƒ³å¯¹ä½ çš„æ•°æ®è¿›è¡Œå¹¿æ³›çš„ç»Ÿè®¡æ¦‚è¿°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨describeæ–¹æ³•ï¼Œè€Œä¸æ˜¯å¯¹æˆ‘ä»¬çš„æ•°æ®æ¡†è¿è¡Œã€‚äºæ˜¯ï¼Œæˆ‘å¦‚æœæ”¹ä¸ºè¿è¡Œdescribeã€‚
- en: instead of medianã€‚ And I run thisã€‚ then this is going to give us a broad overview
    of some different statsã€‚ So if we look at the converted comp column hereï¼Œ then
    we can see a few different stats about this columnã€‚ So it gives us the countã€‚
    It gives us the meanï¼Œ it gives us the standard deviationï¼Œ the minimumã€‚ and then
    it also gives us the 25ï¼Œ50 and 75% quantiles hereã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸æ˜¯ä¸­ä½æ•°ã€‚æˆ‘è¿è¡Œè¿™ä¸ªï¼Œè¿™å°†ä¸ºæˆ‘ä»¬æä¾›ä¸€äº›ä¸åŒç»Ÿè®¡æ•°æ®çš„æ¦‚è¿°ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™é‡Œè½¬æ¢çš„è¡¥å¿åˆ—ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›å…³äºè¿™ä¸€åˆ—çš„ä¸åŒç»Ÿè®¡æ•°æ®ã€‚å®ƒç»™æˆ‘ä»¬æä¾›äº†è®¡æ•°ã€å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼ï¼Œä»¥åŠ25%ã€50%å’Œ75%åˆ†ä½æ•°ã€‚
- en: Now this 50% marker is just the median valueï¼Œ by the wayã€‚ And just like we saw
    beforeã€‚ when we look this median value up specificallyï¼Œ this is around 57000ã€‚
    Now this is in scientific notation hereã€‚ So it looks a little bit differentã€‚ Basicallyã€‚
    this means that we just need to move 4 spots over from the decimal pointã€‚ So 1ï¼Œ2ï¼Œ3ï¼Œ4ã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¿™ä¸ª50%çš„æ ‡è®°åªæ˜¯ä¸­ä½æ•°å€¼ã€‚å°±åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œå½“æˆ‘ä»¬å…·ä½“æŸ¥æ‰¾è¿™ä¸ªä¸­ä½æ•°å€¼æ—¶ï¼Œå¤§çº¦æ˜¯57000ã€‚è¿™é‡Œæ˜¯ç§‘å­¦è®°æ•°æ³•ï¼Œæ‰€ä»¥çœ‹èµ·æ¥æœ‰ç‚¹ä¸åŒã€‚åŸºæœ¬ä¸Šï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªéœ€ä»å°æ•°ç‚¹å‘å³ç§»åŠ¨4ä¸ªä½ç½®ã€‚æ‰€ä»¥1ã€2ã€3ã€4ã€‚
- en: So that would be 57000 thereã€‚ So this describe method gives us a bunch of these
    aggregates in one placeã€‚ If we just want to get a quick overview of our dateã€‚Nowã€‚
    if you're wondering why I wanted to look at the median of our salaries instead
    of the meanã€‚ which is the averageï¼Œ basically it's because the mean is affected
    too heavily by outliersã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œä¼šæ˜¯57000ã€‚å› æ­¤ï¼Œè¿™ä¸ªæè¿°æ–¹æ³•ä¼šåœ¨ä¸€ä¸ªåœ°æ–¹ç»™æˆ‘ä»¬æä¾›ä¸€å †èšåˆæ•°æ®ã€‚å¦‚æœæˆ‘ä»¬åªæƒ³å¿«é€Ÿæµè§ˆæˆ‘ä»¬çš„æ•°æ®ã€‚ç°åœ¨ï¼Œå¦‚æœä½ åœ¨æƒ³ï¼Œä¸ºä»€ä¹ˆæˆ‘æƒ³æŸ¥çœ‹è–ªèµ„çš„ä¸­ä½æ•°è€Œä¸æ˜¯å¹³å‡æ•°ï¼Œå…¶å®æ˜¯å› ä¸ºå¹³å‡æ•°ä¼šå—åˆ°å¼‚å¸¸å€¼çš„å½±å“å¤ªå¤§ã€‚
- en: It's not really a good metric to use because a few outliers can affect the average
    very heavily we can see that the mean salary up hereã€‚ if I highlight this right
    hereã€‚ if we were to count this upã€‚ then that's actually about 127000 on averageï¼Œ
    but that gives us an unrealistic expectation of what a typical developer salary
    isã€‚ because the largest salaries on our data set are just pulling up that average
    so heavilyã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„æŒ‡æ ‡ï¼Œå› ä¸ºå°‘æ•°å¼‚å¸¸å€¼å¯ä»¥æå¤§åœ°å½±å“å¹³å‡æ•°ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šé¢çš„å¹³å‡è–ªèµ„ï¼Œå¦‚æœæˆ‘å¼ºè°ƒè¿™ä¸€ç‚¹ã€‚å¦‚æœæˆ‘ä»¬è¦è®¡ç®—è¿™ä¸ªï¼Œé‚£ä¹ˆå®é™…ä¸Šå¤§çº¦æ˜¯127000çš„å¹³å‡å€¼ï¼Œä½†è¿™ç»™æˆ‘ä»¬ä¸€ä¸ªä¸åˆ‡å®é™…çš„æœŸæœ›ï¼Œå…³äºå…¸å‹å¼€å‘è€…çš„è–ªèµ„ï¼Œå› ä¸ºæ•°æ®é›†ä¸­çš„æœ€é«˜è–ªèµ„å°†å¹³å‡å€¼æ‹‰å¾—å¤ªé«˜ã€‚
- en: So in cases like thatï¼Œ you definitely want to use the mean instead I think that's
    a better representation or I'm sorry you're going to want to use the median instead
    because that's a better representationã€‚ Now if we only wanted to get this overview
    for a single column then we could just run this describe method on a single column
    as well and get those results for thatã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ è‚¯å®šæƒ³ä½¿ç”¨ä¸­ä½æ•°ï¼Œæˆ‘è®¤ä¸ºé‚£æ˜¯æ›´å¥½çš„è¡¨ç¤ºï¼ŒæŠ±æ­‰ï¼Œä½ ä¼šæƒ³ä½¿ç”¨ä¸­ä½æ•°ï¼Œå› ä¸ºé‚£æ˜¯æ›´å¥½çš„è¡¨ç¤ºã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬åªæƒ³è·å¾—å•åˆ—çš„æ¦‚è¿°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨å•åˆ—ä¸Šè¿è¡Œè¿™ä¸ªæè¿°æ–¹æ³•ï¼Œå¹¶å¾—åˆ°ç›¸åº”çš„ç»“æœã€‚
- en: Nowï¼Œ you might be wondering what that count value is listed at the top of these
    described resultsã€‚ Nowï¼Œ the count value is the number of non in a rowsã€‚ which
    basically means that it counts the non missing rowsã€‚ So in the context of this
    surveyã€‚ a missing row just means that the respondent didn't answer that a specific
    questionã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ å¯èƒ½åœ¨æƒ³ï¼Œè¿™äº›æè¿°ç»“æœé¡¶éƒ¨åˆ—å‡ºçš„è®¡æ•°å€¼æ˜¯ä»€ä¹ˆã€‚è®¡æ•°å€¼æ˜¯éç¼ºå¤±è¡Œçš„æ•°é‡ï¼Œè¿™åŸºæœ¬ä¸Šæ„å‘³ç€å®ƒè®¡ç®—äº†éç¼ºå¤±è¡Œã€‚å› æ­¤ï¼Œåœ¨è¿™æ¬¡è°ƒæŸ¥çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œç¼ºå¤±è¡Œä»…æ„å‘³ç€å—è®¿è€…æ²¡æœ‰å›ç­”ç‰¹å®šé—®é¢˜ã€‚
- en: So if I look at the count for the converted comp columnã€‚ So I'm going to go
    up hereã€‚And grab thisã€‚ And instead of grabbing the medianã€‚I'm just going grab
    the countã€‚ We can see here that only about 55 to 65 or 55 to 56000 people answered
    that questionã€‚ Nowã€‚ I think there are about 89000 rows for this dataï¼Œ So that
    means that there are about 30000 people or so who didn't answer the salary question
    on this surveyã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘æŸ¥çœ‹è½¬æ¢çš„compåˆ—çš„è®¡æ•°ã€‚æˆ‘å°†ä¼šä¸Šå»æŠ“å–è¿™ä¸ªã€‚è€Œä¸æ˜¯æŠ“å–ä¸­ä½æ•°ï¼Œæˆ‘åªæ˜¯è¦æŠ“å–è®¡æ•°ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåªæœ‰å¤§çº¦55000åˆ°65000äººå›ç­”äº†è¿™ä¸ªé—®é¢˜ã€‚ç°åœ¨ï¼Œæˆ‘æƒ³è¿™ä¸ªæ•°æ®å¤§çº¦æœ‰89000è¡Œï¼Œè¿™æ„å‘³ç€å¤§çº¦æœ‰30000äººæ²¡æœ‰å›ç­”è¿™æ¬¡è°ƒæŸ¥çš„è–ªèµ„é—®é¢˜ã€‚
- en: Nowï¼Œ I sometimes see the mistake that some people think that the count function
    will count up the individual values and a specific row and report how many of
    those values were in the columnã€‚ But if that's what you're trying to doã€‚ then
    that's what we would use the value counts function forã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘æœ‰æ—¶ä¼šçœ‹åˆ°ä¸€äº›äººçŠ¯é”™ï¼Œä»–ä»¬è®¤ä¸ºè®¡æ•°å‡½æ•°ä¼šè®¡ç®—ç‰¹å®šè¡Œä¸­çš„ä¸ªåˆ«å€¼ï¼Œå¹¶æŠ¥å‘Šè¿™äº›å€¼åœ¨åˆ—ä¸­æœ‰å¤šå°‘ä¸ªã€‚ä½†æ˜¯å¦‚æœè¿™å°±æ˜¯ä½ æƒ³åšçš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å€¼è®¡æ•°å‡½æ•°ã€‚
- en: Nowï¼Œ in case that doesn't quite make senseã€‚ let's look at an exampleï¼Œ to see
    what this looks likeã€‚ Soï¼Œ for exampleï¼Œ we had the question on the survey that
    asked each person whether they coded in their free time as a hobbyã€‚ So to see
    all of these responses for that questionã€‚ we can look at the hobbyist columnã€‚
    So I'll just accessã€‚That hobbyist column here and run thatã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœè¿™å¬èµ·æ¥æœ‰ç‚¹ä¸å¤ªæ˜ç™½ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼Œçœ‹çœ‹è¿™æ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨è°ƒæŸ¥ä¸­æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œé—®æ¯ä¸ªäººæ˜¯å¦åœ¨ç©ºé—²æ—¶é—´æŠŠç¼–ç å½“ä½œçˆ±å¥½ã€‚ä¸ºäº†æŸ¥çœ‹è¿™ä¸ªé—®é¢˜çš„æ‰€æœ‰å›ç­”ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹çˆ±å¥½è€…åˆ—ã€‚æ‰€ä»¥æˆ‘ä¼šè®¿é—®è¿™ä¸ªçˆ±å¥½è€…åˆ—å¹¶è¿è¡Œå®ƒã€‚
- en: And we can see that we get a series returned hereã€‚ And these are just a bunch
    of yes or no questionsã€‚ So it was just a yes or no question that each person answeredã€‚
    So you might get the survey results back and you might think to yourselfï¼Œ okayï¼Œ
    wellã€‚ I can see the responses here in the surveyï¼Œ But I just want to know how
    many people answeredã€‚ yes and how many people answeredï¼Œ noï¼Œ So how would we do
    thatã€‚ wellã€‚ we can get that information with the value counts functionã€‚ So if
    I just look at the value countsã€‚ and that is value underscore countsã€‚ if we run
    that method on that series then that is going to give us a breakdown of how many
    people answeredã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œè¿”å›äº†ä¸€ç³»åˆ—æ•°æ®ã€‚è¿™äº›åªæ˜¯ä¸€äº›æ˜¯æˆ–å¦çš„é—®é¢˜ã€‚æ¯ä¸ªäººéƒ½å›ç­”äº†ä¸€ä¸ªæ˜¯æˆ–å¦çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå½“ä½ å¯èƒ½æ”¶åˆ°è°ƒæŸ¥ç»“æœæ—¶ï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œå¥½å§ï¼Œæˆ‘å¯ä»¥åœ¨è°ƒæŸ¥ä¸­çœ‹åˆ°å“åº”ï¼Œä½†æˆ‘åªæƒ³çŸ¥é“æœ‰å¤šå°‘äººå›ç­”äº†æ˜¯ï¼Œæœ‰å¤šå°‘äººå›ç­”äº†ä¸ã€‚é‚£ä¹ˆæˆ‘ä»¬è¯¥å¦‚ä½•åšåˆ°å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡å€¼è®¡æ•°å‡½æ•°è·å¾—è¿™äº›ä¿¡æ¯ã€‚æ‰€ä»¥å¦‚æœæˆ‘åªçœ‹å€¼è®¡æ•°ï¼Œé‚£å°±æ˜¯value
    underscore countsã€‚å¦‚æœæˆ‘ä»¬å¯¹é‚£ä¸ªç³»åˆ—è¿è¡Œè¿™ä¸ªæ–¹æ³•ï¼Œé‚£ä¹ˆè¿™å°†ç»™æˆ‘ä»¬ä¸€ä¸ªå›ç­”äººæ•°çš„ç»†åˆ†ã€‚
- en: yesï¼Œ and how many people answeredï¼Œ no as the whether or not they code as a hobbyã€‚
    So I use the value counts all the time when exploring dataã€‚ And we can find out
    some interesting things from our survey by using this on some different fieldsã€‚
    Soï¼Œ for exampleï¼Œ there is a question on this surveyã€‚ğŸ˜Šã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿˜æœ‰å¤šå°‘äººå›ç­”äº†ä¸ï¼Œæ˜¯å¦å°†ç¼–ç ä½œä¸ºä¸€ç§çˆ±å¥½ã€‚å› æ­¤ï¼Œæˆ‘åœ¨æ¢ç´¢æ•°æ®æ—¶æ€»æ˜¯ä½¿ç”¨å€¼è®¡æ•°ã€‚é€šè¿‡å¯¹ä¸€äº›ä¸åŒçš„å­—æ®µä½¿ç”¨è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥ä»æˆ‘ä»¬çš„è°ƒæŸ¥ä¸­å‘ç°ä¸€äº›æœ‰è¶£çš„äº‹æƒ…ã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ªè°ƒæŸ¥ä¸­æœ‰ä¸€ä¸ªé—®é¢˜ã€‚ğŸ˜Šã€‚
- en: That ask each person what social media platform they use the mostã€‚ So if you're
    building an app or a website and want to keep track of the most popular social
    media sitesã€‚ then you might be interested in what the most popular answers to
    that question so to view these results we can access the social media column of
    the surveyã€‚ So let me do thatã€‚ And before I run value counts on thisã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¢é—®æ¯ä¸ªäººæœ€å¸¸ä½¿ç”¨å“ªä¸ªç¤¾äº¤åª’ä½“å¹³å°ã€‚å› æ­¤ï¼Œå¦‚æœä½ åœ¨æ„å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºæˆ–ç½‘ç«™å¹¶æƒ³è·Ÿè¸ªæœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ï¼Œé‚£ä¹ˆä½ å¯èƒ½ä¼šå¯¹è¿™ä¸ªé—®é¢˜çš„æœ€å—æ¬¢è¿ç­”æ¡ˆæ„Ÿå…´è¶£ã€‚ä¸ºäº†æŸ¥çœ‹è¿™äº›ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®è°ƒæŸ¥çš„ç¤¾äº¤åª’ä½“åˆ—ã€‚æ‰€ä»¥è®©æˆ‘è¿™æ ·åšã€‚åœ¨æˆ‘å¯¹è¿™ä¸ªè¿è¡Œå€¼è®¡æ•°ä¹‹å‰ã€‚
- en: let me just show you what this column looks likeã€‚ So this column was called
    social media So I'm going run thisã€‚ and we can see that respondent number one
    said that they used Twitter more than any other social mediaã€‚ this person used
    Instagramï¼Œ Redditï¼Œ Redditï¼Œ Facebookï¼Œ YouTube and so onã€‚ Now I've pointed this
    out in previous video so farã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ç»™ä½ å±•ç¤ºä¸€ä¸‹è¿™ä¸ªåˆ—æ˜¯ä»€ä¹ˆæ ·çš„ã€‚å› æ­¤ï¼Œè¿™ä¸€åˆ—å«åšç¤¾äº¤åª’ä½“ï¼Œæ‰€ä»¥æˆ‘å°†è¿è¡Œè¿™ä¸ªã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå—è®¿è€…ç¼–å·ä¸€è¯´ä»–ä»¬ä½¿ç”¨Twitteræ¯”å…¶ä»–ä»»ä½•ç¤¾äº¤åª’ä½“æ›´å¤šï¼Œè¿™ä¸ªäººè¿˜ä½¿ç”¨äº†Instagramã€Redditã€Facebookã€YouTubeç­‰ç­‰ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­æåˆ°è¿‡è¿™ä¸€ç‚¹ã€‚
- en: But if you've forgotten or if this is your first video that you've watched in
    this series then at the top of my notebook hereã€‚ I've also loaded in a schema
    data frame right here and thisã€‚Data frame tells us the exact question that was
    asked on the survey for each of these column namesã€‚ Soï¼Œ for exampleï¼Œ if we want
    to see the exact question that was asked for this social media columnã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ å¿˜äº†ï¼Œæˆ–è€…è¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡è§‚çœ‹è¿™ä¸ªç³»åˆ—çš„è§†é¢‘ï¼Œé‚£ä¹ˆåœ¨æˆ‘çš„ç¬”è®°æœ¬é¡¶éƒ¨ï¼Œæˆ‘ä¹Ÿåœ¨è¿™é‡ŒåŠ è½½äº†ä¸€ä¸ªæ¨¡å¼æ•°æ®æ¡†ã€‚è¿™ä¸ªæ•°æ®æ¡†å‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªåˆ—ååœ¨è°ƒæŸ¥ä¸­æå‡ºçš„ç¡®åˆ‡é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³çœ‹åˆ°è¿™ä¸ªç¤¾äº¤åª’ä½“åˆ—æ‰€æé—®çš„ç¡®åˆ‡é—®é¢˜ã€‚
- en: then I can just access that schema data frame and do a dot Lo because the indexes
    are going to be the column names And then we can just search for social media
    and if I run thatã€‚ then we can see that the question that they asked on the survey
    specifically was what social media site do you use the most so we can see that
    we get a few different responses here But which of these are the most popular
    So to find that outã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¯ä»¥ç›´æ¥è®¿é—®é‚£ä¸ªæ¨¡å¼æ•°æ®æ¡†å¹¶è¿›è¡Œç‚¹é€‰æ‹©ï¼Œå› ä¸ºç´¢å¼•å°†æ˜¯åˆ—åã€‚ç„¶åæˆ‘ä»¬å¯ä»¥æœç´¢ç¤¾äº¤åª’ä½“ï¼Œå¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è°ƒæŸ¥ä¸­ä»–ä»¬å…·ä½“é—®çš„é—®é¢˜æ˜¯ä½ æœ€å¸¸ä½¿ç”¨å“ªä¸ªç¤¾äº¤åª’ä½“ç½‘ç«™ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰å‡ ä¸ªä¸åŒçš„å›ç­”ï¼Œä½†è¿™äº›ä¸­å“ªä¸ªæ˜¯æœ€å—æ¬¢è¿çš„å‘¢ï¼Ÿæ‰€ä»¥è¦æ‰¾å‡ºè¿™ä¸ªã€‚
- en: let's look at the value counts of this series to see what the most popular social
    media sites are overall for these developers So I'm going to run this and then
    I'm going to run that value counts function hereã€‚ And now we can see here at the
    top that Reddit was the most popular with about 14000 people and then we haveã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªç³»åˆ—çš„å€¼è®¡æ•°ï¼Œä»¥äº†è§£è¿™äº›å¼€å‘è€…çš„æ€»ä½“æœ€å—æ¬¢è¿ç¤¾äº¤åª’ä½“ç½‘ç«™æ˜¯ä»€ä¹ˆã€‚å› æ­¤ï¼Œæˆ‘å°†è¿è¡Œè¿™ä¸ªï¼Œç„¶ååœ¨è¿™é‡Œè¿è¡Œå€¼è®¡æ•°å‡½æ•°ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒRedditåœ¨é¡¶éƒ¨æ˜¯æœ€å—æ¬¢è¿çš„ï¼Œå¤§çº¦æœ‰14000äººï¼Œç„¶åæˆ‘ä»¬æœ‰ã€‚
- en: YouTubeï¼Œ Whatsappï¼Œ Facebookï¼Œ Twitterï¼Œ Instagramï¼Œ I don't use social media was
    one of the answersã€‚ Now we also have some foreign social networks hereã€‚ So I've
    never heard of theseã€‚ but I believe these are Chinese characters So this is probably
    a Chinese social media site I don't know really Russian writingã€‚ but I would assume
    that this is Russian writing hereã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: YouTubeã€Whatsappã€Facebookã€Twitterã€Instagramï¼Œæˆ‘ä¸ä½¿ç”¨ç¤¾äº¤åª’ä½“æ˜¯å…¶ä¸­ä¸€ä¸ªç­”æ¡ˆã€‚æˆ‘ä»¬è¿˜æœ‰ä¸€äº›å¤–å›½ç¤¾äº¤ç½‘ç»œåœ¨è¿™é‡Œã€‚æˆ‘ä»æœªå¬è¯´è¿‡è¿™äº›ï¼Œä½†æˆ‘ç›¸ä¿¡è¿™äº›æ˜¯ä¸­æ–‡å­—ç¬¦ã€‚æ‰€ä»¥è¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¸­å›½ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚æˆ‘å¯¹ä¿„æ–‡ä¸å¤ªäº†è§£ï¼Œä½†æˆ‘çŒœè¿™å¯èƒ½æ˜¯ä¿„æ–‡ã€‚
- en: So this is probably a Russian social media siteã€‚ So it's kind of interesting
    seeing all of these different answers from around the worldã€‚ Now one more quick
    tip if we want to see these broken down by percentage instead of raw numbersã€‚
    then we can pass in the normalized argument to the value counts function and set
    that equal to trueã€‚ So let me show you what this looks likeã€‚ So I can say normalizedize
    equals trueã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¿„ç½—æ–¯ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚çœ‹åˆ°æ¥è‡ªä¸–ç•Œå„åœ°çš„è¿™äº›ä¸åŒç­”æ¡ˆçœŸæœ‰è¶£ã€‚è¿˜æœ‰ä¸€ä¸ªå°æç¤ºï¼Œå¦‚æœæˆ‘ä»¬æƒ³çœ‹åˆ°æŒ‰ç™¾åˆ†æ¯”åˆ’åˆ†çš„ç»“æœï¼Œè€Œä¸æ˜¯åŸå§‹æ•°å­—ï¼Œæˆ‘ä»¬å¯ä»¥å°†å½’ä¸€åŒ–å‚æ•°ä¼ é€’ç»™å€¼è®¡æ•°å‡½æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºtrueã€‚è®©æˆ‘ç»™ä½ å±•ç¤ºè¿™æ˜¯ä»€ä¹ˆæ ·å­ã€‚æ‰€ä»¥æˆ‘å¯ä»¥è¯´normalizedizeç­‰äºtrueã€‚
- en: And now we're gonna get these broken down by percentage So 17% of the people
    so that they use Reddit 16 said YouTubeã€‚About 16 said Whatsapp and so onã€‚ Okayï¼Œ
    so we can see that we have some social media sites here from some other countriesã€‚
    So obviouslyï¼Œ this is most likely a regional thingã€‚ My guess would be that the
    popularity of the social media platformsã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†æŒ‰ç™¾åˆ†æ¯”è¿›è¡Œåˆ’åˆ†ã€‚æ‰€ä»¥17%çš„äººè¯´ä»–ä»¬ä½¿ç”¨Redditï¼Œ16%è¯´YouTubeï¼Œçº¦16%è¯´Whatsappï¼Œç­‰ç­‰ã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œæœ‰ä¸€äº›æ¥è‡ªå…¶ä»–å›½å®¶çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚è¿™æ˜¾ç„¶å¯èƒ½æ˜¯åœ°åŒºæ€§çš„é—®é¢˜ã€‚æˆ‘çŒœç¤¾äº¤åª’ä½“å¹³å°çš„å—æ¬¢è¿ç¨‹åº¦ã€‚
- en: varies a lot based on what country you're inã€‚ So how would we break up these
    results so that we can see the most popular social media sites for each countryã€‚
    Nowï¼Œ in order to do thisï¼Œ we're going to have to learn about grouping our dataã€‚
    So againã€‚ this is a topic that can be a little confusing when you first see itã€‚
    So let me start off slow so that we can see exactly what's going on hereã€‚ So first
    of allã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å› å›½å®¶è€Œå¼‚ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•æ‹†åˆ†è¿™äº›ç»“æœï¼Œä»¥ä¾¿çœ‹åˆ°æ¯ä¸ªå›½å®¶æœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™å‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¹ å¦‚ä½•å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ã€‚æ‰€ä»¥è¿™åˆæ˜¯ä¸€ä¸ªåˆçœ‹å¯èƒ½ä¼šè®©äººå›°æƒ‘çš„è¯é¢˜ã€‚è®©æˆ‘æ…¢æ…¢å¼€å§‹ï¼Œä»¥ä¾¿æˆ‘ä»¬ç¡®åˆ‡äº†è§£è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆã€‚é¦–å…ˆã€‚
- en: if we want to see specific results based on the country or based on some other
    columnã€‚ then we're going to have to group on that specific columnã€‚ And we have
    the group by function for thisã€‚ So what actually does it mean to say that we're
    going to use the group by functionã€‚ So in the pandas documentationï¼Œ it says that
    a group by operation involves some combinationã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³çœ‹åˆ°åŸºäºå›½å®¶æˆ–å…¶ä»–æŸä¸€åˆ—çš„ç‰¹å®šç»“æœï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦åœ¨é‚£ä¸ªç‰¹å®šåˆ—ä¸Šè¿›è¡Œåˆ†ç»„ã€‚æˆ‘ä»¬æœ‰ç”¨äºæ­¤çš„group byå‡½æ•°ã€‚é‚£ä¹ˆï¼Œä½¿ç”¨group byå‡½æ•°å®é™…ä¸Šæ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿåœ¨pandasæ–‡æ¡£ä¸­ï¼Œå®ƒè¯´åˆ†ç»„æ“ä½œæ¶‰åŠæŸç§ç»„åˆã€‚
- en: Of splitting the objectï¼Œ applying a function and combining the resultsã€‚ So I'm
    going to try to walk through each of those processes one at a time so that we
    can see exactly how this worksã€‚ So againï¼Œ in the pandas documentationï¼Œ it says
    that a group by operation involves some combination of splitting up our object
    applying a function and then combining those resultsã€‚ So let's do each of thoseã€‚
    Nowï¼Œ firstï¼Œ just for a referenceã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹†åˆ†å¯¹è±¡ã€åº”ç”¨å‡½æ•°å’Œç»„åˆç»“æœã€‚æ‰€ä»¥æˆ‘å°†å°è¯•é€ä¸€è®²è§£æ¯ä¸ªè¿‡ç¨‹ï¼Œä»¥ä¾¿æˆ‘ä»¬ç¡®åˆ‡äº†è§£è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æ‰€ä»¥åœ¨pandasæ–‡æ¡£ä¸­ï¼Œå®ƒè¯´åˆ†ç»„æ“ä½œæ¶‰åŠæŸç§ç»„åˆï¼Œæ‹†åˆ†å¯¹è±¡ã€åº”ç”¨å‡½æ•°ï¼Œç„¶åç»„åˆè¿™äº›ç»“æœã€‚è®©æˆ‘ä»¬é€ä¸€è¿›è¡Œã€‚é¦–å…ˆï¼Œä½œä¸ºå‚è€ƒã€‚
- en: let's display the value counts for each country so that we can see the countries
    that have the most results for this particular survey so to do this we can just
    access the country columnã€‚ And if I run thisï¼Œ we can see that this gives us the
    country that each respondent said that they were fromã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ˜¾ç¤ºæ¯ä¸ªå›½å®¶çš„å€¼è®¡æ•°ï¼Œä»¥ä¾¿çœ‹åˆ°åœ¨è¿™æ¬¡ç‰¹å®šè°ƒæŸ¥ä¸­ç»“æœæœ€å¤šçš„å›½å®¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€è®¿é—®å›½å®¶åˆ—ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸ªå—è®¿è€…æ‰€è¯´çš„å›½å®¶ã€‚
- en: And if we look at the value counts for thisã€‚ then this is going to tally up
    all of the unique responses so we can see that the majority of this survey was
    answered by developers in the United Statesã€‚And in second was Indiaï¼Œ then Germanyï¼Œ
    United Kingdomï¼Œ Canada and so onã€‚ Okayã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªçš„å€¼è®¡æ•°ï¼Œè¿™å°†æ±‡æ€»æ‰€æœ‰ç‹¬ç‰¹çš„å“åº”ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§å¤šæ•°è°ƒæŸ¥æ˜¯ç”±ç¾å›½çš„å¼€å‘äººå‘˜å›ç­”çš„ã€‚å…¶æ¬¡æ˜¯å°åº¦ï¼Œç„¶åæ˜¯å¾·å›½ã€è‹±å›½ã€åŠ æ‹¿å¤§ç­‰ã€‚å¥½çš„ã€‚
- en: so now let's look at how to use the group by function on our country columnã€‚
    So firstã€‚ we're going to split the objectã€‚ and then we're going to apply a functionã€‚
    And then it will combine those resultsã€‚ So firstï¼Œ let's look at splitting the
    objectã€‚ Nowã€‚ in this caseï¼Œ we want to group all of the results by countryã€‚ So
    to do thisã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨æˆ‘ä»¬çš„å›½å®¶åˆ—ä¸Šä½¿ç”¨åˆ†ç»„å‡½æ•°ã€‚æ‰€ä»¥é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ‹†åˆ†å¯¹è±¡ï¼Œç„¶ååº”ç”¨ä¸€ä¸ªå‡½æ•°ã€‚ç„¶åå®ƒå°†ç»„åˆè¿™äº›ç»“æœã€‚æ‰€ä»¥é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ‹†åˆ†å¯¹è±¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æƒ³æŒ‰å›½å®¶åˆ†ç»„æ‰€æœ‰ç»“æœã€‚æ‰€ä»¥ä¸ºæ­¤ã€‚
- en: we can simply say D F dot group byã€‚ And then we will pass inã€‚ This is going
    to be a list of columns that we want to group1ã€‚ And I'm just going to pass in
    a single column here for countryã€‚ So if I run thisã€‚ then what we get back here
    is this data frame group by objectã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¯´`DF.group_by`ã€‚ç„¶åæˆ‘ä»¬å°†ä¼ å…¥ã€‚è¿™å°†æ˜¯æˆ‘ä»¬æƒ³è¦åˆ†ç»„çš„åˆ—çš„åˆ—è¡¨ã€‚æˆ‘å°†åªä¼ å…¥ä¸€ä¸ªåˆ—ï¼Œè¿™é‡Œæ˜¯å›½å®¶ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿”å›çš„æ˜¯è¿™ä¸ªæ•°æ®æ¡†çš„åˆ†ç»„å¯¹è±¡ã€‚
- en: So what is this object and what exactly can we do with thisã€‚ So firstã€‚ let's
    explain a bit what this isã€‚ So this object contains a bunch of groups and better
    understand what this isã€‚Let's take a look at an individual group that this beta
    frame hasã€‚ Nowï¼Œ before we do thatã€‚ I'm going to set this as a variable so that
    we can reuse this and not have to retype our code over and over and also it'll
    be easier to readã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™ä¸ªå¯¹è±¡æ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬åˆ°åº•å¯ä»¥å¯¹å…¶åšä»€ä¹ˆï¼Ÿæ‰€ä»¥é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç¨å¾®è§£é‡Šä¸€ä¸‹è¿™æ˜¯ä»€ä¹ˆã€‚è¿™ä¸ªå¯¹è±¡åŒ…å«ä¸€å †ç»„ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™æ˜¯ä»€ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªæ•°æ®æ¡†çš„ä¸€ä¸ªå•ç‹¬ç»„ã€‚åœ¨æˆ‘ä»¬è¿™æ ·åšä¹‹å‰ï¼Œæˆ‘å°†æŠŠå®ƒè®¾ç½®ä¸ºä¸€ä¸ªå˜é‡ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥é‡ç”¨å®ƒï¼Œè€Œä¸å¿…åå¤è¾“å…¥ä»£ç ï¼ŒåŒæ—¶å®ƒä¹Ÿæ›´å®¹æ˜“é˜…è¯»ã€‚
- en: So I'm going to call this country group and I'm just going to set this equal
    to this Df do group by and now instead of typing this every time we can just reference
    this country group variable hereã€‚ So now let's take a look at one of these groupsã€‚
    So since we grouped our rows by country then we can grab a specific group by country
    nameã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†è¿™ä¸ªç§°ä¸ºå›½å®¶ç»„ï¼Œæˆ‘å°†å…¶è®¾ç½®ä¸ºè¿™ä¸ª`DF.do_group_by`ï¼Œç°åœ¨ä¸å¿…æ¯æ¬¡éƒ½è¾“å…¥è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥åªå¼•ç”¨è¿™ä¸ªå›½å®¶ç»„å˜é‡ã€‚é‚£ä¹ˆç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›ç»„ä¸­çš„ä¸€ä¸ªã€‚ç”±äºæˆ‘ä»¬æŒ‰å›½å®¶åˆ†ç»„äº†æˆ‘ä»¬çš„è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡å›½å®¶åç§°æŠ“å–ç‰¹å®šç»„ã€‚
- en: So I'll grab the group for the United Statesã€‚ so to do this we can say country
    group do git underscore group and then pass in the name of the group in this case
    I'm want to get the group for United Statesã€‚ So if I run this cell whoops and
    this is telling me that country group is not definedã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¼šè·å–ç¾å›½çš„ç»„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å›½å®¶ç»„åš`git_group`ï¼Œç„¶åä¼ å…¥ç»„çš„åç§°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æƒ³è·å–ç¾å›½çš„ç»„ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªå•å…ƒï¼Œå“ï¼Œè¿™å‘Šè¯‰æˆ‘å›½å®¶ç»„æ²¡æœ‰å®šä¹‰ã€‚
- en: And it's because I didn't rerun this cell up here after I set that variableã€‚
    So if I run this and grab the group for the United Statesã€‚ then we can see that
    we get a data frame returned here with some survey resultsã€‚ So this doesn't look
    like anything special yetã€‚ But if I look at the country name for each of these
    survey resultsã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºæˆ‘åœ¨è®¾ç½®å˜é‡åæ²¡æœ‰é‡æ–°è¿è¡Œä¸Šé¢çš„è¿™ä¸ªå•å…ƒã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªå¹¶è·å–ç¾å›½çš„ç»„ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œè¿”å›äº†ä¸€ä¸ªåŒ…å«ä¸€äº›è°ƒæŸ¥ç»“æœçš„æ•°æ®æ¡†ã€‚æ‰€ä»¥è¿™çœ‹èµ·æ¥ä¼¼ä¹æ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚ä½†æ˜¯å¦‚æœæˆ‘æŸ¥çœ‹æ¯ä¸ªè°ƒæŸ¥ç»“æœçš„å›½å®¶åç§°ã€‚
- en: the country is listed right hereã€‚ then we can see that all of these responses
    are from people who said that they were from the United Statesã€‚ And if I look
    at the group for Indiaã€‚ So if I instead change United States to India here and
    grab that groupã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›½å®¶å°±åœ¨è¿™é‡Œåˆ—å‡ºã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ‰€æœ‰è¿™äº›å›åº”éƒ½æ˜¯æ¥è‡ªå£°ç§°ä»–ä»¬æ¥è‡ªç¾å›½çš„äººã€‚å¦‚æœæˆ‘æŸ¥çœ‹å°åº¦çš„ç»„ã€‚å¦‚æœæˆ‘æŠŠç¾å›½æ¢æˆå°åº¦å¹¶è·å–é‚£ä¸ªç»„ã€‚
- en: If we look at the country hereã€‚ then these are all the survey results for people
    who said that they were from Indiaã€‚ So that's what our data frame group by object
    that we saw before consists ofã€‚ it has broken up all of the different responses
    into groups by country nameã€‚ So this would be similar to running a filter on our
    original data frameã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™é‡Œçš„å›½å®¶ï¼Œé‚£ä¹ˆè¿™äº›éƒ½æ˜¯å£°ç§°ä»–ä»¬æ¥è‡ªå°åº¦çš„äººçš„è°ƒæŸ¥ç»“æœã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ•°æ®æ¡†åˆ†ç»„å¯¹è±¡çš„ç»„æˆã€‚å®ƒå·²å°†æ‰€æœ‰ä¸åŒçš„å›åº”æŒ‰å›½å®¶åç§°åˆ†æˆäº†ç»„ã€‚å› æ­¤è¿™ç±»ä¼¼äºåœ¨æˆ‘ä»¬çš„åŸå§‹æ•°æ®æ¡†ä¸Šè¿è¡Œè¿‡æ»¤å™¨ã€‚
- en: So I should be able to get these sameã€‚ultFor a single countryã€‚ just by doing
    what we've seen in previous videos and creating a filterã€‚ So I could sayï¼Œ okayã€‚
    I want to grabã€‚I want our filter to be equal to anytime the country is equal to
    theã€‚United Statesã€‚ and then I can apply this to our data frame by sayingï¼Œ okayã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘åº”è¯¥èƒ½å¤Ÿè·å¾—è¿™äº›ç›¸åŒçš„ç»“æœï¼Œå¯¹äºå•ä¸ªå›½å®¶ï¼Œåªéœ€æŒ‰ç…§æˆ‘ä»¬åœ¨ä¹‹å‰è§†é¢‘ä¸­çœ‹åˆ°çš„åšå¹¶åˆ›å»ºä¸€ä¸ªè¿‡æ»¤å™¨ã€‚æ‰€ä»¥æˆ‘å¯ä»¥è¯´ï¼Œå¥½çš„ã€‚æˆ‘æƒ³æŠ“å–ã€‚æˆ‘å¸Œæœ›æˆ‘ä»¬çš„è¿‡æ»¤å™¨ç­‰äºå›½å®¶ç­‰äºç¾å›½æ—¶ã€‚ç„¶åæˆ‘å¯ä»¥é€šè¿‡è¯´ï¼Œå¥½çš„ï¼Œå°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„æ•°æ®æ¡†ã€‚
- en: Df do Lo and give me all the results that match that filterã€‚ And if I run this
    cellã€‚ then we can see over here in the country column that all of these results
    are respondents from the United Statesã€‚ So if we're just looking to get information
    on a single countryã€‚ then it's very similar to just creating a filter like we
    did hereã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ¡†è°ƒç”¨Loå¹¶ç»™æˆ‘æ‰€æœ‰ç¬¦åˆè¯¥è¿‡æ»¤å™¨çš„ç»“æœã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªå•å…ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨å›½å®¶åˆ—ä¸­çœ‹åˆ°æ‰€æœ‰è¿™äº›ç»“æœéƒ½æ˜¯æ¥è‡ªç¾å›½çš„å“åº”è€…ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬åªæ˜¯æƒ³è·å–å•ä¸ªå›½å®¶çš„ä¿¡æ¯ï¼Œé‚£ä¹ˆè¿™ä¸æˆ‘ä»¬åœ¨è¿™é‡Œåˆ›å»ºä¸€ä¸ªè¿‡æ»¤å™¨éå¸¸ç›¸ä¼¼ã€‚
- en: But instead of just grabbing the results for one countryã€‚ group by instead splits
    all of these responses up by country nameã€‚ So now that we have all of those split
    up and grouped by country nameã€‚ now we can apply a function and bring those results
    back togetherã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¸æ˜¯ä»…ä»…è·å–ä¸€ä¸ªå›½å®¶çš„ç»“æœã€‚æŒ‰ç»„åˆ†ç»„ä¼šå°†æ‰€æœ‰è¿™äº›å“åº”æŒ‰å›½å®¶åç§°æ‹†åˆ†ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å·²ç»å°†æ‰€æœ‰è¿™äº›æŒ‰å›½å®¶åç§°æ‹†åˆ†å’Œåˆ†ç»„ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åº”ç”¨ä¸€ä¸ªå‡½æ•°ï¼Œå¹¶å°†è¿™äº›ç»“æœåˆå¹¶åœ¨ä¸€èµ·ã€‚
- en: So what kind of function would we like to applyã€‚ Wellï¼Œ like I mentioned beforeã€‚
    maybe we want to see the most popular social media sites broken down by countryã€‚
    Nowã€‚ if you just wanted to get the most popular social media sites by theã€‚United
    States or by Indiaã€‚ then we've already seen how we can do thisã€‚ So right here
    I have some filtered results down to where we have the responses for the United
    Statesã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬æƒ³åº”ç”¨ä»€ä¹ˆæ ·çš„å‡½æ•°å‘¢ï¼Ÿæ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œä¹Ÿè®¸æˆ‘ä»¬æƒ³çœ‹çœ‹æŒ‰å›½å®¶åˆ’åˆ†çš„æœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚ç°åœ¨ï¼Œå¦‚æœä½ åªæ˜¯æƒ³è·å–ç¾å›½æˆ–å°åº¦æœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ï¼Œé‚£ä¹ˆæˆ‘ä»¬å·²ç»çœ‹åˆ°è¿‡å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘æœ‰ä¸€äº›è¿‡æ»¤åçš„ç»“æœï¼Œæ¶‰åŠåˆ°ç¾å›½çš„å“åº”ã€‚
- en: so we can just do what we did before where we ran the value counts method on
    the social media columnã€‚ So I could just say here at the endã€‚ I could access that
    social media column of that filtered data frameã€‚ And then I could just run value
    countsã€‚Hereã€‚So if I run thisã€‚ then we can see that for the United Statesï¼Œ we have
    Reddit and Twitter and Facebook and YouTube as the top four social media sitesã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬å¯ä»¥åšæˆ‘ä»¬ä¹‹å‰åšçš„äº‹æƒ…ï¼Œå³åœ¨ç¤¾äº¤åª’ä½“åˆ—ä¸Šè¿è¡Œå€¼è®¡æ•°æ–¹æ³•ã€‚æ‰€ä»¥æˆ‘å¯ä»¥åœ¨è¿™é‡Œè¯´ï¼Œåœ¨æœ€åã€‚æˆ‘å¯ä»¥è®¿é—®é‚£ä¸ªè¿‡æ»¤åçš„æ•°æ®æ¡†çš„ç¤¾äº¤åª’ä½“åˆ—ã€‚ç„¶åæˆ‘å¯ä»¥è¿è¡Œå€¼è®¡æ•°ã€‚åœ¨è¿™é‡Œã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¯¹äºç¾å›½ï¼ŒRedditã€Twitterã€Facebookå’ŒYouTubeæ˜¯å››å¤§ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚
- en: And if we wanted to look at these specifically for India then I could instead
    change that filter for India and run this and we can see that Whatsapp came first
    and then YouTube then LinkedIn and then Facebookã€‚ So these are the results for
    one specific countryã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³å…·ä½“æŸ¥çœ‹å°åº¦çš„æƒ…å†µï¼Œé‚£ä¹ˆæˆ‘å¯ä»¥å°†è¿‡æ»¤å™¨æ›´æ”¹ä¸ºå°åº¦ï¼Œç„¶åè¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°Whatsappæ’åœ¨ç¬¬ä¸€ï¼Œæ¥ç€æ˜¯YouTubeï¼ŒLinkedInå’ŒFacebookã€‚æ‰€ä»¥è¿™æ˜¯é’ˆå¯¹ä¸€ä¸ªç‰¹å®šå›½å®¶çš„ç»“æœã€‚
- en: but if we were to run this on our data frame group by object then it will give
    us the results for all of those country groupsã€‚ So if it helps you with how you
    think about thisï¼Œ you can imagine that it's similar to running a filter and then
    applying a function like we did here with a single countryã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœæˆ‘ä»¬åœ¨æ•°æ®æ¡†ä¸ŠæŒ‰å¯¹è±¡è¿›è¡Œåˆ†ç»„ï¼Œé‚£ä¹ˆå®ƒä¼šç»™æˆ‘ä»¬æ‰€æœ‰è¿™äº›å›½å®¶ç»„çš„ç»“æœã€‚æ‰€ä»¥å¦‚æœè¿™æœ‰åŠ©äºä½ ç†è§£ï¼Œå¯ä»¥æƒ³è±¡è¿™ç±»ä¼¼äºè¿è¡Œä¸€ä¸ªè¿‡æ»¤å™¨ï¼Œç„¶ååº”ç”¨ä¸€ä¸ªå‡½æ•°ï¼Œå°±åƒæˆ‘ä»¬åœ¨è¿™é‡Œå¯¹ä¸€ä¸ªå›½å®¶æ‰€åšçš„é‚£æ ·ã€‚
- en: but when we group these using the group by function and then apply a function
    then it will combine those groups to give us the results for all of those unique
    countriesã€‚ So I think this will make sense once we just see this hereã€‚ So remember
    I called our group up hereã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å½“æˆ‘ä»¬ä½¿ç”¨åˆ†ç»„åŠŸèƒ½å°†è¿™äº›æ•°æ®åˆ†ç»„ï¼Œç„¶ååº”ç”¨ä¸€ä¸ªå‡½æ•°æ—¶ï¼Œå®ƒä¼šå°†è¿™äº›ç»„åˆå¹¶ï¼Œä»¥ä¾¿ç»™æˆ‘ä»¬æ‰€æœ‰ç‹¬ç‰¹å›½å®¶çš„ç»“æœã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºä¸€æ—¦æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°è¿™ä¸€ç‚¹å°±ä¼šæ˜ç™½ã€‚æ‰€ä»¥è®°å¾—æˆ‘åœ¨è¿™é‡Œè°ƒç”¨äº†æˆ‘ä»¬çš„ç»„ã€‚
- en: country groupï¼Œ so if we come down here to the bottomï¼Œ then we can sayï¼Œ okayï¼Œ
    for the country groupã€‚ now I want to look at the social media column and I want
    to grab the value countsã€‚For that column for that entire country groupã€‚ So if
    I run thisã€‚ then what this returns is a series with the most popular social media
    sites broken down by countryã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å›½å®¶ç»„ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬åˆ°è¿™é‡Œåº•éƒ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œå¯¹äºå›½å®¶ç»„ã€‚ç°åœ¨æˆ‘æƒ³æŸ¥çœ‹ç¤¾äº¤åª’ä½“åˆ—ï¼Œå¹¶è·å–è¯¥åˆ—åœ¨æ•´ä¸ªå›½å®¶ç»„çš„å€¼è®¡æ•°ã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆè¿”å›çš„æ˜¯ä¸€ä¸ªç³»åˆ—ï¼ŒæŒ‰å›½å®¶åˆ’åˆ†çš„æœ€å—æ¬¢è¿ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚
- en: Nowï¼Œ this actually cuts off a little early hereã€‚ So let me grab a larger chunk
    of this series to get a better idea of what this looks likeã€‚ So right here at
    the endï¼Œ I'm just gonna say dothead and look at the top 50 results or soã€‚ So if
    we run thisï¼Œ then we can see here that our first country is Afghanistanã€‚ and we
    can look at the most popular social media for thatã€‚ And then go down the listã€‚
    Albaniaã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¿™å®é™…ä¸Šåœ¨è¿™é‡Œæå‰æˆªæ–­äº†ä¸€ç‚¹ã€‚æ‰€ä»¥è®©æˆ‘è·å–è¿™ä¸ªç³»åˆ—æ›´å¤§çš„ä¸€éƒ¨åˆ†ï¼Œä»¥ä¾¿æ›´å¥½åœ°äº†è§£å®ƒçš„æ ·å­ã€‚æ‰€ä»¥åœ¨è¿™é‡Œçš„ç»“å°¾ï¼Œæˆ‘å°±è¯´.dotheadï¼Œçœ‹çœ‹å‰50ä¸ªç»“æœå·¦å³ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ä¸€ä¸ªå›½å®¶æ˜¯é˜¿å¯Œæ±—ã€‚æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹é‚£é‡Œçš„æœ€å—æ¬¢è¿ç¤¾äº¤åª’ä½“ï¼Œç„¶åç»§ç»­å¾€ä¸‹åˆ—è¡¨ï¼Œé˜¿å°”å·´å°¼äºšã€‚
- en: Algeriaï¼Œ Argentina and so onã€‚ Now this is actually returning a series and this
    series has multiple indexesã€‚ It has this country index and this social media indexã€‚
    Now we haven't discussed multiple indexes in this series yetã€‚ But if anyone is
    curious about how this worksï¼Œ then maybe just leave a comment in the description
    section belowã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é˜¿å°”åŠåˆ©äºšã€é˜¿æ ¹å»·ç­‰ç­‰ã€‚ç°åœ¨è¿™å®é™…ä¸Šæ˜¯è¿”å›ä¸€ä¸ªç³»åˆ—ï¼Œè€Œè¿™ä¸ªç³»åˆ—æœ‰å¤šä¸ªç´¢å¼•ã€‚å®ƒæœ‰å›½å®¶ç´¢å¼•å’Œç¤¾äº¤åª’ä½“ç´¢å¼•ã€‚ç°åœ¨æˆ‘ä»¬è¿˜æ²¡æœ‰è®¨è®ºè¿™ä¸ªç³»åˆ—ä¸­çš„å¤šä¸ªç´¢å¼•ã€‚ä½†æ˜¯å¦‚æœæœ‰äººå¯¹è¿™å¦‚ä½•è¿ä½œæ„Ÿåˆ°å¥½å¥‡ï¼Œæˆ–è®¸å¯ä»¥åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†ç•™è¨€ã€‚
- en: And maybe we can cover that topic in a future videoã€‚ But the country is the
    first indexã€‚And we can grab these just like we would with any other seriesã€‚ Soï¼Œ
    againã€‚ if I wanted to grab those most popular social media sites for Indiaï¼Œ for
    exampleã€‚ then I could just come up hereã€‚ And with that returned seriesã€‚ actuallyã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥åœ¨æœªæ¥çš„è§†é¢‘ä¸­è®¨è®ºè¿™ä¸ªè¯é¢˜ã€‚ä½†å›½å®¶æ˜¯ç¬¬ä¸€ä¸ªç´¢å¼•ã€‚æˆ‘ä»¬å¯ä»¥åƒå¤„ç†ä»»ä½•å…¶ä»–ç³»åˆ—ä¸€æ ·è·å–è¿™äº›æ•°æ®ã€‚æ‰€ä»¥ï¼Œå¦‚æœæˆ‘æƒ³è·å–å°åº¦æœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ï¼Œé‚£ä¹ˆæˆ‘å¯ä»¥ç›´æ¥æ¥åˆ°è¿™é‡Œï¼Œä½¿ç”¨é‚£ä¸ªè¿”å›çš„ç³»åˆ—ã€‚
- en: let's take a look at this againã€‚ So here's the index hereã€‚ I can grab that series
    just by saying dot Loã€‚And then looking for Indiaã€‚ And we can see that those are
    the same results that we got beforeã€‚ Nowï¼Œ you might be wonderingã€‚ wellï¼Œ heyï¼Œ if
    those are the same results that I got beforeï¼Œ then why is this even usefulã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å†çœ‹ä¸€ä¸‹ã€‚è¿™æ˜¯è¿™é‡Œçš„ç´¢å¼•ã€‚æˆ‘å¯ä»¥é€šè¿‡è¯´ç‚¹ç‚¹Loæ¥è·å–è¿™ä¸ªç³»åˆ—ï¼Œç„¶åæŸ¥æ‰¾å°åº¦ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™äº›ç»“æœå’Œä¹‹å‰å¾—åˆ°çš„æ˜¯ä¸€æ ·çš„ã€‚ç°åœ¨ï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œå—¯ï¼Œå¦‚æœè¿™äº›ç»“æœå’Œæˆ‘ä¹‹å‰å¾—åˆ°çš„æ˜¯ä¸€æ ·çš„ï¼Œé‚£è¿™è¿˜æœ‰ä»€ä¹ˆç”¨å‘¢ã€‚
- en: And it's useful because now we can see this result with any country without
    running a filter on each individual country in the worldã€‚ Soï¼Œ for exampleï¼Œ if
    I wanted to see the most popular social media sites for the United Statesã€‚ then
    nowï¼Œ instead of you know changing a filter over and overï¼Œ I could justã€‚ you know
    go here and look at the United States index for this return seriesã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä»»ä½•å›½å®¶çš„ç»“æœï¼Œè€Œä¸éœ€è¦å¯¹ä¸–ç•Œä¸Šæ¯ä¸ªå›½å®¶å•ç‹¬è¿è¡Œè¿‡æ»¤å™¨ã€‚æ‰€ä»¥ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæˆ‘æƒ³æŸ¥çœ‹ç¾å›½æœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ï¼Œé‚£ä¹ˆç°åœ¨ï¼Œé™¤äº†ä¸æ–­æ›´æ”¹è¿‡æ»¤å™¨ï¼Œæˆ‘å¯ä»¥ç›´æ¥æŸ¥çœ‹è¿™ä¸ªè¿”å›ç³»åˆ—çš„ç¾å›½ç´¢å¼•ã€‚
- en: And now we can see those resultsã€‚ So I think it's really interesting being able
    to play around with your data like this and being able to exploreã€‚ I really like
    seeing the different results for different countriesã€‚ And a lot of these sites
    I've never heard ofã€‚ So for exampleã€‚ if we look at the most popular social media
    sites in China or in Russiaã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™äº›ç»“æœã€‚æ‰€ä»¥æˆ‘è§‰å¾—èƒ½å¤Ÿåƒè¿™æ ·ç©å¼„æ•°æ®å¹¶è¿›è¡Œæ¢ç´¢çœŸçš„å¾ˆæœ‰è¶£ã€‚æˆ‘éå¸¸å–œæ¬¢çœ‹åˆ°ä¸åŒå›½å®¶çš„ä¸åŒç»“æœï¼Œè€Œè¿™äº›ç½‘ç«™ä¸­æœ‰å¾ˆå¤šæˆ‘ä»æœªå¬è¯´è¿‡ã€‚æ‰€ä»¥ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä¸­å›½æˆ–ä¿„ç½—æ–¯æœ€å—æ¬¢è¿çš„ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚
- en: then let me look at China hereã€‚We can see that yeah it does look like that was
    a Chinese social media siteã€‚ this waychat or Wechatï¼Œ and then we have I'm assuming
    this is pronounced Webo maybe but yeah I think that's very interesting if we want
    to look at Russia then we can't actually say just Russia in this survey Russia
    was called the Russian Federation I've made that mistake before where I just type
    in Russia and it'll tell you that it cannot find an index with that name so this
    is actually Russian Federation and if we search for that then we can see I don't
    know how to pronounce this but the one that I thought was Russian writing before
    it does look like that was in fact Russian and just remember if it makes more
    sense for you to look at percentages instead of just raw numbers here then you
    can always set normalizedize equal to true and it will give you percentage results
    instead of the raw numberã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£è®©æˆ‘åœ¨è¿™é‡Œçœ‹çœ‹ä¸­å›½ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™ç¡®å®çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªä¸­å›½ç¤¾äº¤åª’ä½“ç½‘ç«™ï¼Œå¯èƒ½æ˜¯å¾®ä¿¡ï¼Œç„¶åæˆ‘çŒœè¿™ä¸ªå‘éŸ³æ˜¯å¾®åšï¼Œä½†æˆ‘è§‰å¾—è¿™éå¸¸æœ‰è¶£ã€‚å¦‚æœæˆ‘ä»¬æƒ³çœ‹çœ‹ä¿„ç½—æ–¯ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸ªè°ƒæŸ¥ä¸­æˆ‘ä»¬ä¸èƒ½ä»…ä»…è¯´ä¿„ç½—æ–¯ï¼Œä¿„ç½—æ–¯è¢«ç§°ä¸ºä¿„ç½—æ–¯è”é‚¦ã€‚æˆ‘ä¹‹å‰çŠ¯è¿‡è¿™æ ·çš„é”™è¯¯ï¼Œåªè¾“å…¥ä¿„ç½—æ–¯ï¼Œå®ƒä¼šå‘Šè¯‰ä½ æ‰¾ä¸åˆ°è¿™ä¸ªåç§°çš„ç´¢å¼•ï¼Œæ‰€ä»¥å®é™…ä¸Šæ˜¯ä¿„ç½—æ–¯è”é‚¦ã€‚å¦‚æœæˆ‘ä»¬æœç´¢è¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä¸çŸ¥é“è¯¥æ€ä¹ˆå‘éŸ³ï¼Œä½†æˆ‘ä¹‹å‰è®¤ä¸ºæ˜¯ä¿„æ–‡çš„é‚£ä¸ªï¼Œç¡®å®çœ‹èµ·æ¥æ˜¯ä¿„æ–‡ã€‚è¯·è®°ä½ï¼Œå¦‚æœä½ è§‰å¾—æŸ¥çœ‹ç™¾åˆ†æ¯”æ¯”å•çº¯çš„åŸå§‹æ•°å­—æ›´æœ‰æ„ä¹‰ï¼Œä½ æ€»æ˜¯å¯ä»¥å°†normalizedizeè®¾ç½®ä¸ºtrueï¼Œè¿™æ ·å®ƒä¼šç»™ä½ ç™¾åˆ†æ¯”ç»“æœï¼Œè€Œä¸æ˜¯åŸå§‹æ•°å­—ã€‚
- en: So we can see that this Russian social media site here has 30% or 30% of the
    people from Russia said that that was their most popular social networkã€‚ And if
    we go back to China then we can see that this one here at the topã€‚ that has 67%
    of the developers from China said that that was the social media site that they
    used the mostã€‚ So I just thought that was really interesting being able to play
    around with these numbers and seeing the different results for different countriesã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªä¿„ç½—æ–¯ç¤¾äº¤åª’ä½“ç½‘ç«™æœ‰30%çš„äººè¡¨ç¤ºè¿™æ˜¯ä»–ä»¬æœ€å—æ¬¢è¿çš„ç¤¾äº¤ç½‘ç»œã€‚å¦‚æœæˆ‘ä»¬å›åˆ°ä¸­å›½ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é¡¶éƒ¨çš„è¿™ä¸ªï¼Œæœ‰67%çš„å¼€å‘è€…è¡¨ç¤ºè¿™æ˜¯ä»–ä»¬ä½¿ç”¨æœ€å¤šçš„ç¤¾äº¤åª’ä½“ç½‘ç«™ã€‚æ‰€ä»¥æˆ‘è§‰å¾—èƒ½ç©å¼„è¿™äº›æ•°å­—ï¼Œçœ‹åˆ°ä¸åŒå›½å®¶çš„ä¸åŒç»“æœçœŸçš„å¾ˆæœ‰è¶£ã€‚
- en: And this is the kind of thing that we can do once we've got these skills down
    within pandasã€‚ And a lot of the times it's just fun being able to explore your
    data like thisã€‚ and finding things within your data that you might not have expectedã€‚
    Now bringing this back to what we were discussing at the beginning of the video
    we can also use this to run more traditional aggregate functions like mean median
    and things like thatã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æŒæ¡äº†è¿™äº›æŠ€èƒ½ï¼Œæˆ‘ä»¬å°±å¯ä»¥åšè¿™æ ·çš„äº‹æƒ…ã€‚åœ¨å¾ˆå¤šæ—¶å€™ï¼Œèƒ½å¤Ÿä»¥è¿™ç§æ–¹å¼æ¢ç´¢æ•°æ®æœ¬èº«å°±æ˜¯ä¸€ç§ä¹è¶£ï¼Œå¹¶å‘ç°æ•°æ®ä¸­å¯èƒ½æœªæ›¾é¢„æ–™åˆ°çš„ä¸œè¥¿ã€‚ç°åœ¨å›åˆ°è§†é¢‘å¼€å§‹æ—¶è®¨è®ºçš„å†…å®¹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨å®ƒæ¥è¿è¡Œæ›´ä¼ ç»Ÿçš„æ±‡æ€»å‡½æ•°ï¼Œå¦‚å‡å€¼ã€ä¸­ä½æ•°ç­‰ã€‚
- en: So before we looked at the median salaries for the entire surveyã€‚ But now let's
    break these downã€‚ğŸ˜Šã€‚Co insteadã€‚ So just like we looked at the value counts of the
    social media column we can look at the median of the salary column and that salary
    column is labeled converted compã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å‰é¢æˆ‘ä»¬æŸ¥çœ‹äº†æ•´ä¸ªè°ƒæŸ¥çš„ä¸­ä½æ•°å·¥èµ„ã€‚ä½†ç°åœ¨è®©æˆ‘ä»¬å°†å…¶æ‹†åˆ†ã€‚ğŸ˜Šå› æ­¤ï¼Œå°±åƒæˆ‘ä»¬æŸ¥çœ‹ç¤¾äº¤åª’ä½“åˆ—çš„å€¼è®¡æ•°ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹å·¥èµ„åˆ—çš„ä¸­ä½æ•°ï¼Œè¯¥å·¥èµ„åˆ—æ ‡è®°ä¸ºè½¬æ¢çš„è¡¥å¿ã€‚
- en: So to do thisï¼Œ I can just grab our country group hereã€‚ and we want to look at
    this converted comp column and now we need to tell it what aggregate function
    we want to see for all these countriesã€‚ and I want to see the median salaryaries
    for all these countriesã€‚ So if I run this then we can see that our result here
    is that it says okayã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å¯ä»¥ç›´æ¥æŠ“å–æˆ‘ä»¬å›½å®¶ç»„çš„æ•°æ®ã€‚æˆ‘ä»¬æƒ³æŸ¥çœ‹è¿™ä¸ªè½¬æ¢çš„è¡¥å¿åˆ—ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦å‘Šè¯‰å®ƒæˆ‘ä»¬æƒ³çœ‹åˆ°ä»€ä¹ˆæ±‡æ€»å‡½æ•°ã€‚æˆ‘æƒ³æŸ¥çœ‹æ‰€æœ‰å›½å®¶çš„ä¸­ä½æ•°å·¥èµ„ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç»“æœæ˜¾ç¤ºæ­£å¸¸ã€‚
- en: here is the median salary in Afghanistan here it is for Albania and so onã€‚ So
    now if you wanted toã€‚ for exampleï¼Œ see the median salary in a place like Germanyã€‚
    then we can just simply come up hereã€‚ and this is the result that we get hereã€‚
    and these are our indexesã€‚ So the indexã€‚ the indexes are country nameã€‚ So if I
    want to grab a specific country then I can just use dot Lo and type in the countryã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é˜¿å¯Œæ±—çš„ä¸­ä½æ•°å·¥èµ„ï¼Œè¿™æ˜¯é˜¿å°”å·´å°¼äºšçš„ï¼Œä¾æ­¤ç±»æ¨ã€‚æ‰€ä»¥å¦‚æœä½ æƒ³çœ‹åˆ°åƒå¾·å›½è¿™æ ·çš„åœ°æ–¹çš„ä¸­ä½æ•°å·¥èµ„ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°æŸ¥çœ‹è¿™é‡Œã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¾—åˆ°çš„ç»“æœï¼Œè¿™äº›æ˜¯æˆ‘ä»¬çš„ç´¢å¼•ã€‚ç´¢å¼•æ˜¯å›½å®¶åç§°ã€‚å¦‚æœæˆ‘æƒ³è·å–ç‰¹å®šå›½å®¶çš„æ•°æ®ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ç‚¹ç¬¦å·ï¼Œè¾“å…¥å›½å®¶åç§°ã€‚
- en: So if I run thisï¼Œ then we can see that the median salary here in Germany is
    about 63000ã€‚ Nowã€‚ maybe you're working on some analysis where you want to group
    your dataã€‚ but you also want to run multiple aggregate functions on your groupã€‚
    So let's say that we just didn't want to see the medianã€‚ But we also wanted to
    see the mean as wellã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾·å›½çš„ä¸­ä½æ•°å·¥èµ„å¤§çº¦ä¸º63000ã€‚ç°åœ¨ï¼Œæˆ–è®¸ä½ æ­£åœ¨è¿›è¡Œä¸€äº›åˆ†æï¼Œæƒ³è¦å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ï¼ŒåŒæ—¶æƒ³åœ¨ä½ çš„ç»„ä¸Šè¿è¡Œå¤šä¸ªæ±‡æ€»å‡½æ•°ã€‚å‡è®¾æˆ‘ä»¬ä¸ä»…æƒ³çœ‹åˆ°ä¸­ä½æ•°ï¼Œè¿˜æƒ³çœ‹åˆ°å‡å€¼ã€‚
- en: So to do thisï¼Œ we can use the ag method A G G and pass in all of the aggregate
    functions that we want to useã€‚ So to do this hereï¼Œ I could just sayï¼Œ let me grab
    where we ran our median hereã€‚ instead of running just the median aggregate functionã€‚
    We're going to use this ag method hereã€‚ A G Gã€‚ and now we're going to pass in
    a list of the aggregate functionsã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ag æ–¹æ³• A G Gï¼Œå¹¶ä¼ å…¥æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ‰€æœ‰æ±‡æ€»å‡½æ•°ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å¯ä»¥è¯´ï¼Œè®©æˆ‘æŠ“å–æˆ‘ä»¬è¿è¡Œä¸­ä½æ•°çš„åœ°æ–¹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ª
    ag æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä»…ä»…è¿è¡Œä¸­ä½æ•°æ±‡æ€»å‡½æ•°ã€‚ç°åœ¨æˆ‘ä»¬å°†ä¼ å…¥ä¸€ä¸ªæ±‡æ€»å‡½æ•°çš„åˆ—è¡¨ã€‚
- en: So let's say that I want to get the median firstã€‚ And then I also want to be
    able to see the meanã€‚ So if we run thisã€‚ Then we can see that we get a data frame
    with the mean and the median salaries for every countryã€‚ And againï¼Œ just like
    we did beforeã€‚OrIf I wanted to narrow this down by a specific countryã€‚ then we
    could easily do that just by grabbing one of these indexes here by country nameã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘æƒ³å…ˆè·å–ä¸­ä½æ•°ï¼Œç„¶åæˆ‘ä¹Ÿæƒ³èƒ½å¤Ÿçœ‹åˆ°å¹³å‡æ•°ã€‚å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾—åˆ°ä¸€ä¸ªåŒ…å«æ¯ä¸ªå›½å®¶çš„å¹³å‡å’Œä¸­ä½æ•°å·¥èµ„çš„æ•°æ®æ¡†ã€‚å†æ¬¡ï¼Œå¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ã€‚å¦‚æœæˆ‘æƒ³æŒ‰ç‰¹å®šå›½å®¶ç¼©å°èŒƒå›´ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªéœ€é€šè¿‡å›½å®¶åç§°è·å–è¿™äº›ç´¢å¼•ä¹‹ä¸€å³å¯ã€‚
- en: So if we wanted to look at the mean and median salaries for Canadaã€‚ then I could
    just come up here and say dot Lo and then pass inã€‚Canada hereã€‚ Let me spell that
    correctlyã€‚ And now we can see the median salary and the mean salary for Canadaã€‚
    Nowï¼Œ depending on what you're trying to doï¼Œ you might run into some issues that
    you didn't quite expectã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³æŸ¥çœ‹åŠ æ‹¿å¤§çš„å¹³å‡å’Œä¸­ä½æ•°å·¥èµ„ã€‚é‚£ä¹ˆæˆ‘å¯ä»¥ç›´æ¥åœ¨è¿™é‡Œè¾“å…¥dot Loï¼Œç„¶åä¼ å…¥åŠ æ‹¿å¤§ã€‚è®©æˆ‘æ‹¼å†™æ­£ç¡®ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŠ æ‹¿å¤§çš„ä¸­ä½æ•°å·¥èµ„å’Œå¹³å‡å·¥èµ„ã€‚æ ¹æ®ä½ çš„éœ€æ±‚ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æ„æƒ³ä¸åˆ°çš„é—®é¢˜ã€‚
- en: Soï¼Œ for exampleï¼Œ let's say that you're trying to figure out how many people
    in each country know how to use Pythonã€‚ So before we do this to our groupï¼Œ let's
    first look at how we do this with a single country using the filtering approach
    that we used earlierã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾ä½ æƒ³çŸ¥é“æ¯ä¸ªå›½å®¶æœ‰å¤šå°‘äººä¼šä½¿ç”¨Pythonã€‚åœ¨æˆ‘ä»¬å¯¹ç»„è¿›è¡Œæ­¤æ“ä½œä¹‹å‰ï¼Œå…ˆçœ‹çœ‹æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ä¹‹å‰ä½¿ç”¨çš„è¿‡æ»¤æ–¹æ³•æ¥å¤„ç†ä¸€ä¸ªå›½å®¶ã€‚
- en: So I'm going to scroll up to where we had that filterã€‚ And I'm going to copy
    that and paste that in down hereã€‚ And then I'm just going to get rid of this value
    counts section hereã€‚ So currentlyã€‚ the filter that we have here is we are filtering
    the countries down to people who said that they were from Indiaã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¦æ»šåŠ¨åˆ°æˆ‘ä»¬æœ‰é‚£ä¸ªè¿‡æ»¤å™¨çš„åœ°æ–¹ã€‚ç„¶åæˆ‘å°†å¤åˆ¶å¹¶ç²˜è´´åˆ°è¿™é‡Œã€‚ç„¶åæˆ‘å°†åˆ é™¤è¿™ä¸ªå€¼è®¡æ•°éƒ¨åˆ†ã€‚å› æ­¤ï¼Œç›®å‰æˆ‘ä»¬è¿™é‡Œçš„è¿‡æ»¤å™¨æ˜¯æˆ‘ä»¬å°†å›½å®¶è¿‡æ»¤åˆ°è‡ªç§°æ¥è‡ªå°åº¦çš„äººã€‚
- en: So nowï¼Œ in order to figure out how many people said that they knew Python within
    this surveyã€‚ we're going to use these string methods that we've seen in previous
    videosã€‚ And if you don't remember what these look likeã€‚ then we could do thisã€‚By
    doing something like thisã€‚ we could sayï¼Œ okayï¼Œ I want all of the responses for
    the people who said that they were from Indiaã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†å¼„æ¸…æ¥šåœ¨è¿™é¡¹è°ƒæŸ¥ä¸­æœ‰å¤šå°‘äººè¡¨ç¤ºçŸ¥é“Pythonã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰è§†é¢‘ä¸­è§è¿‡çš„å­—ç¬¦ä¸²æ–¹æ³•ã€‚å¦‚æœä½ ä¸è®°å¾—è¿™äº›æ˜¯ä»€ä¹ˆæ ·çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·åšã€‚é€šè¿‡åšè¿™æ ·çš„äº‹æƒ…ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œæˆ‘æƒ³è¦æ‰€æœ‰è¡¨ç¤ºæ¥è‡ªå°åº¦çš„äººçš„å›åº”ã€‚
- en: and now when I get that resultï¼Œ remember that this result here is just going
    to be a filtered version of our data frameã€‚ our original data frame and now we
    can say okayï¼Œ I also wantã€‚The language worked with is where they put difference
    the different languages that they actually useã€‚ So if we look at this language
    worked with column hereã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå½“æˆ‘å¾—åˆ°è¿™ä¸ªç»“æœæ—¶ï¼Œè¯·è®°ä½ï¼Œè¿™ä¸ªç»“æœåªæ˜¯æˆ‘ä»¬æ•°æ®æ¡†çš„ä¸€ä¸ªè¿‡æ»¤ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„åŸå§‹æ•°æ®æ¡†ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œæˆ‘è¿˜æƒ³è¦ã€‚ä½¿ç”¨çš„è¯­è¨€æ˜¯ä»–ä»¬å®é™…ä½¿ç”¨çš„ä¸åŒè¯­è¨€ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªè¯­è¨€å·¥ä½œåˆ—ã€‚
- en: then we can see that they list all of the languages that they said that they
    know and to see if Python is within this column here then I can say dot STR and
    use the string class on that return series and sayã€‚ okayï¼Œ we want where the STr
    dot contains Pythonã€‚ So this will return true for the rows that have Python and
    the languages worked with and false for the responses that don't So if I run thisã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»–ä»¬åˆ—å‡ºæ‰€æœ‰è‡ªç§°ä¼šçš„è¯­è¨€ï¼Œä»¥æ£€æŸ¥Pythonæ˜¯å¦åœ¨è¿™ä¸€åˆ—ä¸­ï¼Œç„¶åæˆ‘å¯ä»¥è¯´dot STRå¹¶åœ¨è¿”å›çš„ç³»åˆ—ä¸Šä½¿ç”¨å­—ç¬¦ä¸²ç±»ï¼Œè¡¨ç¤ºã€‚å¥½å§ï¼Œæˆ‘ä»¬æƒ³è¦STR.dotåŒ…å«Pythonçš„åœ°æ–¹ã€‚å› æ­¤ï¼Œè¿™å°†ä¸ºåŒ…å«Pythonçš„è¡Œè¿”å›trueï¼Œè€Œå¯¹ä¸åŒ…å«çš„å›åº”è¿”å›falseã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªã€‚
- en: then this just returns a series of true and false values where it tells us whether
    the language worked with column for each respondent contained that string of Pythonã€‚
    Nowï¼Œ if we want to actually count the number of people who know Pythonã€‚ then we
    can use the sum function to add all of these upã€‚ Now normally youã€‚think that some
    would only work with numerical dataï¼Œ but some will also work on bulloleanionsã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™å°†è¿”å›ä¸€ç³»åˆ—çœŸå‡å€¼ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªå—è®¿è€…åœ¨è¯­è¨€å·¥ä½œåˆ—ä¸­æ˜¯å¦åŒ…å«é‚£ä¸²Pythonå­—ç¬¦ä¸²ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å®é™…è®¡ç®—çŸ¥é“Pythonçš„äººæ•°ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨sumå‡½æ•°å°†è¿™äº›å€¼ç›¸åŠ ã€‚é€šå¸¸ï¼Œä½ ä¼šè®¤ä¸ºsumåªé€‚ç”¨äºæ•°å€¼æ•°æ®ï¼Œä½†å®ƒä¹Ÿé€‚ç”¨äºå¸ƒå°”å€¼ã€‚
- en: It will count all of the trues as one and all the falses as 0ã€‚ So to find out
    how many people know Python Then I could simply just do a dot sum here at the
    endã€‚ And if I run thisï¼Œ then we can see that around 3100 people from India who
    answered the survey said that they knew Python as one of the languages that they
    work withã€‚ Nowï¼Œ before when we wanted to run a similar aggregation function on
    our data frame group by objectã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†æŠŠæ‰€æœ‰çš„çœŸå€¼è®¡ä¸º 1ï¼ŒæŠŠæ‰€æœ‰çš„å‡å€¼è®¡ä¸º 0ã€‚æ‰€ä»¥è¦æ‰¾å‡ºå¤šå°‘äººçŸ¥é“ Pythonï¼Œæˆ‘åªéœ€åœ¨æœ€ååšä¸€ä¸ª dot sumã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§çº¦
    3100 åæ¥è‡ªå°åº¦çš„è°ƒæŸ¥è€…è¡¨ç¤ºï¼Œä»–ä»¬çŸ¥é“ Python æ˜¯ä»–ä»¬ä½¿ç”¨çš„è¯­è¨€ä¹‹ä¸€ã€‚ç°åœ¨ï¼Œåœ¨æˆ‘ä»¬æƒ³è¦å¯¹æ•°æ®æ¡†ç»„å¯¹è±¡è¿è¡Œç±»ä¼¼çš„èšåˆå‡½æ•°ä¹‹å‰ã€‚
- en: we simply took the same approach on our group by objectã€‚ Soï¼Œ for exampleã€‚ you
    might think that we could just do something like this to see all of these to see
    how many peopleã€‚ new Python from each countryï¼Œ you might think that we could sayï¼Œ
    okayï¼Œ Wellã€‚ I should just be able to do thisã€‚ I could just sayï¼Œ okayï¼Œ for this
    country groupã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹ç»„å¯¹è±¡é‡‡å–äº†ç›¸åŒçš„æ–¹æ³•ã€‚æ‰€ä»¥ä¾‹å¦‚ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼Œçœ‹çœ‹æœ‰å¤šå°‘äººæ¥è‡ªæ¯ä¸ªå›½å®¶çŸ¥é“ Pythonã€‚ä½ å¯èƒ½ä¼šè®¤ä¸ºæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œæˆ‘åº”è¯¥èƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘å¯ä»¥è¯´ï¼Œå¯¹äºè¿™ä¸ªå›½å®¶ç»„ã€‚
- en: I want to look at this language worked withã€‚Column and then see the strings
    that contain Python and sum those upã€‚ But if I run this hereã€‚ then we can see
    that we get an errorã€‚ Nowï¼Œ like I said in a previous videoã€‚ Sometimes it can be
    hard to read these panddas errors and understand exactly what we did wrongã€‚ But
    in this caseï¼Œ it actually gives us a pretty good clue as to what we did wrongã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³æŸ¥çœ‹è¿™ä¸ªè¯­è¨€å·¥ä½œçš„åˆ—ï¼Œç„¶åæŸ¥çœ‹åŒ…å« Python çš„å­—ç¬¦ä¸²å¹¶å°†å…¶ç›¸åŠ ã€‚ä½†æ˜¯å¦‚æœæˆ‘åœ¨è¿™é‡Œè¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‡ºç°äº†ä¸€ä¸ªé”™è¯¯ã€‚æ­£å¦‚æˆ‘åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­æ‰€è¯´ï¼Œæœ‰æ—¶é˜…è¯»è¿™äº›
    Pandas é”™è¯¯å¹¶ç†è§£æˆ‘ä»¬åšé”™äº†ä»€ä¹ˆå¯èƒ½å¾ˆå›°éš¾ã€‚ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå®é™…ä¸Šç»™äº†æˆ‘ä»¬ä¸€ä¸ªç›¸å½“ä¸é”™çš„çº¿ç´¢ï¼Œè¯´æ˜æˆ‘ä»¬åšé”™äº†ä»€ä¹ˆã€‚
- en: It tells us that we cannot access the attribute string of a series group by
    objectã€‚ And then it saysï¼Œ try using the apply method insteadã€‚ So the reason that
    we get this error hereã€‚ is because this is no longer just a seriesã€‚ Insteadï¼Œ this
    is a series group by objectã€‚ and it tells us to instead use the apply methodã€‚
    So when we run an apply method on a group object like thisã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå‘Šè¯‰æˆ‘ä»¬ï¼Œæ— æ³•è®¿é—®ç³»åˆ—ç»„å¯¹è±¡çš„å­—ç¬¦ä¸²å±æ€§ã€‚ç„¶åå®ƒè¯´ï¼Œå°è¯•ä½¿ç”¨ apply æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¾—åˆ°è¿™ä¸ªé”™è¯¯çš„åŸå› æ˜¯ï¼Œå› ä¸ºè¿™ä¸å†åªæ˜¯ä¸€ä¸ªç³»åˆ—ï¼Œè€Œæ˜¯ä¸€ä¸ªç³»åˆ—ç»„å¯¹è±¡ã€‚å®ƒå‘Šè¯‰æˆ‘ä»¬æ”¹ç”¨
    apply æ–¹æ³•ã€‚æ‰€ä»¥å½“æˆ‘ä»¬å¯¹åƒè¿™æ ·çš„ç»„å¯¹è±¡è¿è¡Œ apply æ–¹æ³•æ—¶ã€‚
- en: we're going to specify a function that we want to be run on every series in
    this groupã€‚ And I know that can sound a little bit confusingï¼Œ So let's actually
    see what this looks likeã€‚ and hopefully it'll clear this up a bitã€‚Soã€‚Instead of
    accessing this string class directly hereã€‚ I'm instead going to use the apply
    methodã€‚ And for anybody following along or who will download thisã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æŒ‡å®šä¸€ä¸ªå‡½æ•°ï¼Œåœ¨è¿™ä¸ªç»„ä¸­çš„æ¯ä¸ªç³»åˆ—ä¸Šè¿è¡Œã€‚æˆ‘çŸ¥é“è¿™å¯èƒ½å¬èµ·æ¥æœ‰ç‚¹å›°æƒ‘ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯ä»€ä¹ˆæ ·å­ã€‚å¸Œæœ›è¿™èƒ½ç¨å¾®æ¸…æ™°ä¸€äº›ã€‚æ‰€ä»¥ï¼Œä¸æ˜¯ç›´æ¥è®¿é—®è¿™é‡Œçš„å­—ç¬¦ä¸²ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨
    apply æ–¹æ³•ã€‚å¯¹äºä»»ä½•è·Ÿéšçš„æˆ–å°†è¦ä¸‹è½½è¿™ä¸ªçš„äººã€‚
- en: I'm going to go ahead and leave this cell with this error here so that you can
    run that and reproduce that errorã€‚ And then I'm going to do the correct way in
    this cellã€‚ So againã€‚ instead of using the string class directly on this series
    group objectã€‚ I'm instead going to use the apply methodã€‚ So let me just cut that
    outã€‚ And I'll say dot applyã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ç»§ç»­ä¿ç•™è¿™ä¸ªå•å…ƒæ ¼ä¸­çš„é”™è¯¯ï¼Œä»¥ä¾¿ä½ å¯ä»¥è¿è¡Œå®ƒå¹¶é‡ç°è¯¥é”™è¯¯ã€‚ç„¶åæˆ‘å°†åœ¨è¿™ä¸ªå•å…ƒæ ¼ä¸­åšæ­£ç¡®çš„æ–¹å¼ã€‚æ‰€ä»¥å†æ¬¡å¼ºè°ƒï¼Œä¸æ˜¯ç›´æ¥åœ¨è¿™ä¸ªç³»åˆ—ç»„å¯¹è±¡ä¸Šä½¿ç”¨å­—ç¬¦ä¸²ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨
    apply æ–¹æ³•ã€‚æ‰€ä»¥è®©æˆ‘æŠŠå®ƒå‰ªåˆ‡æ‰ï¼Œç„¶åæˆ‘ä¼šè¯´ç‚¹ applyã€‚
- en: And now we can apply a function that we want to run on each series in this groupã€‚
    So if you've seen one of the previous videosã€‚ Then you'll know that if we just
    want a nice quickã€‚ easy functionï¼Œ then we can use a lambda functionï¼Œ you could
    write another separate function if you wanted toã€‚ But here I'm going to use lambmbdaã€‚
    So lambmbda here is going to beã€‚ğŸ˜Šï¼ŒA seriesã€‚ So now we can sayã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åº”ç”¨ä¸€ä¸ªæˆ‘ä»¬æƒ³åœ¨è¿™ä¸ªç»„ä¸­çš„æ¯ä¸ªç³»åˆ—ä¸Šè¿è¡Œçš„å‡½æ•°ã€‚å¦‚æœä½ çœ‹è¿‡ä¹‹å‰çš„è§†é¢‘ï¼Œä½ ä¼šçŸ¥é“å¦‚æœæˆ‘ä»¬åªæƒ³è¦ä¸€ä¸ªç®€å•å¿«æ·çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ lambda
    å‡½æ•°ã€‚å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥å†™å¦ä¸€ä¸ªå•ç‹¬çš„å‡½æ•°ã€‚ä½†åœ¨è¿™é‡Œæˆ‘å°†ä½¿ç”¨ lambdaã€‚æ‰€ä»¥è¿™é‡Œçš„ lambda æ˜¯ã€‚ğŸ˜Šï¼Œä¸€ä¸ªç³»åˆ—ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å¯ä»¥è¯´ã€‚
- en: okayï¼Œ wellï¼Œ what do we want to returnï¼Ÿ Al rightï¼Œ Wellï¼Œ I want to return X and
    thenã€‚Since this is a seriesï¼Œ we can say x dot string dot contains Python dot sumã€‚
    So againã€‚ just one more timeï¼Œ we are running the apply method on this series groupã€‚
    and then we are passing in a function that is going to run on each one of these
    seriesã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæˆ‘ä»¬æƒ³è¦è¿”å›ä»€ä¹ˆï¼Ÿå¥½çš„ï¼Œæˆ‘æƒ³è¿”å› Xï¼Œç„¶åã€‚ç”±äºè¿™æ˜¯ä¸€ä¸ªç³»åˆ—ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ x.dot.string.dot.contains Python.dot.sumã€‚æ‰€ä»¥å†æ¬¡å¼ºè°ƒï¼Œå°±å†æ¥ä¸€æ¬¡ï¼Œæˆ‘ä»¬æ­£åœ¨å¯¹è¿™ä¸ªç³»åˆ—ç»„è¿è¡Œ
    apply æ–¹æ³•ï¼Œç„¶åä¼ é€’ä¸€ä¸ªå°†åœ¨è¿™äº›ç³»åˆ—ä¸Šè¿è¡Œçš„å‡½æ•°ã€‚
- en: and the function that we want or what we want returned from that function is
    the sum of any of the values in that series that contain the string Pythonã€‚ and
    it's going to do that for every country since we're using this country groupã€‚
    So if I run thisã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦çš„å‡½æ•°æˆ–è¿”å›çš„å†…å®¹æ˜¯è¯¥ç³»åˆ—ä¸­åŒ…å«å­—ç¬¦ä¸²Pythonçš„å€¼çš„æ€»å’Œï¼Œè¿™å°†åœ¨æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå›½å®¶åˆ†ç»„æ—¶ä¸ºæ¯ä¸ªå›½å®¶æ‰§è¡Œæ­¤æ“ä½œã€‚æ‰€ä»¥å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªã€‚
- en: Then we can see here that we seeï¼Œ okayï¼Œ in Afghanistanã€‚8 of the respondents
    said that they know Pythonã€‚ Albania was 23 and so onã€‚ Nowã€‚ seeing these numbers
    by itself isn't really that big of a help if we're trying to get an understanding
    of the percentage of people in each country who said that they know Python because
    with these results hereã€‚ we only see a single numberã€‚ we'd have to go back and
    forth and compareï¼Œ okayã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨é˜¿å¯Œæ±—ï¼Œ8åå—è®¿è€…è¡¨ç¤ºä»–ä»¬ä¼šä½¿ç”¨Pythonï¼Œé˜¿å°”å·´å°¼äºšåˆ™æœ‰23åï¼Œç­‰ç­‰ã€‚ç°åœ¨ï¼Œä»…ä»…çœ‹åˆ°è¿™äº›æ•°å­—å¹¶æ²¡æœ‰å¤ªå¤§çš„å¸®åŠ©ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³äº†è§£æ¯ä¸ªå›½å®¶å£°ç§°ä¼šä½¿ç”¨Pythonçš„äººçš„ç™¾åˆ†æ¯”ï¼Œå•é è¿™äº›ç»“æœï¼Œæˆ‘ä»¬åªèƒ½çœ‹åˆ°ä¸€ä¸ªæ•°å­—ï¼Œæˆ‘ä»¬éœ€è¦æ¥å›æ¯”è¾ƒã€‚
- en: how many people answer the survey from each country and how many of them use
    Pythonã€‚ And then we can do a calculation from there to figure out the percentage
    of people from that country who knew Pythonã€‚ But we don't want to do thatã€‚ That
    is too much to do manuallyã€‚ So we want to figure our way so that we can get Python
    and pandas to do this calculation for usã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå›½å®¶çš„è°ƒæŸ¥æœ‰å¤šå°‘äººå›ç­”ï¼Œä»¥åŠå…¶ä¸­æœ‰å¤šå°‘äººä¼šä½¿ç”¨Pythonã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä»é‚£é‡Œè®¡ç®—å‡ºè¿™ä¸ªå›½å®¶ä¸­ä¼šä½¿ç”¨Pythonçš„äººçš„ç™¾åˆ†æ¯”ã€‚ä½†æˆ‘ä»¬ä¸æƒ³æ‰‹åŠ¨è®¡ç®—ï¼Œè¿™æ ·å¤ªç¹çäº†ã€‚æ‰€ä»¥æˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œè®©Pythonå’Œpandasä¸ºæˆ‘ä»¬å®Œæˆè¿™ä¸ªè®¡ç®—ã€‚
- en: Nowï¼Œ a lot of people have asked me to put together coding problems to practice
    what we learn in these videos So you can think of this as practiceã€‚ So I'll do
    this hereã€‚ So can any of you think of a way where we canã€‚Figure out what percentage
    of people in each country know how to use Pythonã€‚ If you think that you can figure
    that outï¼Œ then you can pause the video here and try to work through this yourselfã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¾ˆå¤šäººéƒ½è®©æˆ‘æ•´ç†ä¸€äº›ç¼–ç¨‹é—®é¢˜ï¼Œä»¥ä¾¿ç»ƒä¹ æˆ‘ä»¬åœ¨è¿™äº›è§†é¢‘ä¸­å­¦åˆ°çš„å†…å®¹ã€‚æ‰€ä»¥ä½ å¯ä»¥æŠŠè¿™ä¸ªå½“ä½œç»ƒä¹ ã€‚æ‰€ä»¥æˆ‘å°†åœ¨è¿™é‡Œè¿›è¡Œå°è¯•ã€‚é‚£ä¹ˆï¼Œä½ ä»¬ä¸­æœ‰æ²¡æœ‰äººæƒ³è¿‡ä¸€ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç®—å‡ºæ¯ä¸ªå›½å®¶ä¸­ä¼šä½¿ç”¨Pythonçš„äººæ‰€å çš„ç™¾åˆ†æ¯”ï¼Ÿå¦‚æœä½ è®¤ä¸ºä½ èƒ½æ‰¾å‡ºè¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥åœ¨è¿™é‡Œæš‚åœè§†é¢‘ï¼Œè‡ªå·±è¯•ç€è§£å†³ã€‚
- en: and it's going to combine a few topics that we've discussed in the series so
    far in order to do thisã€‚ But with that saidï¼Œ I'm going to go ahead and move along
    with my solution So againã€‚ if you want to try to figure that that out on your
    ownã€‚ then you can pause the video and try to work that outã€‚ And if you did do
    thatã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”è¿™å°†ç»“åˆæˆ‘ä»¬åœ¨è¿™ä¸€ç³»åˆ—è¯¾ç¨‹ä¸­è®¨è®ºçš„ä¸€äº›ä¸»é¢˜æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚è¯è™½å¦‚æ­¤ï¼Œæˆ‘å°†ç»§ç»­æˆ‘çš„è§£å†³æ–¹æ¡ˆã€‚æ‰€ä»¥å¦‚æœä½ æƒ³è‡ªå·±è¯•ç€æ‰¾å‡ºè¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æš‚åœè§†é¢‘ï¼Œè¯•ç€è‡ªå·±è§£å†³ã€‚å¦‚æœä½ è¿™æ ·åšäº†ã€‚
- en: then I hope that you were able to get something figured out thereã€‚ But if notã€‚
    then no worries let's go ahead and walk through my solution here so that you can
    use this as practice to get better with pandas so that you can do this type of
    analysis in the futureã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¸Œæœ›ä½ èƒ½å¤Ÿåœ¨è¿™é‡Œæ‰¾åˆ°ä¸€äº›è§£å†³æ–¹æ¡ˆã€‚ä½†å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹Ÿæ²¡å…³ç³»ï¼Œè®©æˆ‘ä»¬ç»§ç»­æ¥çœ‹çœ‹æˆ‘çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æ ·ä½ å¯ä»¥ç”¨å®ƒä½œä¸ºç»ƒä¹ ï¼Œæå‡è‡ªå·±åœ¨pandasæ–¹é¢çš„æŠ€èƒ½ï¼Œä»¥ä¾¿å°†æ¥èƒ½å¤Ÿè¿›è¡Œè¿™ç§ç±»å‹çš„åˆ†æã€‚
- en: So like I saidï¼Œ in order to get the percentage of developers who know Python
    for each countryã€‚ we're going to use a combination of a few different things that
    we've learned throughout this series so farã€‚ Now there are probably several different
    ways of answering this questionã€‚ And if you have a different way that you answered
    this questionã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œä¸ºäº†è®¡ç®—æ¯ä¸ªå›½å®¶ä¼šä½¿ç”¨Pythonçš„å¼€å‘è€…ç™¾åˆ†æ¯”ï¼Œæˆ‘ä»¬å°†ç»“åˆæˆ‘ä»¬åœ¨è¿™ä¸€ç³»åˆ—è¯¾ç¨‹ä¸­å­¦åˆ°çš„ä¸€äº›ä¸åŒçš„çŸ¥è¯†ã€‚ç°åœ¨ï¼Œå›ç­”è¿™ä¸ªé—®é¢˜å¯èƒ½æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•ã€‚å¦‚æœä½ æœ‰ä¸åŒçš„è§£ç­”æ–¹æ³•ï¼Œæ¬¢è¿åˆ†äº«ã€‚
- en: Than me then definitely leave it in the description section below so that people
    can see different approaches to thisã€‚ You knowï¼Œ it's absolutely possible that
    there's a more efficient way than how I'm about to do it hereã€‚ So if there isï¼Œ
    then I'll highlight that so others can see what the best approach isã€‚ But here's
    how I'm going to do thisã€‚ So firstï¼Œ I'm going grab the total number of respondents
    from each countryã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ‰¾åˆ°äº†æ–¹æ³•ï¼Œè¯·åœ¨ä¸‹é¢çš„æè¿°åŒºç•™è¨€ï¼Œè®©å¤§å®¶å¯ä»¥çœ‹åˆ°ä¸åŒçš„æ–¹æ³•ã€‚ä½ çŸ¥é“ï¼Œç»å¯¹æœ‰å¯èƒ½æœ‰æ¯”æˆ‘æ¥ä¸‹æ¥è¦åšçš„æ›´æœ‰æ•ˆçš„æ–¹å¼ã€‚å¦‚æœæœ‰çš„è¯ï¼Œæˆ‘ä¼šå¼ºè°ƒä¸€ä¸‹ï¼Œè¿™æ ·å…¶ä»–äººä¹Ÿèƒ½çœ‹åˆ°æœ€ä½³æ–¹æ³•ã€‚ä½†æ˜¯æˆ‘æ‰“ç®—è¿™æ ·åšã€‚é¦–å…ˆï¼Œæˆ‘å°†è·å–æ¯ä¸ªå›½å®¶çš„æ€»å—è®¿è€…äººæ•°ã€‚
- en: That way we know the total number of people from each country who responded
    to this surveyã€‚ So I will just call this country respondentsï¼Œ and I will set this
    equal toã€‚Weã€‚Want to grab the value countsã€‚Of the countries hereï¼Œ soã€‚If I print
    out what we get hereã€‚ we've seen this beforeã€‚ whoopsï¼Œ and I got an error there
    because I put countyã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·æˆ‘ä»¬å°±çŸ¥é“æ¯ä¸ªå›½å®¶å›åº”è¿™é¡¹è°ƒæŸ¥çš„äººæ•°æ€»å’Œã€‚æ‰€ä»¥æˆ‘å°†ç§°ä¹‹ä¸ºå›½å®¶å›åº”è€…ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºã€‚æˆ‘ä»¬æƒ³è¦è·å–è¿™é‡Œå›½å®¶çš„å€¼è®¡æ•°ã€‚æ‰€ä»¥ã€‚å¦‚æœæˆ‘æ‰“å°å‡ºæˆ‘ä»¬å¾—åˆ°çš„ç»“æœã€‚æˆ‘ä»¬ä¹‹å‰è§è¿‡è¿™ä¸ªã€‚å“å‘€ï¼Œæˆ‘åœ¨è¿™é‡Œå‡ºé”™äº†ï¼Œå› ä¸ºæˆ‘å†™æˆäº†å¿ã€‚
- en: I'm into book countryã€‚ So if I look at thisã€‚ then these are the total number
    of respondents who said that they were from each countryã€‚ And againï¼Œ we saw this
    earlier in the videoã€‚ So now I'm going to grab the total number of people from
    each country who know Pythonã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æŠŠå®ƒæ”¹å›å›½å®¶ã€‚æ‰€ä»¥å¦‚æœæˆ‘çœ‹çœ‹è¿™ä¸ªã€‚é‚£ä¹ˆè¿™äº›æ˜¯è¯´ä»–ä»¬æ¥è‡ªæ¯ä¸ªå›½å®¶çš„æ€»å›åº”è€…äººæ•°ã€‚æˆ‘ä»¬åœ¨è§†é¢‘ä¸­ä¹Ÿçœ‹åˆ°è¿‡è¿™ä¸ªã€‚æ‰€ä»¥ç°åœ¨æˆ‘å°†è·å–æ¯ä¸ªå›½å®¶çŸ¥é“ Python çš„æ€»äººæ•°ã€‚
- en: And we just did this a second ago right hereã€‚ But I'll go ahead and do this
    again and set it as a variable so that we have all of these stepsã€‚ So I'm going
    to grabã€‚All of that that we just calculatedã€‚ And now I'm going to set this as
    a variableã€‚ And I'm going to call thisï¼Œ you knowã€‚ country uses Pythonã€‚ and then
    I'll set it equal to thatã€‚ And nowã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšæ‰å°±åœ¨è¿™é‡Œåšè¿‡è¿™ä¸ªã€‚ä½†æˆ‘ä¼šç»§ç»­è¿™æ ·åšï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºå˜é‡ï¼Œä»¥ä¾¿æˆ‘ä»¬è®°å½•æ‰€æœ‰è¿™äº›æ­¥éª¤ã€‚æ‰€ä»¥æˆ‘å°†æŠ“å–ã€‚æˆ‘ä»¬åˆšåˆšè®¡ç®—å‡ºçš„æ‰€æœ‰å†…å®¹ã€‚ç°åœ¨æˆ‘å°†æŠŠè¿™ä¸ªè®¾ç½®ä¸ºä¸€ä¸ªå˜é‡ã€‚æˆ‘å°†ç§°ä¹‹ä¸ºï¼Œä½ çŸ¥é“çš„ã€‚å›½å®¶ä½¿ç”¨
    Pythonã€‚ç„¶åæˆ‘å°†æŠŠå®ƒè®¾ä¸ºç­‰äºè¿™ä¸ªã€‚ç°åœ¨ã€‚
- en: Let's print out that variable as wellã€‚ So let me go to the next line hereï¼Œ my
    computersã€‚Kind of give them me some griefã€‚ Okayï¼Œ so these are all the people from
    each country who said that they know how to use Pythonã€‚ So now we have one variable
    that is a series that has the total number of people from each country right here
    called country respondentsã€‚ And then we have another variable that is a seriesã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¹Ÿæ‰“å°å‡ºé‚£ä¸ªå˜é‡ã€‚æ‰€ä»¥è®©æˆ‘åœ¨ä¸‹ä¸€è¡Œè¿›è¡Œï¼Œè¿™è®©æˆ‘ç”µè„‘ã€‚æœ‰ç‚¹è®©æˆ‘çƒ¦æ¼ã€‚å¥½çš„ï¼Œè¿™äº›æ˜¯æ¥è‡ªæ¯ä¸ªå›½å®¶çš„æ‰€æœ‰äººï¼Œä»–ä»¬è¯´ä»–ä»¬çŸ¥é“å¦‚ä½•ä½¿ç”¨ Pythonã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªå˜é‡æ˜¯ä¸€ä¸ªç³»åˆ—ï¼ŒåŒ…å«æ¥è‡ªæ¯ä¸ªå›½å®¶çš„æ€»äººæ•°ï¼Œç§°ä¸ºå›½å®¶å›åº”è€…ã€‚ç„¶åæˆ‘ä»¬è¿˜æœ‰å¦ä¸€ä¸ªå˜é‡æ˜¯ä¸€ä¸ªç³»åˆ—ã€‚
- en: That is the total number of people from each country who know Pythonã€‚ So now
    we need to combine these twoã€‚ Nowï¼Œ I'm actually going to use a method here that
    we haven't discussed in this series yetã€‚ So if you got stuck hereï¼Œ then that's
    completely understandableã€‚ I probably should have mention this in the video where
    we appended rows to a data frameã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ¥è‡ªæ¯ä¸ªå›½å®¶çš„çŸ¥é“ Python çš„æ€»äººæ•°ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬éœ€è¦å°†è¿™ä¸¤ä¸ªç»“åˆèµ·æ¥ã€‚ç°åœ¨ï¼Œæˆ‘å®é™…ä¸Šå°†ä½¿ç”¨ä¸€ä¸ªåœ¨è¿™ä¸ªç³»åˆ—ä¸­è¿˜æœªè®¨è®ºè¿‡çš„æ–¹æ³•ã€‚æ‰€ä»¥å¦‚æœä½ åœ¨è¿™é‡Œå¡ä½äº†ï¼Œé‚£æ˜¯å®Œå…¨å¯ä»¥ç†è§£çš„ã€‚æˆ‘å¯èƒ½åº”è¯¥åœ¨æˆ‘ä»¬å°†è¡Œé™„åŠ åˆ°æ•°æ®æ¡†çš„é‚£æ®µè§†é¢‘ä¸­æåˆ°è¿™ä¸ªã€‚
- en: but we can combine more than one series together using the pandas concat functionã€‚
    So let's see what this would look likeã€‚ So I can sayï¼Œ and I'll just call this
    data frame Python Dfã€‚ And now I'm going create a data frame where we can cat those
    two series in the oneã€‚ So I can say P D dot concatã€‚And now I'm going to pass in
    a list of the series that we want to concatenateã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ pandas çš„ concat å‡½æ•°å°†å¤šä¸ªç³»åˆ—ç»„åˆåœ¨ä¸€èµ·ã€‚æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¼šæ˜¯ä»€ä¹ˆæ ·å­ã€‚æˆ‘å¯ä»¥è¯´ï¼Œæˆ‘å°±å«è¿™ä¸ªæ•°æ®æ¡† Python Dfã€‚ç°åœ¨æˆ‘è¦åˆ›å»ºä¸€ä¸ªæ•°æ®æ¡†ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸¤ä¸ªç³»åˆ—è¿æ¥åœ¨ä¸€èµ·ã€‚æ‰€ä»¥æˆ‘å¯ä»¥è¯´
    P D.dot concatã€‚ç°åœ¨æˆ‘è¦ä¼ å…¥ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦æ‹¼æ¥çš„ç³»åˆ—åˆ—è¡¨ã€‚
- en: So I want this to be our country respondentsã€‚ And I also want toã€‚Add in this
    country uses Python seriesï¼Œ and nowã€‚We also want to set axis equal to columns
    because by defaultã€‚ it's going to try to concatenate these on rowï¼Œ but we want
    to match up the indexes here so that it concates it that way insteadã€‚ So we want
    to say axis is equal to columns and then finallyã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¸Œæœ›è¿™äº›æ˜¯æˆ‘ä»¬å›½å®¶çš„å›åº”è€…ã€‚è€Œä¸”æˆ‘è¿˜æƒ³ã€‚åŠ å…¥è¿™ä¸ªå›½å®¶ä½¿ç”¨ Python çš„ç³»åˆ—ï¼Œç°åœ¨ã€‚æˆ‘ä»¬è¿˜æƒ³å°†è½´è®¾ç½®ä¸ºåˆ—ï¼Œå› ä¸ºé»˜è®¤æƒ…å†µä¸‹ã€‚å®ƒä¼šå°è¯•åœ¨è¡Œä¸Šè¿›è¡Œæ‹¼æ¥ï¼Œä½†æˆ‘ä»¬å¸Œæœ›å°†ç´¢å¼•åŒ¹é…ï¼Œè¿™æ ·æ‰èƒ½ä»¥é‚£ç§æ–¹å¼è¿›è¡Œæ‹¼æ¥ã€‚æ‰€ä»¥æˆ‘ä»¬æƒ³è¯´è½´ç­‰äºåˆ—ï¼Œç„¶åæœ€åã€‚
- en: I'm also going to put sort is equal to falseã€‚ Nowï¼Œ if you watch a previous videoã€‚
    this isn't absolutely necessaryï¼Œ but if you run it without sort equal to falseã€‚
    then it'll give you a warning saying that in a future version of pandasã€‚ that
    it'll sort by default or sort by false on defaultã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜å°†è®¾ç½® sort ç­‰äº falseã€‚ç°åœ¨ï¼Œå¦‚æœä½ è§‚çœ‹äº†ä¹‹å‰çš„è§†é¢‘ã€‚è¿™ä¸æ˜¯ç»å¯¹å¿…è¦çš„ï¼Œä½†å¦‚æœä½ åœ¨æ²¡æœ‰è®¾ç½® sort ç­‰äº false çš„æƒ…å†µä¸‹è¿è¡Œå®ƒã€‚å®ƒä¼šç»™ä½ ä¸€ä¸ªè­¦å‘Šï¼Œå‘Šè¯‰ä½ åœ¨æœªæ¥ç‰ˆæœ¬çš„
    pandas ä¸­ã€‚å®ƒä¼šé»˜è®¤æ’åºæˆ–é»˜è®¤ä¸æ’åºã€‚
- en: So it's better just to go ahead and specify if you want the resulting data frame
    sorted or notã€‚ So now let's look at this concatenated data frame hereã€‚ Okayï¼Œ so
    now we have a data frame hereã€‚ where these two series have been concatenated and
    match up on the same indexã€‚ So this is a lot more useful becauseã€‚Now we can seeï¼Œ
    okay there were about 20ï¼Œ000 or 21ã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æœ€å¥½ç›´æ¥æŒ‡å®šä½ æ˜¯å¦å¸Œæœ›ç»“æœæ•°æ®æ¡†æ’åºã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªè¿æ¥çš„æ•°æ®æ¡†ã€‚å¥½çš„ï¼Œç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®æ¡†ï¼Œå…¶ä¸­è¿™ä¸¤ä¸ªç³»åˆ—å·²è¢«è¿æ¥ï¼Œå¹¶åœ¨ç›¸åŒçš„ç´¢å¼•ä¸ŠåŒ¹é…ã€‚è¿™æ›´æœ‰ç”¨ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¤§çº¦æœ‰20000æˆ–21000ã€‚
- en: 000 people who said that they were from the United States and about 10000 people
    who said that they know Python so that's definitely a lot better and more useful
    information Now one thing about this new data frame that we have is some columns
    that don't really relate to what we're talking about anymore we can see here that
    this one is just called country and this one is called languages worked with so
    let's rename these so that they make more sense in the context of what we're actually
    trying to do and we saw how to rename columns in a previous video as wellã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 1000åå£°ç§°æ¥è‡ªç¾å›½çš„äººå’Œå¤§çº¦10000åå£°ç§°çŸ¥é“Pythonçš„äººï¼Œæ‰€ä»¥è¿™è‚¯å®šæ˜¯æ›´å¥½å’Œæ›´æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç°åœ¨å…³äºè¿™ä¸ªæ–°æ•°æ®æ¡†çš„ä¸€ä»¶äº‹æ˜¯ï¼Œæœ‰äº›åˆ—ä¸æˆ‘ä»¬æ­£åœ¨è®¨è®ºçš„å†…å®¹å¹¶æ²¡æœ‰çœŸæ­£çš„å…³ç³»ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªå«åšå›½å®¶ï¼Œè€Œè¿™ä¸ªå«åšä½¿ç”¨çš„è¯­è¨€ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬é‡å‘½åè¿™äº›ï¼Œä½¿å®ƒä»¬åœ¨æˆ‘ä»¬å®é™…å°è¯•åšçš„ä¸Šä¸‹æ–‡ä¸­æ›´æœ‰æ„ä¹‰ï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°å¦‚ä½•åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­é‡å‘½ååˆ—ã€‚
- en: but if you forgotï¼Œ then you can do this just by grabbing our data frame here
    and I'll say Python Dfã€‚ which is our data frame dot rename and now what do we
    want to rename we want to rename the columns and now I'm going to pass in a dictionary
    here whereã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœä½ å¿˜è®°äº†ï¼Œé‚£ä¹ˆä½ å¯ä»¥é€šè¿‡æŠ“å–æˆ‘ä»¬çš„æ•°æ®æ¡†æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä¼šè¯´Python Dfï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æ•°æ®æ¡†.dot renameï¼Œç°åœ¨æˆ‘ä»¬æƒ³è¦é‡å‘½åä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬æƒ³è¦é‡å‘½ååˆ—ï¼Œæˆ‘å°†ä¼ å…¥ä¸€ä¸ªå­—å…¸ã€‚
- en: The key is the previous valueï¼Œ and the value is going to be the updated valueã€‚
    So I will call this a number of respondentsã€‚And then I also want to change this
    languages worked with column hereã€‚ And I want to change this to beã€‚Let's call
    this nu nose Pythonã€‚ And if I run thisã€‚ then we can see that this looks goodã€‚
    We have number of respondents from the United States and number nose Python from
    the United Statesã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®åœ¨äºä¹‹å‰çš„å€¼ï¼Œè€Œè¿™ä¸ªå€¼å°†æ˜¯æ›´æ–°åçš„å€¼ã€‚æ‰€ä»¥æˆ‘å°†å…¶ç§°ä¸ºå›åº”è€…æ•°é‡ã€‚ç„¶åæˆ‘è¿˜æƒ³æ›´æ”¹è¿™ä¸ªâ€œä½¿ç”¨çš„è¯­è¨€â€åˆ—ã€‚æˆ‘æƒ³æŠŠå®ƒæ”¹æˆï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºçŸ¥é“Pythonçš„äººæ•°ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™çœ‹èµ·æ¥ä¸é”™ã€‚æˆ‘ä»¬æœ‰æ¥è‡ªç¾å›½çš„å›åº”è€…æ•°é‡å’ŒçŸ¥é“Pythonçš„æ•°é‡ã€‚
- en: So that looks good to meã€‚ So since it looks goodï¼Œ I'm going to say in place
    is equal to trueã€‚ so that it actually modifies our data frameã€‚ So if I run that
    and then look at our data frame one more timeã€‚ thenã€‚ğŸ˜Šï¼ŒWe can see that it has been
    updated with those new columnsã€‚ Now we have the total number of respondents from
    each country and the number of people who know Python from each country in one
    data frameã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™çœ‹èµ·æ¥ä¸é”™ã€‚ç”±äºå®ƒçœ‹èµ·æ¥ä¸é”™ï¼Œæˆ‘å°†è¯´in placeç­‰äºtrueï¼Œè¿™æ ·å®ƒå®é™…ä¼šä¿®æ”¹æˆ‘ä»¬çš„æ•°æ®æ¡†ã€‚å¦‚æœæˆ‘è¿è¡Œè¿™ä¸ªï¼Œç„¶åå†æŸ¥çœ‹æˆ‘ä»¬çš„æ•°æ®æ¡†ï¼Œé‚£ä¹ˆğŸ˜Šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒå·²ç”¨è¿™äº›æ–°åˆ—æ›´æ–°ã€‚ç°åœ¨æˆ‘ä»¬æœ‰æ¥è‡ªæ¯ä¸ªå›½å®¶çš„å›åº”è€…æ€»æ•°ï¼Œä»¥åŠæ¯ä¸ªå›½å®¶çŸ¥é“Pythonçš„äººæ•°ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨ä¸€ä¸ªæ•°æ®æ¡†ä¸­ã€‚
- en: So we have all the information that we need to calculate a percentageã€‚ Now all
    we need to do is create a new column and calculate thisã€‚ So if you remember in
    order to create a new columnï¼Œ we can simply just assign itã€‚ So I will call this
    column PCt for percentageï¼Œ knows Pythonã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æœ‰è®¡ç®—ç™¾åˆ†æ¯”æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚ç°åœ¨æˆ‘ä»¬æ‰€éœ€çš„å°±æ˜¯åˆ›å»ºä¸€ä¸ªæ–°åˆ—å¹¶è¿›è¡Œè®¡ç®—ã€‚æ‰€ä»¥å¦‚æœä½ è¿˜è®°å¾—ï¼Œä¸ºäº†åˆ›å»ºä¸€ä¸ªæ–°åˆ—ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°èµ‹å€¼ã€‚æ‰€ä»¥æˆ‘å°†è¿™ä¸ªåˆ—ç§°ä¸ºPCtï¼Œç”¨äºè¡¨ç¤ºçŸ¥é“Pythonçš„ç™¾åˆ†æ¯”ã€‚
- en: And now what do we want this to be equal toã€‚ Wellã€‚ if you don't know how to
    calculate a percentage mathematicallyã€‚ basically what you do is you take the part
    and then divide that by the wholeã€‚ and then you multiply that by 100ã€‚ So our part
    here is the number of people who know Pythonã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³è¦è¿™ä¸ªç­‰äºä»€ä¹ˆå‘¢ï¼Ÿå¥½å§ï¼Œå¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•ç”¨æ•°å­¦è®¡ç®—ç™¾åˆ†æ¯”ï¼ŒåŸºæœ¬ä¸Šä½ éœ€è¦å…ˆå–å‡ºéƒ¨åˆ†ï¼Œç„¶åç”¨å®ƒé™¤ä»¥æ•´ä½“ï¼Œå†ä¹˜ä»¥100ã€‚æ‰€ä»¥è¿™é‡Œçš„éƒ¨åˆ†æ˜¯çŸ¥é“Pythonçš„äººæ•°ã€‚
- en: So I will grab that and sayã€‚on underscore DF and access that seriesï¼Œ access
    that columnã€‚ and then I want to divide that by the whole and the whole are the
    total number of people from that countryã€‚ so that is nu respondentsã€‚And nowï¼Œ if
    we want this to be a whole number percentageã€‚ then we can multiply this by 100ã€‚
    Okayï¼Œ so if I did all of this correctlyã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¼šæŠ“å–è¿™ä¸ªï¼Œå«åšon underscore DFï¼Œå¹¶è®¿é—®é‚£ä¸ªç³»åˆ—ï¼Œè®¿é—®é‚£ä¸ªåˆ—ã€‚ç„¶åæˆ‘æƒ³ç”¨è¿™ä¸ªéƒ¨åˆ†é™¤ä»¥æ•´ä½“ï¼Œè€Œæ•´ä½“æ˜¯æ¥è‡ªé‚£ä¸ªå›½å®¶çš„æ€»äººæ•°ï¼Œä¹Ÿå°±æ˜¯å›åº”è€…æ•°é‡ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦è¿™ä¸ªæ˜¯ä¸€ä¸ªæ•´æ•°ç™¾åˆ†æ¯”ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä¹˜ä»¥100ã€‚å¥½çš„ï¼Œå¦‚æœæˆ‘åšäº†æ‰€æœ‰è¿™äº›æ­£ç¡®çš„è¯ã€‚
- en: and it's very possible I made a mistakeã€‚ But if I did all this correctlyã€‚ then
    we should have a data frame here with the percentage of people who know Python
    from each countryã€‚ And now we can work with this just like any other data frameã€‚
    So let's say that we wanted to sort these resultsã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”å¾ˆæœ‰å¯èƒ½æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ã€‚ä½†å¦‚æœæˆ‘æ­£ç¡®åœ°å®Œæˆäº†æ‰€æœ‰è¿™äº›ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥åœ¨è¿™é‡Œæœ‰ä¸€ä¸ªæ•°æ®æ¡†ï¼Œæ˜¾ç¤ºæ¯ä¸ªå›½å®¶çŸ¥é“ Python çš„äººæ‰€å çš„ç™¾åˆ†æ¯”ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åƒå¤„ç†ä»»ä½•å…¶ä»–æ•°æ®æ¡†ä¸€æ ·å¤„ç†è¿™ä¸ªæ•°æ®æ¡†ã€‚æ‰€ä»¥å‡è®¾æˆ‘ä»¬æƒ³è¦å¯¹è¿™äº›ç»“æœè¿›è¡Œæ’åºã€‚
- en: Now we learned this in a previous video on how to sort values in a seriesã€‚ So
    let's say that we want to sort the countries by the largest percentage of respondents
    who know Pythonã€‚ So to do thisï¼Œ I can just say Python D Fã€‚t sortã€‚Underscore valuesã€‚
    And if you forget how to do any of thisï¼Œ then you can always go back to our pandas
    video where we learn about sortingã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åœ¨ä¹‹å‰çš„è§†é¢‘ä¸­å­¦ä¹ äº†å¦‚ä½•å¯¹ç³»åˆ—ä¸­çš„å€¼è¿›è¡Œæ’åºã€‚æ‰€ä»¥å‡è®¾æˆ‘ä»¬æƒ³æŒ‰ç…§çŸ¥é“ Python çš„å—è®¿è€…æ‰€å çš„æœ€å¤§ç™¾åˆ†æ¯”æ¥æ’åºå„ä¸ªå›½å®¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘å¯ä»¥ç®€å•åœ°è¯´
    `Python D F.t sort.Underscore values`ã€‚å¦‚æœä½ å¿˜è®°äº†å¦‚ä½•åšè¿™äº›äº‹æƒ…ï¼Œé‚£ä¹ˆä½ å¯ä»¥éšæ—¶å›åˆ°æˆ‘ä»¬çš„ pandas è§†é¢‘ï¼Œé‚£é‡Œæˆ‘ä»¬å­¦ä¹ äº†æ’åºã€‚
- en: So in order to sort by the people who know Python or the percentageï¼Œ we can
    sayï¼Œ okayï¼Œ sort byã€‚ what did I call this hereã€‚ Percent knows Pythonã€‚ And then
    I actually want this to be in ascendingã€‚Order equal to falseï¼Œ because I want the
    largest percentage of people who know Python at the topã€‚And I was about to put
    in place equals true firstã€‚ But let's see what this looks likeã€‚ Okayã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸ºäº†æŒ‰ç…§çŸ¥é“ Python çš„äººæˆ–å…¶ç™¾åˆ†æ¯”è¿›è¡Œæ’åºï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¥½å§ï¼ŒæŒ‰ä»€ä¹ˆæ’åºã€‚æˆ‘åœ¨è¿™é‡Œç§°ä¹‹ä¸ºâ€œçŸ¥é“ Python çš„ç™¾åˆ†æ¯”â€ã€‚ç„¶åæˆ‘å®é™…ä¸Šå¸Œæœ›è¿™ä»¥å‡åºæ’åˆ—ã€‚`Order`
    ç­‰äº `false`ï¼Œå› ä¸ºæˆ‘å¸Œæœ›çŸ¥é“ Python çš„äººæ‰€å çš„æœ€å¤§ç™¾åˆ†æ¯”æ’åœ¨æœ€ä¸Šé¢ã€‚ç„¶åæˆ‘æœ¬æ¥æ‰“ç®—å…ˆå°† `in place` è®¾ç½®ä¸º `true`ã€‚ä½†è®©æˆ‘ä»¬çœ‹çœ‹è¿™æ˜¯ä»€ä¹ˆæ ·å­ã€‚å¥½çš„ã€‚
- en: so it looks like that that sort workedï¼Œ and it looks goodã€‚ So now I'll say in
    place is equal to true so that it modifies our data frameã€‚ And now we can look
    at our results hereã€‚ So we can see here that some of these are a little misleading
    here because you knowã€‚100% of people from Sal May and Princippeï¼Œ No Pythonã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥çœ‹èµ·æ¥é‚£ä¸ªæ’åºæœ‰æ•ˆï¼Œå¹¶ä¸”æ•ˆæœä¸é”™ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä¼šè¯´ `in place` ç­‰äº `true`ï¼Œè¿™æ ·å®ƒå°±ä¼šä¿®æ”¹æˆ‘ä»¬çš„æ•°æ®æ¡†ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æˆ‘ä»¬çš„ç»“æœã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„ä¸€äº›æ•°æ®æœ‰ç‚¹è¯¯å¯¼ï¼Œå› ä¸ºä½ çŸ¥é“ã€‚æ¥è‡ªåœ£å¤šç¾å’Œæ™®æ—è¥¿æ¯”çš„
    100% çš„äººä¸æ‡‚ Pythonã€‚
- en: But we only had one person from the country who answered the surveyã€‚ And he
    happens to know Python or sheã€‚ So that is 100%ã€‚ So insteadï¼Œ let's look at the
    head hereã€‚ and grabã€‚See if we can find a country here with a larger number of
    respondentsã€‚ Soï¼Œ okayã€‚ we have 72 people from Uganda and 47 of them new Pythonã€‚
    So that's 65%ã€‚ that's pretty goodã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ¥è‡ªè¿™ä¸ªå›½å®¶çš„äººå›ç­”äº†è°ƒæŸ¥ã€‚è€Œä»–æ°å¥½çŸ¥é“ Pythonï¼Œæˆ–è€…å¥¹ã€‚å› æ­¤è¿™æ˜¯ 100%ã€‚æ‰€ä»¥ç›¸åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¤´éƒ¨ï¼Œå¹¶æŠ“å–ã€‚çœ‹çœ‹æˆ‘ä»¬èƒ½å¦æ‰¾åˆ°ä¸€ä¸ªå—è®¿è€…æ•°é‡æ›´å¤šçš„å›½å®¶ã€‚æ‰€ä»¥ï¼Œå¥½å§ã€‚æˆ‘ä»¬æœ‰
    72 åæ¥è‡ªä¹Œå¹²è¾¾çš„äººï¼Œå…¶ä¸­ 47 åçŸ¥é“ Pythonã€‚æ‰€ä»¥è¿™æ˜¯ 65%ã€‚è¿™ç›¸å½“ä¸é”™ã€‚
- en: we have ohï¼Œ okayï¼Œ so this is United Statesï¼Œ that's not bad eitherã€‚ We have about
    21000 hereã€‚ about 10000 new Pythonã€‚ So that's 48%ã€‚ So that's in the higher rangeã€‚
    that's pretty goodã€‚ Soï¼Œ yeahã€‚ I think this is a great way to practice working
    with pandasã€‚ And alsoã€‚ it's just fun being able to explore your information in
    this wayã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰å“¦ï¼Œå¥½çš„ï¼Œè¿™æ˜¯ç¾å›½ï¼Œæƒ…å†µä¹Ÿä¸é”™ã€‚è¿™é‡Œå¤§çº¦æœ‰ 21000 äººï¼Œå¤§çº¦æœ‰ 10000 æ–°çš„ Python ä½¿ç”¨è€…ã€‚æ‰€ä»¥è¿™æ˜¯ 48%ã€‚æ‰€ä»¥è¿™åœ¨è¾ƒé«˜çš„èŒƒå›´å†…ï¼Œè¿™å¾ˆå¥½ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ç»ƒä¹ ä½¿ç”¨
    pandas çš„å¥½æ–¹æ³•ã€‚è€Œä¸”ï¼Œèƒ½å¤Ÿä»¥è¿™ç§æ–¹å¼æ¢ç´¢ä½ çš„ä¿¡æ¯ä¹Ÿå¾ˆæœ‰è¶£ã€‚
- en: And now that we have a data frame with all this informationã€‚ Then we can also
    inspect a specific country to see what the percentage of developers are from a
    specific country who know Pythonã€‚ Soï¼Œ for exampleï¼Œ instead of looking throughï¼Œ
    what if I wanted to see Japanã€‚ instead of looking through all of theseã€‚ I could
    just sayï¼Œ okayï¼Œ Pythonã€‚ğŸ˜Šï¼ŒD F dot Loã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¿™äº›ä¿¡æ¯çš„æ•°æ®æ¡†ã€‚ç„¶åæˆ‘ä»¬è¿˜å¯ä»¥æ£€æŸ¥ç‰¹å®šå›½å®¶ï¼Œä»¥æŸ¥çœ‹æ¥è‡ªç‰¹å®šå›½å®¶çš„å¼€å‘è€…ä¸­çŸ¥é“ Python çš„ç™¾åˆ†æ¯”ã€‚æ‰€ä»¥ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘æƒ³æŸ¥çœ‹æ—¥æœ¬ï¼Œè€Œä¸æ˜¯é€ä¸€æŸ¥çœ‹æ‰€æœ‰è¿™äº›ã€‚æˆ‘å¯ä»¥ç®€å•åœ°è¯´ï¼Œå¥½å§ï¼Œ`Python.ğŸ˜Šï¼ŒD
    F dot Lo`ã€‚
- en: And since our country names are our indexes hereã€‚ then we can just do a dot
    Lo of Japanã€‚ and then we can see that we get these statistics for that specific
    countryã€‚ Okayã€‚ so I know that that may have been a lot to take in and that we
    covered a lot of ground in this videoã€‚ We definitely covered some more advanced
    topics here than we did in previous videosã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„å›½å®¶åç§°æ˜¯è¿™é‡Œçš„ç´¢å¼•ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å¯¹æ—¥æœ¬ä½¿ç”¨ `dot Lo`ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬å¾—åˆ°äº†è¯¥ç‰¹å®šå›½å®¶çš„ç»Ÿè®¡æ•°æ®ã€‚å¥½çš„ã€‚æ‰€ä»¥æˆ‘çŸ¥é“è¿™å¯èƒ½æœ‰å¾ˆå¤šå†…å®¹éœ€è¦æ¶ˆåŒ–ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨è¿™ä¸ªè§†é¢‘ä¸­è¦†ç›–äº†å¾ˆå¤šå†…å®¹ã€‚æˆ‘ä»¬ç¡®å®æ¶µç›–äº†ä¸€äº›æ¯”ä¹‹å‰è§†é¢‘æ›´é«˜çº§çš„ä¸»é¢˜ã€‚
- en: but I hope this kind of got you a little excited to learn what you can do with
    pandasã€‚ and the types of problems that we can solveã€‚ You knowã€‚ when you are exploring
    through your data like thisã€‚ you're probably going to make a ton of mistakes along
    the wayï¼Œ you knowã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œæˆ‘å¸Œæœ›è¿™èƒ½è®©ä½ å¯¹ä½¿ç”¨pandasæ‰€èƒ½åšçš„äº‹æƒ…æ„Ÿåˆ°å…´å¥‹ï¼Œä»¥åŠæˆ‘ä»¬å¯ä»¥è§£å†³çš„é—®é¢˜ç±»å‹ã€‚ä½ çŸ¥é“ï¼Œå½“ä½ åƒè¿™æ ·æ¢ç´¢æ•°æ®æ—¶ï¼Œä½ å¯èƒ½ä¼šåœ¨è¿‡ç¨‹ä¸­çŠ¯å¾ˆå¤šé”™è¯¯ã€‚
- en: I still make mistakes in pandas all the timeï¼Œ even in these videosï¼Œ I've made
    some mistakesã€‚ and I have these scripted outã€‚ So it definitely happensã€‚ But you
    knowã€‚ each problem that we work through similar to thisã€‚ just makes it easier
    and easier each time to work through additional problemsã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä½¿ç”¨pandasæ—¶ä»ç„¶ä¼šçŠ¯é”™è¯¯ï¼Œç”šè‡³åœ¨è¿™äº›è§†é¢‘ä¸­ï¼Œæˆ‘ä¹ŸçŠ¯äº†ä¸€äº›é”™è¯¯ã€‚è€Œä¸”è¿™äº›éƒ½æ˜¯æˆ‘äº‹å…ˆå‡†å¤‡å¥½çš„ã€‚æ‰€ä»¥è¿™ç¡®å®ä¼šå‘ç”Ÿã€‚ä½†æ˜¯ä½ çŸ¥é“ï¼Œæ¯ä¸€ä¸ªæˆ‘ä»¬è§£å†³çš„é—®é¢˜ç±»ä¼¼äºè¿™ä¸ªï¼Œéƒ½è®©åç»­å¤„ç†å…¶ä»–é—®é¢˜å˜å¾—è¶Šæ¥è¶Šå®¹æ˜“ã€‚
- en: So if you need to go back and rewatch some of these steps in order to work through
    these problems like this on your ownã€‚ then that'sã€‚ğŸ˜Šï¼ŒCompletely normalã€‚ knowï¼Œ don't
    think that just because this may have seemed difficult that there's something
    wrong with youã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ éœ€è¦å›å»é‡çœ‹è¿™äº›æ­¥éª¤ï¼Œä»¥ä¾¿è‡ªå·±è§£å†³è¿™äº›é—®é¢˜ï¼Œé‚£æ˜¯å®Œå…¨æ­£å¸¸çš„ã€‚ğŸ˜Šï¼Œä¸è¦è§‰å¾—å¦‚æœè¿™ä¼¼ä¹å¾ˆå›°éš¾ï¼Œé‚£å°±æ˜¯ä½ æœ‰ä»€ä¹ˆé—®é¢˜ã€‚
- en: It's definitely normal for this stuff to be a lot of information to take in
    And alsoã€‚ like I said beforeï¼Œ if you have some other ways of solving the problems
    that we answered hereã€‚ then like I saidï¼Œ definitely leave a comment with your
    solution in the description section belowã€‚ and I'll take a look at thoseã€‚ and
    I'll highlight some if they are better than what I did hereã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç±»ä¿¡æ¯ç¡®å®å¾ˆå¤šï¼Œç†è§£èµ·æ¥å¾ˆæ­£å¸¸ã€‚è€Œä¸”ï¼Œæ­£å¦‚æˆ‘ä¹‹å‰æ‰€è¯´ï¼Œå¦‚æœä½ æœ‰å…¶ä»–è§£å†³æˆ‘ä»¬åœ¨è¿™é‡Œå›ç­”çš„é—®é¢˜çš„æ–¹æ³•ï¼Œé‚£ä¹ˆè¯·åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†ç•™è¨€åˆ†äº«ä½ çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä¼šçœ‹ä¸€ä¸‹è¿™äº›æ–¹æ¡ˆï¼Œå¦‚æœå®ƒä»¬æ¯”æˆ‘åœ¨è¿™é‡Œçš„åšæ³•æ›´å¥½ï¼Œæˆ‘ä¼šç‰¹åˆ«æåŠã€‚
- en: Okay so before we end here I would like to mention the sponsor of this videoã€‚
    And that is brilliantã€‚ So in this seriesï¼Œ we've been learning about pandas and
    how to analyze data and Python and brilliant would be an excellent way to supplement
    what you learn here with their handson courses they have some excellent courses
    and lessons that do a deep dive on how to think about and analyze data correctlyã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œåœ¨æˆ‘ä»¬ç»“æŸä¹‹å‰ï¼Œæˆ‘æƒ³æä¸€ä¸‹æœ¬è§†é¢‘çš„èµåŠ©å•†ã€‚é‚£å°±æ˜¯**Brilliant**ã€‚åœ¨è¿™ä¸ªç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å…³äºpandasä»¥åŠå¦‚ä½•ä½¿ç”¨Pythonåˆ†ææ•°æ®ï¼Œ**Brilliant**å°†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¡¥å……ï¼Œæä¾›åŠ¨æ‰‹å®è·µçš„è¯¾ç¨‹ï¼Œä»–ä»¬æœ‰ä¸€äº›ä¼˜ç§€çš„è¯¾ç¨‹å’Œè¯¾ç¨‹å†…å®¹ï¼Œæ·±å…¥æ¢è®¨å¦‚ä½•æ­£ç¡®æ€è€ƒå’Œåˆ†ææ•°æ®ã€‚
- en: for data analysis fundamentalsï¼Œ I would really recommend checking out their
    statistics courseã€‚ which shows you how to analyze graphs and determine significance
    in the dataã€‚ and I would also recommend their machine learning courseï¼Œ which takes
    data analysisã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®åˆ†æåŸºç¡€ï¼Œæˆ‘å¼ºçƒˆæ¨èæŸ¥çœ‹ä»–ä»¬çš„ç»Ÿè®¡è¯¾ç¨‹ï¼Œå®ƒæ•™ä½ å¦‚ä½•åˆ†æå›¾è¡¨å¹¶ç¡®å®šæ•°æ®çš„æ˜¾è‘—æ€§ã€‚æˆ‘è¿˜æ¨èä»–ä»¬çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ï¼Œå®ƒæ¶‰åŠæ•°æ®åˆ†æã€‚
- en: '![](img/f68605379f547ff8d305f85d92ec3497_1.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68605379f547ff8d305f85d92ec3497_1.png)'
- en: '![](img/f68605379f547ff8d305f85d92ec3497_2.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68605379f547ff8d305f85d92ec3497_2.png)'
- en: '![](img/f68605379f547ff8d305f85d92ec3497_3.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68605379f547ff8d305f85d92ec3497_3.png)'
- en: '![](img/f68605379f547ff8d305f85d92ec3497_4.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68605379f547ff8d305f85d92ec3497_4.png)'
- en: '![](img/f68605379f547ff8d305f85d92ec3497_5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f68605379f547ff8d305f85d92ec3497_5.png)'
- en: to a new level where you'll learn about the techniques being used that allow
    machines to make decisions where there's just too many variables for a human to
    considerã€‚ So to support my channel and learn more about brilliantã€‚ you can go
    to brilliantg cs to sign up for free and also the first 200 people that go to
    that link will get 20% off the annual premium subscriptionã€‚ and you can find that
    link in the description section below againã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡åˆ°ä¸€ä¸ªæ–°å±‚æ¬¡ï¼Œä½ å°†å­¦ä¹ åˆ°å¦‚ä½•è®©æœºå™¨åœ¨å˜é‡è¿‡å¤šçš„æƒ…å†µä¸‹åšå‡ºå†³ç­–çš„æŠ€æœ¯ã€‚ä¸ºäº†æ”¯æŒæˆ‘çš„é¢‘é“å¹¶äº†è§£æ›´å¤šå…³äº**Brilliant**çš„ä¿¡æ¯ï¼Œä½ å¯ä»¥å»brilliantg
    csæ³¨å†Œå…è´¹è´¦æˆ·ï¼Œå‰200ä½è®¿é—®è¯¥é“¾æ¥çš„äººå°†è·å¾—å¹´åº¦é«˜çº§è®¢é˜…20%çš„æŠ˜æ‰£ã€‚ä½ å¯ä»¥åœ¨ä¸‹é¢çš„æè¿°éƒ¨åˆ†æ‰¾åˆ°è¿™ä¸ªé“¾æ¥ã€‚
- en: that's brilliantg so I think that's going to do it for this pandas video I hope
    you feel like you got a good idea for how to use these aggregate functions and
    also how we can group our data so that we can explore our data in interesting
    waysã€‚ I would really encourage you to take some time after this video and play
    around with the data a bit see if you can answer certain questions that someone
    might have about this dataã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è®¤ä¸ºè¿™å°±æ˜¯æˆ‘ä»¬æœ¬æ¬¡å…³äºpandasè§†é¢‘çš„å…¨éƒ¨å†…å®¹ï¼Œå¸Œæœ›ä½ å¯¹å¦‚ä½•ä½¿ç”¨è¿™äº›èšåˆå‡½æ•°ä»¥åŠå¦‚ä½•å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ä»¥æ¢ç´¢æ•°æ®æœ‰äº†è‰¯å¥½çš„ç†è§£ã€‚æˆ‘çœŸçš„é¼“åŠ±ä½ åœ¨è§‚çœ‹å®Œè¿™ä¸ªè§†é¢‘åèŠ±ç‚¹æ—¶é—´ç©ç©æ•°æ®ï¼Œçœ‹çœ‹èƒ½å¦å›ç­”æŸäº›äººå¯èƒ½å¯¹è¿™äº›æ•°æ®æå‡ºçš„é—®é¢˜ã€‚
- en: So for exampleï¼Œ what is the most common education level for people who answered
    this survey that's definitely something that we could answer by what we learned
    hereã€‚ğŸ˜Šï¼ŒI hope you feel like you got a good introduction to being able to answer
    those types of questionsã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºå›ç­”è¿™é¡¹è°ƒæŸ¥çš„äººï¼Œæœ€å¸¸è§çš„æ•™è‚²æ°´å¹³æ˜¯ä»€ä¹ˆï¼Œè¿™ç»å¯¹æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™é‡Œå­¦åˆ°çš„çŸ¥è¯†æ¥å›ç­”çš„é—®é¢˜ã€‚ğŸ˜Š æˆ‘å¸Œæœ›ä½ æ„Ÿè§‰è‡ªå·±å¯¹å›ç­”è¿™ç±»é—®é¢˜æœ‰äº†å¾ˆå¥½çš„ä»‹ç»ã€‚
- en: Nowï¼Œ in the next videoï¼Œ we're gonna be learning about how to handle missing
    data and how to clean up your dataã€‚ It's very common for data to have missing
    valuesã€‚ So knowing how to sanitize and clean our data is definitely going to be
    importantã€‚ But if anyone has any questions about what we covered in this videoã€‚
    then feel free to ask in the comment section belowï¼Œ and I'll do my best to answer
    thoseã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®ä»¥åŠå¦‚ä½•æ¸…ç†æ•°æ®ã€‚æ•°æ®ä¸­ç¼ºå¤±å€¼æ˜¯éå¸¸å¸¸è§çš„ï¼Œå› æ­¤äº†è§£å¦‚ä½•æ¸…ç†å’Œæ•´ç†æˆ‘ä»¬çš„æ•°æ®ç»å¯¹å¾ˆé‡è¦ã€‚å¦‚æœæœ‰äººå¯¹æˆ‘ä»¬åœ¨è¿™ä¸ªè§†é¢‘ä¸­è®¨è®ºçš„å†…å®¹æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶åœ¨ä¸‹é¢çš„è¯„è®ºåŒºæé—®ï¼Œæˆ‘ä¼šå°½åŠ›å›ç­”ã€‚
- en: And if you enjoy these tutorials and would like to support themã€‚ Then there
    are several ways you can do thatã€‚ The easiest ways to simply like the video and
    give it a thumbs upã€‚ And alsoï¼Œ it's a huge help to share these videos with anyone
    who you think would find them usefulã€‚ And if you have the means you can contribute
    through Patreonã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™äº›æ•™ç¨‹å¹¶æƒ³æ”¯æŒå®ƒä»¬ï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥åšåˆ°ã€‚æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯ç‚¹èµè§†é¢‘å¹¶ç»™äºˆå¥½è¯„ã€‚æ­¤å¤–ï¼Œä¸ä»»ä½•ä½ è®¤ä¸ºä¼šè§‰å¾—æœ‰ç”¨çš„äººåˆ†äº«è¿™äº›è§†é¢‘ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„å¸®åŠ©ã€‚å¦‚æœä½ æœ‰èƒ½åŠ›çš„è¯ï¼Œå¯ä»¥é€šè¿‡Patreonè¿›è¡Œæ”¯æŒã€‚
- en: And there's a link to that page in the description section belowã€‚ be sure to
    subscribe to your future videos and thank you all for watchingã€‚ğŸ˜Šã€‚![](img/f68605379f547ff8d305f85d92ec3497_7.png)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„æè¿°éƒ¨åˆ†æœ‰ä¸€ä¸ªé“¾æ¥æŒ‡å‘è¯¥é¡µé¢ã€‚è¯·ç¡®ä¿è®¢é˜…ä½ æœªæ¥çš„è§†é¢‘ï¼Œæ„Ÿè°¢å¤§å®¶çš„è§‚çœ‹ã€‚ğŸ˜Šï¼[](img/f68605379f547ff8d305f85d92ec3497_7.png)
