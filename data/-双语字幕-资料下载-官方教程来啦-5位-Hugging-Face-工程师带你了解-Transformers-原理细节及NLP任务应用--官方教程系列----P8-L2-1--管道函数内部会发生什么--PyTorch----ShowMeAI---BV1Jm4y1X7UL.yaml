- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P8ï¼šL2.1- ç®¡é“å‡½æ•°å†…éƒ¨ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ(PyTorch) - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥äº†ï¼5 ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠ NLP ä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P8ï¼šL2.1- ç®¡é“å‡½æ•°å†…éƒ¨ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ(PyTorch) - ShowMeAI - BV1Jm4y1X7UL
- en: What happens inside the by functionï¼ŸIn this videoï¼Œ we'll look at what actually
    happens when we use the pipeline function of the Transforms libraryã€‚Now specificallyï¼Œ
    well look at the sentiment analysis pipeline and then it went from the two following
    sentences so the positive and negative labels were respective scoresã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿåœ¨è¿™ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å½“ä½¿ç”¨ Transforms åº“çš„ç®¡é“å‡½æ•°æ—¶å®é™…å‘ç”Ÿäº†ä»€ä¹ˆã€‚ç°åœ¨ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹æƒ…æ„Ÿåˆ†æç®¡é“ï¼Œç„¶åå®ƒä»ä»¥ä¸‹ä¸¤ä¸ªå¥å­ä¸­å¾—å‡ºæ­£è´Ÿæ ‡ç­¾çš„ç›¸åº”åˆ†æ•°ã€‚
- en: As we've seen in the pipeline plan presentationã€‚There are three stages in the
    pipelineã€‚Firstã€‚ we convert a verex to numbers the model can make signs of using
    a tokenizerã€‚Then those numbers go through the model which outputs lowsã€‚Finallyã€‚
    the first processing steps transform Vo gate into labels and scoresã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ç®¡é“è®¡åˆ’æ¼”ç¤ºä¸­çœ‹åˆ°çš„ï¼Œç®¡é“ä¸­æœ‰ä¸‰ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªé¡¶ç‚¹è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ä½¿ç”¨çš„æ•°å­—ï¼Œé€šè¿‡ tokenizerã€‚ç„¶åè¿™äº›æ•°å­—é€šè¿‡æ¨¡å‹è¾“å‡ºã€‚æœ€åï¼Œç¬¬ä¸€å¤„ç†æ­¥éª¤å°†
    Vo gate è½¬æ¢ä¸ºæ ‡ç­¾å’Œåˆ†æ•°ã€‚
- en: Let's look in details at those three steps and how to replicate their music
    the Transformerss libraryã€‚ beginning with the first stage tokenizationã€‚So organization
    process has several steps firstã€‚ the text is split into small chunks called tokensã€‚They
    can be wordsï¼Œ part of wordsã€‚ or punctuation symbolsã€‚Then the tokenizer will add
    some special tokens if the model expectedã€‚Hereã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹è¿™ä¸‰æ­¥ä»¥åŠå¦‚ä½•å¤åˆ¶å®ƒä»¬åœ¨ Transformers åº“ä¸­çš„éŸ³ä¹ã€‚é¦–å…ˆä»ç¬¬ä¸€ä¸ªé˜¶æ®µçš„æ ‡è®°åŒ–å¼€å§‹ã€‚æ‰€ä»¥ç»„ç»‡è¿‡ç¨‹æœ‰å‡ ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç§°ä¸ºæ ‡è®°çš„å°å—ã€‚å®ƒä»¬å¯ä»¥æ˜¯å•è¯ã€å•è¯çš„ä¸€éƒ¨åˆ†æˆ–æ ‡ç‚¹ç¬¦å·ã€‚ç„¶åï¼Œtokenizer
    å¦‚æœæ¨¡å‹é¢„æœŸä¼šæ·»åŠ ä¸€äº›ç‰¹æ®Šæ ‡è®°ã€‚
- en: the model used expect a seal token at the beginning and a sep token at the end
    of the sentence to classifyã€‚Lastlyï¼Œ the tokenazer patches each token to its unique
    ID in the vocabulary of the portrayed modelã€‚To load such a tokenizerï¼Œ the transformformers
    library provides the Utokenizer APIã€‚The most important method of this class is
    from Pretrainedã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æœŸå¾…åœ¨å¥å­çš„å¼€å¤´æœ‰ä¸€ä¸ª seal tokenï¼Œåœ¨å¥å­çš„ç»“å°¾æœ‰ä¸€ä¸ª sep token è¿›è¡Œåˆ†ç±»ã€‚æœ€åï¼Œtokenazer å°†æ¯ä¸ª token åŒ¹é…åˆ°æ‰€æç»˜æ¨¡å‹è¯æ±‡ä¸­çš„å”¯ä¸€
    IDã€‚è¦åŠ è½½è¿™æ ·çš„ tokenizerï¼ŒTransformers åº“æä¾›äº† Utokenizer APIã€‚è¿™ä¸ªç±»æœ€é‡è¦çš„æ–¹æ³•æ˜¯ from Pretrainedã€‚
- en: which will download and cache the configuration and the vocabulary associated
    to a given checkpointã€‚Hereï¼Œ the checkpoint used by default for the sentiment analysis
    pipeline is distber based case fine tuned SS2 Englishã€‚ which is a bit of a mouthfulã€‚We
    instant to tookken associated with a checkpointã€‚ and feed it to the two sentencesã€‚Since
    the two sentences are not of the same sizeã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ä¸‹è½½å¹¶ç¼“å­˜ä¸ç»™å®šæ£€æŸ¥ç‚¹ç›¸å…³çš„é…ç½®å’Œè¯æ±‡ã€‚è¿™é‡Œï¼Œæƒ…æ„Ÿåˆ†æç®¡é“é»˜è®¤ä½¿ç”¨çš„æ£€æŸ¥ç‚¹æ˜¯åŸºäº distber çš„å¾®è°ƒ SS2 è‹±æ–‡ã€‚è¿™æœ‰ç‚¹å†—é•¿ã€‚æˆ‘ä»¬å®ä¾‹åŒ–ä¸æ£€æŸ¥ç‚¹ç›¸å…³çš„
    tokenï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ä¸¤ä¸ªå¥å­ä¸­ã€‚ç”±äºè¿™ä¸¤ä¸ªå¥å­çš„å¤§å°ä¸åŒã€‚
- en: well need to pad the shest one to be able to build an arrayã€‚This is done by
    the tokenizer with the option padding equal trueã€‚With truation equal2ã€‚ we ensure
    that any sentence longer and the maximum the middle can handle is truncatedã€‚Lastlyã€‚
    the return tensil option tells the tokenizer to return the byytch tensilã€‚Looking
    at a resultã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦å¡«å……æœ€åä¸€ä¸ªä»¥ä¾¿èƒ½å¤Ÿæ„å»ºä¸€ä¸ªæ•°ç»„ã€‚è¿™æ˜¯é€šè¿‡ tokenizer å®Œæˆçš„ï¼Œé€‰é¡¹å¡«å……ä¸º trueã€‚é€šè¿‡ truation ç­‰äº 2ï¼Œæˆ‘ä»¬ç¡®ä¿ä»»ä½•æ¯”æœ€å¤§ä¸­é—´é•¿åº¦é•¿çš„å¥å­éƒ½ä¼šè¢«æˆªæ–­ã€‚æœ€åï¼Œè¿”å›
    tensil é€‰é¡¹å‘Šè¯‰ tokenizer è¿”å›æ‰¹å¤„ç† tensilã€‚çœ‹ç»“æœã€‚
- en: we see we have a dictionary with two keysï¼Œ input ID contains the ideas of both
    sentences with zero where the padding is appliedã€‚The second key attention mask
    indicates where patting has been appliedã€‚ so the model does not pay attention
    to itã€‚This is all what is inside the token stepã€‚Now let's have a look at the second
    stepã€‚ä¸‰ã‚‚ã©ã€‚As also to anã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸ï¼Œè¾“å…¥ ID åŒ…å«ä¸¤ä¸ªå¥å­çš„æƒ³æ³•ï¼Œå¡«å……åº”ç”¨çš„ä½ç½®ä¸ºé›¶ã€‚ç¬¬äºŒä¸ªé”®æ³¨æ„æ©ç æŒ‡ç¤ºäº†å¡«å……çš„ä½ç½®ï¼Œä»¥ä¾¿æ¨¡å‹ä¸å…³æ³¨å®ƒã€‚è¿™å°±æ˜¯æ ‡è®°æ­¥éª¤ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ç¬¬äºŒæ­¥ã€‚ä¸‰ã‚‚ã©ã€‚ä¹Ÿå¦‚åŒä¸€ä¸ªã€‚
- en: there is a notomod API with from prettrain methodã€‚ it would download and cache
    the configuration of the modelï¼Œ as well as the pretrain weightã€‚Howeverã€‚ the Automod
    API will only instantiate the body of the modelã€‚ that is the part of the model
    that is left once the pro traininging head is removedã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ª notomod APIï¼Œä½¿ç”¨ pretrain æ–¹æ³•ã€‚å®ƒå°†ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹çš„é…ç½®ä»¥åŠé¢„è®­ç»ƒæƒé‡ã€‚ç„¶è€Œï¼ŒAutomod API åªä¼šå®ä¾‹åŒ–æ¨¡å‹çš„ä¸»ä½“ï¼Œä¹Ÿå°±æ˜¯åœ¨å»æ‰é¢„è®­ç»ƒå¤´ä¹‹åå‰©ä¸‹çš„éƒ¨åˆ†ã€‚
- en: It will output a high dimensional tensorï¼Œ that is a representation of the sentence's
    pastã€‚ but which is not directly useful for our classification programã€‚Hereï¼Œ the
    tensor has two sentencesã€‚ each of 16 tokensï¼Œ and the last dimension is the Indian
    size of our modelï¼Œ 768ã€‚To get an output linked to our classification problemã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå°†è¾“å‡ºä¸€ä¸ªé«˜ç»´å¼ é‡ï¼Œå³å¥å­è¿‡å»çš„è¡¨ç¤ºï¼Œä½†å¯¹äºæˆ‘ä»¬çš„åˆ†ç±»ç¨‹åºæ¥è¯´å¹¶ä¸ç›´æ¥æœ‰ç”¨ã€‚åœ¨è¿™é‡Œï¼Œå¼ é‡åŒ…å«ä¸¤ä¸ªå¥å­ï¼Œæ¯ä¸ªå¥å­æœ‰16ä¸ªæ ‡è®°ï¼Œæœ€åä¸€ä¸ªç»´åº¦æ˜¯æˆ‘ä»¬æ¨¡å‹çš„å°åº¦å¤§å°ï¼Œ768ã€‚ä¸ºäº†å¾—åˆ°ä¸æˆ‘ä»¬çš„åˆ†ç±»é—®é¢˜ç›¸å…³çš„è¾“å‡ºã€‚
- en: we need to use the Automodal for sequence classificationification classã€‚It works
    exactly as you to model classï¼Œ except that12 build a model with a classification
    headã€‚ğŸ˜Šã€‚Praise one auto class for each common NLP task in the transformformers
    libraryã€‚Hereã€‚ after giving all models of two sentencesã€‚We get a tensor of size
    2 by2ã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä½¿ç”¨Automodalè¿›è¡Œåºåˆ—åˆ†ç±»ç±»ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ä¸æ¨¡å‹ç±»å®Œå…¨ç›¸åŒï¼Œé™¤äº†å®ƒæ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹ã€‚ğŸ˜Šåœ¨transformersåº“ä¸­ï¼Œæ¯ä¸ªå¸¸è§çš„NLPä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªè‡ªåŠ¨ç±»ã€‚åœ¨è¿™é‡Œï¼Œåœ¨ç»™å‡ºä¸¤ä¸ªå¥å­çš„æ‰€æœ‰æ¨¡å‹åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸º2ä¹˜2çš„å¼ é‡ã€‚
- en: one result for each sentence and for each possible levelã€‚Those outputs are not
    probabilities yetã€‚ we can see they don't sum to oneã€‚This is because each model
    of the transformformers's library returns look itã€‚To make sense of look itï¼Œ we
    need to dig into the third and last step of the pipelineã€‚Plus processingã€‚To conduct
    Lo into probabilitiesï¼Œ we need to apply a softmax layers to themã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå¥å­å’Œæ¯ä¸ªå¯èƒ½çº§åˆ«çš„ä¸€ä¸ªç»“æœã€‚è¿™äº›è¾“å‡ºå°šæœªæ˜¯æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬çš„æ€»å’Œä¸ä¸º1ã€‚è¿™æ˜¯å› ä¸ºæ¯ä¸ªtransformersåº“çš„æ¨¡å‹è¿”å›çš„æ˜¯è¿™æ ·çš„ç»“æœã€‚è¦ç†è§£è¿™äº›ç»“æœï¼Œæˆ‘ä»¬éœ€è¦æ·±å…¥æ¢è®¨ç®¡é“çš„ç¬¬ä¸‰ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªæ­¥éª¤ï¼Œå¤„ç†ã€‚ä¸ºäº†å°†å…¶è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬åº”ç”¨softmaxå±‚ã€‚
- en: As we can seeï¼Œ this transforms them into positive number that's a up to1ã€‚The
    last step is to know which of those corresponds to the positive of the negative
    labelã€‚This is given by the I2lipol field of the model congã€‚The first probabilityba
    is index0ã€‚ correspond to the negative level and the seconds index1 correspond
    to the positive levelã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè¿™å°†å®ƒä»¬è½¬æ¢ä¸ºæ­£æ•°ï¼Œæœ€å¤šä¸º1ã€‚æœ€åä¸€æ­¥æ˜¯çŸ¥é“å…¶ä¸­å“ªä¸ªå¯¹åº”äºæ­£æ ‡ç­¾æˆ–è´Ÿæ ‡ç­¾ã€‚è¿™æ˜¯é€šè¿‡æ¨¡å‹çš„I2lipolå­—æ®µç»™å‡ºçš„ã€‚ç¬¬ä¸€ä¸ªæ¦‚ç‡baçš„ç´¢å¼•0å¯¹åº”äºè´Ÿçº§åˆ«ï¼Œç¬¬äºŒä¸ªç´¢å¼•1å¯¹åº”äºæ­£çº§åˆ«ã€‚
- en: This is how our classifier built with the pipeline function pickeded with labels
    and compute those scoresã€‚ğŸ˜Šï¼ŒNow that you know how each step worksï¼Œ you can easily
    tweak them to your needsã€‚![](img/be1ca15e381cec8388260069320209cf_1.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬çš„åˆ†ç±»å™¨å¦‚ä½•é€šè¿‡ç®¡é“å‡½æ•°æ„å»ºï¼Œå¸¦æœ‰æ ‡ç­¾å¹¶è®¡ç®—è¿™äº›åˆ†æ•°ã€‚ğŸ˜Šç°åœ¨ä½ çŸ¥é“æ¯ä¸ªæ­¥éª¤æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä½ å¯ä»¥è½»æ¾è°ƒæ•´å®ƒä»¬ä»¥æ»¡è¶³ä½ çš„éœ€æ±‚ã€‚![](img/be1ca15e381cec8388260069320209cf_1.png)
- en: '![](img/be1ca15e381cec8388260069320209cf_2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be1ca15e381cec8388260069320209cf_2.png)'
- en: ã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ã€‚
