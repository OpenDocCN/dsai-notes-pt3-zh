- en: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P1：L1- PyTorch 简介
    - ShowMeAI - BV19L4y1t7tu
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】140分钟入门 PyTorch，官方教程手把手教你训练第一个深度学习模型！＜官方教程系列＞ - P1：L1- PyTorch 简介
    - ShowMeAI - BV19L4y1t7tu
- en: Hello， my name is Brad Heinz。 I'm a partner engineer working with the Piytorrch
    team at Facebook。In this video， I'll be giving you an introduction to Pytorrch，
    its features。 key concepts and associated tools and libraries。 This overview assumes
    that you are new to doing machine learning with Pytorrch。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你好，我的名字是Brad Heinz。我是Facebook Pytorrch团队的合作工程师。在这个视频中，我将向你介绍Pytorrch，它的特性、关键概念和相关工具及库。这个概述假设你是第一次接触Pytorrch进行机器学习。
- en: '![](img/1181a3784e7b926ea0c178203ada7ae1_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1181a3784e7b926ea0c178203ada7ae1_1.png)'
- en: In this video， we're going to cover an overview of Pytorr and related projects。Tenssors。
    which are the core data abstraction of Ptorrch。Autograd。 which tries the eager
    mode computation that makes rapid iteration in your model possible。We'll talk
    about building a model with Ptorrch modules。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，我们将涵盖Pytorrch和相关项目的概述。张量，它们是Pytorch的核心数据抽象。自动微分，它尝试快速计算模式，使模型快速迭代成为可能。我们将讨论如何使用Pytorch模块构建模型。
- en: We'll talk about how to load your data efficiently to train your model。We'll
    demonstrate a basic training loop。And finally， we'll talk about deployment with
    Torchscript。![](img/1181a3784e7b926ea0c178203ada7ae1_3.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论如何高效加载数据以训练模型。我们将演示一个基本的训练循环。最后，我们将讨论使用Torchscript进行部署。![](img/1181a3784e7b926ea0c178203ada7ae1_3.png)
- en: Before we get started， you'll want to install Pytorrch and torch visions so
    you can follow along with the demos and exercises。 If you haven't installed the
    latest version of Pytorrch yet， visit Pytorrch dot org。The front page has an installed
    wizard shown here。There are two important things to note here。 First。 coupa drivers
    are not available for the Mac。 Therefore。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，你需要安装Pytorrch和torchvisions，以便能够跟随演示和练习。如果你还没有安装最新版本的Pytorrch，请访问Pytorrch.org。首页有一个安装向导。这里有两个重要事项需要注意。首先，Mac上不提供CUDA驱动程序。因此。
- en: GP PU acceleration is not going to be available by a pietorch on the Mac。Second。
    if you're working on a Linux or Windows machine with one or more NviIdia couda
    compatible GPUs attached。 make sure the version of couta tool kit you installed
    matches the couda drivers on your machine。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mac上不会提供GPU加速。其次，如果你在配备一个或多个Nvidia CUDA兼容GPU的Linux或Windows机器上工作，请确保你安装的CUDA工具包版本与机器上的CUDA驱动程序匹配。
- en: '![](img/1181a3784e7b926ea0c178203ada7ae1_5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1181a3784e7b926ea0c178203ada7ae1_5.png)'
- en: So what is Pytorch？
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么Pytorch是什么呢？
- en: '![](img/1181a3784e7b926ea0c178203ada7ae1_7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1181a3784e7b926ea0c178203ada7ae1_7.png)'
- en: Pytorch。org。Tells us that Pytorch is an open source machine learning framework
    that accelerates the path from research prototyping to production deployment。Let's
    unpack that。First。Pytorchches software for machine learning。 It contains a full
    toolkit for building and deploying M L applications。 including deep learning primitives，
    such as neural network layer types。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch.org告诉我们，Pytorch是一个开源机器学习框架，能够加速从研究原型到生产部署的路径。让我们详细了解一下。首先，Pytorch是机器学习的软件。它包含构建和部署机器学习应用程序的完整工具包，包括深度学习的基本元素，如神经网络层类型。
- en: activation functions and gradient based optimizers。 It has hardware acceleration
    on Nvidia GPus。 and it has associated libraries for computer vision。 text and
    natural language and audio applications。Torchssion。 the Pytorrch Library for Computer
    vision applications。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数和基于梯度的优化器。它在Nvidia GPU上具有硬件加速，并且有与计算机视觉、文本、自然语言和音频应用相关的库。Torchssion，Pytorch计算机视觉应用的库。
- en: also includes pre trained models and packaged data sets that you can use to
    train your own models。Pytorch is built to enable fast iteration on your M L models
    and applications。 You can work in regular idiomatic Python。 There is no new domain
    specific language to learn to build your computation graph with autograd。 Pytorch's
    automatic differentiation engine， the backward passover your model is done with
    a single function call and done correctly。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 还包括预训练模型和打包数据集，你可以用来训练自己的模型。Pytorch旨在快速迭代你的机器学习模型和应用程序。你可以在常规的Python中工作。构建计算图时无需学习新的特定领域语言，Pytorch的自动微分引擎，模型的反向传播通过单个函数调用正确完成。
- en: no matter which path through the code a computation took。 offering you unparalleled
    flexibility in model design。Pytorch has the tooling to work at enterprise scale
    with tools like Torchcr。 which is a way to create serializable and optimizable
    models from your Pytorrch code， Torchserv。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 无论计算在代码中走哪条路径，都提供了无与伦比的模型设计灵活性。Pytorch 拥有适用于企业规模的工具，如 Torchcr，它可以从 Pytorrch
    代码创建可序列化和可优化的模型，还有 Torchserv。
- en: Pytorrch's model serving solution and multiple options for quantizing your model
    for performance。And finally， Pytorrch is free in open source software。 free to
    use and open to contributions from the community。![](img/1181a3784e7b926ea0c178203ada7ae1_9.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorrch 的模型服务解决方案以及多种量化模型以提高性能的选项。最后，Pytorrch 是一个免费开源软件，免费使用，欢迎社区的贡献。![](img/1181a3784e7b926ea0c178203ada7ae1_9.png)
- en: Its open source nature fosters a rich ecosystem of community projects as well。
    supporting use cases from satochastic processes to graph based neural networks。![](img/1181a3784e7b926ea0c178203ada7ae1_11.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它的开源特性也促进了丰富的社区项目生态系统，支持从随机过程到基于图的神经网络的用例。![](img/1181a3784e7b926ea0c178203ada7ae1_11.png)
- en: The Pyetorrch community is large and growing， with over 1200 contributors to
    the project from around the world and over 50% cent year on year growth in research
    paper citations。![](img/1181a3784e7b926ea0c178203ada7ae1_13.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorrch 社区庞大且不断壮大，来自全球的贡献者超过 1200 名，研究论文引用年增长率超过 50%。![](img/1181a3784e7b926ea0c178203ada7ae1_13.png)
- en: Petorrch is in use at top tier companies like these and provides the foundations
    for projects like Alan N LP。 the Open source Research Library for deep learninging
    with Natural Language， Fast AI。 which simplifies training fast and accurate neural
    nets using best modern practices。Classy vision and end to end framework for image
    and video classification and cap。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Petorrch 在顶级公司中得到了应用，并为像 Alan N LP 这样的项目提供了基础。它是一个开源深度学习研究库，专注于自然语言处理，Fast AI
    简化了使用现代最佳实践进行快速和准确的神经网络训练。Classy Vision 是一个用于图像和视频分类的端到端框架。
- en: an open source extensible library that helps you understand and interpret your
    model's behavior。Now that you've been introduced to Pytorrch， let's look under
    the hood。 Tensors will be at the center of everything you do in P torch。 Your
    models， inputs。 outputs and learning weights are all in the form of tensors。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个开源可扩展库，帮助你理解和解释模型的行为。现在你已经了解了 Pytorrch，让我们深入探讨一下。张量将是你在 Pytorrch 中所做的一切的核心。你的模型、输入、输出和学习权重都以张量的形式存在。
- en: '![](img/1181a3784e7b926ea0c178203ada7ae1_15.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1181a3784e7b926ea0c178203ada7ae1_15.png)'
- en: Now， if Tensor is not a part of your normal mathematical vocabulary。 just know
    that in this context。 we're talking about a multidisional array， but with a lot
    of extra bells and whistles。Pietorrch tensors come bundled with over 300 mathematical
    and logical operations that can be performed on them。Though you access tensors
    through a Python API， the computation actually happens in compiled C++ code optimized
    for CPUU and GPU。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果张量不是你常用的数学词汇，只需知道在这个上下文中，我们谈论的是多维数组，但带有许多额外的特性。Pietorrch 张量附带超过 300 种可对其执行的数学和逻辑操作。尽管你是通过
    Python API 访问张量，但计算实际上是在为 CPU 和 GPU 优化的编译 C++ 代码中进行的。
- en: Let's look at some typical tensor manipulations in Pytorrch。![](img/1181a3784e7b926ea0c178203ada7ae1_17.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Pytorrch 中一些典型的张量操作。![](img/1181a3784e7b926ea0c178203ada7ae1_17.png)
- en: The first thing we'll need to do is import pi torch with the import torch call。Then
    we'll go ahead and create our first tensor here。 I'm going to take create a two
    dimensional tensor with five rows and three columns and fill it with zeros。 I'm
    going to query it for the data type of those zeros。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是通过 `import torch` 调用来导入 Pytorrch。然后我们将继续在这里创建我们的第一个张量。我将创建一个具有五行三列的二维张量，并用零填充它。我将查询这些零的数据类型。
- en: And here you can see I got my requested matrix of 15 zeros， and the data is
    32 B floating point。 By default， pi torch creates all tensors as 32 bit floating
    point。What if you wanted integers instead， you can always override the default。Here
    in the next cell。 I create a tensor full of ones。 I request that they be 16 B
    integers and note that when I print it without being asked。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到我得到了请求的 15 个零的矩阵，数据类型为 32 位浮点数。默认情况下，Pytorrch 创建的所有张量都是 32 位浮点数。如果你想要整数，你可以随时覆盖默认设置。在下一个单元中，我创建了一个充满
    1 的张量。我请求它们为 16 位整数，并注意到在我没有被问及时打印出来。
- en: Pytorrch tells me that these are 16 B integers because it's not the default
    that might not be what I expect。It's common to initialize learning weights randomly。
    often with a specific seed for the random number generators that you can reproduce
    your results on subsequent runs here。 we demonstrate。Seeding the pytorch random
    mirror generator with a specific number。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorrch告诉我这些是16 B整数，因为它不是默认的，可能与我预期的不符。通常会随机初始化学习权重，常常使用特定的随机数生成器种子，以便在后续运行中重现结果。我们将演示如何用特定数字对Pytorch随机镜像生成器进行加种。
- en: Generating a random tensor。Generating a second random tensor。 which we expect
    to be different from the first reseing the random number generator with the same
    input。And then finally， creating another random tensor， which we expect to match
    the first since it was the first thing created after seeding the R G。And sure
    enough， those are the results we get。First tensor and the third tensor do match。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个随机张量。生成第二个随机张量，预期它与第一个不同，通过使用相同输入重新设置随机数生成器。最后，创建另一个随机张量，预期它与第一个匹配，因为它是在对随机生成器加种后创建的第一个张量。果然，这些结果是我们得到的。第一个张量和第三个张量确实匹配。
- en: and the second one does not。Arithmetic with Pytorch tensors is intuitive。 Tensors
    of similar shapes may be added， multiplied， et cetera。 and operations between
    a scalar and a tensor will distribute over all the cells of the tensor。 So let's
    look at a couple of examples。First， I'm just going to create a tensor full of
    ones。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 而第二个则不行。Pytorch张量的算术操作是直观的。相似形状的张量可以相加、相乘等等，而标量与张量之间的操作将分布到张量的所有单元上。所以让我们看几个例子。首先，我将创建一个满是1的张量。
- en: Then I'm going to create another tensor full of ones。 but I'm going to multiply
    it by a scalar 2。 And what's going to happen is all of those ones are going to
    become twos。 The multiplication is distributed over every element of the tensor。Then
    I'll add the two tensors。 I can do this because they're of the same shape。The
    operation happens element wise between the two of them。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我要创建一个满是1的张量，但我将其乘以标量2。结果是所有的1都会变成2。这个乘法在张量的每个元素上分布。然后我会将这两个张量相加。我能这样做是因为它们形状相同。操作在它们之间按元素进行。
- en: And we get out now a tensor full of threes。 When I query that tensor for its
    shape。 it's the same shape as the two input tensors from the addition operation。Finally。
    I create two random tensors of different shapes and attempt to add them。 I get
    a runtime error because there is no clean way to do element wise arithmetic operations
    between two tensors of different shapes。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在得到了一个满是3的张量。当我查询该张量的形状时，它与加法操作中的两个输入张量形状相同。最后，我创建两个不同形状的随机张量并尝试将它们相加。因为没有干净的方式在两个不同形状的张量之间进行按元素的算术操作，所以我得到了一个运行时错误。
- en: Here is a small sample of the mathematical operations available on Pytorch tensors。
    I'm going to create a random tensor and adjust it so its values are between -1
    and 1。I can take the absolute value of it and see all the values turn positive。I
    can take the inverse sign of it because values be are between minus1 and 1 and
    get an angle back。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Pytorch张量上可用的数学操作的小样本。我将创建一个随机张量并调整它，使其值在-1和1之间。我可以取它的绝对值，看到所有值变成正数。我可以取它的反函数，因为值在-1和1之间，得到一个角度。
- en: I can do linear algebra operations like taking the determinant or doing singular
    value decomposition。And there are statistical and aggregate operations as well。Means
    and standard deviations and minimums and maximumims， et cetera。There's a good
    deal more to know about the power of Pytorch tensors。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以执行线性代数操作，比如求行列式或进行奇异值分解。此外，还有统计和汇总操作，比如均值、标准差、最小值和最大值等等。关于Pytorch张量的强大功能还有很多要了解的。
- en: including how to set them up for parallel computation on GPU。 We'll be going
    into more depth in another video。![](img/1181a3784e7b926ea0c178203ada7ae1_19.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 包括如何为GPU的并行计算设置它们。我们将在另一个视频中深入讨论。![](img/1181a3784e7b926ea0c178203ada7ae1_19.png)
- en: '![](img/1181a3784e7b926ea0c178203ada7ae1_20.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1181a3784e7b926ea0c178203ada7ae1_20.png)'
- en: As an introduction to autorad， Pytorrchs automated differentiation engine。 Let's
    consider the basic mechanics of a single training pass。For this example。 we'll
    use a simple recurrent neural network or RN。We start with four tensors， X， the
    input H。 the hidden state of the R N N that gives it its memory and two sets of
    learning weights。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对自动求导的介绍，Pytorcht的自动微分引擎。我们来考虑一次单一训练传递的基本机制。在这个例子中，我们将使用一个简单的递归神经网络（RNN）。我们开始时有四个张量，X，输入H，RNN的隐藏状态，这赋予了它记忆，以及两组学习权重。
- en: 1 each for the input and the hidden state。Next， we multiply the weights by their
    respective tensors。In M here stands for Nature's multiplication。After that。 we
    add the outputs of the two matrix multiplications。And pass the result through
    an activation function here， Hybolic tangent。And finally。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入和隐藏状态都有一个。接下来，我们将权重乘以各自的张量。在M中，代表自然乘法。之后，我们将两个矩阵乘法的输出相加。并将结果通过激活函数，这里是双曲正切。最后。
- en: we compute the loss for this output。 The loss is the difference between the
    correct output and the actual prediction of our model。So we've taken a training
    input， run it through a model， gotten an output and determined the loss。This is
    the point in the training loop， where we have to compute the derivatives of that
    loss with respect to every parameter of the model and use the gradients over learning
    weights to decide how to adjust those weights in a way that reduces the loss。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算这个输出的损失。损失是正确输出与我们模型的实际预测之间的差异。因此，我们已经取了一个训练输入，通过模型运行，得到了输出并确定了损失。这是训练循环中的一个环节，我们必须计算该损失对模型每个参数的导数，并利用梯度调整学习权重，以减少损失的方式来调整这些权重。
- en: Even for a small model like this， that's a bunch of parameters and a lot of
    derivatives to compute。But here's the good news。 You can do it in one line of
    code。Each tensor generated by this computation knows how it came to be。 For example。
    I to H carries metadata indicating that it came from the matrix multiplication
    of W X and X。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是这样一个小模型，也有一堆参数和大量的导数需要计算。但好消息是，你可以用一行代码完成。每个由此计算生成的张量都知道它是如何产生的。例如，I到H携带元数据，表明它来自W
    X和X的矩阵乘法。
- en: And so it continues down the rest of the graph。This history tracking enables
    the backward method to rapidly calculate the gradients your model needs for learning。This
    history tracking is one of the things that enables flexibility and rapid iteration
    in your models。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它会继续向下延续到图的其余部分。这种历史跟踪使得反向方法能够快速计算模型在学习中所需的梯度。这种历史跟踪是使模型灵活和快速迭代的因素之一。
- en: Even in a complex model with decision branches and loops。 the computation history
    will track the particular path through the model that a particular input took
    and compute the backward derivatives correctly。In a later video， we'll show you
    how to do more tricks with autograd。 like using the autograd profiler and taking
    second derivatives and how to turn off autograd when you don't need it。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在具有决策分支和循环的复杂模型中，计算历史也会跟踪特定输入在模型中所走的路径，并正确计算反向导数。在后续视频中，我们将向你展示如何使用autograd进行更多技巧，比如使用autograd分析器、计算二阶导数，以及如何在不需要时关闭autograd。
- en: We've talked so far about tensors and automatic differentiation and some of
    the ways they interact with your Pytorrch model。 But what does that model look
    like in code。Let's build and run a simple one to get a feel for it。First， we're
    going to import pie torch。We're also going to import torchsha Nn。 which contains
    the neural network layers that we're going to compose into our model。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们谈到了张量、自动微分以及它们如何与Pytorcht模型相互作用。但这个模型在代码中是什么样子的呢？让我们构建并运行一个简单的模型以获取感觉。首先，我们将导入PyTorch。我们还将导入torch.nn，它包含我们将组合成模型的神经网络层。
- en: as well as the parent class of the model itself。 And we're going to import torchta
    Nn do functional to give us activation functions and max pullinging functions
    that we'll use to connect the layers。So here we have a diagram of Linenette 5。
    It's one of the earliest convolutional neural networks and one of the drivers
    of the explosion and deep learning。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 还有模型本身的父类。我们将导入torch.nn.functional，以便提供激活函数和最大池化函数，这些函数将用于连接层。因此，这里我们有一个Linenette
    5的示意图。它是最早的卷积神经网络之一，也是深度学习爆炸的推动者之一。
- en: It was built to read small images of handwritten numbers。 The eddenist data
    set and correctly classify which digit was represented in the image。Here's the
    abridged version of how it works。 Layer C 1 is a convolutional layer。 meaning
    that it scans the input image for features it learn during training。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它是为了读取手写数字的小图像而构建的。该数据集旨在正确分类图像中所表示的数字。下面是其工作原理的简要版本。层C1是卷积层，这意味着它扫描输入图像以识别在训练期间学习到的特征。
- en: It outputs a map of where it saw each each of its learned features in this image。
    This activation map is down sampled in layer S 2。Layer C 3 is another convolutional
    layer。 This time scanning C1's activation map for combinations of features。 It
    also puts out an activation map describing the spatial locations of these feature
    combinations。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 它输出了在该图像中看到的每个学习到的特征的位置图。这个激活图在层S2中进行了下采样。层C3是另一个卷积层，这次扫描C1的激活图以寻找特征组合。它还输出一个激活图，描述这些特征组合的空间位置。
- en: which is down sampled in layer S 4。Finally， the fully connected layers of the
    end at 5 F 6 and output are a classifier that takes the final activation map and
    classifies it into one of 10 bins representing the 10 digits。So how do we express
    this simple neural network in code？Looking over this code。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出在层S4中进行了下采样。最后，位于5F6的全连接层和输出是一个分类器，它接受最终的激活图并将其分类为表示10个数字的10个箱子之一。那么我们如何在代码中表达这个简单的神经网络呢？看看这段代码。
- en: you should be able to spot some structural similarities with a diagram above。This
    demonstrates the structure of a typical pi torch model。 It inherits from torrssha
    and N dot module。And modules may be nested。 In fact。 even the co2 D and linear
    layers here are subclasss of Torta and end dot module。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到与上面的图表有一些结构上的相似之处。这展示了一个典型的PyTorch模型的结构。它继承自Torch和N.dot模块，模块可以嵌套。实际上，这里的2D卷积层和线性层也是Torta和N.dot模块的子类。
- en: Every model will have an in it where it constructs the layers that it will compose
    into its computation graph。And loads any data artifacts it might need。 For example，
    an NLP model might load a vocabulary。A model will have a forward function。 This
    is where the actual computation happens。 and input is passed through the network
    layers in various functions to generate an output。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都会有一个初始化方法，在其中构建将组成其计算图的层，并加载可能需要的任何数据工件。例如，一个NLP模型可能会加载一个词汇表。模型将具有一个前向函数。这是实际计算发生的地方，输入通过网络层在各种函数中传递以生成输出。
- en: a prediction。Other than that， you can build out your model class like any other
    Python class。 adding whatever properties and methods you need to support your
    model's computation。So let's instanttantiate this。And run an input through it。So
    there are a few important things happening here。 We're creating an instance of
    limit。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个预测。除此之外，你可以像构建其他Python类一样构建你的模型类，添加任何你需要支持模型计算的属性和方法。所以让我们实例化这个，并通过它运行一个输入。这里发生了几个重要的事情。我们正在创建一个限制的实例。
- en: We are printing the object。 now a sub of Tortra and in a module。Will report
    the layers it has created and their shapes and parameters。 This can provide a
    handy overview of a model if you want to get the gist of its processing。Below
    that， we created dummy input representing a 32 by 32 image with one color channel。Normally。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在打印对象，现在是Tortra的一个子类并在一个模块中。将报告它创建的层及其形状和参数。这可以为你提供一个模型的方便概述，以便你了解其处理的要点。下面，我们创建了一个表示32×32图像和一个颜色通道的虚拟输入。通常。
- en: you would load an image tile and convert it to a tensor of this shape。You may
    have noticed an extra dimension to our tensor。 This is the batch dimension。Pytorrch
    models assume they are working on batches of data。 For example。 a batch of 16
    of our image tiles would have the shape 16 by 1 by 32 by 32。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要加载一个图像切片并将其转换为这种形状的张量。你可能注意到我们的张量有一个额外的维度。这是批量维度。PyTorch模型假设它们在处理数据批量。例如，一批16个图像切片将具有形状16×1×32×32。
- en: Since we're only using one image， we create a batch of one with shape 1 by one
    by 32 by 32。We ask the model for an inference by calling it like a function， net
    input。The output of this call represents the model's confidence that the input
    represents a particular digit。Since this instance of the model hasn't been trained。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只使用一张图像，我们创建了一个形状为1×1×32×32的批量。我们通过调用它作为一个函数来请求模型进行推断，net input。这个调用的输出表示模型对输入表示特定数字的信心。因为该模型实例尚未经过训练。
- en: we shouldn't expect to see any signal in the output。Looking at the shape of
    the output。 we can see that it also has a batch dimension， the size of which should
    always match the input batch dimension。 Had we passed in an input batch of 16
    instances， output would have a shape of 16 by 10。You've seen how a model is built
    and how to give it a batch of input and examine the output。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该期望在输出中看到任何信号。查看输出的形状，我们可以看到它还有一个批次维度，大小应该始终与输入批次维度相匹配。如果我们传入一个16个实例的输入批次，输出的形状将是16×10。你已经了解了如何构建模型，以及如何提供一批输入并检查输出。
- en: The model didn't do much， though， because it hasn't been trained yet for that。
    We'll need to feed it a bunch of data。In order to train our model。 we're going
    to need a way to feed it data in bulk。 This is where the Ptorrch data set and
    data load classes come into play。Let's see them in action。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，模型并没有做太多，因为它尚未经过训练。我们需要提供大量数据，以便训练我们的模型。我们需要一种方法来批量提供数据。这就是Pytorch数据集和数据加载类发挥作用的地方。让我们看看它们的实际应用。
- en: So here I'm declaring Matpllib in line because we'll be rendering some images
    in the notebook book。 I'm importing pitorrch。 I'm also importing torch vision
    and torch vision transforms。 These are going to give us our data sets and some
    transforms that we need to apply to the images。To make them digestible by our
    pieytorrch model。So the first thing we need to do is transform our incoming images
    into a pi torch tensor。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里我在线声明Matplotlib，因为我们将在笔记本中渲染一些图像。我正在导入Pytorch，同时也导入torchvision和torchvision变换。这些将为我们提供所需的数据集和一些需要应用于图像的变换，以便让它们适合我们的Pytorch模型。因此，我们需要做的第一件事是将传入的图像转换为Pytorch张量。
- en: Here we specify two transformations for our input Transs to Tensor takes images
    loaded by the pillow library。And converts them into Py toch tensors。 transformers
    dot normalize。 adjust the values of the tensor so that their average is 0， and
    their standard deviation is 0。5。 Most activation functions had their strongest
    radiance around the0 point。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为输入指定了两个变换。Transs to Tensor将由Pillow库加载的图像转换为Pytorch张量。transformers.dot
    normalize会调整张量的值，使其平均值为0，标准差为0.5。大多数激活函数在0点附近的辐射最强。
- en: So centering our data there can speed learning。There are many more transforms
    available。 including cropping， centering， rotation， reflection。 and most of the
    other things you might do to an image。Next。 we're going to create an instance
    of the Car 10 data set。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，中心化我们的数据可以加速学习。还有许多其他可用的变换，包括裁剪、中心化、旋转、反射，以及你可能对图像进行的其他大多数操作。接下来，我们将创建一个Car
    10数据集的实例。
- en: This is a set of 32 by 32 color image tiles representing 10 classes of objects。6
    of animals and four vehicles。When you're on this all above。 it may take a minute
    or two for this the data set to finish downloading for you。 so be aware of that。So
    this is an example of creating a data set in Pytorrch。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一组32×32的彩色图像瓷砖，代表10类物体，包括6种动物和4种车辆。当你准备好以上所有内容时，可能需要一两分钟的时间来下载这个数据集，所以请注意这一点。这是使用Pytorch创建数据集的一个示例。
- en: downloadable data sets like Siffer 10 above are subclasses of Torch， Us data
    data set。Data set classes in pitorrch include the downloadable dataset sets in
    torchvision。 Torch text and torch audio， as well as utility data set classes such
    as Torchvision datasets do image folder。 which will read a folder of labeled images。
    You can also create your own subclasses of data set。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可下载的数据集，如上面的Siffer 10，是Torch和Us数据集的子类。Pytorch中的数据集类包括可在torchvision中下载的数据集，以及Torch文本和torch音频，还有一些实用的数据集类，例如Torchvision
    datasets中的图像文件夹，它将读取一个标记图像的文件夹。你也可以创建自己的数据集子类。
- en: When we instantiate our data set， we need to tell it a few things。The file system
    path where we want the data to go， whether or not we're using this set for training。
    because most data sets will be split between training and test subsets。Whether
    we would like to download the data set if we haven't already and the transformations
    that we want to apply to the images。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实例化我们的数据集时，需要告诉它一些信息，包括我们希望数据存放的文件系统路径，是否将此数据集用于训练，因为大多数数据集会在训练和测试子集之间进行拆分。是否希望下载数据集（如果我们还没有下载的话）以及我们希望对图像应用的变换。
- en: Once you have your data set ready， you can give it to the data loader。Now a
    data set subclass wraps access to the data and is specialized the type of the
    data is serving。The data loader knows nothing about the data， but organizes the
    input tensors served by the data set into batches with the parameters you specify
    In the example above。 we've asked a data loader to give us batches of four images
    from train set。Randomizing their order。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你准备好数据集，就可以将其交给数据加载器。现在，数据集子类封装了对数据的访问，并专门针对其所服务的数据类型。数据加载器对数据一无所知，但根据你在上面的示例中指定的参数，将数据集提供的输入张量组织成批次。我们请求数据加载器从训练集中给我们提供四张图像的批次，随机化它们的顺序。
- en: With shuffle equals true。 And we told to spin up two workers to load data from
    disk。 It's good practice to visualize the batches your data loader serves。Rning
    the cells should show you a strip of four images and you should see a correct
    label for each one。And so here are our four images which do in fact look like
    a cat， a deer in two trucks。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置为shuffle为真时，我们告诉它启动两个工作线程从磁盘加载数据。可视化数据加载器提供的批次是一种良好的实践。运行这些单元应该会显示出四张图像的条带，并且你应该能看到每张图像的正确标签。那么，这里是我们的四张图像，实际上看起来像是一只猫和一只在两辆卡车里的鹿。
- en: We've looked under the hood at tensors and autograd。 and we've seen how pi torch
    models are constructed and how to efficiently feed them datatum。It's time to put
    all the pieces together and see how a model gets trained。So here we are back in
    our notebook， you'll see the imports here。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入了解了张量和自动求导，并看到了PyTorch模型是如何构建的，以及如何高效地为它们提供数据。是时候把所有的部分结合起来，看看一个模型是如何训练的。所以，我们回到了笔记本，你会看到这里的导入。
- en: all of these should look familiar from earlier in the video except for Torchdot
    Opum。 which I'll be talking about soon。The first thing we'll need is training
    and test data sets。 So if you haven't already run the cell below and make sure
    the data set is downloaded。 you may take a minute if you haven't done so already。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Torchdot Opum，所有这些在视频早些时候应该看起来都很熟悉。我们首先需要的是训练和测试数据集。如果你还没有运行下面的单元，请确保数据集已下载。如果你还没这样做，可能需要花一分钟的时间。
- en: We'll run our check on the output from the data loader。And again。 we should
    see a strip of four images， a plain， plain， plain ship。 That looks correct。 Sorry。
    data letter is good。This is the model we'll train。 Now。 this model looks familiar
    It's because it's a variant of Lyette。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查数据加载器的输出。再次，我们应该看到四张图像的条带，这看起来是正确的。抱歉，数据加载器很好。这是我们将训练的模型。现在，这个模型看起来很熟悉，因为它是Lyette的一个变体。
- en: which we discussed earlier in this video， but it's adapted to take three color
    images。The final ingredients we need are a loss function， and an optimizer。The
    loss function。 as discussed earlier in this video， is a measure of how far from
    our ideal output the model's prediction was。 Cross entropy loss is a typical loss
    function for classification models like ours。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本视频早些时候讨论过这一点，但它被调整为处理三种颜色的图像。我们需要的最终组成部分是损失函数和优化器。损失函数，如本视频早些时候讨论的，是衡量模型预测与理想输出之间差距的指标。交叉熵损失是像我们这样的分类模型的典型损失函数。
- en: The optimizer is what drives the learning。Here， we've created an optimizer that
    implements stochastic gradient descent。 one of the more straightforward optimization
    algorithms。 Besides parameters of the algorithm。 like the learning rate and momentum。
    we also pass in net dot parameters。 which is a collection of all the learning
    weights in the model， which is what the optimizer adjusts。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是推动学习的关键。在这里，我们创建了一个实现随机梯度下降的优化器，这是最简单的优化算法之一。除了算法的参数，如学习率和动量外，我们还传入了net的参数，这是模型中所有学习权重的集合，优化器会调整这些权重。
- en: Finally， all this is assembled into the training loop。Go ahead and run this
    cell。 as it'll take a couple of minutes to execute。So here we're only doing two
    training epics。 as you can see from line 1， that is two complete passes over the
    training data set。Each pass has an inner loop that iterates over the training
    data。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有这些都汇集到训练循环中。继续运行这个单元，因为它会花几分钟来执行。从第一行你可以看到，我们只进行了两次训练历程，即对训练数据集的两次完整遍历。每次遍历都有一个内循环，用于迭代训练数据。
- en: serving batches of transformed images in their correct labels。Zing the gradients
    in line 9 is a very important step。 When you run a batch。 gradients are accumulated
    over that batch。 And if we don't reset the gradients for every batch。 they will
    keep accumulating and provide incorrect values， and learning will stop。In line
    12。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 处理批次的变换图像及其正确标签。在第9行中处理梯度是一个非常重要的步骤。当你运行一个批次时，梯度会在该批次中累积。如果我们不为每个批次重置梯度，它们将继续累积并提供不正确的值，从而导致学习停止。在第12行中。
- en: we ask the model for its actual prediction on the batch。In the following line，
    line 13。 we compute the loss。 The difference between the outputs and the labels。In
    line 14。 we do our backward pass and calculate the gradients that will direct
    the learning。In line 15。 the optimizer performs one learning step。It uses the
    gradients from the backward call to nudge the learning weights in the direction
    it thinks will reduce the loss。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向模型请求其对该批次的实际预测。在接下来的行中，行13，我们计算损失。输出与标签之间的差异。在行14中，我们进行反向传播，计算将引导学习的梯度。在行15中，优化器执行一次学习步骤。它利用反向调用中的梯度来推动学习权重朝着减少损失的方向调整。
- en: So the remainder of the loop just does some light reporting on the epic number
    and how many training instances have been completed。What the collective losses
    is over the training epic。So note that the loss is monotonomically descended。
    indicating that our model is continuing to improve its performance in the training
    data set。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以循环的其余部分只是对史诗编号和已完成的训练实例数量进行一些简单报告。总体损失在训练史诗上是多少。因此请注意，损失是单调下降的，这表明我们的模型在训练数据集上的表现不断提高。
- en: As a final step， we should check that the model is actually doing general learning
    and not simply memorizing a data set。 This is called overfitting and will often
    indicate that either your data set is too small and doesn't have enough examples
    or that your model is too large that it's overspecified。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们应该检查模型是否真的在进行一般学习，而不是简单地记忆数据集。这称为过拟合，通常表明你的数据集太小，示例不足，或模型过大，超出了规范。
- en: For modeling the data。You're treatinging it。So our training was done。嗯。So anyways，
    the way we。Check for overfitting and guard against it is to test the model on
    data it hasn't trained on。 that's where we have a test data set。So here I'm just
    going to run the test data through。 we'll get in accuracy measure out。55% Okay，
    so that's not exactly the state of the art。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据进行建模。你正在处理它。所以我们的训练完成了。嗯。无论如何，我们检测过拟合并防止它的方法是测试模型在未训练过的数据上。这就是我们拥有测试数据集的原因。所以我将运行测试数据，我们将得到一个准确度度量。55%
    好吧，这并不算先进。
- en: but it's much better than the 10% we'd expect to see from a random output。![](img/1181a3784e7b926ea0c178203ada7ae1_22.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但这比我们期望的随机输出的10%要好得多。![](img/1181a3784e7b926ea0c178203ada7ae1_22.png)
- en: This demonstrates that some general learning did happen in the model。Now。 when
    you go to the trouble of building and training a non trivial model is usually
    because you want to use it for something。 You need to connect it to a system that
    feeds its inputs and processes the model's predictions。If you're keen on optimizing
    performance， you may want to do this without a dependency on the Python interpreter。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明模型确实发生了一些一般学习。现在，当你花费时间构建和训练一个非平凡模型时，通常是因为你希望将其用于某个目的。你需要将其连接到一个系统，以便输入数据并处理模型的预测。如果你热衷于优化性能，可能希望在不依赖Python解释器的情况下做到这一点。
- en: The good news is that Pytorrch accommodates you with torch script。😊，Torchscript
    is a static。 high performance subset of Python。 When you convert a model to torchscript。
    the dynamic and pythonic nature of your model is fully preserved。 Control flow
    is preserved when convert to torchscript。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是PyTorch为你提供了torch script。😊，Torchscript是Python的一个静态高性能子集。当你将模型转换为torchscript时，模型的动态和Pythonic特性会被完全保留。控制流在转换为torchscript时得到保留。
- en: and you can still use convenient Python data structures， like lists and dictionaries。Looking
    at the code on the right， you'll see a py torch model defined in Python。Below
    that。 in instance of the model is created， and then we'll call torch dot dot script
    my module。That one line of code is all it takes to convert your Python model to
    torchscript。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 而且你仍然可以使用方便的Python数据结构，如列表和字典。查看右侧的代码，你会看到用Python定义的py torch模型。在此之下，创建模型的实例，然后我们将调用torch
    dot dot script my module。只需一行代码，就可以将你的Python模型转换为torchscript。
- en: The serialized version of this gets saved in the final line。 and it contains
    all the information about your model's computation graph and its learning weights。The
    torch grip rendering of the model is shown at the right。Torch script is meant
    to be consumed by the piytorrch just in time compiler or Jit。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该序列化版本保存在最后一行，其中包含有关您模型计算图及其学习权重的所有信息。模型的torch grip渲染显示在右侧。Torch脚本旨在被PyTorch即时编译器或Jit使用。
- en: The Jit seeks runtime optimizations such as operation reordering and layer fusion
    to maximize your model's performance on CPU or GP hardware。![](img/1181a3784e7b926ea0c178203ada7ae1_24.png)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Jit寻求运行时优化，如操作重排序和层融合，以最大化您模型在CPU或GPU硬件上的性能。![](img/1181a3784e7b926ea0c178203ada7ae1_24.png)
- en: So how do you load and execute a torch script model。 You start by loading the
    serialized package with a torch shott Jet dot load。 and then you can call it just
    like any other model。What's more， you can do this in Python or。You can load it
    into the Pytorrch C plus plus runtime to remove the interpreted language dependency。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您如何加载和执行torch脚本模型呢？您可以通过torch shott Jet dot load加载序列化包，然后像其他模型一样调用它。更重要的是，您可以在Python中这样做，也可以将其加载到Pytorrch
    C++运行时中，以消除对解释语言的依赖。
- en: In subsequent videos we'll go into more detail about Torch script， best practices
    for deployment。 and will cover Torch Ser P Torch's model serving solution。![](img/1181a3784e7b926ea0c178203ada7ae1_26.png)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续视频中，我们将更详细地讨论Torch脚本、部署的最佳实践，并涵盖Torch Ser P Torch的模型服务解决方案。![](img/1181a3784e7b926ea0c178203ada7ae1_26.png)
- en: So that's our lightning， fast overview of Pyetorrch。 The models and data sets
    we used here were quite simple。 But Pytorrchches used in production at large enterprises
    for powerful real world use cases。 like translating between human languages， describing
    the content of video scenes or generating realistic human voices in the videos
    to follow will' give you access to that power will go deeper on all the topics
    covered here with more complex use cases like the ones you'll see in the real
    world。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们对Pyetorrch的快速概述。我们在这里使用的模型和数据集相对简单。但Pytorrch在大型企业中用于强大的实际应用场景，例如在人类语言之间进行翻译、描述视频场景的内容或在视频中生成逼真的人声。接下来将深入探讨这里讨论的所有主题，涵盖更复杂的用例，就像您在现实世界中看到的那样。
- en: 😊，Thank you for your time and attention。 And I hope to see you around the Pytorrch
    Fors。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，感谢您的时间和关注。希望在Pytorrch Fors见到您。
