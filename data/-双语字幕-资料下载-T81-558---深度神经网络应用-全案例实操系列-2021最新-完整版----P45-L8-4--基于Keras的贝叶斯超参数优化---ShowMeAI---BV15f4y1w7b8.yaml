- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P45ï¼šL8.4- åŸºäºKerasçš„è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘T81-558 ï½œ æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨-å…¨æ¡ˆä¾‹å®æ“ç³»åˆ—(2021æœ€æ–°Â·å®Œæ•´ç‰ˆ) - P45ï¼šL8.4- åŸºäºKerasçš„è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
    - ShowMeAI - BV15f4y1w7b8
- en: '![](img/9925006dbf7cc9645dc4cf05b451019d_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9925006dbf7cc9645dc4cf05b451019d_0.png)'
- en: Hiï¼Œ this is Jeff Heatonã€‚ welcome to applications of deep neural networks with
    Washington University In this videoã€‚ we're going to look at how you can use Bayesian
    optimization to tell you how to architect your neural network for the latest on
    my AI course and projectsã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ï¼Œæˆ‘æ˜¯æ°å¤«Â·å¸Œé¡¿ã€‚æ¬¢è¿æ¥åˆ°åç››é¡¿å¤§å­¦çš„æ·±åº¦ç¥ç»ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™æ®µè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ¥å‘Šè¯‰ä½ å¦‚ä½•ä¸ºä½ çš„ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ã€‚æœ‰å…³æˆ‘æœ€æ–°çš„AIè¯¾ç¨‹å’Œé¡¹ç›®çš„ä¿¡æ¯ã€‚
- en: click subscribe and the bell next to it to be notified of every new video Now
    we're going to look at Bayesian hyperparameter optimizationã€‚ believe me this is
    a very big thing in kagle currentlyï¼Œ this is how you optimize hyperparaã€‚ the hyperpara
    like we've talked about many times before are those things that you have to set
    for a particular modelã€‚ Almost all model types have hyperpara to some degree neural
    networks have the number of hidden layers the neuron counts if you're using batch
    regularization a variety of other thingsã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»è®¢é˜…å’Œæ—è¾¹çš„é“ƒé“›ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ–°è§†é¢‘å‘å¸ƒæ—¶è·å¾—é€šçŸ¥ã€‚ç°åœ¨æˆ‘ä»¬è¦çœ‹çœ‹è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–ã€‚ç›¸ä¿¡æˆ‘ï¼Œè¿™åœ¨Kaggleä¸Šæ˜¯éå¸¸é‡è¦çš„ï¼Œè¿™å°±æ˜¯ä½ å¦‚ä½•ä¼˜åŒ–è¶…å‚æ•°ã€‚æˆ‘ä»¬ä¹‹å‰å¤šæ¬¡è®¨è®ºè¿‡çš„è¶…å‚æ•°å°±æ˜¯ä½ éœ€è¦ä¸ºç‰¹å®šæ¨¡å‹è®¾ç½®çš„é‚£äº›ä¸œè¥¿ã€‚å‡ ä¹æ‰€æœ‰æ¨¡å‹ç±»å‹åœ¨æŸç§ç¨‹åº¦ä¸Šéƒ½æœ‰è¶…å‚æ•°ï¼Œç¥ç»ç½‘ç»œæœ‰éšè—å±‚çš„æ•°é‡ã€ç¥ç»å…ƒè®¡æ•°ï¼Œå¦‚æœä½ ä½¿ç”¨æ‰¹é‡æ­£åˆ™åŒ–ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–å› ç´ ã€‚
- en: Basically there's two sides for thisã€‚ There's parameters and their hyperpara
    are the weights and other things of the neural network that back propagation will
    adjust for youã€‚ğŸ˜Šã€‚![](img/9925006dbf7cc9645dc4cf05b451019d_2.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œè¿™é‡Œæœ‰ä¸¤ä¸ªæ–¹é¢ã€‚ä¸€ä¸ªæ˜¯å‚æ•°ï¼Œå¦ä¸€ä¸ªæ˜¯è¶…å‚æ•°ï¼Œè¶…å‚æ•°æ˜¯ç¥ç»ç½‘ç»œçš„æƒé‡å’Œå…¶ä»–å°†ç”±åå‘ä¼ æ’­ä¸ºä½ è°ƒæ•´çš„ä¸œè¥¿ã€‚ğŸ˜Šã€‚![](img/9925006dbf7cc9645dc4cf05b451019d_2.png)
- en: expects you to go into the neural network and set each of the weights by handã€‚
    They haven't done that since the 80sã€‚ Howeverï¼Œ you do have to set your hyperparametersã€‚
    There is back propagation is not going to go in and remove layers or add layers
    or other things for you You're going to have to set those on your own or use something
    like Bayesian hyperparameter optimization Now Bayesian hyperparameter optimization
    is one of manyã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœŸæœ›ä½ è¿›å…¥ç¥ç»ç½‘ç»œå¹¶æ‰‹åŠ¨è®¾ç½®æ¯ä¸ªæƒé‡ã€‚è‡ª80å¹´ä»£ä»¥æ¥ä»–ä»¬å°±æ²¡æœ‰è¿™æ ·åšäº†ã€‚ç„¶è€Œï¼Œä½ ç¡®å®éœ€è¦è®¾ç½®ä½ çš„è¶…å‚æ•°ã€‚åå‘ä¼ æ’­ä¸ä¼šä¸ºä½ å»åˆ é™¤å±‚æˆ–æ·»åŠ å±‚æˆ–å…¶ä»–äº‹æƒ…ã€‚ä½ å¿…é¡»è‡ªå·±è®¾ç½®è¿™äº›ï¼Œæˆ–è€…ä½¿ç”¨åƒè´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–è¿™æ ·çš„ä¸œè¥¿ã€‚ç°åœ¨ï¼Œè´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–æ˜¯ä¼—å¤šæ–¹æ³•ä¹‹ä¸€ã€‚
- en: manyï¼Œ many optimization algorithms Most of these optimization algorithms work
    by simply taking a vector or a very long list of numbers and they adjust those
    numbers so that some objective function is minimized or maximized This is what
    back propagation does back propagation basically uses derivatives and calculus
    and fundamentally gradient descent to adjust those weights to minimize your error
    function you cannot use back propagationã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šï¼Œè®¸å¤šä¼˜åŒ–ç®—æ³•ã€‚å¤§å¤šæ•°è¿™äº›ä¼˜åŒ–ç®—æ³•çš„å·¥ä½œæ–¹å¼æ˜¯ç®€å•åœ°å–ä¸€ä¸ªå‘é‡æˆ–ä¸€ä¸ªå¾ˆé•¿çš„æ•°å­—åˆ—è¡¨ï¼Œå¹¶è°ƒæ•´è¿™äº›æ•°å­—ï¼Œä»¥ä½¿æŸä¸ªç›®æ ‡å‡½æ•°æœ€å°åŒ–æˆ–æœ€å¤§åŒ–ã€‚è¿™å°±æ˜¯åå‘ä¼ æ’­æ‰€åšçš„ï¼Œåå‘ä¼ æ’­åŸºæœ¬ä¸Šä½¿ç”¨å¯¼æ•°å’Œå¾®ç§¯åˆ†ï¼Œæ ¹æœ¬ä¸Šæ˜¯æ¢¯åº¦ä¸‹é™æ¥è°ƒæ•´è¿™äº›æƒé‡ï¼Œä»¥æœ€å°åŒ–ä½ çš„è¯¯å·®å‡½æ•°ã€‚ä½ ä¸èƒ½ä½¿ç”¨åå‘ä¼ æ’­ã€‚
- en: To adjust the hidden layers of your neural network or the hidden layer counts
    because the count of hidden layers and neuron counts is not part of the error
    functionã€‚ and is also not differentiableã€‚ You can't take a derivative of how many
    hidden layers you have So you need to use a non-differiable optimization function
    on top of your differentiable optimization functionã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒæ•´ä½ çš„ç¥ç»ç½‘ç»œçš„éšè—å±‚æˆ–éšè—å±‚è®¡æ•°ï¼Œå› ä¸ºéšè—å±‚çš„æ•°é‡å’Œç¥ç»å…ƒçš„æ•°é‡ä¸æ˜¯è¯¯å·®å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”ä¹Ÿä¸å¯å¾®åˆ†ã€‚ä½ æ— æ³•å¯¹ä½ æœ‰å¤šå°‘ä¸ªéšè—å±‚è¿›è¡Œæ±‚å¯¼ã€‚å› æ­¤ï¼Œä½ éœ€è¦åœ¨å¯å¾®åˆ†ä¼˜åŒ–å‡½æ•°ä¹‹ä¸Šä½¿ç”¨ä¸€ä¸ªä¸å¯å¾®åˆ†çš„ä¼˜åŒ–å‡½æ•°ã€‚
- en: which is back propagation usually for neural networksã€‚ the atom update ruleã€‚
    which is a variant of back propagationã€‚ So what we need to do is we are going
    to as we saw in the previous part of this module we are going to create a function
    that creates neural networks based on a vector that is sent to itã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­é€šå¸¸ç”¨äºç¥ç»ç½‘ç»œã€‚åŸå­æ›´æ–°è§„åˆ™ï¼Œè¿™æ˜¯åå‘ä¼ æ’­çš„ä¸€ç§å˜ä½“ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œå¦‚æˆ‘ä»¬åœ¨æœ¬æ¨¡å—çš„å‰ä¸€éƒ¨åˆ†ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåŸºäºä¼ é€’ç»™å®ƒçš„å‘é‡åˆ›å»ºç¥ç»ç½‘ç»œçš„å‡½æ•°ã€‚
- en: So just to briefly review what we had in the previous one we're going to go
    ahead and run this which is uses the simple data that I gave you previouslyã€‚doesnn't
    really matter what we're trying to solve with with the neural network and then
    I'm going to run this part that evaluates the neural networkã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç®€å•å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åœ¨å‰ä¸€ä¸ªéƒ¨åˆ†ä¸­æ‰€è®¨è®ºçš„å†…å®¹ï¼Œæˆ‘ä»¬å°†ç»§ç»­è¿è¡Œè¿™ä¸ªï¼Œå®ƒä½¿ç”¨çš„æ˜¯æˆ‘ä¹‹å‰ç»™ä½ çš„ç®€å•æ•°æ®ã€‚æˆ‘ä»¬è¯•å›¾ç”¨ç¥ç»ç½‘ç»œè§£å†³çš„å†…å®¹å¹¶ä¸é‡è¦ï¼Œç„¶åæˆ‘å°†è¿è¡Œè¿™ä¸ªéƒ¨åˆ†æ¥è¯„ä¼°ç¥ç»ç½‘ç»œã€‚
- en: Now it takes it a moment to runã€‚ So we're going to go ahead and let it runã€‚
    see the star shows that it is actually runningã€‚ You're passing it a variety of
    parameters that drop out the learning rate the neuron percent in the neuron shrink
    these last two are not standard cures or Tensorflow concepts there concepts that
    I am introducing here just get this to a vector So we now have a vector of four
    numbers that we're trying to optimize Now you may want to put well more than that
    in there but we will basically show you how those workã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ƒéœ€è¦ä¸€ç‚¹æ—¶é—´æ¥è¿è¡Œã€‚æ‰€ä»¥æˆ‘ä»¬å°†ç»§ç»­è®©å®ƒè¿è¡Œã€‚çœ‹åˆ°æ˜Ÿæ˜Ÿè¡¨æ˜å®ƒç¡®å®åœ¨è¿è¡Œã€‚ä½ ä¼ é€’ç»™å®ƒå„ç§å‚æ•°ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€ç¥ç»å…ƒç™¾åˆ†æ¯”å’Œç¥ç»å…ƒç¼©å‡ï¼Œè¿™æœ€åä¸¤ä¸ªä¸æ˜¯æ ‡å‡†çš„è¶…å‚æ•°æˆ–Tensorflowæ¦‚å¿µï¼Œè€Œæ˜¯æˆ‘åœ¨è¿™é‡Œå¼•å…¥çš„æ¦‚å¿µï¼Œä»¥ä¾¿å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªå››ä¸ªæ•°å­—çš„å‘é‡ï¼Œæˆ‘ä»¬æ­£åœ¨å°è¯•ä¼˜åŒ–ã€‚ä½ å¯èƒ½æƒ³åœ¨é‡Œé¢æ”¾æ›´å¤šï¼Œä½†æˆ‘ä»¬åŸºæœ¬ä¸Šå°†å‘ä½ å±•ç¤ºå®ƒä»¬çš„å·¥ä½œåŸç†ã€‚
- en: So the neuron count is going to be basically this neuron percent So this neuron
    percent is what percent of 5000 maximum neurons that we' have Now unfortunately
    this 5000 neurons that's a number that's something we're setting so this is a
    hyper hyperparameter butã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç¥ç»å…ƒçš„æ•°é‡åŸºæœ¬ä¸Šå°†æ˜¯è¿™ä¸ªç¥ç»å…ƒç™¾åˆ†æ¯”ã€‚æ‰€ä»¥è¿™ä¸ªç¥ç»å…ƒç™¾åˆ†æ¯”æ˜¯æˆ‘ä»¬æœ€å¤§5000ä¸ªç¥ç»å…ƒä¸­çš„ç™¾åˆ†æ¯”ã€‚é—æ†¾çš„æ˜¯ï¼Œè¿™5000ä¸ªç¥ç»å…ƒæ˜¯æˆ‘ä»¬è®¾å®šçš„ä¸€ä¸ªæ•°å­—ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªè¶…è¶…å‚æ•°ï¼Œä½†ã€‚
- en: You have to set some sort of an up boundã€‚ So it's the best I can doã€‚ Sps isï¼Œ
    by the wayã€‚ how many times we're evaluating this because neural networks are stochastic
    and you will get different results each time you try to train it So I set it pretty
    lowã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¿…é¡»è®¾å®šæŸç§ä¸Šé™ã€‚å› æ­¤ï¼Œè¿™æ˜¯æˆ‘èƒ½åšåˆ°çš„æœ€å¥½ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼ŒSpsæ˜¯æˆ‘ä»¬è¯„ä¼°è¿™ä¸€ç‚¹çš„æ¬¡æ•°ï¼Œå› ä¸ºç¥ç»ç½‘ç»œæ˜¯éšæœºçš„ï¼Œæ¯æ¬¡è®­ç»ƒæ—¶ä½ éƒ½ä¼šå¾—åˆ°ä¸åŒçš„ç»“æœï¼Œæ‰€ä»¥æˆ‘æŠŠå®ƒè®¾å¾—ç›¸å½“ä½ã€‚
- en: you might want to set it higher to actually get something better out of this
    So we get our neuron count and then this neuron shrink basically most neural networks
    set up their hidden layers pyramid styles so that you're either starting with
    a bunch and fewerã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½æƒ³æŠŠå®ƒè®¾å¾—æ›´é«˜ï¼Œä»¥ä¾¿å®é™…ä¸Šå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°æˆ‘ä»¬çš„ç¥ç»å…ƒæ•°é‡ï¼Œç„¶åè¿™ä¸ªç¥ç»å…ƒç¼©å‡åŸºæœ¬ä¸Šæ˜¯å¤§å¤šæ•°ç¥ç»ç½‘ç»œä»¥é‡‘å­—å¡”æ ·å¼è®¾ç½®å®ƒä»¬çš„éšè—å±‚ï¼Œè¿™æ ·ä½ è¦ä¹ˆä»ä¸€å¤§å †å¼€å§‹ï¼Œè¦ä¹ˆå‡å°‘ã€‚
- en: fewerï¼Œ fewer or you're starting with a few and more more more tabular data usually
    not hard and fastã€‚ but usually you'll start out with a fair number of hidden neurons
    and you shrinkã€‚ shrink shrink as you go forwardã€‚ So that's what I'm basically
    doing hereã€‚ I have a neuronã€‚ I have the neuron countï¼Œ and then we're going to
    use the neuron shrink down here where we're actually building the neural networkã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å°‘ï¼Œæ›´å°‘ï¼Œæˆ–è€…ä½ å¼€å§‹æ—¶æœ‰ä¸€äº›ï¼Œæ›´å¤šæ›´å¤šæ›´å¤šçš„è¡¨æ ¼æ•°æ®é€šå¸¸å¹¶ä¸æ˜¯å›ºå®šçš„ã€‚ä½†é€šå¸¸ä½ ä¼šä»ç›¸å½“æ•°é‡çš„éšè—ç¥ç»å…ƒå¼€å§‹ï¼Œå¹¶åœ¨å‰è¿›è¿‡ç¨‹ä¸­å‡å°‘ã€‚å°±æ˜¯è¿™æ ·æˆ‘åŸºæœ¬ä¸Šåœ¨åšçš„äº‹æƒ…ã€‚æˆ‘æœ‰ä¸€ä¸ªç¥ç»å…ƒã€‚æˆ‘æœ‰ç¥ç»å…ƒçš„æ•°é‡ï¼Œç„¶åæˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨ç¥ç»å…ƒç¼©å‡ï¼Œå®é™…æ„å»ºç¥ç»ç½‘ç»œã€‚
- en: We're going to go up to as many layers as it takes so long as it's less than
    10ã€‚Againã€‚ another potential hyper hyperparameter and we're going to continue long
    as we have at least 25 neuronsã€‚ So what this is doing is we're basically picking
    the neuron percent which tells us how many neurons we're going to have overall
    and then the neuron shrink which tells us how rapidly it shrinks down to build
    that pyramid style So we might start out with a00 in the first one and then if
    the shrink is 0ã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¢åŠ åˆ°æ‰€éœ€çš„å±‚æ•°ï¼Œåªè¦ä¸è¶…è¿‡10å±‚ã€‚å†ä¸€æ¬¡ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªæ½œåœ¨çš„è¶…å‚æ•°ï¼Œåªè¦æˆ‘ä»¬è‡³å°‘æœ‰25ä¸ªç¥ç»å…ƒï¼Œæˆ‘ä»¬å°†ç»§ç»­ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯åœ¨é€‰æ‹©ç¥ç»å…ƒç™¾åˆ†æ¯”ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬æ•´ä½“å°†æœ‰å¤šå°‘ä¸ªç¥ç»å…ƒï¼Œç„¶åæ˜¯ç¥ç»å…ƒç¼©å‡ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬ç¼©å‡çš„é€Ÿåº¦æ¥æ„å»ºè¿™ç§é‡‘å­—å¡”é£æ ¼ã€‚æ‰€ä»¥æˆ‘ä»¬å¯èƒ½åœ¨ç¬¬ä¸€ä¸ªå±‚ä¸­ä»a00å¼€å§‹ï¼Œç„¶åå¦‚æœç¼©å‡ä¸º0ã€‚
- en: 8 we'd shrink down to 80% of thatã€‚ so we'd have 800 in the next one and then
    we shrink we keep shrinking by that amount each time through and then we also
    have the dropout percentages and the learning rate so we're going to use Bayesian
    optimization to optimize all of these I did not define my hours minutes string
    up there that would have just told me how long this took to actually calculate
    so that that doesn't matter a great deal but the log loss that we got on this
    was negative 62 Now the reason I am using a negativeã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç¼©å°åˆ° 80%ã€‚æ‰€ä»¥ä¸‹ä¸€ä¸ªå°†æ˜¯ 800ï¼Œç„¶åæˆ‘ä»¬æ¯æ¬¡éƒ½ä¼šä»¥è¿™ä¸ªæ•°å€¼ç»§ç»­ç¼©å°ï¼Œæˆ‘ä»¬è¿˜æœ‰ dropout ç™¾åˆ†æ¯”å’Œå­¦ä¹ ç‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ¥ä¼˜åŒ–æ‰€æœ‰è¿™äº›ã€‚æˆ‘æ²¡æœ‰åœ¨ä¸Šé¢å®šä¹‰æˆ‘çš„å°æ—¶å’Œåˆ†é’Ÿå­—ç¬¦ä¸²ï¼Œè¿™æœ¬æ¥å¯ä»¥å‘Šè¯‰æˆ‘å®é™…è®¡ç®—èŠ±äº†å¤šé•¿æ—¶é—´ï¼Œæ‰€ä»¥è¿™å¹¶ä¸é‡è¦ï¼Œä½†æˆ‘ä»¬å¾—åˆ°çš„å¯¹æ•°æŸå¤±æ˜¯è´Ÿ
    62ã€‚ä½¿ç”¨è´Ÿå€¼çš„åŸå› æ˜¯ã€‚
- en: Lo lossã€‚ you'll see that thereã€‚ I have a negative right in thereã€‚ The reason
    that I have a negative here is because the Bayesian optimization wants to maximizeã€‚
    You don't want to maximize a log lossã€‚ So just flipping it around causes it to
    effectively minimize Now to see the part that we want to run in this section I
    have to set up some values here in the bounds for the Bayesian optimizationã€‚ Oh
    and one quick thing you do have to install the Bayesian optimizer that Pip command
    will install it for you if if it's not already been installed and it just tells
    me requirement already satisfiedã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ•°æŸå¤±ã€‚ä½ ä¼šçœ‹åˆ°é‚£é‡Œã€‚æˆ‘æœ‰ä¸€ä¸ªè´Ÿå·åœ¨é‚£ã€‚ä¹‹æ‰€ä»¥è¿™æ ·åšæ˜¯å› ä¸ºè´å¶æ–¯ä¼˜åŒ–æƒ³è¦æœ€å¤§åŒ–ã€‚ä½ ä¸æƒ³æœ€å¤§åŒ–å¯¹æ•°æŸå¤±ã€‚æ‰€ä»¥ç¿»è½¬ä¸€ä¸‹å®é™…ä¸Šä½¿å¾—å®ƒæœ€å°åŒ–ã€‚ç°åœ¨ï¼Œä¸ºäº†çœ‹åˆ°æˆ‘ä»¬åœ¨è¿™ä¸€éƒ¨åˆ†æƒ³è¦è¿è¡Œçš„éƒ¨åˆ†ï¼Œæˆ‘éœ€è¦åœ¨è´å¶æ–¯ä¼˜åŒ–çš„ç•Œé™ä¸­è®¾ç½®ä¸€äº›å€¼ã€‚å“¦ï¼Œè¿˜æœ‰ä¸€ä»¶äº‹ï¼Œå¦‚æœè´å¶æ–¯ä¼˜åŒ–å™¨è¿˜æ²¡æœ‰å®‰è£…ï¼Œä½ éœ€è¦å®‰è£…å®ƒï¼Œé‚£ä¸ª
    Pip å‘½ä»¤ä¼šä¸ºä½ å®‰è£…ï¼Œå¦‚æœå·²ç»å®‰è£…äº†ï¼Œå®ƒä¼šå‘Šè¯‰æˆ‘è¦æ±‚å·²æ»¡è¶³ã€‚
- en: which is good I already have it installed these bounds say that dropout can
    go between 0 and 0ã€‚4990ã€‚499 is already a pretty high dropout by the wayï¼Œ I'm going
    go ahead and and run this Now this can actually take a while to run So I'm not
    going to actually run it I'm showing you the optimal values that we got down hereã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆå¥½ï¼Œæˆ‘å·²ç»å®‰è£…å¥½äº†ï¼Œè¿™äº›ç•Œé™è¡¨ç¤º dropout å¯ä»¥åœ¨ 0 åˆ° 0.499 ä¹‹é—´ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œ0.499 å·²ç»æ˜¯ä¸€ä¸ªç›¸å½“é«˜çš„ dropout äº†ï¼Œæˆ‘ç°åœ¨å°±è¦è¿è¡Œè¿™ä¸ªã€‚å®é™…ä¸Šï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´æ¥è¿è¡Œï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šçœŸçš„è¿è¡Œå®ƒï¼Œæˆ‘åªæ˜¯ç»™ä½ ä»¬å±•ç¤ºæˆ‘ä»¬åœ¨è¿™é‡Œå¾—åˆ°çš„æœ€ä½³å€¼ã€‚
- en: So here we are setting the dropout range between 0 and 499 learning rate betweenã€‚0ã€‚0
    and 0ã€‚1ã€‚ That's probably pretty reasonableã€‚ The neuron percentã€‚ we are going between
    the full range of 1% and 100% and any value in between neuron shrink againã€‚ the
    full percentage rangeã€‚ So these are the four that we're trying to optimizeã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°† dropout èŒƒå›´è®¾ç½®åœ¨ 0 åˆ° 0.499 ä¹‹é—´ï¼Œå­¦ä¹ ç‡åœ¨ 0.0 å’Œ 0.1 ä¹‹é—´ã€‚è¿™å¯èƒ½æ˜¯ç›¸å½“åˆç†çš„ã€‚ç¥ç»å…ƒç™¾åˆ†æ¯”ï¼Œæˆ‘ä»¬åœ¨
    1% åˆ° 100% çš„å…¨èŒƒå›´ä¹‹é—´ï¼Œä»»ä½•ä¸­é—´å€¼ç¥ç»å…ƒç¼©å‡ï¼Œå†æ¬¡æ˜¯å®Œæ•´çš„ç™¾åˆ†æ¯”èŒƒå›´ã€‚æ‰€ä»¥è¿™å››ä¸ªæ˜¯æˆ‘ä»¬å°è¯•ä¼˜åŒ–çš„ã€‚
- en: That's the whole trick to Bayesian optimizationã€‚ You need to turn this into
    a vector that will have all of the hyperparameter summarizedã€‚ So I'm not truly
    putting in every hyperparameterï¼Œ like how many rows and how many neurons in each
    of those rows are hidden layers I should call themã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯è´å¶æ–¯ä¼˜åŒ–çš„æ•´ä¸ªæŠ€å·§ã€‚ä½ éœ€è¦å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ï¼Œä»¥æ€»ç»“æ‰€æœ‰çš„è¶…å‚æ•°ã€‚æ‰€ä»¥æˆ‘å¹¶æ²¡æœ‰çœŸæ­£æ”¾å…¥æ¯ä¸ªè¶…å‚æ•°ï¼Œæ¯”å¦‚æ¯è¡Œæœ‰å¤šå°‘è¡Œå’Œæ¯ä¸ªéšè—å±‚ä¸­æœ‰å¤šå°‘ç¥ç»å…ƒï¼Œæˆ‘åº”è¯¥ç§°å®ƒä»¬ä¸ºéšè—å±‚ã€‚
- en: This is now down to a single vectorã€‚ And reallyï¼Œ you could use any optimizer
    that you wanted for thisã€‚ Any optimizer that does not require derivativeã€‚ So you
    could use things like genetic algorithmsã€‚ Neder meadã€‚ Howeverï¼Œ the thing that
    is really good about Bayesian optimization is it causes your optimization functionã€‚
    your objective function to be called relatively sparingly because theã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™æ˜¯ä¸€ä¸ªå•ä¸€çš„å‘é‡ã€‚å®é™…ä¸Šï¼Œä½ å¯ä»¥ä¸ºæ­¤ä½¿ç”¨ä»»ä½•ä¼˜åŒ–å™¨ã€‚ä»»ä½•ä¸éœ€è¦å¯¼æ•°çš„ä¼˜åŒ–å™¨ã€‚å› æ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨é—ä¼ ç®—æ³•ã€Nelder-Mead ç­‰ã€‚ç„¶è€Œï¼Œè´å¶æ–¯ä¼˜åŒ–çš„ä¸€ä¸ªå¾ˆå¥½çš„åœ°æ–¹æ˜¯ï¼Œå®ƒä½¿å¾—ä½ çš„ä¼˜åŒ–å‡½æ•°ã€ä½ çš„ç›®æ ‡å‡½æ•°è¢«è°ƒç”¨çš„æ¬¡æ•°ç›¸å¯¹è¾ƒå°‘ï¼Œå› ä¸ºã€‚
- en: Objective function here is very expensiveã€‚ It trains a neural networkã€‚ So this
    could potentially go for many minutes or even many hoursã€‚ That's why Bayesian
    optimization is considered better than a lot of the other ones like a Mellder
    mead or something such as thatã€‚ it's very optimal on how many times that is actually
    calledã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ç›®æ ‡å‡½æ•°éå¸¸æ˜‚è´µã€‚å®ƒè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œã€‚æ‰€ä»¥è¿™å¯èƒ½ä¼šæŒç»­å¾ˆå¤šåˆ†é’Ÿç”šè‡³å‡ ä¸ªå°æ—¶ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè´å¶æ–¯ä¼˜åŒ–è¢«è®¤ä¸ºæ¯”å…¶ä»–å¾ˆå¤šæ–¹æ³•æ›´å¥½çš„åŸå› ï¼Œæ¯”å¦‚ Nelder-Mead
    æˆ–å…¶ä»–ç±»ä¼¼æ–¹æ³•ã€‚å®ƒåœ¨å®é™…è°ƒç”¨æ¬¡æ•°ä¸Šéå¸¸é«˜æ•ˆã€‚
- en: Then you're going to construct the Bayesian optizerã€‚ You give the function of
    evaluate network that we defined up thereã€‚ We pass in the P boundsã€‚ We're going
    to make it fairly verboseã€‚ I'm going to go ahead and run it so that we can see
    some of the resultsã€‚ We won't be able to run it to entire TV as it takes a long
    timeã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å°†æ„å»ºè´å¶æ–¯ä¼˜åŒ–å™¨ã€‚ä½ ç»™å‡ºæˆ‘ä»¬åœ¨ä¸Šé¢å®šä¹‰çš„è¯„ä¼°ç½‘ç»œçš„å‡½æ•°ã€‚æˆ‘ä»¬ä¼ å…¥Pçš„è¾¹ç•Œã€‚æˆ‘ä»¬å°†ä½¿å…¶ç›¸å½“è¯¦ç»†ã€‚æˆ‘å°†ç»§ç»­è¿è¡Œå®ƒï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€äº›ç»“æœã€‚æˆ‘ä»¬æ— æ³•å°†å…¶è¿è¡Œåˆ°æ•´ä¸ªç”µè§†ï¼Œå› ä¸ºè¿™ä¼šèŠ±è´¹å¾ˆé•¿æ—¶é—´ã€‚
- en: And then we're going to So since we say verbose 2ã€‚ you're seeing it's starting
    to build this alreadyã€‚ Now you can see the first iteration alreadyã€‚ The log losses
    around 74ï¼Œ which is not particularly goodã€‚ After running this for a few hoursã€‚
    by the wayï¼Œ we get to thisã€‚ when I'll explain that in a momentã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¦è¿™æ ·åšï¼Œç”±äºæˆ‘ä»¬è®¾å®šä¸ºè¯¦ç»†æ¨¡å¼2ã€‚ä½ ä¼šçœ‹åˆ°å®ƒå·²ç»å¼€å§‹æ„å»ºè¿™ä¸ªäº†ã€‚ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°ç¬¬ä¸€è½®å·²ç»å®Œæˆã€‚æ—¥å¿—æŸå¤±åœ¨74å·¦å³ï¼Œè¿™å¹¶ä¸æ˜¯ç‰¹åˆ«å¥½ã€‚ç»è¿‡å‡ ä¸ªå°æ—¶çš„è¿è¡Œã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†è¿™ä¸ªã€‚å½“æˆ‘ç¨åè§£é‡Šã€‚
- en: And the random state that's just a seedã€‚ Now this is important hereã€‚ you have
    the number of a knit points and theã€‚ğŸ˜Šï¼ŒNumber of iterationsã€‚ This has to do with
    the whole multi bandit in armed bandit optimizationã€‚ You can think of thisã€‚ the
    multiarmed banditï¼Œ if you've not heard of that beforeï¼Œ it comes from the casinosã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºçŠ¶æ€ä»…ä»…æ˜¯ä¸€ä¸ªç§å­ã€‚ç°åœ¨è¿™é‡Œå¾ˆé‡è¦ã€‚ä½ æœ‰ä¸€ä¸ªç‚¹çš„æ•°é‡å’ŒğŸ˜Šï¼Œè¿­ä»£çš„æ•°é‡ã€‚è¿™ä¸æ•´ä¸ªå¤šè‡‚å¼ºç›—ä¼˜åŒ–æœ‰å…³ã€‚ä½ å¯ä»¥æƒ³è±¡ä¸€ä¸‹è¿™ä¸ªå¤šè‡‚å¼ºç›—ï¼Œå¦‚æœä½ ä»¥å‰æ²¡å¬è¯´è¿‡ï¼Œå®ƒæ¥è‡ªèµŒåœºã€‚
- en: the slot machinesã€‚ So say you have a row of slot machines just like this you
    have to optimize between exploit and exploit Exp means trying different slot machines
    in the group because one slot machine might be more generous than the othersã€‚
    Explo means once you found a pretty generous slot machineï¼Œ spending your time
    not Exp the othersã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è€è™æœºã€‚å‡è®¾ä½ æœ‰ä¸€æ’è€è™æœºï¼Œå°±åƒè¿™æ ·ï¼Œä½ å¿…é¡»åœ¨åˆ©ç”¨å’Œæ¢ç´¢ä¹‹é—´è¿›è¡Œä¼˜åŒ–ã€‚æ¢ç´¢æ„å‘³ç€å°è¯•è¿™ä¸€ç»„ä¸­çš„ä¸åŒè€è™æœºï¼Œå› ä¸ºæœ‰ä¸€ä¸ªè€è™æœºå¯èƒ½æ¯”å…¶ä»–çš„æ›´æ…·æ…¨ã€‚åˆ©ç”¨æ„å‘³ç€ä¸€æ—¦ä½ æ‰¾åˆ°ä¸€ä¸ªç›¸å½“æ…·æ…¨çš„è€è™æœºï¼Œå°±èŠ±æ—¶é—´ä¸å»æ¢ç´¢å…¶ä»–çš„ã€‚
- en: but pulling the arm on that slot machine and winding as much money as you can
    Looking back at the codeã€‚ we can see that the 10ï¼Œ that's the exploitã€‚ So that's
    the number of points we're going to look atã€‚ So we're going try 10 slot machines
    so to speakã€‚ and then once we focus on one is pretty goodã€‚ We're going to go deep
    enoughï¼Œ we're going to exploit to 100 iterationsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹‰åŠ¨é‚£ä¸ªè€è™æœºçš„æ‰‹è‡‚ï¼Œå°½å¯èƒ½å¤šåœ°èµ¢å–é’±ã€‚å›é¡¾ä»£ç ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°10ï¼Œè¿™æ˜¯åˆ©ç”¨ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬å°†è¦è§‚å¯Ÿçš„ç‚¹çš„æ•°é‡ã€‚å¯ä»¥è¯´æˆ‘ä»¬å°†å°è¯•10ä¸ªè€è™æœºã€‚ç„¶åä¸€æ—¦æˆ‘ä»¬ä¸“æ³¨äºä¸€ä¸ªè¡¨ç°ä¸é”™çš„ï¼Œæˆ‘ä»¬å°†æ·±å…¥è¶³å¤Ÿï¼Œåˆ©ç”¨åˆ°100æ¬¡è¿­ä»£ã€‚
- en: And here you can see this continuing to runã€‚ It's going to tell you okay so
    the secondã€‚Iteration we locked in on a slightly better oneã€‚ This is going to result
    in about 110 rows because you have 10 plus 100ã€‚ And then the best one that I got
    when I ran it for this full amount of time was a log loss of 59 which is actually
    much better than I got manually optimizing those hyperparameters and I didn't
    even put every hyperparameter into hereã€‚ you can definitely make that vectorã€‚
    you could take that vector up to maybe 10 or 20 and just come up with more creative
    ways to break apart and to encode those hyperparameter to encode the architecture
    of the neural network and here you can see basically the optimal parametersã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°å®ƒç»§ç»­è¿è¡Œã€‚å®ƒä¼šå‘Šè¯‰ä½ ï¼Œå¥½å§ï¼Œç¬¬äºŒæ¬¡ã€‚è¿­ä»£æˆ‘ä»¬é”å®šåœ¨ä¸€ä¸ªç¨å¾®æ›´å¥½çš„é€‰é¡¹ä¸Šã€‚è¿™å°†å¯¼è‡´å¤§çº¦110è¡Œï¼Œå› ä¸ºä½ æœ‰10åŠ 100ã€‚ç„¶åå½“æˆ‘è¿è¡Œè¿™ä¸ªå…¨æ—¶é—´æ®µæ—¶ï¼Œå¾—åˆ°çš„æœ€ä½³ç»“æœæ˜¯æ—¥å¿—æŸå¤±59ï¼Œè¿™å®é™…ä¸Šæ¯”æˆ‘æ‰‹åŠ¨ä¼˜åŒ–é‚£äº›è¶…å‚æ•°è¦å¥½å¾—å¤šï¼Œè€Œä¸”æˆ‘ç”šè‡³æ²¡æœ‰æŠŠæ¯ä¸ªè¶…å‚æ•°éƒ½æ”¾è¿›æ¥ã€‚ä½ ç»å¯¹å¯ä»¥åˆ¶ä½œé‚£ä¸ªå‘é‡ã€‚ä½ å¯ä»¥å°†é‚£ä¸ªå‘é‡å¢åŠ åˆ°10æˆ–20ï¼Œæƒ³å‡ºæ›´å¤šåˆ›é€ æ€§çš„æ–¹å¼æ¥æ‹†åˆ†å’Œç¼–ç é‚£äº›è¶…å‚æ•°ï¼Œç¼–ç ç¥ç»ç½‘ç»œçš„æ¶æ„ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°åŸºæœ¬çš„æœ€ä½³å‚æ•°ã€‚
- en: we ended up liking a dropout rate of about 13 learning rate pretty low 0ã€‚01
    neuron percent pretty low and not a huge neural network and shrinking by about
    30% each time This could be something useful for your kaggel competitionã€‚ you
    might want to useã€‚In optimization to optimize your hyperparametersã€‚ not just for
    your neural networkï¼Œ but if you're using other model types as well Thank you for
    this video and next're the of for course content changes often so up to date and
    and artificial intelligenceã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€ç»ˆå–œæ¬¢å¤§çº¦13çš„ dropout ç‡ï¼Œå­¦ä¹ ç‡ç›¸å½“ä½0.01ï¼Œç¥ç»å…ƒç™¾åˆ†æ¯”ç›¸å½“ä½ï¼Œè€Œä¸”ä¸æ˜¯ä¸€ä¸ªå·¨å¤§çš„ç¥ç»ç½‘ç»œï¼Œæ¯æ¬¡ç¼©å°çº¦30%ã€‚è¿™å¯èƒ½å¯¹ä½ çš„ Kaggle
    ç«èµ›æœ‰ç”¨ï¼Œä½ å¯èƒ½æƒ³ç”¨æ¥ä¼˜åŒ–ä½ çš„è¶…å‚æ•°ã€‚ä¸ä»…ä»…æ˜¯é’ˆå¯¹ä½ çš„ç¥ç»ç½‘ç»œï¼Œå¦‚æœä½ ä½¿ç”¨å…¶ä»–æ¨¡å‹ç±»å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ„Ÿè°¢ä½ è§‚çœ‹è¿™ä¸ªè§†é¢‘ï¼Œä¸‹ä¸€ä¸ªæ˜¯è¯¾ç¨‹å†…å®¹ï¼Œå˜åŒ–é¢‘ç¹ï¼Œæ‰€ä»¥è¯·ä¿æŒæ›´æ–°ï¼Œå…³äºäººå·¥æ™ºèƒ½ã€‚
- en: '![](img/9925006dbf7cc9645dc4cf05b451019d_4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9925006dbf7cc9645dc4cf05b451019d_4.png)'
