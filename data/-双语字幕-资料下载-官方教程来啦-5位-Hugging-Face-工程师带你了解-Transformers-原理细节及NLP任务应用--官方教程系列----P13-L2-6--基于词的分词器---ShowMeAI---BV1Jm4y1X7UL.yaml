- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P13ï¼šL2.6- åŸºäºè¯çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘å®˜æ–¹æ•™ç¨‹æ¥å•¦ï¼5ä½ Hugging Face å·¥ç¨‹å¸ˆå¸¦ä½ äº†è§£ Transformers åŸç†ç»†èŠ‚åŠNLPä»»åŠ¡åº”ç”¨ï¼ï¼œå®˜æ–¹æ•™ç¨‹ç³»åˆ—ï¼
    - P13ï¼šL2.6- åŸºäºè¯çš„åˆ†è¯å™¨ - ShowMeAI - BV1Jm4y1X7UL
- en: Yesã€‚Let's take a look at word based tokenizationã€‚World based organization is
    the idea of splitting the raw text into words by splitting on spaces or other
    specific rulesã€‚Like punctuationã€‚In this algorithmï¼Œ each word has a specific number
    or ID attributed to itã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹åŸºäºè¯çš„åˆ†è¯ã€‚åŸºäºè¯çš„ç»„ç»‡æ€æƒ³æ˜¯é€šè¿‡åœ¨ç©ºæ ¼æˆ–å…¶ä»–ç‰¹å®šè§„åˆ™ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰å¤„åˆ†å‰²åŸå§‹æ–‡æœ¬æ¥å°†å…¶æ‹†åˆ†æˆå•è¯ã€‚åœ¨è¿™ä¸ªç®—æ³•ä¸­ï¼Œæ¯ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ªç‰¹å®šçš„ç¼–å·æˆ–IDã€‚
- en: Here lets has VI 250ï¼Œ Du has 861 and tokenization followed by an exclamation
    mark has a 345ã€‚ğŸ˜Šã€‚This approach is interestingï¼Œ as the model has representations
    that are based on entire wordsã€‚The information held in a single number is highï¼Œ
    as a word contains a lot of contextual and semantic informationã€‚Howeverï¼Œ this
    approach does have its limitsã€‚ğŸ˜Šï¼ŒFor exampleã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ VI 250ï¼ŒDu æœ‰ 861ï¼Œå¸¦æœ‰æ„Ÿå¹å·çš„åˆ†è¯æ˜¯ 345ã€‚ğŸ˜Šã€‚è¿™ä¸ªæ–¹æ³•å¾ˆæœ‰è¶£ï¼Œå› ä¸ºæ¨¡å‹çš„è¡¨ç¤ºæ˜¯åŸºäºæ•´ä¸ªå•è¯çš„ã€‚å•ä¸ªæ•°å­—æ‰€æ‰¿è½½çš„ä¿¡æ¯é‡å¾ˆå¤§ï¼Œå› ä¸ºä¸€ä¸ªå•è¯åŒ…å«äº†å¾ˆå¤šä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç¡®å®æœ‰å…¶å±€é™æ€§ã€‚ğŸ˜Šï¼Œä¾‹å¦‚ã€‚
- en: the word dog and the word dogs are very similarï¼Œ and their meaning is closeã€‚The
    word based tokenization hallï¼Œ howeverï¼Œ will attribute entirely different ideas
    to these two wordsã€‚ and the model will therefore learn two different embeddings
    for these two wordsã€‚This is unfortunateã€‚ as we would like the model to understand
    that these words are indeed relatedã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å•è¯ dog å’Œå•è¯ dogs éå¸¸ç›¸ä¼¼ï¼Œä¸”å®ƒä»¬çš„å«ä¹‰ç›¸è¿‘ã€‚ç„¶è€Œï¼ŒåŸºäºè¯çš„åˆ†è¯å™¨ä¼šå¯¹è¿™ä¸¤ä¸ªè¯èµ‹äºˆå®Œå…¨ä¸åŒçš„æ¦‚å¿µã€‚å› æ­¤ï¼Œæ¨¡å‹å°†ä¸ºè¿™ä¸¤ä¸ªè¯å­¦ä¹ åˆ°ä¸¤ä¸ªä¸åŒçš„åµŒå…¥ã€‚è¿™æ˜¯éå¸¸é—æ†¾çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹ç†è§£è¿™ä¸¤ä¸ªè¯ç¡®å®æ˜¯ç›¸å…³çš„ã€‚
- en: And that dogs is simply the plural form of the word dogã€‚Another issue with this
    approach is that there are a lot of different words in an languageã€‚If we want
    our model to understand all possible sentences in that languageã€‚ then we will
    need to have an ID for each different wordã€‚And the total number of wordsã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œ dogs ä»…ä»…æ˜¯å•è¯ dog çš„å¤æ•°å½¢å¼ã€‚è¿™ä¸ªæ–¹æ³•çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè¯­è¨€ä¸­æœ‰å¾ˆå¤šä¸åŒçš„å•è¯ã€‚å¦‚æœæˆ‘ä»¬å¸Œæœ›æ¨¡å‹ç†è§£è¯¥è¯­è¨€ä¸­çš„æ‰€æœ‰å¯èƒ½å¥å­ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªä¸åŒçš„å•è¯åˆ†é…ä¸€ä¸ªIDã€‚æ€»çš„å•è¯æ•°é‡ã€‚
- en: which is also known as the vocabulary sizeï¼Œ can quickly become very largeã€‚This
    is an issue because each ID is mapped to a large vector that represents the words
    meanã€‚And keeping track of these mappings requires an enormous number of weightsã€‚When
    the vocabulary size is very largeã€‚ğŸ˜Šï¼ŒIf we want our models to stay leanã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿè¢«ç§°ä¸ºè¯æ±‡å¤§å°ï¼Œå¯ä»¥è¿…é€Ÿå˜å¾—éå¸¸åºå¤§ã€‚è¿™æ˜¯ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¯ä¸ªIDéƒ½æ˜ å°„åˆ°ä¸€ä¸ªå¤§å‹å‘é‡ï¼Œè¯¥å‘é‡è¡¨ç¤ºå•è¯çš„å«ä¹‰ã€‚è·Ÿè¸ªè¿™äº›æ˜ å°„éœ€è¦å¤§é‡çš„æƒé‡ã€‚å½“è¯æ±‡å¤§å°éå¸¸å¤§æ—¶ã€‚ğŸ˜Šï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹ä¿æŒç®€æ´ã€‚
- en: we can opt for our tokenizer to ignore certain words that we don't necessarily
    needã€‚For exampleã€‚ hereï¼Œ when training our tokenizer in textï¼Œ we might want to
    take only the 10ã€‚000 most frequent words in that textï¼Œ rather than taking all
    words from in that text or all language's wordsã€‚To create our basic vocabularyã€‚The
    tokenizer will know how to convert those 10ã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€‰æ‹©è®©åˆ†è¯å™¨å¿½ç•¥æŸäº›æˆ‘ä»¬ä¸ä¸€å®šéœ€è¦çš„å•è¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½åªæƒ³å–æ–‡æœ¬ä¸­æœ€å¸¸è§çš„10,000ä¸ªå•è¯ï¼Œè€Œä¸æ˜¯ä»æ–‡æœ¬ä¸­å–æ‰€æœ‰å•è¯æˆ–æ‰€æœ‰è¯­è¨€çš„å•è¯ã€‚ä»¥åˆ›å»ºæˆ‘ä»¬çš„åŸºç¡€è¯æ±‡ã€‚åˆ†è¯å™¨ä¼šçŸ¥é“å¦‚ä½•å°†è¿™10ä¸ªè¯è½¬æ¢ä¸ºæ•°å­—ã€‚
- en: 000 words into numbersï¼Œ but any other word will be converted to the out of vocabulary
    wordã€‚ or like shown hereï¼Œ the unknown wordã€‚Unfortunatelyï¼Œ this is a compromiseã€‚
    the model will have the exact same representation for all words that it doesn't
    knowã€‚ğŸ˜Šã€‚Which can result in a lot of lost information if many unknown words are
    presentã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 000ä¸ªå•è¯è¢«è½¬æ¢ä¸ºæ•°å­—ï¼Œä½†ä»»ä½•å…¶ä»–å•è¯å°†è¢«è½¬æ¢ä¸ºè¶…å‡ºè¯æ±‡è¡¨çš„è¯ã€‚æˆ–è€…åƒè¿™é‡Œæ‰€ç¤ºï¼ŒæœªçŸ¥è¯ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™æ˜¯ä¸€ç§æŠ˜è¡·ã€‚æ¨¡å‹å¯¹æ‰€æœ‰å®ƒä¸çŸ¥é“çš„å•è¯å°†å…·æœ‰å®Œå…¨ç›¸åŒçš„è¡¨ç¤ºã€‚ğŸ˜Šã€‚å¦‚æœå­˜åœ¨è®¸å¤šæœªçŸ¥è¯ï¼Œå¯èƒ½ä¼šå¯¼è‡´å¤§é‡ä¿¡æ¯çš„ä¸¢å¤±ã€‚
- en: '![](img/2d2fd0eb1726791600f98ab3dc5465fa_1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d2fd0eb1726791600f98ab3dc5465fa_1.png)'
- en: '![](img/2d2fd0eb1726791600f98ab3dc5465fa_2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d2fd0eb1726791600f98ab3dc5465fa_2.png)'
- en: Yeahã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚
