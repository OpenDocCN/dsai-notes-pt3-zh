- en: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P7：L7- 函数式 API 的更深入示例
    - ShowMeAI - BV1em4y1U7ib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】“当前最好的 TensorFlow 教程！”，看完就能自己动手做项目啦！＜实战教程系列＞ - P7：L7- 函数式 API 的更深入示例
    - ShowMeAI - BV1em4y1U7ib
- en: '![](img/2b04a13feb7a7ead68512f94b99b8884_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b04a13feb7a7ead68512f94b99b8884_0.png)'
- en: What's going on guys， hope you're doing awesome and welcome back for another
    Tensorflowlow tutorial so I feel a little bit bad because so far we've been using
    both the sequential and the functional API but really for the examples that I've
    given that really wouldn't be a point to use the functional API so I want to give
    you a more real example where you can't actually use just a sequential API and
    so we're taking another look at MNist but with a little twist we now have two
    MIS digits per example for example here we have the digit zero and one。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，希望你们过得不错，欢迎回来参加另一个TensorFlow教程。我有点愧疚，因为到目前为止，我们一直在使用顺序和函数式API，但实际上我提供的示例并不需要使用函数式API，所以我想给你一个更真实的例子，在这个例子中你确实不能只使用顺序API。我们再来看一下MNIST，但稍微有些不同，我们现在每个示例有两个数字，例如这里有数字零和一。
- en: And I'm going have a link in the description for you to download this so that
    you can also follow along with the video。 but anyways we're not gonna to actually
    focus on on the custom data。 so loading the actual data that's going to be for
    a future separate video for now I just want to give you a little bit more an actual
    example for when the functional API becomes useful so let's get to the code and
    what we have right here is just the basic imports that we've been using so we
    also have one more which is pandas and this is going to be used to load the data
    set so you can just do Conda install pandas I believe and then you'll have this
    and so as I said we're not gonna to focus on the actual data loading part meaning
    I'm going to copy paste and stuff here and I don't like copy pasting stuff but
    we're not going to focus on on that part in this video so you can also there's
    going to be a link in the description where you can copy paste this code as well。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在描述中提供一个链接，以便你可以下载这些内容，以便你也能跟着视频一起学习。不过，我们现在不会专注于自定义数据。因此，实际数据的加载将在未来的单独视频中进行。现在我只是想给你一个更实际的例子，说明函数式API如何变得有用。所以让我们开始代码，这里是我们一直在使用的基本导入，还有一个是pandas，这将用于加载数据集，你可以使用Conda安装pandas，我相信这样你就会得到这个。因此，正如我所说，我们不会专注于实际的数据加载部分，意味着我将在这里复制粘贴一些东西，我不喜欢复制粘贴，但我们不会在这个视频中关注那部分，所以在描述中也会有一个链接，你可以复制粘贴这段代码。
- en: '![](img/2b04a13feb7a7ead68512f94b99b8884_2.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b04a13feb7a7ead68512f94b99b8884_2.png)'
- en: I'm just going to paste that and basically right here this is for loading the
    actual data all right so we're using pandas to read from a CSV file and then we're
    using Tf do data and again I'm going to cover this in a separate video to actually
    load the data So what I do want to focus on is that now we have two target values
    for each example meaning we can't use the sequential because remember sequential
    can only map one input to one output but now we actually have two outputs so we're
    going to build a model and remember we have to use the functional now so we're
    going to do start with Ks。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我将粘贴代码，基本上这是用于加载实际数据的代码。所以我们使用pandas从CSV文件读取数据，然后使用Tf数据，再次强调，我将在另一个视频中详细讲解如何加载数据。所以我想重点关注的是，现在每个示例有两个目标值，这意味着我们不能使用顺序模型，因为顺序模型只能将一个输入映射到一个输出，但现在我们实际上有两个输出，所以我们要构建一个模型，记住我们必须现在使用函数式API，所以我们将从Keras开始。
- en: input we're going to specify the input shape and in this case they are 64 by
    64 pixels and then they are one channel just because they are grayscale and then
    we're going to do a comm layer comm 2D and we're going to specify filters let's
    say 32 we're going specify the current。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 输入形状我们将指定为64乘64像素，并且它们只有一个通道，因为它们是灰度图像，然后我们将使用一个2D卷积层，假设过滤器数量为32，我们将指定当前值。
- en: Size let's say3 just for keeping it simple。 and then yeah。 I guess we can do
    padding equals same and then we're going to also do regularization。 So let's actually
    go to the top here and I also want to specify just some hyperparameter。 so let's
    do hyperparameters。And let's specify the batch size。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设大小为3，为了简单起见。然后是的。我想我们可以设置相同的填充，然后我们还要进行正则化。所以让我们回到顶部，我还想指定一些超参数。让我们来设置超参数。并且指定批处理大小。
- en: Let's do 64 and let's specify weight decay to be 0。001。 so for L2 normalization
    that we did two videos ago， I believe。 and then let's specify the learning rate
    0。001。Alright， so let's go back now to our model。 So we're going to do kernel
    regularizer。 I'm going to T cart。Actually， we can。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做64，然后指定权重衰减为0.001。关于我们两部视频前所做的L2归一化，我相信。然后我们指定学习率为0.001。好了，现在我们回到模型。我们将做卷积正则化器。我将T
    cart。实际上，我们可以。
- en: we can also import that。 So let's do layers。Input regulars。And then， go back。😔。And
    then regularizers dot L2， and then of weight decay。All right。And then we got a
    of inputs right there。Now let's do a batch layer。 So layers batch normalization
    of X。 So so far nothing is new， right， we've done all of this before。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以导入这一点。所以我们来做层。输入正则项。然后，返回。😔。然后正则化器点L2，再加上权重衰减。好的。然后我们得到一组输入。现在我们来做一个批量层。所以层批量归一化X。到目前为止，没什么新鲜事，对吧，我们之前都做过这些。
- en: And there's not that much different， but I just wanted to give you sort of a
    more in depth example where you would actually use this。 So the difference is
    going to be when when we get to the output。But anyways。 then let's you do kas
    that activation， thatt re。Of x， then let's do another comm to D。to the。Of。 I don't
    know， let's say。64，3， and then kernel regularizer equal regulars dot L2。Of weight
    decay。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 而且没什么不同，但我只是想给你一个更深入的示例，实际上你会使用这个。所以区别在于我们得到输出时。但无论如何。然后让我们做Keras的激活函数为relu，然后再做一个卷积到。嗯，我不知道，假设64，3，然后卷积正则化器等于正则化L2的权重衰减。
- en: And then let's also send in the input X on that。 and again。 we're going to do
    just a batch normalization， send in x， and then Kaos activations that relu。Of
    x。And let's do a max pooling right here。So。I don't know。 Let's do another comlay。Com
    to the。Let's just specify 64，3。Then let's just do relu。 So no bathroom here and
    then kernel regularizer。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们也将输入X传入。再一次。我们将做一个批量归一化，传入x，然后Keras激活为relu。对x做一个最大池化。嗯。我不知道。我们再做一个卷积层。卷积到。让我们指定64和3。然后做relu。所以这里没有池化，然后是卷积正则化器。
- en: Caras just regulars that L2 of weight。Okay。And then。Yeah， we can do one more
    so come to。To D 128 filters now， so let's double it，3 activation equals Relu，
    then just send in x。And then we can do one max max pooling and then。Let's flatten
    it。 and then let's now get to the output。So what we're going to do is we're going
    to do one dense layer。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Keras只是正则化L2的权重。好的。然后。是的，我们可以再做一层，所以到D 128的过滤器，现在我们将其加倍，激活为Relu，然后传入x。然后我们可以做一次最大池化，然后。让我们扁平化。现在我们来得到输出。我们要做的是做一个全连接层。
- en: so layer den， it's do 128 nodes。And let's set it activation equals value on
    that and sending next。And。Let's also add some regularization so let's do layers
    drop out 0。5 and then send in X。And then。Yeah， we can actually do one more layer
    then so let's do layer down 64 activation equals rather of x。 And now we get to
    the actual。 So now we get to the actual output。 So what we want to do now。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以层是128个节点。让我们设置激活函数为值，然后发送下一个。并且。我们还要添加一些正则化，所以我们来做层丢弃0.5，然后传入X。然后。是的，我们实际上可以再做一层，所以我们来做层64，激活等于X的值。现在我们得到实际的输出。我们现在要做的是什么。
- en: let's do output one and let's do a dense layer。You going do 10 nodes， and then。Let's
    call it。 let's give it a name。 So it's our first number。And then we're going to
    send in x， and then output 2。 we're going to do layer den of 10 name。Second number。
    and then we're gonna send in x。 So as you can see here we we're using the input
    X to map to two different outputs right so this would essentially give two different
    branches of this thing right here and this is when things like the functional
    API becomes useful right just these two lines right here because we so the sequential
    is nice。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来做输出1，并做一个全连接层。你将做10个节点，然后。我们给它一个名字。让它成为我们的第一个数字。然后我们将传入x，再做输出2。我们将做层10，名字为第二个数字。然后我们将传入x。所以如你所见，我们使用输入X映射到两个不同的输出，对吧，这实际上会给出两个不同的分支，这就是功能API变得有用的时候，对吧，就这两行，因为我们使用顺序模型是不错的。
- en: it's very simple， it's very compact but it can't do things like this and that's。That's
    this narrow when we actually have to use the functional and you could also combine
    them。 So for example you could， you could do this right here in in a sequential
    and then you could just do these two on the functional。 So you could you can also
    combine the two。 but anyways。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简单，非常紧凑，但它不能像这样做事情。这就是当我们实际上必须使用功能时，它是如此狭窄，你也可以将它们结合起来。例如，你可以在顺序模型中这样做，然后你可以在功能模型上做这两个。所以你也可以结合这两者。但无论如何。
- en: I'm just gonna so this is just for two outputs and then I'm gonna do model。![](img/2b04a13feb7a7ead68512f94b99b8884_4.png)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是想说这只是为了两个输出，然后我要做模型。![](img/2b04a13feb7a7ead68512f94b99b8884_4.png)
- en: '![](img/2b04a13feb7a7ead68512f94b99b8884_5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b04a13feb7a7ead68512f94b99b8884_5.png)'
- en: modelel was cares that model input equals input outputs is now a list and we're
    going to do output 1。 outputs 2。And then when we do model that。😔，Compile。We're
    going to set the optimizer。 let's do optimizers do Adam。And let's set learning
    rate here。And then we're going to specify the loss。 and then the loss is actually
    going to be two losses。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型关心的是模型输入等于输入输出现在是一个列表，我们将进行输出1。输出2。当我们执行模型时。😔，编译。我们将设置优化器。让我们使用Adam优化器。然后在这里设置学习率。接着我们将指定损失。而损失实际上将是两个损失。
- en: So we're going to do losses， sparse。 We're going to use sparse categorical。Pross
    entropy。 and then from large， it equals true。Or actually， let's remove that this
    time。 So let's do。We can specify here activation equals softftmax。Like that。 So
    that then we don't need to do from Loit。So activation softmax。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将进行损失，稀疏。我们将使用稀疏分类交叉熵。并且从large，它等于true。或者实际上，这次我们去掉它。这样做。我们可以在这里指定激活等于softmax。就这样。这样我们就不需要从Loit做了。所以激活是softmax。
- en: And then we're going to do one more loss function。And I think there's a way
    to make this more compact。 I haven't actually tried it， but I think that if you
    would just specify this。 that would be for both。 but you could try that out。 So
    the safe way is just to write them all up So we're going to use spars。Categorical
    cross entropy。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行一个损失函数。我想有一种方法可以让这个更紧凑。我实际上还没有尝试过，但我认为如果你只是指定这个，那将对两者都适用。但你可以试试看。所以安全的方式就是将它们全部写上。因此我们将使用稀疏的分类交叉熵。
- en: For both。And then we want to keep track of metrics equals。Accuracy。And now as
    usual we're going to do model do fit in this case。 we're just going to send in
    train data and batch size and everything is taking care inside this data loading
    part which I haven't covered again。 but we don't care too much about that then
    let's do epoch 5 both equals2 and then model dot evaluate。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两者。然后我们想跟踪指标等于。准确性。现在和往常一样，我们将在这种情况下执行模型训练。我们只需传入训练数据和批量大小，所有内容都在数据加载部分内部处理，而我还没有再次覆盖。但我们不太关心这个，然后让我们做5个epoch，两个都等于2，然后模型进行评估。
- en: Evaluate on the on the test data set。And then verbose equals2。And yeah。 so let's
    run this and let's see what kind of accuracy we get when we're training on this
    multi digigit Mist if we do not get the errors。 so we。Cars has no attribute。😔，Activation，
    so we' got to do activations， I believe。 so let's find that error， let's see。Activations。Go
    back。😔，There we go activations。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据集上进行评估。然后verbose等于2。是的。所以让我们运行这个，看看当我们在这个多位数的Mist上训练时，如果不出错我们会得到什么样的准确性。所以我们。Cars没有属性。😔，激活，所以我相信我们得做激活。让我们找出那个错误，看看。激活。回去。😔，来了激活。
- en: So activations right there。 And let's hope there's nowhere probably。There's
    going be more。All right。 so I've probably forgot to send it in right here。😔，Are
    there any more errors， let's see。I cannot find anymore， so let's run it again。Alright，
    outputs  one is outputs once。 Yeah。 we can do outputs。 So I call it output。Like
    that。Now， please work。 got them。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以激活就在这里。希望没有其他地方。会有更多。好的，所以我可能忘记在这里发送它。😔，还有其他错误吗，让我们看看。我找不到更多错误，所以我们再运行一次。好的，输出一次是输出一次。是的，我们可以做输出。所以我称之为输出。就这样。现在，请工作。得到了。
- en: No data provided for first。Number。Alright， so after a very long time， I think
    I found the error。 So the error was no data provided for first number。 knee data
    for each key in first number and second number。 Alright， and surprisingly。 I have
    no idea why， but if I just remove。This so that it says first nu and second nu。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 没有为第一个提供数据。数字。好的，所以经过很长时间，我想我找到了错误。错误是没有为第一个数字提供数据。每个键在第一个数字和第二个数字中都有数据。好的，令人惊讶的是。我不知道为什么，但如果我只是去掉。这样说第一和第二。
- en: then if I rerun it。It actually starts training without an error and。I have no
    idea， actually。 why that is。 If you know， then， please do comment becauseuse I'm
    very surprised by that。 Probably one of the weirdest errors I've had。After five
    entire epochs we have 96。3% on the training on both pretty much both of the digits
    and that's a little bit interesting to see that when it start to recognize one
    of the digits it also starts to recognize the other one so they improve pretty
    much on equal level and then so if you would train this for longer I would suspect
    that you would improve this quite a bit but then on the test set anyways。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果我重新运行它，它实际上开始训练而没有错误。实际上我不知道这是为什么。如果你知道，请评论，因为我对此非常惊讶。这可能是我遇到的最奇怪的错误之一。在五个完整的训练周期后，我们在训练集上对这两个数字几乎都达到了96.3%。有趣的是，当它开始识别其中一个数字时，它也开始识别另一个数字，因此它们几乎是同等水平地提高。如果你继续训练，我猜它们会有相当大的提升，但在测试集上又如何呢。
- en: we get about 90% but then on the second value would get 83 so I guess on the
    test set might be more difficult to recognize the second number but anyways one
    thing I also wanted to check is that what if we remove and we just have a single
    loss function it would be nice if this would then extend to both of them so let's
    try and see if that works and that does seem to work all right so that's it for
    this video and taking a deep。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了大约90%，但是在第二个值上只得到了83，所以我猜在测试集上可能更难识别第二个数字。不过，我还想检查的一个事情是，如果我们去掉，然后只用一个损失函数，那如果能扩展到两个函数就好了，所以我们来试试看，这似乎是有效的。好了，这就是本视频的内容，感谢你的观看。
- en: look at the Fun API with a more real example， thank you so much for watching
    the video and I hope to see you in the next one。![](img/2b04a13feb7a7ead68512f94b99b8884_7.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 看看Fun API的一个更实际的例子，非常感谢你观看这个视频，希望在下一个视频中再见到你。![](img/2b04a13feb7a7ead68512f94b99b8884_7.png)
