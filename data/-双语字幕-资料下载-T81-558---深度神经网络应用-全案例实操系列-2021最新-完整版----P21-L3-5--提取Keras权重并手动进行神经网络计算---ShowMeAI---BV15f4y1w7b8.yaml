- en: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P21：L3.5- 提取Keras权重并手动进行神经网络计算
    - ShowMeAI - BV15f4y1w7b8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 【双语字幕+资料下载】T81-558 ｜ 深度神经网络应用-全案例实操系列(2021最新·完整版) - P21：L3.5- 提取Keras权重并手动进行神经网络计算
    - ShowMeAI - BV15f4y1w7b8
- en: Hi， this is Jeffine。 Welcome to applications of deep neural networks with Washington
    University。 In this video， we're going to see how to actually extract the weights
    from a neural network created by Kes and put those weights into an equation so
    that we can actually calculate the output from the neural network and see that
    this there's really no magic to this process。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是Jeffine。欢迎来到华盛顿大学的深度神经网络应用。在这个视频中，我们将看到如何实际提取Kes创建的神经网络中的权重，并将这些权重放入方程中，以便我们可以实际计算神经网络的输出，并看到这个过程并没有什么魔法。
- en: 😊，That it is simply weights multiplied against the inputs that produces a final
    output for the latest on my AI course and projects。 click subscribe in the bell
    next to it to be notified of every new video。 This is code from Ks and Tensorflowlow，
    But this applies to really just about anything。 I am going to show you how to
    actually extract the weights from the neural network。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这仅仅是权重与输入相乘，产生我的AI课程和项目的最终输出。点击旁边的铃铛订阅，以便接收每个新视频的通知。这是Ks和Tensorflowlow的代码，但这实际上适用于几乎任何东西。我将向你展示如何实际提取神经网络中的权重。
- en: so that we can then put them onto a diagram and actually actually calculate
    it and come up with the same number that Tensorflowlow would have。😊。![](img/99c863804a8e2678a48ffb4d5482d3aa_1.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以将它们放入图表中，实际进行计算，得出与Tensorflowlow相同的数字。😊。![](img/99c863804a8e2678a48ffb4d5482d3aa_1.png)
- en: For this， we're going to use an exclusive or neural network， that's the XOR
    function。These are the inputs to it，0，0， the usual truth table for any sort of
    an and or an or。 This is the expected output。 The thing to remember with X O R
    is if the two inputs are the same。 it's going to be 0。If the two outputs are different，
    zero and 1 versus 10， it's going to be one。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个，我们将使用一个异或神经网络，即XOR函数。这是它的输入，0，0，任何与与或相关的真值表。预期的输出是。记住XOR的要点是，如果两个输入相同，输出就是0。如果两个输出不同，零和1与1和0相比，输出就是1。
- en: Here we set up a neural network。 It has two inputs and a hidden layer of two。
    We're going for the absolute smallest neural network just to show that you don't
    really need that much to calculate an excluvo。Also， since we are going to calculate
    this by hand or just being lazy。 we don't I don't want to give you a truly deep，
    deep neural network and then calculate it by hand。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们设置了一个神经网络。它有两个输入和一个隐藏层，层中有两个神经元。我们追求绝对最小的神经网络，只是为了表明你实际上并不需要太多就能计算一个异或。此外，由于我们将手动计算这个，或者说懒惰一些，我不想给你一个真正深、深的神经网络，然后再手动计算。
- en: that would not be fine。So we're going to optimize it with mean square error。
    and the final output is going to be one neural network， it's a regression neural
    network。 you can do XOR as classification or regression in this case， I'm doing
    it as regression。I basically train it here。 I am training it for100，000 epochs。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是不行的。所以我们将用均方误差来优化它，最终输出将是一个神经网络，这是一种回归神经网络。你可以将XOR用作分类或回归，在这种情况下，我将其作为回归来处理。我基本上在这里训练它。我训练了100,000个周期。
- en: It may take longer because there's so few weights in this that your initial
    random values or your weights are really going to have a lot of determination
    on the success of your training。 We could do this in way fewer than this many
    epochs。 This is basically grilling a cheese sandwich with a gen engine， but。I
    am really not trying to show you how to tune these things right now。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会花更长的时间，因为这里的权重很少，你的初始随机值或权重将对你的训练成功有很大影响。我们可以用远少于这个数量的周期来完成这项工作。这基本上就像用发电机烤奶酪三明治，但我现在并不想向你展示如何调整这些东西。
- en: I just want to get the weights set up so that they're good for a exclusive or
    network。 we're going to put those on a diagram and show you that basically you
    can then calculate the same output so you can see the weights behind what Kis
    gives you。I train it， I predict it， this is good for the neural network because。Each
    of these four correspond to these four up here in the input。 This is scientific
    notation。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是想设置权重，以便它们适合一个异或网络。我们将把这些放在图表上，向你展示基本上你可以计算相同的输出，这样你就可以看到Kis给你的权重。我训练它，我预测它，这对神经网络来说是好的，因为。这四个权重对应于输入中的这四个。这是科学记数法。
- en: So to the negative 4， that's 0。000。 That number。 So these two are effectively0。
    and these two in the middle are effectively one。 So this may look weird if you're
    not used to seeing numbers like this。 but this is a great output。 the neural network
    has trained quite well。So then what we're able to do is I write， I wrote this
    little program here， this dumps the weights。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以负4是0.000。那个数字。因此这两个实际上是0，而中间的这两个实际上是1。所以如果你不习惯看到这样的数字，这可能看起来很奇怪，但这是一个很好的输出，神经网络训练得相当好。然后我们能做的是，我写了这个小程序，它会转储权重。
- en: so you're seeing that the layer0 counting starting at zero。Weight from the bias
    to the next layer layer 1。 again， we're counting with 0。 neuron 1。 neuron neuron
    0， neuron 1。 These are the biases。And these are the actual weights。So there's
    not a lot of weights and biases in this neural network， it's a fairly small one。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你看到层0的计数是从零开始的。偏置到下一层1的权重。再一次，我们是从0开始计数。神经元1，神经元0，神经元1。这些是偏置。这些是真正的权重。因此，这个神经网络中没有很多权重和偏置，它相对较小。
- en: So I'm going to show draw a diagram here in just a minute。 and we're going to
    copy all of these values up onto the diagram。 Then we're going to calculate it
    by hand。 I give you the code down here to calculate it by hand as well。 if you
    want to do it， you set the input to 1 and 0， So1 in 0 and an X O R。 Those are
    different。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将在这里稍后画一个图，我们将把这些值复制到图上。然后我们将手动计算。如果你想这样做，我在这里给你代码，你把输入设置为1和0，So1在0和X O R。这是不同的。
- en: So it should be 1。 So it outputs a 0。96。 Now， since we're calculating this by
    hand。 I truncated quite a few of these numbers。 So 1。29 becomes 1。3， and we'll
    do that when we diagram it。 So we lose a little bit of accuracy， but。These are
    the two input neurons。We are going to feed this values 0 and  one。 So that's what
    we're calculating for。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它应该是1。它输出的是0.96。现在，由于我们是手动计算的，我截断了相当多的数字。所以1.29变成了1.3，当我们画图时会这样做。因此我们失去了一些准确性，但这两个输入神经元，我们将给它们输入值0和1。这就是我们要计算的。
- en: You could calculate it， really， for。Just about any other value that you wanted
    to。 But for the other four that you would have in the exclusive  four。 we're also
    going to have the bias neuron。 So like we saw in earlier class videos。 you're
    essentially calculating。A weighted sum over and over and over again for a neural
    network。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以真的计算它，任何你想要的其他值。但对于那四个独占的值，我们还将有偏置神经元。所以像我们在之前的课程视频中看到的，你实际上是在不断计算一个加权和，用于神经网络。
- en: So we're going to calculate the value for hidden neuron0。H0。Each zero has inputs
    from those previous three。These all have weights。 as we saw from the output from
    the program before， so I'm just going to copy those in。So this is going to become
    essentially a weighted sum that we're going to use to calculate what hidden 0
    is actually what its value actually is。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将计算隐藏神经元0的值。H0。每个零都有来自前面三个的输入。这些都有权重，正如我们之前程序输出所看到的，我将把它们复制过来。因此，这将成为我们用于计算隐藏0的值的加权和。
- en: So to do that， we're going to multiply the input 0 times its weight， which is
    1。3。我是。1 times 1。3。That one is from the first， which is actually the second input
    neuron。 And then we have to add the bias。 and we basically perform that。Calculation，
    which ends up being 0。 So the value for hidden in hidden neuron 0 is 0。 Next，
    we're going to calculate the value for。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以要做到这一点，我们将输入0乘以它的权重，即1.3。我是1乘以1.3。那个是来自第一个，实际上是第二个输入神经元。然后我们必须加上偏置。我们基本上执行这个计算，结果是0。因此隐藏神经元0的值是0。接下来，我们将计算。
- en: Hid neuroron 1， which is the second one。 It has a similar sort of thing going
    on。 It has three weights coming into it。Is going to copy those values from the
    output that we had from the Kiras program。So that is simply。1。2， we are calculating
    basically the weighted sum again。The first input I0 drops out， second one contributes
    that one point。1。2 to it。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏神经元1，即第二个神经元。它也有类似的情况。它有三个权重进入。将从Keras程序的输出中复制这些值。因此这就是简单的1.2，我们基本上再次计算加权和。第一个输入I0消失，第二个输入贡献了1.1.2。
- en: and then the intercept or the bias for。For this， for the second hidden neuron
    is zero。This whole thing， together。Is basically equal to 1。2。 Then this value。
    we have to pass it through the through the activation function。 Both of these
    needed to go through the activation function。 That is the rectified linear unit。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个隐藏神经元的截距或偏置为零。这整个内容，加起来，基本上等于 1。2。然后这个值，我们需要通过激活函数。两个都需要经过激活函数。那就是修正线性单元。
- en: So that is basically going to be the max。Of 0 and 1。2。Which is also 1。2。So the
    output from this hidden neuron here is 1。2。Same sort of deal up here to apply
    its activation function。 It's going to be maxed because it's also the value。 the
    max 0，0 is 0。activation functions that you can use， I may give you a different
    one for for the midterm examination on this one。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这将是最大值。0 和 1。2。也就是 1。2。所以这里的隐藏神经元的输出是 1。2。上面的处理方式类似，用它的激活函数。它将被最大化，因为它也是这个值。最大值
    0，0 是 0。可以使用的激活函数，我可能会在期中考试中给你一个不同的。
- en: The only other ones that you will ever see， at least in this class， are sigmoid。Pollic
    tangent。There are others， but these are for regression。If you're doing a classification。
    you might also see the softm but。Focus primarily on these。Now the output。 the
    final layer so that we actually get what this neural network is providing for
    us。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这门课上你会见到的其他激活函数只有 sigmoid 和双曲正切。还有其他的，但这些是用于回归的。如果你在做分类，你可能还会看到 softmax。但主要关注这些。现在输出是最终层，这样我们才能实际获得这个神经网络为我们提供的内容。
- en: This is going to have similar to before。 We do have another bias connection。
    So you're going to have essentially three and bound connections again。And we're
    essentially going to just copy the weights that we had from the output in Caras。There
    is 0。And this results in the final equation that ties this whole thing together。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这将与之前类似。我们确实有另一个偏置连接。所以你将基本上再次拥有三条有界连接。我们基本上会复制在 Caras 中的输出权重。这里是 0。最终的方程将整个内容联系在一起。
- en: And that is going to basically be。Is the output from from hidden hidden 0。Times
    1。6。Plus， 1。2。Times 0。8。You can see the 1。2 times the 0。8 and then plus the plus
    the。The bias or the intercept。 which is zero。And then this whole thing equals
    0。96。That's your final output that is approximately equal to 1。0。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是从隐藏神经元 0 输出的内容。乘以 1。6，加上 1。2 乘以 0。8。你可以看到 1。2 乘以 0。8，然后加上偏置或截距，这里是零。然后整个内容等于
    0。96。这是你的最终输出，约等于 1。0。
- en: which is what you would have expected from a exclusive or。Of。Zero and what。There
    you have it。 That is the calculation for this neural network done using the weights
    that Kiro is actually trained for us。 So we can see that there is really no magic
    there。😊，This simple of a neural network。 you could have actually just hand coded
    these weights that would have not been difficult。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你从异或运算中预期的结果。零和什么。就是这样。这是使用 Kiro 为我们训练的权重计算出来的神经网络。所以我们可以看到，这里真的没有什么魔法。😊
    这么简单的神经网络，你实际上可以手动编码这些权重，难度不大。
- en: I'll probably do a video on that as well so that you can see how to actually
    hand code one of these。 You do need about this size of neural network。 You definitely
    need the hidden layer。![](img/99c863804a8e2678a48ffb4d5482d3aa_3.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能还会做一个视频，这样你可以看到如何实际手动编写其中一个。你确实需要这个大小的神经网络。你绝对需要隐藏层。![](img/99c863804a8e2678a48ffb4d5482d3aa_3.png)
- en: Thank you for watching this video。 In the next video。 we're going to see more
    advanced training techniques that we can use with a neural network to。😊。To get
    better results and to measure or error in a variety of different ways。 this content
    changes often。 so subscribe to the channel to stay up to date on this course and
    other topics in artificial intelligence。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢观看这个视频。在下一个视频中，我们将看到可以与神经网络一起使用的更高级的训练技术。😊 以获得更好的结果，并以各种不同的方式测量我们的错误。内容经常变化，所以请订阅频道，以便随时了解本课程和其他人工智能主题。
