- en: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL15-
    LDA - ShowMeAI - BV1wS4y1f7z1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ã€åŒè¯­å­—å¹•+èµ„æ–™ä¸‹è½½ã€‘ç”¨ Python å’Œ Numpy å®ç°æœ€çƒ­é—¨çš„12ä¸ªæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå½»åº•ææ¸…æ¥šå®ƒä»¬çš„å·¥ä½œåŸç†ï¼ï¼œå®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼ - P15ï¼šL15-
    LDA - ShowMeAI - BV1wS4y1f7z1
- en: Heyï¼Œ guysï¼Œ welcome to a new machine learning from Sc tutorialã€‚ Todayã€‚ we are
    going to implement the linearar discriminant analysis algorithm or Sha LD using
    only Python and numpy LDA is a dimensionality reduction technique and a popular
    preprocessing step in machine learning pipelinesã€‚
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: å˜¿ï¼Œå¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°æ–°çš„æœºå™¨å­¦ä¹ Scæ•™ç¨‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Pythonå’ŒNumpyå®ç°çº¿æ€§åˆ¤åˆ«åˆ†æç®—æ³•æˆ–LDAã€‚LDAæ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼Œæ˜¯æœºå™¨å­¦ä¹ ç®¡é“ä¸­ä¸€ä¸ªæµè¡Œçš„é¢„å¤„ç†æ­¥éª¤ã€‚
- en: LDA is similar to the PCA technique that I already showed in a previous tutorialã€‚
    The approach and the implementation of PCA and LDA have a lot in commonã€‚ So I
    highly recommend that you watch this video firstã€‚ And now let's talk quickly about
    the concept of LDA before we jump to the codeã€‚ So the goalã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: LDAç±»ä¼¼äºæˆ‘åœ¨ä¹‹å‰çš„æ•™ç¨‹ä¸­å±•ç¤ºçš„PCAæŠ€æœ¯ã€‚PCAå’ŒLDAçš„æ–¹å¼å’Œå®ç°æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚å› æ­¤ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ å…ˆè§‚çœ‹è¿™ä¸ªè§†é¢‘ã€‚ç°åœ¨åœ¨æˆ‘ä»¬è·³åˆ°ä»£ç ä¹‹å‰ï¼Œå…ˆå¿«é€Ÿè°ˆä¸€ä¸‹LDAçš„æ¦‚å¿µã€‚ç›®æ ‡å°±æ˜¯ã€‚
- en: as I already saidï¼Œ is feature reductionã€‚ So we want to project our data sets
    onto a lower dimensional space and find a good class separationã€‚ğŸ˜Šï¼ŒSo here we have
    the difference between PCA and LEAï¼Œ so in PCA or principal component analysisã€‚
    we want to find new a onto which we project our data such that we maximize the
    variance on the new axisã€‚And now in L Aï¼Œ the big difference is that we know the
    feature labelsã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘å·²ç»è¯´è¿‡çš„ï¼Œè¿™æ˜¯ç‰¹å¾å‡å°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ•°æ®é›†æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ï¼Œå¹¶æ‰¾åˆ°è‰¯å¥½çš„ç±»é—´åˆ†ç¦»ã€‚ğŸ˜Šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰PCAå’ŒLDAä¹‹é—´çš„åŒºåˆ«ï¼Œåœ¨PCAæˆ–ä¸»æˆåˆ†åˆ†æä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾åˆ°ä¸€ä¸ªæ–°çš„è½´ï¼Œä»¥ä¾¿æœ€å¤§åŒ–åœ¨æ–°è½´ä¸Šçš„æ–¹å·®ã€‚è€Œåœ¨LDAä¸­ï¼Œæœ€å¤§åŒºåˆ«æ˜¯æˆ‘ä»¬çŸ¥é“ç‰¹å¾æ ‡ç­¾ã€‚
- en: So this is a supervised techniqueã€‚And here we want to find new axis such such
    that the class separation is maximizedã€‚ So if you have a look at this image hereï¼Œ
    we have two different classesã€‚ and then we could project our data either onto
    the Y axis or onto the X axisã€‚ And now in this caseã€‚ the Y axis would not be a
    good choiceï¼Œ but the X axis is a good choice because here we still have a good
    class separationã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§ç›‘ç£æŠ€æœ¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°æ–°çš„è½´ï¼Œä½¿å¾—ç±»é—´åˆ†ç¦»æœ€å¤§åŒ–ã€‚å¦‚æœä½ çœ‹çœ‹è¿™é‡Œçš„å›¾ç‰‡ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªä¸åŒçš„ç±»ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†æ•°æ®æŠ•å½±åˆ°Yè½´æˆ–Xè½´ä¸Šã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒYè½´ä¸æ˜¯ä¸€ä¸ªå¥½çš„é€‰æ‹©ï¼Œä½†Xè½´æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå› ä¸ºåœ¨è¿™é‡Œæˆ‘ä»¬ä»ç„¶æœ‰è‰¯å¥½çš„ç±»é—´åˆ†ç¦»ã€‚
- en: So this is the concept of the LDAã€‚And here I listed the differences again between
    PCA and LDAã€‚ So in PCA againï¼Œ we want to find the component axis that maximize
    the variance of our dataã€‚ and in LDAï¼Œ we want to do this twoã€‚ So within one class
    within the green field and within the blue fieldã€‚ we still want to have a good
    variance between the single featuresã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯LDAçš„æ¦‚å¿µã€‚æˆ‘åœ¨è¿™é‡Œå†æ¬¡åˆ—å‡ºäº†PCAå’ŒLDAä¹‹é—´çš„å·®å¼‚ã€‚å› æ­¤ï¼Œåœ¨PCAä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾åˆ°æœ€å¤§åŒ–æ•°æ®æ–¹å·®çš„æˆåˆ†è½´ã€‚è€Œåœ¨LDAä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæƒ³è¿™æ ·åšã€‚åœ¨ç»¿è‰²åŒºåŸŸå’Œè“è‰²åŒºåŸŸå†…ï¼Œæˆ‘ä»¬ä»ç„¶å¸Œæœ›å•ä¸ªç‰¹å¾ä¹‹é—´æœ‰è‰¯å¥½çš„æ–¹å·®ã€‚
- en: but also additionally here we are interested in the axis that maximize the separation
    between multiple classesã€‚ So this difference here basically should be maximized
    in the new axisã€‚And yeahã€‚ LDA is supervised learningã€‚ So we know our labels and
    PCA is unsupervisedã€‚ So this is an important thing that we should rememberã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æœ€å¤§åŒ–å¤šä¸ªç±»ä¹‹é—´åˆ†ç¦»çš„è½´æ„Ÿå…´è¶£ã€‚å› æ­¤ï¼Œè¿™é‡ŒåŸºæœ¬ä¸Šåº”è¯¥åœ¨æ–°çš„è½´ä¸Šæœ€å¤§åŒ–è¿™ä¸ªå·®å¼‚ã€‚æ˜¯çš„ï¼ŒLDAæ˜¯ç›‘ç£å­¦ä¹ ã€‚æ‰€ä»¥æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„æ ‡ç­¾ï¼Œè€ŒPCAæ˜¯æ— ç›‘ç£çš„ã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„äº‹æƒ…ï¼Œæˆ‘ä»¬åº”è¯¥è®°ä½ã€‚
- en: And now let's jump to the math here we have the socalled scatter matrixã€‚ and
    we have two different scatter mattressesï¼Œ the within class scatter and the between
    class scatterã€‚ this basically represents what I was talking about hereã€‚ So the
    within class scatter makes sure that our features within one class are good separatedã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è·³åˆ°æ•°å­¦éƒ¨åˆ†ï¼Œè¿™é‡Œæˆ‘ä»¬æœ‰æ‰€è°“çš„æ•£å¸ƒçŸ©é˜µã€‚æˆ‘ä»¬æœ‰ä¸¤ä¸ªä¸åŒçš„æ•£å¸ƒçŸ©é˜µï¼Œç±»å†…æ•£å¸ƒå’Œç±»é—´æ•£å¸ƒã€‚è¿™åŸºæœ¬ä¸Šä»£è¡¨äº†æˆ‘åœ¨è¿™é‡Œæ‰€è°ˆè®ºçš„å†…å®¹ã€‚å› æ­¤ï¼Œç±»å†…æ•£å¸ƒç¡®ä¿æˆ‘ä»¬åœ¨ä¸€ä¸ªç±»å†…çš„ç‰¹å¾è¢«å¾ˆå¥½åœ°åˆ†å¼€ã€‚
- en: and the between class scatter makes sure that the two or all the class are good
    separatedã€‚ And if we translate this to the math and we have to deal with the mean
    values and the variances So the formula of the within class scatter is the sum
    over the scatters and each scatter of one class is the sum over and then the feature
    value minus the mean value of all the featureã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œç±»é—´æ•£å¸ƒç¡®ä¿ä¸¤ä¸ªæˆ–æ‰€æœ‰ç±»è¢«å¾ˆå¥½åœ°åˆ†å¼€ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶è½¬åŒ–ä¸ºæ•°å­¦ï¼Œæˆ‘ä»¬å¿…é¡»å¤„ç†å‡å€¼å’Œæ–¹å·®ã€‚å› æ­¤ï¼Œç±»å†…æ•£å¸ƒçš„å…¬å¼æ˜¯å¯¹æ•£å¸ƒçš„æ€»å’Œï¼Œæ¯ä¸ªç±»çš„æ•£å¸ƒæ˜¯ç‰¹å¾å€¼å‡å»æ‰€æœ‰ç‰¹å¾çš„å‡å€¼çš„æ€»å’Œã€‚
- en: Only in this class and then times the same transposedã€‚ and then we sum over
    all the features in this classã€‚ So this is basically the same as in the PC algorithm
    where we want to compute the covariance matrixã€‚ So this is almost the same formula
    is for the covariance matrixã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…åœ¨è¿™ä¸ªç±»ä¸­ï¼Œç„¶åä¹˜ä»¥ç›¸åŒçš„è½¬ç½®ã€‚ç„¶åæˆ‘ä»¬å¯¹è¯¥ç±»ä¸­çš„æ‰€æœ‰ç‰¹å¾æ±‚å’Œã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šä¸æˆ‘ä»¬æƒ³è¦è®¡ç®—åæ–¹å·®çŸ©é˜µçš„ PCA ç®—æ³•æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œè¿™å‡ ä¹æ˜¯åæ–¹å·®çŸ©é˜µçš„ç›¸åŒå…¬å¼ã€‚
- en: except that we don't have the scaling at the beginningã€‚ So this is the within
    class getterã€‚ and then the between class getter hereï¼Œ the formula is the sum over
    all the classã€‚ and then for each classï¼Œ we have the number of features in this
    class or sorryã€‚ the number of labels in this classã€‚ and then times the mean valueã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åªæ˜¯åœ¨å¼€å§‹æ—¶æˆ‘ä»¬æ²¡æœ‰ç¼©æ”¾ã€‚å› æ­¤ï¼Œè¿™æ˜¯ç±»å†…æ•£å¸ƒå™¨ã€‚ç„¶åæ˜¯ç±»é—´æ•£å¸ƒå™¨ï¼Œè¿™é‡Œçš„å…¬å¼æ˜¯å¯¹æ‰€æœ‰ç±»æ±‚å’Œã€‚ç„¶åå¯¹äºæ¯ä¸ªç±»ï¼Œæˆ‘ä»¬æœ‰è¯¥ç±»ä¸­çš„ç‰¹å¾æ•°é‡ï¼Œå“¦ï¼Œä¸ï¼Œåº”è¯¥æ˜¯è¯¥ç±»ä¸­çš„æ ‡ç­¾æ•°é‡ï¼Œç„¶åä¹˜ä»¥å‡å€¼ã€‚
- en: So this x bar is the mean the mean value of the features in this class minus
    the mean value in totalã€‚ So the totalã€‚Of all featuresã€‚ and then times the same
    transposedã€‚ So these are the two matrices that we have to computeã€‚ and then we
    calculate the inverse of the within class scatterter and multiply that with the
    between class scatterterã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ª x bar æ˜¯è¯¥ç±»ç‰¹å¾çš„å‡å€¼å‡å»æ‰€æœ‰ç‰¹å¾çš„å‡å€¼ã€‚æ‰€ä»¥æ€»çš„ã€‚ç„¶åä¹˜ä»¥ç›¸åŒçš„è½¬ç½®ã€‚æ‰€ä»¥è¿™ä¸¤è€…æ˜¯æˆ‘ä»¬å¿…é¡»è®¡ç®—çš„çŸ©é˜µã€‚ç„¶åæˆ‘ä»¬è®¡ç®—ç±»å†…æ•£å¸ƒå™¨çš„é€†ï¼Œå¹¶å°†å…¶ä¸ç±»é—´æ•£å¸ƒå™¨ç›¸ä¹˜ã€‚
- en: And this is our eigenvalue and eigenvector problem that we have to solveã€‚ So
    this is the same as in the PCã€‚ I will not go into detail againã€‚ So please make
    sure thatã€‚ you know what eigenvalues and eigenvectors areã€‚ So basicallyã€‚ what
    we have to do then is for this formulaã€‚ We have to calculate the eigenvaluesã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¿…é¡»è§£å†³çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡é—®é¢˜ã€‚æ‰€ä»¥è¿™ä¸ PCA ä¸­çš„æƒ…å†µç›¸åŒã€‚æˆ‘ä¸ä¼šå†è¯¦ç»†è¯´æ˜äº†ã€‚å› æ­¤ï¼Œè¯·ç¡®ä¿ä½ çŸ¥é“ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ˜¯ä»€ä¹ˆã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å¿…é¡»åšçš„æ˜¯è®¡ç®—è¿™äº›å…¬å¼çš„ç‰¹å¾å€¼ã€‚
- en: And then so let's have a look at the whole approach again hereã€‚ So here I summarize
    itã€‚ So first we want to calculate the between class scatterter and the within
    class scatterterã€‚ then here we calculate the inverse of the within class scatter
    and multiply it with the between class scatterã€‚ Then of this we calculate the
    eigenvectors and eigenvaluesã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®©æˆ‘ä»¬å†æ¬¡çœ‹çœ‹æ•´ä¸ªæ–¹æ³•ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘å¯¹å®ƒè¿›è¡Œæ€»ç»“ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è¦è®¡ç®—ç±»é—´æ•£å¸ƒå™¨å’Œç±»å†…æ•£å¸ƒå™¨ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—ç±»å†…æ•£å¸ƒçš„é€†ï¼Œå¹¶å°†å…¶ä¸ç±»é—´æ•£å¸ƒç›¸ä¹˜ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ã€‚
- en: then we sort the eigenvectors according to their eigenvalues in decreasing order
    and then we choose only the first k eigenvectors that we specifiedã€‚ So only the
    k dimensions that we want to keep and these eigenvectors are called the linear
    discriminants that's why it has this name and then we transform our original data
    points onto this k dimensions and this transformation is basically just a projectã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æ ¹æ®ç‰¹å¾å€¼æŒ‰é™åºæ’åºç‰¹å¾å‘é‡ï¼Œç„¶åæˆ‘ä»¬åªé€‰æ‹©æˆ‘ä»¬æŒ‡å®šçš„å‰ k ä¸ªç‰¹å¾å‘é‡ã€‚æ‰€ä»¥ä»…ä¿ç•™æˆ‘ä»¬æƒ³è¦çš„ k ä¸ªç»´åº¦ï¼Œè¿™äº›ç‰¹å¾å‘é‡è¢«ç§°ä¸ºçº¿æ€§åˆ¤åˆ«ï¼Œå› æ­¤å®ƒæ‰æœ‰è¿™ä¸ªåç§°ã€‚æ¥ç€æˆ‘ä»¬å°†åŸå§‹æ•°æ®ç‚¹è½¬æ¢åˆ°è¿™
    k ä¸ªç»´åº¦ï¼Œè¿™ç§è½¬æ¢åŸºæœ¬ä¸Šå°±æ˜¯ä¸€ä¸ªæŠ•å½±ã€‚
- en: With the dot productsã€‚ So this whole approach is the same as in the PC A algorithmã€‚
    except that we have to solve the eigenvalueian eigenvector problem for a different
    formula in the beginningã€‚ So that's the approachã€‚ And now let's jump to the codeã€‚![](img/c6d1044d6beca5a8660b61779287752d_1.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç‚¹ç§¯ã€‚å› æ­¤ï¼Œè¿™æ•´ä¸ªæ–¹æ³•ä¸ PCA ç®—æ³•ä¸­çš„æ–¹æ³•æ˜¯ç›¸åŒçš„ï¼Œåªæ˜¯æˆ‘ä»¬å¿…é¡»ä¸ºä¸åŒçš„å…¬å¼è§£å†³ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡é—®é¢˜ã€‚æ‰€ä»¥è¿™å°±æ˜¯æ–¹æ³•ã€‚ç°åœ¨è®©æˆ‘ä»¬è·³åˆ°ä»£ç éƒ¨åˆ†ã€‚![](img/c6d1044d6beca5a8660b61779287752d_1.png)
- en: Soï¼Œ of courseï¼Œ we import nuy S N Pã€‚ and then we define our classã€‚ And let's
    call this LD Aã€‚ And here we define our in itï¼Œ which has selfã€‚ And it also gets
    the number of components that we want to keepã€‚ And here we simply store itã€‚ So
    we say self andã€‚Components equals n componentsã€‚ And we also create a variable
    that we call self dot linear thisã€‚Preriminenceã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬å¯¼å…¥ nuy S N Pã€‚ç„¶åæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ç±»ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º LD Aã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„åˆå§‹åŒ–ï¼Œå®ƒæœ‰ selfã€‚å¹¶ä¸”å®ƒè¿˜è·å–æˆ‘ä»¬æƒ³è¦ä¿ç•™çš„ç»„ä»¶æ•°é‡ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°å­˜å‚¨å®ƒã€‚æ‰€ä»¥æˆ‘ä»¬è¯´
    self å’Œã€‚Components ç­‰äº n componentsã€‚æˆ‘ä»¬è¿˜åˆ›å»ºä¸€ä¸ªå˜é‡ï¼Œç§°ä¸º self dot linear thisã€‚Preriminenceã€‚
- en: and this is none in the beginningã€‚ And here we want to store the eigenvectors
    that we computeã€‚And then we define our fit methodã€‚ So here we have selfï¼Œ and then
    we have Xï¼Œ and we also have yã€‚ because rememberï¼Œ this is a supervised techniqueã€‚And
    then we also implement not the predict methodã€‚ but we call it transformã€‚ So transformã€‚This
    is the same as in the PCAã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€å¼€å§‹æ˜¯ noneã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³å­˜å‚¨æˆ‘ä»¬è®¡ç®—çš„ç‰¹å¾å‘é‡ã€‚ç„¶åæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ fit æ–¹æ³•ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ selfï¼Œè¿˜æœ‰ Xï¼Œè¿˜æœ‰ yã€‚å› ä¸ºè®°ä½ï¼Œè¿™æ˜¯ä¸€ä¸ªç›‘ç£æŠ€æœ¯ã€‚æ¥ç€æˆ‘ä»¬è¿˜å®ç°äº†ä¸€ä¸ªä¸æ˜¯
    predict çš„æ–¹æ³•ï¼Œè€Œæ˜¯ç§°ä¹‹ä¸º transformã€‚å˜æ¢ä¸ PCA ä¸­æ˜¯ç›¸åŒçš„ã€‚
- en: And here we want to get the new features that we want to projectã€‚So let's implement
    the fit methodã€‚ So hereï¼Œ firstï¼Œ what we want to get is the number of featuresã€‚
    and we get this by saying x dot shape and then the index1ã€‚ So index0 is the number
    of samplesã€‚ And here we only want to have the number of featuresã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³è·å¾—æƒ³è¦æŠ•å½±çš„æ–°ç‰¹å¾ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å®ç° fit æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è¦è·å–ç‰¹å¾çš„æ•°é‡ã€‚æˆ‘ä»¬é€šè¿‡è¯´ x dot shape æ¥è·å–è¿™ä¸ªå€¼ï¼Œç„¶åæ˜¯
    index1ã€‚æ‰€ä»¥ index0 æ˜¯æ ·æœ¬çš„æ•°é‡ï¼Œè€Œæˆ‘ä»¬åªæƒ³å¾—åˆ°ç‰¹å¾çš„æ•°é‡ã€‚
- en: Then we also want to get all the different class labelsã€‚ So let's call this
    class labelsã€‚ and this is equal to nuyï¼Œ and then we can apply the unique function
    of yã€‚ So this will only only return the unique values in our labels as a listã€‚
    And now we want to calculate the two scatter mattressesã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è¿˜æƒ³è·å¾—æ‰€æœ‰ä¸åŒçš„ç±»åˆ«æ ‡ç­¾ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºç±»æ ‡ç­¾ï¼Œè¿™ç­‰äº nuyï¼Œç„¶åæˆ‘ä»¬å¯ä»¥åº”ç”¨ y çš„å”¯ä¸€å‡½æ•°ã€‚å› æ­¤ï¼Œè¿™å°†ä»…è¿”å›æˆ‘ä»¬æ ‡ç­¾ä¸­çš„å”¯ä¸€å€¼åˆ—è¡¨ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è®¡ç®—ä¸¤ä¸ªæ•£å¸ƒçŸ©é˜µã€‚
- en: So S underscore W for the within class gather and S underscore B for the between
    classã€‚ So let's do thisã€‚ And first of allï¼Œ I want to calculate the mean of all
    our samples because we need thisã€‚for one of the formulasï¼Œ we say meanã€‚Overallï¼Œ
    equals numpy dot mean of xã€‚ And then along the axis 0ã€‚ And then let's initialize
    our two mattressesã€‚ So we say S W or S underscore W equals nuy zerosã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ S underscore W æ˜¯ç±»å†…çš„èšåˆï¼ŒS underscore B æ˜¯ç±»é—´çš„èšåˆã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚é¦–å…ˆï¼Œæˆ‘æƒ³è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å‡å€¼ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦è¿™ä¸ªå€¼ã€‚å¯¹äºä¸€ä¸ªå…¬å¼ï¼Œæˆ‘ä»¬è¯´å‡å€¼ã€‚æ€»ä½“å‡å€¼ç­‰äº
    numpy dot mean of xï¼Œç„¶åæ²¿ç€è½´ 0ã€‚æ¥ç€è®©æˆ‘ä»¬åˆå§‹åŒ–ä¸¤ä¸ªçŸ©é˜µã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ S W æˆ– S underscore W ç­‰äº nuy zerosã€‚
- en: So we want to fill this with zerosã€‚ And we want to give this a size of the number
    of features times the number of featuresã€‚And the same thing with the between class
    scatterã€‚ So we initialized this with zerosã€‚ So laterã€‚ we want to test thisï¼Œ for
    exampleï¼Œ with the features of the iris data setã€‚ So this has a size ofã€‚ this hasï¼Œ
    I think it's 150 samples and four featuresã€‚ So this has size 4 times 4ã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›ç”¨é›¶å¡«å……è¿™ä¸ªï¼Œå¹¶å¸Œæœ›å°†å…¶å¤§å°è®¾ç½®ä¸ºç‰¹å¾æ•°é‡ä¹˜ä»¥ç‰¹å¾æ•°é‡ã€‚ç±»é—´æ•£å¸ƒä¹Ÿæ˜¯åŒæ ·çš„å¤„ç†ã€‚æ‰€ä»¥æˆ‘ä»¬ç”¨é›¶åˆå§‹åŒ–è¿™ä¸ªã€‚ç¨åï¼Œæˆ‘ä»¬æƒ³ç”¨ä¾‹å¦‚é¸¢å°¾èŠ±æ•°æ®é›†çš„ç‰¹å¾æ¥æµ‹è¯•è¿™ä¸ªã€‚æ‰€ä»¥è¿™ä¸ªå¤§å°æ˜¯ã€‚æˆ‘è®¤ä¸ºæ˜¯
    150 ä¸ªæ ·æœ¬å’Œå››ä¸ªç‰¹å¾ã€‚æ‰€ä»¥è¿™ä¸ªå¤§å°æ˜¯ 4 ä¹˜ä»¥ 4ã€‚
- en: And this is the same  four times 4ã€‚![](img/c6d1044d6beca5a8660b61779287752d_3.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨å››æ¬¡å››çš„æƒ…å†µä¸‹æ˜¯ç›¸åŒçš„ã€‚![](img/c6d1044d6beca5a8660b61779287752d_3.png)
- en: And now we have to apply the two formulasã€‚ So we have to sum over all the classes
    and then apply these two formulas So we can do this in one for loopã€‚![](img/c6d1044d6beca5a8660b61779287752d_5.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦åº”ç”¨è¿™ä¸¤ä¸ªå…¬å¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ‰€æœ‰ç±»è¿›è¡Œæ±‚å’Œï¼Œç„¶ååº”ç”¨è¿™ä¸¤ä¸ªå…¬å¼ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨ä¸€ä¸ªå¾ªç¯ä¸­å®Œæˆã€‚![](img/c6d1044d6beca5a8660b61779287752d_5.png)
- en: So we say for C in class labels that we computedã€‚ And then what we want to get
    first is we want to get only the samples of this classã€‚ So we say Xï¼Œ C equals
    Xï¼Œ where Y equals equals Cã€‚ So where we have this label in the current iterationã€‚And
    then we want to get the mean from these featuresã€‚ mean C equalsã€‚ And this is Ny
    dot mean of X C along x is 0ã€‚ So the same as we are doing it hereã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´å¯¹äºæˆ‘ä»¬è®¡ç®—çš„ç±»æ ‡ç­¾ä¸­çš„ Cã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è¦è·å–è¯¥ç±»çš„æ ·æœ¬ã€‚å› æ­¤æˆ‘ä»¬è¯´ Xï¼ŒC ç­‰äº Xï¼Œå…¶ä¸­ Y ç­‰äº Cã€‚åœ¨å½“å‰è¿­ä»£ä¸­æˆ‘ä»¬æœ‰è¿™ä¸ªæ ‡ç­¾ã€‚ç„¶åæˆ‘ä»¬æƒ³ä»è¿™äº›ç‰¹å¾ä¸­è·å–å‡å€¼ã€‚mean
    C ç­‰äºã€‚è¿™æ˜¯ Ny dot mean of X C æ²¿ç€ x ä¸º 0ã€‚æ‰€ä»¥ä¸æˆ‘ä»¬åœ¨è¿™é‡Œåšçš„ç›¸åŒã€‚
- en: but only for the features in this classã€‚And then let's have a look at thereï¼Œ
    within classã€‚![](img/c6d1044d6beca5a8660b61779287752d_7.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä»…é’ˆå¯¹è¿™ä¸€ç±»ä¸­çš„ç‰¹å¾ã€‚ç„¶åæˆ‘ä»¬æ¥çœ‹çœ‹ç±»å†…çš„æƒ…å†µã€‚![](img/c6d1044d6beca5a8660b61779287752d_7.png)
- en: Forulaã€‚So hereï¼Œ here we have our feature and then subtract the mean valueã€‚ And
    then this is basically the dot product times the transposedã€‚![](img/c6d1044d6beca5a8660b61779287752d_9.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å…¬å¼ã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ç‰¹å¾ï¼Œç„¶åå‡å»å‡å€¼ã€‚åŸºæœ¬ä¸Šè¿™æ˜¯ç‚¹ç§¯ä¹˜ä»¥è½¬ç½®ã€‚![](img/c6d1044d6beca5a8660b61779287752d_9.png)
- en: And so let's do thisã€‚ So here we say our S within plus equalsã€‚ because here
    we sum over all the classesã€‚ So plus equalsã€‚ And then here we say x Cã€‚![](img/c6d1044d6beca5a8660b61779287752d_11.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬æ¥åšè¿™ä¸ªã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯´æˆ‘ä»¬çš„ S within åŠ ä¸Šç­‰äºã€‚å› ä¸ºåœ¨è¿™é‡Œæˆ‘ä»¬å¯¹æ‰€æœ‰ç±»æ±‚å’Œã€‚æ‰€ä»¥åŠ ä¸Šç­‰äºã€‚ç„¶åæˆ‘ä»¬è¯´ x Cã€‚![](img/c6d1044d6beca5a8660b61779287752d_11.png)
- en: '![](img/c6d1044d6beca5a8660b61779287752d_12.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6d1044d6beca5a8660b61779287752d_12.png)'
- en: Minus mean Cã€‚ And then I transpo this and calculate the dot product times the
    same as we are doing it hereã€‚Soã€‚Here we have to be carefulã€‚ So if we have a look
    at the formula again and we see that I have to transpo term at the endã€‚ and here
    I transpose the first termã€‚ And this is because here we are having one more sumã€‚
    So we do this for all the samples in this classã€‚ And here we do this sum in one
    operation with the dot productã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å»å‡å€¼ Cã€‚ç„¶åæˆ‘è½¬ç½®è¿™ä¸ªå¹¶è®¡ç®—ç‚¹ç§¯ï¼Œå’Œæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„ä¸€æ ·ã€‚æ‰€ä»¥ã€‚è¿™é‡Œæˆ‘ä»¬éœ€è¦å°å¿ƒã€‚å¦‚æœæˆ‘ä»¬å†çœ‹ä¸€ä¸‹å…¬å¼ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æˆ‘å¿…é¡»åœ¨æœ€åè½¬ç½®é¡¹ã€‚è¿™é‡Œæˆ‘è½¬ç½®äº†ç¬¬ä¸€ä¸ªé¡¹ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¿™é‡Œæœ‰ä¸€ä¸ªé¢å¤–çš„æ±‚å’Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªç±»åˆ«ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½è¿™æ ·åšã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæ“ä½œä¸­ç”¨ç‚¹ç§¯æ¥è¿›è¡Œè¿™ä¸ªæ±‚å’Œã€‚
- en: So with our numpy operationã€‚ And then we have to be careful with the sizesã€‚
    So what we want at the end again is a four times4 matrix like hereã€‚ because we
    appendice to these matesã€‚ And in the beginningã€‚ our X C and our mean C has the
    size number of samples in this class times 4ã€‚ğŸ˜Šã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ numpy æ“ä½œã€‚ç„¶åæˆ‘ä»¬å¿…é¡»æ³¨æ„å°ºå¯¸ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ€åæƒ³è¦çš„æ˜¯ä¸€ä¸ªåƒè¿™é‡Œä¸€æ ·çš„ 4 ä¹˜ 4 çŸ©é˜µã€‚å› ä¸ºæˆ‘ä»¬å°†å…¶æ·»åŠ åˆ°è¿™äº›çŸ©é˜µä¸­ã€‚åœ¨å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬çš„
    X C å’Œæˆ‘ä»¬çš„å‡å€¼ C çš„å¤§å°ä¸ºè¯¥ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°é‡ä¹˜ä»¥ 4ã€‚ğŸ˜Šã€‚
- en: '![](img/c6d1044d6beca5a8660b61779287752d_14.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6d1044d6beca5a8660b61779287752d_14.png)'
- en: '![](img/c6d1044d6beca5a8660b61779287752d_15.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6d1044d6beca5a8660b61779287752d_15.png)'
- en: '![](img/c6d1044d6beca5a8660b61779287752d_16.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6d1044d6beca5a8660b61779287752d_16.png)'
- en: '![](img/c6d1044d6beca5a8660b61779287752d_17.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6d1044d6beca5a8660b61779287752d_17.png)'
- en: So we have to turn this aroundã€‚ So we have to say this is size 4 times number
    of samples in this classã€‚Because when we multiply this or when we compute the
    dot product with this one hereã€‚ which is not transpoposedseã€‚ So here we have the
    number of samples in this class times 4ã€‚ And then if we multiply thisã€‚ then we
    get a matrix of the size 4 times 4ã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è°ƒæ•´è¿™ä¸ªé¡ºåºã€‚æˆ‘ä»¬å¿…é¡»è¯´è¿™æ˜¯ 4 ä¹˜ä»¥è¯¥ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°é‡ã€‚å› ä¸ºå½“æˆ‘ä»¬å°†å…¶ç›¸ä¹˜æˆ–è®¡ç®—ä¸è¿™é‡Œæœªè½¬ç½®çš„è¿™ä¸ªç‚¹ç§¯æ—¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿™é‡Œæœ‰è¯¥ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°é‡ä¹˜ä»¥
    4ã€‚ç„¶åå¦‚æœæˆ‘ä»¬ä¹˜ä»¥è¿™ä¸ªï¼Œå°±å¾—åˆ°ä¸€ä¸ªå¤§å°ä¸º 4 ä¹˜ 4 çš„çŸ©é˜µã€‚
- en: So these are basic rules of matrix operationsã€‚ be sure that you understand thisã€‚
    So the last dimension of the first matrix must match the first dimension of the
    second matrixã€‚ And then the final output size is composed of these two sizesã€‚
    So this is why we have to transpose the first term hereã€‚ So this might be a little
    bit confusingã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™äº›æ˜¯çŸ©é˜µæ“ä½œçš„åŸºæœ¬è§„åˆ™ã€‚ç¡®ä¿ä½ ç†è§£è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œç¬¬ä¸€ä¸ªçŸ©é˜µçš„æœ€åä¸€ä¸ªç»´åº¦å¿…é¡»ä¸ç¬¬äºŒä¸ªçŸ©é˜µçš„ç¬¬ä¸€ä¸ªç»´åº¦åŒ¹é…ã€‚ç„¶åæœ€ç»ˆè¾“å‡ºå¤§å°ç”±è¿™ä¸¤ä¸ªå¤§å°ç»„æˆã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œè½¬ç½®ç¬¬ä¸€ä¸ªé¡¹ã€‚å› æ­¤ï¼Œè¿™å¯èƒ½æœ‰ç‚¹ä»¤äººå›°æƒ‘ã€‚
- en: make sure to double check this for yourselfã€‚And then we have the within class
    scatterã€‚ And now for the between class scatterï¼Œ what we want to get is the number
    of samples in this classã€‚ We get this N C by saying this is equal to Xï¼Œ C dot
    shapeã€‚ And here we want to have the index0 because we want to have the number
    of samplesã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿è‡ªå·±ä»”ç»†æ£€æŸ¥è¿™ä¸€ç‚¹ã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°äº†ç±»å†…æ•£å¸ƒã€‚ç°åœ¨å¯¹äºç±»é—´æ•£å¸ƒï¼Œæˆ‘ä»¬æƒ³è¦çš„æ˜¯è¯¥ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°é‡ã€‚æˆ‘ä»¬é€šè¿‡è¯´è¿™æ˜¯ç­‰äº Xï¼ŒC ç‚¹å½¢çŠ¶æ¥å¾—åˆ°è¿™ä¸ª N Cã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¸Œæœ›å¾—åˆ°ç´¢å¼•
    0ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦æ ·æœ¬æ•°é‡ã€‚
- en: And then here againï¼Œ we have to be careful because we have to reshape our vectorã€‚
    So let's say our mean divã€‚![](img/c6d1044d6beca5a8660b61779287752d_19.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦é‡å¡‘æˆ‘ä»¬çš„å‘é‡ã€‚å‡è®¾æˆ‘ä»¬çš„å‡å€¼é™¤ã€‚![](img/c6d1044d6beca5a8660b61779287752d_19.png)
- en: Let's have a look at the formula againã€‚ Hereã€‚ We calculate the mean of this
    class minus to total meansã€‚ Ohï¼Œ let's do thisã€‚![](img/c6d1044d6beca5a8660b61779287752d_21.png)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡æŸ¥çœ‹å…¬å¼ã€‚åœ¨è¿™é‡Œã€‚æˆ‘ä»¬è®¡ç®—è¯¥ç±»åˆ«çš„å‡å€¼å‡å»æ€»ä½“å‡å€¼ã€‚å“¦ï¼Œæ¥å§ï¼![](img/c6d1044d6beca5a8660b61779287752d_21.png)
- en: So this isï¼Œ let's say we have the mean of this class minus the mean overã€‚And
    this is only one dimensionalï¼Œ but we want to so this isï¼Œ if we have a look at
    the shapeã€‚ then this would say four comma nothingã€‚ but we want to have it to be
    4 by oneã€‚ So we have to say reshapeã€‚ And then the number of features times or
    by oneã€‚And this is becauseã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ï¼Œå‡è®¾æˆ‘ä»¬æœ‰è¯¥ç±»åˆ«çš„å‡å€¼å‡å»æ€»ä½“å‡å€¼ã€‚è¿™æ˜¯å•ç»´çš„ï¼Œä½†æˆ‘ä»¬æƒ³è¦çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹å½¢çŠ¶ï¼Œåˆ™ä¼šè¯´å››ï¼Œé›¶ã€‚ä½†æˆ‘ä»¬å¸Œæœ›å®ƒæ˜¯ 4 ä¹˜ 1ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è¯´é‡å¡‘ã€‚ç„¶åæ˜¯ç‰¹å¾æ•°é‡ä¹˜ä»¥
    1ã€‚è¿™æ˜¯å› ä¸ºã€‚
- en: againï¼Œ if we have a look at the final multiplicationã€‚ So the same way as we
    are doing a tierã€‚ we want to have a matrix of size 4 by one and multiply it with
    a matrix of one by 4ã€‚ So this is basically4 by one transposedã€‚And then we get
    a  four by four outputã€‚ So this is why we have to apply the reshape hereã€‚ And
    then we say S Bã€‚Plus equalsã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹æœ€ç»ˆçš„ä¹˜æ³•ã€‚ä¸æˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„å±‚æ¬¡ç›¸åŒã€‚æˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå¤§å°ä¸º 4 ä¹˜ 1 çš„çŸ©é˜µï¼Œå¹¶å°†å…¶ä¸ä¸€ä¸ª 1 ä¹˜ 4 çš„çŸ©é˜µç›¸ä¹˜ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯
    4 ä¹˜ 1 è½¬ç½®ã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª 4 ä¹˜ 4 çš„è¾“å‡ºã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œåº”ç”¨é‡å¡‘ã€‚ç„¶åæˆ‘ä»¬è¯´ S Bã€‚åŠ ç­‰äºã€‚
- en: And then here we have the number of samples in this class timesã€‚ And here we
    have the mean theã€‚Dotã€‚ the mean diffã€‚Transposedï¼Œ and these are both of our matricesã€‚
    So we finally have the matrices nowã€‚ And nowï¼Œ as I saidï¼Œ we have to get the inverse
    of the within class get and then multiplied with the between class getã€‚ So we
    get the inverseã€‚ Also inverse also with numpy by saying nuy L alkã€‚Dot in of S
    Wã€‚ And then dotã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™é‡Œæˆ‘ä»¬æœ‰è¿™ä¸ªç±»çš„æ ·æœ¬æ•°é‡ä¹˜ä»¥ã€‚è¿™é‡Œæœ‰å‡å€¼çš„ç‚¹ç§¯ï¼Œå‡å€¼å·®çš„è½¬ç½®ï¼Œè¿™ä¸¤ä¸ªéƒ½æ˜¯æˆ‘ä»¬çš„çŸ©é˜µã€‚æˆ‘ä»¬ç»ˆäºå¾—åˆ°äº†è¿™äº›çŸ©é˜µã€‚ç°åœ¨ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬å¿…é¡»è·å–ç±»å†…æ•£å¸ƒçš„é€†ï¼Œå¹¶ä¸ç±»é—´æ•£å¸ƒç›¸ä¹˜ã€‚æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°é€†ã€‚ä¹Ÿæ˜¯ç”¨numpyè·å–çš„ï¼Œé€šè¿‡è¯´nuy
    L alkã€‚Dot in of S Wã€‚ç„¶åç‚¹ç§¯ã€‚
- en: we multiplied with the between class scatterã€‚ And let's call this a and store
    this in this matrixã€‚ And then for thisï¼Œ we have to solve the eigenvalue and eigenvector
    problemã€‚ So we have to calculate the eigenvalues and eigenvectorsã€‚ And now the
    following code is exactly the same as in the PC A algorithmã€‚ So please check thatã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ç±»é—´æ•£å¸ƒç›¸ä¹˜ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºaï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨è¿™ä¸ªçŸ©é˜µä¸­ã€‚ç„¶åä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚æ¥ä¸‹æ¥çš„ä»£ç ä¸PC
    Aç®—æ³•å®Œå…¨ç›¸åŒã€‚æ‰€ä»¥è¯·æ£€æŸ¥ä¸€ä¸‹ã€‚
- en: So we get the eigenvalues and the eigenvectors by sayingï¼Œ this is numpyã€‚Lin
    Ark dot Ig of aã€‚And then we sort the eigenvectors and the eigenvaluesã€‚ And for
    thisã€‚ the same as we are doing it in the PC algorithmã€‚ So we transpose the eigenvectors
    by saying eigenvectors equals eigenvectors dot Tã€‚ So this makes the calculation
    easier And then we sort the eigenvaluesã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬é€šè¿‡è¯´ï¼Œè¿™æ˜¯numpyã€‚Lin Ark dot Ig of aæ¥è·å–ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚ç„¶åæˆ‘ä»¬å¯¹ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼è¿›è¡Œæ’åºã€‚å¯¹äºè¿™ä¸€ç‚¹ï¼Œå’Œæˆ‘ä»¬åœ¨PCç®—æ³•ä¸­åšçš„ç›¸åŒã€‚æ‰€ä»¥æˆ‘ä»¬é€šè¿‡è¯´ç‰¹å¾å‘é‡ç­‰äºç‰¹å¾å‘é‡ç‚¹Tæ¥è½¬ç½®ç‰¹å¾å‘é‡ã€‚è¿™æ ·å¯ä»¥ç®€åŒ–è®¡ç®—ï¼Œç„¶åæˆ‘ä»¬å¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åºã€‚
- en: So we say indices equals nuy dot arc sort offã€‚ And here we say the eigenvaluesã€‚
    and to make it a little bit niceã€‚ So we actually want the absolute value of the
    eigenvaluesã€‚And then we want to sort this in decreasing orderã€‚ So we use this
    slicing and use this little trick from start to end with a step of -1ã€‚ So this
    will turn the indices aroundã€‚ And then we have it in decreasing orderã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯´ç´¢å¼•ç­‰äºnuy dot arc sort offã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯´ç‰¹å¾å€¼ã€‚ä¸ºäº†ä½¿å…¶æ›´ç¾è§‚ï¼Œæˆ‘ä»¬å®é™…ä¸Šæƒ³è¦ç‰¹å¾å€¼çš„ç»å¯¹å€¼ã€‚ç„¶åæˆ‘ä»¬æƒ³æŒ‰é™åºæ’åºã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªåˆ‡ç‰‡å¹¶ç”¨æ­¥é•¿ä¸º-1çš„è¿™ä¸ªå°æŠ€å·§ä»å¼€å§‹åˆ°ç»“æŸã€‚è¿™æ ·å°±å¯ä»¥å°†ç´¢å¼•åè½¬ã€‚ç„¶åæˆ‘ä»¬å°±å¾—åˆ°äº†é™åºæ’åˆ—ã€‚
- en: So now let's get our eigenvalues in decreasing order by saying eigenvalues equals
    eigenvalues of these indices and the same with the eigenvectors or eigenvectors
    equals eigenvectorsã€‚ of this indicesã€‚ And then we want to store only the first
    n eigenvectorsã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬é€šè¿‡è¯´ç‰¹å¾å€¼ç­‰äºè¿™äº›ç´¢å¼•çš„ç‰¹å¾å€¼ï¼ŒæŒ‰é™åºè·å–ç‰¹å¾å€¼ï¼Œç‰¹å¾å‘é‡ä¹Ÿä¸€æ ·ï¼Œç‰¹å¾å‘é‡ç­‰äºè¿™äº›ç´¢å¼•çš„ç‰¹å¾å‘é‡ã€‚ç„¶åæˆ‘ä»¬åªæƒ³å­˜å‚¨å‰nä¸ªç‰¹å¾å‘é‡ã€‚
- en: and we store this in our linear discriminants that we have hereã€‚ So we say self
    dotã€‚Linear discriminants equals eigenvectorsã€‚ And then from the startã€‚ So the
    biggest eigenvector with the biggest or the highest eigenvalueã€‚ And then two self
    dot number of components that we specifiedã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…¶å­˜å‚¨åœ¨æˆ‘ä»¬è¿™é‡Œçš„çº¿æ€§åˆ¤åˆ«ä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´self dotã€‚çº¿æ€§åˆ¤åˆ«ç­‰äºç‰¹å¾å‘é‡ã€‚ä»å¼€å§‹çš„æœ€å¤§ç‰¹å¾å‘é‡å’Œæœ€å¤§æˆ–æœ€é«˜çš„ç‰¹å¾å€¼ã€‚ç„¶åæ˜¯self dotæŒ‡å®šçš„åˆ†é‡æ•°é‡ã€‚
- en: So this is the number of dimensions that we keepã€‚And now we are finally done
    with the fit methodã€‚ So this is the whole fit methodã€‚ and then under a transformã€‚
    the only thing that we do here is we project our data onto this new componentsã€‚
    and the transformation is nothing elseï¼Œ then the dot productã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬ä¿ç•™çš„ç»´åº¦æ•°é‡ã€‚ç°åœ¨æˆ‘ä»¬ç»ˆäºå®Œæˆäº†æ‹Ÿåˆæ–¹æ³•ã€‚è¿™å°±æ˜¯æ•´ä¸ªæ‹Ÿåˆæ–¹æ³•ã€‚ç„¶ååœ¨ä¸€ä¸ªå˜æ¢ä¸‹ï¼Œæˆ‘ä»¬è¦åšçš„å”¯ä¸€äº‹æƒ…å°±æ˜¯å°†æˆ‘ä»¬çš„æ•°æ®æŠ•å½±åˆ°è¿™äº›æ–°åˆ†é‡ä¸Šï¼Œè€Œè¿™ä¸ªå˜æ¢æ— éå°±æ˜¯ç‚¹ç§¯ã€‚
- en: So we can write this in one line and return nuy dotã€‚ and then we project our
    data onto the self dot linear discriminantsã€‚ And since we are transposing it hereï¼Œ
    we have to transpose it again hereã€‚ And then we are doneã€‚So this isï¼Œ againï¼Œ the
    same as in the PCAã€‚ Please double check this for yourselfã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆä¸€è¡Œï¼Œå¹¶è¿”å›nuy dotã€‚ç„¶åæˆ‘ä»¬å°†æ•°æ®æŠ•å½±åˆ°self dotçº¿æ€§åˆ¤åˆ«ä¸Šã€‚ç”±äºåœ¨è¿™é‡Œè¿›è¡Œäº†è½¬ç½®ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œå†æ¬¡è½¬ç½®ã€‚ç„¶åæˆ‘ä»¬å®Œæˆäº†ã€‚è¿™åŒæ ·æ˜¯PCAä¸­çš„å†…å®¹ã€‚è¯·è‡ªè¡Œä»”ç»†æ£€æŸ¥ã€‚
- en: and and now we are done and now we can run the scriptã€‚ So here I have a little
    test scriptã€‚ and this is basically the same as in the PCA testsã€‚The only thing
    that I exchange here is instead of PCã€‚ we create the LEA and want to keep two
    componentsï¼Œ and then we call the fit and the transformã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ï¼Œå¯ä»¥è¿è¡Œè„šæœ¬ã€‚è¿™é‡Œæˆ‘æœ‰ä¸€ä¸ªå°çš„æµ‹è¯•è„šæœ¬ï¼Œè¿™åŸºæœ¬ä¸Šå’ŒPCAæµ‹è¯•ä¸­çš„ç›¸åŒã€‚å”¯ä¸€è¦äº¤æ¢çš„æ˜¯ï¼Œæˆ‘ä»¬åˆ›å»ºLEAå¹¶å¸Œæœ›ä¿ç•™ä¸¤ä¸ªåˆ†é‡ï¼Œç„¶åè°ƒç”¨æ‹Ÿåˆå’Œå˜æ¢ã€‚
- en: and we do this for the Iis data setï¼Œ and then I plot the new labels that are
    project projected onto the new two dimensionsã€‚So let's run thisã€‚ So let's say
    Python LDAã€‚Underscore test up pieã€‚ and hope that everything's workingã€‚ And yeahï¼Œ
    so here we see our transposed features in only two dimensions nowã€‚ And we see
    that the classes are very goodï¼Œ separatedã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹Irisæ•°æ®é›†æ‰§è¡Œè¿™ä¸ªæ“ä½œï¼Œç„¶åæˆ‘å°†æ–°çš„æ ‡ç­¾ç»˜åˆ¶åœ¨æ–°çš„ä¸¤ä¸ªç»´åº¦ä¸Šã€‚è®©æˆ‘ä»¬è¿è¡Œè¿™ä¸ªã€‚å‡è®¾æ˜¯Python LDAã€‚ä¸‹åˆ’çº¿æµ‹è¯•é¥¼å›¾ã€‚å¸Œæœ›ä¸€åˆ‡æ­£å¸¸ã€‚æ˜¯çš„ï¼Œè¿™é‡Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„ç‰¹å¾åœ¨åªæœ‰ä¸¤ä¸ªç»´åº¦ä¸­è¢«è½¬ç½®ã€‚æˆ‘ä»¬çœ‹åˆ°ç±»åˆ«éå¸¸å¥½ï¼Œå½¼æ­¤åˆ†ç¦»ã€‚
- en: So here we have the three different iris classesã€‚ and we see that this is workingã€‚![](img/c6d1044d6beca5a8660b61779287752d_23.png)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æœ‰ä¸‰ä¸ªä¸åŒçš„é¸¢å°¾èŠ±ç±»åˆ«ã€‚æˆ‘ä»¬çœ‹åˆ°è¿™ä¸ªæ˜¯æœ‰æ•ˆçš„ã€‚![](img/c6d1044d6beca5a8660b61779287752d_23.png)
- en: So our LDA feature reduction method worksã€‚ And yeahï¼Œ pleaseï¼Œ againã€‚ compare
    this with the PCA algorithmã€‚ And I hope you enjoyed this tutorialã€‚ If you like
    thisã€‚ then please subscribe to the channel and see you next timeï¼Œ byeã€‚ğŸ˜Šã€‚![](img/c6d1044d6beca5a8660b61779287752d_25.png)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬çš„LDAç‰¹å¾é™ç»´æ–¹æ³•æœ‰æ•ˆã€‚è¯·å†æ¬¡å°†å…¶ä¸PCAç®—æ³•è¿›è¡Œæ¯”è¾ƒã€‚å¸Œæœ›ä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªï¼Œè¯·è®¢é˜…é¢‘é“ï¼Œæˆ‘ä»¬ä¸‹æ¬¡å†è§ï¼Œæ‹œã€‚ğŸ˜Šã€‚![](img/c6d1044d6beca5a8660b61779287752d_25.png)
